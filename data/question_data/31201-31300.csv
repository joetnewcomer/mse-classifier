,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Probability and permutations,Probability and permutations,,"I performed an experiment in which an individual had to order 5 items (i.e. his ""response"" was something like $(3,2,1,4,5)$ or some other permutation). The correct ordering was $(1,2,3,4,5)$ and I want to know the probability of getting 1, 2, 3 or 5 of the numbers right by chance alone. I know that the probability of getting all right by chance alone is $1/5!$, and I think that probability of getting 3 right is: $$ \frac{5 \choose 2}{5!} $$ Because if you get 3 right you are just ""swapping"" two items from $(1,2,3,4,5)$. I am having difficulty to deduce the rest of the probabilities. I have written an R code that simulates the process, so I have an approximate answer. I just wanted to know how to do it by hand. Thank you in advance!","I performed an experiment in which an individual had to order 5 items (i.e. his ""response"" was something like $(3,2,1,4,5)$ or some other permutation). The correct ordering was $(1,2,3,4,5)$ and I want to know the probability of getting 1, 2, 3 or 5 of the numbers right by chance alone. I know that the probability of getting all right by chance alone is $1/5!$, and I think that probability of getting 3 right is: $$ \frac{5 \choose 2}{5!} $$ Because if you get 3 right you are just ""swapping"" two items from $(1,2,3,4,5)$. I am having difficulty to deduce the rest of the probabilities. I have written an R code that simulates the process, so I have an approximate answer. I just wanted to know how to do it by hand. Thank you in advance!",,"['probability', 'combinatorics']"
1,"Probability - Couples randomly sitting at a table, calculate the probability that they are together","Probability - Couples randomly sitting at a table, calculate the probability that they are together",,"I'm currently practicing for my first actuarial exam and came across this problem.  The posted solution doesn't make sense to me, and even if I'm right I don't know the correct way to do it. The problem: 13 married couples are seated randomly at a round table.  Calculate E(X), where X is the number of husbands sitting next to their wives. The given solution: Consider an individual couple.  The probability that that couple is seated together is $\frac 2 {25}$, so E(X) = $13(\frac 2 {25})$ = $\frac {26} {25}$ Me: What?  These aren't independent events!  I'm going to brute force a smaller version of this problem... So I decided to tackle the problem for 2 couples instead of 13.  This gives us 24 permutations, 17 of which have both couples sitting together (X=2) and the rest of which have none (X=0).  Therefore E(X) = $\frac {34} {24}$ Using the solution from above, $2 (\frac 2 3) = \frac 4 3$. To repeat my actual question: I'm pretty sure the given solution is wrong but I don't know what right is, so I'm looking for either an explanation for the flaw in my reasoning or the correct answer. EDIT: OK, I rechecked my work and found my error.  There are actually 16 permutations making the answer for N=2 $\frac {32} {24} = \frac 4 3$.  I'll be off to bed now.","I'm currently practicing for my first actuarial exam and came across this problem.  The posted solution doesn't make sense to me, and even if I'm right I don't know the correct way to do it. The problem: 13 married couples are seated randomly at a round table.  Calculate E(X), where X is the number of husbands sitting next to their wives. The given solution: Consider an individual couple.  The probability that that couple is seated together is $\frac 2 {25}$, so E(X) = $13(\frac 2 {25})$ = $\frac {26} {25}$ Me: What?  These aren't independent events!  I'm going to brute force a smaller version of this problem... So I decided to tackle the problem for 2 couples instead of 13.  This gives us 24 permutations, 17 of which have both couples sitting together (X=2) and the rest of which have none (X=0).  Therefore E(X) = $\frac {34} {24}$ Using the solution from above, $2 (\frac 2 3) = \frac 4 3$. To repeat my actual question: I'm pretty sure the given solution is wrong but I don't know what right is, so I'm looking for either an explanation for the flaw in my reasoning or the correct answer. EDIT: OK, I rechecked my work and found my error.  There are actually 16 permutations making the answer for N=2 $\frac {32} {24} = \frac 4 3$.  I'll be off to bed now.",,['probability']
2,Probability of Monkey typing keyboard,Probability of Monkey typing keyboard,,"A monkey types at a 26-letter keyboard with one key corresponding to each of the lower-case English letters. Each keystroke is chosen independently and uniformly at random from the 26 possibilities. If the monkey types 1 million letters, what is the expected number of times the sequence ""bonbon"" appears? P.S bonbonbon counts as two appearances. My approach is Let the indicator $I_B$ be the event that ""bonbon"" appears, then $P(I_B) = 1/26^6$ Then $E(X) = E(I_{B_1} + I_{B_2} + \dotsb + I_{B_\text{1 million}}) = 1\text{ million} \times 1/26^6$ Is my approach right? Somehow I feel I have done something wrong here.","A monkey types at a 26-letter keyboard with one key corresponding to each of the lower-case English letters. Each keystroke is chosen independently and uniformly at random from the 26 possibilities. If the monkey types 1 million letters, what is the expected number of times the sequence ""bonbon"" appears? P.S bonbonbon counts as two appearances. My approach is Let the indicator $I_B$ be the event that ""bonbon"" appears, then $P(I_B) = 1/26^6$ Then $E(X) = E(I_{B_1} + I_{B_2} + \dotsb + I_{B_\text{1 million}}) = 1\text{ million} \times 1/26^6$ Is my approach right? Somehow I feel I have done something wrong here.",,"['probability', 'statistics']"
3,Function of a random variable: expectation,Function of a random variable: expectation,,"Let $\{X_i\}_{i=1}^n$  be a sequence of i.i.d. random variables (i.e. a random sample) with pdf: $$f_X(x) = e^{-(x-\theta)} \, e^{-e^{-(x-\theta)}} · \mathbf{1}_{x\in \mathbf{R}}$$ The goal is to find the distribution of $T = \sum_{i=1}^n e^{-X_i}$ and also to compute $\textbf{E}(\log T)$  and $\textbf{V}(\log T)$. Some thoughts: I think I have found the distribution of $T$ by applying the transformation $Y =  e^{-X}$. If I am not wrong, it is quite easy to see that  $Y \sim \textrm{Exponential}(e^{\theta})$. Therefore, $T = \sum_{i=1}^n Y_i \sim \textrm{Gamma} (n, 1/e^{\theta})$. However, I am unable to find a reasonable way to compute $\textbf{E}(\log T)$ or $\textbf{V}(\log T)$. The formula for the expectation of a function of a random variable leads to a very complicated integral and the only alternative I can think of, which is yet another transformation, is even worse!","Let $\{X_i\}_{i=1}^n$  be a sequence of i.i.d. random variables (i.e. a random sample) with pdf: $$f_X(x) = e^{-(x-\theta)} \, e^{-e^{-(x-\theta)}} · \mathbf{1}_{x\in \mathbf{R}}$$ The goal is to find the distribution of $T = \sum_{i=1}^n e^{-X_i}$ and also to compute $\textbf{E}(\log T)$  and $\textbf{V}(\log T)$. Some thoughts: I think I have found the distribution of $T$ by applying the transformation $Y =  e^{-X}$. If I am not wrong, it is quite easy to see that  $Y \sim \textrm{Exponential}(e^{\theta})$. Therefore, $T = \sum_{i=1}^n Y_i \sim \textrm{Gamma} (n, 1/e^{\theta})$. However, I am unable to find a reasonable way to compute $\textbf{E}(\log T)$ or $\textbf{V}(\log T)$. The formula for the expectation of a function of a random variable leads to a very complicated integral and the only alternative I can think of, which is yet another transformation, is even worse!",,['probability']
4,The number of functions with a certain property,The number of functions with a certain property,,"Let $f$ be chosen uniformly at random from all functions $f:\{1,\ldots,n\}\rightarrow\{1,\ldots,n\}$ such that $f(k)\in\{1,\ldots,k\}$ for $1\leq k\leq n$. What is the probability that $f$ is non-decreasing? Now, the number of all the function is $n!$ (if I'm not wrong) so I want to know the number of the non-decreasing ones. My approach was to build a rooted tree, in an obvious way such that the number of functions is exactly the number of vertices in the last level. But I'm not able to count this number, could any of you help me? (if you have a different approch is good).","Let $f$ be chosen uniformly at random from all functions $f:\{1,\ldots,n\}\rightarrow\{1,\ldots,n\}$ such that $f(k)\in\{1,\ldots,k\}$ for $1\leq k\leq n$. What is the probability that $f$ is non-decreasing? Now, the number of all the function is $n!$ (if I'm not wrong) so I want to know the number of the non-decreasing ones. My approach was to build a rooted tree, in an obvious way such that the number of functions is exactly the number of vertices in the last level. But I'm not able to count this number, could any of you help me? (if you have a different approch is good).",,"['probability', 'combinatorics']"
5,Is order of variables important in probability chain rule,Is order of variables important in probability chain rule,,"Is the order of random variables important in the chain rule? I mean, is this true: $P(A,B,C) = P(A)\times P(B|A)\times P(C|A,B) = P(C)\times P(B|C)\times P(A|B,C) = P(C,B,A)$? If it is, what is the meaning of such order? Thank you.","Is the order of random variables important in the chain rule? I mean, is this true: $P(A,B,C) = P(A)\times P(B|A)\times P(C|A,B) = P(C)\times P(B|C)\times P(A|B,C) = P(C,B,A)$? If it is, what is the meaning of such order? Thank you.",,['probability']
6,"Confidence interval for parameter of normal distribution $X_i\sim N(\theta,\theta^2)$ with equal mean and standard deviation",Confidence interval for parameter of normal distribution  with equal mean and standard deviation,"X_i\sim N(\theta,\theta^2)","A sample $X_1,\dots,X_n$ is drawn from the normal distribution $N(\theta,\theta^2)$ . I am asked to find a $90\%$ confidence interval for the population mean $\theta$ . Let $X_i\sim N(\theta,\theta^2)$ with $$\mathbb{E}(X_i)=\theta \text{ and } \mathbb{V}(X_i)=\theta^2$$ then the random variable $\bar{X}$ has $\mathbb{E}(\bar{X})=\theta \text{ and }\mathbb{V}(\bar{X})=\frac{\theta^2}{n}$ so, by virtue of the CLT we have that $$\bar{X}\sim N\Big(\theta,\frac{\theta^2}{n}\Big)$$ Now, standardizing we get $$\frac{\bar{X}-\theta}{\frac{\theta}{\sqrt{n}}}\sim N(0,1)$$ If we are asked to give a $90\%$ confidence interval for $\theta$ and we know that the $90\%$ confidence interval for a $N(0,1)$ is $$(-1.64,1.64)$$ we can see that $$-1.64<\frac{\bar{X}-\theta}{\frac{\theta}{\sqrt{n}}}<1.64$$ $$-\frac{1.64}{\sqrt{n}}<\frac{\bar{X}-\theta}{\theta}<\frac{1.64}{\sqrt{n}}$$ $$1-\frac{1.64}{\sqrt{n}}<\frac{\bar{X}}{\theta}<1+\frac{1.64}{\sqrt{n}}$$ $$\frac{1-\frac{1.64}{\sqrt{n}}}{\bar{X}}<\frac{1}{\theta}<\frac{1+\frac{1.64}{\sqrt{n}}}{\bar{X}}$$ $$\frac{\bar{X}}{1+\frac{1.64}{\sqrt{n}}}<\theta<\frac{\bar{X}}{1-\frac{1.64}{\sqrt{n}}}$$ is my confidence interval, right? I wanted to be sure that my result was correct and so I wanted to know your oppinion. Have I done everything correctly? Thank you.","A sample is drawn from the normal distribution . I am asked to find a confidence interval for the population mean . Let with then the random variable has so, by virtue of the CLT we have that Now, standardizing we get If we are asked to give a confidence interval for and we know that the confidence interval for a is we can see that is my confidence interval, right? I wanted to be sure that my result was correct and so I wanted to know your oppinion. Have I done everything correctly? Thank you.","X_1,\dots,X_n N(\theta,\theta^2) 90\% \theta X_i\sim N(\theta,\theta^2) \mathbb{E}(X_i)=\theta \text{ and } \mathbb{V}(X_i)=\theta^2 \bar{X} \mathbb{E}(\bar{X})=\theta \text{ and }\mathbb{V}(\bar{X})=\frac{\theta^2}{n} \bar{X}\sim N\Big(\theta,\frac{\theta^2}{n}\Big) \frac{\bar{X}-\theta}{\frac{\theta}{\sqrt{n}}}\sim N(0,1) 90\% \theta 90\% N(0,1) (-1.64,1.64) -1.64<\frac{\bar{X}-\theta}{\frac{\theta}{\sqrt{n}}}<1.64 -\frac{1.64}{\sqrt{n}}<\frac{\bar{X}-\theta}{\theta}<\frac{1.64}{\sqrt{n}} 1-\frac{1.64}{\sqrt{n}}<\frac{\bar{X}}{\theta}<1+\frac{1.64}{\sqrt{n}} \frac{1-\frac{1.64}{\sqrt{n}}}{\bar{X}}<\frac{1}{\theta}<\frac{1+\frac{1.64}{\sqrt{n}}}{\bar{X}} \frac{\bar{X}}{1+\frac{1.64}{\sqrt{n}}}<\theta<\frac{\bar{X}}{1-\frac{1.64}{\sqrt{n}}}","['probability', 'statistics', 'normal-distribution', 'central-limit-theorem', 'confidence-interval']"
7,4-digit password that have exactly 2 digits in common with a given password.,4-digit password that have exactly 2 digits in common with a given password.,,"Question: A system generates a 4 unique digits password. Extracting 2 passwords created independently and randomly, what is the probability that they have exactly 2 digits in common? My attempt: There are $\frac{10!}{(10-4)!} = 5040 $ different passwords in the sample space. If we take one them, say $2647$ ,  there are ${4 \choose 2} = 6$ cases of coincidence: $\{2,6\},\{2,4\},\{2,7\},\{6,4\},\{6,7\},\{4,7\}$ . So, if we consider the first case, there are 30 passwords that have a 2 and a 6 and also don't have a 4 and a 7. If we shuffle the digits, we still get valid passwords, so in the first case there are $30 \times 4! = 720$ passwords. The reasoning for the other cases is identical, so we must have $720 \times 6$ passwords in total that match the given password in exatcly 2 digits. So the probability asked is $\frac{720 \times 6}{5040} = \frac{6}{7}$ the supposed answer is $\frac{3}{7}$ What did I miss? Thanks in advance.","Question: A system generates a 4 unique digits password. Extracting 2 passwords created independently and randomly, what is the probability that they have exactly 2 digits in common? My attempt: There are different passwords in the sample space. If we take one them, say ,  there are cases of coincidence: . So, if we consider the first case, there are 30 passwords that have a 2 and a 6 and also don't have a 4 and a 7. If we shuffle the digits, we still get valid passwords, so in the first case there are passwords. The reasoning for the other cases is identical, so we must have passwords in total that match the given password in exatcly 2 digits. So the probability asked is the supposed answer is What did I miss? Thanks in advance.","\frac{10!}{(10-4)!} = 5040  2647 {4 \choose 2} = 6 \{2,6\},\{2,4\},\{2,7\},\{6,4\},\{6,7\},\{4,7\} 30 \times 4! = 720 720 \times 6 \frac{720 \times 6}{5040} = \frac{6}{7} \frac{3}{7}","['probability', 'combinatorics']"
8,Expected winning score in a round robin tournament,Expected winning score in a round robin tournament,,"Suppose you have a tournament consisting of 100 players in which each player plays each other player exactly once. In any given match, the chance of either playing winning is equally likely, and there are no ties. Winning a match gives $1$ point, and losing a match gives no points. After all matches have been played, we rank the players according to their score (from highest to lowest), with ties broken at random. For each $n=1,2,\dots, 100$ , let $S_n$ be the score of the player ranked $n$ out of $100$ . Question: What is the expected value of $S_1$ ? If it's difficult to find this analytically, how can we approximate the value? The same question holds for other variables, e.g. $S_{10}$ , $S_{50}$ . Each individual player's score follows a binomial distribution, but these aren't independent: for instance, the total number of points is fixed. One can bound $S_n$ through a bit of combinatorics: e.g., given that there are $4950$ points in total, we must have $S_1 \geq 50$ , and we can construct a scenario in which $S_1 = 99$ (which is maximal), so perhaps $S_1 \approx 75$ , the midpoint? If I'm looking at $S_{10}$ instead, similar considerations give what I think is $45 \leq S_{10} \leq 94$ . But I'm not sure how to evaluate $\mathbb E(S_{10})$ without further considerations.","Suppose you have a tournament consisting of 100 players in which each player plays each other player exactly once. In any given match, the chance of either playing winning is equally likely, and there are no ties. Winning a match gives point, and losing a match gives no points. After all matches have been played, we rank the players according to their score (from highest to lowest), with ties broken at random. For each , let be the score of the player ranked out of . Question: What is the expected value of ? If it's difficult to find this analytically, how can we approximate the value? The same question holds for other variables, e.g. , . Each individual player's score follows a binomial distribution, but these aren't independent: for instance, the total number of points is fixed. One can bound through a bit of combinatorics: e.g., given that there are points in total, we must have , and we can construct a scenario in which (which is maximal), so perhaps , the midpoint? If I'm looking at instead, similar considerations give what I think is . But I'm not sure how to evaluate without further considerations.","1 n=1,2,\dots, 100 S_n n 100 S_1 S_{10} S_{50} S_n 4950 S_1 \geq 50 S_1 = 99 S_1 \approx 75 S_{10} 45 \leq S_{10} \leq 94 \mathbb E(S_{10})","['probability', 'combinatorics', 'expected-value']"
9,Probability of line segments intersecting on a plane - A generalization to Buffon's needle problem,Probability of line segments intersecting on a plane - A generalization to Buffon's needle problem,,"I came up with this problem: If I draw a length 1 line segment randomly, then draw another one, what's the probability that they'll intersect? More precisely, Consider a rectangular area of size $w\times l$ with periodic boundary conditions, where $w\ge 2,l\ge 2$ . Draw two length 1 line segments randomly. Here, by randomly it means, first choose a random point with uniform distribution over the rectangular area, then from the circle that is centered at the first point with radius one, randomly choose a point with uniform distribution. These two points are the end points of the line segment. Question: What's the probability that the two line segments intersect? The solution can be found by simply integrating the probability densities, which gives $p=\frac{1}{A}\times 4\times\frac{1}{2\pi}(p_1+p_2+p_3+p_4)$ , where $A=lw$ is the area, and $p_1=\int_0^1 rdr\int_0^{\frac{\pi}{2}}\theta-\sin^{-1}(r\sin\theta)d\theta$ $p_2=\int_{\frac{1}{2}}^1dx\int_0^\sqrt{1-x^2}\tan^{-1}\frac{x}{y}+\tan^{-1}\frac{1-x}{y}dy$ $\quad=\frac{1}{2}\int_0^1 rdr\int_{\sin^{-1}\frac{r}{2}}^{\frac{\pi}{2}}\theta+\tan^{-1}\frac{1-r\sin\theta}{r\cos\theta}d\theta$ $p_3=\int_0^1 rdr\int_0^{\sin^{-1}\frac{r}{2}}\theta+\cos^{-1}(r\cos\theta)d\theta$ $p_4=\int_0^{\frac{1}{2}}dx\int_\sqrt{1-x^2}^1 2\cos^{-1}y\ dy$ And the results are $p_1=\frac{1}{4}$ $p_2=0.583664$ $p_3=0.155138$ $p_4=0.011197 =\frac{1}{72}(27 - 3 \sqrt 3 \pi - \pi^2)$ Details can be found on my blog post . The summation of these numbers is 0.999999, which is probably not a coincidence. My questions are, Do they sum up to exactly 1, so that the probability is exactly $\frac{2}{\pi A}$ ? (Probably true, just need to find the exact integration value.) (✓ Proved) Is there an easier and more intuitive way to prove this? How can this be generalized? I searched similar problems but I didn't find anything quite the same. Considering that Buffon's needle problem has probability $\frac{2l}{\pi t}$ , are they somehow related? A more general description of this problem can be stated as the following: Assuming there is a random uniform distribution with density $\frac{1}{A}$ of line segments with length 1 on the 2D plane. More precisely, the distribution is, the middle points of the line segments are uniformly distributed on the plane with density $\frac{1}{A}$ and the angles are also uniformly distributed from 0 to $2\pi$ . Then, if we draw another line segment of length one randomly, as described in the beginning, the expected number of intersections between this line segment and the line segments in the ""background"" is $\frac{2}{\pi A}$ . Edit: I think, indeed, the Buffon's needle problem can be seen as a special case of this problem . If we see each line as an infinite chain of length 1 line segments, and the distance between the lines is $t$ , then each line segment occupies an area $t$ , thus the expectation of the number of intersections between a length 1 needle with the lines is $\frac{2}{\pi A}=\frac{2}{\pi t}$ . Due to the linearity of expectation, a length $l$ needle will have expectation $\frac{2l}{\pi t}$ . When $l\leq t$ , there can be at most one intersection, thus this is the probability of them intersecting. Update 2023.10.30 After seeing @Claude Leibovici's answer, it seems that the integration of $p_3$ is particularly complicated, so I thought maybe doing it in the other direction is easier. If we integrate $r$ first instead,the interval of $\theta$ is $[0,\frac{\pi}{6}]$ and the interval of $r$ is $[2\sin\theta,1]$ . The integration is $p_3=\int_0^{\frac{\pi}{6}}d\theta\int_{2\sin\theta}^1 (\theta+\cos^{-1}(r\cos\theta))rdr$ $\quad=\int_0^{\frac{\pi}{6}}\frac{\theta}{2}(1-4\sin^2\theta)+\left[\frac{r^2\cos^2\theta}{2}\cos^{-1}(r\cos\theta)+\frac{1}{4}\sin^{-1}(r\cos\theta)-\frac{r\cos\theta}{4}\sqrt{1-r^2\cos^2\theta}\right]_{2\sin\theta}^1\frac{1}{\cos^2\theta}d\theta$ $\quad=\int_0^{\frac{\pi}{6}}\frac{\theta}{2}(1-4\sin^2\theta)+\left(\theta\frac{\cos^2\theta}{2}+\frac{1}{4}(\frac{\pi}{2}-\theta)-\frac{\sin\theta\cos\theta}{4}-\frac{\sin^2 2\theta}{2}(\frac{\pi}{2}-2\theta)-\frac{1}{4}(2\theta)+\frac{\sin 2\theta \cos 2\theta}{4}\right)\frac{1}{\cos^2\theta}d\theta$ which is exactly $\frac{1}{36}(9+3\sqrt 3\pi-2\pi^2)$ . Combined with the other exact results, I think it's safe to say that the probability equals $\frac{2}{\pi A}$ is proved? Some variations that I can think of: Change background line segments into disks of radius 1. (Should be $\frac{1}{A}\left(\pi+\int_1^{\sqrt 2} 2r\sin^{-1}\left(\frac{1}{r}\right)dr+\int_{\sqrt 2}^2 2r\cos^{-1}\left(\frac{r}{2}\right)dr\right)=\frac{\pi+2}{A}$ if my calculation is correct.) Disks of radius R. Empty circles of radius R<0.5. (Expected number of intersections, or probability of intersecting.) Empty circles of radius R>0.5. (Expected number of intersections, or probability of intersecting.) Are the expected numbers for cases 3 and 4 the same as considering the circle as the limit of an $n$ -gon as $n\rightarrow\infty$ ? From the linearity of expectation, I expect that... If these problems turn out to be interesting, I'll post them in a seperate question.","I came up with this problem: If I draw a length 1 line segment randomly, then draw another one, what's the probability that they'll intersect? More precisely, Consider a rectangular area of size with periodic boundary conditions, where . Draw two length 1 line segments randomly. Here, by randomly it means, first choose a random point with uniform distribution over the rectangular area, then from the circle that is centered at the first point with radius one, randomly choose a point with uniform distribution. These two points are the end points of the line segment. Question: What's the probability that the two line segments intersect? The solution can be found by simply integrating the probability densities, which gives , where is the area, and And the results are Details can be found on my blog post . The summation of these numbers is 0.999999, which is probably not a coincidence. My questions are, Do they sum up to exactly 1, so that the probability is exactly ? (Probably true, just need to find the exact integration value.) (✓ Proved) Is there an easier and more intuitive way to prove this? How can this be generalized? I searched similar problems but I didn't find anything quite the same. Considering that Buffon's needle problem has probability , are they somehow related? A more general description of this problem can be stated as the following: Assuming there is a random uniform distribution with density of line segments with length 1 on the 2D plane. More precisely, the distribution is, the middle points of the line segments are uniformly distributed on the plane with density and the angles are also uniformly distributed from 0 to . Then, if we draw another line segment of length one randomly, as described in the beginning, the expected number of intersections between this line segment and the line segments in the ""background"" is . Edit: I think, indeed, the Buffon's needle problem can be seen as a special case of this problem . If we see each line as an infinite chain of length 1 line segments, and the distance between the lines is , then each line segment occupies an area , thus the expectation of the number of intersections between a length 1 needle with the lines is . Due to the linearity of expectation, a length needle will have expectation . When , there can be at most one intersection, thus this is the probability of them intersecting. Update 2023.10.30 After seeing @Claude Leibovici's answer, it seems that the integration of is particularly complicated, so I thought maybe doing it in the other direction is easier. If we integrate first instead,the interval of is and the interval of is . The integration is which is exactly . Combined with the other exact results, I think it's safe to say that the probability equals is proved? Some variations that I can think of: Change background line segments into disks of radius 1. (Should be if my calculation is correct.) Disks of radius R. Empty circles of radius R<0.5. (Expected number of intersections, or probability of intersecting.) Empty circles of radius R>0.5. (Expected number of intersections, or probability of intersecting.) Are the expected numbers for cases 3 and 4 the same as considering the circle as the limit of an -gon as ? From the linearity of expectation, I expect that... If these problems turn out to be interesting, I'll post them in a seperate question.","w\times l w\ge 2,l\ge 2 p=\frac{1}{A}\times 4\times\frac{1}{2\pi}(p_1+p_2+p_3+p_4) A=lw p_1=\int_0^1 rdr\int_0^{\frac{\pi}{2}}\theta-\sin^{-1}(r\sin\theta)d\theta p_2=\int_{\frac{1}{2}}^1dx\int_0^\sqrt{1-x^2}\tan^{-1}\frac{x}{y}+\tan^{-1}\frac{1-x}{y}dy \quad=\frac{1}{2}\int_0^1 rdr\int_{\sin^{-1}\frac{r}{2}}^{\frac{\pi}{2}}\theta+\tan^{-1}\frac{1-r\sin\theta}{r\cos\theta}d\theta p_3=\int_0^1 rdr\int_0^{\sin^{-1}\frac{r}{2}}\theta+\cos^{-1}(r\cos\theta)d\theta p_4=\int_0^{\frac{1}{2}}dx\int_\sqrt{1-x^2}^1 2\cos^{-1}y\ dy p_1=\frac{1}{4} p_2=0.583664 p_3=0.155138 p_4=0.011197 =\frac{1}{72}(27 - 3 \sqrt 3 \pi - \pi^2) \frac{2}{\pi A} \frac{2l}{\pi t} \frac{1}{A} \frac{1}{A} 2\pi \frac{2}{\pi A} t t \frac{2}{\pi A}=\frac{2}{\pi t} l \frac{2l}{\pi t} l\leq t p_3 r \theta [0,\frac{\pi}{6}] r [2\sin\theta,1] p_3=\int_0^{\frac{\pi}{6}}d\theta\int_{2\sin\theta}^1 (\theta+\cos^{-1}(r\cos\theta))rdr \quad=\int_0^{\frac{\pi}{6}}\frac{\theta}{2}(1-4\sin^2\theta)+\left[\frac{r^2\cos^2\theta}{2}\cos^{-1}(r\cos\theta)+\frac{1}{4}\sin^{-1}(r\cos\theta)-\frac{r\cos\theta}{4}\sqrt{1-r^2\cos^2\theta}\right]_{2\sin\theta}^1\frac{1}{\cos^2\theta}d\theta \quad=\int_0^{\frac{\pi}{6}}\frac{\theta}{2}(1-4\sin^2\theta)+\left(\theta\frac{\cos^2\theta}{2}+\frac{1}{4}(\frac{\pi}{2}-\theta)-\frac{\sin\theta\cos\theta}{4}-\frac{\sin^2 2\theta}{2}(\frac{\pi}{2}-2\theta)-\frac{1}{4}(2\theta)+\frac{\sin 2\theta \cos 2\theta}{4}\right)\frac{1}{\cos^2\theta}d\theta \frac{1}{36}(9+3\sqrt 3\pi-2\pi^2) \frac{2}{\pi A} \frac{1}{A}\left(\pi+\int_1^{\sqrt 2} 2r\sin^{-1}\left(\frac{1}{r}\right)dr+\int_{\sqrt 2}^2 2r\cos^{-1}\left(\frac{r}{2}\right)dr\right)=\frac{\pi+2}{A} n n\rightarrow\infty","['probability', 'integration', 'geometry', 'geometric-probability']"
10,Random walk on a k-dimensional grid,Random walk on a k-dimensional grid,,"Consider a $k$ -dimensional grid with integer points and let's begin our random walk at the origin $(0,0,\dots,0)$ . Every step you have to move on each cartesian axis in the following way: $x_1$ axis: one step forward with probability $\frac{1}{3}$ , one step backward with probability $\frac{1}{3}$ , remain where you were with probability $\frac{1}{3}$ . $ \vdots $ $x_k$ axis : one step forward with probability $\frac{1}{3}$ , one step backward with probability $\frac{1}{3}$ , remain where you were with probability $\frac{1}{3}$ . So we can define the random discrete vector $U_i=(X_{i1},\dots,X_{ik})$ that register the random actions taken on each axis at the $i$ -th step of our random walk. The marginal density function of the random variable $X_{ij}$ is: $$ f_{X_{ij}}(t) = \begin{cases} \frac{1}{3} \mbox{ if $t\in\{-1,0,1\}$} \\ 0 \mbox{ otherwise}\end{cases} $$ We would like to compute the following probability: $$ P\Biggl(\bigcup_{j=1}^{k}\Bigl\{\Bigl|\sum_{i=1}^{N}X_{ij}\Bigr|>\sqrt{N}\Bigr\}\Biggr) $$ as the number of steps $N$ approaches infinity. So we would like to know the probability after $N$ steps (where $N$ is a sufficiently large number) to be outside the $k$ -dimensional cube with size $2\sqrt{N}$ . $k=1$ case We begin with the 1-dimensional case (a line). We want to compute: $$ P\Bigl(\Bigl|\sum_{i=1}^{N}X_i\Bigr|>\sqrt{N}\Bigr) = P\Bigl(\sum_{i=1}^{N}X_i>\sqrt{N}\Bigr) + P\Bigl(\sum_{i=1}^{N}X_i<-\sqrt{N}\Bigr) = 2\cdot P\Bigl(\sum_{i=1}^{N}X_i>\sqrt{N}\Bigr) $$ Since the random variables $\{X_i\}_{i=1}^{N}$ are indipendents and equally distributed, and $\mathbb{E}(X_i) = 0$ , $Var(X_i) = \frac{2}{3}$ , then we can apply the central limit theorem, so that: $$ P\Bigl(\sum_{i=1}^{N}X_i>\sqrt{N}\Bigr) = P\Bigl(\frac{\sum_{i=1}^{N}X_i}{\sqrt{N}\sqrt{\frac{2}{3}}}>\frac{\sqrt{N}}{\sqrt{N}\sqrt{\frac{2}{3}}}\Bigr) = 1- P\Bigl(\frac{\sum_{i=1}^{N}X_i}{\sqrt{N}\sqrt{\frac{2}{3}}}\leq\sqrt{\frac{3}{2}}\Bigr) \longrightarrow 1-\phi\Bigl(\sqrt{\frac{3}{2}}\Bigr) \approx 0.1112 $$ where $\phi(t) = \frac{1}{\sqrt{2\pi}}\int_{\infty}^{t}e^{-\frac{s^2}{2}}ds$ is the cumulative distribution function of a standard normal variable ( $Z\sim\mathcal{N}(0,1)$ ). So the probability requested is $2\cdot 0.1112 = 0.2224$ . $k$ -dimensional case Call $P\Bigl(\Bigl|\sum_{i=1}^{N}X_{ij}\Bigr|>\sqrt{N}\Bigr) = p = 0.2224$ which is the same for all axes ( $p$ is indipendent from the index $j$ ). $$ P\Biggl(\bigcup_{j=1}^{k}\Bigl\{\Bigl|\sum_{i=1}^{N}X_{ij}\Bigr|>\sqrt{N}\Bigr\}\Biggr) = \sum_{j=1}^{k}p - \sum_{j<m}^{k}p^2 + \dots + (-1)^{k+1}p^k = \sum_{h=1}^{k}(-1)^{h+1}\binom{k}{h}p^h = 1-(1-p)^{k} $$ Where we have applied the fact that the random variables $\Bigl|\sum_{i=1}^{N}X_{i1}\Bigr|,\dots,\Bigl|\sum_{i=1}^{N}X_{ik}\Bigr|$ are indipendent, so for example in the second term: $$ P\Bigl(\Bigl\{\Bigl|\sum_{i=1}^{N}X_{ij}\Bigr|>\sqrt{N}\Bigr\}\cap\Bigl\{\Bigl|\sum_{i=1}^{N}X_{im}\Bigr|>\sqrt{N}\Bigr\}\Bigr) = P\Bigl(\Bigl\{\Bigl|\sum_{i=1}^{N}X_{ij}\Bigr|>\sqrt{N}\Bigr\}\Bigr)P\Bigl(\Bigl\{\Bigl|\sum_{i=1}^{N}X_{im}\Bigr|>\sqrt{N}\Bigr\}\Bigr) = p^{2}. $$ Limit case: $k\to\infty$ We can now easily compute the following probability: $$ P\Biggl(\bigcup_{j=1}^{\infty}\Bigl\{\Bigl|\sum_{i=1}^{N}X_{ij}\Bigr|>\sqrt{N}\Bigr\}\Biggr) = \lim_{k\to\infty} 1-(1-p)^{k} = 1 $$ since $p=0.2224\in(0,1) \implies (1-p)\in(0,1)$ . So the probability of escaping from a $k$ -dimensional grid of size $2\sqrt{N}$ as $N\to\infty$ and $k\to\infty$ approaches the value 1! My question: Do you think that this process is correct? I first applied the CLT and then used the indipendence of these random variables (since each step our ""point"" has to make a move across all the axes, and the movement made on the axis $x_i$ does not influence the movement made on the axis $x_j$ ). Table of values: $k=1 \implies P = 0.2224$ ; $k=2 \implies P = 0.3978$ ; $k=3 \implies P = 0.5327$ ; $\vdots$","Consider a -dimensional grid with integer points and let's begin our random walk at the origin . Every step you have to move on each cartesian axis in the following way: axis: one step forward with probability , one step backward with probability , remain where you were with probability . axis : one step forward with probability , one step backward with probability , remain where you were with probability . So we can define the random discrete vector that register the random actions taken on each axis at the -th step of our random walk. The marginal density function of the random variable is: We would like to compute the following probability: as the number of steps approaches infinity. So we would like to know the probability after steps (where is a sufficiently large number) to be outside the -dimensional cube with size . case We begin with the 1-dimensional case (a line). We want to compute: Since the random variables are indipendents and equally distributed, and , , then we can apply the central limit theorem, so that: where is the cumulative distribution function of a standard normal variable ( ). So the probability requested is . -dimensional case Call which is the same for all axes ( is indipendent from the index ). Where we have applied the fact that the random variables are indipendent, so for example in the second term: Limit case: We can now easily compute the following probability: since . So the probability of escaping from a -dimensional grid of size as and approaches the value 1! My question: Do you think that this process is correct? I first applied the CLT and then used the indipendence of these random variables (since each step our ""point"" has to make a move across all the axes, and the movement made on the axis does not influence the movement made on the axis ). Table of values: ; ; ;","k (0,0,\dots,0) x_1 \frac{1}{3} \frac{1}{3} \frac{1}{3}  \vdots  x_k \frac{1}{3} \frac{1}{3} \frac{1}{3} U_i=(X_{i1},\dots,X_{ik}) i X_{ij} 
f_{X_{ij}}(t) = \begin{cases} \frac{1}{3} \mbox{ if t\in\{-1,0,1\}} \\
0 \mbox{ otherwise}\end{cases}
 
P\Biggl(\bigcup_{j=1}^{k}\Bigl\{\Bigl|\sum_{i=1}^{N}X_{ij}\Bigr|>\sqrt{N}\Bigr\}\Biggr)
 N N N k 2\sqrt{N} k=1 
P\Bigl(\Bigl|\sum_{i=1}^{N}X_i\Bigr|>\sqrt{N}\Bigr) = P\Bigl(\sum_{i=1}^{N}X_i>\sqrt{N}\Bigr) + P\Bigl(\sum_{i=1}^{N}X_i<-\sqrt{N}\Bigr) = 2\cdot P\Bigl(\sum_{i=1}^{N}X_i>\sqrt{N}\Bigr)
 \{X_i\}_{i=1}^{N} \mathbb{E}(X_i) = 0 Var(X_i) = \frac{2}{3} 
P\Bigl(\sum_{i=1}^{N}X_i>\sqrt{N}\Bigr) = P\Bigl(\frac{\sum_{i=1}^{N}X_i}{\sqrt{N}\sqrt{\frac{2}{3}}}>\frac{\sqrt{N}}{\sqrt{N}\sqrt{\frac{2}{3}}}\Bigr) = 1- P\Bigl(\frac{\sum_{i=1}^{N}X_i}{\sqrt{N}\sqrt{\frac{2}{3}}}\leq\sqrt{\frac{3}{2}}\Bigr) \longrightarrow 1-\phi\Bigl(\sqrt{\frac{3}{2}}\Bigr) \approx 0.1112
 \phi(t) = \frac{1}{\sqrt{2\pi}}\int_{\infty}^{t}e^{-\frac{s^2}{2}}ds Z\sim\mathcal{N}(0,1) 2\cdot 0.1112 = 0.2224 k P\Bigl(\Bigl|\sum_{i=1}^{N}X_{ij}\Bigr|>\sqrt{N}\Bigr) = p = 0.2224 p j 
P\Biggl(\bigcup_{j=1}^{k}\Bigl\{\Bigl|\sum_{i=1}^{N}X_{ij}\Bigr|>\sqrt{N}\Bigr\}\Biggr) = \sum_{j=1}^{k}p - \sum_{j<m}^{k}p^2 + \dots + (-1)^{k+1}p^k = \sum_{h=1}^{k}(-1)^{h+1}\binom{k}{h}p^h = 1-(1-p)^{k}
 \Bigl|\sum_{i=1}^{N}X_{i1}\Bigr|,\dots,\Bigl|\sum_{i=1}^{N}X_{ik}\Bigr| 
P\Bigl(\Bigl\{\Bigl|\sum_{i=1}^{N}X_{ij}\Bigr|>\sqrt{N}\Bigr\}\cap\Bigl\{\Bigl|\sum_{i=1}^{N}X_{im}\Bigr|>\sqrt{N}\Bigr\}\Bigr) = P\Bigl(\Bigl\{\Bigl|\sum_{i=1}^{N}X_{ij}\Bigr|>\sqrt{N}\Bigr\}\Bigr)P\Bigl(\Bigl\{\Bigl|\sum_{i=1}^{N}X_{im}\Bigr|>\sqrt{N}\Bigr\}\Bigr) = p^{2}.
 k\to\infty 
P\Biggl(\bigcup_{j=1}^{\infty}\Bigl\{\Bigl|\sum_{i=1}^{N}X_{ij}\Bigr|>\sqrt{N}\Bigr\}\Biggr) = \lim_{k\to\infty} 1-(1-p)^{k} = 1
 p=0.2224\in(0,1) \implies (1-p)\in(0,1) k 2\sqrt{N} N\to\infty k\to\infty x_i x_j k=1 \implies P = 0.2224 k=2 \implies P = 0.3978 k=3 \implies P = 0.5327 \vdots","['probability', 'random-walk', 'probability-limit-theorems']"
11,Roll a die until the sum of the rolls is prime. Repeat this again and again. Show that the expected value of the number of rolls is $>2.428$ [closed],Roll a die until the sum of the rolls is prime. Repeat this again and again. Show that the expected value of the number of rolls is  [closed],>2.428,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question So, let's denote the number of throws with $X$ . For example, if the first roll is a $2$ ,then $X=1$ . If the first roll is a $1$ and the second roll is a $6$ , then $X=2$ . If the first one is $4$ ,the second is $2$ and the third $5$ , then $X=3$ , etc. Show that the expected value of $X$ is at least $2.428$ . // Edit: I have first tried to prove it by writing a program that simulates all possible outcome, but even after numerous changes and optimization, it does not run under reasonable time (I am not even sure it would reach $2.428$ under $20$ minutes, and the ideal runtime would be under a minute). I have no idea what other way I can approach the problem, I have tried to represent the possibilities as a tree, but I haven't figured out any pattern in it yet.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question So, let's denote the number of throws with . For example, if the first roll is a ,then . If the first roll is a and the second roll is a , then . If the first one is ,the second is and the third , then , etc. Show that the expected value of is at least . // Edit: I have first tried to prove it by writing a program that simulates all possible outcome, but even after numerous changes and optimization, it does not run under reasonable time (I am not even sure it would reach under minutes, and the ideal runtime would be under a minute). I have no idea what other way I can approach the problem, I have tried to represent the possibilities as a tree, but I haven't figured out any pattern in it yet.",X 2 X=1 1 6 X=2 4 2 5 X=3 X 2.428 2.428 20,"['probability', 'statistics', 'prime-numbers']"
12,What is the probability that the circumscribed circle $B$ of three randomly selected points within a circle $A$ will be included in circle $A$?,What is the probability that the circumscribed circle  of three randomly selected points within a circle  will be included in circle ?,B A A,"If three points are randomly selected within a circle $A$ (area uniformly), what is the likelihood that the circumscribed circle $B$ of these points will be completely contained within circle $A$ ? Using Monto Carlo Method, implement it in Mathematica, the result is roughly $0.40$ . But, Is there an analytic solution? how to get it? n = 10^6;  Table[circle = Circumsphere[RandomPoint[Disk[], 3]];      (Norm[circle[[1]]] + circle[[2]]) <= 1, n]       //Count[#, True] &       //(#/n) &       // N // AbsoluteTiming {71.4554, 0.400337}   {74.6617, 0.40019} Update I found a post on another website , showing the result is: $$ P_n=\frac{2^{n^2} \pi^{\frac{n}{2}-1} n !\left(\frac{n \Gamma\left(\frac{n}{2}\right)}{\Gamma\left(\frac{n+1}{2}\right)}\right)^n \Gamma\left(\frac{n^2+1}{2}\right)^2}{\left(n^2+n\right) !} $$ where $n$ means $n$ -dimension unit-sphere. for $n=2$ , it produces $\frac{2}{5}$ But I can't understand the derivation steps. Any help would be appreciated.","If three points are randomly selected within a circle (area uniformly), what is the likelihood that the circumscribed circle of these points will be completely contained within circle ? Using Monto Carlo Method, implement it in Mathematica, the result is roughly . But, Is there an analytic solution? how to get it? n = 10^6;  Table[circle = Circumsphere[RandomPoint[Disk[], 3]];      (Norm[circle[[1]]] + circle[[2]]) <= 1, n]       //Count[#, True] &       //(#/n) &       // N // AbsoluteTiming {71.4554, 0.400337}   {74.6617, 0.40019} Update I found a post on another website , showing the result is: where means -dimension unit-sphere. for , it produces But I can't understand the derivation steps. Any help would be appreciated.","A B A 0.40 
P_n=\frac{2^{n^2} \pi^{\frac{n}{2}-1} n !\left(\frac{n \Gamma\left(\frac{n}{2}\right)}{\Gamma\left(\frac{n+1}{2}\right)}\right)^n \Gamma\left(\frac{n^2+1}{2}\right)^2}{\left(n^2+n\right) !}
 n n n=2 \frac{2}{5}","['probability', 'geometry', 'probability-theory']"
13,Intuition for Markov process,Intuition for Markov process,,"Why is the process $Q(t)$ defined below a Markov process ? $$Q(t)=Q(0)+A(t)- S\left(\int_0^t Q(s)\,\mathrm ds\right)$$ where $A$ and $S$ are unit rate Poisson process. Since the integral depends on the past, shouldn't this be non-Markov ? What is the intuition for this ?","Why is the process defined below a Markov process ? where and are unit rate Poisson process. Since the integral depends on the past, shouldn't this be non-Markov ? What is the intuition for this ?","Q(t) Q(t)=Q(0)+A(t)- S\left(\int_0^t Q(s)\,\mathrm ds\right) A S","['probability', 'probability-theory', 'stochastic-processes', 'markov-process']"
14,"Problem 12.7 in ""A Probabilistic Theory of Pattern Recognition""","Problem 12.7 in ""A Probabilistic Theory of Pattern Recognition""",,"I'm trying to understand the Problem 12.7 in the book A Probabilistic Theory of Pattern Recognition . This is part of the proof of Theorem 12.7. For the sake of clarity, I'll introduce a few notations before presenting the problem: let $Z_1, Z_2, \ldots, Z_n, Z_1', Z_2', \ldots, Z_n'$ be iid random variables; for any measurable set $A$ in some class of sets $\mathcal{A}$ , \begin{aligned} \nu(A) & = \mathbf{P}\left\{Z \in A\right\}, \\ \nu_n(A) & = \frac{1}{n} \sum_{i = 1}^{n} I_{\left\{Z_i \in A\right\}}, \\ \nu_n'(A) & = \frac{1}{n} \sum_{i = 1}^{n} I_{\left\{Z_i' \in A\right\}}. \end{aligned} The problem, as in the book, is presented below. PROBLEM 12.7. Prove that $$ \mathbf{P}\left\{\sup _{A: v_n(A)=0}\left|v_n(A)-v(A)\right|>\epsilon\right\} \leq 2 \mathbf{P}\left\{\sup _{A: v_n(A)=0}\left|v_n(A)-v_n^{\prime}(A)\right|>\frac{\epsilon}{2}\right\} $$ holds if $n \epsilon>2$ . This inequality is needed to complete the proof of Theorem 12.7. HINT: Proceed as in the proof of Theorem 12.5. Introduce $A^*$ with $v_n\left(A^*\right)=0$ and justify the validity of the steps of the following chain of inequalities: $$ \begin{aligned} \mathbf{P} & \left\{\sup _{A: v_n(A)=0}\left|v_n(A)-v_n^{\prime}(A)\right|>\epsilon / 2\right\} \\ & \geq \mathbf{E}\left\{I_{\left\{v\left(A^*\right)>\epsilon\right\}} \mathbf{P}\left\{v_n^{\prime}\left(A^*\right) \geq \frac{\epsilon}{2} \mid Z_1, \ldots, Z_n\right\}\right\} \\ & \geq \mathbf{P}\left\{B(n, \epsilon)>\frac{n \epsilon}{2}\right\} \mathbf{P}\left\{\left|v_n\left(A^*\right)-v\left(A^*\right)\right|>\epsilon\right\}, \end{aligned} $$ where $B(n, \epsilon)$ is a binomial random variable with parameters $n$ and $\epsilon$ . Finish the proof by showing that the probability on the right-hand side is greater than or equal to $1 / 2$ if $n \epsilon>2$ . (Under the slightly more restrictive condition $n \epsilon>8$ , this follows from Chebyshev's inequality.) Based on the hint above, and the proof of Theorem 12.4 (which is related to Theorem 12.5), I started my solution: Let $A^* \in \mathcal{A}$ be a set for which $$\nu\left(A^*\right)>\varepsilon \text{ and } \nu_n\left(A^*\right)=0.$$ If such a set does not exist, let $A^*$ be a fixed set that guarantees $\nu_n\left(A^*\right)=0$ . Then, \begin{aligned} \mathbf{P}&\left(\sup _{A: \nu_n(A)= 0} \left|\nu_n(A)-\nu_n^{\prime}(A)\right|>\varepsilon / 2\right) \\ & \geq \mathbf{P}\left(\left|\nu_n\left(A^*\right)-\nu_n^{\prime}\left(A^*\right)\right|>\varepsilon / 2\right)=\mathbf{P}\left(\nu_n^{\prime}(A^*) >\varepsilon / 2\right) \\ & \geq \mathbf{P}\left(\nu\left(A^*\right)>\varepsilon,\nu_n^{\prime}\left(A^*\right)>\varepsilon / 2\right) \\ & =E\left\{I_{\left\{v\left(A^{*}\right)>\varepsilon\right\}} \mathbf{P}\left[v_n^{\prime}\left(A^*\right)>\varepsilon/2 \mid Z_1, \ldots, Z_n\right]\right\} \\ & \stackrel{(* *)}{=} \mathbf{P}\left[\nu_n^{\prime}\left(A^*\right)>\varepsilon / 2\right] E\left[I_{\left\{\nu\left(A^*\right)>\varepsilon\right\}}\right] \\ & =\mathbf{P}\left(\sum_{i=1}^n I_{\left\{Z_i^{\prime} \in A^*\right\}}> n \varepsilon / 2\right) \mathbf{P}\left(\nu\left(A^*\right)>\varepsilon\right) \\ &\equiv \mathbf{P}\left(B(n, \nu(A^*))> n \varepsilon / 2\right) \mathbf{P}\left(\left|\nu_n(A^*) - \nu\left(A^*\right)\right|>\varepsilon\right), \end{aligned} where in $(* *)$ I used the fact that $Z_i$ s and $Z_i'$ s are independent. Am I doing something wrong? More specifically, my questions are: I don't know how they came to a $B(n, \epsilon)$ in the last inequality of the HINT. Looking at other proofs in the book (e.g. Theorem 12.4), I think it is necessary to show that, if $n\varepsilon > 2$ , then $\mathbf{P}\left\{B(n, \epsilon)>\frac{n \epsilon}{2}\right\} \geq \frac{1}{2}$ . The result makes sense, but don't know to show this. Furthermore, I don't know how to use Chebyshev's inequality, since it provides upper bounds for this kind of probability and we need a lower bound. Finally, based on my solution, I think that I can do $\mathbf{P}\left(B(n, \nu(A^*))> n \varepsilon / 2\right) \geq \mathbf{P}\left(B(n, \varepsilon)> n \varepsilon / 2\right)$ , but I don't know how to show this either.","I'm trying to understand the Problem 12.7 in the book A Probabilistic Theory of Pattern Recognition . This is part of the proof of Theorem 12.7. For the sake of clarity, I'll introduce a few notations before presenting the problem: let be iid random variables; for any measurable set in some class of sets , The problem, as in the book, is presented below. PROBLEM 12.7. Prove that holds if . This inequality is needed to complete the proof of Theorem 12.7. HINT: Proceed as in the proof of Theorem 12.5. Introduce with and justify the validity of the steps of the following chain of inequalities: where is a binomial random variable with parameters and . Finish the proof by showing that the probability on the right-hand side is greater than or equal to if . (Under the slightly more restrictive condition , this follows from Chebyshev's inequality.) Based on the hint above, and the proof of Theorem 12.4 (which is related to Theorem 12.5), I started my solution: Let be a set for which If such a set does not exist, let be a fixed set that guarantees . Then, where in I used the fact that s and s are independent. Am I doing something wrong? More specifically, my questions are: I don't know how they came to a in the last inequality of the HINT. Looking at other proofs in the book (e.g. Theorem 12.4), I think it is necessary to show that, if , then . The result makes sense, but don't know to show this. Furthermore, I don't know how to use Chebyshev's inequality, since it provides upper bounds for this kind of probability and we need a lower bound. Finally, based on my solution, I think that I can do , but I don't know how to show this either.","Z_1, Z_2, \ldots, Z_n, Z_1', Z_2', \ldots, Z_n' A \mathcal{A} \begin{aligned}
\nu(A) & = \mathbf{P}\left\{Z \in A\right\}, \\
\nu_n(A) & = \frac{1}{n} \sum_{i = 1}^{n} I_{\left\{Z_i \in A\right\}}, \\
\nu_n'(A) & = \frac{1}{n} \sum_{i = 1}^{n} I_{\left\{Z_i' \in A\right\}}.
\end{aligned} 
\mathbf{P}\left\{\sup _{A: v_n(A)=0}\left|v_n(A)-v(A)\right|>\epsilon\right\} \leq 2 \mathbf{P}\left\{\sup _{A: v_n(A)=0}\left|v_n(A)-v_n^{\prime}(A)\right|>\frac{\epsilon}{2}\right\}
 n \epsilon>2 A^* v_n\left(A^*\right)=0 
\begin{aligned}
\mathbf{P} & \left\{\sup _{A: v_n(A)=0}\left|v_n(A)-v_n^{\prime}(A)\right|>\epsilon / 2\right\} \\
& \geq \mathbf{E}\left\{I_{\left\{v\left(A^*\right)>\epsilon\right\}} \mathbf{P}\left\{v_n^{\prime}\left(A^*\right) \geq \frac{\epsilon}{2} \mid Z_1, \ldots, Z_n\right\}\right\} \\
& \geq \mathbf{P}\left\{B(n, \epsilon)>\frac{n \epsilon}{2}\right\} \mathbf{P}\left\{\left|v_n\left(A^*\right)-v\left(A^*\right)\right|>\epsilon\right\},
\end{aligned}
 B(n, \epsilon) n \epsilon 1 / 2 n \epsilon>2 n \epsilon>8 A^* \in \mathcal{A} \nu\left(A^*\right)>\varepsilon \text{ and } \nu_n\left(A^*\right)=0. A^* \nu_n\left(A^*\right)=0 \begin{aligned}
\mathbf{P}&\left(\sup _{A: \nu_n(A)= 0} \left|\nu_n(A)-\nu_n^{\prime}(A)\right|>\varepsilon / 2\right) \\
& \geq \mathbf{P}\left(\left|\nu_n\left(A^*\right)-\nu_n^{\prime}\left(A^*\right)\right|>\varepsilon / 2\right)=\mathbf{P}\left(\nu_n^{\prime}(A^*) >\varepsilon / 2\right) \\
& \geq \mathbf{P}\left(\nu\left(A^*\right)>\varepsilon,\nu_n^{\prime}\left(A^*\right)>\varepsilon / 2\right) \\
& =E\left\{I_{\left\{v\left(A^{*}\right)>\varepsilon\right\}} \mathbf{P}\left[v_n^{\prime}\left(A^*\right)>\varepsilon/2 \mid Z_1, \ldots, Z_n\right]\right\} \\
& \stackrel{(* *)}{=} \mathbf{P}\left[\nu_n^{\prime}\left(A^*\right)>\varepsilon / 2\right] E\left[I_{\left\{\nu\left(A^*\right)>\varepsilon\right\}}\right] \\
& =\mathbf{P}\left(\sum_{i=1}^n I_{\left\{Z_i^{\prime} \in A^*\right\}}> n \varepsilon / 2\right) \mathbf{P}\left(\nu\left(A^*\right)>\varepsilon\right) \\
&\equiv \mathbf{P}\left(B(n, \nu(A^*))> n \varepsilon / 2\right) \mathbf{P}\left(\left|\nu_n(A^*) - \nu\left(A^*\right)\right|>\varepsilon\right),
\end{aligned} (* *) Z_i Z_i' B(n, \epsilon) n\varepsilon > 2 \mathbf{P}\left\{B(n, \epsilon)>\frac{n \epsilon}{2}\right\} \geq \frac{1}{2} \mathbf{P}\left(B(n, \nu(A^*))> n \varepsilon / 2\right) \geq \mathbf{P}\left(B(n, \varepsilon)> n \varepsilon / 2\right)","['probability', 'statistics', 'inequality']"
15,Entropy and probabilistic Algorithms,Entropy and probabilistic Algorithms,,"Recall entropy, from basic information theory: The entropy of a probability distribution $D$ on a finite set $X$ is $$H(D)=\sum_{x\in X}{p(x) \cdot \log_2{\!(1/p(x))}}$$ I was able to prove that the maximum entropy of any distribution over $[n]$ is $\log{\!(n)}$ and it is achieved by the uniform distribution.  Also, I was able to prove that when we group 2 parameters from $D$ —​let's say, $D=\{p_1,\dots,p_n\}$ and $D'=\{p_1,\dots,p_{n-2},p_{n-1} + p_n\}$ —​then $H(D')\leq H(D)$ . I need to show using the above that if I have a biased coin with probabilities $p$ and $1 − p$ of each outcome, that in order to obtain a length- $k$ sequence of unbiased coin-flips, then you need on average to use $k/H(p, 1 − p)$ tosses of your biased coin. My calculations are as follows:- for the biased coin, in order to make an unbiased coin from it then I flip the coin twice, if it lands on different faces then I take the first face as the answer , otherwise, I have the same result in the 2 tosses then I toss again. the expectation of this is $1/(2p(1-p))$ so in order to get $k$ tosses the expected value would be $k/(2p(1-p))$ and the entropy $$H(p, 1 − p) = p\cdot\log((1-p)/p) + \log(1/(1-p))$$ I can't make a connection between the entropy and my answer.","Recall entropy, from basic information theory: The entropy of a probability distribution on a finite set is I was able to prove that the maximum entropy of any distribution over is and it is achieved by the uniform distribution.  Also, I was able to prove that when we group 2 parameters from —​let's say, and —​then . I need to show using the above that if I have a biased coin with probabilities and of each outcome, that in order to obtain a length- sequence of unbiased coin-flips, then you need on average to use tosses of your biased coin. My calculations are as follows:- for the biased coin, in order to make an unbiased coin from it then I flip the coin twice, if it lands on different faces then I take the first face as the answer , otherwise, I have the same result in the 2 tosses then I toss again. the expectation of this is so in order to get tosses the expected value would be and the entropy I can't make a connection between the entropy and my answer.","D X H(D)=\sum_{x\in X}{p(x) \cdot \log_2{\!(1/p(x))}} [n] \log{\!(n)} D D=\{p_1,\dots,p_n\} D'=\{p_1,\dots,p_{n-2},p_{n-1} + p_n\} H(D')\leq H(D) p 1 − p k k/H(p, 1 − p) 1/(2p(1-p)) k k/(2p(1-p)) H(p, 1 − p) = p\cdot\log((1-p)/p) + \log(1/(1-p))","['probability', 'information-theory', 'entropy']"
16,Tails of the normal distribution,Tails of the normal distribution,,"I'm currently reading Roman Vershynin's High-Dimensional Probability. For Proposition $2.1.2$ , I wonder how the lower bound is obtained. I understand that the lower bound is correct, but I don't know where the term $-3x^{-4}$ comes from. Thanks.","I'm currently reading Roman Vershynin's High-Dimensional Probability. For Proposition , I wonder how the lower bound is obtained. I understand that the lower bound is correct, but I don't know where the term comes from. Thanks.",2.1.2 -3x^{-4},['probability']
17,Bhatia—Davis inequality: first recorded occurrence?,Bhatia—Davis inequality: first recorded occurrence?,,"The Bhatia–Davis inequality states that, for any random variable $X$ such that $m \leq X \leq M$ a.s., $$ \operatorname{Var}[X] \leq (M-\mathbb{E}[X])(\mathbb{E}[X]-m) $$ This is a strenghtening of another inequality , attributed to Popovicius (1935). However, Bhatia and Davis only published their paper in 2000. Was there no earlier recorded occurrence of this inequality? It seems a little strange for it to have waited 65 years...","The Bhatia–Davis inequality states that, for any random variable such that a.s., This is a strenghtening of another inequality , attributed to Popovicius (1935). However, Bhatia and Davis only published their paper in 2000. Was there no earlier recorded occurrence of this inequality? It seems a little strange for it to have waited 65 years...","X m \leq X \leq M 
\operatorname{Var}[X] \leq (M-\mathbb{E}[X])(\mathbb{E}[X]-m)
","['probability', 'inequality', 'math-history']"
18,Visualising independence of events,Visualising independence of events,,"Let's say we have a fair five-sided die. The sides of the die are numbered from 1 to 5. Each die roll is independent and all faces are equally likely. We roll twice. Event A = the total of two rolls is 10 Event B = at least one roll resulted in 5 I get that these are clearly dependent. The $P(B\mid A) = 1$ because if you get two rolls = 10, they had to be 5 and 5, so clearly B occurs. But how would I visualize this on a Venn diagram? Like I'm not sure how the P(B|A) = mA intersect B/P(A) and it equals 1. To be clear, I know why the answer is so intuitively/logically but not sure how it would look visually (like Venn diagram) and mathematically.  How do we get a 1?","Let's say we have a fair five-sided die. The sides of the die are numbered from 1 to 5. Each die roll is independent and all faces are equally likely. We roll twice. Event A = the total of two rolls is 10 Event B = at least one roll resulted in 5 I get that these are clearly dependent. The because if you get two rolls = 10, they had to be 5 and 5, so clearly B occurs. But how would I visualize this on a Venn diagram? Like I'm not sure how the P(B|A) = mA intersect B/P(A) and it equals 1. To be clear, I know why the answer is so intuitively/logically but not sure how it would look visually (like Venn diagram) and mathematically.  How do we get a 1?",P(B\mid A) = 1,"['probability', 'statistics']"
19,Finding a rare biased coin from an infinite set,Finding a rare biased coin from an infinite set,,"I'm trying to develop an algorithm for finding biased coins. The basic problem formulation is this: There are an infinite number of coins Some proportion $t$ of the coins is biased (this number is known) All biased coins have the same probability $p_b$ of coming up heads (this number is also known) All other coins are fair Biased coins are otherwise indistinguishable from fair ones The task is to find one biased coin, with some confidence, using the fewest number of coin flips. I know the basic solution to a related problem, i.e. determining whether a single coin is biased. Following the formulation in https://en.wikipedia.org/wiki/Checking_whether_a_coin_is_fair , I can set my desired maximum error $E$ to be equal to $|p_b - 0.5|$ and use the equation $n = \frac{Z^2}{4E^2}$ to get the number of coin tosses $n$ required to determine whether the coin is indeed fair using a given $Z$ value. However, I'm curious how the method might change given my formulation, where there are multiple coins and a known proportion are biased. A brute force algorithm, I suppose, would be to select a coin, flip it $n$ times, select another coin, flip it $n$ times, etc. until a biased one is found. But this feels sub-optimal. Is it possible, for instance, to abandon a coin before $n$ flips is reached based on some criteria, i.e. using the evidence collected so far to judge whether it is worthwhile to keep flipping that coin or move on to another? It seems like the value of $t$ , particularly if it is low, should be a useful prior that I can leverage. I'm also concerned that if I test multiple coins, I am at risk of inadvertently finding significance where there is none.","I'm trying to develop an algorithm for finding biased coins. The basic problem formulation is this: There are an infinite number of coins Some proportion of the coins is biased (this number is known) All biased coins have the same probability of coming up heads (this number is also known) All other coins are fair Biased coins are otherwise indistinguishable from fair ones The task is to find one biased coin, with some confidence, using the fewest number of coin flips. I know the basic solution to a related problem, i.e. determining whether a single coin is biased. Following the formulation in https://en.wikipedia.org/wiki/Checking_whether_a_coin_is_fair , I can set my desired maximum error to be equal to and use the equation to get the number of coin tosses required to determine whether the coin is indeed fair using a given value. However, I'm curious how the method might change given my formulation, where there are multiple coins and a known proportion are biased. A brute force algorithm, I suppose, would be to select a coin, flip it times, select another coin, flip it times, etc. until a biased one is found. But this feels sub-optimal. Is it possible, for instance, to abandon a coin before flips is reached based on some criteria, i.e. using the evidence collected so far to judge whether it is worthwhile to keep flipping that coin or move on to another? It seems like the value of , particularly if it is low, should be a useful prior that I can leverage. I'm also concerned that if I test multiple coins, I am at risk of inadvertently finding significance where there is none.",t p_b E |p_b - 0.5| n = \frac{Z^2}{4E^2} n Z n n n t,['probability']
20,Variance of the minimum of two r.v.'s,Variance of the minimum of two r.v.'s,,"For two nonnegative independent r.v.'s, $X,Y$ , with the same distribution and finite second moment, I'm trying to show that $Var[\min(X,Y)]\leqslant Var[X]$ . Attempt 1. For the continuous case, I've written the first and second moments of $\min(X,Y)$ in terms of $X$ but have no idea how to proceed with them. Specifically, with $Z=\min(X,Y)$ , I have $\mathbb{E}\left[Z\right]=2\mathbb{E}[X]+\int_{0}^{\infty}F^2_{X}(z)dz$ , $\mathbb{E}\left[Z^2\right]=2\mathbb{E}\left[X^{2}\right]+2\int_{0}^{\infty}zF_{X}^{2}(z)dz$ , (where $F_{X}$ is the cdf of $X$ ) but don't know what to do with the integrals in the RHS's of the above expressions. Attempt 2. Noting that $\min(X,Y)=\frac{1}{2}\left(X+Y-|X-Y|\right)$ , I can write \begin{equation} \label{eq1} \begin{split} Var\left[\min(X,Y)\right] & = Var[X]-\frac{1}{4}\left(\mathbb{E}\left[\left|X-Y\right|^2\right]+4\text{Cov}\left(X,|X-Y|\right)\right),  \\ \end{split}  \end{equation} but am struggling to show that the RHS's second term (1/4(...)) is less than or equal to zero. Any suggestions about how I might proceed or confirmation these are dead-ends would be appreciated.","For two nonnegative independent r.v.'s, , with the same distribution and finite second moment, I'm trying to show that . Attempt 1. For the continuous case, I've written the first and second moments of in terms of but have no idea how to proceed with them. Specifically, with , I have , , (where is the cdf of ) but don't know what to do with the integrals in the RHS's of the above expressions. Attempt 2. Noting that , I can write but am struggling to show that the RHS's second term (1/4(...)) is less than or equal to zero. Any suggestions about how I might proceed or confirmation these are dead-ends would be appreciated.","X,Y Var[\min(X,Y)]\leqslant Var[X] \min(X,Y) X Z=\min(X,Y) \mathbb{E}\left[Z\right]=2\mathbb{E}[X]+\int_{0}^{\infty}F^2_{X}(z)dz \mathbb{E}\left[Z^2\right]=2\mathbb{E}\left[X^{2}\right]+2\int_{0}^{\infty}zF_{X}^{2}(z)dz F_{X} X \min(X,Y)=\frac{1}{2}\left(X+Y-|X-Y|\right) \begin{equation} \label{eq1}
\begin{split}
Var\left[\min(X,Y)\right] & = Var[X]-\frac{1}{4}\left(\mathbb{E}\left[\left|X-Y\right|^2\right]+4\text{Cov}\left(X,|X-Y|\right)\right),  \\
\end{split} 
\end{equation}","['probability', 'variance', 'order-statistics', 'cumulative-distribution-functions']"
21,Find the probability that $[x+y+z]=[x]+[y]+[z]+2$,Find the probability that,[x+y+z]=[x]+[y]+[z]+2,"Find the probability that the equation $[x+y+z]=[x]+[y]+[z]+2$ is true, where $x,y,z \in R$ . [.] Represents the greatest integer function. I got two different answers by two different methods. 1st method: $x=[x]+\{x \}$ etc in LHS, where {} is the fractional part function. So $[\{x \}+\{y \}+ \{z \}]=2$ So $\{x \}+\{y \}+ \{z \}$ is between $[2,3)$ . All $\{x \},\{y \},\{z \}$ are between $[0,1)$ and are uniformly distributed in this interval. So if we consider the ""expectation"" instead of actual probability. The expectation that $\{x \},\{y \},\{z \}$ is between $[2,3)$ . = Expectation that $3\{x \} $ is between $[2,3)$ . = Expectation that ${x}$ is between $[2/3,1)$ $= (1-\frac{2}{3})/1 = \frac{1}{3}$ (Due to uniform distribution of {x}). Can we call this the final required probability? Method 2: consider a unit cube with vertices $(0,0,0),(1,0,0),(0,1,0),(0,0,1),(1,1,0),(1,0,1),(0,1,1),(1,1,1)$ and the plane $x+y+z=2$ . The required probability (of $\{x \}+\{y \}+ \{z \}$ is between $[2,3)$ ). is the volume of the cube cut out of the plane(not including the origin) /volume of cube = volume of tetrahedron with vertices $(1,1,1),(1,1,0),(1,0,1),(0,1,1)$ /1 = $\frac{1}{6}$ . Which method is correct (if at all any) and is there any other way to solve this question?","Find the probability that the equation is true, where . [.] Represents the greatest integer function. I got two different answers by two different methods. 1st method: etc in LHS, where {} is the fractional part function. So So is between . All are between and are uniformly distributed in this interval. So if we consider the ""expectation"" instead of actual probability. The expectation that is between . = Expectation that is between . = Expectation that is between (Due to uniform distribution of {x}). Can we call this the final required probability? Method 2: consider a unit cube with vertices and the plane . The required probability (of is between ). is the volume of the cube cut out of the plane(not including the origin) /volume of cube = volume of tetrahedron with vertices /1 = . Which method is correct (if at all any) and is there any other way to solve this question?","[x+y+z]=[x]+[y]+[z]+2 x,y,z \in R x=[x]+\{x \} [\{x \}+\{y \}+ \{z \}]=2 \{x \}+\{y \}+ \{z \} [2,3) \{x \},\{y \},\{z \} [0,1) \{x \},\{y \},\{z \} [2,3) 3\{x \}  [2,3) {x} [2/3,1) = (1-\frac{2}{3})/1 = \frac{1}{3} (0,0,0),(1,0,0),(0,1,0),(0,0,1),(1,1,0),(1,0,1),(0,1,1),(1,1,1) x+y+z=2 \{x \}+\{y \}+ \{z \} [2,3) (1,1,1),(1,1,0),(1,0,1),(0,1,1) \frac{1}{6}","['probability', 'solution-verification', 'ceiling-and-floor-functions']"
22,Dice game - deciding whether to re-roll or not,Dice game - deciding whether to re-roll or not,,"I am working on the following problem from a book: A casino has a dice game. You can roll as many times as you want. For each roll you get paid $M$ dollars where $M$ is the number of dots on the roll as long as you do not roll a 6. The payment for each roll is additive. However, if you roll a 6, the game terminates and you lose your accumulated profit thus far. How much are you willing to spend on this game? I am looking at the solution provided by the book, and I am confused. The solution is posted below. The part I am confused about is examining the threshhold for $n$ at which $$ 5/6 \cdot n + 2.5 > n $$ Equality in the above expression holds when $n = 15$ . I understand how they determined this solution, but it is not clear to me why this is the most optimal threshhold because the equation $5/6 * n + 2.5$ is derived from assuming you can only roll 1 more time. So if we have $n = \$16$ , the solution is telling us that we shouldn't re-roll because the expectation of the profit of an additional roll is less than the current profit. But this assumes that we can only roll 1 additional time. Shouldn't be consider the cases of rolling more than 1 time if we have $n = \$16$ already?","I am working on the following problem from a book: A casino has a dice game. You can roll as many times as you want. For each roll you get paid dollars where is the number of dots on the roll as long as you do not roll a 6. The payment for each roll is additive. However, if you roll a 6, the game terminates and you lose your accumulated profit thus far. How much are you willing to spend on this game? I am looking at the solution provided by the book, and I am confused. The solution is posted below. The part I am confused about is examining the threshhold for at which Equality in the above expression holds when . I understand how they determined this solution, but it is not clear to me why this is the most optimal threshhold because the equation is derived from assuming you can only roll 1 more time. So if we have , the solution is telling us that we shouldn't re-roll because the expectation of the profit of an additional roll is less than the current profit. But this assumes that we can only roll 1 additional time. Shouldn't be consider the cases of rolling more than 1 time if we have already?","M M n 
5/6 \cdot n + 2.5 > n
 n = 15 5/6 * n + 2.5 n = \16 n = \16","['probability', 'expected-value', 'markov-process', 'dice', 'gambling']"
23,Counting Questions for precalculus,Counting Questions for precalculus,,"1) In a coin collection, each coin has some combination of the following characteristics: One of five different colors (white, black, silver, gold, copper) One of three different shapes (circle, square, hexagon) One of three letters imprinted on it (A, B, C) There is exactly one coin with each combination of characteristics. There is one black circle coin with an A on it, one gold square coin with a C on it, and so on. a. How many coins are in this collection? I think this is just the Fundamental Counting principle so it is just $5*3*3=45$ b. How many silver coins are in the collection? I'm confused on this one.. There are $3*2=6$ total outcomes of shapes and letters. Do I just multiply that by $5C1$ c. How many coins have the letter A on them? Again, I'm confused, is it: $3C1 * 5*3$ ? 2) Jeff and Caitlin are playing a game. Jeff chooses 4 balls from a bucket of 18 balls numbered 1 to 18. To win, Caitlin must correctly guess the numbers on the four balls. a. How many ways can Jeff choose four balls? $18C4$ b. What is the probability that Caitlin correctly guesses the four numbers? I'm confused about this too. Is it $1/18*1/18*1/18*1/18$","1) In a coin collection, each coin has some combination of the following characteristics: One of five different colors (white, black, silver, gold, copper) One of three different shapes (circle, square, hexagon) One of three letters imprinted on it (A, B, C) There is exactly one coin with each combination of characteristics. There is one black circle coin with an A on it, one gold square coin with a C on it, and so on. a. How many coins are in this collection? I think this is just the Fundamental Counting principle so it is just b. How many silver coins are in the collection? I'm confused on this one.. There are total outcomes of shapes and letters. Do I just multiply that by c. How many coins have the letter A on them? Again, I'm confused, is it: ? 2) Jeff and Caitlin are playing a game. Jeff chooses 4 balls from a bucket of 18 balls numbered 1 to 18. To win, Caitlin must correctly guess the numbers on the four balls. a. How many ways can Jeff choose four balls? b. What is the probability that Caitlin correctly guesses the four numbers? I'm confused about this too. Is it",5*3*3=45 3*2=6 5C1 3C1 * 5*3 18C4 1/18*1/18*1/18*1/18,"['probability', 'combinatorics', 'permutations']"
24,Election between $2$ candidates ends in a tie: probability one candidate leads until the penultimate vote,Election between  candidates ends in a tie: probability one candidate leads until the penultimate vote,2,"Assume there are two candidate $C_1$ and $C_2$ . At the end of the election both candidates receive the same amount of votes. What is the probability $P$ that candidate $C_1$ leads during the whole election process until the penultimate vote? (The last vote must always be in favor of candidate $C_2$ ) This question was presented in our lecture in the context of the ballot-theorem . So one should think of paths which start at $(0, 0)$ along the $x$ -axis and end at some point $(n,s)$ , where $n,s \in \mathbb{Z}$ . My approach: My sample space $\Omega$ includes all possible paths along the $x$ -axis. If the path is above the $x$ -axis then candidate $C_1$ has more votes and if the path is below then $C_2$ has more votes . If the paths touches the $x$ -axis then both candidates have the same amount of votes. Hence, $|\Omega|={2p \choose p}$ , where $p \in \mathbb{N}$ is the number of votes of each candidate. Firstly, I count all paths which start at $(1,1)$ and end at $(2p,0)$ . These are ${2p-1 \choose p-1}$ many. Now I subtract all paths that touch the $x$ -axis, these are ${2p-2 \choose p-2}$ many. So in total I count ${2p-1 \choose p-1}-{2p-2 \choose p-2}$ paths which do not touch the $x$ -axis. One can interpret all these paths as desired outcomes, i.e. where candidate $C_1$ leads until the penultimate vote. As all paths are equally probable I get the solution just by dividing by $|\Omega|={2p \choose p}$ . Hence, $P = \frac{{2p-1 \choose p-1}-{2p-2 \choose p-2}}{{2p \choose p}}$ . I am not sure if this is correct. May be someone can check it or comment on it.","Assume there are two candidate and . At the end of the election both candidates receive the same amount of votes. What is the probability that candidate leads during the whole election process until the penultimate vote? (The last vote must always be in favor of candidate ) This question was presented in our lecture in the context of the ballot-theorem . So one should think of paths which start at along the -axis and end at some point , where . My approach: My sample space includes all possible paths along the -axis. If the path is above the -axis then candidate has more votes and if the path is below then has more votes . If the paths touches the -axis then both candidates have the same amount of votes. Hence, , where is the number of votes of each candidate. Firstly, I count all paths which start at and end at . These are many. Now I subtract all paths that touch the -axis, these are many. So in total I count paths which do not touch the -axis. One can interpret all these paths as desired outcomes, i.e. where candidate leads until the penultimate vote. As all paths are equally probable I get the solution just by dividing by . Hence, . I am not sure if this is correct. May be someone can check it or comment on it.","C_1 C_2 P C_1 C_2 (0, 0) x (n,s) n,s \in \mathbb{Z} \Omega x x C_1 C_2 x |\Omega|={2p \choose p} p \in \mathbb{N} (1,1) (2p,0) {2p-1 \choose p-1} x {2p-2 \choose p-2} {2p-1 \choose p-1}-{2p-2 \choose p-2} x C_1 |\Omega|={2p \choose p} P = \frac{{2p-1 \choose p-1}-{2p-2 \choose p-2}}{{2p \choose p}}","['probability', 'combinatorics', 'statistics', 'solution-verification']"
25,What's the probability of generating an existing account id?,What's the probability of generating an existing account id?,,"I'm working on a website that supposed to generate a unique account id for every person that registers for the site. What is the probability of having the same user id being generated twice? Each user id is $32$ characters in length. Each character is randomly chosen and replaced from a set of characters A set is a combination of [a-zA-Z] that is $52$ characters in length. import secrets from string import ascii_letters as letters   def get_random_user_id():     while True:         user_id = ''.join([letters[secrets.randbelow(len(letters))]                             for _ in range(32)])         try:             User.objects.get(uid=user_id)         except User.DoesNotExist:             return user_id That's my current function def prob_of_col(n: int, set_size: int, output_size: int) -> float:     '''Calculates the probability of a collision.      Args:         n: The number of users.          set_size: The number of characters to choose from.         output_size: The number of characters to return.      Returns:         A float that ranges from 0 to 1.     '''      e = 2.7182818284590452353602875     return 1 - 1.0 / pow(e, ((n*(n-1))/(2*pow(set_size, output_size))))   n = pow(10, 27) set_size = 52 output_size = 32  prob = prob_of_col(n, set_size, output_size) print(prob)","I'm working on a website that supposed to generate a unique account id for every person that registers for the site. What is the probability of having the same user id being generated twice? Each user id is characters in length. Each character is randomly chosen and replaced from a set of characters A set is a combination of [a-zA-Z] that is characters in length. import secrets from string import ascii_letters as letters   def get_random_user_id():     while True:         user_id = ''.join([letters[secrets.randbelow(len(letters))]                             for _ in range(32)])         try:             User.objects.get(uid=user_id)         except User.DoesNotExist:             return user_id That's my current function def prob_of_col(n: int, set_size: int, output_size: int) -> float:     '''Calculates the probability of a collision.      Args:         n: The number of users.          set_size: The number of characters to choose from.         output_size: The number of characters to return.      Returns:         A float that ranges from 0 to 1.     '''      e = 2.7182818284590452353602875     return 1 - 1.0 / pow(e, ((n*(n-1))/(2*pow(set_size, output_size))))   n = pow(10, 27) set_size = 52 output_size = 32  prob = prob_of_col(n, set_size, output_size) print(prob)",32 52,"['probability', 'statistics']"
26,Given $n$ different points in a plane.,Given  different points in a plane.,n,"Given $n$ different points in a plane, $8$ of them are on one straight line. The other points are in general everywhere else, so there are no $3$ points on same one straight line. How many different triangles can you create from these n points? What I think is: since no three points in the plane are collinear, a triangle can be formed by selecting any three of the $n$ points. The three points can be selected in $^nC_3$ ways, similarly, the number of triangles formed by $8$ collinear points when no three points are collinear in the plane is $^8C_3$ . However, the triangles formed by these points are not allowed, so the total number of triangles formed would be ${^nC_3}- {^8C_3}$ Is that correct?","Given different points in a plane, of them are on one straight line. The other points are in general everywhere else, so there are no points on same one straight line. How many different triangles can you create from these n points? What I think is: since no three points in the plane are collinear, a triangle can be formed by selecting any three of the points. The three points can be selected in ways, similarly, the number of triangles formed by collinear points when no three points are collinear in the plane is . However, the triangles formed by these points are not allowed, so the total number of triangles formed would be Is that correct?",n 8 3 n ^nC_3 8 ^8C_3 {^nC_3}- {^8C_3},"['probability', 'abstract-algebra', 'combinatorics', 'discrete-mathematics']"
27,How to show that $\lim_{n\to\infty}\mathbb{P}\left(\bigg|\frac{1}{n}S_n-f(n)\bigg|>\varepsilon\right)=0$,How to show that,\lim_{n\to\infty}\mathbb{P}\left(\bigg|\frac{1}{n}S_n-f(n)\bigg|>\varepsilon\right)=0,"Consider a collection of independent events $(X_i)$ with $\mathbb{I}_{X_i}$ being the indicator random variable for $X_i$ . Let $$f(n)=\frac{1}{n}\sum_{i=1}^{n}\mathbb{P}(X_i)\quad\text{and}\quad S_n=\sum_{i=1}^{n}\mathbb{I}_{X_i}.$$ I'm interested in showing that $$\lim_{n\to\infty}\mathbb{P}\left(\bigg|\frac{1}{n}S_n-f(n)\bigg|>\varepsilon\right)=0,\quad\forall\varepsilon>0$$ (in other words show convergence in probability). My second attempt: (using the hints given in the comments below) Let $\varepsilon>0$ . $$\lim_{n\to\infty}\mathbb{P}\left(\bigg|\frac{1}{n}S_n-f(n)\bigg|>\varepsilon\right)=\lim_{n\to\infty}\mathbb{P}\left(\frac{1}{n}\bigg|\sum_{i=1}^{n}\big[\mathbb{I}_{X_i}-\mathbb{P}(X_i)\big]\bigg|>\varepsilon\right)$$ Let us use the Chebyshev's inequality, i.e., $\mathbb{P}(|Y|\geq a)\leq\frac{\mathbb{E}(Y^2)}{a^2}$ which leads to \begin{align} \lim_{n\to\infty}\mathbb{P}\left(\frac{1}{n}\bigg|\sum_{i=1}^{n}\big[\mathbb{I}_{X_i}-\mathbb{P}(X_i)\big]\bigg|>\varepsilon\right)& \leq\lim_{n\to\infty}\frac{\mathbb{E}\bigg(\frac{1}{n^2}\big(\sum_{i=1}^{n}\mathbb{I}_{X_i}-\mathbb{P}(X_i)\big)^2\bigg)}{n\varepsilon}\\ & =\lim_{n\to\infty}\sum_{i=1}^{n}\frac{\mathbb{E}\bigg(\big(\sum_{i=1}^{n}\mathbb{I}_{X_i}-\mathbb{P}(X_i)\big)^2\bigg)}{n^2\varepsilon}. \end{align} Can we argue that as the term $\sum_{i=1}^{n}\mathbb{I}_{X_i}-\mathbb{P}(X_i)$ is just a summation of numbers, the expectation is constant and so taking the limit of $\frac{\text{constant}}{n^2}$ is equal to zero? I'd appreciate any hints or help.","Consider a collection of independent events with being the indicator random variable for . Let I'm interested in showing that (in other words show convergence in probability). My second attempt: (using the hints given in the comments below) Let . Let us use the Chebyshev's inequality, i.e., which leads to Can we argue that as the term is just a summation of numbers, the expectation is constant and so taking the limit of is equal to zero? I'd appreciate any hints or help.","(X_i) \mathbb{I}_{X_i} X_i f(n)=\frac{1}{n}\sum_{i=1}^{n}\mathbb{P}(X_i)\quad\text{and}\quad S_n=\sum_{i=1}^{n}\mathbb{I}_{X_i}. \lim_{n\to\infty}\mathbb{P}\left(\bigg|\frac{1}{n}S_n-f(n)\bigg|>\varepsilon\right)=0,\quad\forall\varepsilon>0 \varepsilon>0 \lim_{n\to\infty}\mathbb{P}\left(\bigg|\frac{1}{n}S_n-f(n)\bigg|>\varepsilon\right)=\lim_{n\to\infty}\mathbb{P}\left(\frac{1}{n}\bigg|\sum_{i=1}^{n}\big[\mathbb{I}_{X_i}-\mathbb{P}(X_i)\big]\bigg|>\varepsilon\right) \mathbb{P}(|Y|\geq a)\leq\frac{\mathbb{E}(Y^2)}{a^2} \begin{align}
\lim_{n\to\infty}\mathbb{P}\left(\frac{1}{n}\bigg|\sum_{i=1}^{n}\big[\mathbb{I}_{X_i}-\mathbb{P}(X_i)\big]\bigg|>\varepsilon\right)& \leq\lim_{n\to\infty}\frac{\mathbb{E}\bigg(\frac{1}{n^2}\big(\sum_{i=1}^{n}\mathbb{I}_{X_i}-\mathbb{P}(X_i)\big)^2\bigg)}{n\varepsilon}\\
& =\lim_{n\to\infty}\sum_{i=1}^{n}\frac{\mathbb{E}\bigg(\big(\sum_{i=1}^{n}\mathbb{I}_{X_i}-\mathbb{P}(X_i)\big)^2\bigg)}{n^2\varepsilon}.
\end{align} \sum_{i=1}^{n}\mathbb{I}_{X_i}-\mathbb{P}(X_i) \frac{\text{constant}}{n^2}","['probability', 'probability-theory', 'solution-verification']"
28,Stuck on a probability law problem,Stuck on a probability law problem,,"I'm currently trying to solve a problem, I completed the first question but I am stuck at the second one, here is the problem: One person roll a die until the result is a $1$ , a second person toss a coin until he gets $3$ tails. How many tries are they going to make on average. $X$ = number of die rolls $Y$ = number of time we toss a coin $X$ follows a geometric law with parameter $1/6$ , $E(X) = 6$ and $V(X) = 30$ . $Y$ follows a negative binomial law with $n = 3$ and $p = 1/2,$ $E(Y) = 3/(1/2) = 6$ and $V(Y) = n(1-p)/p^2 = 6.$ What is the probability that they both stop at the same time $(p(X = Y)).$ I found $p(X=Y) =  \sum_{k=3}^\infty p(X=k, Y=k) $ so the same sum with $p(X=k)p(Y=k)$ since they are independent. Then $p(X=k)p(Y=k)  = (5/6)^{k-1} \cdot\frac16 \begin{pmatrix} k-1 \\ 2 \\ \end{pmatrix}   \left(\frac12\right)^{k-3}\left(\frac12\right)^3$ But now i don't know how to compute it so i can get the value of that probability? Any help would be very appreciated. Thanks","I'm currently trying to solve a problem, I completed the first question but I am stuck at the second one, here is the problem: One person roll a die until the result is a , a second person toss a coin until he gets tails. How many tries are they going to make on average. = number of die rolls = number of time we toss a coin follows a geometric law with parameter , and . follows a negative binomial law with and and What is the probability that they both stop at the same time I found so the same sum with since they are independent. Then But now i don't know how to compute it so i can get the value of that probability? Any help would be very appreciated. Thanks","1 3 X Y X 1/6 E(X) = 6 V(X) = 30 Y n = 3 p = 1/2, E(Y) = 3/(1/2) = 6 V(Y) = n(1-p)/p^2 = 6. (p(X = Y)). p(X=Y) =  \sum_{k=3}^\infty p(X=k, Y=k)  p(X=k)p(Y=k) p(X=k)p(Y=k)  = (5/6)^{k-1} \cdot\frac16 \begin{pmatrix} k-1 \\ 2 \\ \end{pmatrix} 
 \left(\frac12\right)^{k-3}\left(\frac12\right)^3","['probability', 'combinatorics', 'summation']"
29,Expected number of regions with $n$ random lines in a circle,Expected number of regions with  random lines in a circle,n,"There are $n$ random lines drawn in a circle, defined by endpoints being uniform on circle. I am trying to figure out the expected number of regions separated by $n$ lines. I know $f(0)=1$ , $f(1)=2$ and $f(2)=\frac{10}{3}$ . Though naive, I can work it out through enumeration. But when $n$ is large, is there a formula to do this?","There are random lines drawn in a circle, defined by endpoints being uniform on circle. I am trying to figure out the expected number of regions separated by lines. I know , and . Though naive, I can work it out through enumeration. But when is large, is there a formula to do this?",n n f(0)=1 f(1)=2 f(2)=\frac{10}{3} n,"['probability', 'statistics', 'uniform-distribution']"
30,"Gaussian 2-D Mixture, mean mode median of marginals and 2-D","Gaussian 2-D Mixture, mean mode median of marginals and 2-D",,"Let p(x1,x2) = $\dfrac {4}{10}\mathcal{N}\left( \begin{bmatrix} 10 \\ 2 \end{bmatrix},\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}\right) + \dfrac {6}{10} \mathcal{N}\left( \begin{bmatrix} 0 \\ 0 \end{bmatrix},\begin{bmatrix} 8.4 & 2.0 \\ 2.0 & 1.7 \end{bmatrix}\right)$ be a mixture of two gaussians. a. Compute the marginal distributions for each dimension. b. Compute the mean, mode and median for each marginal distribution. c. Compute the mean and mode for the two-dimensional distribution. I find this problem confusing, my approach for a.) I've used the fact  p(x1) = $\mathcal{N}\left(x1|\mu_{x1},\sum _{11} \right)$ and a formula for the 1-D mixture: $p\left( x\right) =\alpha p_{1}\left( x\right) +\left( 1-\alpha\right) p_{2}\left( x\right)$ $\mu _{x}=\alpha\mu _{1}+ \left( 1-\alpha\right) \mu_{2}$ and $\sigma =  \alpha \left( \mu ^{2}_{1} + \sigma^{2}_{1} \right) + (1-\alpha)\left( \mu ^{2}_{2} + \sigma^{2}_{2} \right)$ to achieve $\mu_{x1} = 4/10.10+0.6.0 = 4 $ and $\sum _{11} = 0.4(10^{2}+1) + 6/10(0+8.4^{2})$ =82.736 and similarly $\mu_{x2} = 0.8$ and $\sum _{22} = 3.734$ for part b) the mean mode and median should be the mean? and part c) p(x1,x2) = $\mathcal{N}\left( \begin{bmatrix} \mu_{x1} \\ \mu_{x2} \end{bmatrix},\begin{bmatrix} \sum _{11} & \sum _{12} \\ \sum _{21} & \sum _{22} \end{bmatrix}\right)$ where I need to calculate the covariances. I am sure this approach is not correct, any help would be appreciated as this problem is completely different to the others I have been working through.","Let p(x1,x2) = be a mixture of two gaussians. a. Compute the marginal distributions for each dimension. b. Compute the mean, mode and median for each marginal distribution. c. Compute the mean and mode for the two-dimensional distribution. I find this problem confusing, my approach for a.) I've used the fact  p(x1) = and a formula for the 1-D mixture: and to achieve and =82.736 and similarly and for part b) the mean mode and median should be the mean? and part c) p(x1,x2) = where I need to calculate the covariances. I am sure this approach is not correct, any help would be appreciated as this problem is completely different to the others I have been working through.","\dfrac {4}{10}\mathcal{N}\left( \begin{bmatrix} 10 \\ 2 \end{bmatrix},\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}\right) + \dfrac {6}{10}
\mathcal{N}\left( \begin{bmatrix} 0 \\ 0 \end{bmatrix},\begin{bmatrix} 8.4 & 2.0 \\ 2.0 & 1.7 \end{bmatrix}\right) \mathcal{N}\left(x1|\mu_{x1},\sum _{11}
\right) p\left( x\right) =\alpha p_{1}\left( x\right) +\left( 1-\alpha\right) p_{2}\left( x\right) \mu _{x}=\alpha\mu _{1}+ \left( 1-\alpha\right) \mu_{2} \sigma =  \alpha \left( \mu ^{2}_{1} + \sigma^{2}_{1} \right) + (1-\alpha)\left( \mu ^{2}_{2} + \sigma^{2}_{2} \right) \mu_{x1} = 4/10.10+0.6.0 = 4  \sum _{11} = 0.4(10^{2}+1) + 6/10(0+8.4^{2}) \mu_{x2} = 0.8 \sum _{22} = 3.734 \mathcal{N}\left( \begin{bmatrix} \mu_{x1} \\ \mu_{x2} \end{bmatrix},\begin{bmatrix} \sum _{11} & \sum _{12} \\ \sum _{21} & \sum _{22} \end{bmatrix}\right)","['probability', 'multivariable-calculus', 'probability-distributions', 'conditional-probability', 'marginal-probability']"
31,Expectation of nonnegative random variable when passed through nonnegative increasing differentiable function. Part II: Electric Boogaloo,Expectation of nonnegative random variable when passed through nonnegative increasing differentiable function. Part II: Electric Boogaloo,,"This is a follow up to my previous question: Expectation of nonnegative random variable when passed through nonnegative increasing differentiable function I am now wanting to establish a follow up to the above problem. Specifically, if $X$ is a nonnegative random variable and $g:\mathbb{R}\rightarrow\mathbb{R}$ is a nonnegative, strictly increasing, differentiable function, then $$\mathbb{E}g(X)<\infty \iff \sum_{n=1}^{\infty}g^{\prime}(n)\mathbb{P}(X>n)<\infty$$ I believe I can show the inequality when $g(x)=x^{p}$ for $p\in\mathbb{N}$ , but the case of a general $g$ is more mysterious to me. My attempt for the converse proceeds in the following way: If you assume that the series converges then (by the linked question) \begin{equation} \mathbb{E}g(X) =  g(0)+\int_{0}^{\infty}g^{\prime}(X)\mathbb{P}(X>x)dx \\  =  g(0)+\sum_{n=0}^{\infty}\int_{n}^{n+1}g^{\prime}(x)\mathbb{P}(X>x)dx \\ \leq  g(0)+\sum_{n=0}^{\infty}(g^{\prime}(n+1)+g^{\prime}(n))\mathbb{P}(X>n) \\ =  g(0)+\left(\sum_{n=0}^{\infty}g^{\prime}(n+1)\mathbb{P}(X>n)\right)+\left(\sum_{n=0}^{\infty}g^{\prime}(n)\mathbb{P}(X>n)\right). \end{equation} However I am unsure how to proceed from here. I don't see how the middle series would converge without more assumptions on $g$ . Any help with the equivalence in general would be appreciated.","This is a follow up to my previous question: Expectation of nonnegative random variable when passed through nonnegative increasing differentiable function I am now wanting to establish a follow up to the above problem. Specifically, if is a nonnegative random variable and is a nonnegative, strictly increasing, differentiable function, then I believe I can show the inequality when for , but the case of a general is more mysterious to me. My attempt for the converse proceeds in the following way: If you assume that the series converges then (by the linked question) However I am unsure how to proceed from here. I don't see how the middle series would converge without more assumptions on . Any help with the equivalence in general would be appreciated.","X g:\mathbb{R}\rightarrow\mathbb{R} \mathbb{E}g(X)<\infty \iff \sum_{n=1}^{\infty}g^{\prime}(n)\mathbb{P}(X>n)<\infty g(x)=x^{p} p\in\mathbb{N} g \begin{equation}
\mathbb{E}g(X) =  g(0)+\int_{0}^{\infty}g^{\prime}(X)\mathbb{P}(X>x)dx \\ 
=  g(0)+\sum_{n=0}^{\infty}\int_{n}^{n+1}g^{\prime}(x)\mathbb{P}(X>x)dx \\
\leq  g(0)+\sum_{n=0}^{\infty}(g^{\prime}(n+1)+g^{\prime}(n))\mathbb{P}(X>n) \\
=  g(0)+\left(\sum_{n=0}^{\infty}g^{\prime}(n+1)\mathbb{P}(X>n)\right)+\left(\sum_{n=0}^{\infty}g^{\prime}(n)\mathbb{P}(X>n)\right).
\end{equation} g","['probability', 'random-variables', 'expected-value']"
32,"Is ""ensemble"" a standard term in probability theory?","Is ""ensemble"" a standard term in probability theory?",,"The book ""information theory, inference, and learning algorithms"" uses a definition to formalize probability: An ensemble $X$ is a triple $(x,\mathcal A_X,\mathcal P_X)$ where the outcome $x$ is the value of a random variable, which takes on one of a set of possible values, $\mathcal A_x =\{a_1,a_2,\dots,a_i,\dots,a_I\}$ , having probabilities $\mathcal P_X = \{p_1,p_2,\dots,p_I\}$ , with $P(x=a_i)=p_i$ , $p_i\ge0$ and $\sum\limits_{a_i\in\mathcal A_X} P(x=a_i)=1$ . I haven't seen this definition before (I am used to the concept of a probability space). Is this a standard definition? Does it relate to ""ensemble"" in statistical physics in any way? I haven't found much on it online.","The book ""information theory, inference, and learning algorithms"" uses a definition to formalize probability: An ensemble is a triple where the outcome is the value of a random variable, which takes on one of a set of possible values, , having probabilities , with , and . I haven't seen this definition before (I am used to the concept of a probability space). Is this a standard definition? Does it relate to ""ensemble"" in statistical physics in any way? I haven't found much on it online.","X (x,\mathcal A_X,\mathcal P_X) x \mathcal A_x =\{a_1,a_2,\dots,a_i,\dots,a_I\} \mathcal P_X = \{p_1,p_2,\dots,p_I\} P(x=a_i)=p_i p_i\ge0 \sum\limits_{a_i\in\mathcal A_X} P(x=a_i)=1","['probability', 'terminology']"
33,Proving $\frac{S_n}{n^{1/p}} \to 0$ almost surely implies $E|X_i|^p < \infty$ [duplicate],Proving  almost surely implies  [duplicate],\frac{S_n}{n^{1/p}} \to 0 E|X_i|^p < \infty,"This question already has answers here : Almost Sure Convergence and Bounded Expectation (2 answers) Closed 3 years ago . The following is a execise from Durrett's: Probability: Theory and Examples . Let $p>0$ .  Let $X_i$ be i.i.d random variables such that $EX_i =0$ , and define $S_n = \sum_{i=1}^n X_i$ . Show that if $\dfrac{S_n}{n^{1/p}}\to 0\,$ almost surely, then $E|X_i|^p < \infty$ . He says it's an easy exercise so I imagine there is a simple trick here. To start, I know that $$E|X_i|^p = \int_0^\infty P(|X_i|^p > x)dx = \int_0^\infty P(|X_i| > x^{1/p}) dx \leq 1+ \sum_{n=1}^\infty P(|X_n|>n^{1/p})$$ which I think might be helpful. I also think a contrapositive proof might be best? Suppose $E|X_i|^p = \infty$ . Then this implies $\sum_{n=1}^\infty P(|X_n|>n^{1/p}) = \infty$ . Since the $X_i$ 's are independent, the second Borel-Cantelli lemma then tells us: $$P(|X_n|>n^{1/p} \quad i.o) =  1.$$ And I think that this implies $S_n/n^{1/p}$ cannot converge to $0$ , since for the above implies $S_n/n^{1/p}>1$ infinitely often. Hence, giving the result by contrapositive. I'm really not confident with my probability theory. So I want to check my reasoning is correct here.","This question already has answers here : Almost Sure Convergence and Bounded Expectation (2 answers) Closed 3 years ago . The following is a execise from Durrett's: Probability: Theory and Examples . Let .  Let be i.i.d random variables such that , and define . Show that if almost surely, then . He says it's an easy exercise so I imagine there is a simple trick here. To start, I know that which I think might be helpful. I also think a contrapositive proof might be best? Suppose . Then this implies . Since the 's are independent, the second Borel-Cantelli lemma then tells us: And I think that this implies cannot converge to , since for the above implies infinitely often. Hence, giving the result by contrapositive. I'm really not confident with my probability theory. So I want to check my reasoning is correct here.","p>0 X_i EX_i =0 S_n = \sum_{i=1}^n X_i \dfrac{S_n}{n^{1/p}}\to 0\, E|X_i|^p < \infty E|X_i|^p = \int_0^\infty P(|X_i|^p > x)dx = \int_0^\infty P(|X_i| > x^{1/p}) dx \leq 1+ \sum_{n=1}^\infty P(|X_n|>n^{1/p}) E|X_i|^p = \infty \sum_{n=1}^\infty P(|X_n|>n^{1/p}) = \infty X_i P(|X_n|>n^{1/p} \quad i.o) =  1. S_n/n^{1/p} 0 S_n/n^{1/p}>1","['probability', 'probability-theory', 'proof-verification']"
34,Difference between Bernoulli random variables,Difference between Bernoulli random variables,,"Given are $n$ independent Bernoulli random variables with parameters $p_1,\dots,p_n$ . We want to split them into two parts so as to minimize the expectation $\mathbb{E}[|X-Y|]$ , where $X$ is the sum of the first part and $Y$ the sum of the second part. What is the best way to split so that this expectation is minimized? A reasonable guess is that it is always to make the sums of the parameters $p_i$ 's in the two parts as close as possible. Are there examples where this is not optimal?","Given are independent Bernoulli random variables with parameters . We want to split them into two parts so as to minimize the expectation , where is the sum of the first part and the sum of the second part. What is the best way to split so that this expectation is minimized? A reasonable guess is that it is always to make the sums of the parameters 's in the two parts as close as possible. Are there examples where this is not optimal?","n p_1,\dots,p_n \mathbb{E}[|X-Y|] X Y p_i","['probability', 'probability-theory', 'independence']"
35,Conditional running maximum of Geometric Brownian Motion (maximum of Brownian Bridge),Conditional running maximum of Geometric Brownian Motion (maximum of Brownian Bridge),,"I would appreciate help proving a formula that I came across on page 774 of these lecture note slides on pricing barrier options : https://www.csie.ntu.edu.tw/~lyuu/finance1/2015/20150520.pdf Assume the stock price follows a Geometric Brownian Motion, i.e. $dS_t = \mu S_t dt + \sigma S_t dW_t$ for $t \in [0,T]$ , and $W_t$ is a Brownian Motion (seeded at 0) under the measure $\mathbb{P}$ . Let $H$ be a barrier satisfying $H > S(0)$ and $H > S(T)$ . Then the formula i'm trying to prove is : $$\mathbb{P} \Big[ \max_{t \in [0,T]} S(t) < H \  | \ S(0) = S_0, S(T) = S_T \Big] = 1 - \exp \Big(-\frac{2\ln(H/S_0)\ln(H/S_T)}{\sigma^2 T} \Big).$$ Using the solution to the SDE, namely $S(t) = S_0 \exp((\mu - \sigma^2/2)t + \sigma W_t)$ , we can rewrite this probability as $$\mathbb{P} \Big[ \max_{t \in [0,T]} W_t + \theta t < b \  | \ W_0 = 0, W_T + \theta T = a \Big] $$ $$ = \mathbb{E}^{\mathbb{P}} \Big[ \mathbb{1}_{ \{\max_{t \in [0,T]} W_t + \theta t < b \}}  | \ W_0 = 0, W_T + \theta T = a \Big],$$ where $\theta := \mu/\sigma -\sigma/2$ , $b:= \ln(H/S_0)/\sigma$ and $a:= \ln(S_T/S_0)/\sigma$ . My understanding is that under the measure $\mathbb{P}$ , $W_t$ is a Brownian Motion, but $W_t + \theta t$ isn't. So what I would have done is to apply Girsanov's theorem. Setting $\tilde{W}_t := W_t + \theta t$ , then we know that $\tilde{W}_t$ is a Brownian Motion (also seeded at 0) under $\tilde{\mathbb{P}}$ , satisfying $$\frac{d\tilde{\mathbb{P}}}{d\mathbb{P}} = \exp(-\theta W_T -\theta^2 T /2) = \exp(-\theta \tilde{W}_T + \theta^2 T /2).$$ Then our calculation becomes equal to $$\mathbb{E}^{\mathbb{\tilde{P}}} \Big[ \frac{d\mathbb{P}}{d\tilde{\mathbb{P}}}  \mathbb{1}_{ \{\max_{t \in [0,T]} \tilde{W}_t < b \}}  | \ \tilde{W}_0 = 0, \tilde{W}_T = a \Big]$$ $$=\exp(\theta a - \theta^2 T /2) \mathbb{E}^{\mathbb{\tilde{P}}} \Big[ \mathbb{1}_{ \{\max_{t \in [0,T]} \tilde{W}_t < b \}}  | \ \tilde{W}_0 = 0, \tilde{W}_T = a \Big]$$ $$=\exp(\theta a - \theta^2 T /2) \mathbb{\tilde{P}} \Big[ \max_{t \in [0,T]} \tilde{W_t} < b \  | \ \tilde{W}_0 = 0, \tilde{W}_T = a \Big].$$ The latter probability is ""well-known"" as the probability of the running maximum of a Brownian Bridge. An online derivation is given in : https://eventuallyalmosteverywhere.wordpress.com/tag/brownian-bridge/ This probability is equal to $$\mathbb{\tilde{P}} \Big[ \max_{t \in [0,T]} \tilde{W_t} < b \  | \ \tilde{W}_0 = 0, \tilde{W}_T = a \Big] = 1 - \exp \Big( \frac{a^2 - (2b-a)^2}{2T} \Big).$$ Plugging in the values of $a$ and $b$ we find that $$\mathbb{\tilde{P}} \Big[ \max_{t \in [0,T]} \tilde{W_t} < b \  | \ \tilde{W}_0 = 0, \tilde{W}_T = a \Big] = 1 - \exp \Big(-\frac{2\ln(H/S_0)\ln(H/S_T)}{\sigma^2 T} \Big) \ !!$$ But this is supposed to be the final answer to my problem, so it appears that the factor $\exp(\theta a - \theta^2 T /2)$ is not supposed to be there.... Any help will be greatly appreciated !","I would appreciate help proving a formula that I came across on page 774 of these lecture note slides on pricing barrier options : https://www.csie.ntu.edu.tw/~lyuu/finance1/2015/20150520.pdf Assume the stock price follows a Geometric Brownian Motion, i.e. for , and is a Brownian Motion (seeded at 0) under the measure . Let be a barrier satisfying and . Then the formula i'm trying to prove is : Using the solution to the SDE, namely , we can rewrite this probability as where , and . My understanding is that under the measure , is a Brownian Motion, but isn't. So what I would have done is to apply Girsanov's theorem. Setting , then we know that is a Brownian Motion (also seeded at 0) under , satisfying Then our calculation becomes equal to The latter probability is ""well-known"" as the probability of the running maximum of a Brownian Bridge. An online derivation is given in : https://eventuallyalmosteverywhere.wordpress.com/tag/brownian-bridge/ This probability is equal to Plugging in the values of and we find that But this is supposed to be the final answer to my problem, so it appears that the factor is not supposed to be there.... Any help will be greatly appreciated !","dS_t = \mu S_t dt + \sigma S_t dW_t t \in [0,T] W_t \mathbb{P} H H > S(0) H > S(T) \mathbb{P} \Big[ \max_{t \in [0,T]} S(t) < H \  | \ S(0) = S_0, S(T) = S_T \Big] = 1 - \exp \Big(-\frac{2\ln(H/S_0)\ln(H/S_T)}{\sigma^2 T} \Big). S(t) = S_0 \exp((\mu - \sigma^2/2)t + \sigma W_t) \mathbb{P} \Big[ \max_{t \in [0,T]} W_t + \theta t < b \  | \ W_0 = 0, W_T + \theta T = a \Big]   = \mathbb{E}^{\mathbb{P}} \Big[ \mathbb{1}_{ \{\max_{t \in [0,T]} W_t + \theta t < b \}}  | \ W_0 = 0, W_T + \theta T = a \Big], \theta := \mu/\sigma -\sigma/2 b:= \ln(H/S_0)/\sigma a:= \ln(S_T/S_0)/\sigma \mathbb{P} W_t W_t + \theta t \tilde{W}_t := W_t + \theta t \tilde{W}_t \tilde{\mathbb{P}} \frac{d\tilde{\mathbb{P}}}{d\mathbb{P}} = \exp(-\theta W_T -\theta^2 T /2) = \exp(-\theta \tilde{W}_T + \theta^2 T /2). \mathbb{E}^{\mathbb{\tilde{P}}} \Big[ \frac{d\mathbb{P}}{d\tilde{\mathbb{P}}}  \mathbb{1}_{ \{\max_{t \in [0,T]} \tilde{W}_t < b \}}  | \ \tilde{W}_0 = 0, \tilde{W}_T = a \Big] =\exp(\theta a - \theta^2 T /2) \mathbb{E}^{\mathbb{\tilde{P}}} \Big[ \mathbb{1}_{ \{\max_{t \in [0,T]} \tilde{W}_t < b \}}  | \ \tilde{W}_0 = 0, \tilde{W}_T = a \Big] =\exp(\theta a - \theta^2 T /2) \mathbb{\tilde{P}} \Big[ \max_{t \in [0,T]} \tilde{W_t} < b \  | \ \tilde{W}_0 = 0, \tilde{W}_T = a \Big]. \mathbb{\tilde{P}} \Big[ \max_{t \in [0,T]} \tilde{W_t} < b \  | \ \tilde{W}_0 = 0, \tilde{W}_T = a \Big] = 1 - \exp \Big( \frac{a^2 - (2b-a)^2}{2T} \Big). a b \mathbb{\tilde{P}} \Big[ \max_{t \in [0,T]} \tilde{W_t} < b \  | \ \tilde{W}_0 = 0, \tilde{W}_T = a \Big] = 1 - \exp \Big(-\frac{2\ln(H/S_0)\ln(H/S_T)}{\sigma^2 T} \Big) \ !! \exp(\theta a - \theta^2 T /2)","['probability', 'probability-distributions', 'brownian-motion', 'conditional-probability']"
36,Expected distance squared of random walk on an infinite hexagonal grid,Expected distance squared of random walk on an infinite hexagonal grid,,I had a probability test and that was one of the questions: We have an infinite grid of hexagons like this: Each edge has a length of 1 and all the degrees are 120°. There's a particle in one of the vertices and each second it randomly moves to one of it's neighbours. After n seconds what is the expected distance squared of the particle from it's starting position? I spent a lot of time on that but I really have no idea how to even approached this question.,I had a probability test and that was one of the questions: We have an infinite grid of hexagons like this: Each edge has a length of 1 and all the degrees are 120°. There's a particle in one of the vertices and each second it randomly moves to one of it's neighbours. After n seconds what is the expected distance squared of the particle from it's starting position? I spent a lot of time on that but I really have no idea how to even approached this question.,,"['probability', 'random-walk', 'expected-value']"
37,Compute area of a sphere through a Dirac delta,Compute area of a sphere through a Dirac delta,,"I've been having issues with integrating with a Dirac delta. To compute the area of a sphere centered at $(0,0,0)$ it seems to work just fine: $$\int_{0}^{2\pi}{\int_{0}^{\pi}{\int_{0}^{\infty}{\delta(r-\rho)r^2\sin\theta\, dr}\,d\theta}\,d\phi} = 4\pi\rho^2$$ Now I will take the same sphere but offset by $(0,0,\rho)$ , that is: $x^2 + y^2 + (z-\rho)^2 = \rho^2$ . Going to spherical coordinates yields: $r^2\cos^2\phi\sin^2\theta + r^2\sin^2\phi\sin^2\theta + (r\cos\theta-\rho)^2 = \rho^2$ , which yields: $r(r-2\rho\cos\theta)=0$ , and we can express the sphere in spherical coordinates as: $r(\theta) = 2\rho\cos\theta, \theta \in [0,\pi/2], \phi\in[0,2\pi]$ . Integrating yields: $$\int_{0}^{2\pi}{\int_{0}^{\frac{\pi}{2}}{\int_{0}^{\infty}{\delta(r-2\rho\cos\theta)r^2\sin\theta\, dr}\,d\theta}\,d\phi} = \frac{8\pi\rho^2}{3}$$ Now this is not right clearly. The only reason I can think of has to do something with properties of the Dirac  delta I am unaware of. Note that I have not studied measure theory. I need the Dirac delta and not a surface integral  because I will be using this to compute transformations of probability density functions which I will need to write through a Dirac delta. Edit: References covering this for engineers/computer science students are welcome. Edit 2: Taking into account David Holden's answer I came up with the following fact which must hold (I hope it's correct): $$\int_{V}{\delta(f(x)) \,dx} = \int_{S = \{x|f(x) =0\}}{\,dA}$$ Edit 3: I found some more information on the subject: Impulse functions over curves and surfaces Properties_in_n_dimensions Surface area from indicator function Property of Dirac delta function in $\mathbb{R}^n$ Does the coarea formula hold for delta-function? I believe the issue was that whenever I offset the sphere the Dirac delta changed such that $\delta(f(r)) \rightarrow \delta(g(r,\theta))$ and $g$ was then a non-trivial mapping (so it's not the one-dimensional dirac delta I am used to anymore). Based on the first article I believe that I can rewrite it as a surface Dirac delta $\delta(g(r,\theta)) = \delta_S(r,\theta)$ which yields the surface integral giving a correct result. The other threads and wikipedia state that I should have a normalization by the magnitude of the gradient. I think I am missing an important piece since for the result to be correct this normalization factor should cancel out with something. More precisely: $$\int_V{\delta(r-2\rho\cos\theta)r^2\sin\theta \,dr\,d\theta\,d\phi} = \int_S{\frac{\,d\sigma}{\sqrt{r^2+\rho^2-2r\rho\cos\theta}}}$$ The only idea I have is that somehow the normalization factor will pop out of the $\,d\sigma$ . No idea though since it's supposed to be a 'Minkowski content measure' which is way over my head as a computer science student. To add to this I would also like to be able to solve the same problem with a heaviside function (for integrating the volume of the offset ball). I am unsure whether similar considerations apply there, however if I integrate it, the result seems correct. I still want to make sure this is valid for other volumes also (maybe it's just a coincidence like the sphere centered at $(0,0,0)$ ). So I would be grateful if somebody with more knowledge on geometric measure theory could clarify all of the points.","I've been having issues with integrating with a Dirac delta. To compute the area of a sphere centered at it seems to work just fine: Now I will take the same sphere but offset by , that is: . Going to spherical coordinates yields: , which yields: , and we can express the sphere in spherical coordinates as: . Integrating yields: Now this is not right clearly. The only reason I can think of has to do something with properties of the Dirac  delta I am unaware of. Note that I have not studied measure theory. I need the Dirac delta and not a surface integral  because I will be using this to compute transformations of probability density functions which I will need to write through a Dirac delta. Edit: References covering this for engineers/computer science students are welcome. Edit 2: Taking into account David Holden's answer I came up with the following fact which must hold (I hope it's correct): Edit 3: I found some more information on the subject: Impulse functions over curves and surfaces Properties_in_n_dimensions Surface area from indicator function Property of Dirac delta function in Does the coarea formula hold for delta-function? I believe the issue was that whenever I offset the sphere the Dirac delta changed such that and was then a non-trivial mapping (so it's not the one-dimensional dirac delta I am used to anymore). Based on the first article I believe that I can rewrite it as a surface Dirac delta which yields the surface integral giving a correct result. The other threads and wikipedia state that I should have a normalization by the magnitude of the gradient. I think I am missing an important piece since for the result to be correct this normalization factor should cancel out with something. More precisely: The only idea I have is that somehow the normalization factor will pop out of the . No idea though since it's supposed to be a 'Minkowski content measure' which is way over my head as a computer science student. To add to this I would also like to be able to solve the same problem with a heaviside function (for integrating the volume of the offset ball). I am unsure whether similar considerations apply there, however if I integrate it, the result seems correct. I still want to make sure this is valid for other volumes also (maybe it's just a coincidence like the sphere centered at ). So I would be grateful if somebody with more knowledge on geometric measure theory could clarify all of the points.","(0,0,0) \int_{0}^{2\pi}{\int_{0}^{\pi}{\int_{0}^{\infty}{\delta(r-\rho)r^2\sin\theta\, dr}\,d\theta}\,d\phi} = 4\pi\rho^2 (0,0,\rho) x^2 + y^2 + (z-\rho)^2 = \rho^2 r^2\cos^2\phi\sin^2\theta + r^2\sin^2\phi\sin^2\theta + (r\cos\theta-\rho)^2 = \rho^2 r(r-2\rho\cos\theta)=0 r(\theta) = 2\rho\cos\theta, \theta \in [0,\pi/2], \phi\in[0,2\pi] \int_{0}^{2\pi}{\int_{0}^{\frac{\pi}{2}}{\int_{0}^{\infty}{\delta(r-2\rho\cos\theta)r^2\sin\theta\, dr}\,d\theta}\,d\phi} = \frac{8\pi\rho^2}{3} \int_{V}{\delta(f(x)) \,dx} = \int_{S = \{x|f(x) =0\}}{\,dA} \mathbb{R}^n \delta(f(r)) \rightarrow \delta(g(r,\theta)) g \delta(g(r,\theta)) = \delta_S(r,\theta) \int_V{\delta(r-2\rho\cos\theta)r^2\sin\theta \,dr\,d\theta\,d\phi} = \int_S{\frac{\,d\sigma}{\sqrt{r^2+\rho^2-2r\rho\cos\theta}}} \,d\sigma (0,0,0)","['probability', 'multivariable-calculus', 'geometric-measure-theory']"
38,"Drawing balls of an urn, probability of one colour run out (with replacement)","Drawing balls of an urn, probability of one colour run out (with replacement)",,"I am trying to solve a probability problem but I do not manage to figure out a solution. I implemented the problem in C++ to convince myself of a solution but it didn't help (btw it shows that some colors run out with a low probability) Here is the problem: ""In a Urn, there are 17 red balls, 15 blue balls and 13 yellow balls. Each time randomly pick two balls, if they are in the same color, return both of them to the urn, if they are in different colors, they replace them with 2 balls in the third color. Will we run out of one of the color of balls?"" If you have any hint to help. I would be glad to hear any of them. Thank you EDIT: (Thanks to Daniel Mathias, adding a precision on the different outcomes: each state can still remain unchanged) Can we take an example with smaller numbers of balls and extend it to the previous problem? Let's say we have 1 red ball; 3 blue balls and 5 yellow balls. Then we have the couple (1,3,5) -- state 0 The different outcomes of the state 0 are: (0,2,7) -- game ends (0,5,4) -- game ends (3,2,4) -- state 1 (1,3,5) -- return state 0 Then the outcomes of the state 1 are: (5,1,3) -- return state 0 (2,4,3) -- return state 1 (2,1,6) -- state 2 The outcomes of the state 2 are: (4,0,5) -- game ends (1,0,8) -- game ends (1,3,5) -- return state 0 Then it is clear that the probability of one of the color of balls runs out is different from 0.","I am trying to solve a probability problem but I do not manage to figure out a solution. I implemented the problem in C++ to convince myself of a solution but it didn't help (btw it shows that some colors run out with a low probability) Here is the problem: ""In a Urn, there are 17 red balls, 15 blue balls and 13 yellow balls. Each time randomly pick two balls, if they are in the same color, return both of them to the urn, if they are in different colors, they replace them with 2 balls in the third color. Will we run out of one of the color of balls?"" If you have any hint to help. I would be glad to hear any of them. Thank you EDIT: (Thanks to Daniel Mathias, adding a precision on the different outcomes: each state can still remain unchanged) Can we take an example with smaller numbers of balls and extend it to the previous problem? Let's say we have 1 red ball; 3 blue balls and 5 yellow balls. Then we have the couple (1,3,5) -- state 0 The different outcomes of the state 0 are: (0,2,7) -- game ends (0,5,4) -- game ends (3,2,4) -- state 1 (1,3,5) -- return state 0 Then the outcomes of the state 1 are: (5,1,3) -- return state 0 (2,4,3) -- return state 1 (2,1,6) -- state 2 The outcomes of the state 2 are: (4,0,5) -- game ends (1,0,8) -- game ends (1,3,5) -- return state 0 Then it is clear that the probability of one of the color of balls runs out is different from 0.",,"['probability', 'combinatorics']"
39,First read for rigorous probability theory,First read for rigorous probability theory,,"I am looking for a book for self-study of rigorous probability theory. I would like a book which is at a completly introductory level, but which is rigorously written. Especially not welcome are books like Introduction to Probability by Sheldon Ross ( which is a great book in its own right ). The book should have as little assumptions as previous knowledge as possible. Analog to the book I am looking for is something like Spivaks Calculus or Apostols Calculus but for probability theory. I am really really stuck,since I suck at learning non-rigorous mathematics,and I always happen to land into some corner case when I try to apply it. Thanks in advance","I am looking for a book for self-study of rigorous probability theory. I would like a book which is at a completly introductory level, but which is rigorously written. Especially not welcome are books like Introduction to Probability by Sheldon Ross ( which is a great book in its own right ). The book should have as little assumptions as previous knowledge as possible. Analog to the book I am looking for is something like Spivaks Calculus or Apostols Calculus but for probability theory. I am really really stuck,since I suck at learning non-rigorous mathematics,and I always happen to land into some corner case when I try to apply it. Thanks in advance",,"['probability', 'probability-theory', 'reference-request', 'book-recommendation']"
40,Explanation for counter-intuitive discrete probability results,Explanation for counter-intuitive discrete probability results,,"Suppose there're two players $A$ , $B$ playing a dice game, $A$ has a normal dice whose faces are numbered 1 to 6, $B$ , on the other hand, has a (regular) icosahedron dice with faces numbered 1 to 20. Each round, $A$ rolls its dice 3 times and get the sum as his score, whereas $B$ rolls only once and get the result as his score, and the one with the greater score wins and gets 1 point. If scores are equal, then they both get 0.5 points.  Is this a fair game? A quick calculation shows that the expected scores for both players are 3.5, so it looks like it's perfectly fair. But to my surprise, when I was doing a Monte Carlo in Python, the programmed told me, out of 10^5 simulations, $A$ has only around 48.x% of the total 10^5 points, and such a 1.x% disadvantage was consistent across several tests. Now move on to a more complicated version. This time let's introduce a new player $C$ , who plays by exactly the same rule as $B$ does. Again, each round the player with the highest score wins; if two player has equal scores that are higher than the third player's, then they both get 1/2 points; and if all three have equal scores then they all get 1/3 points. Is this a fair game? I thought with a firm ""yes"" for the same reasons as previously. But the simulation results totally astounded me, much more than the previous 1.x% discrepancy: now in the 3 player game, out of 10^5 simulations, the player $A$ only gets about 27.x% of the points, whereas $B$ and $C$ both get about 36.x% - $A$ has an almost 9% disadvantage! I know to get everything clear it's best to explicitly compute the probability distribution case by case, and admittedly it's not hard in any technical sense. But, before doing that, is there any easy explanation about the discrepancies between intuition and reality as shown above? Thanks! Attached below is the code: import random import numpy as np import pandas as pd  def get_A_score():     return sum(random.randint(1, 6) for i in range(3))   def get_B_score():     return random.randint(1, 20)   if __name__ == '__main__':     n_sim = 100000     A_wins = 0.0     for i in range(n_sim):         if get_A_score() > get_B_score():             A_wins += 1         elif get_A_score() == get_B_score():             A_wins += 1/2   # in case of equal numbers     print(A_wins / n_sim)      print()      wins = np.array([0.0, 0.0, 0.0])     for i in range(n_sim):         A_score = get_A_score()         B_score = get_B_score()         C_score = get_B_score()         scores = np.array([A_score, B_score, C_score])         winner_index = np.argwhere(scores == np.amax(scores))         n_winners = len(winner_index)         wins[winner_index] += 1 / n_winners      print(wins / n_sim)","Suppose there're two players , playing a dice game, has a normal dice whose faces are numbered 1 to 6, , on the other hand, has a (regular) icosahedron dice with faces numbered 1 to 20. Each round, rolls its dice 3 times and get the sum as his score, whereas rolls only once and get the result as his score, and the one with the greater score wins and gets 1 point. If scores are equal, then they both get 0.5 points.  Is this a fair game? A quick calculation shows that the expected scores for both players are 3.5, so it looks like it's perfectly fair. But to my surprise, when I was doing a Monte Carlo in Python, the programmed told me, out of 10^5 simulations, has only around 48.x% of the total 10^5 points, and such a 1.x% disadvantage was consistent across several tests. Now move on to a more complicated version. This time let's introduce a new player , who plays by exactly the same rule as does. Again, each round the player with the highest score wins; if two player has equal scores that are higher than the third player's, then they both get 1/2 points; and if all three have equal scores then they all get 1/3 points. Is this a fair game? I thought with a firm ""yes"" for the same reasons as previously. But the simulation results totally astounded me, much more than the previous 1.x% discrepancy: now in the 3 player game, out of 10^5 simulations, the player only gets about 27.x% of the points, whereas and both get about 36.x% - has an almost 9% disadvantage! I know to get everything clear it's best to explicitly compute the probability distribution case by case, and admittedly it's not hard in any technical sense. But, before doing that, is there any easy explanation about the discrepancies between intuition and reality as shown above? Thanks! Attached below is the code: import random import numpy as np import pandas as pd  def get_A_score():     return sum(random.randint(1, 6) for i in range(3))   def get_B_score():     return random.randint(1, 20)   if __name__ == '__main__':     n_sim = 100000     A_wins = 0.0     for i in range(n_sim):         if get_A_score() > get_B_score():             A_wins += 1         elif get_A_score() == get_B_score():             A_wins += 1/2   # in case of equal numbers     print(A_wins / n_sim)      print()      wins = np.array([0.0, 0.0, 0.0])     for i in range(n_sim):         A_score = get_A_score()         B_score = get_B_score()         C_score = get_B_score()         scores = np.array([A_score, B_score, C_score])         winner_index = np.argwhere(scores == np.amax(scores))         n_winners = len(winner_index)         wins[winner_index] += 1 / n_winners      print(wins / n_sim)",A B A B A B A C B A B C A,"['probability', 'discrete-mathematics', 'intuition', 'dice']"
41,Find the limit of $\sum\limits_{r=\lfloor an \rfloor}^{\lfloor bn \rfloor} {n \choose r } p^r (1-p)^{n-r}$ using the central limit theorem,Find the limit of  using the central limit theorem,\sum\limits_{r=\lfloor an \rfloor}^{\lfloor bn \rfloor} {n \choose r } p^r (1-p)^{n-r},"Let $p \in(0,1)$. What is the distribution of the sum of $n$ independent Bernoulli random variables with parameter $p$? Let $0 \leq a < b \leq 1$. Use approprtiate limit theorems to determine how the following depends on $a$ and $b$:    $$\lim_{n\to\infty} \sum_{r=\lfloor an \rfloor}^{\lfloor bn \rfloor} {n \choose r } p^r (1-p)^{n-r}\ $$ I know that the sum of $n$ Bernoulli random variables with parameter $p$ has binomial distribution $Bin(n,p)$. I see that the above is equal to $P(an \leq X \leq bn) $ where $X$ is the sum, and I can see how to use the weak law of large numbers in the cases where $p \neq a$ and $p \neq b$ to find the appropriate limits, but I can't see what I have to do when $p=a$ or $p=b$. I assume I have to apply the central limit theorem, but I really don't see how to do that. Any help you could give me would be really appreciated.","Let $p \in(0,1)$. What is the distribution of the sum of $n$ independent Bernoulli random variables with parameter $p$? Let $0 \leq a < b \leq 1$. Use approprtiate limit theorems to determine how the following depends on $a$ and $b$:    $$\lim_{n\to\infty} \sum_{r=\lfloor an \rfloor}^{\lfloor bn \rfloor} {n \choose r } p^r (1-p)^{n-r}\ $$ I know that the sum of $n$ Bernoulli random variables with parameter $p$ has binomial distribution $Bin(n,p)$. I see that the above is equal to $P(an \leq X \leq bn) $ where $X$ is the sum, and I can see how to use the weak law of large numbers in the cases where $p \neq a$ and $p \neq b$ to find the appropriate limits, but I can't see what I have to do when $p=a$ or $p=b$. I assume I have to apply the central limit theorem, but I really don't see how to do that. Any help you could give me would be really appreciated.",,"['probability', 'probability-theory', 'probability-distributions', 'central-limit-theorem', 'law-of-large-numbers']"
42,Bounds on moments of sample mean,Bounds on moments of sample mean,,"Let $X_i$s be i.i.d zero mean random variables whose $p$-th moments are finite. Prove   $$E\left[\left(\sum_{i=1}^{n}X_i\right)^p\right]\leq C_p n^{p/2}$$   where $C_p$ is a constant independent of $n$. My effort: I think we have to expand the sum and then say each term is of $k$-th moment, count the number of them and the upper bound. This seems to be messy. Is there a better solution? Or, is it a name of a theorem?","Let $X_i$s be i.i.d zero mean random variables whose $p$-th moments are finite. Prove   $$E\left[\left(\sum_{i=1}^{n}X_i\right)^p\right]\leq C_p n^{p/2}$$   where $C_p$ is a constant independent of $n$. My effort: I think we have to expand the sum and then say each term is of $k$-th moment, count the number of them and the upper bound. This seems to be messy. Is there a better solution? Or, is it a name of a theorem?",,"['probability', 'probability-theory', 'expected-value']"
43,"Bernoulli trials (n,p) - probability for even/odd number of successes","Bernoulli trials (n,p) - probability for even/odd number of successes",,"I came across this problem. It asks what is the probability of even number of successes in a series of $n$ Bernoulli trials with probability of success in each trial equal to $p \neq \frac{1}{2}$. Knowing the formula for $k$ number of successes I was able to immediately write down this: $$ p_{\ even} = \sum_{k=0}^{\lfloor\frac{n}{2}\rfloor}{n \choose 2k} p^{2k}(1-p)^{n-2k} $$ I think that is correct, right? But after that when I saw the solution which they give in the book, I found (as I suspected by the way) that there's a closed form formula and it goes. $$ p_{\ even} =  \frac{1}{2} + \frac{1}{2} (1-2p)^{n}$$ The proof is simple, one basically argues inductively over $n$. This made me think (and this is my question here)... Is there an algebraic way to prove the following equality? I mean some way which simply manipulates these binomial coefficients and uses some of their known properties. $$ \sum_{k=0}^{\lfloor\frac{n}{2}\rfloor}{n \choose 2k} p^{2k}(1-p)^{n-2k} = \frac{1}{2} + \frac{1}{2} (1-2p)^{n} $$","I came across this problem. It asks what is the probability of even number of successes in a series of $n$ Bernoulli trials with probability of success in each trial equal to $p \neq \frac{1}{2}$. Knowing the formula for $k$ number of successes I was able to immediately write down this: $$ p_{\ even} = \sum_{k=0}^{\lfloor\frac{n}{2}\rfloor}{n \choose 2k} p^{2k}(1-p)^{n-2k} $$ I think that is correct, right? But after that when I saw the solution which they give in the book, I found (as I suspected by the way) that there's a closed form formula and it goes. $$ p_{\ even} =  \frac{1}{2} + \frac{1}{2} (1-2p)^{n}$$ The proof is simple, one basically argues inductively over $n$. This made me think (and this is my question here)... Is there an algebraic way to prove the following equality? I mean some way which simply manipulates these binomial coefficients and uses some of their known properties. $$ \sum_{k=0}^{\lfloor\frac{n}{2}\rfloor}{n \choose 2k} p^{2k}(1-p)^{n-2k} = \frac{1}{2} + \frac{1}{2} (1-2p)^{n} $$",,"['probability', 'combinatorics', 'probability-theory', 'binomial-coefficients', 'combinations']"
44,Players A and B repeatedly flip a (possibly unfair) coin until one loses everything. Each starting with a different amount of money,Players A and B repeatedly flip a (possibly unfair) coin until one loses everything. Each starting with a different amount of money,,"There are two players $A$ and $B$. At the beginning $A$ has $a_0 ∈ [0,1]$ amount of money and $B$ has $b_0 = 1 - a_0$ amount of money. They have one possibly unfair coin which they flip repeatedly. $A$ always guesses $Heads$ and $B$ always guesses $Tails$. Before each flip, they both bet a minimum of what each of them has (i.e. $bet_i = min(a_i, b_i)$). After the coin is tossed, the winner takes the $bet_i$ and they play again until one of them has no more money to play with. I've been thinking about it for some time now and was unable to find a closed formula nor find it mentioned anywhere. So any more info would be appreciated. The best I could do was a simulation. Interestingly it seems that when the coin is fair, the probability of $A$ winning is proportional to $a_0$ but when the coin is unfair, the graph looks more peculiar: Here, the horizontal axis represents $a_0$ and the vertical one represents $P(A\ wins\ given\ that\ the\ coin\ is\ fair)$ In the next graph, the horizontal axis also represents $a_0$ and the vertical one represents $P(A\ wins\ given\ that\ the\ coin\ ends\ up\ Heads\ 1\ in\ 3\ times)$. The code for the simulation can be found here .","There are two players $A$ and $B$. At the beginning $A$ has $a_0 ∈ [0,1]$ amount of money and $B$ has $b_0 = 1 - a_0$ amount of money. They have one possibly unfair coin which they flip repeatedly. $A$ always guesses $Heads$ and $B$ always guesses $Tails$. Before each flip, they both bet a minimum of what each of them has (i.e. $bet_i = min(a_i, b_i)$). After the coin is tossed, the winner takes the $bet_i$ and they play again until one of them has no more money to play with. I've been thinking about it for some time now and was unable to find a closed formula nor find it mentioned anywhere. So any more info would be appreciated. The best I could do was a simulation. Interestingly it seems that when the coin is fair, the probability of $A$ winning is proportional to $a_0$ but when the coin is unfair, the graph looks more peculiar: Here, the horizontal axis represents $a_0$ and the vertical one represents $P(A\ wins\ given\ that\ the\ coin\ is\ fair)$ In the next graph, the horizontal axis also represents $a_0$ and the vertical one represents $P(A\ wins\ given\ that\ the\ coin\ ends\ up\ Heads\ 1\ in\ 3\ times)$. The code for the simulation can be found here .",,"['probability', 'economics']"
45,Why does experimental probability approach theoretical probability? Why does it converge only when there are large samples and not when it's small?,Why does experimental probability approach theoretical probability? Why does it converge only when there are large samples and not when it's small?,,"I went through Khan Academy's lecture on theoretical and experimental probability . I also read through a Wikipedia article on this but was still not clear. I understand how it approaches (as explained in the video) but unable to understand why experimental probability approaches theoretical probability. What is the reason for this? I think that the general sense is, if I take a large enough sample, I am going to end up getting the expected mean of the sample. The more experiments I do, the more it converges. Sure, I get that. But why does it converge only when there are large samples and not when it's small?","I went through Khan Academy's lecture on theoretical and experimental probability . I also read through a Wikipedia article on this but was still not clear. I understand how it approaches (as explained in the video) but unable to understand why experimental probability approaches theoretical probability. What is the reason for this? I think that the general sense is, if I take a large enough sample, I am going to end up getting the expected mean of the sample. The more experiments I do, the more it converges. Sure, I get that. But why does it converge only when there are large samples and not when it's small?",,"['probability', 'statistics']"
46,S.N. Bernstein Law of Large Numbers,S.N. Bernstein Law of Large Numbers,,"while reading a paper named ""Network Embedding as Matrix Factorization: UnifyingDeepWalk, LINE, PTE, and node2vec"" ( http://keg.cs.tsinghua.edu.cn/jietang/publications/WSDM18-Qiu-et-al-NetMF-network-embedding.pdf ), the authors give in the Appendix a lemma which is stated as follows: (S.N. Bernstein Law of Large Numbers) Let $Y_1,Y_2,\ldots$ be a sequence of random variables with finite expectation $E(Y_j)<\infty$ (which, according to my understanding, is invariant for different $Y_i$), and uniformly bounded variance $Var(Y_j)<K<\infty$, $j\geq 1$, and covariances are s.t.  $$ Cov(Y_i, Y_j)\rightarrow 0, |i-j|\rightarrow\infty $$ Then the law of large number holds. However, I failed to find a proof, or a similar statement of Bernstein's Law of Large Numbers after I tried to google it. The authors cited for this lemma Problems in Probabilities by Albert N. Shiryaev, which I believe is an exercise book. The interesting fact is that the statement does not assume that random variables are independent. Would anyone tell me where I can find the source of the theorem and the proof?  Thanks! I hope this question is appropriate; if not, please tell me and I'll remove it.","while reading a paper named ""Network Embedding as Matrix Factorization: UnifyingDeepWalk, LINE, PTE, and node2vec"" ( http://keg.cs.tsinghua.edu.cn/jietang/publications/WSDM18-Qiu-et-al-NetMF-network-embedding.pdf ), the authors give in the Appendix a lemma which is stated as follows: (S.N. Bernstein Law of Large Numbers) Let $Y_1,Y_2,\ldots$ be a sequence of random variables with finite expectation $E(Y_j)<\infty$ (which, according to my understanding, is invariant for different $Y_i$), and uniformly bounded variance $Var(Y_j)<K<\infty$, $j\geq 1$, and covariances are s.t.  $$ Cov(Y_i, Y_j)\rightarrow 0, |i-j|\rightarrow\infty $$ Then the law of large number holds. However, I failed to find a proof, or a similar statement of Bernstein's Law of Large Numbers after I tried to google it. The authors cited for this lemma Problems in Probabilities by Albert N. Shiryaev, which I believe is an exercise book. The interesting fact is that the statement does not assume that random variables are independent. Would anyone tell me where I can find the source of the theorem and the proof?  Thanks! I hope this question is appropriate; if not, please tell me and I'll remove it.",,"['probability', 'probability-theory', 'law-of-large-numbers']"
47,Will a 2 dimensional random walk with random orientations almost certainly return near the origin infinitely often?,Will a 2 dimensional random walk with random orientations almost certainly return near the origin infinitely often?,,"It is well known that if you perform a random walk on a 2 dimensional lattice then you will almost certainly reach every lattice point infinitely many times.  Is the same result true if, instead of walking on a lattice, we walk in a random orientation (always using a distance of 1)? Of course, we cannot expect to land on any given point with positive probability, so we modify the question to ask: in any disk, is the probability 1 that the random walk will eventually enter?  I think this is equivalent to asking if the random walk will eventually any specific disk infinitely many times (e.g. one around the origin), because then there is a positive probability of taking any set of paths with a positive probability to get to the other disk, which is in theory not hard to construct. What I have tried: One approach is to use the result on the lattice (by converting a random walk on the plane to a random walk in the lattice) to prove this result, but I haven't made any meaningful progress in this direction. Another approach is to mimic a proof that works over the lattice.  The only proof strategy I am somewhat familiar with to prove the result over the lattice (although I am aware that there are others) is to show that the sum of the probabilities that you are at the origin after $n$ steps for each $n$ diverges, and then showing that this implies that the probability that you will return infinitely many times is 1.  But it seems neither step generalizes directly.  Something we could try is to prove that. for any point $P$ and any circle of radius $r$ containing $P$, the sum of the probabilities that the walk (re-)enters that circle on step $n$ is infinite; I think this would fix the second step to work in this case by using the argument from page 163 of https://services.math.duke.edu/~rtd/PTE/PTE4_1.pdf (the proof of theorem 4.2.2) by letting $r$ be half the radius of the original disk and always choosing a circle containing the origin.","It is well known that if you perform a random walk on a 2 dimensional lattice then you will almost certainly reach every lattice point infinitely many times.  Is the same result true if, instead of walking on a lattice, we walk in a random orientation (always using a distance of 1)? Of course, we cannot expect to land on any given point with positive probability, so we modify the question to ask: in any disk, is the probability 1 that the random walk will eventually enter?  I think this is equivalent to asking if the random walk will eventually any specific disk infinitely many times (e.g. one around the origin), because then there is a positive probability of taking any set of paths with a positive probability to get to the other disk, which is in theory not hard to construct. What I have tried: One approach is to use the result on the lattice (by converting a random walk on the plane to a random walk in the lattice) to prove this result, but I haven't made any meaningful progress in this direction. Another approach is to mimic a proof that works over the lattice.  The only proof strategy I am somewhat familiar with to prove the result over the lattice (although I am aware that there are others) is to show that the sum of the probabilities that you are at the origin after $n$ steps for each $n$ diverges, and then showing that this implies that the probability that you will return infinitely many times is 1.  But it seems neither step generalizes directly.  Something we could try is to prove that. for any point $P$ and any circle of radius $r$ containing $P$, the sum of the probabilities that the walk (re-)enters that circle on step $n$ is infinite; I think this would fix the second step to work in this case by using the argument from page 163 of https://services.math.duke.edu/~rtd/PTE/PTE4_1.pdf (the proof of theorem 4.2.2) by letting $r$ be half the radius of the original disk and always choosing a circle containing the origin.",,"['probability', 'random-walk']"
48,Expected value and standard deviation of $\sqrt{X}$,Expected value and standard deviation of,\sqrt{X},"Let X be a poisson-distributed stochastic variable, where  $$E(X) = m$$  $$ V(X) = m.$$  Let $Y=\sqrt{X}.$ Calculate (approximatively) $$E(Y),$$ $$V(Y).$$ Now, the answer is  $$E(Y) \approx \sqrt{m},$$ $$V(Y) \approx \left( \frac{1}{2\sqrt{m}} \right)^2$$ I've looked in my textbook but I don't know where this is coming from. Why is this the case? What (simple) theorem can I use to verify that it's true?.","Let X be a poisson-distributed stochastic variable, where  $$E(X) = m$$  $$ V(X) = m.$$  Let $Y=\sqrt{X}.$ Calculate (approximatively) $$E(Y),$$ $$V(Y).$$ Now, the answer is  $$E(Y) \approx \sqrt{m},$$ $$V(Y) \approx \left( \frac{1}{2\sqrt{m}} \right)^2$$ I've looked in my textbook but I don't know where this is coming from. Why is this the case? What (simple) theorem can I use to verify that it's true?.",,"['probability', 'probability-theory', 'statistics', 'expectation']"
49,Colour change in drawing balls (Expectation same),Colour change in drawing balls (Expectation same),,"You are given an urn with 100 balls (50 black and 50 white). You pick balls from urn one by one without replacements until all the balls are out. A black followed by a white or a white followed by a black is ""a colour change"". Calculate the expected number of colour changes if the balls are being picked randomly from the urn. The solutions for this puzzle goes as: There are 99 consecutive pairs. Let $X_i$ be a random variable taking value 1 if $i$th pair has a colour change and zero otherwise. We have to find expected value of $E[X_1 + X_2 + ... + X_{99}]$ Since all $X_i$ are equivalent, the answer is $99\, E[X_1]$ $E[X_1] = (50/100)\, (50/99)+(50/100)\, (50/99) = 50/99$ What is the intuition or proof behind all the $X_i$ being equivalent ?","You are given an urn with 100 balls (50 black and 50 white). You pick balls from urn one by one without replacements until all the balls are out. A black followed by a white or a white followed by a black is ""a colour change"". Calculate the expected number of colour changes if the balls are being picked randomly from the urn. The solutions for this puzzle goes as: There are 99 consecutive pairs. Let $X_i$ be a random variable taking value 1 if $i$th pair has a colour change and zero otherwise. We have to find expected value of $E[X_1 + X_2 + ... + X_{99}]$ Since all $X_i$ are equivalent, the answer is $99\, E[X_1]$ $E[X_1] = (50/100)\, (50/99)+(50/100)\, (50/99) = 50/99$ What is the intuition or proof behind all the $X_i$ being equivalent ?",,"['probability', 'combinatorics']"
50,How many (hypothetical) packs do I need to get every card in a set?,How many (hypothetical) packs do I need to get every card in a set?,,"The Question Let's say a hypothetical company makes trading cards. There are 50 unique cards, and you can buy the cards in packs of 5. Cards don't repeat within a pack. There's no rarity and their packing methods are perfect, so the cards are evenly distributed. How many packs would I need to buy for a 95% chance of getting a complete set (at least 1 of each card)? My Guesses I found this page, which has the formula for the number of packs I'd need to open to get a given card with 90% certainty. I plugged that back into the ln(1-chance)/ln(1-odds) to get the odds of that happening for every card, but I'm not sure that's the right approach. Thanks a bunch!","The Question Let's say a hypothetical company makes trading cards. There are 50 unique cards, and you can buy the cards in packs of 5. Cards don't repeat within a pack. There's no rarity and their packing methods are perfect, so the cards are evenly distributed. How many packs would I need to buy for a 95% chance of getting a complete set (at least 1 of each card)? My Guesses I found this page, which has the formula for the number of packs I'd need to open to get a given card with 90% certainty. I plugged that back into the ln(1-chance)/ln(1-odds) to get the odds of that happening for every card, but I'm not sure that's the right approach. Thanks a bunch!",,"['probability', 'combinatorics']"
51,Solving P(TRUE) of finding counterpart pairs in 2 sets with constraints involving the Universal Genetic Code,Solving P(TRUE) of finding counterpart pairs in 2 sets with constraints involving the Universal Genetic Code,,"Solving P(TRUE) of finding counterpart pairs in 2 sets with constraints involving the Universal Genetic Code Posting this question here as opposed to in biology, etc. as I only want the answer from a purely mathematical standpoint without any contextual bias. I apologize for the lack or incorrect usage of formal notation.  Did my best. I do not know if this can be done via some form of clever mathematics or I'd just need to brute force through with a computer(s) in some way to obtain a sample and then extrapolate that to the population. Regardless, here is my question. ( My use of ""|"" ANYWHERE means OR ) $$D = D\_EitherUnlessInitialSpecificed = (D\_Initial | D_CounterpartToInitial)$$ $$B = B\_EitherUnlessInitialSpecificed = (B\_Initial | B\_CounterpartToInitial)$$ Yes, I wrote that redundantly, but hopefully clearly.I hope I wrote that correctly. Definition of counterparts: a , c , g , and t do not represent anything quantifiable.  They only represent a quality. As stated more generally above, and more specifically down below: $$D\; can\; only = a\;  or \; c\; ; \; a < -- > c$$ $$if\;  D = a,\; then \;D_c = c$$$$if\;  D = c,\; then \;D_c = a$$ $$B\; can\; only = t\;  or \; g\; ; \; t < -- > g$$ $$if\;  B = t,\; then \;B_c = g$$$$if\;  B = g,\; then \;D_c = t$$ In the end, everything comes down to the placement of $a$, $c$, $g$, and $t$ within constraints I'll define shortly. $$W = (D\;|B)$$ and we'll be using 2 instances of $W$, $W_1$ & $W_2$ which are not exclusive of each other in any context, nor are they exclusive/equal when the same instance is repeated in the same or a different set/array/etc. They are more like placeholders for a position which you're trying to find the counterpart to. If I use $W_2$ or $W_1$ and then use it again multiple times, each time it takes on a value which is assigned actually only by you. Now, $$W_1 = (D | B)$$$$W_2 = (D | B)$$ $$Wa\; =\; an\; instance\; of\; the  \;set \;or\; what\; I\; am\; going \;to \;begin \;calling\; an\;""array""\; \{W_1,W_2\}$$ We'll make 8 individual instances of these paired arrays and then store them into an array, $ArrayTop$ aka $ArrT$ $$ArrT\;=\;\{Wa_1,Wa_2,Wa_3,Wa_4,Wa_5,Wa_6,Wa_7,Wa_8\}$$ You can also reference the items in the array from their order via  $ArrT_1,ArrT_2...etc.$ if needed. Now, we'll make 16 more individual instances of these paired arrays and then store them into another array, $ArrayBottom$ aka $ArrB$. However, we'll start $Wa_x$ at 9. $$ArrB\;=\;\{Wa_9,Wa_{10},Wa_{11},Wa_{12},Wa_{13},Wa_{14},Wa_{15},Wa_{16},Wa_{17},Wa_{18},Wa_{19},Wa_{20},Wa_{21},Wa_{22},Wa_{23},Wa_{24}\}$$ or $$ArrB_1, ArrB_2...ArrB_{16}$$ If, given $ArrT$ & $ArrB$ and these constraints , $$Each$$ $$ a, c, g,\ and\ t$$ $$has \;a \;required \;""point\; usage"" \;of \;exactly \;16\; for\; W_1 \;collectively\; and\; W_2\; collectively.$$ Every time one of these 4 letters is used in: $ArrT$, it acquires 4 points. $ArrB1$ or $ArrB2$, then it acquires 3 points. $ArrB3$ through to $ArrB14$, then it acquires 2 points. $ArrB15$ or $ArrB16$, then it acquires 1 point. To clarify further, a letter acquires points for it's $W_1$ and separately for its $W_2$. Find the number of all possible scenarios for the placements of  $$a,t,g,\; and\; c$$ where, for each item in $ArrT$, two items in $ArrB$ contain the respective counterparts of $W_1$ and $W_2$ from that item in $ArrT = TRUE$. Also, the total # of possible scenarios if you want, although that's somewhat easy to figure out. EDIT: I have included a picture to provide the context for what this is for and where it comes from to hopefully help resolve any confusion. One very important thing I need to stress here and which can cause confusion with this picture is the fact that there are actually a few symmetries with very low probabilities to occur especially in tandem between the two halves (pending you're looking for them).  I will say that this is our genetic code for everything on earth...so that's kind of cool but the question I'm asking has absolutely nothing to do with symmetry and only the presence of things in the top group that then have their counterparts in the bottom regardless of their location.  If you're just looking at how I define the problem above in the mathematical sense you would already understand that though. You can basically ignore the upper left part too....the picture explains the rest.","Solving P(TRUE) of finding counterpart pairs in 2 sets with constraints involving the Universal Genetic Code Posting this question here as opposed to in biology, etc. as I only want the answer from a purely mathematical standpoint without any contextual bias. I apologize for the lack or incorrect usage of formal notation.  Did my best. I do not know if this can be done via some form of clever mathematics or I'd just need to brute force through with a computer(s) in some way to obtain a sample and then extrapolate that to the population. Regardless, here is my question. ( My use of ""|"" ANYWHERE means OR ) $$D = D\_EitherUnlessInitialSpecificed = (D\_Initial | D_CounterpartToInitial)$$ $$B = B\_EitherUnlessInitialSpecificed = (B\_Initial | B\_CounterpartToInitial)$$ Yes, I wrote that redundantly, but hopefully clearly.I hope I wrote that correctly. Definition of counterparts: a , c , g , and t do not represent anything quantifiable.  They only represent a quality. As stated more generally above, and more specifically down below: $$D\; can\; only = a\;  or \; c\; ; \; a < -- > c$$ $$if\;  D = a,\; then \;D_c = c$$$$if\;  D = c,\; then \;D_c = a$$ $$B\; can\; only = t\;  or \; g\; ; \; t < -- > g$$ $$if\;  B = t,\; then \;B_c = g$$$$if\;  B = g,\; then \;D_c = t$$ In the end, everything comes down to the placement of $a$, $c$, $g$, and $t$ within constraints I'll define shortly. $$W = (D\;|B)$$ and we'll be using 2 instances of $W$, $W_1$ & $W_2$ which are not exclusive of each other in any context, nor are they exclusive/equal when the same instance is repeated in the same or a different set/array/etc. They are more like placeholders for a position which you're trying to find the counterpart to. If I use $W_2$ or $W_1$ and then use it again multiple times, each time it takes on a value which is assigned actually only by you. Now, $$W_1 = (D | B)$$$$W_2 = (D | B)$$ $$Wa\; =\; an\; instance\; of\; the  \;set \;or\; what\; I\; am\; going \;to \;begin \;calling\; an\;""array""\; \{W_1,W_2\}$$ We'll make 8 individual instances of these paired arrays and then store them into an array, $ArrayTop$ aka $ArrT$ $$ArrT\;=\;\{Wa_1,Wa_2,Wa_3,Wa_4,Wa_5,Wa_6,Wa_7,Wa_8\}$$ You can also reference the items in the array from their order via  $ArrT_1,ArrT_2...etc.$ if needed. Now, we'll make 16 more individual instances of these paired arrays and then store them into another array, $ArrayBottom$ aka $ArrB$. However, we'll start $Wa_x$ at 9. $$ArrB\;=\;\{Wa_9,Wa_{10},Wa_{11},Wa_{12},Wa_{13},Wa_{14},Wa_{15},Wa_{16},Wa_{17},Wa_{18},Wa_{19},Wa_{20},Wa_{21},Wa_{22},Wa_{23},Wa_{24}\}$$ or $$ArrB_1, ArrB_2...ArrB_{16}$$ If, given $ArrT$ & $ArrB$ and these constraints , $$Each$$ $$ a, c, g,\ and\ t$$ $$has \;a \;required \;""point\; usage"" \;of \;exactly \;16\; for\; W_1 \;collectively\; and\; W_2\; collectively.$$ Every time one of these 4 letters is used in: $ArrT$, it acquires 4 points. $ArrB1$ or $ArrB2$, then it acquires 3 points. $ArrB3$ through to $ArrB14$, then it acquires 2 points. $ArrB15$ or $ArrB16$, then it acquires 1 point. To clarify further, a letter acquires points for it's $W_1$ and separately for its $W_2$. Find the number of all possible scenarios for the placements of  $$a,t,g,\; and\; c$$ where, for each item in $ArrT$, two items in $ArrB$ contain the respective counterparts of $W_1$ and $W_2$ from that item in $ArrT = TRUE$. Also, the total # of possible scenarios if you want, although that's somewhat easy to figure out. EDIT: I have included a picture to provide the context for what this is for and where it comes from to hopefully help resolve any confusion. One very important thing I need to stress here and which can cause confusion with this picture is the fact that there are actually a few symmetries with very low probabilities to occur especially in tandem between the two halves (pending you're looking for them).  I will say that this is our genetic code for everything on earth...so that's kind of cool but the question I'm asking has absolutely nothing to do with symmetry and only the presence of things in the top group that then have their counterparts in the bottom regardless of their location.  If you're just looking at how I define the problem above in the mathematical sense you would already understand that though. You can basically ignore the upper left part too....the picture explains the rest.",,"['probability', 'probability-theory']"
52,Clarification on higher dimensional unit sphere/unit cube properties,Clarification on higher dimensional unit sphere/unit cube properties,,"I've been learning probability lately and I have come across this neat result (in Rick Durrett's book): Most of the volume of the unit cube in $\mathbb{R}^n$ comes from the set $A_{n,\epsilon} := \{x \in \mathbb{R}^n \, : \,(1-\epsilon)\sqrt{\frac{n}{3}} < |x| < (1+\epsilon)\sqrt{\frac{n}{3}} \},$ which is almost the sphere of radius $\frac{n}{3}.$ At the same time, we know the two basic facts that the volume of the unit ball in $\mathbb{R}^n$ goes to zero as $n$ grows and the volume of the unit cube remains the same, but most of the volume get concentrated in the corners of the cube. I realize that these two points are pretty different ideas altogether, but it seems strange to me that most of the volume in a high dimensional cube can be contained within a sphere and yet also be concentrated in the corners.  Could someone clarify what is really going on? Or perhaps my understanding is off.. I am not very experienced in geometry at all.  Thanks!","I've been learning probability lately and I have come across this neat result (in Rick Durrett's book): Most of the volume of the unit cube in $\mathbb{R}^n$ comes from the set $A_{n,\epsilon} := \{x \in \mathbb{R}^n \, : \,(1-\epsilon)\sqrt{\frac{n}{3}} < |x| < (1+\epsilon)\sqrt{\frac{n}{3}} \},$ which is almost the sphere of radius $\frac{n}{3}.$ At the same time, we know the two basic facts that the volume of the unit ball in $\mathbb{R}^n$ goes to zero as $n$ grows and the volume of the unit cube remains the same, but most of the volume get concentrated in the corners of the cube. I realize that these two points are pretty different ideas altogether, but it seems strange to me that most of the volume in a high dimensional cube can be contained within a sphere and yet also be concentrated in the corners.  Could someone clarify what is really going on? Or perhaps my understanding is off.. I am not very experienced in geometry at all.  Thanks!",,"['probability', 'geometry']"
53,How to pick an apartment -- a probability problem,How to pick an apartment -- a probability problem,,"Knowing a solution to the following well-defined problem could help many students and other low-wage workers feel more confident when selecting a home in a new town.  Its conditions are inspired by my current experience hunting for a room in Santa Cruz, CA.  I'd certainly apply the solution to my current search. The Problem: Andy is trying to find a room to rent in cool house.  Let's suppose there are (countably) infinitely many houses to pick from and Andy... visits one house each day. must, each day, decide to move in to that day's house, or abandon the option and keep looking. immediately knows how a house ranks relative to the other houses he's seen.  I.e. after any number of days, $k$, he can write down an ordered list $(h_1, h_2, ..., h_k)$ of the houses he's seen (ordered by his personal preference). has a constant probability, $p$, of being accepted a house once he asks to move in.  If accepted he keeps his word, if not, he keeps looking. has $N$ days to pick a house -- after $N$ days Andy must ask to move into every house he visits. Find a strategy that maximizes the expected percentile (among the infinitely many houses available) of the house Andy ends up in. Note, I've come across related problems phrased in terms of: * receiving an envelope containing an amount of money and deciding to keep the money, or go on to the next envelope. * walking down a street picking a (maybe Chinese) restaurant. If you know the common name for these types of problems, please comment.","Knowing a solution to the following well-defined problem could help many students and other low-wage workers feel more confident when selecting a home in a new town.  Its conditions are inspired by my current experience hunting for a room in Santa Cruz, CA.  I'd certainly apply the solution to my current search. The Problem: Andy is trying to find a room to rent in cool house.  Let's suppose there are (countably) infinitely many houses to pick from and Andy... visits one house each day. must, each day, decide to move in to that day's house, or abandon the option and keep looking. immediately knows how a house ranks relative to the other houses he's seen.  I.e. after any number of days, $k$, he can write down an ordered list $(h_1, h_2, ..., h_k)$ of the houses he's seen (ordered by his personal preference). has a constant probability, $p$, of being accepted a house once he asks to move in.  If accepted he keeps his word, if not, he keeps looking. has $N$ days to pick a house -- after $N$ days Andy must ask to move into every house he visits. Find a strategy that maximizes the expected percentile (among the infinitely many houses available) of the house Andy ends up in. Note, I've come across related problems phrased in terms of: * receiving an envelope containing an amount of money and deciding to keep the money, or go on to the next envelope. * walking down a street picking a (maybe Chinese) restaurant. If you know the common name for these types of problems, please comment.",,"['probability', 'bayesian', 'stopping-times']"
54,Probability of a survivor in chick-pecking tournament,Probability of a survivor in chick-pecking tournament,,"This is a follow up to this question : Suppose that $n$ chicks are arranged in a circle. Every chick randomly pecks either the chick to their right or the chick to their left. By the other question, the expected number of unpecked chicks is $n/4$. Instead of ending there, make a tournament out of it. Remove the pecked chicks from the circle and repeat the experiment with the remaining chicks. Iterate as long as possible. It is easy to see that the process ends with either $0$ or $1$ remaining chick. Question: Let $p(n)$ denote the probability that the process ends with $1$ chick. What can be said about $\lim_{n \rightarrow \infty}p(n)$? The following graph shows the result of Monte Carlo simulations which estimate $p(n)$ for all $n$ in the range $1$ to $1000$ (and using $1000$ tournaments for each $n$). The wave-like nature of the graph is interesting. To get a better handle on it, we need the exact probabilities. The following is based on a nice formula by @6005 in the comments to this answer to the other question: $p(0) = 0$ and $p(1) = 1$. For any $n \geq 2$ we have: $$p(n) = \begin{cases}          \sum_{k=0}^\frac{n}{2} \left(\frac{\binom{n}{2k} + (-1)^k \binom{n/2}{k}}{2^{n-1}}\right) p(k) & \text{if $n$ is even} \\          \sum_{k=0}^\frac{n-1}{2} \left(\frac{\binom{n}{2k}}{2^{n-1}}\right)p(k) & \text{otherwise}. \end{cases} $$ The following shows the graph of $p(n)$ from $n=1$ to $1000$: The local maxima and minima appear approximately at powers of $2$ (sometimes shifted by $1$). The maxima are at powers of $2$ which are also powers of $4$ and the minima at the other powers of $2$ (hence of the form $2 \cdot 4^k$). This is somewhat intuitive given that the expected number of survisors in a single round is $\frac{n}{4}$. Furthermore, this expected value is also the most likely value (in the case that $n$ is a multiple of $4$). For example if you start with $128$, the first round could be expected to get you to $32$, the next round to $8$, from thence to $2$, which prompty peck each other, leaving you with $0$. This is a hueristic way of reasoning that becomes somewhat less plausible with each factor of $4$. So the question: Does $\lim_{n\rightarrow \infty} p(n)$ exist, and, if so, to what? My conjecture is that the observed oscillations get damped in the limit, and that the resulting limit is $0.5$, but I do not know how to compute such limits.","This is a follow up to this question : Suppose that $n$ chicks are arranged in a circle. Every chick randomly pecks either the chick to their right or the chick to their left. By the other question, the expected number of unpecked chicks is $n/4$. Instead of ending there, make a tournament out of it. Remove the pecked chicks from the circle and repeat the experiment with the remaining chicks. Iterate as long as possible. It is easy to see that the process ends with either $0$ or $1$ remaining chick. Question: Let $p(n)$ denote the probability that the process ends with $1$ chick. What can be said about $\lim_{n \rightarrow \infty}p(n)$? The following graph shows the result of Monte Carlo simulations which estimate $p(n)$ for all $n$ in the range $1$ to $1000$ (and using $1000$ tournaments for each $n$). The wave-like nature of the graph is interesting. To get a better handle on it, we need the exact probabilities. The following is based on a nice formula by @6005 in the comments to this answer to the other question: $p(0) = 0$ and $p(1) = 1$. For any $n \geq 2$ we have: $$p(n) = \begin{cases}          \sum_{k=0}^\frac{n}{2} \left(\frac{\binom{n}{2k} + (-1)^k \binom{n/2}{k}}{2^{n-1}}\right) p(k) & \text{if $n$ is even} \\          \sum_{k=0}^\frac{n-1}{2} \left(\frac{\binom{n}{2k}}{2^{n-1}}\right)p(k) & \text{otherwise}. \end{cases} $$ The following shows the graph of $p(n)$ from $n=1$ to $1000$: The local maxima and minima appear approximately at powers of $2$ (sometimes shifted by $1$). The maxima are at powers of $2$ which are also powers of $4$ and the minima at the other powers of $2$ (hence of the form $2 \cdot 4^k$). This is somewhat intuitive given that the expected number of survisors in a single round is $\frac{n}{4}$. Furthermore, this expected value is also the most likely value (in the case that $n$ is a multiple of $4$). For example if you start with $128$, the first round could be expected to get you to $32$, the next round to $8$, from thence to $2$, which prompty peck each other, leaving you with $0$. This is a hueristic way of reasoning that becomes somewhat less plausible with each factor of $4$. So the question: Does $\lim_{n\rightarrow \infty} p(n)$ exist, and, if so, to what? My conjecture is that the observed oscillations get damped in the limit, and that the resulting limit is $0.5$, but I do not know how to compute such limits.",,['probability']
55,Why probability cannot be defined on the whole power set?,Why probability cannot be defined on the whole power set?,,"I'm studying probability. After dealing with discrete probability, my book states that we cannot define such function on an uncountable set, but we need to focus on a subset of it, thus introduces sigma algebras. But why do we need them in the first place? Why isn't the power set a sigma algebra itself? I cannot see where it does not follow the axioms. Can you list some examples please?","I'm studying probability. After dealing with discrete probability, my book states that we cannot define such function on an uncountable set, but we need to focus on a subset of it, thus introduces sigma algebras. But why do we need them in the first place? Why isn't the power set a sigma algebra itself? I cannot see where it does not follow the axioms. Can you list some examples please?",,"['probability', 'measure-theory', 'elementary-set-theory']"
56,How to determine the weights in the asymmetric Lovasz Local Lemma,How to determine the weights in the asymmetric Lovasz Local Lemma,,"The application of the asymmetric Lovasz Local Lemma requires finding a weight function $x$ on the bad events satisfying the property in the link.  Often, one uses a constant weight function, giving rise to symmetric LLL. Suppose this fails.  Is there a systematic way to search for a better choice of $x$?","The application of the asymmetric Lovasz Local Lemma requires finding a weight function $x$ on the bad events satisfying the property in the link.  Often, one uses a constant weight function, giving rise to symmetric LLL. Suppose this fails.  Is there a systematic way to search for a better choice of $x$?",,"['probability', 'combinatorics', 'discrete-mathematics', 'probabilistic-method']"
57,Sample space for identical objects,Sample space for identical objects,,"Sample space for a set of balls in an urn out of which say $p$ are of blue color (hence identical), $q$ are of red color is written as a multiset and not as a set, or rather when it is written as a set the identical objects are written as different objects in the following way:- $\text{Set representation} - \{b_1,b_2,\ldots,b_p,r_1, r_2,\ldots,r_q\}\\ \text{Multiset representation}-\{b,b,\ldots\text{p times}, r,r,\ldots\text{q times}\}$ And based on the above sample space if we are asked to find the probability of selecting a blue ball we get it as $\dfrac{p}{p+q}$, which was found using the classical definition of probability . According to the classical definition of probability, probability of an event $E$ as given as  $$P(E)=\dfrac{n(E)}{n(S)}$$ Now why is that the ways of selecting $r$(here, $r=1$) things out of $n$ identical things is ${n}\choose{r}$ and not $1$ as is the case in combinatorics . Edit 1:- To further clarify my point consider these two contrasting cases below:- Case 1:- Consider an urn in which there are $30$ distinct balls out of which there are $10$ distinct blue balls and $20$ distinct white balls. Let's say the distinction b/w the balls of identical color is made by marking them with numbers which cant be felt by the one drawing the ball. Now in a random draw if we are asked the probability of picking a blue ball, then according to the classical definition of probability it will be $\dfrac{10}{30}=\dfrac{1}{3}$. We had arrived the above probability because we know that  $$n(E:\text{drawing a blue ball})={{10}\choose{1}}=10\\ n(S:\text{drawing a ball from the earn})={{30}\choose{1}}=30$$ Just listing the sets $E$(event space) and $S$(sample space) for comparing these with the next case. $E=\{B_1,B_2,B_3,\ldots,B_{10}\}\\ S=\{B_1,B_2,B_3,\ldots,B_{10},W_1,W_2,W_3,\ldots, W_{20}\}$ Case 2:- The  setting for this case is the same as the previous case the only difference is that that the balls of same color are indistinguishable. So, if we are asked the probability of drawing a blue ball the probability would come out as $\dfrac{1}{2}$. My thinking behind this probability is that the sample space for this random draw would be $$S=\{W,B\}$$ as all the white balls are identical to each other and so are all the blue balls. Similarly, the event space would be $$E=\{B\}$$ as all the blue balls are identical. But by this logic the probability of drawing a blue ball would still be $\dfrac{1}{2}$, when the blue balls are present in a vast quantity as compared to the white balls which would be other than the expected result. So, my question again boils down to the same one as in the pre edit part of the post, i.e. Why do we use the multi-set representation for identical objects or while writing it in set notation why do we consider the identical objects as distinct objects?","Sample space for a set of balls in an urn out of which say $p$ are of blue color (hence identical), $q$ are of red color is written as a multiset and not as a set, or rather when it is written as a set the identical objects are written as different objects in the following way:- $\text{Set representation} - \{b_1,b_2,\ldots,b_p,r_1, r_2,\ldots,r_q\}\\ \text{Multiset representation}-\{b,b,\ldots\text{p times}, r,r,\ldots\text{q times}\}$ And based on the above sample space if we are asked to find the probability of selecting a blue ball we get it as $\dfrac{p}{p+q}$, which was found using the classical definition of probability . According to the classical definition of probability, probability of an event $E$ as given as  $$P(E)=\dfrac{n(E)}{n(S)}$$ Now why is that the ways of selecting $r$(here, $r=1$) things out of $n$ identical things is ${n}\choose{r}$ and not $1$ as is the case in combinatorics . Edit 1:- To further clarify my point consider these two contrasting cases below:- Case 1:- Consider an urn in which there are $30$ distinct balls out of which there are $10$ distinct blue balls and $20$ distinct white balls. Let's say the distinction b/w the balls of identical color is made by marking them with numbers which cant be felt by the one drawing the ball. Now in a random draw if we are asked the probability of picking a blue ball, then according to the classical definition of probability it will be $\dfrac{10}{30}=\dfrac{1}{3}$. We had arrived the above probability because we know that  $$n(E:\text{drawing a blue ball})={{10}\choose{1}}=10\\ n(S:\text{drawing a ball from the earn})={{30}\choose{1}}=30$$ Just listing the sets $E$(event space) and $S$(sample space) for comparing these with the next case. $E=\{B_1,B_2,B_3,\ldots,B_{10}\}\\ S=\{B_1,B_2,B_3,\ldots,B_{10},W_1,W_2,W_3,\ldots, W_{20}\}$ Case 2:- The  setting for this case is the same as the previous case the only difference is that that the balls of same color are indistinguishable. So, if we are asked the probability of drawing a blue ball the probability would come out as $\dfrac{1}{2}$. My thinking behind this probability is that the sample space for this random draw would be $$S=\{W,B\}$$ as all the white balls are identical to each other and so are all the blue balls. Similarly, the event space would be $$E=\{B\}$$ as all the blue balls are identical. But by this logic the probability of drawing a blue ball would still be $\dfrac{1}{2}$, when the blue balls are present in a vast quantity as compared to the white balls which would be other than the expected result. So, my question again boils down to the same one as in the pre edit part of the post, i.e. Why do we use the multi-set representation for identical objects or while writing it in set notation why do we consider the identical objects as distinct objects?",,"['probability', 'discrete-mathematics']"
58,The motivating properites of beta distribution and how its density function is developed?,The motivating properites of beta distribution and how its density function is developed?,,"The density of beta distribution is given by the following $$f(x\mid \alpha ,\beta ) = \frac 1 {\operatorname{B}(\alpha,\beta)} x^{\alpha  - 1} (1 - x)^{\beta  - 1}$$ where $$ \operatorname{B}(\alpha,\beta) = \int_0^1 x^{\alpha  - 1} (1 - x)^{\beta  - 1} \,dx $$ is the Beta function as the normalizing constant. I understand one use of Beta distribution is to draw random probabilities from it. I am interested in what gives rise to this distribution? In particular , why it is ""$x^{\alpha  - 1} (1 - x)^{\beta  - 1}$"", not something else? What kinds of desired properties uniquely determine this density? And how the density formula is derived? I have googled but could not find a good explanation. Many text books just throw this in front of you like it comes from nowhere.","The density of beta distribution is given by the following $$f(x\mid \alpha ,\beta ) = \frac 1 {\operatorname{B}(\alpha,\beta)} x^{\alpha  - 1} (1 - x)^{\beta  - 1}$$ where $$ \operatorname{B}(\alpha,\beta) = \int_0^1 x^{\alpha  - 1} (1 - x)^{\beta  - 1} \,dx $$ is the Beta function as the normalizing constant. I understand one use of Beta distribution is to draw random probabilities from it. I am interested in what gives rise to this distribution? In particular , why it is ""$x^{\alpha  - 1} (1 - x)^{\beta  - 1}$"", not something else? What kinds of desired properties uniquely determine this density? And how the density formula is derived? I have googled but could not find a good explanation. Many text books just throw this in front of you like it comes from nowhere.",,['probability']
59,Asymptotic probability that two binomial variables are equal,Asymptotic probability that two binomial variables are equal,,"$X_1,\ldots,X_k$ are independent random variables distributed like $\text{Binomial}[n,p]$. What is the probability that they are all equal, as a function of $k$ $p$ and $n$, when $n$ is very large? Currently I have two solutions. Solution A is general but quite informal. When $n$ is large, a Binomial random variable behaves like a normal random variable, and most of its probability mass is concentrated in the interval $\mu \pm \sigma$, where $\mu$ is the mean value and $\sigma$ is the standard deviation. So we can approximate the variables as being drawn independently at random from a set with $2\sigma$ elements. After the first variable is determined, each of the remaining $k-1$ variables has probability $\approx 1/(2\sigma)$ to have the same value, so the probability that all are equal is $\approx 1/(2\sigma)^{k-1}$.  Here $\sigma=\sqrt{p (1-p) n}$, so the probability is: $${1 \over (4 p (1-p) n)^{(k-1)/2}} $$ Solution B is more formal but works only for $p=1/2$ and $k=2$ variables. Define $Y_2 := n - X_2$. So the event $X_1=X_2$ is identical to the event $X_1+Y_2 = n$. $Y_2$ is distributed like $\text{Binomial}[n,1-p]$, but here $p=1-p$ so it is distributed like $X_1$. Therefore $X_1+Y_2$ is distributed like $\text{Binomial}[2n,1/2]$ and the probability that it equals $n$ is just:  ${2n\choose n}2^{-2n}$. By Stirling's approximation , the binomial coefficient is $\approx {4^n \over \sqrt{\pi n}}$, so the probability that $X_1=X_2$ is approximately: $$ {1\over \sqrt{\pi n}}$$ This is a constant times the outcome of Solution A, which means that both solutions are at least asymptotically correct. What is a solution that works for every $p$ and $k$? EDIT: See also: What is the probability that two univariate Gaussian random variables are equal?","$X_1,\ldots,X_k$ are independent random variables distributed like $\text{Binomial}[n,p]$. What is the probability that they are all equal, as a function of $k$ $p$ and $n$, when $n$ is very large? Currently I have two solutions. Solution A is general but quite informal. When $n$ is large, a Binomial random variable behaves like a normal random variable, and most of its probability mass is concentrated in the interval $\mu \pm \sigma$, where $\mu$ is the mean value and $\sigma$ is the standard deviation. So we can approximate the variables as being drawn independently at random from a set with $2\sigma$ elements. After the first variable is determined, each of the remaining $k-1$ variables has probability $\approx 1/(2\sigma)$ to have the same value, so the probability that all are equal is $\approx 1/(2\sigma)^{k-1}$.  Here $\sigma=\sqrt{p (1-p) n}$, so the probability is: $${1 \over (4 p (1-p) n)^{(k-1)/2}} $$ Solution B is more formal but works only for $p=1/2$ and $k=2$ variables. Define $Y_2 := n - X_2$. So the event $X_1=X_2$ is identical to the event $X_1+Y_2 = n$. $Y_2$ is distributed like $\text{Binomial}[n,1-p]$, but here $p=1-p$ so it is distributed like $X_1$. Therefore $X_1+Y_2$ is distributed like $\text{Binomial}[2n,1/2]$ and the probability that it equals $n$ is just:  ${2n\choose n}2^{-2n}$. By Stirling's approximation , the binomial coefficient is $\approx {4^n \over \sqrt{\pi n}}$, so the probability that $X_1=X_2$ is approximately: $$ {1\over \sqrt{\pi n}}$$ This is a constant times the outcome of Solution A, which means that both solutions are at least asymptotically correct. What is a solution that works for every $p$ and $k$? EDIT: See also: What is the probability that two univariate Gaussian random variables are equal?",,['probability']
60,Coupon collector without replacement,Coupon collector without replacement,,"Given a urn containing balls of $k$ distinct color such that $i$th color has $c_i$ number of balls. How many balls in expectation one needs to sample in order to to get at least one ball of each color. Problem looks very similar to coupon collector(each color being coupon) but without replacement. I am able to solve 2 color case. For example if urn contain r red balls and b blue balls then: Let S be the event that first ball is red and we have  $$E[X]=P(S)E[X|S]+P(\bar S)E[X|\bar S]$$ Now $E[X|S]=1+(\frac{r-1}{b+1}+1)$, where $\frac{r-1}{b+1}$ is the expected number of red ball one needs to sample to get blue ball from bag that contatin $r-1$ red balls and $b$ blue balls. Similarly $E[X|\bar S]=1+(\frac{b-1}{r+1}+1)$, using these we can get $E[X]$. But I don't know how to generalize this to k color case. Any hints?","Given a urn containing balls of $k$ distinct color such that $i$th color has $c_i$ number of balls. How many balls in expectation one needs to sample in order to to get at least one ball of each color. Problem looks very similar to coupon collector(each color being coupon) but without replacement. I am able to solve 2 color case. For example if urn contain r red balls and b blue balls then: Let S be the event that first ball is red and we have  $$E[X]=P(S)E[X|S]+P(\bar S)E[X|\bar S]$$ Now $E[X|S]=1+(\frac{r-1}{b+1}+1)$, where $\frac{r-1}{b+1}$ is the expected number of red ball one needs to sample to get blue ball from bag that contatin $r-1$ red balls and $b$ blue balls. Similarly $E[X|\bar S]=1+(\frac{b-1}{r+1}+1)$, using these we can get $E[X]$. But I don't know how to generalize this to k color case. Any hints?",,"['probability', 'combinatorics', 'discrete-mathematics', 'coupon-collector']"
61,Kernel vs Distribution?,Kernel vs Distribution?,,"I see the terms kernel and distribution used - what I presume to be - interchangeably all the time and hence my understanding is that they are the same e.g. within a publication the phrase ""a gaussian kernel"" and ""a gaussian distribution"" appears - to me - synonymous. However, it is possible, as can be the case, that some nuance is missing from this comparison. In both Meaning of ""kernel"" and What does kernel mean no definite answer is given. What are the most overloaded words in mathematics highlights that often terms in mathematics may have non-unique meanings - especially across fields. So is there a formal or otherwise distinction between a kernel and a distribution ? Or is a kernel just any symmetric function that integrates to 1? i.e. $$K(-u) = K(u)$$ and $$\int\limits_{-\infty}^\infty K(u)\mathbb{d}u=1$$ Notably if $K(-u) = K(u)$ then any tailed distribution (e.g. Weibull, Gamma, etc) is not a kernel ? Further befuddlement stems from articles like this , stating that: kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. Such phrasing is, again, symmetric and - to me - implies that if a kernel estimation estimates a probability function, then a tried-and-true kernel is a probability function. If kernels are not probability-distributions , what is a good / accessible resource to clarify my confusion?","I see the terms kernel and distribution used - what I presume to be - interchangeably all the time and hence my understanding is that they are the same e.g. within a publication the phrase ""a gaussian kernel"" and ""a gaussian distribution"" appears - to me - synonymous. However, it is possible, as can be the case, that some nuance is missing from this comparison. In both Meaning of ""kernel"" and What does kernel mean no definite answer is given. What are the most overloaded words in mathematics highlights that often terms in mathematics may have non-unique meanings - especially across fields. So is there a formal or otherwise distinction between a kernel and a distribution ? Or is a kernel just any symmetric function that integrates to 1? i.e. $$K(-u) = K(u)$$ and $$\int\limits_{-\infty}^\infty K(u)\mathbb{d}u=1$$ Notably if $K(-u) = K(u)$ then any tailed distribution (e.g. Weibull, Gamma, etc) is not a kernel ? Further befuddlement stems from articles like this , stating that: kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. Such phrasing is, again, symmetric and - to me - implies that if a kernel estimation estimates a probability function, then a tried-and-true kernel is a probability function. If kernels are not probability-distributions , what is a good / accessible resource to clarify my confusion?",,"['probability', 'statistics', 'probability-distributions', 'harmonic-functions']"
62,Expectation of quartic form for multivariate Gaussian,Expectation of quartic form for multivariate Gaussian,,"I am reading the 2012 version The Matrix Cookbook . On Page 43, Section 8.2.4, “Mean of Quartic Forms”: there is a formula that really confuses me: $$E[x^TAxx^TBx]=Tr[A\Sigma(B+B^T)\Sigma]+m^T(A+A^T)\Sigma(B+B^T)m+(Tr(A\Sigma)+m^TAm)(Tr(B\Sigma)+m^TBm)$$ I do not check formulae of $E[xx^Txx^T]$ , $E[xx^TAxx^T]$ , or $E[x^Txx^Tx]$ in Cookbook, as they are the special cases of this one. Note that $x\sim N(m,\Sigma)$ . My question is how to prove it? The following is my attempt: Suppose consider the simpler situation: a quadratic form with $A$ , $B$ symmetric. I use the same technique appeared on Page 11 of Seber and Lee's book ""Linear Regression Analysis"" to re-organize these two quadratic forms, then $E[x^TAxx^TBx]=E\Big\{[(x-m)^TA(x-m)+2m^TA(x-m)+m^TAm)] *[(x-m)^TB(x-m)+2m^TB(x-m)+m^TBm)]\Big\}$ $=E\Big\{(x-m)^TA(x-m)(x-m)^TB(x-m)\Big\}+E\Big\{(x-m)^TA(x-m)m^TBm\Big\}+E\Big\{m^TAm(x-m)^TB(x-m)\Big\}+E\Big\{m^TAmm^TBm\Big\}+2E\Big\{m^TA(x-m)m^TBm\Big\}+2E\Big\{m^TAmm^TB(x-m)\Big\}+2E\Big\{(x-m)^TA(x-m)m^TB(x-m)\Big\}+2E\Big\{m^TA(x-m)(x-m)^TB(x-m)\Big\}+4E\Big\{m^TA(x-m)m^TB(x-m)\Big\}$ in which the fifth and sixth items are zeros and the first four items can be further rearranged such that they equate $(Tr(A\Sigma)+m^TAm)(Tr(B\Sigma)+m^TBm)$ by the fact $E\Big\{(x-m)^TQ(x-m)\Big\}=Tr(Q\Sigma)$ The last item can be reduced if we notice $(scalar)^T=scalar$ : $E\Big\{m^TA(x-m)\cdot m^TB(x-m)\Big\}=E\Big\{m^TA(x-m)\cdot (x-m)^TBm\Big\}=m^TAE\Big\{(x-m)\cdot (x-m)^T\Big\}Bm=m^TA\Sigma Bm$ Consequently, ( This is my detailed question, how to show it? ) it suffices to show that the seventh and eighth items are equal to the first item in RHS of the initial formula, that is, $2E\Big\{(x-m)^TA(x-m)m^TB(x-m)\Big\}+2E\Big\{m^TA(x-m)(x-m)^TB(x-m)\Big\}=Tr[A\Sigma(B+B^T)\Sigma]=2Tr(A\Sigma B\Sigma)$ I searched it in this website, the link is useful: Expected value using the Kronecker Product The RHS of my above equation, by the formula $Tr(ABCD)=vec(B^T)^T(A^T\otimes C)vec(D)$ can be re-written as: $Tr(A\Sigma B\Sigma)=vec(\Sigma)^T(A\otimes B)vec(\Sigma)$ The LHS, by the link, by the fact $x\otimes y=vec(yx^T)$ for two column vectors $x$ and $y$ , and by $scalar=Tr(scalar)$ , are equivalent to: $E\Big\{(x-m)^TA(x-m)\cdot m^TB(x-m)\Big\}+E\Big\{m^TA(x-m)\cdot (x-m)^TB(x-m)\Big\}=E\Big\{\Big((x-m)^TA(x-m)\Big)\otimes\Big(m^TB(x-m)\Big)\Big\}+E\Big\{\Big(m^TA(x-m)\Big)\otimes\Big((x-m)^TB(x-m)\Big)\Big\}=E\Big\{\Big((x-m)^T\otimes m^T\Big)\Big(A\otimes B\Big)\Big((x-m)\otimes(x-m)\Big)\Big\}+E\Big\{\Big(m^T\otimes (x-m)^T\Big)\Big(A\otimes B\Big)\Big((x-m)\otimes(x-m)\Big)\Big\}=E\Big\{vec\Big((x-m)m^T+m(x-m)^T\Big)^T\Big(A\otimes B\Big)vec\Big((x-m)(x-m)^T\Big)\Big\}$ In normal distribution assumption, $vec(E[(x-m)(x-m)^T])=vec(\Sigma)$ . Therefore it seems to be really near the final equating relation (it ought to be): $vec(\Sigma)^T(A\otimes B)vec(\Sigma)=E\Big\{vec\Big((x-m)m^T+m(x-m)^T\Big)^T\Big(A\otimes B\Big)vec\Big((x-m)(x-m)^T\Big)\Big\}$ Who can help me prove the above equation? Or are there any mistakes in derivations before? ============================= Here is my second trial: Inspired by Seber and Lee's book again, on the same page 11, who can help me expand $E[x^TAxx^TBx]$ into such coordinate forms to get same solution? though I estimate the working time is huge. A natural question is, based on my first attempt, how to coordinately expand $2E\Big\{(x-m)^TA(x-m)m^TB(x-m)\Big\}+2E\Big\{m^TA(x-m)(x-m)^TB(x-m)\Big\}=Tr[A\Sigma(B+B^T)\Sigma]=2Tr(A\Sigma B\Sigma)$ from one side then simplified to the other side? It's relatively easy than expanding $E[x^TAxx^TBx]$ for sure, but how to expand $Tr(A\Sigma B\Sigma)$ ? There are 4 matrices rather than vectors. ============================== My last question : are there any books which not only collect matrix formulae but provide proofs and solutions as well? I need your recommendation! Honestly thank you in advance!","I am reading the 2012 version The Matrix Cookbook . On Page 43, Section 8.2.4, “Mean of Quartic Forms”: there is a formula that really confuses me: I do not check formulae of , , or in Cookbook, as they are the special cases of this one. Note that . My question is how to prove it? The following is my attempt: Suppose consider the simpler situation: a quadratic form with , symmetric. I use the same technique appeared on Page 11 of Seber and Lee's book ""Linear Regression Analysis"" to re-organize these two quadratic forms, then in which the fifth and sixth items are zeros and the first four items can be further rearranged such that they equate by the fact The last item can be reduced if we notice : Consequently, ( This is my detailed question, how to show it? ) it suffices to show that the seventh and eighth items are equal to the first item in RHS of the initial formula, that is, I searched it in this website, the link is useful: Expected value using the Kronecker Product The RHS of my above equation, by the formula can be re-written as: The LHS, by the link, by the fact for two column vectors and , and by , are equivalent to: In normal distribution assumption, . Therefore it seems to be really near the final equating relation (it ought to be): Who can help me prove the above equation? Or are there any mistakes in derivations before? ============================= Here is my second trial: Inspired by Seber and Lee's book again, on the same page 11, who can help me expand into such coordinate forms to get same solution? though I estimate the working time is huge. A natural question is, based on my first attempt, how to coordinately expand from one side then simplified to the other side? It's relatively easy than expanding for sure, but how to expand ? There are 4 matrices rather than vectors. ============================== My last question : are there any books which not only collect matrix formulae but provide proofs and solutions as well? I need your recommendation! Honestly thank you in advance!","E[x^TAxx^TBx]=Tr[A\Sigma(B+B^T)\Sigma]+m^T(A+A^T)\Sigma(B+B^T)m+(Tr(A\Sigma)+m^TAm)(Tr(B\Sigma)+m^TBm) E[xx^Txx^T] E[xx^TAxx^T] E[x^Txx^Tx] x\sim N(m,\Sigma) A B E[x^TAxx^TBx]=E\Big\{[(x-m)^TA(x-m)+2m^TA(x-m)+m^TAm)]
*[(x-m)^TB(x-m)+2m^TB(x-m)+m^TBm)]\Big\} =E\Big\{(x-m)^TA(x-m)(x-m)^TB(x-m)\Big\}+E\Big\{(x-m)^TA(x-m)m^TBm\Big\}+E\Big\{m^TAm(x-m)^TB(x-m)\Big\}+E\Big\{m^TAmm^TBm\Big\}+2E\Big\{m^TA(x-m)m^TBm\Big\}+2E\Big\{m^TAmm^TB(x-m)\Big\}+2E\Big\{(x-m)^TA(x-m)m^TB(x-m)\Big\}+2E\Big\{m^TA(x-m)(x-m)^TB(x-m)\Big\}+4E\Big\{m^TA(x-m)m^TB(x-m)\Big\} (Tr(A\Sigma)+m^TAm)(Tr(B\Sigma)+m^TBm) E\Big\{(x-m)^TQ(x-m)\Big\}=Tr(Q\Sigma) (scalar)^T=scalar E\Big\{m^TA(x-m)\cdot m^TB(x-m)\Big\}=E\Big\{m^TA(x-m)\cdot (x-m)^TBm\Big\}=m^TAE\Big\{(x-m)\cdot (x-m)^T\Big\}Bm=m^TA\Sigma Bm 2E\Big\{(x-m)^TA(x-m)m^TB(x-m)\Big\}+2E\Big\{m^TA(x-m)(x-m)^TB(x-m)\Big\}=Tr[A\Sigma(B+B^T)\Sigma]=2Tr(A\Sigma B\Sigma) Tr(ABCD)=vec(B^T)^T(A^T\otimes C)vec(D) Tr(A\Sigma B\Sigma)=vec(\Sigma)^T(A\otimes B)vec(\Sigma) x\otimes y=vec(yx^T) x y scalar=Tr(scalar) E\Big\{(x-m)^TA(x-m)\cdot m^TB(x-m)\Big\}+E\Big\{m^TA(x-m)\cdot (x-m)^TB(x-m)\Big\}=E\Big\{\Big((x-m)^TA(x-m)\Big)\otimes\Big(m^TB(x-m)\Big)\Big\}+E\Big\{\Big(m^TA(x-m)\Big)\otimes\Big((x-m)^TB(x-m)\Big)\Big\}=E\Big\{\Big((x-m)^T\otimes m^T\Big)\Big(A\otimes B\Big)\Big((x-m)\otimes(x-m)\Big)\Big\}+E\Big\{\Big(m^T\otimes (x-m)^T\Big)\Big(A\otimes B\Big)\Big((x-m)\otimes(x-m)\Big)\Big\}=E\Big\{vec\Big((x-m)m^T+m(x-m)^T\Big)^T\Big(A\otimes B\Big)vec\Big((x-m)(x-m)^T\Big)\Big\} vec(E[(x-m)(x-m)^T])=vec(\Sigma) vec(\Sigma)^T(A\otimes B)vec(\Sigma)=E\Big\{vec\Big((x-m)m^T+m(x-m)^T\Big)^T\Big(A\otimes B\Big)vec\Big((x-m)(x-m)^T\Big)\Big\} E[x^TAxx^TBx] 2E\Big\{(x-m)^TA(x-m)m^TB(x-m)\Big\}+2E\Big\{m^TA(x-m)(x-m)^TB(x-m)\Big\}=Tr[A\Sigma(B+B^T)\Sigma]=2Tr(A\Sigma B\Sigma) E[x^TAxx^TBx] Tr(A\Sigma B\Sigma)","['probability', 'matrices', 'statistics', 'normal-distribution']"
63,Negative Hypergeometric Distribution expectation,Negative Hypergeometric Distribution expectation,,"I am reading Introduction to Probability by Blitzstein and Hwang - Expectation. The book states : An urn contains $w$ white balls and $b$ black balls, which are randomly drawn one by one without replacement. The number of black balls drawn before drawing any white balls has a negative hypergeometric distribution. For example, if we shuffle a deck of cards and deal them one at a time, the number of cards dealt before uncovering the first ace is a negative hypergeometric with $w=4,b=48$ . Finding the expected value of a negative hypergeometric r.v. directly from the definition results in very complicated sums of products. But the answer is very simple-looking: $b/(w+1)$ . Let us prove this using indicator r.v.s. Label the black balls as $1,2,3,\ldots,b$ and let $I_{j}$ be the indicator of the black ball $j$ being drawn before any white balls have been drawn. Then, $P(I_{j}=1)=1/(w+1)$ , since listing out the order in which black ball $j$ and the white balls are drawn (ignoring the other balls), all orders are equally likely by symmetry, and $I_{j}=1$ is equivalent to black balls $j$ being first in this list. I know that, when sampling without replacement, the number of failures(drawing a black ball) until the first success(drawing a white ball) is a negative hypergeometric r.v. But, why is $P(I_{j}=1)=1/(w+1)$ ? And why do we ignore the other black balls?","I am reading Introduction to Probability by Blitzstein and Hwang - Expectation. The book states : An urn contains white balls and black balls, which are randomly drawn one by one without replacement. The number of black balls drawn before drawing any white balls has a negative hypergeometric distribution. For example, if we shuffle a deck of cards and deal them one at a time, the number of cards dealt before uncovering the first ace is a negative hypergeometric with . Finding the expected value of a negative hypergeometric r.v. directly from the definition results in very complicated sums of products. But the answer is very simple-looking: . Let us prove this using indicator r.v.s. Label the black balls as and let be the indicator of the black ball being drawn before any white balls have been drawn. Then, , since listing out the order in which black ball and the white balls are drawn (ignoring the other balls), all orders are equally likely by symmetry, and is equivalent to black balls being first in this list. I know that, when sampling without replacement, the number of failures(drawing a black ball) until the first success(drawing a white ball) is a negative hypergeometric r.v. But, why is ? And why do we ignore the other black balls?","w b w=4,b=48 b/(w+1) 1,2,3,\ldots,b I_{j} j P(I_{j}=1)=1/(w+1) j I_{j}=1 j P(I_{j}=1)=1/(w+1)","['probability', 'probability-distributions', 'polya-urn-model']"
64,The Three-Cornered Duel,The Three-Cornered Duel,,"I am analyzing the following problem from the the book ""Fifty Challenging Problems in Probability with Solution"" by Frederick Mosteller. It seems to me that the solution to the The Three-Cornered Duel Problem presented in the book is incomplete. Can anybody confirm if my calculation is ok? The problem A, B and C are to fight a three-cornered pistol duel. All know that   A's chance of hitting his target is 0.3, C's is 0.5, and B never   misses. They are to fire at their choice of target in succession in   the order A, B, C, cyclically (but a hit man loses further turns and   is no longer shot at) until only one man is left unit. What should A's   strategy be? Mosteller's solution: A is naturally not feeling cheery about this enterprise. Having the   first shot he sees that, if he hits C, B will then surely hit him, and   so he is not going to shoot at C. If he shoots at B and misses him,   then B clearly shoots the more dangerous C first, and A gets one shot   at B with probability 0.3 of succeeding. If he misses this time, the   less said the better. On the other hand, suppose A hits B.  Then C and   A shoot alternately until one hits. A's chance of winning is   $$(.5)(.3)+(.5)^2(.7)(.3)+(.5)^3(.7)^2(.3)+…$$   Each term corresponds to a sequence of misses by both C and A ending with a final hit by A. Summing the geometric series we get   $$(.5)(.3)+\{1+(.5)(.7)+[(.5)(.7)]^2+… \}= \frac{(.5)(.3)}{1-(.5)(.7)}= \frac{0.15}{0.65}= \frac{3}{13} < \frac{3}{10}$$    Thus hitting B and finishing off with C has less   probability of winning for A than just missing the first shot. So A   fires his first shot into the ground and then tries to hit B with his   next shot. C is out of luck. My calculation case 1 $$P(A survives \ in \ case \ A \ shots \  B )= \mathbf{0.3} [(.5)(.3)+(.5)^2(.7)(.3)+(.5)^3(.7)^2(.3)+…] = \mathbf{0.3} \frac{(.5)(.3)}{1-(.5)(.7)}= \frac{\mathbf{0.3}(0.15)}{0.65}=0.069$$ case 2 $$P(A \ survives \ in \ case \ A \ misses \ the \ shot \ at \  B )= (0.7) \ 1 \ (0.3) = 0.21$$ Probability of survival is higher in case 2 therefore A should miss the first shot at B. Is my calculation correct?","I am analyzing the following problem from the the book ""Fifty Challenging Problems in Probability with Solution"" by Frederick Mosteller. It seems to me that the solution to the The Three-Cornered Duel Problem presented in the book is incomplete. Can anybody confirm if my calculation is ok? The problem A, B and C are to fight a three-cornered pistol duel. All know that   A's chance of hitting his target is 0.3, C's is 0.5, and B never   misses. They are to fire at their choice of target in succession in   the order A, B, C, cyclically (but a hit man loses further turns and   is no longer shot at) until only one man is left unit. What should A's   strategy be? Mosteller's solution: A is naturally not feeling cheery about this enterprise. Having the   first shot he sees that, if he hits C, B will then surely hit him, and   so he is not going to shoot at C. If he shoots at B and misses him,   then B clearly shoots the more dangerous C first, and A gets one shot   at B with probability 0.3 of succeeding. If he misses this time, the   less said the better. On the other hand, suppose A hits B.  Then C and   A shoot alternately until one hits. A's chance of winning is   $$(.5)(.3)+(.5)^2(.7)(.3)+(.5)^3(.7)^2(.3)+…$$   Each term corresponds to a sequence of misses by both C and A ending with a final hit by A. Summing the geometric series we get   $$(.5)(.3)+\{1+(.5)(.7)+[(.5)(.7)]^2+… \}= \frac{(.5)(.3)}{1-(.5)(.7)}= \frac{0.15}{0.65}= \frac{3}{13} < \frac{3}{10}$$    Thus hitting B and finishing off with C has less   probability of winning for A than just missing the first shot. So A   fires his first shot into the ground and then tries to hit B with his   next shot. C is out of luck. My calculation case 1 $$P(A survives \ in \ case \ A \ shots \  B )= \mathbf{0.3} [(.5)(.3)+(.5)^2(.7)(.3)+(.5)^3(.7)^2(.3)+…] = \mathbf{0.3} \frac{(.5)(.3)}{1-(.5)(.7)}= \frac{\mathbf{0.3}(0.15)}{0.65}=0.069$$ case 2 $$P(A \ survives \ in \ case \ A \ misses \ the \ shot \ at \  B )= (0.7) \ 1 \ (0.3) = 0.21$$ Probability of survival is higher in case 2 therefore A should miss the first shot at B. Is my calculation correct?",,"['probability', 'game-theory']"
65,What fraction of the fund should one bet?,What fraction of the fund should one bet?,,"Say we have a gambler who makes money through sports betting. My aim is to develop a model to help our gambler maximise his winnings and minimize losses. In my model, rather than betting a fixed amount of money, the gambler bets a certain fraction $0 < r < 1$ of his current betting fund. He continues betting that fraction as his betting fund increases or decreases until he cashes out after a certain number of sessions $n$. The gambler's initial fund shall be $F_0$. His fund after $i$ sessions shall be $F_i$. His probability of making a correct prediction shall be $0 < p < 1$. If our gambler had a $p$ of $0$ or $1$, then the entire model would be useless. The average odds with which our gambler deals with is $a > 1$. The gambler's minimum desired profit upon cash out is $T$. $$T \le F_n - F_0 \tag{1}$$ If we expressed everything as a multiple of $F_0$, $(1)$ can be rewritten as: $$T \le F_n - 1 \tag{1.1}$$ It follows that the following are known: $T$, $a$, $F_0$, $p$. Should our gambler lose a particular session say $i+1$, $$F_{i+1} = (1-r)F_i \tag{2.1}$$ Should he win that particular session $$F_{i+1} = F_i(1-r + ra) \tag{2.2}$$ Given that the gambler plays $n$ sessioms before cashing out. His expected number of wins = $p*n$        $(3.1)$ His expected number of losses = $(1-p)*n$         $(3.2)$ Now there are many different ways to distribute the gambler's losses and wins{$n \Bbb P pn$} and while calculating all scenarios and finding average $F_n$ may be ideal, it is computationally very expensive. So I decided to  model the problem assuming the losses take place in the worst way possible( back to back at the very beginning of the match). The gambler's revenue after $n$ matches is given by the formula: $F_n = (1-r)^{(1-p)n}\{(1-r)+ra\}^{pn}$             $(4)$ Now we know that our gambler wants to make a minimum profit of $T$ so we transform $(4)$ into an inequality using $(1.1)$ We get: $(1-r)^{(1-p)n}\{(1-r)+ra\}^{pn}$ $ \ge T + 1$         $(4.1)$ Taking the Natural logarithm of both sides, I get: $ln(1-r)*(1-p)(n) + ln(1-r + ra)*pn \ge ln(T+1)$           $(4.2)$ $n\{ln(1-r)(1-p) + ln(r(a-1)+1)(p) \} \ge ln(T+1)$    $(4.3)$ Giving the constraints on the variables and constants, I want to determine the minimum value of $n$ and maximum value of $r$ that satisfies $(4.1) / (4.3)$ (whichever is easier to solve) for any given $T$, $a$, $p$. MAJOR EDIT Thanks to @Rodrigo de Azevedo, I discovered Kelly's Criterion. I was sold on it, and decided to implement it into my gambling method. For the purposes of my method Kelly's criterion is given by: $r_i = p - $ ${1 - p}\over{a_i - 1}$  $(5)$ Where: $r_i$ is the ratio at session $i$ $a_i$ is the odds at session $i$ Now $r: 0 \lt r \lt 1$  $(5.1)$ Applying $(5.1)$ to $(5)$ we get: ${p(a - 1) - (1 -p)}\over{a - 1}$ $ \gt \frac{0}{1}$ Cross multiply. $p(a-1) - (1 - p) \gt 0(a-1)$ $pa - p - 1 + p \gt 0$ $pa - 1 > 0$ $pa > 1$ $p > 1/a$  $(5.2)$ Now that that's out of the way, we still have the problem of determining minimum $n$ such that we make a profit $ \ge T$. In order to do this, we'll assume a ""mean"" value for $a$ then find the minimum value for $n$ that satisfies $(4.1)$ Due to the fact, that you do not know the odds for the matches in advance, your mean odds at $i$ say $a_{\mu i}$ may not be the mean odds at $n$ $a_{\mu n}$. In order to protect against this(and because I'm not a very big risk taker), I'll assume a value for $a_{\mu}$, that is less than $a_{\mu}$ called $a_{det}$. $a_{det} = a_{\mu} - k\sigma$ Where $a_{\mu}$ is the Geometric Mean as opposed to the arithmetic mean of the odds and $\sigma$ is associated S.D Using Chebyshev's Inequality, at least $k^{2} - 1 \over k^2$ of the distribution of the odds lie above $a_{det}$. Picking a $k$ of $2.5$ $2.5^{2}-1\over 2.5^{2}$ $0.84$ So our $a_{det}$ is lower than at least $84$% of the distribution of the odds. This is safe enough for me. $a_{det} = a_{\mu} - 2.5\sigma$ Using $a_{det}$, we'll calculate the minimum $n$ that satisfies $(4.1)$ Subbing $5$ and $a_{det}$ into $(4.1)$ we get: $\left(1-\left(p - \frac{1-p}{a_{det}-1}  \right) \right)^{n - np} \cdot \left(\left(p - \frac{1-p}{a_{det}-1}  \right)\cdot(a_{det} - 1)\right)^{np}$  $ \ge T + 1$   $(6.0)$ This can be simplified further to: $\left({a_{det}-1-(pa_{det}-1)}\over{a_{det}-1}\right)^{n(1-p)}\cdot\left(pa_{det}-1+1\right)^{np}$ $\left({a_{det}-pa_{det}}\over{a_{det}-1}\right)^{n(1-p)}\cdot\left(pa_{det}\right)^{np}$ $\left(\left(\frac{a_{det}*(1-p)}{a_{det}-1}\right)^{n(1-p)}\cdot\left(pa_{det}\right)^{np}\right)$ $(6.1)$ P.S due to my particularly low $a_{det}$ we'll likely make much more profit than $T$, but that's loads better than choosing a higher $a_{det}$ and making less.","Say we have a gambler who makes money through sports betting. My aim is to develop a model to help our gambler maximise his winnings and minimize losses. In my model, rather than betting a fixed amount of money, the gambler bets a certain fraction $0 < r < 1$ of his current betting fund. He continues betting that fraction as his betting fund increases or decreases until he cashes out after a certain number of sessions $n$. The gambler's initial fund shall be $F_0$. His fund after $i$ sessions shall be $F_i$. His probability of making a correct prediction shall be $0 < p < 1$. If our gambler had a $p$ of $0$ or $1$, then the entire model would be useless. The average odds with which our gambler deals with is $a > 1$. The gambler's minimum desired profit upon cash out is $T$. $$T \le F_n - F_0 \tag{1}$$ If we expressed everything as a multiple of $F_0$, $(1)$ can be rewritten as: $$T \le F_n - 1 \tag{1.1}$$ It follows that the following are known: $T$, $a$, $F_0$, $p$. Should our gambler lose a particular session say $i+1$, $$F_{i+1} = (1-r)F_i \tag{2.1}$$ Should he win that particular session $$F_{i+1} = F_i(1-r + ra) \tag{2.2}$$ Given that the gambler plays $n$ sessioms before cashing out. His expected number of wins = $p*n$        $(3.1)$ His expected number of losses = $(1-p)*n$         $(3.2)$ Now there are many different ways to distribute the gambler's losses and wins{$n \Bbb P pn$} and while calculating all scenarios and finding average $F_n$ may be ideal, it is computationally very expensive. So I decided to  model the problem assuming the losses take place in the worst way possible( back to back at the very beginning of the match). The gambler's revenue after $n$ matches is given by the formula: $F_n = (1-r)^{(1-p)n}\{(1-r)+ra\}^{pn}$             $(4)$ Now we know that our gambler wants to make a minimum profit of $T$ so we transform $(4)$ into an inequality using $(1.1)$ We get: $(1-r)^{(1-p)n}\{(1-r)+ra\}^{pn}$ $ \ge T + 1$         $(4.1)$ Taking the Natural logarithm of both sides, I get: $ln(1-r)*(1-p)(n) + ln(1-r + ra)*pn \ge ln(T+1)$           $(4.2)$ $n\{ln(1-r)(1-p) + ln(r(a-1)+1)(p) \} \ge ln(T+1)$    $(4.3)$ Giving the constraints on the variables and constants, I want to determine the minimum value of $n$ and maximum value of $r$ that satisfies $(4.1) / (4.3)$ (whichever is easier to solve) for any given $T$, $a$, $p$. MAJOR EDIT Thanks to @Rodrigo de Azevedo, I discovered Kelly's Criterion. I was sold on it, and decided to implement it into my gambling method. For the purposes of my method Kelly's criterion is given by: $r_i = p - $ ${1 - p}\over{a_i - 1}$  $(5)$ Where: $r_i$ is the ratio at session $i$ $a_i$ is the odds at session $i$ Now $r: 0 \lt r \lt 1$  $(5.1)$ Applying $(5.1)$ to $(5)$ we get: ${p(a - 1) - (1 -p)}\over{a - 1}$ $ \gt \frac{0}{1}$ Cross multiply. $p(a-1) - (1 - p) \gt 0(a-1)$ $pa - p - 1 + p \gt 0$ $pa - 1 > 0$ $pa > 1$ $p > 1/a$  $(5.2)$ Now that that's out of the way, we still have the problem of determining minimum $n$ such that we make a profit $ \ge T$. In order to do this, we'll assume a ""mean"" value for $a$ then find the minimum value for $n$ that satisfies $(4.1)$ Due to the fact, that you do not know the odds for the matches in advance, your mean odds at $i$ say $a_{\mu i}$ may not be the mean odds at $n$ $a_{\mu n}$. In order to protect against this(and because I'm not a very big risk taker), I'll assume a value for $a_{\mu}$, that is less than $a_{\mu}$ called $a_{det}$. $a_{det} = a_{\mu} - k\sigma$ Where $a_{\mu}$ is the Geometric Mean as opposed to the arithmetic mean of the odds and $\sigma$ is associated S.D Using Chebyshev's Inequality, at least $k^{2} - 1 \over k^2$ of the distribution of the odds lie above $a_{det}$. Picking a $k$ of $2.5$ $2.5^{2}-1\over 2.5^{2}$ $0.84$ So our $a_{det}$ is lower than at least $84$% of the distribution of the odds. This is safe enough for me. $a_{det} = a_{\mu} - 2.5\sigma$ Using $a_{det}$, we'll calculate the minimum $n$ that satisfies $(4.1)$ Subbing $5$ and $a_{det}$ into $(4.1)$ we get: $\left(1-\left(p - \frac{1-p}{a_{det}-1}  \right) \right)^{n - np} \cdot \left(\left(p - \frac{1-p}{a_{det}-1}  \right)\cdot(a_{det} - 1)\right)^{np}$  $ \ge T + 1$   $(6.0)$ This can be simplified further to: $\left({a_{det}-1-(pa_{det}-1)}\over{a_{det}-1}\right)^{n(1-p)}\cdot\left(pa_{det}-1+1\right)^{np}$ $\left({a_{det}-pa_{det}}\over{a_{det}-1}\right)^{n(1-p)}\cdot\left(pa_{det}\right)^{np}$ $\left(\left(\frac{a_{det}*(1-p)}{a_{det}-1}\right)^{n(1-p)}\cdot\left(pa_{det}\right)^{np}\right)$ $(6.1)$ P.S due to my particularly low $a_{det}$ we'll likely make much more profit than $T$, but that's loads better than choosing a higher $a_{det}$ and making less.",,"['probability', 'mathematical-modeling', 'operations-research', 'gambling']"
66,Probability that the first cell is empty,Probability that the first cell is empty,,"There are three distinctive balls to distribute to 8 cells. Each cell can hold multiple balls. I'm trying to figure out the probability $P(A)$ that, after distribution, the first cell is empty. My thoughts: In total, there are $8^3$ possibilities to distribute the three distinctive balls to the cells, and there are $7^3$ possibilities to distribute the balls to all cells but the first. So $P(A) = 7^3/8^3.$ Is this correct? I'm confused since this could also be the probability of any one cell being empty.","There are three distinctive balls to distribute to 8 cells. Each cell can hold multiple balls. I'm trying to figure out the probability $P(A)$ that, after distribution, the first cell is empty. My thoughts: In total, there are $8^3$ possibilities to distribute the three distinctive balls to the cells, and there are $7^3$ possibilities to distribute the balls to all cells but the first. So $P(A) = 7^3/8^3.$ Is this correct? I'm confused since this could also be the probability of any one cell being empty.",,['probability']
67,What is the difference between moment projection and information projection?,What is the difference between moment projection and information projection?,,"Moment projection is defined as $$\text{arg min}_{q\in Q} D(p||q)$$ while information projection is defined as $$\text{arg min}_{q\in Q} D(q||p)$$. Aside from the difference in the formula, how should one interpret the difference in the two measure intuitively? And when should one use moment projection over information projection, and vice versa?","Moment projection is defined as $$\text{arg min}_{q\in Q} D(p||q)$$ while information projection is defined as $$\text{arg min}_{q\in Q} D(q||p)$$. Aside from the difference in the formula, how should one interpret the difference in the two measure intuitively? And when should one use moment projection over information projection, and vice versa?",,"['probability', 'map-projections']"
68,Stopping rule for quality control problem,Stopping rule for quality control problem,,"Problem: Suppose I have a production process that yields output in batches of $n$ items. For each batch, I can test whether they are of good or bad quality. Let $q_i\in\{1,0\}$  be the quality of tested item $i$. If more than half of the items are ‘bad’, the batch should be discarded. In other words: the batch should be discarded if the average quality of items is $\bar{q}(n)<0.5$. Suppose testing is sequential (i.e. you learn the quality of items one at the time), and each test is a random draw from a Bernoulli distribution with unknown mean $p$. I would like to know when to stop testing items in a batch, in order to decide whether to discard it without testing all items,  and yet be statistically confident that the outcome (discard vs not discard) is the same as the one that would have been reached if all items had been tested. What I currently have: If I knew $p$, I could use a normal approximation to estimate the confidence interval of the binomial proportion $\bar{q}(k)$ for any $k$. With this, for any arbitrary width of the interval, I could calculate the number of tests that are required to achieve a given confidence level (let me call such number of tests $k^*$). Since I don’t know $p$, one option would be to solve for $k^*$ assuming the worst case scenario (i.e. $p=0.5$). This was proposed (and discussed) in a related question here . This approach, however, does not take into account that $n$ is finite, so once many items have been tested, it becomes very unlikely that one more test will swing the outcome...","Problem: Suppose I have a production process that yields output in batches of $n$ items. For each batch, I can test whether they are of good or bad quality. Let $q_i\in\{1,0\}$  be the quality of tested item $i$. If more than half of the items are ‘bad’, the batch should be discarded. In other words: the batch should be discarded if the average quality of items is $\bar{q}(n)<0.5$. Suppose testing is sequential (i.e. you learn the quality of items one at the time), and each test is a random draw from a Bernoulli distribution with unknown mean $p$. I would like to know when to stop testing items in a batch, in order to decide whether to discard it without testing all items,  and yet be statistically confident that the outcome (discard vs not discard) is the same as the one that would have been reached if all items had been tested. What I currently have: If I knew $p$, I could use a normal approximation to estimate the confidence interval of the binomial proportion $\bar{q}(k)$ for any $k$. With this, for any arbitrary width of the interval, I could calculate the number of tests that are required to achieve a given confidence level (let me call such number of tests $k^*$). Since I don’t know $p$, one option would be to solve for $k^*$ assuming the worst case scenario (i.e. $p=0.5$). This was proposed (and discussed) in a related question here . This approach, however, does not take into account that $n$ is finite, so once many items have been tested, it becomes very unlikely that one more test will swing the outcome...",,"['probability', 'statistics', 'stochastic-processes', 'bayesian']"
69,Role of Radon-Nikodym theorem in definition of conditional probabilities,Role of Radon-Nikodym theorem in definition of conditional probabilities,,"Let $(\Omega,\mathscr{F},P)$ be a probability space and $\mathscr{G}\subset\mathscr{F}$ a $\sigma$-field contained in $\mathscr{F}$. If we define the finite measure v on $\mathscr{G}$ by $\text{v}(G)=P(A\cap G)$ for all $G\in\mathscr{G}$, then my question concerns the role of the Radon-Nikodym theorem in the definition of conditional probabilities used by Patrick Billingsley (1995) in his textbook ""Probability and Measure"", which he denotes by $P[A\|\mathscr{G}]$, and defines by; i) $P[A\|\mathscr{G}]$ being measurable $\mathscr{G}$, ii) $\int_G P[A\|\mathscr{G}] \, dP=P(A\cap G):=\operatorname{v}(A\cap G)$ for all $G\in\mathscr{G}$. If $P_0$ is $P$ restricted to $\mathscr{G}$, then since $\operatorname{v}\ll P_0$ for $P_0<\infty$, and therefore $\operatorname{v}<\infty$, I think I can see that the Radon-Nikodym theorem guarantees the existence of a real-valued, non-negative and $\mathscr{G}$-measurable function $f$, integrable w.r.t $P_0$ and hence $P$, satisfying i) and ii). Billingsley then labels $f:=P[A\|\mathscr{G}]$ (note: the definition above uses $P$ not $P_0$ I think since $\operatorname{v}(G)=\int_G f \, dP=\int_G f \, dP_0$ for all $G\in\mathscr{G}$). My question is why does the Radon-Nikodym theorem guarantee that $f$ is a probability, since as far as I can tell it only guarantees it is non-negative? I can see that $f$ is a random variable with source probability space $(\Omega,\mathscr{F},P_0)$ and target space $(\mathbb{R},\mathscr{R},\mu)$ where $\mu$ is the distribution of $f$ satisfying $\mu(A)=P_0[f\in A]$ for all $A\in\mathscr{R}$, but that doesn't mean $f$ always lies in $[0,1]$ and so can be considered a probability?","Let $(\Omega,\mathscr{F},P)$ be a probability space and $\mathscr{G}\subset\mathscr{F}$ a $\sigma$-field contained in $\mathscr{F}$. If we define the finite measure v on $\mathscr{G}$ by $\text{v}(G)=P(A\cap G)$ for all $G\in\mathscr{G}$, then my question concerns the role of the Radon-Nikodym theorem in the definition of conditional probabilities used by Patrick Billingsley (1995) in his textbook ""Probability and Measure"", which he denotes by $P[A\|\mathscr{G}]$, and defines by; i) $P[A\|\mathscr{G}]$ being measurable $\mathscr{G}$, ii) $\int_G P[A\|\mathscr{G}] \, dP=P(A\cap G):=\operatorname{v}(A\cap G)$ for all $G\in\mathscr{G}$. If $P_0$ is $P$ restricted to $\mathscr{G}$, then since $\operatorname{v}\ll P_0$ for $P_0<\infty$, and therefore $\operatorname{v}<\infty$, I think I can see that the Radon-Nikodym theorem guarantees the existence of a real-valued, non-negative and $\mathscr{G}$-measurable function $f$, integrable w.r.t $P_0$ and hence $P$, satisfying i) and ii). Billingsley then labels $f:=P[A\|\mathscr{G}]$ (note: the definition above uses $P$ not $P_0$ I think since $\operatorname{v}(G)=\int_G f \, dP=\int_G f \, dP_0$ for all $G\in\mathscr{G}$). My question is why does the Radon-Nikodym theorem guarantee that $f$ is a probability, since as far as I can tell it only guarantees it is non-negative? I can see that $f$ is a random variable with source probability space $(\Omega,\mathscr{F},P_0)$ and target space $(\mathbb{R},\mathscr{R},\mu)$ where $\mu$ is the distribution of $f$ satisfying $\mu(A)=P_0[f\in A]$ for all $A\in\mathscr{R}$, but that doesn't mean $f$ always lies in $[0,1]$ and so can be considered a probability?",,"['probability', 'probability-theory', 'measure-theory']"
70,How to explain combinatorial identities?,How to explain combinatorial identities?,,"The setup of binomial expansion formula can be traced by two paths, one of which is ""pure"" proof by induction (using properties of combinatorial numbers), the other is ""practical"" comprehension by operation (considering subsets of a finite set). There are some more examples of things like this. $$\binom n 1 + 2\binom n 2 + \cdots + n\binom n n = n 2^{n - 1}$$ (Make a team out of $n$ people, and appoint a leader.) or $$\binom n 0 ^2 + \binom n 1 ^2 + \cdots + \binom n n ^2 = \binom {2n} n$$ (Choose $n$ people from $n$ ladies and $n$ gentlemen.) Sadly I cannnot figure out what this means ""in real life"": $$\binom n 1 + 3\binom n 3 + \cdots = 2\binom n 2 + 4\binom n 4 + \cdots$$ Any hint will be appreciated. (BTW: Is it always possible to ""explain"" combinatorial identities by ""reality""? I wonder sometimes it may seem too ""artificial""...)","The setup of binomial expansion formula can be traced by two paths, one of which is ""pure"" proof by induction (using properties of combinatorial numbers), the other is ""practical"" comprehension by operation (considering subsets of a finite set). There are some more examples of things like this. $$\binom n 1 + 2\binom n 2 + \cdots + n\binom n n = n 2^{n - 1}$$ (Make a team out of $n$ people, and appoint a leader.) or $$\binom n 0 ^2 + \binom n 1 ^2 + \cdots + \binom n n ^2 = \binom {2n} n$$ (Choose $n$ people from $n$ ladies and $n$ gentlemen.) Sadly I cannnot figure out what this means ""in real life"": $$\binom n 1 + 3\binom n 3 + \cdots = 2\binom n 2 + 4\binom n 4 + \cdots$$ Any hint will be appreciated. (BTW: Is it always possible to ""explain"" combinatorial identities by ""reality""? I wonder sometimes it may seem too ""artificial""...)",,"['probability', 'combinatorics', 'algebra-precalculus', 'probability-theory']"
71,Order statistics for discrete uniform random variables,Order statistics for discrete uniform random variables,,"Let $X_i, i=1,\cdots,N$ be i.i.d. discrete uniform random variables, taking values in the range $\{0,1,...,M-1\}$. Let $X_{(i)}$ denote the $i$-th order statistic. What are the values of $\displaystyle\mathbb{E}\left[ \sum_{i=1}^{N/2} X_{(i)}\right]$ and $\displaystyle \mathbb{E}\left[\sum_{i=N/2 +1}^N X_{(i)}\right]$ when $N$ is large? I did some Monte Carlo simulations. It seems that  $$\displaystyle\frac{\displaystyle \mathbb{E}\left[ \sum_{i=1}^{N/2} X_{(i)}\right]}{\displaystyle \mathbb{E}\left[\sum_{i=N/2 +1}^N X_{(i)}\right]}$$ converges to some value, but I am not able to obtain any analytical expression.","Let $X_i, i=1,\cdots,N$ be i.i.d. discrete uniform random variables, taking values in the range $\{0,1,...,M-1\}$. Let $X_{(i)}$ denote the $i$-th order statistic. What are the values of $\displaystyle\mathbb{E}\left[ \sum_{i=1}^{N/2} X_{(i)}\right]$ and $\displaystyle \mathbb{E}\left[\sum_{i=N/2 +1}^N X_{(i)}\right]$ when $N$ is large? I did some Monte Carlo simulations. It seems that  $$\displaystyle\frac{\displaystyle \mathbb{E}\left[ \sum_{i=1}^{N/2} X_{(i)}\right]}{\displaystyle \mathbb{E}\left[\sum_{i=N/2 +1}^N X_{(i)}\right]}$$ converges to some value, but I am not able to obtain any analytical expression.",,"['probability', 'statistics', 'discrete-mathematics', 'uniform-distribution', 'order-statistics']"
72,Are there order statistics for a Gaussian variable raised to a power?,Are there order statistics for a Gaussian variable raised to a power?,,"Let $X$ be a random variable with a standard normal distribution. Let $Y = |X|^{2p}$. I am trying to find the distribution for $Y_{(n)}$, i.e., the largest value of $Y$ out of $n$ samples. I have derived the pdf to be: $$f_{Y_{(n)}} = n \left(\frac{1}{p\sqrt{2\pi}} y^{\frac{1}{2p} - 1} \exp\left(-\frac{1}{2}y^{1/p} \right)\right) \left(\int_0^y \frac{1}{p\sqrt{2\pi}} t^{\frac{1}{2p} - 1} \exp\left(-\frac{1}{2}t^{1/p}\right) \, dt \right)^{n-1}$$ But Mathematica says $EY_{(n)}$ is infinite. Intuitively, I feel that it should be some finite value in terms of p and n. Any ideas?","Let $X$ be a random variable with a standard normal distribution. Let $Y = |X|^{2p}$. I am trying to find the distribution for $Y_{(n)}$, i.e., the largest value of $Y$ out of $n$ samples. I have derived the pdf to be: $$f_{Y_{(n)}} = n \left(\frac{1}{p\sqrt{2\pi}} y^{\frac{1}{2p} - 1} \exp\left(-\frac{1}{2}y^{1/p} \right)\right) \left(\int_0^y \frac{1}{p\sqrt{2\pi}} t^{\frac{1}{2p} - 1} \exp\left(-\frac{1}{2}t^{1/p}\right) \, dt \right)^{n-1}$$ But Mathematica says $EY_{(n)}$ is infinite. Intuitively, I feel that it should be some finite value in terms of p and n. Any ideas?",,"['probability', 'statistics', 'normal-distribution', 'order-statistics']"
73,Probabilistic method proof,Probabilistic method proof,,"Let $v_1, v_2,...,v_n \in \mathbb{R}^n$ be unit vectors. Use probabilistic method to show that there exist constants $a_1, a_2,..., a_n \in \{-1,1\}$ such that $||a_1 v_1 + a_2 v_2 + ... + a_n v_n ||_2 \leq \sqrt{n}$. I think I should make a n random variable $X_i$ that take values in $\{-1,1\}$ each with probability ½, and somehow show that the probability that the squared $2-$norm of the linear combination with coefficients $X_i$ is greater than n, is less than 1. But I am not sure how I should go about that. Any hint would be appreciated.","Let $v_1, v_2,...,v_n \in \mathbb{R}^n$ be unit vectors. Use probabilistic method to show that there exist constants $a_1, a_2,..., a_n \in \{-1,1\}$ such that $||a_1 v_1 + a_2 v_2 + ... + a_n v_n ||_2 \leq \sqrt{n}$. I think I should make a n random variable $X_i$ that take values in $\{-1,1\}$ each with probability ½, and somehow show that the probability that the squared $2-$norm of the linear combination with coefficients $X_i$ is greater than n, is less than 1. But I am not sure how I should go about that. Any hint would be appreciated.",,"['probability', 'probabilistic-method']"
74,When Superposition of Two Renewal Processes is another Renewal Process?,When Superposition of Two Renewal Processes is another Renewal Process?,,"When superposition of two renewal processes is another renewal process? If you merge (superpose) two Poisson processes with parameters $\lambda_1$ and $\lambda_2$, the outcome is another Poisson process with parameter $\lambda_1+\lambda_2$. But, how is that for two general renewal processes? Is there a class of renewal processes that merging two of the members of it makes another renewal process? More generally, under what conditions we can make sure the merged process is a renewal process?","When superposition of two renewal processes is another renewal process? If you merge (superpose) two Poisson processes with parameters $\lambda_1$ and $\lambda_2$, the outcome is another Poisson process with parameter $\lambda_1+\lambda_2$. But, how is that for two general renewal processes? Is there a class of renewal processes that merging two of the members of it makes another renewal process? More generally, under what conditions we can make sure the merged process is a renewal process?",,"['probability', 'probability-theory', 'stochastic-processes', 'renewal-processes']"
75,Details about Brownian motion,Details about Brownian motion,,"Unfortunately I could not make it to the last probability theory lecture and now I am reading through the notes and have some troubles understanding what is going on. So we defined Brownian motion as a stochastic process on $[0,\infty)$ such that 1.) $t \mapsto X_t(\omega)$ is a.s. continuous 2.) $X_t$ has stationary $X_{t_i}-X_{t_{i-1}} \sim N(0,t_i-t_{i-1})$ and independent increments where $X_t \sim N(0,t).$ Now, we are following the book ""Continuous Time Markov Processes"" and I refer to the beginning of Chapter 1.7. It says: It is most convenient in the case of Brownian motion to take the probability space $\Omega$ to be the space $C[0,\infty)$ of all continuous functions $\omega(.)$ on $[0,\infty).$ This choice is natural because Brownian paths are continuous. The process is defined by $X(t,\omega) = \omega(t).$ The $\sigma$- algebra $F$ is taken to be the smallest one for which the projection $\omega \mapsto \omega(t)$ is measurable for each $t$. Rather than having one probability measue on $(\Omega,F)$ we now have  a family $(P^x)$ of probability measures indexed by $x \in \mathbb{R}$. The probability measure is the distribution of $x+B(.),$ where $B$ is standard Brownian motion. This is all a little bit confusing to me. So far I regarded Brownian motion as a map $X: \Omega \rightarrow \big(C[0,\infty), B(C[0,\infty))\big)$ where $B$ is the Borel sigma algebra on $C[0,\infty)$ and $\Omega$ was a space that I did not really care about. But now they seem to be changing $\Omega$ which is fairly bizarre to me. Does anybody understand what they want to do and from where to where (in particular equipped with which sigma algebra and measures) they want the brownian motion to go? As it could still be that they want to denote something different with omega, I wanted to ask the experts here.","Unfortunately I could not make it to the last probability theory lecture and now I am reading through the notes and have some troubles understanding what is going on. So we defined Brownian motion as a stochastic process on $[0,\infty)$ such that 1.) $t \mapsto X_t(\omega)$ is a.s. continuous 2.) $X_t$ has stationary $X_{t_i}-X_{t_{i-1}} \sim N(0,t_i-t_{i-1})$ and independent increments where $X_t \sim N(0,t).$ Now, we are following the book ""Continuous Time Markov Processes"" and I refer to the beginning of Chapter 1.7. It says: It is most convenient in the case of Brownian motion to take the probability space $\Omega$ to be the space $C[0,\infty)$ of all continuous functions $\omega(.)$ on $[0,\infty).$ This choice is natural because Brownian paths are continuous. The process is defined by $X(t,\omega) = \omega(t).$ The $\sigma$- algebra $F$ is taken to be the smallest one for which the projection $\omega \mapsto \omega(t)$ is measurable for each $t$. Rather than having one probability measue on $(\Omega,F)$ we now have  a family $(P^x)$ of probability measures indexed by $x \in \mathbb{R}$. The probability measure is the distribution of $x+B(.),$ where $B$ is standard Brownian motion. This is all a little bit confusing to me. So far I regarded Brownian motion as a map $X: \Omega \rightarrow \big(C[0,\infty), B(C[0,\infty))\big)$ where $B$ is the Borel sigma algebra on $C[0,\infty)$ and $\Omega$ was a space that I did not really care about. But now they seem to be changing $\Omega$ which is fairly bizarre to me. Does anybody understand what they want to do and from where to where (in particular equipped with which sigma algebra and measures) they want the brownian motion to go? As it could still be that they want to denote something different with omega, I wanted to ask the experts here.",,"['probability', 'probability-theory', 'stochastic-processes', 'brownian-motion']"
76,Probability that a person is infected if test is positive?,Probability that a person is infected if test is positive?,,"I have the following problem: $0.5$ % of a population are infected with a dangerous virus. A diagnostic test for the identification of the virus is positive in $99$ % for infected people and in $2$ % for not infected people. Please estimate the probability that a person whose test was positive is infected with the virus. And this is my solution. Since $0.5$ % of the population is infected, then $99.5$ % is not infected. Since $99$ % of the tests on infected people is positive, then $1$ % is negative. Similarly, we can conclude that $98$ % of the tests in not infected people is negative. What we need to find, probability that a person whose test was positive is infected with the virus or in other words that a person is infected given that the test was positive , can be expressed (using the Bayes' theorem) as $$p( P_I \mid T_P) = \frac{p(P_I)\cdot p(T_P \mid P_I)}{p(T_P)}$$ Where $P_I$ means person is infected and $T_P$ means tests are positive . Now, we know $p(P_I)$ , that is $\frac{0.5}{100} = 0.005$ , and we also know $p(T_P \mid P_I) = \frac{99}{100} = 0.99$ . We only need to find $p(T_P)$ , that is the probability that tests are positive . For this purpose, we can use the law of the total probability in the following way: $$p(T_P) = p(P_I)\cdot p(T_P \mid P_I) + p(\overline{P_I}) \cdot p(T_P \mid \overline{P_I}) = 0.005 \cdot 0.99 + 0.995\cdot 0.02 = 0.02485$$ We can now plug the numbers in the first equation $$p( P_I \mid T_P) = \frac{0.005 \cdot 0.99}{0.02485} = 0.19919517102615694$$ That is the probability that person is infected given that the tests are positive is roughly $20$ %. Is my solution correct? This $20$ % does not convince me honestly...","I have the following problem: % of a population are infected with a dangerous virus. A diagnostic test for the identification of the virus is positive in % for infected people and in % for not infected people. Please estimate the probability that a person whose test was positive is infected with the virus. And this is my solution. Since % of the population is infected, then % is not infected. Since % of the tests on infected people is positive, then % is negative. Similarly, we can conclude that % of the tests in not infected people is negative. What we need to find, probability that a person whose test was positive is infected with the virus or in other words that a person is infected given that the test was positive , can be expressed (using the Bayes' theorem) as Where means person is infected and means tests are positive . Now, we know , that is , and we also know . We only need to find , that is the probability that tests are positive . For this purpose, we can use the law of the total probability in the following way: We can now plug the numbers in the first equation That is the probability that person is infected given that the tests are positive is roughly %. Is my solution correct? This % does not convince me honestly...",0.5 99 2 0.5 99.5 99 1 98 p( P_I \mid T_P) = \frac{p(P_I)\cdot p(T_P \mid P_I)}{p(T_P)} P_I T_P p(P_I) \frac{0.5}{100} = 0.005 p(T_P \mid P_I) = \frac{99}{100} = 0.99 p(T_P) p(T_P) = p(P_I)\cdot p(T_P \mid P_I) + p(\overline{P_I}) \cdot p(T_P \mid \overline{P_I}) = 0.005 \cdot 0.99 + 0.995\cdot 0.02 = 0.02485 p( P_I \mid T_P) = \frac{0.005 \cdot 0.99}{0.02485} = 0.19919517102615694 20 20,['probability']
77,Very fascinating probability game about maximising greed?,Very fascinating probability game about maximising greed?,,"Two people play a mathematical game. Each person chooses a number between 1 and 100 inclusive, with both numbers revealed at the same time. The person who has a smaller number will keep their number value while the person who has a larger number will halve their number value. Disregard any draws. For example, if the two players play 50 and 70, the first player will retain 50 points while the second will only get 35. There are five turns in total and each person receives a score equal to the sum of their five values. What is the optimum winning strategy? Obviously playing 100 each turn is a bad strategy since if the other player plays 70 then they gain 20 points more than you. Similarly, playing 1 is also a bad move since you are guaranteed to receive less points than your opponent. If we assume that our opponent is a computer that picks numbers from 1 to 100 with equal probability, we can work out the expected value which will maximise our score relative to the computer's. (I have worked out this to be 60 something - I think) But, if this is true then the computer will realise that it is pointless to play anything less than 30 something so we can further assume the computer will not play such low numbers. This gives a different optimal number to play each time. Repeating this method will give different values of the 'best' number to play. I'm just wondering what this number is. Also, the 'five turns' thing is of course irrelevant, but with a human it is interesting to predict the other player's strategy and moves. So does there exist a number, which will maximise the total expected value? (We can assume our opponent has the same amount of knowledge as us)","Two people play a mathematical game. Each person chooses a number between 1 and 100 inclusive, with both numbers revealed at the same time. The person who has a smaller number will keep their number value while the person who has a larger number will halve their number value. Disregard any draws. For example, if the two players play 50 and 70, the first player will retain 50 points while the second will only get 35. There are five turns in total and each person receives a score equal to the sum of their five values. What is the optimum winning strategy? Obviously playing 100 each turn is a bad strategy since if the other player plays 70 then they gain 20 points more than you. Similarly, playing 1 is also a bad move since you are guaranteed to receive less points than your opponent. If we assume that our opponent is a computer that picks numbers from 1 to 100 with equal probability, we can work out the expected value which will maximise our score relative to the computer's. (I have worked out this to be 60 something - I think) But, if this is true then the computer will realise that it is pointless to play anything less than 30 something so we can further assume the computer will not play such low numbers. This gives a different optimal number to play each time. Repeating this method will give different values of the 'best' number to play. I'm just wondering what this number is. Also, the 'five turns' thing is of course irrelevant, but with a human it is interesting to predict the other player's strategy and moves. So does there exist a number, which will maximise the total expected value? (We can assume our opponent has the same amount of knowledge as us)",,"['probability', 'recreational-mathematics', 'puzzle', 'game-theory', 'average']"
78,"Picking random integers $a_i$ until the $\gcd(a_1,a_2,\dots,a_n)$ is 1",Picking random integers  until the  is 1,"a_i \gcd(a_1,a_2,\dots,a_n)","When we say ""pick a random integer"", the integers are in the range $[1, N]$ . Problem: Consider the following instructions: Pick a random integer. Find the greatest common divisor of all the integers which have been picked so far. If the greatest common divisor is not 1, go back to the first step to pick a random integer. When this process ends, let $X$ be the total number of integers you have picked. I would like to find $E(X)$ in terms of $N$ . Example of this process: (where $N=4$ ) I picked the random integer, 2. $\gcd(2)=2$ I picked another random integer, 4. $\gcd(2, 4) = 2$ I picked another random integer, 2. $\gcd(2, 4, 2) = 2$ I picked another random integer, 3. $\gcd(2, 4, 2, 3) = 1$ In total, I picked $X=4$ integers, $2, 4, 2, 3$ . My ideas: A easy upper bound can be shown to be $N$ , which is the expected number of numbers to be chosen before getting a 1. Let $M(g, N)$ be the expected number of integers I will need after getting $g$ as the greatest common divisor of the integers so far. We have $E(X)=1+\frac{\sum_{i=1}^NM(g, N)}{N}$ . Small cases: $N=1\rightarrow E(X)=1$ $N=2\rightarrow E(X)=2$","When we say ""pick a random integer"", the integers are in the range . Problem: Consider the following instructions: Pick a random integer. Find the greatest common divisor of all the integers which have been picked so far. If the greatest common divisor is not 1, go back to the first step to pick a random integer. When this process ends, let be the total number of integers you have picked. I would like to find in terms of . Example of this process: (where ) I picked the random integer, 2. I picked another random integer, 4. I picked another random integer, 2. I picked another random integer, 3. In total, I picked integers, . My ideas: A easy upper bound can be shown to be , which is the expected number of numbers to be chosen before getting a 1. Let be the expected number of integers I will need after getting as the greatest common divisor of the integers so far. We have . Small cases:","[1, N] X E(X) N N=4 \gcd(2)=2 \gcd(2, 4) = 2 \gcd(2, 4, 2) = 2 \gcd(2, 4, 2, 3) = 1 X=4 2, 4, 2, 3 N M(g, N) g E(X)=1+\frac{\sum_{i=1}^NM(g, N)}{N} N=1\rightarrow E(X)=1 N=2\rightarrow E(X)=2","['probability', 'elementary-number-theory']"
79,Is a Markov process uniquely determined?,Is a Markov process uniquely determined?,,"Let $E$ be a Polish space and $\mathcal E$ be the Borel $\sigma$-algebra on $E$ $I\subseteq[0,\infty)$ be closed under addition and $0\in I$ Please consider the following result: Let $(\kappa_t:t\in I)$ be a Markovian semigroup on $(E,\mathcal E)$ $\Rightarrow$ There is a measurable space $(\Omega,\mathcal A)$ and a Markov process $X$ with distributions $(\operatorname P_x)_{x\in E}$ such that $$\operatorname P_x\left[X_t\in B\right]=\kappa_t(x,B)\;\;\;\text{for all }x\in E,B\in\mathcal E\text{ and }t\in I\;.\tag 1$$ Conversely, given a Markov process $X$ with distributions $(\operatorname P_x)_{x\in E}$ on a measurable space $(\Omega,\mathcal A)$, a Markovian semigroup $(\kappa_t:t\in I)$ is defined by $(1)$. It turns out that $X$ in the first part of the statement can be constructed as the family of coordinate maps on $(\Omega,\mathcal A)=(E^I,\mathcal E^{\otimes I})$. I've seen that many authors assume that Markov processes are such coordinate maps. Why can they do that? The statement above doesn't state, that given $(\Omega,\mathcal A)$ there is one unique Markov process, does it? However, the finite-dimensional distributions of $X$, i.e. $$\operatorname P_x\left[X\in\;\cdot\;\right]\circ\pi_J^{-1}\;\;\;\text{for }J\subseteq I\text{ with }|J|<\infty\;,\tag 2$$ where $\pi_J:E^I\to E^J$ are the canonical projections, are uniquely determined by $(1)$. Maybe $(\operatorname P_x)_{x\in E}$ (not only the finite-dimensional distributions) are uniquely determined by $(1)$, if $I\subseteq \mathbb N_0$ or $I$ is at least almost countable or when $E$ is almost countable. So, why does the stated result allows us to think about $X$ as being uniquely determined?","Let $E$ be a Polish space and $\mathcal E$ be the Borel $\sigma$-algebra on $E$ $I\subseteq[0,\infty)$ be closed under addition and $0\in I$ Please consider the following result: Let $(\kappa_t:t\in I)$ be a Markovian semigroup on $(E,\mathcal E)$ $\Rightarrow$ There is a measurable space $(\Omega,\mathcal A)$ and a Markov process $X$ with distributions $(\operatorname P_x)_{x\in E}$ such that $$\operatorname P_x\left[X_t\in B\right]=\kappa_t(x,B)\;\;\;\text{for all }x\in E,B\in\mathcal E\text{ and }t\in I\;.\tag 1$$ Conversely, given a Markov process $X$ with distributions $(\operatorname P_x)_{x\in E}$ on a measurable space $(\Omega,\mathcal A)$, a Markovian semigroup $(\kappa_t:t\in I)$ is defined by $(1)$. It turns out that $X$ in the first part of the statement can be constructed as the family of coordinate maps on $(\Omega,\mathcal A)=(E^I,\mathcal E^{\otimes I})$. I've seen that many authors assume that Markov processes are such coordinate maps. Why can they do that? The statement above doesn't state, that given $(\Omega,\mathcal A)$ there is one unique Markov process, does it? However, the finite-dimensional distributions of $X$, i.e. $$\operatorname P_x\left[X\in\;\cdot\;\right]\circ\pi_J^{-1}\;\;\;\text{for }J\subseteq I\text{ with }|J|<\infty\;,\tag 2$$ where $\pi_J:E^I\to E^J$ are the canonical projections, are uniquely determined by $(1)$. Maybe $(\operatorname P_x)_{x\in E}$ (not only the finite-dimensional distributions) are uniquely determined by $(1)$, if $I\subseteq \mathbb N_0$ or $I$ is at least almost countable or when $E$ is almost countable. So, why does the stated result allows us to think about $X$ as being uniquely determined?",,"['probability', 'probability-theory', 'stochastic-processes', 'markov-chains', 'markov-process']"
80,Continuous Approximation for The Kelly Criterion,Continuous Approximation for The Kelly Criterion,,"I am trying to follow the derivation of Kelly Criterion, the continuous case. Dr. Thorp shows the basics of the derivation here , pg. 22. With initial capital $V_0$, betting fraction $f$, and $X$ is a random variable representing returns where $$ P(X = m+s) = P(X = m-s) = 0.5$$ The final capital is, $$ V(f) = V_0 (1 + (1-f)r + fX)  $$ $$ V(f) = V_0 (1 + r + f(X - r))  $$ His eventual goal is to find the $f$ for the maximum $E[\log(V_f)]$, and do this on a continuous scale. So he subdivides the time into $n$ pieces, $m$, $s^2$, and $r$ are replaced by $m/n$, $s^2/n$ and $r/n$ respectively, $$ P(X_i = m/n + s/\sqrt{n}) = P(X_i = m/n - m/\sqrt{n}) = 0.5$$ $$ V_n(f)/V_0 = \prod _{i=1}^{n} (1 + r + f(X_i - r))  $$ then says take log of both sides, and apply the expectation operator. I did that, $$ \log V_n(f)/V_0 = \sum _{i=1}^{n} \log (1 + r + f(X_i - r))  $$ $$ E[\log V_n(f)/V_0] = \sum _{i=1}^{n} E[\log (1 + r + fX_i - fr)]  $$ This is where I get stuck, Thorp mentions ""we expand the result in a power series"", and I've seen a similar trick in a different book , pg 137, where the author reaches a statement like $1/1+fg$ after the derivative on a log, and he turned that into $1 - fg + ..$. However I am not able to reach a similar statement. $$  = n E[\log (1 + r + fX_n - fr)]  $$ Thorp eventually reaches a formula like $$ g(f) = r + f(m-r) - s^2f^2/2 + O(n^{-1/2})$$ Any ideas? Thanks,","I am trying to follow the derivation of Kelly Criterion, the continuous case. Dr. Thorp shows the basics of the derivation here , pg. 22. With initial capital $V_0$, betting fraction $f$, and $X$ is a random variable representing returns where $$ P(X = m+s) = P(X = m-s) = 0.5$$ The final capital is, $$ V(f) = V_0 (1 + (1-f)r + fX)  $$ $$ V(f) = V_0 (1 + r + f(X - r))  $$ His eventual goal is to find the $f$ for the maximum $E[\log(V_f)]$, and do this on a continuous scale. So he subdivides the time into $n$ pieces, $m$, $s^2$, and $r$ are replaced by $m/n$, $s^2/n$ and $r/n$ respectively, $$ P(X_i = m/n + s/\sqrt{n}) = P(X_i = m/n - m/\sqrt{n}) = 0.5$$ $$ V_n(f)/V_0 = \prod _{i=1}^{n} (1 + r + f(X_i - r))  $$ then says take log of both sides, and apply the expectation operator. I did that, $$ \log V_n(f)/V_0 = \sum _{i=1}^{n} \log (1 + r + f(X_i - r))  $$ $$ E[\log V_n(f)/V_0] = \sum _{i=1}^{n} E[\log (1 + r + fX_i - fr)]  $$ This is where I get stuck, Thorp mentions ""we expand the result in a power series"", and I've seen a similar trick in a different book , pg 137, where the author reaches a statement like $1/1+fg$ after the derivative on a log, and he turned that into $1 - fg + ..$. However I am not able to reach a similar statement. $$  = n E[\log (1 + r + fX_n - fr)]  $$ Thorp eventually reaches a formula like $$ g(f) = r + f(m-r) - s^2f^2/2 + O(n^{-1/2})$$ Any ideas? Thanks,",,"['probability', 'probability-theory', 'stochastic-processes', 'gambling']"
81,distribution of one random over the sum of random variables,distribution of one random over the sum of random variables,,"Suppose that $X_1,\ldots,X_n$ are independent random variables with $X_i\sim Gamma(\alpha_i,\beta)$. Define $U_i=\frac{X_i}{X_1+\cdots+X_n}$ for $i=1,2,\ldots,n$. Show that $U_i\sim Beta(\alpha_i,\sum_{j\neq i}\alpha_j)$. This is a question of past comprehensive exam. It also gave a hint: Think of $U_i$ as $X_i/(X_i+W)$, where $W=\sum_{j\neq i}X_j$ is independent of $X_i$. Can someone give me more hint about it?","Suppose that $X_1,\ldots,X_n$ are independent random variables with $X_i\sim Gamma(\alpha_i,\beta)$. Define $U_i=\frac{X_i}{X_1+\cdots+X_n}$ for $i=1,2,\ldots,n$. Show that $U_i\sim Beta(\alpha_i,\sum_{j\neq i}\alpha_j)$. This is a question of past comprehensive exam. It also gave a hint: Think of $U_i$ as $X_i/(X_i+W)$, where $W=\sum_{j\neq i}X_j$ is independent of $X_i$. Can someone give me more hint about it?",,"['probability', 'statistics', 'statistical-inference']"
82,Computing Conditional Characteristic Function,Computing Conditional Characteristic Function,,"I am trying to compute the characteristic function of the following: Let $X$ and $Y$ be random variables such that $Y\mid X = x\sim N(0, x)$ with $X\sim\mathrm{Po}(\lambda)$. Find the characteristic function of $Y$. I know the characteristic function is $\varphi_X(t)=E[e^{itX}]=\int e^{itx}f_X(x) \, dx$.  How do I take into account the fact that $Y$ is conditional on $X$ to compute the characteristic function? In my case I need to find $f_Y(y)$ in order to solve $\varphi_Y(t)=E[e^{itY}]=\int e^{ity}f_Y(y) \, dy$. To find the probability density of y, I need to solve $P(Y=y)=\sum_{n=0}^{\infty}P(Y|X=x)*P(X=x)$, but I don't know how to reduce this.","I am trying to compute the characteristic function of the following: Let $X$ and $Y$ be random variables such that $Y\mid X = x\sim N(0, x)$ with $X\sim\mathrm{Po}(\lambda)$. Find the characteristic function of $Y$. I know the characteristic function is $\varphi_X(t)=E[e^{itX}]=\int e^{itx}f_X(x) \, dx$.  How do I take into account the fact that $Y$ is conditional on $X$ to compute the characteristic function? In my case I need to find $f_Y(y)$ in order to solve $\varphi_Y(t)=E[e^{itY}]=\int e^{ity}f_Y(y) \, dy$. To find the probability density of y, I need to solve $P(Y=y)=\sum_{n=0}^{\infty}P(Y|X=x)*P(X=x)$, but I don't know how to reduce this.",,"['probability', 'random-variables', 'characteristic-functions']"
83,Changing-sided dice probability problem.,Changing-sided dice probability problem.,,"Suppose you roll a fair $6$-sided dice, and that the number you roll is $m$. If $m=1$, stop. Otherwise, roll an $m$-sided dice. The number you roll is $n$. If $n=1$, stop. Otherwise roll an $n$-sided dice... etc. What is the probability it will take exactly $x$ rolls to roll a 1? So far I see a pattern, but I'm wondering if there's a better way of expressing these nasty sums, or if it's even possible? $$P(1)=\frac{1}{6}$$ $$P(2)=\frac{1}{6}(\frac{1}{6}+\frac{1}{5}+\frac{1}{4}+\frac{1}{3}+\frac{1}{2})$$ $$P(x)=\underbrace{\frac{1}{6}\sum_{j_1=2}^{6}\left(\frac{1}{j_1}\sum_{j_2=2}^{j_1}\left[\frac{1}{j_2}\sum_{j_3=2}^{j_2}(\ldots)\right]\right)}_{x-1 \mbox{ sigmas}}$$","Suppose you roll a fair $6$-sided dice, and that the number you roll is $m$. If $m=1$, stop. Otherwise, roll an $m$-sided dice. The number you roll is $n$. If $n=1$, stop. Otherwise roll an $n$-sided dice... etc. What is the probability it will take exactly $x$ rolls to roll a 1? So far I see a pattern, but I'm wondering if there's a better way of expressing these nasty sums, or if it's even possible? $$P(1)=\frac{1}{6}$$ $$P(2)=\frac{1}{6}(\frac{1}{6}+\frac{1}{5}+\frac{1}{4}+\frac{1}{3}+\frac{1}{2})$$ $$P(x)=\underbrace{\frac{1}{6}\sum_{j_1=2}^{6}\left(\frac{1}{j_1}\sum_{j_2=2}^{j_1}\left[\frac{1}{j_2}\sum_{j_3=2}^{j_2}(\ldots)\right]\right)}_{x-1 \mbox{ sigmas}}$$",,"['probability', 'summation', 'dice']"
84,Expected value of a function of a random variable [duplicate],Expected value of a function of a random variable [duplicate],,"This question already has an answer here : What is the name of this theorem, and are there any caveats? (1 answer) Closed 9 years ago . Let X be a random variable whose PDF is $f(x)$, and $g$ a function of random variable X. I want to prove that $$E[g(X)] = \int{g(x)f(x)dx} $$ I've perfectly understood it in discrete case and I managed to prove also for continous case when $g$ is an increasing function. What is a formal proof for this formula? And what is a good intuition for it?","This question already has an answer here : What is the name of this theorem, and are there any caveats? (1 answer) Closed 9 years ago . Let X be a random variable whose PDF is $f(x)$, and $g$ a function of random variable X. I want to prove that $$E[g(X)] = \int{g(x)f(x)dx} $$ I've perfectly understood it in discrete case and I managed to prove also for continous case when $g$ is an increasing function. What is a formal proof for this formula? And what is a good intuition for it?",,"['probability', 'probability-theory', 'probability-distributions']"
85,Not getting the answer as given in Feller,Not getting the answer as given in Feller,,"Find the probability that the equation $x^2-2ax+b=0$ has complex roots, if $a,b$ are random variables following the Uniform $(0,h)$ distribution individually and independently. So we effectively need to determine $P(b>a^2)$ which, in my case turns out to be $1-\dfrac{h}{3}$ if $h\leq 1$ and $\dfrac{2}{3\sqrt{h}}$ if $h>1$ . However, the answers as mentioned at the back of the text, are $\dfrac{h}{3}$ and $\dfrac{1}{3\sqrt{h}}$ respectively. I would have got those answers if I had calculated $P(b<a^2)$ which is clearly not correct as we are looking for complex roots. My working sketch: We want $\int_{a}P(b>a^2)\dfrac{1}{h}da$ . So to make the probability positive, we must have $0<a<\min\{h,\sqrt{h}\}$ So we break into two cases, one where $h\leq1$ and the other where $h>1$ and perform the integration in each case, subject to $0<a<h$ in first case and $0<a<\sqrt{h}$ in second case. Please point out any error you can find.","Find the probability that the equation has complex roots, if are random variables following the Uniform distribution individually and independently. So we effectively need to determine which, in my case turns out to be if and if . However, the answers as mentioned at the back of the text, are and respectively. I would have got those answers if I had calculated which is clearly not correct as we are looking for complex roots. My working sketch: We want . So to make the probability positive, we must have So we break into two cases, one where and the other where and perform the integration in each case, subject to in first case and in second case. Please point out any error you can find.","x^2-2ax+b=0 a,b (0,h) P(b>a^2) 1-\dfrac{h}{3} h\leq 1 \dfrac{2}{3\sqrt{h}} h>1 \dfrac{h}{3} \dfrac{1}{3\sqrt{h}} P(b<a^2) \int_{a}P(b>a^2)\dfrac{1}{h}da 0<a<\min\{h,\sqrt{h}\} h\leq1 h>1 0<a<h 0<a<\sqrt{h}","['probability', 'probability-theory', 'probability-distributions', 'uniform-distribution']"
86,Does independence of events depend upon underlying probability model?,Does independence of events depend upon underlying probability model?,,"Consider a sample space of two coin tosses = $\{HH, HT, TH, TT\}$. Suppose that the coin is fair and therefore every outcome has probability $\frac{1}{4}$. Now, consider another probability model where the coin is biased and head occurs with probability $\frac{3}{4}$. In this model the corresponding probabilities = $\{\frac{9}{16}, \frac{3}{16}, \frac{3}{16}, \frac{1}{16}\}$. Definition of independence $P(A \cup B) = P(A)P(B)$ Event $A$ = first coin toss results in head        = $\{HT, HH\}$ Event $B$ = both coin toss results in same outcome = $\{HH, TT\}$ First model: $P(A) = P(\{HT,HH\}) = \frac{1}{4} + \frac{1}{4} = \frac{1}{2}$ $P(B) = P(\{HH,TT\}) = \frac{1}{4} + \frac{1}{4} = \frac{1}{2}$ $P(A \cup B) = P(\{HH\}) = \frac{1}{4} = \frac{1}{2}\times\frac{1}{2} = P(A)P(B)$ Therefore, in this probability model events $A$ and $B$ are independent. Second model : $P(A) = P(\{HT,HH\}) = \frac{3}{16} + \frac{9}{16} = \frac{12}{16}$ $P(B) = P(\{HH,TT\}) = \frac{9}{16} + \frac{1}{16} = \frac{10}{16}$ $P(A \cup B) = P(\{HH\}) = \frac{9}{16}$ which is not equal to $P(A)P(B) = \frac{15}{32}$ Therefore, in this probability model events $A$ and $B$ are not independent Thus, independence of the events depend upon the underlying probability model. However, I am not getting the intuition. Can anyone explain? Also Bayes' rule and law of total probability is applicable irrespective of underlying probability model so why the independence of events differs from one probability model to another?","Consider a sample space of two coin tosses = $\{HH, HT, TH, TT\}$. Suppose that the coin is fair and therefore every outcome has probability $\frac{1}{4}$. Now, consider another probability model where the coin is biased and head occurs with probability $\frac{3}{4}$. In this model the corresponding probabilities = $\{\frac{9}{16}, \frac{3}{16}, \frac{3}{16}, \frac{1}{16}\}$. Definition of independence $P(A \cup B) = P(A)P(B)$ Event $A$ = first coin toss results in head        = $\{HT, HH\}$ Event $B$ = both coin toss results in same outcome = $\{HH, TT\}$ First model: $P(A) = P(\{HT,HH\}) = \frac{1}{4} + \frac{1}{4} = \frac{1}{2}$ $P(B) = P(\{HH,TT\}) = \frac{1}{4} + \frac{1}{4} = \frac{1}{2}$ $P(A \cup B) = P(\{HH\}) = \frac{1}{4} = \frac{1}{2}\times\frac{1}{2} = P(A)P(B)$ Therefore, in this probability model events $A$ and $B$ are independent. Second model : $P(A) = P(\{HT,HH\}) = \frac{3}{16} + \frac{9}{16} = \frac{12}{16}$ $P(B) = P(\{HH,TT\}) = \frac{9}{16} + \frac{1}{16} = \frac{10}{16}$ $P(A \cup B) = P(\{HH\}) = \frac{9}{16}$ which is not equal to $P(A)P(B) = \frac{15}{32}$ Therefore, in this probability model events $A$ and $B$ are not independent Thus, independence of the events depend upon the underlying probability model. However, I am not getting the intuition. Can anyone explain? Also Bayes' rule and law of total probability is applicable irrespective of underlying probability model so why the independence of events differs from one probability model to another?",,"['probability', 'probability-theory']"
87,Probability that rolling X dice with Y sides and summing the highest Z values is above some value k,Probability that rolling X dice with Y sides and summing the highest Z values is above some value k,,"Some background: There is an RPG called Legend of the Five Rings, with an interesting dice system.  You roll X dice, and keep the highest Z of them.  You add those Z dice together.  This is phrased as ""X keep Z"".  All of the dice have ten sides, and if you roll a 10, it ""explodes"" (you roll again).  It continues to explode until you roll something other than a ten, adding all the values together, so that one die is worth more than 10. I'm trying to create my own RPG system, and am looking at different dice options.  That said, I'd like to know how to calculate several different probabilities similar to the L5R roll and keep system: 1) What is the probability that X keep Z on Y sided dice is greater than k, with exploding dice? 2) What is the probability that X keep Z on Y sided dice is greater than k, without exploding dice? 3) What is the probability that X keep Z lowest on Y sided dice is greater than k?","Some background: There is an RPG called Legend of the Five Rings, with an interesting dice system.  You roll X dice, and keep the highest Z of them.  You add those Z dice together.  This is phrased as ""X keep Z"".  All of the dice have ten sides, and if you roll a 10, it ""explodes"" (you roll again).  It continues to explode until you roll something other than a ten, adding all the values together, so that one die is worth more than 10. I'm trying to create my own RPG system, and am looking at different dice options.  That said, I'd like to know how to calculate several different probabilities similar to the L5R roll and keep system: 1) What is the probability that X keep Z on Y sided dice is greater than k, with exploding dice? 2) What is the probability that X keep Z on Y sided dice is greater than k, without exploding dice? 3) What is the probability that X keep Z lowest on Y sided dice is greater than k?",,"['probability', 'combinatorics', 'dice']"
88,What is a mathematically rigorous justification for multiplying edge probabilities of a tree diagram,What is a mathematically rigorous justification for multiplying edge probabilities of a tree diagram,,"I was trying to understand why it was mathematically justified to multiply edge probabilities in a tree diagra and I came across the following question: Why do we multiply in tree diagrams? The second answer is nearly exactly what I needed, however, there are some things that I still don't understand about it. Or maybe there are just some things about the answer that are not mathematically rigorous enough for me (or trivial enough for the poster that the details were omitted), specially in the context of view probability with set theory . That is what I wish to address: how to treat probability tree diagrams with set theory rigorously and thus calculate probabilities on trees in a mathematically justified way. The main issue that I have is how leaves/outcomes are specified with set notation in a sloppy way, which leads has lead to weird justification of calculating probabilities in tree diagrams. I will try to address what I think the issue are in detail in the frame of set theory to make sure that everything is precisely and clearly defined. The exact issue that I am having is with the notation $\cup$ and $\cap$ being used to describe probabilistic statements. In high school we are taught to think about these as AND and ORs. I wish to abandon that mentality (since I think its one of the reasons for my confusion) and be extremely precise on the use of $\cap$ and $\cup$. Intersection and Union are two operations that only apply to sets . I will use them in that way and wish to address their correct use in probability theory. First lets try to define ""outcome"" and ""events"" precisely and see how they relate to tree diagrams. An outcome normally means a specific way of specifying the result of an experiment. For example, in the monty hall problem we can specify the outcome of the experiment by the following triplet: outcome = (car location, player's initial guess, door revealed by the host). i.e. an outcome is fully specified when we specify the location where the car actually is, what the players initial guess is and the door that was revealed by the host. Hence resulting in the following tree diagram: ( which I got from MIT's course for mathematics for computer science 6.042 ). as it can be appreciated, the leaves of the tree are the outcomes and all the leaves are the whole samples space $S$. In these terms the samples space is the set of triples: Now, an event is a subset of this samples space, i.e. choosing a subset of the leaves. The issue that I have is that I have seen the leaves of such a tree trees diagram denoted as $(A \cap A \cap B)$ (for the first one on my example) instead of $(A, A, B)$. For me, these two are not the same. The second triplet is just a sequence that acts as an ""index"" to specify a specific outcome in the sample space (which is an element of the set $S$). The notation with intersection (i.e. $A \cap A \cap B$) tries to specify a leaf but it seems plain wrong to me and confusing (or horrible abuse of notation? not sure...). Let me justify why I think its an incorrect way to specify a leaf: firstly, it is not clear to me what $(A \cap A \cap B)$ even means. For me, that just means the empty set, because intersection should only be applied to sets and $(A \cap A \cap B)$ has no intersection. secondly, even if you try to ""repair"" the first issue by insisting that the first position, second and third position are simply events and taking intersections of them is valid, still brings problems. i.e. $(A \cap A \cap B)$ are intersection of ""events"" is still wrong I believe. That solution only bring further problems/question. First, if A is now an event, then, what exactly is it a subset of? (since thats what an event is. If you are trying to use set notation to denote stuff, you better specify what the sets/subsets are). How do we re-define the sample space so that this notation indication of a leaf is justified? If we could do this, then (maybe) the justification explained in the question I posted might be valid (with further justifications). If you try to use set notation to specify a leaf, it seems to me that the correct way to do it is by unions, not intersections. The reason is because, that would actually lead to the correct meaning of what a triplet specifies (and avoid having the issue of the empty set that I specified in my first point). However, since the order of the elements of a sets ""don't matter"" and because the triplets are sequences (where the order maters), the way to fix the new problem I have introduced by using unions is by using a subscript on the position of the triplet (kind of defining a bijection) i.e. the outcome $(A, A, B)$ corresponds $\{ A_1 \cup A_2 \cup B_3\}$. Anyway, taking this definition doesn't help that much, because its not clear to me how to use the general chain rule of probability to justify the probability of a leaf. Basically, how do you rigorously justify using the chain rule of probability to calculate the probability of a single outcome in a probability tree diagram?","I was trying to understand why it was mathematically justified to multiply edge probabilities in a tree diagra and I came across the following question: Why do we multiply in tree diagrams? The second answer is nearly exactly what I needed, however, there are some things that I still don't understand about it. Or maybe there are just some things about the answer that are not mathematically rigorous enough for me (or trivial enough for the poster that the details were omitted), specially in the context of view probability with set theory . That is what I wish to address: how to treat probability tree diagrams with set theory rigorously and thus calculate probabilities on trees in a mathematically justified way. The main issue that I have is how leaves/outcomes are specified with set notation in a sloppy way, which leads has lead to weird justification of calculating probabilities in tree diagrams. I will try to address what I think the issue are in detail in the frame of set theory to make sure that everything is precisely and clearly defined. The exact issue that I am having is with the notation $\cup$ and $\cap$ being used to describe probabilistic statements. In high school we are taught to think about these as AND and ORs. I wish to abandon that mentality (since I think its one of the reasons for my confusion) and be extremely precise on the use of $\cap$ and $\cup$. Intersection and Union are two operations that only apply to sets . I will use them in that way and wish to address their correct use in probability theory. First lets try to define ""outcome"" and ""events"" precisely and see how they relate to tree diagrams. An outcome normally means a specific way of specifying the result of an experiment. For example, in the monty hall problem we can specify the outcome of the experiment by the following triplet: outcome = (car location, player's initial guess, door revealed by the host). i.e. an outcome is fully specified when we specify the location where the car actually is, what the players initial guess is and the door that was revealed by the host. Hence resulting in the following tree diagram: ( which I got from MIT's course for mathematics for computer science 6.042 ). as it can be appreciated, the leaves of the tree are the outcomes and all the leaves are the whole samples space $S$. In these terms the samples space is the set of triples: Now, an event is a subset of this samples space, i.e. choosing a subset of the leaves. The issue that I have is that I have seen the leaves of such a tree trees diagram denoted as $(A \cap A \cap B)$ (for the first one on my example) instead of $(A, A, B)$. For me, these two are not the same. The second triplet is just a sequence that acts as an ""index"" to specify a specific outcome in the sample space (which is an element of the set $S$). The notation with intersection (i.e. $A \cap A \cap B$) tries to specify a leaf but it seems plain wrong to me and confusing (or horrible abuse of notation? not sure...). Let me justify why I think its an incorrect way to specify a leaf: firstly, it is not clear to me what $(A \cap A \cap B)$ even means. For me, that just means the empty set, because intersection should only be applied to sets and $(A \cap A \cap B)$ has no intersection. secondly, even if you try to ""repair"" the first issue by insisting that the first position, second and third position are simply events and taking intersections of them is valid, still brings problems. i.e. $(A \cap A \cap B)$ are intersection of ""events"" is still wrong I believe. That solution only bring further problems/question. First, if A is now an event, then, what exactly is it a subset of? (since thats what an event is. If you are trying to use set notation to denote stuff, you better specify what the sets/subsets are). How do we re-define the sample space so that this notation indication of a leaf is justified? If we could do this, then (maybe) the justification explained in the question I posted might be valid (with further justifications). If you try to use set notation to specify a leaf, it seems to me that the correct way to do it is by unions, not intersections. The reason is because, that would actually lead to the correct meaning of what a triplet specifies (and avoid having the issue of the empty set that I specified in my first point). However, since the order of the elements of a sets ""don't matter"" and because the triplets are sequences (where the order maters), the way to fix the new problem I have introduced by using unions is by using a subscript on the position of the triplet (kind of defining a bijection) i.e. the outcome $(A, A, B)$ corresponds $\{ A_1 \cup A_2 \cup B_3\}$. Anyway, taking this definition doesn't help that much, because its not clear to me how to use the general chain rule of probability to justify the probability of a leaf. Basically, how do you rigorously justify using the chain rule of probability to calculate the probability of a single outcome in a probability tree diagram?",,"['probability', 'probability-theory']"
89,Can 2 different random variables have the same CDF?,Can 2 different random variables have the same CDF?,,"I'm looking for proof that two different random variables can have the same Cumulative Distribution Function; in other words, I'd like to disprove that a CDF uniquely defines a random variable. (Probably a silly question, but I couldn't find anything usable so far).","I'm looking for proof that two different random variables can have the same Cumulative Distribution Function; in other words, I'd like to disprove that a CDF uniquely defines a random variable. (Probably a silly question, but I couldn't find anything usable so far).",,"['probability', 'statistics', 'probability-theory', 'probability-distributions']"
90,"Relationship ""finite mean"" <-> ""absolutely integrable","Relationship ""finite mean"" <-> ""absolutely integrable",,"What is the relationship between the property of a random variable (i.e. a measurable function defined on some probability space) being absolutely integrable, i.e. $$\mathbb{E}|X|<\infty$$and having finite mean, i.e.$$\mathbb{E}X<\infty\quad ?$$Obviously the first implies the second, but does the converse hold too ?","What is the relationship between the property of a random variable (i.e. a measurable function defined on some probability space) being absolutely integrable, i.e. $$\mathbb{E}|X|<\infty$$and having finite mean, i.e.$$\mathbb{E}X<\infty\quad ?$$Obviously the first implies the second, but does the converse hold too ?",,"['probability', 'probability-theory']"
91,"exactly k consecutive heads, n tosses","exactly k consecutive heads, n tosses",,"What is the expected number of strings of exactly k consecutive heads if a fair coin is tossed n times? My current answer is $$ {n-1\choose k} (\frac{1}{2})^{(k-1)} $$ Is this correct? A possible string : HTHHHTHH. This has 3 strings for 2 consecutive heads. The string for 3 Hs, is also 2 strings for 2 consecutive Hs.","What is the expected number of strings of exactly k consecutive heads if a fair coin is tossed n times? My current answer is $$ {n-1\choose k} (\frac{1}{2})^{(k-1)} $$ Is this correct? A possible string : HTHHHTHH. This has 3 strings for 2 consecutive heads. The string for 3 Hs, is also 2 strings for 2 consecutive Hs.",,"['probability', 'statistics', 'statistical-inference']"
92,Square integrable stochastic process,Square integrable stochastic process,,"Suppose that for a stochastic process we have \begin{align} \mathbb{E}\left[\int_{0}^{T}X^{2}(t)dt \right]<\infty \end{align} where $T<\infty$. Does it  holds that $|X(t)|<M$, where $M$ constant? I am a little confused.","Suppose that for a stochastic process we have \begin{align} \mathbb{E}\left[\int_{0}^{T}X^{2}(t)dt \right]<\infty \end{align} where $T<\infty$. Does it  holds that $|X(t)|<M$, where $M$ constant? I am a little confused.",,"['probability', 'stochastic-processes']"
93,Probability that an element belongs to an infinite subset of a set,Probability that an element belongs to an infinite subset of a set,,"$\newcommand{\Sym}{\operatorname{Sym}}$ I was studying $\Sym(\mathbb{N})$, the set consisting of all the bijections from $\mathbb{N}$ to itself. Since it is a group, the concept of ""period of an element"" has a sense, and it is the smallest positive integer $n$ such that $f^n = e$, where $f$ is one of those bijections and $e$ is the identity of the group (the identity function). I was interested in the subset of all the elements of the group that have finite period. My question is: if I randomly choose an element of $\Sym(\mathbb{N})$, is there a way to know... if it's more likely to get an element of infinite period, or an element of finite period? The problem is that, according to the results I got, both $\Sym(\mathbb{N})$ and its subset I'm interested in are infinite sets that have the cardinality of the continuum. Am I unawarely asking a stupid/impossible question, or are there any mathematical tools to know what that probability is?","$\newcommand{\Sym}{\operatorname{Sym}}$ I was studying $\Sym(\mathbb{N})$, the set consisting of all the bijections from $\mathbb{N}$ to itself. Since it is a group, the concept of ""period of an element"" has a sense, and it is the smallest positive integer $n$ such that $f^n = e$, where $f$ is one of those bijections and $e$ is the identity of the group (the identity function). I was interested in the subset of all the elements of the group that have finite period. My question is: if I randomly choose an element of $\Sym(\mathbb{N})$, is there a way to know... if it's more likely to get an element of infinite period, or an element of finite period? The problem is that, according to the results I got, both $\Sym(\mathbb{N})$ and its subset I'm interested in are infinite sets that have the cardinality of the continuum. Am I unawarely asking a stupid/impossible question, or are there any mathematical tools to know what that probability is?",,"['probability', 'group-theory', 'descriptive-set-theory']"
94,Covariance of $X^2$ and $X^3$ when $X$ is exponentially distributed,Covariance of  and  when  is exponentially distributed,X^2 X^3 X,"Here is my work.... $\begin{align*}  Cov(Y,Z) &= E(YZ) - E(Y)E(Z)\\ &= E(X^2\cdot X^3) - E(X^2)E(X^3)\\ &= E(X^5) - E(X^2)E(X^3) \end{align*}$ And we know $E(X^n) = \frac{n!}{\lambda^n}$ so, $\begin{align*}  Cov(Y,Z) &= \frac{5!}{\lambda^5} - \frac{2!}{\lambda^2}\cdot \frac{3!}{\lambda^3}\\ &= \frac{120}{\lambda^5} - \frac{12}{\lambda^5}\\ &= \frac{108}{\lambda^5} \end{align*}$ Does this seem like it is correct? Thanks!","Here is my work.... $\begin{align*}  Cov(Y,Z) &= E(YZ) - E(Y)E(Z)\\ &= E(X^2\cdot X^3) - E(X^2)E(X^3)\\ &= E(X^5) - E(X^2)E(X^3) \end{align*}$ And we know $E(X^n) = \frac{n!}{\lambda^n}$ so, $\begin{align*}  Cov(Y,Z) &= \frac{5!}{\lambda^5} - \frac{2!}{\lambda^2}\cdot \frac{3!}{\lambda^3}\\ &= \frac{120}{\lambda^5} - \frac{12}{\lambda^5}\\ &= \frac{108}{\lambda^5} \end{align*}$ Does this seem like it is correct? Thanks!",,"['probability', 'covariance']"
95,What is the probability of a number from 1-25 being an odd number or a factor of 20?,What is the probability of a number from 1-25 being an odd number or a factor of 20?,,"What is the probability of a number from 1-25 being an odd number or a factor of 20? Here's my working out: Odd numbers: 12/25 (1, 3, 5, 7, 9, 11, 13, 15, 19, 21, 23, 25) Factors of 20: 6/25 (1, 2, 4, 5, 10, 20) Both: 2/25 (1, 5) P(odd number or factor of 15) = P(odd number) + P(fator of 20) -   P(both) = 12/25 + 6/25 - 2/25 = 16/25 16/25 was too long for MyMaths' input box so I converted it to a decimal: 16 / 25 = 0.64 That was incorrect.","What is the probability of a number from 1-25 being an odd number or a factor of 20? Here's my working out: Odd numbers: 12/25 (1, 3, 5, 7, 9, 11, 13, 15, 19, 21, 23, 25) Factors of 20: 6/25 (1, 2, 4, 5, 10, 20) Both: 2/25 (1, 5) P(odd number or factor of 15) = P(odd number) + P(fator of 20) -   P(both) = 12/25 + 6/25 - 2/25 = 16/25 16/25 was too long for MyMaths' input box so I converted it to a decimal: 16 / 25 = 0.64 That was incorrect.",,['probability']
96,Help with the Probabilty of Rolling Two Ten-Sided Dice Multiple Times Until 100 is Reached,Help with the Probabilty of Rolling Two Ten-Sided Dice Multiple Times Until 100 is Reached,,"I need some help figuring out the probability of reaching or exceeding 100 based on a number of rolls of two, ten-sided dice. Here's the scenario. I am starting from zero. I am rolling two (fair) ten-sided dice, to generate a result between 2 and 20. After the roll, I'm recording the number rolled as the 'total' and then rolling again. I'm taking the new result and adding it the total, then rolling again and so on. I'm trying to have the total reach or exceed 100. Example: On my first roll, I get 12. I record 12 and roll again. On my second roll, I get 7. I add 7 to the current total of 12 to have a new total of 19. Then I roll again. How many rolls must I make to have a 25% chance of reaching or exceeding 100? How many rolls must I make to have a 50% chance of reaching or exceeding 100? How many rolls must I make to have a 75% chance of reaching or exceeding 100? How many rolls must I make to have a 90% chance of reaching or exceeding 100?","I need some help figuring out the probability of reaching or exceeding 100 based on a number of rolls of two, ten-sided dice. Here's the scenario. I am starting from zero. I am rolling two (fair) ten-sided dice, to generate a result between 2 and 20. After the roll, I'm recording the number rolled as the 'total' and then rolling again. I'm taking the new result and adding it the total, then rolling again and so on. I'm trying to have the total reach or exceed 100. Example: On my first roll, I get 12. I record 12 and roll again. On my second roll, I get 7. I add 7 to the current total of 12 to have a new total of 19. Then I roll again. How many rolls must I make to have a 25% chance of reaching or exceeding 100? How many rolls must I make to have a 50% chance of reaching or exceeding 100? How many rolls must I make to have a 75% chance of reaching or exceeding 100? How many rolls must I make to have a 90% chance of reaching or exceeding 100?",,['probability']
97,find a chance that all N points lie on the half circle. [duplicate],find a chance that all N points lie on the half circle. [duplicate],,"This question already has answers here : Probability that n points on a circle are in one semicircle (6 answers) Closed 10 years ago . We are given a circle with N randomly allocated points on it. Task is to find a chance that all N points lie on the one half of circle. I have drafted some solution: 1. Since there are no way to put two points on circle, so that they were not on the same half-circle, $P_1$ and $P_2$ picked randomly and didn't affect the chance. So, required probability is: $P(P_3) \cdot P(P_4)\: \cdot ... \cdot P(P_n)$, where $P(P_i)$ is the chance that i-th point lays on the proper half of circle. 2. Let's visualize what's $P(P_3)$, $P(P_4)$ look like: Grey sector highlights forbidden part of circle. It's obvious from pictures, that $P(P_i)$ approaching 0.5 as point amount increases For this specific example, we could write: $P(P_3) = 1 - \frac {\Delta(\theta_2,\theta_1)} {2\pi}$ $P(P_4) = 1 - \frac {\Delta(\theta_3,\theta_1)} {2\pi}$. 3. Then if we generalize, $P(P_i) = 1 - \frac {\Delta_i} {2\pi}$, where $\Delta_i$ is a difference of angles of the most distant points. And I'm understand that here I should introduce some generalized formula but I don't see it and don't want to make guesses. So I would appreciate any help.","This question already has answers here : Probability that n points on a circle are in one semicircle (6 answers) Closed 10 years ago . We are given a circle with N randomly allocated points on it. Task is to find a chance that all N points lie on the one half of circle. I have drafted some solution: 1. Since there are no way to put two points on circle, so that they were not on the same half-circle, $P_1$ and $P_2$ picked randomly and didn't affect the chance. So, required probability is: $P(P_3) \cdot P(P_4)\: \cdot ... \cdot P(P_n)$, where $P(P_i)$ is the chance that i-th point lays on the proper half of circle. 2. Let's visualize what's $P(P_3)$, $P(P_4)$ look like: Grey sector highlights forbidden part of circle. It's obvious from pictures, that $P(P_i)$ approaching 0.5 as point amount increases For this specific example, we could write: $P(P_3) = 1 - \frac {\Delta(\theta_2,\theta_1)} {2\pi}$ $P(P_4) = 1 - \frac {\Delta(\theta_3,\theta_1)} {2\pi}$. 3. Then if we generalize, $P(P_i) = 1 - \frac {\Delta_i} {2\pi}$, where $\Delta_i$ is a difference of angles of the most distant points. And I'm understand that here I should introduce some generalized formula but I don't see it and don't want to make guesses. So I would appreciate any help.",,"['probability', 'geometry', 'geometric-probability']"
98,Variance of Time-Integrated Ornstein-Uhlenbeck Process,Variance of Time-Integrated Ornstein-Uhlenbeck Process,,"I'm attempting to filter white noise from a deterministic, finite-power signal using a low-pass filter. This filter can be described using an exponentially-decaying response function: $$ h(t) = \gamma \exp (-\gamma t) $$ Evaluating the Wiener integral for the filtered noise results in a zero-mean Ornstein-Uhlenbeck process, whose variance is: $$\sigma^2_{OU}  =\frac{\gamma}{2}\left( 1-\exp(-2\gamma t)\right)$$ The task now is to find the variance of the time-integrated process. My question is whether the following procedure, based on Wiener integrals, is correct, and if not, what is the correct procedure? First, we use the fact that Gaussian random variables are determined by their means and variances to express the zero-mean OU process as the product of a deterministic function and a Wiener process: $$ \sigma^2_W = t \quad \therefore \quad \sigma^2_{F\times W} = F^2(t)\times t $$ $$ F(t) = \frac{\gamma}{2} \sqrt{\frac{1-\exp(-2\gamma t)}{t}} \quad \rightarrow \quad \sigma^2_{F\times W} = \sigma^2_{OU}$$ We perform integration by parts: $$ \int_0^t W(\tau)F(\tau)d\tau = \left. \left[W(\tau)\int_{0}^{\tau}F(\tau ')d\tau ' \right] \right \vert_0^t - \int_0^t \left( \int_{0}^{\tau}F(\tau ')d\tau ' \right)dW(\tau)$$ Labelling $\int_{0}^{\tau}F(\tau ')d\tau '$ as $\tilde{F}(\tau)$, we write the variance of the integrated process as the sum of the variances of the two terms above. The variance of the left term is $ \tilde{F}^2(t)\times t$, since this is just the Wiener process, multiplied by a deterministic function. The term on the right is a Wiener integral, whose variance is $\int_0^t \tilde{F}^2(t) dt$. I'll leave $\tilde{F}(t)$ unspecified here, since the integral is difficult. So, have I made some error in deriving the variance?","I'm attempting to filter white noise from a deterministic, finite-power signal using a low-pass filter. This filter can be described using an exponentially-decaying response function: $$ h(t) = \gamma \exp (-\gamma t) $$ Evaluating the Wiener integral for the filtered noise results in a zero-mean Ornstein-Uhlenbeck process, whose variance is: $$\sigma^2_{OU}  =\frac{\gamma}{2}\left( 1-\exp(-2\gamma t)\right)$$ The task now is to find the variance of the time-integrated process. My question is whether the following procedure, based on Wiener integrals, is correct, and if not, what is the correct procedure? First, we use the fact that Gaussian random variables are determined by their means and variances to express the zero-mean OU process as the product of a deterministic function and a Wiener process: $$ \sigma^2_W = t \quad \therefore \quad \sigma^2_{F\times W} = F^2(t)\times t $$ $$ F(t) = \frac{\gamma}{2} \sqrt{\frac{1-\exp(-2\gamma t)}{t}} \quad \rightarrow \quad \sigma^2_{F\times W} = \sigma^2_{OU}$$ We perform integration by parts: $$ \int_0^t W(\tau)F(\tau)d\tau = \left. \left[W(\tau)\int_{0}^{\tau}F(\tau ')d\tau ' \right] \right \vert_0^t - \int_0^t \left( \int_{0}^{\tau}F(\tau ')d\tau ' \right)dW(\tau)$$ Labelling $\int_{0}^{\tau}F(\tau ')d\tau '$ as $\tilde{F}(\tau)$, we write the variance of the integrated process as the sum of the variances of the two terms above. The variance of the left term is $ \tilde{F}^2(t)\times t$, since this is just the Wiener process, multiplied by a deterministic function. The term on the right is a Wiener integral, whose variance is $\int_0^t \tilde{F}^2(t) dt$. I'll leave $\tilde{F}(t)$ unspecified here, since the integral is difficult. So, have I made some error in deriving the variance?",,"['probability', 'stochastic-processes', 'stochastic-calculus', 'stochastic-integrals']"
99,IID Random Variables that are not constant can't converge almost surely,IID Random Variables that are not constant can't converge almost surely,,"I am trying to prove the following. If $\{ X_n \}$ are iid random variables and not constant, then $R:=P\{ \omega \mid  X_n(\omega)\text{ converges} \}=0$ Using independence I know that by Kolmogorov's 0-1 law, that if $R$ is not $0$ then $R=1$.  So I think the way to do this proof is by contradiction.  So I am trying to show $R=1$ implies the $X_n$ are constant using their identical distribution but sadly it is not working.  Help would  be appreciated.  Thanks!","I am trying to prove the following. If $\{ X_n \}$ are iid random variables and not constant, then $R:=P\{ \omega \mid  X_n(\omega)\text{ converges} \}=0$ Using independence I know that by Kolmogorov's 0-1 law, that if $R$ is not $0$ then $R=1$.  So I think the way to do this proof is by contradiction.  So I am trying to show $R=1$ implies the $X_n$ are constant using their identical distribution but sadly it is not working.  Help would  be appreciated.  Thanks!",,"['probability', 'probability-theory', 'probability-distributions']"
