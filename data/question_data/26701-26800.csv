,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Positive-definite function and Positive-definite matrix,Positive-definite function and Positive-definite matrix,,I am trying to understand Positive-definite function and read the wikipedia link: https://en.wikipedia.org/wiki/Positive-definite_function It has a relation to Positive-definite matrix and I did not understand how clearly. My questions : is the Positive-definite function $f$ mentioned in the wikipedia link is continuous? It is related to to the element of a Positive-definite matrix which are just numbers what it signifies? Can anyone please clarify?,I am trying to understand Positive-definite function and read the wikipedia link: https://en.wikipedia.org/wiki/Positive-definite_function It has a relation to Positive-definite matrix and I did not understand how clearly. My questions : is the Positive-definite function $f$ mentioned in the wikipedia link is continuous? It is related to to the element of a Positive-definite matrix which are just numbers what it signifies? Can anyone please clarify?,,"['linear-algebra', 'functional-analysis', 'definition']"
1,Is theoretical Linear Algebra still an active field of research?,Is theoretical Linear Algebra still an active field of research?,,"Numerical Linear Algebra seems to be a very active area right now, but is there any work still being done on the purely theoretical side? To put it another way...is it possible for someone to write a book titled ""All of Linear Algebra"" and actually create a closed set of concepts that encompass this field? For example, I'm thinking a book like Peter Lax's Linear Algebra and its Applications seems to be close to this, but a lot of reviewers of the book stated it was a select group of topics.","Numerical Linear Algebra seems to be a very active area right now, but is there any work still being done on the purely theoretical side? To put it another way...is it possible for someone to write a book titled ""All of Linear Algebra"" and actually create a closed set of concepts that encompass this field? For example, I'm thinking a book like Peter Lax's Linear Algebra and its Applications seems to be close to this, but a lot of reviewers of the book stated it was a select group of topics.",,['linear-algebra']
2,Why does a symmetric matrix have a complete set of eigenvectors and eigenvalues?,Why does a symmetric matrix have a complete set of eigenvectors and eigenvalues?,,"I am attempting to learn more about the adjacency matrix(graph theory) but given that I have forgotten a lot of linear algebra, I can't seem to know why this is true. Can someone give me a proof?","I am attempting to learn more about the adjacency matrix(graph theory) but given that I have forgotten a lot of linear algebra, I can't seem to know why this is true. Can someone give me a proof?",,['linear-algebra']
3,Find all possible Jordan Canonical forms of $A^2$,Find all possible Jordan Canonical forms of,A^2,"Finding Jordan Canonical forms seems pretty straightforward mostly, but this one threw me off: Let $A\in M_n(\Bbb{F}) $ be a matrix with a minimal polynomial $m_A(t)=(t-\lambda)^n$. Find all the possible Jordan Canonical forms of $A^2$. I've looked through all my notes and material and didn't find a single example of finding a Jordan Canonical form for a matrix by any power. I know how to find it well enough for $A$ alone, but wouldn't know how raising it by the power of 2 (or any power) would affect that. Any tips? Thanks!","Finding Jordan Canonical forms seems pretty straightforward mostly, but this one threw me off: Let $A\in M_n(\Bbb{F}) $ be a matrix with a minimal polynomial $m_A(t)=(t-\lambda)^n$. Find all the possible Jordan Canonical forms of $A^2$. I've looked through all my notes and material and didn't find a single example of finding a Jordan Canonical form for a matrix by any power. I know how to find it well enough for $A$ alone, but wouldn't know how raising it by the power of 2 (or any power) would affect that. Any tips? Thanks!",,"['linear-algebra', 'jordan-normal-form']"
4,Let $\text{Rank}{(A - \lambda I)^k} = \text{Rank}{(B - \lambda I)^k}$. Why are $A$ and $B$ similar?,Let . Why are  and  similar?,\text{Rank}{(A - \lambda I)^k} = \text{Rank}{(B - \lambda I)^k} A B,"Let $A$ and $B \in M_n$ be two matrices such that $$\forall k=1,2,\dots,n,\ \forall \lambda\  \text{eigenvalue of $A$},\ \text{Rank}{(A - \lambda I)^k} = \text{Rank}{(B - \lambda I)^k}.$$ Why are $A$ and $B$ similar?","Let $A$ and $B \in M_n$ be two matrices such that $$\forall k=1,2,\dots,n,\ \forall \lambda\  \text{eigenvalue of $A$},\ \text{Rank}{(A - \lambda I)^k} = \text{Rank}{(B - \lambda I)^k}.$$ Why are $A$ and $B$ similar?",,"['linear-algebra', 'matrices']"
5,"$\text{Alt}\,(\phi_1 \otimes \phi_2 \otimes \phi_3)$",,"\text{Alt}\,(\phi_1 \otimes \phi_2 \otimes \phi_3)","How do I write out $\text{Alt}(\phi_1 \otimes \phi_2 \otimes \phi_3)$ for $\phi_1, \phi_2, \phi_3 \in V^*$?","How do I write out $\text{Alt}(\phi_1 \otimes \phi_2 \otimes \phi_3)$ for $\phi_1, \phi_2, \phi_3 \in V^*$?",,"['linear-algebra', 'multivariable-calculus']"
6,A question about biorthogonal basis composed of eigenvectors of a finite-dimensional non-self-adjoint matrix,A question about biorthogonal basis composed of eigenvectors of a finite-dimensional non-self-adjoint matrix,,"The non-self-adjoint matrix M has non-degenerate eigenvalues, that is $M \psi_i = e_i \psi_i$, and its adjoint matrix satisfies $M^\dagger \chi_j= e_j^*  \chi_j$. I know that $(\chi_j, \psi_i) = (\psi_i, \chi_j ) = 0$ for $i \neq j$, but why $(\chi_i, \psi_i), \forall i$ cannot be zero?","The non-self-adjoint matrix M has non-degenerate eigenvalues, that is $M \psi_i = e_i \psi_i$, and its adjoint matrix satisfies $M^\dagger \chi_j= e_j^*  \chi_j$. I know that $(\chi_j, \psi_i) = (\psi_i, \chi_j ) = 0$ for $i \neq j$, but why $(\chi_i, \psi_i), \forall i$ cannot be zero?",,['linear-algebra']
7,Name of the LU decomposition algorithm,Name of the LU decomposition algorithm,,On the wikipedia page of LU decomposition there is an algorithm that produce the decomposition. It is called Doolittle algorithm . I'm really interested who is Doolittle? Or from where the name comes from? Is there any citation for the original work of the algorithm? As I know the decomposition is invented be Alan Turing.,On the wikipedia page of LU decomposition there is an algorithm that produce the decomposition. It is called Doolittle algorithm . I'm really interested who is Doolittle? Or from where the name comes from? Is there any citation for the original work of the algorithm? As I know the decomposition is invented be Alan Turing.,,"['linear-algebra', 'matrices', 'numerical-methods', 'matrix-decomposition']"
8,Under what conditions is the product of two invertible diagonalizable matrices diagonalizable?,Under what conditions is the product of two invertible diagonalizable matrices diagonalizable?,,"The answer in this question gives an example for the statement product of two invertible diagonalizable matrices is not diagonalizable. My question is: Are there some conditions, perhaps on the eigenvalues and eigenvectors of matrices, under which the product of two invertible diagonalizable matrices is diagonalizable?","The answer in this question gives an example for the statement product of two invertible diagonalizable matrices is not diagonalizable. My question is: Are there some conditions, perhaps on the eigenvalues and eigenvectors of matrices, under which the product of two invertible diagonalizable matrices is diagonalizable?",,[]
9,"What is a linear combination, exactly?","What is a linear combination, exactly?",,"I'm used to the definition of linear combination used in linear algebra textbooks. I'm reading the book Algebra by Artin and on page 357 he says: If $R$ is the ring $\mathbb{Z}[x]$ of integer polynomials, the notation $(2,x)$ stands for the ideal of linear combinations of $2$ and $x$ with integer polynomial coefficients. According to the definition above, the term $x\cdot x+2 = x^2+2$ is a linear combination of $x$ and $2$, which doesn't feel like linear to me. Here is my question: What is the definition of a linear combination? My confusion may arise because in a vector space there is no such thing as product of vectors.","I'm used to the definition of linear combination used in linear algebra textbooks. I'm reading the book Algebra by Artin and on page 357 he says: If $R$ is the ring $\mathbb{Z}[x]$ of integer polynomials, the notation $(2,x)$ stands for the ideal of linear combinations of $2$ and $x$ with integer polynomial coefficients. According to the definition above, the term $x\cdot x+2 = x^2+2$ is a linear combination of $x$ and $2$, which doesn't feel like linear to me. Here is my question: What is the definition of a linear combination? My confusion may arise because in a vector space there is no such thing as product of vectors.",,"['linear-algebra', 'abstract-algebra', 'ring-theory', 'ideals']"
10,A conjecture about the eigenvalues of symmetric pentadiagonal Toeplitz matrix,A conjecture about the eigenvalues of symmetric pentadiagonal Toeplitz matrix,,"Is there a way to find out the exact eigenvalues and eigenvectors of a real symmetric pentadiagonal Toeplitz $n\times n$ matrix with the form given below? $$ \begin{pmatrix} a & b & c & 0 & \cdots & \cdots & 0 \\ b & a & b & c & 0 & \cdots & 0 \\ c & b & a & b & c & \ddots & \vdots \\ 0 & \ddots & \ddots & \ddots & \ddots & \ddots & \vdots \\ 0 & \ddots & \ddots & \ddots & \ddots & b & c \\ \vdots & \ddots & 0 & c & b & a & b \\ 0 & \cdots & 0 & 0 & c & b & a \end{pmatrix} $$ In particular, I'm dealing with this conjecture (verified by numerical diagonalization): The difference of the smallest two eigenvalues are of the order $1/n^2$ if $f(k)$ has one minimum, or $1/n^3$ if $f(k)$ has two minima, where $f(k)=a+2b\cos(k)+2c\cos(2k)$ and $k\in[0,2\pi)$.","Is there a way to find out the exact eigenvalues and eigenvectors of a real symmetric pentadiagonal Toeplitz $n\times n$ matrix with the form given below? $$ \begin{pmatrix} a & b & c & 0 & \cdots & \cdots & 0 \\ b & a & b & c & 0 & \cdots & 0 \\ c & b & a & b & c & \ddots & \vdots \\ 0 & \ddots & \ddots & \ddots & \ddots & \ddots & \vdots \\ 0 & \ddots & \ddots & \ddots & \ddots & b & c \\ \vdots & \ddots & 0 & c & b & a & b \\ 0 & \cdots & 0 & 0 & c & b & a \end{pmatrix} $$ In particular, I'm dealing with this conjecture (verified by numerical diagonalization): The difference of the smallest two eigenvalues are of the order $1/n^2$ if $f(k)$ has one minimum, or $1/n^3$ if $f(k)$ has two minima, where $f(k)=a+2b\cos(k)+2c\cos(2k)$ and $k\in[0,2\pi)$.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
11,the algorithm and computation cost for truncated SVD in rank k,the algorithm and computation cost for truncated SVD in rank k,,It seems that the time cost of truncated SVD in rank k for matrix $A\in R^{m\times m}$ is $O(m^2 k)$. Could anyone show me some algorithms to calculate truncated SVD with the above time complexity?,It seems that the time cost of truncated SVD in rank k for matrix $A\in R^{m\times m}$ is $O(m^2 k)$. Could anyone show me some algorithms to calculate truncated SVD with the above time complexity?,,"['calculus', 'linear-algebra', 'matrices', 'numerical-methods', 'computational-complexity']"
12,"If the rank of $A$ is equal to the number of non-zero eigenvalues, do $A$ and $A^2$ have the same rank?","If the rank of  is equal to the number of non-zero eigenvalues, do  and  have the same rank?",A A A^2,"Let $A$ be an $n$-by-$n$ matrix over some field. If it happens that $\operatorname{rank}(A)=$ number of non-zero eigenvalues of $A$, can we say that $\operatorname{rank}(A^2)=\operatorname{rank}(A)$? I believe we can say this (thinking about idempotent matrices) but I am not sure about the proof.  Please give some hints to get started and the main idea.","Let $A$ be an $n$-by-$n$ matrix over some field. If it happens that $\operatorname{rank}(A)=$ number of non-zero eigenvalues of $A$, can we say that $\operatorname{rank}(A^2)=\operatorname{rank}(A)$? I believe we can say this (thinking about idempotent matrices) but I am not sure about the proof.  Please give some hints to get started and the main idea.",,"['linear-algebra', 'vector-spaces', 'eigenvalues-eigenvectors', 'matrix-rank']"
13,"Dual Space Annihilator in C[0,1]","Dual Space Annihilator in C[0,1]",,"Let $V = C[0,1]$ and let U be the subspace of functions of the form  $y(x) = ax+b$ for some a, b depending on the function.  Give an explicit family of functionals $F\subset U^\perp$ such that for any $y \in V$ satisfying f(y) = 0$\ \  \forall f \in F$, we have$\  y\in U$. In other words, in $V^{**}$, we have$$span F^\perp\cap\phi(V) = \phi(U).$$ Help Please","Let $V = C[0,1]$ and let U be the subspace of functions of the form  $y(x) = ax+b$ for some a, b depending on the function.  Give an explicit family of functionals $F\subset U^\perp$ such that for any $y \in V$ satisfying f(y) = 0$\ \  \forall f \in F$, we have$\  y\in U$. In other words, in $V^{**}$, we have$$span F^\perp\cap\phi(V) = \phi(U).$$ Help Please",,['real-analysis']
14,Wedge product of a direct sum and the Yoneda Lemma,Wedge product of a direct sum and the Yoneda Lemma,,"In a comment to https://math.stackexchange.com/a/344851/58601 , Martin Brandenburg suggests that one may prove the existence of the canonical isomorphism $\wedge^n(W_1 \oplus W_2) \to \bigoplus_{p+q=n} \wedge^p(W_1) \otimes \wedge^q(W_2)$ with the Yoneda Lemma. Is there a reference for this technique? I'm interested in a more conceptual proof of the fact that the map is an isomorphism than the ""pick a basis"" proof.","In a comment to https://math.stackexchange.com/a/344851/58601 , Martin Brandenburg suggests that one may prove the existence of the canonical isomorphism $\wedge^n(W_1 \oplus W_2) \to \bigoplus_{p+q=n} \wedge^p(W_1) \otimes \wedge^q(W_2)$ with the Yoneda Lemma. Is there a reference for this technique? I'm interested in a more conceptual proof of the fact that the map is an isomorphism than the ""pick a basis"" proof.",,"['linear-algebra', 'category-theory', 'alternative-proof']"
15,If $K \leq L$ a finite extension then it is algebraic.,If  a finite extension then it is algebraic.,K \leq L,"I am looking at the proof of If $K \leq L$ a finite extension then it is algebraic. The proof is the following: Let $[L:K]=n<\infty$ . Let $a \in L$ . We will show that $\exists$ a non-zero $f(x) \in K[x]$ with $f(a)=0$ . We take $1, a, a^2, \dots , a^n \in L$ , that are $(n+1)$ elements of $L$ . So, it is $K-$ linear dependent, so there are $k_0, k_1, \dots , k_n \in K$ not all zero, with $k_0 \cdot 1 +k_1 \cdot a+k_2 \cdot a^2 + \dots + k_n a^n=0$ . Then $f(x)=k_0+k_1x+ \dots + k_nx^n \in K[x]$ is non-zero and $f(a)=0$ . Therefore, $a$ is algebraic over $K$ . $$$$ Could you explain me the following part?? We take $1, a, a^2, \dots , a^n \in L$ , that are $(n+1)$ elements of $L$ . So, it is $K-$ linear dependent, so there are $k_0, k_1, \dots , k_n \in K$ not all zero, with $k_0 \cdot 1 +k_1 \cdot a+k_2 \cdot a^2 + \dots + k_n a^n=0$ .","I am looking at the proof of If a finite extension then it is algebraic. The proof is the following: Let . Let . We will show that a non-zero with . We take , that are elements of . So, it is linear dependent, so there are not all zero, with . Then is non-zero and . Therefore, is algebraic over . Could you explain me the following part?? We take , that are elements of . So, it is linear dependent, so there are not all zero, with .","K \leq L [L:K]=n<\infty a \in L \exists f(x) \in K[x] f(a)=0 1, a, a^2, \dots , a^n \in L (n+1) L K- k_0, k_1, \dots , k_n \in K k_0 \cdot 1 +k_1 \cdot a+k_2 \cdot a^2 + \dots + k_n a^n=0 f(x)=k_0+k_1x+ \dots + k_nx^n \in K[x] f(a)=0 a K  1, a, a^2, \dots , a^n \in L (n+1) L K- k_0, k_1, \dots , k_n \in K k_0 \cdot 1 +k_1 \cdot a+k_2 \cdot a^2 + \dots + k_n a^n=0","['linear-algebra', 'vector-spaces', 'field-theory']"
16,what can be derived from similar matrix,what can be derived from similar matrix,,"If $A=\begin{pmatrix} 0&\star&\star \\ \star&x&\star \\ \star & \star & 5 \end{pmatrix}$ is similar to $B=\begin{pmatrix} 1&0&0 \\ 0&y&0 \\ 0 & 0 & 10 \end{pmatrix}$,where $\star$ represents unknown number.  Find $x$ and $y$ I know, the trace of $A$ is equal to the trace of $B$, in other words, $5+x=11+y$, and their determinant and characteristic polynomial is also the same. but $\det A$ and $\det(\lambda I -A)$ have a lot of unknown quantity. To what degree we can determine $A$ from the condition?","If $A=\begin{pmatrix} 0&\star&\star \\ \star&x&\star \\ \star & \star & 5 \end{pmatrix}$ is similar to $B=\begin{pmatrix} 1&0&0 \\ 0&y&0 \\ 0 & 0 & 10 \end{pmatrix}$,where $\star$ represents unknown number.  Find $x$ and $y$ I know, the trace of $A$ is equal to the trace of $B$, in other words, $5+x=11+y$, and their determinant and characteristic polynomial is also the same. but $\det A$ and $\det(\lambda I -A)$ have a lot of unknown quantity. To what degree we can determine $A$ from the condition?",,"['linear-algebra', 'matrices']"
17,The basis of a matrix representation,The basis of a matrix representation,,"If I have the linear map $f:\Bbb{R}^n\rightarrow \Bbb{R}^m$ then we can write $f$ as like the following: $$f\left(\vec x\right)=A\vec x$$ Where $A$ is a matrix. I think $A$ is called the standard matrix for $f$ . Linear maps act on vectors and therefore should not be associated with any basis i.e. they act on vectors rather then ' coordinate vectors '. Does this mean that the matrix $A$ is not associated with any basis? (noting that in the standard basis of the two vector spaces, the matrix representatin of $f$ will be equivlent to $A$ ). i.e. is the following statement correct: The matrix $A$ is equivalent to the linear map $f$ when acting on a vector in $\Bbb{R}^n$ . The matrix $\tilde A$ which is the matrix representation of $f$ in the standard bases of $\Bbb{R}^n$ and $\Bbb{R}^m$ has exactly the same components as $A$ but acts on coordinate vectors rather then actual vectors the linear map $f$ acts on. These coordinate vectors will however take exactly the same form, in the standard bases, as the original vectors that $f$ acts on.","If I have the linear map then we can write as like the following: Where is a matrix. I think is called the standard matrix for . Linear maps act on vectors and therefore should not be associated with any basis i.e. they act on vectors rather then ' coordinate vectors '. Does this mean that the matrix is not associated with any basis? (noting that in the standard basis of the two vector spaces, the matrix representatin of will be equivlent to ). i.e. is the following statement correct: The matrix is equivalent to the linear map when acting on a vector in . The matrix which is the matrix representation of in the standard bases of and has exactly the same components as but acts on coordinate vectors rather then actual vectors the linear map acts on. These coordinate vectors will however take exactly the same form, in the standard bases, as the original vectors that acts on.",f:\Bbb{R}^n\rightarrow \Bbb{R}^m f f\left(\vec x\right)=A\vec x A A f A f A A f \Bbb{R}^n \tilde A f \Bbb{R}^n \Bbb{R}^m A f f,"['linear-algebra', 'matrices']"
18,Proof that any set linearly independent has at most $n$ elements (when the vector space has basis with n elements),Proof that any set linearly independent has at most  elements (when the vector space has basis with n elements),n,"My teacher gave us this proof today, but I don't know if I understood it entirely: Theorem: Suppose $V$ a vector space (finitely generated) over the reals. $$B = \{v_1,\cdots, v_n\}$$ where $B$ is a basis for $V$. Suppose, also, a set $S\subset V$ L.I., then $S$ is finite and has at most $n$ elements. Proof: Let $S = \{y_1,\cdots,y_m\}$, with $m>n$. Suppose $S$ is L.I. Then:   $$\alpha_1y_1+\cdots+\alpha_my_m = 0\tag{1}$$ But since $S$ is greater   than $B$, and $B$ is a basis, $S$ can be spanned by $B$. So any   $y_i\in S$ can be written as a linear combination of $B$, in this way:   $$\\y_1 = \beta_{11}v_1+\cdots+\beta_{n1}v_n\\y_2 =  \beta_{12}v_1+\cdots+\beta_{n2}v_n\\\cdots\\y_m =  \beta_{1m}v_1+\cdots+\beta_{nm}v_n$$ Substituting the system above in   $(1)$, we have:   $$\alpha_1(\beta_{11}v_1+\cdots+\beta_{n1}v_n)+\cdots+\alpha_m(\beta_{1m}v_1+\cdots+\beta_{nm}v_n)  = 0\implies\\ (\beta_{11}\alpha_1+\beta_{12}\alpha_2+\cdots+\beta_{1m}\alpha_m)v_1+\cdots+(\beta_{n1}\alpha_1+\beta_{n2}\alpha_2+\cdots+\beta_{nm}\alpha_m)v_m  = 0$$ But since $B$ is L.I., then the linear combination above implies the coefficients are all $0$. Therefore we end up with the system: $$\begin{cases}\beta_{11}\alpha_1+\beta_{12}\alpha_2+\cdots+\beta_{1m}\alpha_m = 0\\  \cdots \\  \beta_{n1}\alpha_1+\beta_{n2}\alpha_2+\cdots+\beta_{nm}\alpha_m = 0\end{cases}$$   that has $n$ equations, and $m$ variables, therefore is compatible and   indeterminated, that implies it has infinitely many solutions. One of   them is not trivial, so $S$ is L.D. I think wikipedia has a similar proof, but there, it proves directly that any basis has the same number of elements ( could you tell me why a vector $v_i$ suddenly appears in the middle of the proof ?. In this proof, the conclusion is that $S$ has at most $n$ elements. Is my proof correct? I understood it correctly? (I didn't copy, this is my understanding of the proof).","My teacher gave us this proof today, but I don't know if I understood it entirely: Theorem: Suppose $V$ a vector space (finitely generated) over the reals. $$B = \{v_1,\cdots, v_n\}$$ where $B$ is a basis for $V$. Suppose, also, a set $S\subset V$ L.I., then $S$ is finite and has at most $n$ elements. Proof: Let $S = \{y_1,\cdots,y_m\}$, with $m>n$. Suppose $S$ is L.I. Then:   $$\alpha_1y_1+\cdots+\alpha_my_m = 0\tag{1}$$ But since $S$ is greater   than $B$, and $B$ is a basis, $S$ can be spanned by $B$. So any   $y_i\in S$ can be written as a linear combination of $B$, in this way:   $$\\y_1 = \beta_{11}v_1+\cdots+\beta_{n1}v_n\\y_2 =  \beta_{12}v_1+\cdots+\beta_{n2}v_n\\\cdots\\y_m =  \beta_{1m}v_1+\cdots+\beta_{nm}v_n$$ Substituting the system above in   $(1)$, we have:   $$\alpha_1(\beta_{11}v_1+\cdots+\beta_{n1}v_n)+\cdots+\alpha_m(\beta_{1m}v_1+\cdots+\beta_{nm}v_n)  = 0\implies\\ (\beta_{11}\alpha_1+\beta_{12}\alpha_2+\cdots+\beta_{1m}\alpha_m)v_1+\cdots+(\beta_{n1}\alpha_1+\beta_{n2}\alpha_2+\cdots+\beta_{nm}\alpha_m)v_m  = 0$$ But since $B$ is L.I., then the linear combination above implies the coefficients are all $0$. Therefore we end up with the system: $$\begin{cases}\beta_{11}\alpha_1+\beta_{12}\alpha_2+\cdots+\beta_{1m}\alpha_m = 0\\  \cdots \\  \beta_{n1}\alpha_1+\beta_{n2}\alpha_2+\cdots+\beta_{nm}\alpha_m = 0\end{cases}$$   that has $n$ equations, and $m$ variables, therefore is compatible and   indeterminated, that implies it has infinitely many solutions. One of   them is not trivial, so $S$ is L.D. I think wikipedia has a similar proof, but there, it proves directly that any basis has the same number of elements ( could you tell me why a vector $v_i$ suddenly appears in the middle of the proof ?. In this proof, the conclusion is that $S$ has at most $n$ elements. Is my proof correct? I understood it correctly? (I didn't copy, this is my understanding of the proof).",,"['linear-algebra', 'vector-spaces', 'proof-verification']"
19,Maximum of a generalized Rayleigh quotient,Maximum of a generalized Rayleigh quotient,,"Given two symmetric positive definite matrices $A,B\in\mathbb{R}^{n\times n}$ and $x\in\mathbb{R}^n$. How do I prove that the generalized Rayleigh quotient $R(A,B,x):=\dfrac{x\cdot A\cdot x}{x\cdot B\cdot x}$ has a maximum $\lambda_{max}$, which corresponds to the largest eigenvalue of the generalized eigenvalue problem $A\cdot x=\lambda\cdot B\cdot x$? I have tried to complete the proof by substituting $B=C^2$ and $y=C\cdot x$ to get $\dfrac{x\cdot A\cdot x}{x\cdot B\cdot x}=\dfrac{y\cdot C^{-T}\cdot A\cdot C^{-1}\cdot y}{y\cdot y}$. Defining $D:=C^{-T}\cdot A\cdot C^{-1}$ we have $D=D^T$ and then it is simple to show that the maximum eigenvalue of $D$ is the maximum of the quotient by substituting $D=\sum\limits_j\lambda_jv_j\otimes v_j$ and $y=\sum\limits_jy_jv_j$ into the quotient. I can, however, not seem to prove that the eigenvalues of $D$ from $D\cdot v_j=\lambda_jv_j\quad\Leftrightarrow\quad C^{-T}\cdot A\cdot C^{-1}\cdot v_j=\lambda_jv_j\quad\Leftrightarrow\quad A\cdot C^{-1}\cdot v_j=\lambda_j\cdot C^T\cdot v_j$ correspond to the eigenvalues of $A\cdot v_j=\lambda_j\cdot C^2\cdot v_j\quad\Leftrightarrow\quad A\cdot v_j=\lambda_j\cdot B\cdot v_j$.","Given two symmetric positive definite matrices $A,B\in\mathbb{R}^{n\times n}$ and $x\in\mathbb{R}^n$. How do I prove that the generalized Rayleigh quotient $R(A,B,x):=\dfrac{x\cdot A\cdot x}{x\cdot B\cdot x}$ has a maximum $\lambda_{max}$, which corresponds to the largest eigenvalue of the generalized eigenvalue problem $A\cdot x=\lambda\cdot B\cdot x$? I have tried to complete the proof by substituting $B=C^2$ and $y=C\cdot x$ to get $\dfrac{x\cdot A\cdot x}{x\cdot B\cdot x}=\dfrac{y\cdot C^{-T}\cdot A\cdot C^{-1}\cdot y}{y\cdot y}$. Defining $D:=C^{-T}\cdot A\cdot C^{-1}$ we have $D=D^T$ and then it is simple to show that the maximum eigenvalue of $D$ is the maximum of the quotient by substituting $D=\sum\limits_j\lambda_jv_j\otimes v_j$ and $y=\sum\limits_jy_jv_j$ into the quotient. I can, however, not seem to prove that the eigenvalues of $D$ from $D\cdot v_j=\lambda_jv_j\quad\Leftrightarrow\quad C^{-T}\cdot A\cdot C^{-1}\cdot v_j=\lambda_jv_j\quad\Leftrightarrow\quad A\cdot C^{-1}\cdot v_j=\lambda_j\cdot C^T\cdot v_j$ correspond to the eigenvalues of $A\cdot v_j=\lambda_j\cdot C^2\cdot v_j\quad\Leftrightarrow\quad A\cdot v_j=\lambda_j\cdot B\cdot v_j$.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'symmetry', 'matrix-decomposition']"
20,How does the representation of co-vectors change if we change the basis of a vector space $V$?,How does the representation of co-vectors change if we change the basis of a vector space ?,V,"I'm trying to understand how vectors, differential forms and multi-linear maps in general transform under change of coordinates. So I start with the simplest case of vectors. Here's my own attempt, please bear with me: Suppose that $V$ is a vector space and $\alpha=\{v_1,\cdots,v_n\}$ and $\beta = \{w_1,\cdots,w_n\}$ are two ordered bases for $V$. $\alpha$ and $\beta$ give rise to the dual bases $\alpha^*=\{v^1,\cdots,v^n\}$ and $\beta^*=\{w^1,\cdots,w^n\}$ for $V^*$ respectively. If $[T]_{\beta}^{\alpha}=[\lambda_{i}^{j}]$ is the matrix representation of coordinate transformation from $\alpha$ to $\beta$, i.e. $$\begin{bmatrix} w_1 \\ \vdots \\ w_n \end{bmatrix}= \begin{bmatrix} \lambda_1^1 & \lambda_1^2 & \dots &\lambda_1^n \\ \vdots & \vdots & \ddots & \vdots \\ \lambda_n^1 & \lambda_n^2 & \cdots & \lambda_n^n\end{bmatrix} \begin{bmatrix} v_1 \\ \vdots \\ v_n \end{bmatrix}$$ What is the matrix of coordinate transformation from $\alpha^*$ to $\beta^*$? We can write $w^j \in \beta^*$ as a linear combination of basis elements in $\alpha^*$: $$w^j=\mu_{1}^{j}v^1+\cdots+\mu_n^{j}v^n$$ We get a matrix representation $[S]_{\beta^*}^{\alpha^*}=[\mu_{i}^{j}]$ as the following: $$\begin{bmatrix} w^1 & \cdots & w^n \end{bmatrix}= \begin{bmatrix} v^1 & \cdots & v^n \end{bmatrix}\begin{bmatrix} \mu_1^1 & \mu_1^2 & \dots &\mu_1^n \\ \vdots & \vdots & \ddots & \vdots \\ \mu_n^1 & \mu_n^2 & \cdots & \mu_n^n\end{bmatrix} $$ We know that $w_i = \lambda_{i}^1v_1+\cdots+\lambda_{i}^nv_n$. Evaluating this functional at $w_i \in V$ we get: $$w^j(w_i)=\mu_{1}^{j}v^1(w_i)+\cdots+\mu_n^{j}v^n(w_i)=\delta_{i}^j$$ $$w^j(w_i)=\mu_{1}^{j}v^1(\lambda_{i}^1v_1+\cdots+\lambda_{i}^nv_n)+\cdots+\mu_n^{j}v^n(\lambda_{i}^1v_1+\cdots+\lambda_{i}^nv_n)=\delta_{i}^j$$ $$w^j(w_i)=\mu_{1}^{j}\lambda_{i}^1+\cdots+\mu_n^{j}\lambda_{i}^n=\sum_{k=1}^n\mu_{k}^j \lambda_{i}^k=\delta_{i}^j$$ But $\sum_{k=1}^n\mu_{k}^j \lambda_{i}^k$ is the $(i,j)$ entry of the matrix product $TS$. Therefore $TS=I_n$ and $S=T^{-1}$. If we want to write down the transformation from $\alpha^*$ to $\beta^*$ as column vectors instead of row vector and name the new matrix that represents this transformation as $U$, we observe that $U=S^{t}$ and therefore $U=(T^{-1})^t$. Therefore if $T$ represents the transformation from $\alpha$ to $\beta$ by the equation $\mathbf{w}=T\mathbf{v}$, then $\mathbf{w^*}=U\mathbf{v^*}$. The important case is when $T=(T^{-1})^t$ which happens if and only if $T$ is an orthonormal transformation. What I don't understand is why the transformation from $\alpha$ to $\beta$ is called a contravariant transformation while the transformation from $\alpha^*$ to $\beta^*$ is called a covariant transformation .  Would you please elaborate on this important point? It's been driving me crazy for the last two days. Thanks in advance.","I'm trying to understand how vectors, differential forms and multi-linear maps in general transform under change of coordinates. So I start with the simplest case of vectors. Here's my own attempt, please bear with me: Suppose that $V$ is a vector space and $\alpha=\{v_1,\cdots,v_n\}$ and $\beta = \{w_1,\cdots,w_n\}$ are two ordered bases for $V$. $\alpha$ and $\beta$ give rise to the dual bases $\alpha^*=\{v^1,\cdots,v^n\}$ and $\beta^*=\{w^1,\cdots,w^n\}$ for $V^*$ respectively. If $[T]_{\beta}^{\alpha}=[\lambda_{i}^{j}]$ is the matrix representation of coordinate transformation from $\alpha$ to $\beta$, i.e. $$\begin{bmatrix} w_1 \\ \vdots \\ w_n \end{bmatrix}= \begin{bmatrix} \lambda_1^1 & \lambda_1^2 & \dots &\lambda_1^n \\ \vdots & \vdots & \ddots & \vdots \\ \lambda_n^1 & \lambda_n^2 & \cdots & \lambda_n^n\end{bmatrix} \begin{bmatrix} v_1 \\ \vdots \\ v_n \end{bmatrix}$$ What is the matrix of coordinate transformation from $\alpha^*$ to $\beta^*$? We can write $w^j \in \beta^*$ as a linear combination of basis elements in $\alpha^*$: $$w^j=\mu_{1}^{j}v^1+\cdots+\mu_n^{j}v^n$$ We get a matrix representation $[S]_{\beta^*}^{\alpha^*}=[\mu_{i}^{j}]$ as the following: $$\begin{bmatrix} w^1 & \cdots & w^n \end{bmatrix}= \begin{bmatrix} v^1 & \cdots & v^n \end{bmatrix}\begin{bmatrix} \mu_1^1 & \mu_1^2 & \dots &\mu_1^n \\ \vdots & \vdots & \ddots & \vdots \\ \mu_n^1 & \mu_n^2 & \cdots & \mu_n^n\end{bmatrix} $$ We know that $w_i = \lambda_{i}^1v_1+\cdots+\lambda_{i}^nv_n$. Evaluating this functional at $w_i \in V$ we get: $$w^j(w_i)=\mu_{1}^{j}v^1(w_i)+\cdots+\mu_n^{j}v^n(w_i)=\delta_{i}^j$$ $$w^j(w_i)=\mu_{1}^{j}v^1(\lambda_{i}^1v_1+\cdots+\lambda_{i}^nv_n)+\cdots+\mu_n^{j}v^n(\lambda_{i}^1v_1+\cdots+\lambda_{i}^nv_n)=\delta_{i}^j$$ $$w^j(w_i)=\mu_{1}^{j}\lambda_{i}^1+\cdots+\mu_n^{j}\lambda_{i}^n=\sum_{k=1}^n\mu_{k}^j \lambda_{i}^k=\delta_{i}^j$$ But $\sum_{k=1}^n\mu_{k}^j \lambda_{i}^k$ is the $(i,j)$ entry of the matrix product $TS$. Therefore $TS=I_n$ and $S=T^{-1}$. If we want to write down the transformation from $\alpha^*$ to $\beta^*$ as column vectors instead of row vector and name the new matrix that represents this transformation as $U$, we observe that $U=S^{t}$ and therefore $U=(T^{-1})^t$. Therefore if $T$ represents the transformation from $\alpha$ to $\beta$ by the equation $\mathbf{w}=T\mathbf{v}$, then $\mathbf{w^*}=U\mathbf{v^*}$. The important case is when $T=(T^{-1})^t$ which happens if and only if $T$ is an orthonormal transformation. What I don't understand is why the transformation from $\alpha$ to $\beta$ is called a contravariant transformation while the transformation from $\alpha^*$ to $\beta^*$ is called a covariant transformation .  Would you please elaborate on this important point? It's been driving me crazy for the last two days. Thanks in advance.",,"['linear-algebra', 'differential-geometry', 'tensors', 'multilinear-algebra']"
21,"Prove that the linear transformation is injective iff $T(f_1),\ldots,T(f_n)$ are linearly independent",Prove that the linear transformation is injective iff  are linearly independent,"T(f_1),\ldots,T(f_n)","$V$ and $W$ are vector spaces with $dim(V)=n$. Prove that a linear   transformation $T:V\rightarrow W$ is injective if and only if for a   basis $B=(f_1,\ldots,f_n)$ of $V$, $T(f_1),\ldots,T(f_n)$ are linearly   independent. I'm first trying to prove the forward direction, by assuming that $T:V\rightarrow W$ is injective and showing that $T(f_1),\ldots,T(f_n)$ are linearly independent. So, by assumption, for any $r_1,r_2\in V$, $T(r_1)=T(r_2)\rightarrow r_1=r_2$. However, I don't see how I can use this to show that $T(f_1),\ldots,T(f_n)$ are linearly independent. I have to show that only the trivial relation holds in $c_1T(f_1)+\cdots+c_nT(f_n)$ for scalars $c_1,\ldots,c_n$, so $c_1=\cdots=c_n=0$, but how does this following from the transformation being injective alone?","$V$ and $W$ are vector spaces with $dim(V)=n$. Prove that a linear   transformation $T:V\rightarrow W$ is injective if and only if for a   basis $B=(f_1,\ldots,f_n)$ of $V$, $T(f_1),\ldots,T(f_n)$ are linearly   independent. I'm first trying to prove the forward direction, by assuming that $T:V\rightarrow W$ is injective and showing that $T(f_1),\ldots,T(f_n)$ are linearly independent. So, by assumption, for any $r_1,r_2\in V$, $T(r_1)=T(r_2)\rightarrow r_1=r_2$. However, I don't see how I can use this to show that $T(f_1),\ldots,T(f_n)$ are linearly independent. I have to show that only the trivial relation holds in $c_1T(f_1)+\cdots+c_nT(f_n)$ for scalars $c_1,\ldots,c_n$, so $c_1=\cdots=c_n=0$, but how does this following from the transformation being injective alone?",,['linear-algebra']
22,What is the quickest way to find the characteristic polynomial of this matrix?,What is the quickest way to find the characteristic polynomial of this matrix?,,"Let $e_k$ be the $k$-th vector of the canonical base of $\mathbb R^n$ and let $$B = [e_2 \mid e_3 \mid \dots \mid e_n \mid e_1]$$ What it the quickest way to show that the charachteristic polynomial of $B$ is $\lambda^n - 1$? My work I can show that if $\lambda$ is an eigenvalue of $B$, then $\lambda^n = 1$ (trivially because $B^n = I$)but this does not suffice as those $\lambda$ can all be equal to one; I want to show that they are distinct. As I did not know how to do that, I resorted to use the fact (which is obvious, altough it should be proved more formally) that $tr(B^k) = 0$ for all $k < n$. Then, as the coefficients in the characteristic polynomial of $B$ (except the first and last one) are all polynomials in the variables $tr(B), tr(B^2), \dots , tr(B^{n-1})$ they are all equal to $0$. And since $\det(B) = (-1)^{n+1}$, we have $$P_B(\lambda) = \lambda^n + (-1)^n(-1)^{n+1} = \lambda^n - 1$$ But it seems like an overkill, especially because I do not know how to prove that those coefficients are indeed polynomials in $tr(B^i)$. Can someone suggest a more elegant way?","Let $e_k$ be the $k$-th vector of the canonical base of $\mathbb R^n$ and let $$B = [e_2 \mid e_3 \mid \dots \mid e_n \mid e_1]$$ What it the quickest way to show that the charachteristic polynomial of $B$ is $\lambda^n - 1$? My work I can show that if $\lambda$ is an eigenvalue of $B$, then $\lambda^n = 1$ (trivially because $B^n = I$)but this does not suffice as those $\lambda$ can all be equal to one; I want to show that they are distinct. As I did not know how to do that, I resorted to use the fact (which is obvious, altough it should be proved more formally) that $tr(B^k) = 0$ for all $k < n$. Then, as the coefficients in the characteristic polynomial of $B$ (except the first and last one) are all polynomials in the variables $tr(B), tr(B^2), \dots , tr(B^{n-1})$ they are all equal to $0$. And since $\det(B) = (-1)^{n+1}$, we have $$P_B(\lambda) = \lambda^n + (-1)^n(-1)^{n+1} = \lambda^n - 1$$ But it seems like an overkill, especially because I do not know how to prove that those coefficients are indeed polynomials in $tr(B^i)$. Can someone suggest a more elegant way?",,"['linear-algebra', 'matrices', 'polynomials', 'eigenvalues-eigenvectors']"
23,"Let U, W be subspaces of a vector space V. Suppose U ⊆ W. Prove or disprove: U + W = W","Let U, W be subspaces of a vector space V. Suppose U ⊆ W. Prove or disprove: U + W = W",,"So, I know that W + W = W. And it makes sense that there is no counterargument that the claim isn't true. So, here is my attempt: Claim: $U + W = W$ Proof: $$W \subset U + W$$ Let $w \in W$. Then $w = w + 0$, which is a sum of an element from $W$ and an element of $U$. Thus, $w \in W + U$, and hence $W \subset U + W$. $$U + W \subset W$$ By definition, $U + W = \{u + w ~:~ u \in U \text{ and } w \in W\}$. That is, every element $v \in U + W$ is of the form $v = u + w$, with $u \in U$ and $w \in W$. And since $U \subseteq W$, it follows that $u \in U$ implies $u \in W$. Thus, every element of $U + W$ is a sum of two elements of $W$, and hence $u + w \in W$ since $W$ is a subspace. Thus, $U + W \subset W$.","So, I know that W + W = W. And it makes sense that there is no counterargument that the claim isn't true. So, here is my attempt: Claim: $U + W = W$ Proof: $$W \subset U + W$$ Let $w \in W$. Then $w = w + 0$, which is a sum of an element from $W$ and an element of $U$. Thus, $w \in W + U$, and hence $W \subset U + W$. $$U + W \subset W$$ By definition, $U + W = \{u + w ~:~ u \in U \text{ and } w \in W\}$. That is, every element $v \in U + W$ is of the form $v = u + w$, with $u \in U$ and $w \in W$. And since $U \subseteq W$, it follows that $u \in U$ implies $u \in W$. Thus, every element of $U + W$ is a sum of two elements of $W$, and hence $u + w \in W$ since $W$ is a subspace. Thus, $U + W \subset W$.",,"['linear-algebra', 'proof-verification']"
24,Symmetrical of a triangle's vertexes,Symmetrical of a triangle's vertexes,,"I have the following problem : Show that the symmetrical (ie reflection) of a triangle's vertexes by the opposite side are aligned iff the distance between the orthocenter and the circumcenter is twice the circumradius. I made a few pictures with GeoGebra to try and find a way to solve that, but it didn't really help. When two of them ($A'$ and $C'$) are in the same place : When they are distinct and aligned :","I have the following problem : Show that the symmetrical (ie reflection) of a triangle's vertexes by the opposite side are aligned iff the distance between the orthocenter and the circumcenter is twice the circumradius. I made a few pictures with GeoGebra to try and find a way to solve that, but it didn't really help. When two of them ($A'$ and $C'$) are in the same place : When they are distinct and aligned :",,"['linear-algebra', 'geometry', 'circles', 'triangles']"
25,Second year mathematics textbook recommendations.,Second year mathematics textbook recommendations.,,"I am currently majoring in pure mathematics, and would like to purchase textbooks that would not only assist with content covered in the courses, but will serve as a great reference throughout my studies. Furthermore, if you believe that I will only ever use the textbook for the semester in which I am studying the subject, then please do not recommend it. Here are my second year mathematics courses: Several Variable Calculus Linear Algebra Theory of Statistics Mathematical Computing Fundamental Analysis and Abstract Algebra Theory and Application of Differential Equations Complex Analysis","I am currently majoring in pure mathematics, and would like to purchase textbooks that would not only assist with content covered in the courses, but will serve as a great reference throughout my studies. Furthermore, if you believe that I will only ever use the textbook for the semester in which I am studying the subject, then please do not recommend it. Here are my second year mathematics courses: Several Variable Calculus Linear Algebra Theory of Statistics Mathematical Computing Fundamental Analysis and Abstract Algebra Theory and Application of Differential Equations Complex Analysis",,"['linear-algebra', 'abstract-algebra']"
26,How to transfrom my equation to $Y=KX^2$,How to transfrom my equation to,Y=KX^2,"In general , $$\vec{C}(u)=\vec{a_0}+\vec{a_1} u+\vec{a_2} u^2$$ is a parabolic arc between the points $\vec{a_0}$ and $\vec{a_0} + \vec{a_1} + \vec{a_2}$. So I'd like to prove it by myself: My trial as below: $\vec{a_i}=(x_i,y_i)^T$ $\Rightarrow$ $$x=x_0+x_1 u+ x_2 u^2 \qquad (1)$$ $$y=y_0+y_1 u+ y_2 u^2 \qquad (2)$$ Obviously, (1) and (2) are the equations about $u,u^2$ So I can denote $u,u^2$ by $x,y$ $$u=p_1 x+q_1y+r_1$$ $$u^2=p_2 x+q_2y+r_2$$ $\Rightarrow$ $$p_2 x+q_2y+r_2=(p_1 x+q_1y+r_1)^2$$ Unfortunately,I didn't know what transformation I need to apply to $x,y$ in the following steps. Can someone help me?","In general , $$\vec{C}(u)=\vec{a_0}+\vec{a_1} u+\vec{a_2} u^2$$ is a parabolic arc between the points $\vec{a_0}$ and $\vec{a_0} + \vec{a_1} + \vec{a_2}$. So I'd like to prove it by myself: My trial as below: $\vec{a_i}=(x_i,y_i)^T$ $\Rightarrow$ $$x=x_0+x_1 u+ x_2 u^2 \qquad (1)$$ $$y=y_0+y_1 u+ y_2 u^2 \qquad (2)$$ Obviously, (1) and (2) are the equations about $u,u^2$ So I can denote $u,u^2$ by $x,y$ $$u=p_1 x+q_1y+r_1$$ $$u^2=p_2 x+q_2y+r_2$$ $\Rightarrow$ $$p_2 x+q_2y+r_2=(p_1 x+q_1y+r_1)^2$$ Unfortunately,I didn't know what transformation I need to apply to $x,y$ in the following steps. Can someone help me?",,['linear-algebra']
27,Prove that this Newton sum value is unique,Prove that this Newton sum value is unique,,"$$\begin{align}a+b+c+d&=1\\ a^2+b^2+c^2+d^2&=2\\ a^3+b^3+c^3+d^3&=3\\ a^4+b^4+c^4+d^4&=4\\ a^5+b^5+c^5+d^5&- ?\end{align}$$ The usual method I see for solving this kind of problems is bashing using the Newton sums. Let $P_n=a^n+b^n+c^n+d^n$. If we take $a,b,c,d$ to be the roots of $$f(x)=x^4-x^3-\frac{1}{2}x^2-\frac{1}{6}x+\frac{1}{24},$$ then they satisfy all the initial conditions and we also have $P_5=\frac{139}{24}$. But what this proves is that there exist $a,b,c,d$ such that the initial conditions are satisfied and $P_5=\frac{139}{24}$. This alone does not prove that this is the only possible value of $P_5$. How to prove that it is, generally? (It might be possible to find the value using only interesting kinds of factorization identities, which would show that the value is unique, but such strategy would not be as general as the Newton sums)","$$\begin{align}a+b+c+d&=1\\ a^2+b^2+c^2+d^2&=2\\ a^3+b^3+c^3+d^3&=3\\ a^4+b^4+c^4+d^4&=4\\ a^5+b^5+c^5+d^5&- ?\end{align}$$ The usual method I see for solving this kind of problems is bashing using the Newton sums. Let $P_n=a^n+b^n+c^n+d^n$. If we take $a,b,c,d$ to be the roots of $$f(x)=x^4-x^3-\frac{1}{2}x^2-\frac{1}{6}x+\frac{1}{24},$$ then they satisfy all the initial conditions and we also have $P_5=\frac{139}{24}$. But what this proves is that there exist $a,b,c,d$ such that the initial conditions are satisfied and $P_5=\frac{139}{24}$. This alone does not prove that this is the only possible value of $P_5$. How to prove that it is, generally? (It might be possible to find the value using only interesting kinds of factorization identities, which would show that the value is unique, but such strategy would not be as general as the Newton sums)",,"['linear-algebra', 'algebra-precalculus', 'roots']"
28,Any article expounding the difference between matrix analysis and functional analysis?,Any article expounding the difference between matrix analysis and functional analysis?,,"I do theoretical physics. For quantum mechanics, the mathematical foundation is rigorously functional analysis. However, people generally take matrix analysis (for finite dimensional vector spaces) to understand theorems and facts in quantum mechanics. But there must be a lot of fundamental difference between them. Is there any article expounding the difference? I would like to see the difference first and then plunge into functional analysis.","I do theoretical physics. For quantum mechanics, the mathematical foundation is rigorously functional analysis. However, people generally take matrix analysis (for finite dimensional vector spaces) to understand theorems and facts in quantum mechanics. But there must be a lot of fundamental difference between them. Is there any article expounding the difference? I would like to see the difference first and then plunge into functional analysis.",,"['linear-algebra', 'matrices', 'functional-analysis']"
29,Are isometries always linear?,Are isometries always linear?,,"Let $E$ be a finite dimensional vector space (over a field of characteristic zero) and $f : E \rightarrow E$ be an isometry fixing 0. Must $f$ be linear in this case ? Note : I am NOT assuming that the norm of $E$ comes from a quadratic form (otherwise I know the answer is yes, as per Should isometries be linear? ). I expect the answer to my question should be no, but I don't have any counter example.","Let $E$ be a finite dimensional vector space (over a field of characteristic zero) and $f : E \rightarrow E$ be an isometry fixing 0. Must $f$ be linear in this case ? Note : I am NOT assuming that the norm of $E$ comes from a quadratic form (otherwise I know the answer is yes, as per Should isometries be linear? ). I expect the answer to my question should be no, but I don't have any counter example.",,"['linear-algebra', 'normed-spaces']"
30,$(X/Y)^*$is isometrically isomorphic to $Y^⊥$,is isometrically isomorphic to,(X/Y)^* Y^⊥,"Let X be a Banach space with a closed subspace Y,  We define the dual mapping $ \pi^*:(X/Y)^*  → X^*$  by $\pi^* (\beta)=\beta\circ\pi$ then $(X/Y)^*$is isometrically isomorphic to $$Y^\perp:=\{f∈  X^*|f(Y)=\{0\}\}$$ we define $T:(X/Y)^*  → Y^\perp$. we must show that $T$ is a linear isomorphism such that $T$ is a isometry from $(X/Y)^*$ to  $Y^\perp $. $T=\pi^*$ (with restricted range). Note that for $ β∈(X/Y)^∗$, and $y∈Y$ we have $$π^∗β(y)=β(πy)=β(0)=0$$ Hence$ π^∗[(X/Y)^∗]⊆Y^⊥$.","Let X be a Banach space with a closed subspace Y,  We define the dual mapping $ \pi^*:(X/Y)^*  → X^*$  by $\pi^* (\beta)=\beta\circ\pi$ then $(X/Y)^*$is isometrically isomorphic to $$Y^\perp:=\{f∈  X^*|f(Y)=\{0\}\}$$ we define $T:(X/Y)^*  → Y^\perp$. we must show that $T$ is a linear isomorphism such that $T$ is a isometry from $(X/Y)^*$ to  $Y^\perp $. $T=\pi^*$ (with restricted range). Note that for $ β∈(X/Y)^∗$, and $y∈Y$ we have $$π^∗β(y)=β(πy)=β(0)=0$$ Hence$ π^∗[(X/Y)^∗]⊆Y^⊥$.",,"['linear-algebra', 'banach-spaces']"
31,Rank one plus diagonal matrix approximation,Rank one plus diagonal matrix approximation,,"Given $A \in R^{n \times n}$, $A$ symmetric. I'm trying to solve the following minimization problem: $\underset{u \in R^n, d \in R^n} \min \, \frac{1}{2} \|X - A\|_F^2$ subject to $X = u u^T + \mathrm{diag}(d)$. Is there a closed form solution to that problem? I'm not even sure if the problem is convex. I know that for best rank-r approximations there is SVD, but I'm not sure what to do with the additional diagonal part. EDIT: And what about the case when $diag(d)$ has to be positive definite?","Given $A \in R^{n \times n}$, $A$ symmetric. I'm trying to solve the following minimization problem: $\underset{u \in R^n, d \in R^n} \min \, \frac{1}{2} \|X - A\|_F^2$ subject to $X = u u^T + \mathrm{diag}(d)$. Is there a closed form solution to that problem? I'm not even sure if the problem is convex. I know that for best rank-r approximations there is SVD, but I'm not sure what to do with the additional diagonal part. EDIT: And what about the case when $diag(d)$ has to be positive definite?",,"['linear-algebra', 'matrices', 'optimization', 'convex-optimization']"
32,"let $\lambda, \mu$ be distinct eigenvalues of a $2 \times 2 $ matrix $A$.Then which of the following statements must be true?",let  be distinct eigenvalues of a  matrix .Then which of the following statements must be true?,"\lambda, \mu 2 \times 2  A","I was thinking about the following problem.. let $\lambda, \mu$ be distinct eigenvalues of a $2 \times 2 $ matrix $A$.Then which of the following statements must be true? a. $A^2$ has distinct eigenvalues. False .Counter Example: $\begin{pmatrix} 1 &0 \\  0 & -1 \end{pmatrix}$ b. $A^n$ is not a scalar multiple of identity for any integer $n$ False .Counter Example: $\begin{pmatrix} 1 &0 \\  0 & -1 \end{pmatrix}$ as this gives $A^2=1.I$ where $I=\begin{pmatrix} 1 &0 \\  0 & 1 \end{pmatrix}$ and so second statement is violated for $n=2$. Am I right? Please feel free to comment. Thanks and regards to all.","I was thinking about the following problem.. let $\lambda, \mu$ be distinct eigenvalues of a $2 \times 2 $ matrix $A$.Then which of the following statements must be true? a. $A^2$ has distinct eigenvalues. False .Counter Example: $\begin{pmatrix} 1 &0 \\  0 & -1 \end{pmatrix}$ b. $A^n$ is not a scalar multiple of identity for any integer $n$ False .Counter Example: $\begin{pmatrix} 1 &0 \\  0 & -1 \end{pmatrix}$ as this gives $A^2=1.I$ where $I=\begin{pmatrix} 1 &0 \\  0 & 1 \end{pmatrix}$ and so second statement is violated for $n=2$. Am I right? Please feel free to comment. Thanks and regards to all.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
33,Find basis such that bilinear forms attain normal form,Find basis such that bilinear forms attain normal form,,"Let $g :=$ symmetric bilinear form : $g(v,w) := \omega(v,Jw)$ with $J \in \textrm{O}(V,g)$ and $\omega := $ skew-symmetric bilinear form : $\omega(v,w) := g(v,Jw)$ with $J \in \textrm{Sp}(v,\omega)$. $J$ is defined as an almost complex structure such that $J \in \textrm{O}(V,g) : J^2 = -\textrm{id}$. The normal form of bilinear forms is defined as either $$J_{2r} = \begin{pmatrix} 0 & I_r \\ -I_r & 0 \end{pmatrix} \qquad \textrm{or} \qquad J_{2r} = \textrm{diag} \left( \begin{pmatrix} 0 & 1 \\ -1 & 0\end{pmatrix},...,\begin{pmatrix} 0 & 1 \\ -1 & 0\end{pmatrix} \right) $$ We also know the equality $B = A \cdot C$ for $A$ the gramian matrix associated to the basis $(v_i)$ of $g$, $B$ the matrix associated to $\omega$ and $C$ the matrix associated to $J$. Show that there exists a basis $(v_i)$ of the $n$-dimensional $\mathbb R$ vector space $V$ such that the gramian matrices for the non degenerate symmetric bilinear form $g$ and the symplectic form $\omega(,) := g( ,J())$ have normal form and show that $g(J(), )$ defines a symplectic structure as well. What is the relationship of $g(J(), )$ to $\omega$ ? So $g$ and $\omega$ are inverse to each other and since $g$ is non degenerate, $\omega$ is also non degenerate (follows from small lemma). To show that $g(J(), )$ also defines a symplectic structure, I tried to show that $g(Jv,w) = -g(Jw,v)$: $$g(Jv,w) \overset{\text{J orthogonal}}{=} g(J^2v, Jw) = g(-v, Jw) = -g(v, Jw) \overset{\text{g symmetric}}{=} -g(Jw, v).$$ So $g$ is alternating and since it is non degenerate by assumption it is symplectic, but how about showing the existence of the basis? Can anybody help???","Let $g :=$ symmetric bilinear form : $g(v,w) := \omega(v,Jw)$ with $J \in \textrm{O}(V,g)$ and $\omega := $ skew-symmetric bilinear form : $\omega(v,w) := g(v,Jw)$ with $J \in \textrm{Sp}(v,\omega)$. $J$ is defined as an almost complex structure such that $J \in \textrm{O}(V,g) : J^2 = -\textrm{id}$. The normal form of bilinear forms is defined as either $$J_{2r} = \begin{pmatrix} 0 & I_r \\ -I_r & 0 \end{pmatrix} \qquad \textrm{or} \qquad J_{2r} = \textrm{diag} \left( \begin{pmatrix} 0 & 1 \\ -1 & 0\end{pmatrix},...,\begin{pmatrix} 0 & 1 \\ -1 & 0\end{pmatrix} \right) $$ We also know the equality $B = A \cdot C$ for $A$ the gramian matrix associated to the basis $(v_i)$ of $g$, $B$ the matrix associated to $\omega$ and $C$ the matrix associated to $J$. Show that there exists a basis $(v_i)$ of the $n$-dimensional $\mathbb R$ vector space $V$ such that the gramian matrices for the non degenerate symmetric bilinear form $g$ and the symplectic form $\omega(,) := g( ,J())$ have normal form and show that $g(J(), )$ defines a symplectic structure as well. What is the relationship of $g(J(), )$ to $\omega$ ? So $g$ and $\omega$ are inverse to each other and since $g$ is non degenerate, $\omega$ is also non degenerate (follows from small lemma). To show that $g(J(), )$ also defines a symplectic structure, I tried to show that $g(Jv,w) = -g(Jw,v)$: $$g(Jv,w) \overset{\text{J orthogonal}}{=} g(J^2v, Jw) = g(-v, Jw) = -g(v, Jw) \overset{\text{g symmetric}}{=} -g(Jw, v).$$ So $g$ is alternating and since it is non degenerate by assumption it is symplectic, but how about showing the existence of the basis? Can anybody help???",,"['linear-algebra', 'bilinear-form', 'almost-complex']"
34,Characterisation of normal matrices,Characterisation of normal matrices,,"How to prove the following? Lemma. Let $C=[A,A^{\star}]$. $A$ is normal iff $[A,C]=0$. One direction is trivial. The other direction reduces to showing that $A^2 A^\star+A^\star A^2=0$ implies that $A$ is normal, but I don't see why that holds.","How to prove the following? Lemma. Let $C=[A,A^{\star}]$. $A$ is normal iff $[A,C]=0$. One direction is trivial. The other direction reduces to showing that $A^2 A^\star+A^\star A^2=0$ implies that $A$ is normal, but I don't see why that holds.",,"['linear-algebra', 'matrices']"
35,$W$ is a subspace of $\mathbb{R}^n$ and $K$ is a compact subset of $V$ with $W \cap K = \emptyset$.,is a subspace of  and  is a compact subset of  with .,W \mathbb{R}^n K V W \cap K = \emptyset,"Suppose $W$ is a subspace of $\mathbb{R}^n$ and $K$ is a compact subset of $V$ with $W \cap K = \emptyset$. Show that there exists a vector $v \in V$ such that $\langle v,w \rangle = 0$ for all $w\in W$ and $\langle w,x \rangle <0$ for all $x\in K$. There is also a hint for this problem: Define $A=\{k-w : k\in K, w\in W \}$ and use Farkas Lemma. I don't know how to apply the hint or how the fact that K is compact influences anything related to the Farkas Lemma.","Suppose $W$ is a subspace of $\mathbb{R}^n$ and $K$ is a compact subset of $V$ with $W \cap K = \emptyset$. Show that there exists a vector $v \in V$ such that $\langle v,w \rangle = 0$ for all $w\in W$ and $\langle w,x \rangle <0$ for all $x\in K$. There is also a hint for this problem: Define $A=\{k-w : k\in K, w\in W \}$ and use Farkas Lemma. I don't know how to apply the hint or how the fact that K is compact influences anything related to the Farkas Lemma.",,['linear-algebra']
36,$\mathfrak{so}(n)$ has trivial center when $n\geq 3$,has trivial center when,\mathfrak{so}(n) n\geq 3,"Is there a nice way to show that $$\mathfrak{so}(n)=\{A \in M(n,\mathbb{R}): A+A^t=0\} $$ has zero center for $n \geq 3$?","Is there a nice way to show that $$\mathfrak{so}(n)=\{A \in M(n,\mathbb{R}): A+A^t=0\} $$ has zero center for $n \geq 3$?",,"['linear-algebra', 'abstract-algebra', 'lie-algebras']"
37,"$F,G \in \text{End} (V)$ share the same eigenvalues for $F \circ G$ and $G \circ F$",share the same eigenvalues for  and,"F,G \in \text{End} (V) F \circ G G \circ F","Problem : Let $V$ be a finite dimensional Vector Space over a field $\mathbb{F}$ and $F,G \in \text{End}(V) $ Show that $F \circ G$ and $G \circ F$ have the same Eigenvalues $\lambda$ My approach : Let $\sigma_F:= \lbrace \lambda_i \in \mathbb{F} \mid  \lambda_i \text{ is a Eigenvalue of } F\rbrace $ Thus I have to show that $\sigma_{F \circ G} = \sigma_{ G \circ F} $ and because of the symmetry it is sufficient to show that for example $\sigma_{ F \circ G} \subset \sigma_{G \circ F} $ Suppose $\lambda$ is a Eigenvalue of $F \circ G$ and $v \in V \setminus \lbrace 0 \rbrace$ the related Eigenvector, it follows that for Case 1) $G(v) \neq 0$: $$ F(G(v))=\lambda v \implies (G\circ F)(Gv)=G(F(Gv))=G(\lambda v)=\lambda G(v) \\ \implies G(v) \text{ is a Eigenvector to $G \circ F$ with Eigenvalue $\lambda$}$$ I am sure the second part is just as easy, but I have been dealing with this problem for too long on my own without consulting help, so I guess I suffer from not seeing the forest for the trees . Case 2) Let $G(v) = 0$, $v$ as above it follows that: $$ F(G(v))=\lambda v = F(0)=0 \implies \lambda =0 \text{ because } v \neq 0  $$ It is clear to me that I cannot use $G(v)= 0$ as an Eigenvector of $G \circ F$ because $G(v)$ is now the zero vector and therefore, by definition, not a Eigenvector. Therefore I have to come up with a better statement $$\ker (G \circ F) \neq \lbrace 0 \rbrace \iff \lambda =0 \text{ is a Eigenvalue of } G \circ F \tag{*} $$ I have several problems now with the above statement (*), focussing on the $\implies$ direction Can I use the same Vector $v \in V \setminus \lbrace 0 \rbrace $ for this statement? It is not really clear to me if and why I can do this, I believe that I have to, because all I know for this case is that $G(v)=0 \implies v \in \ker \lbrace G \rbrace $ and applying $F$ to both sides would give $ F(G(v))=F(0)=0 \implies v \in \ker \lbrace F \circ G \rbrace $. If I for example just say let $ k \in \ker (G \circ F ) \neq \lbrace 0 \rbrace $ such that $k$ is an Eigenvector of $G \circ F$ with Eigenvalue $\lambda$ it would follow that $$G(F(k))= \lambda k = 0 \implies \lambda =0  $$ which seems very artificial (or forced up) to me, furthermore I have not even made use of my premise that $G(v) = 0$, so the above procedure must be wrong. On the other hand if I choose $v \in \ker \lbrace G \circ F\rbrace \neq \lbrace 0 \rbrace $ (which I don't know if I am allowed to do) it would follow that: $$G(F(v))=\lambda v=0 \implies \lambda =0  $$ which looks better to me, but just as artificial and again I have not made use of the premise.","Problem : Let $V$ be a finite dimensional Vector Space over a field $\mathbb{F}$ and $F,G \in \text{End}(V) $ Show that $F \circ G$ and $G \circ F$ have the same Eigenvalues $\lambda$ My approach : Let $\sigma_F:= \lbrace \lambda_i \in \mathbb{F} \mid  \lambda_i \text{ is a Eigenvalue of } F\rbrace $ Thus I have to show that $\sigma_{F \circ G} = \sigma_{ G \circ F} $ and because of the symmetry it is sufficient to show that for example $\sigma_{ F \circ G} \subset \sigma_{G \circ F} $ Suppose $\lambda$ is a Eigenvalue of $F \circ G$ and $v \in V \setminus \lbrace 0 \rbrace$ the related Eigenvector, it follows that for Case 1) $G(v) \neq 0$: $$ F(G(v))=\lambda v \implies (G\circ F)(Gv)=G(F(Gv))=G(\lambda v)=\lambda G(v) \\ \implies G(v) \text{ is a Eigenvector to $G \circ F$ with Eigenvalue $\lambda$}$$ I am sure the second part is just as easy, but I have been dealing with this problem for too long on my own without consulting help, so I guess I suffer from not seeing the forest for the trees . Case 2) Let $G(v) = 0$, $v$ as above it follows that: $$ F(G(v))=\lambda v = F(0)=0 \implies \lambda =0 \text{ because } v \neq 0  $$ It is clear to me that I cannot use $G(v)= 0$ as an Eigenvector of $G \circ F$ because $G(v)$ is now the zero vector and therefore, by definition, not a Eigenvector. Therefore I have to come up with a better statement $$\ker (G \circ F) \neq \lbrace 0 \rbrace \iff \lambda =0 \text{ is a Eigenvalue of } G \circ F \tag{*} $$ I have several problems now with the above statement (*), focussing on the $\implies$ direction Can I use the same Vector $v \in V \setminus \lbrace 0 \rbrace $ for this statement? It is not really clear to me if and why I can do this, I believe that I have to, because all I know for this case is that $G(v)=0 \implies v \in \ker \lbrace G \rbrace $ and applying $F$ to both sides would give $ F(G(v))=F(0)=0 \implies v \in \ker \lbrace F \circ G \rbrace $. If I for example just say let $ k \in \ker (G \circ F ) \neq \lbrace 0 \rbrace $ such that $k$ is an Eigenvector of $G \circ F$ with Eigenvalue $\lambda$ it would follow that $$G(F(k))= \lambda k = 0 \implies \lambda =0  $$ which seems very artificial (or forced up) to me, furthermore I have not even made use of my premise that $G(v) = 0$, so the above procedure must be wrong. On the other hand if I choose $v \in \ker \lbrace G \circ F\rbrace \neq \lbrace 0 \rbrace $ (which I don't know if I am allowed to do) it would follow that: $$G(F(v))=\lambda v=0 \implies \lambda =0  $$ which looks better to me, but just as artificial and again I have not made use of the premise.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'self-learning']"
38,Linear Algebra Self Study,Linear Algebra Self Study,,"I'm currently a high school student with a love for math. I have taken Plane and Coordinate Geometry, both Algebra I and II, Trigonometry, and am halfway done with Calc A. I want to major in quantum physics, and feel that a background with linear algebra would help. As there are no courses available at my school, I must self study. What seems most promising is the MIT OCW course along with the accompanying textbook. Would there be a better book/online resource for teaching myself? Thanks.","I'm currently a high school student with a love for math. I have taken Plane and Coordinate Geometry, both Algebra I and II, Trigonometry, and am halfway done with Calc A. I want to major in quantum physics, and feel that a background with linear algebra would help. As there are no courses available at my school, I must self study. What seems most promising is the MIT OCW course along with the accompanying textbook. Would there be a better book/online resource for teaching myself? Thanks.",,"['linear-algebra', 'self-learning']"
39,Wikipedia's proof of Schur Product Theorem,Wikipedia's proof of Schur Product Theorem,,"The Schur Product Theorem  basically states that the Hadamard product of two semidefinite matrices is semidefinite. The proof from Wikipedia : ==== Proof of positivity ==== Let $M = \sum \mu_i m_i m_i^T$ and $N = \sum \nu_i n_i n_i^T$ .  Then $$ M \circ N = \sum_{ij} \mu_i \nu_j (m_i m_i^T) \circ (n_j n_j^T) = \sum_{ij} \mu_i \nu_j (m_i \circ n_j) (m_i \circ n_j)^T $$ Each $(m_i \circ n_j) (m_i \circ n_j)^T$ is positive (but, except in the 1-dimensional case, not positive definite, since they are rank 1 matrices) and $\mu_i \nu_j > 0$ , thus the sum giving $M \circ N$ is also positive. Here, I am not sure how they got the form of $M$ and $N$ with the $\mu$ and $\nu$ in front. Could anyone be kind enough to give me an explanation of what is going on here? thank you!","The Schur Product Theorem  basically states that the Hadamard product of two semidefinite matrices is semidefinite. The proof from Wikipedia : ==== Proof of positivity ==== Let and .  Then Each is positive (but, except in the 1-dimensional case, not positive definite, since they are rank 1 matrices) and , thus the sum giving is also positive. Here, I am not sure how they got the form of and with the and in front. Could anyone be kind enough to give me an explanation of what is going on here? thank you!","M = \sum \mu_i m_i m_i^T N = \sum \nu_i n_i n_i^T 
M \circ N = \sum_{ij} \mu_i \nu_j (m_i m_i^T) \circ (n_j n_j^T) = \sum_{ij} \mu_i \nu_j (m_i \circ n_j) (m_i \circ n_j)^T
 (m_i \circ n_j) (m_i \circ n_j)^T \mu_i \nu_j > 0 M \circ N M N \mu \nu",['linear-algebra']
40,Randomly generate an matrix $A$ s.t. $A^m = I$,Randomly generate an matrix  s.t.,A A^m = I,"Fixed $n$, I want to randomly generate a $n \times n$ real matrix $A$ from the set: $\{A \in \mathcal{M}_{n \times n}(\mathbb{R}): \exists m \in \mathbb{N} \mbox{ s.t. } A^m = I\}$ I think I should first randomly generate a diagonal matrix $D$ such that $\det(D) = \pm 1$ and then randomly generate an invertible matrix $P$ and then compute $PDP^{-1}$. Is this method correct? Since $|\det(D)|=1$, I just generate a random $n$ unity ($+1$ or $-1$ for $\mathbb{R}$) to get the matrix $D$. But how can I randomly generate an invertible matrix $P$? To make the problem possible to solve, I should add a constraint like the matrix norm $\Vert P \Vert$ of P should satisfy: $0 < m \leq \Vert P \Vert \leq M$ for some constant $m,M$","Fixed $n$, I want to randomly generate a $n \times n$ real matrix $A$ from the set: $\{A \in \mathcal{M}_{n \times n}(\mathbb{R}): \exists m \in \mathbb{N} \mbox{ s.t. } A^m = I\}$ I think I should first randomly generate a diagonal matrix $D$ such that $\det(D) = \pm 1$ and then randomly generate an invertible matrix $P$ and then compute $PDP^{-1}$. Is this method correct? Since $|\det(D)|=1$, I just generate a random $n$ unity ($+1$ or $-1$ for $\mathbb{R}$) to get the matrix $D$. But how can I randomly generate an invertible matrix $P$? To make the problem possible to solve, I should add a constraint like the matrix norm $\Vert P \Vert$ of P should satisfy: $0 < m \leq \Vert P \Vert \leq M$ for some constant $m,M$",,"['linear-algebra', 'algorithms']"
41,Left Eigenvectors vs. Right Eigenvectors,Left Eigenvectors vs. Right Eigenvectors,,"Suppose we have a matrix $A$ and a symmetric invertible matrix $D$ such that $DA$ is symmetric.  The right eigenvectors of $A$ are $v_1,\cdots,v_n$ with eigenvalues $\lambda_1,\cdots, \lambda_n$. Can we use this information to derive (or estimate) left eigenvectors/eigenvalues of $A$?","Suppose we have a matrix $A$ and a symmetric invertible matrix $D$ such that $DA$ is symmetric.  The right eigenvectors of $A$ are $v_1,\cdots,v_n$ with eigenvalues $\lambda_1,\cdots, \lambda_n$. Can we use this information to derive (or estimate) left eigenvectors/eigenvalues of $A$?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
42,Proving that $a$ is eigenvalue of $p(T)$ iff $a=p(\lambda)$ for eigenvalue $\lambda$ of $T$,Proving that  is eigenvalue of  iff  for eigenvalue  of,a p(T) a=p(\lambda) \lambda T,"The solution is below, I just do not understand why if: $p(T)-aI$ is not injective, then $T-\lambda_jI$ is not injective for some j either. Also, what does repeatedly applying T to both sides accomplish? Thanks!","The solution is below, I just do not understand why if: $p(T)-aI$ is not injective, then $T-\lambda_jI$ is not injective for some j either. Also, what does repeatedly applying T to both sides accomplish? Thanks!",,['linear-algebra']
43,Basis with small $\ell_{\infty}$ norm for Subspace,Basis with small  norm for Subspace,\ell_{\infty},"At a high level, I'm interested in finding an orthonormal basis for a $d$-dimensional subspace $U \subset \mathbb{R}^n$ with small entry-wise $\ell_{\infty}$ norm. In particular, suppose $v_1, \ldots, v_d$ are the orthonormal basis vectors for $U$. Let me also write $x_i = (v_{1i}, \ldots, v_{di})^\intercal \in \mathbb{R}^d$ for $i \in \{1, \ldots, n\}$ (i.e. $x_i$ is the $i$th row of the matrix with basis vectors as columns). Then I would like to prove that: $ \max_i ||x_i||^2 \le \sum_{j=1}^d ||v_j||_{\infty}^2 \le \max_i ||x_i||^2 O(\textrm{polylog(d)}) $ The first inequality is true regardless of the choice of basis. The second inequality is not true for all orthonormal bases. So can one construct an orthonormal basis $\{v_i\}$ for $U$ such that the above inequality holds?","At a high level, I'm interested in finding an orthonormal basis for a $d$-dimensional subspace $U \subset \mathbb{R}^n$ with small entry-wise $\ell_{\infty}$ norm. In particular, suppose $v_1, \ldots, v_d$ are the orthonormal basis vectors for $U$. Let me also write $x_i = (v_{1i}, \ldots, v_{di})^\intercal \in \mathbb{R}^d$ for $i \in \{1, \ldots, n\}$ (i.e. $x_i$ is the $i$th row of the matrix with basis vectors as columns). Then I would like to prove that: $ \max_i ||x_i||^2 \le \sum_{j=1}^d ||v_j||_{\infty}^2 \le \max_i ||x_i||^2 O(\textrm{polylog(d)}) $ The first inequality is true regardless of the choice of basis. The second inequality is not true for all orthonormal bases. So can one construct an orthonormal basis $\{v_i\}$ for $U$ such that the above inequality holds?",,"['linear-algebra', 'matrices']"
44,"Matrix of a bilinear form <A,B> = tr(AB)","Matrix of a bilinear form <A,B> = tr(AB)",,"In revision for an upcoming exam, I've come across the following question: Let the bilinear form (A,B) be defined as tr(AB) on the space of 2x2 real matrices. Find an orthogonal basis for the form. I know that when working with vectors, with (x,y) = (xt)Ay that I find the matrix by taking the bilinear form of basis elements, but I'm not sure how to do that here when working with matrices. I also know I'm supposed to get a number from this form (cause its the trace) so I started considering the matrices as 4-d vectors then, although I'm really not sure if I'm on the right track or not. (Not too sure where to go from here) One thought that turned me against it was if I just write the four entries of the vector as the 1st row of the matrix, then 2nd row, I'm not sure how that will get me tr(AB) as opposed to the trace of A times the transpose of B. I do know how to use Gram-Schmidt, but I presume that comes afterwards? Help would be greatly appreciated, as this question has really exposed my lack of knowledge on this topic. Thanks","In revision for an upcoming exam, I've come across the following question: Let the bilinear form (A,B) be defined as tr(AB) on the space of 2x2 real matrices. Find an orthogonal basis for the form. I know that when working with vectors, with (x,y) = (xt)Ay that I find the matrix by taking the bilinear form of basis elements, but I'm not sure how to do that here when working with matrices. I also know I'm supposed to get a number from this form (cause its the trace) so I started considering the matrices as 4-d vectors then, although I'm really not sure if I'm on the right track or not. (Not too sure where to go from here) One thought that turned me against it was if I just write the four entries of the vector as the 1st row of the matrix, then 2nd row, I'm not sure how that will get me tr(AB) as opposed to the trace of A times the transpose of B. I do know how to use Gram-Schmidt, but I presume that comes afterwards? Help would be greatly appreciated, as this question has really exposed my lack of knowledge on this topic. Thanks",,"['linear-algebra', 'bilinear-form']"
45,prove that if $\lambda$ is an eigenvalue of T then $\bar\lambda$ is eigenvalue of $T^*$,prove that if  is an eigenvalue of T then  is eigenvalue of,\lambda \bar\lambda T^*,"I have to prove that if $\lambda$ is an eigenvalue of T then $\bar\lambda$ is eigenvalue of   $T^*$ (adjoint) I know that $<Tv,u> = <\lambda v,u> = \bar\lambda<v,u>=<v,\bar\lambda u>$ and that $<Tv,u>=<v,T^*u>$ but does this imply that there is a $u$ such that $T^*u=\bar\lambda u ?$ I believe something is wrong here. Any help?","I have to prove that if $\lambda$ is an eigenvalue of T then $\bar\lambda$ is eigenvalue of   $T^*$ (adjoint) I know that $<Tv,u> = <\lambda v,u> = \bar\lambda<v,u>=<v,\bar\lambda u>$ and that $<Tv,u>=<v,T^*u>$ but does this imply that there is a $u$ such that $T^*u=\bar\lambda u ?$ I believe something is wrong here. Any help?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'inner-products', 'adjoint-operators']"
46,Instantaneous Axis Of Rotation,Instantaneous Axis Of Rotation,,What is the mathematical theory behind finding a Instaneous Axis/Centre of rotation for a rotation plus translation motion?  Please explain in terms of the group of Rigid Motions(if possible) in $R^2$ & $R^3$ as why always such a centre will exist and why can a rigid motion can be described in terms of only a rotation about that point instead of a rotation plus translation. http://en.wikipedia.org/wiki/Instant_centre_of_rotation,What is the mathematical theory behind finding a Instaneous Axis/Centre of rotation for a rotation plus translation motion?  Please explain in terms of the group of Rigid Motions(if possible) in $R^2$ & $R^3$ as why always such a centre will exist and why can a rigid motion can be described in terms of only a rotation about that point instead of a rotation plus translation. http://en.wikipedia.org/wiki/Instant_centre_of_rotation,,"['linear-algebra', 'group-theory']"
47,Proof that $V = \sum_k U_k$ and $\dim V=\sum_k \dim U_k$ implies $V=\oplus_k U_k$,Proof that  and  implies,V = \sum_k U_k \dim V=\sum_k \dim U_k V=\oplus_k U_k,"Question regarding ""Linear algebra done right"", proposition $2.19$: Proposition $2.19$: Suppose $V$ is finite dimensional and $U_1,...,U_m$ are subspaces of $V$ such that    $$V = U_1 + \dots + U_m\tag{2.20}$$   and   $$\dim V=\dim \, U_1 + \dots + \dim \, U_m.\tag{2.21}$$   Then $V = U_1 \oplus \dots \oplus U_m.$ Proof: Choose a basis for each $U_j$. Put these bases together in one list, forming a list that spans $V$ (by $2.20$) and has length $\dim V$ (by $2.21$). Thus this list is a basis of $V$ (by $2.16$), and in particular it is linearly independent. Now suppose that $u_1 \in U_1,...,u_m \in U_m$ are such that $0=u_1+ ··· +u_m$. We can write each $u_j$ as a linear combination of the basis vectors (chosen above) of $U_j$. Substituting these linear combinations into the expression above, we have written $0$ as a linear combination of the basis of $V$ constructed above. Thus all the scalars used in this linear combination must be $0$. My question Given $(v_1, \ldots, v_n)$ as the basis vectors: \begin{align} u_1 &= a_{11}v_1 + \ldots + a_{1n}v_n \\ & \ \vdots \\ u_m &= a_{m1}v_1 + \ldots + a_{mn}v_n. \end{align} It seems the proof only proves $0 = (a_{11} + \ldots + a_{m1}) = \ldots = (a_{1n} + \ldots + a_{mn})$. But how do we know that each $a_{ij}$ is $0$ too? Thanks.","Question regarding ""Linear algebra done right"", proposition $2.19$: Proposition $2.19$: Suppose $V$ is finite dimensional and $U_1,...,U_m$ are subspaces of $V$ such that    $$V = U_1 + \dots + U_m\tag{2.20}$$   and   $$\dim V=\dim \, U_1 + \dots + \dim \, U_m.\tag{2.21}$$   Then $V = U_1 \oplus \dots \oplus U_m.$ Proof: Choose a basis for each $U_j$. Put these bases together in one list, forming a list that spans $V$ (by $2.20$) and has length $\dim V$ (by $2.21$). Thus this list is a basis of $V$ (by $2.16$), and in particular it is linearly independent. Now suppose that $u_1 \in U_1,...,u_m \in U_m$ are such that $0=u_1+ ··· +u_m$. We can write each $u_j$ as a linear combination of the basis vectors (chosen above) of $U_j$. Substituting these linear combinations into the expression above, we have written $0$ as a linear combination of the basis of $V$ constructed above. Thus all the scalars used in this linear combination must be $0$. My question Given $(v_1, \ldots, v_n)$ as the basis vectors: \begin{align} u_1 &= a_{11}v_1 + \ldots + a_{1n}v_n \\ & \ \vdots \\ u_m &= a_{m1}v_1 + \ldots + a_{mn}v_n. \end{align} It seems the proof only proves $0 = (a_{11} + \ldots + a_{m1}) = \ldots = (a_{1n} + \ldots + a_{mn})$. But how do we know that each $a_{ij}$ is $0$ too? Thanks.",,"['linear-algebra', 'linear-transformations', 'direct-sum']"
48,Eigenvalues involving the product of diagonal and Hermitian matrices,Eigenvalues involving the product of diagonal and Hermitian matrices,,"I have a question with regards to the eigenvalues involving the product of diagonal and Hermitian matrices. Let $A$ be a Hermitian matrix and $B$ be a diagonal matrix. I am seeking to express the eigenvalues of the matrices $ABA$ and $BAB$ in terms of the eigenvalues of $A$ and $B$. While I am able to find the eigenvalues of the two matrices under some very special conditions, I would like to know if there is any way of relating the eigenvalues of the matrices $ABA$ and $BAB$ in terms of the eigenvalues of $A$ and $B$ in general.","I have a question with regards to the eigenvalues involving the product of diagonal and Hermitian matrices. Let $A$ be a Hermitian matrix and $B$ be a diagonal matrix. I am seeking to express the eigenvalues of the matrices $ABA$ and $BAB$ in terms of the eigenvalues of $A$ and $B$. While I am able to find the eigenvalues of the two matrices under some very special conditions, I would like to know if there is any way of relating the eigenvalues of the matrices $ABA$ and $BAB$ in terms of the eigenvalues of $A$ and $B$ in general.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
49,What is the underlying structure that makes this analogy so good?,What is the underlying structure that makes this analogy so good?,,"In ""Linear Algebra Done Right"", the author draws (in my opinion) a fantastic parallel between $\mathbb{C}$ and $\mathcal{L}(V)$ (where $V$ is an $\mathbb{F}$-inner product space).  In this analogy, he establishes: A complex number $z$ corresponds to an operator $T$, The conjugate $\overline{z}$ corresponds to the adjoint $T^*$, The complex number $z$ is real corresponds to $T$ being self-adjoint, The complex number $z$ is non-negative corresponds to $T$ being positive-semidefinite, The complex number $z$ satisfies $|z| = 1$ corresponds to $T$ being an isometry ($TT^* = I$), among others.  I'm curious though if there is some underlying structure linking $\mathbb{C}$ and $\mathcal{L}(V)$ that makes the parallel so great (I know they are both vector spaces), or is this just a coincidental observation by the author?","In ""Linear Algebra Done Right"", the author draws (in my opinion) a fantastic parallel between $\mathbb{C}$ and $\mathcal{L}(V)$ (where $V$ is an $\mathbb{F}$-inner product space).  In this analogy, he establishes: A complex number $z$ corresponds to an operator $T$, The conjugate $\overline{z}$ corresponds to the adjoint $T^*$, The complex number $z$ is real corresponds to $T$ being self-adjoint, The complex number $z$ is non-negative corresponds to $T$ being positive-semidefinite, The complex number $z$ satisfies $|z| = 1$ corresponds to $T$ being an isometry ($TT^* = I$), among others.  I'm curious though if there is some underlying structure linking $\mathbb{C}$ and $\mathcal{L}(V)$ that makes the parallel so great (I know they are both vector spaces), or is this just a coincidental observation by the author?",,"['linear-algebra', 'soft-question']"
50,prove: $A+E$ is not invertible.,prove:  is not invertible.,A+E,"Suppose square matrix $A$ with order-n, and $A^TA=E,|A|=-1$, prove: $A+E$ is not invertible. Here, $A^T$ is transpose, $|A|$ is determinant, $E$ is identity matrix. Prove: we will show $|A+E|=0$. \begin{align}|A+E|=\left|A+A^T A\right|=\left|A\left(E+A^T\right)\right|=\left|A\left\|E+A^T\right.\right|=-\left|E+A^T\right|\end{align} \begin{align}|A+E|=\left|(A+E)^T\right|=\left|A^T+E\right|=\left|E+A^T\right|\end{align} so we've done. Any problems? is that all?","Suppose square matrix $A$ with order-n, and $A^TA=E,|A|=-1$, prove: $A+E$ is not invertible. Here, $A^T$ is transpose, $|A|$ is determinant, $E$ is identity matrix. Prove: we will show $|A+E|=0$. \begin{align}|A+E|=\left|A+A^T A\right|=\left|A\left(E+A^T\right)\right|=\left|A\left\|E+A^T\right.\right|=-\left|E+A^T\right|\end{align} \begin{align}|A+E|=\left|(A+E)^T\right|=\left|A^T+E\right|=\left|E+A^T\right|\end{align} so we've done. Any problems? is that all?",,"['linear-algebra', 'matrices', 'determinant']"
51,There is a subspace $W$ of $V$ such that $V = U \oplus W$,There is a subspace  of  such that,W V V = U \oplus W,"I have a question about the following proof of the statement that for every subspace $U$ of a finite dimensional vector space $V$ there is a subspace $W$ of $V$ such that $V = U \oplus W$. Proof: Because $V$ is finite-dimensional, so is $U$. Thus there is a basis $(u_1,\ldots,u_m)$ of U. Of course $(u_1,\ldots,u_m)$ is a linearly independent list of vectors in $V$, and thus it can be extended to a basis $(u_1,\ldots,u_m, w_1, \ldots,w_n)$ of $V$. Let $W = \text{span}(w_1,\ldots,w_n).$ To prove that $V = U \oplus W$, we need to show that $$V = U + W \text{ and } U \cap W = \{0\}.$$ To prove the first equation, suppose that $v \in V$. Then, because the list $(u_1,\ldots,u_m, w_1, \ldots,w_n)$ spans $V$, there exist scalars $a_1,\ldots,a_m,b_1,\ldots,b_n \in \mathbb{F}$ such that $$v = \underbrace{a_1u_1 + \dotsb + a_mu_m} + \underbrace{b_1w_1 + \dotsb + b_nw_n}.$$ In other words, we have $v = u + w$, where $u \in U$ and $w \in W$ are defined as above. Thus $v \in U + W$. I understand everything up until this point, the next part of the proof is the portion that I wish to clarify. To show that $U \cap W = \{0\}$, suppose $v \in U \cap W$. Then there exist scalars $a_1,\ldots,a_m,b_1,...,b_n \in \mathbb{F}$ such that $$v = a_1u_1 + \dotsb+a_mu_m = b_1w_1 + \dotsb + b_nw_n. $$ Halt! See the above. How does the notion that $v \in U\cap W$ lead to the equality $v = a_1u_1 + \dotsb+a_mu_m = b_1w_1 + \dotsb + b_nw_n$? Or how does the containment of $v$ in  $U$ and $W$ yield the equality $v = a_1u_1 + \dotsb+a_mu_m = b_1w_1 + \dotsb + b_nw_n$?","I have a question about the following proof of the statement that for every subspace $U$ of a finite dimensional vector space $V$ there is a subspace $W$ of $V$ such that $V = U \oplus W$. Proof: Because $V$ is finite-dimensional, so is $U$. Thus there is a basis $(u_1,\ldots,u_m)$ of U. Of course $(u_1,\ldots,u_m)$ is a linearly independent list of vectors in $V$, and thus it can be extended to a basis $(u_1,\ldots,u_m, w_1, \ldots,w_n)$ of $V$. Let $W = \text{span}(w_1,\ldots,w_n).$ To prove that $V = U \oplus W$, we need to show that $$V = U + W \text{ and } U \cap W = \{0\}.$$ To prove the first equation, suppose that $v \in V$. Then, because the list $(u_1,\ldots,u_m, w_1, \ldots,w_n)$ spans $V$, there exist scalars $a_1,\ldots,a_m,b_1,\ldots,b_n \in \mathbb{F}$ such that $$v = \underbrace{a_1u_1 + \dotsb + a_mu_m} + \underbrace{b_1w_1 + \dotsb + b_nw_n}.$$ In other words, we have $v = u + w$, where $u \in U$ and $w \in W$ are defined as above. Thus $v \in U + W$. I understand everything up until this point, the next part of the proof is the portion that I wish to clarify. To show that $U \cap W = \{0\}$, suppose $v \in U \cap W$. Then there exist scalars $a_1,\ldots,a_m,b_1,...,b_n \in \mathbb{F}$ such that $$v = a_1u_1 + \dotsb+a_mu_m = b_1w_1 + \dotsb + b_nw_n. $$ Halt! See the above. How does the notion that $v \in U\cap W$ lead to the equality $v = a_1u_1 + \dotsb+a_mu_m = b_1w_1 + \dotsb + b_nw_n$? Or how does the containment of $v$ in  $U$ and $W$ yield the equality $v = a_1u_1 + \dotsb+a_mu_m = b_1w_1 + \dotsb + b_nw_n$?",,['linear-algebra']
52,A basic question on linear maps and upper triangular form,A basic question on linear maps and upper triangular form,,Let $S$ and $T$ be two linear maps from $V$ to $V$ ($V$ complex vector space) such that $ST=TS$. I need to prove that there exists a basis with respect to which both the matrices are in upper-triangular form. How to proceed ? Because $V$ is a complex vector space there exists a basis wrt to which any linear map can be upper triangularized. But how to prove that both $S$ and $T$ can be upper triangularized wrt to the same basis.,Let $S$ and $T$ be two linear maps from $V$ to $V$ ($V$ complex vector space) such that $ST=TS$. I need to prove that there exists a basis with respect to which both the matrices are in upper-triangular form. How to proceed ? Because $V$ is a complex vector space there exists a basis wrt to which any linear map can be upper triangularized. But how to prove that both $S$ and $T$ can be upper triangularized wrt to the same basis.,,['linear-algebra']
53,Characterising spaces of linear recurrent sequences.,Characterising spaces of linear recurrent sequences.,,"Let $K$ be a field and $\def\N{\mathbf N}K^\N$ the infinite dimensional space of all sequences of elements of$~K$. Any linear recurrence relation of order $d$ with constant coefficients $$ a_{i+d} = c_0a_i+c_1a_{i+1}+\cdots c_{d-1}a_{i+d-1}   \qquad\text{for all $i\in\N$,} $$ defines a $d$-dimensional subspace $V$ of $K^\N$. In studying these sequences one introduces the shift operator $T:(a_0,a_1,a_2,\ldots)\mapsto(a_1,a_2,a_3,\ldots)$, which defines an endomorphism of $V$, and whose (generalised) eigenspace decomposition is useful for describing linear recurrent sequences explicitly. The shift operator $T$ is actually well defined on all of $K^\N$, and can be used for instance to characterise geometric sequences as its eigenvectors. For the indicated application to linear recurrences, it is of vital importance that each space$~V$ defined by a specific recurrence relation is $T$-stable, and this is indeed always the case for the recurrence relations considered (the fact that the coefficients of the recurrence relation are constant is essential for this). Now I was just wondering if this precisely describes the finite dimensional $T$-stable subspaces of $K^\N$: If $V$ is a finite dimensional $T$-stable subspace of $K^\N$, does there exist a linear recurrence relation$~R$ of order $\dim V$ with constant coefficients such that $V$ consists of all sequences in$~K^\N$ satisfying$~R$? I think the answer should be affirmative; maybe there is some elegant way to see this easily.","Let $K$ be a field and $\def\N{\mathbf N}K^\N$ the infinite dimensional space of all sequences of elements of$~K$. Any linear recurrence relation of order $d$ with constant coefficients $$ a_{i+d} = c_0a_i+c_1a_{i+1}+\cdots c_{d-1}a_{i+d-1}   \qquad\text{for all $i\in\N$,} $$ defines a $d$-dimensional subspace $V$ of $K^\N$. In studying these sequences one introduces the shift operator $T:(a_0,a_1,a_2,\ldots)\mapsto(a_1,a_2,a_3,\ldots)$, which defines an endomorphism of $V$, and whose (generalised) eigenspace decomposition is useful for describing linear recurrent sequences explicitly. The shift operator $T$ is actually well defined on all of $K^\N$, and can be used for instance to characterise geometric sequences as its eigenvectors. For the indicated application to linear recurrences, it is of vital importance that each space$~V$ defined by a specific recurrence relation is $T$-stable, and this is indeed always the case for the recurrence relations considered (the fact that the coefficients of the recurrence relation are constant is essential for this). Now I was just wondering if this precisely describes the finite dimensional $T$-stable subspaces of $K^\N$: If $V$ is a finite dimensional $T$-stable subspace of $K^\N$, does there exist a linear recurrence relation$~R$ of order $\dim V$ with constant coefficients such that $V$ consists of all sequences in$~K^\N$ satisfying$~R$? I think the answer should be affirmative; maybe there is some elegant way to see this easily.",,"['linear-algebra', 'recurrence-relations']"
54,Inner Product on Division Algebras,Inner Product on Division Algebras,,"Here , Wikipedia gives a proof that the only finite dimensional associative division algebras over $\mathbb{R}$ are $\mathbb{R}, \mathbb{C}, \mathbb{H}$. The proof proceeds by taking such a division algebra $D$, observing that multiplication by  $d \in D$ gives an endomorphism of $D$ which can be represented by a matrix corresponding to $d$. Then, we consider $V \subset D$, the subspace of $D$ corresponding to matrices with trace $0$. At some point, an inner product of $V$ given by $\langle a, b \rangle=-ab-ba$ appears. At my current understanding (undergraduate mathematics) this inner product seems mysterious. I can prove that it works but I don't understand why that is a natural choice. Can someone give me some insight? Edit. Is there maybe a way to think about this result using Lie Theory?","Here , Wikipedia gives a proof that the only finite dimensional associative division algebras over $\mathbb{R}$ are $\mathbb{R}, \mathbb{C}, \mathbb{H}$. The proof proceeds by taking such a division algebra $D$, observing that multiplication by  $d \in D$ gives an endomorphism of $D$ which can be represented by a matrix corresponding to $d$. Then, we consider $V \subset D$, the subspace of $D$ corresponding to matrices with trace $0$. At some point, an inner product of $V$ given by $\langle a, b \rangle=-ab-ba$ appears. At my current understanding (undergraduate mathematics) this inner product seems mysterious. I can prove that it works but I don't understand why that is a natural choice. Can someone give me some insight? Edit. Is there maybe a way to think about this result using Lie Theory?",,"['linear-algebra', 'abstract-algebra', 'matrices']"
55,$AX = 0$ and $BX = 0 $ implies A and B are row equivalent,and  implies A and B are row equivalent,AX = 0 BX = 0 ,"I'm trying to prove that if the systems $AX = 0$ and $BX=0$ are equivalent, then the matrices $A$ and $B$ are row equivalent. Proving the converse was very simple, but this one seems harder. I saw a few answers to similar questions posted here before, but they were done using some higher level concepts which I am unaware  of right now (such as orthogonal subspaces etc.). I am familiar with the concepts of linear equations, ranks of matrices, and the basic idea of subspaces(only, definition, dimension, basis etc.) Any help would be appreciated.........","I'm trying to prove that if the systems $AX = 0$ and $BX=0$ are equivalent, then the matrices $A$ and $B$ are row equivalent. Proving the converse was very simple, but this one seems harder. I saw a few answers to similar questions posted here before, but they were done using some higher level concepts which I am unaware  of right now (such as orthogonal subspaces etc.). I am familiar with the concepts of linear equations, ranks of matrices, and the basic idea of subspaces(only, definition, dimension, basis etc.) Any help would be appreciated.........",,['linear-algebra']
56,Computing invariant factors from Smith normal form,Computing invariant factors from Smith normal form,,"The goal is to find the Jordan Canonical Form of the matrix $$A=\begin{bmatrix}2&1&1&2\\0&2&0&1\\0&0&2&-1\\0&0&0&1\end{bmatrix}$$ Since the matrix is already upper-triangular, it's obvious that the eigenvalues are 2 and 1, where 1 has geometric and algebraic multiplicity 1, so that I could easily find the JCF by computing $\operatorname{rank}{(A-2I)^{i}}$ for each $i$. However, I thought I would instead try to do it by computing the invariant factors by finding the Smith normal form of the characteristic matrix $xI-A$. The problem is that using elementary row and column operations, I only seem able to obtain the matrix $$\begin{bmatrix}1&0&0&0\\0&1&0&0\\0&0&(x-2)^2&0\\0&0&0&(x-1)(x-2)\end{bmatrix}$$ My questions are: Even though this matrix is not in Smith normal form, is it valid to conclude that the elementary divisors are the powers of the irreducible factors that appear in each diagonal entry, i.e. $(x-1)$, $(x-2)^2$, and $(x-2)$? It happens to be true for this $A$, but would it always be true? How do I coax the above matrix into Smith normal form?","The goal is to find the Jordan Canonical Form of the matrix $$A=\begin{bmatrix}2&1&1&2\\0&2&0&1\\0&0&2&-1\\0&0&0&1\end{bmatrix}$$ Since the matrix is already upper-triangular, it's obvious that the eigenvalues are 2 and 1, where 1 has geometric and algebraic multiplicity 1, so that I could easily find the JCF by computing $\operatorname{rank}{(A-2I)^{i}}$ for each $i$. However, I thought I would instead try to do it by computing the invariant factors by finding the Smith normal form of the characteristic matrix $xI-A$. The problem is that using elementary row and column operations, I only seem able to obtain the matrix $$\begin{bmatrix}1&0&0&0\\0&1&0&0\\0&0&(x-2)^2&0\\0&0&0&(x-1)(x-2)\end{bmatrix}$$ My questions are: Even though this matrix is not in Smith normal form, is it valid to conclude that the elementary divisors are the powers of the irreducible factors that appear in each diagonal entry, i.e. $(x-1)$, $(x-2)^2$, and $(x-2)$? It happens to be true for this $A$, but would it always be true? How do I coax the above matrix into Smith normal form?",,"['linear-algebra', 'matrices', 'smith-normal-form']"
57,$PGL(n + 1)$ acts on sets of $n + 2$ points in $\mathbb{P}^n$ transitively: proof without determinants?,acts on sets of  points in  transitively: proof without determinants?,PGL(n + 1) n + 2 \mathbb{P}^n,"It is ""well known"" that if $p_1, \dots, p_{n + 1}$ are points in $\mathbb{P}^{n -1}$ (over $\mathbb{C}$, say) in general position, and $q_1, \dots, q_{n + 1}$ are another set of such points, then there is a unique matrix in $G \in PGL(n - 1)$ sending the first set to the second. Not wishing to take the author's assertion for granted, I came up with the following proof. Take $p_i$ to be the standard basis vector in $\mathbb{C}^n$ for $1 \leq i \leq n$, and $p_{n + 1} = (1, 1, \cdots, 1)$. Let $A$ be the $n \times n$ matrix constructed by taking the first $n$ of the $q$'s as column vectors, then we must have $G = A D$ for some diagonal matrix $D$, none of whose entries is $0$. Let $d$ be the vector consisting of the diagonal elements of $D$, and $v$ be the vector consisting of the coordinates of $q_{n + 1}$. Then the requirement that $G$ send $p_{n + 1}$ to $q_{n + 1}$ amounts to solving the equation $A d = v$. We see that $A$ must be invertible, and writing $d = A^{-1} v$ and using the cofactor expansion for $A^{-1}$, the condition that each $d_i$ be nonzero can be seen to be equivalent to the condition that the points other than $p_i$ are in general position: $A^{-1} v$ is equal, up to a scalar, to the vector of the $n$ relevant matrix determinants. This proof is fine, and perhaps optimal in some sense: each of the hypotheses gets used exactly once. But it requires an arbitrary choice at the beginning, and relies on messy formulas for matrix determinants. Can this be avoided?","It is ""well known"" that if $p_1, \dots, p_{n + 1}$ are points in $\mathbb{P}^{n -1}$ (over $\mathbb{C}$, say) in general position, and $q_1, \dots, q_{n + 1}$ are another set of such points, then there is a unique matrix in $G \in PGL(n - 1)$ sending the first set to the second. Not wishing to take the author's assertion for granted, I came up with the following proof. Take $p_i$ to be the standard basis vector in $\mathbb{C}^n$ for $1 \leq i \leq n$, and $p_{n + 1} = (1, 1, \cdots, 1)$. Let $A$ be the $n \times n$ matrix constructed by taking the first $n$ of the $q$'s as column vectors, then we must have $G = A D$ for some diagonal matrix $D$, none of whose entries is $0$. Let $d$ be the vector consisting of the diagonal elements of $D$, and $v$ be the vector consisting of the coordinates of $q_{n + 1}$. Then the requirement that $G$ send $p_{n + 1}$ to $q_{n + 1}$ amounts to solving the equation $A d = v$. We see that $A$ must be invertible, and writing $d = A^{-1} v$ and using the cofactor expansion for $A^{-1}$, the condition that each $d_i$ be nonzero can be seen to be equivalent to the condition that the points other than $p_i$ are in general position: $A^{-1} v$ is equal, up to a scalar, to the vector of the $n$ relevant matrix determinants. This proof is fine, and perhaps optimal in some sense: each of the hypotheses gets used exactly once. But it requires an arbitrary choice at the beginning, and relies on messy formulas for matrix determinants. Can this be avoided?",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'linear-groups']"
58,Bessel equation relation,Bessel equation relation,,"Define Bessel function as: $$J_a(x)=\sum_{n=0}^{\infty}{{(-1)^n}\over{\Gamma(a+n+1)n!}}\left({{x}\over{2}}\right)^{a+2n}.$$  Where $a$ not an integer. Need to show $$J_{a+1}(x)J_{-a}(x)+J_a(x)J_{-(a+1)}(x)=-{{2\sin(a\pi)}\over{\pi x}}.$$ Note that the Wronskian of $$W(J_a(x),J_{-a}(x))=-{{2\sin(a\pi)}\over{\pi x}}.$$","Define Bessel function as: $$J_a(x)=\sum_{n=0}^{\infty}{{(-1)^n}\over{\Gamma(a+n+1)n!}}\left({{x}\over{2}}\right)^{a+2n}.$$  Where $a$ not an integer. Need to show $$J_{a+1}(x)J_{-a}(x)+J_a(x)J_{-(a+1)}(x)=-{{2\sin(a\pi)}\over{\pi x}}.$$ Note that the Wronskian of $$W(J_a(x),J_{-a}(x))=-{{2\sin(a\pi)}\over{\pi x}}.$$",,"['linear-algebra', 'ordinary-differential-equations', 'special-functions']"
59,derivation of Support Vector Machine,derivation of Support Vector Machine,,I was watching Andrew Ng's machine learning lecture on SVM. There is one line that puzzles me. $$x^{(i)} - \gamma^{(i)} \frac{w}{||w||}$$ I dont understand how can the line above give the x-coordinate of point B on the decision boundary. Please can someone explain the linear algebra of this line of equation.,I was watching Andrew Ng's machine learning lecture on SVM. There is one line that puzzles me. $$x^{(i)} - \gamma^{(i)} \frac{w}{||w||}$$ I dont understand how can the line above give the x-coordinate of point B on the decision boundary. Please can someone explain the linear algebra of this line of equation.,,"['linear-algebra', 'machine-learning']"
60,"Polar decompostion should be a diffeomorphism, right?","Polar decompostion should be a diffeomorphism, right?",,"I seem to have gotten stuck in the mud verifying what I thought was going to be a completely straightforward fact. I would appreciate if somebody could help dig me out. Inside the $n \times n$ complex matrices $M_n(\mathbb{C})$, let $U, GL, GL_+$ denote, respectively, the unitary matrices, invertible matrices and positive invertible matrices. I was trying to check: The multiplication map $F : U \times GL_+ \to GL$ sending $(u,p) \mapsto up$ is a diffeomorphism. Everything here is being viewed as a submanifold of the $2n^2$-dimensional Euclidean space $M_n(\mathbb{C})$. Even though all the scalars here are complex, the objects are being viewed as real manifolds. Some relevant facts: I calculated the tangent spaces to $U$ and got $T_u(U) = \{ a \in M_n(\mathbb{C}) : u^*a + a^*u = 0 \}.$ That is, $a$ is tangent to $U$ at $u$ if and only if $u^*a$ is anti-self-adjoint.  In particular, $T_1(U)$ is the anti-self-adjoint matrices. $GL_+$ is an open subset of the vector space $SA \subset M_n(\mathbb{C})$ of self-adjoint matrices. So, $T_p(GL_+) = SA$ for all $p \in GL_+$. $GL$ is an open subset of $M_n(\mathbb{C})$ so $T_a(GL) = M_n(\mathbb{C})$ for all $a \in GL$. Since $F$ is a bijection (polar decomposition), we will be done if we can show that $F$ has invertible derivative everywhere. The derivatives are given by $DF_{(u,p)} (a,b) = ap + ub$. I figured the easiest thing to do would be to check that this linear map always has zero kernel (since the domain and codomain are manifolds of the same dimension). Look how well this works at the point $(1,1) \in  U \times GL_+$! If $DF_{(1,1)}(a,b) = a+b = 0$ (where $a$ is anti-self-adjoint and $b$ is self-adjoint) then, taking adjoints, we also have $-a + b = 0$. Combining these equations gives $a = b = 0$. But, I get stuck checking injectivity at other points. Even, say, at $(1,p)$ where $p \in GL_+$. Then, if we take $(a,b) \in T_{(1,p)}(U \times GL_+)$ (i.e. $a^* = -a, b^* = b$) and assume that $DF_{(1,p)}(a,b) = b + ap = 0$, I get stuck doing all sorts of crazy algebra trying to deduce $a=b=0$.","I seem to have gotten stuck in the mud verifying what I thought was going to be a completely straightforward fact. I would appreciate if somebody could help dig me out. Inside the $n \times n$ complex matrices $M_n(\mathbb{C})$, let $U, GL, GL_+$ denote, respectively, the unitary matrices, invertible matrices and positive invertible matrices. I was trying to check: The multiplication map $F : U \times GL_+ \to GL$ sending $(u,p) \mapsto up$ is a diffeomorphism. Everything here is being viewed as a submanifold of the $2n^2$-dimensional Euclidean space $M_n(\mathbb{C})$. Even though all the scalars here are complex, the objects are being viewed as real manifolds. Some relevant facts: I calculated the tangent spaces to $U$ and got $T_u(U) = \{ a \in M_n(\mathbb{C}) : u^*a + a^*u = 0 \}.$ That is, $a$ is tangent to $U$ at $u$ if and only if $u^*a$ is anti-self-adjoint.  In particular, $T_1(U)$ is the anti-self-adjoint matrices. $GL_+$ is an open subset of the vector space $SA \subset M_n(\mathbb{C})$ of self-adjoint matrices. So, $T_p(GL_+) = SA$ for all $p \in GL_+$. $GL$ is an open subset of $M_n(\mathbb{C})$ so $T_a(GL) = M_n(\mathbb{C})$ for all $a \in GL$. Since $F$ is a bijection (polar decomposition), we will be done if we can show that $F$ has invertible derivative everywhere. The derivatives are given by $DF_{(u,p)} (a,b) = ap + ub$. I figured the easiest thing to do would be to check that this linear map always has zero kernel (since the domain and codomain are manifolds of the same dimension). Look how well this works at the point $(1,1) \in  U \times GL_+$! If $DF_{(1,1)}(a,b) = a+b = 0$ (where $a$ is anti-self-adjoint and $b$ is self-adjoint) then, taking adjoints, we also have $-a + b = 0$. Combining these equations gives $a = b = 0$. But, I get stuck checking injectivity at other points. Even, say, at $(1,p)$ where $p \in GL_+$. Then, if we take $(a,b) \in T_{(1,p)}(U \times GL_+)$ (i.e. $a^* = -a, b^* = b$) and assume that $DF_{(1,p)}(a,b) = b + ap = 0$, I get stuck doing all sorts of crazy algebra trying to deduce $a=b=0$.",,"['linear-algebra', 'multivariable-calculus', 'manifolds']"
61,Why does $AX=0$ have only the trivial solution when $A=\left(\int_a^b g_i(x)g_j(x)dx\right)$?,Why does  have only the trivial solution when ?,AX=0 A=\left(\int_a^b g_i(x)g_j(x)dx\right),"The system is $AX=0$, where $$A_{m\times m}=\begin{pmatrix} \int_a^bg_1(x)g_1(x)dx & \cdots & \int_a^bg_1(x)g_m(x)dx \\   \vdots &  & \vdots \\ \int_a^bg_m(x)g_1(x)dx & \cdots & \int_a^bg_m(x)g_m(x)dx \end{pmatrix},$$ $$X=\begin{pmatrix} x_1\\  \vdots\\  x_m \end{pmatrix},0=\begin{pmatrix} 0\\  \vdots\\  0 \end{pmatrix}\in \mathbb{R}^m,$$ and $g_1,...g_n:[a,b]\rightarrow\mathbb{R}$ are linearly independent continuous functions. I've read that this system has only one solution, but I'm not able to prove it. Thanks.","The system is $AX=0$, where $$A_{m\times m}=\begin{pmatrix} \int_a^bg_1(x)g_1(x)dx & \cdots & \int_a^bg_1(x)g_m(x)dx \\   \vdots &  & \vdots \\ \int_a^bg_m(x)g_1(x)dx & \cdots & \int_a^bg_m(x)g_m(x)dx \end{pmatrix},$$ $$X=\begin{pmatrix} x_1\\  \vdots\\  x_m \end{pmatrix},0=\begin{pmatrix} 0\\  \vdots\\  0 \end{pmatrix}\in \mathbb{R}^m,$$ and $g_1,...g_n:[a,b]\rightarrow\mathbb{R}$ are linearly independent continuous functions. I've read that this system has only one solution, but I'm not able to prove it. Thanks.",,"['linear-algebra', 'matrices']"
62,"Union of proper subspaces, which is correct?","Union of proper subspaces, which is correct?",,"My textbook asks this: Suppose that $K$ is a finite field with $k$ elements, and that $V$ is an $r$-dimensional vector space over $K$. Show that if $V = \bigcup_{i=1}^n U_i$, where $U_1,\dotsc,U_n$ are proper subspaces of $V$, then $n\geq (k^r - 1)/(k-1)$. Struggling to prove this for a while, I did some googling and found this paper which claims to show $n = k+1$ is possible, a result which is independent of the dimension of $V$. Which is correct?","My textbook asks this: Suppose that $K$ is a finite field with $k$ elements, and that $V$ is an $r$-dimensional vector space over $K$. Show that if $V = \bigcup_{i=1}^n U_i$, where $U_1,\dotsc,U_n$ are proper subspaces of $V$, then $n\geq (k^r - 1)/(k-1)$. Struggling to prove this for a while, I did some googling and found this paper which claims to show $n = k+1$ is possible, a result which is independent of the dimension of $V$. Which is correct?",,['linear-algebra']
63,Prove that one of the following sets is a subspace and the other isn't?,Prove that one of the following sets is a subspace and the other isn't?,,"OK, here goes another.  Prove that $ W_1 = ${$(a_1, a_2, \ldots, a_n) \in F^n : a_1 + a_2 + \cdots + a_n = 0$} is a subspace of $F^n$ but $ W_2 = ${$(a_1, a_2, \ldots, a_n) \in F^n : a_1 + a_2 + \cdots + a_n = 1$} is not. OK. Any subspace has to contain the zero vector, be closed under addition and scalar multiplication by definition. So to prove this we first see whether the set $W_1$ meet those criteria. Plugging in 0 for $a_i$ obviously works, so the first condition is met. Is it closed under addition? let $b_i$ be the components of an arbitrary vector in $W_1$. So ($b_1, b_2, \ldots, b_n) \in W_1$ and if we add it to $(a_1, a_2, \ldots, a_n)$ we get $(a_1 + b_1) + (a_2 + b_2) + \cdots +(a_n + b_n) = 0$. That's pretty clearly part of $W_1$ and thus closed under addition. Next we see if it is closed under multiplication by a scalar. We pick an arbitrary scalar $c$ and multiply it by $(a_1, a_2, \ldots, a_n)$ to get $(ca_1, ca_2, \ldots, ca_n)$ and plugging that into the original condition we find that it doesn't matter what c is, because $ca_1 + ca_2 + \cdots + ca_n = 0$ and that's still in $W_1.$ Therefore $W_1$ is a subspace of $F^n$. If we do the same procedure with $W_2$, though, we find that $0$ vector is not in the set. Because $a_1 + a_2 + \cdots + a_n = 1$ is a contradiction. Further, we can see that it isn't closed under multiplication either. $ca_1 + ca_2 + \cdots + ca_n = c$ and that will only equal 1 if c=1, so the equation does not hold with an arbitrary $c$. Therefore $W_2$ is NOT a subspace of $F^n$. Any holes in this proof? (Yeah, I have been bothering folks here a lot but I finally feel that I am getting the hang of this and I have an exam tomorrow night).","OK, here goes another.  Prove that $ W_1 = ${$(a_1, a_2, \ldots, a_n) \in F^n : a_1 + a_2 + \cdots + a_n = 0$} is a subspace of $F^n$ but $ W_2 = ${$(a_1, a_2, \ldots, a_n) \in F^n : a_1 + a_2 + \cdots + a_n = 1$} is not. OK. Any subspace has to contain the zero vector, be closed under addition and scalar multiplication by definition. So to prove this we first see whether the set $W_1$ meet those criteria. Plugging in 0 for $a_i$ obviously works, so the first condition is met. Is it closed under addition? let $b_i$ be the components of an arbitrary vector in $W_1$. So ($b_1, b_2, \ldots, b_n) \in W_1$ and if we add it to $(a_1, a_2, \ldots, a_n)$ we get $(a_1 + b_1) + (a_2 + b_2) + \cdots +(a_n + b_n) = 0$. That's pretty clearly part of $W_1$ and thus closed under addition. Next we see if it is closed under multiplication by a scalar. We pick an arbitrary scalar $c$ and multiply it by $(a_1, a_2, \ldots, a_n)$ to get $(ca_1, ca_2, \ldots, ca_n)$ and plugging that into the original condition we find that it doesn't matter what c is, because $ca_1 + ca_2 + \cdots + ca_n = 0$ and that's still in $W_1.$ Therefore $W_1$ is a subspace of $F^n$. If we do the same procedure with $W_2$, though, we find that $0$ vector is not in the set. Because $a_1 + a_2 + \cdots + a_n = 1$ is a contradiction. Further, we can see that it isn't closed under multiplication either. $ca_1 + ca_2 + \cdots + ca_n = c$ and that will only equal 1 if c=1, so the equation does not hold with an arbitrary $c$. Therefore $W_2$ is NOT a subspace of $F^n$. Any holes in this proof? (Yeah, I have been bothering folks here a lot but I finally feel that I am getting the hang of this and I have an exam tomorrow night).",,"['linear-algebra', 'vector-spaces', 'proof-writing']"
64,What is the relation between vectors in physics and algebra?,What is the relation between vectors in physics and algebra?,,"Vector math is something I find very interesting. However, we have never been told the link between vectors in physics (usually represented as arrows, e.g. a force vector) and in algebra (e.g. represented like a column matrix). It was really never explained well in classes. Here are the things I can't wrap my head around: How can a vector (starting from the algebraic definition) be represented as an arrow? Is it correct to assume that a vector (in a 2-dimensional space) $v = [1,1]$ could be represented as an arrow from the origin $[0,0]$ to the point $[1,1]$? If the above assumption is correct, what does it mean in the physics representation to normalize a vector? If I have a vector $[1,1]$, would the vector $[-1,1]$ be orthogonal to that first vector? (Because if you draw the arrows they are perpendicular). How can one translate an object along a vector? Is that simply scalar addition? These questions probably sound really odd, but they come from a lack of decent explanation in both physics and algebra.","Vector math is something I find very interesting. However, we have never been told the link between vectors in physics (usually represented as arrows, e.g. a force vector) and in algebra (e.g. represented like a column matrix). It was really never explained well in classes. Here are the things I can't wrap my head around: How can a vector (starting from the algebraic definition) be represented as an arrow? Is it correct to assume that a vector (in a 2-dimensional space) $v = [1,1]$ could be represented as an arrow from the origin $[0,0]$ to the point $[1,1]$? If the above assumption is correct, what does it mean in the physics representation to normalize a vector? If I have a vector $[1,1]$, would the vector $[-1,1]$ be orthogonal to that first vector? (Because if you draw the arrows they are perpendicular). How can one translate an object along a vector? Is that simply scalar addition? These questions probably sound really odd, but they come from a lack of decent explanation in both physics and algebra.",,"['linear-algebra', 'geometry', 'physics', 'intuition']"
65,calculate kernels of matrices with angles,calculate kernels of matrices with angles,,"So my professor gave me this question: I have to find the basis of the eigenvalues of this matrix \begin{pmatrix}   \cos(q) & \sin(q)\\   \sin(q) & -\cos(q)\\   \end{pmatrix} so I calculate the eigenvalues and I found it is 1 and -1. so now I need to find the basis of the kernel of those matrices \begin{pmatrix}   1-\cos(q) & -\sin(q)\\   -\sin(q) & 1+\cos(q)\\   \end{pmatrix} \begin{pmatrix}   -1-\cos(q) & -\sin(q)\\   -\sin(q) & -1+\cos(q)\\   \end{pmatrix} so actually I need to find \begin{pmatrix}   a \\ b\\   \end{pmatrix} that will be function as basis for the kernel of each one of those matrices. but how do I do it ? I know how to do it when there is no angles meaning I would just compare it to zero. for example in the first matrix I get these two equations : $x-xcos(q)-ysin(q)=0$ $-xsin(q)+y+ycos(q)=0$ so I get this equation: $x(-sin(y)+((1-cos(y))/(sin(y)))+((cos(y)-cos(y)cos(y))/(sin(y))))=0$ which is true for every $x$ and then I can assign some value to $x$ for example zero and then $y$ equal zero also but after I am checking it, it is not true. could u please help me ?  so how can I do it ? All he told us is that $q\neq{0}$ and $q\neq\pi$ and $q\neq{2}\pi $ and nothing more.","So my professor gave me this question: I have to find the basis of the eigenvalues of this matrix \begin{pmatrix}   \cos(q) & \sin(q)\\   \sin(q) & -\cos(q)\\   \end{pmatrix} so I calculate the eigenvalues and I found it is 1 and -1. so now I need to find the basis of the kernel of those matrices \begin{pmatrix}   1-\cos(q) & -\sin(q)\\   -\sin(q) & 1+\cos(q)\\   \end{pmatrix} \begin{pmatrix}   -1-\cos(q) & -\sin(q)\\   -\sin(q) & -1+\cos(q)\\   \end{pmatrix} so actually I need to find \begin{pmatrix}   a \\ b\\   \end{pmatrix} that will be function as basis for the kernel of each one of those matrices. but how do I do it ? I know how to do it when there is no angles meaning I would just compare it to zero. for example in the first matrix I get these two equations : $x-xcos(q)-ysin(q)=0$ $-xsin(q)+y+ycos(q)=0$ so I get this equation: $x(-sin(y)+((1-cos(y))/(sin(y)))+((cos(y)-cos(y)cos(y))/(sin(y))))=0$ which is true for every $x$ and then I can assign some value to $x$ for example zero and then $y$ equal zero also but after I am checking it, it is not true. could u please help me ?  so how can I do it ? All he told us is that $q\neq{0}$ and $q\neq\pi$ and $q\neq{2}\pi $ and nothing more.",,['linear-algebra']
66,Consequences of a rectangular matrix being of maximal rank,Consequences of a rectangular matrix being of maximal rank,,"I have a real matrix $A$, $(m+1) \times m$ and a vector $b  \in \mathbb R^{m+1}$ such that $b_{m+1}=0$. For any vector $u\in \mathbb R^m$, $Au=0 \Rightarrow u=0$. This means that $A$ is a rectangular matrix of maximal rank, i.e. of rank $m$. Since $m< \infty$, I'm told this means there is a solution $u\in \mathbb R^m$ to $Au=b$. I don't understand why: if $\operatorname{rk}(A)=m$, then the dimension of the image of the linear application defined by $A$ in $\mathbb R^{m+1}$ is $m$. So for $b\in\mathbb R^m$ we could guarantee a solution, but how can we guarantee the solution satisfies our $b\in\mathbb R^{m+1}$? Is there something simple I'm missing? (I've asked a couple of classmates who didn't know either, and my professor who told me this in the first place told me to look it up in a book that's in a library that's closed until next week.)","I have a real matrix $A$, $(m+1) \times m$ and a vector $b  \in \mathbb R^{m+1}$ such that $b_{m+1}=0$. For any vector $u\in \mathbb R^m$, $Au=0 \Rightarrow u=0$. This means that $A$ is a rectangular matrix of maximal rank, i.e. of rank $m$. Since $m< \infty$, I'm told this means there is a solution $u\in \mathbb R^m$ to $Au=b$. I don't understand why: if $\operatorname{rk}(A)=m$, then the dimension of the image of the linear application defined by $A$ in $\mathbb R^{m+1}$ is $m$. So for $b\in\mathbb R^m$ we could guarantee a solution, but how can we guarantee the solution satisfies our $b\in\mathbb R^{m+1}$? Is there something simple I'm missing? (I've asked a couple of classmates who didn't know either, and my professor who told me this in the first place told me to look it up in a book that's in a library that's closed until next week.)",,['linear-algebra']
67,Are Clifford groups very *non-commutative*?,Are Clifford groups very *non-commutative*?,,"Clifford groups seem to be very non-commutative by the relation \begin{equation} \gamma_{i}\gamma_{j}=-\gamma_{j}\gamma_{i}. \end{equation} But is it really so? Can we put this degree of non-commutative in a precise form? I tried the standard, namely, the commutator\begin{equation} [Cl(n),Cl(n)]=\{\pm1\}, \end{equation} where $Cl(n)$ is the Clifford group of degree $n$. From this it seems Clifford groups are not really that bad for the commutator is rather small. However the centre $Z(Cl(n))$ is $\{\pm 1\}$ for even $n$ and $\{\pm 1, \pm\gamma_{1}\gamma_{2}\cdots\gamma_{n}\}$ for odd $n$, which seems also small. So my question consists of two parts: 1. Are Clifford groups very non-commutative? and 2. Are there some convenient/ strong indicators of the degree of non-commutativeness? I am actually more interested in the second, and have some rather naive thoughts: Maybe the degrees of irreducible representation of $G$, $\{\operatorname{degree}(\alpha):\alpha\in\hat{G}\}$, would contain some hints. Firstly, $\#\{\operatorname{degree}(\alpha)=1\}=\#G/[G,G]$;secondly, $\operatorname{degree}_{\alpha}|\#G/Z(G)$; last but not least, non-commutative can be thought of as a high dimensional phenomena since 1-by-1 matrices are commutative in multiplication. Again these are just naive thoughts and I would like to hear your opinion on the degree of non-commutativeness . Thanks very much! ( Well, actually this is what truly motivates this question: assume $\#\{\operatorname{degree}(\alpha)=1\}$ and $max_{\alpha}\operatorname{degree}(\alpha)$ or $gcd(\operatorname{degree}(\alpha))$ give some information on the commutativity, then what do things like $\#\{\operatorname{degree}(\alpha)=n\}$ tell us? Higher dimensional commutativity?)","Clifford groups seem to be very non-commutative by the relation \begin{equation} \gamma_{i}\gamma_{j}=-\gamma_{j}\gamma_{i}. \end{equation} But is it really so? Can we put this degree of non-commutative in a precise form? I tried the standard, namely, the commutator\begin{equation} [Cl(n),Cl(n)]=\{\pm1\}, \end{equation} where $Cl(n)$ is the Clifford group of degree $n$. From this it seems Clifford groups are not really that bad for the commutator is rather small. However the centre $Z(Cl(n))$ is $\{\pm 1\}$ for even $n$ and $\{\pm 1, \pm\gamma_{1}\gamma_{2}\cdots\gamma_{n}\}$ for odd $n$, which seems also small. So my question consists of two parts: 1. Are Clifford groups very non-commutative? and 2. Are there some convenient/ strong indicators of the degree of non-commutativeness? I am actually more interested in the second, and have some rather naive thoughts: Maybe the degrees of irreducible representation of $G$, $\{\operatorname{degree}(\alpha):\alpha\in\hat{G}\}$, would contain some hints. Firstly, $\#\{\operatorname{degree}(\alpha)=1\}=\#G/[G,G]$;secondly, $\operatorname{degree}_{\alpha}|\#G/Z(G)$; last but not least, non-commutative can be thought of as a high dimensional phenomena since 1-by-1 matrices are commutative in multiplication. Again these are just naive thoughts and I would like to hear your opinion on the degree of non-commutativeness . Thanks very much! ( Well, actually this is what truly motivates this question: assume $\#\{\operatorname{degree}(\alpha)=1\}$ and $max_{\alpha}\operatorname{degree}(\alpha)$ or $gcd(\operatorname{degree}(\alpha))$ give some information on the commutativity, then what do things like $\#\{\operatorname{degree}(\alpha)=n\}$ tell us? Higher dimensional commutativity?)",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'representation-theory', 'noncommutative-algebra']"
68,"Sequence of matrices, equivalent conditions","Sequence of matrices, equivalent conditions",,"I am trying to prove that: Let $B$ be a square matrix. The following conditions are equivalent: $\lim\limits_{k\rightarrow\infty}B^k = 0$ $\lim\limits_{k\rightarrow\infty}B^kv = 0$ for every vector $v$ $\rho(B)<1$ $\|B\| < 1$ for at least one subordinate matrix norm $\|\cdot\|$ The matrix $(I-B)$ is invertible and $$(I-B)^{-1}\ =\ \lim_{k\rightarrow\infty}(I + B + \ldots + B^k).$$ The matrix $(I-B)$ is invertible and all the eigenvalues of the matrix $(I+2(B-I)^{-1})$ have negative real part. There exists a positive definite Hermitian matrix $H$ such that the (Hermitian) matrix $(H-B^*HB)$ is positive definite. Given any matrix norm $\|\cdot\|$ , there exists an integer $l$ such that $\|B^l\| < 1$ . I already proved almost all, the only proof that I need is (whatever) $\Rightarrow (7)$ , but I really don't know how do it. Please, somebody help me. Thanks in advance.","I am trying to prove that: Let be a square matrix. The following conditions are equivalent: for every vector for at least one subordinate matrix norm The matrix is invertible and The matrix is invertible and all the eigenvalues of the matrix have negative real part. There exists a positive definite Hermitian matrix such that the (Hermitian) matrix is positive definite. Given any matrix norm , there exists an integer such that . I already proved almost all, the only proof that I need is (whatever) , but I really don't know how do it. Please, somebody help me. Thanks in advance.",B \lim\limits_{k\rightarrow\infty}B^k = 0 \lim\limits_{k\rightarrow\infty}B^kv = 0 v \rho(B)<1 \|B\| < 1 \|\cdot\| (I-B) (I-B)^{-1}\ =\ \lim_{k\rightarrow\infty}(I + B + \ldots + B^k). (I-B) (I+2(B-I)^{-1}) H (H-B^*HB) \|\cdot\| l \|B^l\| < 1 \Rightarrow (7),"['linear-algebra', 'sequences-and-series', 'matrices']"
69,how to prove following matrix is invertible? [duplicate],how to prove following matrix is invertible? [duplicate],,This question already has an answer here : Prove that a matrix is invertible [duplicate] (1 answer) Closed 11 years ago . how to prove A is invertible or $\ detA\neq 0$ $$A=\begin{pmatrix}     \frac11 & \frac12 & \frac13 & \cdots & \frac1n \\      \frac12 & \frac13 & \frac14 & \cdots & \frac{1}{n+1} \\      \vdots  & \vdots& \vdots & \ddots & \vdots \\      \frac1n & \frac{1}{n+1} & \frac{1}{n+2} & \cdots & \frac{1}{2n-1}          \end{pmatrix}$$ Thanks in advance,This question already has an answer here : Prove that a matrix is invertible [duplicate] (1 answer) Closed 11 years ago . how to prove A is invertible or $\ detA\neq 0$ $$A=\begin{pmatrix}     \frac11 & \frac12 & \frac13 & \cdots & \frac1n \\      \frac12 & \frac13 & \frac14 & \cdots & \frac{1}{n+1} \\      \vdots  & \vdots& \vdots & \ddots & \vdots \\      \frac1n & \frac{1}{n+1} & \frac{1}{n+2} & \cdots & \frac{1}{2n-1}          \end{pmatrix}$$ Thanks in advance,,"['linear-algebra', 'matrices', 'contest-math']"
70,Proving this is a linear subspace,Proving this is a linear subspace,,"I need to prove the following is a subspace: Let $V$ be a set of vectors over $F=\mathbb{R}$, $V=\operatorname{Functions}(\mathbb{R} ,\mathbb{R})$ and $W$ is a subgroup of $V$ such that $$W=\{f\in V| \, f(x)=f(-x)\}$$ I'm not sure about the ""close under vector addition"" part. My solution: Let be $w_1 , w_2 \in W. w_1=\{f\in V| \, f(x_1)=f(-x_1)\},\,\,\,w_2=\{f\in V| \, f(x_2)=f(-x_2)\}$. Therefore: $$w_1+w_2=f(x_1+x_2)=f(x_1)+f(x_2)=f(-x_1)+f(-x_2)=f\big(-(x_1+x_2)\big)\\ $$ Please tell me if something is wrong here (I feel like something is wrong...). Many thanks for you time and effort.","I need to prove the following is a subspace: Let $V$ be a set of vectors over $F=\mathbb{R}$, $V=\operatorname{Functions}(\mathbb{R} ,\mathbb{R})$ and $W$ is a subgroup of $V$ such that $$W=\{f\in V| \, f(x)=f(-x)\}$$ I'm not sure about the ""close under vector addition"" part. My solution: Let be $w_1 , w_2 \in W. w_1=\{f\in V| \, f(x_1)=f(-x_1)\},\,\,\,w_2=\{f\in V| \, f(x_2)=f(-x_2)\}$. Therefore: $$w_1+w_2=f(x_1+x_2)=f(x_1)+f(x_2)=f(-x_1)+f(-x_2)=f\big(-(x_1+x_2)\big)\\ $$ Please tell me if something is wrong here (I feel like something is wrong...). Many thanks for you time and effort.",,['linear-algebra']
71,"Find $\arg\max_x \operatorname{corr}(Ax, Bx)$ for vector $x$, matrices $A$ and $B$","Find  for vector , matrices  and","\arg\max_x \operatorname{corr}(Ax, Bx) x A B","This is similar to, but not the same as, canonical correlation: For $(n \times m)$ matrices $A$ and $B$, and unit vector $(m \times 1)$ $x$, is there a closed-form solution to maximize the correlation between $Ax$ and $Bx$ w.r.t. $x$? Note that I am optimizing over just one vector (in contrast to canonical correlation).","This is similar to, but not the same as, canonical correlation: For $(n \times m)$ matrices $A$ and $B$, and unit vector $(m \times 1)$ $x$, is there a closed-form solution to maximize the correlation between $Ax$ and $Bx$ w.r.t. $x$? Note that I am optimizing over just one vector (in contrast to canonical correlation).",,"['linear-algebra', 'optimization', 'correlation']"
72,Condition of the eigenvalue problem,Condition of the eigenvalue problem,,"[Ciarlet, 2.3-1] I know this result: Let $A$ a diagonalisable matrix, $P$ a matrix such that   $$P^{-1}AP\ =\ \mbox{diag}(\lambda_i)\ =\ D,$$   and $\|\cdot\|$ a matrix norm satisfying   $$\|\mbox{diag}(d_i)\|\ =\ \max_i|d_i|$$   for every diagonal matrix. Then, for every matrix $\delta A$,   $$\mbox{sp}(A+\delta A)\ =\ \{\mu : (A+\delta A)x=\mu x, \mbox{ for some } x\in\mathbb{C}^n\}\ \subset\ \bigcup_{i=1}^ND_i,$$   where   $$D_i\ =\ \{z\in\mathbb{C}:|z-\lambda_i|\leq\mbox{cond}(P)\|\delta A\|\}.$$ Now, I want to prove that: If there exists an integer $m$ satisfying $1\leq m\leq n$ such that the union $\bigcup\limits_{i=1}^mD_i$ of the $m$ disk $D_i$ is disjoint form the union $\bigcup\limits_{i=m+1}^nD_i$ of the remaining $n-m$ disks (it is always possible to assum that it is the first $m$ and last $n-m$ disks which have this property), then the union $\bigcup\limits_{i=1}^mD_i$ contains exactly $m$ eigenvalues of the matrix $A + \delta A$. Please, somebody have an idea to solve it? Thanks in advance.","[Ciarlet, 2.3-1] I know this result: Let $A$ a diagonalisable matrix, $P$ a matrix such that   $$P^{-1}AP\ =\ \mbox{diag}(\lambda_i)\ =\ D,$$   and $\|\cdot\|$ a matrix norm satisfying   $$\|\mbox{diag}(d_i)\|\ =\ \max_i|d_i|$$   for every diagonal matrix. Then, for every matrix $\delta A$,   $$\mbox{sp}(A+\delta A)\ =\ \{\mu : (A+\delta A)x=\mu x, \mbox{ for some } x\in\mathbb{C}^n\}\ \subset\ \bigcup_{i=1}^ND_i,$$   where   $$D_i\ =\ \{z\in\mathbb{C}:|z-\lambda_i|\leq\mbox{cond}(P)\|\delta A\|\}.$$ Now, I want to prove that: If there exists an integer $m$ satisfying $1\leq m\leq n$ such that the union $\bigcup\limits_{i=1}^mD_i$ of the $m$ disk $D_i$ is disjoint form the union $\bigcup\limits_{i=m+1}^nD_i$ of the remaining $n-m$ disks (it is always possible to assum that it is the first $m$ and last $n-m$ disks which have this property), then the union $\bigcup\limits_{i=1}^mD_i$ contains exactly $m$ eigenvalues of the matrix $A + \delta A$. Please, somebody have an idea to solve it? Thanks in advance.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization']"
73,Eigenvalues and Eigenvectors Diagonilization,Eigenvalues and Eigenvectors Diagonilization,,"Let $ A=\begin{bmatrix}         -7 & -1  \\         12 & 0 \\  \end{bmatrix} $ . Find a matrix $ P $ and a diagonal matrix $D$ such that $PDP^{-1} = A$. Ok so the first thing I need to look for are my eigenvalues and eigenvectors. However, I think I'm doing it wrong and don't seem to get the correct eigenvectors. If $ det(A-\lambda I)X=0 $ then $ det(\begin{bmatrix}         -7-\lambda & -1  \\         12 & -\lambda \\  \end{bmatrix})= \lambda^2+7\lambda+12=0 $ This gives me eigenvalues $\lambda_1=-3 $ and $\lambda_2 = -4 $ Case: $ \lambda_1 = -3$ $ \begin{bmatrix}         -7-(-3) & -1  \\         12 & -(-3) \\  \end{bmatrix} = \begin{bmatrix}         -4 & -1  \\         12 & 3 \\  \end{bmatrix} $ and end up with eigenvector $\begin{bmatrix}         -1/4  \\         1 \\  \end{bmatrix} $ But this is wrong, and not sure where I made the error, or if my whole procedure is off. So I stopped here.","Let $ A=\begin{bmatrix}         -7 & -1  \\         12 & 0 \\  \end{bmatrix} $ . Find a matrix $ P $ and a diagonal matrix $D$ such that $PDP^{-1} = A$. Ok so the first thing I need to look for are my eigenvalues and eigenvectors. However, I think I'm doing it wrong and don't seem to get the correct eigenvectors. If $ det(A-\lambda I)X=0 $ then $ det(\begin{bmatrix}         -7-\lambda & -1  \\         12 & -\lambda \\  \end{bmatrix})= \lambda^2+7\lambda+12=0 $ This gives me eigenvalues $\lambda_1=-3 $ and $\lambda_2 = -4 $ Case: $ \lambda_1 = -3$ $ \begin{bmatrix}         -7-(-3) & -1  \\         12 & -(-3) \\  \end{bmatrix} = \begin{bmatrix}         -4 & -1  \\         12 & 3 \\  \end{bmatrix} $ and end up with eigenvector $\begin{bmatrix}         -1/4  \\         1 \\  \end{bmatrix} $ But this is wrong, and not sure where I made the error, or if my whole procedure is off. So I stopped here.",,"['linear-algebra', 'vector-spaces', 'eigenvalues-eigenvectors']"
74,Hessian of a function that takes matrix arguments,Hessian of a function that takes matrix arguments,,"I have a function that that takes a matrix and returns a scalar, $f : \mathbb{R}^{m\times n} \rightarrow \mathbb{R}$. I know how to calculate the derivative of this function with respect to the matrix argument. For example, if $f(X) = -\log \det X$, then $\nabla_X f(X) = -X^{-1}$. If $f(X) = tr(X^{-1} Y)$, $\nabla_X f(X) = -X^{-1} Y X^{-1}$ (assuming $X$ and $Y$ are symmetric for now). What I don't know how to do is compute the second derivative (Hessian?) of $f$ with respect to $X$. I'm assuming it'll be messy, as the result might require four indices, if I'm not mistaken? Here's the reason I'm curious: I'm trying to prove a function $f : \mathbb{R}^{m\times n} \rightarrow \mathbb{R}$ (specifically $f(X) = \log \det X + tr(X^{-1} Y)$) is convex; one way to do so is to prove that $\nabla^2_X f(X) \succeq 0$ everywhere. At least, this is true for functions $g : \mathbb{R}^n \rightarrow \mathbb{R}$, I haven't been able to find any generalizations of this to matrices, but I think it must be true for the correct notion of ""Hessian.""","I have a function that that takes a matrix and returns a scalar, $f : \mathbb{R}^{m\times n} \rightarrow \mathbb{R}$. I know how to calculate the derivative of this function with respect to the matrix argument. For example, if $f(X) = -\log \det X$, then $\nabla_X f(X) = -X^{-1}$. If $f(X) = tr(X^{-1} Y)$, $\nabla_X f(X) = -X^{-1} Y X^{-1}$ (assuming $X$ and $Y$ are symmetric for now). What I don't know how to do is compute the second derivative (Hessian?) of $f$ with respect to $X$. I'm assuming it'll be messy, as the result might require four indices, if I'm not mistaken? Here's the reason I'm curious: I'm trying to prove a function $f : \mathbb{R}^{m\times n} \rightarrow \mathbb{R}$ (specifically $f(X) = \log \det X + tr(X^{-1} Y)$) is convex; one way to do so is to prove that $\nabla^2_X f(X) \succeq 0$ everywhere. At least, this is true for functions $g : \mathbb{R}^n \rightarrow \mathbb{R}$, I haven't been able to find any generalizations of this to matrices, but I think it must be true for the correct notion of ""Hessian.""",,"['calculus', 'linear-algebra', 'optimization', 'convex-analysis', 'convex-optimization']"
75,Morphism between matrices and linear equations,Morphism between matrices and linear equations,,"I'm currently a beginner at linear algebra. So, in some books I see authors start defining linear equations and then they define matrices and, supposedly, the definition of associative matrix is to handle linear equations easily. However they never establish the connection between both objects and never explain why it is possible to work with matrices in substitution of linear equations. For some classmates this is irrelevant because they say that I just complicate my life with such questions. But it is important and think that the treatment given in such books is either very informal so beginners like me can understand the concepts or maybe is too simple that I'm missing something. I have read that two objects are generally treated as being the same, of course under certain properties, if there is a connection between them in terms of a one-to-one correspondence (something called morphism, isomorphism, monomorphism, etc). So, how would you establish the bijection between linear equations and matrices considering elementary operations?","I'm currently a beginner at linear algebra. So, in some books I see authors start defining linear equations and then they define matrices and, supposedly, the definition of associative matrix is to handle linear equations easily. However they never establish the connection between both objects and never explain why it is possible to work with matrices in substitution of linear equations. For some classmates this is irrelevant because they say that I just complicate my life with such questions. But it is important and think that the treatment given in such books is either very informal so beginners like me can understand the concepts or maybe is too simple that I'm missing something. I have read that two objects are generally treated as being the same, of course under certain properties, if there is a connection between them in terms of a one-to-one correspondence (something called morphism, isomorphism, monomorphism, etc). So, how would you establish the bijection between linear equations and matrices considering elementary operations?",,"['linear-algebra', 'abstract-algebra']"
76,Question on a proof about the Rank of a Matrix,Question on a proof about the Rank of a Matrix,,"The question is: Give a formal proof for the following statement: Given a matrix A and a scalar c, show that rank(cA) = rank(A) Here are the steps that I took to go about the proof: (1) Prove this claim: Let v1, v2, ..., vN be vectors then {v1, v2, ..., vN} is linearly independent  <==> {c* v1, c*v2, ..., c * vN} is also lin. ind. I don't type out the whole thing here, but the proof is trivial by playing around with the coefficients (2) Let (c * A_ij) where i = 1, 2, ..., m; and j is fixed where j belongs to {1, 2, ..., n} denotes a  linearly independent column in matrix cA (3) Then I let S = { (c * A_ij)} be the set of all linearly independent columns in matrix cA, where each element of S satisfies (2) (4) By how I define the set S, all elements in S are lin. ind. columns in matrix cA. Then I use the claim (1) to say that columns A_ij of matrix A must also be lin. ind. I also note that by definition of the rank, it's the maximum number of lin. ind. columns (or rows) in a matrix.  So I think rank(cA) is basically the cardinality of the set S.  Then by (4), I conclude that when I ""move"" from each lin. ind. column of matrix cA to each lin. ind. column in matrix A, I didn't change the number of lin. ind. columns.  Thus, rank(cA) = rank(A). Would someone please help me check if there is anything missing or wrong in my proof ?  Somehow I feel a bit shaky on how I define the indices for the linearly independent columns in matrix cA. Thank you very much ^_^","The question is: Give a formal proof for the following statement: Given a matrix A and a scalar c, show that rank(cA) = rank(A) Here are the steps that I took to go about the proof: (1) Prove this claim: Let v1, v2, ..., vN be vectors then {v1, v2, ..., vN} is linearly independent  <==> {c* v1, c*v2, ..., c * vN} is also lin. ind. I don't type out the whole thing here, but the proof is trivial by playing around with the coefficients (2) Let (c * A_ij) where i = 1, 2, ..., m; and j is fixed where j belongs to {1, 2, ..., n} denotes a  linearly independent column in matrix cA (3) Then I let S = { (c * A_ij)} be the set of all linearly independent columns in matrix cA, where each element of S satisfies (2) (4) By how I define the set S, all elements in S are lin. ind. columns in matrix cA. Then I use the claim (1) to say that columns A_ij of matrix A must also be lin. ind. I also note that by definition of the rank, it's the maximum number of lin. ind. columns (or rows) in a matrix.  So I think rank(cA) is basically the cardinality of the set S.  Then by (4), I conclude that when I ""move"" from each lin. ind. column of matrix cA to each lin. ind. column in matrix A, I didn't change the number of lin. ind. columns.  Thus, rank(cA) = rank(A). Would someone please help me check if there is anything missing or wrong in my proof ?  Somehow I feel a bit shaky on how I define the indices for the linearly independent columns in matrix cA. Thank you very much ^_^",,['linear-algebra']
77,How to show that the unit ball of the dual norm is also polytope?,How to show that the unit ball of the dual norm is also polytope?,,Assuming a unit ball for the p-norm that is a convex polytope. How can one show that the unit ball of the dual's norm is a convex polytope?,Assuming a unit ball for the p-norm that is a convex polytope. How can one show that the unit ball of the dual's norm is a convex polytope?,,"['linear-algebra', 'matrices', 'vector-spaces', 'normed-spaces', 'polytopes']"
78,Let $P_3$ be the space of polynomials of degree $\leq 3$ . Find the kernel and the image of the linear map $f(x) \mapsto f(x + 1)−f(x)$,Let  be the space of polynomials of degree  . Find the kernel and the image of the linear map,P_3 \leq 3 f(x) \mapsto f(x + 1)−f(x),"Let $P_3$ be the space of polynomials of degree $\leq 3$ over the field $\mathbb{Z}/2\mathbb{Z}$. Find the kernel and the image (that is, give bases of these spaces) of the linear map $f(x) \mapsto f(x + 1)−f(x)$ So for this problem, a basis for $P_3$ is $\{1, x, x^2, x^3\}$, but looking at this problem I'm not sure that is even necessary...? I really don't know where to start.","Let $P_3$ be the space of polynomials of degree $\leq 3$ over the field $\mathbb{Z}/2\mathbb{Z}$. Find the kernel and the image (that is, give bases of these spaces) of the linear map $f(x) \mapsto f(x + 1)−f(x)$ So for this problem, a basis for $P_3$ is $\{1, x, x^2, x^3\}$, but looking at this problem I'm not sure that is even necessary...? I really don't know where to start.",,['linear-algebra']
79,Approximate a function using another function,Approximate a function using another function,,"Problem Find the best approximation of $f(t)=t^2$ with $h(t)=ae^t+be^{2t}+c$ everywhere on the interval $[0,4]$. Attempt I know how to solve this problem given sample points, by using least squares, but I am having a hard time figuring out how to setup this problem. I know that I need to use the inner product, $\int_0^4f(t)h(t)dt$. The issue I'm having is with putting this problem in the form of $Ax=b$. Normally I would construct the Gram matrix using the inner product and basis vectors, but no basis vectors were given. How do I proceed with setting this up?","Problem Find the best approximation of $f(t)=t^2$ with $h(t)=ae^t+be^{2t}+c$ everywhere on the interval $[0,4]$. Attempt I know how to solve this problem given sample points, by using least squares, but I am having a hard time figuring out how to setup this problem. I know that I need to use the inner product, $\int_0^4f(t)h(t)dt$. The issue I'm having is with putting this problem in the form of $Ax=b$. Normally I would construct the Gram matrix using the inner product and basis vectors, but no basis vectors were given. How do I proceed with setting this up?",,['linear-algebra']
80,"Normal operator $f \in L(V,V)$ adjoint as a polynomial in $f, f^*=p(f)$.",Normal operator  adjoint as a polynomial in .,"f \in L(V,V) f, f^*=p(f)","I'm preparing for a Linear Algebra exam, grad school level. If $V$ is a complex vector space ""unitaire"" (term in French, but I can't find this term anywhere except in my class notes, I think it's ""hermitien"", meaning a vector space with a scalar product, i.e. complex prehilbert space), and $f$ a normal endomorphism of $V$, I need to show there is a polynomial $P\in\Bbb C[X]$ such that $f^*=P(f)$. I think it's related to $f$ being diagonalizable, because I've found such a $P$ for $f$ and $g$ diagonalizable endomorphisms that commute. The Wikipedia site for normal operators (French version) says a normal endomorphism in a complex prehilbert space is diagonalizable in an orthonormal basis, but I don't have anything in my class notes saying that, a proposition I have requires $P_f$ (characteristic polynomial) be factorizable. Is there some property of normal endomorphisms in a prehilbert space that I'm missing?  Thank you in advance.","I'm preparing for a Linear Algebra exam, grad school level. If $V$ is a complex vector space ""unitaire"" (term in French, but I can't find this term anywhere except in my class notes, I think it's ""hermitien"", meaning a vector space with a scalar product, i.e. complex prehilbert space), and $f$ a normal endomorphism of $V$, I need to show there is a polynomial $P\in\Bbb C[X]$ such that $f^*=P(f)$. I think it's related to $f$ being diagonalizable, because I've found such a $P$ for $f$ and $g$ diagonalizable endomorphisms that commute. The Wikipedia site for normal operators (French version) says a normal endomorphism in a complex prehilbert space is diagonalizable in an orthonormal basis, but I don't have anything in my class notes saying that, a proposition I have requires $P_f$ (characteristic polynomial) be factorizable. Is there some property of normal endomorphisms in a prehilbert space that I'm missing?  Thank you in advance.",,"['linear-algebra', 'vector-spaces']"
81,"Laplacians, Diagonal Perturbations","Laplacians, Diagonal Perturbations",,"Setup: Consider a Laplacian (or Kirchoff) matrix $L = L^T \in \mathbb{R}^{n \times n}$ corresponding to a weighted, undirected and connected graph. That is, a matrix with $L_{ij} \leq 0$ for $i\neq j$ and $L_{ii} = -\sum_{j=1}^n L_{ij} > 0$. So $L$ has zero row sum, and is positive semidefinite with a simple eigenvalue at $0$. It's well known that if you add a small positive (resp. negative) amount to any diagonal element of $L$, the zero eigenvalue is pushed into the right (resp. left) half plane. Question: Consider a diagonal but indefinite matrix $B = \mathrm{diag}(b_{11},\ldots,b_{nn})$. What are sufficient conditions on $B$ such that $L + B$ is positive definite? What I know: Obviously if $B$ was positive semi-definite the result would follow. I've found cases where a small negative $b_{ii}$ cannot be compensated for by a sufficiently large $b_{jj}$, so a condition of the form $Trace(B) >\!\!> 0$ won't work. A Guess: Any negative element added at node $i$ must be corrected for with a positive addition at a (or several) nodes which are ""sufficiently connected"" in the graph to node i. Thoughts appreciated! -John","Setup: Consider a Laplacian (or Kirchoff) matrix $L = L^T \in \mathbb{R}^{n \times n}$ corresponding to a weighted, undirected and connected graph. That is, a matrix with $L_{ij} \leq 0$ for $i\neq j$ and $L_{ii} = -\sum_{j=1}^n L_{ij} > 0$. So $L$ has zero row sum, and is positive semidefinite with a simple eigenvalue at $0$. It's well known that if you add a small positive (resp. negative) amount to any diagonal element of $L$, the zero eigenvalue is pushed into the right (resp. left) half plane. Question: Consider a diagonal but indefinite matrix $B = \mathrm{diag}(b_{11},\ldots,b_{nn})$. What are sufficient conditions on $B$ such that $L + B$ is positive definite? What I know: Obviously if $B$ was positive semi-definite the result would follow. I've found cases where a small negative $b_{ii}$ cannot be compensated for by a sufficiently large $b_{jj}$, so a condition of the form $Trace(B) >\!\!> 0$ won't work. A Guess: Any negative element added at node $i$ must be corrected for with a positive addition at a (or several) nodes which are ""sufficiently connected"" in the graph to node i. Thoughts appreciated! -John",,"['linear-algebra', 'algebraic-graph-theory', 'spectral-graph-theory']"
82,Dimension of solution space for system of linear inequalities,Dimension of solution space for system of linear inequalities,,"Let's say I have a system of inequalities: $Ax \leq g$ for some $A \in \mathbb{R}^{4\times4}$, $x \in \mathbb{R}^4$, $g \in \mathbb{R}^4$, and $A$ is full rank. Here, the $\leq$ denotes element-wise inequality. Specifically, I know that $x$ lies in a two-dimensional subspace of $\mathbb{R}^4$ (determined by the null space of some matrix $N$). What I'm interested in is the dimension of the solution to the above system of inequalities. More succinctly, I'm interested in the dimension of the set $$ \left\{ x \in \mathbb{R}^4 \ \vert \ x \in {\rm Null}(N) , \ Ax \leq g\right\} $$ Understanding more about this set would be nice too, but the dimension would suffice. I'm really not sure how to approach this problem... at all. This problem arose while I was trying to analyze the set of solutions to a linear program, if you're curious.","Let's say I have a system of inequalities: $Ax \leq g$ for some $A \in \mathbb{R}^{4\times4}$, $x \in \mathbb{R}^4$, $g \in \mathbb{R}^4$, and $A$ is full rank. Here, the $\leq$ denotes element-wise inequality. Specifically, I know that $x$ lies in a two-dimensional subspace of $\mathbb{R}^4$ (determined by the null space of some matrix $N$). What I'm interested in is the dimension of the solution to the above system of inequalities. More succinctly, I'm interested in the dimension of the set $$ \left\{ x \in \mathbb{R}^4 \ \vert \ x \in {\rm Null}(N) , \ Ax \leq g\right\} $$ Understanding more about this set would be nice too, but the dimension would suffice. I'm really not sure how to approach this problem... at all. This problem arose while I was trying to analyze the set of solutions to a linear program, if you're curious.",,"['linear-algebra', 'matrices', 'inequality', 'linear-programming']"
83,"""Algorithmic"" proofs in linear algebra","""Algorithmic"" proofs in linear algebra",,"Although I am new to linear algebra, I want to study it with as much rigor as possible. After searching around, I picked up Halmos' Finite Dimensional Vector Spaces and Axler's Linear Algebra Done Right . I've noticed that they state theorems which they prove by a method which I would describe as ""algorithmic"". For example, verbatim from Axler (although Halmos is very similar): Theorem: In a finite-dimensional vector space, the length of every lin. ind. tuple is $\leq$ to the length of every spanning tuple   of vectors. Proof : Suppose that ($u_1$, ... $u_m$) is lin. ind. in $\mathcal{V}$ and ($w_1$, ... $w_n$) spans $\mathcal{V}$. We need to prove $m \leq n$. We do so through   the multi-step process described below.... Step 1 : The tuple $(w_1, ... w_n)$ spans $\mathcal{V}$, and thus adjoining any vector produces a linearly dependent tuple. In particular, the tuple $(u_1, w_1, ... w_n)$ is linearly independent. Thus, by the linear dependence lemma, we can remove one of the $w$'s so that the n -tuple B consisting of $u_1$ and the remaining $w$'s spans $\mathcal{V}$. Step j : The n -tuple B from step $j-1$ spans $\mathcal{V}$, and thus adjoining any vector to it produces a linearly dependent tuple. In particular, the $(n+1)$-tuple obtained by adjoining $u_j$ to B , placing it just after $u_1,...u_{j-1},$ is linearly dependent. By the linear dependence lemma (2.4), one of the vectors in this tuple is in the span of the previous ones.... We can remove that $w$ from $B$ so that the new $n$-tuple $B$ consisting of $u_1, ... u_j$ and the remaining $w$'s spans $\mathcal{V}$. After step $m$, we have added all the $u$'s and the process stops. If at any step we added a $u$ and had no more $w$'s to remove, then we would have a contradiction. Thus there must be at least as many $w$'s as $u$'s. I take issue with the level of rigor of ""algorithmic"" proof. Although I think these proofs might be amenable to treatment by induction, I'm not sure how to carry it out. As they stand, although I get the intuition, they don't really convince me. If I had to be precise about what bothers me, I'd say that the actual sets resulting from each step of the operation aren't stated explicitly, and I'm not sure how these sets are being ordered/indexed (they play fast and loose there). (Disclosure: I am generally not thrilled with ...'s, unless I can see a clear way to come up with an argument which doesn't rely on imagining ""what's going on in there"", so dealing with about ten of these arguments at one sitting is irritating for me.) Is there a way to make these arguments - in particular, this one - more precise?","Although I am new to linear algebra, I want to study it with as much rigor as possible. After searching around, I picked up Halmos' Finite Dimensional Vector Spaces and Axler's Linear Algebra Done Right . I've noticed that they state theorems which they prove by a method which I would describe as ""algorithmic"". For example, verbatim from Axler (although Halmos is very similar): Theorem: In a finite-dimensional vector space, the length of every lin. ind. tuple is $\leq$ to the length of every spanning tuple   of vectors. Proof : Suppose that ($u_1$, ... $u_m$) is lin. ind. in $\mathcal{V}$ and ($w_1$, ... $w_n$) spans $\mathcal{V}$. We need to prove $m \leq n$. We do so through   the multi-step process described below.... Step 1 : The tuple $(w_1, ... w_n)$ spans $\mathcal{V}$, and thus adjoining any vector produces a linearly dependent tuple. In particular, the tuple $(u_1, w_1, ... w_n)$ is linearly independent. Thus, by the linear dependence lemma, we can remove one of the $w$'s so that the n -tuple B consisting of $u_1$ and the remaining $w$'s spans $\mathcal{V}$. Step j : The n -tuple B from step $j-1$ spans $\mathcal{V}$, and thus adjoining any vector to it produces a linearly dependent tuple. In particular, the $(n+1)$-tuple obtained by adjoining $u_j$ to B , placing it just after $u_1,...u_{j-1},$ is linearly dependent. By the linear dependence lemma (2.4), one of the vectors in this tuple is in the span of the previous ones.... We can remove that $w$ from $B$ so that the new $n$-tuple $B$ consisting of $u_1, ... u_j$ and the remaining $w$'s spans $\mathcal{V}$. After step $m$, we have added all the $u$'s and the process stops. If at any step we added a $u$ and had no more $w$'s to remove, then we would have a contradiction. Thus there must be at least as many $w$'s as $u$'s. I take issue with the level of rigor of ""algorithmic"" proof. Although I think these proofs might be amenable to treatment by induction, I'm not sure how to carry it out. As they stand, although I get the intuition, they don't really convince me. If I had to be precise about what bothers me, I'd say that the actual sets resulting from each step of the operation aren't stated explicitly, and I'm not sure how these sets are being ordered/indexed (they play fast and loose there). (Disclosure: I am generally not thrilled with ...'s, unless I can see a clear way to come up with an argument which doesn't rely on imagining ""what's going on in there"", so dealing with about ten of these arguments at one sitting is irritating for me.) Is there a way to make these arguments - in particular, this one - more precise?",,"['linear-algebra', 'vector-spaces', 'proof-writing']"
84,Non linear transformation satisfying $T(x+y)=T(x)+T(y)$,Non linear transformation satisfying,T(x+y)=T(x)+T(y),"Given V a vector space with vectors and scalars $\mathbb{C}$, does there exists a non linear transformation $T:V\rightarrow V$ such that $T(x+y)=T(x)+T(y)$ for all $x,y\in V$? I think such a transformation will be 'like' one that satisfies Cauchy's functional equation $f(x+y)=f(x)+f(y)$ without any other conditions, but other than that, I have no idea.","Given V a vector space with vectors and scalars $\mathbb{C}$, does there exists a non linear transformation $T:V\rightarrow V$ such that $T(x+y)=T(x)+T(y)$ for all $x,y\in V$? I think such a transformation will be 'like' one that satisfies Cauchy's functional equation $f(x+y)=f(x)+f(y)$ without any other conditions, but other than that, I have no idea.",,['linear-algebra']
85,When  image + kernel span the whole space,When  image + kernel span the whole space,,"Can someone tell me if and why $\text{Im}\,f+\ker f=R$ holds for a selfadjoint operator $f:R\rightarrow R$, where $R$ is a finite dimensional inner product space? Can someone give me an example of an operator in an infinite dimensional space for which that equation is not true ?","Can someone tell me if and why $\text{Im}\,f+\ker f=R$ holds for a selfadjoint operator $f:R\rightarrow R$, where $R$ is a finite dimensional inner product space? Can someone give me an example of an operator in an infinite dimensional space for which that equation is not true ?",,['linear-algebra']
86,Are translations of logarithms linearly independent?,Are translations of logarithms linearly independent?,,"I think I proved the following but I am not sure. I will write my answer at the bottom Is the set of logarithms $ \lbrace\ln (t + a_i)\rbrace_{i=1}^N $ with $t,a_i>0$, and all $a_i$ different linearly independent? $N$ is a given integer. Edit :  My solution is the following: The condition for linear dependence is that the function \begin{equation} f(t) = \sum_{i=1}^N c_i \ln (t + a_i) \equiv 0  \end{equation} for some $c_i$ not all zero. The function can be rewritten into \begin{equation} f(t) = \ln \left(\prod_{i=1}^N (t + a_i)^{c_i} \right) \equiv 0 \end{equation} which implies that the function \begin{equation} g(t) = \prod_{i=1}^N (t + a_i)^{c_i} \equiv 1 \end{equation} note that $g(t)$ can never be zero due to the hypothesis on $t$ and $a_i$. Taking derivatives with respect to time we obtain \begin{equation} g'(t) = \left( \sum_{i=1}^{N}\frac{c_i}{t+a_i}\right)\prod_{i=1}^N (t+a_i) ^{c_i} = \sum_{i=1}^{N}\frac{{c}_{i}}{t+{a}_{i}} g(t) \equiv 0 \end{equation} Since $g(t)$ is never zero (and always one) we can remove it and obtain the equality \begin{equation} \sum_{i=0}^{N}\frac{{c}_{i}}{t+{a}_{i}} \equiv 0 \end{equation} Note that this states that translations of negative powers (of degree -1) are also linearly dependent. Using the idea in this answer we can remove the denominators obtaining  \begin{equation} \sum_{i=1}^N {c_i}\prod_{j\neq i}(t + a_j) \equiv 0. \end{equation} The left-hand side of that equation defines a polynomial of degree $N-1$ in the variable $t$. Evaluating it at $N$ different values of $t$ produces a contradiction, since a polynomial of degree $N-1$ cannot have $N$ roots. Hence all $c_i$ must be zero. Therefore a) Translations of negative powers (of degree -1) are linearly independent. b) $g(t) \not\equiv 1$. c) The given logarithms are linearly independent.","I think I proved the following but I am not sure. I will write my answer at the bottom Is the set of logarithms $ \lbrace\ln (t + a_i)\rbrace_{i=1}^N $ with $t,a_i>0$, and all $a_i$ different linearly independent? $N$ is a given integer. Edit :  My solution is the following: The condition for linear dependence is that the function \begin{equation} f(t) = \sum_{i=1}^N c_i \ln (t + a_i) \equiv 0  \end{equation} for some $c_i$ not all zero. The function can be rewritten into \begin{equation} f(t) = \ln \left(\prod_{i=1}^N (t + a_i)^{c_i} \right) \equiv 0 \end{equation} which implies that the function \begin{equation} g(t) = \prod_{i=1}^N (t + a_i)^{c_i} \equiv 1 \end{equation} note that $g(t)$ can never be zero due to the hypothesis on $t$ and $a_i$. Taking derivatives with respect to time we obtain \begin{equation} g'(t) = \left( \sum_{i=1}^{N}\frac{c_i}{t+a_i}\right)\prod_{i=1}^N (t+a_i) ^{c_i} = \sum_{i=1}^{N}\frac{{c}_{i}}{t+{a}_{i}} g(t) \equiv 0 \end{equation} Since $g(t)$ is never zero (and always one) we can remove it and obtain the equality \begin{equation} \sum_{i=0}^{N}\frac{{c}_{i}}{t+{a}_{i}} \equiv 0 \end{equation} Note that this states that translations of negative powers (of degree -1) are also linearly dependent. Using the idea in this answer we can remove the denominators obtaining  \begin{equation} \sum_{i=1}^N {c_i}\prod_{j\neq i}(t + a_j) \equiv 0. \end{equation} The left-hand side of that equation defines a polynomial of degree $N-1$ in the variable $t$. Evaluating it at $N$ different values of $t$ produces a contradiction, since a polynomial of degree $N-1$ cannot have $N$ roots. Hence all $c_i$ must be zero. Therefore a) Translations of negative powers (of degree -1) are linearly independent. b) $g(t) \not\equiv 1$. c) The given logarithms are linearly independent.",,"['linear-algebra', 'functions']"
87,Openness of $\varphi(U_Q \cap U_{Q'})$ in the definition of Grassmannian Manifolds (Lee: Introduction to Smooth Manifolds),Openness of  in the definition of Grassmannian Manifolds (Lee: Introduction to Smooth Manifolds),\varphi(U_Q \cap U_{Q'}),"I am reading Lee's Introduction to Smooth Manifolds and I have some problems with definition of Grassmannian manifold given in Example 1.24, p.22 . I'll write the details below. My question is: Why is the set $\varphi(U_Q \cap U_{Q'}) \subset L(P, Q)$ (i.e., the set of all $A \in L(P, Q)$ whose graphs intersect both $Q$ and $Q'$ trivially) an open set? (Which topology is used on $L(P,Q)$. Should I take the standard topology from $\mathbb R^{k(n-k)}$ and an identification of this space with $L(P,Q)$?) Here is the relevant part from the book: Example 1.24 (Grassmann Manifolds) . Let $V$ be an $n$-dimensional   real vector space. For any integer $0 \le k \le n$, we let $G_k(V)$ denote the set   of all $k$-dimensional linear subspaces of $V$ . We will show that $G_k(V)$ can be   naturally given the structure of a smooth manifold of dimension $k(n-k)$.   The construction is somewhat more involved than the ones we have done   so far, but the basic idea is just to use linear algebra to construct charts for   $G_k(V)$ and then use the smooth manifold construction lemma (Lemma 1.23). Let $P$ and $Q$ be any complementary subspaces of $V$ of dimensions $k$ and   $(n-k)$, respectively, so that $V$ decomposes as a direct sum: $V = P \oplus Q$.   The graph of any linear map $A \colon P \to Q$ is a $k$-dimensional subspace $\Gamma(A) \subset V$,   defined by   $$\Gamma(A)=\{x+Ax: x\in P\}.$$   Any such subspace has the property that its intersection with $Q$ is the zero   subspace. Conversely, any subspace with this property is easily seen to be   the graph of a unique linear map $A\colon P \to Q$. Let $L(P, Q)$ denote the vector space of linear maps from $P$ to $Q$, and   let $U_Q$ denote the subset of $G_k(V)$ consisting of $k$-dimensional subspaces   whose intersection with $Q$ is trivial. Define a map $\psi \colon L(P, Q) \to U_Q$ by   $$\psi(A)=\Gamma(A).$$   The discussion above shows that $\psi$ is a bijection. Let $\varphi = \psi^{-1} \colon U_Q \to L(P, Q)$. By choosing bases for $P$ and $Q$, we can identify $L(P, Q)$ with   $M((n-k)\times k, \mathbb R)$ and hence with $\mathbb R^{k(n-k)}$, and thus we can think of $(U_Q,\varphi)$   as a coordinate chart. Since the image of each chart is all of $L(P, Q)$, condition (i) of Lemma 1.23 is clearly satisfied. Now let $(P', Q')$ be any other such pair of subspaces, and let $\psi'$, $\varphi'$ be   the corresponding maps. The set $\varphi(U_Q \cap U_{Q'} ) \subset L(P, Q)$ consists of all   $A \in L(P, Q)$ whose graphs intersect both $Q$ and $Q'$ trivially, which is easily   seen to be an open set, so (ii) holds.","I am reading Lee's Introduction to Smooth Manifolds and I have some problems with definition of Grassmannian manifold given in Example 1.24, p.22 . I'll write the details below. My question is: Why is the set $\varphi(U_Q \cap U_{Q'}) \subset L(P, Q)$ (i.e., the set of all $A \in L(P, Q)$ whose graphs intersect both $Q$ and $Q'$ trivially) an open set? (Which topology is used on $L(P,Q)$. Should I take the standard topology from $\mathbb R^{k(n-k)}$ and an identification of this space with $L(P,Q)$?) Here is the relevant part from the book: Example 1.24 (Grassmann Manifolds) . Let $V$ be an $n$-dimensional   real vector space. For any integer $0 \le k \le n$, we let $G_k(V)$ denote the set   of all $k$-dimensional linear subspaces of $V$ . We will show that $G_k(V)$ can be   naturally given the structure of a smooth manifold of dimension $k(n-k)$.   The construction is somewhat more involved than the ones we have done   so far, but the basic idea is just to use linear algebra to construct charts for   $G_k(V)$ and then use the smooth manifold construction lemma (Lemma 1.23). Let $P$ and $Q$ be any complementary subspaces of $V$ of dimensions $k$ and   $(n-k)$, respectively, so that $V$ decomposes as a direct sum: $V = P \oplus Q$.   The graph of any linear map $A \colon P \to Q$ is a $k$-dimensional subspace $\Gamma(A) \subset V$,   defined by   $$\Gamma(A)=\{x+Ax: x\in P\}.$$   Any such subspace has the property that its intersection with $Q$ is the zero   subspace. Conversely, any subspace with this property is easily seen to be   the graph of a unique linear map $A\colon P \to Q$. Let $L(P, Q)$ denote the vector space of linear maps from $P$ to $Q$, and   let $U_Q$ denote the subset of $G_k(V)$ consisting of $k$-dimensional subspaces   whose intersection with $Q$ is trivial. Define a map $\psi \colon L(P, Q) \to U_Q$ by   $$\psi(A)=\Gamma(A).$$   The discussion above shows that $\psi$ is a bijection. Let $\varphi = \psi^{-1} \colon U_Q \to L(P, Q)$. By choosing bases for $P$ and $Q$, we can identify $L(P, Q)$ with   $M((n-k)\times k, \mathbb R)$ and hence with $\mathbb R^{k(n-k)}$, and thus we can think of $(U_Q,\varphi)$   as a coordinate chart. Since the image of each chart is all of $L(P, Q)$, condition (i) of Lemma 1.23 is clearly satisfied. Now let $(P', Q')$ be any other such pair of subspaces, and let $\psi'$, $\varphi'$ be   the corresponding maps. The set $\varphi(U_Q \cap U_{Q'} ) \subset L(P, Q)$ consists of all   $A \in L(P, Q)$ whose graphs intersect both $Q$ and $Q'$ trivially, which is easily   seen to be an open set, so (ii) holds.",,"['linear-algebra', 'differential-geometry', 'grassmannian']"
88,Interpreting the determinant of matrices of dot products,Interpreting the determinant of matrices of dot products,,"In the Euclidean space $\mathbb{R}^n$ consider two (ordered) sets of vectors $a_1 \ldots a_k$ and $b_1 \ldots b_k$ with $k \le n$ . Question What is the geometrical interpretation of $\det(a_i \cdot b_j)?$ Is it true that $\det(a_i\cdot b_j)=\det(a'_p\cdot b'_q)$ if $a_1\wedge \ldots \wedge a_k=a'_1\wedge \ldots \wedge a'_k$ and $b_1\wedge \ldots \wedge b_k=b'_1\wedge \ldots \wedge b'_k?$ Since $\det(a_i\cdot a_j)$ equals the squared $k$ -volume spanned by $a_1\ldots a_k$ , I guess that $\det(a_i\cdot b_j)$ may be interpreted as the $k$ -volume spanned by some kind of projection of $b_1\wedge \ldots \wedge b_k$ (thought of as an oriented $k$ -parallelogram) onto $a_1\wedge \ldots \wedge a_k$ . For the same reason I would answer affirmatively to the second question.","In the Euclidean space consider two (ordered) sets of vectors and with . Question What is the geometrical interpretation of Is it true that if and Since equals the squared -volume spanned by , I guess that may be interpreted as the -volume spanned by some kind of projection of (thought of as an oriented -parallelogram) onto . For the same reason I would answer affirmatively to the second question.",\mathbb{R}^n a_1 \ldots a_k b_1 \ldots b_k k \le n \det(a_i \cdot b_j)? \det(a_i\cdot b_j)=\det(a'_p\cdot b'_q) a_1\wedge \ldots \wedge a_k=a'_1\wedge \ldots \wedge a'_k b_1\wedge \ldots \wedge b_k=b'_1\wedge \ldots \wedge b'_k? \det(a_i\cdot a_j) k a_1\ldots a_k \det(a_i\cdot b_j) k b_1\wedge \ldots \wedge b_k k a_1\wedge \ldots \wedge a_k,"['linear-algebra', 'geometry', 'exterior-algebra']"
89,"""Algebraic multiplicity"" for eigenvalues of a Sturm-Liouville-like problem?","""Algebraic multiplicity"" for eigenvalues of a Sturm-Liouville-like problem?",,"Following Coddington-Levinson's book Theory of ordinary differential equations , chapter 7: ""Self-adjoint problems on finite intervals"", let us consider the eigenvalue problem $$\pi(l):\begin{cases} Lx(t)= lx(t) & t \in [a, b] \\ Ux=0 \end{cases}$$ where $Lx=p_0(t)x^{(n)}+p_1(t)x^{(n-1)}+\ldots + p_n(t)x(t)$ (with $p_0(t)\ne 0$, the problem is not singular) and $Ux=0$ stands for the boundary conditions $$U_jx=\sum_{k=1}^n(M_{jk}x^{(k-1)}(a)+N_{jk}x^{(k-1)}(b)),\qquad j=1\ldots n.$$ Also let $\pi$ be self-adjoint , meaning that $\int_a^b Lu\overline{v}\, dt=\int_a^bu\overline{Lv}\, dt$ for all $u, v \in C^n$ satisfying boundary conditions $Uu=Uv=0$. We say that $l\in \mathbb{C}$ is an eigenvalue of $\pi$ if $\pi(l)$ admits non trivial solutions. Coddington-Levinson's theorem 2.1 asserts that all eigenvalues are real and that they have no finite cluster point. What is interesting for this question is the proof: the authors start taking a fundamental system $\{\varphi_j, j=1\ldots n\}$ of solutions of the linear equation $Lx=lx$, observing that each $\varphi_j$ depends analytically on $l$. Then they point out that the generic solution $$x=\sum_{j=1}^nc_j \varphi_j$$ of $Lx=lx$ is an eigenvalue of $\pi$ if and only if $$\tag{1} \sum_{j=1}^n c_j U_k\varphi_j=0 \qquad k=1\ldots n, $$ which is a system of $n$ homogeneous linear equations in $n$ unknowns $c_1 \ldots c_n$. The determinant $\Delta$ of  $(1)$ is an entire function of $l$ and it vanishes exactly at the eigenvalues of $\pi$. At this point the authors finish off their proof, while we proceed to our question. Question This $\Delta$, being entire and vanishing at eigenvalues, might be regarded as an infinite-dimensional analogue of the   characteristic polynomial of a matrix. Is there any relationship   between the multiplicity of its zeros and the geometric multiplicity of the   corresponding eigenvalues (i.e. the dimension of the associated   eigenspaces)? Thank you.","Following Coddington-Levinson's book Theory of ordinary differential equations , chapter 7: ""Self-adjoint problems on finite intervals"", let us consider the eigenvalue problem $$\pi(l):\begin{cases} Lx(t)= lx(t) & t \in [a, b] \\ Ux=0 \end{cases}$$ where $Lx=p_0(t)x^{(n)}+p_1(t)x^{(n-1)}+\ldots + p_n(t)x(t)$ (with $p_0(t)\ne 0$, the problem is not singular) and $Ux=0$ stands for the boundary conditions $$U_jx=\sum_{k=1}^n(M_{jk}x^{(k-1)}(a)+N_{jk}x^{(k-1)}(b)),\qquad j=1\ldots n.$$ Also let $\pi$ be self-adjoint , meaning that $\int_a^b Lu\overline{v}\, dt=\int_a^bu\overline{Lv}\, dt$ for all $u, v \in C^n$ satisfying boundary conditions $Uu=Uv=0$. We say that $l\in \mathbb{C}$ is an eigenvalue of $\pi$ if $\pi(l)$ admits non trivial solutions. Coddington-Levinson's theorem 2.1 asserts that all eigenvalues are real and that they have no finite cluster point. What is interesting for this question is the proof: the authors start taking a fundamental system $\{\varphi_j, j=1\ldots n\}$ of solutions of the linear equation $Lx=lx$, observing that each $\varphi_j$ depends analytically on $l$. Then they point out that the generic solution $$x=\sum_{j=1}^nc_j \varphi_j$$ of $Lx=lx$ is an eigenvalue of $\pi$ if and only if $$\tag{1} \sum_{j=1}^n c_j U_k\varphi_j=0 \qquad k=1\ldots n, $$ which is a system of $n$ homogeneous linear equations in $n$ unknowns $c_1 \ldots c_n$. The determinant $\Delta$ of  $(1)$ is an entire function of $l$ and it vanishes exactly at the eigenvalues of $\pi$. At this point the authors finish off their proof, while we proceed to our question. Question This $\Delta$, being entire and vanishing at eigenvalues, might be regarded as an infinite-dimensional analogue of the   characteristic polynomial of a matrix. Is there any relationship   between the multiplicity of its zeros and the geometric multiplicity of the   corresponding eigenvalues (i.e. the dimension of the associated   eigenspaces)? Thank you.",,"['linear-algebra', 'ordinary-differential-equations', 'spectral-theory']"
90,Lie Algebra Homomorphism Question,Lie Algebra Homomorphism Question,,"So this is a bit of a follow-up to my recent question .  I don't mean to inundate the feed with my quandaries, but as I move through the theory I keep hitting stumbling blocks (which y'all so kindly help me through). As done previously, the group action defined by $h(g)p(z)=p(g^{-1}z)$ where $g\in\rm{SL}(3,\Bbb C)$ , $p$ is from the vector space of polynomials of degree $\le2$ in three variables, and $z\in \Bbb C^3$ . I'm now introduced to a new function, $dh(X)=\left.\frac{d}{dt}h(e^{XT}]\right|_{t=0}$ ,  where $X$ resides in the lie algebra of $\rm SL(3,\Bbb C)$ [ie $\mathfrak{sl}(3,\Bbb C)$ ].  The goal is: show that this is a lie algebra homomorphism $\mathfrak{sl}(3,\Bbb C)\to \mathfrak{gl}(6,\Bbb C)$ . Our basis in our space of polynomials is the standard one.  Namely, the degree 2 terms in their various permutations. I've been looking into the complexification of $\mathfrak{su}(3,\Bbb C)$ as a way of making sense of the polynomial when acted on by h, but I can't seem to get a handle on how to understand the derivative.  I have a hunch this probably isn't even remotely the right course of action. Any and all help is much appreciated.  Feel free to assume I know the bare minimum. Edit:  While the induced homomorphism approach is awesome, a direct proof would help me get a better feel for how the matrix exponential is affecting the polynomial.  Also, I'm too ""machinery illiterate"" to understand some of the more general formalisms at this point.","So this is a bit of a follow-up to my recent question .  I don't mean to inundate the feed with my quandaries, but as I move through the theory I keep hitting stumbling blocks (which y'all so kindly help me through). As done previously, the group action defined by where , is from the vector space of polynomials of degree in three variables, and . I'm now introduced to a new function, ,  where resides in the lie algebra of [ie ].  The goal is: show that this is a lie algebra homomorphism . Our basis in our space of polynomials is the standard one.  Namely, the degree 2 terms in their various permutations. I've been looking into the complexification of as a way of making sense of the polynomial when acted on by h, but I can't seem to get a handle on how to understand the derivative.  I have a hunch this probably isn't even remotely the right course of action. Any and all help is much appreciated.  Feel free to assume I know the bare minimum. Edit:  While the induced homomorphism approach is awesome, a direct proof would help me get a better feel for how the matrix exponential is affecting the polynomial.  Also, I'm too ""machinery illiterate"" to understand some of the more general formalisms at this point.","h(g)p(z)=p(g^{-1}z) g\in\rm{SL}(3,\Bbb C) p \le2 z\in \Bbb C^3 dh(X)=\left.\frac{d}{dt}h(e^{XT}]\right|_{t=0} X \rm SL(3,\Bbb C) \mathfrak{sl}(3,\Bbb C) \mathfrak{sl}(3,\Bbb C)\to \mathfrak{gl}(6,\Bbb C) \mathfrak{su}(3,\Bbb C)","['linear-algebra', 'group-theory', 'differential-geometry', 'lie-algebras', 'lie-groups']"
91,what does following matrix says geometrically,what does following matrix says geometrically,,"Let $M\subset \mathbb C^2$ be a hypersurface defined by $F(z,w)=0$. Then for some point $p\in M$, I've $$\text{ rank of }\left(     \begin{array}{ccc}      0 &\frac{\partial F}{\partial z} &\frac{\partial F}{\partial w} \\  \frac{\partial F}{\partial z} &\frac{\partial^2 F}{\partial ^ 2z} &\frac{\partial^2 F}{\partial z\partial w} \\  \frac{\partial F}{\partial w} &\frac{\partial^2 F}{\partial w\partial z} &  \frac{\partial^2 F}{\partial w^2} \\                     \end{array}                   \right)_{\text{ at p}}=2.$$ What does it mean geometrically? Can anyone give a geometric picture near $p$? Any comment, suggestion, please. Edit: Actually I was reading about Levi flat points and Pseudo-convex domains.  I want to understand the relation between these two concepts. A point p for which the rank of the above matrix is 2 is called Levi flat. If the surface is everywhere Levi flat then it is locally equivalent to $(0,1)\times \mathbb{C}^n$, so I have many examples....but what will happen for others for example take the three sphere in $\mathbb{C}^2$ given by $F(z,w)=|z|^2+|w|^2−1=0$.  This doesn't satisfy the rank 2 condition.  Can I have precisely these two situations?","Let $M\subset \mathbb C^2$ be a hypersurface defined by $F(z,w)=0$. Then for some point $p\in M$, I've $$\text{ rank of }\left(     \begin{array}{ccc}      0 &\frac{\partial F}{\partial z} &\frac{\partial F}{\partial w} \\  \frac{\partial F}{\partial z} &\frac{\partial^2 F}{\partial ^ 2z} &\frac{\partial^2 F}{\partial z\partial w} \\  \frac{\partial F}{\partial w} &\frac{\partial^2 F}{\partial w\partial z} &  \frac{\partial^2 F}{\partial w^2} \\                     \end{array}                   \right)_{\text{ at p}}=2.$$ What does it mean geometrically? Can anyone give a geometric picture near $p$? Any comment, suggestion, please. Edit: Actually I was reading about Levi flat points and Pseudo-convex domains.  I want to understand the relation between these two concepts. A point p for which the rank of the above matrix is 2 is called Levi flat. If the surface is everywhere Levi flat then it is locally equivalent to $(0,1)\times \mathbb{C}^n$, so I have many examples....but what will happen for others for example take the three sphere in $\mathbb{C}^2$ given by $F(z,w)=|z|^2+|w|^2−1=0$.  This doesn't satisfy the rank 2 condition.  Can I have precisely these two situations?",,"['linear-algebra', 'geometry', 'differential-geometry', 'intuition', 'complex-geometry']"
92,Eigenvalues of a parameter dependent matrix,Eigenvalues of a parameter dependent matrix,,"Given the parameter dependent matrix $A=\begin{pmatrix} 0 & I\\ A_1 & kA_2\end{pmatrix}$, with $A_1, A_2\in \mathbb{R}^{n\times n}$ and $k\in\mathbb{R}>0$, is there a way to display the eigenvalues of $A$ as a function of $A_1, A_2, k$? Or to give an estimation of the eigenvalues' location? What can be said about the corresponding eigenvectors of $A$ dependent on $k$? We can suppose that $A_1,A_2$ have full rank and all eigenvalues are negative or have a negative real part. They may have repeated eigenvalues. Clearly if $\lambda(A)$ is an eigenvalue, for $k=0$ we have $\lambda(A)=\pm \sqrt{\lambda(-A_1)}$, and for $k\rightarrow \infty$ we have $n$ eigenvalues $\lambda(A)=0$ and $n$ eigenvalues $\lambda(A)=\lambda(kA_2)$. Furthermore we have $k\mathrm{tr}(B)=\sum_{i=1}^{2n} \lambda_i(A)$. Assuming that $\mathrm{tr}(B)<0$ this would mean that at least some eigenvalues of $A$ have negative real parts of magnitude growing with $k$. I'm interested in what happens for $0\ll k\ll\infty$. My intuition is that with growing $k$ the eigenvalues of $A$ will move into the left half-plane, before tending to $-\infty$ (as all eigenvalues of $A_2$ have negative real parts) or $0$ but so far I haven't been able to find a suitable proof method for this.","Given the parameter dependent matrix $A=\begin{pmatrix} 0 & I\\ A_1 & kA_2\end{pmatrix}$, with $A_1, A_2\in \mathbb{R}^{n\times n}$ and $k\in\mathbb{R}>0$, is there a way to display the eigenvalues of $A$ as a function of $A_1, A_2, k$? Or to give an estimation of the eigenvalues' location? What can be said about the corresponding eigenvectors of $A$ dependent on $k$? We can suppose that $A_1,A_2$ have full rank and all eigenvalues are negative or have a negative real part. They may have repeated eigenvalues. Clearly if $\lambda(A)$ is an eigenvalue, for $k=0$ we have $\lambda(A)=\pm \sqrt{\lambda(-A_1)}$, and for $k\rightarrow \infty$ we have $n$ eigenvalues $\lambda(A)=0$ and $n$ eigenvalues $\lambda(A)=\lambda(kA_2)$. Furthermore we have $k\mathrm{tr}(B)=\sum_{i=1}^{2n} \lambda_i(A)$. Assuming that $\mathrm{tr}(B)<0$ this would mean that at least some eigenvalues of $A$ have negative real parts of magnitude growing with $k$. I'm interested in what happens for $0\ll k\ll\infty$. My intuition is that with growing $k$ the eigenvalues of $A$ will move into the left half-plane, before tending to $-\infty$ (as all eigenvalues of $A_2$ have negative real parts) or $0$ but so far I haven't been able to find a suitable proof method for this.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
93,Does the triangle inequality for the absolute value hold for matrix trace?,Does the triangle inequality for the absolute value hold for matrix trace?,,"It is well-known that, $\left|m-n\right|\ge\left|\left|m\right|-\left|n\right|\right|$ for real numbers. But if one defines $\left|M\right|=\sqrt{M^2}$ for a symmetric matrix $M$, does one have $$\operatorname{trace}\left(\left|M-N\right|\right)\ge\operatorname{trace}\left(\left|\left|M\right|-\left|N\right|\right|\right)$$ if $M\ne\left|M\right|$?","It is well-known that, $\left|m-n\right|\ge\left|\left|m\right|-\left|n\right|\right|$ for real numbers. But if one defines $\left|M\right|=\sqrt{M^2}$ for a symmetric matrix $M$, does one have $$\operatorname{trace}\left(\left|M-N\right|\right)\ge\operatorname{trace}\left(\left|\left|M\right|-\left|N\right|\right|\right)$$ if $M\ne\left|M\right|$?",,"['linear-algebra', 'matrices', 'inequality', 'absolute-value', 'trace']"
94,Inner Product Spaces and Minimum Polynomials,Inner Product Spaces and Minimum Polynomials,,"Problem Let $V$ be the vector space of real polynomials $\mathbb{R}[x]$ endowed with the inner product $$ (f,g)=\int_{-\infty}^{\infty}e^{-|x|}f(x)g(x)dx $$ By considering the series of subspaces $\{V_n\}$ where $V_n=\{f(x) \in \mathbb{R}[X]: \deg(f) \leq n\}$ or otherwise, show that there exist unique monic polynomials $\varphi_n(x)$ for $n \geq 0$ such that $$\int_{-\infty}^{\infty}e^{-|x|}\varphi_n(x)g(x)dx=0$$ whenever deg   $g<n$, and find $\varphi_n(x)$ for $n=0,1,2$. Progress We look to identify $(V_n)^{\perp}$ for  each $n$, i.e. $$(V_n)^{\perp}=\{f(x) \in \mathbb{R}[X]: \langle f,g \rangle=0 \text{ for all } g \in V_n\}$$ If we can show this has dimension $1$, then we can show the existence of a unique monic polynomial. Any thoughts on how this is done (or an alternative method for the problem) would be greatly appreciated. Regards.","Problem Let $V$ be the vector space of real polynomials $\mathbb{R}[x]$ endowed with the inner product $$ (f,g)=\int_{-\infty}^{\infty}e^{-|x|}f(x)g(x)dx $$ By considering the series of subspaces $\{V_n\}$ where $V_n=\{f(x) \in \mathbb{R}[X]: \deg(f) \leq n\}$ or otherwise, show that there exist unique monic polynomials $\varphi_n(x)$ for $n \geq 0$ such that $$\int_{-\infty}^{\infty}e^{-|x|}\varphi_n(x)g(x)dx=0$$ whenever deg   $g<n$, and find $\varphi_n(x)$ for $n=0,1,2$. Progress We look to identify $(V_n)^{\perp}$ for  each $n$, i.e. $$(V_n)^{\perp}=\{f(x) \in \mathbb{R}[X]: \langle f,g \rangle=0 \text{ for all } g \in V_n\}$$ If we can show this has dimension $1$, then we can show the existence of a unique monic polynomial. Any thoughts on how this is done (or an alternative method for the problem) would be greatly appreciated. Regards.",,"['linear-algebra', 'polynomials', 'inner-products']"
95,Prove $k(AB) \leq k(A)k(B)$ where $k(\cdot)$ denotes the condition number,Prove  where  denotes the condition number,k(AB) \leq k(A)k(B) k(\cdot),"Given a $n \times n$ matrix $A$ and $B$, we need to prove  $k(AB) \leq k(A)k(B)$ where $k(\cdot)$ denotes the condition number of a matrix. Is there any thing wrong in the below proof? $$k(AB)    = \|AB\| \cdot \|(AB)^{-1}\| \leq \|A\| \cdot \|B\| \cdot \|B^{-1}\| \cdot \|A^{-1}\|   =k(A)k(B) .$$","Given a $n \times n$ matrix $A$ and $B$, we need to prove  $k(AB) \leq k(A)k(B)$ where $k(\cdot)$ denotes the condition number of a matrix. Is there any thing wrong in the below proof? $$k(AB)    = \|AB\| \cdot \|(AB)^{-1}\| \leq \|A\| \cdot \|B\| \cdot \|B^{-1}\| \cdot \|A^{-1}\|   =k(A)k(B) .$$",,"['linear-algebra', 'matrices', 'condition-number']"
96,Is the Frobenius norm on exterior powers majorized by a power of the norm on matrices?,Is the Frobenius norm on exterior powers majorized by a power of the norm on matrices?,,"Let $A$ be an $n \times n$ matrix over the complex numbers. The Frobenius norm of $A$ is defined by $$ \| A \| = Tr(A \cdot A^*) $$ where $A^*$ is the conjugate transpose of $A$. Now let $\wedge^k A$ be the matrix of $k \times k$ minors of $A$. Then we have a Frobenius norm of $\wedge^k A$, given by $$ \| \wedge^k A \| = Tr(\wedge^k A \cdot \wedge^k A^*). $$ I would like to majorize the norm of $\wedge^k A$ by the one of $A$, and I'm fine with doing this very brutally. So, for example, is there a constant $C$ such that $\| \wedge^k A \| \leq C \| A \|^k$? It seems like that inequality should hold with $C = 1$, but I keep getting lost in the forest of symmetric polynomials on $k$ letters when I try to prove it.","Let $A$ be an $n \times n$ matrix over the complex numbers. The Frobenius norm of $A$ is defined by $$ \| A \| = Tr(A \cdot A^*) $$ where $A^*$ is the conjugate transpose of $A$. Now let $\wedge^k A$ be the matrix of $k \times k$ minors of $A$. Then we have a Frobenius norm of $\wedge^k A$, given by $$ \| \wedge^k A \| = Tr(\wedge^k A \cdot \wedge^k A^*). $$ I would like to majorize the norm of $\wedge^k A$ by the one of $A$, and I'm fine with doing this very brutally. So, for example, is there a constant $C$ such that $\| \wedge^k A \| \leq C \| A \|^k$? It seems like that inequality should hold with $C = 1$, but I keep getting lost in the forest of symmetric polynomials on $k$ letters when I try to prove it.",,"['linear-algebra', 'inequality']"
97,A coordinate free proof of the identity $Tr(A \otimes  B)= Tr(A) \otimes Tr(B)$,A coordinate free proof of the identity,Tr(A \otimes  B)= Tr(A) \otimes Tr(B),"I was going over some of the definitions of trace and was trying to find a way to prove one of the facts stated about the trace listed in the referenced Wikipedia article . Let $M, N$ be finitely generated projective $R$-modules over a commutative ring $R$ with identity. In particular I was wondering if there was a slick proof using the coordinate free language of the trace for the following fact. How do we show that for any $f \in End_R(M), v \in End_R(N)$ then $ Tr( f \otimes_R g) = Tr(f) \otimes_R Tr(g)$?","I was going over some of the definitions of trace and was trying to find a way to prove one of the facts stated about the trace listed in the referenced Wikipedia article . Let $M, N$ be finitely generated projective $R$-modules over a commutative ring $R$ with identity. In particular I was wondering if there was a slick proof using the coordinate free language of the trace for the following fact. How do we show that for any $f \in End_R(M), v \in End_R(N)$ then $ Tr( f \otimes_R g) = Tr(f) \otimes_R Tr(g)$?",,"['linear-algebra', 'abstract-algebra', 'modules']"
98,Construction of a basis,Construction of a basis,,"I know that for a vector space $\mathbb R^n$ one can use the Gram-Schmidt process to construct its basis. But what if the vector space is over some arbitrary field? I am thinking of the following: Pick an arbitrary vector in $V$, label as $v_1$ Pick another arbitrary vector in $V$. From this deduct the component in $v_1$. If this gives the zero vector then do it again with another arbitrary vector, otherwise take this as $v_2$ . Repeat the above until we have found $n$ linearly independent vectors. (Given that $\dim V=n < \infty$); otherwise, we go on forever. (Basically Gram-Schmidt.) This doesn't seem like a particularly efficient algorithm especially for large $n$, are there any better suggestions? Also, I am not sure that my steps are necessarily valid. Is the scalar product -- that obtains the component of an arbitrary vector in the direction of a $v_i$ already in the set -- defined for vector spaces over arbitrary fields? Thanks.","I know that for a vector space $\mathbb R^n$ one can use the Gram-Schmidt process to construct its basis. But what if the vector space is over some arbitrary field? I am thinking of the following: Pick an arbitrary vector in $V$, label as $v_1$ Pick another arbitrary vector in $V$. From this deduct the component in $v_1$. If this gives the zero vector then do it again with another arbitrary vector, otherwise take this as $v_2$ . Repeat the above until we have found $n$ linearly independent vectors. (Given that $\dim V=n < \infty$); otherwise, we go on forever. (Basically Gram-Schmidt.) This doesn't seem like a particularly efficient algorithm especially for large $n$, are there any better suggestions? Also, I am not sure that my steps are necessarily valid. Is the scalar product -- that obtains the component of an arbitrary vector in the direction of a $v_i$ already in the set -- defined for vector spaces over arbitrary fields? Thanks.",,"['linear-algebra', 'algorithms', 'vector-spaces']"
99,"Why is $\det(\vec{A},\vec{B}) = |\vec{A} \times \vec{B}|$?",Why is ?,"\det(\vec{A},\vec{B}) = |\vec{A} \times \vec{B}|","In the multivariable calculus class the teacher showed us the formula of the cross product $$ \vec{A} \times \vec{B} =\begin{vmatrix}\hat{\imath}& \hat{\jmath}& \hat{k} \\ a_1 & a_2 & a_3 \\b_1 & b_2 & b_3 \end{vmatrix}$$ And formula for determinant in two dimensions which can calculate the area of parallelogram in two dimensions by $$\det(\vec{A},\vec{B}) =\begin{vmatrix}a_1 & a_2 \\b_1 & b_2 \\\end{vmatrix}$$ Then teacher talked about the area of a parallelogram also being equal to the length of $\vec{A} \times \vec{B}$, that is $|\vec{A} \times \vec{B}|$, but gave no proof. I wanted to check this, so I used  $a_3=0,b_3=0$ just to have the $3 \times 3$ in the form that could be compared to $\det(\vec{A},\vec{B})$ form. When I expand the calculation, I do end up with  $|\hat{k}(a_1b_2 - a_2b_1)|$, and that equals to $(a_1b_2 - a_2b_1)$ The two forms are equal. Is this reasoning correct?","In the multivariable calculus class the teacher showed us the formula of the cross product $$ \vec{A} \times \vec{B} =\begin{vmatrix}\hat{\imath}& \hat{\jmath}& \hat{k} \\ a_1 & a_2 & a_3 \\b_1 & b_2 & b_3 \end{vmatrix}$$ And formula for determinant in two dimensions which can calculate the area of parallelogram in two dimensions by $$\det(\vec{A},\vec{B}) =\begin{vmatrix}a_1 & a_2 \\b_1 & b_2 \\\end{vmatrix}$$ Then teacher talked about the area of a parallelogram also being equal to the length of $\vec{A} \times \vec{B}$, that is $|\vec{A} \times \vec{B}|$, but gave no proof. I wanted to check this, so I used  $a_3=0,b_3=0$ just to have the $3 \times 3$ in the form that could be compared to $\det(\vec{A},\vec{B})$ form. When I expand the calculation, I do end up with  $|\hat{k}(a_1b_2 - a_2b_1)|$, and that equals to $(a_1b_2 - a_2b_1)$ The two forms are equal. Is this reasoning correct?",,"['linear-algebra', 'cross-product']"
