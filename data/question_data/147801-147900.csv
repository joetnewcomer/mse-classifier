,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,uniform continuity and equivalent sequences,uniform continuity and equivalent sequences,,"Let $X$ be a subset of $\mathbb{R}$, and let $f : X\to \mathbb{R}$ be a function. Then the following two statements are logically equivalent: (a) $f$ is uniformly continuous on $X$. (b) Whenever $(x_n)$ and $(y_n)$ are two equivalent sequences consisting of elements of $X$, the sequences $(f(x_n))$ and $(f(y_n))$ are also equivalent. Proof First I will state the definitions of uniform continuity and equivalent sequences. (Uniform continuity). Let $X$ be a subset of $\mathbb{R}$, and let $f : X\to\mathbb{R}$ be a function. We say that $f$ is uniformly continuous if, for every $\epsilon > 0$, there exists a $\delta > 0$ such that $f(x)$ and $f(x_0)$ are $\epsilon$-close whenever $x, x_0 \in X$ are to points in $X$ which are $\delta$-close. . (Equivalent sequences). Let $m$ be an integer, let $( a_n)_{n=m}^\infty$ and $( b_n)_{n=m}^\infty$ be two sequences of real numbers, and let $\epsilon> 0$ be given. We say that $( a_n)_{n=m}^\infty$ is $\epsilon$-close to $( b_n)_{n=m}^\infty$ iff $a_n$ is $\epsilon$-close to $b_n$ for each $n\geq m$. We say that $( a_n)$ is eventually $\epsilon$-close to $( b_n)$ iff there exists an $N\geq m$ such that the sequences $(a_n)$ and $(b_n)$ are $\epsilon$-close. Two sequences $(a_n)$ and $(b_n)$ are equivalent iff for each $\epsilon> 0$, the sequences $(a_n)$ and $(b_n)$ are eventually $\epsilon$-close. since $x\in X$ is an adherent point to $X$, then there exists a sequence $(a_n)$ such that $a_n \in X$ and converges to x. since f is continuous, then the sequence $(f(a_n))$ converges to $f(x)$. Let $(b_n)$  be a sequence equivalent to $(a_n)$. Therefore, $\forall \epsilon>0, \exists N\text{ such that } |a_n-b_n|\leq\epsilon$. choose $\epsilon=\delta$, then we have $|a_n-b_n|\leq\delta \forall n\geq N$ Hence, $|f(a_n)-f(b_n)|\leq\epsilon\text{   }, \forall n\geq N$ hence, $(f(a_n))$ and  $(f(b_n))$ are equivalent. Is my proof correct?","Let $X$ be a subset of $\mathbb{R}$, and let $f : X\to \mathbb{R}$ be a function. Then the following two statements are logically equivalent: (a) $f$ is uniformly continuous on $X$. (b) Whenever $(x_n)$ and $(y_n)$ are two equivalent sequences consisting of elements of $X$, the sequences $(f(x_n))$ and $(f(y_n))$ are also equivalent. Proof First I will state the definitions of uniform continuity and equivalent sequences. (Uniform continuity). Let $X$ be a subset of $\mathbb{R}$, and let $f : X\to\mathbb{R}$ be a function. We say that $f$ is uniformly continuous if, for every $\epsilon > 0$, there exists a $\delta > 0$ such that $f(x)$ and $f(x_0)$ are $\epsilon$-close whenever $x, x_0 \in X$ are to points in $X$ which are $\delta$-close. . (Equivalent sequences). Let $m$ be an integer, let $( a_n)_{n=m}^\infty$ and $( b_n)_{n=m}^\infty$ be two sequences of real numbers, and let $\epsilon> 0$ be given. We say that $( a_n)_{n=m}^\infty$ is $\epsilon$-close to $( b_n)_{n=m}^\infty$ iff $a_n$ is $\epsilon$-close to $b_n$ for each $n\geq m$. We say that $( a_n)$ is eventually $\epsilon$-close to $( b_n)$ iff there exists an $N\geq m$ such that the sequences $(a_n)$ and $(b_n)$ are $\epsilon$-close. Two sequences $(a_n)$ and $(b_n)$ are equivalent iff for each $\epsilon> 0$, the sequences $(a_n)$ and $(b_n)$ are eventually $\epsilon$-close. since $x\in X$ is an adherent point to $X$, then there exists a sequence $(a_n)$ such that $a_n \in X$ and converges to x. since f is continuous, then the sequence $(f(a_n))$ converges to $f(x)$. Let $(b_n)$  be a sequence equivalent to $(a_n)$. Therefore, $\forall \epsilon>0, \exists N\text{ such that } |a_n-b_n|\leq\epsilon$. choose $\epsilon=\delta$, then we have $|a_n-b_n|\leq\delta \forall n\geq N$ Hence, $|f(a_n)-f(b_n)|\leq\epsilon\text{   }, \forall n\geq N$ hence, $(f(a_n))$ and  $(f(b_n))$ are equivalent. Is my proof correct?",,"['real-analysis', 'analysis', 'proof-verification', 'uniform-continuity', 'epsilon-delta']"
1,Rudin's proof of the change of variable theorem,Rudin's proof of the change of variable theorem,,"I am having trouble with Rudin's proof of the change of variable theorem for multiple integrals. The theorem is for 1-1 $\mathscr{C'}$ mappings from $R^k$ into $R^k$. In theorem 10.7 just before the change of variable theorem, he proves that if $\mathbf{F}(\mathbf{x})$ is a $\mathscr{C'}$ mapping of an open set $E\subset{R^k}$ into $R^k$ with $0\in{E}$, with $\mathbf{F}(\mathbf{0})=0$ and $\mathbf{F'}(0)$ invertible, then there is a neighborhood of $\mathbf{0}$ in which the representation $$\mathbf{F}(\mathbf{x})=B_1\cdots B_{n-1}\mathbf{G}_n\circ \cdots \mathbf{G}_1(\mathbf{x})$$ is valid, with each $\mathbf{G}_i(\mathbf{x})$ being a primitive $\mathscr{C'}$ mapping in some neighborhood of zero, $\mathbf{G}_i(\mathbf{0})=0$, and $\mathbf{G'}_i(0)$ is invertible, and each $B_i$ is either a flip or the identity operator. In the change of variable theorem, he claims that we can write $T(\mathbf{x})$, our 1-1 $\mathscr{C'}$ on $R^k$ mapping, as $$\mathbf{T}(\mathbf{x})=\mathbf{T}(\mathbf{a})+B_1\cdots B_{k-1}\mathbf{G}_k\circ \cdots \mathbf{G}_1(\mathbf{x-a})$$ If $\mathbf{T}(\mathbf{x})$ is linear, I understand how theorem 10.7 applies, because $\mathbf{T}(\mathbf{0})=\mathbf{0}$ for all linear transformations and we can apply the theorem to $\mathbf{T}(\mathbf{x-a})$. But if T is not linear, how does he arrive at this equation? Secondly, even if the equation does hold, clearly $\mathbf{T}(\mathbf{x-a})$ is composition of primitive $\mathscr{C'}$ mappings and flips, but why is $\mathbf{T}(\mathbf{x})$. Doesn't the addition of the constant term $\mathbf{T}(\mathbf{x})$ change things?","I am having trouble with Rudin's proof of the change of variable theorem for multiple integrals. The theorem is for 1-1 $\mathscr{C'}$ mappings from $R^k$ into $R^k$. In theorem 10.7 just before the change of variable theorem, he proves that if $\mathbf{F}(\mathbf{x})$ is a $\mathscr{C'}$ mapping of an open set $E\subset{R^k}$ into $R^k$ with $0\in{E}$, with $\mathbf{F}(\mathbf{0})=0$ and $\mathbf{F'}(0)$ invertible, then there is a neighborhood of $\mathbf{0}$ in which the representation $$\mathbf{F}(\mathbf{x})=B_1\cdots B_{n-1}\mathbf{G}_n\circ \cdots \mathbf{G}_1(\mathbf{x})$$ is valid, with each $\mathbf{G}_i(\mathbf{x})$ being a primitive $\mathscr{C'}$ mapping in some neighborhood of zero, $\mathbf{G}_i(\mathbf{0})=0$, and $\mathbf{G'}_i(0)$ is invertible, and each $B_i$ is either a flip or the identity operator. In the change of variable theorem, he claims that we can write $T(\mathbf{x})$, our 1-1 $\mathscr{C'}$ on $R^k$ mapping, as $$\mathbf{T}(\mathbf{x})=\mathbf{T}(\mathbf{a})+B_1\cdots B_{k-1}\mathbf{G}_k\circ \cdots \mathbf{G}_1(\mathbf{x-a})$$ If $\mathbf{T}(\mathbf{x})$ is linear, I understand how theorem 10.7 applies, because $\mathbf{T}(\mathbf{0})=\mathbf{0}$ for all linear transformations and we can apply the theorem to $\mathbf{T}(\mathbf{x-a})$. But if T is not linear, how does he arrive at this equation? Secondly, even if the equation does hold, clearly $\mathbf{T}(\mathbf{x-a})$ is composition of primitive $\mathscr{C'}$ mappings and flips, but why is $\mathbf{T}(\mathbf{x})$. Doesn't the addition of the constant term $\mathbf{T}(\mathbf{x})$ change things?",,['analysis']
2,"$\int_0^1f(x)dx = 2, \int_0^1g(x)dx = 1, \text{and} \int_0^1[f(x)]^2 dx ≤ C$ for some constant $C > 4.$",for some constant,"\int_0^1f(x)dx = 2, \int_0^1g(x)dx = 1, \text{and} \int_0^1[f(x)]^2 dx ≤ C C > 4.","Suppose $f$ and $g$ are nonnegative measurable functions on the interval $[0,1],$ with the properties $$\int_0^1 f(x)\,dx = 2, \int_0^1g(x)\,dx = 1, \text{ and }\int_0^1[f(x)]^2 dx \le C$$ for some constant $C > 4.$ Let $E = \{x∈[0,1]:f(x)>g(x)\}$. Show that $E$ has measure $m(E) \ge 1/C.$ I am not sure what I should use for this.  Maybe Holder?  Any suggestions?","Suppose $f$ and $g$ are nonnegative measurable functions on the interval $[0,1],$ with the properties $$\int_0^1 f(x)\,dx = 2, \int_0^1g(x)\,dx = 1, \text{ and }\int_0^1[f(x)]^2 dx \le C$$ for some constant $C > 4.$ Let $E = \{x∈[0,1]:f(x)>g(x)\}$. Show that $E$ has measure $m(E) \ge 1/C.$ I am not sure what I should use for this.  Maybe Holder?  Any suggestions?",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
3,prove that $a^b\ge{b}^a$ where $a\le{b}$.,prove that  where .,a^b\ge{b}^a a\le{b},"prove that $a^b\ge{b}^a$ for all $a,b\ge3$. given that $a\le{b}$. I was trying to solve the question by graph. Can anyone help me please?","prove that $a^b\ge{b}^a$ for all $a,b\ge3$. given that $a\le{b}$. I was trying to solve the question by graph. Can anyone help me please?",,"['calculus', 'real-analysis', 'analysis', 'inequality']"
4,What's the spectrum of this operator in $\ell^2$?,What's the spectrum of this operator in ?,\ell^2,"Suppose that $\ell^2 = \biggl\{(x_n)_n \in \mathbb{K}^{\mathbb{N}_0} \biggm| \sum_{n=1}^{\infty}|{x_n}^2| < +\infty \biggr\}$ is a Hilbert-space with the inproduct $\langle\cdot,\cdot\rangle_2: \ell^2 \to \ell^2: (x,y) \mapsto \sum_{n=1}^\infty \overline{x_n}y_n$. Consider the operator $f: \ell^2 \to \ell^2: (x_0, x_1, \ldots) \mapsto (x_0, 0, x_1, 0, \ldots)$. I'm supposed to give the spectrum $\sigma(f) = \{\lambda \in \mathbb{K} \mid f-\lambda I \text{ not invertible}\}$, where $I$ is the identical function. I've already shown that 1 is the only eigenvalue of $f$, so 1 should be part of $\sigma(f)$, because $f-I$ isn't injective. I also think that $f$ isn't surjective, so 0 should be in the spectrum too. Unfortunately, I didn't find a way to calculate the whole spectrum, although I'm having the feeling that it shouldn't be that difficult. How can I do this?","Suppose that $\ell^2 = \biggl\{(x_n)_n \in \mathbb{K}^{\mathbb{N}_0} \biggm| \sum_{n=1}^{\infty}|{x_n}^2| < +\infty \biggr\}$ is a Hilbert-space with the inproduct $\langle\cdot,\cdot\rangle_2: \ell^2 \to \ell^2: (x,y) \mapsto \sum_{n=1}^\infty \overline{x_n}y_n$. Consider the operator $f: \ell^2 \to \ell^2: (x_0, x_1, \ldots) \mapsto (x_0, 0, x_1, 0, \ldots)$. I'm supposed to give the spectrum $\sigma(f) = \{\lambda \in \mathbb{K} \mid f-\lambda I \text{ not invertible}\}$, where $I$ is the identical function. I've already shown that 1 is the only eigenvalue of $f$, so 1 should be part of $\sigma(f)$, because $f-I$ isn't injective. I also think that $f$ isn't surjective, so 0 should be in the spectrum too. Unfortunately, I didn't find a way to calculate the whole spectrum, although I'm having the feeling that it shouldn't be that difficult. How can I do this?",,"['analysis', 'hilbert-spaces', 'spectral-theory']"
5,if $\frac{1}{1+x+f(y)}+\frac{1}{1+y+f(z)}+\frac{1}{1+z+f(x)}=1$ find the function $f(x)$,if  find the function,\frac{1}{1+x+f(y)}+\frac{1}{1+y+f(z)}+\frac{1}{1+z+f(x)}=1 f(x),"Find all functions $f(x):(0,\infty)\to(0,\infty) $satisfying $$\dfrac{1}{1+x+f(y)}+\dfrac{1}{1+y+f(z)}+\dfrac{1}{1+z+f(x)}=1$$ whenever $x,y,z$ are positive numbers and $xyz=1$ I know this if $$xyz=1\Longrightarrow \dfrac{1}{1+x+xy}+\dfrac{1}{1+y+yz}+\dfrac{1}{1+z+zx}=1$$ because \begin{align*}\dfrac{1}{1+x+xy}+\dfrac{1}{1+y+yz}+\dfrac{1}{1+z+zx}&=\dfrac{1}{1+x+xy}+\dfrac{x}{x+xy+xyz}+\dfrac{xy}{xy+xyz+x^2yz}\\ &=\dfrac{1+x+xy}{1+x+xy}\\ &=1 \end{align*} so I guess $$f(x)=\dfrac{1}{x}$$ But I can't prove it.Thank you","Find all functions $f(x):(0,\infty)\to(0,\infty) $satisfying $$\dfrac{1}{1+x+f(y)}+\dfrac{1}{1+y+f(z)}+\dfrac{1}{1+z+f(x)}=1$$ whenever $x,y,z$ are positive numbers and $xyz=1$ I know this if $$xyz=1\Longrightarrow \dfrac{1}{1+x+xy}+\dfrac{1}{1+y+yz}+\dfrac{1}{1+z+zx}=1$$ because \begin{align*}\dfrac{1}{1+x+xy}+\dfrac{1}{1+y+yz}+\dfrac{1}{1+z+zx}&=\dfrac{1}{1+x+xy}+\dfrac{x}{x+xy+xyz}+\dfrac{xy}{xy+xyz+x^2yz}\\ &=\dfrac{1+x+xy}{1+x+xy}\\ &=1 \end{align*} so I guess $$f(x)=\dfrac{1}{x}$$ But I can't prove it.Thank you",,['analysis']
6,Does there exist an $x$ such that $3^x = x^2$?,Does there exist an  such that ?,x 3^x = x^2,I tried solving for $x$ by using $x \log(3) = \log(x^2) $$\log(3) = \frac{\log(x^2)}{x}$$ I'm stuck on this part. how do I isolate $x$ by itself? Any help would be appreciated.,I tried solving for $x$ by using $x \log(3) = \log(x^2) $$\log(3) = \frac{\log(x^2)}{x}$$ I'm stuck on this part. how do I isolate $x$ by itself? Any help would be appreciated.,,['analysis']
7,Conservation of momentum for nonlinear Schrödinger equation,Conservation of momentum for nonlinear Schrödinger equation,,"I am having trouble proving the following momentum conservation law. Given smooth compactly supported solution $u(x,t)\in \mathbb{C}$ where $(x,t)\in \mathbb{R}^n\times \mathbb{R}$ of $iu_t+\Delta u= |u|^{p-1}u$, and $$ \vec{p}(t)= \text{Im} \int_{\mathbb{R}^n} \overline{u}\, \nabla{u}\, dx,$$ how do I show that $\partial_t \vec{p}(t)=0$, i.e. the above quantity is conserved ? For simplicity, take $p$ to be odd integer. I tried to make use of the equation and did integration by parts but it becomes a mess, and I think I must have missed something. Thank you.","I am having trouble proving the following momentum conservation law. Given smooth compactly supported solution $u(x,t)\in \mathbb{C}$ where $(x,t)\in \mathbb{R}^n\times \mathbb{R}$ of $iu_t+\Delta u= |u|^{p-1}u$, and $$ \vec{p}(t)= \text{Im} \int_{\mathbb{R}^n} \overline{u}\, \nabla{u}\, dx,$$ how do I show that $\partial_t \vec{p}(t)=0$, i.e. the above quantity is conserved ? For simplicity, take $p$ to be odd integer. I tried to make use of the equation and did integration by parts but it becomes a mess, and I think I must have missed something. Thank you.",,"['real-analysis', 'analysis', 'multivariable-calculus', 'partial-differential-equations']"
8,prove $| \exp(x) - 1 - x - \frac{x^2}{2!} - \frac{x^3}{3!}| < \frac{e}{24}$,prove,| \exp(x) - 1 - x - \frac{x^2}{2!} - \frac{x^3}{3!}| < \frac{e}{24},"prove $\displaystyle\left\lvert \exp(x) - 1 - x - \frac{x^2}{2!} - \frac{x^3}{3!}\right\rvert| < \frac{e}{24}$ $\forall x \in [-1,1]$ my attempt, we defined $\exp(x) = \displaystyle \sum_{n=0}^{\infty} \dfrac{x^n}{n!}$ using this: $| \exp(x) - 1 - x - \frac{x^2}{2!} - \frac{x^3}{3!}| = \left | \displaystyle \sum_{n=4}^\infty \dfrac{x^n}{n!} \right | \leq \displaystyle\sum_{n=4}^\infty \left |\dfrac{x^n}{n!} \right |  $ since $|x| \leq 1 $ and $e = 2.718...$ $\displaystyle\sum_{n=4}^\infty \left |\dfrac{x^n}{n!} \right | \leq \displaystyle\sum_{n=4}^\infty \left |\dfrac{e^n}{n!} \right | < e^4/24 < e/24$ Is this a valid proof? The reason I ask is because we have just started Taylor Series in my analysis class and I don't see how I'm exactly using this at all in the proof edit using maclaurin: $\exp(x) = 1 + x + x^2/2! + x^3/3! + e^\theta x^4/4!$ for some $\theta \in (0,1)$, since $|x| \leq 1$ $$\dfrac{e^\theta}{4!}|x^4| \leq e^\theta/24 < e/24$$ is this correct?","prove $\displaystyle\left\lvert \exp(x) - 1 - x - \frac{x^2}{2!} - \frac{x^3}{3!}\right\rvert| < \frac{e}{24}$ $\forall x \in [-1,1]$ my attempt, we defined $\exp(x) = \displaystyle \sum_{n=0}^{\infty} \dfrac{x^n}{n!}$ using this: $| \exp(x) - 1 - x - \frac{x^2}{2!} - \frac{x^3}{3!}| = \left | \displaystyle \sum_{n=4}^\infty \dfrac{x^n}{n!} \right | \leq \displaystyle\sum_{n=4}^\infty \left |\dfrac{x^n}{n!} \right |  $ since $|x| \leq 1 $ and $e = 2.718...$ $\displaystyle\sum_{n=4}^\infty \left |\dfrac{x^n}{n!} \right | \leq \displaystyle\sum_{n=4}^\infty \left |\dfrac{e^n}{n!} \right | < e^4/24 < e/24$ Is this a valid proof? The reason I ask is because we have just started Taylor Series in my analysis class and I don't see how I'm exactly using this at all in the proof edit using maclaurin: $\exp(x) = 1 + x + x^2/2! + x^3/3! + e^\theta x^4/4!$ for some $\theta \in (0,1)$, since $|x| \leq 1$ $$\dfrac{e^\theta}{4!}|x^4| \leq e^\theta/24 < e/24$$ is this correct?",,"['analysis', 'inequality', 'exponential-function']"
9,Give an example of a sequence of continuous functions which converges on a compact set to a function that has an infinite number of discontinuities.,Give an example of a sequence of continuous functions which converges on a compact set to a function that has an infinite number of discontinuities.,,"Give an example of a sequence of continuous functions which converges on a compact set to a function that has an infinite number of discontinuities. Analysis is something that is very difficult for me, and I am not fully sure what I am supposed to give an example of, but this is my attempt (hopefully some of it is right): Let C be the Cantor Set (since it is compact and has an infinite number of discontinuities). Let C1, C2,... be subsets of C and the union of all Cn's = C. Let F = (fn) where for all n, fn: [0,1] -> Cn Let fn = |x| Then the midpoints of all Cn will converge to 0 and the endpoints of each Cn will converge to 1. Any help or corrections is greatly appreciated!","Give an example of a sequence of continuous functions which converges on a compact set to a function that has an infinite number of discontinuities. Analysis is something that is very difficult for me, and I am not fully sure what I am supposed to give an example of, but this is my attempt (hopefully some of it is right): Let C be the Cantor Set (since it is compact and has an infinite number of discontinuities). Let C1, C2,... be subsets of C and the union of all Cn's = C. Let F = (fn) where for all n, fn: [0,1] -> Cn Let fn = |x| Then the midpoints of all Cn will converge to 0 and the endpoints of each Cn will converge to 1. Any help or corrections is greatly appreciated!",,['analysis']
10,Prove a lower semi-continuous and coercive function attains its infimum and is bounded below.,Prove a lower semi-continuous and coercive function attains its infimum and is bounded below.,,"A function $f:\mathbb{R}\rightarrow\mathbb{R}$ is $coercive$ if $$ \lim_{||x||\rightarrow\infty} f(x) = \infty.$$ Explicity, this condition means that for any $M>0$ there is an $R>0$ such that $||x||>R$ implies $f(x)\geq M$. Prove that if $f:\mathbb{R}^n\rightarrow \mathbb{R}$ is lower semi-continuous and coercive then $f$ is bounded from below and attains its infimum. The definition I am using for LSC is: A function $f$ is lower semi-continuous on $X$ if for all $x\in X$ and every sequence $x_n\rightarrow x$, we have $$\lim_{n\rightarrow\infty} \inf f(x_n) \geq f(x). $$ This is my solution: Let $M>0$. Then since $f$ is $coercive$ there is an $R>0$ such that $||x||>R$ implies that $f(x)\geq M$. Let $x_n$ be a sequence such that $x_n\rightarrow x$. Since $f$ is LSC we have that $$ f(x)\leq \lim_{n\rightarrow\infty} \inf f(x_n)$$ which implies $$ M\leq f(x) \leq \lim_{n\rightarrow\infty}\inf f(x_n). $$ Hence $f$ is bounded below and attains its infimum.","A function $f:\mathbb{R}\rightarrow\mathbb{R}$ is $coercive$ if $$ \lim_{||x||\rightarrow\infty} f(x) = \infty.$$ Explicity, this condition means that for any $M>0$ there is an $R>0$ such that $||x||>R$ implies $f(x)\geq M$. Prove that if $f:\mathbb{R}^n\rightarrow \mathbb{R}$ is lower semi-continuous and coercive then $f$ is bounded from below and attains its infimum. The definition I am using for LSC is: A function $f$ is lower semi-continuous on $X$ if for all $x\in X$ and every sequence $x_n\rightarrow x$, we have $$\lim_{n\rightarrow\infty} \inf f(x_n) \geq f(x). $$ This is my solution: Let $M>0$. Then since $f$ is $coercive$ there is an $R>0$ such that $||x||>R$ implies that $f(x)\geq M$. Let $x_n$ be a sequence such that $x_n\rightarrow x$. Since $f$ is LSC we have that $$ f(x)\leq \lim_{n\rightarrow\infty} \inf f(x_n)$$ which implies $$ M\leq f(x) \leq \lim_{n\rightarrow\infty}\inf f(x_n). $$ Hence $f$ is bounded below and attains its infimum.",,"['analysis', 'continuity']"
11,Show that the equation $\log(1+e^x)=\cos(x)$ has infinitely many negative solutions.,Show that the equation  has infinitely many negative solutions.,\log(1+e^x)=\cos(x),Show that the equation $\log(1+e^x)=\cos(x)$ has infinitely many negative solutions. Find out if there is a positive solution and if it is unique. From the graph I can see that it has an infinitely many negative solutions and that it has one positive solution. I don't know how to prove it rigorously. Could you help me? Thank you for your time.,Show that the equation has infinitely many negative solutions. Find out if there is a positive solution and if it is unique. From the graph I can see that it has an infinitely many negative solutions and that it has one positive solution. I don't know how to prove it rigorously. Could you help me? Thank you for your time.,\log(1+e^x)=\cos(x),['analysis']
12,"Limit, Limit Sup, Limit Inf","Limit, Limit Sup, Limit Inf",,"For the sequence $$S_n=((-1)^j\times j); \forall j \in \mathbb{N}$$ I am having hard time understanding what Limit of the sequence does not exist but $$\limsup_{n\rightarrow\infty}(S_n)$$ exist and =$\infty$  and $$\liminf_{n\rightarrow\infty}(S_n)$$ exists and =$-\infty$ mean. Added : My only concern here is in case of limit, whenever limit is equal to infinity, we say limit does not exist. But Why does Limit sup =infinity mean limit exists? Why is there such difference?","For the sequence $$S_n=((-1)^j\times j); \forall j \in \mathbb{N}$$ I am having hard time understanding what Limit of the sequence does not exist but $$\limsup_{n\rightarrow\infty}(S_n)$$ exist and =$\infty$  and $$\liminf_{n\rightarrow\infty}(S_n)$$ exists and =$-\infty$ mean. Added : My only concern here is in case of limit, whenever limit is equal to infinity, we say limit does not exist. But Why does Limit sup =infinity mean limit exists? Why is there such difference?",,"['real-analysis', 'analysis']"
13,Convolution of an integrable function of compact support with a bump function.,Convolution of an integrable function of compact support with a bump function.,,Let $f\in L^1(\mathbb{R})$ be of compact support and $\psi(x)=C \exp(-(1-x^2)^{-1})$ where $C$ is chosen so that $\int_{\mathbb{R}} \psi =1$. Show that the convolution $f*\psi(x)=\int_{\mathbb{R}} f(x-y)\psi(y) dy$ is infinitely differentiable.,Let $f\in L^1(\mathbb{R})$ be of compact support and $\psi(x)=C \exp(-(1-x^2)^{-1})$ where $C$ is chosen so that $\int_{\mathbb{R}} \psi =1$. Show that the convolution $f*\psi(x)=\int_{\mathbb{R}} f(x-y)\psi(y) dy$ is infinitely differentiable.,,"['analysis', 'measure-theory', 'convolution']"
14,Existence and uniqueness of solutions to a system of non-linear equations,Existence and uniqueness of solutions to a system of non-linear equations,,"Consider an arbitrary system of non-linear equations $F(x)=0$ where $F:\mathbb{R}^n \rightarrow \mathbb{R}^m$. Are there any properties to check in order to study whether solutions exist, are unique and, more generally, what theories could one use in order to characterize sets of solutions in the case of non-uniqueness? Any references or hints are appreciated. As a note, I am aware of the implicit function theorem and the use of Jacobians to study solutions locally.","Consider an arbitrary system of non-linear equations $F(x)=0$ where $F:\mathbb{R}^n \rightarrow \mathbb{R}^m$. Are there any properties to check in order to study whether solutions exist, are unique and, more generally, what theories could one use in order to characterize sets of solutions in the case of non-uniqueness? Any references or hints are appreciated. As a note, I am aware of the implicit function theorem and the use of Jacobians to study solutions locally.",,"['analysis', 'reference-request']"
15,Dominated Convergence Theorem for Sets,Dominated Convergence Theorem for Sets,,"This was an interesting question, which gives the analog of the better known Dominated Convergence Theorem for Lebesgue integrable functions. Suppose $E_{n}\to E$ pointwise (e.g. the indicator functions $\chi_{E_{n}}\to\chi_{E}$ pointwise) and that $E_{n}\subset F$ for all $n$ where $m(F)<\infty$.  (Note, all sets in this exercise are assumed to be Lebesgue measurable on $\mathbb{R}^{d}$). Then show that $E$ is Lebesgue measurable, that $\lim_{n\to\infty}m(E_{n})=m(\lim_{n\to\infty}E_{n})=m(E)$, and finally that the hypothesis $m(F)<\infty$ cannot be dropped, even when $m(E_{n})<\alpha$ for some uniform bound. The first part is easy, since one just uses the definition of $\lim\sup$ or $\lim\inf$ to express $E$ as a countable intersection/union of Lebesgue measurable sets.  The third part is also clear, since one can just translate well-known examples for Riemann integrals into their set analogs.  For example, take the $E_{n}$ to be the regions in $\mathbb{R}^{2}$ bounded by $n\cdot\chi_{(0,\frac{1}{n})}$ and the x axis; then one has $\lim_{n\to\infty}m(E_{n})=1$, yet $m(\lim_{n\to\infty}E_{n})=0$.  The obvious problem is that the sets $E_{n}$ escape to ""vertical infinity"" near the origin; the dominated convergence shuts down this possibility.  But the example is helpful, since while the process of commuting limits (even in the Lebesgue theory; although at least in the Lebesgue theory, measurable sets and functions are closed under limits, unlike the Riemann/Jordan theory) is not generally permissible, the intuition that they should commute remains valid under the general blanket hypotheses of the convergence theorems, which is where you intuition comes from anyway. Anyway, here's my attempt to the solution of part (2). I have a feeling that it should be simpler than this massive computation; but even if it's not, I want to verify that mine is correct, since I haven't had much experience working with $\lim\sup$ and $\lim\inf$ operations of sets/sequences. Since $E_{n}\to E$, we have the equality of $\lim\sup$ and $\lim\inf$ of the sequence, and so $$E=\bigcup_{j=1}^{\infty}\bigcap_{k=j}^{\infty}=\bigcap_{j=1}^{\infty}\bigcup_{k=j}^{\infty}E_{k}.$$ Thus, $E$ is the countable intersection and union of Lebesgue measurable sets, and so is itself a Lebesgue measurable set. The following computation \begin{align*} m(E) &=m\left(\lim_{n\to\infty}E_{n}\right)\\ &=m\left(\lim\inf\limits_{n\to\infty}E_{n}\right)\\ &=m\left(\bigcup_{n=1}^{\infty}\bigcap_{k=n}^{\infty}E_{k}\right)\\ &=m\left(\lim_{n\to\infty}\bigcap_{k=n}^{\infty}E_{k}\right)\\ &=\lim_{n\to\infty}m\left(\bigcap_{k=n}^{\infty}E_{k}\right)\\ &=\lim_{n\to\infty}m\left(\inf_{k\geq n}E_{k}\right)\\ &\leq\lim_{n\to\infty}\inf_{k\geq n}m(E_{k})\\ &=\lim\inf\limits_{n\to\infty}m(E_{n})\\ &\leq\lim\sup\limits_{n\to\infty}m(E_{n})\\ &=\lim_{n\to\infty}\sup_{k\geq n}m(E_{k})\\ &\leq\lim_{n\to\infty}m\left(\sup_{k\geq n}E_{k}\right)\\ &=\lim_{n\to\infty}m\left(\bigcup_{k=n}^{\infty}E_{k}\right)\\ &=m\left(\lim_{n\to\infty}\bigcup_{k=n}^{\infty}E{k}\right)\\ &=m\left(\bigcap_{n=1}^{\infty}\bigcup_{k=n}^{\infty}E_{k}\right)\\ &=m\left(\lim\sup\limits_{n\to\infty}E_{n}\right)\\ &=m\left(\lim_{n\to\infty}E_{n}\right)\\ &=m(E) \end{align*} The computations are justified by the following observations.  First, $E_{n}\to E$ implies that $\lim\sup_{n\to\infty}E_{n}=\lim\inf_{n\to\infty}E_{n}=\lim_{n\to\infty}E_{n}=E$.  By definition $\lim\inf_{n\to\infty}=\bigcup_{n=1}^{\infty}\bigcap_{k\geq n}^{\infty}E_{k}=\lim_{n\to\infty}\bigcap_{k=n}^{\infty}E_{k}$, and if we define $G_{n}=\bigcap_{k\geq n}^{\infty}E_{k}$, we see that $G_{n}\subset G_{n+1}$ for all $n$.  Hence, the $G_{n}$ form an upward monotonic sequence of sets and so we may apply upward monotonic convergence theorem to establish the commutation of the first limit.  It is helpful to use the technically incorrect notation $G_{n}=\inf_{k\geq n}E_{k}$ (where the $\inf$ is understood to be with respect to set inclusion).  Then it becomes clear that $G_{n}\subset E_{k}$ for all $n\leq k$, so from monotonicity we have $m(G_{n})\leq m(E_{k})$ for all $k\geq n$, and so the above inequality holds.  The remaining statements follow from the definition of $\lim\inf_{n\to\infty}$ for a sequence of real numbers.  Justifications of the $\lim\sup$ computations are essentially the same as for the $\lim\inf$ case (though the computations are backwards since I wanted to combine everything into one inequality); except that we apply the downward monotone convergence theorem to the sequence defined by $G_{n}=\bigcup_{k\geq n}^{\infty}E_{k}$ to establish the commutation of the final limit.  This is valid since $G_{n}\supset G_{n+1}$ for all $n$, and since $E_{k}\subset F$ and $m(E_{k})<m(F)<\infty$ hold for all $k$, we see that each $G_{k}$ is bounded, and so any union of the $G_{k}$ is bounded with finite measure.  The inequality is of course obtained from monotonicity and noting that $G_{n}\supset E_{k}$ for all $k\geq n$.  Putting this together then gives us the inequality $$\lim\inf_{n\to\infty}m(E_{n})\leq m\left(\lim_{n\to\infty}E_{n}\right)\leq\lim\sup\limits_{n\to\infty}m(E_{n}).$$ But since $E_{n}$ converges, it is clear that we have an equality, and so $$\lim\inf_{n\to\infty}m(E_{n})=\lim\sup\limits_{n\to\infty}m(E_{n})=\lim_{n\to\infty}m(E_{n})=m\left(\lim_{n\to\infty}E_{n}\right)=m(E)$$ as required.","This was an interesting question, which gives the analog of the better known Dominated Convergence Theorem for Lebesgue integrable functions. Suppose $E_{n}\to E$ pointwise (e.g. the indicator functions $\chi_{E_{n}}\to\chi_{E}$ pointwise) and that $E_{n}\subset F$ for all $n$ where $m(F)<\infty$.  (Note, all sets in this exercise are assumed to be Lebesgue measurable on $\mathbb{R}^{d}$). Then show that $E$ is Lebesgue measurable, that $\lim_{n\to\infty}m(E_{n})=m(\lim_{n\to\infty}E_{n})=m(E)$, and finally that the hypothesis $m(F)<\infty$ cannot be dropped, even when $m(E_{n})<\alpha$ for some uniform bound. The first part is easy, since one just uses the definition of $\lim\sup$ or $\lim\inf$ to express $E$ as a countable intersection/union of Lebesgue measurable sets.  The third part is also clear, since one can just translate well-known examples for Riemann integrals into their set analogs.  For example, take the $E_{n}$ to be the regions in $\mathbb{R}^{2}$ bounded by $n\cdot\chi_{(0,\frac{1}{n})}$ and the x axis; then one has $\lim_{n\to\infty}m(E_{n})=1$, yet $m(\lim_{n\to\infty}E_{n})=0$.  The obvious problem is that the sets $E_{n}$ escape to ""vertical infinity"" near the origin; the dominated convergence shuts down this possibility.  But the example is helpful, since while the process of commuting limits (even in the Lebesgue theory; although at least in the Lebesgue theory, measurable sets and functions are closed under limits, unlike the Riemann/Jordan theory) is not generally permissible, the intuition that they should commute remains valid under the general blanket hypotheses of the convergence theorems, which is where you intuition comes from anyway. Anyway, here's my attempt to the solution of part (2). I have a feeling that it should be simpler than this massive computation; but even if it's not, I want to verify that mine is correct, since I haven't had much experience working with $\lim\sup$ and $\lim\inf$ operations of sets/sequences. Since $E_{n}\to E$, we have the equality of $\lim\sup$ and $\lim\inf$ of the sequence, and so $$E=\bigcup_{j=1}^{\infty}\bigcap_{k=j}^{\infty}=\bigcap_{j=1}^{\infty}\bigcup_{k=j}^{\infty}E_{k}.$$ Thus, $E$ is the countable intersection and union of Lebesgue measurable sets, and so is itself a Lebesgue measurable set. The following computation \begin{align*} m(E) &=m\left(\lim_{n\to\infty}E_{n}\right)\\ &=m\left(\lim\inf\limits_{n\to\infty}E_{n}\right)\\ &=m\left(\bigcup_{n=1}^{\infty}\bigcap_{k=n}^{\infty}E_{k}\right)\\ &=m\left(\lim_{n\to\infty}\bigcap_{k=n}^{\infty}E_{k}\right)\\ &=\lim_{n\to\infty}m\left(\bigcap_{k=n}^{\infty}E_{k}\right)\\ &=\lim_{n\to\infty}m\left(\inf_{k\geq n}E_{k}\right)\\ &\leq\lim_{n\to\infty}\inf_{k\geq n}m(E_{k})\\ &=\lim\inf\limits_{n\to\infty}m(E_{n})\\ &\leq\lim\sup\limits_{n\to\infty}m(E_{n})\\ &=\lim_{n\to\infty}\sup_{k\geq n}m(E_{k})\\ &\leq\lim_{n\to\infty}m\left(\sup_{k\geq n}E_{k}\right)\\ &=\lim_{n\to\infty}m\left(\bigcup_{k=n}^{\infty}E_{k}\right)\\ &=m\left(\lim_{n\to\infty}\bigcup_{k=n}^{\infty}E{k}\right)\\ &=m\left(\bigcap_{n=1}^{\infty}\bigcup_{k=n}^{\infty}E_{k}\right)\\ &=m\left(\lim\sup\limits_{n\to\infty}E_{n}\right)\\ &=m\left(\lim_{n\to\infty}E_{n}\right)\\ &=m(E) \end{align*} The computations are justified by the following observations.  First, $E_{n}\to E$ implies that $\lim\sup_{n\to\infty}E_{n}=\lim\inf_{n\to\infty}E_{n}=\lim_{n\to\infty}E_{n}=E$.  By definition $\lim\inf_{n\to\infty}=\bigcup_{n=1}^{\infty}\bigcap_{k\geq n}^{\infty}E_{k}=\lim_{n\to\infty}\bigcap_{k=n}^{\infty}E_{k}$, and if we define $G_{n}=\bigcap_{k\geq n}^{\infty}E_{k}$, we see that $G_{n}\subset G_{n+1}$ for all $n$.  Hence, the $G_{n}$ form an upward monotonic sequence of sets and so we may apply upward monotonic convergence theorem to establish the commutation of the first limit.  It is helpful to use the technically incorrect notation $G_{n}=\inf_{k\geq n}E_{k}$ (where the $\inf$ is understood to be with respect to set inclusion).  Then it becomes clear that $G_{n}\subset E_{k}$ for all $n\leq k$, so from monotonicity we have $m(G_{n})\leq m(E_{k})$ for all $k\geq n$, and so the above inequality holds.  The remaining statements follow from the definition of $\lim\inf_{n\to\infty}$ for a sequence of real numbers.  Justifications of the $\lim\sup$ computations are essentially the same as for the $\lim\inf$ case (though the computations are backwards since I wanted to combine everything into one inequality); except that we apply the downward monotone convergence theorem to the sequence defined by $G_{n}=\bigcup_{k\geq n}^{\infty}E_{k}$ to establish the commutation of the final limit.  This is valid since $G_{n}\supset G_{n+1}$ for all $n$, and since $E_{k}\subset F$ and $m(E_{k})<m(F)<\infty$ hold for all $k$, we see that each $G_{k}$ is bounded, and so any union of the $G_{k}$ is bounded with finite measure.  The inequality is of course obtained from monotonicity and noting that $G_{n}\supset E_{k}$ for all $k\geq n$.  Putting this together then gives us the inequality $$\lim\inf_{n\to\infty}m(E_{n})\leq m\left(\lim_{n\to\infty}E_{n}\right)\leq\lim\sup\limits_{n\to\infty}m(E_{n}).$$ But since $E_{n}$ converges, it is clear that we have an equality, and so $$\lim\inf_{n\to\infty}m(E_{n})=\lim\sup\limits_{n\to\infty}m(E_{n})=\lim_{n\to\infty}m(E_{n})=m\left(\lim_{n\to\infty}E_{n}\right)=m(E)$$ as required.",,"['analysis', 'measure-theory']"
16,Proving that a metric space is compact,Proving that a metric space is compact,,"Let $H^\infty$ be the set of real sequences such that each element in each sequence has $|a_n|\leq 1$. The metric is defined as $$d(\{a_n\}, \{b_n\}) = \sum_{n=1}^\infty \frac{|a_n - b_n|}{2^n}.$$ Prove that $H^\infty$ is a compact metric space. To prove this, I want to show that every sequence in $H^\infty$ has a convergent subsequence. I know that if we have a sequence $\{\{a_n\}^{(k)}\}$ in $H^\infty$, then for all $i$, the real sequence $\{a_i^{(k)}\}$ has a convergent subsequence, since it is bounded by 1. So we can get a convergent subsequence $\{a_1^{(k_j)}\}$, and then a convergent subsequence of $\{a_2^{(k_j)}\}$, and continue taking subsequences of subsequences until we have a convergent subsequence of $(a_1, a_2, a_3, ..., a_n)^{(k)}$ with $n$ some positive integer if we stop taking subsequences at the $nth$ subsequence; this gives a sequence $\{x_n\}$ where $x_n$ is the limit of the $n$th convergent subsequence of $\{a_n^{(k)}\}$. Ideally, we could show that the sequence in $H^\infty$ converges to $\{x_n\}$. I know that if we have the $nth$ subsequence of $\{\{a_i\}^{(k)}\}$ defined in the way described above, then for any $\epsilon > 0$ there exist $N_1, ..., N_n$ such that if $k\geq \max_{i\leq n}\{N_i\}$, then for $1\leq i\leq n$, $|a_i^{(k)} - x_i| < \epsilon/2n$. By choosing $n$ sufficiently large that $\sum_{i=n+1}^\infty |a_i^{(k)} - x_i|/2^i\leq \sum_{i=n+1}^\infty 1/2^{i-1} < \epsilon/2$, we can ensure that $d(\{a_n\}^{(k)}, \{x_n\}) < \epsilon$. But the problem here is that for each $\epsilon$, we end up choosing a different convergent subsequence of the first $n$ terms (since we need to choose $n$, which determines how many subsequences of subsequences we take). Any idea on how to proceed?","Let $H^\infty$ be the set of real sequences such that each element in each sequence has $|a_n|\leq 1$. The metric is defined as $$d(\{a_n\}, \{b_n\}) = \sum_{n=1}^\infty \frac{|a_n - b_n|}{2^n}.$$ Prove that $H^\infty$ is a compact metric space. To prove this, I want to show that every sequence in $H^\infty$ has a convergent subsequence. I know that if we have a sequence $\{\{a_n\}^{(k)}\}$ in $H^\infty$, then for all $i$, the real sequence $\{a_i^{(k)}\}$ has a convergent subsequence, since it is bounded by 1. So we can get a convergent subsequence $\{a_1^{(k_j)}\}$, and then a convergent subsequence of $\{a_2^{(k_j)}\}$, and continue taking subsequences of subsequences until we have a convergent subsequence of $(a_1, a_2, a_3, ..., a_n)^{(k)}$ with $n$ some positive integer if we stop taking subsequences at the $nth$ subsequence; this gives a sequence $\{x_n\}$ where $x_n$ is the limit of the $n$th convergent subsequence of $\{a_n^{(k)}\}$. Ideally, we could show that the sequence in $H^\infty$ converges to $\{x_n\}$. I know that if we have the $nth$ subsequence of $\{\{a_i\}^{(k)}\}$ defined in the way described above, then for any $\epsilon > 0$ there exist $N_1, ..., N_n$ such that if $k\geq \max_{i\leq n}\{N_i\}$, then for $1\leq i\leq n$, $|a_i^{(k)} - x_i| < \epsilon/2n$. By choosing $n$ sufficiently large that $\sum_{i=n+1}^\infty |a_i^{(k)} - x_i|/2^i\leq \sum_{i=n+1}^\infty 1/2^{i-1} < \epsilon/2$, we can ensure that $d(\{a_n\}^{(k)}, \{x_n\}) < \epsilon$. But the problem here is that for each $\epsilon$, we end up choosing a different convergent subsequence of the first $n$ terms (since we need to choose $n$, which determines how many subsequences of subsequences we take). Any idea on how to proceed?",,"['real-analysis', 'analysis', 'metric-spaces', 'proof-writing', 'compactness']"
17,Example of nonlinear regular function with constant nonzero Jacobian,Example of nonlinear regular function with constant nonzero Jacobian,,"Can anyone give a nonlinear regular function from C^2 to C^2 with a constant nonzero Jacobian? It seems to me that the only such functions are linear. According to the Jacobian conjecture, a function from C^2 to C^2 with a constant nonzero Jacobian must have an inverse.","Can anyone give a nonlinear regular function from C^2 to C^2 with a constant nonzero Jacobian? It seems to me that the only such functions are linear. According to the Jacobian conjecture, a function from C^2 to C^2 with a constant nonzero Jacobian must have an inverse.",,['analysis']
18,Find every bijection of non-zero real numbers whose inverse is its reciprocal,Find every bijection of non-zero real numbers whose inverse is its reciprocal,,"Find all functions $f $ : $\mathbb R^{×}$ $\to$ $\mathbb R^{×}$ that are one-to-one and onto and such that $f^{−1}(x)= 1/ f(x)$ , Where $\mathbb R^{×}=\mathbb R-$ {0} My approach:= At first I have consider $f(x)=x$ and $f(x)=1/x$ then this does not satisfy $f^{−1}(x)= 1/ f(x)$ Now I am unable to construct such functions.","Find all functions : that are one-to-one and onto and such that , Where {0} My approach:= At first I have consider and then this does not satisfy Now I am unable to construct such functions.",f  \mathbb R^{×} \to \mathbb R^{×} f^{−1}(x)= 1/ f(x) \mathbb R^{×}=\mathbb R- f(x)=x f(x)=1/x f^{−1}(x)= 1/ f(x),['analysis']
19,Find a completion of the following spaces,Find a completion of the following spaces,,"Find a completion of the incomplete metric space $(X,d)$ , where a) $X=\Bbb{Q}$ , where $\forall{q,r}\in{X}$ $d(q,r) = \lvert {\arctan(q) - \arctan(r)}\rvert$ b) $X=(0,1)\setminus \mathbb{Q}$ , where $\forall{x,y}\in{X}$ $d(x,y)=|x-1|+|y-1|$ for $x\neq y$ , $d(x,x)=0$ For part a), I suspect that the metric $d$ is equivalent to the usual Euclidean metric $\lvert {q-r}\rvert$ . If this is correct, then the completion of $X$ would be $\Bbb{R}$ I guess. To show the equivalence, I thought that we can use the sequential characterization of continuity along with the fact that the function $arctan$ is continuous, but I am not sure about this as the domain of $arctan$ is $\Bbb{Q}$ , any idea is highly appreciated. For part b), If $X$ was $(0,1)$ with the same metric, I would argue that any convergent sequence in $X$ converges to $1$ , so the completion might be $(0,1]$ . But $X=(0,1)\setminus \mathbb{Q}$ , so should I still argue that the completion is $(0,1]$ ? My mind is very confused about this part.","Find a completion of the incomplete metric space , where a) , where b) , where for , For part a), I suspect that the metric is equivalent to the usual Euclidean metric . If this is correct, then the completion of would be I guess. To show the equivalence, I thought that we can use the sequential characterization of continuity along with the fact that the function is continuous, but I am not sure about this as the domain of is , any idea is highly appreciated. For part b), If was with the same metric, I would argue that any convergent sequence in converges to , so the completion might be . But , so should I still argue that the completion is ? My mind is very confused about this part.","(X,d) X=\Bbb{Q} \forall{q,r}\in{X} d(q,r) = \lvert {\arctan(q) - \arctan(r)}\rvert X=(0,1)\setminus \mathbb{Q} \forall{x,y}\in{X} d(x,y)=|x-1|+|y-1| x\neq y d(x,x)=0 d \lvert {q-r}\rvert X \Bbb{R} arctan arctan \Bbb{Q} X (0,1) X 1 (0,1] X=(0,1)\setminus \mathbb{Q} (0,1]","['real-analysis', 'analysis', 'metric-spaces', 'complete-spaces']"
20,Compute the following integral: $\lim_{n\rightarrow\infty}\int_0^{\infty}(1+\frac{x}{n})^{-n}\sin(\frac{x}{n})dx$,Compute the following integral:,\lim_{n\rightarrow\infty}\int_0^{\infty}(1+\frac{x}{n})^{-n}\sin(\frac{x}{n})dx,"Question: Compute: $\lim_{n\rightarrow\infty}\int_0^{\infty}(1+\frac{x}{n})^{-n}\sin(\frac{x}{n})dx$ . This is from Folland's Real Analysis book. If we can find an integrable majorant, then the integral will equal $0$ , since $\lim_{n\rightarrow\infty}\sin(x/n)=0$ .  We can use the Binominal Theorem to show $(1+\frac{x}{n})^n\geq 1+x+(\frac{1}{2}-\frac{1}{2n})x^2$ , by noticing that all the terms will be positive since $x$ only takes nonnegative values, so we just ""cut it off"" after those couple of terms.  Next, since $|\sin x|\leq 1$ , we can say $|\frac{\sin(\frac{x}{n})}{(1+\frac{x}{n})^n}|\leq \frac{1}{1+x+(\frac{1}{2}-\frac{1}{2n})x^2}$ , and $\int_0^\infty\frac{1}{1+x+(\frac{1}{2}-\frac{1}{2n})x^2}<\infty$ , so we have our integrable majorant, and by Dominated Convergence Theorem we can pass the limit inside the integral giving us a value of $0$ . I am curious if there are any (unseen by me) holes in this argument, or, maybe, are there any other ways of using DCT to solve this?  Thank you! EDIT: Just a quick note, I wanted to use Bernouli's inequality to get $\frac{1}{(1+\frac{x}{n})^n}\leq\frac{1}{1+n\frac{x}{n}}\leq\frac{1}{1+x}$ ... but $\int_0^\infty\frac{1}{1+x}dx$ doesn't converge.","Question: Compute: . This is from Folland's Real Analysis book. If we can find an integrable majorant, then the integral will equal , since .  We can use the Binominal Theorem to show , by noticing that all the terms will be positive since only takes nonnegative values, so we just ""cut it off"" after those couple of terms.  Next, since , we can say , and , so we have our integrable majorant, and by Dominated Convergence Theorem we can pass the limit inside the integral giving us a value of . I am curious if there are any (unseen by me) holes in this argument, or, maybe, are there any other ways of using DCT to solve this?  Thank you! EDIT: Just a quick note, I wanted to use Bernouli's inequality to get ... but doesn't converge.",\lim_{n\rightarrow\infty}\int_0^{\infty}(1+\frac{x}{n})^{-n}\sin(\frac{x}{n})dx 0 \lim_{n\rightarrow\infty}\sin(x/n)=0 (1+\frac{x}{n})^n\geq 1+x+(\frac{1}{2}-\frac{1}{2n})x^2 x |\sin x|\leq 1 |\frac{\sin(\frac{x}{n})}{(1+\frac{x}{n})^n}|\leq \frac{1}{1+x+(\frac{1}{2}-\frac{1}{2n})x^2} \int_0^\infty\frac{1}{1+x+(\frac{1}{2}-\frac{1}{2n})x^2}<\infty 0 \frac{1}{(1+\frac{x}{n})^n}\leq\frac{1}{1+n\frac{x}{n}}\leq\frac{1}{1+x} \int_0^\infty\frac{1}{1+x}dx,"['real-analysis', 'analysis', 'convergence-divergence', 'solution-verification', 'lebesgue-integral']"
21,"If $\phi$ is in $\mathcal S$ and $\phi(0)=0$, is $\int_0^1 \nabla \phi(tx)dt$ in $\mathcal S$ too?","If  is in  and , is  in  too?",\phi \mathcal S \phi(0)=0 \int_0^1 \nabla \phi(tx)dt \mathcal S,"Let $\phi\in\mathcal S(\mathbb R^n)$ be a Schwartz function vanishing at $0$ . Then the FTC implies $$ \phi(x) = x\cdot \int_0^1 \nabla \phi (tx)\ dt.$$ Let this integral term be $\psi(x) := \int_0^1 \nabla \phi(tx)dt$ . Is it true that $\psi$ is (component-wise) a Schwartz function? It is at least $C^\infty$ . I tried for a while to come up with counterexamples and failed. I have proved it in dimension $1$ , but I need a replacement for division in higher dimensions? The result is false for arbitrary functions $p$ that solve $\phi(x)= x\cdot p(x)$ , even if $\phi$ is Schwartz ( $0 = \binom x y \cdot \binom {-y}x$ ). Also, $\psi$ is not Schwartz if the condition $\phi(0)=0$ is dropped, as then (in 1D) $\psi(x)= \frac{\phi(x)-\phi(0)}x\sim\frac{\phi(0)}x$ for large $x$ which is not enough decay. So a positive proof would need to use something about Schwartz functions and something about vanishing at zero (the two assumptions). I can provide a detailed attempt if wanted; in fact I have deleted many ""answers"" after noticing errors. In particular I have tried merely using the Schwartz seminorms to bound the integral, but it seems that either many factors of $t^{-1}$ appear which make the integral diverge, or (trying to treat $t\ll 1$ separately) I am unable to achieve large decay. Of course, as previously mentioned, the issue at $t=0$ should be somehow resolved with the vanishing at $x=0$ . This comes out of trying to understand what the correct fix for a typo in M Zworski's Semiclassical Analysis p.32 ( Screenshot , Google books link ), in a proof that the Fourier Transform is an automorphism of $\mathcal S$ . The online errata ( screenshot copy) says to set $\phi(0)=0$ , but I'm unsure if the preceeding paragraph is correct with this correction (and in any case, it is stated that $\psi$ may not be in $\mathcal S$ without proof.) Thoughts appreciated! Zworski, Maciej , Semiclassical analysis, Graduate Studies in Mathematics 138. Providence, RI: American Mathematical Society (AMS) (ISBN 978-0-8218-8320-4/hbk). xii, 431 p. (2012). ZBL1252.58001 .","Let be a Schwartz function vanishing at . Then the FTC implies Let this integral term be . Is it true that is (component-wise) a Schwartz function? It is at least . I tried for a while to come up with counterexamples and failed. I have proved it in dimension , but I need a replacement for division in higher dimensions? The result is false for arbitrary functions that solve , even if is Schwartz ( ). Also, is not Schwartz if the condition is dropped, as then (in 1D) for large which is not enough decay. So a positive proof would need to use something about Schwartz functions and something about vanishing at zero (the two assumptions). I can provide a detailed attempt if wanted; in fact I have deleted many ""answers"" after noticing errors. In particular I have tried merely using the Schwartz seminorms to bound the integral, but it seems that either many factors of appear which make the integral diverge, or (trying to treat separately) I am unable to achieve large decay. Of course, as previously mentioned, the issue at should be somehow resolved with the vanishing at . This comes out of trying to understand what the correct fix for a typo in M Zworski's Semiclassical Analysis p.32 ( Screenshot , Google books link ), in a proof that the Fourier Transform is an automorphism of . The online errata ( screenshot copy) says to set , but I'm unsure if the preceeding paragraph is correct with this correction (and in any case, it is stated that may not be in without proof.) Thoughts appreciated! Zworski, Maciej , Semiclassical analysis, Graduate Studies in Mathematics 138. Providence, RI: American Mathematical Society (AMS) (ISBN 978-0-8218-8320-4/hbk). xii, 431 p. (2012). ZBL1252.58001 .",\phi\in\mathcal S(\mathbb R^n) 0  \phi(x) = x\cdot \int_0^1 \nabla \phi (tx)\ dt. \psi(x) := \int_0^1 \nabla \phi(tx)dt \psi C^\infty 1 p \phi(x)= x\cdot p(x) \phi 0 = \binom x y \cdot \binom {-y}x \psi \phi(0)=0 \psi(x)= \frac{\phi(x)-\phi(0)}x\sim\frac{\phi(0)}x x t^{-1} t\ll 1 t=0 x=0 \mathcal S \phi(0)=0 \psi \mathcal S,"['real-analysis', 'analysis', 'fourier-analysis', 'distribution-theory', 'schwartz-space']"
22,Proving that a function of the form f(x+y)=f(x)*f(y) is continuous,Proving that a function of the form f(x+y)=f(x)*f(y) is continuous,,"I have the to solve the following problem: Let $f$ be a function from the real numbers to the real numbers. The function satisfies $f(x+y) = f(x)f(y)$ for all real $x,y$ . Prove that if $f$ is continuous in $0$ then $f$ is continuous in every point. I think I have a solution but I would like to know if it's correct: By letting $y=0$ we get that $f(x)=f(x)f(0)$ meaning that $f(0)=1$ . Thus we know that for every $\epsilon > 0$ and for every $x$ there exists a $\delta$ such that $|f(x)-1| < \frac{\epsilon}{|f(a)|} $ when $|x| < \delta$ . Since this holds for all $x$ we can replace $x$ with $x-a$ . Rewriting this we get that whenever $|x-a| < \delta$ we have that $\,|f(a)f(x-a)-f(a)|<\epsilon \implies |f(x)-f(a)| < \epsilon$ .",I have the to solve the following problem: Let be a function from the real numbers to the real numbers. The function satisfies for all real . Prove that if is continuous in then is continuous in every point. I think I have a solution but I would like to know if it's correct: By letting we get that meaning that . Thus we know that for every and for every there exists a such that when . Since this holds for all we can replace with . Rewriting this we get that whenever we have that .,"f f(x+y) = f(x)f(y) x,y f 0 f y=0 f(x)=f(x)f(0) f(0)=1 \epsilon > 0 x \delta |f(x)-1| < \frac{\epsilon}{|f(a)|}  |x| < \delta x x x-a |x-a| < \delta \,|f(a)f(x-a)-f(a)|<\epsilon \implies |f(x)-f(a)| < \epsilon","['calculus', 'analysis', 'continuity', 'solution-verification']"
23,"Let $f:[0,n]\to \Bbb R$ be continuous with $f(0)=f(n)$. Then there are $n$ pairs of numbers $x,y$ such that $f(x)=f(y)$ and $y-x\in\Bbb N$.",Let  be continuous with . Then there are  pairs of numbers  such that  and .,"f:[0,n]\to \Bbb R f(0)=f(n) n x,y f(x)=f(y) y-x\in\Bbb N","Theorem. Let $f:[0,n]\to \Bbb R$ be continuous with $f(0)=f(n)$ ( $n\in\Bbb N$ ). Then there exist (at least) $n$ distinct pairs of numbers $x,y$ which satisfy $f(x)=f(y)$ and $y-x\in \mathbb{N}$ (where $0$ is not a natural number). Partial results (see the two answers below): Proposition. For $f$ as in the Theorem there exists a $x\in[0,n]$ such that $f(x)=f(x+1)$ . Proof. Define $g(x)=f(x+1)-f(x)$ where $x\in[0,n-1]$ . Note that $\sum_{i=0}^{n-1}g(i)=f(n)-f(0)=0$ . If all $g(i)=0$ then the proposition holds trivially. Otherwise there must be $i\neq j$ such that $g(i)$ and $g(j)$ have different sign. The proposition now follows from the Intermediate Value Theorem. Proposition. The Theorem holds under the additional assumption that $f$ is concave or convex. Proof. See the answer by @Maximilian Janisch. *Remark.*It is not for each $0<m\leq n$ , there must exist $x$ s.t. $f(x)=f(x+m)$ . For example, if $f_{[0,1]}(x)>0 \wedge f_{[n-1,n]}(x)<0$ , then there doesn't exist $x$ s.t. $f(x)=f(x+n-1)$ . However,for some $m$ , it may have more than one $x$ satisfying $f(x)=f(x+m)$ .","Theorem. Let be continuous with ( ). Then there exist (at least) distinct pairs of numbers which satisfy and (where is not a natural number). Partial results (see the two answers below): Proposition. For as in the Theorem there exists a such that . Proof. Define where . Note that . If all then the proposition holds trivially. Otherwise there must be such that and have different sign. The proposition now follows from the Intermediate Value Theorem. Proposition. The Theorem holds under the additional assumption that is concave or convex. Proof. See the answer by @Maximilian Janisch. *Remark.*It is not for each , there must exist s.t. . For example, if , then there doesn't exist s.t. . However,for some , it may have more than one satisfying .","f:[0,n]\to \Bbb R f(0)=f(n) n\in\Bbb N n x,y f(x)=f(y) y-x\in \mathbb{N} 0 f x\in[0,n] f(x)=f(x+1) g(x)=f(x+1)-f(x) x\in[0,n-1] \sum_{i=0}^{n-1}g(i)=f(n)-f(0)=0 g(i)=0 i\neq j g(i) g(j) f 0<m\leq n x f(x)=f(x+m) f_{[0,1]}(x)>0 \wedge f_{[n-1,n]}(x)<0 x f(x)=f(x+n-1) m x f(x)=f(x+m)","['real-analysis', 'analysis', 'functions', 'continuity']"
24,How can we see that the proof of this following real analysis problems is reasonable?,How can we see that the proof of this following real analysis problems is reasonable?,,"Problem: Let $f(x)$ be a real valued functions defined on $\mathbb{R}$ , prove that the point set $$E = \{x\in \mathbb{R}: \lim_{y\rightarrow x} f(y) = +\infty\}$$ is a finite or a countable set. Proof: Let $g(x) = \arctan f(x), x \in \mathbb{R}$ . Then the point set $E$ can be written as $$E = \{x\in \mathbb{R}: \lim_{y\rightarrow x} g(y) = \frac{\pi}{2}\}$$ Therefore $E$ is a finite or a countable set. The above proof is from a textbook of real analysis. How can we see that the set $E$ in this proof is finite or countable?","Problem: Let be a real valued functions defined on , prove that the point set is a finite or a countable set. Proof: Let . Then the point set can be written as Therefore is a finite or a countable set. The above proof is from a textbook of real analysis. How can we see that the set in this proof is finite or countable?","f(x) \mathbb{R} E = \{x\in \mathbb{R}: \lim_{y\rightarrow x} f(y) = +\infty\} g(x) = \arctan f(x), x \in \mathbb{R} E E = \{x\in \mathbb{R}: \lim_{y\rightarrow x} g(y) = \frac{\pi}{2}\} E E","['real-analysis', 'analysis', 'lebesgue-measure']"
25,Explicit expression for $b$ as a function of $a$ where $\log_b a = (a/b)^{1/2}$,Explicit expression for  as a function of  where,b a \log_b a = (a/b)^{1/2},"I was talking with a friend about how to find a more explicit formula for the relationship between the positive numbers $a$ and $b$ in the equation $$ \log_b a = (a/b)^{1/2} $$ after some rearranging we get that this is equivalent to $$ a^{a^{-1/2}}=b^{b^{-1/2}}. $$ The function $f(x)=x^{x^{-1/2}}$ achieves its global maximum of $e^{2/e}$ at $x=e^2$ and is strictly increasing on $(0,e^2)$ and strictly decreasing on $(e^2,\infty)$. Moreover $\lim\limits_{x\to-\infty}f(x)=-\infty,$ $\lim\limits_{x\to-\infty}f(x)=1$ and $f(1)=1$. Hence for any $a \in (1,e^2)$ there exists exactly one other number $b=b(a) \in (e^2,\infty)$ so that $f(a)=f(b).$ Here is a plot for the number $a$ with $f(a)=2:$ $a$ is the $x$-value of the first intersection of the two lines, $b$ is the $x$-value of the second intersection. The function $b:(1,e^2)\to(e^2,\infty)$ is differentiable, strictly decreasing and satisfies $\lim\limits_{a\downarrow 1}b(a)=\infty$ and $\lim\limits_{a\uparrow e^2}b(a)=e^2.$ I think it should be possible to find its derivative given what we have but I'm not sure how to do this. Question: Is it possible to find an explicit expression for the function $b$? Also, is there a name for this way of defining a function?","I was talking with a friend about how to find a more explicit formula for the relationship between the positive numbers $a$ and $b$ in the equation $$ \log_b a = (a/b)^{1/2} $$ after some rearranging we get that this is equivalent to $$ a^{a^{-1/2}}=b^{b^{-1/2}}. $$ The function $f(x)=x^{x^{-1/2}}$ achieves its global maximum of $e^{2/e}$ at $x=e^2$ and is strictly increasing on $(0,e^2)$ and strictly decreasing on $(e^2,\infty)$. Moreover $\lim\limits_{x\to-\infty}f(x)=-\infty,$ $\lim\limits_{x\to-\infty}f(x)=1$ and $f(1)=1$. Hence for any $a \in (1,e^2)$ there exists exactly one other number $b=b(a) \in (e^2,\infty)$ so that $f(a)=f(b).$ Here is a plot for the number $a$ with $f(a)=2:$ $a$ is the $x$-value of the first intersection of the two lines, $b$ is the $x$-value of the second intersection. The function $b:(1,e^2)\to(e^2,\infty)$ is differentiable, strictly decreasing and satisfies $\lim\limits_{a\downarrow 1}b(a)=\infty$ and $\lim\limits_{a\uparrow e^2}b(a)=e^2.$ I think it should be possible to find its derivative given what we have but I'm not sure how to do this. Question: Is it possible to find an explicit expression for the function $b$? Also, is there a name for this way of defining a function?",,"['real-analysis', 'analysis', 'functions', 'terminology', 'recreational-mathematics']"
26,"Why $SO(3,\mathbb{R})$ is $3$-dimensional?",Why  is -dimensional?,"SO(3,\mathbb{R}) 3","$SO(3,\mathbb{R})$ is a 3-dimensional manifold. However, when we can represent a fixed angle using the sphere coordinate, there are only $2$ parameters. Why?","$SO(3,\mathbb{R})$ is a 3-dimensional manifold. However, when we can represent a fixed angle using the sphere coordinate, there are only $2$ parameters. Why?",,"['analysis', 'lie-groups']"
27,Spivak' Calculus Chapter 7 Problem 19(b) [continuity],Spivak' Calculus Chapter 7 Problem 19(b) [continuity],,"The question is from Spivak's Calculus 3rd Ed: Suppose $ 0<a<1 $, but that $a$ is not equal to $1/n$ for any natural number $n$.  Find a function $f$ which is continuous on $[0,1]$ and which satisfies $ f(0) = f(1) $, but which does not satisfy $f(x) = f(x+a)$ for any $x$. The solution from Spivak's solutions book is as follows: In general if $ {1 \over n+1} <a< {1 \over n} $ then define $f$ arbitrarily on $[0,a]$, subject only to $f(0)=0 , f(a)>0$ and  $f(1-na)=-nf(a).$  Then define $f$ on $[ka,(k+1)a]$ by $ f(ka+x)=f(x)+ ka.$  In particular we have, we have $f(1) = f(na + (1-na))= na+f(1-na)=0$ but $f(x+a)-f(a)=f(a)>0$ for all $x$. I don't see how we get $na+f(1-na)=0$ and the final $f(x+a)-f(a)=f(a)>0$ for all $x$. EDIT:  Post below found an error in the answer book.  Instead we require $f(ka+x)=k\color{red}{f(a)}+f(x)$, then we have $f(1)=0$ and the final statement also follows.","The question is from Spivak's Calculus 3rd Ed: Suppose $ 0<a<1 $, but that $a$ is not equal to $1/n$ for any natural number $n$.  Find a function $f$ which is continuous on $[0,1]$ and which satisfies $ f(0) = f(1) $, but which does not satisfy $f(x) = f(x+a)$ for any $x$. The solution from Spivak's solutions book is as follows: In general if $ {1 \over n+1} <a< {1 \over n} $ then define $f$ arbitrarily on $[0,a]$, subject only to $f(0)=0 , f(a)>0$ and  $f(1-na)=-nf(a).$  Then define $f$ on $[ka,(k+1)a]$ by $ f(ka+x)=f(x)+ ka.$  In particular we have, we have $f(1) = f(na + (1-na))= na+f(1-na)=0$ but $f(x+a)-f(a)=f(a)>0$ for all $x$. I don't see how we get $na+f(1-na)=0$ and the final $f(x+a)-f(a)=f(a)>0$ for all $x$. EDIT:  Post below found an error in the answer book.  Instead we require $f(ka+x)=k\color{red}{f(a)}+f(x)$, then we have $f(1)=0$ and the final statement also follows.",,"['calculus', 'analysis', 'continuity']"
28,How to show the real part of a holomorphic function is harmonic,How to show the real part of a holomorphic function is harmonic,,"So I know that for a real valued function $u$, it's harmonic if it's continuously differentiable and it satisfies $$u_{xx}+u_{yy}=0$$ How do I show that for a generic holomorphic function $f$, that the real part $Re(f)(z)$ is harmonic?","So I know that for a real valued function $u$, it's harmonic if it's continuously differentiable and it satisfies $$u_{xx}+u_{yy}=0$$ How do I show that for a generic holomorphic function $f$, that the real part $Re(f)(z)$ is harmonic?",,"['analysis', 'functions', 'harmonic-functions']"
29,"A continuous onto function from $[0,1)$ to $(-1,1)$",A continuous onto function from  to,"[0,1) (-1,1)","How I can construct a continuous onto function from $[0,1)$ to $(-1,1)$ ? I know that such a function exists and also I have a function $\displaystyle f(x)=x^2\sin\frac{1}{1-x}$ which is continuous and onto from $[0,1)$ to $(-1,1)$. But I don't know how I can construct this type of function.  There are many function which is continuous onto from $[0,1)$ to $(-1,1)$. I can construct a continuous onto function from $(0,1)$ to $(-1,1)$. But when the domain interval is left-closed then how I can construct ? Please give the idea to construct the function.","How I can construct a continuous onto function from $[0,1)$ to $(-1,1)$ ? I know that such a function exists and also I have a function $\displaystyle f(x)=x^2\sin\frac{1}{1-x}$ which is continuous and onto from $[0,1)$ to $(-1,1)$. But I don't know how I can construct this type of function.  There are many function which is continuous onto from $[0,1)$ to $(-1,1)$. I can construct a continuous onto function from $(0,1)$ to $(-1,1)$. But when the domain interval is left-closed then how I can construct ? Please give the idea to construct the function.",,"['real-analysis', 'analysis', 'functions']"
30,Is my proof of the uniqueness of $0$ correct?,Is my proof of the uniqueness of  correct?,0,"I am working my way through ""Mathematical Analysis"" by Apostol. What I am attempting to prove is that if there exist  $q_{1}$ and $q_{2}$ such that $x + q_1 = x$ and $y+q_2=y$, then $q_1=q_2$ Sometimes I will be using $q$ to denote $0$ I am using the first $4$ field axioms from the book: Axiom 1: Commutative Laws $x+y=y+x$, $xy=yx$ Axiom 2: Associative Laws $x+(y+z)=(x+y)+z$, $x(yz)=(xy)z$ Axiom 3: Distributive Law $x(y+z)=xy+yz$ Axiom 4: Given any two real numbers $x$ and $y$, there exists a real number $z$ such that $x+z=y$. This $z$ is denoted by $y-x$; the number $x-x$ is denoted by $0$ (it can be proved that $0$ is independent of $x$.) We write $-x$ for $0-x$ and call $-x$ the negative of $x$. Lemma 1: $x + 0 = x$ From axiom 4 we are guaranteed a $z$ such that $x+z=x$. This $z$ is denoted by $x-x$, which is denoted by $0$ Therefore $x+z=x \Longrightarrow$ $x+0=x$ Lemma 2: $x+(-x)=0$ We can rewrite the above as $x + (0-x)$, which, from definition in axiom 4, evaluates to $0$ Lemma 3: If $x+q_a=x$, and $x+q_b=x$, then $q_a=q_b$ From the first sentence of axiom 4 we are guaranteed at least one $q_a$ such that $x + q_a = x$ If $q_a$ is not unique, then we will also have $x+q_b=x$ $x+q_a=x+q_b$ Add $(-x)$ to both sides $(x+q_a)+(-x)=(x+q_b)+(-x)$ Commutative Propriety $(q_a+x)+(-x)=(q_b+x)+(-x)$ Associative Proprety $q_a+(x+(-x))=q_b+(x+(-x))$ Lemma 2 $q_a + 0=q_b+0$ Lemma 1 $q_a=q_b$ Now for the actual proof: We are trying to prove that if there exist  $q_{1}$ and $q_{2}$ such that $x + q_1 = x$ and $y+q_2=y$, then $q_1=q_2$ By the first sentence in axiom 4, we are guaranteed $q_1$ and $q_2$ such $x + q_1 = x$ $x+q_2=x$ By lemma 3, we are guaranteed that if By axiom 4 guaranteed the existence of a $z$ such that $x+z=y$ Now we substitute $(x+z)+q_2=(x+z)$ Add $(-z)$ to both sides $((x+z)+q_2)+(-z)=(x+z)+(-z)$ Associative and commutative proprieties lead to $x+q_2 + (z+(-z))=x+(z+(-z))$ $x+q^2=x$ By lemma 3 we are guaranteed that if $x+q_1=x$, and $x+q_2=x$, then $q_1=q_2$ Therefore $q_1=q_2$ so we have completed the proof.","I am working my way through ""Mathematical Analysis"" by Apostol. What I am attempting to prove is that if there exist  $q_{1}$ and $q_{2}$ such that $x + q_1 = x$ and $y+q_2=y$, then $q_1=q_2$ Sometimes I will be using $q$ to denote $0$ I am using the first $4$ field axioms from the book: Axiom 1: Commutative Laws $x+y=y+x$, $xy=yx$ Axiom 2: Associative Laws $x+(y+z)=(x+y)+z$, $x(yz)=(xy)z$ Axiom 3: Distributive Law $x(y+z)=xy+yz$ Axiom 4: Given any two real numbers $x$ and $y$, there exists a real number $z$ such that $x+z=y$. This $z$ is denoted by $y-x$; the number $x-x$ is denoted by $0$ (it can be proved that $0$ is independent of $x$.) We write $-x$ for $0-x$ and call $-x$ the negative of $x$. Lemma 1: $x + 0 = x$ From axiom 4 we are guaranteed a $z$ such that $x+z=x$. This $z$ is denoted by $x-x$, which is denoted by $0$ Therefore $x+z=x \Longrightarrow$ $x+0=x$ Lemma 2: $x+(-x)=0$ We can rewrite the above as $x + (0-x)$, which, from definition in axiom 4, evaluates to $0$ Lemma 3: If $x+q_a=x$, and $x+q_b=x$, then $q_a=q_b$ From the first sentence of axiom 4 we are guaranteed at least one $q_a$ such that $x + q_a = x$ If $q_a$ is not unique, then we will also have $x+q_b=x$ $x+q_a=x+q_b$ Add $(-x)$ to both sides $(x+q_a)+(-x)=(x+q_b)+(-x)$ Commutative Propriety $(q_a+x)+(-x)=(q_b+x)+(-x)$ Associative Proprety $q_a+(x+(-x))=q_b+(x+(-x))$ Lemma 2 $q_a + 0=q_b+0$ Lemma 1 $q_a=q_b$ Now for the actual proof: We are trying to prove that if there exist  $q_{1}$ and $q_{2}$ such that $x + q_1 = x$ and $y+q_2=y$, then $q_1=q_2$ By the first sentence in axiom 4, we are guaranteed $q_1$ and $q_2$ such $x + q_1 = x$ $x+q_2=x$ By lemma 3, we are guaranteed that if By axiom 4 guaranteed the existence of a $z$ such that $x+z=y$ Now we substitute $(x+z)+q_2=(x+z)$ Add $(-z)$ to both sides $((x+z)+q_2)+(-z)=(x+z)+(-z)$ Associative and commutative proprieties lead to $x+q_2 + (z+(-z))=x+(z+(-z))$ $x+q^2=x$ By lemma 3 we are guaranteed that if $x+q_1=x$, and $x+q_2=x$, then $q_1=q_2$ Therefore $q_1=q_2$ so we have completed the proof.",,"['real-analysis', 'analysis', 'proof-verification', 'axioms']"
31,Proving a set of Lipschitz Continuous functions is closed,Proving a set of Lipschitz Continuous functions is closed,,"Let $X:=C[0,1]$ denote the set of all continuous functions $f:[0,1]\to\mathbb{R}$. For $f,g\in C[0,1]$, define $$d(f,g):=\max_{x\in[0,1]}|f(x)-f(y)|.$$ Consider the subset of X containing all Lipschitz continuous functions with Lipschitz constant $l$: $$\Omega:=\{f\in C[0,1]\mid |f(x)-f(y)|\leq l|x-y|\}.$$ Prove that $\Omega$ is a closed set.","Let $X:=C[0,1]$ denote the set of all continuous functions $f:[0,1]\to\mathbb{R}$. For $f,g\in C[0,1]$, define $$d(f,g):=\max_{x\in[0,1]}|f(x)-f(y)|.$$ Consider the subset of X containing all Lipschitz continuous functions with Lipschitz constant $l$: $$\Omega:=\{f\in C[0,1]\mid |f(x)-f(y)|\leq l|x-y|\}.$$ Prove that $\Omega$ is a closed set.",,"['real-analysis', 'analysis']"
32,"Prove that if $B = \{x-y : x,y \in A\}$, where $A$ is a Borel measurable subset of $R$ with positive measure","Prove that if , where  is a Borel measurable subset of  with positive measure","B = \{x-y : x,y \in A\} A R","Suppose that $m$ is Lebesgue measure, and $A$ is a Borel measurable subset of $R$ with $m(A) > 0$. Prove that if $B = \{x - y : x,y \in A\}$, then $B$ contains a non-empty open interval centered at the origin (Steinhaus theorem). My attempt at a solution: I have two ideas for this proof. My first idea was to show that it was true for intervals, and then generalize to Borel measurable sets using the $\pi-\lambda$ theorem, but it doesn't seem that the set of all subsets of $R$ such that $B$ (defined as above) contains an open interval is a $\lambda$-system. The second idea, which was a hint that was given to me, was to consider the function $f(x) = m((x+A)\cap A)$. If this function could be shown to be continuous, then we could consider $f(0) = m(A) > 0$. I don't really know where this gets us, though.","Suppose that $m$ is Lebesgue measure, and $A$ is a Borel measurable subset of $R$ with $m(A) > 0$. Prove that if $B = \{x - y : x,y \in A\}$, then $B$ contains a non-empty open interval centered at the origin (Steinhaus theorem). My attempt at a solution: I have two ideas for this proof. My first idea was to show that it was true for intervals, and then generalize to Borel measurable sets using the $\pi-\lambda$ theorem, but it doesn't seem that the set of all subsets of $R$ such that $B$ (defined as above) contains an open interval is a $\lambda$-system. The second idea, which was a hint that was given to me, was to consider the function $f(x) = m((x+A)\cap A)$. If this function could be shown to be continuous, then we could consider $f(0) = m(A) > 0$. I don't really know where this gets us, though.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
33,Increasing/Decreasing intervals of a parabola,Increasing/Decreasing intervals of a parabola,,"I am being told to find the intervals on which the function is increasing or decreasing. It is a normal positive parabola with the vertex at $(3,0).$  The equation could be $y = (x-3)^2,$  but my confusion comes from the interval on which the parabola is increasing: I would think increasing is $(3,\infty)$ and decreasing is $(-\infty, 3)$. However the text book teaches to use $[3,\infty)$ and $(-\infty, 3].$ Can you explain this.  I thought the function was constant at $x=3$. Thanks","I am being told to find the intervals on which the function is increasing or decreasing. It is a normal positive parabola with the vertex at $(3,0).$  The equation could be $y = (x-3)^2,$  but my confusion comes from the interval on which the parabola is increasing: I would think increasing is $(3,\infty)$ and decreasing is $(-\infty, 3)$. However the text book teaches to use $[3,\infty)$ and $(-\infty, 3].$ Can you explain this.  I thought the function was constant at $x=3$. Thanks",,"['analysis', 'functions']"
34,Derivative of the power tower,Derivative of the power tower,,May somebody help me to correctly calculate the dervative of the $n$-th power tower function? $$ \begin{align} f_1(x)&=x\\ f_n(x)&=x^{f_{n-1}(x)}\\ &=x^{x^{x^{...^x}}}\text{ where }x\text{ occurs }n\text{ times} \end{align} $$ The solution given here is for the infinite case $f_{\infty}=\lim_{n\to\infty}f_n$.,May somebody help me to correctly calculate the dervative of the $n$-th power tower function? $$ \begin{align} f_1(x)&=x\\ f_n(x)&=x^{f_{n-1}(x)}\\ &=x^{x^{x^{...^x}}}\text{ where }x\text{ occurs }n\text{ times} \end{align} $$ The solution given here is for the infinite case $f_{\infty}=\lim_{n\to\infty}f_n$.,,"['analysis', 'derivatives']"
35,Q: $\lim_{n\to \infty}\left(1 + \frac{1}{n}\right)^{n} = e$,Q:,\lim_{n\to \infty}\left(1 + \frac{1}{n}\right)^{n} = e,"I am having difficulty with the proof $$\lim_{n\to \infty}\left(1 + \frac{1}{n}\right)^{n} = e$$ in Rudin's Principles of Mathematics. In particular, the last few steps.  The proof is as follows: Theorem. $\lim_{n\to\infty} \left(1 + \frac1n \right)^n = e.$ Proof. Let $$s_n = \sum_{k=0}^{n} \frac{1}{k!}, \qquad t_n = \left(1 + \frac1n \right)^n.$$ By the binomial theorem, $$t_n = 1 + 1 + \frac1{2!}\left( 1 - \frac1n \right) + \frac1{3!}\left(1 - \frac1n \right)\left(1 - \frac2n \right) + \dots \\ + \frac1{n!}\left(1 - \frac1n \right)\left(1 - \frac2n \right)\dots\left(1-\frac{n-1}{n}\right).$$ Hence $t_n \leq s_n$, so that $$\limsup_{n\to\infty} t_n \leq e,  \tag{14}$$ by Theorem 3.19. Next, if $n \geq m$, $$t_n \geq 1 + 1 + \frac1{2!}\left(1-\frac1n\right) + \dots + \frac1{m!}\left(1-\frac1n\right)\dots\left(1-\frac{m-1}{n}\right).$$ Let $n\to\infty$, keeping $m$ fixed. We get $$\liminf_{n\to\infty} t_n \geq 1 + 1 + \frac1{2!} + \dots + \frac1{m!},$$ so that $$s_m \leq \liminf_{n\to\infty} t_n.$$ Letting $m\to\infty$, we finally get $$e \leq \liminf_{n\to\infty} t_n. \tag{15}$$ The theorem follows from (14) and (15). (original image) My question: What allows us to say that $$\lim_{n\to \infty}\text{inf } t_{n} \geq s_{m}?$$ I see that $\lim_{n\to \infty}t_{n} \geq s_{m}$ (if it exists, I suppose otherwise it's vacuously true...), but why the $\liminf$?  The text does similar steps in other proofs without explanation, so I am not sure if I am misunderstanding something obvious...","I am having difficulty with the proof $$\lim_{n\to \infty}\left(1 + \frac{1}{n}\right)^{n} = e$$ in Rudin's Principles of Mathematics. In particular, the last few steps.  The proof is as follows: Theorem. $\lim_{n\to\infty} \left(1 + \frac1n \right)^n = e.$ Proof. Let $$s_n = \sum_{k=0}^{n} \frac{1}{k!}, \qquad t_n = \left(1 + \frac1n \right)^n.$$ By the binomial theorem, $$t_n = 1 + 1 + \frac1{2!}\left( 1 - \frac1n \right) + \frac1{3!}\left(1 - \frac1n \right)\left(1 - \frac2n \right) + \dots \\ + \frac1{n!}\left(1 - \frac1n \right)\left(1 - \frac2n \right)\dots\left(1-\frac{n-1}{n}\right).$$ Hence $t_n \leq s_n$, so that $$\limsup_{n\to\infty} t_n \leq e,  \tag{14}$$ by Theorem 3.19. Next, if $n \geq m$, $$t_n \geq 1 + 1 + \frac1{2!}\left(1-\frac1n\right) + \dots + \frac1{m!}\left(1-\frac1n\right)\dots\left(1-\frac{m-1}{n}\right).$$ Let $n\to\infty$, keeping $m$ fixed. We get $$\liminf_{n\to\infty} t_n \geq 1 + 1 + \frac1{2!} + \dots + \frac1{m!},$$ so that $$s_m \leq \liminf_{n\to\infty} t_n.$$ Letting $m\to\infty$, we finally get $$e \leq \liminf_{n\to\infty} t_n. \tag{15}$$ The theorem follows from (14) and (15). (original image) My question: What allows us to say that $$\lim_{n\to \infty}\text{inf } t_{n} \geq s_{m}?$$ I see that $\lim_{n\to \infty}t_{n} \geq s_{m}$ (if it exists, I suppose otherwise it's vacuously true...), but why the $\liminf$?  The text does similar steps in other proofs without explanation, so I am not sure if I am misunderstanding something obvious...",,"['real-analysis', 'analysis']"
36,Are all measures Lebesgue-Stieltjes measures?,Are all measures Lebesgue-Stieltjes measures?,,"In our lecture we ran out of time, so our prof told us a few properties about measure: He said that a measure is $\sigma$-additive iff it has a right-side continuous function that it creates. And he was not only referring to probability measures.  After going through my lecture notes, I thought that this would imply that there can be no other measures than ones having a right-side continuous function (I think they are called Lebesgue-Stieltjes measures) as $\sigma$-additivity is a prerequisite to be a measure. So somehow, this does not fit together. Does anybody know what he could have meant here? Or was he only referring to probability measures? Is anything unclear about my question?","In our lecture we ran out of time, so our prof told us a few properties about measure: He said that a measure is $\sigma$-additive iff it has a right-side continuous function that it creates. And he was not only referring to probability measures.  After going through my lecture notes, I thought that this would imply that there can be no other measures than ones having a right-side continuous function (I think they are called Lebesgue-Stieltjes measures) as $\sigma$-additivity is a prerequisite to be a measure. So somehow, this does not fit together. Does anybody know what he could have meant here? Or was he only referring to probability measures? Is anything unclear about my question?",,"['calculus', 'real-analysis']"
37,"Show that $d(x,y)$ in a metric on $X$.",Show that  in a metric on .,"d(x,y) X","Let $d_a(x,y)=7|x-y|$ and $d_b(x,y)=|x+y|$ be metrics on set $X$. Show that $d(x,y)=d_a(x,y) + d_b(x,y)$ is also a metric on $X$. Would I be correct in writing $d_a(x,y) + d_b(x,y)$ as $7|x-y| + |x+y|$? And then applying the regular properties of a metric? If this is the case then how would we prove the triangle inequality?","Let $d_a(x,y)=7|x-y|$ and $d_b(x,y)=|x+y|$ be metrics on set $X$. Show that $d(x,y)=d_a(x,y) + d_b(x,y)$ is also a metric on $X$. Would I be correct in writing $d_a(x,y) + d_b(x,y)$ as $7|x-y| + |x+y|$? And then applying the regular properties of a metric? If this is the case then how would we prove the triangle inequality?",,"['analysis', 'metric-spaces']"
38,Lebesgue points of density and similar notions,Lebesgue points of density and similar notions,,"Let $F\subset \mathbb{R}^d$ and $\delta(x)=d(x,F)=\inf\{|x-y|:y\in F\}$ be the distance from $x$ to $F$. It is easy to show that $\delta(x+y)\leq |y|$ for all $x\in F$. Prove the more refined estimate:  $$\lim_{|y|\rightarrow0}\frac{\delta(x+y)}{|y|}=0,\text{ for a.e. }x\in F .$$ Notes A hint is given that says ""Assume $x$ is a point of density of $F$ and use the conclusion: If $E$ is a measurable subset of $\mathbb{R}^d$ then almost every $x\in E$ is a point of density of $E$, and almost every $x\in E^c$ is not a point of density of $E$. I have proved the above result (under notes) using the Lebesgue differential theorem in a previous question. But I am having difficulty relating the limit in the question to that in the definition of a Lebesgue point of density, although they seem very similar. Any help is appreciated. Thanks!","Let $F\subset \mathbb{R}^d$ and $\delta(x)=d(x,F)=\inf\{|x-y|:y\in F\}$ be the distance from $x$ to $F$. It is easy to show that $\delta(x+y)\leq |y|$ for all $x\in F$. Prove the more refined estimate:  $$\lim_{|y|\rightarrow0}\frac{\delta(x+y)}{|y|}=0,\text{ for a.e. }x\in F .$$ Notes A hint is given that says ""Assume $x$ is a point of density of $F$ and use the conclusion: If $E$ is a measurable subset of $\mathbb{R}^d$ then almost every $x\in E$ is a point of density of $E$, and almost every $x\in E^c$ is not a point of density of $E$. I have proved the above result (under notes) using the Lebesgue differential theorem in a previous question. But I am having difficulty relating the limit in the question to that in the definition of a Lebesgue point of density, although they seem very similar. Any help is appreciated. Thanks!",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
39,Oscillation and Hölder continuity,Oscillation and Hölder continuity,,"I am studying a proof of a theorem. And I have the following situation in the proof: Consider $\Omega$ is a bounded open set of $\mathbb R^n$ and $u: \Omega \to \mathbb R$ is a function satisfying: $$\operatorname{osc}_{B(x_0,R)} u \leq (1-\delta) \operatorname{osc}_{B(x_0,4R)},$$ for all ${B(x_0,R)} \subset \Omega$ for some $0<\delta <1$ ($\delta$ is independent of the open ball). The book says: Iterating this inequality we have that $u$ is Hölder continuous. Someone can help me understand the proof in the part of the ""Iteration"". Thank you!","I am studying a proof of a theorem. And I have the following situation in the proof: Consider $\Omega$ is a bounded open set of $\mathbb R^n$ and $u: \Omega \to \mathbb R$ is a function satisfying: $$\operatorname{osc}_{B(x_0,R)} u \leq (1-\delta) \operatorname{osc}_{B(x_0,4R)},$$ for all ${B(x_0,R)} \subset \Omega$ for some $0<\delta <1$ ($\delta$ is independent of the open ball). The book says: Iterating this inequality we have that $u$ is Hölder continuous. Someone can help me understand the proof in the part of the ""Iteration"". Thank you!",,"['real-analysis', 'analysis']"
40,Construction of a join function of infinitely many derivatives.,Construction of a join function of infinitely many derivatives.,,"I am curious if anyone can construct a function made up of more than one e.g. $|x| = x, x\geq 0$ and $-x, x\leq 0$. However I would require that it must be infinitely differentiable and in the above case of $|x|$ it must be infinitely differentiable at 0. I suspect that we can't find one but if anyone has an example or a rigorous explanation as to why it doesn't work it would be grateful.","I am curious if anyone can construct a function made up of more than one e.g. $|x| = x, x\geq 0$ and $-x, x\leq 0$. However I would require that it must be infinitely differentiable and in the above case of $|x|$ it must be infinitely differentiable at 0. I suspect that we can't find one but if anyone has an example or a rigorous explanation as to why it doesn't work it would be grateful.",,['analysis']
41,Bounded partial derivatives imply continuity,Bounded partial derivatives imply continuity,,"As stated in my notes: Remark: Suppose $f: E \to \mathbb{R}$, $E \subseteq \mathbb{R}^n$, and $p \in E$.  Also, suppose that $D_if$ exists in some neighborhood of $p$, say, $N(p, h)$ where $h>0$.  If all partial derivatives of $f$ are bounded, then $f$ is continuous on $E$. I found a sketch of the proof here .  I'm wondering if I can adapt this proof as follows: $f(x_1+h_1,...,x_n+h_n)-f(x_1,...,x_n)=f(x_1+h_1,...,x_n+h_n)-f(x_1,x_2+h_2,...,x_n+h_n)-...-f(x_1,x_2,...,x_{n-1}+h_{n-1},x_n+h_n)-f(x_1,...,x_{n-1},x_n+h_n)-f(x_1,...,x_n)$ However, I'm not sure how to apply the contraction principle to finish off the proof.  Is there a more efficient way to prove the above remark?","As stated in my notes: Remark: Suppose $f: E \to \mathbb{R}$, $E \subseteq \mathbb{R}^n$, and $p \in E$.  Also, suppose that $D_if$ exists in some neighborhood of $p$, say, $N(p, h)$ where $h>0$.  If all partial derivatives of $f$ are bounded, then $f$ is continuous on $E$. I found a sketch of the proof here .  I'm wondering if I can adapt this proof as follows: $f(x_1+h_1,...,x_n+h_n)-f(x_1,...,x_n)=f(x_1+h_1,...,x_n+h_n)-f(x_1,x_2+h_2,...,x_n+h_n)-...-f(x_1,x_2,...,x_{n-1}+h_{n-1},x_n+h_n)-f(x_1,...,x_{n-1},x_n+h_n)-f(x_1,...,x_n)$ However, I'm not sure how to apply the contraction principle to finish off the proof.  Is there a more efficient way to prove the above remark?",,['analysis']
42,"If $f_n\colon [0, 1] \to [0, 1]$ are nondecreasing and $\{f_n\}$ converges pointwise to a continuous $f$, then the convergence is uniform","If  are nondecreasing and  converges pointwise to a continuous , then the convergence is uniform","f_n\colon [0, 1] \to [0, 1] \{f_n\} f","Suppose that $\{f_n\}$ is a sequence of nondecreasing functions which map the unit interval into itself. Suppose that $$\lim_{n\rightarrow \infty} f_n(x)=f(x)$$ pointwise and that $f$ is a continuous function. Prove that $f_n(x) \rightarrow f(x)$ uniformly as $n \rightarrow \infty$, $0\leq x\leq1$. Note that the functions $f_n$ are not necessarily continuous. This is one of the preliminary exam from UC Berkeley, the solution goes like this: Because $f$ is continuous on $[0,1]$, which is compact, it is then uniformly continuous. Hence there exists $\delta >0$ such that if $|x-y|<\delta$ then $|f(x)-f(y)|<\epsilon$. We then partition the interval with $x_0=0, \cdots ,x_m=1$ such that the distance $x_{i}-x_{i-1}$ is less than $\delta$. Note that since there are only finite number of $x_m$, there is $N\in \mathbb{N}$ such that if $n\geq N$ then $|f_n(x_i)-f(x_i)|<\epsilon$ where $i=0,\cdots, m$ Now if $x\in[0,1]$, then $x\in[x_{i-1},x_i]$ for some $i\in\{1, \cdots m\}$. My question is how to use the nondecreasingness to arrived at this inequality, for $n\geq N$ $f(x_{i-1})-\epsilon<f_n(x)<f(x_{i-1})+2\epsilon$ Can someone please help, I have been staring at the inequality for about a day now. Thanks.","Suppose that $\{f_n\}$ is a sequence of nondecreasing functions which map the unit interval into itself. Suppose that $$\lim_{n\rightarrow \infty} f_n(x)=f(x)$$ pointwise and that $f$ is a continuous function. Prove that $f_n(x) \rightarrow f(x)$ uniformly as $n \rightarrow \infty$, $0\leq x\leq1$. Note that the functions $f_n$ are not necessarily continuous. This is one of the preliminary exam from UC Berkeley, the solution goes like this: Because $f$ is continuous on $[0,1]$, which is compact, it is then uniformly continuous. Hence there exists $\delta >0$ such that if $|x-y|<\delta$ then $|f(x)-f(y)|<\epsilon$. We then partition the interval with $x_0=0, \cdots ,x_m=1$ such that the distance $x_{i}-x_{i-1}$ is less than $\delta$. Note that since there are only finite number of $x_m$, there is $N\in \mathbb{N}$ such that if $n\geq N$ then $|f_n(x_i)-f(x_i)|<\epsilon$ where $i=0,\cdots, m$ Now if $x\in[0,1]$, then $x\in[x_{i-1},x_i]$ for some $i\in\{1, \cdots m\}$. My question is how to use the nondecreasingness to arrived at this inequality, for $n\geq N$ $f(x_{i-1})-\epsilon<f_n(x)<f(x_{i-1})+2\epsilon$ Can someone please help, I have been staring at the inequality for about a day now. Thanks.",,"['real-analysis', 'analysis', 'metric-spaces']"
43,Determine whether or not the cube root of x is Lipschitz.,Determine whether or not the cube root of x is Lipschitz.,,"Determine whether or not the cube root of x is Lipschitz.   x^(1/3) I understand that this is NOT Lipschitz. I am having trouble properly proving this. I know for other problems I have shown functions are not uniformly continuous to prove that they are not Lipschitz, but I can't seem to figure out the right way to go about this problem. Reviewing for a test, Thanks!","Determine whether or not the cube root of x is Lipschitz.   x^(1/3) I understand that this is NOT Lipschitz. I am having trouble properly proving this. I know for other problems I have shown functions are not uniformly continuous to prove that they are not Lipschitz, but I can't seem to figure out the right way to go about this problem. Reviewing for a test, Thanks!",,"['analysis', 'continuity']"
44,Is Completeness intrinsic to a space?,Is Completeness intrinsic to a space?,,"Is completeness an intrinsic property of a space that is independent of metric? For example, since $\mathbb{R}^n$ is complete with the Euclidean metric, is it complete with any other metric? If completeness is an intrinsic property, why is it intrinsic? Thanks :)","Is completeness an intrinsic property of a space that is independent of metric? For example, since $\mathbb{R}^n$ is complete with the Euclidean metric, is it complete with any other metric? If completeness is an intrinsic property, why is it intrinsic? Thanks :)",,"['analysis', 'convergence-divergence', 'metric-spaces']"
45,Extension of uniformly continuous function,Extension of uniformly continuous function,,"I want to prove this: $f\in C((a,b))$ uniformly continuous. Then there exists $\tilde{f}\in C([a,b])$ extension of $f$. I took $x_n\rightarrow a$ and defined $\tilde{f}(a)=\mathrm{lim}\;f(x_n)$.  I saw that this is a good definition, the only thing that I'm not able to prove is that $\tilde{f}$ is continuous at $a$ (or $b$). Could you help me please?","I want to prove this: $f\in C((a,b))$ uniformly continuous. Then there exists $\tilde{f}\in C([a,b])$ extension of $f$. I took $x_n\rightarrow a$ and defined $\tilde{f}(a)=\mathrm{lim}\;f(x_n)$.  I saw that this is a good definition, the only thing that I'm not able to prove is that $\tilde{f}$ is continuous at $a$ (or $b$). Could you help me please?",,"['real-analysis', 'analysis']"
46,About differentiability,About differentiability,,"Let $f(x)$ be a continuous real-valued function on $\mathbb{R}$. If it is differentiable at every $x\neq0$ and if $\lim_{x\rightarrow0}f'(x)$ exists, does it imply that $f(x)$ is differentiable at $x=0$ ? Intuitively it should be true but I think there might be a counterexample, which exploits switching limits and uniform continuity.","Let $f(x)$ be a continuous real-valued function on $\mathbb{R}$. If it is differentiable at every $x\neq0$ and if $\lim_{x\rightarrow0}f'(x)$ exists, does it imply that $f(x)$ is differentiable at $x=0$ ? Intuitively it should be true but I think there might be a counterexample, which exploits switching limits and uniform continuity.",,"['calculus', 'real-analysis', 'analysis']"
47,Why Lebesgue measure? Why Borel σ-algebra?,Why Lebesgue measure? Why Borel σ-algebra?,,"Is any measure on any σ-algebra inside the power set of $\mathbb{R}^d$ a formal definition (or generalisation) of ""volume"" in $\mathbb{R}^d$ ? What's so special about Lebesgue measure that we choose it as the standard way to assign measure to subsets of $\mathbb{R}^d$ ? What's so special about Borel σ algebra? Why not other σ-algebra? Is there a measure on the Borel σ algebra of $\mathbb{R}^d$ such that $\gamma ((a,b])$ may not be $b-a$ ? For question 2, I guess Lebesgue measure is chosen as the standard way because it's the unique measure on the Borel σ algebra of $\mathbb{R}^d$ such that $\gamma ((a,b])=b-a$ . But I'm not sure if that's the reason, I'm not even sure if the important bit is the ""Borel σ algebra"" or "" $\gamma ((a,b])=b-a$ "". Any help will be appreciated!","Is any measure on any σ-algebra inside the power set of a formal definition (or generalisation) of ""volume"" in ? What's so special about Lebesgue measure that we choose it as the standard way to assign measure to subsets of ? What's so special about Borel σ algebra? Why not other σ-algebra? Is there a measure on the Borel σ algebra of such that may not be ? For question 2, I guess Lebesgue measure is chosen as the standard way because it's the unique measure on the Borel σ algebra of such that . But I'm not sure if that's the reason, I'm not even sure if the important bit is the ""Borel σ algebra"" or "" "". Any help will be appreciated!","\mathbb{R}^d \mathbb{R}^d \mathbb{R}^d \mathbb{R}^d \gamma ((a,b]) b-a \mathbb{R}^d \gamma ((a,b])=b-a \gamma ((a,b])=b-a","['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure', 'borel-sets']"
48,"Minimal Definition of ""Finite Set""","Minimal Definition of ""Finite Set""",,"Walter Rudin's Principles of Mathematical Analysis , page 25 has this definition: For any postive integer $n$ , let $J_n$ be the set whose elements are the integers $1,2,\ldots,n$ . $A$ is finite if $A\sim J_n$ for some $n$ (the empty set is also considered to be finite) Here $B\sim C$ means ``there exists a 1-1 mapping of $B$ onto $C$ ''. Wikipedia has the same definition: Formally, a set $S$ is called finite if there exists a bijection $f:S\rightarrow\{1,\ldots,n\}$ for some natural number $n$ . Question : is it not sufficient to stipulate that the map be 1-1? Requiring also that the map be onto seems to be unnecessary, and it seems like best practice to make minimal definitions.","Walter Rudin's Principles of Mathematical Analysis , page 25 has this definition: For any postive integer , let be the set whose elements are the integers . is finite if for some (the empty set is also considered to be finite) Here means ``there exists a 1-1 mapping of onto ''. Wikipedia has the same definition: Formally, a set is called finite if there exists a bijection for some natural number . Question : is it not sufficient to stipulate that the map be 1-1? Requiring also that the map be onto seems to be unnecessary, and it seems like best practice to make minimal definitions.","n J_n 1,2,\ldots,n A A\sim J_n n B\sim C B C S f:S\rightarrow\{1,\ldots,n\} n","['analysis', 'functions', 'elementary-set-theory', 'set-theory', 'definition']"
49,Reason Behind the name for Jungle-River Metric.,Reason Behind the name for Jungle-River Metric.,,"The Metric defined by $d(x,y)=|x_2-y_2|$ if $x_1=y_1$ and  = $|x_2|+|y_2|+|x_1-y_1|$ if $x_1 \neq y_1$ ; where $x=(x_1,x_2)$ and $y=(y_1,y_2)$ is called the Jungle-River Metric. Is there any special reason for calling this Metric by this name?",The Metric defined by if and  = if ; where and is called the Jungle-River Metric. Is there any special reason for calling this Metric by this name?,"d(x,y)=|x_2-y_2| x_1=y_1 |x_2|+|y_2|+|x_1-y_1| x_1 \neq y_1 x=(x_1,x_2) y=(y_1,y_2)","['real-analysis', 'analysis', 'metric-spaces', 'terminology']"
50,An Simple Analysis Problem,An Simple Analysis Problem,,"$\varphi:\textrm{R}\rightarrow\textrm{R}$ is continuous, $\lim_{x\rightarrow\infty}\varphi(x)-x=\infty$ , and $\{x\in\textrm{R}|\varphi(x)=x\}$ is a finite non-empty set. If $f:\textrm{R}\rightarrow\textrm{R}$ is continuous and $f\circ\varphi=f$ , prove that $f$ is a constant function. I think maybe we can prove it by contradiction, assume that $f$ isn't a constant function, considering $f$ is continuous, it's easy to see that $f$ has an infinite number of different values in some $[a,b](a,b\in\textrm{R})$ . Note that $\varphi$ has finite fixed points, perhaps contradictions can be launched from here. But I failed to make any further progress.","is continuous, , and is a finite non-empty set. If is continuous and , prove that is a constant function. I think maybe we can prove it by contradiction, assume that isn't a constant function, considering is continuous, it's easy to see that has an infinite number of different values in some . Note that has finite fixed points, perhaps contradictions can be launched from here. But I failed to make any further progress.","\varphi:\textrm{R}\rightarrow\textrm{R} \lim_{x\rightarrow\infty}\varphi(x)-x=\infty \{x\in\textrm{R}|\varphi(x)=x\} f:\textrm{R}\rightarrow\textrm{R} f\circ\varphi=f f f f f [a,b](a,b\in\textrm{R}) \varphi",['analysis']
51,Spivak's Calculus: Chapter 13 Question 21,Spivak's Calculus: Chapter 13 Question 21,,"I'm having a bit of trouble with the part b of the following question from Spivak's Calculus: In particular, I'm not certain about what assumptions regarding the function $f^{-1}$ and $f$ , I'm allowed to make, based on the information of the question. Can I assume that $f^{-1}$ is defined everywhere on [a, b], and bounded, and that f is integrable? And if not, what can I assume about $f^{-1}$ and $f$ ? Thanks in advance! Could you also avoid giving hints on how to actually solve the question, as I would still like to attempt it myself.","I'm having a bit of trouble with the part b of the following question from Spivak's Calculus: In particular, I'm not certain about what assumptions regarding the function and , I'm allowed to make, based on the information of the question. Can I assume that is defined everywhere on [a, b], and bounded, and that f is integrable? And if not, what can I assume about and ? Thanks in advance! Could you also avoid giving hints on how to actually solve the question, as I would still like to attempt it myself.",f^{-1} f f^{-1} f^{-1} f,"['calculus', 'analysis', 'definite-integrals']"
52,About $\cos(\sqrt{-x})$,About,\cos(\sqrt{-x}),"By Euler's Formula $e^{ix}=\cos{x}+i\sin{x}$ we can deduce that: $\cos{\sqrt{-x}}=\cosh {\sqrt{x}}$ My question is the following true: $\cos{\sqrt{-x}}=\begin {cases}   \cos{\sqrt{-x}} &  ,x \text{ is negative real number} \\  \cosh {\sqrt{x}} &  ,x \text{ is positive real number} \end{cases}$ and it is differentiable and continuous at zero. If this is true...is it useful? Read the following to know my level in mathematics: I am second year student of mathematics I know calculus 1+2+3 ,ODES,logic and writing proofs . at my current semester I am studying Abstract Algebra 01 ,Elementary Number Theory ,Introduction to Real Analysis ,PDES 01 and Linear Algebra 01. This question comes to my mind because I love mathematics and I am curious about this idea about $\cos{\sqrt{-x}}$ whether it is true or false ,whether it is useful or useless.","By Euler's Formula we can deduce that: My question is the following true: and it is differentiable and continuous at zero. If this is true...is it useful? Read the following to know my level in mathematics: I am second year student of mathematics I know calculus 1+2+3 ,ODES,logic and writing proofs . at my current semester I am studying Abstract Algebra 01 ,Elementary Number Theory ,Introduction to Real Analysis ,PDES 01 and Linear Algebra 01. This question comes to my mind because I love mathematics and I am curious about this idea about whether it is true or false ,whether it is useful or useless.","e^{ix}=\cos{x}+i\sin{x} \cos{\sqrt{-x}}=\cosh {\sqrt{x}} \cos{\sqrt{-x}}=\begin {cases}  
\cos{\sqrt{-x}} &  ,x \text{ is negative real number} \\ 
\cosh {\sqrt{x}} &  ,x \text{ is positive real number}
\end{cases} \cos{\sqrt{-x}}","['calculus', 'analysis']"
53,Measurable functions : $f(A) \in \mathcal{B}$,Measurable functions :,f(A) \in \mathcal{B},"I am new to measure theory and here is the definition I have : (1) A function $f:(X, \mathcal{A}) \to (Y, \mathcal{B})$ is mesurable iff   : $\forall B \in \mathcal{B}, f^{-1}(B) \in \mathcal{A}$ Why this definition and not this one ? (2) A function $f:(X, \mathcal{A}) \to (Y, \mathcal{B})$ is mesurable   iff : $\forall A \in \mathcal{A}, f(A) \in \mathcal{B}$ Thus with (2) a function is measurable iff it maps measurable sets to measurable sets. It's seems more natural to me.  I know that the first definition extend the notion of continuity, but this explanation still doesn't convince me that (1) should be the most natural definition. So are functions that respect (2) have a name ? And why (2) is not the definition of measurable functions ?","I am new to measure theory and here is the definition I have : (1) A function is mesurable iff   : Why this definition and not this one ? (2) A function is mesurable   iff : Thus with (2) a function is measurable iff it maps measurable sets to measurable sets. It's seems more natural to me.  I know that the first definition extend the notion of continuity, but this explanation still doesn't convince me that (1) should be the most natural definition. So are functions that respect (2) have a name ? And why (2) is not the definition of measurable functions ?","f:(X, \mathcal{A}) \to (Y, \mathcal{B}) \forall B \in \mathcal{B}, f^{-1}(B) \in \mathcal{A} f:(X, \mathcal{A}) \to (Y, \mathcal{B}) \forall A \in \mathcal{A}, f(A) \in \mathcal{B}","['real-analysis', 'analysis', 'measure-theory', 'measurable-functions', 'measurable-sets']"
54,Integrate $\int_0^{\int_0^{\vdots}\frac{1}{\sqrt{x}}\text{d}x}\frac{1}{\sqrt{x}}\text{d}x$ and monotonicity of integrals,Integrate  and monotonicity of integrals,\int_0^{\int_0^{\vdots}\frac{1}{\sqrt{x}}\text{d}x}\frac{1}{\sqrt{x}}\text{d}x,"I must evaluate $$\int_0^{\int_0^{\vdots}\frac{1}{\sqrt{x}}\text{d}x}\frac{1}{\sqrt{x}}\text{d}x$$ My idea is that if we set $$L:=\int_0^{\vdots}\frac{1}{\sqrt{x}}\text{d}x$$ Then the integral must satisfy the equation $$\int_0^L \frac{1}{\sqrt{x}}\text{d}x=L$$ And we have that $$\int_0^L \frac{1}{\sqrt{x}}\text{d}x=\lim_{\varepsilon \to 0^+}\int_\varepsilon^L\frac{1}{\sqrt{x}}\text{d}x=2\sqrt{L}$$ So we have the equation $2\sqrt{L}=L$ , that leads to the solutions $L_1=0$ and $L_2=4$ ; now I suspect that $$\int_0^{\int_0^{\vdots}\frac{1}{\sqrt{x}}\text{d}x}\frac{1}{\sqrt{x}}\text{d}x > 0$$ So the only choice left if $L_2=4$ , but I don't actually know how to prove it rigorously; I'm sure that the last integral is $\geq0$ because $\sqrt{x}\geq0$ , but maybe it is $>0$ because the square root can't be $0$ being at the denominator. Two questions: 1) is my argument right? I'm not sure if it is rigorous, especially when I ""substitute"" the upper bound with $L$ ; maybe I can approach it with sequences. 2) In this case the square root was at the denominator so somehow I've excluded the fact that the integrand could be $\geq0$ (if my argument is correct), but in general how can I prove that if $f(x)>0$ then $\int_a^b f(x) \text{d}x >0$ and not $\int_a^b f(x)\text{d}x \geq 0$ (if this is true)? Thanks.","I must evaluate My idea is that if we set Then the integral must satisfy the equation And we have that So we have the equation , that leads to the solutions and ; now I suspect that So the only choice left if , but I don't actually know how to prove it rigorously; I'm sure that the last integral is because , but maybe it is because the square root can't be being at the denominator. Two questions: 1) is my argument right? I'm not sure if it is rigorous, especially when I ""substitute"" the upper bound with ; maybe I can approach it with sequences. 2) In this case the square root was at the denominator so somehow I've excluded the fact that the integrand could be (if my argument is correct), but in general how can I prove that if then and not (if this is true)? Thanks.",\int_0^{\int_0^{\vdots}\frac{1}{\sqrt{x}}\text{d}x}\frac{1}{\sqrt{x}}\text{d}x L:=\int_0^{\vdots}\frac{1}{\sqrt{x}}\text{d}x \int_0^L \frac{1}{\sqrt{x}}\text{d}x=L \int_0^L \frac{1}{\sqrt{x}}\text{d}x=\lim_{\varepsilon \to 0^+}\int_\varepsilon^L\frac{1}{\sqrt{x}}\text{d}x=2\sqrt{L} 2\sqrt{L}=L L_1=0 L_2=4 \int_0^{\int_0^{\vdots}\frac{1}{\sqrt{x}}\text{d}x}\frac{1}{\sqrt{x}}\text{d}x > 0 L_2=4 \geq0 \sqrt{x}\geq0 >0 0 L \geq0 f(x)>0 \int_a^b f(x) \text{d}x >0 \int_a^b f(x)\text{d}x \geq 0,"['analysis', 'definite-integrals']"
55,"Suppose $f\in L^1(\mathbb R)$ satisfies $\int_G f(x)\,dx=\int_{\bar{G}}f(x)\,dx\ \ \text{for all open set } G\subset\mathbb R$, then $f=0$ a.e.","Suppose  satisfies , then  a.e.","f\in L^1(\mathbb R) \int_G f(x)\,dx=\int_{\bar{G}}f(x)\,dx\ \ \text{for all open set } G\subset\mathbb R f=0","Problem: Suppose $f\in L^1(\mathbb R)$ satisfies $$\int_G f(x) dx=\int_{\bar{G}}f(x) dx\ \ \text{for all open sets } G\subset\mathbb R.$$ Show that $f(x)=0$ for almost all $x\in\mathbb R$ . My attempt: For any open set $G\subset\mathbb R$ , we can write $G=\bigcup_{k=1}^\infty (a_k,b_k)$ , where $a_1<b_1\leq a_2<b_2\cdots$ . The condition in the problem implies that $\int_{\cup_k\{a_k,b_k\}}f=0$ , right? But it seems right for all $f\in L^1(\mathbb R)$ since $m(\cup_k\{a_k,b_k\})=0$ . So I'm confused. What did I miss? Any help will be appreciated.","Problem: Suppose satisfies Show that for almost all . My attempt: For any open set , we can write , where . The condition in the problem implies that , right? But it seems right for all since . So I'm confused. What did I miss? Any help will be appreciated.","f\in L^1(\mathbb R) \int_G f(x) dx=\int_{\bar{G}}f(x) dx\ \ \text{for all open sets } G\subset\mathbb R. f(x)=0 x\in\mathbb R G\subset\mathbb R G=\bigcup_{k=1}^\infty (a_k,b_k) a_1<b_1\leq a_2<b_2\cdots \int_{\cup_k\{a_k,b_k\}}f=0 f\in L^1(\mathbb R) m(\cup_k\{a_k,b_k\})=0","['real-analysis', 'analysis', 'measure-theory']"
56,Let $F \subset \mathbb{R}^n$ be a nonempty countable or finite closed set. Prove that $F$ must have an isolated point.,Let  be a nonempty countable or finite closed set. Prove that  must have an isolated point.,F \subset \mathbb{R}^n F,"I tried to think of the problem using contradiction that if $F$ does not have any isolated point (that is, $F$ contains only limit points) then $F$ is uncountable. But then I was stuck into how to prove that $F$ is uncountable. Can anyone provide a method to proving this proposition?","I tried to think of the problem using contradiction that if does not have any isolated point (that is, contains only limit points) then is uncountable. But then I was stuck into how to prove that is uncountable. Can anyone provide a method to proving this proposition?",F F F F,"['real-analysis', 'analysis']"
57,Prove a norm inequality by Lagrange multipliers,Prove a norm inequality by Lagrange multipliers,,I would like to prove that $\left \| x \right \|_1 \le \sqrt{n} \left \| x \right \|_2$ for all $x \in \mathbb{R}^n$ using Lagrangian multipliers. Thank you all for your help!,I would like to prove that $\left \| x \right \|_1 \le \sqrt{n} \left \| x \right \|_2$ for all $x \in \mathbb{R}^n$ using Lagrangian multipliers. Thank you all for your help!,,"['real-analysis', 'analysis', 'inequality', 'normed-spaces', 'lagrange-multiplier']"
58,What is stopping criteria for Newtons Method?,What is stopping criteria for Newtons Method?,,"Use newtons method to find solutions accurate to within $10^{-4}$ for the following: $$\\x^3-2x^2-5=0,\qquad[1,4]$$ Using : $p_{0}=2.0$ $\Rightarrow $ My question for the newtons method is what is the stopping criteria for it? How does one derive the function value? Does one use $$\\ \frac{|P_{n}-P_{n-1}|}{|P_n|}$$ or $$|P_{n}-P_{n-1}|$$","Use newtons method to find solutions accurate to within $10^{-4}$ for the following: $$\\x^3-2x^2-5=0,\qquad[1,4]$$ Using : $p_{0}=2.0$ $\Rightarrow $ My question for the newtons method is what is the stopping criteria for it? How does one derive the function value? Does one use $$\\ \frac{|P_{n}-P_{n-1}|}{|P_n|}$$ or $$|P_{n}-P_{n-1}|$$",,"['analysis', 'numerical-methods', 'newton-series']"
59,Let $(f_n)_n$ a sequence of measurable function. Prove that {$\omega \in \Omega:(f_n(\omega))_n$ converge} is measurable.,Let  a sequence of measurable function. Prove that { converge} is measurable.,(f_n)_n \omega \in \Omega:(f_n(\omega))_n,"Let $(f_n)_n$ a sequence of measurable function.   Prove that  {$\omega \in \Omega:(f_n(\omega))_n$ converges}  is measurable. My idea is write problem as follows: For all $\ n \in \mathbb{N}$, exist $\ k \in \mathbb{N}$ such that for all $\ i,j>k \ $ then $\ |f_i (\omega)- f_j (\omega)|< 1/n$, last part because $\mathbb{R}$ is complete. Now, Let $A_{n,i,j}:=$ {$\omega \in \Omega:|f_i (\omega)- f_j (\omega)|< 1/n$}, then $\cap_{n\in \mathbb{N}} \  \cup_{k\in \mathbb{N}} \cap_{i,j>k}$  $ \ A_{n,i,j}$ ... Thanks for any help!","Let $(f_n)_n$ a sequence of measurable function.   Prove that  {$\omega \in \Omega:(f_n(\omega))_n$ converges}  is measurable. My idea is write problem as follows: For all $\ n \in \mathbb{N}$, exist $\ k \in \mathbb{N}$ such that for all $\ i,j>k \ $ then $\ |f_i (\omega)- f_j (\omega)|< 1/n$, last part because $\mathbb{R}$ is complete. Now, Let $A_{n,i,j}:=$ {$\omega \in \Omega:|f_i (\omega)- f_j (\omega)|< 1/n$}, then $\cap_{n\in \mathbb{N}} \  \cup_{k\in \mathbb{N}} \cap_{i,j>k}$  $ \ A_{n,i,j}$ ... Thanks for any help!",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
60,A demonstration of Lagrange's Form for the Remainder of a Taylor Series,A demonstration of Lagrange's Form for the Remainder of a Taylor Series,,"The following argument for Lagrange's Form for the Remainder of a Taylor polynomial is a typical one in analysis books.  Even in the case of finding the remainder when the Taylor polynomial is a linear polynomial, deciding on the functions $g(x)$ and $h(x)$ is not apparent.  (See the following argument.)  Can someone provide the motivation for these functions?  Also, does anyone know who concocted this argument? Lagrange's Form for the Remainder $f$ is a twice differentiable function defined on an interval $I$, and $a$ is an element in $I$ distinct from any endpoints of $I$. For every real number $x \in I$ distinct from $a$, there is a real number $c$ between $a$ and $x$ such that \begin{equation*} R_{1}(x) = \frac{f^{(2)}(c)}{2!} \, (x - a)^{2} . \end{equation*} So, if $T_{1}$ denotes the linear Taylor polynomial of $f$ at $a$, \begin{equation*} f(x) = T_{1}(x) + \frac{f^{(2)}(c)}{2!} \, (x - a)^{2} . \end{equation*} Demonstration \begin{equation*} g(u) = f(x) - \Bigl[f(u) + f^{\prime}(u)(x - u)\Bigr] \end{equation*} is a differentiable, and \begin{equation*} g^{\prime}(u) = - f^{\prime\prime}(u)(x - u) . \end{equation*} The function \begin{equation*} h(u) = g(u) - g(a) \left(\frac{1}{x - a}\right)^{2} (x - u)^{2} \end{equation*} is a also differentiable, and \begin{equation*} h^{\prime}(u) = - f^{\prime\prime}(u)(x - u) + 2g(a) \left(\frac{1}{x - a}\right)^{2} (x - u) . \end{equation*} Moreover, $h(x) = g(x) = 0$, and $h(a) = 0$. According to Rolle's Theorem, there is a real number $a < c < x$ such that $h^{\prime}(c) = 0$. \begin{equation*} 0 = h^{\prime}(c) = - f^{\prime\prime}(c)(x - c) + 2g(a) \left(\frac{1}{x - a}\right)^{2} (x - c) , \end{equation*} or equivalently, since $x \neq c$ and since \begin{equation*} g(a) = f(x) - \Bigl[f(a) + f^{\prime}(a)(x - a)\Bigr] , \end{equation*} \begin{equation*} f(x) - \Bigl[f(a) + f^{\prime}(a)(x - a)\Bigr] = \frac{f^{\prime\prime}(c)}{2!} \, (x - a)^{2} . \end{equation*}","The following argument for Lagrange's Form for the Remainder of a Taylor polynomial is a typical one in analysis books.  Even in the case of finding the remainder when the Taylor polynomial is a linear polynomial, deciding on the functions $g(x)$ and $h(x)$ is not apparent.  (See the following argument.)  Can someone provide the motivation for these functions?  Also, does anyone know who concocted this argument? Lagrange's Form for the Remainder $f$ is a twice differentiable function defined on an interval $I$, and $a$ is an element in $I$ distinct from any endpoints of $I$. For every real number $x \in I$ distinct from $a$, there is a real number $c$ between $a$ and $x$ such that \begin{equation*} R_{1}(x) = \frac{f^{(2)}(c)}{2!} \, (x - a)^{2} . \end{equation*} So, if $T_{1}$ denotes the linear Taylor polynomial of $f$ at $a$, \begin{equation*} f(x) = T_{1}(x) + \frac{f^{(2)}(c)}{2!} \, (x - a)^{2} . \end{equation*} Demonstration \begin{equation*} g(u) = f(x) - \Bigl[f(u) + f^{\prime}(u)(x - u)\Bigr] \end{equation*} is a differentiable, and \begin{equation*} g^{\prime}(u) = - f^{\prime\prime}(u)(x - u) . \end{equation*} The function \begin{equation*} h(u) = g(u) - g(a) \left(\frac{1}{x - a}\right)^{2} (x - u)^{2} \end{equation*} is a also differentiable, and \begin{equation*} h^{\prime}(u) = - f^{\prime\prime}(u)(x - u) + 2g(a) \left(\frac{1}{x - a}\right)^{2} (x - u) . \end{equation*} Moreover, $h(x) = g(x) = 0$, and $h(a) = 0$. According to Rolle's Theorem, there is a real number $a < c < x$ such that $h^{\prime}(c) = 0$. \begin{equation*} 0 = h^{\prime}(c) = - f^{\prime\prime}(c)(x - c) + 2g(a) \left(\frac{1}{x - a}\right)^{2} (x - c) , \end{equation*} or equivalently, since $x \neq c$ and since \begin{equation*} g(a) = f(x) - \Bigl[f(a) + f^{\prime}(a)(x - a)\Bigr] , \end{equation*} \begin{equation*} f(x) - \Bigl[f(a) + f^{\prime}(a)(x - a)\Bigr] = \frac{f^{\prime\prime}(c)}{2!} \, (x - a)^{2} . \end{equation*}",,"['calculus', 'real-analysis', 'analysis', 'taylor-expansion', 'math-history']"
61,Does continuous extension of a function and its densely defined derivative imply everywhere differentiability?,Does continuous extension of a function and its densely defined derivative imply everywhere differentiability?,,"Let $V \subset \mathbb R^n$ be a closed set, and let $U \subset V$ be open as a subset of $\mathbb R^n$ and dense in $V$. Let $f:V \to \mathbb R$ and $G: V \to \mathbb R^n$ be continuous, with $G = \nabla f$ on $U$. Does it follow that $G = \nabla f$ on the interior of $V$? To put it another way, is $(f,G)$ a Whitney field? The obvious thing to do is the following: let $x \in V \setminus U$. We want to show that $f(x+h) - f(x) - \langle G(x),h \rangle = o(|h|).$ Choose $x_\epsilon$ arbitrarily close to $x$ in $U$, and write $$f(x+h) - f(x) - \langle G(x),h \rangle = f(x+h) - f(x_\epsilon) - \langle G(x_\epsilon),h-x_\epsilon \rangle + f(x_\epsilon) - f(x) $$ $$+ \langle G(x_\epsilon) - G(x), h \rangle - \langle G(x_\epsilon), x - x_\epsilon \rangle.$$ We control the first three terms using the differentiability of $f$ at $x_\epsilon$, the next two using the continuity of $f$ and the fact taht we can take $|x - x_\epsilon|$ as small as we like, and similarly for the last two. Unfortunately, we choose $x_\epsilon$ depending on $h$, and we don't know that $f$ is uniformly differentiable on $U$, so this doesn't quite work.","Let $V \subset \mathbb R^n$ be a closed set, and let $U \subset V$ be open as a subset of $\mathbb R^n$ and dense in $V$. Let $f:V \to \mathbb R$ and $G: V \to \mathbb R^n$ be continuous, with $G = \nabla f$ on $U$. Does it follow that $G = \nabla f$ on the interior of $V$? To put it another way, is $(f,G)$ a Whitney field? The obvious thing to do is the following: let $x \in V \setminus U$. We want to show that $f(x+h) - f(x) - \langle G(x),h \rangle = o(|h|).$ Choose $x_\epsilon$ arbitrarily close to $x$ in $U$, and write $$f(x+h) - f(x) - \langle G(x),h \rangle = f(x+h) - f(x_\epsilon) - \langle G(x_\epsilon),h-x_\epsilon \rangle + f(x_\epsilon) - f(x) $$ $$+ \langle G(x_\epsilon) - G(x), h \rangle - \langle G(x_\epsilon), x - x_\epsilon \rangle.$$ We control the first three terms using the differentiability of $f$ at $x_\epsilon$, the next two using the continuity of $f$ and the fact taht we can take $|x - x_\epsilon|$ as small as we like, and similarly for the last two. Unfortunately, we choose $x_\epsilon$ depending on $h$, and we don't know that $f$ is uniformly differentiable on $U$, so this doesn't quite work.",,"['analysis', 'derivatives']"
62,Partial derivatives bounded implies continuity,Partial derivatives bounded implies continuity,,"Suppose that $f$ is a real-valued function defined in an open set $E \subset \Bbb R^n$, and that the partial derivatives $D_1f, \ldots D_nf$ are bounded in $E$. Prove that $f$ is continuous in $E$. So if $\textbf{x} = (x_1, x_2, x_3, ... , x_n)$ and $\textbf{y} = (y_1, y_2, y_3, ... , y_n)$, then we have to show that for all $\epsilon >0$, there exists a $\delta>0$ such that $d(\textbf{x}, \textbf{y}) < \delta \implies d(f(\textbf{x}), f(\textbf{y})) < \epsilon$. I am reading a solution here and it says we can write $f(x_1 + h_1, x_2 + h_2, x_3 + h_3, ... , x_n + h_n) - f(x_1, x_2, x_3, ... , x_n)$ as: $f(x_1 + h_1, x_2 + h_2, x_3 + h_3, ... , x_n + h_n) - f(x_1, x_2 + h_2, x_3 + h_3, ... , x_n + h_n) + f(x_1, x_2 + h_2, x_3 + h_3, ... , x_n + h_n) - f(x_1, x_2, x_3 + h_3, ... , x_n + h_n) + \ldots + f(x_1, x_2, x_3, ... ,x_n + h_n) - f(x_1, x_2, x_3, ... , x_n)$ and then use the mean value theorem to get: $D_1(x_1 + h_1t_1, x_2 + h_2, x_3 + h_3, ... , x_n + h_n)h_1$ + $D_2(x_1, x_2 + h_2t_2, x_3 + h_3, ... , x_n + h_n)h_2 + \ldots D_n(x_1, x_2, x_3, ... ,x_n + h_nt_n)h_n$ Since each $D_n$ is bounded, take the maximum of these bounds, call it $M$. Then we have that the expression directly above this sentence is $\leq M(|h_1| + |h_2| + \ldots + |h_n|)$, so: $f(x_1 + h_1, x_2 + h_2, x_3 + h_3, ... , x_n + h_n) - f(x_1, x_2, x_3, ... , x_n) \leq M(|h_1| + |h_2| + \ldots + |h_n|)$ Then the proof just stops there and doesn't continue. I don't understand what the $h_n$'s are supposed to represent. Are they real numbers? If so, why are we adding an arbitrary vector $\textbf{h}$ to $\textbf{x}$? After we get $f(x_1 + h_1, x_2 + h_2, x_3 + h_3, ... , x_n + h_n) - f(x_1, x_2, x_3, ... , x_n) \leq M(|h_1| + |h_2| + \ldots + |h_n|)$ , from this how do we show that for all $\epsilon >0$, there exists a $\delta>0$ such that $d(\textbf{x}, \textbf{y}) < \delta \implies d(f(\textbf{x}), f(\textbf{y})) < \epsilon$","Suppose that $f$ is a real-valued function defined in an open set $E \subset \Bbb R^n$, and that the partial derivatives $D_1f, \ldots D_nf$ are bounded in $E$. Prove that $f$ is continuous in $E$. So if $\textbf{x} = (x_1, x_2, x_3, ... , x_n)$ and $\textbf{y} = (y_1, y_2, y_3, ... , y_n)$, then we have to show that for all $\epsilon >0$, there exists a $\delta>0$ such that $d(\textbf{x}, \textbf{y}) < \delta \implies d(f(\textbf{x}), f(\textbf{y})) < \epsilon$. I am reading a solution here and it says we can write $f(x_1 + h_1, x_2 + h_2, x_3 + h_3, ... , x_n + h_n) - f(x_1, x_2, x_3, ... , x_n)$ as: $f(x_1 + h_1, x_2 + h_2, x_3 + h_3, ... , x_n + h_n) - f(x_1, x_2 + h_2, x_3 + h_3, ... , x_n + h_n) + f(x_1, x_2 + h_2, x_3 + h_3, ... , x_n + h_n) - f(x_1, x_2, x_3 + h_3, ... , x_n + h_n) + \ldots + f(x_1, x_2, x_3, ... ,x_n + h_n) - f(x_1, x_2, x_3, ... , x_n)$ and then use the mean value theorem to get: $D_1(x_1 + h_1t_1, x_2 + h_2, x_3 + h_3, ... , x_n + h_n)h_1$ + $D_2(x_1, x_2 + h_2t_2, x_3 + h_3, ... , x_n + h_n)h_2 + \ldots D_n(x_1, x_2, x_3, ... ,x_n + h_nt_n)h_n$ Since each $D_n$ is bounded, take the maximum of these bounds, call it $M$. Then we have that the expression directly above this sentence is $\leq M(|h_1| + |h_2| + \ldots + |h_n|)$, so: $f(x_1 + h_1, x_2 + h_2, x_3 + h_3, ... , x_n + h_n) - f(x_1, x_2, x_3, ... , x_n) \leq M(|h_1| + |h_2| + \ldots + |h_n|)$ Then the proof just stops there and doesn't continue. I don't understand what the $h_n$'s are supposed to represent. Are they real numbers? If so, why are we adding an arbitrary vector $\textbf{h}$ to $\textbf{x}$? After we get $f(x_1 + h_1, x_2 + h_2, x_3 + h_3, ... , x_n + h_n) - f(x_1, x_2, x_3, ... , x_n) \leq M(|h_1| + |h_2| + \ldots + |h_n|)$ , from this how do we show that for all $\epsilon >0$, there exists a $\delta>0$ such that $d(\textbf{x}, \textbf{y}) < \delta \implies d(f(\textbf{x}), f(\textbf{y})) < \epsilon$",,"['real-analysis', 'analysis', 'continuity', 'partial-derivative', 'lipschitz-functions']"
63,A function that satisfies the Intermediate Value Theorem and takes each value only finitely many times is continuous.,A function that satisfies the Intermediate Value Theorem and takes each value only finitely many times is continuous.,,"I'm having a confusion over the veracity of the statement that a function that satisfies the Intermediate Value Theorem and takes each value only finitely many times is continuous. I've seen from a problem in Spivak's Calculus that this statement is true. Proof: For the case where $f$ takes on each value only once: If $f$ is not continuous, there exists $\epsilon gt 0$ such that there are $x$'s arbitrarily close to $a$ with $f(x)\gt f(a)+\epsilon$, or $f(x)\lt f(a)-\epsilon$. Say the first, then there are $x$'s arbitrarily close to $a$ with $x\gt a$ or $x\lt a$, with $f(x)\gt f(a)+\epsilon$. Say the first. Then by IVT, there is $x'\in (a,x)$ with $f(x')\lt f(a)+\epsilon.$ Also by assumption, there is a $y\in (a,x')$ with $f(y)\gt f(a)+\epsilon$. So by IVT, we can find a $x_1\in (y,x'),$ and $x_2\in (x',x)$ with $f(x_1)=f(x_2)=f(a)+\epsilon$, which is a contradiction. The proof for the case where $f$ takes on each value only finitely many times proceeds by inductive reasoning to the above proof. However, I just came across a problem that states that if $h:[0,1]\to R$ takes each value exactly twice, then $h$ cannot be continuous on $[0,1]$. The proof to this problem contradicts the above statement. Proof: Suppose that $h$ is continuous. (1) By the Min-Max Theorem, $\exists c_1\in [0,1]$, which attains the maximum value of the function. Since each value is taken exactly twice, there is another $c_2\in [0,1]$, say $c_1\lt c_2$ such that $h(c_1)=h(c_2)$. Now if $0\lt c_1$, then we can choose $a_1,a_2\in (0,1)$ and a real number $k$ such that $0\lt a_1\lt c_1\lt a_2\lt c_2$ and $h(a_1)\lt k \lt h(c_1), h(a_2)\lt k \lt h(c_2)$. Then since $h(c_1)=h(c_2)$ are maximum and $h$ is continuous, by IVT there are $b_i\in (0,1)$ such that $h(b_i)=k$ and $a_1\lt b_1\lt c_1\lt b_2\lt a_2\lt b_3\lt c_2$. This is a contradiction. Hence $c_1=0.$ Likewise $c_2=1$. (2) Now by the same reasoning as above, we can show that there are $d_1, d_2$ for which $h$ attains the minimum and $d_1=0, d_2=1$. Hence by (1) and (2) h is a constant function, which is a contradiction to the assumption. Now I don't see any errors in the reasoning of either proof, but the conclusions seem contradictory. How can I reconcile this situation?","I'm having a confusion over the veracity of the statement that a function that satisfies the Intermediate Value Theorem and takes each value only finitely many times is continuous. I've seen from a problem in Spivak's Calculus that this statement is true. Proof: For the case where $f$ takes on each value only once: If $f$ is not continuous, there exists $\epsilon gt 0$ such that there are $x$'s arbitrarily close to $a$ with $f(x)\gt f(a)+\epsilon$, or $f(x)\lt f(a)-\epsilon$. Say the first, then there are $x$'s arbitrarily close to $a$ with $x\gt a$ or $x\lt a$, with $f(x)\gt f(a)+\epsilon$. Say the first. Then by IVT, there is $x'\in (a,x)$ with $f(x')\lt f(a)+\epsilon.$ Also by assumption, there is a $y\in (a,x')$ with $f(y)\gt f(a)+\epsilon$. So by IVT, we can find a $x_1\in (y,x'),$ and $x_2\in (x',x)$ with $f(x_1)=f(x_2)=f(a)+\epsilon$, which is a contradiction. The proof for the case where $f$ takes on each value only finitely many times proceeds by inductive reasoning to the above proof. However, I just came across a problem that states that if $h:[0,1]\to R$ takes each value exactly twice, then $h$ cannot be continuous on $[0,1]$. The proof to this problem contradicts the above statement. Proof: Suppose that $h$ is continuous. (1) By the Min-Max Theorem, $\exists c_1\in [0,1]$, which attains the maximum value of the function. Since each value is taken exactly twice, there is another $c_2\in [0,1]$, say $c_1\lt c_2$ such that $h(c_1)=h(c_2)$. Now if $0\lt c_1$, then we can choose $a_1,a_2\in (0,1)$ and a real number $k$ such that $0\lt a_1\lt c_1\lt a_2\lt c_2$ and $h(a_1)\lt k \lt h(c_1), h(a_2)\lt k \lt h(c_2)$. Then since $h(c_1)=h(c_2)$ are maximum and $h$ is continuous, by IVT there are $b_i\in (0,1)$ such that $h(b_i)=k$ and $a_1\lt b_1\lt c_1\lt b_2\lt a_2\lt b_3\lt c_2$. This is a contradiction. Hence $c_1=0.$ Likewise $c_2=1$. (2) Now by the same reasoning as above, we can show that there are $d_1, d_2$ for which $h$ attains the minimum and $d_1=0, d_2=1$. Hence by (1) and (2) h is a constant function, which is a contradiction to the assumption. Now I don't see any errors in the reasoning of either proof, but the conclusions seem contradictory. How can I reconcile this situation?",,"['calculus', 'real-analysis', 'analysis', 'continuity']"
64,High Dimensional Rotation Matrices As Product of In-Plane Rotations,High Dimensional Rotation Matrices As Product of In-Plane Rotations,,"Lately I've been thinking a lot about how to find high-dimensional rotation matrices. In particular, can any rotation in $n$-dimensional space be represented as the product of $2$D plane rotations? I'm having a tough time finding this online. For example, in $4$D, could we just take rotations in the planes $XY$, $XZ$, $YZ$, $XW$, $YW$, $ZW$ and multiply them together? Obviously rotation matrix multiplication is noncommutative, so will any ordering of the multiplication result in an expression that could be used to represent any high-dimentional rotation? Thanks!","Lately I've been thinking a lot about how to find high-dimensional rotation matrices. In particular, can any rotation in $n$-dimensional space be represented as the product of $2$D plane rotations? I'm having a tough time finding this online. For example, in $4$D, could we just take rotations in the planes $XY$, $XZ$, $YZ$, $XW$, $YW$, $ZW$ and multiply them together? Obviously rotation matrix multiplication is noncommutative, so will any ordering of the multiplication result in an expression that could be used to represent any high-dimentional rotation? Thanks!",,"['analysis', 'lie-groups', 'lie-algebras', 'rotations', 'machine-learning']"
65,Proving the limit at $\infty$ of the derivative $f'$ is $0$ if it and the limit of the function $f$ exist. [duplicate],Proving the limit at  of the derivative  is  if it and the limit of the function  exist. [duplicate],\infty f' 0 f,"This question already has answers here : Proving that $\lim\limits_{x\to\infty}f'(x) = 0$ when $\lim\limits_{x\to\infty}f(x)$ and $\lim\limits_{x\to\infty}f'(x)$ exist (6 answers) If a function has a finite limit at infinity, does that imply its derivative goes to zero? (6 answers) Closed 8 years ago . Suppose that $f$ is differentiable for all $x$, and that $\lim_{x\to \infty} f(x)$ exists. Prove that if $\lim_{x\to \infty} f′(x)$ exists, then $\lim_{x\to \infty} f′(x) = 0$, and also, give an example where $\lim_{x\to \infty} f′(x)$ does not exist. I'm at a loss as to how to prove the first part, but for the second part, would a function such as $\sin(x)$ satisfy the problem?","This question already has answers here : Proving that $\lim\limits_{x\to\infty}f'(x) = 0$ when $\lim\limits_{x\to\infty}f(x)$ and $\lim\limits_{x\to\infty}f'(x)$ exist (6 answers) If a function has a finite limit at infinity, does that imply its derivative goes to zero? (6 answers) Closed 8 years ago . Suppose that $f$ is differentiable for all $x$, and that $\lim_{x\to \infty} f(x)$ exists. Prove that if $\lim_{x\to \infty} f′(x)$ exists, then $\lim_{x\to \infty} f′(x) = 0$, and also, give an example where $\lim_{x\to \infty} f′(x)$ does not exist. I'm at a loss as to how to prove the first part, but for the second part, would a function such as $\sin(x)$ satisfy the problem?",,"['real-analysis', 'analysis']"
66,Products of distributions ill-defined,Products of distributions ill-defined,,"This question concerns distributions, as often encountered in PDE theory, which are defined as continuous linear functionals on the space $C_0^{\infty}(\Omega)$ of test functions. The product of two distributions, and the convolution of two distributions, is not defined in general. Are there some easy/well-known counterexamples which show why these operations cannot make sense?","This question concerns distributions, as often encountered in PDE theory, which are defined as continuous linear functionals on the space $C_0^{\infty}(\Omega)$ of test functions. The product of two distributions, and the convolution of two distributions, is not defined in general. Are there some easy/well-known counterexamples which show why these operations cannot make sense?",,"['real-analysis', 'analysis', 'partial-differential-equations', 'distribution-theory']"
67,Proof: $C(X×Y)=C(X)⊗C(Y)$,Proof:,C(X×Y)=C(X)⊗C(Y),"Where I can find the proof of the following theorem: Let $X$ and $Y$ be compact Hausdorff spaces, $C(X)$ and $C(Y)$ the space of continuous functions on $X$ and $Y$ respectively, then we have $C(X×Y)=C(X)⊗C(Y)$.","Where I can find the proof of the following theorem: Let $X$ and $Y$ be compact Hausdorff spaces, $C(X)$ and $C(Y)$ the space of continuous functions on $X$ and $Y$ respectively, then we have $C(X×Y)=C(X)⊗C(Y)$.",,"['analysis', 'operator-algebras']"
68,"Rigorous proof of the ""Lagrange-multiplier theorem""","Rigorous proof of the ""Lagrange-multiplier theorem""",,"From Marsden's Elementary Classical Analysis: Theorem 8 Let $f\colon U \subset \Bbb R^n \to \Bbb R$ and $g\colon U\subset \Bbb R^n \to R$ be given $C^1$ functions. Let $x_0\in U$, $g(x_0)=c_0$ and let $S = g^{-1}(c_0)$ the level set for $g$ with value $c_0$. Assume $\nabla g(x_0)\ne 0$. If $f\restriction S$ has a maximum or minimum at $x$ then there is a real number $\lambda$ such that $$\nabla f(x_0)=\lambda \nabla g(x_0)$$ Proof The only thing not complete about the sketch of the proof given in Section 7.7 is that we need to know that if $v\perp \nabla g(x_0)$ then $v=c'(0)$ for a $C^1$ curve $c(t)$ in $S$, with $c(0) = x_0$. This can be established as follows. By Theorem 3 there is a change of coordinates $h$ such that $g(h(x_1,\dots,x_n)) = x_n$. Thus $h^{-1} (S)$ is the coordinate plane $x_n = c_0$. Let $w=Dh^{-1}(x_0)\cdot v$. We claim that the last coordinate of $w$ is zero, that is, $w$ lies in the plane $x_n = c_0$. Indeed let $e_n = (0,0,\dots,1)$. We shall show that $\langle w, e_n \rangle = 0$. But from the chain rule, $g(h(x_1,\dots,x_n))=x_n$ implies $$\langle\nabla g(x_0),Dh(y_0) \cdot w\rangle=\langle w,e_n\rangle$$ where $h(y_0)=x_0$. But the left side is $\langle\nabla g(x_0),v\rangle=0$. Now let $c(t)=h(y_0+tw)$. This lies in $S$, $c(0)=x_0$, and from the chain rule, $c'(0)=v$. The proof may now be completed as in the text. $\blacksquare$ Two points that I'm having trouble with: (1) In the seventh line from the last, why does $w$'s last coordinate being zero imply that $w$ lies in the plane $x_n=c_0$? I only naively (erroneously) see that the former implies that $w$ lies in the plane $x_n=0$. (2) How is the chain rule being used to obtain the equation in the fourth line from the last?","From Marsden's Elementary Classical Analysis: Theorem 8 Let $f\colon U \subset \Bbb R^n \to \Bbb R$ and $g\colon U\subset \Bbb R^n \to R$ be given $C^1$ functions. Let $x_0\in U$, $g(x_0)=c_0$ and let $S = g^{-1}(c_0)$ the level set for $g$ with value $c_0$. Assume $\nabla g(x_0)\ne 0$. If $f\restriction S$ has a maximum or minimum at $x$ then there is a real number $\lambda$ such that $$\nabla f(x_0)=\lambda \nabla g(x_0)$$ Proof The only thing not complete about the sketch of the proof given in Section 7.7 is that we need to know that if $v\perp \nabla g(x_0)$ then $v=c'(0)$ for a $C^1$ curve $c(t)$ in $S$, with $c(0) = x_0$. This can be established as follows. By Theorem 3 there is a change of coordinates $h$ such that $g(h(x_1,\dots,x_n)) = x_n$. Thus $h^{-1} (S)$ is the coordinate plane $x_n = c_0$. Let $w=Dh^{-1}(x_0)\cdot v$. We claim that the last coordinate of $w$ is zero, that is, $w$ lies in the plane $x_n = c_0$. Indeed let $e_n = (0,0,\dots,1)$. We shall show that $\langle w, e_n \rangle = 0$. But from the chain rule, $g(h(x_1,\dots,x_n))=x_n$ implies $$\langle\nabla g(x_0),Dh(y_0) \cdot w\rangle=\langle w,e_n\rangle$$ where $h(y_0)=x_0$. But the left side is $\langle\nabla g(x_0),v\rangle=0$. Now let $c(t)=h(y_0+tw)$. This lies in $S$, $c(0)=x_0$, and from the chain rule, $c'(0)=v$. The proof may now be completed as in the text. $\blacksquare$ Two points that I'm having trouble with: (1) In the seventh line from the last, why does $w$'s last coordinate being zero imply that $w$ lies in the plane $x_n=c_0$? I only naively (erroneously) see that the former implies that $w$ lies in the plane $x_n=0$. (2) How is the chain rule being used to obtain the equation in the fourth line from the last?",,"['real-analysis', 'analysis', 'multivariable-calculus', 'optimization']"
69,find limit of $a_n=\frac{1}{(n+1)^2}+\frac{1}{(n+2)^2}+...+\frac{1}{(2n)^2}$,find limit of,a_n=\frac{1}{(n+1)^2}+\frac{1}{(n+2)^2}+...+\frac{1}{(2n)^2},"finding limit of $a_n=\frac{1}{(n+1)^2}+\frac{1}{(n+2)^2}+...+\frac{1}{(2n)^2}$ I know that i have to use Stolz Cesaro theorem, but the problem is that i need second sequence.","finding limit of $a_n=\frac{1}{(n+1)^2}+\frac{1}{(n+2)^2}+...+\frac{1}{(2n)^2}$ I know that i have to use Stolz Cesaro theorem, but the problem is that i need second sequence.",,"['calculus', 'analysis']"
70,Convergence of alternating nested radicals,Convergence of alternating nested radicals,,"Last evening, after reading a couple of questions about nested radicals, I started to wonder about problems involving what I will term ""alternating nested radicals;"" below is an example, which I found here . Prove the convergence of, and evaluate, $\sqrt{7 -\sqrt{7 + \sqrt{7...}}}$ It turns out that this nested radical converges to $2$, and this is not especially hard to argue on an ad-hoc basis. However, I became interested in the general solution of the problem of evaluating $\sqrt{q -\sqrt{q + \sqrt{q...}}} \text{ }\text{ }$ for an arbitrary positive real $q$. Despite some searching, I was not able to find a paper which stated a theorem on this, so I resorted to working it out for myself. I have developed an argument which I believe gives the correct result for all $q > 1.$ I have checked its predictions against several alternating nested radicals, and they agree with computation. Theorem: $\sqrt{q -\sqrt{q + \sqrt{q...}}} = \frac{\sqrt{1 + 4(q-1)}-1}{2}$ Argument: We make two significant assumptions: that $\sqrt{q -\sqrt{q + \sqrt{q...}}} \text{ }\text{ }$ converges; and, inspired by the fact that $\sqrt{q +\sqrt{q + \sqrt{q...}}} \text{ }\text{ } = x$ satisfies $x^2 - x - q = 0$, we assume that $\sqrt{q -\sqrt{q + \sqrt{q...}}} \text{ }\text{ } = x$ satisfies $x^2 + x - a$ for some $a$. By the self-similarity of the alternating nested radical, we find \begin{align}  \sqrt{q - \sqrt{q + x}} &= x \\  q - \sqrt{q + x} &= x^2 \\ &= a-x \\ \therefore q + x - \sqrt{q + x} &= a \end{align} Solving this as a quadratic in $\sqrt{q+x}$ yields $$\sqrt{q+x} = \frac{\sqrt{1 + 4a} + 1}{2}.$$ Again using our second assumption, we have $$x = \frac{\sqrt{1 + 4a} - 1}{2}.$$ But now $$ \sqrt{q+x} - x = \frac{\sqrt{1 + 4a} + 1}{2} - \frac{\sqrt{1 + 4a} - 1}{2} = 1$$ and so we get $$ q+x = (1+x)^2$$ and thus $$ x^2 + x - (q-1) = 0.$$ Solving this quadratic for $x$ then gives the theorem. I am pretty sure that my second assumption above would be impossible to defend; so, basically, I have the following questions: How can we rigorously prove that the alternating nested radical converges? How can we show, for a general $q$, that it converges to the value given in the theorem? And one other thing: any general references to ubiquitous convergence theorems or techniques would be greatly appreciated!","Last evening, after reading a couple of questions about nested radicals, I started to wonder about problems involving what I will term ""alternating nested radicals;"" below is an example, which I found here . Prove the convergence of, and evaluate, $\sqrt{7 -\sqrt{7 + \sqrt{7...}}}$ It turns out that this nested radical converges to $2$, and this is not especially hard to argue on an ad-hoc basis. However, I became interested in the general solution of the problem of evaluating $\sqrt{q -\sqrt{q + \sqrt{q...}}} \text{ }\text{ }$ for an arbitrary positive real $q$. Despite some searching, I was not able to find a paper which stated a theorem on this, so I resorted to working it out for myself. I have developed an argument which I believe gives the correct result for all $q > 1.$ I have checked its predictions against several alternating nested radicals, and they agree with computation. Theorem: $\sqrt{q -\sqrt{q + \sqrt{q...}}} = \frac{\sqrt{1 + 4(q-1)}-1}{2}$ Argument: We make two significant assumptions: that $\sqrt{q -\sqrt{q + \sqrt{q...}}} \text{ }\text{ }$ converges; and, inspired by the fact that $\sqrt{q +\sqrt{q + \sqrt{q...}}} \text{ }\text{ } = x$ satisfies $x^2 - x - q = 0$, we assume that $\sqrt{q -\sqrt{q + \sqrt{q...}}} \text{ }\text{ } = x$ satisfies $x^2 + x - a$ for some $a$. By the self-similarity of the alternating nested radical, we find \begin{align}  \sqrt{q - \sqrt{q + x}} &= x \\  q - \sqrt{q + x} &= x^2 \\ &= a-x \\ \therefore q + x - \sqrt{q + x} &= a \end{align} Solving this as a quadratic in $\sqrt{q+x}$ yields $$\sqrt{q+x} = \frac{\sqrt{1 + 4a} + 1}{2}.$$ Again using our second assumption, we have $$x = \frac{\sqrt{1 + 4a} - 1}{2}.$$ But now $$ \sqrt{q+x} - x = \frac{\sqrt{1 + 4a} + 1}{2} - \frac{\sqrt{1 + 4a} - 1}{2} = 1$$ and so we get $$ q+x = (1+x)^2$$ and thus $$ x^2 + x - (q-1) = 0.$$ Solving this quadratic for $x$ then gives the theorem. I am pretty sure that my second assumption above would be impossible to defend; so, basically, I have the following questions: How can we rigorously prove that the alternating nested radical converges? How can we show, for a general $q$, that it converges to the value given in the theorem? And one other thing: any general references to ubiquitous convergence theorems or techniques would be greatly appreciated!",,"['analysis', 'convergence-divergence', 'intuition', 'nested-radicals']"
71,How prove this mathematical analysis by zorich from page 233,How prove this mathematical analysis by zorich from page 233,,"Let $f$ be twice differentiable on an interval $I$,Let $$M_{0}=\sup_{x\in I}{|f(x)|},M_{1}=\sup_{x\in I}{|f'(x)|},M_{2}=\sup_{x\in I}{|f''(x)|}$$ show that (a):$$M_{1}\le 2\sqrt{M_{0}M_{2}}$$   if the length of $I$ is not less than $2\sqrt{\dfrac{M_{0}}{M_{2}}}$ (b):the numbers $2$ and $\sqrt{2}$ (in part a) cannot be replaced by smaller numbers. My try:for part $(a)$ I can prove if the length of $I$ is not less than $4\sqrt{\dfrac{M_{0}}{M_{2}}}$ my proof: $$f(c)-f(x)=f'(x)(c-x)+\dfrac{f''(x+\theta_{1}(c-x))}{2}(c-x)^2$$   let $c-x=h$   then we have   $$f'(x)=\dfrac{f(c)-f(x)}{h}-\dfrac{f''(x+\theta_{1}h)}{2}h$$   Thus   $$f'(x)\le\dfrac{2M_{0}}{h}+\dfrac{1}{2}M_{2}h$$   Now taking $h=2\sqrt{\dfrac{M_{0}}{M_{2}}}$, which in turn implies that   $$M_{1}\le2\sqrt{M_{0}M_{2}}$$   and $$c=x+h=2\sqrt{\dfrac{M_{0}}{M_{2}}}+x>2\sqrt{\dfrac{M_{0}}{M_{2}}}$$   so  the length of $I$ is $2c$ and  not less than $4\sqrt{\dfrac{M_{0}}{M_{2}}}$ so Now  How part(a)? and for part(b) How prove it? and How take example for the best numbers$\sqrt{2}$? By the way This problem is from Mathematical Analysis I(Zorich )Page 233 problem 9. Thank you evryone","Let $f$ be twice differentiable on an interval $I$,Let $$M_{0}=\sup_{x\in I}{|f(x)|},M_{1}=\sup_{x\in I}{|f'(x)|},M_{2}=\sup_{x\in I}{|f''(x)|}$$ show that (a):$$M_{1}\le 2\sqrt{M_{0}M_{2}}$$   if the length of $I$ is not less than $2\sqrt{\dfrac{M_{0}}{M_{2}}}$ (b):the numbers $2$ and $\sqrt{2}$ (in part a) cannot be replaced by smaller numbers. My try:for part $(a)$ I can prove if the length of $I$ is not less than $4\sqrt{\dfrac{M_{0}}{M_{2}}}$ my proof: $$f(c)-f(x)=f'(x)(c-x)+\dfrac{f''(x+\theta_{1}(c-x))}{2}(c-x)^2$$   let $c-x=h$   then we have   $$f'(x)=\dfrac{f(c)-f(x)}{h}-\dfrac{f''(x+\theta_{1}h)}{2}h$$   Thus   $$f'(x)\le\dfrac{2M_{0}}{h}+\dfrac{1}{2}M_{2}h$$   Now taking $h=2\sqrt{\dfrac{M_{0}}{M_{2}}}$, which in turn implies that   $$M_{1}\le2\sqrt{M_{0}M_{2}}$$   and $$c=x+h=2\sqrt{\dfrac{M_{0}}{M_{2}}}+x>2\sqrt{\dfrac{M_{0}}{M_{2}}}$$   so  the length of $I$ is $2c$ and  not less than $4\sqrt{\dfrac{M_{0}}{M_{2}}}$ so Now  How part(a)? and for part(b) How prove it? and How take example for the best numbers$\sqrt{2}$? By the way This problem is from Mathematical Analysis I(Zorich )Page 233 problem 9. Thank you evryone",,"['analysis', 'inequality']"
72,"$f$ integrable, $g$ measurable, $f = g$ almost everywhere implies $g$ integrable","integrable,  measurable,  almost everywhere implies  integrable",f g f = g g,"If $f\in L(X,\mathcal{x},\mu)$, that is: $f\colon X\to R$ is measurable; $\int f^+\,d\mu<+\infty$ and $\int f^-\,d\mu<+\infty$; $\int f\,d\mu=\int f^+\,d\mu-\int f^-\,d\mu$. If $g\colon X\to R$ is measurable and $f=g$ $\mu$-almost everywhere, I need to show that $\int g^+\,d\mu<+\infty$. Any thoughts? Note: The integral of a non-negative measurable function $h$ is defined as $$\int h\,d\mu=\text{sup}\left\{\int\phi\,d\mu: 0\le\phi\le h\right\},$$ where $\phi$ is a simple measurable function.","If $f\in L(X,\mathcal{x},\mu)$, that is: $f\colon X\to R$ is measurable; $\int f^+\,d\mu<+\infty$ and $\int f^-\,d\mu<+\infty$; $\int f\,d\mu=\int f^+\,d\mu-\int f^-\,d\mu$. If $g\colon X\to R$ is measurable and $f=g$ $\mu$-almost everywhere, I need to show that $\int g^+\,d\mu<+\infty$. Any thoughts? Note: The integral of a non-negative measurable function $h$ is defined as $$\int h\,d\mu=\text{sup}\left\{\int\phi\,d\mu: 0\le\phi\le h\right\},$$ where $\phi$ is a simple measurable function.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral']"
73,I want to study $\sqrt[n]{n}$ and its behavior.,I want to study  and its behavior.,\sqrt[n]{n},"As I was studying some limit problems, I came across $$\sqrt[n]{n}$$ and astoundingly found out that the graph of this has a maximum when $n = e$. I thought there is no way that this is not a famous fact and I am very interested in it. I looked up some words such as ""nth roots"" or ""rational exponents"" but I haven't found this fact right away. Can someone guide me to a link or tell me at least what this expression goes by in order to do a little bit of researching ?","As I was studying some limit problems, I came across $$\sqrt[n]{n}$$ and astoundingly found out that the graph of this has a maximum when $n = e$. I thought there is no way that this is not a famous fact and I am very interested in it. I looked up some words such as ""nth roots"" or ""rational exponents"" but I haven't found this fact right away. Can someone guide me to a link or tell me at least what this expression goes by in order to do a little bit of researching ?",,['analysis']
74,Are Trigonometric Functions Dense in $C^k(S^1)?$,Are Trigonometric Functions Dense in,C^k(S^1)?,"Consider the functions $\{e^{2\pi i nx}\}_{n \in \mathbb{Z}}$ defined on the interval $[0,1].$  These are all smooth periodic functions (so functions on $S^1)$ and by the Stone-Weierstrass theorem they are dense in $C^0(S^1)$ when it is given the max norm. Consider the norm on $C^k(S^1)$ given by  $$|f|^{C^k}=\sum_{i=0}^k \text{max}_{x \in S^1}|f^{(k)}(x)|.$$ $C^k(S^1)$ is complete in this norm, but is the collection $\{e^{2\pi i nx}\}_{n \in \mathbb{Z}}$ a dense subset?","Consider the functions $\{e^{2\pi i nx}\}_{n \in \mathbb{Z}}$ defined on the interval $[0,1].$  These are all smooth periodic functions (so functions on $S^1)$ and by the Stone-Weierstrass theorem they are dense in $C^0(S^1)$ when it is given the max norm. Consider the norm on $C^k(S^1)$ given by  $$|f|^{C^k}=\sum_{i=0}^k \text{max}_{x \in S^1}|f^{(k)}(x)|.$$ $C^k(S^1)$ is complete in this norm, but is the collection $\{e^{2\pi i nx}\}_{n \in \mathbb{Z}}$ a dense subset?",,"['analysis', 'fourier-analysis']"
75,Hölder Continuity between metric spaces,Hölder Continuity between metric spaces,,"Hölder continuity has never appeared in my formal education and the wikipedia article seems insufficiently general.  I want to make sure that this definition of Hölder continuity is correct and standard: Let $f:X \rightarrow Y$ be a map of metric spaces.  Then we say $f$ is Hölder continuous with exponent $r \geq 0$ if there exists $C_r$ such that $d_Y(f(x), f(x'))\leq C_r d_X(x, x')^r$ I am specifically asking this to be sure that there is no reason to require the domain to be all of a Euclidean space, as specified in the Wikipedia article, or even Euclidean at all. (Or is it uninteresting unless you at least have a convex body domain in a Euclidean space or something?)  Furthermore, it is my observation that although a lot of functions seem to satisfy Hölder conditions for all exponents in intervals of the form $(0, r)$ or $[0, r)$ that actually in general one does not have that Hölder of any exponent implies Hölder of any other exponent.  In particular, a Lipschitz function can be not Hölder-1/2.  But all Hölder functions are automatically uniformly continuous.  Please let me know if all these are correct.  I guess being familiar with Lipschitz continuity for a long time I should not be bothered by this, but it disturbs me that Holder continuity can be violated ""just because"" the function has bad asymptotic behavior, whereas I think of uniformly continuous as a local adjective.  Moreover, many mathematicians have, in my opinion, spoken of Hölder continuity as if it's ""harder"" to have it for higher exponents.  Is there some reason for this, or maybe it's just that I'm misreading their intentions?","Hölder continuity has never appeared in my formal education and the wikipedia article seems insufficiently general.  I want to make sure that this definition of Hölder continuity is correct and standard: Let $f:X \rightarrow Y$ be a map of metric spaces.  Then we say $f$ is Hölder continuous with exponent $r \geq 0$ if there exists $C_r$ such that $d_Y(f(x), f(x'))\leq C_r d_X(x, x')^r$ I am specifically asking this to be sure that there is no reason to require the domain to be all of a Euclidean space, as specified in the Wikipedia article, or even Euclidean at all. (Or is it uninteresting unless you at least have a convex body domain in a Euclidean space or something?)  Furthermore, it is my observation that although a lot of functions seem to satisfy Hölder conditions for all exponents in intervals of the form $(0, r)$ or $[0, r)$ that actually in general one does not have that Hölder of any exponent implies Hölder of any other exponent.  In particular, a Lipschitz function can be not Hölder-1/2.  But all Hölder functions are automatically uniformly continuous.  Please let me know if all these are correct.  I guess being familiar with Lipschitz continuity for a long time I should not be bothered by this, but it disturbs me that Holder continuity can be violated ""just because"" the function has bad asymptotic behavior, whereas I think of uniformly continuous as a local adjective.  Moreover, many mathematicians have, in my opinion, spoken of Hölder continuity as if it's ""harder"" to have it for higher exponents.  Is there some reason for this, or maybe it's just that I'm misreading their intentions?",,"['real-analysis', 'analysis', 'holder-spaces']"
76,Mutually Singular measures,Mutually Singular measures,,"c.f. Rudin's Real and Complex Analysis (Third Edition 1987) Chapter 6 Q9 Suppose that $\{g_n\}$ is a sequence of positive continuous functions on $I=[0,1]$, $\mu$ is a positive Borel measure on $I$, $m$ is the standard Lebesgue measure, and that (i) $\lim_{n\to\infty}g_n(x)=0$ a.e. $[m]$ (ii) $\int_Ig_ndm=1$ for all $n$, (iii) $\lim_{n\to\infty}\int_Ifg_ndm=\int_Ifd\mu$ for every $f\in C(I)$. Does it follow that the measures $\mu$ and $m$ are mutually singular? I know that $\mu$ and $m$ are mutually singular if they are concentrated in different disjoint sets, but how do I connect that with the 3 properties above? I will appreciate if someone can help me with the proof or counter example.","c.f. Rudin's Real and Complex Analysis (Third Edition 1987) Chapter 6 Q9 Suppose that $\{g_n\}$ is a sequence of positive continuous functions on $I=[0,1]$, $\mu$ is a positive Borel measure on $I$, $m$ is the standard Lebesgue measure, and that (i) $\lim_{n\to\infty}g_n(x)=0$ a.e. $[m]$ (ii) $\int_Ig_ndm=1$ for all $n$, (iii) $\lim_{n\to\infty}\int_Ifg_ndm=\int_Ifd\mu$ for every $f\in C(I)$. Does it follow that the measures $\mu$ and $m$ are mutually singular? I know that $\mu$ and $m$ are mutually singular if they are concentrated in different disjoint sets, but how do I connect that with the 3 properties above? I will appreciate if someone can help me with the proof or counter example.",,"['real-analysis', 'analysis', 'measure-theory', 'singular-measures']"
77,uniform approximation by smooth functions,uniform approximation by smooth functions,,"Let $(M,g)$ be a closed, compact Riemannian manifold. Let $u \in C^{k}(M)$. Can I always find a sequence of $C^\infty$ functions $\{u_n\}$ such that $u_n$ converges to $u$ in $C^k$ norm?","Let $(M,g)$ be a closed, compact Riemannian manifold. Let $u \in C^{k}(M)$. Can I always find a sequence of $C^\infty$ functions $\{u_n\}$ such that $u_n$ converges to $u$ in $C^k$ norm?",,['analysis']
78,Does there exist a submersion $f:\mathbb{R}^{3}\setminus\{0\} \to \mathbb{R}$ with one pre-image compact and another pre-image non-compact?,Does there exist a submersion  with one pre-image compact and another pre-image non-compact?,f:\mathbb{R}^{3}\setminus\{0\} \to \mathbb{R},Does there exist a submersion $f:\mathbb{R}^{3}\setminus\{0\} \to \mathbb{R}$ for which there are $c_1$ and $c_2$ in $\mathbb{R}$ such that $f^{-1}(c_1)$ is compact and $f^{-1}(c_2)$ is non-compact.,Does there exist a submersion $f:\mathbb{R}^{3}\setminus\{0\} \to \mathbb{R}$ for which there are $c_1$ and $c_2$ in $\mathbb{R}$ such that $f^{-1}(c_1)$ is compact and $f^{-1}(c_2)$ is non-compact.,,"['analysis', 'differential-geometry']"
79,Derivatives of infimum,Derivatives of infimum,,"Let $\zeta: \mathbb{R^m} \times \mathbb{R^n} \mapsto \mathbb{R}$ be a smooth function and define $\phi(x) = \inf_{y \in C} \zeta(x,y)$ where $C \subset \mathbb{R^n}$ is compact. Suppose that for every $x \in X \subset \mathbb{R^m}$ there is a unique $y(x) \in C$ such that $\phi(x) = \zeta(x,y(x))$. Now, according to a book a have, $$\frac{\partial \phi}{\partial x}(x_0) =  \frac{\partial \zeta}{\partial x} (x_0, y(x_0)) \quad \forall x_0 \in X$$ (this is let as a exercise). The problem is: 1) this looks simple but I don't where to start to prove this, 2) even if I knew how to deal with the first part, I also need the second differential (the Hessian) of $\phi$ (given in term of $\zeta$, $y$ and their derivatives). I tried to do something like $$\frac{\partial \phi}{\partial x}(x_0) =  \frac{\partial \zeta}{\partial x} (x_0, y(x_0)) + \frac{\partial \zeta}{\partial y} (x_0, y(x_0)) \frac{\partial y}{\partial x}(x_0)$$ (this suggests that $\frac{\partial y}{\partial x}(x_0) = 0$) and find the derivative of $y$ by the implicit function theorem, but I can only get the  tautology $\frac{\partial \phi}{\partial x}(x_0) =\frac{\partial \phi}{\partial x}(x_0)$, so I suspect this isn't the good way. Note: this question is related to this one ( Infimum is a continuous function, compact set ).","Let $\zeta: \mathbb{R^m} \times \mathbb{R^n} \mapsto \mathbb{R}$ be a smooth function and define $\phi(x) = \inf_{y \in C} \zeta(x,y)$ where $C \subset \mathbb{R^n}$ is compact. Suppose that for every $x \in X \subset \mathbb{R^m}$ there is a unique $y(x) \in C$ such that $\phi(x) = \zeta(x,y(x))$. Now, according to a book a have, $$\frac{\partial \phi}{\partial x}(x_0) =  \frac{\partial \zeta}{\partial x} (x_0, y(x_0)) \quad \forall x_0 \in X$$ (this is let as a exercise). The problem is: 1) this looks simple but I don't where to start to prove this, 2) even if I knew how to deal with the first part, I also need the second differential (the Hessian) of $\phi$ (given in term of $\zeta$, $y$ and their derivatives). I tried to do something like $$\frac{\partial \phi}{\partial x}(x_0) =  \frac{\partial \zeta}{\partial x} (x_0, y(x_0)) + \frac{\partial \zeta}{\partial y} (x_0, y(x_0)) \frac{\partial y}{\partial x}(x_0)$$ (this suggests that $\frac{\partial y}{\partial x}(x_0) = 0$) and find the derivative of $y$ by the implicit function theorem, but I can only get the  tautology $\frac{\partial \phi}{\partial x}(x_0) =\frac{\partial \phi}{\partial x}(x_0)$, so I suspect this isn't the good way. Note: this question is related to this one ( Infimum is a continuous function, compact set ).",,['real-analysis']
80,Applying Mean Value Theorem to formula,Applying Mean Value Theorem to formula,,"As I understand it, the mean value theorem is where $${f}'(c)=\frac{f(b)-f(a)}{b-a}$$ if $f$ is continuous on the open interval (a, b) and differentiable on the closed interval [a,b]. A problem in the current homework set on WebAssign has me confused.  Given $f(x)=x^{7}$ on the closed interval   [0, 1], determine whether the MVT can be applied to the closed interval [a,b]. Since $f(x)= x^{7}$ has a similar profile to a cubic function graph, and it is differentiable to ${f}'(x)= 7x^{6}$, it passes two criteria for the MVT. Now, solving the MVT formula: $${f}'(x)=\frac{f(b)-f(a)}{b-a} \Rightarrow \frac{[1^{7}]-[0^{7}]}{1-0} \Rightarrow \frac{1-0}{1-0} \Rightarrow \frac{1}{1}= 1$$ Now, I need to find a number $c$ between 0 and 1 that f'(c)=1.  However, the only whole number possiblities from the [0, 1] interval produce  $${f}'(0)= 7(0)^{6}= 0 \neq 1$$  $${f}'(1)= 7(1)^{6}= 7 \neq 1$$ Am I missing something here?  The question has two parts: identify whether the MVT is applicable, and find the numbers $c$ that fit the theorem on the interval.  The closest number for $c$ that I've found that works is 0.724, which gives a value of 1.00815, but it doesn't match 1 perfectly.","As I understand it, the mean value theorem is where $${f}'(c)=\frac{f(b)-f(a)}{b-a}$$ if $f$ is continuous on the open interval (a, b) and differentiable on the closed interval [a,b]. A problem in the current homework set on WebAssign has me confused.  Given $f(x)=x^{7}$ on the closed interval   [0, 1], determine whether the MVT can be applied to the closed interval [a,b]. Since $f(x)= x^{7}$ has a similar profile to a cubic function graph, and it is differentiable to ${f}'(x)= 7x^{6}$, it passes two criteria for the MVT. Now, solving the MVT formula: $${f}'(x)=\frac{f(b)-f(a)}{b-a} \Rightarrow \frac{[1^{7}]-[0^{7}]}{1-0} \Rightarrow \frac{1-0}{1-0} \Rightarrow \frac{1}{1}= 1$$ Now, I need to find a number $c$ between 0 and 1 that f'(c)=1.  However, the only whole number possiblities from the [0, 1] interval produce  $${f}'(0)= 7(0)^{6}= 0 \neq 1$$  $${f}'(1)= 7(1)^{6}= 7 \neq 1$$ Am I missing something here?  The question has two parts: identify whether the MVT is applicable, and find the numbers $c$ that fit the theorem on the interval.  The closest number for $c$ that I've found that works is 0.724, which gives a value of 1.00815, but it doesn't match 1 perfectly.",,"['calculus', 'analysis']"
81,Proving the Gamma function is analytic over the positive reals without invoking complex analysis,Proving the Gamma function is analytic over the positive reals without invoking complex analysis,,"The gamma function can be defined in a few different ways, the most well known possibly being Euler's second integral definition $$\Gamma(x) = \int_0^\infty t^{x-1}e^{-t}dt, \quad x>0$$ another being Euler's infinite product definition $$\Gamma(x) = \frac{1}{x} \prod_{n=0}^\infty \frac{\left(1+\frac{1}{n}\right)^x}{\left(1+\frac{x}{n}\right)},\quad x>0.$$ It is a standard exercise to show that these agree over the positive real numbers. It is an interesting question to ask whether or not this function is analytic and if so over what intervals? i.e for which $x_0>0$ can we write it as an infinite series of the form $\Gamma(x) = \sum_{n=0}^\infty a_n (x-x_0)^n$ for some real constants $a_n$ such that it converges in an open neighbourhood of $x_0?$ The two definitions of $\Gamma(x)$ given above are well defined for complex $x$ as well (with some restrictions). Since analyticity is a stronger property in the complex plane, it is easier to prove that $\Gamma(z), \Re(z)>0,$ is holomorphic everywhere in it's domain then use analytic continuation to extend elsewhere in the plane. We then know that $\Gamma(x)$ is analytic over $x>0$ , since we can just restrict to the positive reals. There does not seem to be a proof of this theorem that doesn't use complex analysis in someway. To avoid the singularities at negative integer arguments, and to allow us to use the integral definition, I am only interested in proving analyticity over the positive reals. How can we prove the gamma function is analytic over the positive real numbers without using the theorems of complex analysis? My attempt Here is my attempt with gaps. Consider Prym's representation $$\Gamma(x)=\sum_{n=0}^\infty \frac{(-1)^n}{n!}\frac{1}{x+n}+\int_1^\infty t^{x-1}e^{-t}dt.$$ We'll show that both parts of this sum is analytic, thus their sum is analytic and $\Gamma(x)$ is analytic. The infinite sum is analytic because [...] To prove the right hand is analytic, we'll use Bernstein's Theorem from Krantz and Park's book A Primer of Real Analytic Functions Theorem: Let $f$ be a smooth function $(C^\infty)$ on an open interval $I\subset \mathbb{R}$ . If $f$ and all it's derivatives are non-negative on the entire interval $I$ then $f$ is real analytic on $I$ . The smoothness property follows from the fact that $t^{x-1}e^{-t}$ is smooth over $(x,t)\in \left[0,\infty\right) \times \left[1,\infty\right)$ . Then an application of Leibniz theorem gives that \begin{align*}     \frac{d^n}{dx^n}\int_1^\infty t^{x-1}e^{-t}dt &= \int_1^\infty \frac{d^n}{dz^n} t^{x-1}e^{-t}dt \\     &= \int_1^\infty \ln(t)^n t^{x-1}e^{-t}dt \end{align*} now since the integrand is positive for $x>0$ and $1<t<\infty$ , the integral must also be positive. Thus $\int_1^\infty t^{x-1}e^{-t}dt$ is smooth and has positive derivatives for all $x>0$ and so $\int_1^\infty t^{x-1}e^{-t}dt$ is analytic over $x>0$ . Thus $\Gamma(x)$ is analytic over $x>0$ . $\blacksquare$ So it remains to show that the infinite sum defines an analytic function (does the infinite sum of analytic functions converge to an analytic function?) and it would be nice to be less handwavy with the smoothness property.","The gamma function can be defined in a few different ways, the most well known possibly being Euler's second integral definition another being Euler's infinite product definition It is a standard exercise to show that these agree over the positive real numbers. It is an interesting question to ask whether or not this function is analytic and if so over what intervals? i.e for which can we write it as an infinite series of the form for some real constants such that it converges in an open neighbourhood of The two definitions of given above are well defined for complex as well (with some restrictions). Since analyticity is a stronger property in the complex plane, it is easier to prove that is holomorphic everywhere in it's domain then use analytic continuation to extend elsewhere in the plane. We then know that is analytic over , since we can just restrict to the positive reals. There does not seem to be a proof of this theorem that doesn't use complex analysis in someway. To avoid the singularities at negative integer arguments, and to allow us to use the integral definition, I am only interested in proving analyticity over the positive reals. How can we prove the gamma function is analytic over the positive real numbers without using the theorems of complex analysis? My attempt Here is my attempt with gaps. Consider Prym's representation We'll show that both parts of this sum is analytic, thus their sum is analytic and is analytic. The infinite sum is analytic because [...] To prove the right hand is analytic, we'll use Bernstein's Theorem from Krantz and Park's book A Primer of Real Analytic Functions Theorem: Let be a smooth function on an open interval . If and all it's derivatives are non-negative on the entire interval then is real analytic on . The smoothness property follows from the fact that is smooth over . Then an application of Leibniz theorem gives that now since the integrand is positive for and , the integral must also be positive. Thus is smooth and has positive derivatives for all and so is analytic over . Thus is analytic over . So it remains to show that the infinite sum defines an analytic function (does the infinite sum of analytic functions converge to an analytic function?) and it would be nice to be less handwavy with the smoothness property.","\Gamma(x) = \int_0^\infty t^{x-1}e^{-t}dt, \quad x>0 \Gamma(x) = \frac{1}{x} \prod_{n=0}^\infty \frac{\left(1+\frac{1}{n}\right)^x}{\left(1+\frac{x}{n}\right)},\quad x>0. x_0>0 \Gamma(x) = \sum_{n=0}^\infty a_n (x-x_0)^n a_n x_0? \Gamma(x) x \Gamma(z), \Re(z)>0, \Gamma(x) x>0 \Gamma(x)=\sum_{n=0}^\infty \frac{(-1)^n}{n!}\frac{1}{x+n}+\int_1^\infty t^{x-1}e^{-t}dt. \Gamma(x) f (C^\infty) I\subset \mathbb{R} f I f I t^{x-1}e^{-t} (x,t)\in \left[0,\infty\right) \times \left[1,\infty\right) \begin{align*}
    \frac{d^n}{dx^n}\int_1^\infty t^{x-1}e^{-t}dt &= \int_1^\infty \frac{d^n}{dz^n} t^{x-1}e^{-t}dt \\
    &= \int_1^\infty \ln(t)^n t^{x-1}e^{-t}dt
\end{align*} x>0 1<t<\infty \int_1^\infty t^{x-1}e^{-t}dt x>0 \int_1^\infty t^{x-1}e^{-t}dt x>0 \Gamma(x) x>0 \blacksquare","['real-analysis', 'analysis', 'special-functions', 'gamma-function']"
82,How to interpret this sum in Tate's thesis?,How to interpret this sum in Tate's thesis?,,"Let $f$ be a Schwartz function on the ring of adeles $\mathbb{A}$ of a number field $K$ , and $d^\times x$ the multiplicative Haar measure on $\mathbb{A}^\times$ . One can embed $K^\times$ diagonally in $\mathbb{A}^\times$ and can take a quotient $\mathbb{A}^\times/K^\times$ . In Tate's thesis, the integral of $f$ over $\mathbb{A}^\times$ is replaced with an integral over $\mathbb{A}^\times/K^\times$ via $$\int_{\mathbb{A}^\times}f(x)d^\times x = \int_{\mathbb{A}^\times/K^\times}\left(\sum_{\kappa \in K^\times}f(\kappa x)\right) d^\times x.$$ I am wondering how to interpret this sum. In particular, why does it converge, and how big is $\mathbb{A}^\times/K^\times$ ? For example, if in the real archimedean component $f_\infty(x) = e^{-\pi x^2}$ , is he integrating over a function which in this component is equal to $$\sum_{\kappa \in K^\times}e^{-\pi \kappa^2 x^2} $$ and does this converge? Also, how does this relate to the Jacobi theta function?","Let be a Schwartz function on the ring of adeles of a number field , and the multiplicative Haar measure on . One can embed diagonally in and can take a quotient . In Tate's thesis, the integral of over is replaced with an integral over via I am wondering how to interpret this sum. In particular, why does it converge, and how big is ? For example, if in the real archimedean component , is he integrating over a function which in this component is equal to and does this converge? Also, how does this relate to the Jacobi theta function?",f \mathbb{A} K d^\times x \mathbb{A}^\times K^\times \mathbb{A}^\times \mathbb{A}^\times/K^\times f \mathbb{A}^\times \mathbb{A}^\times/K^\times \int_{\mathbb{A}^\times}f(x)d^\times x = \int_{\mathbb{A}^\times/K^\times}\left(\sum_{\kappa \in K^\times}f(\kappa x)\right) d^\times x. \mathbb{A}^\times/K^\times f_\infty(x) = e^{-\pi x^2} \sum_{\kappa \in K^\times}e^{-\pi \kappa^2 x^2} ,"['analysis', 'number-theory', 'measure-theory', 'fourier-analysis', 'analytic-number-theory']"
83,Radial Lipschitz Mapping,Radial Lipschitz Mapping,,"Let $f:\mathbb{R}^2\rightarrow \mathbb{R}^2$ defined by $$f(x,y)=(x\cos(y),x\sin(y)).$$ Is $f$ a Lipschitz function? Is it bi-Lipschitz? I tried this: \begin{align*}|f(x_1 ,y_1) - f(x_2 , y_2)|^2&= x_1^2 +x_2^2 - 2x_1 x_2 (\cos(y_1)\cos(y_2)+\sin(y_1)\sin(y_2))\\&=x_1^2 +x_2^2 - 2x_1 x_2 \cos(y_1 + y_2),\end{align*} but I don't know how to continue to make it appear $|(x_1 ,y_1)-(x_2 , y_2)|$ . Any help is welcome.",Let defined by Is a Lipschitz function? Is it bi-Lipschitz? I tried this: but I don't know how to continue to make it appear . Any help is welcome.,"f:\mathbb{R}^2\rightarrow \mathbb{R}^2 f(x,y)=(x\cos(y),x\sin(y)). f \begin{align*}|f(x_1 ,y_1) - f(x_2 , y_2)|^2&= x_1^2 +x_2^2 - 2x_1 x_2 (\cos(y_1)\cos(y_2)+\sin(y_1)\sin(y_2))\\&=x_1^2 +x_2^2 - 2x_1 x_2 \cos(y_1 + y_2),\end{align*} |(x_1 ,y_1)-(x_2 , y_2)|","['real-analysis', 'analysis', 'functions', 'lipschitz-functions']"
84,Existence of zero for sequence of functions for which the limit function has a zero,Existence of zero for sequence of functions for which the limit function has a zero,,"Let $f_n \colon \mathbb R^d \to \mathbb R^d$ be a sequence of smooth functions and let $f \colon \mathbb R^d \to \mathbb R^d$ be its limit such that $\lim_{n \to \infty} f_n(x) = f(x)$ for all $x \in \mathbb R^d$ , which is also smooth. I assume that there exists $x_0 \in \mathbb R^d$ such that $f(x_0) = 0$ and $\det J_f(x_0) \neq 0$ , where $J_f$ denotes the Jacobian matrix of the function $f$ . I would like to prove (by also adding other assumptions if necessary) or disprove that there exists $N > 0$ such that for all $n > N$ there exists $x_n \in \mathbb R^d$ (not necessarily unique) which satisfies $f_n(x_n) = 0$ and $\lim_{n \to \infty} x_n = x_0$ . In the one-dimensional case ( $d = 1$ ) I proved that the result holds. However, for higher dimensions $d > 1$ this seems either non-trivial or false.","Let be a sequence of smooth functions and let be its limit such that for all , which is also smooth. I assume that there exists such that and , where denotes the Jacobian matrix of the function . I would like to prove (by also adding other assumptions if necessary) or disprove that there exists such that for all there exists (not necessarily unique) which satisfies and . In the one-dimensional case ( ) I proved that the result holds. However, for higher dimensions this seems either non-trivial or false.",f_n \colon \mathbb R^d \to \mathbb R^d f \colon \mathbb R^d \to \mathbb R^d \lim_{n \to \infty} f_n(x) = f(x) x \in \mathbb R^d x_0 \in \mathbb R^d f(x_0) = 0 \det J_f(x_0) \neq 0 J_f f N > 0 n > N x_n \in \mathbb R^d f_n(x_n) = 0 \lim_{n \to \infty} x_n = x_0 d = 1 d > 1,"['real-analysis', 'analysis']"
85,Bartle exercise 9.N from elements of integration,Bartle exercise 9.N from elements of integration,,"The exercise states: Let $X$ be a set, $\mathbf{A}$ an algebra of subsets of $X$ , and $\mu$ a measure on $\mathbf{A}$ . If $B\subset X$ is arbitrary, let $$\mu'(B)=\inf\{\mu(A):B\subset A\in\mathbf{A}\}$$ and $$\mu^*(B)=\inf\{\sum_{n=1}^\infty \mu(I_n):B\subset \bigcup_{n=1}^\infty I_n,\{I_n\}_{n\in \mathbb{N}}\subset \mathbf{A}\}$$ Show that $\mu'(E)=\mu(E)$ for all $E\in \mathbf{A} $ and that $\mu^*(B)\leq \mu'(B)$ . Moreover, $\mu^*=\mu'$ in case $X$ is the countable union of sets with finite $\mu$ -measure. My question: I already proved all but the ""Moreover, $\mu^*=\mu'$ in case $X$ is the countable union of sets with finite $\mu$ -measure"". To prove it one must show that in that case $$\mu^*(B)\geq\mu'(B)$$ for all $B\subset X$ . Any hints or suggestions?","The exercise states: Let be a set, an algebra of subsets of , and a measure on . If is arbitrary, let and Show that for all and that . Moreover, in case is the countable union of sets with finite -measure. My question: I already proved all but the ""Moreover, in case is the countable union of sets with finite -measure"". To prove it one must show that in that case for all . Any hints or suggestions?","X \mathbf{A} X \mu \mathbf{A} B\subset X \mu'(B)=\inf\{\mu(A):B\subset A\in\mathbf{A}\} \mu^*(B)=\inf\{\sum_{n=1}^\infty \mu(I_n):B\subset \bigcup_{n=1}^\infty I_n,\{I_n\}_{n\in \mathbb{N}}\subset \mathbf{A}\} \mu'(E)=\mu(E) E\in \mathbf{A}  \mu^*(B)\leq \mu'(B) \mu^*=\mu' X \mu \mu^*=\mu' X \mu \mu^*(B)\geq\mu'(B) B\subset X","['analysis', 'measure-theory', 'outer-measure']"
86,Does the Doob inequality hold for local martingales?,Does the Doob inequality hold for local martingales?,,"Let $M$ be a real-valued continuous local martingale with $M_0=0$ , and  fix $t_0\geq 0$ . Do we then have the inequality $$\mathbb{E}[~\sup_{t\leq t_0} M_t^2~]\leq C\cdot \mathbb{E}[M_{t_0}^2],$$ where $C$ is a universal constant (i.e. not depending on $t_0$ or $M$ )? The inequality is trivial in case $M_{t_0}\notin L^2$ , so we may assume $\mathbb{E}[M_{t_0}^2]<\infty$ .  This inequality holds when $M$ is a true martingale and $C=4$ , in which case it is known as the Doob inequality. If we localize the inequality and let the stopping times tend to infinity, the left hand side is a monotone limit, but it's not clear what to do with the limit of the right hand side. If somehow the assumption $\mathbb{E}[M_{t_0}^2]<\infty$ implied that $\mathbb{E}[\langle M, M \rangle _{t_0}]<\infty$ (as it would if $M^2-\langle M, M \rangle$ were a true martingale rather than just a local martingale), then it is a theorem that $M_{t\wedge t_0}$ is a true martingale and the normal Doob inequality would apply.","Let be a real-valued continuous local martingale with , and  fix . Do we then have the inequality where is a universal constant (i.e. not depending on or )? The inequality is trivial in case , so we may assume .  This inequality holds when is a true martingale and , in which case it is known as the Doob inequality. If we localize the inequality and let the stopping times tend to infinity, the left hand side is a monotone limit, but it's not clear what to do with the limit of the right hand side. If somehow the assumption implied that (as it would if were a true martingale rather than just a local martingale), then it is a theorem that is a true martingale and the normal Doob inequality would apply.","M M_0=0 t_0\geq 0 \mathbb{E}[~\sup_{t\leq t_0} M_t^2~]\leq C\cdot \mathbb{E}[M_{t_0}^2], C t_0 M M_{t_0}\notin L^2 \mathbb{E}[M_{t_0}^2]<\infty M C=4 \mathbb{E}[M_{t_0}^2]<\infty \mathbb{E}[\langle M, M \rangle _{t_0}]<\infty M^2-\langle M, M \rangle M_{t\wedge t_0}","['analysis', 'stochastic-processes']"
87,"Convergence of Cauchy sequences in $(\ell^1, \|\cdot\|_\infty)$",Convergence of Cauchy sequences in,"(\ell^1, \|\cdot\|_\infty)","Do Cauchy sequences in $(\ell^1, \|\cdot\|_\infty)$ converge in $\ell^1$ ? I feel that the answer is No , but I am not able to find a counterexample. If $x^{(n)}$ is a sequence in $\ell^1$ , then we have $$\sum_{i=1}^\infty |x^{(n)}_i| < \infty$$ for every $n\in\mathbb N$ . Also if $x^{(n)}$ is Cauchy w.r.t the sup-norm, then $$\forall\epsilon>0\exists N\in\mathbb N\forall m,n>N (\|x^{(m)} - x^{(n)}\|_\infty < \epsilon)$$ That is the definition, and that is all I can see. Any ideas?","Do Cauchy sequences in converge in ? I feel that the answer is No , but I am not able to find a counterexample. If is a sequence in , then we have for every . Also if is Cauchy w.r.t the sup-norm, then That is the definition, and that is all I can see. Any ideas?","(\ell^1, \|\cdot\|_\infty) \ell^1 x^{(n)} \ell^1 \sum_{i=1}^\infty |x^{(n)}_i| < \infty n\in\mathbb N x^{(n)} \forall\epsilon>0\exists N\in\mathbb N\forall m,n>N (\|x^{(m)} - x^{(n)}\|_\infty < \epsilon)","['real-analysis', 'analysis', 'cauchy-sequences']"
88,Divergence free vector fields in general measure space,Divergence free vector fields in general measure space,,"I'm reading Ambrosio's Gradient Flows in Metric Spaces and the Space of Probability Measures textbook, and I'm stuck on a part of Lemma 8.4.2. Rather than give the whole Lemma, I'll state the specific claim I'm confused by, and I'll simplify it by taking $p = 2$ ( $p = 2$ is the only case I'm interested in), $X = \mathbb{R}^n$ (rather than some other separable Hilbert space $X$ ). Here $\mu$ is a finite Borel measure on $X$ , and $v,w$ are $L^2(\mu)$ vector fields (the integral of $|v|^2$ w.r.t. $\mu$ is finite). The statement is as follows: \begin{equation*} \int_X v \cdot w \, d \mu(x) = 0 \, , \text{ for any $w \in L^2(\mu)$ s.t. $\nabla \cdot (w \mu) = 0$} \end{equation*} is equivalent to \begin{equation*} v \text{ belongs to the $L^2(\mu)$ closure of } \{\nabla \phi \, : \, \phi \in C_c^\infty(X)\} \, .  \end{equation*} Some clarifications: $\nabla \cdot (w \mu) = 0$ means that, for any test function $\phi \in C_b^1(X)$ , $$\int_X \nabla \phi \cdot w \, d \mu = 0 \, . $$ The second statement means that there is some sequence $\{\phi_n\} \subseteq C_c^\infty(X)$ so that $$\lim_{n \to \infty} \int_X |\nabla \phi_n - v|^2 \, d \mu = 0 \, . $$ Why is the stated equivalence true??? (The textbook states it without justification.) I think the backward implication is clear: if we can write $v = \nabla \phi$ for some $\phi$ , then $\nabla \cdot (w\mu) = 0$ means precisely that $\int_X v \cdot w \, d \mu = 0$ . However, the forward implication is unclear to me. If $\mu$ were just the Lebesgue measure/some absolutely continuous measure w.r.t. Lebesgue then I would try integration by parts (looks like some Sobolev thing going on), but I don't believe this is available to us here? I can't find any integration by parts formulas for arbitrary measures in the textbook.","I'm reading Ambrosio's Gradient Flows in Metric Spaces and the Space of Probability Measures textbook, and I'm stuck on a part of Lemma 8.4.2. Rather than give the whole Lemma, I'll state the specific claim I'm confused by, and I'll simplify it by taking ( is the only case I'm interested in), (rather than some other separable Hilbert space ). Here is a finite Borel measure on , and are vector fields (the integral of w.r.t. is finite). The statement is as follows: is equivalent to Some clarifications: means that, for any test function , The second statement means that there is some sequence so that Why is the stated equivalence true??? (The textbook states it without justification.) I think the backward implication is clear: if we can write for some , then means precisely that . However, the forward implication is unclear to me. If were just the Lebesgue measure/some absolutely continuous measure w.r.t. Lebesgue then I would try integration by parts (looks like some Sobolev thing going on), but I don't believe this is available to us here? I can't find any integration by parts formulas for arbitrary measures in the textbook.","p = 2 p = 2 X = \mathbb{R}^n X \mu X v,w L^2(\mu) |v|^2 \mu \begin{equation*}
\int_X v \cdot w \, d \mu(x) = 0 \, , \text{ for any w \in L^2(\mu) s.t. \nabla \cdot (w \mu) = 0}
\end{equation*} \begin{equation*}
v \text{ belongs to the L^2(\mu) closure of } \{\nabla \phi \, : \, \phi \in C_c^\infty(X)\} \, . 
\end{equation*} \nabla \cdot (w \mu) = 0 \phi \in C_b^1(X) \int_X \nabla \phi \cdot w \, d \mu = 0 \, .  \{\phi_n\} \subseteq C_c^\infty(X) \lim_{n \to \infty} \int_X |\nabla \phi_n - v|^2 \, d \mu = 0 \, .  v = \nabla \phi \phi \nabla \cdot (w\mu) = 0 \int_X v \cdot w \, d \mu = 0 \mu","['analysis', 'measure-theory', 'optimal-transport', 'gradient-flows']"
89,Is it true that $\frac{f(b)-f(a)}{b-a}-\frac{b-a}{g(b)-g(a)}=f'(c)-\frac{1}{g'(c)}$,Is it true that,\frac{f(b)-f(a)}{b-a}-\frac{b-a}{g(b)-g(a)}=f'(c)-\frac{1}{g'(c)},"Let $f$ and $g$ be continuous functions on the closed interval $[a,b]$ , and differentiable on the open interval $(a,b)$ , where $a<b$ . If $g'(x)\neq 0$ on $(a,b)$ , then does there exists $c$ on $(a,b)$ such that $\frac{f(b)-f(a)}{b-a}-\frac{b-a}{g(b)-g(a)}=f'(c)-\frac{1}{g'(c)}$ ? Note that $g(b)-g(a)\neq 0$ by Rolle's theorem. By Lagrange's mean value theorem, there exist $c$ and $d$ on $(a,b)$ , such that $\frac{f(b)-f(a)}{b-a}-\frac{b-a}{g(b)-g(a)}=f'(c)-\frac{1}{g'(d)}$ . I guess that $c$ and $d$ can have the same value.","Let and be continuous functions on the closed interval , and differentiable on the open interval , where . If on , then does there exists on such that ? Note that by Rolle's theorem. By Lagrange's mean value theorem, there exist and on , such that . I guess that and can have the same value.","f g [a,b] (a,b) a<b g'(x)\neq 0 (a,b) c (a,b) \frac{f(b)-f(a)}{b-a}-\frac{b-a}{g(b)-g(a)}=f'(c)-\frac{1}{g'(c)} g(b)-g(a)\neq 0 c d (a,b) \frac{f(b)-f(a)}{b-a}-\frac{b-a}{g(b)-g(a)}=f'(c)-\frac{1}{g'(d)} c d","['calculus', 'analysis']"
90,Proof of first Fundamental theorem of calculus,Proof of first Fundamental theorem of calculus,,"Can you please, check if my proof is correct? Suppose that $f:[a,b]\to \Bbb{R}$ is continuous and $F(x)=\int^{x}_{a}f(t)dt$ , then $F\in C^{1}[a,b]$ and $$\dfrac{d}{dx}\int^{x}_{a}f(t)dt:=F'(x)=f(x)$$ MY PROOF: Credits to Aweygan for the correction Let $x_0\in[a,b]$ and $\epsilon>0$ be given. Since $f$ is continuous at $x_0$ then, there exists $\delta>0$ such that $|t-x_0|<\delta$ implies $$|f(t)-f(x_0)|<\epsilon.$$ Thus, $$f(x_0)=\dfrac{1}{x-x_0}\int^{x}_{x_0}f(x_0)dt,\;\;\text{where}\;\;x\neq x_0.$$ For any $x\in (a,b),$ with $0<|x-x_0|<\delta,$ such that $x_1=\min\{x,x_0\}$ and $x_2=\max\{x,x_0\}$ . So, we have \begin{align}\left| \dfrac{F(x)-F(x_0)}{x-x_0}-f(x_0) \right|&=  \left| \dfrac{1}{x-x_0}\int^{x}_{x_0}(f(t)-f(x_0))dt \right|  \\&\leq  \dfrac{1}{|x-x_0|}\int^{x}_{x_0} \left|f(t)-f(x_0) \right|dt\\&\leq  \dfrac{1}{|x-x_0|}\int^{x_2}_{x_1} \left|f(t)-f(x_0) \right|dt\\&< \dfrac{1}{|x-x_0|}\epsilon|x_1-x_2| \\&\leq \dfrac{1}{|x-x_0|}\epsilon|x-x_0| =\epsilon  \end{align} Hence, $$F\in C^{1}[a,b]\;\;\text{and}\;\;\dfrac{d}{dx}\int^{x}_{a}f(t)dt:=F'(x)=f(x)$$","Can you please, check if my proof is correct? Suppose that is continuous and , then and MY PROOF: Credits to Aweygan for the correction Let and be given. Since is continuous at then, there exists such that implies Thus, For any with such that and . So, we have Hence,","f:[a,b]\to \Bbb{R} F(x)=\int^{x}_{a}f(t)dt F\in C^{1}[a,b] \dfrac{d}{dx}\int^{x}_{a}f(t)dt:=F'(x)=f(x) x_0\in[a,b] \epsilon>0 f x_0 \delta>0 |t-x_0|<\delta |f(t)-f(x_0)|<\epsilon. f(x_0)=\dfrac{1}{x-x_0}\int^{x}_{x_0}f(x_0)dt,\;\;\text{where}\;\;x\neq x_0. x\in (a,b), 0<|x-x_0|<\delta, x_1=\min\{x,x_0\} x_2=\max\{x,x_0\} \begin{align}\left| \dfrac{F(x)-F(x_0)}{x-x_0}-f(x_0) \right|&=  \left| \dfrac{1}{x-x_0}\int^{x}_{x_0}(f(t)-f(x_0))dt \right|  \\&\leq  \dfrac{1}{|x-x_0|}\int^{x}_{x_0} \left|f(t)-f(x_0) \right|dt\\&\leq  \dfrac{1}{|x-x_0|}\int^{x_2}_{x_1} \left|f(t)-f(x_0) \right|dt\\&< \dfrac{1}{|x-x_0|}\epsilon|x_1-x_2| \\&\leq \dfrac{1}{|x-x_0|}\epsilon|x-x_0| =\epsilon  \end{align} F\in C^{1}[a,b]\;\;\text{and}\;\;\dfrac{d}{dx}\int^{x}_{a}f(t)dt:=F'(x)=f(x)","['real-analysis', 'calculus', 'analysis', 'proof-verification', 'riemann-integration']"
91,"$f(x)$ is continuous at $x=\alpha$ ,$g(x)$ is discontinuous at $x=a$ but $g(f(x))$ is continuous at $x=\alpha$","is continuous at  , is discontinuous at  but  is continuous at",f(x) x=\alpha g(x) x=a g(f(x)) x=\alpha,"Suppose $f,g:\mathbb{R} \rightarrow \mathbb{R}$ are such that $f(x)$ is continuous at $x=\alpha$ and $f(\alpha)=a$ and $g(x)$ is discontinuous at $x=a$ , but $g\big(f(x)\big)$ is continuous at $x=\alpha$ .  Also, $f(x),g(x)$ are non-constant functions.  Then, can it be said that $x=\alpha$ is an extremum of $f$ and $x=a$ is an extremum of $g$ ? I have tried to construct examples of functions, but never could figure out a rigorous proof For example take the function $f(x)=x^2$ which is continuous at $0$ , $g(x)=[x]$ which is discontinuous at $0$ , but $g\big(f(x)\big)$ is continuous at $0$ .","Suppose are such that is continuous at and and is discontinuous at , but is continuous at .  Also, are non-constant functions.  Then, can it be said that is an extremum of and is an extremum of ? I have tried to construct examples of functions, but never could figure out a rigorous proof For example take the function which is continuous at , which is discontinuous at , but is continuous at .","f,g:\mathbb{R} \rightarrow \mathbb{R} f(x) x=\alpha f(\alpha)=a g(x) x=a g\big(f(x)\big) x=\alpha f(x),g(x) x=\alpha f x=a g f(x)=x^2 0 g(x)=[x] 0 g\big(f(x)\big) 0","['real-analysis', 'calculus']"
92,Existence of a diffemorphism that maps one curve to another,Existence of a diffemorphism that maps one curve to another,,"Consider $1 < n\in \mathbb {N} $ , let $\gamma_1,\gamma_2 : [0,1] \to \mathbb{R}^{n}$ be smooth paths such that $$\gamma_1 (0) = \gamma_2(0) \neq \gamma_1(1) = \gamma_2(1) $$ and $\gamma_1, \gamma_2$ are injetive functions. Question: Does there exist a diffeomorphism $\varphi: \mathbb{R}^n \to \mathbb{R}^n$ such that $\varphi (\gamma_1 ([0,1])) = \gamma_2 ([0,1]).$ Does anyone know if this result is true? This seems true but I do not know how to prove it, can anyone help me?","Consider , let be smooth paths such that and are injetive functions. Question: Does there exist a diffeomorphism such that Does anyone know if this result is true? This seems true but I do not know how to prove it, can anyone help me?","1 < n\in \mathbb {N}  \gamma_1,\gamma_2 : [0,1] \to \mathbb{R}^{n} \gamma_1 (0) = \gamma_2(0) \neq \gamma_1(1) = \gamma_2(1)  \gamma_1, \gamma_2 \varphi: \mathbb{R}^n \to \mathbb{R}^n \varphi (\gamma_1 ([0,1])) = \gamma_2 ([0,1]).","['analysis', 'differential-geometry', 'differential-topology']"
93,Derivative of $\ln|x|$ is the principal value of $1/x.$ Distribution Theory.,Derivative of  is the principal value of  Distribution Theory.,\ln|x| 1/x.,"I have been looking at the proof for $\frac{d}{dx}\ln|x|=\operatorname{p.v.}\left(\frac{1}{x}\right)$ in the context of distributions and I am having trouble understanding why in the second term after integration by parts the limits we are subbing for $x$ are $\epsilon$ and $-\epsilon$ (the bit is highlighted in blue). The reason for my confusion is that $[a,-a]\backslash[-\epsilon,\epsilon]$ means $[-a,-\epsilon]\cup[\epsilon,a]$ so I am not sure why we are not using this for limits in the integral instead. Please note that $\operatorname{p.v.}\left(\frac{1}{x}\right)$ is Cauchy principal value of $\frac{1}{x}$ defined as: $$\left\langle \operatorname{p.v.}\left(\frac{1}{x}\right), \phi\right\rangle = \lim_{\epsilon\to0} \int_{|x|>\epsilon} \frac{1}{x}\phi(x)\,dx$$ Here is the proof I have been referring to: For any $\phi\in\mathcal D(\mathbb R)$ (i.e. it is a test function) with $\operatorname{supp}\phi = [a,-a]$ we have: \begin{align*} \left\langle\frac{d}{dx}\ln|x|, \phi\right\rangle &=-\langle \ln|x|, \phi'\rangle\\ &= - \lim_{\epsilon\to0} \int_{\mathbb R\backslash[-\epsilon,\epsilon]} \ln|x|\phi'(x) dx\\ &=- \lim_{\epsilon\to0}\left[\int_{[a,-a]\backslash[-\epsilon,\epsilon]} \frac{1}{x}\phi(x) dx + \color{blue}{(\ln|\epsilon|)(\phi(\epsilon)-\phi(-\epsilon))}\right]\\ &= - \lim_{\epsilon\to0} \int_{[a,-a]\backslash[-\epsilon,\epsilon]} \frac{1}{x}\phi(x) dx\\ &=\left\langle\operatorname{p.v.}\left(\frac{1}{x}\right), \phi\right\rangle \end{align*}",I have been looking at the proof for in the context of distributions and I am having trouble understanding why in the second term after integration by parts the limits we are subbing for are and (the bit is highlighted in blue). The reason for my confusion is that means so I am not sure why we are not using this for limits in the integral instead. Please note that is Cauchy principal value of defined as: Here is the proof I have been referring to: For any (i.e. it is a test function) with we have:,"\frac{d}{dx}\ln|x|=\operatorname{p.v.}\left(\frac{1}{x}\right) x \epsilon -\epsilon [a,-a]\backslash[-\epsilon,\epsilon] [-a,-\epsilon]\cup[\epsilon,a] \operatorname{p.v.}\left(\frac{1}{x}\right) \frac{1}{x} \left\langle \operatorname{p.v.}\left(\frac{1}{x}\right), \phi\right\rangle = \lim_{\epsilon\to0} \int_{|x|>\epsilon} \frac{1}{x}\phi(x)\,dx \phi\in\mathcal D(\mathbb R) \operatorname{supp}\phi = [a,-a] \begin{align*}
\left\langle\frac{d}{dx}\ln|x|, \phi\right\rangle
&=-\langle \ln|x|, \phi'\rangle\\
&= - \lim_{\epsilon\to0} \int_{\mathbb R\backslash[-\epsilon,\epsilon]} \ln|x|\phi'(x) dx\\
&=- \lim_{\epsilon\to0}\left[\int_{[a,-a]\backslash[-\epsilon,\epsilon]} \frac{1}{x}\phi(x) dx + \color{blue}{(\ln|\epsilon|)(\phi(\epsilon)-\phi(-\epsilon))}\right]\\
&= - \lim_{\epsilon\to0} \int_{[a,-a]\backslash[-\epsilon,\epsilon]} \frac{1}{x}\phi(x) dx\\
&=\left\langle\operatorname{p.v.}\left(\frac{1}{x}\right), \phi\right\rangle
\end{align*}","['analysis', 'distribution-theory', 'cauchy-principal-value']"
94,Infinite Product Expansion of Hyperbolic Functions,Infinite Product Expansion of Hyperbolic Functions,,"the following equation is from ""[1970] Goodson - Distributed system simulation using infinite product expansions"": \begin{align*} 	\cosh z + \left( c z+ \frac{d}{z} \right) \sinh z & = (1 + d)  \displaystyle\prod_{n=1}^{\infty} \left( 1 + \frac{z^2}{p_n^2} \right)\\ \tan p_n & = \frac{p_n}{c p_n^2 - d}, \quad p_n \ge 0, \text{real} \end{align*} I am not sure if $p_n \ge 0$ is correct, I guess it is $p_n >0$ instead. Maybe someone can give a reference (e.g. a book) where to find the derivation of this equation (or just the equation itself)? This would help me a lot! Best","the following equation is from ""[1970] Goodson - Distributed system simulation using infinite product expansions"": \begin{align*} 	\cosh z + \left( c z+ \frac{d}{z} \right) \sinh z & = (1 + d)  \displaystyle\prod_{n=1}^{\infty} \left( 1 + \frac{z^2}{p_n^2} \right)\\ \tan p_n & = \frac{p_n}{c p_n^2 - d}, \quad p_n \ge 0, \text{real} \end{align*} I am not sure if $p_n \ge 0$ is correct, I guess it is $p_n >0$ instead. Maybe someone can give a reference (e.g. a book) where to find the derivation of this equation (or just the equation itself)? This would help me a lot! Best",,"['analysis', 'infinite-product', 'hyperbolic-functions', 'weierstrass-factorization']"
95,"Existence of positive integers $m,k$ such that $2^m > k\alpha > 2^m - 1$ for fixed irrational number $\alpha$.",Existence of positive integers  such that  for fixed irrational number .,"m,k 2^m > k\alpha > 2^m - 1 \alpha","Let $\alpha \in \mathbb{R} $\ $\mathbb{Q}$, then I claim that there exist positive integers $m,k$ such that $2^m > k\alpha > 2^m - 1$. I tried many elementary approaches but all of them either reformulated the problem or cycled back on itself. This area of maths is uncharted territory for me and so I don't know any advanced approaches I could use, but I am open to anything.","Let $\alpha \in \mathbb{R} $\ $\mathbb{Q}$, then I claim that there exist positive integers $m,k$ such that $2^m > k\alpha > 2^m - 1$. I tried many elementary approaches but all of them either reformulated the problem or cycled back on itself. This area of maths is uncharted territory for me and so I don't know any advanced approaches I could use, but I am open to anything.",,"['analysis', 'inequality', 'approximation', 'diophantine-approximation']"
96,sum of series using mean value theorem,sum of series using mean value theorem,,"Let $f(x)$ be a function which is differentiable on $[0,1]$ with $f(0)=0$ and $f(1)=1$. Show that for every $n\in \Bbb N$ there exists numbers $x_1,x_2,\ldots,x_n\in [0,1]$ such as $$  \sum_{k = 1}^n \frac{1}{f' (x_k)} = n $$ I think the mean value theorem should be applied. So there exists $x_1$ in $[0,1]$  such that $f ' (x_1) = \frac{f(1) - f(0)}{1-0} =1$  and there exists $x_2$ in $[0,x_1]$ such that $f ' (x_2) = \frac{f(x1) - f(0)}{x1-0} = \frac{f(x1)}{x1}$, so on and so forth for $x_3 ,x_4, \ldots,x_n$ and we have the sum $$1+\frac{x_1}{f(x1)} + \frac{x_2}{f(x2)} +\cdots+\frac{x_{n-1}}{f(x_{n-1})}$$  and from here I have no idea what to do . I was wondering if anyone could be so kind to help ?","Let $f(x)$ be a function which is differentiable on $[0,1]$ with $f(0)=0$ and $f(1)=1$. Show that for every $n\in \Bbb N$ there exists numbers $x_1,x_2,\ldots,x_n\in [0,1]$ such as $$  \sum_{k = 1}^n \frac{1}{f' (x_k)} = n $$ I think the mean value theorem should be applied. So there exists $x_1$ in $[0,1]$  such that $f ' (x_1) = \frac{f(1) - f(0)}{1-0} =1$  and there exists $x_2$ in $[0,x_1]$ such that $f ' (x_2) = \frac{f(x1) - f(0)}{x1-0} = \frac{f(x1)}{x1}$, so on and so forth for $x_3 ,x_4, \ldots,x_n$ and we have the sum $$1+\frac{x_1}{f(x1)} + \frac{x_2}{f(x2)} +\cdots+\frac{x_{n-1}}{f(x_{n-1})}$$  and from here I have no idea what to do . I was wondering if anyone could be so kind to help ?",,"['calculus', 'analysis']"
97,Should a metric always map into $\mathbf{R}$?,Should a metric always map into ?,\mathbf{R},"Typically you see the definition of a metric as a function which maps $X\times X\to\mathbf{R},$ but does this always have to be the case? Motivating example: When you complete $\mathbf{Q}$ with the Archimedean metric you think of it as mapping into $\mathbf{R},$ but if you were to choose the $p$-adic metric instead it can only take values in $\{0\}\cup p^\mathbf{Z}$. These are elements of $\mathbf{Q}_p$ as well as $\mathbf{R}.$ The difference is then that either your metric maps into an ordered field or one where you can't define any ordering (but you still know that the distances can have different values). Does not mapping a metric into $\mathbf{R}$ always lead to problems? And are the issues of the example above avoidable?","Typically you see the definition of a metric as a function which maps $X\times X\to\mathbf{R},$ but does this always have to be the case? Motivating example: When you complete $\mathbf{Q}$ with the Archimedean metric you think of it as mapping into $\mathbf{R},$ but if you were to choose the $p$-adic metric instead it can only take values in $\{0\}\cup p^\mathbf{Z}$. These are elements of $\mathbf{Q}_p$ as well as $\mathbf{R}.$ The difference is then that either your metric maps into an ordered field or one where you can't define any ordering (but you still know that the distances can have different values). Does not mapping a metric into $\mathbf{R}$ always lead to problems? And are the issues of the example above avoidable?",,"['analysis', 'metric-spaces']"
98,"Assume that $ f: R \to R $ is uniformly continuous. prove that there are constants A,B such that $ |f(x)| \le A + B|x| $ for all $ x \in R $. [duplicate]","Assume that  is uniformly continuous. prove that there are constants A,B such that  for all . [duplicate]", f: R \to R   |f(x)| \le A + B|x|   x \in R ,"This question already has an answer here : Show that there are $ a,b \geq 0 $ so that $ |f(x)| \leq ax+b, \forall x \geq 0.$ (1 answer) Closed 7 years ago . Assume that $ f: \mathbb R \to \mathbb R $ is uniformly continuous. prove that there are constants $A,B$ such that $ |f(x)| \le A + B|x| $ for all $ x \in \mathbb R $. my concern is just $f$ is uniformly continuous on $\mathbb R$ how this is going to help us to find such $A$ and $B$.","This question already has an answer here : Show that there are $ a,b \geq 0 $ so that $ |f(x)| \leq ax+b, \forall x \geq 0.$ (1 answer) Closed 7 years ago . Assume that $ f: \mathbb R \to \mathbb R $ is uniformly continuous. prove that there are constants $A,B$ such that $ |f(x)| \le A + B|x| $ for all $ x \in \mathbb R $. my concern is just $f$ is uniformly continuous on $\mathbb R$ how this is going to help us to find such $A$ and $B$.",,"['real-analysis', 'analysis', 'continuity', 'uniform-continuity']"
99,simple proof that $\sqrt{1+\frac{1}{x+1/2}}(1+1/x)^x\le e$,simple proof that,\sqrt{1+\frac{1}{x+1/2}}(1+1/x)^x\le e,"It is well known that for $x>0$ that $\left(1+\frac{1}{x}\right)^x\le e\le\left(1+\frac{1}{x}\right)^{x+1}$ (see wikipedia ). However, one can obtain the stronger inequality  $$ \sqrt{1+\frac{1}{x+\frac{1}{2}}}\left(1+\frac{1}{x}\right)^x\le e\le\sqrt{1+\frac{1}{x}}\left(1+\frac{1}{x}\right)^{x} $$  The second inequality can be found in Proposition B.3 of ""Randomized Algorithms"", by Raghaven and Motwani (which itself refers to the book ""Analytic Inequalities"" by Mitrinović) , and can be proven straight-forwardly by calculus (showing a first derivative is non-negative and such). While I can also prove the first inequality using familiar calculus methods, it is a bit messy (ultimately requiring that $\frac{1}{y+2}+\frac{1}{3y+2}\ge \frac{1}{y+1}$ for $y\ge 0$). Does anyone know a ""simple"" proof of $\sqrt{1+\frac{1}{x+\frac{1}{2}}}\left(1+\frac{1}{x}\right)^x\le e$?","It is well known that for $x>0$ that $\left(1+\frac{1}{x}\right)^x\le e\le\left(1+\frac{1}{x}\right)^{x+1}$ (see wikipedia ). However, one can obtain the stronger inequality  $$ \sqrt{1+\frac{1}{x+\frac{1}{2}}}\left(1+\frac{1}{x}\right)^x\le e\le\sqrt{1+\frac{1}{x}}\left(1+\frac{1}{x}\right)^{x} $$  The second inequality can be found in Proposition B.3 of ""Randomized Algorithms"", by Raghaven and Motwani (which itself refers to the book ""Analytic Inequalities"" by Mitrinović) , and can be proven straight-forwardly by calculus (showing a first derivative is non-negative and such). While I can also prove the first inequality using familiar calculus methods, it is a bit messy (ultimately requiring that $\frac{1}{y+2}+\frac{1}{3y+2}\ge \frac{1}{y+1}$ for $y\ge 0$). Does anyone know a ""simple"" proof of $\sqrt{1+\frac{1}{x+\frac{1}{2}}}\left(1+\frac{1}{x}\right)^x\le e$?",,"['analysis', 'inequality', 'exponential-function']"
