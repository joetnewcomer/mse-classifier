,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,What exactly is a cofactor and how is the sign chart derived?,What exactly is a cofactor and how is the sign chart derived?,,"Given \begin{equation*} \begin{bmatrix} a_{11} & \dots & a_{1j} \\ \vdots & \ddots & \vdots \\ a_{i1} & \dots & a_{ij} \end{bmatrix} \end{equation*} I know the cofactor is $A_{ij}=M_{ij}(-1)^{i+j}$ where $M_{ij}$ is the minor of an element. But what does that even mean? What does it mean to take the cofactor of a matrix, is there a geometric visualization of this or anything? Second, where does the sign chart below come from, and why do I have to use it to take the row cofactor sum to find the determinant of the matrix? \begin{equation*} \begin{bmatrix} + & - & + \\ - & + & - \\ + & - & + \end{bmatrix} \end{equation*} I have a decent understanding of why $\Delta x=\Delta_1$ , but it's cofactor expansions and the sign chart that I don't understand (but can do/use).","Given I know the cofactor is where is the minor of an element. But what does that even mean? What does it mean to take the cofactor of a matrix, is there a geometric visualization of this or anything? Second, where does the sign chart below come from, and why do I have to use it to take the row cofactor sum to find the determinant of the matrix? I have a decent understanding of why , but it's cofactor expansions and the sign chart that I don't understand (but can do/use).","\begin{equation*}
\begin{bmatrix}
a_{11} & \dots & a_{1j} \\
\vdots & \ddots & \vdots \\
a_{i1} & \dots & a_{ij}
\end{bmatrix}
\end{equation*} A_{ij}=M_{ij}(-1)^{i+j} M_{ij} \begin{equation*}
\begin{bmatrix}
+ & - & + \\
- & + & - \\
+ & - & +
\end{bmatrix}
\end{equation*} \Delta x=\Delta_1","['linear-algebra', 'matrices', 'algebra-precalculus']"
1,linear algebra : show that $~f~$ is a linear map.,linear algebra : show that  is a linear map.,~f~,"We say that a sequence $(U_n)_{n \in \Bbb N} \subset \mathbb{R}$ is Fibonacci if it satisfies $\ U_{n+2} = U_{n+1} + U_n, \ \forall n \in \mathbb{N}$ .  Let $F$ be the set of all Fibonacci sequences. We have the function $f: F \to \mathbb{R} \times \mathbb{R}$ that exists: $f(U_n) = (U_0,U_1)$ We must demonstrate that $f$ is a linear isomorphism between $F$ and $\Bbb R \times \Bbb R$ . It's it easy to show that $f$ is a linear map. In order to prove that it is an isomorphism, we must demonstrate also that $f$ is one-to-one and and onto. For the injectivity of $f$ , we have $f$ is a linear map so all we have to show is that $\operatorname{ker}(f) = \{0 \}$ . Let $U_n \in \operatorname{ker}(f)$ : then $$ f(U_n) =(0,0) = (U_0,U_1) $$ since if $n=2:\  U_2 = 0 + 0$ and by double recurrence: $U_{n+1} = U_n = 0$ - so it's easy to show that $U_{n+2} =U_{n+1}= 0$ (the hypothesis of the exercise). Now, since $U_n = 0$ we have $\operatorname{ker}(f)= \{0\}$ then $f$ is one to one. To prove the surjectivity of $f$ we must show that $\operatorname{Im}(f) = \mathbb{R} \times \mathbb{R}$ : now the inclusion $\operatorname{Im}(f) \subseteq \mathbb{R} \times \mathbb{R}$ is trivially true but the second is not. I spend so much time without getting nothing: in sum, how can I prove that $$ \operatorname{Im}(f) \supseteq \mathbb{R} \times \mathbb{R}\;? $$","We say that a sequence is Fibonacci if it satisfies .  Let be the set of all Fibonacci sequences. We have the function that exists: We must demonstrate that is a linear isomorphism between and . It's it easy to show that is a linear map. In order to prove that it is an isomorphism, we must demonstrate also that is one-to-one and and onto. For the injectivity of , we have is a linear map so all we have to show is that . Let : then since if and by double recurrence: - so it's easy to show that (the hypothesis of the exercise). Now, since we have then is one to one. To prove the surjectivity of we must show that : now the inclusion is trivially true but the second is not. I spend so much time without getting nothing: in sum, how can I prove that","(U_n)_{n \in \Bbb N} \subset \mathbb{R} \ U_{n+2} = U_{n+1} + U_n, \ \forall n \in \mathbb{N} F f: F \to \mathbb{R} \times \mathbb{R} f(U_n) = (U_0,U_1) f F \Bbb R \times \Bbb R f f f f \operatorname{ker}(f) = \{0 \} U_n \in \operatorname{ker}(f) 
f(U_n) =(0,0) = (U_0,U_1)
 n=2:\  U_2 = 0 + 0 U_{n+1} = U_n = 0 U_{n+2} =U_{n+1}= 0 U_n = 0 \operatorname{ker}(f)= \{0\} f f \operatorname{Im}(f) = \mathbb{R} \times \mathbb{R} \operatorname{Im}(f) \subseteq \mathbb{R} \times \mathbb{R} 
\operatorname{Im}(f) \supseteq \mathbb{R} \times \mathbb{R}\;?
",['linear-algebra']
2,"Is the subspace $N=\{y\in l_\infty: y_k=x_k\ a.a.k,\ x\in M \}$ closed or not?",Is the subspace  closed or not?,"N=\{y\in l_\infty: y_k=x_k\ a.a.k,\ x\in M \}","Let $l_\infty$ be the set of all bounded real sequences. $S:l_\infty\to l_\infty$ be the shift operator defined by $S(x_1,x_2,x_3,\dots)=(x_2,x_3,x_4,\dots)$ for all $x\in l_\infty$ Let $M=\{x-Sx:x\in l_\infty\}$ . Then it is clear to me that $M$ is a linear subspace of $l_\infty$ . For a subset $P$ of $\mathbb N$ the natural/asymptotic density is defined by the limit (if it exists) $$\delta(P)=\lim\limits_{n\to \infty}\frac{|P\cap\{1,2,\dots,n\}|}{n}$$ We say a condition $\mathcal C$ is satisfied by $x_k\ a.a.k$ (for almost all k) iff $E=\{k\in\mathbb N:x_k$ does not satisfy condition $\mathcal C$ $\}\implies\delta(E)=0$ . Let $N=\{y\in l_\infty: y_k=x_k\ a.a.k,\ x\in M \}$ . Then $N$ is a linear subspace of $l_\infty$ . So, $M\subset N\subset l_\infty$ . My question : Is the subspace $N$ closed in $l_\infty$ or not?","Let be the set of all bounded real sequences. be the shift operator defined by for all Let . Then it is clear to me that is a linear subspace of . For a subset of the natural/asymptotic density is defined by the limit (if it exists) We say a condition is satisfied by (for almost all k) iff does not satisfy condition . Let . Then is a linear subspace of . So, . My question : Is the subspace closed in or not?","l_\infty S:l_\infty\to l_\infty S(x_1,x_2,x_3,\dots)=(x_2,x_3,x_4,\dots) x\in l_\infty M=\{x-Sx:x\in l_\infty\} M l_\infty P \mathbb N \delta(P)=\lim\limits_{n\to \infty}\frac{|P\cap\{1,2,\dots,n\}|}{n} \mathcal C x_k\ a.a.k E=\{k\in\mathbb N:x_k \mathcal C \}\implies\delta(E)=0 N=\{y\in l_\infty: y_k=x_k\ a.a.k,\ x\in M \} N l_\infty M\subset N\subset l_\infty N l_\infty","['linear-algebra', 'general-topology', 'functional-analysis', 'density-function']"
3,How to define pivot columns?,How to define pivot columns?,,"When you use Gaussian elimination to solve a homogeneous system of linear equations, you end up with ""pivot variables"" and ""non-pivot variables"".  The non-pivot variables have the property that they can each be chosen freely, and once specified, they uniquely determine a solution to the equation. I'm looking for a characterization of these free variables that depends on the operator $A$ and the choice of basis $\{b_i\}$ but does not refer to the Gaussian elimination process itself. For example, you could look at the equation $x_1 + x_2 + x_3 = 0$ and determine that any two of the variables can be chosen freely and uniquely determine the third, whereas one variable is insufficient and three variables is too many to be chosen freely. For another example, take $$x_0 + x_1 + x_2 = 0\\ x_0 + x_3 + x_4 = 0.$$ The set $\{x_0, x_2,x_3\}$ consists of variables that can be freely chosen and uniquely determine a solution. In contrast, $\{x_0,x_1,x_2\}$ cannot be chosen freely, despite having three variables. My goal is to find a test that can identify which sets of variables uniquely and freely determine a solution. My starting point is Gaussian elimination, where the non-pivot rows show you one such subset of variables. I would like to be able to characterize all such sets of variables, without reference to Gaussian elimination. Here's my attempt. Let $A$ be a matrix with basis $B=\{x_1,\ldots,x_m\}$ . Reduce $A$ using Gaussian elimination. Let $N$ be the submatrix of $\text{rref}(A)$ consisting of the non-pivot columns. I believe $N$ is equivalent to the kernel map for $A$ expressed in our basis, in which case it can be defined without referring to the elimination process—is that right? Consider a subset of variables $E\equiv \{e_1,\ldots, e_d\}\subseteq \{x_1,\ldots,x_m\}$ . These variables might have the desired property, or not. To determine whether $E$ has the desired property (i.e. the variables of $E$ may be chosen freely, and when they are chosen, they uniquely determine a solution to the homogeneous equation.), consider the linear map $Q:\mathbb{R}^d \hookrightarrow \mathbb{R}^m$ induced by the inclusion of $E$ into $B$ .  The requirement is that $QN$ is the identity map $I_{d\times d}$ . (Or maybe just that $QN$ is invertible.) I got this definition by trying to formalize the idea that each basis vector in $E$ meets (has a nonzero dot product with) the columns of the null matrix in exactly one unique place. That is, it forms a kind of identity sub matrix. Because it meets each vector exactly once, we know that the variables can be chosen freely and uniquely determine a solution. Is this right? Is there a better formulation? Thanks for your help. P.S. For example of how to apply this method, consider the following separate problems $A_1 = [1,1]$ , versus $A_2 = [1,0]$ .  Each of these problems is a system with two variables and one equation $(A : \mathbb{R}^m\rightarrow \mathbb{R}^n$ with $m=2$ , $n=1$ ). Each has a one-dimensional space of solutions (the nullity of $A$ is $d=1$ ). Our basis of variables $B$ consists of the standard basis vectors. Any set of $d=1$ vectors (i.e. the singleton sets $\{e_1\}$ , $\{e_2\}$ ) is a candidate for being a complete set of free variables. To test them,  we consider the inclusions of $e_1$ or $e_2$ from $\mathbb{R}^m \hookrightarrow \mathbb{R}^d$ : $$E_1 = \begin{bmatrix}1 & 0\end{bmatrix}$$ $$E_2 = \begin{bmatrix}0 & 1\end{bmatrix}$$ We will need the kernel maps of $A_1$ and $A_2$ . These are maps $\mathbb{R}^d\rightarrow \mathbb{R}^m$ . The kernel of the matrices $A_1$ and $A_2$ are, respectively: $$K_1 = \begin{bmatrix}1\\ -1\end{bmatrix}\qquad u\mapsto \langle u,-u\rangle$$ $$K_2 = \begin{bmatrix}0 \\ 1\end{bmatrix}\qquad v \mapsto \langle 0, v\rangle$$ When we test whether $E_1$ and $E_2$ are free variables for the first matrix, we find: $$E_1K_1 = [1]\\ E_2K_1 = [-1]$$ Whereas for the second matrix, we find: $$E_1K_2 = [0]\\ E_2K_2 = [1]$$ By examining which of these is an invertible transformation $\mathbb{R}^d\rightarrow \mathbb{R}^d$ , we have determined that $\{e_1\}$ and $\{e_2\}$ are complete unique variable sets for the first system $A_1$ , but only $\{e_2\}$ is a complete unique variable set for the second system $A_2$ .","When you use Gaussian elimination to solve a homogeneous system of linear equations, you end up with ""pivot variables"" and ""non-pivot variables"".  The non-pivot variables have the property that they can each be chosen freely, and once specified, they uniquely determine a solution to the equation. I'm looking for a characterization of these free variables that depends on the operator and the choice of basis but does not refer to the Gaussian elimination process itself. For example, you could look at the equation and determine that any two of the variables can be chosen freely and uniquely determine the third, whereas one variable is insufficient and three variables is too many to be chosen freely. For another example, take The set consists of variables that can be freely chosen and uniquely determine a solution. In contrast, cannot be chosen freely, despite having three variables. My goal is to find a test that can identify which sets of variables uniquely and freely determine a solution. My starting point is Gaussian elimination, where the non-pivot rows show you one such subset of variables. I would like to be able to characterize all such sets of variables, without reference to Gaussian elimination. Here's my attempt. Let be a matrix with basis . Reduce using Gaussian elimination. Let be the submatrix of consisting of the non-pivot columns. I believe is equivalent to the kernel map for expressed in our basis, in which case it can be defined without referring to the elimination process—is that right? Consider a subset of variables . These variables might have the desired property, or not. To determine whether has the desired property (i.e. the variables of may be chosen freely, and when they are chosen, they uniquely determine a solution to the homogeneous equation.), consider the linear map induced by the inclusion of into .  The requirement is that is the identity map . (Or maybe just that is invertible.) I got this definition by trying to formalize the idea that each basis vector in meets (has a nonzero dot product with) the columns of the null matrix in exactly one unique place. That is, it forms a kind of identity sub matrix. Because it meets each vector exactly once, we know that the variables can be chosen freely and uniquely determine a solution. Is this right? Is there a better formulation? Thanks for your help. P.S. For example of how to apply this method, consider the following separate problems , versus .  Each of these problems is a system with two variables and one equation with , ). Each has a one-dimensional space of solutions (the nullity of is ). Our basis of variables consists of the standard basis vectors. Any set of vectors (i.e. the singleton sets , ) is a candidate for being a complete set of free variables. To test them,  we consider the inclusions of or from : We will need the kernel maps of and . These are maps . The kernel of the matrices and are, respectively: When we test whether and are free variables for the first matrix, we find: Whereas for the second matrix, we find: By examining which of these is an invertible transformation , we have determined that and are complete unique variable sets for the first system , but only is a complete unique variable set for the second system .","A \{b_i\} x_1 + x_2 + x_3 = 0 x_0 + x_1 + x_2 = 0\\ x_0 + x_3 + x_4 = 0. \{x_0, x_2,x_3\} \{x_0,x_1,x_2\} A B=\{x_1,\ldots,x_m\} A N \text{rref}(A) N A E\equiv \{e_1,\ldots, e_d\}\subseteq \{x_1,\ldots,x_m\} E E Q:\mathbb{R}^d \hookrightarrow \mathbb{R}^m E B QN I_{d\times d} QN E A_1 = [1,1] A_2 = [1,0] (A : \mathbb{R}^m\rightarrow \mathbb{R}^n m=2 n=1 A d=1 B d=1 \{e_1\} \{e_2\} e_1 e_2 \mathbb{R}^m \hookrightarrow \mathbb{R}^d E_1 = \begin{bmatrix}1 & 0\end{bmatrix} E_2 = \begin{bmatrix}0 & 1\end{bmatrix} A_1 A_2 \mathbb{R}^d\rightarrow \mathbb{R}^m A_1 A_2 K_1 = \begin{bmatrix}1\\ -1\end{bmatrix}\qquad u\mapsto \langle u,-u\rangle K_2 = \begin{bmatrix}0 \\ 1\end{bmatrix}\qquad v \mapsto \langle 0, v\rangle E_1 E_2 E_1K_1 = [1]\\ E_2K_1 = [-1] E_1K_2 = [0]\\ E_2K_2 = [1] \mathbb{R}^d\rightarrow \mathbb{R}^d \{e_1\} \{e_2\} A_1 \{e_2\} A_2","['linear-algebra', 'category-theory', 'linear-transformations', 'systems-of-equations', 'gaussian-elimination']"
4,Why is $T: C^\infty(\mathbb{R}) \ni f \mapsto (a_n) = (f^{(n)}(0)) \in \mathbb{R}^\mathbb{N}$ surjective?,Why is  surjective?,T: C^\infty(\mathbb{R}) \ni f \mapsto (a_n) = (f^{(n)}(0)) \in \mathbb{R}^\mathbb{N},"I am reading ""Linear Algebra"" by Takeshi Saito. Let $f \in C^\infty(\mathbb{R})$ . Let $T$ be a mapping such that $T: C^\infty(\mathbb{R}) \ni f \mapsto (a_n) = (f^{(n)}(0)) \in \mathbb{R}^\mathbb{N}$ . $C^\infty(\mathbb{R})$ and $\mathbb{R}^\mathbb{N}$ are linear spaces. $T$ is a linear mapping. In this book, the author says that $T$ is surjective without a proof. Why is $T$ surjective?","I am reading ""Linear Algebra"" by Takeshi Saito. Let . Let be a mapping such that . and are linear spaces. is a linear mapping. In this book, the author says that is surjective without a proof. Why is surjective?",f \in C^\infty(\mathbb{R}) T T: C^\infty(\mathbb{R}) \ni f \mapsto (a_n) = (f^{(n)}(0)) \in \mathbb{R}^\mathbb{N} C^\infty(\mathbb{R}) \mathbb{R}^\mathbb{N} T T T,"['calculus', 'linear-algebra', 'derivatives']"
5,Cochran's Theorem,Cochran's Theorem,,"I am preparing a work on Cochran's theorem and I had two questions : First question : Is there a link between these two statements and to what extent? Would it be redundant to prove each of them separately ? Theorem $1$ (Cochran, algebraic) : Let $E$ be a euclidean space of dimension $n$ and $ u_1, ..., u_p $ be symmetric endomorphisms on E . Suppose that : $(i)$ $\text{rk}(u_1)  +\  ... \ + \text{rk}(u_p) = n$ . $(ii)$ $q_1(x) + \ ... \ + q_p (x) = \langle x,x \rangle\ $ where $q_k(x) =  \langle u_k(x),x\rangle$ . Then $E = \bigoplus\limits_{1\le i \le p}^{\perp}\text{Im}(u_i)$ . Besides, for all $i\in \{1,...,p\}$ , $u_i$ is an orthogonal projector on its image. Theorem $2$ (Cochran, probabilistic) : Let $X \sim \mathcal{N}_d (0, I_d) $ be a gaussian vector and $ \bigoplus \limits_{1\le i \le p}^{\perp} E_i = \mathbb{R}^d $ with $\dim(E_i)= d_i$ for all $i\in \{1,...,p\}$ . Then the orthogonal projections $\pi_{E_1} (X), ..., \pi_{E_p} (X) $ are independent gaussian vectors. Besides, for all $i \in \{1,...,p\}$ : $\Vert \pi_{E_i}(X)\Vert^2 \sim \chi^2(d_i)$ . Second question : Does anyone know of an application of these theorems, other than the statistical gaussian linear model tests ? Specifically is there a purely algebraic application to theorem $1$ ? Thanks in advance.","I am preparing a work on Cochran's theorem and I had two questions : First question : Is there a link between these two statements and to what extent? Would it be redundant to prove each of them separately ? Theorem (Cochran, algebraic) : Let be a euclidean space of dimension and be symmetric endomorphisms on E . Suppose that : . where . Then . Besides, for all , is an orthogonal projector on its image. Theorem (Cochran, probabilistic) : Let be a gaussian vector and with for all . Then the orthogonal projections are independent gaussian vectors. Besides, for all : . Second question : Does anyone know of an application of these theorems, other than the statistical gaussian linear model tests ? Specifically is there a purely algebraic application to theorem ? Thanks in advance.","1 E n  u_1, ..., u_p  (i) \text{rk}(u_1)  +\  ... \ + \text{rk}(u_p) = n (ii) q_1(x) + \ ... \ + q_p (x) = \langle x,x \rangle\  q_k(x) =  \langle u_k(x),x\rangle E = \bigoplus\limits_{1\le i \le p}^{\perp}\text{Im}(u_i) i\in \{1,...,p\} u_i 2 X \sim \mathcal{N}_d (0, I_d)   \bigoplus \limits_{1\le i \le p}^{\perp} E_i = \mathbb{R}^d  \dim(E_i)= d_i i\in \{1,...,p\} \pi_{E_1} (X), ..., \pi_{E_p} (X)  i \in \{1,...,p\} \Vert \pi_{E_i}(X)\Vert^2 \sim \chi^2(d_i) 1","['linear-algebra', 'probability-theory', 'statistics', 'quadratic-forms', 'bilinear-form']"
6,On exponentials of real skew-symmetric matrices,On exponentials of real skew-symmetric matrices,,"Let $M_n(\mathbb{R})$ be the algebra of $n\times n$ matrices with real coefficients, equipped with the operator norm, with respect to the $\ell^2$ norm $\displaystyle\|x\|_2=\left(\sum_{i=1}^n x_i^2\right)^{1/2}$ , namely $\displaystyle\|A\|=\sup_{\|x\|_2=1}\|Ax\|_2$ . Let $\displaystyle\exp(A):=\sum_{n=0}^\infty \frac{A^n}{n!}$ be the exponential map on $M_n(\mathbb{R})$ . Let $A(n)$ be the subspace of skew-symmetric matrices $A=-A^T$ . Let $O(n)$ be the group of orthogonal matrices $A=(A^T)^{-1}$ , and let $SO(n)$ be its subgroup made of determinant 1 matrices. Now take $1\leq k\leq n-1$ and consider: $A(k,n)=\{[a_{ij}]\in A(n)\,:\, a_{ij}=0 \mbox{ for }i\leq k<j\mbox{ and }j\leq k<i\}$ $X=\{[a_{ij}]\in A(n)\,:\, a_{ij}=0 \mbox{ for }i,j\leq k\mbox{ and }k<i,j\}$ In other terms $A(n)=A(k,n)\oplus X$ according to the block decomposition: $\begin{bmatrix}A_1&A_3\\A_2&A_4\end{bmatrix}=\begin{bmatrix}A_1&0_{k\times(n-k)}\\0_{(n-k)\times k}&A_4\end{bmatrix}+\begin{bmatrix}0_{k\times k}&A_3\\A_2&0_{(n-k)\times(n-k)}\end{bmatrix}$ Remark: It is clear that $\exp(X)\exp(A(k,n))\subset SO(n)$ . Claim: For every $T\in SO(n)$ , there exist $B\in X$ and $A\in A(k,n)$ such that $\|B\|\leq \pi$ and $\exp(B)\exp(A)=T$ . Question: How would you establish this claim? If possible, I'd like to see different approaches (diagonalization, Lie algebra arguments, ...).","Let be the algebra of matrices with real coefficients, equipped with the operator norm, with respect to the norm , namely . Let be the exponential map on . Let be the subspace of skew-symmetric matrices . Let be the group of orthogonal matrices , and let be its subgroup made of determinant 1 matrices. Now take and consider: In other terms according to the block decomposition: Remark: It is clear that . Claim: For every , there exist and such that and . Question: How would you establish this claim? If possible, I'd like to see different approaches (diagonalization, Lie algebra arguments, ...).","M_n(\mathbb{R}) n\times n \ell^2 \displaystyle\|x\|_2=\left(\sum_{i=1}^n x_i^2\right)^{1/2} \displaystyle\|A\|=\sup_{\|x\|_2=1}\|Ax\|_2 \displaystyle\exp(A):=\sum_{n=0}^\infty \frac{A^n}{n!} M_n(\mathbb{R}) A(n) A=-A^T O(n) A=(A^T)^{-1} SO(n) 1\leq k\leq n-1 A(k,n)=\{[a_{ij}]\in A(n)\,:\, a_{ij}=0 \mbox{ for }i\leq k<j\mbox{ and }j\leq k<i\} X=\{[a_{ij}]\in A(n)\,:\, a_{ij}=0 \mbox{ for }i,j\leq k\mbox{ and }k<i,j\} A(n)=A(k,n)\oplus X \begin{bmatrix}A_1&A_3\\A_2&A_4\end{bmatrix}=\begin{bmatrix}A_1&0_{k\times(n-k)}\\0_{(n-k)\times k}&A_4\end{bmatrix}+\begin{bmatrix}0_{k\times k}&A_3\\A_2&0_{(n-k)\times(n-k)}\end{bmatrix} \exp(X)\exp(A(k,n))\subset SO(n) T\in SO(n) B\in X A\in A(k,n) \|B\|\leq \pi \exp(B)\exp(A)=T","['linear-algebra', 'matrices', 'lie-groups', 'lie-algebras', 'matrix-exponential']"
7,What is the Largest Dimension of the Set of Matrices with $\text{trace} AB = 0$,What is the Largest Dimension of the Set of Matrices with,\text{trace} AB = 0,"Suppose $W$ is a subspace of $M_n(\mathbb{R})$ with property that $\text{trace}(AB) = 0$ for all $A,B \in W$ . I want to find the largest possible dimension of $W$ . It seems like the answer is $n(n - 1)/2$ , I can find such set of matrices namely the set of all triangular matrices, either upper or lower bot not both, with zero diagonal. They are of the form $$\begin{bmatrix} 0 & * & * & * & \dots & * \\  0 & 0 & * & * & \dots & *\\ 0 & 0 & 0 & * & \dots & *&\\ \vdots & \vdots & \vdots & \vdots & \ddots & *\\ 0 & 0 & 0 & 0 &\dots & 0\end{bmatrix}$$ (or the lower one). I have problem showing this is the largest dimension it can get. Clearly one of the basis of this subspace is the set of matrices with entry 1 in one of those $*$ and zero otherwise, there are $n(n - 1)/2$ such matrices. I can show that when adding one more matrix to this basis the property $\text{trace}(AB)$ can no longer hold. Is this the correct way to prove it? Because I think there's an error with this since I start with the uppertriangular then add one more(not general).","Suppose is a subspace of with property that for all . I want to find the largest possible dimension of . It seems like the answer is , I can find such set of matrices namely the set of all triangular matrices, either upper or lower bot not both, with zero diagonal. They are of the form (or the lower one). I have problem showing this is the largest dimension it can get. Clearly one of the basis of this subspace is the set of matrices with entry 1 in one of those and zero otherwise, there are such matrices. I can show that when adding one more matrix to this basis the property can no longer hold. Is this the correct way to prove it? Because I think there's an error with this since I start with the uppertriangular then add one more(not general).","W M_n(\mathbb{R}) \text{trace}(AB) = 0 A,B \in W W n(n - 1)/2 \begin{bmatrix}
0 & * & * & * & \dots & * \\ 
0 & 0 & * & * & \dots & *\\
0 & 0 & 0 & * & \dots & *&\\
\vdots & \vdots & \vdots & \vdots & \ddots & *\\
0 & 0 & 0 & 0 &\dots & 0\end{bmatrix} * n(n - 1)/2 \text{trace}(AB)","['linear-algebra', 'matrices', 'trace']"
8,Interesting applications of density to prove difficult theorems,Interesting applications of density to prove difficult theorems,,"If we wanted to prove certain statements for every element of a set $S$ , a possible approach is to prove the statement for a certain dense subset $S'\subset S$ (with respect to a certain metric), then show that if the statement is true for $S'$ then it is for $S$ . Sometimes, this approach can reduce the proof of a difficult theorem to something nearly trivial, such as the following proof of Cayley-Hamilton: Theorem. Let $A\in M_n(\mathbb C)$ , and let $p(\lambda)$ be the characteristic polynomial of $A$ . Then $p(A)=0$ . Proof. The statement is trivial for diagonal matrices. If $A$ is diagonalisable, write $A=P\Lambda P^{-1}$ for the diagonal matrix of eigenvalues $\Lambda$ . Since the characteristic polynomial of $A$ is the same as that of $\Lambda$ , we have $p(A)=Pp(\Lambda)P^{-1}$ , and since $p(\Lambda)=0$ , we have $p(A)=0$ . By the density of diagonalisable matrices in $M_n(\mathbb C)$ , we recover the general statement for all matrices in $M_n(\mathbb C)$ . What are some more interesting instances of this method to prove a difficult theorem?","If we wanted to prove certain statements for every element of a set , a possible approach is to prove the statement for a certain dense subset (with respect to a certain metric), then show that if the statement is true for then it is for . Sometimes, this approach can reduce the proof of a difficult theorem to something nearly trivial, such as the following proof of Cayley-Hamilton: Theorem. Let , and let be the characteristic polynomial of . Then . Proof. The statement is trivial for diagonal matrices. If is diagonalisable, write for the diagonal matrix of eigenvalues . Since the characteristic polynomial of is the same as that of , we have , and since , we have . By the density of diagonalisable matrices in , we recover the general statement for all matrices in . What are some more interesting instances of this method to prove a difficult theorem?",S S'\subset S S' S A\in M_n(\mathbb C) p(\lambda) A p(A)=0 A A=P\Lambda P^{-1} \Lambda A \Lambda p(A)=Pp(\Lambda)P^{-1} p(\Lambda)=0 p(A)=0 M_n(\mathbb C) M_n(\mathbb C),"['linear-algebra', 'abstract-algebra', 'algebra-precalculus', 'number-theory', 'big-list']"
9,Matrix inverse algorithm that works for any unitary ring,Matrix inverse algorithm that works for any unitary ring,,"Is there any algorithm to find out if a given square matrix has an inverse (which is both left and right inverse), and compute the inverse, if there is one for any unitary ring, without assuming that it is commutative and without assuming that it has the zero-product property? Making a system of linear equations and solving it by substitution works correctly even for a non-commutative ring, but it can get stuck if there are some non-invertible non-zero elements. Finding the inverse by dividing the adjoint by the determinant works even for rings where not all elements have an inverse, because there are division-free algorithms for adjoint and determinant and we only have to perform one inverse operation in the very end of the computation, but it works only for commutative rings.","Is there any algorithm to find out if a given square matrix has an inverse (which is both left and right inverse), and compute the inverse, if there is one for any unitary ring, without assuming that it is commutative and without assuming that it has the zero-product property? Making a system of linear equations and solving it by substitution works correctly even for a non-commutative ring, but it can get stuck if there are some non-invertible non-zero elements. Finding the inverse by dividing the adjoint by the determinant works even for rings where not all elements have an inverse, because there are division-free algorithms for adjoint and determinant and we only have to perform one inverse operation in the very end of the computation, but it works only for commutative rings.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'ring-theory', 'noncommutative-algebra']"
10,If zero is an eigenvalue are dimensions lost?,If zero is an eigenvalue are dimensions lost?,,"This is likely a silly question so sorry in advance. However, I am wondering if I am right in thinking that if zero is an eigenvalue, then some dimension must be lost. My understanding is that eigenvectors are the vectors that are only scaled when some matrix A is applied. Then, eigenvalues are the amount those vectors are scaled. So, therefore is it true that if an eigenvalue is zero, that vector has been sent to the origin and therefore the dimension of the image is smaller than the original dimension? Would that not also imply that any vector in the kernel of a transformation is an eigenvector?","This is likely a silly question so sorry in advance. However, I am wondering if I am right in thinking that if zero is an eigenvalue, then some dimension must be lost. My understanding is that eigenvectors are the vectors that are only scaled when some matrix A is applied. Then, eigenvalues are the amount those vectors are scaled. So, therefore is it true that if an eigenvalue is zero, that vector has been sent to the origin and therefore the dimension of the image is smaller than the original dimension? Would that not also imply that any vector in the kernel of a transformation is an eigenvector?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'diagonalization']"
11,Prove that $G$ contains subgroups $H_i$ such that $\dots \subset H_{-2} \subset H_{-1} \subset H_0 \subset H_1\subset H_2 \subset \dots$,Prove that  contains subgroups  such that,G H_i \dots \subset H_{-2} \subset H_{-1} \subset H_0 \subset H_1\subset H_2 \subset \dots,"Let $G$ be the group generated by the matrices $$x= \begin{pmatrix}2 & 0 \\ 0 & 1 \end{pmatrix}, \ \ \ y=\begin{pmatrix}1 & 1 \\ 0 & 1 \end{pmatrix},$$ where the group operation is matrix multiplication. Prove that $G$ contains a subgroup $H_i$ for each $i\in \mathbb{Z}$ such that $$\dots \subset H_{-2} \subset H_{-1} \subset H_0 \subset H_1\subset H_2 \subset \dots $$ Here is what I got so far: $$x^my^n=\begin{pmatrix}2^m & n2^m \\ 0 & 1 \end{pmatrix},\ \ \ y^nx^m=\begin{pmatrix}2^m & n \\ 0 & 1 \end{pmatrix}$$ but I have no idea how useful they are. I think it is easy to make infinite amount of subsets in one side, eg $\dots \subset \langle x^8 \rangle \subset \langle x^4 \rangle\subset \langle x^2 \rangle\subset \langle x \rangle$ . But no idea how to make infinite amount of subsets on both sides.","Let be the group generated by the matrices where the group operation is matrix multiplication. Prove that contains a subgroup for each such that Here is what I got so far: but I have no idea how useful they are. I think it is easy to make infinite amount of subsets in one side, eg . But no idea how to make infinite amount of subsets on both sides.","G x= \begin{pmatrix}2 & 0 \\ 0 & 1 \end{pmatrix}, \ \ \ y=\begin{pmatrix}1 & 1 \\ 0 & 1 \end{pmatrix}, G H_i i\in \mathbb{Z} \dots \subset H_{-2} \subset H_{-1} \subset H_0 \subset H_1\subset H_2 \subset \dots  x^my^n=\begin{pmatrix}2^m & n2^m \\ 0 & 1 \end{pmatrix},\ \ \ y^nx^m=\begin{pmatrix}2^m & n \\ 0 & 1 \end{pmatrix} \dots \subset \langle x^8 \rangle \subset \langle x^4 \rangle\subset \langle x^2 \rangle\subset \langle x \rangle","['linear-algebra', 'group-theory']"
12,"Commutative diagrams for vector spaces, dual spaces, and adjoint of linear maps","Commutative diagrams for vector spaces, dual spaces, and adjoint of linear maps",,"I'm looking for a commutative diagram explaining the relationship between a vector space, $X$ the dual space, $X^*$ , the double dual, $X^{**}$ . I'm also looking for a diagram explaining the relationship between a linear map $T : X \longrightarrow Y$ , $T^{\dagger}$ and $T^{\dagger \dagger }$ . Ideally, I would like all of these ideas explained in a single diagram, but I haven't been able to come up with one on my own.","I'm looking for a commutative diagram explaining the relationship between a vector space, the dual space, , the double dual, . I'm also looking for a diagram explaining the relationship between a linear map , and . Ideally, I would like all of these ideas explained in a single diagram, but I haven't been able to come up with one on my own.",X X^* X^{**} T : X \longrightarrow Y T^{\dagger} T^{\dagger \dagger },"['linear-algebra', 'dual-spaces']"
13,There exists a linear operator with no proper invariant subspaces,There exists a linear operator with no proper invariant subspaces,,"Let $A$ be a bounded operator on a Hilbert space $H$ with two invariant subspaces $M$ and $N$ s.t. $N \subset M$ , dim $(M \cap  N^{\perp})> 1$ , and have no invariant subspaces between $N$ and $M$ . Then, show that, there exists an operator $B$ on $H$ which has no proper invariant subspace. All I want a hint for constructing $B$ with the help of $A$ and given conditions, even a little hint will be appreciated. Thanks in advance.","Let be a bounded operator on a Hilbert space with two invariant subspaces and s.t. , dim , and have no invariant subspaces between and . Then, show that, there exists an operator on which has no proper invariant subspace. All I want a hint for constructing with the help of and given conditions, even a little hint will be appreciated. Thanks in advance.",A H M N N \subset M (M \cap  N^{\perp})> 1 N M B H B A,"['linear-algebra', 'functional-analysis', 'operator-algebras']"
14,Least square problem constrained to projection matrices,Least square problem constrained to projection matrices,,"Some times in engineering, it is important to find an optimum subspace in which projecting on it satisfies some properties. Let known matrices $A$ and $B$ belong to $\mathbb{R}^{p\times n}$ and $\|\cdot\|_F$ be Frobenius norm. How can I find the best subspace in which projecting $A$ on it is as close as possible to $B$ ? In other words how can I find a solution to the following constrained optimization problem? \begin{eqnarray} &&\min_P \|PA-B\|_F^2 \\  &&\mathrm{s.t. \ }P^T=P, \, P^2=P \end{eqnarray} Update: I have incorporated the symmetry property ( $P^T=T$ ) in objective function as follows.  Since $P$ is symmetric, $P$ can decompose as $P=Y+Y^T$ where $Y\in \mathbb{R}^{n\times n}$ . Now, the optimization problem reduces to \begin{eqnarray} &&\min_Y \|C(Y+Y^T)-D\|_F^2 \\  &&\mathrm{s.t. \ }\, (Y+Y^T)^2=Y+Y^T, \end{eqnarray} Where $C=A^T$ and $D=B^T$ . Further, using ""vec"" operator, we get vec $(Y^T)=$$K$ vec $(Y)$ , where $K\in \mathbb{R}^{n\times n}$ is a unique and known matrix. Using ""Kronecker product"", our optimization problem will be reduced to \begin{eqnarray} &&\min_y \|(I\otimes C)(I+K)y-d\|_2^2 \\  &&\mathrm{s.t. \ }\, (Y+Y^T)^2=Y+Y^T, \end{eqnarray} where $y=$ vec $(Y)$ and $d=$ vec $(D)$ .","Some times in engineering, it is important to find an optimum subspace in which projecting on it satisfies some properties. Let known matrices and belong to and be Frobenius norm. How can I find the best subspace in which projecting on it is as close as possible to ? In other words how can I find a solution to the following constrained optimization problem? Update: I have incorporated the symmetry property ( ) in objective function as follows.  Since is symmetric, can decompose as where . Now, the optimization problem reduces to Where and . Further, using ""vec"" operator, we get vec vec , where is a unique and known matrix. Using ""Kronecker product"", our optimization problem will be reduced to where vec and vec .","A B \mathbb{R}^{p\times n} \|\cdot\|_F A B \begin{eqnarray}
&&\min_P \|PA-B\|_F^2 \\
 &&\mathrm{s.t. \ }P^T=P, \, P^2=P
\end{eqnarray} P^T=T P P P=Y+Y^T Y\in \mathbb{R}^{n\times n} \begin{eqnarray}
&&\min_Y \|C(Y+Y^T)-D\|_F^2 \\
 &&\mathrm{s.t. \ }\, (Y+Y^T)^2=Y+Y^T,
\end{eqnarray} C=A^T D=B^T (Y^T)=K (Y) K\in \mathbb{R}^{n\times n} \begin{eqnarray}
&&\min_y \|(I\otimes C)(I+K)y-d\|_2^2 \\
 &&\mathrm{s.t. \ }\, (Y+Y^T)^2=Y+Y^T,
\end{eqnarray} y= (Y) d= (D)","['linear-algebra', 'matrices', 'linear-transformations', 'numerical-linear-algebra', 'projection']"
15,Can roots of a polynomial stay on one side of the complex plane as the coefficients vary?,Can roots of a polynomial stay on one side of the complex plane as the coefficients vary?,,"Suppose a fixed $n^{\text{th}}$ degree monic polynomial is given $$ p(x) = x^{n} + a_{n-1} x^{n-1} + \dots + a_0,$$ with coefficients vector $a = (a_{n-1}, \dots , a_0) \in \mathbb R^n$ . Now we consider a parameterized family \begin{align*} p(x, r) = x^{n} + ra_{n-1} x^{n-1} + \dots + ra_0, \end{align*} with $r \in \mathbb R$ . Suppose for some $r_0 \in \mathbb R$ , we have $p(i \alpha, r_0) = 0$ , i.e., $p(x, r_0)$ has a zero on the imaginary axis. Let us further assume for some $\delta > 0$ , we have $p(x, r)$ has all its zeros on the right half plane of $\mathbb C$ for each $r \in (r_0, r_0 + \delta)$ . My question is: is it possible that for all $r \in (r_0 - \delta, r_0)$ we also have $p(x, r)$ has all its zeros on the right half plane (we may include the imaginary axis). That is, is there a monic polynomial, that allows us to parametrize as above, such that the parametrized family has zeros touching the imaginary axis but all the zeros are confined in the right half plane when we vary $r$ continuously? As commented by @saulspatz, $x^2 + r$ will have all its zeros on the imaginary axis if $r < 0$ . I was in mind asking the case that at least there exists some $r \in (r_0 - \delta, r_0)$ such that some of the roots of $p(x, r)$ will move off the imaginary axis.","Suppose a fixed degree monic polynomial is given with coefficients vector . Now we consider a parameterized family with . Suppose for some , we have , i.e., has a zero on the imaginary axis. Let us further assume for some , we have has all its zeros on the right half plane of for each . My question is: is it possible that for all we also have has all its zeros on the right half plane (we may include the imaginary axis). That is, is there a monic polynomial, that allows us to parametrize as above, such that the parametrized family has zeros touching the imaginary axis but all the zeros are confined in the right half plane when we vary continuously? As commented by @saulspatz, will have all its zeros on the imaginary axis if . I was in mind asking the case that at least there exists some such that some of the roots of will move off the imaginary axis.","n^{\text{th}}  p(x) = x^{n} + a_{n-1} x^{n-1} + \dots + a_0, a = (a_{n-1}, \dots , a_0) \in \mathbb R^n \begin{align*}
p(x, r) = x^{n} + ra_{n-1} x^{n-1} + \dots + ra_0,
\end{align*} r \in \mathbb R r_0 \in \mathbb R p(i \alpha, r_0) = 0 p(x, r_0) \delta > 0 p(x, r) \mathbb C r \in (r_0, r_0 + \delta) r \in (r_0 - \delta, r_0) p(x, r) r x^2 + r r < 0 r \in (r_0 - \delta, r_0) p(x, r)","['linear-algebra', 'abstract-algebra', 'algebraic-geometry', 'polynomials']"
16,Functors that aren't continuous/smooth,Functors that aren't continuous/smooth,,"Let $\mathbf{FinVec}$ denote the category of finite-dimensional real vector spaces and linear maps, let $F\colon\mathbf{FinVec}\rightarrow\mathbf{FinVec}$ be a functor. For each pair of objects $V,W$ , the Hom-set $\operatorname{Hom}(V,W)$ is itself a finite-dimensional vector space and we equip it with the unique structure of a topological space/of a smooth manifold that is compatible with its vector space structure. The composition maps are then continuous/smooth. In this way, we view $\mathbf{FinVec}$ as enriched over the category $\mathbf{Top}$ / $\mathbf{Diff}$ respectively. It then makes sense to ask whether the functor $F$ is enriched over $\mathbf{Top}$ / $\mathbf{Diff}$ . That is just to ask whether the induced map $F\colon\operatorname{Hom}(V,W)\rightarrow\operatorname{Hom}(FV,FW)$ is continuous/smooth for all $V,W$ . (Irrelevant to the rest of the question, but the reason this is a useful notion is that it is the natural hypothesis assuring that the functor induces another functor on the category of topological/smooth vector bundles, applying the original functor fiberwise.) In the case $V=W$ , by functoriality, we can restrict $F$ to a group homomorphism $\operatorname{GL}(V)\rightarrow\operatorname{GL}(V)$ . If the functor is continuous/smooth, so is this map. Note that $\operatorname{GL}(V)$ is a Lie group. It is a general result that a measurable group homomorphism between Lie groups is automatically smooth. This begs the following questions: What's an argument for the existence of functors that aren't continuous? Can we construct one? Given the above, it either happens that the restrictions to $\operatorname{GL}(V)\rightarrow\operatorname{GL}(V)$ are continuous and continuity fails elsewhere, or the induced maps will be non-measurable, so that the construction will necessarily have to involve choice to some extent. Are there continuous functors that aren't smooth? Given the above, a continuous functor will have smooth restrictions to $\operatorname{GL}(V)\rightarrow\operatorname{GL}(V)$ , but smoothness may fail elsewhere.","Let denote the category of finite-dimensional real vector spaces and linear maps, let be a functor. For each pair of objects , the Hom-set is itself a finite-dimensional vector space and we equip it with the unique structure of a topological space/of a smooth manifold that is compatible with its vector space structure. The composition maps are then continuous/smooth. In this way, we view as enriched over the category / respectively. It then makes sense to ask whether the functor is enriched over / . That is just to ask whether the induced map is continuous/smooth for all . (Irrelevant to the rest of the question, but the reason this is a useful notion is that it is the natural hypothesis assuring that the functor induces another functor on the category of topological/smooth vector bundles, applying the original functor fiberwise.) In the case , by functoriality, we can restrict to a group homomorphism . If the functor is continuous/smooth, so is this map. Note that is a Lie group. It is a general result that a measurable group homomorphism between Lie groups is automatically smooth. This begs the following questions: What's an argument for the existence of functors that aren't continuous? Can we construct one? Given the above, it either happens that the restrictions to are continuous and continuity fails elsewhere, or the induced maps will be non-measurable, so that the construction will necessarily have to involve choice to some extent. Are there continuous functors that aren't smooth? Given the above, a continuous functor will have smooth restrictions to , but smoothness may fail elsewhere.","\mathbf{FinVec} F\colon\mathbf{FinVec}\rightarrow\mathbf{FinVec} V,W \operatorname{Hom}(V,W) \mathbf{FinVec} \mathbf{Top} \mathbf{Diff} F \mathbf{Top} \mathbf{Diff} F\colon\operatorname{Hom}(V,W)\rightarrow\operatorname{Hom}(FV,FW) V,W V=W F \operatorname{GL}(V)\rightarrow\operatorname{GL}(V) \operatorname{GL}(V) \operatorname{GL}(V)\rightarrow\operatorname{GL}(V) \operatorname{GL}(V)\rightarrow\operatorname{GL}(V)",['category-theory']
17,Show the Polar Factor is the Closest Unitary Matrix Using the Spectral Norm,Show the Polar Factor is the Closest Unitary Matrix Using the Spectral Norm,,"For a square matrix $A \in \mathbb{C}^{n \times n}$ with the singular value decomposition $A = U\Sigma V^*$ , I want to show that $$\|A - P \|_{2} \leq \|A -W \|_{2}$$ Where $P = UV^{*}$ and $W$ is an arbitrary unitary matrix. It is immediately clear to me that $$\|A - P \|_{2} = \|U\Sigma V^* - UV^{*}\|_2 = \|\Sigma - I \|_2 $$ I also know that the singular values of all unitary matrices are all one. However, I don't know how to combine this fact with properties of the spectral norm to get a proof, if this is indeed the right way.","For a square matrix with the singular value decomposition , I want to show that Where and is an arbitrary unitary matrix. It is immediately clear to me that I also know that the singular values of all unitary matrices are all one. However, I don't know how to combine this fact with properties of the spectral norm to get a proof, if this is indeed the right way.",A \in \mathbb{C}^{n \times n} A = U\Sigma V^* \|A - P \|_{2} \leq \|A -W \|_{2} P = UV^{*} W \|A - P \|_{2} = \|U\Sigma V^* - UV^{*}\|_2 = \|\Sigma - I \|_2 ,"['linear-algebra', 'matrices', 'normed-spaces']"
18,"Name of a particular matrix with $M_{ij}=t_{\min(i,j)}$?",Name of a particular matrix with ?,"M_{ij}=t_{\min(i,j)}","I'm looking to see if there's a name for a particular type of matrix $M_{ij}=t_{\min(i,j)}$ , ie.: \begin{bmatrix} t_1 & t_1 & t_1 & t_1 \\ t_1 & t_2 & t_2 & t_2 & \cdots\\ t_1 & t_2 & t_3 & t_3 \\ t_1 & t_2 & t_3 & t_4 \\   & \vdots & & & \ddots  \end{bmatrix} Such a matrix has determinant $t(t_2-t_1)(t_3-t_2)(t_4-t_3)\cdots$ and its inverse is a very simple tridiagonal matrix. But it isn't a Vandermonde matrix or a Moore matrix . It looks like it's an alternant matrix , but that doesn't capture any of the interesting properties of the determinant or inverse. It seems like something with these special properties should be named or well-known somewhere. This matrix came up in looking at a particular probabilistic process, where $P(x_1,t_1 ; x_2,t_2;\cdots)\propto \exp(-\frac{1}{2}\vec{x}^T M^{-1}\vec{x})$ (hence the significance of the simple tridiagonal structure of $M^{-1}$ ).","I'm looking to see if there's a name for a particular type of matrix , ie.: Such a matrix has determinant and its inverse is a very simple tridiagonal matrix. But it isn't a Vandermonde matrix or a Moore matrix . It looks like it's an alternant matrix , but that doesn't capture any of the interesting properties of the determinant or inverse. It seems like something with these special properties should be named or well-known somewhere. This matrix came up in looking at a particular probabilistic process, where (hence the significance of the simple tridiagonal structure of ).","M_{ij}=t_{\min(i,j)} \begin{bmatrix}
t_1 & t_1 & t_1 & t_1 \\
t_1 & t_2 & t_2 & t_2 & \cdots\\
t_1 & t_2 & t_3 & t_3 \\
t_1 & t_2 & t_3 & t_4 \\ 
 & \vdots & & & \ddots 
\end{bmatrix} t(t_2-t_1)(t_3-t_2)(t_4-t_3)\cdots P(x_1,t_1 ; x_2,t_2;\cdots)\propto \exp(-\frac{1}{2}\vec{x}^T M^{-1}\vec{x}) M^{-1}","['linear-algebra', 'matrices', 'terminology']"
19,Characterisation of matrices whose real eigenvalues are positive,Characterisation of matrices whose real eigenvalues are positive,,"My question is the following Is there a characterisation of $n\times n$ matrices with real entries whose real eigenvalues are positive? I am interested in this question because I am analysing some matrices where I know the determinant up to a sign, and I want to show that the sign must be positive. These matrices often have complex eigenvalues, but because the entries are real these eigenvalues will appear in conjugate pairs. So when computing the determinant as the product of the eigenvalues, these complex conjugate pairs will multiply together to make positive quantities (zero is never an eigenvalue of the matrices I am studying). So I want to show that the remaining real eigenvalues must be positive. I understand that there is a possibility that some of the real eigenvalues could be negative and the determinant would still be positive, but I suspect that they are all going to be positive. In order to see if this is true, I am looking for a characterisation of such matrices (as in my question). Any ideas or references to known results about this question are much appreciated. Edit: I found a sufficient condition (although, I'm fairly certain it is not necessary): If for each $0\leq k\leq n$ the sum of the determinants of the principal $k\times k$ minors is positive, then the matrix has no negative eigenvalues.","My question is the following Is there a characterisation of matrices with real entries whose real eigenvalues are positive? I am interested in this question because I am analysing some matrices where I know the determinant up to a sign, and I want to show that the sign must be positive. These matrices often have complex eigenvalues, but because the entries are real these eigenvalues will appear in conjugate pairs. So when computing the determinant as the product of the eigenvalues, these complex conjugate pairs will multiply together to make positive quantities (zero is never an eigenvalue of the matrices I am studying). So I want to show that the remaining real eigenvalues must be positive. I understand that there is a possibility that some of the real eigenvalues could be negative and the determinant would still be positive, but I suspect that they are all going to be positive. In order to see if this is true, I am looking for a characterisation of such matrices (as in my question). Any ideas or references to known results about this question are much appreciated. Edit: I found a sufficient condition (although, I'm fairly certain it is not necessary): If for each the sum of the determinants of the principal minors is positive, then the matrix has no negative eigenvalues.",n\times n 0\leq k\leq n k\times k,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
20,Determinant of matrix formed from blocks of a $2 \times 2$ block partitioned symplectic matrix.,Determinant of matrix formed from blocks of a  block partitioned symplectic matrix.,2 \times 2,"While working on a problem in quantum optics, I came across the following determinant of a complex matrix of size $n \times n$ : $$\mathbb{G}=\det\left[\mathcal{U}_{11}^{}+\mathcal{U}_{12}^{}\mathcal{D}_{S}^{}\right]$$ where $\mathcal{U}_{xy}^{}$ are blocks of a $2 \times 2$ block partitioned complex Symplectic matrix defined as, $$\begin{pmatrix}\mathcal{U}_{11}^{} & \mathcal{U}_{12}^{} \\ \mathcal{U}_{21}^{} & \mathcal{U}_{22}^{}\end{pmatrix}:=\mathcal{U}_{2n \times 2n}^{}=e_{}^{-\mathcal{H}^{}\Sigma_{}^{}}$$ with $\mathcal{H}^{}\Sigma_{}^{}$ being a complex Hamiltonian matrix expressed as a product of complex $2 \times 2$ block partitioned symmetric matrix $$\mathcal{H}^{}=\begin{pmatrix}\mathcal{A}_{n \times n}^{}=\mathcal{A}_{n \times n}^{T} & \mathcal{B}_{n \times n}^{} \\ \mathcal{B}_{n \times n}^{T} & \mathcal{D}_{n \times n}^{}=\mathcal{D}_{n \times n}^{T}\end{pmatrix}$$ and the standard symplectic matrix $$\Sigma_{}^{}=\begin{pmatrix}\mathbb{O}_{n \times n}^{} & \mathbb{I}_{n \times n}^{} \\ -\mathbb{I}_{n \times n}^{} & \mathbb{O}_{n \times n}^{}\end{pmatrix}.$$ Further $\mathcal{D}_{S}^{}$ is a $n \times n$ complex symmetric matrix and . $\mathbb{O}_{n \times n}^{}$ , $\mathbb{I}_{n \times n}^{}$ are respectively null and identity matrices of dimension $n \times n$ . $\mathbf{Question :}$ Is it possible to express $\mathbb{G}$ in terms of expression involving traces or determinants of sums of products of $\mathcal{A}_{n \times n}^{}$ , $\mathcal{B}_{n \times n}^{}$ , $\mathcal{D}_{n \times n}^{}$ , $\mathcal{D}_{S}^{}$ matrices and eigenvalues of the symplectic matrix $\mathcal{H}^{}\Sigma_{}^{}$ without explicit computation of $\mathcal{U}_{2n \times 2n}^{}$ , in an elegant general form? (I could do this for $n=2$ case using combination of Cayley–Hamilton theorem and Faddeev-Leverrier algorithm . For general case of $n$ , I am not able to achieve this task. Without any rigorous reasoning, I suspect some elegant expression for $\mathbb{G}$ can be given. Is this feasible?)","While working on a problem in quantum optics, I came across the following determinant of a complex matrix of size : where are blocks of a block partitioned complex Symplectic matrix defined as, with being a complex Hamiltonian matrix expressed as a product of complex block partitioned symmetric matrix and the standard symplectic matrix Further is a complex symmetric matrix and . , are respectively null and identity matrices of dimension . Is it possible to express in terms of expression involving traces or determinants of sums of products of , , , matrices and eigenvalues of the symplectic matrix without explicit computation of , in an elegant general form? (I could do this for case using combination of Cayley–Hamilton theorem and Faddeev-Leverrier algorithm . For general case of , I am not able to achieve this task. Without any rigorous reasoning, I suspect some elegant expression for can be given. Is this feasible?)",n \times n \mathbb{G}=\det\left[\mathcal{U}_{11}^{}+\mathcal{U}_{12}^{}\mathcal{D}_{S}^{}\right] \mathcal{U}_{xy}^{} 2 \times 2 \begin{pmatrix}\mathcal{U}_{11}^{} & \mathcal{U}_{12}^{} \\ \mathcal{U}_{21}^{} & \mathcal{U}_{22}^{}\end{pmatrix}:=\mathcal{U}_{2n \times 2n}^{}=e_{}^{-\mathcal{H}^{}\Sigma_{}^{}} \mathcal{H}^{}\Sigma_{}^{} 2 \times 2 \mathcal{H}^{}=\begin{pmatrix}\mathcal{A}_{n \times n}^{}=\mathcal{A}_{n \times n}^{T} & \mathcal{B}_{n \times n}^{} \\ \mathcal{B}_{n \times n}^{T} & \mathcal{D}_{n \times n}^{}=\mathcal{D}_{n \times n}^{T}\end{pmatrix} \Sigma_{}^{}=\begin{pmatrix}\mathbb{O}_{n \times n}^{} & \mathbb{I}_{n \times n}^{} \\ -\mathbb{I}_{n \times n}^{} & \mathbb{O}_{n \times n}^{}\end{pmatrix}. \mathcal{D}_{S}^{} n \times n \mathbb{O}_{n \times n}^{} \mathbb{I}_{n \times n}^{} n \times n \mathbf{Question :} \mathbb{G} \mathcal{A}_{n \times n}^{} \mathcal{B}_{n \times n}^{} \mathcal{D}_{n \times n}^{} \mathcal{D}_{S}^{} \mathcal{H}^{}\Sigma_{}^{} \mathcal{U}_{2n \times 2n}^{} n=2 n \mathbb{G},"['linear-algebra', 'matrices', 'symmetric-matrices', 'symplectic-linear-algebra']"
21,Can this basic fact in linear algebra be formulated as a completeness result?,Can this basic fact in linear algebra be formulated as a completeness result?,,"Suppose we have a system of linear equations $S$. If $E$ is some linear equation, we'll say that $E$ is a logical consequence of $S$ if whenever a vector satisfies $S$, it satisfies $E$. Now, in a first course in linear algebra we're taught one simple way to prove that an equation $E$ is a logical consequence of $S$: express it as a linear combination of equations in $S$. But notice that in theory, there could be equations $E$ which are logical consequences of $S$, and yet are not linear combinations of equations in $S$: maybe linear consequence is a strictly stronger notion that mere logical consequence . Well, it turns out this isn't the case . Taking linear combinations is a sufficiently powerful technique to derive all equations which follow from $S$. Now, this definitely feels like a kind of logical completeness result. We could say that we have the following axioms: the equations of $S$, and the axiom that if two equations are true, any linear combination of them is true. What we've shown is that this set of axioms is complete: it proves all true statements in this formal system. Can this be formally stated as an actual logical completeness result? Could somebody explain the finer details of how to do that?","Suppose we have a system of linear equations $S$. If $E$ is some linear equation, we'll say that $E$ is a logical consequence of $S$ if whenever a vector satisfies $S$, it satisfies $E$. Now, in a first course in linear algebra we're taught one simple way to prove that an equation $E$ is a logical consequence of $S$: express it as a linear combination of equations in $S$. But notice that in theory, there could be equations $E$ which are logical consequences of $S$, and yet are not linear combinations of equations in $S$: maybe linear consequence is a strictly stronger notion that mere logical consequence . Well, it turns out this isn't the case . Taking linear combinations is a sufficiently powerful technique to derive all equations which follow from $S$. Now, this definitely feels like a kind of logical completeness result. We could say that we have the following axioms: the equations of $S$, and the axiom that if two equations are true, any linear combination of them is true. What we've shown is that this set of axioms is complete: it proves all true statements in this formal system. Can this be formally stated as an actual logical completeness result? Could somebody explain the finer details of how to do that?",,"['linear-algebra', 'logic']"
22,Multi-dimensional Integration,Multi-dimensional Integration,,"Let $C=\sum_{s=1}^tx_sx_s'+aD^{-1}$, which is symmetric positive definite, vector $x,w\in\mathbb{R}^n$ and a scalar $y\in\mathbb{R}$. The integral I am trying to solve is as follows: $$\sqrt{\frac{a\sqrt{\sigma^2}}{2\pi}}\frac{\int_{\mathbb{R}^n}e^{-\frac{a\sqrt{\sigma^2}}{2}(w' x_{t+1}-y)^2}e^{-\frac{a\sqrt{\sigma^2}}{2}\sum_{s}(w'x_s-y_s)^2-\frac{a^2\sqrt{{\sigma^2}}}{2} w' D^{-1}w}\mathrm{d}w}{\int_{\mathbb{R}^n}e^{-\frac{a\sqrt{\sigma^2}}{2}\sum_{s}(w'x_s-y_s)^2-\frac{a^2\sqrt{{\sigma^2}}}{2} w' D^{-1}w}\mathrm{d}w}$$ By using the arguments mentioned here I can solve the integral in the denominator by letting $b=2\sum_{s=1}^tx_sy_s$ and $\eta=\frac{a\sqrt{\sigma^2}}{2}$: $\int_{\mathbb{R}^n}e^{-\eta w'Cw+\eta b'w-\eta\sum_{s}y_s^2}\mathrm{d}w=e^{-\eta\sum_{s}y_s^2}\prod_{i=1}^n\int_{\mathbb{R}}e^{-\eta\mu^i(w^{i})^2+\eta w^i(Pb)^i}\mathrm{d}w^i=e^{-\eta \sum_{s}y_s^2}\prod_{i=1}^n\left(\frac{\sqrt{\pi}e^{(\eta b^i)^2/4\eta\mu^i}}{\sqrt{\eta\mu^i}}\right)=e^{-\eta \sum_{s}y_s^2}\frac{\pi^{n/2}e^{\frac{\eta^2 b'b}{4\eta (\mu^1+...+\mu^n) }}}{\sqrt{\det \eta C}}$ To solve the numerator I write: $$\int_{\mathbb{R}^n} e^{-\eta\left(\sum_sy_s-y^2\right)+\eta\left(\sum_s y_s x_s- yx_{t+1}\right)'w-\eta w'\left(aD^{-1}+\sum_s x_tx_t'+x_{t+1}x_{t+1}'\right)w}\mathrm{d}w $$$$=\int_{\mathbb{R}^n} e^{-\eta\left(\sum_sy_s-y^2\right)+2\eta\left(\sum_s y_s x_s- yx_{t+1}\right)'w-\eta w'\left(aD^{-1}+\sum_s^{t+1} x_tx_t'\right)w}\mathrm{d}w$$ By using similar arguments as used to solve the denominator I write: $$e^{-\eta \sum_{s}y_s^2-\eta y^2}\frac{\pi^{n/2}e^{\frac{\eta^2 b++'b++}{4\eta (\mu^1+...+\mu^n+x^1+...+x^n)}}}{\sqrt{\det 4\eta C++}}$$ So, I am left with the following: $\sqrt{\frac{\eta}{\pi}}\left(e^{-\eta \sum_{s}y_s^2-\eta  y^2}\frac{\pi^{n/2}e^{\frac{\eta^2 b++'b++}{4\eta (\mu^1+...+\mu^n+x^1+...+x^n)}}}{\sqrt{\det C++}}\div e^{-\eta \sum_{s}y_s^2}\frac{\pi^{n/2}e^{\frac{\eta^2 b'b}{4\eta (\mu^1+...+\mu^n)}}}{\sqrt{\det 4\eta C}}\right)=\sqrt{\frac{\eta}{\pi}}\frac{e^{\frac{\eta y^2+(\eta^2 b++'b++)}{{4\eta (\mu^1+...+\mu^n+x^1+...+x^n)}}-\frac{\eta^2 b'b}{{4\eta (\mu^1+...+\mu^n)}}}\sqrt{\det 4\eta C}}{\sqrt{\det 4\eta C++}}$ where $C++=aD^{-1}+\sum_s^{t+1} x_tx_t'$ and $b++=2\sum_s y_s x_s- 2yx_{t+1}$. So I write: $$\sqrt{\frac{\eta}{\pi}}\frac{e^{\frac{\eta y^2+(\eta^2 b++'b++)}{{4\eta (\mu^1+...+\mu^n+x^1+...+x^n)}}-\frac{\eta^2 b'b}{{4\eta (\mu^1+...+\mu^n)}}}\sqrt{\det 4\eta C}}{\sqrt{\det 4\eta C++}}=\sqrt{\frac{\eta}{\pi}}\left(\frac{e^{\frac{\eta y^2 +\eta^2(b-2yx_{t+1})'(b-2yx_{t+1})}{4\eta (\mu^1+...+\mu^n+x^1+...+x^n)}-\frac{\eta^2b'b}{4\eta (\mu^1+...+\mu^n)}}\sqrt{\det C}}{\sqrt{\det (C+x_{t+1}x_{t+1}')}}\right)$$ Maybe I can proceed further by using the information given here and ( assuming $x_{t+1}x_{t+1}'$ is positive definite(Ideally I would like to avoid this) ) write:   $$\sqrt{\frac{\eta}{\pi}}\left(\frac{e^{\frac{\eta y^2 +\eta^2(b-2yx_{t+1})'(b-2yx_{t+1})}{4\eta (\mu^1+...+\mu^n+x^1+...+x^n)}-\frac{\eta^2b'b}{4\eta (\mu^1+...+\mu^n)}}\sqrt{\det C}}{\sqrt{\det (C+x_{t+1}x_{t+1}')}}\right)=\sqrt{\frac{\eta}{\pi}}\left(\frac{e^{\frac{\eta y^2 +\eta^2(b-2yx_{t+1})'(b-2yx_{t+1})}{4\eta (\mu^1+...+\mu^n+x^1+...+x^n)}-\frac{\eta^2b'b}{4\eta (\mu^1+...+\mu^n+x^1+...+x^n)}}\sqrt{\det C}}{\sqrt{(4\eta)^n\det C\det(I+L'C^{-1}L)}}\right)=\sqrt{\frac{\eta}{\pi}}\left(\frac{e^{\frac{\eta y^2 +\eta^2(b-2yx_{t+1})'(b-2yx_{t+1})}{4\eta (\mu^1+...+\mu^n+x^1+...+x^n)}-\frac{\eta^2b'b}{4\eta (\mu^1+...+\mu^n)}}}{\sqrt{(4\eta)^n\det(I+L'C^{-1}L)}}\right)=\sqrt{\frac{\eta}{\pi}}\left(\frac{e^{\frac{\eta y^2 +\eta^2(b-2yx_{t+1})'(b-2yx_{t+1})-\eta^2b'b4\eta (x^1+...+x^n)}{4\eta (\mu^1+...+\mu^n+x^1+...+x^n)}}}{\sqrt{(4\eta)^n\det(I+L'C^{-1}L)}}\right)$$ Can someone please tell me: what I have done so far is correct and how can I proceed to simplify whats left? Remark: I am hoping to get a Gaussian density. Once I solve, then I can analytically get the mean and variance. It may be worth noting that $C+x_{t+1}x_{t+1}$ is also symmetric and positive definite.","Let $C=\sum_{s=1}^tx_sx_s'+aD^{-1}$, which is symmetric positive definite, vector $x,w\in\mathbb{R}^n$ and a scalar $y\in\mathbb{R}$. The integral I am trying to solve is as follows: $$\sqrt{\frac{a\sqrt{\sigma^2}}{2\pi}}\frac{\int_{\mathbb{R}^n}e^{-\frac{a\sqrt{\sigma^2}}{2}(w' x_{t+1}-y)^2}e^{-\frac{a\sqrt{\sigma^2}}{2}\sum_{s}(w'x_s-y_s)^2-\frac{a^2\sqrt{{\sigma^2}}}{2} w' D^{-1}w}\mathrm{d}w}{\int_{\mathbb{R}^n}e^{-\frac{a\sqrt{\sigma^2}}{2}\sum_{s}(w'x_s-y_s)^2-\frac{a^2\sqrt{{\sigma^2}}}{2} w' D^{-1}w}\mathrm{d}w}$$ By using the arguments mentioned here I can solve the integral in the denominator by letting $b=2\sum_{s=1}^tx_sy_s$ and $\eta=\frac{a\sqrt{\sigma^2}}{2}$: $\int_{\mathbb{R}^n}e^{-\eta w'Cw+\eta b'w-\eta\sum_{s}y_s^2}\mathrm{d}w=e^{-\eta\sum_{s}y_s^2}\prod_{i=1}^n\int_{\mathbb{R}}e^{-\eta\mu^i(w^{i})^2+\eta w^i(Pb)^i}\mathrm{d}w^i=e^{-\eta \sum_{s}y_s^2}\prod_{i=1}^n\left(\frac{\sqrt{\pi}e^{(\eta b^i)^2/4\eta\mu^i}}{\sqrt{\eta\mu^i}}\right)=e^{-\eta \sum_{s}y_s^2}\frac{\pi^{n/2}e^{\frac{\eta^2 b'b}{4\eta (\mu^1+...+\mu^n) }}}{\sqrt{\det \eta C}}$ To solve the numerator I write: $$\int_{\mathbb{R}^n} e^{-\eta\left(\sum_sy_s-y^2\right)+\eta\left(\sum_s y_s x_s- yx_{t+1}\right)'w-\eta w'\left(aD^{-1}+\sum_s x_tx_t'+x_{t+1}x_{t+1}'\right)w}\mathrm{d}w $$$$=\int_{\mathbb{R}^n} e^{-\eta\left(\sum_sy_s-y^2\right)+2\eta\left(\sum_s y_s x_s- yx_{t+1}\right)'w-\eta w'\left(aD^{-1}+\sum_s^{t+1} x_tx_t'\right)w}\mathrm{d}w$$ By using similar arguments as used to solve the denominator I write: $$e^{-\eta \sum_{s}y_s^2-\eta y^2}\frac{\pi^{n/2}e^{\frac{\eta^2 b++'b++}{4\eta (\mu^1+...+\mu^n+x^1+...+x^n)}}}{\sqrt{\det 4\eta C++}}$$ So, I am left with the following: $\sqrt{\frac{\eta}{\pi}}\left(e^{-\eta \sum_{s}y_s^2-\eta  y^2}\frac{\pi^{n/2}e^{\frac{\eta^2 b++'b++}{4\eta (\mu^1+...+\mu^n+x^1+...+x^n)}}}{\sqrt{\det C++}}\div e^{-\eta \sum_{s}y_s^2}\frac{\pi^{n/2}e^{\frac{\eta^2 b'b}{4\eta (\mu^1+...+\mu^n)}}}{\sqrt{\det 4\eta C}}\right)=\sqrt{\frac{\eta}{\pi}}\frac{e^{\frac{\eta y^2+(\eta^2 b++'b++)}{{4\eta (\mu^1+...+\mu^n+x^1+...+x^n)}}-\frac{\eta^2 b'b}{{4\eta (\mu^1+...+\mu^n)}}}\sqrt{\det 4\eta C}}{\sqrt{\det 4\eta C++}}$ where $C++=aD^{-1}+\sum_s^{t+1} x_tx_t'$ and $b++=2\sum_s y_s x_s- 2yx_{t+1}$. So I write: $$\sqrt{\frac{\eta}{\pi}}\frac{e^{\frac{\eta y^2+(\eta^2 b++'b++)}{{4\eta (\mu^1+...+\mu^n+x^1+...+x^n)}}-\frac{\eta^2 b'b}{{4\eta (\mu^1+...+\mu^n)}}}\sqrt{\det 4\eta C}}{\sqrt{\det 4\eta C++}}=\sqrt{\frac{\eta}{\pi}}\left(\frac{e^{\frac{\eta y^2 +\eta^2(b-2yx_{t+1})'(b-2yx_{t+1})}{4\eta (\mu^1+...+\mu^n+x^1+...+x^n)}-\frac{\eta^2b'b}{4\eta (\mu^1+...+\mu^n)}}\sqrt{\det C}}{\sqrt{\det (C+x_{t+1}x_{t+1}')}}\right)$$ Maybe I can proceed further by using the information given here and ( assuming $x_{t+1}x_{t+1}'$ is positive definite(Ideally I would like to avoid this) ) write:   $$\sqrt{\frac{\eta}{\pi}}\left(\frac{e^{\frac{\eta y^2 +\eta^2(b-2yx_{t+1})'(b-2yx_{t+1})}{4\eta (\mu^1+...+\mu^n+x^1+...+x^n)}-\frac{\eta^2b'b}{4\eta (\mu^1+...+\mu^n)}}\sqrt{\det C}}{\sqrt{\det (C+x_{t+1}x_{t+1}')}}\right)=\sqrt{\frac{\eta}{\pi}}\left(\frac{e^{\frac{\eta y^2 +\eta^2(b-2yx_{t+1})'(b-2yx_{t+1})}{4\eta (\mu^1+...+\mu^n+x^1+...+x^n)}-\frac{\eta^2b'b}{4\eta (\mu^1+...+\mu^n+x^1+...+x^n)}}\sqrt{\det C}}{\sqrt{(4\eta)^n\det C\det(I+L'C^{-1}L)}}\right)=\sqrt{\frac{\eta}{\pi}}\left(\frac{e^{\frac{\eta y^2 +\eta^2(b-2yx_{t+1})'(b-2yx_{t+1})}{4\eta (\mu^1+...+\mu^n+x^1+...+x^n)}-\frac{\eta^2b'b}{4\eta (\mu^1+...+\mu^n)}}}{\sqrt{(4\eta)^n\det(I+L'C^{-1}L)}}\right)=\sqrt{\frac{\eta}{\pi}}\left(\frac{e^{\frac{\eta y^2 +\eta^2(b-2yx_{t+1})'(b-2yx_{t+1})-\eta^2b'b4\eta (x^1+...+x^n)}{4\eta (\mu^1+...+\mu^n+x^1+...+x^n)}}}{\sqrt{(4\eta)^n\det(I+L'C^{-1}L)}}\right)$$ Can someone please tell me: what I have done so far is correct and how can I proceed to simplify whats left? Remark: I am hoping to get a Gaussian density. Once I solve, then I can analytically get the mean and variance. It may be worth noting that $C+x_{t+1}x_{t+1}$ is also symmetric and positive definite.",,"['calculus', 'linear-algebra', 'integration', 'fractions', 'matrix-calculus']"
23,Finding the sub-algebras (up to conjugation) of $\frak{sl}_2(\mathbb{C})$,Finding the sub-algebras (up to conjugation) of,\frak{sl}_2(\mathbb{C}),"I am trying to find the connected subgroups of the Lie Group $SL_2(\mathbb{C})$ up to conjugation. My thought was to find the sub-algebras of the Lie Algebra $\frak{sl}_2(\mathbb{C})$, and then exponentiate. For the one dimensional sub algebras, it should just be determined by the Jordan form. I am a little stuck with the 2-dimensional sub algebras. I think that the answer should just be  $$\left\{\begin{pmatrix}a & b\\0 & -a \end{pmatrix}: a,b \in \mathbb{C}\right\} $$ but i'm not sure how to prove it. Any advice would be appreciated.","I am trying to find the connected subgroups of the Lie Group $SL_2(\mathbb{C})$ up to conjugation. My thought was to find the sub-algebras of the Lie Algebra $\frak{sl}_2(\mathbb{C})$, and then exponentiate. For the one dimensional sub algebras, it should just be determined by the Jordan form. I am a little stuck with the 2-dimensional sub algebras. I think that the answer should just be  $$\left\{\begin{pmatrix}a & b\\0 & -a \end{pmatrix}: a,b \in \mathbb{C}\right\} $$ but i'm not sure how to prove it. Any advice would be appreciated.",,"['linear-algebra', 'abstract-algebra', 'lie-groups', 'lie-algebras']"
24,Let $A$ and $B$ be real matrix such that $A+iB$ is non singular show that there exist $t \in \mathbb{R}$ such that $A+tB$ is non singular,Let  and  be real matrix such that  is non singular show that there exist  such that  is non singular,A B A+iB t \in \mathbb{R} A+tB,"Let $A$ and $B$ be real matrices, with $A+iB$ non-singular. I need to show that there exist a real number $t$ such that $ A+tB $ is non-singular. I don't have any idea how I can approach this question... could I please get a hint?","Let $A$ and $B$ be real matrices, with $A+iB$ non-singular. I need to show that there exist a real number $t$ such that $ A+tB $ is non-singular. I don't have any idea how I can approach this question... could I please get a hint?",,['linear-algebra']
25,Constructing a continuous path between two matrices,Constructing a continuous path between two matrices,,"Let $A_1, A_2 \in GL_n(\mathbb R)$ be two fixed matrices with eigenvalues lying on the open left half plane of $\mathbb C$, i.e., with negative real parts and $A_1 \neq A_2$. We may assume $A_1, A_2$ are diagonalizable if necessary. Let $(A_1)_\alpha$ and $(A_2)_\beta$ be the corresponding matrix that scales the eigenvalues by $\alpha \in \mathbb R$ and $\beta \in \mathbb R$ respectively. That is if $(\lambda_1, \dots, \lambda_n)$ are eigenvalues of $A_1$, then $(\alpha \lambda_1, \dots, \alpha \lambda_n)$ are eigenvalues of $(A_1)_{\alpha}$. I am wondering whether it is possible to construct a continuous path $\gamma: [0,1] \to GL_n(\mathbb R)$ with eigenvalues all lying on the left half plane along the path: by choosing a sequence of nonzero real numbers, $(\alpha_n)$ and $(\beta_n)$, $\gamma$ is piecewise linear. For example, suppose there exist some finite sequences $(\alpha_1, \dots, \alpha_k)$ and $(\beta_1, \dots, \beta_l)$, then the path would be \begin{align*} A_1 \xrightarrow{} (A_1)_{\alpha_1} \xrightarrow{} \dots \xrightarrow{} (A_1)_{\alpha_k} \xrightarrow{}  (A_2)_{\beta_1} \xrightarrow{} \dots \xrightarrow{} (A_2)_{\beta_l} \xrightarrow{} A_2. \end{align*} In between, each arrow stands for a convex path. Assume we can construct a continuous path between $A$ and $(A)_{\alpha}$ . If we take a convex combination of $A_1, A_2$, i.e., $A(t) = (1-t)A_1 + tA_2$. Then $\det(A(t))$ is a nonconstant polynomial in $t$ and has at most $n$ zeros. Intuitively I am thinking of taking the path until we meet a singular point of $t$ (by continuity, all eigenvalues should stay in the left half plane) and change path to some $(A_1)_{\alpha}$ and doing this repeatedly. Not sure whether this will work. p.s. The reason for my consideration was because I need to guarantee certain matrix structure along the path. I have worked out scaling eigenvalues can be done within the structure of interest.","Let $A_1, A_2 \in GL_n(\mathbb R)$ be two fixed matrices with eigenvalues lying on the open left half plane of $\mathbb C$, i.e., with negative real parts and $A_1 \neq A_2$. We may assume $A_1, A_2$ are diagonalizable if necessary. Let $(A_1)_\alpha$ and $(A_2)_\beta$ be the corresponding matrix that scales the eigenvalues by $\alpha \in \mathbb R$ and $\beta \in \mathbb R$ respectively. That is if $(\lambda_1, \dots, \lambda_n)$ are eigenvalues of $A_1$, then $(\alpha \lambda_1, \dots, \alpha \lambda_n)$ are eigenvalues of $(A_1)_{\alpha}$. I am wondering whether it is possible to construct a continuous path $\gamma: [0,1] \to GL_n(\mathbb R)$ with eigenvalues all lying on the left half plane along the path: by choosing a sequence of nonzero real numbers, $(\alpha_n)$ and $(\beta_n)$, $\gamma$ is piecewise linear. For example, suppose there exist some finite sequences $(\alpha_1, \dots, \alpha_k)$ and $(\beta_1, \dots, \beta_l)$, then the path would be \begin{align*} A_1 \xrightarrow{} (A_1)_{\alpha_1} \xrightarrow{} \dots \xrightarrow{} (A_1)_{\alpha_k} \xrightarrow{}  (A_2)_{\beta_1} \xrightarrow{} \dots \xrightarrow{} (A_2)_{\beta_l} \xrightarrow{} A_2. \end{align*} In between, each arrow stands for a convex path. Assume we can construct a continuous path between $A$ and $(A)_{\alpha}$ . If we take a convex combination of $A_1, A_2$, i.e., $A(t) = (1-t)A_1 + tA_2$. Then $\det(A(t))$ is a nonconstant polynomial in $t$ and has at most $n$ zeros. Intuitively I am thinking of taking the path until we meet a singular point of $t$ (by continuity, all eigenvalues should stay in the left half plane) and change path to some $(A_1)_{\alpha}$ and doing this repeatedly. Not sure whether this will work. p.s. The reason for my consideration was because I need to guarantee certain matrix structure along the path. I have worked out scaling eigenvalues can be done within the structure of interest.",,"['linear-algebra', 'general-topology', 'path-connected', 'matrix-analysis']"
26,Galois Descent for representations of finite groups OR a question about block matrices where the blocks are Galois conjugates,Galois Descent for representations of finite groups OR a question about block matrices where the blocks are Galois conjugates,,"The question is quite long since I give some background but really I am interested in some very concrete fact about matrix representations. Scroll all the way to the bottom for a self contained question, feel free to ignore the rest. Let $L/K$ be a Galois field extension with Galois group $\Gamma$ . Let $G$ be a finite group and $V$ a representation of $G$ over $L$ (aka, a module over $L[G])$ . Suppose there is a semi linear action of $\Gamma$ on $V$ with respect to $L(G)$ . That is, for all $a \in L[G]$ , $\gamma \in \Gamma$ and $v \in V$ , we have $(av)^\gamma = a^\gamma v^\gamma$ . Does there necessarily exist a $k[G]$ submodule $W \subset L$ such that the natural map $W\otimes_{k[G]} L[G] \to V$ is an isomorphism? I believe I can prove this using general ideas from faithfully flat descent (extend to a separably closed base field...) but I am looking for a more concrete way of proving this along the following lines: Let us decompose $V = \bigoplus_k V_k$ as a sum of irreducible representations. Now, note that we can define conjugate representations $g^\gamma$ on $V$ since $g^\gamma (v) = (g(v^{\gamma^{-1}}))^\gamma \in V.$ Moreover, this conjugate action preserves irreducibility and so the representation $(G^\gamma,V_k)$ corresponds to some $(G,V_{\gamma(k)})$ . In concrete terms, if we fix a basis of vectors fixed under the semilinear action of $\Gamma$ and $g$ is given by a matrix $[a_{ij}]$ on $V_k$ , then $g^\gamma$ is given by the matrix $[\gamma(a_{ij})]$ . Therefore, we can group the conjugate representations together and now suppose $W_k = \bigoplus_{\gamma \in \Gamma}V_k^\gamma$ . (In general, there might be repeated terms in this sum but let me ignore that for now). This is closed under both the $G$ action and the $\Gamma$ action. Concretely, for any $g$ with matrix $[a_{ij}]$ for $W_k$ , the matrix for $W$ is given by a block matrix with $|\Gamma|$ blocks and the block corresponding to $\gamma$ is equal to $[a_{ij}^\gamma]$ . In this case, I want to show that the matrix for each $g$ is similar to a matrix over $k$ and moreover that this similarity is realized by the same matrix (over $L$ ) for all $g \in G$ . The second condition is what is tripping me up. Since the characteristic polynomial for any $g$ is a polynomial over $k$ , the rational canonical form realizes an element in the conjugacy class over the base field. But how do I make sure that I can do just one base change to get all the matrices corresponding to all the $g \in G$ into rational form? Concrete question: Let $L/K$ be a Galois extension with Galois group $\Gamma$ . Let $G$ be a finite group. Let $\rho: G \in M_n(L)$ be a representation and suppose that for each $g, \rho(g)$ is a block matrix with the blocks indexed by $\gamma \in \Gamma$ such that if the block corresponding to the identity in $\Gamma$ is $[a_{ij}]$ , then the block corresponding to $\gamma$ is $[\gamma(a_{ij})]$ . Can I find an invertible matrix $P$ over $L$ such that for all $g \in G$ , $P\rho(g)P^{-1}$ is a matrix over $K$ ? Note that $\rho(g)$ always has characteristic polynomial defined over $K$ so that the rational canonical form gives us some $P_g$ for each $g \in G$ with the above property. The question is eliminate the dependence of $P_g$ on $g$ .","The question is quite long since I give some background but really I am interested in some very concrete fact about matrix representations. Scroll all the way to the bottom for a self contained question, feel free to ignore the rest. Let be a Galois field extension with Galois group . Let be a finite group and a representation of over (aka, a module over . Suppose there is a semi linear action of on with respect to . That is, for all , and , we have . Does there necessarily exist a submodule such that the natural map is an isomorphism? I believe I can prove this using general ideas from faithfully flat descent (extend to a separably closed base field...) but I am looking for a more concrete way of proving this along the following lines: Let us decompose as a sum of irreducible representations. Now, note that we can define conjugate representations on since Moreover, this conjugate action preserves irreducibility and so the representation corresponds to some . In concrete terms, if we fix a basis of vectors fixed under the semilinear action of and is given by a matrix on , then is given by the matrix . Therefore, we can group the conjugate representations together and now suppose . (In general, there might be repeated terms in this sum but let me ignore that for now). This is closed under both the action and the action. Concretely, for any with matrix for , the matrix for is given by a block matrix with blocks and the block corresponding to is equal to . In this case, I want to show that the matrix for each is similar to a matrix over and moreover that this similarity is realized by the same matrix (over ) for all . The second condition is what is tripping me up. Since the characteristic polynomial for any is a polynomial over , the rational canonical form realizes an element in the conjugacy class over the base field. But how do I make sure that I can do just one base change to get all the matrices corresponding to all the into rational form? Concrete question: Let be a Galois extension with Galois group . Let be a finite group. Let be a representation and suppose that for each is a block matrix with the blocks indexed by such that if the block corresponding to the identity in is , then the block corresponding to is . Can I find an invertible matrix over such that for all , is a matrix over ? Note that always has characteristic polynomial defined over so that the rational canonical form gives us some for each with the above property. The question is eliminate the dependence of on .","L/K \Gamma G V G L L[G]) \Gamma V L(G) a \in L[G] \gamma \in \Gamma v \in V (av)^\gamma = a^\gamma v^\gamma k[G] W \subset L W\otimes_{k[G]} L[G] \to V V = \bigoplus_k V_k g^\gamma V g^\gamma (v) = (g(v^{\gamma^{-1}}))^\gamma \in V. (G^\gamma,V_k) (G,V_{\gamma(k)}) \Gamma g [a_{ij}] V_k g^\gamma [\gamma(a_{ij})] W_k = \bigoplus_{\gamma \in \Gamma}V_k^\gamma G \Gamma g [a_{ij}] W_k W |\Gamma| \gamma [a_{ij}^\gamma] g k L g \in G g k g \in G L/K \Gamma G \rho: G \in M_n(L) g, \rho(g) \gamma \in \Gamma \Gamma [a_{ij}] \gamma [\gamma(a_{ij})] P L g \in G P\rho(g)P^{-1} K \rho(g) K P_g g \in G P_g g","['linear-algebra', 'finite-groups', 'representation-theory', 'galois-theory', 'descent']"
27,"Solve $(ST-\lambda I)x=b$ in $O(n^2)$ for upper triangular $S,T$",Solve  in  for upper triangular,"(ST-\lambda I)x=b O(n^2) S,T","Let $S,T\in\mathbb{R}^{n\times n}$ be 2 upper triangular matrixes, and $ST-\lambda I$ be nonsingular where $I$ denotes the identity matrix of order $n$. Solve the system of linear equations in complexity $O(n^2)$ for $b\in\mathbb{R}^n$:   $$(ST-\lambda I)x=b$$ It's known that equations like $Uy=b$ for upper triangular $U$ can be solved in complexity $O(n^2)$ by back substitution method. I tried to rewrite $ST-\lambda I$ as the product of 2 matrixes, but it failed. Now I get stuck on this puzzling problem. Appreciated for any suggestions.","Let $S,T\in\mathbb{R}^{n\times n}$ be 2 upper triangular matrixes, and $ST-\lambda I$ be nonsingular where $I$ denotes the identity matrix of order $n$. Solve the system of linear equations in complexity $O(n^2)$ for $b\in\mathbb{R}^n$:   $$(ST-\lambda I)x=b$$ It's known that equations like $Uy=b$ for upper triangular $U$ can be solved in complexity $O(n^2)$ by back substitution method. I tried to rewrite $ST-\lambda I$ as the product of 2 matrixes, but it failed. Now I get stuck on this puzzling problem. Appreciated for any suggestions.",,"['linear-algebra', 'numerical-methods', 'numerical-linear-algebra']"
28,Have I found the Jordan form correctly?,Have I found the Jordan form correctly?,,I am given that the minimal polynomial and characteristic polynomial of a matrix are both $(x-1)^2(x+1)^2$. I have found the Jordan form to be $$\begin{bmatrix}1&1&0&0\\0&1&0&0\\0&0&-1&1\\0&0&0&-1\end{bmatrix}.$$ Is this correct or have I made a mistake somewhere?,I am given that the minimal polynomial and characteristic polynomial of a matrix are both $(x-1)^2(x+1)^2$. I have found the Jordan form to be $$\begin{bmatrix}1&1&0&0\\0&1&0&0\\0&0&-1&1\\0&0&0&-1\end{bmatrix}.$$ Is this correct or have I made a mistake somewhere?,,"['linear-algebra', 'matrices']"
29,Generalisation of Schur's lemma,Generalisation of Schur's lemma,,"Proposition: Given two square matrices $A,B \in \mathbb{C}^{n \times n}$ which fulfil $AM = MB$ for all $M \in \mathbb{C}^{n \times n}$ they must be identical and a scalar multiple of the identity matrix: $A = B = \lambda \mathbb{1}$, $\lambda \in \mathbb{C}$. Proof: Since the relation $AM = MB$ holds for any matrix $M$, we can in particular choose the matrices $(M_{mn})_{ij} = \delta_{mi} \delta_{nj}$ (all elements are zero, except for $(m,n)$, which is $1$). Thus, $$ \sum_{j=1}^n A_{ij} (M_{mn})_{jl} = \sum_{j=1}^n (M_{mn})_{ij} B_{jl} \Leftrightarrow \sum_{j=1}^n A_{ij} \delta_{mj} \delta_{nl} = \sum_{j=1}^n \delta_{mi} \delta_{nj} B_{jl} \Leftrightarrow A_{im} \delta_{nl} = \delta_{mi} B_{nl} \,. $$ Now, we distinguish the cases $n=l$, $m=i$: $A_{ii} = B_{ll}$, i.e. all diagonal elements must be the same: $A_{ii} = B_{ii} = \lambda$. $n=l$, $m\neq i$: $A_{im} = 0$, i.e. all off-diagonal elements of A must vanish. $n\neq l$, $m=i$: $0 = B_{nl}$, i.e. all off-diagonal elements of B must vanish. Thus, $A=B=\mathrm{diag}(\lambda,\dots,\lambda)$. Question: This proposition reminded me a bit of Schur's lemma from group representation theory, but I think that this case is not covered by Schur's lemma since $A$ and $B$ can be different and $M$ does not need to be invertible. I was wondering whether the proposition is just a special case of a more general form of Schur's lemma or whether there is some other well known theorem that I could refer to instead of the proof above. Bonus question: Is there a proof of this statement that doesn't require to go to component notation?","Proposition: Given two square matrices $A,B \in \mathbb{C}^{n \times n}$ which fulfil $AM = MB$ for all $M \in \mathbb{C}^{n \times n}$ they must be identical and a scalar multiple of the identity matrix: $A = B = \lambda \mathbb{1}$, $\lambda \in \mathbb{C}$. Proof: Since the relation $AM = MB$ holds for any matrix $M$, we can in particular choose the matrices $(M_{mn})_{ij} = \delta_{mi} \delta_{nj}$ (all elements are zero, except for $(m,n)$, which is $1$). Thus, $$ \sum_{j=1}^n A_{ij} (M_{mn})_{jl} = \sum_{j=1}^n (M_{mn})_{ij} B_{jl} \Leftrightarrow \sum_{j=1}^n A_{ij} \delta_{mj} \delta_{nl} = \sum_{j=1}^n \delta_{mi} \delta_{nj} B_{jl} \Leftrightarrow A_{im} \delta_{nl} = \delta_{mi} B_{nl} \,. $$ Now, we distinguish the cases $n=l$, $m=i$: $A_{ii} = B_{ll}$, i.e. all diagonal elements must be the same: $A_{ii} = B_{ii} = \lambda$. $n=l$, $m\neq i$: $A_{im} = 0$, i.e. all off-diagonal elements of A must vanish. $n\neq l$, $m=i$: $0 = B_{nl}$, i.e. all off-diagonal elements of B must vanish. Thus, $A=B=\mathrm{diag}(\lambda,\dots,\lambda)$. Question: This proposition reminded me a bit of Schur's lemma from group representation theory, but I think that this case is not covered by Schur's lemma since $A$ and $B$ can be different and $M$ does not need to be invertible. I was wondering whether the proposition is just a special case of a more general form of Schur's lemma or whether there is some other well known theorem that I could refer to instead of the proof above. Bonus question: Is there a proof of this statement that doesn't require to go to component notation?",,"['linear-algebra', 'matrices']"
30,"If $gh=hg$ and $\rho$ is a representation of $G$, then the matrices $\rho(g)$ and $\rho(h)$ are simulatenously diagonalizable","If  and  is a representation of , then the matrices  and  are simulatenously diagonalizable",gh=hg \rho G \rho(g) \rho(h),"Let $G$ be a finite group, $|G|=n$, and $g,h\in G$ such that $gh=hg$. If $\rho: G \rightarrow GL(V)$ is a representation, where $V$ a vector space over $\mathbb{C}$,  prove that there exists a basis of $V$ in which both $\rho(g)$ and $\rho(h)$ are diagonal. I could prove this result using general methods of linear algebra , as follows: Since $G$ is finite, $|G|=n$,  it follows that $\rho(g)^n =\rho(g^n) = \rho(1) = I$. If $p(X) = X^n-1,$ the condition before shows us that $p(\rho(g))=0$, which ensures that the minimal polynomial for $\rho(g)$ must divide $X^n-1$. Now, $X^n-1$ has no repeated roots and therefore so does the minimal polynomial. We conclude from this that $\rho(g)$ is diagonalizable. The same argument runs for $\rho(h)$. Now, both are diagonalizable and commuting. By linear algebra, they are simultaneously diagonalizable. My question is: is there a way to prove this claim using only representation theory methods? or a proof which is heavily based on representation theory methods?","Let $G$ be a finite group, $|G|=n$, and $g,h\in G$ such that $gh=hg$. If $\rho: G \rightarrow GL(V)$ is a representation, where $V$ a vector space over $\mathbb{C}$,  prove that there exists a basis of $V$ in which both $\rho(g)$ and $\rho(h)$ are diagonal. I could prove this result using general methods of linear algebra , as follows: Since $G$ is finite, $|G|=n$,  it follows that $\rho(g)^n =\rho(g^n) = \rho(1) = I$. If $p(X) = X^n-1,$ the condition before shows us that $p(\rho(g))=0$, which ensures that the minimal polynomial for $\rho(g)$ must divide $X^n-1$. Now, $X^n-1$ has no repeated roots and therefore so does the minimal polynomial. We conclude from this that $\rho(g)$ is diagonalizable. The same argument runs for $\rho(h)$. Now, both are diagonalizable and commuting. By linear algebra, they are simultaneously diagonalizable. My question is: is there a way to prove this claim using only representation theory methods? or a proof which is heavily based on representation theory methods?",,"['linear-algebra', 'representation-theory']"
31,Why is the square root of Cholesky decomposition equal to the lower triangular matrix?,Why is the square root of Cholesky decomposition equal to the lower triangular matrix?,,"I came across this as I was learning unscented Kalman filters. Suppose I have a symmetric and positive definite matrix $P$ . I want to take its square root. After I perform the Cholesky decomposition of $P$ , I get $LL^T$ . One of the resources (inaccessible on the web) I am using says that when you take the square root of $LL^T$ , you get the lower triangular matrix $L$ . That's why I don't understand. My understanding was that matrix $Y$ is a square root of $X$ if the matrix product $YY$ is equal to $X$ . But what seems to be acceptable is that matrix Y can be a square root of X even when the matrix product $Y^TY$ is equal to $X$ .","I came across this as I was learning unscented Kalman filters. Suppose I have a symmetric and positive definite matrix . I want to take its square root. After I perform the Cholesky decomposition of , I get . One of the resources (inaccessible on the web) I am using says that when you take the square root of , you get the lower triangular matrix . That's why I don't understand. My understanding was that matrix is a square root of if the matrix product is equal to . But what seems to be acceptable is that matrix Y can be a square root of X even when the matrix product is equal to .",P P LL^T LL^T L Y X YY X Y^TY X,"['linear-algebra', 'matrices', 'numerical-linear-algebra', 'matrix-decomposition', 'cholesky-decomposition']"
32,Dual of the symmetric algebra of the dual,Dual of the symmetric algebra of the dual,,"Let $V$ be a finite dimensional vector space and let $S^d(V)$ be the degree $d$ part of the symmetric algebra on $V$. Is it true that $S^d(V^*)^*$ is canonically isomorphic to $S^d(V)$? What (I think) I know : $S^d(V^*)$ are homogeneous polynomials of degree $d$ on $n=\dim{V}$ variables with coefficients in the base field $k$, because a basis of $V^*$ is given by the $n$ coordinate functions $x_i$ which are the dual basis of any fixed basis for $V$. So the symmetric algebra $S(V^*)$ really is the polynomial ring $k[x_1,...,x_n]$, right? ( Remark : in this description of the symmetric algebra I had to make a choice, namely fixing a basis of $V$.) What I don't know : I think I don't really understand $S(V)$. Should I just think of it as $k[e_1,...,e_n]$, regarding the $e_i$'s just as formal symbols? In any case, for both descriptions I had to choose a basis. So I was hoping that there may be a canonical isomorphism as in the case of $V\cong (V^*)^*$. Is this true?","Let $V$ be a finite dimensional vector space and let $S^d(V)$ be the degree $d$ part of the symmetric algebra on $V$. Is it true that $S^d(V^*)^*$ is canonically isomorphic to $S^d(V)$? What (I think) I know : $S^d(V^*)$ are homogeneous polynomials of degree $d$ on $n=\dim{V}$ variables with coefficients in the base field $k$, because a basis of $V^*$ is given by the $n$ coordinate functions $x_i$ which are the dual basis of any fixed basis for $V$. So the symmetric algebra $S(V^*)$ really is the polynomial ring $k[x_1,...,x_n]$, right? ( Remark : in this description of the symmetric algebra I had to make a choice, namely fixing a basis of $V$.) What I don't know : I think I don't really understand $S(V)$. Should I just think of it as $k[e_1,...,e_n]$, regarding the $e_i$'s just as formal symbols? In any case, for both descriptions I had to choose a basis. So I was hoping that there may be a canonical isomorphism as in the case of $V\cong (V^*)^*$. Is this true?",,"['linear-algebra', 'abstract-algebra', 'vector-spaces']"
33,Axler's LADR. Algebraic multiplicity of an eigenvalue is the number of times it appears on the diagonal of an upper triangular matrix?,Axler's LADR. Algebraic multiplicity of an eigenvalue is the number of times it appears on the diagonal of an upper triangular matrix?,,"This problem is from Sheldon Axler's Linear Algebra Done Right, Chapter 8. Let $T \in \mathcal{L}(V)$, where $V$ is a finite dimensional complex vector space. If the matrix of $T$ is upper triangular with respect to any basis of $V$, the number of times $\lambda$ appears on the diagonal of this matrix equals the (algebraic) multiplicity of $\lambda$ as an eigenvalue of $T$. It suffices to show that $\dim \text{null } T^n = \dim G(0, T)$ equals the number of $0$'s on the diagonal, where $G(0, T)$ is the space of generalized eigenvectors of $T$ with respect to $0$. I found this rather nice proof on this blog, which uses induction on the dimension of $V$. I'm interested in finding alternative ways to prove this statement, which may provide a different way of looking at it.","This problem is from Sheldon Axler's Linear Algebra Done Right, Chapter 8. Let $T \in \mathcal{L}(V)$, where $V$ is a finite dimensional complex vector space. If the matrix of $T$ is upper triangular with respect to any basis of $V$, the number of times $\lambda$ appears on the diagonal of this matrix equals the (algebraic) multiplicity of $\lambda$ as an eigenvalue of $T$. It suffices to show that $\dim \text{null } T^n = \dim G(0, T)$ equals the number of $0$'s on the diagonal, where $G(0, T)$ is the space of generalized eigenvectors of $T$ with respect to $0$. I found this rather nice proof on this blog, which uses induction on the dimension of $V$. I'm interested in finding alternative ways to prove this statement, which may provide a different way of looking at it.",,"['linear-algebra', 'matrices', 'linear-transformations']"
34,Let $T:\mathbb{R}_n\rightarrow\mathbb{R}_n$ the linear operator defined by $T(p(x))=p(x)+p'(x)$ Calculate $det(T)$,Let  the linear operator defined by  Calculate,T:\mathbb{R}_n\rightarrow\mathbb{R}_n T(p(x))=p(x)+p'(x) det(T),"Let $T:\mathbb{R}_n\rightarrow\mathbb{R}_n$ the linear operator defined by $T(p(x))=p(x)+p'(x)$ Calculate $det(T)$ My work: Let $B=\{1,x,x^2,...,x^n\}$ a basis for $\mathbb{R}_n$. Then, $T(1)=1$ $T(x)=x+1$ $T(x^2)=x^2+2x$ . . . $T(x^n)=x^n+nx^{n-1}$ This implies, $[T_{BB}]=\begin{pmatrix} 1 && 1 && 0 && 0 && ... && 0\\ 0 && 1 && 2 && 0 && ... && 0\\ 0 && 0 && 1 && 3 && ... && 0\\ .\\ .\\ .\\ 0 && 0 && 0 && 0 && ... && n\\ 0 && 0 && 0 && 0 && ... &&1\\ \end{pmatrix}$ As $det(T)=det([T_B])$ and $det([T_B])=det\begin{pmatrix} 1 && 1 && 0 && 0 && ... && 0\\ 0 && 1 && 2 && 0 && ... && 0\\ 0 && 0 && 1 && 3 && ... && 0\\ .\\ .\\ .\\ 0 && 0 && 0 && 0 && ... && n\\ 0 && 0 && 0 && 0 && ... &&1\\ \end{pmatrix}=1$ then $det(T)=1$. I don't sure of my result. I think i have a mistake. Can someone help me?","Let $T:\mathbb{R}_n\rightarrow\mathbb{R}_n$ the linear operator defined by $T(p(x))=p(x)+p'(x)$ Calculate $det(T)$ My work: Let $B=\{1,x,x^2,...,x^n\}$ a basis for $\mathbb{R}_n$. Then, $T(1)=1$ $T(x)=x+1$ $T(x^2)=x^2+2x$ . . . $T(x^n)=x^n+nx^{n-1}$ This implies, $[T_{BB}]=\begin{pmatrix} 1 && 1 && 0 && 0 && ... && 0\\ 0 && 1 && 2 && 0 && ... && 0\\ 0 && 0 && 1 && 3 && ... && 0\\ .\\ .\\ .\\ 0 && 0 && 0 && 0 && ... && n\\ 0 && 0 && 0 && 0 && ... &&1\\ \end{pmatrix}$ As $det(T)=det([T_B])$ and $det([T_B])=det\begin{pmatrix} 1 && 1 && 0 && 0 && ... && 0\\ 0 && 1 && 2 && 0 && ... && 0\\ 0 && 0 && 1 && 3 && ... && 0\\ .\\ .\\ .\\ 0 && 0 && 0 && 0 && ... && n\\ 0 && 0 && 0 && 0 && ... &&1\\ \end{pmatrix}=1$ then $det(T)=1$. I don't sure of my result. I think i have a mistake. Can someone help me?",,"['linear-algebra', 'determinant']"
35,"Is there a ""standard"" name for $n \times n$-matrices whose square is $-1$ times the identity matrix?","Is there a ""standard"" name for -matrices whose square is  times the identity matrix?",n \times n -1,"I have seen the name “skew involution” in a problem in at least one book but none of my colleagues who work in abstract algebra, geometry or linear algebra have ever heard of this name, although they agree it is a reasonable name.  I have used the term “imaginary unit” for such matrices and my colleagues agree that is also a reasonable name, but I wonder if there is a standard name in the literature.","I have seen the name “skew involution” in a problem in at least one book but none of my colleagues who work in abstract algebra, geometry or linear algebra have ever heard of this name, although they agree it is a reasonable name.  I have used the term “imaginary unit” for such matrices and my colleagues agree that is also a reasonable name, but I wonder if there is a standard name in the literature.",,"['linear-algebra', 'matrices', 'geometry', 'algebraic-geometry']"
36,Apostol Calculus I-14.19 exercise 16(missiles problem),Apostol Calculus I-14.19 exercise 16(missiles problem),,"Problem: Due to a mechanical failure,a ground crew has lost control of a missile recently fired.It is known tha the missile will proceed at a constant speed on a straight course of unknown direction.When the missile is 4 miles away,it is sighted for an instant and lost again.Immediately an anti-missile missile is fired with a constant speed three times that of the first missile.What should be the course of the second missile for it to overtake the first one? The book's solution is that using the positive x-axis the line from position sighted four miles away to the ground crew.Proceed three miles along this line(in case the missile is returning to base)and then follow the spiral $ r=e^{θ/\sqrt8}$ (polar coordinates)(assume both missiles move on the same plane). I am trying to figure out why we use that spiral,my thoughts are that since $r(θ)=e^{θ/\sqrt8}$ then $log r=θ/\sqrt8$ and $\frac{dr}{dθ}/r=\frac{1}{\sqrt8}$ where r the length of the possition vector of the anti-missile and θ its angle with x-axis.Suppose $0\le \alpha\le π$ the angle between the velocity vector and the position vector of the anti-missile then I know that(from previous exercise) $\cot\alpha=\frac{dr}{dθ}/r$ so i try to prove that $cot\alpha = 1/\sqrt8$ in any possible position of impact. I also think that if $ φ$ the angle between the velocity vectors of the two missiles in a possition of impact then $α=π-φ$ . Any hint would be helpfull.Thank you!","Problem: Due to a mechanical failure,a ground crew has lost control of a missile recently fired.It is known tha the missile will proceed at a constant speed on a straight course of unknown direction.When the missile is 4 miles away,it is sighted for an instant and lost again.Immediately an anti-missile missile is fired with a constant speed three times that of the first missile.What should be the course of the second missile for it to overtake the first one? The book's solution is that using the positive x-axis the line from position sighted four miles away to the ground crew.Proceed three miles along this line(in case the missile is returning to base)and then follow the spiral (polar coordinates)(assume both missiles move on the same plane). I am trying to figure out why we use that spiral,my thoughts are that since then and where r the length of the possition vector of the anti-missile and θ its angle with x-axis.Suppose the angle between the velocity vector and the position vector of the anti-missile then I know that(from previous exercise) so i try to prove that in any possible position of impact. I also think that if the angle between the velocity vectors of the two missiles in a possition of impact then . Any hint would be helpfull.Thank you!", r=e^{θ/\sqrt8} r(θ)=e^{θ/\sqrt8} log r=θ/\sqrt8 \frac{dr}{dθ}/r=\frac{1}{\sqrt8} 0\le \alpha\le π \cot\alpha=\frac{dr}{dθ}/r cot\alpha = 1/\sqrt8  φ α=π-φ,"['calculus', 'linear-algebra', 'vectors']"
37,Is the natural norm on the exterior algebra submultiplicative?,Is the natural norm on the exterior algebra submultiplicative?,,"For an inner product space $(V,\langle \cdot,\cdot\rangle)$ the exterior algebra $\Lambda V$ inherits an inner product, which satisfies $\langle a_1\wedge \dots \wedge a_n,b_1\wedge\dots\wedge b_n\rangle = \det(a_i,b_j)$ on simple tensors of the same rank and is zero for tensors of different rank. Question: Is the associated norm on $\Lambda V$ submultiplicative, i.e. is $\Vert a \wedge b\Vert\le\Vert a\Vert \cdot \Vert b\Vert$ for all $a,b\in \Lambda V$? If $a$ and $b$ both have rank $1$, then this is true, because $\Vert a \wedge b \Vert^2 = \Vert a \Vert^2 \cdot \Vert b \Vert^2 - \langle a, b\rangle^2$. If $a=a_1\wedge \dots \wedge a_n$ and $b=b_1\wedge\dots\wedge b_n$ are simple rank $n$ tensors ($\neq 0$), then $$\Vert a \wedge b\Vert^2 = \det\left (\begin{matrix}   A & C \\ C & B     \end{matrix}\right) = \det A \cdot \det B \cdot \det(I - B^{-1}C A^{-1}C),$$ where $A,B,C$ are $n\times n-$block matrices containing $\langle a_i,a_j\rangle$ and so on and the second inequaltiy is taken from this Wikipedia article on the determinant of block matrices. If the last determinant was $\le 1$, we'd be done, but I don't see how this would work.","For an inner product space $(V,\langle \cdot,\cdot\rangle)$ the exterior algebra $\Lambda V$ inherits an inner product, which satisfies $\langle a_1\wedge \dots \wedge a_n,b_1\wedge\dots\wedge b_n\rangle = \det(a_i,b_j)$ on simple tensors of the same rank and is zero for tensors of different rank. Question: Is the associated norm on $\Lambda V$ submultiplicative, i.e. is $\Vert a \wedge b\Vert\le\Vert a\Vert \cdot \Vert b\Vert$ for all $a,b\in \Lambda V$? If $a$ and $b$ both have rank $1$, then this is true, because $\Vert a \wedge b \Vert^2 = \Vert a \Vert^2 \cdot \Vert b \Vert^2 - \langle a, b\rangle^2$. If $a=a_1\wedge \dots \wedge a_n$ and $b=b_1\wedge\dots\wedge b_n$ are simple rank $n$ tensors ($\neq 0$), then $$\Vert a \wedge b\Vert^2 = \det\left (\begin{matrix}   A & C \\ C & B     \end{matrix}\right) = \det A \cdot \det B \cdot \det(I - B^{-1}C A^{-1}C),$$ where $A,B,C$ are $n\times n-$block matrices containing $\langle a_i,a_j\rangle$ and so on and the second inequaltiy is taken from this Wikipedia article on the determinant of block matrices. If the last determinant was $\le 1$, we'd be done, but I don't see how this would work.",,"['linear-algebra', 'inner-products', 'multilinear-algebra']"
38,Let $A$ and $B$ be square real matrices such that $A+iB$ is non-singular. Show that there exists $t\in \mathbb{R}$ such that $A+tB$ is non-singular.,Let  and  be square real matrices such that  is non-singular. Show that there exists  such that  is non-singular.,A B A+iB t\in \mathbb{R} A+tB,"Let $A$ and $B$ be square real matrices such that $A+iB$ is non-singular. Show that there exists $t\in \mathbb{R}$ such that $A+tB$ is non-singular. My Attempt: I thought about considering the polynomial over $\mathbb{C}$. Let $$f(t)=|A+tB|.$$ Then $f(i)\not =0.$ This shows that $f\not \equiv 0$ and since $f$ has a finite number of roots of which let $\{t_1,t_2,...,t_k\}$ be the real roots. Then we can easily, find $t\in \mathbb{R}$ such that $f(t)=|A+tB|\not =0$ and hence $A+tB$ will be non-singular. Is this correct reasoning?","Let $A$ and $B$ be square real matrices such that $A+iB$ is non-singular. Show that there exists $t\in \mathbb{R}$ such that $A+tB$ is non-singular. My Attempt: I thought about considering the polynomial over $\mathbb{C}$. Let $$f(t)=|A+tB|.$$ Then $f(i)\not =0.$ This shows that $f\not \equiv 0$ and since $f$ has a finite number of roots of which let $\{t_1,t_2,...,t_k\}$ be the real roots. Then we can easily, find $t\in \mathbb{R}$ such that $f(t)=|A+tB|\not =0$ and hence $A+tB$ will be non-singular. Is this correct reasoning?",,['linear-algebra']
39,Proving set of polynomials is linearly independent,Proving set of polynomials is linearly independent,,"If $n$ is a positive integer, is there an easy way to prove that the set $$\{(nx)^n,((n-1)x+y)^n, ((n-2)x+2y)^n,\ldots, (x+(n-1)y)^n, (ny)^n\}$$ of polynomials in two variables, $x$ and $y$, is linearly independent? I have verified it manually for $n=1,2,3$ by expanding and considering vectors of coefficients.","If $n$ is a positive integer, is there an easy way to prove that the set $$\{(nx)^n,((n-1)x+y)^n, ((n-2)x+2y)^n,\ldots, (x+(n-1)y)^n, (ny)^n\}$$ of polynomials in two variables, $x$ and $y$, is linearly independent? I have verified it manually for $n=1,2,3$ by expanding and considering vectors of coefficients.",,"['linear-algebra', 'polynomials']"
40,Every Linear Functional is zero or surjective,Every Linear Functional is zero or surjective,,"Is the Following Proof Correct ? Theorem. Given that $\mathcal{L}(V,\mathbf{F})$ represent the set of all linear functionals from the finite dimensional vector space $V$ to the scalar field $\mathbf{F}$, every member  of $\mathcal{L}(V,\mathbf{F})$ is either surjective or Zero. Proof. Let $\phi\in \mathcal{L}(V,\mathbf{F})$ and $c\in\mathbf{F}$ assume that $\phi\neq 0$. We may assume without loss of generality that  $$\phi = A_1\phi_1+A_2\phi_2+.\ .\ .\ A_m\phi_m$$ where $A_1,A_2,...A_m\in\mathbf{F}$ and that $\phi_1,\phi_2,...,\phi_m$ is the basis of the dual of $V$ which has the basis $v_1,v_2,...,v_m$ furthermore since $\phi\neq 0$ it follows that the set $\alpha$ $$\alpha = \{A\in\{A_1,A_2,...,A_m\}\big|A\neq 0\} = \{B_1,B_2,...,B_{|\alpha|}\}$$ is such that $\alpha\neq\varnothing$, where $|\alpha|\leq \dim V.$ Now let $$c=\sum_{j=1}^{|\alpha|}C_j$$ where $C_j\in\mathbf{F}$ and consider the vector $v\in V$ such that  $$v = \sum_{j=1}^{|\alpha|}\frac{C_j}{B_j}v_j$$ where $v_j$ and therefore $\phi_j$ corresponds to $B_j$ applying $\phi$ to $v$ yields $$\phi\left(\sum_{j=1}^{|\alpha|}\frac{C_j}{B_j}v_j\right) = c$$ since $c$ was arbitrary it follows that $\phi$ is indeed surjective $\blacksquare$","Is the Following Proof Correct ? Theorem. Given that $\mathcal{L}(V,\mathbf{F})$ represent the set of all linear functionals from the finite dimensional vector space $V$ to the scalar field $\mathbf{F}$, every member  of $\mathcal{L}(V,\mathbf{F})$ is either surjective or Zero. Proof. Let $\phi\in \mathcal{L}(V,\mathbf{F})$ and $c\in\mathbf{F}$ assume that $\phi\neq 0$. We may assume without loss of generality that  $$\phi = A_1\phi_1+A_2\phi_2+.\ .\ .\ A_m\phi_m$$ where $A_1,A_2,...A_m\in\mathbf{F}$ and that $\phi_1,\phi_2,...,\phi_m$ is the basis of the dual of $V$ which has the basis $v_1,v_2,...,v_m$ furthermore since $\phi\neq 0$ it follows that the set $\alpha$ $$\alpha = \{A\in\{A_1,A_2,...,A_m\}\big|A\neq 0\} = \{B_1,B_2,...,B_{|\alpha|}\}$$ is such that $\alpha\neq\varnothing$, where $|\alpha|\leq \dim V.$ Now let $$c=\sum_{j=1}^{|\alpha|}C_j$$ where $C_j\in\mathbf{F}$ and consider the vector $v\in V$ such that  $$v = \sum_{j=1}^{|\alpha|}\frac{C_j}{B_j}v_j$$ where $v_j$ and therefore $\phi_j$ corresponds to $B_j$ applying $\phi$ to $v$ yields $$\phi\left(\sum_{j=1}^{|\alpha|}\frac{C_j}{B_j}v_j\right) = c$$ since $c$ was arbitrary it follows that $\phi$ is indeed surjective $\blacksquare$",,"['linear-algebra', 'proof-verification']"
41,Reducing to reduced echelon form in linear algebra seems to do so many useful things - what gives? Why?,Reducing to reduced echelon form in linear algebra seems to do so many useful things - what gives? Why?,,"What's the underlying thing about reducing a matrix to reduced echelon form that solves so many things for us? Some determinations that can be from reducing to reduced echelon form: Testing linear dependency Seeing if a matrix has an inverse Solving a system of linear equations Finding the number of free parameters in a system of linear equations Finding row and column rank I know why it can determine each individual thing, but there seems to be some underlying essence behind it all. It seems like a very convenient algorithm, so there must be something about it I'm missing that explains why it can explain so many things about a matrix. My guess is that it reduces the matrix down to its basis which may determine the rest by analyzing the basis, but other than that, not sure. What gives? ADDENDUM: I've been told in a comment that ""reducing to RREF reveals the basis of the row-space"". How can a matrix have a different row space and column space simultaneously? What does that mean about matrices, exactly?","What's the underlying thing about reducing a matrix to reduced echelon form that solves so many things for us? Some determinations that can be from reducing to reduced echelon form: Testing linear dependency Seeing if a matrix has an inverse Solving a system of linear equations Finding the number of free parameters in a system of linear equations Finding row and column rank I know why it can determine each individual thing, but there seems to be some underlying essence behind it all. It seems like a very convenient algorithm, so there must be something about it I'm missing that explains why it can explain so many things about a matrix. My guess is that it reduces the matrix down to its basis which may determine the rest by analyzing the basis, but other than that, not sure. What gives? ADDENDUM: I've been told in a comment that ""reducing to RREF reveals the basis of the row-space"". How can a matrix have a different row space and column space simultaneously? What does that mean about matrices, exactly?",,"['linear-algebra', 'matrices', 'systems-of-equations', 'matrix-rank']"
42,Huge matrix multiplication,Huge matrix multiplication,,"I have a sparse A matrix stored in column major order (it is intrisically column major) of ~80GB and another sparse matrix B relatively small (1GB) which can be loaded in row or column major with no particular effort. I need to compute a straight matrix product S = AB. My problem is that I have only 64 GB of RAM (I usually use the Eigen c++ library ) and i need to compute the product by blocks. I was thinking to re store the A matrix in row major (even it could imply a great increase in terms of storage) and later load the new matrix A by blocks, N rows at time, compute and store the various products and at the end assembly all the blocks together. Do you have any better ideas?","I have a sparse A matrix stored in column major order (it is intrisically column major) of ~80GB and another sparse matrix B relatively small (1GB) which can be loaded in row or column major with no particular effort. I need to compute a straight matrix product S = AB. My problem is that I have only 64 GB of RAM (I usually use the Eigen c++ library ) and i need to compute the product by blocks. I was thinking to re store the A matrix in row major (even it could imply a great increase in terms of storage) and later load the new matrix A by blocks, N rows at time, compute and store the various products and at the end assembly all the blocks together. Do you have any better ideas?",,"['linear-algebra', 'matrix-calculus', 'matrix-decomposition', 'sparse-matrices']"
43,find matrix representation of projection onto kernel,find matrix representation of projection onto kernel,,"How can I efficiently find a matrix $P_A$ that projects onto the kernel of another matrix $A$? That is, given $Ax=0$, is there an ""efficient"" (without using inverses) way to find $P_A$ such that $P_A x \in ker(A)$, i.e., $A(P_A y) = 0$ for $Ay \neq 0$? I know that I can do this with the orthogonal projection $Q_A := I - A^{\dagger}A$ for $A^{\dagger} = A^T(AA^T)^{-1}$ when $A$ has full row rank or $A^{\dagger} = (A^TA)^{-1}A^T$ when $A$ has full column rank, but I'm interested in other projections onto $ker(A)$ where we're not required to compute the pseudoinverse because of two reasons SVD of $A^TA$ or $AA^T$ might be too expensive or unreliable (because of size or poorly conditioning, respectively) $A$ might be rank deficient (not have full row rank or full column rank) In these cases, what's the best way to project onto the kernel of $A$? I imagine that we must rely on knowing something about the structure of $A$.","How can I efficiently find a matrix $P_A$ that projects onto the kernel of another matrix $A$? That is, given $Ax=0$, is there an ""efficient"" (without using inverses) way to find $P_A$ such that $P_A x \in ker(A)$, i.e., $A(P_A y) = 0$ for $Ay \neq 0$? I know that I can do this with the orthogonal projection $Q_A := I - A^{\dagger}A$ for $A^{\dagger} = A^T(AA^T)^{-1}$ when $A$ has full row rank or $A^{\dagger} = (A^TA)^{-1}A^T$ when $A$ has full column rank, but I'm interested in other projections onto $ker(A)$ where we're not required to compute the pseudoinverse because of two reasons SVD of $A^TA$ or $AA^T$ might be too expensive or unreliable (because of size or poorly conditioning, respectively) $A$ might be rank deficient (not have full row rank or full column rank) In these cases, what's the best way to project onto the kernel of $A$? I imagine that we must rely on knowing something about the structure of $A$.",,['linear-algebra']
44,Property of skew-symmetric matrices of vectors multiplied by rotation matrices,Property of skew-symmetric matrices of vectors multiplied by rotation matrices,,"Let $v \in \mathbb{R}^3$ and $R$ be a rotation matrix (probably orthogonal is enough). I think the following property is true: $$ [R\cdot v] = R \cdot [v] \cdot R^T$$ where $[v]$ is the skew symmetric form of the vector $v$. I should verify this by direct calculation, but I am not in the mood right now, hence I want to know if the following proof is true: proof: let $u \in \mathbb{R}^3$ hence $$ [R\cdot v] \cdot \left( R \cdot u\right) = \left(R\cdot v\right) \times \left( R\cdot u\right) = R \cdot \left( v\times u\right) = R \cdot [v] \cdot R^T \cdot \left( R \cdot u\right)$$ therefore  $$ [R\cdot v] \cdot \left( R \cdot u\right) = R \cdot [v] \cdot R^T \cdot \left( R \cdot u\right) $$ hence $\left([R\cdot v] - R \cdot [v] \cdot R^T \right) \cdot w = 0$ for all $w = R \cdot u$ with $u \in \mathbb{R}^3$. It follows that $\left([R\cdot v] - R \cdot [v] \cdot R^T \right) = O_{3 \times 3}$ Edit: Suppose $v = \begin{bmatrix} a\\b\\c\end{bmatrix}$ then $[v] = \begin{bmatrix}0 &-c &b\\ c &0 &-a\\ -b &a &0 \end{bmatrix}$","Let $v \in \mathbb{R}^3$ and $R$ be a rotation matrix (probably orthogonal is enough). I think the following property is true: $$ [R\cdot v] = R \cdot [v] \cdot R^T$$ where $[v]$ is the skew symmetric form of the vector $v$. I should verify this by direct calculation, but I am not in the mood right now, hence I want to know if the following proof is true: proof: let $u \in \mathbb{R}^3$ hence $$ [R\cdot v] \cdot \left( R \cdot u\right) = \left(R\cdot v\right) \times \left( R\cdot u\right) = R \cdot \left( v\times u\right) = R \cdot [v] \cdot R^T \cdot \left( R \cdot u\right)$$ therefore  $$ [R\cdot v] \cdot \left( R \cdot u\right) = R \cdot [v] \cdot R^T \cdot \left( R \cdot u\right) $$ hence $\left([R\cdot v] - R \cdot [v] \cdot R^T \right) \cdot w = 0$ for all $w = R \cdot u$ with $u \in \mathbb{R}^3$. It follows that $\left([R\cdot v] - R \cdot [v] \cdot R^T \right) = O_{3 \times 3}$ Edit: Suppose $v = \begin{bmatrix} a\\b\\c\end{bmatrix}$ then $[v] = \begin{bmatrix}0 &-c &b\\ c &0 &-a\\ -b &a &0 \end{bmatrix}$",,"['linear-algebra', 'matrices']"
45,Matrix involving the central moments of a random variable,Matrix involving the central moments of a random variable,,Consider the matrix :   $$M=\begin{pmatrix} 1& 0& \mu_2 \\ 0& \mu_2& \mu_3 \\ \mu_2& \mu_3& \mu_4 \end{pmatrix}$$ where $\mu_k$ denotes the $k^{th}$ central moment of a RV X i.e. $\mu_k := E(X-E(X))^k$. Prove that: (a) $M$ has non-negative determinant. (b) $M$ is non-negative definite. (c) Generalize this result to higher dimensions. My thoughts: (a) I tried to break the moments to lower order moments and to obtain the result but it didn't workout. (b) & (c) Could not come up with anything.,Consider the matrix :   $$M=\begin{pmatrix} 1& 0& \mu_2 \\ 0& \mu_2& \mu_3 \\ \mu_2& \mu_3& \mu_4 \end{pmatrix}$$ where $\mu_k$ denotes the $k^{th}$ central moment of a RV X i.e. $\mu_k := E(X-E(X))^k$. Prove that: (a) $M$ has non-negative determinant. (b) $M$ is non-negative definite. (c) Generalize this result to higher dimensions. My thoughts: (a) I tried to break the moments to lower order moments and to obtain the result but it didn't workout. (b) & (c) Could not come up with anything.,,['linear-algebra']
46,Eigenvalues and Eigenvectors of a block tridiagonal block Matrix,Eigenvalues and Eigenvectors of a block tridiagonal block Matrix,,"Let $T$ be the $2 N \times 2 N$ matrix defined by $$ T =  \begin{pmatrix} A && B\\ -B^* && -A^* \end{pmatrix} $$ where $*$ is entry wise complex conjugation, $A$ is a Hermitian $N \times N$ tridiagonal Toeplitz matrix $$ A = \begin{pmatrix} 	a & \alpha& 0 & \dots & 0\\ 	\alpha^* & a & \alpha & \vdots & \vdots\\ 	0 & \alpha^* & \ddots  & \ddots & \vdots\\ 	\vdots & \ddots & \ddots & a & \alpha\\ 	0 & \dots & \dots & \alpha^* & a 	\end{pmatrix} $$ with $a$ real and $\alpha$ in general complex and $B$ is the symmetric tridiagonal Toeplitz matrix $$ B = \begin{pmatrix} 	b & \beta& 0 & \dots & 0\\ 	\beta & b & \beta & \vdots & \vdots\\ 	0 & \beta & \ddots  & \ddots & \vdots\\ 	\vdots & \ddots & \ddots & b & \beta\\ 	0 & \dots & \dots & \beta & b 	\end{pmatrix} $$ where $b$ is real and $\beta$ is complex. I want to find the eigenvalues of eigenvectors of $T$. Here's what I have so far. If $\alpha$ is real, then the solution is quite simple since $A$ is symmetric. Thus, $T$ can be written as $$ T = i \sigma_x \otimes Im(B)+i\sigma_y \otimes Re(B)+\sigma_z \otimes A $$ where $\sigma_i$ are the usual Pauli matrices. It is well known that symmetric tridiagonal Toeplitz matrices all the same eigenvectors and their eigenvalues are particularly simple. We can simultaneously diagonalize $Re(B), Im(B),$ and $A$ and rewrite $T$ as a sum of $2 \times 2$ block matrices $$ T = \sum_{n =1}^N  \begin{pmatrix} a+ 2 \alpha \cos(\frac{\pi n}{(N+1)}) && b+\beta \cos(\frac{\pi n}{(N+1)})\\ -(b+\beta^* \cos(\frac{\pi n}{(N+1)})) && -(a+ 2 \alpha \cos(\frac{\pi n}{(N+1)})) \end{pmatrix} $$ And finding the eigenvalues and eigenvectors becomes diagonalizing a $2 \times 2$ matrix. Now if $\alpha$ is complex, we encounter a problem. $T$ can now be written as $$ T = Id \otimes Im(A) +i \sigma_x \otimes Im(B)+i\sigma_y \otimes Re(B)+\sigma_z \otimes Re(A) $$ by hermicity of $A$, $Im(A)$ is an antisymmetric tridiagonal Toeplitz matrix which doesn't commute with $Re(A), Re(B), Im(B)$ (but it almost commutes in that the only non zero elements of the commutator are at the top left and bottom right of the matrix). So I tried something else. In a different basis (well really just swapping the tensor product) we get  $$ T = Im(A) \otimes Id + Im(B) \otimes i \sigma_x + Re(B) \otimes i\sigma_y+ Re(A) \otimes \sigma_z $$ Which we can write as a block tridiagonal Toeplitz matrix $$ T  = \begin{pmatrix} 	C & D& 0 & \dots & 0\\ 	E & C & D & \vdots & \vdots\\ 	0 & E & \ddots  & \ddots & \vdots\\ 	\vdots & \ddots & \ddots & C & D\\ 	0 & \dots & \dots & E & C 	\end{pmatrix} $$ with $C, D, E$ the $2 \times 2$ matrices. $$ C =  \begin{pmatrix} a && b\\ -b && -a \end{pmatrix} \: \: \: D = \begin{pmatrix} \alpha^* && \beta\\ -\beta^* && -\alpha \end{pmatrix} \: \: \: E = \begin{pmatrix} \alpha && \beta\\ -\beta^* && -\alpha^* \end{pmatrix} $$ And so the eigenvalue problem becomes a three term difference equation  $$ E\begin{pmatrix}x_{j-1} \\ y_{j-1}\end{pmatrix} + C\begin{pmatrix}x_{j} \\ y_{j}\end{pmatrix} + D\begin{pmatrix}x_{j+1} \\ y_{j+1}\end{pmatrix} = \lambda \begin{pmatrix} x_{j} \\ y_{j} \end{pmatrix} $$ with boundary conditions $x_0 = x_{N+1} = y_0 = y_{N+1} = 0$. Now, like in the regular tridiagonal Topeplitz matrix case, I make an ansatz of $x_j = x r^j$ and $y_j = y r^j$ the difference equation becomes \begin{equation} (rE+(C-\lambda)+r^{-1}D ) \begin{pmatrix} x \\y \end{pmatrix} = 0 \end{equation} For a non trivial solution to our equation we need the determinant to vanish, which is a polynomial of degree four in $r$ and we assume for the moment that there are four distinct roots $r_k$, $k = {1,2,3,4}$. Thus the general solution is of the form $$ \begin{pmatrix} x_j \\ y_j  \end{pmatrix} = \sum_{k = 1}^4 c_k \lambda^j_k  \begin{pmatrix} x_k \\ y_k \end{pmatrix} $$ where $(x_k,y_k)$ is in the kernel of the difference equation above. We need to find the $c_k$ which satisfy the boundary conditions which equivalent to finding a non trivial solution to $$ \begin{pmatrix} x_1 & x_2 & x_3 & x_4 \\ y_1 & y_2 & y_3 & y_4 \\ \lambda_1^{N+1} x_1 & \lambda_2^{N+1} x_2 & \lambda_3^{N+1} x_3 & \lambda_4^{N+1} x_4 \\ \lambda_1^{N+1} y_1 & \lambda_2^{N+1} y_2 & \lambda_3^{N+1} y_3 & \lambda_4^{N+1} y_4 \end{pmatrix} \begin{pmatrix} c_1\\ c_2\\ c_3\\ c_4 \end{pmatrix} = \begin{pmatrix} 0\\ 0\\ 0\\ 0 \end{pmatrix} $$ For a nontrivial solution, we need the determinant to vanish, which imposes a conditions on the roots $\lambda_k$. So I tried something else. The difference equation above implies that once we know $(x_1, y_1)$, we know can recursively find the value of $(x_j, y_j)$ for any $j$. So defined the $2 \times 2$ matrix $T_j$ by $$ \begin{pmatrix} u_j \\ v_j \end{pmatrix} = T_j \begin{pmatrix} u_1 \\ v_1 \end{pmatrix} $$ Then this sequence of matrices satisfies a second order matrix difference equation $$ E T_{j-1} + (C-\lambda)T_j+D T_{j+1} = 0 $$ with boundary conditions $T_0 = 0$ and $T_1 = 1$ (the two $\times$ two identity matrix). Again if $\alpha$ is real, then $E = D$ and we can multiply both sides of the difference equation by $E^{-1}$ to obtain $$ T_{j-1}+E^{-1} (C-\lambda) T_{j} + T_{j+1} = 0 $$ This difference equation is solved by the matrix $$ T_j = \dfrac{\sin(\beta j)}{\sin(\beta)}  $$ where the matrix $\beta$ is defined by $\cos(\beta) = - \frac{1}{2}E^{-1}(C-\lambda) $. The boundary condition for $x_{N+1} = y{N+1} = 0$ means that $(x_1, y_1) $ must be in the kernel of $T_{N+1}$ which can only occur if $\det(T_{N+1}) = 0$. This conditions then translates to a condition on the eigenvalues $\lambda$ and we do in fact recover what we obtained before. But when $\alpha$ is complex, $E \neq D$ and we can't do as we did before since we have to worry about matrices which don't commute. The only time we can in fact do that is if $a = b = 0$ so that our difference equation becomes $$ E T_{j-1}-\lambda T_j + D T_{j+1} $$ Since $E$ and $D$ commute, we can work in the basis which diagonalizes both simultaneously, and our matrix equation just becomes two uncoupled scalar second order difference equations which are easy to solve. All this to say that I'm quite stuck. My other idea was to first find the eigenvalues of $T$ directly by computing the determinant (the determinant must also satisfies some recursion relation, right?) Anyway, any help would be greatly appreciated!","Let $T$ be the $2 N \times 2 N$ matrix defined by $$ T =  \begin{pmatrix} A && B\\ -B^* && -A^* \end{pmatrix} $$ where $*$ is entry wise complex conjugation, $A$ is a Hermitian $N \times N$ tridiagonal Toeplitz matrix $$ A = \begin{pmatrix} 	a & \alpha& 0 & \dots & 0\\ 	\alpha^* & a & \alpha & \vdots & \vdots\\ 	0 & \alpha^* & \ddots  & \ddots & \vdots\\ 	\vdots & \ddots & \ddots & a & \alpha\\ 	0 & \dots & \dots & \alpha^* & a 	\end{pmatrix} $$ with $a$ real and $\alpha$ in general complex and $B$ is the symmetric tridiagonal Toeplitz matrix $$ B = \begin{pmatrix} 	b & \beta& 0 & \dots & 0\\ 	\beta & b & \beta & \vdots & \vdots\\ 	0 & \beta & \ddots  & \ddots & \vdots\\ 	\vdots & \ddots & \ddots & b & \beta\\ 	0 & \dots & \dots & \beta & b 	\end{pmatrix} $$ where $b$ is real and $\beta$ is complex. I want to find the eigenvalues of eigenvectors of $T$. Here's what I have so far. If $\alpha$ is real, then the solution is quite simple since $A$ is symmetric. Thus, $T$ can be written as $$ T = i \sigma_x \otimes Im(B)+i\sigma_y \otimes Re(B)+\sigma_z \otimes A $$ where $\sigma_i$ are the usual Pauli matrices. It is well known that symmetric tridiagonal Toeplitz matrices all the same eigenvectors and their eigenvalues are particularly simple. We can simultaneously diagonalize $Re(B), Im(B),$ and $A$ and rewrite $T$ as a sum of $2 \times 2$ block matrices $$ T = \sum_{n =1}^N  \begin{pmatrix} a+ 2 \alpha \cos(\frac{\pi n}{(N+1)}) && b+\beta \cos(\frac{\pi n}{(N+1)})\\ -(b+\beta^* \cos(\frac{\pi n}{(N+1)})) && -(a+ 2 \alpha \cos(\frac{\pi n}{(N+1)})) \end{pmatrix} $$ And finding the eigenvalues and eigenvectors becomes diagonalizing a $2 \times 2$ matrix. Now if $\alpha$ is complex, we encounter a problem. $T$ can now be written as $$ T = Id \otimes Im(A) +i \sigma_x \otimes Im(B)+i\sigma_y \otimes Re(B)+\sigma_z \otimes Re(A) $$ by hermicity of $A$, $Im(A)$ is an antisymmetric tridiagonal Toeplitz matrix which doesn't commute with $Re(A), Re(B), Im(B)$ (but it almost commutes in that the only non zero elements of the commutator are at the top left and bottom right of the matrix). So I tried something else. In a different basis (well really just swapping the tensor product) we get  $$ T = Im(A) \otimes Id + Im(B) \otimes i \sigma_x + Re(B) \otimes i\sigma_y+ Re(A) \otimes \sigma_z $$ Which we can write as a block tridiagonal Toeplitz matrix $$ T  = \begin{pmatrix} 	C & D& 0 & \dots & 0\\ 	E & C & D & \vdots & \vdots\\ 	0 & E & \ddots  & \ddots & \vdots\\ 	\vdots & \ddots & \ddots & C & D\\ 	0 & \dots & \dots & E & C 	\end{pmatrix} $$ with $C, D, E$ the $2 \times 2$ matrices. $$ C =  \begin{pmatrix} a && b\\ -b && -a \end{pmatrix} \: \: \: D = \begin{pmatrix} \alpha^* && \beta\\ -\beta^* && -\alpha \end{pmatrix} \: \: \: E = \begin{pmatrix} \alpha && \beta\\ -\beta^* && -\alpha^* \end{pmatrix} $$ And so the eigenvalue problem becomes a three term difference equation  $$ E\begin{pmatrix}x_{j-1} \\ y_{j-1}\end{pmatrix} + C\begin{pmatrix}x_{j} \\ y_{j}\end{pmatrix} + D\begin{pmatrix}x_{j+1} \\ y_{j+1}\end{pmatrix} = \lambda \begin{pmatrix} x_{j} \\ y_{j} \end{pmatrix} $$ with boundary conditions $x_0 = x_{N+1} = y_0 = y_{N+1} = 0$. Now, like in the regular tridiagonal Topeplitz matrix case, I make an ansatz of $x_j = x r^j$ and $y_j = y r^j$ the difference equation becomes \begin{equation} (rE+(C-\lambda)+r^{-1}D ) \begin{pmatrix} x \\y \end{pmatrix} = 0 \end{equation} For a non trivial solution to our equation we need the determinant to vanish, which is a polynomial of degree four in $r$ and we assume for the moment that there are four distinct roots $r_k$, $k = {1,2,3,4}$. Thus the general solution is of the form $$ \begin{pmatrix} x_j \\ y_j  \end{pmatrix} = \sum_{k = 1}^4 c_k \lambda^j_k  \begin{pmatrix} x_k \\ y_k \end{pmatrix} $$ where $(x_k,y_k)$ is in the kernel of the difference equation above. We need to find the $c_k$ which satisfy the boundary conditions which equivalent to finding a non trivial solution to $$ \begin{pmatrix} x_1 & x_2 & x_3 & x_4 \\ y_1 & y_2 & y_3 & y_4 \\ \lambda_1^{N+1} x_1 & \lambda_2^{N+1} x_2 & \lambda_3^{N+1} x_3 & \lambda_4^{N+1} x_4 \\ \lambda_1^{N+1} y_1 & \lambda_2^{N+1} y_2 & \lambda_3^{N+1} y_3 & \lambda_4^{N+1} y_4 \end{pmatrix} \begin{pmatrix} c_1\\ c_2\\ c_3\\ c_4 \end{pmatrix} = \begin{pmatrix} 0\\ 0\\ 0\\ 0 \end{pmatrix} $$ For a nontrivial solution, we need the determinant to vanish, which imposes a conditions on the roots $\lambda_k$. So I tried something else. The difference equation above implies that once we know $(x_1, y_1)$, we know can recursively find the value of $(x_j, y_j)$ for any $j$. So defined the $2 \times 2$ matrix $T_j$ by $$ \begin{pmatrix} u_j \\ v_j \end{pmatrix} = T_j \begin{pmatrix} u_1 \\ v_1 \end{pmatrix} $$ Then this sequence of matrices satisfies a second order matrix difference equation $$ E T_{j-1} + (C-\lambda)T_j+D T_{j+1} = 0 $$ with boundary conditions $T_0 = 0$ and $T_1 = 1$ (the two $\times$ two identity matrix). Again if $\alpha$ is real, then $E = D$ and we can multiply both sides of the difference equation by $E^{-1}$ to obtain $$ T_{j-1}+E^{-1} (C-\lambda) T_{j} + T_{j+1} = 0 $$ This difference equation is solved by the matrix $$ T_j = \dfrac{\sin(\beta j)}{\sin(\beta)}  $$ where the matrix $\beta$ is defined by $\cos(\beta) = - \frac{1}{2}E^{-1}(C-\lambda) $. The boundary condition for $x_{N+1} = y{N+1} = 0$ means that $(x_1, y_1) $ must be in the kernel of $T_{N+1}$ which can only occur if $\det(T_{N+1}) = 0$. This conditions then translates to a condition on the eigenvalues $\lambda$ and we do in fact recover what we obtained before. But when $\alpha$ is complex, $E \neq D$ and we can't do as we did before since we have to worry about matrices which don't commute. The only time we can in fact do that is if $a = b = 0$ so that our difference equation becomes $$ E T_{j-1}-\lambda T_j + D T_{j+1} $$ Since $E$ and $D$ commute, we can work in the basis which diagonalizes both simultaneously, and our matrix equation just becomes two uncoupled scalar second order difference equations which are easy to solve. All this to say that I'm quite stuck. My other idea was to first find the eigenvalues of $T$ directly by computing the determinant (the determinant must also satisfies some recursion relation, right?) Anyway, any help would be greatly appreciated!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
47,Eigenspaces of a symmetric matrix and its principal submatrices,Eigenspaces of a symmetric matrix and its principal submatrices,,"There are a lot of well known theorems that relate the eigenvalues of a real symmetric matrix $A$ to the eigenvalues of its principal submatrices. I can't help but wonder, are there similar theorems which relate the eigenspaces in these cases? For instance, if $v$ is an eigenvector for the smallest eigenvalue of a principal submatrix, then can we say anything about the extension of $v$ in relation to the eigenspaces of $A$? I was hoping for something like: If $v$ is an eigenvector for the smallest eigenvalue of a principal submatrix $B$ inside $A$, then $(v,0)$ is contained in the span of the two smallest eigenspaces of $A$. I.e. at worst, you need to use the smallest eigenvalue bigger than the eigenvalue associated to $v$ to describe $(v,0)$. Sorry for being so imprecise, I am frankly unsure of what specific questions one can reasonably ask here.","There are a lot of well known theorems that relate the eigenvalues of a real symmetric matrix $A$ to the eigenvalues of its principal submatrices. I can't help but wonder, are there similar theorems which relate the eigenspaces in these cases? For instance, if $v$ is an eigenvector for the smallest eigenvalue of a principal submatrix, then can we say anything about the extension of $v$ in relation to the eigenspaces of $A$? I was hoping for something like: If $v$ is an eigenvector for the smallest eigenvalue of a principal submatrix $B$ inside $A$, then $(v,0)$ is contained in the span of the two smallest eigenspaces of $A$. I.e. at worst, you need to use the smallest eigenvalue bigger than the eigenvalue associated to $v$ to describe $(v,0)$. Sorry for being so imprecise, I am frankly unsure of what specific questions one can reasonably ask here.",,['linear-algebra']
48,Diagonalisable commuting endomorphisms on infinite dimensional space,Diagonalisable commuting endomorphisms on infinite dimensional space,,"It is well-know that for $V$ a complex finite dimensional vector space and $\{h_i\}$ a family of diagonalisable commuting endomorphisms then there is a basis for $V$ of common eigenvectors. This is because in the weight decomposition $V = \oplus V_\lambda$, the subspaces are $h_k$ invariant and then they are diagonalisable there, and we can decompose each of the $V_\lambda$ into weight spaces and so on, the process will finish eventually in case we have an infinite set of endomorphism, but this might not be true if $V$ is infinite dimensional. Is there an easy counter-example where the $\{h_i\}$ are commuting diagonalisable but there is no basis of common eigenvectors?","It is well-know that for $V$ a complex finite dimensional vector space and $\{h_i\}$ a family of diagonalisable commuting endomorphisms then there is a basis for $V$ of common eigenvectors. This is because in the weight decomposition $V = \oplus V_\lambda$, the subspaces are $h_k$ invariant and then they are diagonalisable there, and we can decompose each of the $V_\lambda$ into weight spaces and so on, the process will finish eventually in case we have an infinite set of endomorphism, but this might not be true if $V$ is infinite dimensional. Is there an easy counter-example where the $\{h_i\}$ are commuting diagonalisable but there is no basis of common eigenvectors?",,"['real-analysis', 'linear-algebra', 'functional-analysis', 'analysis']"
49,Upper bound for $\|A^{-1}\|_F$,Upper bound for,\|A^{-1}\|_F,"I'm trying to come up with an upper bound for the Frobenius norm of the inverse of a non-singular matrix. To be precise, let $A \in \mathbb{C}^{n \times n}$ be non-singular, with entries denoted by $a_{ij}$, and let $$\|A\|_F = \sqrt{\sum_{i,j=1}^n |a_{ij}|^2}$$ be the Frobenius norm of $A$. I'm trying to get an upper bound for $\|A^{-1}\|_F$ such that: 1) This bound doesn't rely on the computation of the determinant of $A$ nor its eigenvalues or singular values. 2) This bound doesn't need to be sharp, but can't be absurdly large (like something of exponential order). Ideally, I'm looking for a bound relying on $n$, on the entries of $A$ and on any norm of $A$. Something like $\|A^{-1}\|_F \leq \sqrt{n}\left( \frac{\|A\|_2}{\|A\|_F} \right)^3$ would be ok (I just invented this bound as an example). In fact, I've tried something, but I think someone could come up a better idea. Here is what I did: Let $\sigma_1 \geq \ldots \sigma_n > 0$ be the singular values of $A$, so $$\|A\|_F = \sqrt{\sigma_1^2 + \ldots + \sigma_n^2}$$ and $$\|A^{-1}\|_F = \sqrt{\frac{1}{\sigma_1^2} + \ldots + \frac{1}{\sigma_n^2}}.$$ By this article (equation (1)), we have the lower bound $$\sigma_n \geq |\det A|\left( \frac{n-1}{\|A\|_F^2} \right)^\frac{n-1}{2}. $$ Therefore, $$\|A^{-1}\|_F = \sqrt{\frac{1}{\sigma_1^2} + \ldots + \frac{1}{\sigma_n^2}} \leq \sqrt{\frac{n}{\sigma_n^2}} = \frac{\sqrt{n}}{\sigma_n} \leq \frac{\sqrt{n}}{|\det A|} \left( \frac{\|A\|_F^2}{n-1} \right)^\frac{n-1}{2}.$$ But now I have the problem of bounding the determinant... EDIT: I'm accepting lower bounds to the determinant and lowest singular value of $A$ as answers. The bound may be large, as long as it is directly accessible from the entries or the norm of $A$. Thanks.","I'm trying to come up with an upper bound for the Frobenius norm of the inverse of a non-singular matrix. To be precise, let $A \in \mathbb{C}^{n \times n}$ be non-singular, with entries denoted by $a_{ij}$, and let $$\|A\|_F = \sqrt{\sum_{i,j=1}^n |a_{ij}|^2}$$ be the Frobenius norm of $A$. I'm trying to get an upper bound for $\|A^{-1}\|_F$ such that: 1) This bound doesn't rely on the computation of the determinant of $A$ nor its eigenvalues or singular values. 2) This bound doesn't need to be sharp, but can't be absurdly large (like something of exponential order). Ideally, I'm looking for a bound relying on $n$, on the entries of $A$ and on any norm of $A$. Something like $\|A^{-1}\|_F \leq \sqrt{n}\left( \frac{\|A\|_2}{\|A\|_F} \right)^3$ would be ok (I just invented this bound as an example). In fact, I've tried something, but I think someone could come up a better idea. Here is what I did: Let $\sigma_1 \geq \ldots \sigma_n > 0$ be the singular values of $A$, so $$\|A\|_F = \sqrt{\sigma_1^2 + \ldots + \sigma_n^2}$$ and $$\|A^{-1}\|_F = \sqrt{\frac{1}{\sigma_1^2} + \ldots + \frac{1}{\sigma_n^2}}.$$ By this article (equation (1)), we have the lower bound $$\sigma_n \geq |\det A|\left( \frac{n-1}{\|A\|_F^2} \right)^\frac{n-1}{2}. $$ Therefore, $$\|A^{-1}\|_F = \sqrt{\frac{1}{\sigma_1^2} + \ldots + \frac{1}{\sigma_n^2}} \leq \sqrt{\frac{n}{\sigma_n^2}} = \frac{\sqrt{n}}{\sigma_n} \leq \frac{\sqrt{n}}{|\det A|} \left( \frac{\|A\|_F^2}{n-1} \right)^\frac{n-1}{2}.$$ But now I have the problem of bounding the determinant... EDIT: I'm accepting lower bounds to the determinant and lowest singular value of $A$ as answers. The bound may be large, as long as it is directly accessible from the entries or the norm of $A$. Thanks.",,"['linear-algebra', 'matrices', 'inequality', 'normed-spaces']"
50,Full rank condition when stacking vector valued function,Full rank condition when stacking vector valued function,,"Let $f: \mathbb{R}^1 \mapsto \mathbb{R}^n$ be a smooth vector-valued function. Consider  the $n \times n$ matrix $A(x)$ obtained from a vector $x \in \mathbb{R}^n$ by appropriately stacking  $[f(x_1),\dots, f(x_n)]$. The question is: what are easily verifiable sufficient conditions which guarantee that the matrix $A$ is generically of full rank, that is, $A(x)$ is full rank except possibly for points $x$ in sets of measure zero? To fix ideas, obviously $f$ cannot be linear, otherwise there is linear dependence in $A$.","Let $f: \mathbb{R}^1 \mapsto \mathbb{R}^n$ be a smooth vector-valued function. Consider  the $n \times n$ matrix $A(x)$ obtained from a vector $x \in \mathbb{R}^n$ by appropriately stacking  $[f(x_1),\dots, f(x_n)]$. The question is: what are easily verifiable sufficient conditions which guarantee that the matrix $A$ is generically of full rank, that is, $A(x)$ is full rank except possibly for points $x$ in sets of measure zero? To fix ideas, obviously $f$ cannot be linear, otherwise there is linear dependence in $A$.",,"['linear-algebra', 'vectors', 'vector-analysis', 'matrix-rank']"
51,Defining an inner product by means of a multilinear functional,Defining an inner product by means of a multilinear functional,,"Let $V$ be a complex vector space and an anti-linear involution $J:V \rightarrow V$ (this means that $J^2 = I$  and if $\lambda \in \mathbb{C}$ and $x, y \in V$ we have $J(\lambda x + y) = \overline{\lambda} J(x)+J(y)$). Fix $N$ a natural number and consider a multilinear functional $\omega:V^{2N}\rightarrow \mathbb{C}$ such that: $\omega(x_1,...,x_{2N}) = \omega(x_2, ..., x_{2N},x_1)$, for every $x_1,x_2,...,x_{2N} \in V$ The matrix $A$ whose matrix elements $A_{ij}$ defined by  $A_{ij} = \omega(Jx_{i1},..., Jx_{iN}, x_{j1},..., x_{jN})$ is positive semi-definite for every $x_{kl} \in V$, $k=1,..., n$ and $l=1,...,N$, where $n$ can be any natural number. I saw this on a paper that I'm currently reading and the author claims that the condition 2 implies that the following: $$\langle x, y \rangle = \omega(x_1,..., x_N, Jy_N,... Jy_1)$$ defines an inner product in $V^N$. I'm not sure about this because this is not linear in the first coordinate. Did I get this wrong?","Let $V$ be a complex vector space and an anti-linear involution $J:V \rightarrow V$ (this means that $J^2 = I$  and if $\lambda \in \mathbb{C}$ and $x, y \in V$ we have $J(\lambda x + y) = \overline{\lambda} J(x)+J(y)$). Fix $N$ a natural number and consider a multilinear functional $\omega:V^{2N}\rightarrow \mathbb{C}$ such that: $\omega(x_1,...,x_{2N}) = \omega(x_2, ..., x_{2N},x_1)$, for every $x_1,x_2,...,x_{2N} \in V$ The matrix $A$ whose matrix elements $A_{ij}$ defined by  $A_{ij} = \omega(Jx_{i1},..., Jx_{iN}, x_{j1},..., x_{jN})$ is positive semi-definite for every $x_{kl} \in V$, $k=1,..., n$ and $l=1,...,N$, where $n$ can be any natural number. I saw this on a paper that I'm currently reading and the author claims that the condition 2 implies that the following: $$\langle x, y \rangle = \omega(x_1,..., x_N, Jy_N,... Jy_1)$$ defines an inner product in $V^N$. I'm not sure about this because this is not linear in the first coordinate. Did I get this wrong?",,"['linear-algebra', 'inner-products']"
52,Any known relationship between the eigenvalues of correlation matrix and covariance matrix?,Any known relationship between the eigenvalues of correlation matrix and covariance matrix?,,"For principal component analysis we usually use correlation matrix when the data are measured in different units. In case where data units are consistent we can use covariance matrix instead. I want to ask if there is any known mathematical relationship between eigenvalues of the correlation matrix and the covariance matrix, so that if we know one of them, we might be able to compute the other by some formula or algorithm simpler than running the entire PCA again?","For principal component analysis we usually use correlation matrix when the data are measured in different units. In case where data units are consistent we can use covariance matrix instead. I want to ask if there is any known mathematical relationship between eigenvalues of the correlation matrix and the covariance matrix, so that if we know one of them, we might be able to compute the other by some formula or algorithm simpler than running the entire PCA again?",,"['linear-algebra', 'matrices', 'statistics']"
53,Find the values of $a$ $\in$ $\mathbb{R}$ where the system $Ax=x$ allows a solution different to the null one,Find the values of    where the system  allows a solution different to the null one,a \in \mathbb{R} Ax=x,"I have to find the values of $a$ $\in$ $\mathbb{R}$ where the system $Ax=x$ allows a solution different to the null one, and then solve the system with those values I found of the following matrix: $A=\begin{bmatrix}2&0&2&\\2&a+1&a&\\-1&a&0&\end{bmatrix}$ So, considering the statement, what I did is the following: First, $Ax-x=0$ Then, $(A-I)x=0$ And after, I found the cases of $a$ $\in$ $\mathbb{R}$ where: $Det(A-I)=0$ And, I did that, considering that the system will admit a solution different from the trivial one, if the determinant of the matrix of this particular system is equal to zero. Is my reasoning correct?","I have to find the values of $a$ $\in$ $\mathbb{R}$ where the system $Ax=x$ allows a solution different to the null one, and then solve the system with those values I found of the following matrix: $A=\begin{bmatrix}2&0&2&\\2&a+1&a&\\-1&a&0&\end{bmatrix}$ So, considering the statement, what I did is the following: First, $Ax-x=0$ Then, $(A-I)x=0$ And after, I found the cases of $a$ $\in$ $\mathbb{R}$ where: $Det(A-I)=0$ And, I did that, considering that the system will admit a solution different from the trivial one, if the determinant of the matrix of this particular system is equal to zero. Is my reasoning correct?",,"['linear-algebra', 'determinant']"
54,Orders of coordinates in a linear subspace,Orders of coordinates in a linear subspace,,"Let $X$ be a linear subspace of $\mathbb{R}^n$. For how many permutations  $p$ on ${1,...,n}$ does there exists $x$ in $X$ with  $x_{p(1)} < x_{p(2)} < ... < x_{p(n)}$? We can test each permutation by solving a linear program, but this is time consuming when $n$ is large. Is there a quicker way to solve this problem, or at least approximate the solution?","Let $X$ be a linear subspace of $\mathbb{R}^n$. For how many permutations  $p$ on ${1,...,n}$ does there exists $x$ in $X$ with  $x_{p(1)} < x_{p(2)} < ... < x_{p(n)}$? We can test each permutation by solving a linear program, but this is time consuming when $n$ is large. Is there a quicker way to solve this problem, or at least approximate the solution?",,"['linear-algebra', 'linear-programming']"
55,Prove that $\operatorname{cond}_2(A) \le n \cdot \operatorname{cond}_2(DAD)$,Prove that,\operatorname{cond}_2(A) \le n \cdot \operatorname{cond}_2(DAD),"Let the matrix $A \in \mathbb{C}^{n\times n}$ be positive definite with the unit diagonal. Let the matrix $D \in \mathbb{C}^{n\times n}$ be diagonal positive definite. Prove that $$\operatorname{cond}_2(A) \le n\cdot \operatorname{cond}_2(DAD),$$ where $\operatorname{cond}_2(A) = \|A\|_2\|A^{-1}\|_2$. Really I have no idea how to prove it. I can only propose several suitable inequalities such that $\|B\|_2^2 \le \|B\|_1\|B\|_\infty$, $\|DAD\|_2 \le d_{\max}^2\|A\|_2$, where $d_{\max} = \|D\|_2$ is the maximal diagonal element of $D$. And $\|D^{-1}A^{-1}D^{-1}\|_2 \le \dfrac{1}{d_{\min}}\|A^{-1}\|_2$, so $\operatorname{cond}_2(D^{-1}AD^{-1}) \le \dfrac{d_{\min}}{d_{\max}} \operatorname{cond}_2(A)$. Great thanks for any help or ideas!","Let the matrix $A \in \mathbb{C}^{n\times n}$ be positive definite with the unit diagonal. Let the matrix $D \in \mathbb{C}^{n\times n}$ be diagonal positive definite. Prove that $$\operatorname{cond}_2(A) \le n\cdot \operatorname{cond}_2(DAD),$$ where $\operatorname{cond}_2(A) = \|A\|_2\|A^{-1}\|_2$. Really I have no idea how to prove it. I can only propose several suitable inequalities such that $\|B\|_2^2 \le \|B\|_1\|B\|_\infty$, $\|DAD\|_2 \le d_{\max}^2\|A\|_2$, where $d_{\max} = \|D\|_2$ is the maximal diagonal element of $D$. And $\|D^{-1}A^{-1}D^{-1}\|_2 \le \dfrac{1}{d_{\min}}\|A^{-1}\|_2$, so $\operatorname{cond}_2(D^{-1}AD^{-1}) \le \dfrac{d_{\min}}{d_{\max}} \operatorname{cond}_2(A)$. Great thanks for any help or ideas!",,"['linear-algebra', 'matrices', 'inequality', 'normed-spaces']"
56,$T$ has $(x-1)^4$ as its characteristic polynomial then what is the largest possible dimension of the centraliser of $T$,has  as its characteristic polynomial then what is the largest possible dimension of the centraliser of,T (x-1)^4 T,Q.If $ I\neq T\in M_{4}(\mathbb{C})$ has $(x-1)^4$ as its characteristic   polynomial then what is the largest possible dimension of the   centraliser of $T$ in $M_{4}(\mathbb{C})$.$(=$the space of all   matrices that commute with $T)?$ I am looking for a hint. I know that $T$ is not diagonalizable. But how should I proceed further?,Q.If $ I\neq T\in M_{4}(\mathbb{C})$ has $(x-1)^4$ as its characteristic   polynomial then what is the largest possible dimension of the   centraliser of $T$ in $M_{4}(\mathbb{C})$.$(=$the space of all   matrices that commute with $T)?$ I am looking for a hint. I know that $T$ is not diagonalizable. But how should I proceed further?,,"['linear-algebra', 'abstract-algebra', 'matrices', 'vector-spaces', 'contest-math']"
57,On group graded algebras and Brauer groups,On group graded algebras and Brauer groups,,"I was reading the paper ""Algebras graded by groups"" by Knus. I want to test and further my understanding of the paper by asking several questions. Since the paper is not readily available I will detail everything necessary to understand my questions. Please do not be intimidated by the length of this post, the prelimenaries are very easy to read and are included in case of confusion. Prelimenaries Let $K$ be a field and let $G$ be an abelian group, a pairing (or bicharacter) of $G$ is a map $\phi:G\times G\rightarrow K^*$ such that $\phi(g_1+g_2,h)=\phi(g_1,h)\phi(g_2,h)$ , $\phi(g,h_1+h_2)=\phi(g,h_1)\phi(g,h_2)$ . A pairing $\phi$ is symmetric if $\phi(g,h)=\phi(h,g)$ . A pairing $\phi$ is called non-degenerate if $(\forall h\in G:\phi(g,h)=1)\Rightarrow g=e_G$ . A unital finite-dimensional $K$ -algebra $A$ is called $G$ -graded if $A=\bigoplus_{g\in G}A_g$ and $A_gA_h\subset A_{gh}$ , the unit lives in degree zero. A subspace $I\subset A$ is graded if it is the direct sum of the intersections $I\cap A_g$ . We call the graded algebra $A$ simple if there are no proper graded two-sided ideals. We call $A$ central if the only homogeneous elements $x\in A$ such that $xa=\phi(x,a)ax$ or $ax=\phi(a,x)xa$ for all homogeneous $a\in A$ , are in $K$ . Let $H^2(G,K^*)$ be the second cohomology group of $G$ with coefficients in the trivial $G$ -module $K^*$ . Let $f$ be a nonmalized $2$ -cocycle on $G$ and denote by $KG$ the group algebra on $G$ . We define the twisted group algebra $K_fG$ by twisting the multiplication on $KG$ , i.e. $g\cdot h:=f(g,h)gh$ . This turns $K_fG$ into an associative algebra. The Brauer group $\text{Br}(K,G)$ Let $\phi$ be a fixed pairing of an abelian group $G$ . Two $G$ -graded central simple algebras $A$ and $B$ are called equivalent if there exist two $G$ -graded vector spaces $V_1$ and $V_2$ such that $$A\otimes \text{End}_K(V_1)\cong B\otimes \text{End}_K(V_2).$$ Let $\text{Br}(K,G)$ denote the set of equivalence classes, the tensor product induces a group structure on $B(K,G)$ . Here $A\otimes B$ is given the structure of an algebra by defining $$(a\otimes b)(a'\otimes b'):=\phi(b,a')(aa'\otimes bb').$$ One can show that the tensor product of two graded central simple algebras as above is again graded central simple. Main theorem Let $G$ be an abelian group and $\phi$ a symmetric pairing on $G$ which is non-degenerate on any finite subgroup of $G$ . Then a graded central simple algebra $A$ such that $\text{char}(K)$ does not divide $\dim_K(A)$ is isomorphic to $$K_f(H)\otimes M_n(D,H').$$ Here $H$ and $H'$ are subgroups of $G$ such that $G=H\times H'$ , $f$ is an abelian class in $H^2(H,K^*)$ . $M_n(D,H')$ is a matrix algebra over a division ring $D$ graded by $H'$ . Questions What exactly is meant by $M_n(D,H')$ is a matrix algebra over a division ring $D$ graded by $H'$ ? Similarly what is the precise meaning of $\text{End}_k(V)$ if $V$ is $G$ -graded? Is this the usual endomorphism ring or a ring of graded morphisms? Suppose $G=\left\{e_G\right\}$ , then any $G$ -graded central simple $K$ -algebra $A$ is simply a central simple $K$ -algebra. Thus by the main theorem $A\cong K_f(G)\otimes M_n(D,G)$ . Clearly $K_f(G)\cong K$ , hence $A\cong M_n(D,G)$ . Whatever the answer to the previous question is, a trivial grading should give that $A\cong M_n(D)$ . Clearly the size of the Brauer group $\text{Br}(K,G)$ depends on the number of division rings over $K$ . Suppose $K=\mathbb{R}$ , by a result of Frobenius the only division rings over $\mathbb{R}$ are $\mathbb{R},\mathbb{C}$ and the quaternions $\mathbb{H}$ . Hence any central simple algebra over $\mathbb{R}$ is isomorphic to either $M_n(\mathbb{R}), M_n(\mathbb{C})$ or $M_n(\mathbb{H})$ . But $M_n(\mathbb{C})$ is not central as the center is two-dimensional. Does it follow that the Brauer group $\text{Br}(K,\left\{e_G\right\})$ contains at most two elements? To be more precise, a priori it's possible that $M_n(\mathbb{H})$ live in different classes for different $n$ . Let $B$ be an ordinary central simple algebra over $K$ , be declaring this thing to live in degree zero we can view any central simple algebra as a $G$ -graded central simple algebra. Can we view the usual Brauer group $\text{Br}(K)$ as a subgroup of $\text{Br}(K,G)$ ? Consider $K=\mathbb{R}$ and $G=\mathbb{Z}_2\times \mathbb{Z}_2$ and let $\phi$ be the trivial pairing. We know that the quaternions $\mathbb{H}$ are a real division ring. On the other hand we could view the quaternions $\mathbb{H}$ as a $G$ -graded algebra by declaring $\deg(i)=(1,0), \deg(j)=(0,1)$ and $\deg(k)=(1,1)$ . Let me denote the graded version by $\mathbb{H}_G$ . Notice that $\mathbb{H}_G$ is a graded central simple algebra. In light of the main theorem, $\mathbb{H}_G\cong K_f(H)\otimes M_n(D,H')$ , can we specify $H,H',f'$ and $n$ ? Depeding on the answer to $1$ , a natural answer could be $\mathbb{H}_G\cong M_1(\mathbb{H}, G)$ . Last but not least: In general one can define the Brauer group of a braided monoidal category. Now let $G$ be a group and $\phi$ a pairing. One can consider the Hopf algebra $KG$ and using $\phi$ one can define coquasi-triangular structure on $KG$ . Denote by $H$ the resulting coquasi-triangular Hopf algebra. Is the Brauer group $\text{Br}(K,G)$ equal to $\text{Br}(\text{Mod}^H)$ , the Brauer group of the braided monoidal category of $H$ -comodules?","I was reading the paper ""Algebras graded by groups"" by Knus. I want to test and further my understanding of the paper by asking several questions. Since the paper is not readily available I will detail everything necessary to understand my questions. Please do not be intimidated by the length of this post, the prelimenaries are very easy to read and are included in case of confusion. Prelimenaries Let be a field and let be an abelian group, a pairing (or bicharacter) of is a map such that , . A pairing is symmetric if . A pairing is called non-degenerate if . A unital finite-dimensional -algebra is called -graded if and , the unit lives in degree zero. A subspace is graded if it is the direct sum of the intersections . We call the graded algebra simple if there are no proper graded two-sided ideals. We call central if the only homogeneous elements such that or for all homogeneous , are in . Let be the second cohomology group of with coefficients in the trivial -module . Let be a nonmalized -cocycle on and denote by the group algebra on . We define the twisted group algebra by twisting the multiplication on , i.e. . This turns into an associative algebra. The Brauer group Let be a fixed pairing of an abelian group . Two -graded central simple algebras and are called equivalent if there exist two -graded vector spaces and such that Let denote the set of equivalence classes, the tensor product induces a group structure on . Here is given the structure of an algebra by defining One can show that the tensor product of two graded central simple algebras as above is again graded central simple. Main theorem Let be an abelian group and a symmetric pairing on which is non-degenerate on any finite subgroup of . Then a graded central simple algebra such that does not divide is isomorphic to Here and are subgroups of such that , is an abelian class in . is a matrix algebra over a division ring graded by . Questions What exactly is meant by is a matrix algebra over a division ring graded by ? Similarly what is the precise meaning of if is -graded? Is this the usual endomorphism ring or a ring of graded morphisms? Suppose , then any -graded central simple -algebra is simply a central simple -algebra. Thus by the main theorem . Clearly , hence . Whatever the answer to the previous question is, a trivial grading should give that . Clearly the size of the Brauer group depends on the number of division rings over . Suppose , by a result of Frobenius the only division rings over are and the quaternions . Hence any central simple algebra over is isomorphic to either or . But is not central as the center is two-dimensional. Does it follow that the Brauer group contains at most two elements? To be more precise, a priori it's possible that live in different classes for different . Let be an ordinary central simple algebra over , be declaring this thing to live in degree zero we can view any central simple algebra as a -graded central simple algebra. Can we view the usual Brauer group as a subgroup of ? Consider and and let be the trivial pairing. We know that the quaternions are a real division ring. On the other hand we could view the quaternions as a -graded algebra by declaring and . Let me denote the graded version by . Notice that is a graded central simple algebra. In light of the main theorem, , can we specify and ? Depeding on the answer to , a natural answer could be . Last but not least: In general one can define the Brauer group of a braided monoidal category. Now let be a group and a pairing. One can consider the Hopf algebra and using one can define coquasi-triangular structure on . Denote by the resulting coquasi-triangular Hopf algebra. Is the Brauer group equal to , the Brauer group of the braided monoidal category of -comodules?","K G G \phi:G\times G\rightarrow K^* \phi(g_1+g_2,h)=\phi(g_1,h)\phi(g_2,h) \phi(g,h_1+h_2)=\phi(g,h_1)\phi(g,h_2) \phi \phi(g,h)=\phi(h,g) \phi (\forall h\in G:\phi(g,h)=1)\Rightarrow g=e_G K A G A=\bigoplus_{g\in G}A_g A_gA_h\subset A_{gh} I\subset A I\cap A_g A A x\in A xa=\phi(x,a)ax ax=\phi(a,x)xa a\in A K H^2(G,K^*) G G K^* f 2 G KG G K_fG KG g\cdot h:=f(g,h)gh K_fG \text{Br}(K,G) \phi G G A B G V_1 V_2 A\otimes \text{End}_K(V_1)\cong B\otimes \text{End}_K(V_2). \text{Br}(K,G) B(K,G) A\otimes B (a\otimes b)(a'\otimes b'):=\phi(b,a')(aa'\otimes bb'). G \phi G G A \text{char}(K) \dim_K(A) K_f(H)\otimes M_n(D,H'). H H' G G=H\times H' f H^2(H,K^*) M_n(D,H') D H' M_n(D,H') D H' \text{End}_k(V) V G G=\left\{e_G\right\} G K A K A\cong K_f(G)\otimes M_n(D,G) K_f(G)\cong K A\cong M_n(D,G) A\cong M_n(D) \text{Br}(K,G) K K=\mathbb{R} \mathbb{R} \mathbb{R},\mathbb{C} \mathbb{H} \mathbb{R} M_n(\mathbb{R}), M_n(\mathbb{C}) M_n(\mathbb{H}) M_n(\mathbb{C}) \text{Br}(K,\left\{e_G\right\}) M_n(\mathbb{H}) n B K G \text{Br}(K) \text{Br}(K,G) K=\mathbb{R} G=\mathbb{Z}_2\times \mathbb{Z}_2 \phi \mathbb{H} \mathbb{H} G \deg(i)=(1,0), \deg(j)=(0,1) \deg(k)=(1,1) \mathbb{H}_G \mathbb{H}_G \mathbb{H}_G\cong K_f(H)\otimes M_n(D,H') H,H',f' n 1 \mathbb{H}_G\cong M_1(\mathbb{H}, G) G \phi KG \phi KG H \text{Br}(K,G) \text{Br}(\text{Mod}^H) H","['linear-algebra', 'abstract-algebra', 'hopf-algebras', 'monoidal-categories', 'brauer-group']"
58,Show that $\lvert \mathbb Z^n / N\rvert = \lvert \det (A) \rvert$. [duplicate],Show that . [duplicate],\lvert \mathbb Z^n / N\rvert = \lvert \det (A) \rvert,"This question already has answers here : Quotient group $\mathbb Z^n/\ \text{im}(A)$ [duplicate] (2 answers) Closed 7 years ago . Let $N$ be a rank $n$ submodule of $\mathbb Z^n$, and let $A$ be the matrix with rows being the generators of $N$. Show that $\lvert \mathbb Z^n / N\rvert = \lvert \det (A) \rvert$. So this is a homework problem, and I am a little confused. Shouldn't it be the case that $\mathbb Z^n$ is the unique (up to isom.) free module of rank $n$? Then, wouldn't this imply that $N = \mathbb Z^n$? I think I am missing something here, but even assuming that this is indeed the case, then we would be trying to prove that $\lvert \det (A) \rvert = 1$ for every invertible matrix $A$ with entries in $\mathbb Z$, which I don't think is true.","This question already has answers here : Quotient group $\mathbb Z^n/\ \text{im}(A)$ [duplicate] (2 answers) Closed 7 years ago . Let $N$ be a rank $n$ submodule of $\mathbb Z^n$, and let $A$ be the matrix with rows being the generators of $N$. Show that $\lvert \mathbb Z^n / N\rvert = \lvert \det (A) \rvert$. So this is a homework problem, and I am a little confused. Shouldn't it be the case that $\mathbb Z^n$ is the unique (up to isom.) free module of rank $n$? Then, wouldn't this imply that $N = \mathbb Z^n$? I think I am missing something here, but even assuming that this is indeed the case, then we would be trying to prove that $\lvert \det (A) \rvert = 1$ for every invertible matrix $A$ with entries in $\mathbb Z$, which I don't think is true.",,"['linear-algebra', 'abstract-algebra', 'ring-theory', 'modules']"
59,Max norm of $\mathbb C$-linear extension of $\mathbb R$-linear map on Hermitian matrices,Max norm of -linear extension of -linear map on Hermitian matrices,\mathbb C \mathbb R,"This question is an extension of Norms of linear maps . Let $\Phi:H_n(\mathbb C)\to H_n(\mathbb C)$ be a real linear map of norm $1$ on Hermitian $n$-by-$n$ matrices ($n>1$) with the operator norm induced by the Euclidean norm on $\mathbb C^n$.  Let $\overset{\sim}\Phi:M_n(\mathbb C)\to M_n(\mathbb C)$ be the $\mathbb C$-linear extension of $\Phi$ defined by $$\overset{\sim}\Phi(A)=\Phi\left({A+A^* \over 2}\right) + i\Phi\left({A-A^* \over 2i}\right).$$ What is the largest possible norm of $\overset{\sim}\Phi$? Remarks: The answer may depend on $n$.  Perhaps the bounds continue to increase with $n$, so an ideal solution may find the answer as a function of $n$, if there is not a unique largest possible norm that applies for all $n$. An upper bound of $2$ follows from  $$\begin{align*}\|\overset{\sim}\Phi(A)\|&=\|\Phi(\mathrm{Re}(A))+i\Phi(\mathrm{Im}(A))\|\\ &\leq \|\Phi(\mathrm{Re}(A))\|+\|\Phi(\mathrm{Im}(A))\|\\ &\leq \|\mathrm{Re}(A)\|+\|\mathrm{Im}(A)\|\\ &\leq 2\|A\|.\end{align*}$$ A lower bound of $\sqrt2$ follows from an example where $n=2$ and $\|\overset{\sim}\Phi\|=\sqrt2$ given at the above linked question. Useful partial answers could involve just answering the question for a few small $n$ even if the general answer isn't revealed, or just finding an example with $\|\overset{\sim}\Phi\| >\sqrt2$ if one exists.","This question is an extension of Norms of linear maps . Let $\Phi:H_n(\mathbb C)\to H_n(\mathbb C)$ be a real linear map of norm $1$ on Hermitian $n$-by-$n$ matrices ($n>1$) with the operator norm induced by the Euclidean norm on $\mathbb C^n$.  Let $\overset{\sim}\Phi:M_n(\mathbb C)\to M_n(\mathbb C)$ be the $\mathbb C$-linear extension of $\Phi$ defined by $$\overset{\sim}\Phi(A)=\Phi\left({A+A^* \over 2}\right) + i\Phi\left({A-A^* \over 2i}\right).$$ What is the largest possible norm of $\overset{\sim}\Phi$? Remarks: The answer may depend on $n$.  Perhaps the bounds continue to increase with $n$, so an ideal solution may find the answer as a function of $n$, if there is not a unique largest possible norm that applies for all $n$. An upper bound of $2$ follows from  $$\begin{align*}\|\overset{\sim}\Phi(A)\|&=\|\Phi(\mathrm{Re}(A))+i\Phi(\mathrm{Im}(A))\|\\ &\leq \|\Phi(\mathrm{Re}(A))\|+\|\Phi(\mathrm{Im}(A))\|\\ &\leq \|\mathrm{Re}(A)\|+\|\mathrm{Im}(A)\|\\ &\leq 2\|A\|.\end{align*}$$ A lower bound of $\sqrt2$ follows from an example where $n=2$ and $\|\overset{\sim}\Phi\|=\sqrt2$ given at the above linked question. Useful partial answers could involve just answering the question for a few small $n$ even if the general answer isn't revealed, or just finding an example with $\|\overset{\sim}\Phi\| >\sqrt2$ if one exists.",,"['linear-algebra', 'operator-theory', 'normed-spaces', 'operator-algebras']"
60,Creating a steady state vector,Creating a steady state vector,,"I'm confused on where the intuition came from to divide $w$ by the sum of its entries to find $q$. I don't really see the relation from the sum of its entries with ""every solution being a multiple of the solution $w$"".","I'm confused on where the intuition came from to divide $w$ by the sum of its entries to find $q$. I don't really see the relation from the sum of its entries with ""every solution being a multiple of the solution $w$"".",,"['linear-algebra', 'probability', 'vector-spaces', 'steady-state']"
61,"Vector space of finite dimension, and a subalgebra of $L(V)$ whose all its elements are nilpotent.","Vector space of finite dimension, and a subalgebra of  whose all its elements are nilpotent.",L(V),"Let $V$ be a vector space of finite dimension, and let $A$ be a subalgebra of $L(V)$ where all its elements are nilpotent. Show that $\bigcap_{f\in A} \ker(f)$ is not reduced to $\{0\}$. $L(V)$ means linear maps in $V$.","Let $V$ be a vector space of finite dimension, and let $A$ be a subalgebra of $L(V)$ where all its elements are nilpotent. Show that $\bigcap_{f\in A} \ker(f)$ is not reduced to $\{0\}$. $L(V)$ means linear maps in $V$.",,"['linear-algebra', 'abstract-algebra', 'vector-spaces']"
62,The largest number of vectors which are orthogonal,The largest number of vectors which are orthogonal,,"$$v_1=(x_1,x_2,x_3\cdot\cdot\cdot,x_n)$$is such a vector. By changing its signs and positions of each component $x_i$, we can get different vectors. When n is odd, it's impossible for any of orthogonal to each other. When n is even, what is the maximum number of vectors that are orthogonal to each other. For example, vectors $(x_1, x_2, x_3, x_4)$, $(-x_2,x_1,x_4,-x_3)$ and $(x_3,x_4,-x_1,-x_2)$ are mutually perpendicular when $n$=$4$. What about $n=4,6,8,10,...$ and how many?","$$v_1=(x_1,x_2,x_3\cdot\cdot\cdot,x_n)$$is such a vector. By changing its signs and positions of each component $x_i$, we can get different vectors. When n is odd, it's impossible for any of orthogonal to each other. When n is even, what is the maximum number of vectors that are orthogonal to each other. For example, vectors $(x_1, x_2, x_3, x_4)$, $(-x_2,x_1,x_4,-x_3)$ and $(x_3,x_4,-x_1,-x_2)$ are mutually perpendicular when $n$=$4$. What about $n=4,6,8,10,...$ and how many?",,"['linear-algebra', 'combinatorics', 'orthogonality']"
63,Probability to obtain no solutions for a linear system,Probability to obtain no solutions for a linear system,,Suppose $Ax=b$ is a linear system and A is a $n \times n$ matrix and vector $b \neq 0$. Suppose all numbers $a_i$ in $A$ and $b_i$ in $b$ belong to  $\mathbb{Z}$ and suppose they are in a fixed range such that $|a_i|<c$ and $|b_i|<c$ with c a certain natural number. Question: what is the probability that $Ax=b$ has no solutions (in $\mathbb{Q}$ obv) ? NOTE: For my $real$ purpose it suffices an answer when $A$ is a $3\times3$ matrix and c=10 FWIW. Thanks.,Suppose $Ax=b$ is a linear system and A is a $n \times n$ matrix and vector $b \neq 0$. Suppose all numbers $a_i$ in $A$ and $b_i$ in $b$ belong to  $\mathbb{Z}$ and suppose they are in a fixed range such that $|a_i|<c$ and $|b_i|<c$ with c a certain natural number. Question: what is the probability that $Ax=b$ has no solutions (in $\mathbb{Q}$ obv) ? NOTE: For my $real$ purpose it suffices an answer when $A$ is a $3\times3$ matrix and c=10 FWIW. Thanks.,,"['linear-algebra', 'probability', 'systems-of-equations']"
64,Every subspace of the dual of a finite-dimensional vector space is an annihilator,Every subspace of the dual of a finite-dimensional vector space is an annihilator,,"Exercise 26 page 115 of Linear Algebra Done Right by Sheldon Axler is the following: Suppose $V$ is finite-dimensional and $\Gamma$ is a subspace of $V'$. Show that $$\Gamma=\{v\in V:\varphi(v)=0\text{ for every }\varphi\in\Gamma\}^0$$ where $V'$ is the dual space of $V$ and, for any $S\subset V$, $S^0$ is the annihilator of $S$. Attempt: Let $S=\{v\in V:\varphi(v)=0\text{ for every }\varphi\in\Gamma\}$. Clearly, $\Gamma\subset S^0$.I tried to show that $S^0\subset\Gamma$ using some bases of $V$ and $V'$, but I failed. I also tried to show that $\dim{\Gamma}=\dim{V}-\dim{S}=\dim{S^°}$. If $s_1,\dots,s_n$ is a basis of $S$ and $s_1',\dots,s_n'$ its dual, and if $\psi_1,\dots,\psi_p$ is a basis of $\Gamma$, it's easy to see that $s_1',\dots,s_n',\psi_1,\dots,\psi_p$ is a linearly independent list of $V'$. Also, if you extend $s_1,\dots,s_n$ to a basis $s_1,\dots,s_n,v_1,\dots,v_m$ of $V$, then its dual $s_1',\dots,s_n',v_1',\dots,v_m'$ is a basis of $V'$; in fact $S^0=\text{span}\{v_1',\dots,v_m'\}$. Two remarkable facts:$$\forall i\in[1,m],\,\exists j\in[1,p],\,\psi_j(v_i)\neq0$$ because $v_j\notin S$, and $$\forall i\in[1,p],\,\exists j\in[1,m],\,\psi_i(v_j)\neq 0$$ because $\psi_i\neq 0$; in other words the matrix of the inclusion map from $\Gamma$ to $V'$ with respect to the basis of $\Gamma$ and the dual base of the chosen basis of $V$ has no $0$ row nor $0$ column. But this doesn't seem to provide a way to prove that any linear comination of the $(v_i')_{1\le i\le m}$ is a linear combination of the elements of the basis of $\Gamma$. I believe that $v_1,\dots, v_m$ should be chosen more carefuly but I fail to. Could you please help me? Thank you in advance!","Exercise 26 page 115 of Linear Algebra Done Right by Sheldon Axler is the following: Suppose $V$ is finite-dimensional and $\Gamma$ is a subspace of $V'$. Show that $$\Gamma=\{v\in V:\varphi(v)=0\text{ for every }\varphi\in\Gamma\}^0$$ where $V'$ is the dual space of $V$ and, for any $S\subset V$, $S^0$ is the annihilator of $S$. Attempt: Let $S=\{v\in V:\varphi(v)=0\text{ for every }\varphi\in\Gamma\}$. Clearly, $\Gamma\subset S^0$.I tried to show that $S^0\subset\Gamma$ using some bases of $V$ and $V'$, but I failed. I also tried to show that $\dim{\Gamma}=\dim{V}-\dim{S}=\dim{S^°}$. If $s_1,\dots,s_n$ is a basis of $S$ and $s_1',\dots,s_n'$ its dual, and if $\psi_1,\dots,\psi_p$ is a basis of $\Gamma$, it's easy to see that $s_1',\dots,s_n',\psi_1,\dots,\psi_p$ is a linearly independent list of $V'$. Also, if you extend $s_1,\dots,s_n$ to a basis $s_1,\dots,s_n,v_1,\dots,v_m$ of $V$, then its dual $s_1',\dots,s_n',v_1',\dots,v_m'$ is a basis of $V'$; in fact $S^0=\text{span}\{v_1',\dots,v_m'\}$. Two remarkable facts:$$\forall i\in[1,m],\,\exists j\in[1,p],\,\psi_j(v_i)\neq0$$ because $v_j\notin S$, and $$\forall i\in[1,p],\,\exists j\in[1,m],\,\psi_i(v_j)\neq 0$$ because $\psi_i\neq 0$; in other words the matrix of the inclusion map from $\Gamma$ to $V'$ with respect to the basis of $\Gamma$ and the dual base of the chosen basis of $V$ has no $0$ row nor $0$ column. But this doesn't seem to provide a way to prove that any linear comination of the $(v_i')_{1\le i\le m}$ is a linear combination of the elements of the basis of $\Gamma$. I believe that $v_1,\dots, v_m$ should be chosen more carefuly but I fail to. Could you please help me? Thank you in advance!",,"['linear-algebra', 'vector-spaces', 'linear-transformations', 'duality-theorems']"
65,Conflicting natural isomorphisms: How can separate identifications interfere with each other?,Conflicting natural isomorphisms: How can separate identifications interfere with each other?,,"I have been self-studying Walter Noll's Finite-Dimensional Spaces .  A tool used often that seems extremely powerful is the identification of various objects through natural isomorphisms.  I have been struggling with the full concept of naturality and trying to avoid getting too involved in category theory, if possible. One particular instance where I could not quite see an issue that was pointed out was in this Noll excerpt: 25 Tensor Products For any linear space $\mathcal V$ there is a natural isomorphism from $\operatorname{Lin}(\Bbb R,\mathcal V)$ onto $\mathcal V$, given by $\mathbf h\mapsto\mathbf h(1)$. The inverse isomorphism associates with $\mathbf v\in\mathcal V$ the mapping $\xi\mapsto\xi\mathbf v$ in $\operatorname{Lin}(\Bbb R,\mathcal V)$. We denote this mapping by $\bf v\otimes$ (read ""vee tensor"") so that   $$\mathbf v\otimes\xi:=\xi\mathbf v\quad\text{for all}\quad\xi\in\Bbb R.\tag{25.1}$$   In particular, there is a natural isomorphism from $\Bbb R$ onto $\Bbb R^*:=\operatorname{Lin}(\Bbb R, \Bbb R)$. It associates with every number $\eta\in\Bbb R$ the operation of multiplication with that number, so that $(25.1)$ reduces to $\eta\otimes\xi=\eta\xi$. We use this isomorphism to identify $\Bbb R^*$ with $\Bbb R$, i.e. we write $\eta=\eta\otimes$. However, when $\mathcal V\ne\Bbb R$, we do not identify $\operatorname{Lin}(\Bbb R,\mathcal V)$ with $\mathcal V$ because such an identification would conflict with the identification $\mathcal V\cong\mathcal V^{**}$ and lead to confusion. If $\boldsymbol\lambda\in\mathcal V^*=\operatorname{Lin}(\mathcal V,\Bbb R)$, we can consider $\boldsymbol\lambda^\top\in\operatorname{Lin}(\Bbb R^*,\mathcal V^*)\cong\operatorname{Lin}(\Bbb R,\mathcal V^*)$. Using the identification $\Bbb R^*\cong\Bbb R$, it follows from $(21.3)$ and $(25.1)$ that $\boldsymbol\lambda^\top\xi=\xi\boldsymbol\lambda=\boldsymbol\lambda\otimes\xi$ for all $\xi\in\Bbb R$, i.e. that $\boldsymbol\lambda^\top=\boldsymbol\lambda\otimes$. However, I feel like understanding this problem precisely could be enlightening. So my question is: can there be a natural isomorphism between two objects, another natural isomorphism between two other objects, and somehow a contradiction or confusion that arises because of an interrelationship? Sticking to the Noll example would be most helpful.  For example, we can make the identification $\mathcal V\cong\operatorname{Lin}(\Bbb R,\mathcal V)$ as defined by Noll's excerpt. We can also make the usual double dual identification $\mathcal V\cong\mathcal V^{**}$. Can someone point out a case where choosing to make both identifications leads to a contradiction or confusion? Ideally, an answer that I am looking for is limited to examples involving real finite-dimensional linear spaces and relies minimally on category theoretic language. On a side note, a suggestion of a reference to a linear algebra book that heavily motivates and makes use of natural isomorphisms would be much appreciated.","I have been self-studying Walter Noll's Finite-Dimensional Spaces .  A tool used often that seems extremely powerful is the identification of various objects through natural isomorphisms.  I have been struggling with the full concept of naturality and trying to avoid getting too involved in category theory, if possible. One particular instance where I could not quite see an issue that was pointed out was in this Noll excerpt: 25 Tensor Products For any linear space $\mathcal V$ there is a natural isomorphism from $\operatorname{Lin}(\Bbb R,\mathcal V)$ onto $\mathcal V$, given by $\mathbf h\mapsto\mathbf h(1)$. The inverse isomorphism associates with $\mathbf v\in\mathcal V$ the mapping $\xi\mapsto\xi\mathbf v$ in $\operatorname{Lin}(\Bbb R,\mathcal V)$. We denote this mapping by $\bf v\otimes$ (read ""vee tensor"") so that   $$\mathbf v\otimes\xi:=\xi\mathbf v\quad\text{for all}\quad\xi\in\Bbb R.\tag{25.1}$$   In particular, there is a natural isomorphism from $\Bbb R$ onto $\Bbb R^*:=\operatorname{Lin}(\Bbb R, \Bbb R)$. It associates with every number $\eta\in\Bbb R$ the operation of multiplication with that number, so that $(25.1)$ reduces to $\eta\otimes\xi=\eta\xi$. We use this isomorphism to identify $\Bbb R^*$ with $\Bbb R$, i.e. we write $\eta=\eta\otimes$. However, when $\mathcal V\ne\Bbb R$, we do not identify $\operatorname{Lin}(\Bbb R,\mathcal V)$ with $\mathcal V$ because such an identification would conflict with the identification $\mathcal V\cong\mathcal V^{**}$ and lead to confusion. If $\boldsymbol\lambda\in\mathcal V^*=\operatorname{Lin}(\mathcal V,\Bbb R)$, we can consider $\boldsymbol\lambda^\top\in\operatorname{Lin}(\Bbb R^*,\mathcal V^*)\cong\operatorname{Lin}(\Bbb R,\mathcal V^*)$. Using the identification $\Bbb R^*\cong\Bbb R$, it follows from $(21.3)$ and $(25.1)$ that $\boldsymbol\lambda^\top\xi=\xi\boldsymbol\lambda=\boldsymbol\lambda\otimes\xi$ for all $\xi\in\Bbb R$, i.e. that $\boldsymbol\lambda^\top=\boldsymbol\lambda\otimes$. However, I feel like understanding this problem precisely could be enlightening. So my question is: can there be a natural isomorphism between two objects, another natural isomorphism between two other objects, and somehow a contradiction or confusion that arises because of an interrelationship? Sticking to the Noll example would be most helpful.  For example, we can make the identification $\mathcal V\cong\operatorname{Lin}(\Bbb R,\mathcal V)$ as defined by Noll's excerpt. We can also make the usual double dual identification $\mathcal V\cong\mathcal V^{**}$. Can someone point out a case where choosing to make both identifications leads to a contradiction or confusion? Ideally, an answer that I am looking for is limited to examples involving real finite-dimensional linear spaces and relies minimally on category theoretic language. On a side note, a suggestion of a reference to a linear algebra book that heavily motivates and makes use of natural isomorphisms would be much appreciated.",,"['linear-algebra', 'vector-spaces', 'category-theory', 'duality-theorems', 'vector-space-isomorphism']"
66,Alternative Proof to irrationality of $\sqrt{2}$ using linear algebra,Alternative Proof to irrationality of  using linear algebra,\sqrt{2},"I am taking my first Proof course, and have been researching alternative proofs to the irrationality of $\sqrt{2}$. One that particularly interested me could be found on this site as number $10$ , by D. Kalman. To avoid butchering the format I'm not going to attempt to rewrite it here, but I would love to see some other proofs to this popular theorem using linear algebra in some way, and I couldn't find any others online. If you happen to know another please share your knowledge, and thanks in advance!!","I am taking my first Proof course, and have been researching alternative proofs to the irrationality of $\sqrt{2}$. One that particularly interested me could be found on this site as number $10$ , by D. Kalman. To avoid butchering the format I'm not going to attempt to rewrite it here, but I would love to see some other proofs to this popular theorem using linear algebra in some way, and I couldn't find any others online. If you happen to know another please share your knowledge, and thanks in advance!!",,"['linear-algebra', 'proof-verification', 'alternative-proof', 'rationality-testing']"
67,Arranging Points Uniformly on the Unit Sphere,Arranging Points Uniformly on the Unit Sphere,,"What is $\min\limits_{{v_i\in R^m,\ \|v_i\|=1,}\atop{i\in\{1,2,\cdots,n\}}}\max\limits_{i,j}v_i^Tv_j$ where $\|\cdot\|$ denotes the Euclidean length? Basically, I would like to distribute a given finite number of unit points ""uniformly"" on a unit sphere. A computationally easier formulation is  $$\max_{{v_i\in R^m,\ \|v_i\|=1,}\atop{i\in\{1,2,\cdots,n\}}}\sum_{i,j}\|v_i-v_j\|^2.$$ Are these the two formulations equivalent? My guess is that they are not. Can anyone give a proof either way?","What is $\min\limits_{{v_i\in R^m,\ \|v_i\|=1,}\atop{i\in\{1,2,\cdots,n\}}}\max\limits_{i,j}v_i^Tv_j$ where $\|\cdot\|$ denotes the Euclidean length? Basically, I would like to distribute a given finite number of unit points ""uniformly"" on a unit sphere. A computationally easier formulation is  $$\max_{{v_i\in R^m,\ \|v_i\|=1,}\atop{i\in\{1,2,\cdots,n\}}}\sum_{i,j}\|v_i-v_j\|^2.$$ Are these the two formulations equivalent? My guess is that they are not. Can anyone give a proof either way?",,"['linear-algebra', 'geometry', 'optimization', 'quadratic-programming']"
68,Cokernel of tensor products of maps,Cokernel of tensor products of maps,,"Let $R$ be a commutative ring and $f\colon A\rightarrow B$, $f'\colon A'\rightarrow B'$ two maps of $R$-modules. Is there a way to express the cokernel $\operatorname{coker} (f\otimes f')$ in terms of the cokernels of $f$ and $f'$? By tensoring $f$ with the identity on $R$, we get $\operatorname{coker}(f)\cong \operatorname{coker}(f\otimes \operatorname{id} )$, so in particular we do not have $\operatorname{coker}(f\otimes \operatorname{id}  )=\operatorname{coker}(f)\otimes \operatorname{coker}(\operatorname{id} )$ as $\operatorname{coker}(id)=0$.","Let $R$ be a commutative ring and $f\colon A\rightarrow B$, $f'\colon A'\rightarrow B'$ two maps of $R$-modules. Is there a way to express the cokernel $\operatorname{coker} (f\otimes f')$ in terms of the cokernels of $f$ and $f'$? By tensoring $f$ with the identity on $R$, we get $\operatorname{coker}(f)\cong \operatorname{coker}(f\otimes \operatorname{id} )$, so in particular we do not have $\operatorname{coker}(f\otimes \operatorname{id}  )=\operatorname{coker}(f)\otimes \operatorname{coker}(\operatorname{id} )$ as $\operatorname{coker}(id)=0$.",,"['linear-algebra', 'abstract-algebra', 'homological-algebra']"
69,The direct sum $\oplus$ versus the cartesian product $\times$,The direct sum  versus the cartesian product,\oplus \times,"In the case of abelian groups, I have been treating these two set operations as more or less indistinguishable. In early mathematics courses, one normally defines $A^n := A\times A\times\ldots\times A$; however in, for example, the fundamental theorem of finitely generated abelian groups, we normally write that every such group is isomorphic to one of the form $$ \mathbb{Z}^n \oplus \mathbb{Z}_{r_1} \oplus \cdots \oplus \mathbb{Z}_{r_t}  $$ where $\mathbb{Z}^n$ now means $\mathbb{Z}\oplus\mathbb{Z}\oplus\cdots\oplus\mathbb{Z}$. From an intuition perspective, and in the sense of sets, is this more or less the same as $\mathbb{Z}\times\mathbb{Z}\times\cdots\times\mathbb{Z}$? (Bear in mind I am normally using these ideas in relation to homology groups.)","In the case of abelian groups, I have been treating these two set operations as more or less indistinguishable. In early mathematics courses, one normally defines $A^n := A\times A\times\ldots\times A$; however in, for example, the fundamental theorem of finitely generated abelian groups, we normally write that every such group is isomorphic to one of the form $$ \mathbb{Z}^n \oplus \mathbb{Z}_{r_1} \oplus \cdots \oplus \mathbb{Z}_{r_t}  $$ where $\mathbb{Z}^n$ now means $\mathbb{Z}\oplus\mathbb{Z}\oplus\cdots\oplus\mathbb{Z}$. From an intuition perspective, and in the sense of sets, is this more or less the same as $\mathbb{Z}\times\mathbb{Z}\times\cdots\times\mathbb{Z}$? (Bear in mind I am normally using these ideas in relation to homology groups.)",,"['group-theory', 'abelian-groups']"
70,Name for the $\otimes$ operator,Name for the  operator,\otimes,"I'm teaching 3D vector stuff to engineers. When we write $\mathbf{u} \cdot \mathbf{v}$, we say ""u dot v"". When we write $\mathbf{u} \times \mathbf{v}$, we say ""u cross v"". When we write $\mathbf{u} \otimes \mathbf{v}$, we say what?? Here $\mathbf{u} \otimes \mathbf{v}$ is the so-called "" outer product "", defined by $\mathbf{u} \otimes \mathbf{v} = \mathbf{u}\mathbf{v}^T$. It's clumsy and verbose to say ""the outer product of u and v"" and it feels odd to say ""u outer v"". Any ideas?","I'm teaching 3D vector stuff to engineers. When we write $\mathbf{u} \cdot \mathbf{v}$, we say ""u dot v"". When we write $\mathbf{u} \times \mathbf{v}$, we say ""u cross v"". When we write $\mathbf{u} \otimes \mathbf{v}$, we say what?? Here $\mathbf{u} \otimes \mathbf{v}$ is the so-called "" outer product "", defined by $\mathbf{u} \otimes \mathbf{v} = \mathbf{u}\mathbf{v}^T$. It's clumsy and verbose to say ""the outer product of u and v"" and it feels odd to say ""u outer v"". Any ideas?",,"['linear-algebra', 'terminology', 'vectors']"
71,Have functions with this scaling behavior been studied? Do they have a special name?,Have functions with this scaling behavior been studied? Do they have a special name?,,"In attempting to understand the mathematical content of certain dimensional analysis procedures one encounters in mathematics, physics, and engineering, I've noticed it's useful to understand functions having certain generalized homogeneity/scaling properties: Given a $k$-by-$n$ matrix with rational entries $A = (a_{ij})$ and $\lambda = (\lambda_1, \lambda_2, \dots, \lambda_k)\in \mathbb R^n_+$, define a transformation $S_{A,\lambda}:\mathbb R^n\to\mathbb R$ as follows: $$   S_{A,\lambda}\begin{pmatrix}x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix} = \begin{pmatrix}   \lambda_1^{a_{11}}\lambda_2^{a_{21}}\cdots\lambda_k^{a_{k1}}x_1 \\   \lambda_1^{a_{12}}\lambda_2^{a_{22}}\cdots\lambda_k^{a_{k2}}x_2 \\   \vdots \\   \lambda_1^{a_{1n}}\lambda_2^{a_{2n}}\cdots\lambda_k^{a_{kn}}x_n \end{pmatrix}. $$ Now suppose that there exists such an $A$ and rational numbers $\Delta_1, \Delta_2, \dots, \Delta_k$, such that $f:\mathbb R^n\to\mathbb R$ satisfies $$   f(S_{A,\lambda}(x)) = \lambda_1^{\Delta_1}\lambda_2^{\Delta_2}\cdots\lambda_k^{\Delta_k}f(x) $$ for all $\lambda\in\mathbb R^n_+$. In the special case: $A$ is the identity matrix, all $\lambda_i$ are equal, and the $\Delta_i$ sum to $m$, I believe $f$ would be called a homogeneous function of degree $m$. Is there a special name for such functions $f$ when there are no such restrictions on $A$, $\lambda_i$, and $\Delta_i$?  Have they been studied in some generality?","In attempting to understand the mathematical content of certain dimensional analysis procedures one encounters in mathematics, physics, and engineering, I've noticed it's useful to understand functions having certain generalized homogeneity/scaling properties: Given a $k$-by-$n$ matrix with rational entries $A = (a_{ij})$ and $\lambda = (\lambda_1, \lambda_2, \dots, \lambda_k)\in \mathbb R^n_+$, define a transformation $S_{A,\lambda}:\mathbb R^n\to\mathbb R$ as follows: $$   S_{A,\lambda}\begin{pmatrix}x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix} = \begin{pmatrix}   \lambda_1^{a_{11}}\lambda_2^{a_{21}}\cdots\lambda_k^{a_{k1}}x_1 \\   \lambda_1^{a_{12}}\lambda_2^{a_{22}}\cdots\lambda_k^{a_{k2}}x_2 \\   \vdots \\   \lambda_1^{a_{1n}}\lambda_2^{a_{2n}}\cdots\lambda_k^{a_{kn}}x_n \end{pmatrix}. $$ Now suppose that there exists such an $A$ and rational numbers $\Delta_1, \Delta_2, \dots, \Delta_k$, such that $f:\mathbb R^n\to\mathbb R$ satisfies $$   f(S_{A,\lambda}(x)) = \lambda_1^{\Delta_1}\lambda_2^{\Delta_2}\cdots\lambda_k^{\Delta_k}f(x) $$ for all $\lambda\in\mathbb R^n_+$. In the special case: $A$ is the identity matrix, all $\lambda_i$ are equal, and the $\Delta_i$ sum to $m$, I believe $f$ would be called a homogeneous function of degree $m$. Is there a special name for such functions $f$ when there are no such restrictions on $A$, $\lambda_i$, and $\Delta_i$?  Have they been studied in some generality?",,"['linear-algebra', 'functions']"
72,Integral of the square of the exponential of a matrix [closed],Integral of the square of the exponential of a matrix [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 7 years ago . Improve this question Let $A$ be a $n\times n$ square matrix such that the real part of all eigenvalues are negative. It is well known that:  $$ \int_0^\infty \exp(At)dt = -A^{-1}.$$ In other word, the element $(i,j)$ of the above matrix is $\int_0^\infty \exp(At)_{ij}dt = -A^{-1}_{ij}$. Is it possible to simplify a similar expression where each element is squared:  $$ \int_0^\infty (\exp(At)_{ij})^2 dt = ??$$ For one-dimensional matrices (a.k.a. real numbers), this is easy and we obtain $(-2A)^{-1}$ because $\exp(-at)^2=\exp(-2at)$. This does not work for $n\ge2$ and in this case the answer is not $(2A)^{-1}$. I am wondering if it is possible to simplify the above expression.","Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 7 years ago . Improve this question Let $A$ be a $n\times n$ square matrix such that the real part of all eigenvalues are negative. It is well known that:  $$ \int_0^\infty \exp(At)dt = -A^{-1}.$$ In other word, the element $(i,j)$ of the above matrix is $\int_0^\infty \exp(At)_{ij}dt = -A^{-1}_{ij}$. Is it possible to simplify a similar expression where each element is squared:  $$ \int_0^\infty (\exp(At)_{ij})^2 dt = ??$$ For one-dimensional matrices (a.k.a. real numbers), this is easy and we obtain $(-2A)^{-1}$ because $\exp(-at)^2=\exp(-2at)$. This does not work for $n\ge2$ and in this case the answer is not $(2A)^{-1}$. I am wondering if it is possible to simplify the above expression.",,"['linear-algebra', 'integration', 'multivariable-calculus']"
73,Brezis' Functional Analysis Exercise 2.5,Brezis' Functional Analysis Exercise 2.5,,"The exercise is from Brezis's functional analysis page 50: Let $E$ be a Banach space and let $\varepsilon_n$ be a sequence of positive   numbers such that $\lim \varepsilon_n = 0$. Further, let $(f_n)$ be a sequence   in $E^{\star}$ statisfying the property,   $$ \left\{ \begin{array}{l}      \exists r > 0, \forall x \in E \, \text{with}\, \| x \| < r, \exists C      (x) \in \mathbb{R} \quad \text{such that}\\      \\      \langle f_n, x \rangle \leqslant \varepsilon_n \| f_n \| + C (x) \quad      \forall n.    \end{array} \right. $$   Prove that $(f_n)$ is bounded. ( Hint : Introduce $g_n = f_n / (1 + \varepsilon_n \| f_n \|)$.) Here is my solution: Fix $x \neq 0$, we have   $$ g_n (x) = \frac{f_n (x)}{1 + \varepsilon_n \| f_n \|} = \frac{2 \| x \|    }{r}  \frac{f_n \left( \frac{r}{2 \| x \|} x \right)}{1 + \varepsilon_n \|    f_n \|} \leqslant \frac{2 \| x \| }{r}  \frac{C \left( \frac{r}{2 \| x \|}    x \right) + \varepsilon_n \| f_n \|}{1 + \varepsilon_n \| f_n \|} $$ $$ = \frac{2    \| x \| }{r} \left( \frac{C (x) - 1}{1 + \varepsilon_n \| f_n \|} + 1    \right) \leqslant \frac{2 \| x \|}{r}  (| C (x) - 1 | + 1) . $$   for all $n \in \mathbb{N}$. Thus $\sup_n g_n (x)$ is finite for all $x \in E$. By uniform bounded principle, we derive that   $$ g_n (x) \leqslant M \| x \| $$   for all $x \in E$, $n \in \mathbb{N}$. So we get   $$ f_n (x) \leqslant M (1 + \varepsilon_n \| f_n \|)  \| x \| . $$   If we restrict $\| x \| \leqslant 1$, and taking the supremum with respect to   $x$, we have   $$ \| f_n \| \leqslant M + \varepsilon_n M \| f_n \| . $$   Since $0 < \varepsilon_n \rightarrow 0$, there exists $n_0 \in \mathbb{N}$   such that $\varepsilon_n M < 1 / 2$ when $n \geqslant n_0 + 1$. In other   words,   $$ \| f_n \| \leqslant 2 M $$   when $n \geqslant n_0 + 1$. Taking $N = \max \{ \| f_1 \|, \cdots, \| f_{n_0} \|, 2 M \}$, we have $\| f_n \| \leqslant N$ for all $n \in \mathbb{N}$.   Therefore, $(f_n)$ is a bounded sequence in $E^{\star}$. Question : I am not quite confident with my proof, because it seems a little bit unclean for me. I would be glad for anyone to check my proof or to give a neat proof. Thanks!","The exercise is from Brezis's functional analysis page 50: Let $E$ be a Banach space and let $\varepsilon_n$ be a sequence of positive   numbers such that $\lim \varepsilon_n = 0$. Further, let $(f_n)$ be a sequence   in $E^{\star}$ statisfying the property,   $$ \left\{ \begin{array}{l}      \exists r > 0, \forall x \in E \, \text{with}\, \| x \| < r, \exists C      (x) \in \mathbb{R} \quad \text{such that}\\      \\      \langle f_n, x \rangle \leqslant \varepsilon_n \| f_n \| + C (x) \quad      \forall n.    \end{array} \right. $$   Prove that $(f_n)$ is bounded. ( Hint : Introduce $g_n = f_n / (1 + \varepsilon_n \| f_n \|)$.) Here is my solution: Fix $x \neq 0$, we have   $$ g_n (x) = \frac{f_n (x)}{1 + \varepsilon_n \| f_n \|} = \frac{2 \| x \|    }{r}  \frac{f_n \left( \frac{r}{2 \| x \|} x \right)}{1 + \varepsilon_n \|    f_n \|} \leqslant \frac{2 \| x \| }{r}  \frac{C \left( \frac{r}{2 \| x \|}    x \right) + \varepsilon_n \| f_n \|}{1 + \varepsilon_n \| f_n \|} $$ $$ = \frac{2    \| x \| }{r} \left( \frac{C (x) - 1}{1 + \varepsilon_n \| f_n \|} + 1    \right) \leqslant \frac{2 \| x \|}{r}  (| C (x) - 1 | + 1) . $$   for all $n \in \mathbb{N}$. Thus $\sup_n g_n (x)$ is finite for all $x \in E$. By uniform bounded principle, we derive that   $$ g_n (x) \leqslant M \| x \| $$   for all $x \in E$, $n \in \mathbb{N}$. So we get   $$ f_n (x) \leqslant M (1 + \varepsilon_n \| f_n \|)  \| x \| . $$   If we restrict $\| x \| \leqslant 1$, and taking the supremum with respect to   $x$, we have   $$ \| f_n \| \leqslant M + \varepsilon_n M \| f_n \| . $$   Since $0 < \varepsilon_n \rightarrow 0$, there exists $n_0 \in \mathbb{N}$   such that $\varepsilon_n M < 1 / 2$ when $n \geqslant n_0 + 1$. In other   words,   $$ \| f_n \| \leqslant 2 M $$   when $n \geqslant n_0 + 1$. Taking $N = \max \{ \| f_1 \|, \cdots, \| f_{n_0} \|, 2 M \}$, we have $\| f_n \| \leqslant N$ for all $n \in \mathbb{N}$.   Therefore, $(f_n)$ is a bounded sequence in $E^{\star}$. Question : I am not quite confident with my proof, because it seems a little bit unclean for me. I would be glad for anyone to check my proof or to give a neat proof. Thanks!",,"['linear-algebra', 'functional-analysis', 'analysis']"
74,Orthogonal verus Perpendicular Terminology,Orthogonal verus Perpendicular Terminology,,"This is a pedantic question about the use of the terms 'orthogonal' and 'perpendicular.' I think when people first hear the term 'orthogonal' they learn it to mean perpendicular, but in the linear algebraic sense, I believe it simply means that the inner product between two vectors is zero. This has no geometrical extension: just take a function space, for example. But even if we are working in, say, $\mathbb{R}^2$, I could define an inner product that is not the standard one, such that two vectors that are $\it{orthogonal}$ are not $\it{perpendicular}$. So in this sense, perpendicularity is a purely geometrical adjective, referring to $\bf physical$ $90^{\circ}$ angles. Is this interpretation correct?","This is a pedantic question about the use of the terms 'orthogonal' and 'perpendicular.' I think when people first hear the term 'orthogonal' they learn it to mean perpendicular, but in the linear algebraic sense, I believe it simply means that the inner product between two vectors is zero. This has no geometrical extension: just take a function space, for example. But even if we are working in, say, $\mathbb{R}^2$, I could define an inner product that is not the standard one, such that two vectors that are $\it{orthogonal}$ are not $\it{perpendicular}$. So in this sense, perpendicularity is a purely geometrical adjective, referring to $\bf physical$ $90^{\circ}$ angles. Is this interpretation correct?",,"['linear-algebra', 'terminology', 'education']"
75,If $T\colon \mathbb R^n \to \mathbb R^n $ linear and $T^2 = kT$   [closed],If  linear and    [closed],T\colon \mathbb R^n \to \mathbb R^n  T^2 = kT,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question It is given that $T$ is a linear transformation from $\mathbb R^n$ to $\mathbb R^n$ such that $T^2 = k T $  for some $k\in \mathbb R$. Then, one or more of the options are true $\|T(x)\| = |k| \|x\|$ for all $x\in \mathbb R^n$. If $\|T(x)\| = \|x\|$ , for some non- zero vector $x\in \mathbb R^n$ then $ k =\pm 1 $. $\|T(x)\|  >  \|x\|$ for some non- zero vector $x\in \mathbb R^n$, then T is singular. $ T = k I $ where I is an identity transformation. Please suggest how to proceed.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question It is given that $T$ is a linear transformation from $\mathbb R^n$ to $\mathbb R^n$ such that $T^2 = k T $  for some $k\in \mathbb R$. Then, one or more of the options are true $\|T(x)\| = |k| \|x\|$ for all $x\in \mathbb R^n$. If $\|T(x)\| = \|x\|$ , for some non- zero vector $x\in \mathbb R^n$ then $ k =\pm 1 $. $\|T(x)\|  >  \|x\|$ for some non- zero vector $x\in \mathbb R^n$, then T is singular. $ T = k I $ where I is an identity transformation. Please suggest how to proceed.",,"['linear-algebra', 'linear-transformations']"
76,Construct a matrix $M$ from $A$ and $B$ such that $\det(M)=\det(A)-\det(B)$,Construct a matrix  from  and  such that,M A B \det(M)=\det(A)-\det(B),"Given two $n \times n$ symmetric matrices $A$ and $B$, is there a generic way to construct a larger block matrix $M$ such that $\det(M) = \det(A) - \det(B)$? A simple block expression is desired, in the sense that the block components of $M$ are constant matrices or obtained by solving matrix equations involving $A$ and $B$.  Constructions of $M$ involving short algebraic expressions for its components in terms of the components of $A$ and $B$ would also be interesting, but not something that expands to an exponential number of terms in the components of $A$ and $B$ like just sticking $\det(A)$ in as a term of $M$. If this is not possible in the general case, what restrictions can be placed on $A$ and $B$ to make this possible? The only case I know of is when the difference of $B$ and $A$ can be written as a product of a column vector $C$ and its transpose: $B-A = CC^T$. This allows us to construct a matrix $M$ such that: $$ M = \begin{bmatrix} A & C \\ C^T & 0 \end{bmatrix} $$ $$ \det(M) = \det(A) - \det(A + CC^T) = \det(A) - \det(B) $$ I'm curious if there is some way to construct an appropriate block matrix to make this possible for arbitrary symmetric matrices $A$ and $B$. The first comment to this question Find a matrix with determinant equals to $\det{(A)}\det{(D)}-\det{(B)}\det{(C)}$ suggests the answer is trivial by choosing $$M = \begin{bmatrix} A & B \\ I & I \end{bmatrix}$$ but that doesn't appear to work when I tried some numerical examples.","Given two $n \times n$ symmetric matrices $A$ and $B$, is there a generic way to construct a larger block matrix $M$ such that $\det(M) = \det(A) - \det(B)$? A simple block expression is desired, in the sense that the block components of $M$ are constant matrices or obtained by solving matrix equations involving $A$ and $B$.  Constructions of $M$ involving short algebraic expressions for its components in terms of the components of $A$ and $B$ would also be interesting, but not something that expands to an exponential number of terms in the components of $A$ and $B$ like just sticking $\det(A)$ in as a term of $M$. If this is not possible in the general case, what restrictions can be placed on $A$ and $B$ to make this possible? The only case I know of is when the difference of $B$ and $A$ can be written as a product of a column vector $C$ and its transpose: $B-A = CC^T$. This allows us to construct a matrix $M$ such that: $$ M = \begin{bmatrix} A & C \\ C^T & 0 \end{bmatrix} $$ $$ \det(M) = \det(A) - \det(A + CC^T) = \det(A) - \det(B) $$ I'm curious if there is some way to construct an appropriate block matrix to make this possible for arbitrary symmetric matrices $A$ and $B$. The first comment to this question Find a matrix with determinant equals to $\det{(A)}\det{(D)}-\det{(B)}\det{(C)}$ suggests the answer is trivial by choosing $$M = \begin{bmatrix} A & B \\ I & I \end{bmatrix}$$ but that doesn't appear to work when I tried some numerical examples.",,"['linear-algebra', 'matrices', 'determinant']"
77,sufficient condition for a real matrix to have all real eigenvalues,sufficient condition for a real matrix to have all real eigenvalues,,"We all know that eigenvalues of a Hermitian matrix are real, but I am looking for sufficient conditions for a general real matrix (not necessarily symmetric) to have only real eigenvalues. So far, I know that totally positive matrices have this property I want. Are there any other sufficient conditions besides total positivity?","We all know that eigenvalues of a Hermitian matrix are real, but I am looking for sufficient conditions for a general real matrix (not necessarily symmetric) to have only real eigenvalues. So far, I know that totally positive matrices have this property I want. Are there any other sufficient conditions besides total positivity?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
78,Is the set of symmetric positive-definite matrices open in the set of symmetric matrices?,Is the set of symmetric positive-definite matrices open in the set of symmetric matrices?,,"I'm not sure if the set of symmetric positive-definite matrices is open in the set of symmetric matrices.  I'm almost certain that it is not open in the larger set of n by n matrices.  So far most of the proofs I've seen don't use symmetry, so I'm not sure if they are correct.  Any hints?  I've been trying to use that the set of positive-definite matrices is path connected, but I can't come up with a solution.  Thanks in advance.","I'm not sure if the set of symmetric positive-definite matrices is open in the set of symmetric matrices.  I'm almost certain that it is not open in the larger set of n by n matrices.  So far most of the proofs I've seen don't use symmetry, so I'm not sure if they are correct.  Any hints?  I've been trying to use that the set of positive-definite matrices is path connected, but I can't come up with a solution.  Thanks in advance.",,"['linear-algebra', 'general-topology', 'matrices']"
79,"Understanding the ""physical significance"" of Tr$(A^{-1}B)<1$ for $A,B$ positive definite trace n matrices","Understanding the ""physical significance"" of Tr for  positive definite trace n matrices","(A^{-1}B)<1 A,B","Let $A$ and $B$ be positive definite, $n\times n$ matrices, such that $\frac1n\operatorname{tr}(A)=\frac1n\operatorname{tr}(B)=1$. Consider the situation where $$\operatorname{tr}(A^{-1}B)<1.$$ My question is: physically, what does this mean? If $A=UDU^T$ and $B=U\Sigma U^T$ were simultaneously diagonalizable, this would mean that  $$\frac1n\sum_{1}^n\frac{\sigma_i}{d_i}<1,$$ so that the average quotient of eigenvalues is less than 1. I believe this is equivalent to the energy of $B$ concentrating in the same places as $A$. (Am I missing the best way of interpreting this case?) What about the general case? Does this tell us anything about the relationship between the eigenstructure of $B$ and $A$? My intuition is that this would mean that $B$ correlates weakly with the weakest eigenspaces of $A$, and therefore most of the energy of $B$ concentrates on the dominant eigenspaces of $A$. Can this be made more rigorous? I would love to read a good exposition of some of the subtleties here.","Let $A$ and $B$ be positive definite, $n\times n$ matrices, such that $\frac1n\operatorname{tr}(A)=\frac1n\operatorname{tr}(B)=1$. Consider the situation where $$\operatorname{tr}(A^{-1}B)<1.$$ My question is: physically, what does this mean? If $A=UDU^T$ and $B=U\Sigma U^T$ were simultaneously diagonalizable, this would mean that  $$\frac1n\sum_{1}^n\frac{\sigma_i}{d_i}<1,$$ so that the average quotient of eigenvalues is less than 1. I believe this is equivalent to the energy of $B$ concentrating in the same places as $A$. (Am I missing the best way of interpreting this case?) What about the general case? Does this tell us anything about the relationship between the eigenstructure of $B$ and $A$? My intuition is that this would mean that $B$ correlates weakly with the weakest eigenspaces of $A$, and therefore most of the energy of $B$ concentrates on the dominant eigenspaces of $A$. Can this be made more rigorous? I would love to read a good exposition of some of the subtleties here.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
80,Whether a given algebra is the algebra of endomorphisms for a vector space.,Whether a given algebra is the algebra of endomorphisms for a vector space.,,"Let $\mathbb{F}$ be a field and let $A$ be an associative unital $\mathbb{F}$-algebra. Is there a criterion to let me know if $A$ is isomorphic to the algebra $\mbox{End}(\mathbf{V})$ of endomorphisms for some $\mathbb{F}$-vector space $\mathbf{V}$? Generalizations to $A$ being an associative unital ring and $\mathbf{V}$ an Abelian group or similar are welcome. Answers for particular cases $\mathbb{F}\in\{\mathbb{R},\mathbb{C}\}$ are also appreciated. Thank you.","Let $\mathbb{F}$ be a field and let $A$ be an associative unital $\mathbb{F}$-algebra. Is there a criterion to let me know if $A$ is isomorphic to the algebra $\mbox{End}(\mathbf{V})$ of endomorphisms for some $\mathbb{F}$-vector space $\mathbf{V}$? Generalizations to $A$ being an associative unital ring and $\mathbf{V}$ an Abelian group or similar are welcome. Answers for particular cases $\mathbb{F}\in\{\mathbb{R},\mathbb{C}\}$ are also appreciated. Thank you.",,"['linear-algebra', 'abstract-algebra']"
81,"Is there a list of all typos in Hoffman and Kunze, Linear Algebra?","Is there a list of all typos in Hoffman and Kunze, Linear Algebra?",,"Where can I find a list of typos for Linear Algebra , 2nd Edition, by Hoffman and Kunze? I searched on Google, but to no avail.","Where can I find a list of typos for Linear Algebra , 2nd Edition, by Hoffman and Kunze? I searched on Google, but to no avail.",,"['linear-algebra', 'reference-request']"
82,Hyperdeterminant of 4x4x4 hypermatrix,Hyperdeterminant of 4x4x4 hypermatrix,,"If given the hypermatrix (which I've written here in bracket notation since I'm not all too sure how to display this) { {{1,1,1,1},{1,1,-1,-1},{1,-1,-1,1},{1,-1,1,-1}}, {{1,1,1,1},{1,1,-1,-1},{1,-1,1,-1},{1,-1,-1,1}}, {{1,1,1,1},{1,-1,1,-1},{1,1,-1,-1},{1,-1,-1,1}}, {{1,-1,1,-1},{1,1,1,1},{1,1,-1,-1},{1,-1,-1,1}} } how would one go about calculating the hyperdeterminant? I know of Cayley's second hyperdeterminant formula for a 2x2x2 cube, however how would one calculate the tensor product for an even numbered dimension other than 2?","If given the hypermatrix (which I've written here in bracket notation since I'm not all too sure how to display this) { {{1,1,1,1},{1,1,-1,-1},{1,-1,-1,1},{1,-1,1,-1}}, {{1,1,1,1},{1,1,-1,-1},{1,-1,1,-1},{1,-1,-1,1}}, {{1,1,1,1},{1,-1,1,-1},{1,1,-1,-1},{1,-1,-1,1}}, {{1,-1,1,-1},{1,1,1,1},{1,1,-1,-1},{1,-1,-1,1}} } how would one go about calculating the hyperdeterminant? I know of Cayley's second hyperdeterminant formula for a 2x2x2 cube, however how would one calculate the tensor product for an even numbered dimension other than 2?",,"['linear-algebra', 'matrices', 'tensor-products']"
83,Is the Null Space of an linear operator the same with the Null Space of its associated hermitian?,Is the Null Space of an linear operator the same with the Null Space of its associated hermitian?,,"Let A be a bounded linear operator on $H$ where $H$ is a (not necessary I think, but in my case separable) Hilbert space. Then, the question: is its null space the same as the null space of the hermitian operator $A^*A$ ? The operator  $A^*$ is its ""adjoint"", defined regarding the inner product on $H$: $\langle Au,v\rangle = \langle u,A^*v\rangle$ for every pair $u,v\in H$, and the null space of $A$ is the set of vectors $u$ in $H$ verifying $Au = 0$. I did not find a written proof, may be because it should be trivial.  So, here are my arguments: if $u \neq 0$ and $Au = 0$, then  $A^*Au = 0$, and first inclusion between null spaces follows. Reverse, if   $A^*Au = 0$, then for every $v$, $\langle A^*Au,v\rangle = 0;$ or, $\langle Au,Av\rangle = 0$, that for $v=u$ becomes: $\Vert Au \Vert^{2} = 0$. It follows that $Au = 0$, proving the reverse inclusion between null subspaces of $A$ and $A^*A$. Am I right? If so, we could go further, proving in the same manner that the null space of $A$ is the same with the null space of $(A^*A)^{1/2}$. Thanks.","Let A be a bounded linear operator on $H$ where $H$ is a (not necessary I think, but in my case separable) Hilbert space. Then, the question: is its null space the same as the null space of the hermitian operator $A^*A$ ? The operator  $A^*$ is its ""adjoint"", defined regarding the inner product on $H$: $\langle Au,v\rangle = \langle u,A^*v\rangle$ for every pair $u,v\in H$, and the null space of $A$ is the set of vectors $u$ in $H$ verifying $Au = 0$. I did not find a written proof, may be because it should be trivial.  So, here are my arguments: if $u \neq 0$ and $Au = 0$, then  $A^*Au = 0$, and first inclusion between null spaces follows. Reverse, if   $A^*Au = 0$, then for every $v$, $\langle A^*Au,v\rangle = 0;$ or, $\langle Au,Av\rangle = 0$, that for $v=u$ becomes: $\Vert Au \Vert^{2} = 0$. It follows that $Au = 0$, proving the reverse inclusion between null subspaces of $A$ and $A^*A$. Am I right? If so, we could go further, proving in the same manner that the null space of $A$ is the same with the null space of $(A^*A)^{1/2}$. Thanks.",,"['linear-algebra', 'functional-analysis', 'operator-theory', 'hilbert-spaces']"
84,Transpose a matrix then flip it over the anti-diagonal. What is the name of this operation?,Transpose a matrix then flip it over the anti-diagonal. What is the name of this operation?,,What do you call a matrix operation where you transpose it and then flip it over its anti-diagonal? I find it very useful in electrical network analysis to flip the input and output of a two-port network. $$ \begin{bmatrix} Y_{11} & Y_{12} \\ Y_{21} & Y_{22} \end{bmatrix} \rightarrow \begin{bmatrix} Y_{22} & Y_{21} \\ Y_{12} & Y_{11} \end{bmatrix} $$,What do you call a matrix operation where you transpose it and then flip it over its anti-diagonal? I find it very useful in electrical network analysis to flip the input and output of a two-port network. $$ \begin{bmatrix} Y_{11} & Y_{12} \\ Y_{21} & Y_{22} \end{bmatrix} \rightarrow \begin{bmatrix} Y_{22} & Y_{21} \\ Y_{12} & Y_{11} \end{bmatrix} $$,,"['linear-algebra', 'matrices']"
85,Jordan form exercise,Jordan form exercise,,"What am I doing wrong? I've been learning how to put matrices into Jordan canonical form and it was going fine until I encountered this $4 \times 4$ matrix: $A=\begin{bmatrix}  2 & 2 & 0 & -1 \\  0 & 0 & 0 & 1 \\ 1 & 5 & 2 & -1 \\ 0 & -4 & 0 & 4 \\ \end{bmatrix} $ Which has as only eigenvalue $\lambda_1=\lambda_2=\lambda_3=\lambda_4=2$ with 2 corresponding eigenvectors, which I will for now call $v_1$ and $v_2$: $v_1 = \pmatrix{0\\0\\1\\0}, v_2=\pmatrix{-3 \\ 1 \\ 0 \\ 2} $ 2 eigenvectors means 2 Jordan blocks so I have 2 possibilities: $J= \pmatrix{2 & 1 & 0 & 0 \\  0 & 2 & 1 & 0 \\ 0 & 0 & 2 & 0 \\ 0 & 0 & 0 & 2} $ or $ J= \pmatrix{2 & 1 & 0 & 0 \\ 0 & 2 & 0 & 0 \\ 0 & 0 & 2 & 1 \\ 0 & 0 & 0 & 2} $ I consider the first possibility. This gives me the relations: $Ax_1=2x_1 \\ Ax_2=x_1+2x_2 \\ Ax_3=2x_3+x_2 \\ Ax_4=2x_4 \\ $ where $x_1$ and $x_4$ should be $v_1$ and $v_2$. From the second relation $(A-2I)x_2=x_1$ I see $\pmatrix{0 & 2 & 0 & -1 \\ 0 & -2 & 0 & 1 \\ 1 & 5 & 0 & -1 \\ 0 & -4 & 0 & 2} \pmatrix{a \\ b \\ c \\ d} =\pmatrix{0 \\ 0 \\ 1 \\ 0} $ ( $v_2= \pmatrix{ -3 \\ 1 \\ 0 \\ 2} $ will give an inconsistent system) Now I get that $x_2 = \pmatrix{-2 \\ 1 \\ 0 \\ 2} $ From the third relation $(A-2I)x_3=x_2$: $\pmatrix{0 & 2 & 0 & -1 \\ 0 & -2 & 0 & 1 \\ 1 & 5 & 0 & -1 \\ 0 & -4 & 0 & 2} \pmatrix{e \\ f \\ g \\ h} =\pmatrix{-2 \\ 1 \\ 0 \\ 2} $ But this system is inconsistent as well! No matter which vectors I try in which places, when I try to generalize eigenvectors I seem to always end up with some inconsistency. Is there something staring me in the face that I am overlooking? Or am I doing it completely wrong (even though this method worked fine for me before)? Sorry for the lengthiness and thank you in advance.","What am I doing wrong? I've been learning how to put matrices into Jordan canonical form and it was going fine until I encountered this $4 \times 4$ matrix: $A=\begin{bmatrix}  2 & 2 & 0 & -1 \\  0 & 0 & 0 & 1 \\ 1 & 5 & 2 & -1 \\ 0 & -4 & 0 & 4 \\ \end{bmatrix} $ Which has as only eigenvalue $\lambda_1=\lambda_2=\lambda_3=\lambda_4=2$ with 2 corresponding eigenvectors, which I will for now call $v_1$ and $v_2$: $v_1 = \pmatrix{0\\0\\1\\0}, v_2=\pmatrix{-3 \\ 1 \\ 0 \\ 2} $ 2 eigenvectors means 2 Jordan blocks so I have 2 possibilities: $J= \pmatrix{2 & 1 & 0 & 0 \\  0 & 2 & 1 & 0 \\ 0 & 0 & 2 & 0 \\ 0 & 0 & 0 & 2} $ or $ J= \pmatrix{2 & 1 & 0 & 0 \\ 0 & 2 & 0 & 0 \\ 0 & 0 & 2 & 1 \\ 0 & 0 & 0 & 2} $ I consider the first possibility. This gives me the relations: $Ax_1=2x_1 \\ Ax_2=x_1+2x_2 \\ Ax_3=2x_3+x_2 \\ Ax_4=2x_4 \\ $ where $x_1$ and $x_4$ should be $v_1$ and $v_2$. From the second relation $(A-2I)x_2=x_1$ I see $\pmatrix{0 & 2 & 0 & -1 \\ 0 & -2 & 0 & 1 \\ 1 & 5 & 0 & -1 \\ 0 & -4 & 0 & 2} \pmatrix{a \\ b \\ c \\ d} =\pmatrix{0 \\ 0 \\ 1 \\ 0} $ ( $v_2= \pmatrix{ -3 \\ 1 \\ 0 \\ 2} $ will give an inconsistent system) Now I get that $x_2 = \pmatrix{-2 \\ 1 \\ 0 \\ 2} $ From the third relation $(A-2I)x_3=x_2$: $\pmatrix{0 & 2 & 0 & -1 \\ 0 & -2 & 0 & 1 \\ 1 & 5 & 0 & -1 \\ 0 & -4 & 0 & 2} \pmatrix{e \\ f \\ g \\ h} =\pmatrix{-2 \\ 1 \\ 0 \\ 2} $ But this system is inconsistent as well! No matter which vectors I try in which places, when I try to generalize eigenvectors I seem to always end up with some inconsistency. Is there something staring me in the face that I am overlooking? Or am I doing it completely wrong (even though this method worked fine for me before)? Sorry for the lengthiness and thank you in advance.",,['linear-algebra']
86,Similarity of real matrices over $\mathbb{C}$,Similarity of real matrices over,\mathbb{C},"$A  \underset{\mathbb{C}}{\sim} B \overset{\text{def}}{\iff} A=C^{-1}BC, \space C\in M_{n}(\mathbb{C})$ and similarly for $\underset{\mathbb{R}}{\sim}$. I want to prove that $ A \underset{\mathbb{C}}{\sim} B$ for $A,B \in M_{n}(\mathbb{R})$ therefore $A \underset{\mathbb{R}}{\sim} B$. My idea is that elementary divisors of $A,B$ over $\mathbb{C}$ are the same, and if $(x-z)^k$ is elementary divisor than $(x-\overline{z})^k$ is also elementary divisor $\implies$ $A,B$ have same elementary divisors over $\mathbb{R}$. But i think it's not clear.","$A  \underset{\mathbb{C}}{\sim} B \overset{\text{def}}{\iff} A=C^{-1}BC, \space C\in M_{n}(\mathbb{C})$ and similarly for $\underset{\mathbb{R}}{\sim}$. I want to prove that $ A \underset{\mathbb{C}}{\sim} B$ for $A,B \in M_{n}(\mathbb{R})$ therefore $A \underset{\mathbb{R}}{\sim} B$. My idea is that elementary divisors of $A,B$ over $\mathbb{C}$ are the same, and if $(x-z)^k$ is elementary divisor than $(x-\overline{z})^k$ is also elementary divisor $\implies$ $A,B$ have same elementary divisors over $\mathbb{R}$. But i think it's not clear.",,"['linear-algebra', 'matrices']"
87,Eigendecomposition and eigenvalue scaling for tensor networks,Eigendecomposition and eigenvalue scaling for tensor networks,,"I've recently been interested in studying tensors and networks of tensors. Consider the following quantity: $$Tr(A^N)$$ Where $A$ is a square matrix and $N$ is large. This is extremely easy - all we need to do is find the eigenvalues and power each to $N$. In the limit that $N$ grows large, the trace is dominated by the largest eigenvalue. In tensor network notation, this is just a bunch of vertices on a line. The vertices are the matrices, and the lines connecting them represent contraction of the corresponding index. $$A_{ab}A_{bc}A_{cd}A_{de}A_{ef}....$$ My problem concerns the analogous scenario with a tensor $W_{abcd}$. Instead of contracting in a line, there are multiple copies of this tensor are arranged in an $N$ x $M$ square lattice and contracted with their nearest neighbors. The north, south, east, and west legs correspond to the a, b, c, and d indices. Is there a way to see how the 'eigenvalues' of this new tensor, or even the trace of the whole quantity, scale with $N$ and $M$? I'm pretty confident that the trace scales as the largest eigenvalue powered to the area of the rectangle, but I'm not sure how to prove it. EDIT: Also, even some simple info about related work on this topic would be greatly appreciated. EDIT 2: Something of note is that if we don't trace out the uncontracted indices, the number of uncontracted indices will scale as the perimeter of the rectangle, and thus the tensor will grow in dimension of its indices. EDIT 3: Some pictures!","I've recently been interested in studying tensors and networks of tensors. Consider the following quantity: $$Tr(A^N)$$ Where $A$ is a square matrix and $N$ is large. This is extremely easy - all we need to do is find the eigenvalues and power each to $N$. In the limit that $N$ grows large, the trace is dominated by the largest eigenvalue. In tensor network notation, this is just a bunch of vertices on a line. The vertices are the matrices, and the lines connecting them represent contraction of the corresponding index. $$A_{ab}A_{bc}A_{cd}A_{de}A_{ef}....$$ My problem concerns the analogous scenario with a tensor $W_{abcd}$. Instead of contracting in a line, there are multiple copies of this tensor are arranged in an $N$ x $M$ square lattice and contracted with their nearest neighbors. The north, south, east, and west legs correspond to the a, b, c, and d indices. Is there a way to see how the 'eigenvalues' of this new tensor, or even the trace of the whole quantity, scale with $N$ and $M$? I'm pretty confident that the trace scales as the largest eigenvalue powered to the area of the rectangle, but I'm not sure how to prove it. EDIT: Also, even some simple info about related work on this topic would be greatly appreciated. EDIT 2: Something of note is that if we don't trace out the uncontracted indices, the number of uncontracted indices will scale as the perimeter of the rectangle, and thus the tensor will grow in dimension of its indices. EDIT 3: Some pictures!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'tensors']"
88,The projection operator on a finite dimensional vector space is diagonalizable,The projection operator on a finite dimensional vector space is diagonalizable,,"This question has been answered before, but I want to check if my solution using minimal polynomials is good. A projection matrix satisfies $M^2 = M$, so it satisfies the polynomial equation $M(M-1) = 0$. Thus the minimal polynomial must be either $M$, $M-1$, or $M(M-1)$. Since in all cases the polynomial factors into distinct linear factors, the matrix is diagonaliazable.","This question has been answered before, but I want to check if my solution using minimal polynomials is good. A projection matrix satisfies $M^2 = M$, so it satisfies the polynomial equation $M(M-1) = 0$. Thus the minimal polynomial must be either $M$, $M-1$, or $M(M-1)$. Since in all cases the polynomial factors into distinct linear factors, the matrix is diagonaliazable.",,['linear-algebra']
89,Clifford Algebra Isomorphic to Exterior Algebra,Clifford Algebra Isomorphic to Exterior Algebra,,"Let $E$ be a vector space over a field $k$ and $Q$ be a quadratic form, that is, $$Q:E\to k$$ such that $$Q(\lambda e)=\lambda^2Q(e)\forall\lambda\in k\,e\in E$$ and such that $P_Q:E^2\to k$ is bilinear, where $$ P_Q(e_1,e_2):=\frac{1}{2}\left[Q(e_1+e_2)-Q(e_1)-Q(e_2)\right]$$ We define the tensor algebra $T(E)$ from $E$ as $$T(E):=\bigoplus_{n=0}^{\infty}E^{\otimes n}$$where $E^{\otimes 0}\equiv k$ and $E^{\otimes n}\equiv E\otimes\dots\otimes E$ (n factors) with the product $$ (e_1\otimes\dots\otimes e_n)\cdot(\tilde{e}_1\otimes\dots\otimes \tilde{e}_\tilde{n}) :=  e_1\otimes\dots\otimes e_n\otimes\tilde{e}_1\otimes\dots\otimes \tilde{e}_\tilde{n} \in E^{\otimes (n+\tilde{n})} $$extended to the whole of $T(E)$ by requiring linearity. Let $I(E,Q)\subseteq T(E)$ be the ideal generated by the set $$ \{ e\otimes e-Q(e)\cdot1_k  \,|\, e\in E\}$$ The the Clifford algebra associated with $E$ and $Q$ is defined as $$ Cl(E,Q) := T(E)/I(E,Q)$$ In Atiyah et al's monograph ""Clifford Modules"" (page 5 point (1.4)) it is claimed that $$ G(Cl(E,Q)) \cong\Lambda(E) $$where $$\Lambda(E)\equiv T(E)/J(E)$$ where $J(E)\subseteq T(E)$ is the ideal generated by the set $$ \{ e\otimes e  \,|\, e\in E\}$$ and $G$ is the associated graded algebra to the filtering of $Cl(E,Q)$ which is induced by the filtering of $T(E)$ given by $$F^q T(E):=\bigoplus_{n=0}^{q}E^{\otimes n}$$ for all $q\in\mathbb{N}_{\geq0}$ This isomorphism is only as vector spaces, not as algebras. My question is: can anyone please write down explicitly what this isomorphism is and describe it? In Wikipedia they give another explicity isomorphism between $Cl(E,Q)$ itself and $\Lambda(E)$, mention the one I'm after, but do not provide it. Because of the complicated nature of the associated graded algebra to a filtered algebra, I'm a bit stumped by this.","Let $E$ be a vector space over a field $k$ and $Q$ be a quadratic form, that is, $$Q:E\to k$$ such that $$Q(\lambda e)=\lambda^2Q(e)\forall\lambda\in k\,e\in E$$ and such that $P_Q:E^2\to k$ is bilinear, where $$ P_Q(e_1,e_2):=\frac{1}{2}\left[Q(e_1+e_2)-Q(e_1)-Q(e_2)\right]$$ We define the tensor algebra $T(E)$ from $E$ as $$T(E):=\bigoplus_{n=0}^{\infty}E^{\otimes n}$$where $E^{\otimes 0}\equiv k$ and $E^{\otimes n}\equiv E\otimes\dots\otimes E$ (n factors) with the product $$ (e_1\otimes\dots\otimes e_n)\cdot(\tilde{e}_1\otimes\dots\otimes \tilde{e}_\tilde{n}) :=  e_1\otimes\dots\otimes e_n\otimes\tilde{e}_1\otimes\dots\otimes \tilde{e}_\tilde{n} \in E^{\otimes (n+\tilde{n})} $$extended to the whole of $T(E)$ by requiring linearity. Let $I(E,Q)\subseteq T(E)$ be the ideal generated by the set $$ \{ e\otimes e-Q(e)\cdot1_k  \,|\, e\in E\}$$ The the Clifford algebra associated with $E$ and $Q$ is defined as $$ Cl(E,Q) := T(E)/I(E,Q)$$ In Atiyah et al's monograph ""Clifford Modules"" (page 5 point (1.4)) it is claimed that $$ G(Cl(E,Q)) \cong\Lambda(E) $$where $$\Lambda(E)\equiv T(E)/J(E)$$ where $J(E)\subseteq T(E)$ is the ideal generated by the set $$ \{ e\otimes e  \,|\, e\in E\}$$ and $G$ is the associated graded algebra to the filtering of $Cl(E,Q)$ which is induced by the filtering of $T(E)$ given by $$F^q T(E):=\bigoplus_{n=0}^{q}E^{\otimes n}$$ for all $q\in\mathbb{N}_{\geq0}$ This isomorphism is only as vector spaces, not as algebras. My question is: can anyone please write down explicitly what this isomorphism is and describe it? In Wikipedia they give another explicity isomorphism between $Cl(E,Q)$ itself and $\Lambda(E)$, mention the one I'm after, but do not provide it. Because of the complicated nature of the associated graded algebra to a filtered algebra, I'm a bit stumped by this.",,"['linear-algebra', 'quadratic-forms', 'clifford-algebras']"
90,The role of the extraction matrix in a Kalman filter,The role of the extraction matrix in a Kalman filter,,"The extraction matrix shown as $H_k$ below, transforms the state vector into a form that can be subtracted from the measurements vector: $\hat{X}_k = \hat{X}_k^- + K_k ({z}_k - H_k \hat{X}_k^-)$ Where $\hat{X}_k^-$ is the projected state (from the state transition model), ${z}_k$ is the measurement vector, and $K_k$ is the Kalman gain. Why is the filter normally formed in this way? Right now $\hat{X}_k^-$ is being transformed to the ""measurement space"" (for lack of a better term) and subtracted from the measurement vector, then it is transformed back into the ""state space"" by being multiplied by $K_k$. Why don't we just transform the measurement matrix instead so that we'd deal with only one transformation? Is there any benefit in doing it this way? Thanks! EDIT: let me explain this better. I'm proposing to modify the measurement update process so it would be of this form: $\hat{X}_k = \hat{X}_k^- + L_k(G_kz_k - \hat{X}_k^-)$. I feel this would be less computationally taxing. The end result would be the same. Instead of multiplying $\hat{X}_k^-$ by $H_k$ I am multiplying $z_k$ by $G_k$. $L_k$ would be the Kalman gain after it has been modified to be compatible with these changes.","The extraction matrix shown as $H_k$ below, transforms the state vector into a form that can be subtracted from the measurements vector: $\hat{X}_k = \hat{X}_k^- + K_k ({z}_k - H_k \hat{X}_k^-)$ Where $\hat{X}_k^-$ is the projected state (from the state transition model), ${z}_k$ is the measurement vector, and $K_k$ is the Kalman gain. Why is the filter normally formed in this way? Right now $\hat{X}_k^-$ is being transformed to the ""measurement space"" (for lack of a better term) and subtracted from the measurement vector, then it is transformed back into the ""state space"" by being multiplied by $K_k$. Why don't we just transform the measurement matrix instead so that we'd deal with only one transformation? Is there any benefit in doing it this way? Thanks! EDIT: let me explain this better. I'm proposing to modify the measurement update process so it would be of this form: $\hat{X}_k = \hat{X}_k^- + L_k(G_kz_k - \hat{X}_k^-)$. I feel this would be less computationally taxing. The end result would be the same. Instead of multiplying $\hat{X}_k^-$ by $H_k$ I am multiplying $z_k$ by $G_k$. $L_k$ would be the Kalman gain after it has been modified to be compatible with these changes.",,"['linear-algebra', 'computational-complexity', 'bayesian-network', 'kalman-filter']"
91,Pointwise convergence of a Legendre polynomial expansion,Pointwise convergence of a Legendre polynomial expansion,,"$\langle\cdot,\cdot\rangle$ is the dot product on the real vector space $\mathcal C ([0,1],\mathbb R)$ defined by $\langle f,g\rangle = \int_{-1}^1 fg$, and $(L_n)$ is the family of normalised Legendre polynomials $((X^2-1)^n)^{(n)}$ divided by their $\langle\cdot,\cdot\rangle$-norm. Given a function $f$ such that the series $$ \sum \langle f,L_n \rangle $$ converges absolutely, I have to show that the series of functions $$ \sum \langle f,L_n \rangle L_n $$ converges pointwise to $f$. Now, I know that it converges to $f$ in the $\mathcal L^2$ sense: $\sum_{k=0}^n\langle f,L_n\rangle L_n \xrightarrow{\|\cdot\|_2}f$ since the $(L_n)$ form a complete orthonormal set of vectors, and I would only need to show the aforementioned series converges pointwise to some function $g$ to conclude, as then $f-g$ would be of ${\|\cdot\|}_2$-norm zero and thus zero. How do I show this ?","$\langle\cdot,\cdot\rangle$ is the dot product on the real vector space $\mathcal C ([0,1],\mathbb R)$ defined by $\langle f,g\rangle = \int_{-1}^1 fg$, and $(L_n)$ is the family of normalised Legendre polynomials $((X^2-1)^n)^{(n)}$ divided by their $\langle\cdot,\cdot\rangle$-norm. Given a function $f$ such that the series $$ \sum \langle f,L_n \rangle $$ converges absolutely, I have to show that the series of functions $$ \sum \langle f,L_n \rangle L_n $$ converges pointwise to $f$. Now, I know that it converges to $f$ in the $\mathcal L^2$ sense: $\sum_{k=0}^n\langle f,L_n\rangle L_n \xrightarrow{\|\cdot\|_2}f$ since the $(L_n)$ form a complete orthonormal set of vectors, and I would only need to show the aforementioned series converges pointwise to some function $g$ to conclude, as then $f-g$ would be of ${\|\cdot\|}_2$-norm zero and thus zero. How do I show this ?",,"['real-analysis', 'linear-algebra', 'sequences-and-series', 'fourier-series', 'legendre-polynomials']"
92,Frobenius determinant theorem,Frobenius determinant theorem,,Can anyone please recommend a paper or a book that gives a detailed proof of the Frobenius determinant theorem? I have read some few papers I saw online but their information are not sufficient for my understanding.Thanks.,Can anyone please recommend a paper or a book that gives a detailed proof of the Frobenius determinant theorem? I have read some few papers I saw online but their information are not sufficient for my understanding.Thanks.,,"['linear-algebra', 'group-theory', 'reference-request', 'finite-groups', 'representation-theory']"
93,Inner product for dual space,Inner product for dual space,,"Suppose we have a Hilbert space $H$. Is there any explicit expression   for the inner product on $H^*$ without resorting to Riesz   representation theorem? I am NOT looking for one that uses the polarization identity - surely there must be a more ""reasonable"" expression? Motivation: In some completely random context, I started with a normed vector space with norm coming from inner product, and was looking at its dual. Embarassingly I couldn't write down its inner product off the top of my head.  I cannot come up with any ""natural"" candidate for the inner product, and while Riesz is nice, it certainly feels unncessarily convoluted, and that one should be able to write down a natural, explicit candidate. A quick search yields this thread , but using polarization identity certainly doesn't help me ""understand"" what this inner product should really be and is thus quite unsatisfying. It would be nice if one can show me such an expression, or give me a reason why it shouldn't exist. Thanks!","Suppose we have a Hilbert space $H$. Is there any explicit expression   for the inner product on $H^*$ without resorting to Riesz   representation theorem? I am NOT looking for one that uses the polarization identity - surely there must be a more ""reasonable"" expression? Motivation: In some completely random context, I started with a normed vector space with norm coming from inner product, and was looking at its dual. Embarassingly I couldn't write down its inner product off the top of my head.  I cannot come up with any ""natural"" candidate for the inner product, and while Riesz is nice, it certainly feels unncessarily convoluted, and that one should be able to write down a natural, explicit candidate. A quick search yields this thread , but using polarization identity certainly doesn't help me ""understand"" what this inner product should really be and is thus quite unsatisfying. It would be nice if one can show me such an expression, or give me a reason why it shouldn't exist. Thanks!",,"['linear-algebra', 'functional-analysis', 'inner-products']"
94,What does the condition number of an adjacency matrix tells us,What does the condition number of an adjacency matrix tells us,,"Assume a directed graph, what does the condition number of its adjacency matrix mean?","Assume a directed graph, what does the condition number of its adjacency matrix mean?",,"['linear-algebra', 'graph-theory', 'numerical-methods']"
95,Gradient descent: L2 norm regularization,Gradient descent: L2 norm regularization,,"So I've worked out Stochastic Gradient Descent to be the following formula approximately for Logistic Regression to be: $ w_{t+1} = w_t - \eta((\sigma({w_t}^Tx_i) - y_t)x_t) $ $p(\mathbf{y} = 1 | \mathbf{x}, \mathbf{w}) = \sigma(\mathbf{w}^T\mathbf{x})$, where $\sigma(t) = \frac{1}{1 + e^{-t}}$ However, I keep screwing something with when adding L2 Norm Regularization: From the HW definition of L2 Norm Regularization: In other words, update $\mathbf{w}_t$ according to $l - \mu  \|\mathbf{w}\|^2 $, where $\mathbf{\mu}$ is a constant. I end up with something like this: $ w_{t+1} = w_t - \eta((\sigma({w_t}^Tx_i) - y_t)x_t + 2\mu w_t) $ I know this isn't right, where am I making a mistake?","So I've worked out Stochastic Gradient Descent to be the following formula approximately for Logistic Regression to be: $ w_{t+1} = w_t - \eta((\sigma({w_t}^Tx_i) - y_t)x_t) $ $p(\mathbf{y} = 1 | \mathbf{x}, \mathbf{w}) = \sigma(\mathbf{w}^T\mathbf{x})$, where $\sigma(t) = \frac{1}{1 + e^{-t}}$ However, I keep screwing something with when adding L2 Norm Regularization: From the HW definition of L2 Norm Regularization: In other words, update $\mathbf{w}_t$ according to $l - \mu  \|\mathbf{w}\|^2 $, where $\mathbf{\mu}$ is a constant. I end up with something like this: $ w_{t+1} = w_t - \eta((\sigma({w_t}^Tx_i) - y_t)x_t + 2\mu w_t) $ I know this isn't right, where am I making a mistake?",,"['linear-algebra', 'multivariable-calculus', 'numerical-optimization', 'gradient-descent', 'regularization']"
96,Spliting subspaces and finite fields,Spliting subspaces and finite fields,,"I'm sure that the following is true, but I can't prove it. Let $R<S<K, R=\mathrm{GF}(q),\  S= \mathrm{GF}(q^n), \ K= \mathrm{GF}(q^{mn})$ be a tower of finite fields and $A = \{\theta\in K: \operatorname{ord}\theta = q^{mn}-1\}$ be the set of primitive elements. I want to prove the following statement. If $_RW$ is a subspace of $_RK$, $\dim_RW = n$  such that for every $\theta \in A$ $$ W\oplus W\theta\oplus\cdots\oplus W\theta^{m-1} = K, $$ then $W = bS$ for some $b\in K$. I will be grateful for the help.","I'm sure that the following is true, but I can't prove it. Let $R<S<K, R=\mathrm{GF}(q),\  S= \mathrm{GF}(q^n), \ K= \mathrm{GF}(q^{mn})$ be a tower of finite fields and $A = \{\theta\in K: \operatorname{ord}\theta = q^{mn}-1\}$ be the set of primitive elements. I want to prove the following statement. If $_RW$ is a subspace of $_RK$, $\dim_RW = n$  such that for every $\theta \in A$ $$ W\oplus W\theta\oplus\cdots\oplus W\theta^{m-1} = K, $$ then $W = bS$ for some $b\in K$. I will be grateful for the help.",,"['linear-algebra', 'abstract-algebra', 'finite-fields', 'primitive-roots']"
97,Frobenius-Perron dimension on a fusion category,Frobenius-Perron dimension on a fusion category,,"Let $C$ be a fusion category with simple objects $V_i\in I$. The fusion rule is $V_i\otimes V_j \cong N_{i,j}^k V_k$. The Frobenius-Perron dimension of a simple object $V_i$, $\mathrm{FPdim}(i)$, is defined as the largest nonnegative real eigenvalue of a matrix $N_i$, where the $(k,j)$ entry is $(N_i)_{k j}=N_{i,j}^k$. I would like to know if we have the following equality or not. $\sum_{j\in I} \mathrm{FPdim}(j) N_{i j}^k=\mathrm{FPdim}(i) \mathrm{FPdim}(k)$ for any fixed $i, k \in I$. If so, I would like to know how to prove this. Thank you.","Let $C$ be a fusion category with simple objects $V_i\in I$. The fusion rule is $V_i\otimes V_j \cong N_{i,j}^k V_k$. The Frobenius-Perron dimension of a simple object $V_i$, $\mathrm{FPdim}(i)$, is defined as the largest nonnegative real eigenvalue of a matrix $N_i$, where the $(k,j)$ entry is $(N_i)_{k j}=N_{i,j}^k$. I would like to know if we have the following equality or not. $\sum_{j\in I} \mathrm{FPdim}(j) N_{i j}^k=\mathrm{FPdim}(i) \mathrm{FPdim}(k)$ for any fixed $i, k \in I$. If so, I would like to know how to prove this. Thank you.",,"['linear-algebra', 'abstract-algebra']"
98,Minimal word length of factorization of invertible matrices into elementary matrices,Minimal word length of factorization of invertible matrices into elementary matrices,,"Let $K$ be a field. As is well known, one can decompose every matrix $A \in GL(n,K)$ into a product of elementary matrices. By an elementary matrix , I mean a matrix which belongs to one of the following types of matrices, which correspond to elementary row resp. column operations (depending on the side from which you multiply one of these matrices): I) $D_{j, \lambda} = diag(1,...,1, \lambda, 1,..., 1)$, a diagonal matrix which has $\lambda \in K \setminus \{ 0 \}$ as its $j$-th diagonal entry, or II) $E_{ij}(\lambda)$ for $i \neq j$ and $\lambda \in K \setminus \{0\}$, which is the $n \times n$-identity matrix plus the matrix which has $(i,j)$-th entry equal to $\lambda$ and $0$ otherwise. Now, given $A \in GL(n,K)$, is there a formula (or, at least a reasonable lower bound) for the minimal length of a factorization of $A$ into elementary matrices? What I have found: Let $E(n,K)$ be the subgroup of $GL(n,K)$ generated by the matrices $E_{ij}(\lambda)$. Then Hinson proves that if $A \in E(n, K)$, then $A$ is a product of at most $n^2+n-2$ matrices of the type II).","Let $K$ be a field. As is well known, one can decompose every matrix $A \in GL(n,K)$ into a product of elementary matrices. By an elementary matrix , I mean a matrix which belongs to one of the following types of matrices, which correspond to elementary row resp. column operations (depending on the side from which you multiply one of these matrices): I) $D_{j, \lambda} = diag(1,...,1, \lambda, 1,..., 1)$, a diagonal matrix which has $\lambda \in K \setminus \{ 0 \}$ as its $j$-th diagonal entry, or II) $E_{ij}(\lambda)$ for $i \neq j$ and $\lambda \in K \setminus \{0\}$, which is the $n \times n$-identity matrix plus the matrix which has $(i,j)$-th entry equal to $\lambda$ and $0$ otherwise. Now, given $A \in GL(n,K)$, is there a formula (or, at least a reasonable lower bound) for the minimal length of a factorization of $A$ into elementary matrices? What I have found: Let $E(n,K)$ be the subgroup of $GL(n,K)$ generated by the matrices $E_{ij}(\lambda)$. Then Hinson proves that if $A \in E(n, K)$, then $A$ is a product of at most $n^2+n-2$ matrices of the type II).",,"['linear-algebra', 'abstract-algebra', 'matrices', 'group-theory', 'lie-groups']"
99,Determinant of a function,Determinant of a function,,"I was thinking about matrices and then why arent there matrices with uncountable many values? (Probably this conecpt already exists for a very long time, but i don't know it) Assume there are matrices $A$ and $B$ with the size $N\times N$. Then the product $C=A*B$ is defined as: $$ C_{y,x}=\sum_{k=1}^N{A_{y,k}*B_{k,x}} $$ Now this could to be done with an uncountable matrix (which is just a function): $$ A(x,y)=x^2+y, B(x,y)=x*y $$ for $$ x, y\in[0,N] $$ Then the value of $C=A*B$ ($*$ for the ""special matrix multiplication"") could be defined as: $$ C(x,y)=\int\limits_0^N{A(k,y)*B(x,k)}\mathrm{d}k $$ Now we can multiply two matrices with uncountable many values. There is also a neutral element which can be defined as $e(x,y)=\delta(x-y)$ (dirac delta function). Of course also vector with uncountable many values could be defined: $$ b(y)=exp(y) $$ It is also no problem to define a multiplication of such a matrix and such a vector: $$ A(x,y)=x^2+y, b(y)=exp(y) $$ for $$ x,y\in[0,N] $$ $$ C(y)=A*b=\int\limits_0^N{A(y,k)*b(k)}\mathrm{d}k $$ And now another thing could be defined. A simple equation ($A,b$ are known, $x$ is unknown): $$ A(x,y)*x(y)=b(y) $$ Or simpler: $$ Ax=b $$ Which is equal to: $$ \int\limits_0^N{A(y,k)*x(k)}\mathrm{d}k=b(y) $$ Now it is not obvious if such an equation has a solution or not. Is it possible to generalize the classic determinant to a function $f(x,y)$, where $x,y\in[0,N]$? So that it is possible to see if such an equation has a unique solution. All things here were just copied from the classic linear algebra, so i think something like a determinant should also be available? Thank you very much best regards Kevin","I was thinking about matrices and then why arent there matrices with uncountable many values? (Probably this conecpt already exists for a very long time, but i don't know it) Assume there are matrices $A$ and $B$ with the size $N\times N$. Then the product $C=A*B$ is defined as: $$ C_{y,x}=\sum_{k=1}^N{A_{y,k}*B_{k,x}} $$ Now this could to be done with an uncountable matrix (which is just a function): $$ A(x,y)=x^2+y, B(x,y)=x*y $$ for $$ x, y\in[0,N] $$ Then the value of $C=A*B$ ($*$ for the ""special matrix multiplication"") could be defined as: $$ C(x,y)=\int\limits_0^N{A(k,y)*B(x,k)}\mathrm{d}k $$ Now we can multiply two matrices with uncountable many values. There is also a neutral element which can be defined as $e(x,y)=\delta(x-y)$ (dirac delta function). Of course also vector with uncountable many values could be defined: $$ b(y)=exp(y) $$ It is also no problem to define a multiplication of such a matrix and such a vector: $$ A(x,y)=x^2+y, b(y)=exp(y) $$ for $$ x,y\in[0,N] $$ $$ C(y)=A*b=\int\limits_0^N{A(y,k)*b(k)}\mathrm{d}k $$ And now another thing could be defined. A simple equation ($A,b$ are known, $x$ is unknown): $$ A(x,y)*x(y)=b(y) $$ Or simpler: $$ Ax=b $$ Which is equal to: $$ \int\limits_0^N{A(y,k)*x(k)}\mathrm{d}k=b(y) $$ Now it is not obvious if such an equation has a solution or not. Is it possible to generalize the classic determinant to a function $f(x,y)$, where $x,y\in[0,N]$? So that it is possible to see if such an equation has a unique solution. All things here were just copied from the classic linear algebra, so i think something like a determinant should also be available? Thank you very much best regards Kevin",,"['linear-algebra', 'definite-integrals', 'determinant', 'matrix-equations']"
