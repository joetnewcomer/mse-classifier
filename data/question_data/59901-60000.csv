,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How can I evaluate the below mentioned series without using a computation software?,How can I evaluate the below mentioned series without using a computation software?,,I have been trying to evaluate $\displaystyle\sum_{m=0}^{2^{2^5}-1}\frac{2}{\prod_{n=1}^5\bigl((m+2)^{\frac{2}{n}}+(m)^{\frac{2}{n}}\bigr)}$ for quite a long time. I tried various approaches but failed.  I'll be grateful if someone can help me out.,I have been trying to evaluate $\displaystyle\sum_{m=0}^{2^{2^5}-1}\frac{2}{\prod_{n=1}^5\bigl((m+2)^{\frac{2}{n}}+(m)^{\frac{2}{n}}\bigr)}$ for quite a long time. I tried various approaches but failed.  I'll be grateful if someone can help me out.,,"['sequences-and-series', 'summation', 'products']"
1,Finite summation of series involving binomial coefficients,Finite summation of series involving binomial coefficients,,"Considering the following summation of series: $$S_n=\sum_{k=0}^{n}(-1)^k{{n}\choose{k}}\sum_{m=0}^{k}(-1)^m\frac{k!}{(k-m)!}b^{-m},$$ where $n$ is a non-negative integer, and $b$ is a known non-zero constant. I computed manually and got $$S_1=b^{-1}, S_2=2b^{-2}, S_3=6b^{-3}.$$ Then I set a hypothesis of $S_n$: $$S_n=n!\cdot b^{-n}.$$ However, I couldn't prove whether it is correct or not. Could someone help me? Thanks very much indeed!","Considering the following summation of series: $$S_n=\sum_{k=0}^{n}(-1)^k{{n}\choose{k}}\sum_{m=0}^{k}(-1)^m\frac{k!}{(k-m)!}b^{-m},$$ where $n$ is a non-negative integer, and $b$ is a known non-zero constant. I computed manually and got $$S_1=b^{-1}, S_2=2b^{-2}, S_3=6b^{-3}.$$ Then I set a hypothesis of $S_n$: $$S_n=n!\cdot b^{-n}.$$ However, I couldn't prove whether it is correct or not. Could someone help me? Thanks very much indeed!",,"['sequences-and-series', 'summation', 'binomial-coefficients']"
2,Convergence of sequence $a_{n+1} = \int_0^{a_n}[1+\frac{\cos^{2n+1} t}{4}]dt$ [closed],Convergence of sequence  [closed],a_{n+1} = \int_0^{a_n}[1+\frac{\cos^{2n+1} t}{4}]dt,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question How to show that sequence   $$a_{n+1} = \int_0^{a_n}\left(1+\frac{\cos^{2n+1} t}{4}\right)\,dt$$   with $a_0 \in (0, 2\pi)$ is convergent?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question How to show that sequence   $$a_{n+1} = \int_0^{a_n}\left(1+\frac{\cos^{2n+1} t}{4}\right)\,dt$$   with $a_0 \in (0, 2\pi)$ is convergent?",,"['sequences-and-series', 'convergence-divergence']"
3,What's the nonstandard way to argue $\lim_{n\to\infty}\sum_{k=0}^n\binom n k (\frac x n)^k = \sum_{k=0}^\infty \frac{x^k}{k!}$,What's the nonstandard way to argue,\lim_{n\to\infty}\sum_{k=0}^n\binom n k (\frac x n)^k = \sum_{k=0}^\infty \frac{x^k}{k!},"First, the equality holds, as: $$\lim_{n\to\infty}\sum_{k=0}^n\binom n k \left(\frac x n\right)^k  =\lim_{n\to\infty}\left(\left(1+\frac{x}{n}\right)^n\right) = e^x =  \sum_{k=0}^\infty \frac{x^k}{k!} $$ Having a limes over a series, this should be a prime target for nonstandard-reasoning, as we can just substitute in $h$ for $n$ where $h$ is an infinite hyperreal and drop the limit: $$\sum_{k=0}^h\binom h k \left(\frac x h\right)^k \tag{i}$$  Now all that should be left to do should be to show that this sum is infinitesimally close to $\displaystyle  \sum_{k=0}^\infty \frac{x^k}{k!}$ So, by transforming $(i)$, we get $$\displaystyle\sum_{k=0}^h\binom h k \left(\frac x h\right)^k  =  \sum_{k=0}^h \frac{h!}{k!(h-k)!} \left(\frac x h\right)^k =  \sum_{k=0}^h \frac{h!}{h^k(h-k)!} \frac {x^k} {k!} $$ Now, we split the sum in two parts: The natural summands, and the (infinite) hypernatural summands. The natural summands should be infinitesimally close to our target sum , and the hypernatural summands should be infinitesimally close to 0. Here's where my argumentation turns grotesque. As far as I know, comparing infinite hyperreals is a fruitless endeavor, as their difference/ratio can be any hyperreal value. So, I have to argue using the known limits of regular sequences: $$  \lim_{k\to\infty} \frac {x^k}{k!} = 0  \qquad \lim_{k\to\infty} \frac {n!}{n^k} = 0   $$ Using these both limits, we can argue (Let $d_1,d_2$ be infinitesimals): $$ \sum_{k\in\mathbb{^*N/N}\\k\le h}^h \frac{h!}{h^k(h-k)!} \frac {x^k} {k!} \approx  \sum_{k\in\mathbb{^*N/N}\\k\le h}^h \frac{h!}{h^k(h-k)!} d_1 \approx \sum_{k\in\mathbb{^*N/N}\\k\le h}^h \frac{d_1 d_2}{(h-k)!}\ \approx 0 $$ The argumentation here is that every summand $a_k$ is infinitely smaller than $h$, so even the infinite sum is infinitesimally close to $0$. (Question 1: Is this argumentation correct, and can it be made easier?) And for the natural part of the sum, using $\frac {h-i}{h} \approx \frac {h}{h}\approx 1$ , for $i\in\mathbb{R}$: $$ \sum_{k\in\mathbb{N}} \frac{h!}{h^k(h-k)!} \frac {x^k} {k!} = \sum_{k\in\mathbb{N}} \frac{(h\cdot (h-1)\cdots(h-k+1))}{h^k} \frac {x^k} {k!} \underbrace{\approx}_{(1)} \sum_{k\in\mathbb{N}} \frac{h^k}{h^k} \frac {x^k} {k!} =  \sum_{k\in\mathbb{N}} \frac {x^k} {k!}$$ Note that (1) holds as there's only finitely many $\approx$-transformations, i.e. the error introduced can't sum up to a significant amount. (Question 2: Is this argumentation correct? It feels fishy) Reacting on the spotted mistakes I've tried to fix the reasoning. Instead of splitting the sum $\displaystyle\sum_{k=0}^h$ in $\displaystyle\sum_{k\in\mathbb{N}}$ and $\displaystyle\sum_{k\in\mathbb{^*N/N}}$ I'm now trying a split into these two sums:  $\displaystyle\sum_{k=0}^{h_1}$ and $\displaystyle\sum_{k=h_1}^{h_2}$ where $h_1,h_2$ are both infinite hyperreals. Now, the responding subsets of $\mathbb{^*N}$ should be internal. The idea behind this approach is that $h_1 << h_2$, so that $\displaystyle\sum_{k=0}^{h_1}$ can take a similar role to the sum  $\displaystyle\sum_{k\in\mathbb{N}}$. Therefore, we choose $h_1$ so that $\frac{h_1}{h_2} \approx 0$. We can do this without loss of generality, as we can find such an infinite hyperreal $h_1$ for any infinite hyperreal $h_2$ (e.g. $h_1 := \lfloor\sqrt{h_2}\rfloor$). Now the argumentation translates rather analogue: $$ \sum_{k=0}^{h_2} \frac{{h_2}!}{{h_2}^k({h_2}-k)!} \frac {x^k} {k!} = \bigg(\sum_{k=0}^{h_1} \frac{{h_2}!}{{h_2}^k({h_2}-k)!} \frac {x^k} {k!}\bigg) +  \bigg(\sum_{k=h_1}^{h_2} \frac{{h_2}!}{{h_2}^k({h_2}-k)!} \frac {x^k} {k!}\bigg) $$ $$ \sum_{k=0}^{{h_1}} \frac{{h_2}!}{{h_2}^k({h_2}-k)!} \frac {x^k} {k!} = \sum_{k=0}^{{h_1}} \frac{({h_2}\cdot ({h_2}-1)\cdots({h_2}-k+1))}{{h_2}^k} \frac {x^k} {k!} =\\ \sum_{k=0}^{h_1} \frac{{h_2}^k}{{h_2}^k} \frac {x^k} {k!} +\Delta_k=  \sum_{k\in\mathbb{N}} \frac {x^k} {k!} + \Delta_k$$ Here, $\Delta_k$ is supposed to be the difference that is defined so that the equation holds.  We now have to show $\sum_{k=0}^{h_1} \Delta_k \approx 0$: $$|\Delta_k| \underbrace{\le}_{\#1} \frac 2 {h_2} \cdot \frac {x^k}{k!} \underbrace{\le}_{\#2} \frac 2 {h_2} \implies \sum_{k=0}^{h_1} |\Delta_k| \le\frac{(h_1+1)\cdot 2} {h_2} \approx 0$$ #1: This one is a long calculation with a few estimations, but it should be rather clear. #2: This inequality holds for almost all $k$, as $\frac{x^k}{k!}$ tends to 0. And therefore, $\sum_{k=0}^{h_1} \Delta_k \approx 0$ holds. Finally, for the sum $\displaystyle \sum_{k=h_1}^{h_2} \frac{{h_2}!}{{h_2}^k({h_2}-k)!} \frac {x^k} {k!}$ we again use analogous arguments: $$ \sum_{k=h_1}^{h_2} \frac{{h_2}!}{{h_2}^k({h_2}-k)!} \frac {x^k} {k!} \approx \sum_{k=h_1}^{h_2} \frac{d_1 \cdot {h_2}!}{{h_2}^k({h_2}-k)!} \approx \sum_{k=h_1}^{h_2} \frac{d_1 d_2}{({h_2}-k)!} =\\ d_1 d_2 \sum_{k=h_1}^{h_2} \frac{1}{({h_2}-k)!} = d_1 d_2 \sum_{k=0}^{h_2-h_1} \frac{1}{k!} \approx d_1 d_2 e \approx 0 $$","First, the equality holds, as: $$\lim_{n\to\infty}\sum_{k=0}^n\binom n k \left(\frac x n\right)^k  =\lim_{n\to\infty}\left(\left(1+\frac{x}{n}\right)^n\right) = e^x =  \sum_{k=0}^\infty \frac{x^k}{k!} $$ Having a limes over a series, this should be a prime target for nonstandard-reasoning, as we can just substitute in $h$ for $n$ where $h$ is an infinite hyperreal and drop the limit: $$\sum_{k=0}^h\binom h k \left(\frac x h\right)^k \tag{i}$$  Now all that should be left to do should be to show that this sum is infinitesimally close to $\displaystyle  \sum_{k=0}^\infty \frac{x^k}{k!}$ So, by transforming $(i)$, we get $$\displaystyle\sum_{k=0}^h\binom h k \left(\frac x h\right)^k  =  \sum_{k=0}^h \frac{h!}{k!(h-k)!} \left(\frac x h\right)^k =  \sum_{k=0}^h \frac{h!}{h^k(h-k)!} \frac {x^k} {k!} $$ Now, we split the sum in two parts: The natural summands, and the (infinite) hypernatural summands. The natural summands should be infinitesimally close to our target sum , and the hypernatural summands should be infinitesimally close to 0. Here's where my argumentation turns grotesque. As far as I know, comparing infinite hyperreals is a fruitless endeavor, as their difference/ratio can be any hyperreal value. So, I have to argue using the known limits of regular sequences: $$  \lim_{k\to\infty} \frac {x^k}{k!} = 0  \qquad \lim_{k\to\infty} \frac {n!}{n^k} = 0   $$ Using these both limits, we can argue (Let $d_1,d_2$ be infinitesimals): $$ \sum_{k\in\mathbb{^*N/N}\\k\le h}^h \frac{h!}{h^k(h-k)!} \frac {x^k} {k!} \approx  \sum_{k\in\mathbb{^*N/N}\\k\le h}^h \frac{h!}{h^k(h-k)!} d_1 \approx \sum_{k\in\mathbb{^*N/N}\\k\le h}^h \frac{d_1 d_2}{(h-k)!}\ \approx 0 $$ The argumentation here is that every summand $a_k$ is infinitely smaller than $h$, so even the infinite sum is infinitesimally close to $0$. (Question 1: Is this argumentation correct, and can it be made easier?) And for the natural part of the sum, using $\frac {h-i}{h} \approx \frac {h}{h}\approx 1$ , for $i\in\mathbb{R}$: $$ \sum_{k\in\mathbb{N}} \frac{h!}{h^k(h-k)!} \frac {x^k} {k!} = \sum_{k\in\mathbb{N}} \frac{(h\cdot (h-1)\cdots(h-k+1))}{h^k} \frac {x^k} {k!} \underbrace{\approx}_{(1)} \sum_{k\in\mathbb{N}} \frac{h^k}{h^k} \frac {x^k} {k!} =  \sum_{k\in\mathbb{N}} \frac {x^k} {k!}$$ Note that (1) holds as there's only finitely many $\approx$-transformations, i.e. the error introduced can't sum up to a significant amount. (Question 2: Is this argumentation correct? It feels fishy) Reacting on the spotted mistakes I've tried to fix the reasoning. Instead of splitting the sum $\displaystyle\sum_{k=0}^h$ in $\displaystyle\sum_{k\in\mathbb{N}}$ and $\displaystyle\sum_{k\in\mathbb{^*N/N}}$ I'm now trying a split into these two sums:  $\displaystyle\sum_{k=0}^{h_1}$ and $\displaystyle\sum_{k=h_1}^{h_2}$ where $h_1,h_2$ are both infinite hyperreals. Now, the responding subsets of $\mathbb{^*N}$ should be internal. The idea behind this approach is that $h_1 << h_2$, so that $\displaystyle\sum_{k=0}^{h_1}$ can take a similar role to the sum  $\displaystyle\sum_{k\in\mathbb{N}}$. Therefore, we choose $h_1$ so that $\frac{h_1}{h_2} \approx 0$. We can do this without loss of generality, as we can find such an infinite hyperreal $h_1$ for any infinite hyperreal $h_2$ (e.g. $h_1 := \lfloor\sqrt{h_2}\rfloor$). Now the argumentation translates rather analogue: $$ \sum_{k=0}^{h_2} \frac{{h_2}!}{{h_2}^k({h_2}-k)!} \frac {x^k} {k!} = \bigg(\sum_{k=0}^{h_1} \frac{{h_2}!}{{h_2}^k({h_2}-k)!} \frac {x^k} {k!}\bigg) +  \bigg(\sum_{k=h_1}^{h_2} \frac{{h_2}!}{{h_2}^k({h_2}-k)!} \frac {x^k} {k!}\bigg) $$ $$ \sum_{k=0}^{{h_1}} \frac{{h_2}!}{{h_2}^k({h_2}-k)!} \frac {x^k} {k!} = \sum_{k=0}^{{h_1}} \frac{({h_2}\cdot ({h_2}-1)\cdots({h_2}-k+1))}{{h_2}^k} \frac {x^k} {k!} =\\ \sum_{k=0}^{h_1} \frac{{h_2}^k}{{h_2}^k} \frac {x^k} {k!} +\Delta_k=  \sum_{k\in\mathbb{N}} \frac {x^k} {k!} + \Delta_k$$ Here, $\Delta_k$ is supposed to be the difference that is defined so that the equation holds.  We now have to show $\sum_{k=0}^{h_1} \Delta_k \approx 0$: $$|\Delta_k| \underbrace{\le}_{\#1} \frac 2 {h_2} \cdot \frac {x^k}{k!} \underbrace{\le}_{\#2} \frac 2 {h_2} \implies \sum_{k=0}^{h_1} |\Delta_k| \le\frac{(h_1+1)\cdot 2} {h_2} \approx 0$$ #1: This one is a long calculation with a few estimations, but it should be rather clear. #2: This inequality holds for almost all $k$, as $\frac{x^k}{k!}$ tends to 0. And therefore, $\sum_{k=0}^{h_1} \Delta_k \approx 0$ holds. Finally, for the sum $\displaystyle \sum_{k=h_1}^{h_2} \frac{{h_2}!}{{h_2}^k({h_2}-k)!} \frac {x^k} {k!}$ we again use analogous arguments: $$ \sum_{k=h_1}^{h_2} \frac{{h_2}!}{{h_2}^k({h_2}-k)!} \frac {x^k} {k!} \approx \sum_{k=h_1}^{h_2} \frac{d_1 \cdot {h_2}!}{{h_2}^k({h_2}-k)!} \approx \sum_{k=h_1}^{h_2} \frac{d_1 d_2}{({h_2}-k)!} =\\ d_1 d_2 \sum_{k=h_1}^{h_2} \frac{1}{({h_2}-k)!} = d_1 d_2 \sum_{k=0}^{h_2-h_1} \frac{1}{k!} \approx d_1 d_2 e \approx 0 $$",,"['sequences-and-series', 'proof-verification', 'nonstandard-analysis']"
4,Proving the uniform convergence of $f_n$,Proving the uniform convergence of,f_n,"I need to show that $f_n(x) = \frac{\ln(1+nx)}{n+x}$ converges uniformly on $X = [0,10]$.  I have tried proving by Cauchy criterion, as well as from the definition (where limit function is $f =0$), but no results so far.","I need to show that $f_n(x) = \frac{\ln(1+nx)}{n+x}$ converges uniformly on $X = [0,10]$.  I have tried proving by Cauchy criterion, as well as from the definition (where limit function is $f =0$), but no results so far.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'uniform-convergence']"
5,Dense sets in Hilbert space of square summable sequences?,Dense sets in Hilbert space of square summable sequences?,,"Let $l^2 = \{x = (x_n)| x_n \in \mathbb{R}, \sum_{n=1}^{\infty} x_n^2 < \infty\}$  be the Hilbert space of square summable sequences and let $e_k$ denote the $k^{th}$ co-ordinate vector (with $1$ in $k^{th}$ place, $0$ elsewhere). Which of the following subspaces is NOT dense in $l^2$? span$\{e_1-e_2, e_2-e_3, \ldots\}$ span$\{2e_1-e_2, 2e_2-e_3, \ldots\}$ span$\{e_1-2e_2, e_2-2e_3, \ldots\}$ span$\{e_2, e_3, \ldots\}$ By definition, we have to prove that for any $x \in l^2$ and for any $\epsilon > 0$, there exists $y$ in the respective sets such that $\|x-y\| < \epsilon$. How to proceed further?","Let $l^2 = \{x = (x_n)| x_n \in \mathbb{R}, \sum_{n=1}^{\infty} x_n^2 < \infty\}$  be the Hilbert space of square summable sequences and let $e_k$ denote the $k^{th}$ co-ordinate vector (with $1$ in $k^{th}$ place, $0$ elsewhere). Which of the following subspaces is NOT dense in $l^2$? span$\{e_1-e_2, e_2-e_3, \ldots\}$ span$\{2e_1-e_2, 2e_2-e_3, \ldots\}$ span$\{e_1-2e_2, e_2-2e_3, \ldots\}$ span$\{e_2, e_3, \ldots\}$ By definition, we have to prove that for any $x \in l^2$ and for any $\epsilon > 0$, there exists $y$ in the respective sets such that $\|x-y\| < \epsilon$. How to proceed further?",,"['sequences-and-series', 'hilbert-spaces', 'lp-spaces', 'orthonormal']"
6,Difference of two compact sets is compact,Difference of two compact sets is compact,,"Consider this question: Let $K_{1}$ and $K_{2}$ be compact sets. Define $A=\{||x_{1}-x_{2}||:x_{1} \in K_{1}, x_{2} \in K_{2}\}$. Show that $A$ is compact. My atempt consist in prove that $A$ is compact using the notion of sequentially compact. $\textbf{My answer}$: Let $K_{1}$ and $K_{2}$ be compact sets. Therefore, every sequence has convergent subsequence. Consider the sequences $(x_{n}^{1})_{n \in \mathbb{N}} \in K_{1}, \forall n \in \mathbb{N}$, and $(x_{n}^{2})_{n \in \mathbb{N}} \in K_{2}, \forall n \in \mathbb{N}$. Hence, $\exists (x_{n_{j}}^{1})_{j \in \mathbb{N}} \in K_{1}:x_{n_{j}}^{1} \to x^{1}$, and $\exists (x_{n_{j}}^{2})_{j \in \mathbb{N}} \in K_{2}:x_{n_{j}}^{2} \to x^{2}$. Since both $K_{1}$ and $K_{2}$ are compact sets, $x^{1} \in K_{1}$ and $x^{2} \in K_{2}$. Therefore, the sequence defined as $y_{n}=x_{n_{j}}^{1}-x_{n_{j}}^{2} \to (x^{1}-x^{2}) \in A$, hence, $A$ is compact. Is this a valid proof?","Consider this question: Let $K_{1}$ and $K_{2}$ be compact sets. Define $A=\{||x_{1}-x_{2}||:x_{1} \in K_{1}, x_{2} \in K_{2}\}$. Show that $A$ is compact. My atempt consist in prove that $A$ is compact using the notion of sequentially compact. $\textbf{My answer}$: Let $K_{1}$ and $K_{2}$ be compact sets. Therefore, every sequence has convergent subsequence. Consider the sequences $(x_{n}^{1})_{n \in \mathbb{N}} \in K_{1}, \forall n \in \mathbb{N}$, and $(x_{n}^{2})_{n \in \mathbb{N}} \in K_{2}, \forall n \in \mathbb{N}$. Hence, $\exists (x_{n_{j}}^{1})_{j \in \mathbb{N}} \in K_{1}:x_{n_{j}}^{1} \to x^{1}$, and $\exists (x_{n_{j}}^{2})_{j \in \mathbb{N}} \in K_{2}:x_{n_{j}}^{2} \to x^{2}$. Since both $K_{1}$ and $K_{2}$ are compact sets, $x^{1} \in K_{1}$ and $x^{2} \in K_{2}$. Therefore, the sequence defined as $y_{n}=x_{n_{j}}^{1}-x_{n_{j}}^{2} \to (x^{1}-x^{2}) \in A$, hence, $A$ is compact. Is this a valid proof?",,"['real-analysis', 'sequences-and-series', 'compactness']"
7,How do I prove that $a_n=\sum_{k=1}^{n} \frac{1}{k}$ is not a Cauchy sequence?,How do I prove that  is not a Cauchy sequence?,a_n=\sum_{k=1}^{n} \frac{1}{k},"First, I have to say that it does not make sense to me, because I know for sure that $\sum_{k=1}^{n}\frac{1}{k^2}$ is a Cauchy sequence so naturally in my mind the discussed one ""should"" converge too. But this is a homework so I guess my intuition is wrong this time. This is what I have done so far: There exists $N\in \mathbb{N}$. So, $$ \begin{aligned} |a_{n+N}-a_n| =|\sum_{k=1}^{n+N} \frac{1}{k}-\sum_{k=1}^{n} \frac{1}{k}|= |\frac{1}{n+1}+\frac{1}{n+2}+...+\frac{1}{n+N}|=\frac{1}{n+1}+\\ \frac{1}{n+2}+...+\frac{1}{n+N} \geq \frac{N}{n+N} \geq \epsilon \end{aligned} $$ I probably not skilled enough in algebra to continue from here. Edit: I have to find an $\epsilon$ that satisfies this expression.","First, I have to say that it does not make sense to me, because I know for sure that $\sum_{k=1}^{n}\frac{1}{k^2}$ is a Cauchy sequence so naturally in my mind the discussed one ""should"" converge too. But this is a homework so I guess my intuition is wrong this time. This is what I have done so far: There exists $N\in \mathbb{N}$. So, $$ \begin{aligned} |a_{n+N}-a_n| =|\sum_{k=1}^{n+N} \frac{1}{k}-\sum_{k=1}^{n} \frac{1}{k}|= |\frac{1}{n+1}+\frac{1}{n+2}+...+\frac{1}{n+N}|=\frac{1}{n+1}+\\ \frac{1}{n+2}+...+\frac{1}{n+N} \geq \frac{N}{n+N} \geq \epsilon \end{aligned} $$ I probably not skilled enough in algebra to continue from here. Edit: I have to find an $\epsilon$ that satisfies this expression.",,"['sequences-and-series', 'limits', 'cauchy-sequences']"
8,Creating a Power Series with Interval of Convergence Given an Interval,Creating a Power Series with Interval of Convergence Given an Interval,,"So I know to determine an interval for a power series, we would use things like the ratio test and then some of the other tests to determine the endpoints but how would you go about doing the reverse? If we had any such interval like (a,b], [a,b], [a,b), or (a,b), how would you go about creating a power series that would have such an interval? An observation I was thinking of, since the general expression for a power series contains $(x-c)$ where the series is centered around $c$, we would have it so that $c$ = $(a+b)/2$ Is there a systematic way of doing such a thing?","So I know to determine an interval for a power series, we would use things like the ratio test and then some of the other tests to determine the endpoints but how would you go about doing the reverse? If we had any such interval like (a,b], [a,b], [a,b), or (a,b), how would you go about creating a power series that would have such an interval? An observation I was thinking of, since the general expression for a power series contains $(x-c)$ where the series is centered around $c$, we would have it so that $c$ = $(a+b)/2$ Is there a systematic way of doing such a thing?",,"['calculus', 'sequences-and-series', 'convergence-divergence', 'power-series']"
9,Infinite product with non homogenous recurrence relation,Infinite product with non homogenous recurrence relation,,Let $a_1=1$ and $a_n=n(a_{n-1}+1)$ Define $$P_n=\prod_{i=1}^n(1+\frac1{a_i})$$ Find $P_n$ as n approaches $\infty$ I'm not sure where to start tbh. I don't know how to solve the recurrence relation because of the '$n$' Any help is appreciated!,Let $a_1=1$ and $a_n=n(a_{n-1}+1)$ Define $$P_n=\prod_{i=1}^n(1+\frac1{a_i})$$ Find $P_n$ as n approaches $\infty$ I'm not sure where to start tbh. I don't know how to solve the recurrence relation because of the '$n$' Any help is appreciated!,,"['sequences-and-series', 'infinite-product']"
10,"Is $\lim_{x\to \infty}{\sum_{i,h=1}^x \frac{1}{i^h} - x-\ln{x}}$ equal to $\gamma$, the Euler-Mascheroni constant? If so, why?","Is  equal to , the Euler-Mascheroni constant? If so, why?","\lim_{x\to \infty}{\sum_{i,h=1}^x \frac{1}{i^h} - x-\ln{x}} \gamma","I was looking at the sum $\sum_{i,h=1}^x \frac{1}{i^h}$ on Desmos, and I realized it seemed to converge to the line $y=x$. When I subtracted x from it and increased the bounds, it seemed to be converging close to the Euler-Mascheroni constant. Unfortunately, it quickly gets difficult to calculate for large x, so the best estimate I could get with Desmos was for $x=10000$, for which $\sum_{i,h=1}^x \frac{1}{i^h} - x-\ln{x}$ is approximately 0.577165, which is very close to the Euler-Mascheroni constant. Is this actually converging to the constant, or just to something close to it? I would guess that it does, due to this series's clear similarity to the harmonic series, however it is interesting that the constant still appears even with the added exponentiation in the denominator. For clarity, an alternate notation for the sum above would be $${\sum_{i=1}^x}{\sum_{h=1}^x {1\over i^h}}$$","I was looking at the sum $\sum_{i,h=1}^x \frac{1}{i^h}$ on Desmos, and I realized it seemed to converge to the line $y=x$. When I subtracted x from it and increased the bounds, it seemed to be converging close to the Euler-Mascheroni constant. Unfortunately, it quickly gets difficult to calculate for large x, so the best estimate I could get with Desmos was for $x=10000$, for which $\sum_{i,h=1}^x \frac{1}{i^h} - x-\ln{x}$ is approximately 0.577165, which is very close to the Euler-Mascheroni constant. Is this actually converging to the constant, or just to something close to it? I would guess that it does, due to this series's clear similarity to the harmonic series, however it is interesting that the constant still appears even with the added exponentiation in the denominator. For clarity, an alternate notation for the sum above would be $${\sum_{i=1}^x}{\sum_{h=1}^x {1\over i^h}}$$",,"['sequences-and-series', 'euler-mascheroni-constant']"
11,Analytic properties of Euler sums,Analytic properties of Euler sums,,"Introduction As far as I know this topic has not been discussed before. I have found interesting results which I wish to share with you in the standard MSE manner by asking questions. Consider the Euler sum $$S(1,s) =\sum _{k=1}^{\infty } \frac{H_k}{k^s}\tag{1}$$ where $H_k = \sum_{j=1}^k 1/j$ is the harmonic number and $s$ is a parameter which in the field of Euler sums is considered to assume integer values $\ge 2$. Here we let $s$ be a complex number and ask for the analytic properties of $S(1,s)$ as a function of $s$. In parallel and for comparison we consider the similar sum with the harmonic number replaced by unitiy. This is the well known zeta function studied first by Euler $$\zeta(s) = \sum _{k=1}^{\infty } \frac{1}{k^s}\tag{2}$$ The analytic properties of this function have been studied first by Bernhard Riemann in his famous paper on the distribution of prime numbers of 1859 [1]. The zeta function is analytic in the whole complex $s$ plane except for a simple pole at $s=1$ with residue $1$. It has trivial simple zeroes at negative even integers and non trivial zeroes on the line $\Re[s] = 1/2$ possibly even all non trivial zeroes are located on this line (Riemann's hypothesis) [2],[3]. Question 1 What are the singularities of $S(1,s)$ in the complex s-plane? What can be said about the zeroes? Compare the results with those of $\zeta(s)$ First solution steps: derive an integral representation of $S(1,s)$, and split is into an holomorphic and a meromorphic part. Study the meromorphic part to find the singularities. Question 2 $S(1,q)$ has a well known closed form for integer argument $q = 2, 3, 4, ..$ which was already found by Euler. It is given by $$S_E(1,q) = \left(\frac{q}{2}+1\right) \zeta (q+1)-\frac{1}{2} \sum _{k=1}^{q-2} \zeta (k+1) \zeta (q-k)\tag{3}$$ The question asks for a possible closed form for real $q \ge 2$ First solution steps: Here I have no idea how to find a better solution than the integral representation. Question 3 Consider the analogue questions 1 and 2 also for the general linear harmonic sum including the generating function via the parameter $x$ $$S_H(p, q, x)=\sum _{k=1}^{\infty } \frac{x^k H_k^{(p)}}{k^q}\tag{4}$$ Here $H_k^{(p)}=\sum_{n=1}^k 1/n^p$ is the generalized harmonic number. As there are three quantities in the game, to be definite, we consider some simplifying cases. Case 1: $x=1$ and $x=-1$ (alternating sum) for fixed $p$ as a function of $q$ Case 2: fixed $x$ and $p=q$ as a function of $p$. Notice that there is an analogous formula to (3) for integer $p$ Case 3: $X$ fixed somewhere within the interval $(-1,1)$, e.g. $x=\frac{1}{2}$ and, say, $p=1$. References [1] http://www.claymath.org/sites/default/files/zeta.pdf (Bernhard Riemann, Über die Anzahl der Primzahlen unter einer gegebenen Größe, Berlin November 1859) [2] https://en.wikipedia.org/wiki/Riemann_zeta_function [3] https://de.wikipedia.org/wiki/Riemannsche_%CE%B6-Funktion","Introduction As far as I know this topic has not been discussed before. I have found interesting results which I wish to share with you in the standard MSE manner by asking questions. Consider the Euler sum $$S(1,s) =\sum _{k=1}^{\infty } \frac{H_k}{k^s}\tag{1}$$ where $H_k = \sum_{j=1}^k 1/j$ is the harmonic number and $s$ is a parameter which in the field of Euler sums is considered to assume integer values $\ge 2$. Here we let $s$ be a complex number and ask for the analytic properties of $S(1,s)$ as a function of $s$. In parallel and for comparison we consider the similar sum with the harmonic number replaced by unitiy. This is the well known zeta function studied first by Euler $$\zeta(s) = \sum _{k=1}^{\infty } \frac{1}{k^s}\tag{2}$$ The analytic properties of this function have been studied first by Bernhard Riemann in his famous paper on the distribution of prime numbers of 1859 [1]. The zeta function is analytic in the whole complex $s$ plane except for a simple pole at $s=1$ with residue $1$. It has trivial simple zeroes at negative even integers and non trivial zeroes on the line $\Re[s] = 1/2$ possibly even all non trivial zeroes are located on this line (Riemann's hypothesis) [2],[3]. Question 1 What are the singularities of $S(1,s)$ in the complex s-plane? What can be said about the zeroes? Compare the results with those of $\zeta(s)$ First solution steps: derive an integral representation of $S(1,s)$, and split is into an holomorphic and a meromorphic part. Study the meromorphic part to find the singularities. Question 2 $S(1,q)$ has a well known closed form for integer argument $q = 2, 3, 4, ..$ which was already found by Euler. It is given by $$S_E(1,q) = \left(\frac{q}{2}+1\right) \zeta (q+1)-\frac{1}{2} \sum _{k=1}^{q-2} \zeta (k+1) \zeta (q-k)\tag{3}$$ The question asks for a possible closed form for real $q \ge 2$ First solution steps: Here I have no idea how to find a better solution than the integral representation. Question 3 Consider the analogue questions 1 and 2 also for the general linear harmonic sum including the generating function via the parameter $x$ $$S_H(p, q, x)=\sum _{k=1}^{\infty } \frac{x^k H_k^{(p)}}{k^q}\tag{4}$$ Here $H_k^{(p)}=\sum_{n=1}^k 1/n^p$ is the generalized harmonic number. As there are three quantities in the game, to be definite, we consider some simplifying cases. Case 1: $x=1$ and $x=-1$ (alternating sum) for fixed $p$ as a function of $q$ Case 2: fixed $x$ and $p=q$ as a function of $p$. Notice that there is an analogous formula to (3) for integer $p$ Case 3: $X$ fixed somewhere within the interval $(-1,1)$, e.g. $x=\frac{1}{2}$ and, say, $p=1$. References [1] http://www.claymath.org/sites/default/files/zeta.pdf (Bernhard Riemann, Über die Anzahl der Primzahlen unter einer gegebenen Größe, Berlin November 1859) [2] https://en.wikipedia.org/wiki/Riemann_zeta_function [3] https://de.wikipedia.org/wiki/Riemannsche_%CE%B6-Funktion",,"['sequences-and-series', 'riemann-zeta', 'harmonic-numbers', 'singularity']"
12,Sum of reciprocals of odd numbers that add up to $2$,Sum of reciprocals of odd numbers that add up to,2,"It is well known that certain sums of reciprocals add up to $2$: the reciprocals of powers of $2$: $$ {1 \over 1}+{1 \over 2}+{1 \over 4}+{1 \over 8}+{1 \over 16}+{1 \over 32}+\cdots =2, $$ the reciprocals of the triangular numbers: $$ {1 \over 1}+{1 \over 3}+{1 \over 6}+{1 \over 10}+{1 \over 15}+{1 \over 21}+\cdots =2, $$ $\ldots$ Are there any sums such that $\sum_{i=0}^{\infty} s_i^{-1}=2$ for which all the $s_i$ are all distinct odd positive integers? Similarly, are there any sums such that $\exists k\in\mathbb{Z}, k>0$ such that $\sum_{i=0}^{k} s_i^{-1}=2$ for which all the $s_i$ are all distinct odd positive integers?","It is well known that certain sums of reciprocals add up to $2$: the reciprocals of powers of $2$: $$ {1 \over 1}+{1 \over 2}+{1 \over 4}+{1 \over 8}+{1 \over 16}+{1 \over 32}+\cdots =2, $$ the reciprocals of the triangular numbers: $$ {1 \over 1}+{1 \over 3}+{1 \over 6}+{1 \over 10}+{1 \over 15}+{1 \over 21}+\cdots =2, $$ $\ldots$ Are there any sums such that $\sum_{i=0}^{\infty} s_i^{-1}=2$ for which all the $s_i$ are all distinct odd positive integers? Similarly, are there any sums such that $\exists k\in\mathbb{Z}, k>0$ such that $\sum_{i=0}^{k} s_i^{-1}=2$ for which all the $s_i$ are all distinct odd positive integers?",,"['sequences-and-series', 'elementary-number-theory']"
13,Partial Differential Equation to evaluate double series,Partial Differential Equation to evaluate double series,,"I was trying to compute a closed form for the double series $$f(x,y,z)=\sum_{m,n=1}^\infty \frac{x^m y^n z^{m+n}}{m!(m+n)!}$$ and I noticed the fact that $$\frac{\partial^2f}{\partial x\partial z}-f=e^y-1$$ I know how to solve some somewhat advanced differential equations for functions of one variable, but I know nothing about how to solve partial differential equations. Can someone show me how to solve this one? I did have one little insight about this equation. Since no derivatives are taken with respect to $y$ on the LHS of the differential equation, I can just treat $e^y-1$ as if it was a constant and instead solve the differential equation as if $f$ were a function $f(x,z)$ of two variables only. But since I don't know how to get started, this doesn't help me.","I was trying to compute a closed form for the double series $$f(x,y,z)=\sum_{m,n=1}^\infty \frac{x^m y^n z^{m+n}}{m!(m+n)!}$$ and I noticed the fact that $$\frac{\partial^2f}{\partial x\partial z}-f=e^y-1$$ I know how to solve some somewhat advanced differential equations for functions of one variable, but I know nothing about how to solve partial differential equations. Can someone show me how to solve this one? I did have one little insight about this equation. Since no derivatives are taken with respect to $y$ on the LHS of the differential equation, I can just treat $e^y-1$ as if it was a constant and instead solve the differential equation as if $f$ were a function $f(x,z)$ of two variables only. But since I don't know how to get started, this doesn't help me.",,"['sequences-and-series', 'ordinary-differential-equations', 'partial-differential-equations', 'power-series']"
14,Disproving a misconception about Grandi's Sum,Disproving a misconception about Grandi's Sum,,"I understand most people do not like questions about Grandi's Sum, but I was hoping to ask more about the reasoning flaw behind it and ask if my proof against it makes sense, as I'm taking real analysis this year and was wondering if I'm using the principles I'm learning in the correct fashion. Essentially, the misconception I'm talking about is the fact that Grandi's sum, which is $1 - 1 + 1 - 1 + 1 - 1 + ... + (-1)^n + ... $, is equivalent to $\frac{1}{2}$. My proof against this claim is as follows, and is actually a proof showing the sum does not equal ANY number at all... Take the sequence $s_n = \sum\limits_{k=0}^{n} (-1)^k$. That is, $s_0 = 1, s_1 = 1-1 = 0, ...$ and so on. We can construct two subsequences from $s_n$: one of the even-indexed terms of $s_n$, and another of the odd-indexed terms of $s_n$: $\{s_{2k}\} = 1,1,1 ...$ $\{s_{2k+1}\} = 0, 0, 0 ...$ As such, we have: \begin{align*} \lim_{k \to \infty} s_{2k} &= 1 \\ \lim_{k \to \infty} s_{2k+1} &= 0 \\ \end{align*} (By the Subsequence Theorem) If the sequence $s_n$ is convergent, i.e. has a limit $L$, then all of its subsequences are convergent to $L$. However, we have two subsequences of $s_n$ converge to different limits, and so we conclude $s_n$ is not convergent, i.e. its limit does not exist. We note the following: $\lim_{n \to \infty} s_n = \sum\limits_{k=0}^{\infty} (-1)^k$ Since the limit of $s_n$ as $n \rightarrow \infty$ does not exist, the summation on the RHS does not converge to a value. As such, we cannot propose that $1 - 1 + 1 - 1 + ... + (-1)^n + ...$ is equal to a number $S$, as that implies the summation is convergent, which we have shown is untrue. Thus, the claim $1 - 1 + 1 - 1 + ... + (-1)^n + ... = \frac{1}{2}$ is invalid. I think one of the fatal flaws in the claim $1 - 1 + 1 - ... = \frac{1}{2}$ is assuming the convergence of the series in the first place, which I am trying to point out using some of the facts I am currently learning. Is this proof correct?","I understand most people do not like questions about Grandi's Sum, but I was hoping to ask more about the reasoning flaw behind it and ask if my proof against it makes sense, as I'm taking real analysis this year and was wondering if I'm using the principles I'm learning in the correct fashion. Essentially, the misconception I'm talking about is the fact that Grandi's sum, which is $1 - 1 + 1 - 1 + 1 - 1 + ... + (-1)^n + ... $, is equivalent to $\frac{1}{2}$. My proof against this claim is as follows, and is actually a proof showing the sum does not equal ANY number at all... Take the sequence $s_n = \sum\limits_{k=0}^{n} (-1)^k$. That is, $s_0 = 1, s_1 = 1-1 = 0, ...$ and so on. We can construct two subsequences from $s_n$: one of the even-indexed terms of $s_n$, and another of the odd-indexed terms of $s_n$: $\{s_{2k}\} = 1,1,1 ...$ $\{s_{2k+1}\} = 0, 0, 0 ...$ As such, we have: \begin{align*} \lim_{k \to \infty} s_{2k} &= 1 \\ \lim_{k \to \infty} s_{2k+1} &= 0 \\ \end{align*} (By the Subsequence Theorem) If the sequence $s_n$ is convergent, i.e. has a limit $L$, then all of its subsequences are convergent to $L$. However, we have two subsequences of $s_n$ converge to different limits, and so we conclude $s_n$ is not convergent, i.e. its limit does not exist. We note the following: $\lim_{n \to \infty} s_n = \sum\limits_{k=0}^{\infty} (-1)^k$ Since the limit of $s_n$ as $n \rightarrow \infty$ does not exist, the summation on the RHS does not converge to a value. As such, we cannot propose that $1 - 1 + 1 - 1 + ... + (-1)^n + ...$ is equal to a number $S$, as that implies the summation is convergent, which we have shown is untrue. Thus, the claim $1 - 1 + 1 - 1 + ... + (-1)^n + ... = \frac{1}{2}$ is invalid. I think one of the fatal flaws in the claim $1 - 1 + 1 - ... = \frac{1}{2}$ is assuming the convergence of the series in the first place, which I am trying to point out using some of the facts I am currently learning. Is this proof correct?",,"['real-analysis', 'sequences-and-series', 'proof-verification']"
15,The exact value of an infinite sum $e^{-n^2}$,The exact value of an infinite sum,e^{-n^2},Good morning! I'm having trouble with this problem... how can we find the exact value of $$\sum_{n=0}^\infty e^{-n^2}.$$ Thank you in advance to anyone who can help.,Good morning! I'm having trouble with this problem... how can we find the exact value of $$\sum_{n=0}^\infty e^{-n^2}.$$ Thank you in advance to anyone who can help.,,"['sequences-and-series', 'closed-form']"
16,Why do we need $\beta_1>0$ and $\alpha_n<\beta_n$ in this proof?,Why do we need  and  in this proof?,\beta_1>0 \alpha_n<\beta_n,"This question is about theorem 3.54 of Baby Rudin: Statement and proof of that theorem are given here: Theorem 3.54: Let $\sum a_n$ be a series of real numbers which converges, but not absolutely. Suppose    $$-\infty \leq \alpha \leq \beta \leq +\infty.$$   Then there exists a rearrangement $\sum a_n^\prime$ with partial sums $s_n^\prime$ such that    $$\lim_{n\to\infty}\inf s_n^\prime = \alpha, \ \ \ \mbox{ and } \ \ \ \lim_{n\to\infty}\sup s_n^\prime = \beta. \tag{24}$$ Proof: Let $$p_n = \frac{|a_n| + a_n}{2}, \ q_n = \frac{|a_n| - a_n}{2} \,\,\,\,\, (n = 1, 2, 3, \ldots). $$      Then $p_n - q_n = a_n$, $p_n + q_n = |a_n|$, $p_n \geq 0$, $q_n \geq 0$. The series $\sum p_n$, $\sum q_n$ must both diverge. For if both were convergent, then $$\sum \left( p_n + q_n \right) = \sum |a_n|$$ would converge, contrary to hypothesis. Since $$ \sum_{n=1}^N a_n = \sum_{n=1}^N \left( p_n - q_n \right) = \sum_{n=1}^N p_n - \sum_{n=1}^N q_n,$$ divergence of $\sum p_n$ or convergence of $\sum q_n$ (or vice versa) implies divergence of $\sum a_n$, again contrary to hypothesis. Now let $P_1, P_2, P_3, \ldots$ denote the non-negative terms of $\sum a_n$, in the order in which they occur, and let $Q_1, Q_2, Q_3, \ldots$ be the absolute values of the negative terms of $\sum a_n$, also in their original order. The series $\sum P_n$, $\sum Q_n$ differ from $\sum p_n$, $\sum q_n$ only by zero terms, and are therefore divergent. We shall construct sequences $\{m_n \}$, $\{k_n\}$, such that the series $$ P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} + P_{m_1 + 1} + \cdots + P_{m_2} \\- Q_{k_1 + 1} - \cdots - Q_{k_2} + \cdots, \tag{25}$$ which clearly is a rearrangement of $\sum a_n$, satisfies (24). Choose real-valued sequences $\{ \alpha_n \}$, $\{ \beta_n \}$ such that $\alpha_n \rightarrow \alpha$, $\beta_n \rightarrow \beta$, $\alpha_n < \beta_n$, $\beta_1 > 0$. Let $m_1$, $k_1$ be the smallest integers such that $$P_1 + \cdots + P_{m_1} > \beta_1,$$ $$P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} < \alpha_1;$$  let $m_2$, $k_2$ be the smallest integers such that $$P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} + P_{m_1 + 1} + \cdots + P_{m_2} > \beta_2,$$ $$P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} + P_{m_1 + 1} + \cdots + P_{m_2} - Q_{k_1 + 1} - \cdots - Q_{k_2} < \alpha_2;$$ and continue in this way. This is possible since $\sum P_n$, $\sum Q_n$ diverge. If $x_n$, $y_n$ denote the partial sums of (25) whose last terms are $P_{m_n}$, $-Q_{k_n}$, then $$ | x_n - \beta_n | \leq P_{m_n}, \ \ \ |y_n - \alpha_n | \leq Q_{k_n}. $$ Since $P_n \rightarrow 0$, $Q_n \rightarrow 0$ as $n \rightarrow \infty$, we see that $x_n \rightarrow \beta$, $y_n \rightarrow \alpha$. Finally, it is clear that no number less than $\alpha$ or greater than $\beta$ can be a subsequential limit of the partial sums of (25). From chatroom, I got to know that $\beta_1>0$ (in fact, we can make it $\beta_1\ge 0$) is needed to ensure $|x_n-\beta_1|\le P_{m_1}$, and by $\alpha_n<\beta_n$, we ensure that $\alpha_n<x_n$ because by construction, $\beta_n<x_n$ ensuring that we need to subtract some $Q$'s to make $y_n<\alpha_n$. My question is: What guarantees us that we need to add at least one $P$ to go from, e.g., $P_1+\ldots+P_{m_1}-Q_1-\ldots-Q_{k_1}<\alpha_1$ to $P_1+\ldots+P_{m_1}-Q_1-\ldots-Q_{k_1}+P_{m_1+1}+\ldots+P_{m_2}>\beta_2$? I mean $\beta_2$ can be so small that we don't need to add $P_{m_1+1}+\ldots+P_{m_2}$. And in this case, I think, expression $|x_2-\beta_2|\le P_{m_2}$ becomes meaningless. Also, another question: Can't we accomplish our goal to have $\alpha_n<x_n$ by just $\alpha_n=\beta_n$ instead of $\alpha_n<\beta_n$?","This question is about theorem 3.54 of Baby Rudin: Statement and proof of that theorem are given here: Theorem 3.54: Let $\sum a_n$ be a series of real numbers which converges, but not absolutely. Suppose    $$-\infty \leq \alpha \leq \beta \leq +\infty.$$   Then there exists a rearrangement $\sum a_n^\prime$ with partial sums $s_n^\prime$ such that    $$\lim_{n\to\infty}\inf s_n^\prime = \alpha, \ \ \ \mbox{ and } \ \ \ \lim_{n\to\infty}\sup s_n^\prime = \beta. \tag{24}$$ Proof: Let $$p_n = \frac{|a_n| + a_n}{2}, \ q_n = \frac{|a_n| - a_n}{2} \,\,\,\,\, (n = 1, 2, 3, \ldots). $$      Then $p_n - q_n = a_n$, $p_n + q_n = |a_n|$, $p_n \geq 0$, $q_n \geq 0$. The series $\sum p_n$, $\sum q_n$ must both diverge. For if both were convergent, then $$\sum \left( p_n + q_n \right) = \sum |a_n|$$ would converge, contrary to hypothesis. Since $$ \sum_{n=1}^N a_n = \sum_{n=1}^N \left( p_n - q_n \right) = \sum_{n=1}^N p_n - \sum_{n=1}^N q_n,$$ divergence of $\sum p_n$ or convergence of $\sum q_n$ (or vice versa) implies divergence of $\sum a_n$, again contrary to hypothesis. Now let $P_1, P_2, P_3, \ldots$ denote the non-negative terms of $\sum a_n$, in the order in which they occur, and let $Q_1, Q_2, Q_3, \ldots$ be the absolute values of the negative terms of $\sum a_n$, also in their original order. The series $\sum P_n$, $\sum Q_n$ differ from $\sum p_n$, $\sum q_n$ only by zero terms, and are therefore divergent. We shall construct sequences $\{m_n \}$, $\{k_n\}$, such that the series $$ P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} + P_{m_1 + 1} + \cdots + P_{m_2} \\- Q_{k_1 + 1} - \cdots - Q_{k_2} + \cdots, \tag{25}$$ which clearly is a rearrangement of $\sum a_n$, satisfies (24). Choose real-valued sequences $\{ \alpha_n \}$, $\{ \beta_n \}$ such that $\alpha_n \rightarrow \alpha$, $\beta_n \rightarrow \beta$, $\alpha_n < \beta_n$, $\beta_1 > 0$. Let $m_1$, $k_1$ be the smallest integers such that $$P_1 + \cdots + P_{m_1} > \beta_1,$$ $$P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} < \alpha_1;$$  let $m_2$, $k_2$ be the smallest integers such that $$P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} + P_{m_1 + 1} + \cdots + P_{m_2} > \beta_2,$$ $$P_1 + \cdots + P_{m_1} - Q_1 - \cdots - Q_{k_1} + P_{m_1 + 1} + \cdots + P_{m_2} - Q_{k_1 + 1} - \cdots - Q_{k_2} < \alpha_2;$$ and continue in this way. This is possible since $\sum P_n$, $\sum Q_n$ diverge. If $x_n$, $y_n$ denote the partial sums of (25) whose last terms are $P_{m_n}$, $-Q_{k_n}$, then $$ | x_n - \beta_n | \leq P_{m_n}, \ \ \ |y_n - \alpha_n | \leq Q_{k_n}. $$ Since $P_n \rightarrow 0$, $Q_n \rightarrow 0$ as $n \rightarrow \infty$, we see that $x_n \rightarrow \beta$, $y_n \rightarrow \alpha$. Finally, it is clear that no number less than $\alpha$ or greater than $\beta$ can be a subsequential limit of the partial sums of (25). From chatroom, I got to know that $\beta_1>0$ (in fact, we can make it $\beta_1\ge 0$) is needed to ensure $|x_n-\beta_1|\le P_{m_1}$, and by $\alpha_n<\beta_n$, we ensure that $\alpha_n<x_n$ because by construction, $\beta_n<x_n$ ensuring that we need to subtract some $Q$'s to make $y_n<\alpha_n$. My question is: What guarantees us that we need to add at least one $P$ to go from, e.g., $P_1+\ldots+P_{m_1}-Q_1-\ldots-Q_{k_1}<\alpha_1$ to $P_1+\ldots+P_{m_1}-Q_1-\ldots-Q_{k_1}+P_{m_1+1}+\ldots+P_{m_2}>\beta_2$? I mean $\beta_2$ can be so small that we don't need to add $P_{m_1+1}+\ldots+P_{m_2}$. And in this case, I think, expression $|x_2-\beta_2|\le P_{m_2}$ becomes meaningless. Also, another question: Can't we accomplish our goal to have $\alpha_n<x_n$ by just $\alpha_n=\beta_n$ instead of $\alpha_n<\beta_n$?",,"['real-analysis', 'sequences-and-series']"
17,Convergence of $(x_n)_n$ if $2018^2=x_{n+1}\sqrt[n+1]{x_1\cdot x_2\cdots x_n}$ $\forall~n$,Convergence of  if,(x_n)_n 2018^2=x_{n+1}\sqrt[n+1]{x_1\cdot x_2\cdots x_n} \forall~n,"I became interested in solving a question in a recently deleted post concerning the relation $$2018^2=x_{n+1}\sqrt[n+1]{x_1\cdot x_2\cdot\cdots\cdot x_n}$$ for $n\in\mathbb{N}$. The question was whether it converges and if so what the limit is. My thoughts : We have $$\begin{array}{c|c}n&x_{n+1}\\\hline1&2018^{2/1}x_1^{-1/2}\\2&2018^{4/3}x_1^{-1/6}\\3&2018^{7/6}x_1^{-1/12}\\...&...\end{array}$$ The pattern is that for $n\in\mathbb{N}$, $$x_{n+1}=2018^{1+[2/n(n-1)]}x_1^{-1/n(n-1)}.$$ Note that we suppose $x_1$ is fixed. This means that as $n\to\infty$, $$x_\infty=2018^{1+0}x_1^{0}=2018$$ so the sequence converges. Is this correct?","I became interested in solving a question in a recently deleted post concerning the relation $$2018^2=x_{n+1}\sqrt[n+1]{x_1\cdot x_2\cdot\cdots\cdot x_n}$$ for $n\in\mathbb{N}$. The question was whether it converges and if so what the limit is. My thoughts : We have $$\begin{array}{c|c}n&x_{n+1}\\\hline1&2018^{2/1}x_1^{-1/2}\\2&2018^{4/3}x_1^{-1/6}\\3&2018^{7/6}x_1^{-1/12}\\...&...\end{array}$$ The pattern is that for $n\in\mathbb{N}$, $$x_{n+1}=2018^{1+[2/n(n-1)]}x_1^{-1/n(n-1)}.$$ Note that we suppose $x_1$ is fixed. This means that as $n\to\infty$, $$x_\infty=2018^{1+0}x_1^{0}=2018$$ so the sequence converges. Is this correct?",,"['real-analysis', 'sequences-and-series', 'algebra-precalculus', 'limits']"
18,Does a closed form formula for the series $\sum\limits_{n = 1}^N {\frac{{1 + {x^{2n - 1}}}}{{1 + {x^{2n + 1}}}}}$ exist?,Does a closed form formula for the series  exist?,\sum\limits_{n = 1}^N {\frac{{1 + {x^{2n - 1}}}}{{1 + {x^{2n + 1}}}}},Does a closed form formula exist for the following series? $\sum\limits_{n = 1}^N {\frac{{1 + {x^{2n - 1}}}}{{1 + {x^{2n + 1}}}}}$,Does a closed form formula exist for the following series? $\sum\limits_{n = 1}^N {\frac{{1 + {x^{2n - 1}}}}{{1 + {x^{2n + 1}}}}}$,,"['sequences-and-series', 'closed-form']"
19,The even-index reciprocal Lucas constant and $\sum_{n=1}^\infty \frac1{x_1^{2n}+x_2^{2n}}$,The even-index reciprocal Lucas constant and,\sum_{n=1}^\infty \frac1{x_1^{2n}+x_2^{2n}},"The sum of reciprocals of even index Lucas numbers has a nice closed-form in terms of theta functions, $$\begin{aligned}S_e  &= \sum_{n=1}^\infty \frac1{L_{2n}}\\ &= \sum_{n=1}^\infty \frac1{\phi^{2n}+\phi^{-2n}}\\ &= \tfrac14\Big(\vartheta_3^2(\phi^{-2})-1\Big)\\ &= 0.56617\dots \end{aligned}$$ with golden ratio $\phi$ and Jacobi theta function $\vartheta_3(q)$. However, it seems this is just a special case. Given the root $x_1>0$ and $x_2<0$ of the quadratic, $$x^2+bx-1=0$$ Is it true that,   $$\sum_{n=1}^\infty \frac1{x_1^{2kn}+x_2^{2kn}} = \tfrac14\Big(\vartheta_3^2(x_1^{-2k})-1\Big) $$   for any integer $k>0$, and where the Lucas numbers was just $b=-1$?","The sum of reciprocals of even index Lucas numbers has a nice closed-form in terms of theta functions, $$\begin{aligned}S_e  &= \sum_{n=1}^\infty \frac1{L_{2n}}\\ &= \sum_{n=1}^\infty \frac1{\phi^{2n}+\phi^{-2n}}\\ &= \tfrac14\Big(\vartheta_3^2(\phi^{-2})-1\Big)\\ &= 0.56617\dots \end{aligned}$$ with golden ratio $\phi$ and Jacobi theta function $\vartheta_3(q)$. However, it seems this is just a special case. Given the root $x_1>0$ and $x_2<0$ of the quadratic, $$x^2+bx-1=0$$ Is it true that,   $$\sum_{n=1}^\infty \frac1{x_1^{2kn}+x_2^{2kn}} = \tfrac14\Big(\vartheta_3^2(x_1^{-2k})-1\Big) $$   for any integer $k>0$, and where the Lucas numbers was just $b=-1$?",,"['sequences-and-series', 'golden-ratio', 'theta-functions']"
20,a question on prime numbers and infinite series,a question on prime numbers and infinite series,,"prove that the infinite sum - $∑(1/2)^p$, where $p$ runs over all the $prime$ numbers,is $irrational$ one idea that may work is to use the given lemma $Lemma:$ $α$ is an irrational number iff there exists two convergent integer sequences ${a_n}$ and $b_n$ such that $(a_n-αb_n)≠0$ for all $n$. but $lim(a_n-αb_n)=0$ The proof is just by contradiction by assuming $α$ to be rational.I have tried to work out this lemma but failed .Plz try this.","prove that the infinite sum - $∑(1/2)^p$, where $p$ runs over all the $prime$ numbers,is $irrational$ one idea that may work is to use the given lemma $Lemma:$ $α$ is an irrational number iff there exists two convergent integer sequences ${a_n}$ and $b_n$ such that $(a_n-αb_n)≠0$ for all $n$. but $lim(a_n-αb_n)=0$ The proof is just by contradiction by assuming $α$ to be rational.I have tried to work out this lemma but failed .Plz try this.",,"['sequences-and-series', 'elementary-number-theory', 'prime-numbers', 'irrational-numbers']"
21,Find $S_1^2+S_2^2+S_3^2+...+S_{2n-1}^2$,Find,S_1^2+S_2^2+S_3^2+...+S_{2n-1}^2,"Question : If $S_1,S_2,S_3,...,S_n$ be the sums of $n$ infinite G.P. series respectively whose first terms are respectively $1,2,3,...,n$ and common ratios $\frac{1}{2},\frac{1}{3},\frac{1}{4},...,\frac{1}{n+1}$ respectively, then find the value of $S_1^2+S_2^2+S_3^2+...+S_{2n-1}^2$. My attempt : $S_k=\frac{k}{1-\frac{1}{k+1}}$ $\implies S_k=k+1$ $\implies S_k^2=(k+1)^2$ $\implies S=\sum\limits_{k=1}^{2n-1}S_k^2$ $\implies S=\sum\limits_{k=1}^{2n-1}(k+1)^2$ $\implies S=\sum\limits_{k=1}^{2n-1}(k^2+1+2k)$ $\implies S=\sum\limits_{k=1}^{2n-1}(k^2)+\sum\limits_{k=1}^{2n-1}(1)+\sum\limits_{k=1}^{2n-1}(2k)$ $\implies S=\frac{(2n-1)(2n)[2(2n-1)+1]}{6}\ \ \ \ \ \ \ +(2n-1)+\frac{2(2n-1)(2n)}{2}$ $\implies S=\frac{(4n^2-2n)(4n-1)}{6}\;\;\;+2n-1+4n^2-2n$ $\implies S=\frac{16n^3-4n^2-8n^2+2n}{6}\;\;+2n-1+4n^2-2n$ $\implies S=\frac{16n^3-4n^2-8n^2+2n-6+24n^2}{6}$ $\implies S=\frac{16n^3+12n^2+2n-6}{6}$ $\implies S=\frac{8n^3+6n^2+n-3}{3}$ My problem : The correct answer is $\frac{n(2n+1)(4n+1)}{3}$ which on simplification gives $\frac{8n^3+6n^2+n}{3}$. Where have i gone wrong? Please help.","Question : If $S_1,S_2,S_3,...,S_n$ be the sums of $n$ infinite G.P. series respectively whose first terms are respectively $1,2,3,...,n$ and common ratios $\frac{1}{2},\frac{1}{3},\frac{1}{4},...,\frac{1}{n+1}$ respectively, then find the value of $S_1^2+S_2^2+S_3^2+...+S_{2n-1}^2$. My attempt : $S_k=\frac{k}{1-\frac{1}{k+1}}$ $\implies S_k=k+1$ $\implies S_k^2=(k+1)^2$ $\implies S=\sum\limits_{k=1}^{2n-1}S_k^2$ $\implies S=\sum\limits_{k=1}^{2n-1}(k+1)^2$ $\implies S=\sum\limits_{k=1}^{2n-1}(k^2+1+2k)$ $\implies S=\sum\limits_{k=1}^{2n-1}(k^2)+\sum\limits_{k=1}^{2n-1}(1)+\sum\limits_{k=1}^{2n-1}(2k)$ $\implies S=\frac{(2n-1)(2n)[2(2n-1)+1]}{6}\ \ \ \ \ \ \ +(2n-1)+\frac{2(2n-1)(2n)}{2}$ $\implies S=\frac{(4n^2-2n)(4n-1)}{6}\;\;\;+2n-1+4n^2-2n$ $\implies S=\frac{16n^3-4n^2-8n^2+2n}{6}\;\;+2n-1+4n^2-2n$ $\implies S=\frac{16n^3-4n^2-8n^2+2n-6+24n^2}{6}$ $\implies S=\frac{16n^3+12n^2+2n-6}{6}$ $\implies S=\frac{8n^3+6n^2+n-3}{3}$ My problem : The correct answer is $\frac{n(2n+1)(4n+1)}{3}$ which on simplification gives $\frac{8n^3+6n^2+n}{3}$. Where have i gone wrong? Please help.",,"['sequences-and-series', 'summation', 'geometric-progressions']"
22,Sequence which changes its sign like this,Sequence which changes its sign like this,,"Which is the general formula of this sequence? $$ x_0 = -1$$ $$ x_{n+1} = ((-1)^n*X_n)/2^n$$ What baffles me more is the sign which is like this: $--++--++--++--++\cdots$ I've been wondering how the sign could change like that, any ideas?","Which is the general formula of this sequence? $$ x_0 = -1$$ $$ x_{n+1} = ((-1)^n*X_n)/2^n$$ What baffles me more is the sign which is like this: $--++--++--++--++\cdots$ I've been wondering how the sign could change like that, any ideas?",,['sequences-and-series']
23,Proof checking: subsequential convergence implies compactness,Proof checking: subsequential convergence implies compactness,,"I attempt to solve this problem: If $A$ is a subset of $\Bbb R^n$ and every sequence $\{p_n\}_{n=1}^{\infty}$ of points in $A$ has a subsequence converging to a point in $A$ , then $A$ is compact. An important theorem (Heine–Borel theorem) to support my proof: $A$ is a closed and bounded subset of $\Bbb R^n$ , $A$ is compact. I was thinking that since if $A$ is closed and bounded, $A$ would be compact so I am trying to prove that $A$ is closed and $A$ is bounded. Here is my proof: $1)$ Suppose $A$ is not bounded, satisfying the condition. Since $A$ is unbounded, there exists a sequence $\{p_i\}_{i=1}^{\infty}$ such that $|p_n|>n$ . Its subsequence does not converge on $\Bbb R^n.$ It contradicts to the condition given by the problem so $A$ is bounded. $2)$ Suppose $A$ is not closed, there exists a limit point $x$ of $A$ , $x \not \in A.$ Since $x$ is a limit point, $\forall n>0, \exists x_n \text{ s.t. } |x_n-x|<\frac{1}{n},x_n \in A, x_n \neq x$ . Then $\{x_n\}_{n=1}^{\infty}$ is a sequence of points. Since the subsequence of $\{x_n\}_{n=1}^{\infty}$ always converges to $x$ . $x \in A$ , which brings out an contradiction. So $A$ is closed. Since $A$ is closed and bounded, by Heine–Borel theorem, $A$ is compact. I am not sure my proof is right or not? Because my proof is different from the proof given by the link . If anything wrong, what is it? Is there any link between the proof given by the hyperlink and the one given by me?","I attempt to solve this problem: If is a subset of and every sequence of points in has a subsequence converging to a point in , then is compact. An important theorem (Heine–Borel theorem) to support my proof: is a closed and bounded subset of , is compact. I was thinking that since if is closed and bounded, would be compact so I am trying to prove that is closed and is bounded. Here is my proof: Suppose is not bounded, satisfying the condition. Since is unbounded, there exists a sequence such that . Its subsequence does not converge on It contradicts to the condition given by the problem so is bounded. Suppose is not closed, there exists a limit point of , Since is a limit point, . Then is a sequence of points. Since the subsequence of always converges to . , which brings out an contradiction. So is closed. Since is closed and bounded, by Heine–Borel theorem, is compact. I am not sure my proof is right or not? Because my proof is different from the proof given by the link . If anything wrong, what is it? Is there any link between the proof given by the hyperlink and the one given by me?","A \Bbb R^n \{p_n\}_{n=1}^{\infty} A A A A \Bbb R^n A A A A A 1) A A \{p_i\}_{i=1}^{\infty} |p_n|>n \Bbb R^n. A 2) A x A x \not \in A. x \forall n>0, \exists x_n \text{ s.t. } |x_n-x|<\frac{1}{n},x_n \in A, x_n \neq x \{x_n\}_{n=1}^{\infty} \{x_n\}_{n=1}^{\infty} x x \in A A A A","['real-analysis', 'sequences-and-series', 'general-topology', 'proof-verification', 'convergence-divergence']"
24,"Limit of recursive sequences: $x_{n+1}=\frac 1 3 (x_n+2y_n), y_{n+1}=\frac 1 3 \left( \frac 1 {x_n}+ \frac 1 {2y_n} \right).$",Limit of recursive sequences:,"x_{n+1}=\frac 1 3 (x_n+2y_n), y_{n+1}=\frac 1 3 \left( \frac 1 {x_n}+ \frac 1 {2y_n} \right).","Let $\left(\,x_{n}\,\right)_{\ n\ \geq\ 1}$ and $\left(\,y_{n}\,\right)_{\ n\ \geq\ 1}$ be two sequences defined as follows $\left(\,x_{1},y_{1} > 0\,\right)$: $$ \left\{\begin{array}{rcl} {\displaystyle x_{n + 1}} & {\displaystyle =} & {\displaystyle{1 \over 3}\left(\,x_{n} + 2y_{n}\,\right)} \\[2mm] {\displaystyle y_{n + 1}} & {\displaystyle =} & {\displaystyle{1 \over 3}\left(\,{1  \over x_{n}} + {1  \over 2y_n}\,\right)} \end{array}\right. $$ Show that the sequences are convergent and find their limits. MY TRY: I think about using the Weierstrass' Theorem ( every bounded and monotone sequence is convergent ), but I can't find the monotonity for any sequence. Afterwards, I can think of denoting the two limits as $L_{1}$ and $L_{2}$ and replacing them in the reccurence relationship.","Let $\left(\,x_{n}\,\right)_{\ n\ \geq\ 1}$ and $\left(\,y_{n}\,\right)_{\ n\ \geq\ 1}$ be two sequences defined as follows $\left(\,x_{1},y_{1} > 0\,\right)$: $$ \left\{\begin{array}{rcl} {\displaystyle x_{n + 1}} & {\displaystyle =} & {\displaystyle{1 \over 3}\left(\,x_{n} + 2y_{n}\,\right)} \\[2mm] {\displaystyle y_{n + 1}} & {\displaystyle =} & {\displaystyle{1 \over 3}\left(\,{1  \over x_{n}} + {1  \over 2y_n}\,\right)} \end{array}\right. $$ Show that the sequences are convergent and find their limits. MY TRY: I think about using the Weierstrass' Theorem ( every bounded and monotone sequence is convergent ), but I can't find the monotonity for any sequence. Afterwards, I can think of denoting the two limits as $L_{1}$ and $L_{2}$ and replacing them in the reccurence relationship.",,"['sequences-and-series', 'limits', 'convergence-divergence']"
25,Proving uniform convergence of $x^n-x^{2n}$,Proving uniform convergence of,x^n-x^{2n},I need help checking if the series of function {$x^n-x^{2n}$} on two cases: when $0\le x\le 1$ when $\frac{1}{3}\le x \le \frac{1}{2}$ I managed to prove that in the first case: the limit function is $f(x)=0$ so $r(x)=x^n-x^{2n}$ I have a maximum point when $x=\frac{1}{2}^{\frac{1}{n}}$ so $lim_{n\rightarrow \infty}r(x)=\frac{1}{4}\ne 0$ my question is why Dini's theorem does not apply on this case. for the second case: I don't have a maximum point so how do I prove it uniformly converges? any insight will be very helpful.,I need help checking if the series of function {$x^n-x^{2n}$} on two cases: when $0\le x\le 1$ when $\frac{1}{3}\le x \le \frac{1}{2}$ I managed to prove that in the first case: the limit function is $f(x)=0$ so $r(x)=x^n-x^{2n}$ I have a maximum point when $x=\frac{1}{2}^{\frac{1}{n}}$ so $lim_{n\rightarrow \infty}r(x)=\frac{1}{4}\ne 0$ my question is why Dini's theorem does not apply on this case. for the second case: I don't have a maximum point so how do I prove it uniformly converges? any insight will be very helpful.,,"['sequences-and-series', 'uniform-convergence']"
26,Notation for a sub-sub-sequence,Notation for a sub-sub-sequence,,"In Real Mathematical Analysis by Charles Pugh, sequences are denoted using the standard notation $(a_n)$, which represents $(a_1, a_2, \dots)$. Furthermore, sub-sequences are denoted by $(a_{n_k})$ which expands to $(a_{n_1}, a_{n_2},\dots)$ i.e. $n_k \in \mathbb{N}$ indicates the index (from the mother sequence) of the $k$-th term of the sub-sequence. These two notations are thus far standard. Finally, the confusion arises with the notation of sub-sub-sequences, which are denoted by $(a_{n_{k(l)}})$. The expansion seems to be on the $l$ variable, so that $(a_{n_{k(l)}}) = (a_{n_{k(1)}}, a_{n_{k(2)}}, \dots)$. If this expansion is correct, what is $n_{k(1)}$ supposed to indicate? Is $k$ now a function, where $n_{k(i)} \in \mathbb{N}$ denotes the index of the $i$-th term in the sub-sub-sequence (from the mother sequence, or grandmother sequence)? Is this notation standard?","In Real Mathematical Analysis by Charles Pugh, sequences are denoted using the standard notation $(a_n)$, which represents $(a_1, a_2, \dots)$. Furthermore, sub-sequences are denoted by $(a_{n_k})$ which expands to $(a_{n_1}, a_{n_2},\dots)$ i.e. $n_k \in \mathbb{N}$ indicates the index (from the mother sequence) of the $k$-th term of the sub-sequence. These two notations are thus far standard. Finally, the confusion arises with the notation of sub-sub-sequences, which are denoted by $(a_{n_{k(l)}})$. The expansion seems to be on the $l$ variable, so that $(a_{n_{k(l)}}) = (a_{n_{k(1)}}, a_{n_{k(2)}}, \dots)$. If this expansion is correct, what is $n_{k(1)}$ supposed to indicate? Is $k$ now a function, where $n_{k(i)} \in \mathbb{N}$ denotes the index of the $i$-th term in the sub-sub-sequence (from the mother sequence, or grandmother sequence)? Is this notation standard?",,"['real-analysis', 'sequences-and-series', 'notation']"
27,Determine for which $\alpha > 0$ the series $\sum_\limits{n=1}^{\infty}\frac{ne^n-\log(1+n)}{n}(\frac{1}{n^{\alpha}})$ converges,Determine for which  the series  converges,\alpha > 0 \sum_\limits{n=1}^{\infty}\frac{ne^n-\log(1+n)}{n}(\frac{1}{n^{\alpha}}),Determine for which $\alpha > 0$ the following series converges $$\sum_\limits{n=1}^{\infty}\frac{ne^n-\log(1+n)}{n}\frac{1}{n^{\alpha}}$$ My attempt: $$\sum_\limits{n=1}^{\infty}\dfrac{ne^n-\log(1+n)}{n}\dfrac{1}{n^{\alpha}}=\sum_\limits{n=1}^{\infty}\dfrac{ne^n-\log(1+n)}{n^{a+1}}$$ Then I separated the series: (Can I do this?) $$\sum_\limits{n=1}^{\infty}\dfrac{ne^n}{n^{a+1}}-\sum_\limits{n=1}^{\infty}\dfrac{\log(1+n)}{n^{a+1}}$$ So: I tried ratio test with the first one and it diverges $\forall \alpha>0$. The second one tends to $\sum_\limits{n=1}^{\infty}\dfrac{\log(n)}{n^{a+1}}$ which converges $\forall \alpha>0$. So the entire series diverges. Have I done something wrong?,Determine for which $\alpha > 0$ the following series converges $$\sum_\limits{n=1}^{\infty}\frac{ne^n-\log(1+n)}{n}\frac{1}{n^{\alpha}}$$ My attempt: $$\sum_\limits{n=1}^{\infty}\dfrac{ne^n-\log(1+n)}{n}\dfrac{1}{n^{\alpha}}=\sum_\limits{n=1}^{\infty}\dfrac{ne^n-\log(1+n)}{n^{a+1}}$$ Then I separated the series: (Can I do this?) $$\sum_\limits{n=1}^{\infty}\dfrac{ne^n}{n^{a+1}}-\sum_\limits{n=1}^{\infty}\dfrac{\log(1+n)}{n^{a+1}}$$ So: I tried ratio test with the first one and it diverges $\forall \alpha>0$. The second one tends to $\sum_\limits{n=1}^{\infty}\dfrac{\log(n)}{n^{a+1}}$ which converges $\forall \alpha>0$. So the entire series diverges. Have I done something wrong?,,['sequences-and-series']
28,Infinite sum with number system conversions,Infinite sum with number system conversions,,"Any number $x \in \mathbb{Z}$ can be written as $\pm\sum\limits_{i=0}^n x_i2^i$ where $x_i \in \text{{0,1}}$ (signed binary representation). It can also be written as $\sum\limits_{i=0}^n x_i(-2)^i$ where $x_i \in \text{{0,1}}$ (negabinary representation). We are given two functions $B(x)$ and $N(x)$, where $B(x)$ interprets the binary representation of the decimal input as a negabinary input, which it converts to binary and then to decimal and where $N(x)$ converts the binary representation of the decimal input to negabinary, which is then interpreted as binary and then as decimal. For example: $B(3_{10}) \rightarrow B(11_{-2}) = -1_{2}=-1_{10}$ and $N(2_{10}) = N(10_{2})=110_{-2} \rightarrow 110_2=6_{10}$ Now the following is true: $\sum\limits_{i=1}^{\infty} \frac{1}{B(2n+1)^2}=\frac{\pi^2}{4}$ because $B(2n+1)^2$ contains the square of all odd numbers twice and the sum of the recipricals of all odd numbers is $\sum\limits_{i=1}^{\infty} \frac{1}{(2n+1)^2}= \frac{\pi^2}{8}$ The question now is, what is $\sum\limits_{n=1}^{\infty} \frac{1}{N(n)^2}$ equal to?","Any number $x \in \mathbb{Z}$ can be written as $\pm\sum\limits_{i=0}^n x_i2^i$ where $x_i \in \text{{0,1}}$ (signed binary representation). It can also be written as $\sum\limits_{i=0}^n x_i(-2)^i$ where $x_i \in \text{{0,1}}$ (negabinary representation). We are given two functions $B(x)$ and $N(x)$, where $B(x)$ interprets the binary representation of the decimal input as a negabinary input, which it converts to binary and then to decimal and where $N(x)$ converts the binary representation of the decimal input to negabinary, which is then interpreted as binary and then as decimal. For example: $B(3_{10}) \rightarrow B(11_{-2}) = -1_{2}=-1_{10}$ and $N(2_{10}) = N(10_{2})=110_{-2} \rightarrow 110_2=6_{10}$ Now the following is true: $\sum\limits_{i=1}^{\infty} \frac{1}{B(2n+1)^2}=\frac{\pi^2}{4}$ because $B(2n+1)^2$ contains the square of all odd numbers twice and the sum of the recipricals of all odd numbers is $\sum\limits_{i=1}^{\infty} \frac{1}{(2n+1)^2}= \frac{\pi^2}{8}$ The question now is, what is $\sum\limits_{n=1}^{\infty} \frac{1}{N(n)^2}$ equal to?",,"['sequences-and-series', 'binary', 'number-systems']"
29,How do I prove this series diverges?,How do I prove this series diverges?,,"Consider a decreasing sequence $(x_n)$ in $\Bbb{R}_0$. There are an infinite amount of $n \in \Bbb{N}_0$ for which $1/n < x_n$. Prove the series $\sum x_n$ diverges. On one hand, I considered proving the sequence $(x_n)$ does not converge to $0$. However, unless I'm mistaken, this is not necessarily true. My next attempt was trying to show the series is not Cauchy. In other words: Find an $\epsilon > 0$ such that for every $n_0 \in \Bbb{N}_0$ an $m,n > n_0$ exists for which $\epsilon \le |\sum_{m}^{n} x_k|$. I figured I'd try to pick one of those $x_n > 1/n$, and a certain $N$ amount of preceding elements to reach the conclusion that $\epsilon \le \frac{N}{n} \le |\sum_{m}^{n} x_k|$. However at this point, I'm completely lost on how to prove these numbers $N$ and $n$ exist. Am I headed in the right direction? And if so, how do I finish the proof? Thanks in advance!","Consider a decreasing sequence $(x_n)$ in $\Bbb{R}_0$. There are an infinite amount of $n \in \Bbb{N}_0$ for which $1/n < x_n$. Prove the series $\sum x_n$ diverges. On one hand, I considered proving the sequence $(x_n)$ does not converge to $0$. However, unless I'm mistaken, this is not necessarily true. My next attempt was trying to show the series is not Cauchy. In other words: Find an $\epsilon > 0$ such that for every $n_0 \in \Bbb{N}_0$ an $m,n > n_0$ exists for which $\epsilon \le |\sum_{m}^{n} x_k|$. I figured I'd try to pick one of those $x_n > 1/n$, and a certain $N$ amount of preceding elements to reach the conclusion that $\epsilon \le \frac{N}{n} \le |\sum_{m}^{n} x_k|$. However at this point, I'm completely lost on how to prove these numbers $N$ and $n$ exist. Am I headed in the right direction? And if so, how do I finish the proof? Thanks in advance!",,"['sequences-and-series', 'divergent-series', 'cauchy-sequences']"
30,Prove that $\sum_{p\leq x}\log(1+\frac{1}{p})=\sum_{p\leq x}\frac{1}{p}+A+\mathcal O\left(\frac{1}{\log(x)}\right)$,Prove that,\sum_{p\leq x}\log(1+\frac{1}{p})=\sum_{p\leq x}\frac{1}{p}+A+\mathcal O\left(\frac{1}{\log(x)}\right),"How can I prove that $$\sum_{p\leq x}\log\left(1+\frac{1}{p}\right)=\sum_{p\leq x}\frac{1}{p}+A+\mathcal O\left(\frac{1}{\log(x)}\right).$$ The thing know is that $$\sum_{p\leq x}\left(\log\left(1+\frac{1}{p}\right)-\frac{1}{p}\right)=A+\mathcal o\left(1\right).$$ Can I conclude directly that the $o(1)$ is also a $\mathcal O\left(\frac{1}{\log(x)}\right) $ ? If no, how can I conclude ?","How can I prove that $$\sum_{p\leq x}\log\left(1+\frac{1}{p}\right)=\sum_{p\leq x}\frac{1}{p}+A+\mathcal O\left(\frac{1}{\log(x)}\right).$$ The thing know is that $$\sum_{p\leq x}\left(\log\left(1+\frac{1}{p}\right)-\frac{1}{p}\right)=A+\mathcal o\left(1\right).$$ Can I conclude directly that the $o(1)$ is also a $\mathcal O\left(\frac{1}{\log(x)}\right) $ ? If no, how can I conclude ?",,"['sequences-and-series', 'number-theory', 'prime-numbers', 'logarithms']"
31,How many fours are needed to represent numbers up to $N$?,How many fours are needed to represent numbers up to ?,N,"The goal of the four fours puzzle is to represent each natural number using four copies of the digit $4$ and common mathematical symbols. For example, $165=\left(\sqrt{4} + \sqrt{\sqrt{{\sqrt{4^{4!}}}}}\right) \div .4$. If we remove the restriction on the number of fours, let $f(N)$ be the number of fours required to be able to represent all positive integers no greater than $N$. What is the asymptotic behaviour of $f(N)$? Can it be shown that $f(N) \sim r \log N$ for some $r$? To be specific, let’s restrict the operations to the following: addition: $x+y$ subtraction: $x-y$ multiplication: $x\times y$ division: $x\div y$ exponentiation: $y^x$ roots: $\sqrt[x]{y}$ square root: $\sqrt{x}$ factorial $n!$ decimal point: $.4$ recurring decimal: $. \overline 4$ It is easy to see that $f(N)$ is $O(\log N)$. For example, with four fours, numbers up to $102$ can be represented (see here for a tool for generating solutions), so, since $96 = 4\times4!$, we can use $6k-2$ fours in the form $(\dots((a_1\times 96+a_2)\times 96+a_3)\dots)\times96+a_k$ to represent every number up to $96^k$. On the other hand, we can try to count the number of distinct expressions that can be made with $k$ fours. For example, if we (arbitrarily) permit factorial only to be applied to the digit $4$, and allow no more than two successive applications of the square root operation, we get $\frac{216^k}{18}C_{k-1}$ distinct expressions where $C_k$ is the $k$th Catalan number. (Of course, many of these expressions won’t represent a positive integer, many different expressions will represent the same number, and the positive integers generated won’t consist of a contiguous range from $1$ to some $N$.) Using Stirling’s formula, for large $k$, this is approximately $\frac{864^k}{72k\sqrt{\pi k}}$. So for $f(N)$ to grow slower than $r\log N$, we’d need to remove the restrictions on the use of unary operations. (It is well-known that the use of logs enables any number to be represented with only four fours.) Can this approach be extended to show that $f(N)$ is $\Omega(\log N)$? Or does unrestricted use of factorial and square roots    mean that $f(N)$ is actually $o(\log N)$?   Is the answer different if the use of $x\%$ (percentages) is also permitted?","The goal of the four fours puzzle is to represent each natural number using four copies of the digit $4$ and common mathematical symbols. For example, $165=\left(\sqrt{4} + \sqrt{\sqrt{{\sqrt{4^{4!}}}}}\right) \div .4$. If we remove the restriction on the number of fours, let $f(N)$ be the number of fours required to be able to represent all positive integers no greater than $N$. What is the asymptotic behaviour of $f(N)$? Can it be shown that $f(N) \sim r \log N$ for some $r$? To be specific, let’s restrict the operations to the following: addition: $x+y$ subtraction: $x-y$ multiplication: $x\times y$ division: $x\div y$ exponentiation: $y^x$ roots: $\sqrt[x]{y}$ square root: $\sqrt{x}$ factorial $n!$ decimal point: $.4$ recurring decimal: $. \overline 4$ It is easy to see that $f(N)$ is $O(\log N)$. For example, with four fours, numbers up to $102$ can be represented (see here for a tool for generating solutions), so, since $96 = 4\times4!$, we can use $6k-2$ fours in the form $(\dots((a_1\times 96+a_2)\times 96+a_3)\dots)\times96+a_k$ to represent every number up to $96^k$. On the other hand, we can try to count the number of distinct expressions that can be made with $k$ fours. For example, if we (arbitrarily) permit factorial only to be applied to the digit $4$, and allow no more than two successive applications of the square root operation, we get $\frac{216^k}{18}C_{k-1}$ distinct expressions where $C_k$ is the $k$th Catalan number. (Of course, many of these expressions won’t represent a positive integer, many different expressions will represent the same number, and the positive integers generated won’t consist of a contiguous range from $1$ to some $N$.) Using Stirling’s formula, for large $k$, this is approximately $\frac{864^k}{72k\sqrt{\pi k}}$. So for $f(N)$ to grow slower than $r\log N$, we’d need to remove the restrictions on the use of unary operations. (It is well-known that the use of logs enables any number to be represented with only four fours.) Can this approach be extended to show that $f(N)$ is $\Omega(\log N)$? Or does unrestricted use of factorial and square roots    mean that $f(N)$ is actually $o(\log N)$?   Is the answer different if the use of $x\%$ (percentages) is also permitted?",,"['combinatorics', 'discrete-mathematics', 'asymptotics', 'recreational-mathematics', 'puzzle']"
32,Why can a Venn diagram for $4+$ sets not be constructed using circles?,Why can a Venn diagram for  sets not be constructed using circles?,4+,"This page gives a few examples of Venn diagrams for $4$ sets. Some examples: Thinking about it for a little, it is impossible to partition the plane into the $16$ segments required for a complete $4$ -set Venn diagram using only circles as we could do for $<4$ sets. Yet it is doable with ellipses or rectangles, so we don't require non-convex shapes as Edwards uses. So what properties of a shape determine its suitability for $n$ -set Venn diagrams? Specifically, why are circles not good enough for the case $n=4$ ?","This page gives a few examples of Venn diagrams for sets. Some examples: Thinking about it for a little, it is impossible to partition the plane into the segments required for a complete -set Venn diagram using only circles as we could do for sets. Yet it is doable with ellipses or rectangles, so we don't require non-convex shapes as Edwards uses. So what properties of a shape determine its suitability for -set Venn diagrams? Specifically, why are circles not good enough for the case ?",4 16 4 <4 n n=4,"['combinatorics', 'geometry', 'logic', 'circles']"
33,There are apparently $3072$ ways to draw this flower. But why?,There are apparently  ways to draw this flower. But why?,3072,"This picture was in my friend's math book: Below the picture it says: There are $3072$ ways to draw this flower, starting from the center of   the petals, without lifting the pen. I know it's based on combinatorics, but I don't know how to show that there are actually $3072$ ways to do this. I'd be glad if someone showed how to show that there are exactly $3072$ ways to draw this flower, starting from the center of the petals, without lifting the pen (assuming that $3072$ is the correct amount).","This picture was in my friend's math book: Below the picture it says: There are $3072$ ways to draw this flower, starting from the center of   the petals, without lifting the pen. I know it's based on combinatorics, but I don't know how to show that there are actually $3072$ ways to do this. I'd be glad if someone showed how to show that there are exactly $3072$ ways to draw this flower, starting from the center of the petals, without lifting the pen (assuming that $3072$ is the correct amount).",,"['combinatorics', 'graph-theory', 'recreational-mathematics']"
34,Good Book On Combinatorics,Good Book On Combinatorics,,"What is your recommendation for an in-depth introductory combinatoric book? A book that doesn't just tell you about the multiplication principle, but rather shows the whole logic behind the questions with full proofs. The book should be for a first-year-student in college. Do you know a good book on the subject? Thanks.","What is your recommendation for an in-depth introductory combinatoric book? A book that doesn't just tell you about the multiplication principle, but rather shows the whole logic behind the questions with full proofs. The book should be for a first-year-student in college. Do you know a good book on the subject? Thanks.",,"['combinatorics', 'reference-request', 'soft-question', 'book-recommendation', 'big-list']"
35,Mondrian Art Problem Upper Bound for defect,Mondrian Art Problem Upper Bound for defect,,"Divide a square of side $n$ into any number of non-congruent rectangles. If all the sides are integers, what is the smallest possible difference in area between the largest and smallest rectangles? This is known as the Mondrian Art Problem .  For example, here's a division for a square of size $n=138$ .  The largest area is 1200, the smallest 1178, with a difference of 22. So far, corresponding sequence A276523 of known optimal Mondrian dissections have values 2, 4, 4, 5, 5, 6, 6, 8, 6, 7, 8, 6, 8, 8, 8, 8, 8, 9, 9, 9, 8, 9, 10, 9, 10, 9, 9, 11, 11, 10, 12, 12, 11, 12, 11, 10, 11, 12, 13, 12, 12. $\left \lceil{\frac n{\log(n)}}\right \rceil +3$ seems to be an upper bound for the Mondrian Art Problem. So far, the differences between that upper bound and the known optimal values comprise A278970 . Values to a(65) are verified, with best known values to a(100). x x 4 2 3 2 2 1 2 0 2 1 1 3 1 1 2 2 2 1 1 2 3 2 1 2 2 3 3 1 2 3 1 1 2 2 3 4 3 2 2 3 3 3 2 3 4 2 4 3 2 4 3 2 3 3 3 3 4 3 3 5 4 4 4 3 1 1 2 1 2 0 1 1 1 2 1 0 1 2 1 2 2 1 1 5 1 3 1 0 1 2 2 0 0 1 1 2 -2 1 From the current best-known solutions, next ""4"" values are at 176, 241, 245, 289. a(86)=a(139)=5, a(526)=6, a(280)=a(435)=7, a(700)=8, a(324)=a(811)=9, a(138)=a(832)=10. I have a ""14"" value for square 758, defect 104 (35964-32860) with 16 rectangles. Can anyone beat the upper bound by more than 14? The last ""hole"" in the first 96 squares was for the following, with best known value a(78)=0 $((\left \lceil{\frac {78}{\log(78)}}\right \rceil +3)-(390-369) = 0)$ . Under 105, the only a(99)=-2 has a best known solution going over the upper bound. Is there a better solution for a(99)?  I'm not sure why that simple upper bound seems to work so well for a problem with this much chaos. Is it valid? For eye candy, here are optimal rectangle dissections for squares 3 to 32 optimally packed into a rectangle ( method ): In a related problem, A279596 looks at the same problem where rectangles can be re-used if they have a different orientation. $\left \lceil{\frac n{\log(n)}}\right \rceil$ seems to be an upper bound. The distance from that bound is currently known as follows for the first 100 terms. The first 45 terms of A279848 are verified optimal by R. Gerbicz. x x 1 1 1 1 1 1 2 1 1 1 2 2 1 1 2 1 1 3    1 2 2 2 2 2 2 2 2 1 3 4 4 2 3 3 3 3 3 3    4 4 4 4 3 3 1 2 1 1 5 2 2 1 2 2 1 1 0 3    0 2 1 2 0 0 1 1 1 1 0 1 1 4 1 0 2 0 3 1    4 3 1 1 4 2 3 1 0 4 4 0 1 1 0 0 -2 1 -1 -1 A plot of best known values to a(96) follows. Here's the best known division of the size 51 square, into rectangles of area 160 to 168, for a defect of 8 (168-160). $\left \lceil{\frac {51}{\log(51)}}\right \rceil=13$ , so this division has a quality of 5 (13-8). From the current best-known solutions, next ""4"" values are at 74, 81, 85, 90, 91, 137, 150, 280, 435. a(151)=a(700)=5. a(324)=a(811)=6. a(138)=a(832)=7. a(103)=9. a(758)=11. Can anyone beat a quality value of 11? The best known values for squares 97, 99, 100 exceed the upper bound. Are those fixable? The last ""hole"" in the first 96 squares was for the following, with best known value a(83)=1 $(\left \lceil{\frac {83}{\log(83)}}\right \rceil -(468-450) = 1)$ . Is upper bound of $\left \lceil{\frac n{\log(n)}}\right \rceil$ valid for this variety of the Mondrian Art problem? Another packing problem with an odd upper bound is Oblongs into minimal squares .","Divide a square of side into any number of non-congruent rectangles. If all the sides are integers, what is the smallest possible difference in area between the largest and smallest rectangles? This is known as the Mondrian Art Problem .  For example, here's a division for a square of size .  The largest area is 1200, the smallest 1178, with a difference of 22. So far, corresponding sequence A276523 of known optimal Mondrian dissections have values 2, 4, 4, 5, 5, 6, 6, 8, 6, 7, 8, 6, 8, 8, 8, 8, 8, 9, 9, 9, 8, 9, 10, 9, 10, 9, 9, 11, 11, 10, 12, 12, 11, 12, 11, 10, 11, 12, 13, 12, 12. seems to be an upper bound for the Mondrian Art Problem. So far, the differences between that upper bound and the known optimal values comprise A278970 . Values to a(65) are verified, with best known values to a(100). x x 4 2 3 2 2 1 2 0 2 1 1 3 1 1 2 2 2 1 1 2 3 2 1 2 2 3 3 1 2 3 1 1 2 2 3 4 3 2 2 3 3 3 2 3 4 2 4 3 2 4 3 2 3 3 3 3 4 3 3 5 4 4 4 3 1 1 2 1 2 0 1 1 1 2 1 0 1 2 1 2 2 1 1 5 1 3 1 0 1 2 2 0 0 1 1 2 -2 1 From the current best-known solutions, next ""4"" values are at 176, 241, 245, 289. a(86)=a(139)=5, a(526)=6, a(280)=a(435)=7, a(700)=8, a(324)=a(811)=9, a(138)=a(832)=10. I have a ""14"" value for square 758, defect 104 (35964-32860) with 16 rectangles. Can anyone beat the upper bound by more than 14? The last ""hole"" in the first 96 squares was for the following, with best known value a(78)=0 . Under 105, the only a(99)=-2 has a best known solution going over the upper bound. Is there a better solution for a(99)?  I'm not sure why that simple upper bound seems to work so well for a problem with this much chaos. Is it valid? For eye candy, here are optimal rectangle dissections for squares 3 to 32 optimally packed into a rectangle ( method ): In a related problem, A279596 looks at the same problem where rectangles can be re-used if they have a different orientation. seems to be an upper bound. The distance from that bound is currently known as follows for the first 100 terms. The first 45 terms of A279848 are verified optimal by R. Gerbicz. x x 1 1 1 1 1 1 2 1 1 1 2 2 1 1 2 1 1 3    1 2 2 2 2 2 2 2 2 1 3 4 4 2 3 3 3 3 3 3    4 4 4 4 3 3 1 2 1 1 5 2 2 1 2 2 1 1 0 3    0 2 1 2 0 0 1 1 1 1 0 1 1 4 1 0 2 0 3 1    4 3 1 1 4 2 3 1 0 4 4 0 1 1 0 0 -2 1 -1 -1 A plot of best known values to a(96) follows. Here's the best known division of the size 51 square, into rectangles of area 160 to 168, for a defect of 8 (168-160). , so this division has a quality of 5 (13-8). From the current best-known solutions, next ""4"" values are at 74, 81, 85, 90, 91, 137, 150, 280, 435. a(151)=a(700)=5. a(324)=a(811)=6. a(138)=a(832)=7. a(103)=9. a(758)=11. Can anyone beat a quality value of 11? The best known values for squares 97, 99, 100 exceed the upper bound. Are those fixable? The last ""hole"" in the first 96 squares was for the following, with best known value a(83)=1 . Is upper bound of valid for this variety of the Mondrian Art problem? Another packing problem with an odd upper bound is Oblongs into minimal squares .",n n=138 \left \lceil{\frac n{\log(n)}}\right \rceil +3 ((\left \lceil{\frac {78}{\log(78)}}\right \rceil +3)-(390-369) = 0) \left \lceil{\frac n{\log(n)}}\right \rceil \left \lceil{\frac {51}{\log(51)}}\right \rceil=13 (\left \lceil{\frac {83}{\log(83)}}\right \rceil -(468-450) = 1) \left \lceil{\frac n{\log(n)}}\right \rceil,"['combinatorics', 'graph-theory', 'recreational-mathematics', 'packing-problem', 'oeis']"
36,"Is there a characterization of groups with the property $\forall N\unlhd G,\:\exists H\leq G\text{ s.t. }H\cong G/N$?",Is there a characterization of groups with the property ?,"\forall N\unlhd G,\:\exists H\leq G\text{ s.t. }H\cong G/N","A common mistake for beginning group theory students is the belief that a quotient of a group $G$ is necessarily isomorphic to a subgroup of $G$ .  Is there a characterization of the groups in which this property holds? If this question is too broad, I might ask if such a characterization exists for $p$ -groups. History : I originally posed the opposite question, regarding groups for which $\exists N\unlhd G\,:\, \not\exists H \unlhd G\, \text{  s.t. } H \cong G/N$ , and crossposted this to MO .  I received an answer there to the (now omitted) peripheral question about probability, which shows that most finite groups probably have this property.  After this, I changed the question to its current state, as this smaller collection of groups is more likely to be characterizable.","A common mistake for beginning group theory students is the belief that a quotient of a group is necessarily isomorphic to a subgroup of .  Is there a characterization of the groups in which this property holds? If this question is too broad, I might ask if such a characterization exists for -groups. History : I originally posed the opposite question, regarding groups for which , and crossposted this to MO .  I received an answer there to the (now omitted) peripheral question about probability, which shows that most finite groups probably have this property.  After this, I changed the question to its current state, as this smaller collection of groups is more likely to be characterizable.","G G p \exists N\unlhd G\,:\, \not\exists H \unlhd G\, \text{  s.t. } H \cong G/N","['combinatorics', 'group-theory', 'finite-groups', 'group-cohomology', 'p-groups']"
37,Making Friends around a Circular Table,Making Friends around a Circular Table,,"I have $n$ people seated around a circular table, initially in arbitrary order. At each step, I choose two people and switch their seats. What is the minimum number of steps required such that every person has sat either to the right or to the left of everyone else? To be specific, we consider two different cases: You can only switch people who are sitting next to each other. You can switch any two people, no matter where they are on the table. The small cases are relatively simple: if we denote the answer in case 1 and 2 for a given value of $n$ as $f(n)$ and $g(n)$ respectively, then we have $f(x)=g(x)=0$ for $x=1, 2, 3$, $f(4)=g(4)=1$. I'm not sure how I would generalize to larger values, though. (I initially claimed that $f(5)=g(5)=2$, but corrected it based on @Ryan's comment). If you're interested, this question came up in a conversation with my friends when we were trying to figure out the best way for a large party of people during dinner to all get to know each other. Edit: The table below compares the current best known value for case 2, $g(n)$, to the theoretical lower bound $\lceil{\frac{1}{8}n(n-3)}\rceil$ for a range of values of $n$. Solutions up to $n=14$ are known to be optimal, in large part due to the work of Andrew Szymczak and PeterKošinár. \begin{array} {|r|r|l|} \hline n & \text{Best known value of g(n)} & \left\lceil{\frac{1}{8}n(n-3)}\right\rceil & \text{Comments}\\ \hline 4 & 1 & 1 & \\ \hline 5 & 3 & 2 & \\ \hline 6 & 4 & 3 & \\ \hline 7 & 4 & 4 & \\ \hline 8 & 6 & 5 & \\ \hline 9 & 8 & 7 & \\ \hline 10 & 10 & 9 & \\ \hline 11 & 12 & 11 & \\ \hline 12 & 14 & 14 & \\ \hline 13 & 17 & 17 & \\ \hline 14 & 20 & 20 & \\ \hline 15 & 24 & 23 & \\ \hline 16 & 28 & 26 & \\ \hline 17 & 32 & 30 & \\ \hline 18 & 37 & 34 & \\ \hline 20 & 47 & 43 & \text{Loose upper bound}\\ \hline 25 & 77 & 69 & \text{Loose upper bound}\\ \hline 30 & 114 & 102 & \text{Loose upper bound}\\ \hline \end{array} The moves corresponding to the current best value are found below. Each ordered pair $(i, j)$ indicates that we switch the people in seats $(i, j)$ with each other, with the seats being labeled from $1 \ldots n$ consecutively around the table. 4  - ((2,1)) 5  - ((2,5),(1,5),(1,3)) 6  - ((5,3),(1,5),(2,5),(3,6)) 7  - ((4,7),(3,7),(1,5),(2,5)) 8  - ((1,2),(4,7),(1,5),(3,7),(1,6),(2,5)) (h.t. PeterKošinár) 9  - ((3,8),(1,4),(6,9),(4,8),(1,6),(5,8),(2,8),(2,9)) 10 - ((3,8),(4,8),(7,10),(1,7),(3,6),(1,5),(2,9),(3,7),(1,4),(3,9)) 11 - ((4,8),(2,9),(5,8),(1,7),(3,9),(7,11),(5,10),(1,4),(5,9),(2,7),(2,6),(5,10)) 12 - ((1,2),(5,10),(1,6),(4,10),(1,7),(8,11),(4,12),(3,12),(1,9),(1,5),(7,11) (1,8),(5,10),(2,6)) 13 - ((1,2),(1,7),(3,9),(6,12),(8,11),(8,12),(1,11),(4,12),(9,12),(6,10),(7,10),(1,6),(2,8),(5,9),(3,8),(8,12),(9,12)) 14 - ((1,4),(1,5),(1,8),(1,12),(4,11),(4,12),(9,13),(1,12),(9,12),(6,10),(6,9),(1,9),(4,7),(4,13),(3,13),(3,10),(2,13),(2,7),(3,13),(4,12)) 15 - ((0,3),(0,10),(4,7),(2,8),(1,8),(5,9),(3,14),(5,13),(2,11),(4,9),(5,14),(4,12),(2,6),(7,14),(0,3),(2,9),(6,10),(8,11),(0,12),(0,4),(0,7),(3,7),(3,10),(2,13)) 16 - ((10,14),(10,13),(0,10),(5,9),(5,8),(2,12),(2,5),(7,12),(2,12),(3,14),(5,11),(0,5),(4,14),(4,7),(3,11),(3,10),(0,8),(0,9),(0,6),(3,6),(1,14),(11,15),(1,5),(6,14),(3,11),(11,14),(0,12),(1,4)) 17 - ((9,15),(5,13),(13,16),(0,13),(2,10),(10,16),(5,16),(5,13),(2,6),(2,10),(10,16),(7,10),(4,15),(1,8),(4,9),(5,12),(4,10),(3,13),(5,14),(1,4),(5,15),(1,6),(5,12),(8,12),(7,12),(4,12),(0,12),(8,11),(8,14),(7,16),(2,3),(1,8)) 18 - ((4,7),(4,14),(6,10),(7,13),(4,7),(8,16),(8,13),(7,13),(3,8),(0,8),(4,8),(6,16),(1,12),(1,5),(5,11),(0,5),(14,17),(1,13),(8,13),(3,13),(0,4),(11,16),(2,10),(11,17),(9,15),(10,15),(1,9),(2,13),(1,4),(5,12),(6,14),(7,16),(13,17),(0,15),(1,15),(6,10),(5,15)) # Note that some solutions are zero-indexed and some are one-indexed. The code I used to generate my the results can be found on Github . Unless otherwise specified, the switches above were found by my code, using a randomized greedy approach. As demonstrated by PeterKošinár, since the total number of possibilities is large, this approach may not find the best result even after many trials.","I have $n$ people seated around a circular table, initially in arbitrary order. At each step, I choose two people and switch their seats. What is the minimum number of steps required such that every person has sat either to the right or to the left of everyone else? To be specific, we consider two different cases: You can only switch people who are sitting next to each other. You can switch any two people, no matter where they are on the table. The small cases are relatively simple: if we denote the answer in case 1 and 2 for a given value of $n$ as $f(n)$ and $g(n)$ respectively, then we have $f(x)=g(x)=0$ for $x=1, 2, 3$, $f(4)=g(4)=1$. I'm not sure how I would generalize to larger values, though. (I initially claimed that $f(5)=g(5)=2$, but corrected it based on @Ryan's comment). If you're interested, this question came up in a conversation with my friends when we were trying to figure out the best way for a large party of people during dinner to all get to know each other. Edit: The table below compares the current best known value for case 2, $g(n)$, to the theoretical lower bound $\lceil{\frac{1}{8}n(n-3)}\rceil$ for a range of values of $n$. Solutions up to $n=14$ are known to be optimal, in large part due to the work of Andrew Szymczak and PeterKošinár. \begin{array} {|r|r|l|} \hline n & \text{Best known value of g(n)} & \left\lceil{\frac{1}{8}n(n-3)}\right\rceil & \text{Comments}\\ \hline 4 & 1 & 1 & \\ \hline 5 & 3 & 2 & \\ \hline 6 & 4 & 3 & \\ \hline 7 & 4 & 4 & \\ \hline 8 & 6 & 5 & \\ \hline 9 & 8 & 7 & \\ \hline 10 & 10 & 9 & \\ \hline 11 & 12 & 11 & \\ \hline 12 & 14 & 14 & \\ \hline 13 & 17 & 17 & \\ \hline 14 & 20 & 20 & \\ \hline 15 & 24 & 23 & \\ \hline 16 & 28 & 26 & \\ \hline 17 & 32 & 30 & \\ \hline 18 & 37 & 34 & \\ \hline 20 & 47 & 43 & \text{Loose upper bound}\\ \hline 25 & 77 & 69 & \text{Loose upper bound}\\ \hline 30 & 114 & 102 & \text{Loose upper bound}\\ \hline \end{array} The moves corresponding to the current best value are found below. Each ordered pair $(i, j)$ indicates that we switch the people in seats $(i, j)$ with each other, with the seats being labeled from $1 \ldots n$ consecutively around the table. 4  - ((2,1)) 5  - ((2,5),(1,5),(1,3)) 6  - ((5,3),(1,5),(2,5),(3,6)) 7  - ((4,7),(3,7),(1,5),(2,5)) 8  - ((1,2),(4,7),(1,5),(3,7),(1,6),(2,5)) (h.t. PeterKošinár) 9  - ((3,8),(1,4),(6,9),(4,8),(1,6),(5,8),(2,8),(2,9)) 10 - ((3,8),(4,8),(7,10),(1,7),(3,6),(1,5),(2,9),(3,7),(1,4),(3,9)) 11 - ((4,8),(2,9),(5,8),(1,7),(3,9),(7,11),(5,10),(1,4),(5,9),(2,7),(2,6),(5,10)) 12 - ((1,2),(5,10),(1,6),(4,10),(1,7),(8,11),(4,12),(3,12),(1,9),(1,5),(7,11) (1,8),(5,10),(2,6)) 13 - ((1,2),(1,7),(3,9),(6,12),(8,11),(8,12),(1,11),(4,12),(9,12),(6,10),(7,10),(1,6),(2,8),(5,9),(3,8),(8,12),(9,12)) 14 - ((1,4),(1,5),(1,8),(1,12),(4,11),(4,12),(9,13),(1,12),(9,12),(6,10),(6,9),(1,9),(4,7),(4,13),(3,13),(3,10),(2,13),(2,7),(3,13),(4,12)) 15 - ((0,3),(0,10),(4,7),(2,8),(1,8),(5,9),(3,14),(5,13),(2,11),(4,9),(5,14),(4,12),(2,6),(7,14),(0,3),(2,9),(6,10),(8,11),(0,12),(0,4),(0,7),(3,7),(3,10),(2,13)) 16 - ((10,14),(10,13),(0,10),(5,9),(5,8),(2,12),(2,5),(7,12),(2,12),(3,14),(5,11),(0,5),(4,14),(4,7),(3,11),(3,10),(0,8),(0,9),(0,6),(3,6),(1,14),(11,15),(1,5),(6,14),(3,11),(11,14),(0,12),(1,4)) 17 - ((9,15),(5,13),(13,16),(0,13),(2,10),(10,16),(5,16),(5,13),(2,6),(2,10),(10,16),(7,10),(4,15),(1,8),(4,9),(5,12),(4,10),(3,13),(5,14),(1,4),(5,15),(1,6),(5,12),(8,12),(7,12),(4,12),(0,12),(8,11),(8,14),(7,16),(2,3),(1,8)) 18 - ((4,7),(4,14),(6,10),(7,13),(4,7),(8,16),(8,13),(7,13),(3,8),(0,8),(4,8),(6,16),(1,12),(1,5),(5,11),(0,5),(14,17),(1,13),(8,13),(3,13),(0,4),(11,16),(2,10),(11,17),(9,15),(10,15),(1,9),(2,13),(1,4),(5,12),(6,14),(7,16),(13,17),(0,15),(1,15),(6,10),(5,15)) # Note that some solutions are zero-indexed and some are one-indexed. The code I used to generate my the results can be found on Github . Unless otherwise specified, the switches above were found by my code, using a randomized greedy approach. As demonstrated by PeterKošinár, since the total number of possibilities is large, this approach may not find the best result even after many trials.",,"['combinatorics', 'optimization']"
38,"Help me put these enormous numbers in order:  googol, googol-plex-bang, googol-stack and so on","Help me put these enormous numbers in order:  googol, googol-plex-bang, googol-stack and so on",,"Popular mathematics folklore provides some simple tools enabling us compactly to describe some truly enormous numbers. For example, the number $10^{100}$ is commonly known as a googol , and a googol plex is $10^{10^{100}}$. For any number $x$, we have the common vernacular: $x$ bang is the factorial number $x!$ $x$ plex is the exponential number $10^x$ $x$ stack is the number obtained by iterated exponentiation  (associated upwards) in a tower of height $x$, also denoted $10\uparrow\uparrow x$, $$10\uparrow\uparrow x = 10^{10^{10^{\cdot^{\cdot^{10}}}}}{\large\rbrace} x\text{ times}.$$ Thus, a googol bang is $(10^{100})!$, and a googol stack is $10\uparrow\uparrow 10^{100}$. The vocabulary enables us to name larger numbers with ease: googol bang plex stack.  (This is the exponential tower $10^{10^{\cdot^{\cdot^{^{10}}}}}$ of height $10^{(10^{100})!}$) googol stack bang stack bang googol bang bang stack plex stack and so on… Consider the collection of all numbers that can be named in this scheme, by a term starting with googol and having finitely many adjectival operands: bang, stack, plex, in any finite pattern, repetitions allowed. (For the purposes of this question, let us limit ourselves to these three operations and please accept the base 10 presumption of the stack and plex terminology simply as an artifact of its origin in popular mathematics.) My goal is to sort all such numbers nameable in this vocabulary by size. A few simple observations get us started. Once $x$ is large enough (about 20), then the factors of $x!$ above $10$ compensate for the few below $10$, and so we see that $10^x\lt x!$, or in other words, $x$ plex is less than $x$ bang. Similarly, $10^{10^{:^{10}}}x$ times is much larger than $x!$, since $10^y\gt (y+1)y$ for large $y$, and so for large values we have $x$ plex $\lt$ $x$ bang $\lt$ $x$ stack. In particular, the order for names having at most one adjective is: googol  googol plex  googol bang  googol stack And more generally, replacing plex with bang or bang with stack in any of our names results in a strictly (and much) larger number. Continuing, since $x$ stack plex $= (x+1)$ stack, it follows that $x$ stack plex $\lt x$ plex stack. Similarly, for large values, $x$ plex bang $\lt x$ bang plex, because $(10^x)!\lt (10^x)^{10^x}=10^{x10^x}\lt 10^{x!}$. Also, $x$ stack bang $\lt x$ plex stack $\lt x$ bang stack, because $(10\uparrow\uparrow x)!\lt (10\uparrow\uparrow x)^{10\uparrow\uparrow x}\lt 10\uparrow\uparrow 2x\lt 10\uparrow\uparrow 10^x\lt 10\uparrow\uparrow x!$. It also appears to be true for large values that $x$ bang bang $\lt x$ stack. Indeed, one may subsume many more iterations of plex and bang into a single stack. Note also for large values that $x$ bang $\lt x$ plex plex since $x!\lt x^x$, and this is seen to be less than $10^{10^x}$ by taking logarithms. The observations above enable us to form the following order of all names using at most two adjectives. googol  googol plex  googol bang  googol plex plex  googol plex bang  googol bang plex  googol bang bang  googol stack  googol stack plex  googol stack bang  googol plex stack  googol bang stack  googol stack stack My request is for any or all of the following: Expand the list above to include numbers named using more than two adjectives. (This will not be an end-extension of the current list, since googol plex plex plex and googol bang bang bang will still appear before googol stack.) If people post partial progress, we can assemble them into a master list later. Provide general comparison criteria that will assist such an on-going effort. Provide a complete comparison algorithm that works for any two expressions having the same number of adjectives. Provide a complete comparison algorithm that compares any two expressions. Of course, there is in principle a computable comparison procedure, since we may program a Turing machine to actually compute the two values and compare their size. What is desired, however, is a simple, feasible algorithm. For example, it would seem that we could hope for an algorithm that would compare any two names in polynomial time of the length of the names.","Popular mathematics folklore provides some simple tools enabling us compactly to describe some truly enormous numbers. For example, the number $10^{100}$ is commonly known as a googol , and a googol plex is $10^{10^{100}}$. For any number $x$, we have the common vernacular: $x$ bang is the factorial number $x!$ $x$ plex is the exponential number $10^x$ $x$ stack is the number obtained by iterated exponentiation  (associated upwards) in a tower of height $x$, also denoted $10\uparrow\uparrow x$, $$10\uparrow\uparrow x = 10^{10^{10^{\cdot^{\cdot^{10}}}}}{\large\rbrace} x\text{ times}.$$ Thus, a googol bang is $(10^{100})!$, and a googol stack is $10\uparrow\uparrow 10^{100}$. The vocabulary enables us to name larger numbers with ease: googol bang plex stack.  (This is the exponential tower $10^{10^{\cdot^{\cdot^{^{10}}}}}$ of height $10^{(10^{100})!}$) googol stack bang stack bang googol bang bang stack plex stack and so on… Consider the collection of all numbers that can be named in this scheme, by a term starting with googol and having finitely many adjectival operands: bang, stack, plex, in any finite pattern, repetitions allowed. (For the purposes of this question, let us limit ourselves to these three operations and please accept the base 10 presumption of the stack and plex terminology simply as an artifact of its origin in popular mathematics.) My goal is to sort all such numbers nameable in this vocabulary by size. A few simple observations get us started. Once $x$ is large enough (about 20), then the factors of $x!$ above $10$ compensate for the few below $10$, and so we see that $10^x\lt x!$, or in other words, $x$ plex is less than $x$ bang. Similarly, $10^{10^{:^{10}}}x$ times is much larger than $x!$, since $10^y\gt (y+1)y$ for large $y$, and so for large values we have $x$ plex $\lt$ $x$ bang $\lt$ $x$ stack. In particular, the order for names having at most one adjective is: googol  googol plex  googol bang  googol stack And more generally, replacing plex with bang or bang with stack in any of our names results in a strictly (and much) larger number. Continuing, since $x$ stack plex $= (x+1)$ stack, it follows that $x$ stack plex $\lt x$ plex stack. Similarly, for large values, $x$ plex bang $\lt x$ bang plex, because $(10^x)!\lt (10^x)^{10^x}=10^{x10^x}\lt 10^{x!}$. Also, $x$ stack bang $\lt x$ plex stack $\lt x$ bang stack, because $(10\uparrow\uparrow x)!\lt (10\uparrow\uparrow x)^{10\uparrow\uparrow x}\lt 10\uparrow\uparrow 2x\lt 10\uparrow\uparrow 10^x\lt 10\uparrow\uparrow x!$. It also appears to be true for large values that $x$ bang bang $\lt x$ stack. Indeed, one may subsume many more iterations of plex and bang into a single stack. Note also for large values that $x$ bang $\lt x$ plex plex since $x!\lt x^x$, and this is seen to be less than $10^{10^x}$ by taking logarithms. The observations above enable us to form the following order of all names using at most two adjectives. googol  googol plex  googol bang  googol plex plex  googol plex bang  googol bang plex  googol bang bang  googol stack  googol stack plex  googol stack bang  googol plex stack  googol bang stack  googol stack stack My request is for any or all of the following: Expand the list above to include numbers named using more than two adjectives. (This will not be an end-extension of the current list, since googol plex plex plex and googol bang bang bang will still appear before googol stack.) If people post partial progress, we can assemble them into a master list later. Provide general comparison criteria that will assist such an on-going effort. Provide a complete comparison algorithm that works for any two expressions having the same number of adjectives. Provide a complete comparison algorithm that compares any two expressions. Of course, there is in principle a computable comparison procedure, since we may program a Turing machine to actually compute the two values and compare their size. What is desired, however, is a simple, feasible algorithm. For example, it would seem that we could hope for an algorithm that would compare any two names in polynomial time of the length of the names.",,"['combinatorics', 'elementary-number-theory', 'logic', 'big-numbers']"
39,Identity for convolution of central binomial coefficients: $\sum\limits_{k=0}^n \binom{2k}{k}\binom{2(n-k)}{n-k}=2^{2n}$,Identity for convolution of central binomial coefficients:,\sum\limits_{k=0}^n \binom{2k}{k}\binom{2(n-k)}{n-k}=2^{2n},"It's not difficult to show that $$(1-z^2)^{-1/2}=\sum_{n=0}^\infty \binom{2n}{n}2^{-2n}z^{2n}$$ On the other hand, we have $(1-z^2)^{-1}=\sum z^{2n}$.  Squaring the first power series and comparing terms gives us $$\sum_{k=0}^n \binom{2k}{k}\binom{2(n-k)}{n-k}2^{-2n}=1$$ that is, $$\sum_{k=0}^n \binom{2k}{k}\binom{2(n-k)}{n-k}=2^{2n}$$ My question: is there a more direct, combinatorial proof of this identity?  I've been racking my brains trying to come up with one but I'm not having much success.","It's not difficult to show that $$(1-z^2)^{-1/2}=\sum_{n=0}^\infty \binom{2n}{n}2^{-2n}z^{2n}$$ On the other hand, we have $(1-z^2)^{-1}=\sum z^{2n}$.  Squaring the first power series and comparing terms gives us $$\sum_{k=0}^n \binom{2k}{k}\binom{2(n-k)}{n-k}2^{-2n}=1$$ that is, $$\sum_{k=0}^n \binom{2k}{k}\binom{2(n-k)}{n-k}=2^{2n}$$ My question: is there a more direct, combinatorial proof of this identity?  I've been racking my brains trying to come up with one but I'm not having much success.",,"['combinatorics', 'summation', 'binomial-coefficients', 'convolution', 'combinatorial-proofs']"
40,Cutting sticks puzzle,Cutting sticks puzzle,,"This was asked on sci.math ages ago, and never got a satisfactory answer. Given a number of sticks of integral length  $ \ge n$ whose lengths   add to $n(n+1)/2$. Can these always be broken (by cuts) into sticks of   lengths $1,2,3, \ldots ,n$? You are not allowed to glue sticks back together. Assume you have an accurate measuring device. More formally, is the following conjecture true? (Taken from iwriteiam link below). Cutting Sticks Conjecture : For all natural numbers $n$, and any given sequence $a_1, .., a_k$ of   natural numbers greater or equal $n$ of which the sum equals   $n(n+1)/2$, there exists a partitioning $(P_1, .., P_k)$ of $\{1, .., n\}$   such that sum of the numbers in $P_i$ equals $a_i$, for all $1 \leq i \leq k$. Some links which discuss this problem: http://www.iwriteiam.nl/cutsticks.html","This was asked on sci.math ages ago, and never got a satisfactory answer. Given a number of sticks of integral length  $ \ge n$ whose lengths   add to $n(n+1)/2$. Can these always be broken (by cuts) into sticks of   lengths $1,2,3, \ldots ,n$? You are not allowed to glue sticks back together. Assume you have an accurate measuring device. More formally, is the following conjecture true? (Taken from iwriteiam link below). Cutting Sticks Conjecture : For all natural numbers $n$, and any given sequence $a_1, .., a_k$ of   natural numbers greater or equal $n$ of which the sum equals   $n(n+1)/2$, there exists a partitioning $(P_1, .., P_k)$ of $\{1, .., n\}$   such that sum of the numbers in $P_i$ equals $a_i$, for all $1 \leq i \leq k$. Some links which discuss this problem: http://www.iwriteiam.nl/cutsticks.html",,"['combinatorics', 'puzzle']"
41,$6!\cdot 7!=10!$. Is there a natural bijection between $S_6\times S_7$ and $S_{10}$?,. Is there a natural bijection between  and ?,6!\cdot 7!=10! S_6\times S_7 S_{10},"Aside from $1!\cdot n!=n!$ and $(n!-1)!\cdot n! = (n!)!$ , the only product of factorials known is $6!\cdot 7!=10!$ . One might naturally associate these numbers with the permutations on $6, 7,$ and $10$ objects, respectively, and hope that this result has some kind of connection to a sporadic relation between such permutations - numerical ""coincidences"" often have deep math behind them, like how $1^2+2^2+\ldots+24^2=70^2$ can be viewed as an ingredient that makes the Leech lattice work. The most natural thing to hope for would be a product structure on the groups $S_6$ and $S_7$ mapping to $S_{10}$ , but as this MathOverflow thread shows, one cannot find disjoint copies of $S_6$ and $S_7$ living in $S_{10}$ , so a product structure seems unlikely. However, I'm holding out hope that some weaker kind of bijection can be found in a ""natural"" way. Obviously one can exhibit a bijection. For instance, identify the relative ordering of $1,2,\ldots 7$ in a permutation of size $10$ , and then biject $_{10}P_{3}=720$ with $S_6$ in some way. But I'd like to know if there is a way to define such a bijection which arises naturally from the permutation structures on these sets, and makes it clear why the construction does not extend to other orders. I tried doing something with orderings on polar axes of the dodecahedron ( $10!$ ) and orderings on polar axes of the icosahedron ( $6!$ ), in the hopes that the sporadic structure and symmetry of these Platonic solids would allow for interesting constructions that don't generalize, but ran into issues with the dodecahedron (sequences of dodecahedral axes aren't particularly nice objects) and the question of how to extract a permutation of length $7$ . I'm curious if someone can either devise a natural bijection between these sets or link to previous work on this question.","Aside from and , the only product of factorials known is . One might naturally associate these numbers with the permutations on and objects, respectively, and hope that this result has some kind of connection to a sporadic relation between such permutations - numerical ""coincidences"" often have deep math behind them, like how can be viewed as an ingredient that makes the Leech lattice work. The most natural thing to hope for would be a product structure on the groups and mapping to , but as this MathOverflow thread shows, one cannot find disjoint copies of and living in , so a product structure seems unlikely. However, I'm holding out hope that some weaker kind of bijection can be found in a ""natural"" way. Obviously one can exhibit a bijection. For instance, identify the relative ordering of in a permutation of size , and then biject with in some way. But I'd like to know if there is a way to define such a bijection which arises naturally from the permutation structures on these sets, and makes it clear why the construction does not extend to other orders. I tried doing something with orderings on polar axes of the dodecahedron ( ) and orderings on polar axes of the icosahedron ( ), in the hopes that the sporadic structure and symmetry of these Platonic solids would allow for interesting constructions that don't generalize, but ran into issues with the dodecahedron (sequences of dodecahedral axes aren't particularly nice objects) and the question of how to extract a permutation of length . I'm curious if someone can either devise a natural bijection between these sets or link to previous work on this question.","1!\cdot n!=n! (n!-1)!\cdot n! = (n!)! 6!\cdot 7!=10! 6, 7, 10 1^2+2^2+\ldots+24^2=70^2 S_6 S_7 S_{10} S_6 S_7 S_{10} 1,2,\ldots 7 10 _{10}P_{3}=720 S_6 10! 6! 7",['combinatorics']
42,Combinatorial proof of summation of $\sum\limits_{k = 0}^n {n \choose k}^2= {2n \choose n}$,Combinatorial proof of summation of,\sum\limits_{k = 0}^n {n \choose k}^2= {2n \choose n},"I was hoping to find a more ""mathematical"" proof, instead of proving logically $\displaystyle \sum_{k = 0}^n {n \choose k}^2= {2n \choose n}$. I already know the logical Proof: $${n \choose k}^2 = {n \choose k}{ n \choose n-k}$$ Hence summation can be expressed as: $$\binom{n}{0}\binom{n}{n} + \binom{n}{1}\binom{n}{n-1} + \cdots + \binom{n}{n}\binom{n}{0}$$ One can think of it as choosing $n$ people from a group of $2n$  (imagine dividing a group of $2n$ into $2$ groups of $n$ people each. I can get $k$ people from group $1$ and another $n-k$ people from group $2$. We do this from $k = 0$ to $n$.","I was hoping to find a more ""mathematical"" proof, instead of proving logically $\displaystyle \sum_{k = 0}^n {n \choose k}^2= {2n \choose n}$. I already know the logical Proof: $${n \choose k}^2 = {n \choose k}{ n \choose n-k}$$ Hence summation can be expressed as: $$\binom{n}{0}\binom{n}{n} + \binom{n}{1}\binom{n}{n-1} + \cdots + \binom{n}{n}\binom{n}{0}$$ One can think of it as choosing $n$ people from a group of $2n$  (imagine dividing a group of $2n$ into $2$ groups of $n$ people each. I can get $k$ people from group $1$ and another $n-k$ people from group $2$. We do this from $k = 0$ to $n$.",,"['combinatorics', 'summation', 'binomial-coefficients', 'combinatorial-proofs']"
43,Can a row of five equilateral triangles tile a big equilateral triangle?,Can a row of five equilateral triangles tile a big equilateral triangle?,,"Can rotations and translations of this shape perfectly tile some equilateral triangle? I've now also asked this question on mathoverflow . Notes: Obviously I'm ignoring the triangle of side $0$ . Because the area of the triangle has to be a multiple of the area of the tile, the triangle must have side length divisible by $5$ (where $1$ is the length of the short edges of the tile). The analogous tile made of three equilateral triangles can tile any equilateral triangle with side length divisible by three. There is a computer program, Burr Tools , which was designed to solve this kind of problem. Josh B. has used it to prove by exhaustive search that there is no solution when the side length of the triangle is $5$ , $10$ , $15$ , $20$ or $25$ . Lengths of $30$ or more will take a very long time to check. This kind of problem can often be solved be a colouring argument but I've failed to find a suitable colouring. (See below.) Lee Mosher pointed me in the direction of Conway's theory of tiling groups . This theory can be used to show that if the tile can cover an equilateral triangle of side length $n$ then $a^nb^nc^n=e$ in the group $\left<a,b,c\;\middle|\;a^3ba^{-2}c=a^{-3}b^{-1}a^2c^{-1}=b^3cb^{-2}a=b^{-3}c^{-1}b^2a^{-1}=c^3ac^{-2}b=c^{-3}a^{-1}c^2b^{-1}=e\right>$ . But sadly it turns out that we do have that $a^nb^nc^n=e$ in this group whenever $n$ divides by $5$ . In fact one can use the methods in this paper of Michael Reid to prove that this tile's homotopy group is the cyclic group with $5$ elements. I think this means that the only thing these group theoretic methods can tell us is a fact we already knew: that the side length must be divisible by $5$ . These group theoretic methods are also supposed to subsume all possible colouring arguments, which means that any proof based purely on colouring is probably futile. The smallest area that can be left uncovered when trying to cover a triangle of side length $(1,\dots,20)$ is $($ $1$ $,\,$ $4$ $,\,$ $4$ $,\,$ $1$ $,\,$ $5$ $,\,$ $6$ $,\,$ $4$ $,\,$ $4$ $,\,$ $6$ $,\,$ $5$ $,\,$ $6$ $,\,$ $4$ $,\,$ $4$ $,\,$ $6$ $,\,$ $5$ $,\,$ $6$ $,\,$ $4$ $,\,$ $4$ $,\,$ $6$ $,\,$ $5$ $)$ small triangles. In particular it's surprising that when the area is $1\;\mathrm{mod}\;5$ one must sometimes leave six triangles uncovered rather than just one. We can look for ""near misses"" in which all but $5$ of the small triangles are covered and in which $4$ of the missing small triangles could be covered by the same tile. There's essentially only one near miss for the triangle of side $5$ , none for the triangle of side $10$ and six ( 1 , 2 , 3 , 4 , 5 , 6 ) for the triangle of side $15$ . (All other near misses can be generated from these by rotation, reflection, and by reorienting the three tiles that go around the lonesome missing triangle.) This set of six near misses are very interesting since the positions of the single triangle and the place where it ""should"" go are very constrained.","Can rotations and translations of this shape perfectly tile some equilateral triangle? I've now also asked this question on mathoverflow . Notes: Obviously I'm ignoring the triangle of side . Because the area of the triangle has to be a multiple of the area of the tile, the triangle must have side length divisible by (where is the length of the short edges of the tile). The analogous tile made of three equilateral triangles can tile any equilateral triangle with side length divisible by three. There is a computer program, Burr Tools , which was designed to solve this kind of problem. Josh B. has used it to prove by exhaustive search that there is no solution when the side length of the triangle is , , , or . Lengths of or more will take a very long time to check. This kind of problem can often be solved be a colouring argument but I've failed to find a suitable colouring. (See below.) Lee Mosher pointed me in the direction of Conway's theory of tiling groups . This theory can be used to show that if the tile can cover an equilateral triangle of side length then in the group . But sadly it turns out that we do have that in this group whenever divides by . In fact one can use the methods in this paper of Michael Reid to prove that this tile's homotopy group is the cyclic group with elements. I think this means that the only thing these group theoretic methods can tell us is a fact we already knew: that the side length must be divisible by . These group theoretic methods are also supposed to subsume all possible colouring arguments, which means that any proof based purely on colouring is probably futile. The smallest area that can be left uncovered when trying to cover a triangle of side length is small triangles. In particular it's surprising that when the area is one must sometimes leave six triangles uncovered rather than just one. We can look for ""near misses"" in which all but of the small triangles are covered and in which of the missing small triangles could be covered by the same tile. There's essentially only one near miss for the triangle of side , none for the triangle of side and six ( 1 , 2 , 3 , 4 , 5 , 6 ) for the triangle of side . (All other near misses can be generated from these by rotation, reflection, and by reorienting the three tiles that go around the lonesome missing triangle.) This set of six near misses are very interesting since the positions of the single triangle and the place where it ""should"" go are very constrained.","0 5 1 5 10 15 20 25 30 n a^nb^nc^n=e \left<a,b,c\;\middle|\;a^3ba^{-2}c=a^{-3}b^{-1}a^2c^{-1}=b^3cb^{-2}a=b^{-3}c^{-1}b^2a^{-1}=c^3ac^{-2}b=c^{-3}a^{-1}c^2b^{-1}=e\right> a^nb^nc^n=e n 5 5 5 (1,\dots,20) ( 1 ,\, 4 ,\, 4 ,\, 1 ,\, 5 ,\, 6 ,\, 4 ,\, 4 ,\, 6 ,\, 5 ,\, 6 ,\, 4 ,\, 4 ,\, 6 ,\, 5 ,\, 6 ,\, 4 ,\, 4 ,\, 6 ,\, 5 ) 1\;\mathrm{mod}\;5 5 4 5 10 15","['combinatorics', 'group-theory', 'geometry', 'discrete-geometry', 'tiling']"
44,Number of ways to write n as a sum of k nonnegative integers,Number of ways to write n as a sum of k nonnegative integers,,"How many ways can I write a positive integer $n$ as a sum of $k$ nonnegative integers up to commutativity? For example, I can write $4$ as $0+0+4$, $0+1+3$, $0+2+2$, and $1+1+2$. I know how to find the number of noncommutative ways to form the sum: Imagine a line of $n+k-1$ positions, where each position can contain either a cat or a divider. If you have $n$ (nameless) cats and $k-1$ dividers, you can split the cats in to $k$ groups by choosing positions for the dividers: $\binom{n+k-1}{k-1}$. The size of each group of cats corresponds to one of the nonnegative integers in the sum.","How many ways can I write a positive integer $n$ as a sum of $k$ nonnegative integers up to commutativity? For example, I can write $4$ as $0+0+4$, $0+1+3$, $0+2+2$, and $1+1+2$. I know how to find the number of noncommutative ways to form the sum: Imagine a line of $n+k-1$ positions, where each position can contain either a cat or a divider. If you have $n$ (nameless) cats and $k-1$ dividers, you can split the cats in to $k$ groups by choosing positions for the dividers: $\binom{n+k-1}{k-1}$. The size of each group of cats corresponds to one of the nonnegative integers in the sum.",,"['combinatorics', 'integer-partitions']"
45,"Determining answers to a true/false test by guessing optimally ($k2^{k-1}$ questions, $2^k$ attempts)","Determining answers to a true/false test by guessing optimally ( questions,  attempts)",k2^{k-1} 2^k,"A student has to pass a exam, with $k2^{k-1}$ questions to be answered by yes or no, on a subject he knows nothing about. The student is allowed to pass mock exams who have the same questions as the real exam. After each mock exam the teacher tells the student how many right answers he got, and when the student feels ready, he can pass the real exam. Show that if the student is good at combinatorics he can guess all the answers after only $2^{k}$ mock exams. For those who prefer a more formal presentation of the problem: if $E=\{1,-1\}^{N}$ we seek $n$ so that for some vectors $v_{1},v_{2}, \ldots, v_{n}\in E$ \begin{align*} \phi\colon E &\rightarrow \mathbb{N}^{n},\\ v &\mapsto (\langle v,v_{1}\rangle, \langle v,v_{2}\rangle, \ldots, \langle v,v_{n}\rangle) \end{align*} is injective. Show that it is possible to find such vectors when $N=k2^{k-1}$ and $n=2^{k}$. It is possible to use duality to transform again the problem. We seek a $n\times N$ matrix $M$ such that $v\mapsto Mv$ is injective over $E$. If $X$ is the formal polynomial vector $X=(X_{1},X_{2}, \ldots,X_{n})$ then $v\mapsto Mv$ is injective iff $v\mapsto \langle X,Mv \rangle$ also is. But $\langle X,Mv \rangle = \langle M^{T}X, v \rangle$ and it is easily seen that $v\mapsto \langle M^{T}X,v \rangle$ is injective iff the $N$ column vectors $x_{i}$ of $M$ are such that $\sum_{i\in I} x_{i}\neq \sum_{j\in J} x_{j}$ for any two different subsets $I$, $J$ of $\{1,\dots,N\}$. This problem comes from an olympiad-like contest. The original problem was formulated with 30 questions and the aim was to prove that the student could guess with 24 trials. One of my teachers came up with the result above (which would give 16 trials for 30 questions), but I can't remember his proof or find another one by myself. I tagged this information theory because some probabilistic arguments show that it is impossible to do better than this asymptotically. More precisely, if we choose the coordinates of the $v$ defined above randomly, then \begin{equation*} N =  H(\phi(v)) \leqslant  \sum_{i} H(\langle v,v_{i} \rangle) = nH(B(N,1/2)) \sim (n/2)\log_{2}(N) \end{equation*} where $H$ designates entropy. With $N=k2^{k-1}$ we get $n\geqslant c\frac{k2^{k}}{k-1+\log_{2}(k)}\geqslant c2^{k}$ for all $c<1$ and $k$ large enough.","A student has to pass a exam, with $k2^{k-1}$ questions to be answered by yes or no, on a subject he knows nothing about. The student is allowed to pass mock exams who have the same questions as the real exam. After each mock exam the teacher tells the student how many right answers he got, and when the student feels ready, he can pass the real exam. Show that if the student is good at combinatorics he can guess all the answers after only $2^{k}$ mock exams. For those who prefer a more formal presentation of the problem: if $E=\{1,-1\}^{N}$ we seek $n$ so that for some vectors $v_{1},v_{2}, \ldots, v_{n}\in E$ \begin{align*} \phi\colon E &\rightarrow \mathbb{N}^{n},\\ v &\mapsto (\langle v,v_{1}\rangle, \langle v,v_{2}\rangle, \ldots, \langle v,v_{n}\rangle) \end{align*} is injective. Show that it is possible to find such vectors when $N=k2^{k-1}$ and $n=2^{k}$. It is possible to use duality to transform again the problem. We seek a $n\times N$ matrix $M$ such that $v\mapsto Mv$ is injective over $E$. If $X$ is the formal polynomial vector $X=(X_{1},X_{2}, \ldots,X_{n})$ then $v\mapsto Mv$ is injective iff $v\mapsto \langle X,Mv \rangle$ also is. But $\langle X,Mv \rangle = \langle M^{T}X, v \rangle$ and it is easily seen that $v\mapsto \langle M^{T}X,v \rangle$ is injective iff the $N$ column vectors $x_{i}$ of $M$ are such that $\sum_{i\in I} x_{i}\neq \sum_{j\in J} x_{j}$ for any two different subsets $I$, $J$ of $\{1,\dots,N\}$. This problem comes from an olympiad-like contest. The original problem was formulated with 30 questions and the aim was to prove that the student could guess with 24 trials. One of my teachers came up with the result above (which would give 16 trials for 30 questions), but I can't remember his proof or find another one by myself. I tagged this information theory because some probabilistic arguments show that it is impossible to do better than this asymptotically. More precisely, if we choose the coordinates of the $v$ defined above randomly, then \begin{equation*} N =  H(\phi(v)) \leqslant  \sum_{i} H(\langle v,v_{i} \rangle) = nH(B(N,1/2)) \sim (n/2)\log_{2}(N) \end{equation*} where $H$ designates entropy. With $N=k2^{k-1}$ we get $n\geqslant c\frac{k2^{k}}{k-1+\log_{2}(k)}\geqslant c2^{k}$ for all $c<1$ and $k$ large enough.",,"['combinatorics', 'contest-math', 'linear-programming', 'information-theory', 'integer-programming']"
46,100 Soldiers riddle,100 Soldiers riddle,,"One of my friends found this riddle. There are 100 soldiers. 85 lose a left leg, 80 lose a right leg, 75 lose a left arm, 70 lose a right arm. What is the minimum number of soldiers losing all 4 limbs? We can't seem to agree on a way to approach this. Right off the bat I said that: 85 lost a left leg, 80 lost a right leg, 75 lost a left arm, 70 lost a right arm. 100 - 85 = 15 100 - 80 = 20 100 - 75 = 25 100 - 70 = 30 15 + 20 + 25 + 30 = 90 100 - 90 = 10 My friend doesn't agree with my answer as he says not all subsets were taken into consideration. I am unable to defend my answer as this was just the first, and most logical, answer that sprang to mind.","One of my friends found this riddle. There are 100 soldiers. 85 lose a left leg, 80 lose a right leg, 75 lose a left arm, 70 lose a right arm. What is the minimum number of soldiers losing all 4 limbs? We can't seem to agree on a way to approach this. Right off the bat I said that: 85 lost a left leg, 80 lost a right leg, 75 lost a left arm, 70 lost a right arm. 100 - 85 = 15 100 - 80 = 20 100 - 75 = 25 100 - 70 = 30 15 + 20 + 25 + 30 = 90 100 - 90 = 10 My friend doesn't agree with my answer as he says not all subsets were taken into consideration. I am unable to defend my answer as this was just the first, and most logical, answer that sprang to mind.",,"['combinatorics', 'pigeonhole-principle', 'puzzle', 'inclusion-exclusion']"
47,What is the proof that the total number of subsets of a set is $2^n$? [closed],What is the proof that the total number of subsets of a set is ? [closed],2^n,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question What is the proof that given a set of $n$ elements there are $2^n$ possible subsets (including the empty-set and the original set).","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question What is the proof that given a set of $n$ elements there are $2^n$ possible subsets (including the empty-set and the original set).",,"['combinatorics', 'elementary-set-theory', 'discrete-mathematics']"
48,Can Erdős-Turán $\frac{5}{8}$ theorem be generalised that way?,Can Erdős-Turán  theorem be generalised that way?,\frac{5}{8},"Suppose for an arbitrary group word $w$ ower the alphabet of $n$ symbols $\mathfrak{U_w}$ is a variety of all groups $G$ ,  that satisfy an identity $\forall a_1, … , a_n \in G$ $w(a_1, … , a_n) = e$ . Is it true, that for any group word $w$ there exists a positive real number $\epsilon (w) > 0$ , such that any finite group $G$ is in $\mathfrak{U_w}$ iff $$\frac{\lvert\{(a_1, … , a_n) \in G^n : w(a_1, … , a_n) = e\}\rvert}{{|G|}^n} > 1 - \epsilon(w)?$$ How did this question arise? There is a widely known theorem proved by P. Erdős and P. Turán that states: A finite group $G$ is abelian iff $$\frac{|\{(a, b) \in G^2 : [a, b] = e\}|}{{|G|}^2} > \frac{5}{8}.$$ This theorem can be rephrased using aforementioned terminology as $\epsilon([a, b]) = \frac{3}{8}$ . There also is a generalisation of this theorem, stating that a finite group $G$ is nilpotent of class $n$ iff $$\frac{|\{(a_0, a_1, … , a_n) \in G^{n + 1} : [ … [[a_0, a_1], a_2]… a_n] = e\}|}{{|G|}^{n + 1}} > 1 - \frac{3}{2^{n + 2}},$$ thus making $\epsilon([ … [[a_0, a_1], a_2]… a_n]) = \frac{3}{2^{n + 2}}$ . However, I have never seen similar statements about other one-word varieties being proved or disproved, despite such question seeming quite natural . . . Actually, I doubt that the conjecture in the main part of question is true. However, I failed to find any counterexamples myself.","Suppose for an arbitrary group word ower the alphabet of symbols is a variety of all groups ,  that satisfy an identity . Is it true, that for any group word there exists a positive real number , such that any finite group is in iff How did this question arise? There is a widely known theorem proved by P. Erdős and P. Turán that states: A finite group is abelian iff This theorem can be rephrased using aforementioned terminology as . There also is a generalisation of this theorem, stating that a finite group is nilpotent of class iff thus making . However, I have never seen similar statements about other one-word varieties being proved or disproved, despite such question seeming quite natural . . . Actually, I doubt that the conjecture in the main part of question is true. However, I failed to find any counterexamples myself.","w n \mathfrak{U_w} G \forall a_1, … , a_n \in G w(a_1, … , a_n) = e w \epsilon (w) > 0 G \mathfrak{U_w} \frac{\lvert\{(a_1, … , a_n) \in G^n : w(a_1, … , a_n) = e\}\rvert}{{|G|}^n} > 1 - \epsilon(w)? G \frac{|\{(a, b) \in G^2 : [a, b] = e\}|}{{|G|}^2} > \frac{5}{8}. \epsilon([a, b]) = \frac{3}{8} G n \frac{|\{(a_0, a_1, … , a_n) \in G^{n + 1} : [ … [[a_0, a_1], a_2]… a_n] = e\}|}{{|G|}^{n + 1}} > 1 - \frac{3}{2^{n + 2}}, \epsilon([ … [[a_0, a_1], a_2]… a_n]) = \frac{3}{2^{n + 2}}","['combinatorics', 'group-theory', 'finite-groups', 'conjectures', 'universal-algebra']"
49,What is the math behind the game Spot It?,What is the math behind the game Spot It?,,"I just purchased the game Spot It . As per this site , the structure of the game is as follows: Game has 55 round playing cards. Each card has eight randomly placed symbols. There are a total of 50 different symbols through the deck. The most fascinating feature of this game is any two cards selected will always have ONE (and only one) matching symbol to be found on both cards. Is there a formula you can use to create a derivative of this game with different numbers of symbols displayed on each card. Assuming the following variables: S = total number of symbols C = total number of cards N = number of symbols per card Can you mathematically demonstrate the minimum number of cards (C) and symbols (S) you need based on the number of symbols per card (N)?","I just purchased the game Spot It . As per this site , the structure of the game is as follows: Game has 55 round playing cards. Each card has eight randomly placed symbols. There are a total of 50 different symbols through the deck. The most fascinating feature of this game is any two cards selected will always have ONE (and only one) matching symbol to be found on both cards. Is there a formula you can use to create a derivative of this game with different numbers of symbols displayed on each card. Assuming the following variables: S = total number of symbols C = total number of cards N = number of symbols per card Can you mathematically demonstrate the minimum number of cards (C) and symbols (S) you need based on the number of symbols per card (N)?",,"['combinatorics', 'recreational-mathematics', 'card-games']"
50,Proof of the hockey stick/Zhu Shijie identity $\sum\limits_{t=0}^n \binom tk = \binom{n+1}{k+1}$,Proof of the hockey stick/Zhu Shijie identity,\sum\limits_{t=0}^n \binom tk = \binom{n+1}{k+1},"After reading this question , the most popular answer use the identity $$\sum_{t=0}^n \binom{t}{k} = \binom{n+1}{k+1},$$ or, what is equivalent, $$\sum_{t=k}^n \binom{t}{k} = \binom{n+1}{k+1}.$$ What's the name of this identity? Is it the identity of the Pascal's triangle modified. How can we prove it? I tried by induction, but without success. Can we also prove it algebraically? Thanks for your help. EDIT 01 : This identity is known as the hockey-stick identity because, on Pascal's triangle, when the addends represented in the summation and the sum itself are highlighted, a hockey-stick shape is revealed.","After reading this question , the most popular answer use the identity or, what is equivalent, What's the name of this identity? Is it the identity of the Pascal's triangle modified. How can we prove it? I tried by induction, but without success. Can we also prove it algebraically? Thanks for your help. EDIT 01 : This identity is known as the hockey-stick identity because, on Pascal's triangle, when the addends represented in the summation and the sum itself are highlighted, a hockey-stick shape is revealed.","\sum_{t=0}^n \binom{t}{k} = \binom{n+1}{k+1}, \sum_{t=k}^n \binom{t}{k} = \binom{n+1}{k+1}.","['combinatorics', 'summation', 'combinations', 'binomial-coefficients', 'faq']"
51,"Why is the Traveling Salesperson Problem ""Difficult""?","Why is the Traveling Salesperson Problem ""Difficult""?",,"The Traveling Salesperson Problem is originally a mathematics/computer science optimization problem in which the goal is to determine a path to take between a group of cities such that you return to the starting city after visiting each city exactly once and the total distance (longitude/latitude) traveled is minimized. For $n$ cities, there are $(n-1)!/2$ unique paths - and we can see that as $n$ increases, the number of paths to consider becomes enormous in size. For even a small number of cities (e.g. 15 cities), modern computers are unable to solve this problem using ""brute force"" (i.e. calculate all possible routes and return the shortest route) - as a result, sophisticated optimization algorithms and approximate methods are used to tackle this problem in real life. I was trying to explain this problem to my friend, and I couldn't think of an example which shows why the Travelling Salesperson Problem is difficult! Off the top of my head, I tried to give an example where someone is required to find the shortest route between Boston, Chicago and Los Angeles - but then I realized that the shortest path in this case is pretty obvious! (i.e. Move in the general East to West direction). Real world applications of the Travelling Salesperson Problem tend to have an additional layer of complexity as they generally have a ""cost"" associated between pairs of cities - and this cost doesn't have to be symmetric. For example, buses might be scheduled more frequently to go from a small city to a big city, but scheduled less frequently to return from the big city to the small city - thus, we might be able to associate a ""cost"" with each direction. Or even a simpler example, you might have to drive ""uphill"" to go from City A to City B, but drive ""downhill"" to go from City B to City A - thus there is likely a greater cost to go from City A to City B. Many times, these ""costs"" are not fully known and have to be approximated with some statistical model. However, all this can become a bit complicated to explain to someone who isn't familiar with all these terms. But I am still looking for an example to explain to my friend - can someone please help me think of an obvious and simple example of the Travelling Salesperson Problem where it becomes evidently clear that the choice of the shortest path is not obvious? Every simple example I try to think of tends to be very obvious (e.g. Manhattan, Newark, Nashville) - I don't want to overwhelm my friend with an example of 1000 cities across the USA : just something simple with 4-5 cities in which it is not immediately clear (and perhaps even counterintuitive) which path should be taken? I tried to show an example using the R programming language in which there are 10 (random) points on a grid - starting from the lowest point, the path taken involves choosing the nearest point from each current point: library(ggplot2)  set.seed(123)  x_cor = rnorm(5,100,100) y_cor = rnorm(5,100,100)   my_data = data.frame(x_cor,y_cor)        x_cor     y_cor 1  43.95244 271.50650 2  76.98225 146.09162 3 255.87083 -26.50612 4 107.05084  31.31471 5 112.92877  55.43380   ggplot(my_data, aes(x=x_cor, y=y_cor)) + geom_point() + ggtitle(""Travelling Salesperson Example"") But even in this example, the shortest path looks ""obvious"" (imagine you are required to start this problem from the bottom most right point): I tried with more points: set.seed(123)  x_cor = rnorm(20,100,100) y_cor = rnorm(20,100,100)   my_data = data.frame(x_cor,y_cor)  ggplot(my_data, aes(x = x_cor, y = y_cor)) +     geom_path() +     geom_point(size = 2) But my friend still argues that the ""find the nearest point from the current point and repeat"" (imagine you are required to start this problem from the bottom most right point): How do I convince my friend that what he is doing corresponds to a ""Greedy Search"" that is only returning a ""local minimum"" and it's very likely that a shorter path exists? (not even the ""shortest path"" - just a ""shorter path"" than the ""Greedy Search"") I tried to illustrate this example by linking him to the Wikipedia Page on Greedy Search that shows why Greedy Search can often miss the true minimum : https://en.wikipedia.org/wiki/Greedy_algorithm#/media/File:Greedy-search-path-example.gif Could someone help me think of an example to show my friend in which choosing the immediate nearest point from where you are, does not result in the total shortest path? (e.g. some example that appears counterintuitive, i.e. if you choose a path always based on the nearest point from your current position, you can clearly see that this is not the optimal path) Is there a mathematical proof that shows that the ""Greedy Search"" algorithm in Travelling Salesperson has the possibility of sometimes missing the true optimal path? Thanks!","The Traveling Salesperson Problem is originally a mathematics/computer science optimization problem in which the goal is to determine a path to take between a group of cities such that you return to the starting city after visiting each city exactly once and the total distance (longitude/latitude) traveled is minimized. For cities, there are unique paths - and we can see that as increases, the number of paths to consider becomes enormous in size. For even a small number of cities (e.g. 15 cities), modern computers are unable to solve this problem using ""brute force"" (i.e. calculate all possible routes and return the shortest route) - as a result, sophisticated optimization algorithms and approximate methods are used to tackle this problem in real life. I was trying to explain this problem to my friend, and I couldn't think of an example which shows why the Travelling Salesperson Problem is difficult! Off the top of my head, I tried to give an example where someone is required to find the shortest route between Boston, Chicago and Los Angeles - but then I realized that the shortest path in this case is pretty obvious! (i.e. Move in the general East to West direction). Real world applications of the Travelling Salesperson Problem tend to have an additional layer of complexity as they generally have a ""cost"" associated between pairs of cities - and this cost doesn't have to be symmetric. For example, buses might be scheduled more frequently to go from a small city to a big city, but scheduled less frequently to return from the big city to the small city - thus, we might be able to associate a ""cost"" with each direction. Or even a simpler example, you might have to drive ""uphill"" to go from City A to City B, but drive ""downhill"" to go from City B to City A - thus there is likely a greater cost to go from City A to City B. Many times, these ""costs"" are not fully known and have to be approximated with some statistical model. However, all this can become a bit complicated to explain to someone who isn't familiar with all these terms. But I am still looking for an example to explain to my friend - can someone please help me think of an obvious and simple example of the Travelling Salesperson Problem where it becomes evidently clear that the choice of the shortest path is not obvious? Every simple example I try to think of tends to be very obvious (e.g. Manhattan, Newark, Nashville) - I don't want to overwhelm my friend with an example of 1000 cities across the USA : just something simple with 4-5 cities in which it is not immediately clear (and perhaps even counterintuitive) which path should be taken? I tried to show an example using the R programming language in which there are 10 (random) points on a grid - starting from the lowest point, the path taken involves choosing the nearest point from each current point: library(ggplot2)  set.seed(123)  x_cor = rnorm(5,100,100) y_cor = rnorm(5,100,100)   my_data = data.frame(x_cor,y_cor)        x_cor     y_cor 1  43.95244 271.50650 2  76.98225 146.09162 3 255.87083 -26.50612 4 107.05084  31.31471 5 112.92877  55.43380   ggplot(my_data, aes(x=x_cor, y=y_cor)) + geom_point() + ggtitle(""Travelling Salesperson Example"") But even in this example, the shortest path looks ""obvious"" (imagine you are required to start this problem from the bottom most right point): I tried with more points: set.seed(123)  x_cor = rnorm(20,100,100) y_cor = rnorm(20,100,100)   my_data = data.frame(x_cor,y_cor)  ggplot(my_data, aes(x = x_cor, y = y_cor)) +     geom_path() +     geom_point(size = 2) But my friend still argues that the ""find the nearest point from the current point and repeat"" (imagine you are required to start this problem from the bottom most right point): How do I convince my friend that what he is doing corresponds to a ""Greedy Search"" that is only returning a ""local minimum"" and it's very likely that a shorter path exists? (not even the ""shortest path"" - just a ""shorter path"" than the ""Greedy Search"") I tried to illustrate this example by linking him to the Wikipedia Page on Greedy Search that shows why Greedy Search can often miss the true minimum : https://en.wikipedia.org/wiki/Greedy_algorithm#/media/File:Greedy-search-path-example.gif Could someone help me think of an example to show my friend in which choosing the immediate nearest point from where you are, does not result in the total shortest path? (e.g. some example that appears counterintuitive, i.e. if you choose a path always based on the nearest point from your current position, you can clearly see that this is not the optimal path) Is there a mathematical proof that shows that the ""Greedy Search"" algorithm in Travelling Salesperson has the possibility of sometimes missing the true optimal path? Thanks!",n (n-1)!/2 n,"['combinatorics', 'discrete-mathematics', 'graph-theory', 'optimization']"
52,Proof a graph is bipartite if and only if it contains no odd cycles,Proof a graph is bipartite if and only if it contains no odd cycles,,"How can we prove that a graph is bipartite if and only if all of its cycles have even order? Also, does this theorem have a common name? I found it in a maths Olympiad toolbox.","How can we prove that a graph is bipartite if and only if all of its cycles have even order? Also, does this theorem have a common name? I found it in a maths Olympiad toolbox.",,"['combinatorics', 'graph-theory', 'reference-request', 'bipartite-graphs']"
53,Combinatorial proof that $\sum \limits_{k=0}^n \binom{2k}{k} \binom{2n-2k}{n-k} (-1)^k = 2^n \binom{n}{n/2}$ when $n$ is even,Combinatorial proof that  when  is even,\sum \limits_{k=0}^n \binom{2k}{k} \binom{2n-2k}{n-k} (-1)^k = 2^n \binom{n}{n/2} n,"In my answer here I prove, using generating functions, a statement equivalent to  $$\sum_{k=0}^n \binom{2k}{k} \binom{2n-2k}{n-k} (-1)^k = 2^n \binom{n}{n/2}$$ when $n$ is even.  (Clearly the sum is $0$ when $n$ is odd.)  The nice expression on the right-hand side indicates that there should be a pretty combinatorial proof of this statement.  The proof should start by associating objects with even parity and objects with odd parity counted by the left-hand side.  The number of leftover (unassociated) objects should have even parity and should ""obviously"" be $2^n \binom{n}{n/2}$.  I'm having trouble finding such a proof, though.  So, my question is Can someone produce a combinatorial proof that, for even $n$, $$\sum_{k=0}^n \binom{2k}{k} \binom{2n-2k}{n-k} (-1)^k = 2^n \binom{n}{n/2}?$$ Some thoughts so far: Combinatorial proofs for $\sum_{k=0}^n \binom{2k}{k} \binom{2n-2k}{n-k}  = 4^n$ are given by Phira here and by Brian M. Scott here .  The proofs are basically equivalent.  In Phira's argument, both sides count the number of paths of length $2n$ starting from $(0,0)$ using steps of $(1,1)$ and $(1,-1)$.  By conditioning on the largest value of $2k$ for which a particular path returns to the horizontal axis at $(2k,0)$ and using the facts that there are $\binom{2k}{k}$ paths from $(0,0)$ to $(2k,0)$ and $\binom{2n-2k}{n-k}$ paths of length $2n-2k$ that start at the horizontal axis but never return to the axis we obtain the left-hand side. With these interpretations of the central binomial coefficients $2^n \binom{n}{n/2}$ could count (1) paths that do not return to the horizontal axis by the path's halfway point of $(n,0)$, or (2) paths that touch the point $(n,0)$.  But I haven't been able to construct the association that makes these the leftover paths (nor do all of these paths have even parity anyway).  So perhaps there's some other interpretation of $2^n \binom{n}{n/2}$ as the number of leftover paths. Update. Some more thoughts : There's another way to view the identity $\sum_{k=0}^n \binom{2k}{k} \binom{2n-2k}{n-k}  = 4^n$.  Both sides count the number of lattice paths of length $n$ when north, south, east, and west steps are allowed.  The right side is obvious. The left side has a similar interpretation as before: $\binom{2k}{k}$ counts the number of NSEW lattice paths of length $k$ that end on the line $y=0$, and $\binom{2n-2k}{n-k}$ counts the number of NSEW lattice paths of length $n-k$ that never return to the line $y =0$.  So far, this isn't much different as before.  However, $2^n \binom{n}{n/2}$ has an intriguing interpretation: It counts the number of NSEW lattice paths that end on the diagonal $y = x$ (or, equivalently, $y = -x$).  So maybe there's an involution that leaves these as the leftover paths.  (Proofs of all of these claims can be found on this blog post , for those who are interested.)","In my answer here I prove, using generating functions, a statement equivalent to  $$\sum_{k=0}^n \binom{2k}{k} \binom{2n-2k}{n-k} (-1)^k = 2^n \binom{n}{n/2}$$ when $n$ is even.  (Clearly the sum is $0$ when $n$ is odd.)  The nice expression on the right-hand side indicates that there should be a pretty combinatorial proof of this statement.  The proof should start by associating objects with even parity and objects with odd parity counted by the left-hand side.  The number of leftover (unassociated) objects should have even parity and should ""obviously"" be $2^n \binom{n}{n/2}$.  I'm having trouble finding such a proof, though.  So, my question is Can someone produce a combinatorial proof that, for even $n$, $$\sum_{k=0}^n \binom{2k}{k} \binom{2n-2k}{n-k} (-1)^k = 2^n \binom{n}{n/2}?$$ Some thoughts so far: Combinatorial proofs for $\sum_{k=0}^n \binom{2k}{k} \binom{2n-2k}{n-k}  = 4^n$ are given by Phira here and by Brian M. Scott here .  The proofs are basically equivalent.  In Phira's argument, both sides count the number of paths of length $2n$ starting from $(0,0)$ using steps of $(1,1)$ and $(1,-1)$.  By conditioning on the largest value of $2k$ for which a particular path returns to the horizontal axis at $(2k,0)$ and using the facts that there are $\binom{2k}{k}$ paths from $(0,0)$ to $(2k,0)$ and $\binom{2n-2k}{n-k}$ paths of length $2n-2k$ that start at the horizontal axis but never return to the axis we obtain the left-hand side. With these interpretations of the central binomial coefficients $2^n \binom{n}{n/2}$ could count (1) paths that do not return to the horizontal axis by the path's halfway point of $(n,0)$, or (2) paths that touch the point $(n,0)$.  But I haven't been able to construct the association that makes these the leftover paths (nor do all of these paths have even parity anyway).  So perhaps there's some other interpretation of $2^n \binom{n}{n/2}$ as the number of leftover paths. Update. Some more thoughts : There's another way to view the identity $\sum_{k=0}^n \binom{2k}{k} \binom{2n-2k}{n-k}  = 4^n$.  Both sides count the number of lattice paths of length $n$ when north, south, east, and west steps are allowed.  The right side is obvious. The left side has a similar interpretation as before: $\binom{2k}{k}$ counts the number of NSEW lattice paths of length $k$ that end on the line $y=0$, and $\binom{2n-2k}{n-k}$ counts the number of NSEW lattice paths of length $n-k$ that never return to the line $y =0$.  So far, this isn't much different as before.  However, $2^n \binom{n}{n/2}$ has an intriguing interpretation: It counts the number of NSEW lattice paths that end on the diagonal $y = x$ (or, equivalently, $y = -x$).  So maybe there's an involution that leaves these as the leftover paths.  (Proofs of all of these claims can be found on this blog post , for those who are interested.)",,"['combinatorics', 'binomial-coefficients', 'combinatorial-proofs']"
54,Can $18$ consecutive integers be separated into two groups such that their product is equal?,Can  consecutive integers be separated into two groups such that their product is equal?,18,"Can $18$ consecutive positive integers be separated into two groups such that their product is equal?  We cannot leave out any number and neither we can take any number more than once. My work: When the smallest number is not $17$ or its multiple, there cannot exist any such arrangement as $17$ is a prime. When the smallest number is a multiple of $17$ but not of $13$ or $11$ , then no such arrangement exists. But what happens, when the smallest number is a multiple of $ 17 $ and $13$ or $11$ or both? Please help!","Can consecutive positive integers be separated into two groups such that their product is equal?  We cannot leave out any number and neither we can take any number more than once. My work: When the smallest number is not or its multiple, there cannot exist any such arrangement as is a prime. When the smallest number is a multiple of but not of or , then no such arrangement exists. But what happens, when the smallest number is a multiple of and or or both? Please help!",18 17 17 17 13 11  17  13 11,"['combinatorics', 'elementary-number-theory']"
55,A comprehensive list of binomial identities?,A comprehensive list of binomial identities?,,"Is there a comprehensive resource listing binomial identities? I am more interested in combinatorial proofs of such identities, but even a list without proofs will do.","Is there a comprehensive resource listing binomial identities? I am more interested in combinatorial proofs of such identities, but even a list without proofs will do.",,"['combinatorics', 'reference-request']"
56,"In combinatorics, how can one verify that one has counted correctly?","In combinatorics, how can one verify that one has counted correctly?",,"This is a soft question, but I've tried to be specific about my concerns. When studying basic combinatorics, I was struck by the fact that it seems hard to verify if one has counted correctly. It's easiest to explain with an example, so I'll give one (it's fun!) and then pose my questions at the bottom. In this example there are two ways to count the number of ways $2n$ people can be placed into $n$ partnerships (from the book Introduction to Probability by Blitzstein and Hwang): $$\frac{(2n)!}{2^n \cdot n!} = (2n - 1)(2n - 3) \cdots 3 \cdot 1$$ Story proof for the right side: For the first person you have $(2n - 1)$ choices for partner. Then for the next person you have $(2n - 3)$ choices, etc. Story proof for the left side: Line all $2n$ people up, walk down the line, and select every 2 people as a pair. The ordering you chose for the line determines the pairs, and there are $2n!$ ways to order $2n$ people. But divide by $2^n$ because the order within each pairing doesn't matter. Also divide by $n!$ because the order of the pairings doesn't matter. My question is: What if I had been tasked with getting this number at work, and I chose the approach on the left side, but I neglected to divide by $2^n$ and only divided by $n!$? I'd be wrong of course, but how could I have known? I suppose one answer to my question could just be ""Try hard, try to think of ways you might be over/undercounting, look at other examples, and continue to study theory."" But the prospect of not knowing if I'm wrong makes me nervous. So I'm looking for more concrete things I can do. So, three questions of increasing rigor: What are specific ""habits of mind"" that people in combinatorics use to avoid error? What are specific validation techniques that such people use to check their work? (One does come to mind from my example: Calculate the same thing two ways and confirm equality) Is there any formal list of questions that, if answered, should verify that one's approach is correct?","This is a soft question, but I've tried to be specific about my concerns. When studying basic combinatorics, I was struck by the fact that it seems hard to verify if one has counted correctly. It's easiest to explain with an example, so I'll give one (it's fun!) and then pose my questions at the bottom. In this example there are two ways to count the number of ways $2n$ people can be placed into $n$ partnerships (from the book Introduction to Probability by Blitzstein and Hwang): $$\frac{(2n)!}{2^n \cdot n!} = (2n - 1)(2n - 3) \cdots 3 \cdot 1$$ Story proof for the right side: For the first person you have $(2n - 1)$ choices for partner. Then for the next person you have $(2n - 3)$ choices, etc. Story proof for the left side: Line all $2n$ people up, walk down the line, and select every 2 people as a pair. The ordering you chose for the line determines the pairs, and there are $2n!$ ways to order $2n$ people. But divide by $2^n$ because the order within each pairing doesn't matter. Also divide by $n!$ because the order of the pairings doesn't matter. My question is: What if I had been tasked with getting this number at work, and I chose the approach on the left side, but I neglected to divide by $2^n$ and only divided by $n!$? I'd be wrong of course, but how could I have known? I suppose one answer to my question could just be ""Try hard, try to think of ways you might be over/undercounting, look at other examples, and continue to study theory."" But the prospect of not knowing if I'm wrong makes me nervous. So I'm looking for more concrete things I can do. So, three questions of increasing rigor: What are specific ""habits of mind"" that people in combinatorics use to avoid error? What are specific validation techniques that such people use to check their work? (One does come to mind from my example: Calculate the same thing two ways and confirm equality) Is there any formal list of questions that, if answered, should verify that one's approach is correct?",,"['combinatorics', 'soft-question']"
57,7 fishermen caught exactly 100 fish and no two had caught the same number of fish. Then there are three who have together captured at least 50 fish.,7 fishermen caught exactly 100 fish and no two had caught the same number of fish. Then there are three who have together captured at least 50 fish.,,"$7$ fishermen caught exactly $100$ fish and no two had caught the same number of fish. Prove that there are three fishermen who have captured together at least $50$ fish. Try: Suppose $k$ th fisher caught $r_k$ fishes and that we have $$r_1<r_2<r_3<r_4<r_5<r_6<r_7$$ and let $r(ijk) := r_i+r_j+r_k$ . Now suppose $r(ijk)<49$ for all triples $\{i,j,k\}$ . Then we have $$r(123)<r(124)<r(125)<r(345)<r(367)<r(467)<r(567)\leq 49$$ so $$300\leq 3(r_1+\cdots+r_7)\leq 49+48+47+46+45+44+43= 322$$ and no contradiction. Any idea how to resolve this? Edit: Actually we have from $r(5,6,7)\leq 49$ that $r(4,6,7)\leq 48$ and $r(3,6,7)\leq 47$ and then $r(3,4,5)\leq r(3,6,7) - 4 \leq 43$ and $r(1,2,5)\leq r(3,4,5)-4\leq 39$ and $r(1,2,4)\leq 38$ and $r(1,2,3)\leq 37$ so we have: $$300\leq 49+48+47+43+39+38+37= 301$$ but again no contradiction.",fishermen caught exactly fish and no two had caught the same number of fish. Prove that there are three fishermen who have captured together at least fish. Try: Suppose th fisher caught fishes and that we have and let . Now suppose for all triples . Then we have so and no contradiction. Any idea how to resolve this? Edit: Actually we have from that and and then and and and so we have: but again no contradiction.,"7 100 50 k r_k r_1<r_2<r_3<r_4<r_5<r_6<r_7 r(ijk) := r_i+r_j+r_k r(ijk)<49 \{i,j,k\} r(123)<r(124)<r(125)<r(345)<r(367)<r(467)<r(567)\leq 49 300\leq 3(r_1+\cdots+r_7)\leq 49+48+47+46+45+44+43= 322 r(5,6,7)\leq 49 r(4,6,7)\leq 48 r(3,6,7)\leq 47 r(3,4,5)\leq r(3,6,7) - 4 \leq 43 r(1,2,5)\leq r(3,4,5)-4\leq 39 r(1,2,4)\leq 38 r(1,2,3)\leq 37 300\leq 49+48+47+43+39+38+37= 301","['combinatorics', 'discrete-mathematics', 'contest-math', 'pigeonhole-principle', 'discrete-optimization']"
58,Optimal strategy for cutting a sausage?,Optimal strategy for cutting a sausage?,,"You are a student, assigned to work in the cafeteria today, and it is your duty to divide the available food between all students. The food today is a sausage of 1m length, and you need to cut it into as many pieces as students come for lunch, including yourself. The problem is, the knife is operated by the rotating door through which the students enter, so every time a student comes in, the knife comes down and you place the cut. There is no way for you to know if more students will come or not, so after each cut, the sausage should be cut into pieces of approximately equal length. So here the question - is it possible to place the cuts in a manner to ensure the ratio of the largest and the smallest piece is always below 2? And if so, what is the smallest possible ratio? Example 1 (unit is cm): 1st cut: 50 : 50     ratio: 1 2nd cut: 50 : 25 : 25   ratio: 2 - bad Example 2 1st cut: 40 : 60              ratio: 1.5 2nd cut: 40 : 30 : 30            ratio: 1.33 3rd cut: 20 : 20 : 30 : 30    ratio: 1.5 4th cut: 20 : 20 : 30 : 15 : 15  ratio: 2 - bad Sorry for the awful analogy, I think this is a math problem but I have no real idea how to formulate this in a proper mathematical way.","You are a student, assigned to work in the cafeteria today, and it is your duty to divide the available food between all students. The food today is a sausage of 1m length, and you need to cut it into as many pieces as students come for lunch, including yourself. The problem is, the knife is operated by the rotating door through which the students enter, so every time a student comes in, the knife comes down and you place the cut. There is no way for you to know if more students will come or not, so after each cut, the sausage should be cut into pieces of approximately equal length. So here the question - is it possible to place the cuts in a manner to ensure the ratio of the largest and the smallest piece is always below 2? And if so, what is the smallest possible ratio? Example 1 (unit is cm): 1st cut: 50 : 50     ratio: 1 2nd cut: 50 : 25 : 25   ratio: 2 - bad Example 2 1st cut: 40 : 60              ratio: 1.5 2nd cut: 40 : 30 : 30            ratio: 1.33 3rd cut: 20 : 20 : 30 : 30    ratio: 1.5 4th cut: 20 : 20 : 30 : 15 : 15  ratio: 2 - bad Sorry for the awful analogy, I think this is a math problem but I have no real idea how to formulate this in a proper mathematical way.",,"['combinatorics', 'logic', 'recreational-mathematics', 'fair-division']"
59,Number of onto functions,Number of onto functions,,"What are the number of onto functions from a set $\Bbb A $ containing m elements to a set $\Bbb B$ containing n elements. I found that if $m = 4$ and $n = 2$ the number of onto functions is $14$ . But is there a way to generalise this using a formula? If yes, what is this formula and how is it derived? reference I referred to this question but my doubt was not cleared: How many one to one and onto functions are there between two finite sets? If not, Then what is the standard way of doing it? If you explain this to me with an example please explain with the example of $m = 5$ and $n = 3$ .","What are the number of onto functions from a set containing m elements to a set containing n elements. I found that if and the number of onto functions is . But is there a way to generalise this using a formula? If yes, what is this formula and how is it derived? reference I referred to this question but my doubt was not cleared: How many one to one and onto functions are there between two finite sets? If not, Then what is the standard way of doing it? If you explain this to me with an example please explain with the example of and .",\Bbb A  \Bbb B m = 4 n = 2 14 m = 5 n = 3,"['combinatorics', 'inclusion-exclusion']"
60,How to find a total order with constrained comparisons,How to find a total order with constrained comparisons,,"There are $25$ horses with different speeds. My goal is to rank all of them, by using only runs with $5$ horses, and taking partial rankings. How many runs do I need, at minimum, to complete my task? As a partial answer, I know that is possible to determine the first $3$ horses with $7$ runs, and, by a slight generalization of the optimal algorithm used to find the first three, have the complete ranking in $20$ runs. Is it possible to do better? What if we have $n$ horses and want to rank them with runs with $k$ horses?","There are horses with different speeds. My goal is to rank all of them, by using only runs with horses, and taking partial rankings. How many runs do I need, at minimum, to complete my task? As a partial answer, I know that is possible to determine the first horses with runs, and, by a slight generalization of the optimal algorithm used to find the first three, have the complete ranking in runs. Is it possible to do better? What if we have horses and want to rank them with runs with horses?",25 5 3 7 20 n k,"['combinatorics', 'order-theory']"
61,Why are asymptotically one half of the integer compositions gap-free?,Why are asymptotically one half of the integer compositions gap-free?,,"Question summary The number of gap-free compositions of $n$ can already for quite small $n$ be very well approximated by the total number of compositions of $n$ divided by $2$. This question seeks to understand why. The details A composition of an integer $n$ is a way of writing $n$ as the sum of a sequence of positive integers where order is important. This is different from partitions, where order is unimportant. It can easily be shown that the number of compositions of $n$ is $2^{n-1}$. See the Wikipedia article for a proof. A lot has been published about how the number of compositions changes when you put restrictions on them. One really interesting such restriction is ""gap-freeness"", i.e. containing every integer which is between the smallest and the largest integer in the composition. For example, $2+3+2+4+5$ is a gap-free composition of $16$, but $2+4+2+1+5+2$ is not, since there is a gap between $2$ and $4$. Furthermore, a composition is called ""complete"" if it is gap-free and contains the number $1$, i.e. all numbers from $1$ up to some integer $m$ are used in the summation. An example, the compositions of $n = 5$. g stands for gap-free and c for complete. 5           g 4+1 3+2         g 3+1+1 2+3         g 2+2+1       g, c 2+1+2       g, c 2+1+1+1     g, c 1+4 1+3+1 1+2+2       g, c 1+2+1+1     g, c 1+1+3 1+1+2+1     g, c 1+1+1+2     g, c 1+1+1+1+1   g, c So, the number of compositions of $n=5$ is $2^{5-1} = 16$, the number of gap-free compositions is $11$ and the number of complete compositions is $8$. Enumerating the number of compositions (#c), the number of gap-free compositions (#gc) and the number of complete compositions (#cc) for n from 1 to 10 gives the following table. n       #c      #gc     #cc 1       1       1       1 2       2       2       1 3       4       4       3 4       8       6       4 5       16      11      8 6       32      21      18 7       64      39      33 8       128     71      65 9       256     141     127 10      512     276     264 Continuations of these series can be found at oeis.org/A107428 and oeis.org/A107429 respectively. From this short sample, it can be conjectured that the number of complete compositions is asymptotically half of the total number of compositions. (This is also true for the gap-free compositions, since almost all of the gap-free compositions are complete, but it's not as obvious from this short sample.) A plot of the deviation between the percentage of gap-free compositions from $\frac12$ for $50 \leqslant n \leqslant 4000$ is quite remarkable. $50 < n < 500$ $500 < n < 4000$ Not only does it seem to converge quite rapidly, it also shows a strange oscillating behavior. A couple of proofs of this fact have been published $[1,2]$. The first paper was written by Hitczenko and Knopfmacher $[1]$, where they used randomly generated compositions to obtain a probability that a composition was complete (a probability that approaches $1/2$ as $n \to \infty$). The proof takes a probabilistic view of the compositions and says nothing (at least as far as I can understand) about why the ratio is exactly one half. However, the fact that asymptotically exactly half of the number of compositions are complete has lead me to think that there should be a way of thinking about this that is more intuitive or, perhaps, combinatorial in nature. For example, the number of complete compositions of $n$ is asymptotically the same as the total number of compositions of $n-1$. A bijection can of course not be found, since it is only an asymptotic relation. But for large $n$, is there some way to make a qualitative (and asymptotic) relation between these two sets of compositions? Are there other ""asymptotic bijections"" that can be used to shed some light on this? Maybe something can be said about how the completeness or gap-freeness restriction relates to other restrictions? I am not looking for rigorous proof here, it's just that it seems as there should be some way to think about this more intuitively. References $[1]$ P. Hitczenko and A. Knopfmacher, Gap-free compositions and gap-free samples of geometric random variables, Discrete Math. 294 (2005) 225-239 $[2]$ R. Warlimont, Complete compositions of a natural number, Quaestiones Mathematicae Volume 29, Issue 2, 2006 Edit July 1, 2013 No progress has been made here, but I thought I'd just write down some facts that might give some inspiration and food for thought. The number of complete partitions of $n$, $\# p_c(n)$, is the same as the number of partitions into distinct parts. The bijection can easily be seen using a Ferrers diagram. This is a very well-known series with the generating function $\prod_{k \ge 1} (1+x^k)$. ( A000009 at oeis.org ) The number of complete compositions of $n$, $\# c_c(n)$, can be calculated by counting the number of permutations of all complete partitions. In more detail: Generate a list of all complete partitions of $n$. For each partition $P$ in the list, check the maximum part $m$. For each number $k = \{1,2,\dots,m\}$, check the number of occurences $\alpha_k$ of $k$ in $P$. The number of complete compositions generated by $P$ is $\frac{m!} {\alpha_1! \dots \alpha_m!}$, i.e. the multinomial $\binom{m}{\alpha_1,\dots,\alpha_m}$ Can something be said about the asymptotic behavior of how the above operation maps $\# p_c(n)$ to $\# c_c(n)$? If so, $\# c_c(n) \sim f(n) \cdot \# p_c(n)$, where $f(n)$ is this asymptotic function. $\# p_c(n)$ is asymptotic to $$\frac{e^{\pi(n-\frac{1}{24})^{1/2}}}{4 \cdot 3^{1/4}(n-\frac{1}{24})^{3/4}}$$ (according to oeis.org ). We $\begin{cases} \text{know that} \\ \text{would like to prove that } \\ \text{would like to understand why} \end{cases} \Biggr\}$ the number of complete compositions of $n$ $\# c_c(n) \sim 2^{n-2}$. Edit July 9, 2013 Just a small note and a humble request. The number of compositions of $n$ starting with 1 is $2^{n-2}$. (Link) Are there other enumerations that have (asymptotically) the same dependence of $n$? Thought it might give some ideas/insight. Edit Feb 13, 2015 The generation of the number of gap-free and complete compositions can be done using recursive formulas published as Maple code on OEIS . Using memoization techniques, the sequences for $n$ up to $4000$ were generated. The oscillating behavior can be quite well modeled by the following function $$f(n)=\frac{An^B}{\log Cn}\sin\left(\frac{2 \pi}D \log(Dn+E)+F\right)$$ $$\begin{align} A &= 3.53292387 \cdot 10^{-4}\\ B &= -8.47099853 \cdot 10^{-1}\\ C &= 9.11885892 \cdot 10^{-3}\\ D &= 6.94057582 \cdot 10^{-1}\\ E &= 1.35920759 \cdot 10^1\\ F &= -1.15542736 \cdot 10^1\\ \end{align}$$","Question summary The number of gap-free compositions of $n$ can already for quite small $n$ be very well approximated by the total number of compositions of $n$ divided by $2$. This question seeks to understand why. The details A composition of an integer $n$ is a way of writing $n$ as the sum of a sequence of positive integers where order is important. This is different from partitions, where order is unimportant. It can easily be shown that the number of compositions of $n$ is $2^{n-1}$. See the Wikipedia article for a proof. A lot has been published about how the number of compositions changes when you put restrictions on them. One really interesting such restriction is ""gap-freeness"", i.e. containing every integer which is between the smallest and the largest integer in the composition. For example, $2+3+2+4+5$ is a gap-free composition of $16$, but $2+4+2+1+5+2$ is not, since there is a gap between $2$ and $4$. Furthermore, a composition is called ""complete"" if it is gap-free and contains the number $1$, i.e. all numbers from $1$ up to some integer $m$ are used in the summation. An example, the compositions of $n = 5$. g stands for gap-free and c for complete. 5           g 4+1 3+2         g 3+1+1 2+3         g 2+2+1       g, c 2+1+2       g, c 2+1+1+1     g, c 1+4 1+3+1 1+2+2       g, c 1+2+1+1     g, c 1+1+3 1+1+2+1     g, c 1+1+1+2     g, c 1+1+1+1+1   g, c So, the number of compositions of $n=5$ is $2^{5-1} = 16$, the number of gap-free compositions is $11$ and the number of complete compositions is $8$. Enumerating the number of compositions (#c), the number of gap-free compositions (#gc) and the number of complete compositions (#cc) for n from 1 to 10 gives the following table. n       #c      #gc     #cc 1       1       1       1 2       2       2       1 3       4       4       3 4       8       6       4 5       16      11      8 6       32      21      18 7       64      39      33 8       128     71      65 9       256     141     127 10      512     276     264 Continuations of these series can be found at oeis.org/A107428 and oeis.org/A107429 respectively. From this short sample, it can be conjectured that the number of complete compositions is asymptotically half of the total number of compositions. (This is also true for the gap-free compositions, since almost all of the gap-free compositions are complete, but it's not as obvious from this short sample.) A plot of the deviation between the percentage of gap-free compositions from $\frac12$ for $50 \leqslant n \leqslant 4000$ is quite remarkable. $50 < n < 500$ $500 < n < 4000$ Not only does it seem to converge quite rapidly, it also shows a strange oscillating behavior. A couple of proofs of this fact have been published $[1,2]$. The first paper was written by Hitczenko and Knopfmacher $[1]$, where they used randomly generated compositions to obtain a probability that a composition was complete (a probability that approaches $1/2$ as $n \to \infty$). The proof takes a probabilistic view of the compositions and says nothing (at least as far as I can understand) about why the ratio is exactly one half. However, the fact that asymptotically exactly half of the number of compositions are complete has lead me to think that there should be a way of thinking about this that is more intuitive or, perhaps, combinatorial in nature. For example, the number of complete compositions of $n$ is asymptotically the same as the total number of compositions of $n-1$. A bijection can of course not be found, since it is only an asymptotic relation. But for large $n$, is there some way to make a qualitative (and asymptotic) relation between these two sets of compositions? Are there other ""asymptotic bijections"" that can be used to shed some light on this? Maybe something can be said about how the completeness or gap-freeness restriction relates to other restrictions? I am not looking for rigorous proof here, it's just that it seems as there should be some way to think about this more intuitively. References $[1]$ P. Hitczenko and A. Knopfmacher, Gap-free compositions and gap-free samples of geometric random variables, Discrete Math. 294 (2005) 225-239 $[2]$ R. Warlimont, Complete compositions of a natural number, Quaestiones Mathematicae Volume 29, Issue 2, 2006 Edit July 1, 2013 No progress has been made here, but I thought I'd just write down some facts that might give some inspiration and food for thought. The number of complete partitions of $n$, $\# p_c(n)$, is the same as the number of partitions into distinct parts. The bijection can easily be seen using a Ferrers diagram. This is a very well-known series with the generating function $\prod_{k \ge 1} (1+x^k)$. ( A000009 at oeis.org ) The number of complete compositions of $n$, $\# c_c(n)$, can be calculated by counting the number of permutations of all complete partitions. In more detail: Generate a list of all complete partitions of $n$. For each partition $P$ in the list, check the maximum part $m$. For each number $k = \{1,2,\dots,m\}$, check the number of occurences $\alpha_k$ of $k$ in $P$. The number of complete compositions generated by $P$ is $\frac{m!} {\alpha_1! \dots \alpha_m!}$, i.e. the multinomial $\binom{m}{\alpha_1,\dots,\alpha_m}$ Can something be said about the asymptotic behavior of how the above operation maps $\# p_c(n)$ to $\# c_c(n)$? If so, $\# c_c(n) \sim f(n) \cdot \# p_c(n)$, where $f(n)$ is this asymptotic function. $\# p_c(n)$ is asymptotic to $$\frac{e^{\pi(n-\frac{1}{24})^{1/2}}}{4 \cdot 3^{1/4}(n-\frac{1}{24})^{3/4}}$$ (according to oeis.org ). We $\begin{cases} \text{know that} \\ \text{would like to prove that } \\ \text{would like to understand why} \end{cases} \Biggr\}$ the number of complete compositions of $n$ $\# c_c(n) \sim 2^{n-2}$. Edit July 9, 2013 Just a small note and a humble request. The number of compositions of $n$ starting with 1 is $2^{n-2}$. (Link) Are there other enumerations that have (asymptotically) the same dependence of $n$? Thought it might give some ideas/insight. Edit Feb 13, 2015 The generation of the number of gap-free and complete compositions can be done using recursive formulas published as Maple code on OEIS . Using memoization techniques, the sequences for $n$ up to $4000$ were generated. The oscillating behavior can be quite well modeled by the following function $$f(n)=\frac{An^B}{\log Cn}\sin\left(\frac{2 \pi}D \log(Dn+E)+F\right)$$ $$\begin{align} A &= 3.53292387 \cdot 10^{-4}\\ B &= -8.47099853 \cdot 10^{-1}\\ C &= 9.11885892 \cdot 10^{-3}\\ D &= 6.94057582 \cdot 10^{-1}\\ E &= 1.35920759 \cdot 10^1\\ F &= -1.15542736 \cdot 10^1\\ \end{align}$$",,"['combinatorics', 'discrete-mathematics', 'asymptotics', 'integer-partitions']"
62,How many 7-note musical scales are possible within the 12-note system?,How many 7-note musical scales are possible within the 12-note system?,,"This combinatorial question has a musical motivation, which I provide below using as little musical jargon as I can. But first, I'll present a purely mathematical formulation for those not interested in the motivation: Define a signature as a 7-tuple over the set $\{1,2,3\}$ such that the sum of the elements in the tuple is $12$. Two signatures are said to be equivalent if they are either identical or there is a circular shift relation between them (i.e. one can be circularly shifted between 1 and 6 times to obtain the other). How many unique signatures are there? Most modern, Western music is based on the equal temperament system , which divides the octave logarithmically equally into 12 notes . Let's refer to 2 adjacent notes from these 12 as being ""1 step"" apart, two notes with one skipped note in between them as being ""2 steps"" apart, and so on. The (arguably) most natural scale is the major scale, which uses 7 notes from these 12, and has the following signature : $$\text{major signature}=(2,2,1,2,2,2,1)$$ This means that we can construct a major scale as follows: given any note to start from, the second note is 2 steps away from first note, the third note is 1 step away from the 2nd note, and so on according to the above signature. Now this signature really has not just one, but seven scales embedded in it. This is because we can circularly shift the signature, effectively meaning that we are picking a different degree of the major scale to serve as our home note (these seven 'permutations' are called the modes of the signature). Thus, for example, $(1,2,2,2,1,2,2)$ is not a new signature, but just the major signature circularly shifted left twice (and is called the Phrygian mode). My question is: how many unique 7-note signatures are there under the restriction that any signature must not contain an interval greater than 3 steps (this is to respect the fact that any 7-note scale in common use uses only 1, 2 and 3 step intervals, to the best of my knowledge). Some signatures in common use are: $$\text{harmonic minor signature} = (2,1,2,2,1,3,1)$$ $$\text{melodic minor signature} = (2,1,2,2,2,2,1)$$ $$\text{harmonic major signature} = (2,2,1,2,1,3,1)$$ The number of possible 7-note scales within the 12-note system is simply given by multiplying the number of unique signatures by 7.","This combinatorial question has a musical motivation, which I provide below using as little musical jargon as I can. But first, I'll present a purely mathematical formulation for those not interested in the motivation: Define a signature as a 7-tuple over the set $\{1,2,3\}$ such that the sum of the elements in the tuple is $12$. Two signatures are said to be equivalent if they are either identical or there is a circular shift relation between them (i.e. one can be circularly shifted between 1 and 6 times to obtain the other). How many unique signatures are there? Most modern, Western music is based on the equal temperament system , which divides the octave logarithmically equally into 12 notes . Let's refer to 2 adjacent notes from these 12 as being ""1 step"" apart, two notes with one skipped note in between them as being ""2 steps"" apart, and so on. The (arguably) most natural scale is the major scale, which uses 7 notes from these 12, and has the following signature : $$\text{major signature}=(2,2,1,2,2,2,1)$$ This means that we can construct a major scale as follows: given any note to start from, the second note is 2 steps away from first note, the third note is 1 step away from the 2nd note, and so on according to the above signature. Now this signature really has not just one, but seven scales embedded in it. This is because we can circularly shift the signature, effectively meaning that we are picking a different degree of the major scale to serve as our home note (these seven 'permutations' are called the modes of the signature). Thus, for example, $(1,2,2,2,1,2,2)$ is not a new signature, but just the major signature circularly shifted left twice (and is called the Phrygian mode). My question is: how many unique 7-note signatures are there under the restriction that any signature must not contain an interval greater than 3 steps (this is to respect the fact that any 7-note scale in common use uses only 1, 2 and 3 step intervals, to the best of my knowledge). Some signatures in common use are: $$\text{harmonic minor signature} = (2,1,2,2,1,3,1)$$ $$\text{melodic minor signature} = (2,1,2,2,2,2,1)$$ $$\text{harmonic major signature} = (2,2,1,2,1,3,1)$$ The number of possible 7-note scales within the 12-note system is simply given by multiplying the number of unique signatures by 7.",,"['combinatorics', 'permutations', 'combinations', 'music-theory']"
63,Why a complete graph has $\frac{n(n-1)}{2}$ edges?,Why a complete graph has  edges?,\frac{n(n-1)}{2},"I'm studying graphs in algorithm and complexity, (but I'm not very good at math) as in title: Why a complete graph has $\frac{n(n-1)}{2}$ edges? And how this is related with combinatorics?","I'm studying graphs in algorithm and complexity, (but I'm not very good at math) as in title: Why a complete graph has $\frac{n(n-1)}{2}$ edges? And how this is related with combinatorics?",,"['combinatorics', 'discrete-mathematics', 'graph-theory']"
64,Quadratic reciprocity via generalized Fibonacci numbers?,Quadratic reciprocity via generalized Fibonacci numbers?,,"This is a pet idea of mine which I thought I'd share.  Fix a prime $q$ congruent to $1 \bmod 4$ and define a sequence $F_n$ by $F_0 = 0, F_1 = 1$, and $\displaystyle F_{n+2} = F_{n+1} + \frac{q-1}{4} F_n.$ Then $F_n = \frac{\alpha^n - \beta^n}{\alpha - \beta}$ where $\alpha, \beta$ are the two roots of $f(x) = x^2 - x - \frac{q-1}{4}$.  When $q = 5$ we recover the ordinary Fibonacci numbers.  The discriminant of $f(x)$ is $q$, so it splits $\bmod p$ if and only if $q$ is a quadratic residue $\bmod p$. If $\left( \frac{q}{p} \right) = -1$, then the Frobenius morphism $x \mapsto x^p$ swaps $\alpha$ and $\beta$ (working over $\mathbb{F}_p$), hence $F_p \equiv -1 \bmod p$.  And if $\left( \frac{q}{p} \right) = 1$, then the Frobenius morphism fixes $\alpha$ and $\beta$, hence $F_p \equiv 1 \bmod p$.  In other words, $\displaystyle F_p \equiv \left( \frac{q}{p} \right) \bmod p.$ Quadratic reciprocity in this case is equivalent to the statement that $\displaystyle F_p \equiv \left( \frac{p}{q} \right) \bmod p.$ Question: Does anyone have any ideas about how to prove this directly, thereby proving quadratic reciprocity in the case that $q \equiv 1 \bmod 4$? My pet approach is to think of $F_p$ as counting the number of ways to tile a row of length $p-1$ by tiles of size $1$ and $2$, where there is one type of tile of size $1$ and $\frac{q-1}{4}$ types of tiles of size $2$.  The problem is that I don't see, say, an obvious action of the cyclic group $\mathbb{Z}/p\mathbb{Z}$ on this set.  Any ideas?","This is a pet idea of mine which I thought I'd share.  Fix a prime $q$ congruent to $1 \bmod 4$ and define a sequence $F_n$ by $F_0 = 0, F_1 = 1$, and $\displaystyle F_{n+2} = F_{n+1} + \frac{q-1}{4} F_n.$ Then $F_n = \frac{\alpha^n - \beta^n}{\alpha - \beta}$ where $\alpha, \beta$ are the two roots of $f(x) = x^2 - x - \frac{q-1}{4}$.  When $q = 5$ we recover the ordinary Fibonacci numbers.  The discriminant of $f(x)$ is $q$, so it splits $\bmod p$ if and only if $q$ is a quadratic residue $\bmod p$. If $\left( \frac{q}{p} \right) = -1$, then the Frobenius morphism $x \mapsto x^p$ swaps $\alpha$ and $\beta$ (working over $\mathbb{F}_p$), hence $F_p \equiv -1 \bmod p$.  And if $\left( \frac{q}{p} \right) = 1$, then the Frobenius morphism fixes $\alpha$ and $\beta$, hence $F_p \equiv 1 \bmod p$.  In other words, $\displaystyle F_p \equiv \left( \frac{q}{p} \right) \bmod p.$ Quadratic reciprocity in this case is equivalent to the statement that $\displaystyle F_p \equiv \left( \frac{p}{q} \right) \bmod p.$ Question: Does anyone have any ideas about how to prove this directly, thereby proving quadratic reciprocity in the case that $q \equiv 1 \bmod 4$? My pet approach is to think of $F_p$ as counting the number of ways to tile a row of length $p-1$ by tiles of size $1$ and $2$, where there is one type of tile of size $1$ and $\frac{q-1}{4}$ types of tiles of size $2$.  The problem is that I don't see, say, an obvious action of the cyclic group $\mathbb{Z}/p\mathbb{Z}$ on this set.  Any ideas?",,"['number-theory', 'combinatorics', 'fibonacci-numbers']"
65,An illusionist and their assistant are about to perform the following magic trick,An illusionist and their assistant are about to perform the following magic trick,,"Let $k$ be a positive integer. A spectator is given $n=k!+k−1$ balls numbered $1,2,\dotsc,n$ . Unseen by the illusionist, the spectator arranges the balls into a sequence as they see fit. The assistant studies the sequence, chooses some block of $k$ consecutive balls, and covers them under their scarf. Then the illusionist looks at the newly obscured sequence and guesses the precise order of the $k$ balls they do not see. Devise a strategy for the illusionist and the assistant to follow so that the trick always works. (The strategy needs to be constructed explicitly. For instance, it should be possible to implement the strategy, as described by the solver, in the form of a computer program that takes $k$ and the obscured sequence as input and then runs in time polynomial in $n$ . A mere proof that an appropriate strategy exists does not qualify as a complete solution.) Source: Komal, October 2019, problem A $760$ . Proposed by Nikolai Beluhov, Bulgaria, and Palmer Mebane, USA I can prove that such a strategy must exist: We have a set $A$ of all permutations (what assistant sees) and a set $B$ of all possible positions of a scarf (mark it $0$ ) and remaining numbers (what the illusionist sees). We connect each $a$ in $A$ with $b$ in $B$ if a sequence $b$ without $0$ matches with some consecutive subsequence in $a$ .  Then each $a$ has degree $n-k+1$ and each $b$ has degree $k!$ . Now take an arbitrary subset $X$ in $A$ and let $E$ be a set of all edges from $X$ , and $E'$ set of all edges from $N(X)$ (the set of all neighbours of vertices in $X$ ). Then we have $E\subseteq E'$ and so $|E|\leq |E'|$ . Now $|E|= (n-k+1)|X|$ and $|E'| = k!|N(X)|$ , so we have $$ (n-k+1)|X| \leq k!|N(X)|\implies |X|\leq |N(X)|.$$ By Hall marriage theorem there exists a perfect matching between $A$ and $B$ ... ...but I can not find one explicitly. Any idea? Update: 2020. 12. 20. https://artofproblemsolving.com/community/c6t309f6h2338577_the_magic_trick https://dgrozev.wordpress.com/2020/11/14/magic-recovery-a-komal-problem-about-magic/","Let be a positive integer. A spectator is given balls numbered . Unseen by the illusionist, the spectator arranges the balls into a sequence as they see fit. The assistant studies the sequence, chooses some block of consecutive balls, and covers them under their scarf. Then the illusionist looks at the newly obscured sequence and guesses the precise order of the balls they do not see. Devise a strategy for the illusionist and the assistant to follow so that the trick always works. (The strategy needs to be constructed explicitly. For instance, it should be possible to implement the strategy, as described by the solver, in the form of a computer program that takes and the obscured sequence as input and then runs in time polynomial in . A mere proof that an appropriate strategy exists does not qualify as a complete solution.) Source: Komal, October 2019, problem A . Proposed by Nikolai Beluhov, Bulgaria, and Palmer Mebane, USA I can prove that such a strategy must exist: We have a set of all permutations (what assistant sees) and a set of all possible positions of a scarf (mark it ) and remaining numbers (what the illusionist sees). We connect each in with in if a sequence without matches with some consecutive subsequence in .  Then each has degree and each has degree . Now take an arbitrary subset in and let be a set of all edges from , and set of all edges from (the set of all neighbours of vertices in ). Then we have and so . Now and , so we have By Hall marriage theorem there exists a perfect matching between and ... ...but I can not find one explicitly. Any idea? Update: 2020. 12. 20. https://artofproblemsolving.com/community/c6t309f6h2338577_the_magic_trick https://dgrozev.wordpress.com/2020/11/14/magic-recovery-a-komal-problem-about-magic/","k n=k!+k−1 1,2,\dotsc,n k k k n 760 A B 0 a A b B b 0 a a n-k+1 b k! X A E X E' N(X) X E\subseteq E' |E|\leq |E'| |E|= (n-k+1)|X| |E'| = k!|N(X)|  (n-k+1)|X| \leq k!|N(X)|\implies |X|\leq |N(X)|. A B","['combinatorics', 'discrete-mathematics', 'contest-math', 'puzzle', 'coding-theory']"
66,What structure does the alternating group preserve?,What structure does the alternating group preserve?,,"A common way to define a group is as the group of structure-preserving transformations on some structured set.  For example, the symmetric group on a set $X$ preserves no structure: or, in other words, it preserves only the structure of being a set.  When $X$ is finite, what structure can the alternating group be said to preserve? As a way of making the question precise, is there a natural definition of a category $C$ equipped with a faithful functor to $\text{FinSet}$ such that the skeleton of the underlying groupoid of $C$ is the groupoid with objects $X_n$ such that $\text{Aut}(X_n) \simeq A_n$? Edit: I've been looking for a purely combinatorial answer, but upon reflection a geometric answer might be more appropriate.  If someone can provide a convincing argument why a geometric answer is more natural than a combinatorial answer I will be happy to accept that answer (or Omar's answer).","A common way to define a group is as the group of structure-preserving transformations on some structured set.  For example, the symmetric group on a set $X$ preserves no structure: or, in other words, it preserves only the structure of being a set.  When $X$ is finite, what structure can the alternating group be said to preserve? As a way of making the question precise, is there a natural definition of a category $C$ equipped with a faithful functor to $\text{FinSet}$ such that the skeleton of the underlying groupoid of $C$ is the groupoid with objects $X_n$ such that $\text{Aut}(X_n) \simeq A_n$? Edit: I've been looking for a purely combinatorial answer, but upon reflection a geometric answer might be more appropriate.  If someone can provide a convincing argument why a geometric answer is more natural than a combinatorial answer I will be happy to accept that answer (or Omar's answer).",,"['combinatorics', 'group-theory', 'category-theory', 'groupoids']"
67,How do the Catalan numbers turn up here?,How do the Catalan numbers turn up here?,,"The Catalan numbers have a reputation for turning up everywhere, but the occurrence described below, in the analysis of an (incorrect) algorithm, is still mysterious to me, and I'm curious to find an explanation. For situations where a quadratic-time sorting algorithm is fast enough, I usually use the following: //Given array a[1], ... a[n] for i = 1 to n:     for j = i+1 to n:         if a[i] > a[j]:             swap(a[i],a[j]) It looks like bubble sort, but is closer to selection sort. It is easy to see why it works: in each iteration of the outer loop, a[i] is set to be the smallest element of a[i…n] . In a programming contest many years ago, one of the problems essentially boiled down to sorting: Given a list of distinct values $W_1, W_2, \dots, W_n$ , find the indices when it is sorted in ascending order. In other words, find the permutation $(S_1, S_2, \dots, S_n)$ for which $W_{S_1} < W_{S_2} < \dots < W_{S_n}$ . This is simply a matter of operating on the indices rather than on the array directly, so the correct code would be: //Given arrays S[1], ..., S[n] (initially S[i]=i ∀i) and W[1], ..., W[n] for i = 1 to n:     for j = i+1 to n:         if W[S[i]] > W[S[j]]:             swap(S[i],S[j]) But in the heat of the contest, I instead coded a program that did, incorrectly: for i = 1 to n:     for j = i+1 to n:         if W[i] > W[j]:             swap(S[i],S[j]) I realised the mistake after the contest ended, and later while awaiting the results, with desperate optimism I tried to figure out the odds that for some inputs, my program would accidentally give the right answer anyway. Specifically, I counted the number of permutations of an arbitrary list $W_1, \dots, W_n$ with distinct values (since only their order matters, not their actual values) for which the incorrect algorithm above gives the correct answer, for each n: n       Number of ""lucky"" permutations 0       1 1       1 2       2 3       5 4       14 5       42 6       132 7       429 8       1430 9       4862 10      16796 11      58786 12      208012 These are the Catalan numbers! But why? I've tried to prove this occasionally in my free time, but never succeeded. What I've tried: The (pseudo)algorithm can be represented in more formal notation as the product of all inversions in a permutation. That is, we want to prove that the number of permutations $\sigma \in S_n$ such that $$\prod_{i=1}^{n}\prod_{\substack{j \in \left\{i+1,i+2,\ldots,n\right\}; \\ \sigma_i > \sigma_j}}(i,j) = \sigma^{-1}$$ (with the convention that multiplication is done left to right) is $C_n$ . This change of notation does not make the problem any simpler. I briefly skimmed through Stanley's famous list of Catalan problems , but this does not seem to be (directly) in the list. :-) Some computer experimentation suggests that the lucky permutations are those that avoid the pattern 312, the number of which is apparently the Catalan numbers. But I have no idea how to prove this, and it may not be the best approach...","The Catalan numbers have a reputation for turning up everywhere, but the occurrence described below, in the analysis of an (incorrect) algorithm, is still mysterious to me, and I'm curious to find an explanation. For situations where a quadratic-time sorting algorithm is fast enough, I usually use the following: //Given array a[1], ... a[n] for i = 1 to n:     for j = i+1 to n:         if a[i] > a[j]:             swap(a[i],a[j]) It looks like bubble sort, but is closer to selection sort. It is easy to see why it works: in each iteration of the outer loop, a[i] is set to be the smallest element of a[i…n] . In a programming contest many years ago, one of the problems essentially boiled down to sorting: Given a list of distinct values , find the indices when it is sorted in ascending order. In other words, find the permutation for which . This is simply a matter of operating on the indices rather than on the array directly, so the correct code would be: //Given arrays S[1], ..., S[n] (initially S[i]=i ∀i) and W[1], ..., W[n] for i = 1 to n:     for j = i+1 to n:         if W[S[i]] > W[S[j]]:             swap(S[i],S[j]) But in the heat of the contest, I instead coded a program that did, incorrectly: for i = 1 to n:     for j = i+1 to n:         if W[i] > W[j]:             swap(S[i],S[j]) I realised the mistake after the contest ended, and later while awaiting the results, with desperate optimism I tried to figure out the odds that for some inputs, my program would accidentally give the right answer anyway. Specifically, I counted the number of permutations of an arbitrary list with distinct values (since only their order matters, not their actual values) for which the incorrect algorithm above gives the correct answer, for each n: n       Number of ""lucky"" permutations 0       1 1       1 2       2 3       5 4       14 5       42 6       132 7       429 8       1430 9       4862 10      16796 11      58786 12      208012 These are the Catalan numbers! But why? I've tried to prove this occasionally in my free time, but never succeeded. What I've tried: The (pseudo)algorithm can be represented in more formal notation as the product of all inversions in a permutation. That is, we want to prove that the number of permutations such that (with the convention that multiplication is done left to right) is . This change of notation does not make the problem any simpler. I briefly skimmed through Stanley's famous list of Catalan problems , but this does not seem to be (directly) in the list. :-) Some computer experimentation suggests that the lucky permutations are those that avoid the pattern 312, the number of which is apparently the Catalan numbers. But I have no idea how to prove this, and it may not be the best approach...","W_1, W_2, \dots, W_n (S_1, S_2, \dots, S_n) W_{S_1} < W_{S_2} < \dots < W_{S_n} W_1, \dots, W_n \sigma \in S_n \prod_{i=1}^{n}\prod_{\substack{j \in \left\{i+1,i+2,\ldots,n\right\}; \\ \sigma_i > \sigma_j}}(i,j) = \sigma^{-1} C_n","['combinatorics', 'algorithms', 'catalan-numbers']"
68,How many values of $2^{2^{2^{.^{.^{.^{2}}}}}}$ depending on parenthesis?,How many values of  depending on parenthesis?,2^{2^{2^{.^{.^{.^{2}}}}}},Suppose we have a power tower consisting of $2$ occurring $n$ times: $$\huge2^{2^{2^{.^{.^{.^{2}}}}}}$$ How many values can we generate by placing any number of parenthesis? It is fairly simple for the first few values of $n$: There is $1$ value for $n=1$: $2=2$ There is $1$ value for $n=2$: $4=2^{2}$ There is $1$ value for $n=3$: $16=({2^{2})^{2}}=2^{(2^{2})}$ There are $2$ values for $n=4$: $256=(({2^{2})^{2}})^2=(2^{(2^{2})})^2=(2^{2})^{(2^{2})}$ $65536=2^{(({2^{2})^{2}})}=2^{(2^{(2^{2})})}$ Any idea how to formulate a general solution? I'm thinking that it might be feasible using a recurrence relation. Thanks,Suppose we have a power tower consisting of $2$ occurring $n$ times: $$\huge2^{2^{2^{.^{.^{.^{2}}}}}}$$ How many values can we generate by placing any number of parenthesis? It is fairly simple for the first few values of $n$: There is $1$ value for $n=1$: $2=2$ There is $1$ value for $n=2$: $4=2^{2}$ There is $1$ value for $n=3$: $16=({2^{2})^{2}}=2^{(2^{2})}$ There are $2$ values for $n=4$: $256=(({2^{2})^{2}})^2=(2^{(2^{2})})^2=(2^{2})^{(2^{2})}$ $65536=2^{(({2^{2})^{2}})}=2^{(2^{(2^{2})})}$ Any idea how to formulate a general solution? I'm thinking that it might be feasible using a recurrence relation. Thanks,,"['combinatorics', 'elementary-number-theory', 'recurrence-relations', 'power-towers']"
69,Do circles divide the plane into more regions than lines?,Do circles divide the plane into more regions than lines?,,"In this post it is mentioned that $n$ straight lines can divide the plane into a maximum number of $(n^{2}+n+2)/2$ different regions. What happens if we use circles instead of lines? That is, what is the maximum number of regions into which n circles can divide the plane? After some exploration it seems to me that in order to get maximum division the circles must intersect pairwise, with no two of them tangent, none of them being inside another and no three of them concurrent (That is no three intersecting at a point). The answer seems to me to be affirmative, as the number I obtain is $n^{2}-n+2$ different regions. Is that correct?","In this post it is mentioned that $n$ straight lines can divide the plane into a maximum number of $(n^{2}+n+2)/2$ different regions. What happens if we use circles instead of lines? That is, what is the maximum number of regions into which n circles can divide the plane? After some exploration it seems to me that in order to get maximum division the circles must intersect pairwise, with no two of them tangent, none of them being inside another and no three of them concurrent (That is no three intersecting at a point). The answer seems to me to be affirmative, as the number I obtain is $n^{2}-n+2$ different regions. Is that correct?",,"['combinatorics', 'geometry', 'circles']"
70,Crazy pattern in the simple continued fraction for $\sum_{k=1}^\infty \frac{1}{(2^k)!}$,Crazy pattern in the simple continued fraction for,\sum_{k=1}^\infty \frac{1}{(2^k)!},"The continued fraction of this series exhibits a truly crazy pattern and I found no reference for it so far. We have: $$\sum_{k=1}^\infty \frac{1}{(2^k)!}=0.5416914682540160487415778421$$ But the continued fraction is just beautiful: [1, 1, 5, 2, 69, 1, 1, 5, 1, 1, 12869, 2, 5, 1, 1, 69, 2, 5, 1, 1, 601080389, 2, 5, 2, 69, 1, 1, 5, 2, 12869, 1, 1, 5, 1, 1, 69, 2, 5, 1, 1, 1832624140942590533, 2, 5, 2, 69, 1, 1, 5, 1, 1, 12869, 2, 5, 1, 1, 69, 2, 5, 2, 601080389, 1, 1, 5, 2, 69, 1, 1, 5, 2, 12869, 1, 1, 5, 1, 1, 69, 2, 5, 1, 1, 23951146041928082866135587776380551749, 2, 5, 2, 69, 1, 1, 5, 1, 1, 12869, 2, 5, 1, 1, 69, 2, 5, 1, 1, 601080389, 2, 5, 2, 69, 1, 1, 5, 2, 12869, 1, 1, 5, 1, 1, 69, 2, 5, 2, 1832624140942590533, 1, 1, 5, 2, 69, 1, 1, 5, 1, 1, 12869, 2, 5, 1, 1, 69, 2, 5, 2, 601080389, 1, 1, 5, 2, 69, 1, 1, 5, 2,...] All of these large numbers are not just random - they have a simple closed form: $$A_n= \left( \begin{array}( 2^n \\ 2^{n-1}  \end{array} \right) -1$$ $$A_1=1$$ $$A_2=5$$ $$A_3=69$$ $$A_4=12869$$ $$A_5=601080389$$ And so on. This sequence is not in OEIS, only the larger sequence is, which contains this one as a subsequence https://oeis.org/A014495 What is the explanation for this? Is there a regular pattern in this continued fraction (in the positions of numbers)? Is there generalizations for other sums of the form $\sum_{k=1}^\infty \frac{1}{(a^k)!}$ ? Edit I think a good move will be to rename the strings of small numbers: $$a=1, 1, 5, 2,\qquad b=1, 1, 5, 1, 1,\qquad c=2,5,1,1,\qquad d=2, 5, 2$$ As a side note if we could set $1,1=2$ then all these strings will be the same. Now we rewrite the sequence. I will denote $A_n$ by just their indices $n$ : $$[a, 3, b, 4, c, 3, c, 5, d, 3, a, 4, b, 3, c, 6, d, 3, b, 4, c, 3, d, 5, a, 3, a, 4, b, 3, c, 7, \\ d, 3, b, 4, c, 3, c, 5, d, 3, a, 4, b, 3, d, 6, a, 3, b, 4, c, 3, d, 5, a, 3, a,...]$$ $$[a3b4c3c5d3a4b3c6d3b4c3d5a3a4b3c7d3b4c3c5d3a4b3d6a3b4c3d5a3a,...]$$ Now we have new large numbers $A_n$ appear at positions $2^n$ . And postitions of the same numbers are in a simple arithmetic progression with a difference $2^n$ as well. Now we only have to figure out the pattern (if any exists) for $a,b,c,d$ . The $10~000$ terms of the continued fraction are uploaded at github here . I also link my related question , from the iformation there we can conclude that the series above provide a greedy algorithm Egyptian fraction expansion of the number, and the number is irrational by the theorem stated in this paper .","The continued fraction of this series exhibits a truly crazy pattern and I found no reference for it so far. We have: But the continued fraction is just beautiful: [1, 1, 5, 2, 69, 1, 1, 5, 1, 1, 12869, 2, 5, 1, 1, 69, 2, 5, 1, 1, 601080389, 2, 5, 2, 69, 1, 1, 5, 2, 12869, 1, 1, 5, 1, 1, 69, 2, 5, 1, 1, 1832624140942590533, 2, 5, 2, 69, 1, 1, 5, 1, 1, 12869, 2, 5, 1, 1, 69, 2, 5, 2, 601080389, 1, 1, 5, 2, 69, 1, 1, 5, 2, 12869, 1, 1, 5, 1, 1, 69, 2, 5, 1, 1, 23951146041928082866135587776380551749, 2, 5, 2, 69, 1, 1, 5, 1, 1, 12869, 2, 5, 1, 1, 69, 2, 5, 1, 1, 601080389, 2, 5, 2, 69, 1, 1, 5, 2, 12869, 1, 1, 5, 1, 1, 69, 2, 5, 2, 1832624140942590533, 1, 1, 5, 2, 69, 1, 1, 5, 1, 1, 12869, 2, 5, 1, 1, 69, 2, 5, 2, 601080389, 1, 1, 5, 2, 69, 1, 1, 5, 2,...] All of these large numbers are not just random - they have a simple closed form: And so on. This sequence is not in OEIS, only the larger sequence is, which contains this one as a subsequence https://oeis.org/A014495 What is the explanation for this? Is there a regular pattern in this continued fraction (in the positions of numbers)? Is there generalizations for other sums of the form ? Edit I think a good move will be to rename the strings of small numbers: As a side note if we could set then all these strings will be the same. Now we rewrite the sequence. I will denote by just their indices : Now we have new large numbers appear at positions . And postitions of the same numbers are in a simple arithmetic progression with a difference as well. Now we only have to figure out the pattern (if any exists) for . The terms of the continued fraction are uploaded at github here . I also link my related question , from the iformation there we can conclude that the series above provide a greedy algorithm Egyptian fraction expansion of the number, and the number is irrational by the theorem stated in this paper .","\sum_{k=1}^\infty \frac{1}{(2^k)!}=0.5416914682540160487415778421 A_n= \left( \begin{array}( 2^n \\ 2^{n-1}  \end{array} \right) -1 A_1=1 A_2=5 A_3=69 A_4=12869 A_5=601080389 \sum_{k=1}^\infty \frac{1}{(a^k)!} a=1, 1, 5, 2,\qquad b=1, 1, 5, 1, 1,\qquad c=2,5,1,1,\qquad d=2, 5, 2 1,1=2 A_n n [a, 3, b, 4, c, 3, c, 5, d, 3, a, 4, b, 3, c, 6, d, 3, b, 4, c, 3, d, 5, a, 3, a, 4, b, 3, c, 7, \\ d, 3, b, 4, c, 3, c, 5, d, 3, a, 4, b, 3, d, 6, a, 3, b, 4, c, 3, d, 5, a, 3, a,...] [a3b4c3c5d3a4b3c6d3b4c3d5a3a4b3c7d3b4c3c5d3a4b3d6a3b4c3d5a3a,...] A_n 2^n 2^n a,b,c,d 10~000","['combinatorics', 'number-theory', 'continued-fractions']"
71,Why are generating functions useful?,Why are generating functions useful?,,"I was under the mistaken impression that if one could find the generating function for a sequence of numbers, you could just plug in a natural number $n$ to find the nth term of the sequence. I realize now that I was confusing this with a closed form formula. So if that is not the case, then what is the point of generating functions? How do they make understanding counting sequences easier? For example, suppose  I had a problem where I wanted to count how many ways I could buy $n$ pieces of apples, oranges, and pears given that I want an even number of apples, an odd number of oranges, and at most 3 pears. This would be the number of nonnegative integer solutions to $a+b+c=n$ with $a$ even, $b$ odd, and $0\leq c\leq 3$. This is the same as the coefficient of $x^n$ in the product $$ (1+x^2+x^4+\cdots)(x+x^3+x^5+\cdots)(1+x+x^2+x^3) = \frac{1}{1-x^2}\cdot\frac{x}{1-x^2}\cdot\frac{1-x^4}{1-x}$$ But what good is that? I don't see how this is much better. Also, with use of exponential generating functions, it seems the choice of monomials we use as place holders for the terms of the sequence can be arbitrary. Then then $n$th term of the sequence is just the coefficient of the $n$th monomial that you've chosen to build the generating function with. What is the real advantage of doing things like this? Many problems I see tend to ask me find the generating function, but then I'm rarely asked to do anything with it.","I was under the mistaken impression that if one could find the generating function for a sequence of numbers, you could just plug in a natural number $n$ to find the nth term of the sequence. I realize now that I was confusing this with a closed form formula. So if that is not the case, then what is the point of generating functions? How do they make understanding counting sequences easier? For example, suppose  I had a problem where I wanted to count how many ways I could buy $n$ pieces of apples, oranges, and pears given that I want an even number of apples, an odd number of oranges, and at most 3 pears. This would be the number of nonnegative integer solutions to $a+b+c=n$ with $a$ even, $b$ odd, and $0\leq c\leq 3$. This is the same as the coefficient of $x^n$ in the product $$ (1+x^2+x^4+\cdots)(x+x^3+x^5+\cdots)(1+x+x^2+x^3) = \frac{1}{1-x^2}\cdot\frac{x}{1-x^2}\cdot\frac{1-x^4}{1-x}$$ But what good is that? I don't see how this is much better. Also, with use of exponential generating functions, it seems the choice of monomials we use as place holders for the terms of the sequence can be arbitrary. Then then $n$th term of the sequence is just the coefficient of the $n$th monomial that you've chosen to build the generating function with. What is the real advantage of doing things like this? Many problems I see tend to ask me find the generating function, but then I'm rarely asked to do anything with it.",,"['combinatorics', 'generating-functions']"
72,How to reverse the $n$ choose $k$ formula?,How to reverse the  choose  formula?,n k,"If I want to find how many possible ways there are to choose k out of n elements I know you can use the simple formula below: $$ \binom{n}{k} = \frac{n! }{ k!(n-k)! } .$$ What if I want to go the other way around though? That is, I know I want to have $X$ possible combinations, and I want to find all the various pairs of $n$ and $k$ that will give me that number of combinations. For example, if the number of combinations I want is $3$, I want a formula/method to find that all the pairs that will result in that number of combinations are $(3,1)$ and $(3,2)$ I know I could test all the possible pairs, but this would be impractical for large numbers. But perhaps there's no easier way of doing this then the brute force approach?","If I want to find how many possible ways there are to choose k out of n elements I know you can use the simple formula below: $$ \binom{n}{k} = \frac{n! }{ k!(n-k)! } .$$ What if I want to go the other way around though? That is, I know I want to have $X$ possible combinations, and I want to find all the various pairs of $n$ and $k$ that will give me that number of combinations. For example, if the number of combinations I want is $3$, I want a formula/method to find that all the pairs that will result in that number of combinations are $(3,1)$ and $(3,2)$ I know I could test all the possible pairs, but this would be impractical for large numbers. But perhaps there's no easier way of doing this then the brute force approach?",,"['combinatorics', 'binomial-coefficients']"
73,Is it possible to place 26 points inside a rectangle that is 20 cm by 15 cm so that the distance between every pair of points is greater than 5 cm?,Is it possible to place 26 points inside a rectangle that is 20 cm by 15 cm so that the distance between every pair of points is greater than 5 cm?,,"I need help to answer the following question: Is it possible to place 26 points inside a rectangle that is $20\, cm$ by   $15\,cm$ so that the distance between every pair of points is greater   than $5\, cm$? I haven't learned any mathematical ways to find a solution; whether it maybe yes or no, to a problem like this so it would be very helpful if you could help me with this question.","I need help to answer the following question: Is it possible to place 26 points inside a rectangle that is $20\, cm$ by   $15\,cm$ so that the distance between every pair of points is greater   than $5\, cm$? I haven't learned any mathematical ways to find a solution; whether it maybe yes or no, to a problem like this so it would be very helpful if you could help me with this question.",,"['combinatorics', 'rectangles']"
74,"Time to reach a final state in a random dynamical system (answer known, proof unknown)","Time to reach a final state in a random dynamical system (answer known, proof unknown)",,"Consider a dynamical system with state space $2^n$ represented as a sequence of $n$ black or white characters, such as $BWBB\ldots WB$. At every step, we choose a random pair $(i,j)$ with $i<j$ and copy the $i$-th color into the $j$-th position. For example, if the state is $BWW$ and we choose $(1,3)$, then the next state would be $BWB$. Starting with an initial state $BWW\ldots W$, we will reach a final state $BB\ldots B$ in finite time with probability $1$ (because any initial segment of blacks persists forever). What is the expected time to reach this final state $BB\ldots B$? Low-dimensional analysis shows that the answer is exactly $(n-1)^2$ whenever $n>2$; this was ""verified"" up to $n=13$ or so. But we can't find a proof of this conjecture, nor even show that the answer is an integer. One expects that, if the answer is that easy, there should also be an elementary proof. I tried various kinds of induction. For example, I tried to use induction to compute the expected time to reach the state $BB\ldots B*$ where $*$ is something , and reduced the initial conjecture to a claim that the expected time from $BB\ldots B*$ to $BB\ldots BB$ is exactly $1$ . But I don't know how to prove that it is $1$.","Consider a dynamical system with state space $2^n$ represented as a sequence of $n$ black or white characters, such as $BWBB\ldots WB$. At every step, we choose a random pair $(i,j)$ with $i<j$ and copy the $i$-th color into the $j$-th position. For example, if the state is $BWW$ and we choose $(1,3)$, then the next state would be $BWB$. Starting with an initial state $BWW\ldots W$, we will reach a final state $BB\ldots B$ in finite time with probability $1$ (because any initial segment of blacks persists forever). What is the expected time to reach this final state $BB\ldots B$? Low-dimensional analysis shows that the answer is exactly $(n-1)^2$ whenever $n>2$; this was ""verified"" up to $n=13$ or so. But we can't find a proof of this conjecture, nor even show that the answer is an integer. One expects that, if the answer is that easy, there should also be an elementary proof. I tried various kinds of induction. For example, I tried to use induction to compute the expected time to reach the state $BB\ldots B*$ where $*$ is something , and reduced the initial conjecture to a claim that the expected time from $BB\ldots B*$ to $BB\ldots BB$ is exactly $1$ . But I don't know how to prove that it is $1$.",,"['combinatorics', 'probability-theory', 'discrete-mathematics', 'markov-chains']"
75,What is the shortest string that contains all permutations of an alphabet?,What is the shortest string that contains all permutations of an alphabet?,,"What is the shortest string $S$ over an alphabet of size $n$ , such that every permutation of the alphabet is a substring of $S$ ? Edit 2019-10-17: This is called a superpermutation, and there is a wikipedia page that keeps track of the best results. Turns out the problem is still open.","What is the shortest string over an alphabet of size , such that every permutation of the alphabet is a substring of ? Edit 2019-10-17: This is called a superpermutation, and there is a wikipedia page that keeps track of the best results. Turns out the problem is still open.",S n S,"['combinatorics', 'permutations']"
76,A zero sum subset of a sum-full set,A zero sum subset of a sum-full set,,"I had seen this problem a long time back and wasn't able to solve it. For some reason I was reminded of it and thought it might be interesting to the visitors here. Apparently, this problem is from a mathematics magazine of some university in the United States (sorry, no idea about either). So the problem is: Suppose $S \subset \mathbb{Z}$ (set of integers) such that 1) $|S| = 15$ 2) $\forall ~s \in S, \exists ~a,b \in S$ such that $s = a+b$ Show that for every such $S$, there is a non-empty subset $T$ of $S$ such that the sum of elements of $T$ is zero and $|T| \leq 7$. Update (Sep 13) Here is an approach which seems promising and others might be able to take it ahead perhaps. If you look at the set as a vector $s$, then there is a matrix $A$ with the main diagonal being all $1$, each row containing exactly one $1$ and one $-1$ (or a single $2$) in the non-diagonal position such that $As = 0$. The problem becomes equivalent to proving that for any such matrix $A$ the row space of $A$ contains a vector with all zeroes except for a $1$ and $-1$ or a vector with all zeroes except $\leq 7$ ones. This implies that the numbers in the set $S$ themselves don't matter and we can perhaps replace them with elements from a different field (like say reals, or complex numbers).","I had seen this problem a long time back and wasn't able to solve it. For some reason I was reminded of it and thought it might be interesting to the visitors here. Apparently, this problem is from a mathematics magazine of some university in the United States (sorry, no idea about either). So the problem is: Suppose $S \subset \mathbb{Z}$ (set of integers) such that 1) $|S| = 15$ 2) $\forall ~s \in S, \exists ~a,b \in S$ such that $s = a+b$ Show that for every such $S$, there is a non-empty subset $T$ of $S$ such that the sum of elements of $T$ is zero and $|T| \leq 7$. Update (Sep 13) Here is an approach which seems promising and others might be able to take it ahead perhaps. If you look at the set as a vector $s$, then there is a matrix $A$ with the main diagonal being all $1$, each row containing exactly one $1$ and one $-1$ (or a single $2$) in the non-diagonal position such that $As = 0$. The problem becomes equivalent to proving that for any such matrix $A$ the row space of $A$ contains a vector with all zeroes except for a $1$ and $-1$ or a vector with all zeroes except $\leq 7$ ones. This implies that the numbers in the set $S$ themselves don't matter and we can perhaps replace them with elements from a different field (like say reals, or complex numbers).",,['combinatorics']
77,Paradox: Roots of a polynomial require less information to express than coefficients?,Paradox: Roots of a polynomial require less information to express than coefficients?,,"A somewhat information theoretical paradox occurred to me, and I was wondering if anyone could resolve it. Let $p(x) = x^n + c_{n-1} x^{n-1} + \cdots + c_0 = (x - r_0) \cdots (x - r_{n-1})$ be a degree $n$ polynomial with leading coefficient $1$ . Clearly, the polynomial can be specified exactly by its $n$ coefficients $c=\{c_{n-1}, \ldots, c_0\}$ OR by its $n$ roots $r=\{r_{n-1}, \ldots, r_0\}$ . So the roots and the coefficients contain the same information. However, it takes less information to specify the roots, because their order doesn't matter . (i.e. the roots of the polynomial require $\lg(n!)$ bits less information to specify than the coefficients). Isn't this a paradox? Or is my logic off somewhere? Edit: To clarify, all values belong to any algebraically closed field (such as the complex numbers). And note that the leading coefficient is specified to be 1, meaning that there is absolutely a one-to-one correspondence between the $n$ remaining coefficients $c$ and the $n$ roots $r$ .","A somewhat information theoretical paradox occurred to me, and I was wondering if anyone could resolve it. Let be a degree polynomial with leading coefficient . Clearly, the polynomial can be specified exactly by its coefficients OR by its roots . So the roots and the coefficients contain the same information. However, it takes less information to specify the roots, because their order doesn't matter . (i.e. the roots of the polynomial require bits less information to specify than the coefficients). Isn't this a paradox? Or is my logic off somewhere? Edit: To clarify, all values belong to any algebraically closed field (such as the complex numbers). And note that the leading coefficient is specified to be 1, meaning that there is absolutely a one-to-one correspondence between the remaining coefficients and the roots .","p(x) = x^n + c_{n-1} x^{n-1} + \cdots + c_0 = (x - r_0) \cdots (x - r_{n-1}) n 1 n c=\{c_{n-1}, \ldots, c_0\} n r=\{r_{n-1}, \ldots, r_0\} \lg(n!) n c n r","['combinatorics', 'polynomials', 'roots', 'information-theory']"
78,How many ways can I arrange the numbers $1$ to $N$ with this divisibility condition?,How many ways can I arrange the numbers  to  with this divisibility condition?,1 N,"For the numbers $1, \ldots, N$ , how many ways can I arrange them such that either: The number at $i$ is evenly divisible by $i$ , or $i$ is evenly divisible by the number at $i$ . Example: for $N = 2$ , we have: $\{1, 2\}$ number at $i = 1$ is $1$ and is evenly divisible by $i = 1$ . number at $i = 2$ is $2$ and $i = 2$ is evenly divisible by $2$ . $\{2, 1\}$ number at $i = 1$ is $2$ and is evenly divisible by $i = 1$ . number at $i = 2$ is $1$ and $i = 1$ is evenly divisible by $1$ . so there are two such arrangements for $N = 2$ .","For the numbers , how many ways can I arrange them such that either: The number at is evenly divisible by , or is evenly divisible by the number at . Example: for , we have: number at is and is evenly divisible by . number at is and is evenly divisible by . number at is and is evenly divisible by . number at is and is evenly divisible by . so there are two such arrangements for .","1, \ldots, N i i i i N = 2 \{1, 2\} i = 1 1 i = 1 i = 2 2 i = 2 2 \{2, 1\} i = 1 2 i = 1 i = 2 1 i = 1 1 N = 2","['combinatorics', 'number-theory', 'elementary-number-theory', 'permutations', 'divisibility']"
79,"In a village, $90\%$ of people drink Tea, $80\%$ Coffee, $70\%$ Whiskey, $60\%$ Gin. Nobody drinks all four. What percentage of people drinks alcohol?","In a village,  of people drink Tea,  Coffee,  Whiskey,  Gin. Nobody drinks all four. What percentage of people drinks alcohol?",90\% 80\% 70\% 60\%,"In a small village $90\%$ of the people drink Tea, $80\%$ Coffee, $70\%$ Whiskey and $60\%$ Gin. Nobody drinks all four beverages. What percentage of people of this village drinks alcohol? I got this riddle from a relative and first thought it can be solved with the inclusion-, exclusion principle. That the percentage of people who drink alcohol has to be in the range from $70\%$ to $100\%$ is obvious to me When $T$ , $C$ , $W$ , and $G$ are sets, and I assume a village with $100$ people, then what I am looking for is $$\lvert W\cup G\rvert = \lvert W\rvert+\lvert G\rvert-\lvert W\cap G\rvert$$ I know that $$\lvert T \cap C \cap W \cap G \rvert = 0$$ and also the absolute values of the singletons. But I do not see how this brings me any closer, since I still need to figure out what $\lvert W\cap G\rvert$ is and that looks similar hard at this point On the way there I also noticed that $\lvert T\cap C\rvert \ge 70$ and similar $\lvert W\cap G\rvert \ge 30$ By now I think there is too little information to solve it precisely.","In a small village of the people drink Tea, Coffee, Whiskey and Gin. Nobody drinks all four beverages. What percentage of people of this village drinks alcohol? I got this riddle from a relative and first thought it can be solved with the inclusion-, exclusion principle. That the percentage of people who drink alcohol has to be in the range from to is obvious to me When , , , and are sets, and I assume a village with people, then what I am looking for is I know that and also the absolute values of the singletons. But I do not see how this brings me any closer, since I still need to figure out what is and that looks similar hard at this point On the way there I also noticed that and similar By now I think there is too little information to solve it precisely.",90\% 80\% 70\% 60\% 70\% 100\% T C W G 100 \lvert W\cup G\rvert = \lvert W\rvert+\lvert G\rvert-\lvert W\cap G\rvert \lvert T \cap C \cap W \cap G \rvert = 0 \lvert W\cap G\rvert \lvert T\cap C\rvert \ge 70 \lvert W\cap G\rvert \ge 30,"['combinatorics', 'algebra-precalculus', 'puzzle']"
80,Identity involving Euler's totient function: $\sum \limits_{k=1}^n \left\lfloor \frac{n}{k} \right\rfloor \varphi(k) = \frac{n(n+1)}{2}$,Identity involving Euler's totient function:,\sum \limits_{k=1}^n \left\lfloor \frac{n}{k} \right\rfloor \varphi(k) = \frac{n(n+1)}{2},"Let $\varphi(n)$ be Euler's totient function, the number of positive integers less than or equal to $n$ and relatively prime to $n$. Challenge: Prove $$\sum_{k=1}^n \left\lfloor \frac{n}{k} \right\rfloor \varphi(k) = \frac{n(n+1)}{2}.$$ I have two proofs, one of which is partially combinatorial. I'm posing this problem partly because I think some folks on this site would be interested in working on it and partly because I would like to see a purely combinatorial proof.  (But please post any proofs; I would be interested in noncombinatorial ones, too.  I've learned a lot on this site by reading alternative proofs of results I already know.) I'll wait a few days to give others a chance to respond before posting my proofs. EDIT: The two proofs in full are now given among the answers.","Let $\varphi(n)$ be Euler's totient function, the number of positive integers less than or equal to $n$ and relatively prime to $n$. Challenge: Prove $$\sum_{k=1}^n \left\lfloor \frac{n}{k} \right\rfloor \varphi(k) = \frac{n(n+1)}{2}.$$ I have two proofs, one of which is partially combinatorial. I'm posing this problem partly because I think some folks on this site would be interested in working on it and partly because I would like to see a purely combinatorial proof.  (But please post any proofs; I would be interested in noncombinatorial ones, too.  I've learned a lot on this site by reading alternative proofs of results I already know.) I'll wait a few days to give others a chance to respond before posting my proofs. EDIT: The two proofs in full are now given among the answers.",,"['combinatorics', 'elementary-number-theory', 'summation', 'ceiling-and-floor-functions', 'totient-function']"
81,How many triangles,How many triangles,,"I saw this question today, it asks how many triangles are in this picture. I don't know how to solve this (without counting directly), though I guess it has something to do with some recurrence. How can count the number of all triangles in the picture ?","I saw this question today, it asks how many triangles are in this picture. I don't know how to solve this (without counting directly), though I guess it has something to do with some recurrence. How can count the number of all triangles in the picture ?",,"['combinatorics', 'puzzle', 'recreational-mathematics']"
82,What is the next number on the constructibility sequence? And what is the asymptotic growth?,What is the next number on the constructibility sequence? And what is the asymptotic growth?,,"Let us systematically generate all constructible points in the plane. We begin with just two points, which specify the unit distance. With the straightedge, we may construct the line joining them. And with the compass, we may construct the two circles centered at each of them, having that unit segment as radius. These circles intersect each other and the line, creating four additional points of intersection. Thus, we have now six points in all. Using these six points, we proceed to the next stage, constructing all possible lines and circles using those six points, and finding the resulting points of intersection. I believe that we now have 203 points. Let us proceed in this way to systematically construct all constructible points in the plane, in a hierarchy of finite stages. At each stage, we form all possible lines and circles that may be formed from our current points using straightedge and compass, and then we find all points of intersection from the resulting figures. This produces what I call the constructibility sequence : $$2\qquad\qquad 6\qquad\qquad 203\qquad\qquad ?$$ Each entry is the number of points constructed at that stage. I have a number of questions about the constructibility sequence: Question 1. What is the next constructibility number? There is no entry in the online encyclopedia of integer sequences beginning 2, 6, 203, and so I would like to create an entry for the constructibility sequence. But they request at least four numbers, and so we seem to need to know the next number. I'm not sure exactly how to proceed with this, since if one proceeds computationally, then one will inevitably have to decide if two very-close points count as identical are not, and I don't see any principled way to ensure that this is done correctly. So it seems that one will need to proceed with some kind of idealized geometric calculus, which gets the right answer about coincidence of intersection points. [ Update: The sequence now exists as A333944 .] Question 2. What kind of asymptotic upper bounds can you prove on the growth of the constructibility sequence? At each stage, every pair of points determine a line and two circles. And every intersection point is realized as the intersection of two lines, two circles or a line and circle, which have at most two intersection points in each case. So a rough upper bound is that from $k$ points, we produce no more than $3k^2$ many lines and circles, and so at most $(3k^2)^2$ many pairs of line and circles, and so at most $2(3k^2)^2$ many points of intersection. This leads to an upper bound of growth something like $18^n2^{4^n}$ after $n$ stages. Can anyone give a better bound? Question 3. And what of lower bounds? I suspect that the sequence grows very quickly, probably doubly exponentially. But to prove this, we would seem to need to identify a realm of construction patterns where there is little interference of intersection coincidence, so that one can be sure of a certain known growth in new points.","Let us systematically generate all constructible points in the plane. We begin with just two points, which specify the unit distance. With the straightedge, we may construct the line joining them. And with the compass, we may construct the two circles centered at each of them, having that unit segment as radius. These circles intersect each other and the line, creating four additional points of intersection. Thus, we have now six points in all. Using these six points, we proceed to the next stage, constructing all possible lines and circles using those six points, and finding the resulting points of intersection. I believe that we now have 203 points. Let us proceed in this way to systematically construct all constructible points in the plane, in a hierarchy of finite stages. At each stage, we form all possible lines and circles that may be formed from our current points using straightedge and compass, and then we find all points of intersection from the resulting figures. This produces what I call the constructibility sequence : Each entry is the number of points constructed at that stage. I have a number of questions about the constructibility sequence: Question 1. What is the next constructibility number? There is no entry in the online encyclopedia of integer sequences beginning 2, 6, 203, and so I would like to create an entry for the constructibility sequence. But they request at least four numbers, and so we seem to need to know the next number. I'm not sure exactly how to proceed with this, since if one proceeds computationally, then one will inevitably have to decide if two very-close points count as identical are not, and I don't see any principled way to ensure that this is done correctly. So it seems that one will need to proceed with some kind of idealized geometric calculus, which gets the right answer about coincidence of intersection points. [ Update: The sequence now exists as A333944 .] Question 2. What kind of asymptotic upper bounds can you prove on the growth of the constructibility sequence? At each stage, every pair of points determine a line and two circles. And every intersection point is realized as the intersection of two lines, two circles or a line and circle, which have at most two intersection points in each case. So a rough upper bound is that from points, we produce no more than many lines and circles, and so at most many pairs of line and circles, and so at most many points of intersection. This leads to an upper bound of growth something like after stages. Can anyone give a better bound? Question 3. And what of lower bounds? I suspect that the sequence grows very quickly, probably doubly exponentially. But to prove this, we would seem to need to identify a realm of construction patterns where there is little interference of intersection coincidence, so that one can be sure of a certain known growth in new points.",2\qquad\qquad 6\qquad\qquad 203\qquad\qquad ? k 3k^2 (3k^2)^2 2(3k^2)^2 18^n2^{4^n} n,"['combinatorics', 'geometry', 'geometric-construction']"
83,What is the total sum of the cardinalities of all subsets of a set?,What is the total sum of the cardinalities of all subsets of a set?,,"I'm having a hard time finding the pattern. Let's say we have a set $$S = \{1, 2, 3\}$$ The subsets are: $$P = \{ \{\}, \{1\}, \{2\}, \{3\}, \{1, 2\}, \{1, 3\}, \{2, 3\}, \{1, 2, 3\} \}$$ And the value I'm looking for, is the sum of the cardinalities of all of these subsets. That is, for this example, $$0+1+1+1+2+2+2+3=12$$ What's the formula for this value? I can sort of see a pattern, but I can't generalize it.","I'm having a hard time finding the pattern. Let's say we have a set $$S = \{1, 2, 3\}$$ The subsets are: $$P = \{ \{\}, \{1\}, \{2\}, \{3\}, \{1, 2\}, \{1, 3\}, \{2, 3\}, \{1, 2, 3\} \}$$ And the value I'm looking for, is the sum of the cardinalities of all of these subsets. That is, for this example, $$0+1+1+1+2+2+2+3=12$$ What's the formula for this value? I can sort of see a pattern, but I can't generalize it.",,"['combinatorics', 'elementary-set-theory', 'summation']"
84,Choosing office hours to maximise number of students who can attend at least one time slot,Choosing office hours to maximise number of students who can attend at least one time slot,,"I have the following (real world!) problem which is most easily described using an example. I ask my six students when the best time to hold office hours would be. I give them four options, and say that I'll hold two hours in total. The poll's results are as follows (1 means yes, 0 means no): $$\begin{array}{l|c|c|c|c}  \text{Name} & 9 \text{ am} & 10 \text{ am} & 11 \text{ am} & 12 \text{ pm} \\ \hline \text{Alice} & 1 & 1 & 0 & 0 \\  \text{Bob} & 1 & 1 & 0 & 1 \\  \text{Charlotte} & 1 & 1 & 0 & 1 \\  \text{Daniel} & 0 & 1 & 1 & 1 \\ \text{Eve} & 0 & 0 & 1 & 1 \\ \text{Frank} & 0 & 0 & 1 & 0 \\ \hline \text{Total} & 3 & 4 & 3 & 4 \end{array}$$ Naively, one might pick columns 2 and 4, which have the greatest totals. However, the solution which allows the most distinct people to attend is to pick columns 2 and 3. In practice, however, I have ~30 possible timeslots and over 100 students, and want to pick, say, five different timeslots for office hours. How do I pick the timeslots which maximise the number of distinct students who can attend?","I have the following (real world!) problem which is most easily described using an example. I ask my six students when the best time to hold office hours would be. I give them four options, and say that I'll hold two hours in total. The poll's results are as follows (1 means yes, 0 means no): $$\begin{array}{l|c|c|c|c}  \text{Name} & 9 \text{ am} & 10 \text{ am} & 11 \text{ am} & 12 \text{ pm} \\ \hline \text{Alice} & 1 & 1 & 0 & 0 \\  \text{Bob} & 1 & 1 & 0 & 1 \\  \text{Charlotte} & 1 & 1 & 0 & 1 \\  \text{Daniel} & 0 & 1 & 1 & 1 \\ \text{Eve} & 0 & 0 & 1 & 1 \\ \text{Frank} & 0 & 0 & 1 & 0 \\ \hline \text{Total} & 3 & 4 & 3 & 4 \end{array}$$ Naively, one might pick columns 2 and 4, which have the greatest totals. However, the solution which allows the most distinct people to attend is to pick columns 2 and 3. In practice, however, I have ~30 possible timeslots and over 100 students, and want to pick, say, five different timeslots for office hours. How do I pick the timeslots which maximise the number of distinct students who can attend?",,"['combinatorics', 'optimization']"
85,"Guessing a subset of $\{1,...,N\}$",Guessing a subset of,"\{1,...,N\}","I pick a random subset $S$ of $\{1,\ldots,N\}$, and you have to guess what it is. After each guess $G$, I tell you the number of elements in $G \cap S$. How many guesses do you need?","I pick a random subset $S$ of $\{1,\ldots,N\}$, and you have to guess what it is. After each guess $G$, I tell you the number of elements in $G \cap S$. How many guesses do you need?",,"['combinatorics', 'puzzle']"
86,How many distinct functions can be defined from set A to B?,How many distinct functions can be defined from set A to B?,,"In my discrete mathematics class our notes say that between set $A$ (having $6$ elements) and set $B$ (having $8$ elements), there are $8^6$ distinct functions that can be formed, in other words: $|B|^{|A|}$ distinct functions. But no explanation is offered and I can't seem to figure out why this is true. Can anyone elaborate?","In my discrete mathematics class our notes say that between set (having elements) and set (having elements), there are distinct functions that can be formed, in other words: distinct functions. But no explanation is offered and I can't seem to figure out why this is true. Can anyone elaborate?",A 6 B 8 8^6 |B|^{|A|},"['combinatorics', 'discrete-mathematics']"
87,How are we able to calculate specific numbers in the Fibonacci Sequence?,How are we able to calculate specific numbers in the Fibonacci Sequence?,,"I was reading up on the Fibonacci Sequence, $1,1,2,3,5,8,13,\ldots $ when I noticed some were able to calculate specific numbers. So far I've only figured out creating an array and counting to the value, which is incredibly simple, but I reckon I can't find any formula for calculating a Fibonacci number based on it's position. Is there a way to do this? If so, how are we able to apply these formulas to arrays?","I was reading up on the Fibonacci Sequence, when I noticed some were able to calculate specific numbers. So far I've only figured out creating an array and counting to the value, which is incredibly simple, but I reckon I can't find any formula for calculating a Fibonacci number based on it's position. Is there a way to do this? If so, how are we able to apply these formulas to arrays?","1,1,2,3,5,8,13,\ldots ","['combinatorics', 'generating-functions', 'fibonacci-numbers']"
88,How many scientists can survive?,How many scientists can survive?,,"Yesterday the aliens took 100 scientists from Earth as prisoners. They want to test how smart the humans are. The aliens made 101 headbands, numbered from 1 to 101. On the contest day, they throw away one of the headbands, and then from the remaining headbands randomly put one on each of the scientists. Then they line up the scientists in a queue in such a way that each person can see the numbers written on the headbands of the people who stand in front of him, but can't see his/her own number, nor the number of the scientists behind him. Then the aliens will force the scientists to guess their numbers, and after everybody finished saying a number, they will kill those who said a number different from what is written on their headbands. Note that each scientist can only say one number in the range 1-101, and no one can say a number that has already been said. Each scientist can independently choose when to speak their guess; there is no pre-set order on when they have to speak. As you are very smart, the scientists asked you to find a way to save the most of them. Find this way, and then prove it.","Yesterday the aliens took 100 scientists from Earth as prisoners. They want to test how smart the humans are. The aliens made 101 headbands, numbered from 1 to 101. On the contest day, they throw away one of the headbands, and then from the remaining headbands randomly put one on each of the scientists. Then they line up the scientists in a queue in such a way that each person can see the numbers written on the headbands of the people who stand in front of him, but can't see his/her own number, nor the number of the scientists behind him. Then the aliens will force the scientists to guess their numbers, and after everybody finished saying a number, they will kill those who said a number different from what is written on their headbands. Note that each scientist can only say one number in the range 1-101, and no one can say a number that has already been said. Each scientist can independently choose when to speak their guess; there is no pre-set order on when they have to speak. As you are very smart, the scientists asked you to find a way to save the most of them. Find this way, and then prove it.",,"['combinatorics', 'puzzle']"
89,"For which $n\in\Bbb N$ can we divide $\{1,2,3,...,3n\}$ into $n$ subsets each with $3$ elements such that in each subset $\{x,y,z\}$ we have $x+y=3z$?",For which  can we divide  into  subsets each with  elements such that in each subset  we have ?,"n\in\Bbb N \{1,2,3,...,3n\} n 3 \{x,y,z\} x+y=3z","For which $n\in \mathbb{N}$ can we divide the set $\{1,2,3,\ldots,3n\}$ into $n$ subsets each with $3$ elements such that in each subset $\{x,y,z\}$ we have $x+y=3z$ ? Since $x_i+y_i=3z_i$ for each subset $A_i=\{x_i,y_i,z_i\}$ , we have $$4\sum _{i=1}^n z_i=\sum _{i=1}^{3n}i = {3n(3n+1)\over 2}  \implies  8\mid n(3n+1) $$ so $n=8k$ or $n=8k-3$ . Now it is not difficult to see that if $k=1$ we have such partition. For $n=5$ we have: $$A_1= \{9,12,15\},  A_2= \{4,6,14\}, A_3= \{2,5,13\}, \\A_4= \{10,7,11\}, A_5= \{1,3,8\}$$ For $n=8$ we have: $$A_1= \{24,21,15\},  A_2= \{23,19,14\}, A_3= \{22,2,8\}, A_4= \{20,1,7\}, \\A_5= \{17,16,11\}, A_6= \{18,12,10\}, A_7= \{13,5,6\}, A_8= \{9,3,4\}$$ What about for $k\geq 2$ ? Some clever induction step? Or some ''well'' known configuration? Source: Serbia 1983, municipal round, 3. grade","For which can we divide the set into subsets each with elements such that in each subset we have ? Since for each subset , we have so or . Now it is not difficult to see that if we have such partition. For we have: For we have: What about for ? Some clever induction step? Or some ''well'' known configuration? Source: Serbia 1983, municipal round, 3. grade","n\in \mathbb{N} \{1,2,3,\ldots,3n\} n 3 \{x,y,z\} x+y=3z x_i+y_i=3z_i A_i=\{x_i,y_i,z_i\} 4\sum _{i=1}^n z_i=\sum _{i=1}^{3n}i = {3n(3n+1)\over 2}  \implies  8\mid n(3n+1)  n=8k n=8k-3 k=1 n=5 A_1= \{9,12,15\},  A_2= \{4,6,14\}, A_3= \{2,5,13\}, \\A_4= \{10,7,11\}, A_5= \{1,3,8\} n=8 A_1= \{24,21,15\},  A_2= \{23,19,14\}, A_3= \{22,2,8\}, A_4= \{20,1,7\}, \\A_5= \{17,16,11\}, A_6= \{18,12,10\}, A_7= \{13,5,6\}, A_8= \{9,3,4\} k\geq 2","['combinatorics', 'graph-theory', 'contest-math', 'constructive-mathematics', 'combinatorial-designs']"
90,Why is the number of ways of choosing $0$ items from $n$ items $1$?,Why is the number of ways of choosing  items from  items ?,0 n 1,It seems easy to grasp that number of ways of choosing $n$ items from $n$ items is 1. But I am unable to understand why is it 1 for choosing 0 items.,It seems easy to grasp that number of ways of choosing items from items is 1. But I am unable to understand why is it 1 for choosing 0 items.,n n,['combinatorics']
91,why is $\sum\limits_{k=1}^{n} k^m$ a polynomial with degree $m+1$ in $n$,why is  a polynomial with degree  in,\sum\limits_{k=1}^{n} k^m m+1 n,why is $\sum\limits_{k=1}^{n} k^m$ a polynomial with degree $m+1$  in $n$? I know this is well-known. But how to prove it rigorously? Even mathematical induction does not seem so straight-forward. Thanks.,why is $\sum\limits_{k=1}^{n} k^m$ a polynomial with degree $m+1$  in $n$? I know this is well-known. But how to prove it rigorously? Even mathematical induction does not seem so straight-forward. Thanks.,,"['combinatorics', 'polynomials', 'summation']"
92,You have to estimate $\binom{63}{19}$ in $2$ minutes to save your life.,You have to estimate  in  minutes to save your life.,\binom{63}{19} 2,"This is from the lecture notes in this course of discrete mathematics I am following. The professor is writing about how fast binomial coefficients grow. So, suppose you had 2 minutes to save your life and had to estimate, up to a factor of $100$, the value of, say, $\binom{63}{19}$. How would you do it? I will leave this (hopefully intriguing!) question hanging and maybe come back to the topic of efficiently estimating binomial coefficients later. Any ideas/hints on how to do it?","This is from the lecture notes in this course of discrete mathematics I am following. The professor is writing about how fast binomial coefficients grow. So, suppose you had 2 minutes to save your life and had to estimate, up to a factor of $100$, the value of, say, $\binom{63}{19}$. How would you do it? I will leave this (hopefully intriguing!) question hanging and maybe come back to the topic of efficiently estimating binomial coefficients later. Any ideas/hints on how to do it?",,"['combinatorics', 'discrete-mathematics', 'binomial-coefficients']"
93,Expected maximum number of unpaired socks,Expected maximum number of unpaired socks,,"Like all combinatoric problems, this one is probably equivalent to another, well-known one, but I haven't managed to find such an equivalent problem (and OEIS didn't help), so I offer this one as being possibly new and possibly interesting. Problem statement I have $2N$ socks in a laundry basket, and I am hanging them on the hot pipes to dry. To make life easier later, I want to hang them in pairs. Since it is dark where the pipes are, I adopt the following algorithm: Take a sock at random from the basket. If it matches one that is already on my arm, hang them both on the pipes: the one in my hand and the matching one taken from my arm. If it does not match one that is already on my arm, hang it on my arm with the others. Do this $2N$ times. The question is: How long does my arm have to be? Clearly, the minimum length is $1$ , for instance if the socks come out in the order $AABBCC$ . Equally clearly, the maximum length is $N$ , for instance if the socks come out as $ABCABC$ . But what is the likeliest length? Or the average length? Or what sort of distribution do the required lengths have? It turns out to be easiest to parameterise the results not by $2N$ , the number of socks, but by $2N-1$ , which I will call $M$ . The first few results (Notation: $n!!$ is the semifactorial, the factorial including only odd numbers; thus $7!!=7\times 5\times 3\times 1$ ). In each case I provide the frequency for each possible arm length, starting with a length of 1. I use frequencies rather than probabilities because they are easier to type, but you can get the probabilities by dividing by $M!!$ . $$ \begin{array}{c|rrrrr} M \\ \hline 1 & 1 \\ 3 & 1 & 2 \\ 5 & 1 & 8 & 6 \\ 7 & 1 & 30 & 50 & 24 \\ 9 & 1 & 148 & 340 & 336 & 120 \\ \end{array} $$ It would be good to know (for example) if these frequencies tend to some sort of known distribution as $M\to\infty$ , just as the binomial coefficients do. But, as I said at the beginning, this may just be a re-encoding of a known combinatorial problem, carrying a lot of previously worked out results along with it. I thought, for instance, of the lengths of random walks in $N$ dimensions with only one step forward and one step back being allowed in each dimension – but that looked too complicated to give any straightforward direction to follow. Background: methods In case it is interesting or helpful, I obtained the results above by means of a two-dimensional generating function, in which the coefficient of $y^n$ identified the arm length needed and the coefficient of $x^n$ identified how many socks had been retrieved at the [first] time that this length was reached. Calling the resulting generating function $A_M(x,y)$ , the recurrence I used was: $$A_M=MxyA_{M-2}+x^2(x-y)\frac\partial{\partial x}A_{M-2}+(1-x^2)xy$$ which is based on sound first principles and matches the results of manual calculation up to $M=5$ . Having found a polynomial, I substitute $x=1$ and the numbers in the table above are then the coefficients of the powers of $y$ . But, mathematics being close to comedy, all this elaboration may be an unnecessarily complicated way to get to a result too trivial to be found even in OEIS. Is it?","Like all combinatoric problems, this one is probably equivalent to another, well-known one, but I haven't managed to find such an equivalent problem (and OEIS didn't help), so I offer this one as being possibly new and possibly interesting. Problem statement I have socks in a laundry basket, and I am hanging them on the hot pipes to dry. To make life easier later, I want to hang them in pairs. Since it is dark where the pipes are, I adopt the following algorithm: Take a sock at random from the basket. If it matches one that is already on my arm, hang them both on the pipes: the one in my hand and the matching one taken from my arm. If it does not match one that is already on my arm, hang it on my arm with the others. Do this times. The question is: How long does my arm have to be? Clearly, the minimum length is , for instance if the socks come out in the order . Equally clearly, the maximum length is , for instance if the socks come out as . But what is the likeliest length? Or the average length? Or what sort of distribution do the required lengths have? It turns out to be easiest to parameterise the results not by , the number of socks, but by , which I will call . The first few results (Notation: is the semifactorial, the factorial including only odd numbers; thus ). In each case I provide the frequency for each possible arm length, starting with a length of 1. I use frequencies rather than probabilities because they are easier to type, but you can get the probabilities by dividing by . It would be good to know (for example) if these frequencies tend to some sort of known distribution as , just as the binomial coefficients do. But, as I said at the beginning, this may just be a re-encoding of a known combinatorial problem, carrying a lot of previously worked out results along with it. I thought, for instance, of the lengths of random walks in dimensions with only one step forward and one step back being allowed in each dimension – but that looked too complicated to give any straightforward direction to follow. Background: methods In case it is interesting or helpful, I obtained the results above by means of a two-dimensional generating function, in which the coefficient of identified the arm length needed and the coefficient of identified how many socks had been retrieved at the [first] time that this length was reached. Calling the resulting generating function , the recurrence I used was: which is based on sound first principles and matches the results of manual calculation up to . Having found a polynomial, I substitute and the numbers in the table above are then the coefficients of the powers of . But, mathematics being close to comedy, all this elaboration may be an unnecessarily complicated way to get to a result too trivial to be found even in OEIS. Is it?","2N 2N 1 AABBCC N ABCABC 2N 2N-1 M n!! 7!!=7\times 5\times 3\times 1 M!! 
\begin{array}{c|rrrrr}
M \\
\hline
1 & 1 \\
3 & 1 & 2 \\
5 & 1 & 8 & 6 \\
7 & 1 & 30 & 50 & 24 \\
9 & 1 & 148 & 340 & 336 & 120 \\
\end{array}
 M\to\infty N y^n x^n A_M(x,y) A_M=MxyA_{M-2}+x^2(x-y)\frac\partial{\partial x}A_{M-2}+(1-x^2)xy M=5 x=1 y","['combinatorics', 'generating-functions']"
94,Combinatorial Argument for Exponential and Logarithmic Function Being Inverse,Combinatorial Argument for Exponential and Logarithmic Function Being Inverse,,"Consider the following two generating functions: $$e^x=\sum_{n=0}^{\infty}\frac{x^n}{n!}$$ $$\log\left(\frac{1}{1-x}\right)=\sum_{n=1}^{\infty}\frac{x^n}{n}.$$ If we live in function-land, it's clear enough that there is an inverse relationship between these two things. In particular, $$e^{\log\left(\frac{1}{1-x}\right)}=1+x+x^2+x^3+\ldots$$ If we live in generating-function-land, this identity is really not so obvious. We can figure out that the coefficient of $x^n$ in $e^{\log\left(\frac{1}{1-x}\right)}$ is given as $$\sum_{a_1+\ldots+a_k=n}\frac{1}{a_1\cdot \cdots \cdot a_k}\cdot \frac{1}{k!}$$ where the sum runs over all ways to write $n$ as an ordered sum of positive integers. Supposedly, for each choice of $n$ , this thing sums to $1$ . I really don't see why. Is there a combinatorial argument that establishes this?","Consider the following two generating functions: If we live in function-land, it's clear enough that there is an inverse relationship between these two things. In particular, If we live in generating-function-land, this identity is really not so obvious. We can figure out that the coefficient of in is given as where the sum runs over all ways to write as an ordered sum of positive integers. Supposedly, for each choice of , this thing sums to . I really don't see why. Is there a combinatorial argument that establishes this?",e^x=\sum_{n=0}^{\infty}\frac{x^n}{n!} \log\left(\frac{1}{1-x}\right)=\sum_{n=1}^{\infty}\frac{x^n}{n}. e^{\log\left(\frac{1}{1-x}\right)}=1+x+x^2+x^3+\ldots x^n e^{\log\left(\frac{1}{1-x}\right)} \sum_{a_1+\ldots+a_k=n}\frac{1}{a_1\cdot \cdots \cdot a_k}\cdot \frac{1}{k!} n n 1,"['combinatorics', 'generating-functions']"
95,"On the equivalence relation $(a,b) \sim (c,d)\iff a+b=c+d$",On the equivalence relation,"(a,b) \sim (c,d)\iff a+b=c+d","Setup Let $A=\{a_1<a_2<\cdots<a_p\}$ and $B=\{b_1<b_2<\cdots<b_q\}$ be two finite sets of real numbers. Define an equivalence relation $\sim_{A,B}$ on $R_{p,q}=\{1,\dots,p\}\times\{1,\dots,q\}$ by setting $$(i,j)\sim_{A,B}(k,l)\iff a_i+b_j=a_k+b_l$$ and further define a total order on the set of equivalence classes $R_{p,q}/\sim_{A,B}$ by setting, for any equivalence classes $c,d$ and representatives $(i,j)\in c$ and $(k,l)\in d$ $$c<_{A,B}d\iff a_i+b_j<a_k+b_l$$ This question is about characterizing such equivalence relations $\sim_{A,B}$ and total orders $<_{A,B}$ . An equivalence relation $\sim$ on $R_{p,q}$ for which there exist $A,B$ with $\sim\;=\;\sim_{A,B}$ is called realizable . If $\sim$ is realizable, a total order $<$ on $R_{p,q}/\sim$ is called realizable if we can find $A,B$ with $\sim\;=\;\sim_{A,B}$ and $<\;=\;<_{A,B}$ . In both cases we say that the pair $(A,B)$ realizes $\sim$ (resp. $(\sim,<)$ .) Problem 1. Characterize realizable equivalence relations on $R_{p,q}$ . Problem 2. Suppose $\sim$ is realizable, can one find $\mathcal{A},\mathcal{B}\subset\Bbb{N}$ with $\sim\;=\;\sim_{\mathcal{A}, \mathcal{B}}$ ? Problem 3. Characterize realizable orders on $R_{p,q}/\sim$ (where $\sim$ is realizable.) Have these questions been considered / solved somewhere? Motivation As suggested in the comments I should give some context. The question arose while solving Problem 3.5 in David Eisenbud's Commutative Algebra with a View Toward Algebraic Geometry . In this problem one considers a graded ring $R=\bigoplus_{\gamma\in\Gamma} R_\gamma$ and a graded module $M=\bigoplus_{\lambda\in\Lambda}M_\lambda$ where $(\Gamma,+,0,<)$ is a totally ordered abelian monoid acting freely and compatibly on the totally ordered set $(\Lambda,<)$ . When working out the solution to this exercise I found myself drawing rectangles $R_{p,q}=\{1,\dots,p\}\times\{1,\dots,q\}$ and writing $\gamma_i+\lambda_j$ at position $(i,j)$ , where I had picked some subsets $\{\gamma_1<\gamma_2<\cdots<\gamma_p\}\subset\Gamma$ and $\{\lambda_1<\lambda_2<\cdots<\lambda_q\}\subset\Lambda$ . I was trying to figure out when $R_{\gamma_i}\cdot M_{\lambda_j}$ and $R_{\gamma_k}\cdot M_{\lambda_l}$ lie in the same $M_{\lambda}$ i.e. what and one say about the equivalence relation on $R_{p,q}$ defined by $(i,j)\sim(k,l)\iff\gamma_i+\lambda_j=\gamma_k+\lambda_l$ ? For concreteness I considered $\Gamma=\Lambda=\Bbb{N}$ or $\Gamma=\Lambda=\Bbb{R}_+$ but the problem makes sense in the broader context described by Eisenbud. I realized that there were some nice pictures to be drawn and that there was a not so obvious combinatorial problem of independent interest which is Problem 1 . Problem 2 arose when comparing the cases $\Bbb{N}$ and $\Bbb{R}_+$ : do we recover the same equivalence relations? Or would some not be realizable using integers? Problem 3 arose when drawing not only the blobs representing equivalence classes but the total order between them. What constraints must such orders satisfy? Necessary conditions and proposed characterizations While thinking about these problems I made some progress and was able to identify some necessary conditions for realizability of $\sim$ and $<$ . I wouldn't call any of these ""conjectures"" per se ... Is there an acceptable phrase for ""half-assed guesses""? Using terminology introduced below I wonder Observation and Guess 1. Any realizable equivalence relation $\sim$ on $R_{p,q}$ satisfies Inclination , Non Crossing and both Antidiagonal Propagation conditions. Conversely, are these conditions enough to characterize realizability of $\sim$ ? I'm not sure these conditions alone will do the job ... but they may. It seems in that for $p=2$ these conditions are sufficient (with an algorithm for finding explicit sets $A,B$ of rational numbers realizing $\sim$ .) Using terminology introduced below, there are some simple necessary Visibility conditions satisfies by realizable orders. There are also some less obvious but still simple Horizontal and Vertical Coherence conditions that such orders must satisfy. Observation and Guess 2. Any realizable order satisfies Visibility and both Horizontal and Vertical Coherence conditions. Conversely, are these enough to characterize realizability of $<$ ? Examples Here are some simple examples representing equivalence classes of $\sim_{A,B}$ and the order relation $<_{A,B}$ . In the pictures below we represent equivalence classes as ""blobs"" and signify $c<_{A,B}d$ by drawing an arrow from one equivalence class to another. Example 1. Let $A=\{0<1<2<\cdots<p-1\}$ and $B=\{0<p<2p<\cdots<(q-1)p\}$ . The equivalence classes of $\sim_{A,B}$ are singletons (this is uniqueness of euclidean division). Example 2. Let $A=\{0<1<2<\cdots<p-1\}$ and $B=\{0<1<2<\cdots<q-1\}$ . The equivalence classes of $\sim_{A,B}$ are antidiagonals. The answer to Problem 2 may be positive. Something along the lines of ""the condition is open and as long as one moves in tandem the things that are correlated (i.e. related by $\sim_{A,B}$ ) one can change coordinates to rational ones"" ought to ""prove"" the assertion. Maybe results from real algebraic geometry and semi-rational sets of degree 1 could also answer the question, but I know nothing about this material. Once one has rational $A$ and $B$ one can multiply everything by some integer and get the desired integer $\mathcal{A}$ and $\mathcal{B}$ . There may be a connection with configuration spaces. There certainly is a connection with geometry (existence of rational points on a rational algebraic set) and hyperplane arrangements but there, too, I'm not quite sure how to make it precise. Notation Given an equivalence class $c$ for $\sim_{A,B}$ we define its sum to be $a_i+b_j$ for any $(i,j)\in c$ . Given $(i,j)\in R_{p,q}$ we define the vertical set $V_i=\{(i,a)\mid a=1,\dots,q\}$ and the horizontal set $H_j=\{(a,j)\mid a=1,\dots,p\}$ . Given an equivalence class $c$ we define two subsets of the rectangle: $c^+$ (the green shaded region in  the picture below) is the set of all $(i,j)$ such that there exists some $(a,b)\in c$ with $(a,b)\neq(i,j)$ and $a\leq i$ and $b\leq j$ and $c^-$ (the orange shaded region in  the picture below) is the set of all $(i,j)$ such that there exists some $(a,b)\in c$ with $(a,b)\neq(i,j)$ and $a\geq i$ and $b\geq j$ . In the picture below the blue dots represent an equivalence class, the green region is $c^+$ and the orange region is $c^-$ . Inclination and Non Crossing (necessary conditions for realizability of $\sim$ ) The following are simple necessary conditions on an equivalence relation $\sim$ on $R_{p,q}$ for it to be of the form $\sim_{A,B}$ : Inclination. Two equivalent elements may not have the same first or second coordinate, i.e. any intersection $c\cap V_i$ and $c\cap H_j$ contains at most one element. Non Crossing. Given distinct equivalence classes $c,c'$ , the equivalence class $c'$ may not intersect both $c^+$ and $c^-$ . The necessity of both conditions is clear (the sum of an equivalence class meeting $c^+$ (resp. $c^-$ ) is greater (resp. smaller) than that of $c$ ). Actually the first condition is redundant since it is contained in the second. These conditions alone, however, don't characterize equivalence classes of the form $\sim_{A,B}$ , see the counter example below. Constraints and counter example Some configurations of equivalence classes have a constraining effect on the rest of the equivalence classes. Consider for example the case where we have an antidiagonal and subantidiagonal equivalence class (say with $p=q=n+1\geq 3$ ) i.e. $(1,n)\sim_{A,B}(2,n-1)\sim_{A,B}\cdots\sim_{A,B}(n,1)$ and $(1,n-1)\sim_{A,B}(2,n-2)\sim_{A,B}\cdots\sim_{A,B}(n-1,1)$ . Then $(i,j)\sim_{A,B}(k,l)\iff i+j=k+l$ . This implies that some equivalence relations are not of the form $\sim_{A,B}$ even though they satisfy the Inclination and Non Crossing conditions. For example the one depicted below: These can't represent the equivalence classes of a relation $\sim_{A,B}$ : the presence of the $3$ element equivalence classes would impose the relations $(1,3)\sim_{A,B}(2,2)$ and $(2,7)\sim_{A,B}(3,6)$ (both represented in blue on the right). We thus get a third necessary condition: Suppose $A'=\{a_1',\dots,a_r'\}\subset A$ and $B'=\{b_1',\dots,b_r'\}\subset B$ for $r\geq 3$ satisfy either that $(1,r-1)\sim_{A',B'}(2,r-2)\sim_{A',B'}\cdots\sim_{A',B'}(r-1,1)$ and $(1,r-2)\sim_{A',B'}(2,r-3)\sim_{A',B'}\cdots\sim_{A',B'}(r-2,1)$ or the analoguous condition with the antidiagonal above the principal antidiaongal. Then for all $1\leq i,j,k,l\leq r$ , $(i,j)\sim_{A',B'}(k,l)\iff i+j=k+l$ . It turns out we can improve on this condition. Antidiagonal Propagation (necessary condition for realizability of $\sim$ ) There are two versions of antidiagonal propagation: the one described below where we consider a main antidiagonal and one below it, but one should add the other possibility too, where the smaller antidiagonal is above the larger one. Antidiagonal Propagation. Suppose $A=\{a_1<a_2<\cdots<a_n\}$ , $B=\{b_1<b_2<\cdots<b_n\}$ with $n=mT+r$ with $m+1\geq 3$ , $T\geq 1$ , $\newcommand{\T}{[\![1,T]\!]}r\in\T$ . Suppose $$\newcommand{\n}{[\![1,n]\!]} \forall i,j\in\n, \quad \begin{cases} i+j=n+1:& a_i+b_j=C\\ i+j=n+1-T:& a_i+b_j=D \end{cases}$$ Then $$\newcommand{\n}{[\![1,n]\!]} \forall i,j,k,l\in\n, \quad \left\{ \begin{array}{l} i\equiv k\mod T,\\ j\equiv l\,\mod T,\\ \text{and }i+j=k+l \end{array} \right\} \implies (i,j)\sim_{A,B}(k,l)$$ In other words if $(i,j)$ and $(k,l)$ lie on the same antidiagonal and are a multiple of $T$ positions apart then they are $\sim_{A,B}$ -equivalent. Proof. The condition implies that, setting $\Delta=C-D$ , $$ \left\{ \begin{array}{lll} a_{1+kT}=a_1+k\Delta, & b_{1+kT}=b_1+k\Delta & ~\text{for }k=0,1,\dots,m-1,m\\ a_{2+kT}=a_2+k\Delta, & b_{2+kT}=b_2+k\Delta & ~\text{for }k=0,1,\dots,m-1,m\\ \qquad\vdots&\qquad\vdots&\qquad\vdots\\ a_{r+kT}=a_r+k\Delta, & b_{r+kT}=b_r+k\Delta & ~\text{for }k=0,1,\dots,m-1,m\\[2mm]\hline a_{r+1+kT}=a_{r+1}+k\Delta, & b_{r+1+kT}=b_{r+1}+k\Delta & ~\text{for }k=0,1,\dots,m-1\\ \qquad\vdots&\qquad\vdots&\qquad\vdots\\ a_{T+kT}=a_T+k\Delta, & b_{T+kT}=b_T+k\Delta & ~\text{for }k=0,1,\dots,m-1 \end{array} \right. $$ and thus if $i\equiv k\mod T$ , say $i=IT+\tau$ , $k=KT+\tau$ , $j\equiv l\mod T$ , say $j=JT+\sigma$ , $l=LT+\sigma$ , and $i+j=k+l$ i.e. $I+J=K+L$ , then $$ a_i+b_j = a_\tau+b_\sigma+(I+J)\Delta = a_\tau+b_\sigma+(K+L)\Delta = a_k+b_l $$ Here is an illustration. You have two large equivalence classes: You focus on the lines and columns that contain these equivalence classes and chuck the others out for now. Then there are some automatic equivalences depicted below. Equivalence classes are always subsets of antidiagonals, i.e. sets of the form $\{(i,j)\mid i+j=\mathrm{cst}\}$ and are color coded there. For extra clarity I've also added some markings to some of the equivalence classes. Funnily these look like designs straight out of the 1960ies and 1970ies. Visibility (necessary condition for realizability of $<$ ) There is an obvious necessary condition for a total order on $R_{p,q}/\sim$ , where $\sim$ is realizable, to be realizable. Suppose $c,d$ are equivalence classes. We say that $d$ is visibly greater than $c$ if there exists some $i$ with $V_i$ intersecting both $c$ and $d$ and such that if $(i,a)\in c$ and $(i,b)\in d$ then $a<b$ , or if there is some $j$ with $H_j$ intersecting both $c$ and $d$ and if $(a,j)\in c$ and $(b,j)\in d$ then $a<b$ . We allow ourselves to say that $c$ is visibly greater than $d$ if there is a chain $c=c_0,c_1,\dots,c_n=d$ with $c_i$ visibly greater than $c_{i-1}$ (i.e. we consider the transitive closure of the previously defined relation). Visibility. If $c$ is visibly greater than $d$ then $c<d$ . Visibility is a necessary property satisfied by realizable $<$ but it is not enough. Counter Example for realizability of $<$ There's more to this, yet. Consider the following discrete equivalence relation (i.e. equivalence classes are singletons) and total order $<$ . This total order satisfies all previously proposed conditions but can't arise as a $\sim_{A,B}$ and $<_{A,B}$ . Here's a simpler counter example showing that the individual $3\times 3$ orders are realized. Horizontal and Vertical Coherence (necessary conditions for realizability of $<$ ) The previous example shows that Visibility is not enough to characterize realizable orders. A problem arose where traversing too many horizontal gaps when following $<$ along equivalence classes led to inconsistencies in the values of those gaps. There are some horizontal and vertical consistency conditions that a realizable $<$ must satisfy. Let's start with Horizontal Consistency conditions. Take $(i,j)$ with $1<i$ . The sum of the classes containing $(i-1,j)$ and $(i,j)$ differ by $(\Delta a)_i=a_i-a_{i-1}$ . If we follow the $<$ -path from the $\sim$ -class containing $(i-1,j)$ to that containing $(i,j)$ we get $$(\Delta a)_i=\text{sum of positive terms, some $(\Delta a)_{k}$, some $(\Delta b)_l$}$$ from which we extract a condition $$H_{ij}:(\Delta a)_i>\text{sum of some $(\Delta a)_k$ and some $(\Delta b)_l$}$$ where the $(\Delta a)_k$ and $(\Delta b)_l$ to take into account are found using visibility relations. Similarly there are Vertical Consistency conditions of the same ilk $$V_{ij}:(\Delta b)_j>\text{sum of some $(\Delta a)_k$ and some $(\Delta b)_l$}$$ where again, the $(\Delta a)_k$ and $(\Delta b)_l$ that appear on the right hand side are deduced from visibility relations along the $<$ -path of equivalence classes linking the $\sim$ equivalence class containing $(i,j-1)$ to the $\sim$ equivalence class containing $(i,j)$ Beginnings of a positive solution for $p=2$ I believe the answer is positive to both questions at least when $p=2$ . WLOG we can take $a_1=0$ , $a_2=1$ and $b_0=0$ . There seems to exist an explicit algorithm to find a rational solution to the problem. I've illustrated it below. The necessary and sufficient condition on $\sim$ in the case $p=2$ seems to be that the lines associated to $2$ -element equivalence classes don't intersect. The algorithm is simple but I haven't conceptualized it really: You start with $b_1=0$ , the sum in the the lower left corner $(1,1)$ is thus $0$ since $a_1=0$ You put $1$ in the lower right corner $(2,1)$ , the sum is $1$ since $a_2=1$ If the lower right corner is part of a $2$ element equivalence class you put are forced to attribute $1$ to the other member $(1,r)$ . if $r>2$ you set $b_r=1$ . This is the case in the example below so you set $b_3=1$ . Then you are forced to put a $2$ in position $(2,r)$ since $a_2=1$ . You keep on going until you don't fall into a 2 element equivalence class. Then you have to insert values for $b_2,\dots,b_{r-1}$ . You pick them uniformly apart between $b_1=0$ and $b_r=1$ , that is $b_k=\frac{k-1}r$ . Then you are forced to inscribe $b_k+1$ on the right hand side. If the right hand side is part of a $2$ element equivalence class you are forced to set the same value for some new $b_s$ etc ... I hope the picture below (and the color coded rounds) are more explicit than the half-baked algorithm above. I don't know if this will work for $p\geq 3$ . A connection with topology $\newcommand{\O}{\mathcal{O}}\newcommand{\R}{\Bbb{R}}$ Fix $\sim$ an equivalence relation on $R_{p,q}$ and $<$ a total order on $R_{p,q}/\sim$ . Define $$\O_{p,q}[\sim,<]=\{(A,B)\in\O_p\times\O_q\text{ inducing $\sim$ and }<\}$$ where $\O_n=\{(x_1,\dots,x_n)\in\R^n\mid x_1<x_2<\cdots<x_n\}$ . Then $\O_{p,q}[\sim,<]$ is nonempty iff $(\sim,<)$ is realizable. When it is nonempty it is a convex cell. The $\O_{p,q}[\sim,<]$ partition $\O_p\times\O_q\simeq \R^{p+q}$ . It would be interesting to understand the poset structure associated to containment of closures of these. It seems natural to expect: the poset structure is closely related to refinements of $(\sim,<)$ ; maximal elements correspond to the discrete equivalence relation with the compatible total orders; $(\sim_{A,B},<_{A,B})$ for $A=\{1<2<\cdots<p\}$ and $B=\{1<2<\cdots<q\}$ is minimal but there may be other minimal elements. Let us put $\newcommand{\real}{\mathfrak{R}}\real_{p,q}$ the set of realizable pairs $(\sim,<)$ . We define an order relation on $\real_{p,q}$ by setting $$(\sim,<)\prec(\sim',<')\iff\text{there is an onto map of posets } (R_{p,q}/\sim',<')\to(R_{p,q}/\sim,<)$$ Lemma. Let $(\sim,<),(\sim',<')\in\real_{p,q}$ be realizable. The following are equivalent: $\overline{\O_{p,q}[\sim,<]}\subset\overline{\O_{p,q}[\sim',<']}$ $\O_{p,q}[\sim,<]\subset\overline{\O_{p,q}[\sim',<']}$ $\O_{p,q}[\sim,<]\cap\overline{\O_{p,q}[\sim',<']}\neq\emptyset$ $(\sim,<)\prec(\sim',<')$ Sketch of Proof. $1$ and $2$ are always equivalent and clearly imply $3$ . If we let $(A_n',B_n')\in\O_{p,q}[\sim',<']$ tend to $(A,B)\in\O_{p,q}[\sim,<]$ then what can happen is fusion of $<'$ -intervals of $\sim'$ -equivalence relations and $<$ and $<'$ have to be compatible which amounts to the existence of an onto homomorphism. This will prove $3\implies 4$ . To finish the proof we show that $4\implies 2$ . It is enough to notice that if $(\sim,<)\prec(\sim',<')$ and $(A,B)\in\O_{p,q}[\sim,<]$ and $(A',B')\in\O_{p,q}[\sim',<']$ then for all $t\in[0,1)$ , $$\qquad\qquad\qquad(tA+(1-t)A', tB+(1-t)B')\in\O_{p,q}[\sim',<'].\qquad\qquad\qquad\square$$ This has desirable homotopical consequences: one can use these cells to compute homotopy colimits of subcategories of the poset $(\real_{p,q,\prec})$ if one ever wanted to do so.","Setup Let and be two finite sets of real numbers. Define an equivalence relation on by setting and further define a total order on the set of equivalence classes by setting, for any equivalence classes and representatives and This question is about characterizing such equivalence relations and total orders . An equivalence relation on for which there exist with is called realizable . If is realizable, a total order on is called realizable if we can find with and . In both cases we say that the pair realizes (resp. .) Problem 1. Characterize realizable equivalence relations on . Problem 2. Suppose is realizable, can one find with ? Problem 3. Characterize realizable orders on (where is realizable.) Have these questions been considered / solved somewhere? Motivation As suggested in the comments I should give some context. The question arose while solving Problem 3.5 in David Eisenbud's Commutative Algebra with a View Toward Algebraic Geometry . In this problem one considers a graded ring and a graded module where is a totally ordered abelian monoid acting freely and compatibly on the totally ordered set . When working out the solution to this exercise I found myself drawing rectangles and writing at position , where I had picked some subsets and . I was trying to figure out when and lie in the same i.e. what and one say about the equivalence relation on defined by ? For concreteness I considered or but the problem makes sense in the broader context described by Eisenbud. I realized that there were some nice pictures to be drawn and that there was a not so obvious combinatorial problem of independent interest which is Problem 1 . Problem 2 arose when comparing the cases and : do we recover the same equivalence relations? Or would some not be realizable using integers? Problem 3 arose when drawing not only the blobs representing equivalence classes but the total order between them. What constraints must such orders satisfy? Necessary conditions and proposed characterizations While thinking about these problems I made some progress and was able to identify some necessary conditions for realizability of and . I wouldn't call any of these ""conjectures"" per se ... Is there an acceptable phrase for ""half-assed guesses""? Using terminology introduced below I wonder Observation and Guess 1. Any realizable equivalence relation on satisfies Inclination , Non Crossing and both Antidiagonal Propagation conditions. Conversely, are these conditions enough to characterize realizability of ? I'm not sure these conditions alone will do the job ... but they may. It seems in that for these conditions are sufficient (with an algorithm for finding explicit sets of rational numbers realizing .) Using terminology introduced below, there are some simple necessary Visibility conditions satisfies by realizable orders. There are also some less obvious but still simple Horizontal and Vertical Coherence conditions that such orders must satisfy. Observation and Guess 2. Any realizable order satisfies Visibility and both Horizontal and Vertical Coherence conditions. Conversely, are these enough to characterize realizability of ? Examples Here are some simple examples representing equivalence classes of and the order relation . In the pictures below we represent equivalence classes as ""blobs"" and signify by drawing an arrow from one equivalence class to another. Example 1. Let and . The equivalence classes of are singletons (this is uniqueness of euclidean division). Example 2. Let and . The equivalence classes of are antidiagonals. The answer to Problem 2 may be positive. Something along the lines of ""the condition is open and as long as one moves in tandem the things that are correlated (i.e. related by ) one can change coordinates to rational ones"" ought to ""prove"" the assertion. Maybe results from real algebraic geometry and semi-rational sets of degree 1 could also answer the question, but I know nothing about this material. Once one has rational and one can multiply everything by some integer and get the desired integer and . There may be a connection with configuration spaces. There certainly is a connection with geometry (existence of rational points on a rational algebraic set) and hyperplane arrangements but there, too, I'm not quite sure how to make it precise. Notation Given an equivalence class for we define its sum to be for any . Given we define the vertical set and the horizontal set . Given an equivalence class we define two subsets of the rectangle: (the green shaded region in  the picture below) is the set of all such that there exists some with and and and (the orange shaded region in  the picture below) is the set of all such that there exists some with and and . In the picture below the blue dots represent an equivalence class, the green region is and the orange region is . Inclination and Non Crossing (necessary conditions for realizability of ) The following are simple necessary conditions on an equivalence relation on for it to be of the form : Inclination. Two equivalent elements may not have the same first or second coordinate, i.e. any intersection and contains at most one element. Non Crossing. Given distinct equivalence classes , the equivalence class may not intersect both and . The necessity of both conditions is clear (the sum of an equivalence class meeting (resp. ) is greater (resp. smaller) than that of ). Actually the first condition is redundant since it is contained in the second. These conditions alone, however, don't characterize equivalence classes of the form , see the counter example below. Constraints and counter example Some configurations of equivalence classes have a constraining effect on the rest of the equivalence classes. Consider for example the case where we have an antidiagonal and subantidiagonal equivalence class (say with ) i.e. and . Then . This implies that some equivalence relations are not of the form even though they satisfy the Inclination and Non Crossing conditions. For example the one depicted below: These can't represent the equivalence classes of a relation : the presence of the element equivalence classes would impose the relations and (both represented in blue on the right). We thus get a third necessary condition: Suppose and for satisfy either that and or the analoguous condition with the antidiagonal above the principal antidiaongal. Then for all , . It turns out we can improve on this condition. Antidiagonal Propagation (necessary condition for realizability of ) There are two versions of antidiagonal propagation: the one described below where we consider a main antidiagonal and one below it, but one should add the other possibility too, where the smaller antidiagonal is above the larger one. Antidiagonal Propagation. Suppose , with with , , . Suppose Then In other words if and lie on the same antidiagonal and are a multiple of positions apart then they are -equivalent. Proof. The condition implies that, setting , and thus if , say , , , say , , and i.e. , then Here is an illustration. You have two large equivalence classes: You focus on the lines and columns that contain these equivalence classes and chuck the others out for now. Then there are some automatic equivalences depicted below. Equivalence classes are always subsets of antidiagonals, i.e. sets of the form and are color coded there. For extra clarity I've also added some markings to some of the equivalence classes. Funnily these look like designs straight out of the 1960ies and 1970ies. Visibility (necessary condition for realizability of ) There is an obvious necessary condition for a total order on , where is realizable, to be realizable. Suppose are equivalence classes. We say that is visibly greater than if there exists some with intersecting both and and such that if and then , or if there is some with intersecting both and and if and then . We allow ourselves to say that is visibly greater than if there is a chain with visibly greater than (i.e. we consider the transitive closure of the previously defined relation). Visibility. If is visibly greater than then . Visibility is a necessary property satisfied by realizable but it is not enough. Counter Example for realizability of There's more to this, yet. Consider the following discrete equivalence relation (i.e. equivalence classes are singletons) and total order . This total order satisfies all previously proposed conditions but can't arise as a and . Here's a simpler counter example showing that the individual orders are realized. Horizontal and Vertical Coherence (necessary conditions for realizability of ) The previous example shows that Visibility is not enough to characterize realizable orders. A problem arose where traversing too many horizontal gaps when following along equivalence classes led to inconsistencies in the values of those gaps. There are some horizontal and vertical consistency conditions that a realizable must satisfy. Let's start with Horizontal Consistency conditions. Take with . The sum of the classes containing and differ by . If we follow the -path from the -class containing to that containing we get from which we extract a condition where the and to take into account are found using visibility relations. Similarly there are Vertical Consistency conditions of the same ilk where again, the and that appear on the right hand side are deduced from visibility relations along the -path of equivalence classes linking the equivalence class containing to the equivalence class containing Beginnings of a positive solution for I believe the answer is positive to both questions at least when . WLOG we can take , and . There seems to exist an explicit algorithm to find a rational solution to the problem. I've illustrated it below. The necessary and sufficient condition on in the case seems to be that the lines associated to -element equivalence classes don't intersect. The algorithm is simple but I haven't conceptualized it really: You start with , the sum in the the lower left corner is thus since You put in the lower right corner , the sum is since If the lower right corner is part of a element equivalence class you put are forced to attribute to the other member . if you set . This is the case in the example below so you set . Then you are forced to put a in position since . You keep on going until you don't fall into a 2 element equivalence class. Then you have to insert values for . You pick them uniformly apart between and , that is . Then you are forced to inscribe on the right hand side. If the right hand side is part of a element equivalence class you are forced to set the same value for some new etc ... I hope the picture below (and the color coded rounds) are more explicit than the half-baked algorithm above. I don't know if this will work for . A connection with topology Fix an equivalence relation on and a total order on . Define where . Then is nonempty iff is realizable. When it is nonempty it is a convex cell. The partition . It would be interesting to understand the poset structure associated to containment of closures of these. It seems natural to expect: the poset structure is closely related to refinements of ; maximal elements correspond to the discrete equivalence relation with the compatible total orders; for and is minimal but there may be other minimal elements. Let us put the set of realizable pairs . We define an order relation on by setting Lemma. Let be realizable. The following are equivalent: Sketch of Proof. and are always equivalent and clearly imply . If we let tend to then what can happen is fusion of -intervals of -equivalence relations and and have to be compatible which amounts to the existence of an onto homomorphism. This will prove . To finish the proof we show that . It is enough to notice that if and and then for all , This has desirable homotopical consequences: one can use these cells to compute homotopy colimits of subcategories of the poset if one ever wanted to do so.","A=\{a_1<a_2<\cdots<a_p\} B=\{b_1<b_2<\cdots<b_q\} \sim_{A,B} R_{p,q}=\{1,\dots,p\}\times\{1,\dots,q\} (i,j)\sim_{A,B}(k,l)\iff a_i+b_j=a_k+b_l R_{p,q}/\sim_{A,B} c,d (i,j)\in c (k,l)\in d c<_{A,B}d\iff a_i+b_j<a_k+b_l \sim_{A,B} <_{A,B} \sim R_{p,q} A,B \sim\;=\;\sim_{A,B} \sim < R_{p,q}/\sim A,B \sim\;=\;\sim_{A,B} <\;=\;<_{A,B} (A,B) \sim (\sim,<) R_{p,q} \sim \mathcal{A},\mathcal{B}\subset\Bbb{N} \sim\;=\;\sim_{\mathcal{A}, \mathcal{B}} R_{p,q}/\sim \sim R=\bigoplus_{\gamma\in\Gamma} R_\gamma M=\bigoplus_{\lambda\in\Lambda}M_\lambda (\Gamma,+,0,<) (\Lambda,<) R_{p,q}=\{1,\dots,p\}\times\{1,\dots,q\} \gamma_i+\lambda_j (i,j) \{\gamma_1<\gamma_2<\cdots<\gamma_p\}\subset\Gamma \{\lambda_1<\lambda_2<\cdots<\lambda_q\}\subset\Lambda R_{\gamma_i}\cdot M_{\lambda_j} R_{\gamma_k}\cdot M_{\lambda_l} M_{\lambda} R_{p,q} (i,j)\sim(k,l)\iff\gamma_i+\lambda_j=\gamma_k+\lambda_l \Gamma=\Lambda=\Bbb{N} \Gamma=\Lambda=\Bbb{R}_+ \Bbb{N} \Bbb{R}_+ \sim < \sim R_{p,q} \sim p=2 A,B \sim < \sim_{A,B} <_{A,B} c<_{A,B}d A=\{0<1<2<\cdots<p-1\} B=\{0<p<2p<\cdots<(q-1)p\} \sim_{A,B} A=\{0<1<2<\cdots<p-1\} B=\{0<1<2<\cdots<q-1\} \sim_{A,B} \sim_{A,B} A B \mathcal{A} \mathcal{B} c \sim_{A,B} a_i+b_j (i,j)\in c (i,j)\in R_{p,q} V_i=\{(i,a)\mid a=1,\dots,q\} H_j=\{(a,j)\mid a=1,\dots,p\} c c^+ (i,j) (a,b)\in c (a,b)\neq(i,j) a\leq i b\leq j c^- (i,j) (a,b)\in c (a,b)\neq(i,j) a\geq i b\geq j c^+ c^- \sim \sim R_{p,q} \sim_{A,B} c\cap V_i c\cap H_j c,c' c' c^+ c^- c^+ c^- c \sim_{A,B} p=q=n+1\geq 3 (1,n)\sim_{A,B}(2,n-1)\sim_{A,B}\cdots\sim_{A,B}(n,1) (1,n-1)\sim_{A,B}(2,n-2)\sim_{A,B}\cdots\sim_{A,B}(n-1,1) (i,j)\sim_{A,B}(k,l)\iff i+j=k+l \sim_{A,B} \sim_{A,B} 3 (1,3)\sim_{A,B}(2,2) (2,7)\sim_{A,B}(3,6) A'=\{a_1',\dots,a_r'\}\subset A B'=\{b_1',\dots,b_r'\}\subset B r\geq 3 (1,r-1)\sim_{A',B'}(2,r-2)\sim_{A',B'}\cdots\sim_{A',B'}(r-1,1) (1,r-2)\sim_{A',B'}(2,r-3)\sim_{A',B'}\cdots\sim_{A',B'}(r-2,1) 1\leq i,j,k,l\leq r (i,j)\sim_{A',B'}(k,l)\iff i+j=k+l \sim A=\{a_1<a_2<\cdots<a_n\} B=\{b_1<b_2<\cdots<b_n\} n=mT+r m+1\geq 3 T\geq 1 \newcommand{\T}{[\![1,T]\!]}r\in\T \newcommand{\n}{[\![1,n]\!]}
\forall i,j\in\n,
\quad
\begin{cases}
i+j=n+1:& a_i+b_j=C\\
i+j=n+1-T:& a_i+b_j=D
\end{cases} \newcommand{\n}{[\![1,n]\!]}
\forall i,j,k,l\in\n,
\quad
\left\{
\begin{array}{l}
i\equiv k\mod T,\\
j\equiv l\,\mod T,\\
\text{and }i+j=k+l
\end{array}
\right\}
\implies (i,j)\sim_{A,B}(k,l) (i,j) (k,l) T \sim_{A,B} \Delta=C-D 
\left\{
\begin{array}{lll}
a_{1+kT}=a_1+k\Delta, & b_{1+kT}=b_1+k\Delta & ~\text{for }k=0,1,\dots,m-1,m\\
a_{2+kT}=a_2+k\Delta, & b_{2+kT}=b_2+k\Delta & ~\text{for }k=0,1,\dots,m-1,m\\
\qquad\vdots&\qquad\vdots&\qquad\vdots\\
a_{r+kT}=a_r+k\Delta, & b_{r+kT}=b_r+k\Delta & ~\text{for }k=0,1,\dots,m-1,m\\[2mm]\hline
a_{r+1+kT}=a_{r+1}+k\Delta, & b_{r+1+kT}=b_{r+1}+k\Delta & ~\text{for }k=0,1,\dots,m-1\\
\qquad\vdots&\qquad\vdots&\qquad\vdots\\
a_{T+kT}=a_T+k\Delta, & b_{T+kT}=b_T+k\Delta & ~\text{for }k=0,1,\dots,m-1
\end{array}
\right.
 i\equiv k\mod T i=IT+\tau k=KT+\tau j\equiv l\mod T j=JT+\sigma l=LT+\sigma i+j=k+l I+J=K+L 
a_i+b_j
=
a_\tau+b_\sigma+(I+J)\Delta
=
a_\tau+b_\sigma+(K+L)\Delta
=
a_k+b_l
 \{(i,j)\mid i+j=\mathrm{cst}\} < R_{p,q}/\sim \sim c,d d c i V_i c d (i,a)\in c (i,b)\in d a<b j H_j c d (a,j)\in c (b,j)\in d a<b c d c=c_0,c_1,\dots,c_n=d c_i c_{i-1} c d c<d < < < \sim_{A,B} <_{A,B} 3\times 3 < < < (i,j) 1<i (i-1,j) (i,j) (\Delta a)_i=a_i-a_{i-1} < \sim (i-1,j) (i,j) (\Delta a)_i=\text{sum of positive terms, some (\Delta a)_{k}, some (\Delta b)_l} H_{ij}:(\Delta a)_i>\text{sum of some (\Delta a)_k and some (\Delta b)_l} (\Delta a)_k (\Delta b)_l V_{ij}:(\Delta b)_j>\text{sum of some (\Delta a)_k and some (\Delta b)_l} (\Delta a)_k (\Delta b)_l < \sim (i,j-1) \sim (i,j) p=2 p=2 a_1=0 a_2=1 b_0=0 \sim p=2 2 b_1=0 (1,1) 0 a_1=0 1 (2,1) 1 a_2=1 2 1 (1,r) r>2 b_r=1 b_3=1 2 (2,r) a_2=1 b_2,\dots,b_{r-1} b_1=0 b_r=1 b_k=\frac{k-1}r b_k+1 2 b_s p\geq 3 \newcommand{\O}{\mathcal{O}}\newcommand{\R}{\Bbb{R}} \sim R_{p,q} < R_{p,q}/\sim \O_{p,q}[\sim,<]=\{(A,B)\in\O_p\times\O_q\text{ inducing \sim and }<\} \O_n=\{(x_1,\dots,x_n)\in\R^n\mid x_1<x_2<\cdots<x_n\} \O_{p,q}[\sim,<] (\sim,<) \O_{p,q}[\sim,<] \O_p\times\O_q\simeq \R^{p+q} (\sim,<) (\sim_{A,B},<_{A,B}) A=\{1<2<\cdots<p\} B=\{1<2<\cdots<q\} \newcommand{\real}{\mathfrak{R}}\real_{p,q} (\sim,<) \real_{p,q} (\sim,<)\prec(\sim',<')\iff\text{there is an onto map of posets }
(R_{p,q}/\sim',<')\to(R_{p,q}/\sim,<) (\sim,<),(\sim',<')\in\real_{p,q} \overline{\O_{p,q}[\sim,<]}\subset\overline{\O_{p,q}[\sim',<']} \O_{p,q}[\sim,<]\subset\overline{\O_{p,q}[\sim',<']} \O_{p,q}[\sim,<]\cap\overline{\O_{p,q}[\sim',<']}\neq\emptyset (\sim,<)\prec(\sim',<') 1 2 3 (A_n',B_n')\in\O_{p,q}[\sim',<'] (A,B)\in\O_{p,q}[\sim,<] <' \sim' < <' 3\implies 4 4\implies 2 (\sim,<)\prec(\sim',<') (A,B)\in\O_{p,q}[\sim,<] (A',B')\in\O_{p,q}[\sim',<'] t\in[0,1) \qquad\qquad\qquad(tA+(1-t)A', tB+(1-t)B')\in\O_{p,q}[\sim',<'].\qquad\qquad\qquad\square (\real_{p,q,\prec})","['combinatorics', 'discrete-mathematics', 'equivalence-relations', 'additive-combinatorics', 'configuration-space']"
96,How do I count the subsets of a set whose number of elements is divisible by 3? 4?,How do I count the subsets of a set whose number of elements is divisible by 3? 4?,,"Let $S$ be a set of size $n$.  There is an easy way to count the number of subsets with an even number of elements.  Algebraically, it comes from the fact that $\displaystyle \sum_{k=0}^{n} {n \choose k} = (1 + 1)^n$ while $\displaystyle \sum_{k=0}^{n} (-1)^k {n \choose k} = (1 - 1)^n$. It follows that $\displaystyle \sum_{k=0}^{n/2} {n \choose 2k} = 2^{n-1}$. A direct combinatorial proof is as follows: fix an element $s \in S$.  If a given subset has $s$ in it, add it in; otherwise, take it out.  This defines a bijection between the number of subsets with an even number of elements and the number of subsets with an odd number of elements. The analogous formulas for the subsets with a number of elements divisible by $3$ or $4$ are more complicated, and divide into cases depending on the residue of $n \bmod 6$ and $n \bmod 8$, respectively.  The algebraic derivations of these formulas are as follows (with $\omega$ a primitive third root of unity):  observe that $\displaystyle \sum_{k=0}^{n} \omega^k {n \choose k} = (1 + \omega)^n = (-\omega^2)^n$ while $\displaystyle \sum_{k=0}^{n} \omega^{2k} {n \choose k} = (1 + \omega^2)^n = (-\omega)^n$ and that $1 + \omega^k + \omega^{2k} = 0$ if $k$ is not divisible by $3$ and equals $3$ otherwise.  (This is a special case of the discrete Fourier transform.)  It follows that $\displaystyle \sum_{k=0}^{n/3} {n \choose 3k} = \frac{2^n + (-\omega)^n + (-\omega)^{2n}}{3}.$ $-\omega$ and $-\omega^2$ are sixth roots of unity, so this formula splits into six cases (or maybe three).  Similar observations about fourth roots of unity show that $\displaystyle \sum_{k=0}^{n/4} {n \choose 4k} = \frac{2^n + (1+i)^n + (1-i)^n}{4}$ where $1+i = \sqrt{2} e^{ \frac{\pi i}{4} }$ is a scalar multiple of an eighth root of unity, so this formula splits into eight cases (or maybe four). Question: Does anyone know a direct combinatorial proof of these identities?","Let $S$ be a set of size $n$.  There is an easy way to count the number of subsets with an even number of elements.  Algebraically, it comes from the fact that $\displaystyle \sum_{k=0}^{n} {n \choose k} = (1 + 1)^n$ while $\displaystyle \sum_{k=0}^{n} (-1)^k {n \choose k} = (1 - 1)^n$. It follows that $\displaystyle \sum_{k=0}^{n/2} {n \choose 2k} = 2^{n-1}$. A direct combinatorial proof is as follows: fix an element $s \in S$.  If a given subset has $s$ in it, add it in; otherwise, take it out.  This defines a bijection between the number of subsets with an even number of elements and the number of subsets with an odd number of elements. The analogous formulas for the subsets with a number of elements divisible by $3$ or $4$ are more complicated, and divide into cases depending on the residue of $n \bmod 6$ and $n \bmod 8$, respectively.  The algebraic derivations of these formulas are as follows (with $\omega$ a primitive third root of unity):  observe that $\displaystyle \sum_{k=0}^{n} \omega^k {n \choose k} = (1 + \omega)^n = (-\omega^2)^n$ while $\displaystyle \sum_{k=0}^{n} \omega^{2k} {n \choose k} = (1 + \omega^2)^n = (-\omega)^n$ and that $1 + \omega^k + \omega^{2k} = 0$ if $k$ is not divisible by $3$ and equals $3$ otherwise.  (This is a special case of the discrete Fourier transform.)  It follows that $\displaystyle \sum_{k=0}^{n/3} {n \choose 3k} = \frac{2^n + (-\omega)^n + (-\omega)^{2n}}{3}.$ $-\omega$ and $-\omega^2$ are sixth roots of unity, so this formula splits into six cases (or maybe three).  Similar observations about fourth roots of unity show that $\displaystyle \sum_{k=0}^{n/4} {n \choose 4k} = \frac{2^n + (1+i)^n + (1-i)^n}{4}$ where $1+i = \sqrt{2} e^{ \frac{\pi i}{4} }$ is a scalar multiple of an eighth root of unity, so this formula splits into eight cases (or maybe four). Question: Does anyone know a direct combinatorial proof of these identities?",,"['number-theory', 'combinatorics', 'binomial-coefficients']"
97,"Why the ""self-referential number"" function eventually fixes every point","Why the ""self-referential number"" function eventually fixes every point",,"Given an 8-digit decimal number $N$ , output a new 8-digit number $f(N)$ whose first digit is the number of zeroes in $N$ , the second the number of ones, ..., the seventh the number of sixes, and the eight the number of distinct digits of $N$ . The MoMath posted a puzzle that boils down to ""find the (unique) fixed point of $f$ "", and the solution given was to start with an arbitrary seed number $N$ and apply $f$ until one finds the fixed point. They comment on why there's no reason a priori this would work, and admit they're not sure why this works. Here are my related questions: Is there a way to see that $f$ has a unique fixed point? Is there a way to see that applying $f$ starting from any arbitrary seed $N$ , you get to the fixed point and don't get caught in a cycle when applying $f$ ? They remark that no matter what seed you pick, $f$ finds its fixed point relatively quickly (say within $10$ applications of $f$ ). Does anyone have a reason for why one should find the fixed point so soon? I don't have a good sense for how to bound how quickly this happens.","Given an 8-digit decimal number , output a new 8-digit number whose first digit is the number of zeroes in , the second the number of ones, ..., the seventh the number of sixes, and the eight the number of distinct digits of . The MoMath posted a puzzle that boils down to ""find the (unique) fixed point of "", and the solution given was to start with an arbitrary seed number and apply until one finds the fixed point. They comment on why there's no reason a priori this would work, and admit they're not sure why this works. Here are my related questions: Is there a way to see that has a unique fixed point? Is there a way to see that applying starting from any arbitrary seed , you get to the fixed point and don't get caught in a cycle when applying ? They remark that no matter what seed you pick, finds its fixed point relatively quickly (say within applications of ). Does anyone have a reason for why one should find the fixed point so soon? I don't have a good sense for how to bound how quickly this happens.",N f(N) N N f N f f f N f f 10 f,"['combinatorics', 'puzzle']"
98,Proving a jigsaw is possible,Proving a jigsaw is possible,,"This is an offshoot of this question . Suppose we have jigsaw puzzle pieces which are basically squares but where each side can be either straight, concave or convex. An example of three such pieces is shown below: It is clear that there can be $3^4=81$ different types of pieces. I was wondering, given one of each type (and with no rotations or flips allowed), whether it was possible to create a standard jigsaw puzzle, using just those $81$ pieces. By ""standard jigsaw puzzle"" I mean one of dimensions $m \times n$ where all perimeter sides are straight. A $1\times 81$ puzzle is clearly not possible as it would require all $81$ pieces to be straight on both the left and the right side. A similar argument holds for a $81\times 1$ puzzle. A $3\times 27$ puzzle would require $27$ pieces with a left straight side and $27$ pieces with a right straight edge, and while it is true that such two sets exist, they have an overlap of $9$ pieces. This is therefore not possible. As before, a similar argument holds for a $27\times 3$ puzzle. This leaves the $9\times 9$ possibility. A priori, I can see no reason this shouldn't be possible. And I think I have a proof that it is possible, which is what I would like your opinion on. Given a $9\times 9$ puzzle, we have the situation below: Each black side of a piece is fixed, but each green side of a piece represents a possible connection type. A connection type could be a ""straight - straight"", ""concave - convex"" or ""convex - concave"" type. It seems to me that if we run through every possible connection type for each piece, one of those scenarios must give a puzzle where each of the $81$ piece types is used exactly once. Am I right?","This is an offshoot of this question . Suppose we have jigsaw puzzle pieces which are basically squares but where each side can be either straight, concave or convex. An example of three such pieces is shown below: It is clear that there can be $3^4=81$ different types of pieces. I was wondering, given one of each type (and with no rotations or flips allowed), whether it was possible to create a standard jigsaw puzzle, using just those $81$ pieces. By ""standard jigsaw puzzle"" I mean one of dimensions $m \times n$ where all perimeter sides are straight. A $1\times 81$ puzzle is clearly not possible as it would require all $81$ pieces to be straight on both the left and the right side. A similar argument holds for a $81\times 1$ puzzle. A $3\times 27$ puzzle would require $27$ pieces with a left straight side and $27$ pieces with a right straight edge, and while it is true that such two sets exist, they have an overlap of $9$ pieces. This is therefore not possible. As before, a similar argument holds for a $27\times 3$ puzzle. This leaves the $9\times 9$ possibility. A priori, I can see no reason this shouldn't be possible. And I think I have a proof that it is possible, which is what I would like your opinion on. Given a $9\times 9$ puzzle, we have the situation below: Each black side of a piece is fixed, but each green side of a piece represents a possible connection type. A connection type could be a ""straight - straight"", ""concave - convex"" or ""convex - concave"" type. It seems to me that if we run through every possible connection type for each piece, one of those scenarios must give a puzzle where each of the $81$ piece types is used exactly once. Am I right?",,"['combinatorics', 'recreational-mathematics', 'puzzle']"
99,Putting many disks in the unit square,Putting many disks in the unit square,,"Consider a square of side equal to $1$. Prove that we can place inside the square a finite number of disjoint discs, with different radii of the form $1/k$ with $k$ a positive integer, such that the area of the remaining region is at most $0.0001$. If we consider all the discs of this form, their total area is $\sum_{k \geq 1}\displaystyle \pi \frac{1}{k^2} - \pi=\frac{\pi^3}{6}-\pi\simeq 2.02$ which is greater than the area of the square. (I subtracted $\pi$ because we cannot place a disc of radius $1$ inside the square). So the discs of this form can cover the square very well, but how can I prove that there is a disjoint family which leaves out a small portion of the area?","Consider a square of side equal to $1$. Prove that we can place inside the square a finite number of disjoint discs, with different radii of the form $1/k$ with $k$ a positive integer, such that the area of the remaining region is at most $0.0001$. If we consider all the discs of this form, their total area is $\sum_{k \geq 1}\displaystyle \pi \frac{1}{k^2} - \pi=\frac{\pi^3}{6}-\pi\simeq 2.02$ which is greater than the area of the square. (I subtracted $\pi$ because we cannot place a disc of radius $1$ inside the square). So the discs of this form can cover the square very well, but how can I prove that there is a disjoint family which leaves out a small portion of the area?",,"['geometry', 'combinatorics', 'packing-problem']"
