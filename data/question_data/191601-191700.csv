,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Finding $d^2y/dx^2$,Finding,d^2y/dx^2,A while ago I did a problem where I needed to find $\frac{d^2y}{dx^2}$ with $x=5+t^2$ and $y=t^2+t^3$. I found $dy/dx$ to be $1+\frac{3t}{2}$. But I never wrote down how I found  $\frac{d^2y}{dx^2}$. Or any other problems like it. The answer was $\frac{3}{4t}$. But I can't figure out how to get to that again. Can someone explain to me how you get to that?,A while ago I did a problem where I needed to find $\frac{d^2y}{dx^2}$ with $x=5+t^2$ and $y=t^2+t^3$. I found $dy/dx$ to be $1+\frac{3t}{2}$. But I never wrote down how I found  $\frac{d^2y}{dx^2}$. Or any other problems like it. The answer was $\frac{3}{4t}$. But I can't figure out how to get to that again. Can someone explain to me how you get to that?,,"['calculus', 'derivatives']"
1,Work differential: Why $\mathrm{d}W = f \mathrm{d}s$ not $\mathrm{d}W = f \mathrm{d}s + s\space \mathrm{d}f$,Work differential: Why  not,\mathrm{d}W = f \mathrm{d}s \mathrm{d}W = f \mathrm{d}s + s\space \mathrm{d}f,"Starting from the formula for work given a constant force $W = f s$, if you take the differential of both sides you would expect to get: $$\mathrm{d}W = f \mathrm{d}s + s \space \mathrm{d}f$$ by an application of the product rule on the right hand side when taking the differential. However, clearly this is incorrect, as by intuition the force across a segment $ds$ remains approximately constant, leading to the correct answer: $$\mathrm{d}W = f \mathrm{d}s$$ Why did directly taking the differential on both sides not work? I suspect it might be related to how $f$ is itself a function of $s$.","Starting from the formula for work given a constant force $W = f s$, if you take the differential of both sides you would expect to get: $$\mathrm{d}W = f \mathrm{d}s + s \space \mathrm{d}f$$ by an application of the product rule on the right hand side when taking the differential. However, clearly this is incorrect, as by intuition the force across a segment $ds$ remains approximately constant, leading to the correct answer: $$\mathrm{d}W = f \mathrm{d}s$$ Why did directly taking the differential on both sides not work? I suspect it might be related to how $f$ is itself a function of $s$.",,"['calculus', 'derivatives']"
2,Derivative of a negative order?,Derivative of a negative order?,,"Below, $\Delta$ means taking the derivative, $\frac{d}{dx}$. For $n\in\mathbb{Z}$, $n\geq 0$, we have  $$\Delta^n\sin{x}=\sin{(x+n\tau/4)} \\ \Delta^n\cos{x}=\cos{(x+n\tau/4)}$$ I found that out while thinking about $\sin$, $\cos$ and $\Delta$. I understand what $\Delta$ does (or at least, I think I do), but I'm wondering if it works if you raise it to a negative power, and if you can do it then what does it even mean to raise it to a negative power? I've heard that an integral is in some way the opposite to a differential, so maybe it would have something to do with integrals? I'd just like to know if you can raise $\Delta$ to a negative power and if so, what exactly it means to do so. I am not sure what tags would best suit this, so feel free to suggest some.","Below, $\Delta$ means taking the derivative, $\frac{d}{dx}$. For $n\in\mathbb{Z}$, $n\geq 0$, we have  $$\Delta^n\sin{x}=\sin{(x+n\tau/4)} \\ \Delta^n\cos{x}=\cos{(x+n\tau/4)}$$ I found that out while thinking about $\sin$, $\cos$ and $\Delta$. I understand what $\Delta$ does (or at least, I think I do), but I'm wondering if it works if you raise it to a negative power, and if you can do it then what does it even mean to raise it to a negative power? I've heard that an integral is in some way the opposite to a differential, so maybe it would have something to do with integrals? I'd just like to know if you can raise $\Delta$ to a negative power and if so, what exactly it means to do so. I am not sure what tags would best suit this, so feel free to suggest some.",,"['calculus', 'real-analysis', 'integration', 'derivatives', 'fractional-calculus']"
3,Integral by the given $u$-substitution,Integral by the given -substitution,u,Original equation $$\int\sqrt{e^t-5}dt \qquad u=\sqrt{e^t-5}$$ This is what I've done so far$$\int{u}dt$$ $$du=\frac{e^t}{2\sqrt{e^t-5}}dt$$$$dt=\frac{2\sqrt{e^t-5}}{e^t}du$$$$\int{u}{\frac{2\sqrt{e^t-5}}{e^t}}du$$ From this point on what would i need to do?,Original equation $$\int\sqrt{e^t-5}dt \qquad u=\sqrt{e^t-5}$$ This is what I've done so far$$\int{u}dt$$ $$du=\frac{e^t}{2\sqrt{e^t-5}}dt$$$$dt=\frac{2\sqrt{e^t-5}}{e^t}du$$$$\int{u}{\frac{2\sqrt{e^t-5}}{e^t}}du$$ From this point on what would i need to do?,,"['derivatives', 'indefinite-integrals', 'substitution']"
4,Evaluating an integral and differentiation,Evaluating an integral and differentiation,,"I'm trying to understand the math in a journal paper, but I'm stuck on figuring out one of the integrals.  Here is the paper called, ""Simultaneous optimization of the material properties and the topology of functionally graded structures"" http://www.sciencedirect.com/science/article/pii/S0010448508000249 Here is the problem equation part $$ \alpha*\int_{\Omega} \left|\nabla w(x)\right|^2dx \tag 1 $$ where the omega and dx indicate an area integral over a domain. Then the equation is differentiated with respect to omega Then the authors say they integrate by parts to get $$ -\int_{\Omega} \alpha \Delta\omega dx + \int_{\Gamma}\nabla\omega(x)*nds \tag 2  $$ I'm not sure how they got the equation. When I differentiate the first equation, I get this, but I'm not sure how integration by parts makes it simplier. It looks like they used the divergence theorem in there somewhere, but I'm not sure. $$ 2*\nabla(w)*\frac{d\nabla(w)} {dw} \tag 3 $$ Thanks for the help. It is equation 15 and 17 in the paper, if that helps. Anthony","I'm trying to understand the math in a journal paper, but I'm stuck on figuring out one of the integrals.  Here is the paper called, ""Simultaneous optimization of the material properties and the topology of functionally graded structures"" http://www.sciencedirect.com/science/article/pii/S0010448508000249 Here is the problem equation part $$ \alpha*\int_{\Omega} \left|\nabla w(x)\right|^2dx \tag 1 $$ where the omega and dx indicate an area integral over a domain. Then the equation is differentiated with respect to omega Then the authors say they integrate by parts to get $$ -\int_{\Omega} \alpha \Delta\omega dx + \int_{\Gamma}\nabla\omega(x)*nds \tag 2  $$ I'm not sure how they got the equation. When I differentiate the first equation, I get this, but I'm not sure how integration by parts makes it simplier. It looks like they used the divergence theorem in there somewhere, but I'm not sure. $$ 2*\nabla(w)*\frac{d\nabla(w)} {dw} \tag 3 $$ Thanks for the help. It is equation 15 and 17 in the paper, if that helps. Anthony",,"['integration', 'derivatives']"
5,What is wrong in my $f'(x)$?,What is wrong in my ?,f'(x),"We have $f:\mathbb{R}\rightarrow\mathbb{R}, f(x)=\frac{x^2-x+1}{x^2+x+1}$ and we need to find $f'(x)$. Here is all my steps: $$\begin{align}f'(x)&=\frac{(2x-1)(x^2+x+1)-(x^2-x+1)(2x+1)}{(x^2+x+1)^2}\\&=\frac{(2x-1)(x^2+1)-(2x+1)(x^2+1)}{(\cdots)^2}\\&=\frac{(x^2+1)(2x-1-2x-1)}{(\cdots)^2}\\&=\frac{2(1-x)(1+x)}{(\cdots)^2},\forall x\in\mathbb{R}\end{align}$$ But in my book they say that $f'(x)=\frac{2(x-1)(x+1)}{(x^2+x+1)^2}$. What is wrong in my method ?","We have $f:\mathbb{R}\rightarrow\mathbb{R}, f(x)=\frac{x^2-x+1}{x^2+x+1}$ and we need to find $f'(x)$. Here is all my steps: $$\begin{align}f'(x)&=\frac{(2x-1)(x^2+x+1)-(x^2-x+1)(2x+1)}{(x^2+x+1)^2}\\&=\frac{(2x-1)(x^2+1)-(2x+1)(x^2+1)}{(\cdots)^2}\\&=\frac{(x^2+1)(2x-1-2x-1)}{(\cdots)^2}\\&=\frac{2(1-x)(1+x)}{(\cdots)^2},\forall x\in\mathbb{R}\end{align}$$ But in my book they say that $f'(x)=\frac{2(x-1)(x+1)}{(x^2+x+1)^2}$. What is wrong in my method ?",,"['calculus', 'derivatives']"
6,What is the minimum degree of a polynomial for it to satisfy the following conditions?,What is the minimum degree of a polynomial for it to satisfy the following conditions?,,"This is the first part of a problem in the high-school exit exam of this year, in Italy. The differentiable function $y=f(x)$ has, for $x\in[-3,3]$, the graph $\Gamma$ below: $\Gamma$ exhibits horizontal tangents at $x=-1,1,2$. The areas of the regions $A,B,C$ and $D$ are respectively $2,3,3$ and $1$. (Also, note that we are supposed to deduce $f(-2)=f(0)=f(2)=0$ from the graph) If $f(x)$ is a polynomial, what is its minimum degree? Let me explain the issue with this. In fact, the question in bold is a reformulation of mine, while the original was In case $f(x)$ were expressible with a polynomial, what could be its minimum degree? The use of ""could"" has been criticized because in fact it does not exclude incorrect answers such as $0$. Then again, it is argued that such a lexical choice was due to the high difficulty (for a high-school student) of an answer to the more precise question ""what is its minimum degree?"", therefore ""necessary, not sufficient, accessible and not trivial conditions have been provided."" (there are several articles on the subject, in Italian) Nonetheless, there is an answer generally regarded as correct: $4$. Most students came up with that, and important websites provided it as well. Their reasoning is simple: since $f'(x)$ has $3$ zeros, its degree is at least $3$, and thus $f(x)$ is at least a quartic. However, it is also relatively simple, solving a system with enough of the information we are given, that the assumption $f(x)=a_4x^4+a_3x^3+a_2x^2+a_1x+a_0$ yields null coefficients. I personally didn't go further, but according to some articles I would have stopped at degree $9$, thus the answer to the question in bold; though this polynomial ""in any case doesn't abide by $\Gamma$"". Here's my objection. It is clearly specified that $\Gamma$ is the plot of $f(x)$ in the considered interval, hence the minimum degree cannot be that of a polynomial which does not abide by it. The polynomial $P(x)$ must satisfy \begin{cases} \int_{-3}^{-2}P(x)\,dx+2=\int_{-2}^0 P(x)\,dx-3=\int_0^2 P(x) \, dx + 3 = \int_2^3 P(x)\,dx+1=0 \\[6pt] P(-2)=P(0)=P(2)=0 \\[6pt] P'(-1)=P'(1)=P'(2)=0\\[6pt] P''(x)=0 \ \text{twice in $[-3,3]$, at the same points where $\Gamma$ changes concavity} \end{cases} Of course not knowing the exact coordinates of the inflection points is problematic, but in such an exam a strong resemblance would be enough. With these constraints, is there really no hope?","This is the first part of a problem in the high-school exit exam of this year, in Italy. The differentiable function $y=f(x)$ has, for $x\in[-3,3]$, the graph $\Gamma$ below: $\Gamma$ exhibits horizontal tangents at $x=-1,1,2$. The areas of the regions $A,B,C$ and $D$ are respectively $2,3,3$ and $1$. (Also, note that we are supposed to deduce $f(-2)=f(0)=f(2)=0$ from the graph) If $f(x)$ is a polynomial, what is its minimum degree? Let me explain the issue with this. In fact, the question in bold is a reformulation of mine, while the original was In case $f(x)$ were expressible with a polynomial, what could be its minimum degree? The use of ""could"" has been criticized because in fact it does not exclude incorrect answers such as $0$. Then again, it is argued that such a lexical choice was due to the high difficulty (for a high-school student) of an answer to the more precise question ""what is its minimum degree?"", therefore ""necessary, not sufficient, accessible and not trivial conditions have been provided."" (there are several articles on the subject, in Italian) Nonetheless, there is an answer generally regarded as correct: $4$. Most students came up with that, and important websites provided it as well. Their reasoning is simple: since $f'(x)$ has $3$ zeros, its degree is at least $3$, and thus $f(x)$ is at least a quartic. However, it is also relatively simple, solving a system with enough of the information we are given, that the assumption $f(x)=a_4x^4+a_3x^3+a_2x^2+a_1x+a_0$ yields null coefficients. I personally didn't go further, but according to some articles I would have stopped at degree $9$, thus the answer to the question in bold; though this polynomial ""in any case doesn't abide by $\Gamma$"". Here's my objection. It is clearly specified that $\Gamma$ is the plot of $f(x)$ in the considered interval, hence the minimum degree cannot be that of a polynomial which does not abide by it. The polynomial $P(x)$ must satisfy \begin{cases} \int_{-3}^{-2}P(x)\,dx+2=\int_{-2}^0 P(x)\,dx-3=\int_0^2 P(x) \, dx + 3 = \int_2^3 P(x)\,dx+1=0 \\[6pt] P(-2)=P(0)=P(2)=0 \\[6pt] P'(-1)=P'(1)=P'(2)=0\\[6pt] P''(x)=0 \ \text{twice in $[-3,3]$, at the same points where $\Gamma$ changes concavity} \end{cases} Of course not knowing the exact coordinates of the inflection points is problematic, but in such an exam a strong resemblance would be enough. With these constraints, is there really no hope?",,"['real-analysis', 'derivatives', 'polynomials', 'definite-integrals']"
7,Is the opposite of the Second Derivative Test also true?,Is the opposite of the Second Derivative Test also true?,,"Given the Second Derivative Test , one case says : If $f(x_0)''<0$, then $f$ has a local maximum at $x_0$. Is it also true that, if $f$ has a local maximum at $x_0$,  $f(x_0)'' < 0$ ?","Given the Second Derivative Test , one case says : If $f(x_0)''<0$, then $f$ has a local maximum at $x_0$. Is it also true that, if $f$ has a local maximum at $x_0$,  $f(x_0)'' < 0$ ?",,"['calculus', 'derivatives', 'optimization']"
8,Help with Calculus Newton's Method,Help with Calculus Newton's Method,,"Alright So I have kind of an interesting question involving Newton's method and have been at this for quite some time, and have come up with that it is not possible. I would like some input to if this is correct or not. Here is the question: Find an approximate solution, accurate to the nearest $0.0001$, using   Newton’s method, to the equation $$e^{-x^2}=x$$ Use an initial guess of  and submit your solution with the following   format – completing each cell until you stop. Any insight is appreciated. I have been using excel to obtain my answer. Thanks","Alright So I have kind of an interesting question involving Newton's method and have been at this for quite some time, and have come up with that it is not possible. I would like some input to if this is correct or not. Here is the question: Find an approximate solution, accurate to the nearest $0.0001$, using   Newton’s method, to the equation $$e^{-x^2}=x$$ Use an initial guess of  and submit your solution with the following   format – completing each cell until you stop. Any insight is appreciated. I have been using excel to obtain my answer. Thanks",,"['calculus', 'integration', 'derivatives']"
9,What does the 2nd degree derivative of a cubic Bezier curve actually represent?,What does the 2nd degree derivative of a cubic Bezier curve actually represent?,,"I have a $3D$ Bezier curve. Each co-ordinate along its path is defined by the equation: $$ f(t) = t^3 \bigl(a_2+3(c_1-c_2)-a_1\bigr) + 3t^2 (a_1-2c_1+c_2) + 3t(c_1-a_1) + a_1 $$ where $a_1, a_2$ are the anchor points and $c_1, c_2$ the control points (a.k.a. tangents at $a_1,a_2$). I want a $3D$ car model to move along this path. In order to orient the car, I use the 1st degree derivative which happens to be the curve's tangent (a.k.a. the function for the curve's direction ) : $$ f'(t) = 3t^2 \bigl(a_2+3(c_1-c_2)-a_1\bigr) + 6t(a_1-2c_1+c_2) + 3(c_1-a_1) $$ I orient the car using this function's output as direction - considering (0,1,0) as the ""up"" vector. It works a treat. Now, I wanted to make the car's front wheels turn left & right according to the car turning left & right. I thought I should use the 2nd degree derivative and use its output to orient the wheels. $$ f''(t) = 6t \bigl(a_2+3(c_1-c_2)-a_1\bigr) + 6(a_1-2c_1+c_2) $$ It turns out to be a nightmare, thus my concept must be wrong. My biggest query is that $f''(t)$ is non-zero even when the Bezier is degenerated to a straight line (all 4 points belonging to a straight line)! Since, in the case of a straight line, the direction $f'(t)$ is constant, shouldn't its derivative be always zero ? For example, for $a_1,a_2,c_1,c_2$ respectively: Vector3D(-4.01,0.00,-1.90) Vector3D(4.01,0.00,-1.90) Vector3D(-2.01,0.00,-1.90) Vector3D(2.01,0.00,-1.90) I get a constant $f'(t)$ and a variable $f''(t)$ !! f'(0.08)=Vector3D(-1.00,0.00,0.00) f''(0.08)=Vector3D(10.14,0.00,0.00) f'(0.11)=Vector3D(-1.00,0.00,0.00) f''(0.11)=Vector3D(9.42,0.00,0.00) f'(0.15)=Vector3D(-1.00,0.00,0.00) f''(0.15)=Vector3D(8.44,0.00,0.00) f'(0.18)=Vector3D(-1.00,0.00,0.00) f''(0.18)=Vector3D(7.69,0.00,0.00) f'(0.21)=Vector3D(-1.00,0.00,0.00) f''(0.21)=Vector3D(6.87,0.00,0.00) f'(0.24)=Vector3D(-1.00,0.00,0.00) f''(0.24)=Vector3D(6.16,0.00,0.00) f'(0.27)=Vector3D(-1.00,0.00,0.00) f''(0.27)=Vector3D(5.47,0.00,0.00) f'(0.30)=Vector3D(-1.00,0.00,0.00) f''(0.30)=Vector3D(4.70,0.00,0.00) f'(0.33)=Vector3D(-1.00,0.00,0.00) f''(0.33)=Vector3D(4.03,0.00,0.00) f'(0.36)=Vector3D(-1.00,0.00,0.00) f''(0.36)=Vector3D(3.37,0.00,0.00) f'(0.39)=Vector3D(-1.00,0.00,0.00) f''(0.39)=Vector3D(2.63,0.00,0.00) f'(0.42)=Vector3D(-1.00,0.00,0.00) f''(0.42)=Vector3D(1.99,0.00,0.00) f'(0.44)=Vector3D(-1.00,0.00,0.00) f''(0.44)=Vector3D(1.34,0.00,0.00) f'(0.47)=Vector3D(-1.00,0.00,0.00) f''(0.47)=Vector3D(0.62,0.00,0.00) f'(0.50)=Vector3D(-1.00,0.00,0.00) f''(0.50)=Vector3D(-0.02,0.00,0.00) f'(0.53)=Vector3D(-1.00,0.00,0.00) f''(0.53)=Vector3D(-0.74,0.00,0.00) f'(0.56)=Vector3D(-1.00,0.00,0.00) f''(0.56)=Vector3D(-1.38,0.00,0.00) f'(0.58)=Vector3D(-1.00,0.00,0.00) f''(0.58)=Vector3D(-2.03,0.00,0.00) f'(0.61)=Vector3D(-1.00,0.00,0.00) f''(0.61)=Vector3D(-2.67,0.00,0.00) f'(0.64)=Vector3D(-1.00,0.00,0.00) f''(0.64)=Vector3D(-3.41,0.00,0.00) f'(0.67)=Vector3D(-1.00,0.00,0.00) f''(0.67)=Vector3D(-4.07,0.00,0.00) f'(0.70)=Vector3D(-1.00,0.00,0.00) f''(0.70)=Vector3D(-4.74,0.00,0.00) f'(0.73)=Vector3D(-1.00,0.00,0.00) f''(0.73)=Vector3D(-5.51,0.00,0.00) f'(0.76)=Vector3D(-1.00,0.00,0.00) f''(0.76)=Vector3D(-6.20,0.00,0.00) f'(0.79)=Vector3D(-1.00,0.00,0.00) f''(0.79)=Vector3D(-6.91,0.00,0.00) f'(0.82)=Vector3D(-1.00,0.00,0.00) f''(0.82)=Vector3D(-7.74,0.00,0.00) f'(0.85)=Vector3D(-1.00,0.00,0.00) f''(0.85)=Vector3D(-8.49,0.00,0.00) f'(0.89)=Vector3D(-1.00,0.00,0.00) f''(0.89)=Vector3D(-9.27,0.00,0.00) f'(0.92)=Vector3D(-1.00,0.00,0.00) f''(0.92)=Vector3D(-10.19,0.00,0.00) f'(0.96)=Vector3D(-1.00,0.00,0.00) f''(0.96)=Vector3D(-11.06,0.00,0.00) f'(1.00)=Vector3D(-1.00,0.00,0.00) f''(1.00)=Vector3D(-11.98,0.00,0.00) EDIT: Questions: 1) What does the 2nd degree derivative of a cubic Bezier curve actually represent? (found in the title) 2) How can the 2nd degree derivative be variable when the 1st degree derivative is constant? OPTIONAL : 3) If my concept to use $f''(t)$ to orient the wheels is wrong, what is the theoretically correct way to orient them? EDIT 2 : The curve function and 1st derivative work a treat:","I have a $3D$ Bezier curve. Each co-ordinate along its path is defined by the equation: $$ f(t) = t^3 \bigl(a_2+3(c_1-c_2)-a_1\bigr) + 3t^2 (a_1-2c_1+c_2) + 3t(c_1-a_1) + a_1 $$ where $a_1, a_2$ are the anchor points and $c_1, c_2$ the control points (a.k.a. tangents at $a_1,a_2$). I want a $3D$ car model to move along this path. In order to orient the car, I use the 1st degree derivative which happens to be the curve's tangent (a.k.a. the function for the curve's direction ) : $$ f'(t) = 3t^2 \bigl(a_2+3(c_1-c_2)-a_1\bigr) + 6t(a_1-2c_1+c_2) + 3(c_1-a_1) $$ I orient the car using this function's output as direction - considering (0,1,0) as the ""up"" vector. It works a treat. Now, I wanted to make the car's front wheels turn left & right according to the car turning left & right. I thought I should use the 2nd degree derivative and use its output to orient the wheels. $$ f''(t) = 6t \bigl(a_2+3(c_1-c_2)-a_1\bigr) + 6(a_1-2c_1+c_2) $$ It turns out to be a nightmare, thus my concept must be wrong. My biggest query is that $f''(t)$ is non-zero even when the Bezier is degenerated to a straight line (all 4 points belonging to a straight line)! Since, in the case of a straight line, the direction $f'(t)$ is constant, shouldn't its derivative be always zero ? For example, for $a_1,a_2,c_1,c_2$ respectively: Vector3D(-4.01,0.00,-1.90) Vector3D(4.01,0.00,-1.90) Vector3D(-2.01,0.00,-1.90) Vector3D(2.01,0.00,-1.90) I get a constant $f'(t)$ and a variable $f''(t)$ !! f'(0.08)=Vector3D(-1.00,0.00,0.00) f''(0.08)=Vector3D(10.14,0.00,0.00) f'(0.11)=Vector3D(-1.00,0.00,0.00) f''(0.11)=Vector3D(9.42,0.00,0.00) f'(0.15)=Vector3D(-1.00,0.00,0.00) f''(0.15)=Vector3D(8.44,0.00,0.00) f'(0.18)=Vector3D(-1.00,0.00,0.00) f''(0.18)=Vector3D(7.69,0.00,0.00) f'(0.21)=Vector3D(-1.00,0.00,0.00) f''(0.21)=Vector3D(6.87,0.00,0.00) f'(0.24)=Vector3D(-1.00,0.00,0.00) f''(0.24)=Vector3D(6.16,0.00,0.00) f'(0.27)=Vector3D(-1.00,0.00,0.00) f''(0.27)=Vector3D(5.47,0.00,0.00) f'(0.30)=Vector3D(-1.00,0.00,0.00) f''(0.30)=Vector3D(4.70,0.00,0.00) f'(0.33)=Vector3D(-1.00,0.00,0.00) f''(0.33)=Vector3D(4.03,0.00,0.00) f'(0.36)=Vector3D(-1.00,0.00,0.00) f''(0.36)=Vector3D(3.37,0.00,0.00) f'(0.39)=Vector3D(-1.00,0.00,0.00) f''(0.39)=Vector3D(2.63,0.00,0.00) f'(0.42)=Vector3D(-1.00,0.00,0.00) f''(0.42)=Vector3D(1.99,0.00,0.00) f'(0.44)=Vector3D(-1.00,0.00,0.00) f''(0.44)=Vector3D(1.34,0.00,0.00) f'(0.47)=Vector3D(-1.00,0.00,0.00) f''(0.47)=Vector3D(0.62,0.00,0.00) f'(0.50)=Vector3D(-1.00,0.00,0.00) f''(0.50)=Vector3D(-0.02,0.00,0.00) f'(0.53)=Vector3D(-1.00,0.00,0.00) f''(0.53)=Vector3D(-0.74,0.00,0.00) f'(0.56)=Vector3D(-1.00,0.00,0.00) f''(0.56)=Vector3D(-1.38,0.00,0.00) f'(0.58)=Vector3D(-1.00,0.00,0.00) f''(0.58)=Vector3D(-2.03,0.00,0.00) f'(0.61)=Vector3D(-1.00,0.00,0.00) f''(0.61)=Vector3D(-2.67,0.00,0.00) f'(0.64)=Vector3D(-1.00,0.00,0.00) f''(0.64)=Vector3D(-3.41,0.00,0.00) f'(0.67)=Vector3D(-1.00,0.00,0.00) f''(0.67)=Vector3D(-4.07,0.00,0.00) f'(0.70)=Vector3D(-1.00,0.00,0.00) f''(0.70)=Vector3D(-4.74,0.00,0.00) f'(0.73)=Vector3D(-1.00,0.00,0.00) f''(0.73)=Vector3D(-5.51,0.00,0.00) f'(0.76)=Vector3D(-1.00,0.00,0.00) f''(0.76)=Vector3D(-6.20,0.00,0.00) f'(0.79)=Vector3D(-1.00,0.00,0.00) f''(0.79)=Vector3D(-6.91,0.00,0.00) f'(0.82)=Vector3D(-1.00,0.00,0.00) f''(0.82)=Vector3D(-7.74,0.00,0.00) f'(0.85)=Vector3D(-1.00,0.00,0.00) f''(0.85)=Vector3D(-8.49,0.00,0.00) f'(0.89)=Vector3D(-1.00,0.00,0.00) f''(0.89)=Vector3D(-9.27,0.00,0.00) f'(0.92)=Vector3D(-1.00,0.00,0.00) f''(0.92)=Vector3D(-10.19,0.00,0.00) f'(0.96)=Vector3D(-1.00,0.00,0.00) f''(0.96)=Vector3D(-11.06,0.00,0.00) f'(1.00)=Vector3D(-1.00,0.00,0.00) f''(1.00)=Vector3D(-11.98,0.00,0.00) EDIT: Questions: 1) What does the 2nd degree derivative of a cubic Bezier curve actually represent? (found in the title) 2) How can the 2nd degree derivative be variable when the 1st degree derivative is constant? OPTIONAL : 3) If my concept to use $f''(t)$ to orient the wheels is wrong, what is the theoretically correct way to orient them? EDIT 2 : The curve function and 1st derivative work a treat:",,"['derivatives', '3d', 'bezier-curve', 'orientation']"
10,"Does the function $f(x)=x$, $x\in (0,1)$ have a maximum and minimum value?","Does the function ,  have a maximum and minimum value?","f(x)=x x\in (0,1)","My book says that since we cannot determine the value of x when it is just less than 1 and just greater than 0, hence the function does not have a maxima or minima. But the fact confuses me because although we cannot determine those values, they still exist. Where am I going wrong?","My book says that since we cannot determine the value of x when it is just less than 1 and just greater than 0, hence the function does not have a maxima or minima. But the fact confuses me because although we cannot determine those values, they still exist. Where am I going wrong?",,"['calculus', 'derivatives']"
11,"How to show $\,f(x)=3e^{2x} -10x -7x^2\,$ has a minimum on $\,[0, 1]$",How to show  has a minimum on,"\,f(x)=3e^{2x} -10x -7x^2\, \,[0, 1]","I have been told that $$f(x)=3e^{2x} -10x -7x^2$$ and I need to show that it has a local minimum on the interval $[0,1]$. How would you show this?","I have been told that $$f(x)=3e^{2x} -10x -7x^2$$ and I need to show that it has a local minimum on the interval $[0,1]$. How would you show this?",,['calculus']
12,Examples of bounded continuous functions which are not differentiable,Examples of bounded continuous functions which are not differentiable,,"Most often examples given for bounded continuous functions which are not differentiable anywhere are fractals.If we include probabilistic fractals exact self-similarity is not required. Are their examples of functions which are bounded ,continuous, not differentiable anywhere and can not be modeled as fractals?","Most often examples given for bounded continuous functions which are not differentiable anywhere are fractals.If we include probabilistic fractals exact self-similarity is not required. Are their examples of functions which are bounded ,continuous, not differentiable anywhere and can not be modeled as fractals?",,"['derivatives', 'continuity', 'fractals']"
13,Limit of Derivative and Derivative of Limit,Limit of Derivative and Derivative of Limit,,"Assuming the integral is finite and $f$ is continuous, does this argument always work? If not, what do we need more? $\displaystyle \frac{d}{dx}\int_x^{\infty} f(t) \, \mathrm{d}t =$ $\displaystyle \frac{d}{dx} \lim_{u\to\infty}\int_x^u f(t) \, \mathrm{d}t =$ $\displaystyle \lim_{u\to\infty} \frac{d}{dx} \int_x^u f(t) \, \mathrm{d}t =$ $\displaystyle \lim_{u\to\infty} (-f(x)) = -f(x)$ Thank you.","Assuming the integral is finite and $f$ is continuous, does this argument always work? If not, what do we need more? $\displaystyle \frac{d}{dx}\int_x^{\infty} f(t) \, \mathrm{d}t =$ $\displaystyle \frac{d}{dx} \lim_{u\to\infty}\int_x^u f(t) \, \mathrm{d}t =$ $\displaystyle \lim_{u\to\infty} \frac{d}{dx} \int_x^u f(t) \, \mathrm{d}t =$ $\displaystyle \lim_{u\to\infty} (-f(x)) = -f(x)$ Thank you.",,"['calculus', 'integration', 'derivatives']"
14,derivative of $\ln((1+\beta)^x-1)$,derivative of,\ln((1+\beta)^x-1),How do I differentiate the term $\ln((1+\beta)^x-1)$ with respect to $x$? Is it possible to do it this way: $$\frac{1}{(1+\beta)^x-1}$$ But i get stuck if i do the normal differentiation.,How do I differentiate the term $\ln((1+\beta)^x-1)$ with respect to $x$? Is it possible to do it this way: $$\frac{1}{(1+\beta)^x-1}$$ But i get stuck if i do the normal differentiation.,,"['calculus', 'derivatives']"
15,Why is the derivative of the arccos the negative derivative of arcsin?,Why is the derivative of the arccos the negative derivative of arcsin?,,$$ \dfrac{d}{dx} \sin^{-1}x = \dfrac{1}{\sqrt{1-x^2}}$$ $$\dfrac{d}{dx} \cos^{-1}x = - \dfrac{d}{dx} \sin^{-1}x$$ What is the reason for this?,$$ \dfrac{d}{dx} \sin^{-1}x = \dfrac{1}{\sqrt{1-x^2}}$$ $$\dfrac{d}{dx} \cos^{-1}x = - \dfrac{d}{dx} \sin^{-1}x$$ What is the reason for this?,,"['trigonometry', 'derivatives']"
16,Derivability of a piecewise function,Derivability of a piecewise function,,"Let's say I have a continuous piecewise function of a single variable, so that $y = f(x)$ if $x < c$ and $y = g(x)$ if $x>=c$. Is it right to say that the derivative of the function at $x=c$ exists iff $f'(c-)=g'(c+)$, where $f'$ and $g'$ are obtained using derivative rules? This would seem reasonable to me, and I fail to find an example where this does not hold. However, my calculus professors have always taught me that the only way to evaluate a derivative of such a point is using the limit definition of the derivative.","Let's say I have a continuous piecewise function of a single variable, so that $y = f(x)$ if $x < c$ and $y = g(x)$ if $x>=c$. Is it right to say that the derivative of the function at $x=c$ exists iff $f'(c-)=g'(c+)$, where $f'$ and $g'$ are obtained using derivative rules? This would seem reasonable to me, and I fail to find an example where this does not hold. However, my calculus professors have always taught me that the only way to evaluate a derivative of such a point is using the limit definition of the derivative.",,"['derivatives', 'continuity']"
17,What is the best way to find the derivative of binomials to a power? ((x+x^{-1})^3)',What is the best way to find the derivative of binomials to a power? ((x+x^{-1})^3)',,"I came to a problem on my homework and I want to know the best way to solve it.  We are doing derivatives in Calculus.  I've got the following: $$H(x)=(x+x^{-1})^3$$ $$H'(x)=((x+x^{-1})^3)'$$ I am trying to avoid using the product theorem to do them 2 at a time, because that just sounds nasty. I am trying something like this: $$((x+x^{-1})^3)' = ((3x+3x^{-1})^2)'$$ But it feels like I'm doing something wrong..  Do I do the product theorem from here?","I came to a problem on my homework and I want to know the best way to solve it.  We are doing derivatives in Calculus.  I've got the following: $$H(x)=(x+x^{-1})^3$$ $$H'(x)=((x+x^{-1})^3)'$$ I am trying to avoid using the product theorem to do them 2 at a time, because that just sounds nasty. I am trying something like this: $$((x+x^{-1})^3)' = ((3x+3x^{-1})^2)'$$ But it feels like I'm doing something wrong..  Do I do the product theorem from here?",,"['calculus', 'derivatives']"
18,Intuition behind the definition of a derivative by Lang,Intuition behind the definition of a derivative by Lang,,"In Serge Lang's Introduction to Differentiable Manifolds he says that a function $f:U\to F$ is differentiable at a point $x_0\in U$ if there exists a linear map $\lambda$ of $E$ into $F$ such that, if we let $$f(x_0+y)=f(x_0)+\lambda y+\varphi(y)$$ for small $y$, then $\varphi$ is tangent to $0$. Tangent to $0$ is defined as follows: A real valued function of a real variable, defined on some neighborhood of $0$ is said to be $o(t)$ if $$\lim_{t\to 0}o(t)/t=0.$$ Let $E,F$ be two vector spaces, and $\varphi$ a mapping of a neighborhood of $0$ in $E$ into $F$. We say that $\varphi$ is tangent to $0$ if, given a neighborhood $W$ of $0$ in $F$, there exists a neighborhood $V$ of $0$ in $E$ such that $$\varphi(tV)\subset o(t)W$$ What is the intuition behind defining it in this manner?","In Serge Lang's Introduction to Differentiable Manifolds he says that a function $f:U\to F$ is differentiable at a point $x_0\in U$ if there exists a linear map $\lambda$ of $E$ into $F$ such that, if we let $$f(x_0+y)=f(x_0)+\lambda y+\varphi(y)$$ for small $y$, then $\varphi$ is tangent to $0$. Tangent to $0$ is defined as follows: A real valued function of a real variable, defined on some neighborhood of $0$ is said to be $o(t)$ if $$\lim_{t\to 0}o(t)/t=0.$$ Let $E,F$ be two vector spaces, and $\varphi$ a mapping of a neighborhood of $0$ in $E$ into $F$. We say that $\varphi$ is tangent to $0$ if, given a neighborhood $W$ of $0$ in $F$, there exists a neighborhood $V$ of $0$ in $E$ such that $$\varphi(tV)\subset o(t)W$$ What is the intuition behind defining it in this manner?",,"['derivatives', 'differential-topology']"
19,How to prove that $d \sin(x)/dx = \cos(x)$ without circular logic such as L'Hôpital's rule?,How to prove that  without circular logic such as L'Hôpital's rule?,d \sin(x)/dx = \cos(x),How do I prove that the derivative of $\sin$ is $\cos$ without resorting to L'Hôpital's rule (circular logic) ? This part is easy: $$ \begin{align*} \sin'(x) &= \lim_{\Delta x \to 0} \frac{\sin(x + \Delta x) - \sin(x)}{\Delta x}  \\ \sin'(x) &= \lim_{\Delta x \to 0} \frac{\cos(x)\sin(\Delta x) + \sin(x) \cos(\Delta x) - \sin(x)}{\Delta x}  \\ \sin'(x) &= \lim_{\Delta x \to 0} \left(\cos(x)\frac{\sin(\Delta x)}{\Delta x} + \sin(x)\frac{\cos(\Delta x) - 1}{\Delta x}\right)  \\ \end{align*} $$ but where do I go from here?,How do I prove that the derivative of $\sin$ is $\cos$ without resorting to L'Hôpital's rule (circular logic) ? This part is easy: $$ \begin{align*} \sin'(x) &= \lim_{\Delta x \to 0} \frac{\sin(x + \Delta x) - \sin(x)}{\Delta x}  \\ \sin'(x) &= \lim_{\Delta x \to 0} \frac{\cos(x)\sin(\Delta x) + \sin(x) \cos(\Delta x) - \sin(x)}{\Delta x}  \\ \sin'(x) &= \lim_{\Delta x \to 0} \left(\cos(x)\frac{\sin(\Delta x)}{\Delta x} + \sin(x)\frac{\cos(\Delta x) - 1}{\Delta x}\right)  \\ \end{align*} $$ but where do I go from here?,,['derivatives']
20,Can't understand how to find a limit $f(x)=\sqrt{|x|}$,Can't understand how to find a limit,f(x)=\sqrt{|x|},"I need to find when $f(x)=\sqrt{|x|}$ is differentiable and find the derivative. I found that it's differentiable when $x \neq 0$ and $f'(x)=\frac{1}{2 \sqrt{x}} \ for \ x>0$ and $f'(x)=-\frac{1}{2 \sqrt{-x}} \ for \ x<0$. The problem is when checking the limits in 0. It is very clear to me that $\lim_{x \rightarrow 0^+} \frac{\sqrt{x}}{x}=\infty$, but when I want to find the limit as x goes to $0^-$, I get the expression $lim_{x \rightarrow 0^-} \frac{\sqrt{-x}}{x}$ which is going to $-\infty$, but I don't know how to show it. Please help, thank you!","I need to find when $f(x)=\sqrt{|x|}$ is differentiable and find the derivative. I found that it's differentiable when $x \neq 0$ and $f'(x)=\frac{1}{2 \sqrt{x}} \ for \ x>0$ and $f'(x)=-\frac{1}{2 \sqrt{-x}} \ for \ x<0$. The problem is when checking the limits in 0. It is very clear to me that $\lim_{x \rightarrow 0^+} \frac{\sqrt{x}}{x}=\infty$, but when I want to find the limit as x goes to $0^-$, I get the expression $lim_{x \rightarrow 0^-} \frac{\sqrt{-x}}{x}$ which is going to $-\infty$, but I don't know how to show it. Please help, thank you!",,"['calculus', 'derivatives']"
21,Examples where derivatives are used (outside of math classes),Examples where derivatives are used (outside of math classes),,"I want to know what is the use of derivatives in our daily life. I have searched it on google but i haven't find any accurate answer. I think it is mostly used in Maths but I want to know its use in other departments i.e physics, chemistry, biology and economics.","I want to know what is the use of derivatives in our daily life. I have searched it on google but i haven't find any accurate answer. I think it is mostly used in Maths but I want to know its use in other departments i.e physics, chemistry, biology and economics.",,"['calculus', 'soft-question', 'derivatives', 'big-list']"
22,Generalizing the Product Rule,Generalizing the Product Rule,,"How would I go about generalizing the product rule to the product of $n$ functions $\psi_1(x), \ \psi_2(x), ..., \ \psi_n(x)$? That is, I'm hoping to obtain an expression for $$ \frac{d}{dx} \prod_{j = 1}^n \psi_j(x) $$","How would I go about generalizing the product rule to the product of $n$ functions $\psi_1(x), \ \psi_2(x), ..., \ \psi_n(x)$? That is, I'm hoping to obtain an expression for $$ \frac{d}{dx} \prod_{j = 1}^n \psi_j(x) $$",,"['calculus', 'derivatives', 'products']"
23,Derivative: chain rule applied to $\cos(\pi x)$,Derivative: chain rule applied to,\cos(\pi x),What is the derivative of the function $f(x)= \cos(\pi x)$? I found the derivative to be $f^{\prime}(x)= -\pi\sin(\pi x)$. Am I correct? Can you show me how to find the answer step by step? This is a homework question: What is $x$ equal to if $-\pi\sin(\pi x)=0$?,What is the derivative of the function $f(x)= \cos(\pi x)$? I found the derivative to be $f^{\prime}(x)= -\pi\sin(\pi x)$. Am I correct? Can you show me how to find the answer step by step? This is a homework question: What is $x$ equal to if $-\pi\sin(\pi x)=0$?,,"['calculus', 'derivatives']"
24,Derivative of Standard Normal Inverse,Derivative of Standard Normal Inverse,,"How can I calculate the derivative of the standard normal inverse. I think the derivative of $\Phi^{-1}(x)$ is $$\frac{1}{\phi(\Phi^{-1}(x))}.$$ I would like to know how to find the derivative of $$\Phi^{-1} \left(\frac{x}{c}\right),$$ where $c$ is a known constant. Any help would be much appreciated.","How can I calculate the derivative of the standard normal inverse. I think the derivative of $\Phi^{-1}(x)$ is $$\frac{1}{\phi(\Phi^{-1}(x))}.$$ I would like to know how to find the derivative of $$\Phi^{-1} \left(\frac{x}{c}\right),$$ where $c$ is a known constant. Any help would be much appreciated.",,"['probability', 'derivatives', 'inverse']"
25,"Proving $f(x) = x^2 \sin(1/x)$, $f(0)=0$ is differentiable at $0$, with derivative $f'(0)= 0$ at zero [duplicate]","Proving ,  is differentiable at , with derivative  at zero [duplicate]",f(x) = x^2 \sin(1/x) f(0)=0 0 f'(0)= 0,"This question already has answers here : Show that the function $g(x) = x^2 \sin(\frac{1}{x}) ,(g(0) = 0)$ is everywhere differentiable and that $g′(0) = 0$ (2 answers) Let $f(x) = x^2 \sin (1/x^2),\,x\ne 0,$ and $ f(0)=0.$ Prove $f$ is differentiable on $\mathbb R$ (1 answer) Closed 11 years ago . I need a solution for this question. I've been trying out this question for days and I haven't been able to find out its solution yet. And some explanation would help too. Show that the function f defined by:   $$f(x):= \begin{cases}   x^2\sin(1/x) &:\text{if $x \ne 0$} \\   0 &:\text{if $x=0$} \end{cases}$$   is differentiable at $x=0$, and that $f'(0)=0$.","This question already has answers here : Show that the function $g(x) = x^2 \sin(\frac{1}{x}) ,(g(0) = 0)$ is everywhere differentiable and that $g′(0) = 0$ (2 answers) Let $f(x) = x^2 \sin (1/x^2),\,x\ne 0,$ and $ f(0)=0.$ Prove $f$ is differentiable on $\mathbb R$ (1 answer) Closed 11 years ago . I need a solution for this question. I've been trying out this question for days and I haven't been able to find out its solution yet. And some explanation would help too. Show that the function f defined by:   $$f(x):= \begin{cases}   x^2\sin(1/x) &:\text{if $x \ne 0$} \\   0 &:\text{if $x=0$} \end{cases}$$   is differentiable at $x=0$, and that $f'(0)=0$.",,['derivatives']
26,derivative of zero order bessel function of first kind,derivative of zero order bessel function of first kind,,I want to know the derivative of the zero order bessel function of first kind ($J_0(x)$). and how it changes with $x$ and I also would like to know the roots of this function. could anyone help?,I want to know the derivative of the zero order bessel function of first kind ($J_0(x)$). and how it changes with $x$ and I also would like to know the roots of this function. could anyone help?,,['derivatives']
27,If derivative $f'$ of a function $f$ satisfies $0 < C \leq f'(x)$ for all $x$ then $f$ is bijective,If derivative  of a function  satisfies  for all  then  is bijective,f' f 0 < C \leq f'(x) x f,"Proposition If derivative $f'$ of a function $f:\mathbb R\to\mathbb R$ satisfies $0 < C \leq f'(x)$ for all $x\in\mathbb R$ then $f$ is bijective. It is clear that if there exist $a,b\in\mathbb R$ satisfy $f(a)=f(b)$ then there exist $c\in(a,b)$ such that $f'(c) = 0$ and this contradicts that $f'(x) > 0$, so necessarily $f$ is injective. However, I don't know how to prove that $f$ is surjective. I would be pleased if you give me a hint, thanks in advance.","Proposition If derivative $f'$ of a function $f:\mathbb R\to\mathbb R$ satisfies $0 < C \leq f'(x)$ for all $x\in\mathbb R$ then $f$ is bijective. It is clear that if there exist $a,b\in\mathbb R$ satisfy $f(a)=f(b)$ then there exist $c\in(a,b)$ such that $f'(c) = 0$ and this contradicts that $f'(x) > 0$, so necessarily $f$ is injective. However, I don't know how to prove that $f$ is surjective. I would be pleased if you give me a hint, thanks in advance.",,"['real-analysis', 'derivatives']"
28,How to find a local maximum and local minimum of a function?,How to find a local maximum and local minimum of a function?,,"My question is related to how to find a local maximum and local minimum. As far I know, for the first we should find  derivative of the function and set it  to zero. For exmaple, suppose our function is given by: $$f(x)=x^3+4x^2+5x+6$$ We first differentiate it: $$f'(x)=3x^2+8x+5$$ For optimal points of $3x^2+8x+5=0$ we find $x_1=-1$  and  $x_2=-5/3$. For local maximum and/or local minimum, we should choose neighbor points of critical  points, for $x_1=-1$, we choose  two points, $-2$  and $-0$, and after we insert into first equation: $$f(-2)=4$$ $$f(-1)=-8+16-10+6=4$$ $$f(0)=6$$ So, it means that  points  $x_1=-1$ is local minimum for this case, right?  Because it has minimum  output among  $-2$ and  $-0$, right? For this  case, $f(-2)=f(-1)$, but does it change something? Just consider for first point, so if  $f(-1)<f(-2)$, then it means that it would be local minim as well, but if $f(-2)>(-1)$, then it  would be saddle point.","My question is related to how to find a local maximum and local minimum. As far I know, for the first we should find  derivative of the function and set it  to zero. For exmaple, suppose our function is given by: $$f(x)=x^3+4x^2+5x+6$$ We first differentiate it: $$f'(x)=3x^2+8x+5$$ For optimal points of $3x^2+8x+5=0$ we find $x_1=-1$  and  $x_2=-5/3$. For local maximum and/or local minimum, we should choose neighbor points of critical  points, for $x_1=-1$, we choose  two points, $-2$  and $-0$, and after we insert into first equation: $$f(-2)=4$$ $$f(-1)=-8+16-10+6=4$$ $$f(0)=6$$ So, it means that  points  $x_1=-1$ is local minimum for this case, right?  Because it has minimum  output among  $-2$ and  $-0$, right? For this  case, $f(-2)=f(-1)$, but does it change something? Just consider for first point, so if  $f(-1)<f(-2)$, then it means that it would be local minim as well, but if $f(-2)>(-1)$, then it  would be saddle point.",,"['calculus', 'derivatives']"
29,why constant derivatives?,why constant derivatives?,,"Really simple question here. Consider two functions $f(x)$ and $g(y)$ . Then, why if $$\frac{d f(x)}{dx} = \frac{d g(y)}{dy} ,$$ then both derivatives are constant? Thank you all very much","Really simple question here. Consider two functions and . Then, why if then both derivatives are constant? Thank you all very much","f(x) g(y) \frac{d f(x)}{dx} = \frac{d g(y)}{dy} ,","['calculus', 'derivatives']"
30,When would a function be differentiable at the end point?,When would a function be differentiable at the end point?,,"Let's say the function is defined on $[x,y]$ I just don't know what to think of here. I think that every function is not differentiable at the end points because they are points! How can the limit exist from the other side of the end point? More importantly, when would a function be differentiable at the end point? Can anyone provide an example and explain how he came with it?","Let's say the function is defined on $[x,y]$ I just don't know what to think of here. I think that every function is not differentiable at the end points because they are points! How can the limit exist from the other side of the end point? More importantly, when would a function be differentiable at the end point? Can anyone provide an example and explain how he came with it?",,"['calculus', 'derivatives']"
31,Simple partial differentiation $x = r\cos\theta$ and $y = r\sin\theta$,Simple partial differentiation  and,x = r\cos\theta y = r\sin\theta,"If \begin{align} x &= r\cos\theta,\\ y &= r\sin\theta, \end{align} find $$\dfrac{\partial^2\theta}{\partial{x}\partial{y}}.$$ How can I find this partial derivative? I need to prove that $$ \frac{\partial^2\theta}{\partial{x}\partial{y}} = -\frac{\cos2\theta}{r^2}.$$","If \begin{align} x &= r\cos\theta,\\ y &= r\sin\theta, \end{align} find $$\dfrac{\partial^2\theta}{\partial{x}\partial{y}}.$$ How can I find this partial derivative? I need to prove that $$ \frac{\partial^2\theta}{\partial{x}\partial{y}} = -\frac{\cos2\theta}{r^2}.$$",,"['derivatives', 'polar-coordinates']"
32,A limit question related to the nth derivative of a function,A limit question related to the nth derivative of a function,,"This evening I thought of the following question that isn't related to homework, but it's a question that seems very challenging to me, and I take some interest in it. Let's consider the following function: $$ f(x)= \left(\frac{\sin x}{x}\right)^\frac{x}{\sin x}$$ I wonder what is the first derivative (1st, 2nd, 3rd ...)  such that $\lim\limits_{x\to0} f^{(n)}(x)$ is different from $0$ or $+\infty$, $-\infty$, where $f^{(n)}(x)$ is the nth derivative of $f(x)$ (if such a case is possible). I tried to use W|A, but it simply fails to work out such limits. Maybe i need the W|A Pro version.","This evening I thought of the following question that isn't related to homework, but it's a question that seems very challenging to me, and I take some interest in it. Let's consider the following function: $$ f(x)= \left(\frac{\sin x}{x}\right)^\frac{x}{\sin x}$$ I wonder what is the first derivative (1st, 2nd, 3rd ...)  such that $\lim\limits_{x\to0} f^{(n)}(x)$ is different from $0$ or $+\infty$, $-\infty$, where $f^{(n)}(x)$ is the nth derivative of $f(x)$ (if such a case is possible). I tried to use W|A, but it simply fails to work out such limits. Maybe i need the W|A Pro version.",,"['calculus', 'real-analysis', 'derivatives']"
33,First principle proof for derivatives of $\arcsin{x}$,First principle proof for derivatives of,\arcsin{x},One popular proof is to take $\sin{y} = x$ and then differentiate on both sides. But how do you prove it from first principles? Help very much appreciated.,One popular proof is to take $\sin{y} = x$ and then differentiate on both sides. But how do you prove it from first principles? Help very much appreciated.,,"['calculus', 'derivatives']"
34,Finding $\frac{d}{dx} \frac{x^2}{y}$,Finding,\frac{d}{dx} \frac{x^2}{y},"$$\frac{d}{dx} \frac{x^2}{y}$$ According to Wolframalpha I ""factor out constants"" $$\frac{\frac{d}{dx} x^2}{y}$$ Then I will get $\frac{2x}{y}$. Is that right? But $y$ is not a constant? What I did actually (quotient rule got me stuck) The actual question is ""Find $\frac{d^2y}{dx^2}$ of $2x^3 - 3y^2 = 8$"" I got $$\frac{dy}{dx} = \frac{x^2}{y}$$ Then $$\frac{d^2y}{dx^2} = \frac{y \cdot 2x - x^2 \cdot \frac{dy}{dx}}{y^2}$$ $$ = \frac{2xy - x^2 \cdot \frac{x^2}{y}}{y^2}$$ $$ = \frac{2xy^2 - x^4}{y^3}$$ Is this correct? It doesn't look like a ""simple"" answer (or whats in wolfram)?","$$\frac{d}{dx} \frac{x^2}{y}$$ According to Wolframalpha I ""factor out constants"" $$\frac{\frac{d}{dx} x^2}{y}$$ Then I will get $\frac{2x}{y}$. Is that right? But $y$ is not a constant? What I did actually (quotient rule got me stuck) The actual question is ""Find $\frac{d^2y}{dx^2}$ of $2x^3 - 3y^2 = 8$"" I got $$\frac{dy}{dx} = \frac{x^2}{y}$$ Then $$\frac{d^2y}{dx^2} = \frac{y \cdot 2x - x^2 \cdot \frac{dy}{dx}}{y^2}$$ $$ = \frac{2xy - x^2 \cdot \frac{x^2}{y}}{y^2}$$ $$ = \frac{2xy^2 - x^4}{y^3}$$ Is this correct? It doesn't look like a ""simple"" answer (or whats in wolfram)?",,['derivatives']
35,Why is tangent space independent of choice of coordinates?,Why is tangent space independent of choice of coordinates?,,"This is a question about Spviak's Calculus on Manifolds , page 115. Let $M$ be a $k$-dimension manifold in $\mathbb{R}^n$ and let $f:W\rightarrow \mathbb{R}^n$ be a coordinate system around $x=f(a)$, where $W$ is some open subset of $\mathbb{R}^k$. We know $f'(a)$ has rank $k$, by definition), so the linear transformation $f_*:\mathbb{R}^k_a\rightarrow \mathbb{R}^n_x$ is 1-1, and the image $f(\mathbb{R}^k_a)$ is $k$-dimensional. If $g:V\rightarrow \mathbb{R}^n$ is another coordinate system, with $x=g(b)$. Spviak claims: $$g_*(\mathbb{R}^K_b) = f_*(f^{-1}\circ g)_*(\mathbb{R}^K_b) = f_*(\mathbb{R}^K_a)$$ I realize the chain rule is being used, but I am having trouble following the implications. In particular: 1) I think the chain rule gives the first implication, but I don't see why we can take the derivative of $f^{-1}$, because it is a map between spaces of different dimensions (so the inverse function theorem doesn't apply). 2) The last equality implies that $(f^{-1}\circ g)'$ is the identity. I do not see why this is true (but I do see that $(f^{-1}\circ g)(b)=a$, so the subscript works out).","This is a question about Spviak's Calculus on Manifolds , page 115. Let $M$ be a $k$-dimension manifold in $\mathbb{R}^n$ and let $f:W\rightarrow \mathbb{R}^n$ be a coordinate system around $x=f(a)$, where $W$ is some open subset of $\mathbb{R}^k$. We know $f'(a)$ has rank $k$, by definition), so the linear transformation $f_*:\mathbb{R}^k_a\rightarrow \mathbb{R}^n_x$ is 1-1, and the image $f(\mathbb{R}^k_a)$ is $k$-dimensional. If $g:V\rightarrow \mathbb{R}^n$ is another coordinate system, with $x=g(b)$. Spviak claims: $$g_*(\mathbb{R}^K_b) = f_*(f^{-1}\circ g)_*(\mathbb{R}^K_b) = f_*(\mathbb{R}^K_a)$$ I realize the chain rule is being used, but I am having trouble following the implications. In particular: 1) I think the chain rule gives the first implication, but I don't see why we can take the derivative of $f^{-1}$, because it is a map between spaces of different dimensions (so the inverse function theorem doesn't apply). 2) The last equality implies that $(f^{-1}\circ g)'$ is the identity. I do not see why this is true (but I do see that $(f^{-1}\circ g)(b)=a$, so the subscript works out).",,"['manifolds', 'derivatives']"
36,Derivative of exponential function $\frac{d}{dx}a^x$,Derivative of exponential function,\frac{d}{dx}a^x,"I am trying to compute simple derivatives of simple functions, but I got stuck on $\frac{d}{dx}a^x=(\ln{a})a^x$. I suppose the proof is a simple corollary of $\frac{d}{dx}e^x=e^x$, but I am unable to find it. Can anybody help me?","I am trying to compute simple derivatives of simple functions, but I got stuck on $\frac{d}{dx}a^x=(\ln{a})a^x$. I suppose the proof is a simple corollary of $\frac{d}{dx}e^x=e^x$, but I am unable to find it. Can anybody help me?",,"['calculus', 'exponentiation', 'derivatives']"
37,"Is $f(x) = \sin x$ the unique function satisfying all five: $f(0)=0;\ f'(0)=1;\ f(\pi/2)=1;\ f'(\pi/2)=0;\ -1\leq f''(x)\leq 0$ for $x\in [0,\pi/2] ?$",Is  the unique function satisfying all five:  for,"f(x) = \sin x f(0)=0;\ f'(0)=1;\ f(\pi/2)=1;\ f'(\pi/2)=0;\ -1\leq f''(x)\leq 0 x\in [0,\pi/2] ?","I would like to prove or find a counter-example to the following proposition (which I came up with), please. Suppose $f:[0,\pi/2]\to [0,1]$ is twice differentiable in the interval $[0,\pi/2]$ . Suppose further that, $f(0) = 0;$ $f'(0) = 1;$ $f(\pi/2) = 1;$ $f'(\pi/2) = 0.$ Then, if $-1\leq f''(x)\leq 0$ for $x\in [0,\pi/2],$ then $f(x) = \sin  x.$ I think if we did not impose the condition that $-1\leq f''(x)\leq 0$ for $x\in [0,\pi/2],$ then I think we can find a counter-example like: $f(x)$ is convex - i.e. $f''(x)>0$ - on $[0,0.1]$ , and then concave all the way to the point $(\pi/2,1).$ I first tried supposing that $f(x)$ is a quadratic, and got a contradiction, so no quadratics satisfy the first four conditions. I then tried supposing that $f(x)$ is a cubic, and determined that the only cubic satisfying the first four conditions is, $$ f(x) = \frac{4\pi-16}{\pi^3}x^3 + \frac{12 - 4\pi}{\pi^2} x^2 + x, $$ but then $f''(\pi/2) = -1.158\ldots < -1.$ So we can rule out cubics also. I'm not sure quite where to go from here, other than more tedious calculations trying quartics, quintics etc. Does anyone have the answer to this question off hand, or have any ideas for a counter-example or an affirmative proof of the proposition by contradiction?","I would like to prove or find a counter-example to the following proposition (which I came up with), please. Suppose is twice differentiable in the interval . Suppose further that, Then, if for then I think if we did not impose the condition that for then I think we can find a counter-example like: is convex - i.e. - on , and then concave all the way to the point I first tried supposing that is a quadratic, and got a contradiction, so no quadratics satisfy the first four conditions. I then tried supposing that is a cubic, and determined that the only cubic satisfying the first four conditions is, but then So we can rule out cubics also. I'm not sure quite where to go from here, other than more tedious calculations trying quartics, quintics etc. Does anyone have the answer to this question off hand, or have any ideas for a counter-example or an affirmative proof of the proposition by contradiction?","f:[0,\pi/2]\to [0,1] [0,\pi/2] f(0) = 0; f'(0) = 1; f(\pi/2) = 1; f'(\pi/2) = 0. -1\leq f''(x)\leq 0 x\in [0,\pi/2], f(x) = \sin
 x. -1\leq f''(x)\leq 0 x\in [0,\pi/2], f(x) f''(x)>0 [0,0.1] (\pi/2,1). f(x) f(x)  f(x) = \frac{4\pi-16}{\pi^3}x^3 + \frac{12 - 4\pi}{\pi^2} x^2 + x,  f''(\pi/2) = -1.158\ldots < -1.","['calculus', 'derivatives', 'trigonometry', 'polynomials']"
38,Calculate $I(\alpha)=\int_0^\pi \frac{x}{1+\sin(x)\cos(\alpha)}dx$ [closed],Calculate  [closed],I(\alpha)=\int_0^\pi \frac{x}{1+\sin(x)\cos(\alpha)}dx,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 26 days ago . Improve this question I have tried to derive it 1 and 2 times but I do not see any relation to the original $I(\alpha)$ , is there any breakthrough that I am missing?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 26 days ago . Improve this question I have tried to derive it 1 and 2 times but I do not see any relation to the original , is there any breakthrough that I am missing?",I(\alpha),"['integration', 'derivatives']"
39,$|f(x)-f(y)| < |g(x) - g(y)|$ when $|f'| < |g'|$,when,|f(x)-f(y)| < |g(x) - g(y)| |f'| < |g'|,"Let $f, g: \mathbb R \rightarrow \mathbb R$ be differentiable. Suppose that $|f'(x)| < |g'(x)|$ for all $x \in \mathbb R$ . The goal is to prove that $|f(x) - f(y)| < |g(x) - g(y)|$ for $x, y \in \mathbb R$ with $x \ne y$ . By the Darboux theorem, $g'$ must be either positive or negative everywhere. With extra assumptions that $f, g$ are absolutely continuous on every compact interval (e.g. holds when $f', g'$ are bounded on every compact interval), the desired result follows by the fundamental theorem of Calculus for Lebesgue integrals. Here are my questions: (1) Is the statement true without the additional assumptions? If so, what would be a proof? (2) In case it is true, is there a ""short"" proof of the statement without using any measure theory? I would appreciate any hint or reference.","Let be differentiable. Suppose that for all . The goal is to prove that for with . By the Darboux theorem, must be either positive or negative everywhere. With extra assumptions that are absolutely continuous on every compact interval (e.g. holds when are bounded on every compact interval), the desired result follows by the fundamental theorem of Calculus for Lebesgue integrals. Here are my questions: (1) Is the statement true without the additional assumptions? If so, what would be a proof? (2) In case it is true, is there a ""short"" proof of the statement without using any measure theory? I would appreciate any hint or reference.","f, g: \mathbb R \rightarrow \mathbb R |f'(x)| < |g'(x)| x \in \mathbb R |f(x) - f(y)| < |g(x) - g(y)| x, y \in \mathbb R x \ne y g' f, g f', g'","['real-analysis', 'derivatives']"
40,What kind of functions satisfy $y'(x) =x'(y)$,What kind of functions satisfy,y'(x) =x'(y),"What kind of functions satisfy the property $y'(x) =x'(y)$ ? I don't know if it makes sense. For example, for $x^2+y^2=1$ , $$\begin{align}y'(x)& = \frac{-x}{\sqrt{1-x^2}}\\ x'(y) & = \frac{-y}{\sqrt{1-y^2}}\end{align}$$ What I am trying to ask is that what kind of other functions are symmetric like this, as it only changes the $x$ and $y$ between $\frac{dx}{dy}$ and $\frac{dy}{dx}$ .","What kind of functions satisfy the property ? I don't know if it makes sense. For example, for , What I am trying to ask is that what kind of other functions are symmetric like this, as it only changes the and between and .","y'(x) =x'(y) x^2+y^2=1 \begin{align}y'(x)& = \frac{-x}{\sqrt{1-x^2}}\\
x'(y) & = \frac{-y}{\sqrt{1-y^2}}\end{align} x y \frac{dx}{dy} \frac{dy}{dx}","['calculus', 'derivatives']"
41,Finding the general term of a Taylor series expansion of $f(x)=\frac{1}{\sqrt{x}}$,Finding the general term of a Taylor series expansion of,f(x)=\frac{1}{\sqrt{x}},"I came across a question in a workbook that asked me to find the general expression of the function $f(x)=\frac{1}{\sqrt{x}}$ centred at $x=4$ . I approached this problem by first finding its fourth-degree taylor series (the problem did not specify the degree of the taylor series, just the expression of the general term). $$f(x)=\frac{1}{\sqrt{x}}\Longrightarrow f(4)=\frac{1}{\sqrt{4}}=\frac{1}{2}$$ $$f'(x)=\frac{-1}{2x^\frac{3}{2}}\Longrightarrow f'(4)=\frac{-1}{2{\sqrt{4}}^3}=\frac{-1}{16}$$ $$f''(x)=\frac{3}{4x^{\frac{5}{2}}}\Longrightarrow f''(4)=\frac{3}{4{\sqrt{4}}^5}=\frac{3}{128}$$ $$f'''(x)=\frac{-15}{8x^{\frac{7}{2}}}\Longrightarrow f'''(4)=\frac{-15}{8{\sqrt{4}}^7}=\frac{-15}{1024}$$ $$f^{(4)}(x)=\frac{105}{16x^{\frac{9}{2}}}\Longrightarrow f^{(4)}(4)=\frac{105}{16{\sqrt{4}}^9}=\frac{105}{8192}$$ As such, the fourth-degree taylor series may be written as: $$P_4(x)=\frac{1}{2}-\frac{1}{16}(x-4)+\frac{3}{256}(x-4)^{2}-\frac{15}{6144}(x-4)^{3}+\frac{105}{196608}(x-4)^4$$ However, this is where the problem appears. I am struggling to find a representation for the numerator part of the coefficients. The pattern that I found seems to be $1\times3\times5...\times(2n-1)$ . However, I have no idea how I may represent this algebraically. As such, the best general term expression that I can do is $\sum_{n=0}^{\infty} {\frac{(-1)^{n}}{n!}}⋅{\frac{1⋅3⋅5⋅...⋅(2n-1)}{2^{n}⋅2^{2n+1}}}⋅(x-4)^{n}$ , which I am not quite satisfied with. Is there a better way to show this?","I came across a question in a workbook that asked me to find the general expression of the function centred at . I approached this problem by first finding its fourth-degree taylor series (the problem did not specify the degree of the taylor series, just the expression of the general term). As such, the fourth-degree taylor series may be written as: However, this is where the problem appears. I am struggling to find a representation for the numerator part of the coefficients. The pattern that I found seems to be . However, I have no idea how I may represent this algebraically. As such, the best general term expression that I can do is , which I am not quite satisfied with. Is there a better way to show this?",f(x)=\frac{1}{\sqrt{x}} x=4 f(x)=\frac{1}{\sqrt{x}}\Longrightarrow f(4)=\frac{1}{\sqrt{4}}=\frac{1}{2} f'(x)=\frac{-1}{2x^\frac{3}{2}}\Longrightarrow f'(4)=\frac{-1}{2{\sqrt{4}}^3}=\frac{-1}{16} f''(x)=\frac{3}{4x^{\frac{5}{2}}}\Longrightarrow f''(4)=\frac{3}{4{\sqrt{4}}^5}=\frac{3}{128} f'''(x)=\frac{-15}{8x^{\frac{7}{2}}}\Longrightarrow f'''(4)=\frac{-15}{8{\sqrt{4}}^7}=\frac{-15}{1024} f^{(4)}(x)=\frac{105}{16x^{\frac{9}{2}}}\Longrightarrow f^{(4)}(4)=\frac{105}{16{\sqrt{4}}^9}=\frac{105}{8192} P_4(x)=\frac{1}{2}-\frac{1}{16}(x-4)+\frac{3}{256}(x-4)^{2}-\frac{15}{6144}(x-4)^{3}+\frac{105}{196608}(x-4)^4 1\times3\times5...\times(2n-1) \sum_{n=0}^{\infty} {\frac{(-1)^{n}}{n!}}⋅{\frac{1⋅3⋅5⋅...⋅(2n-1)}{2^{n}⋅2^{2n+1}}}⋅(x-4)^{n},"['calculus', 'sequences-and-series', 'derivatives', 'taylor-expansion', 'factorial']"
42,Why would the |f(x)| be non differentiable for x belonging to Real Numbers?,Why would the |f(x)| be non differentiable for x belonging to Real Numbers?,,"I'm just started calculus and come across a statement in 1 of my books ( VG Advanced Problems in Mathematics ) that goes like: If $y = f(x)$ is differentiable for x belonging to the set of Real numbers, then $y=|f(x)|$ is not differentiable for all x belonging to the set of Real numbers. So what I inferred initially is that modulus function are continuous but non differentiable at the point where $f(x)=0$ (where $f(x)=|x|$ ofc) as the left hand derivative and the right hand derivatives are different. But then I know if $f(x)$ = $|x^3|$ at $x=0$ the left and the right hand derivative would be defined and equal, thus being defined for all x belonging to Real numbers. And moreover $x^3$ is inside the modulus ( which ummm... is not defined at $f(x)=0$ as modulus functions work that way ? ). In short how can the statement written in my book be correct if $f(x)=|x^3|$ ?","I'm just started calculus and come across a statement in 1 of my books ( VG Advanced Problems in Mathematics ) that goes like: If is differentiable for x belonging to the set of Real numbers, then is not differentiable for all x belonging to the set of Real numbers. So what I inferred initially is that modulus function are continuous but non differentiable at the point where (where ofc) as the left hand derivative and the right hand derivatives are different. But then I know if = at the left and the right hand derivative would be defined and equal, thus being defined for all x belonging to Real numbers. And moreover is inside the modulus ( which ummm... is not defined at as modulus functions work that way ? ). In short how can the statement written in my book be correct if ?",y = f(x) y=|f(x)| f(x)=0 f(x)=|x| f(x) |x^3| x=0 x^3 f(x)=0 f(x)=|x^3|,"['calculus', 'derivatives', 'continuity', 'graphing-functions', 'piecewise-continuity']"
43,"Constructing a counter-example of an increasing function such that $|\alpha(x)/x|, |\alpha(x)/\alpha(x/2)| \to \infty$ as $x\to 0^+$",Constructing a counter-example of an increasing function such that  as,"|\alpha(x)/x|, |\alpha(x)/\alpha(x/2)| \to \infty x\to 0^+","Consider a continuous function $\alpha:[0,\infty) \to [0,\infty)$ satisfying the following properties: $\alpha(0)=0;$ $\alpha$ is a strictly increasing function; $\alpha$ is smooth in $(0,\infty);$ and $|\alpha(x)/x| \to \infty$ as $x\to 0^+$ . Under these conditions, is it true that there exists $\delta>0$ and $Μ >0$ such that $$ \left|\frac{\alpha(x)}{\alpha(x/2)}\right| \leq M,\ \forall\ x\in(0,\delta)\ ?$$ I do not know if the above question is either true or false. Could someone please help me to solve this question? Searching for a counter-example: Since I could not prove the above question, I started looking for a counter-example. However, the examples that I was able to find that satisfy 1-4, such as $\alpha(x) = x^\beta$ , for $0<\beta<1$ ; $\alpha(x) = -x \log (x)$ in a neighborhood of $0;$ and $\alpha(x) = (1-x^x)/x$ in a neighborhood of $0.$ However, all these functions fulfil the desired property.","Consider a continuous function satisfying the following properties: is a strictly increasing function; is smooth in and as . Under these conditions, is it true that there exists and such that I do not know if the above question is either true or false. Could someone please help me to solve this question? Searching for a counter-example: Since I could not prove the above question, I started looking for a counter-example. However, the examples that I was able to find that satisfy 1-4, such as , for ; in a neighborhood of and in a neighborhood of However, all these functions fulfil the desired property.","\alpha:[0,\infty) \to [0,\infty) \alpha(0)=0; \alpha \alpha (0,\infty); |\alpha(x)/x| \to \infty x\to 0^+ \delta>0 Μ >0  \left|\frac{\alpha(x)}{\alpha(x/2)}\right| \leq M,\ \forall\ x\in(0,\delta)\ ? \alpha(x) = x^\beta 0<\beta<1 \alpha(x) = -x \log (x) 0; \alpha(x) = (1-x^x)/x 0.","['real-analysis', 'calculus', 'derivatives', 'examples-counterexamples']"
44,Integrating by integrating under the integral sign — the other Feynman trick?,Integrating by integrating under the integral sign — the other Feynman trick?,,"Having been introduced to the Feynman technique of integration , it seemed natural to wonder if it could be done the other way: Introduce a new parameter $a$ Integrate with respect to $a$ Integrate with respect to the variable $x$ Differentiate with respect to $a$ Set $a=1$ and add a constant by hand For instance, \begin{align} \int x \cos(x) dx =& \frac{d}{da}\int \int x \cos(ax) da dx\\ =& \frac{d}{da}\int \frac{x \sin(ax)}{x}dx\\ =& \frac{d}{da}\int \sin(ax)dx\\ =& \frac{d}{da}\frac{-\cos(ax)}{a}\\ =& \frac{x \sin(ax)}{a} + \frac{\cos(ax)}{a^2}\\ =& x \sin(x) + \cos(x) + C \end{align} And that is the right answer. But I couldn't think of any problems to solve with this method that I couldn't have done with integration by parts. So I wonder if this is actually an integration technique, and whether there are problems that are best solved with that method.","Having been introduced to the Feynman technique of integration , it seemed natural to wonder if it could be done the other way: Introduce a new parameter Integrate with respect to Integrate with respect to the variable Differentiate with respect to Set and add a constant by hand For instance, And that is the right answer. But I couldn't think of any problems to solve with this method that I couldn't have done with integration by parts. So I wonder if this is actually an integration technique, and whether there are problems that are best solved with that method.","a a x a a=1 \begin{align}
\int x \cos(x) dx =& \frac{d}{da}\int \int x \cos(ax) da dx\\
=& \frac{d}{da}\int \frac{x \sin(ax)}{x}dx\\
=& \frac{d}{da}\int \sin(ax)dx\\
=& \frac{d}{da}\frac{-\cos(ax)}{a}\\
=& \frac{x \sin(ax)}{a} + \frac{\cos(ax)}{a^2}\\
=& x \sin(x) + \cos(x) + C
\end{align}","['calculus', 'integration', 'derivatives']"
45,"Using related rates, why can we ignore dimensions and consider rates of change, when in seemingly identical situations, we must consider both?","Using related rates, why can we ignore dimensions and consider rates of change, when in seemingly identical situations, we must consider both?",,"So I was preparing a lesson on related rates for the calc 1 class I am a TA for and I realized that the two problems below in the photo are basically identical: Given a right triangle, x, x', y, y' are known, Find z' (or s'). Problem #1 and $4 are solved identically, but in problem #4, we can use a ""cheat"" and just consider a right triangle with legs x'=25 and y'=60 and hypotenuse=s'. Solving for $s'... \\s'=\sqrt{x'^2+y'^2}=\sqrt{25^2+60^2}=\sqrt{4225}=65 $ This implies the distance between the cars is changing at constant rate, independent of the location of the cars. But this method does not work for the seemingly identical problem #1. I am conflicted... why is this ""cheat"" only viable for some instances of these problems and not all? I checked back in my own notes from calc 1 and this ""cheat"" could be used on other problems too, so it's not something unique with the numbers in #4.","So I was preparing a lesson on related rates for the calc 1 class I am a TA for and I realized that the two problems below in the photo are basically identical: Given a right triangle, x, x', y, y' are known, Find z' (or s'). Problem #1 and $4 are solved identically, but in problem #4, we can use a ""cheat"" and just consider a right triangle with legs x'=25 and y'=60 and hypotenuse=s'. Solving for This implies the distance between the cars is changing at constant rate, independent of the location of the cars. But this method does not work for the seemingly identical problem #1. I am conflicted... why is this ""cheat"" only viable for some instances of these problems and not all? I checked back in my own notes from calc 1 and this ""cheat"" could be used on other problems too, so it's not something unique with the numbers in #4.",s'... \\s'=\sqrt{x'^2+y'^2}=\sqrt{25^2+60^2}=\sqrt{4225}=65 ,"['calculus', 'derivatives', 'physics', 'related-rates']"
46,Maxima CAS: How to differentiate w.r.t. an expression?,Maxima CAS: How to differentiate w.r.t. an expression?,,"Is there a way to differentiate with respect to an expression, instead of a single variable in Maxima CAS ? Here is a toy example, which should give $\frac{\partial}{\partial x^2}x^2=1$ : diff(x^2,x^2); with an error in wxMaxima 20.06.6 diff: second argument must be a variable; found x^2 For clarity, I meant first derivative with respect to an expression of $x$ , not a second derivative with respect to $x$ itself. Note that the latter gives $\frac{\partial^2}{\partial x^2}x^2=2\ne 1$ and can be easily computed as diff(x^2,x,2)","Is there a way to differentiate with respect to an expression, instead of a single variable in Maxima CAS ? Here is a toy example, which should give : diff(x^2,x^2); with an error in wxMaxima 20.06.6 diff: second argument must be a variable; found x^2 For clarity, I meant first derivative with respect to an expression of , not a second derivative with respect to itself. Note that the latter gives and can be easily computed as diff(x^2,x,2)",\frac{\partial}{\partial x^2}x^2=1 x x \frac{\partial^2}{\partial x^2}x^2=2\ne 1,"['derivatives', 'computer-algebra-systems', 'maxima-software']"
47,Prove that $\ln\left(1 + \frac{1}{|x|}\right) - \frac{1}{1 + |x|}$ is always positive,Prove that  is always positive,\ln\left(1 + \frac{1}{|x|}\right) - \frac{1}{1 + |x|},"I have to study when the function $$f'(x) = \ln\left(1 + \frac{1}{|x|}\right) - \frac{1}{1 + |x|}$$ is positive. I tried to use the inequality $\ln(1 + t) < t$ , $\forall t > -1$ but this could not help me ( $\frac{1}{|x|} - \frac{1}{1 + |x|}$ is always a positive quantity but I cannot say anything about $f'(x)$ ). Is there a quick method? Thanks.","I have to study when the function is positive. I tried to use the inequality , but this could not help me ( is always a positive quantity but I cannot say anything about ). Is there a quick method? Thanks.",f'(x) = \ln\left(1 + \frac{1}{|x|}\right) - \frac{1}{1 + |x|} \ln(1 + t) < t \forall t > -1 \frac{1}{|x|} - \frac{1}{1 + |x|} f'(x),"['derivatives', 'inequality']"
48,Gradient in cylindrical coordinates,Gradient in cylindrical coordinates,,I am a bit confused on why the vector gradient of a function $f$ in a cylindrical coordinate is equal to: $$\mathbf{e}_r\frac {\partial f}{\partial r} + \mathbf{e}_\theta\frac 1 r \frac {\partial f}{\partial\theta} + \mathbf{e}_z\frac {\partial f}{\partial z}.$$ From where did we get the $1/r$ ?,I am a bit confused on why the vector gradient of a function in a cylindrical coordinate is equal to: From where did we get the ?,f \mathbf{e}_r\frac {\partial f}{\partial r} + \mathbf{e}_\theta\frac 1 r \frac {\partial f}{\partial\theta} + \mathbf{e}_z\frac {\partial f}{\partial z}. 1/r,"['coordinate-systems', 'derivatives', 'vector-fields']"
49,Differentiating under the integral sign in Kelvin's theorem,Differentiating under the integral sign in Kelvin's theorem,,"I know well that this has already been asked here: Differentiation under the integral sign - line integral? ..but the answer given there already assumes that the length of the contour does not depend on time.. which is the actual point of the question. So I'll rewrite here the problem. Suppose you have an ideal isoentropic fluid, and consider the circulation on a contour $C(t)$ evolving in time $$\Gamma(t) =  \oint_{C(t)}\textbf{v}(\textbf{x}(s,t),t)\cdot\frac{\partial}{\partial s}\textbf{x}(s,t)ds.$$ Kelvin's Theorem proves $\Gamma$ to be constant in time. But its time derivative is $$ \frac{d\Gamma}{dt} = \oint_{C(t)}\frac{d\textbf{v}}{dt}\cdot\frac{\partial}{\partial s}\textbf{x}(s,t)ds + \oint_{C(t)}\textbf{v}\cdot\frac{d}{dt}\frac{\partial}{\partial s}\textbf{x}ds + BTs,$$ where I denoted with $BTs$ the boundary terms. Now, the issue is that, according to my textbook, and to the aforementioned answer (and to the wikipedia page https://en.wikipedia.org/wiki/Kelvin%27s_circulation_theorem as well), these boundary terms are vanishing. So..why is that? Shouldn't the length of the line on which we take the circulation depend on time?","I know well that this has already been asked here: Differentiation under the integral sign - line integral? ..but the answer given there already assumes that the length of the contour does not depend on time.. which is the actual point of the question. So I'll rewrite here the problem. Suppose you have an ideal isoentropic fluid, and consider the circulation on a contour evolving in time Kelvin's Theorem proves to be constant in time. But its time derivative is where I denoted with the boundary terms. Now, the issue is that, according to my textbook, and to the aforementioned answer (and to the wikipedia page https://en.wikipedia.org/wiki/Kelvin%27s_circulation_theorem as well), these boundary terms are vanishing. So..why is that? Shouldn't the length of the line on which we take the circulation depend on time?","C(t) \Gamma(t) =  \oint_{C(t)}\textbf{v}(\textbf{x}(s,t),t)\cdot\frac{\partial}{\partial s}\textbf{x}(s,t)ds. \Gamma  \frac{d\Gamma}{dt} = \oint_{C(t)}\frac{d\textbf{v}}{dt}\cdot\frac{\partial}{\partial s}\textbf{x}(s,t)ds + \oint_{C(t)}\textbf{v}\cdot\frac{d}{dt}\frac{\partial}{\partial s}\textbf{x}ds + BTs, BTs","['derivatives', 'fluid-dynamics']"
50,How to obtain $ \frac{\mathrm{d}}{\mathrm{d} \mathbf{T}} (\mathbf{Z} \circ (\mathbf{T}\mathbf{X})) = \mathbf{X}^T \otimes Diag(\mathbf{Z}) $?,How to obtain ?, \frac{\mathrm{d}}{\mathrm{d} \mathbf{T}} (\mathbf{Z} \circ (\mathbf{T}\mathbf{X})) = \mathbf{X}^T \otimes Diag(\mathbf{Z}) ,"$$ \frac{\mathrm{d}}{\mathrm{d} \mathbf{T}} (\mathbf{Z} \circ (\mathbf{T}\mathbf{X})) = \mathbf{X}^T \otimes Diag(\mathbf{Z}) $$ For $T \in \mathbb{R}^{K \times K}, Z,X \in \mathbb{R}^{K \times 1}$ I want to solve this derivative, however no matter what I found online, I just couldn't find a way to construct an equation with Kronecker product as shown above here $\circ$ means Hadamard product. Any helps is much appreciated!","For I want to solve this derivative, however no matter what I found online, I just couldn't find a way to construct an equation with Kronecker product as shown above here means Hadamard product. Any helps is much appreciated!","
\frac{\mathrm{d}}{\mathrm{d} \mathbf{T}} (\mathbf{Z} \circ (\mathbf{T}\mathbf{X})) = \mathbf{X}^T \otimes Diag(\mathbf{Z})
 T \in \mathbb{R}^{K \times K}, Z,X \in \mathbb{R}^{K \times 1} \circ","['linear-algebra', 'derivatives', 'optimization']"
51,"$|f''(x)|\leq 1$, $\exists\ x_1\neq x_2, \in [0,1]$ such that $f(x_1)=f(x_2)=0$. Show that $|f(x)|\leq 1, \forall\ x\in [0,1].$ [closed]",",  such that . Show that  [closed]","|f''(x)|\leq 1 \exists\ x_1\neq x_2, \in [0,1] f(x_1)=f(x_2)=0 |f(x)|\leq 1, \forall\ x\in [0,1].","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question $|f''(x)|\leq 1$ , $\exists\ x_1\neq x_2, \in [0,1]$ such that $f(x_1)=f(x_2)=0$ . Show that $|f(x)|\leq 1, \forall\ x\in [0,1].$ If $x_1=0, x_2=1$ , then Taylor expansion would help. But here we only know that $x_1\neq x_2$ ! What this helps?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question , such that . Show that If , then Taylor expansion would help. But here we only know that ! What this helps?","|f''(x)|\leq 1 \exists\ x_1\neq x_2, \in [0,1] f(x_1)=f(x_2)=0 |f(x)|\leq 1, \forall\ x\in [0,1]. x_1=0, x_2=1 x_1\neq x_2","['calculus', 'derivatives', 'taylor-expansion']"
52,How come $\sin(x^3)$ is treated like a composite function but $\sin(x)$ isn't?,How come  is treated like a composite function but  isn't?,\sin(x^3) \sin(x),"Preface: I'm not talking about sin in particular; any arbitrary trig function can be used. I just started learning the chain rule. I noticed for a certain example: when applying chain rule to $\sin(x^3)$ , it's treated as two nested functions, [1] $\sin(x^3)$ , and [2] $x^3$ . But when simply given some function, $\sin(x)$ , the derivative is just $\cos(x)$ . How come it's not treated as two functions, [1] $\sin(x)$ and [2] $g(x) = x$ ?","Preface: I'm not talking about sin in particular; any arbitrary trig function can be used. I just started learning the chain rule. I noticed for a certain example: when applying chain rule to , it's treated as two nested functions, [1] , and [2] . But when simply given some function, , the derivative is just . How come it's not treated as two functions, [1] and [2] ?",\sin(x^3) \sin(x^3) x^3 \sin(x) \cos(x) \sin(x) g(x) = x,"['calculus', 'derivatives', 'chain-rule']"
53,Proving existence of inverse function using the rule of differentiating inverse functions.,Proving existence of inverse function using the rule of differentiating inverse functions.,,"I am working from a textbook that gives us that The derivative of the inverse function is $$ \frac{dx}{dy} = \large{\frac{1}{\frac{dy}{dx}}}.\qquad (*)$$ $$$$ Now I am stuck on part $(a)$ of the following question: A function is defined by $\ f(x) =x^3 + 3x + 2.$ $(a)$ By considering $f'(x),\ $ prove that $\ f(x)\ $ has an inverse function. $(b)\ $ Find the gradient of the graph of $y=f^{-1}(x)$ at the point where $x=2.$ $$$$ For $(b),\ f(x) = 2\ \iff x=0\ $ . Therefore, the gradient of the graph of $y=f^{-1}(x)$ at the point where $\ x=2\ $ is equal to $\ \large{\frac{1}{f'(0)}} = \frac{1}{3}.$ But I'm simply a bit confused unsure on what it wants for part $(a)$ ... The derivative of the inverse function is $\ \large{\frac{1}{3}\cdot \frac{1}{x^2+1} },\ $ but this does not say that, for example, the derivative of the inverse function $\ y=f^{-1}(x)\ $ at the point $\ x=3\ $ is $\ \large{\frac{1}{3}\cdot \frac{1}{3^2+1} },\ $ which it isn't. This means I find $(*)$ confusing as to what it is saying exactly, and like I say, I'm confused about this and not sure how to tackle part $(a)$ . Please can someone clarify. Thanks.","I am working from a textbook that gives us that The derivative of the inverse function is Now I am stuck on part of the following question: A function is defined by By considering prove that has an inverse function. Find the gradient of the graph of at the point where For . Therefore, the gradient of the graph of at the point where is equal to But I'm simply a bit confused unsure on what it wants for part ... The derivative of the inverse function is but this does not say that, for example, the derivative of the inverse function at the point is which it isn't. This means I find confusing as to what it is saying exactly, and like I say, I'm confused about this and not sure how to tackle part . Please can someone clarify. Thanks."," \frac{dx}{dy} = \large{\frac{1}{\frac{dy}{dx}}}.\qquad (*)  (a) \ f(x) =x^3 + 3x + 2. (a) f'(x),\  \ f(x)\  (b)\  y=f^{-1}(x) x=2.  (b),\ f(x) = 2\ \iff x=0\  y=f^{-1}(x) \ x=2\  \ \large{\frac{1}{f'(0)}} = \frac{1}{3}. (a) \ \large{\frac{1}{3}\cdot \frac{1}{x^2+1} },\  \ y=f^{-1}(x)\  \ x=3\  \ \large{\frac{1}{3}\cdot \frac{1}{3^2+1} },\  (*) (a)","['derivatives', 'inverse-function']"
54,Second derivative test and continuity of $f''$? [duplicate],Second derivative test and continuity of ? [duplicate],f'',"This question already has an answer here : Counterexample to second derivative test when f''(x) is not continuously differentiable (1 answer) Closed 3 years ago . In the 8th edition of Calculus by Stewart, The second derivative test is stated as follows The Second Derivative Test Suppose $f''$ is continuous near c. (a) If $f'(c)=0$ and $f''(c)>0$ , then f has a local minimum at c. (b) If $f'(c)=0$ and $f''(c)<0$ , then f has a local maximum at c. I don't see why continuity of $f''$ is an assumption here. Can you provide a function that discontinuity of $f''(c)$ affects the test?","This question already has an answer here : Counterexample to second derivative test when f''(x) is not continuously differentiable (1 answer) Closed 3 years ago . In the 8th edition of Calculus by Stewart, The second derivative test is stated as follows The Second Derivative Test Suppose is continuous near c. (a) If and , then f has a local minimum at c. (b) If and , then f has a local maximum at c. I don't see why continuity of is an assumption here. Can you provide a function that discontinuity of affects the test?",f'' f'(c)=0 f''(c)>0 f'(c)=0 f''(c)<0 f'' f''(c),"['calculus', 'derivatives']"
55,Is this intuitive by graph ? Or its needs rigorous proof,Is this intuitive by graph ? Or its needs rigorous proof,,"A double differentiable function $f : \mathbb{R} \to \mathbb{R}$ satisfy $f(x)f^{\prime \prime}(x) \ne 0 \ \forall x \in \mathbb{R}$ . Can we conclude $f(x)f^{\prime \prime}(x) >0$ for all $\mathbb{R}$ , even though continuity of $f''(x)$ is not given. I thought of making a contradiction by graph supposing $f(x)>0$ and $f''(x) <0$ , I thought graph would need to cut x axis hence after that $f(x)<0$ ( contradiction) but I don't have a good calculus based proof on this fact. As continuity of $f''(x)$ not given is also a problem for all $\mathbb{R}$","A double differentiable function satisfy . Can we conclude for all , even though continuity of is not given. I thought of making a contradiction by graph supposing and , I thought graph would need to cut x axis hence after that ( contradiction) but I don't have a good calculus based proof on this fact. As continuity of not given is also a problem for all",f : \mathbb{R} \to \mathbb{R} f(x)f^{\prime \prime}(x) \ne 0 \ \forall x \in \mathbb{R} f(x)f^{\prime \prime}(x) >0 \mathbb{R} f''(x) f(x)>0 f''(x) <0 f(x)<0 f''(x) \mathbb{R},['calculus']
56,Differentiate $\frac{e^{-2x}}{\sqrt x}$,Differentiate,\frac{e^{-2x}}{\sqrt x},"Differentiate, with respect to $x$ , $\frac{e^{-2x}}{\sqrt x}$ . I  a having difficulties with differentiation. The answer is $$-\frac{e^{-2x}(4x+1)}{2x\sqrt{x}}$$ But I am looking at my working out and can't seem to solve the question. This is what I did: $$\frac{d}{dx}\left(\frac{e^{-2x}}{\sqrt{x}}\right) = \frac{\sqrt{x}\times \frac{d}{dx}(e^{-2x})-e^{-2x}\times \frac{d}{dx}(\sqrt{x})}{(\sqrt{x})^2} = \frac{\sqrt{x}(-2e^{2x})-e^{-2x}\frac12(\sqrt{x})^{-1/2}}{x}$$ $$ = \frac{\sqrt{x}(-2e^{2x})-e^{-2x}}{2x\sqrt{x}}$$ image of my work If someone could please help me solve this question or give advise please help me! Thank You!","Differentiate, with respect to , . I  a having difficulties with differentiation. The answer is But I am looking at my working out and can't seem to solve the question. This is what I did: image of my work If someone could please help me solve this question or give advise please help me! Thank You!",x \frac{e^{-2x}}{\sqrt x} -\frac{e^{-2x}(4x+1)}{2x\sqrt{x}} \frac{d}{dx}\left(\frac{e^{-2x}}{\sqrt{x}}\right) = \frac{\sqrt{x}\times \frac{d}{dx}(e^{-2x})-e^{-2x}\times \frac{d}{dx}(\sqrt{x})}{(\sqrt{x})^2} = \frac{\sqrt{x}(-2e^{2x})-e^{-2x}\frac12(\sqrt{x})^{-1/2}}{x}  = \frac{\sqrt{x}(-2e^{2x})-e^{-2x}}{2x\sqrt{x}},"['calculus', 'derivatives']"
57,An other total-variation inequality of mine,An other total-variation inequality of mine,,"Given differentiable, continuous $f\left ( x \right )$ on the interval $\left [ 0, 1 \right ]$ so that $\int_{0}^{1}f\left ( x \right ){\rm d}x= 0.$ Prove that $$\left | \int_{0}^{1}xf\left ( x \right ){\rm d}x \right |\leq \frac{1}{12}\max\left | {f}'\left ( x \right ) \right |$$ I think I should transform the constant $1/12$ into an integral like $k\int_{0}^{1}x^{2}{\rm d}x,$ but $k$ is very unusual, I need to your helps, even an example of $f\left ( x \right )$ so that $\int_{0}^{1}f\left ( x \right ){\rm d}x= 0$ in order to know what I must do with the constant. Thanks a real lot.","Given differentiable, continuous on the interval so that Prove that I think I should transform the constant into an integral like but is very unusual, I need to your helps, even an example of so that in order to know what I must do with the constant. Thanks a real lot.","f\left ( x \right ) \left [ 0, 1 \right ] \int_{0}^{1}f\left ( x \right ){\rm d}x= 0. \left | \int_{0}^{1}xf\left ( x \right ){\rm d}x \right |\leq \frac{1}{12}\max\left | {f}'\left ( x \right ) \right | 1/12 k\int_{0}^{1}x^{2}{\rm d}x, k f\left ( x \right ) \int_{0}^{1}f\left ( x \right ){\rm d}x= 0","['integration', 'derivatives']"
58,$\frac{d}{dx}(\sin(x^{\frac{1}{3}}))$ from first principle,from first principle,\frac{d}{dx}(\sin(x^{\frac{1}{3}})),"The question contains a hint: at the appropriate point use the result $a^{3} - b^{3} = \left(a - b\right)\left(a^{2} + b^{2} + ab\right)$ . $$ \frac{{\rm d}}{{\rm d}x}\sin\left(x^{1/3}\,\right) $$ My attempt: I did not use the hint as it was not immediately obvious to me what the simplification was. Instead I wrote with binomial expansion that $(x+h)^{\frac{1}{3}}=x^{\frac{1}{3}}(1+\frac{h}{x})^{\frac{1}{3}} \approx x^{\frac{1}{3}}(1+(\frac{1}{3})\frac{h}{x})$ , used $\sin(()+())=\sin()\cos()+\sin()\cos()$ and applied $\cos() \approx 1$ , $\sin() \approx ()$ for small $h$ which gave the correct answer. But I do not see the point of using the cubes difference formula given in the question? Could this be a typo because it would make more sense to apply the identity to something like e.g. $\sin({x})^{\frac{1}{3}} (\text{not }\sin(x^{\frac{1}{3}}))\to \sin(x+h)^{\frac{1}{3}}-\sin(x)^{\frac{1}{3}} \iff (\sin(x+h)-\sin(x)=(\sin(x+h)^{\frac{1}{3}}-\sin(x)^{\frac{1}{3}})(\sin(x+h)^2+\sin(x)^2+\sin(x+h)\sin(x))$ Or am I missing something here?","The question contains a hint: at the appropriate point use the result . My attempt: I did not use the hint as it was not immediately obvious to me what the simplification was. Instead I wrote with binomial expansion that , used and applied , for small which gave the correct answer. But I do not see the point of using the cubes difference formula given in the question? Could this be a typo because it would make more sense to apply the identity to something like e.g. Or am I missing something here?","a^{3} - b^{3} =
\left(a - b\right)\left(a^{2} + b^{2} + ab\right) 
\frac{{\rm d}}{{\rm d}x}\sin\left(x^{1/3}\,\right)
 (x+h)^{\frac{1}{3}}=x^{\frac{1}{3}}(1+\frac{h}{x})^{\frac{1}{3}} \approx x^{\frac{1}{3}}(1+(\frac{1}{3})\frac{h}{x}) \sin(()+())=\sin()\cos()+\sin()\cos() \cos() \approx 1 \sin() \approx () h \sin({x})^{\frac{1}{3}} (\text{not }\sin(x^{\frac{1}{3}}))\to \sin(x+h)^{\frac{1}{3}}-\sin(x)^{\frac{1}{3}} \iff (\sin(x+h)-\sin(x)=(\sin(x+h)^{\frac{1}{3}}-\sin(x)^{\frac{1}{3}})(\sin(x+h)^2+\sin(x)^2+\sin(x+h)\sin(x))","['calculus', 'derivatives', 'trigonometry', 'trigonometric-series']"
59,Finding Integer Solutions to $x^2-y^2-n=0$,Finding Integer Solutions to,x^2-y^2-n=0,"I am trying to find the integer solutions to the equation $x^2-y^2-n=0$ . Effectively, I am trying to find when the difference of two perfect squares is $n$ I have tried using a modified Newton's method to find the nearest point where the equation reaches parity; however, it ends up finding non-integer solutions. I am wondering if there is a way to factor/find all the integer solutions to this equation. For my algorithm, taking the square root is prohibitively costly and I am trying to avoid using it. Thanks for the help!","I am trying to find the integer solutions to the equation . Effectively, I am trying to find when the difference of two perfect squares is I have tried using a modified Newton's method to find the nearest point where the equation reaches parity; however, it ends up finding non-integer solutions. I am wondering if there is a way to factor/find all the integer solutions to this equation. For my algorithm, taking the square root is prohibitively costly and I am trying to avoid using it. Thanks for the help!",x^2-y^2-n=0 n,"['discrete-mathematics', 'derivatives', 'diophantine-equations', 'integers']"
60,Derivative of Inverse of sum of matrices,Derivative of Inverse of sum of matrices,,"Given is the function $f : \mathbb{R}^p \to \mathbb{R}$ with $$ f(x) = q(x)^{\top} G^{-1} q(x) $$ where $G = A + x_1 B_1 + \ldots + x_p B_p$ . The matrices $A, B_1, \ldots, B_p \in \mathbb{R}^{n \times n}$ are all symmetric positive definit. $q: \mathbb{R}^n \to \mathbb{R}^n$ and the Jacobian $\nabla q$ is known. Is it possible to derive a closed form for $\nabla f$ ? For me, the hard part is $G^{-1}$ . Any hints or suggestions are really appreciated!","Given is the function with where . The matrices are all symmetric positive definit. and the Jacobian is known. Is it possible to derive a closed form for ? For me, the hard part is . Any hints or suggestions are really appreciated!","f : \mathbb{R}^p \to \mathbb{R} 
f(x) = q(x)^{\top} G^{-1} q(x)
 G = A + x_1 B_1 + \ldots + x_p B_p A, B_1, \ldots, B_p \in \mathbb{R}^{n \times n} q: \mathbb{R}^n \to \mathbb{R}^n \nabla q \nabla f G^{-1}","['derivatives', 'inverse', 'matrix-calculus']"
61,Mean value theorem for $\displaystyle f(x)=\frac{3-x^{2}}{x-2}$,Mean value theorem for,\displaystyle f(x)=\frac{3-x^{2}}{x-2},"Given the following function $$f(x)=\frac{3-x^{2}}{x-2}$$ does the Mean value theorem applies to if in the interval $[1,3]$ ? Since $f$ is not defined for $x=2$ then $f$ is continuous and differentiable in $[1,3]-\{2\}$ . So, the theorem is valid. But I can't find a number $c\in(1,3)$ such that $$f'(c)=-2$$","Given the following function does the Mean value theorem applies to if in the interval ? Since is not defined for then is continuous and differentiable in . So, the theorem is valid. But I can't find a number such that","f(x)=\frac{3-x^{2}}{x-2} [1,3] f x=2 f [1,3]-\{2\} c\in(1,3) f'(c)=-2","['calculus', 'derivatives']"
62,Show that $f(x) = x|x|$ is continuous and differentiable - solution verification?,Show that  is continuous and differentiable - solution verification?,f(x) = x|x|,Another exercise I did without any solutions. I highly doubt this is correct so pls correct me :) Let $f: \mathbf{R} \rightarrow \mathbf{R}$ be given by $f(x):=x|x| .$ Show that $f$ is continuous and differentiable on $\mathrm{R}$ $$ \begin{array}{l} \text { Continuous: } \lim _{x \rightarrow c} f(x)=f(c) \\ \begin{aligned} \lim _{x \rightarrow c} x \cdot|x| &=\lim _{x \rightarrow c} x \cdot \lim _{x \rightarrow c}|x|=f(c) \\ &=\lim _{x \rightarrow c} c \cdot \lim _{x \rightarrow c}|c|=f(c) \\ &=c \cdot|c|=f(c)=c \cdot|c| \end{aligned} \end{array} $$ So $f(x)$ is continuous Differentiable: show $f^{\prime}(x)$ exists atall $x \in \mathbb{R}$ : $$ \begin{array}{l}\lim _{h \rightarrow 0} \frac{f(x+h)-f(x)}{h} \\  \lim _{h \rightarrow 0} \frac{(x \cdot|x|)+h-(x \cdot|x|)}{h} \\ =\lim _{h \rightarrow 0} \frac{h}{h}=1\end{array} $$ $$ So f(x) \text { is differentiable } $$,Another exercise I did without any solutions. I highly doubt this is correct so pls correct me :) Let be given by Show that is continuous and differentiable on So is continuous Differentiable: show exists atall :,"f: \mathbf{R} \rightarrow \mathbf{R} f(x):=x|x| . f \mathrm{R} 
\begin{array}{l}
\text { Continuous: } \lim _{x \rightarrow c} f(x)=f(c) \\
\begin{aligned}
\lim _{x \rightarrow c} x \cdot|x| &=\lim _{x \rightarrow c} x \cdot \lim _{x \rightarrow c}|x|=f(c) \\
&=\lim _{x \rightarrow c} c \cdot \lim _{x \rightarrow c}|c|=f(c) \\
&=c \cdot|c|=f(c)=c \cdot|c|
\end{aligned}
\end{array}
 f(x) f^{\prime}(x) x \in \mathbb{R} 
\begin{array}{l}\lim _{h \rightarrow 0} \frac{f(x+h)-f(x)}{h} \\ 
\lim _{h \rightarrow 0} \frac{(x \cdot|x|)+h-(x \cdot|x|)}{h} \\ =\lim _{h \rightarrow 0} \frac{h}{h}=1\end{array}
 
So f(x) \text { is differentiable }
","['calculus', 'derivatives', 'continuity', 'solution-verification']"
63,Is it possible that $|| f^{(n)} ||_1 \to \infty$ exponentially for a compactly supported $C^\infty$-functions?,Is it possible that  exponentially for a compactly supported -functions?,|| f^{(n)} ||_1 \to \infty C^\infty,"Suppose that $f \in C^\infty(\mathbb R)$ has compact support in $[-T,T]$ where $T>0$ . Is it possible that the $L^1$ -norm of its derivatives are growing exponentially? That means $$ \| f^{(n)} \|_1 \to \infty $$ exponentially. For me it's hard to imagine that such a function could exist. Is there a way to construct such a function or argue that it cannot exist?",Suppose that has compact support in where . Is it possible that the -norm of its derivatives are growing exponentially? That means exponentially. For me it's hard to imagine that such a function could exist. Is there a way to construct such a function or argue that it cannot exist?,"f \in C^\infty(\mathbb R) [-T,T] T>0 L^1 
\| f^{(n)} \|_1 \to \infty
","['real-analysis', 'derivatives', 'smooth-functions']"
64,"How do I find all functions $F$ with $F(x_1) − F(x_2) \le (x_1 − x_2)^2$ for all $x_1, x_2$?",How do I find all functions  with  for all ?,"F F(x_1) − F(x_2) \le (x_1 − x_2)^2 x_1, x_2","In calculus class we were given this so-called ""coffin problem"" originally from Moscow State University. Find all real functions $F(x)$ , having the property that for any $x_1$ and $x_2$ the following inequality holds: $$F(x_1) − F(x_2) \le (x_1 − x_2)^2$$ I have the solution to this problem, which is supposed to make the question very intuitive once you see it. However, I still do not quite understand it, and I would appreciate your help. Solution: The inequality implies $$\frac{F(x_1) − F(x_2)}{|x_1 − x_2|} \le |x_1 − x_2|,$$ so the derivative of $F$ at any point $x_2$ exists and is equal to zero. Therefore, by the fundamental theorem of calculus, the constant functions are exactly the functions with the desired property. Based on this solution, I substituted $x_1=x_2+h$ and took the limit as $h$ approaches zero, therefore by first principles, the derivative of $F(x)$ at $x_2$ is less than or equal to zero. Where do I proceed from here?","In calculus class we were given this so-called ""coffin problem"" originally from Moscow State University. Find all real functions , having the property that for any and the following inequality holds: I have the solution to this problem, which is supposed to make the question very intuitive once you see it. However, I still do not quite understand it, and I would appreciate your help. Solution: The inequality implies so the derivative of at any point exists and is equal to zero. Therefore, by the fundamental theorem of calculus, the constant functions are exactly the functions with the desired property. Based on this solution, I substituted and took the limit as approaches zero, therefore by first principles, the derivative of at is less than or equal to zero. Where do I proceed from here?","F(x) x_1 x_2 F(x_1) − F(x_2) \le (x_1 − x_2)^2 \frac{F(x_1) − F(x_2)}{|x_1 − x_2|} \le |x_1 − x_2|, F x_2 x_1=x_2+h h F(x) x_2","['calculus', 'derivatives', 'functional-inequalities']"
65,Showing $\frac{d\theta }{ d \tan \theta}=\frac{ 1}{ 1+ \tan^2 \theta}$,Showing,\frac{d\theta }{ d \tan \theta}=\frac{ 1}{ 1+ \tan^2 \theta},I suppose that $$ \frac{d\theta }{ d \tan \theta}=\frac{d \arctan  x }{ d x}=  \frac{1}{1+x^2}=\frac{ 1}{ 1+ \tan^2 \theta}  $$ So is $$ \frac{d\theta }{ d \tan \theta}=\frac{ 1}{ 1+ \tan^2 \theta}  $$ correct? And $$ \frac{d (\theta)  }{ d \tan \frac{\theta}{2}}=\frac{ 2}{ 1+ \tan^2 \frac{\theta}{2}} \; ? $$,I suppose that So is correct? And,"
\frac{d\theta }{ d \tan \theta}=\frac{d \arctan  x }{ d x}=  \frac{1}{1+x^2}=\frac{ 1}{ 1+ \tan^2 \theta} 
 
\frac{d\theta }{ d \tan \theta}=\frac{ 1}{ 1+ \tan^2 \theta} 
 
\frac{d (\theta)  }{ d \tan \frac{\theta}{2}}=\frac{ 2}{ 1+ \tan^2 \frac{\theta}{2}} \; ?
","['derivatives', 'trigonometry']"
66,Show that not exists any polynomial function such that $f(x) = \log (1+x)$. [duplicate],Show that not exists any polynomial function such that . [duplicate],f(x) = \log (1+x),"This question already has answers here : Show that, for $t>0$, $\log t$ is not a polynomial. (9 answers) Closed 4 years ago . Does anyone have any idea on that problem? Let $f : \mathbb{R} \to \mathbb{R}$ be a polynomial function. Show that not exists any $f$ such that $f(x) = \log (1+x)$ . It's easy to show that $a_0 = 0$ and $a_1 = 1$ . But after i don't have any idea. Any point? Thanks!","This question already has answers here : Show that, for $t>0$, $\log t$ is not a polynomial. (9 answers) Closed 4 years ago . Does anyone have any idea on that problem? Let be a polynomial function. Show that not exists any such that . It's easy to show that and . But after i don't have any idea. Any point? Thanks!",f : \mathbb{R} \to \mathbb{R} f f(x) = \log (1+x) a_0 = 0 a_1 = 1,"['calculus', 'derivatives', 'polynomials']"
67,Is $\int_0^x\left|\sin\left(\frac{1}{t}\right)\right|\mathrm{d}t$ differentiable at $0$?,Is  differentiable at ?,\int_0^x\left|\sin\left(\frac{1}{t}\right)\right|\mathrm{d}t 0,Let $f(x)=\int_0^x\left|\sin\left(\frac{1}{t}\right)\right|\mathrm{d}t$ for $x\in\mathbb{R}$ . Is $f$ differentiable at $x=0$ ?,Let for . Is differentiable at ?,f(x)=\int_0^x\left|\sin\left(\frac{1}{t}\right)\right|\mathrm{d}t x\in\mathbb{R} f x=0,"['integration', 'derivatives']"
68,How does this L1 regularization derivation follow? (Proof it makes sparse models),How does this L1 regularization derivation follow? (Proof it makes sparse models),,"I'm reading the ""Deep Learning""(Goodfellow et al, 2016) book and on pages 231-232(you can check them here ) they show a very unique proof how L1 regularization makes model sparse. You can skip to the last two expressions for the actual question, but some context if you want it: The regularized objetive function $\tilde{J}(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y})$ is given by: $$ \tilde{J}(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y})=\alpha\|\boldsymbol{w}\|_{1}+J(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y})$$ where $\boldsymbol{w}$ is the parameter vector and $\boldsymbol{X}, \boldsymbol{y}$ are the design matrix(inputs) and the outputs of the model, respectively. If we make a second order Taylor series approximation of the unregularized loss function of our model, which we will assume it is a linear model to ensure it has a clean analytical solution we have $$\hat{J}(\boldsymbol{\theta})=J\left(\boldsymbol{w}^{*}\right)+\frac{1}{2}\left(\boldsymbol{w}-\boldsymbol{w}^{*}\right)^{\top} \boldsymbol{H}\left(\boldsymbol{w}-\boldsymbol{w}^{*}\right)$$ where $\boldsymbol{H}$ is the Hessian matrix of $J$ with respect to $\boldsymbol{w}$ evaluated at $\boldsymbol{w}^{*}$ and there is no ﬁrst-order term in this quadratic approximation because $\boldsymbol{w}^{*}$ is deﬁned to be a minimum. The minimum of $\hat{J}$ occurs where its gradient $$\nabla_{\boldsymbol{w}} \hat{J}(\boldsymbol{w})=\boldsymbol{H}\left(\boldsymbol{w}-\boldsymbol{w}^{*}\right)$$ If we make the further assumption that the Hessian is diagonal $\boldsymbol{H}=\operatorname{diag}\left(\left[H_{1,1}, \ldots, H_{n, n}\right]\right), $ where each $ H_{i, i}>0$ (more details on the book), we have that our quadratic approximation of the L1 regularized objective function decomposes into a sum over the parameters: $$\hat{J}(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y})=J\left(\boldsymbol{w}^{*} ; \boldsymbol{X}, \boldsymbol{y}\right)+\sum_{i}\left[\frac{1}{2} H_{i, i}\left(\boldsymbol{w}_{i}-\boldsymbol{w}_{i}^{*}\right)^{2}+\alpha\left|w_{i}\right|\right]$$ Then they say ( and this is where I don't see how they did it, specifically that max function ) The problem of minimizing this approximate cost function has an analytical solution(for each dimension i), with the following form: $$w_{i}=\operatorname{sign}\left(w_{i}^{*}\right) \max \left\{\left|w_{i}^{*}\right|-\frac{\alpha}{H_{i, i}}, 0\right\}$$ How did they get that expression? I've reached a similar one, but without that max function... For further insights, please refer to the book.","I'm reading the ""Deep Learning""(Goodfellow et al, 2016) book and on pages 231-232(you can check them here ) they show a very unique proof how L1 regularization makes model sparse. You can skip to the last two expressions for the actual question, but some context if you want it: The regularized objetive function is given by: where is the parameter vector and are the design matrix(inputs) and the outputs of the model, respectively. If we make a second order Taylor series approximation of the unregularized loss function of our model, which we will assume it is a linear model to ensure it has a clean analytical solution we have where is the Hessian matrix of with respect to evaluated at and there is no ﬁrst-order term in this quadratic approximation because is deﬁned to be a minimum. The minimum of occurs where its gradient If we make the further assumption that the Hessian is diagonal where each (more details on the book), we have that our quadratic approximation of the L1 regularized objective function decomposes into a sum over the parameters: Then they say ( and this is where I don't see how they did it, specifically that max function ) The problem of minimizing this approximate cost function has an analytical solution(for each dimension i), with the following form: How did they get that expression? I've reached a similar one, but without that max function... For further insights, please refer to the book.","\tilde{J}(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y})  \tilde{J}(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y})=\alpha\|\boldsymbol{w}\|_{1}+J(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y}) \boldsymbol{w} \boldsymbol{X}, \boldsymbol{y} \hat{J}(\boldsymbol{\theta})=J\left(\boldsymbol{w}^{*}\right)+\frac{1}{2}\left(\boldsymbol{w}-\boldsymbol{w}^{*}\right)^{\top} \boldsymbol{H}\left(\boldsymbol{w}-\boldsymbol{w}^{*}\right) \boldsymbol{H} J \boldsymbol{w} \boldsymbol{w}^{*} \boldsymbol{w}^{*} \hat{J} \nabla_{\boldsymbol{w}} \hat{J}(\boldsymbol{w})=\boldsymbol{H}\left(\boldsymbol{w}-\boldsymbol{w}^{*}\right) \boldsymbol{H}=\operatorname{diag}\left(\left[H_{1,1}, \ldots, H_{n, n}\right]\right),   H_{i, i}>0 \hat{J}(\boldsymbol{w} ; \boldsymbol{X}, \boldsymbol{y})=J\left(\boldsymbol{w}^{*} ; \boldsymbol{X}, \boldsymbol{y}\right)+\sum_{i}\left[\frac{1}{2} H_{i, i}\left(\boldsymbol{w}_{i}-\boldsymbol{w}_{i}^{*}\right)^{2}+\alpha\left|w_{i}\right|\right] w_{i}=\operatorname{sign}\left(w_{i}^{*}\right) \max \left\{\left|w_{i}^{*}\right|-\frac{\alpha}{H_{i, i}}, 0\right\}","['linear-algebra', 'derivatives', 'normed-spaces', 'machine-learning', 'regularization']"
69,Derivative of polynomial root function,Derivative of polynomial root function,,"Find the derivative of $f(x)=-10\sqrt{x^{20}+9}$ with respect to $x$ I know to take the constant out and let $u=x^{20}+9$ \begin{align} f'=&-10 \cfrac{df}{dx}(u)^{1/2} \hspace{2cm} (1) \end{align} \begin{align} f'=&-10 \cfrac{1}{2}(u)^{-1/2} \hspace{2cm} (2) \\ f'=& \cfrac{-10}{2\sqrt{u}} \hspace{3.75cm} (3) \\ f' =& \cfrac{-5}{\sqrt{x^{20}+9}} \hspace{2.8cm} (4) \end{align} I know this is very wrong, but I don't understand why the correct answer is $-10 \cdot \cfrac{1}{2\sqrt{x^{20}+9}}\cdot 20x^{19}$ . I understand everything except the last term, $20x^{19}$ . I'm aware it is the derivative of $u$ , but I don't understand why we multiply by that to the numerator after having already taken the derivative of root $u$ as shown in line $2$ . can anyone explain why multiplying that term is necessary?","Find the derivative of with respect to I know to take the constant out and let I know this is very wrong, but I don't understand why the correct answer is . I understand everything except the last term, . I'm aware it is the derivative of , but I don't understand why we multiply by that to the numerator after having already taken the derivative of root as shown in line . can anyone explain why multiplying that term is necessary?","f(x)=-10\sqrt{x^{20}+9} x u=x^{20}+9 \begin{align}
f'=&-10 \cfrac{df}{dx}(u)^{1/2} \hspace{2cm} (1)
\end{align} \begin{align}
f'=&-10 \cfrac{1}{2}(u)^{-1/2} \hspace{2cm} (2) \\ f'=& \cfrac{-10}{2\sqrt{u}} \hspace{3.75cm} (3) \\ f' =& \cfrac{-5}{\sqrt{x^{20}+9}} \hspace{2.8cm} (4)
\end{align} -10 \cdot \cfrac{1}{2\sqrt{x^{20}+9}}\cdot 20x^{19} 20x^{19} u u 2","['calculus', 'derivatives']"
70,"A continuously differentiable function $f$ Satisfying $ \| f ( x ) - f ( y ) \| \geq \| x - y \| , \forall x , y \in \mathbb { R } ^ { n } $ is onto",A continuously differentiable function  Satisfying  is onto,"f  \| f ( x ) - f ( y ) \| \geq \| x - y \| , \forall x , y \in \mathbb { R } ^ { n } ","Let $f: \mathbb { R^n } \rightarrow \mathbb { R } ^ { n }$ be continuously differentiable, Satisfying $$ \| f ( x ) - f ( y ) \| \geqslant \| x - y \| , \forall x , y \in \mathbb { R } ^ { n } $$ then how to prove that $f$ is onto. My work: Consider $f:\mathbb R^n \rightarrow Range(f),$ Then clearly $f$ is bijective from $\mathbb { R } ^ { n }$ onto $Range( f )$ also since $$\left\| f ^ { - 1 } ( x ) -f ^ { - l } ( y ) \right\| \leq \| x - y \|$$ $f ^ { - 1 }: \operatorname { Range } (f) \rightarrow \operatorname {\mathbb R^n }$ Continuous. Thus $Range(f) \subseteq \mathbb { R } ^ { n }$ is homeomorphic to $\mathbb { R } ^ { n }$ , is the way is correct? I am not getting idea further.","Let be continuously differentiable, Satisfying then how to prove that is onto. My work: Consider Then clearly is bijective from onto also since Continuous. Thus is homeomorphic to , is the way is correct? I am not getting idea further.","f: \mathbb { R^n } \rightarrow \mathbb { R } ^ { n } 
\| f ( x ) - f ( y ) \| \geqslant \| x - y \| , \forall x , y \in \mathbb { R } ^ { n }
 f f:\mathbb R^n \rightarrow Range(f), f \mathbb { R } ^ { n } Range( f ) \left\| f ^ { - 1 } ( x ) -f ^ { - l } ( y ) \right\| \leq \| x - y \| f ^ { - 1 }: \operatorname { Range } (f) \rightarrow \operatorname {\mathbb R^n } Range(f) \subseteq \mathbb { R } ^ { n } \mathbb { R } ^ { n }","['real-analysis', 'derivatives', 'frechet-derivative']"
71,Computing the nth derivative using the binomial theorem.,Computing the nth derivative using the binomial theorem.,,"Let n be an element of the set of  natural numbers Let $F(x)=x^2(1+x)^n$ and write $F^n$ for the nth derivative of the function $F$ . Compute $F^n$ by applying the Binomial Theorem to $(1+x)^n$ . I don't understand the step where I need to find the derivative of these terms: = $x^2 + {n \choose 1}x^3 + {n \choose 2} x^4 + .... + {n \choose n-1} x^{n+1} + x^{n+2} $ Even though I do understand how to show that for the first 3 terms the nth derivative is 0, but for the last two I have no clue?? Really appreciate any hints!!","Let n be an element of the set of  natural numbers Let and write for the nth derivative of the function . Compute by applying the Binomial Theorem to . I don't understand the step where I need to find the derivative of these terms: = Even though I do understand how to show that for the first 3 terms the nth derivative is 0, but for the last two I have no clue?? Really appreciate any hints!!",F(x)=x^2(1+x)^n F^n F F^n (1+x)^n x^2 + {n \choose 1}x^3 + {n \choose 2} x^4 + .... + {n \choose n-1} x^{n+1} + x^{n+2} ,"['derivatives', 'binomial-theorem']"
72,Differentiability implies bounded variation?,Differentiability implies bounded variation?,,"Does a differentiable function on $[a,b]$ has bounded variation ? I recall that differentiable on $[a,b]$ means differentiable on $(a,b)$ and $\lim_{x\to a^+}f(x)$ and $\lim_{x\to b^-}f(x)$ exist. I know that if $f'$ is integrable on $[a,b]$ , then it works (since we can majorate $V_{[a,b]}(f)$ by $\int_a^b|f'|$ ). But if $f'$ is not integrable ? Can we find a differentiable function on $[a,b]$ that has no bounded variation ?","Does a differentiable function on has bounded variation ? I recall that differentiable on means differentiable on and and exist. I know that if is integrable on , then it works (since we can majorate by ). But if is not integrable ? Can we find a differentiable function on that has no bounded variation ?","[a,b] [a,b] (a,b) \lim_{x\to a^+}f(x) \lim_{x\to b^-}f(x) f' [a,b] V_{[a,b]}(f) \int_a^b|f'| f' [a,b]","['real-analysis', 'derivatives', 'bounded-variation']"
73,Function is differentiable at a point so its differentiable in a region from the point,Function is differentiable at a point so its differentiable in a region from the point,,"Given some function $f: I \subseteq\mathbb R \rightarrow \mathbb R$ , Which is differentiable twice at some point $a\in I$ . Can one say that there is a region around the point where the function is differentiable twice, without any other information? so I assume that its true because if we look at the first derivative which is: $lim_{h\rightarrow0} \frac {f(a+h)-f(a)}h $ then obliviously we can ""take"" h to be smaller as we want and then I can assume that if the limit exists then it exists at some region of that point a. so I guess the same goes for the second derivative.","Given some function , Which is differentiable twice at some point . Can one say that there is a region around the point where the function is differentiable twice, without any other information? so I assume that its true because if we look at the first derivative which is: then obliviously we can ""take"" h to be smaller as we want and then I can assume that if the limit exists then it exists at some region of that point a. so I guess the same goes for the second derivative.",f: I \subseteq\mathbb R \rightarrow \mathbb R a\in I lim_{h\rightarrow0} \frac {f(a+h)-f(a)}h ,"['calculus', 'derivatives', 'differential']"
74,"Show $f(f(b))-f(f(a))=(f'(c))^2(b-a)$ for some $c \in (a, b)$",Show  for some,"f(f(b))-f(f(a))=(f'(c))^2(b-a) c \in (a, b)","Let $f:[a,b]\rightarrow [a,b]$ be a differentiable function with continuous and positive first derivative. Prove that there exists $c\in(a,b)$ ,such that $$f(f(b))-f(f(a))=(f'(c))^2(b-a).$$ My Attempt: I approached the LHS in following manner: $$\left(\frac{f(f(b))-f(f(a))}{f(b)-f(a)}\right)\left(\frac{f(b)-f(a)}{b-a}\right)=(f'(c))^2$$ But I am not able to decide which function to use so that LMVT may be applied","Let be a differentiable function with continuous and positive first derivative. Prove that there exists ,such that My Attempt: I approached the LHS in following manner: But I am not able to decide which function to use so that LMVT may be applied","f:[a,b]\rightarrow [a,b] c\in(a,b) f(f(b))-f(f(a))=(f'(c))^2(b-a). \left(\frac{f(f(b))-f(f(a))}{f(b)-f(a)}\right)\left(\frac{f(b)-f(a)}{b-a}\right)=(f'(c))^2","['real-analysis', 'derivatives']"
75,How to derive derivative of the logarithm of a summation?,How to derive derivative of the logarithm of a summation?,,"I'm currently reading the book Deep Learning (Goodfellow et al., 2015) and had a question regarding the calculation of a gradient when explaining backpropagation for a certain example. For anyone who's curious, this is from section 6.5.9: Differentiation outside the Deep Learning Community . Suppose we have variables $p_1, p_2, ... , p_n$ representing probabilities and variables $z_1, z_2, ... , z_n$ representing unnormalized log probabilities. Suppose we define $$q_i = \frac{e^{z_i}}{\sum_i e^{z_i}}$$ where we build the softmax function out of exponentiation, summation and division operations, and construct a cross-entropy loss $J = -\sum_i p_i \log{q_i}$ . A human mathematician can observe that the derivateive of $J$ with respect to $z_i$ takes a very simple form: $q_i - p_i$ . I don't know how this result was derived, and was hoping that someone could give me some tips or advice. What I have so far is $$\log{q_i} = \log{e^{z_i}} - \log({\sum_i e^{z_i}})$$ $$ \begin{align} p_i\log{q_i} & = p_i \log{e^{z_i}} - p_i \log({\sum_i e^{z_i}}) \\ & = p_iz_i - p_i\log(\sum_i e^{z_i}) \end{align}$$ If we take the derivative of $J = p_i\log{q_i}$ then I can understand that $d/dz_i (p_i z_i) = p_i$ , but how do we differentiate the second term that contains the logarithm of the summation? Thank you.","I'm currently reading the book Deep Learning (Goodfellow et al., 2015) and had a question regarding the calculation of a gradient when explaining backpropagation for a certain example. For anyone who's curious, this is from section 6.5.9: Differentiation outside the Deep Learning Community . Suppose we have variables representing probabilities and variables representing unnormalized log probabilities. Suppose we define where we build the softmax function out of exponentiation, summation and division operations, and construct a cross-entropy loss . A human mathematician can observe that the derivateive of with respect to takes a very simple form: . I don't know how this result was derived, and was hoping that someone could give me some tips or advice. What I have so far is If we take the derivative of then I can understand that , but how do we differentiate the second term that contains the logarithm of the summation? Thank you.","p_1, p_2, ... , p_n z_1, z_2, ... , z_n q_i = \frac{e^{z_i}}{\sum_i e^{z_i}} J = -\sum_i p_i \log{q_i} J z_i q_i - p_i \log{q_i} = \log{e^{z_i}} - \log({\sum_i e^{z_i}}) 
\begin{align}
p_i\log{q_i} & = p_i \log{e^{z_i}} - p_i \log({\sum_i e^{z_i}}) \\
& = p_iz_i - p_i\log(\sum_i e^{z_i})
\end{align} J = p_i\log{q_i} d/dz_i (p_i z_i) = p_i","['derivatives', 'summation', 'logarithms']"
76,Variant of integration form for delta function $\frac{1}{2}\int_{-\infty}^\infty |k| \exp(ikx)dk$,Variant of integration form for delta function,\frac{1}{2}\int_{-\infty}^\infty |k| \exp(ikx)dk,"I want to simplify the integration $$\frac{1}{2}\int_{-\infty}^\infty |k| \exp(ikx)dk$$ $$=\int_0^\infty k \cos(kx)dk$$ using some kind of delta function. Delta function has integration form, such as $$\delta(x)=\frac{1}{2\pi}\int_{-\infty}^\infty e^{ikx} dk$$ $$         =\frac{1}{\pi}\int_{0}^\infty \cos{(kx)} dk$$ Also, the derivative of delta function has integration form, $$\delta'(x)=\frac{1}{2\pi}\int_{-\infty}^\infty ike^{ikx} dk$$ $$         =-\frac{1}{\pi}\int_{0}^\infty k\sin{(kx)} dk$$ All integration looks very similar. So, I think the first integration can be written using delta function. If you know how to do this, please tell me.","I want to simplify the integration using some kind of delta function. Delta function has integration form, such as Also, the derivative of delta function has integration form, All integration looks very similar. So, I think the first integration can be written using delta function. If you know how to do this, please tell me.",\frac{1}{2}\int_{-\infty}^\infty |k| \exp(ikx)dk =\int_0^\infty k \cos(kx)dk \delta(x)=\frac{1}{2\pi}\int_{-\infty}^\infty e^{ikx} dk          =\frac{1}{\pi}\int_{0}^\infty \cos{(kx)} dk \delta'(x)=\frac{1}{2\pi}\int_{-\infty}^\infty ike^{ikx} dk          =-\frac{1}{\pi}\int_{0}^\infty k\sin{(kx)} dk,"['integration', 'derivatives']"
77,Find coordinate points where the tangent line is horizontal for $f(x) = –\sin(8x) + 6\cos(4x) – 8x$,Find coordinate points where the tangent line is horizontal for,f(x) = –\sin(8x) + 6\cos(4x) – 8x,"Problem: Consider the function $f(x) = –\sin(8x) + 6\cos(4x) – 8x$ where $–\pi/4 < x < \pi/2$ .  Find the exact $x$ -coordinates of the points on the graph of f at which there is a horizontal tangent line. This is my attempt. Find derivative $$ f’(x) = -\cos(8x)\cdot8 – 6\sin(4x)\cdot4 – 8 $$ $$f’(x) = -8\cos(8x) – 24\sin(4x) – 8$$ $$ f’(x) = -8(\cos(8x) + 3\sin(4x))$$ $$f’(x) = 0 $$ $$0 = -8(\cos(8x) + 3\sin(4x) + 1)$$ Use double angle formula $1-\sin(4x)$ to replace $\cos(8x)$ $$0 = -8(1 – \sin^2(4x) + 3\sin(4x) + 1)$$ Let $u = \sin(4x)$ $$ 0 = 1 – u^2 + 3u + 1$$ $$ 0 = -u^2 + 3u + 2$$ Use quadratic formula to solve for $u$ . $$\sin(x) = (3\pm \sqrt{17}) / 2$$ Answer can only be negative due to out of bounds so $\sin(x) = (3-\sqrt17) / 2$ Multiply equation by $4$ because $\cos(4x) $ $$\sin(4x) = [4\cdot(3-\sqrt{17})] / 2$$ I tried multiplying by $4$ but $\sin$ becomes out of bounds So I just tried to get the inverse of $\sin(x)$ to find the reference angle. $$\sin^{-1}(\frac{3–\sqrt{17}}{2}) = -0.596\text{rad} = -34^\circ$$ $$x = (\pi + 0.596), (2\pi-0.596), (-0.596), (-\pi+0.596)$$ After graphing these tangent lines in a calculator, the points are close but not exact and thus are not horizontal. I've done other horizontal tangent line problems but this one, it seems I have to use the quadratic formula to find $\sin(x)$ , rather than factoring. Any advice on what I'm doing wrong would be greatly appreciated.","Problem: Consider the function where .  Find the exact -coordinates of the points on the graph of f at which there is a horizontal tangent line. This is my attempt. Find derivative Use double angle formula to replace Let Use quadratic formula to solve for . Answer can only be negative due to out of bounds so Multiply equation by because I tried multiplying by but becomes out of bounds So I just tried to get the inverse of to find the reference angle. After graphing these tangent lines in a calculator, the points are close but not exact and thus are not horizontal. I've done other horizontal tangent line problems but this one, it seems I have to use the quadratic formula to find , rather than factoring. Any advice on what I'm doing wrong would be greatly appreciated.","f(x) = –\sin(8x) + 6\cos(4x) – 8x –\pi/4 < x < \pi/2 x  f’(x) = -\cos(8x)\cdot8 – 6\sin(4x)\cdot4 – 8  f’(x) = -8\cos(8x) – 24\sin(4x) – 8  f’(x) = -8(\cos(8x) + 3\sin(4x)) f’(x) = 0  0 = -8(\cos(8x) + 3\sin(4x) + 1) 1-\sin(4x) \cos(8x) 0 = -8(1 – \sin^2(4x) + 3\sin(4x) + 1) u = \sin(4x)  0 = 1 – u^2 + 3u + 1  0 = -u^2 + 3u + 2 u \sin(x) = (3\pm \sqrt{17}) / 2 \sin(x) = (3-\sqrt17) / 2 4 \cos(4x)  \sin(4x) = [4\cdot(3-\sqrt{17})] / 2 4 \sin \sin(x) \sin^{-1}(\frac{3–\sqrt{17}}{2}) = -0.596\text{rad} = -34^\circ x = (\pi + 0.596), (2\pi-0.596), (-0.596), (-\pi+0.596) \sin(x)","['calculus', 'derivatives', 'trigonometry', 'tangent-line']"
78,$\int_0^1\bigl( f(x)+f^{-1}(x )\bigr)\>dx=1$ [duplicate],[duplicate],\int_0^1\bigl( f(x)+f^{-1}(x )\bigr)\>dx=1,"This question already has answers here : Show: $ f(a) = a,\ f(b) = b \implies \int_a^b \left[ f(x) + f^{-1}(x) \right] \, \mathrm{d}x = b^2 - a^2 $ [duplicate] (3 answers) Closed 4 years ago . Given any strictly-increasing continuous function $f$ such that $f(0) = 0$ and $f(1) = 1$ , show that $$\int_0^1 [f(x)+f^{-1}(x) ]dx= 1.$$ I tried using mean value theorem but it won't work. I tried few examples it is correct. Edit: If I draw  graph of picture, and seeing integral as area, we can see the integral is just area of the square $\{(0,0),(0,1),(1,1),(1,0)\}$ . Please give me a hint to attack this problem analytically.","This question already has answers here : Show: $ f(a) = a,\ f(b) = b \implies \int_a^b \left[ f(x) + f^{-1}(x) \right] \, \mathrm{d}x = b^2 - a^2 $ [duplicate] (3 answers) Closed 4 years ago . Given any strictly-increasing continuous function such that and , show that I tried using mean value theorem but it won't work. I tried few examples it is correct. Edit: If I draw  graph of picture, and seeing integral as area, we can see the integral is just area of the square . Please give me a hint to attack this problem analytically.","f f(0) = 0 f(1) = 1 \int_0^1 [f(x)+f^{-1}(x) ]dx= 1. \{(0,0),(0,1),(1,1),(1,0)\}","['real-analysis', 'calculus', 'integration', 'derivatives']"
79,How do I find the derivative of $y = e^{-x^2}$ with respect to $y$?,How do I find the derivative of  with respect to ?,y = e^{-x^2} y,"I want to find the derivative of $y = e^{-x^2}$ with $-1 \leq x \leq 1$ Can I take the log of both sides? $$\ln y = -x^2$$ From here, can I say that $-x^2$ is always = $x^2$ ? If so, I get: $$\ln y = x^2 \implies \sqrt{\ln y} = x$$ What's the derivative? $$\frac{dx}{dy} = \frac{1}{2} \left(\ln y^{\frac{-1}{2}} \right) \cdot \frac{1}{y} = \frac{1}{2y \sqrt{\ln y}}$$ Is that right?","I want to find the derivative of with Can I take the log of both sides? From here, can I say that is always = ? If so, I get: What's the derivative? Is that right?",y = e^{-x^2} -1 \leq x \leq 1 \ln y = -x^2 -x^2 x^2 \ln y = x^2 \implies \sqrt{\ln y} = x \frac{dx}{dy} = \frac{1}{2} \left(\ln y^{\frac{-1}{2}} \right) \cdot \frac{1}{y} = \frac{1}{2y \sqrt{\ln y}},"['calculus', 'derivatives']"
80,Simplify recurrence $\frac{d}{dx} f_{n-1}(x)= f_n(x)- f_{n-1}(x) f_1(x)$,Simplify recurrence,\frac{d}{dx} f_{n-1}(x)= f_n(x)- f_{n-1}(x) f_1(x),Suppose we have a sequence of infinitely differentiable functions $ \{ f_k(x) \}$ .   Now suppose that these functions satisfy the following recursion: \begin{align} \frac{d}{dx} f_{n-1}(x)= f_n(x)-  f_{n-1}(x) f_1(x) \end{align} where $f_0(x)=1$ for all $x$ .  Can we re-write $f_n(x)$ only in terms of $f_1(x)$ and derivatives of $f_1(x)$ ? It is not difficult to see that this is possible. The difficulty is to create the exact formula.,Suppose we have a sequence of infinitely differentiable functions .   Now suppose that these functions satisfy the following recursion: where for all .  Can we re-write only in terms of and derivatives of ? It is not difficult to see that this is possible. The difficulty is to create the exact formula.," \{ f_k(x) \} \begin{align}
\frac{d}{dx} f_{n-1}(x)= f_n(x)-  f_{n-1}(x) f_1(x)
\end{align} f_0(x)=1 x f_n(x) f_1(x) f_1(x)","['calculus', 'derivatives', 'recurrence-relations']"
81,$\int(x+1)\cdot f'(x)= x^3+x^2-x+c$ and $f(0)=\frac{1}{2}$. What is $f(-1)$?,and . What is ?,\int(x+1)\cdot f'(x)= x^3+x^2-x+c f(0)=\frac{1}{2} f(-1),"$\int(x+1)\cdot f'(x)= x^3+x^2-x+c$ and $f(0)=\frac{1}{2}$ . What is $f(-1)$ ? I took the derivative of both sides and then factored the quadratic equation on the right: $$(x+1)\cdot f'(x)=3x^2+2x-1$$ $$(x+1)\cdot f'(x)=(3x-1)(x+1)$$ At this point, if I divide both sides of the equation by $x+1$ , taking the integral becomes easy and I can figure out the answer as $3$ . Though, because we divided by $x+1$ at the beginning, we should note that $x\not = -1$ . Therefore, I actually can't compute $f(-1)$ . Is there a different simple way of solving this problem?","and . What is ? I took the derivative of both sides and then factored the quadratic equation on the right: At this point, if I divide both sides of the equation by , taking the integral becomes easy and I can figure out the answer as . Though, because we divided by at the beginning, we should note that . Therefore, I actually can't compute . Is there a different simple way of solving this problem?",\int(x+1)\cdot f'(x)= x^3+x^2-x+c f(0)=\frac{1}{2} f(-1) (x+1)\cdot f'(x)=3x^2+2x-1 (x+1)\cdot f'(x)=(3x-1)(x+1) x+1 3 x+1 x\not = -1 f(-1),"['calculus', 'integration', 'derivatives']"
82,If $D$ be the differentiation operator on $V$. Find $D^*$.,If  be the differentiation operator on . Find .,D V D^*,"Let $V$ be the vector space of the polynomials over $R$ of degree less than or   equal to $3$ with the inner product space $(f|g)=\int_{0} ^{1}f(t)g(t) dt$ , and let $D$ be the differentiation operator on V. Find $D^*$ Attempt As I did the calculation was very large, so I will explain what I did step by step so that you say if I made a mistake or I hit STEP 1 : First I considered a canonical basis of $R _{\leq 2} [X]$ that is $ \{ 1,x,x^2,x^3\}$ . I did the Gram-Schmidt Process to orthogonalize this base. Resulted in $$ \left\{ 1, x-\frac{1}{2}, x^2-x+ \frac{1}{6}, x^3-\frac{3}{2}x^2+\frac{3}{5}x- \frac{1}{2} \right\}$$ STEP 2: Orthonormalize this base, dividing each element by its norm (in this part I may have been wrong because they gave many calculations). Resulted in $$ \{ 1, \sqrt{12} \left(x-\frac{1}{2} \right) , \sqrt{180} \left( x^2-x+ \frac{1}{6} \right) , \sqrt{\frac{33600}{18772}} \left( x^3-\frac{3}{2}x^2+\frac{3}{5}x- \frac{1}{2} \right) \}$$ STEP 3: Write the matrix of $D$ $$\begin{bmatrix}0&\sqrt{12}&0& \frac{1}{10} \sqrt{\frac{33600}{18772}}\\0&0&2\sqrt{15}& 0\\ 0&0&0& \sqrt{\frac{33600}{33378960}} \\ 0&0&0& 0 \end{bmatrix}$$ STEP4: The matrix of $D^*$ is the conjugate transpose of the matrix of $D$ ,(in this case only transpose) according to the corollary: Let $V$ be a finite-dimensional inner product space, and let $T$ be a linear operator on $V$ . In any orthonormal basis for $V$ , the matrix of $T^*$ is the conjugate transpose of the matrix of $T$ . At least the idea is correct? Is there any easier way to do this exercise?","Let be the vector space of the polynomials over of degree less than or   equal to with the inner product space , and let be the differentiation operator on V. Find Attempt As I did the calculation was very large, so I will explain what I did step by step so that you say if I made a mistake or I hit STEP 1 : First I considered a canonical basis of that is . I did the Gram-Schmidt Process to orthogonalize this base. Resulted in STEP 2: Orthonormalize this base, dividing each element by its norm (in this part I may have been wrong because they gave many calculations). Resulted in STEP 3: Write the matrix of STEP4: The matrix of is the conjugate transpose of the matrix of ,(in this case only transpose) according to the corollary: Let be a finite-dimensional inner product space, and let be a linear operator on . In any orthonormal basis for , the matrix of is the conjugate transpose of the matrix of . At least the idea is correct? Is there any easier way to do this exercise?","V R 3 (f|g)=\int_{0} ^{1}f(t)g(t) dt D D^* R _{\leq 2} [X]  \{ 1,x,x^2,x^3\}  \left\{ 1, x-\frac{1}{2}, x^2-x+ \frac{1}{6}, x^3-\frac{3}{2}x^2+\frac{3}{5}x- \frac{1}{2} \right\}  \{ 1, \sqrt{12} \left(x-\frac{1}{2} \right) , \sqrt{180} \left( x^2-x+ \frac{1}{6} \right) , \sqrt{\frac{33600}{18772}} \left( x^3-\frac{3}{2}x^2+\frac{3}{5}x- \frac{1}{2} \right) \} D \begin{bmatrix}0&\sqrt{12}&0& \frac{1}{10} \sqrt{\frac{33600}{18772}}\\0&0&2\sqrt{15}& 0\\ 0&0&0& \sqrt{\frac{33600}{33378960}} \\ 0&0&0& 0 \end{bmatrix} D^* D V T V V T^* T","['linear-algebra', 'derivatives', 'proof-verification', 'adjoint-operators']"
83,Using the chain rule to guess the derivative of $\frac{1}{x}$,Using the chain rule to guess the derivative of,\frac{1}{x},"So I learned that you can use the product rule to guess what the derivative of $\frac{1}{x}$ should be, you just use the fact that $(\frac{1}{x}\cdot x) = 1$ and differentiate both sides and solve for $(\frac{1}{x})'$ . My teacher said that there is a similar trick for guessing the derivative of $(\frac{1}{x})$ using the chain rule, I tried using the fact that $\frac{1}{x}\circ\frac{1}{x}= x$ but I am stuck.","So I learned that you can use the product rule to guess what the derivative of should be, you just use the fact that and differentiate both sides and solve for . My teacher said that there is a similar trick for guessing the derivative of using the chain rule, I tried using the fact that but I am stuck.",\frac{1}{x} (\frac{1}{x}\cdot x) = 1 (\frac{1}{x})' (\frac{1}{x}) \frac{1}{x}\circ\frac{1}{x}= x,"['calculus', 'derivatives']"
84,When is the polynomial $P(|x+y|)$ total differentiable?,When is the polynomial  total differentiable?,P(|x+y|),"If $P \in \mathbb{R}[x]$ is a polynomial, under which sufficient condition is the function: $$f: \mathbb{R}^2 \to \mathbb{R}: f(x,y) = P(|x+y|)$$ total differentiable? So for a function to be total differentiable, every partial derivative need to be continuous or the function needs to fulfill the definition of total differentiability: $$\lim_{x \to a} \frac{||f(x) - f(a) -A(x-a)||}{||x-a||} = 0$$ where A is the linear map with matrix $A = (df)(a).$ So the first condition I have is that $x+y \neq 0 $ . But I have the feeling that there needs be a more general or specific condition.","If is a polynomial, under which sufficient condition is the function: total differentiable? So for a function to be total differentiable, every partial derivative need to be continuous or the function needs to fulfill the definition of total differentiability: where A is the linear map with matrix So the first condition I have is that . But I have the feeling that there needs be a more general or specific condition.","P \in \mathbb{R}[x] f: \mathbb{R}^2 \to \mathbb{R}: f(x,y) = P(|x+y|) \lim_{x \to a} \frac{||f(x) - f(a) -A(x-a)||}{||x-a||} = 0 A = (df)(a). x+y \neq 0 ","['real-analysis', 'derivatives', 'absolute-value']"
85,Explain this derivative identity: $ \frac{1}{2^n} \frac{d^n}{dy^n} \frac{(1+y)^{2n+3}(1-y)}{((1+y)^2 -2yx)^2} \bigg|_{y=0} = (n+1)! x^n $,Explain this derivative identity:, \frac{1}{2^n} \frac{d^n}{dy^n} \frac{(1+y)^{2n+3}(1-y)}{((1+y)^2 -2yx)^2} \bigg|_{y=0} = (n+1)! x^n ,"I have the following result that I believe to be true: $$ \frac{1}{2^n} \frac{d^n}{dy^n} \frac{(1+y)^{2n+3}(1-y)}{((1+y)^2 -2yx)^2} \bigg|_{y=0} = (n+1)! x^n $$ The LHS is something that arose in physics research. The RHS has been inferred by checking with Mathematica for n from 0 to 100. However, proving this result has evaded me. It's not surprising that the RHS is a polynomial in x with coefficient $(n+1)!$ (the only way to get an $x^n$ term is to operate the derivative on the denominator repeatedly). What is surprising is that the coefficients vanish for all but the leading term. This looks like it might be amenable to induction or recursion, but I was unable to make any meaningful headway with those techniques. Good luck!","I have the following result that I believe to be true: The LHS is something that arose in physics research. The RHS has been inferred by checking with Mathematica for n from 0 to 100. However, proving this result has evaded me. It's not surprising that the RHS is a polynomial in x with coefficient (the only way to get an term is to operate the derivative on the denominator repeatedly). What is surprising is that the coefficients vanish for all but the leading term. This looks like it might be amenable to induction or recursion, but I was unable to make any meaningful headway with those techniques. Good luck!","
\frac{1}{2^n} \frac{d^n}{dy^n} \frac{(1+y)^{2n+3}(1-y)}{((1+y)^2 -2yx)^2} \bigg|_{y=0} = (n+1)! x^n
 (n+1)! x^n","['derivatives', 'induction', 'recursion']"
86,Numerical (Second) Derivative of Time Series Data,Numerical (Second) Derivative of Time Series Data,,"First and second order derivatives are often used in chromatography to detect hidden peaks. The time series data consists of Instrumental Response vs. Time at very short time intervals (250 Hz). I wanted to calculate the second derivative of the data numerically in Excel. The simple option is that we calculate the first derivative and then calculate the first derivative of the first derivative to get the second derivative.  The other option is to use the direct approach using central difference formula for the second derivative. The question is about the denominator of the second derivative from the central difference formula. It should the square of the time interval. This is my understanding and it is consistent dimensionally for example distance x (m) becomes acceleration (m/s2) as the second derivative of x. A reviewer wrote a rather denigrating comment saying that there is a lack of understanding of the second derivative ""definition"" where the authors assert that the definition of a second derivative requires division by the square of the time interval. This reference to the square of a time interval suggests a worrying lack of understanding of the nature of the derivative 𝒅𝟐/𝒅𝒕𝟐 as an operator and not as an algebraic variable. Do mathematicians agree with the above comment? Can we interpret d^2/dt^2 as if it is repeating the d operator twice divided by time interval squared? Thanks.","First and second order derivatives are often used in chromatography to detect hidden peaks. The time series data consists of Instrumental Response vs. Time at very short time intervals (250 Hz). I wanted to calculate the second derivative of the data numerically in Excel. The simple option is that we calculate the first derivative and then calculate the first derivative of the first derivative to get the second derivative.  The other option is to use the direct approach using central difference formula for the second derivative. The question is about the denominator of the second derivative from the central difference formula. It should the square of the time interval. This is my understanding and it is consistent dimensionally for example distance x (m) becomes acceleration (m/s2) as the second derivative of x. A reviewer wrote a rather denigrating comment saying that there is a lack of understanding of the second derivative ""definition"" where the authors assert that the definition of a second derivative requires division by the square of the time interval. This reference to the square of a time interval suggests a worrying lack of understanding of the nature of the derivative 𝒅𝟐/𝒅𝒕𝟐 as an operator and not as an algebraic variable. Do mathematicians agree with the above comment? Can we interpret d^2/dt^2 as if it is repeating the d operator twice divided by time interval squared? Thanks.",,"['calculus', 'discrete-mathematics', 'derivatives', 'operator-theory']"
87,Find the derivative of an integral function,Find the derivative of an integral function,,Find the derivative of the function: $$g(x)=\int _{ 1 }^{ \tan(x) }{ e^ {t^ 2}dt } $$ I have the answer as $\sec^2(x)\tan(x).$ Can anyone show the steps of how to solve this problem...,Find the derivative of the function: $$g(x)=\int _{ 1 }^{ \tan(x) }{ e^ {t^ 2}dt } $$ I have the answer as $\sec^2(x)\tan(x).$ Can anyone show the steps of how to solve this problem...,,"['calculus', 'integration', 'derivatives', 'chain-rule']"
88,Derivitive of $e^{-x^2}$,Derivitive of,e^{-x^2},"I am currently re-learning high school/university calculus for a project that I am working on. I know that the derivitive of the above function is as shown below: $$ -2 \cdot x \cdot e^{-x^2} $$ I can get at this solution using the chain rule: $$ g(x) = f(h(x) $$ $$ g^{'}(x) = f^{'}(h(x))  \cdot  h^{'}(x) $$ However, when I write $ e^{-x^2} $ as $$ \frac{1}{e^{x^2}} $$ and then apply the quotient rule, I get the following: $$ g(x) = \frac{f(x)}{h(x)} $$ $$ g^{'}(x) = f^{'}(x) \cdot h(x) - f(x) \cdot h^{'}(x) $$ Apply this to my fraction I get: $$ 0 \cdot e^{x^2} - e^{x^2}  \cdot  2 \cdot x $$ which is $$ -2 \cdot x \cdot e^{x^2} $$ which is not the correct solution. Am I missing something here? Any help would be appreciated.","I am currently re-learning high school/university calculus for a project that I am working on. I know that the derivitive of the above function is as shown below: $$ -2 \cdot x \cdot e^{-x^2} $$ I can get at this solution using the chain rule: $$ g(x) = f(h(x) $$ $$ g^{'}(x) = f^{'}(h(x))  \cdot  h^{'}(x) $$ However, when I write $ e^{-x^2} $ as $$ \frac{1}{e^{x^2}} $$ and then apply the quotient rule, I get the following: $$ g(x) = \frac{f(x)}{h(x)} $$ $$ g^{'}(x) = f^{'}(x) \cdot h(x) - f(x) \cdot h^{'}(x) $$ Apply this to my fraction I get: $$ 0 \cdot e^{x^2} - e^{x^2}  \cdot  2 \cdot x $$ which is $$ -2 \cdot x \cdot e^{x^2} $$ which is not the correct solution. Am I missing something here? Any help would be appreciated.",,"['calculus', 'derivatives']"
89,Generalization of the chain rule using upper derivatives,Generalization of the chain rule using upper derivatives,,"I'm having trouble with the following exercise, taken from Chapter 6 of Royden and Fitzpatrick's Real Analysis : Let $f$ be defined on $[a,b]$ and $g$ a continuous function on $[\alpha, \beta]$ that is differentiable at $\gamma \in (\alpha, \beta)$ with $g(\gamma) = c \in (a,b)$. Show that if $g'(\gamma) > 0$, then $\bar D(f \circ g)(\gamma) = \bar D f(c) \cdot g'(\gamma)$ where $$\bar Df(x) = \lim_{h \to 0} \sup_{0 < |t| \leq h} \frac{f(x + t) - f(x)}{t}$$ If $g$ were linear, the problem is straight-forward: \begin{multline} \bar D(f \circ g)(\gamma) = \lim_{h \to 0}\sup_{0 < |t| \leq h}\frac{f(c + g'(\gamma)\cdot t) - f(c)}{t} = \\\lim_{h \to 0}\sup_{0 < |s| \leq g'(\gamma)\cdot h}\frac{f(c + s) - f(c)}{s}\cdot g'(\gamma) = \bar D f(c) \cdot g'(\gamma) \end{multline} I'm having trouble in the general case, in part because the problem places so little structure on $f$. The error from linearly approximating $g(\cdot)$ at $\gamma$ can be made arbitrarily small, but without a continuity condition on $f$ I'm unsure how to make the approximation error ""go away"". Thank you!","I'm having trouble with the following exercise, taken from Chapter 6 of Royden and Fitzpatrick's Real Analysis : Let $f$ be defined on $[a,b]$ and $g$ a continuous function on $[\alpha, \beta]$ that is differentiable at $\gamma \in (\alpha, \beta)$ with $g(\gamma) = c \in (a,b)$. Show that if $g'(\gamma) > 0$, then $\bar D(f \circ g)(\gamma) = \bar D f(c) \cdot g'(\gamma)$ where $$\bar Df(x) = \lim_{h \to 0} \sup_{0 < |t| \leq h} \frac{f(x + t) - f(x)}{t}$$ If $g$ were linear, the problem is straight-forward: \begin{multline} \bar D(f \circ g)(\gamma) = \lim_{h \to 0}\sup_{0 < |t| \leq h}\frac{f(c + g'(\gamma)\cdot t) - f(c)}{t} = \\\lim_{h \to 0}\sup_{0 < |s| \leq g'(\gamma)\cdot h}\frac{f(c + s) - f(c)}{s}\cdot g'(\gamma) = \bar D f(c) \cdot g'(\gamma) \end{multline} I'm having trouble in the general case, in part because the problem places so little structure on $f$. The error from linearly approximating $g(\cdot)$ at $\gamma$ can be made arbitrarily small, but without a continuity condition on $f$ I'm unsure how to make the approximation error ""go away"". Thank you!",,"['real-analysis', 'derivatives']"
90,Is twice differentiability an open property?,Is twice differentiability an open property?,,"A function $f : \mathbb{R} \to \mathbb{R} $ which is differentiable at a point $a \in \mathbb{R}$ need not be differentiable in a neighbourhood of $a$. For example, $$ f : \mathbb{R} \to \mathbb{R} : x \mapsto \begin{cases} x^2 && x \in \mathbb{Q} \\ -x^2 && x \in \mathbb{R} \setminus \mathbb{Q} \end{cases} $$ is differentiable only in $0$. If $f$ is twice differentiable in $a$, then of course $f$ is differentiable in a neighbourhood of $a$. But is $f$ twice differentiable in a neighbourhood of $a$? I find it hard to come up with a natural counterexample, but see no reason why this statement should hold either.","A function $f : \mathbb{R} \to \mathbb{R} $ which is differentiable at a point $a \in \mathbb{R}$ need not be differentiable in a neighbourhood of $a$. For example, $$ f : \mathbb{R} \to \mathbb{R} : x \mapsto \begin{cases} x^2 && x \in \mathbb{Q} \\ -x^2 && x \in \mathbb{R} \setminus \mathbb{Q} \end{cases} $$ is differentiable only in $0$. If $f$ is twice differentiable in $a$, then of course $f$ is differentiable in a neighbourhood of $a$. But is $f$ twice differentiable in a neighbourhood of $a$? I find it hard to come up with a natural counterexample, but see no reason why this statement should hold either.",,"['calculus', 'derivatives']"
91,Integral without differential,Integral without differential,,"It is clear to me what the meaning of this is: $$\int_a^b f(x)dx$$ Also, I know that $$\int_a^b \frac{df}{dx}dx=f(b)-f(a)$$ However, I don't quite understand what the meaning of this is, namely if we treat $dx$ as a variable and simplify the fraction: $$\int_a^b df= f(b)-f(a) ??$$ And generally, what is the meaning of this? (without the $dx$) $$\int_a^bf(x)$$","It is clear to me what the meaning of this is: $$\int_a^b f(x)dx$$ Also, I know that $$\int_a^b \frac{df}{dx}dx=f(b)-f(a)$$ However, I don't quite understand what the meaning of this is, namely if we treat $dx$ as a variable and simplify the fraction: $$\int_a^b df= f(b)-f(a) ??$$ And generally, what is the meaning of this? (without the $dx$) $$\int_a^bf(x)$$",,"['calculus', 'integration', 'derivatives', 'definite-integrals']"
92,"My students don't appreciate the marginal cost function, please help,","My students don't appreciate the marginal cost function, please help,",,"Given a cost function c(x), and knowing the cost for some large enough x, say, 500 units, we can find the incremental cost of producing the 501st unit, by computing c'(500).  And then we can also find the incremental cost of producing the 502nd unit by computing c'(501).  So, with this information, we know the incremental costs for producing an extra 2 units, from 500 to 502. My students weren't satisfied at all with this example, because many of them were perplexed about why I couldn't just compute c(502) - c(500) to get my answer.  I was stumped and told them that I don't think they are wrong, but at the moment, I did not have a better answer for them to convince them that the marginal cost function + using derivatives is a good thing. What can I tell them to highlight the usefulness of the marginal cost function and derivatives?","Given a cost function c(x), and knowing the cost for some large enough x, say, 500 units, we can find the incremental cost of producing the 501st unit, by computing c'(500).  And then we can also find the incremental cost of producing the 502nd unit by computing c'(501).  So, with this information, we know the incremental costs for producing an extra 2 units, from 500 to 502. My students weren't satisfied at all with this example, because many of them were perplexed about why I couldn't just compute c(502) - c(500) to get my answer.  I was stumped and told them that I don't think they are wrong, but at the moment, I did not have a better answer for them to convince them that the marginal cost function + using derivatives is a good thing. What can I tell them to highlight the usefulness of the marginal cost function and derivatives?",,"['derivatives', 'education', 'economics']"
93,Find tangent equation for a curve which is perpendicular to a line,Find tangent equation for a curve which is perpendicular to a line,,"How do I find the point on the curve $y=1/(2x−1)$ where the tangent line will be perpendicular to the line defined by $x−2y−1=0$? What I have so far:  The equation of the given line is $y= 0.5x+1$ Since the slope is $-0.5$, the tangent slope must be $2$ .: $y=-2x+b$ (tangent line) The differential of the curve is $y=-2/((2x-1)^2)$. I can't figure out how to get the b value for the tangent, nor the point at which this happens.","How do I find the point on the curve $y=1/(2x−1)$ where the tangent line will be perpendicular to the line defined by $x−2y−1=0$? What I have so far:  The equation of the given line is $y= 0.5x+1$ Since the slope is $-0.5$, the tangent slope must be $2$ .: $y=-2x+b$ (tangent line) The differential of the curve is $y=-2/((2x-1)^2)$. I can't figure out how to get the b value for the tangent, nor the point at which this happens.",,"['calculus', 'derivatives', 'tangent-line']"
94,Curiosity on function maxima,Curiosity on function maxima,,"I was recently working with an equation of the form: $$ \frac{\sqrt{x}}{a+bx+c\sqrt{x}} $$ And I realized that the maxima (only considering positive real numbers) would always be at the point where: $$ x=\frac ab  $$ This is straightforward to prove by finding where the first derivative equals 0. Given this 'easy' result, I tried to find the logic behind it, which should probably be something easy, but I do not find it (I'm evidently no expert in mathematics, just curious). My question is, should it be evident that the function has a maxima at that point without having to calculate the derivative? In the case it should, could someone explain me the reasoning behind it? Thank you in advance. Kind regards, J.","I was recently working with an equation of the form: $$ \frac{\sqrt{x}}{a+bx+c\sqrt{x}} $$ And I realized that the maxima (only considering positive real numbers) would always be at the point where: $$ x=\frac ab  $$ This is straightforward to prove by finding where the first derivative equals 0. Given this 'easy' result, I tried to find the logic behind it, which should probably be something easy, but I do not find it (I'm evidently no expert in mathematics, just curious). My question is, should it be evident that the function has a maxima at that point without having to calculate the derivative? In the case it should, could someone explain me the reasoning behind it? Thank you in advance. Kind regards, J.",,"['derivatives', 'maxima-minima']"
95,Why does the derivative of $\ln(\sin x)$ = $\cot x$?,Why does the derivative of  = ?,\ln(\sin x) \cot x,I thought the derivative of $\ln(u)$ was $\frac{u'}{u}$.  So why does the derivative of $\ln(\sin x)$ = $\frac{1}{\sin x}(\cos x)$ instead of $\frac{\cos x}{\sin x}(\cos x)$?,I thought the derivative of $\ln(u)$ was $\frac{u'}{u}$.  So why does the derivative of $\ln(\sin x)$ = $\frac{1}{\sin x}(\cos x)$ instead of $\frac{\cos x}{\sin x}(\cos x)$?,,"['calculus', 'derivatives']"
96,How to differentiate $y=\ln(x+\sqrt{1+x^2})$?,How to differentiate ?,y=\ln(x+\sqrt{1+x^2}),"I'm trying to differentiate the equation below but I fear there must have been an error made. I can't seem to reconcile to the correct answer. The problem comes from James Stewart's Calculus Early Transcendentals, 7th Ed., Page 223, Exercise 25. Please differentiate $y=\ln(x+\sqrt{1+x^2})$ My Answer: Differentiate using the natural log rule: $$y'=\left(\frac{1}{x+(1+x^2)^{1/2}}\right)\cdot\left(x+(1+x^2)^{1/2}\right)'$$ Now to differentiate the second term, note the chain rule applied and then simplification: $$\left(x+(1+x^2)^{1/2}\right)'=1+\frac{1}{2}\cdot(1+x^2)^{-1/2}\cdot(2x)$$ $$1+\frac{1}{2}\cdot(1+x^2)^{-1/2}\cdot(2x)=1+\frac{x}{(1+x^2)^{1/2}}$$ Our expression is now: $$y'=\left(\frac{1}{x+(1+x^2)^{1/2}}\right)\cdot\left(1+\frac{x}{(1+x^2)^{1/2}}\right)$$ Distribute the left term across the two right terms for my result: $$y'=\left(\frac{1}{x+(1+x^2)^{1/2}}\right)+\left(\frac{x}{\left(x+(1+x^2)^{1/2}\right)\left(1+x^2\right)^{1/2}}\right)$$ $$y'=\left(\frac{1}{x+(1+x^2)^{1/2}}\right)+\left(\frac{x}{\left(x(1+x^2)^{1/2}\right)+(1+x^2)^{1}}\right)$$ At this point I can see that if I simplify further by adding the fractions I'll still have too many terms, and it will get awfully messy. The answer per the book (below) has far fewer terms than mine. I'd just like to know where I've gone wrong in my algebra. Thank you for your help. Here's the correct answer: $$y'=\frac{1}{\sqrt{1+x^2}}$$","I'm trying to differentiate the equation below but I fear there must have been an error made. I can't seem to reconcile to the correct answer. The problem comes from James Stewart's Calculus Early Transcendentals, 7th Ed., Page 223, Exercise 25. Please differentiate $y=\ln(x+\sqrt{1+x^2})$ My Answer: Differentiate using the natural log rule: $$y'=\left(\frac{1}{x+(1+x^2)^{1/2}}\right)\cdot\left(x+(1+x^2)^{1/2}\right)'$$ Now to differentiate the second term, note the chain rule applied and then simplification: $$\left(x+(1+x^2)^{1/2}\right)'=1+\frac{1}{2}\cdot(1+x^2)^{-1/2}\cdot(2x)$$ $$1+\frac{1}{2}\cdot(1+x^2)^{-1/2}\cdot(2x)=1+\frac{x}{(1+x^2)^{1/2}}$$ Our expression is now: $$y'=\left(\frac{1}{x+(1+x^2)^{1/2}}\right)\cdot\left(1+\frac{x}{(1+x^2)^{1/2}}\right)$$ Distribute the left term across the two right terms for my result: $$y'=\left(\frac{1}{x+(1+x^2)^{1/2}}\right)+\left(\frac{x}{\left(x+(1+x^2)^{1/2}\right)\left(1+x^2\right)^{1/2}}\right)$$ $$y'=\left(\frac{1}{x+(1+x^2)^{1/2}}\right)+\left(\frac{x}{\left(x(1+x^2)^{1/2}\right)+(1+x^2)^{1}}\right)$$ At this point I can see that if I simplify further by adding the fractions I'll still have too many terms, and it will get awfully messy. The answer per the book (below) has far fewer terms than mine. I'd just like to know where I've gone wrong in my algebra. Thank you for your help. Here's the correct answer: $$y'=\frac{1}{\sqrt{1+x^2}}$$",,"['calculus', 'derivatives']"
97,Is twice-differentiability at a point required for there to exist a point of inflection?,Is twice-differentiability at a point required for there to exist a point of inflection?,,"I'm going off of a similar post and I don't quite understand what mathematicians believe - can $f(x)$ have a point of inflection at $c$ if $f''(c)$ does not exist? The post I linked seems to hint that this is not possible but does not address if there is a change in concavity. If the concavity changes, then shouldn't there exist a point of inflection at the point even if its second derivative does not exist?","I'm going off of a similar post and I don't quite understand what mathematicians believe - can $f(x)$ have a point of inflection at $c$ if $f''(c)$ does not exist? The post I linked seems to hint that this is not possible but does not address if there is a change in concavity. If the concavity changes, then shouldn't there exist a point of inflection at the point even if its second derivative does not exist?",,"['calculus', 'derivatives']"
98,A sequential definition of the derivative,A sequential definition of the derivative,,"For a function $f$ differentiable at a point $c$, take sequences $x_n$ and $y_n$ with $x_n < c < y_n$ for all $n$ and both of those sequences converge to $c$. The problem is to prove that $$\lim_{n\to\infty} \frac{f(y_n)-f(x_n)}{y_n -x_n}=f'(c)$$ The intuition is clear, but I'm struggling to provide a rigorous argument. One approach I was considering was to suppose the equality doesn't hold and then argue that this would imply either $f(y_n)$ or $f(x_n)$ doesn't converge to $f(c)$ and thus the function $f$ could not be continuous which is a contradiction since we assumed $f$ was differentiable. I don't think that argument is valid, and if there is a direct approach, I would be interested to see it.","For a function $f$ differentiable at a point $c$, take sequences $x_n$ and $y_n$ with $x_n < c < y_n$ for all $n$ and both of those sequences converge to $c$. The problem is to prove that $$\lim_{n\to\infty} \frac{f(y_n)-f(x_n)}{y_n -x_n}=f'(c)$$ The intuition is clear, but I'm struggling to provide a rigorous argument. One approach I was considering was to suppose the equality doesn't hold and then argue that this would imply either $f(y_n)$ or $f(x_n)$ doesn't converge to $f(c)$ and thus the function $f$ could not be continuous which is a contradiction since we assumed $f$ was differentiable. I don't think that argument is valid, and if there is a direct approach, I would be interested to see it.",,"['real-analysis', 'derivatives']"
99,Derivative of a function is same the function,Derivative of a function is same the function,,Lets extend to nth order derivative  functions and not merely second order derivative functions. The 4th order(say) derivative of the following functions is same the function itself $$ \begin{aligned} f(x)&=e^{x}\\ f(x)&=0\\ f(x)&=\sin x\\ f(x)&=\cos x \end{aligned} $$ I was curious to know whether there are there any more such unique functions ?,Lets extend to nth order derivative  functions and not merely second order derivative functions. The 4th order(say) derivative of the following functions is same the function itself $$ \begin{aligned} f(x)&=e^{x}\\ f(x)&=0\\ f(x)&=\sin x\\ f(x)&=\cos x \end{aligned} $$ I was curious to know whether there are there any more such unique functions ?,,"['calculus', 'derivatives']"
