,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Convergence of a integral - heat Kernel and dirac delta function,Convergence of a integral - heat Kernel and dirac delta function,,"Consider $\varphi \in S(R^n)$ (space of rapidly decreasing functions). Consider the heat kernel $$ K_t(x) = \displaystyle\frac{1}{{(4\pi t)}^{n/2}} \displaystyle e^{- \displaystyle\frac{|x|^2}{4t}}, t>0 , x \in R^n$$ I want to show that  $$\displaystyle\lim_{t \rightarrow 0^{+}}\displaystyle\int_{R^n} e^{- \displaystyle\frac{|x|^2}{4t}} \varphi(x)  \ dx  = \varphi(0).$$ My try: we have $\displaystyle\int_{R^n} K_t(x) \ dx = 1$ and $\lim_{t \rightarrow 0^{+} }\displaystyle\int_{|x|\geq \epsilon}\displaystyle\frac{1}{{(4\pi t)}^{n/2}}e^{- \displaystyle\frac{|x|^2}{4t}}=0$ for all $\epsilon >0 $. Then $$ \displaystyle\lim_{t \rightarrow 0^{+} } |\displaystyle\int_{R^n} e^{- \displaystyle\frac{|x|^2}{4t}} \varphi(x)  \ dx - \varphi(0)| = \displaystyle\lim_{t \rightarrow 0^{+} } |  \displaystyle\int_{R^n} \displaystyle\frac{1}{{(4\pi t)}^{n/2}}e^{- \displaystyle\frac{|x|^2}{4t}} \varphi(x)  \ dx -\displaystyle\int_{R^n}\displaystyle\frac{1}{{(4\pi t)}^{n/2}} e^{- \displaystyle\frac{|x|^2}{4t}}\varphi(0) \ dx|$$ $$ \leq \displaystyle\lim_{t \rightarrow 0^{+} } || \varphi - \varphi(0)||_{\infty}. \displaystyle\lim_{\epsilon \rightarrow 0^{+} } \displaystyle\int_{|x|\geq \epsilon}\displaystyle\frac{1}{{(4\pi t)}^{n/2}}e^{- \displaystyle\frac{|x|^2}{4t}}  \ dx$$ $$ =\displaystyle\lim_{\epsilon \rightarrow 0^{+} } || \varphi - \varphi(0)||_{\infty} \displaystyle\lim_{t \rightarrow 0^{+} }\displaystyle\int_{|x|\geq \epsilon}\displaystyle\frac{1}{{(4\pi t)}^{n/2}}e^{- \displaystyle\frac{|x|^2}{4t}}  \ dx  = 0$$ I dont know if my solution is correct (i am not sure about the last line). Someone can give me a hint to this exercise ?","Consider $\varphi \in S(R^n)$ (space of rapidly decreasing functions). Consider the heat kernel $$ K_t(x) = \displaystyle\frac{1}{{(4\pi t)}^{n/2}} \displaystyle e^{- \displaystyle\frac{|x|^2}{4t}}, t>0 , x \in R^n$$ I want to show that  $$\displaystyle\lim_{t \rightarrow 0^{+}}\displaystyle\int_{R^n} e^{- \displaystyle\frac{|x|^2}{4t}} \varphi(x)  \ dx  = \varphi(0).$$ My try: we have $\displaystyle\int_{R^n} K_t(x) \ dx = 1$ and $\lim_{t \rightarrow 0^{+} }\displaystyle\int_{|x|\geq \epsilon}\displaystyle\frac{1}{{(4\pi t)}^{n/2}}e^{- \displaystyle\frac{|x|^2}{4t}}=0$ for all $\epsilon >0 $. Then $$ \displaystyle\lim_{t \rightarrow 0^{+} } |\displaystyle\int_{R^n} e^{- \displaystyle\frac{|x|^2}{4t}} \varphi(x)  \ dx - \varphi(0)| = \displaystyle\lim_{t \rightarrow 0^{+} } |  \displaystyle\int_{R^n} \displaystyle\frac{1}{{(4\pi t)}^{n/2}}e^{- \displaystyle\frac{|x|^2}{4t}} \varphi(x)  \ dx -\displaystyle\int_{R^n}\displaystyle\frac{1}{{(4\pi t)}^{n/2}} e^{- \displaystyle\frac{|x|^2}{4t}}\varphi(0) \ dx|$$ $$ \leq \displaystyle\lim_{t \rightarrow 0^{+} } || \varphi - \varphi(0)||_{\infty}. \displaystyle\lim_{\epsilon \rightarrow 0^{+} } \displaystyle\int_{|x|\geq \epsilon}\displaystyle\frac{1}{{(4\pi t)}^{n/2}}e^{- \displaystyle\frac{|x|^2}{4t}}  \ dx$$ $$ =\displaystyle\lim_{\epsilon \rightarrow 0^{+} } || \varphi - \varphi(0)||_{\infty} \displaystyle\lim_{t \rightarrow 0^{+} }\displaystyle\int_{|x|\geq \epsilon}\displaystyle\frac{1}{{(4\pi t)}^{n/2}}e^{- \displaystyle\frac{|x|^2}{4t}}  \ dx  = 0$$ I dont know if my solution is correct (i am not sure about the last line). Someone can give me a hint to this exercise ?",,"['analysis', 'measure-theory', 'partial-differential-equations', 'distribution-theory']"
1,Are there any function which's derivate is the scaled one of the original?( $f'(x) = f(cx)$ ),Are there any function which's derivate is the scaled one of the original?(  ),f'(x) = f(cx),"which function could satisfy the following, for a certain $c\ne1$ $f'(x) = f(cx)$ ...beyond the trivial $f=0$ i've been thinking about it for a while. for a simpler case: $f'(x)= f(x+c)$ i've found $e^{xe^v}$ where $v$ is the solution of $c=\frac{v}{e^v}$","which function could satisfy the following, for a certain $c\ne1$ $f'(x) = f(cx)$ ...beyond the trivial $f=0$ i've been thinking about it for a while. for a simpler case: $f'(x)= f(x+c)$ i've found $e^{xe^v}$ where $v$ is the solution of $c=\frac{v}{e^v}$",,['analysis']
2,Compactly supported continuous function is uniformly continuous,Compactly supported continuous function is uniformly continuous,,"Let $f:\mathbb R \rightarrow \mathbb R$ be continuous and compactly supported. How can I prove that $f$ is uniformly continuous ? I was trying to prove it by contradiction but get stuck. My attempt was as follows: Let $E$ be the compact support of $f$ . On $E$ we know that $f$ is uniformly continuous. If we assume that $f$ is not uniformly continuous we know $$ \exists \epsilon > 0 \forall \delta > 0 \exists x,y \in \mathbb R: |x-y|<\delta \wedge |f(x)-f(y)| \geq \epsilon $$ Let $\delta_n := \frac 1n$ and fix this $\epsilon > 0$ . Compute a $\delta > 0$ s.t. $\forall x,y \in E:|x-y| < \delta \rightarrow |f(x)-f(y)| <\epsilon$ . Let $n \geq N$ s.t. $\frac 1N < \delta$ . Then we may assume wlog that $x_n \in E$ and $y_n \in \mathbb R \setminus E$ wehere $x_n,y_n$ are the points corresponding with $\delta_n$ . This gives a sequence of points where $|x_n-y_n| \rightarrow 0$ and $x_n \in E$ and $y_n \in \mathbb R \setminus E$ . I now want to use somehow the continuitiy of $f$ . How can I do this ? Is this approach a good one ? Can it be more simple ? New idea: I know that $E$ is compact so $(y_n)_{n=0}^\infty$ has a convergent subsequence $(y_{n_j})_{j=0}^\infty$ whit limit say $y \in E$ . Now $$  |x_{n_j}-y| \leq |x_{n_j}-y_{n_j}|+|y_{n_j}+y| \rightarrow 0 $$ So I can take $x_{n_j}$ close to $y$ to get $|f(x_{n_j})-f(y)| = |f(y)| <\frac \epsilon 2$ by the continuity. I can also take $y_{n_j}$ close to $y$ to get $|f(y_{n_j})-f(y)| <\frac \epsilon 2$ . We further have $$  |f(y_{n_j})| \leq |f(y_{n_j})-f(y)| + |f(y)| $$ s.t. $$   |f(y)| \geq |f(y_{n_j})| - |f(y_{n_j})-f(y)| \geq \frac \epsilon 2 $$ because $|f(y_{n_j})| \geq \epsilon$ per construction. So we have $|f(y)| \leq \frac \epsilon 2$ and $|f(y)| > \frac \epsilon 2$ which is a contradiction. New solution: Let $\epsilon > 0$ . Let $\delta_1$ for the uniform continuity on $E$ . Further $$  \forall x \in E,\ \exists \delta_x > 0\ \forall y \in \mathbb{R}: |x-y|< \delta_x \rightarrow |f(x)-f(y)| < \epsilon $$ Compute an open finite over of $E$ $$  E \subseteq \bigcup_{i=1}^N B\left(x_i,\frac{\delta_{x_i}}2\right) $$ Write $\delta_i := \delta_{x_i}$ . Let $\delta_2 := \min_{i=1,\cdots,N} \frac {\delta_i} 2$ and $\delta := \min(\delta_1,\delta_2)$ . Assume $|x-y|< \delta$ . If $x,y \in E$ or $x,y \notin E$ we are done. Otherwise assume $x \in E$ and $y \notin E$ . Then $x \in B\left(x_i,\frac{\delta_i}{2}\right)$ for some $i$ . Further $$  |y-x_i| \leq |y-x|+|x-x_i|\leq \delta_2 + \frac{\delta_i}2 \leq \delta_i $$ thus $y \in B(x_i,\delta_i)$ which proves the claim.",Let be continuous and compactly supported. How can I prove that is uniformly continuous ? I was trying to prove it by contradiction but get stuck. My attempt was as follows: Let be the compact support of . On we know that is uniformly continuous. If we assume that is not uniformly continuous we know Let and fix this . Compute a s.t. . Let s.t. . Then we may assume wlog that and wehere are the points corresponding with . This gives a sequence of points where and and . I now want to use somehow the continuitiy of . How can I do this ? Is this approach a good one ? Can it be more simple ? New idea: I know that is compact so has a convergent subsequence whit limit say . Now So I can take close to to get by the continuity. I can also take close to to get . We further have s.t. because per construction. So we have and which is a contradiction. New solution: Let . Let for the uniform continuity on . Further Compute an open finite over of Write . Let and . Assume . If or we are done. Otherwise assume and . Then for some . Further thus which proves the claim.,"f:\mathbb R \rightarrow \mathbb R f E f E f f 
\exists \epsilon > 0 \forall \delta > 0 \exists x,y \in \mathbb R: |x-y|<\delta \wedge |f(x)-f(y)| \geq \epsilon
 \delta_n := \frac 1n \epsilon > 0 \delta > 0 \forall x,y \in E:|x-y| < \delta \rightarrow |f(x)-f(y)| <\epsilon n \geq N \frac 1N < \delta x_n \in E y_n \in \mathbb R \setminus E x_n,y_n \delta_n |x_n-y_n| \rightarrow 0 x_n \in E y_n \in \mathbb R \setminus E f E (y_n)_{n=0}^\infty (y_{n_j})_{j=0}^\infty y \in E 
 |x_{n_j}-y| \leq |x_{n_j}-y_{n_j}|+|y_{n_j}+y| \rightarrow 0
 x_{n_j} y |f(x_{n_j})-f(y)| = |f(y)| <\frac \epsilon 2 y_{n_j} y |f(y_{n_j})-f(y)| <\frac \epsilon 2 
 |f(y_{n_j})| \leq |f(y_{n_j})-f(y)| + |f(y)|
 
  |f(y)| \geq |f(y_{n_j})| - |f(y_{n_j})-f(y)| \geq \frac \epsilon 2
 |f(y_{n_j})| \geq \epsilon |f(y)| \leq \frac \epsilon 2 |f(y)| > \frac \epsilon 2 \epsilon > 0 \delta_1 E 
 \forall x \in E,\ \exists \delta_x > 0\ \forall y \in \mathbb{R}: |x-y|< \delta_x \rightarrow |f(x)-f(y)| < \epsilon
 E 
 E \subseteq \bigcup_{i=1}^N B\left(x_i,\frac{\delta_{x_i}}2\right)
 \delta_i := \delta_{x_i} \delta_2 := \min_{i=1,\cdots,N} \frac {\delta_i} 2 \delta := \min(\delta_1,\delta_2) |x-y|< \delta x,y \in E x,y \notin E x \in E y \notin E x \in B\left(x_i,\frac{\delta_i}{2}\right) i 
 |y-x_i| \leq |y-x|+|x-x_i|\leq \delta_2 + \frac{\delta_i}2 \leq \delta_i
 y \in B(x_i,\delta_i)","['analysis', 'continuity']"
3,Caccioppoli-Leray Inequality for De Giorgi's regularity theorem,Caccioppoli-Leray Inequality for De Giorgi's regularity theorem,,"I am studying De Giorgi's proof of Holder continuity of solutions of elliptic equations with bounded measurable coefficients. This is the translation of the original paper De Giorgi paper At page 163, De Giorgi refers to a ""Lemma by Caccioppoli and Leray"" but I can't find it anywhere, the referenced book is very hard to come across. If anyone has it (""Equazioni alle derivate parziali di tipo ellittico"" by C.Miranda) and can look at what this lemma at page 153 is it would be great. The inequality I am struggling with, is, in any case: $$\int_{A(k)\cap B(y,\varrho_2)} (u(x)-k)^2 dx\geq (\varrho_2-\varrho_1)^2 \frac{\tau_1}{\tau_2}\sqrt{\int_{A(k)\cap \partial B(y,\varrho _1)}(u(x)-k)^2d\mu_{n-1}\cdot \int_{A(k)\cap \partial B(y,\varrho _1)}|\nabla u(x)|^2d\mu_{n-1}}$$ where $A(k)$ is the subset of the domain where the solution of the elliptic equation (with constants $\tau_1, \tau_2$) $u(x)$ is greater than $k$, $B(x,r)$ is the n-dimensional ball centred at $x$ of radius $r$ and $\partial$ indicates the boundary. Thank you very much for any hint, reference or idea!","I am studying De Giorgi's proof of Holder continuity of solutions of elliptic equations with bounded measurable coefficients. This is the translation of the original paper De Giorgi paper At page 163, De Giorgi refers to a ""Lemma by Caccioppoli and Leray"" but I can't find it anywhere, the referenced book is very hard to come across. If anyone has it (""Equazioni alle derivate parziali di tipo ellittico"" by C.Miranda) and can look at what this lemma at page 153 is it would be great. The inequality I am struggling with, is, in any case: $$\int_{A(k)\cap B(y,\varrho_2)} (u(x)-k)^2 dx\geq (\varrho_2-\varrho_1)^2 \frac{\tau_1}{\tau_2}\sqrt{\int_{A(k)\cap \partial B(y,\varrho _1)}(u(x)-k)^2d\mu_{n-1}\cdot \int_{A(k)\cap \partial B(y,\varrho _1)}|\nabla u(x)|^2d\mu_{n-1}}$$ where $A(k)$ is the subset of the domain where the solution of the elliptic equation (with constants $\tau_1, \tau_2$) $u(x)$ is greater than $k$, $B(x,r)$ is the n-dimensional ball centred at $x$ of radius $r$ and $\partial$ indicates the boundary. Thank you very much for any hint, reference or idea!",,"['analysis', 'reference-request', 'partial-differential-equations', 'sobolev-spaces']"
4,"Wave Equation, Energy methods.","Wave Equation, Energy methods.",,"I am reading the book of Evans, Partial differential Equations ... wave equation section 2.4; subsection 2.4.3: Energy methods. Arriving at the theorem: Theorem 5 (Uniqueness for wave equation). There exists at most one function $u \in C^{2}(\overline{U}_{T})$ solving $u_{tt} -\Delta u=f $ in $ U_{T}$ $u=g $ on $ \Gamma_{T}$ $u_{t}=h$ on $U \times \{t=0\}.$ Proof. If $\tilde{u}$ is another such solution, then $ w:=u-\tilde{u}$ solves $w_{tt} -\Delta w=0 $ in $ U_{T}$ $w=0 $ on $ \Gamma_{T}$ $w_{t}=0$ on $U \times \{t=0\}.$ Define the ""energy"" $e(t):=\frac{1}{2} \int_{U} w^{2}_{t}(x,t)+ \mid Dw(x,t)\mid ^{2} dx (0\leq t \leq T).$ We compute $\dot{e}(t)=\int_{U} w_{t}w_{tt}+ Dw \cdot Dw_{t}dx (\cdot = \frac{d}{dt})$ $=\int_{U}w_{t}(w_{tt} - \Delta w)dx=0$. There is no boundary term since $w=0$, and hence $w_{t}=0$, on $\partial U \times [0,T].$ Thus for all $0\leq t \leq T, e(t)= e(0)=0$, and so $w_{t}, Dw \equiv 0$ within $U_{T}$. Since $w \equiv 0$ on$ U \times \{t=0\}$, we conclude $w=u-\tilde {u}\equiv 0$ in $U_{T}$. I have two questions: 1) What is the motivation for the definition of $e(t)$ 2)$\int_{U} w_{t}w_{tt}+ Dw \cdot Dw_{t}dx $ $=\int_{U}w_{t}(w_{tt} - \Delta w)dx$. How to justify this equality? Thank very much.","I am reading the book of Evans, Partial differential Equations ... wave equation section 2.4; subsection 2.4.3: Energy methods. Arriving at the theorem: Theorem 5 (Uniqueness for wave equation). There exists at most one function $u \in C^{2}(\overline{U}_{T})$ solving $u_{tt} -\Delta u=f $ in $ U_{T}$ $u=g $ on $ \Gamma_{T}$ $u_{t}=h$ on $U \times \{t=0\}.$ Proof. If $\tilde{u}$ is another such solution, then $ w:=u-\tilde{u}$ solves $w_{tt} -\Delta w=0 $ in $ U_{T}$ $w=0 $ on $ \Gamma_{T}$ $w_{t}=0$ on $U \times \{t=0\}.$ Define the ""energy"" $e(t):=\frac{1}{2} \int_{U} w^{2}_{t}(x,t)+ \mid Dw(x,t)\mid ^{2} dx (0\leq t \leq T).$ We compute $\dot{e}(t)=\int_{U} w_{t}w_{tt}+ Dw \cdot Dw_{t}dx (\cdot = \frac{d}{dt})$ $=\int_{U}w_{t}(w_{tt} - \Delta w)dx=0$. There is no boundary term since $w=0$, and hence $w_{t}=0$, on $\partial U \times [0,T].$ Thus for all $0\leq t \leq T, e(t)= e(0)=0$, and so $w_{t}, Dw \equiv 0$ within $U_{T}$. Since $w \equiv 0$ on$ U \times \{t=0\}$, we conclude $w=u-\tilde {u}\equiv 0$ in $U_{T}$. I have two questions: 1) What is the motivation for the definition of $e(t)$ 2)$\int_{U} w_{t}w_{tt}+ Dw \cdot Dw_{t}dx $ $=\int_{U}w_{t}(w_{tt} - \Delta w)dx$. How to justify this equality? Thank very much.",,"['analysis', 'partial-differential-equations']"
5,Critical exponents and point-wise convergence,Critical exponents and point-wise convergence,,"A phase change is only possible in a physical system which obeys the laws of statistical mechanics if the infinite series for the partition function of that system converges non-uniformly (i.e. converges point-wise).  This is because in order for a phase change to occur, the partition function must converge to two different continuous functions in different regions of the phase plane; and this can only happen if the number of terms in the partition function is infinite. It might be possible, using the analogy with phase changes, to make some general statements about partition functions which converge non-uniformly to two different continuous functions.  This is interesting to me because point-wise convergence is a very weak condition, and the analogy with phase changes seems to suggest a path towards a deeper understanding of this form of convergence. One of the most interesting subjects in the statistical mechanics of phase transitions is the so-called ""renormalization group"" (which is not a group) and the critical exponents it predicts (these are non integer numbers which are associated with a ""critical point"" in the phase plane).  Another powerful result from renormalization group theory is the concept of universality, in which the thermodynamic variables of the system are symmetric across all length scales at a certain critical point.  Universality is directly related to the critical exponents and critical point behavior, but as is fairly standard for theoretical physics, the mathematics used is non-rigorous (I recommend the book ""scaling and renormalization in statistical physics"" by Cardy) For the point-wise convergence of a partition function, can we put the foundations of critical exponents on a rigorous mathematical basis; in the sense that we can somehow predict their values from properties (such as the partial derivatives) of the partition function?","A phase change is only possible in a physical system which obeys the laws of statistical mechanics if the infinite series for the partition function of that system converges non-uniformly (i.e. converges point-wise).  This is because in order for a phase change to occur, the partition function must converge to two different continuous functions in different regions of the phase plane; and this can only happen if the number of terms in the partition function is infinite. It might be possible, using the analogy with phase changes, to make some general statements about partition functions which converge non-uniformly to two different continuous functions.  This is interesting to me because point-wise convergence is a very weak condition, and the analogy with phase changes seems to suggest a path towards a deeper understanding of this form of convergence. One of the most interesting subjects in the statistical mechanics of phase transitions is the so-called ""renormalization group"" (which is not a group) and the critical exponents it predicts (these are non integer numbers which are associated with a ""critical point"" in the phase plane).  Another powerful result from renormalization group theory is the concept of universality, in which the thermodynamic variables of the system are symmetric across all length scales at a certain critical point.  Universality is directly related to the critical exponents and critical point behavior, but as is fairly standard for theoretical physics, the mathematics used is non-rigorous (I recommend the book ""scaling and renormalization in statistical physics"" by Cardy) For the point-wise convergence of a partition function, can we put the foundations of critical exponents on a rigorous mathematical basis; in the sense that we can somehow predict their values from properties (such as the partial derivatives) of the partition function?",,"['analysis', 'statistics']"
6,"Is there a function $f:[0;1]\rightarrow \Bbb R ^n$ of class $C^ \infty$ with $f^{(i)}(0)=10^i.f^{(i)}(1) \, \forall \, i \in \Bbb N_0$",Is there a function  of class  with,"f:[0;1]\rightarrow \Bbb R ^n C^ \infty f^{(i)}(0)=10^i.f^{(i)}(1) \, \forall \, i \in \Bbb N_0","I have the function $g:\Bbb R_{>0} \rightarrow [0.1;1)$ given by $$g(x)=10^{-1-\lfloor \log_{10} (x) \rfloor}.x$$ Which kind of ""erase"" the coma in the decimal expresion of $x$ . For example $$g(123,539)=0,123539 \qquad; \qquad g(0,00012)=0,12$$ And I'm trying to find a function $f:[0.1;1]\rightarrow \Bbb R ^n$ of class $C^ \infty$ such that $f  \circ g$ is a function of class $C^ \infty$ (because $g$ it's not a pretty function and I want to make it pretty in order to be able to work with it). I've managed to prove $f  \circ g$ is $C^ \infty$ if and only if $f$ satisfy the following property $$f^{(i)}(0.1)=10^i.f^{(i)}(1) \, \forall \, i \in \Bbb N_0$$ I've also manage to prove this is equivalent to finding a function $h:[0;1]\rightarrow \Bbb R ^n$ of class $C^ \infty$ with the same property $$h^{(i)}(0)=10^i.h^{(i)}(1) \, \forall \, i \in \Bbb N_0$$ Because we can take $f(x)=h((10x-1)/9)$ and it will satisfy the property we were looking for. In order to avoid trivial solutions such as $h$ beeing constant, I will also ask for the following property $$x\neq y \qquad h(x)=h(y) \Rightarrow x=0 \; , \; y=1$$ So $h$ is injective in $(0;1)$ . I haven't been able to find such a function and I don't know how to search for one so I'm kind of lost.","I have the function given by Which kind of ""erase"" the coma in the decimal expresion of . For example And I'm trying to find a function of class such that is a function of class (because it's not a pretty function and I want to make it pretty in order to be able to work with it). I've managed to prove is if and only if satisfy the following property I've also manage to prove this is equivalent to finding a function of class with the same property Because we can take and it will satisfy the property we were looking for. In order to avoid trivial solutions such as beeing constant, I will also ask for the following property So is injective in . I haven't been able to find such a function and I don't know how to search for one so I'm kind of lost.","g:\Bbb R_{>0} \rightarrow [0.1;1) g(x)=10^{-1-\lfloor \log_{10} (x) \rfloor}.x x g(123,539)=0,123539 \qquad; \qquad g(0,00012)=0,12 f:[0.1;1]\rightarrow \Bbb R ^n C^ \infty f  \circ g C^ \infty g f  \circ g C^ \infty f f^{(i)}(0.1)=10^i.f^{(i)}(1) \, \forall \, i \in \Bbb N_0 h:[0;1]\rightarrow \Bbb R ^n C^ \infty h^{(i)}(0)=10^i.h^{(i)}(1) \, \forall \, i \in \Bbb N_0 f(x)=h((10x-1)/9) h x\neq y \qquad h(x)=h(y) \Rightarrow x=0 \; , \; y=1 h (0;1)","['analysis', 'derivatives']"
7,Bounding the ratio of the $l^1$ norms of two vectors to the ratio of their $l^2$ norms,Bounding the ratio of the  norms of two vectors to the ratio of their  norms,l^1 l^2,"I am having difficulty proving or disproving a claim of the following type: Suppose $\{x_1,\ldots,x_n\}$, $\{y_1,\ldots,y_n\}$ are sets of real numbers satisfying $0 < x_j \leq y_j$ for each $1 \leq j \leq n$, and suppose that $X := \sum_{1 \leq j \leq n} x_j^2$, $Y := \sum_{1\leq j \leq n} y_j^2$.  Do there exist absolute constants $c_1,c_2 > 0$ such that \begin{equation*} \frac{\sum_{1 \leq j \leq n} x_j}{\sum_{1 \leq j \leq n} y_j} \leq c_1\left(\frac{X}{Y}\right)^{c_2}. \end{equation*} Evidently, this is interpretable as bounding the ratio of the $l^1$ norms of two vectors by a power of the ratio of their $l^2$ norms. Obviously, we have a trivial bound with $c_2 = 0$ has $c_1 = 1$, which is undesirable. A simple application of the Cauchy-Schwarz inequality to the numerator, and of the simple inequality $\left(\sum_{1 \leq j \leq n} y_j^2\right) \leq \left(\sum_{1 \leq j \leq n} y_j\right)^2$ in the denominator gives the bound with $c_2 = \frac{1}{2}$ but $c_1 = n^{\frac{1}{2}}$, which is not suitable, and furthermore, for large $n$ this is worse than the trivial bound mentioned above. \ In my particular application I am taking $n \rightarrow \infty$, my sequence $\{x_n\}_n$ (and thus $\{y_n\}_n$ as well), each term of which depends in some sense on $n$, is growing with $n$, and $\frac{\max_j x_j}{\min_j x_j} \rightarrow \infty$. The same properties are also true of $\{y_j\}_j$. For this reason, reverse Cauchy-Schwarz type inequalities such as that of Polya-Szego do not produce satisfactory improvements when $n$ is significantly larger than either of $X$ or $Y$. My feeling is that in such generality we ought to allow $c_1$ or $c_2$ to depend on the sequences in question to some degree, but not in a way that reduces to something trivial. For instance, $c_2 = \frac{1}{\log(Y/X)}$ and $c_1 = e$ reduces to the trivial bound by 1, and is thus not interesting. I welcome any help in this regard, though I would prefer that it be given in the form of a hint, rather than as an explicit proof or counterexample. Thank you in advance. Update: I was actually able to prove this with $c_1 = 1$ and $c_2 = \frac{1}{4}$. It turns out in my application, however, that I need $c_2 = \frac{1}{2}$. It is clear that $\frac{1}{2}$ is best possible (e.g., by taking $x_1$ and $y_1$ very large compared to each of the remaining $x_j$ and $y_j$, or by allowing zero components and taking $\mathbf{x} := (x,0,\ldots,0)$ and $\mathbf{y} := (y,0,\ldots,0)$; these examples are obviously morally the same). Any absolute $c_1$ is allowable in my application, so any help with the case $c_2 = \frac{1}{2}$ is warmly welcome, even if it does not yield the best constant $c_1$.","I am having difficulty proving or disproving a claim of the following type: Suppose $\{x_1,\ldots,x_n\}$, $\{y_1,\ldots,y_n\}$ are sets of real numbers satisfying $0 < x_j \leq y_j$ for each $1 \leq j \leq n$, and suppose that $X := \sum_{1 \leq j \leq n} x_j^2$, $Y := \sum_{1\leq j \leq n} y_j^2$.  Do there exist absolute constants $c_1,c_2 > 0$ such that \begin{equation*} \frac{\sum_{1 \leq j \leq n} x_j}{\sum_{1 \leq j \leq n} y_j} \leq c_1\left(\frac{X}{Y}\right)^{c_2}. \end{equation*} Evidently, this is interpretable as bounding the ratio of the $l^1$ norms of two vectors by a power of the ratio of their $l^2$ norms. Obviously, we have a trivial bound with $c_2 = 0$ has $c_1 = 1$, which is undesirable. A simple application of the Cauchy-Schwarz inequality to the numerator, and of the simple inequality $\left(\sum_{1 \leq j \leq n} y_j^2\right) \leq \left(\sum_{1 \leq j \leq n} y_j\right)^2$ in the denominator gives the bound with $c_2 = \frac{1}{2}$ but $c_1 = n^{\frac{1}{2}}$, which is not suitable, and furthermore, for large $n$ this is worse than the trivial bound mentioned above. \ In my particular application I am taking $n \rightarrow \infty$, my sequence $\{x_n\}_n$ (and thus $\{y_n\}_n$ as well), each term of which depends in some sense on $n$, is growing with $n$, and $\frac{\max_j x_j}{\min_j x_j} \rightarrow \infty$. The same properties are also true of $\{y_j\}_j$. For this reason, reverse Cauchy-Schwarz type inequalities such as that of Polya-Szego do not produce satisfactory improvements when $n$ is significantly larger than either of $X$ or $Y$. My feeling is that in such generality we ought to allow $c_1$ or $c_2$ to depend on the sequences in question to some degree, but not in a way that reduces to something trivial. For instance, $c_2 = \frac{1}{\log(Y/X)}$ and $c_1 = e$ reduces to the trivial bound by 1, and is thus not interesting. I welcome any help in this regard, though I would prefer that it be given in the form of a hint, rather than as an explicit proof or counterexample. Thank you in advance. Update: I was actually able to prove this with $c_1 = 1$ and $c_2 = \frac{1}{4}$. It turns out in my application, however, that I need $c_2 = \frac{1}{2}$. It is clear that $\frac{1}{2}$ is best possible (e.g., by taking $x_1$ and $y_1$ very large compared to each of the remaining $x_j$ and $y_j$, or by allowing zero components and taking $\mathbf{x} := (x,0,\ldots,0)$ and $\mathbf{y} := (y,0,\ldots,0)$; these examples are obviously morally the same). Any absolute $c_1$ is allowable in my application, so any help with the case $c_2 = \frac{1}{2}$ is warmly welcome, even if it does not yield the best constant $c_1$.",,"['analysis', 'inequality']"
8,Prove that there is a positive number x such that $ x^3= 5$,Prove that there is a positive number x such that, x^3= 5,"Prove that there is a positive number $x$ such that $ x^3= 5$ Define $S=\{x\in\mathbb{R}\mid x>0,x^3<5\}$. Set $S$ is not empty because $1\in S$ and is bounded above because $2^3=8>5>x^3,\forall x\in S$, by the completeness axiom, $S$ has a least upper bound, $b$. Consider that if $b^3>5$, we can pick a small $\epsilon=(b^3-5)/(3b^2+3))$ such that $b-\epsilon<b$, then we can have $$(b-\epsilon)^3=b^3-3b^2\epsilon+3b\epsilon^2-\epsilon^3>b^3-3b^2\epsilon-3\epsilon^3>b^3-3b^2\epsilon-3\epsilon=5\tag 1$$ So $(b-\epsilon)^3>5>x^3,\forall x\in S$ which contradicts $b$ is the least upper bound. Now consider that $b^3<5$, then so we can pick a $\epsilon=(5-b^3)/(3b^2+3b+1)$ such that $b+r>b$. Then we have $$(b+r)^3=b^3+3b^2\epsilon+3b\epsilon^2+\epsilon^3<b^3+3b^2\epsilon+3b\epsilon+\epsilon=5 \tag 2$$ This shows $b$ is in $S$ which contradicts $b$ is an upper bound. Thus, by the positivity axiom, $b^3=5$. I am following the direction provided from the book, but I am not really good at inequity, so I am not sure $(1)$ and $(2)$ correct or not. If not, can someone give me a hit or suggestion to make $(1)$ and $(2)$ correct? Thanks in advanced.","Prove that there is a positive number $x$ such that $ x^3= 5$ Define $S=\{x\in\mathbb{R}\mid x>0,x^3<5\}$. Set $S$ is not empty because $1\in S$ and is bounded above because $2^3=8>5>x^3,\forall x\in S$, by the completeness axiom, $S$ has a least upper bound, $b$. Consider that if $b^3>5$, we can pick a small $\epsilon=(b^3-5)/(3b^2+3))$ such that $b-\epsilon<b$, then we can have $$(b-\epsilon)^3=b^3-3b^2\epsilon+3b\epsilon^2-\epsilon^3>b^3-3b^2\epsilon-3\epsilon^3>b^3-3b^2\epsilon-3\epsilon=5\tag 1$$ So $(b-\epsilon)^3>5>x^3,\forall x\in S$ which contradicts $b$ is the least upper bound. Now consider that $b^3<5$, then so we can pick a $\epsilon=(5-b^3)/(3b^2+3b+1)$ such that $b+r>b$. Then we have $$(b+r)^3=b^3+3b^2\epsilon+3b\epsilon^2+\epsilon^3<b^3+3b^2\epsilon+3b\epsilon+\epsilon=5 \tag 2$$ This shows $b$ is in $S$ which contradicts $b$ is an upper bound. Thus, by the positivity axiom, $b^3=5$. I am following the direction provided from the book, but I am not really good at inequity, so I am not sure $(1)$ and $(2)$ correct or not. If not, can someone give me a hit or suggestion to make $(1)$ and $(2)$ correct? Thanks in advanced.",,"['analysis', 'proof-verification']"
9,Looking for modern equivalent of Whittaker and Watson.,Looking for modern equivalent of Whittaker and Watson.,,"I am looking for a modern treatment of transcendental functions with an emphasis on difficult calculations similar to the classic text by Whittaker and Watson (now over 100 years old) http://en.wikipedia.org/wiki/Whittaker_and_Watson I have trouble following their exposition and I think a text written in the last 30-40 years  would be useful.  Unfortunately I have trouble finding one that puts as much emphasis on actually doing difficult calculations.  My interests are in mathematical physics and I would like much deeper treatment of Orthogonal Polynomials, Bessel Functions, Gamma Function, Zeta Function, etc. from a calculational viewpoint than what is provided in standard math for physics texts like Boas, etc.","I am looking for a modern treatment of transcendental functions with an emphasis on difficult calculations similar to the classic text by Whittaker and Watson (now over 100 years old) http://en.wikipedia.org/wiki/Whittaker_and_Watson I have trouble following their exposition and I think a text written in the last 30-40 years  would be useful.  Unfortunately I have trouble finding one that puts as much emphasis on actually doing difficult calculations.  My interests are in mathematical physics and I would like much deeper treatment of Orthogonal Polynomials, Bessel Functions, Gamma Function, Zeta Function, etc. from a calculational viewpoint than what is provided in standard math for physics texts like Boas, etc.",,"['analysis', 'book-recommendation']"
10,about periodic functions,about periodic functions,,"Could anyone give some idea about the following problem? Many thanks！ Suppose that $f,g: \mathbb{R}\to\mathbb{R}$ are two periodic functions such that $\lim_{x\to\infty}[f(x)-g(x)]=0$. Show that $f(x)=g(x)$ for all $x\in\mathbb{R}$.","Could anyone give some idea about the following problem? Many thanks！ Suppose that $f,g: \mathbb{R}\to\mathbb{R}$ are two periodic functions such that $\lim_{x\to\infty}[f(x)-g(x)]=0$. Show that $f(x)=g(x)$ for all $x\in\mathbb{R}$.",,['analysis']
11,Example of a diffeomorphism of class $C^{k}$ which is not $C^{k+1}$,Example of a diffeomorphism of class  which is not,C^{k} C^{k+1},"Can anyone give me an example of a map $f:\mathbb{R}\to\mathbb{R}$, which is a diffeomorphism of class $C^{k}$ but it is not a diffeomorphism of class $C^{k+1}$?","Can anyone give me an example of a map $f:\mathbb{R}\to\mathbb{R}$, which is a diffeomorphism of class $C^{k}$ but it is not a diffeomorphism of class $C^{k+1}$?",,"['analysis', 'reference-request', 'differential-topology']"
12,Functional equation $f\left( \dfrac{2xy}{x+y}\right) +f\left( \dfrac{x+y}{2}\right) =f\left( x\right) +f\left( y\right)$,Functional equation,f\left( \dfrac{2xy}{x+y}\right) +f\left( \dfrac{x+y}{2}\right) =f\left( x\right) +f\left( y\right),"I have not any idea, how to attack the equation $$f\left( \dfrac{2xy}{x+y}\right) +f\left( \dfrac{x+y}{2}\right) =f\left( x\right) +f\left( y\right)$$ with  unknown $f:\mathbb{R} _{+}\rightarrow \mathbb{R}$ . Allowing (for a while) that $y$ can be equal to $0$ , we get $$\begin{aligned}f\left( \dfrac{x}{2}\right) =f\left( x\right) \\ f\left( \dfrac{x}{2^{n}}\right) =f\left( x\right) \\ f\left( 2x\right) =f\left( x\right) \\ f\left( 2^{n}x\right) =f\left( x\right) \end{aligned}$$ This suggests that f can be constant. Otherwise, as $$\begin{aligned}HM\left( x,y\right) \cdot AM\left( x,y\right) =GM^{2}\left( x,y\right)= xy,\end{aligned} $$ one can guess that (up to a constant) $f$ can be a logarithmic function. So, $f(x) =A\ln x+B$ . How to perform a rigorous proof of these ( maybe without guessing ?) How to prove, that there's not other solutions ? Thanks in advance for your help...","I have not any idea, how to attack the equation with  unknown . Allowing (for a while) that can be equal to , we get This suggests that f can be constant. Otherwise, as one can guess that (up to a constant) can be a logarithmic function. So, . How to perform a rigorous proof of these ( maybe without guessing ?) How to prove, that there's not other solutions ? Thanks in advance for your help...","f\left( \dfrac{2xy}{x+y}\right) +f\left( \dfrac{x+y}{2}\right) =f\left( x\right) +f\left( y\right) f:\mathbb{R} _{+}\rightarrow \mathbb{R} y 0 \begin{aligned}f\left( \dfrac{x}{2}\right) =f\left( x\right) \\ f\left( \dfrac{x}{2^{n}}\right) =f\left( x\right) \\ f\left( 2x\right) =f\left( x\right) \\ f\left( 2^{n}x\right) =f\left( x\right) \end{aligned} \begin{aligned}HM\left( x,y\right) \cdot AM\left( x,y\right) =GM^{2}\left( x,y\right)= xy,\end{aligned}  f f(x) =A\ln x+B",['analysis']
13,Density of multiplication table,Density of multiplication table,,"Is there any easy way to show $$\lim_{N \to \infty} \frac{1}{N^2}\#\{ab : 1 \le a,b \le N\} = 0$$ A quick calculation I did shows that the  number of positive integers $\le N^2$ with a prime divisor $p > N$ is at most the order of $(\log 2) \cdot N^2$, so just getting rid of the numbers with a high prime divisor is not sufficient.","Is there any easy way to show $$\lim_{N \to \infty} \frac{1}{N^2}\#\{ab : 1 \le a,b \le N\} = 0$$ A quick calculation I did shows that the  number of positive integers $\le N^2$ with a prime divisor $p > N$ is at most the order of $(\log 2) \cdot N^2$, so just getting rid of the numbers with a high prime divisor is not sufficient.",,"['number-theory', 'analysis', 'elementary-number-theory', 'analytic-number-theory']"
14,Why is n-th Fréchet derivative symmetric?,Why is n-th Fréchet derivative symmetric?,,"Let $V,W$ be nonzero normed spaces over $\mathbb{K}$. Let $E$ be open in $V$ and $f:E\rightarrow W$ be a twice Fréchet-differentiable function. Then, $D^2 f: E\rightarrow \mathscr{L}_2(V^2,W)$ is symmetric. That is, at any point $p\in E$, $((D^2 f)(p) )(x,y)= ((D^2 f) (p))(y,x)$. It is not that I don't understand the proof, but I don't understand why it must hold. What's the geometric meaning of higher order Fréchet derivatives? First order Fréchet derivative $(D f)(p)$ is a function that approximates $f(x)$ where $x$ near $p$, by means of a linear transformation. From the definition $(D^2 f)(p)$ is a linear approximation of $(Df)(x)$ where $x$ is near $p$. However, I'm not sure what does $D^2 f$ say about $f$. What does $D^n f$ represent of $f$ exactly? Thank you in advance.","Let $V,W$ be nonzero normed spaces over $\mathbb{K}$. Let $E$ be open in $V$ and $f:E\rightarrow W$ be a twice Fréchet-differentiable function. Then, $D^2 f: E\rightarrow \mathscr{L}_2(V^2,W)$ is symmetric. That is, at any point $p\in E$, $((D^2 f)(p) )(x,y)= ((D^2 f) (p))(y,x)$. It is not that I don't understand the proof, but I don't understand why it must hold. What's the geometric meaning of higher order Fréchet derivatives? First order Fréchet derivative $(D f)(p)$ is a function that approximates $f(x)$ where $x$ near $p$, by means of a linear transformation. From the definition $(D^2 f)(p)$ is a linear approximation of $(Df)(x)$ where $x$ is near $p$. However, I'm not sure what does $D^2 f$ say about $f$. What does $D^n f$ represent of $f$ exactly? Thank you in advance.",,"['analysis', 'multivariable-calculus', 'differential-operators']"
15,On applying Whitney's extension theorem to suitable closed sets,On applying Whitney's extension theorem to suitable closed sets,,"Whitney's extension theorem states that if $D \subset \mathbb{R}^n$ is closed and $f: D \to \mathbb{R}$ is $C^k$ in some sense to be specified below, then $f$ can be extended to $\mathbb{R}^n$ so that it is real analytic on $D^c$. Now let me specify what it means to be $C^k$ in the sense that we want - we will call this Whitney $C^k$. We say that $f: D \to \mathbb{R}$ is Whitney $C^k$ (on $D$) if there exist functions $f_\alpha: D \to \mathbb{R}$, $|\alpha| \le k$ (note here $\alpha$ is a multindex with $n$ entries) s.t. for all $x,y \in D$ and $\alpha \in \mathbb{Z}^n$, $$f_\alpha (x) = \sum_{|\beta| \le k-|\alpha|} \frac{f_{\alpha + \beta}(y)}{\beta !} (x-y)^\beta +R_\alpha(x,y)$$ with $R_\alpha$ having the following property: Given any point $z \in D$ and any $\epsilon > 0$ there exists a $\delta > 0$ s.t. if $x, y \in A$ and $|z-x|< \delta$ and $|z-y|< \delta$ then $|R_\alpha (x,y)| \le \epsilon |x-y|^{k-|\alpha|}$. Now for my question.  I am wondering for what closed sets can we say that if $f$ is $C^k$ (see the Edit below for definition) then it is Whitney $C^k$.  Phrased differently, I am wondering for what closed sets (with $C^k$ functions defined on these sets) can we apply Whitney's extension theorem. I have been able to show that if $D$ is the closure on an open bounded convex set and $f: D \to \mathbb{R}$ is $C^k$ then $f$ is Whitney $C^k$. To see this one applies the multivariable Taylor's theorem (integral version- see here https://en.wikipedia.org/wiki/Taylor%27s_theorem#Taylor.27s_theorem_for_multivariate_functions ). Notice that the proof, and therefore the estimates, rely on integrating along a line segment - this is where convexity is used.  I suspect this can be generalized to $D$ being the closure of a connected open set with suitably smooth boundary (i.e. $C^\infty$). References are also appreciated. Edit:  For completeness, let me add the definition of $C^k$ on the closure of an open set. Let $U$ be an open set, then $f: \overline{U} \to \mathbb{C}$ is $C^k$ if all the partial derivative of $f$ up to order $k$ extend continuously to $\overline{U}$.","Whitney's extension theorem states that if $D \subset \mathbb{R}^n$ is closed and $f: D \to \mathbb{R}$ is $C^k$ in some sense to be specified below, then $f$ can be extended to $\mathbb{R}^n$ so that it is real analytic on $D^c$. Now let me specify what it means to be $C^k$ in the sense that we want - we will call this Whitney $C^k$. We say that $f: D \to \mathbb{R}$ is Whitney $C^k$ (on $D$) if there exist functions $f_\alpha: D \to \mathbb{R}$, $|\alpha| \le k$ (note here $\alpha$ is a multindex with $n$ entries) s.t. for all $x,y \in D$ and $\alpha \in \mathbb{Z}^n$, $$f_\alpha (x) = \sum_{|\beta| \le k-|\alpha|} \frac{f_{\alpha + \beta}(y)}{\beta !} (x-y)^\beta +R_\alpha(x,y)$$ with $R_\alpha$ having the following property: Given any point $z \in D$ and any $\epsilon > 0$ there exists a $\delta > 0$ s.t. if $x, y \in A$ and $|z-x|< \delta$ and $|z-y|< \delta$ then $|R_\alpha (x,y)| \le \epsilon |x-y|^{k-|\alpha|}$. Now for my question.  I am wondering for what closed sets can we say that if $f$ is $C^k$ (see the Edit below for definition) then it is Whitney $C^k$.  Phrased differently, I am wondering for what closed sets (with $C^k$ functions defined on these sets) can we apply Whitney's extension theorem. I have been able to show that if $D$ is the closure on an open bounded convex set and $f: D \to \mathbb{R}$ is $C^k$ then $f$ is Whitney $C^k$. To see this one applies the multivariable Taylor's theorem (integral version- see here https://en.wikipedia.org/wiki/Taylor%27s_theorem#Taylor.27s_theorem_for_multivariate_functions ). Notice that the proof, and therefore the estimates, rely on integrating along a line segment - this is where convexity is used.  I suspect this can be generalized to $D$ being the closure of a connected open set with suitably smooth boundary (i.e. $C^\infty$). References are also appreciated. Edit:  For completeness, let me add the definition of $C^k$ on the closure of an open set. Let $U$ be an open set, then $f: \overline{U} \to \mathbb{C}$ is $C^k$ if all the partial derivative of $f$ up to order $k$ extend continuously to $\overline{U}$.",,"['analysis', 'differential-topology', 'taylor-expansion', 'analyticity']"
16,Is there a measure theoretic version of Stokes's theorem?,Is there a measure theoretic version of Stokes's theorem?,,Is there a way to generalize Stokes's theorem on manifolds to general measure spaces? This idea came from trying to generalize the fundamental theorem of calculus to general function/infinite dimensional spaces. Just wondering if anyone can provide a reference or if this notion would even make sense. Thanks,Is there a way to generalize Stokes's theorem on manifolds to general measure spaces? This idea came from trying to generalize the fundamental theorem of calculus to general function/infinite dimensional spaces. Just wondering if anyone can provide a reference or if this notion would even make sense. Thanks,,"['analysis', 'measure-theory', 'reference-request', 'soft-question']"
17,Application of Implicit Function Theorem in Munkres Analysis on Manifolds,Application of Implicit Function Theorem in Munkres Analysis on Manifolds,,"I'm studying the Implicit Function Theorem and this is a problem from Munkres' Analysis on Manifolds. Let $F:\mathbb{R^2} \to \mathbb{R}$ be of class $C^2$, with $F(0,0)=0$ and $DF(0,0)=\begin{bmatrix} 2 & 3\end{bmatrix}$.  Let $G:\mathbb{R^3} \to \mathbb{R}$ be defined by the equation $$G(x,y,z)=F(x+2y+3z-1, x^3+y^2-z^2).$$ (a) Note that $G(-2,3,-1)=F(0,0)=0$. Show that one can solve the equation $G(x,y,z)=0$ for $z$, say $z=g(x,y)$, for $(x,y)$ in a neighborhood $B$ of $(-2,3)$, such that $g(-2,3)=-1$. (b) Find $Dg(-2,3)$. (c) If $D_1D_1F=3$ and $D_1D_2F=-1$ and $D_2D_2F=5$ at $(0,0)$, find $D_2D_1g(-2,3)$. I've worked through (a) and (b). This is my answer. Define $h(x,y,z)=(x+2y+3z-1,x^3+y^2-z^2)$, then $G=F\circ h$, so by Chain Rule, $DG=DF(h(x,y,z))Dh(x,y,z)$. Since $Dh=\begin{bmatrix} 1 & 2 & 3 \\ 3x^2 & 2y & -2z \\ \end{bmatrix},$ then $DG(-2,3,-1)=\begin{bmatrix} 38 & 22 & 12 \\ \end{bmatrix}$. So $\frac{\partial G}{\partial z}(-2,3,-1)=12\neq 0$. Hence by the Implicit Function Theorem, we can find such $g$ and $B$ in (a). Also, we've got $Dg(-2,3)=\frac{-1}{12}\begin{bmatrix} 38 & 22 \\ \end{bmatrix}$ from the result obtained above. However, I'm lost on how to approach (c). I'm not sure how I'm supposed to find the value. I would appreciate some help on this problem.","I'm studying the Implicit Function Theorem and this is a problem from Munkres' Analysis on Manifolds. Let $F:\mathbb{R^2} \to \mathbb{R}$ be of class $C^2$, with $F(0,0)=0$ and $DF(0,0)=\begin{bmatrix} 2 & 3\end{bmatrix}$.  Let $G:\mathbb{R^3} \to \mathbb{R}$ be defined by the equation $$G(x,y,z)=F(x+2y+3z-1, x^3+y^2-z^2).$$ (a) Note that $G(-2,3,-1)=F(0,0)=0$. Show that one can solve the equation $G(x,y,z)=0$ for $z$, say $z=g(x,y)$, for $(x,y)$ in a neighborhood $B$ of $(-2,3)$, such that $g(-2,3)=-1$. (b) Find $Dg(-2,3)$. (c) If $D_1D_1F=3$ and $D_1D_2F=-1$ and $D_2D_2F=5$ at $(0,0)$, find $D_2D_1g(-2,3)$. I've worked through (a) and (b). This is my answer. Define $h(x,y,z)=(x+2y+3z-1,x^3+y^2-z^2)$, then $G=F\circ h$, so by Chain Rule, $DG=DF(h(x,y,z))Dh(x,y,z)$. Since $Dh=\begin{bmatrix} 1 & 2 & 3 \\ 3x^2 & 2y & -2z \\ \end{bmatrix},$ then $DG(-2,3,-1)=\begin{bmatrix} 38 & 22 & 12 \\ \end{bmatrix}$. So $\frac{\partial G}{\partial z}(-2,3,-1)=12\neq 0$. Hence by the Implicit Function Theorem, we can find such $g$ and $B$ in (a). Also, we've got $Dg(-2,3)=\frac{-1}{12}\begin{bmatrix} 38 & 22 \\ \end{bmatrix}$ from the result obtained above. However, I'm lost on how to approach (c). I'm not sure how I'm supposed to find the value. I would appreciate some help on this problem.",,"['analysis', 'multivariable-calculus', 'vector-analysis']"
18,Product of the first n cyclotomic polynomials.,Product of the first n cyclotomic polynomials.,,"Let $$F_n(\alpha) = \prod_{k = 1}^n \Phi_k(e(\alpha))$$ where $e(\alpha) =  e^{2\pi i\alpha}$ It is clear that $F_n(\alpha) = 0$ iff $\alpha = \frac{a}{q}$ for relatively prime $a, q$ s.t. $q \le n$. Now, let $\frac{p_1}{q_1}, \frac{p_2}{q_2}$ be two such rationals s.t there are no such rationals between the two. How would one estimate values of $F_n(\alpha)$ if $\frac{p_1}{q_1} < \alpha < \frac{p_2}{q_2}$.","Let $$F_n(\alpha) = \prod_{k = 1}^n \Phi_k(e(\alpha))$$ where $e(\alpha) =  e^{2\pi i\alpha}$ It is clear that $F_n(\alpha) = 0$ iff $\alpha = \frac{a}{q}$ for relatively prime $a, q$ s.t. $q \le n$. Now, let $\frac{p_1}{q_1}, \frac{p_2}{q_2}$ be two such rationals s.t there are no such rationals between the two. How would one estimate values of $F_n(\alpha)$ if $\frac{p_1}{q_1} < \alpha < \frac{p_2}{q_2}$.",,"['analysis', 'cyclotomic-polynomials', 'farey-sequences']"
19,Proof that a strictly decreasing sequence of nested intervals boils down to a single point.,Proof that a strictly decreasing sequence of nested intervals boils down to a single point.,,"The nested intervals theorem says the following. If a sequence of intervals $\langle I_n\rangle$ is decreasing, then $\bigcap_{n=1}^{\infty} I_n$ is not empty . However, I'm trying to modify the theorem, say, if the sequence is strictly decreasing , then $\bigcap_{n=1}^{\infty} I_n$  should be a single point . What I tried : Let $I_n = [ a_n, b_n]$. Then $\langle a_n\rangle$ is increasing, while $\langle b_n\rangle$ is decreasing. Since $\langle a_n\rangle$ is bounded, it converges to a point, say $\alpha$. And, since $b_k$ is an upper bound $\forall k$, thus $\alpha \le b_k$. Even if $\langle b_n\rangle$ is decreasing and bounded, I don't know how I can say $\lim_{n \to \infty} b_n = \alpha$. Is there anyone to help me?","The nested intervals theorem says the following. If a sequence of intervals $\langle I_n\rangle$ is decreasing, then $\bigcap_{n=1}^{\infty} I_n$ is not empty . However, I'm trying to modify the theorem, say, if the sequence is strictly decreasing , then $\bigcap_{n=1}^{\infty} I_n$  should be a single point . What I tried : Let $I_n = [ a_n, b_n]$. Then $\langle a_n\rangle$ is increasing, while $\langle b_n\rangle$ is decreasing. Since $\langle a_n\rangle$ is bounded, it converges to a point, say $\alpha$. And, since $b_k$ is an upper bound $\forall k$, thus $\alpha \le b_k$. Even if $\langle b_n\rangle$ is decreasing and bounded, I don't know how I can say $\lim_{n \to \infty} b_n = \alpha$. Is there anyone to help me?",,['analysis']
20,Analog of $(a+b)^2 \leq 2(a^2 + b^2)$,Analog of,(a+b)^2 \leq 2(a^2 + b^2),Is there an inequality such as $$(a+b)^2 \leq 2(a^2 + b^2)$$ for higher powers of $k$ $$(a+b)^k \leq C(a^k + b^k)?$$,Is there an inequality such as $$(a+b)^2 \leq 2(a^2 + b^2)$$ for higher powers of $k$ $$(a+b)^k \leq C(a^k + b^k)?$$,,"['analysis', 'inequality']"
21,"Do injective, yet not bijective, functions have an inverse?","Do injective, yet not bijective, functions have an inverse?",,"The formal definition I was given was that in order for a function $f(x)$ to have an inverse, $f(x)$ is required to be bijective. Nevertheless, further on, I was introduced to the inverse of trigonometric functions, such as the inverse of $\sin(x)$ . But $\sin(x)$ is not bijective, but only injective (when restricting its domain). As you can see the topics I'm studying are probably very basic, so excuse me if my question is silly, but ultimately does a function need to be bijective in order to have an inverse? If this is the case, how can we talk about the inverse of trigonometric functions such as $\sin$ and $\cos$ ?","The formal definition I was given was that in order for a function to have an inverse, is required to be bijective. Nevertheless, further on, I was introduced to the inverse of trigonometric functions, such as the inverse of . But is not bijective, but only injective (when restricting its domain). As you can see the topics I'm studying are probably very basic, so excuse me if my question is silly, but ultimately does a function need to be bijective in order to have an inverse? If this is the case, how can we talk about the inverse of trigonometric functions such as and ?",f(x) f(x) \sin(x) \sin(x) \sin \cos,"['analysis', 'functions', 'inverse']"
22,The set of numbers whose decimal expansions contain only 4 and 7,The set of numbers whose decimal expansions contain only 4 and 7,,"Let $S$ be the set of numbers in $X=[0,1]$ that when expanded as a decimal form, the numbers are 4 or 7 only. The following are the problems. a), Is S countable ? b), Is it dense in $X$ ? c), Is it compact ? d), Is it perfect ? For a), I want to say that it is intuitively, but I have no idea how to prove this. I tried to come up with a bijection between $S$ and $\Bbb Z$ but I couldn't find one. For b), my understanding of a set being ""dense"" means that all points in $X$ is either a limit point or a point in $S$. Am I right? Even if I were, I am not sure how to show this. For c), my intuition tells me that it is because it is bounded. So if I could show that it is closed I will be done, I think.  But I am still iffy with the idea of limit points, and I am not sure what kind of limit points there are in $S$. For d), Because I can't show that it's closed I am completely stuck. I am teaching myself analysis, and I only know up to abstract algebra. Since I never took topology, please give me an explanation that helps without knowledge of advanced math.","Let $S$ be the set of numbers in $X=[0,1]$ that when expanded as a decimal form, the numbers are 4 or 7 only. The following are the problems. a), Is S countable ? b), Is it dense in $X$ ? c), Is it compact ? d), Is it perfect ? For a), I want to say that it is intuitively, but I have no idea how to prove this. I tried to come up with a bijection between $S$ and $\Bbb Z$ but I couldn't find one. For b), my understanding of a set being ""dense"" means that all points in $X$ is either a limit point or a point in $S$. Am I right? Even if I were, I am not sure how to show this. For c), my intuition tells me that it is because it is bounded. So if I could show that it is closed I will be done, I think.  But I am still iffy with the idea of limit points, and I am not sure what kind of limit points there are in $S$. For d), Because I can't show that it's closed I am completely stuck. I am teaching myself analysis, and I only know up to abstract algebra. Since I never took topology, please give me an explanation that helps without knowledge of advanced math.",,['analysis']
23,Follow-up to Baby Rudin,Follow-up to Baby Rudin,,"I would like to continue my study of analysis, albeit temporarily in self-study, and I was wondering what would be the best ""sequel"" to baby Rudin.  Thank you very much for your advice.","I would like to continue my study of analysis, albeit temporarily in self-study, and I was wondering what would be the best ""sequel"" to baby Rudin.  Thank you very much for your advice.",,"['analysis', 'reference-request', 'soft-question', 'self-learning']"
24,Show that $\frac{1}{n}\sum_{j=1}^\infty \left( 1 - (1-p_j)^n\right) \to 0$ as $n \to \infty$,Show that  as,\frac{1}{n}\sum_{j=1}^\infty \left( 1 - (1-p_j)^n\right) \to 0 n \to \infty,"Given $p_j \geq 0$ for all $j \geq 1$ and $\sum_{j=1}^\infty p_j = 1$ , I am asked to show that $$\frac{1}{n}\sum_{j=1}^\infty \left( 1 - (1-p_j)^n\right) \to 0 ~~ \textrm{as} ~~ n \to \infty.$$ Unfortunately, using only the fact that $(1-p_j)^n \geq 1 - np_j$ will not be enough, as this inequality only gives us $$\frac{1}{n}\sum_{j=1}^\infty \left( 1 - (1-p_j)^n\right) \leq \frac{1}{n}\sum_{j=1}^\infty np_j = 1.$$ Can anyone provide a hint towards the proof? Remark: This problem is related to exercise 3.8 in this monograph .","Given for all and , I am asked to show that Unfortunately, using only the fact that will not be enough, as this inequality only gives us Can anyone provide a hint towards the proof? Remark: This problem is related to exercise 3.8 in this monograph .",p_j \geq 0 j \geq 1 \sum_{j=1}^\infty p_j = 1 \frac{1}{n}\sum_{j=1}^\infty \left( 1 - (1-p_j)^n\right) \to 0 ~~ \textrm{as} ~~ n \to \infty. (1-p_j)^n \geq 1 - np_j \frac{1}{n}\sum_{j=1}^\infty \left( 1 - (1-p_j)^n\right) \leq \frac{1}{n}\sum_{j=1}^\infty np_j = 1.,"['analysis', 'asymptotics']"
25,I would like to know an intuitive way to understand a Cauchy sequence and the Cauchy criterion.,I would like to know an intuitive way to understand a Cauchy sequence and the Cauchy criterion.,,"My understanding from the definition in my book (Rudin) is this. A seq. $\{p_n\}$ in a metric space $X$ (I only really know $\mathbb R^k$) is said to be a Cauchy sequence if for any given $\epsilon > 0$, $\exists N\in \mathbb N$ such that $\forall n,m\ge N$, $d(p_n,p_m)<\epsilon$. (1) I see it as, given any tiny value $\epsilon$, we can find a natural number $N$ large enough so that the distance between $p_n$ and $p_m$ is less than $\epsilon$. Am I right ? The reason I'm asking this is because I was trying to understand the proof of how $$\sum a_nb_n$$ can converge, and the book said this $$\left\lvert \sum_{n=p}^{q}a_nb_n\right\rvert \leq \epsilon$$ satisfies the Cauchy criterion and therefore it converges. I read other questions and answers about the Cauchy sequence, but it didn't really help me… Can someone explain me what's going on? Edit: Suppose a) the partial sums of $A_n = \Sigma a_n$ form a bounded sequence b) $b_0 \geq b_1 \geq \dotsb$ c) $\lim_{b \to \infty} b_n = 0$ Using the partial summation formula, algebraically the equation in the bottom is proved $$\left\lvert \sum_{n=p}^{q}a_nb_n \right\rvert \leq \epsilon$$ Algebraically I had no problem, but I don't know why this proves convergence. I thought to show that a sequence is Cauchy, we need to find the distance between two terms in a sequence. That's where I'm confused.","My understanding from the definition in my book (Rudin) is this. A seq. $\{p_n\}$ in a metric space $X$ (I only really know $\mathbb R^k$) is said to be a Cauchy sequence if for any given $\epsilon > 0$, $\exists N\in \mathbb N$ such that $\forall n,m\ge N$, $d(p_n,p_m)<\epsilon$. (1) I see it as, given any tiny value $\epsilon$, we can find a natural number $N$ large enough so that the distance between $p_n$ and $p_m$ is less than $\epsilon$. Am I right ? The reason I'm asking this is because I was trying to understand the proof of how $$\sum a_nb_n$$ can converge, and the book said this $$\left\lvert \sum_{n=p}^{q}a_nb_n\right\rvert \leq \epsilon$$ satisfies the Cauchy criterion and therefore it converges. I read other questions and answers about the Cauchy sequence, but it didn't really help me… Can someone explain me what's going on? Edit: Suppose a) the partial sums of $A_n = \Sigma a_n$ form a bounded sequence b) $b_0 \geq b_1 \geq \dotsb$ c) $\lim_{b \to \infty} b_n = 0$ Using the partial summation formula, algebraically the equation in the bottom is proved $$\left\lvert \sum_{n=p}^{q}a_nb_n \right\rvert \leq \epsilon$$ Algebraically I had no problem, but I don't know why this proves convergence. I thought to show that a sequence is Cauchy, we need to find the distance between two terms in a sequence. That's where I'm confused.",,"['analysis', 'nonstandard-analysis', 'cauchy-sequences']"
26,Measurable function remaining constant,Measurable function remaining constant,,"This is a problem which appeared in one of my tests, which i wasn't able to solve. Let $\Omega$ be a uncountable set. Let $S$ be the collection of subsets of $\Omega$ given by: $A \in S$ if and only if $A$ is countable or $A^{c}$ is countable. Suppose $f: \Omega \to \mathbb{R}$ is a real measurable function. Prove that there exists a $y \in \mathbb{R}$ and a countable set $B$ such that the $f(x)=y$ is on $B^{c}$.","This is a problem which appeared in one of my tests, which i wasn't able to solve. Let $\Omega$ be a uncountable set. Let $S$ be the collection of subsets of $\Omega$ given by: $A \in S$ if and only if $A$ is countable or $A^{c}$ is countable. Suppose $f: \Omega \to \mathbb{R}$ is a real measurable function. Prove that there exists a $y \in \mathbb{R}$ and a countable set $B$ such that the $f(x)=y$ is on $B^{c}$.",,['analysis']
27,"The measure of $([0,1]\cap \mathbb{Q})×([0,1]\cap\mathbb{Q})$",The measure of,"([0,1]\cap \mathbb{Q})×([0,1]\cap\mathbb{Q})","We know that $[0,1]\cap \mathbb{Q}$ is a dense subset of $[0,1]$ and has measure zero, but what about $([0,1]\cap \mathbb{Q})\times([0,1]\cap \mathbb{Q})$? Is it also a dense subset of $[0,1]\times[0,1]$ and has measure zero too? Besides, what about its complement? Is it dense in $[0,1]\times[0,1]$ and has measure zero?","We know that $[0,1]\cap \mathbb{Q}$ is a dense subset of $[0,1]$ and has measure zero, but what about $([0,1]\cap \mathbb{Q})\times([0,1]\cap \mathbb{Q})$? Is it also a dense subset of $[0,1]\times[0,1]$ and has measure zero too? Besides, what about its complement? Is it dense in $[0,1]\times[0,1]$ and has measure zero?",,"['analysis', 'measure-theory']"
28,Establishing the inequality at the heart of a popular sequence which converges to powers of $e$,Establishing the inequality at the heart of a popular sequence which converges to powers of,e,"I know that the sequence $$\displaystyle (1+kx)^\frac{1}{k},$$ where the sequence $\{k_i\}$ converges to zero, converges to $e^x$. I also know the sequence is increasing. How does one show this is increasing?  I am interested in neat ways of establishing the inequality, $(1+ax)^\frac{1}{a} \ge (1+bx)^\frac{1}{b}$ if $b \ge a$ rather than the sequence itself.","I know that the sequence $$\displaystyle (1+kx)^\frac{1}{k},$$ where the sequence $\{k_i\}$ converges to zero, converges to $e^x$. I also know the sequence is increasing. How does one show this is increasing?  I am interested in neat ways of establishing the inequality, $(1+ax)^\frac{1}{a} \ge (1+bx)^\frac{1}{b}$ if $b \ge a$ rather than the sequence itself.",,"['analysis', 'inequality']"
29,Which functions commute with exponentials?,Which functions commute with exponentials?,,"If $f : \mathbb{R} \to \mathbb{R}$ satisfies $$f(e^x) = e^{f(x)},$$ must it also be an exponential function?","If $f : \mathbb{R} \to \mathbb{R}$ satisfies $$f(e^x) = e^{f(x)},$$ must it also be an exponential function?",,"['analysis', 'partial-differential-equations']"
30,Existence of sequences converging to $\sup S$ and $\inf S$,Existence of sequences converging to  and,\sup S \inf S,"If $S$ is a bounded set of real numbers, how can we prove that there are distinct sequences in $S$ that converge to $\sup S$ and $\inf S$? I'm not even sure how to begin on this problem. I know the set $S$ is bounded so it has a supremum and an infimum.","If $S$ is a bounded set of real numbers, how can we prove that there are distinct sequences in $S$ that converge to $\sup S$ and $\inf S$? I'm not even sure how to begin on this problem. I know the set $S$ is bounded so it has a supremum and an infimum.",,['analysis']
31,Definite integral involving logarithm of cosine,Definite integral involving logarithm of cosine,,Does anyone know the provenance of or the answer to the following integral $$\int_0^\infty\ \frac{\ln|\cos(x)|}{x^2} dx $$ Thanks.,Does anyone know the provenance of or the answer to the following integral $$\int_0^\infty\ \frac{\ln|\cos(x)|}{x^2} dx $$ Thanks.,,"['analysis', 'reference-request', 'definite-integrals', 'logarithms']"
32,"If |f| is Riemann integrable, then f is Riemann integrable???","If |f| is Riemann integrable, then f is Riemann integrable???",,"So i am stuck here.. how do i prove the first & second inequalities? Also if |f| is Riemann integrable, then f is Riemann integrable. I think it's ture but i dont know how to prove it. any hints would be appreciated! Thank You","So i am stuck here.. how do i prove the first & second inequalities? Also if |f| is Riemann integrable, then f is Riemann integrable. I think it's ture but i dont know how to prove it. any hints would be appreciated! Thank You",,['analysis']
33,how to prove $f(x) = x^s$ Lipschitz continuous,how to prove  Lipschitz continuous,f(x) = x^s,"I want to show that for $0<s \leq 1$ $$f: \mathbb{R}_{\geq0} \to \mathbb{R}_{\geq0}$$ $$x \mapsto x^s$$ is Lipschitz continuous on$[a,\infty)$ for any $a \in\mathbb{R}_{>0}$, and that it is NOT on $[0,\infty)$. So what I want to show is that $\exists \hspace{2 mm} C \in \mathbb{R}_{\geq0}$ sucht that for all $ x,y \in [a,\infty)  $ $$|x^s -y^s| \leq C|x-y|$$ and therefore, assuming, wlog $\hspace{1mm} x>y$ $$(x^s -y^s) \leq C(x-y),$$ And that $\nexists $ such $C$ on $[0,\infty).$ Once again my (lack of ) mastery with inequalities is not yet sufficient for this type of problem. Can someone lead the way (without recourse to concave/convex functions)?","I want to show that for $0<s \leq 1$ $$f: \mathbb{R}_{\geq0} \to \mathbb{R}_{\geq0}$$ $$x \mapsto x^s$$ is Lipschitz continuous on$[a,\infty)$ for any $a \in\mathbb{R}_{>0}$, and that it is NOT on $[0,\infty)$. So what I want to show is that $\exists \hspace{2 mm} C \in \mathbb{R}_{\geq0}$ sucht that for all $ x,y \in [a,\infty)  $ $$|x^s -y^s| \leq C|x-y|$$ and therefore, assuming, wlog $\hspace{1mm} x>y$ $$(x^s -y^s) \leq C(x-y),$$ And that $\nexists $ such $C$ on $[0,\infty).$ Once again my (lack of ) mastery with inequalities is not yet sufficient for this type of problem. Can someone lead the way (without recourse to concave/convex functions)?",,"['analysis', 'continuity', 'lipschitz-functions']"
34,"Prove partial derivatives exist, but not all directional derivatives exists.","Prove partial derivatives exist, but not all directional derivatives exists.",,"During my analysis course my teacher explained the difference between partial derivatives and directional derivatives using the notion that a partial derivatives looks at the function as approaching a point along the axes (in case of of the plane), and a directional derivative as approaching a point from any direction in the plane. He also explained that the existence of directional derivatives is a stronger notion than the existence of partial derivatives exists: if all directional derivatives exist, then the partial derivatives exist too. I am to show (not necessarily prove) a case where the partial derivatives exist, but not all directional derivatives exist (hence, f is not differentiable).","During my analysis course my teacher explained the difference between partial derivatives and directional derivatives using the notion that a partial derivatives looks at the function as approaching a point along the axes (in case of of the plane), and a directional derivative as approaching a point from any direction in the plane. He also explained that the existence of directional derivatives is a stronger notion than the existence of partial derivatives exists: if all directional derivatives exist, then the partial derivatives exist too. I am to show (not necessarily prove) a case where the partial derivatives exist, but not all directional derivatives exist (hence, f is not differentiable).",,"['analysis', 'derivatives', 'partial-derivative']"
35,Is the supremum of a function squared the square of its supremum,Is the supremum of a function squared the square of its supremum,,Let $f$  be a holomorphic function on the open unit disc. Is $(\sup \vert f \vert)^2 = \sup (\vert f\vert^2)$?,Let $f$  be a holomorphic function on the open unit disc. Is $(\sup \vert f \vert)^2 = \sup (\vert f\vert^2)$?,,['analysis']
36,Functions on P(R) - are there examples?,Functions on P(R) - are there examples?,,What are some examples of functions on the Power Set of the Reals? Is this an abuse of terminology - functions on the reals can be thought of as functions on the power set of the naturals with a specific ordering. I was hoping someone would kindly refer me to a text or article where explicit (not necessarily 'useful') examples of functions with the domain P(R) are given; or if this is confused idea why there is nothing to it. Thanks!,What are some examples of functions on the Power Set of the Reals? Is this an abuse of terminology - functions on the reals can be thought of as functions on the power set of the naturals with a specific ordering. I was hoping someone would kindly refer me to a text or article where explicit (not necessarily 'useful') examples of functions with the domain P(R) are given; or if this is confused idea why there is nothing to it. Thanks!,,"['analysis', 'elementary-set-theory']"
37,Continuous functions with domain in the Natural Numbers,Continuous functions with domain in the Natural Numbers,,"Can functions with domain in the Natural Numbers be continuous? In the high school, it is teached an intuitive notion of continuous functions: functions which will always appear as an ""unbroken curve"", no matter how close one zooms in the graph. This would be the opposite of discrete functions, which would appear as ""dots"" if one zooms close enough. In Analysis, this intuitive notion is abandoned to make way for formalism. Rudin, in ""Principles of Mathematical Analysis"" defines continuous functions as it follows: Which, for me, can be written in formal language as: $\forall\varepsilon>0\quad\exists\delta>0 \quad\forall x \in E\quad  ((d_{x}(x,p)<\delta)\implies(d_{y}(f(x),f(y))<\epsilon))$ Functions with domain in the natural numbers will appear as dots in a graph, instead of an unbroken curve. I am trying to formally prove that no function with domain on the Naturals is continuous, can somebody help me out? PS: What is the domain of $\varepsilon$  and $\delta $?","Can functions with domain in the Natural Numbers be continuous? In the high school, it is teached an intuitive notion of continuous functions: functions which will always appear as an ""unbroken curve"", no matter how close one zooms in the graph. This would be the opposite of discrete functions, which would appear as ""dots"" if one zooms close enough. In Analysis, this intuitive notion is abandoned to make way for formalism. Rudin, in ""Principles of Mathematical Analysis"" defines continuous functions as it follows: Which, for me, can be written in formal language as: $\forall\varepsilon>0\quad\exists\delta>0 \quad\forall x \in E\quad  ((d_{x}(x,p)<\delta)\implies(d_{y}(f(x),f(y))<\epsilon))$ Functions with domain in the natural numbers will appear as dots in a graph, instead of an unbroken curve. I am trying to formally prove that no function with domain on the Naturals is continuous, can somebody help me out? PS: What is the domain of $\varepsilon$  and $\delta $?",,"['analysis', 'elementary-number-theory']"
38,Chess rating calculating algorithm,Chess rating calculating algorithm,,"In competitive chess tournaments,  there is a complex rating system that evaluates your rating based on how you do well you do playing games. I am referring to the FIDE system not USCF. Are there FIDE chess players who know for the function of evaluating rating and understand it to the excess of being able to explain it thoroughly to me? Thank you very much. If your curious, I'm FIDE 1760.","In competitive chess tournaments,  there is a complex rating system that evaluates your rating based on how you do well you do playing games. I am referring to the FIDE system not USCF. Are there FIDE chess players who know for the function of evaluating rating and understand it to the excess of being able to explain it thoroughly to me? Thank you very much. If your curious, I'm FIDE 1760.",,['analysis']
39,Sum of Measures of Two Subsets = Sum of Measures of Their Union & Intersection,Sum of Measures of Two Subsets = Sum of Measures of Their Union & Intersection,,"I have this solution to a problem in measure theory, I am posting it here to check my basic understanding: Let $(X, \mathscr A, \mu)$ be measure space with $A, B \in \mathscr A$, show that   $$\mu(A) + \mu(B) = \mu(A \cup B) + \mu ( A \cap B).$$ I would like to solve this problem by presenting three cases: (1) If $A$ and $B$ are disjoint, $A \cap B = \emptyset$: $$\begin{align} A + B &= (A \cup B) \tag{a}\\ \therefore \ \mu(A) + \mu(B) &= \mu (A \cup B) + 0 \tag{b}\\ &= \mu (A \cup B) + \mu (\emptyset) \tag{c}\\ &= \mu (A \cup B) + \mu(A \cap B) \tag{d}\\ \end{align}$$ (2) If $A$ is proper subset of $B$, $A \subset B$: $$\begin{align} B &= A \cup B \tag{a}\\ \therefore \mu(B) &= \mu (A \cup B)\tag{b}\\ A &= A \cap B \tag{c}\\ \therefore \mu(A) &= \mu (A \cap B)\tag{d}\\ \therefore \   \mu(A) + \mu(B)&= \mu (A \cup B) + \mu(A \cap B)\tag{e}\\ \end{align}$$ (3) If A intersects B, $A \cap B \neq \emptyset$, $$\begin{align} A \cup B &= (A + B) - (A \cap B)\tag{a}\\ \therefore \ \mu(A \cup B) &= \mu(A) + \mu(B) - \mu (A \cap B)\tag{b}\\ \mu(A \cup B) + \mu (A \cap B)&= \mu(A) + \mu(B) \tag{c}\\ \mu(A) + \mu(B) &= \mu(A \cup B) + \mu (A \cap B) \tag{d}\\ \end{align}$$ Here is my question: Am I making invalid conclusions by jumping from (1a) to (1b), from (2a) to (2b), from (2c) to (2d), etc.? Do let me know if you have a more elegant solution. Thank you for your time and effort.","I have this solution to a problem in measure theory, I am posting it here to check my basic understanding: Let $(X, \mathscr A, \mu)$ be measure space with $A, B \in \mathscr A$, show that   $$\mu(A) + \mu(B) = \mu(A \cup B) + \mu ( A \cap B).$$ I would like to solve this problem by presenting three cases: (1) If $A$ and $B$ are disjoint, $A \cap B = \emptyset$: $$\begin{align} A + B &= (A \cup B) \tag{a}\\ \therefore \ \mu(A) + \mu(B) &= \mu (A \cup B) + 0 \tag{b}\\ &= \mu (A \cup B) + \mu (\emptyset) \tag{c}\\ &= \mu (A \cup B) + \mu(A \cap B) \tag{d}\\ \end{align}$$ (2) If $A$ is proper subset of $B$, $A \subset B$: $$\begin{align} B &= A \cup B \tag{a}\\ \therefore \mu(B) &= \mu (A \cup B)\tag{b}\\ A &= A \cap B \tag{c}\\ \therefore \mu(A) &= \mu (A \cap B)\tag{d}\\ \therefore \   \mu(A) + \mu(B)&= \mu (A \cup B) + \mu(A \cap B)\tag{e}\\ \end{align}$$ (3) If A intersects B, $A \cap B \neq \emptyset$, $$\begin{align} A \cup B &= (A + B) - (A \cap B)\tag{a}\\ \therefore \ \mu(A \cup B) &= \mu(A) + \mu(B) - \mu (A \cap B)\tag{b}\\ \mu(A \cup B) + \mu (A \cap B)&= \mu(A) + \mu(B) \tag{c}\\ \mu(A) + \mu(B) &= \mu(A \cup B) + \mu (A \cap B) \tag{d}\\ \end{align}$$ Here is my question: Am I making invalid conclusions by jumping from (1a) to (1b), from (2a) to (2b), from (2c) to (2d), etc.? Do let me know if you have a more elegant solution. Thank you for your time and effort.",,"['analysis', 'measure-theory']"
40,How prove this $\frac{af(a)+bf(b)}{a+b}\ge f(a+b)$,How prove this,\frac{af(a)+bf(b)}{a+b}\ge f(a+b),"Assume that  $f(x)$ has two derivatives  on $(0,2)$ and $0<a<b<a+b<2$. I have to prove that, if $f(a)\ge f(a+b)$ and $f''(x)\le 0$, then: $$\dfrac{af(a)+bf(b)}{a+b}\ge f(a+b).$$ I think we also have $$f(b)\ge f(a+b)$$ so $$af(a)+bf(b)\ge af(a+b)+bf(a+b)=(a+b)f(a+b)$$ But if this true, then we can prove it.","Assume that  $f(x)$ has two derivatives  on $(0,2)$ and $0<a<b<a+b<2$. I have to prove that, if $f(a)\ge f(a+b)$ and $f''(x)\le 0$, then: $$\dfrac{af(a)+bf(b)}{a+b}\ge f(a+b).$$ I think we also have $$f(b)\ge f(a+b)$$ so $$af(a)+bf(b)\ge af(a+b)+bf(a+b)=(a+b)f(a+b)$$ But if this true, then we can prove it.",,"['analysis', 'convex-analysis']"
41,Greatest lower bound property and least upper bound property,Greatest lower bound property and least upper bound property,,"A Completeness principle in mathematical analysis is a principle by the help of which we can establish (prove) the completeness of an ordered field( About whom I am going to post one question later). (If I am wrong in defining then It will make me immensely happy if someone corrects it). Cauchy's completeness principle, Dedekind's principle, Cantor's principle, Weierstrass's principle, greatest lower bound principle  and least upper bound principle  are the completeness principles we come across in analysis. We can take anyone of this as an axiom and derive the remaining. Now let us consider proving the Least upper bound principle  taking the greatest lower bound principle as an axiom. My proof goes in the following way. Let us consider an ordered field $F$. Now taking the glbp as an axiom I can say that any non empty subset, $P$, of this ordered field definitely posses an element $k$  ($k$ belongs to $P$) such that all the elements of $P$ are greater than or equal to $k$. Any element  of the field less than $k$ serves as a lower bound for $P$. Now let us consider the set   $V = \{ -x\mid x\text{ belongs to }P\}$. Now $-k$ serves the least upper bound  for $V$. By GLBP axiom there exists a greatest lower bound for $V$ as $V$ is also a non empty subset of $F$. By similar arguments I can assert $P$ also has a least upper bound (which is the negative of the GLB of $V$}. Hence for every non empty subset of $F$ , there exists a least upper bound. In this way I obtained LUB property from GLB property. But, I have not yet found any analysis book which carries on this task along this lines. It obviously forces me to  question : Does my way of proving lack rigor? If yes, then where?","A Completeness principle in mathematical analysis is a principle by the help of which we can establish (prove) the completeness of an ordered field( About whom I am going to post one question later). (If I am wrong in defining then It will make me immensely happy if someone corrects it). Cauchy's completeness principle, Dedekind's principle, Cantor's principle, Weierstrass's principle, greatest lower bound principle  and least upper bound principle  are the completeness principles we come across in analysis. We can take anyone of this as an axiom and derive the remaining. Now let us consider proving the Least upper bound principle  taking the greatest lower bound principle as an axiom. My proof goes in the following way. Let us consider an ordered field $F$. Now taking the glbp as an axiom I can say that any non empty subset, $P$, of this ordered field definitely posses an element $k$  ($k$ belongs to $P$) such that all the elements of $P$ are greater than or equal to $k$. Any element  of the field less than $k$ serves as a lower bound for $P$. Now let us consider the set   $V = \{ -x\mid x\text{ belongs to }P\}$. Now $-k$ serves the least upper bound  for $V$. By GLBP axiom there exists a greatest lower bound for $V$ as $V$ is also a non empty subset of $F$. By similar arguments I can assert $P$ also has a least upper bound (which is the negative of the GLB of $V$}. Hence for every non empty subset of $F$ , there exists a least upper bound. In this way I obtained LUB property from GLB property. But, I have not yet found any analysis book which carries on this task along this lines. It obviously forces me to  question : Does my way of proving lack rigor? If yes, then where?",,"['analysis', 'order-theory']"
42,Prove that $a \leq 0$ if $a \leq \frac 1 n$ for all $n$,Prove that  if  for all,a \leq 0 a \leq \frac 1 n n,"The full question reads: Suppose that $a$ is a number that has the property that for every $n \in \mathbb{N}$, $a \leq 1/n$. Prove $a \leq 0$. Is there anyway to show this using Archimedean Property, or is it something related to the Completeness Axiom?  The problem using the Archimedean Property is that I get up to $a< \epsilon$ but from there I am not able to conclude anything about whether $a \leq 0$ because $\epsilon > 0$.","The full question reads: Suppose that $a$ is a number that has the property that for every $n \in \mathbb{N}$, $a \leq 1/n$. Prove $a \leq 0$. Is there anyway to show this using Archimedean Property, or is it something related to the Completeness Axiom?  The problem using the Archimedean Property is that I get up to $a< \epsilon$ but from there I am not able to conclude anything about whether $a \leq 0$ because $\epsilon > 0$.",,['analysis']
43,"Given $\lim\limits_{x\to a}{f^\prime(x)}=\infty$, what can be concluded about $f(a)$?","Given , what can be concluded about ?",\lim\limits_{x\to a}{f^\prime(x)}=\infty f(a),"A related question to this question , I am wondering $$\lim_{x\to a}{f^\prime(x)}=+\infty,$$ what can be concluded about $f(a)$? Does this invalidate that $f(x)$ is not continuous at $a$ because of the non-existence of $f^\prime(a)$? Does this condition also imply maybe $$\lim_{x\to a}{f(x)}=+\infty?$$ I would think $$\lim_{x\to a}{f(x)}=+\infty,$$ because here $f(a+h)-f(a)$ can be arbitrarily large no matter how small $h$ is. EDIT Okay, I see where I got it wrong. Even though $$\lim_{x\to a}{f^\prime(x)}=+\infty,$$ it does not mean $f(a+h)-f(a)$ is arbitrarily large, because an $\infty$ times an infinitesimal quantity may not be determinate. I just wonder another related question: given $$\lim_{x\to a}{f(x)}=+\infty,$$ what can be concluded to $f^\prime(a)$? Can it be finite or non-existent? How about also when $a=\infty$ in this case.","A related question to this question , I am wondering $$\lim_{x\to a}{f^\prime(x)}=+\infty,$$ what can be concluded about $f(a)$? Does this invalidate that $f(x)$ is not continuous at $a$ because of the non-existence of $f^\prime(a)$? Does this condition also imply maybe $$\lim_{x\to a}{f(x)}=+\infty?$$ I would think $$\lim_{x\to a}{f(x)}=+\infty,$$ because here $f(a+h)-f(a)$ can be arbitrarily large no matter how small $h$ is. EDIT Okay, I see where I got it wrong. Even though $$\lim_{x\to a}{f^\prime(x)}=+\infty,$$ it does not mean $f(a+h)-f(a)$ is arbitrarily large, because an $\infty$ times an infinitesimal quantity may not be determinate. I just wonder another related question: given $$\lim_{x\to a}{f(x)}=+\infty,$$ what can be concluded to $f^\prime(a)$? Can it be finite or non-existent? How about also when $a=\infty$ in this case.",,['analysis']
44,"Understanding an example of ""for all"" and ""for some"" usage in statements.","Understanding an example of ""for all"" and ""for some"" usage in statements.",,"I'm reading ""Analysis I"" by Tao and reviewing an appendix chapter on logic. In there he gives an example on how ""for all x"" is usually much stronger than just saying ""for some x"": ""$6<2x<4$ for all $3<x<2$"" is vacuously true, but ""$6<2x<4$ for some $3<x<2$"" is false. I can see how the first statement is vacuously true: the hypothesis ""for all $3<x<2$"" is false as there is no $x$ that satisfies both $3<x$ and $x<2$, meaning the statement is true by default. But I don't see how the second statement works. I presume to say that statement 2 is false, one has to show that the implication (""$6<2x<4$"") is false when the hypothesis (""for some $3<x<2$"") is true. But is there a $x$ such that ""$3<x<2$"" is true? I'm obviously missing something here so clarification on this would be appreciated.","I'm reading ""Analysis I"" by Tao and reviewing an appendix chapter on logic. In there he gives an example on how ""for all x"" is usually much stronger than just saying ""for some x"": ""$6<2x<4$ for all $3<x<2$"" is vacuously true, but ""$6<2x<4$ for some $3<x<2$"" is false. I can see how the first statement is vacuously true: the hypothesis ""for all $3<x<2$"" is false as there is no $x$ that satisfies both $3<x$ and $x<2$, meaning the statement is true by default. But I don't see how the second statement works. I presume to say that statement 2 is false, one has to show that the implication (""$6<2x<4$"") is false when the hypothesis (""for some $3<x<2$"") is true. But is there a $x$ such that ""$3<x<2$"" is true? I'm obviously missing something here so clarification on this would be appreciated.",,"['analysis', 'inequality', 'logic']"
45,"If $\sum_{n=1}^\infty \frac{1}{a_n}$ converges, must $\sum_{n=1}^\infty \frac{n}{a_1 + \dots + a_n}$ converge?","If  converges, must  converge?",\sum_{n=1}^\infty \frac{1}{a_n} \sum_{n=1}^\infty \frac{n}{a_1 + \dots + a_n},"Suppose $\sum_{n=1}^\infty \frac{1}{a_n} = A$ is summable, with $a_n > 0,$ $n = 1,2,3,\cdots.$ How can we prove that $\sum_{n=1}^\infty \frac{n}{a_1 + \dots + a_n}$ is also summable? This question came from a problem-solving seminar, but I'm quite stuck without a push in the right direction. I tried a few things, including Cauchy-Schwarz (which says $\sum_{n=1}^\infty \frac{n}{a_1 + \dots + a_n} < \sum_{n=1}^\infty \frac{A}{n}$) and also the idea of assuming the latter series diverges and attempting to deduce the divergence of the former series from that, using facts such as $\sum a_n = \infty \implies \sum \frac{a_n}{a_1 + \cdots + a_n} = \infty$. Nothing has worked so far.","Suppose $\sum_{n=1}^\infty \frac{1}{a_n} = A$ is summable, with $a_n > 0,$ $n = 1,2,3,\cdots.$ How can we prove that $\sum_{n=1}^\infty \frac{n}{a_1 + \dots + a_n}$ is also summable? This question came from a problem-solving seminar, but I'm quite stuck without a push in the right direction. I tried a few things, including Cauchy-Schwarz (which says $\sum_{n=1}^\infty \frac{n}{a_1 + \dots + a_n} < \sum_{n=1}^\infty \frac{A}{n}$) and also the idea of assuming the latter series diverges and attempting to deduce the divergence of the former series from that, using facts such as $\sum a_n = \infty \implies \sum \frac{a_n}{a_1 + \cdots + a_n} = \infty$. Nothing has worked so far.",,"['analysis', 'convergence-divergence', 'contest-math', 'means']"
46,Proof of Schur's test via Young's inequality,Proof of Schur's test via Young's inequality,,"I am able to prove the following generalization of Schur's test using the Riesz-Thorin interpolation theorem, however I have been stuck for days now trying to prove it using Young's inequality: Let the integral operator $T$ from functions $f: X \to \Bbb C$ to functions $Tf: Y \to \Bbb C$ be defined via the kernel $K: X \times Y \to \Bbb C$ , which is some measurable function.  Moreover, let $$ \| K(x,\cdot) \| _{L^{q_0}(Y) } \leq 1 $$ $$\| K(\cdot,y)\|_{L^{p_1'}(X) } \leq 1$$ for all $x\in X$ and all $y\in Y$. Then for every $0<\theta<1$ and all $f\in L^{p_\theta}$ $$\| Tf \| _{L^{q_\theta}(Y) } \leq \| f\|_{L^{p_\theta}(X) } $$ where $${1\over p_\theta} = {1-\theta\over p_0} + {\theta\over p_1}   $$ $${1\over q_\theta} = {1-\theta\over q_0} + {\theta\over q_1}   $$ $${1} = {1\over p_1} + {1\over p_1'}   $$ and $p_0=1$ and $q_1=+\infty$. I am able to prove the special case $p_1'=q_0=1$  via Hölder's as well as Young inequalities.  However, I am making zero progress trying to prove the general case.  I am struggling with this now for almost a week and I would greatly appreciate help! From online sources I know that a proof based on Young's inequality is possible. Young's inequality for non-negative reals $x,y$ is  $$ xy \leq x^r/r + y^s/s$$ for dual exponents satisfying $1/r+1/s=1$ for $1<r<\infty$.  Thanks in advance.","I am able to prove the following generalization of Schur's test using the Riesz-Thorin interpolation theorem, however I have been stuck for days now trying to prove it using Young's inequality: Let the integral operator $T$ from functions $f: X \to \Bbb C$ to functions $Tf: Y \to \Bbb C$ be defined via the kernel $K: X \times Y \to \Bbb C$ , which is some measurable function.  Moreover, let $$ \| K(x,\cdot) \| _{L^{q_0}(Y) } \leq 1 $$ $$\| K(\cdot,y)\|_{L^{p_1'}(X) } \leq 1$$ for all $x\in X$ and all $y\in Y$. Then for every $0<\theta<1$ and all $f\in L^{p_\theta}$ $$\| Tf \| _{L^{q_\theta}(Y) } \leq \| f\|_{L^{p_\theta}(X) } $$ where $${1\over p_\theta} = {1-\theta\over p_0} + {\theta\over p_1}   $$ $${1\over q_\theta} = {1-\theta\over q_0} + {\theta\over q_1}   $$ $${1} = {1\over p_1} + {1\over p_1'}   $$ and $p_0=1$ and $q_1=+\infty$. I am able to prove the special case $p_1'=q_0=1$  via Hölder's as well as Young inequalities.  However, I am making zero progress trying to prove the general case.  I am struggling with this now for almost a week and I would greatly appreciate help! From online sources I know that a proof based on Young's inequality is possible. Young's inequality for non-negative reals $x,y$ is  $$ xy \leq x^r/r + y^s/s$$ for dual exponents satisfying $1/r+1/s=1$ for $1<r<\infty$.  Thanks in advance.",,"['analysis', 'interpolation', 'integral-inequality']"
47,Continuous function on unit circle has fixed point,Continuous function on unit circle has fixed point,,"The question I have is: Let $f: S^1 \rightarrow S^1$ be a continuous function, where $S^1$ is the unit circle. Prove that if $f$ is not onto, then $f$ must have a fixed point.","The question I have is: Let $f: S^1 \rightarrow S^1$ be a continuous function, where $S^1$ is the unit circle. Prove that if $f$ is not onto, then $f$ must have a fixed point.",,"['analysis', 'functions', 'fixed-point-theorems']"
48,Approximation by a $G_{\delta}$ set in Outer Measure implies Measurability,Approximation by a  set in Outer Measure implies Measurability,G_{\delta},"Question: If $E\subseteq {\mathbb R}$ and if there is some $G_{\delta}$ set $G$ such that $E\subseteq G$ and $m^{\ast}(G - E) = 0$ (where this is the outer measure that is used to define the Lebesgue measure), then $E$ is (Lebesgue) measurable. Motivation: This is part of a question in Royden's book (page 64) which probably should be obvious to me.  The way that I wanted to do this question was to use the equivalence that if, given $\epsilon > 0$, there is some open set $U\supseteq E$ such that $m^{\ast}(U-E) < \epsilon$ then $E$ is measurable. Since a $G_{\delta}$ set is not necessarily open, but made up of a countable intersection of open sets, I wanted to make it the limit of some open sets.  So, if $U_{i}$ are open, I wanted to say that $G = \bigcap_{i}U_{i}$, and we can find an open set that we need just by taking some finite intersection (say, up to $N$) of the $U_{i}$.  I'm not entirely convinced that I'm given these $U_{i}$, though, if I just know that $G$ is a $G_{\delta}$ set. ( Also, I think this question is different from the other few asking about related issues, but if it is not, then I will delete it.)","Question: If $E\subseteq {\mathbb R}$ and if there is some $G_{\delta}$ set $G$ such that $E\subseteq G$ and $m^{\ast}(G - E) = 0$ (where this is the outer measure that is used to define the Lebesgue measure), then $E$ is (Lebesgue) measurable. Motivation: This is part of a question in Royden's book (page 64) which probably should be obvious to me.  The way that I wanted to do this question was to use the equivalence that if, given $\epsilon > 0$, there is some open set $U\supseteq E$ such that $m^{\ast}(U-E) < \epsilon$ then $E$ is measurable. Since a $G_{\delta}$ set is not necessarily open, but made up of a countable intersection of open sets, I wanted to make it the limit of some open sets.  So, if $U_{i}$ are open, I wanted to say that $G = \bigcap_{i}U_{i}$, and we can find an open set that we need just by taking some finite intersection (say, up to $N$) of the $U_{i}$.  I'm not entirely convinced that I'm given these $U_{i}$, though, if I just know that $G$ is a $G_{\delta}$ set. ( Also, I think this question is different from the other few asking about related issues, but if it is not, then I will delete it.)",,['analysis']
49,Example of a non measurable function!,Example of a non measurable function!,,"Can we have a measurable function $f$, whose inverse is not measurable?","Can we have a measurable function $f$, whose inverse is not measurable?",,['analysis']
50,"How to show that $\varphi(x,y)=(x+f(y),f(x)+y)$ is bijective?",How to show that  is bijective?,"\varphi(x,y)=(x+f(y),f(x)+y)","Let $f:\mathbb{R}\to\mathbb{R}$ be a $C^1$ function such that $|f'(t)|\leq k<1$ for all $t\in \mathbb{R}$. Let $\varphi:\mathbb{R}^2\to\mathbb{R}^2$ be the function given by $\varphi(x,y)=(x+f(y),f(x)+y)$. The problem is to show that $\varphi$ is a diffeomorphism. Notice that $$\det J_\varphi (x,y)=0\quad\Rightarrow \quad f'(x)f'(y)=1\quad \Rightarrow\quad 1=|f'(x)||f'(y)|\leq k^2<1.$$ So, $\det J_\varphi (x,y)\neq 0$ for all $(x,y)\in\mathbb{R}^2$ and thus $\varphi$ is a local diffeomorphism. Therefore, to show that $\varphi$ is a diffeomorphism it's enough to show that $\varphi$ is injective and $\varphi(\mathbb{R}^2)=\mathbb{R}^2$. $\varphi$ is injective. $$\varphi(x_1,y_1)=\varphi(x_2,y_2)\quad\Rightarrow\quad x_1+f(y_1)=x_2+f(y_2)\text{ and }f(x_1)+y_1=f(x_2)+y_2$$ How to conclude that $x_1=x_2$ and $y_1=y_2$? $\varphi$ is surjective. Let $(a,b)\in\mathbb{R}^2$. We need to show that there exists $(x,y)\in \mathbb{R}^2$ such that $$\left\{\begin{align*} x+f(y)=a\\ f(x)+y=b \end{align*}\right.$$ How can we do it? Thanks.","Let $f:\mathbb{R}\to\mathbb{R}$ be a $C^1$ function such that $|f'(t)|\leq k<1$ for all $t\in \mathbb{R}$. Let $\varphi:\mathbb{R}^2\to\mathbb{R}^2$ be the function given by $\varphi(x,y)=(x+f(y),f(x)+y)$. The problem is to show that $\varphi$ is a diffeomorphism. Notice that $$\det J_\varphi (x,y)=0\quad\Rightarrow \quad f'(x)f'(y)=1\quad \Rightarrow\quad 1=|f'(x)||f'(y)|\leq k^2<1.$$ So, $\det J_\varphi (x,y)\neq 0$ for all $(x,y)\in\mathbb{R}^2$ and thus $\varphi$ is a local diffeomorphism. Therefore, to show that $\varphi$ is a diffeomorphism it's enough to show that $\varphi$ is injective and $\varphi(\mathbb{R}^2)=\mathbb{R}^2$. $\varphi$ is injective. $$\varphi(x_1,y_1)=\varphi(x_2,y_2)\quad\Rightarrow\quad x_1+f(y_1)=x_2+f(y_2)\text{ and }f(x_1)+y_1=f(x_2)+y_2$$ How to conclude that $x_1=x_2$ and $y_1=y_2$? $\varphi$ is surjective. Let $(a,b)\in\mathbb{R}^2$. We need to show that there exists $(x,y)\in \mathbb{R}^2$ such that $$\left\{\begin{align*} x+f(y)=a\\ f(x)+y=b \end{align*}\right.$$ How can we do it? Thanks.",,"['analysis', 'multivariable-calculus']"
51,How to prove that $a<S_n-[S_n]<b$ infinitely often,How to prove that  infinitely often,a<S_n-[S_n]<b,"Let $S_n=1+\frac{1}{2}+\frac{1}{3}+\ldots+\frac{1}{n}$, where $n$ is a positive integer. Prove that for any real numbers $a,b,0\le a\le b\le 1$, there exist infinite many $n\in\mathbb{N}$ such that   $$a<S_n-[S_n]<b$$   where $[x]$ represents the largest integer not exceeding $x$. This problem is from China 2012 China Second Round (High school math competition) competition last problem, I think this problem has more nice methods, maybe using analytic methods.","Let $S_n=1+\frac{1}{2}+\frac{1}{3}+\ldots+\frac{1}{n}$, where $n$ is a positive integer. Prove that for any real numbers $a,b,0\le a\le b\le 1$, there exist infinite many $n\in\mathbb{N}$ such that   $$a<S_n-[S_n]<b$$   where $[x]$ represents the largest integer not exceeding $x$. This problem is from China 2012 China Second Round (High school math competition) competition last problem, I think this problem has more nice methods, maybe using analytic methods.",,"['analysis', 'contest-math', 'ceiling-and-floor-functions', 'harmonic-numbers']"
52,Alternatives to show that $|\mathbb{R}|>|\mathbb{Z}|$,Alternatives to show that,|\mathbb{R}|>|\mathbb{Z}|,"Cantor's Diagonal Argument is the standard proof of this theorem. However there must be other proofs, what are some of these proofs? I am asking this because whenever I think of this question, I immediately think of the Cantor's Argument, which kills the possibility of  other interesting finds.","Cantor's Diagonal Argument is the standard proof of this theorem. However there must be other proofs, what are some of these proofs? I am asking this because whenever I think of this question, I immediately think of the Cantor's Argument, which kills the possibility of  other interesting finds.",,"['analysis', 'elementary-set-theory']"
53,A number $a$ is a square in $\mathbf{Q}$ if and only if it is a square in $\mathbf{R}$ and $\mathbf{Q}_p$ for all primes $p$,A number  is a square in  if and only if it is a square in  and  for all primes,a \mathbf{Q} \mathbf{R} \mathbf{Q}_p p,"Problem from Schikhof's Ultrametric Calculus. As I understand it, the intersection of $\mathbf{R}$ and all $\mathbf{Q}_p$ is just $\mathbf{Q},$ so it seems that $x^2-a$ having a zero in $\mathbf{Q}$ implies that $\sqrt{a}\in\mathbf{R}$ and $\mathbf{Q}_p$ for all primes $p.$ And on the other hand, if $\sqrt{a}\in\mathbf{R}$ and $\mathbf{Q}_p$ for all $p,$ then in particular it is in their intersection, which implies $\sqrt{a}\in\mathbf{Q}.$ But this seems like such a trivial proof that I feel like I've misunderstood what is really going on, because it suggests a more general principle that a polynomial has a rational zero if and only if it has a real zero and a $p$-adic zero for any prime $p.$ In that case, why should the question have been specifically about squares? Hoping someone can correct or verify what is going on here.","Problem from Schikhof's Ultrametric Calculus. As I understand it, the intersection of $\mathbf{R}$ and all $\mathbf{Q}_p$ is just $\mathbf{Q},$ so it seems that $x^2-a$ having a zero in $\mathbf{Q}$ implies that $\sqrt{a}\in\mathbf{R}$ and $\mathbf{Q}_p$ for all primes $p.$ And on the other hand, if $\sqrt{a}\in\mathbf{R}$ and $\mathbf{Q}_p$ for all $p,$ then in particular it is in their intersection, which implies $\sqrt{a}\in\mathbf{Q}.$ But this seems like such a trivial proof that I feel like I've misunderstood what is really going on, because it suggests a more general principle that a polynomial has a rational zero if and only if it has a real zero and a $p$-adic zero for any prime $p.$ In that case, why should the question have been specifically about squares? Hoping someone can correct or verify what is going on here.",,"['analysis', 'number-theory', 'proof-verification', 'p-adic-number-theory']"
54,Something about $\frac{\log x}{x}$,Something about,\frac{\log x}{x},"Denote $\log x = \log_ex$. Let's consider the below function $$\frac{\log x}{x}$$. Apparently, It's maximum is $\frac{1}{e}$. and strictly increasing in $(0,e]$, strictly decreasing in $[e,+\infty)$. If we draw a line $y=a$, where $0<a<\frac{1}{e}$. It will have two intersection point $x_1,x_2$. the question is how to prove $$x_1x_2 > e^2$$","Denote $\log x = \log_ex$. Let's consider the below function $$\frac{\log x}{x}$$. Apparently, It's maximum is $\frac{1}{e}$. and strictly increasing in $(0,e]$, strictly decreasing in $[e,+\infty)$. If we draw a line $y=a$, where $0<a<\frac{1}{e}$. It will have two intersection point $x_1,x_2$. the question is how to prove $$x_1x_2 > e^2$$",,"['analysis', 'inequality', 'logarithms']"
55,Direct proof of compactness of $\mathbb{Z}_p$,Direct proof of compactness of,\mathbb{Z}_p,"Let $\mathbb{Z}_{p}$ be completion of $\mathbb{Z}$ with respect to $p-$norms. Actually I know that $\mathbb{Z}_{p}$ is bijective to Cantor set, which is compact, therefore by homeomorphism, it is also compact. However, Is there any direct proof of the compactness of $\mathbb{Z}_{p}$? What I mean ""direct"" proof here is that we can only use definition of compactness, i.e., every open cover has finite subcover. What I tried to do to show compactness is that, if $\cup_{i}^{\infty}O_{i}$ is open cover, then it contain $\cup_{x \in \mathbb{Z}_p} B_{r_x}(x)$ for each $r_{x}>0$. Now I want to pick some balls using totally boundedness, but I don't know how to expand this argument.","Let $\mathbb{Z}_{p}$ be completion of $\mathbb{Z}$ with respect to $p-$norms. Actually I know that $\mathbb{Z}_{p}$ is bijective to Cantor set, which is compact, therefore by homeomorphism, it is also compact. However, Is there any direct proof of the compactness of $\mathbb{Z}_{p}$? What I mean ""direct"" proof here is that we can only use definition of compactness, i.e., every open cover has finite subcover. What I tried to do to show compactness is that, if $\cup_{i}^{\infty}O_{i}$ is open cover, then it contain $\cup_{x \in \mathbb{Z}_p} B_{r_x}(x)$ for each $r_{x}>0$. Now I want to pick some balls using totally boundedness, but I don't know how to expand this argument.",,"['analysis', 'compactness', 'p-adic-number-theory']"
56,"Integral $\int_0^\infty \frac{1}{(1+x^m)(1+x^2)}\,dx$ [duplicate]",Integral  [duplicate],"\int_0^\infty \frac{1}{(1+x^m)(1+x^2)}\,dx",This question already has answers here : Is the integral $\int_0^\infty \frac{\mathrm{d} x}{(1+x^2)(1+x^a)}$ equal for all $a \neq 0$? (4 answers) Closed 10 years ago . I saw somewhere that the above integral is equal to $\pi/4$ for all real number $m$. This seems to be surprising. Does anyone have a nice proof?,This question already has answers here : Is the integral $\int_0^\infty \frac{\mathrm{d} x}{(1+x^2)(1+x^a)}$ equal for all $a \neq 0$? (4 answers) Closed 10 years ago . I saw somewhere that the above integral is equal to $\pi/4$ for all real number $m$. This seems to be surprising. Does anyone have a nice proof?,,"['analysis', 'definite-integrals']"
57,How to show $\lim_{n \rightarrow \infty} \int_0^1 n x^n f(x) \; dx = f(1)$ for continuous $f$?,How to show  for continuous ?,\lim_{n \rightarrow \infty} \int_0^1 n x^n f(x) \; dx = f(1) f,"Let $f$ be a continuous function. I wish to show $$\lim_{n \rightarrow \infty} \int_0^1 n x^n f(x) \; dx = f(1)$$ I can try to split up the integral over intervals $[0, 1-\delta]$, $[1-\delta, 1]$. The integral over $[0, 1-\delta]$ vanishes as $n \rightarrow \infty$, but I am having difficulty estimating the integral over $[1-\delta, 1]$. Thanks!","Let $f$ be a continuous function. I wish to show $$\lim_{n \rightarrow \infty} \int_0^1 n x^n f(x) \; dx = f(1)$$ I can try to split up the integral over intervals $[0, 1-\delta]$, $[1-\delta, 1]$. The integral over $[0, 1-\delta]$ vanishes as $n \rightarrow \infty$, but I am having difficulty estimating the integral over $[1-\delta, 1]$. Thanks!",,['analysis']
58,Tao Analysis. Definition of positive rational numbers.,Tao Analysis. Definition of positive rational numbers.,,"Tao Analysis. Definition of positive rational numbers. Definition $4.2.6.$ A rational number $ x$ is said to be positive iff we have $x = a/b$ for some positive integers $a$ and $b$ . It is said to be negative iff we have $x = −y$ for some positive rational $y$ ( $i.e., $$x$ = $(−a)/b$ for some positive integers $a$ and $b$ ). My question: Does not $x = a/b$ for some negative integers $a$ and $b$ satisfy? as in $x = -2/-3$ . So why does this definition use iff?  Thank you!",Tao Analysis. Definition of positive rational numbers. Definition A rational number is said to be positive iff we have for some positive integers and . It is said to be negative iff we have for some positive rational ( = for some positive integers and ). My question: Does not for some negative integers and satisfy? as in . So why does this definition use iff?  Thank you!,"4.2.6.  x x = a/b a b x = −y y i.e., x (−a)/b a b x = a/b a b x = -2/-3",['analysis']
59,Proving that the exponential function is continuous,Proving that the exponential function is continuous,,"We aren't allowed to use many tricks such as difference quotient / integral calculus... Prove that $\exp$ is continuous at $x_{0}=0$ ............................................................................................................................... Given: $$\exp: \mathbb{R} \ni  x \mapsto \sum_{k=0}^{\infty } \frac{1}{k!} x^{k} \in \mathbb{R}$$ also $e = \exp(1)$. For all $x \in \mathbb{R}$ with $\left | x \right | \leq 1$:  $$\left | \exp(x) - 1 \right | \leq \left | x \right | \cdot (e-1)$$ and $\exp(0) = 1$ ............................................................................................................................... If I remember correctly, we said that if $|f(x) - f(x_0)| < \varepsilon$ is true then it's continuous. So I think it would be good to start with: $$e^x = \lim_{n \to \infty}\left(1 + \frac{x}{n}\right)^n$$ then show this is convergent: $$\lim_{x \to x_0} \lim_{n \to \infty} \left(1 + \frac{x}{n}\right)^n = \lim_{x \to x_0} e^x = e^{x_0} = \lim_{n \to \infty} \left(1 + \frac{x_0}{n}\right)^n = \lim_{n \to \infty} \lim_{x \to x_0} \left(1 + \frac{x}{n}\right)^n$$ and in the end put it somehow in $|f(x) - f(x_0)| < \varepsilon$ to show $\exp$ continuous? I don't know exactly how to do that but the way is correct so far?","We aren't allowed to use many tricks such as difference quotient / integral calculus... Prove that $\exp$ is continuous at $x_{0}=0$ ............................................................................................................................... Given: $$\exp: \mathbb{R} \ni  x \mapsto \sum_{k=0}^{\infty } \frac{1}{k!} x^{k} \in \mathbb{R}$$ also $e = \exp(1)$. For all $x \in \mathbb{R}$ with $\left | x \right | \leq 1$:  $$\left | \exp(x) - 1 \right | \leq \left | x \right | \cdot (e-1)$$ and $\exp(0) = 1$ ............................................................................................................................... If I remember correctly, we said that if $|f(x) - f(x_0)| < \varepsilon$ is true then it's continuous. So I think it would be good to start with: $$e^x = \lim_{n \to \infty}\left(1 + \frac{x}{n}\right)^n$$ then show this is convergent: $$\lim_{x \to x_0} \lim_{n \to \infty} \left(1 + \frac{x}{n}\right)^n = \lim_{x \to x_0} e^x = e^{x_0} = \lim_{n \to \infty} \left(1 + \frac{x_0}{n}\right)^n = \lim_{n \to \infty} \lim_{x \to x_0} \left(1 + \frac{x}{n}\right)^n$$ and in the end put it somehow in $|f(x) - f(x_0)| < \varepsilon$ to show $\exp$ continuous? I don't know exactly how to do that but the way is correct so far?",,"['analysis', 'exponential-function']"
60,"For which infinite dimensional real normed linear spaces $X$ , can we say that every infinite dimensional subspace of it is closed in $X$ ?","For which infinite dimensional real normed linear spaces  , can we say that every infinite dimensional subspace of it is closed in  ?",X X,"For which infinite dimensional real normed linear spaces $X$ , can we say that every infinite dimensional subspace of it is closed in $X$ ? Or , does every infinite dimensional normed linear space has an infinite dimensional proper subspace which is not closed  ?","For which infinite dimensional real normed linear spaces $X$ , can we say that every infinite dimensional subspace of it is closed in $X$ ? Or , does every infinite dimensional normed linear space has an infinite dimensional proper subspace which is not closed  ?",,"['analysis', 'vector-spaces']"
61,What is the importance of Jacobian Conjecture and any progress on it?,What is the importance of Jacobian Conjecture and any progress on it?,,What is the importance of Jacobian Conjecture?Are there any important central problem with the conjecture as precondition?  and any progress on it?,What is the importance of Jacobian Conjecture?Are there any important central problem with the conjecture as precondition?  and any progress on it?,,"['analysis', 'algebraic-geometry', 'reference-request']"
62,How find this$\frac{1}{{{p}_{1}}}+\frac{1}{{{p}_{2}}}+...+\frac{1}{{{p}_{n}}}<10$,How find this,\frac{1}{{{p}_{1}}}+\frac{1}{{{p}_{2}}}+...+\frac{1}{{{p}_{n}}}<10,"Let ${{p}_{1}},{{p}_{2}},...,{{p}_{n}}$ be the prime numbers less than ${{2}^{100}}$. Prove that $$\frac{1}{{{p}_{1}}}+\frac{1}{{{p}_{2}}}+...+\frac{1}{{{p}_{n}}}<10$$ This problem is from this Romania National Olympiad 2013,grade 10 -P4 http://www.artofproblemsolving.com/Forum/viewtopic.php?p=3000224&sid=70162fff5664ceb6c25410e6fe0a42f6#p3000224 So  I think this problem have nice methods,it must have  without anlaysis numbers methods This following is ugly methods By mathlinks Lemma 1, We have that $$\prod_{p\ prime \leq x}p\leq 4^x$$ for any positive integer $x$. Proof : We use induction. The base cases are trivial. Passing from an odd integer to an even one is very easy (as LHS remains the same while the RHS increases). So we only show how to pass from an even integer to an odd one (i.e. prove the inequality for $x=2k+1$). We will use the binomial coefficient $\dbinom{2k+1}{k+1}$. It is easy to see that it is divisible by $\displaystyle \prod_{k+1<p\leq 2k+1}p$ (the product is taken over primes). Therefore $\prod_{p\leq 2k+1}p\leq \left(\prod_{p\leq k+1}p\right)\left(\prod_{k+1<p\leq 2k+1}p\right)\leq 4^{k+1}\dbinom{2k+1}{k+1}\leq 4^{2k+1}$ (we used the induction hypothesis and the fact that $\dbinom{2k+1}{k+1}\leq 2^{2k}$, which is an easy exercise). The lemma is proved. Lemma 2:(Partial summation) Let $a_n$ ($n\in\mathbb{N}$) be a sequence, so that $a_n=0$ for $n<x_0$, and $S(x)=\displaystyle\sum_{n\leq x} a_n$. Let $f$ be a function with continuous derivative. Then $\sum_{n\leq x} a_nf(n)=S(x)f(x)-\int_{x_0}^{x}S(t)f'(t)\mathrm{d} t$ Proof:Let us note that $$\sum_{n\leq x} a_nf(n)=\sum_{n\leq x} (S(n)-S(n-1))f(n)=S(x)f(x)-$$  $$\sum_{n\leq x} S(n-1)(f(n)-f(n-1))=S(x)f(x)-\sum_{n\leq x-1} S(n)\int_{n}^{n+1}f'(t)\mathrm{d}t$$.  As $S$ behaves like a step function constant on $[n,n+1)$, we get that $\sum_{n\leq x}a_nf(n)=S(x)f(x)-\int_{0}^xS(t)f'(t)\ \mathrm{d}t=S(x)f(x)-\int_{x_0}^xS(t)f'(t)\mathrm{d}t$, as $a_n=0$ for $n<x_0$. With these two lemmas we are ready to prove the problem. From the first lemma we have (by taking logarithms) that $\sum_{p\leq n}\log(p)\leq n\log(4)$, for any integer $n$ so the relation actually holds even if $n$ is any positive real number. We use lemma 2 with $a_n=\log(n)$ if $n$ is prime and $a_n=0$ otherwise. We take $f(x)=\frac{1}{x\log(x)}$. We can take in the lemma $x_0=2$. We have that $$\sum_{p\leq x}\frac{1}{p}=\sum_{n\leq x} a_nf(n)=S(x)f(x)-\int_{2}^{x}S(t)f'(t)\mathrm{d} t=\frac{S(x)}{x\log(x)}+\int_{2}^x\frac{S(t)(1+\log(t))}{t^2\log^2(t)}\mathrm{d}t$$ Using that $S(x) \leq x\log(4)$ we get that $$\sum_{p\leq x}\frac{1}{p}\leq \frac{\log(4)}{\log(x)}+\int_{2}^x\frac{\log(4)}{t\log^2(t)}\mathrm{d}t+\int_{2}^x\frac{\log(4)}{t\log(t)}\mathrm{d}t$$. The antiderivative of the function in the first integral is $\frac{1}{\log(t)}$, so the first integral is at most $\frac{\log(4)}{\log(2)}=2$, and the antiderivative of the function in the second integral is $\log(\log(x))$. We therefore have that $$\sum_{p\leq x}\frac{1}{p}\leq \frac{\log(4)}{\log(x)}+2+\log(4)(\log(\log(x))-\log(\log(2)))$. For $x=2^{100}$$ we have that $$\sum_{p\leq 2^{100}}\frac{1}{p}\leq \frac{1}{50}+2+\log(4)\log(100)$$, and the last expression is less than $8.405$ (in an olympiad one might use that $e$ is greater than $2.7$ and use this to estimate $\log(2)$ and $\log(10)$)","Let ${{p}_{1}},{{p}_{2}},...,{{p}_{n}}$ be the prime numbers less than ${{2}^{100}}$. Prove that $$\frac{1}{{{p}_{1}}}+\frac{1}{{{p}_{2}}}+...+\frac{1}{{{p}_{n}}}<10$$ This problem is from this Romania National Olympiad 2013,grade 10 -P4 http://www.artofproblemsolving.com/Forum/viewtopic.php?p=3000224&sid=70162fff5664ceb6c25410e6fe0a42f6#p3000224 So  I think this problem have nice methods,it must have  without anlaysis numbers methods This following is ugly methods By mathlinks Lemma 1, We have that $$\prod_{p\ prime \leq x}p\leq 4^x$$ for any positive integer $x$. Proof : We use induction. The base cases are trivial. Passing from an odd integer to an even one is very easy (as LHS remains the same while the RHS increases). So we only show how to pass from an even integer to an odd one (i.e. prove the inequality for $x=2k+1$). We will use the binomial coefficient $\dbinom{2k+1}{k+1}$. It is easy to see that it is divisible by $\displaystyle \prod_{k+1<p\leq 2k+1}p$ (the product is taken over primes). Therefore $\prod_{p\leq 2k+1}p\leq \left(\prod_{p\leq k+1}p\right)\left(\prod_{k+1<p\leq 2k+1}p\right)\leq 4^{k+1}\dbinom{2k+1}{k+1}\leq 4^{2k+1}$ (we used the induction hypothesis and the fact that $\dbinom{2k+1}{k+1}\leq 2^{2k}$, which is an easy exercise). The lemma is proved. Lemma 2:(Partial summation) Let $a_n$ ($n\in\mathbb{N}$) be a sequence, so that $a_n=0$ for $n<x_0$, and $S(x)=\displaystyle\sum_{n\leq x} a_n$. Let $f$ be a function with continuous derivative. Then $\sum_{n\leq x} a_nf(n)=S(x)f(x)-\int_{x_0}^{x}S(t)f'(t)\mathrm{d} t$ Proof:Let us note that $$\sum_{n\leq x} a_nf(n)=\sum_{n\leq x} (S(n)-S(n-1))f(n)=S(x)f(x)-$$  $$\sum_{n\leq x} S(n-1)(f(n)-f(n-1))=S(x)f(x)-\sum_{n\leq x-1} S(n)\int_{n}^{n+1}f'(t)\mathrm{d}t$$.  As $S$ behaves like a step function constant on $[n,n+1)$, we get that $\sum_{n\leq x}a_nf(n)=S(x)f(x)-\int_{0}^xS(t)f'(t)\ \mathrm{d}t=S(x)f(x)-\int_{x_0}^xS(t)f'(t)\mathrm{d}t$, as $a_n=0$ for $n<x_0$. With these two lemmas we are ready to prove the problem. From the first lemma we have (by taking logarithms) that $\sum_{p\leq n}\log(p)\leq n\log(4)$, for any integer $n$ so the relation actually holds even if $n$ is any positive real number. We use lemma 2 with $a_n=\log(n)$ if $n$ is prime and $a_n=0$ otherwise. We take $f(x)=\frac{1}{x\log(x)}$. We can take in the lemma $x_0=2$. We have that $$\sum_{p\leq x}\frac{1}{p}=\sum_{n\leq x} a_nf(n)=S(x)f(x)-\int_{2}^{x}S(t)f'(t)\mathrm{d} t=\frac{S(x)}{x\log(x)}+\int_{2}^x\frac{S(t)(1+\log(t))}{t^2\log^2(t)}\mathrm{d}t$$ Using that $S(x) \leq x\log(4)$ we get that $$\sum_{p\leq x}\frac{1}{p}\leq \frac{\log(4)}{\log(x)}+\int_{2}^x\frac{\log(4)}{t\log^2(t)}\mathrm{d}t+\int_{2}^x\frac{\log(4)}{t\log(t)}\mathrm{d}t$$. The antiderivative of the function in the first integral is $\frac{1}{\log(t)}$, so the first integral is at most $\frac{\log(4)}{\log(2)}=2$, and the antiderivative of the function in the second integral is $\log(\log(x))$. We therefore have that $$\sum_{p\leq x}\frac{1}{p}\leq \frac{\log(4)}{\log(x)}+2+\log(4)(\log(\log(x))-\log(\log(2)))$. For $x=2^{100}$$ we have that $$\sum_{p\leq 2^{100}}\frac{1}{p}\leq \frac{1}{50}+2+\log(4)\log(100)$$, and the last expression is less than $8.405$ (in an olympiad one might use that $e$ is greater than $2.7$ and use this to estimate $\log(2)$ and $\log(10)$)",,"['analysis', 'number-theory', 'inequality']"
63,Proof for Strong Induction Principle,Proof for Strong Induction Principle,,"I am currently studying analysis and I came across the following exercise. Proposotion 2.2.14 Let $m_0$ be a natural number and let $P(m)$ be a property pertaining to an arbitrary natural number $m$. Suppose that for each $m\geq m_0$, we have the following implication: if $P(m')$ is true for all natural numbers $m_0\leq m'< m$, then $P(m)$ is also true. (In particular, this means that $P(m_0)$ is true, since in this case the hypothesis is vacuous.) Then we can conclude that $P(m)$ is true for all natural numbers $m\geq m_0$. Prove Proposition 2.2.14. (Hint: define $Q(n)$ to be the property that $P(m)$ is true for all $m_0\leq m < n$; note that $Q(n)$ is vacuously true when $n<m_0$.) I have difficulty understanding how I should use the hint and in general what the framework of this proof would look like (probably an inductive proof; but on what variable do we induct, what will be the induction hypothesis and how would I go about proving the inductive step etc.?). Could anyone please provide me with some hints to help me get started?","I am currently studying analysis and I came across the following exercise. Proposotion 2.2.14 Let $m_0$ be a natural number and let $P(m)$ be a property pertaining to an arbitrary natural number $m$. Suppose that for each $m\geq m_0$, we have the following implication: if $P(m')$ is true for all natural numbers $m_0\leq m'< m$, then $P(m)$ is also true. (In particular, this means that $P(m_0)$ is true, since in this case the hypothesis is vacuous.) Then we can conclude that $P(m)$ is true for all natural numbers $m\geq m_0$. Prove Proposition 2.2.14. (Hint: define $Q(n)$ to be the property that $P(m)$ is true for all $m_0\leq m < n$; note that $Q(n)$ is vacuously true when $n<m_0$.) I have difficulty understanding how I should use the hint and in general what the framework of this proof would look like (probably an inductive proof; but on what variable do we induct, what will be the induction hypothesis and how would I go about proving the inductive step etc.?). Could anyone please provide me with some hints to help me get started?",,"['analysis', 'elementary-set-theory', 'induction']"
64,Divergent sequence where $\lim_{n\to\infty}(a_{n+p}-a_n) = 0 $,Divergent sequence where,\lim_{n\to\infty}(a_{n+p}-a_n) = 0 ,"I need to show that $(a_n)=\sin(\ln n)$ is a bounded sequence that has no limit, but for which $$\lim_{n\to\infty}(a_{n+p}-a_n )= 0\; \forall p \in \mathbb N.$$ I can show that the sequence is bounded and that it diverges, but I'm stuck with the other part. So far I've found that $\lim_{n\to\infty} (\ln (n+p)-\ln n) = 0$, but I don't really know where to go next. The easy way to get the sines in would of course be $$ \lim\ln(n+p)-\lim\ln n = 0 \Leftrightarrow \lim\sin(\ln(n+p))-\lim\sin(\ln n)=0 \Leftrightarrow \lim(\sin (\ln (n+p)) - \sin (\ln n)) = 0,$$ but as both $\sin x$ and $\ln x$ diverge, I'm not sure that'd be ok. So am I anywhere near the right direction or should I try a different approach?","I need to show that $(a_n)=\sin(\ln n)$ is a bounded sequence that has no limit, but for which $$\lim_{n\to\infty}(a_{n+p}-a_n )= 0\; \forall p \in \mathbb N.$$ I can show that the sequence is bounded and that it diverges, but I'm stuck with the other part. So far I've found that $\lim_{n\to\infty} (\ln (n+p)-\ln n) = 0$, but I don't really know where to go next. The easy way to get the sines in would of course be $$ \lim\ln(n+p)-\lim\ln n = 0 \Leftrightarrow \lim\sin(\ln(n+p))-\lim\sin(\ln n)=0 \Leftrightarrow \lim(\sin (\ln (n+p)) - \sin (\ln n)) = 0,$$ but as both $\sin x$ and $\ln x$ diverge, I'm not sure that'd be ok. So am I anywhere near the right direction or should I try a different approach?",,['analysis']
65,From $\sum_p \frac{\log p}{p^s} = \frac{1}{s-1} + O(1)$ conclude that $\sum_p \frac{1}{p^s} = \log \frac{1}{s-1} + O(1)$,From  conclude that,\sum_p \frac{\log p}{p^s} = \frac{1}{s-1} + O(1) \sum_p \frac{1}{p^s} = \log \frac{1}{s-1} + O(1),"I'm reading a book on analytic number theory. It asks me to prove: $$ \sum_p \frac{\log p}{p^s} = \frac{1}{s-1} + O(1) \tag{A}$$   and conclude, via integration, that:     $$ \sum_p \frac{1}{p^s} = \log \frac{1}{s-1} + O(1) \tag{B}$$ Now, I know how to prove $(A)$ via Abel Summation. However, when it comes to $(B),$ I have the problem that although: $$\frac{d}{dx} \log(x-1) = \frac{1}{x-1}$$ and $$\frac{d}{ds} p^{-s} = (-\log p)p^{-s}$$ I have the problem that when I integrate over $O(1)$, I get $\infty$, not $O(1)$. What am I doing wrong? How do I get from $(A)$ to $(B)$?","I'm reading a book on analytic number theory. It asks me to prove: $$ \sum_p \frac{\log p}{p^s} = \frac{1}{s-1} + O(1) \tag{A}$$   and conclude, via integration, that:     $$ \sum_p \frac{1}{p^s} = \log \frac{1}{s-1} + O(1) \tag{B}$$ Now, I know how to prove $(A)$ via Abel Summation. However, when it comes to $(B),$ I have the problem that although: $$\frac{d}{dx} \log(x-1) = \frac{1}{x-1}$$ and $$\frac{d}{ds} p^{-s} = (-\log p)p^{-s}$$ I have the problem that when I integrate over $O(1)$, I get $\infty$, not $O(1)$. What am I doing wrong? How do I get from $(A)$ to $(B)$?",,['number-theory']
66,Is there a differentiable function that approaches infinity but has a bounded derivative?,Is there a differentiable function that approaches infinity but has a bounded derivative?,,"I am looking for a function $\,f : \mathbb{R} \rightarrow \mathbb{R}$ that is differentiable except at one point $x$ at which it approaches infinity. Furthermore the derivative of $\,f\,$ should be bounded in a neighborhood around $x$ and not approach infinity as one approaches $x$. Is there such a function, or is this requirement contradictory?","I am looking for a function $\,f : \mathbb{R} \rightarrow \mathbb{R}$ that is differentiable except at one point $x$ at which it approaches infinity. Furthermore the derivative of $\,f\,$ should be bounded in a neighborhood around $x$ and not approach infinity as one approaches $x$. Is there such a function, or is this requirement contradictory?",,['analysis']
67,Generalization of the series for $\frac{\pi^2}{6}$? Is there a more elementary proof?,Generalization of the series for ? Is there a more elementary proof?,\frac{\pi^2}{6},"In the same vein as: $ \frac{\pi ^2}{6} = 1 + \frac{1}{4} + \frac{1}{9} + \frac{1}{25} \cdots $ Starting with: $ \displaystyle \prod_{n=1}^{\infty} \left( 1 -\frac{q^2}{n^2} \right) = \frac{\sin(\pi q)}{\pi q}$ I've noticed that: $ - \frac{\pi ^2}{3!} = \displaystyle \sum_{j_1=1}^{\infty} -j_1^{-2} $ $ \frac{\pi ^4}{5!} = \displaystyle  \sum_{j_1,j_2=1 \atop j_1 \neq j_2}^{\infty} (j_1j_2)^{-2}$ $ - \frac{\pi ^6}{7!} = \displaystyle \sum_{j_1,j_2,j_3=1 \atop j_i \neq j_k} - (j_1j_2j_3)^{-2}$ $ \vdots $ $ \frac{\pi ^{2n}}{(2n+1)!} = \displaystyle \sum_{j_1,...j_n=1 \atop j_i \neq j_k}^{\infty} (j_1j_2...j_n)^{-2}$ $ \vdots $ (Steps shown here: http://www.futurebird.com/?p=156 ) Is there a more direct way to reach the same result that avoids a high power theorem like the Weierstrass factorization theorem ... which is what I use. I'm enjoying playing with these concepts so I'd also like reading recommendations.","In the same vein as: $ \frac{\pi ^2}{6} = 1 + \frac{1}{4} + \frac{1}{9} + \frac{1}{25} \cdots $ Starting with: $ \displaystyle \prod_{n=1}^{\infty} \left( 1 -\frac{q^2}{n^2} \right) = \frac{\sin(\pi q)}{\pi q}$ I've noticed that: $ - \frac{\pi ^2}{3!} = \displaystyle \sum_{j_1=1}^{\infty} -j_1^{-2} $ $ \frac{\pi ^4}{5!} = \displaystyle  \sum_{j_1,j_2=1 \atop j_1 \neq j_2}^{\infty} (j_1j_2)^{-2}$ $ - \frac{\pi ^6}{7!} = \displaystyle \sum_{j_1,j_2,j_3=1 \atop j_i \neq j_k} - (j_1j_2j_3)^{-2}$ $ \vdots $ $ \frac{\pi ^{2n}}{(2n+1)!} = \displaystyle \sum_{j_1,...j_n=1 \atop j_i \neq j_k}^{\infty} (j_1j_2...j_n)^{-2}$ $ \vdots $ (Steps shown here: http://www.futurebird.com/?p=156 ) Is there a more direct way to reach the same result that avoids a high power theorem like the Weierstrass factorization theorem ... which is what I use. I'm enjoying playing with these concepts so I'd also like reading recommendations.",,"['analysis', 'generating-functions', 'products']"
68,$a^n+ b^n$ is not a prime number when $n$ is not a power of $2$.,is not a prime number when  is not a power of .,a^n+ b^n n 2,"Suppose $a, b$ and $n$ are positive integers, $a+b>2$, and $n$ is not a power of $2$. Prove that $a^n+ b^n$ is not a prime number.","Suppose $a, b$ and $n$ are positive integers, $a+b>2$, and $n$ is not a power of $2$. Prove that $a^n+ b^n$ is not a prime number.",,"['number-theory', 'analysis', 'prime-numbers', 'divisibility']"
69,Application of implicit function theorem?,Application of implicit function theorem?,,"Let $f: U \subset \mathbb{R}^2 \to \mathbb{R}$ be a continuous function in the open subset $U$ of $\mathbb{R}^2$ such that $$(x^2+y^4)f(x,y)+f(x,y)^3 = 1, \forall (x,y) \in U$$ Show that $f$ is of class $C^1$ in $U$. I think that is an application of implicit function theorem, but I don't know how to solve it, because I only saw examples about system of linear equations.","Let $f: U \subset \mathbb{R}^2 \to \mathbb{R}$ be a continuous function in the open subset $U$ of $\mathbb{R}^2$ such that $$(x^2+y^4)f(x,y)+f(x,y)^3 = 1, \forall (x,y) \in U$$ Show that $f$ is of class $C^1$ in $U$. I think that is an application of implicit function theorem, but I don't know how to solve it, because I only saw examples about system of linear equations.",,"['analysis', 'derivatives', 'manifolds']"
70,Prove that $\cos (n \arccos (x))$ is a polynomial of $n$-th degree,Prove that  is a polynomial of -th degree,\cos (n \arccos (x)) n,"So, we got an assignment from the mathematical analysis class to prove that $\cos (n \arccos (x))$ is a polynomial of $n$th degree. I tried to prove it with mathematical induction and so far what I got is: basis :- $T(1) - \cos (1\cdot\arccos (x)) = \cos(\arccos(x)) = x$ Inductive step : $T(n+1)$ - assume it holds for $T(n)$, then it must hold for $T(n+1)$ $$\begin{align}\cos[(n+1) \arccos(x)] = &\cos[n \arccos(x) + \arccos(x)] &\\=  &\cos[n \arccos(x)] \cos[\arccos(x)] - \sin[n \arccos(x)] \sin[\arccos(x)]  &\\=& \cos[n\arccos(x)] x - \sin[n \arccos(x)] \sin[\arccos(x)]\end{align}$$ and this is where I got stuck because I don't know what to do with the sine part. like, I get that the $x\cos(n \arccos(x))$ holds because, by inductive step, it holds that $\cos(n\arccos(x))$ is the polynomial of nth degree, so $x\cos(n \arccos(x))$ is polynomial of $(n+1)$th degree. But I don't know if this is enough to prove this or if I need to do something with the sine too.","So, we got an assignment from the mathematical analysis class to prove that $\cos (n \arccos (x))$ is a polynomial of $n$th degree. I tried to prove it with mathematical induction and so far what I got is: basis :- $T(1) - \cos (1\cdot\arccos (x)) = \cos(\arccos(x)) = x$ Inductive step : $T(n+1)$ - assume it holds for $T(n)$, then it must hold for $T(n+1)$ $$\begin{align}\cos[(n+1) \arccos(x)] = &\cos[n \arccos(x) + \arccos(x)] &\\=  &\cos[n \arccos(x)] \cos[\arccos(x)] - \sin[n \arccos(x)] \sin[\arccos(x)]  &\\=& \cos[n\arccos(x)] x - \sin[n \arccos(x)] \sin[\arccos(x)]\end{align}$$ and this is where I got stuck because I don't know what to do with the sine part. like, I get that the $x\cos(n \arccos(x))$ holds because, by inductive step, it holds that $\cos(n\arccos(x))$ is the polynomial of nth degree, so $x\cos(n \arccos(x))$ is polynomial of $(n+1)$th degree. But I don't know if this is enough to prove this or if I need to do something with the sine too.",,"['analysis', 'induction']"
71,Is a surjective mapping of R2 to itself with full rank derivative everywhere necessarily injective?,Is a surjective mapping of R2 to itself with full rank derivative everywhere necessarily injective?,,"If $f:\mathbb R^2\rightarrow\mathbb R^2$ has rank 2 derivative everywhere, then by the inverse function theorem it is locally injective. If it is surjective, is it then necessarily globally injective as well? What if we consider the same case, but for a map of $\mathbb R^{2+}$ to itself (i.e. ($x\geq 0,y\geq 0$)?","If $f:\mathbb R^2\rightarrow\mathbb R^2$ has rank 2 derivative everywhere, then by the inverse function theorem it is locally injective. If it is surjective, is it then necessarily globally injective as well? What if we consider the same case, but for a map of $\mathbb R^{2+}$ to itself (i.e. ($x\geq 0,y\geq 0$)?",,"['analysis', 'differential-geometry']"
72,Proof of Strong principle of Induction (T. Tao Analysis I),Proof of Strong principle of Induction (T. Tao Analysis I),,"I have no idea how to prove it by using only what the book has talked about so far. Can anyone help? The proof shouldn't be using set theory as set theory is only mentioned in the following chapter. The proof should only make use of the addition of natural numbers, order properties of natural numbers, the trichotomy of order for natural numbers and principle of induction. Proposition 2.2.14 (Strong principle of induction). Let $m_0$ be a natural number, and let $P(m)$ be a property pertaining to an arbitrary natural number $m$ . Suppose that for each $m ≥ m_0$ , we have the following implication: if $P(m')$ is true for all natural numbers $m_0 ≤ m' < m$ , then $P(m)$ is also true. (In particular, this means that $P(m_0)$ is true, since in this case the hypothesis is vacuous.) Then we can conclude that $P(m)$ is true for all natural numbers $m ≥ m_0$ . Exercise 2.2.5. Prove Proposition 2.2.14. (Hint: define $Q(n)$ to be the property that $P(m)$ is true for all $m_0 ≤ m < n$ ; note that $Q(n)$ is vacuously true when $n < m_0$ .) Thanks!","I have no idea how to prove it by using only what the book has talked about so far. Can anyone help? The proof shouldn't be using set theory as set theory is only mentioned in the following chapter. The proof should only make use of the addition of natural numbers, order properties of natural numbers, the trichotomy of order for natural numbers and principle of induction. Proposition 2.2.14 (Strong principle of induction). Let be a natural number, and let be a property pertaining to an arbitrary natural number . Suppose that for each , we have the following implication: if is true for all natural numbers , then is also true. (In particular, this means that is true, since in this case the hypothesis is vacuous.) Then we can conclude that is true for all natural numbers . Exercise 2.2.5. Prove Proposition 2.2.14. (Hint: define to be the property that is true for all ; note that is vacuously true when .) Thanks!",m_0 P(m) m m ≥ m_0 P(m') m_0 ≤ m' < m P(m) P(m_0) P(m) m ≥ m_0 Q(n) P(m) m_0 ≤ m < n Q(n) n < m_0,['analysis']
73,Fourier series simplification,Fourier series simplification,,"I want to show that $$\frac{1}{\pi} \int_{-\pi}^{\pi} f(x)g(x)dx = \frac{a_0\alpha_0}{2} + \sum_{n=1}^{\infty} (a_n\alpha_n + b_n\beta_n)$$ where $f,g: [-\pi,\pi] \to \mathbb{R}$ are integral functions and that the Fourier series of $f$ and $g$ are uniformly convergent to $f,g$ respectively. $a_0,a_n,b_n,\alpha_0,\alpha_n,\beta_n$ are the fourier coefficients of $f,g$ respectively. I thought we could simply expand the LHS, simplify, and then show equality. I start by multiplying $f(x)g(x)$ via their respective Fourier series. $$a_0\alpha_0 + a_0(\sum_{n=1}^{\infty} \alpha_n\cos(nx) + \beta_n\sin(nx))+\alpha_0(\sum_{n=1}^{\infty} a_n\cos(nx) + b_n\sin(nx)) + (\sum_{n=1}^{\infty} \alpha_n\cos(nx) + \beta_n\sin(nx))(\sum_{n=1}^{\infty} a_n\cos(nx) + b_n\sin(nx))$$ How do I simplify this? Because we have uniform convergence, we can switch the places of the Sigma and Integral symbols yeah? Even then though..","I want to show that $$\frac{1}{\pi} \int_{-\pi}^{\pi} f(x)g(x)dx = \frac{a_0\alpha_0}{2} + \sum_{n=1}^{\infty} (a_n\alpha_n + b_n\beta_n)$$ where $f,g: [-\pi,\pi] \to \mathbb{R}$ are integral functions and that the Fourier series of $f$ and $g$ are uniformly convergent to $f,g$ respectively. $a_0,a_n,b_n,\alpha_0,\alpha_n,\beta_n$ are the fourier coefficients of $f,g$ respectively. I thought we could simply expand the LHS, simplify, and then show equality. I start by multiplying $f(x)g(x)$ via their respective Fourier series. $$a_0\alpha_0 + a_0(\sum_{n=1}^{\infty} \alpha_n\cos(nx) + \beta_n\sin(nx))+\alpha_0(\sum_{n=1}^{\infty} a_n\cos(nx) + b_n\sin(nx)) + (\sum_{n=1}^{\infty} \alpha_n\cos(nx) + \beta_n\sin(nx))(\sum_{n=1}^{\infty} a_n\cos(nx) + b_n\sin(nx))$$ How do I simplify this? Because we have uniform convergence, we can switch the places of the Sigma and Integral symbols yeah? Even then though..",,"['analysis', 'fourier-series']"
74,trace map is continuous,trace map is continuous,,"Prove that $tr: M_n(k)\to k$ is continuous. I did continuity of determinant map using induction, but how to prove trace map is continuous. please give a thorough answer. My analysis is not too good.","Prove that $tr: M_n(k)\to k$ is continuous. I did continuity of determinant map using induction, but how to prove trace map is continuous. please give a thorough answer. My analysis is not too good.",,"['analysis', 'manifolds']"
75,Which $f \in L^\infty$ are the Fourier transform of a bounded complex measure?,Which  are the Fourier transform of a bounded complex measure?,f \in L^\infty,"A measure on $\mathbb R$ is a set function $\mu,$ defined for all Borel sets of $\mathbb R,$ which is countably additive(that is, $\mu(E)=\sum \mu(E_{i})$ if $E$ is the union of the countable family $\{E_{i}\}$ of pairwise disjoint Borel sets of $\mathbb R$), and for which $\mu(E)$ is finite if the closure of $E$ is compact. With each $\mu$ on $\mathbb R$ there is associated a set function $|\mu|,$ the total variation of $\mu,$ defined by, $$|\mu|(E)= \sup \sum |\mu(E_{i})|,$$ the supreme being taken over all finite collections of pairwise disjoint  Borel sets $E_{i}$ whose union is $E.$ Then $|\mu|$ is also a measure on $\mathbb R.$ If $\|\mu\|= |\mu|(\mathbb R)< \infty,$ we say $\mu$ is a bounded complex Borel measure on $\mathbb R.$ Put, $M(\mathbb R)=$ The space of all bounded complex Borel measures on $\mathbb R.$ Put, $ L^{\infty}(\mathbb R)=$ The space of essentially bounded functions on $\mathbb R.$ My Question is : Let $f\in L^{\infty}(\mathbb R).$ Can we expect to find bounded complex Borel measure $\mu$ on $\mathbb R$ such that its Fourier–Stieltjes transform is $f$, that is,   $$f(\xi)=\hat{\mu}(\xi)=\int_{\mathbb R}  e^{-2\pi i x\cdot \xi} d\mu(x); (\xi\in \mathbb R) ?$$ If answer is negative,then what extra condition one should think of to  impose on the given member of $L^{\infty}$, to solve the above integral  equation ? Thanks, Edit : We note that, Fourerie-Steltije transform maps, $M(\mathbb R) \to L^{\infty}(\mathbb R): \mu \mapsto \hat{u}$ with inequality, $\|\hat{\mu}\|_{L^{\infty}} \leq \|\mu\|.$","A measure on $\mathbb R$ is a set function $\mu,$ defined for all Borel sets of $\mathbb R,$ which is countably additive(that is, $\mu(E)=\sum \mu(E_{i})$ if $E$ is the union of the countable family $\{E_{i}\}$ of pairwise disjoint Borel sets of $\mathbb R$), and for which $\mu(E)$ is finite if the closure of $E$ is compact. With each $\mu$ on $\mathbb R$ there is associated a set function $|\mu|,$ the total variation of $\mu,$ defined by, $$|\mu|(E)= \sup \sum |\mu(E_{i})|,$$ the supreme being taken over all finite collections of pairwise disjoint  Borel sets $E_{i}$ whose union is $E.$ Then $|\mu|$ is also a measure on $\mathbb R.$ If $\|\mu\|= |\mu|(\mathbb R)< \infty,$ we say $\mu$ is a bounded complex Borel measure on $\mathbb R.$ Put, $M(\mathbb R)=$ The space of all bounded complex Borel measures on $\mathbb R.$ Put, $ L^{\infty}(\mathbb R)=$ The space of essentially bounded functions on $\mathbb R.$ My Question is : Let $f\in L^{\infty}(\mathbb R).$ Can we expect to find bounded complex Borel measure $\mu$ on $\mathbb R$ such that its Fourier–Stieltjes transform is $f$, that is,   $$f(\xi)=\hat{\mu}(\xi)=\int_{\mathbb R}  e^{-2\pi i x\cdot \xi} d\mu(x); (\xi\in \mathbb R) ?$$ If answer is negative,then what extra condition one should think of to  impose on the given member of $L^{\infty}$, to solve the above integral  equation ? Thanks, Edit : We note that, Fourerie-Steltije transform maps, $M(\mathbb R) \to L^{\infty}(\mathbb R): \mu \mapsto \hat{u}$ with inequality, $\|\hat{\mu}\|_{L^{\infty}} \leq \|\mu\|.$",,"['analysis', 'measure-theory', 'fourier-analysis', 'harmonic-analysis', 'lebesgue-measure']"
76,Radius of convergence of $\sum\limits_{n \ge 1} a_n z^n$ where $a_n$ is the number of divisors of $n^{50}$,Radius of convergence of  where  is the number of divisors of,\sum\limits_{n \ge 1} a_n z^n a_n n^{50},"Consider the power series $\sum_{n \ge 1} a_n z^n$, where $a_n$ is the number of divisors of $n^{50}$. What is its radius of convergence? My attempt $a_n < n^{50}$ $\forall$ $n$. So $\lim \{a_n\}^{\frac{1}{n}} \le \{\lim n^{\frac{1}{n}}\}^{50} = 1$. So radius of convergence is 1.","Consider the power series $\sum_{n \ge 1} a_n z^n$, where $a_n$ is the number of divisors of $n^{50}$. What is its radius of convergence? My attempt $a_n < n^{50}$ $\forall$ $n$. So $\lim \{a_n\}^{\frac{1}{n}} \le \{\lim n^{\frac{1}{n}}\}^{50} = 1$. So radius of convergence is 1.",,"['analysis', 'power-series']"
77,Proof on showing function $f \in C^1$ on an open & convex set $U \subset \mathbb R^n$ is Lipschitz on compact subsets of $U$,Proof on showing function  on an open & convex set  is Lipschitz on compact subsets of,f \in C^1 U \subset \mathbb R^n U,"The question is as follows: Given: (1) function $f: U \subset \mathbb R^n ==> \mathbb R$ (2) $U$ is open and convex set (3) $f \in  C^1$ in $U$ Goal: Show that $f$ is Lipschitz on any compact subset of $U$ By now, I have various ideas come to mind, but I can't connect the dots >_< Here are my thoughts so far: (1) Recall definitions: (i) $f \in C^k$ means all partial derivatives up to (and including) order $k$ exist and continuous.  Here $k = 1$ (ii) a set $U$ is convex if for any 2 points $x, y$ in $U$, the segment joining $x$ and $y$ is totally inside $U$ (iii) function $f$ is Lipschitz if there is a bound $M$ such that $|f(x) - f(y)| \leq M |x-y|$ (2)By a theorem in my book: if function $f \in C^1$ on an open & convex set $U$, then for any 2   points $x$ and $y$ in $U$, there is a point $s$ lying on the segment   joining $x$ and $y$ such that $f(x) - f(y) = Df(s) * (x - y)$ I firstly have a feeling that I may need this theorem in the proof, but I can't see the connection between my desired bound $M$ with $Df(s)$.  So I tried to think of other ideas and ... (3) It turns out that by all the given information, I think if I can show function $f$ is convex, then I'm done because there is a theorem which said: a convex function on a open, convex set $U$ should be Lipschitz on $U$, thus I think it must also be Lipschitz on any subset of $U$, either that subset is compact or not. However, how can I prove function $f$ is convex, based on given information?  I have a feeling that I have to use the fact that set $U$ is convex, but then I'm stuck on how to proceed further >_> *Would someone please help me on this question? Thank you very much ^_^*","The question is as follows: Given: (1) function $f: U \subset \mathbb R^n ==> \mathbb R$ (2) $U$ is open and convex set (3) $f \in  C^1$ in $U$ Goal: Show that $f$ is Lipschitz on any compact subset of $U$ By now, I have various ideas come to mind, but I can't connect the dots >_< Here are my thoughts so far: (1) Recall definitions: (i) $f \in C^k$ means all partial derivatives up to (and including) order $k$ exist and continuous.  Here $k = 1$ (ii) a set $U$ is convex if for any 2 points $x, y$ in $U$, the segment joining $x$ and $y$ is totally inside $U$ (iii) function $f$ is Lipschitz if there is a bound $M$ such that $|f(x) - f(y)| \leq M |x-y|$ (2)By a theorem in my book: if function $f \in C^1$ on an open & convex set $U$, then for any 2   points $x$ and $y$ in $U$, there is a point $s$ lying on the segment   joining $x$ and $y$ such that $f(x) - f(y) = Df(s) * (x - y)$ I firstly have a feeling that I may need this theorem in the proof, but I can't see the connection between my desired bound $M$ with $Df(s)$.  So I tried to think of other ideas and ... (3) It turns out that by all the given information, I think if I can show function $f$ is convex, then I'm done because there is a theorem which said: a convex function on a open, convex set $U$ should be Lipschitz on $U$, thus I think it must also be Lipschitz on any subset of $U$, either that subset is compact or not. However, how can I prove function $f$ is convex, based on given information?  I have a feeling that I have to use the fact that set $U$ is convex, but then I'm stuck on how to proceed further >_> *Would someone please help me on this question? Thank you very much ^_^*",,"['analysis', 'multivariable-calculus']"
78,Exact definition of convergence,Exact definition of convergence,,"Let us consider a sequence $x_n$. Now let it converge to a limit $L$. Now which one of the following is the correct definition of convergence? A sequence  $x_n$ is said to be convergent to a limit $L$ if given any integer $n$ there exists a positive real number  $\epsilon$  such that for all $M\gt n$, $|x_M-L|\lt\epsilon$. A sequence $x_n$ is said to be convergent to a limit $L$ if given any real positive number $\epsilon$ there exists an integer $n$ such that for all $M\gt n$, $|x_M-L|\lt\epsilon$. If the two definitions are equivalent then how to prove it?","Let us consider a sequence $x_n$. Now let it converge to a limit $L$. Now which one of the following is the correct definition of convergence? A sequence  $x_n$ is said to be convergent to a limit $L$ if given any integer $n$ there exists a positive real number  $\epsilon$  such that for all $M\gt n$, $|x_M-L|\lt\epsilon$. A sequence $x_n$ is said to be convergent to a limit $L$ if given any real positive number $\epsilon$ there exists an integer $n$ such that for all $M\gt n$, $|x_M-L|\lt\epsilon$. If the two definitions are equivalent then how to prove it?",,['analysis']
79,How to prove the error estimate of the Newton-iteration?,How to prove the error estimate of the Newton-iteration?,,"I'm trying to get familiar with the Newton-iteration over here but I got stuck at the proof of the error estimate. Let $f: [a,b] \rightarrow \mathbb{R}$ be continuously differentiable twice, concave or convex and $f' \neq 0 \;\; \forall x \in [a,b]$. Let $\xi$ be the root of $f$. We define the Newton-iteration for $k \in \mathbb{Z}_{\geq 0}$: $$x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)}$$ Also, we assume $x_1 \in [a,b]$ for $x_0 = a$ and $x_0 = b$. I already showed that the sequence $(x_k)_{k \in \mathbb{N}}$ converges to $\xi$. Now, I want to show the following error estimate: $$|\xi - x_{k+1}| \leq \frac{\max_{a \leq x \leq b} |f''(x)|}{2 \min_{a \leq x \leq b} |f'(x)|} (x_{k+1}-x_k)^2$$ I am quite sure I will have to combine the mean value theorem and Taylor's theorem (and Lagrange's remainder), but I have no idea, how to. I don't quite know at what point I should use Taylor's theorem, also, I don't know between which two points I should apply the mean value theorem. I'd be very happy if somebody could give me a little hint so that I can proceed.","I'm trying to get familiar with the Newton-iteration over here but I got stuck at the proof of the error estimate. Let $f: [a,b] \rightarrow \mathbb{R}$ be continuously differentiable twice, concave or convex and $f' \neq 0 \;\; \forall x \in [a,b]$. Let $\xi$ be the root of $f$. We define the Newton-iteration for $k \in \mathbb{Z}_{\geq 0}$: $$x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)}$$ Also, we assume $x_1 \in [a,b]$ for $x_0 = a$ and $x_0 = b$. I already showed that the sequence $(x_k)_{k \in \mathbb{N}}$ converges to $\xi$. Now, I want to show the following error estimate: $$|\xi - x_{k+1}| \leq \frac{\max_{a \leq x \leq b} |f''(x)|}{2 \min_{a \leq x \leq b} |f'(x)|} (x_{k+1}-x_k)^2$$ I am quite sure I will have to combine the mean value theorem and Taylor's theorem (and Lagrange's remainder), but I have no idea, how to. I don't quite know at what point I should use Taylor's theorem, also, I don't know between which two points I should apply the mean value theorem. I'd be very happy if somebody could give me a little hint so that I can proceed.",,"['analysis', 'numerical-methods']"
80,Stuck at the proof of the existence of the partial fraction expansion,Stuck at the proof of the existence of the partial fraction expansion,,"Let $P$ and $Q$ be complex polynomials with $deg(P) < deg(Q)$. Let $Q(z) = (z-z_1)^{k_1} (z-z_2)^{k_2}...(z-z_m)^{k_m}, z_i \in \mathbb{C} \text{ and } k_i \in \mathbb{N}$ be a complete decomposition. Then, I have to prove that there exists a unique exposition of the following form: $\frac{P(z)}{Q(z)} = \sum\limits_{i=1}^m \sum\limits_{j=1}^{k_i} \frac{a_{ij}}{(z-z_i)^j}, a_{ij} \in \mathbb{C}$ I started with the proof of the uniqueness of this exposition and succeeded. However, I don't feel like I am doing well now, while trying to prove its existence. Basically, I want to do two inductions: First I want to do an induction over $deg(Q)$ with $deg(P) = 0$, then I want to go on with an induction over $deg(P)$ with an arbitrary $deg(Q)$. The start of the first induction is easy, but I can't get to an end at the induction step. I have something like: $\frac{P}{Q} = \frac{P}{(z-z*)Q'} = \frac{1}{z-z*} \frac{P}{Q'}$, where $Q'$ is a polynomial with $deg(Q') = n$. What do I do next? I know that a unique exposition exists for the latter fraction and if $z* = z_i$ for some $i$, it's proven I guess. But what if this $z*$ is a completely new complex number?","Let $P$ and $Q$ be complex polynomials with $deg(P) < deg(Q)$. Let $Q(z) = (z-z_1)^{k_1} (z-z_2)^{k_2}...(z-z_m)^{k_m}, z_i \in \mathbb{C} \text{ and } k_i \in \mathbb{N}$ be a complete decomposition. Then, I have to prove that there exists a unique exposition of the following form: $\frac{P(z)}{Q(z)} = \sum\limits_{i=1}^m \sum\limits_{j=1}^{k_i} \frac{a_{ij}}{(z-z_i)^j}, a_{ij} \in \mathbb{C}$ I started with the proof of the uniqueness of this exposition and succeeded. However, I don't feel like I am doing well now, while trying to prove its existence. Basically, I want to do two inductions: First I want to do an induction over $deg(Q)$ with $deg(P) = 0$, then I want to go on with an induction over $deg(P)$ with an arbitrary $deg(Q)$. The start of the first induction is easy, but I can't get to an end at the induction step. I have something like: $\frac{P}{Q} = \frac{P}{(z-z*)Q'} = \frac{1}{z-z*} \frac{P}{Q'}$, where $Q'$ is a polynomial with $deg(Q') = n$. What do I do next? I know that a unique exposition exists for the latter fraction and if $z* = z_i$ for some $i$, it's proven I guess. But what if this $z*$ is a completely new complex number?",,['analysis']
81,Definition of compact support,Definition of compact support,,"According to http://mathworld.wolfram.com/CompactSupport.html , A function has compact support if it is zero outside of a compact set. Alternatively, one can say that a function has compact support if its support is a compact set. My question is, which is the common definition of compact support, $1$ or $2$ ?","According to http://mathworld.wolfram.com/CompactSupport.html , A function has compact support if it is zero outside of a compact set. Alternatively, one can say that a function has compact support if its support is a compact set. My question is, which is the common definition of compact support, or ?",1 2,['analysis']
82,Constructing reals: Prove $i$ not real,Constructing reals: Prove  not real,i,"So I need to prove, from the definition of reals as Cauchy sequences of rationals, that $i$ is not a real number.  The guidance given is to assume that $a\sim b$ are equivalent Cauchy sequences of rationals such that $\displaystyle \lim_{k\rightarrow \infty} a_{k}b_{k} = -1$.  Apparently I'm to do this by contradiction. What I've done so far is (in outline) to suppose these things, and let $P=\max{\{M,N\}}$ by the larger of the ranks of the two sequences such that both are constrained by any arbitrary $\varepsilon>0$ for all $p>P$.  For abbreviation let $a_{p}=a, b_{p}=b$. I've done a lot of algebraic manipulation on $|ab+1|$ and $|a-b|$ using the fact that both are positive and less than $\varepsilon>0$.  I've also been trying to do this with the constraint that $\varepsilon <1$ since I thought maybe I could show that by combining them in the right ways I could get something bigger than 1.  But so far no luck. The algebra I've tried: $|ab+1|+|a-b|$ $|ab+1|^2 + |a-b|$ $2|a-b|^2 + |ab+1|$ And many, many other permutations on these.  But I haven't found any way to squeeze a contradiction out of any of them.  Any guidance? Thank you!","So I need to prove, from the definition of reals as Cauchy sequences of rationals, that $i$ is not a real number.  The guidance given is to assume that $a\sim b$ are equivalent Cauchy sequences of rationals such that $\displaystyle \lim_{k\rightarrow \infty} a_{k}b_{k} = -1$.  Apparently I'm to do this by contradiction. What I've done so far is (in outline) to suppose these things, and let $P=\max{\{M,N\}}$ by the larger of the ranks of the two sequences such that both are constrained by any arbitrary $\varepsilon>0$ for all $p>P$.  For abbreviation let $a_{p}=a, b_{p}=b$. I've done a lot of algebraic manipulation on $|ab+1|$ and $|a-b|$ using the fact that both are positive and less than $\varepsilon>0$.  I've also been trying to do this with the constraint that $\varepsilon <1$ since I thought maybe I could show that by combining them in the right ways I could get something bigger than 1.  But so far no luck. The algebra I've tried: $|ab+1|+|a-b|$ $|ab+1|^2 + |a-b|$ $2|a-b|^2 + |ab+1|$ And many, many other permutations on these.  But I haven't found any way to squeeze a contradiction out of any of them.  Any guidance? Thank you!",,['analysis']
83,How to find a power series representation for a divergent product?,How to find a power series representation for a divergent product?,,"Euler used the identity $$ \frac{ \sin(x) }{x} = \prod_{n=1}^{\infty} \left(1 - \frac{x^2}{n^2 \pi^2 } \right) = \sum_{n=0}^{\infty} \frac{ (-1)^n }{(2n + 1)! } x^{2n} $$ to solve the Basel problem . The product is obtained by noting that the sine function is 'just' an infinite polynomial, which can be rewritten as the product of its zeroes. The sum is found by writing down the taylor series expansion of the sine function and dividing by $x$. Now, I am interested in finding the sum representation of the following product: $$ \prod_{n=1}^{\infty} \left(1 - \frac{x}{n \pi} \right) ,$$ which is divergent (see this article ). The infinite sum representation of this product is not as easily found (at least not by me) because it does not have an obvious formal representation like $\frac{\sin(x)}{x}$ above. Questions : what is the infinite sum  representation of the second product I mentioned? How does one obtain this sum? And is there any 'formal' represenation for these formulae (like  $\frac{\sin(x)}{x}$ above).","Euler used the identity $$ \frac{ \sin(x) }{x} = \prod_{n=1}^{\infty} \left(1 - \frac{x^2}{n^2 \pi^2 } \right) = \sum_{n=0}^{\infty} \frac{ (-1)^n }{(2n + 1)! } x^{2n} $$ to solve the Basel problem . The product is obtained by noting that the sine function is 'just' an infinite polynomial, which can be rewritten as the product of its zeroes. The sum is found by writing down the taylor series expansion of the sine function and dividing by $x$. Now, I am interested in finding the sum representation of the following product: $$ \prod_{n=1}^{\infty} \left(1 - \frac{x}{n \pi} \right) ,$$ which is divergent (see this article ). The infinite sum representation of this product is not as easily found (at least not by me) because it does not have an obvious formal representation like $\frac{\sin(x)}{x}$ above. Questions : what is the infinite sum  representation of the second product I mentioned? How does one obtain this sum? And is there any 'formal' represenation for these formulae (like  $\frac{\sin(x)}{x}$ above).",,"['analysis', 'divergent-series']"
84,Is the Hausdorff outer measure regular?,Is the Hausdorff outer measure regular?,,"An outer measure $\mu^*$ is said to be regular if for every set $A \subset X$ $$\mu^\ast (A)=\inf\{\mu^*(E) : E\supset A \text{ is } \mu^\ast\text{-measurable} \}$$ To check that an outer measure is regular, we just have to check whether $$\mu^\ast(A)\geq\inf\{\mu^*(E) : E\supset A \text{ is } \mu^\ast\text{-measurable} \}$$ since the other inequality follows from the outer measure axioms. I have to find out whether the $s$ -dimensional Hausdorff outer measure For any $s \geq 0$ and $\delta\gt0$ we define the $\delta$ -approximating $s$ -dimensional Hausdorff outer measure, $$\mathfrak{h}_{s,\delta}^\ast (A)=  \alpha_s \inf\left\{\sum_{i=1}^\infty \text{diam}^s(A_i): A \subset \bigcup_{i=1}^\infty A_i, \text{diam}(A_i)\lt\delta\right\}$$ and the $s$ -dimensional Hausdorff measure, $$\mathfrak{h}_{s}^\ast(A)=\sup_{ \delta\gt0}  \mathfrak{h}_{s,\delta}^\ast(A)= \lim_{\delta \downarrow 0} \mathfrak{h}_{s, \delta}^\ast(A)$$ Here $0\lt\alpha_s\lt\infty$ is chosen so that for $s \in \mathbb{N}$ the $s$ -dimensional Hausdorff measure of the $s$ -dimensional unit cube is one. is regular. Since the Lebesgue outer measure is one of these measures, and it is regular, I'm trying to prove the Hausdorff outer measure is regular. So far I've got to for any $A \subset X $ and $\delta\gt0$ $$\mathfrak{h}_s^\ast(A) \geq \inf\{\mathfrak{h}_{s,\delta}^\ast(E):A \subset E \text{ is } \mathfrak{h}_s^\ast\text{-measurable}\}$$ but I don't know how to show I'm allowed to swap my sup and my inf.  If indeed I am.","An outer measure is said to be regular if for every set To check that an outer measure is regular, we just have to check whether since the other inequality follows from the outer measure axioms. I have to find out whether the -dimensional Hausdorff outer measure For any and we define the -approximating -dimensional Hausdorff outer measure, and the -dimensional Hausdorff measure, Here is chosen so that for the -dimensional Hausdorff measure of the -dimensional unit cube is one. is regular. Since the Lebesgue outer measure is one of these measures, and it is regular, I'm trying to prove the Hausdorff outer measure is regular. So far I've got to for any and but I don't know how to show I'm allowed to swap my sup and my inf.  If indeed I am.","\mu^* A \subset X \mu^\ast (A)=\inf\{\mu^*(E) : E\supset A \text{ is } \mu^\ast\text{-measurable} \} \mu^\ast(A)\geq\inf\{\mu^*(E) : E\supset A \text{ is } \mu^\ast\text{-measurable} \} s s \geq 0 \delta\gt0 \delta s \mathfrak{h}_{s,\delta}^\ast (A)=  \alpha_s \inf\left\{\sum_{i=1}^\infty \text{diam}^s(A_i): A \subset \bigcup_{i=1}^\infty A_i, \text{diam}(A_i)\lt\delta\right\} s \mathfrak{h}_{s}^\ast(A)=\sup_{ \delta\gt0}  \mathfrak{h}_{s,\delta}^\ast(A)= \lim_{\delta \downarrow 0} \mathfrak{h}_{s, \delta}^\ast(A) 0\lt\alpha_s\lt\infty s \in \mathbb{N} s s A \subset X  \delta\gt0 \mathfrak{h}_s^\ast(A) \geq \inf\{\mathfrak{h}_{s,\delta}^\ast(E):A \subset E \text{ is } \mathfrak{h}_s^\ast\text{-measurable}\}","['analysis', 'measure-theory', 'geometric-measure-theory']"
85,A limit of a uniformly convergent sequence of smooth functions,A limit of a uniformly convergent sequence of smooth functions,,"Is it possible to uniformly approximate every continuous function $f: \mathbf{R} \rightarrow \mathbf{R}$ by smooth functions? In other words, is it true that for each continuous function $f: \mathbf{R} \rightarrow \mathbf{R}$ there exists a sequence $(f_n)$ of smooth functions $f_n: \mathbf{R} \rightarrow \mathbf{R}$ such that $f_n \rightarrow f$, as $n\rightarrow \infty$, uniformly on $\mathbf{R}$? Thanks.","Is it possible to uniformly approximate every continuous function $f: \mathbf{R} \rightarrow \mathbf{R}$ by smooth functions? In other words, is it true that for each continuous function $f: \mathbf{R} \rightarrow \mathbf{R}$ there exists a sequence $(f_n)$ of smooth functions $f_n: \mathbf{R} \rightarrow \mathbf{R}$ such that $f_n \rightarrow f$, as $n\rightarrow \infty$, uniformly on $\mathbf{R}$? Thanks.",,['analysis']
86,What do I know when the curl of a vector field equals 0?,What do I know when the curl of a vector field equals 0?,,"I'm currently doing a multiple-choice exercise which includes the following question: Let $F: \mathbb{R}^2 \setminus \{0\} \to \mathbb{R}^2$ be defined by $F(x,y) = \left( - \frac{y}{x^2+y^2}, \frac{x}{x^2+y^2} \right)$. Which of the following statements are true? There exists a closed path $\gamma$ in $\mathbb{R}^2 \setminus \{0\}$ such that   $$\int_\gamma F \cdot \mathrm ds < 0.$$ There exists a $\epsilon \in (0,1)$ such that for all closed $C^1$-paths $\gamma$ with Im $\gamma \subseteq B_\epsilon((1,0))$   $$\int_\gamma F \cdot \mathrm ds = 0. $$ F is a gradient field. Now up to now I thought that whenever the curl of a vector field equals 0, firstly the vector field is a gradient field and secondly the integral around every closed paths equals 0. So this would make the second and the third statement to be correct whilst the first statement obviously would be wrong. However, I don't feel like my reasoning is correct. It would be somewhat too easy, so there has to be some mistake in my train of thought, some point I've been missing. Where did I make a mistake or is it really that easy? Thanks for any answer in advance.","I'm currently doing a multiple-choice exercise which includes the following question: Let $F: \mathbb{R}^2 \setminus \{0\} \to \mathbb{R}^2$ be defined by $F(x,y) = \left( - \frac{y}{x^2+y^2}, \frac{x}{x^2+y^2} \right)$. Which of the following statements are true? There exists a closed path $\gamma$ in $\mathbb{R}^2 \setminus \{0\}$ such that   $$\int_\gamma F \cdot \mathrm ds < 0.$$ There exists a $\epsilon \in (0,1)$ such that for all closed $C^1$-paths $\gamma$ with Im $\gamma \subseteq B_\epsilon((1,0))$   $$\int_\gamma F \cdot \mathrm ds = 0. $$ F is a gradient field. Now up to now I thought that whenever the curl of a vector field equals 0, firstly the vector field is a gradient field and secondly the integral around every closed paths equals 0. So this would make the second and the third statement to be correct whilst the first statement obviously would be wrong. However, I don't feel like my reasoning is correct. It would be somewhat too easy, so there has to be some mistake in my train of thought, some point I've been missing. Where did I make a mistake or is it really that easy? Thanks for any answer in advance.",,['analysis']
87,Why do differential forms and integrands have different transformation behaviours under diffeomorphisms?,Why do differential forms and integrands have different transformation behaviours under diffeomorphisms?,,"Let $f$ be a diffeomorphism, say from $\mathbb R^n$ to $\mathbb R^n$ , such as the transition map between two coordinate charts on a differentiable manifold. A differential $n$-form (or rather its coefficient function which is obtained by using the canonical one-chart atlas on $\mathbb R^n$) then transforms essentially by multiplication with $\mathrm{det}(Df)$, while integrals transform essentially by multiplication of the integrand with $\lvert\mathrm{det}(Df)\rvert$. (This is the reason for the necessity to choose an orientation in order to define the integral of a top form on a differentiable manifold.) Question: What is an intuitive or conceptional reason for these different transformation behaviours of forms and integrands?","Let $f$ be a diffeomorphism, say from $\mathbb R^n$ to $\mathbb R^n$ , such as the transition map between two coordinate charts on a differentiable manifold. A differential $n$-form (or rather its coefficient function which is obtained by using the canonical one-chart atlas on $\mathbb R^n$) then transforms essentially by multiplication with $\mathrm{det}(Df)$, while integrals transform essentially by multiplication of the integrand with $\lvert\mathrm{det}(Df)\rvert$. (This is the reason for the necessity to choose an orientation in order to define the integral of a top form on a differentiable manifold.) Question: What is an intuitive or conceptional reason for these different transformation behaviours of forms and integrands?",,"['analysis', 'manifolds']"
88,Analyzing the P.D.E that describes the displacement of a circular membrane,Analyzing the P.D.E that describes the displacement of a circular membrane,,"The following PDE describes the displacement $u(r,\theta,t)$ of a circular membrane,  $$u_{tt}=c^2\Delta u$$ with B.C. $u(a,\theta,t)=u_r(a,\theta,t)$. I am asked to $(a)$ Show that this membrane only oscillates $(b)$ Obatain an expression that determines the natural frequencies $(c)$ Solve the initial value problem if  $$u(r,\theta,0)=0,\quad u_t(r,\theta,0)=\alpha(r)\sin(3\theta).$$ What I have tried is the following: After some calculation I was able to seperate the pde into  $$h''(t)=-\lambda c^2h(t),\quad g''(\theta)=-\mu g(\theta),\quad\text{and}\quad r^2f''(r)+rf'(r)+(\lambda r^2-\mu)f(r)=0$$ where $u(t)=f(r)g(\theta)h(t)$. Also, since the membrane is circular, we have $g(\pi)=g(-\pi)=g'(\pi)=g'(-\pi)$ and $|f(0)|<\infty$.  From the B.C. given we also have $f(a)=-f'(a).$ To answer $(a)$, I assumed we need to show that the solution to $g''=-\mu g$ is a linear combination of sines and cosines, which is trivially shown by using the conditions $g(\pi)=g(-\pi)=g'(\pi)=g'(-\pi)$. To answer $(b)$, I referred to $h''=-\lambda c^2h$, which with $\lambda>0$, has natural frequencies given by $c\sqrt{\lambda}$. For $(c)$, using the substitution $z=\sqrt{\lambda}r$ and using the fact that $\mu=n^2$ from the second ode, then we encounter Bessel's equation. Since $|f(0)|<\infty$, then the solution is in the form $f(r)=c_1J_n(\sqrt{\lambda}r)$, however, here I don't know how to implement the fact that $f(a)=f'(a)$. Is my work correct so far? A detailed answer to part $(c)$ would be appreciated. Thank you!","The following PDE describes the displacement $u(r,\theta,t)$ of a circular membrane,  $$u_{tt}=c^2\Delta u$$ with B.C. $u(a,\theta,t)=u_r(a,\theta,t)$. I am asked to $(a)$ Show that this membrane only oscillates $(b)$ Obatain an expression that determines the natural frequencies $(c)$ Solve the initial value problem if  $$u(r,\theta,0)=0,\quad u_t(r,\theta,0)=\alpha(r)\sin(3\theta).$$ What I have tried is the following: After some calculation I was able to seperate the pde into  $$h''(t)=-\lambda c^2h(t),\quad g''(\theta)=-\mu g(\theta),\quad\text{and}\quad r^2f''(r)+rf'(r)+(\lambda r^2-\mu)f(r)=0$$ where $u(t)=f(r)g(\theta)h(t)$. Also, since the membrane is circular, we have $g(\pi)=g(-\pi)=g'(\pi)=g'(-\pi)$ and $|f(0)|<\infty$.  From the B.C. given we also have $f(a)=-f'(a).$ To answer $(a)$, I assumed we need to show that the solution to $g''=-\mu g$ is a linear combination of sines and cosines, which is trivially shown by using the conditions $g(\pi)=g(-\pi)=g'(\pi)=g'(-\pi)$. To answer $(b)$, I referred to $h''=-\lambda c^2h$, which with $\lambda>0$, has natural frequencies given by $c\sqrt{\lambda}$. For $(c)$, using the substitution $z=\sqrt{\lambda}r$ and using the fact that $\mu=n^2$ from the second ode, then we encounter Bessel's equation. Since $|f(0)|<\infty$, then the solution is in the form $f(r)=c_1J_n(\sqrt{\lambda}r)$, however, here I don't know how to implement the fact that $f(a)=f'(a)$. Is my work correct so far? A detailed answer to part $(c)$ would be appreciated. Thank you!",,"['analysis', 'partial-differential-equations', 'bessel-functions']"
89,More Questions from Mathematical Analysis by Apostol,More Questions from Mathematical Analysis by Apostol,,"I was solving the exercise questions of the book ""Mathematical Analysis - 2nd Edition"" by Tom Apostol and I came across the questions mentioned below. While I was able to solve a few questions, the others I did not even get any hint of! 1. (a) By equating imaginary parts in DeMoivre's Formula prove that $$\sin {n\theta} = \sin^n\theta \left\lbrace \binom{n}{1} \cot^{n - 1}\theta - \binom{n}{3} \cot^{n - 3}\theta + \binom{n}{5} \cot^{n - 5}\theta - + \cdots \right\rbrace$$ (b) If $0 < \theta < \dfrac{\pi}{2}$, prove that $$\sin{\left( 2m + 1 \right)\theta} = \sin^{2m+1}\theta . P_m\left( \cot^2 \theta \right)$$ where $P_m$ is the polynomial of degree $m$ given by $$P_m(x) = \binom{2m + 1}{1} x^m - \binom{2m + 1}{3} x^{m - 1} + \binom{2m + 1}{5} x^{m - 2} - + \cdots$$ Use this to show that $P_m$ has zeros at $m$ distinct points $x_k = \cot^2 \left( \dfrac{k\pi}{2m + 1} \right)$ for $k = 1, 2, \dots, m$. (c) Show that the sum of zeros of $P_m$ is given by $$\sum\limits_{k = 1}^{m} \cot^2 \dfrac{k\pi}{2m + 1} = \dfrac{m \left( 2m - 1 \right)}{3}$$ and that the sum of there squares is given by $$\sum\limits_{k = 1}^{m} \cot^4 \dfrac{k\pi}{2m + 1} = \dfrac{m\left( 2m - 1 \right) \left( 4m^2 + 10m - 9 \right)}{45}$$ Prove that $z^n - 1 = \prod\limits_{k = 1}^{n} \left( z - e^{\dfrac{2ki\pi}{n}} \right)$ for all complex $z$. Use this to derive the formula $$\prod\limits_{k = 1}^{n - 1} \sin \dfrac{k\pi}{n} = \dfrac{n}{2^{n - 1}}$$ As far as the solutions are concerned, I am through with the 1st part of 1st question and even half of the second part. But, in the second question, proving the zeros and their sum (and the sum of their squares) is getting really difficult. I am not getting any sort of hint as to how to prove it further. And for the second question, I could do the first half part since it was essentially finding the $n$ roots of unity. But for the second part, I have nearly proved everything but what was asked. Many times I came to the conclusion that $$\prod\limits_{k = 1}^{n} \sin \dfrac{k\pi}{n} = 0$$ which is obvious because at $k = n$, we have a term of $\sin \pi$ which is equal to $0$. I am not getting how to remove that last term from the product using the result we just proved above! Help will be appreciated!","I was solving the exercise questions of the book ""Mathematical Analysis - 2nd Edition"" by Tom Apostol and I came across the questions mentioned below. While I was able to solve a few questions, the others I did not even get any hint of! 1. (a) By equating imaginary parts in DeMoivre's Formula prove that $$\sin {n\theta} = \sin^n\theta \left\lbrace \binom{n}{1} \cot^{n - 1}\theta - \binom{n}{3} \cot^{n - 3}\theta + \binom{n}{5} \cot^{n - 5}\theta - + \cdots \right\rbrace$$ (b) If $0 < \theta < \dfrac{\pi}{2}$, prove that $$\sin{\left( 2m + 1 \right)\theta} = \sin^{2m+1}\theta . P_m\left( \cot^2 \theta \right)$$ where $P_m$ is the polynomial of degree $m$ given by $$P_m(x) = \binom{2m + 1}{1} x^m - \binom{2m + 1}{3} x^{m - 1} + \binom{2m + 1}{5} x^{m - 2} - + \cdots$$ Use this to show that $P_m$ has zeros at $m$ distinct points $x_k = \cot^2 \left( \dfrac{k\pi}{2m + 1} \right)$ for $k = 1, 2, \dots, m$. (c) Show that the sum of zeros of $P_m$ is given by $$\sum\limits_{k = 1}^{m} \cot^2 \dfrac{k\pi}{2m + 1} = \dfrac{m \left( 2m - 1 \right)}{3}$$ and that the sum of there squares is given by $$\sum\limits_{k = 1}^{m} \cot^4 \dfrac{k\pi}{2m + 1} = \dfrac{m\left( 2m - 1 \right) \left( 4m^2 + 10m - 9 \right)}{45}$$ Prove that $z^n - 1 = \prod\limits_{k = 1}^{n} \left( z - e^{\dfrac{2ki\pi}{n}} \right)$ for all complex $z$. Use this to derive the formula $$\prod\limits_{k = 1}^{n - 1} \sin \dfrac{k\pi}{n} = \dfrac{n}{2^{n - 1}}$$ As far as the solutions are concerned, I am through with the 1st part of 1st question and even half of the second part. But, in the second question, proving the zeros and their sum (and the sum of their squares) is getting really difficult. I am not getting any sort of hint as to how to prove it further. And for the second question, I could do the first half part since it was essentially finding the $n$ roots of unity. But for the second part, I have nearly proved everything but what was asked. Many times I came to the conclusion that $$\prod\limits_{k = 1}^{n} \sin \dfrac{k\pi}{n} = 0$$ which is obvious because at $k = n$, we have a term of $\sin \pi$ which is equal to $0$. I am not getting how to remove that last term from the product using the result we just proved above! Help will be appreciated!",,"['analysis', 'complex-numbers']"
90,Real vs Complex Interpolation,Real vs Complex Interpolation,,"The two major classical interpolation theorems in analysis are Riesz-Thorin Theorem (complex method) and Marcinkiewicz Theorem (real method). One can see the statements of the theorems and realize the differences between them (sublinearity, weak end-point estimates, and so on). I also know that these theorems can be extended to whole theories. As far as I know, the complex method is called like so because in Riesz-Thorin Theorem it is neccesary that the scalar field is $\mathbb{C}$ (to use complex analysis tools). My questions are: Is there any real difference if the scalar field is $\mathbb{R}$ or $\mathbb{C}$? Isn't the scalar field of all $L^p$ spaces $\mathbb{C}$? (I can also define them over $\mathbb{R}$ but I cannot see the advantage).  Do I need my functions to be real-valued to use Marcinkiewicz? Can I use Riesz-Thorin to real-valued functions? What is the different between both interpolation methods in this aspect? Thank you.","The two major classical interpolation theorems in analysis are Riesz-Thorin Theorem (complex method) and Marcinkiewicz Theorem (real method). One can see the statements of the theorems and realize the differences between them (sublinearity, weak end-point estimates, and so on). I also know that these theorems can be extended to whole theories. As far as I know, the complex method is called like so because in Riesz-Thorin Theorem it is neccesary that the scalar field is $\mathbb{C}$ (to use complex analysis tools). My questions are: Is there any real difference if the scalar field is $\mathbb{R}$ or $\mathbb{C}$? Isn't the scalar field of all $L^p$ spaces $\mathbb{C}$? (I can also define them over $\mathbb{R}$ but I cannot see the advantage).  Do I need my functions to be real-valued to use Marcinkiewicz? Can I use Riesz-Thorin to real-valued functions? What is the different between both interpolation methods in this aspect? Thank you.",,"['analysis', 'harmonic-analysis', 'interpolation', 'interpolation-theory']"
91,Is continuous $f$ constant if every point of $\mathbb{R}$ is local minimum of $f$?,Is continuous  constant if every point of  is local minimum of ?,f \mathbb{R} f,Suppose $f:\mathbb{R} \rightarrow \mathbb{R}$ is continuous. Is $f$ constant if every point of $\mathbb{R}$ is local minimum of $f$? What metric spaces we can use instead of $\mathbb{R}$? I guess we have same result for $f:\mathbb{R}^n \rightarrow \mathbb{R}$.,Suppose $f:\mathbb{R} \rightarrow \mathbb{R}$ is continuous. Is $f$ constant if every point of $\mathbb{R}$ is local minimum of $f$? What metric spaces we can use instead of $\mathbb{R}$? I guess we have same result for $f:\mathbb{R}^n \rightarrow \mathbb{R}$.,,"['analysis', 'continuity', 'problem-solving']"
92,Something that isn't continuous can be proven to be continuous (so it is continuous - definitions - but doesn't look it!),Something that isn't continuous can be proven to be continuous (so it is continuous - definitions - but doesn't look it!),,"I'm sorry to post this, either I am right and it is continuous, or because I am on $\mathbb{Q}$ not $\mathbb{R}$ that saying ""if that delta works, any smaller delta will!"" (which can be proven by by some * value theorem) does not work. Consider this, now I was trying to prove it isn't continuous, I am now convinced it is. $f:\mathbb{Q}\rightarrow\{1,2\}$ (I chose 1 and 2 to make the sketch nicer) given by $f(x)=1$ if $x<\sqrt{2}$ and $f(x)=2$ otherwise/$x>\sqrt{2}$ - it is important that this change happen about a number in $\mathbb{R}$ but not $\mathbb{Q}$. Now I remember when proving the continuity of 1/x over a year ago, I learnt that it is helpful (necessary) to bound delta above when you have points of discontinuity, this stops it getting too close, it also keeps it to one side of the discontinuous point. I seek to prove $\forall\epsilon>0\exists\delta>0:|x-a|<\delta\implies|f(x)-f(a)|<\epsilon$ to mean continuous at $a$. Now that upper bound I mentioned, because this function is flat I don't need to find a smaller delta that's a function of epsilon and take the minimum. The proof is trivial. Let $\delta=|\sqrt{2}-a|$, now if $|x-a|<\delta$ I am saying in words ""The distance from x to a is less than the distance from a to that nasty point"" which means $x\in(a-\delta,a+\delta)$ which is clearly a ... chunk of the domain either entirely before that $\sqrt{2}$ or after. On this $f(x)-f(a)=0$ which is less than $\epsilon$ for all $\epsilon>0$, thus I have proved that this function is continuous. EVEN though it has a jump in it! It is continuous on $\mathbb{Q}$ What I think I have learnt I think I have learnt that while 1 and 2 seem far apart for real numbers, or even fractions, in the set {1,2} there is no middle value. So the jump is not actually a jump at all. With this I am not sure if the $\sqrt{2}$ thing is actually significant. If we had a function that was 1 if $x\le\frac{1}{2}$ say, else 2. does this have the ""no-jump"" quality? I can see a case for no, if $\epsilon<1$  then no it cannot be continuous, because there can be a change near $x=\frac{1}{2}$ by a value of more than one, no matter how small $\delta$ (at x=0.5). HOWEVER it might be yes. If you say ""the change must be -1,0 or 1"" thus confining $\epsilon$ to take 1, it is still no (as 1 is not less than 1) but one could question whether it is fair to try and impose this ""less than"" on {1,2} in this way. If it ever changes there can be no smaller jump. I am reading about the issue, I'm posing this because my foundations have somewhat crumbled, I'd like some help patching them back up. My apologies for the naff format/style of this question, it suffers from me thinking about what I am writing and flicking between several different ways and making sure it is consistent. I've read it twice and it is awful, but I cannot think how else to phrase it.","I'm sorry to post this, either I am right and it is continuous, or because I am on $\mathbb{Q}$ not $\mathbb{R}$ that saying ""if that delta works, any smaller delta will!"" (which can be proven by by some * value theorem) does not work. Consider this, now I was trying to prove it isn't continuous, I am now convinced it is. $f:\mathbb{Q}\rightarrow\{1,2\}$ (I chose 1 and 2 to make the sketch nicer) given by $f(x)=1$ if $x<\sqrt{2}$ and $f(x)=2$ otherwise/$x>\sqrt{2}$ - it is important that this change happen about a number in $\mathbb{R}$ but not $\mathbb{Q}$. Now I remember when proving the continuity of 1/x over a year ago, I learnt that it is helpful (necessary) to bound delta above when you have points of discontinuity, this stops it getting too close, it also keeps it to one side of the discontinuous point. I seek to prove $\forall\epsilon>0\exists\delta>0:|x-a|<\delta\implies|f(x)-f(a)|<\epsilon$ to mean continuous at $a$. Now that upper bound I mentioned, because this function is flat I don't need to find a smaller delta that's a function of epsilon and take the minimum. The proof is trivial. Let $\delta=|\sqrt{2}-a|$, now if $|x-a|<\delta$ I am saying in words ""The distance from x to a is less than the distance from a to that nasty point"" which means $x\in(a-\delta,a+\delta)$ which is clearly a ... chunk of the domain either entirely before that $\sqrt{2}$ or after. On this $f(x)-f(a)=0$ which is less than $\epsilon$ for all $\epsilon>0$, thus I have proved that this function is continuous. EVEN though it has a jump in it! It is continuous on $\mathbb{Q}$ What I think I have learnt I think I have learnt that while 1 and 2 seem far apart for real numbers, or even fractions, in the set {1,2} there is no middle value. So the jump is not actually a jump at all. With this I am not sure if the $\sqrt{2}$ thing is actually significant. If we had a function that was 1 if $x\le\frac{1}{2}$ say, else 2. does this have the ""no-jump"" quality? I can see a case for no, if $\epsilon<1$  then no it cannot be continuous, because there can be a change near $x=\frac{1}{2}$ by a value of more than one, no matter how small $\delta$ (at x=0.5). HOWEVER it might be yes. If you say ""the change must be -1,0 or 1"" thus confining $\epsilon$ to take 1, it is still no (as 1 is not less than 1) but one could question whether it is fair to try and impose this ""less than"" on {1,2} in this way. If it ever changes there can be no smaller jump. I am reading about the issue, I'm posing this because my foundations have somewhat crumbled, I'd like some help patching them back up. My apologies for the naff format/style of this question, it suffers from me thinking about what I am writing and flicking between several different ways and making sure it is consistent. I've read it twice and it is awful, but I cannot think how else to phrase it.",,['analysis']
93,"Min/Max of $f(x,y) = e^{xy}$ where $x^3+y^3=16$",Min/Max of  where,"f(x,y) = e^{xy} x^3+y^3=16","Use Lagrange multipliers to find the maximum and minimum values of the function :$$f(x,y)=e^{xy}$$  constraint $$x^3+y^3=16$$ This is my problem in my workbook. When I solve, I'm just have one solution, so I cannot find other. Here is my solution: $$f(x,y) = e^{xy}$$ $$g(x,y) = x^3+y^3-16$$ $t(x,y) = f(x,y) + \lambda*g(x,y)$ So we will have three equations by Lagrange Multiplier: $$(1) y*e^{xy} + 3*\lambda*x^2 = 0$$ $$(2) x*e^{xy} + 3*\lambda*y^2 = 0$$ $$(3)x^3+y^3 = 16$$ If $x=0$ or $y=0$ $==>$ $y=0$ or $x=0$ --> false if $t=0$ $==>$ $x=y=0$ ---> false So, $x$,$y$ and $t$ cannot equal to 0. So, we have from (1) (2) and (3): $$\frac{e^{xy}}{-3*\lambda} = \frac{x^2}{y}$$ $$\frac{e^{xy}}{-3*\lambda} = \frac{y^2}{x}$$ $$==> x= y $$ ==>$$ x = y = 2 $$ That is my solution. I just have one no, so I cannot find both min and max. Maybe something wrong with my solution. Please helps me. Thanks :)","Use Lagrange multipliers to find the maximum and minimum values of the function :$$f(x,y)=e^{xy}$$  constraint $$x^3+y^3=16$$ This is my problem in my workbook. When I solve, I'm just have one solution, so I cannot find other. Here is my solution: $$f(x,y) = e^{xy}$$ $$g(x,y) = x^3+y^3-16$$ $t(x,y) = f(x,y) + \lambda*g(x,y)$ So we will have three equations by Lagrange Multiplier: $$(1) y*e^{xy} + 3*\lambda*x^2 = 0$$ $$(2) x*e^{xy} + 3*\lambda*y^2 = 0$$ $$(3)x^3+y^3 = 16$$ If $x=0$ or $y=0$ $==>$ $y=0$ or $x=0$ --> false if $t=0$ $==>$ $x=y=0$ ---> false So, $x$,$y$ and $t$ cannot equal to 0. So, we have from (1) (2) and (3): $$\frac{e^{xy}}{-3*\lambda} = \frac{x^2}{y}$$ $$\frac{e^{xy}}{-3*\lambda} = \frac{y^2}{x}$$ $$==> x= y $$ ==>$$ x = y = 2 $$ That is my solution. I just have one no, so I cannot find both min and max. Maybe something wrong with my solution. Please helps me. Thanks :)",,"['analysis', 'inequality']"
94,Hardy's inequality again,Hardy's inequality again,,"How can I prove that the constant in classical Hardy's inequality is optimal? $$\int_0^{\infty}\left(\frac{1}{x}\int_0^xf(s)ds\right)^p dx\leq \left(\frac{p}{p-1}\right)^p\int_0^{\infty}(f(x))^pdx,$$ where $f\geq0$ and $f\in L^p(0,\infty)$. This inequality fails for $p=1$ and $p=\infty$ ?","How can I prove that the constant in classical Hardy's inequality is optimal? $$\int_0^{\infty}\left(\frac{1}{x}\int_0^xf(s)ds\right)^p dx\leq \left(\frac{p}{p-1}\right)^p\int_0^{\infty}(f(x))^pdx,$$ where $f\geq0$ and $f\in L^p(0,\infty)$. This inequality fails for $p=1$ and $p=\infty$ ?",,"['analysis', 'inequality']"
95,Showing that a real function is convex,Showing that a real function is convex,,"How can I show that if $\varphi$ is a real function such that $$\varphi \left(\int_0^1 f\right)\leqslant \int_0^1 \varphi (f)$$  for any Borel-measurable real function $f$, then $\varphi$ is convex. Ps: I realize homework questions have to be tagged as such. This isn't a homework problem. I came across this question in a text book, and I thought it was interesting.","How can I show that if $\varphi$ is a real function such that $$\varphi \left(\int_0^1 f\right)\leqslant \int_0^1 \varphi (f)$$  for any Borel-measurable real function $f$, then $\varphi$ is convex. Ps: I realize homework questions have to be tagged as such. This isn't a homework problem. I came across this question in a text book, and I thought it was interesting.",,['analysis']
96,"$-\Delta u=\lambda u$ in $\Omega$, $u=0$ in some ball, then $u\equiv 0$","in ,  in some ball, then",-\Delta u=\lambda u \Omega u=0 u\equiv 0,"Let $-\Delta u=\lambda u$ in $\Omega\subset\mathbb{R}^n$ , $\lambda>0$ . Suppose $u=0$ in a ball $B\subset\Omega$ . I want to prove that $u\equiv0$ in $\Omega$ . If $\lambda\leq 0$ , integrating by part solves this problem. But if $\lambda>0$ , things become more subtle. Besides, I think $\Omega$ must be connected. (am I right?) Any hints will be appreciated a lot!","Let in , . Suppose in a ball . I want to prove that in . If , integrating by part solves this problem. But if , things become more subtle. Besides, I think must be connected. (am I right?) Any hints will be appreciated a lot!",-\Delta u=\lambda u \Omega\subset\mathbb{R}^n \lambda>0 u=0 B\subset\Omega u\equiv0 \Omega \lambda\leq 0 \lambda>0 \Omega,"['analysis', 'partial-differential-equations']"
97,About the sequence $a_n=\{\pi^n\}$,About the sequence,a_n=\{\pi^n\},"Is the sequence $\{\pi^n\}=\pi^n-\lfloor\pi^n\rfloor$ dense? In other words for any given $\varepsilon>0$ and $t\in[0,1]$ is there a proper $n\in\mathbb{N}$ satisfying $|\{\pi^n\}-t|<\varepsilon$ ? (*) What is the condition on $q$ to make the sequence $\{q^n\}$ dense? I know the necessary and sufficient condition for $\{nq\}$ is $q\not\in\mathbb{Q}$. Also, to make the question * nicer, extend it for all (positive and negative) integers.","Is the sequence $\{\pi^n\}=\pi^n-\lfloor\pi^n\rfloor$ dense? In other words for any given $\varepsilon>0$ and $t\in[0,1]$ is there a proper $n\in\mathbb{N}$ satisfying $|\{\pi^n\}-t|<\varepsilon$ ? (*) What is the condition on $q$ to make the sequence $\{q^n\}$ dense? I know the necessary and sufficient condition for $\{nq\}$ is $q\not\in\mathbb{Q}$. Also, to make the question * nicer, extend it for all (positive and negative) integers.",,['analysis']
98,Prove Lagrange's Identity without induction,Prove Lagrange's Identity without induction,,"Prove Lagrange's Identity without induction. $$ \sum_{1\leq j <k\leq n}(a_jb_k-a_kb_j)^2=\left( \sum_{k=1}^na_k^2 \right)\left( \sum_{k=1}^n b_k^2 \right)-\left( \sum_{k=1}^na_kb_k \right)^2 $$ I tried expanding the left side but I could never get anywhere, I'm looking for some tips on how to get started on the right direction, not complete solutions . Thanks!","Prove Lagrange's Identity without induction. $$ \sum_{1\leq j <k\leq n}(a_jb_k-a_kb_j)^2=\left( \sum_{k=1}^na_k^2 \right)\left( \sum_{k=1}^n b_k^2 \right)-\left( \sum_{k=1}^na_kb_k \right)^2 $$ I tried expanding the left side but I could never get anywhere, I'm looking for some tips on how to get started on the right direction, not complete solutions . Thanks!",,"['analysis', 'summation', 'real-numbers']"
99,Why is boundedness of the ball multiplier equivalent to the convergence of Fourier transform in Lp?,Why is boundedness of the ball multiplier equivalent to the convergence of Fourier transform in Lp?,,"Let $\mathcal{F}$ be the fourier transform operator and let $T_R$ =  $\mathcal{F}^* \chi_R\mathcal{F}$ where $\chi_R$ is the indicator function on the ball of radius $R$. Hence $T_R$ is the fourier multiplier operator with the indicator function on ball of radius R. I am interested in understanding the question for what $p$ with $1\leq p\leq \infty$ does $T_R(f) \to f$ in $L^p$ as $R\to \infty$ for all $f\in L^p(\mathbb{R}^n)$ ? I am only interested in $n\geq2$. Now first I am a little confused about how to make the question precise. Now $T_R(f) = f*\hat{\chi_R}$ and by stationary phase arguments I know that the exact asymptotics of $\hat{\chi_R}(\xi)$ is like $|\xi|^{-(n+1)/2}$ as $\xi \to \infty$ and hence $\hat{\chi_R} \in L^p(\mathbb{R}^n)$ iff $p> \frac{2n}{n+1}$. Hence the problem definitely doesn't make sense (i.e. there is trivial schwartz function for which the statement fails) for $1\leq p \leq \frac{2n}{n+1}$. By using Young's inequality, we know that $T_R(f)$ is in $L^1_{loc}$ for $ f \in L^p$ for the range $ \frac{2n}{n+1}<p < \frac{2n}{n-1}$ hence the problem is potentially meaningful/interesting in this range. But what about $p \geq \frac{2n}{n-1}$? Is there a clear counterexample or an argument which rules out this range of $p$? I am not even sure whether $T_R(f)$ is in $L^1_{loc}$ for $p$ in this range. In a lot of places I have read that the original problem is equivalent to the problem of boundedness of the operator $T_R$ from $L^p \to L^p$ (Stein mensions this in his book Harmonic Analysis page 389). I understand that if $T_R$ is bounded, then the original problem is solved for that $p$ by a simple density argument. However I do not see why we really need to have boundedness of the operators $T_R$. Grafakos in his book Modern Fourier Analysis ""proves"" that we really need boundedness of $T_R$ in the exercise 10.2.1 page 366 and the proof is via uniform boundedness principle. However I do not see how we can apply uniform boundedness principle as the principle only applies if we apriori know that the individual operators are bounded. For example consider an infinite dimensional Banach space $X$. By axiom of choice, let us chose a hamel basis and collect an countable infinite among them $x_1, x_2,..$. We define the unbounded operators $T_n:X \to X$ by defining them on the chosen basis vectors and extending linearly. Define $T_n(x_i) = i*x_i$ for $i\geq n$ and zero for $i<n$, and zero on all other basis vectors. We clearly see that $T_n$ are unbounded operators, for every $f\in X, sup\|T_n(f)\| < \infty$ and $T_n(f) \to 0$ for every $f\in X$. So to summarize I have 2 questions: 1) Whether the question $T_R(f) \to f$ in $L^p$ for all $f\in L^p$ makes sense (or does not makes sense) for $p \geq \frac{2n}{n-1}$ 2) How does $T_R(f) \to f$ in $L^p$ for all $f\in L^p$ imply the boundedness of the opertors $T_R$ I am sorry for the long post. Any help would be appreciated!","Let $\mathcal{F}$ be the fourier transform operator and let $T_R$ =  $\mathcal{F}^* \chi_R\mathcal{F}$ where $\chi_R$ is the indicator function on the ball of radius $R$. Hence $T_R$ is the fourier multiplier operator with the indicator function on ball of radius R. I am interested in understanding the question for what $p$ with $1\leq p\leq \infty$ does $T_R(f) \to f$ in $L^p$ as $R\to \infty$ for all $f\in L^p(\mathbb{R}^n)$ ? I am only interested in $n\geq2$. Now first I am a little confused about how to make the question precise. Now $T_R(f) = f*\hat{\chi_R}$ and by stationary phase arguments I know that the exact asymptotics of $\hat{\chi_R}(\xi)$ is like $|\xi|^{-(n+1)/2}$ as $\xi \to \infty$ and hence $\hat{\chi_R} \in L^p(\mathbb{R}^n)$ iff $p> \frac{2n}{n+1}$. Hence the problem definitely doesn't make sense (i.e. there is trivial schwartz function for which the statement fails) for $1\leq p \leq \frac{2n}{n+1}$. By using Young's inequality, we know that $T_R(f)$ is in $L^1_{loc}$ for $ f \in L^p$ for the range $ \frac{2n}{n+1}<p < \frac{2n}{n-1}$ hence the problem is potentially meaningful/interesting in this range. But what about $p \geq \frac{2n}{n-1}$? Is there a clear counterexample or an argument which rules out this range of $p$? I am not even sure whether $T_R(f)$ is in $L^1_{loc}$ for $p$ in this range. In a lot of places I have read that the original problem is equivalent to the problem of boundedness of the operator $T_R$ from $L^p \to L^p$ (Stein mensions this in his book Harmonic Analysis page 389). I understand that if $T_R$ is bounded, then the original problem is solved for that $p$ by a simple density argument. However I do not see why we really need to have boundedness of the operators $T_R$. Grafakos in his book Modern Fourier Analysis ""proves"" that we really need boundedness of $T_R$ in the exercise 10.2.1 page 366 and the proof is via uniform boundedness principle. However I do not see how we can apply uniform boundedness principle as the principle only applies if we apriori know that the individual operators are bounded. For example consider an infinite dimensional Banach space $X$. By axiom of choice, let us chose a hamel basis and collect an countable infinite among them $x_1, x_2,..$. We define the unbounded operators $T_n:X \to X$ by defining them on the chosen basis vectors and extending linearly. Define $T_n(x_i) = i*x_i$ for $i\geq n$ and zero for $i<n$, and zero on all other basis vectors. We clearly see that $T_n$ are unbounded operators, for every $f\in X, sup\|T_n(f)\| < \infty$ and $T_n(f) \to 0$ for every $f\in X$. So to summarize I have 2 questions: 1) Whether the question $T_R(f) \to f$ in $L^p$ for all $f\in L^p$ makes sense (or does not makes sense) for $p \geq \frac{2n}{n-1}$ 2) How does $T_R(f) \to f$ in $L^p$ for all $f\in L^p$ imply the boundedness of the opertors $T_R$ I am sorry for the long post. Any help would be appreciated!",,"['analysis', 'fourier-analysis', 'harmonic-analysis']"
