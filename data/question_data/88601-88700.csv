,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Inequality for Weak $L^p$ spaces,Inequality for Weak  spaces,L^p,"For each measurable function $f: X \to \mathbf{R}$ , define it's distribution function $F: [0,\infty] \to [0,\infty]$ by $F(t) = |{x : |f(x)| > t}|$ . Then, for each $p \geq 1$ , define the weak $L^p$ norm $\| f \|_{p,\infty}$ as the smallest value $A$ such that for each $t \in [0,\infty]$ , $$ F(t) \leq A^p / t^p. $$ It is easy to see that $\| f \|_{p,\infty}$ is a quasinorm, i.e. that $$ \| f_1 + \dots + f_N \|_{p,\infty} \leq N( \| f_1 \|_{p,\infty} + \dots + \| f_N \|_{p,\infty}). $$ However, for $p > 1$ , $\| \cdot \|_{p,\infty}$ is comparable to a norm. In particular, this implies that we should be able to obtain an inequality of the form $$ \| f_1 + \dots + f_N \|_{p,\infty} \lesssim_p \| f_1 \|_{p,\infty} + \dots + \| f_N \|_{p,\infty}, $$ independantly of $N$ . Is there an elementary proof of the fact that $$ \| f_1 + \dots + f_N \|_{p,\infty} \lesssim_p \| f_1 \|_{p,\infty} + \dots + \| f_N \|_{p,\infty}? $$","For each measurable function , define it's distribution function by . Then, for each , define the weak norm as the smallest value such that for each , It is easy to see that is a quasinorm, i.e. that However, for , is comparable to a norm. In particular, this implies that we should be able to obtain an inequality of the form independantly of . Is there an elementary proof of the fact that","f: X \to \mathbf{R} F: [0,\infty] \to [0,\infty] F(t) = |{x : |f(x)| > t}| p \geq 1 L^p \| f \|_{p,\infty} A t \in [0,\infty]  F(t) \leq A^p / t^p.  \| f \|_{p,\infty}  \| f_1 + \dots + f_N \|_{p,\infty} \leq N( \| f_1 \|_{p,\infty} + \dots + \| f_N \|_{p,\infty}).  p > 1 \| \cdot \|_{p,\infty}  \| f_1 + \dots + f_N \|_{p,\infty} \lesssim_p \| f_1 \|_{p,\infty} + \dots + \| f_N \|_{p,\infty},  N  \| f_1 + \dots + f_N \|_{p,\infty} \lesssim_p \| f_1 \|_{p,\infty} + \dots + \| f_N \|_{p,\infty}? ","['functional-analysis', 'inequality', 'harmonic-analysis']"
1,Strong and Weak Continuity in $L^p$ Spaces,Strong and Weak Continuity in  Spaces,L^p,"Set-Up Let $\Omega \subseteq \mathbb{R}^n$ be a domain, $T>0$ . We find $u_m(x,t) : \Omega \times [0,T] \rightarrow \mathbb{R}^n$ which converge weakly to some $u \in L^{\infty}(0,T; L_{\sigma}^2)$ . Here, $L_{\sigma}^2 = \{ f \in L(\Omega)^n \ | \ \text{div}f = 0 \}$ We know that $||u_m||$ is uniform bounded, and $u_m(\cdot , t) := u(t)$ is continuous with respect to $t$ for all $m$ . We also have that $u$ is weakly continuous in $L_{\sigma}^2$ with respect to $t$ . That is, $(u(t) , f) : \mathbb{R} \rightarrow \mathbb{R}$ is continuous, for all $f \in L^{2}_{\sigma}$ , where $(\cdot , \cdot)$ is the inner-product on $L^2$ . (See this previous question for a full explanation). Claim Our claim is that the inner-product $(u(t),u_m(t))$ is also continuous with respect to $t$ , for all $m \in \mathbb{N}$ . That is, the integral $\int_{\Omega} u(t)u_m(t) \text{d}x$ is continuous with respect to $t$ . I believe that the above claim should hold, by the weak continuity of $u(t)$ , and strong continuity of $u_m(t)$ , but I cannot find an obvious way to show this... Does anyone know even how I might start? Thank you.","Set-Up Let be a domain, . We find which converge weakly to some . Here, We know that is uniform bounded, and is continuous with respect to for all . We also have that is weakly continuous in with respect to . That is, is continuous, for all , where is the inner-product on . (See this previous question for a full explanation). Claim Our claim is that the inner-product is also continuous with respect to , for all . That is, the integral is continuous with respect to . I believe that the above claim should hold, by the weak continuity of , and strong continuity of , but I cannot find an obvious way to show this... Does anyone know even how I might start? Thank you.","\Omega \subseteq \mathbb{R}^n T>0 u_m(x,t) : \Omega \times [0,T] \rightarrow \mathbb{R}^n u \in L^{\infty}(0,T; L_{\sigma}^2) L_{\sigma}^2 = \{ f \in L(\Omega)^n \ | \ \text{div}f = 0 \} ||u_m|| u_m(\cdot , t) := u(t) t m u L_{\sigma}^2 t (u(t) , f) : \mathbb{R} \rightarrow \mathbb{R} f \in L^{2}_{\sigma} (\cdot , \cdot) L^2 (u(t),u_m(t)) t m \in \mathbb{N} \int_{\Omega} u(t)u_m(t) \text{d}x t u(t) u_m(t)","['real-analysis', 'integration', 'functional-analysis', 'lp-spaces', 'inner-products']"
2,Domain of definition of a hamiltonian with delta(contact) potential,Domain of definition of a hamiltonian with delta(contact) potential,,"I am having a hard time making sense of the so-called ""delta function potential well"" in quantum theory. The Hamiltonian operator is defined as (with $\mathscr D_H\subset \mathscr H=L^2(\mathbb R)$ ) $$H:\mathscr D_H\rightarrow \mathscr H$$ $$\psi\mapsto H\psi$$ And $$(H\psi)(x):=-\frac{d^2}{dx^2}\psi(x)-\lambda\delta(x)\psi(x).$$ My job is to find the spectrum of this operator given a $\lambda>0$ . My problems are: How do I construct $\mathscr D_H$ ? What definition of the ""delta function"" is suitable for this kind of job?","I am having a hard time making sense of the so-called ""delta function potential well"" in quantum theory. The Hamiltonian operator is defined as (with ) And My job is to find the spectrum of this operator given a . My problems are: How do I construct ? What definition of the ""delta function"" is suitable for this kind of job?",\mathscr D_H\subset \mathscr H=L^2(\mathbb R) H:\mathscr D_H\rightarrow \mathscr H \psi\mapsto H\psi (H\psi)(x):=-\frac{d^2}{dx^2}\psi(x)-\lambda\delta(x)\psi(x). \lambda>0 \mathscr D_H,"['functional-analysis', 'hilbert-spaces', 'quantum-mechanics']"
3,Decay of Sobolev functions in one dimension,Decay of Sobolev functions in one dimension,,"Suppose $u\in H^3(\mathbb{R})$ .  I know that since we are in one dimensional setting, $u$ is continuous (i.e., has continuous representative).  What can we say about the decay of $u$ and its derivatives against ""x"", namely can we say anything about limits such as $\displaystyle\lim_{|x|\rightarrow\infty}xu(x)$ or $\displaystyle\lim_{|x|\rightarrow\infty}x|u'(x)|^2?$","Suppose .  I know that since we are in one dimensional setting, is continuous (i.e., has continuous representative).  What can we say about the decay of and its derivatives against ""x"", namely can we say anything about limits such as or",u\in H^3(\mathbb{R}) u u \displaystyle\lim_{|x|\rightarrow\infty}xu(x) \displaystyle\lim_{|x|\rightarrow\infty}x|u'(x)|^2?,"['functional-analysis', 'sobolev-spaces']"
4,Characterization of weak compactness in a Banach space,Characterization of weak compactness in a Banach space,,"While studying the textbook of Fernando Albiac and Nigel J. Kalton (Topics in Banach Space Theory), I came across the following result: A subset $A$ of a Banach space $X$ is relatively weakly compact if and only if it is norm-bounded and the $\sigma(X^{**},X^*)$ -closure of $A$ in $X^{**}$ is contained in $A$ . I am relatively unexperienced with weak topologies, so I would appreciate help in comprehending how that result might be proven. I am especially interested in the ""if""-direction. A first idea to prove the ""if"" direction is, to apply the Banach-Alaoglu theorem in $X^{**}$ and use that the canonical map $i:X\to i(X) $ given by $i(x):=(X^*\ni f\mapsto f(x))$ is actually a homeomorphism with respect to $(X,\sigma(X,X^*))$ and $(X^{**},\sigma(X^{**},X^*))$ . If that might work, I would appreciate getting ideas on how to prove the $(X,\sigma(X,X^*))$ , $(X^{**},\sigma(X^{**},X^*))$ - continuity of $i$ .","While studying the textbook of Fernando Albiac and Nigel J. Kalton (Topics in Banach Space Theory), I came across the following result: A subset of a Banach space is relatively weakly compact if and only if it is norm-bounded and the -closure of in is contained in . I am relatively unexperienced with weak topologies, so I would appreciate help in comprehending how that result might be proven. I am especially interested in the ""if""-direction. A first idea to prove the ""if"" direction is, to apply the Banach-Alaoglu theorem in and use that the canonical map given by is actually a homeomorphism with respect to and . If that might work, I would appreciate getting ideas on how to prove the , - continuity of .","A X \sigma(X^{**},X^*) A X^{**} A X^{**} i:X\to i(X)  i(x):=(X^*\ni f\mapsto f(x)) (X,\sigma(X,X^*)) (X^{**},\sigma(X^{**},X^*)) (X,\sigma(X,X^*)) (X^{**},\sigma(X^{**},X^*)) i","['functional-analysis', 'compactness', 'weak-topology']"
5,"Is the closed, bounded and convex subset version of Shauder-Tychonoff Fixed Point Theorem really in the literature?","Is the closed, bounded and convex subset version of Shauder-Tychonoff Fixed Point Theorem really in the literature?",,"In 1930, J. Schauder extended Brouwer's work to arbitrary Banach spaces by stating the theorem; Schauder Fixed Point Theorem: Let $K\subset E$ be a compact convex set where $ E $ is Banach and $ T :\,K\longrightarrow K $ a continuous map. Then, $ T $ has a fixed point. However, there is another fixed point theorem called Shauder-Tychonov Fixed Point Theorem, it states like this; Shauder-Tychonoff Fixed Point Theorem: Let $E$ be a Banach space and $K\subset E$ be a non-empty closed, bounded and convex set. Suppose that $T:K\longrightarrow K$ is completely continuous, then there exists $x^*\in K\;\text{such that} \;Tx^*=x^*.$ I have tried all my best to get this paper. I don't know which of the authors actually wrote the theorem. Several papers I found are misleading. Question: Can anyone please, direct me to a link on the exact paper where Shauder-Tychonoff Fixed Point Theorem was derived? I need it for my literature review. Thanks.","In 1930, J. Schauder extended Brouwer's work to arbitrary Banach spaces by stating the theorem; Schauder Fixed Point Theorem: Let be a compact convex set where is Banach and a continuous map. Then, has a fixed point. However, there is another fixed point theorem called Shauder-Tychonov Fixed Point Theorem, it states like this; Shauder-Tychonoff Fixed Point Theorem: Let be a Banach space and be a non-empty closed, bounded and convex set. Suppose that is completely continuous, then there exists I have tried all my best to get this paper. I don't know which of the authors actually wrote the theorem. Several papers I found are misleading. Question: Can anyone please, direct me to a link on the exact paper where Shauder-Tychonoff Fixed Point Theorem was derived? I need it for my literature review. Thanks.","K\subset E  E   T :\,K\longrightarrow K   T  E K\subset E T:K\longrightarrow K x^*\in K\;\text{such that} \;Tx^*=x^*.","['functional-analysis', 'reference-request']"
6,Point spectrum of an integral operator,Point spectrum of an integral operator,,"Let we have $$Tu(x) = \cfrac{1}{x}\int_0^x u(y)dy$$ so that $u \in L^2(0,1)$ . How can I show that $(0,2) \subset \sigma_p(T)$ and $T$ is not compact?",Let we have so that . How can I show that and is not compact?,"Tu(x) = \cfrac{1}{x}\int_0^x u(y)dy u \in L^2(0,1) (0,2) \subset \sigma_p(T) T","['functional-analysis', 'spectral-theory', 'integral-operators']"
7,Applying functional calculus to the bounded operator $(T \pm iI)^{-1} $,Applying functional calculus to the bounded operator,(T \pm iI)^{-1} ,"This is the context: What I wish to prove: 2.3 page 16. Let $T$ be an essentially self-adjoint operator on a Hilbert space $H$ . (Now we take its closure) There is a unique homomoprhism of $C^*$ algebras from the algebra of continuous, bounded functions on $\Bbb R$ into the algebra of bounded operators on $H$ which maps the functions $(x \pm i)^{-1}$ to the operators $(T \pm iI)^{-1}$ . I have proven that $T$ satisfying the condition implies $(T\pm iI)^{-1}$ is a bounded operator normal operator. The proof given in the text is as follows. The spectral theorem is proven by observing $(T\pm iI)^{-1}$ generate a commutative $C^*$ algebra of operators. By Gelfand Naimark theorem, every commutative $C^*$ -algebra is isomoprhic to $C_0(X)$ for some locally compact space $X$ . ** In this case $X$ may be identified with a closed subset of $\Bbb R$ (the spectrum of $T$ ) in such a way that the operators $(T\pm i I)^{-1}$ correspond to the functions $(x \pm iI)^{-1}$ . I am fine until $**$ . I don't see how the identification works, especially when we are applying to $(T+ I)^{-1}$ , so the by GN we should have isomoprhism with $C(\sigma( (T + iI)^{-1})$ .","This is the context: What I wish to prove: 2.3 page 16. Let be an essentially self-adjoint operator on a Hilbert space . (Now we take its closure) There is a unique homomoprhism of algebras from the algebra of continuous, bounded functions on into the algebra of bounded operators on which maps the functions to the operators . I have proven that satisfying the condition implies is a bounded operator normal operator. The proof given in the text is as follows. The spectral theorem is proven by observing generate a commutative algebra of operators. By Gelfand Naimark theorem, every commutative -algebra is isomoprhic to for some locally compact space . ** In this case may be identified with a closed subset of (the spectrum of ) in such a way that the operators correspond to the functions . I am fine until . I don't see how the identification works, especially when we are applying to , so the by GN we should have isomoprhism with .",T H C^* \Bbb R H (x \pm i)^{-1} (T \pm iI)^{-1} T (T\pm iI)^{-1} (T\pm iI)^{-1} C^* C^* C_0(X) X X \Bbb R T (T\pm i I)^{-1} (x \pm iI)^{-1} ** (T+ I)^{-1} C(\sigma( (T + iI)^{-1}),"['functional-analysis', 'operator-theory', 'operator-algebras', 'functional-calculus']"
8,Any isometry in $\mathcal{L}(G)$ must be a unitary,Any isometry in  must be a unitary,\mathcal{L}(G),"Let $G$ be a countable group with neutral element $e$ . Consider the Hilbert space $$\ell^2(G):=\left \{ x:G\to \mathbb{C}\mid \sum_{t\in G}|x(t)|^2<\infty \right \}$$ with inner product $\left \langle x,y \right \rangle=\sum_{t\in G}x(t)\overline{y(t)}$ for $x,y\in \ell^2(G)$ . For each $t\in G$ , let $\delta_t\in \ell^2(G)$ be defined by $\delta_t(t)=1$ and $\delta_t(s)=0$ if $s\neq t$ . The set $(\delta_t)_{t\in G}$ is an orthonormal basis for $\ell^2(G)$ , and $x(t)=\left \langle x,\delta_t \right \rangle$ for $x\in \ell^2(G)$ and $t\in G$ . For each $t\in G$ , consider the operator $U_t$ on $\ell^2(G)$ given by $(U_tx)(s)=x(t^{-1}s)$ for $x\in \ell^2(G)$ and $s\in G$ . Put $\mathcal{L}(G)=\{ U_t\mid t\in G\}''$ . Consider the state $\tau$ on $\mathcal{L}(G)$ defined by $\tau(T)=\left \langle T\delta_e,\delta_e \right \rangle$ for $T\in \mathcal{L}(G)$ . Problems 1) Show that $\tau(ST)=\tau(TS)$ for all for all $S,T\in \mathcal{L}(G)$ . 3) Show that any isometry in $\mathcal{L}(G)$ must be a unitary My answers : 1) I am not sure about this part. If $S,T\in \mathcal{L}(G)$ , then we can write $S=\sum_{s\in G}\alpha_sU_s$ and $T=\sum_{t\in G}\beta_tU_t$ . Can I write the multiplication $$ ST=\sum_{(s,t)\in S\times T}\gamma_{s,t}U_sU_t $$ where $\gamma_{s,t}$ 's are complex numbers in terms of $\alpha$ and $\beta$ ? If not, how would you write it mathematically? Assume that this is true. I have shown that $U_{s}U_{t}=U_{st}$ and $U_t\delta_s=\delta_{ts}$ . Then we have $$ \tau(ST)=\sum_{(s,t)\in S\times T}\gamma_{s,t}\tau(U_{st})=\sum_{(s,t)\in S\times T}\gamma_{s,t}\left \langle U_{st}\delta_e,\delta_e \right \rangle=\sum_{(s,t)\in S\times T}\gamma_{s,t}\left \langle \delta_{st},\delta_e \right \rangle=\sum_{(s,t)\in S\times T}\gamma_{s,t} \delta_{st}(e)=\sum_{(s,t)\in S\times T}\gamma_{s,t} \delta_{ts}(e)=\dots=\tau(TS) $$ Is this correct? I am not sure about the last problem. Could you help me with it? The problem 2) was as follows: Show that $\tau(T^* T)=0$ implies $T=0$ for all $T\in \mathcal{L}(G)$ . I have solved this one. Just informing it if it is relevant for the last problem. Is some informations are missing, please let me know.","Let be a countable group with neutral element . Consider the Hilbert space with inner product for . For each , let be defined by and if . The set is an orthonormal basis for , and for and . For each , consider the operator on given by for and . Put . Consider the state on defined by for . Problems 1) Show that for all for all . 3) Show that any isometry in must be a unitary My answers : 1) I am not sure about this part. If , then we can write and . Can I write the multiplication where 's are complex numbers in terms of and ? If not, how would you write it mathematically? Assume that this is true. I have shown that and . Then we have Is this correct? I am not sure about the last problem. Could you help me with it? The problem 2) was as follows: Show that implies for all . I have solved this one. Just informing it if it is relevant for the last problem. Is some informations are missing, please let me know.","G e \ell^2(G):=\left \{ x:G\to \mathbb{C}\mid \sum_{t\in G}|x(t)|^2<\infty \right \} \left \langle x,y \right \rangle=\sum_{t\in G}x(t)\overline{y(t)} x,y\in \ell^2(G) t\in G \delta_t\in \ell^2(G) \delta_t(t)=1 \delta_t(s)=0 s\neq t (\delta_t)_{t\in G} \ell^2(G) x(t)=\left \langle x,\delta_t \right \rangle x\in \ell^2(G) t\in G t\in G U_t \ell^2(G) (U_tx)(s)=x(t^{-1}s) x\in \ell^2(G) s\in G \mathcal{L}(G)=\{ U_t\mid t\in G\}'' \tau \mathcal{L}(G) \tau(T)=\left \langle T\delta_e,\delta_e \right \rangle T\in \mathcal{L}(G) \tau(ST)=\tau(TS) S,T\in \mathcal{L}(G) \mathcal{L}(G) S,T\in \mathcal{L}(G) S=\sum_{s\in G}\alpha_sU_s T=\sum_{t\in G}\beta_tU_t 
ST=\sum_{(s,t)\in S\times T}\gamma_{s,t}U_sU_t
 \gamma_{s,t} \alpha \beta U_{s}U_{t}=U_{st} U_t\delta_s=\delta_{ts} 
\tau(ST)=\sum_{(s,t)\in S\times T}\gamma_{s,t}\tau(U_{st})=\sum_{(s,t)\in S\times T}\gamma_{s,t}\left \langle U_{st}\delta_e,\delta_e \right \rangle=\sum_{(s,t)\in S\times T}\gamma_{s,t}\left \langle \delta_{st},\delta_e \right \rangle=\sum_{(s,t)\in S\times T}\gamma_{s,t} \delta_{st}(e)=\sum_{(s,t)\in S\times T}\gamma_{s,t} \delta_{ts}(e)=\dots=\tau(TS)
 \tau(T^* T)=0 T=0 T\in \mathcal{L}(G)","['functional-analysis', 'operator-algebras', 'c-star-algebras']"
9,"Uniqueness in $C([0,T])$ of solution found by Picard-Lindelof Theorem",Uniqueness in  of solution found by Picard-Lindelof Theorem,"C([0,T])","Full statement of the question: Suppose $\alpha \in \mathbb{R}$ , also $f: \mathbb{R} \rightarrow \mathbb{R}, f \in C^{1}(\mathbb{R}), and f(0) = 0$ Consider the following ODE: $\partial_{t} u(t) = f(u(t)), \ \text{for} \ 0 < t < T $ $u(0) = \alpha $ where $u:[0,T] \rightarrow \mathbb{R}$ is unknown, and the integral equation: $u(t) = \alpha + \int^{t}_{0} f(u(s)) \text{d}s, \ 0<t<T $ Prove that there exists $T>0$ such that the above integral equation has a unique solution $u \in C([0, T])$ . Furthermore, show that $u$ satisfies $u \in C^{1}([0, T])$ and the above ODE. My Question: I have shown just about every point of this question, by using the Picard-Lindelof theorem on the following set: $X = \{ u \in C([0,T]) : u(0) = \alpha , || u - \alpha || \leq K\}$ , where $K \in \mathbb{R}_{> 0} , \ \alpha \in \mathbb{R}$ and the norm is $|| u || = ^{\text{sup}}_{t \in (0,T)} |u(t)|$ . However, while this proves there is a $T>0$ such that a solution unique in $X$ exists, I cannot figure how to show such a solution is unique in all of $C([0,T])$ . Can someone please inform me how I might show this?","Full statement of the question: Suppose , also Consider the following ODE: where is unknown, and the integral equation: Prove that there exists such that the above integral equation has a unique solution . Furthermore, show that satisfies and the above ODE. My Question: I have shown just about every point of this question, by using the Picard-Lindelof theorem on the following set: , where and the norm is . However, while this proves there is a such that a solution unique in exists, I cannot figure how to show such a solution is unique in all of . Can someone please inform me how I might show this?","\alpha \in \mathbb{R} f: \mathbb{R} \rightarrow \mathbb{R}, f \in C^{1}(\mathbb{R}), and f(0) = 0 \partial_{t} u(t) = f(u(t)), \ \text{for} \ 0 < t < T  u(0) = \alpha  u:[0,T] \rightarrow \mathbb{R} u(t) = \alpha + \int^{t}_{0} f(u(s)) \text{d}s, \ 0<t<T  T>0 u \in C([0, T]) u u \in C^{1}([0, T]) X = \{ u \in C([0,T]) : u(0) = \alpha , || u - \alpha || \leq K\} K \in \mathbb{R}_{> 0} , \ \alpha \in \mathbb{R} || u || = ^{\text{sup}}_{t \in (0,T)} |u(t)| T>0 X C([0,T])","['real-analysis', 'functional-analysis', 'ordinary-differential-equations']"
10,"Proving that $\|Av\|\geq \lvert \langle u, v\rangle\rvert\cdot \|A\|$ for $\|Au\| = \|A\|$ in a Hilbert space",Proving that  for  in a Hilbert space,"\|Av\|\geq \lvert \langle u, v\rangle\rvert\cdot \|A\| \|Au\| = \|A\|","I've shown that for a matrix $A\in \mathbb{R}^{n\times n}$ and an arbitrary $v\in \mathbb{R}^n$ , we have the inequality $$\|Av\|\geq \lvert \langle u, v\rangle\rvert\cdot \|A\|$$ where $\|\cdot\|$ is the $2$ -norm and $u$ satisfies $\|u\| = 1$ and $\|Au\| = \|A\|$ (the operator norm of $A$ corresponding to the vector $2$ -norm). To do this, I used the singular value decomposition. However, I was wondering if there's a different proof that relies on more general properties of Hilbert spaces (under the assumption that $u$ exists in the more general setting where $\langle \cdot, \cdot\rangle$ and $\|\cdot\|$ are the Hilbert space inner product and norm). My own short proof is below: We first handle the case where $A = \operatorname{diag}(d_1, \ldots, d_n)$ for some $d_1\geq \cdots\geq d_n\geq 0$ . Then, $u = e_1$ , so $$\lvert \langle u, v\rangle\rvert^2\|A\|^2 = d_1^2v_1^2\leq \sum_{i=1}^n d_i^2v_i^2 = \|Av\|^2$$ Now, we consider general $A$ , which has a singular value decomposition $A = U\Sigma V^{\mathrm{T}}$ for $U$ and $V$ orthogonal and $\Sigma$ diagonal in the above form. First, we recall that multiplication by orthogonal matrices preserves the dot product, i.e. $\langle Qx, Qy\rangle = \langle x, y\rangle$ for orthogonal $Q$ . This allows us to write $$\|Av\|^2 = \langle U\Sigma V^{\mathrm{T}}v, U\Sigma V^{\mathrm{T}}v\rangle = \langle \Sigma V^{\mathrm{T}}v, \Sigma V^{\mathrm{T}}v\rangle = \|\Sigma V^{\mathrm{T}}v\|^2$$ for arbitrary $v\in \mathbb{R}$ . Furthermore, as $\|Au\|^2 = \|\Sigma V^{\mathrm{T}}u\|^2$ and $\|V^{\mathrm{T}}u\|^2 = \|u\|^2 = 1$ , we have that $\|A\| = \|\Sigma\|$ and that $y = V^{\mathrm{T}}u$ satisfies $\|\Sigma y\| = \|\Sigma\|$ . Combining our earlier results, $$\|Av\|^2 = \|\Sigma V^{\mathrm{T}}v\|^2\geq \lvert\langle y, V^{\mathrm{T}}v\rangle\rvert^2\|\Sigma\|^2 = \lvert\langle V^{\mathrm{T}}u, V^{\mathrm{T}}v\rangle\rvert^2\|A\|^2 = \lvert\langle u, v\rangle\rvert^2\|A\|^2$$","I've shown that for a matrix and an arbitrary , we have the inequality where is the -norm and satisfies and (the operator norm of corresponding to the vector -norm). To do this, I used the singular value decomposition. However, I was wondering if there's a different proof that relies on more general properties of Hilbert spaces (under the assumption that exists in the more general setting where and are the Hilbert space inner product and norm). My own short proof is below: We first handle the case where for some . Then, , so Now, we consider general , which has a singular value decomposition for and orthogonal and diagonal in the above form. First, we recall that multiplication by orthogonal matrices preserves the dot product, i.e. for orthogonal . This allows us to write for arbitrary . Furthermore, as and , we have that and that satisfies . Combining our earlier results,","A\in \mathbb{R}^{n\times n} v\in \mathbb{R}^n \|Av\|\geq \lvert \langle u, v\rangle\rvert\cdot \|A\| \|\cdot\| 2 u \|u\| = 1 \|Au\| = \|A\| A 2 u \langle \cdot, \cdot\rangle \|\cdot\| A = \operatorname{diag}(d_1, \ldots, d_n) d_1\geq \cdots\geq d_n\geq 0 u = e_1 \lvert \langle u, v\rangle\rvert^2\|A\|^2 = d_1^2v_1^2\leq \sum_{i=1}^n d_i^2v_i^2 = \|Av\|^2 A A = U\Sigma V^{\mathrm{T}} U V \Sigma \langle Qx, Qy\rangle = \langle x, y\rangle Q \|Av\|^2 = \langle U\Sigma V^{\mathrm{T}}v, U\Sigma V^{\mathrm{T}}v\rangle = \langle \Sigma V^{\mathrm{T}}v, \Sigma V^{\mathrm{T}}v\rangle = \|\Sigma V^{\mathrm{T}}v\|^2 v\in \mathbb{R} \|Au\|^2 = \|\Sigma V^{\mathrm{T}}u\|^2 \|V^{\mathrm{T}}u\|^2 = \|u\|^2 = 1 \|A\| = \|\Sigma\| y = V^{\mathrm{T}}u \|\Sigma y\| = \|\Sigma\| \|Av\|^2 = \|\Sigma V^{\mathrm{T}}v\|^2\geq \lvert\langle y, V^{\mathrm{T}}v\rangle\rvert^2\|\Sigma\|^2 = \lvert\langle V^{\mathrm{T}}u, V^{\mathrm{T}}v\rangle\rvert^2\|A\|^2 = \lvert\langle u, v\rangle\rvert^2\|A\|^2","['linear-algebra', 'functional-analysis', 'hilbert-spaces']"
11,Show that the following is bounded and surjective (Folland exercise 5.36) [duplicate],Show that the following is bounded and surjective (Folland exercise 5.36) [duplicate],,"This question already has answers here : Folland, ""Real Analysis"", Chapter 5.3, Exercise 36. (3 answers) Closed 5 years ago . Let $X$ be a separable Banach space and let $\mu$ be the counting measure on $\mathbb{N}$ . If $\{x_n\}_{n=1}^{\infty}$ is a countable dense subset of the unit ball of $X$ , and $T:L^1(\mu)\to X$ is defined by $Tf = \sum_{n=1}^{\infty}f(n)x_n$ , then $T$ is bounded and surjective. This came up as an exercise in a book I'm reading and it seems simple enough on the outside but I can't seem to reason why it is true. Does anyone have an easy way to see why this is true?","This question already has answers here : Folland, ""Real Analysis"", Chapter 5.3, Exercise 36. (3 answers) Closed 5 years ago . Let be a separable Banach space and let be the counting measure on . If is a countable dense subset of the unit ball of , and is defined by , then is bounded and surjective. This came up as an exercise in a book I'm reading and it seems simple enough on the outside but I can't seem to reason why it is true. Does anyone have an easy way to see why this is true?",X \mu \mathbb{N} \{x_n\}_{n=1}^{\infty} X T:L^1(\mu)\to X Tf = \sum_{n=1}^{\infty}f(n)x_n T,"['functional-analysis', 'banach-spaces']"
12,Trying to understand a standard example of weakly but not strongly measurable function,Trying to understand a standard example of weakly but not strongly measurable function,,"I consider quite a standard example of a function that is weakly measurable but is not strongly measurable. Unfortunatelly, I don't fully understand it. Let $X=l_2([0,1])$ . Then $X$ equipped with a standard inner product is a nonseparable Hilbert space. Since every Hilbert space has an orthonormal basis, let $\{e_t\,:\, t\in[0,1]\}$ denote an orthonormal basis of $X$ . Define a map $f:[0,1]\to X$ by $$f(t):=e_t$$ for all $t\in [0,1]$ . Assume the the measure $\mu$ we consider is the Lebesgue measure on $[0,1]$ . First, I'd like to prove that $f$ is weakly- $\mu$ measurable. Pick $x^*\in X^*$ . Then, invoking Riesz Representation Theorem we deduce that  there is unique $x\in X$ such that $$\langle x^*, f(t)\rangle_{X^*\times X}=\langle x, f(t) \rangle_{X}.$$ Hence, in our situation, we obtain that $$\langle x^*, f(t)\rangle_{X^*\times X}=\langle x, f(t) \rangle_{X}=\langle x, e_t \rangle_{X},$$ for all $t\in [0,1]$ . How to deduce that a function $g(t)=\langle x^*, f(t)\rangle_{X^*\times X}=0$ $\mu$ a.e. on $[0,1]$ ??? The only that comes to my mind is that $x=\sum_{t\in[0,1]}\langle x, e_t\rangle e_t$ , so $$\langle x^*, f(t)\rangle_{X^*\times X}=\langle x, f(t) \rangle_{X}=\langle x, e_t \rangle_{X}=\langle \sum_{t\in[0,1]}\langle x, e_t\rangle e_t, e_t \rangle_{X}=\sum_{t\in[0,1]}\langle x, e_t\rangle.$$ Anyway, having that $\langle x^*, f(\cdot)\rangle$ is $0$ $\mu$ a.e. will give its measurablity and in consequance the weak measurablity of $f$ . How to prove that $f$ is not strongly measurable? I try arguing by contradiction. Assume that $f$ is strongle measurble. Then, by Pettis's theorem, we deduce that there exists a subset $E\subset[0,1]$ such that $\mu(E)=0$ and $f([0,1]\setminus E)$ is (norm) separable. If $\mu(E)= 0$ , we deduce that $E$ is at most countable and so $[0,1]\setminus E$ is uncountable. Hot to obtain a contradiction now?","I consider quite a standard example of a function that is weakly measurable but is not strongly measurable. Unfortunatelly, I don't fully understand it. Let . Then equipped with a standard inner product is a nonseparable Hilbert space. Since every Hilbert space has an orthonormal basis, let denote an orthonormal basis of . Define a map by for all . Assume the the measure we consider is the Lebesgue measure on . First, I'd like to prove that is weakly- measurable. Pick . Then, invoking Riesz Representation Theorem we deduce that  there is unique such that Hence, in our situation, we obtain that for all . How to deduce that a function a.e. on ??? The only that comes to my mind is that , so Anyway, having that is a.e. will give its measurablity and in consequance the weak measurablity of . How to prove that is not strongly measurable? I try arguing by contradiction. Assume that is strongle measurble. Then, by Pettis's theorem, we deduce that there exists a subset such that and is (norm) separable. If , we deduce that is at most countable and so is uncountable. Hot to obtain a contradiction now?","X=l_2([0,1]) X \{e_t\,:\, t\in[0,1]\} X f:[0,1]\to X f(t):=e_t t\in [0,1] \mu [0,1] f \mu x^*\in X^* x\in X \langle x^*, f(t)\rangle_{X^*\times X}=\langle x, f(t) \rangle_{X}. \langle x^*, f(t)\rangle_{X^*\times X}=\langle x, f(t) \rangle_{X}=\langle x, e_t \rangle_{X}, t\in [0,1] g(t)=\langle x^*, f(t)\rangle_{X^*\times X}=0 \mu [0,1] x=\sum_{t\in[0,1]}\langle x, e_t\rangle e_t \langle x^*, f(t)\rangle_{X^*\times X}=\langle x, f(t) \rangle_{X}=\langle x, e_t \rangle_{X}=\langle \sum_{t\in[0,1]}\langle x, e_t\rangle e_t, e_t \rangle_{X}=\sum_{t\in[0,1]}\langle x, e_t\rangle. \langle x^*, f(\cdot)\rangle 0 \mu f f f E\subset[0,1] \mu(E)=0 f([0,1]\setminus E) \mu(E)= 0 E [0,1]\setminus E","['real-analysis', 'integration', 'functional-analysis', 'measure-theory', 'hilbert-spaces']"
13,Does integral of function against all test function derivative imply function is equal to 0?,Does integral of function against all test function derivative imply function is equal to 0?,,"Howdy so I know that if $\int fv = 0 $ for all test functions $v$ then $f=0$ If you have however $\int fv_x = 0 $ for all test functions $v$ then does it mean that $f=0$ ? I guess my thought is that the if you take the test function space and differentiate all elements, then its the same space again - right? I just wanted someones opinion on this.","Howdy so I know that if for all test functions then If you have however for all test functions then does it mean that ? I guess my thought is that the if you take the test function space and differentiate all elements, then its the same space again - right? I just wanted someones opinion on this.",\int fv = 0  v f=0 \int fv_x = 0  v f=0,"['functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
14,"Find $f\in L^1(\mathbb{R})$ such that $\|\int_{-N}^N \hat{f}(\xi)e^{2\pi i\xi \cdot}\operatorname{d}\xi-f\|_1\nrightarrow 0, N\to\infty?$",Find  such that,"f\in L^1(\mathbb{R}) \|\int_{-N}^N \hat{f}(\xi)e^{2\pi i\xi \cdot}\operatorname{d}\xi-f\|_1\nrightarrow 0, N\to\infty?","I'm wondering how to find a function $f\in L^1(\mathbb{R})$ such that, if $\hat{f}$ is its Fourier transform, i.e.: $$\forall \xi\in\mathbb{R},\hat f (\xi):=\int_\mathbb{R} f(t)e^{-2\pi i\xi t}\operatorname{d}t$$ then $$\left\|\int_{-N}^N \hat{f}(\xi)e^{2\pi i\xi \cdot}\operatorname{d}\xi-f\right\|_1=\int_{\mathbb{R}}\left|\int_{-N}^N \hat{f}(\xi)e^{2\pi i\xi x}\operatorname{d}\xi-f(x)\right|\operatorname{d}x\nrightarrow 0, N\to\infty$$ I know that such a function must exist since in the book Fourier Analysis by Javier Duoandikoetxea on page 59 it is stated without proof that for $f\in L^1(\mathbb{R})$ , in general we can't expect that $\int_{-N}^N \hat{f}(\xi)e^{2\pi i\xi \cdot}\operatorname{d}\xi$ converges in $L^1$ norm to $f$ but that anyway it converges to $f$ in measure. Can anyone provide any reference or just a proof that guarantees the existence of such a function?","I'm wondering how to find a function such that, if is its Fourier transform, i.e.: then I know that such a function must exist since in the book Fourier Analysis by Javier Duoandikoetxea on page 59 it is stated without proof that for , in general we can't expect that converges in norm to but that anyway it converges to in measure. Can anyone provide any reference or just a proof that guarantees the existence of such a function?","f\in L^1(\mathbb{R}) \hat{f} \forall \xi\in\mathbb{R},\hat f (\xi):=\int_\mathbb{R} f(t)e^{-2\pi i\xi t}\operatorname{d}t \left\|\int_{-N}^N \hat{f}(\xi)e^{2\pi i\xi \cdot}\operatorname{d}\xi-f\right\|_1=\int_{\mathbb{R}}\left|\int_{-N}^N \hat{f}(\xi)e^{2\pi i\xi x}\operatorname{d}\xi-f(x)\right|\operatorname{d}x\nrightarrow 0, N\to\infty f\in L^1(\mathbb{R}) \int_{-N}^N \hat{f}(\xi)e^{2\pi i\xi \cdot}\operatorname{d}\xi L^1 f f","['functional-analysis', 'fourier-transform']"
15,Confusion about real separable normed space problem,Confusion about real separable normed space problem,,"Suppose that $X$ is a real separable vector space, and $W$ a closed linear   subspace of $X$ . Show that there exists a sequence $(z_j )\in X$ such that $z_{j+1} \notin W_j := \mathrm{span} \ W \cup \{z_1, . . . , z_j\}$ and if we define $W_\infty = \mathrm{span} \ W \cup \{z_j\}_{j=1}^\infty$ then $\overline{W_\infty} = X$ . This is a problem in a (non-credit) example sheet I am working on currently. After spending some time trying it I have realised that it is probably incorrect as it stands, but I am not sure. I think that (even with the necessary assumption that $W$ is a proper subspace), $W=\mathbb{C}$ (over $\mathbb{R}$ ) is a counterexample? Maybe there is some implicit assumption in the problem that I am missing. I would appreciate if someone could clarify what the correct version of the problem would be. Please do not give me answers to the problem though, as I would like to first try it myself.","Suppose that is a real separable vector space, and a closed linear   subspace of . Show that there exists a sequence such that and if we define then . This is a problem in a (non-credit) example sheet I am working on currently. After spending some time trying it I have realised that it is probably incorrect as it stands, but I am not sure. I think that (even with the necessary assumption that is a proper subspace), (over ) is a counterexample? Maybe there is some implicit assumption in the problem that I am missing. I would appreciate if someone could clarify what the correct version of the problem would be. Please do not give me answers to the problem though, as I would like to first try it myself.","X W X (z_j )\in X z_{j+1} \notin W_j := \mathrm{span} \ W \cup \{z_1, . . . , z_j\} W_\infty = \mathrm{span} \ W \cup \{z_j\}_{j=1}^\infty \overline{W_\infty} = X W W=\mathbb{C} \mathbb{R}","['functional-analysis', 'normed-spaces', 'separable-spaces']"
16,Question on Banach-Alaoglu theorem: Bounded subset of a set contained in the dual space,Question on Banach-Alaoglu theorem: Bounded subset of a set contained in the dual space,,"So the Banach-Alaoglu theorem states: Let $X$ be the dual space to some Banach separable space $Z$ , i.e $X=Z^*$ . Take $M$ a bounded subset of $X$ . Then any sequence in $M$ has a weak-* convergent subsequence. My question is: If we know that $Y$ is contained in the dual space of some Banach separable space $Z$ , then if $M$ is a bounded subset of $Y$ , is there still a weak-* convergent subsequence in $Y$ ? The reason I ask is: Let $\Gamma$ be a 2-dimensional compact manifold. I know that the Lebesgue-Bochner space $L^1(0,T,C_c(\Gamma))$ is Banach and separable. It is also known that the dual space of $C_c(\Gamma)$ is the space of Radon measures on $\Gamma$ with finite mass, denoted by $\mathcal M(\Gamma)$ . However since $\mathcal M(\Gamma)$ is not separable we can not claim that the dual space of $L^1(0,T, C_c(\Gamma))$ is $L^{\infty}(0,T,\mathcal M(\Gamma))$ . But we do know that $L^1(0,T, C_c(\Gamma))^*$ contains always $L^{\infty}(0,T,\mathcal M(\Gamma))$ . I have a sequence of functions bounded in $L^{\infty}(0,T,L^1(\Gamma))$ . So I wonder if $Y=L^{\infty}(0,T,\mathcal M(\Gamma))$ , $Z=L^1(0,T, C_c(\Gamma))$ and $M=L^{\infty}(0,T,L^1(\Gamma))$ , can I deduce a weak-* convergent subsequence then? It seems true to me but after a lot hours of studying, I would like if somebody else could verify this. Moreover, if this is not valid, then what can we say in this case? Is there any other theorem more suitable for that case that I miss at the moment? Any help is much appreciated. Thanks in advance!","So the Banach-Alaoglu theorem states: Let be the dual space to some Banach separable space , i.e . Take a bounded subset of . Then any sequence in has a weak-* convergent subsequence. My question is: If we know that is contained in the dual space of some Banach separable space , then if is a bounded subset of , is there still a weak-* convergent subsequence in ? The reason I ask is: Let be a 2-dimensional compact manifold. I know that the Lebesgue-Bochner space is Banach and separable. It is also known that the dual space of is the space of Radon measures on with finite mass, denoted by . However since is not separable we can not claim that the dual space of is . But we do know that contains always . I have a sequence of functions bounded in . So I wonder if , and , can I deduce a weak-* convergent subsequence then? It seems true to me but after a lot hours of studying, I would like if somebody else could verify this. Moreover, if this is not valid, then what can we say in this case? Is there any other theorem more suitable for that case that I miss at the moment? Any help is much appreciated. Thanks in advance!","X Z X=Z^* M X M Y Z M Y Y \Gamma L^1(0,T,C_c(\Gamma)) C_c(\Gamma) \Gamma \mathcal M(\Gamma) \mathcal M(\Gamma) L^1(0,T, C_c(\Gamma)) L^{\infty}(0,T,\mathcal M(\Gamma)) L^1(0,T, C_c(\Gamma))^* L^{\infty}(0,T,\mathcal M(\Gamma)) L^{\infty}(0,T,L^1(\Gamma)) Y=L^{\infty}(0,T,\mathcal M(\Gamma)) Z=L^1(0,T, C_c(\Gamma)) M=L^{\infty}(0,T,L^1(\Gamma))","['functional-analysis', 'convergence-divergence', 'weak-convergence', 'duality-theorems', 'dual-spaces']"
17,Compact embedding of the domain and compact inverse,Compact embedding of the domain and compact inverse,,"I have several problems in showing this point of a problem: we consider $X$ Banach space and $T: D(T) \to X$ a closed operator with domain $D(T) \subseteq X$ . Let be $T$ bounded, invertible and suppose the embedding $(D(T),\|\cdot \|_T) \to (X,\|\cdot\|_X)$ is compact. I have to show that $T^{-1}$ is compact. Firstly I consider $\|\cdot \|_T$ as the graph norm. Then I started thinking that an unbounded operator $T$ with domain $D(T)$ is bounded, invertible if there is a map $T^{-1}$ with image $D(T)$ and $TT^{-1}x = x$ for every $x \in X$ and $T^{-1}Tu = u$ for every $u \in D(T)$ . But I don't have any idea how to proceed. Could someone help me to show the compactness?","I have several problems in showing this point of a problem: we consider Banach space and a closed operator with domain . Let be bounded, invertible and suppose the embedding is compact. I have to show that is compact. Firstly I consider as the graph norm. Then I started thinking that an unbounded operator with domain is bounded, invertible if there is a map with image and for every and for every . But I don't have any idea how to proceed. Could someone help me to show the compactness?","X T: D(T) \to X D(T) \subseteq X T (D(T),\|\cdot \|_T) \to (X,\|\cdot\|_X) T^{-1} \|\cdot \|_T T D(T) T^{-1} D(T) TT^{-1}x = x x \in X T^{-1}Tu = u u \in D(T)","['functional-analysis', 'inverse', 'compact-operators']"
18,Understanding compact extensions and almost-periodic functions,Understanding compact extensions and almost-periodic functions,,"This question comes from my attempt to understand theorem $7.21$ in E-W . This concerns the dichotomy between relatively weak-mixing extensions and compact extensions. I cannot understand the proof as I do not understand these concepts. I will sketch my understanding of the proof until the part I'm certain I don't understand. All systems are taken to be ergodic, invertible, and Borel. Definition 1: An extension $\psi:(X,\mathscr{B},\mu,T) \to (Y,\mathscr{A},\nu,S)$ is said to be relatively weak-mixing if the system $(X\times X,\mathscr{B}\otimes\mathscr{B},\bar{\mu},T\times T)$ is ergodic. Here $\bar{\mu}$ is the relatively independent join over $Y$ (See E-W definition $6.15$ ). Definition 2: Again, let $\psi:(X,\mathscr{B},\mu,T) \to (Y,\mathscr{A},\nu,S)$ be an extension. Let $\mu^{\psi^{-1}\mathscr{A}}_x$ be the conditional measures on $(X,\mathscr{B})$ . A function $f\in L^2(X,\mathscr{B},\mu)$ is almost-periodic $(AP)$ with respect to $Y$ if given $\varepsilon>0$ , there exist $g_1,\dots,g_r\in L^2(X,\mathscr{B},\mu)$ such that $\forall n \in \mathbb{Z}$ , $\nu$ -a.e $y$ , $$\min_{i}\|U_T^nf-g_i\|_{L^2(\mu_y^{\mathscr{A}})}<\varepsilon.$$ Here $U_Tf:=f\circ T$ and $\mu_y^{\mathscr{A}}$ are the measures on $(X,\mathscr{B})$ obtained by completing the conditional measure diagram: $\require{AMScd}$ \begin{CD} X @>\mu^{\psi^{-1}\mathscr{A}}_.>> M\left(\bar{X}\right)\\ @V{\psi}VV  @| \\ Y @>\mu^{\mathscr{A}}_.>> M\left(\bar{X}\right) \end{CD} Definition 3: The extension $\psi:(X,\mathscr{B},\mu,T) \to (Y,\mathscr{A},\nu,S)$ is said to be a compact extension if the set of $AP$ (with respect to $Y$ ) functions in $L^2(X,\mathscr{B},\mu)$ is dense. Theorem 7.21: If the extension $\psi:(X,\mathscr{B},\mu,T) \to (Y,\mathscr{A},\nu,S)$ is not relatively weak-mixing, there exists a non-trivial intermediate factor $X\to X^* \to Y$ with the property that $X^* \to Y$ is a compact extension. Sketch of proof: (For details see E-W page $200$ onwards) By the hypothesis, there exists a non-constant $H \in \mathscr{L}^\infty(X\times X,\bar{\mu})$ invariant under $T\times T$ ( $\mathscr{L}$ denotes the fact that $H$ is bounded and not just essentially-bounded). For $\phi \in L^2(X,\mathscr{B},\mu)$ , define $$H*\phi(x) = \int H(x,x')\phi(x')d\mu^{\psi^{-1}\mathscr{A}}_x(x').$$ One can show this formula defines a bounded operator $L^2(X,\mathscr{B},\mu)\to L^2(X,\mathscr{B},\mu).$ And, a priori, this formula defines a compact operator $L^2(X,\mathscr{B},\mu^{\psi^{-1}\mathscr{A}}_x) \to L^2(X,\mathscr{B},\mu^{\psi^{-1}\mathscr{A}}_x)$ . Using this and the additional fact that $U_T(H*\phi)=H*(U_T\phi)$ , one shows that $\lbrace H*\phi : \phi \in \mathscr{L}^{\infty}(X,\mathscr{B},\mu)\rbrace$ is an $AP$ (wrt. $Y$ ) family in $L^{\infty}(X,\mathscr{B},\mu)$ . Moreover, one can show that this family contains functions which are not $\psi^{-1}\mathscr{A}$ -measureable. Then if we define $\mathscr{F}=\lbrace f\in L^{\infty}(X,\mathscr{B},\mu): f \text{ is } AP \text{ wrt. } Y\rbrace $ and $\mathscr{B}^*$ to be the smallest $\sigma$ -algebra making the functions in $\mathscr{F}$ measurable, one can show that $\mathscr{F} \subset L^2(X,\mathscr{B}^*,\mu)$ is dense. Here's the part I don't understand: It is then claimed that the $T$ -invariant $\sigma$ -algebra $\mathscr{B}^* (\subset \mathscr{B}$ ) gives rise to the intermediate, non-trivial, compact extension. I assume that this is done by completing the following system of extensions from $M\left(\bar{X}\right)$ to $Y$ : \begin{CD} X @>\psi>> Y\\ @V\mu^{\mathscr{B}^*}_.VV   \\  M\left(\bar{X}\right)  \end{CD} The $T$ -invariance of $\mathscr{B}^*$ shows that $\mu^{\mathscr{B}^*}_.$ gives an extension. And I guess the density of $\mathscr{F}$ will be used to show $M\left( \bar{X}\right)$ is a compact extension of $Y$ . Question: To complete the above diagram, don't we need to show that $\psi^{-1}\mathscr{A}\subset \mathscr{B}^*$ ? Is this obvious? It is not explicitly addressed in the text. Thanks for reading, all help is appreciated. Edit: As John Griesmer points out, $\psi^{-1}\mathscr{A}\subset\mathscr{B}^*$ would follow if one could show every characteristic function $1_{\psi^{-1}A}$ on $X$ was AP with respect to Y. I don't quite see why such a statement should be true though. Take the function $1_{A}\circ\psi$ . I want to find functions $g_i\in L^2(X,\mathscr{B},\mu)$ and estimate $$\int|1_{A}(\psi(T^nx))-g_i(x)|^2d\mu_{x'}^{\psi^{-1}\mathscr{A}}(x) = \int|1_{A}(S^n(\psi x))-g_i(x)|^2d\mu_{x'}^{\psi^{-1}\mathscr{A}}(x)$$ as $x'$ varies in $X$ . Now if the $g_i$ 's happened to be functions on $Y$ , I could rewrite the last integral as $$\int|1_{A}(S^ny)-g_i(y)|^2d\nu_{\psi x'}^{\mathscr{A}}(x) = E\left(|U_S^n1_A-g_i|^2:\mathscr{A},\nu\right)(\psi(x')) = |U^n_S1_A - g_i|^2(\psi(x'))$$ by the properties of the conditional expectation operator. So I guess, I'm left trying to play around with the equation $$\min_i|U_s^n1_A(y) - g_i(y)|^2 < \varepsilon, $$ and hope that it holds for almost every $y$ and for all $n$ . Am I on the right track? what would be sensible choices for the $g_i$ ? Apologies if I'm missing something obvious; I only understand these things formally and have no intuition for what's actually going on.","This question comes from my attempt to understand theorem in E-W . This concerns the dichotomy between relatively weak-mixing extensions and compact extensions. I cannot understand the proof as I do not understand these concepts. I will sketch my understanding of the proof until the part I'm certain I don't understand. All systems are taken to be ergodic, invertible, and Borel. Definition 1: An extension is said to be relatively weak-mixing if the system is ergodic. Here is the relatively independent join over (See E-W definition ). Definition 2: Again, let be an extension. Let be the conditional measures on . A function is almost-periodic with respect to if given , there exist such that , -a.e , Here and are the measures on obtained by completing the conditional measure diagram: Definition 3: The extension is said to be a compact extension if the set of (with respect to ) functions in is dense. Theorem 7.21: If the extension is not relatively weak-mixing, there exists a non-trivial intermediate factor with the property that is a compact extension. Sketch of proof: (For details see E-W page onwards) By the hypothesis, there exists a non-constant invariant under ( denotes the fact that is bounded and not just essentially-bounded). For , define One can show this formula defines a bounded operator And, a priori, this formula defines a compact operator . Using this and the additional fact that , one shows that is an (wrt. ) family in . Moreover, one can show that this family contains functions which are not -measureable. Then if we define and to be the smallest -algebra making the functions in measurable, one can show that is dense. Here's the part I don't understand: It is then claimed that the -invariant -algebra ) gives rise to the intermediate, non-trivial, compact extension. I assume that this is done by completing the following system of extensions from to : The -invariance of shows that gives an extension. And I guess the density of will be used to show is a compact extension of . Question: To complete the above diagram, don't we need to show that ? Is this obvious? It is not explicitly addressed in the text. Thanks for reading, all help is appreciated. Edit: As John Griesmer points out, would follow if one could show every characteristic function on was AP with respect to Y. I don't quite see why such a statement should be true though. Take the function . I want to find functions and estimate as varies in . Now if the 's happened to be functions on , I could rewrite the last integral as by the properties of the conditional expectation operator. So I guess, I'm left trying to play around with the equation and hope that it holds for almost every and for all . Am I on the right track? what would be sensible choices for the ? Apologies if I'm missing something obvious; I only understand these things formally and have no intuition for what's actually going on.","7.21 \psi:(X,\mathscr{B},\mu,T) \to (Y,\mathscr{A},\nu,S) (X\times X,\mathscr{B}\otimes\mathscr{B},\bar{\mu},T\times T) \bar{\mu} Y 6.15 \psi:(X,\mathscr{B},\mu,T) \to (Y,\mathscr{A},\nu,S) \mu^{\psi^{-1}\mathscr{A}}_x (X,\mathscr{B}) f\in L^2(X,\mathscr{B},\mu) (AP) Y \varepsilon>0 g_1,\dots,g_r\in L^2(X,\mathscr{B},\mu) \forall n \in \mathbb{Z} \nu y \min_{i}\|U_T^nf-g_i\|_{L^2(\mu_y^{\mathscr{A}})}<\varepsilon. U_Tf:=f\circ T \mu_y^{\mathscr{A}} (X,\mathscr{B}) \require{AMScd} \begin{CD}
X @>\mu^{\psi^{-1}\mathscr{A}}_.>> M\left(\bar{X}\right)\\
@V{\psi}VV  @| \\
Y @>\mu^{\mathscr{A}}_.>> M\left(\bar{X}\right)
\end{CD} \psi:(X,\mathscr{B},\mu,T) \to (Y,\mathscr{A},\nu,S) AP Y L^2(X,\mathscr{B},\mu) \psi:(X,\mathscr{B},\mu,T) \to (Y,\mathscr{A},\nu,S) X\to X^* \to Y X^* \to Y 200 H \in \mathscr{L}^\infty(X\times X,\bar{\mu}) T\times T \mathscr{L} H \phi \in L^2(X,\mathscr{B},\mu) H*\phi(x) = \int H(x,x')\phi(x')d\mu^{\psi^{-1}\mathscr{A}}_x(x'). L^2(X,\mathscr{B},\mu)\to L^2(X,\mathscr{B},\mu). L^2(X,\mathscr{B},\mu^{\psi^{-1}\mathscr{A}}_x) \to L^2(X,\mathscr{B},\mu^{\psi^{-1}\mathscr{A}}_x) U_T(H*\phi)=H*(U_T\phi) \lbrace H*\phi : \phi \in \mathscr{L}^{\infty}(X,\mathscr{B},\mu)\rbrace AP Y L^{\infty}(X,\mathscr{B},\mu) \psi^{-1}\mathscr{A} \mathscr{F}=\lbrace f\in L^{\infty}(X,\mathscr{B},\mu): f \text{ is } AP \text{ wrt. } Y\rbrace  \mathscr{B}^* \sigma \mathscr{F} \mathscr{F} \subset L^2(X,\mathscr{B}^*,\mu) T \sigma \mathscr{B}^* (\subset \mathscr{B} M\left(\bar{X}\right) Y \begin{CD}
X @>\psi>> Y\\
@V\mu^{\mathscr{B}^*}_.VV   \\
 M\left(\bar{X}\right) 
\end{CD} T \mathscr{B}^* \mu^{\mathscr{B}^*}_. \mathscr{F} M\left( \bar{X}\right) Y \psi^{-1}\mathscr{A}\subset \mathscr{B}^* \psi^{-1}\mathscr{A}\subset\mathscr{B}^* 1_{\psi^{-1}A} X 1_{A}\circ\psi g_i\in L^2(X,\mathscr{B},\mu) \int|1_{A}(\psi(T^nx))-g_i(x)|^2d\mu_{x'}^{\psi^{-1}\mathscr{A}}(x) = \int|1_{A}(S^n(\psi x))-g_i(x)|^2d\mu_{x'}^{\psi^{-1}\mathscr{A}}(x) x' X g_i Y \int|1_{A}(S^ny)-g_i(y)|^2d\nu_{\psi x'}^{\mathscr{A}}(x) = E\left(|U_S^n1_A-g_i|^2:\mathscr{A},\nu\right)(\psi(x')) = |U^n_S1_A - g_i|^2(\psi(x')) \min_i|U_s^n1_A(y) - g_i(y)|^2 < \varepsilon,  y n g_i","['functional-analysis', 'measure-theory', 'dynamical-systems', 'ergodic-theory', 'additive-combinatorics']"
19,Inverting the Laplacian,Inverting the Laplacian,,"I've had a hard time looking for literature on this, so here's my question: We take a look at the Laplacian $-\Delta$ as an unbounded operator on $\mathrm{L}^2(\mathbb{R}^3)$ . We know that $-\Delta$ is unitary equivalent to the multiplication operator with $|p|^2$ in Fourier space, so $-\Delta=\mathcal{F}^{-1} |p|^2 \mathcal{F}$ . So one could now use functional calculus and define an inverse Laplacian by setting $(-\Delta)^{-1}=\mathcal{F}^{-1} (1/|p|^2 )\mathcal{F}$ , which will also be unbounded of course. It is also known from theory of PDEs that one can invert the Laplacian on Schwartz functions using the Green's function $1/4\pi |x|$ , which is derived as a distributional Fourier transform of $1/|p|^2$ . So define $G$ on $\mathrm{L}^2(\mathbb{R}^3)$ by convolution with $1/4\pi |x|$ : $$(G\phi)(x)=\int \frac{\phi(y)}{4\pi |x-y|} dy$$ $G$ is also unbounded and coincides at least for the Schwartz functions with the above defined inverse Laplacian, $G\phi=(-\Delta)^{-1}\phi$ for all $\phi \in\mathcal{S}(\mathbb{R}^3)$ . Now my question is: Are both operators the same? Does $G=(-\Delta)^{-1}$ hold, i. e. is $D(G)=D((-\Delta)^{-1})$ and $G\phi=(-\Delta)^{-1}\phi$ for all $\phi \in D(G)=D((-\Delta)^{-1})$ ? My guess is that $\mathcal{S}(\mathbb{R}^3)$ is a core of $(-\Delta)^{-1}$ , and as $(-\Delta)^{-1}|_{\mathcal{S}(\mathbb{R}^3)}=G|_{\mathcal{S}(\mathbb{R}^3)}$ equality should follow by closing the restrictions. Any comments, hints on how to proceed or references are welcome! Thank you.","I've had a hard time looking for literature on this, so here's my question: We take a look at the Laplacian as an unbounded operator on . We know that is unitary equivalent to the multiplication operator with in Fourier space, so . So one could now use functional calculus and define an inverse Laplacian by setting , which will also be unbounded of course. It is also known from theory of PDEs that one can invert the Laplacian on Schwartz functions using the Green's function , which is derived as a distributional Fourier transform of . So define on by convolution with : is also unbounded and coincides at least for the Schwartz functions with the above defined inverse Laplacian, for all . Now my question is: Are both operators the same? Does hold, i. e. is and for all ? My guess is that is a core of , and as equality should follow by closing the restrictions. Any comments, hints on how to proceed or references are welcome! Thank you.",-\Delta \mathrm{L}^2(\mathbb{R}^3) -\Delta |p|^2 -\Delta=\mathcal{F}^{-1} |p|^2 \mathcal{F} (-\Delta)^{-1}=\mathcal{F}^{-1} (1/|p|^2 )\mathcal{F} 1/4\pi |x| 1/|p|^2 G \mathrm{L}^2(\mathbb{R}^3) 1/4\pi |x| (G\phi)(x)=\int \frac{\phi(y)}{4\pi |x-y|} dy G G\phi=(-\Delta)^{-1}\phi \phi \in\mathcal{S}(\mathbb{R}^3) G=(-\Delta)^{-1} D(G)=D((-\Delta)^{-1}) G\phi=(-\Delta)^{-1}\phi \phi \in D(G)=D((-\Delta)^{-1}) \mathcal{S}(\mathbb{R}^3) (-\Delta)^{-1} (-\Delta)^{-1}|_{\mathcal{S}(\mathbb{R}^3)}=G|_{\mathcal{S}(\mathbb{R}^3)},"['functional-analysis', 'partial-differential-equations', 'operator-theory', 'functional-calculus']"
20,Showing that is a normal operator,Showing that is a normal operator,,"Let $H$ is a Hilbert space $I$ is unit operator, $T \in B(H)$ and $\lambda \in \mathbb C$ $T$ is normal operator $\Rightarrow$ $T-\lambda I$ is a normal operator too. I could only write : I must show that $(T-\lambda I)(T-\lambda I)^{\ast}=(T-\lambda I)^{\ast}(T-\lambda I)$ $TT^{\ast}=T^{\ast}T$ $I^{\ast}=I$ $(T-\lambda I)^{\ast}=T^{\ast}- \bar{\lambda}I$ (where $\ast$ means adjoint and $\bar{\lambda}I$ means complex conjugate. I cannot continue. I really stuck Thanks for any help","Let is a Hilbert space is unit operator, and is normal operator is a normal operator too. I could only write : I must show that (where means adjoint and means complex conjugate. I cannot continue. I really stuck Thanks for any help",H I T \in B(H) \lambda \in \mathbb C T \Rightarrow T-\lambda I (T-\lambda I)(T-\lambda I)^{\ast}=(T-\lambda I)^{\ast}(T-\lambda I) TT^{\ast}=T^{\ast}T I^{\ast}=I (T-\lambda I)^{\ast}=T^{\ast}- \bar{\lambda}I \ast \bar{\lambda}I,"['functional-analysis', 'operator-theory', 'hilbert-spaces', 'adjoint-operators']"
21,"Showing any linear operator $T : X \to Y$ is bounded, where $X$ is a finite dimensional normed vector space, and $Y$ any normed vector space.","Showing any linear operator  is bounded, where  is a finite dimensional normed vector space, and  any normed vector space.",T : X \to Y X Y,"Let $X$ be a nite dimensional normed vector space and $Y$ an arbitrary normed vector space. Show that any linear operator $T : X \to Y$ is bounded. I got the hint to first show that $\| x\|_0 := \| x \| + \| Tx\|$ , $x \in X$ , denes a norm on $X$ , but I do not know how this should help me. Further I should calculate $\|T\|$ for where $X = K^n$ , equipped with the Euclidean norm $\|\cdot\|_2$ , $Y := \ell_1(\mathbb{N})$ and $Tx := (x_1,\ldots,x_n,0,0,\ldots) \in \ell_1(\mathbb{N})$ , for all $x = (x_1,\ldots,x_n) \in K^n$ . Can please someone help?","Let be a nite dimensional normed vector space and an arbitrary normed vector space. Show that any linear operator is bounded. I got the hint to first show that , , denes a norm on , but I do not know how this should help me. Further I should calculate for where , equipped with the Euclidean norm , and , for all . Can please someone help?","X Y T : X \to Y \| x\|_0 := \| x \| + \| Tx\| x \in X X \|T\| X = K^n \|\cdot\|_2 Y := \ell_1(\mathbb{N}) Tx := (x_1,\ldots,x_n,0,0,\ldots) \in \ell_1(\mathbb{N}) x = (x_1,\ldots,x_n) \in K^n","['functional-analysis', 'vector-spaces', 'normed-spaces']"
22,Regarding Hahn Banach theorem and supporting hyperplane,Regarding Hahn Banach theorem and supporting hyperplane,,"In the above image from the book Linear Analysis by Bela Bollobas, corollary 7 gives the first consequence to the Hahn Banach Theorem. In the paragraph below corollary 8 they define a supporting functional and support plane. For a linear functional $f$ on a Banach space $X$ $$ I(f)=\{x\in X: f(x)=1\},$$ and $B(X)$ is the closed unit ball in $X$ . The second last line of the bottomost paragraph states that $I(f)$ contains no interior point of $B(X)$ . Can anyone tell why?","In the above image from the book Linear Analysis by Bela Bollobas, corollary 7 gives the first consequence to the Hahn Banach Theorem. In the paragraph below corollary 8 they define a supporting functional and support plane. For a linear functional on a Banach space and is the closed unit ball in . The second last line of the bottomost paragraph states that contains no interior point of . Can anyone tell why?","f X  I(f)=\{x\in X: f(x)=1\}, B(X) X I(f) B(X)","['functional-analysis', 'linear-transformations']"
23,Is the Schwartz topologically emebedded in space of tempered distributions?,Is the Schwartz topologically emebedded in space of tempered distributions?,,"Let $g\mapsto ( \cdot, g)_2$ denote the map from the Schwartz space $S$ into its dual space $S'$ where $(f,g)_2$ is the inner product in $L^2$ . Then is this a linear topological embedding ( $S'$ is endowed with the weak* topology)? Can anyone provide a reference or a simple proof?",Let denote the map from the Schwartz space into its dual space where is the inner product in . Then is this a linear topological embedding ( is endowed with the weak* topology)? Can anyone provide a reference or a simple proof?,"g\mapsto ( \cdot, g)_2 S S' (f,g)_2 L^2 S'","['functional-analysis', 'schwartz-space']"
24,Show there is an $n$-th degree polynomial $p(x)$ such that $||f(x)-p(x)||_\infty \leq ||f(x)-q(x)||_\infty$.,Show there is an -th degree polynomial  such that .,n p(x) ||f(x)-p(x)||_\infty \leq ||f(x)-q(x)||_\infty,"Show that for each $f \in C[0,1]$ there is an $n$ -th degree polynomial $p(x)$ on $[0,1]$ such that $||f(x)-p(x)||_\infty \leq ||f(x)-q(x)||_\infty$ for any other $n$ -th degree polynomial $q(x)$ . This looks similar to the following If $A \subseteq (X,||\cdot||)$ is compact and non-empty then for each $x \in X$ there is some $y_0 \in A$ such that $$||x-y_0||=\inf\{||x-y||: y \in A\}$$ However, the set of $n$ -th degree polynomial is finite-dimensional (hence closed), can we prove the it is compact?","Show that for each there is an -th degree polynomial on such that for any other -th degree polynomial . This looks similar to the following If is compact and non-empty then for each there is some such that However, the set of -th degree polynomial is finite-dimensional (hence closed), can we prove the it is compact?","f \in C[0,1] n p(x) [0,1] ||f(x)-p(x)||_\infty \leq ||f(x)-q(x)||_\infty n q(x) A \subseteq (X,||\cdot||) x \in X y_0 \in A ||x-y_0||=\inf\{||x-y||: y \in A\} n","['real-analysis', 'functional-analysis', 'normed-spaces']"
25,Path integral solution to heat equation,Path integral solution to heat equation,,"Let $(M,g)$ be a compact Riemannian manifold. Then the solution $u:M\times [0,\infty)\to \mathbb{R}$ of the heat equation $\partial_t u=\Delta_gu$ starting at $u_0\in C^{\infty}(M)$ is given by the path integral $$ u(x,t)=\int_{\gamma \in H_x} e^{-E(\gamma)/4t}u_0(\gamma(1))d\gamma, $$ where the integral is taken over the classical Wiener space $H_x\subset L^{2,1}([0,1],M)$ of finite energy paths $\gamma:[0,1]\to M$ starting at $x$ (i.e. $\gamma(0)=x$ ). Also, here $$ E(\gamma)=\int_0^1\Big|\frac{d\gamma}{ds}\Big|^2ds $$ is the Dirichlet energy of a curve in $(M,g)$ and the measure of integration is the Wiener measure. See this article for a survey of the above. Question: Does the above path integral formula for $u$ hold for more general parabolic PDEs? More precisely, let $L:C^{\infty}(M)\to C^{\infty}(M)$ be a second order elliptic operator (e.g. above we took $L=\Delta_g$ ). Then can we write the solution $u:M\times [0,T)\to \mathbb{R}$ of the parabolic PDE $$ \partial_tu=Lu $$ with initial condition $u_0\in C^{\infty}(M)$ as the path integral $$ u(x,t)=\int_{\gamma \in H_x} e^{-S(\gamma)/4t}u_0(\gamma(1))D\gamma, $$ for some functional $S:H_x\to \mathbb{R}$ on the path space? What is $S$ in this case? Note that $S=E$ when $L=\Delta$ .","Let be a compact Riemannian manifold. Then the solution of the heat equation starting at is given by the path integral where the integral is taken over the classical Wiener space of finite energy paths starting at (i.e. ). Also, here is the Dirichlet energy of a curve in and the measure of integration is the Wiener measure. See this article for a survey of the above. Question: Does the above path integral formula for hold for more general parabolic PDEs? More precisely, let be a second order elliptic operator (e.g. above we took ). Then can we write the solution of the parabolic PDE with initial condition as the path integral for some functional on the path space? What is in this case? Note that when .","(M,g) u:M\times [0,\infty)\to \mathbb{R} \partial_t u=\Delta_gu u_0\in C^{\infty}(M) 
u(x,t)=\int_{\gamma \in H_x} e^{-E(\gamma)/4t}u_0(\gamma(1))d\gamma,
 H_x\subset L^{2,1}([0,1],M) \gamma:[0,1]\to M x \gamma(0)=x 
E(\gamma)=\int_0^1\Big|\frac{d\gamma}{ds}\Big|^2ds
 (M,g) u L:C^{\infty}(M)\to C^{\infty}(M) L=\Delta_g u:M\times [0,T)\to \mathbb{R} 
\partial_tu=Lu
 u_0\in C^{\infty}(M) 
u(x,t)=\int_{\gamma \in H_x} e^{-S(\gamma)/4t}u_0(\gamma(1))D\gamma,
 S:H_x\to \mathbb{R} S S=E L=\Delta","['functional-analysis', 'ordinary-differential-equations', 'differential-geometry', 'partial-differential-equations', 'mathematical-physics']"
26,hard exercice in Lieb--Loss Analysis book,hard exercice in Lieb--Loss Analysis book,,I am looking the solution of the exercice 7.4 in the book Analysis of Lieb--Loss: Suppose that $f\in H^1(\mathbb R^n)$ . Show that for each $1\leq i\leq n$ $$ \int_{\mathbb R^n}|\partial_if|^2=\lim_{t\to 0}\frac{1}{t^2}\int_{\mathbb R^n}|f(x+t\mathbf{e}_i)-f(x)|^2{\rm d}x $$ where $\mathbf{e}_i$ is the unit vector in the direction $i$ . My approach is to approximate $f\in H^1(\mathbb R^n)$ by a sequence of $C_c^\infty(\mathbb R^n)$ functions. Then we can check the above identity for this smooth with compact support function. But then I do not know how to go back to the function in $H^1(\mathbb R^n)$ .,I am looking the solution of the exercice 7.4 in the book Analysis of Lieb--Loss: Suppose that . Show that for each where is the unit vector in the direction . My approach is to approximate by a sequence of functions. Then we can check the above identity for this smooth with compact support function. But then I do not know how to go back to the function in .,"f\in H^1(\mathbb R^n) 1\leq i\leq n 
\int_{\mathbb R^n}|\partial_if|^2=\lim_{t\to 0}\frac{1}{t^2}\int_{\mathbb R^n}|f(x+t\mathbf{e}_i)-f(x)|^2{\rm d}x
 \mathbf{e}_i i f\in H^1(\mathbb R^n) C_c^\infty(\mathbb R^n) H^1(\mathbb R^n)","['functional-analysis', 'sobolev-spaces']"
27,Surjectivity of the sum of two bounded operators,Surjectivity of the sum of two bounded operators,,"Let $H$ be a Hilbert space. Let $A\in B\left( H\right) $ be surjective, and let $Q\in B\left( H\right) $ be quasi-nilpotent such that $AQ=QA$ Prove that $A+Q$ is surjective. Thank you.","Let be a Hilbert space. Let be surjective, and let be quasi-nilpotent such that Prove that is surjective. Thank you.",H A\in B\left( H\right)  Q\in B\left( H\right)  AQ=QA A+Q,"['linear-algebra', 'functional-analysis', 'operator-theory', 'linear-transformations']"
28,"Showing that $Y^c$ is dense in $X$, if $ Y \subset (X, \| \cdot \|)$","Showing that  is dense in , if","Y^c X  Y \subset (X, \| \cdot \|)","Exercise : Let $X$ be a normed space $(X, \| \cdot \|)$ and $Y$ be a proper subspace of $X$ , $Y \subset X$ . Show that the complement set $Y^c$ is dense in $X$ . Question : I'm totally at loss on how to start this exercise. I know that a normed space means it's a linear space carrying the definition and properties of the norm, while on the other hand, a subset $A$ of a topological space $X$ is called dense (in $X$ ) if every point $x$ in $X$ either belongs to $A$ or is a limit point of $A$ ; that is, the closure of $A$ is constituting the whole set $X$ . But how would I proceed to showing rigorously the statement of the exercise ?","Exercise : Let be a normed space and be a proper subspace of , . Show that the complement set is dense in . Question : I'm totally at loss on how to start this exercise. I know that a normed space means it's a linear space carrying the definition and properties of the norm, while on the other hand, a subset of a topological space is called dense (in ) if every point in either belongs to or is a limit point of ; that is, the closure of is constituting the whole set . But how would I proceed to showing rigorously the statement of the exercise ?","X (X, \| \cdot \|) Y X Y \subset X Y^c X A X X x X A A A X","['real-analysis', 'functional-analysis', 'normed-spaces']"
29,Representation of ordinary differential operators in terms of a given regular operator,Representation of ordinary differential operators in terms of a given regular operator,,"I'm trying to understand Lemma 3.2 from p. 355 of the paper R. C. Carlson and K. R. Goodearl, Commutants of Ordinary Differential Operators , Journal of Differenial Equations 35 (1980), 339365. The authors define $B$ as a matrix coefficient with $C^\infty$ entries, $D$ is the ordinary derivative operator on $\bigoplus^k C^\infty(\mathscr J)$ for some open interval $\mathscr J$ , and $L$ as the differential operator given below (from the previous page) $\textbf{Question}$ : I can't see how this is true that any differential operator can be written in this way. I'm trying to see it in the scalar case, i.e. where $A,B$ are $1\times 1$ matrices but I can't even construct non-trivial examples that can be written in that way. Moreover, I don't even understand what the proof is doing.","I'm trying to understand Lemma 3.2 from p. 355 of the paper R. C. Carlson and K. R. Goodearl, Commutants of Ordinary Differential Operators , Journal of Differenial Equations 35 (1980), 339365. The authors define as a matrix coefficient with entries, is the ordinary derivative operator on for some open interval , and as the differential operator given below (from the previous page) : I can't see how this is true that any differential operator can be written in this way. I'm trying to see it in the scalar case, i.e. where are matrices but I can't even construct non-trivial examples that can be written in that way. Moreover, I don't even understand what the proof is doing.","B C^\infty D \bigoplus^k C^\infty(\mathscr J) \mathscr J L \textbf{Question} A,B 1\times 1","['real-analysis', 'functional-analysis', 'ordinary-differential-equations', 'operator-theory', 'differential-operators']"
30,Convergence in measure and boundness implies weak convergence in $L^p$,Convergence in measure and boundness implies weak convergence in,L^p,"Let $(f_n)_{n=1}^\infty$ be a sequence in $L^p ([0,1]), 1\leq p<\infty$ . Suppose that $f_n\rightarrow f$ in measure and that $\sup\limits_{n\in\mathbb{N}} \|f_n\|<\infty$ . Show that $f_n\rightarrow f$ in the weak topology of $L^p([0,1])$ . As a reminder: a net $\{g_\alpha\}_{\alpha\in I}$ converges in the weak topology of $X$ if and only if $\phi(g_\alpha)\rightarrow \phi(g)$ for all $\phi\in X^*$ . We know convergence in measure and boundness in the $L^p$ norm gives us convergence in $L^p$ . I think I could use the Dominated Convergence Theorem but I'm not sure how to proceed. I also found a proof of this Theorem but with the hypothesis $f_n\rightarrow f$ a.e., and they use Egorov's theorem, but I rather not use it. I thank any suggestion you have.","Let be a sequence in . Suppose that in measure and that . Show that in the weak topology of . As a reminder: a net converges in the weak topology of if and only if for all . We know convergence in measure and boundness in the norm gives us convergence in . I think I could use the Dominated Convergence Theorem but I'm not sure how to proceed. I also found a proof of this Theorem but with the hypothesis a.e., and they use Egorov's theorem, but I rather not use it. I thank any suggestion you have.","(f_n)_{n=1}^\infty L^p ([0,1]), 1\leq p<\infty f_n\rightarrow f \sup\limits_{n\in\mathbb{N}} \|f_n\|<\infty f_n\rightarrow f L^p([0,1]) \{g_\alpha\}_{\alpha\in I} X \phi(g_\alpha)\rightarrow \phi(g) \phi\in X^* L^p L^p f_n\rightarrow f","['functional-analysis', 'measure-theory', 'convergence-divergence', 'lp-spaces', 'weak-convergence']"
31,Eigenvalues of an operator in a real separable Hilbert space as smooth functions of a parameter,Eigenvalues of an operator in a real separable Hilbert space as smooth functions of a parameter,,"Suppose $V$ is a infinite-dimensional separable real Hilbert space and $$ T\,\colon\,I \to B(V), $$ where $I = [0,a) \subset \mathbb{R}$ and $B(V)$ denotes the space of bounded linear operators on $V$ . The short-hand notation is $T(\epsilon) \equiv T_{\epsilon}$ for any $\epsilon \in I$ . Suppose further that $T$ is continuous (possibly Frechet differentiable?) and that $T_0$ is simply given by $$T_0v = cv,\quad(1)$$ where $c > 0$ . Final assumptions are that for any $\epsilon \in I$ , $T_{\epsilon}$ is self-adjoint and is an isomorphism  and for $\epsilon > 0$ it is not of the form given in (1). (Side note: I am looking at a specific example but I wanted to look at the problem more abstractly, perhaps some of the 'assumptions' either follow from the others or are not needed). I would like to be able to characterise the set of eigenvalues and eigenvectors of $T_{\epsilon}$ and how they depend on the parameter $\epsilon$ . Trivially, the form of $T_0$ implies that $c$ is the sole eigenvalue of $T_0$ and its corresponding eigenspace is the whole of $V$ . It seems fairly natural that the eigenvalues will be continuous functions of $\epsilon$ since $T$ is a continuous function and so if $\lambda_\epsilon$ is an eigenvalue of $T_\epsilon$ with the corresponding eigenvector $v_{\epsilon}$ then $$ (\lambda - c)v_{\epsilon} = (T_{\epsilon}-T_0)v_{\epsilon} \implies |\lambda - c|\|v_{\epsilon}\| \leq \|(T_{\epsilon}-T_0)\|\|v_{\epsilon}\| \implies |\lambda - c| \leq C\epsilon, $$ with the first equality following from the fact that $v_{\epsilon}$ is also an eigenvector for $T_0$ . But what gives us the certainty that $T_{\epsilon}$ has any eigenvalues at all? How many eigenvalues can it have? Since it is self-adjoint and $V$ is separable, there can be at most countably many eigenvalues, so it is tempting to conjecture that the set of eigenvalues is $$(\lambda_1(\epsilon),\lambda_2(\epsilon),\dots)$$ with each entry being a continous function of $\epsilon$ with $\lambda_i(0) = c$ and possibly $\lambda_i(\epsilon) = \lambda_j(\epsilon)$ for some $i\neq j$ . A further conjecture would be that each $\lambda_i(\epsilon)$ can be associated with one eigenvector $v_i$ such that $(v_i,v_j) = 0$ and the closure of ${\rm span}(v_1,v_2,\dots)$ is the whole of $V$ . For that it feels like we should employ Spectral Theorem, so in particular perhaps establish that $ (T_{\epsilon} - T_0)$ is a compact operator? Is it? I do not quite see it. I will humbly appreciate any potential comments, I am quite confused here, in particular the notion of orthonormal basis for infinite-dimensional Hilbert space seems a bit elusive. As I mentioned in the side-note, I am keen to consider this more broadly, so perhaps if one of the assumptions makes the problem trivial (perhaps the fact that $T_{\epsilon}$ is an isomorphism?), please feel free to comment more broadly.","Suppose is a infinite-dimensional separable real Hilbert space and where and denotes the space of bounded linear operators on . The short-hand notation is for any . Suppose further that is continuous (possibly Frechet differentiable?) and that is simply given by where . Final assumptions are that for any , is self-adjoint and is an isomorphism  and for it is not of the form given in (1). (Side note: I am looking at a specific example but I wanted to look at the problem more abstractly, perhaps some of the 'assumptions' either follow from the others or are not needed). I would like to be able to characterise the set of eigenvalues and eigenvectors of and how they depend on the parameter . Trivially, the form of implies that is the sole eigenvalue of and its corresponding eigenspace is the whole of . It seems fairly natural that the eigenvalues will be continuous functions of since is a continuous function and so if is an eigenvalue of with the corresponding eigenvector then with the first equality following from the fact that is also an eigenvector for . But what gives us the certainty that has any eigenvalues at all? How many eigenvalues can it have? Since it is self-adjoint and is separable, there can be at most countably many eigenvalues, so it is tempting to conjecture that the set of eigenvalues is with each entry being a continous function of with and possibly for some . A further conjecture would be that each can be associated with one eigenvector such that and the closure of is the whole of . For that it feels like we should employ Spectral Theorem, so in particular perhaps establish that is a compact operator? Is it? I do not quite see it. I will humbly appreciate any potential comments, I am quite confused here, in particular the notion of orthonormal basis for infinite-dimensional Hilbert space seems a bit elusive. As I mentioned in the side-note, I am keen to consider this more broadly, so perhaps if one of the assumptions makes the problem trivial (perhaps the fact that is an isomorphism?), please feel free to comment more broadly.","V 
T\,\colon\,I \to B(V),
 I = [0,a) \subset \mathbb{R} B(V) V T(\epsilon) \equiv T_{\epsilon} \epsilon \in I T T_0 T_0v = cv,\quad(1) c > 0 \epsilon \in I T_{\epsilon} \epsilon > 0 T_{\epsilon} \epsilon T_0 c T_0 V \epsilon T \lambda_\epsilon T_\epsilon v_{\epsilon} 
(\lambda - c)v_{\epsilon} = (T_{\epsilon}-T_0)v_{\epsilon} \implies |\lambda - c|\|v_{\epsilon}\| \leq \|(T_{\epsilon}-T_0)\|\|v_{\epsilon}\| \implies |\lambda - c| \leq C\epsilon,
 v_{\epsilon} T_0 T_{\epsilon} V (\lambda_1(\epsilon),\lambda_2(\epsilon),\dots) \epsilon \lambda_i(0) = c \lambda_i(\epsilon) = \lambda_j(\epsilon) i\neq j \lambda_i(\epsilon) v_i (v_i,v_j) = 0 {\rm span}(v_1,v_2,\dots) V  (T_{\epsilon} - T_0) T_{\epsilon}","['functional-analysis', 'hilbert-spaces', 'inner-products', 'spectral-theory']"
32,Norm of a linear operator from $\mathbb{R}^2\to\mathbb{R}^3$,Norm of a linear operator from,\mathbb{R}^2\to\mathbb{R}^3,"Find $\|T\|_{\mathcal{L}}$ where $T\in\mathcal{L}(\mathbb{R}^2,\mathbb{R}^3)$ is defined by $$T(\mathbf{x})=(x,2x, 3x)\ \ \ \forall \mathbf{x}=(x,y)\in\mathbb{R}^2$$ My approach $T$ is a linear operator (easy to prove); $\|\mathbf{x}\|=\sqrt{x^2+y^2}\ \ \ \forall \mathbf{x}\in\mathbb{R}^2$ $\|T(\mathbf{x})\|=\sqrt{x^2+4x^2+9x^2}=\sqrt{14}|x|$ so $\frac{\|T(\mathbb{x})\|}{\|\mathbf{x}\|}=\frac{\sqrt{14}|x|}{\sqrt{x^2+y^2}}\le\frac{\sqrt{14}|x|}{|x|}=\sqrt{14}\ \ \ \forall\mathbf{x}\ne \mathbf{0}$ This means that $\|T\|_{\mathcal{L}}\le\sqrt{14}$. For $\mathbf{x}=(1,0)$ we have that $\|T(\mathbf{x})\|=\sqrt{14}$ so $\|T\|_{\mathcal{L}}=\sqrt{14}$. Second approach Let $\mathbf{x}$ be a unitary vector of $\mathbb{R}^2$, such that $$\|\mathbb{x}\|=1\implies x^2+y^2=1 \ \ \ \to |x|=\sqrt{1-y^2}\ \ \ \forall y\in [-1,1]$$ so $$\|T(\mathbf{x})\|=\sqrt{14}|x|=\sqrt{14}\sqrt{1-y^2}\le\sqrt{14}\ \ \ \forall y\in [-1,1]$$ In particular $\|T(\mathbf{x})\|=\sqrt{14}$ for $(x,y)=(1,0)$, hence $\|T\|_{\mathcal{L}}=\sqrt{14}$. Are these solutions ok? Thanks.","Find $\|T\|_{\mathcal{L}}$ where $T\in\mathcal{L}(\mathbb{R}^2,\mathbb{R}^3)$ is defined by $$T(\mathbf{x})=(x,2x, 3x)\ \ \ \forall \mathbf{x}=(x,y)\in\mathbb{R}^2$$ My approach $T$ is a linear operator (easy to prove); $\|\mathbf{x}\|=\sqrt{x^2+y^2}\ \ \ \forall \mathbf{x}\in\mathbb{R}^2$ $\|T(\mathbf{x})\|=\sqrt{x^2+4x^2+9x^2}=\sqrt{14}|x|$ so $\frac{\|T(\mathbb{x})\|}{\|\mathbf{x}\|}=\frac{\sqrt{14}|x|}{\sqrt{x^2+y^2}}\le\frac{\sqrt{14}|x|}{|x|}=\sqrt{14}\ \ \ \forall\mathbf{x}\ne \mathbf{0}$ This means that $\|T\|_{\mathcal{L}}\le\sqrt{14}$. For $\mathbf{x}=(1,0)$ we have that $\|T(\mathbf{x})\|=\sqrt{14}$ so $\|T\|_{\mathcal{L}}=\sqrt{14}$. Second approach Let $\mathbf{x}$ be a unitary vector of $\mathbb{R}^2$, such that $$\|\mathbb{x}\|=1\implies x^2+y^2=1 \ \ \ \to |x|=\sqrt{1-y^2}\ \ \ \forall y\in [-1,1]$$ so $$\|T(\mathbf{x})\|=\sqrt{14}|x|=\sqrt{14}\sqrt{1-y^2}\le\sqrt{14}\ \ \ \forall y\in [-1,1]$$ In particular $\|T(\mathbf{x})\|=\sqrt{14}$ for $(x,y)=(1,0)$, hence $\|T\|_{\mathcal{L}}=\sqrt{14}$. Are these solutions ok? Thanks.",,"['functional-analysis', 'measure-theory', 'linear-transformations']"
33,"Assume $\int^a_b \operatorname{tr}(A(t)B(t))~dt=0$ for any $B$, where $A,B$ are $n \times n$ matrices. Does this imply $A=0$?","Assume  for any , where  are  matrices. Does this imply ?","\int^a_b \operatorname{tr}(A(t)B(t))~dt=0 B A,B n \times n A=0","Assume $\displaystyle\int^a_b  \operatorname{tr}(A(t)B(t))~dt=0$ for any $B$, where $A,B$ are $n \times n$ matrices. Does this imply $A=0$? If this is not true, can we add some conditions for $A, B$ to make the proposition true?  For example, add some conditions like $A$ is symmetric, $A$ is skew-symmetric, both $A~\text{and}~B \in SO(n)$, etc.","Assume $\displaystyle\int^a_b  \operatorname{tr}(A(t)B(t))~dt=0$ for any $B$, where $A,B$ are $n \times n$ matrices. Does this imply $A=0$? If this is not true, can we add some conditions for $A, B$ to make the proposition true?  For example, add some conditions like $A$ is symmetric, $A$ is skew-symmetric, both $A~\text{and}~B \in SO(n)$, etc.",,"['calculus', 'real-analysis', 'linear-algebra', 'functional-analysis', 'lie-groups']"
34,Is injective co-isometry an isometry?,Is injective co-isometry an isometry?,,"Let $E$ be a normed space. Let $T:E\to E$ be an injective continuous linear map, such that $T^{*}$ is an isometry. Does it follows that $T$ itself is an isometry (in fact it is then an isometric isomorphism)? This is true if $E$ is reflexive. Indeed, if $T$ is injective, $T^{*}$ has a dense range. An isometry with a dense range must be an isometric isomorphism, and so $T=T^{**}$ is also an isometric isomorphism. However, if $E$ is not reflexive we cannot conclude that $T^{*}$ has a dense image, only weak* dense, and so I expect that there is a counterexample.","Let $E$ be a normed space. Let $T:E\to E$ be an injective continuous linear map, such that $T^{*}$ is an isometry. Does it follows that $T$ itself is an isometry (in fact it is then an isometric isomorphism)? This is true if $E$ is reflexive. Indeed, if $T$ is injective, $T^{*}$ has a dense range. An isometry with a dense range must be an isometric isomorphism, and so $T=T^{**}$ is also an isometric isomorphism. However, if $E$ is not reflexive we cannot conclude that $T^{*}$ has a dense image, only weak* dense, and so I expect that there is a counterexample.",,"['functional-analysis', 'banach-spaces']"
35,The function space in which the force field of Navier-Stokes equation lives,The function space in which the force field of Navier-Stokes equation lives,,"Concerning the setting for Navier-Stokes equation, we often see the conditions for various functions involved. To be specific, consider we ought to derive the existence of weak solution of the Navier-Stokes equation with some boundary condition. The equation is as you know  $$ \partial \mathbf{v} - \nu \Delta \mathbf{v} + (\mathbf{v} \cdot \nabla) \mathbf{v} + \nabla p = \mathbf{g} $$ with some boundary condition where $\mathbf{v}$ is the unknown vector field and $\mathbf{g}$ stands for the force density. Some authors often requires that $\mathbf{g}$ is  in the direct sum $V=L^1(0,T;L_\sigma^2 (\mathbb{R}^3)) + L^2(0,T;W_{0,\sigma}^{-1,2}(\mathbb{R}^3))$ where $L_\sigma^2(\mathbb{R}^3)$ is the completion of $C_{0,\sigma}^\infty ( \mathbb{R}^3)$, the space of smooth divergence-free vector fields, with respect to the norm $\| \cdot \|_2$ and $W_{0,\sigma}^{-1,2}$ is the dual of $W_{0,\sigma}^{1,2}(\mathbb{R})$ where $W_{0,\sigma}^{1,2}(\mathbb{R})$ is the completion of $C_{0,\sigma}^\infty ( \mathbb{R}^3)$ with respect to the norm $\| \cdot \|_{1,2}$. Here I would like to raise two questions. Why $V$ is direct sum of the two spaces? Why it is natural to require such a seemingly artificial condition? For the appearence of such a condition, see, for example, https://onlinelibrary.wiley.com/doi/abs/10.1002/mma.1059","Concerning the setting for Navier-Stokes equation, we often see the conditions for various functions involved. To be specific, consider we ought to derive the existence of weak solution of the Navier-Stokes equation with some boundary condition. The equation is as you know  $$ \partial \mathbf{v} - \nu \Delta \mathbf{v} + (\mathbf{v} \cdot \nabla) \mathbf{v} + \nabla p = \mathbf{g} $$ with some boundary condition where $\mathbf{v}$ is the unknown vector field and $\mathbf{g}$ stands for the force density. Some authors often requires that $\mathbf{g}$ is  in the direct sum $V=L^1(0,T;L_\sigma^2 (\mathbb{R}^3)) + L^2(0,T;W_{0,\sigma}^{-1,2}(\mathbb{R}^3))$ where $L_\sigma^2(\mathbb{R}^3)$ is the completion of $C_{0,\sigma}^\infty ( \mathbb{R}^3)$, the space of smooth divergence-free vector fields, with respect to the norm $\| \cdot \|_2$ and $W_{0,\sigma}^{-1,2}$ is the dual of $W_{0,\sigma}^{1,2}(\mathbb{R})$ where $W_{0,\sigma}^{1,2}(\mathbb{R})$ is the completion of $C_{0,\sigma}^\infty ( \mathbb{R}^3)$ with respect to the norm $\| \cdot \|_{1,2}$. Here I would like to raise two questions. Why $V$ is direct sum of the two spaces? Why it is natural to require such a seemingly artificial condition? For the appearence of such a condition, see, for example, https://onlinelibrary.wiley.com/doi/abs/10.1002/mma.1059",,"['functional-analysis', 'partial-differential-equations']"
36,When $\|f_n\|_{L^2}=1$ where $f_n(x)= f(x-n)+(-1)^n f(x+n)$?,When  where ?,\|f_n\|_{L^2}=1 f_n(x)= f(x-n)+(-1)^n f(x+n),"Let $f: \mathbb R \to \mathbb C $ be a function and define $f_n(x)= f(x-n)+(-1)^n f(x+n).$ Can we expect to choose $f\in L^2(\mathbb R)$ such that    $\|f_n\|^2_{L^2}=1$ for all $n\in\mathbb Z$? Side thought :  We know that $L^2(\mathbb R)$ is complex Hilbert space with inner product $\langle f, g \rangle = \int f \bar{g}$  and using properties of inner product we have  $$\|f_n\|_{L^2}^2= 2\|f\|^2_{L^2}+ 2 \text{Re} \langle f(x-n), (-1)^nf(x+n)\rangle $$","Let $f: \mathbb R \to \mathbb C $ be a function and define $f_n(x)= f(x-n)+(-1)^n f(x+n).$ Can we expect to choose $f\in L^2(\mathbb R)$ such that    $\|f_n\|^2_{L^2}=1$ for all $n\in\mathbb Z$? Side thought :  We know that $L^2(\mathbb R)$ is complex Hilbert space with inner product $\langle f, g \rangle = \int f \bar{g}$  and using properties of inner product we have  $$\|f_n\|_{L^2}^2= 2\|f\|^2_{L^2}+ 2 \text{Re} \langle f(x-n), (-1)^nf(x+n)\rangle $$",,"['functional-analysis', 'analysis', 'hilbert-spaces', 'examples-counterexamples']"
37,"Prove that a functional in $L^2(0, \pi)$ is bounded",Prove that a functional in  is bounded,"L^2(0, \pi)","Let $\Phi$ be a functional defined on $L^2(0, \pi)$ such that $\Phi(\sin(nx))=a_n$ on the basis $\{\sin(nx)\}_{n=1}^{\infty}$ with $\{a_n\}$ sequence of complex numbers. What are the conditions on $\{a_n\}$ for which $\Phi$ is bounded? My attempt. For any $f\in L^2(0, \pi)$, I can write $f(x)=\sum_{k=1}^\infty f_k\sin(kx)$. Then $\Phi(f)=\sum_{k=1}^\infty f_ka_k$. By Cauchy-Schwarz inequality, we have that $$ |\Phi(f)|^2\leq\sum_{k=1}^n|f_k|^2\sum_{k=1}^n|a_k|^2. $$ Now,  $$ \|f\|_{L^2(0, \pi)}^2=\sum_{k=1}^\infty\int_0^{\pi}f_k^2\sin^2(kx)\ dx=\frac{\pi}{2}\sum_{k=1}^\infty f_k^2 $$ and then $\sum_{k=1}^\infty |f_k|^2=\frac{2}{\pi}\|f\|_{L^2(0, \pi)}$. It follows that, if $\sum_{k=1}^\infty|a_k|^2=s<\infty$, then $$ |\Phi(f)|^2\leq\frac{2s}{\pi}\|f\|^2_{L^2(0, \pi)}<\infty. $$ Is my attempt correct? Thank You","Let $\Phi$ be a functional defined on $L^2(0, \pi)$ such that $\Phi(\sin(nx))=a_n$ on the basis $\{\sin(nx)\}_{n=1}^{\infty}$ with $\{a_n\}$ sequence of complex numbers. What are the conditions on $\{a_n\}$ for which $\Phi$ is bounded? My attempt. For any $f\in L^2(0, \pi)$, I can write $f(x)=\sum_{k=1}^\infty f_k\sin(kx)$. Then $\Phi(f)=\sum_{k=1}^\infty f_ka_k$. By Cauchy-Schwarz inequality, we have that $$ |\Phi(f)|^2\leq\sum_{k=1}^n|f_k|^2\sum_{k=1}^n|a_k|^2. $$ Now,  $$ \|f\|_{L^2(0, \pi)}^2=\sum_{k=1}^\infty\int_0^{\pi}f_k^2\sin^2(kx)\ dx=\frac{\pi}{2}\sum_{k=1}^\infty f_k^2 $$ and then $\sum_{k=1}^\infty |f_k|^2=\frac{2}{\pi}\|f\|_{L^2(0, \pi)}$. It follows that, if $\sum_{k=1}^\infty|a_k|^2=s<\infty$, then $$ |\Phi(f)|^2\leq\frac{2s}{\pi}\|f\|^2_{L^2(0, \pi)}<\infty. $$ Is my attempt correct? Thank You",,"['functional-analysis', 'hilbert-spaces', 'lp-spaces']"
38,Weak continuity of functions with values in a Banach space,Weak continuity of functions with values in a Banach space,,"I have problems understanding the proof of the following lemma: Let $X$, $Y$ be Banach spaces, $X$ reflexive, and assume that $X$ is continuously, densely embeded into $Y$. Let $I \subset \mathbb{R}$ be open, bounded interval. Consider a function $\varphi \in L^{\infty}(I;X)$ such that $\varphi$ is weakly continuous from $\bar{I}$ to $Y$. $\space$ Then $\varphi$ is also weakly continuous from $\bar{I}$ to $X$. In the proof, $I$ stands for not only the interval, but also for the mapping that represents the embedding from the assumptions of the lemma. Moreover, $I^{*}$ stands for the associated continuous, dense embedding of $Y^{*}$ to $X^{*}$. I understand the proof until the point when they claim that $I \widetilde{\varphi}(t)=I\varphi(t)$ and $\widetilde{\varphi}(t)=\varphi(t)$. I don't see what it follows from. I'll be thankful for any help.","I have problems understanding the proof of the following lemma: Let $X$, $Y$ be Banach spaces, $X$ reflexive, and assume that $X$ is continuously, densely embeded into $Y$. Let $I \subset \mathbb{R}$ be open, bounded interval. Consider a function $\varphi \in L^{\infty}(I;X)$ such that $\varphi$ is weakly continuous from $\bar{I}$ to $Y$. $\space$ Then $\varphi$ is also weakly continuous from $\bar{I}$ to $X$. In the proof, $I$ stands for not only the interval, but also for the mapping that represents the embedding from the assumptions of the lemma. Moreover, $I^{*}$ stands for the associated continuous, dense embedding of $Y^{*}$ to $X^{*}$. I understand the proof until the point when they claim that $I \widetilde{\varphi}(t)=I\varphi(t)$ and $\widetilde{\varphi}(t)=\varphi(t)$. I don't see what it follows from. I'll be thankful for any help.",,"['functional-analysis', 'banach-spaces', 'weak-convergence', 'bochner-spaces']"
39,consequence of Hahn-Banach theorem,consequence of Hahn-Banach theorem,,"In Wikipedia, it says that Hahn-Banach Theorem shows there are ""enough"" continuous linear functionals. But, why is that so in a space that is not necessarily normed? How does the statement of Hahn-Banach show this?","In Wikipedia, it says that Hahn-Banach Theorem shows there are ""enough"" continuous linear functionals. But, why is that so in a space that is not necessarily normed? How does the statement of Hahn-Banach show this?",,['functional-analysis']
40,The continuity of linear functionals with respect to uniform convergence of entire functions on balls,The continuity of linear functionals with respect to uniform convergence of entire functions on balls,,"Let $X$ be a Banach space and $H_b (X)$ be the algebra of complex-valued entire functions on $X$ which are bounded on bounded sets, with the topology of uniform convergence on bounded sets. Let $\varphi \in H_b^* (X)$ (which is the dual space of $H_b (X)$). Each $\varphi \in H_b^* (X)$ is continuous with respect to the norm of uniform convergence on some ball in $X$. Define the radius function $R$ on $H_b^* (X)$ by declaring $R(\varphi) $ to be the infimum of all $r >0$ such that $\varphi$ is continuous with respect to the norm of uniform convergence on the ball $r B$. I don't understand the above bold statement. I am a bit confused because I think $\varphi \in H_b^* (X)$ is continuous on every ball in $X$ (so, the radius of $\varphi$ is always $0$). I want to know what I missed.","Let $X$ be a Banach space and $H_b (X)$ be the algebra of complex-valued entire functions on $X$ which are bounded on bounded sets, with the topology of uniform convergence on bounded sets. Let $\varphi \in H_b^* (X)$ (which is the dual space of $H_b (X)$). Each $\varphi \in H_b^* (X)$ is continuous with respect to the norm of uniform convergence on some ball in $X$. Define the radius function $R$ on $H_b^* (X)$ by declaring $R(\varphi) $ to be the infimum of all $r >0$ such that $\varphi$ is continuous with respect to the norm of uniform convergence on the ball $r B$. I don't understand the above bold statement. I am a bit confused because I think $\varphi \in H_b^* (X)$ is continuous on every ball in $X$ (so, the radius of $\varphi$ is always $0$). I want to know what I missed.",,"['functional-analysis', 'banach-spaces', 'uniform-convergence', 'banach-algebras', 'analyticity']"
41,Compactness in $\ell ^1$. Is $K := \{ (z_n)_n \in \ell^1 : |z_n| \leq |x_n|\}$ compact?,Compactness in . Is  compact?,\ell ^1 K := \{ (z_n)_n \in \ell^1 : |z_n| \leq |x_n|\},"I'm having some trouble with the following: Consider the following set $K := \{ (z_n)_n \in \ell^1 : |z_n| \leq |x_n|\}$ for a given $(x_n)_n \in \ell^1$. Is $K$ a secuencially compact space? This is my thoughts so far: I suspect $K$ is in fact compact. We want to see that given a $(\xi_k)_k \subset \ell^1$ we can find a convergent subsequence. So let $\xi_k= (z^k_n)_n$ and $(\xi_k)_k \subset K$. For a fixed $n$ we have that $(z^k_n)_k \subset \bar B_{|x_n|}(0) \subset \mathbb{R} $, which is compact. It follows then that $(z^k_n)_k$ has a partial convergent subsequence. Let then, $(z^{k'}_n)_{k' \in I(n)}$ where $I(n) \subset \mathbb{N}$ is an index set, denote such convergent subsuccession. My idea is that if we can see that $|\cap_n I(n)| = \infty$ then $(\xi_{k'})_{k'}$ with $k' \in \cap_n I(n)$ would be a convergent subsequence of $(\xi_k)_k$. I'm a bit stuck on how to proceed from here, suposing that this leads somewhere interesting.","I'm having some trouble with the following: Consider the following set $K := \{ (z_n)_n \in \ell^1 : |z_n| \leq |x_n|\}$ for a given $(x_n)_n \in \ell^1$. Is $K$ a secuencially compact space? This is my thoughts so far: I suspect $K$ is in fact compact. We want to see that given a $(\xi_k)_k \subset \ell^1$ we can find a convergent subsequence. So let $\xi_k= (z^k_n)_n$ and $(\xi_k)_k \subset K$. For a fixed $n$ we have that $(z^k_n)_k \subset \bar B_{|x_n|}(0) \subset \mathbb{R} $, which is compact. It follows then that $(z^k_n)_k$ has a partial convergent subsequence. Let then, $(z^{k'}_n)_{k' \in I(n)}$ where $I(n) \subset \mathbb{N}$ is an index set, denote such convergent subsuccession. My idea is that if we can see that $|\cap_n I(n)| = \infty$ then $(\xi_{k'})_{k'}$ with $k' \in \cap_n I(n)$ would be a convergent subsequence of $(\xi_k)_k$. I'm a bit stuck on how to proceed from here, suposing that this leads somewhere interesting.",,"['functional-analysis', 'compactness']"
42,Evaluation of a path integral,Evaluation of a path integral,,"I am stuck weeks in the following calculation from a paper: Path integral $I$ is written as $$ I = \int_{\mathbf{r}_0 = \mathbf{0}}^{\mathbf{r}_L=\mathbf{R}}\exp\left[-A\int_0^L\dot{\mathbf{r}}_t^2\,dt+\frac{i\eta}{2L^2}\int_0^L\int_0^L (\mathbf{r}_t-\mathbf{r}_s)^2\,ds\,dt\right]{\cal D}\mathbf{r}.$$ Since the contribution of the path $\mathbf{r}_t$ to $I$ becomes most dominant when $\mathbf{r}_t$ makes the exponent stationary, we have $\mathbf{r}_t$ as the solution of the following Euler-Lagrange equation subject to the boundary conditions $\mathbf{r}_0 = \mathbf{0}$ and $\mathbf{r}_L = \mathbf{R}$: $$\ddot{\mathbf{r}}_t + \alpha^2\mathbf{r}_t - \beta\int_0^L\mathbf{r}_s\,ds=0,$$ where $\alpha^2 = \dfrac{i\eta}{AL}$ and $\beta = \dfrac{i\eta}{AL^2}$. With this solution, $I$ is expressed as  $$I = \exp(-A\mathbf{r}_L\dot{\mathbf{r}}_L).$$ I don't have any problem with the Euler-Lagrange equation but cannot obtain the last expression. Anyone help me out of this?","I am stuck weeks in the following calculation from a paper: Path integral $I$ is written as $$ I = \int_{\mathbf{r}_0 = \mathbf{0}}^{\mathbf{r}_L=\mathbf{R}}\exp\left[-A\int_0^L\dot{\mathbf{r}}_t^2\,dt+\frac{i\eta}{2L^2}\int_0^L\int_0^L (\mathbf{r}_t-\mathbf{r}_s)^2\,ds\,dt\right]{\cal D}\mathbf{r}.$$ Since the contribution of the path $\mathbf{r}_t$ to $I$ becomes most dominant when $\mathbf{r}_t$ makes the exponent stationary, we have $\mathbf{r}_t$ as the solution of the following Euler-Lagrange equation subject to the boundary conditions $\mathbf{r}_0 = \mathbf{0}$ and $\mathbf{r}_L = \mathbf{R}$: $$\ddot{\mathbf{r}}_t + \alpha^2\mathbf{r}_t - \beta\int_0^L\mathbf{r}_s\,ds=0,$$ where $\alpha^2 = \dfrac{i\eta}{AL}$ and $\beta = \dfrac{i\eta}{AL^2}$. With this solution, $I$ is expressed as  $$I = \exp(-A\mathbf{r}_L\dot{\mathbf{r}}_L).$$ I don't have any problem with the Euler-Lagrange equation but cannot obtain the last expression. Anyone help me out of this?",,"['functional-analysis', 'calculus-of-variations']"
43,Dual of $L^p$ space avoiding reflexivity and Radon-Nikodym Theorem,Dual of  space avoiding reflexivity and Radon-Nikodym Theorem,L^p,"Let $(X,\mathscr{F},\mu)$ be a measure space, with $\mu(X)<+\infty$, let $p\in [1,+\infty[$ and $q$ its Hlder-conjugate (that is, $1/p+1/q=1$). If $T\in (L^p(X,d\mu))^*$ is a continuous functional such that for every nonnegative $f\in L^p$ one have $T(f)\geq 0$, we wish to show that there exists a function $g\geq 0$ in $L^q$ such that  $$ T(f) = \int_X fg \; d\mu, \quad \forall f\in L^p. $$ The procedure is as follows: Show that $\lambda(A)=T(\chi_A)$ for every $A\in\mathscr{F}$ defines a finite positive measure on $X$. Let $\nu=\mu+\lambda$. Prove that $\mu(A)=0$ iff $\nu(A)=0$ and that $\mu(A)=0\Rightarrow \lambda(A)=0$. Let $S:L^2(X,d\nu)\to\mathbb{R}$ be defined by $$ S(f) = \int_X f \; d\lambda, \quad \forall f\in L^2(X, d\nu). $$ Show that $S$ is a well-defined linear continuouos functional and that $$ |S(f)|\leq \|f\|_{L^2(X,d\nu)}\sqrt{\lambda(X)}. $$ Using the Riesz's representation theorem for Hilbert spaces show that there exists a function $h\in L^2(X,d\nu)$ such that $$ \int_X f \; d\lambda = \int_X fh \; d\nu, \quad \forall f\in L^2(X,d\nu). $$ By considering $f=\chi_{h<0}$ and $f=\chi_{h\geq 1}$ show that $0\leq h<1$ $\nu$-a.e. and so we can replace $h$ by a function $\nu$-a.e. equal to $h$ taking values in the interval $[0,1[$. Show that for every function $f\geq 0$ in $L^p(X,d\mu)$ $$ T(f) = \int_X fh \; d\mu + \int_X fh \; d\lambda \quad \text{and} \quad T(f(1-h)) = \int_X fh \; d\mu. $$ Show that for every positive integer $k$ the function $\min(f/(1-h),k)$ belongs to $L^p(X,d\mu)$ and that $$ T(\min(f/(1-h),k)) = \int_X \min(f/(1-h),k) \; d\mu. $$ Take $g=h/(1-h)$ and conclude by making $k\to +\infty$. I'm stuck in the 7th step of this proof, so if someone can help me with a hint or a proof for this part, I'll be very gratefull. I don't need already known proofs as Brezis (using reflexivity) or Folland (using Radon-Nikodym Theorem) present.","Let $(X,\mathscr{F},\mu)$ be a measure space, with $\mu(X)<+\infty$, let $p\in [1,+\infty[$ and $q$ its Hlder-conjugate (that is, $1/p+1/q=1$). If $T\in (L^p(X,d\mu))^*$ is a continuous functional such that for every nonnegative $f\in L^p$ one have $T(f)\geq 0$, we wish to show that there exists a function $g\geq 0$ in $L^q$ such that  $$ T(f) = \int_X fg \; d\mu, \quad \forall f\in L^p. $$ The procedure is as follows: Show that $\lambda(A)=T(\chi_A)$ for every $A\in\mathscr{F}$ defines a finite positive measure on $X$. Let $\nu=\mu+\lambda$. Prove that $\mu(A)=0$ iff $\nu(A)=0$ and that $\mu(A)=0\Rightarrow \lambda(A)=0$. Let $S:L^2(X,d\nu)\to\mathbb{R}$ be defined by $$ S(f) = \int_X f \; d\lambda, \quad \forall f\in L^2(X, d\nu). $$ Show that $S$ is a well-defined linear continuouos functional and that $$ |S(f)|\leq \|f\|_{L^2(X,d\nu)}\sqrt{\lambda(X)}. $$ Using the Riesz's representation theorem for Hilbert spaces show that there exists a function $h\in L^2(X,d\nu)$ such that $$ \int_X f \; d\lambda = \int_X fh \; d\nu, \quad \forall f\in L^2(X,d\nu). $$ By considering $f=\chi_{h<0}$ and $f=\chi_{h\geq 1}$ show that $0\leq h<1$ $\nu$-a.e. and so we can replace $h$ by a function $\nu$-a.e. equal to $h$ taking values in the interval $[0,1[$. Show that for every function $f\geq 0$ in $L^p(X,d\mu)$ $$ T(f) = \int_X fh \; d\mu + \int_X fh \; d\lambda \quad \text{and} \quad T(f(1-h)) = \int_X fh \; d\mu. $$ Show that for every positive integer $k$ the function $\min(f/(1-h),k)$ belongs to $L^p(X,d\mu)$ and that $$ T(\min(f/(1-h),k)) = \int_X \min(f/(1-h),k) \; d\mu. $$ Take $g=h/(1-h)$ and conclude by making $k\to +\infty$. I'm stuck in the 7th step of this proof, so if someone can help me with a hint or a proof for this part, I'll be very gratefull. I don't need already known proofs as Brezis (using reflexivity) or Folland (using Radon-Nikodym Theorem) present.",,"['functional-analysis', 'lp-spaces', 'riesz-representation-theorem']"
44,Vanishing on the boundary for Sobolev subspace,Vanishing on the boundary for Sobolev subspace,,"Let $W^{m,p}(\Omega) = \{ f \in L^p(\Omega): \partial^\alpha f \in L^p(\Omega) \text{ for multi-indices } |\alpha| \leq m\}$, where $\partial$ denotes the weak derivative. Define $W_0^{m,p}$ to be the closure of $C_c^\infty(\Omega)$ in $W^{m,p}(\Omega)$. From my understanding, all elements of $W_0^{m,p}$ must vanish on the boundary of $\Omega$, as well as their derivatives up to order $m-1$. I don't quite understand why the derivative of order $m$ need not vanish. Take for example $m=1$ and $p=2$, with norm $||f||^2_{W^{1,2}}=||f||_{L^2} + ||D^1 f ||_{L^2}$. If $f \in W_0^{1,2}$, then there exists a sequence $f_k \in C^{\infty}_c$ such that $f_k \to f$ in $W^{1,2}$. Thus $$||f_k - f||_{W^{(1,2)}} = ||f-f_k||_{L^2} + ||D^1 (f-f_k) ||_{L^2} \to 0 \text{ as } k \to \infty$$ Given each $f_k$ has vanishing derivative on the boundary (more so, outside its support), I don't see why $f$ need not have vanishing derivative at the boundary. I guess this question is an extended question from this post: Some basics of Sobolev spaces","Let $W^{m,p}(\Omega) = \{ f \in L^p(\Omega): \partial^\alpha f \in L^p(\Omega) \text{ for multi-indices } |\alpha| \leq m\}$, where $\partial$ denotes the weak derivative. Define $W_0^{m,p}$ to be the closure of $C_c^\infty(\Omega)$ in $W^{m,p}(\Omega)$. From my understanding, all elements of $W_0^{m,p}$ must vanish on the boundary of $\Omega$, as well as their derivatives up to order $m-1$. I don't quite understand why the derivative of order $m$ need not vanish. Take for example $m=1$ and $p=2$, with norm $||f||^2_{W^{1,2}}=||f||_{L^2} + ||D^1 f ||_{L^2}$. If $f \in W_0^{1,2}$, then there exists a sequence $f_k \in C^{\infty}_c$ such that $f_k \to f$ in $W^{1,2}$. Thus $$||f_k - f||_{W^{(1,2)}} = ||f-f_k||_{L^2} + ||D^1 (f-f_k) ||_{L^2} \to 0 \text{ as } k \to \infty$$ Given each $f_k$ has vanishing derivative on the boundary (more so, outside its support), I don't see why $f$ need not have vanishing derivative at the boundary. I guess this question is an extended question from this post: Some basics of Sobolev spaces",,['functional-analysis']
45,Are points negligible for weak derivative in dimension 2?,Are points negligible for weak derivative in dimension 2?,,"Let $f\in L^2\mathbb{R}^d$ such that $\nabla f$ (in the distributional sense) coincides with an $L^2$-function outside of $0$. In which dimensions $d$ do we automatically have that $\nabla f\in (L^2\mathbb{R}^d)^d$? For $d=1$ it is wrong, e.g. the Heaviside functions provides a counterexample. For $d\ge3$, I can prove the statement: We have equality of $\nabla f$ with some $L^2$-function when testet against a test function which vanishes at $0$. In order to extend this result to arbitrary test functions, I want to use a collection of cutoff functions $\phi_\epsilon\colon \mathbb{R}^d\rightarrow B(0,1)$ with support in  $B(0,\epsilon)$. The crucial point that would allow to pass to the limit $\epsilon \rightarrow 0$ is that $$  \nabla\phi_\epsilon \rightarrow 0 \quad \text{weakly in } L^2. $$ For that I considered $\psi(t)=\exp(-t^2/(1-t^2))$ and $\phi_\epsilon(x)=\psi(\vert x \vert/\epsilon)$, which is the standard way of producing bumps. Then $$ \vert \nabla\phi_\epsilon(x)\vert \lesssim 1/\epsilon, \quad \text{supp}(\nabla \phi_\epsilon)\subset B(0,\epsilon), $$ hence for any $L^2$-function $g$ we have $$ \vert\int g \nabla \phi_\epsilon\vert \lesssim \frac{1}{\epsilon}\int_{B(0,\epsilon)} \vert g\vert \le \frac{1}{\epsilon} \Vert g\Vert_{L^2}\cdot \vert B(0,\epsilon)\vert^{1/2} = O(\epsilon^{d/2 -1}), $$ thus the result follows if $d \ge 3$. Question: Is the result true for $d=2$ or does there exist a counterexample?","Let $f\in L^2\mathbb{R}^d$ such that $\nabla f$ (in the distributional sense) coincides with an $L^2$-function outside of $0$. In which dimensions $d$ do we automatically have that $\nabla f\in (L^2\mathbb{R}^d)^d$? For $d=1$ it is wrong, e.g. the Heaviside functions provides a counterexample. For $d\ge3$, I can prove the statement: We have equality of $\nabla f$ with some $L^2$-function when testet against a test function which vanishes at $0$. In order to extend this result to arbitrary test functions, I want to use a collection of cutoff functions $\phi_\epsilon\colon \mathbb{R}^d\rightarrow B(0,1)$ with support in  $B(0,\epsilon)$. The crucial point that would allow to pass to the limit $\epsilon \rightarrow 0$ is that $$  \nabla\phi_\epsilon \rightarrow 0 \quad \text{weakly in } L^2. $$ For that I considered $\psi(t)=\exp(-t^2/(1-t^2))$ and $\phi_\epsilon(x)=\psi(\vert x \vert/\epsilon)$, which is the standard way of producing bumps. Then $$ \vert \nabla\phi_\epsilon(x)\vert \lesssim 1/\epsilon, \quad \text{supp}(\nabla \phi_\epsilon)\subset B(0,\epsilon), $$ hence for any $L^2$-function $g$ we have $$ \vert\int g \nabla \phi_\epsilon\vert \lesssim \frac{1}{\epsilon}\int_{B(0,\epsilon)} \vert g\vert \le \frac{1}{\epsilon} \Vert g\Vert_{L^2}\cdot \vert B(0,\epsilon)\vert^{1/2} = O(\epsilon^{d/2 -1}), $$ thus the result follows if $d \ge 3$. Question: Is the result true for $d=2$ or does there exist a counterexample?",,"['real-analysis', 'functional-analysis', 'sobolev-spaces', 'distribution-theory', 'weak-derivatives']"
46,Continuity of $T \mapsto \pi_{\operatorname{ker}T}$ w.r.t. the SOT,Continuity of  w.r.t. the SOT,T \mapsto \pi_{\operatorname{ker}T},"I came across the following technical question, to which I could not - after some time of thinking - find an answer: Let $\mathcal{U},\mathcal{H}$ be two real (in general infinite dimensional) separable Hilbert spaces. For some linear subspace $\bar{U} \subseteq \mathcal{U}$, let $\Pi_U$ denote the orthogonal projection on this subspace. The question is: Is the mapping $T \mapsto \Pi_{\operatorname{ker}T}$ continuous from $L(\mathcal{U},\mathcal{H})$ to $L(\mathcal{U})$ when both spaces are endowed with the strong operator topology? Any hints and thoughts on this are more than appreciated!","I came across the following technical question, to which I could not - after some time of thinking - find an answer: Let $\mathcal{U},\mathcal{H}$ be two real (in general infinite dimensional) separable Hilbert spaces. For some linear subspace $\bar{U} \subseteq \mathcal{U}$, let $\Pi_U$ denote the orthogonal projection on this subspace. The question is: Is the mapping $T \mapsto \Pi_{\operatorname{ker}T}$ continuous from $L(\mathcal{U},\mathcal{H})$ to $L(\mathcal{U})$ when both spaces are endowed with the strong operator topology? Any hints and thoughts on this are more than appreciated!",,"['functional-analysis', 'continuity', 'operator-theory']"
47,The proof about the dual space of $L^1$ and $L^{\infty}$,The proof about the dual space of  and,L^1 L^{\infty},"Let $(X,\mu,M)$ be a measure space with $X=[0,1]$, $\mu$=counting measure and $M$ is the $\sigma$-algebra contains all the subset $E$ of $[0,1]$ such that either $E$ or $E^c$(complement) is countable. Since $X$ is not $\sigma$-finite in $\mu$, $(L^1)^* \neq L^{\infty}$. The proof is as follow: Let $g(x)=x$ which is not measurable. Let $\Lambda f=\int fg d\mu$ for all $f\in L^1$, the integration here makes sense since we can prove $fg$ is measurable. We check that $\|\Lambda \|\leq1$ hence $\Lambda $ is a linear bounded functional. Suppose there is $h \in L^{\infty}$ such that $\Lambda f=\int fh d\mu=\int fg d\mu$, then we can choose some special $f$ to obtain $h=g$ a.e.. Thus we have a contradiction here since $h$ is measurable but $g$ is not. 2 Let $(X,\mu,M)$ be a measure space with $X=[0,1]$, $\mu$=Lebesgue measure. It is well known that $(L^{\infty})^* \neq L^{1}$. The proof is as follow: Let $C(X)$ be the space containing all continuous functions in $X$, which is proper closed subspace of $L^{\infty}$. Thus, by Hahn-Banach theorem, there is $\Lambda \in (L^{\infty})^*$ such that $\Lambda =0$ on $C(X)$ and $\Lambda \neq 0$ in $L^{\infty}$. Suppose there is $g\in L^1$ such that $\Lambda f= \int fgd\mu $. But $\Lambda f= \int fgd\mu=0 $ for all $f \in C(X)$, we can prove that $g=0$ which implies $\Lambda = 0$ contradicts to $\Lambda \neq 0$ in $L^{\infty}$. In both of the proof, it seems that they assume that the space $X$ is the dual of $Y$ if and only if they have a Riesz representation. But I am not sure if it is true. The above proof shows that the map $g \mapsto \Lambda$ by $\Phi(g)=\Lambda_g(f)=\int fg d\mu$ is not a isometry (not onto). But it does not mean that there exist no other isometry between the space.","Let $(X,\mu,M)$ be a measure space with $X=[0,1]$, $\mu$=counting measure and $M$ is the $\sigma$-algebra contains all the subset $E$ of $[0,1]$ such that either $E$ or $E^c$(complement) is countable. Since $X$ is not $\sigma$-finite in $\mu$, $(L^1)^* \neq L^{\infty}$. The proof is as follow: Let $g(x)=x$ which is not measurable. Let $\Lambda f=\int fg d\mu$ for all $f\in L^1$, the integration here makes sense since we can prove $fg$ is measurable. We check that $\|\Lambda \|\leq1$ hence $\Lambda $ is a linear bounded functional. Suppose there is $h \in L^{\infty}$ such that $\Lambda f=\int fh d\mu=\int fg d\mu$, then we can choose some special $f$ to obtain $h=g$ a.e.. Thus we have a contradiction here since $h$ is measurable but $g$ is not. 2 Let $(X,\mu,M)$ be a measure space with $X=[0,1]$, $\mu$=Lebesgue measure. It is well known that $(L^{\infty})^* \neq L^{1}$. The proof is as follow: Let $C(X)$ be the space containing all continuous functions in $X$, which is proper closed subspace of $L^{\infty}$. Thus, by Hahn-Banach theorem, there is $\Lambda \in (L^{\infty})^*$ such that $\Lambda =0$ on $C(X)$ and $\Lambda \neq 0$ in $L^{\infty}$. Suppose there is $g\in L^1$ such that $\Lambda f= \int fgd\mu $. But $\Lambda f= \int fgd\mu=0 $ for all $f \in C(X)$, we can prove that $g=0$ which implies $\Lambda = 0$ contradicts to $\Lambda \neq 0$ in $L^{\infty}$. In both of the proof, it seems that they assume that the space $X$ is the dual of $Y$ if and only if they have a Riesz representation. But I am not sure if it is true. The above proof shows that the map $g \mapsto \Lambda$ by $\Phi(g)=\Lambda_g(f)=\int fg d\mu$ is not a isometry (not onto). But it does not mean that there exist no other isometry between the space.",,"['real-analysis', 'functional-analysis', 'measure-theory', 'lp-spaces']"
48,Position operator,Position operator,,"I'm learning about unbounded operator densely defined on Hilbert spaces from ""Reed & Simon, Functional Analysis"". The first example was the position operator , which is defined as follows. Let $D(T) = \{ \varphi \in L^2(\mathbb R) : \int_\mathbb R x^2|\varphi(x)|^2 {\rm d}x < \infty \}$ and $T\colon D(T)\to L^2(\mathbb R)$ be given by $T\varphi(x) = x\varphi(x)$. Well, the domain $D(T)$ of $T$ that was chosen is the largest one for which the range of $T$ is in $L^2(\mathbb R)$. I am wondering: Why such operator is called position operator? What is its meaning for physicists? What is the intuition in defining such operator like this? Thanks in advance!","I'm learning about unbounded operator densely defined on Hilbert spaces from ""Reed & Simon, Functional Analysis"". The first example was the position operator , which is defined as follows. Let $D(T) = \{ \varphi \in L^2(\mathbb R) : \int_\mathbb R x^2|\varphi(x)|^2 {\rm d}x < \infty \}$ and $T\colon D(T)\to L^2(\mathbb R)$ be given by $T\varphi(x) = x\varphi(x)$. Well, the domain $D(T)$ of $T$ that was chosen is the largest one for which the range of $T$ is in $L^2(\mathbb R)$. I am wondering: Why such operator is called position operator? What is its meaning for physicists? What is the intuition in defining such operator like this? Thanks in advance!",,"['functional-analysis', 'reference-request', 'physics', 'intuition', 'mathematical-physics']"
49,"A closed convex set in a separable normed space is an intersection of closed regions, defined by hyperplanes","A closed convex set in a separable normed space is an intersection of closed regions, defined by hyperplanes",,"Let $X$ be a separable normed space and $K$ closed convex subset of $X$. Prove that for some $(x_n^{\star})\subset X^\star$ and $(\lambda_n)\subset \mathbb{R}$ we have $\displaystyle K=\bigcap_{n=1}^{\infty}L_n$, where $L_n=\{x\in X: x_n^\star(x)\leq \lambda_n\}.$ Attempt. Since X is separable and $X\setminus K$ is open, we have  $X\setminus K=\bigcup_{n=1}^{\infty}B(x_n,\epsilon_n)$ for some $x_n\in X,~\epsilon_n>0.$ By the separation theorem, for all $n$ we have $$\sup_{x\in K}x_n^\star(x)\leq \inf_{x\in B(x_n,\epsilon_n)}x_n^\star(x)$$ for some $x_n^\star\in X^\star$ and set $\displaystyle \lambda_n=\inf_{x\in B(x_n,\epsilon_n)}x_n^\star(x)$. Then for $L_n=\{x\in X: x_n^\star(x)\leq \lambda_n\}$, if $x\in K$ then $\displaystyle x_n^\star(x)\leq \sup_{x\in K}x_n^\star(x)\leq\lambda_n$ and $x\in L_n$ for all $n$. If $x\notin K$, then $x\in  B(x_n,\epsilon_n)$ for some  $n$ and $x_n^\star(x)\geq \lambda_n$. This is where I am stuck : we would like to have $x_n^\star(x)>\lambda_n$, so $x\notin L_n$ for this $n$. But i don't seem to able to prove this. Thanks in advance for the help.","Let $X$ be a separable normed space and $K$ closed convex subset of $X$. Prove that for some $(x_n^{\star})\subset X^\star$ and $(\lambda_n)\subset \mathbb{R}$ we have $\displaystyle K=\bigcap_{n=1}^{\infty}L_n$, where $L_n=\{x\in X: x_n^\star(x)\leq \lambda_n\}.$ Attempt. Since X is separable and $X\setminus K$ is open, we have  $X\setminus K=\bigcup_{n=1}^{\infty}B(x_n,\epsilon_n)$ for some $x_n\in X,~\epsilon_n>0.$ By the separation theorem, for all $n$ we have $$\sup_{x\in K}x_n^\star(x)\leq \inf_{x\in B(x_n,\epsilon_n)}x_n^\star(x)$$ for some $x_n^\star\in X^\star$ and set $\displaystyle \lambda_n=\inf_{x\in B(x_n,\epsilon_n)}x_n^\star(x)$. Then for $L_n=\{x\in X: x_n^\star(x)\leq \lambda_n\}$, if $x\in K$ then $\displaystyle x_n^\star(x)\leq \sup_{x\in K}x_n^\star(x)\leq\lambda_n$ and $x\in L_n$ for all $n$. If $x\notin K$, then $x\in  B(x_n,\epsilon_n)$ for some  $n$ and $x_n^\star(x)\geq \lambda_n$. This is where I am stuck : we would like to have $x_n^\star(x)>\lambda_n$, so $x\notin L_n$ for this $n$. But i don't seem to able to prove this. Thanks in advance for the help.",,"['real-analysis', 'functional-analysis', 'separation-axioms']"
50,Proving weak convergence without Vitali's convergence theorem.,Proving weak convergence without Vitali's convergence theorem.,,"Problem: Let $(f_{n})_{n=1}^{\infty}$ be sequence in $L^{2}([0,1])$ such that uniformly bounded, i.e., $\sup_{n \in \mathbb{N}}||f_{n}||_{L^{2}} =M < +\infty$ and $f_{n} \to f$ in measure. Then show that $f_{n} \to f$ weakly in $L^{2}$. I think I solve this problem without using Vitali's convergence theorem, however, hint of this problem says use the Vitali's theorem. Could you check that my attempt is okay? Please let me know this argument is right or wrong. My attempt: Since $L^{2}$ is reflexive (actually Hilbert space), it suffices to show that $\forall g \in L^{2}$, $\int f_{n}g \to \int fg$ as $n \to \infty$. Let $f_{n_{j}}$ be any subsequence of $f_{n}$. Then it also coverges in measure. So it has a subsequence $f_{n_{j_{k}}}$ converges to $f$ pointwise a.e. Also, by the Holder's inequality,   $$ \int|f_{n_{j_{k}}}g| \leq ||f_{n_{j_{k}}}||_{L^{2}}||g||_{L^{2}} \leq M||g||_{L^{2}} < +\infty. $$   Hence, by the Dominated Convergence theorem, $\lim_{k \to \infty}\int f_{n_{j_{k}}}g = \int fg.$ This shows that every subsequence of  $\int f_{n}g$ has convergent sub-subsequence to $\int fg$. Hence, $f_{n} \to f$ weakly in $L^{2}$.","Problem: Let $(f_{n})_{n=1}^{\infty}$ be sequence in $L^{2}([0,1])$ such that uniformly bounded, i.e., $\sup_{n \in \mathbb{N}}||f_{n}||_{L^{2}} =M < +\infty$ and $f_{n} \to f$ in measure. Then show that $f_{n} \to f$ weakly in $L^{2}$. I think I solve this problem without using Vitali's convergence theorem, however, hint of this problem says use the Vitali's theorem. Could you check that my attempt is okay? Please let me know this argument is right or wrong. My attempt: Since $L^{2}$ is reflexive (actually Hilbert space), it suffices to show that $\forall g \in L^{2}$, $\int f_{n}g \to \int fg$ as $n \to \infty$. Let $f_{n_{j}}$ be any subsequence of $f_{n}$. Then it also coverges in measure. So it has a subsequence $f_{n_{j_{k}}}$ converges to $f$ pointwise a.e. Also, by the Holder's inequality,   $$ \int|f_{n_{j_{k}}}g| \leq ||f_{n_{j_{k}}}||_{L^{2}}||g||_{L^{2}} \leq M||g||_{L^{2}} < +\infty. $$   Hence, by the Dominated Convergence theorem, $\lim_{k \to \infty}\int f_{n_{j_{k}}}g = \int fg.$ This shows that every subsequence of  $\int f_{n}g$ has convergent sub-subsequence to $\int fg$. Hence, $f_{n} \to f$ weakly in $L^{2}$.",,"['real-analysis', 'functional-analysis', 'lp-spaces']"
51,Gaussian Hilbert space from white noise,Gaussian Hilbert space from white noise,,"There are two common ways of obtaining Gaussian processes on Hilbert space: either from a random element of a larger space, or directly as a collection of random variables indexed by the Hilbert space. My question involves the passage from the former to the latter. Let $\Phi\to H$ be a rigged Hilbert space (a nuclear space densely embedded in Hilbert space) and let $\Phi'$ denote the continuous dual of $\Phi$. Embed $H$ in $\Phi'$ such that the duality pairing on $\Phi'\times \Phi$ and the inner product of $H$ are consistent. By the Minlos theorem there is a unique Borel measure $\mu$ (which we call the white noise measure ) on $\Phi'$ satisfying $$ \mu(\exp i\langle \cdot,f\rangle)=\exp \Bigl(-\tfrac12\langle f,f\rangle\Bigr),\qquad f\in \Phi. $$ A standard Gaussian process on $H$ is a collection of (centered) Gaussian random variables $\{X_f\colon f\in H\}$ defined on a common probability space $(\Omega,\nu)$ satisfying $\text{Cov}(X_f,X_g)=\langle f,g\rangle$. At the formal level, one obtains a standard Gaussian process by setting $(\Omega,\nu)=(\Phi',\mu)$ and $X_f=\langle \cdot,f\rangle$. However, it is not clear that this is well-defined since $\langle \cdot,\cdot\rangle$ was defined on $\Phi'\times \Phi$ and $H\times H$, but not on $\Phi'\times H$. Question. What is an explicit choice of $\{X_f\colon f\in H\}$ and $(\Omega,\nu)$ given $(\Phi',\mu)$ that gives a standard Gaussian process on $H$?","There are two common ways of obtaining Gaussian processes on Hilbert space: either from a random element of a larger space, or directly as a collection of random variables indexed by the Hilbert space. My question involves the passage from the former to the latter. Let $\Phi\to H$ be a rigged Hilbert space (a nuclear space densely embedded in Hilbert space) and let $\Phi'$ denote the continuous dual of $\Phi$. Embed $H$ in $\Phi'$ such that the duality pairing on $\Phi'\times \Phi$ and the inner product of $H$ are consistent. By the Minlos theorem there is a unique Borel measure $\mu$ (which we call the white noise measure ) on $\Phi'$ satisfying $$ \mu(\exp i\langle \cdot,f\rangle)=\exp \Bigl(-\tfrac12\langle f,f\rangle\Bigr),\qquad f\in \Phi. $$ A standard Gaussian process on $H$ is a collection of (centered) Gaussian random variables $\{X_f\colon f\in H\}$ defined on a common probability space $(\Omega,\nu)$ satisfying $\text{Cov}(X_f,X_g)=\langle f,g\rangle$. At the formal level, one obtains a standard Gaussian process by setting $(\Omega,\nu)=(\Phi',\mu)$ and $X_f=\langle \cdot,f\rangle$. However, it is not clear that this is well-defined since $\langle \cdot,\cdot\rangle$ was defined on $\Phi'\times \Phi$ and $H\times H$, but not on $\Phi'\times H$. Question. What is an explicit choice of $\{X_f\colon f\in H\}$ and $(\Omega,\nu)$ given $(\Phi',\mu)$ that gives a standard Gaussian process on $H$?",,"['functional-analysis', 'probability-theory', 'measure-theory', 'stochastic-processes']"
52,The Sequence of Eigenfunctions Of a Self-adjoint Operator is Complete,The Sequence of Eigenfunctions Of a Self-adjoint Operator is Complete,,"I saw this theorem somewhere and I have problem finding proof for the third part. Is it even true? If yes I'd be glad if someone could suggest me a book for reference or give me some hints for proving it myself: Consider the eigenvalue problem: $$\mathbf Ty_n=\lambda_nw(x)y_n(x),n=0,1,...,x\in\Omega.$$ Where $\mathbf T$ is linear and dense in $L^2([a,b]).$ If $\mathbf T$ is a self-adjoint operator then: 1. All the eigenvalues $\lambda_n$ are real. 2. Any two eigenfunctions of $\mathbf T$ belonging to different eigenvalues are orthogonal to each other. 3.The set of eigenfunctions is complete(dense) in the corresponding space.","I saw this theorem somewhere and I have problem finding proof for the third part. Is it even true? If yes I'd be glad if someone could suggest me a book for reference or give me some hints for proving it myself: Consider the eigenvalue problem: $$\mathbf Ty_n=\lambda_nw(x)y_n(x),n=0,1,...,x\in\Omega.$$ Where $\mathbf T$ is linear and dense in $L^2([a,b]).$ If $\mathbf T$ is a self-adjoint operator then: 1. All the eigenvalues $\lambda_n$ are real. 2. Any two eigenfunctions of $\mathbf T$ belonging to different eigenvalues are orthogonal to each other. 3.The set of eigenfunctions is complete(dense) in the corresponding space.",,"['functional-analysis', 'eigenvalues-eigenvectors', 'operator-theory']"
53,"$G=\{(x_n)\in l_1 : x_1 - 3x_2 = 0\}$, $f$ on G is defined by $f(x) = x_1$. Show $g(x) = (3/4)(x_1+x_2)$ is the unique Hahn-Banach extension for $f$.",",  on G is defined by . Show  is the unique Hahn-Banach extension for .",G=\{(x_n)\in l_1 : x_1 - 3x_2 = 0\} f f(x) = x_1 g(x) = (3/4)(x_1+x_2) f,"Let $G=\{(x_n)\in l_1 : x_1 - 3x_2 = 0\}$ and $f : G\to \mathbb{R}$ be defined by $f(x) = x_1$. Prove that $g(x) = (3/4)(x_1+x_2)$ is the unique Hahn-Banach extension for $f$. (I found $g$ is an extension for $f$, $||g||=3/4$. But how to prove $g$ is unique. By Hahn-Banach theorem, for a bounded linear functional $f$ defined on a subspace $G$ there exists an extension $g$ with $||f||=||g||$.)","Let $G=\{(x_n)\in l_1 : x_1 - 3x_2 = 0\}$ and $f : G\to \mathbb{R}$ be defined by $f(x) = x_1$. Prove that $g(x) = (3/4)(x_1+x_2)$ is the unique Hahn-Banach extension for $f$. (I found $g$ is an extension for $f$, $||g||=3/4$. But how to prove $g$ is unique. By Hahn-Banach theorem, for a bounded linear functional $f$ defined on a subspace $G$ there exists an extension $g$ with $||f||=||g||$.)",,"['functional-analysis', 'linear-transformations', 'dual-spaces']"
54,Is $ \{ a_n \}_{n \in \mathbb{N}} \mapsto \sum_{n \in N} a_n x^n$ a norm on the space of sequences $\mathbb{R}^\mathbb{Z}$?,Is  a norm on the space of sequences ?, \{ a_n \}_{n \in \mathbb{N}} \mapsto \sum_{n \in N} a_n x^n \mathbb{R}^\mathbb{Z},"The set of sequences of integers real numbers $\{ a_n \}_{n \in \mathbb{N}}$ with $a_n \in \mathbb{R}$ is not a Hilbert space but it is a Banach space vector space.  However, it can be a Hilbert space if I give it a norm such as : $$ \{ a_n \}_{n \in \mathbb{N}} \mapsto \sum_{n \in N} |a_n|^2 $$ and require that the norm be less than infinity.  What happens if I use a different equation. $$ \{ a_n \}_{n \in \mathbb{N}} \mapsto \sum_{n \in N} a_n x^n$$ Could this constitute a norm on the Banach space of sequence of integers (I forget the name in the textbook).  Maybe we need to say $0 < x < 1$ and / or put absolute value signs. ""Norm"" on an infinite dimensional space means we have to strict to convergent subsequences (or we could accept the case $||v|| = \infty$ as an outcome.  The vector $\vec{1} = (1,1,1,\dots)$ is has infinite norm in the first case. In terms of sequences spaces the norm I have written is $\ell^2(\mathbb{N})$.  Certainly there is an $\ell^1(\mathbb{N})$, even though it can diverge for many sequences. $$ \{ a_n \}_{n \in \mathbb{N}} \mapsto \sum_{n \in N} |a_n| $$ Then I am asking about a weighted version of $\ell^1$.  with $0 < x < 1$.  Is that still a norm? $$\ell^1(\mathbb{N}) \subseteq\left\{  \{ a_n \}_{n \in \mathbb{N}} :  \sum_{n \in N} |a_n| x^n < \infty \right\} $$ I believe the left side is strictly smaller than the right side.  There could also be an analogue of $\ell^2$: $$\ell^2(\mathbb{N}) \subseteq\left\{  \{ a_n \}_{n \in \mathbb{N}} :  \sum_{n \in N} |a_n|^2 x^n < \infty \right\} $$","The set of sequences of integers real numbers $\{ a_n \}_{n \in \mathbb{N}}$ with $a_n \in \mathbb{R}$ is not a Hilbert space but it is a Banach space vector space.  However, it can be a Hilbert space if I give it a norm such as : $$ \{ a_n \}_{n \in \mathbb{N}} \mapsto \sum_{n \in N} |a_n|^2 $$ and require that the norm be less than infinity.  What happens if I use a different equation. $$ \{ a_n \}_{n \in \mathbb{N}} \mapsto \sum_{n \in N} a_n x^n$$ Could this constitute a norm on the Banach space of sequence of integers (I forget the name in the textbook).  Maybe we need to say $0 < x < 1$ and / or put absolute value signs. ""Norm"" on an infinite dimensional space means we have to strict to convergent subsequences (or we could accept the case $||v|| = \infty$ as an outcome.  The vector $\vec{1} = (1,1,1,\dots)$ is has infinite norm in the first case. In terms of sequences spaces the norm I have written is $\ell^2(\mathbb{N})$.  Certainly there is an $\ell^1(\mathbb{N})$, even though it can diverge for many sequences. $$ \{ a_n \}_{n \in \mathbb{N}} \mapsto \sum_{n \in N} |a_n| $$ Then I am asking about a weighted version of $\ell^1$.  with $0 < x < 1$.  Is that still a norm? $$\ell^1(\mathbb{N}) \subseteq\left\{  \{ a_n \}_{n \in \mathbb{N}} :  \sum_{n \in N} |a_n| x^n < \infty \right\} $$ I believe the left side is strictly smaller than the right side.  There could also be an analogue of $\ell^2$: $$\ell^2(\mathbb{N}) \subseteq\left\{  \{ a_n \}_{n \in \mathbb{N}} :  \sum_{n \in N} |a_n|^2 x^n < \infty \right\} $$",,"['sequences-and-series', 'functional-analysis', 'hilbert-spaces', 'banach-spaces', 'normed-spaces']"
55,Is it correct way to show that $\mathcal C_{\mathcal C}(\mathbb R)$ is incomplete,Is it correct way to show that  is incomplete,\mathcal C_{\mathcal C}(\mathbb R),"This question below is inspired by this one . Based on the thought acquired from there, I was trying to solve the following question from Kesavan's Functional Analysis : Here is my attempt: For $n\in\mathbb N,$ define $f_n:\mathbb R\to\mathbb R$ by $f_n(x)=\dfrac{1}{m}\sin x,$ if $(m-1)\pi\le x\le m\pi$ for some $m\in\{1,2,...,n\}$ $=0,$ otherwise Choose $\epsilon>0.$ Then $\exists~p\in\mathbb N$ such that $\epsilon>\dfrac{1}{p+1}.$ Then for $m,n\ge p,$ $||f_m-f_n||=\sup_{x\in\mathbb R}|\dfrac{1}{\min\{m,n\}+1}|\le\dfrac{1}{p+1}<\epsilon$. So $\{f_n\}$ is Cauchy. If possible let $f_n\to f$ in $\mathcal C_{\mathcal C}(\mathbb R).$ Then $\text{supp}f\subset[a,b]$ for some $a,b\in R.$ Choose an odd natural number $k\in\mathbb N$ such that $k.\dfrac{\pi}{2}>b$ Then $f(k.\dfrac{\pi}{2})=0.$ But $f_n(k.\dfrac{\pi}{2})=\dfrac{1}{\frac{k-1}{2}+1}=\dfrac{2}{k+1}~\forall~n\ge k$ since $\frac{k-1}{2}\pi\le\frac{k}{2}\pi\le\frac{k+1}{2}\pi.$ So $||f_n-f||\ge|f_n(k.\dfrac{\pi}{2})-f(k.\dfrac{\pi}{2})|=\dfrac{2}{k+1}$ for all $n\ge k,$ a contradiction. Hence $\mathcal C_{\mathcal C}(\mathbb R)$ is not complete. Am I correct?","This question below is inspired by this one . Based on the thought acquired from there, I was trying to solve the following question from Kesavan's Functional Analysis : Here is my attempt: For $n\in\mathbb N,$ define $f_n:\mathbb R\to\mathbb R$ by $f_n(x)=\dfrac{1}{m}\sin x,$ if $(m-1)\pi\le x\le m\pi$ for some $m\in\{1,2,...,n\}$ $=0,$ otherwise Choose $\epsilon>0.$ Then $\exists~p\in\mathbb N$ such that $\epsilon>\dfrac{1}{p+1}.$ Then for $m,n\ge p,$ $||f_m-f_n||=\sup_{x\in\mathbb R}|\dfrac{1}{\min\{m,n\}+1}|\le\dfrac{1}{p+1}<\epsilon$. So $\{f_n\}$ is Cauchy. If possible let $f_n\to f$ in $\mathcal C_{\mathcal C}(\mathbb R).$ Then $\text{supp}f\subset[a,b]$ for some $a,b\in R.$ Choose an odd natural number $k\in\mathbb N$ such that $k.\dfrac{\pi}{2}>b$ Then $f(k.\dfrac{\pi}{2})=0.$ But $f_n(k.\dfrac{\pi}{2})=\dfrac{1}{\frac{k-1}{2}+1}=\dfrac{2}{k+1}~\forall~n\ge k$ since $\frac{k-1}{2}\pi\le\frac{k}{2}\pi\le\frac{k+1}{2}\pi.$ So $||f_n-f||\ge|f_n(k.\dfrac{\pi}{2})-f(k.\dfrac{\pi}{2})|=\dfrac{2}{k+1}$ for all $n\ge k,$ a contradiction. Hence $\mathcal C_{\mathcal C}(\mathbb R)$ is not complete. Am I correct?",,"['functional-analysis', 'banach-spaces', 'normed-spaces', 'complete-spaces']"
56,Example for not having closest polynomial of $f$ in some infinite dimensional vector space,Example for not having closest polynomial of  in some infinite dimensional vector space,f,"I have a question about some subspace of $C[0,1]$ in my text book introducing.( $C[0,1]$ is all the rea-valued continuous function space on [0.1]). here is contents in my textbooks Consider the subspace $S=\{h \in C[0,1] : h(0)=0\}$ and $T=\{h \in S : \int_{0}^{1}h(x) =0 \}$ Let $g(x)=x$ and consider the distance of $g$ to $T$. Note that $g(0)=0$ but $$\int_{0}^{1}g(x)dx=\frac{1}{2}$$ suppose that $h\in T$, and compute that $$\frac{1}{2}=\int_{0}^{1}(g(x)-h(x))dx \le \int_{0}^{1}||g-h||_\infty dx =||g-h||_\infty$$ if $||g-h||_\infty=\frac{1}{2}$ then this inequality must be an equality. this can occur only if    * $$g(x)-h(x)=||g-h||_\infty=\frac{1}{2}$$***** this implies that $h(x)=x-\frac{1}{2}.$ Note that $h$ does not lie in $T$ because $h(0)=0$ So the distance $\frac{1}{2}$ is not attained I am curious about how to happen ""$||g-h||_\infty = \frac{1}{2}$ $\Rightarrow$ $g(x)-h(x)=||g-h||_\infty=\frac{1}{2}$"" on the above text.the converse is trivial, but origin proposition need to be proved. please give me a hint about proposition","I have a question about some subspace of $C[0,1]$ in my text book introducing.( $C[0,1]$ is all the rea-valued continuous function space on [0.1]). here is contents in my textbooks Consider the subspace $S=\{h \in C[0,1] : h(0)=0\}$ and $T=\{h \in S : \int_{0}^{1}h(x) =0 \}$ Let $g(x)=x$ and consider the distance of $g$ to $T$. Note that $g(0)=0$ but $$\int_{0}^{1}g(x)dx=\frac{1}{2}$$ suppose that $h\in T$, and compute that $$\frac{1}{2}=\int_{0}^{1}(g(x)-h(x))dx \le \int_{0}^{1}||g-h||_\infty dx =||g-h||_\infty$$ if $||g-h||_\infty=\frac{1}{2}$ then this inequality must be an equality. this can occur only if    * $$g(x)-h(x)=||g-h||_\infty=\frac{1}{2}$$***** this implies that $h(x)=x-\frac{1}{2}.$ Note that $h$ does not lie in $T$ because $h(0)=0$ So the distance $\frac{1}{2}$ is not attained I am curious about how to happen ""$||g-h||_\infty = \frac{1}{2}$ $\Rightarrow$ $g(x)-h(x)=||g-h||_\infty=\frac{1}{2}$"" on the above text.the converse is trivial, but origin proposition need to be proved. please give me a hint about proposition",,"['functional-analysis', 'analysis', 'approximation']"
57,Reference request for centralizer of a Banach space,Reference request for centralizer of a Banach space,,"Definition : Let $(X,\|\cdot\|)$ be a Banach space over $\mathbb{R}.$ Let $ext(X^*)$ be the set of extreme points of the   closed unit ball of the continuous dual space $X^*.$ A continuous linear linear operator $T:X\to X$ is said to be a multiplier if every point $p$ in $ext(X*)$ is an eigenvector for the adjoint operator $T*:X^*\to X^*.$ That is, there exists a function   $a_T:ext(X^*)\to \mathbb{R}$ such that  $$p\circ T = a_T(p)p$$ for all   $p\in Ext(X^*).$ The centralizer of $X$, denoted $Z(X),$ is the set of all   multipliers on $X.$ To my knowledge, monographs which contains information on centralizer above are (Fleming and Jamison) Isometries in Banach Spaces: Vector-valued Function Spaces and Operator Spaces, Volume Two and (Behrends) M-Structure and the Banach-Stone Theorem . One article that I came across containing centralizer above is Aroujo's paper . Question : Does there exist any monograph, other than the two above, containing information on centralizer? If yes, may I know its   title? I am interested to know more about centralizer stuffs.","Definition : Let $(X,\|\cdot\|)$ be a Banach space over $\mathbb{R}.$ Let $ext(X^*)$ be the set of extreme points of the   closed unit ball of the continuous dual space $X^*.$ A continuous linear linear operator $T:X\to X$ is said to be a multiplier if every point $p$ in $ext(X*)$ is an eigenvector for the adjoint operator $T*:X^*\to X^*.$ That is, there exists a function   $a_T:ext(X^*)\to \mathbb{R}$ such that  $$p\circ T = a_T(p)p$$ for all   $p\in Ext(X^*).$ The centralizer of $X$, denoted $Z(X),$ is the set of all   multipliers on $X.$ To my knowledge, monographs which contains information on centralizer above are (Fleming and Jamison) Isometries in Banach Spaces: Vector-valued Function Spaces and Operator Spaces, Volume Two and (Behrends) M-Structure and the Banach-Stone Theorem . One article that I came across containing centralizer above is Aroujo's paper . Question : Does there exist any monograph, other than the two above, containing information on centralizer? If yes, may I know its   title? I am interested to know more about centralizer stuffs.",,"['real-analysis', 'functional-analysis', 'reference-request', 'banach-spaces']"
58,"Motivation for the norm in $W^{s,p}(\Omega )$",Motivation for the norm in,"W^{s,p}(\Omega )","Let $\Omega \subset \mathbb R^n$ a bounded domain with good condition. In the fractional Sobolev space,  $$W^{s,p}(\Omega )=\left\{u\in L^p(\Omega )\;\Bigg|\; \frac{|u(x)-u(y)|}{|x-y|^{\frac{n}{p}+s}}\in L^p(\Omega \times \Omega )\right\},$$ and we give to this space the norm $$\|u\|_{W^{1,s}(\Omega )}=\left(\|u\|_{L^p(\Omega )}^p+[u]_{W^{s,p}(\Omega )}^p\right)^{1/p},$$ where $$[u]_{W^{s,p}(\Omega )}=\left(\iint_{\Omega \times \Omega }\frac{|u(x)-u(y)|^p}{|x-y|^{n+sp}}dxdy\right)^{1/p}.$$ Question What is the motivation for $[u]_{W^{s,p}(\Omega )}$ ? Why such an expression ? And if $s=1$ (or maybe $s\to 1$), do we have that $$\lim_{s\to 1}\ [u]_{W^{s,p}(\Omega )}^p=\sum_{i=1}^n\left\|\frac{\partial u}{\partial x_i}\right\|^p_{L^p(\Omega )}\ \ ?$$   If not, I don't understand where would come from $[u]_{W^{s,p}(\Omega )}$.","Let $\Omega \subset \mathbb R^n$ a bounded domain with good condition. In the fractional Sobolev space,  $$W^{s,p}(\Omega )=\left\{u\in L^p(\Omega )\;\Bigg|\; \frac{|u(x)-u(y)|}{|x-y|^{\frac{n}{p}+s}}\in L^p(\Omega \times \Omega )\right\},$$ and we give to this space the norm $$\|u\|_{W^{1,s}(\Omega )}=\left(\|u\|_{L^p(\Omega )}^p+[u]_{W^{s,p}(\Omega )}^p\right)^{1/p},$$ where $$[u]_{W^{s,p}(\Omega )}=\left(\iint_{\Omega \times \Omega }\frac{|u(x)-u(y)|^p}{|x-y|^{n+sp}}dxdy\right)^{1/p}.$$ Question What is the motivation for $[u]_{W^{s,p}(\Omega )}$ ? Why such an expression ? And if $s=1$ (or maybe $s\to 1$), do we have that $$\lim_{s\to 1}\ [u]_{W^{s,p}(\Omega )}^p=\sum_{i=1}^n\left\|\frac{\partial u}{\partial x_i}\right\|^p_{L^p(\Omega )}\ \ ?$$   If not, I don't understand where would come from $[u]_{W^{s,p}(\Omega )}$.",,"['functional-analysis', 'operator-theory', 'sobolev-spaces', 'fractional-sobolev-spaces']"
59,Rudin - Functional analysis excercise 12.7,Rudin - Functional analysis excercise 12.7,,"Suppose $U \in \mathcal{B}(H)$ is unitary, and $\epsilon > 0$. Prove that scalars $\alpha_{0}, ..., \alpha_{n}$ can be chosen so that  \begin{align*} \| U^{-1} - \alpha_{0}I - \alpha_{1}U - ... - \alpha_{n}U^{n} \| < \epsilon \end{align*} if $\sigma(U)$ is a proper subset of the unit circle, but that this norm is never less than $1$ if $\sigma(U)$ covers the whole circle. For the first case, I have tried considering expressing $U$ in terms of $e^{iV}$ where $V$ is self adjoint, but can not proceed any further..Please Help!!!!!","Suppose $U \in \mathcal{B}(H)$ is unitary, and $\epsilon > 0$. Prove that scalars $\alpha_{0}, ..., \alpha_{n}$ can be chosen so that  \begin{align*} \| U^{-1} - \alpha_{0}I - \alpha_{1}U - ... - \alpha_{n}U^{n} \| < \epsilon \end{align*} if $\sigma(U)$ is a proper subset of the unit circle, but that this norm is never less than $1$ if $\sigma(U)$ covers the whole circle. For the first case, I have tried considering expressing $U$ in terms of $e^{iV}$ where $V$ is self adjoint, but can not proceed any further..Please Help!!!!!",,"['functional-analysis', 'spectral-theory']"
60,Weak topology = topology of pointwise convergence?,Weak topology = topology of pointwise convergence?,,"In a book, I found the following theorem (slightly simplified here): Consider $\mathbb{R}$ with the Borel $\sigma$-field. Let $B$ be the set of all measurable, bounded, real-valued functions on $\mathbb{R}$. Let $M$ be the set of all finite, signed measures on $\mathbb{R}$. Let the weak topology on $B$ be defined to be the coarsest topology such that all functions $B \to \mathbb{R},\quad f \mapsto \int f \mathrm{d} \mu,\quad \mu \in M$, are continuous. Theorem: On any $\| \cdot \|_\infty$-bounded subset of $B$, the weak topology and the topology of pointwise convergence coincide. Proof. Since $M$ contains all one-point probability measures, convergence in the weak topology implies pointwise convergence. Conversely, since sets bounded in $\| \cdot \|_\infty$-norm are also pointwise bounded, we can deduce from Lebesgue's dominated convergence theorem that pointwise convergence implies convergence in the weak topology. $\square$ Now, I do not understand why having the same convergent sequences in this case should imply that the topologies are the same. One direction seems clear: The topology of pointwise convergence is coarser than the weak topology, since the topology of pointwise convergence is the initial topology w.r.t. the family of all functionals $f \mapsto \int f \mathrm{d} \delta_x$. Yet I have not been able to get rid of the sequences argument in the other direction, and, thinking about how open sets in $B$ look like in the two topologies, I am not even sure anymore that the theorem is correct. Questions: (a) Is the theorem correct? (b) Is the proof correct, and, if yes, why?","In a book, I found the following theorem (slightly simplified here): Consider $\mathbb{R}$ with the Borel $\sigma$-field. Let $B$ be the set of all measurable, bounded, real-valued functions on $\mathbb{R}$. Let $M$ be the set of all finite, signed measures on $\mathbb{R}$. Let the weak topology on $B$ be defined to be the coarsest topology such that all functions $B \to \mathbb{R},\quad f \mapsto \int f \mathrm{d} \mu,\quad \mu \in M$, are continuous. Theorem: On any $\| \cdot \|_\infty$-bounded subset of $B$, the weak topology and the topology of pointwise convergence coincide. Proof. Since $M$ contains all one-point probability measures, convergence in the weak topology implies pointwise convergence. Conversely, since sets bounded in $\| \cdot \|_\infty$-norm are also pointwise bounded, we can deduce from Lebesgue's dominated convergence theorem that pointwise convergence implies convergence in the weak topology. $\square$ Now, I do not understand why having the same convergent sequences in this case should imply that the topologies are the same. One direction seems clear: The topology of pointwise convergence is coarser than the weak topology, since the topology of pointwise convergence is the initial topology w.r.t. the family of all functionals $f \mapsto \int f \mathrm{d} \delta_x$. Yet I have not been able to get rid of the sequences argument in the other direction, and, thinking about how open sets in $B$ look like in the two topologies, I am not even sure anymore that the theorem is correct. Questions: (a) Is the theorem correct? (b) Is the proof correct, and, if yes, why?",,"['functional-analysis', 'measure-theory', 'weak-convergence', 'pointwise-convergence']"
61,"Prove $C_b^j (I, \mathbb{R})$ is a complete space",Prove  is a complete space,"C_b^j (I, \mathbb{R})","Let I be a real interval (possibly infinite). Let $C_b^j (I, \mathbb{R}) = ${$f:I \rightarrow \mathbb{R}$ such that $f$ is j-times continuously differentiable and $f^{(m)}$ is bounded for $m \leq j$} Define a norm on this space to be $$||f||_{C^j} = \sum_{m=0}^j || f^{(m)}||_{sup}$$ I want to prove that this space (with this norm) is complete.  I know that any metric space can be naturally completed, so WLOG, if {$f_n$} is any Cauchy sequence in $C_b^j (I, \mathbb{R})$, we know there exists a limit function $f = lim_{n \rightarrow \infty} f_n$.  The task is to show that f is a member of $C_b^j (I, \mathbb{R})$. However, here I am having technical issues.  If I want to show that f is bounded, continuous, or differentiable (j times), then I end up needing epsilon delta arguments based on the distance between $f$ and some $f_n$.  However, the distance between $f$ and $f_n$ is based on the norm which already assumes that f is j times differentiable and that the derivatives are bounded.  Can anyone shed some light on this circular bit of logic?","Let I be a real interval (possibly infinite). Let $C_b^j (I, \mathbb{R}) = ${$f:I \rightarrow \mathbb{R}$ such that $f$ is j-times continuously differentiable and $f^{(m)}$ is bounded for $m \leq j$} Define a norm on this space to be $$||f||_{C^j} = \sum_{m=0}^j || f^{(m)}||_{sup}$$ I want to prove that this space (with this norm) is complete.  I know that any metric space can be naturally completed, so WLOG, if {$f_n$} is any Cauchy sequence in $C_b^j (I, \mathbb{R})$, we know there exists a limit function $f = lim_{n \rightarrow \infty} f_n$.  The task is to show that f is a member of $C_b^j (I, \mathbb{R})$. However, here I am having technical issues.  If I want to show that f is bounded, continuous, or differentiable (j times), then I end up needing epsilon delta arguments based on the distance between $f$ and some $f_n$.  However, the distance between $f$ and $f_n$ is based on the norm which already assumes that f is j times differentiable and that the derivatives are bounded.  Can anyone shed some light on this circular bit of logic?",,"['real-analysis', 'functional-analysis']"
62,"Characterization of weak convergence in $W^{1, p}(\Omega)$",Characterization of weak convergence in,"W^{1, p}(\Omega)","I have to prove that, given $\Omega\subset\mathbb{R}^n$ an open subset, $1\leq p<\infty$ and $\{u_n\}\subset W^{1, p}(\Omega)$, we have $u_n\rightharpoonup u$ in $W^{1, p}(\Omega)$ if and only if $u_n\rightharpoonup u$ in $L^p(\Omega)$ and $Du_n\rightharpoonup Du$ in $L^p(\Omega, \mathbb{R}^n)$. The hint of the exercise is to consider the map  $$ T:W^{1, p}(\Omega)\longrightarrow L^p(\Omega)\times L^p(\Omega, \mathbb{R}^n) $$ and prove that it is an isometry. So, my attempt is: by giving $W^{1,p}(\Omega)$ the norm $$ \|u\|_{W^{1, p}(\Omega)}=\left(\|u\|_{L^p(\Omega)}^p+\sum_{i=1}^n\left\|\frac{\partial u}{\partial x_i}\right\|_{L^p(\Omega)}^p\right)^{\frac{1}{p}} $$ and by giving $L^p(\Omega)\times L^p(\Omega, \mathbb{R}^n)$ the norm $$ (u, v) = \left(\|u\|^p_{L^p(\Omega)}+\sum_{i=1}^n\|v_i\|^p_{L^p(\Omega)}\right)^{\frac{1}{p}}, $$ the application $T$ is an isometry (right?). But now, how can I conclude that the equivalence of the assertions follows? The definition of weak convergence in $W^{1, p}(\Omega)$ is that $u_n\rightharpoonup u$ in $W^{1, p}(\Omega)$ if and only if $L(u_n)\longrightarrow L(u)$ for all $L\in (W^{1, p}(\Omega))'$. By the Riesz Representation Theorem on $W^{1, p}(\Omega)$ I know that there exist $f_0,\ldots,f_n\in L^{p'}(\Omega)$ such that, for every $L\in (W^{1, p}(\Omega))'$, $$ L(u)=\int_{\Omega}\left(f_0(x)u(x)+\sum_{i=1}^nf_i(x)\frac{\partial u}{\partial x_i}(x)\right)\ dx $$ for all $u\in W^{1, p}(\Omega)$ and $$ \|L\|_{(W^{1, p}(\Omega))'}=\left(\sum_{i=0}^n\|f_i\|_{L^{p'}(\Omega)}^{p'}\right)^{\frac{1}{p'}}. $$ Thank you","I have to prove that, given $\Omega\subset\mathbb{R}^n$ an open subset, $1\leq p<\infty$ and $\{u_n\}\subset W^{1, p}(\Omega)$, we have $u_n\rightharpoonup u$ in $W^{1, p}(\Omega)$ if and only if $u_n\rightharpoonup u$ in $L^p(\Omega)$ and $Du_n\rightharpoonup Du$ in $L^p(\Omega, \mathbb{R}^n)$. The hint of the exercise is to consider the map  $$ T:W^{1, p}(\Omega)\longrightarrow L^p(\Omega)\times L^p(\Omega, \mathbb{R}^n) $$ and prove that it is an isometry. So, my attempt is: by giving $W^{1,p}(\Omega)$ the norm $$ \|u\|_{W^{1, p}(\Omega)}=\left(\|u\|_{L^p(\Omega)}^p+\sum_{i=1}^n\left\|\frac{\partial u}{\partial x_i}\right\|_{L^p(\Omega)}^p\right)^{\frac{1}{p}} $$ and by giving $L^p(\Omega)\times L^p(\Omega, \mathbb{R}^n)$ the norm $$ (u, v) = \left(\|u\|^p_{L^p(\Omega)}+\sum_{i=1}^n\|v_i\|^p_{L^p(\Omega)}\right)^{\frac{1}{p}}, $$ the application $T$ is an isometry (right?). But now, how can I conclude that the equivalence of the assertions follows? The definition of weak convergence in $W^{1, p}(\Omega)$ is that $u_n\rightharpoonup u$ in $W^{1, p}(\Omega)$ if and only if $L(u_n)\longrightarrow L(u)$ for all $L\in (W^{1, p}(\Omega))'$. By the Riesz Representation Theorem on $W^{1, p}(\Omega)$ I know that there exist $f_0,\ldots,f_n\in L^{p'}(\Omega)$ such that, for every $L\in (W^{1, p}(\Omega))'$, $$ L(u)=\int_{\Omega}\left(f_0(x)u(x)+\sum_{i=1}^nf_i(x)\frac{\partial u}{\partial x_i}(x)\right)\ dx $$ for all $u\in W^{1, p}(\Omega)$ and $$ \|L\|_{(W^{1, p}(\Omega))'}=\left(\sum_{i=0}^n\|f_i\|_{L^{p'}(\Omega)}^{p'}\right)^{\frac{1}{p'}}. $$ Thank you",,"['functional-analysis', 'sobolev-spaces', 'lp-spaces', 'weak-convergence']"
63,Measurability of functions with values in Banach spaces,Measurability of functions with values in Banach spaces,,"The function $f:M\subset \mathbb{R}^n\to Y$ with values in Banach space is called measurable iff the following hold 1) The domain is measurable 2) There exists a sequence $(f_j)$ of step functions $f_j :M\to Y$ such that $$\lim_{j\to\infty}f_j(x)=f(x)$$ for almost every $x\in X$. Proposition $\,$ $f$ is measurable if the following hold: 1) $M$ is measurable and $Y$ is separable Banach space 2) $f$ is continuous almost everywhere What is the proof of that fact? One should construct a sequance of step functions. To do so, separability would be useful I guess. If $Y$ is separable we may take the values needed for step functions from the dense and countable subset of $Y$. How to define $M_i$? The very first idea that came to my mind is: if $a_i$ is a value from dense and contable subset of $Y$, why not to set $M_i=f^{-1}(a_i)$?","The function $f:M\subset \mathbb{R}^n\to Y$ with values in Banach space is called measurable iff the following hold 1) The domain is measurable 2) There exists a sequence $(f_j)$ of step functions $f_j :M\to Y$ such that $$\lim_{j\to\infty}f_j(x)=f(x)$$ for almost every $x\in X$. Proposition $\,$ $f$ is measurable if the following hold: 1) $M$ is measurable and $Y$ is separable Banach space 2) $f$ is continuous almost everywhere What is the proof of that fact? One should construct a sequance of step functions. To do so, separability would be useful I guess. If $Y$ is separable we may take the values needed for step functions from the dense and countable subset of $Y$. How to define $M_i$? The very first idea that came to my mind is: if $a_i$ is a value from dense and contable subset of $Y$, why not to set $M_i=f^{-1}(a_i)$?",,"['real-analysis', 'functional-analysis', 'measure-theory', 'bochner-spaces']"
64,A question about quasi-nilpotent operators,A question about quasi-nilpotent operators,,"Let $X$ be an infinite dimensional Banach space, and let $Q\in B(X)$ be a bounded quasi-nilpotent operator ($\sigma(Q)=\{0\}$). I am trying to prove that for every $\epsilon >0$ we can find an infinite dimensional subspace $Y\subset X$ such that the restriction $\left. Q\right\vert _{Y}:Y\rightarrow X$ of $Q$ to $Y$ is such that $\left\Vert \left.Q\right\vert _{Y}\right\Vert <\epsilon $.  Any help please ? Thank you.","Let $X$ be an infinite dimensional Banach space, and let $Q\in B(X)$ be a bounded quasi-nilpotent operator ($\sigma(Q)=\{0\}$). I am trying to prove that for every $\epsilon >0$ we can find an infinite dimensional subspace $Y\subset X$ such that the restriction $\left. Q\right\vert _{Y}:Y\rightarrow X$ of $Q$ to $Y$ is such that $\left\Vert \left.Q\right\vert _{Y}\right\Vert <\epsilon $.  Any help please ? Thank you.",,"['functional-analysis', 'operator-theory', 'linear-transformations', 'banach-spaces', 'spectral-theory']"
65,Hahn Banach Theorem: Transfinite Induction,Hahn Banach Theorem: Transfinite Induction,,"In the 4th book of Stein and Shakarchi, the statement of the Hahn-Banach theorem is as follows: Suppose $V_0$ is a linear subspace of $V$ and $p$ is a real sub-linear function on $V$, and that we are given a linear functional $l_0$ on $V_0$ that satisfies $$l_0(v)\leq p(v) ~\forall ~v\in V_0$$   Then $l_0$ can be extended to a linear functional $l$ on $V$ that satisfies    $$l(v) \leq p(v)~ \forall ~ v \in V$$ The proof given is as follows: We have already proved that if $l_0$ can be extended to a subspace W, then it can also be extended to the subspace $W+ w$ for any vector $w$. We well order all the vectors in $V$ that do not belong to $V_0$, and denote this ordering by $<$. Among these vectors, we call a vector $v$ extendable if the linear functional $l_0$ has an extension of the kind desired to the subspace spanned by $V_0.v$ and all the vectors $<v$. What we want to prove is in effect that all vectors not in $V_0$ are extendable. Assume the contrary, then because of the well-ordering we can find the smallest $v_1$ that is not extendable. Now if $V_0'$ is the space spanned by $V_0$ and all the vectors $<v_1$,then by assumption $l_0$ extends to $V_0'$. The previous step, with $V_0'$ in place of $V_0$ allows us then to extend $l_0$ to the subspace spanned by $V_0'$ and $v_1$, reaching a contradiction. This proves the theorem. Firstly, how does proving that all vectors not in $V_0$ are extendable imply that an extension exists for the whole space? We know that for any $v \notin V_0$ there exists a valid extension, say $l_v$ for the space spanned by $V_0,v$ and all vectors $<v$. Say we define $l(v):=l_v(v)$. Now we take an arbitrary $w>v$. Couldn't it happen that $l_w(v) \ne l_v(v)$. In this case how can one define $l$? Will it help if we change the definition of ""extendable"" to something like: A vector $v \notin V_0$ is called extendable if $l_0$ has an extension of the kind desired to the relevant subspace which is equal to the already defined functionals $l_w~ \forall w<v$ in the relevant subspaces? Also, how does one get the statement :""Now if $V_0'$ is the space spanned by $V_0$ and all the vectors $<v_1$,then by assumption $l_0$ extends to $V_0'$.""","In the 4th book of Stein and Shakarchi, the statement of the Hahn-Banach theorem is as follows: Suppose $V_0$ is a linear subspace of $V$ and $p$ is a real sub-linear function on $V$, and that we are given a linear functional $l_0$ on $V_0$ that satisfies $$l_0(v)\leq p(v) ~\forall ~v\in V_0$$   Then $l_0$ can be extended to a linear functional $l$ on $V$ that satisfies    $$l(v) \leq p(v)~ \forall ~ v \in V$$ The proof given is as follows: We have already proved that if $l_0$ can be extended to a subspace W, then it can also be extended to the subspace $W+ w$ for any vector $w$. We well order all the vectors in $V$ that do not belong to $V_0$, and denote this ordering by $<$. Among these vectors, we call a vector $v$ extendable if the linear functional $l_0$ has an extension of the kind desired to the subspace spanned by $V_0.v$ and all the vectors $<v$. What we want to prove is in effect that all vectors not in $V_0$ are extendable. Assume the contrary, then because of the well-ordering we can find the smallest $v_1$ that is not extendable. Now if $V_0'$ is the space spanned by $V_0$ and all the vectors $<v_1$,then by assumption $l_0$ extends to $V_0'$. The previous step, with $V_0'$ in place of $V_0$ allows us then to extend $l_0$ to the subspace spanned by $V_0'$ and $v_1$, reaching a contradiction. This proves the theorem. Firstly, how does proving that all vectors not in $V_0$ are extendable imply that an extension exists for the whole space? We know that for any $v \notin V_0$ there exists a valid extension, say $l_v$ for the space spanned by $V_0,v$ and all vectors $<v$. Say we define $l(v):=l_v(v)$. Now we take an arbitrary $w>v$. Couldn't it happen that $l_w(v) \ne l_v(v)$. In this case how can one define $l$? Will it help if we change the definition of ""extendable"" to something like: A vector $v \notin V_0$ is called extendable if $l_0$ has an extension of the kind desired to the relevant subspace which is equal to the already defined functionals $l_w~ \forall w<v$ in the relevant subspaces? Also, how does one get the statement :""Now if $V_0'$ is the space spanned by $V_0$ and all the vectors $<v_1$,then by assumption $l_0$ extends to $V_0'$.""",,"['functional-analysis', 'transfinite-induction']"
66,About the dual space of $V=\{u\in H_0^1(\Omega): \text{div}u=0 \}$ and its relations to $H^{-1}(\Omega)$.,About the dual space of  and its relations to .,V=\{u\in H_0^1(\Omega): \text{div}u=0 \} H^{-1}(\Omega),"I read about some things about $V=\{u\in H_0^1(\Omega): \text{div}u=0 \}$ and its dual space and I began to mix some of these things together. As a result: irritation. I hope you can help me out. First off, $V$ has the same topology as $H_0^1$ hence it is a Hilbert space with the scalar product $(u,v)_V=\int_\Omega \nabla u \cdot \nabla v dx$. Of course $V \subset H_0^1$ and $V$ is continuously embedded in $H_0^1$ as $\|v\|_{H_0^1}=\|v\|_V$ for all $v \in V$. Hence we also know that $H^{-1}$ is a subspace of $V'$. Girault & Raviart use in their book [Finite Element Approximation of the Navier-Stokes Equations] in Theorem 1.2 on page 158 that $f \in L^2(0,T;V')$, but by assumption we only know $f \in L^2(0,T;H^{-1})$. Hence they seem to use $L^2(0,T;H^{-1})\subset L^2(0,T;V')$. But for this wouldn't I need that $H^{-1}$ is continuously embedded in $V'$? To have the existence of a constant $C>0$ such that $$\|u\|_{L^1(0,T;V')}=\int_0^T \|u\|_{V'} dt \leq \int_0^T C \|u\|_{H^{-1}} dt =\|u\|_{L^1(0,T;H^{-1})}.$$ J. Simon proves in his paper [On the existence of the Pressure for Solutions of the Variational Navier-Stokes Equations]: '$H^{-1}(\Omega)^d$ and $V'$ themselves cannot be imbedded in the same Hausdorff space'. (p.226, 4th line) Now, I see a contradiction between [2.] and [3.]. If we have [2.] then $H^{-1}$ is continuously embedded in $V'$. Hence $H^{-1}$ and $V'$ can both be embedded in a Hausdorff-space, namely $V'$ (which is even a Banach space as it is the dual of a Hilbert space I thought). EDIT: After the great help in the comments, I know that $H^{-1}$ is continuously embedded in $V'$. Hence, Girault and Raviart can of course use the fact $L^2(0,T;H^{-1})\subset L^2(0,T;V')$. Now I look again at the paper of J. Simon and try to find out what he means. I will edit it here if I find something. Otherwise, I am always thankful for comments, answers and hints. EDIT2: Okay, I think I know the answer. The problem lies in the word ""embedding"" that I totally mixed, duh. Girault and Raviart only use that the restriction operator from $H^{-1}$ to $V'$ is continuous - nothing more! But Simon includes injectivity in his definition of imbedding. But, since $V$ is not dense in $H_0^1$, $H^{-1}$ can't be injective in $V'$ (with the restriction operator). If there are no objections in the next few days, I will post EDIT2 as an answer.","I read about some things about $V=\{u\in H_0^1(\Omega): \text{div}u=0 \}$ and its dual space and I began to mix some of these things together. As a result: irritation. I hope you can help me out. First off, $V$ has the same topology as $H_0^1$ hence it is a Hilbert space with the scalar product $(u,v)_V=\int_\Omega \nabla u \cdot \nabla v dx$. Of course $V \subset H_0^1$ and $V$ is continuously embedded in $H_0^1$ as $\|v\|_{H_0^1}=\|v\|_V$ for all $v \in V$. Hence we also know that $H^{-1}$ is a subspace of $V'$. Girault & Raviart use in their book [Finite Element Approximation of the Navier-Stokes Equations] in Theorem 1.2 on page 158 that $f \in L^2(0,T;V')$, but by assumption we only know $f \in L^2(0,T;H^{-1})$. Hence they seem to use $L^2(0,T;H^{-1})\subset L^2(0,T;V')$. But for this wouldn't I need that $H^{-1}$ is continuously embedded in $V'$? To have the existence of a constant $C>0$ such that $$\|u\|_{L^1(0,T;V')}=\int_0^T \|u\|_{V'} dt \leq \int_0^T C \|u\|_{H^{-1}} dt =\|u\|_{L^1(0,T;H^{-1})}.$$ J. Simon proves in his paper [On the existence of the Pressure for Solutions of the Variational Navier-Stokes Equations]: '$H^{-1}(\Omega)^d$ and $V'$ themselves cannot be imbedded in the same Hausdorff space'. (p.226, 4th line) Now, I see a contradiction between [2.] and [3.]. If we have [2.] then $H^{-1}$ is continuously embedded in $V'$. Hence $H^{-1}$ and $V'$ can both be embedded in a Hausdorff-space, namely $V'$ (which is even a Banach space as it is the dual of a Hilbert space I thought). EDIT: After the great help in the comments, I know that $H^{-1}$ is continuously embedded in $V'$. Hence, Girault and Raviart can of course use the fact $L^2(0,T;H^{-1})\subset L^2(0,T;V')$. Now I look again at the paper of J. Simon and try to find out what he means. I will edit it here if I find something. Otherwise, I am always thankful for comments, answers and hints. EDIT2: Okay, I think I know the answer. The problem lies in the word ""embedding"" that I totally mixed, duh. Girault and Raviart only use that the restriction operator from $H^{-1}$ to $V'$ is continuous - nothing more! But Simon includes injectivity in his definition of imbedding. But, since $V$ is not dense in $H_0^1$, $H^{-1}$ can't be injective in $V'$ (with the restriction operator). If there are no objections in the next few days, I will post EDIT2 as an answer.",,"['functional-analysis', 'sobolev-spaces', 'bochner-spaces', 'dual-spaces']"
67,An approximate eigenvalue that is not an eigenvalue,An approximate eigenvalue that is not an eigenvalue,,Could you please help me understand why $\lambda$ in the example below is not an eigenvalue? It's easy to see that each $\lambda_n$ is an eigenvalue but I am having difficulty ascertaining that their limit is not. Thank you in advance.,Could you please help me understand why $\lambda$ in the example below is not an eigenvalue? It's easy to see that each $\lambda_n$ is an eigenvalue but I am having difficulty ascertaining that their limit is not. Thank you in advance.,,"['functional-analysis', 'eigenvalues-eigenvectors', 'eigenfunctions']"
68,"In the space of bounded sequences which converge to $0$ there is one such sequence with a minimum distance to $(1,1,1,...)$",In the space of bounded sequences which converge to  there is one such sequence with a minimum distance to,"0 (1,1,1,...)","In $\ell^{\infty}$, there is exactly one sequence in $c_0$ that has the minimum distance to $(1,1,1,...)$. I believe this statement is actually false. However, I am having trouble formulating two such sequences with minimal distance to disprove the statement. Could anyone provide a counter-example? Or is this statement actually true?","In $\ell^{\infty}$, there is exactly one sequence in $c_0$ that has the minimum distance to $(1,1,1,...)$. I believe this statement is actually false. However, I am having trouble formulating two such sequences with minimal distance to disprove the statement. Could anyone provide a counter-example? Or is this statement actually true?",,['functional-analysis']
69,Bessel functions in resolution of second order ODE,Bessel functions in resolution of second order ODE,,"We have the following equation: $$ v''(r)+\frac{1}{r} v'(r) - w v(r)=0, \quad 0 < r < ; \\ v'(0)=0, $$ where $w$ is an positive constant. To resolve this problem, we introduce a new independent variable $\xi= \sqrt{w} r$ and a new function $z(\xi)= v(r)$. Then  $$ v'(r)=\sqrt{w} z'(\xi), \quad v''(r)= w z''(\xi),\\ z''(\xi)+\dfrac{1}{\xi} z'(\xi)- z(\xi)=0, \quad 0 < \xi <  \sqrt{w},\\ z'(0)=0. $$ Let $\zeta= i \xi (i=\sqrt{-1})$. Then  $$ y''(\zeta)+ \dfrac{1}{\zeta} y'(\zeta)+ y(\zeta)=0, \quad 0 < |\zeta| <  \sqrt{w}. $$ We have  $$ y(\zeta)= C J_0(\zeta), $$ where $J_0$ is the Bessel function of the zero order. We will use the asymptotic expansion  $$ J_0(\zeta)=1- \dfrac{\zeta^2}{2^2}+ \frac{\zeta^4}{2^2 \cdot 4^2}- \cdots $$ Solution of the problem writes  $$ z(\zeta)= y(\zeta)= C J_0(i \xi)= C \left(1+\frac{\xi^2}{2^2}+ \dfrac{\xi^4}{2^2 \cdot 4^2}+\cdots \right). $$ My questions are: i don't understand the method used to resolve this problem and why and how we introduce the Bessel functions?","We have the following equation: $$ v''(r)+\frac{1}{r} v'(r) - w v(r)=0, \quad 0 < r < ; \\ v'(0)=0, $$ where $w$ is an positive constant. To resolve this problem, we introduce a new independent variable $\xi= \sqrt{w} r$ and a new function $z(\xi)= v(r)$. Then  $$ v'(r)=\sqrt{w} z'(\xi), \quad v''(r)= w z''(\xi),\\ z''(\xi)+\dfrac{1}{\xi} z'(\xi)- z(\xi)=0, \quad 0 < \xi <  \sqrt{w},\\ z'(0)=0. $$ Let $\zeta= i \xi (i=\sqrt{-1})$. Then  $$ y''(\zeta)+ \dfrac{1}{\zeta} y'(\zeta)+ y(\zeta)=0, \quad 0 < |\zeta| <  \sqrt{w}. $$ We have  $$ y(\zeta)= C J_0(\zeta), $$ where $J_0$ is the Bessel function of the zero order. We will use the asymptotic expansion  $$ J_0(\zeta)=1- \dfrac{\zeta^2}{2^2}+ \frac{\zeta^4}{2^2 \cdot 4^2}- \cdots $$ Solution of the problem writes  $$ z(\zeta)= y(\zeta)= C J_0(i \xi)= C \left(1+\frac{\xi^2}{2^2}+ \dfrac{\xi^4}{2^2 \cdot 4^2}+\cdots \right). $$ My questions are: i don't understand the method used to resolve this problem and why and how we introduce the Bessel functions?",,"['functional-analysis', 'ordinary-differential-equations']"
70,How to solve the homogeneous differential equation?,How to solve the homogeneous differential equation?,,"EDITED WITH FINAL ANSWER: Solve the following differential equation: $$y' = \frac{2xy}{x^2-y^2}$$ Someone please help me to finish this problem. My solution so far: $$\frac{dy}{dx} = \frac{\frac{1}{x^2}(2xy)}{\frac{1}{x^2}(x^2-y^2)}$$ $$\frac{dy}{dx}= 2\frac{y}{x} * \frac{1}{1-{\frac{y^2}{x^2}}}$$ Let $v = \frac{y}{x}$, $y=vx$ then $\frac{dy}{dx} = v+x\frac{dv}{dx}$ $$\frac{dy}{dx} = \frac{2v}{1-v^2}$$ Setting the two equations equal to one another: $$v+x\frac{dv}{dx} = \frac{2v}{1-v^2}$$ $$x\frac{dv}{dx} = \frac{2v}{1-v^2} - \frac{v-v^3}{1-v^2}$$ $$x\frac{dv}{dx} = \frac{v+v^3}{1-v^2}$$ $$xdv = \frac{v+v^3}{1-v^2}dx$$ $$\frac{1-v^2}{v+v^3}dv = \frac{1}{x}dx$$ $$\int\frac{1-v^2}{v+v^3}dv = \int\frac{1}{x}dx$$ $$\ln \left|v\right|-\ln \left|v^2+1\right| = ln|x| + c$$ Substituting $\frac{y}{x}$ back for $v$: $$\ln \left|\frac{y}{x}\right|-\ln \left|\frac{y^2}{x^2}+1\right| = ln|x| + c$$ $$\ln \left|{y}\right|-\ln|x|-\ln \left|\frac{y^2}{x^2}+1\right| = ln|x| + c$$ Taking $e$ to everything we obtain: $$y - x - (\frac{y^2}{x^2}+1) = x + e^c$$ $$y - (\frac{y^2}{x^2}+1) = 2x + e^c$$ $$y - \frac{y^2}{x^2} - 1= 2x + e^c$$ $$y - \frac{y^2}{x^2} = 2x + e^c + 1$$ $$\frac{x^2y-y^2}{x^2} = 2x + e^c + 1$$ $$x^2y-y^2 = 2x^3 + x^2e^c + x^2$$ $$0 = y^2 - x^2y + 2x^3 + x^2e^c + x^2$$ Using the quadratic formula we obtain $$y = \frac{x^2\sqrt{x^4-8x^3-4x^2e^c-4x^2}}{2}$$ $$y = \frac{x^2\sqrt{x^2(x^2-8x-4e^c-4)}}{2}$$ $$y = \frac{x^2x\sqrt{x^2-8x-4e^c-4}}{2}$$","EDITED WITH FINAL ANSWER: Solve the following differential equation: $$y' = \frac{2xy}{x^2-y^2}$$ Someone please help me to finish this problem. My solution so far: $$\frac{dy}{dx} = \frac{\frac{1}{x^2}(2xy)}{\frac{1}{x^2}(x^2-y^2)}$$ $$\frac{dy}{dx}= 2\frac{y}{x} * \frac{1}{1-{\frac{y^2}{x^2}}}$$ Let $v = \frac{y}{x}$, $y=vx$ then $\frac{dy}{dx} = v+x\frac{dv}{dx}$ $$\frac{dy}{dx} = \frac{2v}{1-v^2}$$ Setting the two equations equal to one another: $$v+x\frac{dv}{dx} = \frac{2v}{1-v^2}$$ $$x\frac{dv}{dx} = \frac{2v}{1-v^2} - \frac{v-v^3}{1-v^2}$$ $$x\frac{dv}{dx} = \frac{v+v^3}{1-v^2}$$ $$xdv = \frac{v+v^3}{1-v^2}dx$$ $$\frac{1-v^2}{v+v^3}dv = \frac{1}{x}dx$$ $$\int\frac{1-v^2}{v+v^3}dv = \int\frac{1}{x}dx$$ $$\ln \left|v\right|-\ln \left|v^2+1\right| = ln|x| + c$$ Substituting $\frac{y}{x}$ back for $v$: $$\ln \left|\frac{y}{x}\right|-\ln \left|\frac{y^2}{x^2}+1\right| = ln|x| + c$$ $$\ln \left|{y}\right|-\ln|x|-\ln \left|\frac{y^2}{x^2}+1\right| = ln|x| + c$$ Taking $e$ to everything we obtain: $$y - x - (\frac{y^2}{x^2}+1) = x + e^c$$ $$y - (\frac{y^2}{x^2}+1) = 2x + e^c$$ $$y - \frac{y^2}{x^2} - 1= 2x + e^c$$ $$y - \frac{y^2}{x^2} = 2x + e^c + 1$$ $$\frac{x^2y-y^2}{x^2} = 2x + e^c + 1$$ $$x^2y-y^2 = 2x^3 + x^2e^c + x^2$$ $$0 = y^2 - x^2y + 2x^3 + x^2e^c + x^2$$ Using the quadratic formula we obtain $$y = \frac{x^2\sqrt{x^4-8x^3-4x^2e^c-4x^2}}{2}$$ $$y = \frac{x^2\sqrt{x^2(x^2-8x-4e^c-4)}}{2}$$ $$y = \frac{x^2x\sqrt{x^2-8x-4e^c-4}}{2}$$",,"['calculus', 'real-analysis', 'functional-analysis', 'ordinary-differential-equations']"
71,Spectral Measures - Spectral Theorem,Spectral Measures - Spectral Theorem,,"I'm trying to understand the following lemma from Reed and Simon's text: Lemma 1: Let $A$ be a bounded self-adjoint operator with cyclic vector $\psi$. Then, there is a unitary operator $U : \mathscr{H} \to L^2(\sigma(A), d\mu_{\psi})$ with $$(UAU^{-1}f)(\lambda) = \lambda f(\lambda).$$ Proof: Define $U$ by $U \phi(f) \psi \equiv f$, where $f$ is continuous. To show that $U$ is well defined, we compute $$\| \phi(f)\psi \|^2 = \langle \psi, \phi^{\ast}(f)\phi(f) \psi \rangle = \langle \psi, \phi(f\overline{f})\psi \rangle = \int \left| f(\lambda) \right|^2 d\mu_{\psi}.$$ I'm not understanding the last two equalities. Thanks","I'm trying to understand the following lemma from Reed and Simon's text: Lemma 1: Let $A$ be a bounded self-adjoint operator with cyclic vector $\psi$. Then, there is a unitary operator $U : \mathscr{H} \to L^2(\sigma(A), d\mu_{\psi})$ with $$(UAU^{-1}f)(\lambda) = \lambda f(\lambda).$$ Proof: Define $U$ by $U \phi(f) \psi \equiv f$, where $f$ is continuous. To show that $U$ is well defined, we compute $$\| \phi(f)\psi \|^2 = \langle \psi, \phi^{\ast}(f)\phi(f) \psi \rangle = \langle \psi, \phi(f\overline{f})\psi \rangle = \int \left| f(\lambda) \right|^2 d\mu_{\psi}.$$ I'm not understanding the last two equalities. Thanks",,"['functional-analysis', 'analysis']"
72,Second Isomorphism Theorem for Banach Spaces,Second Isomorphism Theorem for Banach Spaces,,"I am looking for verification of the following: Let $X$ be a Banach space. We will show that if $M, N$ and $M+N$ are closed subspaces of $X$, then $$\dfrac{M+N}{N} \cong \dfrac{M}{M \cap N},$$ using the map $\phi : M \to \dfrac{M+N}{N}$, where $x \mapsto x + N$. For any $x,y \in M$, we have that, $$\phi(x+y) = (x+y) + N = (x + N) + (y + N) = \phi(x) + \phi(y)$$ Also, for any $\lambda \in \mathbb{F}$, we have, $$\phi(\lambda x) = (\lambda x) + N = \lambda(x + N) = \lambda\phi(x)$$ Then: \begin{align*} \ker(\phi) &= \left\{ m \in M : \phi(m) = e_{\frac{M+N}{N}} \right\} \hspace{1cm}\text{should this be sent to 0 instead of the ""identity""?}\\ &= \left\{ m \in M : m + N = N \right\} \\ &= \left\{ m \in M : m \in N \right\} \\ &= M \cap N \end{align*} Then, we see that $\phi$ is a surjection because  $$(m+n) + N = m + N \in \dfrac{M+N}{N}$$ which is $\phi(m)$. Then, we use the first isomorphism theorem, that is, for any operator $\phi: X \to Y$ between Banach spaces, $$X / \ker(\phi) \cong \text{im} \phi \iff \text{im}\phi \space\ \text{is closed in} \space\ Y$$ So that here, $X = M$, $\ker(\phi) = M \cap N$, and $Y = \text{im}(\phi) = \dfrac{M+N}{N}$, so that the result follows.","I am looking for verification of the following: Let $X$ be a Banach space. We will show that if $M, N$ and $M+N$ are closed subspaces of $X$, then $$\dfrac{M+N}{N} \cong \dfrac{M}{M \cap N},$$ using the map $\phi : M \to \dfrac{M+N}{N}$, where $x \mapsto x + N$. For any $x,y \in M$, we have that, $$\phi(x+y) = (x+y) + N = (x + N) + (y + N) = \phi(x) + \phi(y)$$ Also, for any $\lambda \in \mathbb{F}$, we have, $$\phi(\lambda x) = (\lambda x) + N = \lambda(x + N) = \lambda\phi(x)$$ Then: \begin{align*} \ker(\phi) &= \left\{ m \in M : \phi(m) = e_{\frac{M+N}{N}} \right\} \hspace{1cm}\text{should this be sent to 0 instead of the ""identity""?}\\ &= \left\{ m \in M : m + N = N \right\} \\ &= \left\{ m \in M : m \in N \right\} \\ &= M \cap N \end{align*} Then, we see that $\phi$ is a surjection because  $$(m+n) + N = m + N \in \dfrac{M+N}{N}$$ which is $\phi(m)$. Then, we use the first isomorphism theorem, that is, for any operator $\phi: X \to Y$ between Banach spaces, $$X / \ker(\phi) \cong \text{im} \phi \iff \text{im}\phi \space\ \text{is closed in} \space\ Y$$ So that here, $X = M$, $\ker(\phi) = M \cap N$, and $Y = \text{im}(\phi) = \dfrac{M+N}{N}$, so that the result follows.",,"['functional-analysis', 'proof-verification', 'banach-spaces']"
73,"Are measures uniquely characterized by integrals against $\frac{1-x^\lambda}{1-x}$, $\lambda>0$","Are measures uniquely characterized by integrals against ,",\frac{1-x^\lambda}{1-x} \lambda>0,"Let $\mu$ be a Radon measure on $[0,1]$. Does $$ \int_{[0,1]} \frac{1-x^{\lambda}}{1-x}\mu(dx)=0 \quad\forall \lambda>0 $$ imply $\mu\equiv 0$? (With $\frac{1-x^{\lambda}}{1-x}:=\lambda$ for $x=1$.) EDIT: After some thought, I realized that, by the Hahn-Banach theorem, I may equivalently ask whether the span of $\{\frac{1-x^{\lambda}}{1-x}: \lambda>0\}$ is dense in $(C([0,1]),\|\cdot\|_{\sup})$, which shows that my attempt below is somehow the only way to go. Attempt : I can show the claim under the additional assumption that $1\not\in\text{supp}\, \mu$. Indeed, I may then define  $$ \tilde{\mu}(dx):=\frac{\mu(dx)}{1-x} $$ and proceed in two steps, using that $$ \int_{[0,1]}(1-x^{\lambda})\tilde{\mu}(dx)=0\quad\forall \lambda>0\quad (1) $$ 1) Letting $\lambda\to\infty$, we get uniform convergence $1-x^{\lambda}\to 1$ on $\text{supp}\, \tilde{\mu}$, thus $\int \tilde{\mu}=0$. 2) Thus, Equation (1) simplifies to  $$ \int_{[0,1]}x^\lambda\tilde{\mu}(dx)=0\quad\forall\lambda>0, $$  from which we conclude $\tilde{\mu}\equiv 0$ by the Weierstrass theorem and the fact that the signed measures on $[0,1]$ are the dual space of the continuous functions. Unfortunately, I was not able to get rid of the additional assumption. The obvious way would be to cut off $\mu$ to be supported on $[0,1-1/n]$. Equation (1) would then become $$ \int_{[0,1]}(1-x^\lambda)\tilde{\mu}_{n}(dx)=\int_{[1-1/n,1]}\frac{1-x^{\lambda}}{1-x}\mu(dx)\quad\forall\lambda>0  $$  I don't know how to proceed from here though.","Let $\mu$ be a Radon measure on $[0,1]$. Does $$ \int_{[0,1]} \frac{1-x^{\lambda}}{1-x}\mu(dx)=0 \quad\forall \lambda>0 $$ imply $\mu\equiv 0$? (With $\frac{1-x^{\lambda}}{1-x}:=\lambda$ for $x=1$.) EDIT: After some thought, I realized that, by the Hahn-Banach theorem, I may equivalently ask whether the span of $\{\frac{1-x^{\lambda}}{1-x}: \lambda>0\}$ is dense in $(C([0,1]),\|\cdot\|_{\sup})$, which shows that my attempt below is somehow the only way to go. Attempt : I can show the claim under the additional assumption that $1\not\in\text{supp}\, \mu$. Indeed, I may then define  $$ \tilde{\mu}(dx):=\frac{\mu(dx)}{1-x} $$ and proceed in two steps, using that $$ \int_{[0,1]}(1-x^{\lambda})\tilde{\mu}(dx)=0\quad\forall \lambda>0\quad (1) $$ 1) Letting $\lambda\to\infty$, we get uniform convergence $1-x^{\lambda}\to 1$ on $\text{supp}\, \tilde{\mu}$, thus $\int \tilde{\mu}=0$. 2) Thus, Equation (1) simplifies to  $$ \int_{[0,1]}x^\lambda\tilde{\mu}(dx)=0\quad\forall\lambda>0, $$  from which we conclude $\tilde{\mu}\equiv 0$ by the Weierstrass theorem and the fact that the signed measures on $[0,1]$ are the dual space of the continuous functions. Unfortunately, I was not able to get rid of the additional assumption. The obvious way would be to cut off $\mu$ to be supported on $[0,1-1/n]$. Equation (1) would then become $$ \int_{[0,1]}(1-x^\lambda)\tilde{\mu}_{n}(dx)=\int_{[1-1/n,1]}\frac{1-x^{\lambda}}{1-x}\mu(dx)\quad\forall\lambda>0  $$  I don't know how to proceed from here though.",,"['real-analysis', 'functional-analysis', 'probability-theory', 'measure-theory']"
74,A Lemma of Helly,A Lemma of Helly,,"I am asked to prove a lemma of Helly, and then to use it to obtain a proof of Goldstine's Theorem. Let $X$ be a Banach space, fix $f_i\in X^*,\   c_i\in \mathbb C, \ 0\le i\le  n.$ Then the following properties are equivalent: $1)$ For all $\epsilon >0$, there is an $x\in X$ such that $\left \| x \right \|\le 1, $ and $|f_ix-c_i|<\epsilon .$ $2)$ For all $(d_1,\cdots ,d_n)\in \mathbb C^n,\ \left | \sum_{i=1}^{n}d_ic_i \right |\le \left \|\sum_{i=1}^{n}d_if_i  \right \|.$ $1)\Rightarrow 2):$ $\left | \sum_{i=1}^{n}d_ic_i \right |=\left | \sum_{i=1}^{n}d_i(c_i-f_ix) \right |+\left | \sum_{i=1}^{n}d_ifx_i \right |<\epsilon\left | \sum_{i=1}^{n}d_i \right |+\left \|\sum_{i=1}^{n}d_if_i  \right \|.$ $2)\Rightarrow 1):$ (this is where I am having trouble. Is the following correct? Is there an easier way to do this? If $1)$ is false, then there is an $\epsilon >0$ such that for all $x\in X$ with $\left \| x \right \|\le 1,\ $ there is an $1\le i\le n$ such that $|f_ix-c_i|>\epsilon. $  This means that for each $x\in X; \left \| x \right \|\le 1$, there is a neighborhood $U_x$ of $(f_1(x),\cdots ,f_n(x))$ which does not contain $(c_1,\cdots, c_n).$ So there is a $\Lambda:\mathbb C^n\to \mathbb C$ and $r\in \mathbb R$ such that $\Re \Lambda u\le r\le \Re \Lambda (c_1,\cdots, c_n)$ for all $u\in \bigcup _{\left \| x \right \|\le 1}U_x.$ But since $\mathbb C^n$ is an inner product space, we can use Riesz to say that $\lambda x=\langle x,d\rangle$ for some $d=(d_1,\cdots, d_n).$ Then we have $\sup_{\left \| x \right \|\le 1} \Re \sum_{i=1}^{n}d_if_i(x))\le \Re \sum_{i=1}^{n}d_ic_i \le \left | \sum_{i=1}^{n}d_ic_i \right |$. Now we use the fact that $\left \{ \left \| x\le1 \right \| \right \}$ is balanced to prove that the left hand side of this inequality is $\left \|\sum_{i=1}^{n}d_if_i  \right \|,$ which would give the desired contradiction: in fact, in general, if $f$ is a continuous linear functional, then there is a sequence $(x_n)$ in $B(0,1)$ such that $|f(x_n)|\to \left \| f \right \|.$ Now choose $\beta_n\in \mathbb C$ so that $|\beta_n|=1$ and $\beta_nf(x_n)$ is real. Then, $|f(x_n)|=|\beta_nf(x_n)|=|f(\beta_nx_n)|\to \left \| f \right \|.$ Since $\beta_nx_n\in B(0,1),$ the result follows because now we have $\left \| f \right \|=\lim |f(\beta_nx_n)|\le\sup_{\left \| x\le 1 \right \|}\Re f(x)\le \sup_{\left \| x\le 1 \right \|}|f(x)|=\left \| f \right \|.$ From here Goldstine's Theorem follows because if we take a $\gamma \in X^{**}$ in the closed unit weak*- ball, consider the arbitrary neighborhood $U=\left \{ \eta\in X^{**}:|\eta f_i-\gamma f_i|<\epsilon ; 1\le i\le n \right \}$ and note that since $\left \| \gamma \right \|\le 1, $ and since for any $d_i\in \mathbb C$ we have $\left | \sum_{i=1}^{n}d_i \gamma (f_i) \right |=\left | \gamma \left ( \sum_{i=1}^{n}d_if_i \right ) \ \right |\le \left \|  \sum_{i=1}^{n}d_if_i \right \|, $ the lemma applies to show there is an $x\in X$ such that $\left \| x\right \|\le 1$ and $|f_ix-c_i|<\epsilon$ and all it remains to do is take $c_i=\gamma f_i$ to see that $f_ix\in U.$","I am asked to prove a lemma of Helly, and then to use it to obtain a proof of Goldstine's Theorem. Let $X$ be a Banach space, fix $f_i\in X^*,\   c_i\in \mathbb C, \ 0\le i\le  n.$ Then the following properties are equivalent: $1)$ For all $\epsilon >0$, there is an $x\in X$ such that $\left \| x \right \|\le 1, $ and $|f_ix-c_i|<\epsilon .$ $2)$ For all $(d_1,\cdots ,d_n)\in \mathbb C^n,\ \left | \sum_{i=1}^{n}d_ic_i \right |\le \left \|\sum_{i=1}^{n}d_if_i  \right \|.$ $1)\Rightarrow 2):$ $\left | \sum_{i=1}^{n}d_ic_i \right |=\left | \sum_{i=1}^{n}d_i(c_i-f_ix) \right |+\left | \sum_{i=1}^{n}d_ifx_i \right |<\epsilon\left | \sum_{i=1}^{n}d_i \right |+\left \|\sum_{i=1}^{n}d_if_i  \right \|.$ $2)\Rightarrow 1):$ (this is where I am having trouble. Is the following correct? Is there an easier way to do this? If $1)$ is false, then there is an $\epsilon >0$ such that for all $x\in X$ with $\left \| x \right \|\le 1,\ $ there is an $1\le i\le n$ such that $|f_ix-c_i|>\epsilon. $  This means that for each $x\in X; \left \| x \right \|\le 1$, there is a neighborhood $U_x$ of $(f_1(x),\cdots ,f_n(x))$ which does not contain $(c_1,\cdots, c_n).$ So there is a $\Lambda:\mathbb C^n\to \mathbb C$ and $r\in \mathbb R$ such that $\Re \Lambda u\le r\le \Re \Lambda (c_1,\cdots, c_n)$ for all $u\in \bigcup _{\left \| x \right \|\le 1}U_x.$ But since $\mathbb C^n$ is an inner product space, we can use Riesz to say that $\lambda x=\langle x,d\rangle$ for some $d=(d_1,\cdots, d_n).$ Then we have $\sup_{\left \| x \right \|\le 1} \Re \sum_{i=1}^{n}d_if_i(x))\le \Re \sum_{i=1}^{n}d_ic_i \le \left | \sum_{i=1}^{n}d_ic_i \right |$. Now we use the fact that $\left \{ \left \| x\le1 \right \| \right \}$ is balanced to prove that the left hand side of this inequality is $\left \|\sum_{i=1}^{n}d_if_i  \right \|,$ which would give the desired contradiction: in fact, in general, if $f$ is a continuous linear functional, then there is a sequence $(x_n)$ in $B(0,1)$ such that $|f(x_n)|\to \left \| f \right \|.$ Now choose $\beta_n\in \mathbb C$ so that $|\beta_n|=1$ and $\beta_nf(x_n)$ is real. Then, $|f(x_n)|=|\beta_nf(x_n)|=|f(\beta_nx_n)|\to \left \| f \right \|.$ Since $\beta_nx_n\in B(0,1),$ the result follows because now we have $\left \| f \right \|=\lim |f(\beta_nx_n)|\le\sup_{\left \| x\le 1 \right \|}\Re f(x)\le \sup_{\left \| x\le 1 \right \|}|f(x)|=\left \| f \right \|.$ From here Goldstine's Theorem follows because if we take a $\gamma \in X^{**}$ in the closed unit weak*- ball, consider the arbitrary neighborhood $U=\left \{ \eta\in X^{**}:|\eta f_i-\gamma f_i|<\epsilon ; 1\le i\le n \right \}$ and note that since $\left \| \gamma \right \|\le 1, $ and since for any $d_i\in \mathbb C$ we have $\left | \sum_{i=1}^{n}d_i \gamma (f_i) \right |=\left | \gamma \left ( \sum_{i=1}^{n}d_if_i \right ) \ \right |\le \left \|  \sum_{i=1}^{n}d_if_i \right \|, $ the lemma applies to show there is an $x\in X$ such that $\left \| x\right \|\le 1$ and $|f_ix-c_i|<\epsilon$ and all it remains to do is take $c_i=\gamma f_i$ to see that $f_ix\in U.$",,"['real-analysis', 'functional-analysis']"
75,How to relate the spectrum of a self-adjoint unbounded operator to the spectrum of a compact solution operator,How to relate the spectrum of a self-adjoint unbounded operator to the spectrum of a compact solution operator,,"Apologies if this is a trivial question. I'm having some trouble getting this straight in my head and would appreciate either an explanation or a pointer to a good reference. If this looks too long, here's a tl;dr Say I have a densely defined, bounded below, symmetric operator $T$ defined in a Hilbert space $H$. After shifting by some $\sigma$ one can show there exists a compact solution operator $K$. Why does the spectrum of $T$ coincide with (a shift of) the spectrum of $K$? Setup Let $H$ be a Hilbert space with inner product $(\cdot,\cdot)$. Let $T$ be a densely defined symmetric operator bounded below by some constant $c$, with $D = \operatorname{dom}(T)$. Define the form $\mathfrak{t}$ on $D\times D$ by $\mathfrak{t}(u,v) = (Tu,v)$. By symmetry $\mathfrak{t}$ is sesquilinear. Because $T$ is bounded below, if we choose $\sigma > \max\{1,-c\}$, the form defined on $D\times D$ by  $$ \mathfrak{t}_\sigma(u,v) = \mathfrak{t}(u,v) + \sigma\cdot(u,v) $$ is a positive definite Hermitian inner product on $D$. Define by $V$ the completion of $D$ with respect to $\mathfrak{t}_\sigma$. As $\mathfrak{t}_\sigma$ majorizes the norm on $H$ the injection $D\hookrightarrow H$ extends to a bounded embedding $\iota : V\hookrightarrow H$. Now assume that $V$ compactly embeds in $H$. (Here, if $T$ is not self-adjoint, take the Friedrichs extension.) Define an eigenvalue of $\mathfrak{t}$ to be any $\mu\in\mathbb{C}$ such that there exists some $u\in V$ so for all $v\in V$ we have $\mathfrak{t}(u,v) = \mu(u,v)$. I do not assume eigenvectors of $\mathfrak{t}$ are elements of $D$. By Riesz there exists a solution operator $S_\sigma: V^*\to V$. There is a bounded map $P:H\to V^*$ given by $u\mapsto (v\mapsto (u,v))$. (This is bounded by an application of Cauchy-Schwarz.) Composing these with the compact injection $\iota$ gives a compact map $K = S_\sigma P\iota:V\to V$. Note that $K$ has the property that for any $u\in V$,  $$ \mathfrak{t}_\sigma(Ku, v) = (u,v). $$ By the spectral theorem for compact operators, the spectrum of $K$ is discrete accumulating only at zero, and all nonzero elements of the spectrum are eigenvalues of finite multiplicity. Suppose $u$ is an eigenvalue of $K$ with eigenvector $\mu$. Then for arbitrary $v\in V$, \begin{align*} \mathfrak{t}(u,v) &= \mathfrak{t}_\sigma(u,v) - \sigma(u,v) \\   &= \frac{1}{\mu}\mathfrak{t}_\sigma(Ku, v) - \sigma(u,v) \\   &= \bigg(\frac{1}{\mu}-\sigma\bigg)(u,v) \end{align*} and so we have that $\frac{1 - \mu\sigma}{\mu}$ is an eigenvalue of $\mathfrak{t}$ with eigenvector $u$. Confusion Does it follow that   $$ \mbox{spectrum of } T = \bigg\{ \frac{1-\mu\sigma}{\mu}\ \bigg|\ \mu\in\mbox{spectrum of }K - \{0\} \bigg\}? $$ Thoughts I've been worrying at this for a couple of days and trying to prove something like the following: If $\lambda$ is in the resolvent set of $T$, then $\frac{1}{\sigma+\lambda}$ is in the resolvent set of $K$. This would immediately imply what I want modulo showing that that eigenvalues of $\mathfrak{t}$ are contained in the spectrum of $T$. (This is not a concern, see the note below.) Some algebra and head-scratching later, nothing has panned out -- it looks promising, but I haven't found the zinger yet. Maybe more to the point, I don't have enough intuition for where the algebra ought to go. I've started by assuming that $u$ is a $\mu$-eigenvector of $K$ in $H$ (after extending showing $K$ is symmetric and extending $K$ to a map $H\to H$ by density of $V$ and compactness), assuming that $\frac{1}{\lambda + \sigma}$ is in the resolvent set of $T$, and showing that we cannot have $\mu = \frac{1}{\lambda + \sigma}$, but this approach doesn't seem to rely on the assumption that the domain of $(T-\lambda)^{-1}$  is all of $H$, nor that it is bounded. I'm aware that this closely follows standard reference materials (Gilbarg-Trudinger Ch 8, Evans ch 6.5). In fact in Evans 6.5, at the end of the proof of Theorem 1, Evans writes, ""But observe as well that for $\eta\neq 0$, we have $Sw = \eta w$ if and only if $Lw = \lambda w$ for $\lambda = \frac{1}{\eta}$."" You might interpret this question as, ""why does the 'only if' follow?"" There are at least two other approaches I'm aware of to showing the spectrum of $T$ is discrete. One uses the Rayleigh quotient, shows there exists a lowest eigenvalue, and proceeds by induction to exhaust $H$ with orthonormal eigenspaces (e.g. Gilbarg-Trudinger, last section of ch 8). The other uses the theory of spectral measures (e.g. ch 13 of Green Rudin). I'd prefer to avoid those approaches if possible because I'd like to understand all three in as much detail as possible, in order to more thoroughly understand how they relate to each other. (My intuition is that the exhaustion proof is related to how this proof would go if one unpacked the spectral theorem for compact operators, while the theory of spectral measures is ""not homotopic,"" as it were, to this proof, but that's not yet clear to me.) Anyway, I'm missing a piece of the puzzle here and I'm not sure where the missing piece fits to look more closely for it. Notes In the context of PDEs I'm happy to assume the facts bolded in the setup: $V$ is related to a Sobolev space $W^{1,1}$ so Rellich-Kondrachov comes into play, and elliptic regularity gives that the eigenvectors of $\mathfrak{t}$ are in fact eigenvectors of $T$. If you've read this far, thanks for making it through my long-winded question :)","Apologies if this is a trivial question. I'm having some trouble getting this straight in my head and would appreciate either an explanation or a pointer to a good reference. If this looks too long, here's a tl;dr Say I have a densely defined, bounded below, symmetric operator $T$ defined in a Hilbert space $H$. After shifting by some $\sigma$ one can show there exists a compact solution operator $K$. Why does the spectrum of $T$ coincide with (a shift of) the spectrum of $K$? Setup Let $H$ be a Hilbert space with inner product $(\cdot,\cdot)$. Let $T$ be a densely defined symmetric operator bounded below by some constant $c$, with $D = \operatorname{dom}(T)$. Define the form $\mathfrak{t}$ on $D\times D$ by $\mathfrak{t}(u,v) = (Tu,v)$. By symmetry $\mathfrak{t}$ is sesquilinear. Because $T$ is bounded below, if we choose $\sigma > \max\{1,-c\}$, the form defined on $D\times D$ by  $$ \mathfrak{t}_\sigma(u,v) = \mathfrak{t}(u,v) + \sigma\cdot(u,v) $$ is a positive definite Hermitian inner product on $D$. Define by $V$ the completion of $D$ with respect to $\mathfrak{t}_\sigma$. As $\mathfrak{t}_\sigma$ majorizes the norm on $H$ the injection $D\hookrightarrow H$ extends to a bounded embedding $\iota : V\hookrightarrow H$. Now assume that $V$ compactly embeds in $H$. (Here, if $T$ is not self-adjoint, take the Friedrichs extension.) Define an eigenvalue of $\mathfrak{t}$ to be any $\mu\in\mathbb{C}$ such that there exists some $u\in V$ so for all $v\in V$ we have $\mathfrak{t}(u,v) = \mu(u,v)$. I do not assume eigenvectors of $\mathfrak{t}$ are elements of $D$. By Riesz there exists a solution operator $S_\sigma: V^*\to V$. There is a bounded map $P:H\to V^*$ given by $u\mapsto (v\mapsto (u,v))$. (This is bounded by an application of Cauchy-Schwarz.) Composing these with the compact injection $\iota$ gives a compact map $K = S_\sigma P\iota:V\to V$. Note that $K$ has the property that for any $u\in V$,  $$ \mathfrak{t}_\sigma(Ku, v) = (u,v). $$ By the spectral theorem for compact operators, the spectrum of $K$ is discrete accumulating only at zero, and all nonzero elements of the spectrum are eigenvalues of finite multiplicity. Suppose $u$ is an eigenvalue of $K$ with eigenvector $\mu$. Then for arbitrary $v\in V$, \begin{align*} \mathfrak{t}(u,v) &= \mathfrak{t}_\sigma(u,v) - \sigma(u,v) \\   &= \frac{1}{\mu}\mathfrak{t}_\sigma(Ku, v) - \sigma(u,v) \\   &= \bigg(\frac{1}{\mu}-\sigma\bigg)(u,v) \end{align*} and so we have that $\frac{1 - \mu\sigma}{\mu}$ is an eigenvalue of $\mathfrak{t}$ with eigenvector $u$. Confusion Does it follow that   $$ \mbox{spectrum of } T = \bigg\{ \frac{1-\mu\sigma}{\mu}\ \bigg|\ \mu\in\mbox{spectrum of }K - \{0\} \bigg\}? $$ Thoughts I've been worrying at this for a couple of days and trying to prove something like the following: If $\lambda$ is in the resolvent set of $T$, then $\frac{1}{\sigma+\lambda}$ is in the resolvent set of $K$. This would immediately imply what I want modulo showing that that eigenvalues of $\mathfrak{t}$ are contained in the spectrum of $T$. (This is not a concern, see the note below.) Some algebra and head-scratching later, nothing has panned out -- it looks promising, but I haven't found the zinger yet. Maybe more to the point, I don't have enough intuition for where the algebra ought to go. I've started by assuming that $u$ is a $\mu$-eigenvector of $K$ in $H$ (after extending showing $K$ is symmetric and extending $K$ to a map $H\to H$ by density of $V$ and compactness), assuming that $\frac{1}{\lambda + \sigma}$ is in the resolvent set of $T$, and showing that we cannot have $\mu = \frac{1}{\lambda + \sigma}$, but this approach doesn't seem to rely on the assumption that the domain of $(T-\lambda)^{-1}$  is all of $H$, nor that it is bounded. I'm aware that this closely follows standard reference materials (Gilbarg-Trudinger Ch 8, Evans ch 6.5). In fact in Evans 6.5, at the end of the proof of Theorem 1, Evans writes, ""But observe as well that for $\eta\neq 0$, we have $Sw = \eta w$ if and only if $Lw = \lambda w$ for $\lambda = \frac{1}{\eta}$."" You might interpret this question as, ""why does the 'only if' follow?"" There are at least two other approaches I'm aware of to showing the spectrum of $T$ is discrete. One uses the Rayleigh quotient, shows there exists a lowest eigenvalue, and proceeds by induction to exhaust $H$ with orthonormal eigenspaces (e.g. Gilbarg-Trudinger, last section of ch 8). The other uses the theory of spectral measures (e.g. ch 13 of Green Rudin). I'd prefer to avoid those approaches if possible because I'd like to understand all three in as much detail as possible, in order to more thoroughly understand how they relate to each other. (My intuition is that the exhaustion proof is related to how this proof would go if one unpacked the spectral theorem for compact operators, while the theory of spectral measures is ""not homotopic,"" as it were, to this proof, but that's not yet clear to me.) Anyway, I'm missing a piece of the puzzle here and I'm not sure where the missing piece fits to look more closely for it. Notes In the context of PDEs I'm happy to assume the facts bolded in the setup: $V$ is related to a Sobolev space $W^{1,1}$ so Rellich-Kondrachov comes into play, and elliptic regularity gives that the eigenvectors of $\mathfrak{t}$ are in fact eigenvectors of $T$. If you've read this far, thanks for making it through my long-winded question :)",,"['functional-analysis', 'eigenvalues-eigenvectors', 'spectral-theory', 'compact-operators', 'unbounded-operators']"
76,Non-unitary isometries in $B(H)$ are far away from invertible elements,Non-unitary isometries in  are far away from invertible elements,B(H),"I am struggling with the second part of this problem: Let $A$ be a unital Banach Algebra, and let $x$ and $y$ be elements in A such that that $xy=1_A$ and $yx\neq1_A$. (i) Let $z$ be an element in $A$, such that $\|x-z\|<\frac{1}{\|y\|}$. Show that $z$ is not invertible. (ii) Let $H$ be an infinite dimensional Hilbert space. Let $S\in B(H)$ be a non-unitary isometry (i.e. $S^*S=I$ and $SS^* \neq I$).  Show that $$1=\|S\|=\textrm{dist}(S,G(B(H))),$$ where $G(B(H))$ is the subset of invertible elements in $B(H)$. This is the progress, I have made: I have finished the proof of part (i) using the fact that if $\|1_A-x\|<1$, for some $x\in A$, then $x$ is invertible . Further, it is easy to see that $\|S\|=\|S^*\|=1$. It therefore follows from part (i) that $\textrm{dist}(S^*, G(B(H))\geq 1$. Finally, it is evident that neither $S$ nor $S^*$ are invertible. Any hints would be appreciated.","I am struggling with the second part of this problem: Let $A$ be a unital Banach Algebra, and let $x$ and $y$ be elements in A such that that $xy=1_A$ and $yx\neq1_A$. (i) Let $z$ be an element in $A$, such that $\|x-z\|<\frac{1}{\|y\|}$. Show that $z$ is not invertible. (ii) Let $H$ be an infinite dimensional Hilbert space. Let $S\in B(H)$ be a non-unitary isometry (i.e. $S^*S=I$ and $SS^* \neq I$).  Show that $$1=\|S\|=\textrm{dist}(S,G(B(H))),$$ where $G(B(H))$ is the subset of invertible elements in $B(H)$. This is the progress, I have made: I have finished the proof of part (i) using the fact that if $\|1_A-x\|<1$, for some $x\in A$, then $x$ is invertible . Further, it is easy to see that $\|S\|=\|S^*\|=1$. It therefore follows from part (i) that $\textrm{dist}(S^*, G(B(H))\geq 1$. Finally, it is evident that neither $S$ nor $S^*$ are invertible. Any hints would be appreciated.",,"['real-analysis', 'functional-analysis', 'operator-theory', 'operator-algebras', 'banach-algebras']"
77,Adjoint Norms in Banach Space,Adjoint Norms in Banach Space,,"If $T:X\to Y$ is a bounded linear transformation of Banach spaces $X$ and $Y$ , then there is an adjoint transformation $Y^*\to X^*$ that satifies $\langle Tx,y^* \rangle =\langle x,T^{*}y^* \rangle$ for all $x\in X$ and $y^*\in Y^*.$ One standard result is that $\left \| T \right \|=\left \| T^{*} \right \|.$ I have seen several proofs of this fact but I have a question about Rudin's. Rudin writes the following sequence of equalities: $\left \| T \right \|=\sup \left \{\langle Tx,y^{*} \rangle:\left \| x\leq 1 \right \|,\left \|  y^{*}\le 1 \right \| \right \}=\\\sup \left \{\langle x,T^*y^{*} \rangle:\left \| x\leq 1 \right \|,\left \|  y^{*}\le 1 \right \| \right \}=\\ \sup \left \{T^*y^{*}:\left \| y^{*}\le 1 \right \| \right \}=\left \| T^{*} \right \|$ which I take to mean: $\left \| T \right \|=\sup_\left \{ \left \| x \right \|\le 1 \right \}(\sup_\left \{ \left \| y^* \right \|\le 1 \right \}\left \{\langle Tx,y^{*} \rangle  \right \})=\\ \left \| T \right \|=\sup_\left \{ \left \| x \right \|\le 1 \right \}(\sup_\left \{ \left \| y^* \right \|\le 1 \right \}\left \{\langle x,T^*y^{*} \rangle  \right \})=\\ \left \| T \right \|=\sup_\left \{ \left \| y^* \right \|\le 1 \right \}(\sup_\left \{ \left \| x \right \|\le 1 \right \}\left \{\langle x,T^*y^{*} \rangle  \right \})=\left \| T^* \right \|.$ My question is simple: what justifies the interchange of the suprema? The way Rudin writes it, he seems to be claiming in general that that if $f\in \mathscr F, $ and $x\in X$ , then $\sup_{\mathscr F}\sup_{X}f(x)=\sup_{X}\sup_{\mathscr F}f(x), $ which is intuitive enough, but I have not been able to prove it.","If is a bounded linear transformation of Banach spaces and , then there is an adjoint transformation that satifies for all and One standard result is that I have seen several proofs of this fact but I have a question about Rudin's. Rudin writes the following sequence of equalities: which I take to mean: My question is simple: what justifies the interchange of the suprema? The way Rudin writes it, he seems to be claiming in general that that if and , then which is intuitive enough, but I have not been able to prove it.","T:X\to Y X Y Y^*\to X^* \langle Tx,y^* \rangle =\langle x,T^{*}y^* \rangle x\in X y^*\in Y^*. \left \| T \right \|=\left \| T^{*} \right \|. \left \| T \right \|=\sup \left \{\langle Tx,y^{*} \rangle:\left \| x\leq 1 \right \|,\left \|  y^{*}\le 1 \right \| \right \}=\\\sup \left \{\langle x,T^*y^{*} \rangle:\left \| x\leq 1 \right \|,\left \|  y^{*}\le 1 \right \| \right \}=\\ \sup \left \{T^*y^{*}:\left \| y^{*}\le 1 \right \| \right \}=\left \| T^{*} \right \| \left \| T \right \|=\sup_\left \{ \left \| x \right \|\le 1 \right \}(\sup_\left \{ \left \| y^* \right \|\le 1 \right \}\left \{\langle Tx,y^{*} \rangle  \right \})=\\ \left \| T \right \|=\sup_\left \{ \left \| x \right \|\le 1 \right \}(\sup_\left \{ \left \| y^* \right \|\le 1 \right \}\left \{\langle x,T^*y^{*} \rangle  \right \})=\\ \left \| T \right \|=\sup_\left \{ \left \| y^* \right \|\le 1 \right \}(\sup_\left \{ \left \| x \right \|\le 1 \right \}\left \{\langle x,T^*y^{*} \rangle  \right \})=\left \| T^* \right \|. f\in \mathscr F,  x\in X \sup_{\mathscr F}\sup_{X}f(x)=\sup_{X}\sup_{\mathscr F}f(x), ","['real-analysis', 'functional-analysis']"
78,Frchet derivative of a functional.,Frchet derivative of a functional.,,"I need to show that the functional defined by $$\int_{0}^{1} \cos(u(x)) dx$$ is not Frchet differentiable in $L^2((0,1))$. A hint is that the requested property of the remainder for a Frchet derivative is already violated for step functions. I was looking for similar questions and see that under a Taylor expansion, one can introduce a first order aproximation of $\cos(\cdot)$ and then take the second order residual as a step function. For example, If we consider the functional defined as $$y(x)=\sin(u(x)), \quad u \in L^2(\Omega)$$ then we have $$\sin(0+h(x))=\sin(0)+\cos(0)h(x)+\int_{0}^{1}[\cos(0+sh(x))-\cos(0)]ds$$ for $h\in L^p(0,1)$ given and $s \in (0,1)$. As far I know, that expansion is a  Taylor expansion in the integral form, but I dont see why we can put those terms in the integral, because I know that a expansion of this kind wold be of the form: $$f(a+x)=f(a)+f'(a)x+\int_{0}^{1}\frac{f''(a+sx)}{2}x^2ds.$$ Is the last expression wrong? And, would it be usefull a Taylor expansion to show the non-differentiability of the first functional given? A important fact is that in the $\sin(\cdot)$ example, taking $h$ as the step function $$h(x)=\begin{cases}1,&x\in [0,\epsilon]\\0,&x\in[\epsilon,1]\end{cases}$$ it's possible to show the non-differentiability of $\sin(\cdot)$ as a funtional, and I was thinking in a similar argument. Please any help with this question will be aprecciated. Thanks.","I need to show that the functional defined by $$\int_{0}^{1} \cos(u(x)) dx$$ is not Frchet differentiable in $L^2((0,1))$. A hint is that the requested property of the remainder for a Frchet derivative is already violated for step functions. I was looking for similar questions and see that under a Taylor expansion, one can introduce a first order aproximation of $\cos(\cdot)$ and then take the second order residual as a step function. For example, If we consider the functional defined as $$y(x)=\sin(u(x)), \quad u \in L^2(\Omega)$$ then we have $$\sin(0+h(x))=\sin(0)+\cos(0)h(x)+\int_{0}^{1}[\cos(0+sh(x))-\cos(0)]ds$$ for $h\in L^p(0,1)$ given and $s \in (0,1)$. As far I know, that expansion is a  Taylor expansion in the integral form, but I dont see why we can put those terms in the integral, because I know that a expansion of this kind wold be of the form: $$f(a+x)=f(a)+f'(a)x+\int_{0}^{1}\frac{f''(a+sx)}{2}x^2ds.$$ Is the last expression wrong? And, would it be usefull a Taylor expansion to show the non-differentiability of the first functional given? A important fact is that in the $\sin(\cdot)$ example, taking $h$ as the step function $$h(x)=\begin{cases}1,&x\in [0,\epsilon]\\0,&x\in[\epsilon,1]\end{cases}$$ it's possible to show the non-differentiability of $\sin(\cdot)$ as a funtional, and I was thinking in a similar argument. Please any help with this question will be aprecciated. Thanks.",,"['functional-analysis', 'analysis', 'optimization']"
79,Ultraproduct of partitions of an $L^p$-space isomorphic to itself?,Ultraproduct of partitions of an -space isomorphic to itself?,L^p,"Let $L$ be an $L^p$-space, $1<p<\infty$, associated with an arbitrary measure space $(X, \mathcal F, \mu)$. A semi-partition of $X$ is a finite disjoint collection of measurable sets with finite measures. Let $A$ be the set of all semi-partitions of $X$. We introduce a partial order into $A$ as $\alpha\leq \alpha'$ meaning that each set in $\alpha$ is a union of some sets in $\alpha'$, this makes $A$ a directed set. Let $E_\alpha:L\to L$ be the conditional expectation operator with respect to the semi-partition $\alpha$, mapping functions to their average values on the sets of $\alpha$ and to zero outside of these sets. We have that for fixed $f\in L$, the net $\{E_\alpha f\}$ converges to $f$. Let $L_\alpha$ be the range of $E_\alpha$ (this is a finite dimensional $L^p$-space). Let $\mathcal U$ be an ultrafilter on $A$ containing all sets $\{\alpha\in A: \alpha\geq\alpha_0\}$ for arbitrary $\alpha_0\in A$. I would like to know if the ultraproduct $W$ of the $L_\alpha$ with respect to $\mathcal U$ is isometric isomorphic to $L$ itself. If $A$ contains a largest element this is true of course, but my intuition says it should be also true otherwise. Here is what I have tried: Let $\phi: L\to W, f\mapsto (E_\alpha f)_\mathcal U$, where $(x_\alpha)_\mathcal U$ denotes the equivalence class of $(x_\alpha)_{\alpha\in A}$ with respect to $\mathcal U$. The mapping is isometric: $\|(E_\alpha f)_\mathcal U\| = \lim_\mathcal U\|E_\alpha f\|$. Since $\|E_\alpha f\|$ net-converges to $\|f\|$ this is also true for the ultrafilter limit due to the property we demanded of our ultrafilter. I have problems with the surjectivity. Let $(f_\alpha)_\mathcal U\in W$ be arbitrary. My idea was to define a functional on $\varphi$ on $L^q(X)$ via $\varphi(g) = \lim_\mathcal U\int_{L_\alpha}(E_\alpha g)f_\alpha$. The limit exists because $\int_{L_\alpha}(E_\alpha g)f_\alpha$ is bounded. This also shows that $\varphi$ is bounded, hence there is $f\in L$ which induces $\varphi$. I think that we should have $(E_\alpha f)_\mathcal U = (f_\alpha)_\mathcal U$, but I can't show that $\lim_\mathcal U\|E_\alpha f-f_\alpha\| = 0$. If $(f_\alpha)_\mathcal U$ is already of the form $(E_\alpha f)_\mathcal U$ then this construction with the functional indeed gives back $f$, but in the general case I don't see it. Can anybody help me?","Let $L$ be an $L^p$-space, $1<p<\infty$, associated with an arbitrary measure space $(X, \mathcal F, \mu)$. A semi-partition of $X$ is a finite disjoint collection of measurable sets with finite measures. Let $A$ be the set of all semi-partitions of $X$. We introduce a partial order into $A$ as $\alpha\leq \alpha'$ meaning that each set in $\alpha$ is a union of some sets in $\alpha'$, this makes $A$ a directed set. Let $E_\alpha:L\to L$ be the conditional expectation operator with respect to the semi-partition $\alpha$, mapping functions to their average values on the sets of $\alpha$ and to zero outside of these sets. We have that for fixed $f\in L$, the net $\{E_\alpha f\}$ converges to $f$. Let $L_\alpha$ be the range of $E_\alpha$ (this is a finite dimensional $L^p$-space). Let $\mathcal U$ be an ultrafilter on $A$ containing all sets $\{\alpha\in A: \alpha\geq\alpha_0\}$ for arbitrary $\alpha_0\in A$. I would like to know if the ultraproduct $W$ of the $L_\alpha$ with respect to $\mathcal U$ is isometric isomorphic to $L$ itself. If $A$ contains a largest element this is true of course, but my intuition says it should be also true otherwise. Here is what I have tried: Let $\phi: L\to W, f\mapsto (E_\alpha f)_\mathcal U$, where $(x_\alpha)_\mathcal U$ denotes the equivalence class of $(x_\alpha)_{\alpha\in A}$ with respect to $\mathcal U$. The mapping is isometric: $\|(E_\alpha f)_\mathcal U\| = \lim_\mathcal U\|E_\alpha f\|$. Since $\|E_\alpha f\|$ net-converges to $\|f\|$ this is also true for the ultrafilter limit due to the property we demanded of our ultrafilter. I have problems with the surjectivity. Let $(f_\alpha)_\mathcal U\in W$ be arbitrary. My idea was to define a functional on $\varphi$ on $L^q(X)$ via $\varphi(g) = \lim_\mathcal U\int_{L_\alpha}(E_\alpha g)f_\alpha$. The limit exists because $\int_{L_\alpha}(E_\alpha g)f_\alpha$ is bounded. This also shows that $\varphi$ is bounded, hence there is $f\in L$ which induces $\varphi$. I think that we should have $(E_\alpha f)_\mathcal U = (f_\alpha)_\mathcal U$, but I can't show that $\lim_\mathcal U\|E_\alpha f-f_\alpha\| = 0$. If $(f_\alpha)_\mathcal U$ is already of the form $(E_\alpha f)_\mathcal U$ then this construction with the functional indeed gives back $f$, but in the general case I don't see it. Can anybody help me?",,"['functional-analysis', 'lp-spaces', 'filters']"
80,Fourier transform in the tempered distributions space,Fourier transform in the tempered distributions space,,"I'm trying to prove that the following formula holds in the tempered distributions space $S'(\mathbb{R}^n)$: $$\widehat{\exp(-a|x|^2)}=\left(\frac{\pi}{a}\right)^{\frac{n}{2}}\exp\left(-\frac{\pi^2|\xi|^2}{a}\right)\hspace{0.1cm};\hspace{0.1cm} Re(a)\geq0\hspace{0.1cm};\hspace{0.1cm} a\neq0,$$ where $\sqrt{a}$ is defined as the branch with $Re(a)>0$. I have no problem when I consider $a\in\mathbb{R}$ because in that case $\exp(-a|x|^2)\in L^1(\mathbb{R}^n)$ and its Fourier transform (given by that formula) coincides with the transform in the tempered distributions sense. But I don't know what to do in the general case ($a\in \mathbb{C}$). The book (Introduction to Nonlinear Dispersive Equations - Linares,Ponce) suggest to use an analytic continuation argument, but I don't really see how to do it. Can anyone help me with this, please? Thanks.","I'm trying to prove that the following formula holds in the tempered distributions space $S'(\mathbb{R}^n)$: $$\widehat{\exp(-a|x|^2)}=\left(\frac{\pi}{a}\right)^{\frac{n}{2}}\exp\left(-\frac{\pi^2|\xi|^2}{a}\right)\hspace{0.1cm};\hspace{0.1cm} Re(a)\geq0\hspace{0.1cm};\hspace{0.1cm} a\neq0,$$ where $\sqrt{a}$ is defined as the branch with $Re(a)>0$. I have no problem when I consider $a\in\mathbb{R}$ because in that case $\exp(-a|x|^2)\in L^1(\mathbb{R}^n)$ and its Fourier transform (given by that formula) coincides with the transform in the tempered distributions sense. But I don't know what to do in the general case ($a\in \mathbb{C}$). The book (Introduction to Nonlinear Dispersive Equations - Linares,Ponce) suggest to use an analytic continuation argument, but I don't really see how to do it. Can anyone help me with this, please? Thanks.",,"['complex-analysis', 'functional-analysis', 'distribution-theory', 'fourier-transform']"
81,Graduate Functional Analysis Course : Background Required [closed],Graduate Functional Analysis Course : Background Required [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 7 years ago . Improve this question I'm an undergraduate with some measure and integration theory background. The undergrad analysis course I took at my institution covered the equivalent of the first two chapters of Folland's Real Analysis (ie introductory measure theory constructions, the convergence theorems for Lebesgue intergrals and Fubini-Tonelli) along with parts of chapters 5 and 6 on Elementary Functional Analysis and Lp Spaces. I was told that with some preparation I could probably take the second semester graduate introductory course on Functional Analysis that follows the first semester measure theory course. However, I am not certain what exactly I am to focus on in my preparation. Currently, I intend to do: 1) Chapter 3 of Folland which covers the Lebesgue Differentiation and the Radon-Nikodym Theorems since this appears to be the one major area which the intro grad course covered but I didn't. 2) Review Point Set Topology from say, Munkres (I'm mostly familiar with this up till Arzela Ascoli and Stone-Weierstrass). 3) Read an undergrad book like Kreyzig's. Is there anything else I should focus on? The Functional Analysis course will be using Functional Analysis, Sobolev Spaces and Partial Differential Equations by Brezis. We probably won't cover the entire book. Thanks in advance.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 7 years ago . Improve this question I'm an undergraduate with some measure and integration theory background. The undergrad analysis course I took at my institution covered the equivalent of the first two chapters of Folland's Real Analysis (ie introductory measure theory constructions, the convergence theorems for Lebesgue intergrals and Fubini-Tonelli) along with parts of chapters 5 and 6 on Elementary Functional Analysis and Lp Spaces. I was told that with some preparation I could probably take the second semester graduate introductory course on Functional Analysis that follows the first semester measure theory course. However, I am not certain what exactly I am to focus on in my preparation. Currently, I intend to do: 1) Chapter 3 of Folland which covers the Lebesgue Differentiation and the Radon-Nikodym Theorems since this appears to be the one major area which the intro grad course covered but I didn't. 2) Review Point Set Topology from say, Munkres (I'm mostly familiar with this up till Arzela Ascoli and Stone-Weierstrass). 3) Read an undergrad book like Kreyzig's. Is there anything else I should focus on? The Functional Analysis course will be using Functional Analysis, Sobolev Spaces and Partial Differential Equations by Brezis. We probably won't cover the entire book. Thanks in advance.",,"['real-analysis', 'functional-analysis', 'analysis', 'advice']"
82,Sobolev inequality for fractional derivative,Sobolev inequality for fractional derivative,,"Let $f:\mathbb R^n \to \mathbb C$ be a nice function. We define the gradient of $f$ as  $$\nabla f(x)= (\frac{\partial f(x)}{\partial x_1},..., \frac{\partial f(x)}{\partial x_n})$$ for $x=(x_1,...,x_n)\in \mathbb R^n.$ My Question is: How to define the fractional gradient(derivative)? Dose it make sense to talk of $|\nabla  |^{\alpha} f     \    \    (\alpha \in \mathbb R)$? We have the Sobolev embedding:  $\|u\|_{L^{p*}} \leq C \|\nabla u \|_{L^p}$ for $p<n$ and $p*=\frac{np}{n-p}.$ Can we use Sobolev embedding  to show: $\|u\|_{L^{\frac{2n^2(n+2)}{(n+4)(n-2)^2}}} \leq C_1 \||\nabla|^{4/n+2} u \|_{L^{\frac{2n^2(n+2)}{n^3-4n+16}}} (n\geq 3)$?","Let $f:\mathbb R^n \to \mathbb C$ be a nice function. We define the gradient of $f$ as  $$\nabla f(x)= (\frac{\partial f(x)}{\partial x_1},..., \frac{\partial f(x)}{\partial x_n})$$ for $x=(x_1,...,x_n)\in \mathbb R^n.$ My Question is: How to define the fractional gradient(derivative)? Dose it make sense to talk of $|\nabla  |^{\alpha} f     \    \    (\alpha \in \mathbb R)$? We have the Sobolev embedding:  $\|u\|_{L^{p*}} \leq C \|\nabla u \|_{L^p}$ for $p<n$ and $p*=\frac{np}{n-p}.$ Can we use Sobolev embedding  to show: $\|u\|_{L^{\frac{2n^2(n+2)}{(n+4)(n-2)^2}}} \leq C_1 \||\nabla|^{4/n+2} u \|_{L^{\frac{2n^2(n+2)}{n^3-4n+16}}} (n\geq 3)$?",,"['functional-analysis', 'analysis', 'derivatives', 'sobolev-spaces', 'fractional-calculus']"
83,Increasing sequence of closed subspaces of $L^2$ and error estimate of a product of orthogonal projections,Increasing sequence of closed subspaces of  and error estimate of a product of orthogonal projections,L^2,"We define  an increasing sequence of closed subspaces \begin{align*} V_{0} \subset V_{1} \subset V_{\ell} \subset \dots \end{align*} of $L^2(I)$ where $I=(0,x_{max})$, and each $V_{\ell}$ is equipped with a $L^{2}$ basis $ \{\phi^{\ell}_i\}_{i=1}^{m_{\ell}} $ ( $\phi^{\ell}_i$ are piece-wise polynomials of order $p$). I define two types of orthogonal projections \begin{align*}  P_L : & L^2(I) \rightarrow V_L\\ &f \rightarrow P_L f \end{align*} and  a nested projection for  $\ell\leq L$ \begin{align*}  P_{\ell,\ell-1} : & V_{\ell}\rightarrow V_{\ell-1}\\ & f^{\ell}  \rightarrow P_{\ell,\ell-1} f^{\ell}=\tilde{f}^{\ell-1} \end{align*} I have a problem where I am interested on deriving an upper bound estimate for \begin{align} \mathrel{\Big|} \mathrel{\Big|} \tilde{f}^{\ell}- \tilde{f}^{\ell-1} \mathrel{\Big|} \mathrel{\Big|}_{L^2(I)}^2. \end{align} A tentative way I did, is the following \begin{align} \mathrel{\Big|} \mathrel{\Big|} \tilde{f}^{\ell}- \tilde{f}^{\ell-1} \mathrel{\Big|} \mathrel{\Big|}_{L^2(I)}^2 &=  \mathrel{\Big|} \mathrel{\Big|}\tilde{f}^{\ell}- P_{\ell,\ell-1} \tilde{f}^{\ell} \mathrel{\Big|} \mathrel{\Big|}_{L^{2}(I)}^2 \nonumber \\  &\leq  \mathrel{\Big|} \mathrel{\Big|}\tilde{f}^{\ell}-  f \mathrel{\Big|} \mathrel{\Big|}_{L^{2}(I)}^2+  \mathrel{\Big|} \mathrel{\Big|} f - P_{\ell,\ell-1} \tilde{f}^{\ell} \mathrel{\Big|} \mathrel{\Big|}_{L^{2}(I)}^2 \nonumber\\  & \le \mathrel{\Big|} \mathrel{\Big|} \left( P_{\ell+1,\ell} \dots P_{L,L-1}  P_{L}\right) f -  f \mathrel{\Big|} \mathrel{\Big|}_{L^{2}(I)}^2+  \mathrel{\Big|} \mathrel{\Big|} f- \left( P_{\ell,\ell-1} P_{\ell+1,\ell} \dots P_{L,L-1} P_{L} \right) f \mathrel{\Big|} \mathrel{\Big|}_{L^{2}(I)}^2 \nonumber\\  & \le C h_{\ell-1}^{2p+2}  \mathrel{\Big|} \mathrel{\Big|} f^{(p+1)} \mathrel{\Big|} \mathrel{\Big|}_{L^{2}(I)}^2, \end{align} where $h_{\ell-1}$ is the discretization mesh size of $V_{\ell-1}$, and where I used the interpolation error estimate as an upper bound. I am not sure if this is correct, or if there are sharper bounds. Any hint or reference in this regard. Thanks.","We define  an increasing sequence of closed subspaces \begin{align*} V_{0} \subset V_{1} \subset V_{\ell} \subset \dots \end{align*} of $L^2(I)$ where $I=(0,x_{max})$, and each $V_{\ell}$ is equipped with a $L^{2}$ basis $ \{\phi^{\ell}_i\}_{i=1}^{m_{\ell}} $ ( $\phi^{\ell}_i$ are piece-wise polynomials of order $p$). I define two types of orthogonal projections \begin{align*}  P_L : & L^2(I) \rightarrow V_L\\ &f \rightarrow P_L f \end{align*} and  a nested projection for  $\ell\leq L$ \begin{align*}  P_{\ell,\ell-1} : & V_{\ell}\rightarrow V_{\ell-1}\\ & f^{\ell}  \rightarrow P_{\ell,\ell-1} f^{\ell}=\tilde{f}^{\ell-1} \end{align*} I have a problem where I am interested on deriving an upper bound estimate for \begin{align} \mathrel{\Big|} \mathrel{\Big|} \tilde{f}^{\ell}- \tilde{f}^{\ell-1} \mathrel{\Big|} \mathrel{\Big|}_{L^2(I)}^2. \end{align} A tentative way I did, is the following \begin{align} \mathrel{\Big|} \mathrel{\Big|} \tilde{f}^{\ell}- \tilde{f}^{\ell-1} \mathrel{\Big|} \mathrel{\Big|}_{L^2(I)}^2 &=  \mathrel{\Big|} \mathrel{\Big|}\tilde{f}^{\ell}- P_{\ell,\ell-1} \tilde{f}^{\ell} \mathrel{\Big|} \mathrel{\Big|}_{L^{2}(I)}^2 \nonumber \\  &\leq  \mathrel{\Big|} \mathrel{\Big|}\tilde{f}^{\ell}-  f \mathrel{\Big|} \mathrel{\Big|}_{L^{2}(I)}^2+  \mathrel{\Big|} \mathrel{\Big|} f - P_{\ell,\ell-1} \tilde{f}^{\ell} \mathrel{\Big|} \mathrel{\Big|}_{L^{2}(I)}^2 \nonumber\\  & \le \mathrel{\Big|} \mathrel{\Big|} \left( P_{\ell+1,\ell} \dots P_{L,L-1}  P_{L}\right) f -  f \mathrel{\Big|} \mathrel{\Big|}_{L^{2}(I)}^2+  \mathrel{\Big|} \mathrel{\Big|} f- \left( P_{\ell,\ell-1} P_{\ell+1,\ell} \dots P_{L,L-1} P_{L} \right) f \mathrel{\Big|} \mathrel{\Big|}_{L^{2}(I)}^2 \nonumber\\  & \le C h_{\ell-1}^{2p+2}  \mathrel{\Big|} \mathrel{\Big|} f^{(p+1)} \mathrel{\Big|} \mathrel{\Big|}_{L^{2}(I)}^2, \end{align} where $h_{\ell-1}$ is the discretization mesh size of $V_{\ell-1}$, and where I used the interpolation error estimate as an upper bound. I am not sure if this is correct, or if there are sharper bounds. Any hint or reference in this regard. Thanks.",,"['linear-algebra', 'functional-analysis', 'reference-request', 'operator-theory', 'interpolation']"
84,Complete vector sets in a Hilbert space and orthogonality.,Complete vector sets in a Hilbert space and orthogonality.,,"I am reading the following: The first definition says a vector set is complete if $(y,x_a) = 0 \implies y \;\forall a$. I thought the word ""complete"" usually refers to ""the biggest and filling up"" (Hilbert spaces are complete by definition, so that ""complete"" doesn't interfere with this one). What does the condition in the definition even mean? That $x_a$ is orthogonal to everything in the set? Does it have anything to do with extending a set to a basis? Also what is the decomposition property they are talking about? The complement decomposition? Can someone sketch an outline of the proof. I just want to read the ideas.","I am reading the following: The first definition says a vector set is complete if $(y,x_a) = 0 \implies y \;\forall a$. I thought the word ""complete"" usually refers to ""the biggest and filling up"" (Hilbert spaces are complete by definition, so that ""complete"" doesn't interfere with this one). What does the condition in the definition even mean? That $x_a$ is orthogonal to everything in the set? Does it have anything to do with extending a set to a basis? Also what is the decomposition property they are talking about? The complement decomposition? Can someone sketch an outline of the proof. I just want to read the ideas.",,"['functional-analysis', 'hilbert-spaces', 'complete-spaces']"
85,Construction of tensor products of Hilbert spaces with different scalar fields.,Construction of tensor products of Hilbert spaces with different scalar fields.,,"Let $H_1$ be a $\mathbb{C}$-Hilbert space and $H_2$ be a $\mathbb{R}$-Hilbert space, both assumed separable. For any $\phi\in H_1,\psi\in H_2$ we may define the (bilinear? by the wikipedia definition this can't be bilinear since $H_1$ and $H_2$ has different scalar fields) mapping $\phi\otimes \psi:H_1\times H_2 \to \mathbb{C}$ by $$ \phi\otimes \psi(x,y) = \langle x,\phi \rangle_1 \langle y,\psi \rangle_2 $$ We note that since only $H_1$ has a complex inner product, we can in general for $\lambda\in \mathbb{C}$ only say that $$ \lambda (\phi \otimes \psi) = (\lambda \psi)\otimes \psi \quad \quad \text{and not}\quad \quad  \lambda (\phi \otimes \psi) =  \psi\otimes (\lambda\psi). $$ Anyways I have proven (I can add the proof, but i think it is correct) that we can create an inner product of the space $\mathcal{E}$ of all finite linear combinations of the bilinear mappings considered above, by letting  $$ \langle \phi_1 \otimes \psi_1 , \phi_2\otimes \psi_2 \rangle = \langle\phi_1,\phi_2 \rangle_1 \langle\psi_1,\psi_2 \rangle_2 $$ and extending it to finite linear combinations in the following way $$ \Big\langle \sum_{i=1}^n a_i ( \phi_i\otimes \psi_i), \sum_{i=1}^m b_i (\beta_i \otimes \gamma_i) \Big\rangle  = \sum_{i=1}^n \sum_{j=1}^m a_i \bar{b}_j \langle \phi_i\otimes \psi_i, \beta_j \otimes \gamma_j \rangle $$ We now define $H_1\otimes H_2$ as the completion of the space of finite linear combinations with respect to the metric induced by the above inner product. Furhtermore it is well-known that the above inner product can be extended to $H_1\otimes H_2$, such that it satisfies $$ \langle \iota(\phi\otimes \psi), \iota(\gamma\otimes \beta) \rangle_{H_1\otimes H_2} = \langle\phi_1,\gamma\rangle_1 \langle \psi,\beta\rangle_2 $$ for any $\phi,\gamma\in H_1$ and $\psi,\beta\in H_2$, where $\iota$ is the linear isometric embedding into the completion. Question : Is this a valid construction of the tensor product of two Hilbert spaces with different fields? I could not find any mistake, but the ideas are taken from some notes that considers two real Hilbert spaces. Also every source of the tensor product of Hilbert spaces that I have encountered considers either two real or two complex Hilbert spaces, which is why I'm worried I have made a mistake. Edited to reflect only the above question remains.","Let $H_1$ be a $\mathbb{C}$-Hilbert space and $H_2$ be a $\mathbb{R}$-Hilbert space, both assumed separable. For any $\phi\in H_1,\psi\in H_2$ we may define the (bilinear? by the wikipedia definition this can't be bilinear since $H_1$ and $H_2$ has different scalar fields) mapping $\phi\otimes \psi:H_1\times H_2 \to \mathbb{C}$ by $$ \phi\otimes \psi(x,y) = \langle x,\phi \rangle_1 \langle y,\psi \rangle_2 $$ We note that since only $H_1$ has a complex inner product, we can in general for $\lambda\in \mathbb{C}$ only say that $$ \lambda (\phi \otimes \psi) = (\lambda \psi)\otimes \psi \quad \quad \text{and not}\quad \quad  \lambda (\phi \otimes \psi) =  \psi\otimes (\lambda\psi). $$ Anyways I have proven (I can add the proof, but i think it is correct) that we can create an inner product of the space $\mathcal{E}$ of all finite linear combinations of the bilinear mappings considered above, by letting  $$ \langle \phi_1 \otimes \psi_1 , \phi_2\otimes \psi_2 \rangle = \langle\phi_1,\phi_2 \rangle_1 \langle\psi_1,\psi_2 \rangle_2 $$ and extending it to finite linear combinations in the following way $$ \Big\langle \sum_{i=1}^n a_i ( \phi_i\otimes \psi_i), \sum_{i=1}^m b_i (\beta_i \otimes \gamma_i) \Big\rangle  = \sum_{i=1}^n \sum_{j=1}^m a_i \bar{b}_j \langle \phi_i\otimes \psi_i, \beta_j \otimes \gamma_j \rangle $$ We now define $H_1\otimes H_2$ as the completion of the space of finite linear combinations with respect to the metric induced by the above inner product. Furhtermore it is well-known that the above inner product can be extended to $H_1\otimes H_2$, such that it satisfies $$ \langle \iota(\phi\otimes \psi), \iota(\gamma\otimes \beta) \rangle_{H_1\otimes H_2} = \langle\phi_1,\gamma\rangle_1 \langle \psi,\beta\rangle_2 $$ for any $\phi,\gamma\in H_1$ and $\psi,\beta\in H_2$, where $\iota$ is the linear isometric embedding into the completion. Question : Is this a valid construction of the tensor product of two Hilbert spaces with different fields? I could not find any mistake, but the ideas are taken from some notes that considers two real Hilbert spaces. Also every source of the tensor product of Hilbert spaces that I have encountered considers either two real or two complex Hilbert spaces, which is why I'm worried I have made a mistake. Edited to reflect only the above question remains.",,"['functional-analysis', 'analysis', 'measure-theory', 'hilbert-spaces', 'tensor-products']"
86,Reflexive fractional Sobolev spaces,Reflexive fractional Sobolev spaces,,"I have the following question: Let $(u_n)_n \in W^{s,1}(0,\pi)$, $s\in (0,1)$, be a uniformly bounded sequence with the $||.||_{W^{s,1}(0,\pi)}$ norm (with Gagliardo semi-norm). From the compact embedding $W^{s,1}(0,\pi)\subset\subset L^1(0,\pi)$ there exists a subsequence $(u_{n_k})$ that strongly converges to $u\in L^1(0,\pi)$. Is the limit function $u$ in $W^{s,1}(0,\pi)$? I can't find any reference that proves or disproves the reflexivity of this space (although I think it is not). Any help will be appreciated! Thank you in advance. EDIT: Taking a look at the book ""Theory of functions spaces"" (Hans Triebel, 1983), page 178, the Besov space $B^{s,1}$ is not reflexive, and since the space $B^{s,1}$ coincides with the space $W^{s,1}$, I cannot use reflexivity to prove the above problem (am I saying it wrong...? I know very little to nothing about Besov spaces).","I have the following question: Let $(u_n)_n \in W^{s,1}(0,\pi)$, $s\in (0,1)$, be a uniformly bounded sequence with the $||.||_{W^{s,1}(0,\pi)}$ norm (with Gagliardo semi-norm). From the compact embedding $W^{s,1}(0,\pi)\subset\subset L^1(0,\pi)$ there exists a subsequence $(u_{n_k})$ that strongly converges to $u\in L^1(0,\pi)$. Is the limit function $u$ in $W^{s,1}(0,\pi)$? I can't find any reference that proves or disproves the reflexivity of this space (although I think it is not). Any help will be appreciated! Thank you in advance. EDIT: Taking a look at the book ""Theory of functions spaces"" (Hans Triebel, 1983), page 178, the Besov space $B^{s,1}$ is not reflexive, and since the space $B^{s,1}$ coincides with the space $W^{s,1}$, I cannot use reflexivity to prove the above problem (am I saying it wrong...? I know very little to nothing about Besov spaces).",,"['functional-analysis', 'banach-spaces', 'sobolev-spaces', 'calculus-of-variations', 'weak-convergence']"
87,Projection Lemma for Hilbert C-star modules,Projection Lemma for Hilbert C-star modules,,"The projection lemma (for example, Rudin's functional analysis book theorem 12.4) says that if $M$ is a closed vector subspace of a $\mathbb{C}$-Hilbert space $\mathcal{H}$ then $$\mathcal{H}=M\oplus M^{\perp}$$This lemma is a crucial ingredient in the proof of the Riesz representation theorem (stating that $\mathcal{H}\cong\mathcal{H}^{\ast}$), namely, in order to show surjectivity of the map $\mathcal{H}\to\mathcal{H}^{\ast}$. For Hilbert C-star modules (for example defined in Blackadar) the Riesz representation theorem does not hold. If so far what I wrote is correct, I was wondering exactly how the projection lemma fails for that case? Let $\mathscr{E}$ be an $A$-right-Hilbert module, where $A$ is a separable C-star algebra. Let $\mathscr{F}\subseteq\mathscr{E}$ be a closed (in the topology induced by the inner product) $A$-submodule. We would have liked to show that $$ \mathscr{E}=\mathscr{F}\oplus \mathscr{F}^{\perp} $$ and our goal is to see exactly why this fails for Hilbert $A$-modules.","The projection lemma (for example, Rudin's functional analysis book theorem 12.4) says that if $M$ is a closed vector subspace of a $\mathbb{C}$-Hilbert space $\mathcal{H}$ then $$\mathcal{H}=M\oplus M^{\perp}$$This lemma is a crucial ingredient in the proof of the Riesz representation theorem (stating that $\mathcal{H}\cong\mathcal{H}^{\ast}$), namely, in order to show surjectivity of the map $\mathcal{H}\to\mathcal{H}^{\ast}$. For Hilbert C-star modules (for example defined in Blackadar) the Riesz representation theorem does not hold. If so far what I wrote is correct, I was wondering exactly how the projection lemma fails for that case? Let $\mathscr{E}$ be an $A$-right-Hilbert module, where $A$ is a separable C-star algebra. Let $\mathscr{F}\subseteq\mathscr{E}$ be a closed (in the topology induced by the inner product) $A$-submodule. We would have liked to show that $$ \mathscr{E}=\mathscr{F}\oplus \mathscr{F}^{\perp} $$ and our goal is to see exactly why this fails for Hilbert $A$-modules.",,"['functional-analysis', 'hilbert-spaces', 'noncommutative-geometry', 'hilbert-modules']"
88,"sequence of bounded linear operators Tn: X to Y where X, Y are Banach spaces, ||Tn|| goes to infinity. Show there exists x ||Tnx|| goes to infinity","sequence of bounded linear operators Tn: X to Y where X, Y are Banach spaces, ||Tn|| goes to infinity. Show there exists x ||Tnx|| goes to infinity",,"Suppose that $X,Y$ are Banach spaces. Let $T_n: X\to Y$ be linear operators for all $n\in \mathbb{N}$ such that $\lim_{n\to\infty} \|T_n\|=\infty$. Show that $\exists x_0\in X$ such that $\lim_{n\to \infty}\|T_nx_0\|=\infty$. I know that if $\sup_{n\in\mathbb{N}}\|T_n\|=\infty$, then by Uniform Boundedness Prinnciple, $\exists x_0\in X$ such that $\sup_{n\in\mathbb{N}}\|T_nx_0\|=\infty$. But the question I am asking is stronger. I want to proceed by contradiction. Suppose there is a subsequence $\{T_{n_k}\}$ such that $\|T_{n_k}x_0\|$ is bounded. How to reach a contradiction?","Suppose that $X,Y$ are Banach spaces. Let $T_n: X\to Y$ be linear operators for all $n\in \mathbb{N}$ such that $\lim_{n\to\infty} \|T_n\|=\infty$. Show that $\exists x_0\in X$ such that $\lim_{n\to \infty}\|T_nx_0\|=\infty$. I know that if $\sup_{n\in\mathbb{N}}\|T_n\|=\infty$, then by Uniform Boundedness Prinnciple, $\exists x_0\in X$ such that $\sup_{n\in\mathbb{N}}\|T_nx_0\|=\infty$. But the question I am asking is stronger. I want to proceed by contradiction. Suppose there is a subsequence $\{T_{n_k}\}$ such that $\|T_{n_k}x_0\|$ is bounded. How to reach a contradiction?",,['functional-analysis']
89,Two versions of the Fredholm alternatives,Two versions of the Fredholm alternatives,,"Here are two version of the Fredholm alternatives among which I would like to know the relation: One version is from the appendix of Evans's Partial Differential Equations: Here $H$ is a Hilbert space. Another version is from an old post in Terry Tao's blog: Having gone through the details of each of the theorems above, I have the following questions : Can one get Theorem 5 from Theorem 1 by making $\lambda=1$? Are these two versions of the Fredholm alternatives equivalent when $X$ is assumed to be a Hilbert space in Theorem 1?","Here are two version of the Fredholm alternatives among which I would like to know the relation: One version is from the appendix of Evans's Partial Differential Equations: Here $H$ is a Hilbert space. Another version is from an old post in Terry Tao's blog: Having gone through the details of each of the theorems above, I have the following questions : Can one get Theorem 5 from Theorem 1 by making $\lambda=1$? Are these two versions of the Fredholm alternatives equivalent when $X$ is assumed to be a Hilbert space in Theorem 1?",,[]
90,Is the Laplace transform a linear operator?,Is the Laplace transform a linear operator?,,"Many sources online states that the Laplace transform $\mathcal{L}: V \to W,  f(t) \mapsto F(s)$ is a linear operator For example: http://www2.fiu.edu/~aladrog/PropLaplaceTransform.pdf ( Wayback Machine ) http://www.saylor.org/site/wp-content/uploads/2013/04/ME401-1.2.2-LaplaceTransform.pdf However, closely examining the definition of linear operator on Wikipedia, it says: In mathematics, a linear map (also called a linear mapping, linear transformation or, in some contexts, linear function) is a mapping V  W between two modules (including vector spaces) that preserves (in the sense defined below) the operations of addition and scalar multiplication. An important special case is when V = W, in which case the map is called a linear operator ,[1] or an endomorphism of V. Do we know that the Laplace transform is a endomorphism? It doesn't seem plausible given that we are taking a function in time domain, say $L_2[0, \infty)$ space, and mapping it into $H_2$ space So should the Laplace transform be a linear operator?","Many sources online states that the Laplace transform is a linear operator For example: http://www2.fiu.edu/~aladrog/PropLaplaceTransform.pdf ( Wayback Machine ) http://www.saylor.org/site/wp-content/uploads/2013/04/ME401-1.2.2-LaplaceTransform.pdf However, closely examining the definition of linear operator on Wikipedia, it says: In mathematics, a linear map (also called a linear mapping, linear transformation or, in some contexts, linear function) is a mapping V  W between two modules (including vector spaces) that preserves (in the sense defined below) the operations of addition and scalar multiplication. An important special case is when V = W, in which case the map is called a linear operator ,[1] or an endomorphism of V. Do we know that the Laplace transform is a endomorphism? It doesn't seem plausible given that we are taking a function in time domain, say space, and mapping it into space So should the Laplace transform be a linear operator?","\mathcal{L}: V \to W,  f(t) \mapsto F(s) L_2[0, \infty) H_2","['functional-analysis', 'fourier-analysis', 'operator-theory', 'laplace-transform', 'signal-processing']"
91,"If $\mathfrak{Y}$ is a closed subspace of a reflexive Banach space $\mathfrak{X}$, then is $\mathfrak{Y}$ reflexive too?","If  is a closed subspace of a reflexive Banach space , then is  reflexive too?",\mathfrak{Y} \mathfrak{X} \mathfrak{Y},"Exercise 2.4.8 in Analysis Now by Pedersen: If $\mathfrak{Y}$ is a closed subspace of a reflexive Banach space $\mathfrak{X}$, show that $\mathfrak{Y}$ and $\mathfrak{X}/\mathfrak{Y}$ are reflexive using the following theorem: 2.4.13. Proposition. Consider a closed subspace $\mathfrak{Y}$ of a normed space $\mathfrak{X}$. Let $I:\mathfrak{Y}\to \mathfrak{X}$ denote the inclusion map and $Q:\mathfrak{X}\to\mathfrak{X}/\mathfrak{Y}$ denote the quotient map. Then we may identify $Q^\ast$ with the inclusion map of $\mathfrak{Y}^\perp$ into $\mathfrak{X}^\ast$ and $I^\ast$ with the quotient map of $\mathfrak{X}^\ast$ onto $\mathfrak{X}^\ast/\mathfrak{Y}^\perp$.","Exercise 2.4.8 in Analysis Now by Pedersen: If $\mathfrak{Y}$ is a closed subspace of a reflexive Banach space $\mathfrak{X}$, show that $\mathfrak{Y}$ and $\mathfrak{X}/\mathfrak{Y}$ are reflexive using the following theorem: 2.4.13. Proposition. Consider a closed subspace $\mathfrak{Y}$ of a normed space $\mathfrak{X}$. Let $I:\mathfrak{Y}\to \mathfrak{X}$ denote the inclusion map and $Q:\mathfrak{X}\to\mathfrak{X}/\mathfrak{Y}$ denote the quotient map. Then we may identify $Q^\ast$ with the inclusion map of $\mathfrak{Y}^\perp$ into $\mathfrak{X}^\ast$ and $I^\ast$ with the quotient map of $\mathfrak{X}^\ast$ onto $\mathfrak{X}^\ast/\mathfrak{Y}^\perp$.",,"['functional-analysis', 'analysis', 'alternative-proof', 'topological-vector-spaces']"
92,Showing the 'integral metric' satisfies the triangle inequality axiom of a metric,Showing the 'integral metric' satisfies the triangle inequality axiom of a metric,,"If this question has already been asked somewhere please let me know and I will close my question, I tried searching for a while but was not sure what to search since I am not sure if 'integral metric' is actually the common name for this metric, or if it even has one. I did find this , but it did not answer my question. Define $d$ as  $$d: C[0,1]^2 \to \mathbb{R}, \hspace{3mm}d: (f,g) \mapsto \int_0^1 |f(x)-g(x)| \,dx.$$ I am interested in showing that this function satisfies the triangle inequality axiom of a metric ($d(x,y) \leq d(x,z) + d(z,y)$ for all $x,y,z \in C[0,1]$). How I proceeded in showing this was: Let $\, f,g,h \in C[0,1]$. Consider $$\int_0^1 |f(x)-g(x)| \,dx + \int_0^1 |g(x)-h(x)| \, dx$$ $$= \int_0^1 |f(x)-g(x)| + |g(x)-h(x)| \,dx.$$ Now by the triangle inequality for real numbers (which we accept without proof) we know that the following statement must hold for any $x \in [0,1]$ $$ |f(x)-g(x)+ g(x)-h(x)| = |f(x)-h(x)| \leq |f(x)-g(x)| + |g(x)-h(x)|.$$ $$\implies \int_0^1 |f(x)-h(x)| \leq \int_0^1 |f(x)-g(x)| + |g(x)-h(x)|,$$ which completes the proof. My question is if you feel my proof is acceptable. When I was thinking it in my head, I wasn't fully convinced but now that I actually typed it up here I actually feel it might be fine. Particularly, the parts that were bothering me were applying the triangle inequality to the integrand (mainly because $x$ is variable) and the final implication, which seems intuitive, but not fully justified? Thank you for any comments, critiques or discussion.","If this question has already been asked somewhere please let me know and I will close my question, I tried searching for a while but was not sure what to search since I am not sure if 'integral metric' is actually the common name for this metric, or if it even has one. I did find this , but it did not answer my question. Define $d$ as  $$d: C[0,1]^2 \to \mathbb{R}, \hspace{3mm}d: (f,g) \mapsto \int_0^1 |f(x)-g(x)| \,dx.$$ I am interested in showing that this function satisfies the triangle inequality axiom of a metric ($d(x,y) \leq d(x,z) + d(z,y)$ for all $x,y,z \in C[0,1]$). How I proceeded in showing this was: Let $\, f,g,h \in C[0,1]$. Consider $$\int_0^1 |f(x)-g(x)| \,dx + \int_0^1 |g(x)-h(x)| \, dx$$ $$= \int_0^1 |f(x)-g(x)| + |g(x)-h(x)| \,dx.$$ Now by the triangle inequality for real numbers (which we accept without proof) we know that the following statement must hold for any $x \in [0,1]$ $$ |f(x)-g(x)+ g(x)-h(x)| = |f(x)-h(x)| \leq |f(x)-g(x)| + |g(x)-h(x)|.$$ $$\implies \int_0^1 |f(x)-h(x)| \leq \int_0^1 |f(x)-g(x)| + |g(x)-h(x)|,$$ which completes the proof. My question is if you feel my proof is acceptable. When I was thinking it in my head, I wasn't fully convinced but now that I actually typed it up here I actually feel it might be fine. Particularly, the parts that were bothering me were applying the triangle inequality to the integrand (mainly because $x$ is variable) and the final implication, which seems intuitive, but not fully justified? Thank you for any comments, critiques or discussion.",,"['functional-analysis', 'proof-verification', 'metric-spaces']"
93,"Prove $f\in L_2$ if $\langle f,g\rangle< C\|g\|_{L_2}$ for any $g\in D$ and $\overline{D}=L_2$",Prove  if  for any  and,"f\in L_2 \langle f,g\rangle< C\|g\|_{L_2} g\in D \overline{D}=L_2","Let $D$ be a subset of and dense in $L_2$. If $\langle f,g\rangle<C\|g\|_{L_2}$ for a constant $C$ and any $g\in D$, then $f$ must be in $L_2$. I read this statement in a paper. I think it is right, because the following argument makes sense. The fact that $\langle f,g\rangle< C\|g\|_{L_2}$ for any $g\in D$ implies that $f$ is a linear functional on $L_2$. It is known that the dual of $L_2$ is $L_2$. Thus $f$ should be in $L_2$. This argument is not rigorous and I think the duality of $L_2$ is a bit too advanced for this statement. Could anyone give a tip of an elementary proof that does not require the duality property? If needed, $D$ is the set of compact support functions in $C^{\infty}$ and $\langle f,g\rangle=\int f\bar{g}$. We can consider the problem in $L_2(\mathbb{R})$. But I guess that what $D$ is exactly does not matter, except that it is dense in $L_2$.","Let $D$ be a subset of and dense in $L_2$. If $\langle f,g\rangle<C\|g\|_{L_2}$ for a constant $C$ and any $g\in D$, then $f$ must be in $L_2$. I read this statement in a paper. I think it is right, because the following argument makes sense. The fact that $\langle f,g\rangle< C\|g\|_{L_2}$ for any $g\in D$ implies that $f$ is a linear functional on $L_2$. It is known that the dual of $L_2$ is $L_2$. Thus $f$ should be in $L_2$. This argument is not rigorous and I think the duality of $L_2$ is a bit too advanced for this statement. Could anyone give a tip of an elementary proof that does not require the duality property? If needed, $D$ is the set of compact support functions in $C^{\infty}$ and $\langle f,g\rangle=\int f\bar{g}$. We can consider the problem in $L_2(\mathbb{R})$. But I guess that what $D$ is exactly does not matter, except that it is dense in $L_2$.",,"['real-analysis', 'functional-analysis']"
94,Proving that an operator is closed,Proving that an operator is closed,,"This is a question about the notes located at https://www.math.univ-toulouse.fr/~raymond/book-ficus.pdf In section 6.2.2, theorem 6.2.3, the author would like to prove that $(A,D(A))$ is the infinitesimal generator of a semigroup of contractions on $Y = L^2(\Omega) \times H^{-1}(\Omega)$, $D(A) = H_0^1(\Omega) \times L^2(\Omega)$, where $A$ is defined by $$A(y_1,y_2) = (y_2, \tilde{A}y_1),$$ and where $\tilde{A}$ is defined by its action as $$(\tilde{A}y_1, \xi) = - \int \nabla y_1\cdot \nabla (-\Delta)^{-1}\xi,$$ in the usual sense for $(-\Delta)^{-1}$. It must be shown that the operator is dense and closed.  No proof is given for this. Density seems to be immediate.  For closed, I tried to follow the proof from the previous page.  Let $(y_{1n},y_{2n})$ converge to $(y_1,y_2)$ in $H_0^1(\Omega)\times L^2(\Omega)$, and $A(y_{1n},y_{2n})$ converge to some $(f,g) \in L^2 \times H^{-1}$. Because $\nabla$ is bounded from $H_0^1 \mapsto L^2$, and bounded operators preserve (weak) convergence, by the definition, we have that $y_2 = f$, and $\tilde{A} y_1 = g$.  But then how can we show convergence of $y_{1n}$?  We would need a Cauchy estimate like $$||y_{1n}-y_{1m}||_{H_0^1} \leq C||\tilde{A}y_{1n} - \tilde{A}y_{1m}||_{H^{-1}}.$$ Is this somehow immediately true and I'm missing something? Edit:  The second part of this doesn't make sense either.  The author tries to prove the resolvent operator bound by using the $\Delta$ operator, but of course we must use $\tilde{A}$.  Is there a reason we can use $\Delta$ instead of $\tilde{A}$?","This is a question about the notes located at https://www.math.univ-toulouse.fr/~raymond/book-ficus.pdf In section 6.2.2, theorem 6.2.3, the author would like to prove that $(A,D(A))$ is the infinitesimal generator of a semigroup of contractions on $Y = L^2(\Omega) \times H^{-1}(\Omega)$, $D(A) = H_0^1(\Omega) \times L^2(\Omega)$, where $A$ is defined by $$A(y_1,y_2) = (y_2, \tilde{A}y_1),$$ and where $\tilde{A}$ is defined by its action as $$(\tilde{A}y_1, \xi) = - \int \nabla y_1\cdot \nabla (-\Delta)^{-1}\xi,$$ in the usual sense for $(-\Delta)^{-1}$. It must be shown that the operator is dense and closed.  No proof is given for this. Density seems to be immediate.  For closed, I tried to follow the proof from the previous page.  Let $(y_{1n},y_{2n})$ converge to $(y_1,y_2)$ in $H_0^1(\Omega)\times L^2(\Omega)$, and $A(y_{1n},y_{2n})$ converge to some $(f,g) \in L^2 \times H^{-1}$. Because $\nabla$ is bounded from $H_0^1 \mapsto L^2$, and bounded operators preserve (weak) convergence, by the definition, we have that $y_2 = f$, and $\tilde{A} y_1 = g$.  But then how can we show convergence of $y_{1n}$?  We would need a Cauchy estimate like $$||y_{1n}-y_{1m}||_{H_0^1} \leq C||\tilde{A}y_{1n} - \tilde{A}y_{1m}||_{H^{-1}}.$$ Is this somehow immediately true and I'm missing something? Edit:  The second part of this doesn't make sense either.  The author tries to prove the resolvent operator bound by using the $\Delta$ operator, but of course we must use $\tilde{A}$.  Is there a reason we can use $\Delta$ instead of $\tilde{A}$?",,"['functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
95,"The space $C^1[a,b]$ with respect to the norm $\int ^a _b |f| +\int ^a _b |f'|$ is not complete",The space  with respect to the norm  is not complete,"C^1[a,b] \int ^a _b |f| +\int ^a _b |f'|","I am trying to find a Cauchy sequence that does not converge in this space. My attempt is $f_n = \sqrt{x^2+\frac{1}{n}}$, but I do not know how to prove it. Also, what would be the completion of this space? I have the feeling that it should be the space of absolute continuous functions since we need something that is differentiable, but I still do not know how to prove this.","I am trying to find a Cauchy sequence that does not converge in this space. My attempt is $f_n = \sqrt{x^2+\frac{1}{n}}$, but I do not know how to prove it. Also, what would be the completion of this space? I have the feeling that it should be the space of absolute continuous functions since we need something that is differentiable, but I still do not know how to prove this.",,"['real-analysis', 'functional-analysis']"
96,Find an integrable dominated function for a convolution,Find an integrable dominated function for a convolution,,"Let $h\in L^1(\mathbb{R}^n)$. Let $\varphi\in S(\mathbb{R}^n)$, $\int_{\mathbb{R}^n}\varphi(x) dx=1$, where $S(\mathbb{R}^n)$ is the Schwartz function space and $\varphi$ is nonnegative, radial, and radially decreasing. Let $\varphi_k(x)=k^n\varphi(kx)$, $k=1,2,...$, which is a sequence of function approximations to the Dirac delta function $\delta_0$. Recall that $||h\ast \varphi_k||_1\le ||\varphi_k||_1||h||_1=||h||_1$ by Young's inequality. Then is there a function $g\in L^1(\mathbb{R}^n)$ such that $${\rm{sup}}_{k\ge1}|h\ast\varphi_k|(x)\le g(x),\ a.e. \ ?$$ Remark: (1)Using the Hardy-Littlewood maximal function we have ${\rm{sup}}_{k\ge1}|h\ast\varphi_k|(x)\le Mh(x)$, but unfortunately $Mh\notin L^1(\mathbb{R}^n)$ whenever $h\ne 0$ on a set with positive measure. (2)By the Hardy-Littlewood maximal theorem, if $h\in L^p(\mathbb{R}^n)$, $1<p\le \infty$, then $${\rm{sup}}_{k\ge1}|h\ast\varphi_k|(x)\le Mh(x)\in L^p(\mathbb{R}^n).$$","Let $h\in L^1(\mathbb{R}^n)$. Let $\varphi\in S(\mathbb{R}^n)$, $\int_{\mathbb{R}^n}\varphi(x) dx=1$, where $S(\mathbb{R}^n)$ is the Schwartz function space and $\varphi$ is nonnegative, radial, and radially decreasing. Let $\varphi_k(x)=k^n\varphi(kx)$, $k=1,2,...$, which is a sequence of function approximations to the Dirac delta function $\delta_0$. Recall that $||h\ast \varphi_k||_1\le ||\varphi_k||_1||h||_1=||h||_1$ by Young's inequality. Then is there a function $g\in L^1(\mathbb{R}^n)$ such that $${\rm{sup}}_{k\ge1}|h\ast\varphi_k|(x)\le g(x),\ a.e. \ ?$$ Remark: (1)Using the Hardy-Littlewood maximal function we have ${\rm{sup}}_{k\ge1}|h\ast\varphi_k|(x)\le Mh(x)$, but unfortunately $Mh\notin L^1(\mathbb{R}^n)$ whenever $h\ne 0$ on a set with positive measure. (2)By the Hardy-Littlewood maximal theorem, if $h\in L^p(\mathbb{R}^n)$, $1<p\le \infty$, then $${\rm{sup}}_{k\ge1}|h\ast\varphi_k|(x)\le Mh(x)\in L^p(\mathbb{R}^n).$$",,"['real-analysis', 'functional-analysis', 'harmonic-analysis']"
97,"Prove this function defined for a convex, bounded, open, symmetric set $V$ is a norm on $\mathbb{R}^n$","Prove this function defined for a convex, bounded, open, symmetric set  is a norm on",V \mathbb{R}^n,"$V\subset \mathbb{R}^n $ be open, symmetric and contains 0 . Define $||\cdot||:\mathbb{R}^n \rightarrow \mathbb{R}$ as: \begin{equation} ||x||=\inf\{ \lambda > 0: \lambda^{-1}x\in V \}  \end{equation}   for all x $\in \mathbb{R}^n$. Prove this is a norm $\iff$ $V$ is convex and bounded with respect to the euclidean metric. Approach: $||x||\ge 0$ (Done) $||\alpha x||= |\alpha|||x||$ (not done) triangle in equality (not done)","$V\subset \mathbb{R}^n $ be open, symmetric and contains 0 . Define $||\cdot||:\mathbb{R}^n \rightarrow \mathbb{R}$ as: \begin{equation} ||x||=\inf\{ \lambda > 0: \lambda^{-1}x\in V \}  \end{equation}   for all x $\in \mathbb{R}^n$. Prove this is a norm $\iff$ $V$ is convex and bounded with respect to the euclidean metric. Approach: $||x||\ge 0$ (Done) $||\alpha x||= |\alpha|||x||$ (not done) triangle in equality (not done)",,"['functional-analysis', 'convex-analysis', 'normed-spaces']"
98,Equivalence to essentially self-adjoint,Equivalence to essentially self-adjoint,,"In a book I have found a statement (without proof): Let $A$ be a symmetric operator on a Hilbert space $\mathcal H$ , the following are equivalent: $A$ is essentially self-adjoint $\nu(i)=\nu(-i)=0$ The ranges of $A-i \mathbb{1}$ and $A+i\mathbb 1$ are dense in $\mathcal H$ Here $\nu(\lambda)$ is defined as the dimension of the $\ker(A^*-\lambda \mathbb 1)$ . I don't really know where to begin with the proof. For 2. and 3. is it necessary to have both parts of the statement or are they already equivalent to each other? How can the statement be proven? I think at this point I would appreciate hints more than the direct proof.","In a book I have found a statement (without proof): Let be a symmetric operator on a Hilbert space , the following are equivalent: is essentially self-adjoint The ranges of and are dense in Here is defined as the dimension of the . I don't really know where to begin with the proof. For 2. and 3. is it necessary to have both parts of the statement or are they already equivalent to each other? How can the statement be proven? I think at this point I would appreciate hints more than the direct proof.",A \mathcal H A \nu(i)=\nu(-i)=0 A-i \mathbb{1} A+i\mathbb 1 \mathcal H \nu(\lambda) \ker(A^*-\lambda \mathbb 1),"['functional-analysis', 'operator-theory', 'hilbert-spaces']"
99,How does one derive Radial Basis Function (RBF) Networks as the smoothest interpolation of points?,How does one derive Radial Basis Function (RBF) Networks as the smoothest interpolation of points?,,"I was reading/watching CalTech's ML course and it said that one could derive the RBF Gaussian kernel from the solution to smoothest interpolation that minimizes squared loss. i.e. one can derive the predictor/interpolator: $$ f(x) = \sum^{K}_{k=1} c_k \exp( -\beta_k \| x - w_k \|^2 )$$ from the Empirical Risk minimizer with a smoothest Regularizer: $$ f^* = arg \min_f \mathcal{E}_S(f) = \sum^{N}_{n=1} (f(x_n) - y_n)^2  + \lambda R(f) $$ $$ f^* = arg \min_f \mathcal{E}_S(f) = \sum^{N}_{n=1} (f(x_n) - y_n)^2  + \lambda \sum^{\infty}_{k=0} a_j \int^{\infty}_{- \infty} \left( \frac{d^k h}{d x^k} \right) dx$$ unfortunately, they do not show the derivation of this. Thus I was wondering if someone could show me how minimizing the ERM using that regularizer, one could derive the RBF kernel function. In particular I am very interested in the exact mathematical details and if there is any maths I need to learn I am motivated to learn it to understand this derivation. I believe they mentioned that this regularizer was for some simplified case (not sure which one) but I would be interested to start of in the simple explanation of this derivation (I believe its using only 1D calculus?) and then generalizing as it needed. As a first suggestion the generalization could be the answer to the question Why does minimizing $H[f] =\sum^{N}_{i=1}(y_i-f(x_i))^2+\lambda \| Pf \|^2 $ leads to solution of the form $ f(x) =\sum^N_{i=1}c_iG(x; x_i)+p(x)$? .","I was reading/watching CalTech's ML course and it said that one could derive the RBF Gaussian kernel from the solution to smoothest interpolation that minimizes squared loss. i.e. one can derive the predictor/interpolator: $$ f(x) = \sum^{K}_{k=1} c_k \exp( -\beta_k \| x - w_k \|^2 )$$ from the Empirical Risk minimizer with a smoothest Regularizer: $$ f^* = arg \min_f \mathcal{E}_S(f) = \sum^{N}_{n=1} (f(x_n) - y_n)^2  + \lambda R(f) $$ $$ f^* = arg \min_f \mathcal{E}_S(f) = \sum^{N}_{n=1} (f(x_n) - y_n)^2  + \lambda \sum^{\infty}_{k=0} a_j \int^{\infty}_{- \infty} \left( \frac{d^k h}{d x^k} \right) dx$$ unfortunately, they do not show the derivation of this. Thus I was wondering if someone could show me how minimizing the ERM using that regularizer, one could derive the RBF kernel function. In particular I am very interested in the exact mathematical details and if there is any maths I need to learn I am motivated to learn it to understand this derivation. I believe they mentioned that this regularizer was for some simplified case (not sure which one) but I would be interested to start of in the simple explanation of this derivation (I believe its using only 1D calculus?) and then generalizing as it needed. As a first suggestion the generalization could be the answer to the question Why does minimizing $H[f] =\sum^{N}_{i=1}(y_i-f(x_i))^2+\lambda \| Pf \|^2 $ leads to solution of the form $ f(x) =\sum^N_{i=1}c_iG(x; x_i)+p(x)$? .",,"['functional-analysis', 'machine-learning', 'approximation-theory']"
