,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"If eigenvalues are positive, is the matrix positive definite?","If eigenvalues are positive, is the matrix positive definite?",,"If the matrix is positive definite, then all its eigenvalues are strictly positive. Is the converse also true? That is, if the eigenvalues are strictly positive, then matrix is positive definite? Can you give example of $2 \times 2$ matrix with $2$ positive eigenvalues but is not positive definite?","If the matrix is positive definite, then all its eigenvalues are strictly positive. Is the converse also true? That is, if the eigenvalues are strictly positive, then matrix is positive definite? Can you give example of $2 \times 2$ matrix with $2$ positive eigenvalues but is not positive definite?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'positive-definite']"
1,Linear Algebra Versus Functional Analysis,Linear Algebra Versus Functional Analysis,,"As it is mentioned in the answer by Sheldon Axler in this post , we usually  restrict linear algebra to finite dimensional linear spaces and study the infinite dimensional ones in functional analysis. I am wondering that what are those parts of the theory in linear algebra that restrict it to finite dimensions. To clarify myself, here is the main question Question . What are the main theorems in linear algebra that are just valid for finite dimensional linear spaces and are propagated in the sequel of theory, used to prove the following theorems? Please note that I want to know the main theorems that are just valid for finite dimensions not all of them. By main , I mean the minimum number of theorems of this kind such that all other such theorems can be concluded from these.","As it is mentioned in the answer by Sheldon Axler in this post , we usually  restrict linear algebra to finite dimensional linear spaces and study the infinite dimensional ones in functional analysis. I am wondering that what are those parts of the theory in linear algebra that restrict it to finite dimensions. To clarify myself, here is the main question Question . What are the main theorems in linear algebra that are just valid for finite dimensional linear spaces and are propagated in the sequel of theory, used to prove the following theorems? Please note that I want to know the main theorems that are just valid for finite dimensions not all of them. By main , I mean the minimum number of theorems of this kind such that all other such theorems can be concluded from these.",,"['linear-algebra', 'functional-analysis']"
2,"If $e^A$ and $e^B$ commute, do $A$ and $B$ commute for finite dimensional matrices?","If  and  commute, do  and  commute for finite dimensional matrices?",e^A e^B A B,"It is known that if two matrices $A,B \in M_n(\mathbb{C})$ commute, then $e^A$ and $e^B$ commute. Is the converse true? If $e^A$ and $e^B$ commute, do $A$ and $B$ commute? Edit: Additionally, what happens in $M_n(\mathbb{R})$ ? Nota Bene: As a corollary of the counterexamples below, we deduce that if $A$ is not diagonal then $e^A$ may be diagonal.","It is known that if two matrices commute, then and commute. Is the converse true? If and commute, do and commute? Edit: Additionally, what happens in ? Nota Bene: As a corollary of the counterexamples below, we deduce that if is not diagonal then may be diagonal.","A,B \in M_n(\mathbb{C}) e^A e^B e^A e^B A B M_n(\mathbb{R}) A e^A","['linear-algebra', 'matrices', 'exponential-function', 'matrix-exponential']"
3,why geometric multiplicity is bounded by algebraic multiplicity?,why geometric multiplicity is bounded by algebraic multiplicity?,,"Define The algebraic multiplicity of $\lambda_{i}$ to be the degree of the root $\lambda_i$ in the polynomial $\det(A-\lambda I)$ . The geometric multiplicity the be the dimension of the eigenspace associated with the eigenvalue $\lambda_i$ . For example: $\begin{bmatrix}1&1\\0&1\end{bmatrix}$ has root $1$ with algebraic multiplicity $2$ , but the geometric multiplicity $1$ . My Question : Why is the geometric multiplicity always bounded by algebraic multiplicity? Thanks.","Define The algebraic multiplicity of to be the degree of the root in the polynomial . The geometric multiplicity the be the dimension of the eigenspace associated with the eigenvalue . For example: has root with algebraic multiplicity , but the geometric multiplicity . My Question : Why is the geometric multiplicity always bounded by algebraic multiplicity? Thanks.",\lambda_{i} \lambda_i \det(A-\lambda I) \lambda_i \begin{bmatrix}1&1\\0&1\end{bmatrix} 1 2 1,"['linear-algebra', 'matrices']"
4,How unique are $U$ and $V$ in the singular value decomposition $A=UDV^\dagger$?,How unique are  and  in the singular value decomposition ?,U V A=UDV^\dagger,"According to Wikipedia: A common convention is to list the singular values in descending order. In this case, the diagonal matrix $\Sigma$ is uniquely determined by $M$ (though the matrices $U$ and $V$ are not). My question is, are $U$ and $V$ uniquely determined up to some equivalence relation (and what equivalence relation)?","According to Wikipedia: A common convention is to list the singular values in descending order. In this case, the diagonal matrix is uniquely determined by (though the matrices and are not). My question is, are and uniquely determined up to some equivalence relation (and what equivalence relation)?",\Sigma M U V U V,"['linear-algebra', 'matrices', 'matrix-decomposition', 'svd', 'singular-values']"
5,Why learn to solve differential equations when computers can do it?,Why learn to solve differential equations when computers can do it?,,"I'm getting started learning engineering math. I'm really interested in physics especially quantum mechanics, and I'm coming from a strong CS background. One question is haunting me. Why do I need to learn to do complex math operations on paper when most can be done automatically in software like Maple. For instance, as long as I learn the concept and application for how aspects of linear algebra and differential equations work, won't I be able to enter the appropriate info into such a software program and not have to manually do the calculations? Is the point of math and math classes to learn the big-picture concepts of how to apply mathematical tools or is the point to learn the details to the ground level? Just to clarify, I'm not trying to offend any mathematicians or to belittle the importance of math. From CS I recognize that knowing the deep details of an algorithm can be useful, but that is equally important to be able to work abstractly. Just trying to get some perspective on how to approach the next few years of study.","I'm getting started learning engineering math. I'm really interested in physics especially quantum mechanics, and I'm coming from a strong CS background. One question is haunting me. Why do I need to learn to do complex math operations on paper when most can be done automatically in software like Maple. For instance, as long as I learn the concept and application for how aspects of linear algebra and differential equations work, won't I be able to enter the appropriate info into such a software program and not have to manually do the calculations? Is the point of math and math classes to learn the big-picture concepts of how to apply mathematical tools or is the point to learn the details to the ground level? Just to clarify, I'm not trying to offend any mathematicians or to belittle the importance of math. From CS I recognize that knowing the deep details of an algorithm can be useful, but that is equally important to be able to work abstractly. Just trying to get some perspective on how to approach the next few years of study.",,"['linear-algebra', 'ordinary-differential-equations', 'self-learning', 'physics']"
6,Why does the spectral norm equal the largest singular value?,Why does the spectral norm equal the largest singular value?,,This may be a trivial question yet I was unable to find an answer: $$\left \| A \right \| _2=\sqrt{\lambda_{\text{max}}(A^{^*}A)}=\sigma_{\text{max}}(A)$$ where the spectral norm $\left \| A \right \| _2$ of a complex matrix $A$ is defined as $$\text{max} \left\{ \|Ax\|_2 : \|x\| = 1 \right\}$$ How does one prove the first and the second equality?,This may be a trivial question yet I was unable to find an answer: $$\left \| A \right \| _2=\sqrt{\lambda_{\text{max}}(A^{^*}A)}=\sigma_{\text{max}}(A)$$ where the spectral norm $\left \| A \right \| _2$ of a complex matrix $A$ is defined as $$\text{max} \left\{ \|Ax\|_2 : \|x\| = 1 \right\}$$ How does one prove the first and the second equality?,,"['linear-algebra', 'matrices', 'svd', 'singular-values', 'spectral-norm']"
7,Effect of elementary row operations on determinant?,Effect of elementary row operations on determinant?,,"1) Switching two rows or columns causes the determinant to switch sign 2) Adding a multiple of one row to another causes the determinant to remain the same 3) Multiplying a row as a constant results in the determinant scaling by that constant. Using the geometric definition of the determinant as the area spanned by the columns of the matrix, could someone give me a geometric interpretation of the above theorems? Thanks.","1) Switching two rows or columns causes the determinant to switch sign 2) Adding a multiple of one row to another causes the determinant to remain the same 3) Multiplying a row as a constant results in the determinant scaling by that constant. Using the geometric definition of the determinant as the area spanned by the columns of the matrix, could someone give me a geometric interpretation of the above theorems? Thanks.",,"['linear-algebra', 'matrices', 'vector-spaces', 'intuition', 'determinant']"
8,Are there open problems in Linear Algebra?,Are there open problems in Linear Algebra?,,"I'm reading some stuff about algebraic K-theory, which can be regarded as a ""generalization"" of linear algebra, because we want to use the same tools like in linear algebra in module theory. There are a lot of open problems and conjectures in K-theory, which are ""sometimes"" inspired by linear algebra. So I just want to know: What are open problems in ""pure"" linear algebra? (Pure means not numerical!) Thanks","I'm reading some stuff about algebraic K-theory, which can be regarded as a ""generalization"" of linear algebra, because we want to use the same tools like in linear algebra in module theory. There are a lot of open problems and conjectures in K-theory, which are ""sometimes"" inspired by linear algebra. So I just want to know: What are open problems in ""pure"" linear algebra? (Pure means not numerical!) Thanks",,"['linear-algebra', 'matrices', 'soft-question', 'open-problem', 'algebraic-k-theory']"
9,Do commuting matrices share the same eigenvectors?,Do commuting matrices share the same eigenvectors?,,"In one of my exams I'm asked to prove the following Suppose $A,B\in \mathbb R^{n\times n}$ , and $AB=BA$ , then $A,B$ share the same eigenvectors. My attempt is let $\xi$ be an eigenvector corresponding to $\lambda$ of $A$ , then $A\xi=\lambda\xi$ , then I want to show $\xi$ is also some eigenvector of $B$ but I get stuck.","In one of my exams I'm asked to prove the following Suppose , and , then share the same eigenvectors. My attempt is let be an eigenvector corresponding to of , then , then I want to show is also some eigenvector of but I get stuck.","A,B\in \mathbb R^{n\times n} AB=BA A,B \xi \lambda A A\xi=\lambda\xi \xi B","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
10,A linear operator commuting with all such operators is a scalar multiple of the identity.,A linear operator commuting with all such operators is a scalar multiple of the identity.,,"The question is from Axler's "" Linear Algebra Done Right "", which I'm using for self-study. We are given a linear operator $T$ over a finite dimensional vector space $V$. We have to show that $T$ is a scalar multiple of the identity iff $\forall S \in {\cal L}(V), TS = ST$. Here, ${\cal L}(V)$ denotes the set of all linear operators over $V$. One direction is easy to prove. If $T$ is a scalar multiple of the identity, then there exists a scalar $a$ such that $Tv = av$, $\forall v \in V$. Hence, given an arbitrary vector $w$, $$TS(w) = T(Sw) = a(Sw) = S(aw) = S(Tw) = ST(w)$$ where the third equality is possible because $S$ is a linear operator. Then, it follows that $TS = ST$, as required. I am, however, at a loss as to how to tackle the other direction. I thought that a proof by contradiction,  ultimately constructing a linear operator $S$ for which $TS \neq ST$, might be the way to go, but haven't made much progress. Thanks in advance!","The question is from Axler's "" Linear Algebra Done Right "", which I'm using for self-study. We are given a linear operator $T$ over a finite dimensional vector space $V$. We have to show that $T$ is a scalar multiple of the identity iff $\forall S \in {\cal L}(V), TS = ST$. Here, ${\cal L}(V)$ denotes the set of all linear operators over $V$. One direction is easy to prove. If $T$ is a scalar multiple of the identity, then there exists a scalar $a$ such that $Tv = av$, $\forall v \in V$. Hence, given an arbitrary vector $w$, $$TS(w) = T(Sw) = a(Sw) = S(aw) = S(Tw) = ST(w)$$ where the third equality is possible because $S$ is a linear operator. Then, it follows that $TS = ST$, as required. I am, however, at a loss as to how to tackle the other direction. I thought that a proof by contradiction,  ultimately constructing a linear operator $S$ for which $TS \neq ST$, might be the way to go, but haven't made much progress. Thanks in advance!",,"['linear-algebra', 'vector-spaces', 'self-learning']"
11,What do eigenvalues have to do with pictures?,What do eigenvalues have to do with pictures?,,"I am trying to write a program that will perform OCR on a mobile phone, and I recently encountered this article : Can someone explain this to me ?","I am trying to write a program that will perform OCR on a mobile phone, and I recently encountered this article : Can someone explain this to me ?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'svd', 'image-processing']"
12,Is there a definition of determinants that does not rely on how they are calculated?,Is there a definition of determinants that does not rely on how they are calculated?,,"In the few linear algebra texts I have read, the determinant is introduced in the following manner; “Here is a formula for what we call $\det A$ . Here are some other formulas. And finally, here are some nice properties of the determinant.” For example, in very elementary textbooks it is introduced by giving the co-factor expansion formula. In Axler’s “Linear Algebra Done Right” it is defined, for $T\in L(V)$ to be ${(-1)}^{\dim V}$ times the constant term in the characteristic polynomial of $T$ . However I find this somewhat unsatisfactory. It’s like the real definition of the determinant is hidden. Ideally, wouldn’t the determinant be defined in the following manner: “Given a matrix $A$ , let $\det A$ be an element of $\mathbb{F}$ such that $x$ , $y$ and $z$ .” Then one would proceed to prove that this element is unique, and derive the familiar formulae. So my question is: Does a definition of the latter type exist, is there some minimal set of properties sufficient to define what a determinant is? If not, can you explain why?","In the few linear algebra texts I have read, the determinant is introduced in the following manner; “Here is a formula for what we call . Here are some other formulas. And finally, here are some nice properties of the determinant.” For example, in very elementary textbooks it is introduced by giving the co-factor expansion formula. In Axler’s “Linear Algebra Done Right” it is defined, for to be times the constant term in the characteristic polynomial of . However I find this somewhat unsatisfactory. It’s like the real definition of the determinant is hidden. Ideally, wouldn’t the determinant be defined in the following manner: “Given a matrix , let be an element of such that , and .” Then one would proceed to prove that this element is unique, and derive the familiar formulae. So my question is: Does a definition of the latter type exist, is there some minimal set of properties sufficient to define what a determinant is? If not, can you explain why?",\det A T\in L(V) {(-1)}^{\dim V} T A \det A \mathbb{F} x y z,['linear-algebra']
13,How do I exactly project a vector onto a subspace?,How do I exactly project a vector onto a subspace?,,"I am trying to understand how - exactly - I go about projecting a vector onto a subspace. Now, I know enough about linear algebra to know about projections, dot products, spans, etc etc, so I am not sure if I am reading too much into this, or if this is something that I have missed. For a class I am taking, the proff is saying that we take a vector, and 'simply project it onto a subspace', (where that subspace is formed from a set of orthogonal basis vectors). Now, I know that a subspace is really, at the end of the day, just a set of vectors. (That satisfy properties here ). I get that part - that its this set of vectors. So, how do I ""project a vector on this subspace""? Am I projecting my one vector, (lets call it a[n]) onto ALL the vectors in this subspace? (What if there is an infinite number of them?) For further context, the proff was saying that lets say we found a set of basis vectors for a signal, (lets call them b[n] and c[n]) then we would project a[n] onto its signal subspace . We project a[n] onto the signal-subspace formed by b[n] and c[n]. Well, how is this done exactly?.. Thanks in advance, let me know if I can clarify anything! P.S. I appreciate your help, and I would really like for the clarification to this problem to be somewhat 'concrete' - for example, something that I can show for myself over MATLAB. Analogues using 2-D or 3-D space so that I can visualize what is going on would be very much appreciated as well. Thanks again.","I am trying to understand how - exactly - I go about projecting a vector onto a subspace. Now, I know enough about linear algebra to know about projections, dot products, spans, etc etc, so I am not sure if I am reading too much into this, or if this is something that I have missed. For a class I am taking, the proff is saying that we take a vector, and 'simply project it onto a subspace', (where that subspace is formed from a set of orthogonal basis vectors). Now, I know that a subspace is really, at the end of the day, just a set of vectors. (That satisfy properties here ). I get that part - that its this set of vectors. So, how do I ""project a vector on this subspace""? Am I projecting my one vector, (lets call it a[n]) onto ALL the vectors in this subspace? (What if there is an infinite number of them?) For further context, the proff was saying that lets say we found a set of basis vectors for a signal, (lets call them b[n] and c[n]) then we would project a[n] onto its signal subspace . We project a[n] onto the signal-subspace formed by b[n] and c[n]. Well, how is this done exactly?.. Thanks in advance, let me know if I can clarify anything! P.S. I appreciate your help, and I would really like for the clarification to this problem to be somewhat 'concrete' - for example, something that I can show for myself over MATLAB. Analogues using 2-D or 3-D space so that I can visualize what is going on would be very much appreciated as well. Thanks again.",,"['linear-algebra', 'matrices', 'signal-processing']"
14,Determinant of a matrix that contains the first $n^2$ primes.,Determinant of a matrix that contains the first  primes.,n^2,"Let $n$ be an integer and $p_1,\ldots,p_{n^2}$ be the first prime numbers. Writing them down in a matrix $$ \left(\begin{matrix} p_1 & p_2 & \cdots & p_n \\ p_{n+1} & p_{n+2} & \cdots & p_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ \cdots & \cdots & \cdots & p_{n^2} \end{matrix} \right) $$ we can take the determinant. How to prove that determinant is not zero for every $n$?","Let $n$ be an integer and $p_1,\ldots,p_{n^2}$ be the first prime numbers. Writing them down in a matrix $$ \left(\begin{matrix} p_1 & p_2 & \cdots & p_n \\ p_{n+1} & p_{n+2} & \cdots & p_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ \cdots & \cdots & \cdots & p_{n^2} \end{matrix} \right) $$ we can take the determinant. How to prove that determinant is not zero for every $n$?",,"['linear-algebra', 'number-theory', 'prime-numbers', 'determinant']"
15,What are some examples of infinite dimensional vector spaces?,What are some examples of infinite dimensional vector spaces?,,I would like to have some examples of infinite dimensional vector spaces that help me to break my habit of thinking of $\mathbb{R}^n$ when thinking about vector spaces.,I would like to have some examples of infinite dimensional vector spaces that help me to break my habit of thinking of when thinking about vector spaces.,\mathbb{R}^n,"['linear-algebra', 'functional-analysis', 'vector-spaces', 'examples-counterexamples', 'infinity']"
16,Very good linear algebra book.,Very good linear algebra book.,,"I plan to self-study linear algebra this summer. I am sorta already familiar with vectors, vector spaces and subspaces and I am really interested in everything about matrices (diagonalization, ...), linear maps and their matrix representation and eigenvectors and eigenvalues. I am looking for a book that handles every of the aforementioned topics in details. I also want to build a solid basis of the mathematical way of thinking to get ready to an exciting abstract algebra next semester, so my main aim is to work on proofs for somehow hard problems. I got Lang's ""Intro. to Linear Algebra"" and it is too easy, superficial. Can you advise me a good book for all of the above? Please take into consideration that it is for self-study, so that it' gotta work on its own. Thanks.","I plan to self-study linear algebra this summer. I am sorta already familiar with vectors, vector spaces and subspaces and I am really interested in everything about matrices (diagonalization, ...), linear maps and their matrix representation and eigenvectors and eigenvalues. I am looking for a book that handles every of the aforementioned topics in details. I also want to build a solid basis of the mathematical way of thinking to get ready to an exciting abstract algebra next semester, so my main aim is to work on proofs for somehow hard problems. I got Lang's ""Intro. to Linear Algebra"" and it is too easy, superficial. Can you advise me a good book for all of the above? Please take into consideration that it is for self-study, so that it' gotta work on its own. Thanks.",,"['linear-algebra', 'reference-request', 'soft-question', 'book-recommendation']"
17,How to find a basis for the intersection of two vector spaces in $\mathbb{R}^n$?,How to find a basis for the intersection of two vector spaces in ?,\mathbb{R}^n,"What is the general way of finding the basis for intersection of two vector spaces in $\mathbb{R}^n$? Suppose I'm given the bases of two vector spaces U and W: $$ \mathrm{Base}(U)= \left\{ \left(1,1,0,-1\right), \left(0,1,3,1\right) \right\} $$ $$ \mathrm{Base}(W) =\left\{ \left(0,-1,-2,1\right), \left(1,2,2,-2\right) \right\} $$ I already calculated $U+W$, and the dimension is $3$ meaning the dimension of $ U \cap W $ is $1$. The answer is supposedly obvious, one vector is the basis of $ U \cap W $ but how do I calculate it?","What is the general way of finding the basis for intersection of two vector spaces in $\mathbb{R}^n$? Suppose I'm given the bases of two vector spaces U and W: $$ \mathrm{Base}(U)= \left\{ \left(1,1,0,-1\right), \left(0,1,3,1\right) \right\} $$ $$ \mathrm{Base}(W) =\left\{ \left(0,-1,-2,1\right), \left(1,2,2,-2\right) \right\} $$ I already calculated $U+W$, and the dimension is $3$ meaning the dimension of $ U \cap W $ is $1$. The answer is supposedly obvious, one vector is the basis of $ U \cap W $ but how do I calculate it?",,"['linear-algebra', 'vector-spaces']"
18,Generating correlated random numbers: Why does Cholesky decomposition work?,Generating correlated random numbers: Why does Cholesky decomposition work?,,"Let's say I want to generate correlated random variables. I understand that I can use Cholesky decomposition of the correlation matrix to obtain the correlated values. If $C$ is the correlation matrix, then we can do the cholesky decomposition: $LL^{T}=C$ Then I can easily generate correlated random variables: $LX=Y$, where $X$ are uncorrelated values and $Y$ are correlated values. If I want two correlated random variables then $L$ is: $L = \left[ {\begin{array}{*{20}c}    1 & 0  \\    \rho  & {\sqrt {1 - \rho ^2 } }  \\ \end{array}} \right] $ I understand that this works, but I don't really understand why... My question is: Why does this work?","Let's say I want to generate correlated random variables. I understand that I can use Cholesky decomposition of the correlation matrix to obtain the correlated values. If $C$ is the correlation matrix, then we can do the cholesky decomposition: $LL^{T}=C$ Then I can easily generate correlated random variables: $LX=Y$, where $X$ are uncorrelated values and $Y$ are correlated values. If I want two correlated random variables then $L$ is: $L = \left[ {\begin{array}{*{20}c}    1 & 0  \\    \rho  & {\sqrt {1 - \rho ^2 } }  \\ \end{array}} \right] $ I understand that this works, but I don't really understand why... My question is: Why does this work?",,"['linear-algebra', 'correlation', 'cholesky-decomposition']"
19,Why are (representations of ) quivers such a big deal?,Why are (representations of ) quivers such a big deal?,,"Quivers are directed graphs where loops and multi-arrows are allowed. And we can talk about representations of quivers by assigning each vertex a vector space and each arrow a homomorphism. Moreover, Gabriel gives a complete classification of quivers of finite type using just five Dynkin diagrams. Although these are both deep and surprising, but I am not sure why quivers deserve so much attention. The only potential application I can think of (although highly unlikely to be true) that they might be useful to answer certain questions in category theory since the notion of quivers are similar to categories, and a representation is very much like a functor from a quiver to some $\mathcal{k}$-$\operatorname{vect}$. So I wonder whether someone can give a hint why quivers deserve so much attention? Do they naturally show up in problems? And do representations of quivers really help to solve these problems?","Quivers are directed graphs where loops and multi-arrows are allowed. And we can talk about representations of quivers by assigning each vertex a vector space and each arrow a homomorphism. Moreover, Gabriel gives a complete classification of quivers of finite type using just five Dynkin diagrams. Although these are both deep and surprising, but I am not sure why quivers deserve so much attention. The only potential application I can think of (although highly unlikely to be true) that they might be useful to answer certain questions in category theory since the notion of quivers are similar to categories, and a representation is very much like a functor from a quiver to some $\mathcal{k}$-$\operatorname{vect}$. So I wonder whether someone can give a hint why quivers deserve so much attention? Do they naturally show up in problems? And do representations of quivers really help to solve these problems?",,"['linear-algebra', 'reference-request', 'category-theory', 'representation-theory', 'quiver']"
20,Why does the Cauchy-Schwarz Inequality even have a name?,Why does the Cauchy-Schwarz Inequality even have a name?,,"When I came across the Cauchy-Schwarz inequality the other day, I found it really weird that this was its own thing, and it had lines upon lines of proof . I've always thought the geometric definition of dot multiplication:  $$|{\bf a }||{\bf b }|\cos \theta$$ is equivalent to the other, algebraic definition: $$a_1\cdot b_1+a_2\cdot b_2+\cdots+a_n\cdot b_n$$ And since the inequality is directly implied by the geometric definition (the fact that $\cos(\theta)$ is $1$ only when $\bf a$ and $\bf b$ are collinear), then shouldn't the Cauchy-Schwarz inequality be the world's most obvious and almost-no-proof-needed thing? Can someone correct me on where my thought process went wrong?","When I came across the Cauchy-Schwarz inequality the other day, I found it really weird that this was its own thing, and it had lines upon lines of proof . I've always thought the geometric definition of dot multiplication:  $$|{\bf a }||{\bf b }|\cos \theta$$ is equivalent to the other, algebraic definition: $$a_1\cdot b_1+a_2\cdot b_2+\cdots+a_n\cdot b_n$$ And since the inequality is directly implied by the geometric definition (the fact that $\cos(\theta)$ is $1$ only when $\bf a$ and $\bf b$ are collinear), then shouldn't the Cauchy-Schwarz inequality be the world's most obvious and almost-no-proof-needed thing? Can someone correct me on where my thought process went wrong?",,"['linear-algebra', 'geometry', 'inequality', 'terminology', 'inner-products']"
21,Why is cross product defined in the way that it is?,Why is cross product defined in the way that it is?,,$\mathbf{a}\times \mathbf{b}$ follows the right hand rule? Why not left hand rule? Why is it $a b \sin (x)$ times the perpendicular vector?  Why is $\sin (x)$ used with the vectors but $\cos(x)$ is a scalar product? So why is cross product defined in the way that it is? I am mainly interested in the right hand rule defintion too as it is out of reach?,$\mathbf{a}\times \mathbf{b}$ follows the right hand rule? Why not left hand rule? Why is it $a b \sin (x)$ times the perpendicular vector?  Why is $\sin (x)$ used with the vectors but $\cos(x)$ is a scalar product? So why is cross product defined in the way that it is? I am mainly interested in the right hand rule defintion too as it is out of reach?,,"['linear-algebra', 'vectors', 'cross-product', 'convention']"
22,Why are $3D$ transformation matrices $4 \times 4$ instead of $3 \times 3$?,Why are  transformation matrices  instead of ?,3D 4 \times 4 3 \times 3,"Background: Many (if not all) of the transformation matrices used in $3D$ computer graphics are $4\times 4$, including the three values for $x$, $y$ and $z$, plus an additional term which usually has a value of $1$. Given the extra computing effort required to multiply $4\times 4$ matrices instead of $3\times 3$ matrices, there must be a substantial benefit to including that extra fourth term, even though $3\times 3$ matrices should (?) be sufficient to describe points and transformations in 3D space. Question: Why is the inclusion of a fourth term beneficial? I can guess that it makes the computations easier in some manner, but I would really like to know why that is the case.","Background: Many (if not all) of the transformation matrices used in $3D$ computer graphics are $4\times 4$, including the three values for $x$, $y$ and $z$, plus an additional term which usually has a value of $1$. Given the extra computing effort required to multiply $4\times 4$ matrices instead of $3\times 3$ matrices, there must be a substantial benefit to including that extra fourth term, even though $3\times 3$ matrices should (?) be sufficient to describe points and transformations in 3D space. Question: Why is the inclusion of a fourth term beneficial? I can guess that it makes the computations easier in some manner, but I would really like to know why that is the case.",,"['linear-algebra', 'matrices', 'geometry']"
23,How to find the multiplicity of eigenvalues?,How to find the multiplicity of eigenvalues?,,"I don't understand how to find the multiplicity for an eigenvalue. To be honest, I am not sure what the books means by multiplicity. For instance, finding the multiplicty of each eigenvalue for the given matrix: $$\begin{bmatrix}1 & 4\\2 & 3\end{bmatrix}$$ I found the eigenvalues of this matrix are -1 and 5, but what are the multiplicities of these?","I don't understand how to find the multiplicity for an eigenvalue. To be honest, I am not sure what the books means by multiplicity. For instance, finding the multiplicty of each eigenvalue for the given matrix: $$\begin{bmatrix}1 & 4\\2 & 3\end{bmatrix}$$ I found the eigenvalues of this matrix are -1 and 5, but what are the multiplicities of these?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
24,An intuitive approach to the Jordan Normal form,An intuitive approach to the Jordan Normal form,,"I want to understand the meaning behind the Jordan Normal form, as I think this is crucial for a mathematician. As far as I understand this, the idea is to get the closest representation of an arbitrary endomorphism towards the diagonal form. As diagonalization is only possible if there are sufficient eigenvectors, we try to get a representation of the endomorphism with respect to its generalized eigenspaces, as their sum always gives us the whole space. Therefore bringing an endomorphism to its Jordan normal form is always possible. How often an eigenvalue appears on the diagonal in the JNF is determined by its algebraic multiplicity.  The number of blocks is determined by its geometric multiplicity. Here I am not sure whether I've got the idea right. I mean, I have trouble interpreting this statement. What is the meaning behind a Jordan normal block and why is the number of these blocks equal to the number of linearly independent eigenvectors? I do not want to see a rigorous proof, but maybe someone could answer for me the following sub-questions. (a) Why do we have to start a new block for each new linearly independent eigenvector that we can find? (b) Why do we not have one block for each generalized eigenspace? (c) What is the intuition behind the fact that the Jordan blocks that contain  at least $k+1$ entries of the eigenvalue $\lambda$ are determined by the following? $$\dim(\ker(A-\lambda I)^{k+1}) - \dim(\ker(A-\lambda I)^k)$$","I want to understand the meaning behind the Jordan Normal form, as I think this is crucial for a mathematician. As far as I understand this, the idea is to get the closest representation of an arbitrary endomorphism towards the diagonal form. As diagonalization is only possible if there are sufficient eigenvectors, we try to get a representation of the endomorphism with respect to its generalized eigenspaces, as their sum always gives us the whole space. Therefore bringing an endomorphism to its Jordan normal form is always possible. How often an eigenvalue appears on the diagonal in the JNF is determined by its algebraic multiplicity.  The number of blocks is determined by its geometric multiplicity. Here I am not sure whether I've got the idea right. I mean, I have trouble interpreting this statement. What is the meaning behind a Jordan normal block and why is the number of these blocks equal to the number of linearly independent eigenvectors? I do not want to see a rigorous proof, but maybe someone could answer for me the following sub-questions. (a) Why do we have to start a new block for each new linearly independent eigenvector that we can find? (b) Why do we not have one block for each generalized eigenspace? (c) What is the intuition behind the fact that the Jordan blocks that contain  at least entries of the eigenvalue are determined by the following?",k+1 \lambda \dim(\ker(A-\lambda I)^{k+1}) - \dim(\ker(A-\lambda I)^k),"['linear-algebra', 'matrices']"
25,What is the origin of the determinant in linear algebra?,What is the origin of the determinant in linear algebra?,,"We often learn in a standard linear algebra course that a determinant is a number associated with a square matrix. We can define the determinant also by saying that it is the sum of all the possible configurations picking an element from a matrix from different rows and different columns multiplied by (-1) or (1) according to the number inversions. But how is this notion of a 'determinant' derived? What is a determinant, actually? I searched up the history of the determinant and it looks like it predates matrices. How did the modern definition of a determinant come about? Why do we need to multiply some terms of the determinant sum by (-1) based on the number of inversions? I just can't understand the motivation that created determinants. We can define determinants, and see their properties, but I want to understand how they were defined and why they were defined to get a better idea of their important and application.","We often learn in a standard linear algebra course that a determinant is a number associated with a square matrix. We can define the determinant also by saying that it is the sum of all the possible configurations picking an element from a matrix from different rows and different columns multiplied by (-1) or (1) according to the number inversions. But how is this notion of a 'determinant' derived? What is a determinant, actually? I searched up the history of the determinant and it looks like it predates matrices. How did the modern definition of a determinant come about? Why do we need to multiply some terms of the determinant sum by (-1) based on the number of inversions? I just can't understand the motivation that created determinants. We can define determinants, and see their properties, but I want to understand how they were defined and why they were defined to get a better idea of their important and application.",,"['linear-algebra', 'matrices', 'determinant', 'math-history']"
26,"Is there a 3-dimensional ""matrix"" by ""matrix"" product?","Is there a 3-dimensional ""matrix"" by ""matrix"" product?",,"Is it possible to multiply A[m,n,k] by B[p,q,r]? Does the regular matrix product have generalized form? I would appreciate it if you could help me to find out some tutorials online or mathematical 'word' which means N-dimensional matrix product. Upd. I'm writing a program that can perform matrix calculations. I created a class called matrix and made it independent from the storage using object oriented features of C++. But when I started to write this program I thought that it was some general operation to multiply for all kinds of arrays(matrices). And my plan was to implement this multiplication (and other operators) and get generalized class of objects. Since this site is not concerned with programming I didn't post too much technical details earlier. Now I'm not quite sure if that one general procedure exists. Thanks for all comments.","Is it possible to multiply A[m,n,k] by B[p,q,r]? Does the regular matrix product have generalized form? I would appreciate it if you could help me to find out some tutorials online or mathematical 'word' which means N-dimensional matrix product. Upd. I'm writing a program that can perform matrix calculations. I created a class called matrix and made it independent from the storage using object oriented features of C++. But when I started to write this program I thought that it was some general operation to multiply for all kinds of arrays(matrices). And my plan was to implement this multiplication (and other operators) and get generalized class of objects. Since this site is not concerned with programming I didn't post too much technical details earlier. Now I'm not quite sure if that one general procedure exists. Thanks for all comments.",,"['linear-algebra', 'matrices']"
27,What is a good book to study linear algebra? [duplicate],What is a good book to study linear algebra? [duplicate],,"This question already has answers here : Prerequisites/Books for A First Course in Linear Algebra (6 answers) Closed last year . I'm looking for a book to learn Algebra. The programme is the following. The units marked with a $\star$ are the ones I'm most interested in (in the sense I know nothing about) and those with a $\circ$ are those which I'm mildly comfortable with. The ones that aren't marked shouldn't be of importance. Any important topic inside a unite will be boldfaced. U1: Vector Algebra. Points in the $n$-dimensional space. Vectors. Scalar product. Norm. Lines and planes. Vectorial product. $\circ$ U2: Vector Spaces. Definition. Subspaces. Linear independence. Linear combination. Generating systems. Basis. Dimesion. Sum and intersection of subspaces. Direct sum. Spaces with inner products. $\circ$ U3: Matrices and determinants. Matrix Spaces. Sum and product of matrices. Linear ecuations. Gauss-Jordan elimination. Range. Roché Frobenius Theorem. Determinants. Properties. Determinant of a product. Determinants and inverses. $\star$ U4: Linear transformations. Definition. Nucleus and image. Monomorphisms, epimorphisms and isomorphisms. Composition of linear transformations. Inverse linear tranforms. U5: Complex numbers and polynomials. Complex numbers. Operations. Binomial and trigonometric form. De Möivre's Theorem.  Solving equations. Polynomials. Degree. Operations. Roots. Remainder theorem. Factorial decomposition. FTA. Lagrange interpolation. $\star$ U6: Linear transformations and matrices. Matrix of a linear transformation. Matrix of the composition. Matrix of the inverse. Base changes. $\star$ U7: Eigen values and eigen vectors Eigen values and eigen vectors. Characteristc polynomial. Aplications. Invariant subspaces. Diagonalization. To let you know, I own a copy of Apostol's Calculus $\mathrm I $ which has some of those topics, precisely: Linear Spaces Linear Transformations and Matrices. I also have a copy of Apostol's second book of Calc $\mathrm II$which continues with Determinants Eigenvalues and eigenvectors Eigenvalues of operators in Euclidean spaces. I was reccommended Linear Algebra by Armando Rojo and have Linear Algebra by Carlos Ivorra , which seems quite a good text. What do you reccomend?","This question already has answers here : Prerequisites/Books for A First Course in Linear Algebra (6 answers) Closed last year . I'm looking for a book to learn Algebra. The programme is the following. The units marked with a $\star$ are the ones I'm most interested in (in the sense I know nothing about) and those with a $\circ$ are those which I'm mildly comfortable with. The ones that aren't marked shouldn't be of importance. Any important topic inside a unite will be boldfaced. U1: Vector Algebra. Points in the $n$-dimensional space. Vectors. Scalar product. Norm. Lines and planes. Vectorial product. $\circ$ U2: Vector Spaces. Definition. Subspaces. Linear independence. Linear combination. Generating systems. Basis. Dimesion. Sum and intersection of subspaces. Direct sum. Spaces with inner products. $\circ$ U3: Matrices and determinants. Matrix Spaces. Sum and product of matrices. Linear ecuations. Gauss-Jordan elimination. Range. Roché Frobenius Theorem. Determinants. Properties. Determinant of a product. Determinants and inverses. $\star$ U4: Linear transformations. Definition. Nucleus and image. Monomorphisms, epimorphisms and isomorphisms. Composition of linear transformations. Inverse linear tranforms. U5: Complex numbers and polynomials. Complex numbers. Operations. Binomial and trigonometric form. De Möivre's Theorem.  Solving equations. Polynomials. Degree. Operations. Roots. Remainder theorem. Factorial decomposition. FTA. Lagrange interpolation. $\star$ U6: Linear transformations and matrices. Matrix of a linear transformation. Matrix of the composition. Matrix of the inverse. Base changes. $\star$ U7: Eigen values and eigen vectors Eigen values and eigen vectors. Characteristc polynomial. Aplications. Invariant subspaces. Diagonalization. To let you know, I own a copy of Apostol's Calculus $\mathrm I $ which has some of those topics, precisely: Linear Spaces Linear Transformations and Matrices. I also have a copy of Apostol's second book of Calc $\mathrm II$which continues with Determinants Eigenvalues and eigenvectors Eigenvalues of operators in Euclidean spaces. I was reccommended Linear Algebra by Armando Rojo and have Linear Algebra by Carlos Ivorra , which seems quite a good text. What do you reccomend?",,"['linear-algebra', 'reference-request', 'book-recommendation']"
28,"If the field of a vector space weren't characteristic zero, then what would change in the theory?","If the field of a vector space weren't characteristic zero, then what would change in the theory?",,"In the book of Linear Algebra by Werner Greub, whenever we choose a field for our vector spaces, we always choose an arbitrary field $F$ of characteristic zero , but to understand the importance of the this property, I am wondering what would we lose if the field weren't characteristic zero ? I mean, right now I'm in the middle of the Chapter 4, and up to now we have used the fact that the field is characteristic zero once in a single proof, so as main theorems and properties, if the field weren't characteristic zero, what we would we lose ? Note, I'm asking this particular question to understand the importance and the place of this fact in the subject, so if you have any other idea to convey this, I'm also OK with that. Note: Since this a broad question, it is unlikely that one person will cover all the cases, so I will not accept any answer so that you can always post answers.","In the book of Linear Algebra by Werner Greub, whenever we choose a field for our vector spaces, we always choose an arbitrary field $F$ of characteristic zero , but to understand the importance of the this property, I am wondering what would we lose if the field weren't characteristic zero ? I mean, right now I'm in the middle of the Chapter 4, and up to now we have used the fact that the field is characteristic zero once in a single proof, so as main theorems and properties, if the field weren't characteristic zero, what we would we lose ? Note, I'm asking this particular question to understand the importance and the place of this fact in the subject, so if you have any other idea to convey this, I'm also OK with that. Note: Since this a broad question, it is unlikely that one person will cover all the cases, so I will not accept any answer so that you can always post answers.",,"['linear-algebra', 'matrices', 'vector-spaces', 'field-theory', 'linear-transformations']"
29,What is the difference between kernel and null space?,What is the difference between kernel and null space?,,"What is the difference, if any, between kernel and null space ? I previously understood the kernel to be of a linear map and the null space to be of a matrix: i.e., for any linear map $f : V \to W$, $$ \ker(f) \cong \operatorname{null}(A), $$ where $\cong$ represents isomorphism with respect to $+$ and $\cdot$, and $A$ is the matrix of $f$ with respect to some source and target bases. However, I took a class with a professor last year who used $\ker$ on matrices. Was that just an abuse of notation or have I had things mixed up all along?","What is the difference, if any, between kernel and null space ? I previously understood the kernel to be of a linear map and the null space to be of a matrix: i.e., for any linear map $f : V \to W$, $$ \ker(f) \cong \operatorname{null}(A), $$ where $\cong$ represents isomorphism with respect to $+$ and $\cdot$, and $A$ is the matrix of $f$ with respect to some source and target bases. However, I took a class with a professor last year who used $\ker$ on matrices. Was that just an abuse of notation or have I had things mixed up all along?",,"['linear-algebra', 'notation', 'terminology']"
30,Axiom of choice and automorphisms of vector spaces,Axiom of choice and automorphisms of vector spaces,,"A classical exercise in group theory is ""Show that if a group has a trivial automorphism group, then it is of order $1$ or $2$."" I think that the straightforward solution uses that a exponent two group is a vector space over $\operatorname{GF}(2)$, and therefore has nontrivial automorphisms as soon as its dimension is at least $2$ (simply transposing two basis vectors). My question is now natural: Is it possible, without the axiom of choice, to construct a vector space $E$ over $\operatorname{GF}(2)$, different from $\{0\}$ or $\operatorname{GF}(2)$, whose automorphism group $\operatorname{GL}(E)$ is trivial?","A classical exercise in group theory is ""Show that if a group has a trivial automorphism group, then it is of order $1$ or $2$."" I think that the straightforward solution uses that a exponent two group is a vector space over $\operatorname{GF}(2)$, and therefore has nontrivial automorphisms as soon as its dimension is at least $2$ (simply transposing two basis vectors). My question is now natural: Is it possible, without the axiom of choice, to construct a vector space $E$ over $\operatorname{GF}(2)$, different from $\{0\}$ or $\operatorname{GF}(2)$, whose automorphism group $\operatorname{GL}(E)$ is trivial?",,"['linear-algebra', 'axiom-of-choice']"
31,Difference between bijection and isomorphism?,Difference between bijection and isomorphism?,,"First, let me admit that I suffer from a fundamental confusion here, and so I will likely say something wrong. No pretenses here, just looking for a good explanation. There is a theorem from linear algebra that two vector spaces are isomorphic if and only if they have the same dimension. It is also well-known that two sets have the same cardinality if and only if there exists a bijection between them. Herein lies the issue... Obviously $|\mathbb{R}| = |\mathbb{R^2}| = \mathfrak c.$ This is often stated as ""there are as many points in the plane as there are on a line."" Why, then, are $\mathbb{R}$ and $\mathbb{R^2}$ not isomorphic? It makes intuitive sense that they shouldn't be. After all, I can only ""match up"" each point in $\mathbb{R}$ with the first coordinate of $\mathbb{R^2}.$ I cannot trust this intuition, however, because it fails when considering the possibility of a bijection $f : \mathbb{N} \rightarrow \mathbb{Q}!$ Even more confusing: As real vector spaces, $\mathbb{C}$ is isomorphic to $\mathbb{R^2}.$ However there is a bijection between $\mathbb{C}$ and $\mathbb{R}$ (just consider the line $\rightarrow$ plane example as above). If you can explain the error in my thinking, please help!","First, let me admit that I suffer from a fundamental confusion here, and so I will likely say something wrong. No pretenses here, just looking for a good explanation. There is a theorem from linear algebra that two vector spaces are isomorphic if and only if they have the same dimension. It is also well-known that two sets have the same cardinality if and only if there exists a bijection between them. Herein lies the issue... Obviously $|\mathbb{R}| = |\mathbb{R^2}| = \mathfrak c.$ This is often stated as ""there are as many points in the plane as there are on a line."" Why, then, are $\mathbb{R}$ and $\mathbb{R^2}$ not isomorphic? It makes intuitive sense that they shouldn't be. After all, I can only ""match up"" each point in $\mathbb{R}$ with the first coordinate of $\mathbb{R^2}.$ I cannot trust this intuition, however, because it fails when considering the possibility of a bijection $f : \mathbb{N} \rightarrow \mathbb{Q}!$ Even more confusing: As real vector spaces, $\mathbb{C}$ is isomorphic to $\mathbb{R^2}.$ However there is a bijection between $\mathbb{C}$ and $\mathbb{R}$ (just consider the line $\rightarrow$ plane example as above). If you can explain the error in my thinking, please help!",,"['linear-algebra', 'elementary-set-theory']"
32,In which cases is the inverse of a matrix equal to its transpose?,In which cases is the inverse of a matrix equal to its transpose?,,"In which cases is the inverse of a matrix equal to its transpose, that is, when do we have $A^{-1} = A^{T}$ ? Is it when $A$ is orthogonal?","In which cases is the inverse of a matrix equal to its transpose, that is, when do we have ? Is it when is orthogonal?",A^{-1} = A^{T} A,"['linear-algebra', 'matrices', 'orthogonal-matrices']"
33,How to interpret the adjoint?,How to interpret the adjoint?,,"Let $V \neq \{\mathbf{0}\}$ be a inner product space, and let $f:V \to V$ be a linear transformation on $V$. I understand the definition 1 of the adjoint of $f$ (denoted by $f^*$), but I can't say I really grok this other linear transformation $f^*$. For example, it is completely unexpected to me that to say that $f^* = f^{-1}$ is equivalent to saying that $f$ preserves all distances and angles (as defined by the inner product on $V$). It is even more surprising to me to learn that to say that $f^* = f$ is equivalent to saying that there exists an orthonormal basis for $V$ that consists entirely of eigenvectors of $f$. Now, I can follow the proofs of these theorems perfectly well, but the exercise gives me no insight into the nature of the adjoint . For example, I can visualize a linear transformation $f:V\to V$ whose eigenvectors are orthogonal and span the space, but this visualization tells me nothing about what $f^*$ should be like when this is the case, largely because I'm completely in the dark about the adjoint in general . Similarly, I can visualize a linear transformation $f:V\to V$ that preserves lengths and angles, but, again, and for the same reason, this visualization tells me nothing about what this implies for $f^*$. Is there (coordinate-free, representation-agnostic) way to interpret the adjoint that will make theorems like the ones mentioned above less surprising? 1 The adjoint of $f:V\to V$ is the unique linear transformation $f^*:V\to V$ (guaranteed to exist for every such linear transformation $f$) such that, for all $u, v \in V$, $$ \langle f(u), v\rangle  = \langle u, f^*(v)\rangle \,.$$","Let $V \neq \{\mathbf{0}\}$ be a inner product space, and let $f:V \to V$ be a linear transformation on $V$. I understand the definition 1 of the adjoint of $f$ (denoted by $f^*$), but I can't say I really grok this other linear transformation $f^*$. For example, it is completely unexpected to me that to say that $f^* = f^{-1}$ is equivalent to saying that $f$ preserves all distances and angles (as defined by the inner product on $V$). It is even more surprising to me to learn that to say that $f^* = f$ is equivalent to saying that there exists an orthonormal basis for $V$ that consists entirely of eigenvectors of $f$. Now, I can follow the proofs of these theorems perfectly well, but the exercise gives me no insight into the nature of the adjoint . For example, I can visualize a linear transformation $f:V\to V$ whose eigenvectors are orthogonal and span the space, but this visualization tells me nothing about what $f^*$ should be like when this is the case, largely because I'm completely in the dark about the adjoint in general . Similarly, I can visualize a linear transformation $f:V\to V$ that preserves lengths and angles, but, again, and for the same reason, this visualization tells me nothing about what this implies for $f^*$. Is there (coordinate-free, representation-agnostic) way to interpret the adjoint that will make theorems like the ones mentioned above less surprising? 1 The adjoint of $f:V\to V$ is the unique linear transformation $f^*:V\to V$ (guaranteed to exist for every such linear transformation $f$) such that, for all $u, v \in V$, $$ \langle f(u), v\rangle  = \langle u, f^*(v)\rangle \,.$$",,"['linear-algebra', 'fourier-analysis', 'hilbert-spaces', 'inner-products']"
34,How to identify surfaces of revolution,How to identify surfaces of revolution,,"Given a surface $f(x,y,z)=0$, how would you determine whether or not it's a surface of revolution, and find the axis of rotation? The special case where $f$ is a polynomial is also of interest. A few ideas that might lead somewhere, maybe: (1) For algebra folks: Surfaces of the form $z = g(x^2 + y^2)$ are always surfaces of revolution. I don't know if the converse is true. If it is, then we just need to find a coordinate system in which $f$ has this particular form. Finding special coordinate systems that simplify things often involves finding eigenvalues. You use eigenvalues to tackle the special case of quadric surfaces, anyway. (2) For differential geometry folks: Surfaces of revolution have a very special pattern of lines of curvature. One family of lines of curvature is a set of coaxial circles. I don't know if this property characterizes surfaces of revolution, but it sounds promising. (3) For physicists & engineers: The axis of rotation must be one of the principal axes for the centroidal moments of inertia, according to physics notes I have read. So, we should compute the centroid and the inertia tensor. I'm not sure how. Then, diagonalize, so eigenvalues, again. Maybe this is actually the same as idea #1. (4) For classical geometry folks: What characterizes surfaces of revolution (I think) is that every line that's normal to the surface intersects some fixed line, which is the axis of rotation. So, construct the normal lines at a few points on the surface (how many??), and see if there is some line $L$ that intersects all of these normal lines (how?). See this related question . If there is, then this line $L$ is (probably) the desired axis of rotation. This seems somehow related to the answers given by Holographer and zyx. Why is this is interesting/important? Because surfaces of revolution are easier to handle computationally (e.g. area and volume calculations), and easy to manufacture (using a lathe). So, it's useful to be able to identify them, so that they can be treated as special cases. The question is related to this one about symmetric surfaces , I think. The last three paragraphs of that question (about centroids) apply here, too. Specifically, if we have a bounded (compact) surface of revolution, then its axis of rotation must pass through its centroid, so some degrees of freedom disappear. If you want to test out your ideas, you can try experimenting with $$ f(x,y,z) = -6561 + 5265 x^2 + 256 x^4 + 4536 x y - 1792 x^3 y + 2592 y^2 +   4704 x^2 y^2 - 5488 x y^3 + 2401 y^4 + 2592 x z - 1024 x^3 z -   4536 y z + 5376 x^2 y z - 9408 x y^2 z + 5488 y^3 z + 5265 z^2 +   1536 x^2 z^2 - 5376 x y z^2 + 4704 y^2 z^2 - 1024 x z^3 +   1792 y z^3 + 256 z^4 $$ This is a surface of revolution, and it's compact. Sorry it's such a big ugly mess. It was the simplest compact non-quadric example I could invent. If we rotate to a $(u,v,w)$ coordinate system, where \begin{align} u &= \tfrac{1}{9}( x - 4 y + 8 z) \\ v &= \tfrac{1}{9}(8 x + 4 y + z)  \\ w &= \tfrac{1}{9}(-4 x + 7 y + 4 z) \end{align} then the surface becomes $$ u^2 + v^2 + w^4 - 1 = 0 $$ which is a surface of revolution having the $w$-axis as its axis of rotation.","Given a surface $f(x,y,z)=0$, how would you determine whether or not it's a surface of revolution, and find the axis of rotation? The special case where $f$ is a polynomial is also of interest. A few ideas that might lead somewhere, maybe: (1) For algebra folks: Surfaces of the form $z = g(x^2 + y^2)$ are always surfaces of revolution. I don't know if the converse is true. If it is, then we just need to find a coordinate system in which $f$ has this particular form. Finding special coordinate systems that simplify things often involves finding eigenvalues. You use eigenvalues to tackle the special case of quadric surfaces, anyway. (2) For differential geometry folks: Surfaces of revolution have a very special pattern of lines of curvature. One family of lines of curvature is a set of coaxial circles. I don't know if this property characterizes surfaces of revolution, but it sounds promising. (3) For physicists & engineers: The axis of rotation must be one of the principal axes for the centroidal moments of inertia, according to physics notes I have read. So, we should compute the centroid and the inertia tensor. I'm not sure how. Then, diagonalize, so eigenvalues, again. Maybe this is actually the same as idea #1. (4) For classical geometry folks: What characterizes surfaces of revolution (I think) is that every line that's normal to the surface intersects some fixed line, which is the axis of rotation. So, construct the normal lines at a few points on the surface (how many??), and see if there is some line $L$ that intersects all of these normal lines (how?). See this related question . If there is, then this line $L$ is (probably) the desired axis of rotation. This seems somehow related to the answers given by Holographer and zyx. Why is this is interesting/important? Because surfaces of revolution are easier to handle computationally (e.g. area and volume calculations), and easy to manufacture (using a lathe). So, it's useful to be able to identify them, so that they can be treated as special cases. The question is related to this one about symmetric surfaces , I think. The last three paragraphs of that question (about centroids) apply here, too. Specifically, if we have a bounded (compact) surface of revolution, then its axis of rotation must pass through its centroid, so some degrees of freedom disappear. If you want to test out your ideas, you can try experimenting with $$ f(x,y,z) = -6561 + 5265 x^2 + 256 x^4 + 4536 x y - 1792 x^3 y + 2592 y^2 +   4704 x^2 y^2 - 5488 x y^3 + 2401 y^4 + 2592 x z - 1024 x^3 z -   4536 y z + 5376 x^2 y z - 9408 x y^2 z + 5488 y^3 z + 5265 z^2 +   1536 x^2 z^2 - 5376 x y z^2 + 4704 y^2 z^2 - 1024 x z^3 +   1792 y z^3 + 256 z^4 $$ This is a surface of revolution, and it's compact. Sorry it's such a big ugly mess. It was the simplest compact non-quadric example I could invent. If we rotate to a $(u,v,w)$ coordinate system, where \begin{align} u &= \tfrac{1}{9}( x - 4 y + 8 z) \\ v &= \tfrac{1}{9}(8 x + 4 y + z)  \\ w &= \tfrac{1}{9}(-4 x + 7 y + 4 z) \end{align} then the surface becomes $$ u^2 + v^2 + w^4 - 1 = 0 $$ which is a surface of revolution having the $w$-axis as its axis of rotation.",,"['linear-algebra', 'geometry', 'algebraic-geometry', 'differential-geometry', 'surfaces']"
35,Matrix multiplication: interpreting and understanding the process,Matrix multiplication: interpreting and understanding the process,,"I have just watched the first half of the 3rd lecture of Gilbert Strang on the open course ware with link: http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/ It seems that with a matrix multiplication $AB=C$, that the entries as scalars, are formed from the dot product computations of the rows of $A$ with the columns of $B$. Visual interpretations from mechanics of overlpaing forces come to mind immediately because that is the source for the dot product (inner product). I see the rows of $C$ as being the dot product of the rows of $B$, with the dot product of a particular row of $A$. Similar to the above and it is easy to see this from the individual entries in the matrix $C$ as to which elements change to give which dot products. For understanding matrix multiplication there is the geometrical interpretation, that the matrix multiplication is a change in the reference system since matrix $B$ can be seen as a transormation operator for rotation, scalling, reflection and skew. It is easy to see this by constructing example $B$ matrices with these effects on $A$. This decomposition is a strong argument and is strongly convincing of its generality. This interpreation is strong but not smooth because I would find smoother an explanation which would be an interpretation begining from the dot product of vectors and using this to explain the process and the interpretation of the results (one which is a bit easier to see without many examples of the putting numbers in and seeing what comes out which students go through). I can hope that sticking to dot products throughout the explanation and THEN seeing how these can be seen to produce scalings, rotations, and skewings would be better. But, after some simple graphical examples I saw this doesn't work as the order of the columns in matrix $B$ are important and don't show in the graphical representation. The best explanation I can find is at Yahoo Answers . It is convincing but a bit disappointing (explains why this approach preserves the ""composition of linear transformations""; thanks @Arturo Magidin). So the question is: Why does matrix multiplication happen as it does, and are there good practical examples to support it? Preferably not via rotations/scalings/skews (thanks @lhf).","I have just watched the first half of the 3rd lecture of Gilbert Strang on the open course ware with link: http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/ It seems that with a matrix multiplication $AB=C$, that the entries as scalars, are formed from the dot product computations of the rows of $A$ with the columns of $B$. Visual interpretations from mechanics of overlpaing forces come to mind immediately because that is the source for the dot product (inner product). I see the rows of $C$ as being the dot product of the rows of $B$, with the dot product of a particular row of $A$. Similar to the above and it is easy to see this from the individual entries in the matrix $C$ as to which elements change to give which dot products. For understanding matrix multiplication there is the geometrical interpretation, that the matrix multiplication is a change in the reference system since matrix $B$ can be seen as a transormation operator for rotation, scalling, reflection and skew. It is easy to see this by constructing example $B$ matrices with these effects on $A$. This decomposition is a strong argument and is strongly convincing of its generality. This interpreation is strong but not smooth because I would find smoother an explanation which would be an interpretation begining from the dot product of vectors and using this to explain the process and the interpretation of the results (one which is a bit easier to see without many examples of the putting numbers in and seeing what comes out which students go through). I can hope that sticking to dot products throughout the explanation and THEN seeing how these can be seen to produce scalings, rotations, and skewings would be better. But, after some simple graphical examples I saw this doesn't work as the order of the columns in matrix $B$ are important and don't show in the graphical representation. The best explanation I can find is at Yahoo Answers . It is convincing but a bit disappointing (explains why this approach preserves the ""composition of linear transformations""; thanks @Arturo Magidin). So the question is: Why does matrix multiplication happen as it does, and are there good practical examples to support it? Preferably not via rotations/scalings/skews (thanks @lhf).",,"['linear-algebra', 'matrices', 'intuition', 'faq']"
36,Motivation to understand double dual space,Motivation to understand double dual space,,"I am helping my brother with Linear Algebra.  I am not able to motivate him to understand what double dual space is.  Is there a nice way of explaining the concept?  Thanks for your advices, examples and theories.","I am helping my brother with Linear Algebra.  I am not able to motivate him to understand what double dual space is.  Is there a nice way of explaining the concept?  Thanks for your advices, examples and theories.",,"['linear-algebra', 'duality-theorems', 'dual-spaces']"
37,"Is there an abstract definition of a matrix being ""upper triangular""?","Is there an abstract definition of a matrix being ""upper triangular""?",,"Another question brought this up.  The only definition I have ever seen for a matrix being upper triangular is, written in component forms, ""all the components below the main diagonal are zero.""  But of course that property is basis dependent.  It is not preserved under change of basis. Yet it doesn't seem as if it would be purely arbitrary because the product of upper triangular matrices is upper triangular, and so forth.  It has closure.  Is there some other sort of transformation besides a basis transformation that might be relevant here?  It seems as if a set of matrices having this property should have some sort of invariants. Is there some sort of isomorphism between the sets of upper triangular matrices in different bases?","Another question brought this up.  The only definition I have ever seen for a matrix being upper triangular is, written in component forms, ""all the components below the main diagonal are zero.""  But of course that property is basis dependent.  It is not preserved under change of basis. Yet it doesn't seem as if it would be purely arbitrary because the product of upper triangular matrices is upper triangular, and so forth.  It has closure.  Is there some other sort of transformation besides a basis transformation that might be relevant here?  It seems as if a set of matrices having this property should have some sort of invariants. Is there some sort of isomorphism between the sets of upper triangular matrices in different bases?",,"['linear-algebra', 'matrices', 'change-of-basis', 'triangularization']"
38,Is there a vector space that cannot be an inner product space?,Is there a vector space that cannot be an inner product space?,,"Quick question: Can I define some inner product on any arbitrary vector space such that it becomes an inner product space? If yes, how can I prove this? If no, what would be a counter example? Thanks a lot in advance.","Quick question: Can I define some inner product on any arbitrary vector space such that it becomes an inner product space? If yes, how can I prove this? If no, what would be a counter example? Thanks a lot in advance.",,"['linear-algebra', 'vector-spaces', 'inner-products']"
39,Best Fitting Plane given a Set of Points,Best Fitting Plane given a Set of Points,,Nothing more to explain. I just don't know how to find the best fitting plane given a set of $N$ points in a $3D$ space. I then have to write the corresponding algorithm. Thank you ;),Nothing more to explain. I just don't know how to find the best fitting plane given a set of $N$ points in a $3D$ space. I then have to write the corresponding algorithm. Thank you ;),,['linear-algebra']
40,Is a matrix $A$ with an eigenvalue of $0$ invertible?,Is a matrix  with an eigenvalue of  invertible?,A 0,"Just wanted some input to see if my proof is satisfactory or if it needs some cleaning up. Here is what I have. Proof Suppose $A$ is square matrix and invertible and, for the sake of contradiction, let $0$ be an eigenvalue.  Consider, $(A-\lambda I)\cdot v = 0$ with $\lambda=0 $    $$\Rightarrow (A- 0\cdot I)v=0$$ $$\Rightarrow(A-0)v=0$$ $$\Rightarrow Av=0$$ We know $A$ is an invertible and in order for $Av = 0$, $v = 0$, but $v$ must be non-trivial such that $\det(A-\lambda I) = 0$. Here lies our contradiction. Hence, $0$ cannot be an eigenvalue. Revised Proof Suppose $A$ is square matrix and has an eigenvalue of $0$.  For the sake of contradiction, lets assume $A$ is invertible. Consider, $Av = \lambda v$, with $\lambda = 0$ means there exists a non-zero $v$ such that $Av = 0$. This implies $Av = 0v \Rightarrow Av = 0$ For an invertible matrix $A$, $Av = 0$ implies $v = 0$.  So, $Av = 0 = A\cdot 0$. Since $v$  cannot be $0$,this means $A$ must not have been one-to-one.  Hence, our contradiction, $A$ must not be invertible.","Just wanted some input to see if my proof is satisfactory or if it needs some cleaning up. Here is what I have. Proof Suppose $A$ is square matrix and invertible and, for the sake of contradiction, let $0$ be an eigenvalue.  Consider, $(A-\lambda I)\cdot v = 0$ with $\lambda=0 $    $$\Rightarrow (A- 0\cdot I)v=0$$ $$\Rightarrow(A-0)v=0$$ $$\Rightarrow Av=0$$ We know $A$ is an invertible and in order for $Av = 0$, $v = 0$, but $v$ must be non-trivial such that $\det(A-\lambda I) = 0$. Here lies our contradiction. Hence, $0$ cannot be an eigenvalue. Revised Proof Suppose $A$ is square matrix and has an eigenvalue of $0$.  For the sake of contradiction, lets assume $A$ is invertible. Consider, $Av = \lambda v$, with $\lambda = 0$ means there exists a non-zero $v$ such that $Av = 0$. This implies $Av = 0v \Rightarrow Av = 0$ For an invertible matrix $A$, $Av = 0$ implies $v = 0$.  So, $Av = 0 = A\cdot 0$. Since $v$  cannot be $0$,this means $A$ must not have been one-to-one.  Hence, our contradiction, $A$ must not be invertible.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'solution-verification', 'determinant']"
41,Why is the ring of matrices over a field simple?,Why is the ring of matrices over a field simple?,,"Denote by $M_{n \times n}(k)$ the ring of $n$ by $n$ matrices with coefficients in the field $k$.  Then why does this ring not contain any two-sided ideal? Thanks for any clarification, and this is an exercise from the notes of Commutative Algebra by Pete L Clark, of which I thought as simple but I cannot figure it out now.","Denote by $M_{n \times n}(k)$ the ring of $n$ by $n$ matrices with coefficients in the field $k$.  Then why does this ring not contain any two-sided ideal? Thanks for any clarification, and this is an exercise from the notes of Commutative Algebra by Pete L Clark, of which I thought as simple but I cannot figure it out now.",,"['linear-algebra', 'abstract-algebra']"
42,"What's the sign of $\det\left(\sqrt{i^2+j^2}\right)_{1\le i,j\le n}$?",What's the sign of ?,"\det\left(\sqrt{i^2+j^2}\right)_{1\le i,j\le n}",Suppose $A=(a_{ij})$ is a $n×n$ matrix by $a_{ij}=\sqrt{i^2+j^2}$ . I have tried to check its sign by matlab. l find that the determinant is positive when n is odd and negative when n is even. How to prove it？,Suppose is a matrix by . I have tried to check its sign by matlab. l find that the determinant is positive when n is odd and negative when n is even. How to prove it？,A=(a_{ij}) n×n a_{ij}=\sqrt{i^2+j^2},"['linear-algebra', 'determinant']"
43,Solutions to the matrix equation $\mathbf{AB-BA=I}$ over general fields,Solutions to the matrix equation  over general fields,\mathbf{AB-BA=I},"Some days ago, I was thinking on a problem, which states that $$AB-BA=I$$ does not have a solution in $M_{n\times n}(\mathbb R)$ and $M_{n\times n}(\mathbb C)$ . (Here $M_{n\times n}(\mathbb F)$ denotes the set of all $n\times n$ matrices with entries from the field $\mathbb F$ and $I$ is the identity matrix.) Although I couldn't solve the problem, I came up with this problem: Does there exist a field $\mathbb F$ for which that equation $AB-BA=I$ has a solution in $M_{n\times n}(\mathbb F)$ ? I'd really appreciate your help.","Some days ago, I was thinking on a problem, which states that does not have a solution in and . (Here denotes the set of all matrices with entries from the field and is the identity matrix.) Although I couldn't solve the problem, I came up with this problem: Does there exist a field for which that equation has a solution in ? I'd really appreciate your help.",AB-BA=I M_{n\times n}(\mathbb R) M_{n\times n}(\mathbb C) M_{n\times n}(\mathbb F) n\times n \mathbb F I \mathbb F AB-BA=I M_{n\times n}(\mathbb F),"['linear-algebra', 'matrices', 'field-theory', 'matrix-equations']"
44,Getting the inverse of a lower/upper triangular matrix,Getting the inverse of a lower/upper triangular matrix,,"For a lower triangular matrix, the inverse of itself should be easy to find because that's the idea of the LU decomposition, am I right? For many of the lower or upper triangular matrices, often I could just flip the signs to get its inverse. For eg: $$\begin{bmatrix} 1 & 0 & 0\\  0 & 1 & 0\\  -1.5 & 0 & 1 \end{bmatrix}^{-1}= \begin{bmatrix} 1 & 0 & 0\\  0 & 1 & 0\\  1.5 & 0 & 1 \end{bmatrix}$$ I just flipped from -1.5 to 1.5 and I got the inverse. But this apparently doesn't work all the time. Say in this matrix:  $$\begin{bmatrix} 1 & 0 & 0\\  -2 & 1 & 0\\  3.5 & -2.5 & 1 \end{bmatrix}^{-1}\neq  \begin{bmatrix} 1 & 0 & 0\\  2 & 1 & 0\\  -3.5 & 2.5 & 1 \end{bmatrix}$$ By flipping the signs, the inverse is wrong. But if I go through the whole tedious step of gauss-jordan elimination, I would get its correct inverse like this: $\begin{bmatrix} 1 & 0 & 0\\  -2 & 1 & 0\\  3.5 & -2.5 & 1 \end{bmatrix}^{-1}= \begin{bmatrix} 1 & 0 & 0\\  2 & 1 & 0\\  1.5 & 2.5 & 1 \end{bmatrix}$ And it looks like some entries could just flip its signs but not for others. Then this is kind of weird because I thought the whole idea of getting the lower and upper triangular matrices is to avoid the need to go through the tedious process of gauss-jordan elimination and can get the inverse quickly by flipping signs? Maybe I have missed something out here. How should I get an inverse of a lower or an upper matrix quickly?","For a lower triangular matrix, the inverse of itself should be easy to find because that's the idea of the LU decomposition, am I right? For many of the lower or upper triangular matrices, often I could just flip the signs to get its inverse. For eg: $$\begin{bmatrix} 1 & 0 & 0\\  0 & 1 & 0\\  -1.5 & 0 & 1 \end{bmatrix}^{-1}= \begin{bmatrix} 1 & 0 & 0\\  0 & 1 & 0\\  1.5 & 0 & 1 \end{bmatrix}$$ I just flipped from -1.5 to 1.5 and I got the inverse. But this apparently doesn't work all the time. Say in this matrix:  $$\begin{bmatrix} 1 & 0 & 0\\  -2 & 1 & 0\\  3.5 & -2.5 & 1 \end{bmatrix}^{-1}\neq  \begin{bmatrix} 1 & 0 & 0\\  2 & 1 & 0\\  -3.5 & 2.5 & 1 \end{bmatrix}$$ By flipping the signs, the inverse is wrong. But if I go through the whole tedious step of gauss-jordan elimination, I would get its correct inverse like this: $\begin{bmatrix} 1 & 0 & 0\\  -2 & 1 & 0\\  3.5 & -2.5 & 1 \end{bmatrix}^{-1}= \begin{bmatrix} 1 & 0 & 0\\  2 & 1 & 0\\  1.5 & 2.5 & 1 \end{bmatrix}$ And it looks like some entries could just flip its signs but not for others. Then this is kind of weird because I thought the whole idea of getting the lower and upper triangular matrices is to avoid the need to go through the tedious process of gauss-jordan elimination and can get the inverse quickly by flipping signs? Maybe I have missed something out here. How should I get an inverse of a lower or an upper matrix quickly?",,"['linear-algebra', 'matrices']"
45,Why $\{\mathbf{0}\}$ has dimension zero?,Why  has dimension zero?,\{\mathbf{0}\},According to C.H. Edwards' Advanced Calculus of Several Variables : The dimension of the subspace $V$ is defined to be the minimal number of vectors required to generate $V$ (pp. 4). Then why does $\{\mathbf{0}\}$ have dimension zero instead of one? Shouldn't it be true that only the empty set has dimension zero?,According to C.H. Edwards' Advanced Calculus of Several Variables : The dimension of the subspace is defined to be the minimal number of vectors required to generate (pp. 4). Then why does have dimension zero instead of one? Shouldn't it be true that only the empty set has dimension zero?,V V \{\mathbf{0}\},['linear-algebra']
46,Inverse matrix’s eigenvalue?,Inverse matrix’s eigenvalue?,,"It’s from the book “Linear Algebra and its Applications” by Gilbert Strang, page 260. $$(I-A)^{-1}=I+A+A^2+A^3+\ldots$$ Nonnegative matrix $A$ has the largest eigenvalue $\lambda_1<1$ . Then, the book says $(I-A)^{-1}$ has the same eigenvector, with eigenvalue $1/(1-\lambda_1)$ . Why? Is there any other formulas between inverse matrix and eigenvalue that I don’t know?","It’s from the book “Linear Algebra and its Applications” by Gilbert Strang, page 260. Nonnegative matrix has the largest eigenvalue . Then, the book says has the same eigenvector, with eigenvalue . Why? Is there any other formulas between inverse matrix and eigenvalue that I don’t know?",(I-A)^{-1}=I+A+A^2+A^3+\ldots A \lambda_1<1 (I-A)^{-1} 1/(1-\lambda_1),"['linear-algebra', 'eigenvalues-eigenvectors']"
47,Inverse of an invertible triangular matrix (either upper or lower) is triangular of the same kind,Inverse of an invertible triangular matrix (either upper or lower) is triangular of the same kind,,How can we prove that the inverse of an upper (lower) triangular matrix is upper (lower) triangular?,How can we prove that the inverse of an upper (lower) triangular matrix is upper (lower) triangular?,,"['linear-algebra', 'matrices', 'inverse']"
48,What kind of matrices are non-diagonalizable?,What kind of matrices are non-diagonalizable?,,"I'm trying to build an intuitive geometric picture about diagonalization. Let me show what I got so far. Eigenvector of some linear operator signifies a direction in which operator just ''works'' like a stretching, in other words, operator preserves the direction of its eigenvector. Corresponding eigenvalue is just a value which tells us for how much operator stretches the eigenvector (negative stretches = flipping in the opposite direction). When we limit ourselves to real vector spaces, it's intuitively clear that rotations don't preserve direction of any non-zero vector. Actually, I'm thinking about 2D and 3D spaces as I write, so I talk about ''rotations''... for n-dimensional spaces it would be better to talk about ''operators which act like rotations on some 2D subspace''. But, there are non-diagonalizable matrices that aren't rotations - all non-zero nilpotent matrices. My intuitive view of nilpotent matrices is that they ''gradually collapse all dimensions/gradually lose all the information'' (if we use them over and over again), so it's clear to me why they can't be diagonalizable. But, again, there are non-diagonalizable matrices that aren't rotations nor nilpotent, for an example: $$ \begin{pmatrix}   1 & 1  \\   0 & 1   \end{pmatrix} $$ So, what's the deal with them? Is there any kind of intuitive geometric reasoning that would help me grasp why there are matrices like this one? What's their characteristic that stops them from being diagonalizable?","I'm trying to build an intuitive geometric picture about diagonalization. Let me show what I got so far. Eigenvector of some linear operator signifies a direction in which operator just ''works'' like a stretching, in other words, operator preserves the direction of its eigenvector. Corresponding eigenvalue is just a value which tells us for how much operator stretches the eigenvector (negative stretches = flipping in the opposite direction). When we limit ourselves to real vector spaces, it's intuitively clear that rotations don't preserve direction of any non-zero vector. Actually, I'm thinking about 2D and 3D spaces as I write, so I talk about ''rotations''... for n-dimensional spaces it would be better to talk about ''operators which act like rotations on some 2D subspace''. But, there are non-diagonalizable matrices that aren't rotations - all non-zero nilpotent matrices. My intuitive view of nilpotent matrices is that they ''gradually collapse all dimensions/gradually lose all the information'' (if we use them over and over again), so it's clear to me why they can't be diagonalizable. But, again, there are non-diagonalizable matrices that aren't rotations nor nilpotent, for an example: $$ \begin{pmatrix}   1 & 1  \\   0 & 1   \end{pmatrix} $$ So, what's the deal with them? Is there any kind of intuitive geometric reasoning that would help me grasp why there are matrices like this one? What's their characteristic that stops them from being diagonalizable?",,"['linear-algebra', 'matrices', 'soft-question', 'vector-spaces', 'eigenvalues-eigenvectors']"
49,Must eigenvalues be numbers?,Must eigenvalues be numbers?,,"This is more a conceptual question than any other kind. As far as I know, one can define matrices over arbitrary fields, and so do linear algebra in different settings than in the typical freshman-year course. Now, how does the concept of eigenvalues translate when doing so? Of course, a matrix need not have any eigenvalues in a given field, that I know. But do the eigenvalues need to be numbers? There are examples of fields such as that of the rational functions. If we have a matrix over that field, can we have rational functions as eigenvalues?","This is more a conceptual question than any other kind. As far as I know, one can define matrices over arbitrary fields, and so do linear algebra in different settings than in the typical freshman-year course. Now, how does the concept of eigenvalues translate when doing so? Of course, a matrix need not have any eigenvalues in a given field, that I know. But do the eigenvalues need to be numbers? There are examples of fields such as that of the rational functions. If we have a matrix over that field, can we have rational functions as eigenvalues?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'soft-question', 'eigenvalues-eigenvectors']"
50,Understanding Dot and Cross Product,Understanding Dot and Cross Product,,What purposes do the Dot and Cross products serve? Do you have any clear examples of when you would use them?,What purposes do the Dot and Cross products serve? Do you have any clear examples of when you would use them?,,"['linear-algebra', 'inner-products', 'cross-product']"
51,Simultaneous diagonalization of commuting linear transformations,Simultaneous diagonalization of commuting linear transformations,,"Let $V$ be a vector space of finite dimension and let $T,S$ linear diagonalizable  transformations from $V$ to itself. I need to prove that if $TS=ST$ every eigenspace $V_\lambda$ of $S$ is $T$ -invariant and the restriction of $T$ to $V_\lambda$ ( $T:{V_{\lambda }}\rightarrow V_{\lambda }$ ) is diagonalizable. In addition, I need to show that there's a base $B$ of $V$ such that $[S]_{B}^{B}$ , $[T]_{B}^{B}$ are diagonalizable if and only if $TS=ST$ . Ok, so first let $v\in V_\lambda$ . From $TS=ST$ we get that $\lambda T(v)= S(T(v))$ so $T(v)$ is eigenvector of $S$ and we get what we want. I want to use that in order to get the following claim, I just don't know how. One direction of the ""iff"" is obvious, the other one is more tricky to me.","Let be a vector space of finite dimension and let linear diagonalizable  transformations from to itself. I need to prove that if every eigenspace of is -invariant and the restriction of to ( ) is diagonalizable. In addition, I need to show that there's a base of such that , are diagonalizable if and only if . Ok, so first let . From we get that so is eigenvector of and we get what we want. I want to use that in order to get the following claim, I just don't know how. One direction of the ""iff"" is obvious, the other one is more tricky to me.","V T,S V TS=ST V_\lambda S T T V_\lambda T:{V_{\lambda }}\rightarrow V_{\lambda } B V [S]_{B}^{B} [T]_{B}^{B} TS=ST v\in V_\lambda TS=ST \lambda T(v)= S(T(v)) T(v) S",['linear-algebra']
52,What is the significance of left and right eigenvectors?,What is the significance of left and right eigenvectors?,,"I know that in $A\textbf{x}=\lambda \textbf{x}$ , $\textbf{x}$ is the right eigenvector, while in $\textbf{y}A =\lambda \textbf{y}$ , $\textbf{y}$ is the left eigenvector. But what is the significance of left and right eigenvectors? How do they differ from each other geometrically?","I know that in , is the right eigenvector, while in , is the left eigenvector. But what is the significance of left and right eigenvectors? How do they differ from each other geometrically?",A\textbf{x}=\lambda \textbf{x} \textbf{x} \textbf{y}A =\lambda \textbf{y} \textbf{y},"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
53,Determine if vectors are linearly independent,Determine if vectors are linearly independent,,"Determine if the following set of vectors is linearly independent: $$\left[\begin{array}{r}2\\2\\0\end{array}\right],\left[\begin{array}{r}1\\-1\\1\end{array}\right],\left[\begin{array}{r}4\\2\\-2\end{array}\right]$$ I've done the following system of equations, and I think I did it right... It's been such a long time since I did this sort of thing... Assume the following: \begin{equation*} a\left[\begin{array}{r}2\\2\\0\end{array}\right]+b\left[\begin{array}{r}1\\-1\\1\end{array}\right]+c\left[\begin{array}{r}4\\2\\-2\end{array}\right]=\left[\begin{array}{r}0\\0\\0\end{array}\right] \end{equation*} Determine if $a=b=c=0$: \begin{align} 2a+b+4c&=0&&(1)\\ 2a-b+2c&=0&&(2)\\ b-2c&=0&&(3) \end{align} Subtract $(2)$ from $(1)$: \begin{align} b+c&=0&&(4)\\ b-2c&=0&&(5) \end{align} Substitute $(5)$ into $(4)$, we get $c=0$. So now what do I do with this fact? I'm tempted to say that only $c=0$, and $a$ and $b$ can be something else, but I don't trust that my intuition is right.","Determine if the following set of vectors is linearly independent: $$\left[\begin{array}{r}2\\2\\0\end{array}\right],\left[\begin{array}{r}1\\-1\\1\end{array}\right],\left[\begin{array}{r}4\\2\\-2\end{array}\right]$$ I've done the following system of equations, and I think I did it right... It's been such a long time since I did this sort of thing... Assume the following: \begin{equation*} a\left[\begin{array}{r}2\\2\\0\end{array}\right]+b\left[\begin{array}{r}1\\-1\\1\end{array}\right]+c\left[\begin{array}{r}4\\2\\-2\end{array}\right]=\left[\begin{array}{r}0\\0\\0\end{array}\right] \end{equation*} Determine if $a=b=c=0$: \begin{align} 2a+b+4c&=0&&(1)\\ 2a-b+2c&=0&&(2)\\ b-2c&=0&&(3) \end{align} Subtract $(2)$ from $(1)$: \begin{align} b+c&=0&&(4)\\ b-2c&=0&&(5) \end{align} Substitute $(5)$ into $(4)$, we get $c=0$. So now what do I do with this fact? I'm tempted to say that only $c=0$, and $a$ and $b$ can be something else, but I don't trust that my intuition is right.",,"['linear-algebra', 'vectors']"
54,What is a basis for the vector space of continuous functions?,What is a basis for the vector space of continuous functions?,,"A natural vector space is the set of continuous functions on $\mathbb{R}$. Is there a nice basis for this vector space? Or is this one of those situations where we're guaranteed a basis by invoking the Axiom of Choice, but are left rather unsatisfied?","A natural vector space is the set of continuous functions on $\mathbb{R}$. Is there a nice basis for this vector space? Or is this one of those situations where we're guaranteed a basis by invoking the Axiom of Choice, but are left rather unsatisfied?",,"['linear-algebra', 'functions', 'axiom-of-choice', 'hamel-basis']"
55,Proving the trace of a transformation is independent of the basis chosen,Proving the trace of a transformation is independent of the basis chosen,,How would you prove the trace of a transformation from V to V (where V is finite dimensional) is independent of the basis chosen?,How would you prove the trace of a transformation from V to V (where V is finite dimensional) is independent of the basis chosen?,,['linear-algebra']
56,Intuitive proof of multivariable changing of variables formula (Jacobian) without using mapping and/or measure theory?,Intuitive proof of multivariable changing of variables formula (Jacobian) without using mapping and/or measure theory?,,"What is an intuitive proof of the multivariable changing of variables formula (Jacobian) without using mapping and/or measure theory? I think that textbooks overcomplicate the proof. If possible, use linear algebra and calculus to solve it, since that would be the simplest for me to understand.","What is an intuitive proof of the multivariable changing of variables formula (Jacobian) without using mapping and/or measure theory? I think that textbooks overcomplicate the proof. If possible, use linear algebra and calculus to solve it, since that would be the simplest for me to understand.",,"['linear-algebra', 'multivariable-calculus', 'intuition', 'change-of-variable']"
57,Can a real symmetric matrix have complex eigenvectors?,Can a real symmetric matrix have complex eigenvectors?,,"A Hermitian matrix always has real eigenvalues and real or complex orthogonal eigenvectors. A real symmetric matrix is a special case of Hermitian matrices, so it too has orthogonal eigenvectors and real eigenvalues, but could it ever have complex eigenvectors? My intuition is that the eigenvectors are always real, but I can't quite nail it down.","A Hermitian matrix always has real eigenvalues and real or complex orthogonal eigenvectors. A real symmetric matrix is a special case of Hermitian matrices, so it too has orthogonal eigenvectors and real eigenvalues, but could it ever have complex eigenvectors? My intuition is that the eigenvectors are always real, but I can't quite nail it down.",,"['linear-algebra', 'matrices', 'complex-numbers', 'eigenvalues-eigenvectors', 'symmetric-matrices']"
58,Finding the basis of a null space,Finding the basis of a null space,,"I am trying to understand why the method used in my linear algebra textbook to find the basis of the null space works. The textbook is 'Elementary Linear Algebra' by Anton. According to the textbook, the basis of the null space for the following matrix: $A=\left(\begin{array}{rrrrrr} 1 & 3 & -2 & 0 & 2 & 0 \\ 2 & 6 & -5 & -2 & 4 & -3 \\ 0 & 0 & 5 & 10 & 0 & 15 \\ 2 & 6 & 0 & 8 & 4 & 18 \end{array}\right) $ is found by first finding the reduced row echelon form, which leads to the following: $(x_1,x_2,x_3,x_4,x_5,x_6)=(-3r-4s-2t,r,-2s,s,t,0)$ or, alternatively as $(x_1,x_2,x_3,x_4,x_5,x_6)=r(-3,1,0,0,0,0)+s(-4,0,-2,1,0,0)+t(-2,0,0,0,1,0)$ This shows that the vectors ${\bf v_1}=(-3,1,0,0,0,0),\hspace{0.5in} {\bf v_2}=(-4,0,-2,1,0,0),\hspace{0.5in} {\bf v_3}=(-2,0,0,0,1,0)$ span the solution space. It can be shown that for a homogenous linear system, this method always produces a basis for the solution space of the system. Question I don't understand why this method will always produce a basis for $Ax=0$. Could someone please explain to me why this method will always work? If it helps to explain, I already understand the process of finding the basis of a column space and row space. I also understand why elementary row operations do not alter the null space of a matrix. What specific properties of matrices or vector space that I need to be aware of in order to understand why this method works?","I am trying to understand why the method used in my linear algebra textbook to find the basis of the null space works. The textbook is 'Elementary Linear Algebra' by Anton. According to the textbook, the basis of the null space for the following matrix: $A=\left(\begin{array}{rrrrrr} 1 & 3 & -2 & 0 & 2 & 0 \\ 2 & 6 & -5 & -2 & 4 & -3 \\ 0 & 0 & 5 & 10 & 0 & 15 \\ 2 & 6 & 0 & 8 & 4 & 18 \end{array}\right) $ is found by first finding the reduced row echelon form, which leads to the following: $(x_1,x_2,x_3,x_4,x_5,x_6)=(-3r-4s-2t,r,-2s,s,t,0)$ or, alternatively as $(x_1,x_2,x_3,x_4,x_5,x_6)=r(-3,1,0,0,0,0)+s(-4,0,-2,1,0,0)+t(-2,0,0,0,1,0)$ This shows that the vectors ${\bf v_1}=(-3,1,0,0,0,0),\hspace{0.5in} {\bf v_2}=(-4,0,-2,1,0,0),\hspace{0.5in} {\bf v_3}=(-2,0,0,0,1,0)$ span the solution space. It can be shown that for a homogenous linear system, this method always produces a basis for the solution space of the system. Question I don't understand why this method will always produce a basis for $Ax=0$. Could someone please explain to me why this method will always work? If it helps to explain, I already understand the process of finding the basis of a column space and row space. I also understand why elementary row operations do not alter the null space of a matrix. What specific properties of matrices or vector space that I need to be aware of in order to understand why this method works?",,['linear-algebra']
59,What does double vertical-line means in linear algebra?,What does double vertical-line means in linear algebra?,,"I have a formula, which I have no idea how to solve, because I don't know that double vertical-line sign: $\|{\rm Ax} \|$? $${\rm x} \ne 0 \in \Bbb R^n, \quad 0 < m \le \frac {\| {\rm Ax} \|} {\| {\rm x} \|} \le M, \quad cond(A) \le \frac M m .$$ What does it mean? How should I solve this?","I have a formula, which I have no idea how to solve, because I don't know that double vertical-line sign: $\|{\rm Ax} \|$? $${\rm x} \ne 0 \in \Bbb R^n, \quad 0 < m \le \frac {\| {\rm Ax} \|} {\| {\rm x} \|} \le M, \quad cond(A) \le \frac M m .$$ What does it mean? How should I solve this?",,['linear-algebra']
60,The transpose of a permutation matrix is its inverse.,The transpose of a permutation matrix is its inverse.,,"This is a question from the free Harvard online abstract algebra lectures .  I'm posting my solutions here to get some feedback on them.  For a fuller explanation, see this post. This problem is from assignment 4. Prove that the transpose of a permutation matrix $P$ is its inverse. A permutation matrix $P$ has a single 1 in each row and a single 1 in each column, all other entries being 0.  So column $j$ has a single 1 at position $e_{i_jj}$.  $P$ acts by moving row $j$ to row $i_j$ for each column $j$.  Taking the transpose of $P$ moves each 1 entry from $e_{i_jj}$ to $e_{ji_j}$.  Then $P^t$ acts by moving row $i_j$ to row $j$ for each row $i_j$.  Since this is the inverse operation, $P^t=P^{-1}$. Again, I welcome any critique of my reasoning and/or my style as well as alternative solutions to the problem. Thanks.","This is a question from the free Harvard online abstract algebra lectures .  I'm posting my solutions here to get some feedback on them.  For a fuller explanation, see this post. This problem is from assignment 4. Prove that the transpose of a permutation matrix $P$ is its inverse. A permutation matrix $P$ has a single 1 in each row and a single 1 in each column, all other entries being 0.  So column $j$ has a single 1 at position $e_{i_jj}$.  $P$ acts by moving row $j$ to row $i_j$ for each column $j$.  Taking the transpose of $P$ moves each 1 entry from $e_{i_jj}$ to $e_{ji_j}$.  Then $P^t$ acts by moving row $i_j$ to row $j$ for each row $i_j$.  Since this is the inverse operation, $P^t=P^{-1}$. Again, I welcome any critique of my reasoning and/or my style as well as alternative solutions to the problem. Thanks.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'permutations', 'permutation-matrices']"
61,When are minimal and characteristic polynomials the same?,When are minimal and characteristic polynomials the same?,,"Assume that we are working over a complex space $W$ of dimension $n$. When would an operator on this space have the same characteristic and minimal polynomial? I think the easy case is when the operator has $n$ distinct eigenvalues, but what about if it is diagonalizable? Is that sufficient, or can there be cases (with repeated eigvals) when char poly doesn't equal min poly? What are the general conditions when the equality holds? Is it possible to define them without use of determinant? (I am working by Axler and he doesn't like it.) Thanks.","Assume that we are working over a complex space $W$ of dimension $n$. When would an operator on this space have the same characteristic and minimal polynomial? I think the easy case is when the operator has $n$ distinct eigenvalues, but what about if it is diagonalizable? Is that sufficient, or can there be cases (with repeated eigvals) when char poly doesn't equal min poly? What are the general conditions when the equality holds? Is it possible to define them without use of determinant? (I am working by Axler and he doesn't like it.) Thanks.",,"['linear-algebra', 'minimal-polynomials', 'characteristic-polynomial']"
62,Why isn't lambda notation popular among mathematicians?,Why isn't lambda notation popular among mathematicians?,,"I am relatively new to the world of academical mathematics, but I have noticed that most, if not all, mathematical textbooks that I've had the chance to come across, seem completely oblivious to the existence of lambda notation. More specifically, in a linear algebra course I'm taking, I found it a lot easier to understand ""higher order functionals"" from the second dual space, by putting them in lambda expressions. It makes a lot more sense to me to put them in the neat, clear notation of lambda expressions, rather than in multiple variable functions where not all the arguments are of the same ""class"" as some are linear functionals and others are vectors. For example, consider the canonical isomorphism -  $$A:V \rightarrow V^{**}$$ It would usually be expressed by $$Av(f) = f(v)$$ This was a notation I found particularly difficult to understand at first as there are several processes taking place ""under the hood"", that can be put a lot more clearly, in my opinion, this way: $$A = \lambda v \in V. \lambda f \in V^{*}. f(v)$$ I agree that this notation may become tedious and over-explanatory over time, but as a first introduction of the concept I find it a lot easier as it makes it very clear what goes where. My question is, basically, why isn't this widespread, super popular notation in the world of computer science, not as popular in the field of mathematics? Or is it, and I'm just not aware?","I am relatively new to the world of academical mathematics, but I have noticed that most, if not all, mathematical textbooks that I've had the chance to come across, seem completely oblivious to the existence of lambda notation. More specifically, in a linear algebra course I'm taking, I found it a lot easier to understand ""higher order functionals"" from the second dual space, by putting them in lambda expressions. It makes a lot more sense to me to put them in the neat, clear notation of lambda expressions, rather than in multiple variable functions where not all the arguments are of the same ""class"" as some are linear functionals and others are vectors. For example, consider the canonical isomorphism -  $$A:V \rightarrow V^{**}$$ It would usually be expressed by $$Av(f) = f(v)$$ This was a notation I found particularly difficult to understand at first as there are several processes taking place ""under the hood"", that can be put a lot more clearly, in my opinion, this way: $$A = \lambda v \in V. \lambda f \in V^{*}. f(v)$$ I agree that this notation may become tedious and over-explanatory over time, but as a first introduction of the concept I find it a lot easier as it makes it very clear what goes where. My question is, basically, why isn't this widespread, super popular notation in the world of computer science, not as popular in the field of mathematics? Or is it, and I'm just not aware?",,"['linear-algebra', 'notation', 'lambda-calculus', 'dual-spaces']"
63,Eigenvalues and power of a matrix,Eigenvalues and power of a matrix,,"Let $A$ be an n×n matrix with eigenvalues $\lambda_i, i=1,2,\dots,n$. Then $\lambda_1^k,\dots,\lambda_n^k$ are eigenvalues of $A^k$. I was wondering if $\lambda_1^k,\dots,\lambda_n^k$ are all the eigenvalues of $A^k$? Are the algebraic and geometric multiplicities of $\lambda_i^k$ for $A^k$ same as those of $\lambda_i$ for $A$ respectively? Thanks!","Let $A$ be an n×n matrix with eigenvalues $\lambda_i, i=1,2,\dots,n$. Then $\lambda_1^k,\dots,\lambda_n^k$ are eigenvalues of $A^k$. I was wondering if $\lambda_1^k,\dots,\lambda_n^k$ are all the eigenvalues of $A^k$? Are the algebraic and geometric multiplicities of $\lambda_i^k$ for $A^k$ same as those of $\lambda_i$ for $A$ respectively? Thanks!",,"['linear-algebra', 'matrices']"
64,block matrix multiplication,block matrix multiplication,,"If $A,B$ are $2 \times 2$ matrices of real or complex numbers, then $$AB = \left[ \begin{array}{cc} a_{11} & a_{12} \\ a_{21} & a_{22} \end{array} \right]\cdot \left[ \begin{array}{cc} b_{11} & b_{12} \\ b_{21} & b_{22} \end{array} \right] =  \left[ \begin{array}{cc} a_{11}b_{11}+a_{12}b_{21} & a_{11}b_{12}+a_{12}b_{22} \\ a_{21}b_{11}+a_{22}b_{21} & a_{22}b_{12}+a_{22}b_{22} \end{array} \right] $$ What if the entries $a_{ij}, b_{ij}$ are themselves $2 \times 2$ matrices?  Does matrix multiplication hold in some sort of ""block"" form ? $$AB = \left[ \begin{array}{c|c} A_{11} & A_{12} \\\hline A_{21} & A_{22} \end{array} \right]\cdot \left[ \begin{array}{c|c} B_{11} & B_{12} \\\hline B_{21} & B_{22} \end{array} \right] =  \left[ \begin{array}{c|c} A_{11}B_{11}+A_{12}B_{21} & A_{11}B_{12}+A_{12}B_{22} \\\hline A_{21}B_{11}+A_{22}B_{21} & A_{22}B_{12}+A_{22}B_{22} \end{array} \right] $$ This identity would be very useful in my research.","If $A,B$ are $2 \times 2$ matrices of real or complex numbers, then $$AB = \left[ \begin{array}{cc} a_{11} & a_{12} \\ a_{21} & a_{22} \end{array} \right]\cdot \left[ \begin{array}{cc} b_{11} & b_{12} \\ b_{21} & b_{22} \end{array} \right] =  \left[ \begin{array}{cc} a_{11}b_{11}+a_{12}b_{21} & a_{11}b_{12}+a_{12}b_{22} \\ a_{21}b_{11}+a_{22}b_{21} & a_{22}b_{12}+a_{22}b_{22} \end{array} \right] $$ What if the entries $a_{ij}, b_{ij}$ are themselves $2 \times 2$ matrices?  Does matrix multiplication hold in some sort of ""block"" form ? $$AB = \left[ \begin{array}{c|c} A_{11} & A_{12} \\\hline A_{21} & A_{22} \end{array} \right]\cdot \left[ \begin{array}{c|c} B_{11} & B_{12} \\\hline B_{21} & B_{22} \end{array} \right] =  \left[ \begin{array}{c|c} A_{11}B_{11}+A_{12}B_{21} & A_{11}B_{12}+A_{12}B_{22} \\\hline A_{21}B_{11}+A_{22}B_{21} & A_{22}B_{12}+A_{22}B_{22} \end{array} \right] $$ This identity would be very useful in my research.",,"['linear-algebra', 'matrices', 'numerical-linear-algebra']"
65,Row vector vs. Column vector,Row vector vs. Column vector,,"I'm a student in an elementary linear algebra course. Without bashing on my professor, I must say that s/he is very poor at answering questions, often not addressing the question itself. Throughout the course, there have been multiple questions that have gone unanswered, but I must have the answer to this one question. ""Why are some vectors written as row vectors and others as column vectors?"" I understand that if I transpose one, it becomes the other. However, I'd like to understand the purpose behind writing a vector in a certain format. Taking examples from my lectures, I see that when I'm trying to prove linear independence of a group of vectors, the vectors are written as column vectors in a matrix, and the row reduced form is found. Other times, like trying to find the cross product or just solving a matrix, I see the vectors written as row vectors. My professor is very vague on the notations and explanations, and it bugs me as a person who needs to know the reason behind every small thing, why this variation occurs in the format. Any input is greatly appreciated.","I'm a student in an elementary linear algebra course. Without bashing on my professor, I must say that s/he is very poor at answering questions, often not addressing the question itself. Throughout the course, there have been multiple questions that have gone unanswered, but I must have the answer to this one question. ""Why are some vectors written as row vectors and others as column vectors?"" I understand that if I transpose one, it becomes the other. However, I'd like to understand the purpose behind writing a vector in a certain format. Taking examples from my lectures, I see that when I'm trying to prove linear independence of a group of vectors, the vectors are written as column vectors in a matrix, and the row reduced form is found. Other times, like trying to find the cross product or just solving a matrix, I see the vectors written as row vectors. My professor is very vague on the notations and explanations, and it bugs me as a person who needs to know the reason behind every small thing, why this variation occurs in the format. Any input is greatly appreciated.",,"['linear-algebra', 'vectors']"
66,Coordinate-free proof of $\operatorname{Tr}(AB)=\operatorname{Tr}(BA)$?,Coordinate-free proof of ?,\operatorname{Tr}(AB)=\operatorname{Tr}(BA),"I am searching for a short coordinate-free proof of $\operatorname{Tr}(AB)=\operatorname{Tr}(BA)$ for linear operators $A$ , $B$ between finite dimensional vector spaces of the same dimension. The usual proof is to represent the operators as matrices and then use matrix multiplication. I want a coordinate-free proof. That is, one that does not  make reference to an explicit matrix representation of the operator. I define trace as the sum of the eigenvalues of an operator. Ideally, the proof the should be shorter and require fewer preliminary lemmas than the one given in this blog post . I would be especially interested in a proof that generalizes to the trace class of operators on a Hilbert space.","I am searching for a short coordinate-free proof of for linear operators , between finite dimensional vector spaces of the same dimension. The usual proof is to represent the operators as matrices and then use matrix multiplication. I want a coordinate-free proof. That is, one that does not  make reference to an explicit matrix representation of the operator. I define trace as the sum of the eigenvalues of an operator. Ideally, the proof the should be shorter and require fewer preliminary lemmas than the one given in this blog post . I would be especially interested in a proof that generalizes to the trace class of operators on a Hilbert space.",\operatorname{Tr}(AB)=\operatorname{Tr}(BA) A B,"['linear-algebra', 'matrices', 'trace']"
67,What exactly is a matrix?,What exactly is a matrix?,,"I know how basic operations are performed on matrices, I can do transformations, find inverses, etc. But now that I think about it, I actually don't ""understand"" or know what I've been doing all this time. Our teacher made us memorise some rules and I've been following it like a machine. So what exactly is a matrix? And what is a determinant? What do they represent? Is there a geometrical interpretation? How are they used? Or, rather, what for are they used? How do I understand the ""properties"" of matrix? I just don't wanna mindlessly cram all those properties, I want to understand them better. Any links, which would improve my understanding towards determinants and matrices? Please use simpler words. Thanks :)","I know how basic operations are performed on matrices, I can do transformations, find inverses, etc. But now that I think about it, I actually don't ""understand"" or know what I've been doing all this time. Our teacher made us memorise some rules and I've been following it like a machine. So what exactly is a matrix? And what is a determinant? What do they represent? Is there a geometrical interpretation? How are they used? Or, rather, what for are they used? How do I understand the ""properties"" of matrix? I just don't wanna mindlessly cram all those properties, I want to understand them better. Any links, which would improve my understanding towards determinants and matrices? Please use simpler words. Thanks :)",,"['linear-algebra', 'matrices', 'soft-question']"
68,Difference between sum and direct sum,Difference between sum and direct sum,,What is the difference between sum of two vectors and direct sum of two vector subspaces? My textbook is confusing about it. Any help would be appreciated.,What is the difference between sum of two vectors and direct sum of two vector subspaces? My textbook is confusing about it. Any help would be appreciated.,,"['linear-algebra', 'vector-spaces', 'vectors', 'direct-sum']"
69,What does 'linear' mean in Linear Algebra?,What does 'linear' mean in Linear Algebra?,,"Why Linear Algebra named in that way? Especially, why we call it linear ? What does it mean?","Why Linear Algebra named in that way? Especially, why we call it linear ? What does it mean?",,"['linear-algebra', 'terminology']"
70,"Is there a list of all typos in Hoffman and Kunze, Linear Algebra?","Is there a list of all typos in Hoffman and Kunze, Linear Algebra?",,"Where can I find a list of typos for Linear Algebra , 2nd Edition, by Hoffman and Kunze? I searched on Google, but to no avail.","Where can I find a list of typos for Linear Algebra , 2nd Edition, by Hoffman and Kunze? I searched on Google, but to no avail.",,"['linear-algebra', 'reference-request']"
71,Why is the complex number $z=a+bi$ equivalent to the matrix form $\left(\begin{smallmatrix}a &-b\\b&a\end{smallmatrix}\right)$ [duplicate],Why is the complex number  equivalent to the matrix form  [duplicate],z=a+bi \left(\begin{smallmatrix}a &-b\\b&a\end{smallmatrix}\right),"This question already has answers here : Closed 11 years ago . Possible Duplicate: Relation of this antisymmetric matrix $r = \!\left(\begin{smallmatrix}0 &1\\-1 & 0\end{smallmatrix}\right)$ to $i$ On Wikipedia, it says that: Matrix representation of complex numbers Complex numbers $z=a+ib$ can also be represented by $2\times2$ matrices that have the following form: $$\pmatrix{a&-b\\b&a}$$ I don't understand why they can be represented by these matrices or where these matrices come from.","This question already has answers here : Closed 11 years ago . Possible Duplicate: Relation of this antisymmetric matrix to On Wikipedia, it says that: Matrix representation of complex numbers Complex numbers can also be represented by matrices that have the following form: I don't understand why they can be represented by these matrices or where these matrices come from.",r = \!\left(\begin{smallmatrix}0 &1\\-1 & 0\end{smallmatrix}\right) i z=a+ib 2\times2 \pmatrix{a&-b\\b&a},"['linear-algebra', 'matrices', 'complex-numbers', 'quaternions']"
72,Diagonalizable transformation restricted to an invariant subspace is diagonalizable,Diagonalizable transformation restricted to an invariant subspace is diagonalizable,,"Suppose $V$ is a vector space over $\mathbb{C}$ , and $A$ is a linear transformation on $V$ which is diagonalizable. I.e. there is a basis of $V$ consisting of eigenvectors of $A$ . If $W\subseteq V$ is an invariant subspace of $A$ (so $A(W)\subseteq W$ ), show that $A|_W$ is also diagonalizable. I tried supposing $A$ has distinct eigenvalues $\lambda_1,\ldots,\lambda_m$ , with $V_i=\{v\in V: Av=\lambda_i v\}$ . Then we can write $V=V_1\oplus\cdots\oplus V_m,$ but I'm not sure whether it is true that $$W=(W\cap V_1)\oplus\cdots\oplus (W\cap V_m),.$$ If it is true, then we're done, but it may be wrong.","Suppose is a vector space over , and is a linear transformation on which is diagonalizable. I.e. there is a basis of consisting of eigenvectors of . If is an invariant subspace of (so ), show that is also diagonalizable. I tried supposing has distinct eigenvalues , with . Then we can write but I'm not sure whether it is true that If it is true, then we're done, but it may be wrong.","V \mathbb{C} A V V A W\subseteq V A A(W)\subseteq W A|_W A \lambda_1,\ldots,\lambda_m V_i=\{v\in V: Av=\lambda_i v\} V=V_1\oplus\cdots\oplus V_m, W=(W\cap V_1)\oplus\cdots\oplus (W\cap V_m),.","['linear-algebra', 'eigenvalues-eigenvectors', 'diagonalization']"
73,Reflection across a line?,Reflection across a line?,,The linear transformation matrix for a reflection across the line $y = mx$ is: $$\frac{1}{1 + m^2}\begin{pmatrix}1-m^2&2m\\2m&m^2-1\end{pmatrix} $$ My professor gave us the formula above with no explanation why it works. I am completely new to linear algebra so I have absolutely no idea how to go about deriving the formula.  Could someone explain to me how the formula is derived ? Thanks,The linear transformation matrix for a reflection across the line is: My professor gave us the formula above with no explanation why it works. I am completely new to linear algebra so I have absolutely no idea how to go about deriving the formula.  Could someone explain to me how the formula is derived ? Thanks,y = mx \frac{1}{1 + m^2}\begin{pmatrix}1-m^2&2m\\2m&m^2-1\end{pmatrix} ,"['linear-algebra', 'matrices', 'vector-spaces', 'linear-transformations']"
74,Affine transformation applied to a multivariate Gaussian random variable - what is the mean vector and covariance matrix of the new variable?,Affine transformation applied to a multivariate Gaussian random variable - what is the mean vector and covariance matrix of the new variable?,,"Given a random vector $\mathbf x \sim N(\mathbf{\bar x}, \mathbf{C_x})$ with normal distribution. $\mathbf{\bar x}$ is the mean value vector and $\mathbf{C_x}$ is the covariance matrix of $\mathbf{x}$. An affine transformation is applied to the $\mathbf{x}$ vector to create a new random $\mathbf{y}$ vector: $$ \mathbf{y} = \mathbf{Ax} + \mathbf{b} $$ Can we find mean value $\mathbf{\bar y}$ and covariance matrix $\mathbf{C_y}$ of this new vector $\mathbf{y}$ in terms of already given parameters ($\mathbf{\bar x}$, $\mathbf{C_x}$, $\mathbf{A}$ and $\mathbf{b}$)? Can you please show the steps. Once I learn the method, I will use it on several other distributions myself.","Given a random vector $\mathbf x \sim N(\mathbf{\bar x}, \mathbf{C_x})$ with normal distribution. $\mathbf{\bar x}$ is the mean value vector and $\mathbf{C_x}$ is the covariance matrix of $\mathbf{x}$. An affine transformation is applied to the $\mathbf{x}$ vector to create a new random $\mathbf{y}$ vector: $$ \mathbf{y} = \mathbf{Ax} + \mathbf{b} $$ Can we find mean value $\mathbf{\bar y}$ and covariance matrix $\mathbf{C_y}$ of this new vector $\mathbf{y}$ in terms of already given parameters ($\mathbf{\bar x}$, $\mathbf{C_x}$, $\mathbf{A}$ and $\mathbf{b}$)? Can you please show the steps. Once I learn the method, I will use it on several other distributions myself.",,"['linear-algebra', 'multivariable-calculus', 'normal-distribution', 'random-variables']"
75,How can I prove that 3 planes are arranged in a triangle-like shape without calculating their intersection lines?,How can I prove that 3 planes are arranged in a triangle-like shape without calculating their intersection lines?,,"The problem So recently in school, we should do a task somewhat like this (roughly translated): Assign a system of linear equations to each drawing Then, there were some systems of three linear equations (SLEs) where each equation was describing a plane in their coordinate form and some sketches of three planes in some relation (e.g. parallel or intersecting at 90°-angles. My question For some reason, I immediately knew that these planes: belonged to this SLE: $$ x_1 -3x_2 +2x_3 = -2 $$ $$ x_1 +3x_2 -2x_3 = 5 $$ $$-6x_2 + 4x_3 = 3$$ And it turned out to be true. In school, we proved this by determining the planes' intersecting lines and showing that they are parallel, but not identical. However, I believe that it must be possible to show the planes are arranged like this without a lot of calculation. Since I immediately saw/""felt"" that the planes described in the SLE must be arranged in the way they are in the picture (like a triangle). I could also determine the same ""shape"" on a similar question, so I do not believe that it was just coincidence. What needs to be shown? So we must show that the three planes described by the SLE cut each other in a way that I do not really know how to describe. They do not intersect with each other perpendicular (at least they don' have to to be arranged in a triangle), but there is no point in which all three planes intersect. If you were to put a line in the center of the triangle, it would be parallel to all planes. The three planes do not share one intersecting line as it would be in this case: (which was another drawing from the task, but is not relevant to this question except for that it has to be excluded) My thoughts If you were to look at the planes exactly from the direction in which the parallel line from the previous section leads, you would see something like this: The red arrows represent the normal of each plane (they should be perpendicular). You can see that the normals somehow are part of one (new) plane. This is already given by the manner how the planes intersect with each other (as I described before). If you now were to align your coordinate system in such a way that the plane in which the normals lie is the $x_1 x_2$ -plane, each normals would have an $x_3$ value of $0$ . If you were now to further align the coordinate axes so that the $x_1$ -axis is identical to one of the normals (let's just choose the bottom one), the values of the normals would be somehow like this: $n_1=\begin{pmatrix} a \\ 0 \\ 0  \end{pmatrix}$ for the bottom normal $n_2=\begin{pmatrix} a \\ a \\ 0  \end{pmatrix}$ for the upper right normal and $n_3=\begin{pmatrix} a \\ -a \\ 0  \end{pmatrix}$ for the upper left normal Of course, the planes do not have to be arranged in a way that the vectors line up so nicely that they are in one of the planes of our coordinate system. However, in the SLE, I noticed the following: -The three normals (we can simpla read the coefficients since the equations are in coordinate form) are $n_1=\begin{pmatrix} 1 \\ -3 \\ 2  \end{pmatrix}$ , $n_2=\begin{pmatrix} 1 \\ 3 \\ -2  \end{pmatrix}$ and $n_3=\begin{pmatrix} 0 \\ -6 \\ 4  \end{pmatrix}$ . As we can see, $n_1$ and $n_2$ have the same values for $x_1$ and that $x_2(n_1)=-x_2(n_2)$ ; $x_3(n_1)=-x_3(n_2)$ Also, $n_3$ is somewhat similar in that its $x_2$ and $x_3$ values are the same as the $x_2$ and $x_3$ values of $n_1$ , but multiplied by the factor $2$ . I also noticed that $n_3$ has no $x_1$ value (or, more accurately, the value is $0$ ), while for $n_1$ and $n_2$ , the value for $x_1$ is identical ( $n_1=1$ ). Conclusion I feel like I am very close to a solution, I just don't know what to do with my thoughts/approaches regarding the normals of the planes. Any help would be greatly appreciated. How can I show that the three planes are arranged in this triangular-like shape by using their normals, i.e. without having to calculate the planes' intersection lines? (Probably we will need more than normals, but I believe that they are the starting point). Update: I posted a new question that is related to this problem, but is (at least in my opinion) not the same question.","The problem So recently in school, we should do a task somewhat like this (roughly translated): Assign a system of linear equations to each drawing Then, there were some systems of three linear equations (SLEs) where each equation was describing a plane in their coordinate form and some sketches of three planes in some relation (e.g. parallel or intersecting at 90°-angles. My question For some reason, I immediately knew that these planes: belonged to this SLE: And it turned out to be true. In school, we proved this by determining the planes' intersecting lines and showing that they are parallel, but not identical. However, I believe that it must be possible to show the planes are arranged like this without a lot of calculation. Since I immediately saw/""felt"" that the planes described in the SLE must be arranged in the way they are in the picture (like a triangle). I could also determine the same ""shape"" on a similar question, so I do not believe that it was just coincidence. What needs to be shown? So we must show that the three planes described by the SLE cut each other in a way that I do not really know how to describe. They do not intersect with each other perpendicular (at least they don' have to to be arranged in a triangle), but there is no point in which all three planes intersect. If you were to put a line in the center of the triangle, it would be parallel to all planes. The three planes do not share one intersecting line as it would be in this case: (which was another drawing from the task, but is not relevant to this question except for that it has to be excluded) My thoughts If you were to look at the planes exactly from the direction in which the parallel line from the previous section leads, you would see something like this: The red arrows represent the normal of each plane (they should be perpendicular). You can see that the normals somehow are part of one (new) plane. This is already given by the manner how the planes intersect with each other (as I described before). If you now were to align your coordinate system in such a way that the plane in which the normals lie is the -plane, each normals would have an value of . If you were now to further align the coordinate axes so that the -axis is identical to one of the normals (let's just choose the bottom one), the values of the normals would be somehow like this: for the bottom normal for the upper right normal and for the upper left normal Of course, the planes do not have to be arranged in a way that the vectors line up so nicely that they are in one of the planes of our coordinate system. However, in the SLE, I noticed the following: -The three normals (we can simpla read the coefficients since the equations are in coordinate form) are , and . As we can see, and have the same values for and that ; Also, is somewhat similar in that its and values are the same as the and values of , but multiplied by the factor . I also noticed that has no value (or, more accurately, the value is ), while for and , the value for is identical ( ). Conclusion I feel like I am very close to a solution, I just don't know what to do with my thoughts/approaches regarding the normals of the planes. Any help would be greatly appreciated. How can I show that the three planes are arranged in this triangular-like shape by using their normals, i.e. without having to calculate the planes' intersection lines? (Probably we will need more than normals, but I believe that they are the starting point). Update: I posted a new question that is related to this problem, but is (at least in my opinion) not the same question."," x_1 -3x_2 +2x_3 = -2   x_1 +3x_2 -2x_3 = 5  -6x_2 + 4x_3 = 3 x_1 x_2 x_3 0 x_1 n_1=\begin{pmatrix}
a \\
0 \\
0 
\end{pmatrix} n_2=\begin{pmatrix}
a \\
a \\
0 
\end{pmatrix} n_3=\begin{pmatrix}
a \\
-a \\
0 
\end{pmatrix} n_1=\begin{pmatrix}
1 \\
-3 \\
2 
\end{pmatrix} n_2=\begin{pmatrix}
1 \\
3 \\
-2 
\end{pmatrix} n_3=\begin{pmatrix}
0 \\
-6 \\
4 
\end{pmatrix} n_1 n_2 x_1 x_2(n_1)=-x_2(n_2) x_3(n_1)=-x_3(n_2) n_3 x_2 x_3 x_2 x_3 n_1 2 n_3 x_1 0 n_1 n_2 x_1 n_1=1","['linear-algebra', 'geometry', 'vectors', 'systems-of-equations']"
76,Natural and coordinate free definition for the Riemannian volume form?,Natural and coordinate free definition for the Riemannian volume form?,,"In linear algebra and differential geometry, there are various structures which we calculate with in a basis or local coordinates, but which we would like to have a meaning which is basis independent or coordinate independent, or at least, changes in some covariant way under changes of basis or coordinates. One way to ensure that our structures adhere to this principle is to give their definitions without reference to a basis. Often we employ universal properties, functors, and natural transformations to encode these natural, coordinate/basis free structures. But the Riemannian volume form does not appear to admit such a description, nor does its pointwise analogue in linear algebra. Let me list several examples. In linear algebra, an inner product on $V$ is an element of $\operatorname{Sym}^2{V^*}$. The symmetric power is a space which may be defined by a universal property, and constructed via a quotient of a tensor product. No choice of basis necessary. Alternatively an inner product can be given by an $n\times n$ symmetric matrix. The correspondence between the two alternatives is given by $g_{ij}=g(e_i,e_j)$. Calculations are easy with this formulation, but one should check (or require) that the matrix transforms appropriately under changes of basis. In linear algebra, a volume form is an element of $\Lambda^n(V^*)$. Alternatively one may define a volume form operator as the determinant of the matrix of the components of $n$ vectors, relative to some basis. In linear algebra, an orientation is an element of $\Lambda^n(V^*)/\mathbb{R}^>$. In linear algebra, a symplectic form is an element of $\Lambda^2(V^*)$. Alternatively may be given as some $\omega_{ij}\,dx^i\wedge dx^j$. In linear algebra, given a symplectic form, a canonical volume form may be chosen as $\operatorname{vol}=\omega^n$. This operation can be described as a natural transformation $\Lambda^2\to\Lambda^n$. That is, to each vector space $V$, we have a map $\Lambda^2(V)\to\Lambda^n(V)$ taking $\omega\mapsto \omega^n$ and this map commutes with linear maps between spaces. In differential geometry, all the above linear algebra concepts may be specified pointwise. Any smooth functor of vector spaces may be applied to the tangent bundle to give a smooth vector bundle. Thus a Riemannian metric is a section of the bundle $\operatorname{Sym}^2{T^*M}$, etc. A symplectic form is a section of the bundle $\Lambda^2(M)$, and the wedge product extends to an operation on sections, and gives a symplectic manifold a volume form. This is a global operation; this definition of a Riemannian metric gives a smoothly varying inner product on every tangent space of the manifold, even if the manifold is not covered by a single coordinate patch In differential geometry, sometimes vectors are defined as $n$-tuples which transform as $v^i\to \tilde{v}^j\frac{\partial x^i}{\partial \tilde{x}^j}$ under a change of coordinates $x \to \tilde{x}$. But a more invariant definition is to say a vector is a derivation of the algebra of smooth functions. Cotangent vectors can be defined with a slightly different transformation rule, or else invariantly as the dual space to the tangent vectors. Similar remarks hold for higher rank tensors. In differential geometry, one defines a connection on a bundle. The local coordinates definition makes it appear to be a tensor, but it does not behave the transformation rules set forth above. It's only clear why when one sees the invariant definition. In differential geometry, there is a derivation on the exterior algebra called the exterior derivative. It may be defined as $d\sigma = \partial_j\sigma_I\,dx^j\wedge dx^I$ in local coordinates, or better via an invariant formula $d\sigma(v_1,\dotsc,v_n) = \sum_i(-1)^iv_i(\sigma(v_1,\dotsc,\hat{v_i},\dotsc,v_n)) + \sum_{i+j}(-1)^{i+j}\sigma([v_i,v_j],v_1,\dotsc,\hat{v_i},\dotsc,\hat{v_j},\dotsc,v_n)$ Finally, the volume form on an oriented inner product space (or volume density on an inner product space) in linear algebra, and its counterpart the Riemannian volume form on an oriented Riemannian manifold (or volume density form on a Riemannian manifold) in differential geometry. Unlike the above examples which all admit global basis-free/coordinate-free definitions, we can define it only in a single coordinate patch or basis at a time, and glue together to obtain a globally defined structure. There are two definitions seen in the literature: choose an (oriented) coordinate neighborhood of a point, so we have a basis for each tangent space. Write the metric tensor in terms of that basis. Pretend that the bilinear form is actually a linear transformation (this can always be done because once a basis is chosen, we have an isomorphism to $\mathbb{R}^n$ which is isomorphic to its dual (via a different isomorphism than that provided by the inner product)). Then take the determinant of resulting mutated matrix, take the square root, multiply by the wedge of the basis one-forms (the positive root may be chosen in the oriented case; in the unoriented case, take the absolute value to obtain a density). Choose an oriented orthonormal coframe in a neighborhood. Wedge it together. (Finally take the absolute value in the unoriented case). Does anyone else think that one of these definitions sticks out like a sore thumb? Does it bother anyone else that in linear algebra, the volume form on an oriented inner product space doesn't exist as natural transformation $\operatorname{Sym}^2 \to \Lambda^n$? Do the instructions to ""take the determinant of a bilinear form"" scream out to anyone else that we're doing it wrong? Does it bother anyone else that in Riemannian geometry, in stark contrast to the superficially similar symplectic case, the volume form cannot be defined using invariant terminology for the whole manifold, but rather requires one to break the manifold into patches, and choose a basis for each? Is there any other structure in linear algebra or differential geometry which suffers from this defect? Answer: I've accepted Willie Wong's answer below, but let me also sum it up, since it's spread across several different places. There is a canonical construction of the Riemannian volume form on an oriented vector space, or pseudoform on a vector space. At the level of level of vector spaces, we may define an inner product on the dual space $V^*$ by $\tilde{g}(\sigma,\tau)=g(u,v)$ where $u,v$ are the dual vectors to $\sigma,\tau$ under the isomorphism between $V,V^*$ induced by $g$ (which is nondegenerate). Then extend $\tilde{g}$ to $\bigotimes^k V^*$ by defining $\hat{g}(a\otimes b\otimes c,\dotsb,x\otimes y\otimes z\dotsb)=\tilde{g}(a,x)\tilde{g}(b,y)\tilde{g}(c,z)\dotsb$. Then the space of alternating forms may be viewed as a subspace of $\bigotimes^k V^*$, and so inherits an inner product as well (note, however that while the alternating map may be defined canonically, there are varying normalization conventions which do not affect the kernel. I.e. $v\wedge w = k! Alt(v\otimes w)$ or $v\wedge w = Alt(v\otimes w)$). Then $\hat{g}(a\wedge b\dotsb,x\wedge y\dotsb)=\det[\tilde{g}(a,x)\dotsc]$ (with perhaps a normalization factor required here, depending on how Alt was defined). Thus $g$ extends to an inner product on $\Lambda^n(V^*)$, which is a 1 dimensional space, so there are only two unit vectors, and if $V$ is oriented, there is a canonical choice of volume form. And in any event, there is a canonical pseudoform.","In linear algebra and differential geometry, there are various structures which we calculate with in a basis or local coordinates, but which we would like to have a meaning which is basis independent or coordinate independent, or at least, changes in some covariant way under changes of basis or coordinates. One way to ensure that our structures adhere to this principle is to give their definitions without reference to a basis. Often we employ universal properties, functors, and natural transformations to encode these natural, coordinate/basis free structures. But the Riemannian volume form does not appear to admit such a description, nor does its pointwise analogue in linear algebra. Let me list several examples. In linear algebra, an inner product on $V$ is an element of $\operatorname{Sym}^2{V^*}$. The symmetric power is a space which may be defined by a universal property, and constructed via a quotient of a tensor product. No choice of basis necessary. Alternatively an inner product can be given by an $n\times n$ symmetric matrix. The correspondence between the two alternatives is given by $g_{ij}=g(e_i,e_j)$. Calculations are easy with this formulation, but one should check (or require) that the matrix transforms appropriately under changes of basis. In linear algebra, a volume form is an element of $\Lambda^n(V^*)$. Alternatively one may define a volume form operator as the determinant of the matrix of the components of $n$ vectors, relative to some basis. In linear algebra, an orientation is an element of $\Lambda^n(V^*)/\mathbb{R}^>$. In linear algebra, a symplectic form is an element of $\Lambda^2(V^*)$. Alternatively may be given as some $\omega_{ij}\,dx^i\wedge dx^j$. In linear algebra, given a symplectic form, a canonical volume form may be chosen as $\operatorname{vol}=\omega^n$. This operation can be described as a natural transformation $\Lambda^2\to\Lambda^n$. That is, to each vector space $V$, we have a map $\Lambda^2(V)\to\Lambda^n(V)$ taking $\omega\mapsto \omega^n$ and this map commutes with linear maps between spaces. In differential geometry, all the above linear algebra concepts may be specified pointwise. Any smooth functor of vector spaces may be applied to the tangent bundle to give a smooth vector bundle. Thus a Riemannian metric is a section of the bundle $\operatorname{Sym}^2{T^*M}$, etc. A symplectic form is a section of the bundle $\Lambda^2(M)$, and the wedge product extends to an operation on sections, and gives a symplectic manifold a volume form. This is a global operation; this definition of a Riemannian metric gives a smoothly varying inner product on every tangent space of the manifold, even if the manifold is not covered by a single coordinate patch In differential geometry, sometimes vectors are defined as $n$-tuples which transform as $v^i\to \tilde{v}^j\frac{\partial x^i}{\partial \tilde{x}^j}$ under a change of coordinates $x \to \tilde{x}$. But a more invariant definition is to say a vector is a derivation of the algebra of smooth functions. Cotangent vectors can be defined with a slightly different transformation rule, or else invariantly as the dual space to the tangent vectors. Similar remarks hold for higher rank tensors. In differential geometry, one defines a connection on a bundle. The local coordinates definition makes it appear to be a tensor, but it does not behave the transformation rules set forth above. It's only clear why when one sees the invariant definition. In differential geometry, there is a derivation on the exterior algebra called the exterior derivative. It may be defined as $d\sigma = \partial_j\sigma_I\,dx^j\wedge dx^I$ in local coordinates, or better via an invariant formula $d\sigma(v_1,\dotsc,v_n) = \sum_i(-1)^iv_i(\sigma(v_1,\dotsc,\hat{v_i},\dotsc,v_n)) + \sum_{i+j}(-1)^{i+j}\sigma([v_i,v_j],v_1,\dotsc,\hat{v_i},\dotsc,\hat{v_j},\dotsc,v_n)$ Finally, the volume form on an oriented inner product space (or volume density on an inner product space) in linear algebra, and its counterpart the Riemannian volume form on an oriented Riemannian manifold (or volume density form on a Riemannian manifold) in differential geometry. Unlike the above examples which all admit global basis-free/coordinate-free definitions, we can define it only in a single coordinate patch or basis at a time, and glue together to obtain a globally defined structure. There are two definitions seen in the literature: choose an (oriented) coordinate neighborhood of a point, so we have a basis for each tangent space. Write the metric tensor in terms of that basis. Pretend that the bilinear form is actually a linear transformation (this can always be done because once a basis is chosen, we have an isomorphism to $\mathbb{R}^n$ which is isomorphic to its dual (via a different isomorphism than that provided by the inner product)). Then take the determinant of resulting mutated matrix, take the square root, multiply by the wedge of the basis one-forms (the positive root may be chosen in the oriented case; in the unoriented case, take the absolute value to obtain a density). Choose an oriented orthonormal coframe in a neighborhood. Wedge it together. (Finally take the absolute value in the unoriented case). Does anyone else think that one of these definitions sticks out like a sore thumb? Does it bother anyone else that in linear algebra, the volume form on an oriented inner product space doesn't exist as natural transformation $\operatorname{Sym}^2 \to \Lambda^n$? Do the instructions to ""take the determinant of a bilinear form"" scream out to anyone else that we're doing it wrong? Does it bother anyone else that in Riemannian geometry, in stark contrast to the superficially similar symplectic case, the volume form cannot be defined using invariant terminology for the whole manifold, but rather requires one to break the manifold into patches, and choose a basis for each? Is there any other structure in linear algebra or differential geometry which suffers from this defect? Answer: I've accepted Willie Wong's answer below, but let me also sum it up, since it's spread across several different places. There is a canonical construction of the Riemannian volume form on an oriented vector space, or pseudoform on a vector space. At the level of level of vector spaces, we may define an inner product on the dual space $V^*$ by $\tilde{g}(\sigma,\tau)=g(u,v)$ where $u,v$ are the dual vectors to $\sigma,\tau$ under the isomorphism between $V,V^*$ induced by $g$ (which is nondegenerate). Then extend $\tilde{g}$ to $\bigotimes^k V^*$ by defining $\hat{g}(a\otimes b\otimes c,\dotsb,x\otimes y\otimes z\dotsb)=\tilde{g}(a,x)\tilde{g}(b,y)\tilde{g}(c,z)\dotsb$. Then the space of alternating forms may be viewed as a subspace of $\bigotimes^k V^*$, and so inherits an inner product as well (note, however that while the alternating map may be defined canonically, there are varying normalization conventions which do not affect the kernel. I.e. $v\wedge w = k! Alt(v\otimes w)$ or $v\wedge w = Alt(v\otimes w)$). Then $\hat{g}(a\wedge b\dotsb,x\wedge y\dotsb)=\det[\tilde{g}(a,x)\dotsc]$ (with perhaps a normalization factor required here, depending on how Alt was defined). Thus $g$ extends to an inner product on $\Lambda^n(V^*)$, which is a 1 dimensional space, so there are only two unit vectors, and if $V$ is oriented, there is a canonical choice of volume form. And in any event, there is a canonical pseudoform.",,"['linear-algebra', 'differential-geometry', 'category-theory']"
77,"Can commuting matrices $X,Y$ always be written as polynomials of some matrix $A$?",Can commuting matrices  always be written as polynomials of some matrix ?,"X,Y A","Consider square matrices over a field $K$. I don't think additional assumptions about $K$ like algebraically closed or characteristic $0$ are pertinent, but feel free to make them for comfort. For any such matrix $A$, the set $K[A]$ of polynomials in $A$ is a commutative subalgebra of $M_n(K)$; the question is whether for any pair of commuting matrices $X,Y$ at least one such commutative subalgebra can be found that contains both $X$ and $Y$. I was asking myself this in connection with frequently recurring requests to completely characterise commuting pairs of matrices, like this one . While providing a useful characterisation seems impossible, a positive anwer to the current question would at least provide some answer. Note that in many rather likely situations one can in fact take $A$ to be one of the matrices $X,Y$, for instance when one of the matrices has distinct eigenvalues , or more generally if its minimal polynomial has degree $n$ (so coincides with the characteristic polynomial). However this is not always possible, as can be easily seen for instance for diagonal matrices $X=\operatorname{diag}(0,0,1)$ and $Y=\operatorname{diag}(0,1,1)$. However in that case both will be polynomials in $A=\operatorname{diag}(x,y,z)$ for any distinct values $x,y,z$ (then $K[A]$ consists of all diagonal matrices); although in the example in this answer the matrices are not both diagonalisable, an appropriate $A$ can be found there as well. I thought for some time that any maximal commutative subalgebra of $M_n(K)$ was of the form $K[A]$ (which would imply a positive answer) for some $A$ with minimal polynomial of degree$~n$, and that a positive answer to my question was in fact instrumental in proving this. However I was wrong on both counts: there exist (for $n\geq 4$) commutative subalgebras of dimension${}>n$ (whereas $\dim_KK[A]\leq n$ for all $A\in M_n(K)$) as shown in this MathOverflow answer , and I was forced to correct an anwer I gave here in the light of this; however it seems (at least in the cases I looked at) that many (all?) pairs of matrices $X,Y$ in such a subalgebra still admit a matrix $A$ (which in general is not in the subalgebra ) such that $X,Y\in K[A]$. This indicates that a positive answer to my question would not contradict the existence of such large commutative subalgebras: it would just mean that to obtain a maximal dimensional subalgebra containing $X,Y$ one should in general avoid throwing in an $A$ with $X,Y\in K[A]$. I do think these large subalgebras easily show that my question but for three commuting matrices has a negative answer. Finally I note that this other answer to the cited MO question mentions a result by Gerstenhaber that the dimension of the subalgebra generated two commuting matrices in $M_n(K)$ cannot exceed$~n$. This unfortunately does not settle my question (if $X,Y$ would generate a subalgebra of dimension${}>n$, it would have proved a negative answer); it just might be that the mentioned result is true because of the existence of $A$ (I don't have access to a proof right now, but given the formulation it seems unlikely that it was done this way). OK, I've tried to build up the suspense. Honesty demands that I say that I do know the answer to my question, since a colleague of mine provided a convincing one. I will however not give this answer right away, but post it once there has been some time for gathering answers here; who knows somebody will prove a different answer than the one I have (heaven forbid), or at least give the same answer with a different justification.","Consider square matrices over a field $K$. I don't think additional assumptions about $K$ like algebraically closed or characteristic $0$ are pertinent, but feel free to make them for comfort. For any such matrix $A$, the set $K[A]$ of polynomials in $A$ is a commutative subalgebra of $M_n(K)$; the question is whether for any pair of commuting matrices $X,Y$ at least one such commutative subalgebra can be found that contains both $X$ and $Y$. I was asking myself this in connection with frequently recurring requests to completely characterise commuting pairs of matrices, like this one . While providing a useful characterisation seems impossible, a positive anwer to the current question would at least provide some answer. Note that in many rather likely situations one can in fact take $A$ to be one of the matrices $X,Y$, for instance when one of the matrices has distinct eigenvalues , or more generally if its minimal polynomial has degree $n$ (so coincides with the characteristic polynomial). However this is not always possible, as can be easily seen for instance for diagonal matrices $X=\operatorname{diag}(0,0,1)$ and $Y=\operatorname{diag}(0,1,1)$. However in that case both will be polynomials in $A=\operatorname{diag}(x,y,z)$ for any distinct values $x,y,z$ (then $K[A]$ consists of all diagonal matrices); although in the example in this answer the matrices are not both diagonalisable, an appropriate $A$ can be found there as well. I thought for some time that any maximal commutative subalgebra of $M_n(K)$ was of the form $K[A]$ (which would imply a positive answer) for some $A$ with minimal polynomial of degree$~n$, and that a positive answer to my question was in fact instrumental in proving this. However I was wrong on both counts: there exist (for $n\geq 4$) commutative subalgebras of dimension${}>n$ (whereas $\dim_KK[A]\leq n$ for all $A\in M_n(K)$) as shown in this MathOverflow answer , and I was forced to correct an anwer I gave here in the light of this; however it seems (at least in the cases I looked at) that many (all?) pairs of matrices $X,Y$ in such a subalgebra still admit a matrix $A$ (which in general is not in the subalgebra ) such that $X,Y\in K[A]$. This indicates that a positive answer to my question would not contradict the existence of such large commutative subalgebras: it would just mean that to obtain a maximal dimensional subalgebra containing $X,Y$ one should in general avoid throwing in an $A$ with $X,Y\in K[A]$. I do think these large subalgebras easily show that my question but for three commuting matrices has a negative answer. Finally I note that this other answer to the cited MO question mentions a result by Gerstenhaber that the dimension of the subalgebra generated two commuting matrices in $M_n(K)$ cannot exceed$~n$. This unfortunately does not settle my question (if $X,Y$ would generate a subalgebra of dimension${}>n$, it would have proved a negative answer); it just might be that the mentioned result is true because of the existence of $A$ (I don't have access to a proof right now, but given the formulation it seems unlikely that it was done this way). OK, I've tried to build up the suspense. Honesty demands that I say that I do know the answer to my question, since a colleague of mine provided a convincing one. I will however not give this answer right away, but post it once there has been some time for gathering answers here; who knows somebody will prove a different answer than the one I have (heaven forbid), or at least give the same answer with a different justification.",,"['linear-algebra', 'abstract-algebra', 'matrices']"
78,"A $\{0,1\}$-matrix with positive spectrum must have all eigenvalues equal to $1$",A -matrix with positive spectrum must have all eigenvalues equal to,"\{0,1\} 1","Here's a cute problem that was frequently given by the late Herbert Wilf during his talks. Problem: Let $A$ be an $n \times n$ matrix with entries from $\{0,1\}$ having all positive eigenvalues. Prove that all of the eigenvalues of $A$ are $1$. Proof: Use the AM-GM inequality to relate the trace and determinant. Is there any other proof?","Here's a cute problem that was frequently given by the late Herbert Wilf during his talks. Problem: Let $A$ be an $n \times n$ matrix with entries from $\{0,1\}$ having all positive eigenvalues. Prove that all of the eigenvalues of $A$ are $1$. Proof: Use the AM-GM inequality to relate the trace and determinant. Is there any other proof?",,"['linear-algebra', 'eigenvalues-eigenvectors', 'alternative-proof']"
79,Systems of linear equations: Why does no one plug back in?,Systems of linear equations: Why does no one plug back in?,,"When someone wants to solve a system of linear equations like $$\begin{cases} 2x+y=0 \\ 3x+y=4 \end{cases}\,,$$ they might use this logic: $$\begin{align} \begin{cases} 2x+y=0 \\ 3x+y=4 \end{cases}  \iff &\begin{cases} -2x-y=0 \\ 3x+y=4 \end{cases}  \\  \color{maroon}{\implies} &\begin{cases} -2x-y=0\\ x=4 \end{cases}  \iff \begin{cases} -2(4)-y=0\\ x=4 \end{cases}  \iff \begin{cases} y=-8\\ x=4 \end{cases} \,.\end{align}$$ Then they conclude that $(x, y) = (4, -8)$ is a solution to the system. This turns out to be correct, but the logic seems flawed to me. As I see it, all this proves is that  $$   \forall{x,y\in\mathbb{R}}\quad   \bigg(   \begin{cases} 2x+y=0 \\ 3x+y=4 \end{cases}    \color{maroon}{\implies}   \begin{cases} y=-8\\ x=4 \end{cases}   \bigg)\,. $$ But this statement leaves the possibility open that there is no pair $(x, y)$ in $\mathbb{R}^2$ that satisfies the system of equations. $$  \text{What if}\;  \begin{cases} 2x+y=0 \\ 3x+y=4 \end{cases}  \;\text{has no solution?} $$ It seems to me that to really be sure we've solved the equation, we have to plug back in for $x$ and $y$. I'm not talking about checking our work for simple mistakes. This seems like a matter of logical necessity. But of course, most people don't bother to plug back in, and it never seems to backfire on them. So why does no one plug back in? P.S. It would be great if I could understand this for systems of two variables, but I would be deeply thrilled to understand it for systems of $n$ variables. I'm starting to use Gaussian elimination on big systems in my linear algebra class, where intuition is weaker and calculations are more complex, and still no one feels the need to plug back in.","When someone wants to solve a system of linear equations like $$\begin{cases} 2x+y=0 \\ 3x+y=4 \end{cases}\,,$$ they might use this logic: $$\begin{align} \begin{cases} 2x+y=0 \\ 3x+y=4 \end{cases}  \iff &\begin{cases} -2x-y=0 \\ 3x+y=4 \end{cases}  \\  \color{maroon}{\implies} &\begin{cases} -2x-y=0\\ x=4 \end{cases}  \iff \begin{cases} -2(4)-y=0\\ x=4 \end{cases}  \iff \begin{cases} y=-8\\ x=4 \end{cases} \,.\end{align}$$ Then they conclude that $(x, y) = (4, -8)$ is a solution to the system. This turns out to be correct, but the logic seems flawed to me. As I see it, all this proves is that  $$   \forall{x,y\in\mathbb{R}}\quad   \bigg(   \begin{cases} 2x+y=0 \\ 3x+y=4 \end{cases}    \color{maroon}{\implies}   \begin{cases} y=-8\\ x=4 \end{cases}   \bigg)\,. $$ But this statement leaves the possibility open that there is no pair $(x, y)$ in $\mathbb{R}^2$ that satisfies the system of equations. $$  \text{What if}\;  \begin{cases} 2x+y=0 \\ 3x+y=4 \end{cases}  \;\text{has no solution?} $$ It seems to me that to really be sure we've solved the equation, we have to plug back in for $x$ and $y$. I'm not talking about checking our work for simple mistakes. This seems like a matter of logical necessity. But of course, most people don't bother to plug back in, and it never seems to backfire on them. So why does no one plug back in? P.S. It would be great if I could understand this for systems of two variables, but I would be deeply thrilled to understand it for systems of $n$ variables. I'm starting to use Gaussian elimination on big systems in my linear algebra class, where intuition is weaker and calculations are more complex, and still no one feels the need to plug back in.",,"['linear-algebra', 'algebra-precalculus', 'proof-writing', 'systems-of-equations']"
80,"Why is that if every row of a matrix sums to 1, then the rows of the inverse matrix sums to 1 too?","Why is that if every row of a matrix sums to 1, then the rows of the inverse matrix sums to 1 too?",,"Why is that if every row of a matrix sums to $1$ then the rows of its inverse matrix sum to $1$ too? For example, consider $$A=\begin{pmatrix} 1/3 & 2/3 \\ 3/4 & 1/4 \end{pmatrix}$$ then its inverse is $$A^{-1}=\begin{pmatrix} -3/5 & 8/5 \\ 9/5 & -4/5 \end{pmatrix},$$ which satisfies the condition. Is it true for every such matrix?","Why is that if every row of a matrix sums to $1$ then the rows of its inverse matrix sum to $1$ too? For example, consider $$A=\begin{pmatrix} 1/3 & 2/3 \\ 3/4 & 1/4 \end{pmatrix}$$ then its inverse is $$A^{-1}=\begin{pmatrix} -3/5 & 8/5 \\ 9/5 & -4/5 \end{pmatrix},$$ which satisfies the condition. Is it true for every such matrix?",,"['linear-algebra', 'matrices', 'inverse']"
81,How can we prove Sylvester's determinant identity?,How can we prove Sylvester's determinant identity?,,"Sylvester's determinant identity states that if $A$ and $B$ are matrices of sizes $m\times n$ and $n\times m$, then $$ \det(I_m+AB) = \det(I_n+BA)$$ where $I_m$ and $I_n$ denote the $m \times m$ and $n \times n$ identity matrices, respectively. Could you sketch a proof for me, or point to an accessible reference?","Sylvester's determinant identity states that if $A$ and $B$ are matrices of sizes $m\times n$ and $n\times m$, then $$ \det(I_m+AB) = \det(I_n+BA)$$ where $I_m$ and $I_n$ denote the $m \times m$ and $n \times n$ identity matrices, respectively. Could you sketch a proof for me, or point to an accessible reference?",,"['linear-algebra', 'matrices', 'determinant']"
82,How to count number of bases and subspaces of a given dimension in a vector space over a finite field? [duplicate],How to count number of bases and subspaces of a given dimension in a vector space over a finite field? [duplicate],,"This question already has answers here : How many k-dimensional subspaces there are in n-dimensional vector space over $\mathbb F_p$? (3 answers) Closed 5 years ago . Let $V_{n}(F)$ be a vector space over the field $F=\mathbb Z_{p}$ with $\dim V_{n} = n$, i.e., the cardinality of $V_{n}(\mathbb Z_{p}) = p^{n}$. What is a general criterion to find the number of bases in such a vector space? For example, find the number of bases in $ V_{2}(\mathbb Z_{3})$. Further, how can we find the number of subspaces of dimension, say, $r$? I need a justification with proof. I have a formula, but I am unable to understand the basic idea behind that formula.","This question already has answers here : How many k-dimensional subspaces there are in n-dimensional vector space over $\mathbb F_p$? (3 answers) Closed 5 years ago . Let $V_{n}(F)$ be a vector space over the field $F=\mathbb Z_{p}$ with $\dim V_{n} = n$, i.e., the cardinality of $V_{n}(\mathbb Z_{p}) = p^{n}$. What is a general criterion to find the number of bases in such a vector space? For example, find the number of bases in $ V_{2}(\mathbb Z_{3})$. Further, how can we find the number of subspaces of dimension, say, $r$? I need a justification with proof. I have a formula, but I am unable to understand the basic idea behind that formula.",,"['linear-algebra', 'combinatorics', 'finite-fields']"
83,Why are invertible matrices called 'non-singular'?,Why are invertible matrices called 'non-singular'?,,"Where in the history of linear algebra did we pick up on referring to invertible matrices as 'non-singular'?  In fact, since the null space of an invertible matrix has a single vector an invertible matrix has a single solution for every possible $b$ in $AX=b$ it's easy to imagine that that invertible matrices would be called 'singular'.  What gives?","Where in the history of linear algebra did we pick up on referring to invertible matrices as 'non-singular'?  In fact, since the null space of an invertible matrix has a single vector an invertible matrix has a single solution for every possible $b$ in $AX=b$ it's easy to imagine that that invertible matrices would be called 'singular'.  What gives?",,"['linear-algebra', 'terminology']"
84,"""Well defined"" function - What does it mean?","""Well defined"" function - What does it mean?",,What does it mean for a function to be well-defined? I encountered this term in an exercise asking to check if a linear transformation is well-defined.,What does it mean for a function to be well-defined? I encountered this term in an exercise asking to check if a linear transformation is well-defined.,,"['linear-algebra', 'functions', 'transformation']"
85,Word origin / meaning of 'kernel' in linear algebra,Word origin / meaning of 'kernel' in linear algebra,,"It may be the dumbest question ever asked on math.SE, but... Given a real matrix $\mathbf A\in\mathbb R^{m\times n}$, the column space is defined as  $$C(\mathbf A) = \{\mathbf A \mathbf x : \mathbf x \in \mathbb{R}^n\} \subseteq \mathbb R^m.$$ It is sometimes called image or range . I'm OK with the name 'column space' because $C(\mathbf A)$ is the set of all possible linear combinations of $\mathbf A$'s column vectors. I'm OK with the name 'image' because if I consider $\mathbf A \mathbf x$ as a function then $C(\mathbf A)$ is this function's image (the subset of a function's codomain ). I'm OK with the name 'range' because I can consider $C(\mathbf A)$ as a range of a function $f(\mathbf x) = \mathbf A \mathbf x$. Unfortunately, I'm not happy with the name kernel . $$\ker(\mathbf A) = \{\mathbf x: \mathbf A\mathbf x = \mathbf 0\}\subseteq \mathbb R^n$$ The kernel is sometimes called null space and I can fairly understand where this name came from -- it's because this set contains all the elements in $\mathbb R^n$ that are mapped to zero by $\mathbf A$. Then why is it called 'kernel'? Any historic background or colloquial meaning that I completely missed?","It may be the dumbest question ever asked on math.SE, but... Given a real matrix $\mathbf A\in\mathbb R^{m\times n}$, the column space is defined as  $$C(\mathbf A) = \{\mathbf A \mathbf x : \mathbf x \in \mathbb{R}^n\} \subseteq \mathbb R^m.$$ It is sometimes called image or range . I'm OK with the name 'column space' because $C(\mathbf A)$ is the set of all possible linear combinations of $\mathbf A$'s column vectors. I'm OK with the name 'image' because if I consider $\mathbf A \mathbf x$ as a function then $C(\mathbf A)$ is this function's image (the subset of a function's codomain ). I'm OK with the name 'range' because I can consider $C(\mathbf A)$ as a range of a function $f(\mathbf x) = \mathbf A \mathbf x$. Unfortunately, I'm not happy with the name kernel . $$\ker(\mathbf A) = \{\mathbf x: \mathbf A\mathbf x = \mathbf 0\}\subseteq \mathbb R^n$$ The kernel is sometimes called null space and I can fairly understand where this name came from -- it's because this set contains all the elements in $\mathbb R^n$ that are mapped to zero by $\mathbf A$. Then why is it called 'kernel'? Any historic background or colloquial meaning that I completely missed?",,"['linear-algebra', 'matrices', 'soft-question', 'terminology', 'math-history']"
86,"Polynomial equations $p(A, B) = 0$ for matrices that ensure $AB = BA$",Polynomial equations  for matrices that ensure,"p(A, B) = 0 AB = BA","Let $k$ be a field with characteristic different from $2$, and $A$ and $B$ be $2 \times 2$ matrices with entries in $k$. Then we can prove, with a bit art, that $A^2 - 2AB + B^2 = O$ implies $AB = BA$, hence $(A - B)^2 = O$. It came to a surprise for me when I first succeeded in proving this, for this seemed quite nontrivial to me. I am curious if there is a similar or more general result for the polynomial equations of matrices that ensures commutativity. (Of course, we do not consider trivial cases such as the polynomial $p(X, Y) = XY - YX$ corresponding to commutator) p.s. This question is purely out of curiosity. I do not know even this kind of problem is worth considering, so you may regard this question as a recreational one.","Let $k$ be a field with characteristic different from $2$, and $A$ and $B$ be $2 \times 2$ matrices with entries in $k$. Then we can prove, with a bit art, that $A^2 - 2AB + B^2 = O$ implies $AB = BA$, hence $(A - B)^2 = O$. It came to a surprise for me when I first succeeded in proving this, for this seemed quite nontrivial to me. I am curious if there is a similar or more general result for the polynomial equations of matrices that ensures commutativity. (Of course, we do not consider trivial cases such as the polynomial $p(X, Y) = XY - YX$ corresponding to commutator) p.s. This question is purely out of curiosity. I do not know even this kind of problem is worth considering, so you may regard this question as a recreational one.",,"['linear-algebra', 'matrices']"
87,Why is the determinant defined in terms of permutations?,Why is the determinant defined in terms of permutations?,,"Where does the definition of the determinant come from, and is the definition in terms of permutations the first and basic one? What is the deep reason for giving such a definition in terms of permutations? $$ \text{det}(A)=\sum_{p}\sigma(p)a_{1p_1}a_{2p_2}...a_{np_n}. $$ I have found this one useful: Thomas Muir, Contributions to the History of Determinants 1900-1920 .","Where does the definition of the determinant come from, and is the definition in terms of permutations the first and basic one? What is the deep reason for giving such a definition in terms of permutations? I have found this one useful: Thomas Muir, Contributions to the History of Determinants 1900-1920 .","
\text{det}(A)=\sum_{p}\sigma(p)a_{1p_1}a_{2p_2}...a_{np_n}.
","['linear-algebra', 'matrices', 'permutations', 'definition', 'determinant']"
88,"In plain language, what's the significance of a field?","In plain language, what's the significance of a field?",,"I just started Linear Algebra. Yesterday, I read about the ten properties of fields. As far as I can tell a field is a mathematical system that we can use to do common arithmetic. Is that correct?","I just started Linear Algebra. Yesterday, I read about the ten properties of fields. As far as I can tell a field is a mathematical system that we can use to do common arithmetic. Is that correct?",,"['linear-algebra', 'field-theory']"
89,Why is the determinant of a symplectic matrix 1?,Why is the determinant of a symplectic matrix 1?,,"Suppose $A \in M_{2n}(\mathbb{R})$ . and $$J=\begin{pmatrix} 0 & E_n\\   -E_n&0  \end{pmatrix}$$ where $E_n$ represents identity matrix. if $A$ satisfies $$AJA^T=J.$$ How to figure out $$\det(A)=1~?$$ My approach: I have tried to separate $A$ into four submartix: $$A=\begin{pmatrix}A_1&A_2 \\A_3&A_4 \end{pmatrix}$$ and I must add a assumption that $A_1$ is invertible. by elementary transfromation: $$\begin{pmatrix}A_1&A_2 \\ A_3&A_4\end{pmatrix}\rightarrow \begin{pmatrix}A_1&A_2 \\ 0&A_4-A_3A_1^{-1}A_2\end{pmatrix}$$ we have: $$\det(A)=\det(A_1)\det(A_4-A_3A_1^{-1}A_2).$$ From $$\begin{pmatrix}A_1&A_2 \\ A_3&A_4\end{pmatrix}\begin{pmatrix}0&E_n \\ -E_n&0\end{pmatrix}\begin{pmatrix}A_1&A_2 \\ A_3&A_4\end{pmatrix}^T=\begin{pmatrix}0&E_n \\ -E_n&0\end{pmatrix}.$$ we get two equalities: $$A_1A_2^T=A_2A_1^T$$ and $$A_1A_4^T-A_2A_3^T=E_n.$$ then $$\det(A)=\det(A_1(A_4-A_3A_1^{-1}A_2)^T)=\det(A_1A_4^T-A_1A_2^T(A_1^T)^{-1}A_3^T)=\det(A_1A_4^T-A_2A_1^T(A_1^T)^{-1}A_3^T)=\det(E_n)=1,$$ but I have no idea to deal with this problem when $A_1$ is not invertible.",Suppose . and where represents identity matrix. if satisfies How to figure out My approach: I have tried to separate into four submartix: and I must add a assumption that is invertible. by elementary transfromation: we have: From we get two equalities: and then but I have no idea to deal with this problem when is not invertible.,"A \in M_{2n}(\mathbb{R}) J=\begin{pmatrix}
0 & E_n\\ 
 -E_n&0 
\end{pmatrix} E_n A AJA^T=J. \det(A)=1~? A A=\begin{pmatrix}A_1&A_2 \\A_3&A_4 \end{pmatrix} A_1 \begin{pmatrix}A_1&A_2 \\ A_3&A_4\end{pmatrix}\rightarrow \begin{pmatrix}A_1&A_2 \\ 0&A_4-A_3A_1^{-1}A_2\end{pmatrix} \det(A)=\det(A_1)\det(A_4-A_3A_1^{-1}A_2). \begin{pmatrix}A_1&A_2 \\ A_3&A_4\end{pmatrix}\begin{pmatrix}0&E_n \\ -E_n&0\end{pmatrix}\begin{pmatrix}A_1&A_2 \\ A_3&A_4\end{pmatrix}^T=\begin{pmatrix}0&E_n \\ -E_n&0\end{pmatrix}. A_1A_2^T=A_2A_1^T A_1A_4^T-A_2A_3^T=E_n. \det(A)=\det(A_1(A_4-A_3A_1^{-1}A_2)^T)=\det(A_1A_4^T-A_1A_2^T(A_1^T)^{-1}A_3^T)=\det(A_1A_4^T-A_2A_1^T(A_1^T)^{-1}A_3^T)=\det(E_n)=1, A_1","['linear-algebra', 'matrices', 'determinant', 'symplectic-linear-algebra']"
90,Prove that the eigenvalues of a real symmetric matrix are real,Prove that the eigenvalues of a real symmetric matrix are real,,"I am having a difficult time with the following question. Any help will be much appreciated. Let $A$ be an $n×n$ real matrix such that $A^T = A$. We call such matrices “symmetric.” Prove that the eigenvalues of a real symmetric matrix are real (i.e. if $\lambda$ is an eigenvalue of $A$, show that $\lambda = \overline{\lambda}$ )","I am having a difficult time with the following question. Any help will be much appreciated. Let $A$ be an $n×n$ real matrix such that $A^T = A$. We call such matrices “symmetric.” Prove that the eigenvalues of a real symmetric matrix are real (i.e. if $\lambda$ is an eigenvalue of $A$, show that $\lambda = \overline{\lambda}$ )",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'inner-products', 'symmetric-matrices']"
91,"Given a matrix, is there always another matrix which commutes with it?","Given a matrix, is there always another matrix which commutes with it?",,"Given a matrix $A$ over a field $F$, does there always exist a matrix $B$ such that $AB = BA$? (except the trivial case and the polynomial ring?)","Given a matrix $A$ over a field $F$, does there always exist a matrix $B$ such that $AB = BA$? (except the trivial case and the polynomial ring?)",,"['linear-algebra', 'matrices']"
92,Finding a unit vector perpendicular to another vector,Finding a unit vector perpendicular to another vector,,"For example we have the vector $8i + 4j - 6k$, how can we find a unit vector perpendicular to this vector?","For example we have the vector $8i + 4j - 6k$, how can we find a unit vector perpendicular to this vector?",,"['linear-algebra', 'vector-spaces']"
93,Sylvester rank inequality: $\operatorname{rank} A + \operatorname{rank}B \leq \operatorname{rank} AB + n$ [duplicate],Sylvester rank inequality:  [duplicate],\operatorname{rank} A + \operatorname{rank}B \leq \operatorname{rank} AB + n,"This question already has answers here : Prove Sylvester rank inequality: $\text{rank}(AB)\ge\text{rank}(A)+\text{rank}(B)-n$ (5 answers) Closed 2 years ago . If $A$ and $B$ are two matrices of the same order $n$, then   $$ \operatorname{rank} A + \operatorname{rank}B \leq \operatorname{rank} AB + n. $$ I don't know how to start proving this inequality. I would be very pleased if someone helps me. Thanks! Edit I. Rank of $A$ is the same of the equivalent matrix $A' =\begin{pmatrix}I_r & 0 \\ 0 & 0\end{pmatrix}$. Analogously for $B$, ranks of $A$ and $B$ are $r,s\leq n$. Hence, since $\operatorname{rank}AB = \min\{r,s\}$, then $r+s\leq \min\{r,s\} + n$. (This is not correct since $\operatorname{rank} AB \leq \min\{r,s\}$. Edit II. A discussion on the rank of a product of $H_f(A)$ and $H_c(B)$ would correct this, but I don't know how to formalize that $\operatorname{rank}H_f(A) +\operatorname{rank}H_c(B) - n \leq \operatorname{rank}[H_f(A)H_c(B)]$.","This question already has answers here : Prove Sylvester rank inequality: $\text{rank}(AB)\ge\text{rank}(A)+\text{rank}(B)-n$ (5 answers) Closed 2 years ago . If $A$ and $B$ are two matrices of the same order $n$, then   $$ \operatorname{rank} A + \operatorname{rank}B \leq \operatorname{rank} AB + n. $$ I don't know how to start proving this inequality. I would be very pleased if someone helps me. Thanks! Edit I. Rank of $A$ is the same of the equivalent matrix $A' =\begin{pmatrix}I_r & 0 \\ 0 & 0\end{pmatrix}$. Analogously for $B$, ranks of $A$ and $B$ are $r,s\leq n$. Hence, since $\operatorname{rank}AB = \min\{r,s\}$, then $r+s\leq \min\{r,s\} + n$. (This is not correct since $\operatorname{rank} AB \leq \min\{r,s\}$. Edit II. A discussion on the rank of a product of $H_f(A)$ and $H_c(B)$ would correct this, but I don't know how to formalize that $\operatorname{rank}H_f(A) +\operatorname{rank}H_c(B) - n \leq \operatorname{rank}[H_f(A)H_c(B)]$.",,"['linear-algebra', 'matrices', 'inequality', 'matrix-rank']"
94,When can two linear operators on a finite-dimensional space be simultaneously Jordanized?,When can two linear operators on a finite-dimensional space be simultaneously Jordanized?,,"IN a comment to Qiaochu's answer here it is mentioned that two commuting matrices can be simultaneously Jordanized (sorry that this sounds less appealing then ""diagonalized"" :P ), i.e. can be brought to a Jordan normal form by the same similarity transformation. I was wondering about the converse - when can two linear operators acting on a finite-dimensional vector space (over an algebraically closed field) be simultaneously Jordanized? Unlike the case of simultaneous diagonalization, I don't think commutativity is forced on the transformations in this case, and I'm interested in other natural conditions which guarantee that this is possible. EDIT: as Georges pointed out, the statements that two commuting matrices are simultaneously Jordanizable is in fact wrong. Nevertheless, I am still interested in interesting conditions on a pair of operators which ensures a simultaneous Jordanization (of course, there are some obvious sufficient conditions, i.e. that the two matrices are actually diagonalizable and commute, but this is not very appealing...)","IN a comment to Qiaochu's answer here it is mentioned that two commuting matrices can be simultaneously Jordanized (sorry that this sounds less appealing then ""diagonalized"" :P ), i.e. can be brought to a Jordan normal form by the same similarity transformation. I was wondering about the converse - when can two linear operators acting on a finite-dimensional vector space (over an algebraically closed field) be simultaneously Jordanized? Unlike the case of simultaneous diagonalization, I don't think commutativity is forced on the transformations in this case, and I'm interested in other natural conditions which guarantee that this is possible. EDIT: as Georges pointed out, the statements that two commuting matrices are simultaneously Jordanizable is in fact wrong. Nevertheless, I am still interested in interesting conditions on a pair of operators which ensures a simultaneous Jordanization (of course, there are some obvious sufficient conditions, i.e. that the two matrices are actually diagonalizable and commute, but this is not very appealing...)",,"['linear-algebra', 'linear-transformations', 'jordan-normal-form']"
95,A matrix is similar to its transpose [duplicate],A matrix is similar to its transpose [duplicate],,"This question already has answers here : Closed 12 years ago . Possible Duplicate: Matrix is conjugate to its own transpose How can I prove that a matrix is similar to its transpose? My approach is: if $A$ is the matrix then $f$ is the associated application from $K^n\rightarrow K^n$. Define $g:K^n\rightarrow (K^n)^*$ by $g(e_i)=e_i^*$, and define $f^T$ to be the transpose application of $f$. I proved that $f^T=gfg^{-1}$. What I don't understand is, what is the matrix associated to $g$, so I can write $A^T=PAP^{-1}$.","This question already has answers here : Closed 12 years ago . Possible Duplicate: Matrix is conjugate to its own transpose How can I prove that a matrix is similar to its transpose? My approach is: if $A$ is the matrix then $f$ is the associated application from $K^n\rightarrow K^n$. Define $g:K^n\rightarrow (K^n)^*$ by $g(e_i)=e_i^*$, and define $f^T$ to be the transpose application of $f$. I proved that $f^T=gfg^{-1}$. What I don't understand is, what is the matrix associated to $g$, so I can write $A^T=PAP^{-1}$.",,['linear-algebra']
96,A rank-one matrix is the product of two vectors,A rank-one matrix is the product of two vectors,,Let $A$ be an $n\times m$  matrix. Prove that $\operatorname{rank} (A) = 1$ if and only if there exist column vectors $v \in \mathbb{R}^n$ and $w \in \mathbb{R}^m$ such that $A=vw^t$. Progress: I'm going back and forth between using the definitions of rank: $\operatorname{rank} (A) = \dim(\operatorname{col}(A)) = \dim(\operatorname{row}(A))$ or using the rank theorem that says $ \operatorname{rank}(A)+\operatorname{nullity}(A) = m$. So in the second case I have to prove that $\operatorname{nullity}(A)=m$-1,Let $A$ be an $n\times m$  matrix. Prove that $\operatorname{rank} (A) = 1$ if and only if there exist column vectors $v \in \mathbb{R}^n$ and $w \in \mathbb{R}^m$ such that $A=vw^t$. Progress: I'm going back and forth between using the definitions of rank: $\operatorname{rank} (A) = \dim(\operatorname{col}(A)) = \dim(\operatorname{row}(A))$ or using the rank theorem that says $ \operatorname{rank}(A)+\operatorname{nullity}(A) = m$. So in the second case I have to prove that $\operatorname{nullity}(A)=m$-1,,"['linear-algebra', 'matrices', 'vector-spaces', 'matrix-rank', 'rank-1-matrices']"
97,"Proving: ""The trace of an idempotent matrix equals the rank of the matrix""","Proving: ""The trace of an idempotent matrix equals the rank of the matrix""",,"How could we prove that the ""The trace of an idempotent matrix equals the rank of the matrix""? This is another property that is used in my module without any proof, could anybody tell me how to prove this one?","How could we prove that the ""The trace of an idempotent matrix equals the rank of the matrix""? This is another property that is used in my module without any proof, could anybody tell me how to prove this one?",,"['linear-algebra', 'matrices', 'matrix-rank', 'trace', 'idempotents']"
98,Matrices which are both unitary and Hermitian,Matrices which are both unitary and Hermitian,,"Matrices such as $$ \begin{bmatrix}   \cos\theta & \sin\theta \\    \sin\theta & -\cos\theta \end{bmatrix} \text{ or } \begin{bmatrix}   \cos\theta & i\sin\theta \\    -i\sin\theta & -\cos\theta \end{bmatrix} \text{ or } \begin{bmatrix}   \pm 1 & 0 \\    0 & \pm 1 \end{bmatrix} $$ are both unitary and Hermitian (for $0 \le \theta \le 2\pi$). I call the latter type trivial , since its columns equal to plus/minus columns of the identity matrix. Do such matrices have any significance (in theory or practice)? In the answer to this question , it is said that ""for every Hilbert space except $\mathbb{C}^2$, a unitary matrix cannot be Hermitian and vice versa."" It was commented that identity matrices are always both unitary and Hermitian, and so this rule is not true. In fact, all trivial matrices (as defined above) have this property. Moreover, matrices such as $$ \begin{bmatrix}   \sqrt {0.5} & 0 & \sqrt {0.5} \\    0 & 1 & 0 \\   \sqrt {0.5} & 0 & -\sqrt {0.5} \end{bmatrix} $$ are both unitary and Hermitian. So, the general rule in the aforementioned question seems to be pointless. It seems that, for any $n > 1$, infinitely many matrices over the Hilbert space $\mathbb{C}^n$ are simultaneously unitary and Hermitian, right?","Matrices such as $$ \begin{bmatrix}   \cos\theta & \sin\theta \\    \sin\theta & -\cos\theta \end{bmatrix} \text{ or } \begin{bmatrix}   \cos\theta & i\sin\theta \\    -i\sin\theta & -\cos\theta \end{bmatrix} \text{ or } \begin{bmatrix}   \pm 1 & 0 \\    0 & \pm 1 \end{bmatrix} $$ are both unitary and Hermitian (for $0 \le \theta \le 2\pi$). I call the latter type trivial , since its columns equal to plus/minus columns of the identity matrix. Do such matrices have any significance (in theory or practice)? In the answer to this question , it is said that ""for every Hilbert space except $\mathbb{C}^2$, a unitary matrix cannot be Hermitian and vice versa."" It was commented that identity matrices are always both unitary and Hermitian, and so this rule is not true. In fact, all trivial matrices (as defined above) have this property. Moreover, matrices such as $$ \begin{bmatrix}   \sqrt {0.5} & 0 & \sqrt {0.5} \\    0 & 1 & 0 \\   \sqrt {0.5} & 0 & -\sqrt {0.5} \end{bmatrix} $$ are both unitary and Hermitian. So, the general rule in the aforementioned question seems to be pointless. It seems that, for any $n > 1$, infinitely many matrices over the Hilbert space $\mathbb{C}^n$ are simultaneously unitary and Hermitian, right?",,"['linear-algebra', 'matrices', 'unitary-matrices', 'hermitian-matrices']"
99,Diagonalizable matrices with complex values are dense in set of $n\times n$ complex matrices.,Diagonalizable matrices with complex values are dense in set of  complex matrices.,n\times n,"My linear algebra professor proved that Diagonalizable matrices with complex values are dense in set of $n \times n$ complex matrices. He defined a metric (I believe) that was somehow related to the usual metric on $\mathbb{R}^{n^2}$. Then somehow proved that diagonalizable matrices were dense because for any matrix $A$ if $\det(A - \lambda I) = 0$ on an open subset, then $\det(A - \lambda I)$ was the zero polynomial. I Googled around a bit and found some stuff talking about the Zariski topology, and I am not sure this is what I want. Most of what I have found on this topology is much more general than what he was doing. Does anyone know any resources that would explain how to go about proving this statement? Edit: I found out how to prove this the way my professor did. We define the norm of a matrix by $$ |A| = \max\{|Ax| : |x| = 1 \}.$$ Now we have a distance $d(A, B) < \epsilon$. You can prove that if $(A - B)_{ij} < \epsilon/n$, then $|A - B| < \epsilon$. Let $p(x)$ be the characteristic polynomial of $A$, an $n \times n$ matrix.  $$p(x) = \prod_1^n (x - x_i) = x^n + (-1)\sigma_1 x^{n - 1} + \cdots + (-1)^n\sigma_n$$ where $\sigma_1, \ldots, \sigma_n$ are the elementary symmetric polynomials of $x_1, \ldots, x_n$ which are the eigenvalues. The discriminant of the characteristic polynomial is a symmetric polynomial, therefore it can be written in terms of the elementary symmetric polynomials, which in turn can be written in terms of the entries of the matrix. But since the discriminant is a polynomial, it only has finitely many roots. Therefore for $\epsilon > 0$, by changing the entries of the matrix less than $\epsilon / n$ we can find a new matrix $B$ such that $|B - A| < \epsilon$ and the discriminant is not zero. The discriminant being not zero means $B$ has distinct eigenvalues, thus has a basis of eigenvectors. Therefore, it is diagonalizable. Thus the set of diagonalizable matrices is dense in the set of matrices with respect to that metric.","My linear algebra professor proved that Diagonalizable matrices with complex values are dense in set of $n \times n$ complex matrices. He defined a metric (I believe) that was somehow related to the usual metric on $\mathbb{R}^{n^2}$. Then somehow proved that diagonalizable matrices were dense because for any matrix $A$ if $\det(A - \lambda I) = 0$ on an open subset, then $\det(A - \lambda I)$ was the zero polynomial. I Googled around a bit and found some stuff talking about the Zariski topology, and I am not sure this is what I want. Most of what I have found on this topology is much more general than what he was doing. Does anyone know any resources that would explain how to go about proving this statement? Edit: I found out how to prove this the way my professor did. We define the norm of a matrix by $$ |A| = \max\{|Ax| : |x| = 1 \}.$$ Now we have a distance $d(A, B) < \epsilon$. You can prove that if $(A - B)_{ij} < \epsilon/n$, then $|A - B| < \epsilon$. Let $p(x)$ be the characteristic polynomial of $A$, an $n \times n$ matrix.  $$p(x) = \prod_1^n (x - x_i) = x^n + (-1)\sigma_1 x^{n - 1} + \cdots + (-1)^n\sigma_n$$ where $\sigma_1, \ldots, \sigma_n$ are the elementary symmetric polynomials of $x_1, \ldots, x_n$ which are the eigenvalues. The discriminant of the characteristic polynomial is a symmetric polynomial, therefore it can be written in terms of the elementary symmetric polynomials, which in turn can be written in terms of the entries of the matrix. But since the discriminant is a polynomial, it only has finitely many roots. Therefore for $\epsilon > 0$, by changing the entries of the matrix less than $\epsilon / n$ we can find a new matrix $B$ such that $|B - A| < \epsilon$ and the discriminant is not zero. The discriminant being not zero means $B$ has distinct eigenvalues, thus has a basis of eigenvectors. Therefore, it is diagonalizable. Thus the set of diagonalizable matrices is dense in the set of matrices with respect to that metric.",,"['linear-algebra', 'matrices']"
