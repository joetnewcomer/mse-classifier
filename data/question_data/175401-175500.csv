,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Explicit formula for higher order derivatives in higher dimensions,Explicit formula for higher order derivatives in higher dimensions,,"Let $f:\mathbb{R}^n\to\mathbb{R}^m$ be a function at least $C^k$ and fix some point $x_0\in\mathbb{R}^n$. I want an explicit formula for its derivatives of higher order. I already read this threads: What are higher derivatives? Using higher order derivatives One thing is clear, $D^kf(x_0)$ is a $k$-linear map from $\underbrace{\mathbb{R}^n\times\ldots\times\mathbb{R}^n}_{k \text{ times}}$ to $\mathbb{R}^m$. Yet, the answers given do not address my question, they only give the abstract description but I want a concrete description. For $k=1$ we have that $Df(x_0)v = [Df(x_0)]\cdot v$, where $ [Df(x_0)] = \left(\frac{\partial f_i(x_0)}{\partial x_j}\right)$ stands for the Jacobian matrix of $Df(x_0)$. So we have an explicit formula. For $k\geq 2$, it's not clear how to get an explicit formula for $D^kf(x_0)(v_1,\ldots,v_k)$ from the definition and the case $k=1$. There must be some way to do this. I hope you can help me. Thank you very much.","Let $f:\mathbb{R}^n\to\mathbb{R}^m$ be a function at least $C^k$ and fix some point $x_0\in\mathbb{R}^n$. I want an explicit formula for its derivatives of higher order. I already read this threads: What are higher derivatives? Using higher order derivatives One thing is clear, $D^kf(x_0)$ is a $k$-linear map from $\underbrace{\mathbb{R}^n\times\ldots\times\mathbb{R}^n}_{k \text{ times}}$ to $\mathbb{R}^m$. Yet, the answers given do not address my question, they only give the abstract description but I want a concrete description. For $k=1$ we have that $Df(x_0)v = [Df(x_0)]\cdot v$, where $ [Df(x_0)] = \left(\frac{\partial f_i(x_0)}{\partial x_j}\right)$ stands for the Jacobian matrix of $Df(x_0)$. So we have an explicit formula. For $k\geq 2$, it's not clear how to get an explicit formula for $D^kf(x_0)(v_1,\ldots,v_k)$ from the definition and the case $k=1$. There must be some way to do this. I hope you can help me. Thank you very much.",,"['multivariable-calculus', 'derivatives', 'multilinear-algebra']"
1,What is the physical meaning behind the surface integral,What is the physical meaning behind the surface integral,,"For example, I know that the physical meaning behind a standard, single integral is the area under the curve (with respect to the x or y axes).  Likewise, the a line integral can be physically visualized as a ""wall"" with the base of the wall bordering along the line and the top bordering the surface of interest--the line integral is the area of that wall. A double integral is the volume under the surface of interest (with respect to the xy/xz/yz plane). What is the surface integral then? If the surface integral is the 3d analog of the line integral, is it then the volume under one surface with respect to another surface, instead of the xy/xz/yz plane? If anybody could help me physically visualize the surface integral, I would be extremely grateful!!","For example, I know that the physical meaning behind a standard, single integral is the area under the curve (with respect to the x or y axes).  Likewise, the a line integral can be physically visualized as a ""wall"" with the base of the wall bordering along the line and the top bordering the surface of interest--the line integral is the area of that wall. A double integral is the volume under the surface of interest (with respect to the xy/xz/yz plane). What is the surface integral then? If the surface integral is the 3d analog of the line integral, is it then the volume under one surface with respect to another surface, instead of the xy/xz/yz plane? If anybody could help me physically visualize the surface integral, I would be extremely grateful!!",,"['multivariable-calculus', 'intuition', 'surface-integrals']"
2,Chain Rule in Multivariable Calculus made easy,Chain Rule in Multivariable Calculus made easy,,"I'm learning Chain Rule in Multivariable Calculus through James Stewart's book. Everything seemed great in the beginning, until I stumbled upon an example that requires using the Chain Rule twice (second-order derivative). It follows: From the ""But, using the Chain Rule again"" forward I just can't grasp what he has done (I'm looking at it for the past two days, I have absolutely no clue, even though I'm familiar with the Chain Rule concept). Can someone explain that part?","I'm learning Chain Rule in Multivariable Calculus through James Stewart's book. Everything seemed great in the beginning, until I stumbled upon an example that requires using the Chain Rule twice (second-order derivative). It follows: From the ""But, using the Chain Rule again"" forward I just can't grasp what he has done (I'm looking at it for the past two days, I have absolutely no clue, even though I'm familiar with the Chain Rule concept). Can someone explain that part?",,"['multivariable-calculus', 'chain-rule']"
3,Degree two homogeneous differentiable function is a quadratic form.,Degree two homogeneous differentiable function is a quadratic form.,,"Let $f: \Bbb R^n \to \Bbb R^k$ be a ${\cal C}^2$ function such that $f(tx) = t^2f(x)$ for all $t \in \Bbb R$ and all $x \in \Bbb R^n$. Then there is a bilinear map $B: \Bbb R^n \times \Bbb R^n\to \Bbb R^k$ such that $f(x) = B(x,x)$, for all $x \in \Bbb R^n$. Here's some progress. Using some reverse engineering, we can say what $B$ must be. If $\Delta$ is the diagonal inclusion, differentiating $f = B \circ \Delta$ at $x$ and evaluating at $y$ gives $B(x,y) = \frac{1}{2}{\rm d}f(x)(y)$. Then we're only left with proving that $B$ defined by that expression is linear in the first variable. We know that the maps $$\begin{matrix} \Bbb R \times \Bbb R^n & \stackrel{q \times f}{\longrightarrow} & \Bbb R \times \Bbb R^k &  \stackrel{m}{\longrightarrow} & \Bbb R^k \\ (t,x) & \mapsto & (t^2,f(x)) & \mapsto & t^2f(x)\end{matrix} \quad \text{ and }\quad \begin{matrix} \Bbb R \times \Bbb R^n & \stackrel{m}{\longrightarrow} & \Bbb R^n  & \stackrel{f}{\longrightarrow} & \Bbb R^k \\ (t,x) & \mapsto & tx & \mapsto & f(tx)\end{matrix}  $$are equal (yes, I'm abusing notation using $m$ twice). Differentiating $m \circ (q \times f) = f \circ m$ at $(t,x)$ gives: $${\rm d}m(t^2,f(x))\circ ({\rm d}q(t)\times {\rm d}f(x)) = {\rm d}f(tx)\circ {\rm d}m(t,x).$$Applying both sides on some $(s,y)$ gives (after a couple of steps): $$t^2 {\rm d}f(x)(y) + 2ts f(x) = t {\rm d}f(tx)(y) + s {\rm d}f(tx)(x).$$Making $s = 0$ we obtaing $t{\rm d}f(x) = {\rm d}f(tx)$ for all $t \neq 0$. Since ${\rm d}f(0) = 0$ (easy to check), we actually have ${\rm d}f(tx) = t{\rm d}f(x)$ for all real $t$. I don't know how to prove that ${\rm d}f(x_1+x_2) = {\rm d}f(x_1)+{\rm d}f(x_2)$; I don't know where does $f$ being ${\cal C}^2$ comes in. So far it seems that differentiability is enough. Help?","Let $f: \Bbb R^n \to \Bbb R^k$ be a ${\cal C}^2$ function such that $f(tx) = t^2f(x)$ for all $t \in \Bbb R$ and all $x \in \Bbb R^n$. Then there is a bilinear map $B: \Bbb R^n \times \Bbb R^n\to \Bbb R^k$ such that $f(x) = B(x,x)$, for all $x \in \Bbb R^n$. Here's some progress. Using some reverse engineering, we can say what $B$ must be. If $\Delta$ is the diagonal inclusion, differentiating $f = B \circ \Delta$ at $x$ and evaluating at $y$ gives $B(x,y) = \frac{1}{2}{\rm d}f(x)(y)$. Then we're only left with proving that $B$ defined by that expression is linear in the first variable. We know that the maps $$\begin{matrix} \Bbb R \times \Bbb R^n & \stackrel{q \times f}{\longrightarrow} & \Bbb R \times \Bbb R^k &  \stackrel{m}{\longrightarrow} & \Bbb R^k \\ (t,x) & \mapsto & (t^2,f(x)) & \mapsto & t^2f(x)\end{matrix} \quad \text{ and }\quad \begin{matrix} \Bbb R \times \Bbb R^n & \stackrel{m}{\longrightarrow} & \Bbb R^n  & \stackrel{f}{\longrightarrow} & \Bbb R^k \\ (t,x) & \mapsto & tx & \mapsto & f(tx)\end{matrix}  $$are equal (yes, I'm abusing notation using $m$ twice). Differentiating $m \circ (q \times f) = f \circ m$ at $(t,x)$ gives: $${\rm d}m(t^2,f(x))\circ ({\rm d}q(t)\times {\rm d}f(x)) = {\rm d}f(tx)\circ {\rm d}m(t,x).$$Applying both sides on some $(s,y)$ gives (after a couple of steps): $$t^2 {\rm d}f(x)(y) + 2ts f(x) = t {\rm d}f(tx)(y) + s {\rm d}f(tx)(x).$$Making $s = 0$ we obtaing $t{\rm d}f(x) = {\rm d}f(tx)$ for all $t \neq 0$. Since ${\rm d}f(0) = 0$ (easy to check), we actually have ${\rm d}f(tx) = t{\rm d}f(x)$ for all real $t$. I don't know how to prove that ${\rm d}f(x_1+x_2) = {\rm d}f(x_1)+{\rm d}f(x_2)$; I don't know where does $f$ being ${\cal C}^2$ comes in. So far it seems that differentiability is enough. Help?",,"['real-analysis', 'multivariable-calculus', 'derivatives']"
4,Continuity of a 2 variable function - Munkres exercise,Continuity of a 2 variable function - Munkres exercise,,"Let $f:\mathbb{R}^2\rightarrow \mathbb{R}$ be defined as $f(0,0)=0$  and $f(x,y)=\frac{xy(x^2-y^2)}{x^2+y^2}$ for $(x,y)\neq (0,0)$ Then question asks to prove that $f$ is differentiable. Hint that is given is : Show that $D_1f$ equals product of $y$ and a bounded function and $D_2f$  equals product of $x$ and a bounded function. I calculated $D_1f=y\frac{x^4+4x^2y^2-y^4}{(x^2+y^2)^2}$ and clearly $\left|\frac{x^4+4x^2y^2-y^4}{(x^2+y^2)^2}\right|\leq 3$ So, we have $D_1f$ as product of $y$ and a bounded function.. Similarly $D_2f$ is product of $x$ with a bounded function.. I know that if $D_1f$ and $D_2f$ are bounded then $f$ is continuous.. But here it is product of a bounded function with $y$.. I do not know how to proceed.. Please help me... P.S : I am supposed to prove that it is differentiable. I think i should use condition that if partial derivatives are continuous then $f$ is differentiable...","Let $f:\mathbb{R}^2\rightarrow \mathbb{R}$ be defined as $f(0,0)=0$  and $f(x,y)=\frac{xy(x^2-y^2)}{x^2+y^2}$ for $(x,y)\neq (0,0)$ Then question asks to prove that $f$ is differentiable. Hint that is given is : Show that $D_1f$ equals product of $y$ and a bounded function and $D_2f$  equals product of $x$ and a bounded function. I calculated $D_1f=y\frac{x^4+4x^2y^2-y^4}{(x^2+y^2)^2}$ and clearly $\left|\frac{x^4+4x^2y^2-y^4}{(x^2+y^2)^2}\right|\leq 3$ So, we have $D_1f$ as product of $y$ and a bounded function.. Similarly $D_2f$ is product of $x$ with a bounded function.. I know that if $D_1f$ and $D_2f$ are bounded then $f$ is continuous.. But here it is product of a bounded function with $y$.. I do not know how to proceed.. Please help me... P.S : I am supposed to prove that it is differentiable. I think i should use condition that if partial derivatives are continuous then $f$ is differentiable...",,"['multivariable-calculus', 'derivatives', 'partial-derivative']"
5,Spivak Calculus on Manifolds - Theorem 4-10,Spivak Calculus on Manifolds - Theorem 4-10,,"Part (4) of Theorem 4-10 in Spivak's Calculus on Manifolds says the following: If $\omega$ is a $k$-form on $\mathbb{R}^m$ and $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ is differentiable, then $f^{*}(d \omega) = d(f^{*}\omega)$. In the Proof, Spivak says that it is clear if $\omega$ is a $0$-form. I tried expanding both sides using the definitions, but I'm not getting the desired result even after a lot of effort. I suppose I'm only missing something straightforward. Could anyone please help me out? Thanks in advance. Edit: Some of what I attempted is as follows: \begin{align} &\ f^{*} d\omega (p) (v_p)\\ =&\ f^{*} \bigl(d\omega (f(p))\bigr)(v_p)\\ =&\ d\omega \bigl(f(p)\bigr) (f_{*}v_p)\\ =&\ d\omega \bigl(f(p)\bigr) \bigl(Df(p)(v)\bigr)_{f(p)}\\ =&\ D\omega (f(p))(Df(p)(v)) \end{align} I also tried writing $d\omega$ as $\sum_{i=1}^n \omega_i dx^i$, so that \begin{align} f^{*} d\omega &= f^{*}\left(\sum_{i=1}^n \omega_i dx^i\right)\\ &= \sum_{i=i}^n f^{*}(\omega_i dx^i)\\ &= \sum_{i=1}^n \omega_i \circ f \cdot f^* (dx^i)\\ &= \sum_{i=1}^n \omega_i \circ f \cdot \sum_{j=1}^n D_j f^i \cdot dx^j \end{align} Then, $d(f^*\omega)(p)(v_p) = D(f^*\omega)(p)(v)$, but I don't know how to connect this with the last line. As I understand, a $0$-form is just a function from $\mathbb{R}^n$ to $\mathbb{R}$. The operator $d$ takes a $k$-form and converts it into a $k+1$-form.","Part (4) of Theorem 4-10 in Spivak's Calculus on Manifolds says the following: If $\omega$ is a $k$-form on $\mathbb{R}^m$ and $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ is differentiable, then $f^{*}(d \omega) = d(f^{*}\omega)$. In the Proof, Spivak says that it is clear if $\omega$ is a $0$-form. I tried expanding both sides using the definitions, but I'm not getting the desired result even after a lot of effort. I suppose I'm only missing something straightforward. Could anyone please help me out? Thanks in advance. Edit: Some of what I attempted is as follows: \begin{align} &\ f^{*} d\omega (p) (v_p)\\ =&\ f^{*} \bigl(d\omega (f(p))\bigr)(v_p)\\ =&\ d\omega \bigl(f(p)\bigr) (f_{*}v_p)\\ =&\ d\omega \bigl(f(p)\bigr) \bigl(Df(p)(v)\bigr)_{f(p)}\\ =&\ D\omega (f(p))(Df(p)(v)) \end{align} I also tried writing $d\omega$ as $\sum_{i=1}^n \omega_i dx^i$, so that \begin{align} f^{*} d\omega &= f^{*}\left(\sum_{i=1}^n \omega_i dx^i\right)\\ &= \sum_{i=i}^n f^{*}(\omega_i dx^i)\\ &= \sum_{i=1}^n \omega_i \circ f \cdot f^* (dx^i)\\ &= \sum_{i=1}^n \omega_i \circ f \cdot \sum_{j=1}^n D_j f^i \cdot dx^j \end{align} Then, $d(f^*\omega)(p)(v_p) = D(f^*\omega)(p)(v)$, but I don't know how to connect this with the last line. As I understand, a $0$-form is just a function from $\mathbb{R}^n$ to $\mathbb{R}$. The operator $d$ takes a $k$-form and converts it into a $k+1$-form.",,['multivariable-calculus']
6,What is the derivation of the derivative of softmax regression (or multinomial logistic regression)?,What is the derivation of the derivative of softmax regression (or multinomial logistic regression)?,,"Consider the training cost for softmax regression (I will use the term multinomial logistic regression): $$ J( \theta ) = - \sum^m_{i=1} \sum^K_{k=1} 1 \{ y^{(i)} = k \} \log p(y^{(i)} =  k \mid x^{(i)} ; \theta) $$ according to the UFLDL tutorial the derivative of the above function is: $$ \bigtriangledown_{ \theta^{(k)} }J( \theta ) = -\sum^{m}_{i=1} [x^{(i)} (1 \{ y^{(i)} = k \} - p(y^{(i)} =  k \mid x^{(i)} ; \theta) ) ] $$ however, they didn't include the derivation. Does someone know what the derivation is? I have tried taking the derivative of it but even my initial steps seems to disagree with the final form they have. So I first took the gradient $\bigtriangledown_{ \theta^{(k)} }J( \theta )$ as they suggested: $$ \bigtriangledown_{ \theta^{(k)} } J( \theta ) =  - \bigtriangledown_{ \theta^{(k)} } \sum^m_{i=1} \sum^K_{k=1} 1 \{ y^{(i)} = k \} \log p(y^{(i)} =  k \mid x^{(i)} ; \theta) $$ but since we are taking the gradient with respect to $\theta^{(k)}$, only the term that matches this specific k will be non-zero when we taking derivatives. Hence: $$ \bigtriangledown_{ \theta^{(k)} } J( \theta ) =  -  \sum^m_{i=1} \bigtriangledown_{ \theta^{(k)} } \log p(y^{(i)} =  k \mid x^{(i)} ; \theta) $$ then if we proceed we get: $$  -  \sum^m_{i=1} \frac{1}{p(y^{(i)} =  k \mid x^{(i)} ; \theta)} \bigtriangledown_{ \theta^{(k)} } p(y^{(i)} =  k \mid x^{(i)} ; \theta) $$ however, at this point the equation looks so different from what the UDFL tutorial has plus the indicator function disappeared completely, that it makes me suspect that I probably made a mistake somewhere. On top of that it seems that the final derivative has difference, but I don't see any differences/subtractions on my derivation. I suspect a difference might come in when expressing the Quotient rule but the indicator function disappearing still worries me. Any ideas?","Consider the training cost for softmax regression (I will use the term multinomial logistic regression): $$ J( \theta ) = - \sum^m_{i=1} \sum^K_{k=1} 1 \{ y^{(i)} = k \} \log p(y^{(i)} =  k \mid x^{(i)} ; \theta) $$ according to the UFLDL tutorial the derivative of the above function is: $$ \bigtriangledown_{ \theta^{(k)} }J( \theta ) = -\sum^{m}_{i=1} [x^{(i)} (1 \{ y^{(i)} = k \} - p(y^{(i)} =  k \mid x^{(i)} ; \theta) ) ] $$ however, they didn't include the derivation. Does someone know what the derivation is? I have tried taking the derivative of it but even my initial steps seems to disagree with the final form they have. So I first took the gradient $\bigtriangledown_{ \theta^{(k)} }J( \theta )$ as they suggested: $$ \bigtriangledown_{ \theta^{(k)} } J( \theta ) =  - \bigtriangledown_{ \theta^{(k)} } \sum^m_{i=1} \sum^K_{k=1} 1 \{ y^{(i)} = k \} \log p(y^{(i)} =  k \mid x^{(i)} ; \theta) $$ but since we are taking the gradient with respect to $\theta^{(k)}$, only the term that matches this specific k will be non-zero when we taking derivatives. Hence: $$ \bigtriangledown_{ \theta^{(k)} } J( \theta ) =  -  \sum^m_{i=1} \bigtriangledown_{ \theta^{(k)} } \log p(y^{(i)} =  k \mid x^{(i)} ; \theta) $$ then if we proceed we get: $$  -  \sum^m_{i=1} \frac{1}{p(y^{(i)} =  k \mid x^{(i)} ; \theta)} \bigtriangledown_{ \theta^{(k)} } p(y^{(i)} =  k \mid x^{(i)} ; \theta) $$ however, at this point the equation looks so different from what the UDFL tutorial has plus the indicator function disappeared completely, that it makes me suspect that I probably made a mistake somewhere. On top of that it seems that the final derivative has difference, but I don't see any differences/subtractions on my derivation. I suspect a difference might come in when expressing the Quotient rule but the indicator function disappearing still worries me. Any ideas?",,"['multivariable-calculus', 'optimization', 'machine-learning']"
7,Perturb a piecewise-linear path to make it $C^\infty$,Perturb a piecewise-linear path to make it,C^\infty,"I'm trying to prove that any two points on a path connected smooth manifold can be joined by a smooth path. It becomes easy if I can prove the following: Given a curve $\gamma :\mathbb{R} \to \mathbb{R}^n$ that is   $C^\infty$ at each point except at $t=0$ (and which has nonvanishing first derivative), and given a neighborhood $N$   of $\gamma(0)$ in $\mathbb{R}^n$, there is a $C^\infty$ curve $\tilde{\gamma}:\mathbb{R} \to \mathbb{R}^n$ that agrees   with $\gamma$ outside $\gamma^{-1}(N)$ (and has nonvanishing first derivative). (I assume this is true. If there is some counter-example, we could make the domain of the curve compact if that would help.) When referring to the fact that you can make a small pertubation of a path to make it smooth, some people have waved hands and said ""you do it with bump functions"". I am familiar with bump functions, but I don't know how to use one to accomplish this. I was thinking convolution with an approximate identity, but that seems to change the function everywhere, not just on a given neighborhood. Could someone give me a thorough explanation (or a reference for one)?","I'm trying to prove that any two points on a path connected smooth manifold can be joined by a smooth path. It becomes easy if I can prove the following: Given a curve $\gamma :\mathbb{R} \to \mathbb{R}^n$ that is   $C^\infty$ at each point except at $t=0$ (and which has nonvanishing first derivative), and given a neighborhood $N$   of $\gamma(0)$ in $\mathbb{R}^n$, there is a $C^\infty$ curve $\tilde{\gamma}:\mathbb{R} \to \mathbb{R}^n$ that agrees   with $\gamma$ outside $\gamma^{-1}(N)$ (and has nonvanishing first derivative). (I assume this is true. If there is some counter-example, we could make the domain of the curve compact if that would help.) When referring to the fact that you can make a small pertubation of a path to make it smooth, some people have waved hands and said ""you do it with bump functions"". I am familiar with bump functions, but I don't know how to use one to accomplish this. I was thinking convolution with an approximate identity, but that seems to change the function everywhere, not just on a given neighborhood. Could someone give me a thorough explanation (or a reference for one)?",,"['real-analysis', 'multivariable-calculus', 'differential-topology']"
8,Apparent discrepancy between change of variables in one versus multiple dimensions.,Apparent discrepancy between change of variables in one versus multiple dimensions.,,"My freshman calculus book gives the change of variables formula in one dimension and then eight chapters later gives it in $n$ dimensions.  But when it generalizes to $n$ dimensions it requires the transformation between domains be invertible.  But in one dimension there's no such restriction. So my question is, since it's not necessary for one dimension why is it necessary for higher dimensions?  And if it's not necessary, what is the general statement? Thanks in advance for any insights.","My freshman calculus book gives the change of variables formula in one dimension and then eight chapters later gives it in $n$ dimensions.  But when it generalizes to $n$ dimensions it requires the transformation between domains be invertible.  But in one dimension there's no such restriction. So my question is, since it's not necessary for one dimension why is it necessary for higher dimensions?  And if it's not necessary, what is the general statement? Thanks in advance for any insights.",,"['calculus', 'integration', 'multivariable-calculus']"
9,Trigonometric parametrization of a genus g surface?,Trigonometric parametrization of a genus g surface?,,"It is possible to find functions $\phi, \psi \in \mathbb{R}[sin(x), sin(y), cos(x), cos(y)]$, so that $S^2 = \phi( [0,1]^2)$ and $\psi( [0,1]^2)$ is a torus. Is it possible find, for any genus g, functions $f_g \in \mathbb{R}[sin(x), sin(y), cos(x), cos(y)]$ so that $f_g([0,1])^2$ is $S_g$, the surface of genus $g$? Since the torus and the sphere are surfaces of revolution, and the other genus g surfaces are not (is this true?), I expect that perhaps this cannot be done. I do not know how to make this intuition precise. If there is no trigonometric polynomial parametrization as above, then what is the correct generalization of these parametrizations? So the most modest form of my question is: How can I produce parametrized genus g surfaces in $R^3$, for arbitrary $g$?","It is possible to find functions $\phi, \psi \in \mathbb{R}[sin(x), sin(y), cos(x), cos(y)]$, so that $S^2 = \phi( [0,1]^2)$ and $\psi( [0,1]^2)$ is a torus. Is it possible find, for any genus g, functions $f_g \in \mathbb{R}[sin(x), sin(y), cos(x), cos(y)]$ so that $f_g([0,1])^2$ is $S_g$, the surface of genus $g$? Since the torus and the sphere are surfaces of revolution, and the other genus g surfaces are not (is this true?), I expect that perhaps this cannot be done. I do not know how to make this intuition precise. If there is no trigonometric polynomial parametrization as above, then what is the correct generalization of these parametrizations? So the most modest form of my question is: How can I produce parametrized genus g surfaces in $R^3$, for arbitrary $g$?",,"['multivariable-calculus', 'differential-geometry', 'differential-topology', 'surfaces']"
10,Calculation of $A'(0)$ (first variation of the area functional).,Calculation of  (first variation of the area functional).,A'(0),"I'm trying to do the calculation that shows that a surface in $\Bbb R^3$ is area minimizing if and only if the mean curvature is zero. I'm getting a sign wrong and I'm going crazy, I need help. Notations: Fix a domain $D$. Here ${\bf x}$ is a parametrization, ${\bf x}^t = {\bf x}+t{\bf V}$ is a variation, with ${\bf V}$ being zero on $\partial D$, $\bf N$ is the normal unit vector and $A(t)$ is the area of ${\bf x}^t$. So far, I have $$A'(0) = \iint_D \langle {\bf N}, {\bf x}_u \times {\bf V}_v + {\bf V}_u \times {\bf x}_v \rangle \,{\rm d}u\,{\rm d}v,$$ and I'm positive that so far, so good. Then, the trick seems to use Green-Stokes with $P = \langle {\bf N}, {\bf V}\times {\bf x}_u\rangle$ and $Q =  \langle {\bf N}, {\bf V}\times {\bf x}_v\rangle$. $$\begin{align} \frac{\partial Q}{\partial u} - \frac{\partial P}{\partial v}  &= \langle {\bf N}_u, {\bf V}\times {\bf x}_v\rangle+\langle {\bf N}, {\bf V}_u\times {\bf x}_v\rangle+\langle {\bf N}, {\bf V}\times {\bf x}_{uv}\rangle \\ &\qquad -\langle {\bf N}_v, {\bf V}\times {\bf x}_u\rangle-\langle {\bf N}, {\bf V}_v\times {\bf x}_u\rangle-\langle {\bf N}, {\bf V}\times {\bf x}_{uv} \rangle \\ &= \langle {\bf N}_u, {\bf V}\times {\bf x}_v\rangle-\langle {\bf N}_v, {\bf V}\times {\bf x}_u\rangle + \langle {\bf N}, {\bf x}_u \times {\bf V}_v + {\bf V}_u \times {\bf x}_v \rangle\end{align}$$ Now let's look only at: $$\begin{align} \langle {\bf N}_u, {\bf V}\times {\bf x}_v\rangle-\langle {\bf N}_v, {\bf V}\times {\bf x}_u\rangle &= \langle {\bf V}\times{\bf x}_v, {\bf N}_u\rangle  - \langle {\bf V}\times{\bf x}_u,{\bf N}_v\rangle \\ &=  \langle {\bf V},{\bf x}_v\times {\bf N}_u\rangle  - \langle {\bf V},{\bf x}_u\times{\bf N}_v\rangle  \\ &= \langle {\bf V}, {\bf N}_v\times{\bf x}_u+{\bf x}_v\times{\bf N}_u\rangle \\ &= -2H\langle {\bf V},{\bf x}_u\times{\bf x}_v\rangle,\end{align}$$ using that $${\cal S}{\bf v}\times{\bf w}+{\bf v}\times{\cal S}{\bf w} = 2H {\bf v}\times{\bf w},$$ where ${\cal S}$ stands for the shape operator, with ${\bf v} = {\bf x}_v$ and ${\bf w}={\bf x}_u$. The line integral that appears after using Green-Stokes is zero because $\bf V$ is zero in $\partial D$ and we get: $$A'(0) = 2\iint_D H \langle {\bf V},{\bf N}\rangle \,{\rm d}A.$$ But all books say that $$A'(0) = \color{red}{-}2\iint_D H \langle {\bf V},{\bf N}\rangle \,{\rm d}A.$$ Where is the mistake?","I'm trying to do the calculation that shows that a surface in $\Bbb R^3$ is area minimizing if and only if the mean curvature is zero. I'm getting a sign wrong and I'm going crazy, I need help. Notations: Fix a domain $D$. Here ${\bf x}$ is a parametrization, ${\bf x}^t = {\bf x}+t{\bf V}$ is a variation, with ${\bf V}$ being zero on $\partial D$, $\bf N$ is the normal unit vector and $A(t)$ is the area of ${\bf x}^t$. So far, I have $$A'(0) = \iint_D \langle {\bf N}, {\bf x}_u \times {\bf V}_v + {\bf V}_u \times {\bf x}_v \rangle \,{\rm d}u\,{\rm d}v,$$ and I'm positive that so far, so good. Then, the trick seems to use Green-Stokes with $P = \langle {\bf N}, {\bf V}\times {\bf x}_u\rangle$ and $Q =  \langle {\bf N}, {\bf V}\times {\bf x}_v\rangle$. $$\begin{align} \frac{\partial Q}{\partial u} - \frac{\partial P}{\partial v}  &= \langle {\bf N}_u, {\bf V}\times {\bf x}_v\rangle+\langle {\bf N}, {\bf V}_u\times {\bf x}_v\rangle+\langle {\bf N}, {\bf V}\times {\bf x}_{uv}\rangle \\ &\qquad -\langle {\bf N}_v, {\bf V}\times {\bf x}_u\rangle-\langle {\bf N}, {\bf V}_v\times {\bf x}_u\rangle-\langle {\bf N}, {\bf V}\times {\bf x}_{uv} \rangle \\ &= \langle {\bf N}_u, {\bf V}\times {\bf x}_v\rangle-\langle {\bf N}_v, {\bf V}\times {\bf x}_u\rangle + \langle {\bf N}, {\bf x}_u \times {\bf V}_v + {\bf V}_u \times {\bf x}_v \rangle\end{align}$$ Now let's look only at: $$\begin{align} \langle {\bf N}_u, {\bf V}\times {\bf x}_v\rangle-\langle {\bf N}_v, {\bf V}\times {\bf x}_u\rangle &= \langle {\bf V}\times{\bf x}_v, {\bf N}_u\rangle  - \langle {\bf V}\times{\bf x}_u,{\bf N}_v\rangle \\ &=  \langle {\bf V},{\bf x}_v\times {\bf N}_u\rangle  - \langle {\bf V},{\bf x}_u\times{\bf N}_v\rangle  \\ &= \langle {\bf V}, {\bf N}_v\times{\bf x}_u+{\bf x}_v\times{\bf N}_u\rangle \\ &= -2H\langle {\bf V},{\bf x}_u\times{\bf x}_v\rangle,\end{align}$$ using that $${\cal S}{\bf v}\times{\bf w}+{\bf v}\times{\cal S}{\bf w} = 2H {\bf v}\times{\bf w},$$ where ${\cal S}$ stands for the shape operator, with ${\bf v} = {\bf x}_v$ and ${\bf w}={\bf x}_u$. The line integral that appears after using Green-Stokes is zero because $\bf V$ is zero in $\partial D$ and we get: $$A'(0) = 2\iint_D H \langle {\bf V},{\bf N}\rangle \,{\rm d}A.$$ But all books say that $$A'(0) = \color{red}{-}2\iint_D H \langle {\bf V},{\bf N}\rangle \,{\rm d}A.$$ Where is the mistake?",,"['multivariable-calculus', 'differential-geometry']"
11,Why Isn't the $ {L}_{2} $ Norm Differentiable at $ x = 0 $?,Why Isn't the  Norm Differentiable at ?, {L}_{2}   x = 0 ,Why doesn't the $L_2$ norm differentiable at $x=0$? Let's define $N(x)$ as the norm function. I know that for every $x\ne 0$: $$\frac{\partial N}{\partial x_i}(x) = \frac{x_i}{\|x\|}$$ What happens at the origin? I'd be glad to get an explanation involving minimal linear algebra :) Thanks in advance.,Why doesn't the $L_2$ norm differentiable at $x=0$? Let's define $N(x)$ as the norm function. I know that for every $x\ne 0$: $$\frac{\partial N}{\partial x_i}(x) = \frac{x_i}{\|x\|}$$ What happens at the origin? I'd be glad to get an explanation involving minimal linear algebra :) Thanks in advance.,,"['calculus', 'linear-algebra', 'multivariable-calculus', 'derivatives', 'normed-spaces']"
12,Non-gradient vector field in $\mathbb{R}^3 - \lbrace\mathbf{0}\rbrace$ with zero curl,Non-gradient vector field in  with zero curl,\mathbb{R}^3 - \lbrace\mathbf{0}\rbrace,"I'm self-studying multi-variable calculus using MIT's publicly available materials.  One of the practice questions for the final exam asks that I determine the truth or falsity of the following statement: Let $\mathbf{F}$ be a vector field on $\mathbb{R}^3 - \lbrace \mathbf{0}\rbrace$ with $\nabla \times \mathbf{F} = 0$.  Then there exists a scalar field on $\mathbb{R}^3 - \lbrace \mathbf{0}\rbrace$ such that $\mathbf{F} = \nabla f$. Now, as I understand it, this assertion should be false, because $\nabla \times \mathbf{F} = 0$ only implies that $\mathbf{F}$ is conservative on a region if the region in question is star convex, which $\mathbf{R}^3 - \lbrace \mathbf{0} \rbrace$ is not (unless I really don't understand the notion of star convexity).  However, to do the exercise well, I need a counterexample, and here I'm having problems. I am of course familiar with the canonical counterexample in $\mathbf{R}^2$ (and, by extension, in $\mathbf{R}^3$ less the $z$-axis), i.e. $\mathbf{F} = \left(-\frac{y}{x^2 + y^2}, \frac{x}{x^2 + y^2}, 0\right)$.  I'm having difficulty extending this to $\mathbf{R}^3 - \lbrace \mathbf{0} \rbrace$, however -- although poking around here and on the Internet generally has led me to suspect that any counterexample is going to be analogous.  (The phrase 'de Rham cohomology' keeps popping up....) At any rate, I would appreciate a gentle hint.  Am I on the right track?  Have I missed something fundamental?","I'm self-studying multi-variable calculus using MIT's publicly available materials.  One of the practice questions for the final exam asks that I determine the truth or falsity of the following statement: Let $\mathbf{F}$ be a vector field on $\mathbb{R}^3 - \lbrace \mathbf{0}\rbrace$ with $\nabla \times \mathbf{F} = 0$.  Then there exists a scalar field on $\mathbb{R}^3 - \lbrace \mathbf{0}\rbrace$ such that $\mathbf{F} = \nabla f$. Now, as I understand it, this assertion should be false, because $\nabla \times \mathbf{F} = 0$ only implies that $\mathbf{F}$ is conservative on a region if the region in question is star convex, which $\mathbf{R}^3 - \lbrace \mathbf{0} \rbrace$ is not (unless I really don't understand the notion of star convexity).  However, to do the exercise well, I need a counterexample, and here I'm having problems. I am of course familiar with the canonical counterexample in $\mathbf{R}^2$ (and, by extension, in $\mathbf{R}^3$ less the $z$-axis), i.e. $\mathbf{F} = \left(-\frac{y}{x^2 + y^2}, \frac{x}{x^2 + y^2}, 0\right)$.  I'm having difficulty extending this to $\mathbf{R}^3 - \lbrace \mathbf{0} \rbrace$, however -- although poking around here and on the Internet generally has led me to suspect that any counterexample is going to be analogous.  (The phrase 'de Rham cohomology' keeps popping up....) At any rate, I would appreciate a gentle hint.  Am I on the right track?  Have I missed something fundamental?",,['multivariable-calculus']
13,Example of non-differentiable continuous function with all partial derivatives well defined,Example of non-differentiable continuous function with all partial derivatives well defined,,"Give an example of a function $f : \mathbb{R}^3 \to \mathbb{R}$ such that the partial derivatives exist at $(0,0,0)$, and $f$ is continuous at $(0,0,0)$, but it is not differentiable at $(0,0,0)$. Any hint?","Give an example of a function $f : \mathbb{R}^3 \to \mathbb{R}$ such that the partial derivatives exist at $(0,0,0)$, and $f$ is continuous at $(0,0,0)$, but it is not differentiable at $(0,0,0)$. Any hint?",,"['multivariable-calculus', 'derivatives', 'continuity', 'examples-counterexamples', 'partial-derivative']"
14,"When does a double integral represent a surface area, and when does it represent a volume?","When does a double integral represent a surface area, and when does it represent a volume?",,"When does $\int_Af(x,y)dA$ represent a surface area geometrically, and when does it represent a volume? In my lecture notes I'm told it represent the volume underneath the surface $z=f(x,y)$, but I've found examples online computing double integrals to find the surface area of a surface. Thanks a bundle","When does $\int_Af(x,y)dA$ represent a surface area geometrically, and when does it represent a volume? In my lecture notes I'm told it represent the volume underneath the surface $z=f(x,y)$, but I've found examples online computing double integrals to find the surface area of a surface. Thanks a bundle",,"['integration', 'multivariable-calculus']"
15,Affine Functions as Equality Constraints in Convex Optimization Problems,Affine Functions as Equality Constraints in Convex Optimization Problems,,"I am studying an introduction to convex optimization. When defining a convex optimization problem, we have a convex objective function, $f(x)$, a set of convex functions $g_i(x)$ where the feasible region is the intersection of their $0$-sublevel sets, $G_i=\{x \mid g_i(x) \leq 0\}$. We can have equality constraints as well, which are defined using affine functions $h_i$, where the constrained sets are $H_i=\{x \mid h_i(x)=0\}$. So the final feasible region for that problem should be $G_1 \cap G_2 \cap \dots \cap G_n \cap H_1 \cap \dots \cap H_m$. For $n$ inequality and $m$ equality constraints. I have two questions about this definition of the convex optimization problem. As far as I know, in very general, an affine function is defined as $h(x) = Ax + b$, where $A$ is a $m \times n$ matrix and $x \in \mathbb{R}^n$. But since here we deal with scalar valued functions, due to the nature of the optimization problem, I assume that $h(x)$ functions should have the form of $h(x) = w^Tx + b$ where $b$ is just a scalar. Is this true? I have the following understanding about introducing the affine equality constraints into the problem. When the domain of the problem is $D$ dimensional, equality constraints introduce a $D-1$ dimensional surface limitation into the feasible region. This is a line in a two dimensional problem, a plane in a three dimensional problem and a hyperplane for higher dimensions, due to the form of the constraint $h(x) = w^Tx + b = 0$, assuming my first question is valid. Since the subset of $\mathbb{R}^D$ which belongs to a constraint must constitute a convex set, and a function of the form $h(x)=0$ defines a $D-1$ dimensional level surface in $\mathbb{R}^D$, the only way to make this surface convex is to define it as a hyperplane, which is what the affine function defines. Is this view about affine constraints correct? Thanks in advance.","I am studying an introduction to convex optimization. When defining a convex optimization problem, we have a convex objective function, $f(x)$, a set of convex functions $g_i(x)$ where the feasible region is the intersection of their $0$-sublevel sets, $G_i=\{x \mid g_i(x) \leq 0\}$. We can have equality constraints as well, which are defined using affine functions $h_i$, where the constrained sets are $H_i=\{x \mid h_i(x)=0\}$. So the final feasible region for that problem should be $G_1 \cap G_2 \cap \dots \cap G_n \cap H_1 \cap \dots \cap H_m$. For $n$ inequality and $m$ equality constraints. I have two questions about this definition of the convex optimization problem. As far as I know, in very general, an affine function is defined as $h(x) = Ax + b$, where $A$ is a $m \times n$ matrix and $x \in \mathbb{R}^n$. But since here we deal with scalar valued functions, due to the nature of the optimization problem, I assume that $h(x)$ functions should have the form of $h(x) = w^Tx + b$ where $b$ is just a scalar. Is this true? I have the following understanding about introducing the affine equality constraints into the problem. When the domain of the problem is $D$ dimensional, equality constraints introduce a $D-1$ dimensional surface limitation into the feasible region. This is a line in a two dimensional problem, a plane in a three dimensional problem and a hyperplane for higher dimensions, due to the form of the constraint $h(x) = w^Tx + b = 0$, assuming my first question is valid. Since the subset of $\mathbb{R}^D$ which belongs to a constraint must constitute a convex set, and a function of the form $h(x)=0$ defines a $D-1$ dimensional level surface in $\mathbb{R}^D$, the only way to make this surface convex is to define it as a hyperplane, which is what the affine function defines. Is this view about affine constraints correct? Thanks in advance.",,"['multivariable-calculus', 'optimization', 'convex-analysis', 'convex-optimization']"
16,Vector Calculus intuition: Why is the magnitude of a velocity vector the speed?,Vector Calculus intuition: Why is the magnitude of a velocity vector the speed?,,"From my understanding of basic Calculus (which could very well be completely flawed), the derivative of position with respect to time would give us the slope at every point of that function, which would be the speed at that point. In that case, there is no notion of magnitude of vector since the slope is just a measure of inclination of the tangent line at that point. I don't understand how the magnitude of a velocity vector in a vector-valued function gives us the speed at a point, the equivalent of the slope of a tangent line in a ""normal"" non-vector-valued function. Why does that make sense? Sorry if the functional-analysis tag is wrong, I wasn't sure but it seemed to fit the subject.","From my understanding of basic Calculus (which could very well be completely flawed), the derivative of position with respect to time would give us the slope at every point of that function, which would be the speed at that point. In that case, there is no notion of magnitude of vector since the slope is just a measure of inclination of the tangent line at that point. I don't understand how the magnitude of a velocity vector in a vector-valued function gives us the speed at a point, the equivalent of the slope of a tangent line in a ""normal"" non-vector-valued function. Why does that make sense? Sorry if the functional-analysis tag is wrong, I wasn't sure but it seemed to fit the subject.",,"['multivariable-calculus', 'vector-analysis']"
17,How to find that triple integral?,How to find that triple integral?,,"How to find the triple integral of $$ \frac{(z-z_0)z}{\sqrt{(x-x_0)^2+(y-y_0)^2+(z-z_0)^2}}$$ over the sphere $ \{(x,y,z):x^2+y^2+z^2 \le 1 \}  $ under the assumption $x_0^2+y_0^2+z_0^2 \le 1?$ Its physical interpretation suggests  the integral can be expressed through elementary functions of the parameters.","How to find the triple integral of $$ \frac{(z-z_0)z}{\sqrt{(x-x_0)^2+(y-y_0)^2+(z-z_0)^2}}$$ over the sphere $ \{(x,y,z):x^2+y^2+z^2 \le 1 \}  $ under the assumption $x_0^2+y_0^2+z_0^2 \le 1?$ Its physical interpretation suggests  the integral can be expressed through elementary functions of the parameters.",,['multivariable-calculus']
18,Math GRE: Calculus Textbooks - is Spivak + Stewart + Rudin sufficient?,Math GRE: Calculus Textbooks - is Spivak + Stewart + Rudin sufficient?,,"Recently, I splurged and spent $1000 in math textbooks in preparation for the Mathematics GRE subject test. So far, in terms of calculus books, I have purchased Spivak, Stewart, and Baby Rudin. Is this sufficient preparation for the Math GRE or do I need to get my hands on Apostol? FYI: I am extremely familiar with the univariate calculus and real analysis material. Multivariate, not so much.","Recently, I splurged and spent $1000 in math textbooks in preparation for the Mathematics GRE subject test. So far, in terms of calculus books, I have purchased Spivak, Stewart, and Baby Rudin. Is this sufficient preparation for the Math GRE or do I need to get my hands on Apostol? FYI: I am extremely familiar with the univariate calculus and real analysis material. Multivariate, not so much.",,"['multivariable-calculus', 'reference-request', 'gre-exam']"
19,Hadamard variational formula Evans chapter 6 problem 15,Hadamard variational formula Evans chapter 6 problem 15,,"This is Evans' chapter 6 problem 15. Consider a family of smooth, bounded domains $U(\tau) \subset \mathbb{R}^{n}$ that depend smoothly upon the parameter $\tau \in \mathbb{R}$. As $\tau$ changes, each point on $\partial U(\tau)$ moves with velocity $v$. For each $\tau$, we consider eigenvector $\lambda= \lambda(\tau)$ and corresponding eigenfunction $w = w(x,\tau) $ such that $-\Delta w = \lambda w$ in $U(\tau)$, $w=0$ on $\partial U$, normalized so that $||w||_{L^{2}(U(\tau))} = 1$. Suppose $\lambda, w$ are smooth. Prove Hadamard's variational formular $\partial_{\tau}\lambda = -\int_{\partial U(\tau)} \left|\frac{\partial w}{\partial \nu} \right|^{2} v \cdot \nu$ where $v \cdot \nu$ is normal velocity of $\partial U(\tau)$. Using hints on evans, I get $$\frac{\partial}{\partial\tau}\int_{U(\tau)}\lambda dx = \int_{\partial U}\lambda v \cdot \nu + \int_{U(\tau)}\lambda_{\tau}$$ But I don't know how to prove the rest. Could you give some idea?","This is Evans' chapter 6 problem 15. Consider a family of smooth, bounded domains $U(\tau) \subset \mathbb{R}^{n}$ that depend smoothly upon the parameter $\tau \in \mathbb{R}$. As $\tau$ changes, each point on $\partial U(\tau)$ moves with velocity $v$. For each $\tau$, we consider eigenvector $\lambda= \lambda(\tau)$ and corresponding eigenfunction $w = w(x,\tau) $ such that $-\Delta w = \lambda w$ in $U(\tau)$, $w=0$ on $\partial U$, normalized so that $||w||_{L^{2}(U(\tau))} = 1$. Suppose $\lambda, w$ are smooth. Prove Hadamard's variational formular $\partial_{\tau}\lambda = -\int_{\partial U(\tau)} \left|\frac{\partial w}{\partial \nu} \right|^{2} v \cdot \nu$ where $v \cdot \nu$ is normal velocity of $\partial U(\tau)$. Using hints on evans, I get $$\frac{\partial}{\partial\tau}\int_{U(\tau)}\lambda dx = \int_{\partial U}\lambda v \cdot \nu + \int_{U(\tau)}\lambda_{\tau}$$ But I don't know how to prove the rest. Could you give some idea?",,"['multivariable-calculus', 'partial-differential-equations']"
20,Difference between Vector Functions and Vector Field,Difference between Vector Functions and Vector Field,,I understand that a vector function is a function that has a domain $\mathbb{R}^n$ and range on $\mathbb{R}^m$ so it takes vectors and gives vectors right? So what is a vector field?And how can I visualize them?,I understand that a vector function is a function that has a domain $\mathbb{R}^n$ and range on $\mathbb{R}^m$ so it takes vectors and gives vectors right? So what is a vector field?And how can I visualize them?,,"['calculus', 'multivariable-calculus', 'vector-analysis', 'advice', 'vector-fields']"
21,"Calculating $\iint xy \,\mathrm{d}S$ where $S$ is the surface of the tetrahedron with sides $z=0$, $y = 0$, $x + z = 1$, $x = y$","Calculating  where  is the surface of the tetrahedron with sides , , ,","\iint xy \,\mathrm{d}S S z=0 y = 0 x + z = 1 x = y","Calculate $\iint xy \,\mathrm{d}S$ where $S$ is the surface of the tetrahedron with sides $z=0$, $y=0$, $x + z = 1$ and $x=y$. The answer is given as: $(3\sqrt{2}+5)/24$ \begin{align*}    &\, \iint xy \,\mathrm{d}S \\   =&\, \iint xy \sqrt{1 + (z_x)^2 + (z_y)^2} \,\mathrm{d}A \\   =&\, \int_{x=0}^1 \int_{y=0}^x xy \sqrt{1 + (-1)^2 + 0^2}        \,\mathrm{d}y \,\mathrm{d}x \\   =&\, \sqrt{2}        \int_{x=0}^1          \left[ \frac{xy^2}{2} \right]_{y=0}^x        \,\mathrm{d}x \\   =&\, \sqrt{2}          \int_{x=0}^1 \frac{x^3}{2}        \,\mathrm{d}x \\   =&\, \frac{\sqrt{2}}{8}. \end{align*}","Calculate $\iint xy \,\mathrm{d}S$ where $S$ is the surface of the tetrahedron with sides $z=0$, $y=0$, $x + z = 1$ and $x=y$. The answer is given as: $(3\sqrt{2}+5)/24$ \begin{align*}    &\, \iint xy \,\mathrm{d}S \\   =&\, \iint xy \sqrt{1 + (z_x)^2 + (z_y)^2} \,\mathrm{d}A \\   =&\, \int_{x=0}^1 \int_{y=0}^x xy \sqrt{1 + (-1)^2 + 0^2}        \,\mathrm{d}y \,\mathrm{d}x \\   =&\, \sqrt{2}        \int_{x=0}^1          \left[ \frac{xy^2}{2} \right]_{y=0}^x        \,\mathrm{d}x \\   =&\, \sqrt{2}          \int_{x=0}^1 \frac{x^3}{2}        \,\mathrm{d}x \\   =&\, \frac{\sqrt{2}}{8}. \end{align*}",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
22,Question on Curl F,Question on Curl F,,The problem in the book asks what the curl of $\operatorname{curl}\vec F(\vec r)= \frac {\vec r}{\|\vec r\|}$. Can someone give me a good explanation on why the curl will be zero? I would really appreciate it.,The problem in the book asks what the curl of $\operatorname{curl}\vec F(\vec r)= \frac {\vec r}{\|\vec r\|}$. Can someone give me a good explanation on why the curl will be zero? I would really appreciate it.,,['multivariable-calculus']
23,"Show that there is $(a,b)$ s.t. $f_x(a,b)^2+f_y(a,b)^2<4$",Show that there is  s.t.,"(a,b) f_x(a,b)^2+f_y(a,b)^2<4","Let $f:D\to\mathbb{R}$ where $D:=\{(x,y):x^2+y^2\leq 1\}$. Suppose all partial derivatives of $f$ are continuous and $|f|\leq 1$. Show that there exists $(a,b)\in Int_D$ s.t.: $$\left[\frac{\partial f}{\partial x}(a,b)\right]^2+\left[\frac{\partial f}{\partial y}(a,b)\right]^2<4$$ Here $Int_D$ means the interior of $D$, that is , the open unit disk. I try to reduce to the one dimension case, that is: there exists $a\in(-\sqrt{1-y^2},\sqrt{1-y^2})$ s.t $\left[\frac{\partial f}{\partial x}(a,y)\right]^2\leq 2$.","Let $f:D\to\mathbb{R}$ where $D:=\{(x,y):x^2+y^2\leq 1\}$. Suppose all partial derivatives of $f$ are continuous and $|f|\leq 1$. Show that there exists $(a,b)\in Int_D$ s.t.: $$\left[\frac{\partial f}{\partial x}(a,b)\right]^2+\left[\frac{\partial f}{\partial y}(a,b)\right]^2<4$$ Here $Int_D$ means the interior of $D$, that is , the open unit disk. I try to reduce to the one dimension case, that is: there exists $a\in(-\sqrt{1-y^2},\sqrt{1-y^2})$ s.t $\left[\frac{\partial f}{\partial x}(a,y)\right]^2\leq 2$.",,"['real-analysis', 'multivariable-calculus']"
24,"Using change of variables, solve the integral and show the domain obtained by the change.","Using change of variables, solve the integral and show the domain obtained by the change.",,"I need to solve the following integral using change of variables: $$\int\int_D\frac{\sqrt[3]{y-x}}{1+x+y}dA$$ where D is the triangle with vertices $(0,0)$, $(1,0)$ and $(0,1)$. I tried to change the variables to $u=y-x$ and $v=1+x+y$, but then I couldn't solve the integral. Any tips on how to solve it (or the full solution) would be highly appreciated! Thanks in advance! EDIT: I should've been more detailed in what I did. I tried the change $u=y-x$ and $v=1+x+y$, found the new domain and calculated the Jacobian. The integral then is: $$\int_1^2\int_{1-v}^{v-1}\frac{\sqrt[3]{u}}{v}\frac{1}{2}dudv$$ That is what I can't solve. I can solve it in u, but I don't even know if it's right: $$\frac{3}{8}\int_1^2\frac{{(v-1)}^{4/3}-{(1-v)}^{4/3}}{v}dv$$","I need to solve the following integral using change of variables: $$\int\int_D\frac{\sqrt[3]{y-x}}{1+x+y}dA$$ where D is the triangle with vertices $(0,0)$, $(1,0)$ and $(0,1)$. I tried to change the variables to $u=y-x$ and $v=1+x+y$, but then I couldn't solve the integral. Any tips on how to solve it (or the full solution) would be highly appreciated! Thanks in advance! EDIT: I should've been more detailed in what I did. I tried the change $u=y-x$ and $v=1+x+y$, found the new domain and calculated the Jacobian. The integral then is: $$\int_1^2\int_{1-v}^{v-1}\frac{\sqrt[3]{u}}{v}\frac{1}{2}dudv$$ That is what I can't solve. I can solve it in u, but I don't even know if it's right: $$\frac{3}{8}\int_1^2\frac{{(v-1)}^{4/3}-{(1-v)}^{4/3}}{v}dv$$",,"['integration', 'multivariable-calculus', 'definite-integrals']"
25,"$f(x,y)=\sqrt{|xy|}$",,"f(x,y)=\sqrt{|xy|}","$f(x,y)=\sqrt{|xy|}$ I need to calculate partial derivatives at $(0,0)$, and conclude whether it is differentiable there. $f_x(x,y)=\lim_{h\to 0}{f(x+h,y)-f(x,y)\over h}=0=f_y(x,y)$, so can I conclude that $f_x(0,0)=0$ and $f$ is differentiable there?","$f(x,y)=\sqrt{|xy|}$ I need to calculate partial derivatives at $(0,0)$, and conclude whether it is differentiable there. $f_x(x,y)=\lim_{h\to 0}{f(x+h,y)-f(x,y)\over h}=0=f_y(x,y)$, so can I conclude that $f_x(0,0)=0$ and $f$ is differentiable there?",,['multivariable-calculus']
26,Finding the most general class of solutions to $x\partial_{y}f = y\partial_{x}f$,Finding the most general class of solutions to,x\partial_{y}f = y\partial_{x}f,"Consider the following PDE for $f(x, y)$: $$ x\frac{\partial f}{\partial y} = y\frac{\partial f}{\partial x}\tag{1} $$ Clearly, one can separate the variables, so take $f(x, y) = p(x)q(y)$: $$ xp(x)q'(y) = yq(y)p'(x)\implies \frac{p'(x)}{xp(x)}=\frac{q'(x)}{yq(x)}=C $$ Then: $$ \frac{p'}{p}=Cx\implies d(\ln p) = d(Cx^2/2)\implies p(x) = e^{Cx^2/2} $$ Similarly for $y$, so we have that $f(x, y) = e^{C(x^2 + y^2)/2}$. Now here's the thing, one could notice here that in fact $f(x, y) = g(u(x, y)), u(x,y) = C(x^2 + y^2)$ for any $g$ solves the equation as: $$ \frac{\partial g}{\partial x} = \frac{\partial g}{\partial u}2Cx \quad\quad \frac{\partial g}{\partial y} = \frac{\partial g}{\partial u}2Cy $$ Which clearly solves $(1)$. By separating the variables, of course I've eliminated the possibility of finding solutions such as $f(x, y) = x^2 + y^2$, so what I want to know is whether or not there's any method of solving $(1)$ that finds the most general class of solutions $g(x^2 + y^2)$?","Consider the following PDE for $f(x, y)$: $$ x\frac{\partial f}{\partial y} = y\frac{\partial f}{\partial x}\tag{1} $$ Clearly, one can separate the variables, so take $f(x, y) = p(x)q(y)$: $$ xp(x)q'(y) = yq(y)p'(x)\implies \frac{p'(x)}{xp(x)}=\frac{q'(x)}{yq(x)}=C $$ Then: $$ \frac{p'}{p}=Cx\implies d(\ln p) = d(Cx^2/2)\implies p(x) = e^{Cx^2/2} $$ Similarly for $y$, so we have that $f(x, y) = e^{C(x^2 + y^2)/2}$. Now here's the thing, one could notice here that in fact $f(x, y) = g(u(x, y)), u(x,y) = C(x^2 + y^2)$ for any $g$ solves the equation as: $$ \frac{\partial g}{\partial x} = \frac{\partial g}{\partial u}2Cx \quad\quad \frac{\partial g}{\partial y} = \frac{\partial g}{\partial u}2Cy $$ Which clearly solves $(1)$. By separating the variables, of course I've eliminated the possibility of finding solutions such as $f(x, y) = x^2 + y^2$, so what I want to know is whether or not there's any method of solving $(1)$ that finds the most general class of solutions $g(x^2 + y^2)$?",,"['multivariable-calculus', 'partial-differential-equations']"
27,Double integral with messy variable substitution,Double integral with messy variable substitution,,"I was looking through old multivariable analysis exams and found this double integral (with solution). My problem is I can't seem to understand how the transformation to the new area of integration is done. So here's the question: Calculate $\int\int_D (2x^2+y)\,dx\,dy $ where $D$ is limited by the functions:  $x = 0, x= 1, y= 0, y=\frac{1}{x}$ and $y = x^2 + 1$ $D$ looks somewhat like a house and the intersection $x^2 + 1=\frac{1}{x}$ gives a messy solution so that's why this substitution is used (in the supplied solution) instead: $\begin{cases} u = xy \\ v = y -x^2 \end{cases}  $ We get the new area $E$: $\begin{cases} u-1 \leq v \leq 1 \\ 0 \leq u \leq 1 \end{cases} $ From here it's very easy to solve since: $\dfrac{d(u,v)}{d(x,y)} = y+2x^2 $  so we have  $(y+2x^2)\,dx\,dy = du\,dv$ What I don't understand is how $v$ gets the lower limit $ u-1 $. How this way of solving the problem should become obvious at all is also still a mystery to me.","I was looking through old multivariable analysis exams and found this double integral (with solution). My problem is I can't seem to understand how the transformation to the new area of integration is done. So here's the question: Calculate $\int\int_D (2x^2+y)\,dx\,dy $ where $D$ is limited by the functions:  $x = 0, x= 1, y= 0, y=\frac{1}{x}$ and $y = x^2 + 1$ $D$ looks somewhat like a house and the intersection $x^2 + 1=\frac{1}{x}$ gives a messy solution so that's why this substitution is used (in the supplied solution) instead: $\begin{cases} u = xy \\ v = y -x^2 \end{cases}  $ We get the new area $E$: $\begin{cases} u-1 \leq v \leq 1 \\ 0 \leq u \leq 1 \end{cases} $ From here it's very easy to solve since: $\dfrac{d(u,v)}{d(x,y)} = y+2x^2 $  so we have  $(y+2x^2)\,dx\,dy = du\,dv$ What I don't understand is how $v$ gets the lower limit $ u-1 $. How this way of solving the problem should become obvious at all is also still a mystery to me.",,"['integration', 'multivariable-calculus']"
28,What is a directional derivative?,What is a directional derivative?,,"I have encountered this in an online PDE course I'm following but I've never really been exposed to it. I've looked for the 'formal' definitions but I've never really understood any concept by looking at the formal, mathematical definition so can anyone elucidate this concept?","I have encountered this in an online PDE course I'm following but I've never really been exposed to it. I've looked for the 'formal' definitions but I've never really understood any concept by looking at the formal, mathematical definition so can anyone elucidate this concept?",,['multivariable-calculus']
29,Is this always true?,Is this always true?,,"Suppose $\left|x_{1}\right|\ge\left|x_{2}\right|\ge\left|x_{3}\right|$, $\left|y_{1}\right|\ge\left|y_{2}\right|\ge\left|y_{3}\right|$, and $$\left(x_{1}-y_{1}\right)\left(x_{2}-y_{2}\right)\left(x_{1}-y_{2}\right)\left(x_{2}-y_{1}\right)<0,$$ is it true that $$\sqrt{\left|t\right|}+\sqrt{\left|x_{1}+x_{2}+x_{3}-y_{1}-y_{2}-y_{3}-t\right|}\ge\left|\sqrt{\left|x_{1}\right|}-\sqrt{\left|y_{1}\right|}\right|+\left|\sqrt{\left|x_{2}\right|}-\sqrt{\left|y_{2}\right|}\right|+\left|\sqrt{\left|x_{3}\right|}-\sqrt{\left|y_{3}\right|}\right|$$ for any $t$?","Suppose $\left|x_{1}\right|\ge\left|x_{2}\right|\ge\left|x_{3}\right|$, $\left|y_{1}\right|\ge\left|y_{2}\right|\ge\left|y_{3}\right|$, and $$\left(x_{1}-y_{1}\right)\left(x_{2}-y_{2}\right)\left(x_{1}-y_{2}\right)\left(x_{2}-y_{1}\right)<0,$$ is it true that $$\sqrt{\left|t\right|}+\sqrt{\left|x_{1}+x_{2}+x_{3}-y_{1}-y_{2}-y_{3}-t\right|}\ge\left|\sqrt{\left|x_{1}\right|}-\sqrt{\left|y_{1}\right|}\right|+\left|\sqrt{\left|x_{2}\right|}-\sqrt{\left|y_{2}\right|}\right|+\left|\sqrt{\left|x_{3}\right|}-\sqrt{\left|y_{3}\right|}\right|$$ for any $t$?",,"['calculus', 'real-analysis', 'multivariable-calculus', 'inequality']"
30,How to compute $\int_{\mathbb{C}} \frac{|dz|^2}{|z-a_1| \cdot |z-a_2| \cdot |z-a_3|}$ explicitly?,How to compute  explicitly?,\int_{\mathbb{C}} \frac{|dz|^2}{|z-a_1| \cdot |z-a_2| \cdot |z-a_3|},"I have these integrals : $$I_1= \int_{\mathbb{C}} \frac{|dz|^2}{|z-a_1| \cdot |z-a_2| \cdot |z-a_3|},$$ and $$I_2= \int_{\mathbb{C}} \frac{|dz|^2}{|z-a_1| \cdot |z-a_2| \cdot |z-a_3| \cdot |z-a_4|}.$$ wherer $a_i \in \mathbb{C}$, and the $a_i$ are distincts. I can see that those integrals are well defined, but I need to evaluate them. Can they be computed explicitly ? Note : I would like an explicit value for these integrals, but failing that I am interested in the asymptotic value when $a_2 \rightarrow a_1$.","I have these integrals : $$I_1= \int_{\mathbb{C}} \frac{|dz|^2}{|z-a_1| \cdot |z-a_2| \cdot |z-a_3|},$$ and $$I_2= \int_{\mathbb{C}} \frac{|dz|^2}{|z-a_1| \cdot |z-a_2| \cdot |z-a_3| \cdot |z-a_4|}.$$ wherer $a_i \in \mathbb{C}$, and the $a_i$ are distincts. I can see that those integrals are well defined, but I need to evaluate them. Can they be computed explicitly ? Note : I would like an explicit value for these integrals, but failing that I am interested in the asymptotic value when $a_2 \rightarrow a_1$.",,"['multivariable-calculus', 'definite-integrals', 'closed-form']"
31,minimum of this function,minimum of this function,,"Define $f(x,y)  =  (a - bx - by)e^{-(x^2+c y^2)}$, where $c > 1$ and $b>a$ then define $g(x,y)  =  f(x,y)$ if $x<y$ and $g(x,y)=f(y,x)$ if $x>=y$. Look at the function $g(x,y)$. Obviously $g(x,y)$ is symmetric about $x=y$ and not differentiable at $x=y$, but I don't know why the minimum of $g(x,y)$ also occurs at $x=y$?","Define $f(x,y)  =  (a - bx - by)e^{-(x^2+c y^2)}$, where $c > 1$ and $b>a$ then define $g(x,y)  =  f(x,y)$ if $x<y$ and $g(x,y)=f(y,x)$ if $x>=y$. Look at the function $g(x,y)$. Obviously $g(x,y)$ is symmetric about $x=y$ and not differentiable at $x=y$, but I don't know why the minimum of $g(x,y)$ also occurs at $x=y$?",,"['multivariable-calculus', 'optimization']"
32,Differentiation chain rule,Differentiation chain rule,,"If $f(tx,ty,tz) = t^nf(x,y,z)$ then in my lecture notes it says differentiating this equation with respect to $t$ gives: $$x\frac{df}{dx} + y\frac{df}{dy} + z\frac{df}{dz} = nt^{n-1}f(x,y,z)$$ But why isn't it: $$x\frac{df}{d(tx)} + y\frac{df}{d(ty)} + z\frac{df}{d(tz)} = nt^{n-1}f(x,y,z)\ ?$$","If $f(tx,ty,tz) = t^nf(x,y,z)$ then in my lecture notes it says differentiating this equation with respect to $t$ gives: $$x\frac{df}{dx} + y\frac{df}{dy} + z\frac{df}{dz} = nt^{n-1}f(x,y,z)$$ But why isn't it: $$x\frac{df}{d(tx)} + y\frac{df}{d(ty)} + z\frac{df}{d(tz)} = nt^{n-1}f(x,y,z)\ ?$$",,['multivariable-calculus']
33,Integrating by using change of variables and by making a substitution,Integrating by using change of variables and by making a substitution,,"Let $D$ be the region bounded by $x=0$, $y=0$, $x+y=1$ and $x+y=4$. Evaluate $$\iint_D \frac{dx\,dy}{x+y}$$ by making the change of variables $x=u-uv$, $y=uv$ My attempt I understand I must first find the domain.   $$x=u-uv, y=uv$$ $$x=u-y$$ $$u=x+y$$ and $$v=\frac{u}{y}$$    This gives me the domain $$ (x,y) \rightarrow(u,v) $$ $$ (0,1) \rightarrow(1,1) $$ $$ (1,0) \rightarrow(1,0) $$$$ (0,4) \rightarrow(4,1) $$ $$ (4,0) \rightarrow(4,0) $$ After i graph these points on the new $(u,v)$ graph i get a box and so my limits are $$ 1 \le u \le 4$$ $$0 \le v \le 1 $$ and now I am having trouble finding the jacobian since u is a function of v. Any help would be great.","Let $D$ be the region bounded by $x=0$, $y=0$, $x+y=1$ and $x+y=4$. Evaluate $$\iint_D \frac{dx\,dy}{x+y}$$ by making the change of variables $x=u-uv$, $y=uv$ My attempt I understand I must first find the domain.   $$x=u-uv, y=uv$$ $$x=u-y$$ $$u=x+y$$ and $$v=\frac{u}{y}$$    This gives me the domain $$ (x,y) \rightarrow(u,v) $$ $$ (0,1) \rightarrow(1,1) $$ $$ (1,0) \rightarrow(1,0) $$$$ (0,4) \rightarrow(4,1) $$ $$ (4,0) \rightarrow(4,0) $$ After i graph these points on the new $(u,v)$ graph i get a box and so my limits are $$ 1 \le u \le 4$$ $$0 \le v \le 1 $$ and now I am having trouble finding the jacobian since u is a function of v. Any help would be great.",,"['integration', 'multivariable-calculus']"
34,"Evaluate the limit: $\lim_{(x,y,z)\to(0,0,0)}\frac{xy+yz^2+xz^2}{x^2+y^2+z^4}$",Evaluate the limit:,"\lim_{(x,y,z)\to(0,0,0)}\frac{xy+yz^2+xz^2}{x^2+y^2+z^4}","Could someone give me a hint? I would like to continue to attempt it. The limit to evaluate that I would like a hint on is: $$\lim_{(x,y,z)\to(0,0,0)}\frac{xy+yz^2+xz^2}{x^2+y^2+z^4}$$","Could someone give me a hint? I would like to continue to attempt it. The limit to evaluate that I would like a hint on is: $$\lim_{(x,y,z)\to(0,0,0)}\frac{xy+yz^2+xz^2}{x^2+y^2+z^4}$$",,['multivariable-calculus']
35,Volume bounded by sphere $x^2+y^2+z^2=a^2$ and cylinder $x^2+y^2=a|x|$,Volume bounded by sphere  and cylinder,x^2+y^2+z^2=a^2 x^2+y^2=a|x|,What is the volume bounded by the sphere $x^2+y^2+z^2=a^2$ and the cylinder $x^2+y^2=a|x|$? The answer can be in terms of the value $a$ (or $r$). Does someone know how to do this? Thank you in advance!,What is the volume bounded by the sphere $x^2+y^2+z^2=a^2$ and the cylinder $x^2+y^2=a|x|$? The answer can be in terms of the value $a$ (or $r$). Does someone know how to do this? Thank you in advance!,,"['multivariable-calculus', 'volume']"
36,Pointwise supremum of a convex function collection,Pointwise supremum of a convex function collection,,"In Hoang Tuy, Convex Analysis and Global Optimization, Kluwer, pag. 46, I read: A positive combination of finitely many proper convex functions on $R^n$ is convex. The upper envelope (pointwise supremum) of an arbitrary family of convex functions is convex. In order to prove the second claim the author sets the pointwise supremum as: $$ f(x) = \sup \{f_i(x) \mid i \in I\}  $$ Then $$ \mathrm{epi} f = \bigcap_{i \in I} \mathrm{epi} f_i  $$ As  ""the intersection of a family of convex sets is a convex set"" the thesis follows. The claim on the intersections raises some doubts for me. If $(x,t)$ is in the epigraph of $f$ , $f(x) \leq t)$ and, as $f(x)\geq f_i(x)$ , also $f_i(x) \leq t)$ ; therefore $(x,t)$ is in the epigraph of every $f_i$ and the intersection proposition follows. Now, what if, for some (not all) $\hat{i}$ , $f_{\hat{i}}(x^0)$ is not defined? The sup still applies to the other $f_i$ and so, in as far as the sup is finite, $f(x^0)$ is  defined. In this case when $(x^0,t) \in  \mathrm{epi} f$ not $\in \mathrm{epi} f_{\hat{i}}$ too, since $x^0 \not \in \mathrm{dom}\, f_{\hat{i}}$ . So it is simple to say that $f$ is convex over $\bigcap_{i \in I} \mathrm{dom} f_i\subset \mathrm{dom} f$ , because in this set every $x  \in \mathrm{dom}\, f_{\hat{i}}$ . What can we say when $x \in \mathrm{dom}\, f$ but $x \not \in \mathrm{dom}\, f_{\hat{i}}$ for some $i$ ?","In Hoang Tuy, Convex Analysis and Global Optimization, Kluwer, pag. 46, I read: A positive combination of finitely many proper convex functions on is convex. The upper envelope (pointwise supremum) of an arbitrary family of convex functions is convex. In order to prove the second claim the author sets the pointwise supremum as: Then As  ""the intersection of a family of convex sets is a convex set"" the thesis follows. The claim on the intersections raises some doubts for me. If is in the epigraph of , and, as , also ; therefore is in the epigraph of every and the intersection proposition follows. Now, what if, for some (not all) , is not defined? The sup still applies to the other and so, in as far as the sup is finite, is  defined. In this case when not too, since . So it is simple to say that is convex over , because in this set every . What can we say when but for some ?","R^n 
f(x) = \sup \{f_i(x) \mid i \in I\} 
 
\mathrm{epi} f = \bigcap_{i \in I} \mathrm{epi} f_i 
 (x,t) f f(x) \leq t) f(x)\geq f_i(x) f_i(x) \leq t) (x,t) f_i \hat{i} f_{\hat{i}}(x^0) f_i f(x^0) (x^0,t) \in  \mathrm{epi} f \in \mathrm{epi} f_{\hat{i}} x^0 \not \in \mathrm{dom}\, f_{\hat{i}} f \bigcap_{i \in I} \mathrm{dom} f_i\subset \mathrm{dom} f x  \in \mathrm{dom}\, f_{\hat{i}} x \in \mathrm{dom}\, f x \not \in \mathrm{dom}\, f_{\hat{i}} i","['real-analysis', 'multivariable-calculus', 'convex-analysis', 'convex-optimization']"
37,Multiple Integral Equation,Multiple Integral Equation,,"$$f(x) = 2a \int_{0}^{x}{f(t)\;dt} - \left(\frac{b^2}{2}\right)\int_{0}^{1}{|x-t|f(t)\;dt}$$ where $0<a<b$ My task is to solve for $f(x)$.  I'm having difficulty solving this integral equation.  What makes it really hard is the variable upper bound in the first integral, therefore implies that this is a Volterra integral equation.  However this is a double integral equation.  The second equation is a Fredholm integral equation.  That's where I'm stuck.  I've tried hitting it from different angles, but only to be in vain.","$$f(x) = 2a \int_{0}^{x}{f(t)\;dt} - \left(\frac{b^2}{2}\right)\int_{0}^{1}{|x-t|f(t)\;dt}$$ where $0<a<b$ My task is to solve for $f(x)$.  I'm having difficulty solving this integral equation.  What makes it really hard is the variable upper bound in the first integral, therefore implies that this is a Volterra integral equation.  However this is a double integral equation.  The second equation is a Fredholm integral equation.  That's where I'm stuck.  I've tried hitting it from different angles, but only to be in vain.",,"['multivariable-calculus', 'calculus-of-variations', 'integral-equations']"
38,Multivariable Calculus Integral Proof,Multivariable Calculus Integral Proof,,"This problem is being very difficult for me to solve, I need help. Consider $F:\mathbb{R}^2\rightarrow\mathbb{R}$ of class $C^1$, suppose that the level curves of $F$ are closed and that $\nabla F$ is never $0$ for $x\neq0$. Consider the region $D$ between the curves $F=a$ and $F=b$. For each $r$ in $[a,b]$, let $c_r$ be the curve $F=r$. Let $f:D\rightarrow\mathbb{R}$ continuous. I have to show that $$\int_Df=\int_a^b\bigg(\int_{c_r}\frac{f}{|\nabla F|}\bigg)dr$$ Usually when I ask something here I show my attempts or my observations, but in this case I couldn't even start doing this! Thank you very much for helping!","This problem is being very difficult for me to solve, I need help. Consider $F:\mathbb{R}^2\rightarrow\mathbb{R}$ of class $C^1$, suppose that the level curves of $F$ are closed and that $\nabla F$ is never $0$ for $x\neq0$. Consider the region $D$ between the curves $F=a$ and $F=b$. For each $r$ in $[a,b]$, let $c_r$ be the curve $F=r$. Let $f:D\rightarrow\mathbb{R}$ continuous. I have to show that $$\int_Df=\int_a^b\bigg(\int_{c_r}\frac{f}{|\nabla F|}\bigg)dr$$ Usually when I ask something here I show my attempts or my observations, but in this case I couldn't even start doing this! Thank you very much for helping!",,"['multivariable-calculus', 'vector-analysis']"
39,Locate and classify critical points of a function?,Locate and classify critical points of a function?,,"Assuming $f(x,y) = 24xy + 4xy^2 + 3x^3$ and taking partial differentiation respect to $x$ and $y$ I found to be: $$\dfrac{\partial f}{\partial x} = 24y + 4y^2 +9x^2 \\ \dfrac{\partial f}{\partial y} = 24x + 8xy$$","Assuming $f(x,y) = 24xy + 4xy^2 + 3x^3$ and taking partial differentiation respect to $x$ and $y$ I found to be: $$\dfrac{\partial f}{\partial x} = 24y + 4y^2 +9x^2 \\ \dfrac{\partial f}{\partial y} = 24x + 8xy$$",,['multivariable-calculus']
40,Help with greens function/fourier transformation to solve screened poisson equation,Help with greens function/fourier transformation to solve screened poisson equation,,"I am having trouble getting from one line to the next from this wiki page. http://en.wikipedia.org/wiki/Screened_Poisson_equation . I am referring to "" Green's function in r is therefore given by the inverse Fourier transform"" where $$G(r) = \frac{1}{(2\pi)^3} \int\int\int d^3k \frac{e^{ik*r}}{k^2+\lambda^2}$$ goes to $$G(r) = \frac{1}{2\pi^2r} \int^{+\infty}_0 dk_r \frac{k_r \sin(k_r r)}{k_r^2+\lambda^2}$$ where does the $\frac{1}{r}$ term come from and what is $k_r$. How did they simplify the tripple integral? Divergence theorem? Stokes? Detailed steps would be much appreciated.","I am having trouble getting from one line to the next from this wiki page. http://en.wikipedia.org/wiki/Screened_Poisson_equation . I am referring to "" Green's function in r is therefore given by the inverse Fourier transform"" where $$G(r) = \frac{1}{(2\pi)^3} \int\int\int d^3k \frac{e^{ik*r}}{k^2+\lambda^2}$$ goes to $$G(r) = \frac{1}{2\pi^2r} \int^{+\infty}_0 dk_r \frac{k_r \sin(k_r r)}{k_r^2+\lambda^2}$$ where does the $\frac{1}{r}$ term come from and what is $k_r$. How did they simplify the tripple integral? Divergence theorem? Stokes? Detailed steps would be much appreciated.",,"['multivariable-calculus', 'mathematical-physics']"
41,Non surjectivity of $C^1$ function. [duplicate],Non surjectivity of  function. [duplicate],C^1,"This question already has an answer here : There is no $C^1$ function $f$ mapping an open interval in $\mathbb{R}$ onto open ball in $\mathbb{R}^2$ (1 answer) Closed 11 years ago . I am trying to figure out how to do the following question, but i seems to not have any success. If $f:\mathbb{R}^1\to \mathbb{R}^2$ is of class $C^1$ mapping, show that $f$ does not carry $\mathbb{R}^1$ onto $\mathbb{R}^2$. I know that if suppose that $f$ is onto, then let $g$ be a right inverse of $f$ where $f(g(x,y))=x$.  If I differentiate $f(g(x,y))=x$, using the chain rule, i get some expression, but there is a problem because i am not sure if $g(x,y)$ is itself differentiable to begin with. Alternatively, if i let the mapping of $f$ be written as $f(t)=(\phi_1(t), \phi_2(t))$, then the Jacobian matrix $Df(t)$ is a $2$ by $1$ matrix.  So the column rank of $Df(t)$ is $1$.  But how can i use this information to conclude that $f$ is not onto, since if $f$ were onto in the first place, won't the column rank of its Jacobian be equal to the dimension of the function's codomain. Thanks in advance","This question already has an answer here : There is no $C^1$ function $f$ mapping an open interval in $\mathbb{R}$ onto open ball in $\mathbb{R}^2$ (1 answer) Closed 11 years ago . I am trying to figure out how to do the following question, but i seems to not have any success. If $f:\mathbb{R}^1\to \mathbb{R}^2$ is of class $C^1$ mapping, show that $f$ does not carry $\mathbb{R}^1$ onto $\mathbb{R}^2$. I know that if suppose that $f$ is onto, then let $g$ be a right inverse of $f$ where $f(g(x,y))=x$.  If I differentiate $f(g(x,y))=x$, using the chain rule, i get some expression, but there is a problem because i am not sure if $g(x,y)$ is itself differentiable to begin with. Alternatively, if i let the mapping of $f$ be written as $f(t)=(\phi_1(t), \phi_2(t))$, then the Jacobian matrix $Df(t)$ is a $2$ by $1$ matrix.  So the column rank of $Df(t)$ is $1$.  But how can i use this information to conclude that $f$ is not onto, since if $f$ were onto in the first place, won't the column rank of its Jacobian be equal to the dimension of the function's codomain. Thanks in advance",,['multivariable-calculus']
42,$f(y) \leq f(x)+\nabla f(x)\cdot (y-x) $ and $f(x)\geq 0$ implies that $f$ is constant.,and  implies that  is constant.,f(y) \leq f(x)+\nabla f(x)\cdot (y-x)  f(x)\geq 0 f,"Here is the question. Suppose that $f: \mathbb R^n \rightarrow \mathbb R$ has two derivatives and the associated hessian matrix is negative semidefinite on all of $\mathbb R^n$. Show that for any $x,y\in \mathbb R^n$ $$f(y) \leq f(x)+\nabla f(x)\cdot (y-x) $$ I can show this using Taylor's formula. But the question also says: If in addition $f(x)\geq 0$ for all $x\in \mathbb R^n$ then show that f must be constant. Can you help me on the second part. I know that I need to show that $\nabla f(x)=0, \forall x\in \mathbb R^n$. Any hints?","Here is the question. Suppose that $f: \mathbb R^n \rightarrow \mathbb R$ has two derivatives and the associated hessian matrix is negative semidefinite on all of $\mathbb R^n$. Show that for any $x,y\in \mathbb R^n$ $$f(y) \leq f(x)+\nabla f(x)\cdot (y-x) $$ I can show this using Taylor's formula. But the question also says: If in addition $f(x)\geq 0$ for all $x\in \mathbb R^n$ then show that f must be constant. Can you help me on the second part. I know that I need to show that $\nabla f(x)=0, \forall x\in \mathbb R^n$. Any hints?",,"['multivariable-calculus', 'optimization', 'taylor-expansion']"
43,When does a gradient vector of a function not exist?,When does a gradient vector of a function not exist?,,I have a question that gives me a 3d function and asks me to calculate the gradient vector of it. This part I understand. It then asks me to indicate the points at which it does not exist. When does a gradient vector not exist? Is it when it equals to zero? Or does it mean when the function DNE. Thanks.,I have a question that gives me a 3d function and asks me to calculate the gradient vector of it. This part I understand. It then asks me to indicate the points at which it does not exist. When does a gradient vector not exist? Is it when it equals to zero? Or does it mean when the function DNE. Thanks.,,['multivariable-calculus']
44,Surface integral (Flux),Surface integral (Flux),,"Evaluate the surface integral $  \int_{S}\int \vec{F} \cdot \vec{n}\,  dS,$ with the vector field $ \vec{F} = zx\vec{i} + xy\vec{j} +  yz\vec{k} \ $. $S$ is the closed surface composed of a portion of the    cylinder $ x^2 + y^2 = R^2 $ that lies in the first octant, and    portions of the planes $ x=0, y =0, z = 0\,\,\text{and}\,\, z = H $. $\vec{n}$ is the outward unit normal vector. Attempt: I said $S$ consisted of the five surfaces $ S_1, S_2, S_3, S_4 $ and $ S_5$ $S_1 $ being the portion of the cylinder, $S_2$ being where the plane $ z=0$ cuts the cylinder and similarly, $ S_3, S_4 ,S_5 $ being where the planes $ x = 0, y = 0 $ and $z = H $ cut the cylinder. For $ S_2 $, the normal vector points in the -k direction. so the required integral over $S_2$ is: $$ \int_{0}^{R} \int_{0}^{\sqrt{R^2-x^2}} -yz\,dy\,dx $$ Am I correct? I think for the surface $ S_5$ the only thing that would change in the above would be that the unit normal vector points in the positive k direction? I need some guidance on how to set up the integrals for the rest of the surfaces. I tried $ \int_{0}^{R} \int_{0}^{H} -xy\,dz\,dx $ for the y = 0 plane intersection with the cylinder, but I am not sure if this is correct. Any advice on how to tackle the remaining surfaces would be very helpful. Many thanks.","Evaluate the surface integral $  \int_{S}\int \vec{F} \cdot \vec{n}\,  dS,$ with the vector field $ \vec{F} = zx\vec{i} + xy\vec{j} +  yz\vec{k} \ $. $S$ is the closed surface composed of a portion of the    cylinder $ x^2 + y^2 = R^2 $ that lies in the first octant, and    portions of the planes $ x=0, y =0, z = 0\,\,\text{and}\,\, z = H $. $\vec{n}$ is the outward unit normal vector. Attempt: I said $S$ consisted of the five surfaces $ S_1, S_2, S_3, S_4 $ and $ S_5$ $S_1 $ being the portion of the cylinder, $S_2$ being where the plane $ z=0$ cuts the cylinder and similarly, $ S_3, S_4 ,S_5 $ being where the planes $ x = 0, y = 0 $ and $z = H $ cut the cylinder. For $ S_2 $, the normal vector points in the -k direction. so the required integral over $S_2$ is: $$ \int_{0}^{R} \int_{0}^{\sqrt{R^2-x^2}} -yz\,dy\,dx $$ Am I correct? I think for the surface $ S_5$ the only thing that would change in the above would be that the unit normal vector points in the positive k direction? I need some guidance on how to set up the integrals for the rest of the surfaces. I tried $ \int_{0}^{R} \int_{0}^{H} -xy\,dz\,dx $ for the y = 0 plane intersection with the cylinder, but I am not sure if this is correct. Any advice on how to tackle the remaining surfaces would be very helpful. Many thanks.",,['multivariable-calculus']
45,Triple integration in cylindrical coordinates,Triple integration in cylindrical coordinates,,"Determine the value of $ \int_{0}^{2} \int_{0}^{\sqrt{2x - x^2}} \int_{0}^{1} z \sqrt{x^2 +y^2} dz\,dy\,dx $ My attempt: So in cylindrical coordinates, the integrand is simply $ \rho$. $\sqrt{2x-x^2} $ is a circle of centre (1,0) in the xy plane. So $ x^2 + y^2 = 2x => \rho^2 = 2\rho\cos\theta => \rho = 2\cos\theta $ Therfore, I arrived at the limit transformations, $ 0 < \rho < 2\cos\theta,\,\, 0 < z < 1, \text{and}\,\,0 < \theta < \frac{\pi}{2}  $ Bringing this together gives $  \int_{0}^{\frac{\pi}{2}} \int_{0}^{2\cos\theta} \int_{0}^{1} z\,\,\rho^3\,dz\,d\rho\,d\theta $ in cylindrical coordinates. Is this correct?","Determine the value of $ \int_{0}^{2} \int_{0}^{\sqrt{2x - x^2}} \int_{0}^{1} z \sqrt{x^2 +y^2} dz\,dy\,dx $ My attempt: So in cylindrical coordinates, the integrand is simply $ \rho$. $\sqrt{2x-x^2} $ is a circle of centre (1,0) in the xy plane. So $ x^2 + y^2 = 2x => \rho^2 = 2\rho\cos\theta => \rho = 2\cos\theta $ Therfore, I arrived at the limit transformations, $ 0 < \rho < 2\cos\theta,\,\, 0 < z < 1, \text{and}\,\,0 < \theta < \frac{\pi}{2}  $ Bringing this together gives $  \int_{0}^{\frac{\pi}{2}} \int_{0}^{2\cos\theta} \int_{0}^{1} z\,\,\rho^3\,dz\,d\rho\,d\theta $ in cylindrical coordinates. Is this correct?",,['multivariable-calculus']
46,Double integral of an function odd with respect to $y$ over a domain symmetric with respect to $x$ axis,Double integral of an function odd with respect to  over a domain symmetric with respect to  axis,y x,"Let $D$ be a region given as the set of $(x, y)$ with $$a \leq x \leq b\quad\text{and}\quad-\Phi(x) \leq y \leq \Phi(x)$$ where $Φ$ is a nonnegative continuous function on the interval $[a, b]$. Let $f(x, y)$ be a function on $D$ such that $$f(x, y) = - f(x, - y)$$ for all $(x , y) \in D$. Argue that $$\displaystyle \iint_D f(x, y) dA = 0.$$","Let $D$ be a region given as the set of $(x, y)$ with $$a \leq x \leq b\quad\text{and}\quad-\Phi(x) \leq y \leq \Phi(x)$$ where $Φ$ is a nonnegative continuous function on the interval $[a, b]$. Let $f(x, y)$ be a function on $D$ such that $$f(x, y) = - f(x, - y)$$ for all $(x , y) \in D$. Argue that $$\displaystyle \iint_D f(x, y) dA = 0.$$",,"['integration', 'multivariable-calculus']"
47,Lie brackets of $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$,Lie brackets of  and,\frac{\partial f}{\partial x} \frac{\partial f}{\partial y},"Let $f$ be a smooth function, $f\colon\mathbb{R}^2\to \mathbb{R}$. What is $\left[\frac{\partial f}{\partial x},\frac{\partial f}{\partial y}\right]$ ? I want to say it's $0$ since $\frac{\partial f^2}{\partial x\partial y}=\frac{\partial f^2}{\partial y\partial x}$ but I am unsure about why for the two functions : $\frac{\partial f}{\partial x}$ ,$\frac{\partial f}{\partial y}$ the composition is the same... Can someone please help with this ? [I didn't know what tag to give this post, hope it's ok]","Let $f$ be a smooth function, $f\colon\mathbb{R}^2\to \mathbb{R}$. What is $\left[\frac{\partial f}{\partial x},\frac{\partial f}{\partial y}\right]$ ? I want to say it's $0$ since $\frac{\partial f^2}{\partial x\partial y}=\frac{\partial f^2}{\partial y\partial x}$ but I am unsure about why for the two functions : $\frac{\partial f}{\partial x}$ ,$\frac{\partial f}{\partial y}$ the composition is the same... Can someone please help with this ? [I didn't know what tag to give this post, hope it's ok]",,"['multivariable-calculus', 'lie-algebras']"
48,Second order partial derivatives - notation,Second order partial derivatives - notation,,"I have seen both of these used, and people around me seem to disagree, so which one is correct: (first derivative with respect to x, then y): (1) $$\frac{\partial }{\partial y}(\frac{\partial f}{\partial x}) = \frac{\partial^{2} f}{\partial x\partial y}$$ (2) $$\frac{\partial }{\partial y}(\frac{\partial f}{\partial x}) = \frac{\partial^{2} f}{\partial y\partial x}$$ and why? (reasons, history?)","I have seen both of these used, and people around me seem to disagree, so which one is correct: (first derivative with respect to x, then y): (1) $$\frac{\partial }{\partial y}(\frac{\partial f}{\partial x}) = \frac{\partial^{2} f}{\partial x\partial y}$$ (2) $$\frac{\partial }{\partial y}(\frac{\partial f}{\partial x}) = \frac{\partial^{2} f}{\partial y\partial x}$$ and why? (reasons, history?)",,"['calculus', 'multivariable-calculus', 'notation', 'derivatives']"
49,Cross products and integrals,Cross products and integrals,,"I have been told that the following equation is true, but I don't think it is all that obvious... could someone please explain why it is necessarily true? Suppose $C_1$ and $C_2$ are closed paths in $\mathbb R^3$, and $\vec{r}$ is a position vector, then $$\oint_{C_1} d\vec{s}\times \left(\oint_{C_2}{d\vec{s'}\times \hat{r}\over |\vec{r}|^2}\right)=-\oint_{C_2} d\vec{s'}\times \left(\oint_{C_1}{d\vec{s}\times \hat{r}\over |\vec{r}|^2}\right)$$ My guess would be that we can somehow write the expressions on both sides as a simpler cross-product and $\oint_{C_1} d\vec{s}\times \left(\oint_{C_2}{d\vec{s'}\times \hat{r}\over |\vec{r}|^2}\right)$ say equals to $\vec{a}\times \vec{b}$ and $\oint_{C_2} d\vec{s'}\times \left(\oint_{C_1}{d\vec{s}\times \hat{r}\over |\vec{r}|^2}\right)$ equals $\vec{b}\times \vec{a}$? Just to be clear, $C_1,C_2$ are fixed in  shape and location in $\mathbb R^3$. As @joriki has pointed out, $\vec{r}$ should be $s-s'$ (resp. $s'-s$)","I have been told that the following equation is true, but I don't think it is all that obvious... could someone please explain why it is necessarily true? Suppose $C_1$ and $C_2$ are closed paths in $\mathbb R^3$, and $\vec{r}$ is a position vector, then $$\oint_{C_1} d\vec{s}\times \left(\oint_{C_2}{d\vec{s'}\times \hat{r}\over |\vec{r}|^2}\right)=-\oint_{C_2} d\vec{s'}\times \left(\oint_{C_1}{d\vec{s}\times \hat{r}\over |\vec{r}|^2}\right)$$ My guess would be that we can somehow write the expressions on both sides as a simpler cross-product and $\oint_{C_1} d\vec{s}\times \left(\oint_{C_2}{d\vec{s'}\times \hat{r}\over |\vec{r}|^2}\right)$ say equals to $\vec{a}\times \vec{b}$ and $\oint_{C_2} d\vec{s'}\times \left(\oint_{C_1}{d\vec{s}\times \hat{r}\over |\vec{r}|^2}\right)$ equals $\vec{b}\times \vec{a}$? Just to be clear, $C_1,C_2$ are fixed in  shape and location in $\mathbb R^3$. As @joriki has pointed out, $\vec{r}$ should be $s-s'$ (resp. $s'-s$)",,"['multivariable-calculus', 'integration']"
50,Continuous second order partial derivates in open region $R$,Continuous second order partial derivates in open region,R,"Does having continuous second order partial derivatives at a point $(x_0,y_0)$ in an open region of $\mathbb{R}^2$ imply having continuous first order partial derivatives in the same open region $\mathbb{R}^2$ ?",Does having continuous second order partial derivatives at a point in an open region of imply having continuous first order partial derivatives in the same open region ?,"(x_0,y_0) \mathbb{R}^2 \mathbb{R}^2",['multivariable-calculus']
51,"$\frac{d}{dx}(b^TAx)$ where $b, x \in R^{n\times 1}$ and $A \in R^{n\times n}$",where  and,"\frac{d}{dx}(b^TAx) b, x \in R^{n\times 1} A \in R^{n\times n}","How do you differentiate the following expressions with respect to the vector $x$. I think I might be a little conceptually confused on what happens when you take the derivative with respect to a vector.  What dimensions should you end with?  For the problem, I am trying to solve, I think I should end up for each of these derivatives with the results in the derivative to be $R^{1\times1}$. $\frac{d}{dx}(b^TAx)$ $\frac{d}{dx}(x^TAb)$ $\frac{d}{dx}(x^TAx)$ where $b, x \in R^{n\times 1}$ and $A \in R^{n\times n}$ Also, if it helps A for this case is symmetric. Update: Thanks to the extra motivation by Mike and joriki, I think I now have them solved. $\frac{d}{dx}(b^TAx) = Ab$ $\frac{d}{dx}(x^TAb) = Ab$ $\frac{d}{dx}(x^TAx) =  \textbf{A}\textbf{x} + \textbf{A}^T\textbf{x}$ But if anyone, would like to double check it that would be great.","How do you differentiate the following expressions with respect to the vector $x$. I think I might be a little conceptually confused on what happens when you take the derivative with respect to a vector.  What dimensions should you end with?  For the problem, I am trying to solve, I think I should end up for each of these derivatives with the results in the derivative to be $R^{1\times1}$. $\frac{d}{dx}(b^TAx)$ $\frac{d}{dx}(x^TAb)$ $\frac{d}{dx}(x^TAx)$ where $b, x \in R^{n\times 1}$ and $A \in R^{n\times n}$ Also, if it helps A for this case is symmetric. Update: Thanks to the extra motivation by Mike and joriki, I think I now have them solved. $\frac{d}{dx}(b^TAx) = Ab$ $\frac{d}{dx}(x^TAb) = Ab$ $\frac{d}{dx}(x^TAx) =  \textbf{A}\textbf{x} + \textbf{A}^T\textbf{x}$ But if anyone, would like to double check it that would be great.",,['multivariable-calculus']
52,Lagrange multipliers with non-smooth constraints,Lagrange multipliers with non-smooth constraints,,"I read in a textbook a passing comment that Lagrange multipliers are not applicable if there are points of non-differentiability in the constraints (even if the constraints are continuous).  For example, in the following problem: $\min_{\boldsymbol x} \boldsymbol{a} \cdot \boldsymbol{x}$ s.t. $\max(x_1, x_2) = x_3$ for vectors $\boldsymbol a \in \mathbb{R}^3$ and $\boldsymbol x \in \mathbb{R}^3$. Why can't I use Lagrange multipliers here?  If I push through the standard steps of constructing the Lagrangian, differentiating w.r.t. to variables and Lagrange multipliers, setting the partial derivatives to 0, and solving (assuming I'm able to), what goes wrong?  Is there some related but alternative method that I can use? Thanks in advance. Edit: I'm ok finding any critical point.  I'm looking for issues beyond the standard one that just because you find a point with gradient = 0 doesn't mean you've found the optimum.","I read in a textbook a passing comment that Lagrange multipliers are not applicable if there are points of non-differentiability in the constraints (even if the constraints are continuous).  For example, in the following problem: $\min_{\boldsymbol x} \boldsymbol{a} \cdot \boldsymbol{x}$ s.t. $\max(x_1, x_2) = x_3$ for vectors $\boldsymbol a \in \mathbb{R}^3$ and $\boldsymbol x \in \mathbb{R}^3$. Why can't I use Lagrange multipliers here?  If I push through the standard steps of constructing the Lagrangian, differentiating w.r.t. to variables and Lagrange multipliers, setting the partial derivatives to 0, and solving (assuming I'm able to), what goes wrong?  Is there some related but alternative method that I can use? Thanks in advance. Edit: I'm ok finding any critical point.  I'm looking for issues beyond the standard one that just because you find a point with gradient = 0 doesn't mean you've found the optimum.",,"['multivariable-calculus', 'optimization', 'lagrange-multiplier', 'non-smooth-analysis', 'non-smooth-optimization']"
53,Uniqueness of Helmholtz decomposition,Uniqueness of Helmholtz decomposition,,"Helmholtz theorem states that given a smooth vector field $\mathbf{H}$, there are a scalar field $\phi$ and a vector field $\mathbf{G}$ such that $\mathbf{H}=\nabla \phi +\nabla \times \mathbf{G}$ and $\nabla \mathbf{\cdot G}=0$ Is this decomposition unique? That is, given $\mathbf{H}$, are the fields $\phi$, $\mathbf{G}$ satisfying the above equations unique? Edit: Unique, up to an additive constant. Thanks","Helmholtz theorem states that given a smooth vector field $\mathbf{H}$, there are a scalar field $\phi$ and a vector field $\mathbf{G}$ such that $\mathbf{H}=\nabla \phi +\nabla \times \mathbf{G}$ and $\nabla \mathbf{\cdot G}=0$ Is this decomposition unique? That is, given $\mathbf{H}$, are the fields $\phi$, $\mathbf{G}$ satisfying the above equations unique? Edit: Unique, up to an additive constant. Thanks",,"['multivariable-calculus', 'physics']"
54,Root of a multi-variable derivative,Root of a multi-variable derivative,,"How I got to the problem: Let $f(x,y)=\frac{1}{\sqrt{(x-a_1)^2+(y-a_2)^2}}+\frac{1}{\sqrt{(x-b_1)^2+(y-b_2)^2}}$, where $a_1,a_2,b_1,b_2 \in \mathbb{R}, a=(a_1,a_2)\neq (b_1,b_2)=b$ are fixed and $x,y \in \mathbb{R}, b \neq (x,y)\neq a$ (You can also see $a$ and $b$ as vectors in $\mathbb{R^2}$. I want to show that the derivative of this function has exactly one point $z=(x_0,y_0)$ where the derivative is zero e.g. $D(f(z))=0$. One can easily see that $p_1((x,y))=\frac{\partial}{\partial x}f=\frac{a_1-x}{\left(\left(x-a_1\right){}^2+\left(y-a_2\right){}^2\right){}^{3/2}}+\frac{b_1-x}{\left(\left(x-b_1\right){}^2+\left(y-b_2\right){}^2\right){}^{3/2}}$ whereas $p_2((x,y))=\frac{\partial}{\partial y}f=\frac{a_2-y}{\left(\left(x-a_1\right){}^2+\left(y-a_2\right){}^2\right){}^{3/2}} +\frac{b_2-y}{\left(\left(x-b_1\right){}^2+\left(y-b_2\right){}^2\right){}^{3/2}}$ Before going any deeper, let us look at an example function: I set $a=(0,0)$ and $b=(1,0)$ and get We can see right easily that the only point where the derivative shoul be zero is exactly between the peaks so we guess that $z=\frac{a+b}{2}$. We set it in and easily verify that $p_1(z)=p_2(z)=0$ (so the total derivative will also be 0 at z). Now we have to proove that this is the only solution which is the hard part for me. We can play a bit around with this and get to two final equations (will be right below this). Now the real problem: $(1) 0=\left(a_1-x\right)\left(\left(b_2-y\right){}^2+(\text{b1}-x)^2\right){}^{3/2}+\left(b_1-x\right)\left(\left(a_1-x\right){}^2+\left(a_2-y\right){}^2\right){}^{3/2}$ $(2) 0=\left(b_2-y\right)    \left(\left(a_1-x\right){}^2+\left(a_2-y\right){}^2\right){}^{3/2}+\left(a_2-y\right)    \left(\left(b_1-x\right){}^2+\left(b_2-y\right){}^2\right){}^{3/2}$ We are done if I can show that $(1)$ and $(2)$ imply that $(x,y)=z=\frac{a+b}{2}$. But even Mathematica fails on that one, I hope you can provide some help on this final step.","How I got to the problem: Let $f(x,y)=\frac{1}{\sqrt{(x-a_1)^2+(y-a_2)^2}}+\frac{1}{\sqrt{(x-b_1)^2+(y-b_2)^2}}$, where $a_1,a_2,b_1,b_2 \in \mathbb{R}, a=(a_1,a_2)\neq (b_1,b_2)=b$ are fixed and $x,y \in \mathbb{R}, b \neq (x,y)\neq a$ (You can also see $a$ and $b$ as vectors in $\mathbb{R^2}$. I want to show that the derivative of this function has exactly one point $z=(x_0,y_0)$ where the derivative is zero e.g. $D(f(z))=0$. One can easily see that $p_1((x,y))=\frac{\partial}{\partial x}f=\frac{a_1-x}{\left(\left(x-a_1\right){}^2+\left(y-a_2\right){}^2\right){}^{3/2}}+\frac{b_1-x}{\left(\left(x-b_1\right){}^2+\left(y-b_2\right){}^2\right){}^{3/2}}$ whereas $p_2((x,y))=\frac{\partial}{\partial y}f=\frac{a_2-y}{\left(\left(x-a_1\right){}^2+\left(y-a_2\right){}^2\right){}^{3/2}} +\frac{b_2-y}{\left(\left(x-b_1\right){}^2+\left(y-b_2\right){}^2\right){}^{3/2}}$ Before going any deeper, let us look at an example function: I set $a=(0,0)$ and $b=(1,0)$ and get We can see right easily that the only point where the derivative shoul be zero is exactly between the peaks so we guess that $z=\frac{a+b}{2}$. We set it in and easily verify that $p_1(z)=p_2(z)=0$ (so the total derivative will also be 0 at z). Now we have to proove that this is the only solution which is the hard part for me. We can play a bit around with this and get to two final equations (will be right below this). Now the real problem: $(1) 0=\left(a_1-x\right)\left(\left(b_2-y\right){}^2+(\text{b1}-x)^2\right){}^{3/2}+\left(b_1-x\right)\left(\left(a_1-x\right){}^2+\left(a_2-y\right){}^2\right){}^{3/2}$ $(2) 0=\left(b_2-y\right)    \left(\left(a_1-x\right){}^2+\left(a_2-y\right){}^2\right){}^{3/2}+\left(a_2-y\right)    \left(\left(b_1-x\right){}^2+\left(b_2-y\right){}^2\right){}^{3/2}$ We are done if I can show that $(1)$ and $(2)$ imply that $(x,y)=z=\frac{a+b}{2}$. But even Mathematica fails on that one, I hope you can provide some help on this final step.",,"['calculus', 'real-analysis', 'multivariable-calculus']"
55,What is the vector form of Taylor's Theorem?,What is the vector form of Taylor's Theorem?,,"I checked most of the posts about Taylor expansion with scalar functions. Could anyone tell me what is the multivariate version of Taylor's Theorem, and how I can use it?","I checked most of the posts about Taylor expansion with scalar functions. Could anyone tell me what is the multivariate version of Taylor's Theorem, and how I can use it?",,"['linear-algebra', 'multivariable-calculus']"
56,What do $du$ and $dv$ mean when used in an equation without an integral sign?,What do  and  mean when used in an equation without an integral sign?,du dv,"Main Question How do I evaluate the expression $L\,du^{2} + 2 M\,du\,dv + N\,dv^{2}$ , given values for $L(u, v)$ , $M(u, v)$ , and $N(u, v)$ ? Background While working on a personal project, I came upon this definition from Wikipedia for the second fundamental form (lightly paraphrased): Let $\mathbf{r} = \mathbf{r}(u,v)$ be a regular parametrization of a surface $S$ in $ℝ^3$ , where $\mathbf{r}$ is a smooth vector-valued function of two variables. It is common to denote the partial derivatives of $\mathbf{r}$ with respect to $u$ and $v$ by $\mathbf{r}_{u}$ and $\mathbf{r}_{v}$ . Regularity of the parametrization means that $\mathbf{r}_{u}$ and $\mathbf{r}_{v}$ are linearly independent for any $(u,v)$ in the domain of $\mathbf{r}$ , and hence span the tangent plane to $S$ at each point. Equivalently, the cross product $\mathbf{r}_{u} × \mathbf{r}_{v}$ is a nonzero vector normal to the surface. The parametrization thus defines a field of unit normal vectors $\mathbf{n}$ : $$ \mathbf{n} =\frac{\mathbf{r}_{u} × \mathbf{r}_{v}}{|\mathbf{r}_{u} × \mathbf{r}_{v}|} $$ The second fundamental form is usually written as $$ \mathrm{I\!I} = L\,du^{2} + 2 M\,du\,dv + N\,dv^{2} $$ where the coefficients $L$ , $M$ , and $N$ at a given point can be computed with the aid of the dot product as follows: $$ L = \mathbf{r}_{uu} ⋅ \mathbf{n}, \quad M = \mathbf{r}_{uv} ⋅ \mathbf{n}, \quad N = \mathbf{r}_{vv} ⋅ \mathbf{n} $$ The surface $S$ with which I am dealing is is a smooth, closed, convex surface parametrized as $$ \begin{alignat}{3} X(θ, φ) &= \sin(θ) ⋅ \cos(φ)\:& ⋅ &\:h(θ, φ) + \cos(θ) ⋅ \cos(φ)\:& ⋅ &\:h_θ(θ, φ) - \frac{\sin(φ)}{\sin(θ)} ⋅ h_φ(θ, φ), \\ Y(θ, φ) &= \sin(θ) ⋅ \sin(φ)\:& ⋅ &\:h(θ, φ) + \cos(θ) ⋅ \sin(φ)\:& ⋅ &\:h_θ(θ, φ) + \frac{\cos(φ)}{\sin(θ)} ⋅ h_φ(θ, φ), \\ Z(θ, φ) &= \cos(θ)\:& ⋅ &\:h(θ, φ) - \sin(θ)\:& ⋅ &\:h_θ(θ, φ), \end{alignat} $$ given a support function $h(θ, φ)$ and its partial derivatives $h_θ(θ, φ) = \frac{∂ h}{∂ θ}$ and $h_φ(θ, φ) = \frac{∂ h}{∂ φ}$ , with spherical coordinates $0 ≤ θ ≤ π$ and $0 ≤ φ ≤ 2 π$ . I also know that the unit normal vector of $S$ at a point $P(u, v) = \bigl(X(u, v), Y(u, v), Z(u, v)\bigr)$ is $$ \mathbf{n}(u, v) = \bigl(\sin(u) ⋅ \cos(v), \,\sin(u) ⋅ \sin(v), \,\cos(u)\bigr) $$ From all of the above, and with the help of WolframAlpha, I've worked out that $$ \begin{align} L &= -h(u, v) - h_{uu}(u, v), \\ M &= \cot(u) ⋅ h_{u}(u, v) - h_{uv}(u, v), \\ N &= -\sin(u)^2 ⋅ h(u, v) - h_{vv}(u, v)  - \cos(u) ⋅ \sin(u) ⋅ h_{u}(u, v) \end{align} $$ However, it's been a while since my calculus education, and I can't remember what it means to have $du$ and $dv$ in an equation like $\mathrm{I\!I} = L\,du^{2} + 2 M\,du\,dv + N\,dv^{2}$ without integral signs. I've tried searching online, but haven't found a solid answer. Are they just there as a reminder that, for example, $L$ is a product of the second derivative of $h$ with respect to $u$ , without actually indicating a mathematical operation to be carried out on $L$ ? Are they saying I should integrate, with the integration signs being implied? Or do they mean something else?","Main Question How do I evaluate the expression , given values for , , and ? Background While working on a personal project, I came upon this definition from Wikipedia for the second fundamental form (lightly paraphrased): Let be a regular parametrization of a surface in , where is a smooth vector-valued function of two variables. It is common to denote the partial derivatives of with respect to and by and . Regularity of the parametrization means that and are linearly independent for any in the domain of , and hence span the tangent plane to at each point. Equivalently, the cross product is a nonzero vector normal to the surface. The parametrization thus defines a field of unit normal vectors : The second fundamental form is usually written as where the coefficients , , and at a given point can be computed with the aid of the dot product as follows: The surface with which I am dealing is is a smooth, closed, convex surface parametrized as given a support function and its partial derivatives and , with spherical coordinates and . I also know that the unit normal vector of at a point is From all of the above, and with the help of WolframAlpha, I've worked out that However, it's been a while since my calculus education, and I can't remember what it means to have and in an equation like without integral signs. I've tried searching online, but haven't found a solid answer. Are they just there as a reminder that, for example, is a product of the second derivative of with respect to , without actually indicating a mathematical operation to be carried out on ? Are they saying I should integrate, with the integration signs being implied? Or do they mean something else?","L\,du^{2} + 2 M\,du\,dv + N\,dv^{2} L(u, v) M(u, v) N(u, v) \mathbf{r} = \mathbf{r}(u,v) S ℝ^3 \mathbf{r} \mathbf{r} u v \mathbf{r}_{u} \mathbf{r}_{v} \mathbf{r}_{u} \mathbf{r}_{v} (u,v) \mathbf{r} S \mathbf{r}_{u} × \mathbf{r}_{v} \mathbf{n} 
\mathbf{n} =\frac{\mathbf{r}_{u} × \mathbf{r}_{v}}{|\mathbf{r}_{u} × \mathbf{r}_{v}|}
 
\mathrm{I\!I} = L\,du^{2} + 2 M\,du\,dv + N\,dv^{2}
 L M N 
L = \mathbf{r}_{uu} ⋅ \mathbf{n}, \quad M = \mathbf{r}_{uv} ⋅ \mathbf{n}, \quad N = \mathbf{r}_{vv} ⋅ \mathbf{n}
 S 
\begin{alignat}{3}
X(θ, φ) &= \sin(θ) ⋅ \cos(φ)\:& ⋅ &\:h(θ, φ) + \cos(θ) ⋅ \cos(φ)\:& ⋅ &\:h_θ(θ, φ) - \frac{\sin(φ)}{\sin(θ)} ⋅ h_φ(θ, φ), \\
Y(θ, φ) &= \sin(θ) ⋅ \sin(φ)\:& ⋅ &\:h(θ, φ) + \cos(θ) ⋅ \sin(φ)\:& ⋅ &\:h_θ(θ, φ) + \frac{\cos(φ)}{\sin(θ)} ⋅ h_φ(θ, φ), \\
Z(θ, φ) &= \cos(θ)\:& ⋅ &\:h(θ, φ) - \sin(θ)\:& ⋅ &\:h_θ(θ, φ),
\end{alignat}
 h(θ, φ) h_θ(θ, φ) = \frac{∂ h}{∂ θ} h_φ(θ, φ) = \frac{∂ h}{∂ φ} 0 ≤ θ ≤ π 0 ≤ φ ≤ 2 π S P(u, v) = \bigl(X(u, v), Y(u, v), Z(u, v)\bigr) 
\mathbf{n}(u, v) = \bigl(\sin(u) ⋅ \cos(v), \,\sin(u) ⋅ \sin(v), \,\cos(u)\bigr)
 
\begin{align}
L &= -h(u, v) - h_{uu}(u, v), \\
M &= \cot(u) ⋅ h_{u}(u, v) - h_{uv}(u, v), \\
N &= -\sin(u)^2 ⋅ h(u, v) - h_{vv}(u, v)  - \cos(u) ⋅ \sin(u) ⋅ h_{u}(u, v)
\end{align}
 du dv \mathrm{I\!I} = L\,du^{2} + 2 M\,du\,dv + N\,dv^{2} L h u L","['calculus', 'multivariable-calculus']"
57,Help finding second directional derivative,Help finding second directional derivative,,"I'm studying multivariable-calculus and I'm trying to solve this question: Let $GL(n,\Bbb R)$ be the group of $n×n$ invertible matrices of real numbers now let $A=\begin{pmatrix} 0 & 1 \\ 2 & 3  \end{pmatrix}$ $B=\begin{pmatrix} 1 & 2 \\ 3 & 0  \end{pmatrix}$ $C= \begin{pmatrix} 1 & 2 \\ 0 & 3  \end{pmatrix}$ $I= \begin{pmatrix} 1 & 0 \\ 0 & 1  \end{pmatrix}$ Define $F:GL(2,\Bbb R) \to GL(2,\Bbb R)$ given by $F(X)=X^{-1}AX$ let $f:GL(2,\Bbb R) \to \Bbb R\ $ given by $\ f(X)=[F(X)]_{11}\ $ when $[F(X)]_{11}$ is the component in the first row and first column of the matrix $F(X)$ I need to find the second directional deriative $D_{B}D_{C}f(I)$ what I did: first I calculated $f(X)$ if $X= \begin{pmatrix}x & y \\z & w \end{pmatrix}\ $ then $f(X) = \frac{-2xy+z(w-3y)}{(xw-yz)}\ $ so now I tried to find an expression for the function $D_{C}f\ $ but no matter which of the 2 definition of the directional deriative the I know I have tried both of them got me stuck because I got to a point where I needed to do alot of complex calculation and I don't think that is the point of the question. The methods I know 1.with the limit definition: $\ \lim_{t\to 0}\frac{f(X+tC)-f(X)}{t}$ 2 . with the gradient : $D_{C} f = \nabla f \cdot{C}$ My question is am I right that there is an easier way to solve this question? and if so it means that my understanding of this topic is not so good, so can you refer me to some notes\videos\books that can help me get a better understanding? also if you can give me a hint on how to solve this question that will be great too","I'm studying multivariable-calculus and I'm trying to solve this question: Let be the group of invertible matrices of real numbers now let Define given by let given by when is the component in the first row and first column of the matrix I need to find the second directional deriative what I did: first I calculated if then so now I tried to find an expression for the function but no matter which of the 2 definition of the directional deriative the I know I have tried both of them got me stuck because I got to a point where I needed to do alot of complex calculation and I don't think that is the point of the question. The methods I know 1.with the limit definition: 2 . with the gradient : My question is am I right that there is an easier way to solve this question? and if so it means that my understanding of this topic is not so good, so can you refer me to some notes\videos\books that can help me get a better understanding? also if you can give me a hint on how to solve this question that will be great too","GL(n,\Bbb R) n×n A=\begin{pmatrix}
0 & 1 \\
2 & 3 
\end{pmatrix} B=\begin{pmatrix}
1 & 2 \\
3 & 0 
\end{pmatrix} C= \begin{pmatrix}
1 & 2 \\
0 & 3 
\end{pmatrix} I= \begin{pmatrix}
1 & 0 \\
0 & 1 
\end{pmatrix} F:GL(2,\Bbb R) \to GL(2,\Bbb R) F(X)=X^{-1}AX f:GL(2,\Bbb R) \to \Bbb R\  \ f(X)=[F(X)]_{11}\  [F(X)]_{11} F(X) D_{B}D_{C}f(I) f(X) X= \begin{pmatrix}x & y \\z & w \end{pmatrix}\  f(X) = \frac{-2xy+z(w-3y)}{(xw-yz)}\  D_{C}f\  \ \lim_{t\to 0}\frac{f(X+tC)-f(X)}{t} D_{C} f = \nabla f \cdot{C}","['multivariable-calculus', 'reference-request', 'partial-derivative']"
58,Is knowing unit basis vector enough to specify a coordinate system?,Is knowing unit basis vector enough to specify a coordinate system?,,"In orthonormal curvilinear coordinate system, we define unit basis vector as $$\mathbf{\hat{e}}_u = \frac{1}{h_u} \frac{d\mathbf{r}}{du},$$ where $h_u = |\frac{d\mathbf{r}}{du}|$ . Suppose we only know $\mathbf{e}_u$ in relation to Cartesian coordinates, are we able to construct coordinate system from there? Using polar coordinates $(s,\phi)$ as an example, given only the information $$\mathbf{\hat{s}}=\frac{x\mathbf{\hat{x}} + y \mathbf{\hat{y}}}{\sqrt{x^2+y^2}}, \qquad \mathbf{\hat{\phi}}=\frac{-y\mathbf{\hat{x}}+x\mathbf{\hat{y}}}{\sqrt{x^2+y^2}}, $$ arę we able to construct a coordinate system with $\mathbf{\hat{s}}$ and $\mathbf{\hat{\phi}}$ as basis unit vector? More generally, suppose I want to construct a new coordinate system $(u,v)$ but is only given $$ \mathbf{\hat{u}}=f(x,y)\mathbf{\hat{x}} + g(x,y)\mathbf{\hat{y}}, \qquad \mathbf{\hat{v}}=g(x,y)\mathbf{\hat{x}} - f(x,y)\mathbf{\hat{y}} $$ as the unit basis vector. Is there a way to find $$\mathbf{\hat{u}}=f'(u,v)\mathbf{\hat{x}} + g'(u,v)\mathbf{\hat{y}}, \qquad \mathbf{\hat{v}}=f'(u,v)\mathbf{\hat{x}} - g'(u,v)\mathbf{\hat{y}} ,$$ which gives a relationship between $x=f(u,v), y=g(u,v)$ . This will enable the computation of norm $h_u$ of tangent vector $\frac{d\mathbf{r}}{du}$ . Then, we are able to specify the differential in $(u,v)$ as $$ d\mathbf{r}= \mathbf{\hat{u}} h_u du + \mathbf{\hat{v}} h_v dv .$$","In orthonormal curvilinear coordinate system, we define unit basis vector as where . Suppose we only know in relation to Cartesian coordinates, are we able to construct coordinate system from there? Using polar coordinates as an example, given only the information arę we able to construct a coordinate system with and as basis unit vector? More generally, suppose I want to construct a new coordinate system but is only given as the unit basis vector. Is there a way to find which gives a relationship between . This will enable the computation of norm of tangent vector . Then, we are able to specify the differential in as","\mathbf{\hat{e}}_u = \frac{1}{h_u} \frac{d\mathbf{r}}{du}, h_u = |\frac{d\mathbf{r}}{du}| \mathbf{e}_u (s,\phi) \mathbf{\hat{s}}=\frac{x\mathbf{\hat{x}} + y \mathbf{\hat{y}}}{\sqrt{x^2+y^2}}, \qquad \mathbf{\hat{\phi}}=\frac{-y\mathbf{\hat{x}}+x\mathbf{\hat{y}}}{\sqrt{x^2+y^2}},  \mathbf{\hat{s}} \mathbf{\hat{\phi}} (u,v)  \mathbf{\hat{u}}=f(x,y)\mathbf{\hat{x}} + g(x,y)\mathbf{\hat{y}}, \qquad \mathbf{\hat{v}}=g(x,y)\mathbf{\hat{x}} - f(x,y)\mathbf{\hat{y}}  \mathbf{\hat{u}}=f'(u,v)\mathbf{\hat{x}} + g'(u,v)\mathbf{\hat{y}}, \qquad \mathbf{\hat{v}}=f'(u,v)\mathbf{\hat{x}} - g'(u,v)\mathbf{\hat{y}} , x=f(u,v), y=g(u,v) h_u \frac{d\mathbf{r}}{du} (u,v)  d\mathbf{r}= \mathbf{\hat{u}} h_u du + \mathbf{\hat{v}} h_v dv .","['multivariable-calculus', 'differential-geometry', 'polar-coordinates', 'change-of-basis', 'curvilinear-coordinates']"
59,Divergence theorem when $\nabla \cdot \vec{F} = 0$,Divergence theorem when,\nabla \cdot \vec{F} = 0,"Calculate the flow of the vector field $$\mathbf{F}(x, y, z) = \frac{1}{(x^2 + y^2 + z^2)^{\frac{3}{2}}} (x, y, z)$$ out of a sphere with radius $10$ and center at the origin. This what I did: $$\frac{\partial F}{\partial x} = \frac{(x^2 + y^2 + z^2)^{\frac32} - 3x^2\sqrt{x^2+y^2+z^2}}{(x^2 + y^2 + z^2)^{3}} = \frac{1-3\cos^2(\theta)\sin^2(\phi)}{r^3}$$ $$\frac{\partial F}{\partial y} = \frac{(x^2 + y^2 + z^2)^{\frac32} - 3y^2\sqrt{x^2+y^2+z^2}}{(x^2 + y^2 + z^2)^{3}} = \frac{1-3\sin^2(\theta)\sin^2(\phi)}{r^3}$$ $$\frac{\partial F}{\partial z} = \frac{(x^2 + y^2 + z^2)^{\frac32} - 3z^2\sqrt{x^2+y^2+z^2}}{(x^2 + y^2 + z^2)^{3}} = \frac{1-3\cos^2(\phi)}{r^3}$$ $$ \nabla \cdot \vec{F} = \frac{\partial F}{\partial x} + \frac{\partial F}{\partial y} + \frac{\partial F}{\partial z} = 0 $$ $$\iiint_{V} 0 dV = 0$$ But the correct answer is $4\pi$ . What am I doing wrong?",Calculate the flow of the vector field out of a sphere with radius and center at the origin. This what I did: But the correct answer is . What am I doing wrong?,"\mathbf{F}(x, y, z) = \frac{1}{(x^2 + y^2 + z^2)^{\frac{3}{2}}} (x, y, z) 10 \frac{\partial F}{\partial x} = \frac{(x^2 + y^2 + z^2)^{\frac32} - 3x^2\sqrt{x^2+y^2+z^2}}{(x^2 + y^2 + z^2)^{3}} = \frac{1-3\cos^2(\theta)\sin^2(\phi)}{r^3} \frac{\partial F}{\partial y} = \frac{(x^2 + y^2 + z^2)^{\frac32} - 3y^2\sqrt{x^2+y^2+z^2}}{(x^2 + y^2 + z^2)^{3}} = \frac{1-3\sin^2(\theta)\sin^2(\phi)}{r^3} \frac{\partial F}{\partial z} = \frac{(x^2 + y^2 + z^2)^{\frac32} - 3z^2\sqrt{x^2+y^2+z^2}}{(x^2 + y^2 + z^2)^{3}} = \frac{1-3\cos^2(\phi)}{r^3}  \nabla \cdot \vec{F} = \frac{\partial F}{\partial x} + \frac{\partial F}{\partial y} + \frac{\partial F}{\partial z} = 0  \iiint_{V} 0 dV = 0 4\pi","['calculus', 'integration', 'multivariable-calculus', 'partial-derivative', 'divergence-theorem']"
60,Sign of a function near the boundary,Sign of a function near the boundary,,"Let $\Omega \subset \mathbb R^N$ , $N \geq 2$ , be a smooth bounded open set. Let $f \in C^1(\overline \Omega)$ , and suppose that $$ f = 0, \quad \frac{\partial f}{\partial \nu} < 0 \qquad \text{on } \quad \partial \Omega, $$ where $\nu$ is the outer unit normal vector to $\partial \Omega$ . It is very intuitive that $f > 0$ in an $\varepsilon$ -neighborhood of $\partial \Omega$ , that is, $f > 0$ in the set $$ A_\varepsilon := \{x \in \Omega \ : \ d(x, \partial \Omega) < \varepsilon\} $$ for some $\varepsilon > 0$ . Nonetheless, I am failing to obtain a proof with sufficient details to really convince me. My attempt begins with taking a neighborhood of each point $x \in \partial \Omega$ where $f > 0$ and then using the compactness of $\overline \Omega$ to find the neighborhood I want. The compactness part is OK, but the beginning is kind of tricky to me. Any suggestions on how to start? Thanks in advance.","Let , , be a smooth bounded open set. Let , and suppose that where is the outer unit normal vector to . It is very intuitive that in an -neighborhood of , that is, in the set for some . Nonetheless, I am failing to obtain a proof with sufficient details to really convince me. My attempt begins with taking a neighborhood of each point where and then using the compactness of to find the neighborhood I want. The compactness part is OK, but the beginning is kind of tricky to me. Any suggestions on how to start? Thanks in advance.","\Omega \subset \mathbb R^N N \geq 2 f \in C^1(\overline \Omega) 
f = 0, \quad \frac{\partial f}{\partial \nu} < 0 \qquad \text{on } \quad \partial \Omega,
 \nu \partial \Omega f > 0 \varepsilon \partial \Omega f > 0 
A_\varepsilon := \{x \in \Omega \ : \ d(x, \partial \Omega) < \varepsilon\}
 \varepsilon > 0 x \in \partial \Omega f > 0 \overline \Omega","['real-analysis', 'multivariable-calculus']"
61,What does $\det f'(a)=0$ tell us about the function $f$ at $a$?,What does  tell us about the function  at ?,\det f'(a)=0 f a,"Let $f:\mathbb{R}^n\to\mathbb{R}^n$ be a function differentiable at $a\in \mathbb{R}^n$ .  The property that $\det f'(a)\ne 0$ is relevant in multiple theorems e.g. the Inverse Function Theorem, yet I have trouble building an intuition of what such property tells us about $f$ at $a$ . There is, of course, the geometric interpretation of determinants as signed volume, and thus  one could say that "" $\det f'(a)= 0$ tells us that $f'(a)$ i.e. the (best) linear approximation of the function $$\mathbb{R}^n\to\mathbb{R}^n : h \mapsto \Big[f(a+h)-f(a)\Big]$$ 'compresses' any $n$ -dimensional shape into a shape of lower dimension."" By itself, such explanation does not give me much of a geometric intuition of the behavior of $f$ around $a$ , and thus I was wondering if anyone could add something to the story. $$\textbf{What does $\det f'(a)=0$ tell us about the function $f$ at $a$?}$$ $$\textbf{Is there another geometric intuition corresponding to $\det f'(a)=0$?}$$","Let be a function differentiable at .  The property that is relevant in multiple theorems e.g. the Inverse Function Theorem, yet I have trouble building an intuition of what such property tells us about at . There is, of course, the geometric interpretation of determinants as signed volume, and thus  one could say that "" tells us that i.e. the (best) linear approximation of the function 'compresses' any -dimensional shape into a shape of lower dimension."" By itself, such explanation does not give me much of a geometric intuition of the behavior of around , and thus I was wondering if anyone could add something to the story.",f:\mathbb{R}^n\to\mathbb{R}^n a\in \mathbb{R}^n \det f'(a)\ne 0 f a \det f'(a)= 0 f'(a) \mathbb{R}^n\to\mathbb{R}^n : h \mapsto \Big[f(a+h)-f(a)\Big] n f a \textbf{What does \det f'(a)=0 tell us about the function f at a?} \textbf{Is there another geometric intuition corresponding to \det f'(a)=0?},"['real-analysis', 'multivariable-calculus', 'derivatives', 'soft-question', 'determinant']"
62,Using Green's formula on closed curves,Using Green's formula on closed curves,,"I want to determine the following curve integral $$ \int_{\gamma} \vec{F} \cdot d\vec{r} \qquad,\qquad \vec{F}= \bigl( \sin(y-x) , 2xy+\sin(x-y) \bigr) \quad,\quad \gamma \quad : \quad y=\sqrt{x} \quad 0 \leq x \leq 1. $$ So I thought that Green's formula would be useful, and therefore we need a closed curve to use it, i choose the following So we get $$ \int_\gamma + \int_u + \int_v \quad= \quad \iint_K  \qquad , \quad \text{K: The inside of the closed curve}. $$ For the first line segment we get the following parameterization $$ \int_u \quad : x=1 \quad \rightarrow \quad dx=0 \quad : \quad \int_1^0 \sin(t-1) \cdot 0 + 2t+\sin(1-t) \, dt = [t^2+ \cos(1-t)]_1^0 \quad=\quad \cos(1)-2 $$ And the second $$\int_v \quad : y=0 \quad \rightarrow \quad dy=0 \quad : \quad \int_1^0 \sin(0-t) \cdot dt + (0 + \sin(t-0) \cdot 0  dt = [\cos(t)]_1^0 \quad=\quad 1 - \cos(1) $$ Applying Green's formula: $$ \iint_K \bigl( \frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} \bigr) dxdy = \int_0^1 \int_0^{\sqrt{x}} 2y- \cos(x-y) + \cos(x-y) dxdy =$$ $$ \int_0^1 \int_0^{\sqrt{x}} 2y dxdy = \int_0^1 [y^2]_0^{\sqrt{x}} dx \quad=\quad \int_0^1 x dx \quad=\quad \frac{1}{2} $$ Finally we get: $$ \int_\gamma = \iint_K - \int_u - \int_v \quad=\quad \frac{1}{2} - \cos(1) + 2 - 1 + \cos (1) =  \frac{3}{2} $$ But if try another approach with the following closed curve I get for the line integral $$\int_W \quad : y=t \quad \rightarrow \quad x=t \quad : \quad \int_1^0 sin(t-t) \cdot dt + (2t^2 + sin(t-t) \cdot  dt = [\frac{2t^3}{3}]_1^0 = \frac{-2}{3} $$ Green's formula $$ \iint_K \bigl( \frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} \bigr) dxdy = \int_0^1 \int_x^{\sqrt{x}} 2y dxdy  \quad=\quad \int_0^1 [y^2]_x^{\sqrt{x}} dx  = \int_0^1 x-x^2 dx  = \frac{1}{6} $$ So we get $$ \int_\gamma = \frac{1}{6} + \frac{2}{3} = \frac{5}{6} $$ Which is not the same answer! But I should get the same answer regardless of the curve I choose right? What did I go wrong here?","I want to determine the following curve integral So I thought that Green's formula would be useful, and therefore we need a closed curve to use it, i choose the following So we get For the first line segment we get the following parameterization And the second Applying Green's formula: Finally we get: But if try another approach with the following closed curve I get for the line integral Green's formula So we get Which is not the same answer! But I should get the same answer regardless of the curve I choose right? What did I go wrong here?"," \int_{\gamma} \vec{F} \cdot d\vec{r} \qquad,\qquad \vec{F}= \bigl( \sin(y-x) , 2xy+\sin(x-y) \bigr) \quad,\quad \gamma \quad : \quad y=\sqrt{x} \quad 0 \leq x \leq 1.   \int_\gamma + \int_u + \int_v \quad= \quad \iint_K  \qquad , \quad \text{K: The inside of the closed curve}.   \int_u \quad : x=1 \quad \rightarrow \quad dx=0 \quad : \quad \int_1^0 \sin(t-1) \cdot 0 + 2t+\sin(1-t) \, dt = [t^2+ \cos(1-t)]_1^0 \quad=\quad \cos(1)-2  \int_v \quad : y=0 \quad \rightarrow \quad dy=0 \quad : \quad \int_1^0 \sin(0-t) \cdot dt + (0 + \sin(t-0) \cdot 0  dt = [\cos(t)]_1^0 \quad=\quad 1 - \cos(1)   \iint_K \bigl( \frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} \bigr) dxdy = \int_0^1 \int_0^{\sqrt{x}} 2y- \cos(x-y) + \cos(x-y) dxdy =  \int_0^1 \int_0^{\sqrt{x}} 2y dxdy = \int_0^1 [y^2]_0^{\sqrt{x}} dx \quad=\quad \int_0^1 x dx \quad=\quad \frac{1}{2}   \int_\gamma = \iint_K - \int_u - \int_v \quad=\quad \frac{1}{2} - \cos(1) + 2 - 1 + \cos (1) =  \frac{3}{2}  \int_W \quad : y=t \quad \rightarrow \quad x=t \quad : \quad \int_1^0 sin(t-t) \cdot dt + (2t^2 + sin(t-t) \cdot  dt = [\frac{2t^3}{3}]_1^0 = \frac{-2}{3}   \iint_K \bigl( \frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} \bigr) dxdy = \int_0^1 \int_x^{\sqrt{x}} 2y dxdy  \quad=\quad \int_0^1 [y^2]_x^{\sqrt{x}} dx  = \int_0^1 x-x^2 dx  = \frac{1}{6}   \int_\gamma = \frac{1}{6} + \frac{2}{3} = \frac{5}{6} ","['integration', 'multivariable-calculus', 'curves', 'multiple-integral']"
63,How to minimize $(x-1)^2+(y-1)^2+(z-1)^2$ under the constraint $xyz=s$?,How to minimize  under the constraint ?,(x-1)^2+(y-1)^2+(z-1)^2 xyz=s,"Let $s \in (0,1)$ be a parameter. Can we find an exact closed form expression for $$ F(s)=\min_{xyz=s,x,y,z>0}(x-1)^2+(y-1)^2+(z-1)^2, \tag{1} $$ and exact formulas the minimizers $x(s) \le y(s) \le z(s)$ ? I tried using Mathematica but failed (However I am not very skillful). Using Lagrange's multipliers, one gets $$ (x-1,y-1,z-1)=\lambda (\frac{1}{x},\frac{1}{y},\frac{1}{z}), $$ so $x,y,z$ should all satisfy the quadratic equation $t(1-t)=-\lambda$ . If $t$ is a solution then so is $1-t$ ; thus, if we denote the solution of this equation by $a$ , then $$ \{x,y,z\} \subseteq \{a,1-a \}. $$ One possible solution is the symmetric solution $x=y=z=\sqrt[3] s$ . If $x,y,z$ do not all coincide, then they must take both values $a$ and $1-a$ . Since we required $x,y,z>0$ this means that $a,1-a$ should be positive, so $0<a<1$ . After a possible a renaming/switching, we may assume that $x=a, y=z=1-a$ , so the value $1-a$ is attained twice. So, if we define, $$ G(s)=\min_{a \in (0,1),a(1-a)^{2}=s} (1-a)^2+2a^2, \tag{2} $$ then $$ F(s)=\min \{3(1-\sqrt[3]s)^2,G(s)\}. $$ Since $\max_{a \in (0,1)}a(1-a)^{2}=4/27$ , it follows that the non-symmetric solution is possible only if $s \le 4/27$ . The term $3(1-\sqrt[3]s)^2$ comes from comparing with the symmetric solution $x=y=s$ . I am not sure how to continue, since analyzing explicitly the solutions to the quintic equation is not so nice . Motivation: This problem is a special case of the problem of finding the closest matrix to $\text{SO}_n$ with a given determinant.","Let be a parameter. Can we find an exact closed form expression for and exact formulas the minimizers ? I tried using Mathematica but failed (However I am not very skillful). Using Lagrange's multipliers, one gets so should all satisfy the quadratic equation . If is a solution then so is ; thus, if we denote the solution of this equation by , then One possible solution is the symmetric solution . If do not all coincide, then they must take both values and . Since we required this means that should be positive, so . After a possible a renaming/switching, we may assume that , so the value is attained twice. So, if we define, then Since , it follows that the non-symmetric solution is possible only if . The term comes from comparing with the symmetric solution . I am not sure how to continue, since analyzing explicitly the solutions to the quintic equation is not so nice . Motivation: This problem is a special case of the problem of finding the closest matrix to with a given determinant.","s \in (0,1) 
F(s)=\min_{xyz=s,x,y,z>0}(x-1)^2+(y-1)^2+(z-1)^2, \tag{1}
 x(s) \le y(s) \le z(s) 
(x-1,y-1,z-1)=\lambda (\frac{1}{x},\frac{1}{y},\frac{1}{z}),
 x,y,z t(1-t)=-\lambda t 1-t a 
\{x,y,z\} \subseteq \{a,1-a \}.
 x=y=z=\sqrt[3] s x,y,z a 1-a x,y,z>0 a,1-a 0<a<1 x=a, y=z=1-a 1-a 
G(s)=\min_{a \in (0,1),a(1-a)^{2}=s} (1-a)^2+2a^2, \tag{2}
 
F(s)=\min \{3(1-\sqrt[3]s)^2,G(s)\}.
 \max_{a \in (0,1)}a(1-a)^{2}=4/27 s \le 4/27 3(1-\sqrt[3]s)^2 x=y=s \text{SO}_n","['multivariable-calculus', 'optimization', 'nonlinear-optimization', 'lagrange-multiplier', 'symmetry']"
64,Motion along a circle,Motion along a circle,,"The problem below is from the textbook I'm reading from. I'm also stating the solution to the problem after the problem statement. Someone kindly verify the legitimacy of my solution. Problem Statement : Show that the vector valued function $$  \textbf{r}(t)=2\textbf{i}+2\textbf{j}+\textbf{k}+\cos  t\left(\frac{1}{\sqrt{2}}\textbf{i}-\frac{1}{\sqrt{2}}\textbf{j}\right)+\sin  t\left(\frac{1}{\sqrt{3}}\textbf{i}+\frac{1}{\sqrt{3}}\textbf{j}+\frac{1}{\sqrt{3}}\textbf{k}\right)  $$ describes the motion of a particle moving in the circle of radius $1$ , centered at $(2,2,1)$ . My Solution : Part 1 : Proving $\textbf{r}(t)$ represents a circular path. Let us for now assume this function indeed defines a circular path and let the center be $(a,b,c)$ . Let $\textbf{R}(t)$ be the radius vector from the center to any point on the curve $\textbf{r}(t)$ . Then $$ \textbf{R}(t)=\textbf{r}(t)-\langle a,b,c \rangle=\left\langle 2-a+\frac{\cos t}{\sqrt{2}}+\frac{\sin t}{\sqrt{3}}, 2-b-\frac{\cos t}{\sqrt{2}}+\frac{\sin t}{\sqrt{3}},1-c+\frac{\sin t}{\sqrt{3}} \right\rangle $$ The tangent vector to $\textbf{r}(t)$ is obtained by differentiating $\textbf{r}(t)$ . Let it be $\textbf{v}(t)$ . $$ \textbf{v}(t)=\textbf{r}'(t)=\left\langle -\frac{\sin t}{\sqrt{2}}+\frac{\cos t}{\sqrt{3}}, \frac{\sin t}{\sqrt{2}}+\frac{\cos t}{\sqrt{3}},\frac{\cos t}{\sqrt{3}} \right\rangle $$ If $\textbf{r}(t)$ represents a circular path then $\textbf{R}(t)\cdot\textbf{v}(t)=0$ . This gives us $$ \textbf{R}(t)\cdot\textbf{v}(t)=\left(\frac{a-b}{\sqrt{2}}\right)\sin t + \left(\frac{5-a-b-c}{\sqrt{3}}\right)\cos t=0 $$ only if $$ a=b \text{ & } a+b+c=5 $$ We can see that $(a,b,c)$ exist that can satisfy this condition and hence $\textbf{r}(t)$ represents a circular path. Part 2 : Finding the center. Since $\textbf{r}(t)$ is a circular locus, the radius vector's magnitude would be constant, i.e $|\textbf{R}(t)|=k$ . This means $$ \frac{d}{dt}|\textbf{R}(t)|=0 \implies \frac{d}{dt}|\textbf{R}(t)|^2=0 $$ Doing differentiation and some algebra gives us $$ |\textbf{R}(t)|\frac{d}{dt}|\textbf{R}(t)|=\left(\frac{a-b}{\sqrt{2}}\right)\sin t+\left(\frac{4-a-b}{\sqrt{3}}\right)\cos t=0 $$ From part 1, we know that $a=b$ and $a+b+c=5$ . Using this we get $$ \left(\frac{4-2a}{\sqrt{3}}\right)\cos t=0 \implies a=2 \implies b=2 \implies c=1. $$ Part 3 : Finding radius Substituting values of $a$ , $b$ and $c$ into the expression for $|\textbf{R}(t)|$ gives $|\textbf{R}(t)|=1$ . I've spared the algebra. This completes my solution/proof. My main question In Part 1, I have assumed $\textbf{r}(t)$ to be a circular path and found that real numbers $a$ , $b$ and $c$ exist that satisfy the condition for $\textbf{r}(t)$ to be circle: that radius is perpendicular to tangent. If $\textbf{r}(t)$ wasn't a circle, then $a$ and/or $b$ and/or $c$ wouldn't exist. Is this argument watertight? I think so because there is no other curve that satisfies this condition, at least as far as I know.","The problem below is from the textbook I'm reading from. I'm also stating the solution to the problem after the problem statement. Someone kindly verify the legitimacy of my solution. Problem Statement : Show that the vector valued function describes the motion of a particle moving in the circle of radius , centered at . My Solution : Part 1 : Proving represents a circular path. Let us for now assume this function indeed defines a circular path and let the center be . Let be the radius vector from the center to any point on the curve . Then The tangent vector to is obtained by differentiating . Let it be . If represents a circular path then . This gives us only if We can see that exist that can satisfy this condition and hence represents a circular path. Part 2 : Finding the center. Since is a circular locus, the radius vector's magnitude would be constant, i.e . This means Doing differentiation and some algebra gives us From part 1, we know that and . Using this we get Part 3 : Finding radius Substituting values of , and into the expression for gives . I've spared the algebra. This completes my solution/proof. My main question In Part 1, I have assumed to be a circular path and found that real numbers , and exist that satisfy the condition for to be circle: that radius is perpendicular to tangent. If wasn't a circle, then and/or and/or wouldn't exist. Is this argument watertight? I think so because there is no other curve that satisfies this condition, at least as far as I know.","
 \textbf{r}(t)=2\textbf{i}+2\textbf{j}+\textbf{k}+\cos
 t\left(\frac{1}{\sqrt{2}}\textbf{i}-\frac{1}{\sqrt{2}}\textbf{j}\right)+\sin
 t\left(\frac{1}{\sqrt{3}}\textbf{i}+\frac{1}{\sqrt{3}}\textbf{j}+\frac{1}{\sqrt{3}}\textbf{k}\right)
  1 (2,2,1) \textbf{r}(t) (a,b,c) \textbf{R}(t) \textbf{r}(t) 
\textbf{R}(t)=\textbf{r}(t)-\langle a,b,c \rangle=\left\langle 2-a+\frac{\cos t}{\sqrt{2}}+\frac{\sin t}{\sqrt{3}}, 2-b-\frac{\cos t}{\sqrt{2}}+\frac{\sin t}{\sqrt{3}},1-c+\frac{\sin t}{\sqrt{3}} \right\rangle
 \textbf{r}(t) \textbf{r}(t) \textbf{v}(t) 
\textbf{v}(t)=\textbf{r}'(t)=\left\langle -\frac{\sin t}{\sqrt{2}}+\frac{\cos t}{\sqrt{3}}, \frac{\sin t}{\sqrt{2}}+\frac{\cos t}{\sqrt{3}},\frac{\cos t}{\sqrt{3}} \right\rangle
 \textbf{r}(t) \textbf{R}(t)\cdot\textbf{v}(t)=0 
\textbf{R}(t)\cdot\textbf{v}(t)=\left(\frac{a-b}{\sqrt{2}}\right)\sin t + \left(\frac{5-a-b-c}{\sqrt{3}}\right)\cos t=0
 
a=b \text{ & } a+b+c=5
 (a,b,c) \textbf{r}(t) \textbf{r}(t) |\textbf{R}(t)|=k 
\frac{d}{dt}|\textbf{R}(t)|=0 \implies \frac{d}{dt}|\textbf{R}(t)|^2=0
 
|\textbf{R}(t)|\frac{d}{dt}|\textbf{R}(t)|=\left(\frac{a-b}{\sqrt{2}}\right)\sin t+\left(\frac{4-a-b}{\sqrt{3}}\right)\cos t=0
 a=b a+b+c=5 
\left(\frac{4-2a}{\sqrt{3}}\right)\cos t=0 \implies a=2 \implies b=2 \implies c=1.
 a b c |\textbf{R}(t)| |\textbf{R}(t)|=1 \textbf{r}(t) a b c \textbf{r}(t) \textbf{r}(t) a b c","['multivariable-calculus', 'solution-verification', 'vectors', 'vector-analysis']"
65,"Volume above a cone and within a sphere, using triple integrals and cylindrical polar coordinates","Volume above a cone and within a sphere, using triple integrals and cylindrical polar coordinates",,"Consider the region above the cone $z = \sqrt{x^2+y^2}$ and inside the sphere $x^2+y^2+z^2 = 16$ . Use cylindrical polar coordinates to show that the volume of region $R$ is $\frac{64\pi}{3}(2-\sqrt{2})$ . To solve this, I took the limits of $\theta$ as $0$ to $2π$ . I then took the limits of $z$ by substituting $z^2 = x^2+y^2$ (cone) into the equation of the sphere, giving me $z = 2\sqrt{2}$ as my upper bound (and $0$ as the lower bound). I finally then took the limits of $r$ by changing the equation of the sphere into $r = \sqrt{16-z^2}$ (using $r^2 = x^2+y^2$ ), and $r = z$ (using $r^2 = x^2+y^2 = z^2$ , the equation of the cone). This gave me $$\int_{0}^{2\pi}\int_{0}^{2\sqrt{2}}\int_{z}^{\sqrt{16-z^{2}}}rdrdzd\theta$$ ""∫2π 0 ∫2√2 0 ∫(√16-Z^2) Z  rdrdZdθ"" However, the answer I got from this integral was $2π(16√2 - 16/3(√2))$ , which was incorrect.","Consider the region above the cone and inside the sphere . Use cylindrical polar coordinates to show that the volume of region is . To solve this, I took the limits of as to . I then took the limits of by substituting (cone) into the equation of the sphere, giving me as my upper bound (and as the lower bound). I finally then took the limits of by changing the equation of the sphere into (using ), and (using , the equation of the cone). This gave me ""∫2π 0 ∫2√2 0 ∫(√16-Z^2) Z  rdrdZdθ"" However, the answer I got from this integral was , which was incorrect.",z = \sqrt{x^2+y^2} x^2+y^2+z^2 = 16 R \frac{64\pi}{3}(2-\sqrt{2}) \theta 0 2π z z^2 = x^2+y^2 z = 2\sqrt{2} 0 r r = \sqrt{16-z^2} r^2 = x^2+y^2 r = z r^2 = x^2+y^2 = z^2 \int_{0}^{2\pi}\int_{0}^{2\sqrt{2}}\int_{z}^{\sqrt{16-z^{2}}}rdrdzd\theta 2π(16√2 - 16/3(√2)),"['calculus', 'integration', 'multivariable-calculus', 'polar-coordinates', 'volume']"
66,Chain rule and generalized composition of multilinear maps,Chain rule and generalized composition of multilinear maps,,"I know that if functions $f : \mathbb{R}^n \to \mathbb{R}^m$ and $g : \mathbb{R}^m \to \mathbb{R}^p$ are differentiable at $x \in \mathbb{R}^n$ and $f(x) \in \mathbb{R}^m$ , respectively, with derivative values $Df(x) \in L(\mathbb{R}^n, \mathbb{R}^m)$ and $Dg(f(x)) \in L(\mathbb{R}^m, \mathbb{R}^p)$ , then it is possible to define composition function $(g \circ f) : \mathbb{R}^n \to \mathbb{R}^p$ for all $x \in \mathbb{R}^n$ as $(g \circ f)(x) = g(f(x))$ and show that it is also differentiable with a derivative, where for visual simplicity I use square brackets for evaluation of a linear function: $$ D(g \circ f)(x) = Dg(f(x)) \circ Df(x)$$ Now, my problem is how to generalize this result to higher order derivative and multilinear maps. To see where the problem arises, let me consider the second derivative. First, motivated by the result of the chain rule, define functions $Dg \circ f : \mathbb{R}^n \to L(\mathbb{R}^m, \mathbb{R}^p)$ for all $x \in \mathbb{R}^n$ as $(Dg \circ f)(x) = Dg(f(x))$ , and $Df : \mathbb{R}^n \to L(\mathbb{R}^n, \mathbb{R}^m)$ for all $x \in \mathbb{R}^n$ as $Df(x)$ . Then, define composition of linear functions, where $x \in \mathbb{R}^n$ and $A,B$ are from correct $L$ spaces, as $(A \circ B)(x) = A(x) \circ B(x)$ . All of these definitions are made such that the chain rule for all $x \in \mathbb{R}^n$ can be written as follows. $$ D(g \circ f)(x) = ((Dg \circ f) \circ Df)(x) $$ Then, to motivate what the second derivative should be, consider the difference of $D(g \circ f)(x)$ , where we would like to use that $Dg \circ f$ and $Df$ are also differentiable. This means that for $y$ close to $x$ we have: $$(Dg \circ f)(y) - (Dg \circ f)(x) \approx D(Dg \circ f)(x)[y-x]$$ $$Df(y) - Df(x) \approx D(Df)(x)[y-x] $$ Here, $D(Dg \circ f)(x) \in L(\mathbb{R}^n, L(\mathbb{R}^m, \mathbb{R}^p)) =: L^2(\mathbb{R}^n, \mathbb{R}^m, \mathbb{R}^p)$ and $D(Df)(x) \in L(\mathbb{R}^n, L(\mathbb{R}^n, \mathbb{R}^m)) =: L^2(\mathbb{R}^n, \mathbb{R}^n, \mathbb{R}^m)$ . So, intuition for the second derivative chain rule formula would be as follows. $$ D(g \circ f)(y) - D(g \circ f)(x) = (Dg \circ f)(y) \circ Df(y) - (Dg \circ f)(x) \circ Df(x) =  ((Dg \circ f)(y) - (Dg \circ f)(x)) \circ Df(y) + (Dg \circ f)(x) \circ (Df(y) - Df(x)) \approx D(Dg \circ f)(x)[y-x] \circ Df(x) + (Dg \circ f)(x) \circ D(Df)(x)[y-x]$$ On the other hand, we want the previous equation to be equal to $D(D(g \circ f))(x)[y-x]$ . Therefore, I feel that one is motivated to define the following for appropriate multilinear maps $A,B$ , which would be what I call ""generalized composition"": $$ (A \circ_{L^2, L^1} B)[x] := A[x] \circ B,$$ $$ (A \circ_{L^1, L^2} B)[x] := A \circ B[x].$$ Then, the result would be written as follows. $$ D(D(g \circ f))(x) = D(Dg \circ f)(x) \circ_{L^2, L^1} Df(x) + (Dg \circ f)(x) \circ_{L^1, L^2} D(Df)(x)$$ However, if I now consider third derivative of $g \circ f$ , this approach seems to ""fail"" in the following way. If $A, B$ are both bilinear functions, so, belong to some $L^2$ , there is not a unique extension of my approach to generalized composition, as both are possible but I believe they are not equivalent. $$ (A \circ_{L^2, L^2} B)[x] = A[x] \circ_{L^1, L^2} B, $$ $$ (A \circ_{L^2, L^2} B)[x] = A \circ_{L^2, L^1} B[x].$$ Question : How to define and motivate unique generalized composition between multilinear functions so that it naturally appears in higher order chain rule results and that is well defined?","I know that if functions and are differentiable at and , respectively, with derivative values and , then it is possible to define composition function for all as and show that it is also differentiable with a derivative, where for visual simplicity I use square brackets for evaluation of a linear function: Now, my problem is how to generalize this result to higher order derivative and multilinear maps. To see where the problem arises, let me consider the second derivative. First, motivated by the result of the chain rule, define functions for all as , and for all as . Then, define composition of linear functions, where and are from correct spaces, as . All of these definitions are made such that the chain rule for all can be written as follows. Then, to motivate what the second derivative should be, consider the difference of , where we would like to use that and are also differentiable. This means that for close to we have: Here, and . So, intuition for the second derivative chain rule formula would be as follows. On the other hand, we want the previous equation to be equal to . Therefore, I feel that one is motivated to define the following for appropriate multilinear maps , which would be what I call ""generalized composition"": Then, the result would be written as follows. However, if I now consider third derivative of , this approach seems to ""fail"" in the following way. If are both bilinear functions, so, belong to some , there is not a unique extension of my approach to generalized composition, as both are possible but I believe they are not equivalent. Question : How to define and motivate unique generalized composition between multilinear functions so that it naturally appears in higher order chain rule results and that is well defined?","f : \mathbb{R}^n \to \mathbb{R}^m g : \mathbb{R}^m \to \mathbb{R}^p x \in \mathbb{R}^n f(x) \in \mathbb{R}^m Df(x) \in L(\mathbb{R}^n, \mathbb{R}^m) Dg(f(x)) \in L(\mathbb{R}^m, \mathbb{R}^p) (g \circ f) : \mathbb{R}^n \to \mathbb{R}^p x \in \mathbb{R}^n (g \circ f)(x) = g(f(x))  D(g \circ f)(x) = Dg(f(x)) \circ Df(x) Dg \circ f : \mathbb{R}^n \to L(\mathbb{R}^m, \mathbb{R}^p) x \in \mathbb{R}^n (Dg \circ f)(x) = Dg(f(x)) Df : \mathbb{R}^n \to L(\mathbb{R}^n, \mathbb{R}^m) x \in \mathbb{R}^n Df(x) x \in \mathbb{R}^n A,B L (A \circ B)(x) = A(x) \circ B(x) x \in \mathbb{R}^n  D(g \circ f)(x) = ((Dg \circ f) \circ Df)(x)  D(g \circ f)(x) Dg \circ f Df y x (Dg \circ f)(y) - (Dg \circ f)(x) \approx D(Dg \circ f)(x)[y-x] Df(y) - Df(x) \approx D(Df)(x)[y-x]  D(Dg \circ f)(x) \in L(\mathbb{R}^n, L(\mathbb{R}^m, \mathbb{R}^p)) =: L^2(\mathbb{R}^n, \mathbb{R}^m, \mathbb{R}^p) D(Df)(x) \in L(\mathbb{R}^n, L(\mathbb{R}^n, \mathbb{R}^m)) =: L^2(\mathbb{R}^n, \mathbb{R}^n, \mathbb{R}^m)  D(g \circ f)(y) - D(g \circ f)(x) = (Dg \circ f)(y) \circ Df(y) - (Dg \circ f)(x) \circ Df(x) =  ((Dg \circ f)(y) - (Dg \circ f)(x)) \circ Df(y) + (Dg \circ f)(x) \circ (Df(y) - Df(x)) \approx D(Dg \circ f)(x)[y-x] \circ Df(x) + (Dg \circ f)(x) \circ D(Df)(x)[y-x] D(D(g \circ f))(x)[y-x] A,B  (A \circ_{L^2, L^1} B)[x] := A[x] \circ B,  (A \circ_{L^1, L^2} B)[x] := A \circ B[x].  D(D(g \circ f))(x) = D(Dg \circ f)(x) \circ_{L^2, L^1} Df(x) + (Dg \circ f)(x) \circ_{L^1, L^2} D(Df)(x) g \circ f A, B L^2  (A \circ_{L^2, L^2} B)[x] = A[x] \circ_{L^1, L^2} B,   (A \circ_{L^2, L^2} B)[x] = A \circ_{L^2, L^1} B[x].","['multivariable-calculus', 'derivatives', 'frechet-derivative']"
67,Riemann Integral vs. Scalar Field Integral vs. Vector Field Integral (In single variable case),Riemann Integral vs. Scalar Field Integral vs. Vector Field Integral (In single variable case),,"I am a bit confused about the relation between these three integrals. As my textbook doesn't seem to draw a very clear distinction between them. [this is somewhat related to this question Are regular/Riemann integrals a special case of a line integral? ] Suppose there is a single-variable real-valued function $y=f(x)$ The Riemann integral is then: $$\int_a^bf(x)dx,\ \ \ \text{where$\int_a^b=-\int_b^a$}$$ The Vector Field line integral is then: $$\int_a^bf(x)\vec j\cdot \vec idx=0,\ \ \ \text{where$\int_a^b=-\int_b^a$}$$ The Scalar Field line integral is then: $$\int_a^bf(x)|dx|,\ \ \ \text{where$\int_a^b=\int_b^a$}$$ The Scalar Field Integral has the same absolute value as the Riemann Integral, but different in the property $\int_a^b=-\int_b^a$ The Vector Field Integral has the same property $\int_a^b=-\int_b^a$ as the Riemann Integral, but different in the absolute value (in this particular case, the value of vector integral is zero). [Q1]: Are these three types of integral inherently different things? Or they can be treated as special cases of a generalized form [Q2]: When parametrize a two variable scalar field line integral. $$\int_Cf(x,y)ds=\int_a^bf(t)\left\| {\,\vec r'\left( t \right)} \right\|dt$$ My textbook assumed that the parametrized integral is a Riemann integral, but why?","I am a bit confused about the relation between these three integrals. As my textbook doesn't seem to draw a very clear distinction between them. [this is somewhat related to this question Are regular/Riemann integrals a special case of a line integral? ] Suppose there is a single-variable real-valued function The Riemann integral is then: The Vector Field line integral is then: The Scalar Field line integral is then: The Scalar Field Integral has the same absolute value as the Riemann Integral, but different in the property The Vector Field Integral has the same property as the Riemann Integral, but different in the absolute value (in this particular case, the value of vector integral is zero). [Q1]: Are these three types of integral inherently different things? Or they can be treated as special cases of a generalized form [Q2]: When parametrize a two variable scalar field line integral. My textbook assumed that the parametrized integral is a Riemann integral, but why?","y=f(x) \int_a^bf(x)dx,\ \ \ \text{where\int_a^b=-\int_b^a} \int_a^bf(x)\vec j\cdot \vec idx=0,\ \ \ \text{where\int_a^b=-\int_b^a} \int_a^bf(x)|dx|,\ \ \ \text{where\int_a^b=\int_b^a} \int_a^b=-\int_b^a \int_a^b=-\int_b^a \int_Cf(x,y)ds=\int_a^bf(t)\left\| {\,\vec r'\left( t \right)} \right\|dt","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
68,Why is this substitution in the double integral effective?,Why is this substitution in the double integral effective?,,"I'm working with double integrals, and came about a problem where I had to calculate the following integral: $$ \int \int_D \left | 3x+4y \right|dx dy$$ where D is the region contained within the circle whose equation is given by $x^2+y^2 \leq 2$ . At first I thought I directly might use polar coordinates, but that seemed to complicate things when we have absolute values in the integrand. I went to check on if there was any hints to the problem, and there was. The book hinted that a substitution of the form $$\begin{pmatrix} u \\v \end{pmatrix} = \begin{pmatrix}3/5 & 4/5 \\-4/5& 3/5 \\\end{pmatrix} \begin{pmatrix}x \\y \end{pmatrix}$$ And when I made that substitution, I realized that the new region, let's call it $A$ became $u^2+v^2 \leq 2$ , the Jacobian became 1, and the double integral became: $$\int \int_A 5 |u| du dv$$ which then easily can be calculated through polar coordinates. It was the first time seeing the substitution, so I tried to generalize it for a double integral of the form: $$I := \int \int_D \left | \alpha x +\beta y \right | dx dy$$ where $D$ is a region given by the circle whose equation is $x^2+y^2 \leq \gamma^2$ . I later used the same method. First I let: $$\begin{pmatrix} u \\v \end{pmatrix} = \frac{1}{||(\alpha, \beta)||}\begin{pmatrix}\alpha & \beta \\- \beta& \alpha \\\end{pmatrix} \begin{pmatrix}x \\y \end{pmatrix}$$ Then, as previously, I get the equivalent region $A$ given by $u^2+v^2\leq \gamma^2$ , and the Jacobian $J = 1$ , just as in the previous numerical example. We also realize that $\left| \alpha x +\beta y \right | = ||(\alpha, \beta)|| \cdot|u|$ , and so our double integral just becomes: $$||(\alpha, \beta)|| \int \int_A |u| dudv$$ which then, through polar coordinates, gives us that $I$ can be evaluated to $\frac{4 ||(\alpha, \beta)|| \gamma^3}{3}$ . What I realized from this, is that this substitution is really efficient for calculating double integrals with the given integrand. However, I can't really geometrically see how these transformations look like. I think that if someone could provide any geometrical intution on what these transformations look like between the $xy$ - plane, to the $uv$ - plane and to the $r\theta$ - plane, this would really deepen my understanding on why this substitution is efficient in the way it is. Thank you in advance for any such contributions.","I'm working with double integrals, and came about a problem where I had to calculate the following integral: where D is the region contained within the circle whose equation is given by . At first I thought I directly might use polar coordinates, but that seemed to complicate things when we have absolute values in the integrand. I went to check on if there was any hints to the problem, and there was. The book hinted that a substitution of the form And when I made that substitution, I realized that the new region, let's call it became , the Jacobian became 1, and the double integral became: which then easily can be calculated through polar coordinates. It was the first time seeing the substitution, so I tried to generalize it for a double integral of the form: where is a region given by the circle whose equation is . I later used the same method. First I let: Then, as previously, I get the equivalent region given by , and the Jacobian , just as in the previous numerical example. We also realize that , and so our double integral just becomes: which then, through polar coordinates, gives us that can be evaluated to . What I realized from this, is that this substitution is really efficient for calculating double integrals with the given integrand. However, I can't really geometrically see how these transformations look like. I think that if someone could provide any geometrical intution on what these transformations look like between the - plane, to the - plane and to the - plane, this would really deepen my understanding on why this substitution is efficient in the way it is. Thank you in advance for any such contributions."," \int \int_D \left | 3x+4y \right|dx dy x^2+y^2 \leq 2 \begin{pmatrix}
u \\v \end{pmatrix} = \begin{pmatrix}3/5 & 4/5 \\-4/5& 3/5 \\\end{pmatrix} \begin{pmatrix}x \\y \end{pmatrix} A u^2+v^2 \leq 2 \int \int_A 5 |u| du dv I := \int \int_D \left | \alpha x +\beta y \right | dx dy D x^2+y^2 \leq \gamma^2 \begin{pmatrix}
u \\v \end{pmatrix} = \frac{1}{||(\alpha, \beta)||}\begin{pmatrix}\alpha & \beta \\- \beta& \alpha \\\end{pmatrix} \begin{pmatrix}x \\y \end{pmatrix} A u^2+v^2\leq \gamma^2 J = 1 \left| \alpha x +\beta y \right | = ||(\alpha, \beta)|| \cdot|u| ||(\alpha, \beta)|| \int \int_A |u| dudv I \frac{4 ||(\alpha, \beta)|| \gamma^3}{3} xy uv r\theta","['integration', 'multivariable-calculus', 'linear-transformations']"
69,Deriving Finsler geodesic equations from the energy functional,Deriving Finsler geodesic equations from the energy functional,,"I'm struggling to derive the Finsler geodesic equations. The books I know either skip the computation or use the length functional directly. I want to use the energy. Let $(M,F)$ be a Finsler manifold and consider the energy functional $$E[\gamma] = \frac{1}{2}\int_I F^2_{\gamma(t)}(\dot{\gamma}(t))\,{\rm d}t\tag{1}$$ evaluated along a (regular) curve $\gamma\colon I \to M$ . We use tangent coordinates $(x^1,\ldots,x^n,v^1,\ldots, v^n)$ on $TM$ and write $g_{ij}(x,v)$ for the components of the fundamental tensor of $(M,F)$ . We may take for granted (using Einstein's convention) that $$F^2_x(v) = g_{ij}(x,v)v^iv^j, \quad \frac{1}{2}\frac{\partial F^2}{\partial v^i}(x,v) = g_{ij}(x,v)v^j, \quad\frac{\partial g_{ij}}{\partial v^k}(x,v)v^k = 0.\tag{2} $$ Setting $L(x,v) = (1/2) F_x^2(v)$ , and writing $(\gamma(t),\dot{\gamma}(t)) \sim (x(t),v(t))$ , the Euler-Lagrange equations are $$0 = \frac{{\rm d}}{{\rm d}t}\left(\frac{\partial L}{\partial v^k}(x(t),v(t))\right) -\frac{\partial L}{\partial x^k}(x(t),v(t)),\quad k=1,\ldots, n=\dim(M).\tag{3}$$ It's easy to see (omitting application points) that $$\frac{\partial L}{\partial x^k} = \frac{1}{2}\frac{\partial g_{ij}}{\partial x^k}\dot{x}^i\dot{x}^j\quad\mbox{and}\quad \frac{\partial L}{\partial v^k} = g_{ik}\dot{x}^i,\tag{4}$$ so $$\frac{\rm d}{{\rm d}t}\left(\frac{\partial L}{\partial v^k}\right) = \frac{\partial g_{ik}}{\partial x^j}\dot{x}^j\dot{x}^i +{\color{red}{ \frac{\partial g_{ik}}{\partial v^j} \ddot{x}^j\dot{x}^i }}+ g_{ik}\ddot{x}^i\tag{5}$$ Problem: I cannot see for the life of me how to get rid of these $v^j$ -derivatives indicated in red, even using the last relation in (2), as the indices simply don't match. I am surely missing something obvious. Once we know that this term does vanish, then (4) and (5) combine to give $$ g_{ik}\ddot{x}^i + \left(\frac{\partial g_{ik}}{\partial x^j} - \frac{1}{2}\frac{\partial g_{ij}}{\partial x^k}\right)\dot{x}^i\dot{x}^j =0\tag{6}$$ as in the Wikipedia page .","I'm struggling to derive the Finsler geodesic equations. The books I know either skip the computation or use the length functional directly. I want to use the energy. Let be a Finsler manifold and consider the energy functional evaluated along a (regular) curve . We use tangent coordinates on and write for the components of the fundamental tensor of . We may take for granted (using Einstein's convention) that Setting , and writing , the Euler-Lagrange equations are It's easy to see (omitting application points) that so Problem: I cannot see for the life of me how to get rid of these -derivatives indicated in red, even using the last relation in (2), as the indices simply don't match. I am surely missing something obvious. Once we know that this term does vanish, then (4) and (5) combine to give as in the Wikipedia page .","(M,F) E[\gamma] = \frac{1}{2}\int_I F^2_{\gamma(t)}(\dot{\gamma}(t))\,{\rm d}t\tag{1} \gamma\colon I \to M (x^1,\ldots,x^n,v^1,\ldots, v^n) TM g_{ij}(x,v) (M,F) F^2_x(v) = g_{ij}(x,v)v^iv^j, \quad \frac{1}{2}\frac{\partial F^2}{\partial v^i}(x,v) = g_{ij}(x,v)v^j, \quad\frac{\partial g_{ij}}{\partial v^k}(x,v)v^k = 0.\tag{2}  L(x,v) = (1/2) F_x^2(v) (\gamma(t),\dot{\gamma}(t)) \sim (x(t),v(t)) 0 = \frac{{\rm d}}{{\rm d}t}\left(\frac{\partial L}{\partial v^k}(x(t),v(t))\right) -\frac{\partial L}{\partial x^k}(x(t),v(t)),\quad k=1,\ldots, n=\dim(M).\tag{3} \frac{\partial L}{\partial x^k} = \frac{1}{2}\frac{\partial g_{ij}}{\partial x^k}\dot{x}^i\dot{x}^j\quad\mbox{and}\quad \frac{\partial L}{\partial v^k} = g_{ik}\dot{x}^i,\tag{4} \frac{\rm d}{{\rm d}t}\left(\frac{\partial L}{\partial v^k}\right) = \frac{\partial g_{ik}}{\partial x^j}\dot{x}^j\dot{x}^i +{\color{red}{ \frac{\partial g_{ik}}{\partial v^j} \ddot{x}^j\dot{x}^i }}+ g_{ik}\ddot{x}^i\tag{5} v^j  g_{ik}\ddot{x}^i + \left(\frac{\partial g_{ik}}{\partial x^j} - \frac{1}{2}\frac{\partial g_{ij}}{\partial x^k}\right)\dot{x}^i\dot{x}^j =0\tag{6}","['multivariable-calculus', 'differential-geometry', 'euler-lagrange-equation', 'finsler-geometry']"
70,"Optimal value in $\max_{x} \max_{y} f(x,y) = \max_{x,\ y} f(x,y)$",Optimal value in,"\max_{x} \max_{y} f(x,y) = \max_{x,\ y} f(x,y)","I am trying to understand a few things about sequential and simultaneous optimization in [1] . In this post, it is shown that $$\max_{x} \max_{y} f(x,y) = \max_{x,\ y} f(x,y).\tag{1}$$ Thanks to @Shiv Tavker comment, I understand that in order to get the optimal value $z^*=(x^*, y^*)$ in the RHS of $(1)$ , we have to solve the system $\nabla_z f(z) = 0$ w.r.t. $z$ , where $z = (x,y)$ . In addition, from the LHS of $(1)$ we have, $${x^*}' = \arg \max_x f(x, y)\tag{2}$$ and $${y^*}' = \arg \max_y f({x^*}', y)\tag{3}.$$ Let ${z^*}' = ({x^*}', {y^*}')$ . As far as I understand, I think that given $(1)$ , we can state that ${z^*}' \equiv {z^*}$ . However, I am thinking if there are any cases that ${z^*}' \equiv {z^*}$ does not hold? Could you please someone give some comments or an answer of things are not so simple? Any help is highly appreciated. EDIT1: Let $$\mathcal{f}(x, y) = -\frac{1}{2}\:\mathbf{z}^T \left(\mathbf{A} + \frac{xy}{2} \:\mathbf{I}\right)^{-1} \mathbf{z} - \frac{y}{6} \lambda - \frac{y}{12}x^3,$$ where $x\geq 0$ , $y,\lambda > 0$ , $\mathbf{A}$ a real symmetric positive semmi-definite, and $\mathbf{z}$ are fixed. EDIT2: Let $\mathbf{t}(x,y) = - (\mathbf{A} + 0.5 x y\: \mathbf{I})^{-1} \mathbf{z}$ . If we first solve $\partial_x f(x,y) = 0$ we get $$x = \| \mathbf{t}(x,y)\|\tag{4},$$ for $y >0$ . Then, if we solve $\partial_y f(x,y) = 0$ and use $(4)$ we get $$\sqrt[3]{\lambda} = \| \mathbf{t}(x,y)\|.\tag{5}$$ Next, suppose that we first solve $x = \| \mathbf{t}(x,y)\|$ w.r.t. $x$ to get an optimal ${x^*}'$ and then solve $\sqrt[3]{\lambda} = \| \mathbf{t}({x^*}', y)\|$ w.r.t. $y$ to get ${y^*}'$ . Can we say that ${z^*}' \equiv {z^*}$ ?","I am trying to understand a few things about sequential and simultaneous optimization in [1] . In this post, it is shown that Thanks to @Shiv Tavker comment, I understand that in order to get the optimal value in the RHS of , we have to solve the system w.r.t. , where . In addition, from the LHS of we have, and Let . As far as I understand, I think that given , we can state that . However, I am thinking if there are any cases that does not hold? Could you please someone give some comments or an answer of things are not so simple? Any help is highly appreciated. EDIT1: Let where , , a real symmetric positive semmi-definite, and are fixed. EDIT2: Let . If we first solve we get for . Then, if we solve and use we get Next, suppose that we first solve w.r.t. to get an optimal and then solve w.r.t. to get . Can we say that ?","\max_{x} \max_{y} f(x,y) = \max_{x,\ y} f(x,y).\tag{1} z^*=(x^*, y^*) (1) \nabla_z f(z) = 0 z z = (x,y) (1) {x^*}' = \arg \max_x f(x, y)\tag{2} {y^*}' = \arg \max_y f({x^*}', y)\tag{3}. {z^*}' = ({x^*}', {y^*}') (1) {z^*}' \equiv {z^*} {z^*}' \equiv {z^*} \mathcal{f}(x, y) = -\frac{1}{2}\:\mathbf{z}^T \left(\mathbf{A} + \frac{xy}{2} \:\mathbf{I}\right)^{-1} \mathbf{z} - \frac{y}{6} \lambda - \frac{y}{12}x^3, x\geq 0 y,\lambda > 0 \mathbf{A} \mathbf{z} \mathbf{t}(x,y) = - (\mathbf{A} + 0.5 x y\: \mathbf{I})^{-1} \mathbf{z} \partial_x f(x,y) = 0 x = \| \mathbf{t}(x,y)\|\tag{4}, y >0 \partial_y f(x,y) = 0 (4) \sqrt[3]{\lambda} = \| \mathbf{t}(x,y)\|.\tag{5} x = \| \mathbf{t}(x,y)\| x {x^*}' \sqrt[3]{\lambda} = \| \mathbf{t}({x^*}', y)\| y {y^*}' {z^*}' \equiv {z^*}","['multivariable-calculus', 'optimization', 'convex-optimization', 'nonlinear-optimization', 'non-convex-optimization']"
71,Volume inside an ellipsoid and offset cylinder,Volume inside an ellipsoid and offset cylinder,,"I want to find the volume of the region inside the ellipsoid $$\frac{x^2}{4}+\frac{y^2}{4}+z^2=1$$ and the cylinder $$x^2+(y-1)^2=1$$ I tried shifting the axes so that the cylinder was centered at the origin, then evaluating an integral in cylindrical/polar coordinates. $$\int^{2\pi}_{0}\int^{1}_{0}\int^{\frac{1}{2}\sqrt{4-r^2-2rsin(\theta)-1}}_{-\frac{1}{2}\sqrt{4-r^2-2rsin(\theta)-1}}r dzdrd\theta$$ $$\int^{2\pi}_{0}\int^{1}_{0}r\sqrt{3-r^2-2rsin(\theta)} drd\theta$$ However, this integral gets messy. Is there an easier method of finding the volume?","I want to find the volume of the region inside the ellipsoid and the cylinder I tried shifting the axes so that the cylinder was centered at the origin, then evaluating an integral in cylindrical/polar coordinates. However, this integral gets messy. Is there an easier method of finding the volume?",\frac{x^2}{4}+\frac{y^2}{4}+z^2=1 x^2+(y-1)^2=1 \int^{2\pi}_{0}\int^{1}_{0}\int^{\frac{1}{2}\sqrt{4-r^2-2rsin(\theta)-1}}_{-\frac{1}{2}\sqrt{4-r^2-2rsin(\theta)-1}}r dzdrd\theta \int^{2\pi}_{0}\int^{1}_{0}r\sqrt{3-r^2-2rsin(\theta)} drd\theta,"['integration', 'multivariable-calculus', 'multiple-integral', 'cylindrical-coordinates']"
72,"Existence of a unique function $f:\Bbb R^2\setminus\{(0,0)\}\to\Bbb R, f(x,y)^3=xy\cos(xyf(x,y))-x^2y^2f(x,y), (x,y)\in\Bbb R^2\setminus\{(0,0)\}.$",Existence of a unique function,"f:\Bbb R^2\setminus\{(0,0)\}\to\Bbb R, f(x,y)^3=xy\cos(xyf(x,y))-x^2y^2f(x,y), (x,y)\in\Bbb R^2\setminus\{(0,0)\}.","Does there exist a unique continuous function $f:\Bbb R^2\setminus\{(0,0)\}\to\Bbb R$ such that $$f(x,y)^3=xy\cos(xyf(x,y))-x^2y^2f(x,y),\quad (x,y)\in\Bbb R^2\setminus\{(0,0)\}.$$ Is it of the class $C^1$ ? My attempt: Let $F(x,y,z)=z^3-xy\cos(xyz)+x^2y^2z$ and let $t=xy.$ Then, $F(x,y,1)=1-xy\cos(xy)+x^2y^2=1-t\cos(t)+t^2\ge1-|t|+t^2>0,\forall t\in\Bbb R,$ that is, $\forall (x,y)\in\Bbb R^2$ . Also, $F(x,y,-1)=-1-xy\cos(xy)-x^2y^2=-1-t\cos(t)-t^2\le-1+|t|-t^2<0,\forall t\in\Bbb R,$ that is, $\forall (x,y)\in\Bbb R^2.$ Fix $(x_0,y_0)\in\Bbb R^2$ and define $G:[-1,1]\to\Bbb R,\quad G(z)=F(x_0,y_0,z).$ Since $G$ is continuous and $\lim\limits_{z\to -1^+} G(z)=G(-1)=\ell_1<0$ and $\lim\limits_{z\to 1^-} G(z)=G(1)=\ell_2>0,$ by the mean value theorem,there is $z_0\in(-1,1)$ such that $G(z_0)=F(x_0,y_0,z_0)=0.$ Now, let's look at $G'(z)=\frac{\partial F(x_0,y_0,z)}{\partial z}=3z^2+x^2y^2\sin(xyz)+x^2+y^2=3z^2+x^2y^2(\sin(xyz)+1)\ge 0.$ I think $G'(z)=0\iff z=0$ and $x=0$ or $y=0$ . I would like to apply the implicit function theorem, but, if $x=0$ or $y=0$ and $z=0,$ then I found an example of $F(x_0,y_0,z_0)=0$ and $\frac{\partial F(x_0,y_0,z)}{\partial z}=0$ so I don't know if $f\in C^1(\Bbb R^2\setminus\{(0,0)\})$ . (1) Should, therefore, the domain of the function $f$ be $\Bbb R^2\setminus\{(x,y)\in\Bbb R^2\mid x=0\text{ or } y=0\}?$ In that case, $G'(z)>0\implies G$ is strictly increasing and hence, has a unique root $z_0\in[-1,1]$ and we could define $f(x_0,y_0)=z_0$ for each fixed $(x_0,y_0)\in\Bbb R^2$ and, then, apply the implicit function theorem that guarantees the existence of some open intervals $I,J,K\subseteq\Bbb R$ containing $x_0,y_0,z_0$ respectively and a unique function $g:I\times J\to K$ of the class $C^1$ such that $F(x,y,g(x,y))=0,\forall (x,y)\in I\times J.$ Since $F(x,y,z)=0\iff z=f(x,y),$ we conclude that $f_{\mid I\times J}=g\in C^1$ . As being of the class $C^1$ is a local notion, we conclude $f\in C^1.$ Could anybody verify my answer?","Does there exist a unique continuous function such that Is it of the class ? My attempt: Let and let Then, that is, . Also, that is, Fix and define Since is continuous and and by the mean value theorem,there is such that Now, let's look at I think and or . I would like to apply the implicit function theorem, but, if or and then I found an example of and so I don't know if . (1) Should, therefore, the domain of the function be In that case, is strictly increasing and hence, has a unique root and we could define for each fixed and, then, apply the implicit function theorem that guarantees the existence of some open intervals containing respectively and a unique function of the class such that Since we conclude that . As being of the class is a local notion, we conclude Could anybody verify my answer?","f:\Bbb R^2\setminus\{(0,0)\}\to\Bbb R f(x,y)^3=xy\cos(xyf(x,y))-x^2y^2f(x,y),\quad (x,y)\in\Bbb R^2\setminus\{(0,0)\}. C^1 F(x,y,z)=z^3-xy\cos(xyz)+x^2y^2z t=xy. F(x,y,1)=1-xy\cos(xy)+x^2y^2=1-t\cos(t)+t^2\ge1-|t|+t^2>0,\forall t\in\Bbb R, \forall (x,y)\in\Bbb R^2 F(x,y,-1)=-1-xy\cos(xy)-x^2y^2=-1-t\cos(t)-t^2\le-1+|t|-t^2<0,\forall t\in\Bbb R, \forall (x,y)\in\Bbb R^2. (x_0,y_0)\in\Bbb R^2 G:[-1,1]\to\Bbb R,\quad G(z)=F(x_0,y_0,z). G \lim\limits_{z\to -1^+} G(z)=G(-1)=\ell_1<0 \lim\limits_{z\to 1^-} G(z)=G(1)=\ell_2>0, z_0\in(-1,1) G(z_0)=F(x_0,y_0,z_0)=0. G'(z)=\frac{\partial F(x_0,y_0,z)}{\partial z}=3z^2+x^2y^2\sin(xyz)+x^2+y^2=3z^2+x^2y^2(\sin(xyz)+1)\ge 0. G'(z)=0\iff z=0 x=0 y=0 x=0 y=0 z=0, F(x_0,y_0,z_0)=0 \frac{\partial F(x_0,y_0,z)}{\partial z}=0 f\in C^1(\Bbb R^2\setminus\{(0,0)\}) f \Bbb R^2\setminus\{(x,y)\in\Bbb R^2\mid x=0\text{ or } y=0\}? G'(z)>0\implies G z_0\in[-1,1] f(x_0,y_0)=z_0 (x_0,y_0)\in\Bbb R^2 I,J,K\subseteq\Bbb R x_0,y_0,z_0 g:I\times J\to K C^1 F(x,y,g(x,y))=0,\forall (x,y)\in I\times J. F(x,y,z)=0\iff z=f(x,y), f_{\mid I\times J}=g\in C^1 C^1 f\in C^1.","['real-analysis', 'multivariable-calculus', 'functions', 'solution-verification', 'implicit-function-theorem']"
73,Finding the gradient of the restricted function in terms of the gradient of the original function,Finding the gradient of the restricted function in terms of the gradient of the original function,,"The following question showed up as part of a proof that I am doing for my research thesis. If we have a differentiable function $f: \mathbb{R}^n \to \mathbb{R}$ and then set $n-d$ coordinates to zero we get a new differentiable function $g: \mathbb{R}^d \to \mathbb{R}$ . Now, given the gradient $\nabla_x f(x)$ , how one can get $\nabla_y g(y)$ ? My try Let $x \in \mathbb{R}^n$ and $S \subset \{1,\dots,n\}$ such that $|S|=d$ where $|\cdot|$ is the cardinality of the set. Let $U_S$ be a restricted identity matrix such that the $j$ -th entry of the diagonal matrix is maintained if $j \in S$ otherwise it is set to zero. Also, let $I_S$ be the restriction of $U_S$ where we keep nonzero columns and remove zero columns. Hence, $$ g(y)=f(U_Sx) $$ where $y=I_S^{\top}x$ . The above is the translation of what I stated in terms of functions $f$ and $g$ . From this point things are a little bit unclear. I think the answer should be $\nabla_y g(y)=I_S^{\top} \nabla_x f(x)$ but I do not know how to get it. Also, I know using the chain rule $J_x f(U_S x)=J_{W} f(W)J_x W= J_{W} f(W)U_S$ where $J$ is the Jacobian and $W=U_S x$ . In addition, $\nabla^{\top}_x f(U_Sx) = J_x f(U_S x)=J_{W} f(W)U_S$ . I do not know how to put things together.","The following question showed up as part of a proof that I am doing for my research thesis. If we have a differentiable function and then set coordinates to zero we get a new differentiable function . Now, given the gradient , how one can get ? My try Let and such that where is the cardinality of the set. Let be a restricted identity matrix such that the -th entry of the diagonal matrix is maintained if otherwise it is set to zero. Also, let be the restriction of where we keep nonzero columns and remove zero columns. Hence, where . The above is the translation of what I stated in terms of functions and . From this point things are a little bit unclear. I think the answer should be but I do not know how to get it. Also, I know using the chain rule where is the Jacobian and . In addition, . I do not know how to put things together.","f: \mathbb{R}^n \to \mathbb{R} n-d g: \mathbb{R}^d \to \mathbb{R} \nabla_x f(x) \nabla_y g(y) x \in \mathbb{R}^n S \subset \{1,\dots,n\} |S|=d |\cdot| U_S j j \in S I_S U_S 
g(y)=f(U_Sx)
 y=I_S^{\top}x f g \nabla_y g(y)=I_S^{\top} \nabla_x f(x) J_x f(U_S x)=J_{W} f(W)J_x W= J_{W} f(W)U_S J W=U_S x \nabla^{\top}_x f(U_Sx) = J_x f(U_S x)=J_{W} f(W)U_S","['multivariable-calculus', 'derivatives', 'partial-derivative', 'scalar-fields']"
74,"$\iint \sqrt{1-y^2}$ on a disk. Converges and satisfies Fubini, but doing it as an iterated integral diverges?","on a disk. Converges and satisfies Fubini, but doing it as an iterated integral diverges?",\iint \sqrt{1-y^2},"I am trying to evaluate the integral: $$\iint_D \sqrt{1-y^2} dx dy$$ Where D is the disk centered at $(1, 0)$ and with radius 1. Notice that this integral clearly converges: in the disk, $y^2<1$ , so $|\sqrt{1-y^2}|<1$ . This also means the integral satisfies Fubini. One change of variables $(x, y) \mapsto (x+1, y)$ moves the circle to the origin, and leaves the integrand unchanged. I then change to polar coordinates: $(\rho, \theta) \mapsto (\rho\cos \theta, \rho \sin \theta)$ obtaining: $$ \int_0^{2\pi} \int_0^1 (1-\rho^2\sin^2\theta)^\frac12\rho \ d\rho d\theta $$ I found an antiderivative of the integrand as a function of $\rho$ : $$ -\frac 1 {3\sin^2\theta} (1-\rho^2\sin^2\theta)^\frac32 $$ Evaluating it at $0$ and $1$ reduces the inner integral to the following: $$ \int_0^{2\pi} -\frac 1 {3\sin^2\theta} \left [ (1-\sin^2\theta)^\frac32-1 \right ] d\theta$$ This simplifies to: $$ -\frac {1} 3 \int_0^{2\pi} \frac{\cos^3\theta-1}{\sin^2\theta}d\theta$$ But Wolframalpha says this does not converge. Where did I go wrong?","I am trying to evaluate the integral: Where D is the disk centered at and with radius 1. Notice that this integral clearly converges: in the disk, , so . This also means the integral satisfies Fubini. One change of variables moves the circle to the origin, and leaves the integrand unchanged. I then change to polar coordinates: obtaining: I found an antiderivative of the integrand as a function of : Evaluating it at and reduces the inner integral to the following: This simplifies to: But Wolframalpha says this does not converge. Where did I go wrong?","\iint_D \sqrt{1-y^2} dx dy (1, 0) y^2<1 |\sqrt{1-y^2}|<1 (x, y) \mapsto (x+1, y) (\rho, \theta) \mapsto (\rho\cos \theta, \rho \sin \theta)  \int_0^{2\pi} \int_0^1 (1-\rho^2\sin^2\theta)^\frac12\rho \ d\rho d\theta  \rho  -\frac 1 {3\sin^2\theta} (1-\rho^2\sin^2\theta)^\frac32  0 1  \int_0^{2\pi} -\frac 1 {3\sin^2\theta} \left [ (1-\sin^2\theta)^\frac32-1 \right ] d\theta  -\frac {1} 3 \int_0^{2\pi} \frac{\cos^3\theta-1}{\sin^2\theta}d\theta","['multivariable-calculus', 'multiple-integral']"
75,Doubts regarding the change of order of integration,Doubts regarding the change of order of integration,,"$$\int_0^1\int_0^x \sqrt{x+y^2}\,dydx = \int_0^x\int_0^1 \sqrt{x+y^2}\,dxdy$$ Are these two integrals equivalent to each other? I assumed they weren't after imagining that one should also change the order of integration (in that case, analysing the region and changing the limits of integration). Am I confused about something, or does my reasoning make sense? If I'm right, I'd like to understand why you have to change the limits if you change the order of integration (I know that one can use Fubini's theorem on rectangular regions). Sorry if this isn't in the correct place, I have never posted here.","Are these two integrals equivalent to each other? I assumed they weren't after imagining that one should also change the order of integration (in that case, analysing the region and changing the limits of integration). Am I confused about something, or does my reasoning make sense? If I'm right, I'd like to understand why you have to change the limits if you change the order of integration (I know that one can use Fubini's theorem on rectangular regions). Sorry if this isn't in the correct place, I have never posted here.","\int_0^1\int_0^x \sqrt{x+y^2}\,dydx = \int_0^x\int_0^1 \sqrt{x+y^2}\,dxdy","['calculus', 'multivariable-calculus']"
76,"Application of the fundamental theorem of calculus in $n$ dimensions: Stokes', divergence or gradient theorem?","Application of the fundamental theorem of calculus in  dimensions: Stokes', divergence or gradient theorem?",n,"Consider the following linear integral solved by the Fundamental Theorem of Calculus: $$\int_{a_1}^{a_2}dx\, e^{ixk}=\int_{a_1}^{a_2}dx\, \frac{d}{dx}\left(\frac{e^{ixk}}{ik}\right)= \frac{e^{ixk}}{ik}\Biggr\rvert_{a_1}^{a_2}=\frac{e^{ia_2 k}- e^{i a_1 k}}{ik}\,.$$ Is there a way to make use of a generalization of this theorem in $n$ dimensions (maybe the Gradient Theorem, or Divergence Theorem, or Stokes' theorem?) such that the multidimensional integral defined over a $n$ -dimensional generic volume $V$ $$\int_{V}d\mathbf{x}\, e^{i\mathbf{x}\cdot \mathbf{k}}$$ is solved in terms of the integrand function evalutated at the $(n-1)$ -dimensional boundary surface $\partial V$ of the volume $V$ : $$\int_{V}d\mathbf{x}\, e^{i\mathbf{x}\cdot \mathbf{k}}=\int_V d\mathbf{x}\, \nabla_{\mathbf{x}}(e^{i\mathbf{x}\cdot \mathbf{k}})\cdot \frac{\mathbf{k}}{i k^2} \propto e^{i\mathbf{x}\cdot \mathbf{k}}\rvert_{\mathbf{x}\in \partial V}\, ?$$ Indeed in the linear example above, the integral solution is expressed in terms of the exponential function appearing in the argument, evaluated at the two points corresponding to the borders $x=a_1, a_2$ of the linear domain $[a_1,a_2]$ .","Consider the following linear integral solved by the Fundamental Theorem of Calculus: Is there a way to make use of a generalization of this theorem in dimensions (maybe the Gradient Theorem, or Divergence Theorem, or Stokes' theorem?) such that the multidimensional integral defined over a -dimensional generic volume is solved in terms of the integrand function evalutated at the -dimensional boundary surface of the volume : Indeed in the linear example above, the integral solution is expressed in terms of the exponential function appearing in the argument, evaluated at the two points corresponding to the borders of the linear domain .","\int_{a_1}^{a_2}dx\, e^{ixk}=\int_{a_1}^{a_2}dx\, \frac{d}{dx}\left(\frac{e^{ixk}}{ik}\right)= \frac{e^{ixk}}{ik}\Biggr\rvert_{a_1}^{a_2}=\frac{e^{ia_2 k}- e^{i a_1 k}}{ik}\,. n n V \int_{V}d\mathbf{x}\, e^{i\mathbf{x}\cdot \mathbf{k}} (n-1) \partial V V \int_{V}d\mathbf{x}\, e^{i\mathbf{x}\cdot \mathbf{k}}=\int_V d\mathbf{x}\, \nabla_{\mathbf{x}}(e^{i\mathbf{x}\cdot \mathbf{k}})\cdot \frac{\mathbf{k}}{i k^2} \propto e^{i\mathbf{x}\cdot \mathbf{k}}\rvert_{\mathbf{x}\in \partial V}\, ? x=a_1, a_2 [a_1,a_2]","['calculus', 'multivariable-calculus', 'definite-integrals', 'stokes-theorem', 'divergence-theorem']"
77,Change of variables on triangular region.,Change of variables on triangular region.,,"Given an integral $$\iint\limits_\triangle f(x,y) ~dx~dy$$ switch the variables to polar coordinates, where $\triangle = \{\text{Triangle whose vertices are at } (1,0),(0,2),(5,3)\}$ Please see the edit at the bottom (Attached the hand drawn figure below). I'm taking the substitution $$x = 1 + r\cos\theta, \qquad y = r\sin\theta$$ that is a circle centred at $(1,0)$ . As shown in the figure, one can fill one part of triangle with circles (say the part $\Omega_1$ ). In $\Omega_1$ , we have $$0 \le r \le \frac{7}{\sqrt{26}}$$ (where $|AH|=\frac{7}{\sqrt{26}})$ and $$\arctan \frac 34 \le \theta \le \pi - \arctan 2$$ that is the range of angle between the sides $AB$ and $AC$ . $$\iint\limits_{\Omega_1} f(x,y)~dx~dy = \int\limits_{\arctan \frac 34}^{\pi - \arctan 2} d\theta \int\limits_{0}^{7/\sqrt{26}} f(1+r\cos\theta, r\sin\theta)|J|dr$$ Well, $\Omega_1$ is done. Now let $\Omega_2$ be the unfilled region with circles which contains the point $B$ (upper-right region) and $\Omega_3$ be the unfilled region with circles which contains the point $C$ (upper-left region). While dealing with these regions, however, $r$ and $\theta$ seems to be ""dynamic"" in the sense that their bounds are dependent to the other variable. I'm not sure how to proceed. Edit: With the following new figure, I seem to have a better approach. Then $$\iint\limits_\triangle f(x,y) ~dx~dy = \int_{\arctan(3/4)}^{\pi-\arctan(2)} d\theta \int_0^{7/(5\sin\theta - \cos\theta)} f(1+r\cos\theta, r\sin\theta)r ~dr$$ since $$BC: y = \frac{1}{5}x+2 \\ \implies r\sin\theta = \frac15(r\cos\theta + 1) + 2 \\ \implies r = \frac{7}{5\sin\theta - \cos\theta}$$ Is it correct?","Given an integral switch the variables to polar coordinates, where Please see the edit at the bottom (Attached the hand drawn figure below). I'm taking the substitution that is a circle centred at . As shown in the figure, one can fill one part of triangle with circles (say the part ). In , we have (where and that is the range of angle between the sides and . Well, is done. Now let be the unfilled region with circles which contains the point (upper-right region) and be the unfilled region with circles which contains the point (upper-left region). While dealing with these regions, however, and seems to be ""dynamic"" in the sense that their bounds are dependent to the other variable. I'm not sure how to proceed. Edit: With the following new figure, I seem to have a better approach. Then since Is it correct?","\iint\limits_\triangle f(x,y) ~dx~dy \triangle = \{\text{Triangle whose vertices are at } (1,0),(0,2),(5,3)\} x = 1 + r\cos\theta, \qquad y = r\sin\theta (1,0) \Omega_1 \Omega_1 0 \le r \le \frac{7}{\sqrt{26}} |AH|=\frac{7}{\sqrt{26}}) \arctan \frac 34 \le \theta \le \pi - \arctan 2 AB AC \iint\limits_{\Omega_1} f(x,y)~dx~dy = \int\limits_{\arctan \frac 34}^{\pi - \arctan 2} d\theta \int\limits_{0}^{7/\sqrt{26}} f(1+r\cos\theta, r\sin\theta)|J|dr \Omega_1 \Omega_2 B \Omega_3 C r \theta \iint\limits_\triangle f(x,y) ~dx~dy = \int_{\arctan(3/4)}^{\pi-\arctan(2)} d\theta \int_0^{7/(5\sin\theta - \cos\theta)} f(1+r\cos\theta, r\sin\theta)r ~dr BC: y = \frac{1}{5}x+2 \\
\implies r\sin\theta = \frac15(r\cos\theta + 1) + 2 \\
\implies r = \frac{7}{5\sin\theta - \cos\theta}","['multivariable-calculus', 'solution-verification', 'multiple-integral']"
78,"If $f: \mathbb{R}^{n} \to \mathbb{R}$ satisfies $|f(x)| \leq ||x||^{2}$, then, $f$ is differentiable at the origin.","If  satisfies , then,  is differentiable at the origin.",f: \mathbb{R}^{n} \to \mathbb{R} |f(x)| \leq ||x||^{2} f,"For a function $f: \mathbb{R}^{n} \to \mathbb{R}$ , I want to prove that if $f$ satisfies $|f(x)| \leq ||x||^{2}$ . Then, $f$ is differentiable at the origin. So far, I try to reduce this problem to the case $n=2$ , that is $f: \mathbb{R}^{2} \to \mathbb{R}$ is such that $|f(x_{1},x_{2})| \leq x_1^{2}+ x_2^{2}$ . And according to Wikipedia , I need to prove that there exists a linear map $J:\mathbb{R}^2 \rightarrow \mathbb{R}$ such that $$\lim_{(x_{1},x_{2}) \rightarrow 0}\frac{|f(x_1,x_2)-f(0,0)-J(x_1,x_2)|}{\|(x_1,x_2)\|_{\mathbb{R}^2}}=0$$ . So by taking $J: \mathbb{R}^{2} \to \mathbb{R}$ equal to the linear map zero I got $$\frac{|f(x_1,x_2)-f(0,0)|}{\sqrt{x_1^{2}+x_2^{2}}}.$$ But as $f(0,0)=0$ by the first answer we got that $$\frac{|f(x_1,x_2)-f(0,0)|}{\sqrt{x_1^{2}+x_2^{2}}}=\frac{|f(x_1,x_2)|}{\sqrt{x_1^{2}+x_2^{2}}} \leq \frac{(x_{1}^{2} + x_{2}^{2})}{\sqrt{x_1^{2}+x_2^{2}}}.$$ But limit of the last term of this inequality is zero as $(x_{1}, x_{2})$ aproaches to zero. And in this step is where maybe I need to use my hypothesis $|f(x_{1},x_{2})| \leq x_1^{2}+ x_2^{2}$ . But Im run out of ideas about how to proceed from here. Also Im not sure if the trick changes for $f: \mathbb{R}^{n} \to \mathbb{R}$ ?","For a function , I want to prove that if satisfies . Then, is differentiable at the origin. So far, I try to reduce this problem to the case , that is is such that . And according to Wikipedia , I need to prove that there exists a linear map such that . So by taking equal to the linear map zero I got But as by the first answer we got that But limit of the last term of this inequality is zero as aproaches to zero. And in this step is where maybe I need to use my hypothesis . But Im run out of ideas about how to proceed from here. Also Im not sure if the trick changes for ?","f: \mathbb{R}^{n} \to \mathbb{R} f |f(x)| \leq ||x||^{2} f n=2 f: \mathbb{R}^{2} \to \mathbb{R} |f(x_{1},x_{2})| \leq x_1^{2}+ x_2^{2} J:\mathbb{R}^2 \rightarrow \mathbb{R} \lim_{(x_{1},x_{2}) \rightarrow 0}\frac{|f(x_1,x_2)-f(0,0)-J(x_1,x_2)|}{\|(x_1,x_2)\|_{\mathbb{R}^2}}=0 J: \mathbb{R}^{2} \to \mathbb{R} \frac{|f(x_1,x_2)-f(0,0)|}{\sqrt{x_1^{2}+x_2^{2}}}. f(0,0)=0 \frac{|f(x_1,x_2)-f(0,0)|}{\sqrt{x_1^{2}+x_2^{2}}}=\frac{|f(x_1,x_2)|}{\sqrt{x_1^{2}+x_2^{2}}} \leq \frac{(x_{1}^{2} + x_{2}^{2})}{\sqrt{x_1^{2}+x_2^{2}}}. (x_{1}, x_{2}) |f(x_{1},x_{2})| \leq x_1^{2}+ x_2^{2} f: \mathbb{R}^{n} \to \mathbb{R}","['real-analysis', 'calculus', 'multivariable-calculus', 'derivatives']"
79,"Evaluate the surface integral $\iint_\Sigma (y^2-z^2)e^{yz}\,ds$ using Stokes' Theorem",Evaluate the surface integral  using Stokes' Theorem,"\iint_\Sigma (y^2-z^2)e^{yz}\,ds","I found the following problem in a textbook: Evaluate $$\iint_\Sigma (y^2-z^2)e^{yz}\,ds\,,$$ where $\;\Sigma\;$ is given by $x^2+y^2+z^2=1$ , $\;z\geq0$ , by evaluating the rotation of the field $\;\mathbf{F}=(e^{yz}, 0, 0)$ . I evaluate the rotation to be $(0, ye^{yz}, -ze^{yz})$ , and I also see that the integrand can be written $(0,y,z)\cdot \operatorname{rot}(\mathbf{F})$ . From here I am not sure what to do next. Help appreciated!","I found the following problem in a textbook: Evaluate where is given by , , by evaluating the rotation of the field . I evaluate the rotation to be , and I also see that the integrand can be written . From here I am not sure what to do next. Help appreciated!","\iint_\Sigma (y^2-z^2)e^{yz}\,ds\,, \;\Sigma\; x^2+y^2+z^2=1 \;z\geq0 \;\mathbf{F}=(e^{yz}, 0, 0) (0, ye^{yz}, -ze^{yz}) (0,y,z)\cdot \operatorname{rot}(\mathbf{F})","['multivariable-calculus', 'vector-analysis', 'multiple-integral', 'surface-integrals', 'stokes-theorem']"
80,"What is the higher dimensional analogy of $x^3, x^4$, etc.?","What is the higher dimensional analogy of , etc.?","x^3, x^4","We know that a paraboloid, $x_1^2 + x_2^2 = x^T x$ is the generalization of a quadratic $x^2$ . Is there some function similar to $x^Tx$ that can be used to represent cubic, quartic, quintic...polynomials? I was thinking of $x x^Tx$ for $x^3$ but obviously that doesn't work. Or perhaps $x_1^3 + x_2^3$ is the natural generalization that you simply cannot be put into a dot product between two vectors.","We know that a paraboloid, is the generalization of a quadratic . Is there some function similar to that can be used to represent cubic, quartic, quintic...polynomials? I was thinking of for but obviously that doesn't work. Or perhaps is the natural generalization that you simply cannot be put into a dot product between two vectors.",x_1^2 + x_2^2 = x^T x x^2 x^Tx x x^Tx x^3 x_1^3 + x_2^3,"['calculus', 'linear-algebra', 'multivariable-calculus', 'vector-analysis', 'graphing-functions']"
81,How do you integrate over the directional derivative?,How do you integrate over the directional derivative?,,"Question: How do you integrate the directional derivative of a function over a rectangle? Let's say $K$ is a rectangle in $\mathbb{R}^2$ , and let's say that $\beta$ is a 2D vector that specifies a direction. Lastly, say $u(x,y)$ is a multivariable scalar function. How do I solve the following \begin{equation} \int _K \beta \,\,\cdot \nabla u \end{equation} where $\beta \,\,\cdot\nabla$ denotes the directional derivative in the direction of $\beta$ . I know that in 1D integrating over the derivative of a function gives back the original function, but I'm not sure how this works in 2D. Thanks!","Question: How do you integrate the directional derivative of a function over a rectangle? Let's say is a rectangle in , and let's say that is a 2D vector that specifies a direction. Lastly, say is a multivariable scalar function. How do I solve the following where denotes the directional derivative in the direction of . I know that in 1D integrating over the derivative of a function gives back the original function, but I'm not sure how this works in 2D. Thanks!","K \mathbb{R}^2 \beta u(x,y) \begin{equation}
\int _K \beta \,\,\cdot \nabla u
\end{equation} \beta \,\,\cdot\nabla \beta",['multivariable-calculus']
82,Partial Differentiation using chain rule,Partial Differentiation using chain rule,,"I am trying to express $\frac{\partial^2u}{\partial y^2}$ and $\frac{\partial^2u}{\partial x\partial y}$ of the function $u=f(x,y,g(x,y))$ with partial derivative notation of $f$ and $g$ . However I am having a hard time doing so because I am a bit confused with the notation. I know that $\frac{\partial u}{\partial y}=\frac{\partial f}{\partial y}+ \frac{\partial f}{\partial g}\frac{\partial g}{\partial y}$ . But the problem is that I am not sure how to express $\frac{\partial^2u}{\partial y^2}$ . If I differentiate the $\frac{\partial u}{\partial y}$ by $y$ , I am thinking I will get $\frac{\partial^2 u}{\partial y^2}=\frac{\partial^2 f}{\partial y^2}+ ...$ , but I am not quite sure how the second term of the right hand side ( $\frac{\partial f}{\partial g}\frac{\partial g}{\partial y}$ ) will look like. Similarly, I have trouble in expressing $\frac{\partial^2u}{\partial x\partial y}$ using partial differentiation notation of $f$ and $g$ . How should it be done? Thanks.","I am trying to express and of the function with partial derivative notation of and . However I am having a hard time doing so because I am a bit confused with the notation. I know that . But the problem is that I am not sure how to express . If I differentiate the by , I am thinking I will get , but I am not quite sure how the second term of the right hand side ( ) will look like. Similarly, I have trouble in expressing using partial differentiation notation of and . How should it be done? Thanks.","\frac{\partial^2u}{\partial y^2} \frac{\partial^2u}{\partial x\partial y} u=f(x,y,g(x,y)) f g \frac{\partial u}{\partial y}=\frac{\partial f}{\partial y}+ \frac{\partial f}{\partial g}\frac{\partial g}{\partial y} \frac{\partial^2u}{\partial y^2} \frac{\partial u}{\partial y} y \frac{\partial^2 u}{\partial y^2}=\frac{\partial^2 f}{\partial y^2}+ ... \frac{\partial f}{\partial g}\frac{\partial g}{\partial y} \frac{\partial^2u}{\partial x\partial y} f g","['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative', 'chain-rule']"
83,Doubts about directional derivative in one variable,Doubts about directional derivative in one variable,,"Sometimes I read that directional derivatives for function in one variables are right and left derivatives. But this doesn't make sense to me. The only unit vector in $\mathbb{R}$ are $\pm1 $ , so we are saying that: $$D_1f(x)=f'_+(x)=\lim_{t\to 0^+} \frac{f(x+t)-f(x)}{t}$$ $$D_{-1}f(x)=f'_-(x)=\lim_{t\to 0^-} \frac{f(x+t)-f(x)}{t}$$ But by definition: $$D_{\mathbf{v}} f(\mathbf{x})=\lim_{t\to 0} \frac{f(\mathbf{x}+t\mathbf{v})-f(\mathbf{x})}{t}$$ By this logic: $$D_{\mathbf{-v}} f(\mathbf{x})=\lim_{t\to 0} \frac{f(\mathbf{x}-t\mathbf{v})-f(\mathbf{x})}{t}$$ I can substitute $-t=u$ ( $-t$ doesn't assume infinite times the value $0$ so I can apply composite function limit theorem), so: $$D_{\mathbf{-v}} f(\mathbf{x})=\lim_{u\to 0} \frac{f(\mathbf{x}+u\mathbf{v})-f(\mathbf{x})}{-u}=-\lim_{u\to 0} \frac{f(\mathbf{x}+u\mathbf{v})-f(\mathbf{x})}{u}=-D_{\mathbf{v}} f(\mathbf{x})$$ This should mean that: $$f'_+(x)=-f'_-(x)$$ That in general is false! Wouldn't be more correct to say that by convention in one variable we use derivative along the unit vector $1$ and that right and left derivatives are simply the right and left directional derivatives along the unit vector $1$ . Thanks in advance.","Sometimes I read that directional derivatives for function in one variables are right and left derivatives. But this doesn't make sense to me. The only unit vector in are , so we are saying that: But by definition: By this logic: I can substitute ( doesn't assume infinite times the value so I can apply composite function limit theorem), so: This should mean that: That in general is false! Wouldn't be more correct to say that by convention in one variable we use derivative along the unit vector and that right and left derivatives are simply the right and left directional derivatives along the unit vector . Thanks in advance.",\mathbb{R} \pm1  D_1f(x)=f'_+(x)=\lim_{t\to 0^+} \frac{f(x+t)-f(x)}{t} D_{-1}f(x)=f'_-(x)=\lim_{t\to 0^-} \frac{f(x+t)-f(x)}{t} D_{\mathbf{v}} f(\mathbf{x})=\lim_{t\to 0} \frac{f(\mathbf{x}+t\mathbf{v})-f(\mathbf{x})}{t} D_{\mathbf{-v}} f(\mathbf{x})=\lim_{t\to 0} \frac{f(\mathbf{x}-t\mathbf{v})-f(\mathbf{x})}{t} -t=u -t 0 D_{\mathbf{-v}} f(\mathbf{x})=\lim_{u\to 0} \frac{f(\mathbf{x}+u\mathbf{v})-f(\mathbf{x})}{-u}=-\lim_{u\to 0} \frac{f(\mathbf{x}+u\mathbf{v})-f(\mathbf{x})}{u}=-D_{\mathbf{v}} f(\mathbf{x}) f'_+(x)=-f'_-(x) 1 1,"['multivariable-calculus', 'derivatives']"
84,Find the extreme values of $x^2+xy+y^2-27=0$,Find the extreme values of,x^2+xy+y^2-27=0,"I need to find the extreme values of $x^2+xy+y^2-27=0$ . The hint is I need to use two points and use implicit function derivation. My thoughts were to pick up two points and then use the differentiation of this implicit function to get extrema by doing $g'(x)=0$ . However, I don't even understand the hint, why would I pick two random points, and how do I know if I properly pick them?","I need to find the extreme values of . The hint is I need to use two points and use implicit function derivation. My thoughts were to pick up two points and then use the differentiation of this implicit function to get extrema by doing . However, I don't even understand the hint, why would I pick two random points, and how do I know if I properly pick them?",x^2+xy+y^2-27=0 g'(x)=0,"['real-analysis', 'calculus', 'multivariable-calculus', 'functions']"
85,Finding the curve on a surface with a specific curvature,Finding the curve on a surface with a specific curvature,,"I wish to find a curve $\langle x(t), y(t), z(t) \rangle$ on the surface $x + y + z = (x-y)^2 + (y-z)^2$ whose curvature is 1/2 for all $t$ . I am struggling with how to proceed. I initially tried expanding the right side and attempted to parametrize but because of the $yz$ and $xy$ terms, I wasn't able to do so. I would greatly appreciate any approaches to this problem. Thank you!","I wish to find a curve on the surface whose curvature is 1/2 for all . I am struggling with how to proceed. I initially tried expanding the right side and attempted to parametrize but because of the and terms, I wasn't able to do so. I would greatly appreciate any approaches to this problem. Thank you!","\langle x(t), y(t), z(t) \rangle x + y + z = (x-y)^2 + (y-z)^2 t yz xy","['calculus', 'multivariable-calculus', 'differential-geometry']"
86,volume of solid obtained by rotating the curve about $x=2$ line,volume of solid obtained by rotating the curve about  line,x=2,"Finding volume of solid obtained by rotating the regin enclosed by $y=x^3, y=0, x=1$ about $x=2$ line is What i try:: Volume of solid $$V=\pi\int^{8}_{0}\bigg[(2-x)^2-1\bigg]dy$$ $$V=\pi\int^{8}_{0}\bigg[(2-y^{\frac{1}{3}})^2-1\bigg]dy=-\frac{24\pi}{5}$$ Whats wrong with my solution, please Help me. Thanks","Finding volume of solid obtained by rotating the regin enclosed by about line is What i try:: Volume of solid Whats wrong with my solution, please Help me. Thanks","y=x^3, y=0, x=1 x=2 V=\pi\int^{8}_{0}\bigg[(2-x)^2-1\bigg]dy V=\pi\int^{8}_{0}\bigg[(2-y^{\frac{1}{3}})^2-1\bigg]dy=-\frac{24\pi}{5}","['multivariable-calculus', 'solid-of-revolution']"
87,Explaining Directional derivatives,Explaining Directional derivatives,,"I'm trying to understand the concept of the directional derivative, from the perspective of my multivariable calculus textbook. I've typed out a summary of the explanation, with the questions I couldn't answer in boldface. Any intuitive answers, geometrical answers, physical answers are welcome. Formal, rigorous answers are also welcome. Partial explanations (answering only one of the questions etc) are also very welcome! Consider the problem of calculating the rate of change of $\phi$ in some particular direction. For an infinitesimal vector displacement $d \mathbf{r},$ forming its scalar product with $\nabla \phi$ we obtain $$ \begin{aligned} \nabla \phi \cdot d \mathbf{r} &=\left(\mathbf{i} \frac{\partial \phi}{\partial x}+\mathbf{j} \frac{\partial \phi}{\partial y}+\mathbf{k} \frac{\partial \phi}{\partial z}\right) \cdot(\mathbf{i} d x+\mathbf{j} d y+\mathbf{k} d x) \\ &=\frac{\partial \phi}{\partial x} d x+\frac{\partial \phi}{\partial y} d y+\frac{\partial \phi}{\partial z} d z \\ &=d \phi \end{aligned} $$ which is the infinitesimal change in $\phi$ in going from position $\mathbf{r}$ to $\mathbf{r}+d \mathbf{r} .$ In particular, if $\mathbf{r}$ depends on some parameter $u$ such that $\mathbf{r}(u)$ defines a space curve then the total derivative of $\phi$ with respect to $u$ along the curve is simply $$ \frac{d \phi}{d u}=\nabla \phi \cdot \frac{d \mathbf{r}}{d u}. $$ Question 1: How did we get this? Should I just divide both sides of $\nabla \phi \cdot d \mathbf{r} = d\phi$ by $du$ ? I don't even know if that's a valid operation. In the particular case where the parameter $u$ is the arc length $s$ along the curve, the total derivative of $\phi$ with respect to $s$ along the curve is given by $$ \frac{d \phi}{d s}=\nabla \phi \cdot \hat{\mathbf{t}} $$ where $\hat{\mathbf{t}}$ is the unit tangent to the curve at the given point. Question 2: Then why isn't $\frac{d \phi}{d s} = 0$ ? Surely $\nabla \phi$ is perpendicular/tangent to the surface of $\phi$ , so it will be perpendicular to $\hat{\mathbf{t}}$ ! In general, the rate of change of $\phi$ with respect to the distance $s$ in a particular direction a is given by $$ \frac{d \phi}{d s}=\nabla \phi \cdot \hat{\mathbf{a}} $$ (Question 3: (most burning question) I have no idea how to obtain/understand, the above result/why the above result holds. Also, am I to think $\nabla \phi \cdot \hat{\mathbf{a}} = \nabla \phi \cdot \hat{\mathbf{t}}?$ ) and is called the directional derivative. Since $\hat{\mathbf{a}}$ is a unit vector we have $$ \frac{d \phi}{d s}=|\nabla \phi| \cos \theta $$ where $\theta$ is the angle between $\hat{\mathbf{a}}$ and $\nabla \phi$ . Clearly $\nabla \phi$ lies in the direction of the fastest increase in $\phi$ and $|\nabla \phi|$ is the largest possible value of $d \phi / d s$ . Question 4: I get that the largest possible value of $d \phi / d s$ is when $\theta = 0$ , which is the direction of $\nabla \phi$ , but why does largest $\frac{d \phi}{d s}$ imply direction of fastest increase of $\phi$ ?","I'm trying to understand the concept of the directional derivative, from the perspective of my multivariable calculus textbook. I've typed out a summary of the explanation, with the questions I couldn't answer in boldface. Any intuitive answers, geometrical answers, physical answers are welcome. Formal, rigorous answers are also welcome. Partial explanations (answering only one of the questions etc) are also very welcome! Consider the problem of calculating the rate of change of in some particular direction. For an infinitesimal vector displacement forming its scalar product with we obtain which is the infinitesimal change in in going from position to In particular, if depends on some parameter such that defines a space curve then the total derivative of with respect to along the curve is simply Question 1: How did we get this? Should I just divide both sides of by ? I don't even know if that's a valid operation. In the particular case where the parameter is the arc length along the curve, the total derivative of with respect to along the curve is given by where is the unit tangent to the curve at the given point. Question 2: Then why isn't ? Surely is perpendicular/tangent to the surface of , so it will be perpendicular to ! In general, the rate of change of with respect to the distance in a particular direction a is given by (Question 3: (most burning question) I have no idea how to obtain/understand, the above result/why the above result holds. Also, am I to think ) and is called the directional derivative. Since is a unit vector we have where is the angle between and . Clearly lies in the direction of the fastest increase in and is the largest possible value of . Question 4: I get that the largest possible value of is when , which is the direction of , but why does largest imply direction of fastest increase of ?","\phi d \mathbf{r}, \nabla \phi 
\begin{aligned}
\nabla \phi \cdot d \mathbf{r} &=\left(\mathbf{i} \frac{\partial \phi}{\partial x}+\mathbf{j} \frac{\partial \phi}{\partial y}+\mathbf{k} \frac{\partial \phi}{\partial z}\right) \cdot(\mathbf{i} d x+\mathbf{j} d y+\mathbf{k} d x) \\
&=\frac{\partial \phi}{\partial x} d x+\frac{\partial \phi}{\partial y} d y+\frac{\partial \phi}{\partial z} d z \\
&=d \phi
\end{aligned}
 \phi \mathbf{r} \mathbf{r}+d \mathbf{r} . \mathbf{r} u \mathbf{r}(u) \phi u 
\frac{d \phi}{d u}=\nabla \phi \cdot \frac{d \mathbf{r}}{d u}.
 \nabla \phi \cdot d \mathbf{r} = d\phi du u s \phi s 
\frac{d \phi}{d s}=\nabla \phi \cdot \hat{\mathbf{t}}
 \hat{\mathbf{t}} \frac{d \phi}{d s} = 0 \nabla \phi \phi \hat{\mathbf{t}} \phi s 
\frac{d \phi}{d s}=\nabla \phi \cdot \hat{\mathbf{a}}
 \nabla \phi \cdot \hat{\mathbf{a}} = \nabla \phi \cdot \hat{\mathbf{t}}? \hat{\mathbf{a}} 
\frac{d \phi}{d s}=|\nabla \phi| \cos \theta
 \theta \hat{\mathbf{a}} \nabla \phi \nabla \phi \phi |\nabla \phi| d \phi / d s d \phi / d s \theta = 0 \nabla \phi \frac{d \phi}{d s} \phi","['real-analysis', 'multivariable-calculus', 'vectors', 'vector-analysis', 'vector-fields']"
88,Area of a Section of the Unit Sphere,Area of a Section of the Unit Sphere,,"I am currently studying for the GRE. I came across the following question, and I can't seem to get the correct answer. The question reads: Compute the area of a unit sphere contained between the meridians $\phi =30^{\circ}$ and $\phi = 60^{\circ}$ and parallels $\theta =45^{\circ}$ and $\theta =60^{\circ}$ . First, I recalled that the area element of the unit sphere is: $dA=\sin(\phi)d\phi d\theta$ . From there, I made the following computations: $$\int_{\frac{\pi}{4}}^{\frac{\pi}{3}}\int_{\frac{\pi}{6}}^{\frac{\pi}{3}}\sin(\phi)d\phi d\theta  = \int_{\frac{\pi}{4}}^{\frac{\pi}{3}}(\frac{\sqrt{3}}{2}-\frac{1}{2})d\theta = \frac{\pi}{12}(\frac{\sqrt{3}}{2}-\frac{1}{2})$$ However, the correct answer is: $\frac{\pi}{12}(\sqrt{3}-\sqrt{2})$ . Any guidance on this problem would be greatly appreciated.","I am currently studying for the GRE. I came across the following question, and I can't seem to get the correct answer. The question reads: Compute the area of a unit sphere contained between the meridians and and parallels and . First, I recalled that the area element of the unit sphere is: . From there, I made the following computations: However, the correct answer is: . Any guidance on this problem would be greatly appreciated.",\phi =30^{\circ} \phi = 60^{\circ} \theta =45^{\circ} \theta =60^{\circ} dA=\sin(\phi)d\phi d\theta \int_{\frac{\pi}{4}}^{\frac{\pi}{3}}\int_{\frac{\pi}{6}}^{\frac{\pi}{3}}\sin(\phi)d\phi d\theta  = \int_{\frac{\pi}{4}}^{\frac{\pi}{3}}(\frac{\sqrt{3}}{2}-\frac{1}{2})d\theta = \frac{\pi}{12}(\frac{\sqrt{3}}{2}-\frac{1}{2}) \frac{\pi}{12}(\sqrt{3}-\sqrt{2}),"['integration', 'multivariable-calculus', 'surface-integrals']"
89,"Find the minimum value of $\frac ab+\frac {b}{a+b+1}+\frac {b+1}{a}$ when $a,b>0$",Find the minimum value of  when,"\frac ab+\frac {b}{a+b+1}+\frac {b+1}{a} a,b>0","I was trying to solve this question and I observed that if I use partial derivative then it would be calculating. Moreover, I observed that $\frac ab\times\frac {b}{a+b+1}\times\frac {b+1}{a}=\frac {b+1}{a+b+1}$ . So I thought to use AM-GM inequality and got $3(\frac {b+1}{a+b+1})^\frac13 \leq \frac ab+\frac {b}{a+b+1}+\frac {b+1}{a}$ . Now the equality holds if $\frac ab=\frac {b}{a+b+1}=\frac {b+1}{a}$ . From here I also got these relations $\frac {b+1}{a+b+1}=\frac a{a+b}$ and $\frac ab=\frac {b}{a+b+1}=\frac {b+1}{a}=\frac {a+b+1}{a+b}$ . I am hoping that I will get a finite expression of $(\frac {b+1}{a+b+1})^\frac13 $ and then we are done as the particular values of $a,b$ are not asked. Help me from here. Am I in the correct way? Do you have any other suggestions?","I was trying to solve this question and I observed that if I use partial derivative then it would be calculating. Moreover, I observed that . So I thought to use AM-GM inequality and got . Now the equality holds if . From here I also got these relations and . I am hoping that I will get a finite expression of and then we are done as the particular values of are not asked. Help me from here. Am I in the correct way? Do you have any other suggestions?","\frac ab\times\frac {b}{a+b+1}\times\frac {b+1}{a}=\frac {b+1}{a+b+1} 3(\frac {b+1}{a+b+1})^\frac13 \leq \frac ab+\frac {b}{a+b+1}+\frac {b+1}{a} \frac ab=\frac {b}{a+b+1}=\frac {b+1}{a} \frac {b+1}{a+b+1}=\frac a{a+b} \frac ab=\frac {b}{a+b+1}=\frac {b+1}{a}=\frac {a+b+1}{a+b} (\frac {b+1}{a+b+1})^\frac13  a,b","['calculus', 'multivariable-calculus', 'inequality', 'optimization', 'contest-math']"
90,Show that $\int_a^b f(u) \int_a^u f(v) dv du=\int_a^b f(u) \int_u^b f(v) dv du$,Show that,\int_a^b f(u) \int_a^u f(v) dv du=\int_a^b f(u) \int_u^b f(v) dv du,"Let $f:[a,b]\rightarrow \mathbb{R}$ be integrable on $[a,b]$ . Show that ( $\forall u\in[a,b]$ ) $$\int_a^b f(u)\int_a^u f(v) dv\; du=\int_a^b f(u)\int_u^b f(v) dv\; du$$ What i've done so far: I suppose $\int f$ has $F$ as an antiderivative (which is not guaranted by $f$ being just integrable, but i can't think other way). So, $$\int_a^b f(u)\int_a^u f(v) dv\; du=\int_a^b F(u)f(u)\;du-F(a)\int_a^b f(u) du=\int_a^b F(u)f(u)\;du+F(a)^2-F(a)F(b)$$ $$\int_a^b f(u)\int_u^b f(v) dv\; du=F(b)\int_a^b f(u)\;du-\int_a^b F(u)f(u) du= F(b)^2-F(a)F(b)-\int_a^b F(u)f(u)\;du$$ In terms of areas, for let´s say $f(x)=2x$ with $[-2,3]$ , and $F(x)=x^2$ what we're integrating from $-2$ to $3$ is: $$(F(u)-F(a))f(u)= 2(u^2-(-2)^2)u=2(u^2-4)u$$ and $$(F(b)-F(u))f(u)=2(9-u^2)u$$ If i graph these from $-2$ to $3$ , they don't seem related, but the area under both curves is the same. Can someone give me an approach. ¿Is this a known result? Thanks","Let be integrable on . Show that ( ) What i've done so far: I suppose has as an antiderivative (which is not guaranted by being just integrable, but i can't think other way). So, In terms of areas, for let´s say with , and what we're integrating from to is: and If i graph these from to , they don't seem related, but the area under both curves is the same. Can someone give me an approach. ¿Is this a known result? Thanks","f:[a,b]\rightarrow \mathbb{R} [a,b] \forall u\in[a,b] \int_a^b f(u)\int_a^u f(v) dv\; du=\int_a^b f(u)\int_u^b f(v) dv\; du \int f F f \int_a^b f(u)\int_a^u f(v) dv\; du=\int_a^b F(u)f(u)\;du-F(a)\int_a^b f(u) du=\int_a^b F(u)f(u)\;du+F(a)^2-F(a)F(b) \int_a^b f(u)\int_u^b f(v) dv\; du=F(b)\int_a^b f(u)\;du-\int_a^b F(u)f(u) du=
F(b)^2-F(a)F(b)-\int_a^b F(u)f(u)\;du f(x)=2x [-2,3] F(x)=x^2 -2 3 (F(u)-F(a))f(u)=
2(u^2-(-2)^2)u=2(u^2-4)u (F(b)-F(u))f(u)=2(9-u^2)u -2 3","['calculus', 'integration', 'multivariable-calculus']"
91,Definition of differentiability at one point,Definition of differentiability at one point,,"My Multivariable Calculus notes state that, in order for a function of the type $\mathbb{R}^2\rightarrow \mathbb{R}$ to be differentiable at a given point, the limit $$\lim _{h,\:k\to 0}\left(\frac{E\left(h,\:k\right)\:}{\:\sqrt{h^2+k^2}}\right)=0$$ Where $$f\left(a+h,\:b+k\right)-f\left(a,\:b\right)=h\cdot \frac{\partial }{\partial x}\left(a,\:b\right)+k\cdot \frac{\partial }{\partial y}\left(a,\:b\right)+E\left(h,\:k\right)$$ But everyone just expect us to memorize this and reproduce it on the exam. What does this really mean? Why does the limit must approach $0$ ?","My Multivariable Calculus notes state that, in order for a function of the type to be differentiable at a given point, the limit Where But everyone just expect us to memorize this and reproduce it on the exam. What does this really mean? Why does the limit must approach ?","\mathbb{R}^2\rightarrow \mathbb{R} \lim _{h,\:k\to 0}\left(\frac{E\left(h,\:k\right)\:}{\:\sqrt{h^2+k^2}}\right)=0 f\left(a+h,\:b+k\right)-f\left(a,\:b\right)=h\cdot \frac{\partial }{\partial x}\left(a,\:b\right)+k\cdot \frac{\partial }{\partial y}\left(a,\:b\right)+E\left(h,\:k\right) 0","['multivariable-calculus', 'derivatives']"
92,Volume between surfaces,Volume between surfaces,,"Find $V(T),$ where $T$ is the region bounded by the surfaces $y = kx^2+kz^2$ and $z=kx^2+ky^2,$ where $k\in\mathbb{R}, k > 0.$ I tried solving for the area over which these curves intersect, which gave me $y-kz^2 = z-ky^2.$ Solving gives $y+ky^2 - (z+kz^2) =0\Rightarrow (y-z)(1 + k(y-z)) = 0.$ Since $y$ and $z$ are nonnegative, this implies $y=z.$ We thus obtain $y = kx^2 + ky^2\Rightarrow ky^2 - y +kx^2 = 0\Rightarrow (y -\dfrac{1}{2k})^2 +x^2 = \dfrac{1}{4k^2},$ which is a circle centered at $(0,\dfrac{1}{2k})$ with radius $\dfrac{1}{2k}.$ I am able to solve for $A(x),$ the area in terms of $x$ at a given value of $x,$ and I know $x$ ranges from $-\dfrac{1}{2k}$ to $\dfrac{1}{2k},$ but the resulting integral I have to evaluate is absolutely disgusting! Is there a ""cleaner"" integral I can use?","Find where is the region bounded by the surfaces and where I tried solving for the area over which these curves intersect, which gave me Solving gives Since and are nonnegative, this implies We thus obtain which is a circle centered at with radius I am able to solve for the area in terms of at a given value of and I know ranges from to but the resulting integral I have to evaluate is absolutely disgusting! Is there a ""cleaner"" integral I can use?","V(T), T y = kx^2+kz^2 z=kx^2+ky^2, k\in\mathbb{R}, k > 0. y-kz^2 = z-ky^2. y+ky^2 - (z+kz^2) =0\Rightarrow (y-z)(1 + k(y-z)) = 0. y z y=z. y = kx^2 + ky^2\Rightarrow ky^2 - y +kx^2 = 0\Rightarrow (y -\dfrac{1}{2k})^2 +x^2 = \dfrac{1}{4k^2}, (0,\dfrac{1}{2k}) \dfrac{1}{2k}. A(x), x x, x -\dfrac{1}{2k} \dfrac{1}{2k},",['integration']
93,"Finding the volume of the tetrahedron with vertices $(0,0,0)$, $(2,0,0)$, $(0,2,0)$, $(0,0,2)$. I get $8$; answer is $4/3$.","Finding the volume of the tetrahedron with vertices , , , . I get ; answer is .","(0,0,0) (2,0,0) (0,2,0) (0,0,2) 8 4/3","The following problem is from the 7th edition of the book ""Calculus and Analytic Geometry Part II"". It can be found in section 13.7. It is problem number 5. Find the volume of the tetrahedron whose vertices are the given points: $$ ( 0, 0, 0 ), ( 2, 0, 0 ), ( 0, 2, 0 ), ( 0, 0, 2 ) $$ Answer: In this case, the tetrahedron is a parallelepiped object. If the bounds of such an object is given by the vectors $A$ , $B$ and $C$ then the area of the object is $A \cdot (B \times C)$ . Let $V$ be the volume we are trying to find. \begin{align*} x^2 &= 6 - y^2 - z^2 \\[4pt] A &= ( 2, 0, 0) - (0,0,0) = ( 2, 0, 0) \\ B &= ( 0, 2, 0) - (0,0,0) = ( 0, 2, 0) \\ C &= ( 0, 0, 2) - (0,0,0) = ( 0, 0, 2) \\[4pt] V &= \begin{vmatrix} a_1 & a_2 & a_3 \\ b_1 & b_2 & b_3 \\ c_1 & c_2 & c_3 \\ \end{vmatrix} = \begin{vmatrix} 2 & 0 &0 \\ 0 & 2 & 0 \\ 0 & 0 & 2\\ \end{vmatrix} \\ &= 2 \begin{vmatrix} 2 & 0 \\ 0 & 2\\ \end{vmatrix} = 2(4 - 0) \\ &= 8 \end{align*} However, the book gets $\frac{4}{3}$ .","The following problem is from the 7th edition of the book ""Calculus and Analytic Geometry Part II"". It can be found in section 13.7. It is problem number 5. Find the volume of the tetrahedron whose vertices are the given points: Answer: In this case, the tetrahedron is a parallelepiped object. If the bounds of such an object is given by the vectors , and then the area of the object is . Let be the volume we are trying to find. However, the book gets ."," ( 0, 0, 0 ), ( 2, 0, 0 ), ( 0, 2, 0 ), ( 0, 0, 2 )  A B C A \cdot (B \times C) V \begin{align*}
x^2 &= 6 - y^2 - z^2 \\[4pt]
A &= ( 2, 0, 0) - (0,0,0) = ( 2, 0, 0) \\
B &= ( 0, 2, 0) - (0,0,0) = ( 0, 2, 0) \\
C &= ( 0, 0, 2) - (0,0,0) = ( 0, 0, 2) \\[4pt]
V &= \begin{vmatrix}
a_1 & a_2 & a_3 \\
b_1 & b_2 & b_3 \\
c_1 & c_2 & c_3 \\
\end{vmatrix} =
\begin{vmatrix}
2 & 0 &0 \\
0 & 2 & 0 \\
0 & 0 & 2\\
\end{vmatrix} \\
&= 2 \begin{vmatrix}
2 & 0 \\
0 & 2\\
\end{vmatrix} = 2(4 - 0) \\
&= 8
\end{align*} \frac{4}{3}","['multivariable-calculus', 'solid-geometry']"
94,"If $C$ is a curve with endpoints $(0,1)$ and $(1,0)$, show that $\int_{C}\sqrt{x^2+y^2}ds\ge1$","If  is a curve with endpoints  and , show that","C (0,1) (1,0) \int_{C}\sqrt{x^2+y^2}ds\ge1","Let $I=\int_{C}\sqrt{x^2+y^2}ds$ . So far, I have only managed to prove that $I\ge\sqrt{2}-{1\over2}$ , the proof of which is below. Let $c(t)$ be a unit speed parametrization of $C$ such that $c(0)=(0,1)$ and $c(L)=(1,0)$ , where $L$ is the length of $c$ . Let $f(x,y)=\sqrt{x^2+y^2}$ . Notice that $f(c(t))=\|c(t)\|$ and $\|c'(t)\|=1$ since $c$ is unit speed. Then, $I=\int_{C}fds=\int_{0}^Lf(c(t))\|c'(t)\|dt=\int_{0}^L\|c(t)\|ds$ . Since $c$ is unit speed, $-1\le{d\over{dt}}\|c(t)\|dt\le1$ . Also, $\|c(0)\|=\|c(L)\|=1$ . So, if $L\ge2$ , then $\int_{0}^{1}\|c(t)\|dt\ge{1\over2}$ . Similarly, $\int_{L-1}^{L}\|c(t)\|dt\ge{1\over2}$ , so $\int_{0}^{L}\|c(t)\|dt\ge{1}$ , meaning we now only need consider $L<2$ . If $L<2$ , then $\int_{0}^{L\over2}\|c(t)\|dt\ge{L\over{2}}(1-{L\over2})+{L^2\over8}\le\int_{L\over2}^{L}\|c(t)\|dt$ , so $\int_{0}^{L}\|c(t)\|dt)\ge{L-{L^2\over4}}$ . Finally, since the distance between $(0,1)$ and $(1,0)$ is $\sqrt2$ , $L\ge\sqrt2$ . Since $L-{L^2\over4}$ is increasing over the interval $[\sqrt2,2]$ , this implies $I\ge\sqrt2-{{\sqrt2}^2\over4}=\sqrt2-{1\over2}$ . This bound is somewhat close to $1$ at approximately $0.914...$ , however I have been unable to find any way to proceed with improving upon my current inequality. How can I prove the desired result?","Let . So far, I have only managed to prove that , the proof of which is below. Let be a unit speed parametrization of such that and , where is the length of . Let . Notice that and since is unit speed. Then, . Since is unit speed, . Also, . So, if , then . Similarly, , so , meaning we now only need consider . If , then , so . Finally, since the distance between and is , . Since is increasing over the interval , this implies . This bound is somewhat close to at approximately , however I have been unable to find any way to proceed with improving upon my current inequality. How can I prove the desired result?","I=\int_{C}\sqrt{x^2+y^2}ds I\ge\sqrt{2}-{1\over2} c(t) C c(0)=(0,1) c(L)=(1,0) L c f(x,y)=\sqrt{x^2+y^2} f(c(t))=\|c(t)\| \|c'(t)\|=1 c I=\int_{C}fds=\int_{0}^Lf(c(t))\|c'(t)\|dt=\int_{0}^L\|c(t)\|ds c -1\le{d\over{dt}}\|c(t)\|dt\le1 \|c(0)\|=\|c(L)\|=1 L\ge2 \int_{0}^{1}\|c(t)\|dt\ge{1\over2} \int_{L-1}^{L}\|c(t)\|dt\ge{1\over2} \int_{0}^{L}\|c(t)\|dt\ge{1} L<2 L<2 \int_{0}^{L\over2}\|c(t)\|dt\ge{L\over{2}}(1-{L\over2})+{L^2\over8}\le\int_{L\over2}^{L}\|c(t)\|dt \int_{0}^{L}\|c(t)\|dt)\ge{L-{L^2\over4}} (0,1) (1,0) \sqrt2 L\ge\sqrt2 L-{L^2\over4} [\sqrt2,2] I\ge\sqrt2-{{\sqrt2}^2\over4}=\sqrt2-{1\over2} 1 0.914...","['real-analysis', 'integration', 'multivariable-calculus', 'curves']"
95,Geometric interpretation of Lagrange multiplier with multiple constraints,Geometric interpretation of Lagrange multiplier with multiple constraints,,"A Single Constraint Suppose I want to maximise $f(x,y)=x^2 y$ subject to constraint $g(x,y)=x^2 + y^2 = 1$ . Geometrically, we can say that from a contour plot, $f$ is maximised under the constraint at the point where the level of $f$ is tangential to $x^2+y^2=1$ . This would look something like this: We'll call the position of the tangent, where the thick black line meets the thick green line, $(x_m,y_m)$ . What can be observed is that the gradient of $f$ at this point and the gradient of $g$ at this point, are proportional. Hence, we introduce the Lagrange multiplier, $\lambda$ , a constant of proportionality for this relation: $$\nabla f(x_m,y_m) = \lambda \nabla g(x_m,y_m)$$ From this we get a system of equations and solve for the maximum. Now that was fine, and the idea of $\nabla f$ being proportional to $\nabla g$ is easy to see, with thanks to the geometric interpretation. Where I become confused is when we start adding multiple constraints. Multiple Constraints Suppose I have a function $f(x,y,z)=3x-y-3z$ and I'm trying to maximise/minimise this function subject to constraints $g_1(x,y,z)=x+y-1=0$ and $g_2(x,y,z)=x^2+2z^2-1=0$ . Similar to the single constraint case, part of the process of solving this would be to say that, $$\nabla f=\lambda_1\nabla g_1 + \lambda_2 \nabla g_2$$ And indeed, I suppose we could generalise and say that if we had some $m$ constraints, that we'd have to solve $\nabla f= \sum_{i=1}^m \lambda_i\nabla g_i$ . The Problem However, I am struggling for a geometric interpretation of this relationship between the gradient of $f$ and the gradients of the constraints. Because I'm struggling for a geometric interpretation, I'm struggling to understand what this means at all. Why is the gradient of $f$ a combination of the gradients of the constraints? Does anyone have perspective on this?","A Single Constraint Suppose I want to maximise subject to constraint . Geometrically, we can say that from a contour plot, is maximised under the constraint at the point where the level of is tangential to . This would look something like this: We'll call the position of the tangent, where the thick black line meets the thick green line, . What can be observed is that the gradient of at this point and the gradient of at this point, are proportional. Hence, we introduce the Lagrange multiplier, , a constant of proportionality for this relation: From this we get a system of equations and solve for the maximum. Now that was fine, and the idea of being proportional to is easy to see, with thanks to the geometric interpretation. Where I become confused is when we start adding multiple constraints. Multiple Constraints Suppose I have a function and I'm trying to maximise/minimise this function subject to constraints and . Similar to the single constraint case, part of the process of solving this would be to say that, And indeed, I suppose we could generalise and say that if we had some constraints, that we'd have to solve . The Problem However, I am struggling for a geometric interpretation of this relationship between the gradient of and the gradients of the constraints. Because I'm struggling for a geometric interpretation, I'm struggling to understand what this means at all. Why is the gradient of a combination of the gradients of the constraints? Does anyone have perspective on this?","f(x,y)=x^2 y g(x,y)=x^2 + y^2 = 1 f f x^2+y^2=1 (x_m,y_m) f g \lambda \nabla f(x_m,y_m) = \lambda \nabla g(x_m,y_m) \nabla f \nabla g f(x,y,z)=3x-y-3z g_1(x,y,z)=x+y-1=0 g_2(x,y,z)=x^2+2z^2-1=0 \nabla f=\lambda_1\nabla g_1 + \lambda_2 \nabla g_2 m \nabla f= \sum_{i=1}^m \lambda_i\nabla g_i f f","['multivariable-calculus', 'optimization', 'lagrange-multiplier']"
96,Using Stokes' theorem to find the work of a force on a particle,Using Stokes' theorem to find the work of a force on a particle,,"If $\textbf{F}(x,y,z)=(xy-z,x+y^2,xyz)$ is a force field acting on a particle traversing the intersection of the cones $z=\sqrt{x^2+y^2},z=2-\sqrt{x^2+y^2}$ and the semispace $y\geq0$ , compute the work done through the point $(-1,0,1)$ to the point $(1,0,1)$ . Is my strategy more or less on track ? I ""closed"" the curve $C=C_1 \cup C_2$ to apply Stokes' theorem. $C_1$ is the semicircle and $C_2$ the line from $(1,0,1)$ to the point $(-1,0,1)$ . We know that $$\int_{C_1} \textbf{F} d\textbf{s} + \int_{C_2} \textbf{F} d\textbf{s} = \int_C \textbf{F} d\textbf{s} \stackrel{\text{Stokes}}{=} \iint_S \text{curl} \textbf{ F} d\textbf{S} \iff \\ \int_{C_1} \textbf{F} d\textbf{s} = \iint_S \text{curl} \textbf{ F} d\textbf{S} - \int_{C_2} \textbf{F} d\textbf{s}$$ Given that, for the parametrization $\sigma:[0,1]\to \mathbb{R}^3,\sigma(t)=(1-2t,0,1)$ , $$\int_{C_2} \textbf{F} d\textbf{s} = \int_0^1 \textbf{F}((1-2t,0,1)) \cdot (-2,0,0) dt =2$$ And that, for the parametrization $\gamma:[0,1]\times [0,\pi]\to\mathbb{R}^3,\gamma(u,v)=(u\cos v,u\sin v, 1)$ , $$\iint_S \text{curl} \textbf{ F} d\textbf{S} = \int_0^{\pi}\int_0^1 (u-u^2\cos v) du dv= \frac{\pi}{2}$$ Then $W=\int_{C_1} \textbf{F} d\textbf{s} = \frac{\pi}{2}-2$ ?","If is a force field acting on a particle traversing the intersection of the cones and the semispace , compute the work done through the point to the point . Is my strategy more or less on track ? I ""closed"" the curve to apply Stokes' theorem. is the semicircle and the line from to the point . We know that Given that, for the parametrization , And that, for the parametrization , Then ?","\textbf{F}(x,y,z)=(xy-z,x+y^2,xyz) z=\sqrt{x^2+y^2},z=2-\sqrt{x^2+y^2} y\geq0 (-1,0,1) (1,0,1) C=C_1 \cup C_2 C_1 C_2 (1,0,1) (-1,0,1) \int_{C_1} \textbf{F} d\textbf{s} + \int_{C_2} \textbf{F} d\textbf{s} = \int_C \textbf{F} d\textbf{s} \stackrel{\text{Stokes}}{=} \iint_S \text{curl} \textbf{ F} d\textbf{S} \iff \\ \int_{C_1} \textbf{F} d\textbf{s} = \iint_S \text{curl} \textbf{ F} d\textbf{S} - \int_{C_2} \textbf{F} d\textbf{s} \sigma:[0,1]\to \mathbb{R}^3,\sigma(t)=(1-2t,0,1) \int_{C_2} \textbf{F} d\textbf{s} = \int_0^1 \textbf{F}((1-2t,0,1)) \cdot (-2,0,0) dt =2 \gamma:[0,1]\times [0,\pi]\to\mathbb{R}^3,\gamma(u,v)=(u\cos v,u\sin v, 1) \iint_S \text{curl} \textbf{ F} d\textbf{S} = \int_0^{\pi}\int_0^1 (u-u^2\cos v) du dv= \frac{\pi}{2} W=\int_{C_1} \textbf{F} d\textbf{s} = \frac{\pi}{2}-2","['multivariable-calculus', 'vector-analysis', 'stokes-theorem']"
97,"If $\lim\limits_{(x,y)\to (0,0)}(f(x)+g(y))$ exists, do $\lim\limits_{x\to 0}f(x)$ and $\lim\limits_{y\to 0}g(y)$ exist?","If  exists, do  and  exist?","\lim\limits_{(x,y)\to (0,0)}(f(x)+g(y)) \lim\limits_{x\to 0}f(x) \lim\limits_{y\to 0}g(y)","If the limit $\lim\limits_{(x,y)\to (0,0)}(f(x)+g(y))$ exists.Is it true that the limits $\lim\limits_{x\to 0}f(x)$ and $\lim\limits_{y\to 0}g(y)$ both exist?",If the limit exists.Is it true that the limits and both exist?,"\lim\limits_{(x,y)\to (0,0)}(f(x)+g(y)) \lim\limits_{x\to 0}f(x) \lim\limits_{y\to 0}g(y)",['multivariable-calculus']
98,Double integral boundary choice,Double integral boundary choice,,"I have a hard time wrapping my mind around what boundaries that I should use to evaluate the following integral: $$\iint_M xy\,dx\,dy$$ in the region $M: \{(x,y) \in \mathbb{R}:  |x|\le 1 , |y|\le 1, x^2+y^2 \ge 1\}$ From what I understand, I am supposed to find the volume inside the circle up to the $f(x,y)=xy.$ First, I think that the equation for the circle should be less or equal to $1.$ One of the boundaries should be $-1$ to $1$ and the other one I can't seem to understand. I've tried going from $-1$ to $y\le\sqrt{1-x^2}$ . Evaluating this way I got $0.$ Is the info about the area correctly given or is my assumption right? And what boundaries should I use?","I have a hard time wrapping my mind around what boundaries that I should use to evaluate the following integral: in the region From what I understand, I am supposed to find the volume inside the circle up to the First, I think that the equation for the circle should be less or equal to One of the boundaries should be to and the other one I can't seem to understand. I've tried going from to . Evaluating this way I got Is the info about the area correctly given or is my assumption right? And what boundaries should I use?","\iint_M xy\,dx\,dy M: \{(x,y) \in \mathbb{R}: 
|x|\le 1 , |y|\le 1, x^2+y^2 \ge 1\} f(x,y)=xy. 1. -1 1 -1 y\le\sqrt{1-x^2} 0.","['integration', 'multivariable-calculus', 'multiple-integral']"
99,Find the directional derivatives of a function,Find the directional derivatives of a function,,"Consider the function $f : R^2 → R$ given by $$ f(x, y) = \begin{cases} \begin{matrix} \frac{x^2y}{x^4+y^2} & \mathrm{if}\ (x, y) \ne(0, 0)\\ 0 & \mathrm{if}\ (x, y) = (0, 0) \\ \end{matrix} \end{cases} $$ Using the definition, compute the directional derivative $\partial_uf(0,0) $ for   all directions $u=(u_1, u_2)\ne (0, 0)$ . [Hint: Consider the cases $u_2 \ne 0$ and $u_2 = 0$ separately, and use that $\partial _uf(0, 0) = \lim_{h\to 0} \frac{f(hu_1,hu_2)−f(0,0)}{h}$ .] Here is my solution: We know that $f(0,0)=0$ , we can rewrite the formula for the directional derivative as $$\partial _uf(0, 0) = \lim_{h\to 0} \frac{f(hu_1,hu_2)}{h}$$ We have that in the function, $x$ is denoted by $hu_1$ and $y$ is denoted by $hu_2$ . We now look at each case, in terms of the original function. Case 1: We have that $u_2=0$ , $$\lim_{h\to0}\left(\frac{((hu_1)^2(h\cdot 0))}{h ((hu_1)^4+(hu_2)^2)}\right) = 0$$ Case 2: We have that $u_2\ne0$ , $$\lim_{h\to0}\left(\frac{((hu_1)^2(hu_2))}{h ((hu_1)^4+(hu_2)^2)}\right) = \lim_{h\to0}\left(\frac{h^3(u_1^2u_2)}{h^3(h^2 u_1^4+u_2^2)}\right) = \lim_{h\to0}\left(\frac{u_1^2u_2}{h^2 u_1^4+u_2^2}\right)$$ Solving as $h\to0$ we see that the $\lim\to \frac{u_1^2}{u_2}$ . Therefore the directional derivatives for $\partial_uf(0,0)$ in the direction $u=(u_1,0)$ is $0$ . The directional derivative in the direction $u=(u_1,u_2)$ is $\frac{u_1^2}{u_2}$ . Is this solution correct? How can I improve this answer in general?","Consider the function given by Using the definition, compute the directional derivative for   all directions . [Hint: Consider the cases and separately, and use that .] Here is my solution: We know that , we can rewrite the formula for the directional derivative as We have that in the function, is denoted by and is denoted by . We now look at each case, in terms of the original function. Case 1: We have that , Case 2: We have that , Solving as we see that the . Therefore the directional derivatives for in the direction is . The directional derivative in the direction is . Is this solution correct? How can I improve this answer in general?","f : R^2 → R 
f(x, y) =
\begin{cases}
\begin{matrix}
\frac{x^2y}{x^4+y^2} & \mathrm{if}\ (x, y) \ne(0, 0)\\
0 & \mathrm{if}\ (x, y) = (0, 0) \\
\end{matrix}
\end{cases}
 \partial_uf(0,0)  u=(u_1, u_2)\ne (0, 0) u_2 \ne 0 u_2 = 0 \partial _uf(0, 0) = \lim_{h\to 0} \frac{f(hu_1,hu_2)−f(0,0)}{h} f(0,0)=0 \partial _uf(0, 0) = \lim_{h\to 0} \frac{f(hu_1,hu_2)}{h} x hu_1 y hu_2 u_2=0 \lim_{h\to0}\left(\frac{((hu_1)^2(h\cdot 0))}{h ((hu_1)^4+(hu_2)^2)}\right) = 0 u_2\ne0 \lim_{h\to0}\left(\frac{((hu_1)^2(hu_2))}{h ((hu_1)^4+(hu_2)^2)}\right) = \lim_{h\to0}\left(\frac{h^3(u_1^2u_2)}{h^3(h^2 u_1^4+u_2^2)}\right) = \lim_{h\to0}\left(\frac{u_1^2u_2}{h^2 u_1^4+u_2^2}\right) h\to0 \lim\to \frac{u_1^2}{u_2} \partial_uf(0,0) u=(u_1,0) 0 u=(u_1,u_2) \frac{u_1^2}{u_2}","['real-analysis', 'multivariable-calculus']"
