,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Show that $\lim_{n \to \infty} \int_{0}^{1}|f_n(x)| \, dx= 0$ if $\int_0^1|f_n(x)|^2\,dx < 100$",Show that  if,"\lim_{n \to \infty} \int_{0}^{1}|f_n(x)| \, dx= 0 \int_0^1|f_n(x)|^2\,dx < 100","Let $\{f_n\}$ be a sequence of Lebesgue integrable functions such that (i) $\int_0^1|f_n(x)|^2 dx < 100$ (ii) $ f_n \to 0$ almost everywhere We must show that $$ \lim_{n\to\infty}\int_0^1 |f_n(x)| \, dx = 0$$ I have a solution using Egoroff and Schartz Inequality. Is that necessary? Any other ideas ? Also I prove it by myself without that. I will edit the post later.","Let $\{f_n\}$ be a sequence of Lebesgue integrable functions such that (i) $\int_0^1|f_n(x)|^2 dx < 100$ (ii) $ f_n \to 0$ almost everywhere We must show that $$ \lim_{n\to\infty}\int_0^1 |f_n(x)| \, dx = 0$$ I have a solution using Egoroff and Schartz Inequality. Is that necessary? Any other ideas ? Also I prove it by myself without that. I will edit the post later.",,"['real-analysis', 'limits', 'measure-theory', 'lebesgue-integral']"
1,limit of $ \lim_{x \rightarrow 1} \frac{ f_{n} (x)- f_{n-1} (x)}{ (1-x)^{n} }=? $,limit of, \lim_{x \rightarrow 1} \frac{ f_{n} (x)- f_{n-1} (x)}{ (1-x)^{n} }=? ,Let $$ f_{n} (x)= x^{ x^{\scriptstyle\cdot^{\scriptstyle\cdot^{\scriptstyle\cdot^{\scriptstyle x}}}}}$$ Then $$ \lim_{x \rightarrow 1}  \frac{ f_{n} (x)- f_{n-1} (x)}{ (1-x)^{n} }={?} $$ My try: \begin{align} \lim_{x \rightarrow 1} \frac{f_n-f_{n-1}}{(1-x)^{n}} &=\lim_{x \rightarrow 1}{ \frac{ e^{\ln f_{n} (x) } - e^{\ln f_{n-1} (x) } }{(1-x)^{n}} } \\[6px] &=\lim_{x \rightarrow 1}{ \frac{ e^{f_{n-1} (x)\ln x } - e^{f_{n-2} (x)\ln x } }{(1-x)^{n}} } \\[6px] &=\lim_{x \rightarrow 1} \frac{\ln x(f_{n-1} (x)-f_{n-2} (x))}{ (1-x)^{n} } \end{align} Now?,Let $$ f_{n} (x)= x^{ x^{\scriptstyle\cdot^{\scriptstyle\cdot^{\scriptstyle\cdot^{\scriptstyle x}}}}}$$ Then $$ \lim_{x \rightarrow 1}  \frac{ f_{n} (x)- f_{n-1} (x)}{ (1-x)^{n} }={?} $$ My try: \begin{align} \lim_{x \rightarrow 1} \frac{f_n-f_{n-1}}{(1-x)^{n}} &=\lim_{x \rightarrow 1}{ \frac{ e^{\ln f_{n} (x) } - e^{\ln f_{n-1} (x) } }{(1-x)^{n}} } \\[6px] &=\lim_{x \rightarrow 1}{ \frac{ e^{f_{n-1} (x)\ln x } - e^{f_{n-2} (x)\ln x } }{(1-x)^{n}} } \\[6px] &=\lim_{x \rightarrow 1} \frac{\ln x(f_{n-1} (x)-f_{n-2} (x))}{ (1-x)^{n} } \end{align} Now?,,['limits']
2,Calculate the limit of $\left(\frac{f(x)}{x}\right)^{1/x}$,Calculate the limit of,\left(\frac{f(x)}{x}\right)^{1/x},"Let $f(x),f:R\to R$ be a non-constant continuous function such that  $$\left(e^x-1\right)f(2x)=\left(e^{2x}-1\right)f(x)\,.$$If $f'(0)=1$, then $$\lim_{x\to \infty}\left(\frac{f(x)}{x}\right)^{1/x}$$ My approach : Well I just dont get the idea of how to start, I separated the $f(x)$ terms and tried to solve but got nowhere. Any help will be appreciated, thanks.","Let $f(x),f:R\to R$ be a non-constant continuous function such that  $$\left(e^x-1\right)f(2x)=\left(e^{2x}-1\right)f(x)\,.$$If $f'(0)=1$, then $$\lim_{x\to \infty}\left(\frac{f(x)}{x}\right)^{1/x}$$ My approach : Well I just dont get the idea of how to start, I separated the $f(x)$ terms and tried to solve but got nowhere. Any help will be appreciated, thanks.",,"['limits', 'exponential-function']"
3,"Difficult limit $\lim_{x\to\infty} e^{-x}\int_0^x \int_0^x \frac{e^u-e^v}{u-v} \,\mathrm{d}u\mathrm{d}v$",Difficult limit,"\lim_{x\to\infty} e^{-x}\int_0^x \int_0^x \frac{e^u-e^v}{u-v} \,\mathrm{d}u\mathrm{d}v","I need to calculate this limit: $$\lim_{x\to\infty} e^{-x}\int_0^x \int_0^x \frac{e^u-e^v}{u-v} \,\mathrm{d}u\mathrm{d}v.$$ How do I do it? There's a hint that I should use de l'Hospital's rule.","I need to calculate this limit: $$\lim_{x\to\infty} e^{-x}\int_0^x \int_0^x \frac{e^u-e^v}{u-v} \,\mathrm{d}u\mathrm{d}v.$$ How do I do it? There's a hint that I should use de l'Hospital's rule.",,"['real-analysis', 'limits']"
4,How to prove that $\lim_{x\to 1^-} \left(\left(\sum_{n=1}^{\infty}x^n \right)\cdot \log\left(\frac{1}{x}\right) \right)= 1$ WITHOUT computing the sum,How to prove that  WITHOUT computing the sum,\lim_{x\to 1^-} \left(\left(\sum_{n=1}^{\infty}x^n \right)\cdot \log\left(\frac{1}{x}\right) \right)= 1,That's it. I want to know a way to do it so i can use it to prove other results like this ones: $$ \lim_{x \to 1^-} \left(\left(\sum_{n=1}^{\infty}x^{n^2} \right)\cdot \sqrt{\log\left(\frac{1}{x}\right)} \right)= \frac{\sqrt{\pi}}{2}$$ $$ \lim_{x \to 1^-} \left(\left(\sum_{n=1}^{\infty}x^{n^m} \right)\cdot \log\left(\frac{1}{x}\right)^{1/m} \right)= \frac{\Gamma \left(\frac{1}{m}\right)}{m}$$,That's it. I want to know a way to do it so i can use it to prove other results like this ones: $$ \lim_{x \to 1^-} \left(\left(\sum_{n=1}^{\infty}x^{n^2} \right)\cdot \sqrt{\log\left(\frac{1}{x}\right)} \right)= \frac{\sqrt{\pi}}{2}$$ $$ \lim_{x \to 1^-} \left(\left(\sum_{n=1}^{\infty}x^{n^m} \right)\cdot \log\left(\frac{1}{x}\right)^{1/m} \right)= \frac{\Gamma \left(\frac{1}{m}\right)}{m}$$,,"['limits', 'summation', 'gamma-function']"
5,"Proving $\lim_{(x,y)\to (0,0)}\frac{xy}{\sqrt{x^2 +y^2}}=0$ in a strange way: Does it even make sense?",Proving  in a strange way: Does it even make sense?,"\lim_{(x,y)\to (0,0)}\frac{xy}{\sqrt{x^2 +y^2}}=0","I need to prove that $$\lim_{(x,y)\to (0,0)}\frac{xy}{\sqrt{x^2 +y^2}}=0$$ And I have already found some questions with the exact same question but I guess I proceeded in a somewhat different way. The limit exists if for each $\epsilon>0$, there is $\delta>0$ such that $$\left|\cfrac{xy}{\sqrt{x^2+y^2}} -L\right|<\epsilon$$ And $|x-a|<\delta,|x-a|<\delta$ or $|(x,y)-(a,b)|<\delta$ and $x\neq a, y \neq b $. I made a somewhat experimental approach and would like to talk about it. I learned that we can try to approach the limit with some paths: polar coordinates, lines, etc. So I give a shot: As the limit goes to $(0,0)$, it seemed natural to try: $x \to \frac{1}{m} $ as $m \to \infty $, and $y\to \frac{1}{n}$ as $n\to \infty$, this gives me: $$\left|\cfrac{1}{\sqrt{m^2+n^2}}\right|<\epsilon$$ $$\left|\frac{1}{m}\right |<\delta \quad \quad  \left|\frac{1}{n}\right|<\delta $$ And with this, I guess that we can see that we can approach $0$ arbitrarily. But if we take the other form: $$\left|\sqrt{\frac{1}{m^2}+\frac{1}{n^2}}\right|<\delta$$ $$\left|\sqrt{\frac{m^2+n^2}{m^2n^2}}\right|<\delta$$ $$\left|\frac{\sqrt{m^2+n^2}}{ mn} \right|<\delta$$ But now it seems that we can't have a $\delta$ for every $\epsilon$ chosen. So how can they be equivalent? I may be missing something. Also, one important question: Q: I learned the definitions with $\epsilon, \delta$ and the idea of looking for a possible limit by some paths as different ways: The first one seems to be used to prove that there is a limit, but the second one seems to be a cheap method to gather evidence that it couldn't be (if the value is different for two paths, then the limit does not exist), but in the above example, I merged them both. Is this usually viable or do I incur the same problem of not having the same value for different paths implies the non-existence of the limit? I had also the (incomplete?) idea of trying to approach the limit with a circle $a^2+b^2=c^2$ and take the limit of $c\to 0$ but am a little confused if it could be done. $$\left|\frac{\sqrt{c^2-a^2} \sqrt{c^2-b^2}}{\sqrt{-a^2-b^2+2 c^2}}-0\right|<\epsilon$$ $$\left|\frac{\sqrt{c^2-a^2} \sqrt{c^2-b^2}}{\sqrt{-a^2-b^2+ c^2+c^2}}\right|<\epsilon\tag{$c^2-a^2-c^2=0$}$$ $$\left|\frac{\sqrt{c^2-a^2} \sqrt{c^2-b^2}}{c}\right|<\epsilon$$ As $b^2=c^2-a^2$ and $a^2=c^2-b^2$, then: $$\left|\frac{ba}{c}\right|<\epsilon$$ As $c\to 0$, I guess we can make the substitution $c:= \frac{1}{n}$ with $n\to \infty$, then: $$\left|abn\right|<\epsilon$$ And here, as $n\to \infty $, $c\to 0$ and due to $a^2+b^2=c^2$, then $a,b\to 0$. As for the $\delta$, then: $$\left|\left( \sqrt{c^2-a^2},\sqrt{c^2-b^2} \right) -(0,0)\right|<\delta$$ $$\left| \left( \sqrt{c^2-a^2+c^2-b^2  }\right)\right|<\delta\tag{$c^2-a^2-c^2=0$}$$ $$|c|<\delta$$ Are these moves acceptable? If not, do you know at least one counterexample? I'd like to see why it doesn't work - if it doesn't work.","I need to prove that $$\lim_{(x,y)\to (0,0)}\frac{xy}{\sqrt{x^2 +y^2}}=0$$ And I have already found some questions with the exact same question but I guess I proceeded in a somewhat different way. The limit exists if for each $\epsilon>0$, there is $\delta>0$ such that $$\left|\cfrac{xy}{\sqrt{x^2+y^2}} -L\right|<\epsilon$$ And $|x-a|<\delta,|x-a|<\delta$ or $|(x,y)-(a,b)|<\delta$ and $x\neq a, y \neq b $. I made a somewhat experimental approach and would like to talk about it. I learned that we can try to approach the limit with some paths: polar coordinates, lines, etc. So I give a shot: As the limit goes to $(0,0)$, it seemed natural to try: $x \to \frac{1}{m} $ as $m \to \infty $, and $y\to \frac{1}{n}$ as $n\to \infty$, this gives me: $$\left|\cfrac{1}{\sqrt{m^2+n^2}}\right|<\epsilon$$ $$\left|\frac{1}{m}\right |<\delta \quad \quad  \left|\frac{1}{n}\right|<\delta $$ And with this, I guess that we can see that we can approach $0$ arbitrarily. But if we take the other form: $$\left|\sqrt{\frac{1}{m^2}+\frac{1}{n^2}}\right|<\delta$$ $$\left|\sqrt{\frac{m^2+n^2}{m^2n^2}}\right|<\delta$$ $$\left|\frac{\sqrt{m^2+n^2}}{ mn} \right|<\delta$$ But now it seems that we can't have a $\delta$ for every $\epsilon$ chosen. So how can they be equivalent? I may be missing something. Also, one important question: Q: I learned the definitions with $\epsilon, \delta$ and the idea of looking for a possible limit by some paths as different ways: The first one seems to be used to prove that there is a limit, but the second one seems to be a cheap method to gather evidence that it couldn't be (if the value is different for two paths, then the limit does not exist), but in the above example, I merged them both. Is this usually viable or do I incur the same problem of not having the same value for different paths implies the non-existence of the limit? I had also the (incomplete?) idea of trying to approach the limit with a circle $a^2+b^2=c^2$ and take the limit of $c\to 0$ but am a little confused if it could be done. $$\left|\frac{\sqrt{c^2-a^2} \sqrt{c^2-b^2}}{\sqrt{-a^2-b^2+2 c^2}}-0\right|<\epsilon$$ $$\left|\frac{\sqrt{c^2-a^2} \sqrt{c^2-b^2}}{\sqrt{-a^2-b^2+ c^2+c^2}}\right|<\epsilon\tag{$c^2-a^2-c^2=0$}$$ $$\left|\frac{\sqrt{c^2-a^2} \sqrt{c^2-b^2}}{c}\right|<\epsilon$$ As $b^2=c^2-a^2$ and $a^2=c^2-b^2$, then: $$\left|\frac{ba}{c}\right|<\epsilon$$ As $c\to 0$, I guess we can make the substitution $c:= \frac{1}{n}$ with $n\to \infty$, then: $$\left|abn\right|<\epsilon$$ And here, as $n\to \infty $, $c\to 0$ and due to $a^2+b^2=c^2$, then $a,b\to 0$. As for the $\delta$, then: $$\left|\left( \sqrt{c^2-a^2},\sqrt{c^2-b^2} \right) -(0,0)\right|<\delta$$ $$\left| \left( \sqrt{c^2-a^2+c^2-b^2  }\right)\right|<\delta\tag{$c^2-a^2-c^2=0$}$$ $$|c|<\delta$$ Are these moves acceptable? If not, do you know at least one counterexample? I'd like to see why it doesn't work - if it doesn't work.",,"['real-analysis', 'limits', 'multivariable-calculus']"
6,Find the limit of the expression,Find the limit of the expression,,Find   $$ \lim_{x\to\infty}\frac{\sqrt{1-\cos^3(1/x)}\cdot(3^{1/x}-5^{-1/x})} {\log_2( 1+x^{-2} + x^{-3})}$$ Firstly I replace $x$ with $1/t$. Then $t$ tends to $0$. And then I fix numerator with formulas for limits considering $t \to 0$.  But I don't know what to do with denominator. Any help?,Find   $$ \lim_{x\to\infty}\frac{\sqrt{1-\cos^3(1/x)}\cdot(3^{1/x}-5^{-1/x})} {\log_2( 1+x^{-2} + x^{-3})}$$ Firstly I replace $x$ with $1/t$. Then $t$ tends to $0$. And then I fix numerator with formulas for limits considering $t \to 0$.  But I don't know what to do with denominator. Any help?,,"['calculus', 'limits']"
7,Harlan Brothers' formula of e hidden in pascal triangle,Harlan Brothers' formula of e hidden in pascal triangle,,"I was observing here and conjecture $(1)$ $$\lim_{n\to\infty}{S_{n-1}S_{n+2}\over S_nS_{n+1}}=e^2\tag1$$ Given that Harlan Brothers' formula $(2)$ $$\lim_{n\to\infty}{S_{n-1}S_{n+1}\over S_n^2}=e\tag2$$ Trying to prove $(1)$: $(1)\div(2)$ $$\lim_{n\to\infty}{S_{n-1}S_{n+2}\over S_nS_{n+1}}\times{S_n^2\over S_{n-1}S_{n+1}}=e\tag3$$ Simplified to $$\lim_{n\to\infty}{S_{n}S_{n+2}\over S_{n+1}^2}=e\tag4$$ $(4)$ which is the same as $(2)$, so hence we can say $(1)$ is correct? 2nd conjecture Another conjecture from observing $(1)\div(2)^2$, simplified to $$\lim_{n\to\infty}{S_n^3S_{n+2}\over S_{n-1}S_{n+1}^3}=1\tag5$$ I noticed that it takes the binomial coefficients,you can see a pattern emerges $$\lim_{n\to\infty}{S_n^4S_{n+2}^4\over S_{n-1}S_{n+1}^6S_{n+3}}=1\tag6$$ $$\lim_{n\to\infty}{S_n^5S_{n+2}^{10}S_{n+4}\over S_{n-1}S_{n+1}^{10}S_{n+3}^5}=1\tag7$$ and so on ... We can write as $$\lim_{n\to \infty}\prod_{k=0}^{m}S_{n+k-1}^{(-1)^{k+1}{m\choose k}}=1\tag8$$ $m\ge3$ Numerically we have verified for certain range of $S_n$, but it is not necessarily indicate that it is true for larger values of $S_n$ How can we prove $(8)?$","I was observing here and conjecture $(1)$ $$\lim_{n\to\infty}{S_{n-1}S_{n+2}\over S_nS_{n+1}}=e^2\tag1$$ Given that Harlan Brothers' formula $(2)$ $$\lim_{n\to\infty}{S_{n-1}S_{n+1}\over S_n^2}=e\tag2$$ Trying to prove $(1)$: $(1)\div(2)$ $$\lim_{n\to\infty}{S_{n-1}S_{n+2}\over S_nS_{n+1}}\times{S_n^2\over S_{n-1}S_{n+1}}=e\tag3$$ Simplified to $$\lim_{n\to\infty}{S_{n}S_{n+2}\over S_{n+1}^2}=e\tag4$$ $(4)$ which is the same as $(2)$, so hence we can say $(1)$ is correct? 2nd conjecture Another conjecture from observing $(1)\div(2)^2$, simplified to $$\lim_{n\to\infty}{S_n^3S_{n+2}\over S_{n-1}S_{n+1}^3}=1\tag5$$ I noticed that it takes the binomial coefficients,you can see a pattern emerges $$\lim_{n\to\infty}{S_n^4S_{n+2}^4\over S_{n-1}S_{n+1}^6S_{n+3}}=1\tag6$$ $$\lim_{n\to\infty}{S_n^5S_{n+2}^{10}S_{n+4}\over S_{n-1}S_{n+1}^{10}S_{n+3}^5}=1\tag7$$ and so on ... We can write as $$\lim_{n\to \infty}\prod_{k=0}^{m}S_{n+k-1}^{(-1)^{k+1}{m\choose k}}=1\tag8$$ $m\ge3$ Numerically we have verified for certain range of $S_n$, but it is not necessarily indicate that it is true for larger values of $S_n$ How can we prove $(8)?$",,"['limits', 'products']"
8,Why the limit does not exist,Why the limit does not exist,,Why the limit of this greatest integer function doesn't exist: $$\lim_{x\to0}[x]$$ My attempt: L.H.L.= $$\lim _{x\to0^-}[x]$$ $$=\lim_{h\to0}[0-h]$$ $$=\lim_{h\to0}[0-0]$$ $$=\lim_{h\to0}[0]$$ $$=0$$ R.H.L.= $$\lim _{x\to0^+}[x]$$ $$=\lim_{h\to0}[0+h]$$ $$=\lim_{h\to0}[0+0]$$ $$=\lim_{h\to0}[0]$$ $$=0$$,Why the limit of this greatest integer function doesn't exist: $$\lim_{x\to0}[x]$$ My attempt: L.H.L.= $$\lim _{x\to0^-}[x]$$ $$=\lim_{h\to0}[0-h]$$ $$=\lim_{h\to0}[0-0]$$ $$=\lim_{h\to0}[0]$$ $$=0$$ R.H.L.= $$\lim _{x\to0^+}[x]$$ $$=\lim_{h\to0}[0+h]$$ $$=\lim_{h\to0}[0+0]$$ $$=\lim_{h\to0}[0]$$ $$=0$$,,['limits']
9,How fast does $\lim_{x\to 1^-} \sum_{k=0}^\infty \left( x^{k^2}-x^{(k+\alpha)^2}\right)$ go to $\alpha$?,How fast does  go to ?,\lim_{x\to 1^-} \sum_{k=0}^\infty \left( x^{k^2}-x^{(k+\alpha)^2}\right) \alpha,"In this question: $\lim_{x\to 1^-} \sum_{k=0}^\infty \left( x^{k^2}-x^{(k+\alpha)^2}\right)$ it is established that $$\lim_{x\to 1^-} \sum_{k=0}^\infty \left( x^{k^2}-x^{(k+\alpha)^2}\right) = \alpha$$  for all $\alpha > 0$. I'd like to know how fast it approaches $\alpha$. In particular, to show that the rate of approach is such that for all $\alpha$  $$\lim_{x\to 1^-}\frac{\alpha -  \sum_{k=0}^\infty \left( x^{k^2}-x^{(k+\alpha)^2}\right)}{1-x}$$ exists, and that when $\alpha$ is very small, $$\lim_{\alpha\to 0^+} \left[ \lim_{x\to 1^-}\frac{\alpha -  \sum_{k=0}^\infty \left( x^{k^2}-x^{(k+\alpha)^2}\right)}{1-x} \right]=\frac16$$","In this question: $\lim_{x\to 1^-} \sum_{k=0}^\infty \left( x^{k^2}-x^{(k+\alpha)^2}\right)$ it is established that $$\lim_{x\to 1^-} \sum_{k=0}^\infty \left( x^{k^2}-x^{(k+\alpha)^2}\right) = \alpha$$  for all $\alpha > 0$. I'd like to know how fast it approaches $\alpha$. In particular, to show that the rate of approach is such that for all $\alpha$  $$\lim_{x\to 1^-}\frac{\alpha -  \sum_{k=0}^\infty \left( x^{k^2}-x^{(k+\alpha)^2}\right)}{1-x}$$ exists, and that when $\alpha$ is very small, $$\lim_{\alpha\to 0^+} \left[ \lim_{x\to 1^-}\frac{\alpha -  \sum_{k=0}^\infty \left( x^{k^2}-x^{(k+\alpha)^2}\right)}{1-x} \right]=\frac16$$",,"['limits', 'digamma-function', 'euler-maclaurin', 'lacunary-series']"
10,What is the relationship between big O notation and the limits of functions?,What is the relationship between big O notation and the limits of functions?,,"Say we have two functions: $f(n) = n$ and $g(n) = 2n$. $$\lim_{n\to\infty} \frac{f(n)}{g(n)} = \lim_{n\to\infty} \frac{n}{2n} = \lim_{n\to\infty} \frac{1}{2} = \frac{1}{2}$$ Therefore, according to the answer here , function $g(n)$ grows faster than $f(n)$ because: $$0 \leq \lim_{n\to\infty} \frac{f(n)}{g(n)} < 1$$ But with big O notation: $f(n) = O(n)$ and $g(n) = O(n)$ Which means that both functions grow at the same rate (in an apparently different magnitude). I realise that this is only happening because the coefficient of the largest term is ignored with big O notation but I want to know if you can use limits to give information about the big O notation of a function and vice versa because I find some of this terminology and these concepts hard to differentiate. Thanks.","Say we have two functions: $f(n) = n$ and $g(n) = 2n$. $$\lim_{n\to\infty} \frac{f(n)}{g(n)} = \lim_{n\to\infty} \frac{n}{2n} = \lim_{n\to\infty} \frac{1}{2} = \frac{1}{2}$$ Therefore, according to the answer here , function $g(n)$ grows faster than $f(n)$ because: $$0 \leq \lim_{n\to\infty} \frac{f(n)}{g(n)} < 1$$ But with big O notation: $f(n) = O(n)$ and $g(n) = O(n)$ Which means that both functions grow at the same rate (in an apparently different magnitude). I realise that this is only happening because the coefficient of the largest term is ignored with big O notation but I want to know if you can use limits to give information about the big O notation of a function and vice versa because I find some of this terminology and these concepts hard to differentiate. Thanks.",,"['limits', 'asymptotics']"
11,Prove that $\lim_{x\to0}\frac{\sin x}x=1$ using algebraic manipulations of derivatives?,Prove that  using algebraic manipulations of derivatives?,\lim_{x\to0}\frac{\sin x}x=1,"I can prove that $\lim_{x\to0}\frac{\cos(x)-1}x=0$ since $$\sin^2(x)=1-\cos^2(x)$$ $$\implies2\sin'(x)\sin(x)=-2\cos'(x)\cos(x)$$ $$\sin'(x)\sin(x)=-\cos'(x)\cos(x)$$ at $x=0$, we have $$0=-\cos'(0)$$ Thus, $\cos'(0)=\lim_{x\to0}\frac{\cos(x)-1}x=0$. Can one produce the same result for the famous $\lim\limits_{x\to0}\frac{\sin(x)}x=1$ by manipulating derivatives? Particularly, can we calculate $\sin'(0)$ without first showing that $\sin'(x)=\cos(x)$? Edit: As has been shown, we need more than just trig identities to prove this, since trig identities work regardless of the radian/degrees while the limit does not.  So consider the following information: $$0\le\frac{\sin(x+t)-\sin(x)}t\le\cos(x)\ \forall\ x\in(0,\frac\pi2),\ t\in\left(0,\frac\pi2-x\right)$$ The last inequality proven geometrically in this answer . Thus, we get $$0\le\sin'(0)\le\cos(0)$$ As of yet, I'm unsure what other information should be required, mainly how to deal with the units issue.","I can prove that $\lim_{x\to0}\frac{\cos(x)-1}x=0$ since $$\sin^2(x)=1-\cos^2(x)$$ $$\implies2\sin'(x)\sin(x)=-2\cos'(x)\cos(x)$$ $$\sin'(x)\sin(x)=-\cos'(x)\cos(x)$$ at $x=0$, we have $$0=-\cos'(0)$$ Thus, $\cos'(0)=\lim_{x\to0}\frac{\cos(x)-1}x=0$. Can one produce the same result for the famous $\lim\limits_{x\to0}\frac{\sin(x)}x=1$ by manipulating derivatives? Particularly, can we calculate $\sin'(0)$ without first showing that $\sin'(x)=\cos(x)$? Edit: As has been shown, we need more than just trig identities to prove this, since trig identities work regardless of the radian/degrees while the limit does not.  So consider the following information: $$0\le\frac{\sin(x+t)-\sin(x)}t\le\cos(x)\ \forall\ x\in(0,\frac\pi2),\ t\in\left(0,\frac\pi2-x\right)$$ The last inequality proven geometrically in this answer . Thus, we get $$0\le\sin'(0)\le\cos(0)$$ As of yet, I'm unsure what other information should be required, mainly how to deal with the units issue.",,"['limits', 'trigonometry', 'derivatives', 'alternative-proof']"
12,L'Hôpital's rule does not apply?!,L'Hôpital's rule does not apply?!,,"Apparently, Rogawski's Calculus for AP contains the following problem: 108. Explain why L'Hôpital's rule does not apply to $$ \lim_{x\rightarrow 0}\frac{x^2\sin\frac{1}{x}}{\sin x} $$ It seems to me that it does apply: The L'Hôpital's rule says: if $\lim_{x\rightarrow c}f(x)=\lim_{x\rightarrow c}g(x)=0$ and both $f$ and $g$ are differentiable at $x=c$ and $g'(c)\ne 0$, then $\lim_{x\rightarrow c}\frac{f(x)}{g(x)}$ exists and is equal to $\frac{f'(c)}{g'(c)}$. (Note that nothing is assumed about differentiability of $f$ and $g$ other than at $x=c$). Define the numerator $f(x)=x^2\sin\frac{1}{x}$ to be $f(0)=0$ at $x=0$. Now, both numerator $f$ and denominator $g(x)=\sin(x)$ are continuous at $x=0$ and their values are $f(0)=g(0)=0$. The numerator $f$ is differentiable at $x=0$ and the derivative is $f'(0)=0$ (the derivative itself is discontinuous at 0, but that is irrelevant - even the existence of the derivative at any point other than 0 does NOT matter). One can see that from the definition of the derivative: $f'(0)=\lim_{h\rightarrow 0} \frac{h^2\sin\frac{1}{h}}{h} =  \lim_{x\rightarrow 0} h\sin\frac{1}{h} = 0$ (see PS step 2 below). The denominator $g$ is differentiable at $x=0$ and the derivative is $g'(0)=\cos 0=1$. Thus the limit is $\frac{0}{1} = 0$. What am I missing? PS. Note that I am not asking why the limit is 0. That can be easily seen without L'Hôpital: $\lim_{x\rightarrow 0}\frac{x}{\sin x} = 1$: this is the inverse of the standard limit $\lim_{x\rightarrow 0}\frac{\sin x}{x} = 1$. $\lim_{x\rightarrow 0} x  \sin\frac{1}{x} = 0$ because $\sin\frac{1}{x}$ is bounded and $\lim_{x\rightarrow 0} x = 0$, this follows from Squeeze theorem . the Product Rule for Limits implies that  $$\lim_{x\rightarrow 0}\frac{x^2\sin\frac{1}{x}}{\sin x} =  \lim_{x\rightarrow 0}x\sin\frac{1}{x} \times \lim_{x\rightarrow 0}\frac{x}{\sin x} = 0 \times 1 = 0$$ PPS Here is the scan from the textbook:","Apparently, Rogawski's Calculus for AP contains the following problem: 108. Explain why L'Hôpital's rule does not apply to $$ \lim_{x\rightarrow 0}\frac{x^2\sin\frac{1}{x}}{\sin x} $$ It seems to me that it does apply: The L'Hôpital's rule says: if $\lim_{x\rightarrow c}f(x)=\lim_{x\rightarrow c}g(x)=0$ and both $f$ and $g$ are differentiable at $x=c$ and $g'(c)\ne 0$, then $\lim_{x\rightarrow c}\frac{f(x)}{g(x)}$ exists and is equal to $\frac{f'(c)}{g'(c)}$. (Note that nothing is assumed about differentiability of $f$ and $g$ other than at $x=c$). Define the numerator $f(x)=x^2\sin\frac{1}{x}$ to be $f(0)=0$ at $x=0$. Now, both numerator $f$ and denominator $g(x)=\sin(x)$ are continuous at $x=0$ and their values are $f(0)=g(0)=0$. The numerator $f$ is differentiable at $x=0$ and the derivative is $f'(0)=0$ (the derivative itself is discontinuous at 0, but that is irrelevant - even the existence of the derivative at any point other than 0 does NOT matter). One can see that from the definition of the derivative: $f'(0)=\lim_{h\rightarrow 0} \frac{h^2\sin\frac{1}{h}}{h} =  \lim_{x\rightarrow 0} h\sin\frac{1}{h} = 0$ (see PS step 2 below). The denominator $g$ is differentiable at $x=0$ and the derivative is $g'(0)=\cos 0=1$. Thus the limit is $\frac{0}{1} = 0$. What am I missing? PS. Note that I am not asking why the limit is 0. That can be easily seen without L'Hôpital: $\lim_{x\rightarrow 0}\frac{x}{\sin x} = 1$: this is the inverse of the standard limit $\lim_{x\rightarrow 0}\frac{\sin x}{x} = 1$. $\lim_{x\rightarrow 0} x  \sin\frac{1}{x} = 0$ because $\sin\frac{1}{x}$ is bounded and $\lim_{x\rightarrow 0} x = 0$, this follows from Squeeze theorem . the Product Rule for Limits implies that  $$\lim_{x\rightarrow 0}\frac{x^2\sin\frac{1}{x}}{\sin x} =  \lim_{x\rightarrow 0}x\sin\frac{1}{x} \times \lim_{x\rightarrow 0}\frac{x}{\sin x} = 0 \times 1 = 0$$ PPS Here is the scan from the textbook:",,"['calculus', 'limits']"
13,Calculate $\lim\limits_{n \to \infty} \frac1n\cdot\log\left(3^\frac{n}{1} + 3^\frac{n}{2} + \dots + 3^\frac{n}{n}\right)$ [duplicate],Calculate  [duplicate],\lim\limits_{n \to \infty} \frac1n\cdot\log\left(3^\frac{n}{1} + 3^\frac{n}{2} + \dots + 3^\frac{n}{n}\right),"This question already has answers here : Evaluating $\lim\limits_{n\to\infty}(a_1^n+\dots+a_k^n)^{1\over n}$ where $a_1 \ge \cdots\ge a_k \ge 0$ [duplicate] (2 answers) Closed 6 years ago . Calculate $L = \lim\limits_{n \to \infty} \frac1n\cdot\log\left(3^\frac{n}{1} + 3^\frac{n}{2} + \dots + 3^\frac{n}{n}\right)$ I tried putting $\frac1n$ as a power of the logarithm and taking it out of the limit, so I got $$ L = \log\lim\limits_{n \to \infty} \left(3^\frac{n}{1} + 3^\frac{n}{2} + \dots + 3^\frac{n}{n}\right)^\frac1n $$ At this point I thought of the fact that $\lim\limits_{n \to \infty} \sqrt[n]{a_1^n+a_2^n+\dots+a_k^n} = max\{a_1, a_2, \dots,a_k\}$ but this won't be of any use here, I guess. How can I calculate this limit, please?","This question already has answers here : Evaluating $\lim\limits_{n\to\infty}(a_1^n+\dots+a_k^n)^{1\over n}$ where $a_1 \ge \cdots\ge a_k \ge 0$ [duplicate] (2 answers) Closed 6 years ago . Calculate $L = \lim\limits_{n \to \infty} \frac1n\cdot\log\left(3^\frac{n}{1} + 3^\frac{n}{2} + \dots + 3^\frac{n}{n}\right)$ I tried putting $\frac1n$ as a power of the logarithm and taking it out of the limit, so I got $$ L = \log\lim\limits_{n \to \infty} \left(3^\frac{n}{1} + 3^\frac{n}{2} + \dots + 3^\frac{n}{n}\right)^\frac1n $$ At this point I thought of the fact that $\lim\limits_{n \to \infty} \sqrt[n]{a_1^n+a_2^n+\dots+a_k^n} = max\{a_1, a_2, \dots,a_k\}$ but this won't be of any use here, I guess. How can I calculate this limit, please?",,"['analysis', 'limits']"
14,Convergence of integrals - both finite but different,Convergence of integrals - both finite but different,,"Given a measure space, is it possible to have a sequence of integrable functions that converges pointwise, such that the integral of the limit differs from the limit of the sequence of integrals, both quantities being finite ? All counterexamples I've seen illustrating the utility of Lebesgue's convergence theorems involve at least one of those quantities being infinite. I'm thinking some kind of function with a triangle somewhere that rises and gets narrower such that the area remains constant would do it but I can't quite reach my point","Given a measure space, is it possible to have a sequence of integrable functions that converges pointwise, such that the integral of the limit differs from the limit of the sequence of integrals, both quantities being finite ? All counterexamples I've seen illustrating the utility of Lebesgue's convergence theorems involve at least one of those quantities being infinite. I'm thinking some kind of function with a triangle somewhere that rises and gets narrower such that the area remains constant would do it but I can't quite reach my point",,"['integration', 'limits', 'convergence-divergence']"
15,How to calculate $\lim_{x \to 0} \left( \cos(\sin x) + \frac{x^2}{2} \right)^{\frac{1}{(e^{x^2} -1) \left( 1 + 2x - \sqrt{1 + 4x + 2x^2}\right)}}$?,How to calculate ?,\lim_{x \to 0} \left( \cos(\sin x) + \frac{x^2}{2} \right)^{\frac{1}{(e^{x^2} -1) \left( 1 + 2x - \sqrt{1 + 4x + 2x^2}\right)}},"I need some help with computing this limit: $$\lim_{x \to 0} \left( \cos(\sin x) + \frac{x^2}{2} \right)^{\frac{1}{(e^{x^2} -1) \left( 1 + 2x - \sqrt{1 + 4x + 2x^2}\right)}}$$ I'm guessing that I should do a Taylor expansion of $\cos(\sin x)$ in base, but what should I do with the stuff in the exponent?","I need some help with computing this limit: $$\lim_{x \to 0} \left( \cos(\sin x) + \frac{x^2}{2} \right)^{\frac{1}{(e^{x^2} -1) \left( 1 + 2x - \sqrt{1 + 4x + 2x^2}\right)}}$$ I'm guessing that I should do a Taylor expansion of $\cos(\sin x)$ in base, but what should I do with the stuff in the exponent?",,"['calculus', 'limits']"
16,Proving $\lim\limits_{x\rightarrow 1} \frac{x^2+3}{x+1}=2$ using the formal definition of the limit,Proving  using the formal definition of the limit,\lim\limits_{x\rightarrow 1} \frac{x^2+3}{x+1}=2,"Prove $\lim\limits_{x\rightarrow 1} \frac{x^2+3}{x+1}=2$ using the formal definition of the limit. My question is, I've picked $\delta\lt1$, and I've found that $\delta \lt \min(1,\sqrt{\epsilon})$. Was picking $1$ problematic at all? and is my choice for $\delta$ correct? Rest of Proof: $$0\lt|x-1|\lt \delta \Rightarrow \left|\frac{(x-1)^2}{x+1}\right|\lt \epsilon$$ Picking $\delta \lt 1$: $$|x-1|\lt 1 \Rightarrow -1\lt x-1 \lt 1$$ And we get from that $\frac13 \lt \frac{1}{x+1} \lt 1$ which leads to $\left|\frac{1}{x+1}\right| \lt 1$ Let's go back: $$\left|\frac{(x-1)^2}{x+1}\right|\lt \left|1(x-1)^2\right|\lt \epsilon$$ Since $(x-1)^2\gt 0$ we can get rid of the absolute value and we get $$(x-1)^2\lt \epsilon \rightarrow x-1 \lt \sqrt{\epsilon}$$ Also: What is the difference between picking $\delta=1$ and $\delta \lt 1$","Prove $\lim\limits_{x\rightarrow 1} \frac{x^2+3}{x+1}=2$ using the formal definition of the limit. My question is, I've picked $\delta\lt1$, and I've found that $\delta \lt \min(1,\sqrt{\epsilon})$. Was picking $1$ problematic at all? and is my choice for $\delta$ correct? Rest of Proof: $$0\lt|x-1|\lt \delta \Rightarrow \left|\frac{(x-1)^2}{x+1}\right|\lt \epsilon$$ Picking $\delta \lt 1$: $$|x-1|\lt 1 \Rightarrow -1\lt x-1 \lt 1$$ And we get from that $\frac13 \lt \frac{1}{x+1} \lt 1$ which leads to $\left|\frac{1}{x+1}\right| \lt 1$ Let's go back: $$\left|\frac{(x-1)^2}{x+1}\right|\lt \left|1(x-1)^2\right|\lt \epsilon$$ Since $(x-1)^2\gt 0$ we can get rid of the absolute value and we get $$(x-1)^2\lt \epsilon \rightarrow x-1 \lt \sqrt{\epsilon}$$ Also: What is the difference between picking $\delta=1$ and $\delta \lt 1$",,"['calculus', 'limits', 'epsilon-delta']"
17,Different way solving limit $\lim \limits_{ x\rightarrow 0 }{ { x }^{ x } } $,Different way solving limit,\lim \limits_{ x\rightarrow 0 }{ { x }^{ x } } ,"I know how to solve this problem by using L'Hospital's rule $$\lim \limits_{ x\rightarrow 0 }{ { x }^{ x } } =\lim\limits _{ x\rightarrow 0 }{ { e }^{ x\ln { x }  } } =\lim\limits _{ x\rightarrow 0 }{ { e }^{ \frac { \ln { x }  }{ \frac { 1 }{ x }  }  } } =\lim\limits _{ x\rightarrow 0 }{ { e }^{ \frac { \frac { 1 }{ x }  }{ \frac { 1 }{ { x }^{ 2 } }  }  } } ={ e }^{ 0 }=1,$$ what other different ways can you suggest or show.thanks","I know how to solve this problem by using L'Hospital's rule $$\lim \limits_{ x\rightarrow 0 }{ { x }^{ x } } =\lim\limits _{ x\rightarrow 0 }{ { e }^{ x\ln { x }  } } =\lim\limits _{ x\rightarrow 0 }{ { e }^{ \frac { \ln { x }  }{ \frac { 1 }{ x }  }  } } =\lim\limits _{ x\rightarrow 0 }{ { e }^{ \frac { \frac { 1 }{ x }  }{ \frac { 1 }{ { x }^{ 2 } }  }  } } ={ e }^{ 0 }=1,$$ what other different ways can you suggest or show.thanks",,['real-analysis']
18,"What if epsilon, delta are rational?","What if epsilon, delta are rational?",,"The precise formulation of the (ε, δ)-definition of limit as written here in wikipedia https://en.wikipedia.org/wiki/(%CE%B5,_%CE%B4)-definition_of_limit doesn't specify which kind of number the epsilon, delta are. I wonder if it matters.","The precise formulation of the (ε, δ)-definition of limit as written here in wikipedia https://en.wikipedia.org/wiki/(%CE%B5,_%CE%B4)-definition_of_limit doesn't specify which kind of number the epsilon, delta are. I wonder if it matters.",,"['calculus', 'limits', 'epsilon-delta']"
19,Prove that the equation $x + \cos(x) + e^{x} = 0$ has *exactly* one root,Prove that the equation  has *exactly* one root,x + \cos(x) + e^{x} = 0,"Question : Prove that the equation $x + \cos(x) + e^{x} = 0$ has exactly one root This is what I thought of doing: $$\text{Let} \ \ \  f(x) = x + \cos(x) + e^{x}$$ By using the Intermediate Value Theorem on the open interval $(-\infty, \infty)$, and then by showing that $$\left(\lim_{x \to -\infty}f(x) < 0 < \lim_{x \to +\infty}f(x)\right) \lor \left(\lim_{x \to +\infty}f(x) < 0 < \lim_{x \to -\infty}f(x)\right)$$ I could show that $\exists\  x \in \mathbb{R} \ s.t.\  f(x) = 0$. However this method, although it does show the existence of an $x$ such that $f(x)=0$, it doesn't show that there is only one $x$ that satisfies the statement $f(x)=0$. The original question, suggest's using either Rolle's Theorem or the Mean Value Theorem , however we face the same problem with both theorems as both theorems prove the existence of at least a single $x$ (or any arbitrary number) satisfying their given statements, they don't prove the existence of only one $x$ satisfying their statements. All three theorem's I've mentioned here : Intermediate Value Theorem Rolle's Theorem Mean Value Theorem Can all be used to show that the equation $x + cos(x) + e^{x} = 0$ has at least one root, but they can't be used to show $x + cos(x) + e^{x} = 0$ has only one root. (Or can they?) How can this problem be solved then, using either Rolle's Theorem or the Mean Value Theorem (or even the Intermediate Value Theorem)","Question : Prove that the equation $x + \cos(x) + e^{x} = 0$ has exactly one root This is what I thought of doing: $$\text{Let} \ \ \  f(x) = x + \cos(x) + e^{x}$$ By using the Intermediate Value Theorem on the open interval $(-\infty, \infty)$, and then by showing that $$\left(\lim_{x \to -\infty}f(x) < 0 < \lim_{x \to +\infty}f(x)\right) \lor \left(\lim_{x \to +\infty}f(x) < 0 < \lim_{x \to -\infty}f(x)\right)$$ I could show that $\exists\  x \in \mathbb{R} \ s.t.\  f(x) = 0$. However this method, although it does show the existence of an $x$ such that $f(x)=0$, it doesn't show that there is only one $x$ that satisfies the statement $f(x)=0$. The original question, suggest's using either Rolle's Theorem or the Mean Value Theorem , however we face the same problem with both theorems as both theorems prove the existence of at least a single $x$ (or any arbitrary number) satisfying their given statements, they don't prove the existence of only one $x$ satisfying their statements. All three theorem's I've mentioned here : Intermediate Value Theorem Rolle's Theorem Mean Value Theorem Can all be used to show that the equation $x + cos(x) + e^{x} = 0$ has at least one root, but they can't be used to show $x + cos(x) + e^{x} = 0$ has only one root. (Or can they?) How can this problem be solved then, using either Rolle's Theorem or the Mean Value Theorem (or even the Intermediate Value Theorem)",,"['calculus', 'real-analysis', 'limits', 'derivatives']"
20,Calculate the limit $\lim_{n \to \infty}\frac{ \ln(n)^{(\ln n)}}{n!}$,Calculate the limit,\lim_{n \to \infty}\frac{ \ln(n)^{(\ln n)}}{n!},I wonder what the limit $\lim_{n \to \infty}\frac{ \ln n^{\ln n}}{n!}$ would be equal to. It is well known that the factorial function grow faster than an exponential but slower than $n^n$. But how about a combination of $\ln $ (natural logarithm) and exponential? I guess the answer is $0$ since for $e$ the value is quite small. If I show that the logarithm of the expressions tends to $-\infty$ then I would be done.  Using laws of logarithm I can write $(\ln n)^2-\ln(n)!=(\ln n)^2(1-\frac{\ln(n!)}{(\ln n)^2})$. Now I need to know the limit of $\frac{\ln(n!)}{(\ln n)^2}$. Any suggestions?,I wonder what the limit $\lim_{n \to \infty}\frac{ \ln n^{\ln n}}{n!}$ would be equal to. It is well known that the factorial function grow faster than an exponential but slower than $n^n$. But how about a combination of $\ln $ (natural logarithm) and exponential? I guess the answer is $0$ since for $e$ the value is quite small. If I show that the logarithm of the expressions tends to $-\infty$ then I would be done.  Using laws of logarithm I can write $(\ln n)^2-\ln(n)!=(\ln n)^2(1-\frac{\ln(n!)}{(\ln n)^2})$. Now I need to know the limit of $\frac{\ln(n!)}{(\ln n)^2}$. Any suggestions?,,"['calculus', 'real-analysis', 'limits']"
21,Show that $\frac{\Gamma(n+k)}{\Gamma(n)}\sim n^k$ for large values of n,Show that  for large values of n,\frac{\Gamma(n+k)}{\Gamma(n)}\sim n^k,"In order to prove the above result I proceeded as follows: We know that: $\Gamma(n)=(n-1)\Gamma(n-1)$ Using this fact, we have: $\Gamma(n+k)=(n+k-1)\Gamma(n+k-1)\\=(n+k-1)(n+k-2)\Gamma(n+k-2)\\ =...=(n+k-1)(n+k-2)...(n+k-k)\Gamma(n+k-k)\\=n(n+1)...(n+k-1)\Gamma(n)$ So, now we have: $$\lim\limits_{n\rightarrow\infty}\frac{\Gamma(n+k)}{\Gamma(n)}= \lim\limits_{n\rightarrow\infty}\frac{n(n+1)...(n+k-1)\Gamma(n)}{\Gamma(n)}$$ Now, if $x$ is a variable and $c$ is a constant, then $x+c\approx x$ (for very large values of $x$) ( I am a little doubtful about this ) Using this fact, can we say that: $\lim\limits_{n\rightarrow\infty}\frac{\Gamma(n+k)}{\Gamma(n)}=\lim\limits_{n\rightarrow\infty}n(n+1)...(n+k-1)\\\sim n.n...n\ (k\ times)\\=n^k$ Thus, we have: $$\frac{\Gamma(n+k)}{\Gamma(n)}\sim n^k$$ Is this method valid? Is there a more ""mathematically rigorous"" way to prove this? Thanks in advance!","In order to prove the above result I proceeded as follows: We know that: $\Gamma(n)=(n-1)\Gamma(n-1)$ Using this fact, we have: $\Gamma(n+k)=(n+k-1)\Gamma(n+k-1)\\=(n+k-1)(n+k-2)\Gamma(n+k-2)\\ =...=(n+k-1)(n+k-2)...(n+k-k)\Gamma(n+k-k)\\=n(n+1)...(n+k-1)\Gamma(n)$ So, now we have: $$\lim\limits_{n\rightarrow\infty}\frac{\Gamma(n+k)}{\Gamma(n)}= \lim\limits_{n\rightarrow\infty}\frac{n(n+1)...(n+k-1)\Gamma(n)}{\Gamma(n)}$$ Now, if $x$ is a variable and $c$ is a constant, then $x+c\approx x$ (for very large values of $x$) ( I am a little doubtful about this ) Using this fact, can we say that: $\lim\limits_{n\rightarrow\infty}\frac{\Gamma(n+k)}{\Gamma(n)}=\lim\limits_{n\rightarrow\infty}n(n+1)...(n+k-1)\\\sim n.n...n\ (k\ times)\\=n^k$ Thus, we have: $$\frac{\Gamma(n+k)}{\Gamma(n)}\sim n^k$$ Is this method valid? Is there a more ""mathematically rigorous"" way to prove this? Thanks in advance!",,"['limits', 'gamma-function']"
22,Evaluation of $\lim_{n\rightarrow \infty}\sqrt[n]{\sum^{n}_{k=1}\left(k^{999}+\frac{1}{\sqrt{k}}\right)}$,Evaluation of,\lim_{n\rightarrow \infty}\sqrt[n]{\sum^{n}_{k=1}\left(k^{999}+\frac{1}{\sqrt{k}}\right)},"Evaluation of $$\lim_{n\rightarrow \infty}\sqrt[n]{\sum^{n}_{k=1}\left(k^{999}+\frac{1}{\sqrt{k}}\right)}$$ $\bf{My\; Try::}$ First we will calculate $$\sum^{n}_{k=1}\left(k^{999}+\frac{1}{\sqrt{k}}\right)=n^{1000}\sum^{n}_{k=1}\left(\frac{k}{n}\right)^{999}\cdot \frac{1}{n}-\frac{1}{\sqrt{n}}\sum^{n}_{k=1}\sqrt{\frac{n}{k}}$$ Now How can I solve afeter that, Help me Thanks in Advanced","Evaluation of $$\lim_{n\rightarrow \infty}\sqrt[n]{\sum^{n}_{k=1}\left(k^{999}+\frac{1}{\sqrt{k}}\right)}$$ $\bf{My\; Try::}$ First we will calculate $$\sum^{n}_{k=1}\left(k^{999}+\frac{1}{\sqrt{k}}\right)=n^{1000}\sum^{n}_{k=1}\left(\frac{k}{n}\right)^{999}\cdot \frac{1}{n}-\frac{1}{\sqrt{n}}\sum^{n}_{k=1}\sqrt{\frac{n}{k}}$$ Now How can I solve afeter that, Help me Thanks in Advanced",,['limits']
23,Limit of a periodic function,Limit of a periodic function,,"I stumbled upon this question in my course, and I am out of ideas. Let $f$ be a periodic function $$f(x)=f(x+l), \qquad l>0$$ Prove that if it is not constant, then $\lim_{x\to 0}f\left(\frac1x\right)$ does not exist. I don't understand why it's true, let alone how to prove it.","I stumbled upon this question in my course, and I am out of ideas. Let $f$ be a periodic function $$f(x)=f(x+l), \qquad l>0$$ Prove that if it is not constant, then $\lim_{x\to 0}f\left(\frac1x\right)$ does not exist. I don't understand why it's true, let alone how to prove it.",,"['real-analysis', 'calculus', 'limits', 'periodic-functions']"
24,Does a limit to this Hypergeometric Function Exist Analytically?,Does a limit to this Hypergeometric Function Exist Analytically?,,"I am interested in evaluating limit $$\lim_{x\rightarrow\pi/2}\left[(\cos x)^n\, _2F_1\left(-\frac{n}{2},-n-m+1;\frac{1}{2}-n;-\frac{16m c}{\cos^2x}\right)\right], $$ where $n$ is a positive even integer, $m$ and $c$ are reals. I checked with Mathematica, and this expression goes to a limiting value at $x\rightarrow \pi/2$, but I want to know if there is any analytical expression interms of $m$ and $c$.","I am interested in evaluating limit $$\lim_{x\rightarrow\pi/2}\left[(\cos x)^n\, _2F_1\left(-\frac{n}{2},-n-m+1;\frac{1}{2}-n;-\frac{16m c}{\cos^2x}\right)\right], $$ where $n$ is a positive even integer, $m$ and $c$ are reals. I checked with Mathematica, and this expression goes to a limiting value at $x\rightarrow \pi/2$, but I want to know if there is any analytical expression interms of $m$ and $c$.",,"['calculus', 'limits', 'special-functions', 'hypergeometric-function']"
25,L'hopital's Rule on an indeterminate difference,L'hopital's Rule on an indeterminate difference,,"I have the following question, in which I am told to use L'Hopital's rule: $\lim_{x\to \infty}(-x+5\cdot ln(x))$ From eyeballing it, I would conclude that the polynomial $x$ will decrease faster than the logarithmic $ln$ would increase, meaning the limit would be $-\infty$, but I can't see how to use L'hopitals rule here as I've been told. I know that the idea is to put this into a quotient, but if I do the following: $$-x+5\cdot ln(x) = \frac{-x^2}{x}+\frac{5x\cdot ln(x)}{x} = \frac{-x^2 + 5x\cdot ln(x)}{x}$$ And then try to take the limit, I get a doubly indeterminate form of $\frac{-\infty+\infty}{\infty}$, and using L'Hopitals rule here just ends back inthe same kind of indeterminate difference.","I have the following question, in which I am told to use L'Hopital's rule: $\lim_{x\to \infty}(-x+5\cdot ln(x))$ From eyeballing it, I would conclude that the polynomial $x$ will decrease faster than the logarithmic $ln$ would increase, meaning the limit would be $-\infty$, but I can't see how to use L'hopitals rule here as I've been told. I know that the idea is to put this into a quotient, but if I do the following: $$-x+5\cdot ln(x) = \frac{-x^2}{x}+\frac{5x\cdot ln(x)}{x} = \frac{-x^2 + 5x\cdot ln(x)}{x}$$ And then try to take the limit, I get a doubly indeterminate form of $\frac{-\infty+\infty}{\infty}$, and using L'Hopitals rule here just ends back inthe same kind of indeterminate difference.",,"['calculus', 'limits']"
26,Find $\lim_{x\to0}\frac1{x^3}\left(\left(\frac{2+\cos x}{3}\right)^x-1\right)$ without L'Hopital's rule,Find  without L'Hopital's rule,\lim_{x\to0}\frac1{x^3}\left(\left(\frac{2+\cos x}{3}\right)^x-1\right),"Find the following limit   $$\lim_{x\to0}\frac1{x^3}\left(\left(\frac{2+\cos x}{3}\right)^x-1\right)$$   without using L'Hopital's rule. I tried to solve this using fundamental limits such as $\lim_{x\to0}\left(1+x\right)^\frac1x=e,\lim_{x\to0}x^x=1$ and equivalent infinitesimals at $x\to0$ such as $x\sim\sin x,a^x\sim1+x\ln a,(1+x)^a\sim1+ax$. This is what I did so far: $$\begin{align}\lim_{x\to0}\frac1{x^3}\left(\left(\frac{2+\cos x}{3}\right)^x-1\right)&=\lim_{x\to0}\frac{\left(\frac{3-(1-\cos x)}3\right)^x-1}{x^3}\\&=\lim_{x\to0}\frac{\left(\frac{3-\frac12x^2}3\right)^x-1}{x^3}\\&=\lim_{x\to0}\frac{\left(1-\frac16x^2\right)^x-1}{x^3}\end{align}$$ I used fact that $x\sim\sin x$ at $x\to0$. After that, I tried to simplify $\left(1-\frac16x^2\right)^x$. Problem is because this is not indeterminate, so I cannot use infinitesimals here. Can this be solved algebraically using fundamental limits, or I need different approach?","Find the following limit   $$\lim_{x\to0}\frac1{x^3}\left(\left(\frac{2+\cos x}{3}\right)^x-1\right)$$   without using L'Hopital's rule. I tried to solve this using fundamental limits such as $\lim_{x\to0}\left(1+x\right)^\frac1x=e,\lim_{x\to0}x^x=1$ and equivalent infinitesimals at $x\to0$ such as $x\sim\sin x,a^x\sim1+x\ln a,(1+x)^a\sim1+ax$. This is what I did so far: $$\begin{align}\lim_{x\to0}\frac1{x^3}\left(\left(\frac{2+\cos x}{3}\right)^x-1\right)&=\lim_{x\to0}\frac{\left(\frac{3-(1-\cos x)}3\right)^x-1}{x^3}\\&=\lim_{x\to0}\frac{\left(\frac{3-\frac12x^2}3\right)^x-1}{x^3}\\&=\lim_{x\to0}\frac{\left(1-\frac16x^2\right)^x-1}{x^3}\end{align}$$ I used fact that $x\sim\sin x$ at $x\to0$. After that, I tried to simplify $\left(1-\frac16x^2\right)^x$. Problem is because this is not indeterminate, so I cannot use infinitesimals here. Can this be solved algebraically using fundamental limits, or I need different approach?",,['limits']
27,Limit of $\frac{\frac{1}{e}(1+x)^{1/x}-1+\frac{x}{2}}{x^2}$ when $x\to0$,Limit of  when,\frac{\frac{1}{e}(1+x)^{1/x}-1+\frac{x}{2}}{x^2} x\to0,"Find the limit of $\dfrac{\frac{1}{e}(1+x)^{1/x}-1+\frac{x}{2}}{x^2}$ when $x\to0$. I tried applying L'Hospital rule, but it is not working here. How should I solve this?","Find the limit of $\dfrac{\frac{1}{e}(1+x)^{1/x}-1+\frac{x}{2}}{x^2}$ when $x\to0$. I tried applying L'Hospital rule, but it is not working here. How should I solve this?",,['limits']
28,Does $\lim\limits_{n \to +\infty} \left(\frac{n}{f(1)} \int_0^1 x^n f(x) dx \right)^n$ exists?,Does  exists?,\lim\limits_{n \to +\infty} \left(\frac{n}{f(1)} \int_0^1 x^n f(x) dx \right)^n,"My question is having following one as a root. On one side, for $f : [0,1] \to \mathbb R$ continuous, one can prove that $$\lim\limits_{n \to +\infty} n \int_0^1 x^n f(x) dx =f(1)$$ On the other side, in the post mentionned , it was proven that $$\lim_{n\to +\infty}\left(2n\int_{0}^{1}\dfrac{x^n}{1+x^2}dx\right)^n$$ exists using integration by parts. Therefore my question is the following. What can we say of $$\lim\limits_{n \to +\infty} \left(\frac{n}{f(1)} \int_0^1 x^n f(x) dx \right)^n$$ if $f$ is only supposed continuous with $f(1) \neq 0$?","My question is having following one as a root. On one side, for $f : [0,1] \to \mathbb R$ continuous, one can prove that $$\lim\limits_{n \to +\infty} n \int_0^1 x^n f(x) dx =f(1)$$ On the other side, in the post mentionned , it was proven that $$\lim_{n\to +\infty}\left(2n\int_{0}^{1}\dfrac{x^n}{1+x^2}dx\right)^n$$ exists using integration by parts. Therefore my question is the following. What can we say of $$\lim\limits_{n \to +\infty} \left(\frac{n}{f(1)} \int_0^1 x^n f(x) dx \right)^n$$ if $f$ is only supposed continuous with $f(1) \neq 0$?",,"['real-analysis', 'integration', 'limits', 'continuity']"
29,Limit of $\frac{\sin^2(x^p)}{x^q+x^r}$ as $x \to 0^+$,Limit of  as,\frac{\sin^2(x^p)}{x^q+x^r} x \to 0^+,"Suppose $p,q,r \in \mathbb{R}$ with $p > 0$. Write (without proof) the limit of the following expression: $$\lim_{x \to 0^+} \frac{\sin^2(x^p)}{x^q+x^r}$$ In particular, find and prove the limit of the expression if $p=3$, $q=6$, $r=7$. I got (through countless graphing on Desmos): $$\lim_{x \to 0^+} \frac{\sin^2(x^p)}{x^q+x^r}=\begin{cases} 0 & \text{if either $q < 2p$ or $r < 2p$} \\ \frac 12 & \text{if $q=r=2p$} \\ 1 & \text{if either $q=2p$, $r > 2p$ or $q > 2p$, $r=2p$} \\ \infty & \text{if $q > 2p$, $r > 2p$} \end{cases}$$ Also, in the case of $p=3$, $q=6$, $r=7$, the expression becomes $$\lim_{x \to 0^+} \frac{\sin^2(x^3)}{x^6+x^7}.$$ I was able to prove that $$\lim_{x \to 0^+} \frac{\sin^2(x^3)}{x^6+x^7} = \lim_{x \to 0^+} \frac{(\sin(x^3))^2}{x^6(1+x)} \le \lim_{x \to 0^+} \frac{(x^3)^2}{x^6(1+x)}=\lim_{x \to 0^+}\frac 1{1+x} = 1.$$ Now, the graph of this function on Desmos tells me that, in fact, $$\lim_{x \to 0^+} \frac{\sin^2(x^3)}{x^6+x^7} = 1.$$ How can I prove the other inequality, to achieve this result? Namely, I have trouble with proving $$\lim_{x \to 0^+} \frac{\sin^2(x^3)}{x^6+x^7} \ge 1.$$","Suppose $p,q,r \in \mathbb{R}$ with $p > 0$. Write (without proof) the limit of the following expression: $$\lim_{x \to 0^+} \frac{\sin^2(x^p)}{x^q+x^r}$$ In particular, find and prove the limit of the expression if $p=3$, $q=6$, $r=7$. I got (through countless graphing on Desmos): $$\lim_{x \to 0^+} \frac{\sin^2(x^p)}{x^q+x^r}=\begin{cases} 0 & \text{if either $q < 2p$ or $r < 2p$} \\ \frac 12 & \text{if $q=r=2p$} \\ 1 & \text{if either $q=2p$, $r > 2p$ or $q > 2p$, $r=2p$} \\ \infty & \text{if $q > 2p$, $r > 2p$} \end{cases}$$ Also, in the case of $p=3$, $q=6$, $r=7$, the expression becomes $$\lim_{x \to 0^+} \frac{\sin^2(x^3)}{x^6+x^7}.$$ I was able to prove that $$\lim_{x \to 0^+} \frac{\sin^2(x^3)}{x^6+x^7} = \lim_{x \to 0^+} \frac{(\sin(x^3))^2}{x^6(1+x)} \le \lim_{x \to 0^+} \frac{(x^3)^2}{x^6(1+x)}=\lim_{x \to 0^+}\frac 1{1+x} = 1.$$ Now, the graph of this function on Desmos tells me that, in fact, $$\lim_{x \to 0^+} \frac{\sin^2(x^3)}{x^6+x^7} = 1.$$ How can I prove the other inequality, to achieve this result? Namely, I have trouble with proving $$\lim_{x \to 0^+} \frac{\sin^2(x^3)}{x^6+x^7} \ge 1.$$",,"['real-analysis', 'limits']"
30,Evaluate $\sin(\sin(\cdots\sin(\sin(a)+a)\cdots+a)+a)$ limit as the number of terms goes to infinity,Evaluate  limit as the number of terms goes to infinity,\sin(\sin(\cdots\sin(\sin(a)+a)\cdots+a)+a),"I need help evaluating this limit: $$ \lim_{n \to \infty }\underbrace{\sin( \sin( \cdots \sin( \sin(}_{\text{$n$ compositions}}\,\underbrace{a)+a)\cdots +a)+a)}_{\text{$n$ compositions}},$$ I know that it converges to a specific number, for example when one takes a to be $1$, the limit is at $0.9345632\ldots$","I need help evaluating this limit: $$ \lim_{n \to \infty }\underbrace{\sin( \sin( \cdots \sin( \sin(}_{\text{$n$ compositions}}\,\underbrace{a)+a)\cdots +a)+a)}_{\text{$n$ compositions}},$$ I know that it converges to a specific number, for example when one takes a to be $1$, the limit is at $0.9345632\ldots$",,"['calculus', 'limits']"
31,Simple Derivation of Functional Equation Question (L'Hospital's Rule),Simple Derivation of Functional Equation Question (L'Hospital's Rule),,"First, the question is: $f$ is a differentiable function and $f : R \rightarrow R$ $xf(x)-yf(y)=(x-y)f(x+y)$ $f'(2x)=?$ My approach for problem is using L'Hospital's rule: $$ \frac{xf(x)-yf(y)}{x-y}=f(x+y) $$ Assuming $y=x$ $$ \lim_{y\to{x}} \frac{xf(x)-yf(y)}{x-y} = f(2x)$$ Taking the limit using l'hospital's rule: $$ (yf'(y)+f(y))|_{y=x} = f(2x) $$ Taking the derivative of both sides: $$ f'(x)+xf''(x)+f'(x)=2f'(2x) $$ But the answer is $f'(2x)=f'(x)$ How is this possible?","First, the question is: $f$ is a differentiable function and $f : R \rightarrow R$ $xf(x)-yf(y)=(x-y)f(x+y)$ $f'(2x)=?$ My approach for problem is using L'Hospital's rule: $$ \frac{xf(x)-yf(y)}{x-y}=f(x+y) $$ Assuming $y=x$ $$ \lim_{y\to{x}} \frac{xf(x)-yf(y)}{x-y} = f(2x)$$ Taking the limit using l'hospital's rule: $$ (yf'(y)+f(y))|_{y=x} = f(2x) $$ Taking the derivative of both sides: $$ f'(x)+xf''(x)+f'(x)=2f'(2x) $$ But the answer is $f'(2x)=f'(x)$ How is this possible?",,"['calculus', 'integration', 'limits', 'functions', 'functional-equations']"
32,True/False: Is it possible that the following limits all hold true?,True/False: Is it possible that the following limits all hold true?,,"For  $g:\ \Bbb{R}\to\Bbb{R}$, it is possible that $$\lim\limits_{x\to -3}\frac{g(x)-g(-3)}{x-(-3)}=5$$ with $\lim\limits_{n\to \infty}g\left(-3+\frac{1}{n}\right)=7$, and   $\lim\limits_{n\to \infty}g\left(-3+\frac{\pi}{n^2}\right)=5$. Okay, so I know the last two limits are essentially the same thing, so the answer must be false. But how do I prove that using the definition of a limit?","For  $g:\ \Bbb{R}\to\Bbb{R}$, it is possible that $$\lim\limits_{x\to -3}\frac{g(x)-g(-3)}{x-(-3)}=5$$ with $\lim\limits_{n\to \infty}g\left(-3+\frac{1}{n}\right)=7$, and   $\lim\limits_{n\to \infty}g\left(-3+\frac{\pi}{n^2}\right)=5$. Okay, so I know the last two limits are essentially the same thing, so the answer must be false. But how do I prove that using the definition of a limit?",,"['calculus', 'real-analysis', 'limits']"
33,Infinity - infinity calculus,Infinity - infinity calculus,,$$\lim_{x\to\infty} (x-1)e^{-1/x}-x$$ I know that this limit equals $-2$ but I don't know how to prove it. I can only get to $\infty-\infty=?$,$$\lim_{x\to\infty} (x-1)e^{-1/x}-x$$ I know that this limit equals $-2$ but I don't know how to prove it. I can only get to $\infty-\infty=?$,,"['calculus', 'limits', 'infinity']"
34,Limit of a function of 3 variables,Limit of a function of 3 variables,,"I need help to evaluate the following limit please, I used to use polar coordinates in most 2 variable functions but here I am stuck. $\lim_{(x,y,z)\to(0,0,0)}$$\frac{xy+yz^2+xz^2}{x^2+y^2+z^2}$","I need help to evaluate the following limit please, I used to use polar coordinates in most 2 variable functions but here I am stuck. $\lim_{(x,y,z)\to(0,0,0)}$$\frac{xy+yz^2+xz^2}{x^2+y^2+z^2}$",,['limits']
35,Two definitions of upper limit,Two definitions of upper limit,,"I have some confusion on the definition of upper limit of a sequence. Usually we see this definition: Let $(x_n)$ be a bounded sequence and for each natural number $n$, let $$\overline{x_n}=\sup \left \{ x_k:k\ge n \right \}$$   Then $\overline{x}=\lim \overline{x_n}$ is the upper limit of the sequence $(x_n)$. However, in Rudin's Priciple of Mathemetical Analysis, he wrote: Let $(s_n)$ is the sequence of real numbers. Ler $E$ be the set of numbers $x$ (in the extended real system) such that $s_{n_k}\rightarrow x$ for some subsequence $(s_{n_k})$. Define $$s^*=\sup E$$ then $s^*$ is called the upper limit of the sequence $(s_n)$. I'm confused with the two definitions. I cannot find any connection between them. When doing exercises, I usually use the first definition, but how those two definitions are the same? Thanks a lot.","I have some confusion on the definition of upper limit of a sequence. Usually we see this definition: Let $(x_n)$ be a bounded sequence and for each natural number $n$, let $$\overline{x_n}=\sup \left \{ x_k:k\ge n \right \}$$   Then $\overline{x}=\lim \overline{x_n}$ is the upper limit of the sequence $(x_n)$. However, in Rudin's Priciple of Mathemetical Analysis, he wrote: Let $(s_n)$ is the sequence of real numbers. Ler $E$ be the set of numbers $x$ (in the extended real system) such that $s_{n_k}\rightarrow x$ for some subsequence $(s_{n_k})$. Define $$s^*=\sup E$$ then $s^*$ is called the upper limit of the sequence $(s_n)$. I'm confused with the two definitions. I cannot find any connection between them. When doing exercises, I usually use the first definition, but how those two definitions are the same? Thanks a lot.",,"['real-analysis', 'limits']"
36,"How to solve the limit $\lim_{n\to\infty}\sum_{k=1}^{n} \frac{k}{k\,n+2n^2}$",How to solve the limit,"\lim_{n\to\infty}\sum_{k=1}^{n} \frac{k}{k\,n+2n^2}","I think it is related to squeeze theorem, but could not come up with a solution. The answer here is $1-\ln(9/4)$. Can someone help me with this question? $$ \lim_{n\to\infty}\sum_{k=1}^{n} \frac{k}{k\,n+2n^2} = \lim_{n \to \infty} \left(\frac{1}{n+2n^2} +\frac{2}{2n+2n^2}+\cdots + \frac{n}{n^2+2n^2}\right) $$ Any help will be appreciated! Thanks!","I think it is related to squeeze theorem, but could not come up with a solution. The answer here is $1-\ln(9/4)$. Can someone help me with this question? $$ \lim_{n\to\infty}\sum_{k=1}^{n} \frac{k}{k\,n+2n^2} = \lim_{n \to \infty} \left(\frac{1}{n+2n^2} +\frac{2}{2n+2n^2}+\cdots + \frac{n}{n^2+2n^2}\right) $$ Any help will be appreciated! Thanks!",,"['calculus', 'limits', 'summation']"
37,Limit involving $\sqrt[n]{n!}$,Limit involving,\sqrt[n]{n!},"a)Find $\displaystyle\lim_{n\to\infty}\frac{\sqrt[n]{n!}}{n}$. b)Let $a_n=\frac{\sqrt[n+1]{(n+1)!}}{\sqrt[n]{n!}}$. Find $\displaystyle\lim_{n\to\infty}a_n$ and $\displaystyle\lim_{n\to\infty}\frac{a_n-1}{\ln a_n}$. So, I've managed to find the first two limits, which are $\dfrac1e$ and $1$, but don't know what to do with the last one. Mathematica says it's 1, but I have no idea how to get that. Could I get a hint on how to do this?","a)Find $\displaystyle\lim_{n\to\infty}\frac{\sqrt[n]{n!}}{n}$. b)Let $a_n=\frac{\sqrt[n+1]{(n+1)!}}{\sqrt[n]{n!}}$. Find $\displaystyle\lim_{n\to\infty}a_n$ and $\displaystyle\lim_{n\to\infty}\frac{a_n-1}{\ln a_n}$. So, I've managed to find the first two limits, which are $\dfrac1e$ and $1$, but don't know what to do with the last one. Mathematica says it's 1, but I have no idea how to get that. Could I get a hint on how to do this?",,"['analysis', 'limits']"
38,Wolfram interpretation of arccot(x),Wolfram interpretation of arccot(x),,"I was calculating a $$\lim_{{x \to -\infty}}\left(\frac{4-x}{1-x}\right)^{2x\mathrm{arccot}(x)}$$ and found a problem og how wolframalpha presents $\mathrm{arccot}(x)$. I was taughted that plot of arccot(x) looks like this: But wolframalpha shows it like this That why we have completely different answers. Because according to my representation $$\lim_{{x \to -\infty}}\mathrm{arccot}(x)$$ equals to Pi, when wolfram says it equals to 0. I calculated that the answer is $$e^{6\pi}$$, answer of wolfram is $1$. Please help me understand where is a mistake.","I was calculating a $$\lim_{{x \to -\infty}}\left(\frac{4-x}{1-x}\right)^{2x\mathrm{arccot}(x)}$$ and found a problem og how wolframalpha presents $\mathrm{arccot}(x)$. I was taughted that plot of arccot(x) looks like this: But wolframalpha shows it like this That why we have completely different answers. Because according to my representation $$\lim_{{x \to -\infty}}\mathrm{arccot}(x)$$ equals to Pi, when wolfram says it equals to 0. I calculated that the answer is $$e^{6\pi}$$, answer of wolfram is $1$. Please help me understand where is a mistake.",,"['limits', 'wolfram-alpha']"
39,Nasty Limit of sum at infinity,Nasty Limit of sum at infinity,,$$\lim_{n \to \infty} \left (\sum_{i=1}^n \frac{a\left[\left(\frac{b}{a}\right)^{\frac{i}{n}}-\left(\frac{b}{a}\right)^{\frac{i-1}{n}}\right]}{a\left(\frac{b}{a}\right)^{\frac{i-1}{n}}}\right) $$ How would I even begin to approach this?? Edit: So the hints helped... I ended up with $\space\ln\left(\frac{b}{a}\right)$,$$\lim_{n \to \infty} \left (\sum_{i=1}^n \frac{a\left[\left(\frac{b}{a}\right)^{\frac{i}{n}}-\left(\frac{b}{a}\right)^{\frac{i-1}{n}}\right]}{a\left(\frac{b}{a}\right)^{\frac{i-1}{n}}}\right) $$ How would I even begin to approach this?? Edit: So the hints helped... I ended up with $\space\ln\left(\frac{b}{a}\right)$,,"['calculus', 'limits', 'summation']"
40,Calculation of $\lim\limits_{x \to 0} \frac{\frac{\mathrm d}{\mathrm d x} (e^{\sec x})}{\frac{\mathrm d}{\mathrm d x} (e^{\sec 2x})}$,Calculation of,\lim\limits_{x \to 0} \frac{\frac{\mathrm d}{\mathrm d x} (e^{\sec x})}{\frac{\mathrm d}{\mathrm d x} (e^{\sec 2x})},"I'm a bit rusty with limits and derivatives at the moment. I was doing L'hosp on another problem when I got stuck here. $$\lim_{x \to 0} \dfrac{\dfrac{\mathrm d}{\mathrm d x} (e^{\large \sec x})}{\dfrac{\mathrm d}{\mathrm d x} (e^{\large\sec 2x})}$$ Further L'hosp is a mess. Care to continue, my jolly fellows?","I'm a bit rusty with limits and derivatives at the moment. I was doing L'hosp on another problem when I got stuck here. $$\lim_{x \to 0} \dfrac{\dfrac{\mathrm d}{\mathrm d x} (e^{\large \sec x})}{\dfrac{\mathrm d}{\mathrm d x} (e^{\large\sec 2x})}$$ Further L'hosp is a mess. Care to continue, my jolly fellows?",,['limits']
41,"Prove with Cauchy's limit definition ($\epsilon, \delta$) that $\lim_{x \rightarrow 0} \frac{x^2-8}{x-8}=1$",Prove with Cauchy's limit definition () that,"\epsilon, \delta \lim_{x \rightarrow 0} \frac{x^2-8}{x-8}=1","Prove with Cauchy's limit definition ($\epsilon,  \delta$) that $$\lim_{x \rightarrow 0}  \frac{x^2-8}{x-8}=1$$ Got really troubled with the proper technique of solving this. Any assistance will be much appreciated!","Prove with Cauchy's limit definition ($\epsilon,  \delta$) that $$\lim_{x \rightarrow 0}  \frac{x^2-8}{x-8}=1$$ Got really troubled with the proper technique of solving this. Any assistance will be much appreciated!",,"['calculus', 'limits', 'epsilon-delta']"
42,Is the following limit correct? $[\lim_{n\to\infty}\binom{n}{50}(\frac2n)^{50}(1-\frac2n)^{n-50}]$,Is the following limit correct?,[\lim_{n\to\infty}\binom{n}{50}(\frac2n)^{50}(1-\frac2n)^{n-50}],"$$\lim_{n\to\infty}\binom{n}{50}\left(\frac2n\right)^{50}\left(1-\frac2n\right)^{n-50}$$ Taking $nh=1$ and $K=\binom{n}{50}\left(\frac2n\right)^{50}\left(1-\frac2n\right)^{n-50}$, we have: $$\ln K =\ln\left[\binom{n}{50}(2h)^{50}\right]+(1-50h)\frac{\ln(1-2h)}h \\=\ln\left[\frac{2^{50}}{50!}\prod_{k=1}^{49}(1-kh)\right]+(1-50h)\frac{\ln(1-2h)}h$$ Now I think $$\lim_{n\to\infty}K=\frac{2^{50}}{50!}\frac1{e^2}$$ Is this correct way to solve this? Is my answer correct? Edit: I noted that: $$\left(\frac2n+1-\frac2n\right)^n=\sum_{k=0}^{n}\underbrace{\binom{n}{k}\left(\frac2n\right)^{k}\left(1-\frac2n\right)^{n-k}}_{t_k}$$ As $n\to\infty$, LHS is exactly 1 and we need to find the minute contribution of $t_{50}$.","$$\lim_{n\to\infty}\binom{n}{50}\left(\frac2n\right)^{50}\left(1-\frac2n\right)^{n-50}$$ Taking $nh=1$ and $K=\binom{n}{50}\left(\frac2n\right)^{50}\left(1-\frac2n\right)^{n-50}$, we have: $$\ln K =\ln\left[\binom{n}{50}(2h)^{50}\right]+(1-50h)\frac{\ln(1-2h)}h \\=\ln\left[\frac{2^{50}}{50!}\prod_{k=1}^{49}(1-kh)\right]+(1-50h)\frac{\ln(1-2h)}h$$ Now I think $$\lim_{n\to\infty}K=\frac{2^{50}}{50!}\frac1{e^2}$$ Is this correct way to solve this? Is my answer correct? Edit: I noted that: $$\left(\frac2n+1-\frac2n\right)^n=\sum_{k=0}^{n}\underbrace{\binom{n}{k}\left(\frac2n\right)^{k}\left(1-\frac2n\right)^{n-k}}_{t_k}$$ As $n\to\infty$, LHS is exactly 1 and we need to find the minute contribution of $t_{50}$.",,"['limits', 'binomial-coefficients', 'poisson-distribution']"
43,Prove that the limit exists and then find its limit,Prove that the limit exists and then find its limit,,"We are given a function $h(x)$ which strictly positive. The function $h$ is defined on $\mathbb{R}$ and that satisfies the following:\ $$\lim_{x\to 0}(h(x)+\frac{1}{h(x)})=2$$ The question is to prove that the limit of $h$ exists at$ 0$ and then find its limit as $x\to 0$ If we assume that the limit exists (say it's equal to $\lambda$, then we have $\lambda+\frac{1}{\lambda}=2$ and by solving this equation, we get $\lambda=1$. However, I am completely clueless on how to prove the existence of the limit of $h(x)$ when $x\to 0$.","We are given a function $h(x)$ which strictly positive. The function $h$ is defined on $\mathbb{R}$ and that satisfies the following:\ $$\lim_{x\to 0}(h(x)+\frac{1}{h(x)})=2$$ The question is to prove that the limit of $h$ exists at$ 0$ and then find its limit as $x\to 0$ If we assume that the limit exists (say it's equal to $\lambda$, then we have $\lambda+\frac{1}{\lambda}=2$ and by solving this equation, we get $\lambda=1$. However, I am completely clueless on how to prove the existence of the limit of $h(x)$ when $x\to 0$.",,"['calculus', 'real-analysis', 'limits']"
44,Help with $\lim\limits_{x \to 0^+}{x^a}{\ln x}$,Help with,\lim\limits_{x \to 0^+}{x^a}{\ln x},Evaluate $$\lim\limits_{x \to 0^+}{x^a}{\ln x}$$ I used L'Hopital rule till $\Large \lim\limits_{x \to 0^+}{\frac{x^{a-2}}{-a}}$ but I can't go any further. Thanks for the help in advance!,Evaluate $$\lim\limits_{x \to 0^+}{x^a}{\ln x}$$ I used L'Hopital rule till $\Large \lim\limits_{x \to 0^+}{\frac{x^{a-2}}{-a}}$ but I can't go any further. Thanks for the help in advance!,,"['calculus', 'limits']"
45,Evaluation of a simple limit with Taylor Series,Evaluation of a simple limit with Taylor Series,,I would like to evaluate $$\lim_{x\to0} \frac{e^{\sin x} - \sin^2x - 1}{x}$$ using Taylor Series expansion in a completely rigorous way. What would a rigorous version of the following argument look like? From $$e^{\sin x} -1\sim_0 e^x-1 \sim_0 x \text{ and }\sin^2x \sim_0 x^2$$ we can find the limit $$\lim_{x\to0} \frac{e^{\sin x} - \sin^2x - 1}{x} = \lim_{x\to0} \frac{x-\frac{x^2}{2}}{x} = 1.$$ In particular I'm not exactly sure how to deal with Landau symbols $o$ and $O$ in nested functions.,I would like to evaluate $$\lim_{x\to0} \frac{e^{\sin x} - \sin^2x - 1}{x}$$ using Taylor Series expansion in a completely rigorous way. What would a rigorous version of the following argument look like? From $$e^{\sin x} -1\sim_0 e^x-1 \sim_0 x \text{ and }\sin^2x \sim_0 x^2$$ we can find the limit $$\lim_{x\to0} \frac{e^{\sin x} - \sin^2x - 1}{x} = \lim_{x\to0} \frac{x-\frac{x^2}{2}}{x} = 1.$$ In particular I'm not exactly sure how to deal with Landau symbols $o$ and $O$ in nested functions.,,"['calculus', 'limits']"
46,Evaluating $\lim\limits_{x \to \pi/ 6} \frac{(2\sin x + \cos(6x))^2}{(6x - \pi)\sin(6x)}$,Evaluating,\lim\limits_{x \to \pi/ 6} \frac{(2\sin x + \cos(6x))^2}{(6x - \pi)\sin(6x)},"$$\lim_{x \to \pi/ 6} \frac{(2\sin x + \cos(6x))^2}{(6x - \pi)\sin(6x)}$$ I would expand with Maclaurin series but $x \to \frac \pi 6$ so I cannot do that. So I evaluated it with l'Hopital rule (result is $-\frac 1{12}$), but is there a better way? I had to differentiate two times and it gets really big and complicated.","$$\lim_{x \to \pi/ 6} \frac{(2\sin x + \cos(6x))^2}{(6x - \pi)\sin(6x)}$$ I would expand with Maclaurin series but $x \to \frac \pi 6$ so I cannot do that. So I evaluated it with l'Hopital rule (result is $-\frac 1{12}$), but is there a better way? I had to differentiate two times and it gets really big and complicated.",,['limits']
47,How to calculate factorial function as $x\to\infty$?,How to calculate factorial function as ?,x\to\infty,"I need to calculate $$\lim_{x \to \infty} \frac{((2x)!)^4}{(4x)! ((x+5)!)^2 ((x-5)!)^2}.$$ Even I used Striling Approximation and Wolfram Alpha, they do not help. How can I calculate this? My expectation of the output is about $0.07$. Thank you in advance.","I need to calculate $$\lim_{x \to \infty} \frac{((2x)!)^4}{(4x)! ((x+5)!)^2 ((x-5)!)^2}.$$ Even I used Striling Approximation and Wolfram Alpha, they do not help. How can I calculate this? My expectation of the output is about $0.07$. Thank you in advance.",,"['calculus', 'analysis', 'limits']"
48,Help with a simple derivative,Help with a simple derivative,,I am trying to solve $\dfrac {6} {\sqrt {x^3+6}}$  and so far I made it to $6(x^3+6)^{-\frac 1 2}$  then I continued and now I have $(x^3+6)^{- \frac 3 2} * 3x^2$ and I cannot figure out what how to find the constant that should be before the parenthesis.,I am trying to solve $\dfrac {6} {\sqrt {x^3+6}}$  and so far I made it to $6(x^3+6)^{-\frac 1 2}$  then I continued and now I have $(x^3+6)^{- \frac 3 2} * 3x^2$ and I cannot figure out what how to find the constant that should be before the parenthesis.,,"['calculus', 'limits', 'derivatives']"
49,"Finding $\lim_{x \to 0} \frac {a\sin bx -b\sin ax}{x^2 \sin ax}$ witouth L'Hopital, what is my mistake?","Finding  witouth L'Hopital, what is my mistake?",\lim_{x \to 0} \frac {a\sin bx -b\sin ax}{x^2 \sin ax},"I was working on this question. $\lim_{x \to 0} \dfrac {a\sin bx -b\sin ax}{x^2 \sin ax}$ $\lim_{x \to 0} \dfrac {1}{x^2} \cdot \lim_{x \to 0} \dfrac { \frac {1}{abx}}{\frac {1}{abx}} \cdot \dfrac {a\sin bx -b\sin ax}{\sin ax}$ $\lim_{x \to 0} \dfrac {1}{x^2} \cdot \dfrac {\lim_{x \to 0} \frac {\sin bx}{bx}- \lim_{x \to 0}\frac{\sin ax}{ax}}{\frac 1b \lim_{x \to 0} \frac {\sin ax}{ax}}$ $b \lim_{x \to 0} \dfrac {1}{x^2} \cdot \dfrac {b-a}{a}$ It seems like this limit does not exist, but if you apply L'Hopital's rule you seem to get an answer. What is wrong with what I did?","I was working on this question. $\lim_{x \to 0} \dfrac {a\sin bx -b\sin ax}{x^2 \sin ax}$ $\lim_{x \to 0} \dfrac {1}{x^2} \cdot \lim_{x \to 0} \dfrac { \frac {1}{abx}}{\frac {1}{abx}} \cdot \dfrac {a\sin bx -b\sin ax}{\sin ax}$ $\lim_{x \to 0} \dfrac {1}{x^2} \cdot \dfrac {\lim_{x \to 0} \frac {\sin bx}{bx}- \lim_{x \to 0}\frac{\sin ax}{ax}}{\frac 1b \lim_{x \to 0} \frac {\sin ax}{ax}}$ $b \lim_{x \to 0} \dfrac {1}{x^2} \cdot \dfrac {b-a}{a}$ It seems like this limit does not exist, but if you apply L'Hopital's rule you seem to get an answer. What is wrong with what I did?",,"['limits', 'trigonometry']"
50,Prove $\lim_{x \to \infty} f(x) = L \iff \lim_{x \to 0} g(x) = L$,Prove,\lim_{x \to \infty} f(x) = L \iff \lim_{x \to 0} g(x) = L,"Let $f(x) = g(1/x)$ for $x>0$. Prove: $\lim_{x \to \infty} f(x) = L \iff \lim_{x \to 0} g(x) = L$ for some $L \in \mathbb{R}$. I assume I am supposed to use l'Hopital's rule in some way (considering that is what section we are in). I've tried looking at the definition of a limit and the sequential criterion for limits, but I have no idea where to go. Just a push in the right direction would be awesome. Thanks in advance.","Let $f(x) = g(1/x)$ for $x>0$. Prove: $\lim_{x \to \infty} f(x) = L \iff \lim_{x \to 0} g(x) = L$ for some $L \in \mathbb{R}$. I assume I am supposed to use l'Hopital's rule in some way (considering that is what section we are in). I've tried looking at the definition of a limit and the sequential criterion for limits, but I have no idea where to go. Just a push in the right direction would be awesome. Thanks in advance.",,"['real-analysis', 'limits']"
51,The limit as x approaches infinity,The limit as x approaches infinity,,For some reason I can't solve this. $$ \lim_{x \to \pm \infty}\left(\cos\left(e^{x^{1/3}+\sin x}\right)\right) $$,For some reason I can't solve this. $$ \lim_{x \to \pm \infty}\left(\cos\left(e^{x^{1/3}+\sin x}\right)\right) $$,,"['calculus', 'limits']"
52,Finding the limit of $x \sin\frac{\pi}{x}$,Finding the limit of,x \sin\frac{\pi}{x},How can I find the limit of the following $x\rightarrow\infty$ $x \sin\frac{\pi}{x}$ I did $\dfrac{\sin\frac{\pi}{x}}{\frac{1}{x}}$ I took the derivative using l hospital and got. $\dfrac{-1x^{-2} \cos \dfrac{\pi}{x}}{-1x^{-2}}$ Simplying I get $\cos \frac{\pi}{x}$ but I am stuck. another problem I have is $\dfrac{\ln(x)}{\cot x}$ as $x\rightarrow0$ I did $\dfrac{\dfrac{1}{x}}{-\csc^2(x)}$ But I am unsure how to go on.,How can I find the limit of the following $x\rightarrow\infty$ $x \sin\frac{\pi}{x}$ I did $\dfrac{\sin\frac{\pi}{x}}{\frac{1}{x}}$ I took the derivative using l hospital and got. $\dfrac{-1x^{-2} \cos \dfrac{\pi}{x}}{-1x^{-2}}$ Simplying I get $\cos \frac{\pi}{x}$ but I am stuck. another problem I have is $\dfrac{\ln(x)}{\cot x}$ as $x\rightarrow0$ I did $\dfrac{\dfrac{1}{x}}{-\csc^2(x)}$ But I am unsure how to go on.,,"['calculus', 'limits']"
53,$\lim_{n\to\infty}{\left({\sum_{k=1}^nk^k}\right)/{n^n}}=1$?,?,\lim_{n\to\infty}{\left({\sum_{k=1}^nk^k}\right)/{n^n}}=1,"I'm interested in the following sum $S_n$. $$S_n:=\sum_{k=1}^nk^k=1^1+2^2+3^3+\cdots+n^n.$$ Letting $T_n:={S_n}/{n^n}$, wolfram tells us the followings. $$T_5=1.09216, T_{10}\approx1.04051, T_{30}\approx1.01263, T_{60}\approx1.00622.$$ Then, here is my expectation. My expectation :  $$\lim_{n\to\infty}{T_n}=1.$$ It seems obvious, so I've tried to prove this, but I'm facing difficulty. Then, here is my question. Question : Could you show me how to find $\lim_{n\to\infty}{T_n}$ if it exists?","I'm interested in the following sum $S_n$. $$S_n:=\sum_{k=1}^nk^k=1^1+2^2+3^3+\cdots+n^n.$$ Letting $T_n:={S_n}/{n^n}$, wolfram tells us the followings. $$T_5=1.09216, T_{10}\approx1.04051, T_{30}\approx1.01263, T_{60}\approx1.00622.$$ Then, here is my expectation. My expectation :  $$\lim_{n\to\infty}{T_n}=1.$$ It seems obvious, so I've tried to prove this, but I'm facing difficulty. Then, here is my question. Question : Could you show me how to find $\lim_{n\to\infty}{T_n}$ if it exists?",,"['number-theory', 'limits', 'summation']"
54,Please help me to prove that $|f(x)| \le M \Vert x\Vert$ around $0$ when $|f(x)| \le \Vert x \Vert^\alpha$ around $0$,Please help me to prove that  around  when  around,|f(x)| \le M \Vert x\Vert 0 |f(x)| \le \Vert x \Vert^\alpha 0,"Question: Suppose that $0<r<1$ and that $f\colon B_1(0) \to \Bbb R$ is continuously differentiable. If there is an $\alpha>0$ such that $|f(x)| \le \Vert x \Vert^{\alpha}$ for all $x\in B_r(0)$, then prove that there is an $M>0$ such that $|f(x)| \le M\Vert x \Vert$ for $x\in B_r(0)$ Solution trial: let $x_k\in B_r(0)$ be a sequence with $x_k\to 0$ as $k\to \infty$ Since $f$ is continuously differentiable, $f$ is continouos as well. Since $f$ is continouos and $x_k\to 0$, it must be that $f(x_k)\to f(0)$. That is, $$\lim_{k\to\infty}|f(x_k)|\le \lim_{k\to\infty}\Vert x_k \Vert^{\alpha}=0.$$ This is just an idea that may not work. Please help me find a valid proof. Thank you.","Question: Suppose that $0<r<1$ and that $f\colon B_1(0) \to \Bbb R$ is continuously differentiable. If there is an $\alpha>0$ such that $|f(x)| \le \Vert x \Vert^{\alpha}$ for all $x\in B_r(0)$, then prove that there is an $M>0$ such that $|f(x)| \le M\Vert x \Vert$ for $x\in B_r(0)$ Solution trial: let $x_k\in B_r(0)$ be a sequence with $x_k\to 0$ as $k\to \infty$ Since $f$ is continuously differentiable, $f$ is continouos as well. Since $f$ is continouos and $x_k\to 0$, it must be that $f(x_k)\to f(0)$. That is, $$\lim_{k\to\infty}|f(x_k)|\le \lim_{k\to\infty}\Vert x_k \Vert^{\alpha}=0.$$ This is just an idea that may not work. Please help me find a valid proof. Thank you.",,"['calculus', 'real-analysis', 'limits', 'derivatives']"
55,Harmonic number divided by n [duplicate],Harmonic number divided by n [duplicate],,"This question already has answers here : How to show that $\lim \frac{1}{n} \sum_{i=1}^n \frac{1}{i}=0 $? [duplicate] (8 answers) Closed 9 years ago . How do I prove that $\dfrac{H_n}{n}$ (where $H_n$ is a harmonic number) converges to $0$, as $n \to \infty$?","This question already has answers here : How to show that $\lim \frac{1}{n} \sum_{i=1}^n \frac{1}{i}=0 $? [duplicate] (8 answers) Closed 9 years ago . How do I prove that $\dfrac{H_n}{n}$ (where $H_n$ is a harmonic number) converges to $0$, as $n \to \infty$?",,"['real-analysis', 'limits', 'convergence-divergence', 'harmonic-numbers']"
56,Limit of convex increasing function,Limit of convex increasing function,,"If $f$ is strictly increasing and strictly convex (or $f'>0$ & $f''>0$), then $$\lim_{x\rightarrow∞}{f(x)}=∞$$ Is this statement true? If this statement is true, how can I prove?","If $f$ is strictly increasing and strictly convex (or $f'>0$ & $f''>0$), then $$\lim_{x\rightarrow∞}{f(x)}=∞$$ Is this statement true? If this statement is true, how can I prove?",,['limits']
57,"Proof using $\epsilon, \delta$-definition",Proof using -definition,"\epsilon, \delta",Prove that $\displaystyle \lim_{x\to-2}  (5x^2-3x+4)=30$. I have it factored to $|5x-13||x+2| < \epsilon$. and now I can't figure where to go from there,Prove that $\displaystyle \lim_{x\to-2}  (5x^2-3x+4)=30$. I have it factored to $|5x-13||x+2| < \epsilon$. and now I can't figure where to go from there,,['limits']
58,Proving $\lim \limits_{x \to{1}^{-}} \int_{-x}^{x}\frac { f(t)}{\sqrt { 1-{ t }^{ 2 } } }dt$ exists,Proving  exists,\lim \limits_{x \to{1}^{-}} \int_{-x}^{x}\frac { f(t)}{\sqrt { 1-{ t }^{ 2 } } }dt,"Let $\textit{f} :[-1,1] \rightarrow \mathbb{R}$ continuous on $[-1,1]$ I need to prove that  $$\lim_{x \rightarrow{1}^{-}} \int_{-x}^{x}\frac { f(t)}{\sqrt { 1-{ t }^{ 2 } }}dt$$ exists But I have no clue of the value of the limit (I know it has to be finite) And since there is no information on the sign of $f$, I cannot use any ""it is an increasing and bounded function"" argument Thanks for your help","Let $\textit{f} :[-1,1] \rightarrow \mathbb{R}$ continuous on $[-1,1]$ I need to prove that  $$\lim_{x \rightarrow{1}^{-}} \int_{-x}^{x}\frac { f(t)}{\sqrt { 1-{ t }^{ 2 } }}dt$$ exists But I have no clue of the value of the limit (I know it has to be finite) And since there is no information on the sign of $f$, I cannot use any ""it is an increasing and bounded function"" argument Thanks for your help",,"['calculus', 'real-analysis', 'limits']"
59,Can we prove $\displaystyle \limsup_{n \to \infty} \sin(n) = 1$?,Can we prove ?,\displaystyle \limsup_{n \to \infty} \sin(n) = 1,"Can we prove that $\displaystyle \limsup_{n\to \infty} \sin(n) = 1$? I can prove that the above statement holds assuming that $\displaystyle \frac{\pi}{2}$ is normal (this fact is used somewhat tangentially in the proof, making me wonder whether it can be proved without it):  Fix $\epsilon > 0$ and find $5^n > \frac{1}{\epsilon}$.  We may find a string $00\dots 01$ (in base $5$) with $n$ zeroes occuring at the $m^{th}$ digit and it follows that: $$\frac{\lfloor \frac{\pi}{2} \cdot 5^m \rfloor}{5^m} < \frac{\pi}{2} < \frac{\lfloor \frac{\pi}{2} \cdot 5^m \rfloor + \epsilon}{5^m}$$ So $d(\frac{\pi}{2} \cdot 5^m , \lfloor \frac{\pi}{2} \cdot 5^m \rfloor ) < \epsilon$.  Since $5^m \equiv 1 \mod 4$, we have $\sin(\frac{\pi}{2} \cdot 5^m) = 1$, and since $|\sin|$ dominates the triangle wave, $d(1,\sin(\lfloor \frac{\pi}{2} \cdot 5^m \rfloor ) ) < \displaystyle \frac{\epsilon}{\pi}$, from which the result follows. But $\displaystyle \frac{\pi}{2}$ is not proven to be normal.  Can we prove this fact without using normality?","Can we prove that $\displaystyle \limsup_{n\to \infty} \sin(n) = 1$? I can prove that the above statement holds assuming that $\displaystyle \frac{\pi}{2}$ is normal (this fact is used somewhat tangentially in the proof, making me wonder whether it can be proved without it):  Fix $\epsilon > 0$ and find $5^n > \frac{1}{\epsilon}$.  We may find a string $00\dots 01$ (in base $5$) with $n$ zeroes occuring at the $m^{th}$ digit and it follows that: $$\frac{\lfloor \frac{\pi}{2} \cdot 5^m \rfloor}{5^m} < \frac{\pi}{2} < \frac{\lfloor \frac{\pi}{2} \cdot 5^m \rfloor + \epsilon}{5^m}$$ So $d(\frac{\pi}{2} \cdot 5^m , \lfloor \frac{\pi}{2} \cdot 5^m \rfloor ) < \epsilon$.  Since $5^m \equiv 1 \mod 4$, we have $\sin(\frac{\pi}{2} \cdot 5^m) = 1$, and since $|\sin|$ dominates the triangle wave, $d(1,\sin(\lfloor \frac{\pi}{2} \cdot 5^m \rfloor ) ) < \displaystyle \frac{\epsilon}{\pi}$, from which the result follows. But $\displaystyle \frac{\pi}{2}$ is not proven to be normal.  Can we prove this fact without using normality?",,"['real-analysis', 'limits', 'limsup-and-liminf']"
60,"find the equation of the line tangent to the curve $y=\sqrt{x}$ at the point $(1,1)$",find the equation of the line tangent to the curve  at the point,"y=\sqrt{x} (1,1)","Hello guys i have a homework question that once again has me stumped. I have a less then friendly teacher who thinks all her students are going to be the next Isaac Newton and she barely explains anything.  So here is my question. from reading my book it appears that i have two options. formula 1  says $$ \lim_{x\to a }\frac{f(x)-f(a)}{x-a}$$ and the second says  $$ \lim_{h\to 0 }\frac{f(a+h)-f(a)}{h}$$ so i tried to use formula 2 to solve this problem FIND The equation of the tangent line to the curve at the given point. $$ \sqrt{x},$$ $$ (1,1)$$ step - 1 $$ \frac{f(1+h)-f(1)}{h}$$ step -2  $$\frac{\sqrt{1+h} - 1}{h}$$ step -3 used the conjugate pair to come up with  $$\frac{\sqrt{1+h} - 1}{h} * \frac{\sqrt{1+h} + 1}{\sqrt{1+h} + 1} $$ step -4 the result of the above i would end up with  $$ \frac{1+h-1}{h\sqrt{1+h}}$$ step 5 $$\frac{1}{\sqrt{1+0}} = 1$$ however the answer is $ \frac{1}{2}$ i cant see what I'm doing wrong and when should i use the first formula and when do i use the second formula? any help in answering this question would be most appreciated. Thanks Miguel","Hello guys i have a homework question that once again has me stumped. I have a less then friendly teacher who thinks all her students are going to be the next Isaac Newton and she barely explains anything.  So here is my question. from reading my book it appears that i have two options. formula 1  says $$ \lim_{x\to a }\frac{f(x)-f(a)}{x-a}$$ and the second says  $$ \lim_{h\to 0 }\frac{f(a+h)-f(a)}{h}$$ so i tried to use formula 2 to solve this problem FIND The equation of the tangent line to the curve at the given point. $$ \sqrt{x},$$ $$ (1,1)$$ step - 1 $$ \frac{f(1+h)-f(1)}{h}$$ step -2  $$\frac{\sqrt{1+h} - 1}{h}$$ step -3 used the conjugate pair to come up with  $$\frac{\sqrt{1+h} - 1}{h} * \frac{\sqrt{1+h} + 1}{\sqrt{1+h} + 1} $$ step -4 the result of the above i would end up with  $$ \frac{1+h-1}{h\sqrt{1+h}}$$ step 5 $$\frac{1}{\sqrt{1+0}} = 1$$ however the answer is $ \frac{1}{2}$ i cant see what I'm doing wrong and when should i use the first formula and when do i use the second formula? any help in answering this question would be most appreciated. Thanks Miguel",,"['calculus', 'limits']"
61,$\lim_{x \to 0} \frac {(x^2-\sin x^2) }{ (e^ {x^2}+ e^ {-x^2} -2)} $ solution?,solution?,\lim_{x \to 0} \frac {(x^2-\sin x^2) }{ (e^ {x^2}+ e^ {-x^2} -2)} ,"I recently took an math exam where I had this limit to solve $$ \lim_{x \to 0} \frac {(x^2-\sin x^2) }{ (e^ {x^2}+ e^ {-x^2} -2)}  $$ and I tought I did it right, since I proceeded like this: 1st I applied Taylor expansion of the terms to the second grade of Taylor, but since I found out the grade in the numerator and in the denominator weren't alike, I chose to try and scale down one grade of Taylor, and I found my self like this: $$\frac{(x^2-x^2+o(x^2) )}{( (1+x^2)+(1-x^2)-2+o(x^2) )}$$ which should be: $$\frac{0+o(x^2)}{0+o(x^2)}$$ which should lead to $0$. Well, my teacher valued this wrong, and I think i'm missing something, I either don't understand how to apply Taylor the right way, or my teacher did a mis-correction (I never was able to see where my teacher said I was wrong, so that's why I'm asking you guys) Can someone tell me if I really was wrong, and in case I was explain how I should have solved this? Thanks a lot.","I recently took an math exam where I had this limit to solve $$ \lim_{x \to 0} \frac {(x^2-\sin x^2) }{ (e^ {x^2}+ e^ {-x^2} -2)}  $$ and I tought I did it right, since I proceeded like this: 1st I applied Taylor expansion of the terms to the second grade of Taylor, but since I found out the grade in the numerator and in the denominator weren't alike, I chose to try and scale down one grade of Taylor, and I found my self like this: $$\frac{(x^2-x^2+o(x^2) )}{( (1+x^2)+(1-x^2)-2+o(x^2) )}$$ which should be: $$\frac{0+o(x^2)}{0+o(x^2)}$$ which should lead to $0$. Well, my teacher valued this wrong, and I think i'm missing something, I either don't understand how to apply Taylor the right way, or my teacher did a mis-correction (I never was able to see where my teacher said I was wrong, so that's why I'm asking you guys) Can someone tell me if I really was wrong, and in case I was explain how I should have solved this? Thanks a lot.",,"['calculus', 'limits', 'taylor-expansion']"
62,Limit of a function whose values depend on $x$ being odd or even,Limit of a function whose values depend on  being odd or even,x,"I couldn't find an answer through google or here, so i hope this isn't a duplicate. Let $f(x)$ be given by: $$ f(x) = \begin{cases}        x & : x=2n\\        1/x & : x=2n+1    \end{cases} $$ Find $\lim_{x \to \infty} f(x).$ The limit is different depending on $x$ being odd or even. So what's the limit of $f(x)$? Attempt: this limit doesn't exist because we have different values for $x \to \infty$ which could be either odd or even. My doubt is $\infty$ can't be both even and odd at the same time so one should apply. So what do you ladies/gents think? Also, when confronted with functions like these and one needs to know the limit to gain more information about the behavior of $f(x)$, how should the problem be attacked?","I couldn't find an answer through google or here, so i hope this isn't a duplicate. Let $f(x)$ be given by: $$ f(x) = \begin{cases}        x & : x=2n\\        1/x & : x=2n+1    \end{cases} $$ Find $\lim_{x \to \infty} f(x).$ The limit is different depending on $x$ being odd or even. So what's the limit of $f(x)$? Attempt: this limit doesn't exist because we have different values for $x \to \infty$ which could be either odd or even. My doubt is $\infty$ can't be both even and odd at the same time so one should apply. So what do you ladies/gents think? Also, when confronted with functions like these and one needs to know the limit to gain more information about the behavior of $f(x)$, how should the problem be attacked?",,['limits']
63,Rate of convergence of $\left[1+\frac{a}{x}\right]^x$ to $\operatorname{exp}[a]$ as $x\rightarrow\infty$,Rate of convergence of  to  as,\left[1+\frac{a}{x}\right]^x \operatorname{exp}[a] x\rightarrow\infty,"It's well-known that $\lim_{x\rightarrow\infty}\left[1+\frac{a}{x}\right]^x=\operatorname{exp}[a]$.  I am wondering how fast does the limit converge as $x$ increases, and how the speed of convergence depends on $a$. That is, I would like to find out what $f(x,a)$ is where: $$\left|\left[1+\frac{a}{x}\right]^x-\operatorname{exp}[a]\right|=\mathcal{O}(f(x,a))$$ However, I'm having trouble evaluating that absolute value.  Any tips?  Perhaps this a known result...","It's well-known that $\lim_{x\rightarrow\infty}\left[1+\frac{a}{x}\right]^x=\operatorname{exp}[a]$.  I am wondering how fast does the limit converge as $x$ increases, and how the speed of convergence depends on $a$. That is, I would like to find out what $f(x,a)$ is where: $$\left|\left[1+\frac{a}{x}\right]^x-\operatorname{exp}[a]\right|=\mathcal{O}(f(x,a))$$ However, I'm having trouble evaluating that absolute value.  Any tips?  Perhaps this a known result...",,"['limits', 'asymptotics', 'exponential-function']"
64,If $f$ is a real-valued differentiable function such that $\lim_{x\to \infty}f'(x) = 0$ then $\lim_{x\to \infty}(f(x+1)-f(x))=0$. [duplicate],If  is a real-valued differentiable function such that  then . [duplicate],f \lim_{x\to \infty}f'(x) = 0 \lim_{x\to \infty}(f(x+1)-f(x))=0,"This question already has an answer here : If $f:\mathbb{R}\rightarrow \mathbb{R}$ is differentiable and $\lim_{x\to \infty } f^\prime(x)=0$ show that $\lim _{x\to \infty } (f(x+1)-f(x))=0$. (1 answer) Closed 5 years ago . Prove: If $f$ is a real-valued differentiable function such that $\lim_{x\to \infty}f'(x) = 0$ then $\lim_{x\to \infty}(f(x+1)-f(x))=0$ . Proof: We know $\lim_{x\to \infty}f'(x) = 0$ . (1) By the mean value theorem there exists $c\in [x, x+1]$ such that $f'(c) = \dfrac{f(x+1)-f(x)}{1}$ $\iff \lim_{x\to \infty} f'(c) = \lim_{x\to \infty} f(x+1)-f(x) = 0$ by (1) . End of Proof.",This question already has an answer here : If $f:\mathbb{R}\rightarrow \mathbb{R}$ is differentiable and $\lim_{x\to \infty } f^\prime(x)=0$ show that $\lim _{x\to \infty } (f(x+1)-f(x))=0$. (1 answer) Closed 5 years ago . Prove: If is a real-valued differentiable function such that then . Proof: We know . (1) By the mean value theorem there exists such that by (1) . End of Proof.,"f \lim_{x\to \infty}f'(x) = 0 \lim_{x\to \infty}(f(x+1)-f(x))=0 \lim_{x\to \infty}f'(x) = 0 c\in [x, x+1] f'(c) = \dfrac{f(x+1)-f(x)}{1} \iff \lim_{x\to \infty} f'(c) = \lim_{x\to \infty} f(x+1)-f(x) = 0","['calculus', 'limits', 'derivatives']"
65,Limit:$ \lim\limits_{n\rightarrow\infty}\left ( n\bigl(1-\sqrt[n]{\ln(n)} \bigr) \right )$,Limit:, \lim\limits_{n\rightarrow\infty}\left ( n\bigl(1-\sqrt[n]{\ln(n)} \bigr) \right ),"I find to difficult to evaluate with $$\lim_{n\rightarrow\infty}\left ( n\left(1-\sqrt[n]{\ln(n)} \right) \right )$$ I tried to use the fact, that $$\frac{1}{1-n} \geqslant \ln(n)\geqslant 1+n$$ what gives $$\lim_{n\rightarrow\infty}\left ( n\left(1-\sqrt[n]{\ln(n)} \right) \right ) \geqslant \lim_{n\rightarrow\infty} n(1-\sqrt[n]{1+n}) =\lim_{n\rightarrow\infty}n *\lim_{n\rightarrow\infty}(1-\sqrt[n]{1+n})$$ $$(1-\sqrt[n]{1+n})\rightarrow -1\Rightarrow\lim_{n\rightarrow\infty}\left ( n\left(1-\sqrt[n]{\ln(n)} \right) \right )\rightarrow-\infty$$Is it correct? If not, what do I wrong?","I find to difficult to evaluate with $$\lim_{n\rightarrow\infty}\left ( n\left(1-\sqrt[n]{\ln(n)} \right) \right )$$ I tried to use the fact, that $$\frac{1}{1-n} \geqslant \ln(n)\geqslant 1+n$$ what gives $$\lim_{n\rightarrow\infty}\left ( n\left(1-\sqrt[n]{\ln(n)} \right) \right ) \geqslant \lim_{n\rightarrow\infty} n(1-\sqrt[n]{1+n}) =\lim_{n\rightarrow\infty}n *\lim_{n\rightarrow\infty}(1-\sqrt[n]{1+n})$$ $$(1-\sqrt[n]{1+n})\rightarrow -1\Rightarrow\lim_{n\rightarrow\infty}\left ( n\left(1-\sqrt[n]{\ln(n)} \right) \right )\rightarrow-\infty$$Is it correct? If not, what do I wrong?",,"['real-analysis', 'limits']"
66,How to calculate the limit: $\lim_{n\to\infty}\sum_{k=1}^n\big(\frac{k}{n}\big)^n$ [duplicate],How to calculate the limit:  [duplicate],\lim_{n\to\infty}\sum_{k=1}^n\big(\frac{k}{n}\big)^n,This question already has answers here : What is $\lim_{n\to \infty}\sum_{k=1}^n \left(\frac{k}{n}\right)^n$? [duplicate] (4 answers) Closed 4 years ago . How to calculate the following limit?$$\lim\limits_{n\to\infty}\sum\limits_{k=1}^n\left(\frac{k}{n}\right)^n$$ It is easy to seem the limit's existence.  But I don't know how to calculate its value.,This question already has answers here : What is $\lim_{n\to \infty}\sum_{k=1}^n \left(\frac{k}{n}\right)^n$? [duplicate] (4 answers) Closed 4 years ago . How to calculate the following limit?$$\lim\limits_{n\to\infty}\sum\limits_{k=1}^n\left(\frac{k}{n}\right)^n$$ It is easy to seem the limit's existence.  But I don't know how to calculate its value.,,['limits']
67,Finding Limits of Trig Functions: $\lim_{\theta \rightarrow 0}\frac {\sin^2\theta}{\theta}$,Finding Limits of Trig Functions:,\lim_{\theta \rightarrow 0}\frac {\sin^2\theta}{\theta},"I am asked find the following limit $$\lim_{\theta \rightarrow 0}\frac {\sin^2\theta}{\theta}$$ I recognize that $$\lim_{\theta \rightarrow 0}\frac{\sin\theta}{\theta}=1$$ But because I have $sin^2\theta$ in the numerator, I am left with... $$\lim_{\theta \rightarrow 0}1(\sin\theta)$$ When I think about what this implies, I reason that the ratio of the opposite side over hypotenuse of the angle $\theta$ must approach approach zero, but for this to happen the opposite side would have a value of zero, which means the triangle formed would have no x component. $$\lim_{\theta \rightarrow 0}1(\sin\theta)=0$$ Is my reasoning correct? Am I thinking about this question in a constructive manner?","I am asked find the following limit $$\lim_{\theta \rightarrow 0}\frac {\sin^2\theta}{\theta}$$ I recognize that $$\lim_{\theta \rightarrow 0}\frac{\sin\theta}{\theta}=1$$ But because I have $sin^2\theta$ in the numerator, I am left with... $$\lim_{\theta \rightarrow 0}1(\sin\theta)$$ When I think about what this implies, I reason that the ratio of the opposite side over hypotenuse of the angle $\theta$ must approach approach zero, but for this to happen the opposite side would have a value of zero, which means the triangle formed would have no x component. $$\lim_{\theta \rightarrow 0}1(\sin\theta)=0$$ Is my reasoning correct? Am I thinking about this question in a constructive manner?",,"['calculus', 'trigonometry', 'limits']"
68,Limit when circumference shrinks,Limit when circumference shrinks,,"Let $C_1$ be a fixed circumference with equation $(x-1)^2  + y^2 = 1$ and $C_2$ a  circumference to be shrinked, with center at $(0, 0)$ and radius $r$. Let $P$ be the point $(0, r)$,  $Q$ the upper intersection between $C_1$ and $C_2$  and $R$ the intersection between the line $PQ$ with the $x$ axis. What happens with $R$ when $C_2$ shrinks (i.e., $r \rightarrow 0^+$) ?","Let $C_1$ be a fixed circumference with equation $(x-1)^2  + y^2 = 1$ and $C_2$ a  circumference to be shrinked, with center at $(0, 0)$ and radius $r$. Let $P$ be the point $(0, r)$,  $Q$ the upper intersection between $C_1$ and $C_2$  and $R$ the intersection between the line $PQ$ with the $x$ axis. What happens with $R$ when $C_2$ shrinks (i.e., $r \rightarrow 0^+$) ?",,"['calculus', 'limits']"
69,How can $\mathop{\lim}\limits_{x \to \infty}({x+x\space\sin^2 x})$ exist if $\mathop{\lim}\limits_{x \to \infty}(x\space\sin^2 x)$ does not?,How can  exist if  does not?,\mathop{\lim}\limits_{x \to \infty}({x+x\space\sin^2 x}) \mathop{\lim}\limits_{x \to \infty}(x\space\sin^2 x),"In the text I'm using (Spivak's Calculus , 4E), it is established (problem 5.39(iii)) that $$\mathop{\lim}\limits_{x \to \infty}\left({x\space\sin^{2} x}\right)$$ ""does not exist"". It is also established (5.39(c)) that [A] if $\mathop{\lim}\limits_{x \to \infty}f(x)$ exists, but $\mathop{\lim}\limits_{x \to \infty}g(x)$ does not, then $\mathop{\lim}\limits_{x \to \infty}\left[f(x)+g(x))\right]$ cannot exist. But the text also establishes (5.39(ii)) that $$\mathop{\lim}\limits_{x \to \infty}\left({x+x\space\sin^{2}x}\right)=\infty.$$ which seems to be a contradiction of the just established property of limits, with $f(x)=x$ and $g(x)=x\space\sin^{2} x$. Is there something important going on here with regard to limits that ""equal"" infinity?","In the text I'm using (Spivak's Calculus , 4E), it is established (problem 5.39(iii)) that $$\mathop{\lim}\limits_{x \to \infty}\left({x\space\sin^{2} x}\right)$$ ""does not exist"". It is also established (5.39(c)) that [A] if $\mathop{\lim}\limits_{x \to \infty}f(x)$ exists, but $\mathop{\lim}\limits_{x \to \infty}g(x)$ does not, then $\mathop{\lim}\limits_{x \to \infty}\left[f(x)+g(x))\right]$ cannot exist. But the text also establishes (5.39(ii)) that $$\mathop{\lim}\limits_{x \to \infty}\left({x+x\space\sin^{2}x}\right)=\infty.$$ which seems to be a contradiction of the just established property of limits, with $f(x)=x$ and $g(x)=x\space\sin^{2} x$. Is there something important going on here with regard to limits that ""equal"" infinity?",,"['calculus', 'real-analysis', 'limits']"
70,Please help with differentiation under the integral,Please help with differentiation under the integral,,"This question has an answer that relates differentiation under the integral to the OP. Again, here's the original integral: $$\int_0^\infty\frac{\cos\;x}{1+x^2}\mathrm{d}x$$ ...and we let $$ F(y) = \int\nolimits_{0}^{\infty} \frac{\sin xy}{x(1+x^2)} \ dx \ \ \text{for} \quad\quad y > 0$$ The first part of interest is in showing that $\displaystyle F''(y) - F(y) + \pi/2 = 0$.  Is it necessary to integrate $F(y)$ to show this?  What about the possibility of taking $\lim_{y \to 0+}$ beforehand?  I'm wondering if someone can help explain this step in much greater detail.  I'm a little hazy with the $y>0$ portion of it, and whether or not integration has to occur here.  I'm trying to make sure I thoroughly understand this post so that I can apply it later to different problems.","This question has an answer that relates differentiation under the integral to the OP. Again, here's the original integral: $$\int_0^\infty\frac{\cos\;x}{1+x^2}\mathrm{d}x$$ ...and we let $$ F(y) = \int\nolimits_{0}^{\infty} \frac{\sin xy}{x(1+x^2)} \ dx \ \ \text{for} \quad\quad y > 0$$ The first part of interest is in showing that $\displaystyle F''(y) - F(y) + \pi/2 = 0$.  Is it necessary to integrate $F(y)$ to show this?  What about the possibility of taking $\lim_{y \to 0+}$ beforehand?  I'm wondering if someone can help explain this step in much greater detail.  I'm a little hazy with the $y>0$ portion of it, and whether or not integration has to occur here.  I'm trying to make sure I thoroughly understand this post so that I can apply it later to different problems.",,"['integration', 'limits']"
71,Solving for a limit with an indeterminate fraction and square root of X,Solving for a limit with an indeterminate fraction and square root of X,,I have a test tomorrow and there's one thing I found that I don't understand while reviewing. I've looked all over YouTube and Google and just can't find out how to work this problem: $$ \lim_{x\to16} \frac{x-16}{4-\sqrt{x}} $$ It's obvious that it's indeterminate but I can't get any further than that.,I have a test tomorrow and there's one thing I found that I don't understand while reviewing. I've looked all over YouTube and Google and just can't find out how to work this problem: $$ \lim_{x\to16} \frac{x-16}{4-\sqrt{x}} $$ It's obvious that it's indeterminate but I can't get any further than that.,,"['calculus', 'limits']"
72,A trigonometric integral and algebraic integral limit of the same kind,A trigonometric integral and algebraic integral limit of the same kind,,"Evaluate the value of: $$(i)\lim_{n \to ∞} n^2 \left(\int_0^{\pi/2} (\sin^n(x)+\cos^n(x))^{1/n} dx-\sqrt2 \right)$$ $$(ii)\lim_{n \to ∞} n^2 \left(\int_0^1 (x^n+(1-x)^n)^{1/n} dx-3/4 \right)$$ Both of these limits have a similar form and it's easy to see that it is a $0$ times $∞$ form; it is easy to evaluate both integrals by using the sandwich theorem. In (i), for example, first I broke it into two integrals from $0$ to $\pi/4$ and $\pi/4$ to $\pi/2$ and noted that $\sin(x)>\cos(x)$ in the first interval and vice versa in the second interval. Using these facts, I put a lower bound and upper bound on the integral; $$\int_0^{\pi/4}2^{1/n} \sin x dx+\int_{\pi/4}^{\pi/2} 2^{1/n} \cos x dx<\int_0^{\pi/2} (\sin^n(x)+\cos^n(x))^{1/n} dx <\int_0^{\pi/4}2^{1/n} \cos x dx+\int_{\pi/4}^{\pi/2} 2^{1/n} \sin x dx $$ Upon passing the limit, the value of the integral came out to be $\sqrt2$ . Similarly, for the second integral, I broke it down into an interval from $0$ to $1/2$ and $1/2$ to $1$ to obtain $3/4$ . But, how do I evaluate these infinity times zero indeterminate forms using the sandwich theorem or anything?","Evaluate the value of: Both of these limits have a similar form and it's easy to see that it is a times form; it is easy to evaluate both integrals by using the sandwich theorem. In (i), for example, first I broke it into two integrals from to and to and noted that in the first interval and vice versa in the second interval. Using these facts, I put a lower bound and upper bound on the integral; Upon passing the limit, the value of the integral came out to be . Similarly, for the second integral, I broke it down into an interval from to and to to obtain . But, how do I evaluate these infinity times zero indeterminate forms using the sandwich theorem or anything?",(i)\lim_{n \to ∞} n^2 \left(\int_0^{\pi/2} (\sin^n(x)+\cos^n(x))^{1/n} dx-\sqrt2 \right) (ii)\lim_{n \to ∞} n^2 \left(\int_0^1 (x^n+(1-x)^n)^{1/n} dx-3/4 \right) 0 ∞ 0 \pi/4 \pi/4 \pi/2 \sin(x)>\cos(x) \int_0^{\pi/4}2^{1/n} \sin x dx+\int_{\pi/4}^{\pi/2} 2^{1/n} \cos x dx<\int_0^{\pi/2} (\sin^n(x)+\cos^n(x))^{1/n} dx <\int_0^{\pi/4}2^{1/n} \cos x dx+\int_{\pi/4}^{\pi/2} 2^{1/n} \sin x dx  \sqrt2 0 1/2 1/2 1 3/4,"['real-analysis', 'limits', 'definite-integrals', 'trigonometric-integrals']"
73,Limiting behaviour of state vector under delayed (open-loop) inputs,Limiting behaviour of state vector under delayed (open-loop) inputs,,"Given a discrete-time LTI system as $$x_{i+1} = A x_{i} + B u_{i}$$ and suppose the feedback law $u_{i} = \mathcal{K}(x_{i})$ assymtotically stabilizes the closed-loop system. Now, consider the modified (i.e. delayed) input law $u_{i} = \tilde{\mathcal{K}}(x_{i-I})$ for some constant (but possibly unknown) $I \gg 1$ (e.g. 10). Given an initial condition $x_{0}$ , what can I see about whether there exists some $i$ such that $\Vert x_{i} \Vert \geq \rho $ ? Does the state norm grow beyond some bound, given enough time? And does this depend on $A$ being (Schur) stable? My intuition (and simulation) says that the state trajectory will eventually leave any finite set (regardless of $A$ being Schur ), as we are basically injecting an arbitrary input into the system, which is not related to the current state. Thus, the system is running in open loop. However, how do I prove this, or at least make this more mathematically rigorous? Any help would be appreciated! P.S. This is related to replay attacks in control systems which I'm investigating.","Given a discrete-time LTI system as and suppose the feedback law assymtotically stabilizes the closed-loop system. Now, consider the modified (i.e. delayed) input law for some constant (but possibly unknown) (e.g. 10). Given an initial condition , what can I see about whether there exists some such that ? Does the state norm grow beyond some bound, given enough time? And does this depend on being (Schur) stable? My intuition (and simulation) says that the state trajectory will eventually leave any finite set (regardless of being Schur ), as we are basically injecting an arbitrary input into the system, which is not related to the current state. Thus, the system is running in open loop. However, how do I prove this, or at least make this more mathematically rigorous? Any help would be appreciated! P.S. This is related to replay attacks in control systems which I'm investigating.",x_{i+1} = A x_{i} + B u_{i} u_{i} = \mathcal{K}(x_{i}) u_{i} = \tilde{\mathcal{K}}(x_{i-I}) I \gg 1 x_{0} i \Vert x_{i} \Vert \geq \rho  A A,"['limits', 'convergence-divergence', 'control-theory', 'linear-control']"
74,How to evaluate $\underset{n\rightarrow \infty}{\lim}\left(\frac{\sqrt[n+1]{(n+1)!(2n+1)!!}}{n+1}-\frac{\sqrt[n]{n!(2n-1)!!}}{n}\right) $?,How to evaluate ?,\underset{n\rightarrow \infty}{\lim}\left(\frac{\sqrt[n+1]{(n+1)!(2n+1)!!}}{n+1}-\frac{\sqrt[n]{n!(2n-1)!!}}{n}\right) ,"for $n$$\in$$N^+$ ,solve that: $$\underset{n\rightarrow \infty}{\lim}\left( \frac{\sqrt[n+1]{\left( n+1 \right) !\left( 2n+1 \right) !!}}{n+1}-\frac{\sqrt[n]{n!\left( 2n-1 \right) !!}}{n} \right) $$ i notice double factorials so i try to express it in terms of simple factorials and powers of 2,then it's feasible to use Stirling formula $$f\left( n \right):=\frac{\sqrt[n+1]{\left( n+1 \right) !\left( 2n+1 \right) !!}}{n+1}$$ since $\left( 2n+1 \right) !!=\frac{\left( 2n+1 \right) !}{\left( 2n \right) !!}=\frac{\left( 2n+1 \right) !}{2^nn!}$ and $n!\sim \sqrt{2\pi n}\left( \frac{n}{e} \right) ^n$ ,we have $$f\left( n \right)=\frac{2}{e^2}\left[ 2\pi \left( n+1 \right) \left( 2n+1 \right) \left( n+\frac{1}{4n}+1 \right) ^{2n+1} \right] ^{\frac{1}{2n+2}}$$ hence we should find $$\underset{n\rightarrow \infty}{\frac{2}{e^2}\lim}\left( \left[ 2\pi \left( n+1 \right) \left( 2n+1 \right) \left( n+\frac{1}{4n}+1 \right) ^{2n+1} \right] ^{\frac{1}{2n+2}}-\left[ 2\pi n\left( 2n-1 \right) \left( n+\frac{1}{4n-4} \right) ^{2n-1} \right] ^{\frac{1}{2n}} \right)$$ i guess this can be seen as $$\underset{n\rightarrow \infty}{\frac{2}{e^2}\lim}\left( \left( n+\frac{1}{4n}+1 \right) -\left( n+\frac{1}{4n-4} \right) \right) $$ which equals $\frac{2}{e^2}$ .but how to prove it strictly？or is there any approach  better ?","for ,solve that: i notice double factorials so i try to express it in terms of simple factorials and powers of 2,then it's feasible to use Stirling formula since and ,we have hence we should find i guess this can be seen as which equals .but how to prove it strictly？or is there any approach  better ?",n\inN^+ \underset{n\rightarrow \infty}{\lim}\left( \frac{\sqrt[n+1]{\left( n+1 \right) !\left( 2n+1 \right) !!}}{n+1}-\frac{\sqrt[n]{n!\left( 2n-1 \right) !!}}{n} \right)  f\left( n \right):=\frac{\sqrt[n+1]{\left( n+1 \right) !\left( 2n+1 \right) !!}}{n+1} \left( 2n+1 \right) !!=\frac{\left( 2n+1 \right) !}{\left( 2n \right) !!}=\frac{\left( 2n+1 \right) !}{2^nn!} n!\sim \sqrt{2\pi n}\left( \frac{n}{e} \right) ^n f\left( n \right)=\frac{2}{e^2}\left[ 2\pi \left( n+1 \right) \left( 2n+1 \right) \left( n+\frac{1}{4n}+1 \right) ^{2n+1} \right] ^{\frac{1}{2n+2}} \underset{n\rightarrow \infty}{\frac{2}{e^2}\lim}\left( \left[ 2\pi \left( n+1 \right) \left( 2n+1 \right) \left( n+\frac{1}{4n}+1 \right) ^{2n+1} \right] ^{\frac{1}{2n+2}}-\left[ 2\pi n\left( 2n-1 \right) \left( n+\frac{1}{4n-4} \right) ^{2n-1} \right] ^{\frac{1}{2n}} \right) \underset{n\rightarrow \infty}{\frac{2}{e^2}\lim}\left( \left( n+\frac{1}{4n}+1 \right) -\left( n+\frac{1}{4n-4} \right) \right)  \frac{2}{e^2},"['calculus', 'limits']"
75,Find the limit of $n^{2} ( \sqrt[n]{x} - \sqrt[n+1]{x} )$ as $x \to \infty$,Find the limit of  as,n^{2} ( \sqrt[n]{x} - \sqrt[n+1]{x} ) x \to \infty,"We are tasked with evaluating this limit $$\lim_{x\to \infty} n^{2} ( \sqrt[n]{x} - \sqrt[n+1]{x} )$$ As opposed to quite a lot of variants of this limit, for this one, the one that involves in the limit is $n$ , not $x$ . So we can make $n^2$ quite irrelevant in this case, so $$\lim_{x\to \infty} n^{2} ( \sqrt[n]{x} - \sqrt[n+1]{x} )= n^{2} \lim_{x\to \infty} ( \sqrt[n]{x} - \sqrt[n+1]{x} )$$ Here, I tried to solve for $\lim_{x\to \infty} ( \sqrt[n]{x} - \sqrt[n+1]{x} )$ . The only method I could think of is using the identity $$x^n - y^n = (x-y)(x^{n-1} + x^{n-2} y + ... + x y^{n-2} + y^{n-1})$$ Then we have $$ \begin{split} \lim_{x\to \infty} ( \sqrt[n]{x} - \sqrt[n+1]{x} ) &= \lim_{x\to \infty} ( \sqrt[n]{x}-1+1 - \sqrt[n+1]{x} ) \\ & = \lim_{x\to \infty} ( \sqrt[n]{x}-1) - \lim_{x\to \infty} (\sqrt[n+1]{x} - 1) \\ & = \lim_{x\to \infty} \frac{( \sqrt[n]{x}-1)[ (\sqrt[n]{x})^{n-1} + \cdots + 1 ]}{[(\sqrt[n]{x})^{n-1} + \cdots + 1 ]} - \lim_{x\to \infty} \frac{( \sqrt[n+1]{x}-1)[ (\sqrt[n+1]{x})^{n} + \cdots + 1 ]}{[(\sqrt[n+1]{x})^{n} + \cdots + 1 ]} \end{split} $$ This reduces to $$\lim_{x\to \infty} \frac{x-1}{[(\sqrt[n]{x})^{n-1} + \cdots + 1 ]} - \lim_{x\to \infty} \frac{x-1}{[(\sqrt[n+1]{x})^{n} + \cdots + 1 ]}$$ Now, I have some confusions with this one. So my current situation is that I believe that the denominator will always go to bigger and bigger, to $\infty$ , as far as $n > 0$ , and perhaps most of the time, my assumption is that $$[(\sqrt[n+1]{x})^{n} + \cdots + 1 ] \geq x - 1$$ so the fraction in total will go to $0$ as far as the assumption is given as such. But when I approach this problem in another way, which is by rewrite the whole limit as $$n^2(\lim_{x\to\infty} x^\frac1n - x^\frac{1}{n+1})$$ Using that $$ x^{\frac1n}=e^{\frac{\ln{x}}{n}}$$ The limit becomes $$n^2 \lim_{x\to\infty} (e^{\frac{\ln{x}}{n}} - e^{\frac{\ln{x}}{n+1}})$$ of which as long as $n$ does not reach $\infty$ , the limit goes to infinity, which is contradictory to what the way we have done above results in, given the assumption. I am pretty sure that there are faults, but I do not know how to clearly approach and fix those issues. I also tried to disprove the assumption, but I do not know how. How would I proceed then to solve this limit? Where I am wrong at?","We are tasked with evaluating this limit As opposed to quite a lot of variants of this limit, for this one, the one that involves in the limit is , not . So we can make quite irrelevant in this case, so Here, I tried to solve for . The only method I could think of is using the identity Then we have This reduces to Now, I have some confusions with this one. So my current situation is that I believe that the denominator will always go to bigger and bigger, to , as far as , and perhaps most of the time, my assumption is that so the fraction in total will go to as far as the assumption is given as such. But when I approach this problem in another way, which is by rewrite the whole limit as Using that The limit becomes of which as long as does not reach , the limit goes to infinity, which is contradictory to what the way we have done above results in, given the assumption. I am pretty sure that there are faults, but I do not know how to clearly approach and fix those issues. I also tried to disprove the assumption, but I do not know how. How would I proceed then to solve this limit? Where I am wrong at?","\lim_{x\to \infty} n^{2} ( \sqrt[n]{x} - \sqrt[n+1]{x} ) n x n^2 \lim_{x\to \infty} n^{2} ( \sqrt[n]{x} - \sqrt[n+1]{x} )= n^{2} \lim_{x\to \infty} ( \sqrt[n]{x} - \sqrt[n+1]{x} ) \lim_{x\to \infty} ( \sqrt[n]{x} - \sqrt[n+1]{x} ) x^n - y^n = (x-y)(x^{n-1} + x^{n-2} y + ... + x y^{n-2} + y^{n-1}) 
\begin{split}
\lim_{x\to \infty} ( \sqrt[n]{x} - \sqrt[n+1]{x} ) &= \lim_{x\to \infty} ( \sqrt[n]{x}-1+1 - \sqrt[n+1]{x} ) \\
& = \lim_{x\to \infty} ( \sqrt[n]{x}-1) - \lim_{x\to \infty} (\sqrt[n+1]{x} - 1) \\
& = \lim_{x\to \infty} \frac{( \sqrt[n]{x}-1)[ (\sqrt[n]{x})^{n-1} + \cdots + 1 ]}{[(\sqrt[n]{x})^{n-1} + \cdots + 1 ]} - \lim_{x\to \infty} \frac{( \sqrt[n+1]{x}-1)[ (\sqrt[n+1]{x})^{n} + \cdots + 1 ]}{[(\sqrt[n+1]{x})^{n} + \cdots + 1 ]}
\end{split}
 \lim_{x\to \infty} \frac{x-1}{[(\sqrt[n]{x})^{n-1} + \cdots + 1 ]} - \lim_{x\to \infty} \frac{x-1}{[(\sqrt[n+1]{x})^{n} + \cdots + 1 ]} \infty n > 0 [(\sqrt[n+1]{x})^{n} + \cdots + 1 ] \geq x - 1 0 n^2(\lim_{x\to\infty} x^\frac1n - x^\frac{1}{n+1})  x^{\frac1n}=e^{\frac{\ln{x}}{n}} n^2 \lim_{x\to\infty} (e^{\frac{\ln{x}}{n}} - e^{\frac{\ln{x}}{n+1}}) n \infty","['real-analysis', 'calculus', 'limits', 'analysis']"
76,A conjecture involving series with zeta function,A conjecture involving series with zeta function,,"Recently, I tried to evaluate a limit proposed by MSE user Black Emperor. In the process of evaluating the limit, I have obtained the following equality. $$ \lim_{N\rightarrow \infty} \sum_{n=0}^{N-2}{\frac{\left( -1 \right) ^n}{\left( 2n+1 \right) !}\zeta \left( N-n \right)}=\sin\left(1\right) $$ But I believe this can applied to a broader case of limits. Namely, if we have a function $f$ that is holomorphic around zero with a infinite radius of convergence. The following should hold \begin{align*} \lim_{N\rightarrow \infty} \sum_{n=0}^{N-2}{\frac{1}{n!}\zeta \left( N-n \right)}f^{\left( n \right)}\left( 0 \right)  =&\lim_{N\rightarrow \infty} \sum_{n=0}^{N-2}{\frac{1}{n!}\sum_{k=1}^{\infty}{\frac{k^n}{k^N}}}f^{\left( n \right)}\left( 0 \right)  \\ =&\lim_{N\rightarrow \infty} \sum_{k=1}^{\infty}{\frac{1}{k^N}\sum_{n=0}^{N-2}{\frac{1}{n!}f^{\left( n \right)}\left( 0 \right) k^n}} \\ =&\lim_{N\rightarrow \infty} \sum_{k=1}^{\infty}{\frac{1}{k^N}\sum_{n=0}^{\infty}{\frac{1}{n!}f^{\left( n \right)}\left( 0 \right) k^n}} \\ =&\lim_{N\rightarrow \infty} \sum_{k=1}^{\infty}{\frac{f\left( k \right)}{k^N}}=f\left( 1 \right)  \end{align*} I have verified this result numerically for a some functions, and it seems that this equality holds for functions like: $\mathbb{sin}\left(x\right)$ , $\mathbb{cos}\left(x\right)$ , $\mathbb{exp}\left(x\right)$ , $\mathbb{sinh}\left(x\right)$ , $\mathbb{cosh}\left(x\right)$ , $\sqrt{x+1}$ , $\mathbb{arctan}\left(x\right)$ , $\mathbb{ln}\left(x\right)$ , $\mathbb{erf}\left(x\right)$ . However, it did no longer work when I plugged $\mathrm{W}\left(x\right)$ in the Lambert W function. I have three question here. Is this correct? As you can see, much of the examples do not have a infinite radius of convergence, yet, the equality still holds. Therefore, What's the ture criteria of this equality? Can we generalize this equality to a bigger family of functions?","Recently, I tried to evaluate a limit proposed by MSE user Black Emperor. In the process of evaluating the limit, I have obtained the following equality. But I believe this can applied to a broader case of limits. Namely, if we have a function that is holomorphic around zero with a infinite radius of convergence. The following should hold I have verified this result numerically for a some functions, and it seems that this equality holds for functions like: , , , , , , , , . However, it did no longer work when I plugged in the Lambert W function. I have three question here. Is this correct? As you can see, much of the examples do not have a infinite radius of convergence, yet, the equality still holds. Therefore, What's the ture criteria of this equality? Can we generalize this equality to a bigger family of functions?","
\lim_{N\rightarrow \infty} \sum_{n=0}^{N-2}{\frac{\left( -1 \right) ^n}{\left( 2n+1 \right) !}\zeta \left( N-n \right)}=\sin\left(1\right)
 f \begin{align*}
\lim_{N\rightarrow \infty} \sum_{n=0}^{N-2}{\frac{1}{n!}\zeta \left( N-n \right)}f^{\left( n \right)}\left( 0 \right) 
=&\lim_{N\rightarrow \infty} \sum_{n=0}^{N-2}{\frac{1}{n!}\sum_{k=1}^{\infty}{\frac{k^n}{k^N}}}f^{\left( n \right)}\left( 0 \right) 
\\
=&\lim_{N\rightarrow \infty} \sum_{k=1}^{\infty}{\frac{1}{k^N}\sum_{n=0}^{N-2}{\frac{1}{n!}f^{\left( n \right)}\left( 0 \right) k^n}}
\\
=&\lim_{N\rightarrow \infty} \sum_{k=1}^{\infty}{\frac{1}{k^N}\sum_{n=0}^{\infty}{\frac{1}{n!}f^{\left( n \right)}\left( 0 \right) k^n}}
\\
=&\lim_{N\rightarrow \infty} \sum_{k=1}^{\infty}{\frac{f\left( k \right)}{k^N}}=f\left( 1 \right) 
\end{align*} \mathbb{sin}\left(x\right) \mathbb{cos}\left(x\right) \mathbb{exp}\left(x\right) \mathbb{sinh}\left(x\right) \mathbb{cosh}\left(x\right) \sqrt{x+1} \mathbb{arctan}\left(x\right) \mathbb{ln}\left(x\right) \mathbb{erf}\left(x\right) \mathrm{W}\left(x\right)","['real-analysis', 'limits', 'summation', 'taylor-expansion', 'power-series']"
77,Definition of the Limit: Question about $|f(x)-L|$ manipulation to find a suitable delta,Definition of the Limit: Question about  manipulation to find a suitable delta,|f(x)-L|,"In the definition of the limit, often the first step to find a suitable delta is to manipulate $|f(x)-L|$ to look something like $|x-a|$ , after that we can see delta in terms of epsilon that we can choose as a guess. I would like to ask why is it that we can use this manipulation as delta and end up satisfying the statement: $|f(x)-L|<\epsilon$ whenever $0<|x-a|<\delta$ All the videos I have watched, after extracting $|x-a|$ from $|f(x)-L|$ goes like 'look! we have found an appropriate delta, now we have to check if this works'. I am having trouble understanding how this indeed satisfies the condition, or how the extraction provides us with a good guess for delta. Say for example, given $\lim_{x\to 1}(2x+3)=5$ We write $|2x+3-5|$ , and then manipulate that to be: $|2x-2|$ , $2|x-1|$ . We found $|x-a|$ (in this case, $|x-1|$ ), so we say that the suitable $\delta$ is $\epsilon/2$ , because we manipulated $|2x-2|<\epsilon$ to be $|x-1|<\epsilon/2$ How does this manipulation give us a good guess for delta?","In the definition of the limit, often the first step to find a suitable delta is to manipulate to look something like , after that we can see delta in terms of epsilon that we can choose as a guess. I would like to ask why is it that we can use this manipulation as delta and end up satisfying the statement: whenever All the videos I have watched, after extracting from goes like 'look! we have found an appropriate delta, now we have to check if this works'. I am having trouble understanding how this indeed satisfies the condition, or how the extraction provides us with a good guess for delta. Say for example, given We write , and then manipulate that to be: , . We found (in this case, ), so we say that the suitable is , because we manipulated to be How does this manipulation give us a good guess for delta?",|f(x)-L| |x-a| |f(x)-L|<\epsilon 0<|x-a|<\delta |x-a| |f(x)-L| \lim_{x\to 1}(2x+3)=5 |2x+3-5| |2x-2| 2|x-1| |x-a| |x-1| \delta \epsilon/2 |2x-2|<\epsilon |x-1|<\epsilon/2,"['calculus', 'limits', 'epsilon-delta']"
78,Proving unif. conv. of $\lim_{m \rightarrow \infty} \sum_{k=0}^m \frac{1}{k!} \left( 1 - \frac{1}{n} \right ) \cdots \left( 1 - \frac{k-1}{n} \right)$,Proving unif. conv. of,\lim_{m \rightarrow \infty} \sum_{k=0}^m \frac{1}{k!} \left( 1 - \frac{1}{n} \right ) \cdots \left( 1 - \frac{k-1}{n} \right),"I am currently working on trying to prove that $$\lim_{n \rightarrow \infty} \left( 1 + \frac{1}{n} \right)^n = \sum_{k=0}^\infty \frac{1}{k!}.$$ Here is my progress so far. $$\lim_{n \rightarrow \infty} \left( 1 + \frac{1}{n} \right)^n = \lim_{n \rightarrow \infty} \sum_{k=0}^n \frac{n!}{k! (n-k)!} \left(\frac{1}{n} \right)^n$$ $$\lim_{n \rightarrow \infty} \left( 1 + \frac{1}{n} \right)^n = \lim_{n \rightarrow \infty} \sum_{k=0}^n\frac{1}{k!} \cdot \frac{n}{n} \cdot \frac{n-1}{n} \cdot \frac{n-2}{n} \cdots \frac{n-(k-1)}{n}$$ $$\lim_{n \rightarrow \infty} \left( 1 + \frac{1}{n} \right)^n = \lim_{n \rightarrow \infty} \sum_{k=0}^n\frac{1}{k!} \left( 1 - \frac{1}{n} \right )\left( 1 - \frac{2}{n} \right ) \cdots \left( 1 - \frac{k-1}{n} \right )$$ Since for all $k > n$ , we have $\left( 1 - \frac{1}{n} \right )\left( 1 - \frac{2}{n} \right ) \cdots \left( 1 - \frac{k-1}{n} \right ) = 0$ , since one of the factors of the form $\left( 1 - \frac{i}{n} \right )$ is guaranteed to be zero when $k > n$ , we can make the sum go to infinity without affecting the result. $$\lim_{n \rightarrow \infty} \left( 1 + \frac{1}{n} \right)^n = \lim_{n \rightarrow \infty} \sum_{k=0}^\infty \frac{1}{k!} \left( 1 - \frac{1}{n} \right )\left( 1 - \frac{2}{n} \right ) \cdots \left( 1 - \frac{k-1}{n} \right )$$ $$\lim_{n \rightarrow \infty} \left( 1 + \frac{1}{n} \right)^n = \lim_{n \rightarrow \infty} \lim_{m \rightarrow \infty} \sum_{k=0}^m \frac{1}{k!} \left( 1 - \frac{1}{n} \right )\left( 1 - \frac{2}{n} \right ) \cdots \left( 1 - \frac{k-1}{n} \right )$$ At this point, I'd like to be able to swap the limits , which would allow the summation to reduce to $\sum \frac{1}{k!}$ as desired. To my knowledge, this is only possible when the conditions of the Moore-Osgood Theorem are satisfied, which say that if $f(m,n) = \sum_{k=0}^m \frac{1}{k!} \left( 1 - \frac{1}{n} \right )\left( 1 - \frac{2}{n} \right ) \cdots \left( 1 - \frac{k-1}{n} \right)$ then we must have either $\lim_{m\rightarrow \infty} f(m,n)$ or $\lim_{n\rightarrow \infty} f(m,n)$ be uniformly convergent in order to be able to swap the $m \rightarrow \infty$ and $n \rightarrow \infty$ limits above. If we define $g(n) = \sum_{k=0}^n \frac{1}{k!} \left( 1 - \frac{1}{n} \right )\left( 1 - \frac{2}{n} \right ) \cdots \left( 1 - \frac{k-1}{n} \right )$ , it is fairly evident that $\lim_{m \rightarrow \infty} f(m,n) = g(n)$ pointwise, but only uniform convergence would be sufficient for the Moore-Osgood Theorem. To show uniform convergence here, one would need to prove that $\forall \varepsilon > 0 : \exists M : \forall m \geq M : \forall n$ we have that $|f(m,n) - g(n)| < \varepsilon$ . I managed to deduce that $$|f(m,n) - g(n)|= \sum_{k=m+1}^n \frac{1}{k!} \left( 1 - \frac{1}{n} \right )\left( 1 - \frac{2}{n} \right ) \cdots \left( 1 - \frac{k-1}{n} \right ) \text{if } m< n \text{ and } 0 \text{ otherwise.}$$ At this point I feel like I must be really close, but my brain is a bit fried trying to figure out how to show that this is less than $\varepsilon$ for any arbitrary choice of $\varepsilon > 0$ under all possible values of $m \geq M$ and $n$ for a chosen $M$ . Is this even possible and could anyone provide a possible next step or hint of where to go from here?","I am currently working on trying to prove that Here is my progress so far. Since for all , we have , since one of the factors of the form is guaranteed to be zero when , we can make the sum go to infinity without affecting the result. At this point, I'd like to be able to swap the limits , which would allow the summation to reduce to as desired. To my knowledge, this is only possible when the conditions of the Moore-Osgood Theorem are satisfied, which say that if then we must have either or be uniformly convergent in order to be able to swap the and limits above. If we define , it is fairly evident that pointwise, but only uniform convergence would be sufficient for the Moore-Osgood Theorem. To show uniform convergence here, one would need to prove that we have that . I managed to deduce that At this point I feel like I must be really close, but my brain is a bit fried trying to figure out how to show that this is less than for any arbitrary choice of under all possible values of and for a chosen . Is this even possible and could anyone provide a possible next step or hint of where to go from here?","\lim_{n \rightarrow \infty} \left( 1 + \frac{1}{n} \right)^n = \sum_{k=0}^\infty \frac{1}{k!}. \lim_{n \rightarrow \infty} \left( 1 + \frac{1}{n} \right)^n = \lim_{n \rightarrow \infty} \sum_{k=0}^n \frac{n!}{k! (n-k)!} \left(\frac{1}{n} \right)^n \lim_{n \rightarrow \infty} \left( 1 + \frac{1}{n} \right)^n = \lim_{n \rightarrow \infty} \sum_{k=0}^n\frac{1}{k!} \cdot \frac{n}{n} \cdot \frac{n-1}{n} \cdot \frac{n-2}{n} \cdots \frac{n-(k-1)}{n} \lim_{n \rightarrow \infty} \left( 1 + \frac{1}{n} \right)^n = \lim_{n \rightarrow \infty} \sum_{k=0}^n\frac{1}{k!} \left( 1 - \frac{1}{n} \right )\left( 1 - \frac{2}{n} \right ) \cdots \left( 1 - \frac{k-1}{n} \right ) k > n \left( 1 - \frac{1}{n} \right )\left( 1 - \frac{2}{n} \right ) \cdots \left( 1 - \frac{k-1}{n} \right ) = 0 \left( 1 - \frac{i}{n} \right ) k > n \lim_{n \rightarrow \infty} \left( 1 + \frac{1}{n} \right)^n = \lim_{n \rightarrow \infty} \sum_{k=0}^\infty \frac{1}{k!} \left( 1 - \frac{1}{n} \right )\left( 1 - \frac{2}{n} \right ) \cdots \left( 1 - \frac{k-1}{n} \right ) \lim_{n \rightarrow \infty} \left( 1 + \frac{1}{n} \right)^n = \lim_{n \rightarrow \infty} \lim_{m \rightarrow \infty} \sum_{k=0}^m \frac{1}{k!} \left( 1 - \frac{1}{n} \right )\left( 1 - \frac{2}{n} \right ) \cdots \left( 1 - \frac{k-1}{n} \right ) \sum \frac{1}{k!} f(m,n) = \sum_{k=0}^m \frac{1}{k!} \left( 1 - \frac{1}{n} \right )\left( 1 - \frac{2}{n} \right ) \cdots \left( 1 - \frac{k-1}{n} \right) \lim_{m\rightarrow \infty} f(m,n) \lim_{n\rightarrow \infty} f(m,n) m \rightarrow \infty n \rightarrow \infty g(n) = \sum_{k=0}^n \frac{1}{k!} \left( 1 - \frac{1}{n} \right )\left( 1 - \frac{2}{n} \right ) \cdots \left( 1 - \frac{k-1}{n} \right ) \lim_{m \rightarrow \infty} f(m,n) = g(n) \forall \varepsilon > 0 : \exists M : \forall m \geq M : \forall n |f(m,n) - g(n)| < \varepsilon |f(m,n) - g(n)|= \sum_{k=m+1}^n \frac{1}{k!} \left( 1 - \frac{1}{n} \right )\left( 1 - \frac{2}{n} \right ) \cdots \left( 1 - \frac{k-1}{n} \right ) \text{if } m< n \text{ and } 0 \text{ otherwise.} \varepsilon \varepsilon > 0 m \geq M n M","['limits', 'uniform-convergence', 'eulers-number-e']"
79,What is the limiting mean value of the product of the exponents in the prime factorization of numbers?,What is the limiting mean value of the product of the exponents in the prime factorization of numbers?,,"Let $n = p_1^{a_1}p_2^{a_2}\cdots p_m^{a_m}$ be the prime factorization of $n$ and let $f(n) = a_1 a_2 \cdots a_m$ . Using a heuristic argument I am able to show that the mean value of $\lim_{n \to \infty} \frac{1}{n}\sum_{k = 1}^n f(k)$ is close to $$ \bigg(2 - \frac{1}{\zeta(2)}\bigg)\bigg(N_c + \frac{1}{\zeta(2)} - \frac{2}{\zeta(3)}\bigg) - \frac{1}{\zeta(2)} + \frac{2}{\zeta(3)} \approx 1.95979 $$ where $N_c \approx 1.7052$ is the Niven's constant . Experimental data seems to be consistent with this heuristics. For $n = 10^6$ , the mean is $1.9406$ while for $n = 1.42 \times 10^{10}$ , the mean increased slightly to $1.94357$ . Question : What is the limiting value of the mean? Is there a closed form for the mean of $f(n)$ ?","Let be the prime factorization of and let . Using a heuristic argument I am able to show that the mean value of is close to where is the Niven's constant . Experimental data seems to be consistent with this heuristics. For , the mean is while for , the mean increased slightly to . Question : What is the limiting value of the mean? Is there a closed form for the mean of ?","n = p_1^{a_1}p_2^{a_2}\cdots p_m^{a_m} n f(n) = a_1 a_2 \cdots a_m \lim_{n \to \infty} \frac{1}{n}\sum_{k = 1}^n f(k) 
\bigg(2 - \frac{1}{\zeta(2)}\bigg)\bigg(N_c + \frac{1}{\zeta(2)} - \frac{2}{\zeta(3)}\bigg) - \frac{1}{\zeta(2)} + \frac{2}{\zeta(3)} \approx 1.95979
 N_c \approx 1.7052 n = 10^6 1.9406 n = 1.42 \times 10^{10} 1.94357 f(n)","['limits', 'number-theory', 'prime-numbers', 'divisibility', 'analytic-number-theory']"
80,"show there is no function $f:\mathbb{R}\to\mathbb{R}$ so that $f(0) > 0$ and $f(x+y)\ge f(x)+yf(f(x))\,\forall x,y\in\mathbb{R}$",show there is no function  so that  and,"f:\mathbb{R}\to\mathbb{R} f(0) > 0 f(x+y)\ge f(x)+yf(f(x))\,\forall x,y\in\mathbb{R}","Show there is no function $f:\mathbb{R}\to\mathbb{R}$ so that $f(0) > 0$ and $f(x+y)\ge f(x)+yf(f(x))\,\forall x,y\in\mathbb{R}.$ I think one can show that $f(f(z))$ is positive for some $z$ and that $f(x)\to\infty$ as $x\to \infty$ . Then one might be able to find some $a$ with $f(a) \ge a+1$ . To show $f(f(z))$ is positive for some z, suppose for all $z, f(f(z)) \leq 0.$ Then for all $y\le 0$ , $$f(x+y)\ge f(x) + yf(f(x)) \ge f(x)$$ implying that $f(x)$ is decreasing. If $f(f(x)) < 0$ for some $x$ , then $$f(x+y) \ge f(x) + yf(f(x))$$ for all $y\leq 0,$ so $f(z)$ tends to infinity as $z$ tends to $-\infty$ . Similarly, $f(z)$ tends to $-\infty$ as $z\to\infty.$ If $f(f(0)) = 0,$ then $f(y) \ge f(0) > 0$ for all $y.$ We know $f$ is decreasing so $f(y) \leq f(0)$ for $y\ge 0$ and hence for all $y\ge 0, f(y) = f(0).$ In particular, $f(f(0)) = f(0),$ so we get that for $y\in\mathbb{R}$ $$f(y) \ge f(0) + yf(0) = f(0)(1+y).$$ This contradicts the fact that $f(y) \leq f(0)$ for $y > 0$ . Hence we must have $f(f(0)) < 0$ by assumption. But I'm not sure how to continue from here.","Show there is no function so that and I think one can show that is positive for some and that as . Then one might be able to find some with . To show is positive for some z, suppose for all Then for all , implying that is decreasing. If for some , then for all so tends to infinity as tends to . Similarly, tends to as If then for all We know is decreasing so for and hence for all In particular, so we get that for This contradicts the fact that for . Hence we must have by assumption. But I'm not sure how to continue from here.","f:\mathbb{R}\to\mathbb{R} f(0) > 0 f(x+y)\ge f(x)+yf(f(x))\,\forall x,y\in\mathbb{R}. f(f(z)) z f(x)\to\infty x\to \infty a f(a) \ge a+1 f(f(z)) z, f(f(z)) \leq 0. y\le 0 f(x+y)\ge f(x) + yf(f(x)) \ge f(x) f(x) f(f(x)) < 0 x f(x+y) \ge f(x) + yf(f(x)) y\leq 0, f(z) z -\infty f(z) -\infty z\to\infty. f(f(0)) = 0, f(y) \ge f(0) > 0 y. f f(y) \leq f(0) y\ge 0 y\ge 0, f(y) = f(0). f(f(0)) = f(0), y\in\mathbb{R} f(y) \ge f(0) + yf(0) = f(0)(1+y). f(y) \leq f(0) y > 0 f(f(0)) < 0","['calculus', 'limits', 'inequality', 'contest-math', 'functional-equations']"
81,Limit question involving the the greatest integer function and fractional part,Limit question involving the the greatest integer function and fractional part,,"The value of $$\mathop {\lim }\limits_{n \to \infty } {\left( {\frac{{\sin \left\{ {\frac{2}{n}} \right\}}}{{\left[ {2n\tan \frac{1}{n}} \right]\left( {\tan \frac{1}{n}} \right)}} + \frac{1}{{{n^2} + \cos n}}} \right)^{{n^2}}},$$ where $[.]$ denotes greatest integer function and $\{.\}$ denotes fractional part function, is _________. My approach is as follow $\mathop {\lim }\limits_{n \to \infty } \sin \left\{ {\frac{2}{n}} \right\} = {0^ + };\mathop {\lim }\limits_{n \to \infty } \left[ {2n\tan \frac{1}{n}} \right] = \mathop {\lim }\limits_{n \to \infty } \left[ {2\frac{{\tan \frac{1}{n}}}{{\frac{1}{n}}}} \right] = \mathop {\lim }\limits_{n \to \infty } \left[ {{2^ + }} \right] = 2$ $\mathop {\lim }\limits_{n \to \infty } \tan \frac{1}{n} = 0;\mathop {\lim }\limits_{n \to \infty } \frac{1}{{{n^2} + \cos n}} = \mathop {\lim }\limits_{n \to \infty } \frac{1}{{n\left( {1 + \frac{{\cos n}}{n}} \right)}} = 0.$ Cannot approach from here.","The value of where denotes greatest integer function and denotes fractional part function, is _________. My approach is as follow Cannot approach from here.","\mathop {\lim }\limits_{n \to \infty } {\left( {\frac{{\sin \left\{ {\frac{2}{n}} \right\}}}{{\left[ {2n\tan \frac{1}{n}} \right]\left( {\tan \frac{1}{n}} \right)}} + \frac{1}{{{n^2} + \cos n}}} \right)^{{n^2}}}, [.] \{.\} \mathop {\lim }\limits_{n \to \infty } \sin \left\{ {\frac{2}{n}} \right\} = {0^ + };\mathop {\lim }\limits_{n \to \infty } \left[ {2n\tan \frac{1}{n}} \right] = \mathop {\lim }\limits_{n \to \infty } \left[ {2\frac{{\tan \frac{1}{n}}}{{\frac{1}{n}}}} \right] = \mathop {\lim }\limits_{n \to \infty } \left[ {{2^ + }} \right] = 2 \mathop {\lim }\limits_{n \to \infty } \tan \frac{1}{n} = 0;\mathop {\lim }\limits_{n \to \infty } \frac{1}{{{n^2} + \cos n}} = \mathop {\lim }\limits_{n \to \infty } \frac{1}{{n\left( {1 + \frac{{\cos n}}{n}} \right)}} = 0.","['real-analysis', 'calculus', 'limits']"
82,Prove or disprove: $\lim_{x\rightarrow \infty} g(x) = 0$.,Prove or disprove: .,\lim_{x\rightarrow \infty} g(x) = 0,I am trying to determine whether the following statement is true or false: Let $f(x)$ be an unbounded non decreasing function. Define $g(x) =  f(x) - \frac{f(x)}{\cos \left(\tfrac{1}{f(x)}\right)}$ . Prove or disprove: $\lim_{x\rightarrow \infty} g(x) = 0$ . After trying to find counter examples I believe the statement is true. I have tried to prove the statement using the squeeze theorem but it didn't got me much further. Any hints will be appericiated.,I am trying to determine whether the following statement is true or false: Let be an unbounded non decreasing function. Define . Prove or disprove: . After trying to find counter examples I believe the statement is true. I have tried to prove the statement using the squeeze theorem but it didn't got me much further. Any hints will be appericiated.,f(x) g(x) =  f(x) - \frac{f(x)}{\cos \left(\tfrac{1}{f(x)}\right)} \lim_{x\rightarrow \infty} g(x) = 0,"['real-analysis', 'calculus', 'limits']"
83,Asymptotic equivalence with the Notable Limit of Natural Logarithm,Asymptotic equivalence with the Notable Limit of Natural Logarithm,,"I hope this is not too easy of a question Basically I've been fiddling with Asymptotic Equivalences to solve limits in a faster and less prone to error way, and I am facing the limit: $$ \lim_{x\to0}{\frac{e^x-\ln(e+x)}{x^3-2x}} $$ The approach I wanted to use is that of substitution via Asymptotic Equivalence, aiming at substituting $e^x-1$ with $x$ and $\ln(e+x)$ . My idea was to add and subtract $1$ in the numerator in order to obtain the right situation to substitute $e^x-1$ with $x$ ; this decision probably isn't the best one to solve this limit in a definite manner since I'll remain with $\frac{1}{x^3-2x}$ that is of indefinite form but as I was trying out this path I started asking myself if it was possible to substitute $\ln{e+x}$ via Asymptotic Equivalences. I was thinking that maybe $$ \ln{\frac{1+f(x)}{f(x)}}=1\implies \ln{\frac{e+x}{x}}=e; $$ am I completely wrong? If not can one of you point me to a resource (be it online or a book) in which I can delve deeper into Notable Limits and Asymptotic Equivalences?","I hope this is not too easy of a question Basically I've been fiddling with Asymptotic Equivalences to solve limits in a faster and less prone to error way, and I am facing the limit: The approach I wanted to use is that of substitution via Asymptotic Equivalence, aiming at substituting with and . My idea was to add and subtract in the numerator in order to obtain the right situation to substitute with ; this decision probably isn't the best one to solve this limit in a definite manner since I'll remain with that is of indefinite form but as I was trying out this path I started asking myself if it was possible to substitute via Asymptotic Equivalences. I was thinking that maybe am I completely wrong? If not can one of you point me to a resource (be it online or a book) in which I can delve deeper into Notable Limits and Asymptotic Equivalences?","
\lim_{x\to0}{\frac{e^x-\ln(e+x)}{x^3-2x}}
 e^x-1 x \ln(e+x) 1 e^x-1 x \frac{1}{x^3-2x} \ln{e+x} 
\ln{\frac{1+f(x)}{f(x)}}=1\implies \ln{\frac{e+x}{x}}=e;
","['calculus', 'limits', 'asymptotics']"
84,"Showing $\lim\limits_{n\to\infty}\int_{[-M,M]}f(x)\sin(nx)\,\mathrm dx=0$ for any mixed $M$",Showing  for any mixed,"\lim\limits_{n\to\infty}\int_{[-M,M]}f(x)\sin(nx)\,\mathrm dx=0 M","The problem statement is as follows: Given $f\in L^{1}(\mathbb{R})$ , show that $$\lim_{n\to\infty}\int_{[-M,M]}f(x)\sin(nx)\,\mathrm dx=0$$ for any fixed $M$ . My approach is as follows: We have $\lim\limits_{n\to\infty}\int_{[-M,M]}\sin(nx)\,\mathrm dx=0$ . If $f$ is a simple function taking values $a_{1},\cdots,a_{n}$ , $$\lim_{n\to\infty}\int_{[-M,M]}f(x)\sin(nx)\,\mathrm dx = \lim_{n\to\infty}\sum_{i=1}^{n}a_{i}\int_{[-M,M]}\sin(nx)\,\mathrm dx\\ = \sum_{i=1}^{n}a_{i}\lim_{n\to\infty}\int_{[-M,M]}\sin(nx)\,\mathrm dx = 0.$$ My problem is when $f$ is a non-negative function. If $f\geq 0$ , then there exists a sequence of simple functions $f_{k}\nearrow f$ , then $f_{k}\sin(nx)\nearrow f\sin(nx)$ [n is fixed], then by Monotone convergence theorem, we have $$\lim_{n\to\infty}\int_{[-M,M]}f(x)\sin(nx)\,\mathrm dx = \lim_{n\to\infty}\int_{[-M,M]}\lim_{k\to\infty}f_{k}(x)\sin(nx)\,\mathrm dx\\ = \lim_{n\to\infty}\lim_{k\to\infty}\int_{[-M,M]}f_{k}(x)\sin(nx)\,\mathrm dx.$$ My question is: Can swap the two limits $\lim\limits_{n\to\infty}\lim\limits_{k\to\infty}\int_{[-M,M]}f_{k}(x)\sin(nx)\,\mathrm dx$ ? If I can't, then it seems like my approach doesn't work. What is wrong with my approach or are there other approaches ? I've seen proofs u\sing Fourier Transforms, but I've not progressed that far yet, so I'm seeking proofs u\sing only basic integral theorems. Any help is appreciated.","The problem statement is as follows: Given , show that for any fixed . My approach is as follows: We have . If is a simple function taking values , My problem is when is a non-negative function. If , then there exists a sequence of simple functions , then [n is fixed], then by Monotone convergence theorem, we have My question is: Can swap the two limits ? If I can't, then it seems like my approach doesn't work. What is wrong with my approach or are there other approaches ? I've seen proofs u\sing Fourier Transforms, but I've not progressed that far yet, so I'm seeking proofs u\sing only basic integral theorems. Any help is appreciated.","f\in L^{1}(\mathbb{R}) \lim_{n\to\infty}\int_{[-M,M]}f(x)\sin(nx)\,\mathrm dx=0 M \lim\limits_{n\to\infty}\int_{[-M,M]}\sin(nx)\,\mathrm dx=0 f a_{1},\cdots,a_{n} \lim_{n\to\infty}\int_{[-M,M]}f(x)\sin(nx)\,\mathrm dx = \lim_{n\to\infty}\sum_{i=1}^{n}a_{i}\int_{[-M,M]}\sin(nx)\,\mathrm dx\\
= \sum_{i=1}^{n}a_{i}\lim_{n\to\infty}\int_{[-M,M]}\sin(nx)\,\mathrm dx = 0. f f\geq 0 f_{k}\nearrow f f_{k}\sin(nx)\nearrow f\sin(nx) \lim_{n\to\infty}\int_{[-M,M]}f(x)\sin(nx)\,\mathrm dx = \lim_{n\to\infty}\int_{[-M,M]}\lim_{k\to\infty}f_{k}(x)\sin(nx)\,\mathrm dx\\
= \lim_{n\to\infty}\lim_{k\to\infty}\int_{[-M,M]}f_{k}(x)\sin(nx)\,\mathrm dx. \lim\limits_{n\to\infty}\lim\limits_{k\to\infty}\int_{[-M,M]}f_{k}(x)\sin(nx)\,\mathrm dx","['real-analysis', 'integration', 'limits']"
85,"Is it true that, if $f$ is uniformly continuous in $(a,b),$ then the limits $\lim_{x\to a^+} f(x)$ and $\lim_{x\to b^-} f(x)$ exist?","Is it true that, if  is uniformly continuous in  then the limits  and  exist?","f (a,b), \lim_{x\to a^+} f(x) \lim_{x\to b^-} f(x)","Let $f$ be a continuous function in $(a,b)$ . I know that: If the limits $\lim_{x\to a^+} f(x)$ and $\lim_{x\to b^-} f(x)$ exist, then $f$ is uniformly continuous in $(a,b)$ . Is the inverse also true?","Let be a continuous function in . I know that: If the limits and exist, then is uniformly continuous in . Is the inverse also true?","f (a,b) \lim_{x\to a^+} f(x) \lim_{x\to b^-} f(x) f (a,b)","['calculus', 'limits']"
86,Baby Rudin Theorem 8.2,Baby Rudin Theorem 8.2,,"This is Baby Rudin Theorem 8.2 proof: In the last part of the proof, I don't understand why the following should be true for proof to work: $x>1-\delta$ . Also, what does it have to do with $-1<x<1$ ? Any help is appreciated!","This is Baby Rudin Theorem 8.2 proof: In the last part of the proof, I don't understand why the following should be true for proof to work: . Also, what does it have to do with ? Any help is appreciated!",x>1-\delta -1<x<1,"['limits', 'power-series']"
87,Prove that a non-decreasing function has zero derivative,Prove that a non-decreasing function has zero derivative,,"Let $f : \mathbb{R} \to \mathbb{R}$ be a non-decreasing function (i.e., $x \leq y \implies f(x) \leq f(y)$ ). Suppose at a point $a \in \mathbb{R}$ , there exists a sequence of positive numbers $\{x_n\}$ such that $x_n \to 0$ and $$ \lim_{n\to \infty} \frac{f(a + x_n) - f(a - x_n)}{x_n} = 0. $$ I would like to prove (or find a counterexample) that $f$ is differentiable at $a$ and $f'(a) = 0$ . In particular, I'm trying to show that $$ \lim_{x \to 0} \frac{f(a + x) - f(a - x)}{x} = 0. $$ If I pick an arbitrary $x > 0$ so that $x_n \leq x < x_m$ for some $n, m \geq 1$ , I can get the bound $$ \frac{f(a + x) - f(a - x)}{x} \leq \frac{f(a + x_m) - f(a - x_m)}{x_n} \leq \frac{f(a + x_m) - f(a - x_m)}{x_m} \cdot \frac{x_m}{x_n}. $$ The problem is that $x_m / x_n$ may be large if, for instance, $\{x_n\}$ converges to $0$ too rapidly. Does anyone know how I might be able to proceed?","Let be a non-decreasing function (i.e., ). Suppose at a point , there exists a sequence of positive numbers such that and I would like to prove (or find a counterexample) that is differentiable at and . In particular, I'm trying to show that If I pick an arbitrary so that for some , I can get the bound The problem is that may be large if, for instance, converges to too rapidly. Does anyone know how I might be able to proceed?","f : \mathbb{R} \to \mathbb{R} x \leq y \implies f(x) \leq f(y) a \in \mathbb{R} \{x_n\} x_n \to 0 
\lim_{n\to \infty} \frac{f(a + x_n) - f(a - x_n)}{x_n} = 0.
 f a f'(a) = 0 
\lim_{x \to 0} \frac{f(a + x) - f(a - x)}{x} = 0.
 x > 0 x_n \leq x < x_m n, m \geq 1 
\frac{f(a + x) - f(a - x)}{x} \leq \frac{f(a + x_m) - f(a - x_m)}{x_n}
\leq \frac{f(a + x_m) - f(a - x_m)}{x_m} \cdot \frac{x_m}{x_n}.
 x_m / x_n \{x_n\} 0","['real-analysis', 'limits', 'derivatives']"
88,Applying am-gm on an infinite series expansion of $e^x$,Applying am-gm on an infinite series expansion of,e^x,"We know, $$ e^x-1 = \sum_{i=1}^{\infty} \frac{x^i}{i!}$$ Assume $x>0$ and use am-gm on the equation: $$ e^x -1\geq \lim_{n \to \infty}n\left( \frac{x^{ \frac{n(n+1)}{2} } }{\prod_{i=1}^n (i!)}\right)^{1/n}$$ What would be the lower bound on RHS for a given $x$ ? Would it be possible to evaluate the limit in terms of $n$ ? p.s: I came up with this myself, I am not sure how to start since we have product of factorial on the right side but for those who want to try my question, Stirling's approximation may be helpful","We know, Assume and use am-gm on the equation: What would be the lower bound on RHS for a given ? Would it be possible to evaluate the limit in terms of ? p.s: I came up with this myself, I am not sure how to start since we have product of factorial on the right side but for those who want to try my question, Stirling's approximation may be helpful", e^x-1 = \sum_{i=1}^{\infty} \frac{x^i}{i!} x>0  e^x -1\geq \lim_{n \to \infty}n\left( \frac{x^{ \frac{n(n+1)}{2} } }{\prod_{i=1}^n (i!)}\right)^{1/n} x n,"['limits', 'number-theory', 'taylor-expansion']"
89,Verification of solution for $\lim_{x\to 0^+} \frac{\ln(\sin(x/2))}{\ln(\sin x)}$,Verification of solution for,\lim_{x\to 0^+} \frac{\ln(\sin(x/2))}{\ln(\sin x)},Can you verify the solution to the following (very simple) problem? $$\lim_{x\to 0^+} \frac{\ln(\sin(x/2))}{\ln(\sin x)}.$$ $$\text{limit} = \lim_{x\to 0^+} \frac{\ln(\frac{\sin(x/2)}{x/2})+\ln(x/2)}{\ln(\sin x/x)+\ln(x)} = \lim_{x\to 0^+} \frac{\frac{\ln(\frac{\sin(x/2)}{x/2})}{\ln(x)}+\frac{\ln(x/2)}{\ln(x)}}  {\frac{\ln(\sin x/x)}{\ln(x)}+1} \\=  \lim_{x\to 0^+} \frac{\frac{\ln 1}{-\infty}+\frac{\ln(x/2)}{\ln (x)}}{\frac{\ln 1}{-\infty}+1} = \lim_{x\to 0^+} \frac{\ln(x/2)}{\ln(x)} = \lim_{x\to 0^+} \frac{\ln(x)-\ln 2}{\ln(x)}  =1. $$ What did I do wrong? The given answer is $2$ .,Can you verify the solution to the following (very simple) problem? What did I do wrong? The given answer is .,\lim_{x\to 0^+} \frac{\ln(\sin(x/2))}{\ln(\sin x)}. \text{limit} = \lim_{x\to 0^+} \frac{\ln(\frac{\sin(x/2)}{x/2})+\ln(x/2)}{\ln(\sin x/x)+\ln(x)} = \lim_{x\to 0^+} \frac{\frac{\ln(\frac{\sin(x/2)}{x/2})}{\ln(x)}+\frac{\ln(x/2)}{\ln(x)}}  {\frac{\ln(\sin x/x)}{\ln(x)}+1} \\=  \lim_{x\to 0^+} \frac{\frac{\ln 1}{-\infty}+\frac{\ln(x/2)}{\ln (x)}}{\frac{\ln 1}{-\infty}+1} = \lim_{x\to 0^+} \frac{\ln(x/2)}{\ln(x)} = \lim_{x\to 0^+} \frac{\ln(x)-\ln 2}{\ln(x)}  =1.  2,"['limits', 'solution-verification', 'limits-without-lhopital']"
90,Is $f(x)$ continuous at $x=0$? For $f(x) = \frac{x^3+x}{x}$.,Is  continuous at ? For .,f(x) x=0 f(x) = \frac{x^3+x}{x},"Here's my work flow. I've used this graph from Khan Academy, while it's indeterminate at first glance, it seems like we'd be able to find it's limit by factorising. I also graphed it to see that there's any break in the graph, but can't seem to find a hole/break in the graph that makes is ""not continuous"", I've thus assumed that it IS continuous. MY QUESTION: What did I do wrong and can you help me identify/bridge my knowledge gap please? Here's the original question, which doesn't even mention limit... : If I cheated and used Wolfram alpha using limits it shows there's a break in limit, how did they get the graph like that?","Here's my work flow. I've used this graph from Khan Academy, while it's indeterminate at first glance, it seems like we'd be able to find it's limit by factorising. I also graphed it to see that there's any break in the graph, but can't seem to find a hole/break in the graph that makes is ""not continuous"", I've thus assumed that it IS continuous. MY QUESTION: What did I do wrong and can you help me identify/bridge my knowledge gap please? Here's the original question, which doesn't even mention limit... : If I cheated and used Wolfram alpha using limits it shows there's a break in limit, how did they get the graph like that?",,"['calculus', 'limits']"
91,$\lim_{x \to 1} \frac{1}{x+2} = 1/3$ with $\varepsilon$-$\delta$ definition?,with - definition?,\lim_{x \to 1} \frac{1}{x+2} = 1/3 \varepsilon \delta,I'm trying to use the $\varepsilon$ - $\delta$ definition of a limit to prove that $$ \lim_{x \to 1} \frac{1}{x+2} = 1/3. $$ But I'm getting stuck on finding the correct $\delta$ . Here is my try: \begin{align*} \lvert f(x) - L \rvert < \varepsilon \\ \lvert  \frac{1}{x+2} - \frac{1}{3} \rvert < \varepsilon \\ \lvert  \frac{x -1}{2 + x} \rvert < 3\varepsilon. \end{align*} And then I'm not really sure what to do. How do you proceed from here?,I'm trying to use the - definition of a limit to prove that But I'm getting stuck on finding the correct . Here is my try: And then I'm not really sure what to do. How do you proceed from here?,"\varepsilon \delta 
\lim_{x \to 1} \frac{1}{x+2} = 1/3.
 \delta \begin{align*}
\lvert f(x) - L \rvert < \varepsilon \\
\lvert  \frac{1}{x+2} - \frac{1}{3} \rvert < \varepsilon \\
\lvert  \frac{x -1}{2 + x} \rvert < 3\varepsilon.
\end{align*}","['real-analysis', 'limits', 'epsilon-delta']"
92,"If $f \in C([0, \infty))$ and $\lim_{x\rightarrow\infty} f(x)\int_0^x f^2(t) \ dt = 1$, then $f(x) \sim \left(\frac{1}{3x}\right)^{1/3}$","If  and , then","f \in C([0, \infty)) \lim_{x\rightarrow\infty} f(x)\int_0^x f^2(t) \ dt = 1 f(x) \sim \left(\frac{1}{3x}\right)^{1/3}","Let $f:[0, \infty) \rightarrow \mathbb{R}$ be a continuous function such that $$ \lim_{x\rightarrow\infty} f(x)\int_0^x f^2(t) \ dt = 1 $$ I want to conclude that $$f(x) \sim \left(\frac{1}{3x}\right)^{1/3}$$ i.e. $$\lim_{x \rightarrow \infty} \frac{f(x)}{\left(\frac{1}{3x}\right)^{1/3}} = 1$$ I tried to argue by contradiction, but my attempt was terribly unsuccessful and now seems hopeless: Suppose that the conclusion is false. Then, for some $\varepsilon > 0$ we have that $$ \left|\frac{f(x)}{\left(\frac{1}{3x}\right)^{1/3}}-1\right|\ge \varepsilon $$ for arbitrarily large $x$ . Equivalently, $$ \frac{f(x)}{\left(\frac{1}{3x}\right)^{1/3}} \ge 1 + \varepsilon $$ and OR $$ \frac{f(x)}{\left(\frac{1}{3x}\right)^{1/3}} \le 1 - \varepsilon $$ Any ideas to get started would be appreciated.","Let be a continuous function such that I want to conclude that i.e. I tried to argue by contradiction, but my attempt was terribly unsuccessful and now seems hopeless: Suppose that the conclusion is false. Then, for some we have that for arbitrarily large . Equivalently, and OR Any ideas to get started would be appreciated.","f:[0, \infty) \rightarrow \mathbb{R} 
\lim_{x\rightarrow\infty} f(x)\int_0^x f^2(t) \ dt = 1
 f(x) \sim \left(\frac{1}{3x}\right)^{1/3} \lim_{x \rightarrow \infty} \frac{f(x)}{\left(\frac{1}{3x}\right)^{1/3}} = 1 \varepsilon > 0 
\left|\frac{f(x)}{\left(\frac{1}{3x}\right)^{1/3}}-1\right|\ge \varepsilon
 x 
\frac{f(x)}{\left(\frac{1}{3x}\right)^{1/3}} \ge 1 + \varepsilon
 
\frac{f(x)}{\left(\frac{1}{3x}\right)^{1/3}} \le 1 - \varepsilon
","['integration', 'limits', 'asymptotics', 'epsilon-delta']"
93,Does this weighted sum of Gaussians converge to $1/x^2$?,Does this weighted sum of Gaussians converge to ?,1/x^2,"By coincidence I discovered that a weighted sum of several Gaussians seems to converge to $1/x^2$ . Now I wonder whether that's a well-known property, just a coincidence or if this could be proven. This is a normal Gaussian with an additional weight factor $1/\sigma$ : $$ f(x, \sigma) = \frac{1}{\sigma} \cdot \frac{1}{\sigma \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{x}{\sigma}\right)^2} $$ If I now accumulate several of these weighted Gaussians with exponentially increasing $\sigma$ , I get a curve which closely resembles $1/x^2$ . The additional factor $\sqrt{3}$ was determined by trial and error and may not be the correct value. $$ g(x, n) = \sqrt{3} \cdot \sum_{k=0}^n f\left(x, \frac{2^k}{n}\right) $$ $$ \lim\limits_{n \to \infty} g(x, n) \stackrel{?}{=} \frac{1}{x^2} $$ Does anybody have an idea how this could be proven?","By coincidence I discovered that a weighted sum of several Gaussians seems to converge to . Now I wonder whether that's a well-known property, just a coincidence or if this could be proven. This is a normal Gaussian with an additional weight factor : If I now accumulate several of these weighted Gaussians with exponentially increasing , I get a curve which closely resembles . The additional factor was determined by trial and error and may not be the correct value. Does anybody have an idea how this could be proven?","1/x^2 1/\sigma 
f(x, \sigma) = \frac{1}{\sigma} \cdot \frac{1}{\sigma \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{x}{\sigma}\right)^2}
 \sigma 1/x^2 \sqrt{3} 
g(x, n) = \sqrt{3} \cdot \sum_{k=0}^n f\left(x, \frac{2^k}{n}\right)
 
\lim\limits_{n \to \infty} g(x, n) \stackrel{?}{=} \frac{1}{x^2}
","['limits', 'convergence-divergence', 'gaussian']"
94,show that $\lim_{n\to+\infty}\frac{1}{n}\sum_{k=0}^{n}f(x_{k})$ is exists,show that  is exists,\lim_{n\to+\infty}\frac{1}{n}\sum_{k=0}^{n}f(x_{k}),"let $f:[0,1]\to R^{+}$ real-valued continuous functions,and such $$\int_{0}^{1}f(x)dx=2019,~~\int_{0}^{1}f^2(x)dx=20181027$$ (1):show that:there exists unique sequence $x_{0},x_{1},\cdots,x_{n}\in [0,1]$ ,such $x_{0}<x_{1}<\cdots<x_{n}$ and for any postive integer $k=1,2,\cdots,n$ .such $$\int_{x_{k-1}}^{x_{k}}f(t)dt=\dfrac{1}{n}\int_{0}^{1}f(t)dt$$ (2):and show that $$\lim_{n\to+\infty}\dfrac{1}{n}\sum_{k=0}^{n}f(x_{k})$$ is exists.and find this value. I can do it $(1)$ exists.I think it is First mean value theorem for integration。but How to this sequence is unique. How to solve this (2),if this limt has exists.it seem use Stolz-Cesaro's Lemma: $$\lim_{n\to+\infty}\dfrac{1}{n}\sum_{k=0}^{n}f(x_{k})=\lim_{n\to\infty}f(x_{n})$$ this problem is my teacher gave me the exercise, these two problems I can not solve all , so I want to ask the teacher here,Thanks","let real-valued continuous functions,and such (1):show that:there exists unique sequence ,such and for any postive integer .such (2):and show that is exists.and find this value. I can do it exists.I think it is First mean value theorem for integration。but How to this sequence is unique. How to solve this (2),if this limt has exists.it seem use Stolz-Cesaro's Lemma: this problem is my teacher gave me the exercise, these two problems I can not solve all , so I want to ask the teacher here,Thanks","f:[0,1]\to R^{+} \int_{0}^{1}f(x)dx=2019,~~\int_{0}^{1}f^2(x)dx=20181027 x_{0},x_{1},\cdots,x_{n}\in [0,1] x_{0}<x_{1}<\cdots<x_{n} k=1,2,\cdots,n \int_{x_{k-1}}^{x_{k}}f(t)dt=\dfrac{1}{n}\int_{0}^{1}f(t)dt \lim_{n\to+\infty}\dfrac{1}{n}\sum_{k=0}^{n}f(x_{k}) (1) \lim_{n\to+\infty}\dfrac{1}{n}\sum_{k=0}^{n}f(x_{k})=\lim_{n\to\infty}f(x_{n})","['integration', 'limits']"
95,Some calculus Prove/Disprove question,Some calculus Prove/Disprove question,,"I have 2 questions that I would like to make sure I did the first one right, and get some tips for the next one :) Prove or disprove: Let $f: \mathbb{R} \to \mathbb{R}$ be a differentiable function. If the equation $f'(x)=0$ has exactly one solution, then the equation $f(x)=0$ has at least two solutions. I think this one is false, because I can take $f(x)=x^2$ . It is clear that the function is differentiable, and $f'(x)=2x$ , so the equation $f'(x)=0$ has only one solution as needed, when $x=0$ . And the equation $f(x)=0$ has only one solution, when $x=0$ , which is in contradiction to the fact that $f(x)=0$ has at least two solutions. -- Let $f:[a,b) \to \mathbb{R}$ be a differentiable function such that $\lim_{x \to b^-}f(x)$ does not exist in the extended sence. Then  there exists an $x_{0} \in [a,b)$ for which $f'(x_{0})=0$ . So I have tried to think for functions that I know which for their limits does not exist, like $\cos (\frac{1}{x})$ , and started playing with them, but no matter what I did, $f'$ always had a point $x_{0}$ which for $f'(x_{0})=0$ . So I think that this statement is true, but if you guys can please enlighten me, I will very appreciate that! Thanks a lot!","I have 2 questions that I would like to make sure I did the first one right, and get some tips for the next one :) Prove or disprove: Let be a differentiable function. If the equation has exactly one solution, then the equation has at least two solutions. I think this one is false, because I can take . It is clear that the function is differentiable, and , so the equation has only one solution as needed, when . And the equation has only one solution, when , which is in contradiction to the fact that has at least two solutions. -- Let be a differentiable function such that does not exist in the extended sence. Then  there exists an for which . So I have tried to think for functions that I know which for their limits does not exist, like , and started playing with them, but no matter what I did, always had a point which for . So I think that this statement is true, but if you guys can please enlighten me, I will very appreciate that! Thanks a lot!","f: \mathbb{R} \to \mathbb{R} f'(x)=0 f(x)=0 f(x)=x^2 f'(x)=2x f'(x)=0 x=0 f(x)=0 x=0 f(x)=0 f:[a,b) \to \mathbb{R} \lim_{x \to b^-}f(x) x_{0} \in [a,b) f'(x_{0})=0 \cos (\frac{1}{x}) f' x_{0} f'(x_{0})=0","['calculus', 'limits', 'functions', 'derivatives', 'solution-verification']"
96,"Help with finding a function, $\lim \ne \inf$","Help with finding a function,",\lim \ne \inf,"I am looking for an example for a function where: $$\lim\limits_{x \to a^+} f(x) \neq \inf\, \{f(x) \mid a\leq x \leq b\},\quad a<b,$$ $$f:[a,b]\to\mathbb{R}\;\;\text{ and }\;f\;\text{ is an increasing function.}$$ Can you guys give me a hint of the way of thinking to find such a function? (Notice that $x \to a^{+}$ ) The function must be an increasing function. Thanks!",I am looking for an example for a function where: Can you guys give me a hint of the way of thinking to find such a function? (Notice that ) The function must be an increasing function. Thanks!,"\lim\limits_{x \to a^+} f(x) \neq \inf\, \{f(x) \mid a\leq x \leq b\},\quad a<b, f:[a,b]\to\mathbb{R}\;\;\text{ and }\;f\;\text{ is an increasing function.} x \to a^{+}","['real-analysis', 'calculus']"
97,little-oh notation,little-oh notation,,"Assume I have a function $ f:\mathbb{R}^{m}\to\mathbb{R}^n $ , and let $ h\in \mathbb{R}^m $ . Does the notation $ f\left(x_0+h\right)=o\left(h\right) $ (for fixed $x_0 $ ) mean that $$ \lim_{h\to0}\frac{||f\left(x_{0}+h\right)||_{\mathbb{R}^{n}}}{||h||_{\mathbb{R}^{m}}}=0 ?$$ Because I've seen in a few places that lecturers wrote just $ \lim_{h\to0}\frac{f\left(x_{0}+h\right)}{||h||}=0 $ , which doesn't make any sense to me. I'll be glad for a clarification. What's the acceptable definition worldwide? Thanks in advance","Assume I have a function , and let . Does the notation (for fixed ) mean that Because I've seen in a few places that lecturers wrote just , which doesn't make any sense to me. I'll be glad for a clarification. What's the acceptable definition worldwide? Thanks in advance", f:\mathbb{R}^{m}\to\mathbb{R}^n   h\in \mathbb{R}^m   f\left(x_0+h\right)=o\left(h\right)  x_0   \lim_{h\to0}\frac{||f\left(x_{0}+h\right)||_{\mathbb{R}^{n}}}{||h||_{\mathbb{R}^{m}}}=0 ?  \lim_{h\to0}\frac{f\left(x_{0}+h\right)}{||h||}=0 ,"['limits', 'multivariable-calculus', 'asymptotics', 'definition']"
98,Why doesn't $f'(x) \to 0$ at $x \to \infty$ imply that $\lim_{x \to \infty}{f(x)}$ exists?,Why doesn't  at  imply that  exists?,f'(x) \to 0 x \to \infty \lim_{x \to \infty}{f(x)},"I know there are several examples where $f'(x) \to 0$ at $x \to \infty$ , but $f(x)$ is unbounded, like $f(x) = \ln (x)$ , but I cannot grasp this logically. When the slope of a function is tending to zero, doesn't this mean the function itself is approaching a constant value?","I know there are several examples where at , but is unbounded, like , but I cannot grasp this logically. When the slope of a function is tending to zero, doesn't this mean the function itself is approaching a constant value?",f'(x) \to 0 x \to \infty f(x) f(x) = \ln (x),"['limits', 'functions', 'derivatives']"
99,Trouble with the proof of Cauchy convergence criterion,Trouble with the proof of Cauchy convergence criterion,,"While reading the Analysis 1 textbook by Vladimir A. Zorich I encountered a proof which has this one conclusion I fail to understand. The theorem and proof : (Cauchy’s convergence criterion) A numerical sequence converges if and only if it is a Cauchy sequence. Proof. $\implies$ :(I skipped this part of the proof since I have no issues with it.) $\impliedby$ : Let ${x_k}$ be a fundamental sequence. Given $\epsilon > 0$ , we find an index $N$ such that $|x_m − x_k| < \frac{\epsilon}{3}$ when $m ≥ N$ and $k ≥ N$ . Fixing $m = N$ , we find that for any $k >N$ $$x_N − \frac{\epsilon}{3}<x_k <x_N + \frac{\epsilon}{3}\ \text{,} \ \ \ \ \ \text{(3.1)}$$ but since only a finite number of terms of the sequence have indices not larger than $N$ , we have shown that a fundamental sequence is bounded. $$\text{For}\ n \in \mathbb{N}\ \text{we now set } a_n := \inf_{k≥n} x_k ,\ \text{and }\ b_n := \sup_{k≥n} x_k \ \text{.}$$ It is clear from these definitions that $a_n ≤ a_{n+1} ≤ b_{n+1} ≤ b_n$ (since the greatest lower bound does not decrease and the least upper bound does not increase when we pass to a smaller set). By the nested interval principle, there is a point A common to all of the closed intervals $[a_n, b_n]$ . Since $$a_n ≤ A ≤ b_n$$ for any $n \in \mathbb{N}$ and $$a_n = \inf_{k≥n} x_k ≤ x_k ≤ \sup_{k≥n} x_k = b_n$$ for $k ≥ n$ , it follows that $$|A − x_k| ≤ b_n − a_n\ \text{.}\ \ \ \ \ \text{(3.2)}$$ But it follows from Eq. $\text{(3.1)}$ that $$\underbrace{x_N − \frac{\epsilon}{3}≤ \inf_{k≥n} x_k = a_n ≤ b_n = \sup_{k≥n} x_k ≤ x_N + \frac{\epsilon}{3}}_{\text{The problematic part}}$$ for $n>N$ , and therefore $$b_n − a_n ≤ \frac{2\epsilon}{3} < \epsilon \ \ \ \ \ \text{(3.3)}$$ for $n>m$ . Comparing Eqs. $\text{(3.2)}$ and $\text{(3.3)}$ , we find that $|A −x_k| < \epsilon$ , for any $k > N$ , and we have proved that $\lim_{k \to \infty}x_k = A$ . End of proof. The underbraced part doesn't make sense to me, because from the stated it could happen that $$x_N − \frac{\epsilon}{3} = a_n $$ and since $a_n≤x_k$ it is possible that $a_n=x_k$ and if those equalities hold, then $x_N − \frac{\epsilon}{3} = a_n=x_k$ , but this contradicts what was stated before in $\text{(3.1)}$ , $x_N − \frac{\epsilon}{3}<x_k $ . Why does the problematic part hold despite this ? Thanks","While reading the Analysis 1 textbook by Vladimir A. Zorich I encountered a proof which has this one conclusion I fail to understand. The theorem and proof : (Cauchy’s convergence criterion) A numerical sequence converges if and only if it is a Cauchy sequence. Proof. :(I skipped this part of the proof since I have no issues with it.) : Let be a fundamental sequence. Given , we find an index such that when and . Fixing , we find that for any but since only a finite number of terms of the sequence have indices not larger than , we have shown that a fundamental sequence is bounded. It is clear from these definitions that (since the greatest lower bound does not decrease and the least upper bound does not increase when we pass to a smaller set). By the nested interval principle, there is a point A common to all of the closed intervals . Since for any and for , it follows that But it follows from Eq. that for , and therefore for . Comparing Eqs. and , we find that , for any , and we have proved that . End of proof. The underbraced part doesn't make sense to me, because from the stated it could happen that and since it is possible that and if those equalities hold, then , but this contradicts what was stated before in , . Why does the problematic part hold despite this ? Thanks","\implies \impliedby {x_k} \epsilon > 0 N |x_m − x_k| < \frac{\epsilon}{3} m ≥ N k ≥ N m = N k >N x_N − \frac{\epsilon}{3}<x_k <x_N + \frac{\epsilon}{3}\ \text{,} \ \ \ \ \ \text{(3.1)} N \text{For}\ n \in \mathbb{N}\ \text{we now set } a_n := \inf_{k≥n} x_k ,\ \text{and }\ b_n := \sup_{k≥n} x_k \ \text{.} a_n ≤ a_{n+1} ≤ b_{n+1} ≤ b_n [a_n, b_n] a_n ≤ A ≤ b_n n \in \mathbb{N} a_n = \inf_{k≥n} x_k ≤ x_k ≤ \sup_{k≥n} x_k = b_n k ≥ n |A − x_k| ≤ b_n − a_n\ \text{.}\ \ \ \ \ \text{(3.2)} \text{(3.1)} \underbrace{x_N − \frac{\epsilon}{3}≤ \inf_{k≥n} x_k = a_n ≤ b_n = \sup_{k≥n} x_k ≤ x_N + \frac{\epsilon}{3}}_{\text{The problematic part}} n>N b_n − a_n ≤ \frac{2\epsilon}{3} < \epsilon \ \ \ \ \ \text{(3.3)} n>m \text{(3.2)} \text{(3.3)} |A −x_k| < \epsilon k > N \lim_{k \to \infty}x_k = A x_N − \frac{\epsilon}{3} = a_n  a_n≤x_k a_n=x_k x_N − \frac{\epsilon}{3} = a_n=x_k \text{(3.1)} x_N − \frac{\epsilon}{3}<x_k ","['real-analysis', 'limits', 'proof-writing', 'proof-explanation', 'cauchy-sequences']"
