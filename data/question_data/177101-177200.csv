,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Difficult Calculus(?) problem,Difficult Calculus(?) problem,,"For the function $f(x)$, find the constant n where $f(x)$ has a maximum at $(n,1)$$$f(x)=\frac{x^{n-x}}{(n-x)!}$$ It is roughly $0.561459...$, but this is through numerical guess and check work. I'm fairly confident that the solution will be some infinite series that involves the Euler-Mascheroni constant and the digamma function, along with the Reimann-Zeta function, but I haven't been able to work anything out that leads to any kind of evidence to suggest it even has a way to express $n$ as anything more than a numerical value. Some of you may be wondering how I know that it can even have a maximum at $(n,1)$. you'll notice that when $n=x$, $f(x)=1$ and I'm not sure how to describe this so just play with the value of $n$ on https://www.desmos.com/calculator/jb8x3n0cya and you'll see how the graph works. The values that intercept with $y=1$ are what I've found to be important. P.S. Should $f(x)$ be $f(x,n)$? I'm really not sure.","For the function $f(x)$, find the constant n where $f(x)$ has a maximum at $(n,1)$$$f(x)=\frac{x^{n-x}}{(n-x)!}$$ It is roughly $0.561459...$, but this is through numerical guess and check work. I'm fairly confident that the solution will be some infinite series that involves the Euler-Mascheroni constant and the digamma function, along with the Reimann-Zeta function, but I haven't been able to work anything out that leads to any kind of evidence to suggest it even has a way to express $n$ as anything more than a numerical value. Some of you may be wondering how I know that it can even have a maximum at $(n,1)$. you'll notice that when $n=x$, $f(x)=1$ and I'm not sure how to describe this so just play with the value of $n$ on https://www.desmos.com/calculator/jb8x3n0cya and you'll see how the graph works. The values that intercept with $y=1$ are what I've found to be important. P.S. Should $f(x)$ be $f(x,n)$? I'm really not sure.",,['multivariable-calculus']
1,A continuous a.e orientation-preserving isometry is locally injective?,A continuous a.e orientation-preserving isometry is locally injective?,,"Let $M,N$ be $d$-dimensional Riemannian manifolds. Let $f:M \to N$, and suppose $f$ is continuous, differentiable almost everywhere (a.e ) and that $df$ is an orientation-preserving isometry a.e. Question: Is it true that there exist a ball $B_{\epsilon}(p) \subseteq M$ such that $f|_{B_{\epsilon}(p)}$ is injective?","Let $M,N$ be $d$-dimensional Riemannian manifolds. Let $f:M \to N$, and suppose $f$ is continuous, differentiable almost everywhere (a.e ) and that $df$ is an orientation-preserving isometry a.e. Question: Is it true that there exist a ball $B_{\epsilon}(p) \subseteq M$ such that $f|_{B_{\epsilon}(p)}$ is injective?",,"['multivariable-calculus', 'riemannian-geometry', 'inverse-function', 'isometry']"
2,Implicit Function Theorem and Change of Coordinates(?),Implicit Function Theorem and Change of Coordinates(?),,"Suppose $\rho:\mathbb{R^n} \to \mathbb{R}$ is $C^k$ for some $k \geq 1$ and consider the level set $\rho \equiv 0$.  Suppose that $\rho(x_0) = 0$ and $\frac{\partial \rho}{\partial x_n}(x_0) \neq 0$.  By the implicit function theorem, there is a $C^k$ function $R$ so that in a neighborhood of the first $n-1$ coordinates of $x_0$, $R(x_1, ..., x_{n-1}) = x_n$ and $\rho(x_1, ..., x_{n-1}, R(x_1, ..., x_{n-1})) = 0$. I have seen multiple times in textbooks that we can assume without loss of generality that $\frac{\partial R}{\partial x_j}(x_0) = 0$ for $j = 1, 2, ... n-1$.  Why is that?  I believe this is some sort of change of variables/rotation of the level set, but I have never seen the details provided.  Can someone explain this? $\mathbf{Update:}$ Here is an example of where I have seen this construction ""Represent $\partial \Omega$ near 0 in the form $\partial \Omega =\{x=(x_1,...,x_4)|\text{ } x_4=R(x′) \text{ for } x′=(x_1,x_2,x_3) \in V′\}$ with a $C^\infty$ function $R$ on a neighborhood $V′$ of $0 \in \mathbb{R^3}$ such that $R(0)=0, R_i(0)=0, i=1,2,3$."" I have been told that when we do this construction that we can make $\nabla \rho(0) = (0, 0, 0, 1)$ where $\rho$ is a $C^\infty$ function with its zero set equal to $\partial \Omega$ and $\frac{\partial \rho}{\partial x_n} \neq 0$.  Is this true?","Suppose $\rho:\mathbb{R^n} \to \mathbb{R}$ is $C^k$ for some $k \geq 1$ and consider the level set $\rho \equiv 0$.  Suppose that $\rho(x_0) = 0$ and $\frac{\partial \rho}{\partial x_n}(x_0) \neq 0$.  By the implicit function theorem, there is a $C^k$ function $R$ so that in a neighborhood of the first $n-1$ coordinates of $x_0$, $R(x_1, ..., x_{n-1}) = x_n$ and $\rho(x_1, ..., x_{n-1}, R(x_1, ..., x_{n-1})) = 0$. I have seen multiple times in textbooks that we can assume without loss of generality that $\frac{\partial R}{\partial x_j}(x_0) = 0$ for $j = 1, 2, ... n-1$.  Why is that?  I believe this is some sort of change of variables/rotation of the level set, but I have never seen the details provided.  Can someone explain this? $\mathbf{Update:}$ Here is an example of where I have seen this construction ""Represent $\partial \Omega$ near 0 in the form $\partial \Omega =\{x=(x_1,...,x_4)|\text{ } x_4=R(x′) \text{ for } x′=(x_1,x_2,x_3) \in V′\}$ with a $C^\infty$ function $R$ on a neighborhood $V′$ of $0 \in \mathbb{R^3}$ such that $R(0)=0, R_i(0)=0, i=1,2,3$."" I have been told that when we do this construction that we can make $\nabla \rho(0) = (0, 0, 0, 1)$ where $\rho$ is a $C^\infty$ function with its zero set equal to $\partial \Omega$ and $\frac{\partial \rho}{\partial x_n} \neq 0$.  Is this true?",,"['multivariable-calculus', 'differential-geometry', 'differential-topology', 'implicit-function-theorem', 'change-of-variable']"
3,How do we see if the following are conservative fields?,How do we see if the following are conservative fields?,,"We know that line integrals of conservative vector fields are independent of path . let F be the field and r(t) be the paramatrization of the curve, if $\int_CF dr=0$ for every closed path in D, then F is conservative. For the first image, a closed circle at center is an obvious counterexample, so the field is not conservative. For the second image, it is conservative. However, I found that it is difficult to see that there's no conunterexample. For instance, if i pick a circle in first quadrant, how do you tell that   $\int_CF dr=0$ for sure?","We know that line integrals of conservative vector fields are independent of path . let F be the field and r(t) be the paramatrization of the curve, if $\int_CF dr=0$ for every closed path in D, then F is conservative. For the first image, a closed circle at center is an obvious counterexample, so the field is not conservative. For the second image, it is conservative. However, I found that it is difficult to see that there's no conunterexample. For instance, if i pick a circle in first quadrant, how do you tell that   $\int_CF dr=0$ for sure?",,"['multivariable-calculus', 'curves', 'vector-fields']"
4,"Find the minimum value of $f$ on $\{(a,b,c)\in \mathbb{R^3}\mid a+b+c=1\}$ where $f(a,b,c)=\int_{0}^1 (a+bx+cx^2)^2dx$.",Find the minimum value of  on  where .,"f \{(a,b,c)\in \mathbb{R^3}\mid a+b+c=1\} f(a,b,c)=\int_{0}^1 (a+bx+cx^2)^2dx","Find the minimum value of $f$ on $\{(a,b,c)\in \mathbb{R^3}\mid a+b+c=1\}$ where $f(a,b,c)=\int_0^1 (a+bx+cx^2)^2 \, dx$. What I did is  $$ \int_0^1 (a+bx+cx^2)^2 \, dx=a^2+a \left( b+\frac{2c}{3} \right) + \frac{b^2}{3} + \frac{bc}{2}+\frac{c^2}{5}. $$ Then we consider the function $F(a,b,c)=a^2+a(b+\frac{2c}{3})+\frac{b^2}{3} + \frac{bc}{2} + \frac{c^2}{5}-\lambda(a+b+c-1)$. Thus, we can consider $\nabla F=0$ with $a+b+c=1$ and try to find $a,b$ and $c$. However, since the function and the constraint seem to be related, I am curious whether there is some other way to see the minimum without computing Lagrange multiplier. Thank you.","Find the minimum value of $f$ on $\{(a,b,c)\in \mathbb{R^3}\mid a+b+c=1\}$ where $f(a,b,c)=\int_0^1 (a+bx+cx^2)^2 \, dx$. What I did is  $$ \int_0^1 (a+bx+cx^2)^2 \, dx=a^2+a \left( b+\frac{2c}{3} \right) + \frac{b^2}{3} + \frac{bc}{2}+\frac{c^2}{5}. $$ Then we consider the function $F(a,b,c)=a^2+a(b+\frac{2c}{3})+\frac{b^2}{3} + \frac{bc}{2} + \frac{c^2}{5}-\lambda(a+b+c-1)$. Thus, we can consider $\nabla F=0$ with $a+b+c=1$ and try to find $a,b$ and $c$. However, since the function and the constraint seem to be related, I am curious whether there is some other way to see the minimum without computing Lagrange multiplier. Thank you.",,"['real-analysis', 'multivariable-calculus', 'optimization']"
5,"$f(x,y)=(e^x \cos(y), e^x\sin(y))$ is one-to-one proof. And onto?",is one-to-one proof. And onto?,"f(x,y)=(e^x \cos(y), e^x\sin(y))","Let $A:=\mathbb{R} \times (0,2\pi)$. Show that the function $f:\mathbb{R}^2 \rightarrow \mathbb{R}^2$ defined by $f(x,y)=(e^x \cos(y), e^x\sin(y))$ is one-to-one on $A$. Is it onto? My attempt (one-to-one) Let $f(x_1,y_1)=f(x_2,y_2)$. Then $$e^{x_1}\cos y_1=e^{x_2}\cos y_2$$ and $$e^{x_1}\sin y_1=e^{x_2} \sin y_2$$ Squaring both and then adding we get: $$e^{2x_1}\cos^2y_1+e^{2x_1}\sin^2 y_1=e^{2x_2}\cos^2y_2+e^{2x_2}\sin^2 y_2$$ $$=e^{2x_1}(\cos^2y_1+\sin^2y_1)=e^{2x_2}(\cos^2y_2+\sin^2y_2)$$ $$e^{2x_1}(1)=e^{2x_2}(1)$$ $$x_1=x_2$$ Not sure how to get $y_1=y_2$ here...Also not sure how to intuitively know if this is onto (and provide such proof). Thanks for the help!","Let $A:=\mathbb{R} \times (0,2\pi)$. Show that the function $f:\mathbb{R}^2 \rightarrow \mathbb{R}^2$ defined by $f(x,y)=(e^x \cos(y), e^x\sin(y))$ is one-to-one on $A$. Is it onto? My attempt (one-to-one) Let $f(x_1,y_1)=f(x_2,y_2)$. Then $$e^{x_1}\cos y_1=e^{x_2}\cos y_2$$ and $$e^{x_1}\sin y_1=e^{x_2} \sin y_2$$ Squaring both and then adding we get: $$e^{2x_1}\cos^2y_1+e^{2x_1}\sin^2 y_1=e^{2x_2}\cos^2y_2+e^{2x_2}\sin^2 y_2$$ $$=e^{2x_1}(\cos^2y_1+\sin^2y_1)=e^{2x_2}(\cos^2y_2+\sin^2y_2)$$ $$e^{2x_1}(1)=e^{2x_2}(1)$$ $$x_1=x_2$$ Not sure how to get $y_1=y_2$ here...Also not sure how to intuitively know if this is onto (and provide such proof). Thanks for the help!",,"['real-analysis', 'multivariable-calculus']"
6,Is there a way to find the bounds of integration for triple integrals without drawing pictures (analytically)?,Is there a way to find the bounds of integration for triple integrals without drawing pictures (analytically)?,,"I'm able to solve integrals and draw graphs in 2D, but I have a lot of trouble drawing 3D graphs. As such, I'm seeking a method of finding the bounds of integration for triple integrals without having to graph them. It seems reasonable to me that there is such a method, since we also need a way to solve higher dimensional integrals (4D, 5D, 6D, etc), without the ability to graph them. I found this (relevant) question from 3 years ago, but it was left unanswered: Is there a way to find limits of integration numerically for triple integrals? I would greatly appreciate if someone could please explain or direct me to a method for solving triple integrals (finding their bounds of integration) without having to graph them.","I'm able to solve integrals and draw graphs in 2D, but I have a lot of trouble drawing 3D graphs. As such, I'm seeking a method of finding the bounds of integration for triple integrals without having to graph them. It seems reasonable to me that there is such a method, since we also need a way to solve higher dimensional integrals (4D, 5D, 6D, etc), without the ability to graph them. I found this (relevant) question from 3 years ago, but it was left unanswered: Is there a way to find limits of integration numerically for triple integrals? I would greatly appreciate if someone could please explain or direct me to a method for solving triple integrals (finding their bounds of integration) without having to graph them.",,"['real-analysis', 'integration', 'multivariable-calculus', 'definite-integrals']"
7,Metric on sphere.,Metric on sphere.,,"In the book titled  Space, time, and gravity by Robert Wald, I found the following, and I am quoting him, ""the metric of the sphere is $ds^2=R^2[(d\psi)^2+cos^2\psi(d\phi)^2].$ 1-where $ds$ denotes the distance between two nearby points , $d\psi$ their difference in latitude, $d\phi$ their difference in longitude, and $R$ the radius of the sphere."" 2- Is the above formula correct? I think cosine should be replaced to sine. 3- also, is the exponent 2 on $ds$  square?, or just the symmetric product $dsds$? since I am confused why he squared the other two quantities i,e. $(d\theta)^2$ why not just write the metric the normal way. 4- and is there a way to explain to a laymans person what's the two here $ds^2$ without confusing them that it's a square? 5- if I give you for example that between tow points the difference in longitude and in latitude is $\pi$, and radius is 3, then the distance between the two points is $18\pi^2$ since by these information we get $ds^2= 9[\pi^2+( cos^2(\pi) ) \pi^2]=18\pi^2$?. thank you.","In the book titled  Space, time, and gravity by Robert Wald, I found the following, and I am quoting him, ""the metric of the sphere is $ds^2=R^2[(d\psi)^2+cos^2\psi(d\phi)^2].$ 1-where $ds$ denotes the distance between two nearby points , $d\psi$ their difference in latitude, $d\phi$ their difference in longitude, and $R$ the radius of the sphere."" 2- Is the above formula correct? I think cosine should be replaced to sine. 3- also, is the exponent 2 on $ds$  square?, or just the symmetric product $dsds$? since I am confused why he squared the other two quantities i,e. $(d\theta)^2$ why not just write the metric the normal way. 4- and is there a way to explain to a laymans person what's the two here $ds^2$ without confusing them that it's a square? 5- if I give you for example that between tow points the difference in longitude and in latitude is $\pi$, and radius is 3, then the distance between the two points is $18\pi^2$ since by these information we get $ds^2= 9[\pi^2+( cos^2(\pi) ) \pi^2]=18\pi^2$?. thank you.",,"['multivariable-calculus', 'differential-geometry']"
8,"Show that if $f: \mathbb{R}^2 \to \mathbb{R}$ is differentiable, then so is $f^2$","Show that if  is differentiable, then so is",f: \mathbb{R}^2 \to \mathbb{R} f^2,"Show that if $f: \mathbb{R}^2 \to \mathbb{R}$ is differentiable, then so is $f^2$. I was not able to get too far: $f$ is differentiable: $\Delta f(x,y) = f_x(x,y) h + f_y(x,y) k + \epsilon(h,k) \sqrt{h^2+k^2}$, such that $\lim\limits_{(h,k)\to(0,0)} \epsilon(h,k)=0$ (Notation: $\Delta f=f(x+h,y+k)-f(x,y)$). $f^2$ is differentiable if $\exists A,B \in \mathbb R$ such that $\Delta f^2 = A h + B k + \alpha \sqrt{h^2+k^2}$. I need to show that $\lim\limits_{(h,k)\to(0,0)}\alpha(h,k) = 0$. $$\alpha = \frac{\Delta f^2 -A h-B k}{\sqrt{h^2+k^2}}$$ Hint on how to continue? I tried to set $A=2f_x \cdot f ,~B=2 f_y \cdot f$ and write $\Delta f^2 = \Delta f \cdot \left[f(x+h,y+k)+f(x,y)\right]$, but that didn't help so much either.","Show that if $f: \mathbb{R}^2 \to \mathbb{R}$ is differentiable, then so is $f^2$. I was not able to get too far: $f$ is differentiable: $\Delta f(x,y) = f_x(x,y) h + f_y(x,y) k + \epsilon(h,k) \sqrt{h^2+k^2}$, such that $\lim\limits_{(h,k)\to(0,0)} \epsilon(h,k)=0$ (Notation: $\Delta f=f(x+h,y+k)-f(x,y)$). $f^2$ is differentiable if $\exists A,B \in \mathbb R$ such that $\Delta f^2 = A h + B k + \alpha \sqrt{h^2+k^2}$. I need to show that $\lim\limits_{(h,k)\to(0,0)}\alpha(h,k) = 0$. $$\alpha = \frac{\Delta f^2 -A h-B k}{\sqrt{h^2+k^2}}$$ Hint on how to continue? I tried to set $A=2f_x \cdot f ,~B=2 f_y \cdot f$ and write $\Delta f^2 = \Delta f \cdot \left[f(x+h,y+k)+f(x,y)\right]$, but that didn't help so much either.",,"['multivariable-calculus', 'derivatives']"
9,Limits of Integration Over Iterated Region,Limits of Integration Over Iterated Region,,"For $\epsilon>0$ and $y\gg\epsilon$ I have an integral on the region $$P(n,y)=\left\{(x_1,\ldots,x_n)\in\mathbb{R}^n\;:\; \sum_k^nx_k\geq y, \;x_i\geq \epsilon, \forall i \right\}.$$     I want to explicitly write down the limits of integration when the integral is broken down. For example, for $n=1,2$     $$\int_{P(1,y)}f(x)d(x)=\int_y^{\infty}f(x)dx$$     $$\int_{P(2,y)}f(x)d(x) = \int_{\epsilon}^{y-\epsilon}\int_{y-x_2}^{\infty}f(x_1,x_2)dx_1dx_2+ \int_{y-\epsilon}^{\infty}\int_{\epsilon}^{\infty}f(x_1,x_2)dx_1dx_2$$ Up to $n=3$ it is easy to visualize. The region $P(3,y)$ is just getting everything except a small ``triangle"" near the origin. In the lower level, each slice is simply the region $P(2,\cdot)$, but after the plane $x_1+x_2+x_3=\epsilon$ intersects the $x_3$-axis you just get a giant cube. With this, I have written the regions for $P(3,y)$ as          $$\int_{P_3}dx_1dx_2dx_3=\int_{\epsilon}^{y-2\epsilon}\int_{P(2,\epsilon-u_3)}dx_1dx_2dx_3+\int_{y-2\epsilon}\int_{\epsilon}^{\infty}\int_{\epsilon}^{\infty}dx_1dx_2dx_3.$$     My goal is to write out the integral without any $P_n$'s. For $n=3$, the reasoning above leads to the explicit limits     $$\int_{P_3}=\int_{\epsilon}^{y-2\epsilon}\int_{\epsilon}^{y-x_3-\epsilon}\int_{y-x_1-x_2}^\infty+\int_{\epsilon}^{y-2\epsilon}\int^{\infty}_{y-x_3-\epsilon}\int_{\epsilon}^\infty+\int^{\infty}_{y-2\epsilon}\int_{\epsilon}^{\infty}\int_{\epsilon}^\infty$$ For general $n\geq 2$, I am looking for something like         $$\int_{P_n}dx_1\ldots dx_n=\int_{y-(n-1\epsilon)}^\infty\underbrace{\int_\epsilon^\infty \cdots\int_\epsilon^\infty}_{n\text{ times}}dx_1\ldots dx_n + \int_{\epsilon}^{y-(n-1)\epsilon}``(n-1)\text{ integrals""}dx_1\ldots dx_n$$ Any help finding the $n-1$ ``integrals?"" Thanks, UPDATE: I think I found something much simpler. You can write $$\int_{x_1,\ldots,x_n\geq\epsilon}dx_1\ldots dx_n=\int_{P(n,y)}dx_1\ldots dx_n+\int_{D(n,y)}dx_1\ldots dx_n,$$ with $D(n,y)=\left\{(x_1,\ldots,x_n)\in\mathbb{R}^n\;:\; \sum_k^nx_k< y, \;x_i\geq \epsilon, \forall i \right\}$. Note the region $D$ is the ``triangle"" part I was talking about. The limits over that region for general $n$ are  $$\int_{D(n,y)}dx_1\ldots dx_n=\int_{\epsilon}^{y-(n-1)\epsilon}\int_{\epsilon}^{y-(n-2)\epsilon-x_{n-1}}\int_{\epsilon}^{y-(n-3)\epsilon-x_{n-1}-x_{n-2}}\cdots\int_{\epsilon}^{y-\sum_{i=1}^nx_i}dx_1\ldots dx_n$$ Use these two results to rewrite the integral over $P(n,y)$.","For $\epsilon>0$ and $y\gg\epsilon$ I have an integral on the region $$P(n,y)=\left\{(x_1,\ldots,x_n)\in\mathbb{R}^n\;:\; \sum_k^nx_k\geq y, \;x_i\geq \epsilon, \forall i \right\}.$$     I want to explicitly write down the limits of integration when the integral is broken down. For example, for $n=1,2$     $$\int_{P(1,y)}f(x)d(x)=\int_y^{\infty}f(x)dx$$     $$\int_{P(2,y)}f(x)d(x) = \int_{\epsilon}^{y-\epsilon}\int_{y-x_2}^{\infty}f(x_1,x_2)dx_1dx_2+ \int_{y-\epsilon}^{\infty}\int_{\epsilon}^{\infty}f(x_1,x_2)dx_1dx_2$$ Up to $n=3$ it is easy to visualize. The region $P(3,y)$ is just getting everything except a small ``triangle"" near the origin. In the lower level, each slice is simply the region $P(2,\cdot)$, but after the plane $x_1+x_2+x_3=\epsilon$ intersects the $x_3$-axis you just get a giant cube. With this, I have written the regions for $P(3,y)$ as          $$\int_{P_3}dx_1dx_2dx_3=\int_{\epsilon}^{y-2\epsilon}\int_{P(2,\epsilon-u_3)}dx_1dx_2dx_3+\int_{y-2\epsilon}\int_{\epsilon}^{\infty}\int_{\epsilon}^{\infty}dx_1dx_2dx_3.$$     My goal is to write out the integral without any $P_n$'s. For $n=3$, the reasoning above leads to the explicit limits     $$\int_{P_3}=\int_{\epsilon}^{y-2\epsilon}\int_{\epsilon}^{y-x_3-\epsilon}\int_{y-x_1-x_2}^\infty+\int_{\epsilon}^{y-2\epsilon}\int^{\infty}_{y-x_3-\epsilon}\int_{\epsilon}^\infty+\int^{\infty}_{y-2\epsilon}\int_{\epsilon}^{\infty}\int_{\epsilon}^\infty$$ For general $n\geq 2$, I am looking for something like         $$\int_{P_n}dx_1\ldots dx_n=\int_{y-(n-1\epsilon)}^\infty\underbrace{\int_\epsilon^\infty \cdots\int_\epsilon^\infty}_{n\text{ times}}dx_1\ldots dx_n + \int_{\epsilon}^{y-(n-1)\epsilon}``(n-1)\text{ integrals""}dx_1\ldots dx_n$$ Any help finding the $n-1$ ``integrals?"" Thanks, UPDATE: I think I found something much simpler. You can write $$\int_{x_1,\ldots,x_n\geq\epsilon}dx_1\ldots dx_n=\int_{P(n,y)}dx_1\ldots dx_n+\int_{D(n,y)}dx_1\ldots dx_n,$$ with $D(n,y)=\left\{(x_1,\ldots,x_n)\in\mathbb{R}^n\;:\; \sum_k^nx_k< y, \;x_i\geq \epsilon, \forall i \right\}$. Note the region $D$ is the ``triangle"" part I was talking about. The limits over that region for general $n$ are  $$\int_{D(n,y)}dx_1\ldots dx_n=\int_{\epsilon}^{y-(n-1)\epsilon}\int_{\epsilon}^{y-(n-2)\epsilon-x_{n-1}}\int_{\epsilon}^{y-(n-3)\epsilon-x_{n-1}-x_{n-2}}\cdots\int_{\epsilon}^{y-\sum_{i=1}^nx_i}dx_1\ldots dx_n$$ Use these two results to rewrite the integral over $P(n,y)$.",,"['integration', 'multivariable-calculus', 'definite-integrals', 'iterated-integrals']"
10,Meaning of this line integral,Meaning of this line integral,,"I have seen expressions for 4 different line integrals: $\int f\text{d}s $ $\int f\text{d}\textbf{s} $ $\int \textbf{F} \boldsymbol{\cdot} \text{d}\textbf{s}$ $\int \textbf{F} \times \text{d} \textbf{s}$ These give results that are scalar, vector, scalar, vector respetively. Now the second integral I have only come across in one book, and it was only mentioned as a type of line integral. However I am not sure what it means. The first integral I understand as the area of the 'curtain' formed by the scalar field over a path; the third I understand as a summing of the components of the vector field that are parallel to the tangent to the line scaled by the magnitude of the length of the vector elements (which is one for parameterisation by arc length); the final one is the summing of the perpendicular components of the vector field instead. However what does it mean to have a scalar field integrated over a vector?","I have seen expressions for 4 different line integrals: $\int f\text{d}s $ $\int f\text{d}\textbf{s} $ $\int \textbf{F} \boldsymbol{\cdot} \text{d}\textbf{s}$ $\int \textbf{F} \times \text{d} \textbf{s}$ These give results that are scalar, vector, scalar, vector respetively. Now the second integral I have only come across in one book, and it was only mentioned as a type of line integral. However I am not sure what it means. The first integral I understand as the area of the 'curtain' formed by the scalar field over a path; the third I understand as a summing of the components of the vector field that are parallel to the tangent to the line scaled by the magnitude of the length of the vector elements (which is one for parameterisation by arc length); the final one is the summing of the perpendicular components of the vector field instead. However what does it mean to have a scalar field integrated over a vector?",,"['multivariable-calculus', 'vector-fields', 'line-integrals', 'tangent-line', 'scalar-fields']"
11,"Finding maxima and minima of $f(x,y)= x^4 + y^4 - 2x^2 - 2y^2 + 4xy$",Finding maxima and minima of,"f(x,y)= x^4 + y^4 - 2x^2 - 2y^2 + 4xy","For $f(x,y)=x^4+y^4-2x^2-2y^2+4xy$, I need to find maxima or minima. There are three critical points: $(0,0),(\sqrt2, -\sqrt2),(-\sqrt2,\sqrt2)$ So at $(\sqrt2, -\sqrt2)$, $f$ has minimum value, $-8$ and at $(-\sqrt2,\sqrt2),$ it has same minimum value, $-8$. At $(0,0)$, after inspecting, I get that it has neither maxima or minima. But when we substitute $(0,0),$ we get $0\le f \le8,$ so should not we get $(0,0)$ as a point of maxima?","For $f(x,y)=x^4+y^4-2x^2-2y^2+4xy$, I need to find maxima or minima. There are three critical points: $(0,0),(\sqrt2, -\sqrt2),(-\sqrt2,\sqrt2)$ So at $(\sqrt2, -\sqrt2)$, $f$ has minimum value, $-8$ and at $(-\sqrt2,\sqrt2),$ it has same minimum value, $-8$. At $(0,0)$, after inspecting, I get that it has neither maxima or minima. But when we substitute $(0,0),$ we get $0\le f \le8,$ so should not we get $(0,0)$ as a point of maxima?",,"['multivariable-calculus', 'partial-derivative', 'maxima-minima', 'a.m.-g.m.-inequality', 'cauchy-schwarz-inequality']"
12,"Consider a function $f(x, y) = Ax^5 + Bx^4y + Cx^3y^2 + Dx^2y^3 + xy^4 − y^5$",Consider a function,"f(x, y) = Ax^5 + Bx^4y + Cx^3y^2 + Dx^2y^3 + xy^4 − y^5","Consider a function $f(x, y) = Ax^5 + Bx^4y + Cx^3y^2 + Dx^2y^3 + xy^4 − y^5$ where $A$ , $B$ , $C$ , $D$ are unspecified real numbers. Determine the values of $A$ , $B$ , $C$ , $D$ such that $f(x,y)$ satisfies $f_{xx}(x,y) + f_{yy}(x,y) = 0$ What I did so far, I took double derivative of x and y: $f_{xx}(x,y) = 20Ax^3+12Bx^2y+6Cxy^2+2Dy^3$ $f_{yy}(x,y) = 2Cx^3+6Dx^2y+12xy^2-20y^3$ How can I satisfies $f_{xx}(x,y) + f_{yy}(x,y) = 0$ ? There are no terms that cancels, subtracts or add.","Consider a function where , , , are unspecified real numbers. Determine the values of , , , such that satisfies What I did so far, I took double derivative of x and y: How can I satisfies ? There are no terms that cancels, subtracts or add.","f(x, y) = Ax^5 + Bx^4y + Cx^3y^2 + Dx^2y^3 + xy^4 − y^5 A B C D A B C D f(x,y) f_{xx}(x,y) + f_{yy}(x,y) = 0 f_{xx}(x,y) = 20Ax^3+12Bx^2y+6Cxy^2+2Dy^3 f_{yy}(x,y) = 2Cx^3+6Dx^2y+12xy^2-20y^3 f_{xx}(x,y) + f_{yy}(x,y) = 0","['calculus', 'multivariable-calculus']"
13,Higher-order partial derivatives for functions defined $\mathbb{R}^n \rightarrow \mathbb{R}^m$.,Higher-order partial derivatives for functions defined .,\mathbb{R}^n \rightarrow \mathbb{R}^m,"So far I've only seen examples of high-order (ie: second, third) partial derivatives of functions $f:\mathbb{R^n} \to \mathbb{R}$, but not any with $f:\mathbb{R^n} \to \mathbb{R^m}$. Is there a reason for this? Is this because the definition of a partial derivative is a function $f: \mathbb{R^n} \to \mathbb{R}$? Merry Christmas!","So far I've only seen examples of high-order (ie: second, third) partial derivatives of functions $f:\mathbb{R^n} \to \mathbb{R}$, but not any with $f:\mathbb{R^n} \to \mathbb{R^m}$. Is there a reason for this? Is this because the definition of a partial derivative is a function $f: \mathbb{R^n} \to \mathbb{R}$? Merry Christmas!",,['multivariable-calculus']
14,can continuously differentiability deduced from continuous partial differentiable?,can continuously differentiability deduced from continuous partial differentiable?,,"If not, what is the sufficient and necessary condition of continuously differentiability? I have searched a lot, but I can not find a detailed reference discussing continuously differentiability.","If not, what is the sufficient and necessary condition of continuously differentiability? I have searched a lot, but I can not find a detailed reference discussing continuously differentiability.",,"['real-analysis', 'multivariable-calculus', 'reference-request']"
15,"Suppose $f$ satisfies $f_x(x,y) = f_y(x,y) = 0$. Then $f$ is constant.",Suppose  satisfies . Then  is constant.,"f f_x(x,y) = f_y(x,y) = 0 f","Suppose $f$ is a function with $f_x(x,y)=f_y(x,y)=0$ for all $(x,y)$. Then show that $f(x,y)=c$, a constant. I had the following method in mind. Fix $h,k$ to be any particular point in the $X-Y$ plane. Then, since partial derivatives exist, by the second FTC, $$f(h,k)-f(h,0)=\int_0^k f_y(h,y) dy = 0 \implies f(h,k) = f(h,0)$$ and $$f(h,0) - f(0,0) = \int_0^h f_x(x,0) dx = 0 \implies f(h,0) = f(0,0)$$ $$\implies f(x,y)=f(0,0)=c$$ What is wrong in this proof? I haven't assumed $f$ is differentiable either (though it is since the partial derivatives are continuous)","Suppose $f$ is a function with $f_x(x,y)=f_y(x,y)=0$ for all $(x,y)$. Then show that $f(x,y)=c$, a constant. I had the following method in mind. Fix $h,k$ to be any particular point in the $X-Y$ plane. Then, since partial derivatives exist, by the second FTC, $$f(h,k)-f(h,0)=\int_0^k f_y(h,y) dy = 0 \implies f(h,k) = f(h,0)$$ and $$f(h,0) - f(0,0) = \int_0^h f_x(x,0) dx = 0 \implies f(h,0) = f(0,0)$$ $$\implies f(x,y)=f(0,0)=c$$ What is wrong in this proof? I haven't assumed $f$ is differentiable either (though it is since the partial derivatives are continuous)",,"['calculus', 'multivariable-calculus', 'proof-verification', 'definite-integrals', 'partial-derivative']"
16,Partial derivatives of implicit functions,Partial derivatives of implicit functions,,"So, I have an implicit expression for a change of variables from Cartesian to a different system using $u$ and $v$. This is given by: $G(x,y,z,u,v)=0$, $F(x,y,z,u,v)=0$, where $F$, $G$, $u=f(x,y,z)$ and $v=g(x,y,z)$ are all differentiable. Differentiating F, say, we get: $\frac{dF}{dx} = \frac{\partial{F}}{\partial{x}} + \frac{\partial{F}}{\partial{u}} \frac{\partial{u}}{\partial{x}} + \frac{\partial{F}}{\partial{v}}\frac{\partial{v}}{\partial{x}}$ My questions: Is this because you have $\frac{\partial{x}}{\partial{x}} = 1$, and  $\frac{\partial{y}}{\partial{x}} = \frac{\partial{z}}{\partial{x}} = 0$? This seems like an obvious question but I don't really understand where the $ \frac{\partial{F}}{\partial{x}}$ term is coming from, or why is just appears here. For example if we had $f(x,y,z)=f(x(t), y(t), z(t))$, if we wanted to differentiate that with respect to t we wouldn't have an $ \frac{\partial{f}}{\partial{t}}$ term.","So, I have an implicit expression for a change of variables from Cartesian to a different system using $u$ and $v$. This is given by: $G(x,y,z,u,v)=0$, $F(x,y,z,u,v)=0$, where $F$, $G$, $u=f(x,y,z)$ and $v=g(x,y,z)$ are all differentiable. Differentiating F, say, we get: $\frac{dF}{dx} = \frac{\partial{F}}{\partial{x}} + \frac{\partial{F}}{\partial{u}} \frac{\partial{u}}{\partial{x}} + \frac{\partial{F}}{\partial{v}}\frac{\partial{v}}{\partial{x}}$ My questions: Is this because you have $\frac{\partial{x}}{\partial{x}} = 1$, and  $\frac{\partial{y}}{\partial{x}} = \frac{\partial{z}}{\partial{x}} = 0$? This seems like an obvious question but I don't really understand where the $ \frac{\partial{F}}{\partial{x}}$ term is coming from, or why is just appears here. For example if we had $f(x,y,z)=f(x(t), y(t), z(t))$, if we wanted to differentiate that with respect to t we wouldn't have an $ \frac{\partial{f}}{\partial{t}}$ term.",,"['multivariable-calculus', 'partial-derivative']"
17,"Two spheres, triple integration, not their intersection","Two spheres, triple integration, not their intersection",,"Two spheres, one of radius $1$ and one of radius $\sqrt{2}$, have centres that are $1$ unit apart. Find the volume of the smaller region that is outside one sphere and inside the other. Can use either spherical or cylindrical coordinates. The $2$ spheres can be anywhere in three dimensional space. Apparently using the correct coordinates will lead into an easy integration. Been tossing around the two and haven't seen any of them working out to be easy.","Two spheres, one of radius $1$ and one of radius $\sqrt{2}$, have centres that are $1$ unit apart. Find the volume of the smaller region that is outside one sphere and inside the other. Can use either spherical or cylindrical coordinates. The $2$ spheres can be anywhere in three dimensional space. Apparently using the correct coordinates will lead into an easy integration. Been tossing around the two and haven't seen any of them working out to be easy.",,"['calculus', 'integration', 'multivariable-calculus', 'spheres']"
18,Obtain different answers if we change the order of integration,Obtain different answers if we change the order of integration,,"In Wolfram alpha, it gives us $$\int_0^1 \int_0^1 \frac{x-y}{(x+y)^3} dydx=\frac{1}{2}$$ $$\int_0^1 \int_0^1 \frac{x-y}{(x+y)^3} dxdy=-\frac{1}{2}$$ Why do we have two different answers if we perform the order of integration differently? I tried to look up the Fubini's theorem condition, but to no avail. Because the integral above satisfies Fubini's theorem. Did I miss up something?","In Wolfram alpha, it gives us $$\int_0^1 \int_0^1 \frac{x-y}{(x+y)^3} dydx=\frac{1}{2}$$ $$\int_0^1 \int_0^1 \frac{x-y}{(x+y)^3} dxdy=-\frac{1}{2}$$ Why do we have two different answers if we perform the order of integration differently? I tried to look up the Fubini's theorem condition, but to no avail. Because the integral above satisfies Fubini's theorem. Did I miss up something?",,"['multivariable-calculus', 'multiple-integral']"
19,Closest distance from point to surface is normal proof,Closest distance from point to surface is normal proof,,"If I have a point and a surface, the vector for the closest distance between the point and the surface will be normal to the surface: Intuitively it makes sense. If we assume the contrary (that the closest distance vector is not normal to the surface) then we can minimize the distance by travelling in the direction that makes the distance vector normal. But how would I prove this formally? And does this theorem hold for any type of surface?","If I have a point and a surface, the vector for the closest distance between the point and the surface will be normal to the surface: Intuitively it makes sense. If we assume the contrary (that the closest distance vector is not normal to the surface) then we can minimize the distance by travelling in the direction that makes the distance vector normal. But how would I prove this formally? And does this theorem hold for any type of surface?",,['multivariable-calculus']
20,Disrupt a linear one to one map $L_0$ gives a linear one to one map $L_t$,Disrupt a linear one to one map  gives a linear one to one map,L_0 L_t,"I have a continuous function $f$ from $[0,1]\to\mathcal{L}(\Bbb{R}^n,\Bbb{R}^p)$ such that $f(t)=L_t.$ Assume that $L_0$ is one ton one, I would like to prove that $L_t$ is one to one for $t$ sufficiently small. Set $\varepsilon>0$ there exists $\gamma>0$ such that for any $t\in[0,\gamma)$ we have $$\Vert L_t-L_0\Vert<\varepsilon$$ Now for any $x\in \Bbb{R}^n$ I have $\Vert (L_t-L_0)(x)\Vert\le \Vert L_t-L_0\Vert  \Vert x\Vert$ In fact, I am not sure how can I use the linearity here. If $x\ne0$ I can choose $\varepsilon=\frac{1}{\Vert x\Vert}$ , I will have $\Vert L_t(x)-L_0(x) \Vert<1$ for $t\in [0,\gamma)$ , not sure that help. Geometrically I cannot see why $L_t$ will be one to one, I can imagine that $L_t(x)$ is equal to $L_0(t)+\vec{v}$ and the norm of linear map can be visualize as the taking the biggest vector on the unit sphere. Question: Can someone give a nice view of why is going to be one to one ?","I have a continuous function from such that Assume that is one ton one, I would like to prove that is one to one for sufficiently small. Set there exists such that for any we have Now for any I have In fact, I am not sure how can I use the linearity here. If I can choose , I will have for , not sure that help. Geometrically I cannot see why will be one to one, I can imagine that is equal to and the norm of linear map can be visualize as the taking the biggest vector on the unit sphere. Question: Can someone give a nice view of why is going to be one to one ?","f [0,1]\to\mathcal{L}(\Bbb{R}^n,\Bbb{R}^p) f(t)=L_t. L_0 L_t t \varepsilon>0 \gamma>0 t\in[0,\gamma) \Vert L_t-L_0\Vert<\varepsilon x\in \Bbb{R}^n \Vert (L_t-L_0)(x)\Vert\le \Vert L_t-L_0\Vert  \Vert x\Vert x\ne0 \varepsilon=\frac{1}{\Vert x\Vert} \Vert L_t(x)-L_0(x) \Vert<1 t\in [0,\gamma) L_t L_t(x) L_0(t)+\vec{v}","['multivariable-calculus', 'linear-transformations']"
21,Solve $\iint_D\sqrt{9-x^2-y^2}$ Where $D$ is the positive side of a circle of radius 3,Solve  Where  is the positive side of a circle of radius 3,\iint_D\sqrt{9-x^2-y^2} D,"Solve $\displaystyle\iint_D\sqrt{9-x^2-y^2}$ Where $D$ is the positive   side of a circle of radius 3 ($x^2+y^2=9,x\ge0,y\ge0$) I tried to subsitute variables to $r$ & $\theta$: $$x = r\cos\theta$$ $$y = r\sin\theta$$ $$E = \{0\le r\le3,0\le\theta\le\pi\}$$ $$\displaystyle\iint_D\sqrt{9-x^2-y^2} = \displaystyle\iint_EJ\sqrt{9-(r\cos\theta)^2-(r\sin\theta)^2}=\displaystyle\iint_Er\sqrt{9-(r\cos\theta)^2-(r\sin\theta)^2}$$ But have no clue on how to solve this new integral.","Solve $\displaystyle\iint_D\sqrt{9-x^2-y^2}$ Where $D$ is the positive   side of a circle of radius 3 ($x^2+y^2=9,x\ge0,y\ge0$) I tried to subsitute variables to $r$ & $\theta$: $$x = r\cos\theta$$ $$y = r\sin\theta$$ $$E = \{0\le r\le3,0\le\theta\le\pi\}$$ $$\displaystyle\iint_D\sqrt{9-x^2-y^2} = \displaystyle\iint_EJ\sqrt{9-(r\cos\theta)^2-(r\sin\theta)^2}=\displaystyle\iint_Er\sqrt{9-(r\cos\theta)^2-(r\sin\theta)^2}$$ But have no clue on how to solve this new integral.",,"['integration', 'multivariable-calculus', 'substitution']"
22,"Is the vector $(2,2)$ perpendicular to the level curve 2 of $f(x,y)=4-x^{2}-y^{2}$?",Is the vector  perpendicular to the level curve 2 of ?,"(2,2) f(x,y)=4-x^{2}-y^{2}","Problem:  If $f(x,y)=4-x^{2}-y^{2}$, then the vector $(2,2)$ is orthogonal to the level curve 2 of $f(x,y)$ in the point $(1,1)$. Solution : $z=4-x^{2}-y^{2}$ $=\left \langle z=2 \right \rangle$ $2=4-x^{2}-y^{2}$ $=\left \langle \text{Subtract}\ -4\right \rangle$ $-2=-x^{2}-y^{2}$ $=\left \langle \text{Multiply by}\ -1\ \right \rangle$ $2=x^{2}+y^{2}$ $=\left \langle \text{Divide by}\ 2\right \rangle$ $1=\frac{x^{2}}{2}+\frac{y^{2}}{2}$ So $a=b=\sqrt{2}\approx1.4$ Then I plotted the level curve as follows: Graphically we see that the vector $(2,2)$ is orthogonal to the level curve of 2 at point $(1,1)$. But... how do I make this analytically?","Problem:  If $f(x,y)=4-x^{2}-y^{2}$, then the vector $(2,2)$ is orthogonal to the level curve 2 of $f(x,y)$ in the point $(1,1)$. Solution : $z=4-x^{2}-y^{2}$ $=\left \langle z=2 \right \rangle$ $2=4-x^{2}-y^{2}$ $=\left \langle \text{Subtract}\ -4\right \rangle$ $-2=-x^{2}-y^{2}$ $=\left \langle \text{Multiply by}\ -1\ \right \rangle$ $2=x^{2}+y^{2}$ $=\left \langle \text{Divide by}\ 2\right \rangle$ $1=\frac{x^{2}}{2}+\frac{y^{2}}{2}$ So $a=b=\sqrt{2}\approx1.4$ Then I plotted the level curve as follows: Graphically we see that the vector $(2,2)$ is orthogonal to the level curve of 2 at point $(1,1)$. But... how do I make this analytically?",,['multivariable-calculus']
23,"Calculate: $\iint_{D}\cos\left(\frac{x+2y}{-x+y}\right)dx\,dy$",Calculate:,"\iint_{D}\cos\left(\frac{x+2y}{-x+y}\right)dx\,dy","Let D be the triangle region in $\mathbb{R}^2$ having vertices $(0,0), (-2,4), (-3,3).$ Calculate: $$\iint_{D}\cos\left(\frac{x+2y}{-x+y}\right)dx\,dy$$ I use change variable $x+2y=u$ , $-x+y=v$ . Finally, it leads to calculate: $\int\limits_0^6\int\limits_{\frac1{3}u}^{\frac2{3}u}\frac13\cos(\dfrac{u}{v})dudv$ . I cannot finish with this final step. Could you please help me?","Let D be the triangle region in having vertices Calculate: I use change variable , . Finally, it leads to calculate: . I cannot finish with this final step. Could you please help me?","\mathbb{R}^2 (0,0), (-2,4), (-3,3). \iint_{D}\cos\left(\frac{x+2y}{-x+y}\right)dx\,dy x+2y=u -x+y=v \int\limits_0^6\int\limits_{\frac1{3}u}^{\frac2{3}u}\frac13\cos(\dfrac{u}{v})dudv","['calculus', 'integration', 'multivariable-calculus', 'multiple-integral', 'change-of-variable']"
24,A kind of non-Abelian shift operation,A kind of non-Abelian shift operation,,"I'd like to define (and calculate the properties of) an object like $e^{\boldsymbol{A\cdot\nabla}}f\left(\boldsymbol{x}\right)$, where $A$ is a collection of matrices written in a vector form, say $\boldsymbol{A\cdot\nabla}=A_{x}\partial_{x}+A_{y}\partial_{y}$ for instance (I think that's the simplest non-trivial example), in which case $f\left(\boldsymbol{x}\right)\equiv f\left(x,y\right)$. The difficulty is that $\left[A_{x},A_{y}\right]\neq0$. $f$ has all the properties one needs (smoothness and co ; it comes from a problem of physics), and it commutes with the $A$'s. The $A$'s do not depend on the coordinates (in the example, this property would read $\partial_{x}A_{x,y}=\partial_{y}A_{x,y}=0$) Does this object make any sense ? Does it have a name ? For instance, I'd like to show something like  $$e^{\boldsymbol{A\cdot\nabla}}f\left(\boldsymbol{x}\right)e^{-\boldsymbol{A\cdot\nabla}}g\left(\boldsymbol{x}\right)=f\left(\boldsymbol{x}+\boldsymbol{A}\right)g\left(\boldsymbol{x}\right)$$ but this is obviously wrong by direct expansion (it is true when e.g. $A_{y}=0$ in the example above). At second order there are some commutators $\left[A_{i},A_{j}\right]$ appearing. Any help is warm welcome. (I'm not even sure the tags below are well chosen :-(","I'd like to define (and calculate the properties of) an object like $e^{\boldsymbol{A\cdot\nabla}}f\left(\boldsymbol{x}\right)$, where $A$ is a collection of matrices written in a vector form, say $\boldsymbol{A\cdot\nabla}=A_{x}\partial_{x}+A_{y}\partial_{y}$ for instance (I think that's the simplest non-trivial example), in which case $f\left(\boldsymbol{x}\right)\equiv f\left(x,y\right)$. The difficulty is that $\left[A_{x},A_{y}\right]\neq0$. $f$ has all the properties one needs (smoothness and co ; it comes from a problem of physics), and it commutes with the $A$'s. The $A$'s do not depend on the coordinates (in the example, this property would read $\partial_{x}A_{x,y}=\partial_{y}A_{x,y}=0$) Does this object make any sense ? Does it have a name ? For instance, I'd like to show something like  $$e^{\boldsymbol{A\cdot\nabla}}f\left(\boldsymbol{x}\right)e^{-\boldsymbol{A\cdot\nabla}}g\left(\boldsymbol{x}\right)=f\left(\boldsymbol{x}+\boldsymbol{A}\right)g\left(\boldsymbol{x}\right)$$ but this is obviously wrong by direct expansion (it is true when e.g. $A_{y}=0$ in the example above). At second order there are some commutators $\left[A_{i},A_{j}\right]$ appearing. Any help is warm welcome. (I'm not even sure the tags below are well chosen :-(",,"['linear-algebra', 'multivariable-calculus', 'partial-differential-equations', 'operator-theory', 'mathematical-physics']"
25,Why is multivariable continuous differentiability defined in terms of partial derivatives?,Why is multivariable continuous differentiability defined in terms of partial derivatives?,,"Both in my textbook and on Wikipedia, continuous differentiability of a function $f:\Bbb R^m \to \Bbb R^n$ is defined by the existence and continuity of all of the partial derivatives.  Since there is a notion of a (total) derivative (AKA differential) for multivariable functions, I'm wondering why continuous differentiability is not defined as existence and continuity of the derivative map $Df(a)$?  Is there some reason why having existence and continuity of partials is more convenient or maybe continuity of the total derivative is too strict of a condition?","Both in my textbook and on Wikipedia, continuous differentiability of a function $f:\Bbb R^m \to \Bbb R^n$ is defined by the existence and continuity of all of the partial derivatives.  Since there is a notion of a (total) derivative (AKA differential) for multivariable functions, I'm wondering why continuous differentiability is not defined as existence and continuity of the derivative map $Df(a)$?  Is there some reason why having existence and continuity of partials is more convenient or maybe continuity of the total derivative is too strict of a condition?",,"['real-analysis', 'multivariable-calculus', 'derivatives']"
26,Does symmetry of second derivatives implies continuity?,Does symmetry of second derivatives implies continuity?,,"I'm trying to learn calculus of several variables, and well there's a theorem which says that if all partials up to the second order are continuous then $\frac{\partial f(x,y)}{\partial x\partial y}=\frac{\partial f(x,y)}{\partial y\partial x}$ holds. Well i was wondering if the converse is also true, that is if continuity would be neccesary for them to be equal. my guess is no, since the proposition doesn't say ""if and only if"" but there's a misleading remark in the book i'm using which states ""Exercise 3.16 shows that continuity of $D_{i}D_{j}f$ and $D_{j}D_{i}f$ at $a$ are neccesary for their equality there."" but in the exercise there's nothing to prove.","I'm trying to learn calculus of several variables, and well there's a theorem which says that if all partials up to the second order are continuous then $\frac{\partial f(x,y)}{\partial x\partial y}=\frac{\partial f(x,y)}{\partial y\partial x}$ holds. Well i was wondering if the converse is also true, that is if continuity would be neccesary for them to be equal. my guess is no, since the proposition doesn't say ""if and only if"" but there's a misleading remark in the book i'm using which states ""Exercise 3.16 shows that continuity of $D_{i}D_{j}f$ and $D_{j}D_{i}f$ at $a$ are neccesary for their equality there."" but in the exercise there's nothing to prove.",,"['calculus', 'multivariable-calculus']"
27,How do I calculate the integral $\int_{0}^{1}{\frac{xe^{ax}}{(1+ax)^2}dx}$? [closed],How do I calculate the integral ? [closed],\int_{0}^{1}{\frac{xe^{ax}}{(1+ax)^2}dx},"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question If $a>0$, how do I calculate the following integral? $$\int_{0}^{1}{\frac{xe^{ax}}{(1+ax)^2}dx}$$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question If $a>0$, how do I calculate the following integral? $$\int_{0}^{1}{\frac{xe^{ax}}{(1+ax)^2}dx}$$",,"['integration', 'multivariable-calculus', 'definite-integrals']"
28,How to do a Taylor expansion of a vector-valued function,How to do a Taylor expansion of a vector-valued function,,"Let $f:\Bbb R^2\to \Bbb R^2$ be given by $$f(x,y):= \left(e^x\sin(x+y),e^{y-x}\tanh(y)\right)$$ Find the second-order Taylor expansion of $f$ about (x,y)=(0,0)$. I know how to find the Taylor Expansion of a function $f:\Bbb R^n \to \Bbb R$. But I have never seen $f:\Bbb R^n \to \Bbb R^m$ before.  Do I find the Taylor expansion of $$f_1(x,y)= \sin(x+y)e^{x}\quad  \text{at } (0,0)$$ and $$f_2(x,y) = \tanh(y)e^{y-x}\quad \text{at } (0,0)$$ and then combine the expansions for the Taylor expansion of $f$?","Let $f:\Bbb R^2\to \Bbb R^2$ be given by $$f(x,y):= \left(e^x\sin(x+y),e^{y-x}\tanh(y)\right)$$ Find the second-order Taylor expansion of $f$ about (x,y)=(0,0)$. I know how to find the Taylor Expansion of a function $f:\Bbb R^n \to \Bbb R$. But I have never seen $f:\Bbb R^n \to \Bbb R^m$ before.  Do I find the Taylor expansion of $$f_1(x,y)= \sin(x+y)e^{x}\quad  \text{at } (0,0)$$ and $$f_2(x,y) = \tanh(y)e^{y-x}\quad \text{at } (0,0)$$ and then combine the expansions for the Taylor expansion of $f$?",,"['real-analysis', 'multivariable-calculus', 'taylor-expansion']"
29,Describe the motion of the path,Describe the motion of the path,,Describe the motion of the path $$ \mathbf{r}=3\cos{t}\mathbf{\hat{i}}+4\cos{t}\mathbf{\hat{j}}+5\sin{t}{\mathbf{\hat{k}}} $$ The answers is: Path: the circle of intersection of the sphere $x^2+y^2+z^2=25$ and the plane $4x=3y$. I understand that the sphere has radius 5 since: $$ \left|\mathbf{r}\right|= \sqrt{(3\cos{t})^2+(4\cos{t})^2+(5\sin{t})^2)}= \sqrt{25\cos^2{t}+25\sin^2{t}}=5 $$ But I don't understand the connection to the plane or how to find it... Thanks!,Describe the motion of the path $$ \mathbf{r}=3\cos{t}\mathbf{\hat{i}}+4\cos{t}\mathbf{\hat{j}}+5\sin{t}{\mathbf{\hat{k}}} $$ The answers is: Path: the circle of intersection of the sphere $x^2+y^2+z^2=25$ and the plane $4x=3y$. I understand that the sphere has radius 5 since: $$ \left|\mathbf{r}\right|= \sqrt{(3\cos{t})^2+(4\cos{t})^2+(5\sin{t})^2)}= \sqrt{25\cos^2{t}+25\sin^2{t}}=5 $$ But I don't understand the connection to the plane or how to find it... Thanks!,,"['calculus', 'multivariable-calculus', 'vectors']"
30,Linear Regression with Multiple Targets Derivation,Linear Regression with Multiple Targets Derivation,,"I'm working through the derivation for the weights solution in multivariate linear regression, but keep getting tripped up when I try to solve for the case of multiple targets.  Starting with the residual sum of squares defines as: $$ RSS = \sum_{n=1}^N (\mathbf{t}_n - \mathbf{x}_n^T\mathbf{w})^2     = \sum_{n=1}^N (\mathbf{t}_n - \mathbf{x}_n^T\mathbf{w})(\mathbf{t}_n - \mathbf{x}_n^T\mathbf{w})^T $$ where targets at each sample n are defined as $\mathbf{t}_n = \left[t_{n,1} \: t_{n,2} \: \dotsc \: t_{n,K} \right]$, so that we can predict $K$ targets for each sample $\mathbf{x}_n$ in $D$ dimensions expressed as the column vector: $$ \mathbf{x}_n = \begin{bmatrix} x_{0,n}=1 \\ x_{1,n} \\ x_{2,n} \\ \vdots \\ x_{D,n} \end{bmatrix} $$ This implies that the weights $\mathbf{w}$ should be a $D+1 \times K$ matrix of the form: $$ \mathbf{w} = \begin{bmatrix} w_{0,1} & w_{0,2} & w_{0,3} & \dotsc & w_{0,K} \\ w_{1,1} & w_{1,2} & w_{1,3} & \dotsc & w_{1,K} \\ \vdots \\ w_{D,1} & w_{D,2} & w_{D,3} & \dotsc & w_{D,K} \end{bmatrix} $$ My goal is to get to: $$ \mathbf{X}^T \mathbf{T} = \mathbf{X}^T \mathbf{X} \mathbf{w} $$ and not the solution for $\mathbf{w}$ explicitly because I don't want to do any matrix inversions explicitly, but rather have Python solve the above equation using a line like numpy.linalg.lstsq(np.dot(X.T,X), numpy.dot(X.T, T)) .  The Python stuff is just to provide context. I know the overall trajectory of the derivation is driven by finding $\mathbf{w}$ that minimizes $RSS$.  To minimize $RSS$, start by taking the partial of $RSS$ wrt to $\mathbf{w}$, setting = 0 and do some rearranging: $$ \begin{align*} \frac{\partial RSS}{\partial \mathbf{w}} = \frac{\partial}{\partial \mathbf{w}} \sum_{n=1}^N (\mathbf{t}_n - \mathbf{x}_n^T\mathbf{w})(\mathbf{t}_n - \mathbf{x}_n^T\mathbf{w})^T \\ = \frac{\partial}{\partial \mathbf{w}} \sum_{n=1}^N (\mathbf{t}_n - \mathbf{x}_n^T\mathbf{w})(\mathbf{t}_n^T - \mathbf{w}^T\mathbf{x}_n) \\ = \frac{\partial}{\partial \mathbf{w}} \sum_{n=1}^N (\mathbf{t}_n \mathbf{t}_n^T - \mathbf{t}_n \mathbf{w}^T \mathbf{x}_n - \mathbf{x}_n^T \mathbf{w} \mathbf{t}_n^T + \mathbf{x}_n^T \mathbf{w} \mathbf{w}^T \mathbf{x}_n) \\ = \frac{\partial}{\partial \mathbf{w}} \left[ \mathbf{T} \mathbf{T}^T - \sum_{n=1}^N \mathbf{t}_n \mathbf{w}^T \mathbf{x}_n - \sum_{n=1}^N \mathbf{x}_n^T \mathbf{w} \mathbf{t}_n^T + \sum_{n=1}^N \mathbf{x}_n^T \mathbf{w} \mathbf{w}^T \mathbf{x}_n \right] \\ = - \frac{\partial}{\partial \mathbf{w}} \sum_{n=1}^N \mathbf{t}_n \mathbf{w}^T \mathbf{x}_n - \frac{\partial}{\partial \mathbf{w}} \sum_{n=1}^N \mathbf{x}_n^T \mathbf{w} \mathbf{t}_n^T + \frac{\partial}{\partial \mathbf{w}} \sum_{n=1}^N \mathbf{x}_n^T \mathbf{w} \mathbf{w}^T \mathbf{x}_n \\ = 0 \end{align*} $$ I have 3 specific questions: Is the math to this point correct?  If not, what needs to be corrected? How do you convert the remaining three summation terms into matrices?  I can see simple dot products like $\sum_{n=1}^N \mathbf{t}_n \mathbf{t}_n^T = \mathbf{T} \mathbf{T}^T$, but the other terms are what I'm having trouble with. How do you take the $\frac{\partial}{\partial \mathbf{w}}$ of the three matrix terms obtained from question 2)? After getting my last equation into matrix form, the rest is pretty easy.  I'll start digging into my matrix calculus in the meantime.  Thanks. UPDATE 6/8/2016 The first two summation terms are the transpose of each other, but since these are both scalars, they are equivalent. As an exercise, I expanded both of these terms out and can show that: $$ \mathbf{t}_n \mathbf{w}^T \mathbf{x}_n = \mathbf{x}_n^T \mathbf{w} \mathbf{t}_n^T = \sum_{k=1}^K t_{k,n} \sum_{d=0}^D w_{d,k} x_{d,n} $$ So the above equation: $$ - \frac{\partial}{\partial \mathbf{w}} \sum_{n=1}^N \mathbf{t}_n \mathbf{w}^T \mathbf{x}_n - \frac{\partial}{\partial \mathbf{w}} \sum_{n=1}^N \mathbf{x}_n^T \mathbf{w} \mathbf{t}_n^T + \frac{\partial}{\partial \mathbf{w}} \sum_{n=1}^N \mathbf{x}_n^T \mathbf{w} \mathbf{w}^T \mathbf{x}_n = 0 $$ can be simplified a little further as: $$ 2 \frac{\partial}{\partial \mathbf{w}} \sum_{n=1}^N \mathbf{t}_n \mathbf{w}^T \mathbf{x}_n = \frac{\partial}{\partial \mathbf{w}} \sum_{n=1}^N \mathbf{x}_n^T \mathbf{w} \mathbf{w}^T \mathbf{x}_n $$ This means that if I can show that: $$ \frac{\partial}{\partial \mathbf{w}} \sum_{n=1}^N \mathbf{t}_n \mathbf{w}^T \mathbf{x}_n = \mathbf{X}^T \mathbf{T} $$ and that: $$ \frac{\partial}{\partial \mathbf{w}} \sum_{n=1}^N \mathbf{x}_n^T \mathbf{w} \mathbf{w}^T \mathbf{x}_n = 2 \mathbf{X}^T \mathbf{X} \mathbf{w} $$ I'm done.  It looks like the answer to my 1st question is ""yes"" if the last two expressions can be proved.","I'm working through the derivation for the weights solution in multivariate linear regression, but keep getting tripped up when I try to solve for the case of multiple targets.  Starting with the residual sum of squares defines as: $$ RSS = \sum_{n=1}^N (\mathbf{t}_n - \mathbf{x}_n^T\mathbf{w})^2     = \sum_{n=1}^N (\mathbf{t}_n - \mathbf{x}_n^T\mathbf{w})(\mathbf{t}_n - \mathbf{x}_n^T\mathbf{w})^T $$ where targets at each sample n are defined as $\mathbf{t}_n = \left[t_{n,1} \: t_{n,2} \: \dotsc \: t_{n,K} \right]$, so that we can predict $K$ targets for each sample $\mathbf{x}_n$ in $D$ dimensions expressed as the column vector: $$ \mathbf{x}_n = \begin{bmatrix} x_{0,n}=1 \\ x_{1,n} \\ x_{2,n} \\ \vdots \\ x_{D,n} \end{bmatrix} $$ This implies that the weights $\mathbf{w}$ should be a $D+1 \times K$ matrix of the form: $$ \mathbf{w} = \begin{bmatrix} w_{0,1} & w_{0,2} & w_{0,3} & \dotsc & w_{0,K} \\ w_{1,1} & w_{1,2} & w_{1,3} & \dotsc & w_{1,K} \\ \vdots \\ w_{D,1} & w_{D,2} & w_{D,3} & \dotsc & w_{D,K} \end{bmatrix} $$ My goal is to get to: $$ \mathbf{X}^T \mathbf{T} = \mathbf{X}^T \mathbf{X} \mathbf{w} $$ and not the solution for $\mathbf{w}$ explicitly because I don't want to do any matrix inversions explicitly, but rather have Python solve the above equation using a line like numpy.linalg.lstsq(np.dot(X.T,X), numpy.dot(X.T, T)) .  The Python stuff is just to provide context. I know the overall trajectory of the derivation is driven by finding $\mathbf{w}$ that minimizes $RSS$.  To minimize $RSS$, start by taking the partial of $RSS$ wrt to $\mathbf{w}$, setting = 0 and do some rearranging: $$ \begin{align*} \frac{\partial RSS}{\partial \mathbf{w}} = \frac{\partial}{\partial \mathbf{w}} \sum_{n=1}^N (\mathbf{t}_n - \mathbf{x}_n^T\mathbf{w})(\mathbf{t}_n - \mathbf{x}_n^T\mathbf{w})^T \\ = \frac{\partial}{\partial \mathbf{w}} \sum_{n=1}^N (\mathbf{t}_n - \mathbf{x}_n^T\mathbf{w})(\mathbf{t}_n^T - \mathbf{w}^T\mathbf{x}_n) \\ = \frac{\partial}{\partial \mathbf{w}} \sum_{n=1}^N (\mathbf{t}_n \mathbf{t}_n^T - \mathbf{t}_n \mathbf{w}^T \mathbf{x}_n - \mathbf{x}_n^T \mathbf{w} \mathbf{t}_n^T + \mathbf{x}_n^T \mathbf{w} \mathbf{w}^T \mathbf{x}_n) \\ = \frac{\partial}{\partial \mathbf{w}} \left[ \mathbf{T} \mathbf{T}^T - \sum_{n=1}^N \mathbf{t}_n \mathbf{w}^T \mathbf{x}_n - \sum_{n=1}^N \mathbf{x}_n^T \mathbf{w} \mathbf{t}_n^T + \sum_{n=1}^N \mathbf{x}_n^T \mathbf{w} \mathbf{w}^T \mathbf{x}_n \right] \\ = - \frac{\partial}{\partial \mathbf{w}} \sum_{n=1}^N \mathbf{t}_n \mathbf{w}^T \mathbf{x}_n - \frac{\partial}{\partial \mathbf{w}} \sum_{n=1}^N \mathbf{x}_n^T \mathbf{w} \mathbf{t}_n^T + \frac{\partial}{\partial \mathbf{w}} \sum_{n=1}^N \mathbf{x}_n^T \mathbf{w} \mathbf{w}^T \mathbf{x}_n \\ = 0 \end{align*} $$ I have 3 specific questions: Is the math to this point correct?  If not, what needs to be corrected? How do you convert the remaining three summation terms into matrices?  I can see simple dot products like $\sum_{n=1}^N \mathbf{t}_n \mathbf{t}_n^T = \mathbf{T} \mathbf{T}^T$, but the other terms are what I'm having trouble with. How do you take the $\frac{\partial}{\partial \mathbf{w}}$ of the three matrix terms obtained from question 2)? After getting my last equation into matrix form, the rest is pretty easy.  I'll start digging into my matrix calculus in the meantime.  Thanks. UPDATE 6/8/2016 The first two summation terms are the transpose of each other, but since these are both scalars, they are equivalent. As an exercise, I expanded both of these terms out and can show that: $$ \mathbf{t}_n \mathbf{w}^T \mathbf{x}_n = \mathbf{x}_n^T \mathbf{w} \mathbf{t}_n^T = \sum_{k=1}^K t_{k,n} \sum_{d=0}^D w_{d,k} x_{d,n} $$ So the above equation: $$ - \frac{\partial}{\partial \mathbf{w}} \sum_{n=1}^N \mathbf{t}_n \mathbf{w}^T \mathbf{x}_n - \frac{\partial}{\partial \mathbf{w}} \sum_{n=1}^N \mathbf{x}_n^T \mathbf{w} \mathbf{t}_n^T + \frac{\partial}{\partial \mathbf{w}} \sum_{n=1}^N \mathbf{x}_n^T \mathbf{w} \mathbf{w}^T \mathbf{x}_n = 0 $$ can be simplified a little further as: $$ 2 \frac{\partial}{\partial \mathbf{w}} \sum_{n=1}^N \mathbf{t}_n \mathbf{w}^T \mathbf{x}_n = \frac{\partial}{\partial \mathbf{w}} \sum_{n=1}^N \mathbf{x}_n^T \mathbf{w} \mathbf{w}^T \mathbf{x}_n $$ This means that if I can show that: $$ \frac{\partial}{\partial \mathbf{w}} \sum_{n=1}^N \mathbf{t}_n \mathbf{w}^T \mathbf{x}_n = \mathbf{X}^T \mathbf{T} $$ and that: $$ \frac{\partial}{\partial \mathbf{w}} \sum_{n=1}^N \mathbf{x}_n^T \mathbf{w} \mathbf{w}^T \mathbf{x}_n = 2 \mathbf{X}^T \mathbf{X} \mathbf{w} $$ I'm done.  It looks like the answer to my 1st question is ""yes"" if the last two expressions can be proved.",,"['multivariable-calculus', 'matrix-calculus', 'linear-regression']"
31,Spivak's Calculus on Manifolds - Proof of Inverse Function Theorem,Spivak's Calculus on Manifolds - Proof of Inverse Function Theorem,,"I have a small confusion in a step in the proof of the Inverse Function Theorem from Spivak's Calculus on Manifolds . Theorem 2-11 (Inverse Function Theorem) Suppose that $f : \mathbb{R}^n \to \mathbb{R}^n$ is continuously differentiable in an open set containing $a$, and $\det f'(a) \neq 0$. Then there is an open set $V$ containing $a$ and an open set $W$ containing $f(a)$ such that $f : V \to W$ has a continuous inverse $f^{-1} : W \to V$ which is differentiable and for all $y \in W$ satisfies   $$ (f^{-1})'(y) = (f'(f^{-1}(y)))^{-1}. $$ To prove this, we first reduce it to the case when $Df(a)$ is the identity map. Then, we find a closed rectangle $U$ containing $a$ in its interior which satisfies the following properties: $f(x) \neq f(a)$ for $x \in U$. $\det f'(x) \neq 0$ for $x \in U$. $|D_j f^i(x)-D_j f^i(a)| < 1/2n^2$ for all $i$, $j$, and $x \in U$. $|x_1 - x_2| < 2|f(x_1) - f(x_2)|$ for $x_1, x_2 \in U$. There is also an open ball $W$ around $f(a)$ such that for all $y \in W$ and $x \in \operatorname{boundary} U$, we have $|y - f(a)| < |y - f(x)|$. Next, we will show that for any $y \in W$ there is a unique $x$ in $\operatorname{interior} U$ such that $f(x) = y$. To prove this, consider the function $g : U \to \mathbb{R}$ defined by $$ g(x) = |y-f(x)|^2 = \sum_{i=1}^n (y^i - f^i(x))^2. $$ This function is continuous and therefore has a minimum on $U$. Since $g(a) < g(x)$ for $x \in \operatorname{boundary} U$ by (5), the minimum occurs in the interior. So, there is a point $x$ in the interior of $U$ such that $D_j g(x) = 0$ for all $j$, that is $$ \sum_{i=1}^n 2(y^i - f^i(x))\cdot D_j f^i (x) = 0 \quad \text{for all $j$}. $$ By (2.) the matrix $(D_j f^i (x))$ must have non-zero determinant. Therefore we must have $y^i - f^i(x) = 0$ for all $i$, that is $f(x) = y$. This proves the existence of $x$. Uniqueness follows immediately from (4). Then, Spivak says If $V = (\operatorname{interior} U) \cap f^{-1}(W)$, we have shown that the function $f : V \to W$ has an inverse $f^{-1} : W \to V$. My confusion is that since we have already shown that for all $y \in W$ there is a unique $x$ in the interior of $U$, why is it necessary to take $V = (\operatorname{interior} U) \cap f^{-1}(W)$? Doesn't $V = f^{-1}(W)$ suffice?","I have a small confusion in a step in the proof of the Inverse Function Theorem from Spivak's Calculus on Manifolds . Theorem 2-11 (Inverse Function Theorem) Suppose that $f : \mathbb{R}^n \to \mathbb{R}^n$ is continuously differentiable in an open set containing $a$, and $\det f'(a) \neq 0$. Then there is an open set $V$ containing $a$ and an open set $W$ containing $f(a)$ such that $f : V \to W$ has a continuous inverse $f^{-1} : W \to V$ which is differentiable and for all $y \in W$ satisfies   $$ (f^{-1})'(y) = (f'(f^{-1}(y)))^{-1}. $$ To prove this, we first reduce it to the case when $Df(a)$ is the identity map. Then, we find a closed rectangle $U$ containing $a$ in its interior which satisfies the following properties: $f(x) \neq f(a)$ for $x \in U$. $\det f'(x) \neq 0$ for $x \in U$. $|D_j f^i(x)-D_j f^i(a)| < 1/2n^2$ for all $i$, $j$, and $x \in U$. $|x_1 - x_2| < 2|f(x_1) - f(x_2)|$ for $x_1, x_2 \in U$. There is also an open ball $W$ around $f(a)$ such that for all $y \in W$ and $x \in \operatorname{boundary} U$, we have $|y - f(a)| < |y - f(x)|$. Next, we will show that for any $y \in W$ there is a unique $x$ in $\operatorname{interior} U$ such that $f(x) = y$. To prove this, consider the function $g : U \to \mathbb{R}$ defined by $$ g(x) = |y-f(x)|^2 = \sum_{i=1}^n (y^i - f^i(x))^2. $$ This function is continuous and therefore has a minimum on $U$. Since $g(a) < g(x)$ for $x \in \operatorname{boundary} U$ by (5), the minimum occurs in the interior. So, there is a point $x$ in the interior of $U$ such that $D_j g(x) = 0$ for all $j$, that is $$ \sum_{i=1}^n 2(y^i - f^i(x))\cdot D_j f^i (x) = 0 \quad \text{for all $j$}. $$ By (2.) the matrix $(D_j f^i (x))$ must have non-zero determinant. Therefore we must have $y^i - f^i(x) = 0$ for all $i$, that is $f(x) = y$. This proves the existence of $x$. Uniqueness follows immediately from (4). Then, Spivak says If $V = (\operatorname{interior} U) \cap f^{-1}(W)$, we have shown that the function $f : V \to W$ has an inverse $f^{-1} : W \to V$. My confusion is that since we have already shown that for all $y \in W$ there is a unique $x$ in the interior of $U$, why is it necessary to take $V = (\operatorname{interior} U) \cap f^{-1}(W)$? Doesn't $V = f^{-1}(W)$ suffice?",,['multivariable-calculus']
32,Scalar property of $ C(\Omega)=\sum_{|\alpha|\leq m}\color{blue}{\big|\Omega\big|^{\frac{2|\alpha|-n}{n}}} \int_{\Omega}|D^\alpha f|^2\ dx $,Scalar property of, C(\Omega)=\sum_{|\alpha|\leq m}\color{blue}{\big|\Omega\big|^{\frac{2|\alpha|-n}{n}}} \int_{\Omega}|D^\alpha f|^2\ dx ,"This is closely related to a previous question: Scale invariant definition of the Sobolev norm $\|\|_{m,\Omega}$ for $H^m(\Omega)$ This question focuses on the direct calculation (by change of variables) of the simplest one-dimensional case. Let $\Omega\subset\mathbb{R}^n$ be a nonempty bounded open set. Denote the Lebesgue measure of $\Omega$ as $|\Omega|$. For $f\in H^m(\Omega)$, define $$ \|f\|_{m,\Omega}^2=\sum_{|\alpha|\leq m}\color{blue}{\big|\Omega\big|^{\dfrac{2(|\alpha|-m)}{n}}} \int_{\Omega}|D^\alpha f|^2\ dx. $$ According the so-called ""dimensional analysis"" (see the comment to a previous question ), the quantity $$ C(\Omega):=\dfrac{1}{\big|\Omega\big|^{\dfrac{n-2m}{n}}}\cdot \|f\|_{m,\Omega}^2 $$ is scale invariant, namely, $C(\Omega)=C(\delta\Omega)$ for any $\delta>0$ where $$ \delta\Omega:=\{\delta x\mid x\in \Omega\}. $$ Note that we can rewrite $C(\Omega)$ as $$ C(\Omega)=\sum_{|\alpha|\leq m}\color{blue}{\big|\Omega\big|^{\dfrac{2|\alpha|-n}{n}}} \int_{\Omega}|D^\alpha f|^2\ dx \tag{*} $$ I would like test the statement above by a direct calculation of $C(\delta\Omega)$ using change of variables with the simple case $n=1$ and $\Omega=(0,1)$. In this case, $(*)$ becomes $$ C(\delta\Omega)=\sum_{k\leq m}\color{blue}{\delta^{2k-1}} \int_{\delta\Omega}|D^kf(x)|^2\ dx. $$ To show $C(\Omega)=C(\delta\Omega)$, it suffices to show that $$ \int_0^\delta|D^k f|^2\ dx=\delta^{1-2k}\int_0^1|D^k f(x)|^2\ dx. $$ But even in the case $k=1$, one can check that the identity $$ \int_0^\delta |f'(x)|^2\ dx=\frac{1}{\delta}\int_0^1|f'(x)|^2\ dx $$ is wrong. What is going wrong here?","This is closely related to a previous question: Scale invariant definition of the Sobolev norm $\|\|_{m,\Omega}$ for $H^m(\Omega)$ This question focuses on the direct calculation (by change of variables) of the simplest one-dimensional case. Let $\Omega\subset\mathbb{R}^n$ be a nonempty bounded open set. Denote the Lebesgue measure of $\Omega$ as $|\Omega|$. For $f\in H^m(\Omega)$, define $$ \|f\|_{m,\Omega}^2=\sum_{|\alpha|\leq m}\color{blue}{\big|\Omega\big|^{\dfrac{2(|\alpha|-m)}{n}}} \int_{\Omega}|D^\alpha f|^2\ dx. $$ According the so-called ""dimensional analysis"" (see the comment to a previous question ), the quantity $$ C(\Omega):=\dfrac{1}{\big|\Omega\big|^{\dfrac{n-2m}{n}}}\cdot \|f\|_{m,\Omega}^2 $$ is scale invariant, namely, $C(\Omega)=C(\delta\Omega)$ for any $\delta>0$ where $$ \delta\Omega:=\{\delta x\mid x\in \Omega\}. $$ Note that we can rewrite $C(\Omega)$ as $$ C(\Omega)=\sum_{|\alpha|\leq m}\color{blue}{\big|\Omega\big|^{\dfrac{2|\alpha|-n}{n}}} \int_{\Omega}|D^\alpha f|^2\ dx \tag{*} $$ I would like test the statement above by a direct calculation of $C(\delta\Omega)$ using change of variables with the simple case $n=1$ and $\Omega=(0,1)$. In this case, $(*)$ becomes $$ C(\delta\Omega)=\sum_{k\leq m}\color{blue}{\delta^{2k-1}} \int_{\delta\Omega}|D^kf(x)|^2\ dx. $$ To show $C(\Omega)=C(\delta\Omega)$, it suffices to show that $$ \int_0^\delta|D^k f|^2\ dx=\delta^{1-2k}\int_0^1|D^k f(x)|^2\ dx. $$ But even in the case $k=1$, one can check that the identity $$ \int_0^\delta |f'(x)|^2\ dx=\frac{1}{\delta}\int_0^1|f'(x)|^2\ dx $$ is wrong. What is going wrong here?",,['real-analysis']
33,Integral conversion to polar coordinates - bounds $\int\limits_0^1 \int\limits_0^1\sqrt{x^2+y^2}\ dxdy$ [duplicate],Integral conversion to polar coordinates - bounds  [duplicate],\int\limits_0^1 \int\limits_0^1\sqrt{x^2+y^2}\ dxdy,"This question already has answers here : Evaluating $\int_{0}^{1}\int_{0}^{1}\sqrt{x^{2}+y^{2}}dxdy$ using polar coordinates. (3 answers) Closed 4 years ago . I have an integral $$\int_0^1 \int_0^1\sqrt{x^2+y^2}\ dxdy $$ and its result is $\approx0.765...$ I convert it to polar coordinates and get $$\int_a^b \int_c^dr\ drd\phi $$ But how can I compute $a,b,c,d$ ?",This question already has answers here : Evaluating $\int_{0}^{1}\int_{0}^{1}\sqrt{x^{2}+y^{2}}dxdy$ using polar coordinates. (3 answers) Closed 4 years ago . I have an integral and its result is I convert it to polar coordinates and get But how can I compute ?,"\int_0^1 \int_0^1\sqrt{x^2+y^2}\ dxdy  \approx0.765... \int_a^b \int_c^dr\ drd\phi  a,b,c,d","['integration', 'multivariable-calculus', 'polar-coordinates']"
34,"Does $f(x,y)=(x^2y+x,6x+y^2)$ have a local inverse at $(1,1)$?",Does  have a local inverse at ?,"f(x,y)=(x^2y+x,6x+y^2) (1,1)","Let $f:\mathbb{R}^2\to\mathbb{R}^2$ be given by  $$ f(x,y)=(x^2y+x,6x+y^2). $$ A direct calculation shows that $\det Df(1,1)=0$. The key assumption in the Inverse Function Theorem is not satisfied. Here is my question : Does there exist a local inverse of $f$ at $(1,1)$?","Let $f:\mathbb{R}^2\to\mathbb{R}^2$ be given by  $$ f(x,y)=(x^2y+x,6x+y^2). $$ A direct calculation shows that $\det Df(1,1)=0$. The key assumption in the Inverse Function Theorem is not satisfied. Here is my question : Does there exist a local inverse of $f$ at $(1,1)$?",,['real-analysis']
35,Lagrange Method Optimization Problem with constraint,Lagrange Method Optimization Problem with constraint,,"The problem is as follows: $$\min_{x,y} x + y$$ subject to $$xy = 0.25$$ My attempt: I used the method of Lagrange multipliers here setting $f(x,y) = x + y$ and $g(x,y) = xy - 0.25 = 0$ So we have $\nabla f = \lambda \nabla f$ $\implies 1 = \lambda y$, and $1 = \lambda x$ $\implies x = y$ and from the constraint we get: $x^2 = 0.25 \implies x = \pm \sqrt{0.25} = y$ Clearly we choose $x = y = - \sqrt{0.25}$ to minimize the function. However, I checked this minimization problem with Wolfram Alpha and it says no global minima where found. Why is this and where did I go wrong? Thanks!","The problem is as follows: $$\min_{x,y} x + y$$ subject to $$xy = 0.25$$ My attempt: I used the method of Lagrange multipliers here setting $f(x,y) = x + y$ and $g(x,y) = xy - 0.25 = 0$ So we have $\nabla f = \lambda \nabla f$ $\implies 1 = \lambda y$, and $1 = \lambda x$ $\implies x = y$ and from the constraint we get: $x^2 = 0.25 \implies x = \pm \sqrt{0.25} = y$ Clearly we choose $x = y = - \sqrt{0.25}$ to minimize the function. However, I checked this minimization problem with Wolfram Alpha and it says no global minima where found. Why is this and where did I go wrong? Thanks!",,['multivariable-calculus']
36,An infimum of a double integral on the unit disk,An infimum of a double integral on the unit disk,,"The following question comes from Arnold's Trivium of $1991$ and it is problem $68$. I do not have a solution neither can I come up with something. Find $$\inf  \iint \limits_{x^2+y^2 \leq 1} \left[ \left ( \frac{\partial u}{\partial x} \right)^2 + \left ( \frac{\partial u}{\partial y} \right )^2 \right ]\, {\rm d}x \, {\rm d}y$$ for $C^{\infty}$ functions $u$ that vanish at $0$ and equal $1$ on $x^2+y^2=1$.","The following question comes from Arnold's Trivium of $1991$ and it is problem $68$. I do not have a solution neither can I come up with something. Find $$\inf  \iint \limits_{x^2+y^2 \leq 1} \left[ \left ( \frac{\partial u}{\partial x} \right)^2 + \left ( \frac{\partial u}{\partial y} \right )^2 \right ]\, {\rm d}x \, {\rm d}y$$ for $C^{\infty}$ functions $u$ that vanish at $0$ and equal $1$ on $x^2+y^2=1$.",,"['real-analysis', 'multivariable-calculus', 'contest-math']"
37,Computing the limits of integration when doing a nontrivial coordinate transform,Computing the limits of integration when doing a nontrivial coordinate transform,,"In Aigner, Ziegler: Proofs_from_THE_BOOK they explain an elegant derivation of $\zeta(2)$ given by Beukers, Calabi and Kolk. One starts from the integral $$ J = \int_0^1 \int_0^1 \frac{1}{1-x^2y^2}dxdy = \frac{3}{4}\zeta(2) $$ To compute the integral they use the following non-trivial coordinate transform $$ x = \frac{\sin u}{\cos v}, y = \frac{\sin v}{\cos u} $$ The reason for this transform is that the Jacobi determinate of this transformation turns out to be $$ |D| = 1-\frac{\sin^2u\sin^2v}{\cos^2u\cos^2v} = 1 - x^2y^2 $$ So after the transformation the integral becomes magically $$ \int_{f_1(u,v)}^{f_2(u,v)} \int_{g_1(u,v)}^{g_2(u,v)} 1 dudv $$ Now they argue that the limits of the integration are $$ f_1(u,v) = g_1(u,v) = 0 \\ f_2(u,v) = \frac{\pi}{2}\\ g_2(u,v) = \frac{\pi}{2}-v $$ so that the integral turns out to be $$ J = \frac{\pi^2}{8} $$ But I have problems understanding how one comes up with the limits of integration for such a coordinate transform (especially I wonder about the ""$-v$""). Can someone give maybe a step-by-step solution for computing the limits of integration here? I would also be interested how the limits would look like, if I would want to compute $\zeta(4)$ from the corresponding quadruple integral.","In Aigner, Ziegler: Proofs_from_THE_BOOK they explain an elegant derivation of $\zeta(2)$ given by Beukers, Calabi and Kolk. One starts from the integral $$ J = \int_0^1 \int_0^1 \frac{1}{1-x^2y^2}dxdy = \frac{3}{4}\zeta(2) $$ To compute the integral they use the following non-trivial coordinate transform $$ x = \frac{\sin u}{\cos v}, y = \frac{\sin v}{\cos u} $$ The reason for this transform is that the Jacobi determinate of this transformation turns out to be $$ |D| = 1-\frac{\sin^2u\sin^2v}{\cos^2u\cos^2v} = 1 - x^2y^2 $$ So after the transformation the integral becomes magically $$ \int_{f_1(u,v)}^{f_2(u,v)} \int_{g_1(u,v)}^{g_2(u,v)} 1 dudv $$ Now they argue that the limits of the integration are $$ f_1(u,v) = g_1(u,v) = 0 \\ f_2(u,v) = \frac{\pi}{2}\\ g_2(u,v) = \frac{\pi}{2}-v $$ so that the integral turns out to be $$ J = \frac{\pi^2}{8} $$ But I have problems understanding how one comes up with the limits of integration for such a coordinate transform (especially I wonder about the ""$-v$""). Can someone give maybe a step-by-step solution for computing the limits of integration here? I would also be interested how the limits would look like, if I would want to compute $\zeta(4)$ from the corresponding quadruple integral.",,"['integration', 'multivariable-calculus']"
38,How to setup a double integral when the region is bounded by a circle and a parabola?,How to setup a double integral when the region is bounded by a circle and a parabola?,,"My task is this; Calculate$$\iint\limits_{A}y\:dA.$$ Where $A$ is the region in the $xy-$plane such that $x^2\leq y,\: x^2 + y^2 \leq 2$. My work so far: Our region $A$ is in the first and seccond quadrant above the parabola $x^2$ and below the circle centered at the origin with a radius of $\sqrt{2}$. Switching to polar coordinates gives us (remember the jacobian):$$\int\limits_{0}^{\pi}\int\limits_{r^2\cos^2(\theta)}^{\sqrt{2}}r^2\sin(\theta)\: dr\:d\theta.$$ However this setup leads to an answer with a variable $r$ and since the answer is a real number i must have set this one up wrong. Hints are welcome, and don't show calculations that reveal the answer as i would very much like to do that on my own:) Thanks in advance!","My task is this; Calculate$$\iint\limits_{A}y\:dA.$$ Where $A$ is the region in the $xy-$plane such that $x^2\leq y,\: x^2 + y^2 \leq 2$. My work so far: Our region $A$ is in the first and seccond quadrant above the parabola $x^2$ and below the circle centered at the origin with a radius of $\sqrt{2}$. Switching to polar coordinates gives us (remember the jacobian):$$\int\limits_{0}^{\pi}\int\limits_{r^2\cos^2(\theta)}^{\sqrt{2}}r^2\sin(\theta)\: dr\:d\theta.$$ However this setup leads to an answer with a variable $r$ and since the answer is a real number i must have set this one up wrong. Hints are welcome, and don't show calculations that reveal the answer as i would very much like to do that on my own:) Thanks in advance!",,"['integration', 'multivariable-calculus', 'polar-coordinates']"
39,How to solve a triple integral with a circle not centered at origin?,How to solve a triple integral with a circle not centered at origin?,,"My task is this: Use cylinder coordinates to calculate:$$\iiint\limits_{A}z\sqrt{x^2 + y^2}dA, \enspace A = \left\{(x,y,z):x^2 + (y - 1)^2 \leq 1,\: 0 \leq z \leq 2\right\}.$$ My works so far is this; Switching to cylindrical coordinates we get:$$A = \left\{(r,\theta,z):0\leq r\leq 1,\: 0\leq \theta \leq 2\pi,\: 0\leq z \leq 2\right\}.$$ Now my book tells me that if you want the center in another point $(a,b,c)$, you should use the substitution: $$x = a + r\cos(\theta),\: y = b + r\sin(\theta),\: z = c + z.$$ With this in mind we change to cylindrical and add the bounderies (don't forget the jacobian).$$\int\limits_{0}^{2\pi}\int\limits_{0}^{1}\int\limits_{0}^{2}zr\sqrt{r^2\cos^2(\theta) + (1 + r\sin(\theta))^2}dz\:dr\:d\theta \:=\:2\int\limits_{0}^{2\pi}\int\limits_{0}^{1} r\sqrt{r^2\cos^2(\theta) + (1 + r\sin(\theta))^2}dr \:d\theta.$$ Now this is the part where i get stuck, if i did my calculations right teh expression under the root becomes $r^2 + 1 + 2r\sin(\theta).$ I'm not sure where to go from here so any tips and tricks would be appreciated. I would very much like to see how this is done with this substitution, but alternative solution that leads me to the right answer would also be of great value. Finally, don't show me the calculations down to the answer as i would like to do that myself. Thanks in advance!","My task is this: Use cylinder coordinates to calculate:$$\iiint\limits_{A}z\sqrt{x^2 + y^2}dA, \enspace A = \left\{(x,y,z):x^2 + (y - 1)^2 \leq 1,\: 0 \leq z \leq 2\right\}.$$ My works so far is this; Switching to cylindrical coordinates we get:$$A = \left\{(r,\theta,z):0\leq r\leq 1,\: 0\leq \theta \leq 2\pi,\: 0\leq z \leq 2\right\}.$$ Now my book tells me that if you want the center in another point $(a,b,c)$, you should use the substitution: $$x = a + r\cos(\theta),\: y = b + r\sin(\theta),\: z = c + z.$$ With this in mind we change to cylindrical and add the bounderies (don't forget the jacobian).$$\int\limits_{0}^{2\pi}\int\limits_{0}^{1}\int\limits_{0}^{2}zr\sqrt{r^2\cos^2(\theta) + (1 + r\sin(\theta))^2}dz\:dr\:d\theta \:=\:2\int\limits_{0}^{2\pi}\int\limits_{0}^{1} r\sqrt{r^2\cos^2(\theta) + (1 + r\sin(\theta))^2}dr \:d\theta.$$ Now this is the part where i get stuck, if i did my calculations right teh expression under the root becomes $r^2 + 1 + 2r\sin(\theta).$ I'm not sure where to go from here so any tips and tricks would be appreciated. I would very much like to see how this is done with this substitution, but alternative solution that leads me to the right answer would also be of great value. Finally, don't show me the calculations down to the answer as i would like to do that myself. Thanks in advance!",,"['integration', 'multivariable-calculus']"
40,Implicit function whose domain is $\mathbb R^2$,Implicit function whose domain is,\mathbb R^2,"Let $F(x, y, z)=z^3+3z+2x^4+y^2-x^2-2y$. I want to show that the equation $F(x, y, z)=0$ defines a $C^2$ function $z=f(x, y)$ whose domain is $\mathbb R^2$. By the implicit function theorem, it's easy to see that such an $f$ exists in a neighborhood of $(0, 0)$, however, I don't know how to show that we can extend this function to $\mathbb R^2$.","Let $F(x, y, z)=z^3+3z+2x^4+y^2-x^2-2y$. I want to show that the equation $F(x, y, z)=0$ defines a $C^2$ function $z=f(x, y)$ whose domain is $\mathbb R^2$. By the implicit function theorem, it's easy to see that such an $f$ exists in a neighborhood of $(0, 0)$, however, I don't know how to show that we can extend this function to $\mathbb R^2$.",,[]
41,Implicit function theorem from PMA Rudin,Implicit function theorem from PMA Rudin,,It's the statement of Implicit function theorem from Rudin's PMA. Why here consider mapping from $\mathbb{R}^{n+m}$ into $\mathbb{R}^{n}$? Why the dimension of domain is bigger than codomain? I guess that here $m\geqslant 1$. I can't understand this moment,It's the statement of Implicit function theorem from Rudin's PMA. Why here consider mapping from $\mathbb{R}^{n+m}$ into $\mathbb{R}^{n}$? Why the dimension of domain is bigger than codomain? I guess that here $m\geqslant 1$. I can't understand this moment,,['multivariable-calculus']
42,What's the point of the fancy notation for surface integrals and line integrals?,What's the point of the fancy notation for surface integrals and line integrals?,,"Most of the times you see line integrals of a vector field written as this $$ \int_C\mathbf{F\cdot ds} $$ And surface integrals like $$\iint_\Sigma \mathbf{F\cdot n}\,\mathrm dS$$ My question is, what's the point of all this symbology? It seems like the $\mathbf {ds}$ and $\mathbf{n}\, \mathrm dS$ are there just to remind the reader ""hey! This is a little tiny piece of curve/surface!"" or whatever heuristic you have to explain this integrals. If you know that $C\subseteq \Bbb R^k$ is a curve, and $\Sigma\subseteq \Bbb R^k$ is a surface, why not just write $\int_C \bf F$ and $\iint_\Sigma \bf F$ (or even $\int_\Sigma \bf F$ (although the double integral sign makes more sense, because in the end you end up calculating a double integral))?","Most of the times you see line integrals of a vector field written as this $$ \int_C\mathbf{F\cdot ds} $$ And surface integrals like $$\iint_\Sigma \mathbf{F\cdot n}\,\mathrm dS$$ My question is, what's the point of all this symbology? It seems like the $\mathbf {ds}$ and $\mathbf{n}\, \mathrm dS$ are there just to remind the reader ""hey! This is a little tiny piece of curve/surface!"" or whatever heuristic you have to explain this integrals. If you know that $C\subseteq \Bbb R^k$ is a curve, and $\Sigma\subseteq \Bbb R^k$ is a surface, why not just write $\int_C \bf F$ and $\iint_\Sigma \bf F$ (or even $\int_\Sigma \bf F$ (although the double integral sign makes more sense, because in the end you end up calculating a double integral))?",,"['integration', 'multivariable-calculus', 'notation', 'surface-integrals', 'line-integrals']"
43,"Bounding the Jacobian determinant for ""sublipschitz"" function","Bounding the Jacobian determinant for ""sublipschitz"" function",,"I'm practicing for my upcoming exam in calculus 3. I came across the following question in a practice paper: $ f:\mathbb R^3 \rightarrow \mathbb R^3 $, f is $C^1$. Also, $K|p-p'| \le |f(p)-f(p')|$ for all $p, p' \in \mathbb R^3$ (where $|\cdot|$ is the Euclidean norm and $K>0$ is a real constant). Prove that $K^3 v(\Omega) \le v(f(\Omega))$ for every Jordan measurable $\Omega\subset\mathbb R^3$ ($v$ is the Jordan measure on $\mathbb R^3$) I approached the question as follows. First, I noticed I can prove $f$ is a diffeomorphism $\mathbb R^3 \rightarrow f(\mathbb R^3)$ such that $D_f (x)$ is an invertible matrix for every $x\in\mathbb R^3$. Provided this is true, using variable substitution theorem, I can obtain the equality $v(f(\Omega))=\int_{f(\Omega))} 1 = \int_{\Omega} |J_f(x)|dx$, where $J_f (x) = det(D_f(x))$ is the Jacobian determinant. Now, I wanted to prove $(*)$ $|J_f (x)| \ge K^3$. From this I'll get $\int_{\Omega} |J_f(x)|dx \ge K^3 \int_{\Omega} 1 = K^3 v(\Omega)$, thus completing the proof. However, though $(*)$ seems intuitively true to me, I couldn't manage to prove it. In particular, I tried primarily to rely on the equality $det(D_f(x)) = lim_{Q\downarrow x} \frac{v(f(Q))}{v(Q)}$, where $Q$ is a cube centered around $x$ and $Q\downarrow x$ means $Q$'s area approaches zero. Regardless, nothing I tried worked. Any help in proving $(*)$ will be much appreciated, particularly if it was done with the equality I mentioned above. Thanks!","I'm practicing for my upcoming exam in calculus 3. I came across the following question in a practice paper: $ f:\mathbb R^3 \rightarrow \mathbb R^3 $, f is $C^1$. Also, $K|p-p'| \le |f(p)-f(p')|$ for all $p, p' \in \mathbb R^3$ (where $|\cdot|$ is the Euclidean norm and $K>0$ is a real constant). Prove that $K^3 v(\Omega) \le v(f(\Omega))$ for every Jordan measurable $\Omega\subset\mathbb R^3$ ($v$ is the Jordan measure on $\mathbb R^3$) I approached the question as follows. First, I noticed I can prove $f$ is a diffeomorphism $\mathbb R^3 \rightarrow f(\mathbb R^3)$ such that $D_f (x)$ is an invertible matrix for every $x\in\mathbb R^3$. Provided this is true, using variable substitution theorem, I can obtain the equality $v(f(\Omega))=\int_{f(\Omega))} 1 = \int_{\Omega} |J_f(x)|dx$, where $J_f (x) = det(D_f(x))$ is the Jacobian determinant. Now, I wanted to prove $(*)$ $|J_f (x)| \ge K^3$. From this I'll get $\int_{\Omega} |J_f(x)|dx \ge K^3 \int_{\Omega} 1 = K^3 v(\Omega)$, thus completing the proof. However, though $(*)$ seems intuitively true to me, I couldn't manage to prove it. In particular, I tried primarily to rely on the equality $det(D_f(x)) = lim_{Q\downarrow x} \frac{v(f(Q))}{v(Q)}$, where $Q$ is a cube centered around $x$ and $Q\downarrow x$ means $Q$'s area approaches zero. Regardless, nothing I tried worked. Any help in proving $(*)$ will be much appreciated, particularly if it was done with the equality I mentioned above. Thanks!",,"['calculus', 'integration', 'multivariable-calculus', 'multiple-integral']"
44,Verifying the divergence theorem; What is this surface?,Verifying the divergence theorem; What is this surface?,,"Verify the divergence theorem for the function $\textbf{V} = xy  \textbf{i} - y^2 \textbf{j} + z \textbf{k}$ and the surface enclosed by the three parts (i) $z = 0, s < 1, s^2 = x^2 + y^2$, (ii) $s = 1, 0 \le z \le 1$ and (iii) $z^2 = a^2 + (1 - a^2)s^2, 1 \le z \le a, a > 1$. Normally questions like these wouldn't pose a problem for me. However, I am having trouble interpreting the surfaces being described. Hopefully someone can shed some light on this.","Verify the divergence theorem for the function $\textbf{V} = xy  \textbf{i} - y^2 \textbf{j} + z \textbf{k}$ and the surface enclosed by the three parts (i) $z = 0, s < 1, s^2 = x^2 + y^2$, (ii) $s = 1, 0 \le z \le 1$ and (iii) $z^2 = a^2 + (1 - a^2)s^2, 1 \le z \le a, a > 1$. Normally questions like these wouldn't pose a problem for me. However, I am having trouble interpreting the surfaces being described. Hopefully someone can shed some light on this.",,['multivariable-calculus']
45,Regarding gauss law differential form,Regarding gauss law differential form,,"I have a big issue regarding the equality of integrands in gauss law. Given the integral form we have that $$\oint_{\partial\Omega}\vec{E}\cdot\vec{dS}=\int_{\Omega}\nabla\cdot \vec{E}dV={1\over \epsilon_0}\int_{\Omega}\rho \ dV$$ Here in this link https://physics.stackexchange.com/questions/23190/where-is-the-flaw-in-deriving-gausss-law-in-its-differential-form it says that we can conclude that $$\nabla\cdot \vec{E}={1\over \epsilon_0}\rho$$ because the equality of integrals is valid for all region $\Omega$ of the space. But how can we $\mathbf{formally}$ prove this result? so we can formulate the next theorem: Let $f,g:\mathbb R^3 \to \mathbb R$. Let $\Omega$ be any arbitrary region in $\mathbb R^3$ suppose that $$\int_{\Omega}f=\int_{\Omega}g$$ then $f=g$ Know even if this theorem holds, the second problem is that the region in gauss law is a closed region (becuase we use the divergence theorem) so my second question is that if the theorem would also be true just for closed regions? I would really appreciate if you can help me with this problem","I have a big issue regarding the equality of integrands in gauss law. Given the integral form we have that $$\oint_{\partial\Omega}\vec{E}\cdot\vec{dS}=\int_{\Omega}\nabla\cdot \vec{E}dV={1\over \epsilon_0}\int_{\Omega}\rho \ dV$$ Here in this link https://physics.stackexchange.com/questions/23190/where-is-the-flaw-in-deriving-gausss-law-in-its-differential-form it says that we can conclude that $$\nabla\cdot \vec{E}={1\over \epsilon_0}\rho$$ because the equality of integrals is valid for all region $\Omega$ of the space. But how can we $\mathbf{formally}$ prove this result? so we can formulate the next theorem: Let $f,g:\mathbb R^3 \to \mathbb R$. Let $\Omega$ be any arbitrary region in $\mathbb R^3$ suppose that $$\int_{\Omega}f=\int_{\Omega}g$$ then $f=g$ Know even if this theorem holds, the second problem is that the region in gauss law is a closed region (becuase we use the divergence theorem) so my second question is that if the theorem would also be true just for closed regions? I would really appreciate if you can help me with this problem",,"['multivariable-calculus', 'physics', 'vector-analysis', 'mathematical-physics']"
46,Approximation formula on a surface,Approximation formula on a surface,,"[Beginning calculus question.] We can get an approximation to the value of a function of two variables, I think, by saying $$ f(a+\Delta x , b+ \Delta y) \approx f(a,b) + f_x(a,b)\Delta x +f_y(a,b)\Delta y .$$ I am visualizing this by bending a piece of paper into a bendy surface, and putting a rigid clipboard tangent to it. I know the height of the paper at a point (a,b), according to an imagined coordinate system in the room. If I want to know the  height of a point on the surface nearby $f(a+\Delta x , b+ \Delta y)$, I could use that formula, by moving up the clipboard along the $x$ direction in the amount $f_x$ and up the $y$ direction in the amount $f_y$. However, if I first move up the appropriate amount along the $x$ axis, $f_y(x+\Delta x, y)$ could be different than it was at $f_y(x,y)$. But the approximation formula doesn't seem to take notice of that. It seems like I should be using a different $y$-derivative (without loss of generality), when I am moving along 2 axes, than if I were only moving along one. What am I missing? Why does the approximation formula work like that?","[Beginning calculus question.] We can get an approximation to the value of a function of two variables, I think, by saying $$ f(a+\Delta x , b+ \Delta y) \approx f(a,b) + f_x(a,b)\Delta x +f_y(a,b)\Delta y .$$ I am visualizing this by bending a piece of paper into a bendy surface, and putting a rigid clipboard tangent to it. I know the height of the paper at a point (a,b), according to an imagined coordinate system in the room. If I want to know the  height of a point on the surface nearby $f(a+\Delta x , b+ \Delta y)$, I could use that formula, by moving up the clipboard along the $x$ direction in the amount $f_x$ and up the $y$ direction in the amount $f_y$. However, if I first move up the appropriate amount along the $x$ axis, $f_y(x+\Delta x, y)$ could be different than it was at $f_y(x,y)$. But the approximation formula doesn't seem to take notice of that. It seems like I should be using a different $y$-derivative (without loss of generality), when I am moving along 2 axes, than if I were only moving along one. What am I missing? Why does the approximation formula work like that?",,"['multivariable-calculus', 'approximation']"
47,How to compute the area of the portion of a paraboloid cut off by a plane?,How to compute the area of the portion of a paraboloid cut off by a plane?,,"How to compute: The area of that portion of the paraboloid  $x^2+z^2=2ay$ which is cut off by the plane $y=a$ ? I think I have to compute $\iint f(x,z) dx dz$ , where $f(x,z)=\sqrt{(x^2+z^2)/2a}$ ; but I can't figure out what should be the limits of integration . Please help . Thanks in advance .","How to compute: The area of that portion of the paraboloid  $x^2+z^2=2ay$ which is cut off by the plane $y=a$ ? I think I have to compute $\iint f(x,z) dx dz$ , where $f(x,z)=\sqrt{(x^2+z^2)/2a}$ ; but I can't figure out what should be the limits of integration . Please help . Thanks in advance .",,['multivariable-calculus']
48,Why is divergence defined as $\mathbf{\nabla} \cdot \mathbf{v}$?,Why is divergence defined as ?,\mathbf{\nabla} \cdot \mathbf{v},"Suppose I am working in $\Bbb R^3$. Suppose I have a pond and I drop some dust on the surface. If the materials spread out, I have positive divergence, usually. Let $\mathbf{v}(\mathbf{x})$ denote the velocity of a dust particle. I assume my divergence is a measure of the magnitude of this particle's tendency to move away. So if I want to find the magnitude the particle will move, why wouldn't $||\mathbf{v}(\mathbf{x})||$ denote my divergence?","Suppose I am working in $\Bbb R^3$. Suppose I have a pond and I drop some dust on the surface. If the materials spread out, I have positive divergence, usually. Let $\mathbf{v}(\mathbf{x})$ denote the velocity of a dust particle. I assume my divergence is a measure of the magnitude of this particle's tendency to move away. So if I want to find the magnitude the particle will move, why wouldn't $||\mathbf{v}(\mathbf{x})||$ denote my divergence?",,"['multivariable-calculus', 'vector-analysis']"
49,"Minimal c satisfying $x+y-(xy)^c \geq 0$ for all $x,y\in [0,1]$",Minimal c satisfying  for all,"x+y-(xy)^c \geq 0 x,y\in [0,1]","What is the minimal real $c$ satisfying $x+y-(xy)^c \geq 0$ for all $x,y \in [0,1]$? Experimentally (though my experiments weren't necessarily accurate enough) I reached as low as $c=\tfrac{13}{32}$, where $c=\tfrac{12}{32}$ violates it.","What is the minimal real $c$ satisfying $x+y-(xy)^c \geq 0$ for all $x,y \in [0,1]$? Experimentally (though my experiments weren't necessarily accurate enough) I reached as low as $c=\tfrac{13}{32}$, where $c=\tfrac{12}{32}$ violates it.",,"['calculus', 'multivariable-calculus']"
50,How to find parametric equation of the intersection $x²+y²+z²=2$ and $y=x$?,How to find parametric equation of the intersection  and ?,x²+y²+z²=2 y=x,"I know that if I substitute $y=x$ into $x²+y²+z²=2$ I get $$2x²+z²=2$$ which in some way gives me $$x²+\frac{z²}{2} = 1$$ which is an ellipse. My parametric equation goes from $(0,0,\sqrt{2})$ to $(1,1,0)$, so I need to find an equation that follows this path. My idea was to take $x=t$ as a paremeter, and then I'd have: $\frac{z²}{2} = 1-x² \implies z² = 2(1-x²) \implies z = \sqrt{2}\sqrt{1-x²}$ (since $x\ge 0, y\ge 0, z\ge 0$ in my exercise) So I end up with the parametrization $$(t,t,\sqrt{2}\sqrt{1-t²})$$ But $t$ goes from what to what? Also, this will only work if $z\ge 0$. My teacher made something in the class that used $\sin(t)$ and $\cos(t)$ in the equation, how do I get this? Do I need to use spherical coordinates? Because she didn't use and I don't know how to do it.","I know that if I substitute $y=x$ into $x²+y²+z²=2$ I get $$2x²+z²=2$$ which in some way gives me $$x²+\frac{z²}{2} = 1$$ which is an ellipse. My parametric equation goes from $(0,0,\sqrt{2})$ to $(1,1,0)$, so I need to find an equation that follows this path. My idea was to take $x=t$ as a paremeter, and then I'd have: $\frac{z²}{2} = 1-x² \implies z² = 2(1-x²) \implies z = \sqrt{2}\sqrt{1-x²}$ (since $x\ge 0, y\ge 0, z\ge 0$ in my exercise) So I end up with the parametrization $$(t,t,\sqrt{2}\sqrt{1-t²})$$ But $t$ goes from what to what? Also, this will only work if $z\ge 0$. My teacher made something in the class that used $\sin(t)$ and $\cos(t)$ in the equation, how do I get this? Do I need to use spherical coordinates? Because she didn't use and I don't know how to do it.",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'parametric']"
51,${\int\int\int}_B dxdydz$ where $B$ is the region delimited by $x²+y²+z² = 4$ and $x²+y²=3z$,where  is the region delimited by  and,{\int\int\int}_B dxdydz B x²+y²+z² = 4 x²+y²=3z,"Take the following integral over the specified region: ${\int\int\int}_B dxdydz$ where $B$ is the region delimited by $x²+y²+z² = 4$ and $x²+y²=3z$ (i'm answering my own question because I was writing it and then found out my error, so I didn't want to erase all my work)","Take the following integral over the specified region: ${\int\int\int}_B dxdydz$ where $B$ is the region delimited by $x²+y²+z² = 4$ and $x²+y²=3z$ (i'm answering my own question because I was writing it and then found out my error, so I didn't want to erase all my work)",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'polar-coordinates']"
52,How to Prove $\oint ({\mathcal{\hat{r}}} \cdot \vec r') \mathrm{ d\vec l'} = -{\mathcal{\hat{r}}} \times \int \mathrm{d\vec a'}$?,How to Prove ?,\oint ({\mathcal{\hat{r}}} \cdot \vec r') \mathrm{ d\vec l'} = -{\mathcal{\hat{r}}} \times \int \mathrm{d\vec a'},$$\oint ({\mathcal{\hat{r}}} \cdot \vec r') \mathrm{ d\vec l'} = -{\mathcal{\hat{r}}} \times \int \mathrm{d\vec a'}$$ Here the integration in the LHS is around a certain loop and the $d\vec a'$ represents any surface enclosed by the loop.$\vec r' $ is the vector from origin to a point on the loop and $\mathcal{\hat{r}}$ is constant. I am unable to prove the integral.,$$\oint ({\mathcal{\hat{r}}} \cdot \vec r') \mathrm{ d\vec l'} = -{\mathcal{\hat{r}}} \times \int \mathrm{d\vec a'}$$ Here the integration in the LHS is around a certain loop and the $d\vec a'$ represents any surface enclosed by the loop.$\vec r' $ is the vector from origin to a point on the loop and $\mathcal{\hat{r}}$ is constant. I am unable to prove the integral.,,"['integration', 'multivariable-calculus', 'vectors', 'vector-analysis']"
53,Maximization via Lagrange multipliers vs. substitution and partial derivatives,Maximization via Lagrange multipliers vs. substitution and partial derivatives,,"Consider the example of maximizing $x^2 y z$ under the constraint that $x^2 + y^2 + z^2 = 5$. One way to do this is to use lagrange multipliers, solving the system of equations $$2xyz = 2x \lambda$$ $$x^2 z = 2 y \lambda$$ $$x^2 y = 2 z \lambda$$ $$x^2 + y^2 + z^2 = 5$$ However, couldn't you just substitute $x^2 = 5 - y^2 - z^2$ into the expression you want to maximize to get: $y z \left(-y^2-z^2+5\right)$ and then just maximize that by setting the $y$ and $z$ partial derivatives equal to zero? Then you just have to solve the arguably simpler system of equations: $$3 y^2 z+z^3=5 z$$ $$y^3+3 y z^2=5 y$$ where the $z$s and $y$s cancel out nicely on both sides. Why is maximize by lagrange multipliers necessary when you can always substitute and maximize the resulting function? When should you choose one over the other?","Consider the example of maximizing $x^2 y z$ under the constraint that $x^2 + y^2 + z^2 = 5$. One way to do this is to use lagrange multipliers, solving the system of equations $$2xyz = 2x \lambda$$ $$x^2 z = 2 y \lambda$$ $$x^2 y = 2 z \lambda$$ $$x^2 + y^2 + z^2 = 5$$ However, couldn't you just substitute $x^2 = 5 - y^2 - z^2$ into the expression you want to maximize to get: $y z \left(-y^2-z^2+5\right)$ and then just maximize that by setting the $y$ and $z$ partial derivatives equal to zero? Then you just have to solve the arguably simpler system of equations: $$3 y^2 z+z^3=5 z$$ $$y^3+3 y z^2=5 y$$ where the $z$s and $y$s cancel out nicely on both sides. Why is maximize by lagrange multipliers necessary when you can always substitute and maximize the resulting function? When should you choose one over the other?",,"['calculus', 'multivariable-calculus', 'optimization', 'partial-derivative', 'lagrange-multiplier']"
54,"Is every closed set , the set of zeroes (resp.critical points) of some smooth real valued function? [duplicate]","Is every closed set , the set of zeroes (resp.critical points) of some smooth real valued function? [duplicate]",,"This question already has answers here : Infinitely differentiable function with given zero set? (3 answers) Closed 8 years ago . Let $A$ be a closed subset of $\mathbb R^n$ : 1) Is it true that for some smooth function $f: \mathbb R^n \to \mathbb R$ , $A=f^{-1}(\{0\})$ 2)Is it true that for some smooth function $f: \mathbb R^n \to \mathbb R$ , $A$ is the set of all critical points of $f$ ?","This question already has answers here : Infinitely differentiable function with given zero set? (3 answers) Closed 8 years ago . Let $A$ be a closed subset of $\mathbb R^n$ : 1) Is it true that for some smooth function $f: \mathbb R^n \to \mathbb R$ , $A=f^{-1}(\{0\})$ 2)Is it true that for some smooth function $f: \mathbb R^n \to \mathbb R$ , $A$ is the set of all critical points of $f$ ?",,['multivariable-calculus']
55,Equivalent condition to differentiability of a function in a general set.,Equivalent condition to differentiability of a function in a general set.,,If $A\subset \mathbb{R}^k$ is an arbitrary set one said that $f:A \rightarrow \mathbb{R}^n$ is differentiable if for each point $x\in A$ exists an open set $U_x$ and a function $\tilde{f}:U_x \rightarrow \mathbb{R}^n$ such that $\tilde{f}$ is differentiable and $\tilde{f}|_{A \cap U_x}=f|_{A \cap U_x}$. Is this equivalent to the existence of a single open set $A\subset U$ and a differentiable function $\tilde{f}:U \rightarrow \mathbb{R}^n$ such that $\tilde{f}|_A=f|_A$? I try to use partitions of unity but I couldn't do it.,If $A\subset \mathbb{R}^k$ is an arbitrary set one said that $f:A \rightarrow \mathbb{R}^n$ is differentiable if for each point $x\in A$ exists an open set $U_x$ and a function $\tilde{f}:U_x \rightarrow \mathbb{R}^n$ such that $\tilde{f}$ is differentiable and $\tilde{f}|_{A \cap U_x}=f|_{A \cap U_x}$. Is this equivalent to the existence of a single open set $A\subset U$ and a differentiable function $\tilde{f}:U \rightarrow \mathbb{R}^n$ such that $\tilde{f}|_A=f|_A$? I try to use partitions of unity but I couldn't do it.,,"['real-analysis', 'general-topology', 'multivariable-calculus', 'derivatives']"
56,Rudin's application of the mean value theorem,Rudin's application of the mean value theorem,,"I am studying theorem 6.26 (page 152) in Rudin's ""Functional Analysis"" that presents distributions as derivatives of continuous functions. Right at the beginning of the proof, if $\Omega$ is the usual open subset, $Q = [0,1] ^n \subset \Omega$ is the unit cube and $\psi \in \mathcal D _Q (\Omega)$ (the space of test functions on $\Omega$ with support in $Q$), he applies the mean value theorem and states that $| \psi | \le \max \limits _{x \in Q} | ( \partial _i \psi ) (x) |$. What precise statement of the MVT does he use, and how? What I can think of is that the differential form of the MVT looks like $| \psi (x) - \psi (0) | \le \| \Bbb d \psi (\xi) \| \cdot \|x - 0\|$. Since $\text{supp} \ \psi \subset Q$ and $0 \in \partial Q$, then $\psi (0) = 0$, so the best I can get is $| \psi (x) | \le \| \Bbb d \psi (\xi) \| \cdot \|x\|$. Why is there no $\| x \|$ in Rudin's right-hand side? Does he work with $\| \cdot \| _\infty$, to make it $1$ on $Q$? But what then if $x \in \Omega \setminus Q$? And, in general, what is he doing here?","I am studying theorem 6.26 (page 152) in Rudin's ""Functional Analysis"" that presents distributions as derivatives of continuous functions. Right at the beginning of the proof, if $\Omega$ is the usual open subset, $Q = [0,1] ^n \subset \Omega$ is the unit cube and $\psi \in \mathcal D _Q (\Omega)$ (the space of test functions on $\Omega$ with support in $Q$), he applies the mean value theorem and states that $| \psi | \le \max \limits _{x \in Q} | ( \partial _i \psi ) (x) |$. What precise statement of the MVT does he use, and how? What I can think of is that the differential form of the MVT looks like $| \psi (x) - \psi (0) | \le \| \Bbb d \psi (\xi) \| \cdot \|x - 0\|$. Since $\text{supp} \ \psi \subset Q$ and $0 \in \partial Q$, then $\psi (0) = 0$, so the best I can get is $| \psi (x) | \le \| \Bbb d \psi (\xi) \| \cdot \|x\|$. Why is there no $\| x \|$ in Rudin's right-hand side? Does he work with $\| \cdot \| _\infty$, to make it $1$ on $Q$? But what then if $x \in \Omega \setminus Q$? And, in general, what is he doing here?",,"['calculus', 'functional-analysis', 'multivariable-calculus', 'distribution-theory']"
57,Setting limits on a Triple Integral,Setting limits on a Triple Integral,,"The original problem: Find the Moment of Inertia $I$ of a solid sphere of uniform density $\rho$ . $$$$ I thought of doing it using Triple Integrals. Moment of Inertia for Continuous Distribution of particles is $\int r^2 dm$ with suitable limits ( $r$ represents the radius while $dm$ is an elementary mass). $$dm=\rho dV$$ $$$$ Thus, the moment of Inertia can be represented as $$\int\int\int_V r^2\rho dV$$ From the Jacobian, $dV=r dr d\theta dz$ (converting into cylindrical coordinates). $$$$ Hence, $$I=\int\int\int r^3 \rho drd\theta dz$$ $$$$ I can't set limits on $r$ and $z$ . I would be  grateful if somebody could help.","The original problem: Find the Moment of Inertia of a solid sphere of uniform density . I thought of doing it using Triple Integrals. Moment of Inertia for Continuous Distribution of particles is with suitable limits ( represents the radius while is an elementary mass). Thus, the moment of Inertia can be represented as From the Jacobian, (converting into cylindrical coordinates). Hence, I can't set limits on and . I would be  grateful if somebody could help.",I \rho  \int r^2 dm r dm dm=\rho dV  \int\int\int_V r^2\rho dV dV=r dr d\theta dz  I=\int\int\int r^3 \rho drd\theta dz  r z,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
58,Tangent planes to $2+x^2+y^2$ and that contains the $x$ axis,Tangent planes to  and that contains the  axis,2+x^2+y^2 x,"I need to find the tangent planes to $f(x,y) = 2+x^2+y^2$ and that contains the $x$ axis, so that's what I did: $$z = z_0 + \frac{\partial f(x_0,y_0)}{\partial x}(x-x_0)+\frac{\partial f(x_0,y_0)}{\partial y}(y-y_0) \implies \\ z = 2 + x_0^2 + y_0^2 + 2x_0(x-x_0) + 2y_0(y-y_0) \implies \\ 2xx_0 + 2yy_0-z-x_0^2-y_0^2+2=0$$ So since the plane must contain the $x$ axis, its normal vector must have the form $(0,y,z)$. The normal vector fot the plane I found is: $$(2x_0, 2y_0, -1)$$ so $x_0 = 0, y_0 = y_0$ our plane has the form: $$2y_0y -z -y_0^2+2 = 0$$ but when I plot this graph for some values of $y_0$ I only get 1 tangent plane at $y_0\approx 1.5$","I need to find the tangent planes to $f(x,y) = 2+x^2+y^2$ and that contains the $x$ axis, so that's what I did: $$z = z_0 + \frac{\partial f(x_0,y_0)}{\partial x}(x-x_0)+\frac{\partial f(x_0,y_0)}{\partial y}(y-y_0) \implies \\ z = 2 + x_0^2 + y_0^2 + 2x_0(x-x_0) + 2y_0(y-y_0) \implies \\ 2xx_0 + 2yy_0-z-x_0^2-y_0^2+2=0$$ So since the plane must contain the $x$ axis, its normal vector must have the form $(0,y,z)$. The normal vector fot the plane I found is: $$(2x_0, 2y_0, -1)$$ so $x_0 = 0, y_0 = y_0$ our plane has the form: $$2y_0y -z -y_0^2+2 = 0$$ but when I plot this graph for some values of $y_0$ I only get 1 tangent plane at $y_0\approx 1.5$",,"['calculus', 'linear-algebra', 'multivariable-calculus']"
59,"Derivative of $f(3x+1,3x-1)=4$",Derivative of,"f(3x+1,3x-1)=4","This exercise asks me to take the derivative of $$f(3x+1,3x-1)=4$$ where this equality is said to be valid for all $x$. The exercise specifically asks me to prove that $$\frac{∂}{∂x}f(3x+1,3x-1)=-\frac{∂}{∂x}f(3x+1,3x-1)$$ The first thing I though was to apply the partial derivative operator to the both sides of the function: $$\frac{∂}{∂x}f(3x+1,3x-1)=\frac{∂}{∂x}4 \implies \frac{∂}{∂x}f(3x+1,3x-1) = 0 \implies \\ \frac{∂}{∂x}f(3x+1,3x-1) = - \frac{∂}{∂x}f(3x+1,3x-1)$$ but the exercise uses the chain rule, so I'm assuming that this can't be made. Could someone clarify for me what am I doing wrong here?","This exercise asks me to take the derivative of $$f(3x+1,3x-1)=4$$ where this equality is said to be valid for all $x$. The exercise specifically asks me to prove that $$\frac{∂}{∂x}f(3x+1,3x-1)=-\frac{∂}{∂x}f(3x+1,3x-1)$$ The first thing I though was to apply the partial derivative operator to the both sides of the function: $$\frac{∂}{∂x}f(3x+1,3x-1)=\frac{∂}{∂x}4 \implies \frac{∂}{∂x}f(3x+1,3x-1) = 0 \implies \\ \frac{∂}{∂x}f(3x+1,3x-1) = - \frac{∂}{∂x}f(3x+1,3x-1)$$ but the exercise uses the chain rule, so I'm assuming that this can't be made. Could someone clarify for me what am I doing wrong here?",,"['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
60,Finding the net outward flux of a sphere,Finding the net outward flux of a sphere,,"Use the Divergence Theorem to compute the net outward flux of: $$ F = \langle x^2, y^2, z^2 \rangle $$ $S$ is the sphere: $$ \{(x,y,z): x^2 + y^2 + z^2 = 25\} $$ First, I took: $$ \nabla \cdot F = 2x + 2y + 2z $$ Then, I tried setting up the triple integral with spherical coordinates, but it is just not working out for me.","Use the Divergence Theorem to compute the net outward flux of: $$ F = \langle x^2, y^2, z^2 \rangle $$ $S$ is the sphere: $$ \{(x,y,z): x^2 + y^2 + z^2 = 25\} $$ First, I took: $$ \nabla \cdot F = 2x + 2y + 2z $$ Then, I tried setting up the triple integral with spherical coordinates, but it is just not working out for me.",,['multivariable-calculus']
61,Using Stokes' Theorem Finding $\int_C{F\bullet dr}$,Using Stokes' Theorem Finding,\int_C{F\bullet dr},"Suppose that $C$ is the intersection of $z=2x+5y$ and $x^2+y^2=1$ which is oriented counterclockwise when viewed from above. Now let $$F=\langle \sin{x}+y, \sin{y}+z, \sin{z}+x \rangle$$ How can I find $\int_C{F\bullet dr}$? So far I know that the intersection is between a plane and a cylinder. And I think that Stokes' Theorem should be usable in this problem.","Suppose that $C$ is the intersection of $z=2x+5y$ and $x^2+y^2=1$ which is oriented counterclockwise when viewed from above. Now let $$F=\langle \sin{x}+y, \sin{y}+z, \sin{z}+x \rangle$$ How can I find $\int_C{F\bullet dr}$? So far I know that the intersection is between a plane and a cylinder. And I think that Stokes' Theorem should be usable in this problem.",,['multivariable-calculus']
62,How to solve this 2-D integration?,How to solve this 2-D integration?,,"How to solve  $$I=\int_{[0,1]^2}\frac{dxdy}{(1+xy)(1+x^2)}$$ I've tried using the diffeomorphism $(x,y)=(u,v/u)$ from $\text{int}\{(u,v)\mid 0\le u\le 1,0\le v\le u\}$ to $\text{int}[0,1]^2$, but since the Jaccobian determinant becomes unbounded at the boundary, the transformation leads to an improper integral, which I think makes it invalid. If I integrate directly over the original domain, the result will be too complicated to calculate. I can't come up with another diffeomorphism. Is there another method to go on?","How to solve  $$I=\int_{[0,1]^2}\frac{dxdy}{(1+xy)(1+x^2)}$$ I've tried using the diffeomorphism $(x,y)=(u,v/u)$ from $\text{int}\{(u,v)\mid 0\le u\le 1,0\le v\le u\}$ to $\text{int}[0,1]^2$, but since the Jaccobian determinant becomes unbounded at the boundary, the transformation leads to an improper integral, which I think makes it invalid. If I integrate directly over the original domain, the result will be too complicated to calculate. I can't come up with another diffeomorphism. Is there another method to go on?",,"['calculus', 'integration', 'multivariable-calculus']"
63,"Computing the volume of this weird object,","Computing the volume of this weird object,",,"Let $f: [-1,1] \to \mathbb{R}$ be a continuously differentiable function such that $f(-1) = f(1) = 0$ and $0<f(x)\le 1$ for all $x \in (-1,1)$.  Let $S$ be the surface in $\mathbb{R}^3$ obtained by revolving the curve $y=f(x)$ around the $x$-axis and $V$ the volume inside $S$.  Determine the volume of the object obtained by removing this volume $V$ from the inside of the sphere of radius $2$ centered at $0$. Edit: I think evgeny's hint (see below) in the comments is a good one to go with, so I computed the volume V of the surface S with triple integration of the 1-function, with $-1 \le x \le 1$ $-1 \le z \le 1$ -min{f(x), $\sqrt(1-x^2)$} $\le y \le$ +min{f(x), $\sqrt{1-x^2}$}, as my bounds of integration. The volume of the object is then $\frac {32\pi}{3}$ - 8*min{f(x), $\sqrt{1-x^2}$}. Feel free to comment on this answer or offer another solution.  My only concern is that we don't seem to have used the $C^1$ condition on f.  (Unless it was given primarily so that we know the function is smooth enough from x=-1 to x=1, e.g., no sharp corners along the curve.)","Let $f: [-1,1] \to \mathbb{R}$ be a continuously differentiable function such that $f(-1) = f(1) = 0$ and $0<f(x)\le 1$ for all $x \in (-1,1)$.  Let $S$ be the surface in $\mathbb{R}^3$ obtained by revolving the curve $y=f(x)$ around the $x$-axis and $V$ the volume inside $S$.  Determine the volume of the object obtained by removing this volume $V$ from the inside of the sphere of radius $2$ centered at $0$. Edit: I think evgeny's hint (see below) in the comments is a good one to go with, so I computed the volume V of the surface S with triple integration of the 1-function, with $-1 \le x \le 1$ $-1 \le z \le 1$ -min{f(x), $\sqrt(1-x^2)$} $\le y \le$ +min{f(x), $\sqrt{1-x^2}$}, as my bounds of integration. The volume of the object is then $\frac {32\pi}{3}$ - 8*min{f(x), $\sqrt{1-x^2}$}. Feel free to comment on this answer or offer another solution.  My only concern is that we don't seem to have used the $C^1$ condition on f.  (Unless it was given primarily so that we know the function is smooth enough from x=-1 to x=1, e.g., no sharp corners along the curve.)",,"['real-analysis', 'multivariable-calculus', 'derivatives', 'continuity', 'volume']"
64,Can I calculate the scalar potential from the electric field using: $\phi = \int \nabla \phi = - \int \vec{E}$,Can I calculate the scalar potential from the electric field using:,\phi = \int \nabla \phi = - \int \vec{E},"If I have a relation between the electric field and the radius, can I calculate the relation between the scalar potential and the radius using: $\phi = \int \nabla \phi = - \int \vec{E}$? $$\nabla \phi = - \vec{E}$$  $$\phi = \int \nabla \phi = - \int \vec{E}$$? If so, how do I fix the vector to magnitude conversion for the equation below? $E = \frac{\lambda}{2\pi \epsilon_0 r}$ $\phi =  - \int \vec{E} = -\int \frac{\lambda}{2\pi \epsilon_0 r} dr = -\frac{\lambda}{2\pi \epsilon_0} ln(r)$? When calculation $- \int \vec{E}$, but $E = \frac{\lambda}{2\pi \epsilon_0 r}$ is not a vector.","If I have a relation between the electric field and the radius, can I calculate the relation between the scalar potential and the radius using: $\phi = \int \nabla \phi = - \int \vec{E}$? $$\nabla \phi = - \vec{E}$$  $$\phi = \int \nabla \phi = - \int \vec{E}$$? If so, how do I fix the vector to magnitude conversion for the equation below? $E = \frac{\lambda}{2\pi \epsilon_0 r}$ $\phi =  - \int \vec{E} = -\int \frac{\lambda}{2\pi \epsilon_0 r} dr = -\frac{\lambda}{2\pi \epsilon_0} ln(r)$? When calculation $- \int \vec{E}$, but $E = \frac{\lambda}{2\pi \epsilon_0 r}$ is not a vector.",,"['calculus', 'multivariable-calculus', 'physics']"
65,"relation between $\frac{\partial(x,0)}{\partial x}$ and $\left.\frac{\partial(x,t)}{\partial x}\right|_{t=0}$",relation between  and,"\frac{\partial(x,0)}{\partial x} \left.\frac{\partial(x,t)}{\partial x}\right|_{t=0}","if $u(x,t)$ differentiable function and i only have $u(x,0)$, then is it right $\frac{\partial(x,0)}{\partial x} = \left.\frac{\partial(x,t)}{\partial x}\right|_{t=0}$ or can i derive $u(x,0)$ to $x$ without to have to know exactly $u(x,t)$? What the counterexample if it's wrong and what's  the prove or the theorem to use if it's right? Thank you for attention.","if $u(x,t)$ differentiable function and i only have $u(x,0)$, then is it right $\frac{\partial(x,0)}{\partial x} = \left.\frac{\partial(x,t)}{\partial x}\right|_{t=0}$ or can i derive $u(x,0)$ to $x$ without to have to know exactly $u(x,t)$? What the counterexample if it's wrong and what's  the prove or the theorem to use if it's right? Thank you for attention.",,"['calculus', 'multivariable-calculus']"
66,Find the flux across a part of the surface $\frac{x^2}{4}+\frac{y^2}{9}+\frac{z^2}{16}=1$,Find the flux across a part of the surface,\frac{x^2}{4}+\frac{y^2}{9}+\frac{z^2}{16}=1,"Consider the vector field $$F(x, y, z)= \frac{(x{\rm i} + y{\rm j} + z{\rm k})} {(x ^2+ y ^2 + z ^2)^\frac{3}{2}},$$ and let $S$ be the part of the surface $$\frac{x^2}{4}+\frac{y^2}{9}+\frac{z^2}{16}=1$$ in the first octant bounded by the planes $y = 0, y =x\sqrt{3}$ and $z = 0$, oriented upwards. Find the flux of $F$ across the surface $S$.","Consider the vector field $$F(x, y, z)= \frac{(x{\rm i} + y{\rm j} + z{\rm k})} {(x ^2+ y ^2 + z ^2)^\frac{3}{2}},$$ and let $S$ be the part of the surface $$\frac{x^2}{4}+\frac{y^2}{9}+\frac{z^2}{16}=1$$ in the first octant bounded by the planes $y = 0, y =x\sqrt{3}$ and $z = 0$, oriented upwards. Find the flux of $F$ across the surface $S$.",,"['calculus', 'multivariable-calculus', 'vector-analysis', 'surface-integrals']"
67,Mean Value Theorem Like Statement About Manifolds,Mean Value Theorem Like Statement About Manifolds,,"Let $S$ be a connected $m$-dimensional embedded subamnifold in $\mathbf R^m\times \mathbf R^n$. Suppose that $S$ intersects $\{\mathbf 0\}\times \mathbf R^n$ at two different points. Conjecture: Then there is a point $\mathbf p$ on $S$ at which $S$ is not transverse to $\mathbf R^n$, that is, $T_{\mathbf p}S\cap \mathbf R^n\neq \{\mathbf 0\}$. (One may assume that $S$ admits a global chart because right now that is the most important case for me.) The statement is true if $m=n=1$, for then the situation is almost like that in Rolle's Theorem. I came up with the above ""conjecture"" while trying to prove the implicit function theorem. Can somebody help? Thanks.","Let $S$ be a connected $m$-dimensional embedded subamnifold in $\mathbf R^m\times \mathbf R^n$. Suppose that $S$ intersects $\{\mathbf 0\}\times \mathbf R^n$ at two different points. Conjecture: Then there is a point $\mathbf p$ on $S$ at which $S$ is not transverse to $\mathbf R^n$, that is, $T_{\mathbf p}S\cap \mathbf R^n\neq \{\mathbf 0\}$. (One may assume that $S$ admits a global chart because right now that is the most important case for me.) The statement is true if $m=n=1$, for then the situation is almost like that in Rolle's Theorem. I came up with the above ""conjecture"" while trying to prove the implicit function theorem. Can somebody help? Thanks.",,"['multivariable-calculus', 'differential-geometry', 'smooth-manifolds']"
68,Multivariable Calculus with Tensors,Multivariable Calculus with Tensors,,I'm looking for a book at the undergraduate level on multivariable calculus (for a 2nd course of multivariable calculus) that introduces and makes use of tensors to describe higher order derivatives -- and maybe differential forms for integration.  Does anyone know of a multivariable book that takes this tact?,I'm looking for a book at the undergraduate level on multivariable calculus (for a 2nd course of multivariable calculus) that introduces and makes use of tensors to describe higher order derivatives -- and maybe differential forms for integration.  Does anyone know of a multivariable book that takes this tact?,,"['multivariable-calculus', 'reference-request', 'tensors', 'book-recommendation']"
69,Integral Inequality for Bound on Gradient of Solution to Heat Equation,Integral Inequality for Bound on Gradient of Solution to Heat Equation,,"My overall aim is to show that, for a bounded solution $u(x,t)$ to the heat equation in $\mathbb{R}^n \times [0,T]$ with boundary condition $u(x,0) = f(x)$, $$\max |\nabla u(x,T) | \leq \frac C{\sqrt{T}} \max \left|f\right|.$$ I've been able to bound the term on the left by $$\left(\frac 1{(4 \pi t)^n 4t^2} \int_{\mathbb{R}^n} f(y)^2 e^{-\frac{|x-y|^2}{2t}} |x-y| dy\right)^{\frac12}.$$ How can I show that this term is less than or equal to the right side of my original inequality? EDIT: I can actually get a slightly stronger bound on the left by using Cauchy-Schwarz in earlier calculations: $$\left(\frac 1{(4 \pi t)^n 4t^2} \int_{\mathbb{R}^n} f(y)^2 dy \int_{\mathbb{R}^n} e^{-\frac{|x-y|^2}{2t}} |x-y| dy.\right)^{\frac12}$$ If $y \in \mathbb{R}^1$ then the second integral becomes simply $2t$, though I'm not sure how the evaluation changes in higher dimensions. But the first integral is, of course, the $L^2$-norm for the function f (once you take into account the power of $1/2$ around the whole expression) so that's something at least.","My overall aim is to show that, for a bounded solution $u(x,t)$ to the heat equation in $\mathbb{R}^n \times [0,T]$ with boundary condition $u(x,0) = f(x)$, $$\max |\nabla u(x,T) | \leq \frac C{\sqrt{T}} \max \left|f\right|.$$ I've been able to bound the term on the left by $$\left(\frac 1{(4 \pi t)^n 4t^2} \int_{\mathbb{R}^n} f(y)^2 e^{-\frac{|x-y|^2}{2t}} |x-y| dy\right)^{\frac12}.$$ How can I show that this term is less than or equal to the right side of my original inequality? EDIT: I can actually get a slightly stronger bound on the left by using Cauchy-Schwarz in earlier calculations: $$\left(\frac 1{(4 \pi t)^n 4t^2} \int_{\mathbb{R}^n} f(y)^2 dy \int_{\mathbb{R}^n} e^{-\frac{|x-y|^2}{2t}} |x-y| dy.\right)^{\frac12}$$ If $y \in \mathbb{R}^1$ then the second integral becomes simply $2t$, though I'm not sure how the evaluation changes in higher dimensions. But the first integral is, of course, the $L^2$-norm for the function f (once you take into account the power of $1/2$ around the whole expression) so that's something at least.",,"['real-analysis', 'multivariable-calculus', 'partial-differential-equations', 'vector-analysis']"
70,Verifying Stokes' Theorem on two intersecting cylinders,Verifying Stokes' Theorem on two intersecting cylinders,,"Let $S\subset \mathbb{R}^3$ be the portion of the cylinder $y^2 + z^2 = 4$ with $z>0$ and $x^2 + y^2 ≤ 1$. Let $\mathbf{f}(x,y,z) = (zx-y)\mathbf{i}$, where $\mathbf{i}$ is the usual unit vector in the $x$-direction. Verify that Stokes' Theorem is satisfied in this case. So this question is in two parts: I first have to evaluate $\iint_S \nabla \times \mathbf{f} \cdot \mathrm{d}\mathbf{S}$, then $\oint_{\partial S}\mathbf{f} \cdot \mathrm{d} \mathbf{r}$, and finally show that they are equal. My Attempt I first calculated $\nabla \times \mathbf{f}$ to be $\begin{vmatrix}\ \mathbf{i} & \mathbf{j} & \mathbf{k}\\ \frac{\partial}{\partial x} & \frac{\partial}{\partial y} & \frac{\partial}{\partial z}\\ (zx-y) & 0 & 0\\ \end{vmatrix} = (0,x,1).$ I then parameterised the cylinder by $\mathbf{r}(\theta,z) = (\cos\theta,\sin\theta,z)$ , with my normal $\frac{\partial \mathbf{r}}{\partial \theta} \times \frac{\partial \mathbf{r}}{\partial z} = (\cos\theta,\sin\theta,0)$ which is outward pointing as required. And my integral becomes $$\begin{align} \int_{\theta = 0} ^{2\pi}\int_{z=0}^{\sqrt{4-\sin^2\theta}}(0,\cos\theta,1)\cdot(\cos\theta,\sin\theta,0)\mathrm{d}z\mathrm{d}\theta & = \int_{\theta = 0} ^{2\pi}\int_{z=0}^{\sqrt{4-\sin^2\theta}}\sin\theta\cos\theta\mathrm{d}z\mathrm{d}\theta\\ & = \frac{1}{2}\int_{\theta = 0} ^{2\pi}\sin2\theta\sqrt{4-\sin^2\theta}\mathrm{d}\theta\end{align}$$ at which point I am stuck. I think perhaps my parameterisation is wrong, or maybe I have to calculate the surface integrals in two steps, but the latter method didn't seem to work either and I can't find any better parameterisations. I also am having trouble on how to parameterise the line integral I need to compute. Should there be two boundaries which I need to combine to find the whole boundary $\partial S$?","Let $S\subset \mathbb{R}^3$ be the portion of the cylinder $y^2 + z^2 = 4$ with $z>0$ and $x^2 + y^2 ≤ 1$. Let $\mathbf{f}(x,y,z) = (zx-y)\mathbf{i}$, where $\mathbf{i}$ is the usual unit vector in the $x$-direction. Verify that Stokes' Theorem is satisfied in this case. So this question is in two parts: I first have to evaluate $\iint_S \nabla \times \mathbf{f} \cdot \mathrm{d}\mathbf{S}$, then $\oint_{\partial S}\mathbf{f} \cdot \mathrm{d} \mathbf{r}$, and finally show that they are equal. My Attempt I first calculated $\nabla \times \mathbf{f}$ to be $\begin{vmatrix}\ \mathbf{i} & \mathbf{j} & \mathbf{k}\\ \frac{\partial}{\partial x} & \frac{\partial}{\partial y} & \frac{\partial}{\partial z}\\ (zx-y) & 0 & 0\\ \end{vmatrix} = (0,x,1).$ I then parameterised the cylinder by $\mathbf{r}(\theta,z) = (\cos\theta,\sin\theta,z)$ , with my normal $\frac{\partial \mathbf{r}}{\partial \theta} \times \frac{\partial \mathbf{r}}{\partial z} = (\cos\theta,\sin\theta,0)$ which is outward pointing as required. And my integral becomes $$\begin{align} \int_{\theta = 0} ^{2\pi}\int_{z=0}^{\sqrt{4-\sin^2\theta}}(0,\cos\theta,1)\cdot(\cos\theta,\sin\theta,0)\mathrm{d}z\mathrm{d}\theta & = \int_{\theta = 0} ^{2\pi}\int_{z=0}^{\sqrt{4-\sin^2\theta}}\sin\theta\cos\theta\mathrm{d}z\mathrm{d}\theta\\ & = \frac{1}{2}\int_{\theta = 0} ^{2\pi}\sin2\theta\sqrt{4-\sin^2\theta}\mathrm{d}\theta\end{align}$$ at which point I am stuck. I think perhaps my parameterisation is wrong, or maybe I have to calculate the surface integrals in two steps, but the latter method didn't seem to work either and I can't find any better parameterisations. I also am having trouble on how to parameterise the line integral I need to compute. Should there be two boundaries which I need to combine to find the whole boundary $\partial S$?",,"['integration', 'multivariable-calculus', 'line-integrals', 'surface-integrals']"
71,Differential Form Pullback Definition,Differential Form Pullback Definition,,"I'm having some difficulty following how Spivak (Calculus on Manifolds) has induced the pullback on page 89. From reading elsewhere online it seems convention is to define the induced map of the pushforward of a differentiable function $f: \mathbb R^n \to \mathbb R^m$ with the corresponding linear transformation $Df(p): \mathbb R^n \to \mathbb R^m$ as $$f_*: \mathbb R^n_p \to\mathbb R^m_{f(p)}$$$$f_*(v_p) = (Df(p)(v))_{f(p)}$$ and to then use this definition for the pullback, defined as $$f^*:\Lambda(\mathbb R^m_{f(p)})\to \Lambda(\mathbb R^n_p)$$$$f^*\omega(p)(v_1, .., v_k) = \omega(f(p))(f_*(v_1),..., f_*(v_k)),$$where $\omega$ is a k-form on $\mathbb R^m.$ However Spivak has offered the induced definition for the pullback as $$(f^*\omega)(p) = f^*(\omega(f(p))).$$ which then leads to the above definition. I'd be grateful for any help explaining the intuition behind this.","I'm having some difficulty following how Spivak (Calculus on Manifolds) has induced the pullback on page 89. From reading elsewhere online it seems convention is to define the induced map of the pushforward of a differentiable function $f: \mathbb R^n \to \mathbb R^m$ with the corresponding linear transformation $Df(p): \mathbb R^n \to \mathbb R^m$ as $$f_*: \mathbb R^n_p \to\mathbb R^m_{f(p)}$$$$f_*(v_p) = (Df(p)(v))_{f(p)}$$ and to then use this definition for the pullback, defined as $$f^*:\Lambda(\mathbb R^m_{f(p)})\to \Lambda(\mathbb R^n_p)$$$$f^*\omega(p)(v_1, .., v_k) = \omega(f(p))(f_*(v_1),..., f_*(v_k)),$$where $\omega$ is a k-form on $\mathbb R^m.$ However Spivak has offered the induced definition for the pullback as $$(f^*\omega)(p) = f^*(\omega(f(p))).$$ which then leads to the above definition. I'd be grateful for any help explaining the intuition behind this.",,"['multivariable-calculus', 'differential-geometry', 'differential-forms']"
72,Cone into cartesian coordinates,Cone into cartesian coordinates,,"Let $K$ be the surface of an inﬁnite cone with circular cross section, vertex at the origin and axis lying along the positive $z$-axis. If the angle between the $z$-axis and the surface of the cone is $α$, ﬁnd expressions for $K$ in terms of Cartesian, spherical and cylindrical polar coordinates. Solution: In terms of spherical polars, $K = \{(ρ,ϕ,θ) : 0 \leq ρ < ∞,ϕ = α,0 \leq θ < 2π\}$. By simple trigonometry, $tanα = r/z = (\sqrt{x^2 + y^2})/z$. In terms of Cartesian coodinates then, $K = \{(x,y,z) : x^2 +y^2 = z \tan α\}$ and, in terms of cylindrical polars, $K = \{(r,θ,z) : r = z tanα,0 \leq θ < 2π\}$. For the Cartesian, shouldn't it say $(z \tan α)^2$ instead, since it is the radius?","Let $K$ be the surface of an inﬁnite cone with circular cross section, vertex at the origin and axis lying along the positive $z$-axis. If the angle between the $z$-axis and the surface of the cone is $α$, ﬁnd expressions for $K$ in terms of Cartesian, spherical and cylindrical polar coordinates. Solution: In terms of spherical polars, $K = \{(ρ,ϕ,θ) : 0 \leq ρ < ∞,ϕ = α,0 \leq θ < 2π\}$. By simple trigonometry, $tanα = r/z = (\sqrt{x^2 + y^2})/z$. In terms of Cartesian coodinates then, $K = \{(x,y,z) : x^2 +y^2 = z \tan α\}$ and, in terms of cylindrical polars, $K = \{(r,θ,z) : r = z tanα,0 \leq θ < 2π\}$. For the Cartesian, shouldn't it say $(z \tan α)^2$ instead, since it is the radius?",,['multivariable-calculus']
73,An improper triple integral where the domain is a cube,An improper triple integral where the domain is a cube,,"I am trying to find the value of the following integral: $$\iiint\limits_D {{x^2} + {y^2}dV}$$ where the domain $D$ is a cube $0 \leqslant x,y,z \leqslant 1$. Solution (attempt 1) The first thing I did was to rewrite the domain, adding the upper/lower bounds (infinity in my case), so the domain becomes $$0 \leqslant x < \infty , \quad -\infty  < y < \infty ,\quad -\infty  < z \leqslant 1$$ This is good since it states clear upper/lower bounds for each iteration. $$\iiint\limits_D {{x^2} + {y^2}dV} = \mathop {\lim }\limits_{R \to \infty } \int\limits_{ - R}^R {{y^2}dy\int\limits_0^R {{x^2}dx\int\limits_0^1 {dz = } } } \\ =\mathop {\lim }\limits_{R \to \infty } \left[ {\frac{{{y^3}}}{3}} \right]_{ - R}^R\left[ {\frac{{{x^3}}}{3}} \right]_0^R\left[ z \right]_0^1 = \mathop {\lim }\limits_{R \to \infty } \frac{{2{R^6}}}{9}(1 - R)$$ Solution (attempt 2) I observed at least one symmetry in the integral with respect to $y$. In other words, it can be rewritten as: $$\iiint\limits_D {{x^2} + {y^2}dV} = \mathop {\lim }\limits_{R \to \infty } \int\limits_{ - R}^R {{y^2}dy\int\limits_0^R {{x^2}dx\int\limits_0^1 {dz = } } } \mathop {\lim }\limits_{R \to \infty } 2\int\limits_0^R {{y^2}dy\int\limits_0^R {{x^2}dx\int\limits_0^1 {dz =\\= \mathop {\lim }\limits_{R \to \infty } } } } 2\left[ {\frac{{{y^3}}}{3}} \right]_0^R\left[ {\frac{{{x^3}}}{3}} \right]_0^R\left[ z \right]_0^1 = \frac{{2{R^6}}}{9}$$ The strange observation It's strange that both values support the idea that the hypervolume is infinitely big, but when I look in the key, it's simply $\frac{2}{3}$. Does anyone see the flaw in my reasoning?","I am trying to find the value of the following integral: $$\iiint\limits_D {{x^2} + {y^2}dV}$$ where the domain $D$ is a cube $0 \leqslant x,y,z \leqslant 1$. Solution (attempt 1) The first thing I did was to rewrite the domain, adding the upper/lower bounds (infinity in my case), so the domain becomes $$0 \leqslant x < \infty , \quad -\infty  < y < \infty ,\quad -\infty  < z \leqslant 1$$ This is good since it states clear upper/lower bounds for each iteration. $$\iiint\limits_D {{x^2} + {y^2}dV} = \mathop {\lim }\limits_{R \to \infty } \int\limits_{ - R}^R {{y^2}dy\int\limits_0^R {{x^2}dx\int\limits_0^1 {dz = } } } \\ =\mathop {\lim }\limits_{R \to \infty } \left[ {\frac{{{y^3}}}{3}} \right]_{ - R}^R\left[ {\frac{{{x^3}}}{3}} \right]_0^R\left[ z \right]_0^1 = \mathop {\lim }\limits_{R \to \infty } \frac{{2{R^6}}}{9}(1 - R)$$ Solution (attempt 2) I observed at least one symmetry in the integral with respect to $y$. In other words, it can be rewritten as: $$\iiint\limits_D {{x^2} + {y^2}dV} = \mathop {\lim }\limits_{R \to \infty } \int\limits_{ - R}^R {{y^2}dy\int\limits_0^R {{x^2}dx\int\limits_0^1 {dz = } } } \mathop {\lim }\limits_{R \to \infty } 2\int\limits_0^R {{y^2}dy\int\limits_0^R {{x^2}dx\int\limits_0^1 {dz =\\= \mathop {\lim }\limits_{R \to \infty } } } } 2\left[ {\frac{{{y^3}}}{3}} \right]_0^R\left[ {\frac{{{x^3}}}{3}} \right]_0^R\left[ z \right]_0^1 = \frac{{2{R^6}}}{9}$$ The strange observation It's strange that both values support the idea that the hypervolume is infinitely big, but when I look in the key, it's simply $\frac{2}{3}$. Does anyone see the flaw in my reasoning?",,"['calculus', 'multivariable-calculus']"
74,What is a parametrized surface? How is it different from a surface? (Multivariable Calculus),What is a parametrized surface? How is it different from a surface? (Multivariable Calculus),,"My textbook defines it like this: Let F be a continuous function from a subset D(F) R 2 into R q . Suppose that D(F) is pathwise connected, and that every point in D(F) is either an interior point of D(F), or on the boundary of the interior of D(F).  Then F is called a parametrized surface in R q . I understand what everything in the definition means and understand how to check for all the conditions.  However, I do NOT understand what type of surface is being described.  How do these special qualities of the domain enhance our understanding of the surface? Can someone explain what a parametrized surface is (intuitively, not formally) and how that relates to this formal definition? What consequences are their to a surface being a ""parametrized surface?""","My textbook defines it like this: Let F be a continuous function from a subset D(F) R 2 into R q . Suppose that D(F) is pathwise connected, and that every point in D(F) is either an interior point of D(F), or on the boundary of the interior of D(F).  Then F is called a parametrized surface in R q . I understand what everything in the definition means and understand how to check for all the conditions.  However, I do NOT understand what type of surface is being described.  How do these special qualities of the domain enhance our understanding of the surface? Can someone explain what a parametrized surface is (intuitively, not formally) and how that relates to this formal definition? What consequences are their to a surface being a ""parametrized surface?""",,"['linear-algebra', 'general-topology', 'multivariable-calculus', 'surfaces']"
75,Does differentiability have a geometric interpretation for high dimensional functions?,Does differentiability have a geometric interpretation for high dimensional functions?,,"A function $f:\mathbb{R^n} \to \mathbb{R^m}$ is said to be differentiable if the limit $$ \lim_{x \to a} \dfrac{||f(x) - f(a) - D_{f(a)}(x-a) ||}{||x-a ||}=0$$ exists where $D_{f(a)}$ is the $m \times n$ matrix of partial derivatives of the function $f$. Now, if the function is a scalar valued function, where $m=1$ then there is a clear geometric interpretation: $L(x)= f(a) + D_{f(a)}(x-a)$ is the function of the linear approximation for $f$ in the neighborhood of $a$. It is a line for $n=1$, a plane for $n=2$ and hyperplane for higher dimensional input spaces. Is there such an interpretation as well when $f$ is a vector function where $m>1$? When this is the case and $f(x)=(u_1(x),\dots,u_m(x))^T$, then the definition of the derivative defines separate linear approximations for each $u_i(x)$ component of $f(x)$ via the each row of the partial derivative matrix $D_{f(a)}$. This looks like a straightforward expansion of the scalar case to vector valued functions. But I cannot imagine a geometric justification of this like in the scalar case (where $m=1$). So, is this just an obvious generalization of the definition of differentiability to vector valued functions or does it have a geometric justification as well, like in the $m=1$ case? (Linear approximation around $a$)","A function $f:\mathbb{R^n} \to \mathbb{R^m}$ is said to be differentiable if the limit $$ \lim_{x \to a} \dfrac{||f(x) - f(a) - D_{f(a)}(x-a) ||}{||x-a ||}=0$$ exists where $D_{f(a)}$ is the $m \times n$ matrix of partial derivatives of the function $f$. Now, if the function is a scalar valued function, where $m=1$ then there is a clear geometric interpretation: $L(x)= f(a) + D_{f(a)}(x-a)$ is the function of the linear approximation for $f$ in the neighborhood of $a$. It is a line for $n=1$, a plane for $n=2$ and hyperplane for higher dimensional input spaces. Is there such an interpretation as well when $f$ is a vector function where $m>1$? When this is the case and $f(x)=(u_1(x),\dots,u_m(x))^T$, then the definition of the derivative defines separate linear approximations for each $u_i(x)$ component of $f(x)$ via the each row of the partial derivative matrix $D_{f(a)}$. This looks like a straightforward expansion of the scalar case to vector valued functions. But I cannot imagine a geometric justification of this like in the scalar case (where $m=1$). So, is this just an obvious generalization of the definition of differentiability to vector valued functions or does it have a geometric justification as well, like in the $m=1$ case? (Linear approximation around $a$)",,"['calculus', 'real-analysis', 'multivariable-calculus', 'derivatives']"
76,Why we cannot simplify $\partial x$?,Why we cannot simplify ?,\partial x,"First consider the formula: $$\frac{df}{dt}=\frac{df}{dx}\frac{dx}{dt}$$ As we can see, $dx$ can be simplified from the RHS to get the LHS. This can be explained like this: define $y=x'(c)(t-c)+x(c)$ the tangent of $x(t)$ at $c$. Then $dx$ in $df/dx$ is simply $\Delta x$, while $dx$ in $\frac{dx}{dt}$ is $\Delta y$. And since $\Delta x\approx \Delta y$ when $\Delta t$ is very small, we can simplify $dx$ in the above formula. Now consider the formula: $$\frac{\partial f}{\partial u}=\frac{\partial f}{\partial x}\frac{\partial x}{\partial u}+\frac{\partial f}{\partial y}\frac{\partial y}{\partial u}$$ where $x,y$ are functions with variables $u,v$ and $f$ is function with variables $x,y$. Of course we cannot simplify $\partial x$. I tried to use the same argument for $dx$, but it is more difficult to imagine. Maybe I need some exact definition of $\partial f$ and $\partial x$ (just a guess, maybe the definition is related to a tangent plane?) Thanks so much.","First consider the formula: $$\frac{df}{dt}=\frac{df}{dx}\frac{dx}{dt}$$ As we can see, $dx$ can be simplified from the RHS to get the LHS. This can be explained like this: define $y=x'(c)(t-c)+x(c)$ the tangent of $x(t)$ at $c$. Then $dx$ in $df/dx$ is simply $\Delta x$, while $dx$ in $\frac{dx}{dt}$ is $\Delta y$. And since $\Delta x\approx \Delta y$ when $\Delta t$ is very small, we can simplify $dx$ in the above formula. Now consider the formula: $$\frac{\partial f}{\partial u}=\frac{\partial f}{\partial x}\frac{\partial x}{\partial u}+\frac{\partial f}{\partial y}\frac{\partial y}{\partial u}$$ where $x,y$ are functions with variables $u,v$ and $f$ is function with variables $x,y$. Of course we cannot simplify $\partial x$. I tried to use the same argument for $dx$, but it is more difficult to imagine. Maybe I need some exact definition of $\partial f$ and $\partial x$ (just a guess, maybe the definition is related to a tangent plane?) Thanks so much.",,"['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
77,Rewrite triple integral from Cartesian to cylindrical or spherical coordinates,Rewrite triple integral from Cartesian to cylindrical or spherical coordinates,,"How to rewrite this integral in either cylindrical or spherical coordinates? (whichever is easier). $$\int_0^1 \int_0^x \int_0^{ \sqrt{x^2+y^2}} {(x^2+y^2)^{3/2}\over x^2+y^2+z^2}\,dz\,dy\,dx$$","How to rewrite this integral in either cylindrical or spherical coordinates? (whichever is easier). $$\int_0^1 \int_0^x \int_0^{ \sqrt{x^2+y^2}} {(x^2+y^2)^{3/2}\over x^2+y^2+z^2}\,dz\,dy\,dx$$",,"['calculus', 'multivariable-calculus']"
78,Showing differentiability for a multivariable piecewise function,Showing differentiability for a multivariable piecewise function,,"Let   $$f(x,y)=\begin{cases} xy\sin(x/y) & y\neq 0 \\ 0 & y=0\end{cases},$$   show whether $f(x,y)$ is differentiable at $(0,0)$. It seems that there are multiple ways to do this but there is no clear example online or in texbooks.. The provided solution begins with showing that the two partial derivatives equal zero when at $(0, 0)$, to me they look like they're supposed to be undefined.. using the definition of partial derivatives, isn't this the case: $$ f_x=xcos(x/y)+ysin(x/y) \\ f_y=ysin(x/y)-\frac{x}{y}sin(x/y) $$ ?? and even if they were true, the solution uses the definition of differential df to prove the case. Solution is this: I vaguely understand what it's doing because the existence of a tagent plane approximation implies differentiability. But I can't seem to understand where the $\epsilon_1$ and $\epsilon_2$ came from.","Let   $$f(x,y)=\begin{cases} xy\sin(x/y) & y\neq 0 \\ 0 & y=0\end{cases},$$   show whether $f(x,y)$ is differentiable at $(0,0)$. It seems that there are multiple ways to do this but there is no clear example online or in texbooks.. The provided solution begins with showing that the two partial derivatives equal zero when at $(0, 0)$, to me they look like they're supposed to be undefined.. using the definition of partial derivatives, isn't this the case: $$ f_x=xcos(x/y)+ysin(x/y) \\ f_y=ysin(x/y)-\frac{x}{y}sin(x/y) $$ ?? and even if they were true, the solution uses the definition of differential df to prove the case. Solution is this: I vaguely understand what it's doing because the existence of a tagent plane approximation implies differentiability. But I can't seem to understand where the $\epsilon_1$ and $\epsilon_2$ came from.",,"['multivariable-calculus', 'derivatives', 'partial-derivative']"
79,Time derivative of Jacobian,Time derivative of Jacobian,,"Say I have a function $f(p) : \mathbb{R}^3 \to \mathbb{R}$, where $p = (x,y,z)^T$. I know that the Jacobian $J$ is $f_p = (f_x, f_y, f_z)$. I know that the time derivative of the Jacobian, $J'$, is $\dfrac{\partial J}{\partial p} \cdot\dfrac{dp}{dt}$. The quantity $\dfrac{dp}{dt}$ is simply the velocity $v$ at $p$. But what is $\dfrac{\partial J}{\partial p}$? Is it the Jacobian of the Jacobian? How can I express this as a matrix (and not a tensor)? Context: I am trying to apply equation (11) of Constrained Dynamics .","Say I have a function $f(p) : \mathbb{R}^3 \to \mathbb{R}$, where $p = (x,y,z)^T$. I know that the Jacobian $J$ is $f_p = (f_x, f_y, f_z)$. I know that the time derivative of the Jacobian, $J'$, is $\dfrac{\partial J}{\partial p} \cdot\dfrac{dp}{dt}$. The quantity $\dfrac{dp}{dt}$ is simply the velocity $v$ at $p$. But what is $\dfrac{\partial J}{\partial p}$? Is it the Jacobian of the Jacobian? How can I express this as a matrix (and not a tensor)? Context: I am trying to apply equation (11) of Constrained Dynamics .",,['multivariable-calculus']
80,Differentiating a multivariable function,Differentiating a multivariable function,,"Knowing that $$z(x,y)=f(\frac{x}{y})$$I'm supposed to find $$x\frac{\partial z}{\partial x} + y\frac{\partial z}{\partial y}$$ . This problem makes no sense to me, can anyone help with the differentiation ? I really don't get how to apply the chain rule this time .","Knowing that $$z(x,y)=f(\frac{x}{y})$$I'm supposed to find $$x\frac{\partial z}{\partial x} + y\frac{\partial z}{\partial y}$$ . This problem makes no sense to me, can anyone help with the differentiation ? I really don't get how to apply the chain rule this time .",,"['multivariable-calculus', 'partial-derivative']"
81,Limit of non-linear multi-variable function,Limit of non-linear multi-variable function,,"I'm trying to prove the limit of the following function is $0$: $\lim_{(x,y) \to (1,-1)} {x^3} - {2xy^2} + 1$ I know that I'm trying to find a $\delta$ s.t $ 0 < \sqrt{(x - 1)^2 + (y + 1)^2} < \delta $ which implies $|{x^3} - {2xy^2} + 1| < \epsilon$ I tried factoring $x$ so that $|x({x^2} - {2y^2}) + 1|$ and then adding 1 and - 1 using the triangle inequality to try to simplify into the expression I wanted, but I don't seem to be getting anywhere. I think I should try to get $|\sqrt{y^2}|$ so I could say it's less than $|\sqrt{ (x - 1)^2 + (y + 1)^2}|$, and subsequently define what $\delta$ is, but I'm just having a hard time how I could factorize this with the $+1$ in the problem. Any suggestions?","I'm trying to prove the limit of the following function is $0$: $\lim_{(x,y) \to (1,-1)} {x^3} - {2xy^2} + 1$ I know that I'm trying to find a $\delta$ s.t $ 0 < \sqrt{(x - 1)^2 + (y + 1)^2} < \delta $ which implies $|{x^3} - {2xy^2} + 1| < \epsilon$ I tried factoring $x$ so that $|x({x^2} - {2y^2}) + 1|$ and then adding 1 and - 1 using the triangle inequality to try to simplify into the expression I wanted, but I don't seem to be getting anywhere. I think I should try to get $|\sqrt{y^2}|$ so I could say it's less than $|\sqrt{ (x - 1)^2 + (y + 1)^2}|$, and subsequently define what $\delta$ is, but I'm just having a hard time how I could factorize this with the $+1$ in the problem. Any suggestions?",,"['calculus', 'multivariable-calculus']"
82,Proving that something is a manifold from the definition,Proving that something is a manifold from the definition,,"Consider a set $$M = \{ (s\cos t, s\sin t, t) \colon s,t\in \mathbb{R}\}\subset \mathbb{R}^3.$$ I am asked to show from the definition that $M$ is a 2-dimensional submanifold of $\mathbb{R}^3$ which is say for each $x\in M$ there is an open set $U$ (relative to $M$), an open set $V\subset \mathbb{R}^2$ together with a diffeomorphism $\varphi\colon U\to M$. Building $\varphi$ (subject to $x$) seems to be difficult. I tried to employ some version of theorem about local dipheomorphisms but no version seems to be applicable here. May I ask for help?","Consider a set $$M = \{ (s\cos t, s\sin t, t) \colon s,t\in \mathbb{R}\}\subset \mathbb{R}^3.$$ I am asked to show from the definition that $M$ is a 2-dimensional submanifold of $\mathbb{R}^3$ which is say for each $x\in M$ there is an open set $U$ (relative to $M$), an open set $V\subset \mathbb{R}^2$ together with a diffeomorphism $\varphi\colon U\to M$. Building $\varphi$ (subject to $x$) seems to be difficult. I tried to employ some version of theorem about local dipheomorphisms but no version seems to be applicable here. May I ask for help?",,"['multivariable-calculus', 'manifolds', 'smooth-manifolds']"
83,Improper integral (is it convergent?),Improper integral (is it convergent?),,"I would like to either prove or disapprove the following: Let $\alpha\in (-1/2,0)$ be given. Then we can find $\gamma \in (1,2)$ such that $$\int_0^1 \int_0^{u} \frac{((1-v)^{\alpha}-(1-u)^{\alpha})^2}{(u-v)^{\gamma}}dvdu < \infty.$$ I have tried a couple of things with no success.. Any ideas are welcome. Thank you guys!","I would like to either prove or disapprove the following: Let $\alpha\in (-1/2,0)$ be given. Then we can find $\gamma \in (1,2)$ such that $$\int_0^1 \int_0^{u} \frac{((1-v)^{\alpha}-(1-u)^{\alpha})^2}{(u-v)^{\gamma}}dvdu < \infty.$$ I have tried a couple of things with no success.. Any ideas are welcome. Thank you guys!",,"['real-analysis', 'integration', 'multivariable-calculus', 'improper-integrals']"
84,Computing $\int_B x^2y^3$ over $1 \leq xy \leq 2$ and $x \leq y \leq 4x$.,Computing  over  and .,\int_B x^2y^3 1 \leq xy \leq 2 x \leq y \leq 4x,"Let B be the portion of the first quadrant in $\Bbb R^2$ lying between the hyperbolas $xy=1, xy=2$ and lines $y=x, y=4x$. Evaluate $$\int _Bx^2y^3.$$ Hint: set $x=u/v$ and $y=uv$. First, I substituted the $u$'s and $v$'s and got values for $u$ and $v$. However, I do not know how to proceed further. Can someone please help me to use Change of Variables theorem to finish it? My idea was just to substitute $u$'s and $v$'s into the integral and give them the values I have got from finding where the functions cross. But I am not sure. The answer I got was $1$. It is the first time I am doing these so I am not good at it yet. Any help would be appreciated! Thanks in advance!","Let B be the portion of the first quadrant in $\Bbb R^2$ lying between the hyperbolas $xy=1, xy=2$ and lines $y=x, y=4x$. Evaluate $$\int _Bx^2y^3.$$ Hint: set $x=u/v$ and $y=uv$. First, I substituted the $u$'s and $v$'s and got values for $u$ and $v$. However, I do not know how to proceed further. Can someone please help me to use Change of Variables theorem to finish it? My idea was just to substitute $u$'s and $v$'s into the integral and give them the values I have got from finding where the functions cross. But I am not sure. The answer I got was $1$. It is the first time I am doing these so I am not good at it yet. Any help would be appreciated! Thanks in advance!",,"['integration', 'multivariable-calculus']"
85,Curl Vector: What exactly is rotating?,Curl Vector: What exactly is rotating?,,"I am a little bit confused over the exact conceptual meaning of the curl vector. So I am familiar with the paddle wheel interpretation, but I don't think I am satisfied with that analogy because it could give different meanings. There are two ways which I am imagining the curl. To make this explanation easier, I will talk about a fluid on a two dimensional plane where the vector field is the velocity vector field given by F = M(x,y) i + N(x,y) j , as usual. So when we talk about some fluid, say water, and we say there is a curl vector field on the plane, are we saying that it is the water molecules themselves which are rotating about their axis due to this field? (suppose that these water molecules are perfect spheres and it's axis is parallel to the z-axis).  In other words, is it the water molecule itself that is located at (x_o,y_o) rotating about it's axis which is parallel to the z-axis? Or, are we saying that these water molecules (perhaps some bunch) are ""rotating"" in the sense that they follow a circular path which is centered at (x_o, y_o) and so it is not actually the molecules itself rotating about their own axis. I am thinking it is the first one. Thanks in advance! P.S. if anyone would like the edit my question using Latex, please do so! Thank you.","I am a little bit confused over the exact conceptual meaning of the curl vector. So I am familiar with the paddle wheel interpretation, but I don't think I am satisfied with that analogy because it could give different meanings. There are two ways which I am imagining the curl. To make this explanation easier, I will talk about a fluid on a two dimensional plane where the vector field is the velocity vector field given by F = M(x,y) i + N(x,y) j , as usual. So when we talk about some fluid, say water, and we say there is a curl vector field on the plane, are we saying that it is the water molecules themselves which are rotating about their axis due to this field? (suppose that these water molecules are perfect spheres and it's axis is parallel to the z-axis).  In other words, is it the water molecule itself that is located at (x_o,y_o) rotating about it's axis which is parallel to the z-axis? Or, are we saying that these water molecules (perhaps some bunch) are ""rotating"" in the sense that they follow a circular path which is centered at (x_o, y_o) and so it is not actually the molecules itself rotating about their own axis. I am thinking it is the first one. Thanks in advance! P.S. if anyone would like the edit my question using Latex, please do so! Thank you.",,"['calculus', 'multivariable-calculus', 'vector-analysis']"
86,Partial derivative and change of coordinates,Partial derivative and change of coordinates,,"A colleague posted this on the door outside his office: $$\frac{\partial}{\partial(x+y)}(xy)=?$$ Trying to be helpful, I gave it a shot: $$u = x + y \\v = x - y \\ x = \frac{u+v}{2} \\ y = \frac{u-v}{2} \\ xy = \frac{u^2 - v^2}{4}$$ Then $$\frac{\partial}{\partial(x+y)}(xy) = \frac{1}{4}\frac{\partial}{\partial u}(u^2 - v^2) = \frac{u}{2} = \frac{x+y}{2}.$$ Then when I got back to my desk I was concerned that I skimmed over some details and missed a constant somewhere, since I'm changing the scale with the change of variables.  I'd like to try with $$u' = \frac{x+y}{\sqrt{2}} \\ v' = \frac{x-y}{\sqrt{2}}$$ so that the scale doesn't change: $$x = \frac{u' + v'}{\sqrt{2}} \\ y = \frac{u' - v'}{\sqrt{2}}$$ But then I'm left with interpreting $$\frac{\partial}{\partial \left(\sqrt{2}u'\right)},$$ which I'm not sure how to do. Does my conern matter?","A colleague posted this on the door outside his office: $$\frac{\partial}{\partial(x+y)}(xy)=?$$ Trying to be helpful, I gave it a shot: $$u = x + y \\v = x - y \\ x = \frac{u+v}{2} \\ y = \frac{u-v}{2} \\ xy = \frac{u^2 - v^2}{4}$$ Then $$\frac{\partial}{\partial(x+y)}(xy) = \frac{1}{4}\frac{\partial}{\partial u}(u^2 - v^2) = \frac{u}{2} = \frac{x+y}{2}.$$ Then when I got back to my desk I was concerned that I skimmed over some details and missed a constant somewhere, since I'm changing the scale with the change of variables.  I'd like to try with $$u' = \frac{x+y}{\sqrt{2}} \\ v' = \frac{x-y}{\sqrt{2}}$$ so that the scale doesn't change: $$x = \frac{u' + v'}{\sqrt{2}} \\ y = \frac{u' - v'}{\sqrt{2}}$$ But then I'm left with interpreting $$\frac{\partial}{\partial \left(\sqrt{2}u'\right)},$$ which I'm not sure how to do. Does my conern matter?",,['multivariable-calculus']
87,Can Anyone help me with Lagrange multiplier problem,Can Anyone help me with Lagrange multiplier problem,,"I need to find absolute maximum and minimum of thi function $$F(x,y) = x^{2} - y^{2} - 2y$$   over $$R = \{ (x,y)\ |\ x^{2} + {y^2} \leq 1\} $$ Thanks for help","I need to find absolute maximum and minimum of thi function $$F(x,y) = x^{2} - y^{2} - 2y$$   over $$R = \{ (x,y)\ |\ x^{2} + {y^2} \leq 1\} $$ Thanks for help",,"['multivariable-calculus', 'lagrange-multiplier']"
88,Parameterize Tangent Line to Level Curve,Parameterize Tangent Line to Level Curve,,"$$f(x, y) = x^2y + yx - xy^2$$ Find a parameterization of the tangent line to the level curve of $f$ through the point $(1, 1)$ . I have already computed the gradient of $f$ and found the critical points. Any help would be appreciated. Thanks!",Find a parameterization of the tangent line to the level curve of through the point . I have already computed the gradient of and found the critical points. Any help would be appreciated. Thanks!,"f(x, y) = x^2y + yx - xy^2 f (1, 1) f",['multivariable-calculus']
89,"Given a vector field $\mathbf{H}$, find a vector field $\mathbf{F}$ and a scalar field g, such that $\mathbf{H}$ = curl(F) + ∇(g).","Given a vector field , find a vector field  and a scalar field g, such that  = curl(F) + ∇(g).",\mathbf{H} \mathbf{F} \mathbf{H},"Let$\;\mathbf{H}(x,y,z) = x^2y\mathbf{i}+y^2z\mathbf{j}+z^2x\mathbf{k}$. Find a vector field $\mathbf{F}$ and a scalar field g, such that $\mathbf{H}$ = curl(F) + ∇(g). I took divergence on both sides which gave me $2xy+2yz+2xz=∇^2g$. I took curl on both sides which gave me three horrible equations one of which I have written down below: $$\frac{\partial^2f_1}{\partial y^2}-\frac{\partial^2f_2}{\partial x\partial y}-\frac{\partial^2f_3}{\partial x\partial z}+\frac{\partial^2f_1}{\partial z\partial y}=-y^2$$ where I have taken $$\mathbf{F}=f_1\mathbf{i}+f_2\mathbf{j}+f_3\mathbf{k}$$ I tried to assign some values hoping to work out the rest but this gave me nothing. Please help.","Let$\;\mathbf{H}(x,y,z) = x^2y\mathbf{i}+y^2z\mathbf{j}+z^2x\mathbf{k}$. Find a vector field $\mathbf{F}$ and a scalar field g, such that $\mathbf{H}$ = curl(F) + ∇(g). I took divergence on both sides which gave me $2xy+2yz+2xz=∇^2g$. I took curl on both sides which gave me three horrible equations one of which I have written down below: $$\frac{\partial^2f_1}{\partial y^2}-\frac{\partial^2f_2}{\partial x\partial y}-\frac{\partial^2f_3}{\partial x\partial z}+\frac{\partial^2f_1}{\partial z\partial y}=-y^2$$ where I have taken $$\mathbf{F}=f_1\mathbf{i}+f_2\mathbf{j}+f_3\mathbf{k}$$ I tried to assign some values hoping to work out the rest but this gave me nothing. Please help.",,"['multivariable-calculus', 'vector-analysis']"
90,Converting plane equation from $ax+by+cz=d$ to $r=a+\lambda b+\mu c$,Converting plane equation from  to,ax+by+cz=d r=a+\lambda b+\mu c,"The equation of the plane Π is   $$2x + 3y + 4z= 48$$ Obtain a vector equation of Π in the form   $r = a + λb + μc$,   where a, b and c are of the form pi , qi + rj and si + tk respectively, and where $p, q, r, s, t$ are integers My Attempt: $a=(p,0,0)$ $2p+3(0)+4(0)=48 \implies p=24$ Then for the other variables I wrote down two equations: (1) $(2,3,4) \cdot (x,y,z)= (24+\lambda q+ μ s, \lambda r, μ t)$ (2) Cross product of $b$ and $c$ is equal to $(2,3,4)$ This gives $(rt,-qt,rs)=(2,3,4)$ It seems impossible to me. Too many  variables that my mind can't process. Please help.","The equation of the plane Π is   $$2x + 3y + 4z= 48$$ Obtain a vector equation of Π in the form   $r = a + λb + μc$,   where a, b and c are of the form pi , qi + rj and si + tk respectively, and where $p, q, r, s, t$ are integers My Attempt: $a=(p,0,0)$ $2p+3(0)+4(0)=48 \implies p=24$ Then for the other variables I wrote down two equations: (1) $(2,3,4) \cdot (x,y,z)= (24+\lambda q+ μ s, \lambda r, μ t)$ (2) Cross product of $b$ and $c$ is equal to $(2,3,4)$ This gives $(rt,-qt,rs)=(2,3,4)$ It seems impossible to me. Too many  variables that my mind can't process. Please help.",,"['multivariable-calculus', 'vectors']"
91,How to check extrema if second derivative test fails,How to check extrema if second derivative test fails,,"I have to find minima and maxima of $f(x,y)=x^4+6y^2-4xy^3-1$ I found three points that could be extrema - $(0,0)$, $(1,1)$ and $(-1,-1)$ I already checked $(1,1)$ and $(-1,-1)$ with second derivative test, but determinant of Hesian at $(0,0)$ equals $0$ and I can't show that there is no extremum (but maybe there is?). What do I do now?","I have to find minima and maxima of $f(x,y)=x^4+6y^2-4xy^3-1$ I found three points that could be extrema - $(0,0)$, $(1,1)$ and $(-1,-1)$ I already checked $(1,1)$ and $(-1,-1)$ with second derivative test, but determinant of Hesian at $(0,0)$ equals $0$ and I can't show that there is no extremum (but maybe there is?). What do I do now?",,"['calculus', 'multivariable-calculus']"
92,Existence of partial derivative,Existence of partial derivative,,"I know how to compute partial derivatives of functions with more than one variable. But how can i assert that the partial derivatives of a given function exist at a point without computing it? Consider the following functions for example: $f(x,y)=xy(x^2-y^2)/(x^2+y^2)$ and $f(0,0)=0$ How do I show that the first partial derivative of $f(x,y)$ exists at $0$? Thanks for you help in advance!","I know how to compute partial derivatives of functions with more than one variable. But how can i assert that the partial derivatives of a given function exist at a point without computing it? Consider the following functions for example: $f(x,y)=xy(x^2-y^2)/(x^2+y^2)$ and $f(0,0)=0$ How do I show that the first partial derivative of $f(x,y)$ exists at $0$? Thanks for you help in advance!",,"['real-analysis', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
93,Evaluating a double integral,Evaluating a double integral,,"I have to evaluate this double integral: $$\int_0^1\int_0^1\cos\ (\max \ \{x^3,y^{\frac{3}{2}} \} )\ dxdy$$ I have hint with me that this is to be done with help of Greens theorem but i dont know how to start it Please help me with this. Thanks","I have to evaluate this double integral: $$\int_0^1\int_0^1\cos\ (\max \ \{x^3,y^{\frac{3}{2}} \} )\ dxdy$$ I have hint with me that this is to be done with help of Greens theorem but i dont know how to start it Please help me with this. Thanks",,['multivariable-calculus']
94,I don't understand the calculus problem from my note?,I don't understand the calculus problem from my note?,,I don't understand the highlight part in my note from class. Why we have to multiply this? Can someone explain it for me? Why $n-1$ power? Thanks!,I don't understand the highlight part in my note from class. Why we have to multiply this? Can someone explain it for me? Why $n-1$ power? Thanks!,,['calculus']
95,Proof of Gauss theorem (divergence theorem) in $\mathbb R^2$,Proof of Gauss theorem (divergence theorem) in,\mathbb R^2,"I am trying to solve an exercise in where it is asked to show the divergence theorem, or also known as Gauss theorem, in $\mathbb R^2$ using Green's theorem. I suppose that the divergence theorem in $\mathbb R^2$ is that for a $C^1$ vector field $F:\mathbb R^2 \to \mathbb R^2$ defined on a $3$ type region $D$ such that $\partial D$ is a closed curve, we have $$\iint_D div(F)dA=\int_{\partial D} Fds$$ I've tried to prove this theorem applying Green's theorem but I coudn't, I would appreciate if someone could provide a solution using Green's theorem (or at least the steps I should follow to prove the equality).","I am trying to solve an exercise in where it is asked to show the divergence theorem, or also known as Gauss theorem, in $\mathbb R^2$ using Green's theorem. I suppose that the divergence theorem in $\mathbb R^2$ is that for a $C^1$ vector field $F:\mathbb R^2 \to \mathbb R^2$ defined on a $3$ type region $D$ such that $\partial D$ is a closed curve, we have $$\iint_D div(F)dA=\int_{\partial D} Fds$$ I've tried to prove this theorem applying Green's theorem but I coudn't, I would appreciate if someone could provide a solution using Green's theorem (or at least the steps I should follow to prove the equality).",,"['integration', 'multivariable-calculus']"
96,"If $f:[a,b]\to \mathbb R^2$ has non vanishing derivative then $f(x)=y$ has finitely many solutions",If  has non vanishing derivative then  has finitely many solutions,"f:[a,b]\to \mathbb R^2 f(x)=y","I can prove this claim if the derivative is further assumed continuous, i.e. $f\in C^1$: Assume $f_i(x)=y_i$, $i=1,2$ had infinitely many solutions $t_n\in [a,b]$. By compactness, $t_n$ has a monotone subsequence that converges, WLOG we take the subsequence $p_n$ to be stricty increasing. As $f_i(p_n)=f_i(p_{n+1})$ we obtain $\xi_{i,n}\in [p_n,p_{n+1}]$ with $f'_i(\xi_{i,n})=0$ by the Mean Value Theorem. But if $p_n\to p$ then $\xi_{i,n}\to p$ hence $f'_i(p)=0$ contradicting that $(f'_1,f'_2)\neq (0,0)$. The last argument ($f'_i(p)=0$) doesn't apply if the derivative of $f$ is not assumed continuous.","I can prove this claim if the derivative is further assumed continuous, i.e. $f\in C^1$: Assume $f_i(x)=y_i$, $i=1,2$ had infinitely many solutions $t_n\in [a,b]$. By compactness, $t_n$ has a monotone subsequence that converges, WLOG we take the subsequence $p_n$ to be stricty increasing. As $f_i(p_n)=f_i(p_{n+1})$ we obtain $\xi_{i,n}\in [p_n,p_{n+1}]$ with $f'_i(\xi_{i,n})=0$ by the Mean Value Theorem. But if $p_n\to p$ then $\xi_{i,n}\to p$ hence $f'_i(p)=0$ contradicting that $(f'_1,f'_2)\neq (0,0)$. The last argument ($f'_i(p)=0$) doesn't apply if the derivative of $f$ is not assumed continuous.",,"['real-analysis', 'multivariable-calculus']"
97,Changing order of integration (multiple integral),Changing order of integration (multiple integral),,"Prove $$ \int_0^a\left( \int_0^x \left( \int_0^y \left( \int_0^z f(u) \, du \right) dz \right) dy \right) dx = \int_0^a \frac {(a-t)^3}{3!} f(t) dt $$ where $a$ is constant. So I began with two most inner integrals i.e. the double integral $$ \int_0^y \left( \int_0^z f(u) du \right) dz $$ We are doing this over $0 \leq u \leq z \leq y \leq x \leq a$. So we want $$ \int_0^y \left( \int_0^z f(u) du \right) dz = \int_?^? \left( \int_?^? f(u) dz \right) du  $$ And immediately this problem got me stumped. How can one tell what the upper/lower bounds become? Keep in mind that drawing this region won't do much good as we are working in four dimensions. EDIT: Forgot something crucial, edited now!","Prove $$ \int_0^a\left( \int_0^x \left( \int_0^y \left( \int_0^z f(u) \, du \right) dz \right) dy \right) dx = \int_0^a \frac {(a-t)^3}{3!} f(t) dt $$ where $a$ is constant. So I began with two most inner integrals i.e. the double integral $$ \int_0^y \left( \int_0^z f(u) du \right) dz $$ We are doing this over $0 \leq u \leq z \leq y \leq x \leq a$. So we want $$ \int_0^y \left( \int_0^z f(u) du \right) dz = \int_?^? \left( \int_?^? f(u) dz \right) du  $$ And immediately this problem got me stumped. How can one tell what the upper/lower bounds become? Keep in mind that drawing this region won't do much good as we are working in four dimensions. EDIT: Forgot something crucial, edited now!",,"['integration', 'multivariable-calculus', 'definite-integrals']"
98,Improper integral $\int_{B}\frac {1}{|x|^\alpha}dV$,Improper integral,\int_{B}\frac {1}{|x|^\alpha}dV,"Let B be the ball $|x|\le 1$, $x\in R^n$. For what $\alpha$ does $$\int_{B}\frac {1}{|x|^\alpha}dV$$ exists? I find it hard when it comes to generalize this statement in $R^n$. I've been able to do something for $n=2$ and $n=3$, always stating $r^2=|x|$, and working with polar or spherical coordinates, but it's imposible to keep going, obiously. Any ideas?","Let B be the ball $|x|\le 1$, $x\in R^n$. For what $\alpha$ does $$\int_{B}\frac {1}{|x|^\alpha}dV$$ exists? I find it hard when it comes to generalize this statement in $R^n$. I've been able to do something for $n=2$ and $n=3$, always stating $r^2=|x|$, and working with polar or spherical coordinates, but it's imposible to keep going, obiously. Any ideas?",,"['real-analysis', 'multivariable-calculus', 'improper-integrals']"
99,Parametrizing to Calculate Flux,Parametrizing to Calculate Flux,,"Evaluate the flux of $\mathbf{f}$ across the oriented surface $\Sigma$ by computing the surface integral $\iint_{\Sigma} \mathbf{f} \cdot d\sigma$, where $\Sigma$ is the surface $z=xe^y$ for $0 \leq x \leq 1$ and $0 \leq y \leq 1$ with upward orientation. The vector field is $\mathbf{f}(x,y,z)=\langle xy, 4x^2, yz \rangle$. Using the surface integral to evaluate flux, $$\iint_{\Sigma} \mathbf{f} \cdot d\sigma= \iint_R f(x(u,v)),y(u,v),z(u,v)) \left|\left| \frac{\partial r}{\partial u} \times \frac{\partial r}{\partial u} \right|\right| du dv$$ What would $u$ and $v$ be? I'm not sure how to parametrize this.","Evaluate the flux of $\mathbf{f}$ across the oriented surface $\Sigma$ by computing the surface integral $\iint_{\Sigma} \mathbf{f} \cdot d\sigma$, where $\Sigma$ is the surface $z=xe^y$ for $0 \leq x \leq 1$ and $0 \leq y \leq 1$ with upward orientation. The vector field is $\mathbf{f}(x,y,z)=\langle xy, 4x^2, yz \rangle$. Using the surface integral to evaluate flux, $$\iint_{\Sigma} \mathbf{f} \cdot d\sigma= \iint_R f(x(u,v)),y(u,v),z(u,v)) \left|\left| \frac{\partial r}{\partial u} \times \frac{\partial r}{\partial u} \right|\right| du dv$$ What would $u$ and $v$ be? I'm not sure how to parametrize this.",,"['multivariable-calculus', 'surfaces']"
