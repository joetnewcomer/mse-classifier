,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,What is the difference between first and second right eigenvectors of a row stochastic matrix and their meaning?,What is the difference between first and second right eigenvectors of a row stochastic matrix and their meaning?,,In an $n\times n$ non negative row stochastic matrix (rows sum up to 1). The entries of the stochastic matrix I have represent directed links between countries. Why is the first right eigenvector a vector of ones and what does that imply in that case? Why do we want to calculate the second right eigenvector in that case?,In an $n\times n$ non negative row stochastic matrix (rows sum up to 1). The entries of the stochastic matrix I have represent directed links between countries. Why is the first right eigenvector a vector of ones and what does that imply in that case? Why do we want to calculate the second right eigenvector in that case?,,"['linear-algebra', 'matrices', 'graph-theory', 'eigenvalues-eigenvectors', 'algebraic-graph-theory']"
1,Commuting Skew-symmetric Nilpotent 4x4 Matrices,Commuting Skew-symmetric Nilpotent 4x4 Matrices,,"Suppose $A$ and $B$ are nonzero, commuting, skew-symmetric, nilpotent matrices in $M_4(k)$, $k$ a field (char $k\ne 2$).  Must $A=\lambda B$ for some $\lambda\in k$?  I have shown that this is true for $3\times 3$ matrices, and I believe it should also be true for $4\times 4$ matrices. Thanks in advance to anyone willing to help me with this fairly dry question.","Suppose $A$ and $B$ are nonzero, commuting, skew-symmetric, nilpotent matrices in $M_4(k)$, $k$ a field (char $k\ne 2$).  Must $A=\lambda B$ for some $\lambda\in k$?  I have shown that this is true for $3\times 3$ matrices, and I believe it should also be true for $4\times 4$ matrices. Thanks in advance to anyone willing to help me with this fairly dry question.",,['linear-algebra']
2,Which of the following statements is not necessarily true?,Which of the following statements is not necessarily true?,,Let $A∈M_{5×6}(\mathbb R)$ and $B∈M_{6×5}(\mathbb R)$ such that $(AB)^{100}=0$. Which of the following statements is not necessarily true? (a) $(AB)^{4}=0$ (b) $(BA)^{6}=0$ (c) $\det(BA)=0$ (d) $\operatorname{tr}(AB)=0$,Let $A∈M_{5×6}(\mathbb R)$ and $B∈M_{6×5}(\mathbb R)$ such that $(AB)^{100}=0$. Which of the following statements is not necessarily true? (a) $(AB)^{4}=0$ (b) $(BA)^{6}=0$ (c) $\det(BA)=0$ (d) $\operatorname{tr}(AB)=0$,,"['linear-algebra', 'matrices']"
3,"Size of conjugacy classes in $GL(4,2)$",Size of conjugacy classes in,"GL(4,2)","I'm asked to find out all of the conjugacy classes, their order and their size for $GL(4,2)$. Finding representatives is possible by looking for all the rational canonical forms over the  field and the order, just by taking powers of the representatives. Now, about calculating the size of each class, I know it can be done by trying to calculate the size of the center for each class, $C_G(x_i)$, where $x_i$ are the representatives, and then $\frac{|GL(4,2)|}{|C_G(x_i)|}$ is the size of the class, but it appears to be very difficult to do it straightforward. Can someone suggest me a better way to do it? Thanks Note: $GL(4,2)$ is all invertible matrices of size $4\times4$ over $F_2$. http://en.wikipedia.org/wiki/General_linear_group#Over_finite_fields","I'm asked to find out all of the conjugacy classes, their order and their size for $GL(4,2)$. Finding representatives is possible by looking for all the rational canonical forms over the  field and the order, just by taking powers of the representatives. Now, about calculating the size of each class, I know it can be done by trying to calculate the size of the center for each class, $C_G(x_i)$, where $x_i$ are the representatives, and then $\frac{|GL(4,2)|}{|C_G(x_i)|}$ is the size of the class, but it appears to be very difficult to do it straightforward. Can someone suggest me a better way to do it? Thanks Note: $GL(4,2)$ is all invertible matrices of size $4\times4$ over $F_2$. http://en.wikipedia.org/wiki/General_linear_group#Over_finite_fields",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'matrices']"
4,"Let $a,b$ and $c$ be real numbers.evaluate the following determinant: |$b^2c^2 ,bc, b+c;c^2a^2,ca,c+a;a^2b^2,ab,a+b$|",Let  and  be real numbers.evaluate the following determinant: ||,"a,b c b^2c^2 ,bc, b+c;c^2a^2,ca,c+a;a^2b^2,ab,a+b","Let $a,b$ and $c$ be real numbers. Evaluate the following determinant: $$\begin{vmatrix}b^2c^2 &bc& b+c\cr c^2a^2&ca&c+a\cr a^2b^2&ab&a+b\cr\end{vmatrix}$$ after long calculation I get that the answer will be $0$. Is there any short processs? Please help someone thank you.","Let $a,b$ and $c$ be real numbers. Evaluate the following determinant: $$\begin{vmatrix}b^2c^2 &bc& b+c\cr c^2a^2&ca&c+a\cr a^2b^2&ab&a+b\cr\end{vmatrix}$$ after long calculation I get that the answer will be $0$. Is there any short processs? Please help someone thank you.",,['linear-algebra']
5,Matrices with columns which are eigenvectors,Matrices with columns which are eigenvectors,,"In this question , the OP asks about finding the matrix exponential of the matrix $$M=\begin{bmatrix} 1 & 1 & 1\\ 1 & 1 & 1\\ 1 & 1 & 1 \end{bmatrix}.$$ It works out quite nicely because $M^2 = 3M$ so $M^n = 3^{n-1}M$. The reason this occurs is that the vector $$v = \begin{bmatrix} 1\\ 1\\ 1 \end{bmatrix}$$ is an eigenvector for $M$ (with eigenvalue $3$). The same is true of the $n\times n$ matrix consisting of all ones and the corresponding vector. With this in mind, I ask the following: Can we find a standard form for an $n\times n$ matrix with the property that each of its columns are eigenvectors? ( Added later: This may be easier to answer if we allow columns to be zero vectors. Thanks Adam W for the comment.) What about the case when we require all the eigenvectors to correspond to the same eigenvalue? The matrices in the second question are precisely the ones for which the calculation of the matrix exponential would be analogous to that for $M$ as above. Added later: In Hurkyl's answer, he/she shows that an invertible matrix satisfying 1 is diagonal. For the case $n=2$, it is fairly easy to see that any non-invertible matrix satisfies 1 (which is generalised by the situation in Seirios's answer). However, for $n > 2$, not every non-invertible matrix satisfies this property (as one may expect). For example, $$M = \begin{bmatrix} 1 & 0 & 0\\ 0 & 0 & 0\\ 1 & 0 & 1 \end{bmatrix}.$$","In this question , the OP asks about finding the matrix exponential of the matrix $$M=\begin{bmatrix} 1 & 1 & 1\\ 1 & 1 & 1\\ 1 & 1 & 1 \end{bmatrix}.$$ It works out quite nicely because $M^2 = 3M$ so $M^n = 3^{n-1}M$. The reason this occurs is that the vector $$v = \begin{bmatrix} 1\\ 1\\ 1 \end{bmatrix}$$ is an eigenvector for $M$ (with eigenvalue $3$). The same is true of the $n\times n$ matrix consisting of all ones and the corresponding vector. With this in mind, I ask the following: Can we find a standard form for an $n\times n$ matrix with the property that each of its columns are eigenvectors? ( Added later: This may be easier to answer if we allow columns to be zero vectors. Thanks Adam W for the comment.) What about the case when we require all the eigenvectors to correspond to the same eigenvalue? The matrices in the second question are precisely the ones for which the calculation of the matrix exponential would be analogous to that for $M$ as above. Added later: In Hurkyl's answer, he/she shows that an invertible matrix satisfying 1 is diagonal. For the case $n=2$, it is fairly easy to see that any non-invertible matrix satisfies 1 (which is generalised by the situation in Seirios's answer). However, for $n > 2$, not every non-invertible matrix satisfies this property (as one may expect). For example, $$M = \begin{bmatrix} 1 & 0 & 0\\ 0 & 0 & 0\\ 1 & 0 & 1 \end{bmatrix}.$$",,"['linear-algebra', 'eigenvalues-eigenvectors']"
6,Diagonalizable vs Normal,Diagonalizable vs Normal,,"If I have real valued matrix $A$, are these two notions of being Diagonalizable and being Normal equivalent?","If I have real valued matrix $A$, are these two notions of being Diagonalizable and being Normal equivalent?",,"['linear-algebra', 'matrices']"
7,trace of the matrix $I + M + M^2$ is,trace of the matrix  is,I + M + M^2,Let $ \alpha = e^{\frac{2\pi \iota}{5}}$  and the matrix    $$ M= \begin{pmatrix}1 & \alpha & \alpha^2 & \alpha^3 & \alpha^4\\  0 & \alpha & \alpha^2 & \alpha^3 & \alpha^4\\ 0 & 0 & \alpha^2 & \alpha^3 & \alpha^4 \\ 0 & 0 & 0 & \alpha^3 & \alpha^4\\  0 & 0 & 0 & 0 & \alpha^4 \end{pmatrix}$$ Then the trace of the  matrix $I + M + M^2$ is $-5$; $0$; $3$; $5$. I am stuck on this problem. Can anyone help me please? I got trace of the matrix $$\operatorname{tr}(I+M+M^2) = 7 + \alpha + 2  \alpha^2   + \alpha^3 + 2 \alpha^4 + \alpha^6 +\alpha^8.$$ Now what to do?,Let $ \alpha = e^{\frac{2\pi \iota}{5}}$  and the matrix    $$ M= \begin{pmatrix}1 & \alpha & \alpha^2 & \alpha^3 & \alpha^4\\  0 & \alpha & \alpha^2 & \alpha^3 & \alpha^4\\ 0 & 0 & \alpha^2 & \alpha^3 & \alpha^4 \\ 0 & 0 & 0 & \alpha^3 & \alpha^4\\  0 & 0 & 0 & 0 & \alpha^4 \end{pmatrix}$$ Then the trace of the  matrix $I + M + M^2$ is $-5$; $0$; $3$; $5$. I am stuck on this problem. Can anyone help me please? I got trace of the matrix $$\operatorname{tr}(I+M+M^2) = 7 + \alpha + 2  \alpha^2   + \alpha^3 + 2 \alpha^4 + \alpha^6 +\alpha^8.$$ Now what to do?,,"['linear-algebra', 'matrices']"
8,"What is an ""indecomposable"" matrix?","What is an ""indecomposable"" matrix?",,"What is an indecomposable matrix? I tried to find what it is, but Wikipedia does not have an entry for it. Also, what properties does such matrix have?","What is an indecomposable matrix? I tried to find what it is, but Wikipedia does not have an entry for it. Also, what properties does such matrix have?",,"['linear-algebra', 'matrices', 'terminology']"
9,Determinants of block matrices,Determinants of block matrices,,"Let $A,B \in \mathbb{R}^{n,n}$. Now $C = \begin{pmatrix}  A & iB \\ -iB & A \end{pmatrix}$ and $D = \begin{pmatrix} A & B \\ -B & A \end{pmatrix}$. Show that $\det(C) \in \mathbb{R}$ and $\det(D) \ge 0$. I tried to transform $C$ so I could use $\det \begin{pmatrix} E & F \\ 0 & G \end{pmatrix} = \det(E)\det(G)$, but I didn't manage to. I have no clue how to show $\det(D) \ge 0$. I'd rather have hints than fully-fledged solutions. Thanks.","Let $A,B \in \mathbb{R}^{n,n}$. Now $C = \begin{pmatrix}  A & iB \\ -iB & A \end{pmatrix}$ and $D = \begin{pmatrix} A & B \\ -B & A \end{pmatrix}$. Show that $\det(C) \in \mathbb{R}$ and $\det(D) \ge 0$. I tried to transform $C$ so I could use $\det \begin{pmatrix} E & F \\ 0 & G \end{pmatrix} = \det(E)\det(G)$, but I didn't manage to. I have no clue how to show $\det(D) \ge 0$. I'd rather have hints than fully-fledged solutions. Thanks.",,"['linear-algebra', 'matrices', 'determinant', 'block-matrices']"
10,Vector spaces inquiry,Vector spaces inquiry,,"Denote By $V$ the real vector spaces of all real polynomials in one variable, and let $P : V \rightarrow \mathbb{R}$ be a linear map. Suppose that $\forall$ $f,g \in V$ with $P(fg) = 0$ we have $P(f) = 0$ or $P(g) = 0$. Prove that there exists real numbers $x_0, c$ such that $P(f) = cf(x_0)$ for all $f$.","Denote By $V$ the real vector spaces of all real polynomials in one variable, and let $P : V \rightarrow \mathbb{R}$ be a linear map. Suppose that $\forall$ $f,g \in V$ with $P(fg) = 0$ we have $P(f) = 0$ or $P(g) = 0$. Prove that there exists real numbers $x_0, c$ such that $P(f) = cf(x_0)$ for all $f$.",,"['linear-algebra', 'abstract-algebra', 'vector-spaces', 'recreational-mathematics']"
11,Solid angle between vectors in n-dimensional space,Solid angle between vectors in n-dimensional space,,"There is a formula of to calculate the angle between two normalized vectors: $$\alpha=\arccos \frac {\vec{a} \cdot\ \vec{b}} {|\vec {a}||\vec {b}|}.$$ The formula of 3D solid angle between three normalized vectors is ( borrowed from wikipedia ): $$\alpha = 2\arctan \frac{|\vec {a}\ \vec {b}\ \vec {c}|}{|\vec{a}| |\vec{b}| |\vec{c}| + (\vec{a} \cdot \ \vec{b})|\vec{c}| + (\vec{a} \cdot \vec {c})|\vec{b}| + (\vec{b} \cdot \vec{c})|\vec{a}|}.$$ How to figure out a formula of solid angle between $n$ normalized vectors in $n$-dimensional space? UPDATE OK, I've found out the following formula for the angle between two vectors in 2d space through the $\arctan$: $$\alpha= 2\arctan  \frac {|\vec{a} \wedge \vec {b}|} {|\vec{a}| |\vec{b}| + \vec{a}\ \cdot\ \vec{b}}$$ How can I apply this $\arctan$ formula for the 4D space, for example? For more detail, I explain my assumptions. The solid angle of orthogonal basis in 4D space must be $\alpha = \frac {2 \cdot \pi ^ 2 \cdot R^3} {2 ^ 4} = \frac {\pi ^ 2 R^3} {8} $. Here we can see a $\pi^2$ factor. Does this mean that the 4D solid angle formula contains multiplication of two $\arctan$ there is one $\arctan$ in this formula? I think yes, but still have some difficulties with such formula inference.","There is a formula of to calculate the angle between two normalized vectors: $$\alpha=\arccos \frac {\vec{a} \cdot\ \vec{b}} {|\vec {a}||\vec {b}|}.$$ The formula of 3D solid angle between three normalized vectors is ( borrowed from wikipedia ): $$\alpha = 2\arctan \frac{|\vec {a}\ \vec {b}\ \vec {c}|}{|\vec{a}| |\vec{b}| |\vec{c}| + (\vec{a} \cdot \ \vec{b})|\vec{c}| + (\vec{a} \cdot \vec {c})|\vec{b}| + (\vec{b} \cdot \vec{c})|\vec{a}|}.$$ How to figure out a formula of solid angle between $n$ normalized vectors in $n$-dimensional space? UPDATE OK, I've found out the following formula for the angle between two vectors in 2d space through the $\arctan$: $$\alpha= 2\arctan  \frac {|\vec{a} \wedge \vec {b}|} {|\vec{a}| |\vec{b}| + \vec{a}\ \cdot\ \vec{b}}$$ How can I apply this $\arctan$ formula for the 4D space, for example? For more detail, I explain my assumptions. The solid angle of orthogonal basis in 4D space must be $\alpha = \frac {2 \cdot \pi ^ 2 \cdot R^3} {2 ^ 4} = \frac {\pi ^ 2 R^3} {8} $. Here we can see a $\pi^2$ factor. Does this mean that the 4D solid angle formula contains multiplication of two $\arctan$ there is one $\arctan$ in this formula? I think yes, but still have some difficulties with such formula inference.",,"['linear-algebra', 'geometry', 'vector-spaces', 'euclidean-geometry']"
12,Image of determinant on symplectic/orthogonal matrix group,Image of determinant on symplectic/orthogonal matrix group,,"Let $\mathbb K$ be a field, $n\geq 1$, and $G=GL_n({\mathbb K})$ be the group of invertible $n \times n$ matrices with coefficients in $\mathbb K$. For $J\in G$, we can define (in analogy to orthogonal&symplectic matrix groups) $$ H=\lbrace M \in G | {}^tMJM=J\rbrace $$ This is a subgroup of $G$, and the determinant defines a homomorphism $H \to \lbrace -1,1\rbrace$ (because $({\sf det} M)^2=1$ for any $M\in H$, by taking determinants in the equation above). So the image of this homomorphism is either $\lbrace 1 \rbrace$ (trivial) or $\lbrace -1,1\rbrace$ (full). In some cases (say $\mathbb K=\mathbb R$ and $J=I_n$ : then $H$ is the group of orthogonal matrices), the image is full (for example, ${\sf diag}(1,-1)$ has determinant $-1$). In others (such as when $J$ is as in this recent Math.stackexchange question ), the image is trivial. The (obvious) question is : for which $J$ is the image trivial or full ?","Let $\mathbb K$ be a field, $n\geq 1$, and $G=GL_n({\mathbb K})$ be the group of invertible $n \times n$ matrices with coefficients in $\mathbb K$. For $J\in G$, we can define (in analogy to orthogonal&symplectic matrix groups) $$ H=\lbrace M \in G | {}^tMJM=J\rbrace $$ This is a subgroup of $G$, and the determinant defines a homomorphism $H \to \lbrace -1,1\rbrace$ (because $({\sf det} M)^2=1$ for any $M\in H$, by taking determinants in the equation above). So the image of this homomorphism is either $\lbrace 1 \rbrace$ (trivial) or $\lbrace -1,1\rbrace$ (full). In some cases (say $\mathbb K=\mathbb R$ and $J=I_n$ : then $H$ is the group of orthogonal matrices), the image is full (for example, ${\sf diag}(1,-1)$ has determinant $-1$). In others (such as when $J$ is as in this recent Math.stackexchange question ), the image is trivial. The (obvious) question is : for which $J$ is the image trivial or full ?",,"['linear-algebra', 'group-theory', 'matrices']"
13,How can one rigorously determine the cardinality of an infinite dimensional vector space?,How can one rigorously determine the cardinality of an infinite dimensional vector space?,,"Suppose $V$ is a vector space over a scalar field $F$. If $\dim(V)=n$, then $|V|=|F|^n$. How can I rigorously determine the cardinality of $V$ when $V$ is infinite dimensional? My thought was that if $\mathscr{B}$ is an ordered basis for $V$, then the cardinality of $V$ is given by the set of functions from $\mathscr{B}\to F$, by identifying elements of $V$ with their $\mathscr{B}$-coordinate vector. However, I feel that we should only count functions with finite support since infinite sums don't make sense. Is this correct? If so, how does one find the cardinality of $\{f\colon\mathscr{B}\to F\mid \mathrm{supp }(f)<\infty\}$, in terms of say $|F|$ and $|\mathscr{B}|$? Thanks.","Suppose $V$ is a vector space over a scalar field $F$. If $\dim(V)=n$, then $|V|=|F|^n$. How can I rigorously determine the cardinality of $V$ when $V$ is infinite dimensional? My thought was that if $\mathscr{B}$ is an ordered basis for $V$, then the cardinality of $V$ is given by the set of functions from $\mathscr{B}\to F$, by identifying elements of $V$ with their $\mathscr{B}$-coordinate vector. However, I feel that we should only count functions with finite support since infinite sums don't make sense. Is this correct? If so, how does one find the cardinality of $\{f\colon\mathscr{B}\to F\mid \mathrm{supp }(f)<\infty\}$, in terms of say $|F|$ and $|\mathscr{B}|$? Thanks.",,"['linear-algebra', 'cardinals']"
14,Hermitian/positive definite matrices and their analogues in complex numbers,Hermitian/positive definite matrices and their analogues in complex numbers,,"I've heard a couple of times some people say that in a way, Hermitian matrices are to matrices as real numbers are to complex numbers. I know two examples where this is sort of true: Complex conjugate vs conjugate transpose We can realize complex numbers as a set of matrices, where complex number $z = a + bi$ corresponds to a matrix $$A = \begin{bmatrix}a & -b \\ b & a\end{bmatrix}$$ Then the conjugate $\bar{z} = a - bi$ corresponds to the conjugate transpose $A^*$. A complex number is real if and only if $z = \bar{z}$. On the other hand, matrix $A$ is called Hermitian if and only if $A = A^*$. Polar form vs polar decomposition: We can represent every complex number $z$ as $z = re^{i\varphi}$, where $r \geq 0$ and $\varphi \in [0, 2\pi[$. This representation is unique when $z \neq 0$. On the other hand, any $n \times n$ matrix $A$ with complex entries can be represented as $A = RU$ where $R$ is positive semidefinite (thus Hermitian) and $U$ is unitary. This representation is unique when $A$ is invertible. Also, if $\det A = re^{i\varphi}$, then $\det R = r$ and $\det U = e^{i\varphi}$. Taking the second example really far, you could say positive semidefinite matrices are like nonnegative real numbers and unitary matrices are like points on the unit circle. Are there more examples? I think this is interesting, but is thinking like this useful at all? It seems like oversimplifying things and I doubt if this leads to anything other than some fun facts.","I've heard a couple of times some people say that in a way, Hermitian matrices are to matrices as real numbers are to complex numbers. I know two examples where this is sort of true: Complex conjugate vs conjugate transpose We can realize complex numbers as a set of matrices, where complex number $z = a + bi$ corresponds to a matrix $$A = \begin{bmatrix}a & -b \\ b & a\end{bmatrix}$$ Then the conjugate $\bar{z} = a - bi$ corresponds to the conjugate transpose $A^*$. A complex number is real if and only if $z = \bar{z}$. On the other hand, matrix $A$ is called Hermitian if and only if $A = A^*$. Polar form vs polar decomposition: We can represent every complex number $z$ as $z = re^{i\varphi}$, where $r \geq 0$ and $\varphi \in [0, 2\pi[$. This representation is unique when $z \neq 0$. On the other hand, any $n \times n$ matrix $A$ with complex entries can be represented as $A = RU$ where $R$ is positive semidefinite (thus Hermitian) and $U$ is unitary. This representation is unique when $A$ is invertible. Also, if $\det A = re^{i\varphi}$, then $\det R = r$ and $\det U = e^{i\varphi}$. Taking the second example really far, you could say positive semidefinite matrices are like nonnegative real numbers and unitary matrices are like points on the unit circle. Are there more examples? I think this is interesting, but is thinking like this useful at all? It seems like oversimplifying things and I doubt if this leads to anything other than some fun facts.",,"['linear-algebra', 'matrices', 'complex-numbers']"
15,Diagonalizing Matrices over UFDs,Diagonalizing Matrices over UFDs,,"Suppose $R$ is a UFD, $K$ its field of fractions. If a matrix $M$ with entries in $R$ has distinct eigenvalues, then it is certainly diagonalizable over $K$. If those eigenvalues are actually in $R$, must it be diagonalizable over $R$? If it helps: the specific case I'm interested in is when $K$ is a local field of characteristic $0$ and $R$ its integers.","Suppose $R$ is a UFD, $K$ its field of fractions. If a matrix $M$ with entries in $R$ has distinct eigenvalues, then it is certainly diagonalizable over $K$. If those eigenvalues are actually in $R$, must it be diagonalizable over $R$? If it helps: the specific case I'm interested in is when $K$ is a local field of characteristic $0$ and $R$ its integers.",,"['linear-algebra', 'abstract-algebra']"
16,Help Understanding Proof of Replacement Theorem?,Help Understanding Proof of Replacement Theorem?,,"Sorry if this is a trivial question. The book is Linear Algebra Done Right by Axler, page 25-26. Theorem: In a finite-dimensional vector space, the length of every linearly independent list of vectors is less than or equal to the length of every spanning list of vectors. Proof: Suppose that $(u_1 ,\ldots, u_m)$ is linearly independent in $V$ and    that $(w_1,\ldots ,w_n)$ spans V. We need to prove that $m \leq n$. We do so    through the multistep process described below; note that in each step    we add one of the $u$'s and remove one of the $w$'s. Step 1: The list $(w_1,\ldots, w_n)$ spans $V$, and thus adjoining any vector to it    produces a linearly dependent list. In particular, the list $(u_1,w_1, \ldots,w_n)$ is linearly dependent. Question: Why is $(u_1,w_1, \ldots,w_n)$ is linearly dependent?","Sorry if this is a trivial question. The book is Linear Algebra Done Right by Axler, page 25-26. Theorem: In a finite-dimensional vector space, the length of every linearly independent list of vectors is less than or equal to the length of every spanning list of vectors. Proof: Suppose that $(u_1 ,\ldots, u_m)$ is linearly independent in $V$ and    that $(w_1,\ldots ,w_n)$ spans V. We need to prove that $m \leq n$. We do so    through the multistep process described below; note that in each step    we add one of the $u$'s and remove one of the $w$'s. Step 1: The list $(w_1,\ldots, w_n)$ spans $V$, and thus adjoining any vector to it    produces a linearly dependent list. In particular, the list $(u_1,w_1, \ldots,w_n)$ is linearly dependent. Question: Why is $(u_1,w_1, \ldots,w_n)$ is linearly dependent?",,['linear-algebra']
17,"Gaussian Kernels, Why are they full rank?","Gaussian Kernels, Why are they full rank?",,"I'd like to ask why a Gaussian Kernel's Gram Matrix is of full rank. Lots of texts and articles always write about assuming this is the case, and refer me to an unavailable research article online, but I haven't been able to find a single source that sheds light on why this is the case.","I'd like to ask why a Gaussian Kernel's Gram Matrix is of full rank. Lots of texts and articles always write about assuming this is the case, and refer me to an unavailable research article online, but I haven't been able to find a single source that sheds light on why this is the case.",,"['linear-algebra', 'reference-request']"
18,Some questions about Hilbert matrix,Some questions about Hilbert matrix,,"This Exercise $12$ page $27$ from Hoffman and Kunze's book Linear Algebra . The result o Example $16$ suggests that perhaps the matrix $$A = \begin{bmatrix} 1 & \frac{1}{2} & \ldots & \frac{1}{n}\\  \frac{1}{2} & \frac{1}{3} & \ldots & \frac{1}{n+1}\\ \vdots & \vdots &  & \vdots \\ \frac{1}{n} &\frac{1}{n+1} &\ldots &\frac{1}{2n-1} \end{bmatrix}$$ is invertible and $A^{-1}$ has integer entries. Can you prove that? I will copy their words before that example: Some people find it less awkward to carry along two sequences of   matrices, one describing the reduction of $A$ to the identity and the   other recording the effect of the same sequence of operations starting   from the identity. In this post , Adrián Barquero says: If your linear algebra textbook is Kenneth Hoffmann and Ray Kunze's   Linear Algebra book then I was in the same situation you're right now   a few years ago, but I was adviced at the moment that this particular   exercise wasn't as easy as one might expect at first. The matrix you have is called the Hilbert matrix and the question you   have was already asked a couple of times in math overflow here and here . They have excellent answers so I will just point you to them. My question: Is it possible answer the question made by the authors without using high techniques?  I am not allowed to use determinants here. PS: Those answers in MO don't answer the question above. I apologise if this question is unappropriated or if I was unable to understand their answers.","This Exercise $12$ page $27$ from Hoffman and Kunze's book Linear Algebra . The result o Example $16$ suggests that perhaps the matrix $$A = \begin{bmatrix} 1 & \frac{1}{2} & \ldots & \frac{1}{n}\\  \frac{1}{2} & \frac{1}{3} & \ldots & \frac{1}{n+1}\\ \vdots & \vdots &  & \vdots \\ \frac{1}{n} &\frac{1}{n+1} &\ldots &\frac{1}{2n-1} \end{bmatrix}$$ is invertible and $A^{-1}$ has integer entries. Can you prove that? I will copy their words before that example: Some people find it less awkward to carry along two sequences of   matrices, one describing the reduction of $A$ to the identity and the   other recording the effect of the same sequence of operations starting   from the identity. In this post , Adrián Barquero says: If your linear algebra textbook is Kenneth Hoffmann and Ray Kunze's   Linear Algebra book then I was in the same situation you're right now   a few years ago, but I was adviced at the moment that this particular   exercise wasn't as easy as one might expect at first. The matrix you have is called the Hilbert matrix and the question you   have was already asked a couple of times in math overflow here and here . They have excellent answers so I will just point you to them. My question: Is it possible answer the question made by the authors without using high techniques?  I am not allowed to use determinants here. PS: Those answers in MO don't answer the question above. I apologise if this question is unappropriated or if I was unable to understand their answers.",,['linear-algebra']
19,Commuting block diagonal matrices,Commuting block diagonal matrices,,"Suppose we have the $n \times n$ block matrix  $$M = [M_{i,j}] = M_{1,1} \oplus \cdots \oplus M_{n,n}$$ such that each $M_{i,i}$ is also square and has exactly one eigenvalue $\lambda_i$ and $\lambda_i = \lambda_j \implies i=j$. I need to show that the only matrices which commute with $M$ are also block diagonal with the same block sizes as $M$. My approach so far was to try and write out a general matrix $A$ in the same block form as $M$ and assume it commutes with $M$. Then compute the multiplication on the left and right and equate the blocks, with the goal of showing that all the non-diagonal blocks must be zero. I didn't make much progress with this approach. Do you have any suggestions?","Suppose we have the $n \times n$ block matrix  $$M = [M_{i,j}] = M_{1,1} \oplus \cdots \oplus M_{n,n}$$ such that each $M_{i,i}$ is also square and has exactly one eigenvalue $\lambda_i$ and $\lambda_i = \lambda_j \implies i=j$. I need to show that the only matrices which commute with $M$ are also block diagonal with the same block sizes as $M$. My approach so far was to try and write out a general matrix $A$ in the same block form as $M$ and assume it commutes with $M$. Then compute the multiplication on the left and right and equate the blocks, with the goal of showing that all the non-diagonal blocks must be zero. I didn't make much progress with this approach. Do you have any suggestions?",,"['linear-algebra', 'matrices', 'block-matrices']"
20,Notation for elements of a vector,Notation for elements of a vector,,"If I want $x_i$ to be an arbitrary element of a vector $\vec{x}$ can I use the following notation: $x_i \in \vec{x}=[x_1\;x_2\;\cdots\;x_n]^T\in \mathbb{R}^n$ ? And if I later want to specify the interval of each $x_i$ to be between $0$ and $1$ , can I then say that $x_i \in [0,1]\;\forall i$ ? Is this mathematically correct usage of $\in$ for both cases? The actual problem I have is that I want to say that $y_i\in\vec{y}$ for $i\in\{1,2,\cdots,n\}$ and that each $y_i$ is binary $y_i\in\{-1,+1\}$ . Should I stick to something like $\vec{y}\in\{-1,+1\}^n$ instead?","If I want to be an arbitrary element of a vector can I use the following notation: ? And if I later want to specify the interval of each to be between and , can I then say that ? Is this mathematically correct usage of for both cases? The actual problem I have is that I want to say that for and that each is binary . Should I stick to something like instead?","x_i \vec{x} x_i \in \vec{x}=[x_1\;x_2\;\cdots\;x_n]^T\in \mathbb{R}^n x_i 0 1 x_i \in [0,1]\;\forall i \in y_i\in\vec{y} i\in\{1,2,\cdots,n\} y_i y_i\in\{-1,+1\} \vec{y}\in\{-1,+1\}^n","['linear-algebra', 'elementary-set-theory', 'notation']"
21,Taking powers of a triangular matrix?,Taking powers of a triangular matrix?,,"So there is a formula for the $n$th power of a matrix in Jordan normal form. Is there a formula for the $n$th power of a general triangular matrix? If not, are there known formulas for ""nice"" upper triangular matrices? Like those consisting of all 1s and 0s.","So there is a formula for the $n$th power of a matrix in Jordan normal form. Is there a formula for the $n$th power of a general triangular matrix? If not, are there known formulas for ""nice"" upper triangular matrices? Like those consisting of all 1s and 0s.",,['linear-algebra']
22,Is this equivalent to Cauchy-Schwarz Inequality?,Is this equivalent to Cauchy-Schwarz Inequality?,,$(x^*Ay)^2 \leq (x^*Ax)(y^*Ay)$ Does this inequality hold for positive semi-definite matrix A? I wonder if this is equivalent to Cauchy-Schwarz Inequality. I tried to diagonalize A but it still half-way towards Cauchy-Schwarz Inequality. Any thoughts on this inequality? Am I going on the right direction? Thanks a lot!,$(x^*Ay)^2 \leq (x^*Ax)(y^*Ay)$ Does this inequality hold for positive semi-definite matrix A? I wonder if this is equivalent to Cauchy-Schwarz Inequality. I tried to diagonalize A but it still half-way towards Cauchy-Schwarz Inequality. Any thoughts on this inequality? Am I going on the right direction? Thanks a lot!,,"['linear-algebra', 'matrices', 'cauchy-schwarz-inequality']"
23,Conditions for a real matrix to have real eigenvalues,Conditions for a real matrix to have real eigenvalues,,The eigenvalues of a symmetric real matrix are all real. I was wondering if there are conditions either more general than symmetry or that may or may not overlap with symmetry to ensure eigenvalues to be real? Thanks! Motivation : A real matrix admits a real Schur decomposition if and only if all of   its eigenvalues are real.,The eigenvalues of a symmetric real matrix are all real. I was wondering if there are conditions either more general than symmetry or that may or may not overlap with symmetry to ensure eigenvalues to be real? Thanks! Motivation : A real matrix admits a real Schur decomposition if and only if all of   its eigenvalues are real.,,"['linear-algebra', 'matrices']"
24,Getting the determinant value of the original matrix from its upper triangular matrix,Getting the determinant value of the original matrix from its upper triangular matrix,,"Sometimes when finding the upper triangular of a matrix, I may just happen to switch rows to make the whole process shorter. Say for this matrix: $$ A=\begin{bmatrix} 1 & -1 & -1\\  3 & -3 & 2\\  2 & 1 & 1 \end{bmatrix} $$ The determinant of it is $\left | A \right | = -5$. Then to find the upper triangular matrix of A, I thought maybe switching row2 and row 3 would make the process simpler. So I did this: $$ \begin{bmatrix} 1 & 0 & 0\\  0 & 0 & 1\\  0 & 1 & 0 \end{bmatrix}\cdot A=\begin{bmatrix} 1 & -1 & -1\\  2 & -1 & 1\\  3 & -3 & 2 \end{bmatrix} $$ And I let this be B this way: $B=\begin{bmatrix} 1 & -1 & -1\\  2 & -1 & 1\\  3 & -3 & 2 \end{bmatrix}$. I figure out the elementary rows: $$ E_{21} = \begin{bmatrix} 1 & 0 & 0\\  -2& 1 & 0\\  0 & 0 & 1 \end{bmatrix},  E_{31} = \begin{bmatrix} 1 & 0 & 0\\  0 & 1 & 0\\  -3 & 0 & 1 \end{bmatrix} $$ Multiply the elementary row matrices to the matrix $B$: $$ E_{31}\cdot E_{21}\cdot B= \begin{bmatrix} 1 & -1 & -1\\  0 & 1 & 3\\  0 & 0 & 5 \end{bmatrix} $$ Now I got the upper triangle of matrix B, which is also the upper triangle of matrix $A$. Then I do a check on its determinant: $1\cdot 1\cdot 5=5$. To my surprise, I get $5$ instead of $-5$! I realise that this is because $\left | B \right | = 5$. But since I had a row exchange to get the upper triangle, how can I make sure that its determinant is also the same as its original matrix $A$, which should be $-5$? I thought this is quite important because sometimes I use this to find the determinant value of higher dimension matrices. Thanks for any help.","Sometimes when finding the upper triangular of a matrix, I may just happen to switch rows to make the whole process shorter. Say for this matrix: $$ A=\begin{bmatrix} 1 & -1 & -1\\  3 & -3 & 2\\  2 & 1 & 1 \end{bmatrix} $$ The determinant of it is $\left | A \right | = -5$. Then to find the upper triangular matrix of A, I thought maybe switching row2 and row 3 would make the process simpler. So I did this: $$ \begin{bmatrix} 1 & 0 & 0\\  0 & 0 & 1\\  0 & 1 & 0 \end{bmatrix}\cdot A=\begin{bmatrix} 1 & -1 & -1\\  2 & -1 & 1\\  3 & -3 & 2 \end{bmatrix} $$ And I let this be B this way: $B=\begin{bmatrix} 1 & -1 & -1\\  2 & -1 & 1\\  3 & -3 & 2 \end{bmatrix}$. I figure out the elementary rows: $$ E_{21} = \begin{bmatrix} 1 & 0 & 0\\  -2& 1 & 0\\  0 & 0 & 1 \end{bmatrix},  E_{31} = \begin{bmatrix} 1 & 0 & 0\\  0 & 1 & 0\\  -3 & 0 & 1 \end{bmatrix} $$ Multiply the elementary row matrices to the matrix $B$: $$ E_{31}\cdot E_{21}\cdot B= \begin{bmatrix} 1 & -1 & -1\\  0 & 1 & 3\\  0 & 0 & 5 \end{bmatrix} $$ Now I got the upper triangle of matrix B, which is also the upper triangle of matrix $A$. Then I do a check on its determinant: $1\cdot 1\cdot 5=5$. To my surprise, I get $5$ instead of $-5$! I realise that this is because $\left | B \right | = 5$. But since I had a row exchange to get the upper triangle, how can I make sure that its determinant is also the same as its original matrix $A$, which should be $-5$? I thought this is quite important because sometimes I use this to find the determinant value of higher dimension matrices. Thanks for any help.",,"['linear-algebra', 'matrices']"
25,"Existence of a matrix, characteristic polynomial and minimal polynomial","Existence of a matrix, characteristic polynomial and minimal polynomial",,"Let $k$ be a field and let $p(x),q(x)$ be elements of $k[x]$. If $q(x)$ divides $p(x)$ and if every root of $p(x)$ is a root of $q(x)$ prove there exists a matrix $A$ having minimal polynomial equal $q(x)$ and characteristic polynomial equal $p(x)$. So I tried first an example. Take $p(x)=(x-3)^{2}(x-5)$ and $q(x)=(x-3)(x-5)$. Now set: $C= \left( \begin{array}{ccc} 3 & 0 & 0\\ 0 & 3 & 0\\ 0 & 0 & 5\end{array} \right)$ Then $C$ is a diagonal matrix and its characteristic polynomial is equal $p(x)$ while its minimal polynomial is $q(x)$. Now how to deal with the general case? I'm not sure the same argument would hold for any general $p(x)$, can you please help?","Let $k$ be a field and let $p(x),q(x)$ be elements of $k[x]$. If $q(x)$ divides $p(x)$ and if every root of $p(x)$ is a root of $q(x)$ prove there exists a matrix $A$ having minimal polynomial equal $q(x)$ and characteristic polynomial equal $p(x)$. So I tried first an example. Take $p(x)=(x-3)^{2}(x-5)$ and $q(x)=(x-3)(x-5)$. Now set: $C= \left( \begin{array}{ccc} 3 & 0 & 0\\ 0 & 3 & 0\\ 0 & 0 & 5\end{array} \right)$ Then $C$ is a diagonal matrix and its characteristic polynomial is equal $p(x)$ while its minimal polynomial is $q(x)$. Now how to deal with the general case? I'm not sure the same argument would hold for any general $p(x)$, can you please help?",,['linear-algebra']
26,a question about calculating eigenvectors,a question about calculating eigenvectors,,"It has to be the silliest question, but it’s not clear to me how to calculate eigenvectors quickly. I am just talking about a very simple 2-by-2 matrix. When I have already calculated the eigenvalues from a characteristic polynomial, I can start to solve the equations with $A\mathbf{v}_1 = e_1\mathbf{v}_1$ and A\mathbf{v}_2 = e_2\mathbf{v}_2$, but in this case it always requires writing lines of equations and solving them. On the other hand I figured out that just by looking at the matrix you can come up with the eigenvectors very quickly. But I'm a bit confused in this part. When you have the matrix with subtracted $e_1$ values like this: $$\left(\begin{array}{cc} A&B\\ C&D \end{array}\right).$$ Then for me, it always worked to use the eigenvector. $$\left(\begin{array}{r}-B\\A\end{array}\right)$$ But in some guides I find that they are using A C as an eigenvector. $$\left(\begin{array}{c}A\\C \end{array}\right).$$ And when I check it, they are indeed multiples of each other. But this other method is not clear to me, how could $A$ and $C$ mean anything about the eigenvector, when both of them are connected to $x$, without having to do anything with $y$. But it’s still working. Was it just a coincidence? So is the recommended method for calculating them is just to subtract the eigenvalues from the matrix and look at  $$\left(\begin{array}{r} -B\\A\end{array}\right)\qquad\text{or}\qquad\left(\begin{array}{r} -D\\A \end{array}\right).$$","It has to be the silliest question, but it’s not clear to me how to calculate eigenvectors quickly. I am just talking about a very simple 2-by-2 matrix. When I have already calculated the eigenvalues from a characteristic polynomial, I can start to solve the equations with $A\mathbf{v}_1 = e_1\mathbf{v}_1$ and A\mathbf{v}_2 = e_2\mathbf{v}_2$, but in this case it always requires writing lines of equations and solving them. On the other hand I figured out that just by looking at the matrix you can come up with the eigenvectors very quickly. But I'm a bit confused in this part. When you have the matrix with subtracted $e_1$ values like this: $$\left(\begin{array}{cc} A&B\\ C&D \end{array}\right).$$ Then for me, it always worked to use the eigenvector. $$\left(\begin{array}{r}-B\\A\end{array}\right)$$ But in some guides I find that they are using A C as an eigenvector. $$\left(\begin{array}{c}A\\C \end{array}\right).$$ And when I check it, they are indeed multiples of each other. But this other method is not clear to me, how could $A$ and $C$ mean anything about the eigenvector, when both of them are connected to $x$, without having to do anything with $y$. But it’s still working. Was it just a coincidence? So is the recommended method for calculating them is just to subtract the eigenvalues from the matrix and look at  $$\left(\begin{array}{r} -B\\A\end{array}\right)\qquad\text{or}\qquad\left(\begin{array}{r} -D\\A \end{array}\right).$$",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
27,eigenvalues and eigenvectors for rectangular matrices,eigenvalues and eigenvectors for rectangular matrices,,"We can generalize matrix inverses from non-singular square matrices to rectangular matrices in general, for example, the well-known Moore–Penrose pseudoinverse. I am wondering how this can be done for eigenvalues and eigenvectors. Though $\det(|A-\lambda I|)=0$ cannot be used any more when $A$ is not square, there is nothing that prevents one to consider $Av=\lambda v$ for non-zero vector $v$ except the possibility of having an inconsistent linear system. Please give your comments and provide some references if there are some. Many thanks. Edit I know SVD. But it does not seem to be the one I wanted. For SVD of real matrix $A$, $A=UDV^T$ where $U, V$ are orthogonal matrices and $D$ is diagonal (with possibly zeros in the diagonal). We only have $AV_{*k}=\sigma_{k}U_{*k}$, $V_{*k}$ is the $k^\text{th}$ column of $V$. Since $V_{*k}$ and $U_{*k}$ are in general different, it does not resemble $Av=\lambda v$ for non-zero vector $v$ in the definition of eigenvectors. Also, even if we can have $A^TAV_{*k}=\lambda V_{*k}$, but this is for the (square) matrix $A^TA$, rather than $A$ itself.","We can generalize matrix inverses from non-singular square matrices to rectangular matrices in general, for example, the well-known Moore–Penrose pseudoinverse. I am wondering how this can be done for eigenvalues and eigenvectors. Though $\det(|A-\lambda I|)=0$ cannot be used any more when $A$ is not square, there is nothing that prevents one to consider $Av=\lambda v$ for non-zero vector $v$ except the possibility of having an inconsistent linear system. Please give your comments and provide some references if there are some. Many thanks. Edit I know SVD. But it does not seem to be the one I wanted. For SVD of real matrix $A$, $A=UDV^T$ where $U, V$ are orthogonal matrices and $D$ is diagonal (with possibly zeros in the diagonal). We only have $AV_{*k}=\sigma_{k}U_{*k}$, $V_{*k}$ is the $k^\text{th}$ column of $V$. Since $V_{*k}$ and $U_{*k}$ are in general different, it does not resemble $Av=\lambda v$ for non-zero vector $v$ in the definition of eigenvectors. Also, even if we can have $A^TAV_{*k}=\lambda V_{*k}$, but this is for the (square) matrix $A^TA$, rather than $A$ itself.",,['linear-algebra']
28,How can I find a basis for the kernel of a homomorphism on a free abelian group?,How can I find a basis for the kernel of a homomorphism on a free abelian group?,,"Let $\phi\colon F\to G$ be a homomorphism of finitely generated abelian groups. If $F$ is free, then $\ker(\phi)$ is also free and thus admits a basis. Question: Is there a general procedure to find a basis for $\ker(\phi)$? As an example, consider the homomorphism $\phi\colon\mathbb Z^3\to (\mathbb Z/2)^2$ given by the matrix $$ \begin{pmatrix} 1&1&0\\0&1&1 \end{pmatrix}. $$ After playing around, I guess that the elements $\begin{pmatrix}0\\2\\0\end{pmatrix}$, $\begin{pmatrix}1\\1\\0\end{pmatrix}$ and $\begin{pmatrix}0\\1\\1\end{pmatrix}$ form a basis. Is this true? How could I see this in a systematic way?","Let $\phi\colon F\to G$ be a homomorphism of finitely generated abelian groups. If $F$ is free, then $\ker(\phi)$ is also free and thus admits a basis. Question: Is there a general procedure to find a basis for $\ker(\phi)$? As an example, consider the homomorphism $\phi\colon\mathbb Z^3\to (\mathbb Z/2)^2$ given by the matrix $$ \begin{pmatrix} 1&1&0\\0&1&1 \end{pmatrix}. $$ After playing around, I guess that the elements $\begin{pmatrix}0\\2\\0\end{pmatrix}$, $\begin{pmatrix}1\\1\\0\end{pmatrix}$ and $\begin{pmatrix}0\\1\\1\end{pmatrix}$ form a basis. Is this true? How could I see this in a systematic way?",,"['linear-algebra', 'abstract-algebra']"
29,transpose of positive matrix is positive,transpose of positive matrix is positive,,"how to prove it? I am talking about matrixes which satisfy: $$( Ax , x ) > 0\quad \text{ for any}\quad \;x \neq 0.$$ How to prove that $A^T\;$ is also positive? $$x^T A  x = ( x^T  A  x )^T$$ and what?",how to prove it? I am talking about matrixes which satisfy: How to prove that is also positive? and what?,"( Ax , x ) > 0\quad \text{ for any}\quad \;x \neq 0. A^T\; x^T A  x = ( x^T  A  x )^T","['linear-algebra', 'matrices']"
30,Area preserving transformations,Area preserving transformations,,"Suppose $A$ is a linear transformation from $R^3$ to $R^3$ and $|det(A)| = 1$. I know that $A$ is volume preserving, but is it also area preserving? For example, if $a$ and $b$ are two vectors in $R^3$ that span a parallelogram, is the area of this parallelogram equal to the area of the paralellogram spanned by $A(a)$ and $A(b)$? Thank you!","Suppose $A$ is a linear transformation from $R^3$ to $R^3$ and $|det(A)| = 1$. I know that $A$ is volume preserving, but is it also area preserving? For example, if $a$ and $b$ are two vectors in $R^3$ that span a parallelogram, is the area of this parallelogram equal to the area of the paralellogram spanned by $A(a)$ and $A(b)$? Thank you!",,['linear-algebra']
31,"How to prove $\mathbf{1}^\top\mathbf{Q}^+\mathbf{Q}=\mathbf{1}^\top$, where $\mathbf{Q}$ is any element-wise squared correlation matrix?","How to prove , where  is any element-wise squared correlation matrix?",\mathbf{1}^\top\mathbf{Q}^+\mathbf{Q}=\mathbf{1}^\top \mathbf{Q},"Let $(X_1,…,X_n)$ be a random vector with $0<\prod_{j=1}^n\text{Var}(X_j)<∞$ . Let $\mathbf{Q}=(\mathbf{q}_{1},…,\mathbf{q}_{n})=(ρ_{jk}^2)_{n×n}$ , where $ρ_{jk}$ is the Pearson correlation coefficient between $X_j$ and $X_k$ . How to prove or disprove $$\mathbf{1}^\top\mathbf{Q}^+\mathbf{Q}=\mathbf{1}^\top$$ where $\mathbf{Q}^+$ is the Moore-Penrose inverse of $\mathbf{Q}$ and $\mathbf{1}^\top$ is a row vector of ones? $$$$ It seems that when any row or column vectors in $\mathbf{Q}$ are linearly dependent, they must be equal. If it is true, let $\mathbf{H}_{n×n}=(\mathbf{h}_1,…,\mathbf{h}_r,\mathbf{0},…,\mathbf{0})^\top$ be the reduced row echelon form of $\mathbf{Q}$ , where $r=\text{rank}(\mathbf{Q})$ ; and there is $\mathbf{h}_{j}^\top\mathbf{h}_{k}=0$ for all $j\neq k$ . Then, let the column indices of the leading ones in each nonzero rows of $\mathbf{H}$ be $j_1,…,j_r$ , and let $$\mathbf{F}_{n×r}=(\mathbf{q}_{j_1},...,\mathbf{q}_{j_r}),\; \mathbf{G}_{r×n}=(\mathbf{h}_1,...,\mathbf{h}_r)^\top$$ According to rank factorization from reduced row echelon forms , we have $\mathbf{Q}=\mathbf{FG}$ According to construction of Moore–Penrose inverse by rank decomposition , we have $$\mathbf{Q}^+=\mathbf{G}^\top(\mathbf{GG^\top})^{-1}(\mathbf{F^\top F})^{-1}\mathbf{F}^\top$$ Thus, \begin{equation} \begin{split} & \mathbf{1}^\top\mathbf{Q}^+\mathbf{Q}  = \mathbf{1}^\top\mathbf{G}^\top(\mathbf{GG^\top})^{-1}\mathbf{G} \\ & = \mathbf{1}^\top \begin{bmatrix}   \mathbf{h}_1 & \cdots & \mathbf{h}_r  \end{bmatrix} \begin{bmatrix}   \mathbf{h}_1^\top\mathbf{h}_1 & \cdots &     \mathbf{h}_1^\top\mathbf{h}_r \\   \vdots &  \ddots &  \vdots \\   \mathbf{h}_r^\top\mathbf{h}_1 & \cdots &      \mathbf{h}_r^\top\mathbf{h}_r \\ \end{bmatrix}^{-1}   \begin{bmatrix}   \mathbf{h}_1^\top \\   \vdots \\ \mathbf{h}_r^\top \\ \end{bmatrix} \\ & = \mathbf{1}^\top\sum_{i=0}^r (\mathbf{h}_i^\top\mathbf{h}_i)^{-1} \mathbf{h}_i \mathbf{h}_i^\top = \mathbf{1}^\top \end{split} \end{equation} where $\mathbf{h}_i^\top\mathbf{h}_i$ is the number of ones in $\mathbf{h}_i$ and $\mathbf{h}_i\mathbf{h}_i^\top$ is an $n×n$ block diagonal matrix with main-diagonal blocks of either ones or zeros. Therefore, the question may turn into how to prove or disprove that, when any rows in $\mathbf{Q}$ are linearly dependent, they must be the same.","Let be a random vector with . Let , where is the Pearson correlation coefficient between and . How to prove or disprove where is the Moore-Penrose inverse of and is a row vector of ones? It seems that when any row or column vectors in are linearly dependent, they must be equal. If it is true, let be the reduced row echelon form of , where ; and there is for all . Then, let the column indices of the leading ones in each nonzero rows of be , and let According to rank factorization from reduced row echelon forms , we have According to construction of Moore–Penrose inverse by rank decomposition , we have Thus, where is the number of ones in and is an block diagonal matrix with main-diagonal blocks of either ones or zeros. Therefore, the question may turn into how to prove or disprove that, when any rows in are linearly dependent, they must be the same.","(X_1,…,X_n) 0<\prod_{j=1}^n\text{Var}(X_j)<∞ \mathbf{Q}=(\mathbf{q}_{1},…,\mathbf{q}_{n})=(ρ_{jk}^2)_{n×n} ρ_{jk} X_j X_k \mathbf{1}^\top\mathbf{Q}^+\mathbf{Q}=\mathbf{1}^\top \mathbf{Q}^+ \mathbf{Q} \mathbf{1}^\top  \mathbf{Q} \mathbf{H}_{n×n}=(\mathbf{h}_1,…,\mathbf{h}_r,\mathbf{0},…,\mathbf{0})^\top \mathbf{Q} r=\text{rank}(\mathbf{Q}) \mathbf{h}_{j}^\top\mathbf{h}_{k}=0 j\neq k \mathbf{H} j_1,…,j_r \mathbf{F}_{n×r}=(\mathbf{q}_{j_1},...,\mathbf{q}_{j_r}),\;
\mathbf{G}_{r×n}=(\mathbf{h}_1,...,\mathbf{h}_r)^\top \mathbf{Q}=\mathbf{FG} \mathbf{Q}^+=\mathbf{G}^\top(\mathbf{GG^\top})^{-1}(\mathbf{F^\top F})^{-1}\mathbf{F}^\top \begin{equation}
\begin{split}
& \mathbf{1}^\top\mathbf{Q}^+\mathbf{Q} 
= \mathbf{1}^\top\mathbf{G}^\top(\mathbf{GG^\top})^{-1}\mathbf{G} \\
& = \mathbf{1}^\top
\begin{bmatrix}
  \mathbf{h}_1 & \cdots & \mathbf{h}_r 
\end{bmatrix}
\begin{bmatrix}
  \mathbf{h}_1^\top\mathbf{h}_1 & \cdots &
    \mathbf{h}_1^\top\mathbf{h}_r \\
  \vdots &  \ddots &  \vdots \\
  \mathbf{h}_r^\top\mathbf{h}_1 & \cdots & 
    \mathbf{h}_r^\top\mathbf{h}_r \\
\end{bmatrix}^{-1}
  \begin{bmatrix}
  \mathbf{h}_1^\top \\
  \vdots \\
\mathbf{h}_r^\top \\
\end{bmatrix} \\
& = \mathbf{1}^\top\sum_{i=0}^r (\mathbf{h}_i^\top\mathbf{h}_i)^{-1} \mathbf{h}_i \mathbf{h}_i^\top = \mathbf{1}^\top
\end{split}
\end{equation} \mathbf{h}_i^\top\mathbf{h}_i \mathbf{h}_i \mathbf{h}_i\mathbf{h}_i^\top n×n \mathbf{Q}","['linear-algebra', 'matrices', 'correlation', 'pseudoinverse', 'idempotents']"
32,Is there a category-theoretic explanation for this analogy between $\mathbf{Set}$ and $\mathbf{Vect}_k$?,Is there a category-theoretic explanation for this analogy between  and ?,\mathbf{Set} \mathbf{Vect}_k,"You can tell whether there exists an injective, surjective, or bijective map $A\to B$ by comparing the cardinalities of $A$ and $B$ . Similarly, you can tell whether there exists an injective, surjective, or bijective linear map $U\to V$ by comparing the dimensions of $U$ and $V$ (i.e., the cardinalities of their Hamel bases). Is there a category-theoretic explanation for this analogy? I'm inclined to consider the functor $\mathbf{Set}\to\mathbf{Vect}_k$ given by formal linear combinations on objects and linear extensions on arrows, but I'm not sure where to go from there.","You can tell whether there exists an injective, surjective, or bijective map by comparing the cardinalities of and . Similarly, you can tell whether there exists an injective, surjective, or bijective linear map by comparing the dimensions of and (i.e., the cardinalities of their Hamel bases). Is there a category-theoretic explanation for this analogy? I'm inclined to consider the functor given by formal linear combinations on objects and linear extensions on arrows, but I'm not sure where to go from there.",A\to B A B U\to V U V \mathbf{Set}\to\mathbf{Vect}_k,"['linear-algebra', 'vector-spaces', 'category-theory']"
33,Question About Basic Rules of Elementary Algebra,Question About Basic Rules of Elementary Algebra,,"I am attempting to learn linear algebra in a very thorough manner. The more questions I ask, the more I realize there are some fundamental issues I don't understand. Here is one. For $x\in{}\mathbb{R}$ , we learn to solve an equation like $5x=15$ by multiplying both sides of the equation by the multiplicative inverse of $5$ , i.e., $\frac{1}{5}$ . Someone tells us that this operation does not change the solution set of the equation, and most people will accept this. Thus we find that the solution set is $S=\{3\}$ However, multiplying both sides of the equation by 0 yields an equation $0=0$ with solution set $S=\mathbb{R}$ . Why the discrepancy?","I am attempting to learn linear algebra in a very thorough manner. The more questions I ask, the more I realize there are some fundamental issues I don't understand. Here is one. For , we learn to solve an equation like by multiplying both sides of the equation by the multiplicative inverse of , i.e., . Someone tells us that this operation does not change the solution set of the equation, and most people will accept this. Thus we find that the solution set is However, multiplying both sides of the equation by 0 yields an equation with solution set . Why the discrepancy?",x\in{}\mathbb{R} 5x=15 5 \frac{1}{5} S=\{3\} 0=0 S=\mathbb{R},"['linear-algebra', 'algebra-precalculus', 'elementary-set-theory', 'logic', 'systems-of-equations']"
34,If $L \in GL_n(\mathbb{Q})$ such that $L^{-1} = L + L^2$ then $3\mid n$,If  such that  then,L \in GL_n(\mathbb{Q}) L^{-1} = L + L^2 3\mid n,"Suppose $L \in GL_n(\mathbb{Q})$ , i.e, a linear invertible map on $\mathbb{Q}^n$ . Then  I want to see that if $L^{-1} = L + L^2$ then $3\mid n$ . I have no idea how to start this.","Suppose , i.e, a linear invertible map on . Then  I want to see that if then . I have no idea how to start this.",L \in GL_n(\mathbb{Q}) \mathbb{Q}^n L^{-1} = L + L^2 3\mid n,"['linear-algebra', 'matrices', 'linear-transformations']"
35,Classifying Vector Spaces without AC,Classifying Vector Spaces without AC,,"Using the axiom of choice we can give a simple classification of all vector spaces over a given field $K$ up to isomorphism: Any $K$ -vector-space $V$ is just isomorphic to $\bigoplus_{i\in B}K$ where $B$ is a basis for $V$ . Given AC we even know that there is some aleph $\aleph_\alpha$ such that $\vert B\vert = \aleph_\alpha$ and thus any vector-space-structure over $K$ appears in the sequence $\lbrace \bigoplus_{i\in \aleph_\alpha}K:\alpha\in Ord\rbrace$ . Now I was wondering whether a similar (weaker) classification still holds without AC. In this case not every vector space has a basis and not every cardinality is equal to some aleph, so clearly  the classification above fails. However one could try something like this: Edit* Thanks to Asaf's answer I realised that my previous attempt at ""covering all vector spaces"" was flawed, so here is a more general approach: As far as I can tell ""new"" vector spaces can be created using the following  ""Gödel operations for vector spaces"": taking the sum or direct product over a family of spaces, taking the quotient of two spaces, constructing the space of homomorphisms between two spaces considering the subspace of any space resulting from the above operations We can thus construct a hierarchy which should be closed under all these operations: Let $X_0:=\lbrace K\rbrace$ and for $\alpha>0$ let $X_\alpha:=$ $\lbrace W:$ there exists $\beta < \alpha$ and $W_0,W_1\in \overline{X}_\beta$ such that $W=W_1/W_0$ or $W=Hom_K(W_0,W_1)$ or there exists $I\in V_\alpha$ (in the Von-Neumann-hierarchy) and a family $\lbrace W_i:i\in I\rbrace \subseteq \overline{X}_\beta$ such that $W=\sum_{i\in I}W_i$ or $W=\prod_{i\in I}W_i\rbrace$ where $\overline{X}_\alpha:=\lbrace W'\leq W: W\in X_\alpha\rbrace$ . I suspect that every vector space over $K$ should be isomorphic to some element of $\bigcup_{\alpha\in Ord}\overline{X}_\alpha$ . Is that correct and is there maybe a more ""practical"" approach to characterizing all vector spaces over $K$ in ZF?","Using the axiom of choice we can give a simple classification of all vector spaces over a given field up to isomorphism: Any -vector-space is just isomorphic to where is a basis for . Given AC we even know that there is some aleph such that and thus any vector-space-structure over appears in the sequence . Now I was wondering whether a similar (weaker) classification still holds without AC. In this case not every vector space has a basis and not every cardinality is equal to some aleph, so clearly  the classification above fails. However one could try something like this: Edit* Thanks to Asaf's answer I realised that my previous attempt at ""covering all vector spaces"" was flawed, so here is a more general approach: As far as I can tell ""new"" vector spaces can be created using the following  ""Gödel operations for vector spaces"": taking the sum or direct product over a family of spaces, taking the quotient of two spaces, constructing the space of homomorphisms between two spaces considering the subspace of any space resulting from the above operations We can thus construct a hierarchy which should be closed under all these operations: Let and for let there exists and such that or or there exists (in the Von-Neumann-hierarchy) and a family such that or where . I suspect that every vector space over should be isomorphic to some element of . Is that correct and is there maybe a more ""practical"" approach to characterizing all vector spaces over in ZF?","K K V \bigoplus_{i\in B}K B V \aleph_\alpha \vert B\vert = \aleph_\alpha K \lbrace \bigoplus_{i\in \aleph_\alpha}K:\alpha\in Ord\rbrace X_0:=\lbrace K\rbrace \alpha>0 X_\alpha:= \lbrace W: \beta < \alpha W_0,W_1\in \overline{X}_\beta W=W_1/W_0 W=Hom_K(W_0,W_1) I\in V_\alpha \lbrace W_i:i\in I\rbrace \subseteq \overline{X}_\beta W=\sum_{i\in I}W_i W=\prod_{i\in I}W_i\rbrace \overline{X}_\alpha:=\lbrace W'\leq W: W\in X_\alpha\rbrace K \bigcup_{\alpha\in Ord}\overline{X}_\alpha K","['linear-algebra', 'vector-spaces', 'set-theory', 'axiom-of-choice', 'transfinite-recursion']"
36,How to define linear equations in an introductory linear algebra class,How to define linear equations in an introductory linear algebra class,,"I believe there is an issue of clarification with respect to the definition of linear equations in many linear algebra texts. Here is a typical one ``A linear equation in the $n$ variables $x_1,x_2, ..., x_n$ is an equation that can be written in the form $a_1x_1+a_2x_2+ ... +a_nx_n=b$ where the coefficients $a_1,a_2,...,  a_n$ and the constant term $b$ are constants.'' No problem with the definition so far except that the text proceeds to state that $x^2-y^3=5$ is not linear because ""it contains powers"".  There is no further explanation. Of course a well-prepared student would say: $(x+y+5)^2-(x+y)^2=35$ appears to contain powers and is a linear equation, how do you know that $x^2-y^3=5$ cannot be simplified? I want to say the issue is with ""[it] can be written"" in the definition. That some operational explanation should be added. Here are my two questions: (a) Which introductory textbooks go deeper in the issue? (b) How do you approach the issue? (Again this is introductory linear algebra, students come in with just calculus I as prerequisite).","I believe there is an issue of clarification with respect to the definition of linear equations in many linear algebra texts. Here is a typical one ``A linear equation in the variables is an equation that can be written in the form where the coefficients and the constant term are constants.'' No problem with the definition so far except that the text proceeds to state that is not linear because ""it contains powers"".  There is no further explanation. Of course a well-prepared student would say: appears to contain powers and is a linear equation, how do you know that cannot be simplified? I want to say the issue is with ""[it] can be written"" in the definition. That some operational explanation should be added. Here are my two questions: (a) Which introductory textbooks go deeper in the issue? (b) How do you approach the issue? (Again this is introductory linear algebra, students come in with just calculus I as prerequisite).","n x_1,x_2, ..., x_n a_1x_1+a_2x_2+ ... +a_nx_n=b a_1,a_2,...,  a_n b x^2-y^3=5 (x+y+5)^2-(x+y)^2=35 x^2-y^3=5",['linear-algebra']
37,Integer Linear Programming Conditional Constraints,Integer Linear Programming Conditional Constraints,,"How can I create a constraint that reflects the following: if $x_{ij} = 1$ AND $x_{jk} = 1$ THEN $x_{ik} = 1$ ? All my variables $x_{ij}$ are binary. To provide some context: I'm trying to create a linear equation system to be solved via the simplex algorithm that provides a solution to the problems schools face when creating class groups. Each student chooses 5 other students that he would like to be with in the following year. The school promises that each student will be in a class with at least one of the students they chose. To create the equation system I decided that my variables will be boolean and represent the following: $x_{ij} = 1$ if student $i$ is with student $j$ and $x_{ij} = 0$ otherwise. Thus, $x_{ij}= x_{ji}$ and $x_{ii} = 1$ . However, I'm having trouble with the following constraint: if student $i$ is together with student $j$ and student $j$ is together with student $k$ , then inevitably student $i$ will be together with student $k$ . This is represented by the constraint I mentioned at the beginning of the question. I tried using the big M approach as mentioned in other questions but to no avail. In these questions there was only one condition but I have two. Even if I solve this problem, how can this be scalable? For example: if $x_{12} = x_{23} = x_{14} = 1$ then $x_{13} = x_{34} = x_{24} = 1$ . Maybe the variables I chose are not correct and I'm overcomplicating things. If this is the case, any guidance in the right direction would be more than welcomed. Thanks for the help in advance!","How can I create a constraint that reflects the following: if AND THEN ? All my variables are binary. To provide some context: I'm trying to create a linear equation system to be solved via the simplex algorithm that provides a solution to the problems schools face when creating class groups. Each student chooses 5 other students that he would like to be with in the following year. The school promises that each student will be in a class with at least one of the students they chose. To create the equation system I decided that my variables will be boolean and represent the following: if student is with student and otherwise. Thus, and . However, I'm having trouble with the following constraint: if student is together with student and student is together with student , then inevitably student will be together with student . This is represented by the constraint I mentioned at the beginning of the question. I tried using the big M approach as mentioned in other questions but to no avail. In these questions there was only one condition but I have two. Even if I solve this problem, how can this be scalable? For example: if then . Maybe the variables I chose are not correct and I'm overcomplicating things. If this is the case, any guidance in the right direction would be more than welcomed. Thanks for the help in advance!",x_{ij} = 1 x_{jk} = 1 x_{ik} = 1 x_{ij} x_{ij} = 1 i j x_{ij} = 0 x_{ij}= x_{ji} x_{ii} = 1 i j j k i k x_{12} = x_{23} = x_{14} = 1 x_{13} = x_{34} = x_{24} = 1,"['linear-algebra', 'linear-programming', 'integer-programming']"
38,Simultaneous diagonalizability of infinitely many endomorphisms in infinite dimension,Simultaneous diagonalizability of infinitely many endomorphisms in infinite dimension,,"Let $V$ be a vector space and let $(f_i)_{i \in I}$ be a family of endomorphisms of $V$ . Question. Suppose that each $f_i$ is diagonalizable and that $f_i$ commutes with $f_j$ for any two indices $i$ and $j$ . Is the family $(f_i)_{i \in I}$ simultaneously diagonalizable? I’m using here the following notion of simultaneous diagonalizability. The family $(f_i)_{i \in I}$ is simultaneously diagonalizable if we have the decomposition $$     V = \bigoplus_{(\lambda_i)_{i \in I}} \operatorname{Eig}( (f_i)_{i \in I}, (\lambda_i)_{i \in I} )   $$ where $\operatorname{Eig}( (f_i)_{i \in I}, (\lambda_i)_{i \in I} ) = \{ v \in V \mid \text{$f_i(v) = \lambda_i v$ for every $i \in I$}\}$ is the common eigenspace of the family $(f_i)_{i \in I}$ for the eigenvalues $(\lambda_i)_{i \in I}$ . Equivalently, $V$ admits a basis consisting of common eigenvectors of the $f_i$ . It is (well-)known that the the answer to the question is yes if $I$ is finite; $V$ is finite-dimensional; or more generally if the linear subspace of $\operatorname{End}(V)$ spanned by the $f_i$ is finite-dimensional. But I don’t know what happens in general if both $I$ is infinite and $V$ is infinite-dimensional.","Let be a vector space and let be a family of endomorphisms of . Question. Suppose that each is diagonalizable and that commutes with for any two indices and . Is the family simultaneously diagonalizable? I’m using here the following notion of simultaneous diagonalizability. The family is simultaneously diagonalizable if we have the decomposition where is the common eigenspace of the family for the eigenvalues . Equivalently, admits a basis consisting of common eigenvectors of the . It is (well-)known that the the answer to the question is yes if is finite; is finite-dimensional; or more generally if the linear subspace of spanned by the is finite-dimensional. But I don’t know what happens in general if both is infinite and is infinite-dimensional.","V (f_i)_{i \in I} V f_i f_i f_j i j (f_i)_{i \in I} (f_i)_{i \in I} 
    V = \bigoplus_{(\lambda_i)_{i \in I}} \operatorname{Eig}( (f_i)_{i \in I}, (\lambda_i)_{i \in I} )
   \operatorname{Eig}( (f_i)_{i \in I}, (\lambda_i)_{i \in I} ) = \{ v \in V \mid \text{f_i(v) = \lambda_i v for every i \in I}\} (f_i)_{i \in I} (\lambda_i)_{i \in I} V f_i I V \operatorname{End}(V) f_i I V","['linear-algebra', 'diagonalization']"
39,Why is a matrix called an operator?,Why is a matrix called an operator?,,"I heard my math teacher mentioning that a matrix is an operator. This confused me, so I looked up what an operator was. In simple terms, it is a function that maps from one space to another space. Examples I can think up are as follows: \begin{equation}   f(x) = x^{2} \end{equation} This function takes some number in the set of all complex numbers, and outputs another number in the set of all complex numbers, and is thus an operator. \begin{equation}   \frac{\mathrm{d}}{\mathrm{d}x} f(x) = \frac{\mathrm{d}}{\mathrm{d}x} (x^{2} + 2x + 3) \end{equation} The $\mathrm{d}/\mathrm{d}x$ here is an operator as it takes a function from a set of functions that may have complex coefficients, and outputs another function that may have complex coefficients. So I understand why both of these are ""operators"". But let's take a look at a matrix: \begin{equation}   \mathbb{M} =      \begin{pmatrix}       3 & 2 \\       4 & 1     \end{pmatrix} \end{equation} How can this be called an operator? It's not taking any element from a set and then outputting an element from another set. I think of it as similar to a scalar, like $3$ —is it not an operator, but you can use an operator, such as addition or multiplication on scalars, and the addition or multiplication can be considered an operator. Why would a matrix such as $\mathbb{M}$ be called an operator? Can any $n$ -rank tensor then be called an operator as well if a matrix is an operator?","I heard my math teacher mentioning that a matrix is an operator. This confused me, so I looked up what an operator was. In simple terms, it is a function that maps from one space to another space. Examples I can think up are as follows: This function takes some number in the set of all complex numbers, and outputs another number in the set of all complex numbers, and is thus an operator. The here is an operator as it takes a function from a set of functions that may have complex coefficients, and outputs another function that may have complex coefficients. So I understand why both of these are ""operators"". But let's take a look at a matrix: How can this be called an operator? It's not taking any element from a set and then outputting an element from another set. I think of it as similar to a scalar, like —is it not an operator, but you can use an operator, such as addition or multiplication on scalars, and the addition or multiplication can be considered an operator. Why would a matrix such as be called an operator? Can any -rank tensor then be called an operator as well if a matrix is an operator?","\begin{equation}
  f(x) = x^{2}
\end{equation} \begin{equation}
  \frac{\mathrm{d}}{\mathrm{d}x} f(x) = \frac{\mathrm{d}}{\mathrm{d}x} (x^{2} + 2x + 3)
\end{equation} \mathrm{d}/\mathrm{d}x \begin{equation}
  \mathbb{M} = 
    \begin{pmatrix}
      3 & 2 \\
      4 & 1
    \end{pmatrix}
\end{equation} 3 \mathbb{M} n","['linear-algebra', 'matrices', 'terminology']"
40,Invariant subspaces to the permuting action of the alternating group.,Invariant subspaces to the permuting action of the alternating group.,,"This question arose while researching Galois groups of random reciprocal polynomials. Throughout $A_n$ is the alternating group. Let us define the following group action of $A_n$ on the vector space $\mathbb{C}^n$ $$\sigma \circlearrowright (x_1,\dots,x_n)=(x_{\sigma^{-1}(1)},\dots,x_{\sigma^{-1}(n)})$$ My interest is to classify all the subspaces invariant to this action. My idea was to classify them in terms of linear systems. the invariant spaces I found are the following ones: $\{0\}\subseteq \mathbb{C}^n$ which corresponds to $x_1 = \dots = x_n = 0$ , $V_1=\operatorname{span}\{(1,\dots, 1)\}\subseteq \mathbb{C}^n$ which corresponds to $x_1 = \dots = x_n$ , $V_2\subseteq \mathbb{C}^n$ which corresponds to $\sum_{i=1}^nx_i=0$ , $\mathbb{C}^n \space$ , the whole space. In case we extend the action to the whole symmetric group $S_n$ it can be shown that those truly are all the invariant subspaces, however, for $A_n$ I suspect I missed a few. Any Help would be much appreciated!","This question arose while researching Galois groups of random reciprocal polynomials. Throughout is the alternating group. Let us define the following group action of on the vector space My interest is to classify all the subspaces invariant to this action. My idea was to classify them in terms of linear systems. the invariant spaces I found are the following ones: which corresponds to , which corresponds to , which corresponds to , , the whole space. In case we extend the action to the whole symmetric group it can be shown that those truly are all the invariant subspaces, however, for I suspect I missed a few. Any Help would be much appreciated!","A_n A_n \mathbb{C}^n \sigma \circlearrowright (x_1,\dots,x_n)=(x_{\sigma^{-1}(1)},\dots,x_{\sigma^{-1}(n)}) \{0\}\subseteq \mathbb{C}^n x_1 = \dots = x_n = 0 V_1=\operatorname{span}\{(1,\dots, 1)\}\subseteq \mathbb{C}^n x_1 = \dots = x_n V_2\subseteq \mathbb{C}^n \sum_{i=1}^nx_i=0 \mathbb{C}^n \space S_n A_n","['linear-algebra', 'group-theory', 'representation-theory', 'group-actions', 'permutation-matrices']"
41,Prove that vectors of a real inner product space are linearly independent.,Prove that vectors of a real inner product space are linearly independent.,,"Let $V$ be a real inner product space. Let $u, v_1, ..., v_m \in V$ such that $$\langle u, v_i \rangle > 0, \ \forall{i}$$ $$ \langle v_i, v_j \rangle \leq 0, \ \forall i \neq j.$$ Prove that vectors $v_1, ..., v_m$ are linearly independent. There is a hint which says if a linear combination of the vectors with nonnegative coefficients is equal to zero, then all the coefficients must be equal to zero. I'm not sure how to carry out this proof using the hint.","Let be a real inner product space. Let such that Prove that vectors are linearly independent. There is a hint which says if a linear combination of the vectors with nonnegative coefficients is equal to zero, then all the coefficients must be equal to zero. I'm not sure how to carry out this proof using the hint.","V u, v_1, ..., v_m \in V \langle u, v_i \rangle > 0, \ \forall{i}  \langle v_i, v_j \rangle \leq 0, \ \forall i \neq j. v_1, ..., v_m","['linear-algebra', 'vector-spaces', 'inner-products']"
42,The set of all sequences of complex numbers with limit $0$ is a subspace of $\mathbb{C}^{\infty}$,The set of all sequences of complex numbers with limit  is a subspace of,0 \mathbb{C}^{\infty},"In Axler's Linear Algebra Done Right , they set an example for a subspace: The set of all sequences of complex numbers with limit 0 is a subspace of $\mathbb{C}^{\infty}$ , where $\mathbb{C}^{\infty}$ denotes the vector space of complex sequences over $\mathbb{C}$ . How can I interpret the ¨with limit 0¨ part? does it mean, looking at an element of the subspace as a function f(z), that $$\lim_{z\rightarrow z_o} f(z) = 0$$ ? How can we go on about to prove sub set of functions is a subspace?","In Axler's Linear Algebra Done Right , they set an example for a subspace: The set of all sequences of complex numbers with limit 0 is a subspace of , where denotes the vector space of complex sequences over . How can I interpret the ¨with limit 0¨ part? does it mean, looking at an element of the subspace as a function f(z), that ? How can we go on about to prove sub set of functions is a subspace?",\mathbb{C}^{\infty} \mathbb{C}^{\infty} \mathbb{C} \lim_{z\rightarrow z_o} f(z) = 0,['linear-algebra']
43,How is the rank of a matrix affected by centering the columns of a matrix?,How is the rank of a matrix affected by centering the columns of a matrix?,,"For some $n$ by $p$ matrix $X$ , I'm trying to figure out how the rank of $X$ is affected if each column in $X$ is centered by the mean of that column (call the centered design matrix $Z$ ). If $p < n$ and $X$ is full column rank, $Z$ is full column rank if multicollinearity is not present. If $p = n$ and $X$ is full rank, $Z$ has rank $n-1$ due to the constraint from centering the variables, regardless of whether multicollinearity is present or not. If $p > n$ and $X$ is full row rank, $Z$ has rank $n-1$ due to the constraint imposed from centering the variables Which means rank of $Z \leq$ rank of $X$ . I'm wondering if these observations are correct, and if so, if there's a technical way to show them, especially a).","For some by matrix , I'm trying to figure out how the rank of is affected if each column in is centered by the mean of that column (call the centered design matrix ). If and is full column rank, is full column rank if multicollinearity is not present. If and is full rank, has rank due to the constraint from centering the variables, regardless of whether multicollinearity is present or not. If and is full row rank, has rank due to the constraint imposed from centering the variables Which means rank of rank of . I'm wondering if these observations are correct, and if so, if there's a technical way to show them, especially a).",n p X X X Z p < n X Z p = n X Z n-1 p > n X Z n-1 Z \leq X,"['linear-algebra', 'matrices', 'matrix-rank']"
44,"If $A,B$ are diagonalisable, does $AB$ diagonalisable imply $BA$ diagonalisable?","If  are diagonalisable, does  diagonalisable imply  diagonalisable?","A,B AB BA","As discussed in this other post , $AB$ and $BA$ always have the same characteristic polynomial, but not necessarily the same minimal polynomial. This means that $AB$ diagonalisable does not imply $BA$ diagonalisable, as shown by the example $$A=\begin{pmatrix}0&1\\0&0\end{pmatrix},\qquad B=\begin{pmatrix}0&0\\0&1\end{pmatrix}.$$ On the other hand, we know that if both $A,B$ are invertible , then the statement is true. As mentioned here , it is also the case that the non-zero spectrum of $AB$ and $BA$ is always the same. I am looking for an answer to the slightly different question: assuming $A,B$ are both diagonalisable, is it true that $AB$ diagonalisable implies $BA$ diagonalisable? The counterexamples I've seen (the post linked above and this one ) all involve at least one non-diagonalisable matrix, hence my question. If the above is not true, what about the more restrictive case of $A,B$ both normal? Or maybe the case with $AB,BA\neq0$ ?","As discussed in this other post , and always have the same characteristic polynomial, but not necessarily the same minimal polynomial. This means that diagonalisable does not imply diagonalisable, as shown by the example On the other hand, we know that if both are invertible , then the statement is true. As mentioned here , it is also the case that the non-zero spectrum of and is always the same. I am looking for an answer to the slightly different question: assuming are both diagonalisable, is it true that diagonalisable implies diagonalisable? The counterexamples I've seen (the post linked above and this one ) all involve at least one non-diagonalisable matrix, hence my question. If the above is not true, what about the more restrictive case of both normal? Or maybe the case with ?","AB BA AB BA A=\begin{pmatrix}0&1\\0&0\end{pmatrix},\qquad B=\begin{pmatrix}0&0\\0&1\end{pmatrix}. A,B AB BA A,B AB BA A,B AB,BA\neq0","['linear-algebra', 'eigenvalues-eigenvectors', 'spectral-theory', 'diagonalization', 'characteristic-polynomial']"
45,Which linear maps $\mathbb{R}^{n^2} \to \mathbb{R}^{n^2} $ map $\text{GL}_n$ into $\text{GL}_n$?,Which linear maps  map  into ?,\mathbb{R}^{n^2} \to \mathbb{R}^{n^2}  \text{GL}_n \text{GL}_n,"Can we characterize all linear maps $\mathbb{R}^{n^2} \to \mathbb{R}^{n^2} $ which map $\text{GL}_n$ into $\text{GL}_n$ ? In particular, is it true that every such map is given by $X \to AXB$ or $X \to AX^TB$ where $A,B \in \text{GL}_n$ and $X^T$ denotes the usual transpose operation on matrices. (Note that changing the metric which respect to we are transposing amounts to changing $A,B$ ).","Can we characterize all linear maps which map into ? In particular, is it true that every such map is given by or where and denotes the usual transpose operation on matrices. (Note that changing the metric which respect to we are transposing amounts to changing ).","\mathbb{R}^{n^2} \to \mathbb{R}^{n^2}  \text{GL}_n \text{GL}_n X \to AXB X \to AX^TB A,B \in \text{GL}_n X^T A,B","['linear-algebra', 'matrices', 'lie-groups', 'matrix-calculus', 'general-linear-group']"
46,Set of Matrices Over Finite Field Whose Pairwise Differences are Invertible,Set of Matrices Over Finite Field Whose Pairwise Differences are Invertible,,"Given the full matrix ring over a finite field, $M := M_{n \times n}(\mathbb{F}_q)$ for prime $q$ and integer $n$ , what can one say about subsets $S$ of $M$ satisfying the condition that: $A,B \in S$ implies $A - B$ is invertible unless $A = B$ (n.b. I'm not making any assumptions on the invertibility of elements of $S$ themselves). Is it possible to construct, or show the existence of, relatively large subsets satisfying this condition? I'm particularly interested in the case where $q$ scales asymptotically and $n$ is a fixed, small constant, and by large subsets I'm attempting to construct $S$ with $\vert S \vert \approx O(q^n)$ , but suspect this is infeasible. As for my own attempts, I thought initially that given the collection of all subspaces of $\mathbb{F}_q^n$ of dimension $n-1$ , one ought to be able to choose one element from each subspace so that the difference of any two elements is full rank, and thus invertible. This would lead to a set $S$ of size ${n \choose n-1}_q = 1+ \dots + q^{n-1}$ , but I am unable to prove that this is either achievable or impossible. Asymptotically, I've only been able to come up with obvious naive sets of size $q$ , but playing around with small cases suggests that larger sets are possible in some cases, just they do not exhibit an obvious pattern (at least not obvious to me).","Given the full matrix ring over a finite field, for prime and integer , what can one say about subsets of satisfying the condition that: implies is invertible unless (n.b. I'm not making any assumptions on the invertibility of elements of themselves). Is it possible to construct, or show the existence of, relatively large subsets satisfying this condition? I'm particularly interested in the case where scales asymptotically and is a fixed, small constant, and by large subsets I'm attempting to construct with , but suspect this is infeasible. As for my own attempts, I thought initially that given the collection of all subspaces of of dimension , one ought to be able to choose one element from each subspace so that the difference of any two elements is full rank, and thus invertible. This would lead to a set of size , but I am unable to prove that this is either achievable or impossible. Asymptotically, I've only been able to come up with obvious naive sets of size , but playing around with small cases suggests that larger sets are possible in some cases, just they do not exhibit an obvious pattern (at least not obvious to me).","M := M_{n \times n}(\mathbb{F}_q) q n S M A,B \in S A - B A = B S q n S \vert S \vert \approx O(q^n) \mathbb{F}_q^n n-1 S {n \choose n-1}_q = 1+ \dots + q^{n-1} q","['linear-algebra', 'matrices', 'finite-fields', 'matrix-rank']"
47,Proving a statement about determinants [duplicate],Proving a statement about determinants [duplicate],,"This question already has an answer here : If $A^2=-I$, Prove that $\det{A}=1$ (1 answer) Closed 4 years ago . Given an $n \times n$ matrix $A$ with real entries such that $A^2 = -I$ , prove that $\det(A) = 1$ . This question is multi-part, but I happen to be stuck on this one. The previous parts showed: $A$ is nonsingular, $n$ is even, and $A$ has no real eigenvalues. I know that $\det(A)^2 = 1$ since $A$ has real entries and $n$ is even, but am not sure how to show that $\det(A)$ , which can be either $1$ or $-1$ , is not $-1$ . Does anyone know how to continue from here?","This question already has an answer here : If $A^2=-I$, Prove that $\det{A}=1$ (1 answer) Closed 4 years ago . Given an matrix with real entries such that , prove that . This question is multi-part, but I happen to be stuck on this one. The previous parts showed: is nonsingular, is even, and has no real eigenvalues. I know that since has real entries and is even, but am not sure how to show that , which can be either or , is not . Does anyone know how to continue from here?",n \times n A A^2 = -I \det(A) = 1 A n A \det(A)^2 = 1 A n \det(A) 1 -1 -1,"['linear-algebra', 'determinant']"
48,Solving systems of quadratic equations,Solving systems of quadratic equations,,"There are efficient algorithms for solving a system of linear equations of the form $$\forall i \qquad 0 = a^i + \sum_j b^i_j x^j$$ or $$\mathbf{0} = \mathbf{a} + \mathbf{b} \cdot \mathbf{x}$$ Are there efficient algorithms for solving a system of quadratic equations of the form $$\forall i \qquad 0 = a^i + \sum_j b^i_j x^j + \sum_k \sum_j c^i_{jk} x^j x^k$$ or $$\mathbf{0} = \mathbf{a} + \mathbf{b} \cdot \mathbf{x} + \mathbf{c} \cdot \mathbf{x} \cdot \mathbf{x}$$ and if so, what are they?","There are efficient algorithms for solving a system of linear equations of the form or Are there efficient algorithms for solving a system of quadratic equations of the form or and if so, what are they?",\forall i \qquad 0 = a^i + \sum_j b^i_j x^j \mathbf{0} = \mathbf{a} + \mathbf{b} \cdot \mathbf{x} \forall i \qquad 0 = a^i + \sum_j b^i_j x^j + \sum_k \sum_j c^i_{jk} x^j x^k \mathbf{0} = \mathbf{a} + \mathbf{b} \cdot \mathbf{x} + \mathbf{c} \cdot \mathbf{x} \cdot \mathbf{x},"['linear-algebra', 'algorithms', 'systems-of-equations', 'quadratics', 'tensors']"
49,"What are the eigenvalues of $X = xx^{T}, x\in\mathbb{R}^{d}$? [duplicate]",What are the eigenvalues of ? [duplicate],"X = xx^{T}, x\in\mathbb{R}^{d}","This question already has answers here : Eigenvalues of the rank one matrix $uv^T$ (3 answers) Closed 4 years ago . I'm given the matrix $X = xx^{T}\in\mathbb{R}^{d \ x \ d}, x\in\mathbb{R}^{d}$ . Does somebody know how to compute $\lambda_{max}(X)$ or $\lambda_{min}(X)$ ? I only want to know these two eigenvalues, the others are not really important. I seem to be stuck. I'm thankful for any answer.","This question already has answers here : Eigenvalues of the rank one matrix $uv^T$ (3 answers) Closed 4 years ago . I'm given the matrix . Does somebody know how to compute or ? I only want to know these two eigenvalues, the others are not really important. I seem to be stuck. I'm thankful for any answer.","X = xx^{T}\in\mathbb{R}^{d \ x \ d}, x\in\mathbb{R}^{d} \lambda_{max}(X) \lambda_{min}(X)",['linear-algebra']
50,"$\exp: \mathfrak{so}(1,n) \rightarrow SO(1,n)$ is surjective",is surjective,"\exp: \mathfrak{so}(1,n) \rightarrow SO(1,n)","I am looking for a reference where it is proven that the exponential map described above is surjective. Here, I am denoting by $\mathfrak{so}(1,n)$ the Lie Algebra of the group $SO(1,n)$ . So we have that \begin{align} SO(1,n) & = \Big\{A\in \mathbb{R}^{n\times 1, n\times 1}:AHA^t=H, \det A = 1\Big\} \\ \mathfrak{so}(1,n) & = \Big\{ B\in\mathbb{R}^{n\times1, n\times1}: BH + HB^t = 0\Big\} \end{align} And also $H = diag(1,-1,-1,...,-1)$ And $\exp$ is the matrix exponential. Now, I know this is true but I'm specifically looking for a book or paper where this is proven.","I am looking for a reference where it is proven that the exponential map described above is surjective. Here, I am denoting by the Lie Algebra of the group . So we have that And also And is the matrix exponential. Now, I know this is true but I'm specifically looking for a book or paper where this is proven.","\mathfrak{so}(1,n) SO(1,n) \begin{align}
SO(1,n) & = \Big\{A\in \mathbb{R}^{n\times 1, n\times 1}:AHA^t=H, \det A = 1\Big\} \\
\mathfrak{so}(1,n) & = \Big\{ B\in\mathbb{R}^{n\times1, n\times1}: BH + HB^t = 0\Big\}
\end{align} H = diag(1,-1,-1,...,-1) \exp","['linear-algebra', 'reference-request', 'lie-groups', 'lie-algebras', 'general-relativity']"
51,"Maximum ""stretch factor"" of linear map","Maximum ""stretch factor"" of linear map",,"Let $A \in M_n (\mathbb R)$ be diagonalizable matrix, let $\lambda_1, \dots, \lambda_n$ be its eigenvalues. I want to know if the maximum ""stretch factor"" of $A$ is the maximum of its unsigned eigenvalues i.e. $$\sup_{\Vert X \Vert = 1} \Vert AX\Vert = \max_{1\leq i\leq n} |\lambda_i|$$ Any help would be appreciated. $\Vert \cdot \Vert$ will be the euclidean norm of $\mathbb R^n$ .","Let be diagonalizable matrix, let be its eigenvalues. I want to know if the maximum ""stretch factor"" of is the maximum of its unsigned eigenvalues i.e. Any help would be appreciated. will be the euclidean norm of .","A \in M_n (\mathbb R) \lambda_1, \dots, \lambda_n A \sup_{\Vert X \Vert = 1} \Vert AX\Vert = \max_{1\leq i\leq n} |\lambda_i| \Vert \cdot \Vert \mathbb R^n","['linear-algebra', 'matrices']"
52,Eigenvalue of rank 2 matrix,Eigenvalue of rank 2 matrix,,"Suppose $x$ and $y$ are two linearly independent nonzero vectors in $\mathbb{R}^n$ . Then we know that the matrix $M = xy^T + yx^T$ is a rank 2 matrix. I seem to have made the observation that the two nontrivial eigenvalues of $M$ are given by $\lambda_{1,2} = x^Ty \pm \|x\|\|y\|$ . For example if $x = \begin{bmatrix}a & b\end{bmatrix}^T$ and $x = \begin{bmatrix}c & d\end{bmatrix}^T$ , then the characteristic polynomial is given by $${{s}^{2}}-\left( 2{{x}^{T}}y \right)\cdot s-{{\left( bc-ad \right)}^{2}}$$ and application of the quadratic formula verifies that discriminant part (under the square root) is given by: $4\left( {{a}^{2}}+{{b}^{2}} \right)\left( {{c}^{2}}+{{d}^{2}} \right)$ so the observation holds. I have tried some numeric computation for $n=3,4,5$ and the formula seems to work. But I haven't  been able to prove it. Approach one: Write, \begin{align}   \det \left( M-\lambda I \right) &=\det \left( x{{y}^{T}}+\underbrace{\left( -y{{x}^{T}}-\lambda I \right)}_{:=A} \right) \\   & =-\left( 1+{{y}^{T}}{{\left( -y{{x}^{T}}-\lambda I \right)}^{-1}}x \right)\det \left( y{{x}^{T}}+\lambda I \right) \\  \end{align} and try to use the lemma but that didn't take me anywhere. Approach two: \begin{align}   & Mv=x{{y}^{T}}v+y{{x}^{T}}v=\left( {{x}^{T}}y+\left\| x \right\|\left\| y \right\| \right)v \\   & \Rightarrow x{{y}^{T}}v+y{{x}^{T}}v={{x}^{T}}yv+v\sqrt{{{x}^{T}}x{{y}^{T}}y} \\  \end{align} and try to match the left and right hand side, but I couldn't get that to work either. Can someone provide a hint or proof? Also, how would one go about deriving what the eigenvectors corresponding to the two eigenvalues look like?","Suppose and are two linearly independent nonzero vectors in . Then we know that the matrix is a rank 2 matrix. I seem to have made the observation that the two nontrivial eigenvalues of are given by . For example if and , then the characteristic polynomial is given by and application of the quadratic formula verifies that discriminant part (under the square root) is given by: so the observation holds. I have tried some numeric computation for and the formula seems to work. But I haven't  been able to prove it. Approach one: Write, and try to use the lemma but that didn't take me anywhere. Approach two: and try to match the left and right hand side, but I couldn't get that to work either. Can someone provide a hint or proof? Also, how would one go about deriving what the eigenvectors corresponding to the two eigenvalues look like?","x y \mathbb{R}^n M = xy^T + yx^T M \lambda_{1,2} = x^Ty \pm \|x\|\|y\| x = \begin{bmatrix}a & b\end{bmatrix}^T x = \begin{bmatrix}c & d\end{bmatrix}^T {{s}^{2}}-\left( 2{{x}^{T}}y \right)\cdot s-{{\left( bc-ad \right)}^{2}} 4\left( {{a}^{2}}+{{b}^{2}} \right)\left( {{c}^{2}}+{{d}^{2}} \right) n=3,4,5 \begin{align}
  \det \left( M-\lambda I \right) &=\det \left( x{{y}^{T}}+\underbrace{\left( -y{{x}^{T}}-\lambda I \right)}_{:=A} \right) \\ 
 & =-\left( 1+{{y}^{T}}{{\left( -y{{x}^{T}}-\lambda I \right)}^{-1}}x \right)\det \left( y{{x}^{T}}+\lambda I \right) \\ 
\end{align} \begin{align}
  & Mv=x{{y}^{T}}v+y{{x}^{T}}v=\left( {{x}^{T}}y+\left\| x \right\|\left\| y \right\| \right)v \\ 
 & \Rightarrow x{{y}^{T}}v+y{{x}^{T}}v={{x}^{T}}yv+v\sqrt{{{x}^{T}}x{{y}^{T}}y} \\ 
\end{align}","['linear-algebra', 'eigenvalues-eigenvectors']"
53,$A$ nilpotent and $A+c_iB$ is nilpotent then $B$ is nilpotent.,nilpotent and  is nilpotent then  is nilpotent.,A A+c_iB B,"Let $A$ and $B$ be $n \times n$ matrices over some field with $A$ nilpotent. Now let $c_1,\ldots,c_{n+1}$ be $n+1$ distinct scalars such that $A+c_i B$ is nilpotent for all $i=1, \ldots,n+1$ . Then how can I show that $B$ is also nilpotent? Thanks",Let and be matrices over some field with nilpotent. Now let be distinct scalars such that is nilpotent for all . Then how can I show that is also nilpotent? Thanks,"A B n \times n A c_1,\ldots,c_{n+1} n+1 A+c_i B i=1, \ldots,n+1 B","['linear-algebra', 'abstract-algebra', 'matrices', 'linear-transformations', 'nilpotence']"
54,Can a vector space over finite field be written as union of finite number of proper subspaces?,Can a vector space over finite field be written as union of finite number of proper subspaces?,,"Recently, I solved a problem that says- If $V$ is a vector space over an infinite field. Prove that, V cannot be written as set-thoretic union of a finite number of proper subspaces. But is this result true in case of finite field? . I can't get such an example where a vector space over finite field can be written as union of finite number of proper subspaces. Can anybody give such an example? Thanks for assistance in advance.","Recently, I solved a problem that says- If is a vector space over an infinite field. Prove that, V cannot be written as set-thoretic union of a finite number of proper subspaces. But is this result true in case of finite field? . I can't get such an example where a vector space over finite field can be written as union of finite number of proper subspaces. Can anybody give such an example? Thanks for assistance in advance.",V,"['linear-algebra', 'vector-spaces', 'finite-fields']"
55,Proving right angle using vectors,Proving right angle using vectors,,"Let $ABCD$ be a rectangle, $E$ midpoint of $\overline{DC}$ and $G$ point on $\overline{AC}$ such that $\vec{BG}$ is perpendicular to $\vec{AC}$ . Also, let $F$ be  a midpoint of $\overline{AG}$ . Prove that angle $\angle BFE =\pi/2$ . So, I need to prove that $\vec{EF}\cdot \vec{FB}=0$ .  I tried to just express these vectors as sums of vectors of rectangle but haven't find any ""elegant"" way to prove it. I did found one ""ugly"" way to prove it: Let $|AB|=a$ , $|AD|=b$ and $\vec{AB}=\vec{a}$ , $\vec{AD}=\vec{b}$ , so $\vec{AC}=\vec{a}+\vec{b}$ and since $\vec{a}\cdot\vec{b}=0$ we have $|AC|^2=a^2+b^2$ .  We can see that $\triangle ABC$ and $\triangle BCG$ are similar so $$|CG|=|CB|^2/|AC|$$ and since $\vec{CG}=\lambda \cdot  \vec{CA}$ we get $$\lambda=\frac{b^2}{a^2+b^2}.$$ Now, we can express $\vec{EF}$ and $\vec{FB}$ in the terms of $\vec{a}$ , $\vec{b}$ , more precisely: $$\vec{EF}=\frac{1}{2}\left(\frac{-b^2}{a^2+b^2}\right)\vec{a}+\frac{1}{2}\left(\frac{a^2}{a^2+b^2}-2\right)\vec{b}$$ and $$\vec{FB}=\frac{1}{2}\left(2-\frac{a^2}{a^2+b^2}\right)\vec{a}+\frac{1}{2}\left(\frac{-a^2}{a^2+b^2}\right)\vec{b}$$ and if we multiply we get that dot product is $0$ . But as you can see, this $\lambda$ is ""weird"" and I am wondering if someone sees a more elegant way to prove this ?","Let be a rectangle, midpoint of and point on such that is perpendicular to . Also, let be  a midpoint of . Prove that angle . So, I need to prove that .  I tried to just express these vectors as sums of vectors of rectangle but haven't find any ""elegant"" way to prove it. I did found one ""ugly"" way to prove it: Let , and , , so and since we have .  We can see that and are similar so and since we get Now, we can express and in the terms of , , more precisely: and and if we multiply we get that dot product is . But as you can see, this is ""weird"" and I am wondering if someone sees a more elegant way to prove this ?",ABCD E \overline{DC} G \overline{AC} \vec{BG} \vec{AC} F \overline{AG} \angle BFE =\pi/2 \vec{EF}\cdot \vec{FB}=0 |AB|=a |AD|=b \vec{AB}=\vec{a} \vec{AD}=\vec{b} \vec{AC}=\vec{a}+\vec{b} \vec{a}\cdot\vec{b}=0 |AC|^2=a^2+b^2 \triangle ABC \triangle BCG |CG|=|CB|^2/|AC| \vec{CG}=\lambda \cdot  \vec{CA} \lambda=\frac{b^2}{a^2+b^2}. \vec{EF} \vec{FB} \vec{a} \vec{b} \vec{EF}=\frac{1}{2}\left(\frac{-b^2}{a^2+b^2}\right)\vec{a}+\frac{1}{2}\left(\frac{a^2}{a^2+b^2}-2\right)\vec{b} \vec{FB}=\frac{1}{2}\left(2-\frac{a^2}{a^2+b^2}\right)\vec{a}+\frac{1}{2}\left(\frac{-a^2}{a^2+b^2}\right)\vec{b} 0 \lambda,"['linear-algebra', 'geometry', 'vectors', 'euclidean-geometry', 'analytic-geometry']"
56,Farkas' lemma proof explanation,Farkas' lemma proof explanation,,"I have been studying the proof of the following variant of Farkas' Lemma: A system of linear equations $A \mathbf{x} = \mathbf{b}$ in $d$ variables has a solution iff for all $\mathbf{\lambda} \in \mathbb{R}^d, \lambda^T A = \mathbf{0}^T$ implies $\lambda^T \mathbf{b} = 0$. For the direction $\Rightarrow$ the proof is easy: Suppose that $A\mathbf{x} = \mathbf{b}$ has a solution $\bar{\mathbf{x}}$. Then $\lambda^T A = \mathbf{0}^T \Rightarrow \lambda^TA\bar{\mathbf{x}}=\lambda^{T}\mathbf{b}=0$ For the other direction the proof that the notes give proceeds as follows: The implication $\lambda^TA= \mathbf{0}^T \Rightarrow \lambda^T\mathbf{b}=0$ means that both matrices $A \in \mathbb{R}^{n\times d}$ and $(A|\mathbf{b}) \in \mathbb{R}^{n\times (d+1)}$ have the same linear dependencies among their rows, therefore the same row rank, which means that they have the same column rank. That means that $\mathbf{b}$ is a linear combination of the columns of $A$ which implies that $A\mathbf{x} = \mathbf{b}$ has a solution. What I am missing in the second part of the proof is how the starting claim means that both matrices have the same linear dependencies among their rows. Can anyone give an intuitive explanation?","I have been studying the proof of the following variant of Farkas' Lemma: A system of linear equations $A \mathbf{x} = \mathbf{b}$ in $d$ variables has a solution iff for all $\mathbf{\lambda} \in \mathbb{R}^d, \lambda^T A = \mathbf{0}^T$ implies $\lambda^T \mathbf{b} = 0$. For the direction $\Rightarrow$ the proof is easy: Suppose that $A\mathbf{x} = \mathbf{b}$ has a solution $\bar{\mathbf{x}}$. Then $\lambda^T A = \mathbf{0}^T \Rightarrow \lambda^TA\bar{\mathbf{x}}=\lambda^{T}\mathbf{b}=0$ For the other direction the proof that the notes give proceeds as follows: The implication $\lambda^TA= \mathbf{0}^T \Rightarrow \lambda^T\mathbf{b}=0$ means that both matrices $A \in \mathbb{R}^{n\times d}$ and $(A|\mathbf{b}) \in \mathbb{R}^{n\times (d+1)}$ have the same linear dependencies among their rows, therefore the same row rank, which means that they have the same column rank. That means that $\mathbf{b}$ is a linear combination of the columns of $A$ which implies that $A\mathbf{x} = \mathbf{b}$ has a solution. What I am missing in the second part of the proof is how the starting claim means that both matrices have the same linear dependencies among their rows. Can anyone give an intuitive explanation?",,"['linear-algebra', 'convex-optimization', 'linear-programming', 'matrix-rank']"
57,Finding eigenvalues for a rotational transformation,Finding eigenvalues for a rotational transformation,,"The question verbatim goes as following: Let the matrix $A$ be the standard matrix for the matrix transformation   $T_{A} : R^{2} -> R^{2}$ which is given with the rotation $\pi/6$   radians. Calculate all real eigenvalues for the matrix $A$ (i.e all eigenvalues which are real, $\lambda \in R$. The answer to this is: The matrix $A$ lacks real eigenvalues. This can be seen with out performing any calculations, since $Ax$ corresponds to the rotation   $\pi/6$ radians, $Ax$ = $\lambda x$ can only be satisfied by the   zero-vector. Is this because the rotation operation only rotates the coordinates and not scales them, this is my intuition behind the reasoning of the answer. However i'm not entirely sure why this transformation does't have any eigenvalues? I would be thankful if somebody could expand this a little for me.","The question verbatim goes as following: Let the matrix $A$ be the standard matrix for the matrix transformation   $T_{A} : R^{2} -> R^{2}$ which is given with the rotation $\pi/6$   radians. Calculate all real eigenvalues for the matrix $A$ (i.e all eigenvalues which are real, $\lambda \in R$. The answer to this is: The matrix $A$ lacks real eigenvalues. This can be seen with out performing any calculations, since $Ax$ corresponds to the rotation   $\pi/6$ radians, $Ax$ = $\lambda x$ can only be satisfied by the   zero-vector. Is this because the rotation operation only rotates the coordinates and not scales them, this is my intuition behind the reasoning of the answer. However i'm not entirely sure why this transformation does't have any eigenvalues? I would be thankful if somebody could expand this a little for me.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'linear-transformations', 'rotations']"
58,How far can two symmetric matrices be from being non-commuting (in a suitable norm)?,How far can two symmetric matrices be from being non-commuting (in a suitable norm)?,,"In some work I've been doing, I've come across a problem which involves a family of symmetric matrices $A_k\in\mathbb R^{n\times n}$ for $1\le k\le m$. The problem is very well-behaved when $A_k$ pair-wise commute, but things become harder if the $A_k$ are very far from commuting. What I would like to do, then, is to maximize $$  \| [A,B]\|_{HS}^2,\quad\hbox{ subject to } \|A\|_{HS},\|B\|_{HS}\le1 $$ where $\|\cdot\|_{HS}$ denotes the Hilbert-Schmidt norm, $\|A\|_{HS}^2=\sum_{i,j}A_{ij}^2=\operatorname{tr}(A^TA)$. I have found that for any maximal pair $(A,B)$, we must have that  $$ \operatorname{tr}ACBB-\operatorname{tr}CBAB=\lambda\operatorname{tr}CA, \quad \operatorname{tr}AACB-\operatorname{tr}ABCA=\eta\operatorname{tr}CB $$ for all $C\in\mathbb R^{n\times n}$, and $\lambda,\eta\ge0$ (this is equivalent to $(A,B)$ being a local maximum). This is equivalent to the condition that $$ ABB-BAB=\lambda A,\quad BAA-ABA=\eta B. $$ But can this condition be reduced further? I'm really more interested in finding the maximum, than finding which $A,B$ attain it.","In some work I've been doing, I've come across a problem which involves a family of symmetric matrices $A_k\in\mathbb R^{n\times n}$ for $1\le k\le m$. The problem is very well-behaved when $A_k$ pair-wise commute, but things become harder if the $A_k$ are very far from commuting. What I would like to do, then, is to maximize $$  \| [A,B]\|_{HS}^2,\quad\hbox{ subject to } \|A\|_{HS},\|B\|_{HS}\le1 $$ where $\|\cdot\|_{HS}$ denotes the Hilbert-Schmidt norm, $\|A\|_{HS}^2=\sum_{i,j}A_{ij}^2=\operatorname{tr}(A^TA)$. I have found that for any maximal pair $(A,B)$, we must have that  $$ \operatorname{tr}ACBB-\operatorname{tr}CBAB=\lambda\operatorname{tr}CA, \quad \operatorname{tr}AACB-\operatorname{tr}ABCA=\eta\operatorname{tr}CB $$ for all $C\in\mathbb R^{n\times n}$, and $\lambda,\eta\ge0$ (this is equivalent to $(A,B)$ being a local maximum). This is equivalent to the condition that $$ ABB-BAB=\lambda A,\quad BAA-ABA=\eta B. $$ But can this condition be reduced further? I'm really more interested in finding the maximum, than finding which $A,B$ attain it.",,"['linear-algebra', 'matrices', 'convex-optimization']"
59,Can one know the rank of a matrix product given the rank of one of them?,Can one know the rank of a matrix product given the rank of one of them?,,"Suppose we have some matrix $Q\in\mathbb{R}^{m\times n}$. We have another matrix $T\in\mathbb{R}^{n\times n}$, which happens to be full-rank. I'm wondering if it is possible to know the rank of the matrix built as $$\hat{Q}=QT$$ My intuition says that, due to $T$ being an injection, $\mathrm{rank}(\hat{Q})=\mathrm{rank}(Q)$, but I'm not sure about this. I find it easy to arrive to this conclusion if the matrix was $\hat{Q}=TQ$, with $Q\in\mathbb{R}^{n\times m}$ but not with the $T$ at the right. Any ideas?","Suppose we have some matrix $Q\in\mathbb{R}^{m\times n}$. We have another matrix $T\in\mathbb{R}^{n\times n}$, which happens to be full-rank. I'm wondering if it is possible to know the rank of the matrix built as $$\hat{Q}=QT$$ My intuition says that, due to $T$ being an injection, $\mathrm{rank}(\hat{Q})=\mathrm{rank}(Q)$, but I'm not sure about this. I find it easy to arrive to this conclusion if the matrix was $\hat{Q}=TQ$, with $Q\in\mathbb{R}^{n\times m}$ but not with the $T$ at the right. Any ideas?",,"['linear-algebra', 'matrices', 'matrix-rank']"
60,Relating the trace and norm in Galois theory with the trace and determinant of a certain matrix,Relating the trace and norm in Galois theory with the trace and determinant of a certain matrix,,"Suppose that $F$ is a finite dimensional extension field of $K$ (both subfields of $\mathbb{C}$). Given $u \in F$, let $g_u : F \to F$ be defined by $v \mapsto uv$. If $B$ is any ($K$-basis) for $F$ and $M = [g_u]_B$, then I would like to show that $\mbox{trace}(M) = \mbox{Tr}_{F/K}(u)$ and moreover $\mbox{det}(M) = \mbox{N}_{F/K}(u)$. I've been trying to prove that this at least holds in the case that $F = K(u)$ (where we would have a basis of the form $\{u^0, u^1, \dots u^n\}$) but haven't succeeded. I am using the Hungerford notion of trace and norm. In particular, let $\sigma_1, \dots, \sigma_t$ be all the distinct $K$-monomorphisms $F \to \mathbb{C}$. If $a \in F$, then $N_{F/K}(a) = \sigma_1(a)\sigma_2(a) \dots \sigma_r(a)$ and $\mbox{Tr}_{F/K}(u) = \sigma_1(a) + \dots + \sigma_r(a)$.","Suppose that $F$ is a finite dimensional extension field of $K$ (both subfields of $\mathbb{C}$). Given $u \in F$, let $g_u : F \to F$ be defined by $v \mapsto uv$. If $B$ is any ($K$-basis) for $F$ and $M = [g_u]_B$, then I would like to show that $\mbox{trace}(M) = \mbox{Tr}_{F/K}(u)$ and moreover $\mbox{det}(M) = \mbox{N}_{F/K}(u)$. I've been trying to prove that this at least holds in the case that $F = K(u)$ (where we would have a basis of the form $\{u^0, u^1, \dots u^n\}$) but haven't succeeded. I am using the Hungerford notion of trace and norm. In particular, let $\sigma_1, \dots, \sigma_t$ be all the distinct $K$-monomorphisms $F \to \mathbb{C}$. If $a \in F$, then $N_{F/K}(a) = \sigma_1(a)\sigma_2(a) \dots \sigma_r(a)$ and $\mbox{Tr}_{F/K}(u) = \sigma_1(a) + \dots + \sigma_r(a)$.",,"['linear-algebra', 'abstract-algebra', 'field-theory', 'galois-theory']"
61,Matrix square roots of -I,Matrix square roots of -I,,"Since we can see matrices as generalizations of complex numbers, I asked myself if there is a way to classify those matrices which are the ""Basis"" for the complex part. That is, I would like to identify the set of $n\times n$ real valued matrices $M$ whose square $M^2$ is equal to $-I$, where $I$ is the $n\times n$-identity matrix. Is this set already classified? The matrix $J = \left( \begin{smallmatrix}0 & -1\\1 & 0 \end{smallmatrix} \right)$ satisfies $J^{2} = -I$ in the 2-dimensional case. EDIT: Thanks to your comments and answers I have the following observation (if its wrong, please tell me): Suppose that $M$ satisfies that $M^t=-M$, then we can observe for two vectors $x,y\in\mathbb{R}^{2n}$ that $y^tAx=(Ax)^{t}y=x^{t}A^ty=-x^tAy$ My interpretation is that if the angle between $x$ and $Ay$ is $\alpha$ then $\pi+\alpha$ is the angle between $Ax$ and $y$, since ${\displaystyle \cos \;x=-\cos(x+\pi )}$ (here we suppose that the length of the vectors are not relevant). If now $y=x$ and then we can see that $Ax$ is orthogonal to $x$. Is it true that $A^2$ will be a rotation of 180 degrees, right? Because of vector length we will have something like $A^2=-CI$, where $C$ is just a constant. Is that true?","Since we can see matrices as generalizations of complex numbers, I asked myself if there is a way to classify those matrices which are the ""Basis"" for the complex part. That is, I would like to identify the set of $n\times n$ real valued matrices $M$ whose square $M^2$ is equal to $-I$, where $I$ is the $n\times n$-identity matrix. Is this set already classified? The matrix $J = \left( \begin{smallmatrix}0 & -1\\1 & 0 \end{smallmatrix} \right)$ satisfies $J^{2} = -I$ in the 2-dimensional case. EDIT: Thanks to your comments and answers I have the following observation (if its wrong, please tell me): Suppose that $M$ satisfies that $M^t=-M$, then we can observe for two vectors $x,y\in\mathbb{R}^{2n}$ that $y^tAx=(Ax)^{t}y=x^{t}A^ty=-x^tAy$ My interpretation is that if the angle between $x$ and $Ay$ is $\alpha$ then $\pi+\alpha$ is the angle between $Ax$ and $y$, since ${\displaystyle \cos \;x=-\cos(x+\pi )}$ (here we suppose that the length of the vectors are not relevant). If now $y=x$ and then we can see that $Ax$ is orthogonal to $x$. Is it true that $A^2$ will be a rotation of 180 degrees, right? Because of vector length we will have something like $A^2=-CI$, where $C$ is just a constant. Is that true?",,"['linear-algebra', 'matrices']"
62,"If for any $k$, $\sum\limits_{n=0}^\infty a_n^k=\sum\limits_{n=0}^\infty b_n^k$, then$(a_n)=(b_{σ(n)}),\ σ \in{\mathfrak S}_{\mathbb N}$","If for any , , then","k \sum\limits_{n=0}^\infty a_n^k=\sum\limits_{n=0}^\infty b_n^k (a_n)=(b_{σ(n)}),\ σ \in{\mathfrak S}_{\mathbb N}","Let $(a_n)_{n≥0}$ and $(b_n)_{n≥0}$ be two sequences of a nomed algebra such that $\sum{\| a_n\|}$ and $\sum{\| b_n\|}$ converge, and$ $$$\forall n, \  a_n, b_n \neq 0$$ Show that $(\forall  k \in \mathbb N^*,  \ \sum_{n=0}^{\infty}{a_n}^k = \sum_{n=0}^{\infty}{b_n}^k) \implies \exists\ \sigma \in{\mathfrak S}_{\mathbb N} \ ,\ (a_n) = (b_{\sigma(n)})$ The vector-space should be a finite-dimensional $\mathbb R$-vector-space. I do not see how to deal with the hypothesis. If you have ideas/hints... It would be relevant to deal with complex sequences even if it is less general. But I don't think one can generalize using real and complex cases.","Let $(a_n)_{n≥0}$ and $(b_n)_{n≥0}$ be two sequences of a nomed algebra such that $\sum{\| a_n\|}$ and $\sum{\| b_n\|}$ converge, and$ $$$\forall n, \  a_n, b_n \neq 0$$ Show that $(\forall  k \in \mathbb N^*,  \ \sum_{n=0}^{\infty}{a_n}^k = \sum_{n=0}^{\infty}{b_n}^k) \implies \exists\ \sigma \in{\mathfrak S}_{\mathbb N} \ ,\ (a_n) = (b_{\sigma(n)})$ The vector-space should be a finite-dimensional $\mathbb R$-vector-space. I do not see how to deal with the hypothesis. If you have ideas/hints... It would be relevant to deal with complex sequences even if it is less general. But I don't think one can generalize using real and complex cases.",,"['linear-algebra', 'sequences-and-series']"
63,"Existence of $V \subset \mathbb{R}^n : V, V^\perp \cap \mathbb{R}_{\geq0}^n = 0$",Existence of,"V \subset \mathbb{R}^n : V, V^\perp \cap \mathbb{R}_{\geq0}^n = 0",Is there a subspace $V \subset \mathbb{R}^n$ with $V \cap \mathbb{R}_{\geq0}^n = V^\perp \cap \mathbb{R}_{\geq0}^n = 0$? My geometric intuition tells me that there is no such $V$ but i'am completely stuck on proof.,Is there a subspace $V \subset \mathbb{R}^n$ with $V \cap \mathbb{R}_{\geq0}^n = V^\perp \cap \mathbb{R}_{\geq0}^n = 0$? My geometric intuition tells me that there is no such $V$ but i'am completely stuck on proof.,,"['linear-algebra', 'inequality']"
64,Find for which real parameter a matrix is diagonalisable,Find for which real parameter a matrix is diagonalisable,,"We're given the folowing matrix. $$A = \begin{bmatrix}0 & 0&2h\\1 & 0&-2-3h\\0&1&3+h \end{bmatrix}$$ and we're asked to find for which $h \in \mathbb{R}$ this matrix is diagonalisable. Here's my approach. I've been able to find the characteristic polynomial of the matrix, which is: $$-\lambda^3+(3+h)\lambda^2 - (2+3h)\lambda + 2h $$ Since I didn't know what to do from here, I tried to see if the matrix is diagonalisable with $h=1$. This gives me the following polynomial: $$-\lambda^3+4\lambda^2-5\lambda+2 \\ = \ - (\lambda-2)(\lambda-1)^2$$ Therefore, to see if the matrix is diagonalisable for $h=1$, I need to see if $\mathrm{dim}(\mathrm{Ker}(A- 1 \cdot I))$ (where $I$ is the indentity matrix) is equal to $2$.$$ A-1\cdot I= \begin{bmatrix}-1&0&2\\1 & -1&-5\\0&1&3 \end{bmatrix}$$ which in echelon form gives me: $$\begin{bmatrix}1 & 0&-2\\0&1&3\\0&0&0 \end{bmatrix}$$ We can see that $\mathrm{dim}(\mathrm{Ker}(A- 1 \cdot I)) \neq 2$. Thus I would conclude that for $h=1$, the matrix A is not diagonalisable. But the correction sheet says that A is diagonalisable for all $h\in \mathbb{R}$. So first of all, I don't understand why it doesn't work $h=1$ (I guess I made a mistake somewhere, but I can't find it). Then, I don't know how I'm supposed to find out that the matrix is diagonalisable for all  $h \in \mathbb{R}$.","We're given the folowing matrix. $$A = \begin{bmatrix}0 & 0&2h\\1 & 0&-2-3h\\0&1&3+h \end{bmatrix}$$ and we're asked to find for which $h \in \mathbb{R}$ this matrix is diagonalisable. Here's my approach. I've been able to find the characteristic polynomial of the matrix, which is: $$-\lambda^3+(3+h)\lambda^2 - (2+3h)\lambda + 2h $$ Since I didn't know what to do from here, I tried to see if the matrix is diagonalisable with $h=1$. This gives me the following polynomial: $$-\lambda^3+4\lambda^2-5\lambda+2 \\ = \ - (\lambda-2)(\lambda-1)^2$$ Therefore, to see if the matrix is diagonalisable for $h=1$, I need to see if $\mathrm{dim}(\mathrm{Ker}(A- 1 \cdot I))$ (where $I$ is the indentity matrix) is equal to $2$.$$ A-1\cdot I= \begin{bmatrix}-1&0&2\\1 & -1&-5\\0&1&3 \end{bmatrix}$$ which in echelon form gives me: $$\begin{bmatrix}1 & 0&-2\\0&1&3\\0&0&0 \end{bmatrix}$$ We can see that $\mathrm{dim}(\mathrm{Ker}(A- 1 \cdot I)) \neq 2$. Thus I would conclude that for $h=1$, the matrix A is not diagonalisable. But the correction sheet says that A is diagonalisable for all $h\in \mathbb{R}$. So first of all, I don't understand why it doesn't work $h=1$ (I guess I made a mistake somewhere, but I can't find it). Then, I don't know how I'm supposed to find out that the matrix is diagonalisable for all  $h \in \mathbb{R}$.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'diagonalization']"
65,Obtaining Determinant without Expanding,Obtaining Determinant without Expanding,,"The question is #653 from Golan's Linear Algebra Every Graduate Student Should Know and while it doesn't explicitly say not to just expand and factor it, I think that's the spirit of the question. $$ \begin{vmatrix} -2a & a+b & a+c\\  a+b & -2b & b+c \\  c+a & c+b & -2c \\ \end{vmatrix} $$ Since the answer is $$ 4(a+b)(b+c)(a+c) $$ I am inclined to think it has something to do with wisely dividing out (a+b), etc. from particular rows or some other linear combination tricks but I can't seem to quite figure it out. Any ideas?","The question is #653 from Golan's Linear Algebra Every Graduate Student Should Know and while it doesn't explicitly say not to just expand and factor it, I think that's the spirit of the question. $$ \begin{vmatrix} -2a & a+b & a+c\\  a+b & -2b & b+c \\  c+a & c+b & -2c \\ \end{vmatrix} $$ Since the answer is $$ 4(a+b)(b+c)(a+c) $$ I am inclined to think it has something to do with wisely dividing out (a+b), etc. from particular rows or some other linear combination tricks but I can't seem to quite figure it out. Any ideas?",,"['linear-algebra', 'matrices', 'determinant']"
66,Prove the bounds for the matrix exponential,Prove the bounds for the matrix exponential,,"Prove that $ e^{-t\|A\|} \le \|e^{tA}\| \le e^{t\|A\|} $ for any matrix $A \in \mathbb{R}^{n \times n}, t > 0$, where $e^{tA} = I + tA + t^2 \dfrac{A^2}{2!} + t^3 \dfrac{A^3}{3!} + \cdots$ It is easy to prove the right part of the inequality $ \|e^{tA}\| \le e^{t\|A\|} $ for $t > 0$: $\|e^{tA}\| = \| I + tA + t^2 \dfrac{A^2}{2!} + t^3 \dfrac{A^3}{3!} + \cdots \| \le 1 + \|tA\| + \dfrac{\|tA\|^2}{2!} + \cdots = e^{\|tA\|}$ But I got stuck with the left part. Great thanks for any help or ideas!","Prove that $ e^{-t\|A\|} \le \|e^{tA}\| \le e^{t\|A\|} $ for any matrix $A \in \mathbb{R}^{n \times n}, t > 0$, where $e^{tA} = I + tA + t^2 \dfrac{A^2}{2!} + t^3 \dfrac{A^3}{3!} + \cdots$ It is easy to prove the right part of the inequality $ \|e^{tA}\| \le e^{t\|A\|} $ for $t > 0$: $\|e^{tA}\| = \| I + tA + t^2 \dfrac{A^2}{2!} + t^3 \dfrac{A^3}{3!} + \cdots \| \le 1 + \|tA\| + \dfrac{\|tA\|^2}{2!} + \cdots = e^{\|tA\|}$ But I got stuck with the left part. Great thanks for any help or ideas!",,"['linear-algebra', 'matrices', 'inequality']"
67,How is $f(x)=x+1$ not backwards stable if I consider the error propagated in the addition?,How is  not backwards stable if I consider the error propagated in the addition?,f(x)=x+1,"Many sources claim that $f(x)=x+1$ is not backwards stable.  That is, it does not give an exact solution to a slightly perturbed (or ""nearby"") problem. e.g. https://www.cs.usask.ca/~spiteri/CMPT898/notes/numericalStability.pdf on page 24. Now, when I work this out myself, I think I'm able to show that $\exists\,\,\, \epsilon $ s.t. the computed problem with errors of $\epsilon_1, \epsilon_2,$ and $\epsilon_3 $ is equal to a slightly perturbed problem.  Since there is an error for rounding on each input, and then an error for the addition, the computed solution is $(x(1+\epsilon_1)+1(1+\epsilon_2))(1+\epsilon_3)$, and the exact solution slightly perturbed is $(x+1)(1+\epsilon)$.  So i am showingthese are equal for some epsilon on the order of machine precision: $(x(1+\epsilon_1)+1(1+\epsilon_2))(1+\epsilon_3)=(x+1)(1+\epsilon)$ multiplying these out we get: $x+\epsilon_1x+1+\epsilon_2+\epsilon_2x+\epsilon_3\epsilon_1x+\epsilon_3+\epsilon_3\epsilon_2=x+1+\epsilon x+\epsilon$ subtract $x$ and $1$ from both sides $\epsilon_1x+\epsilon_2+\epsilon_2x+\epsilon_3\epsilon_1x+\epsilon_3+\epsilon_3\epsilon_2=\epsilon x+\epsilon$ Now, it seems clear to me that we cna always find an epsilon on the right hand side that will complete the equation. Most sources cite $x=0$ as the value that breaks this condition, but if I set $x=0$ I am still able to see an $\epsilon$ that mkaes this work. What am i missing here?","Many sources claim that $f(x)=x+1$ is not backwards stable.  That is, it does not give an exact solution to a slightly perturbed (or ""nearby"") problem. e.g. https://www.cs.usask.ca/~spiteri/CMPT898/notes/numericalStability.pdf on page 24. Now, when I work this out myself, I think I'm able to show that $\exists\,\,\, \epsilon $ s.t. the computed problem with errors of $\epsilon_1, \epsilon_2,$ and $\epsilon_3 $ is equal to a slightly perturbed problem.  Since there is an error for rounding on each input, and then an error for the addition, the computed solution is $(x(1+\epsilon_1)+1(1+\epsilon_2))(1+\epsilon_3)$, and the exact solution slightly perturbed is $(x+1)(1+\epsilon)$.  So i am showingthese are equal for some epsilon on the order of machine precision: $(x(1+\epsilon_1)+1(1+\epsilon_2))(1+\epsilon_3)=(x+1)(1+\epsilon)$ multiplying these out we get: $x+\epsilon_1x+1+\epsilon_2+\epsilon_2x+\epsilon_3\epsilon_1x+\epsilon_3+\epsilon_3\epsilon_2=x+1+\epsilon x+\epsilon$ subtract $x$ and $1$ from both sides $\epsilon_1x+\epsilon_2+\epsilon_2x+\epsilon_3\epsilon_1x+\epsilon_3+\epsilon_3\epsilon_2=\epsilon x+\epsilon$ Now, it seems clear to me that we cna always find an epsilon on the right hand side that will complete the equation. Most sources cite $x=0$ as the value that breaks this condition, but if I set $x=0$ I am still able to see an $\epsilon$ that mkaes this work. What am i missing here?",,"['linear-algebra', 'numerical-methods', 'numerical-linear-algebra', 'stability-theory', 'condition-number']"
68,How to switch $y$ and $z$ axis of a rotation matrix,How to switch  and  axis of a rotation matrix,y z,"very simple question how to switch the $y$ and $z$-axis of a rotation matrix. So far I have rotated the rotation matrix $90$ degrees around $x$. The only thing left would be to inverse the new $y$ axis, which is the old $z$ axis. How can I do this? Or is there a way to do the switch in one step? Thanks!","very simple question how to switch the $y$ and $z$-axis of a rotation matrix. So far I have rotated the rotation matrix $90$ degrees around $x$. The only thing left would be to inverse the new $y$ axis, which is the old $z$ axis. How can I do this? Or is there a way to do the switch in one step? Thanks!",,"['linear-algebra', 'geometry', 'rotations']"
69,Are real numbers vectors or scalars?,Are real numbers vectors or scalars?,,"I am trying to understand the difference between scalars and vectors. I know the basic definition that vectors are magnitude with direction. But we also call vectors those entities that belong to vector spaces and can be added. I had previously asked this question ,  a comment to this answer to the question states that real numbers can be both vectors and scalars, how is that even possible ? If so then what is a vector and what is a scalar ? How can they be different but be represented simultaneously by the same mathematical concept ?","I am trying to understand the difference between scalars and vectors. I know the basic definition that vectors are magnitude with direction. But we also call vectors those entities that belong to vector spaces and can be added. I had previously asked this question ,  a comment to this answer to the question states that real numbers can be both vectors and scalars, how is that even possible ? If so then what is a vector and what is a scalar ? How can they be different but be represented simultaneously by the same mathematical concept ?",,"['linear-algebra', 'real-numbers']"
70,Cauchy-Binet formula: general form,Cauchy-Binet formula: general form,,"In Wikipedia, the Cauchy-Binet formula is stated for  determinant of product of matrices $A_{m\times n}$ and $B_{n\times m}$ . However, Handbook of Linear Algebra states the formula (without proof) as A $k\times k$ minor in product $AB$ can be obtained as sum of products of $k\times k$ minors in $A$ and $k\times k$ minors in $B$ . More precisely let $\alpha,\beta,\gamma$ denotes $k$ -tuples of increasing sequences of positive integers (within maximum sizes of matrices $A,B$ ). Let $A[\alpha,\beta]$ denote the $k\times k$ minor of $A$ whose $k$ rows are w.r.t. indices in $\alpha$ and $k$ columns are w.r.t. indices in $\beta$ . Then $$(AB)[\alpha,\beta] = \sum_{\gamma} A[\alpha,\gamma]B[\gamma,\beta],$$ where sum is over all $k$ -tuples $\gamma$ of increasing sequence of positive integers (within maximum sizes of matrices $A,B$ ). Q.1 Where can I find a proof of this generalized Cauchy-Binet formula? In many books of matrices, I didn't find its proof (except proof of determinant expansion of product $AB$ - the largest minor when sizes of $A,B$ are dual i.e. if $A$ has size $m\times n$ , then $B$ has size $n\times m$ .) Q.2 Can one state some reference or book which contains interesting applications of this formula? (Let me know if there is anything missing in above expansion formula; you may edit as well for correction.)","In Wikipedia, the Cauchy-Binet formula is stated for  determinant of product of matrices and . However, Handbook of Linear Algebra states the formula (without proof) as A minor in product can be obtained as sum of products of minors in and minors in . More precisely let denotes -tuples of increasing sequences of positive integers (within maximum sizes of matrices ). Let denote the minor of whose rows are w.r.t. indices in and columns are w.r.t. indices in . Then where sum is over all -tuples of increasing sequence of positive integers (within maximum sizes of matrices ). Q.1 Where can I find a proof of this generalized Cauchy-Binet formula? In many books of matrices, I didn't find its proof (except proof of determinant expansion of product - the largest minor when sizes of are dual i.e. if has size , then has size .) Q.2 Can one state some reference or book which contains interesting applications of this formula? (Let me know if there is anything missing in above expansion formula; you may edit as well for correction.)","A_{m\times n} B_{n\times m} k\times k AB k\times k A k\times k B \alpha,\beta,\gamma k A,B A[\alpha,\beta] k\times k A k \alpha k \beta (AB)[\alpha,\beta] = \sum_{\gamma} A[\alpha,\gamma]B[\gamma,\beta], k \gamma A,B AB A,B A m\times n B n\times m","['linear-algebra', 'matrices', 'determinant']"
71,"Show that if $a\neq b$, then we have for the $n\times n$-matrix $\textrm{det}=\frac{a^{n+1}-b^{n+1}}{a-b}$.","Show that if , then we have for the -matrix .",a\neq b n\times n \textrm{det}=\frac{a^{n+1}-b^{n+1}}{a-b},"The Problem Show that if $a\neq b$, then we have for the $n\times n$-matrix $$\textrm{det}\begin{pmatrix} a+b & ab & 0 & \ldots & 0 & 0 \\ 1 & a+b & ab & \ldots & 0 & 0 \\ 0 & 1 & a+b & \ldots & 0 & 0 \\ \ldots & \ldots & \ldots & \ldots & \ldots & \ldots \\ 0 & 0 & 0 & \ldots & a+b & ab \\ 0 & 0 & 0 & \ldots & 1 & a+b\end{pmatrix} =\frac{a^{n+1}-b^{n+1}}{a-b}$$ What if $a=b$? My Questions I am not entirely sure how to begin this proof.  I suspect I am missing something that makes it quite simple.  I tried looking for similar problems, and I noticed a common theme being the use of row operations to rewrite the matrix, from which the determinant was found.  However, I still didn't understand many of the intermediate computations when it came to actually finding the determinant.  My questions are as follows. Should I use row operations to rewrite this matrix?  If so, what would be an example of how that would look computationally? In either case, how should I go about actually computing the determinant to show the statement is true?  Is there an algorithm that is helpful here? I noticed for the follow up question that, if $a=b$, then $$\textrm{det}\begin{pmatrix} a+a & aa & 0 & \ldots & 0 & 0 \\ 1 & a+a & aa & \ldots & 0 & 0 \\ 0 & 1 & a+a & \ldots & 0 & 0 \\ \ldots & \ldots & \ldots & \ldots & \ldots & \ldots \\ 0 & 0 & 0 & \ldots & a+a & aa \\ 0 & 0 & 0 & \ldots & 1 & a+a\end{pmatrix}.$$  Am I right in my thinking there? Other Details The book used in the course is Abstract Linear Algebra by Curtis.  It has been of little help to me here...","The Problem Show that if $a\neq b$, then we have for the $n\times n$-matrix $$\textrm{det}\begin{pmatrix} a+b & ab & 0 & \ldots & 0 & 0 \\ 1 & a+b & ab & \ldots & 0 & 0 \\ 0 & 1 & a+b & \ldots & 0 & 0 \\ \ldots & \ldots & \ldots & \ldots & \ldots & \ldots \\ 0 & 0 & 0 & \ldots & a+b & ab \\ 0 & 0 & 0 & \ldots & 1 & a+b\end{pmatrix} =\frac{a^{n+1}-b^{n+1}}{a-b}$$ What if $a=b$? My Questions I am not entirely sure how to begin this proof.  I suspect I am missing something that makes it quite simple.  I tried looking for similar problems, and I noticed a common theme being the use of row operations to rewrite the matrix, from which the determinant was found.  However, I still didn't understand many of the intermediate computations when it came to actually finding the determinant.  My questions are as follows. Should I use row operations to rewrite this matrix?  If so, what would be an example of how that would look computationally? In either case, how should I go about actually computing the determinant to show the statement is true?  Is there an algorithm that is helpful here? I noticed for the follow up question that, if $a=b$, then $$\textrm{det}\begin{pmatrix} a+a & aa & 0 & \ldots & 0 & 0 \\ 1 & a+a & aa & \ldots & 0 & 0 \\ 0 & 1 & a+a & \ldots & 0 & 0 \\ \ldots & \ldots & \ldots & \ldots & \ldots & \ldots \\ 0 & 0 & 0 & \ldots & a+a & aa \\ 0 & 0 & 0 & \ldots & 1 & a+a\end{pmatrix}.$$  Am I right in my thinking there? Other Details The book used in the course is Abstract Linear Algebra by Curtis.  It has been of little help to me here...",,"['linear-algebra', 'determinant']"
72,Definition of the characteristic polynomial,Definition of the characteristic polynomial,,"My textbook defines the characteristic polynomial of an $n \times n$-matrix $A$ as: $$p_A = \text{det}(A-XI_n) = \sum_{\sigma \in S_n} \text{sign } \sigma. (a_{1\sigma(1)}-X\delta_{1\sigma(1)})...(a_{n\sigma(n)}-X\delta_{n\sigma(n)})$$ (where $S_n$ is the symmetric group). Then it just states that by reordering according to the powers of $X$, we get the form $$p_A = a_0+a_1X+...+a_{n-1}X^{n-1}+a_nX^n$$ with coefficients $a_i$ from a field, where $a_0=\text{det } A$, $a_{n-1}=(-1)^{n-1}. \text{tr}(A)$ and $a_n=(-1)^n$. Could someone clarify how to derive this formula from the definition?","My textbook defines the characteristic polynomial of an $n \times n$-matrix $A$ as: $$p_A = \text{det}(A-XI_n) = \sum_{\sigma \in S_n} \text{sign } \sigma. (a_{1\sigma(1)}-X\delta_{1\sigma(1)})...(a_{n\sigma(n)}-X\delta_{n\sigma(n)})$$ (where $S_n$ is the symmetric group). Then it just states that by reordering according to the powers of $X$, we get the form $$p_A = a_0+a_1X+...+a_{n-1}X^{n-1}+a_nX^n$$ with coefficients $a_i$ from a field, where $a_0=\text{det } A$, $a_{n-1}=(-1)^{n-1}. \text{tr}(A)$ and $a_n=(-1)^n$. Could someone clarify how to derive this formula from the definition?",,['linear-algebra']
73,What is the process of combining vectors into a matrix called?,What is the process of combining vectors into a matrix called?,,"What is the process of combining vectors into a matrix called? My question is not how or why to do this.  My question is, ""What is this process called?"" For example, if we need a solution for a system of linear equations, one method is to represent the unknowns as column vectors and then join those column vectors into a single matrix.  One way to describe it might be to say ""indexing"" an array but I'm looking for a formal terminology.  Ultimately, I'd like to know how to represent this in a generalized form. Essentially, I'm asking how to describe and formally write something like this: $$ \begin{equation} \vec v_1=\left(\begin{array}{c} X_1 \\ Y_1 \\ Z_1 \end{array}\right), \vec v_2=\left(\begin{array}{c} X_2 \\ Y_2 \\ Z_2 \end{array}\right) \rightarrow M=\left(\begin{array}{cc} X_1 & X_2 \\ Y_1 & Y_2 \\ Z_1 & Z_2 \end{array}\right) \end{equation} $$ Thanks.","What is the process of combining vectors into a matrix called? My question is not how or why to do this.  My question is, ""What is this process called?"" For example, if we need a solution for a system of linear equations, one method is to represent the unknowns as column vectors and then join those column vectors into a single matrix.  One way to describe it might be to say ""indexing"" an array but I'm looking for a formal terminology.  Ultimately, I'd like to know how to represent this in a generalized form. Essentially, I'm asking how to describe and formally write something like this: $$ \begin{equation} \vec v_1=\left(\begin{array}{c} X_1 \\ Y_1 \\ Z_1 \end{array}\right), \vec v_2=\left(\begin{array}{c} X_2 \\ Y_2 \\ Z_2 \end{array}\right) \rightarrow M=\left(\begin{array}{cc} X_1 & X_2 \\ Y_1 & Y_2 \\ Z_1 & Z_2 \end{array}\right) \end{equation} $$ Thanks.",,"['linear-algebra', 'matrices', 'vectors', 'terminology']"
74,Which symbol can be used to refer to identity matrix?,Which symbol can be used to refer to identity matrix?,,$I$ is commenly used as a notation of identity matrix. I am wondering is there any notation else for identity matrix?,$I$ is commenly used as a notation of identity matrix. I am wondering is there any notation else for identity matrix?,,['linear-algebra']
75,Chameleons of Three Colors puzzle,Chameleons of Three Colors puzzle,,"I am trying to understand the solution to the below puzzle There are 13 Red, 15 Green, and 17 Blue Chameleons at some point of   time. Whenever two Chameleons of the different colors meet both of   them change their color to the third color. Is it ever possible for   all Chameleons to become of the same color? I am able to figure out ""manually step by step"" that it is not possible to get all in one color. However I don't understand the more formal solution using the modulo, for example Solution 1 here . I realize that after every change the differennce between the second and first terms is -3, 0 or 3. What are the vectors (2,2,2) representing? I don't understand the final statement there: We see that, with the initial vector (1, 0, 2) modulo 3, the only possible combinations of colors on the island are (1, 0, 2), (0, 2, 1), and (2, 1, 0), and these occur cyclically with period 3. Can anybody explain in simple way this solution to the problem?","I am trying to understand the solution to the below puzzle There are 13 Red, 15 Green, and 17 Blue Chameleons at some point of   time. Whenever two Chameleons of the different colors meet both of   them change their color to the third color. Is it ever possible for   all Chameleons to become of the same color? I am able to figure out ""manually step by step"" that it is not possible to get all in one color. However I don't understand the more formal solution using the modulo, for example Solution 1 here . I realize that after every change the differennce between the second and first terms is -3, 0 or 3. What are the vectors (2,2,2) representing? I don't understand the final statement there: We see that, with the initial vector (1, 0, 2) modulo 3, the only possible combinations of colors on the island are (1, 0, 2), (0, 2, 1), and (2, 1, 0), and these occur cyclically with period 3. Can anybody explain in simple way this solution to the problem?",,"['linear-algebra', 'modular-arithmetic', 'puzzle', 'proof-explanation']"
76,A linear map $\varphi$ such that $\varphi (GL_n(\mathbb C) )\subseteq GL_n(\mathbb C)$ preserves the rank,A linear map  such that  preserves the rank,\varphi \varphi (GL_n(\mathbb C) )\subseteq GL_n(\mathbb C),"Suppose that $\varphi$ is a linear map from $M_n(\mathbb C)$ into $M_n(\mathbb C)$  such that $\varphi (GL_n(\mathbb C) )\subseteq GL_n(\mathbb C)$, where $M_n(\mathbb C)$ is the vector space of $n \times n$ complex matrices and $GL_n(\mathbb C)$ the general linear group of $M_n(\mathbb C)$. How to prove that $\varphi$ preserves the rank? I.e. that for all $A \in M_n(\mathbb C)$ you have $\text{rank } \varphi(A) = \text{rank } A$. Note: I have a proof based on several questions from a French selective exam (see below), and I was looking for a more straight forward proof... if possible.","Suppose that $\varphi$ is a linear map from $M_n(\mathbb C)$ into $M_n(\mathbb C)$  such that $\varphi (GL_n(\mathbb C) )\subseteq GL_n(\mathbb C)$, where $M_n(\mathbb C)$ is the vector space of $n \times n$ complex matrices and $GL_n(\mathbb C)$ the general linear group of $M_n(\mathbb C)$. How to prove that $\varphi$ preserves the rank? I.e. that for all $A \in M_n(\mathbb C)$ you have $\text{rank } \varphi(A) = \text{rank } A$. Note: I have a proof based on several questions from a French selective exam (see below), and I was looking for a more straight forward proof... if possible.",,"['linear-algebra', 'matrices', 'matrix-rank']"
77,Prove that vectors in Euclidean space are linearly independent,Prove that vectors in Euclidean space are linearly independent,,"Could you help me with an idea of solving the following problem? I think that proof involves the positive definiteness of Gram matrix, but I don't know how. Consider a system of vectors $e_1, e_2, ..., e_n, e_{n+1}$ in some Euclidean space such that dot product $(e_i, e_j) <0$ for all $i \neq j.$ Prove that any $n$ vectors of the system are linearly independent.","Could you help me with an idea of solving the following problem? I think that proof involves the positive definiteness of Gram matrix, but I don't know how. Consider a system of vectors $e_1, e_2, ..., e_n, e_{n+1}$ in some Euclidean space such that dot product $(e_i, e_j) <0$ for all $i \neq j.$ Prove that any $n$ vectors of the system are linearly independent.",,['linear-algebra']
78,"If $\operatorname{rank}(A)$ = $\operatorname{rank}(A^2)$, show that nullspace of $A$ = nullspace of $A^2$","If  = , show that nullspace of  = nullspace of",\operatorname{rank}(A) \operatorname{rank}(A^2) A A^2,"Let $A$ be a square matrix. If $\operatorname{rank}(A)$ = $\operatorname{rank}(A^2)$ Prove that nullspace of $A$ = nullspace of $A^2$ The first thing I notice is that this $\implies$ $\operatorname{nullity}(A)=\operatorname{nullity}(A^2)$ Then I am kinda stuck, any hints?","Let $A$ be a square matrix. If $\operatorname{rank}(A)$ = $\operatorname{rank}(A^2)$ Prove that nullspace of $A$ = nullspace of $A^2$ The first thing I notice is that this $\implies$ $\operatorname{nullity}(A)=\operatorname{nullity}(A^2)$ Then I am kinda stuck, any hints?",,"['linear-algebra', 'matrix-rank']"
79,Formal Power Series as Linear Operators,Formal Power Series as Linear Operators,,"Let $t^k$ act as the $k$-th derivative operator on the set of polynomials.  So $$t^k(x^n)=t^k x^n=(n)_kx^{n-k}$$ where $(n)_k=n(n-1)(n-2)...(n-k+1)$ is the falling factorial.  Then with a formal power series, $f(t)=\sum_{k\ge 0}a_k\frac{t^k}{k!}$, the linear operator $f(t)$ acts as such that $$f(t)(x^n)=f(t)x^n=\sum_{k=0}^n\binom{n}{k}a_k x^{n-k}$$ Therefore, depending on the coefficients of the power series, we can get some interesting binomial identites.  For example, if $f(t)=e^{yt}$, since the coefficients $a_n=y^n$, we get $$e^{yt}x^n=\sum_{k=0}^n\binom{n}{k}y^k x^{n-k}=(x+y)^n$$ by linearity, $$(e^{yt}-1)x^n=(x+y)^n-x^n=\sum_{k=1}^{n}\binom{n}{k}y^k x^{n-k}$$ and perhaps not as obvious $$\left(\frac{e^{yt}-1}{t}\right)x^n=\int_{x}^{x+y}u^ndu$$ Now suppose that $f(t)=e^{yt}-1-yt$.  Then $$(e^{yt}-1-yt)x^n=(x+y)^n-x^n-ynx^{n-1}=\sum_{k=2}^{n}\binom{n}{k}y^k x^{n-k}$$ Obviously there is a nice formed forward difference equation in the previous case that is not happening here.  But there is a relationship with subtracted terms of the binomial expansion.  What i would really like help understanding is whether or not a possible analogous integral representation exists for the following operator: $$\left(\frac{e^{yt}-1-yt}{t^2}\right)x^n=\left(\sum_{k=0}^\infty\frac{y^{k+2}}{(k+2)(k+1)}\frac{t^k}{k!}\right)x^n=\sum_{k=0}^n\binom{n}{k}\frac{y^{k+2}}{(k+1)(k+2)}x^{n-k}$$ $$=\sum_{k=0}^n\binom{n+2}{k+2}\frac{y^{k+2}}{(n+1)(n+2)}x^{n-k}=\frac{1}{(n+1)(n+2)}\sum_{k=2}^{n+2}\binom{n+2}{k}y^kx^{n+2-k}$$ It is not as simple.  Clearly $\frac{d^2}{dx^2}\frac{x^{n+2}}{((n+2)(n+1)}$.  If I integrated below I think the math is correct $$\int_x^{x+y}{\frac{u^{n+1}}{n+1}}du=\frac{1}{(n+1)(n+2)}\sum_{k=1}^{n+2}\binom{n+2}{k}y^kx^{n+2-k}$$ Which is really close, but the lower bound on the summation is $1$, not $2$.  Does any one have any insight in how i can fix this, if possible?","Let $t^k$ act as the $k$-th derivative operator on the set of polynomials.  So $$t^k(x^n)=t^k x^n=(n)_kx^{n-k}$$ where $(n)_k=n(n-1)(n-2)...(n-k+1)$ is the falling factorial.  Then with a formal power series, $f(t)=\sum_{k\ge 0}a_k\frac{t^k}{k!}$, the linear operator $f(t)$ acts as such that $$f(t)(x^n)=f(t)x^n=\sum_{k=0}^n\binom{n}{k}a_k x^{n-k}$$ Therefore, depending on the coefficients of the power series, we can get some interesting binomial identites.  For example, if $f(t)=e^{yt}$, since the coefficients $a_n=y^n$, we get $$e^{yt}x^n=\sum_{k=0}^n\binom{n}{k}y^k x^{n-k}=(x+y)^n$$ by linearity, $$(e^{yt}-1)x^n=(x+y)^n-x^n=\sum_{k=1}^{n}\binom{n}{k}y^k x^{n-k}$$ and perhaps not as obvious $$\left(\frac{e^{yt}-1}{t}\right)x^n=\int_{x}^{x+y}u^ndu$$ Now suppose that $f(t)=e^{yt}-1-yt$.  Then $$(e^{yt}-1-yt)x^n=(x+y)^n-x^n-ynx^{n-1}=\sum_{k=2}^{n}\binom{n}{k}y^k x^{n-k}$$ Obviously there is a nice formed forward difference equation in the previous case that is not happening here.  But there is a relationship with subtracted terms of the binomial expansion.  What i would really like help understanding is whether or not a possible analogous integral representation exists for the following operator: $$\left(\frac{e^{yt}-1-yt}{t^2}\right)x^n=\left(\sum_{k=0}^\infty\frac{y^{k+2}}{(k+2)(k+1)}\frac{t^k}{k!}\right)x^n=\sum_{k=0}^n\binom{n}{k}\frac{y^{k+2}}{(k+1)(k+2)}x^{n-k}$$ $$=\sum_{k=0}^n\binom{n+2}{k+2}\frac{y^{k+2}}{(n+1)(n+2)}x^{n-k}=\frac{1}{(n+1)(n+2)}\sum_{k=2}^{n+2}\binom{n+2}{k}y^kx^{n+2-k}$$ It is not as simple.  Clearly $\frac{d^2}{dx^2}\frac{x^{n+2}}{((n+2)(n+1)}$.  If I integrated below I think the math is correct $$\int_x^{x+y}{\frac{u^{n+1}}{n+1}}du=\frac{1}{(n+1)(n+2)}\sum_{k=1}^{n+2}\binom{n+2}{k}y^kx^{n+2-k}$$ Which is really close, but the lower bound on the summation is $1$, not $2$.  Does any one have any insight in how i can fix this, if possible?",,"['linear-algebra', 'integration', 'power-series', 'formal-power-series']"
80,"For vector p-norm, can we prove it is decreasing without using derivative?","For vector p-norm, can we prove it is decreasing without using derivative?",,"For vector p-norm defined as $(∑_{i=1}^n x_i^p )^{\frac{1}{p}}$ for any $p\ge 1$ and vector ${\bf{x}}=\{x_1,...,x_n\}$. The following proves it is decreasing with respect to $p$ by taking derivative (you don't need to read the whole proof, just have a look), However, I am thinking if there is another approach without using the derivative. Is there any proof for monotonicity of p-norm without using derivatives? The upper bound can be proved by Holder's inequality by Relations between p norms We have plenty of inequalities that lead to the definition of p-norm: Young's inequality, Jensen's inequality, Holder's inequality, Minkowski’s inequality. Maybe there is a proof using those inequalities?","For vector p-norm defined as $(∑_{i=1}^n x_i^p )^{\frac{1}{p}}$ for any $p\ge 1$ and vector ${\bf{x}}=\{x_1,...,x_n\}$. The following proves it is decreasing with respect to $p$ by taking derivative (you don't need to read the whole proof, just have a look), However, I am thinking if there is another approach without using the derivative. Is there any proof for monotonicity of p-norm without using derivatives? The upper bound can be proved by Holder's inequality by Relations between p norms We have plenty of inequalities that lead to the definition of p-norm: Young's inequality, Jensen's inequality, Holder's inequality, Minkowski’s inequality. Maybe there is a proof using those inequalities?",,"['linear-algebra', 'normed-spaces']"
81,A curious determinantal identity,A curious determinantal identity,,"A real $n\times n$ matrix $A$ can be uniquely decomposed into the sum of a symmetric matrix $B$ and a skew-symmetric matrix $C$ by setting $B=\frac{A+A^T}{2}$ and $C=\frac{A-A^T}{2}$. When $n=2$ there is an interesting result for the determinants of these matrices: if $A=\begin{bmatrix}a&b\\c&d\end{bmatrix}$, then $B=\begin{bmatrix}a & \frac{b+c}{2} \\ \frac{b+c}{2} & d\end{bmatrix}$ and $C=\begin{bmatrix}0&\frac{b-c}{2} \\ -\frac{b-c}{2} & 0\end{bmatrix}$, hence $$ \det(B)+\det(C)=ad-\frac{(b+c)^2}{4}+\frac{(b-c)^2}{4}=ad-bc=\det(A) $$ By testing a few examples, I've come to the conclusion that this identity does not hold for $n=3$ or $n=4$. My question, therefore, is whether the equality $\det(A)=\det(B)+\det(C)$ for $2\times 2$ matrices is simply a coincidence, or whether there is something interesting going on.","A real $n\times n$ matrix $A$ can be uniquely decomposed into the sum of a symmetric matrix $B$ and a skew-symmetric matrix $C$ by setting $B=\frac{A+A^T}{2}$ and $C=\frac{A-A^T}{2}$. When $n=2$ there is an interesting result for the determinants of these matrices: if $A=\begin{bmatrix}a&b\\c&d\end{bmatrix}$, then $B=\begin{bmatrix}a & \frac{b+c}{2} \\ \frac{b+c}{2} & d\end{bmatrix}$ and $C=\begin{bmatrix}0&\frac{b-c}{2} \\ -\frac{b-c}{2} & 0\end{bmatrix}$, hence $$ \det(B)+\det(C)=ad-\frac{(b+c)^2}{4}+\frac{(b-c)^2}{4}=ad-bc=\det(A) $$ By testing a few examples, I've come to the conclusion that this identity does not hold for $n=3$ or $n=4$. My question, therefore, is whether the equality $\det(A)=\det(B)+\det(C)$ for $2\times 2$ matrices is simply a coincidence, or whether there is something interesting going on.",,"['linear-algebra', 'matrices', 'determinant']"
82,Dimension of Range and Null Space of Composition of Two Linear Maps,Dimension of Range and Null Space of Composition of Two Linear Maps,,"Question Suppose $U$, $V$ and $W$ are finite-dimensional vector spaces. Let $\mathcal{L}(U,V)$ and $\mathcal{L}(V,W)$ be the vector spaces of all linear maps from $U$ into $V$ and from $V$ into $W$, respectively. Suppose $S \in \mathcal{L}(V,W)$ and $T \in \mathcal{L}(U,V)$. Then prove that $\begin{align} & 1. \, \text{dim null} S \circ T \le \text{dim null} S + \text{dim null} T \\ & 2. \, \text{dim range} S \circ T \le \text{min} \{ \text{dim range} S , \text{dim range} T \} \end{align}$ My Thought To prove the first one, I guess that writing the fundamental theorem of linear maps for $S \circ T$ may be a good start $$\begin{align} \text{dim null} S \circ T &= \text{dim} U - \text{dim range} S \circ T \\ &= \text{dim null} T + \text{dim range} T - \text{dim range} S \circ T \\ &\le \text{dim null} T + \text{dim} V - \text{dim range} S \circ T \\ &= \text{dim null} T + \text{dim null} S + \text{dim range} S- \text{dim range} S \circ T \end{align}$$ So if I can prove that $$\text{dim range} S- \text{dim range} S \circ T \le 0$$ then I am done but this does not seem to be true because it is easy to see that $\text{range} S \circ T \subseteq  \text{range} S$ and hence  $\text{dim range} S \circ T \le \text{dim range} S$ . So I am stuck!  Also, I could observe that $\text{null} T \subseteq \text{null} S \circ T$ and hence $\text{dim null} T  \le \text{dim null} S \circ T$ but I don't know how to use this!","Question Suppose $U$, $V$ and $W$ are finite-dimensional vector spaces. Let $\mathcal{L}(U,V)$ and $\mathcal{L}(V,W)$ be the vector spaces of all linear maps from $U$ into $V$ and from $V$ into $W$, respectively. Suppose $S \in \mathcal{L}(V,W)$ and $T \in \mathcal{L}(U,V)$. Then prove that $\begin{align} & 1. \, \text{dim null} S \circ T \le \text{dim null} S + \text{dim null} T \\ & 2. \, \text{dim range} S \circ T \le \text{min} \{ \text{dim range} S , \text{dim range} T \} \end{align}$ My Thought To prove the first one, I guess that writing the fundamental theorem of linear maps for $S \circ T$ may be a good start $$\begin{align} \text{dim null} S \circ T &= \text{dim} U - \text{dim range} S \circ T \\ &= \text{dim null} T + \text{dim range} T - \text{dim range} S \circ T \\ &\le \text{dim null} T + \text{dim} V - \text{dim range} S \circ T \\ &= \text{dim null} T + \text{dim null} S + \text{dim range} S- \text{dim range} S \circ T \end{align}$$ So if I can prove that $$\text{dim range} S- \text{dim range} S \circ T \le 0$$ then I am done but this does not seem to be true because it is easy to see that $\text{range} S \circ T \subseteq  \text{range} S$ and hence  $\text{dim range} S \circ T \le \text{dim range} S$ . So I am stuck!  Also, I could observe that $\text{null} T \subseteq \text{null} S \circ T$ and hence $\text{dim null} T  \le \text{dim null} S \circ T$ but I don't know how to use this!",,['linear-algebra']
83,Doubt in the proof of Cauchy-Binet formula.,Doubt in the proof of Cauchy-Binet formula.,,I have a doubt in the proof of the Cauchy-Binet formula: Question: In the colored portion. I dont know how the sum multiplying $det(A(J'))$ become $$\sum_\sigma sgn(\sigma) b_{j'_{\tau(1)}1} b_{j'_{\tau(2)}2}\ldots  b_{j'_{\tau(k)}k}$$ Help me in that step. Thanks in advance.,I have a doubt in the proof of the Cauchy-Binet formula: Question: In the colored portion. I dont know how the sum multiplying become Help me in that step. Thanks in advance.,det(A(J')) \sum_\sigma sgn(\sigma) b_{j'_{\tau(1)}1} b_{j'_{\tau(2)}2}\ldots  b_{j'_{\tau(k)}k},"['linear-algebra', 'matrices', 'determinant']"
84,If $A^3=A^2$ then $A^2$ is diagonalizable.,If  then  is diagonalizable.,A^3=A^2 A^2,"Let $A\in \mathbb{k}^{n\times n} $ . Prove that if $A^3=A^2$ then $A^2$ is diagonalizable. Could you give me any hints on how to prove it?. I can't use the minimal polynomial, since we haven't seen it in class. Thanks.","Let . Prove that if then is diagonalizable. Could you give me any hints on how to prove it?. I can't use the minimal polynomial, since we haven't seen it in class. Thanks.",A\in \mathbb{k}^{n\times n}  A^3=A^2 A^2,"['linear-algebra', 'diagonalization']"
85,Matrix of a conformal linear map,Matrix of a conformal linear map,,Could you please explain why every conformal linear map is a scalar times a rotation matrix? I can prove that every scalar-rotation matrix is a conformal map but not the opposite.,Could you please explain why every conformal linear map is a scalar times a rotation matrix? I can prove that every scalar-rotation matrix is a conformal map but not the opposite.,,['linear-algebra']
86,Understanding a proof of RREF uniqueness,Understanding a proof of RREF uniqueness,,"Base Case $(n = 1)$: Suppose $A$ has only one column. If $A$ is the all zero matrix, it is row equivalent only to itself and is in reduced row echelon form. Every nonzero matrix with one column has a nonzero entry, and all such matrices have reduced row echelon form the column vector $(1, 0,\ldots, 0)$ and no other row echelon form. Induction Step: Suppose now that $n > 1$, that the result holds for all $m \times n$ matrices and let $A \in M_{m,n+1}$. For any $M \in M_{m,n+1}$, we let $M' \in M_{m, n}$ be obtained by removing the last column of $M$. Let $B, C$ be rref of $A$. Then $B', C'$ are in rref and row-equivalent to $A'$. By induction $B' = C'.$ In other words $B, C$ are equal except possibly in the last column. Suppose $B \neq C: \exists i (1 \le i \le m)$ s.t. $b_{i,n+1} \neq c_{i,n+1}$. Let $x = (x_1,\ldots, x_{n + 1})$ be a vector with $Bx = 0$. Then $Cx = 0$ implying $(B - C)x = 0.$ Since $B - C$ is zero except in its last column, performing the multiplication of the $i$th row of $B - C$ by $x$ is $(b_{i, n+1} - c_{i, n+ 1})x_{n + 1} = 0$. Since $b_{i, n+1} \neq c_{i, n+ 1}$, we have that $x_{n + 1} = 0.$ Thus $x_{n + 1}$ is not a free variable for either $B$ or $C$, so in each of these matrices the last column must contain a leading entry of $1$ and have all the other entries $0$ . In both $B, C$ the $1$ must lie in the first zero row of $B'$ and $C'.$ Thus $B = C.$ I am having difficulty with the part in bold. Suppose the last column is all $0$'s. Then $x_{n + 1}$ can be anything $\neq 0$ which is not allowed. Is that why the last columns of $B,C$ must contain a leading entry of $1$ and have all the other entries $0$? What about rref matrices whose last columns are made up of constants that are neither $0$, nor $1$ like the one below? $         \begin{matrix}         1 & 0 & 0 & -5 \\         0 & 1 & 0 & 6\\         0 & 0 & 1 & 3\\         \end{matrix} $ Please, explain what the quoted sentence says.","Base Case $(n = 1)$: Suppose $A$ has only one column. If $A$ is the all zero matrix, it is row equivalent only to itself and is in reduced row echelon form. Every nonzero matrix with one column has a nonzero entry, and all such matrices have reduced row echelon form the column vector $(1, 0,\ldots, 0)$ and no other row echelon form. Induction Step: Suppose now that $n > 1$, that the result holds for all $m \times n$ matrices and let $A \in M_{m,n+1}$. For any $M \in M_{m,n+1}$, we let $M' \in M_{m, n}$ be obtained by removing the last column of $M$. Let $B, C$ be rref of $A$. Then $B', C'$ are in rref and row-equivalent to $A'$. By induction $B' = C'.$ In other words $B, C$ are equal except possibly in the last column. Suppose $B \neq C: \exists i (1 \le i \le m)$ s.t. $b_{i,n+1} \neq c_{i,n+1}$. Let $x = (x_1,\ldots, x_{n + 1})$ be a vector with $Bx = 0$. Then $Cx = 0$ implying $(B - C)x = 0.$ Since $B - C$ is zero except in its last column, performing the multiplication of the $i$th row of $B - C$ by $x$ is $(b_{i, n+1} - c_{i, n+ 1})x_{n + 1} = 0$. Since $b_{i, n+1} \neq c_{i, n+ 1}$, we have that $x_{n + 1} = 0.$ Thus $x_{n + 1}$ is not a free variable for either $B$ or $C$, so in each of these matrices the last column must contain a leading entry of $1$ and have all the other entries $0$ . In both $B, C$ the $1$ must lie in the first zero row of $B'$ and $C'.$ Thus $B = C.$ I am having difficulty with the part in bold. Suppose the last column is all $0$'s. Then $x_{n + 1}$ can be anything $\neq 0$ which is not allowed. Is that why the last columns of $B,C$ must contain a leading entry of $1$ and have all the other entries $0$? What about rref matrices whose last columns are made up of constants that are neither $0$, nor $1$ like the one below? $         \begin{matrix}         1 & 0 & 0 & -5 \\         0 & 1 & 0 & 6\\         0 & 0 & 1 & 3\\         \end{matrix} $ Please, explain what the quoted sentence says.",,['linear-algebra']
87,"What is an example of a SVM kernel, where one implicitly uses an infinity-dimensional space?","What is an example of a SVM kernel, where one implicitly uses an infinity-dimensional space?",,"Reading the Wikipedia article about SVMs , I noticed More formally, a support vector machine constructs a hyperplane or set of hyperplanes in a high- or infinite-dimensional space, which can be used for classification, regression, or other tasks. I continued with ""A Tutorial on Support Vector Machines for Pattern Recognition"" by Christopher JC Burges and stumbled over the following (please not that $x \cdot y$ is the dot product): Now suppose we first mapped the data to   some other (possibly infinite dimensional) Euclidean space $\mathcal{H}$, using a mapping which we   will call $\Phi$:   $$\Phi : \mathbb{R}^d \rightarrow \mathcal{H}$$   Then of course the training algorithm would only depend on the data through dot products   in $\mathcal{H}$, i.e. on functions of the form $\Phi(\mathbf{x}_i)\cdot \Phi(\mathbf{x}_j)$. Now if there were a “kernel function” $K$   such that $K(\mathbf{x}_i, \mathbf{x}_j) = \Phi(\mathbf{x}_i)\cdot\Phi(\mathbf{x}_j)$, we would only need to use $K$ in the training algorithm,   and would never need to explicitly even know what $\Phi$ is. One example is   $$K(\mathbf{x}_i, \mathbf{x}_j ) = e^{- \| \mathbf{x}_i - \mathbf{x}_j\|^2 / 2 \sigma^2}$$   In this particular example, $\mathcal{H}$ is infinite dimensional, so it would not be very easy to work   with $\Phi$ explicitly. I have three questions which are closely related to this. I am happy with any answer which answers any of my questions: Why would $\mathcal{H}$ be infinitely dimensional in this case? What is $\Phi$ in this case? In other sources I read that the Kernel function has to be positive definite. Why?","Reading the Wikipedia article about SVMs , I noticed More formally, a support vector machine constructs a hyperplane or set of hyperplanes in a high- or infinite-dimensional space, which can be used for classification, regression, or other tasks. I continued with ""A Tutorial on Support Vector Machines for Pattern Recognition"" by Christopher JC Burges and stumbled over the following (please not that $x \cdot y$ is the dot product): Now suppose we first mapped the data to   some other (possibly infinite dimensional) Euclidean space $\mathcal{H}$, using a mapping which we   will call $\Phi$:   $$\Phi : \mathbb{R}^d \rightarrow \mathcal{H}$$   Then of course the training algorithm would only depend on the data through dot products   in $\mathcal{H}$, i.e. on functions of the form $\Phi(\mathbf{x}_i)\cdot \Phi(\mathbf{x}_j)$. Now if there were a “kernel function” $K$   such that $K(\mathbf{x}_i, \mathbf{x}_j) = \Phi(\mathbf{x}_i)\cdot\Phi(\mathbf{x}_j)$, we would only need to use $K$ in the training algorithm,   and would never need to explicitly even know what $\Phi$ is. One example is   $$K(\mathbf{x}_i, \mathbf{x}_j ) = e^{- \| \mathbf{x}_i - \mathbf{x}_j\|^2 / 2 \sigma^2}$$   In this particular example, $\mathcal{H}$ is infinite dimensional, so it would not be very easy to work   with $\Phi$ explicitly. I have three questions which are closely related to this. I am happy with any answer which answers any of my questions: Why would $\mathcal{H}$ be infinitely dimensional in this case? What is $\Phi$ in this case? In other sources I read that the Kernel function has to be positive definite. Why?",,"['linear-algebra', 'machine-learning']"
88,$\operatorname{spectrum}(AB) = \operatorname{spectrum}(BA)$? [duplicate],? [duplicate],\operatorname{spectrum}(AB) = \operatorname{spectrum}(BA),"This question already has answers here : Relation between AB and BA (4 answers) Closed 8 years ago . Suppose we have two $n \times n$ matrices $A, B$. It seems like $\delta(AB)=\delta(BA)$, but I can't generally poove it. If $\det(A) \neq 0$ then $\det(AB - \lambda I) = 0 \Leftrightarrow $ $ \det(AB - \lambda I) \cdot \det(A) = 0 \Leftrightarrow \det(ABA - \lambda A) = 0 \Leftrightarrow $ $ \det(A) \cdot \det(BA - \lambda I) = 0 \Leftrightarrow \det(BA - \lambda I) = 0$, so $$\delta(AB)=\delta(BA)$$ Same if $\det(B) \neq 0$. But how to prove it for $\det(A) = \det(B) = 0$? Is it still true?","This question already has answers here : Relation between AB and BA (4 answers) Closed 8 years ago . Suppose we have two $n \times n$ matrices $A, B$. It seems like $\delta(AB)=\delta(BA)$, but I can't generally poove it. If $\det(A) \neq 0$ then $\det(AB - \lambda I) = 0 \Leftrightarrow $ $ \det(AB - \lambda I) \cdot \det(A) = 0 \Leftrightarrow \det(ABA - \lambda A) = 0 \Leftrightarrow $ $ \det(A) \cdot \det(BA - \lambda I) = 0 \Leftrightarrow \det(BA - \lambda I) = 0$, so $$\delta(AB)=\delta(BA)$$ Same if $\det(B) \neq 0$. But how to prove it for $\det(A) = \det(B) = 0$? Is it still true?",,"['linear-algebra', 'matrices']"
89,How can i express a quaternion in polar form?,How can i express a quaternion in polar form?,,Im trying to express the following quaternion in polar form (axis-angle) $a=1+i-2j+k$ Would this be the resultant ? $$\cos \frac{θ}{2} +\sin \frac{θ}{2} (i-2j+k)$$,Im trying to express the following quaternion in polar form (axis-angle) $a=1+i-2j+k$ Would this be the resultant ? $$\cos \frac{θ}{2} +\sin \frac{θ}{2} (i-2j+k)$$,,"['linear-algebra', 'vector-spaces', 'complex-numbers', 'quaternions']"
90,Why are Eigenvectors of an orthogonal matrix with respect to different eigenvalues orthogonal to one another.,Why are Eigenvectors of an orthogonal matrix with respect to different eigenvalues orthogonal to one another.,,"Why are Eigenvectors of an orthogonal matrix with respect to different eigenvalues orthogonal to one another? I tried to find this question, if this is a duplicate post a link and I will cancel this one. Also take an orthogonal matrix $A \in O(3)$ and the linear application associated with it $f: R^3 \rightarrow R^3$ Why is it that if $1$ is an eigenvector then $dim(V_1) = R^3$ and $A = I$ but if $1$ is not an eigenvector then $dim(V_2) $ is $2$ or $1$?","Why are Eigenvectors of an orthogonal matrix with respect to different eigenvalues orthogonal to one another? I tried to find this question, if this is a duplicate post a link and I will cancel this one. Also take an orthogonal matrix $A \in O(3)$ and the linear application associated with it $f: R^3 \rightarrow R^3$ Why is it that if $1$ is an eigenvector then $dim(V_1) = R^3$ and $A = I$ but if $1$ is not an eigenvector then $dim(V_2) $ is $2$ or $1$?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
91,Determinant of transpose intuitive proof,Determinant of transpose intuitive proof,,"We are using Artin's Algebra book for our Linear Algebra course. In Artin, $\operatorname{det}(A^T) = \operatorname{det} (A)$ is proved using elementary matrices and invertibility. All of us feel that there should be a 'deeper' or a more fundamental or a more intuitive proof without using elementary matrices or invertibility. The one our professor came up with used linear transformations between tensor algebras, wedges and exterior algebras which we do not understand. Are there any other proofs for $\operatorname{det}(A^T) = \operatorname{det} (A)$ ? Edit : Also, is there a geometric proof? For the $2\times2$ case at least?","We are using Artin's Algebra book for our Linear Algebra course. In Artin, is proved using elementary matrices and invertibility. All of us feel that there should be a 'deeper' or a more fundamental or a more intuitive proof without using elementary matrices or invertibility. The one our professor came up with used linear transformations between tensor algebras, wedges and exterior algebras which we do not understand. Are there any other proofs for ? Edit : Also, is there a geometric proof? For the case at least?",\operatorname{det}(A^T) = \operatorname{det} (A) \operatorname{det}(A^T) = \operatorname{det} (A) 2\times2,"['linear-algebra', 'matrices', 'determinant', 'intuition', 'transpose']"
92,Operator Norm of a Linear Transformation of a Matrix,Operator Norm of a Linear Transformation of a Matrix,,"The book I am using for the ODE course is Differential Equations and Dynamical Systems by Lawrence Perko. I am having a difficult time understanding what an operator norm of a linear transformation is. From the book, they define the operator norm of the linear operator $T$ as   \begin{equation} ||T||=\text{max}_{|\vec{x}|\leq 1}|T(\vec{x})| \end{equation}   where $|\vec{x}|$ denotes the Euclidean norm of $\vec{x}\in \mathbb{R}^n$. Upon reading, it seems the operator norm has all of the usual properties of the Euclidean norm. I have also come to understand that the operator norm is a means to measure the ""size"" of certain linear transformation. I assume this is in relation to boundedness? However, I still don't quite understand what the operator norm of a linear transformation is or what it's purpose it (other than used to define the concept of convergence in a linear space). What stumps me even more is trying to compute the operator norm of any linear transformation, for example \begin{array} d \begin{bmatrix} 1&0\\5&1\\ \end{bmatrix} \end{array} I'm sorry for the long read, but thank you so much for your time. I appreciate any feedback you may give in helping further my understanding of mathematics.","The book I am using for the ODE course is Differential Equations and Dynamical Systems by Lawrence Perko. I am having a difficult time understanding what an operator norm of a linear transformation is. From the book, they define the operator norm of the linear operator $T$ as   \begin{equation} ||T||=\text{max}_{|\vec{x}|\leq 1}|T(\vec{x})| \end{equation}   where $|\vec{x}|$ denotes the Euclidean norm of $\vec{x}\in \mathbb{R}^n$. Upon reading, it seems the operator norm has all of the usual properties of the Euclidean norm. I have also come to understand that the operator norm is a means to measure the ""size"" of certain linear transformation. I assume this is in relation to boundedness? However, I still don't quite understand what the operator norm of a linear transformation is or what it's purpose it (other than used to define the concept of convergence in a linear space). What stumps me even more is trying to compute the operator norm of any linear transformation, for example \begin{array} d \begin{bmatrix} 1&0\\5&1\\ \end{bmatrix} \end{array} I'm sorry for the long read, but thank you so much for your time. I appreciate any feedback you may give in helping further my understanding of mathematics.",,"['linear-algebra', 'ordinary-differential-equations', 'normed-spaces']"
93,"If $A^n = B$ and I know $B$, can I find $A$? [duplicate]","If  and I know , can I find ? [duplicate]",A^n = B B A,"This question already has answers here : Nth roots of square matrices (2 answers) Closed 8 years ago . Suppose that $A$ and $B$ are invertible, $p \times p$ matrices. If $A^n = B$ and I know all of the entries in $B$, can I find an $A$ for some or all integers $n \ge 0$? How many solutions for $A$ exist? If I'm thinking correctly, then $A = B * (A^{-1})^{n-1},$ but this is sort of self referential. Thanks!","This question already has answers here : Nth roots of square matrices (2 answers) Closed 8 years ago . Suppose that $A$ and $B$ are invertible, $p \times p$ matrices. If $A^n = B$ and I know all of the entries in $B$, can I find an $A$ for some or all integers $n \ge 0$? How many solutions for $A$ exist? If I'm thinking correctly, then $A = B * (A^{-1})^{n-1},$ but this is sort of self referential. Thanks!",,"['linear-algebra', 'matrices']"
94,Does the spectral norm of a square matrix equal its largest eigenvalue in absolute value?,Does the spectral norm of a square matrix equal its largest eigenvalue in absolute value?,,"I have one simple question. Given the spectral norm $\left \| \cdot \right \| _2$ of a matrix $A$,   which is equal to the square root of the largest eigenvalue of $A^{^*}A$ $$\left \| A \right \| _2=\sqrt{\lambda_{\text{max}}(A^{^*}A)}=\sigma_{\text{max}}(A)$$ for a square matrix $A$, is $\left \| A \right \| _2$ equal to the largest eigenvalue of $A$ in absolute value? I know it is true for a symmetric matrix but I don't know for a random square one.","I have one simple question. Given the spectral norm $\left \| \cdot \right \| _2$ of a matrix $A$,   which is equal to the square root of the largest eigenvalue of $A^{^*}A$ $$\left \| A \right \| _2=\sqrt{\lambda_{\text{max}}(A^{^*}A)}=\sigma_{\text{max}}(A)$$ for a square matrix $A$, is $\left \| A \right \| _2$ equal to the largest eigenvalue of $A$ in absolute value? I know it is true for a symmetric matrix but I don't know for a random square one.",,"['linear-algebra', 'matrices', 'normed-spaces', 'spectral-theory', 'svd']"
95,Isometries and Preservation of Eigenvalues,Isometries and Preservation of Eigenvalues,,"Does conjugation by an isometry preserve eigenvalues? If not, are there certain (non-trivial) situations where it does?","Does conjugation by an isometry preserve eigenvalues? If not, are there certain (non-trivial) situations where it does?",,['linear-algebra']
96,Prove $\det(I + B) = 2(1 + tr(B)).$,Prove,\det(I + B) = 2(1 + tr(B)).,"Let A be a $3\times 3$ invertible matrix (with real coefficients) and let $B=A^TA^{-1}$. Prove that \begin{equation*} \det(I + B) = 2(1 + tr(B)). \end{equation*} I know that \begin{equation*} \det (I+B)=\lambda_1\lambda_2\lambda_3+\lambda_1\lambda_2 +\lambda_1\lambda_3+\lambda_2\lambda_3+\lambda_1+\lambda_2+\lambda_3+1 \end{equation*} given $\lambda_1,\lambda_2$ and $\lambda_3$ are three distinct eigenvalues of $B$. However, I don't know where to go on from here and how to utilise the fact that $B=A^TA^{-1}$. Any help or direction would be appreciated.","Let A be a $3\times 3$ invertible matrix (with real coefficients) and let $B=A^TA^{-1}$. Prove that \begin{equation*} \det(I + B) = 2(1 + tr(B)). \end{equation*} I know that \begin{equation*} \det (I+B)=\lambda_1\lambda_2\lambda_3+\lambda_1\lambda_2 +\lambda_1\lambda_3+\lambda_2\lambda_3+\lambda_1+\lambda_2+\lambda_3+1 \end{equation*} given $\lambda_1,\lambda_2$ and $\lambda_3$ are three distinct eigenvalues of $B$. However, I don't know where to go on from here and how to utilise the fact that $B=A^TA^{-1}$. Any help or direction would be appreciated.",,"['linear-algebra', 'determinant', 'trace']"
97,Any hint about solving this monster determinant?,Any hint about solving this monster determinant?,,"I'm asked to solve the following determinant: $$|A|= \begin{vmatrix} 1 &2 &3 &\cdots &{n-1} &n\\ 2 &3 &4 &\cdots &n &1\\ \vdots &\vdots &\vdots & &\vdots &\vdots\\ {n-1} &n &1 &\cdots &{n-3} &{n-2}\\ n &1 &2 &\cdots &{n-2} &{n-1} \end{vmatrix} $$ My attempt is to add all the other columns onto the first one, which gives $$|A|=\frac{n(n+1)}{2}|B|$$ where $|B|$ is, however, none the easier than $|A|$. I think the result should be very special, since $A$ is a very special symmetric matrix itself. But I simply get stuck. Can you help me? thanks in advance. EDIT It just occurred to me that definition might work out well here. Am I on the right track? I'm now into another question. If $(j_1,j_2,\cdots,j_n)$ is an $n-th$ permutation of ${1,2,\cdots,n}$ and the number of inversion pairs in there is $\tau$, then what's the number of inversion pairs in its inverse permutation $(j_n,j_{n-1},\cdots,j_2,j_1)$ ? This may shed a light on the problem. Some friend of mine has given me a relatively simple solution, which I will add subsequently  as an answer.","I'm asked to solve the following determinant: $$|A|= \begin{vmatrix} 1 &2 &3 &\cdots &{n-1} &n\\ 2 &3 &4 &\cdots &n &1\\ \vdots &\vdots &\vdots & &\vdots &\vdots\\ {n-1} &n &1 &\cdots &{n-3} &{n-2}\\ n &1 &2 &\cdots &{n-2} &{n-1} \end{vmatrix} $$ My attempt is to add all the other columns onto the first one, which gives $$|A|=\frac{n(n+1)}{2}|B|$$ where $|B|$ is, however, none the easier than $|A|$. I think the result should be very special, since $A$ is a very special symmetric matrix itself. But I simply get stuck. Can you help me? thanks in advance. EDIT It just occurred to me that definition might work out well here. Am I on the right track? I'm now into another question. If $(j_1,j_2,\cdots,j_n)$ is an $n-th$ permutation of ${1,2,\cdots,n}$ and the number of inversion pairs in there is $\tau$, then what's the number of inversion pairs in its inverse permutation $(j_n,j_{n-1},\cdots,j_2,j_1)$ ? This may shed a light on the problem. Some friend of mine has given me a relatively simple solution, which I will add subsequently  as an answer.",,"['linear-algebra', 'matrices', 'determinant']"
98,The number of linearly independent solution of the homogeneous system of linear equations $AX=0$,The number of linearly independent solution of the homogeneous system of linear equations,AX=0,"I came across the following multiple choice question: The number of linearly independent solution of the homogeneous system of linear equations $AX=0$, where $X$ consists of $n$ unknowns and $A$ consists of $m$ linearly independent rows is $(A)$ $m-n$ $\space$$(B)$ $m$ $\space$$(C)$ $n-m$ $\space$$(D)$ none of these I think the answer will be $(D)$ because: When $m=n$, in this case it would mean a square matrix with all linearly independent rows, which implies unique solution. When $m<n$, it would mean that the rank of the matrix is less than number of unknowns in the system, which would mean infinite solutions and these solutions must be linearly dependent (Am I going right?). When $m>n$, there will be no solution (I am not sure about this one) I think there is something wrong about my answer because I remember something like: number of linearly independent solutions = number of unknowns - rank, from my Linear Algebra class. But I am not sure how to relate to it here.. Thanks..","I came across the following multiple choice question: The number of linearly independent solution of the homogeneous system of linear equations $AX=0$, where $X$ consists of $n$ unknowns and $A$ consists of $m$ linearly independent rows is $(A)$ $m-n$ $\space$$(B)$ $m$ $\space$$(C)$ $n-m$ $\space$$(D)$ none of these I think the answer will be $(D)$ because: When $m=n$, in this case it would mean a square matrix with all linearly independent rows, which implies unique solution. When $m<n$, it would mean that the rank of the matrix is less than number of unknowns in the system, which would mean infinite solutions and these solutions must be linearly dependent (Am I going right?). When $m>n$, there will be no solution (I am not sure about this one) I think there is something wrong about my answer because I remember something like: number of linearly independent solutions = number of unknowns - rank, from my Linear Algebra class. But I am not sure how to relate to it here.. Thanks..",,"['linear-algebra', 'matrices', 'matrix-equations']"
99,"Hoffman ""Linear Algebra"": why need such a long proof?","Hoffman ""Linear Algebra"": why need such a long proof?",,"I'm reading ""Linear Algebra"" by Kenneth Hoffman and Ray Kunze. I don't quite understand why there's a long proof in $\S$6.4 Theorem 6. First the triangular matrix is defined: An $n\times n$ matrix $A$ is called triangular if $A_{ij}=0$ whenever $i>j$ or if $A_{ij}=0$ whenever $i<j$. Then defined triangulable: The linear operator $T$ is called triangulable if there is an ordered basis in which $T$ is represented by a triangular matrix. Then there's Theorem 5: Let $V$ be a finite-dimensional vector space over the field $F$ and   let $T$ be a linear operator on $V$. Then $T$ is triangulable if and   only if the minimal polynomial for $T$ is a product of linear   polynomials over $F$. Now it comes Theorem 6: Let $V$ be a finite-dimensional vector space over the field $F$ and   let $T$ be a linear operator on $V$. Then $T$ is diagonalizable if and   only if the minimal polynomial for $T$ has the form $p = (x - c_1) \dots (x - c_k)$ where $c_1, \dots , c_k$ are distinct elements of   $F$. The proof is: (the (1)(2).. numbers are added by me) Proof (1) We have noted earlier that, if $T$ is diagonalizable, its minimal   polynomial is a product of distinct linear factors (see the discussion   prior to Example 4). (2)To prove the converse, let $W$ be the subspace spanned by all of   the characteristic vectors of $T$, and suppose $W \ne V$.  .... What I don't understand is (2) -- why we need such a long proof (details are below) here? Since Theorem 5 already proved that ""minimal polynomial factors $p=(x-c_1)^{r_1}\dots(x-c_k)^{r_k}$, $c_i$ distinct $\Rightarrow$ $T$ is triangulable""; this part of Theorem 6 is ""minimal polynomial factors $p=(x-c_1) \dots(x-c_k)$, $c_i$ distinct $\Rightarrow$ $T$ is triangulable"", so we just need to let all the $r_i$ be $1$, isn't it? Proof details excerpted from Hoffman (1) We have noted earlier that, if $T$ is diagonalizable, its minimal   polynomial is a product of distinct linear factors (see the discussion   prior to Example 4). (2)To prove the converse, let $W$ be the   subspace spanned by all of the characteristic vectors of $T$, and   suppose $W \ne V$. (3)By the lemma used in the proof of Theorem 5,   there is a vector $\alpha$ not in $W$ and a characteristic value $c_j$   of $T$ such that the vector $\beta= (T - c_jI)\alpha$ lies in W. (4)Since $\beta$ is in $W$, $\beta = \beta_1+\dots\beta_k$ where   $T\beta_i = c_i\beta_i$, $1\le i\le k$, and therefore the vector   $h(T)\beta = h(c_1)\beta_1+\dots+h(c_k)\beta_k$ is in $W$, for every   polynomial $h$. (5)Now $p = (x-c_j)q$, for some polynomial $q$. (6)Also $q- q(c_j) = (x - c_j)h$. (7)We have $q(T)\alpha - q(c_j)\alpha = h(T)(T - c_jI)\alpha = h(T)\beta$. (8)But $h(T)\beta$   is in $W$ and, since $0 = p(T)\alpha = (T - c_jI)q(T)\alpha$, the   vector $q(T)\alpha$ is in $W$. (9)Therefore, $q(c_j)\alpha$ is in   $W$. (10)Since $\alpha$ is not in $W$, we have $q(c_j) = 0$. (11)That contradicts the fact that $p$ has distinct roots. QED.","I'm reading ""Linear Algebra"" by Kenneth Hoffman and Ray Kunze. I don't quite understand why there's a long proof in $\S$6.4 Theorem 6. First the triangular matrix is defined: An $n\times n$ matrix $A$ is called triangular if $A_{ij}=0$ whenever $i>j$ or if $A_{ij}=0$ whenever $i<j$. Then defined triangulable: The linear operator $T$ is called triangulable if there is an ordered basis in which $T$ is represented by a triangular matrix. Then there's Theorem 5: Let $V$ be a finite-dimensional vector space over the field $F$ and   let $T$ be a linear operator on $V$. Then $T$ is triangulable if and   only if the minimal polynomial for $T$ is a product of linear   polynomials over $F$. Now it comes Theorem 6: Let $V$ be a finite-dimensional vector space over the field $F$ and   let $T$ be a linear operator on $V$. Then $T$ is diagonalizable if and   only if the minimal polynomial for $T$ has the form $p = (x - c_1) \dots (x - c_k)$ where $c_1, \dots , c_k$ are distinct elements of   $F$. The proof is: (the (1)(2).. numbers are added by me) Proof (1) We have noted earlier that, if $T$ is diagonalizable, its minimal   polynomial is a product of distinct linear factors (see the discussion   prior to Example 4). (2)To prove the converse, let $W$ be the subspace spanned by all of   the characteristic vectors of $T$, and suppose $W \ne V$.  .... What I don't understand is (2) -- why we need such a long proof (details are below) here? Since Theorem 5 already proved that ""minimal polynomial factors $p=(x-c_1)^{r_1}\dots(x-c_k)^{r_k}$, $c_i$ distinct $\Rightarrow$ $T$ is triangulable""; this part of Theorem 6 is ""minimal polynomial factors $p=(x-c_1) \dots(x-c_k)$, $c_i$ distinct $\Rightarrow$ $T$ is triangulable"", so we just need to let all the $r_i$ be $1$, isn't it? Proof details excerpted from Hoffman (1) We have noted earlier that, if $T$ is diagonalizable, its minimal   polynomial is a product of distinct linear factors (see the discussion   prior to Example 4). (2)To prove the converse, let $W$ be the   subspace spanned by all of the characteristic vectors of $T$, and   suppose $W \ne V$. (3)By the lemma used in the proof of Theorem 5,   there is a vector $\alpha$ not in $W$ and a characteristic value $c_j$   of $T$ such that the vector $\beta= (T - c_jI)\alpha$ lies in W. (4)Since $\beta$ is in $W$, $\beta = \beta_1+\dots\beta_k$ where   $T\beta_i = c_i\beta_i$, $1\le i\le k$, and therefore the vector   $h(T)\beta = h(c_1)\beta_1+\dots+h(c_k)\beta_k$ is in $W$, for every   polynomial $h$. (5)Now $p = (x-c_j)q$, for some polynomial $q$. (6)Also $q- q(c_j) = (x - c_j)h$. (7)We have $q(T)\alpha - q(c_j)\alpha = h(T)(T - c_jI)\alpha = h(T)\beta$. (8)But $h(T)\beta$   is in $W$ and, since $0 = p(T)\alpha = (T - c_jI)q(T)\alpha$, the   vector $q(T)\alpha$ is in $W$. (9)Therefore, $q(c_j)\alpha$ is in   $W$. (10)Since $\alpha$ is not in $W$, we have $q(c_j) = 0$. (11)That contradicts the fact that $p$ has distinct roots. QED.",,"['linear-algebra', 'matrices', 'diagonalization', 'minimal-polynomials']"
