,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Minimum of an apparently harmless function of two variables,Minimum of an apparently harmless function of two variables,,"I would like to prove that the minimum of the function $$ f(x,y):=\frac{(1-\cos(\pi x))(1-\cos (\pi y))\sqrt{x^2+y^2}}{x^2 y^2 \sqrt{(1-\cos(\pi x))(2+\cos(\pi y))+(2+\cos(\pi x))(1-\cos(\pi y))}} $$ over the domain $[0,1]^2$ is $2\sqrt{2}$. Looking at the 2D plot of the function one immediately notices that the minimum is $f(1,1) = 2\sqrt{2}$. However, I can't figure out how to prove this in a rigorous way, even if the expression of $f$ seems to have a nice, ""quasi-separable"" structure... Any suggestion is welcomed!","I would like to prove that the minimum of the function $$ f(x,y):=\frac{(1-\cos(\pi x))(1-\cos (\pi y))\sqrt{x^2+y^2}}{x^2 y^2 \sqrt{(1-\cos(\pi x))(2+\cos(\pi y))+(2+\cos(\pi x))(1-\cos(\pi y))}} $$ over the domain $[0,1]^2$ is $2\sqrt{2}$. Looking at the 2D plot of the function one immediately notices that the minimum is $f(1,1) = 2\sqrt{2}$. However, I can't figure out how to prove this in a rigorous way, even if the expression of $f$ seems to have a nice, ""quasi-separable"" structure... Any suggestion is welcomed!",,"['real-analysis', 'multivariable-calculus', 'optimization']"
1,Calculating an area between circles with Double Integral,Calculating an area between circles with Double Integral,,"Hey I need to answer the following question: find the area of $D=[(x,y):(x-1)^2+y^2\leq 1, x^2+y^2\geq 1,0\leq y\leq x] $ I know how to solve this kind of problems with normal integrals but how do i manage to calculate it with double integral?  I tried to use the polar technique but i dont sure im doing it right Thank you!","Hey I need to answer the following question: find the area of $D=[(x,y):(x-1)^2+y^2\leq 1, x^2+y^2\geq 1,0\leq y\leq x] $ I know how to solve this kind of problems with normal integrals but how do i manage to calculate it with double integral?  I tried to use the polar technique but i dont sure im doing it right Thank you!",,"['calculus', 'multivariable-calculus']"
2,Remainder in the multivariate Taylor expansion,Remainder in the multivariate Taylor expansion,,"For the function $f:\mathbb{R}\to\mathbb{R}$, I can write the Taylor expansion  $$f(x+h) = f(x) + f'(x)h + \frac{1}{2!}f''(x)h^2 + O(h^3)$$ where the remainder is $o(h^2)$ as well. I'm confused with what is the remainder written with Big oh for the second-order expansion in the multivariate case $f:\mathbb{R}^n\to\mathbb{R}$, i.e. $$f(\mathbf{x} + \mathbf{h}) = f(\mathbf{x}) + Df(\mathbf{x})\mathbf{h} + \frac{1}{2!}\mathbf{h}'D^2f(\mathbf{x})\mathbf{h} + O(?)$$ Should it be something like $O\left(\sum_{k,l,m}h_kh_lh_m\right)$? I know that with little oh it is simply $o(\|\mathbf{h}\|^2)$.","For the function $f:\mathbb{R}\to\mathbb{R}$, I can write the Taylor expansion  $$f(x+h) = f(x) + f'(x)h + \frac{1}{2!}f''(x)h^2 + O(h^3)$$ where the remainder is $o(h^2)$ as well. I'm confused with what is the remainder written with Big oh for the second-order expansion in the multivariate case $f:\mathbb{R}^n\to\mathbb{R}$, i.e. $$f(\mathbf{x} + \mathbf{h}) = f(\mathbf{x}) + Df(\mathbf{x})\mathbf{h} + \frac{1}{2!}\mathbf{h}'D^2f(\mathbf{x})\mathbf{h} + O(?)$$ Should it be something like $O\left(\sum_{k,l,m}h_kh_lh_m\right)$? I know that with little oh it is simply $o(\|\mathbf{h}\|^2)$.",,"['multivariable-calculus', 'taylor-expansion']"
3,Proof of (MV Calculus) Chain Rule,Proof of (MV Calculus) Chain Rule,,"Ok, so I'm trying to understand the proof of the Chain Rule from Spivak's Calculus on Manifolds, so I (hopefully correctly) opened up the entire proof to try to understand all the algebra and the roles of the epsilons and deltas in the proof. I'd like to get feedback on if what I did looks fine (even though it looks horrible): Theorem (Chain Rule): If $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ is differentiable at $a$, and $g:\mathbb{R}^m \rightarrow \mathbb{R}^p$ is differentiable at $f(a)$, then the  composition $g \circ f: \mathbb{R}^n \rightarrow \mathbb{R}^p$ is differentiable at $a$, and $$D(g \circ f)(a)=Dg(f(a)) \circ Df(a).$$ Proof: Suppose $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ is differentiable at $a$, and $g:\mathbb{R}^m \rightarrow \mathbb{R}^p$ is differentiable at $f(a)$. This means that $$(1) \ \lim_{h \to 0} \frac{\lvert f(a+h)-f(a)-Df(a)(h) \rvert}{\lvert h \rvert}=0,$$ and that $$(2) \ \lim_{h \to 0} \frac{\lvert g(f(a)+h)-g(f(a))-[Dg(f(a))](h) \rvert}{\lvert h \rvert}=0.$$ Note then that  $$\frac{\lvert (g \circ f)(a+h)-(g \circ f)(a)-[Dg(f(a)) \circ Df(a)](h) \rvert}{\lvert h \rvert}=$$ $$\frac{\lvert g(f(a+h))-g(f(a))-Dg(f(a))[Df(a)(h)] \rvert}{\lvert h \rvert}=$$ $$\frac{\lvert g(f(a+h))-g(f(a))-Dg(f(a))[f(a+h)-f(a)-f(a+h)+f(a)+Df(a)(h)] \rvert}{\lvert h \rvert} \leq$$ $$\frac{\lvert g(f(a+h))-g(f(a))-Dg(f(a))[f(a+h)-f(a)] \rvert}{\lvert h \rvert}+\frac{\lvert Dg(f(a))[f(a+h)-f(a)-Df(a)(h)] \rvert}{\lvert h \rvert}.$$ But by (2), for every $\epsilon >0$, we can find $\delta >0$ such that $0<\lvert k \rvert<\delta$ implies that $\lvert g(f(a)+k)-g(f(a))-[Dg(f(a))](k) \rvert<\epsilon \lvert k \rvert$; in particular, if $0<\lvert f(a+h)-f(a) \rvert<\delta$ for any $h$ satisfying $0<\lvert h \rvert<\rho$ for a suitable $\rho>0$, then  $$\frac{\lvert g(f(a+h))-g(f(a))-Dg(f(a))[f(a+h)-f(a)] \rvert}{\lvert h \rvert}<$$ $$\frac{\epsilon \lvert f(a+h)-f(a) \rvert}{\lvert h \rvert}=$$ $$\frac{\epsilon \lvert f(a+h)-f(a)-Df(a)(h)+Df(a)(h) \rvert}{\lvert h \rvert} \leq$$ $$\frac{\epsilon \lvert f(a+h)-f(a)-Df(a)(h) \rvert}{\lvert h \rvert} + \frac{\epsilon \lvert Df(a)(h) \rvert}{\lvert h \rvert}.$$ Now, since $Df(a)$ and $Dg(f(a))$ are linear transformations, we also have that for certain $M,N \in \mathbb{R}$, $$\lvert Df(a)(h) \rvert<M\lvert h \rvert$$ and  $$\lvert Dg(f(a))[f(a+h)-f(a)-Df(a)(h)] \rvert<N\lvert f(a+h)-f(a)-Df(a)(h) \rvert.$$ This in turn gives first that $$\frac{\epsilon \lvert Df(a)(h) \rvert}{\lvert h \rvert}<M \epsilon,$$ and also, by using (1), that for every $\epsilon >0$ we can find $\delta >0$ such that $\lvert h \rvert<\delta$ implies that $\lvert f(a+h)-f(a)-Df(a)(h) \rvert<\epsilon \lvert h \rvert$, from which we also obtain  $$\frac{\lvert f(a+h)-f(a)-Df(a)(h) \rvert}{\lvert h \rvert}<\epsilon$$ and $$\frac{\lvert Dg(f(a))[f(a+h)-f(a)-Df(a)(h)] \rvert}{\lvert h \rvert}<$$ $$\frac{N\lvert f(a+h)-f(a)-Df(a)(h) \rvert}{\lvert h \rvert}<$$ $$\frac{N \epsilon \lvert h \rvert}{\lvert h \rvert}=N \epsilon.$$ Finally, piecing all of the above together (from the last three inequalities we can conclude that both terms on the right hand side of the original inequality are arbitrarily small) gives the conclusion: $$\lim_{h \to 0} \frac{\lvert (g \circ f)(a+h)-(g \circ f)(a)-[Dg(f(a)) \circ Df(a)](h) \rvert}{\lvert h \rvert}=0.$$ Thanks in advance.","Ok, so I'm trying to understand the proof of the Chain Rule from Spivak's Calculus on Manifolds, so I (hopefully correctly) opened up the entire proof to try to understand all the algebra and the roles of the epsilons and deltas in the proof. I'd like to get feedback on if what I did looks fine (even though it looks horrible): Theorem (Chain Rule): If $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ is differentiable at $a$, and $g:\mathbb{R}^m \rightarrow \mathbb{R}^p$ is differentiable at $f(a)$, then the  composition $g \circ f: \mathbb{R}^n \rightarrow \mathbb{R}^p$ is differentiable at $a$, and $$D(g \circ f)(a)=Dg(f(a)) \circ Df(a).$$ Proof: Suppose $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ is differentiable at $a$, and $g:\mathbb{R}^m \rightarrow \mathbb{R}^p$ is differentiable at $f(a)$. This means that $$(1) \ \lim_{h \to 0} \frac{\lvert f(a+h)-f(a)-Df(a)(h) \rvert}{\lvert h \rvert}=0,$$ and that $$(2) \ \lim_{h \to 0} \frac{\lvert g(f(a)+h)-g(f(a))-[Dg(f(a))](h) \rvert}{\lvert h \rvert}=0.$$ Note then that  $$\frac{\lvert (g \circ f)(a+h)-(g \circ f)(a)-[Dg(f(a)) \circ Df(a)](h) \rvert}{\lvert h \rvert}=$$ $$\frac{\lvert g(f(a+h))-g(f(a))-Dg(f(a))[Df(a)(h)] \rvert}{\lvert h \rvert}=$$ $$\frac{\lvert g(f(a+h))-g(f(a))-Dg(f(a))[f(a+h)-f(a)-f(a+h)+f(a)+Df(a)(h)] \rvert}{\lvert h \rvert} \leq$$ $$\frac{\lvert g(f(a+h))-g(f(a))-Dg(f(a))[f(a+h)-f(a)] \rvert}{\lvert h \rvert}+\frac{\lvert Dg(f(a))[f(a+h)-f(a)-Df(a)(h)] \rvert}{\lvert h \rvert}.$$ But by (2), for every $\epsilon >0$, we can find $\delta >0$ such that $0<\lvert k \rvert<\delta$ implies that $\lvert g(f(a)+k)-g(f(a))-[Dg(f(a))](k) \rvert<\epsilon \lvert k \rvert$; in particular, if $0<\lvert f(a+h)-f(a) \rvert<\delta$ for any $h$ satisfying $0<\lvert h \rvert<\rho$ for a suitable $\rho>0$, then  $$\frac{\lvert g(f(a+h))-g(f(a))-Dg(f(a))[f(a+h)-f(a)] \rvert}{\lvert h \rvert}<$$ $$\frac{\epsilon \lvert f(a+h)-f(a) \rvert}{\lvert h \rvert}=$$ $$\frac{\epsilon \lvert f(a+h)-f(a)-Df(a)(h)+Df(a)(h) \rvert}{\lvert h \rvert} \leq$$ $$\frac{\epsilon \lvert f(a+h)-f(a)-Df(a)(h) \rvert}{\lvert h \rvert} + \frac{\epsilon \lvert Df(a)(h) \rvert}{\lvert h \rvert}.$$ Now, since $Df(a)$ and $Dg(f(a))$ are linear transformations, we also have that for certain $M,N \in \mathbb{R}$, $$\lvert Df(a)(h) \rvert<M\lvert h \rvert$$ and  $$\lvert Dg(f(a))[f(a+h)-f(a)-Df(a)(h)] \rvert<N\lvert f(a+h)-f(a)-Df(a)(h) \rvert.$$ This in turn gives first that $$\frac{\epsilon \lvert Df(a)(h) \rvert}{\lvert h \rvert}<M \epsilon,$$ and also, by using (1), that for every $\epsilon >0$ we can find $\delta >0$ such that $\lvert h \rvert<\delta$ implies that $\lvert f(a+h)-f(a)-Df(a)(h) \rvert<\epsilon \lvert h \rvert$, from which we also obtain  $$\frac{\lvert f(a+h)-f(a)-Df(a)(h) \rvert}{\lvert h \rvert}<\epsilon$$ and $$\frac{\lvert Dg(f(a))[f(a+h)-f(a)-Df(a)(h)] \rvert}{\lvert h \rvert}<$$ $$\frac{N\lvert f(a+h)-f(a)-Df(a)(h) \rvert}{\lvert h \rvert}<$$ $$\frac{N \epsilon \lvert h \rvert}{\lvert h \rvert}=N \epsilon.$$ Finally, piecing all of the above together (from the last three inequalities we can conclude that both terms on the right hand side of the original inequality are arbitrarily small) gives the conclusion: $$\lim_{h \to 0} \frac{\lvert (g \circ f)(a+h)-(g \circ f)(a)-[Dg(f(a)) \circ Df(a)](h) \rvert}{\lvert h \rvert}=0.$$ Thanks in advance.",,['multivariable-calculus']
4,Non-differentiability of a function of two variables at a point,Non-differentiability of a function of two variables at a point,,"I have problems understanding this: Function $\;g(x,y)\;$ is given, for which a) $\;g_x(0,0)=7\;$ b) $\;g(t+2t^2,\sin3t+4t^2)=5e^t\;$ c) $\;\lim_{t\to 0}\frac{g(t,2t)-g(3t,4t)}t=10\;$ They ask to show there's a point for which $\;g\;$ isn't differentiable and what is that point. I use b) to have that $g(t+2t^2,\sin3t+4t^2)=5e^t\to 5\;$ with $\;t\to 0\;$, and then it can't be continuous at origin since $\;g(0,0)=7\;$ and then not differentiable, but then I don't get what condition c) tells.","I have problems understanding this: Function $\;g(x,y)\;$ is given, for which a) $\;g_x(0,0)=7\;$ b) $\;g(t+2t^2,\sin3t+4t^2)=5e^t\;$ c) $\;\lim_{t\to 0}\frac{g(t,2t)-g(3t,4t)}t=10\;$ They ask to show there's a point for which $\;g\;$ isn't differentiable and what is that point. I use b) to have that $g(t+2t^2,\sin3t+4t^2)=5e^t\to 5\;$ with $\;t\to 0\;$, and then it can't be continuous at origin since $\;g(0,0)=7\;$ and then not differentiable, but then I don't get what condition c) tells.",,['multivariable-calculus']
5,Partial Differential Equation with a flux term,Partial Differential Equation with a flux term,,"Recall how we derived all our equations: Take an interval $[a,b]$ and consider $$\dfrac{\mathrm d}{\mathrm dt}\int_a^b(\text{quantity) d}x=\big[\text{Flux}\big]_a-\big[{\rm Flux}\big]_b.$$ For our example $$\dfrac{\partial u}{\partial t}+u\dfrac{\partial u}{\partial x}=0,$$   we can write it as $$\dfrac{\partial u}{\partial t}+\dfrac{\partial}{\partial x}\left(\tfrac12u^2\right)=0.$$   $$\text{i.e.}\qquad\dfrac{\partial u}{\partial t}+\dfrac{\partial \phi}{\partial x}=0\qquad\text{where }\quad\phi=\tfrac12u^2.$$ I don't understand why $\phi=\frac{1}{2}u^2$ is the flux in this case?","Recall how we derived all our equations: Take an interval $[a,b]$ and consider $$\dfrac{\mathrm d}{\mathrm dt}\int_a^b(\text{quantity) d}x=\big[\text{Flux}\big]_a-\big[{\rm Flux}\big]_b.$$ For our example $$\dfrac{\partial u}{\partial t}+u\dfrac{\partial u}{\partial x}=0,$$   we can write it as $$\dfrac{\partial u}{\partial t}+\dfrac{\partial}{\partial x}\left(\tfrac12u^2\right)=0.$$   $$\text{i.e.}\qquad\dfrac{\partial u}{\partial t}+\dfrac{\partial \phi}{\partial x}=0\qquad\text{where }\quad\phi=\tfrac12u^2.$$ I don't understand why $\phi=\frac{1}{2}u^2$ is the flux in this case?",,['multivariable-calculus']
6,Boltzmann's transformation and change of variables.,Boltzmann's transformation and change of variables.,,"Boltzmann’s Transformation is used (among other things) to convert Fick's second law into an easily solvable ordinary differential equation. It uses the variable $\xi=\frac{x}{2 \sqrt{t}}$. As far as I can remember, a change of variables has to be a diffeomorphism, and diffeomorphisms are necessarily between manifolds of the same dimension. So there should exist another non trivial variable, function of (x,t), let us call it $ \eta $, such that the transformation $\Phi : (x,t) \rightarrow (\xi,\eta)$ is a diffeomorphism. I have tried a few possibilities without success and searched the ""internets"" for a solution in vain, so I thought I would submit it to the community. Would anybody know either the answer to this, of if there exists a general method to find the missing variable?","Boltzmann’s Transformation is used (among other things) to convert Fick's second law into an easily solvable ordinary differential equation. It uses the variable $\xi=\frac{x}{2 \sqrt{t}}$. As far as I can remember, a change of variables has to be a diffeomorphism, and diffeomorphisms are necessarily between manifolds of the same dimension. So there should exist another non trivial variable, function of (x,t), let us call it $ \eta $, such that the transformation $\Phi : (x,t) \rightarrow (\xi,\eta)$ is a diffeomorphism. I have tried a few possibilities without success and searched the ""internets"" for a solution in vain, so I thought I would submit it to the community. Would anybody know either the answer to this, of if there exists a general method to find the missing variable?",,"['multivariable-calculus', 'partial-differential-equations']"
7,Online classes/books in multivariable calculus?,Online classes/books in multivariable calculus?,,"So does anyone know of any good online courses in multivariable calculus? (Or in a possible alternative leap of curriculum, if said path has proven to be better/moar interesting.) I'm coming straight from BC Calc (5); but my high school doesn't offer math past that. As for descriptions of the course...not being free is tolerable. The only real requirement-requirement would be that the class does not require presence in an online classroom. Also something that is important for me is I need some way of getting credit for the course--not necessarily high school type credits, but more of just a general report-card type thing which ensures to my high school that I'm learning math. (So MIT OCW would be difficult in this aspect...) As for books, I have the book Calculus, a New Horizon (ISBN 0471482730)), which is just a normal textbook that tided me over through single variable calc and supposedly should last through multivariable calculus as well. However if you know of books that are significantly better, please name them!","So does anyone know of any good online courses in multivariable calculus? (Or in a possible alternative leap of curriculum, if said path has proven to be better/moar interesting.) I'm coming straight from BC Calc (5); but my high school doesn't offer math past that. As for descriptions of the course...not being free is tolerable. The only real requirement-requirement would be that the class does not require presence in an online classroom. Also something that is important for me is I need some way of getting credit for the course--not necessarily high school type credits, but more of just a general report-card type thing which ensures to my high school that I'm learning math. (So MIT OCW would be difficult in this aspect...) As for books, I have the book Calculus, a New Horizon (ISBN 0471482730)), which is just a normal textbook that tided me over through single variable calc and supposedly should last through multivariable calculus as well. However if you know of books that are significantly better, please name them!",,['multivariable-calculus']
8,Visual intuition partial/directional derivative,Visual intuition partial/directional derivative,,"I've had some trouble with the (visual) intuition behind the directional derivatives so I decided to take a step back and look up the visual intuition behind partial derivatives, which I think I do understand. See picture below As I understood it, we basically have the purple paraboloid (?) which is a function of (x,y) and then we have the gray plane which is the plane in the direction of the x-axis. If you intersect the 2 planes I would say you get a parabola. If you have a certain point specified on the paraboloid, you can find its partial derivative in the direction of x. The way I draw the connection to a directional derivative is just by saying that you can tilt the gray plane in any direction and find the derivative. Is this correct, and if not, what's wrong?","I've had some trouble with the (visual) intuition behind the directional derivatives so I decided to take a step back and look up the visual intuition behind partial derivatives, which I think I do understand. See picture below As I understood it, we basically have the purple paraboloid (?) which is a function of (x,y) and then we have the gray plane which is the plane in the direction of the x-axis. If you intersect the 2 planes I would say you get a parabola. If you have a certain point specified on the paraboloid, you can find its partial derivative in the direction of x. The way I draw the connection to a directional derivative is just by saying that you can tilt the gray plane in any direction and find the derivative. Is this correct, and if not, what's wrong?",,"['multivariable-calculus', 'intuition']"
9,the spectrum and determinant of the Laplacian on $S^3$,the spectrum and determinant of the Laplacian on,S^3,"I came across the following statement in a paper: On $S^3$, the eigenvalues of the vector Laplacian on divergenceless vector   fields is $(\ell + 1)^2$ with degeneracy $2\ell(\ell+2)$ with $\ell \in \mathbb{ Z}$. Is it possible to prove the spectrum and degeneracy using the representation theory of $SO(4)$?  Perhaps there is a general result for the n-sphere. The paper then proceeds to make the non-sense statement (RHS is divergent): $$ \det \big(-\Delta + a\big) = \prod_{\ell=1}^\infty \big((\ell + 1)^2 + a \big)^{2\ell(\ell+2)}  $$ How do we make sense of the determinant of the Laplacian on the space of divergenceless vector fields?","I came across the following statement in a paper: On $S^3$, the eigenvalues of the vector Laplacian on divergenceless vector   fields is $(\ell + 1)^2$ with degeneracy $2\ell(\ell+2)$ with $\ell \in \mathbb{ Z}$. Is it possible to prove the spectrum and degeneracy using the representation theory of $SO(4)$?  Perhaps there is a general result for the n-sphere. The paper then proceeds to make the non-sense statement (RHS is divergent): $$ \det \big(-\Delta + a\big) = \prod_{\ell=1}^\infty \big((\ell + 1)^2 + a \big)^{2\ell(\ell+2)}  $$ How do we make sense of the determinant of the Laplacian on the space of divergenceless vector fields?",,"['multivariable-calculus', 'eigenvalues-eigenvectors', 'harmonic-analysis', 'spherical-harmonics']"
10,Does uniform continuity of the differential imply uniform differentiability?,Does uniform continuity of the differential imply uniform differentiability?,,"Let $E \subseteq \mathbb{R}^n$ be an open subset. $f:E \to \mathbb{R}$ be differentiable, and suppose that $\nabla f$ is uniformly continuous. Is it true that $f$ is ""uniformly differentiable""? i.e. does there exist, for any $\epsilon >0$ , a $\delta > 0$ such that for all $a,x \in \mathbb{R}^n$ , $$\frac{|f(x) - f(a) - \nabla f (a)\cdot (x-a)|}{|x-a|} <\epsilon$$ whenever $|x-a|<\delta$ . I can prove this for any convex $E$ . (see below). Is it true for non-convex domains as well? My proof: $\nabla f$ uniformly continuous implies that for any $\epsilon >0$ , there is a $\delta>0$ such that for all $x,y \in \mathbb{R}^n$ , $$|x-y|<\delta \Rightarrow |\nabla f(x) - \nabla f(y)|<\epsilon.$$ Let $\epsilon > 0 $ be fixed. Choose $x,a \in \mathbb{R}^n$ such that $|x-a| < \delta$ . By the mean value theorem (for convex domains), there is a $z$ on the line segment connecting $a$ and $x$ such that $$f(x) - f(a) = \nabla f (z) \cdot (x-a).$$ Then $$\begin{align} \frac{|f(x) - f(a) - \nabla f (a)\cdot (x-a)|}{|x-a|} &=     \frac{|(\nabla f(z) - \nabla f(a)) \cdot (x-a)|}{|x-a|} \\ & \leq  \frac{|\nabla f(z) - \nabla f(a)| |x-a|}{|x-a|} \\ & <  \epsilon \end{align},$$ since $|z-a| < |x-a| < \delta$ .","Let be an open subset. be differentiable, and suppose that is uniformly continuous. Is it true that is ""uniformly differentiable""? i.e. does there exist, for any , a such that for all , whenever . I can prove this for any convex . (see below). Is it true for non-convex domains as well? My proof: uniformly continuous implies that for any , there is a such that for all , Let be fixed. Choose such that . By the mean value theorem (for convex domains), there is a on the line segment connecting and such that Then since .","E \subseteq \mathbb{R}^n f:E \to \mathbb{R} \nabla f f \epsilon >0 \delta > 0 a,x \in \mathbb{R}^n \frac{|f(x) - f(a) - \nabla f (a)\cdot (x-a)|}{|x-a|} <\epsilon |x-a|<\delta E \nabla f \epsilon >0 \delta>0 x,y \in \mathbb{R}^n |x-y|<\delta \Rightarrow |\nabla f(x) - \nabla f(y)|<\epsilon. \epsilon > 0  x,a \in \mathbb{R}^n |x-a| < \delta z a x f(x) - f(a) = \nabla f (z) \cdot (x-a). \begin{align}
\frac{|f(x) - f(a) - \nabla f (a)\cdot (x-a)|}{|x-a|} &=  
  \frac{|(\nabla f(z) - \nabla f(a)) \cdot (x-a)|}{|x-a|} \\
& \leq  \frac{|\nabla f(z) - \nabla f(a)| |x-a|}{|x-a|} \\
& <  \epsilon
\end{align}, |z-a| < |x-a| < \delta","['real-analysis', 'multivariable-calculus', 'derivatives', 'uniform-continuity']"
11,Maximum & minimum values of multivariable function,Maximum & minimum values of multivariable function,,"I am to check for max & min values for the given function $$f(x, y, z) = xy^2z^3$$ which is defined on $$M = \left\{x, y, z > 0, x+2y+3z=6\right\}$$ So.. what I did is: $F(x, y, z, \gamma) = xy^2z^3 - \gamma(x+2y+3z-6)$ therefore $\begin{cases} \frac{\partial F}{\partial x} = y^2z^3 - \gamma = 0\\ \frac{\partial F}{\partial y}  = 2xyz^3-2\gamma = 0\\ \frac{\partial F}{\partial z} = 3xy^2z^2 -3\gamma = 0\\ \frac{\partial F}{\partial \gamma} = -x-2y-3z+6 = 0\\ \end{cases}$ After the calculation I get the possible point is $P = (1, 1, 1)$. So I count the 2nd derivatives :  $$\left[\begin{array}{ccc} \frac{\partial^2 f}{\partial x^2} = 0 &\frac{\partial^2 f}{\partial x\partial y} = 2yz^3 &\frac{\partial^2 f}{\partial x\partial z} = 3y^2z^2 \\ \frac{\partial^2 f}{\partial y \partial x} = 2yz^3 &\frac{\partial^2 f}{\partial y^2} = 2xz^3 &\frac{\partial^2 f}{\partial y\partial z} = 6xyz^2 \\ \frac{\partial^2 f}{\partial z \partial x} = 3y^2z^2 &\frac{\partial^2 f}{\partial z \partial y} = 6xyz^2 &\frac{\partial^2 f}{\partial z^2} = 6xy^2z  \end{array}\right]$$ So with our point P it looks like : $$\left[\begin{array}{ccc} 0&2&3\\ 2&2&6\\ 3&6&6 \end{array}\right]$$ And here's where I got lost. $\det_{1} = 0$ therefore we have no idea whether there is an extremum or not, but Wolfram says there's a local maximum in $(1, 1, 1)$. How should I obtain it?","I am to check for max & min values for the given function $$f(x, y, z) = xy^2z^3$$ which is defined on $$M = \left\{x, y, z > 0, x+2y+3z=6\right\}$$ So.. what I did is: $F(x, y, z, \gamma) = xy^2z^3 - \gamma(x+2y+3z-6)$ therefore $\begin{cases} \frac{\partial F}{\partial x} = y^2z^3 - \gamma = 0\\ \frac{\partial F}{\partial y}  = 2xyz^3-2\gamma = 0\\ \frac{\partial F}{\partial z} = 3xy^2z^2 -3\gamma = 0\\ \frac{\partial F}{\partial \gamma} = -x-2y-3z+6 = 0\\ \end{cases}$ After the calculation I get the possible point is $P = (1, 1, 1)$. So I count the 2nd derivatives :  $$\left[\begin{array}{ccc} \frac{\partial^2 f}{\partial x^2} = 0 &\frac{\partial^2 f}{\partial x\partial y} = 2yz^3 &\frac{\partial^2 f}{\partial x\partial z} = 3y^2z^2 \\ \frac{\partial^2 f}{\partial y \partial x} = 2yz^3 &\frac{\partial^2 f}{\partial y^2} = 2xz^3 &\frac{\partial^2 f}{\partial y\partial z} = 6xyz^2 \\ \frac{\partial^2 f}{\partial z \partial x} = 3y^2z^2 &\frac{\partial^2 f}{\partial z \partial y} = 6xyz^2 &\frac{\partial^2 f}{\partial z^2} = 6xy^2z  \end{array}\right]$$ So with our point P it looks like : $$\left[\begin{array}{ccc} 0&2&3\\ 2&2&6\\ 3&6&6 \end{array}\right]$$ And here's where I got lost. $\det_{1} = 0$ therefore we have no idea whether there is an extremum or not, but Wolfram says there's a local maximum in $(1, 1, 1)$. How should I obtain it?",,['multivariable-calculus']
12,Proving that this function must be even,Proving that this function must be even,,"Let $u:\mathbb{R}^d\rightarrow\mathbb{R}$ be a continuous function that is not identically equal to zero.  Suppose further that $u$ is an odd function (ie. $u(\mathbf{x})=-u(-\mathbf{x})$).  Let $g:\mathbb{R}^d\rightarrow\mathbb{R}$ be continuous.  I'd like to prove the following: If $$ \int_{\mathbb{R}^d}u(\mathbf{x})f(g(\mathbf{x}))d\mathbf{x}=0 \qquad \forall f\in C_b(\mathbb{R})$$ then $g(\mathbf{x})=g(-\mathbf{x})$ for all $\mathbf{x}\in\mathbb{R}^d$, that is $g$ is an even function.","Let $u:\mathbb{R}^d\rightarrow\mathbb{R}$ be a continuous function that is not identically equal to zero.  Suppose further that $u$ is an odd function (ie. $u(\mathbf{x})=-u(-\mathbf{x})$).  Let $g:\mathbb{R}^d\rightarrow\mathbb{R}$ be continuous.  I'd like to prove the following: If $$ \int_{\mathbb{R}^d}u(\mathbf{x})f(g(\mathbf{x}))d\mathbf{x}=0 \qquad \forall f\in C_b(\mathbb{R})$$ then $g(\mathbf{x})=g(-\mathbf{x})$ for all $\mathbf{x}\in\mathbb{R}^d$, that is $g$ is an even function.",,"['calculus', 'multivariable-calculus', 'calculus-of-variations']"
13,Finding the area of an ellipse using Divergence Theorem,Finding the area of an ellipse using Divergence Theorem,,"As an example of Divergence Theorem, our textbook mentions finding area of an ellipse, but it isn't clear how it was derived though. Following is an excerpt from the textbook. Suppose there is an ellipse with the following equation, $$\frac{x^2}{a^2}+\frac{y^2}{b^2}=1$$ Then we could parameterize it into $$x=a\cos t, y=b\sin t \space (0\le t\le 2\pi)$$ In order to find the area enclosed by the ellipse, we use the equation $$area=\frac 12\int\mathbf r\cdot \mathbf n \space ds$$ where $$\mathbf r(x,y)=x\mathbf i+y\mathbf j$$ $$\mathbf n=(b\cos t,a\sin t)/v(t)\space (v(t)=\sqrt{b^2\cos^2t+a^2\sin^2t})$$ Hence, the area of the ellipse is $$\begin{align}  area & = \frac 12\int\mathbf r\cdot \mathbf n \space ds \\ & = \frac 12 \int_0^{2\pi} (a\cos t, b\sin t)\cdot(b\cos t, a\sin t)dt \\ & = ab\pi \end{align}$$ What I'm not following is that how we could just ignore $v(t)$ when substituting $\mathbf n$? Or have I missed something along the way? EDIT: I've actually tried throwing the entire equation (including $v(t)$) into wolframalpha, but, unfortunately, the engine couldn't return any meaning results (calculation timeout).","As an example of Divergence Theorem, our textbook mentions finding area of an ellipse, but it isn't clear how it was derived though. Following is an excerpt from the textbook. Suppose there is an ellipse with the following equation, $$\frac{x^2}{a^2}+\frac{y^2}{b^2}=1$$ Then we could parameterize it into $$x=a\cos t, y=b\sin t \space (0\le t\le 2\pi)$$ In order to find the area enclosed by the ellipse, we use the equation $$area=\frac 12\int\mathbf r\cdot \mathbf n \space ds$$ where $$\mathbf r(x,y)=x\mathbf i+y\mathbf j$$ $$\mathbf n=(b\cos t,a\sin t)/v(t)\space (v(t)=\sqrt{b^2\cos^2t+a^2\sin^2t})$$ Hence, the area of the ellipse is $$\begin{align}  area & = \frac 12\int\mathbf r\cdot \mathbf n \space ds \\ & = \frac 12 \int_0^{2\pi} (a\cos t, b\sin t)\cdot(b\cos t, a\sin t)dt \\ & = ab\pi \end{align}$$ What I'm not following is that how we could just ignore $v(t)$ when substituting $\mathbf n$? Or have I missed something along the way? EDIT: I've actually tried throwing the entire equation (including $v(t)$) into wolframalpha, but, unfortunately, the engine couldn't return any meaning results (calculation timeout).",,"['calculus', 'multivariable-calculus', 'area']"
14,Heat Equation & Fundamental Theorem of Calculus,Heat Equation & Fundamental Theorem of Calculus,,"While studying the heat equation, I ran into the following exercise: Consider conservation of thermal energy $(2)$ for any segment of a one-dimensional rod $a\leq x\leq b$. By using the fundamental theorem of calculus, $$ \frac{\partial}{\partial b}\int_a^bf(x)\,dx=f(b),\tag{1} $$ derive the heat equation $(3)$. For this exercise, we have that $$\begin{align} \frac d{dt}\int_a^be(x,t)\,dx&=\varphi(a,t)-\varphi(b,t)+\int_a^bQ(x,t)\,dx,\tag{2}\\ c\rho\frac{\partial u}{\partial t}&=\frac\partial{\partial x}\left(K_0\frac{\partial u}{\partial x}\right)+Q.\tag{3} \end{align}$$ I do not know how to tackle this problem. Any hint would be greatly appreciated. Also, I do not understand why $(1)$ has a partial derivative when $f$ is a function of a single variable. Edit 1 : Following celtschk's advice, I managed to rewrite $(2)$ as $$ \frac d{dt}\int_a^be(x,t)\,dx=\varphi(a,t)-\frac\partial{\partial b}\int_a^b\varphi(x,t)\,dx+\int_a^bQ(x,t)\,dx. $$ Now, I suppose that I must get rid of the $\partial/\partial b$ and subtract $\varphi(a,t)$? Edit 2 One could also rewrite $(2)$ as $$ \frac d{dt}\int_a^be(x,t)\,dx= - \frac{\partial}{\partial a} \int_a^b \varphi(x,t) dx -\frac\partial{\partial b}\int_a^b\varphi(x,t)\,dx+\int_a^bQ(x,t)\,dx. $$ using the fact that $ \frac{\partial}{\partial a} \int_b^a \varphi(x,t) dx$. Is there a way to collect together the two terms with partial derivatives in order to get the desired $\int_a^b \frac{d \varphi}{dx} dx $ term in the heat equation?","While studying the heat equation, I ran into the following exercise: Consider conservation of thermal energy $(2)$ for any segment of a one-dimensional rod $a\leq x\leq b$. By using the fundamental theorem of calculus, $$ \frac{\partial}{\partial b}\int_a^bf(x)\,dx=f(b),\tag{1} $$ derive the heat equation $(3)$. For this exercise, we have that $$\begin{align} \frac d{dt}\int_a^be(x,t)\,dx&=\varphi(a,t)-\varphi(b,t)+\int_a^bQ(x,t)\,dx,\tag{2}\\ c\rho\frac{\partial u}{\partial t}&=\frac\partial{\partial x}\left(K_0\frac{\partial u}{\partial x}\right)+Q.\tag{3} \end{align}$$ I do not know how to tackle this problem. Any hint would be greatly appreciated. Also, I do not understand why $(1)$ has a partial derivative when $f$ is a function of a single variable. Edit 1 : Following celtschk's advice, I managed to rewrite $(2)$ as $$ \frac d{dt}\int_a^be(x,t)\,dx=\varphi(a,t)-\frac\partial{\partial b}\int_a^b\varphi(x,t)\,dx+\int_a^bQ(x,t)\,dx. $$ Now, I suppose that I must get rid of the $\partial/\partial b$ and subtract $\varphi(a,t)$? Edit 2 One could also rewrite $(2)$ as $$ \frac d{dt}\int_a^be(x,t)\,dx= - \frac{\partial}{\partial a} \int_a^b \varphi(x,t) dx -\frac\partial{\partial b}\int_a^b\varphi(x,t)\,dx+\int_a^bQ(x,t)\,dx. $$ using the fact that $ \frac{\partial}{\partial a} \int_b^a \varphi(x,t) dx$. Is there a way to collect together the two terms with partial derivatives in order to get the desired $\int_a^b \frac{d \varphi}{dx} dx $ term in the heat equation?",,"['multivariable-calculus', 'partial-differential-equations']"
15,"If $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ has coordinates $f^1 \ldots f^m$, and each $f^i$ is differentiable at $0$...","If  has coordinates , and each  is differentiable at ...",f: \mathbb{R}^n \rightarrow \mathbb{R}^m f^1 \ldots f^m f^i 0,"...then does it follow that $f$ is differentiable at 0? My motivation for asking this is as follows: in Spivak's Calculus on Manifolds, in theorem 2.9, he uses this with the additional condition that each $f^i$ is continuously differentiable in a nbh of 0, to conclude that $f$ is differentiable and I don't think is necessary. Namely, if each $f^i$ has derivative $Df^i$, I claim the matrix with $i^{th}$ row $Df^i$ will serve as $Df$. Indeed, let $v_j$ be a sequence tending to 0 in $\mathbb{R}^n$,  we have (by the triangle inequality, if you wish)$$\frac{| f(v_j) - f(0) - \sum_i Df^i(v) |}{|v_j|} \leqslant \frac{\sum_i |f^i(v_j) - f^i(0) - Df^i(v_j)|}{|v_j|}$$Taking the limit as $j \rightarrow \infty$, each summand goes to 0 by the differentiability of $Df^i$ (and there's only $m$ of them), hence the limit is 0. Is this wrong? Thanks in advance! EDIT: btw, conditional on the above proof being right and/or the claim being right, does anyone know maybe what Spivak was going for?","...then does it follow that $f$ is differentiable at 0? My motivation for asking this is as follows: in Spivak's Calculus on Manifolds, in theorem 2.9, he uses this with the additional condition that each $f^i$ is continuously differentiable in a nbh of 0, to conclude that $f$ is differentiable and I don't think is necessary. Namely, if each $f^i$ has derivative $Df^i$, I claim the matrix with $i^{th}$ row $Df^i$ will serve as $Df$. Indeed, let $v_j$ be a sequence tending to 0 in $\mathbb{R}^n$,  we have (by the triangle inequality, if you wish)$$\frac{| f(v_j) - f(0) - \sum_i Df^i(v) |}{|v_j|} \leqslant \frac{\sum_i |f^i(v_j) - f^i(0) - Df^i(v_j)|}{|v_j|}$$Taking the limit as $j \rightarrow \infty$, each summand goes to 0 by the differentiability of $Df^i$ (and there's only $m$ of them), hence the limit is 0. Is this wrong? Thanks in advance! EDIT: btw, conditional on the above proof being right and/or the claim being right, does anyone know maybe what Spivak was going for?",,['multivariable-calculus']
16,Integral over the $\mathcal{S}^{n-1}$ sphere,Integral over the  sphere,\mathcal{S}^{n-1},"I have been running into the following integral again and again: Let $S^{n-1}= \{x \in \mathbb{R}^{n} \: | \: ||x||=1 \}$ and let $\lambda_{S^{n-1}}$ denote the surface measure over $S^{n-1}$ as defined in Stroock (2000) page 86. Consider a fixed symmetric, positive definite matrix $Q$ of dimension $n \times n$, and a fixed scalar $a\in \mathbb{R}_{+}$ Question 1) Do you know if there is a closed form solution for the integral: $$\int_{S^{n-1}} \exp\Big(a \omega'Q\omega \Big) \lambda_{S^{n-1}} (d \omega) $$ When $n=2$, I can express this integral as a modified Bessel function of the first kind $I_{v}(x)$ with $v=0$ evaluated at the eigenvalues of $Q$. Question 2) Any suggestion about good numerical method for solving this integral? Thanks! *Stroock (2000) ""A concise introduction to the theory of integration""","I have been running into the following integral again and again: Let $S^{n-1}= \{x \in \mathbb{R}^{n} \: | \: ||x||=1 \}$ and let $\lambda_{S^{n-1}}$ denote the surface measure over $S^{n-1}$ as defined in Stroock (2000) page 86. Consider a fixed symmetric, positive definite matrix $Q$ of dimension $n \times n$, and a fixed scalar $a\in \mathbb{R}_{+}$ Question 1) Do you know if there is a closed form solution for the integral: $$\int_{S^{n-1}} \exp\Big(a \omega'Q\omega \Big) \lambda_{S^{n-1}} (d \omega) $$ When $n=2$, I can express this integral as a modified Bessel function of the first kind $I_{v}(x)$ with $v=0$ evaluated at the eigenvalues of $Q$. Question 2) Any suggestion about good numerical method for solving this integral? Thanks! *Stroock (2000) ""A concise introduction to the theory of integration""",,"['real-analysis', 'multivariable-calculus', 'simulation']"
17,"Find extremes of function $f(x,y,z) = x^2y + y^2z + x - z$",Find extremes of function,"f(x,y,z) = x^2y + y^2z + x - z","I am preparing for an exam tuesday morning and I would like to ask you, if someone could please review my solution for the following excercise. I don't have the correct answer so I am unable to check whether it is OK. Find the extremes of polynomial function $f(x,y,z) = x^2y + y^2z + x - z$ So here is how i solved it: $$ \begin{align*} f'_x &= 2xy + 1 \hspace{10mm} f'_y = x^2 + 2yz \hspace{10mm}f'_z = y^2 - 1\\ \end{align*} $$ $$ \begin{align*} f''_{xx} = 2y \hspace{10mm} f''_{xy} = 2x \hspace{10mm} f''_{xz} = 2y\\ f''_yy = 2z \hspace{10mm} f''_{yz} = 2y \hspace{10mm} f''_{zz} = 0\\ \end{align*} $$ So now I need to solve this system: $$ \begin{align*} 2xy + 1 &= 0\\ x^2 + 2yz &= 0\\ y^2 -1 &= 0 \end{align*} $$ So I get the points: $A = [-\frac{1}{2}, 1, -\frac{1}{8}]$ $B = [\frac{1}{2}, -1, \frac{1}{8}]$ For point A: $$ \begin{align*} f''{xx}(-\frac{1}{2}, 1, -\frac{1}{8}) = 2 \hspace{10mm} f''{xy}(-\frac{1}{2}, 1, -\frac{1}{8}) = -1 \hspace{10mm} f''{xz}(-\frac{1}{2}, 1, -\frac{1}{8}) = 0\\ f''{yy}(-\frac{1}{2}, 1, -\frac{1}{8}) = -\frac{1}{4} \hspace{10mm} f''{yz}(-\frac{1}{2}, 1, -\frac{1}{8}) = 2 \hspace{10mm} f''{zz}(-\frac{1}{2}, 1, -\frac{1}{8}) = 0\\ \end{align*} $$ $$ H =    \begin{pmatrix}    2 & -1 & 0 \\    -1 & -\frac{1}{4} & 2 \\    0  & 2  & 0    \end{pmatrix} $$ Subdeterminants are: $2, -\frac{3}{2}, -8$ Therefore we don't know whether point A is maxima or minima. For point B: $$ \begin{align*} f''{xx}(-\frac{1}{2}, 1, -\frac{1}{8}) = -2 \hspace{10mm} f''{xy}(-\frac{1}{2}, 1, -\frac{1}{8}) = 1 \hspace{10mm} f''{xz}(-\frac{1}{2}, 1, -\frac{1}{8}) = 0\\ f''{yy}(-\frac{1}{2}, 1, -\frac{1}{8}) = \frac{1}{4} \hspace{10mm} f''{yz}(-\frac{1}{2}, 1, -\frac{1}{8}) = -2 \hspace{10mm} f''{zz}(-\frac{1}{2}, 1, -\frac{1}{8}) = 0\\ \end{align*} $$ $$ H =    \begin{pmatrix}    -2 & 1 & 0 \\    1 & \frac{1}{4} & -2 \\    0  & -2  & 0    \end{pmatrix} $$ Subdeterminants are: $2, -\frac{3}{2}, 8$ Therefore point B is a local maximum. So please - is it correct? Thanks in advance.","I am preparing for an exam tuesday morning and I would like to ask you, if someone could please review my solution for the following excercise. I don't have the correct answer so I am unable to check whether it is OK. Find the extremes of polynomial function $f(x,y,z) = x^2y + y^2z + x - z$ So here is how i solved it: $$ \begin{align*} f'_x &= 2xy + 1 \hspace{10mm} f'_y = x^2 + 2yz \hspace{10mm}f'_z = y^2 - 1\\ \end{align*} $$ $$ \begin{align*} f''_{xx} = 2y \hspace{10mm} f''_{xy} = 2x \hspace{10mm} f''_{xz} = 2y\\ f''_yy = 2z \hspace{10mm} f''_{yz} = 2y \hspace{10mm} f''_{zz} = 0\\ \end{align*} $$ So now I need to solve this system: $$ \begin{align*} 2xy + 1 &= 0\\ x^2 + 2yz &= 0\\ y^2 -1 &= 0 \end{align*} $$ So I get the points: $A = [-\frac{1}{2}, 1, -\frac{1}{8}]$ $B = [\frac{1}{2}, -1, \frac{1}{8}]$ For point A: $$ \begin{align*} f''{xx}(-\frac{1}{2}, 1, -\frac{1}{8}) = 2 \hspace{10mm} f''{xy}(-\frac{1}{2}, 1, -\frac{1}{8}) = -1 \hspace{10mm} f''{xz}(-\frac{1}{2}, 1, -\frac{1}{8}) = 0\\ f''{yy}(-\frac{1}{2}, 1, -\frac{1}{8}) = -\frac{1}{4} \hspace{10mm} f''{yz}(-\frac{1}{2}, 1, -\frac{1}{8}) = 2 \hspace{10mm} f''{zz}(-\frac{1}{2}, 1, -\frac{1}{8}) = 0\\ \end{align*} $$ $$ H =    \begin{pmatrix}    2 & -1 & 0 \\    -1 & -\frac{1}{4} & 2 \\    0  & 2  & 0    \end{pmatrix} $$ Subdeterminants are: $2, -\frac{3}{2}, -8$ Therefore we don't know whether point A is maxima or minima. For point B: $$ \begin{align*} f''{xx}(-\frac{1}{2}, 1, -\frac{1}{8}) = -2 \hspace{10mm} f''{xy}(-\frac{1}{2}, 1, -\frac{1}{8}) = 1 \hspace{10mm} f''{xz}(-\frac{1}{2}, 1, -\frac{1}{8}) = 0\\ f''{yy}(-\frac{1}{2}, 1, -\frac{1}{8}) = \frac{1}{4} \hspace{10mm} f''{yz}(-\frac{1}{2}, 1, -\frac{1}{8}) = -2 \hspace{10mm} f''{zz}(-\frac{1}{2}, 1, -\frac{1}{8}) = 0\\ \end{align*} $$ $$ H =    \begin{pmatrix}    -2 & 1 & 0 \\    1 & \frac{1}{4} & -2 \\    0  & -2  & 0    \end{pmatrix} $$ Subdeterminants are: $2, -\frac{3}{2}, 8$ Therefore point B is a local maximum. So please - is it correct? Thanks in advance.",,"['multivariable-calculus', 'optimization']"
18,Jacobian of a function mapping vectors to vectors,Jacobian of a function mapping vectors to vectors,,"I believe the problem of trying to find the Jacobian of the following function highlights a lack of understanding of some concept on my part. I was hoping someone could either provide specific advice about solving this problem, or computing Jacobians in general. Consider the mapping $h : \mathbb{R}^n \rightarrow \mathbb{R}^n$ where the domain is length-$n$ column vectors and the range length-$n$ row vectors (or a transposed vector, if you like). The function is $$h(x) = \frac{\eta v' + (M x)'}{(\eta + u'x)^2},$$ where the constants $v$ and $u$ are (column) vectors, $\eta$ is a scalar, and $M$ is a square matrix. So far as I know, the quotient rule for vectors is $$\nabla\left(\frac{f}{g}\right) = \frac{g\nabla f - f \nabla g}{g^2}$$ and \begin{align*} \nabla f &= M'\\ \nabla g &= 2(\eta + u'x) u' \end{align*} Putting it all together, I get $$\nabla h = \frac{(\eta + u'x)^2 M' - [\eta v' + (M x)'] 2(\eta + u'x) u'}{(\eta + u'x)^4}.$$ This expression is clearly not right, and to see why evaluate the Jacobian at $x = \mathbf{0}$: $$\nabla h(0) = \frac{\eta^2 M' - 2\eta^2 v' u'}{\eta^4}$$ The resulting expression should be a $n \times n$ matrix, but in the second term we have two (row) vectors multiplied by one another. It seems likely there should be some sort of outer product here, but I'm not sure where my math is going wrong. Any help you can provide is greatly appreciated.","I believe the problem of trying to find the Jacobian of the following function highlights a lack of understanding of some concept on my part. I was hoping someone could either provide specific advice about solving this problem, or computing Jacobians in general. Consider the mapping $h : \mathbb{R}^n \rightarrow \mathbb{R}^n$ where the domain is length-$n$ column vectors and the range length-$n$ row vectors (or a transposed vector, if you like). The function is $$h(x) = \frac{\eta v' + (M x)'}{(\eta + u'x)^2},$$ where the constants $v$ and $u$ are (column) vectors, $\eta$ is a scalar, and $M$ is a square matrix. So far as I know, the quotient rule for vectors is $$\nabla\left(\frac{f}{g}\right) = \frac{g\nabla f - f \nabla g}{g^2}$$ and \begin{align*} \nabla f &= M'\\ \nabla g &= 2(\eta + u'x) u' \end{align*} Putting it all together, I get $$\nabla h = \frac{(\eta + u'x)^2 M' - [\eta v' + (M x)'] 2(\eta + u'x) u'}{(\eta + u'x)^4}.$$ This expression is clearly not right, and to see why evaluate the Jacobian at $x = \mathbf{0}$: $$\nabla h(0) = \frac{\eta^2 M' - 2\eta^2 v' u'}{\eta^4}$$ The resulting expression should be a $n \times n$ matrix, but in the second term we have two (row) vectors multiplied by one another. It seems likely there should be some sort of outer product here, but I'm not sure where my math is going wrong. Any help you can provide is greatly appreciated.",,['multivariable-calculus']
19,Can you critique my exposition of $\int_{-\infty}^{+\infty}e^{-x^2}dx = \sqrt \pi$?,Can you critique my exposition of ?,\int_{-\infty}^{+\infty}e^{-x^2}dx = \sqrt \pi,"Prove $$\int_{-\infty}^{+\infty}e^{-x^2}dx = \sqrt \pi$$ using Fubini's Theorem. My solution is below. Proof is Correct. What I want to know is : Is it well written? How could the writing be improved, made clearer, or more rigorous? Solution: We first show that $$\int_{-\infty}^{+\infty}\left [e^{-y^2}\int_{-\infty}^{+\infty}e^{-x^2}dx \right ]dy = \pi.$$ By Fubini's Theorem, $$\int_{-\infty}^{+\infty}\left [e^{-y^2}\int_{-\infty}^{+\infty}e^{-x^2}dx \right ]dy = \iint_{(x,y) \in \mathbb R^2}e^{-(x^2+y^2)}dxdy$$ which, by changing to spherical coordinates, equals $$\begin{align*}\int_{0}^{2\pi}\int_0^\infty re^{-r^2}dr d \theta &= \int_{0}^{2\pi} \Biggr|_0^\infty \frac {-e^{-r^2}}{2} d\theta\\ &=\pi.\end{align*}$$ Now, by linearity of integrals, $$\int_{-\infty}^{+\infty}\left [e^{-y^2}\int_{-\infty}^{+\infty}e^{-x^2}dx \right ]dy = \left [ \int_{-\infty}^{+\infty}e^{-x^2}dx  \right ] \left [ \int_{-\infty}^{+\infty}e^{-y^2}dy \right ]$$ and since $e^{-x^2} > 0$ for all $x$ , $$\int_{-\infty}^{+\infty}e^{-x^2} dx = \sqrt \pi$$ as desired.","Prove using Fubini's Theorem. My solution is below. Proof is Correct. What I want to know is : Is it well written? How could the writing be improved, made clearer, or more rigorous? Solution: We first show that By Fubini's Theorem, which, by changing to spherical coordinates, equals Now, by linearity of integrals, and since for all , as desired.","\int_{-\infty}^{+\infty}e^{-x^2}dx = \sqrt \pi \int_{-\infty}^{+\infty}\left [e^{-y^2}\int_{-\infty}^{+\infty}e^{-x^2}dx \right ]dy = \pi. \int_{-\infty}^{+\infty}\left [e^{-y^2}\int_{-\infty}^{+\infty}e^{-x^2}dx \right ]dy = \iint_{(x,y) \in \mathbb R^2}e^{-(x^2+y^2)}dxdy \begin{align*}\int_{0}^{2\pi}\int_0^\infty re^{-r^2}dr d \theta &= \int_{0}^{2\pi} \Biggr|_0^\infty \frac {-e^{-r^2}}{2} d\theta\\ &=\pi.\end{align*} \int_{-\infty}^{+\infty}\left [e^{-y^2}\int_{-\infty}^{+\infty}e^{-x^2}dx \right ]dy = \left [ \int_{-\infty}^{+\infty}e^{-x^2}dx  \right ] \left [ \int_{-\infty}^{+\infty}e^{-y^2}dy \right ] e^{-x^2} > 0 x \int_{-\infty}^{+\infty}e^{-x^2} dx = \sqrt \pi","['real-analysis', 'multivariable-calculus', 'solution-verification', 'proof-writing', 'fubini-tonelli-theorems']"
20,"On a certain ""anticurl"" operator","On a certain ""anticurl"" operator",,"I recently found myself curious what explicit formula I would get if I traced through the de Rham cohomology proof that if $\mathbf{F}$ is a vector field defined on all of $\mathbb{R}^3$ which has divergence 0, then $\mathbf{F}$ is the curl of some other vector field.  So, tracing through the proof, what I did starting with such a divergence-free vector field was: Convert $\mathbf{F}$ to a 2-form. Pull back along the contraction homotopy $H : \mathbb{R}^3 \times [0, 1] \to \mathbb{R}^3$ , $(x, y, z, t) \mapsto (tx, ty, tz)$ . Apply the integral operator $\mathfrak{I}$ which intuitively sends $dt \wedge \omega \mapsto \int_{t=0}^{t=1} \omega \, dt$ and for other $\omega$ without a $dt$ , only with $dx,dy,dz$ , $\omega \mapsto 0$ . Convert the resulting 1-form back to a vector field. After working through this, the formula I eventually got could be summarized as: $$(\operatorname{anticurl} \mathbf{F})(\mathbf{x}) := \left[ \int_0^1 t \mathbf{F}(t \mathbf{x})\,dt \right] \times \mathbf{x}.$$ This has the expected property that $\operatorname{div} \mathbf{F} = 0 \implies \mathbf{F} = \operatorname{curl} (\operatorname{anticurl} \mathbf{F})$ .  (And trying it out on for example $\mathbf{F}(x, y, z) = (y^a z^b, 0, 0)$ , I get $(\operatorname{anticurl} \mathbf{F})(x, y, z) = \frac{1}{a+b+2} (0, -y^a z^{b+1}, y^{a+1} z^b)$ which does have the expected curl.) It is also interesting that this $\operatorname{anticurl}$ is $\mathbb{R}$ -linear and in addition it respects rotations about axes through the origin. So now, what I was curious about was: this explicit partial inverse of curl seems like something that very likely would have shown up before.  If there's a standard name for it, that would be interesting to know, or otherwise it would be good to see an example usage of this formula in a textbook or other reference more on the introductory than research level.","I recently found myself curious what explicit formula I would get if I traced through the de Rham cohomology proof that if is a vector field defined on all of which has divergence 0, then is the curl of some other vector field.  So, tracing through the proof, what I did starting with such a divergence-free vector field was: Convert to a 2-form. Pull back along the contraction homotopy , . Apply the integral operator which intuitively sends and for other without a , only with , . Convert the resulting 1-form back to a vector field. After working through this, the formula I eventually got could be summarized as: This has the expected property that .  (And trying it out on for example , I get which does have the expected curl.) It is also interesting that this is -linear and in addition it respects rotations about axes through the origin. So now, what I was curious about was: this explicit partial inverse of curl seems like something that very likely would have shown up before.  If there's a standard name for it, that would be interesting to know, or otherwise it would be good to see an example usage of this formula in a textbook or other reference more on the introductory than research level.","\mathbf{F} \mathbb{R}^3 \mathbf{F} \mathbf{F} H : \mathbb{R}^3 \times [0, 1] \to \mathbb{R}^3 (x, y, z, t) \mapsto (tx, ty, tz) \mathfrak{I} dt \wedge \omega \mapsto \int_{t=0}^{t=1} \omega \, dt \omega dt dx,dy,dz \omega \mapsto 0 (\operatorname{anticurl} \mathbf{F})(\mathbf{x}) := \left[ \int_0^1 t \mathbf{F}(t \mathbf{x})\,dt \right] \times \mathbf{x}. \operatorname{div} \mathbf{F} = 0 \implies \mathbf{F} = \operatorname{curl} (\operatorname{anticurl} \mathbf{F}) \mathbf{F}(x, y, z) = (y^a z^b, 0, 0) (\operatorname{anticurl} \mathbf{F})(x, y, z) = \frac{1}{a+b+2} (0, -y^a z^{b+1}, y^{a+1} z^b) \operatorname{anticurl} \mathbb{R}","['multivariable-calculus', 'reference-request', 'de-rham-cohomology', 'curl']"
21,Is this inequality a consequence of strong convexity?,Is this inequality a consequence of strong convexity?,,"Let $V:\mathbb R^2\to \mathbb R$ be a strongly convex , smooth function. By definition there exists $\alpha>0$ such that $$x^T \textsf{Hess} V(\xi) x \geq \alpha\,|x|^2$$ for all $x,\xi\in\mathbb R^2$ . $V$ has a unique minimum point: assume it is at $x=0$ . As a consequence it is easy to check that $$ x\cdot \nabla V(x) \geq \alpha\,|x|^2$$ for all $x\in\mathbb R^2$ . Just Taylor expand $\nabla V(x)= \textsf{Hess} V(\xi)\,x\,$ for some $\xi\in(0,x)$ . My question : assume also that derivatives of $V$ of any order have polynomial growth. Are there $\tilde\alpha>0$ and a non-negative polynomial function $p$ such that $$ x_1\cdot \partial_1 V(x_1,x_2) \geq \tilde\alpha\,|x_1|^2 - p(|x_2|)$$ for all $x=(x_1,x_2)\in\mathbb R^2$ ? This seems to me a sort of convexity condition restricted to the first variable only, keeping the second one fixed. If $V$ is a polynomial , the desired inequality is true. What about the general case? I don't manage to prove it nor to find a counter-example.","Let be a strongly convex , smooth function. By definition there exists such that for all . has a unique minimum point: assume it is at . As a consequence it is easy to check that for all . Just Taylor expand for some . My question : assume also that derivatives of of any order have polynomial growth. Are there and a non-negative polynomial function such that for all ? This seems to me a sort of convexity condition restricted to the first variable only, keeping the second one fixed. If is a polynomial , the desired inequality is true. What about the general case? I don't manage to prove it nor to find a counter-example.","V:\mathbb R^2\to \mathbb R \alpha>0 x^T \textsf{Hess} V(\xi) x \geq \alpha\,|x|^2 x,\xi\in\mathbb R^2 V x=0  x\cdot \nabla V(x) \geq \alpha\,|x|^2 x\in\mathbb R^2 \nabla V(x)= \textsf{Hess} V(\xi)\,x\, \xi\in(0,x) V \tilde\alpha>0 p  x_1\cdot \partial_1 V(x_1,x_2) \geq \tilde\alpha\,|x_1|^2 - p(|x_2|) x=(x_1,x_2)\in\mathbb R^2 V","['real-analysis', 'multivariable-calculus', 'convex-analysis']"
22,Interesting double integral $\int_{0}^{1}\int_{0}^{1}\frac{\log{(1+x^3+y^3)}-\log{(xy)}}{1+x^3+y^3}dxdy$,Interesting double integral,\int_{0}^{1}\int_{0}^{1}\frac{\log{(1+x^3+y^3)}-\log{(xy)}}{1+x^3+y^3}dxdy,"I found this integral in a FB page: $$I=\int_{0}^{1}\int_{0}^{1}\frac{\log{(1+x^3+y^3)}-\log{(xy)}}{1+x^3+y^3}dxdy$$ I am trying to evaluate it, but it is hard. My attemps: convert by using substitution $x=r\cos^{2/3}({\theta}), y=r\sin^{2/3}({\theta})$ then it becomes: $$I=\frac{4}{3}\int_{0}^{\frac{\pi}{4}}\int_{0}^{\frac{1}{\cos^{2/3}({\theta})}}\frac{\log{(1+r^3)}-\log{(r^2\cos^{2/3}({\theta})\sin^{2/3}({\theta}))}}{(1+r^3)\cos^{1/3}({\theta})\sin^{1/3}({\theta})}dxdy$$ And it seems impossible due to the antiderivative is a mess. I also try to transform: $x^3+y^3=(x+y)^3-3xy(x+y)$ so i set $u=x+y, v=xy$ but it is still hard. Another try, because of symmetry then the integral can be rewritten as: $$I=\int_{0}^{1}\int_{0}^{1}\frac{\log{(1+x^3+y^3)}-\log{(x^2)}}{1+x^3+y^3}dxdy$$ But it doesn't help much. I also add more variable like this: $$I=\int_{0}^{1}\int_{0}^{1}\int_{0}^{\infty}(\log{(1+x^3+y^3)}-\log{(x^2)})e^{-z(1+x^3+y^3)}dxdydz$$ And it leads to something messier. So, I need some helps from every one, thank you for your time.","I found this integral in a FB page: I am trying to evaluate it, but it is hard. My attemps: convert by using substitution then it becomes: And it seems impossible due to the antiderivative is a mess. I also try to transform: so i set but it is still hard. Another try, because of symmetry then the integral can be rewritten as: But it doesn't help much. I also add more variable like this: And it leads to something messier. So, I need some helps from every one, thank you for your time.","I=\int_{0}^{1}\int_{0}^{1}\frac{\log{(1+x^3+y^3)}-\log{(xy)}}{1+x^3+y^3}dxdy x=r\cos^{2/3}({\theta}), y=r\sin^{2/3}({\theta}) I=\frac{4}{3}\int_{0}^{\frac{\pi}{4}}\int_{0}^{\frac{1}{\cos^{2/3}({\theta})}}\frac{\log{(1+r^3)}-\log{(r^2\cos^{2/3}({\theta})\sin^{2/3}({\theta}))}}{(1+r^3)\cos^{1/3}({\theta})\sin^{1/3}({\theta})}dxdy x^3+y^3=(x+y)^3-3xy(x+y) u=x+y, v=xy I=\int_{0}^{1}\int_{0}^{1}\frac{\log{(1+x^3+y^3)}-\log{(x^2)}}{1+x^3+y^3}dxdy I=\int_{0}^{1}\int_{0}^{1}\int_{0}^{\infty}(\log{(1+x^3+y^3)}-\log{(x^2)})e^{-z(1+x^3+y^3)}dxdydz",['multivariable-calculus']
23,"Showing that if a plane curve $L$ is always perpendicular to a fixed vector, then $L$ is a line","Showing that if a plane curve  is always perpendicular to a fixed vector, then  is a line",L L,"I am working on the problem Suppose $f : \mathbb{R}^2 \to \mathbb{R}$ is a differentiable function whose gradient is nowhere zero and satisfies $\frac {\partial f}{\partial x} = 2 \frac {\partial f}{\partial y}$ everywhere. Find the level curves of $f$ . Source: Multivariable Mathematics by Ted Shifrin, page 120. I think I've mostly solved this, but I have several doubts about my solution. Could you please enlighten me about the dubious steps? Thank you! Partial Solution We see $$\nabla f =  [ \frac {\partial f}{\partial x} \ \ \ \frac {\partial f}{\partial y} ]^T = \frac {\partial f}{\partial y} [2 \ \ \ 1]^T$$ and $\frac {\partial f}{\partial y}$ is never $0$ .  Let $L$ be the level curve $L = \{(x, y)|f(x, y) = c \},$ and write it in parametric form $[x(t), y(t)]^T.$ Since the gradient is perpendicular to the level curve, we must have $$\forall t: \ \ 2x'(t) + y'(t) = 0$$ so $y(t) = - \frac 12 x(t) + k$ for some constant $k$ . Thus, we see that $L$ must be a $\textit{subset}$ of a line. My questions are: Can we prove that $L$ is indeed an entire, infinite line? How can we justify that we can write the set $L$ as the image of $[x(t) \ \ \ y(t)]$ for some functions $x, y$ ? Thank you!","I am working on the problem Suppose is a differentiable function whose gradient is nowhere zero and satisfies everywhere. Find the level curves of . Source: Multivariable Mathematics by Ted Shifrin, page 120. I think I've mostly solved this, but I have several doubts about my solution. Could you please enlighten me about the dubious steps? Thank you! Partial Solution We see and is never .  Let be the level curve and write it in parametric form Since the gradient is perpendicular to the level curve, we must have so for some constant . Thus, we see that must be a of a line. My questions are: Can we prove that is indeed an entire, infinite line? How can we justify that we can write the set as the image of for some functions ? Thank you!","f : \mathbb{R}^2 \to \mathbb{R} \frac {\partial f}{\partial x} = 2 \frac {\partial f}{\partial y} f \nabla f =  [ \frac {\partial f}{\partial x} \ \ \ \frac {\partial f}{\partial y} ]^T = \frac {\partial f}{\partial y} [2 \ \ \ 1]^T \frac {\partial f}{\partial y} 0 L L = \{(x, y)|f(x, y) = c \}, [x(t), y(t)]^T. \forall t: \ \ 2x'(t) + y'(t) = 0 y(t) = - \frac 12 x(t) + k k L \textit{subset} L L [x(t) \ \ \ y(t)] x, y","['real-analysis', 'multivariable-calculus', 'differential-geometry']"
24,Coordinate Vector Fields,Coordinate Vector Fields,,"I am reading the following notes about coordinate systems in $\mathbb{R}^n$ and it lists four properties that coordinate vector fields should have. For the $(x,y)$ system and the coordinate vectors $\mathbf{i},\mathbf{j}$ , they are: They are orthogonal. They are unit vectors. $\mathbf{i}$ points in the direction in which $x$ increases and $y$ is constant, and $\mathbf{j}$ points in the direction in which $y$ increases and $x$ is constant. $\mathbf{i}$ and $\mathbf{j}$ represent differentiation of a function by $x$ and $y$ . What this means is that if $f$ is a differentiable function, then $$ \frac{\partial f}{\partial x} = D_{\mathbf{i}} f \text{ and } \frac{\partial f}{\partial y} = D_{\mathbf{j}} f $$ In the first paragraph on the second page, it's written Unfortunately, for an arbitrary coordinate system it is impossible to satisfy all four of the properties above. (It is a fairly deep theorem in differential geometry that the only coordinate systems with coordinate vector fields that satisfy all four properties are Euclidean coordinates.) My question is, which theorem is referenced here? Is it related to the necessity of having a vanishing Lie bracket, i.e two linearly independent vector fields $X$ and $Y$ must satisfy $[X, Y] = 0$ in order to find coordinates with $X = x_u$ and $Y = x_v$ ? So is it related to symmetry of higher-order derivatives? Thanks.","I am reading the following notes about coordinate systems in and it lists four properties that coordinate vector fields should have. For the system and the coordinate vectors , they are: They are orthogonal. They are unit vectors. points in the direction in which increases and is constant, and points in the direction in which increases and is constant. and represent differentiation of a function by and . What this means is that if is a differentiable function, then In the first paragraph on the second page, it's written Unfortunately, for an arbitrary coordinate system it is impossible to satisfy all four of the properties above. (It is a fairly deep theorem in differential geometry that the only coordinate systems with coordinate vector fields that satisfy all four properties are Euclidean coordinates.) My question is, which theorem is referenced here? Is it related to the necessity of having a vanishing Lie bracket, i.e two linearly independent vector fields and must satisfy in order to find coordinates with and ? So is it related to symmetry of higher-order derivatives? Thanks.","\mathbb{R}^n (x,y) \mathbf{i},\mathbf{j} \mathbf{i} x y \mathbf{j} y x \mathbf{i} \mathbf{j} x y f 
\frac{\partial f}{\partial x} = D_{\mathbf{i}} f \text{ and } \frac{\partial f}{\partial y} = D_{\mathbf{j}} f
 X Y [X, Y] = 0 X = x_u Y = x_v","['multivariable-calculus', 'differential-geometry', 'partial-differential-equations']"
25,Finding the volume of a pseudosphere that has been parametrised in $\theta$ and $t$,Finding the volume of a pseudosphere that has been parametrised in  and,\theta t,"I've got a problem calculating the volume of the top half of a pseudosphere. The pseudosphere is parametrised by $$\Phi(t,\theta) = \Big ( \frac{\cos(\theta)}{\cosh(t)}, \frac{\sin(\theta)}{\cosh(t)}, t-\tanh(t)\Big)$$ with $0\le t$ and $0\le \theta \lt 2\pi$ So the volume I'm trying to find is that between the $x-y$ axis and the surface of the inside of the pseudosphere. Now, we've been given that the formula for any region $V$ in $\mathbb R^3$ is $$\iiint_V r\tanh^2(t) \ \mathrm dr \ \mathrm d\theta \ \mathrm d t$$ based on the change of variables $x = r\cos(\theta) \ , \ y = r\sin(\theta) \ $ and $ z = t - \tanh(t)$ Since $t\ge 0$ we have $0 \lt r \le 1$ because when $t=0$ is put into $x = \frac{\cos(\theta)}{\cosh(t)}$ we get $x=\cos(\theta)$ and similarly when $t=0$ is put into y = $\frac{\sin(\theta)}{\cosh(t)}$ we get $y = \sin(\theta)$ Then, $$x^2 + y^2 = \cos^2(t) + \sin^2(t) = 1$$ so the radius at $t=0$ is $1$ , and as $t \to \infty$ we get $\cosh(t) \to \infty$ so both $x$ and $y$ approach $0$ so the radius approaches $0$ . So the integral becomes $$\lim_{b \to \infty}\int_{t=0}^{t=b} \int_{\theta=0}^{\theta = 2\pi} \int_{r=0}^{r=1} r\tanh^2(t) \ \mathrm dr \ \mathrm d\theta \ \mathrm d t$$ $$ = \pi \lim_{b \to \infty}\int_{t=0}^{t=b} \tanh^2(t) \ \mathrm d t$$ $$ = \pi \lim_{b \to \infty}\int_{t=0}^{t=b} 1-\text{sech}^2(t) \ \mathrm d t $$ $$ = \pi \lim_{b \to \infty} \Bigg [ t - \tanh(t)\Bigg]_{t=0}^{t=b}$$ which does not converge. I got close by multiplying the inside of the integral by $$\lvert \vec T_t \times \vec T_\theta \rvert = \frac{\sinh(t)}{\cosh^2(t)}$$ so the integral becomes $$\lim_{b \to \infty}\int_{t=0}^{t=b} \int_{\theta=0}^{\theta = 2\pi} \int_{r=0}^{r=1} r\tanh^2(t)\frac{\sinh(t)}{\cosh^2(t)} \ \mathrm dr \ \mathrm d\theta \ \mathrm d t$$ $$ = \lim_{b \to \infty}\int_{t=0}^{t=b} \int_{\theta=0}^{\theta = 2\pi} \int_{r=0}^{r=1} r\frac{\sinh^3(t)}{\cosh^4(t)} \ \mathrm dr \ \mathrm d\theta \ \mathrm d t$$ $$ = \pi \lim_{b \to \infty}\int_{t=0}^{t=b} \frac{\sinh(t)\sinh^2(t)}{\cosh^4(t)}  \ \mathrm d t$$ $$ = \pi \lim_{b \to \infty}\int_{t=0}^{t=b} \frac{\sinh(t)(\cosh^2(t)-1)}{\cosh^4(t)}  \ \mathrm d t$$ make a u-sub: $$u = \cosh(t) \implies \mathrm du = \sinh(t) \mathrm dt$$ with $ u(b) = \cosh(b)$ and $u(0) = 1$ so the integral becomes $$ \pi \lim_{b \to \infty}\int_{u=1}^{u=\cosh(b)} \frac{u^2-1}{u^4}  \ \mathrm d u $$ $$ = \pi \lim_{b \to \infty} \Bigg [ \frac{-1}{u} + \frac{1}{3u^3}\Bigg]_{u=1}^{u=\cosh(b)}$$ $$ = \pi \lim_{b \to \infty} \Bigg [ -\frac{1}{\cosh(b)} + \frac{1}{3\cosh^3(b)} +1 - \frac{1}{3}\Bigg ]$$ $$ = \frac{2\pi}{3}$$ Which I believe is double the correct answer of $\frac{\pi}{3}$ . Can anyone help if possible? Cheers heaps!","I've got a problem calculating the volume of the top half of a pseudosphere. The pseudosphere is parametrised by with and So the volume I'm trying to find is that between the axis and the surface of the inside of the pseudosphere. Now, we've been given that the formula for any region in is based on the change of variables and Since we have because when is put into we get and similarly when is put into y = we get Then, so the radius at is , and as we get so both and approach so the radius approaches . So the integral becomes which does not converge. I got close by multiplying the inside of the integral by so the integral becomes make a u-sub: with and so the integral becomes Which I believe is double the correct answer of . Can anyone help if possible? Cheers heaps!","\Phi(t,\theta) = \Big ( \frac{\cos(\theta)}{\cosh(t)}, \frac{\sin(\theta)}{\cosh(t)}, t-\tanh(t)\Big) 0\le t 0\le \theta \lt 2\pi x-y V \mathbb R^3 \iiint_V r\tanh^2(t) \ \mathrm dr \ \mathrm d\theta \ \mathrm d t x = r\cos(\theta) \ , \ y = r\sin(\theta) \   z = t - \tanh(t) t\ge 0 0 \lt r \le 1 t=0 x = \frac{\cos(\theta)}{\cosh(t)} x=\cos(\theta) t=0 \frac{\sin(\theta)}{\cosh(t)} y = \sin(\theta) x^2 + y^2 = \cos^2(t) + \sin^2(t) = 1 t=0 1 t \to \infty \cosh(t) \to \infty x y 0 0 \lim_{b \to \infty}\int_{t=0}^{t=b} \int_{\theta=0}^{\theta = 2\pi} \int_{r=0}^{r=1} r\tanh^2(t) \ \mathrm dr \ \mathrm d\theta \ \mathrm d t  = \pi \lim_{b \to \infty}\int_{t=0}^{t=b} \tanh^2(t) \ \mathrm d t  = \pi \lim_{b \to \infty}\int_{t=0}^{t=b} 1-\text{sech}^2(t) \ \mathrm d t   = \pi \lim_{b \to \infty} \Bigg [ t - \tanh(t)\Bigg]_{t=0}^{t=b} \lvert \vec T_t \times \vec T_\theta \rvert = \frac{\sinh(t)}{\cosh^2(t)} \lim_{b \to \infty}\int_{t=0}^{t=b} \int_{\theta=0}^{\theta = 2\pi} \int_{r=0}^{r=1} r\tanh^2(t)\frac{\sinh(t)}{\cosh^2(t)} \ \mathrm dr \ \mathrm d\theta \ \mathrm d t  = \lim_{b \to \infty}\int_{t=0}^{t=b} \int_{\theta=0}^{\theta = 2\pi} \int_{r=0}^{r=1} r\frac{\sinh^3(t)}{\cosh^4(t)} \ \mathrm dr \ \mathrm d\theta \ \mathrm d t  = \pi \lim_{b \to \infty}\int_{t=0}^{t=b} \frac{\sinh(t)\sinh^2(t)}{\cosh^4(t)}  \ \mathrm d t  = \pi \lim_{b \to \infty}\int_{t=0}^{t=b} \frac{\sinh(t)(\cosh^2(t)-1)}{\cosh^4(t)}  \ \mathrm d t u = \cosh(t) \implies \mathrm du = \sinh(t) \mathrm dt  u(b) = \cosh(b) u(0) = 1  \pi \lim_{b \to \infty}\int_{u=1}^{u=\cosh(b)} \frac{u^2-1}{u^4}  \ \mathrm d u   = \pi \lim_{b \to \infty} \Bigg [ \frac{-1}{u} + \frac{1}{3u^3}\Bigg]_{u=1}^{u=\cosh(b)}  = \pi \lim_{b \to \infty} \Bigg [ -\frac{1}{\cosh(b)} + \frac{1}{3\cosh^3(b)} +1 - \frac{1}{3}\Bigg ]  = \frac{2\pi}{3} \frac{\pi}{3}","['multivariable-calculus', 'surface-integrals']"
26,Positive-Eigenvalue Jacobian $\Rightarrow$ Invertible?,Positive-Eigenvalue Jacobian  Invertible?,\Rightarrow,"Suppose $f : \mathbb{R}^n \to \mathbb{R}^n$ has Jacobian $Jf : \mathbb{R}^n\to\mathsf{M}_n(\mathbb{R})$ with positive eigenvalues everywhere. Is $f$ (globally) injective (invertible on its range)? If $Jf$ was also guaranteed to be symmetric, this would be true by this question . We also know $f$ is locally invertible by the inverse function theorem.","Suppose has Jacobian with positive eigenvalues everywhere. Is (globally) injective (invertible on its range)? If was also guaranteed to be symmetric, this would be true by this question . We also know is locally invertible by the inverse function theorem.",f : \mathbb{R}^n \to \mathbb{R}^n Jf : \mathbb{R}^n\to\mathsf{M}_n(\mathbb{R}) f Jf f,"['real-analysis', 'multivariable-calculus']"
27,An annoying optimization problem,An annoying optimization problem,,"At first I thought the following problem looked simple, but I've had serious problems pinning it down: Suppose that $\prod_{i=1}^k x_i$ is fixed. Then find the minimum value of $$\sum_{i=1}^k (1 - x_i)^k$$ in the range $0 \leq x_1, \dots, x_k \leq 1$. A first conjecture is that it should be minimised when $x_1 = \dots = x_k$. This turns out to be false when the product is small, but I still suspect it's true when the product is reasonably large, say at least $1/2^k$. Does anyone see a simple or reasonable way to analyze this? You can get somewhere with Lagrange multipliers, but it doesn't seem to be enough. I also thought there might be an elementary argument I'm missing. Perhaps I should flesh this out a little more. Using Lagrange multipliers, one can show that $x_i(1-x_i)^{k-1}$ is the same for each $i$. Since the function $x(1-x)^{k-1}$ has a unique maximum (at $x = 1/k$), this shows that the $x_i$ take at most $2$ values. But I've had trouble reducing the problem any further, partly because, as I said, there are cases where the minimum is not in the expected place.","At first I thought the following problem looked simple, but I've had serious problems pinning it down: Suppose that $\prod_{i=1}^k x_i$ is fixed. Then find the minimum value of $$\sum_{i=1}^k (1 - x_i)^k$$ in the range $0 \leq x_1, \dots, x_k \leq 1$. A first conjecture is that it should be minimised when $x_1 = \dots = x_k$. This turns out to be false when the product is small, but I still suspect it's true when the product is reasonably large, say at least $1/2^k$. Does anyone see a simple or reasonable way to analyze this? You can get somewhere with Lagrange multipliers, but it doesn't seem to be enough. I also thought there might be an elementary argument I'm missing. Perhaps I should flesh this out a little more. Using Lagrange multipliers, one can show that $x_i(1-x_i)^{k-1}$ is the same for each $i$. Since the function $x(1-x)^{k-1}$ has a unique maximum (at $x = 1/k$), this shows that the $x_i$ take at most $2$ values. But I've had trouble reducing the problem any further, partly because, as I said, there are cases where the minimum is not in the expected place.",,"['multivariable-calculus', 'inequality', 'optimization', 'lagrange-multiplier']"
28,Integral of Laplacian of Compactly Supported Function,Integral of Laplacian of Compactly Supported Function,,"Let's say I'm interested in integrating an expression of the form: $$ \int_{\mathbb{R}^d} \Delta u \mathrm{d}x $$ Where $u$ is some compactly supported function with well-defined laplacian. Is it valid to say that: $$ \int_{\mathbb{R}^d}\Delta u \mathrm{d}x = \lim_{r \to \infty}\int_{B_r}\Delta u \mathrm{d}x = \lim_{r \to \infty}\int_{\partial B_r}\nabla u \cdot n \mathrm{d}x $$ (Applying the divergence theorem to obtain the last equality) And because $\nabla u \rightarrow 0 $as $r \rightarrow \infty$ due to the fact that $u$ has compact support, it follows that: $$ \int_{\mathbb{R}^d}\Delta u \mathrm{d}x = \lim_{r \to \infty}\int_{ \partial B_r}\nabla u \cdot n \mathrm{d}x = 0 $$ Is this valid reasoning?","Let's say I'm interested in integrating an expression of the form: $$ \int_{\mathbb{R}^d} \Delta u \mathrm{d}x $$ Where $u$ is some compactly supported function with well-defined laplacian. Is it valid to say that: $$ \int_{\mathbb{R}^d}\Delta u \mathrm{d}x = \lim_{r \to \infty}\int_{B_r}\Delta u \mathrm{d}x = \lim_{r \to \infty}\int_{\partial B_r}\nabla u \cdot n \mathrm{d}x $$ (Applying the divergence theorem to obtain the last equality) And because $\nabla u \rightarrow 0 $as $r \rightarrow \infty$ due to the fact that $u$ has compact support, it follows that: $$ \int_{\mathbb{R}^d}\Delta u \mathrm{d}x = \lim_{r \to \infty}\int_{ \partial B_r}\nabla u \cdot n \mathrm{d}x = 0 $$ Is this valid reasoning?",,"['real-analysis', 'multivariable-calculus', 'vector-analysis', 'divergence-operator']"
29,"In the Change of Variables theorem, why must Dg be invertible a.e.?","In the Change of Variables theorem, why must Dg be invertible a.e.?",,"Theorem: (Change of Variables Theorem). Let $\Omega\subset\Bbb R^n$ be a region and let $U$ be an open set containing $\Omega$ so that ${\bf g}:U\to\Bbb R^n$ is one-to-one and ${\cal C}^1$ with invertible derivative at each point. Suppose $f:{\bf g}(\Omega)\to\Bbb R$ and $(f\circ{\bf g})\lvert\det D{\bf g}\rvert:\Omega\to\Bbb R$ are both integrable. Then   $$\int_{{\bf g}(\Omega)}f({\bf y}) \operatorname d\!V_{\bf y} = \int_\Omega(f\circ{\bf g})(x) \lvert\det D{\bf g}\rvert \operatorname d\!V_{\bf x}.$$ Remark . One can strengthen the theorem, in particular by allowing $D{\bf g}(x)$ to fail to be invertible on a set of volume $0$. This is important for many applications—e.g., polar, cylindrical, and spherical coordinates. But we won’t bother justifying it here. What happens if $D{\bf g}(x)$ fails to be invertible on a set of positive measure? I can't come up with a counterexample in that case. Why is that hypothesis necessary?","Theorem: (Change of Variables Theorem). Let $\Omega\subset\Bbb R^n$ be a region and let $U$ be an open set containing $\Omega$ so that ${\bf g}:U\to\Bbb R^n$ is one-to-one and ${\cal C}^1$ with invertible derivative at each point. Suppose $f:{\bf g}(\Omega)\to\Bbb R$ and $(f\circ{\bf g})\lvert\det D{\bf g}\rvert:\Omega\to\Bbb R$ are both integrable. Then   $$\int_{{\bf g}(\Omega)}f({\bf y}) \operatorname d\!V_{\bf y} = \int_\Omega(f\circ{\bf g})(x) \lvert\det D{\bf g}\rvert \operatorname d\!V_{\bf x}.$$ Remark . One can strengthen the theorem, in particular by allowing $D{\bf g}(x)$ to fail to be invertible on a set of volume $0$. This is important for many applications—e.g., polar, cylindrical, and spherical coordinates. But we won’t bother justifying it here. What happens if $D{\bf g}(x)$ fails to be invertible on a set of positive measure? I can't come up with a counterexample in that case. Why is that hypothesis necessary?",,"['calculus', 'real-analysis', 'multivariable-calculus', 'jacobian', 'change-of-variable']"
30,"A differentiable and unbounded function with infinitely many critical points, all of them being local maxima.","A differentiable and unbounded function with infinitely many critical points, all of them being local maxima.",,"I'm searching for a function $f: \Bbb R^2 \rightarrow \Bbb R$ which has following properties: Both $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ exist and are continuous in $\Bbb R^2$. There are infinitely many critical points* , in all of which, $f$ has local maximum. $f$ is unbounded both from above and from below. Could you give me some hints, or even better, show me explicitly the desired function? *$(x,y)$ such that $\frac{\partial f}{\partial x}(x,y)=\frac{\partial f}{\partial y}(x,y)=0$.","I'm searching for a function $f: \Bbb R^2 \rightarrow \Bbb R$ which has following properties: Both $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ exist and are continuous in $\Bbb R^2$. There are infinitely many critical points* , in all of which, $f$ has local maximum. $f$ is unbounded both from above and from below. Could you give me some hints, or even better, show me explicitly the desired function? *$(x,y)$ such that $\frac{\partial f}{\partial x}(x,y)=\frac{\partial f}{\partial y}(x,y)=0$.",,"['real-analysis', 'multivariable-calculus']"
31,Integrating a partial derivative with second variable a function of the first?,Integrating a partial derivative with second variable a function of the first?,,"These questions are related but don't answer my specific question: 1 , 2 , 3 . Assume we have a two-variable function where the second depends on the first: $z=f(x,y(x))$ Is it possible in this case to calculate the integral of the partial derivative of $f$ with respect to $x$, that is independent of the specific form of $f$ and $y(x)$? $$\int\frac {\partial f}{\partial x}(x,y(x))dx=?$$ We cannot apply the fundamental theorem of calculus directly as we would if it was not a partial but a standard derivative: $$\int\frac {d}{dx}f(x,y(x))dx=f(x,y(x))+C$$ Is there some similar (general!) formula for the partial derivative case?","These questions are related but don't answer my specific question: 1 , 2 , 3 . Assume we have a two-variable function where the second depends on the first: $z=f(x,y(x))$ Is it possible in this case to calculate the integral of the partial derivative of $f$ with respect to $x$, that is independent of the specific form of $f$ and $y(x)$? $$\int\frac {\partial f}{\partial x}(x,y(x))dx=?$$ We cannot apply the fundamental theorem of calculus directly as we would if it was not a partial but a standard derivative: $$\int\frac {d}{dx}f(x,y(x))dx=f(x,y(x))+C$$ Is there some similar (general!) formula for the partial derivative case?",,"['calculus', 'multivariable-calculus', 'partial-derivative']"
32,"Is there a set of ""canonical"" partial derivative exercises that ""tell the whole story""?","Is there a set of ""canonical"" partial derivative exercises that ""tell the whole story""?",,"I find the field of partial differentiation of complicated multivariable expressions to have many subtleties which lead to unexpected results, or to my applying operations incorrectly. I'm wondering if there is some established set of exercises which address the common ""gatcha"" situation.  I'm particularly interested in inverse and implicit mappings. Any suggestions?  I have Edwards's Advanced Calculus of Several Variables, and will admit to not having worked many of the exercises (yet).","I find the field of partial differentiation of complicated multivariable expressions to have many subtleties which lead to unexpected results, or to my applying operations incorrectly. I'm wondering if there is some established set of exercises which address the common ""gatcha"" situation.  I'm particularly interested in inverse and implicit mappings. Any suggestions?  I have Edwards's Advanced Calculus of Several Variables, and will admit to not having worked many of the exercises (yet).",,['multivariable-calculus']
33,Derivation of a vector calculus identity,Derivation of a vector calculus identity,,"The following is from a past qualifying exam, and so should have a relatively slick solution. Let $g: \mathbb{R}^2 \to (0, \infty)$ be a $C^2$ function, and define $\Sigma \in \mathbb{R}^3$ to be the graph of $g$ restricted to the unit disk, i.e. $\{x, y, g(x,y)|x^2 + y^2 \leq 1\}$. Suppose that $\Sigma$ is contained in the ball of radius $R$, and that the ray from the origin to $R$ intersects with $\Sigma$ at most once. If $E$ denotes the set of points on $\mathbb{S}^2(R)$ for which the ray does intersect $\Sigma$ precisely once, then the task is to find an equation relating the area of $E$, $R$ and the following integral: $$ \int_\Sigma \nabla \Gamma(x) \cdot N(x) dS$$ where $\Gamma = \frac{1}{|x|},$ $N$ denotes a unit normal as always. I have tinkered with the idea of the divergence theorem. The idea is something like using the fact that the rays leaving the origin are normal to the sphere, to find some kind of flux integral, but it's just not coming together for me after working through the practice exam. Apologies if it's really simple!","The following is from a past qualifying exam, and so should have a relatively slick solution. Let $g: \mathbb{R}^2 \to (0, \infty)$ be a $C^2$ function, and define $\Sigma \in \mathbb{R}^3$ to be the graph of $g$ restricted to the unit disk, i.e. $\{x, y, g(x,y)|x^2 + y^2 \leq 1\}$. Suppose that $\Sigma$ is contained in the ball of radius $R$, and that the ray from the origin to $R$ intersects with $\Sigma$ at most once. If $E$ denotes the set of points on $\mathbb{S}^2(R)$ for which the ray does intersect $\Sigma$ precisely once, then the task is to find an equation relating the area of $E$, $R$ and the following integral: $$ \int_\Sigma \nabla \Gamma(x) \cdot N(x) dS$$ where $\Gamma = \frac{1}{|x|},$ $N$ denotes a unit normal as always. I have tinkered with the idea of the divergence theorem. The idea is something like using the fact that the rays leaving the origin are normal to the sphere, to find some kind of flux integral, but it's just not coming together for me after working through the practice exam. Apologies if it's really simple!",,"['multivariable-calculus', 'vector-analysis']"
34,(Geometric intuition) Line integral over vector fields,(Geometric intuition) Line integral over vector fields,,"I'm trying to understand the geometric intuition behind the definition of the line integrals over vector fields. The definition is given below: Definition: Let $\vec{F}$ be a continuous vector field defined on a smooth curve $\gamma$ given by a vector function $r(t)$ . Then the line integral of $\vec{F}$ along $\gamma$ is $$\int_{\gamma}\vec{F}\cdot d\vec{r}=\int_{\gamma} \vec{F}\cdot\vec{T} ds$$ Where $T$ is the unit tangent. So the line integral of the vector field $\vec{F}$ along $\gamma$ is defined as line integral over a scalar field. The geometric interpretation of this one  can be found here . So using the geometric interpretation of line integrals over scalar fields, I'm trying to understand this one over vector fields. In the definition above the scalar product $\vec{F}\cdot \vec{T}$ is a function $\alpha(x,y)$ which takes a point in the curve $\gamma$ and gives out a point with $\alpha(x,y)=|\vec{T}|$ (Since $T$ is a unit vector, $\vec{F}\cdot \vec{T}$ is the length of the projection of the vector $\vec{F}(x,y)$ over the tangent). So using the geometric interpretation of the line integral over scalar fields, is the integral $\int_{\gamma}\vec{F}\cdot d\vec{r}$ the area below the curve $\alpha$ ? If yes, why is this geometric relevant?","I'm trying to understand the geometric intuition behind the definition of the line integrals over vector fields. The definition is given below: Definition: Let be a continuous vector field defined on a smooth curve given by a vector function . Then the line integral of along is Where is the unit tangent. So the line integral of the vector field along is defined as line integral over a scalar field. The geometric interpretation of this one  can be found here . So using the geometric interpretation of line integrals over scalar fields, I'm trying to understand this one over vector fields. In the definition above the scalar product is a function which takes a point in the curve and gives out a point with (Since is a unit vector, is the length of the projection of the vector over the tangent). So using the geometric interpretation of the line integral over scalar fields, is the integral the area below the curve ? If yes, why is this geometric relevant?","\vec{F} \gamma r(t) \vec{F} \gamma \int_{\gamma}\vec{F}\cdot d\vec{r}=\int_{\gamma} \vec{F}\cdot\vec{T} ds T \vec{F} \gamma \vec{F}\cdot \vec{T} \alpha(x,y) \gamma \alpha(x,y)=|\vec{T}| T \vec{F}\cdot \vec{T} \vec{F}(x,y) \int_{\gamma}\vec{F}\cdot d\vec{r} \alpha","['multivariable-calculus', 'intuition', 'line-integrals', 'geometric-interpretation']"
35,Representation of symmetric function,Representation of symmetric function,,"I am interested in learning a bit more about symmetric functions of $n$ variables, namely functions that are invariant under permutation of their arguments : $\forall \pi \in \sigma_n$, $$f(x_1,...,x_n) = f(x_{\pi(1)},...,x_{\pi(n)})$$ When $f$ is a polynomial or a rationial function, it can be written in a unique way as a sum of elementary symmetric polynomials or rational functions. This is the fundamental theorem of symmetrical functions (see http://mathworld.wolfram.com/FundamentalTheoremofSymmetricFunctions.html for instance). What can be said about a symmetric function that is not a polynomial or a rational function (only $C^\infty$ or analytic for instance) ?","I am interested in learning a bit more about symmetric functions of $n$ variables, namely functions that are invariant under permutation of their arguments : $\forall \pi \in \sigma_n$, $$f(x_1,...,x_n) = f(x_{\pi(1)},...,x_{\pi(n)})$$ When $f$ is a polynomial or a rationial function, it can be written in a unique way as a sum of elementary symmetric polynomials or rational functions. This is the fundamental theorem of symmetrical functions (see http://mathworld.wolfram.com/FundamentalTheoremofSymmetricFunctions.html for instance). What can be said about a symmetric function that is not a polynomial or a rational function (only $C^\infty$ or analytic for instance) ?",,"['multivariable-calculus', 'symmetric-polynomials', 'symmetric-functions']"
36,"If $f$ is twice differentiable, $\big(f(y) - f(x)\big)/(y-x)$ is differentiable","If  is twice differentiable,  is differentiable",f \big(f(y) - f(x)\big)/(y-x),"Suppose $f: \mathbb{R} \to \mathbb{R}$ is a $C^{1}$ function. Then, define a new function $F: \mathbb{R}^{2} \to \mathbb{R}$ by: $$ F(x,y) = \begin{cases}   \displaystyle \frac{f(y) - f(x)}{y - x} &\text{ if } x \neq y \\   \displaystyle f'(x) &\text{ otherwise.} \end{cases} $$ Then, if $f''(x)$ exists, $F$ is differentiable. I can prove that $F$ is differentiable if $x \neq y$, since under these conditions $F_{x}$ and $F_{y}$ are $C^{1}$. So it's left to prove $F$ is also differentiable if $x = y$. At first, I conjectured that, for example, $F_{x} (a,a)$ would be $f''(a)/2$, but I'm having a hard time proving it. I started using the definition $\lim_{h \to 0} (F(a+h, a) - F(a,a)) / h$ and applying the MVT found $\bar{a}$ between $a$ and $a + h$ s.t. this difference quotient is: $$ \frac{1}{h}(f'(\bar{a}) - f'(a)) $$ so I tried dividing and multiplying by $\bar{a} - a$, thinking it would be possible to prove that $\lim_{h \to 0} (\bar{a} - a)/h = 1$, but so far I've only been able to bound it above by $1$. Is it true? Does this conjecture even makes sense? I'm lost in thinking about any other candidates for the differential in these points. Any help would be appreciated!","Suppose $f: \mathbb{R} \to \mathbb{R}$ is a $C^{1}$ function. Then, define a new function $F: \mathbb{R}^{2} \to \mathbb{R}$ by: $$ F(x,y) = \begin{cases}   \displaystyle \frac{f(y) - f(x)}{y - x} &\text{ if } x \neq y \\   \displaystyle f'(x) &\text{ otherwise.} \end{cases} $$ Then, if $f''(x)$ exists, $F$ is differentiable. I can prove that $F$ is differentiable if $x \neq y$, since under these conditions $F_{x}$ and $F_{y}$ are $C^{1}$. So it's left to prove $F$ is also differentiable if $x = y$. At first, I conjectured that, for example, $F_{x} (a,a)$ would be $f''(a)/2$, but I'm having a hard time proving it. I started using the definition $\lim_{h \to 0} (F(a+h, a) - F(a,a)) / h$ and applying the MVT found $\bar{a}$ between $a$ and $a + h$ s.t. this difference quotient is: $$ \frac{1}{h}(f'(\bar{a}) - f'(a)) $$ so I tried dividing and multiplying by $\bar{a} - a$, thinking it would be possible to prove that $\lim_{h \to 0} (\bar{a} - a)/h = 1$, but so far I've only been able to bound it above by $1$. Is it true? Does this conjecture even makes sense? I'm lost in thinking about any other candidates for the differential in these points. Any help would be appreciated!",,"['multivariable-calculus', 'derivatives', 'partial-derivative']"
37,How can I calculate the flux inside this shape?,How can I calculate the flux inside this shape?,,"Here's a picture from my multivariable calculus textbook. The question is in a chapter about the divergence theorem. I'm going to sum it here (sorry for my bad English): A curved pipe $S$ is illustrated. The boundary of $S$ is a pair of circles $C_1$ and $C_2$. The circle $C_1$ has radius $3$ and is contained in the plane $y=0$ and $C_2$ has radius $1$ and is contained inside the plane $z=4$. The volume of $S$ is $12$. Calculate the flux of the field $F=g(y,z)i+8j-(2z+3)k$, where $g$ has continuous partial derivatives. I took the divergence of $F$, which gave me ${\dfrac{d}{dx}}g-2$. Using the divergence theorem to find the flux, I then multiplied this result by the volume, $12$. The answer of the problem is $83π-24$. I got the $-24$ by multiplying $-2$ by $12$, but where does the $83π$ come from? Thanks!","Here's a picture from my multivariable calculus textbook. The question is in a chapter about the divergence theorem. I'm going to sum it here (sorry for my bad English): A curved pipe $S$ is illustrated. The boundary of $S$ is a pair of circles $C_1$ and $C_2$. The circle $C_1$ has radius $3$ and is contained in the plane $y=0$ and $C_2$ has radius $1$ and is contained inside the plane $z=4$. The volume of $S$ is $12$. Calculate the flux of the field $F=g(y,z)i+8j-(2z+3)k$, where $g$ has continuous partial derivatives. I took the divergence of $F$, which gave me ${\dfrac{d}{dx}}g-2$. Using the divergence theorem to find the flux, I then multiplied this result by the volume, $12$. The answer of the problem is $83π-24$. I got the $-24$ by multiplying $-2$ by $12$, but where does the $83π$ come from? Thanks!",,"['calculus', 'multivariable-calculus']"
38,partial derivative of a facet normal wrt to one of its vertex,partial derivative of a facet normal wrt to one of its vertex,,"I am struggling to understand the derivation of an equation in a paper ( A Bayesian Method for Probable Surface Reconstruction and Decimation , specifically Eqn. 16). Basically they define three vertices of a facet: $x_k, x_{k'},x_{k''}$  The normalized facet normal is defined as: $n_i = \dfrac{(x_{k'}-x_k) \times (x_{k''}-x_k)}{|(x_{k'}-x_k) \times (x_{k''}-x_k)|}$ So far so good. The problem is then that they need to compute $\frac{\partial n_i}{\partial x_k}$. Firstly ${n_i}$ and ${x_k}$ are both vectors, hence I'd expect that this partial derivative notation means in effect the Jacobian of ${n_i}$ wrt. ${x_k}$? In that case that is a 3x3 matrix. However the formula below (see Eqn 16) implies that the result is a 3x1 vector?? (confused!) $\frac{\partial n_i}{\partial x_k} = \frac{I - n_in_i^T}{|(x_{k'} - x_k) \times (x_{k''} - x_k)|} (x_{k''} - x_{k'}) \times x_k$ I was hoping someone could shed some light on the dimensionallity confusion and also how that formula was derived, or if incorrect what is the correct forumation for $\frac{\partial n_i}{\partial x_k}$?  Thanks for the help!","I am struggling to understand the derivation of an equation in a paper ( A Bayesian Method for Probable Surface Reconstruction and Decimation , specifically Eqn. 16). Basically they define three vertices of a facet: $x_k, x_{k'},x_{k''}$  The normalized facet normal is defined as: $n_i = \dfrac{(x_{k'}-x_k) \times (x_{k''}-x_k)}{|(x_{k'}-x_k) \times (x_{k''}-x_k)|}$ So far so good. The problem is then that they need to compute $\frac{\partial n_i}{\partial x_k}$. Firstly ${n_i}$ and ${x_k}$ are both vectors, hence I'd expect that this partial derivative notation means in effect the Jacobian of ${n_i}$ wrt. ${x_k}$? In that case that is a 3x3 matrix. However the formula below (see Eqn 16) implies that the result is a 3x1 vector?? (confused!) $\frac{\partial n_i}{\partial x_k} = \frac{I - n_in_i^T}{|(x_{k'} - x_k) \times (x_{k''} - x_k)|} (x_{k''} - x_{k'}) \times x_k$ I was hoping someone could shed some light on the dimensionallity confusion and also how that formula was derived, or if incorrect what is the correct forumation for $\frac{\partial n_i}{\partial x_k}$?  Thanks for the help!",,"['calculus', 'multivariable-calculus', 'derivatives', 'computer-science', 'partial-derivative']"
39,Second Variation of Area Functional,Second Variation of Area Functional,,"This is a follow up question from this one . I have proved that given a parametrized surface ${\bf x}$, the mean curvature is zero if and only if it is a critical point of the area functional. Then everybody starts handwaving and says that the surface is a minimum point. I want to check that indeed this is the case (mainly because I'll have to redo the calculations in Minkowski space where we also have maximum surfaces, and if I know how to do this here, I can adapt the calculations). That being said, I am not interested in heavy machinery from Riemannian geometry. My notation will be the same as in the other question. For your comfort: Notations: Fix a domain $D$. Here $\bf x$ is a parametrization, ${\bf x}^t={\bf x}+t{\bf V}$ is a variation, with $\bf V$ being zero on $∂D$, $\bf N$ is the normal unit vector and $A(t)$ is the area of ${\bf x}^t$. Assume $H = 0$. I have computed: $$A''(0) = \iint_D \frac{2\langle {\bf x}_u\times{\bf x}_v,{\bf V}_u\times{\bf V}_v\rangle + \|{\bf x}_u\times{\bf V}_v+{\bf V}_u\times{\bf x}_v\|^2}{\|{\bf x}_u\times{\bf x}_v\|^3}\,{\rm d}u\,{\rm d}v$$ I can add the computations I made to get this, if it comes to that. Theory says that we must have this integral being positive . I don't know how to proceed here. Can someone help me please? Thanks. Edit: rephrasing: I want to check that $\bf x$ is a local minimum of the functional. After the comments, I'm not even sure if we'll really have ${\cal A}''(0) > 0 $ then. I need this specific question adressed too. (I'm not sure I even know calculus anymore haha) Edit (05/12): It is of my understanding that if we have a closed contour $\Gamma \subset \Bbb R^3$, then off all surfaces that have $\Gamma$ as boundary, the one that minimizes area has $H = 0$. However, having $H = 0$ does not imply that the surface is area minimizing, as pointed by several people in the comments. Ok. Let's say I am dealing only with a parametrized surface ${\bf x}: D \subset \Bbb R^2 \to {\bf x}(D)\subset \Bbb R^3$. Then the boundary would be just ${\bf x}(\partial D)$. If I consider all the variations ${\bf x}^t$ defined as above, then all of them share the same boundary as $\bf x$. Although ${\bf x}(D)$ may not be the surface having ${\bf x}(\partial D)$ as boundary that minimizes area, I am thinking so far that between all the ${\bf x}^t$, at least , $\bf x$ is the one that gives least area. With this in mind, I wanted to check that ${\cal A}''(0) > 0$.","This is a follow up question from this one . I have proved that given a parametrized surface ${\bf x}$, the mean curvature is zero if and only if it is a critical point of the area functional. Then everybody starts handwaving and says that the surface is a minimum point. I want to check that indeed this is the case (mainly because I'll have to redo the calculations in Minkowski space where we also have maximum surfaces, and if I know how to do this here, I can adapt the calculations). That being said, I am not interested in heavy machinery from Riemannian geometry. My notation will be the same as in the other question. For your comfort: Notations: Fix a domain $D$. Here $\bf x$ is a parametrization, ${\bf x}^t={\bf x}+t{\bf V}$ is a variation, with $\bf V$ being zero on $∂D$, $\bf N$ is the normal unit vector and $A(t)$ is the area of ${\bf x}^t$. Assume $H = 0$. I have computed: $$A''(0) = \iint_D \frac{2\langle {\bf x}_u\times{\bf x}_v,{\bf V}_u\times{\bf V}_v\rangle + \|{\bf x}_u\times{\bf V}_v+{\bf V}_u\times{\bf x}_v\|^2}{\|{\bf x}_u\times{\bf x}_v\|^3}\,{\rm d}u\,{\rm d}v$$ I can add the computations I made to get this, if it comes to that. Theory says that we must have this integral being positive . I don't know how to proceed here. Can someone help me please? Thanks. Edit: rephrasing: I want to check that $\bf x$ is a local minimum of the functional. After the comments, I'm not even sure if we'll really have ${\cal A}''(0) > 0 $ then. I need this specific question adressed too. (I'm not sure I even know calculus anymore haha) Edit (05/12): It is of my understanding that if we have a closed contour $\Gamma \subset \Bbb R^3$, then off all surfaces that have $\Gamma$ as boundary, the one that minimizes area has $H = 0$. However, having $H = 0$ does not imply that the surface is area minimizing, as pointed by several people in the comments. Ok. Let's say I am dealing only with a parametrized surface ${\bf x}: D \subset \Bbb R^2 \to {\bf x}(D)\subset \Bbb R^3$. Then the boundary would be just ${\bf x}(\partial D)$. If I consider all the variations ${\bf x}^t$ defined as above, then all of them share the same boundary as $\bf x$. Although ${\bf x}(D)$ may not be the surface having ${\bf x}(\partial D)$ as boundary that minimizes area, I am thinking so far that between all the ${\bf x}^t$, at least , $\bf x$ is the one that gives least area. With this in mind, I wanted to check that ${\cal A}''(0) > 0$.",,"['multivariable-calculus', 'differential-geometry']"
40,"Is $f(x,y)=xy/(x^{2}+y^{2}) $ differentiable or continuous?",Is  differentiable or continuous?,"f(x,y)=xy/(x^{2}+y^{2}) ","I'm taking a course in Analysis of several variables and the text we're following is Analysis on Manifolds - Munkres. I'm having issues to interpret properly the results I'm getting in my exercises. I'm trying the second exercise in Section 5: Let $f : \mathbb{R}^{2} \to \mathbb{R}$ be defined by setting $f(0)=0$ and $$f(x,y)=xy/(x^{2}+y^{2}) \quad \textit{if} \quad (x,y) \neq 0.$$ (a) For wich vectors $u \neq 0$ does $f^{\prime}(0;u)$ exist? Evaluate it when it exists. (b) Do $D_{1}f$ and $D_{2}f$ exist at $0$ ? (c) Is $f$ differentiable at $0$ ? (d) Is $f$ continous a $0$? For the item (a) I take a vector $u=(h,k)$ and from the definition of $f^{\prime}(a;u)$ I get $$\lim_{t \to 0} \frac{f[(0,0)+ (th,tk)]-f(0,0)}{t}= \lim_{t \to 0} \frac{1}{t}\frac{t^{2}hk}{(th)^{2}+(tk)^{2}}=\lim_{t\to 0} \frac{hk}{t(h^{2}+k^{2})}$$ After this I don't know how to interpret if it actually exist or no, and don't understand either how or what to evaluate when it exists. About the item (b) I did this and I don't know if that's correct: $$D_{1}f(0,0) = \lim_{t \to 0} \frac{f[(0,0)+t(1,0)]-f(0,0)}{t} = lim_{t \to 0} \frac{f(t,0)}{t} = 0$$ And the same with $D_{2}f(0,0) = 0$ Is that correct? With item (c) tried the same with the definition and I got stuck : $$\lim_{h \to 0} \frac{f(0+h)-f(0)-Bh}{|h|} = \lim_{h \to 0} \frac{f(h_{1},h_{2})-B(h_{1},h_{2})}{|(h_{1},h_{2})|}$$ Once I get here I don't know what to do, should I evaluate $f(h_{1},h_{2})$ ? and if I do, then how can I interprate if it is differentialbe at $0$ ? Finally for the item (d) I have no clue how to do it, I really need help for this part. I really appreciate if you help me understading completely how can I do it. Thank you in advance!","I'm taking a course in Analysis of several variables and the text we're following is Analysis on Manifolds - Munkres. I'm having issues to interpret properly the results I'm getting in my exercises. I'm trying the second exercise in Section 5: Let $f : \mathbb{R}^{2} \to \mathbb{R}$ be defined by setting $f(0)=0$ and $$f(x,y)=xy/(x^{2}+y^{2}) \quad \textit{if} \quad (x,y) \neq 0.$$ (a) For wich vectors $u \neq 0$ does $f^{\prime}(0;u)$ exist? Evaluate it when it exists. (b) Do $D_{1}f$ and $D_{2}f$ exist at $0$ ? (c) Is $f$ differentiable at $0$ ? (d) Is $f$ continous a $0$? For the item (a) I take a vector $u=(h,k)$ and from the definition of $f^{\prime}(a;u)$ I get $$\lim_{t \to 0} \frac{f[(0,0)+ (th,tk)]-f(0,0)}{t}= \lim_{t \to 0} \frac{1}{t}\frac{t^{2}hk}{(th)^{2}+(tk)^{2}}=\lim_{t\to 0} \frac{hk}{t(h^{2}+k^{2})}$$ After this I don't know how to interpret if it actually exist or no, and don't understand either how or what to evaluate when it exists. About the item (b) I did this and I don't know if that's correct: $$D_{1}f(0,0) = \lim_{t \to 0} \frac{f[(0,0)+t(1,0)]-f(0,0)}{t} = lim_{t \to 0} \frac{f(t,0)}{t} = 0$$ And the same with $D_{2}f(0,0) = 0$ Is that correct? With item (c) tried the same with the definition and I got stuck : $$\lim_{h \to 0} \frac{f(0+h)-f(0)-Bh}{|h|} = \lim_{h \to 0} \frac{f(h_{1},h_{2})-B(h_{1},h_{2})}{|(h_{1},h_{2})|}$$ Once I get here I don't know what to do, should I evaluate $f(h_{1},h_{2})$ ? and if I do, then how can I interprate if it is differentialbe at $0$ ? Finally for the item (d) I have no clue how to do it, I really need help for this part. I really appreciate if you help me understading completely how can I do it. Thank you in advance!",,"['multivariable-calculus', 'derivatives']"
41,Solving a partial differential equation by transformation of variables?,Solving a partial differential equation by transformation of variables?,,"I found an exercise in a book, where one was supposed to transform the differential equation $$y\frac{\partial f}{\partial x}-x\frac{\partial f}{\partial y}=xyf(x,y)$$ by using the substitutions $x^2+y^2=u$ and $e^{-x^2/2}=v$, and then expressing $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ in terms of $\frac{\partial f}{\partial u}$ and $\frac{\partial f}{\partial v}$. By using the chain rule and simplifying things, I get that $$-xyv\frac{\partial f}{\partial v}=-2xyf(x,y).$$ Now the ""solutions manual"" for this book just divides this with $xy$ on both sides. I do not really know why one can do that. By doing that, we can only say things about the function $f$ off the cordinate axes. Does anyone see anything obvious that I'm missing?","I found an exercise in a book, where one was supposed to transform the differential equation $$y\frac{\partial f}{\partial x}-x\frac{\partial f}{\partial y}=xyf(x,y)$$ by using the substitutions $x^2+y^2=u$ and $e^{-x^2/2}=v$, and then expressing $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ in terms of $\frac{\partial f}{\partial u}$ and $\frac{\partial f}{\partial v}$. By using the chain rule and simplifying things, I get that $$-xyv\frac{\partial f}{\partial v}=-2xyf(x,y).$$ Now the ""solutions manual"" for this book just divides this with $xy$ on both sides. I do not really know why one can do that. By doing that, we can only say things about the function $f$ off the cordinate axes. Does anyone see anything obvious that I'm missing?",,"['multivariable-calculus', 'partial-differential-equations']"
42,"A book like Michael Spivaks Calculus, for multivariate Calculus.","A book like Michael Spivaks Calculus, for multivariate Calculus.",,"Is there a book like Michael Spivaks Calculus, that is for Multivariate Calculus? That is a ""real analysis"" multivariate calculus book?","Is there a book like Michael Spivaks Calculus, that is for Multivariate Calculus? That is a ""real analysis"" multivariate calculus book?",,"['calculus', 'real-analysis', 'multivariable-calculus', 'soft-question', 'book-recommendation']"
43,Geometric intuition for mixed partial derivatives,Geometric intuition for mixed partial derivatives,,"I'm trying to better understand exactly what $f_{xy}(x,y)$ at a point is geometrically, and possibly understand why $f_{xy}$ and $f_{yx}$ should be equivalent, not just because the math happened to make it so. For example, $f_{xx}$ would be like looking at the concavity of the function in only the $x$ direction. My real question is about how the second partial derivative test works. $$D(x,y)=f_{xx}(x,y)f_{yy}(x,y)-(f_{xy}(x,y))^2$$ I can see that the sign of the first term can be interpreted as whether or not the concavity in both the $x$ and $y$ directions are in the same direction. For a critical point $(a,b)$, if the concavities are opposite, then we have a saddle point. It's also easy to see that $D(a,b)<0$. When the concavities do agree, then $f_{xy}$ starts to play a role in the sign of $D$. My current understanding on why this is necessary, is because looking at the second derivatives at only the $x$ and $y$ directions doesn't quite give the entire picture on what is happening at $(a,b)$, but including $f_{xy}$ gives sufficient information to determine if $(a,b)$ is truly a local extremum or a saddle point instead (given that $D\neq0$). My problem is that I still don't exactly get what sort of information $f_{xy}$ entails for a given point.","I'm trying to better understand exactly what $f_{xy}(x,y)$ at a point is geometrically, and possibly understand why $f_{xy}$ and $f_{yx}$ should be equivalent, not just because the math happened to make it so. For example, $f_{xx}$ would be like looking at the concavity of the function in only the $x$ direction. My real question is about how the second partial derivative test works. $$D(x,y)=f_{xx}(x,y)f_{yy}(x,y)-(f_{xy}(x,y))^2$$ I can see that the sign of the first term can be interpreted as whether or not the concavity in both the $x$ and $y$ directions are in the same direction. For a critical point $(a,b)$, if the concavities are opposite, then we have a saddle point. It's also easy to see that $D(a,b)<0$. When the concavities do agree, then $f_{xy}$ starts to play a role in the sign of $D$. My current understanding on why this is necessary, is because looking at the second derivatives at only the $x$ and $y$ directions doesn't quite give the entire picture on what is happening at $(a,b)$, but including $f_{xy}$ gives sufficient information to determine if $(a,b)$ is truly a local extremum or a saddle point instead (given that $D\neq0$). My problem is that I still don't exactly get what sort of information $f_{xy}$ entails for a given point.",,"['multivariable-calculus', 'intuition', 'partial-derivative']"
44,"Why is $a_{n}(x,y)=a_{n}(y)$?",Why is ?,"a_{n}(x,y)=a_{n}(y)","This particular question is connected (with a slight variation in the definition of $g$) to an earlier question. The link is here . The specifics are: Given that $u(x,y)$ is the solution of a PDE ($x$ and $y$ are independent variables) we can expand the solution around an arbitrary non-characteristic singularity manifold given by $g(x,y)=0$ in a power series of the form $\displaystyle\sum_{n=0}^{\infty}a_{n}(x,y)g(x,y)^{n+\alpha},$ where $\alpha$ is a negative integer (to be found using leading order analysis). Given that $g=0$ is assumed to be non-characteristic we have $g_{x}\neq 0$. Thus, by the Implicit Function Theorem we have $g(x,y)=x-f(y)$ near $g=0$, where $f(y)$ is a function of $y$. For a particular PDE it turns out that $\alpha=-2$ and the coefficients $a_{n}(x,y)$ turn out to be functions of $y$ only. Given that $g(x,y)=x-f(y)$ why can we replace $a_{n}(x,y)$ by $a_{n}(y)$?","This particular question is connected (with a slight variation in the definition of $g$) to an earlier question. The link is here . The specifics are: Given that $u(x,y)$ is the solution of a PDE ($x$ and $y$ are independent variables) we can expand the solution around an arbitrary non-characteristic singularity manifold given by $g(x,y)=0$ in a power series of the form $\displaystyle\sum_{n=0}^{\infty}a_{n}(x,y)g(x,y)^{n+\alpha},$ where $\alpha$ is a negative integer (to be found using leading order analysis). Given that $g=0$ is assumed to be non-characteristic we have $g_{x}\neq 0$. Thus, by the Implicit Function Theorem we have $g(x,y)=x-f(y)$ near $g=0$, where $f(y)$ is a function of $y$. For a particular PDE it turns out that $\alpha=-2$ and the coefficients $a_{n}(x,y)$ turn out to be functions of $y$ only. Given that $g(x,y)=x-f(y)$ why can we replace $a_{n}(x,y)$ by $a_{n}(y)$?",,"['multivariable-calculus', 'differential-geometry', 'partial-differential-equations', 'power-series', 'laurent-series']"
45,Easiest way to tell if a critical point is a a min or max,Easiest way to tell if a critical point is a a min or max,,"For $f(x,y) = \sqrt{x^2 + y^2 + \frac{1}{x^2 y^2}}$ the critical points are $(1,1), (1,-1), (-1,1), (-1,-1)$ and all of their values in the function is $3$. How can you tell that these are either maximums or minimums? The example problem states that because the values of the critical points are all $3$, this implies that they are minimums without requiring any additional computations. How can you tell just by observation?","For $f(x,y) = \sqrt{x^2 + y^2 + \frac{1}{x^2 y^2}}$ the critical points are $(1,1), (1,-1), (-1,1), (-1,-1)$ and all of their values in the function is $3$. How can you tell that these are either maximums or minimums? The example problem states that because the values of the critical points are all $3$, this implies that they are minimums without requiring any additional computations. How can you tell just by observation?",,['multivariable-calculus']
46,Finding $\frac{\partial ^8 f}{\partial x^4\partial y^4}$,Finding,\frac{\partial ^8 f}{\partial x^4\partial y^4},"Given the function $f(x,y)=\frac{1}{1-xy}$ find the value of$\frac{\partial ^8 f}{\partial x^4\partial y^4}(0,0)$. First I developed the function into a taylor series using geometric series around $(0,0)$: $$f(x,y)=\frac{1}{1-xy}=\displaystyle{\sum_{n=0}^\infty(xy)^n}$$. The taylor approximation of order 8 is given by $$f(x,y)\thickapprox\displaystyle{\sum_{n=0}^4(xy)^n}=1+xy+(xy)^2+(xy)^3+(xy)^4$$, where the degree of the last element is 8 so indeed we got the approximation. It's the taylor series by its singularity. looking at the element in original taylor series deriving 4 times by x and 4 by y is the only term which is with coefficient 1 (all other derivatives exist twice since around $(0,0)$ f is continuous and so its derivatives). So prima facie $$\frac{\partial ^8 f}{\partial x^4\partial y^4}=8!\cdot 1=40320$$ but mypad claims that $\frac{\partial ^8 f}{\partial x^4\partial y^4}=576$. Where am I mistaken?","Given the function $f(x,y)=\frac{1}{1-xy}$ find the value of$\frac{\partial ^8 f}{\partial x^4\partial y^4}(0,0)$. First I developed the function into a taylor series using geometric series around $(0,0)$: $$f(x,y)=\frac{1}{1-xy}=\displaystyle{\sum_{n=0}^\infty(xy)^n}$$. The taylor approximation of order 8 is given by $$f(x,y)\thickapprox\displaystyle{\sum_{n=0}^4(xy)^n}=1+xy+(xy)^2+(xy)^3+(xy)^4$$, where the degree of the last element is 8 so indeed we got the approximation. It's the taylor series by its singularity. looking at the element in original taylor series deriving 4 times by x and 4 by y is the only term which is with coefficient 1 (all other derivatives exist twice since around $(0,0)$ f is continuous and so its derivatives). So prima facie $$\frac{\partial ^8 f}{\partial x^4\partial y^4}=8!\cdot 1=40320$$ but mypad claims that $\frac{\partial ^8 f}{\partial x^4\partial y^4}=576$. Where am I mistaken?",,"['calculus', 'real-analysis']"
47,Stuck on derivation of divergence in cylindrical coordinates,Stuck on derivation of divergence in cylindrical coordinates,,"I'm having a hard time trying to derive the divergence in cylindrical coordinates from its expression in cartesian coordinates $\dfrac {\partial F_{x}} {\partial x}+\dfrac {\partial F_{y}} {\partial y}+\dfrac {\partial F_{z}} {\partial z}$. I'm trying to proceed as follows: from the cartesian coordinate system $(x,y,z)$, defining \begin{align} r&=\sqrt{x^2+y^2} \\ \theta&=\arctan \frac y x \end{align} With the chain rule, we have: \begin{align} \frac \partial {\partial x}&=\left( \frac {\partial r} {\partial x} \frac {\partial} {\partial r} + \frac {\partial \theta} {\partial x} \frac {\partial } {\partial \theta} \right) = \left( \cos \theta \frac {\partial} {\partial r} - \frac {\sin \theta} r \frac {\partial } {\partial \theta} \right) \\ \frac \partial {\partial y}&=\left( \frac {\partial r} {\partial y} \frac {\partial} {\partial r} + \frac {\partial \theta} {\partial y} \frac {\partial } {\partial \theta} \right) = \left( \sin \theta \frac {\partial} {\partial r} + \frac {\cos \theta} r \frac {\partial } {\partial \theta} \right) \end{align} As $F_{x}=F_{r} \cos F_{\theta}$ and $F_{y}=F_{r} \sin F_{\theta}$: \begin{align} \frac {\partial F_{x}} {\partial x} &= \cos \theta \cos F_{\theta} \frac {\partial F_{r}} {\partial r} - F_{r} \sin F_{\theta} \cos \theta \frac {\partial F_{\theta}} {\partial r} - \frac {\cos F_{\theta} \sin \theta} r \frac {\partial F_{r}} {\partial \theta} + \frac {F_{r}} r \sin \theta \sin F_{\theta} \frac {\partial F_{\theta}} {\partial \theta} \\ \frac {\partial F_{y}} {\partial y} &= \sin \theta \sin F_{\theta} \frac {\partial F_{r}} {\partial r} + F_{r} \cos F_{\theta} \sin \theta \frac {\partial F_{\theta}} {\partial r} + \frac {\sin F_{\theta} \cos \theta} r \frac {\partial F_{r}} {\partial \theta} + \frac {F_{r}} r \cos \theta \cos F_{\theta} \frac {\partial F_{\theta}} {\partial \theta} \end{align} However, this sum leads to a weird combination of trigonometric sums and differences of $F_{\theta}$ and $\theta$. It seems to me that if I suppose $F_{\theta}=\theta$. I'll get the usual formula, but I can't see how this is true. edit: I've used a wrong definition for a vector field on cylindrical coordinates. The correct definition is $\vec F=F_{r}\hat r+F_{\theta} \hat \theta + F_{z} \hat e_{z}$, where $\hat r=\cos \theta \hat i + \sin \theta \hat j$ and $\hat \theta=-\sin \theta \hat i + \cos \theta \hat j$. With this definition, the deduction above works.","I'm having a hard time trying to derive the divergence in cylindrical coordinates from its expression in cartesian coordinates $\dfrac {\partial F_{x}} {\partial x}+\dfrac {\partial F_{y}} {\partial y}+\dfrac {\partial F_{z}} {\partial z}$. I'm trying to proceed as follows: from the cartesian coordinate system $(x,y,z)$, defining \begin{align} r&=\sqrt{x^2+y^2} \\ \theta&=\arctan \frac y x \end{align} With the chain rule, we have: \begin{align} \frac \partial {\partial x}&=\left( \frac {\partial r} {\partial x} \frac {\partial} {\partial r} + \frac {\partial \theta} {\partial x} \frac {\partial } {\partial \theta} \right) = \left( \cos \theta \frac {\partial} {\partial r} - \frac {\sin \theta} r \frac {\partial } {\partial \theta} \right) \\ \frac \partial {\partial y}&=\left( \frac {\partial r} {\partial y} \frac {\partial} {\partial r} + \frac {\partial \theta} {\partial y} \frac {\partial } {\partial \theta} \right) = \left( \sin \theta \frac {\partial} {\partial r} + \frac {\cos \theta} r \frac {\partial } {\partial \theta} \right) \end{align} As $F_{x}=F_{r} \cos F_{\theta}$ and $F_{y}=F_{r} \sin F_{\theta}$: \begin{align} \frac {\partial F_{x}} {\partial x} &= \cos \theta \cos F_{\theta} \frac {\partial F_{r}} {\partial r} - F_{r} \sin F_{\theta} \cos \theta \frac {\partial F_{\theta}} {\partial r} - \frac {\cos F_{\theta} \sin \theta} r \frac {\partial F_{r}} {\partial \theta} + \frac {F_{r}} r \sin \theta \sin F_{\theta} \frac {\partial F_{\theta}} {\partial \theta} \\ \frac {\partial F_{y}} {\partial y} &= \sin \theta \sin F_{\theta} \frac {\partial F_{r}} {\partial r} + F_{r} \cos F_{\theta} \sin \theta \frac {\partial F_{\theta}} {\partial r} + \frac {\sin F_{\theta} \cos \theta} r \frac {\partial F_{r}} {\partial \theta} + \frac {F_{r}} r \cos \theta \cos F_{\theta} \frac {\partial F_{\theta}} {\partial \theta} \end{align} However, this sum leads to a weird combination of trigonometric sums and differences of $F_{\theta}$ and $\theta$. It seems to me that if I suppose $F_{\theta}=\theta$. I'll get the usual formula, but I can't see how this is true. edit: I've used a wrong definition for a vector field on cylindrical coordinates. The correct definition is $\vec F=F_{r}\hat r+F_{\theta} \hat \theta + F_{z} \hat e_{z}$, where $\hat r=\cos \theta \hat i + \sin \theta \hat j$ and $\hat \theta=-\sin \theta \hat i + \cos \theta \hat j$. With this definition, the deduction above works.",,['multivariable-calculus']
48,Invariance of Laplacian under Orthogonal transformations,Invariance of Laplacian under Orthogonal transformations,,"Let $F=f\circ \bf{L}$, where $\bf{L}$ is a Linear transformation with matrix $(c^{i}_{j})$ of $dim=n\times n$ with $i$ for rows, and $j$ for columns. $F$ and $f$ are $C^2$ real-valued functions. We know that The 2nd order partial derivatives of $F$ are $\displaystyle F_{ij}=\sum_{k,l=1}^{n}\frac{\partial^2 f}{\partial x_{k}\partial x_{l}}c^{k}_{i}c^{l}_{j}$ for $i,j=1,...,n$. Now assuming that L is orthogonal, I'm asked to prove that $F_{11}+\dots+F_{nn}=f_{11}+\dots+f_{nn}$ (these are also 2nd order partial derivatives). If L is orthogonal, then, with $l_i$ being the $i$-th column of $L$, $\displaystyle L^t L= I\Leftrightarrow l_i \bullet l_j= \begin{matrix}1, i=j \\ 0,i\neq j \end{matrix} \Leftrightarrow \sum_k c^{k}_{i}c^{k}_{j}=\begin{matrix}1, i=j \\ 0,i\neq j \end{matrix}$, and $L L^t=I\Leftrightarrow \sum_k c^{i}_{k}c^{k}_{j}=\begin{matrix}1, i=j \\ 0,i\neq j \end{matrix}$ . (I haven't found useful this last condition, but I put it here, because one never knows when it might give someone an idea on how to solve my problem) $\displaystyle \sum_i F_{ii}=\sum_{i,k,l}f_{l,k}c^{k}_{i}c^{l}_{i}$ Now assuming that $L$ is symmetric, $\displaystyle \sum_{i,k,l}f_{l,k}c^{k}_{i}c^{l}_{i}=\sum_{i,k,l}f_{l,k}c^{i}_{k}c^{i}_{l}$, which is equal to $\displaystyle f_{l,k}$ whenever $k=l$ and zero otherwise, by the first orthogonality condition deduced previously. However, without assuming that $L$ is symmetric, I cannot prove this invariance of the laplacian. The exercise does not require/(allow to assume) symmetry. Where have I gone wrong? Any help is appreciated. ;)","Let $F=f\circ \bf{L}$, where $\bf{L}$ is a Linear transformation with matrix $(c^{i}_{j})$ of $dim=n\times n$ with $i$ for rows, and $j$ for columns. $F$ and $f$ are $C^2$ real-valued functions. We know that The 2nd order partial derivatives of $F$ are $\displaystyle F_{ij}=\sum_{k,l=1}^{n}\frac{\partial^2 f}{\partial x_{k}\partial x_{l}}c^{k}_{i}c^{l}_{j}$ for $i,j=1,...,n$. Now assuming that L is orthogonal, I'm asked to prove that $F_{11}+\dots+F_{nn}=f_{11}+\dots+f_{nn}$ (these are also 2nd order partial derivatives). If L is orthogonal, then, with $l_i$ being the $i$-th column of $L$, $\displaystyle L^t L= I\Leftrightarrow l_i \bullet l_j= \begin{matrix}1, i=j \\ 0,i\neq j \end{matrix} \Leftrightarrow \sum_k c^{k}_{i}c^{k}_{j}=\begin{matrix}1, i=j \\ 0,i\neq j \end{matrix}$, and $L L^t=I\Leftrightarrow \sum_k c^{i}_{k}c^{k}_{j}=\begin{matrix}1, i=j \\ 0,i\neq j \end{matrix}$ . (I haven't found useful this last condition, but I put it here, because one never knows when it might give someone an idea on how to solve my problem) $\displaystyle \sum_i F_{ii}=\sum_{i,k,l}f_{l,k}c^{k}_{i}c^{l}_{i}$ Now assuming that $L$ is symmetric, $\displaystyle \sum_{i,k,l}f_{l,k}c^{k}_{i}c^{l}_{i}=\sum_{i,k,l}f_{l,k}c^{i}_{k}c^{i}_{l}$, which is equal to $\displaystyle f_{l,k}$ whenever $k=l$ and zero otherwise, by the first orthogonality condition deduced previously. However, without assuming that $L$ is symmetric, I cannot prove this invariance of the laplacian. The exercise does not require/(allow to assume) symmetry. Where have I gone wrong? Any help is appreciated. ;)",,['multivariable-calculus']
49,"Let $f(x,y)=\frac{x^3-y^3}{x^2+y^2}$. Is f differentiable in $(0,0)$?",Let . Is f differentiable in ?,"f(x,y)=\frac{x^3-y^3}{x^2+y^2} (0,0)","Let  $$f(x,y)=\frac{x^3-y^3}{x^2+y^2}$$ My solution manual says that this function is not diffb. in $(0,0)$ because it is not linear. Well my problem is that I don't see why this function is linear, and I also don't see why that would imply that $f$ is not differentiable.","Let  $$f(x,y)=\frac{x^3-y^3}{x^2+y^2}$$ My solution manual says that this function is not diffb. in $(0,0)$ because it is not linear. Well my problem is that I don't see why this function is linear, and I also don't see why that would imply that $f$ is not differentiable.",,['multivariable-calculus']
50,Taylor series of a vector field?,Taylor series of a vector field?,,Can someone give me the definition of the Taylor Series of a vector field in $\mathbb{C}^2$? Thanks!,Can someone give me the definition of the Taylor Series of a vector field in $\mathbb{C}^2$? Thanks!,,"['multivariable-calculus', 'definition', 'taylor-expansion']"
51,vector field as integral,vector field as integral,,Define a vector field $ \vec{f}(\vec{R}) = \oint_C{|\vec{r} - \vec{R}|^2 d\vec{r} }$ where C is a simple closed curve. show that there are constant vectors $ \vec{P} $ and $ \vec{Q} $ such that $ \vec{f}(\vec{R}) = \vec{R} \times \vec{P} + \vec{Q} $,Define a vector field $ \vec{f}(\vec{R}) = \oint_C{|\vec{r} - \vec{R}|^2 d\vec{r} }$ where C is a simple closed curve. show that there are constant vectors $ \vec{P} $ and $ \vec{Q} $ such that $ \vec{f}(\vec{R}) = \vec{R} \times \vec{P} + \vec{Q} $,,"['calculus', 'multivariable-calculus', 'vector-analysis']"
52,"Prove the functions are unique in a volume, vector calculus problem","Prove the functions are unique in a volume, vector calculus problem",,"I am working through the following problem, but finding it hard to know where to go. Using the Divergence theorem and the following identities $\nabla \cdot (A \times B) = B\cdot(\nabla \times A) - A\cdot(\nabla \times B)$ $\nabla \times (\nabla \times A) = \nabla (\nabla \cdot A) - \nabla ^2 A$ In a volume V, enclosed by a surface S, the vector fields X and Y satisfy the coupled equations $\nabla \times \nabla \times X = X+Y$ $\nabla \times \nabla \times Y = Y-X$ If the values of $\nabla \times X$ and $\nabla \times Y$ are given on S, show that X and Y are unique in V. I am assuming that I need to show that $\nabla ^2 X$ and $\nabla ^2 Y$ are equal to zero and that X and Y are zero on S to satisfy the uniqueness theorem for Poisson's equation. But am unsure of a good way to get there, so before I write my scribbles if someone could point me in the write direction it would be great. Any help, pointing in the right direction would be very helpful. EDIT: Fixed the second expression, original didn't make sense.","I am working through the following problem, but finding it hard to know where to go. Using the Divergence theorem and the following identities $\nabla \cdot (A \times B) = B\cdot(\nabla \times A) - A\cdot(\nabla \times B)$ $\nabla \times (\nabla \times A) = \nabla (\nabla \cdot A) - \nabla ^2 A$ In a volume V, enclosed by a surface S, the vector fields X and Y satisfy the coupled equations $\nabla \times \nabla \times X = X+Y$ $\nabla \times \nabla \times Y = Y-X$ If the values of $\nabla \times X$ and $\nabla \times Y$ are given on S, show that X and Y are unique in V. I am assuming that I need to show that $\nabla ^2 X$ and $\nabla ^2 Y$ are equal to zero and that X and Y are zero on S to satisfy the uniqueness theorem for Poisson's equation. But am unsure of a good way to get there, so before I write my scribbles if someone could point me in the write direction it would be great. Any help, pointing in the right direction would be very helpful. EDIT: Fixed the second expression, original didn't make sense.",,"['multivariable-calculus', 'vector-analysis']"
53,A rigorous book (or preferrably set of notes) on classic multivariable calculus-analysis?,A rigorous book (or preferrably set of notes) on classic multivariable calculus-analysis?,,"This is different to (Theoretical) Multivariable Calculus Textbooks as I want a classical treatment of line and surface integrals without the notion of a differential form. Prerequisites: Paths, line integrals, the interior of a curve, orientation, surfaces etc. must be rigorously defined . The theorems of Green, Divergence, Stokes must be rigorously proven in some (or all) of their generality. Finally computational aspects of the theory must be kept to a minimum You can suggest any book you want but I would prefer a set of notes (pdf format) accessible for free on the internet. Thank you for your answers","This is different to (Theoretical) Multivariable Calculus Textbooks as I want a classical treatment of line and surface integrals without the notion of a differential form. Prerequisites: Paths, line integrals, the interior of a curve, orientation, surfaces etc. must be rigorously defined . The theorems of Green, Divergence, Stokes must be rigorously proven in some (or all) of their generality. Finally computational aspects of the theory must be kept to a minimum You can suggest any book you want but I would prefer a set of notes (pdf format) accessible for free on the internet. Thank you for your answers",,"['reference-request', 'multivariable-calculus', 'big-list', 'self-learning']"
54,Quasiconcavity for the sum of specific quasiconcave functions,Quasiconcavity for the sum of specific quasiconcave functions,,"I want to show that a function $ψ(a_1,a_0)$, which is separable (additively decomposed) in two quasiconcave functions, is also quasiconcave (QC). I know that the sum of QC functions is not generally QC, but my numerical simulations seem to suggest that $ψ(a_1,a_0)$ is. I have tried using the definition (i.e. trying to show that it has convex upper contour sets by plugging $ψ\big(\lambda a + (1-\lambda)b\big)\geq$... etc), but I can not prove the inequality. I also tried using the border Hessian, but i cannot sign the determinant.  Any other ideas?? Below is a more detailed description of the problem: $$ψ(a_1,a_0)=S(a_1)+S(a_0)$$ Where $a_1 ∈[0,1]$ and $a_0 ∈[0,1]$, and $S(a_j)=F\big(Bh(a_j)\big) \int_{a_j}^{1} \big(t*P(t)\big)dt +  F\big(Bl(a_j)\big)\int_{a_j}^{1} \big(t*(1-P(t))\big)dt$ for $j∈\{1,0\}$ and the functions $F(.), Bl(.), Bh(.)$ and $P(.)$ are strictly increasing. Additionally, $Bl''>0$, and $Bh''<0$. $F''$ and $P''$ can be positive or negative (as required to guarantee quasiconcavity --my simulations suggest that only $P''>0$ is necessary). I would appreciate any hints --including other forums where I could post this question","I want to show that a function $ψ(a_1,a_0)$, which is separable (additively decomposed) in two quasiconcave functions, is also quasiconcave (QC). I know that the sum of QC functions is not generally QC, but my numerical simulations seem to suggest that $ψ(a_1,a_0)$ is. I have tried using the definition (i.e. trying to show that it has convex upper contour sets by plugging $ψ\big(\lambda a + (1-\lambda)b\big)\geq$... etc), but I can not prove the inequality. I also tried using the border Hessian, but i cannot sign the determinant.  Any other ideas?? Below is a more detailed description of the problem: $$ψ(a_1,a_0)=S(a_1)+S(a_0)$$ Where $a_1 ∈[0,1]$ and $a_0 ∈[0,1]$, and $S(a_j)=F\big(Bh(a_j)\big) \int_{a_j}^{1} \big(t*P(t)\big)dt +  F\big(Bl(a_j)\big)\int_{a_j}^{1} \big(t*(1-P(t))\big)dt$ for $j∈\{1,0\}$ and the functions $F(.), Bl(.), Bh(.)$ and $P(.)$ are strictly increasing. Additionally, $Bl''>0$, and $Bh''<0$. $F''$ and $P''$ can be positive or negative (as required to guarantee quasiconcavity --my simulations suggest that only $P''>0$ is necessary). I would appreciate any hints --including other forums where I could post this question",,"['multivariable-calculus', 'optimization', 'convex-analysis']"
55,Universal Correlation measure -- ranking correlations,Universal Correlation measure -- ranking correlations,,"I have time series data of experimental observations for two related processes. I want to measure correlation for use in further analysis. Correlation of the series changes over time and across different length sliding windows on the data.  To clarify, I might want to look at correlation over 10, 20, 30, 40, ..., n periods, each of these essentially sliding windows across the data.  Kind of analogous to looking at a bunch of simple moving average windows. Historically and over future observations, one of these correlation windows will prove a better representation of the data than the rest.  But the random nature of the underlying processes (whose distribution one may not know) makes settling on one window by evaluating the data historically an unsound approach. Universal - A possible approach? Information theory applied to the areas of data compression and portfolio allocation has produced what often gets referred to as a “Universal” approach to attacking a similar problem. Thomas Cover a principle advocate for the idea saw the universal approach as a general method for multi-variate optimization of random processes even where one had no idea of the underlying distribution. Cover's book, Elements of Information Theory , looks at a couple of examples of this idea. Cover seeing this as an optimization technique led me to ask this question here on the Mathematics SE site rather that say the statistics site. Example - Universal data compression In Universal Data Compression, one can see that an internet router can’t know the optimal data compression algorithm to use prior to its actually reading the packet of information or stream of data.  Universal Data Compression addresses the problem with the following steps: Ranks each of its available compression algorithms as to their effectiveness after each stream of data has passed through the router. Calculates the cumulative ranking for each of the compression algorithms Identifies the mean cumulative rank weighted compression algorithm. Uses this mean cumulative rank weighted algorithm on the next stream of data it receives. Note that the approach does not use a follow the leader strategy. Over time this method will asymptopically approach the effectiveness of the best single algorithm that one could have picked at the beginning.  A Darwin quote captures the key idea behind a Universal approach: ""It is not the strongest of the species that survive, nor the most   intelligent, but the one most responsive to change."" Universal approaches keep one in a pretty good place most of the time, which does very well over time. Universal Correlation So it occurred to me that I might develop a related method, Universal Correlation with the following steps: Rank each correlation window’s effectiveness (more about this below) at each time step. Calculate the cumulative ranking for each of the correlation windows Identify the mean cumulative rank weighted correlation window (or the one nearest the mean). I'll then use this in another stage of analysis. Now my question What measure can I use to rank each correlation window’s effectiveness at each time step? ... The following earlier question may have some bearing on an answer: What is a common way to measure the “goodness of fit” of an individual data point to a correlation? This may help clarify: At a given point in time, I have calculated correlation for some number of windows on the data up until that point in time (going back, 10 periods, 20 periods, 30, periods, ...). Now I need a measure to rank the effectiveness or accuracy of each of the time periods correlation relative to each other. Put yet another way: What do I rank the measures of correlation against?","I have time series data of experimental observations for two related processes. I want to measure correlation for use in further analysis. Correlation of the series changes over time and across different length sliding windows on the data.  To clarify, I might want to look at correlation over 10, 20, 30, 40, ..., n periods, each of these essentially sliding windows across the data.  Kind of analogous to looking at a bunch of simple moving average windows. Historically and over future observations, one of these correlation windows will prove a better representation of the data than the rest.  But the random nature of the underlying processes (whose distribution one may not know) makes settling on one window by evaluating the data historically an unsound approach. Universal - A possible approach? Information theory applied to the areas of data compression and portfolio allocation has produced what often gets referred to as a “Universal” approach to attacking a similar problem. Thomas Cover a principle advocate for the idea saw the universal approach as a general method for multi-variate optimization of random processes even where one had no idea of the underlying distribution. Cover's book, Elements of Information Theory , looks at a couple of examples of this idea. Cover seeing this as an optimization technique led me to ask this question here on the Mathematics SE site rather that say the statistics site. Example - Universal data compression In Universal Data Compression, one can see that an internet router can’t know the optimal data compression algorithm to use prior to its actually reading the packet of information or stream of data.  Universal Data Compression addresses the problem with the following steps: Ranks each of its available compression algorithms as to their effectiveness after each stream of data has passed through the router. Calculates the cumulative ranking for each of the compression algorithms Identifies the mean cumulative rank weighted compression algorithm. Uses this mean cumulative rank weighted algorithm on the next stream of data it receives. Note that the approach does not use a follow the leader strategy. Over time this method will asymptopically approach the effectiveness of the best single algorithm that one could have picked at the beginning.  A Darwin quote captures the key idea behind a Universal approach: ""It is not the strongest of the species that survive, nor the most   intelligent, but the one most responsive to change."" Universal approaches keep one in a pretty good place most of the time, which does very well over time. Universal Correlation So it occurred to me that I might develop a related method, Universal Correlation with the following steps: Rank each correlation window’s effectiveness (more about this below) at each time step. Calculate the cumulative ranking for each of the correlation windows Identify the mean cumulative rank weighted correlation window (or the one nearest the mean). I'll then use this in another stage of analysis. Now my question What measure can I use to rank each correlation window’s effectiveness at each time step? ... The following earlier question may have some bearing on an answer: What is a common way to measure the “goodness of fit” of an individual data point to a correlation? This may help clarify: At a given point in time, I have calculated correlation for some number of windows on the data up until that point in time (going back, 10 periods, 20 periods, 30, periods, ...). Now I need a measure to rank the effectiveness or accuracy of each of the time periods correlation relative to each other. Put yet another way: What do I rank the measures of correlation against?",,"['multivariable-calculus', 'optimization', 'information-theory']"
56,Gradient when changing coordinate system,Gradient when changing coordinate system,,"A change of variables from $\vec{r_1}$, $\vec{r_2}$ to $\vec{r}$, $\vec{R}$ is given by: $$ \vec{r} = \vec{r_1}-\vec{r_2}\text{ , }\vec{R}=c_1 \vec{r_1} + c_2 \vec{r_2} $$ I'm supposed to find $\nabla_1$ expressed in terms of $\nabla_r$ and $\nabla_R$. The suggested solution starts out with $$ \vec{R}=(X, Y, Z)\text{ , } \vec{r} = (x, y, z) $$ and then goes on to state that $$ (\nabla_1)_x = \frac{\partial}{\partial x_1} = \frac{\partial X}{\partial x_1} \frac{\partial}{\partial X} + \frac{\partial x}{\partial x_1} \frac{\partial}{\partial x} \text{ , } $$ where does (the second equality of) this last expression come from?","A change of variables from $\vec{r_1}$, $\vec{r_2}$ to $\vec{r}$, $\vec{R}$ is given by: $$ \vec{r} = \vec{r_1}-\vec{r_2}\text{ , }\vec{R}=c_1 \vec{r_1} + c_2 \vec{r_2} $$ I'm supposed to find $\nabla_1$ expressed in terms of $\nabla_r$ and $\nabla_R$. The suggested solution starts out with $$ \vec{R}=(X, Y, Z)\text{ , } \vec{r} = (x, y, z) $$ and then goes on to state that $$ (\nabla_1)_x = \frac{\partial}{\partial x_1} = \frac{\partial X}{\partial x_1} \frac{\partial}{\partial X} + \frac{\partial x}{\partial x_1} \frac{\partial}{\partial x} \text{ , } $$ where does (the second equality of) this last expression come from?",,"['multivariable-calculus', 'coordinate-systems']"
57,How to optimize a rational function,How to optimize a rational function,,"Just a calculus problem: As a function of $K \geq 1$, what is the minimum value of $f/a + f/b + f/c + f/d + f/e$ subject to the following constraints? $$\begin{cases} 1 \leq a \leq c \\ 1 \leq b \leq c \\ 1 \leq d \\ 1 \leq e \\ f = \frac{a^2 b^2 d e}{c} \\ f = K \end{cases}$$ I am fine with a reasonably detailed ""here is how you do this calculus"" answer, or ""here is how to ask wolfram alpha/maple"" answer (that works).  I need to be able to handle variations (the formula for f is always a ""monomial"" but the powers on the variables can change, and the inequalities amongst the $a,b,c,d,e$ variables might change slightly, though all of them are always at least 1). A version I can do: As a function of K ≥ 1, find the minimum value of $f/a + f/b + f/c + f/d + f/e$ subject to the following constraints: $$\begin{cases} 1 \leq a,b,c,d,e \\ f = abcde \\ f = K \end{cases}$$ This version is highly symmetric and I basically understand the region I am optimizing over. Setting any variable to 1 results in a highly non-optimal solution, so the minimum occurs in the middle of the surface where abcde = K , and so the gradient of the objective function is a scalar multiple of the normal to the surface.  Both are very symmetric and the algebra involved in solving them is almost silly.  The answer is the expected $a=b=c=d=e=K^{1/5}$ due to symmetry. Motivation : In the background, $a,b,c,d,e,f$ are all positive odd integers describing the structure of an unknown group.  In the previous incarnations of this problem, I assumed they were real numbers bounded below by 1, and the calculus minimum was in fact the group theory minimum. On the new problem, I asked maple to give the unconstrained (except for the "" f ="" constraints) problem a shot, and it claims there is only one local extrema, and it involves a lot of negative numbers.  I guess that means the minimum is at a ""corner"" (discontinuity of the function defining the boundary of the feasible region), but I have no idea what that means in more than 2 dimensions, and I am a little nervous that such an answer is wrong, at least from the group theory standpoint.","Just a calculus problem: As a function of $K \geq 1$, what is the minimum value of $f/a + f/b + f/c + f/d + f/e$ subject to the following constraints? $$\begin{cases} 1 \leq a \leq c \\ 1 \leq b \leq c \\ 1 \leq d \\ 1 \leq e \\ f = \frac{a^2 b^2 d e}{c} \\ f = K \end{cases}$$ I am fine with a reasonably detailed ""here is how you do this calculus"" answer, or ""here is how to ask wolfram alpha/maple"" answer (that works).  I need to be able to handle variations (the formula for f is always a ""monomial"" but the powers on the variables can change, and the inequalities amongst the $a,b,c,d,e$ variables might change slightly, though all of them are always at least 1). A version I can do: As a function of K ≥ 1, find the minimum value of $f/a + f/b + f/c + f/d + f/e$ subject to the following constraints: $$\begin{cases} 1 \leq a,b,c,d,e \\ f = abcde \\ f = K \end{cases}$$ This version is highly symmetric and I basically understand the region I am optimizing over. Setting any variable to 1 results in a highly non-optimal solution, so the minimum occurs in the middle of the surface where abcde = K , and so the gradient of the objective function is a scalar multiple of the normal to the surface.  Both are very symmetric and the algebra involved in solving them is almost silly.  The answer is the expected $a=b=c=d=e=K^{1/5}$ due to symmetry. Motivation : In the background, $a,b,c,d,e,f$ are all positive odd integers describing the structure of an unknown group.  In the previous incarnations of this problem, I assumed they were real numbers bounded below by 1, and the calculus minimum was in fact the group theory minimum. On the new problem, I asked maple to give the unconstrained (except for the "" f ="" constraints) problem a shot, and it claims there is only one local extrema, and it involves a lot of negative numbers.  I guess that means the minimum is at a ""corner"" (discontinuity of the function defining the boundary of the feasible region), but I have no idea what that means in more than 2 dimensions, and I am a little nervous that such an answer is wrong, at least from the group theory standpoint.",,"['multivariable-calculus', 'nonlinear-optimization']"
58,How to define Surface Laplacian on the sphere with radius 1,How to define Surface Laplacian on the sphere with radius 1,,"The simbol $\nabla_s f$ appears in a problem of my homework, and my professor thinks it means $$\nabla_s f:= \nabla f - \hat{n}(\hat{n} \cdot \nabla f )$$ or  $$ \nabla_s := (I - \hat{n}\hat{n}^T )\nabla $$ (the surface gradient of a function defined on a surface), where $f$ is a scalar field and $\hat{n}$ is the normal  surface vector (in this case the sphere of radius 1) My question is, how can I define the ''surface Laplacian operator'' ($\nabla_s^2f$)  from the above definition? I need to find a way to calculate the following integral $$ \int \int_{S^2} (u \nabla_s^2 v +\nabla_su\cdot\nabla_sv)dS$$ and I don't know how to calculate $\nabla_s^2 v $ for a given scalar field $v$ to (defined over the sphere) Thanks for your help!","The simbol $\nabla_s f$ appears in a problem of my homework, and my professor thinks it means $$\nabla_s f:= \nabla f - \hat{n}(\hat{n} \cdot \nabla f )$$ or  $$ \nabla_s := (I - \hat{n}\hat{n}^T )\nabla $$ (the surface gradient of a function defined on a surface), where $f$ is a scalar field and $\hat{n}$ is the normal  surface vector (in this case the sphere of radius 1) My question is, how can I define the ''surface Laplacian operator'' ($\nabla_s^2f$)  from the above definition? I need to find a way to calculate the following integral $$ \int \int_{S^2} (u \nabla_s^2 v +\nabla_su\cdot\nabla_sv)dS$$ and I don't know how to calculate $\nabla_s^2 v $ for a given scalar field $v$ to (defined over the sphere) Thanks for your help!",,"['multivariable-calculus', 'differential-geometry']"
59,"Prove that $f:\mathbb{R^2} \rightarrow \mathbb{R} $ , $f(x) = \|x\|^2$ is differentiable.","Prove that  ,  is differentiable.",f:\mathbb{R^2} \rightarrow \mathbb{R}  f(x) = \|x\|^2,"The question is the same as the title. All I could churn out was that if the gradient at x was $\nabla f(x)$ then $$\langle\nabla f(x),x\rangle = 2\|x\|^2 .$$ I was also wondering whether this can be generalised from $\mathbb{R^2}$ to $\mathbb{R^n}$ Edit The norm can be any norm","The question is the same as the title. All I could churn out was that if the gradient at x was $\nabla f(x)$ then $$\langle\nabla f(x),x\rangle = 2\|x\|^2 .$$ I was also wondering whether this can be generalised from $\mathbb{R^2}$ to $\mathbb{R^n}$ Edit The norm can be any norm",,"['calculus', 'multivariable-calculus', 'derivatives']"
60,How do you find the limit of $\frac{4x^4 + 5y^4}{x^2 + y^2}$?,How do you find the limit of ?,\frac{4x^4 + 5y^4}{x^2 + y^2},"Find the limit of $\frac{4x^4 + 5y^4}{x^2 + y^2}$ as $(x,y)\to (0,0)$. Which method do I use to find the limit of that? I tried paths but the limits all came out to be $0$... (as a side question, when do you stop trying paths? I mean there are so many ways to try out when $x$ approaches $0$. You can try $y=0$, $y=x$, $y=x^2$, $y=mx$, and so many more ways. After you get like $0$ for 4 limits, do you just stop there and assume to try another method?) (Also, when I try different ways for paths, will the limits always be either $0$ or a finite number and never DNE?) Thank you","Find the limit of $\frac{4x^4 + 5y^4}{x^2 + y^2}$ as $(x,y)\to (0,0)$. Which method do I use to find the limit of that? I tried paths but the limits all came out to be $0$... (as a side question, when do you stop trying paths? I mean there are so many ways to try out when $x$ approaches $0$. You can try $y=0$, $y=x$, $y=x^2$, $y=mx$, and so many more ways. After you get like $0$ for 4 limits, do you just stop there and assume to try another method?) (Also, when I try different ways for paths, will the limits always be either $0$ or a finite number and never DNE?) Thank you",,['multivariable-calculus']
61,"With $x_1^2+3x_2^2+2x_1x_2=32$, find max value of $|x_1-x_2|$","With , find max value of",x_1^2+3x_2^2+2x_1x_2=32 |x_1-x_2|,"With $x_1^2+3x_2^2+2x_1x_2=32$ , find the maximum of $|x_1-x_2|$ . I have tried with AM-GM, but can't solve it. With $d = |x_1-x_2|$ and $d^2 = 2x_1^2 + 4x_2^2 - 32$ , then I wonder how to do next. Thank a lot for helping!","With , find the maximum of . I have tried with AM-GM, but can't solve it. With and , then I wonder how to do next. Thank a lot for helping!",x_1^2+3x_2^2+2x_1x_2=32 |x_1-x_2| d = |x_1-x_2| d^2 = 2x_1^2 + 4x_2^2 - 32,"['calculus', 'multivariable-calculus', 'inequality', 'optimization']"
62,How to show that limit value exists?,How to show that limit value exists?,,"Here I have function $$f(x,y)=\frac{x^3-xy^2}{x^2+y^2}$$ I know that $\lim\limits_{(x,y) \to (0,0)}f(x,y)=0$ because if we put $y=mx$ , then we have $$f(x,mx)=\frac{x(1-m^2)}{1+m^2}$$ $$\Rightarrow~\lim\limits_{x \to 0}f(x,mx)=0 $$ But how to show through $\epsilon-\delta$ definition that limiting value of the function exists. Thanks in advance","Here I have function I know that because if we put , then we have But how to show through definition that limiting value of the function exists. Thanks in advance","f(x,y)=\frac{x^3-xy^2}{x^2+y^2} \lim\limits_{(x,y) \to (0,0)}f(x,y)=0 y=mx f(x,mx)=\frac{x(1-m^2)}{1+m^2} \Rightarrow~\lim\limits_{x \to 0}f(x,mx)=0  \epsilon-\delta","['calculus', 'multivariable-calculus', 'multivalued-functions']"
63,Determining whether a vector field is conservative,Determining whether a vector field is conservative,,"For a vector field $\vec{F}(x,y,z) = \langle F_1(x,y,z), F_2(x,y,z), F_3(x,y,z) \rangle$ in $\mathbb{R}^3$, how can I use mixed second-order partial derivatives of each of the components to determine whether it is conservative? Which partial derivatives should I compare?","For a vector field $\vec{F}(x,y,z) = \langle F_1(x,y,z), F_2(x,y,z), F_3(x,y,z) \rangle$ in $\mathbb{R}^3$, how can I use mixed second-order partial derivatives of each of the components to determine whether it is conservative? Which partial derivatives should I compare?",,"['multivariable-calculus', 'vector-analysis']"
64,How do you express a circle with vector arithmetic?,How do you express a circle with vector arithmetic?,,"I've spent the day learning elementary vector math and I'm curious: how can a circle be represented through vector notation? My textbook doesn't mention it, and Google doesn't seem to help. Thanks! Edit: in case my question doesn't make sense, what I mean by vector math is, e.g. representing a line using notation such as $$r = (a,b,c) + \alpha(d,e,f)$$","I've spent the day learning elementary vector math and I'm curious: how can a circle be represented through vector notation? My textbook doesn't mention it, and Google doesn't seem to help. Thanks! Edit: in case my question doesn't make sense, what I mean by vector math is, e.g. representing a line using notation such as $$r = (a,b,c) + \alpha(d,e,f)$$",,['multivariable-calculus']
65,Does $\left(\frac{\partial f}{\partial x}\right)^2=\frac{\partial^2f}{\partial x^2}$,Does,\left(\frac{\partial f}{\partial x}\right)^2=\frac{\partial^2f}{\partial x^2},"This may be an obvious question but I'm just not thinking straight, thanks The answer must be no","This may be an obvious question but I'm just not thinking straight, thanks The answer must be no",,['multivariable-calculus']
66,Rigorous proof of surface area formula,Rigorous proof of surface area formula,,"The surface integral over surface $S$ (which is given by $z=f(x,y)$ , where $(x,y)$ is a point from the region $D$ in the $xy$ -plane) is: $$ \iint\limits_{S} g(x,y,z)\ dS = \iint\limits_{D} g(x,y,f(x,y))\ {{\sqrt {\,{{\left[ {{f_x}} \right]}^2} + {{\left[ {{f_y}} \right]}^2} + 1} \,dA}}$$ Does there exist a rigorous proof of this formula in real analysis.","The surface integral over surface (which is given by , where is a point from the region in the -plane) is: Does there exist a rigorous proof of this formula in real analysis.","S z=f(x,y) (x,y) D xy  \iint\limits_{S} g(x,y,z)\ dS = \iint\limits_{D} g(x,y,f(x,y))\ {{\sqrt {\,{{\left[ {{f_x}} \right]}^2} + {{\left[ {{f_y}} \right]}^2} + 1} \,dA}}","['real-analysis', 'calculus', 'multivariable-calculus', 'multiple-integral', 'jacobian']"
67,Which points on the curve $5x^2+4xy+2y^2-6=0$ are closest to the origin.,Which points on the curve  are closest to the origin.,5x^2+4xy+2y^2-6=0,"Which points on the curve $5x^2+4xy+2y^2-6=0$ are closest to the origin. I have solved countless of problems like this but this one is just giving me such a hard time. I'm supposed to solve this with Lagrange's method. So I want to minimize $f(x,y)=x^2+y^2$ due to the constraint $g(x,y)=5x^2+4xy+2y^2-6=0$. Ok easy: Find $x,y$ so the following equations are satisfied: $2x+\lambda(10x+4y)=0$ $2y+\lambda(4y+4x)=0$ $5x^2+4xy+2y^2-6=0$ Right? But however i do, i get very complicated equations with root terms to solve, getting me nowhere. I would love to see how you would solve this. Thanks.","Which points on the curve $5x^2+4xy+2y^2-6=0$ are closest to the origin. I have solved countless of problems like this but this one is just giving me such a hard time. I'm supposed to solve this with Lagrange's method. So I want to minimize $f(x,y)=x^2+y^2$ due to the constraint $g(x,y)=5x^2+4xy+2y^2-6=0$. Ok easy: Find $x,y$ so the following equations are satisfied: $2x+\lambda(10x+4y)=0$ $2y+\lambda(4y+4x)=0$ $5x^2+4xy+2y^2-6=0$ Right? But however i do, i get very complicated equations with root terms to solve, getting me nowhere. I would love to see how you would solve this. Thanks.",,"['calculus', 'multivariable-calculus', 'optimization', 'quadratics', 'lagrange-multiplier']"
68,Zero Partials imply Constant Function Theorem or Proof,Zero Partials imply Constant Function Theorem or Proof,,"Let $f(x,y)$ be a function of two variables. Given that  $$\frac{\partial f}{\partial x}=0$$ for all $y$, and  $$\frac{\partial f}{\partial y}=0$$ for all $x$, is there a theorem that states $f(x,y)=$ constant for all $x$, $y$? Or how would I prove this rigorously?","Let $f(x,y)$ be a function of two variables. Given that  $$\frac{\partial f}{\partial x}=0$$ for all $y$, and  $$\frac{\partial f}{\partial y}=0$$ for all $x$, is there a theorem that states $f(x,y)=$ constant for all $x$, $y$? Or how would I prove this rigorously?",,"['multivariable-calculus', 'partial-derivative']"
69,Prove that $\frac{x^2y}{1+x^4+y^2}$ has no global minimum,Prove that  has no global minimum,\frac{x^2y}{1+x^4+y^2},"I am not sure how to approach this problem. The usual methods do not work to find a minimum. I can see that, but how to show that there must not exist a minimum?","I am not sure how to approach this problem. The usual methods do not work to find a minimum. I can see that, but how to show that there must not exist a minimum?",,"['real-analysis', 'multivariable-calculus', 'maxima-minima']"
70,Prove a cubic equation has at least one real root,Prove a cubic equation has at least one real root,,"Show that the cubic eq: $$x^3+ax^2+bx+c = 0 \quad  a,b,c\in \mathbb{R}$$ has at least one real root. I know that the above equation can be broken down into $(x-a)(x-b)(x-c) = 0$ , but I have no idea what to do next. I can't use IVT to do this because I don't have a specified range. (edit): For others reading this, the equation CANNOT be broken down to $(x-a)(x-b)(x-c) = 0$","Show that the cubic eq: $$x^3+ax^2+bx+c = 0 \quad  a,b,c\in \mathbb{R}$$ has at least one real root. I know that the above equation can be broken down into $(x-a)(x-b)(x-c) = 0$ , but I have no idea what to do next. I can't use IVT to do this because I don't have a specified range. (edit): For others reading this, the equation CANNOT be broken down to $(x-a)(x-b)(x-c) = 0$",,"['calculus', 'multivariable-calculus']"
71,Let $f\colon\Bbb{R}^2\to \Bbb{R}$ such that $|f(x)-f(y)|\leq \Vert x-y\Vert^2.$ Prove that $f$ is a constant,Let  such that  Prove that  is a constant,f\colon\Bbb{R}^2\to \Bbb{R} |f(x)-f(y)|\leq \Vert x-y\Vert^2. f,"Edit: Several questions of this type have been asked here before but not on the same domain $\Bbb{R}^2.$ Please, how do I deal with a function of this type or could anyone show me a reference or a similar question with the same domain? Let $f\colon\Bbb{R}^2\to \Bbb{R}$ such that $|f(x)-f(y)|\leq \Vert x-y\Vert^2.$  Prove that $f$ is a constant. What if we assume that $f$ is differentiable. Is there another way of showing that $f$ is a constant?","Edit: Several questions of this type have been asked here before but not on the same domain $\Bbb{R}^2.$ Please, how do I deal with a function of this type or could anyone show me a reference or a similar question with the same domain? Let $f\colon\Bbb{R}^2\to \Bbb{R}$ such that $|f(x)-f(y)|\leq \Vert x-y\Vert^2.$  Prove that $f$ is a constant. What if we assume that $f$ is differentiable. Is there another way of showing that $f$ is a constant?",,"['calculus', 'real-analysis', 'multivariable-calculus', 'holder-spaces', 'holder-inequality']"
72,Why is $(\mathbf{v} \cdot \nabla)\mathbf{v} = (\nabla \times \mathbf{v}) \times \mathbf{v} + \nabla (\frac{1}{2} \mathbf{v}^2)$?,Why is ?,(\mathbf{v} \cdot \nabla)\mathbf{v} = (\nabla \times \mathbf{v}) \times \mathbf{v} + \nabla (\frac{1}{2} \mathbf{v}^2),"The Convective Derivative or Material Derivative is usually written as $\frac{D}{Dt}=\frac{\partial}{\partial t} + \mathbf{v} \cdot \nabla$.  According to MathWorld , this equation, multiplied with ${\bf{v}}$ equals: $$     \frac{D \mathbf{v}}{Dt} = \frac{\partial \mathbf{v}}{\partial t} + (\nabla \times \mathbf{v}) \times \mathbf{v} + \nabla (\frac{1}{2} \mathbf{v}^2) $$ Clearly, it must hold that; $$     (\mathbf{v} \cdot \nabla)\mathbf{v} = (\nabla \times \mathbf{v}) \times \mathbf{v} + \nabla (\frac{1}{2} \mathbf{v}^2) $$ However, I do not spot why this is true. What is the (trivial) identity that I am missing?","The Convective Derivative or Material Derivative is usually written as $\frac{D}{Dt}=\frac{\partial}{\partial t} + \mathbf{v} \cdot \nabla$.  According to MathWorld , this equation, multiplied with ${\bf{v}}$ equals: $$     \frac{D \mathbf{v}}{Dt} = \frac{\partial \mathbf{v}}{\partial t} + (\nabla \times \mathbf{v}) \times \mathbf{v} + \nabla (\frac{1}{2} \mathbf{v}^2) $$ Clearly, it must hold that; $$     (\mathbf{v} \cdot \nabla)\mathbf{v} = (\nabla \times \mathbf{v}) \times \mathbf{v} + \nabla (\frac{1}{2} \mathbf{v}^2) $$ However, I do not spot why this is true. What is the (trivial) identity that I am missing?",,"['multivariable-calculus', 'fluid-dynamics']"
73,Multivariable chain rule with vector valued function,Multivariable chain rule with vector valued function,,"Suppose $f:\mathbb{R}^n \rightarrow \mathbb{R}$ , $\mathbf{g}:\mathbb{R}^n \rightarrow \mathbb{R}^n$ and $\mathbf{x} \in \mathbb{R}^n$ . How do I find a formula for $\nabla (f \circ g)(x)$ ?","Suppose , and . How do I find a formula for ?",f:\mathbb{R}^n \rightarrow \mathbb{R} \mathbf{g}:\mathbb{R}^n \rightarrow \mathbb{R}^n \mathbf{x} \in \mathbb{R}^n \nabla (f \circ g)(x),"['calculus', 'multivariable-calculus', 'derivatives', 'matrix-calculus']"
74,Find the flux of the vector field,Find the flux of the vector field,,"Find the flux of the vector field $F = [x^2,y^2,z^2]$ outward across the given surfaces. Each surface is oriented, unless otherwise specified, with outward-pointing normal pointing away from the origin. the upper hemisphere of radius 2 centered at the origin. the cone $z = 2\sqrt{x^2+y^2}$, $z$ = 0 to 2 with outward normal pointing upward","Find the flux of the vector field $F = [x^2,y^2,z^2]$ outward across the given surfaces. Each surface is oriented, unless otherwise specified, with outward-pointing normal pointing away from the origin. the upper hemisphere of radius 2 centered at the origin. the cone $z = 2\sqrt{x^2+y^2}$, $z$ = 0 to 2 with outward normal pointing upward",,['multivariable-calculus']
75,Find all points on which a function is discontinuous.,Find all points on which a function is discontinuous.,,"$ f(x,y) = \begin{cases} \dfrac{x^3+y^3}{x^2+y^2} &\quad\text{if} [x,y] \neq [0,0]\\[2ex]  0 &\quad\text{if}[x,y] = [0,0]\\ \end{cases} $ The only point it could be discontinuous in is [0,0] . How do I find the limit of the function for $(x,y) \rightarrow (0,0)$? $ \lim_{(x,y) \rightarrow (0,0)} \frac{x^3+y^3}{x^2+y^2} $ seems pretty hard to analyse.","$ f(x,y) = \begin{cases} \dfrac{x^3+y^3}{x^2+y^2} &\quad\text{if} [x,y] \neq [0,0]\\[2ex]  0 &\quad\text{if}[x,y] = [0,0]\\ \end{cases} $ The only point it could be discontinuous in is [0,0] . How do I find the limit of the function for $(x,y) \rightarrow (0,0)$? $ \lim_{(x,y) \rightarrow (0,0)} \frac{x^3+y^3}{x^2+y^2} $ seems pretty hard to analyse.",,"['multivariable-calculus', 'continuity']"
76,Anyone Understand how the chain rule was applied here?,Anyone Understand how the chain rule was applied here?,,Just start from the top with how they applied the chain rule. What have I tried: Google (googling chain rule multivariate function didn't help. Clearly $\frac{df}{dt}=\sum \frac{\partial x_i}{\partial t}\frac{\partial f}{\partial x_i} $ but how they reached their conclusion is beyond me.),Just start from the top with how they applied the chain rule. What have I tried: Google (googling chain rule multivariate function didn't help. Clearly $\frac{df}{dt}=\sum \frac{\partial x_i}{\partial t}\frac{\partial f}{\partial x_i} $ but how they reached their conclusion is beyond me.),,"['calculus', 'multivariable-calculus', 'manifolds', 'chain-rule']"
77,Proving a limit of a multivariable function does not exist,Proving a limit of a multivariable function does not exist,,"Theorem: If $f(x,y)$ approaches two different values as $(x,y)\to (a,b)$ along two different paths in the domain of $f$,   then $\lim_{(x,y)\to(a,b)}f(x)$ does not exist. In class we had a function $f(x,y)$ with $(x,y) \to (0,0)$ and we showed that the limit of $f$ as $x \to 0$ was the same for all linear paths defined by $y=mx$. In contrast, by making the substitution $y=ax^2$ we showed that the limit of $f$ as $x\to 0$ was different for parabolic paths, and therefore that the limit didn't exist. My question is: How is this possible? Why can't we find some line of the form $y=mx$   for any given parabolic path such that these paths both approach the   limit in the same way? Why is it insufficient to prove that a limit exists just by   substituting $y=mx$? If I remember calculus 2 correctly, we can pretty   much approximate any function near $x=0$ with a linear function. So,   what's going on?","Theorem: If $f(x,y)$ approaches two different values as $(x,y)\to (a,b)$ along two different paths in the domain of $f$,   then $\lim_{(x,y)\to(a,b)}f(x)$ does not exist. In class we had a function $f(x,y)$ with $(x,y) \to (0,0)$ and we showed that the limit of $f$ as $x \to 0$ was the same for all linear paths defined by $y=mx$. In contrast, by making the substitution $y=ax^2$ we showed that the limit of $f$ as $x\to 0$ was different for parabolic paths, and therefore that the limit didn't exist. My question is: How is this possible? Why can't we find some line of the form $y=mx$   for any given parabolic path such that these paths both approach the   limit in the same way? Why is it insufficient to prove that a limit exists just by   substituting $y=mx$? If I remember calculus 2 correctly, we can pretty   much approximate any function near $x=0$ with a linear function. So,   what's going on?",,"['multivariable-calculus', 'continuity']"
78,Best Math Plotting Software for Electrical Engineering,Best Math Plotting Software for Electrical Engineering,,"I am an electrical engineering undergrad. I would like to learn a math plotting software which would be helpful in visualizing topics in advanced calculus (my immediate need). It would also be helpful if the math plotting software was of some use in electrical engineering, but this is not mandatory. The selection criteria is listed here in decreasing weight: Ease of Use (syntax and techniques that are intuitive and easy to adapt to other problem areas) Healthy ecosystem (lots of tutorials, examples online, books and other resources Industry use (looking for the most commonly used software suites within engineering and science) Adaptability (commonly used outside mathematics. ie. electrical engineering, modeling). I have narrowed my search down to: Matlab Mathematica Maple But this list is by no means exclusive. Currently I am leaning towards Matlab, because I have seen it being used in upper year courses in my electrical engineering program. I would appreciate your input with regard to which software suite would be best and why. Thank you.","I am an electrical engineering undergrad. I would like to learn a math plotting software which would be helpful in visualizing topics in advanced calculus (my immediate need). It would also be helpful if the math plotting software was of some use in electrical engineering, but this is not mandatory. The selection criteria is listed here in decreasing weight: Ease of Use (syntax and techniques that are intuitive and easy to adapt to other problem areas) Healthy ecosystem (lots of tutorials, examples online, books and other resources Industry use (looking for the most commonly used software suites within engineering and science) Adaptability (commonly used outside mathematics. ie. electrical engineering, modeling). I have narrowed my search down to: Matlab Mathematica Maple But this list is by no means exclusive. Currently I am leaning towards Matlab, because I have seen it being used in upper year courses in my electrical engineering program. I would appreciate your input with regard to which software suite would be best and why. Thank you.",,"['calculus', 'multivariable-calculus', 'matlab', 'maple', 'mathematica']"
79,"Multivariable limit $\lim\limits_{(x,y,z)\to(0,0,0)}\frac{3xyz}{x^2+y^2+z^2}$",Multivariable limit,"\lim\limits_{(x,y,z)\to(0,0,0)}\frac{3xyz}{x^2+y^2+z^2}",Can anybody give me a hint on how to bound  $$\frac{3xyz}{x^2+y^2+z^2}$$ I'm trying to prove that it converges to zero. Thanks!,Can anybody give me a hint on how to bound  $$\frac{3xyz}{x^2+y^2+z^2}$$ I'm trying to prove that it converges to zero. Thanks!,,"['calculus', 'real-analysis', 'multivariable-calculus']"
80,Really nice functional equation with second partial deratives.,Really nice functional equation with second partial deratives.,,"Show that $f:\mathbb{R}^2\to\mathbb{R}$, $f \in C^{2}$ satisfies the equation $$\frac{\partial^2 f}{\partial x^2} - \frac{\partial^2 f}{\partial y^2} = 0$$ for all points $(x,y) \in \mathbb{R}^2$ if and only if for all $(x,y)\in \mathbb{R}^2$ and $t \in \mathbb{R}$ we have: $$f(x, y + 2t) + f(x, y) = f(x + t,y + t) + f(x - t, y +t).$$ Note . In such case, $f$ is said to satisfy the parallelogram's law .","Show that $f:\mathbb{R}^2\to\mathbb{R}$, $f \in C^{2}$ satisfies the equation $$\frac{\partial^2 f}{\partial x^2} - \frac{\partial^2 f}{\partial y^2} = 0$$ for all points $(x,y) \in \mathbb{R}^2$ if and only if for all $(x,y)\in \mathbb{R}^2$ and $t \in \mathbb{R}$ we have: $$f(x, y + 2t) + f(x, y) = f(x + t,y + t) + f(x - t, y +t).$$ Note . In such case, $f$ is said to satisfy the parallelogram's law .",,"['real-analysis', 'multivariable-calculus', 'partial-differential-equations', 'functional-equations', 'partial-derivative']"
81,"Who ""discovered"" that $\nabla^2\left(\frac{1}{r}\right) = - 4\pi\delta^{(3)}(\mathbf{r})$?","Who ""discovered"" that ?",\nabla^2\left(\frac{1}{r}\right) = - 4\pi\delta^{(3)}(\mathbf{r}),"I believe the question title is sufficiently explanatory - who was the first mathematician to announce/discover that $$\nabla^2\left(\frac{1}{r}\right) = - 4\pi\delta^{(3)}(\mathbf{r})$$ where $r = || \mathbf{r}||^{1/2}$ and $\delta^{(3)}( \cdot )$ is the 3-dimensional delta-function ? While I'm tempted to think it was Gauss (as most proofs use the divergence theorem, which I believe was discovered by Gauss), I haven't been able to find any resources which confirm this, so does anyone have an answer? (Additionally, a source would be quite nice)","I believe the question title is sufficiently explanatory - who was the first mathematician to announce/discover that $$\nabla^2\left(\frac{1}{r}\right) = - 4\pi\delta^{(3)}(\mathbf{r})$$ where $r = || \mathbf{r}||^{1/2}$ and $\delta^{(3)}( \cdot )$ is the 3-dimensional delta-function ? While I'm tempted to think it was Gauss (as most proofs use the divergence theorem, which I believe was discovered by Gauss), I haven't been able to find any resources which confirm this, so does anyone have an answer? (Additionally, a source would be quite nice)",,"['multivariable-calculus', 'math-history']"
82,Showing that a function is not differentiable,Showing that a function is not differentiable,,"I want to show that $f(x,y) = \sqrt{|xy|}$ is not differentiable at $0$. So my idea is to show that $g(x,y) = |xy|$ is not differentiable, and then argue that if $f$ were differentiable, then so would $g$ which is the composition of differentiable functions $\cdot^2$ and $g$. But I'm stuck as to how to do this.  In the one variable case, to show that $q(x) = |x|$ is not differentiable, I can calculate the limit $\frac{|x + h| - |x|}{h}$ as $h\to 0^+$ and $h\to 0^-$, show that the two one-sided limits are distinct, and conclude that the limit $$\lim_{h\to 0}\frac{|x + h| - |x|}{h}$$ does not exist. The reason this is easier is that I do not have to have in mind the derivative of the function $q$ in order to calculate it. But in the case of $g(x,y) = |xy|$, to show that $g$ is not differentiable at $0$, I would need to show that there does not exist a linear transformation $\lambda:\mathbb{R}^{2}\to\mathbb{R}$ such that $$\lim_{(h,k)\to (0,0)}\frac{\left||hk| - \lambda(h,k)\right|}{|(h,k)|} = 0$$ I thought of assuming that I had such a $\lambda$, and letting $(h,k)\to (0,0)$ along both $(\sqrt{t},\sqrt{t})$ and $(-\sqrt{t},-\sqrt{t})$ as $t\to 0^{+}$, but this didn't seem to go anywhere constructive.","I want to show that $f(x,y) = \sqrt{|xy|}$ is not differentiable at $0$. So my idea is to show that $g(x,y) = |xy|$ is not differentiable, and then argue that if $f$ were differentiable, then so would $g$ which is the composition of differentiable functions $\cdot^2$ and $g$. But I'm stuck as to how to do this.  In the one variable case, to show that $q(x) = |x|$ is not differentiable, I can calculate the limit $\frac{|x + h| - |x|}{h}$ as $h\to 0^+$ and $h\to 0^-$, show that the two one-sided limits are distinct, and conclude that the limit $$\lim_{h\to 0}\frac{|x + h| - |x|}{h}$$ does not exist. The reason this is easier is that I do not have to have in mind the derivative of the function $q$ in order to calculate it. But in the case of $g(x,y) = |xy|$, to show that $g$ is not differentiable at $0$, I would need to show that there does not exist a linear transformation $\lambda:\mathbb{R}^{2}\to\mathbb{R}$ such that $$\lim_{(h,k)\to (0,0)}\frac{\left||hk| - \lambda(h,k)\right|}{|(h,k)|} = 0$$ I thought of assuming that I had such a $\lambda$, and letting $(h,k)\to (0,0)$ along both $(\sqrt{t},\sqrt{t})$ and $(-\sqrt{t},-\sqrt{t})$ as $t\to 0^{+}$, but this didn't seem to go anywhere constructive.",,['multivariable-calculus']
83,"$x(t)= a\cos(t)$ , $y(t)= b\sin(t)$ in terms of the arc length $S$",",  in terms of the arc length",x(t)= a\cos(t) y(t)= b\sin(t) S,"I'm trying to parameterize the ellipse $x(t)= a\cos(t)$ , $y(t)= b\sin(t)$ in terms of the arc length $S$ but I don't know how to do it. Supposing that $\gamma:[a,b]\to \mathbb{R}$ is a smooth curve  with $\gamma'(t)\neq 0$ for $t\in [a,b]$ , I know that $s(t)$ = $\int_{a}^{t}\left\|  \gamma'(\psi)\right\|d\psi$ for $t\in [a,b]$ then I find the inverse funtion of $s$ . Can anybody help me find a way to express the parameterization for the ellipse? I’m looking for a solution in terms of sine amplitude and cosine amplitude.","I'm trying to parameterize the ellipse , in terms of the arc length but I don't know how to do it. Supposing that is a smooth curve  with for , I know that = for then I find the inverse funtion of . Can anybody help me find a way to express the parameterization for the ellipse? I’m looking for a solution in terms of sine amplitude and cosine amplitude.","x(t)= a\cos(t) y(t)= b\sin(t) S \gamma:[a,b]\to \mathbb{R} \gamma'(t)\neq 0 t\in [a,b] s(t) \int_{a}^{t}\left\|  \gamma'(\psi)\right\|d\psi t\in [a,b] s","['real-analysis', 'multivariable-calculus', 'differential-geometry', 'parametrization']"
84,"If $\frac{1}{1+a}+\frac{1}{1+b}+\frac{1}{1+c}\le 1$, prove that $(1+a^2)(1+b^2)(1+c^2)\ge 125$.","If , prove that .",\frac{1}{1+a}+\frac{1}{1+b}+\frac{1}{1+c}\le 1 (1+a^2)(1+b^2)(1+c^2)\ge 125,"QUESTION: Let $a,b,c$ be positive real numbers such that $$\cfrac{1}{1+a}+\cfrac{1}{1+b}+\cfrac{1}{1+c}\le 1$$ Prove that $$(1+a^2)(1+b^2)(1+c^2)\ge 125$$ When does equality hold? MY APPROACH: Firstly, let's try to squeeze out all the information we can from what is given. $$\frac{(1+b)(1+c)+(1+a)(1+c)+(1+a)(1+b)}{(1+a)(1+b)(1+c)}≤1$$ Multiplying this out, we get $$3+2(a+b+c)+(ab+bc+ca)≤1+(a+b+c)+(ab+bc+ca)+abc$$ $$\implies 2+(a+b+c)≤abc$$ Also, since $$1≥\sum_{cyc}\frac{1}{1+a}$$ Therefore by AM-GM, $$1≥\sum_{cyc}\frac{3}{\sqrt[3]{(1+a)(1+b)(1+c)}}$$ $$\implies (1+a)(1+b)(1+c)≥27$$ That's all I ended up in.. At first, I thought Hölder's inequality could be employed, but that too requires the sum of the powers to be $=1$ .. and that is not going to be useful in $(1+a^2)(1+b^2)(1+c^2)$ , since here the sum of powers add up to $3$ .. I don't know what to do next.. Any help will be much appreciated..","QUESTION: Let be positive real numbers such that Prove that When does equality hold? MY APPROACH: Firstly, let's try to squeeze out all the information we can from what is given. Multiplying this out, we get Also, since Therefore by AM-GM, That's all I ended up in.. At first, I thought Hölder's inequality could be employed, but that too requires the sum of the powers to be .. and that is not going to be useful in , since here the sum of powers add up to .. I don't know what to do next.. Any help will be much appreciated..","a,b,c \cfrac{1}{1+a}+\cfrac{1}{1+b}+\cfrac{1}{1+c}\le 1 (1+a^2)(1+b^2)(1+c^2)\ge 125 \frac{(1+b)(1+c)+(1+a)(1+c)+(1+a)(1+b)}{(1+a)(1+b)(1+c)}≤1 3+2(a+b+c)+(ab+bc+ca)≤1+(a+b+c)+(ab+bc+ca)+abc \implies 2+(a+b+c)≤abc 1≥\sum_{cyc}\frac{1}{1+a} 1≥\sum_{cyc}\frac{3}{\sqrt[3]{(1+a)(1+b)(1+c)}} \implies (1+a)(1+b)(1+c)≥27 =1 (1+a^2)(1+b^2)(1+c^2) 3","['multivariable-calculus', 'inequality', 'a.m.-g.m.-inequality', 'tangent-line-method']"
85,"Show function $f(x,y)=(x^2-y^2,2xy)$ is $1$-$1$ by Inverse Function Theorem",Show function  is - by Inverse Function Theorem,"f(x,y)=(x^2-y^2,2xy) 1 1","I'm trying to prove the problem below - which comes from Munkres' ""Analysis on Manifolds"" book in the section on the Inverse function theorem. Since its in the chapter on the Inverse Function Theorem I figured I'd start by showing that $f$ satisfies the conditions of the theorem.  Writing the Jacobian shows that it's both $C^r$ and we get $\det f'(x,y)=4x^2+4y^2\neq0$ when $x>0$.  So we can apply the theorem but I'm unsure of how to proceed to show that $f$ is $1$-$1$.  And I didn't see how to use the hint they provided. Thanks! Let $f\colon \mathbf R^2\to \mathbf R^2$ be defined by the equation   $$f(x,y)=(x^2-y^2,2xy).$$   (a) Show that $f$ is one-to-one on the set of all $(x,y)$ with $x>0$. [Hint: If $f(x,y)=f(a,b)$, then $\|f(x,y)\|=\|f(a,b)\|$.]","I'm trying to prove the problem below - which comes from Munkres' ""Analysis on Manifolds"" book in the section on the Inverse function theorem. Since its in the chapter on the Inverse Function Theorem I figured I'd start by showing that $f$ satisfies the conditions of the theorem.  Writing the Jacobian shows that it's both $C^r$ and we get $\det f'(x,y)=4x^2+4y^2\neq0$ when $x>0$.  So we can apply the theorem but I'm unsure of how to proceed to show that $f$ is $1$-$1$.  And I didn't see how to use the hint they provided. Thanks! Let $f\colon \mathbf R^2\to \mathbf R^2$ be defined by the equation   $$f(x,y)=(x^2-y^2,2xy).$$   (a) Show that $f$ is one-to-one on the set of all $(x,y)$ with $x>0$. [Hint: If $f(x,y)=f(a,b)$, then $\|f(x,y)\|=\|f(a,b)\|$.]",,"['calculus', 'real-analysis', 'multivariable-calculus', 'functions', 'inverse-function-theorem']"
86,Compute $\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-(x^2+(x-y)^2+y^2)}dxdy$,Compute,\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-(x^2+(x-y)^2+y^2)}dxdy,"Compute $\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-(x^2+(x-y)^2+y^2)}dxdy$. I tried to do this by using polar coordinate.  Let $x=r\cos t,\ y=r\sin t$, and then $$ \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-(x^2+(x-y)^2+y^2)}dxdy=\int_{0}^{2\pi}\int_{0}^{\infty}e^{-2r^2}e^{2\cos t\sin t}rdrdt=\int_{0}^{2\pi}e^{\sin(2t)}\int_{0}^{\infty}e^{-2r^2}rdrdt=\frac{1}{4}\int_{0}^{2\pi}e^{\sin (2t)}dt. $$ But, I have no idea how to compute $\int_{0}^{2\pi}e^{\sin (2t)}dt$. Please give me some hint or suggestion. Thanks.","Compute $\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-(x^2+(x-y)^2+y^2)}dxdy$. I tried to do this by using polar coordinate.  Let $x=r\cos t,\ y=r\sin t$, and then $$ \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-(x^2+(x-y)^2+y^2)}dxdy=\int_{0}^{2\pi}\int_{0}^{\infty}e^{-2r^2}e^{2\cos t\sin t}rdrdt=\int_{0}^{2\pi}e^{\sin(2t)}\int_{0}^{\infty}e^{-2r^2}rdrdt=\frac{1}{4}\int_{0}^{2\pi}e^{\sin (2t)}dt. $$ But, I have no idea how to compute $\int_{0}^{2\pi}e^{\sin (2t)}dt$. Please give me some hint or suggestion. Thanks.",,"['real-analysis', 'multivariable-calculus', 'definite-integrals']"
87,Is a norm in $\mathbb{R}^d$ increasing in every coordinate?,Is a norm in  increasing in every coordinate?,\mathbb{R}^d,Consider any norm $\|\cdot\|$ in the $d$ dimensional Euclidean space and consider a fix point there $x$ such that all of its coordinates are positive. Let $e_k$ some vector in the standard basis of this space and consider the function $t \mapsto \|x +te_k\|$ for non negative $t$. Is this function increasing?,Consider any norm $\|\cdot\|$ in the $d$ dimensional Euclidean space and consider a fix point there $x$ such that all of its coordinates are positive. Let $e_k$ some vector in the standard basis of this space and consider the function $t \mapsto \|x +te_k\|$ for non negative $t$. Is this function increasing?,,"['multivariable-calculus', 'vector-spaces', 'vector-analysis']"
88,Why are open and simply connected required for $\frac{\delta P}{\delta y} = \frac{\delta Q}{\delta x}$ to imply a conservative field?,Why are open and simply connected required for  to imply a conservative field?,\frac{\delta P}{\delta y} = \frac{\delta Q}{\delta x},"A theorem concerning line integrals runs as follows: I want to develop some intuition as to why this theorem depends on $D$ being open and simply-connected. My thoughts: $D$ being closed (that is, intuitively, if the region had a boundary) should have no bearing on whether $F$ is conservative. A conservative vector field could still exist on boundary points (I can still run around the boundary in two paths from an incident point to a final one and aggregate some equal quantity). If $D$ were not simply connected by, say, containing a hole inside of it, the hole wouldn't be a part of $D$. This means that $\frac{\delta P}{\delta y} = \frac{\delta Q}{\delta x}$  being true still implies a conservative field at all other points (which, collectively form D). Hence, $F$  can still be considered conservative.","A theorem concerning line integrals runs as follows: I want to develop some intuition as to why this theorem depends on $D$ being open and simply-connected. My thoughts: $D$ being closed (that is, intuitively, if the region had a boundary) should have no bearing on whether $F$ is conservative. A conservative vector field could still exist on boundary points (I can still run around the boundary in two paths from an incident point to a final one and aggregate some equal quantity). If $D$ were not simply connected by, say, containing a hole inside of it, the hole wouldn't be a part of $D$. This means that $\frac{\delta P}{\delta y} = \frac{\delta Q}{\delta x}$  being true still implies a conservative field at all other points (which, collectively form D). Hence, $F$  can still be considered conservative.",,"['multivariable-calculus', 'vectors', 'vector-fields']"
89,Gradient steepest direction and normal to surface?,Gradient steepest direction and normal to surface?,,"From this Maths SE question , I now understand the gradient to be the directional derivative that returns the steepest slope at a point. However, reading my textbooks, they all say that the gradient is normal to the tangent plane. In the above SE question, the gradient sounds like it should be parallel to a direction in the $xy$ plane, not actually up on the surface. My question is therefore do I have a misunderstanding of the tangent plane, gradient or directional derivative? In addition, I question how the gradient always passes through the origin or the $xy$ plane? If for instance in the graphic, the gradient had been another direction (say horizontal), then it wouldn't pass through the origin! In addition, the gradient of other points in the graphic would satisfy this example. Many thanks!","From this Maths SE question , I now understand the gradient to be the directional derivative that returns the steepest slope at a point. However, reading my textbooks, they all say that the gradient is normal to the tangent plane. In the above SE question, the gradient sounds like it should be parallel to a direction in the $xy$ plane, not actually up on the surface. My question is therefore do I have a misunderstanding of the tangent plane, gradient or directional derivative? In addition, I question how the gradient always passes through the origin or the $xy$ plane? If for instance in the graphic, the gradient had been another direction (say horizontal), then it wouldn't pass through the origin! In addition, the gradient of other points in the graphic would satisfy this example. Many thanks!",,"['multivariable-calculus', 'vectors']"
90,Minimizing $x^2+y^2+z^2$ subject to $xy -z + 1 = 0$ via Lagrange multipliers,Minimizing  subject to  via Lagrange multipliers,x^2+y^2+z^2 xy -z + 1 = 0,"$$\begin{array}{ll} \text{minimize} & f(x,y,z) := x^2 + y^2 + z^2\\ \text{subject to} & g(x,y,z) := xy - z + 1 = 0\end{array}$$ I tried the Lagrange multipliers method and the system resulted from has no solution. So I posted it to see if the question is wrong by itself or I'm missing something. So I made the Lagrangian equation $L(x,y,z,λ)=x^2 + y^2 + z^2 + λ(xy -z+1)$ and then $θL/θx = 2x + λy =0$ $θL/θy = 2y + λx =0$ $θL/θz = 2z - λ =0$ $θL/θλ = xy -z +1 =0 $ The obvious solution for that system is x=0 , y=0 , z=1 and λ=2 But solving it in an online solver for nonlinear systems of equation the answer I get is that it's unsolvable. So my question is: What I'm doing wrong","I tried the Lagrange multipliers method and the system resulted from has no solution. So I posted it to see if the question is wrong by itself or I'm missing something. So I made the Lagrangian equation and then The obvious solution for that system is x=0 , y=0 , z=1 and λ=2 But solving it in an online solver for nonlinear systems of equation the answer I get is that it's unsolvable. So my question is: What I'm doing wrong","\begin{array}{ll} \text{minimize} & f(x,y,z) := x^2 + y^2 + z^2\\ \text{subject to} & g(x,y,z) := xy - z + 1 = 0\end{array} L(x,y,z,λ)=x^2 + y^2 + z^2 + λ(xy -z+1) θL/θx = 2x + λy =0 θL/θy = 2y + λx =0 θL/θz = 2z - λ =0 θL/θλ = xy -z +1 =0 ","['multivariable-calculus', 'optimization', 'lagrange-multiplier', 'qcqp']"
91,What's the cross product in 2 dimensions? [duplicate],What's the cross product in 2 dimensions? [duplicate],,"This question already has answers here : Is the vector cross product only defined for 3D? (7 answers) Closed 5 years ago . The math book i'm using states that the cross product for two vectors is defined over $R^3$ : $$u = (a,b,c)$$ $$v = (d,e,f)$$ is: $$u \times v = \begin{vmatrix} \hat{i} & \hat{j} & \hat{k} \\ a & b & c \\ d & e & f \\ \end{vmatrix} $$ and the direction of the resultant is determined by curling fingers from vector v to u with thumb pointing in direction of the cross product of u x v. Out of curiosity, what's the cross product if u and v are defined over $R^2$ instead of $R^3$ instead: $$u = (a,b)$$ $$v = (d,e)$$ Is there a ""degenerate"" case for the cross product of $R^2$ instead $R^3$ ?  like this is some type of 2x2 determinant instead? for instance if had a parameterization: $$\Phi(u,\ v) = (\ f(u),\ \ g(v)\ )$$ and needed to calculate in $R^2$ : $$ D = \Bigg| \frac{\partial{\Phi}}{\partial{u}} \times \frac{\partial{\Phi}}{\partial{v}} \Bigg| $$ There are plenty of examples in the book for calculating the determinate D in $R^3$ but none at all for $R^2$ case. As in: $$ \iint_{V} f(x,y) dx\ dy = \iint_{Q} f(\Phi(u,v) \Bigg| \frac{\partial{\Phi}}{\partial{u}} \times \frac{\partial{\Phi}}{\partial{v}} \Bigg| $$ $$ \Phi(u,v)=(2u \cos v,\ \ u \sin v) $$","This question already has answers here : Is the vector cross product only defined for 3D? (7 answers) Closed 5 years ago . The math book i'm using states that the cross product for two vectors is defined over : is: and the direction of the resultant is determined by curling fingers from vector v to u with thumb pointing in direction of the cross product of u x v. Out of curiosity, what's the cross product if u and v are defined over instead of instead: Is there a ""degenerate"" case for the cross product of instead ?  like this is some type of 2x2 determinant instead? for instance if had a parameterization: and needed to calculate in : There are plenty of examples in the book for calculating the determinate D in but none at all for case. As in:","R^3 u = (a,b,c) v = (d,e,f) u \times v = \begin{vmatrix}
\hat{i} & \hat{j} & \hat{k} \\
a & b & c \\
d & e & f \\
\end{vmatrix}
 R^2 R^3 u = (a,b) v = (d,e) R^2 R^3 \Phi(u,\ v) = (\ f(u),\ \ g(v)\ ) R^2 
D = \Bigg| \frac{\partial{\Phi}}{\partial{u}} \times \frac{\partial{\Phi}}{\partial{v}} \Bigg|
 R^3 R^2 
\iint_{V} f(x,y) dx\ dy = \iint_{Q} f(\Phi(u,v) \Bigg| \frac{\partial{\Phi}}{\partial{u}} \times \frac{\partial{\Phi}}{\partial{v}} \Bigg|
 
\Phi(u,v)=(2u \cos v,\ \ u \sin v)
","['multivariable-calculus', 'vectors']"
92,Maximizing $f$ in $\mathbb{R}^3$,Maximizing  in,f \mathbb{R}^3,"Find the domain and the maximum value that the function $$f(x,y,z)=\frac{x+2y+3z}{\sqrt{x^2+y^2+z^2}}$$ may attain in its domain. I have found the domain of the function to be $\mathbb{R^3\backslash\mathbf{0}}$ . To maximize I differentiated in terms of $x,y,z$ having $$f_x=\frac{-2 x y-3 x z+y^2+z^2}{\left(x^2+y^2+z^2\right)^{3/2}},\quad f_y=\frac{2 x^2-x y+z (2 z-3 y)}{\left(x^2+y^2+z^2\right)^{3/2}},\quad f_z=\frac{3 \left(x^2+y^2\right)-z (x+2 y)}{\left(x^2+y^2+z^2\right)^{3/2}}$$ But to solve the system $f_x=0,f_y=0$ and $f_z=0$ is rather hard. What are the plausible values of $x,y,z$ ?",Find the domain and the maximum value that the function may attain in its domain. I have found the domain of the function to be . To maximize I differentiated in terms of having But to solve the system and is rather hard. What are the plausible values of ?,"f(x,y,z)=\frac{x+2y+3z}{\sqrt{x^2+y^2+z^2}} \mathbb{R^3\backslash\mathbf{0}} x,y,z f_x=\frac{-2 x y-3 x z+y^2+z^2}{\left(x^2+y^2+z^2\right)^{3/2}},\quad f_y=\frac{2 x^2-x y+z (2 z-3 y)}{\left(x^2+y^2+z^2\right)^{3/2}},\quad f_z=\frac{3 \left(x^2+y^2\right)-z (x+2 y)}{\left(x^2+y^2+z^2\right)^{3/2}} f_x=0,f_y=0 f_z=0 x,y,z","['multivariable-calculus', 'optimization', 'partial-derivative', 'maxima-minima', 'cauchy-schwarz-inequality']"
93,Differentials in Multivariable Calculus,Differentials in Multivariable Calculus,,"Does the idea of composing/decomposing the fraction notation of the derivative from/into differentials apply in multivariable calculus?  I realize that this practice is considered non-standard and many don't like it even in single variable calculus, but we can multiply both sides of $dy/dx = f'(x)$ by $dx$ yielding $dy = f'(x)dx$ , and invert the process by dividing both sides by $dx$ to return to the original equation. In the multivariable world, differentials are more diverse.  Instead of having only one dimension in which to nudge the input value of a function, we can take an indefinitely small step in an infinite number of directions, a prime candidate being ∂x, a tiny nudge in the direction of the x-axis, as shown. Similarly, $∂y$ , $∂z$ , etc., represent nudges parallel to the relevant input axes.  However, the formula one comes across for the differential of $f$ in multivariable calculus is interesting. Firstly, ""full differentials"" ( $df$ , $dy$ , etc.) are present, whereas one might expect to find ""partial differentials"" ( $∂f$ , $∂y$ , etc.).  What is the meaning of full differentials in multivariable calculus?  Secondly, dividing through by one of the differentials would yield an equation whose truth isn't obvious.  For example, dividing by $dy$ results in $df/dy = f_x dx/dy + f_y + f_z dz/dy$ .  Again, it's not clear what $df/dy$ means when $f$ has a three variable input.  Does this invite thinking about the inputs as living on three non-continuous number lines rather than in a single three-dimensional space, whereas $∂f/∂y$ would indicate the latter?  What about $dx/dy$ and $dz/dy$ , which are input-to-input nudge ratios; are these concerning since they don't involve the output of the function at all (i.e., it might not be possible for $z$ to be a function of $x$ )?  Is this equation and the others like it valid, and are they useful?","Does the idea of composing/decomposing the fraction notation of the derivative from/into differentials apply in multivariable calculus?  I realize that this practice is considered non-standard and many don't like it even in single variable calculus, but we can multiply both sides of by yielding , and invert the process by dividing both sides by to return to the original equation. In the multivariable world, differentials are more diverse.  Instead of having only one dimension in which to nudge the input value of a function, we can take an indefinitely small step in an infinite number of directions, a prime candidate being ∂x, a tiny nudge in the direction of the x-axis, as shown. Similarly, , , etc., represent nudges parallel to the relevant input axes.  However, the formula one comes across for the differential of in multivariable calculus is interesting. Firstly, ""full differentials"" ( , , etc.) are present, whereas one might expect to find ""partial differentials"" ( , , etc.).  What is the meaning of full differentials in multivariable calculus?  Secondly, dividing through by one of the differentials would yield an equation whose truth isn't obvious.  For example, dividing by results in .  Again, it's not clear what means when has a three variable input.  Does this invite thinking about the inputs as living on three non-continuous number lines rather than in a single three-dimensional space, whereas would indicate the latter?  What about and , which are input-to-input nudge ratios; are these concerning since they don't involve the output of the function at all (i.e., it might not be possible for to be a function of )?  Is this equation and the others like it valid, and are they useful?",dy/dx = f'(x) dx dy = f'(x)dx dx ∂y ∂z f df dy ∂f ∂y dy df/dy = f_x dx/dy + f_y + f_z dz/dy df/dy f ∂f/∂y dx/dy dz/dy z x,"['multivariable-calculus', 'partial-derivative', 'vector-analysis', 'intuition', 'differential']"
94,Difference between magnitude of gradient vs directional derivative of gradient,Difference between magnitude of gradient vs directional derivative of gradient,,"I've read that the directional derivative is the rate of change of a function $f$ in a given direction $\mathbf{v}$, given as $\nabla f\cdot \mathbf{v}$. I've also read (perhaps incorrectly) that the magnitude of the gradient also tells us the rate of change. If so, what does the directional derivative of the gradient, i.e. $\nabla f\cdot \nabla f$ tell us?","I've read that the directional derivative is the rate of change of a function $f$ in a given direction $\mathbf{v}$, given as $\nabla f\cdot \mathbf{v}$. I've also read (perhaps incorrectly) that the magnitude of the gradient also tells us the rate of change. If so, what does the directional derivative of the gradient, i.e. $\nabla f\cdot \nabla f$ tell us?",,['multivariable-calculus']
95,Directional directive vs partial derivative,Directional directive vs partial derivative,,"Can someone clarify the relationship between directional derivative and partial derivatives for a function from $\mathbb{R}^n$ to $\mathbb{R}$? To my understanding, if the function is continuously differentiable, then both directional and partial derivatives exist. Is that correct? Consider this function:  \begin{align} f(x,y) =  \begin{cases}       \sin( \frac{y^2}{x})\sqrt{x^2 + y^2}& x \ne 0 \\       0 &  x = 0    \end{cases} \end{align} How would one verify this function has direction derivatives at $(0,0)$?","Can someone clarify the relationship between directional derivative and partial derivatives for a function from $\mathbb{R}^n$ to $\mathbb{R}$? To my understanding, if the function is continuously differentiable, then both directional and partial derivatives exist. Is that correct? Consider this function:  \begin{align} f(x,y) =  \begin{cases}       \sin( \frac{y^2}{x})\sqrt{x^2 + y^2}& x \ne 0 \\       0 &  x = 0    \end{cases} \end{align} How would one verify this function has direction derivatives at $(0,0)$?",,"['real-analysis', 'multivariable-calculus']"
96,Evaluation of $\iint_D \frac {\ln(2 - \sin \xi \cos \eta)\sin \xi} {2 - 2\sin \xi \cos \eta + \sin^2 \xi \cos^2 \eta} \mathrm d\xi \; \mathrm d\eta$,Evaluation of,\iint_D \frac {\ln(2 - \sin \xi \cos \eta)\sin \xi} {2 - 2\sin \xi \cos \eta + \sin^2 \xi \cos^2 \eta} \mathrm d\xi \; \mathrm d\eta,"Evaluate the following integral: $$\iint_D \frac {\ln(2 - \sin \xi \cos \eta)\sin \xi} {2 - 2\sin \xi \cos \eta + \sin^2 \xi \cos^2 \eta}  \mathrm d\xi \; \mathrm d\eta$$ where $D = [ 0, \pi/2] \times [ 0, \pi / 2]$. My observations: The integral can be rewritten as $$\iint_D \sin \xi f(\sin \xi \cos \eta) \mathrm d \xi \mathrm d \eta$$ where $f(x) = \frac  {\ln (2-x)} {(x-1)^2 + 1}$ Substituting $\mu = \pi / 2 - \xi$,  $\nu = \pi / 2 - \eta$ Substituting $x = \sin \eta \cos \xi, y = \sin \eta \sin \xi$ Expanding in terms of series Source: The problem appeared in The American Mathematical Monthly","Evaluate the following integral: $$\iint_D \frac {\ln(2 - \sin \xi \cos \eta)\sin \xi} {2 - 2\sin \xi \cos \eta + \sin^2 \xi \cos^2 \eta}  \mathrm d\xi \; \mathrm d\eta$$ where $D = [ 0, \pi/2] \times [ 0, \pi / 2]$. My observations: The integral can be rewritten as $$\iint_D \sin \xi f(\sin \xi \cos \eta) \mathrm d \xi \mathrm d \eta$$ where $f(x) = \frac  {\ln (2-x)} {(x-1)^2 + 1}$ Substituting $\mu = \pi / 2 - \xi$,  $\nu = \pi / 2 - \eta$ Substituting $x = \sin \eta \cos \xi, y = \sin \eta \sin \xi$ Expanding in terms of series Source: The problem appeared in The American Mathematical Monthly",,"['calculus', 'multivariable-calculus', 'definite-integrals', 'contest-math']"
97,Lagrange multipliers on manifolds in Lee's book,Lagrange multipliers on manifolds in Lee's book,,"Here is Problem 11-11 on page 301 of John Lee’s book: Let $ M $ be a smooth manifold, and $ C \subset M $ be an embedded sub-manifold. Let $ f \in {C^{\infty}}(M) $ , and suppose $ p \in C $ is a point at which $ f $ attains a local maximum or minimum value among points in $ C $ . Given a smooth local defining function $ \Phi: U \to \mathbb{R}^{k} $ for $ C $ on a neighborhood $ U $ of $ p $ in $ M $ , there are real numbers $ \lambda_{1},\ldots,\lambda_{k} $ (called Lagrange multipliers ) such that $$ \mathrm{d} f_{p} = \sum_{i = 1}^{k} \lambda_{i} \cdot \mathrm{d} \Phi^{i}|_{p}. $$ I got confused when I was trying to solve it. Here are my questions: (1) He didn’t say anything about the dimension of $ M $ and $ C $ , nor did he put corrections here . Is it necessary to assume that $ \operatorname{dim}(M) = n > k $ and $ \operatorname{dim}(C) = n - k $ , or do these results implicitly follow from the conditions of this problem? (2) Why do we need the condition that $ C $ is an embedded sub-manifold? Assume $ \operatorname{dim}(C) = n - k $ ; then Theorem 5.8 on page 102 tells us that $ C $ satisfies the local $ k $ -slice condition (this is the only theorem I can think of that is related to this condition), but what good can this condition do for us? (3) I think I need to apply the Lagrange Multiplier Theorem (see page 113) in multi-variable calculus, but we need to make sure that the rank of the Jacobian matrix of $ \Phi $ is of rank $ k $ at the point $ p $ . However, there aren’t any extra conditions on $ \Phi $ . You can either answer my questions separately or show me a detailed proof of it. Thank you in advance!","Here is Problem 11-11 on page 301 of John Lee’s book: Let be a smooth manifold, and be an embedded sub-manifold. Let , and suppose is a point at which attains a local maximum or minimum value among points in . Given a smooth local defining function for on a neighborhood of in , there are real numbers (called Lagrange multipliers ) such that I got confused when I was trying to solve it. Here are my questions: (1) He didn’t say anything about the dimension of and , nor did he put corrections here . Is it necessary to assume that and , or do these results implicitly follow from the conditions of this problem? (2) Why do we need the condition that is an embedded sub-manifold? Assume ; then Theorem 5.8 on page 102 tells us that satisfies the local -slice condition (this is the only theorem I can think of that is related to this condition), but what good can this condition do for us? (3) I think I need to apply the Lagrange Multiplier Theorem (see page 113) in multi-variable calculus, but we need to make sure that the rank of the Jacobian matrix of is of rank at the point . However, there aren’t any extra conditions on . You can either answer my questions separately or show me a detailed proof of it. Thank you in advance!"," M   C \subset M   f \in {C^{\infty}}(M)   p \in C   f   C   \Phi: U \to \mathbb{R}^{k}   C   U   p   M   \lambda_{1},\ldots,\lambda_{k}  
\mathrm{d} f_{p} = \sum_{i = 1}^{k} \lambda_{i} \cdot \mathrm{d} \Phi^{i}|_{p}.
  M   C   \operatorname{dim}(M) = n > k   \operatorname{dim}(C) = n - k   C   \operatorname{dim}(C) = n - k   C   k   \Phi   k   p   \Phi ","['multivariable-calculus', 'differential-geometry', 'smooth-manifolds', 'lagrange-multiplier']"
98,Can I skip computational advanced calculus and work on Spivak's 'Calculus on Manifolds'?,Can I skip computational advanced calculus and work on Spivak's 'Calculus on Manifolds'?,,"I have completed Velleman's book, 'How to prove it'. I have also worked through Apostol Vol.1. I have messed about with many rigorous single variable calculus textbooks, e.g.,Apostol, Spivak, Courant, Lang, etc. I had started working through Lang's, 'Calculus of several variables' but put it up to do a book like Edwards, 'Advanced Calculus:A Differential Forms Approach.' I now see that book to be a waste of time, because it is extremely 'hand-wavy' I can't stand mathematics out of physics texts and I will most certainly not tolerate the same from an actual math book. Therefore I am back to my starting point, I have finished Linear Algebra via Lang and have been perusing Hubbard and Hubbard's book and Artin. I do not like that Hubbard and Hubbard is so wordy. I don't really have the patience to put up with extremely long winded explanations of trivial facts just to get to the meat. I would really like to work through Spivak's, 'Calculus on Manifolds' the problem being that I need to know if I can do without a book like Lang's, 'Calculus of Several Variables'? My goal is to get to manifolds and skip 'Vector Calculus' but I do not want to shortchange myself on computation if Spivak's book would leave me in that state. I need help to determine if it is worth the time to work through Lang's book or can I just skip it, I want to be able to apply forms, etc to physics although I am a math major.","I have completed Velleman's book, 'How to prove it'. I have also worked through Apostol Vol.1. I have messed about with many rigorous single variable calculus textbooks, e.g.,Apostol, Spivak, Courant, Lang, etc. I had started working through Lang's, 'Calculus of several variables' but put it up to do a book like Edwards, 'Advanced Calculus:A Differential Forms Approach.' I now see that book to be a waste of time, because it is extremely 'hand-wavy' I can't stand mathematics out of physics texts and I will most certainly not tolerate the same from an actual math book. Therefore I am back to my starting point, I have finished Linear Algebra via Lang and have been perusing Hubbard and Hubbard's book and Artin. I do not like that Hubbard and Hubbard is so wordy. I don't really have the patience to put up with extremely long winded explanations of trivial facts just to get to the meat. I would really like to work through Spivak's, 'Calculus on Manifolds' the problem being that I need to know if I can do without a book like Lang's, 'Calculus of Several Variables'? My goal is to get to manifolds and skip 'Vector Calculus' but I do not want to shortchange myself on computation if Spivak's book would leave me in that state. I need help to determine if it is worth the time to work through Lang's book or can I just skip it, I want to be able to apply forms, etc to physics although I am a math major.",,"['multivariable-calculus', 'soft-question', 'book-recommendation']"
99,Length of curve in 3D spherical coordinate,Length of curve in 3D spherical coordinate,,"let $r$ be the magnitude of a vector in 3D with Spherical co-ordinate $(r,\theta,\phi)$ and cartesian coordinates is $(x,y,z)$, whose angle with $z$ axis is $\phi$ and projection of the vector makes angle $\theta$ with $x$ axis. We know the  the relations $$x=r\sin\theta\cos\phi$$ $$y=r\sin\theta\sin\phi$$ $$z=r\cos\phi$$ Now $\gamma(t):[0,1]\to \mathbb{R}^3$ be a curve $\gamma(t)=(x(t),y(t),z(t))$ Now could you confirm me that lenghth of the curve is given by the formulae $$l_{\gamma}=\int_{0}^{1}\sqrt{(\frac{dx}{dt})^2+(\frac{dy}{dt})^2+(\frac{dz}{dt})^2}?$$ if yes how did we come to this formulae? please help and from the above relation of cartesian and spherical co ordinates I calculated that the lenght is given by $l_{\gamma}=\int_{a}^{b}\sqrt{(\frac{dr}{dt})^2+r^2(\frac{d\theta}{dt})^2}$, am I right? and what wil be the upper and lower limit of tis integration?","let $r$ be the magnitude of a vector in 3D with Spherical co-ordinate $(r,\theta,\phi)$ and cartesian coordinates is $(x,y,z)$, whose angle with $z$ axis is $\phi$ and projection of the vector makes angle $\theta$ with $x$ axis. We know the  the relations $$x=r\sin\theta\cos\phi$$ $$y=r\sin\theta\sin\phi$$ $$z=r\cos\phi$$ Now $\gamma(t):[0,1]\to \mathbb{R}^3$ be a curve $\gamma(t)=(x(t),y(t),z(t))$ Now could you confirm me that lenghth of the curve is given by the formulae $$l_{\gamma}=\int_{0}^{1}\sqrt{(\frac{dx}{dt})^2+(\frac{dy}{dt})^2+(\frac{dz}{dt})^2}?$$ if yes how did we come to this formulae? please help and from the above relation of cartesian and spherical co ordinates I calculated that the lenght is given by $l_{\gamma}=\int_{a}^{b}\sqrt{(\frac{dr}{dt})^2+r^2(\frac{d\theta}{dt})^2}$, am I right? and what wil be the upper and lower limit of tis integration?",,"['differential-geometry', 'multivariable-calculus', 'analytic-geometry']"
