,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Conditional probability of multivariate gaussian,Conditional probability of multivariate gaussian,,"I'm unsure regarding my (partial) solution/approach to the below problem. Any help/guidance regarding approach would be much appreciated. Let $\mathbf{X} = (X_1, X_2)' \in N(\mu, \Lambda ) $ , where $$\begin{align}     \mu &= \begin{pmatrix}            1 \\            1          \end{pmatrix}   \end{align} $$ $$ \begin{align}     \Lambda &= \begin{pmatrix}            3 \quad 1\\            1 \quad 2          \end{pmatrix}   \end{align} $$ We are tasked with computing: $P(X_1 \geq 2 \mid X_2 +3X_1=3)$ I here begin by doing a transformation, $$ \mathbf{Y} = (Y_1, Y_2)', \qquad Y_1 = X_1, \qquad Y_2 = X_2 + 3X_1$$ We now are interested in the probability, $$P(Y_1 \geq 2 \mid Y_2 = 3)$$ Since we can write that $\mathbf{Y = BX}$ , it follows that, $$\mathbf{Y} \in \mathcal{N}(\mathbf{B\mu, B\Lambda B')})$$ where $$\mathbf{B}= \begin{pmatrix}            1 \quad 0\\            3 \quad 1          \end{pmatrix} \rightarrow \quad \mathbf{B \mu} = \begin{pmatrix}            1 \\            4          \end{pmatrix}, \quad \mathbf{B\Lambda B'}= \begin{pmatrix}            1 \quad 0\\            3 \quad 1          \end{pmatrix} \begin{pmatrix}            3 \quad 1\\            1 \quad 2          \end{pmatrix} \begin{pmatrix}            1 \quad 3\\            0 \quad 1          \end{pmatrix} = \begin{pmatrix}            3 \quad 10\\            10 \; \; 35          \end{pmatrix}$$ We thereafter know that we can obtain the conditional density function by, $$ f_{Y_1\mid Y_2 = 3} (y_1) = \frac{f_{Y_1,Y_2}(y_1, 3)}{f_{Y_2}(3)} \tag 1 $$ The p.d.f. of the bivariate normal distribution, $$f_{Y_1, Y_2}(y_1, y_2) = \frac{1}{2\pi \sigma_1 \sigma_2 \sqrt{1-\rho^2}} e^{\frac{1}{2(1-\rho^2)}(\frac{(y_1 - \mu_1)^2}{\sigma_1^2} - \frac{2 \rho (y_1 - \mu_1)(y_2 - \mu_2)}{\sigma_1 \sigma_2} + \frac{(y_1 - \mu_1)^2}{\sigma_2^2})} $$ The marginal probability density of $Y_2$ , $$f_{Y_2}(y_2) = \frac{1}{\sqrt{2\pi} \sigma_2} e^{-(y_2 - \mu_2)^2 / (2\sigma_2^2)}$$ Given that, $$\sigma_1 = \sqrt{3}, \quad \sigma_2 = \sqrt{35}, \quad \rho = \frac{10}{\sigma_1 \sigma_2 } = \frac{10}{\sqrt{105}}  $$ we are ready to determine (1). However, the resulting expression, which I then need to integrate as follows, $$ Pr(Y_1 \geq 2 \mid Y_2 = 3) = \int_2^\infty f_{Y_1\mid Y_2 = 3} (y_1) \, dy_1 $$ becomes quite ugly, making me unsure whether I've approached the problem in the wrong way? Thanks in advance!","I'm unsure regarding my (partial) solution/approach to the below problem. Any help/guidance regarding approach would be much appreciated. Let , where We are tasked with computing: I here begin by doing a transformation, We now are interested in the probability, Since we can write that , it follows that, where We thereafter know that we can obtain the conditional density function by, The p.d.f. of the bivariate normal distribution, The marginal probability density of , Given that, we are ready to determine (1). However, the resulting expression, which I then need to integrate as follows, becomes quite ugly, making me unsure whether I've approached the problem in the wrong way? Thanks in advance!","\mathbf{X} = (X_1, X_2)' \in N(\mu, \Lambda )  \begin{align}
    \mu &= \begin{pmatrix}
           1 \\
           1
         \end{pmatrix}
  \end{align}
 
\begin{align}
    \Lambda &= \begin{pmatrix}
           3 \quad 1\\
           1 \quad 2
         \end{pmatrix}
  \end{align}
 P(X_1 \geq 2 \mid X_2 +3X_1=3)  \mathbf{Y} = (Y_1, Y_2)', \qquad Y_1 = X_1, \qquad Y_2 = X_2 + 3X_1 P(Y_1 \geq 2 \mid Y_2 = 3) \mathbf{Y = BX} \mathbf{Y} \in \mathcal{N}(\mathbf{B\mu, B\Lambda B')}) \mathbf{B}= \begin{pmatrix}
           1 \quad 0\\
           3 \quad 1
         \end{pmatrix} \rightarrow \quad \mathbf{B \mu} = \begin{pmatrix}
           1 \\
           4
         \end{pmatrix}, \quad \mathbf{B\Lambda B'}= \begin{pmatrix}
           1 \quad 0\\
           3 \quad 1
         \end{pmatrix} \begin{pmatrix}
           3 \quad 1\\
           1 \quad 2
         \end{pmatrix} \begin{pmatrix}
           1 \quad 3\\
           0 \quad 1
         \end{pmatrix} = \begin{pmatrix}
           3 \quad 10\\
           10 \; \; 35
         \end{pmatrix} 
f_{Y_1\mid Y_2 = 3} (y_1) = \frac{f_{Y_1,Y_2}(y_1, 3)}{f_{Y_2}(3)} \tag 1
 f_{Y_1, Y_2}(y_1, y_2) = \frac{1}{2\pi \sigma_1 \sigma_2 \sqrt{1-\rho^2}} e^{\frac{1}{2(1-\rho^2)}(\frac{(y_1 - \mu_1)^2}{\sigma_1^2} - \frac{2 \rho (y_1 - \mu_1)(y_2 - \mu_2)}{\sigma_1 \sigma_2} + \frac{(y_1 - \mu_1)^2}{\sigma_2^2})}  Y_2 f_{Y_2}(y_2) = \frac{1}{\sqrt{2\pi} \sigma_2} e^{-(y_2 - \mu_2)^2 / (2\sigma_2^2)} \sigma_1 = \sqrt{3}, \quad \sigma_2 = \sqrt{35}, \quad \rho = \frac{10}{\sigma_1 \sigma_2 } = \frac{10}{\sqrt{105}}   
Pr(Y_1 \geq 2 \mid Y_2 = 3) = \int_2^\infty f_{Y_1\mid Y_2 = 3} (y_1) \, dy_1
","['probability', 'probability-theory', 'normal-distribution', 'conditional-probability']"
1,Product of a shifted Log-Normal and a Log-Normal distribution,Product of a shifted Log-Normal and a Log-Normal distribution,,"Let $X$ and $Y$ follow Log-Normal distributions, with $\ln X \sim \mathcal{N}(\mu_x, \sigma_x^2)$ and $\ln Y \sim \mathcal{N}(\mu_y, \sigma_y^2)$. $X$ and $Y$ are independent. Let $W = X (Y + c)$, where $c$ is a constant. Is $W$ still Log-Normally distributed? If not, can $W$ be approximated by a Log-Normal distribution? I know that the product of Log-Normal distributions is Log-Normal. Unfortunately, $Y+c$ is no longer a Log-Normal, since its support is $[c, \infty)$. On the contrary, the support of $W$ is again $[0, \infty)$, which does not exclude the possibility that $W$ follows a Log-Normal distribution. Moreover, the numerical approximation of $W$ seems indeed to be Log-Normal - see the superposition of the histograms below, with $\mu_x = 2$, $\mu_y = 4$, $\sigma^2_x = \sigma^2_y = 1$, and $c = 5$.","Let $X$ and $Y$ follow Log-Normal distributions, with $\ln X \sim \mathcal{N}(\mu_x, \sigma_x^2)$ and $\ln Y \sim \mathcal{N}(\mu_y, \sigma_y^2)$. $X$ and $Y$ are independent. Let $W = X (Y + c)$, where $c$ is a constant. Is $W$ still Log-Normally distributed? If not, can $W$ be approximated by a Log-Normal distribution? I know that the product of Log-Normal distributions is Log-Normal. Unfortunately, $Y+c$ is no longer a Log-Normal, since its support is $[c, \infty)$. On the contrary, the support of $W$ is again $[0, \infty)$, which does not exclude the possibility that $W$ follows a Log-Normal distribution. Moreover, the numerical approximation of $W$ seems indeed to be Log-Normal - see the superposition of the histograms below, with $\mu_x = 2$, $\mu_y = 4$, $\sigma^2_x = \sigma^2_y = 1$, and $c = 5$.",,"['probability', 'probability-distributions', 'normal-distribution']"
2,Probability that the convex hull of random points contains sphere's center,Probability that the convex hull of random points contains sphere's center,,What is the probability that the convex hull of $n+2$ random points on $n$-dimensional sphere contains sphere's center?,What is the probability that the convex hull of $n+2$ random points on $n$-dimensional sphere contains sphere's center?,,"['geometry', 'geometric-probability']"
3,are sub-martingales correlated with the indicator function of stopping times?,are sub-martingales correlated with the indicator function of stopping times?,,"Let $X_t$ be any nice enough continuous sub-martingale and let $\tau$ be a stopping time which is, let's say bounded. My question is, are $X_t$ and $1_{\{\tau >t\}}$ positively correlated? That is, is $\mathbb{E}[X_t1_{\{\tau >t\}}] \geq \mathbb{E}[X_t]\mathbb{P}(\tau>t)$? This seems intuitive enough, since the expectation of $X_t$ increases with time, and it will stop increasing whenever $t$ is bigger than $\tau$. However, I cannot see how to prove it and I'm afraid I'm missing something.","Let $X_t$ be any nice enough continuous sub-martingale and let $\tau$ be a stopping time which is, let's say bounded. My question is, are $X_t$ and $1_{\{\tau >t\}}$ positively correlated? That is, is $\mathbb{E}[X_t1_{\{\tau >t\}}] \geq \mathbb{E}[X_t]\mathbb{P}(\tau>t)$? This seems intuitive enough, since the expectation of $X_t$ increases with time, and it will stop increasing whenever $t$ is bigger than $\tau$. However, I cannot see how to prove it and I'm afraid I'm missing something.",,"['probability', 'inequality', 'stochastic-processes', 'martingales', 'correlation']"
4,Maze with a randomly moving obstacle,Maze with a randomly moving obstacle,,"There is a 8*8 maze, and a player starting at (0, 0) wants to arrive at (7, 7) in minimal moves. For each round (a round may contain several moves), the obstacle will be randomly set at a grid it has not been set before, also, it won't be set at either (0, 0) or (7, 7). After the obstacle is set (although the player has no idea where it is in this round), the player has to give a probability distribution of his move in this round, e.g. up: 0.5, down: 0.2, left: 0.2, right: 0.1, then his move will be generated depending on the distribution. If he bumps into the obstacle or gets out of the maze (e.g. move right when he is at (7, 0)), he has to stay at the current grid, and another move will be generated. The process goes on until he moves to a feasible grid. When a round ends, the player will know where the obstacle was in the last round. To be clear, supposed the player is in the nth round, then he will know the history of the obstacle from the first round to the (n-1)th round. My question is, given the history of the obstacle and the current position of the player, is there a strategy to minimized expected moves from a probabilistic perspective ? EDIT I've wrote a simulation program , and tried several strategies with it. Define a strategy consisting of four real values, representing the probability of moving up, down, left, and right, respectively.The naive one [0.25, 0.25, 0.25, 0.25] got an average of 73 moves. Another strategy is defined below: if up and right are both safe, return [0.5, 0, 0, 0.5] if up is safe, return [1, 0, 0, 0]; if right is safe, return [0, 0, 0, 1] if at the right most line, return [0.5, 0, 0.5, 0]; if at the up most line, return [0, 0.5, 0, 0.5] else, return [0.5, 0, 0.5, 0] In the end, the second strategy need 18.4573 moves on average. (10000 trials) It is a huge success when comparing with the naive one, but I am struggling with getting closer to the theoretically minimum of 14 moves. I appreciate any advice or suggestions. Confirmation There is a $(X,Y)=(0:N-1,0:N-1) \in \mathbb{R}^2, N=8$ maze Yes There is no inner walls or paths at the maze (Then it is not a maze??), Yes, I called it a maze only because there is a single randomly appearing obstacle. The positions cannot get outside the domain, hence evidently the movement have to be properly constrained, Yes, if the intended move generated by the probability distribution will make the player get out of the maze, the count of moves will increment by one, and the player has to stay at the current position until an feasible move is generated. There is an changing obstacle at an unknown position $(x^o,y^o)$, for every time $t$, Yes The obstacle position  at $t=n-1$ is known at $t=n$, Yes For every time $n$, the obstacle change their coordinates according an uniform distribution on $(X,Y)$. Hence, the obstacle can ""jump"" to their new position, Yes The obstacle don't take a previous coordinate. Hence the uniform distribution is over $(X,Y)-(X^o,Y^o)$, $(X^o,Y^o)={(x^o(t),y^o(t),t=1...n-1)}$. Hence the obstacle takes its last move at $t=N^2$ and then it disappears (??), The obstacle won't be set on the stating point and the ending point, so the player will lose the game if he can not reach the exit in $N^2 - 2$ rounds The player position $(x(t),y(t))$ one square (up, down, left or right) per time, Yes The player cannot pass, hence make no move at all (??), I am not sure what you are referring. If the obstacle hits the player by a player move or an obstacle change, the player loses, If the obstacle hits the player by a player move, as I stated in my problem, If he bumps into the obstacle or gets out of the maze (e.g. move right when he is at (7, 0)), he has to stay at the current grid, and another move will be generated. The process goes on until he moves to a feasible grid. In short, the player loses the game iff he can not reach the exit in $N^2 - 2$ rounds. If the obstacle hits the player by an obstacle change, we assume that the player is strong enough to lift the obstacle, so he can safely stay with the obstacle in the same grid. If the player reach the exit, the player wins. Yes","There is a 8*8 maze, and a player starting at (0, 0) wants to arrive at (7, 7) in minimal moves. For each round (a round may contain several moves), the obstacle will be randomly set at a grid it has not been set before, also, it won't be set at either (0, 0) or (7, 7). After the obstacle is set (although the player has no idea where it is in this round), the player has to give a probability distribution of his move in this round, e.g. up: 0.5, down: 0.2, left: 0.2, right: 0.1, then his move will be generated depending on the distribution. If he bumps into the obstacle or gets out of the maze (e.g. move right when he is at (7, 0)), he has to stay at the current grid, and another move will be generated. The process goes on until he moves to a feasible grid. When a round ends, the player will know where the obstacle was in the last round. To be clear, supposed the player is in the nth round, then he will know the history of the obstacle from the first round to the (n-1)th round. My question is, given the history of the obstacle and the current position of the player, is there a strategy to minimized expected moves from a probabilistic perspective ? EDIT I've wrote a simulation program , and tried several strategies with it. Define a strategy consisting of four real values, representing the probability of moving up, down, left, and right, respectively.The naive one [0.25, 0.25, 0.25, 0.25] got an average of 73 moves. Another strategy is defined below: if up and right are both safe, return [0.5, 0, 0, 0.5] if up is safe, return [1, 0, 0, 0]; if right is safe, return [0, 0, 0, 1] if at the right most line, return [0.5, 0, 0.5, 0]; if at the up most line, return [0, 0.5, 0, 0.5] else, return [0.5, 0, 0.5, 0] In the end, the second strategy need 18.4573 moves on average. (10000 trials) It is a huge success when comparing with the naive one, but I am struggling with getting closer to the theoretically minimum of 14 moves. I appreciate any advice or suggestions. Confirmation There is a $(X,Y)=(0:N-1,0:N-1) \in \mathbb{R}^2, N=8$ maze Yes There is no inner walls or paths at the maze (Then it is not a maze??), Yes, I called it a maze only because there is a single randomly appearing obstacle. The positions cannot get outside the domain, hence evidently the movement have to be properly constrained, Yes, if the intended move generated by the probability distribution will make the player get out of the maze, the count of moves will increment by one, and the player has to stay at the current position until an feasible move is generated. There is an changing obstacle at an unknown position $(x^o,y^o)$, for every time $t$, Yes The obstacle position  at $t=n-1$ is known at $t=n$, Yes For every time $n$, the obstacle change their coordinates according an uniform distribution on $(X,Y)$. Hence, the obstacle can ""jump"" to their new position, Yes The obstacle don't take a previous coordinate. Hence the uniform distribution is over $(X,Y)-(X^o,Y^o)$, $(X^o,Y^o)={(x^o(t),y^o(t),t=1...n-1)}$. Hence the obstacle takes its last move at $t=N^2$ and then it disappears (??), The obstacle won't be set on the stating point and the ending point, so the player will lose the game if he can not reach the exit in $N^2 - 2$ rounds The player position $(x(t),y(t))$ one square (up, down, left or right) per time, Yes The player cannot pass, hence make no move at all (??), I am not sure what you are referring. If the obstacle hits the player by a player move or an obstacle change, the player loses, If the obstacle hits the player by a player move, as I stated in my problem, If he bumps into the obstacle or gets out of the maze (e.g. move right when he is at (7, 0)), he has to stay at the current grid, and another move will be generated. The process goes on until he moves to a feasible grid. In short, the player loses the game iff he can not reach the exit in $N^2 - 2$ rounds. If the obstacle hits the player by an obstacle change, we assume that the player is strong enough to lift the obstacle, so he can safely stay with the obstacle in the same grid. If the player reach the exit, the player wins. Yes",,"['probability', 'algorithmic-game-theory']"
5,Does $\pi$ have infinitely many prime prefixes?,Does  have infinitely many prime prefixes?,\pi,"This is inspired by this question . Does $\pi$ have infinitely many prime prefixes (in base $10$)? That is, is the sequence A005042 infinite? It says on the OEIS that a naïve probabilistic argument suggests that the sequence of such primes is infinite. What argument would that be? Such primes would be examples of pi-primes .","This is inspired by this question . Does $\pi$ have infinitely many prime prefixes (in base $10$)? That is, is the sequence A005042 infinite? It says on the OEIS that a naïve probabilistic argument suggests that the sequence of such primes is infinite. What argument would that be? Such primes would be examples of pi-primes .",,"['probability', 'prime-numbers', 'formal-languages', 'pi']"
6,"On average, where is the lift?","On average, where is the lift?",,"This started as a computing problem with several variables, and I'd like to know if there's a closed form formula for the average position of the lift. Context: there's a building with $N$ floors and $m$ people distributed randomly across them. If a person is on the floor where they live, they will take the lift to go straight down (no stopping) to the ground floor, and vice-versa. At each step, one person is randomly selected and makes their move. At each step, then, the lift moves either once (already on the floor it's called from) or twice (goes to pick up the person first, then makes the requested move). Each new position of the lift is recorded. On randomness: since this was first a question about a script in Python, the random.randint() method was used, where the documentation states Almost all module functions depend on the basic function random() , which generates a random float uniformly in the semi-open range $[0.0, 1.0)$. Python uses the Mersenne Twister as the core generator. It produces 53-bit precision floats and has a period of $2^{19937}-1$. Question: after $k$ iterations, the algorithm stops and returns the average $\mathcal{A}$ of the recorded positions of the lift. Is it possible to predict either $\lim\limits_{k\to\infty}\mathcal{A}(N,m)$ or $\mathcal{A}(N,m,k)$?","This started as a computing problem with several variables, and I'd like to know if there's a closed form formula for the average position of the lift. Context: there's a building with $N$ floors and $m$ people distributed randomly across them. If a person is on the floor where they live, they will take the lift to go straight down (no stopping) to the ground floor, and vice-versa. At each step, one person is randomly selected and makes their move. At each step, then, the lift moves either once (already on the floor it's called from) or twice (goes to pick up the person first, then makes the requested move). Each new position of the lift is recorded. On randomness: since this was first a question about a script in Python, the random.randint() method was used, where the documentation states Almost all module functions depend on the basic function random() , which generates a random float uniformly in the semi-open range $[0.0, 1.0)$. Python uses the Mersenne Twister as the core generator. It produces 53-bit precision floats and has a period of $2^{19937}-1$. Question: after $k$ iterations, the algorithm stops and returns the average $\mathcal{A}$ of the recorded positions of the lift. Is it possible to predict either $\lim\limits_{k\to\infty}\mathcal{A}(N,m)$ or $\mathcal{A}(N,m,k)$?",,"['probability', 'statistics', 'algorithms', 'expectation', 'python']"
7,Understanding the Memoryless Property,Understanding the Memoryless Property,,"I'm a little confused about the memoryless property of an exponential distribution. I know for the exponential distribution we can write the memoryless property like this, $$Pr(X > x + y | X > x) = Pr(X > y)$$ For the following, assume $X$ is the lifetime of a light bulb which is exponentially distributed with some $\lambda$. My interpretation in English is the following, ""What is the Probability of a lightbulb lasting more than $1000$ hours given it has already lasted $300$ hours"". This is taken to be equivalent to saying, ""What is the probability that the light bulb will last more than $700$ hours"" However, for me, memorylessness would imply the following, ""What is the probability that the light bulb will last more than $1000$ hours give it has already last $300$ hours"". This can then be taken to mean, ""What is the probability the light bulb will last more than $1000$ hours"" To me memorylessness should just mean we drop the ""$|X > x$"" part, however, this clearly isn't the case. Why is that? Secondly, how can the area under the curve between $x + y$ and $\infty$ be equal to the area under the curve from $y$ to $\infty$? I know it's something to do with the conditioning on $X > x + y$ with $X > x$, but I just don't understand how that makes the two areas equivalent. I would really appreciate any input.","I'm a little confused about the memoryless property of an exponential distribution. I know for the exponential distribution we can write the memoryless property like this, $$Pr(X > x + y | X > x) = Pr(X > y)$$ For the following, assume $X$ is the lifetime of a light bulb which is exponentially distributed with some $\lambda$. My interpretation in English is the following, ""What is the Probability of a lightbulb lasting more than $1000$ hours given it has already lasted $300$ hours"". This is taken to be equivalent to saying, ""What is the probability that the light bulb will last more than $700$ hours"" However, for me, memorylessness would imply the following, ""What is the probability that the light bulb will last more than $1000$ hours give it has already last $300$ hours"". This can then be taken to mean, ""What is the probability the light bulb will last more than $1000$ hours"" To me memorylessness should just mean we drop the ""$|X > x$"" part, however, this clearly isn't the case. Why is that? Secondly, how can the area under the curve between $x + y$ and $\infty$ be equal to the area under the curve from $y$ to $\infty$? I know it's something to do with the conditioning on $X > x + y$ with $X > x$, but I just don't understand how that makes the two areas equivalent. I would really appreciate any input.",,"['probability', 'stochastic-processes']"
8,stochastic recurrence relation,stochastic recurrence relation,,"I'm interested in dealing with limit values for a linear recurrence relation, which is complicated slightly by stochastic transformations. Here is the specific question. For $n<0$, let $a_n=a_n^{\ast}=0$. Let $a_0=a_0^{\ast}=a_1=a_1^{\ast}=1$. Then for $n\geq 1$ define: $$a_{n+1}= a^{\ast}_{n}+\sum_{j\geq 1}4ja^{\ast}_{n-j} = a^{\ast}_n +4a^{\ast}_{n-1}+8a^{\ast}_{n-2} + 12 a^{\ast}_{n-3} \dots $$ Then $a_{n+1}^{\ast}$ is defined to be the value of a Poisson random variable with mean $a_{n+1}$. We are interested in the values $a_{n}^{\ast}$ for large $n$. Okay, so in the absence of the stochastic element, if it were simply the case that $a_{n}^{\ast}=a_n$ and this was a standard recurrence relation, since we would then have $a_n=a_{n-1}+\sum_{j\geq 1}4ja_{n-j-1}$ we could rewrite, for $n>0$: $$a_{n+1}= 2a_n + 3n_{n-1} + 4\sum_{j\geq 2}a_{n-j}.$$ Given that $a_{n-2}=2a_{n-3}+3a_{n-4}+4\sum_{j\geq 2} a_{n-j-3}$, this in turn then gives, for $n>0$: $$a_{n+1}= 2a_n + 3a_{n-1} + 5a_{n-2}+2a_{n-3}+a_{n-4}.$$ Since the characteristic polynomial $x^5-2x^4-3x^3-5x^2-2x-1=0$  has a largest root $\alpha \approx 3.38298$, it follows that for some constant $\rho>0$:  $$ \lim_{n\to \infty} \frac{a_n}{\rho\cdot \alpha^n} = 1. $$ OKAY.. so.. for the stochastic version, what I suppose happens is that with probability 1, we have $a^{\ast}_n/\alpha^n$ tending to a limit.  One could presumably go in and prove it from first principles, but I expect this follows fairly directly from known theorems? Many thanks!","I'm interested in dealing with limit values for a linear recurrence relation, which is complicated slightly by stochastic transformations. Here is the specific question. For $n<0$, let $a_n=a_n^{\ast}=0$. Let $a_0=a_0^{\ast}=a_1=a_1^{\ast}=1$. Then for $n\geq 1$ define: $$a_{n+1}= a^{\ast}_{n}+\sum_{j\geq 1}4ja^{\ast}_{n-j} = a^{\ast}_n +4a^{\ast}_{n-1}+8a^{\ast}_{n-2} + 12 a^{\ast}_{n-3} \dots $$ Then $a_{n+1}^{\ast}$ is defined to be the value of a Poisson random variable with mean $a_{n+1}$. We are interested in the values $a_{n}^{\ast}$ for large $n$. Okay, so in the absence of the stochastic element, if it were simply the case that $a_{n}^{\ast}=a_n$ and this was a standard recurrence relation, since we would then have $a_n=a_{n-1}+\sum_{j\geq 1}4ja_{n-j-1}$ we could rewrite, for $n>0$: $$a_{n+1}= 2a_n + 3n_{n-1} + 4\sum_{j\geq 2}a_{n-j}.$$ Given that $a_{n-2}=2a_{n-3}+3a_{n-4}+4\sum_{j\geq 2} a_{n-j-3}$, this in turn then gives, for $n>0$: $$a_{n+1}= 2a_n + 3a_{n-1} + 5a_{n-2}+2a_{n-3}+a_{n-4}.$$ Since the characteristic polynomial $x^5-2x^4-3x^3-5x^2-2x-1=0$  has a largest root $\alpha \approx 3.38298$, it follows that for some constant $\rho>0$:  $$ \lim_{n\to \infty} \frac{a_n}{\rho\cdot \alpha^n} = 1. $$ OKAY.. so.. for the stochastic version, what I suppose happens is that with probability 1, we have $a^{\ast}_n/\alpha^n$ tending to a limit.  One could presumably go in and prove it from first principles, but I expect this follows fairly directly from known theorems? Many thanks!",,"['probability', 'stochastic-processes', 'recurrence-relations']"
9,A probability problem on randomly generated graph,A probability problem on randomly generated graph,,"The graph we discuss here is a directed pseudo-graph (two vertices can have multiple edges) with self-loops . The problem is to prove a very intuitively probability formula at the end. The background Given a set of vertices $V$, suppose we know there are $|E|$ edges, and the edges are generated in the following fashion: for the $i$th edge, randomly choose one ${\cal U}_i∈V$ and then randomly choose one ${\cal V}_i∈V$ where ${\cal U}_i,{\cal V}_i$ are random variables and $i=1,…,|E|$. Since it is pseudo-graph with self-loops, then each choice of vertex can be assumed to have no impact on future choices, i.e. all choices of vertices are independent, then the generation of these edges are also independent, and $\mathbb{P}\left( E \right) = \mathbb{P}\left( {{\mathcal{U}_1} = {u_1},{\mathcal{V}_1} = {v_1}, \ldots ,{\mathcal{U}_{\left| E \right|}} = {u_{\left| E \right|}},{\mathcal{V}_{\left| E \right|}} = {v_{\left| E \right|}}} \right) = \mathop \prod \limits_{i = 1}^{\left| E \right|} \mathbb{P}({\mathcal{U}_i} = {u_i})\mathbb{P}({\mathcal{V}_i} = {v_i}) = \mathop \prod \limits_{i = 1}^{\left| E \right|} \mathbb{P}({\mathcal{U}_i} = {u_i},{\mathcal{V}_i} = {v_i}) = {\left( {\frac{1}{{{{\left| V \right|}^2}}}} \right)^{\left| E \right|}} = \frac{1}{{{{\left| V \right|}^{2\left| E \right|}}}}$ Vertex ""Membership"" Now we can assign each vertex a number in $1,...,k$ and thus partition the graph vertices into $k$ disjoint sets, which we can call ""groups"", the number can be named ""membership"" of each vertex. We use a function $z$ to denote the labeling, and $z_v$ is the membership of vertex $v$. Given $z$, we know the number of vertices of each group, denoted by $n_i,i=1,...,k$. Let $n_{i,j}=\mathop \sum \limits_{u,v \in V} {\mathbb{I}_{{z_u} = i,{z_v} = j}}$ where $\Bbb I$ is an identity function using the subscript as the condition, then this is the maximum number of possible ""unique"" edges from group $i$ to group $j$. Suppose a pseudo-graph is generated in the way mentioned above. Now randomly draw an edge $({\cal U},{\cal V})$ from the generated graph. The knowledge of z does not affect $\mathbb{P}\left( {\mathcal{U} = u,\mathcal{V} = v|z} \right)$, i.e. $\mathbb{P}\left( {\mathcal{U} = u,\mathcal{V} = v|z} \right) = \mathbb{P}\left( {\mathcal{U} = u,\mathcal{V} = v} \right)$ then we have $\begin{gathered} \mathbb{P}\left( {{z_\mathcal{U}} = i,{z_\mathcal{V}} = j{\text{|}}z} \right) = \mathop \sum \limits_{u,v \in V} \mathbb{P}\left( {\mathcal{U} = u,\mathcal{V} = v,{z_u} = i,{z_v} = j{\text{|}}z} \right) \hfill \\ = \mathop \sum \limits_{u,v \in V} \mathbb{P}\left( {\mathcal{U} = u,\mathcal{V} = v|z} \right){\mathbb{I}_{{z_u} = i,{z_v} = j}} = \mathop \sum \limits_{u,v \in V} \mathbb{P}\left( {\mathcal{U} = u,\mathcal{V} = v} \right){\mathbb{I}_{{z_u} = i,{z_v} = j}} \hfill \\ = \mathop \sum \limits_{u,v \in V} \frac{1}{{{{\left| V \right|}^2}}}{\mathbb{I}_{{z_u} = i,{z_v} = j}} = \frac{{{n_{i,j}}}}{{{{\left| V \right|}^2}}} \hfill \\  \end{gathered} $ where $\mathbb{P}\left( {\mathcal{U} = u,\mathcal{V} = v,{z_u} = i,{z_v} = j{\text{|}}z} \right) = \mathbb{P}\left( {\mathcal{U} = u,\mathcal{V} = v|z} \right){\mathbb{I}_{{z_u} = i,{z_v} = j}}$ since $z_u,z_v$ are determined by $z$. My problem Suppose we now can observe the whole generated edge set $E$, and let $e_{i,j}$ be the actual number of edges from group $i$ to group $j$. Still randomly draw an edge $({\cal U},{\cal V})$ from the graph, show $\mathbb{P}\left( {{z_\mathcal{U}} = i,{z_\mathcal{V}} = j{\text{|}}{{z}},E} \right) = \frac{{{e_{i,j}}}}{{\left| E \right|}}$ This is very intuitively true, but I have difficulty showing it rigorously as showing $\mathbb{P}\left( {{z_\mathcal{U}} = i,{z_\mathcal{V}} = j{\text{|}}z} \right) = \frac{{{n_{i,j}}}}{{{{\left| V \right|}^2}}}$ above. Thanks a lot!","The graph we discuss here is a directed pseudo-graph (two vertices can have multiple edges) with self-loops . The problem is to prove a very intuitively probability formula at the end. The background Given a set of vertices $V$, suppose we know there are $|E|$ edges, and the edges are generated in the following fashion: for the $i$th edge, randomly choose one ${\cal U}_i∈V$ and then randomly choose one ${\cal V}_i∈V$ where ${\cal U}_i,{\cal V}_i$ are random variables and $i=1,…,|E|$. Since it is pseudo-graph with self-loops, then each choice of vertex can be assumed to have no impact on future choices, i.e. all choices of vertices are independent, then the generation of these edges are also independent, and $\mathbb{P}\left( E \right) = \mathbb{P}\left( {{\mathcal{U}_1} = {u_1},{\mathcal{V}_1} = {v_1}, \ldots ,{\mathcal{U}_{\left| E \right|}} = {u_{\left| E \right|}},{\mathcal{V}_{\left| E \right|}} = {v_{\left| E \right|}}} \right) = \mathop \prod \limits_{i = 1}^{\left| E \right|} \mathbb{P}({\mathcal{U}_i} = {u_i})\mathbb{P}({\mathcal{V}_i} = {v_i}) = \mathop \prod \limits_{i = 1}^{\left| E \right|} \mathbb{P}({\mathcal{U}_i} = {u_i},{\mathcal{V}_i} = {v_i}) = {\left( {\frac{1}{{{{\left| V \right|}^2}}}} \right)^{\left| E \right|}} = \frac{1}{{{{\left| V \right|}^{2\left| E \right|}}}}$ Vertex ""Membership"" Now we can assign each vertex a number in $1,...,k$ and thus partition the graph vertices into $k$ disjoint sets, which we can call ""groups"", the number can be named ""membership"" of each vertex. We use a function $z$ to denote the labeling, and $z_v$ is the membership of vertex $v$. Given $z$, we know the number of vertices of each group, denoted by $n_i,i=1,...,k$. Let $n_{i,j}=\mathop \sum \limits_{u,v \in V} {\mathbb{I}_{{z_u} = i,{z_v} = j}}$ where $\Bbb I$ is an identity function using the subscript as the condition, then this is the maximum number of possible ""unique"" edges from group $i$ to group $j$. Suppose a pseudo-graph is generated in the way mentioned above. Now randomly draw an edge $({\cal U},{\cal V})$ from the generated graph. The knowledge of z does not affect $\mathbb{P}\left( {\mathcal{U} = u,\mathcal{V} = v|z} \right)$, i.e. $\mathbb{P}\left( {\mathcal{U} = u,\mathcal{V} = v|z} \right) = \mathbb{P}\left( {\mathcal{U} = u,\mathcal{V} = v} \right)$ then we have $\begin{gathered} \mathbb{P}\left( {{z_\mathcal{U}} = i,{z_\mathcal{V}} = j{\text{|}}z} \right) = \mathop \sum \limits_{u,v \in V} \mathbb{P}\left( {\mathcal{U} = u,\mathcal{V} = v,{z_u} = i,{z_v} = j{\text{|}}z} \right) \hfill \\ = \mathop \sum \limits_{u,v \in V} \mathbb{P}\left( {\mathcal{U} = u,\mathcal{V} = v|z} \right){\mathbb{I}_{{z_u} = i,{z_v} = j}} = \mathop \sum \limits_{u,v \in V} \mathbb{P}\left( {\mathcal{U} = u,\mathcal{V} = v} \right){\mathbb{I}_{{z_u} = i,{z_v} = j}} \hfill \\ = \mathop \sum \limits_{u,v \in V} \frac{1}{{{{\left| V \right|}^2}}}{\mathbb{I}_{{z_u} = i,{z_v} = j}} = \frac{{{n_{i,j}}}}{{{{\left| V \right|}^2}}} \hfill \\  \end{gathered} $ where $\mathbb{P}\left( {\mathcal{U} = u,\mathcal{V} = v,{z_u} = i,{z_v} = j{\text{|}}z} \right) = \mathbb{P}\left( {\mathcal{U} = u,\mathcal{V} = v|z} \right){\mathbb{I}_{{z_u} = i,{z_v} = j}}$ since $z_u,z_v$ are determined by $z$. My problem Suppose we now can observe the whole generated edge set $E$, and let $e_{i,j}$ be the actual number of edges from group $i$ to group $j$. Still randomly draw an edge $({\cal U},{\cal V})$ from the graph, show $\mathbb{P}\left( {{z_\mathcal{U}} = i,{z_\mathcal{V}} = j{\text{|}}{{z}},E} \right) = \frac{{{e_{i,j}}}}{{\left| E \right|}}$ This is very intuitively true, but I have difficulty showing it rigorously as showing $\mathbb{P}\left( {{z_\mathcal{U}} = i,{z_\mathcal{V}} = j{\text{|}}z} \right) = \frac{{{n_{i,j}}}}{{{{\left| V \right|}^2}}}$ above. Thanks a lot!",,"['probability', 'graph-theory', 'random-graphs']"
10,"5 cards from a 52 card deck, what is the probability that the sum of cards is greater than 48","5 cards from a 52 card deck, what is the probability that the sum of cards is greater than 48",,"The problem states:  $5$ cards are dealt from a standard $52$ card deck. What is the probability that the sum of the values on the five cards is $48$ or more? It is assumed of course that the value of face cards is $10$ and that of aces $11$. I know I am looking for the ratio between the number of possible outcomes with sum of values at least $48$, and the total number of possible outcomes, but I am having trouble finding the former quantity. Any help is appreciated, thank you!","The problem states:  $5$ cards are dealt from a standard $52$ card deck. What is the probability that the sum of the values on the five cards is $48$ or more? It is assumed of course that the value of face cards is $10$ and that of aces $11$. I know I am looking for the ratio between the number of possible outcomes with sum of values at least $48$, and the total number of possible outcomes, but I am having trouble finding the former quantity. Any help is appreciated, thank you!",,"['probability', 'combinatorics', 'combinations', 'card-games']"
11,Conditional Probability 5 card hand,Conditional Probability 5 card hand,,"We have a 5 card hand from a standard deck. What is the probability that the hand is all Spades, given that it has at least two Spades? I know the formula for conditional probability is: $P(A|B) = \frac{P(A \cap B)}{P(B)}$ So in this case would it be: $P(A|B) = \frac{13 \choose 5}{{13 \choose 5}+{13 \choose 4}{39 \choose 1}+{13 \choose 3}{39 \choose 2}+{13 \choose 2}{39 \choose 3}}$","We have a 5 card hand from a standard deck. What is the probability that the hand is all Spades, given that it has at least two Spades? I know the formula for conditional probability is: $P(A|B) = \frac{P(A \cap B)}{P(B)}$ So in this case would it be: $P(A|B) = \frac{13 \choose 5}{{13 \choose 5}+{13 \choose 4}{39 \choose 1}+{13 \choose 3}{39 \choose 2}+{13 \choose 2}{39 \choose 3}}$",,"['probability', 'combinatorics', 'card-games']"
12,Distribution of eigenvalues of adjacency operators of finite digraphs,Distribution of eigenvalues of adjacency operators of finite digraphs,,"Let $A_n$ be the set of all eigenvalues of binary matrices with trace $0$ (equivalently, the union of spectra of all simple digraphs on $n$ vertices), and let $A=\bigcup A_n$. It is quite easy to prove that $A$ is dense in $\mathbb{C}$, but do we know anything about how it distributes? In particular, given an $r$ do we know to bound $\lim_{n\to\infty} \frac{|A_n\cap B_r|}{|A_n|}$ as a function of $r$ (where $B_r=\{z\mid |z|\le r\}$)?","Let $A_n$ be the set of all eigenvalues of binary matrices with trace $0$ (equivalently, the union of spectra of all simple digraphs on $n$ vertices), and let $A=\bigcup A_n$. It is quite easy to prove that $A$ is dense in $\mathbb{C}$, but do we know anything about how it distributes? In particular, given an $r$ do we know to bound $\lim_{n\to\infty} \frac{|A_n\cap B_r|}{|A_n|}$ as a function of $r$ (where $B_r=\{z\mid |z|\le r\}$)?",,"['probability', 'combinatorics', 'graph-theory']"
13,Probability that partition problem has a solution for sets of random integers,Probability that partition problem has a solution for sets of random integers,,"The Birthday Problem wikipedia page has a section devoted to the problem of finding the minimum $N$ such that the probability $p_N$ that $N$ numbers randomly chosen with replacement from $\{1,2,...,1000000\}$ can be partitioned into two subsets whose sums differ by at most $1$ satisfies $p_N>\frac{1}{2}$. The answer given is $N=23$, and the explanation is that there are $2^{N-1}$ partitions and the distribution of the sum of the weights is Gaussian with width approximately $1000000\sqrt{N}$. It is then claimed that the critical value of $N$ is that for which $1000000\sqrt{N}\approx2^{N-1}$ which gives $N=23$ as the closest value. I do not follow this argument; in particular, I cannot see why the width approximation of $1000000\sqrt{N}$ is used or why $p_N$ will be greater than $\frac{1}{2}$ when the number of partitions is larger than the width. The page references this paper , which investigates the behaviour of the problem as $N\rightarrow\infty$ and the number of weights to choose from is increased proportional to $N$. Looking through the paper, I did not find any result which seemed to answer our problem, as most results were for asymptotically large $N$, and I couldn't find anything which seemed to justify wikipedia's working. Thus I would like the answer to the following problem: let $N$ weights be chosen with integer values randomly drawn from $\{1,2,...,M\}$ (I'd be happy to see results with or without replacement) and let $p(N,M)$ be the probability of existence of an equal-weight (with or without allowance for a discrepancy of 1 unit) two-subset partition of the weights. (Without allowance for discrepancy, $p(N,M)$ is the probability that the partition problem has a solution). Then what is $N_{min}(M)=\min{\left\{N\;|\;p(N,M)>\frac{1}{2}\right\}}$? I have attempted in the past to solve this by trying to find an exact expression for $p(N,M)$, by looking for expressions for the number of possible partitions with equal weights using combinations but I have never had success, and I don't know if this is an appropriate way to go about it. I have searched for any similar question on math.SE also but all of the results seem to be more concerned with complexity and other computational considerations relating to the partition problem. My question is: can anyone find a way of calculating $N_{min}(M)$ or even $p(N,M)$ or present a good approximation, or possibly elucidate wikipedia's calculation?","The Birthday Problem wikipedia page has a section devoted to the problem of finding the minimum $N$ such that the probability $p_N$ that $N$ numbers randomly chosen with replacement from $\{1,2,...,1000000\}$ can be partitioned into two subsets whose sums differ by at most $1$ satisfies $p_N>\frac{1}{2}$. The answer given is $N=23$, and the explanation is that there are $2^{N-1}$ partitions and the distribution of the sum of the weights is Gaussian with width approximately $1000000\sqrt{N}$. It is then claimed that the critical value of $N$ is that for which $1000000\sqrt{N}\approx2^{N-1}$ which gives $N=23$ as the closest value. I do not follow this argument; in particular, I cannot see why the width approximation of $1000000\sqrt{N}$ is used or why $p_N$ will be greater than $\frac{1}{2}$ when the number of partitions is larger than the width. The page references this paper , which investigates the behaviour of the problem as $N\rightarrow\infty$ and the number of weights to choose from is increased proportional to $N$. Looking through the paper, I did not find any result which seemed to answer our problem, as most results were for asymptotically large $N$, and I couldn't find anything which seemed to justify wikipedia's working. Thus I would like the answer to the following problem: let $N$ weights be chosen with integer values randomly drawn from $\{1,2,...,M\}$ (I'd be happy to see results with or without replacement) and let $p(N,M)$ be the probability of existence of an equal-weight (with or without allowance for a discrepancy of 1 unit) two-subset partition of the weights. (Without allowance for discrepancy, $p(N,M)$ is the probability that the partition problem has a solution). Then what is $N_{min}(M)=\min{\left\{N\;|\;p(N,M)>\frac{1}{2}\right\}}$? I have attempted in the past to solve this by trying to find an exact expression for $p(N,M)$, by looking for expressions for the number of possible partitions with equal weights using combinations but I have never had success, and I don't know if this is an appropriate way to go about it. I have searched for any similar question on math.SE also but all of the results seem to be more concerned with complexity and other computational considerations relating to the partition problem. My question is: can anyone find a way of calculating $N_{min}(M)$ or even $p(N,M)$ or present a good approximation, or possibly elucidate wikipedia's calculation?",,"['probability', 'combinatorics', 'optimization']"
14,Probability of common sequences,Probability of common sequences,,"Let $\textbf{X}, \textbf{Y}$ be two fixed length sequences of length $d_x, d_y$, composed of a language of $K$ symbols. $p(\textbf{X}_i, \textbf{Y}_i = k) = \frac{1}{K}$, and there is no seuquential dependence to the symbols. What is the probability that $\textbf{X}$ and $\textbf{Y}$ have a common sequence of length $d'$ ($d' \leq d_x,d_y$), not necessarily in an unbroken sequence. Example: $\textbf{X} = \textbf{1}423\textbf{2}3134\textbf{21}2\textbf{4}211$ $\textbf{Y} = 423\textbf{11}31342211\textbf{4}$ $d_x = 16, d_y = 14, d'=11$. Common sequence: $42331342211$. Here's what I have so far, but I think I'm missing some things. The probability of any given sequence of length $d'$ is $(\frac{1}{K})^{d'}$. It can occur in any of the sequences ${d_x}\choose{d'}$, ${d_y}\choose{d'}$ times. So the probability that it is common to both of them is ${d_x}\choose{d'}$$*$${d_x}\choose{d'}$$*$$(\frac{1}{K})^{d'^{2}}$. Then, since we don't care about which sequence it is, I think we have to sum over all possible strings, and then we get $d'^K {d_x \choose d'} {d_y \choose d'} K^{-2d'}$.","Let $\textbf{X}, \textbf{Y}$ be two fixed length sequences of length $d_x, d_y$, composed of a language of $K$ symbols. $p(\textbf{X}_i, \textbf{Y}_i = k) = \frac{1}{K}$, and there is no seuquential dependence to the symbols. What is the probability that $\textbf{X}$ and $\textbf{Y}$ have a common sequence of length $d'$ ($d' \leq d_x,d_y$), not necessarily in an unbroken sequence. Example: $\textbf{X} = \textbf{1}423\textbf{2}3134\textbf{21}2\textbf{4}211$ $\textbf{Y} = 423\textbf{11}31342211\textbf{4}$ $d_x = 16, d_y = 14, d'=11$. Common sequence: $42331342211$. Here's what I have so far, but I think I'm missing some things. The probability of any given sequence of length $d'$ is $(\frac{1}{K})^{d'}$. It can occur in any of the sequences ${d_x}\choose{d'}$, ${d_y}\choose{d'}$ times. So the probability that it is common to both of them is ${d_x}\choose{d'}$$*$${d_x}\choose{d'}$$*$$(\frac{1}{K})^{d'^{2}}$. Then, since we don't care about which sequence it is, I think we have to sum over all possible strings, and then we get $d'^K {d_x \choose d'} {d_y \choose d'} K^{-2d'}$.",,"['probability', 'combinatorics']"
15,Entropy of the multinomial distribution,Entropy of the multinomial distribution,,"What is the entropy of the multinomial distribution? To fix notation, let us define $n > 0$ as the number of trials, $p_1, \ldots, p_k$ as the probabilities of each of the $k$ possible outcomes and $X_1, \ldots, X_n$ as the outcomes. Recall that the pmf of the multinomial distribution is given by $f(x; n,p) \equiv f(x_1,\ldots, x_k; n, p_1, \ldots, p_k) = \cases{ \frac{n!}{x_1! \ldots x_k!} p_1^{x_1} \ldots p_k^{x_k} \hspace{1cm} \text{if }\sum_{i=1}^{k} x_i = 1 \\ 0 \hspace{4cm} \text{otherwise} }$ The (Shannon) entropy of a distribution measures the amount of stored information or the uncertainty and for this distribution takes the form $H = - \sum f(x; n,p) \log{f(x; n,p)} = E[-\log{f(x; n,p)}]$, where the sum is over all $x = (x_1, \ldots, x_n)$ for which $\sum_{i=1}^{n} x_i = n$. The entropy for the binomial distribution can be calculated (see linked question). However, for the multinomial distribution it has only been shown that the entropy is maximized when $p_i = \frac{1}{k}$ for all $i$ [1, 2]. There is a recent paper [ 3 ] which sets upper and lower bounds on the entropy. However, a closed-form expression for the entropy seems not to have been derived yet. My questions are: (A) Is there a closed form for the special case  $p_i = \frac{1}{k} \hspace{0.5cm} \forall i$ ? (B) Are there other special cases for which the entropy can be calculated?  (C) Why is it so difficult to obtain a closed-form solution for this? Linked questions Entropy of a binomial distribution References [ 1 ]: P. Mateev, On the entropy of a multinomial distribution, Teor. Veroyatnost. i Primenen., 1978, Volume 23, Issue 1, 196–198, link . [ 2 ]: L.A. Shepp, I. Olkin, Entropy of the Sum of Independent Bernoulli Random Variables and of the Multinomial Distribution, Technical Report, 1978, link . [ 3 ]: Yuichi Kaji, Bounds on the entropy of multinomial distribution, 2015 IEEE International Symposium on Information Theory (ISIT), link .","What is the entropy of the multinomial distribution? To fix notation, let us define $n > 0$ as the number of trials, $p_1, \ldots, p_k$ as the probabilities of each of the $k$ possible outcomes and $X_1, \ldots, X_n$ as the outcomes. Recall that the pmf of the multinomial distribution is given by $f(x; n,p) \equiv f(x_1,\ldots, x_k; n, p_1, \ldots, p_k) = \cases{ \frac{n!}{x_1! \ldots x_k!} p_1^{x_1} \ldots p_k^{x_k} \hspace{1cm} \text{if }\sum_{i=1}^{k} x_i = 1 \\ 0 \hspace{4cm} \text{otherwise} }$ The (Shannon) entropy of a distribution measures the amount of stored information or the uncertainty and for this distribution takes the form $H = - \sum f(x; n,p) \log{f(x; n,p)} = E[-\log{f(x; n,p)}]$, where the sum is over all $x = (x_1, \ldots, x_n)$ for which $\sum_{i=1}^{n} x_i = n$. The entropy for the binomial distribution can be calculated (see linked question). However, for the multinomial distribution it has only been shown that the entropy is maximized when $p_i = \frac{1}{k}$ for all $i$ [1, 2]. There is a recent paper [ 3 ] which sets upper and lower bounds on the entropy. However, a closed-form expression for the entropy seems not to have been derived yet. My questions are: (A) Is there a closed form for the special case  $p_i = \frac{1}{k} \hspace{0.5cm} \forall i$ ? (B) Are there other special cases for which the entropy can be calculated?  (C) Why is it so difficult to obtain a closed-form solution for this? Linked questions Entropy of a binomial distribution References [ 1 ]: P. Mateev, On the entropy of a multinomial distribution, Teor. Veroyatnost. i Primenen., 1978, Volume 23, Issue 1, 196–198, link . [ 2 ]: L.A. Shepp, I. Olkin, Entropy of the Sum of Independent Bernoulli Random Variables and of the Multinomial Distribution, Technical Report, 1978, link . [ 3 ]: Yuichi Kaji, Bounds on the entropy of multinomial distribution, 2015 IEEE International Symposium on Information Theory (ISIT), link .",,"['probability', 'closed-form', 'information-theory', 'entropy', 'multinomial-distribution']"
16,Is it true that every finite sigma algebra is 'isomorphic' to some power set sigma algebra?,Is it true that every finite sigma algebra is 'isomorphic' to some power set sigma algebra?,,"Let $(\Omega_1,\mathcal {F_1})$ and $(\Omega_2,\mathcal {F_2})$ be two measurable spaces.we call $\mathcal{F_1}$ and $\mathcal {F_2}$ are isomorphic if there exist a measurable map $f: (\Omega_1,\mathcal {F_1}) \to (\Omega_2,\mathcal {F_2})$ such that the natural map $f_{*}: \mathcal {F_2} \to \mathcal {F_1}$ is bijective and preserves arbitrary union and complements. Let $(\Omega,\mathcal {F})$ $( \vert \mathcal {F}  \vert < \infty)$ and $(\{0,1\}^k,\mathcal {P}(\{0,1\}^k))$ be measurable spaces. Does there always exist a $k$ such that  $\mathcal {F}$ is isomorphic to $\mathcal {P}(\{0,1\}^k)$? I know that cardinality of $\mathcal {F}$ is $2^n$ for some $n$.Any ideas to proceed?","Let $(\Omega_1,\mathcal {F_1})$ and $(\Omega_2,\mathcal {F_2})$ be two measurable spaces.we call $\mathcal{F_1}$ and $\mathcal {F_2}$ are isomorphic if there exist a measurable map $f: (\Omega_1,\mathcal {F_1}) \to (\Omega_2,\mathcal {F_2})$ such that the natural map $f_{*}: \mathcal {F_2} \to \mathcal {F_1}$ is bijective and preserves arbitrary union and complements. Let $(\Omega,\mathcal {F})$ $( \vert \mathcal {F}  \vert < \infty)$ and $(\{0,1\}^k,\mathcal {P}(\{0,1\}^k))$ be measurable spaces. Does there always exist a $k$ such that  $\mathcal {F}$ is isomorphic to $\mathcal {P}(\{0,1\}^k)$? I know that cardinality of $\mathcal {F}$ is $2^n$ for some $n$.Any ideas to proceed?",,"['probability', 'measure-theory', 'category-theory']"
17,Finding a bound on the maximum of the absolute value of normal random variables,Finding a bound on the maximum of the absolute value of normal random variables,,"Suppose that $X_1, \ldots, X_n \sim N(0,1)$ are independent random variables. I am interested in finding a constant C that satisfies: $$ E\left[\max_{1\leq i\leq n}|X_i|\right] \leq C \sqrt{log\  n} $$ I know one method is to employ the moment generating function trick, then take logs of both sides. However, I was wondering if there exists a more direct method. thanks!","Suppose that $X_1, \ldots, X_n \sim N(0,1)$ are independent random variables. I am interested in finding a constant C that satisfies: $$ E\left[\max_{1\leq i\leq n}|X_i|\right] \leq C \sqrt{log\  n} $$ I know one method is to employ the moment generating function trick, then take logs of both sides. However, I was wondering if there exists a more direct method. thanks!",,"['probability', 'probability-theory']"
18,Expected number of rolls to get 1 followed by 2,Expected number of rolls to get 1 followed by 2,,"We have a fair, six-sided dice. The questions are What's the expected number of rolls to get 1 followed by 1? What's the expected number of rolls to get 1 followed by 2? Let $E$ be the expected time to get '11'. From the geometric distribution, we know that it takes on average 6 rolls to get a 1. Let us roll until we have a 1. Then there is a $1/6$ chance of being done in the next roll $$E = 6 + \frac{1}{6} \cdot 1 + \frac{5}{6}(1 + E) \qquad \Rightarrow \qquad E = 42.$$ I am fairly certain that my solution is correct for the first one, but I am a bit confused about the logic for the second one. I know its very similar to the first one but with a twist. Let $E$ be the expected time to get '12'. Then we first have to roll on average 6 times, after which we have a $1/6$ probability of being done (2), a $4/6$ chance of starting all over (3,4,5,6), and a $1/6$ probability of making no progress (1). Then $$E = 6 + \frac16\cdot 1 + \frac46 \cdot (1 + E) + \frac16 \cdot (1+6) \qquad\Rightarrow\qquad E=48.$$ Is the $(1+6)$ part right? For some reason I don't think it is.","We have a fair, six-sided dice. The questions are What's the expected number of rolls to get 1 followed by 1? What's the expected number of rolls to get 1 followed by 2? Let $E$ be the expected time to get '11'. From the geometric distribution, we know that it takes on average 6 rolls to get a 1. Let us roll until we have a 1. Then there is a $1/6$ chance of being done in the next roll $$E = 6 + \frac{1}{6} \cdot 1 + \frac{5}{6}(1 + E) \qquad \Rightarrow \qquad E = 42.$$ I am fairly certain that my solution is correct for the first one, but I am a bit confused about the logic for the second one. I know its very similar to the first one but with a twist. Let $E$ be the expected time to get '12'. Then we first have to roll on average 6 times, after which we have a $1/6$ probability of being done (2), a $4/6$ chance of starting all over (3,4,5,6), and a $1/6$ probability of making no progress (1). Then $$E = 6 + \frac16\cdot 1 + \frac46 \cdot (1 + E) + \frac16 \cdot (1+6) \qquad\Rightarrow\qquad E=48.$$ Is the $(1+6)$ part right? For some reason I don't think it is.",,['probability']
19,"Is it true that the probability that an ""almost prime"" is prime, tends to $0.59$?","Is it true that the probability that an ""almost prime"" is prime, tends to ?",0.59,"According to the estimation of the number of rough numbers I found in Wikipedia, the number of integers from $1$ to $x$ with no prime factor below $x^{1/u}$ tends to $\omega(u)\frac{x}{\ln(x^{1/u})}$ , if $x$ tends to $\infty$, where $\omega(u)$ is the Buchstab function. I concluded that the probability that a number $x$ without a prime factor smaller or equal to $x^{1/3}$ (Let us denote such a number ""almost-prime"", although this name might be a sysnonym for a semi-prime) tends to  $\frac{\omega(3)}{\ln(x^{1/3})}$ , if $x$ tends to $\infty$. Since the probability that a number $x$ is prime tends to $\frac{1}{\ln(x)}$ , if $x$ tends to $\infty$, this implies that the probability that an ""almost-prime"" is prime tends to $\frac{\ln(x^{1/3})}{\omega(3)\ln(x)}=\frac{1}{3\omega(3)}=\frac{1}{3\cdot 0.564382}=0.5906$ Are my thoughts correct ? Calculations seem to backup the claim that the probability tends to roughly $59$% : almostprime(n)={local(w);w=factorint(n);w=component(w,1)~;w=w[1];w>sqrtnint(n,3)}  ? q=0;r=0;for(m=10,100,if(almostprime(m)==1,q=q+1);if(isprime(m)==1,r=r+1));prin t(r,""    "",q,""     "",r/q*1.0) 21    32     0.6562500000000000000000000000 ? q=0;r=0;for(m=100,1000,if(almostprime(m)==1,q=q+1);if(isprime(m)==1,r=r+1));pr int(r,""    "",q,""     "",r/q*1.0) 143    216     0.6620370370370370370370370370 ? q=0;r=0;for(m=1000,10^4,if(almostprime(m)==1,q=q+1);if(isprime(m)==1,r=r+1));p rint(r,""    "",q,""     "",r/q*1.0) 1061    1667     0.6364727054589082183563287343 ? q=0;r=0;for(m=10^4,10^5,if(almostprime(m)==1,q=q+1);if(isprime(m)==1,r=r+1));p rint(r,""    "",q,""     "",r/q*1.0) 8363    13608     0.6145649617871840094062316285 ? q=0;r=0;for(m=10^5,10^6,if(almostprime(m)==1,q=q+1);if(isprime(m)==1,r=r+1));p rint(r,""    "",q,""     "",r/q*1.0) 68906    113951     0.6046985107633983027792647717 ? q=0;r=0;for(m=10^6,10^7,if(almostprime(m)==1,q=q+1);if(isprime(m)==1,r=r+1));p rint(r,""    "",q,""     "",r/q*1.0) 586081    977543     0.5995449816529809941864449953 ? q=0;r=0;for(m=10^7,10^8,if(almostprime(m)==1,q=q+1);if(isprime(m)==1,r=r+1));p rint(r,""    "",q,""     "",r/q*1.0) 5096876    8559062     0.5954946932268979941960929831 ? Can we expect that the probability will soon tend to $0.5906$, if we continue ? I do not expect a strictly motontone decreasing probability, but it should reach a value near $0.5906$ and then maybe oscillating around this value. Can this be proven ?","According to the estimation of the number of rough numbers I found in Wikipedia, the number of integers from $1$ to $x$ with no prime factor below $x^{1/u}$ tends to $\omega(u)\frac{x}{\ln(x^{1/u})}$ , if $x$ tends to $\infty$, where $\omega(u)$ is the Buchstab function. I concluded that the probability that a number $x$ without a prime factor smaller or equal to $x^{1/3}$ (Let us denote such a number ""almost-prime"", although this name might be a sysnonym for a semi-prime) tends to  $\frac{\omega(3)}{\ln(x^{1/3})}$ , if $x$ tends to $\infty$. Since the probability that a number $x$ is prime tends to $\frac{1}{\ln(x)}$ , if $x$ tends to $\infty$, this implies that the probability that an ""almost-prime"" is prime tends to $\frac{\ln(x^{1/3})}{\omega(3)\ln(x)}=\frac{1}{3\omega(3)}=\frac{1}{3\cdot 0.564382}=0.5906$ Are my thoughts correct ? Calculations seem to backup the claim that the probability tends to roughly $59$% : almostprime(n)={local(w);w=factorint(n);w=component(w,1)~;w=w[1];w>sqrtnint(n,3)}  ? q=0;r=0;for(m=10,100,if(almostprime(m)==1,q=q+1);if(isprime(m)==1,r=r+1));prin t(r,""    "",q,""     "",r/q*1.0) 21    32     0.6562500000000000000000000000 ? q=0;r=0;for(m=100,1000,if(almostprime(m)==1,q=q+1);if(isprime(m)==1,r=r+1));pr int(r,""    "",q,""     "",r/q*1.0) 143    216     0.6620370370370370370370370370 ? q=0;r=0;for(m=1000,10^4,if(almostprime(m)==1,q=q+1);if(isprime(m)==1,r=r+1));p rint(r,""    "",q,""     "",r/q*1.0) 1061    1667     0.6364727054589082183563287343 ? q=0;r=0;for(m=10^4,10^5,if(almostprime(m)==1,q=q+1);if(isprime(m)==1,r=r+1));p rint(r,""    "",q,""     "",r/q*1.0) 8363    13608     0.6145649617871840094062316285 ? q=0;r=0;for(m=10^5,10^6,if(almostprime(m)==1,q=q+1);if(isprime(m)==1,r=r+1));p rint(r,""    "",q,""     "",r/q*1.0) 68906    113951     0.6046985107633983027792647717 ? q=0;r=0;for(m=10^6,10^7,if(almostprime(m)==1,q=q+1);if(isprime(m)==1,r=r+1));p rint(r,""    "",q,""     "",r/q*1.0) 586081    977543     0.5995449816529809941864449953 ? q=0;r=0;for(m=10^7,10^8,if(almostprime(m)==1,q=q+1);if(isprime(m)==1,r=r+1));p rint(r,""    "",q,""     "",r/q*1.0) 5096876    8559062     0.5954946932268979941960929831 ? Can we expect that the probability will soon tend to $0.5906$, if we continue ? I do not expect a strictly motontone decreasing probability, but it should reach a value near $0.5906$ and then maybe oscillating around this value. Can this be proven ?",,"['probability', 'number-theory', 'functions', 'prime-numbers']"
20,Bound for sum of normal distributions,Bound for sum of normal distributions,,"I have encountered an exercise that was quite puzzling for me. Maybe someone can help me out here? So let $(X_n)_n $ be $N(-a,1)$ distributed, independent random variables where $a>0$. I need to prove $$P(\sup_{n \in \mathbb N} S_n > x)\leq e^{-2ax}$$ where $S_n= \sum_{k=1}^n X_k$. I am tempted to use Markovs inequality but the supremum kind of irritates me. Can I somehow use that $e^{h S_n}$ is a Martingale if $h=2a$ which I proved in the first part of the exercise?","I have encountered an exercise that was quite puzzling for me. Maybe someone can help me out here? So let $(X_n)_n $ be $N(-a,1)$ distributed, independent random variables where $a>0$. I need to prove $$P(\sup_{n \in \mathbb N} S_n > x)\leq e^{-2ax}$$ where $S_n= \sum_{k=1}^n X_k$. I am tempted to use Markovs inequality but the supremum kind of irritates me. Can I somehow use that $e^{h S_n}$ is a Martingale if $h=2a$ which I proved in the first part of the exercise?",,"['probability', 'probability-theory']"
21,Kolmogoroff 0-1 does this proof work?,Kolmogoroff 0-1 does this proof work?,,"I have thought at this proof of the Kolmogorov 0-1 Law varying a little the sketch found in Probability essentials (Jean Jacod, Philip Protter). My questions are Is it a valid proof? Is it a bad proof? (And by bad I mainly mean that written properly will result too heavy) Given $ \mathcal{B}_n $ the $\sigma$-algebra pulled back from $X_n$ $$\mathcal{C}_n := \sigma\left(\bigcup_{m\geq n} \mathcal{B}_m\right) $$ $$\mathcal{C}_\infty := \sigma\left(\bigcap_{n=1}^{\infty} \mathcal{C}_n\right) $$ $$\mathcal{D}_n := \sigma\left(\bigcup_{i=1}^{n} \mathcal{B}_i\right) $$ $$\mathcal{D}_\infty := \sigma\left(\bigcup_{i=1}^{\infty} \mathcal{C}_i \right) = \mathcal{C}_1 = \sigma\left(\bigcup_{i=1}^{\infty} \mathcal{D}_i \right) $$ I would like to apply what the book calls the Monotone Class Theorem (but Dinkin $\pi$ - $\lambda$ theorem might be a better name according to: https://math.stackexchange.com/a/1841800/88132 ) to $\bigcup_{i=1}^{\infty} \mathcal{D}_i $. That set contains $\Omega$ (every $\mathcal{D}_i$ does) and it is closed under differences and finite intersections (I could show why... but come on! Every finite property of that set relies on the finite properties of a single $\mathcal{D}_i$ since $\mathcal{D}_1 \subseteq\mathcal{D}_2 \subseteq\mathcal{D}_3 \subseteq \dots$ should I be more verbose?). The theorem tells me that $\mathcal{C}_{1} = \sigma \left(\bigcup_{i=1}^{\infty} \mathcal{D}_i\right) $ is the smallest class closed by difference and increasing limit. This means that every element $A$ of $\mathcal{C}_{1}$ can be written as  $$ A = \bigcup_{n=1}^{\infty} A_n \ \text{ where } \ A_n \in \mathcal{D}_n \ \text{ and } \ A_1 \subseteq A_2 \subseteq A_3 \dots $$ or $$ A = \bigcap_{n=1}^{\infty} A_n \ \text{ where } \ A_n \in \mathcal{D}_n \ \text{ and } \ A_1 \supseteq A_2 \supseteq A_3 \dots $$ This is the key point, for me it is in some way obvious that this is true and clearly follows the definition of increasing limit. Should I write something else? What? Given this and the fact that the book shows that for every $A \in \mathcal{D}_i$ and $B \in \mathcal{C}_{i+1}$ $$ P(A \cap B) = P(A)P(B) $$ The same relation must hold for every $A \in \mathcal{D}_i$ and $B \in \mathcal{C}_{\infty}$ and therefore $$ P(A_n \cap A) = P(A_n)P(A) $$ Taking limits we get $$ P(A) = P(A)^2 $$ From which $$ P(A) = 0 \text{ or } 1.$$","I have thought at this proof of the Kolmogorov 0-1 Law varying a little the sketch found in Probability essentials (Jean Jacod, Philip Protter). My questions are Is it a valid proof? Is it a bad proof? (And by bad I mainly mean that written properly will result too heavy) Given $ \mathcal{B}_n $ the $\sigma$-algebra pulled back from $X_n$ $$\mathcal{C}_n := \sigma\left(\bigcup_{m\geq n} \mathcal{B}_m\right) $$ $$\mathcal{C}_\infty := \sigma\left(\bigcap_{n=1}^{\infty} \mathcal{C}_n\right) $$ $$\mathcal{D}_n := \sigma\left(\bigcup_{i=1}^{n} \mathcal{B}_i\right) $$ $$\mathcal{D}_\infty := \sigma\left(\bigcup_{i=1}^{\infty} \mathcal{C}_i \right) = \mathcal{C}_1 = \sigma\left(\bigcup_{i=1}^{\infty} \mathcal{D}_i \right) $$ I would like to apply what the book calls the Monotone Class Theorem (but Dinkin $\pi$ - $\lambda$ theorem might be a better name according to: https://math.stackexchange.com/a/1841800/88132 ) to $\bigcup_{i=1}^{\infty} \mathcal{D}_i $. That set contains $\Omega$ (every $\mathcal{D}_i$ does) and it is closed under differences and finite intersections (I could show why... but come on! Every finite property of that set relies on the finite properties of a single $\mathcal{D}_i$ since $\mathcal{D}_1 \subseteq\mathcal{D}_2 \subseteq\mathcal{D}_3 \subseteq \dots$ should I be more verbose?). The theorem tells me that $\mathcal{C}_{1} = \sigma \left(\bigcup_{i=1}^{\infty} \mathcal{D}_i\right) $ is the smallest class closed by difference and increasing limit. This means that every element $A$ of $\mathcal{C}_{1}$ can be written as  $$ A = \bigcup_{n=1}^{\infty} A_n \ \text{ where } \ A_n \in \mathcal{D}_n \ \text{ and } \ A_1 \subseteq A_2 \subseteq A_3 \dots $$ or $$ A = \bigcap_{n=1}^{\infty} A_n \ \text{ where } \ A_n \in \mathcal{D}_n \ \text{ and } \ A_1 \supseteq A_2 \supseteq A_3 \dots $$ This is the key point, for me it is in some way obvious that this is true and clearly follows the definition of increasing limit. Should I write something else? What? Given this and the fact that the book shows that for every $A \in \mathcal{D}_i$ and $B \in \mathcal{C}_{i+1}$ $$ P(A \cap B) = P(A)P(B) $$ The same relation must hold for every $A \in \mathcal{D}_i$ and $B \in \mathcal{C}_{\infty}$ and therefore $$ P(A_n \cap A) = P(A_n)P(A) $$ Taking limits we get $$ P(A) = P(A)^2 $$ From which $$ P(A) = 0 \text{ or } 1.$$",,"['probability', 'probability-theory', 'measure-theory', 'monotone-class-theorem']"
22,Moment Generating Function for $r$th central moment,Moment Generating Function for th central moment,r,"When using moment generating functions, to find the $n$th raw moment (""$n$th moment about the origin""), you take the $n$th derivative of the MGF and evaluate at $t=0$. To find the $m$th central moment (""$m$th moment about the mean""), e.g. $m=2$ for the variance, you need to evaluate the MGF twice (once for $m=1$, once for $m=2$) and use the relationship: $\text{var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$ ------------------------------------------------------------- Is there an alternative kind of generating function such that to find the $r$th central moment (""$r$th moment about the mean"") you only need to evaluate that generating function a single time?","When using moment generating functions, to find the $n$th raw moment (""$n$th moment about the origin""), you take the $n$th derivative of the MGF and evaluate at $t=0$. To find the $m$th central moment (""$m$th moment about the mean""), e.g. $m=2$ for the variance, you need to evaluate the MGF twice (once for $m=1$, once for $m=2$) and use the relationship: $\text{var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$ ------------------------------------------------------------- Is there an alternative kind of generating function such that to find the $r$th central moment (""$r$th moment about the mean"") you only need to evaluate that generating function a single time?",,"['probability', 'probability-distributions', 'generating-functions', 'moment-generating-functions', 'cumulants']"
23,Placing spheres uniformly at random over $\mathbb{R}^3$,Placing spheres uniformly at random over,\mathbb{R}^3,"Put spheres uniformly at random all over $\mathbb{R}^3$, with density 1 sphere / unit cube. All spheres have the same radius $r$. What is the probability function $p(r)$, that that there is an infinite component, that is an infinite sequence of spheres such that the intersection of $s_n$ with $s_{n+1}$ is nonzero? The distribution can be constructed like this: Partition $\mathbb{R}^3$ into cubes of sidelength $L$ and put $L^3$ spheres uniformly at random inside each. Then we take the distribution that arises as $L$ goes to infinity.","Put spheres uniformly at random all over $\mathbb{R}^3$, with density 1 sphere / unit cube. All spheres have the same radius $r$. What is the probability function $p(r)$, that that there is an infinite component, that is an infinite sequence of spheres such that the intersection of $s_n$ with $s_{n+1}$ is nonzero? The distribution can be constructed like this: Partition $\mathbb{R}^3$ into cubes of sidelength $L$ and put $L^3$ spheres uniformly at random inside each. Then we take the distribution that arises as $L$ goes to infinity.",,['probability']
24,"A ""trick"" with a deck of cards, probability of equal result","A ""trick"" with a deck of cards, probability of equal result",,"I recalled this ""trick"" I read in some magazine years ago and thought I'd try to explore it a bit more. You have standard deck of 52 cards. The cards are on the table, facing up. Your friend secredly selects a number, $n_0$, from 1 to 13. Then your friend lifts $n_0$ cards one by one. Then he selects the value of the $n_0$th card, say $n_1$, as his new number and lifts again $n_1$ cards and takes the value of the $n_1$th as the new value. This continues until all the cards have been lifted. Let the value he has after the final card be $n_v$. All this is done silently. The trick is, the magazine stated, that you can, with somewhat high probability, predict his final number. The idea is to choose your own number, $n_0^*$, and do the above process yourself as your friend does his counting. There is, apparently, high probability that your final value is the same as his. Not a very good trick, in my opinion. I did some simulations. It seems that the probability that the values are the same is a bit lower than $0.7$. How should one analyze this kind of a situation and what is the correct probability? I remember the article mentioned Markov chains, but I cannot see how I could use them in this case. The process is finite and I don't think this has the Markov's property. I tried constructing some large transition matrix, but nothing came of it. I tried some simplifications, like using only three cards with numbers 1,2 and 3, but the calculations become very messy rather quickly. Edit: I did some more simulations, 200,000 runs for each pair $\{n_0,n_0^*\}$ and got the following matrix for probability of same result, rounded to save space. $\left( \begin{array}{ccccccccccccc}  1. & 0.7 & 0.7 & 0.7 & 0.69 & 0.69 & 0.69 & 0.68 & 0.68 & 0.68 & 0.67 & 0.67 & 0.67 \\  0.7 & 1. & 0.7 & 0.69 & 0.69 & 0.69 & 0.68 & 0.68 & 0.67 & 0.67 & 0.67 & 0.66 & 0.66 \\  0.7 & 0.7 & 1. & 0.69 & 0.69 & 0.68 & 0.68 & 0.67 & 0.67 & 0.67 & 0.66 & 0.66 & 0.66 \\  0.7 & 0.69 & 0.69 & 1. & 0.68 & 0.68 & 0.67 & 0.67 & 0.67 & 0.66 & 0.66 & 0.65 & 0.65 \\  0.69 & 0.69 & 0.69 & 0.68 & 1. & 0.68 & 0.67 & 0.67 & 0.66 & 0.66 & 0.65 & 0.65 & 0.65 \\  0.69 & 0.69 & 0.68 & 0.68 & 0.68 & 1. & 0.67 & 0.66 & 0.66 & 0.66 & 0.65 & 0.65 & 0.64 \\  0.69 & 0.68 & 0.68 & 0.67 & 0.67 & 0.67 & 1. & 0.66 & 0.66 & 0.65 & 0.65 & 0.64 & 0.64 \\  0.68 & 0.68 & 0.67 & 0.67 & 0.67 & 0.66 & 0.66 & 1. & 0.65 & 0.65 & 0.64 & 0.64 & 0.63 \\  0.68 & 0.67 & 0.67 & 0.67 & 0.66 & 0.66 & 0.66 & 0.65 & 1. & 0.64 & 0.64 & 0.64 & 0.63 \\  0.68 & 0.67 & 0.67 & 0.66 & 0.66 & 0.66 & 0.65 & 0.65 & 0.64 & 1. & 0.64 & 0.63 & 0.63 \\  0.67 & 0.67 & 0.66 & 0.66 & 0.65 & 0.65 & 0.65 & 0.64 & 0.64 & 0.64 & 1. & 0.63 & 0.62 \\  0.67 & 0.66 & 0.66 & 0.65 & 0.65 & 0.65 & 0.64 & 0.64 & 0.64 & 0.63 & 0.63 & 1. & 0.62 \\  0.67 & 0.66 & 0.66 & 0.65 & 0.65 & 0.64 & 0.64 & 0.63 & 0.63 & 0.63 & 0.62 & 0.62 & 1. \\ \end{array} \right)$ Actually, as the order doesn't matter, I simulated only cases with $n_0^* > n_0$ and copied the results to the lower triangle. The diagonal elements are all one, for if both players select the same starting number, the results are identical. If I calculated it correctly, for the elements outside the diagonal, a 95% two-sided confidence interval is $\text{value}\pm 0.00219131.$ In any case, the best strategy seems to be to select $n_0^* = 1$.","I recalled this ""trick"" I read in some magazine years ago and thought I'd try to explore it a bit more. You have standard deck of 52 cards. The cards are on the table, facing up. Your friend secredly selects a number, $n_0$, from 1 to 13. Then your friend lifts $n_0$ cards one by one. Then he selects the value of the $n_0$th card, say $n_1$, as his new number and lifts again $n_1$ cards and takes the value of the $n_1$th as the new value. This continues until all the cards have been lifted. Let the value he has after the final card be $n_v$. All this is done silently. The trick is, the magazine stated, that you can, with somewhat high probability, predict his final number. The idea is to choose your own number, $n_0^*$, and do the above process yourself as your friend does his counting. There is, apparently, high probability that your final value is the same as his. Not a very good trick, in my opinion. I did some simulations. It seems that the probability that the values are the same is a bit lower than $0.7$. How should one analyze this kind of a situation and what is the correct probability? I remember the article mentioned Markov chains, but I cannot see how I could use them in this case. The process is finite and I don't think this has the Markov's property. I tried constructing some large transition matrix, but nothing came of it. I tried some simplifications, like using only three cards with numbers 1,2 and 3, but the calculations become very messy rather quickly. Edit: I did some more simulations, 200,000 runs for each pair $\{n_0,n_0^*\}$ and got the following matrix for probability of same result, rounded to save space. $\left( \begin{array}{ccccccccccccc}  1. & 0.7 & 0.7 & 0.7 & 0.69 & 0.69 & 0.69 & 0.68 & 0.68 & 0.68 & 0.67 & 0.67 & 0.67 \\  0.7 & 1. & 0.7 & 0.69 & 0.69 & 0.69 & 0.68 & 0.68 & 0.67 & 0.67 & 0.67 & 0.66 & 0.66 \\  0.7 & 0.7 & 1. & 0.69 & 0.69 & 0.68 & 0.68 & 0.67 & 0.67 & 0.67 & 0.66 & 0.66 & 0.66 \\  0.7 & 0.69 & 0.69 & 1. & 0.68 & 0.68 & 0.67 & 0.67 & 0.67 & 0.66 & 0.66 & 0.65 & 0.65 \\  0.69 & 0.69 & 0.69 & 0.68 & 1. & 0.68 & 0.67 & 0.67 & 0.66 & 0.66 & 0.65 & 0.65 & 0.65 \\  0.69 & 0.69 & 0.68 & 0.68 & 0.68 & 1. & 0.67 & 0.66 & 0.66 & 0.66 & 0.65 & 0.65 & 0.64 \\  0.69 & 0.68 & 0.68 & 0.67 & 0.67 & 0.67 & 1. & 0.66 & 0.66 & 0.65 & 0.65 & 0.64 & 0.64 \\  0.68 & 0.68 & 0.67 & 0.67 & 0.67 & 0.66 & 0.66 & 1. & 0.65 & 0.65 & 0.64 & 0.64 & 0.63 \\  0.68 & 0.67 & 0.67 & 0.67 & 0.66 & 0.66 & 0.66 & 0.65 & 1. & 0.64 & 0.64 & 0.64 & 0.63 \\  0.68 & 0.67 & 0.67 & 0.66 & 0.66 & 0.66 & 0.65 & 0.65 & 0.64 & 1. & 0.64 & 0.63 & 0.63 \\  0.67 & 0.67 & 0.66 & 0.66 & 0.65 & 0.65 & 0.65 & 0.64 & 0.64 & 0.64 & 1. & 0.63 & 0.62 \\  0.67 & 0.66 & 0.66 & 0.65 & 0.65 & 0.65 & 0.64 & 0.64 & 0.64 & 0.63 & 0.63 & 1. & 0.62 \\  0.67 & 0.66 & 0.66 & 0.65 & 0.65 & 0.64 & 0.64 & 0.63 & 0.63 & 0.63 & 0.62 & 0.62 & 1. \\ \end{array} \right)$ Actually, as the order doesn't matter, I simulated only cases with $n_0^* > n_0$ and copied the results to the lower triangle. The diagonal elements are all one, for if both players select the same starting number, the results are identical. If I calculated it correctly, for the elements outside the diagonal, a 95% two-sided confidence interval is $\text{value}\pm 0.00219131.$ In any case, the best strategy seems to be to select $n_0^* = 1$.",,['probability']
25,Probability of having at least $j$ collisions when tossing $m$ balls into $n$ bins,Probability of having at least  collisions when tossing  balls into  bins,j m n,"Suppose that we throw $m$ balls into $n$ bins uniformly and independantly at random. We consider collisions as distinct unordered pairs, e.g., if 3 balls are tossed in one bin, we count 3 collisions. What is the probability of having at least $j$ collisions when we throw $m$ balls into $n$ bins, such that at least $j$ bins are empty at the end of the process? ##Case $m \leq n$ : As we suppose $m < n$ , this question is in fact equivalent to computing the probability that we toss $m$ balls in $m-j$ bins. Indeed, we throw our $m-j$ first balls in distinct bins, and the $j$ remaining balls in these $m-j$ bins. We can view tossing $m$ balls in $m-j$ as the following equation: $$x_1 + x_2 + \cdots + x_{m-j} = m$$ where $x_i$ is the number of balls in bin $i$ . The number of solutions in $\mathbb{N}$ of this equation is a $m$ -combination on a set of size $m-j$ , i.e., $\binom{m+m-j-1}{m} = \binom{2m - j - 1}{m}$ . The number of ways to choose $m-j$ bins out of $n$ is $\binom{n}{m-j}$ . The number of possibilities to throw $m$ balls in $n$ bins is $n^m$ , thus $$\mathbb{P}[\text{at least } j \text{ collisions}] = \frac{\binom{2m - j - 1}{m} \cdot \binom{n}{m-j}}{n^m}.$$ What do you think of this approach? Am I overcounting/undercounting something? ##Case $m > n$ : In my opinion, this case is much harder to solve. As I count collisions as distinct unordered pairs, the total number of collisions is $$\sum_{i = 1}^n \binom{x_i}{2}$$ where $x_i$ is the number of balls in bin $i$ . Hence, to have exactly $j$ collisions, a necessary condition is to have $\sum_{i=1}^{n} \binom{x_i}{2} = j$ . Let's tweak this equation a bit. $$ \begin{align*} \sum_{i=1}^{n} \binom{x_i}{2} = j & \iff \sum_{i=1}^n \frac{x_i!}{2 \cdot (x_i-2)!} = j\\ & \iff \sum_{i=1}^{n} \frac{x_i!}{(x_i-2)!} = 2j\\ & \iff \sum_{i=1}^n x_i(x_i-1) = 2j\\ & \iff \sum_{i=1}^n x_i^2 = 2j + \sum_{i=1}^n x_i\\ & \iff \sum_{i=1}^n x_i^2 = 2j + m \end{align*} $$ where the last equality follows from the fact the sum of the balls in each bins is the number of balls tossed. Therefore, this problem is equivalent to solving the following problem: how many representations as a sum of $n$ squares does a number have? This problem seems far from trivial to me, as I did not find easy explicit formulas to work with. However this approach was for finding exactly $j$ collisions, and I am interested in finding at least $j$ collisions. As $m > n$ , we will have some collisions. We can express $m = qn + r$ for some $q, r \in \mathbb{N}$ . The least collisions we will have is when the balls are uniformly distributed amongst the bins, thus we will have at least $$C_{min} = \sum_{i=1}^r \binom{q+1}{2} + \sum_{i=1}^{n-r} \binom{q}{2}$$ collisions, which forces $j > C_{min}$ . The problem thus becomes, in how many ways can we distribute $m$ balls into $n$ bins such that $\sum_{i=1}^n \binom{x_i}{2} \geq j$ . Here I am stuck, I can't find an easy way with combinatorics to solve this. I am fine with this problem being solved asymptotically, so if you have any ideas and/or references on the subject... :). Thanks for reading and helping #Edit I think both cases above are wrong, because I was undercounting some things. $m \leq n$ "" /> This image shows that the probability of having $j$ collisions do not increase nor decrease. I was expecting something like a gaussian, or at least something with a low probabiliy of having a few collisions, a high probability of having an average number of collisions, and a low probability of having a lot of collisions. I thought about another way of counting, and I think the results I obtain are better. For that, I need the following Lemma, taken from this paper, page 16 . Lemma 1: The probability that exactly $k$ bins are not empty after throwing $m$ balls is $\frac{\binom{n}{k}k!S(m,k)}{n^m}$ , where $S(m,k)$ is a Stirling number of the second kind. With this Lemma, I obtain the following result. Lemma 2: Let $m$ be the number of balls thrown uniformly and independently ar random into $n$ bins. The probability of having at least $j>1$ collisions and at least $j$ empty bins is $$ \mathbb{P}[\text{at least } j \text{ collisions}] = \frac{1}{n^{m}} \sum_{i=1}^{n-j} \binom{n}{i} i! S(m,i). $$ Proof: As explained previously, the probability of having at least $j$ collisions and at least $j$ empty bins (or at most $n-j$ non-empty bins) is equal to the probability of tossing $m$ balls in at most $n-j$ bins. Let $A$ be the event ""at most $n-j$ bins are not empty"" and let $A_i$ be the event ""exactly $i$ bins are not empty"". Then $A = \cup_{i=1}^{n-j} A_j$ and $A_i \cap A_k = \emptyset$ if $i \neq k$ , hence $\mathbb{P}[A] = \sum_{i=1}^{n-j}\mathbb{P}[A_j]$ and Lemma 1 concludes the proof. $\square$ We can see on the next plot that this results already seems closer to reality (this was plotted for $n = 500$ , $m = 200$ and $j$ from $1$ to $1000$ ). Moreover, with this approach, we don't need to separate the cases $m \leq n$ and $m > n$ (at least I think, you might need to correct me on this). The downside is that now we have to work with a sum... and Stirling numbers of the second kind... Do you guys have any idea to remove the sum and obtain an upper bound of this?","Suppose that we throw balls into bins uniformly and independantly at random. We consider collisions as distinct unordered pairs, e.g., if 3 balls are tossed in one bin, we count 3 collisions. What is the probability of having at least collisions when we throw balls into bins, such that at least bins are empty at the end of the process? ##Case : As we suppose , this question is in fact equivalent to computing the probability that we toss balls in bins. Indeed, we throw our first balls in distinct bins, and the remaining balls in these bins. We can view tossing balls in as the following equation: where is the number of balls in bin . The number of solutions in of this equation is a -combination on a set of size , i.e., . The number of ways to choose bins out of is . The number of possibilities to throw balls in bins is , thus What do you think of this approach? Am I overcounting/undercounting something? ##Case : In my opinion, this case is much harder to solve. As I count collisions as distinct unordered pairs, the total number of collisions is where is the number of balls in bin . Hence, to have exactly collisions, a necessary condition is to have . Let's tweak this equation a bit. where the last equality follows from the fact the sum of the balls in each bins is the number of balls tossed. Therefore, this problem is equivalent to solving the following problem: how many representations as a sum of squares does a number have? This problem seems far from trivial to me, as I did not find easy explicit formulas to work with. However this approach was for finding exactly collisions, and I am interested in finding at least collisions. As , we will have some collisions. We can express for some . The least collisions we will have is when the balls are uniformly distributed amongst the bins, thus we will have at least collisions, which forces . The problem thus becomes, in how many ways can we distribute balls into bins such that . Here I am stuck, I can't find an easy way with combinatorics to solve this. I am fine with this problem being solved asymptotically, so if you have any ideas and/or references on the subject... :). Thanks for reading and helping #Edit I think both cases above are wrong, because I was undercounting some things. $m \leq n$ "" /> This image shows that the probability of having collisions do not increase nor decrease. I was expecting something like a gaussian, or at least something with a low probabiliy of having a few collisions, a high probability of having an average number of collisions, and a low probability of having a lot of collisions. I thought about another way of counting, and I think the results I obtain are better. For that, I need the following Lemma, taken from this paper, page 16 . Lemma 1: The probability that exactly bins are not empty after throwing balls is , where is a Stirling number of the second kind. With this Lemma, I obtain the following result. Lemma 2: Let be the number of balls thrown uniformly and independently ar random into bins. The probability of having at least collisions and at least empty bins is Proof: As explained previously, the probability of having at least collisions and at least empty bins (or at most non-empty bins) is equal to the probability of tossing balls in at most bins. Let be the event ""at most bins are not empty"" and let be the event ""exactly bins are not empty"". Then and if , hence and Lemma 1 concludes the proof. We can see on the next plot that this results already seems closer to reality (this was plotted for , and from to ). Moreover, with this approach, we don't need to separate the cases and (at least I think, you might need to correct me on this). The downside is that now we have to work with a sum... and Stirling numbers of the second kind... Do you guys have any idea to remove the sum and obtain an upper bound of this?","m n j m n j m \leq n m < n m m-j m-j j m-j m m-j x_1 + x_2 + \cdots + x_{m-j} = m x_i i \mathbb{N} m m-j \binom{m+m-j-1}{m} = \binom{2m - j - 1}{m} m-j n \binom{n}{m-j} m n n^m \mathbb{P}[\text{at least } j \text{ collisions}] = \frac{\binom{2m - j - 1}{m} \cdot \binom{n}{m-j}}{n^m}. m > n \sum_{i = 1}^n \binom{x_i}{2} x_i i j \sum_{i=1}^{n} \binom{x_i}{2} = j  \begin{align*}
\sum_{i=1}^{n} \binom{x_i}{2} = j & \iff \sum_{i=1}^n \frac{x_i!}{2 \cdot (x_i-2)!} = j\\
& \iff \sum_{i=1}^{n} \frac{x_i!}{(x_i-2)!} = 2j\\
& \iff \sum_{i=1}^n x_i(x_i-1) = 2j\\
& \iff \sum_{i=1}^n x_i^2 = 2j + \sum_{i=1}^n x_i\\
& \iff \sum_{i=1}^n x_i^2 = 2j + m
\end{align*}
 n j j m > n m = qn + r q, r \in \mathbb{N} C_{min} = \sum_{i=1}^r \binom{q+1}{2} + \sum_{i=1}^{n-r} \binom{q}{2} j > C_{min} m n \sum_{i=1}^n \binom{x_i}{2} \geq j j k m \frac{\binom{n}{k}k!S(m,k)}{n^m} S(m,k) m n j>1 j  \mathbb{P}[\text{at least } j \text{ collisions}] = \frac{1}{n^{m}} \sum_{i=1}^{n-j} \binom{n}{i} i! S(m,i).  j j n-j m n-j A n-j A_i i A = \cup_{i=1}^{n-j} A_j A_i \cap A_k = \emptyset i \neq k \mathbb{P}[A] = \sum_{i=1}^{n-j}\mathbb{P}[A_j] \square n = 500 m = 200 j 1 1000 m \leq n m > n","['probability', 'combinatorics', 'solution-verification', 'sums-of-squares', 'balls-in-bins']"
26,Finding the mean of $X_t = \int_0^t sW_sdW_s$,Finding the mean of,X_t = \int_0^t sW_sdW_s,"For the stochastic integral, where $W_t$ is a Wiener process, I am trying to find the mean of $X_t = \int_0^t sW_sdW_s$. I have read before that any stochastic integral with $dWt$ has mean zero, but I dont know if it extends to cases where I have a random variable in the integrand as well. My approach is to decompose the integral $X_t$ into: $$ \int_0^t sW_sdW_s = \lim_{n \to \infty}\sum_{j=0}^{n-1}t_jW_{t_i}(W_{t_{i+1}}-W_{t_i}) $$ Then, $$ E\left(\int_0^t sW_sdW_s\right) = \lim_{n \to \infty}\sum_{j=0}^{n-1}E\left(t_jW_{t_i}(W_{t_{i+1}}-W_{t_i})\right) $$ I believe that I can separate the terms in the expectation into: $$ E\left(t_jW_{t_i}(W_{t_{i+1}}-W_{t_i})\right) = E\left(t_jW_{t_i}\right)E\left(W_{t_{i+1}}-W_{t_i}\right) $$ I am not sure how to find $E\left(t_jW_{t_i}\right)$, although I know it is finite so it doesn't matter because $E\left(W_{t_{i+1}}-W_{t_i}\right) = 0$","For the stochastic integral, where $W_t$ is a Wiener process, I am trying to find the mean of $X_t = \int_0^t sW_sdW_s$. I have read before that any stochastic integral with $dWt$ has mean zero, but I dont know if it extends to cases where I have a random variable in the integrand as well. My approach is to decompose the integral $X_t$ into: $$ \int_0^t sW_sdW_s = \lim_{n \to \infty}\sum_{j=0}^{n-1}t_jW_{t_i}(W_{t_{i+1}}-W_{t_i}) $$ Then, $$ E\left(\int_0^t sW_sdW_s\right) = \lim_{n \to \infty}\sum_{j=0}^{n-1}E\left(t_jW_{t_i}(W_{t_{i+1}}-W_{t_i})\right) $$ I believe that I can separate the terms in the expectation into: $$ E\left(t_jW_{t_i}(W_{t_{i+1}}-W_{t_i})\right) = E\left(t_jW_{t_i}\right)E\left(W_{t_{i+1}}-W_{t_i}\right) $$ I am not sure how to find $E\left(t_jW_{t_i}\right)$, although I know it is finite so it doesn't matter because $E\left(W_{t_{i+1}}-W_{t_i}\right) = 0$",,"['probability', 'stochastic-processes', 'stochastic-calculus', 'stochastic-integrals']"
27,What are the elements of a filtration generated by a Wiener process?,What are the elements of a filtration generated by a Wiener process?,,"I understand the concept of filtration intuitively, and I can wrtite down the elements of a filtration for example in the case of a coin toss game, but what are the sets in the filtration of a Wiener process at a given time? How do they look like?","I understand the concept of filtration intuitively, and I can wrtite down the elements of a filtration for example in the case of a coin toss game, but what are the sets in the filtration of a Wiener process at a given time? How do they look like?",,"['probability', 'probability-theory', 'stochastic-processes']"
28,"Convex combination of independent random variables, that minimizes the $p$th moment","Convex combination of independent random variables, that minimizes the th moment",p,"Suppose  $V$ and $W$ are independent. Let \begin{align} f(x)=E[  ((1-x)V+xW )^p] \end{align} for $x \in [0,1]$ and some even $p \ge 2$.   Find  \begin{align} \min_{0 \le x \le 1} f(x) \end{align} Also, suppose that $E[W^{2n+1}]=0$ for $2n+1 <p$, that is all odd   moments of $W$ are zero. (I think this should simplify this a little   bit) Solution, to some of the cases can be done by expanding and taking the derivative. Case of $p=2$ \begin{align} E[ ((1-x)V+xW )^2]= (1-x)^2 E[V^2]+x^2 E[W^2] \end{align}  it is not difficult to check that the minimizing $x= \frac{E[V^2]}{E[V^2]+E[W^2]}$  and the minimum value is given by \begin{align} \min_{x\in [0,1]} f(x) =\frac{E[V^2]E[W^2]}{E[V^2]+E[W^2]} \end{align} Case of $p=4$ \begin{align} E[ ((1-x)V+xW )^2]= (1-x)^4 E[V^4]+6x^2(1-x)^2E[V^2]E[W^2]+x^4 E[W^4] \end{align} and one must solve \begin{align} -4(1-x)^3 E[V^4]+6E[V^2]E[W^2] (2x-5x+x^3)+4x^3E[W^4]=0 \end{align} which I guess can be done. But this procedure becomes increasingly complicated. Are there any other techniques on how to approach this problem? I also feel that this should have come up in some applications? Here is also an example when the problem can be solve in general The problem can be solved if $V \sim \mathcal{N}(0,\sigma_v^2)$ and $V \sim \mathcal{N}(0,\sigma_w^2)$. Since, \begin{align} E[  ((1-x)V+xW )^p] =E \left[(  \sqrt{(1-x)^2\sigma^2_V+x^2\sigma^2_W} Z )^p \right] = ((1-x)^2\sigma^2_V+x^2\sigma^2_W)^{p/2} E[Z^p] \end{align}  where $Z$ is standard norma. It is not difficult to check that the expression is minimized by  $x=\frac{\sigma_V^2}{\sigma_V^2+\sigma_W^2}$ (same as for $p=2$) and we get that \begin{align} \min_{x \in [0,1]} f(x)= \frac{(\sigma_V^2\sigma_W^2)^{p/2}}{(\sigma_V^2+\sigma_W^2)^{p/2}} E[Z^p] \end{align} Upper Bound We can derive the following upper bound on the problem by using value inspired optimal value for $p=2$ and triangle (Minkowski) inequality. We use $x=\frac{E^{2/p}[V^p]}{(E^{2/p}[V^p]+E^{2/p}[W^p])}$ \begin{align}  \min_{x\in [0,1]} E[ ((1-x)*V+x W )^p ] &\le  \frac{ E \left[ (E^{2/p}[W^p] V +E^{2/p}[V^p] W )^p \right])}{ (E^{2/p}[W^p]+E^{2/p}[V^p])^p}  \\ &\le   \frac{ \left(E^{2/p}[W^p] E^{1/p}[V^p] +E^{2/p}[V^p] E^{1/p}[W^p] \right)^p }{ (E^{2/p}[W^p]+E^{2/p}[V^p])^p}\\ &= \frac{E[W^p]E[V^p] ( E^{1/p}[W^p]+E^{1/p}[W^p])^p}{(E^{2/p}[W^p]+E^{2/p} [V^p])^p}\\ & \le 2^p  \frac{E[W^p]E[V^p]}{(E^{2/p}[W^p]+E^{2/p} [V^p])^{p/2}} \end{align}","Suppose  $V$ and $W$ are independent. Let \begin{align} f(x)=E[  ((1-x)V+xW )^p] \end{align} for $x \in [0,1]$ and some even $p \ge 2$.   Find  \begin{align} \min_{0 \le x \le 1} f(x) \end{align} Also, suppose that $E[W^{2n+1}]=0$ for $2n+1 <p$, that is all odd   moments of $W$ are zero. (I think this should simplify this a little   bit) Solution, to some of the cases can be done by expanding and taking the derivative. Case of $p=2$ \begin{align} E[ ((1-x)V+xW )^2]= (1-x)^2 E[V^2]+x^2 E[W^2] \end{align}  it is not difficult to check that the minimizing $x= \frac{E[V^2]}{E[V^2]+E[W^2]}$  and the minimum value is given by \begin{align} \min_{x\in [0,1]} f(x) =\frac{E[V^2]E[W^2]}{E[V^2]+E[W^2]} \end{align} Case of $p=4$ \begin{align} E[ ((1-x)V+xW )^2]= (1-x)^4 E[V^4]+6x^2(1-x)^2E[V^2]E[W^2]+x^4 E[W^4] \end{align} and one must solve \begin{align} -4(1-x)^3 E[V^4]+6E[V^2]E[W^2] (2x-5x+x^3)+4x^3E[W^4]=0 \end{align} which I guess can be done. But this procedure becomes increasingly complicated. Are there any other techniques on how to approach this problem? I also feel that this should have come up in some applications? Here is also an example when the problem can be solve in general The problem can be solved if $V \sim \mathcal{N}(0,\sigma_v^2)$ and $V \sim \mathcal{N}(0,\sigma_w^2)$. Since, \begin{align} E[  ((1-x)V+xW )^p] =E \left[(  \sqrt{(1-x)^2\sigma^2_V+x^2\sigma^2_W} Z )^p \right] = ((1-x)^2\sigma^2_V+x^2\sigma^2_W)^{p/2} E[Z^p] \end{align}  where $Z$ is standard norma. It is not difficult to check that the expression is minimized by  $x=\frac{\sigma_V^2}{\sigma_V^2+\sigma_W^2}$ (same as for $p=2$) and we get that \begin{align} \min_{x \in [0,1]} f(x)= \frac{(\sigma_V^2\sigma_W^2)^{p/2}}{(\sigma_V^2+\sigma_W^2)^{p/2}} E[Z^p] \end{align} Upper Bound We can derive the following upper bound on the problem by using value inspired optimal value for $p=2$ and triangle (Minkowski) inequality. We use $x=\frac{E^{2/p}[V^p]}{(E^{2/p}[V^p]+E^{2/p}[W^p])}$ \begin{align}  \min_{x\in [0,1]} E[ ((1-x)*V+x W )^p ] &\le  \frac{ E \left[ (E^{2/p}[W^p] V +E^{2/p}[V^p] W )^p \right])}{ (E^{2/p}[W^p]+E^{2/p}[V^p])^p}  \\ &\le   \frac{ \left(E^{2/p}[W^p] E^{1/p}[V^p] +E^{2/p}[V^p] E^{1/p}[W^p] \right)^p }{ (E^{2/p}[W^p]+E^{2/p}[V^p])^p}\\ &= \frac{E[W^p]E[V^p] ( E^{1/p}[W^p]+E^{1/p}[W^p])^p}{(E^{2/p}[W^p]+E^{2/p} [V^p])^p}\\ & \le 2^p  \frac{E[W^p]E[V^p]}{(E^{2/p}[W^p]+E^{2/p} [V^p])^{p/2}} \end{align}",,"['probability', 'probability-theory', 'optimization', 'expectation']"
29,Network contagion,Network contagion,,"Suppose I have an undirected, unweighted network in which some node, $i$, is infected.  This (and any) infected node has probability $p$ of infecting adjacent nodes. Is there a closed form expression of the probability that an arbitrary node $j$ becomes infected?  Any references would be much appreciated! My thought is that this should be somehow related to the resistance distance between the nodes. EDIT: So after some simulations there doesn't appear to be much relationship with resistance distance.  I think this is because resistance can be understood as a situation where `voltage' doesn't change as additional paths are added.  Here however, adding more connections to the infected source node (in some sense) increases the outward flow of contagion (a voltage analog) EDIT 2:  Let me be more clear about the process I have in mind.  Take a connected undirected graph,$G$, as given.  Some node, $i$, starts out as infected.  In period 1, each node adjacent to $i$ has probability, $p$, of becoming infected.  In subsequent periods, for each newly infected node, each of its uninfected neighbors has probability $p$ of becoming newly infected for the next period.  Can we characterize the probability that an arbitrary node, $j$, becomes infected after infinitely many periods?","Suppose I have an undirected, unweighted network in which some node, $i$, is infected.  This (and any) infected node has probability $p$ of infecting adjacent nodes. Is there a closed form expression of the probability that an arbitrary node $j$ becomes infected?  Any references would be much appreciated! My thought is that this should be somehow related to the resistance distance between the nodes. EDIT: So after some simulations there doesn't appear to be much relationship with resistance distance.  I think this is because resistance can be understood as a situation where `voltage' doesn't change as additional paths are added.  Here however, adding more connections to the infected source node (in some sense) increases the outward flow of contagion (a voltage analog) EDIT 2:  Let me be more clear about the process I have in mind.  Take a connected undirected graph,$G$, as given.  Some node, $i$, starts out as infected.  In period 1, each node adjacent to $i$ has probability, $p$, of becoming infected.  In subsequent periods, for each newly infected node, each of its uninfected neighbors has probability $p$ of becoming newly infected for the next period.  Can we characterize the probability that an arbitrary node, $j$, becomes infected after infinitely many periods?",,"['probability', 'graph-theory', 'network']"
30,Expected value of the sample covariance,Expected value of the sample covariance,,"Let $X = (X_1,\dots,X_p)$ is a random (column) vector with values in $\mathbb R^p$. The covariance matrix $\mathrm{Cov}(X,X)$ is defined by $$\mathrm{Cov}(X) := E[(X-E[X])(X-E[X])^T]$$ By definition the $(i,j)$-component of the covariance matrix is the covariance $\mathrm{Cov}(X_i,X_j)$ of two random variables. If we have $n$ samples $x_1,\dots,x_n \in \mathbb R^p$, then it is known that the covariance matrix is estimated by the following matrix. $$Q = \frac{1}{n-1}\sum_k (x_k-\bar x)(x_k-\bar x)^T$$ Here $\bar x = n^{-1}\sum_k x_k$ is the vector of sample means of random variables $X_1,\dots,X_p$. I would like to know how to see that $Q$ estimates the covariance matrix. The $(i,j)$-component of $Q$ is given by $$\frac{1}{n-1}\sum_k (x_{ki}-\frac{1}{n}\sum_l x_{li})(x_{kj}-\frac{1}{n}\sum_l x_{lj})$$ Here $x_k = (x_{k1},\dots,x_{kp})^T$. To make notation easier, we put $Y=X_i$ and $Z=X_j$. Then I want to prove the following formula $$\mathrm{Cov}(Y,Z) = E\left[\frac{1}{n-1}\sum_{k=1}^n(Y_k-\bar Y)(Z_k - \bar Z)\right]$$ Here $Y_1,\dots,Y_n,Y$ are iid and $Z_1,\dots,Z_n,Z$ are iid. Actually I am very confused by my formulation and I am missing where to start. I understand the case where $Y=Z$, i.e. the sample variance. The biggest problem would be that the relation between $\mathrm{Cov}(Y,Z)$ and $\sigma_{ij} := \mathrm{Cov}(Y_i,Z_j)$ is unclear.","Let $X = (X_1,\dots,X_p)$ is a random (column) vector with values in $\mathbb R^p$. The covariance matrix $\mathrm{Cov}(X,X)$ is defined by $$\mathrm{Cov}(X) := E[(X-E[X])(X-E[X])^T]$$ By definition the $(i,j)$-component of the covariance matrix is the covariance $\mathrm{Cov}(X_i,X_j)$ of two random variables. If we have $n$ samples $x_1,\dots,x_n \in \mathbb R^p$, then it is known that the covariance matrix is estimated by the following matrix. $$Q = \frac{1}{n-1}\sum_k (x_k-\bar x)(x_k-\bar x)^T$$ Here $\bar x = n^{-1}\sum_k x_k$ is the vector of sample means of random variables $X_1,\dots,X_p$. I would like to know how to see that $Q$ estimates the covariance matrix. The $(i,j)$-component of $Q$ is given by $$\frac{1}{n-1}\sum_k (x_{ki}-\frac{1}{n}\sum_l x_{li})(x_{kj}-\frac{1}{n}\sum_l x_{lj})$$ Here $x_k = (x_{k1},\dots,x_{kp})^T$. To make notation easier, we put $Y=X_i$ and $Z=X_j$. Then I want to prove the following formula $$\mathrm{Cov}(Y,Z) = E\left[\frac{1}{n-1}\sum_{k=1}^n(Y_k-\bar Y)(Z_k - \bar Z)\right]$$ Here $Y_1,\dots,Y_n,Y$ are iid and $Z_1,\dots,Z_n,Z$ are iid. Actually I am very confused by my formulation and I am missing where to start. I understand the case where $Y=Z$, i.e. the sample variance. The biggest problem would be that the relation between $\mathrm{Cov}(Y,Z)$ and $\sigma_{ij} := \mathrm{Cov}(Y_i,Z_j)$ is unclear.",,"['probability', 'statistics', 'covariance']"
31,Differential entropy of the product of Gaussian random variables,Differential entropy of the product of Gaussian random variables,,"Given two independent Gaussian random variables $X \sim \mathcal{N}(\mu_x,\sigma_x^2)$ and $Y \sim \mathcal{N}(\mu_y,\sigma_y^2)$. We look at the product distribution of these two random variables $Z=XY$. My question is, what is the differential entropy $h(Z)$? The differential entropy is defined as $h(Z)=\int_z f(z) \log (f(z)) dz$, where $f(z)$ is the probability density function of $Z$. The probability density function can be computed to be $f(z)=\frac{1}{\pi\sigma_x\sigma_y} K_0(\frac{|z|}{\sigma_x\sigma_y})$, where $K_0$ is the modified Bessel function second order. (see: Wikipedia: product distribtuion , Wolfram: Normal product distribution . ) However, I couldn't find any information on the entropy of the modified bessel function. There seems to be a special case for $K_{\frac{1}{2}}$, but I could find a closed form solution for $K_0$. Does anybody know how to compute $h(Z)$? A good approximation of $K_0$ such that $h(Z)$ can be lower bounded, would be sufficient, in case an exact solution is known to be intractable. It seems that even the most basic lower bounds fail to yield some insights, for example: $h(XY)\geq h(XY|Y) = \int f(y)H(Xy|Y=y) dy = \int f(y) (\log|y|+ h(X))dy=h(X)+\int f(y) \log |y|dy$,  where the relation to $h(X)$ is shadowed by the last term. Thanks in advance, Rick","Given two independent Gaussian random variables $X \sim \mathcal{N}(\mu_x,\sigma_x^2)$ and $Y \sim \mathcal{N}(\mu_y,\sigma_y^2)$. We look at the product distribution of these two random variables $Z=XY$. My question is, what is the differential entropy $h(Z)$? The differential entropy is defined as $h(Z)=\int_z f(z) \log (f(z)) dz$, where $f(z)$ is the probability density function of $Z$. The probability density function can be computed to be $f(z)=\frac{1}{\pi\sigma_x\sigma_y} K_0(\frac{|z|}{\sigma_x\sigma_y})$, where $K_0$ is the modified Bessel function second order. (see: Wikipedia: product distribtuion , Wolfram: Normal product distribution . ) However, I couldn't find any information on the entropy of the modified bessel function. There seems to be a special case for $K_{\frac{1}{2}}$, but I could find a closed form solution for $K_0$. Does anybody know how to compute $h(Z)$? A good approximation of $K_0$ such that $h(Z)$ can be lower bounded, would be sufficient, in case an exact solution is known to be intractable. It seems that even the most basic lower bounds fail to yield some insights, for example: $h(XY)\geq h(XY|Y) = \int f(y)H(Xy|Y=y) dy = \int f(y) (\log|y|+ h(X))dy=h(X)+\int f(y) \log |y|dy$,  where the relation to $h(X)$ is shadowed by the last term. Thanks in advance, Rick",,"['probability', 'information-theory', 'entropy']"
32,Determining number of randomly picked people,Determining number of randomly picked people,,"Firstly I want to put big disclaimer here. This particular problem is a smaller part of my homework. Since even after discussion with my fellow classmates we are not sure how to handle it we decided to post a question here. Basically we are supposed to simulate spreading of disease. Every day every infected person will pick random number of people (possible infected candidates) with Poisson distribution where parameter is 5. Everyone of those selected people (=possible infected candidates) will be infected with probability of 1/2 and at the end of the day the infection takes effect. Next day this particular infected person will be also spreading infection and will again pick random number of people (another possible infected candidates) with Poisson distribution where parameter is again 5. Number of possibly infected candidates and event static whether person will be infected or not are both independent on each other. Let's suppose on the first day there is only one infected person. We should simulate disease spreading for one week. And how many infected persons there will be at the end of 8th day. Now finally to my question. Confusing part for me is how are we suppose to determine the number of randomly picked people which represent possible infected candidates since we don't know how many people is there? I assume that with higher number of people the disease should spread faster? I also think that this number of randomly picked possible candidates should be somehow determined by given parameter and also by Poisson distribution itself but I am struggling how to make these things ""work"" together.","Firstly I want to put big disclaimer here. This particular problem is a smaller part of my homework. Since even after discussion with my fellow classmates we are not sure how to handle it we decided to post a question here. Basically we are supposed to simulate spreading of disease. Every day every infected person will pick random number of people (possible infected candidates) with Poisson distribution where parameter is 5. Everyone of those selected people (=possible infected candidates) will be infected with probability of 1/2 and at the end of the day the infection takes effect. Next day this particular infected person will be also spreading infection and will again pick random number of people (another possible infected candidates) with Poisson distribution where parameter is again 5. Number of possibly infected candidates and event static whether person will be infected or not are both independent on each other. Let's suppose on the first day there is only one infected person. We should simulate disease spreading for one week. And how many infected persons there will be at the end of 8th day. Now finally to my question. Confusing part for me is how are we suppose to determine the number of randomly picked people which represent possible infected candidates since we don't know how many people is there? I assume that with higher number of people the disease should spread faster? I also think that this number of randomly picked possible candidates should be somehow determined by given parameter and also by Poisson distribution itself but I am struggling how to make these things ""work"" together.",,"['probability', 'statistics', 'simulation']"
33,"Boyd & Vandenberghe, example 2.13 — what is the ""convex set of joint probabilities"" in this example?","Boyd & Vandenberghe, example 2.13 — what is the ""convex set of joint probabilities"" in this example?",,"I am reading Boyd & Vandenberghe's Convex Optimization . I can't understand the example below: what is the ""convex set of joint probabilities"" in this example? what is the convex set $C$ here?","I am reading Boyd & Vandenberghe's Convex Optimization . I can't understand the example below: what is the ""convex set of joint probabilities"" in this example? what is the convex set here?",C,"['probability', 'convex-analysis', 'convex-optimization']"
34,What's the probability that Erica has one boy and one girl?,What's the probability that Erica has one boy and one girl?,,"During a flight on an airplane, Eric strikes up a chat with Erica, the person sitting next to him. It turns out that Erica has two kids, and at least one of them is a girl born on a Tuesday. Being a mathematician, Eric decides to find the probability that Erica has a boy and a girl before asking her. What is the probability that Erica has one boy, and one girl? Assume an equal chance of giving birth to either gender and an equal chance to giving birth on any day. I was looking at the solution to this question and for some reason they were looking at the number of pairs of $bg$ in a two week time period. Why not a week period?","During a flight on an airplane, Eric strikes up a chat with Erica, the person sitting next to him. It turns out that Erica has two kids, and at least one of them is a girl born on a Tuesday. Being a mathematician, Eric decides to find the probability that Erica has a boy and a girl before asking her. What is the probability that Erica has one boy, and one girl? Assume an equal chance of giving birth to either gender and an equal chance to giving birth on any day. I was looking at the solution to this question and for some reason they were looking at the number of pairs of $bg$ in a two week time period. Why not a week period?",,['probability']
35,Moment generation function -> characteristic function uniqueness,Moment generation function -> characteristic function uniqueness,,"Here's my proof that moment generation function (if exists) uniquely determines characteristic function. Can you please see how to make it more rigorous or improve in either way (e.g. by citing relevant well-known theorems). Thank you very much - any suggestions are welcome! Theorem. Let $F(X)$ be a probability distribution  of $X$ and suppose the MGF, $M(t)$ exists in an open ball centered at $t = 0$. Then, the characteristic function is uniquely determined by $M(t)$ alone. In other words, if $M(t)=\sum_{x=0}^{\infty} \frac{t^n \mu_n}{n!}$ has a positive radius of convergence about $0$, there is a unique characteristic function associated with it. \newline Proof: The theorem can be easily proven by using analytical continuation technique. Let's extend MGF on a ball $R = \{x | |x| <r\} , r \in \mathbb{R}$ to a strip $S =\{z | |Re(z)| < r\}$ on complex plain. it is easy to see that $M(z)$ is bounded on $S$: $|M(z)| = | \int\limits_{\mathbb{R}} e^{zt} dF(t) | < \int\limits_{\mathbb{R}} |e^{zt}| dF(t) = M(x)$, so using Fubini's theorem we can see that $\int\limits_{\partial B} M(z) dz = \int\limits_{\partial B} \int\limits_{\mathbb{R}} e^{zt} dt dz = \int\limits_{\mathbb{R}}( \int\limits_{\partial B} e^{zt} dz ) dt = 0 $ on every open ball $B \subset S$, so $M(z)$ is analytical continuation on $S$. Because the uniqueness of analytical continuation, if $M_1(x) = M_2(x), x \in R$, then $C_1(x) = M_1(-ix) = M_2(-ix) = C_2(x)$. Q.E.D.","Here's my proof that moment generation function (if exists) uniquely determines characteristic function. Can you please see how to make it more rigorous or improve in either way (e.g. by citing relevant well-known theorems). Thank you very much - any suggestions are welcome! Theorem. Let $F(X)$ be a probability distribution  of $X$ and suppose the MGF, $M(t)$ exists in an open ball centered at $t = 0$. Then, the characteristic function is uniquely determined by $M(t)$ alone. In other words, if $M(t)=\sum_{x=0}^{\infty} \frac{t^n \mu_n}{n!}$ has a positive radius of convergence about $0$, there is a unique characteristic function associated with it. \newline Proof: The theorem can be easily proven by using analytical continuation technique. Let's extend MGF on a ball $R = \{x | |x| <r\} , r \in \mathbb{R}$ to a strip $S =\{z | |Re(z)| < r\}$ on complex plain. it is easy to see that $M(z)$ is bounded on $S$: $|M(z)| = | \int\limits_{\mathbb{R}} e^{zt} dF(t) | < \int\limits_{\mathbb{R}} |e^{zt}| dF(t) = M(x)$, so using Fubini's theorem we can see that $\int\limits_{\partial B} M(z) dz = \int\limits_{\partial B} \int\limits_{\mathbb{R}} e^{zt} dt dz = \int\limits_{\mathbb{R}}( \int\limits_{\partial B} e^{zt} dz ) dt = 0 $ on every open ball $B \subset S$, so $M(z)$ is analytical continuation on $S$. Because the uniqueness of analytical continuation, if $M_1(x) = M_2(x), x \in R$, then $C_1(x) = M_1(-ix) = M_2(-ix) = C_2(x)$. Q.E.D.",,"['probability', 'complex-analysis', 'characteristic-functions', 'moment-generating-functions']"
36,Symmetric proof for the probability of real roots of a quadratic with exponentially distributed parameters,Symmetric proof for the probability of real roots of a quadratic with exponentially distributed parameters,,"What is the probability that the polynomial has real roots? asked for the probability that the quadratic polynomial $ax^2+bx+c$ has real roots if the parameters $a,b,c$ are exponentially distributed with common parameter. (The parameter was specified as $1$ but doesn't matter.) This didn't seem to me like a natural question to ask, since the exponential distribution has nice properties with respect to addition and not so much with respect to multiplication, and the discriminant condition $b^2\ge4ac$ is multiplicative; so I was a bit surprised by the simple answer $\frac13$. This seems to cry out for a symmetric proof, e. g. using a transformation to three equivalent variables one of which must equiprobably be the greatest to fulfill the condition. However, I couldn't come up with such a transformation (I tried $\sqrt{ab}$, $\sqrt{bc}$, $\sqrt{ca}$ and $a-b$, $b-c$, $c-a$) and could only obtain the probability by performing an asymmetric and somewhat involved triple integration. Can you provide a more elegant proof of this result?","What is the probability that the polynomial has real roots? asked for the probability that the quadratic polynomial $ax^2+bx+c$ has real roots if the parameters $a,b,c$ are exponentially distributed with common parameter. (The parameter was specified as $1$ but doesn't matter.) This didn't seem to me like a natural question to ask, since the exponential distribution has nice properties with respect to addition and not so much with respect to multiplication, and the discriminant condition $b^2\ge4ac$ is multiplicative; so I was a bit surprised by the simple answer $\frac13$. This seems to cry out for a symmetric proof, e. g. using a transformation to three equivalent variables one of which must equiprobably be the greatest to fulfill the condition. However, I couldn't come up with such a transformation (I tried $\sqrt{ab}$, $\sqrt{bc}$, $\sqrt{ca}$ and $a-b$, $b-c$, $c-a$) and could only obtain the probability by performing an asymmetric and somewhat involved triple integration. Can you provide a more elegant proof of this result?",,"['probability', 'probability-distributions', 'roots', 'quadratics', 'alternative-proof']"
37,What is the area covered by a Random walk in a 2D grid?,What is the area covered by a Random walk in a 2D grid?,,"I am a biologist and applying for a job, for which I need to solve this question. It is an open book test, where the internet and any other resources are fair game. Here's the question - I'm stuck on how to approach it and would appreciate pointers. My intuition is posted underneath. Background Your neighbor is a farmer with two cows, Clarabelle and Bernadette. Each cow has its own square pen that is 11m on a side (see first figure). The farmer is heading out of town for a trip and plans to leave the cows in their respective pens, which are completely filled with grass to start. The cows begin in the center of the pen, and will slowly move around the pen eating the grass. They move around the pen very slowly, always pausing to eat or rest after every step. If you divide the pen into 1m squares, the cows can move one square in any direction each step (like a king on a chess board), as shown in the second figure. Fig 1/2 After each move, the cow will spend 20 minutes in the new square eating grass, if it is available. Once the grass in a square is eaten, it is gone forever. If the cow moves to a square whose grass was already eaten, then the cow will spend 20 minutes resting in that square. After 20 minutes, whether resting or eating, the cow moves to another square. If a cow is in a square adjacent to the fence, she will never try to move in the direction of the fence. The cows never stay in the same square twice in a row -- they always move to a different one after resting or eating. The first figure shows an example of what a pen might look like after some hours, with the brown spots indicating squares that have been grazed. The first cow, Clarabelle, has no preference for direction when she moves. She is equally likely to move in any direction at all times. Let p be the probability that she moves in a direction, as shown in the first figure below. The second cow, Bernadette, prefers to move towards squares with grass. She is twice as likely to move towards a space that has grass as she is towards a space that she has already eaten, as shown in the second figure below. Fig 3/4 Questions If the farmer returns after 48 hours, what percentage of the grass in her pen do you expect Clarabelle to have eaten? How long do you expect it will take for Bernadette to eat 50% of the grass in her pen? Suppose that if either of the cows go 24 hours without eating any grass, she will die. Which cow is expected to survive longer? My intuition This appears to be modeling a random walk through a 2 dimensional grid. I can for instance figure out the probability of being at a particular node in the grid, after a given time. But I'm not sure how to think about the area covered by the cow as it walks through. Would appreciate any insights.","I am a biologist and applying for a job, for which I need to solve this question. It is an open book test, where the internet and any other resources are fair game. Here's the question - I'm stuck on how to approach it and would appreciate pointers. My intuition is posted underneath. Background Your neighbor is a farmer with two cows, Clarabelle and Bernadette. Each cow has its own square pen that is 11m on a side (see first figure). The farmer is heading out of town for a trip and plans to leave the cows in their respective pens, which are completely filled with grass to start. The cows begin in the center of the pen, and will slowly move around the pen eating the grass. They move around the pen very slowly, always pausing to eat or rest after every step. If you divide the pen into 1m squares, the cows can move one square in any direction each step (like a king on a chess board), as shown in the second figure. Fig 1/2 After each move, the cow will spend 20 minutes in the new square eating grass, if it is available. Once the grass in a square is eaten, it is gone forever. If the cow moves to a square whose grass was already eaten, then the cow will spend 20 minutes resting in that square. After 20 minutes, whether resting or eating, the cow moves to another square. If a cow is in a square adjacent to the fence, she will never try to move in the direction of the fence. The cows never stay in the same square twice in a row -- they always move to a different one after resting or eating. The first figure shows an example of what a pen might look like after some hours, with the brown spots indicating squares that have been grazed. The first cow, Clarabelle, has no preference for direction when she moves. She is equally likely to move in any direction at all times. Let p be the probability that she moves in a direction, as shown in the first figure below. The second cow, Bernadette, prefers to move towards squares with grass. She is twice as likely to move towards a space that has grass as she is towards a space that she has already eaten, as shown in the second figure below. Fig 3/4 Questions If the farmer returns after 48 hours, what percentage of the grass in her pen do you expect Clarabelle to have eaten? How long do you expect it will take for Bernadette to eat 50% of the grass in her pen? Suppose that if either of the cows go 24 hours without eating any grass, she will die. Which cow is expected to survive longer? My intuition This appears to be modeling a random walk through a 2 dimensional grid. I can for instance figure out the probability of being at a particular node in the grid, after a given time. But I'm not sure how to think about the area covered by the cow as it walks through. Would appreciate any insights.",,"['probability', 'algorithms', 'random-walk']"
38,Tail field versus germ field of Brownian motion,Tail field versus germ field of Brownian motion,,"Continuing my foray into Brownian motion (apologies for the bombardment...), I'm trying to verify the details of a proof of Durrett of the following 0-1 property of the tail $\sigma$-algebra of Brownian motion, $\mathcal{T}$: If $A \in \mathcal{T}$, then for all $x$ either $P_x(A) = 0$ or   $P_x(A) = 1$. So to set the stage, let $\mathcal{F}_t' = \sigma(B_s: s \geq t)$ so that $\mathcal{T} = \cap_{t \geq 0} \mathcal{F}_t'$. Also let $\mathcal{F}_0^+$ be $\cap_{t>0} \mathcal{F}_t^0$ where $\mathcal{F}_t^0 = \sigma(B_s: s \leq t)$. Finally set $X_t = t B(1/t)$. The proof of the above fact starts with the claim that $\mathcal{T}$ (for $B_s$) is the same as $\mathcal{F}_0^+$ for the time-inverted process $X_s$ . Now it seems pretty clear to me that $$ \sigma(B_s: s \geq t) = \sigma(X_s: s \leq t) $$ But when I take intersections to get the tail $\sigma$-field, shouldn't the RHS be $\cap_{t \geq 0} \mathcal{F}_t^0$ instead of $\cap_{t > 0} \mathcal{F}_t^0$, which would reduce to the ordinary $\sigma$-field $\mathcal{F}_0^0$? I know that $\mathcal{F}_0^+$ is essentially the same as $\mathcal{F}_0^0$ by Blumenthal's 0-1 law, but I'm still having trouble seeing exactly if and how they differ, and exactly how much it matters. I get a sense that tail algebra events are somehow ""nicer"" than ones in the germ field because whether tail events happen or not is independent of the starting point $x$. I'm just trying to piece together how this relates to the ordinary Brownian motion $B_s$ and the time-inverted one, $X_s$.","Continuing my foray into Brownian motion (apologies for the bombardment...), I'm trying to verify the details of a proof of Durrett of the following 0-1 property of the tail $\sigma$-algebra of Brownian motion, $\mathcal{T}$: If $A \in \mathcal{T}$, then for all $x$ either $P_x(A) = 0$ or   $P_x(A) = 1$. So to set the stage, let $\mathcal{F}_t' = \sigma(B_s: s \geq t)$ so that $\mathcal{T} = \cap_{t \geq 0} \mathcal{F}_t'$. Also let $\mathcal{F}_0^+$ be $\cap_{t>0} \mathcal{F}_t^0$ where $\mathcal{F}_t^0 = \sigma(B_s: s \leq t)$. Finally set $X_t = t B(1/t)$. The proof of the above fact starts with the claim that $\mathcal{T}$ (for $B_s$) is the same as $\mathcal{F}_0^+$ for the time-inverted process $X_s$ . Now it seems pretty clear to me that $$ \sigma(B_s: s \geq t) = \sigma(X_s: s \leq t) $$ But when I take intersections to get the tail $\sigma$-field, shouldn't the RHS be $\cap_{t \geq 0} \mathcal{F}_t^0$ instead of $\cap_{t > 0} \mathcal{F}_t^0$, which would reduce to the ordinary $\sigma$-field $\mathcal{F}_0^0$? I know that $\mathcal{F}_0^+$ is essentially the same as $\mathcal{F}_0^0$ by Blumenthal's 0-1 law, but I'm still having trouble seeing exactly if and how they differ, and exactly how much it matters. I get a sense that tail algebra events are somehow ""nicer"" than ones in the germ field because whether tail events happen or not is independent of the starting point $x$. I'm just trying to piece together how this relates to the ordinary Brownian motion $B_s$ and the time-inverted one, $X_s$.",,"['probability', 'stochastic-processes', 'brownian-motion']"
39,Median of waiting time for $k$-th ace from bridge cards,Median of waiting time for -th ace from bridge cards,k,"I can't figure out how to get median of a waiting time from the exercise 36 from W. Feller's book An Introduction to Probability Theory and Its Applications Vol.1 (bold in the quote): Distribution of aces among $r$ bridge cards . Calculate the probabilities $p_0(r), p_1(r), \dotso, p_4(r)$ that among $r$ bridge cards drawn at random there are $0, 1, \dotso, 4$ aces, respectively. Verify that $p_0(r) = p_4(52-r)$. Continuation: waiting times . If the cards are drawn one by one, find the probabilities $f_1(r), f_2(r), \dotso, f_4(r)$ that the first, ..., fourth ace turns up at the $r$th trial. Guess at the medians of the waiting times for the first, ..., fourth ace and then calculate them . $p_k(r)$ and $f_k(r)$ were easy (I'm not sure about $f_k(r)$ though): $$ p_k(r) 	= \frac{\binom{4}{k} \binom{48}{r-k}}{\binom{52}{r}} 	= \frac{\binom{4}{k} (r)_k (52-r)_{4-k}}{(52)_4} $$ $$ f_k(r) 	= \frac{\binom{4}{k} \binom{r-1}{k-1} (48)_{r-k}}{(52)_r} 	= \frac{\binom{4}{k} \binom{r-1}{k-1} (52-r)_{4-k}}{(52)_4} $$ In answers section, Mr. Feller introduces probabilities that the waiting times for the first,..., fourth ace exceed $r$ ($k$ is for k-th ace): $$ w_k(r) = \sum_{i=0}^{k-1} p_i(r) $$ From this he arrives at $f_k(r)$: $$ f_k(r) = w_k(r-1) - w_k(r) $$ And then he gives computed medians (see spoiler below) without any explanation of how they were derived. $9$, $20$, $33$, $44$ If I'm not mistaken, the median is the solution of $$w_k(r) = 0.5$$ for $r$. This however leads to quite complicated equation with many factorials which I wasn't able solve even with Stirling approximation. How can I easily compute those medians? Graph of functions above:","I can't figure out how to get median of a waiting time from the exercise 36 from W. Feller's book An Introduction to Probability Theory and Its Applications Vol.1 (bold in the quote): Distribution of aces among $r$ bridge cards . Calculate the probabilities $p_0(r), p_1(r), \dotso, p_4(r)$ that among $r$ bridge cards drawn at random there are $0, 1, \dotso, 4$ aces, respectively. Verify that $p_0(r) = p_4(52-r)$. Continuation: waiting times . If the cards are drawn one by one, find the probabilities $f_1(r), f_2(r), \dotso, f_4(r)$ that the first, ..., fourth ace turns up at the $r$th trial. Guess at the medians of the waiting times for the first, ..., fourth ace and then calculate them . $p_k(r)$ and $f_k(r)$ were easy (I'm not sure about $f_k(r)$ though): $$ p_k(r) 	= \frac{\binom{4}{k} \binom{48}{r-k}}{\binom{52}{r}} 	= \frac{\binom{4}{k} (r)_k (52-r)_{4-k}}{(52)_4} $$ $$ f_k(r) 	= \frac{\binom{4}{k} \binom{r-1}{k-1} (48)_{r-k}}{(52)_r} 	= \frac{\binom{4}{k} \binom{r-1}{k-1} (52-r)_{4-k}}{(52)_4} $$ In answers section, Mr. Feller introduces probabilities that the waiting times for the first,..., fourth ace exceed $r$ ($k$ is for k-th ace): $$ w_k(r) = \sum_{i=0}^{k-1} p_i(r) $$ From this he arrives at $f_k(r)$: $$ f_k(r) = w_k(r-1) - w_k(r) $$ And then he gives computed medians (see spoiler below) without any explanation of how they were derived. $9$, $20$, $33$, $44$ If I'm not mistaken, the median is the solution of $$w_k(r) = 0.5$$ for $r$. This however leads to quite complicated equation with many factorials which I wasn't able solve even with Stirling approximation. How can I easily compute those medians? Graph of functions above:",,"['probability', 'combinatorics', 'median']"
40,Difference of Ordered Uniform Random Variables,Difference of Ordered Uniform Random Variables,,"Let $X_1, X_2,..., X_n$ be $n$ random variables distributed uniform(0,1) and $X_{(1)},X_{(2)},..., X_{(n)}$ be the ordered statistics of $X_1,...,X_n$ such that: $X_{(1)} < X_{(2)} < ... < X_{(n)}$ $X_{(1)} = min(X_1,...,X_n)$ $X_{(n)} = max(X_1,...,X_n)$ I know that these variables are distributed: $X_{(i)} \sim Beta(i, n+1-i)$ I am looking to find the distribution of the difference of consecutive ordered statistics: $Y_{i+1,i} = X_{(i+1)} - X_{(i)}$ in order to calculate the total probability: $p = P(Y_{2,1} < d_{2,1} \cap Y_{3,2} < d_{3,2} \cap ... \cap Y_{n,n-1} < d_{n,n-1})$ Where $d_{i+1,i}$ are some given distances This proof, Difference of order statistics in a sample of uniform random variables , suggests that the distribution of $Y_{i+1,i}$ is $Y_{i+1,i} \sim Beta(1,n)$ This suggests that the events in the probability, $p$, above are independent... is this true?","Let $X_1, X_2,..., X_n$ be $n$ random variables distributed uniform(0,1) and $X_{(1)},X_{(2)},..., X_{(n)}$ be the ordered statistics of $X_1,...,X_n$ such that: $X_{(1)} < X_{(2)} < ... < X_{(n)}$ $X_{(1)} = min(X_1,...,X_n)$ $X_{(n)} = max(X_1,...,X_n)$ I know that these variables are distributed: $X_{(i)} \sim Beta(i, n+1-i)$ I am looking to find the distribution of the difference of consecutive ordered statistics: $Y_{i+1,i} = X_{(i+1)} - X_{(i)}$ in order to calculate the total probability: $p = P(Y_{2,1} < d_{2,1} \cap Y_{3,2} < d_{3,2} \cap ... \cap Y_{n,n-1} < d_{n,n-1})$ Where $d_{i+1,i}$ are some given distances This proof, Difference of order statistics in a sample of uniform random variables , suggests that the distribution of $Y_{i+1,i}$ is $Y_{i+1,i} \sim Beta(1,n)$ This suggests that the events in the probability, $p$, above are independent... is this true?",,"['probability', 'statistics', 'probability-distributions', 'order-statistics', 'beta-function']"
41,Simplifying Chain of Conditional Variances given a Markov Chain,Simplifying Chain of Conditional Variances given a Markov Chain,,"$\newcommand{\Var}{\operatorname{Var}}$Suppose $X,Y,W$ form a Markov chain $X \to Y \to W$. Can we simplify the following expression? \begin{align*} E [ \Var ( \Var (X\mid Y) \mid W)] \end{align*} Because we have a Markov chain, that is $p(x\mid y)=p(x\mid y,w)$, we have that \begin{align*} E[X\mid Y]&=E[X\mid Y,W]\\ \Var(X\mid Y)&=\Var(X\mid Y,W)\\ \end{align*} If we were to replace $\Var$ with expected value we can use towering property of expected value that is \begin{align*} E \left[ E \left(E [X\mid Y] \mid W\right)\right]&=E \left[ E \left(E [X\mid Y,W] \mid W\right)\right]\\ &=E[E[X\mid W]]=E[X] \end{align*} Can we do something similar for $E \left[ \Var \left( \Var (X\mid Y) \mid W\right)\right]$? One thing we can show is  \begin{align*} E \left[ \Var \left( \Var (X\mid Y) \mid W\right)\right]=E \left[ \Var \left( \Var (X\mid Y,W) \mid W\right)\right] \end{align*} but I am not sure if this helps at all. Would be grateful for any ideas. Thank you very much","$\newcommand{\Var}{\operatorname{Var}}$Suppose $X,Y,W$ form a Markov chain $X \to Y \to W$. Can we simplify the following expression? \begin{align*} E [ \Var ( \Var (X\mid Y) \mid W)] \end{align*} Because we have a Markov chain, that is $p(x\mid y)=p(x\mid y,w)$, we have that \begin{align*} E[X\mid Y]&=E[X\mid Y,W]\\ \Var(X\mid Y)&=\Var(X\mid Y,W)\\ \end{align*} If we were to replace $\Var$ with expected value we can use towering property of expected value that is \begin{align*} E \left[ E \left(E [X\mid Y] \mid W\right)\right]&=E \left[ E \left(E [X\mid Y,W] \mid W\right)\right]\\ &=E[E[X\mid W]]=E[X] \end{align*} Can we do something similar for $E \left[ \Var \left( \Var (X\mid Y) \mid W\right)\right]$? One thing we can show is  \begin{align*} E \left[ \Var \left( \Var (X\mid Y) \mid W\right)\right]=E \left[ \Var \left( \Var (X\mid Y,W) \mid W\right)\right] \end{align*} but I am not sure if this helps at all. Would be grateful for any ideas. Thank you very much",,"['probability', 'markov-chains', 'conditional-expectation']"
42,Equivalent definitions of Poisson process,Equivalent definitions of Poisson process,,"Define a Poisson process with parameter $\lambda$ is a counting process $(N(t))_{t\ge 0}$ such that: (i) $N(0)=0$; (ii)  It has independent increment property; (iii) $N(t+h)-N(t)$ has Poisson distribution with parameter $\lambda h$. Now I want to prove this definition can lead to the other definition: Poisson process with parameter $\lambda$ is a renewal process in which the interarrival intervals has exponential distribution with parameter $\lambda$. My proof: For all $n$, the event $X_n>t$ is the same as $N(t+S_{n-1})-N(S_{n-1})=0$ where $S_{n-1}=X_1+X_2+\cdots+X_{n-1}$. By (iii): $$P(X_n>t)=P(N(t+S_{n-1})-N(S_{n-1})=0)=e^{-\lambda t}$$ Thus $X_n$ has exponential distribution. By (ii), $X_n$'s are also independent. Is my proof correct? If not, please help me with a correct one. Thank you very much.","Define a Poisson process with parameter $\lambda$ is a counting process $(N(t))_{t\ge 0}$ such that: (i) $N(0)=0$; (ii)  It has independent increment property; (iii) $N(t+h)-N(t)$ has Poisson distribution with parameter $\lambda h$. Now I want to prove this definition can lead to the other definition: Poisson process with parameter $\lambda$ is a renewal process in which the interarrival intervals has exponential distribution with parameter $\lambda$. My proof: For all $n$, the event $X_n>t$ is the same as $N(t+S_{n-1})-N(S_{n-1})=0$ where $S_{n-1}=X_1+X_2+\cdots+X_{n-1}$. By (iii): $$P(X_n>t)=P(N(t+S_{n-1})-N(S_{n-1})=0)=e^{-\lambda t}$$ Thus $X_n$ has exponential distribution. By (ii), $X_n$'s are also independent. Is my proof correct? If not, please help me with a correct one. Thank you very much.",,"['probability', 'stochastic-processes']"
43,Discrete Time Two sided Gaussian Random Walk : Hitting Time Distribution,Discrete Time Two sided Gaussian Random Walk : Hitting Time Distribution,,"I am looking at the hitting time of a two sided Gaussian random walk i.e. $S_{n}=\sum_{i=1}^{n}X_{i}$ where $X_{i}$ are i.i.d normally distributed random variables. The hitting time is $\tau=\inf\{n:S_{n}\notin [a,b]\}$ where a and b are constants. Most of the literature I have come across deals with expectations of $\tau$. I have an upper bound for $P(\tau > t)$. I am trying to find a lower bound or rather the exact distribution for the hitting time. Really appreciate any help you can provide. Thanks in advance!","I am looking at the hitting time of a two sided Gaussian random walk i.e. $S_{n}=\sum_{i=1}^{n}X_{i}$ where $X_{i}$ are i.i.d normally distributed random variables. The hitting time is $\tau=\inf\{n:S_{n}\notin [a,b]\}$ where a and b are constants. Most of the literature I have come across deals with expectations of $\tau$. I have an upper bound for $P(\tau > t)$. I am trying to find a lower bound or rather the exact distribution for the hitting time. Really appreciate any help you can provide. Thanks in advance!",,"['probability', 'random-variables', 'random-walk']"
44,Finding the mode of the negative binomial distribution,Finding the mode of the negative binomial distribution,,"The negative binomial distribution is as follows: $\displaystyle f_X(k)=\binom{k-1}{n-1}p^n(1-p)^{k-n}.$ To find its mode, we want to find the $k$ with the highest probability. So we want to find $P(X=k-1)\leq P(X=k) \geq P(X=k+1).$ I'm getting stuck working with the following: If $P(X=k-1)\leq P(X=k)$ then $$1 \leq \frac{P(X=k)}{P(X=k-1)}=\frac{\binom{k-1}{n-1}p^n(1-p)^{k-n}}{\binom{k-2}{n-1}p^{n}(1-p)^{k-n-1}}.$$ First of all, I'm wondering if I'm on the right track.  Also, I'm having problems simplifying the binomial terms.","The negative binomial distribution is as follows: To find its mode, we want to find the with the highest probability. So we want to find I'm getting stuck working with the following: If then First of all, I'm wondering if I'm on the right track.  Also, I'm having problems simplifying the binomial terms.",\displaystyle f_X(k)=\binom{k-1}{n-1}p^n(1-p)^{k-n}. k P(X=k-1)\leq P(X=k) \geq P(X=k+1). P(X=k-1)\leq P(X=k) 1 \leq \frac{P(X=k)}{P(X=k-1)}=\frac{\binom{k-1}{n-1}p^n(1-p)^{k-n}}{\binom{k-2}{n-1}p^{n}(1-p)^{k-n-1}}.,"['probability', 'statistics']"
45,Skellam CDF Increasing vs Decreasing in a parameter,Skellam CDF Increasing vs Decreasing in a parameter,,"I'm working with the following Poisson difference distribution: $$\text{Prob}\{X_1-X_2 \geq 0\} $$ where $X_1 \sim$ Poisson $(\mu_1)$ is independent from $X_2 \sim$ Poisson $(\mu_2)$. I need to understand the behavior of:  $$\frac{\partial}{\partial \mu_1}\text{Prob}\{X_1-X_2 \geq 0\} $$ My intuition is that the CDF is increasing in $\mu_1$. However, my intuition proved to be wrong in many occasions : ) The Poisson difference distribution (Skellam distribution) has not a convenient closed form distribution, in particular, its PMF includes a BesselI function and it is not straightforward to take the derivative. Therefore, exploiting the fact that $X_1$ and $X_2$ are independent ""we can write"" (this is my claim): $$  \text{Prob}\{X_1-X_2 \geq 0\} = \sum_{k=1}^{\infty}\text{Prob}\{X_1=k\}\text{Prob}\{X_2\leq k\}  $$ $$=\sum_{k=0}^{\infty}\text{Prob}\{X_1=k\} \sum_{h=0}^k\text{Prob}\{X_2=h\}  $$ For a generic $k$ $$e^{-\mu_1}\frac{\mu_1^k}{k!} e^{-\mu_2}\sum_{h=0}^k \frac{\mu_2^h}{h!} $$ In my particular case I have that $\mu_2 = v-\mu_1$. To be more specific,  $\mu_1 = vx $ where $v >1$ and $x \in (0,1)$. So I have  $$\frac{e^{-v}}{k!} \mu_1^k \sum_{h=0}^k \frac{\mu_2^h}{h!} $$ Forgetting the constant term I can show that the expression is increasing in $x$.  Taking the derivative with respect to $x$ and studying the sign I get $$vk\mu_1^{k-1}\sum_{h=0}^k \frac{(1-\mu_1^h)}{h!} -v\mu_1^k\sum_{h=0}^k \frac{(1-\mu_1)^h}{h!} >0   $$ If and only if $$\frac{k}{v}>x $$ or $k > \mu_1$ if we just consider the mean. However, this contrasts the simulations I made using the ""proper"" CDF of the Skellam distribution; so I do not know how to approach the problem or where my fault is","I'm working with the following Poisson difference distribution: $$\text{Prob}\{X_1-X_2 \geq 0\} $$ where $X_1 \sim$ Poisson $(\mu_1)$ is independent from $X_2 \sim$ Poisson $(\mu_2)$. I need to understand the behavior of:  $$\frac{\partial}{\partial \mu_1}\text{Prob}\{X_1-X_2 \geq 0\} $$ My intuition is that the CDF is increasing in $\mu_1$. However, my intuition proved to be wrong in many occasions : ) The Poisson difference distribution (Skellam distribution) has not a convenient closed form distribution, in particular, its PMF includes a BesselI function and it is not straightforward to take the derivative. Therefore, exploiting the fact that $X_1$ and $X_2$ are independent ""we can write"" (this is my claim): $$  \text{Prob}\{X_1-X_2 \geq 0\} = \sum_{k=1}^{\infty}\text{Prob}\{X_1=k\}\text{Prob}\{X_2\leq k\}  $$ $$=\sum_{k=0}^{\infty}\text{Prob}\{X_1=k\} \sum_{h=0}^k\text{Prob}\{X_2=h\}  $$ For a generic $k$ $$e^{-\mu_1}\frac{\mu_1^k}{k!} e^{-\mu_2}\sum_{h=0}^k \frac{\mu_2^h}{h!} $$ In my particular case I have that $\mu_2 = v-\mu_1$. To be more specific,  $\mu_1 = vx $ where $v >1$ and $x \in (0,1)$. So I have  $$\frac{e^{-v}}{k!} \mu_1^k \sum_{h=0}^k \frac{\mu_2^h}{h!} $$ Forgetting the constant term I can show that the expression is increasing in $x$.  Taking the derivative with respect to $x$ and studying the sign I get $$vk\mu_1^{k-1}\sum_{h=0}^k \frac{(1-\mu_1^h)}{h!} -v\mu_1^k\sum_{h=0}^k \frac{(1-\mu_1)^h}{h!} >0   $$ If and only if $$\frac{k}{v}>x $$ or $k > \mu_1$ if we just consider the mean. However, this contrasts the simulations I made using the ""proper"" CDF of the Skellam distribution; so I do not know how to approach the problem or where my fault is",,"['calculus', 'probability', 'statistics', 'probability-distributions']"
46,Gaussian vector multiplied with a matrix is another Gaussian vector: How to show?,Gaussian vector multiplied with a matrix is another Gaussian vector: How to show?,,"Assume that $w$ is a $M$ dimensional random vector, such that: $w \sim N(w|0,\alpha^{-1} I)$. Now I have a $N \times M$ matrix $\Phi$, which is not random. I want show that the vector $Y= \Phi w$ is another Gaussian vector. What I tried is the following: Each component of $w_i$ is a scalar Gaussian, with zero mean and variance $1/\alpha$. Moreover, these components are independent due to the form of the covariance matrix of the joint distribution. I tried to form the distribution of the following linear combination of deterministic vectors: $$ Y = \sum_{m=1}^{M} w_m\Phi_m$$. $\Phi_m$ is the $m.$ column of the matrix $\Phi$. By using this approach, I can show that each component of the vector $Y_n$ is a Gaussian, but I cannot show that they are jointly Gaussian as well. What should I do instead here? Edit: Note that I just want to show $Y$ is another Gaussian; I am not after showing its mean and covariance, which I can find indeed.","Assume that $w$ is a $M$ dimensional random vector, such that: $w \sim N(w|0,\alpha^{-1} I)$. Now I have a $N \times M$ matrix $\Phi$, which is not random. I want show that the vector $Y= \Phi w$ is another Gaussian vector. What I tried is the following: Each component of $w_i$ is a scalar Gaussian, with zero mean and variance $1/\alpha$. Moreover, these components are independent due to the form of the covariance matrix of the joint distribution. I tried to form the distribution of the following linear combination of deterministic vectors: $$ Y = \sum_{m=1}^{M} w_m\Phi_m$$. $\Phi_m$ is the $m.$ column of the matrix $\Phi$. By using this approach, I can show that each component of the vector $Y_n$ is a Gaussian, but I cannot show that they are jointly Gaussian as well. What should I do instead here? Edit: Note that I just want to show $Y$ is another Gaussian; I am not after showing its mean and covariance, which I can find indeed.",,"['probability', 'probability-distributions', 'normal-distribution']"
47,Hitting time for a random walk on the grid,Hitting time for a random walk on the grid,,"Consider the grid on $n^2$ nodes composed of points $x$ and $y$ with coordinates in the set  $\{1, \ldots, n\}$, and consider the discrete-time Markov chain which transitions to a random neighbor at each step. Up to a constant factor, what is the hitting time from $(x_1,y_1)$ to $(x_2, y_2)$? A resistance based analysis does not seem to solve this, as resistances capture commute time, not hitting time, and I do not see any argument to ensure that hitting times will be symmetric on this graph. The worst-case hitting time is $O(n^2 \log n)$ on this graph which can be proven by looking at resistances (see this paper ), but I'm interested in the scaling with $|x_1 - x_2|$ and $|y_1 - y_2|$. It seems natural to guess the answer is $\Theta \left( (x_1 - x_2)^2 + (y_1 - y_2)^2 \right)$ with some log factors but I'm not sure how to prove this. I also wonder if there is a neat way to prove such statement - a resistance analysis for the commute time allows us to avoid messing around with a system of equations; does a similar tool exist for hitting time?","Consider the grid on $n^2$ nodes composed of points $x$ and $y$ with coordinates in the set  $\{1, \ldots, n\}$, and consider the discrete-time Markov chain which transitions to a random neighbor at each step. Up to a constant factor, what is the hitting time from $(x_1,y_1)$ to $(x_2, y_2)$? A resistance based analysis does not seem to solve this, as resistances capture commute time, not hitting time, and I do not see any argument to ensure that hitting times will be symmetric on this graph. The worst-case hitting time is $O(n^2 \log n)$ on this graph which can be proven by looking at resistances (see this paper ), but I'm interested in the scaling with $|x_1 - x_2|$ and $|y_1 - y_2|$. It seems natural to guess the answer is $\Theta \left( (x_1 - x_2)^2 + (y_1 - y_2)^2 \right)$ with some log factors but I'm not sure how to prove this. I also wonder if there is a neat way to prove such statement - a resistance analysis for the commute time allows us to avoid messing around with a system of equations; does a similar tool exist for hitting time?",,"['probability', 'probability-theory']"
48,Analysis of sorting Algorithm with probably wrong comparator?,Analysis of sorting Algorithm with probably wrong comparator?,,"It is an interesting question from an Interview, I failed it. An array has $n$ different elements $[A_1 , A_2, ..., A_n]$ （random order）. We have a comparator $C$, but it has a probability  $p$ to return correct results. Now we use $C$ to implement sorting algorithm (any kind, bubble, quick etc..) After sorting we have $[A_{i_1}, A_{i_2}, ..., A_{i_n}]$ (It could be wrong) . Now given a number $m$ ($m < n$), the question is as follows: What is Expectation of size $S$ of Intersection between $\{A_1, A_2, ...， A_m\}$ and $\{A_{i_1}, A_{i_2}, ..., A_{i_n}\},$ in other words, what is $E[S]$? Any relationship among $m$, $n$ and $p$ ? If we use different sorting algorithm, how will $E[S]$ change ? My idea is as follows: When $m=n$, $E[S] = n$, surely. When $m=n-1$, $E[S] = n-1+P(A_n$ in $A_{i_n})$. I don't know how to complete the answer but I thought it could be solved through induction.. Any simulation methods would also be fine I think.","It is an interesting question from an Interview, I failed it. An array has $n$ different elements $[A_1 , A_2, ..., A_n]$ （random order）. We have a comparator $C$, but it has a probability  $p$ to return correct results. Now we use $C$ to implement sorting algorithm (any kind, bubble, quick etc..) After sorting we have $[A_{i_1}, A_{i_2}, ..., A_{i_n}]$ (It could be wrong) . Now given a number $m$ ($m < n$), the question is as follows: What is Expectation of size $S$ of Intersection between $\{A_1, A_2, ...， A_m\}$ and $\{A_{i_1}, A_{i_2}, ..., A_{i_n}\},$ in other words, what is $E[S]$? Any relationship among $m$, $n$ and $p$ ? If we use different sorting algorithm, how will $E[S]$ change ? My idea is as follows: When $m=n$, $E[S] = n$, surely. When $m=n-1$, $E[S] = n-1+P(A_n$ in $A_{i_n})$. I don't know how to complete the answer but I thought it could be solved through induction.. Any simulation methods would also be fine I think.",,"['probability', 'combinatorics', 'probability-theory', 'algorithms', 'sorting']"
49,Kruskal Wallis - Effect size,Kruskal Wallis - Effect size,,"I analyse 4 algorithms and 3 sets of metrics for each algorithm in which I apply the non-parametric Kruskal-Wallis test for each metric to detect any differences in performance between these algorithms. I would like to know whether there is a way to calculate the effect size when applying the Kruskal-Wallis test. As mentioned in other posts in CV, a post-hoc analysis for Kruskal-Wallis should use the Dunn's test and not the Mann-Witney test for pairwise comparisons between groups (algorithms). By applying the ""inaccurate"" MW test, I can calculate the effect size, but what can I do if I apply Dunn's test? Thanks in advance for any comment/advice. PS: I posted this question to CV some time ago, but I didn't receive any reply yet. Hence, I post it in this forum too.","I analyse 4 algorithms and 3 sets of metrics for each algorithm in which I apply the non-parametric Kruskal-Wallis test for each metric to detect any differences in performance between these algorithms. I would like to know whether there is a way to calculate the effect size when applying the Kruskal-Wallis test. As mentioned in other posts in CV, a post-hoc analysis for Kruskal-Wallis should use the Dunn's test and not the Mann-Witney test for pairwise comparisons between groups (algorithms). By applying the ""inaccurate"" MW test, I can calculate the effect size, but what can I do if I apply Dunn's test? Thanks in advance for any comment/advice. PS: I posted this question to CV some time ago, but I didn't receive any reply yet. Hence, I post it in this forum too.",,"['probability', 'statistics', 'statistical-inference']"
50,Lower Bound on Expectation of Operator Norm,Lower Bound on Expectation of Operator Norm,,"I've been working through Terence Tao's text on random matrices, and there's a step in a proof that I am having trouble with.  We want to show Proposition 2.3.19.  The assumptions are $M$ symmetric with entries which are variance 1, mean 0, and $O(1)$ in magnitude.  The eventual goal is to conclude that $E{||M||}_{op}$ is bounded below by $(2-o(1)) \sqrt{n}$. Let's say we've shown that $E{||M||}^k_{op} \geq (C_{k/2} +o_{k}(1))n^{k/2}$.  We want to conclude something about $E{||M||}_{op}$ from this, but the k inside of the expectation is bothersome.  The proof says to combine the previous result with a corollary of Talagrand's theorem for the Gaussian decay of the operator norm in such a case; namely $Pr[\big|{||M||}_{op}-E{||M||}_{op}\big| > t] \leq a e^{-bt^2}$.  The formula also holds for the median in place of the mean, if this is helpful. Using the above two facts, we are supposed to deduce that $E{||M||}_{op} \geq (C_{k/2}^{1/k}+o_k(1))\sqrt{n}$ Conceptually for me, I am having trouble connecting concentration of measure about the mean with knowledge of the value of the mean, at least in the lower bound direction. Much appreciated","I've been working through Terence Tao's text on random matrices, and there's a step in a proof that I am having trouble with.  We want to show Proposition 2.3.19.  The assumptions are $M$ symmetric with entries which are variance 1, mean 0, and $O(1)$ in magnitude.  The eventual goal is to conclude that $E{||M||}_{op}$ is bounded below by $(2-o(1)) \sqrt{n}$. Let's say we've shown that $E{||M||}^k_{op} \geq (C_{k/2} +o_{k}(1))n^{k/2}$.  We want to conclude something about $E{||M||}_{op}$ from this, but the k inside of the expectation is bothersome.  The proof says to combine the previous result with a corollary of Talagrand's theorem for the Gaussian decay of the operator norm in such a case; namely $Pr[\big|{||M||}_{op}-E{||M||}_{op}\big| > t] \leq a e^{-bt^2}$.  The formula also holds for the median in place of the mean, if this is helpful. Using the above two facts, we are supposed to deduce that $E{||M||}_{op} \geq (C_{k/2}^{1/k}+o_k(1))\sqrt{n}$ Conceptually for me, I am having trouble connecting concentration of measure about the mean with knowledge of the value of the mean, at least in the lower bound direction. Much appreciated",,"['probability', 'random-matrices']"
51,"Calculating $\mathbf{P}[X < Y]$ for $X, Y$ exponentially distributed?",Calculating  for  exponentially distributed?,"\mathbf{P}[X < Y] X, Y","This is exercise 2.2.1 from Achim Klenke: »Probability Theory — A Comprehensive Course« Let $X$ and $Y$ be independent random variables with $X \sim \exp_\theta$ and $Y \sim \exp_\rho$ for certain $\theta,\rho > 0$. Show that $$\mathbf{P}[X < Y] = \frac{\theta}{\theta +\rho}\, .$$ Now, in practice, this exercise is easy. $\exp_\theta$-distribution is defined as $$ \mathbf{P}[X \leq x] = \int_0^x \theta e^{-\theta t} \, dt \quad \text{ for } x \geq 0\, .$$ We just have to evaluate the integral:  $$\int_0^\infty \mathbf{P}[X \leq x] \cdot \rho e^{-\rho x} \, d x = \int_0^\infty \Bigl(\int_0^x \theta e^{- \theta t} \, d t \Bigr) \cdot \rho e^{-\rho x} \, d x\, ,$$ which gives $\frac{\theta}{\theta +\rho}$. But how does one do it rigorously? Why is the following possible: $$\mathbf{P}[X < Y] = \int_0^\infty \mathbf{P}[X \leq x]\cdot \mathbf{P}[Y = x] \, d x \\ \text{ and using } \mathbf{P}[Y = x] = \rho e^{-\rho x} \, ?$$ Convolution of real valued random variables hasn't been defined yet.","This is exercise 2.2.1 from Achim Klenke: »Probability Theory — A Comprehensive Course« Let $X$ and $Y$ be independent random variables with $X \sim \exp_\theta$ and $Y \sim \exp_\rho$ for certain $\theta,\rho > 0$. Show that $$\mathbf{P}[X < Y] = \frac{\theta}{\theta +\rho}\, .$$ Now, in practice, this exercise is easy. $\exp_\theta$-distribution is defined as $$ \mathbf{P}[X \leq x] = \int_0^x \theta e^{-\theta t} \, dt \quad \text{ for } x \geq 0\, .$$ We just have to evaluate the integral:  $$\int_0^\infty \mathbf{P}[X \leq x] \cdot \rho e^{-\rho x} \, d x = \int_0^\infty \Bigl(\int_0^x \theta e^{- \theta t} \, d t \Bigr) \cdot \rho e^{-\rho x} \, d x\, ,$$ which gives $\frac{\theta}{\theta +\rho}$. But how does one do it rigorously? Why is the following possible: $$\mathbf{P}[X < Y] = \int_0^\infty \mathbf{P}[X \leq x]\cdot \mathbf{P}[Y = x] \, d x \\ \text{ and using } \mathbf{P}[Y = x] = \rho e^{-\rho x} \, ?$$ Convolution of real valued random variables hasn't been defined yet.",,"['probability', 'probability-theory', 'probability-distributions', 'exponential-distribution']"
52,Interchangeability of the malliavin derivative with a lebesgue integral,Interchangeability of the malliavin derivative with a lebesgue integral,,I was curious to know the most general conditions under which a Malliavin derivative $\mathscr{D}_t \int^T_t F_v d\mu(v) = \int^T_t \mathscr{D}_t F_v d\mu(v)$ commutes with a Lebesgue integral? I was just curious to know all the assumptions.,I was curious to know the most general conditions under which a Malliavin derivative $\mathscr{D}_t \int^T_t F_v d\mu(v) = \int^T_t \mathscr{D}_t F_v d\mu(v)$ commutes with a Lebesgue integral? I was just curious to know all the assumptions.,,"['probability', 'analysis', 'stochastic-processes', 'stochastic-analysis', 'malliavin-calculus']"
53,Gaussian processes versus Bayes rule misinterpretation,Gaussian processes versus Bayes rule misinterpretation,,"I would like to use Gaussian processes (GP) for Bayesian classification of medical data. I think I already understand the basic stuff but I have some uncertainties that are perhaps partly related to the notation. It is known that in GP regression (assuming Gaussian likelihood function) the prediction on unknown data using GP can be obtained by using closed form formulas that are based on marginalizing jointly Gaussian multivariate distribution. However, to get more insight into the connection of GP with the Bayesian framework, I would like to see the derivation (or at least understand the idea behind this) using Bayes rule. I am interested in that since it can help me to understand what is happening in the case that likelihood is non-Gaussian and hence an elegant solution is no longer possible. I tried to find the solution to my questions on the Internet but to no avail. The notation I show further was taken from the online lecture on GP: http://videolectures.net/mlss2012_cunningham_gaussian_processes/?q=gaussian%20processed It is assumed that observed data values y are defined in terms of noisy-free function f with added Gaussian noise $\varepsilon $. $$ y=f+\varepsilon  $$ I would solve the problem first by calculating the posterior distribution (1). [1] Data posterior: $$ p\left ( f \right| y)=\frac{p\left (y|f\right)p\left (f\right )}{p\left (y  \right )} $$ However, in order to get posterior (1), one needs to calculate the marginal likelihood, which is generally difficult. [2] Marginal likelihood: $$ p\left ( y \right )=\int p\left ( y|f \right )p\left (f  \right )df $$ Finally, when one obtains the posterior distribution, it is possible to get the predictions by calculating other nasty integrals (3, 4). [3] Predictive posterior: $$ p\left ( f^{*} \right| y)=\int {p\left (f^{*}|f\right)p\left (f|y\right )}df $$ [4] Predictive distribution: $$ p\left ( y^{*} \right| y)=\int{p\left (y^{*}|f^{*}\right)p\left (f{^{*}}|y\right )}df^{*} $$ When I tried to do these calculations, I run into the problems that generated many questions: First, I don't know how to make integrations over the space of functions (equations 2-4). Specifically, marginalization (upon computing marginal likelihood [2]) with respect to the latent function f specified by Gaussian process prior p(f) makes a little sense for me. Second, It is known that for Gaussian likelihood (which is the case), the conjugated prior for the Gaussian process prior is again Gaussian process. It means that posterior probability distribution is Gaussian process and should integrate to 1 if it is valid probability distribution. I don't know what it means that Gaussian process integrate to 1 since Gaussian process is a distribution on function defined via mean function and covariance function (kernel). It is hard to see any relation of GP to some probability density function (pdf) over functions. Third, if we want to do classification, we need change Gaussian likelihood to some function that maps any input given by prior to <0,1> interval. Posterior is not Gaussian process in this case but one should expect that posterior should remain some infinite dimensional distribution since it is a result of the product of likelihood and GP prior (which is itself infinite dimensional). In case it is true, does not have such a distribution positive semidefinite kernel (otherwise it can be GP) ??? Fourth, What is the predictive posterior good for [3] ??. I consider useful only the predictive distribution [4], which give us the predictions on new values of y* [4]. It is however sometimes denoted also as the predictive posterior or posterior predictive, which is a bit confusing. Last, The posterior must be given by the product of prior and likelihood function. However, I  do read often that in the case of GP classification, GP prior is squeezed between <0,1> (using eg. logistic or probit function) rather than being multiplied by sigmoid likelihood function. I am so confused. Any help will be appreciated. Thanks a lot.","I would like to use Gaussian processes (GP) for Bayesian classification of medical data. I think I already understand the basic stuff but I have some uncertainties that are perhaps partly related to the notation. It is known that in GP regression (assuming Gaussian likelihood function) the prediction on unknown data using GP can be obtained by using closed form formulas that are based on marginalizing jointly Gaussian multivariate distribution. However, to get more insight into the connection of GP with the Bayesian framework, I would like to see the derivation (or at least understand the idea behind this) using Bayes rule. I am interested in that since it can help me to understand what is happening in the case that likelihood is non-Gaussian and hence an elegant solution is no longer possible. I tried to find the solution to my questions on the Internet but to no avail. The notation I show further was taken from the online lecture on GP: http://videolectures.net/mlss2012_cunningham_gaussian_processes/?q=gaussian%20processed It is assumed that observed data values y are defined in terms of noisy-free function f with added Gaussian noise $\varepsilon $. $$ y=f+\varepsilon  $$ I would solve the problem first by calculating the posterior distribution (1). [1] Data posterior: $$ p\left ( f \right| y)=\frac{p\left (y|f\right)p\left (f\right )}{p\left (y  \right )} $$ However, in order to get posterior (1), one needs to calculate the marginal likelihood, which is generally difficult. [2] Marginal likelihood: $$ p\left ( y \right )=\int p\left ( y|f \right )p\left (f  \right )df $$ Finally, when one obtains the posterior distribution, it is possible to get the predictions by calculating other nasty integrals (3, 4). [3] Predictive posterior: $$ p\left ( f^{*} \right| y)=\int {p\left (f^{*}|f\right)p\left (f|y\right )}df $$ [4] Predictive distribution: $$ p\left ( y^{*} \right| y)=\int{p\left (y^{*}|f^{*}\right)p\left (f{^{*}}|y\right )}df^{*} $$ When I tried to do these calculations, I run into the problems that generated many questions: First, I don't know how to make integrations over the space of functions (equations 2-4). Specifically, marginalization (upon computing marginal likelihood [2]) with respect to the latent function f specified by Gaussian process prior p(f) makes a little sense for me. Second, It is known that for Gaussian likelihood (which is the case), the conjugated prior for the Gaussian process prior is again Gaussian process. It means that posterior probability distribution is Gaussian process and should integrate to 1 if it is valid probability distribution. I don't know what it means that Gaussian process integrate to 1 since Gaussian process is a distribution on function defined via mean function and covariance function (kernel). It is hard to see any relation of GP to some probability density function (pdf) over functions. Third, if we want to do classification, we need change Gaussian likelihood to some function that maps any input given by prior to <0,1> interval. Posterior is not Gaussian process in this case but one should expect that posterior should remain some infinite dimensional distribution since it is a result of the product of likelihood and GP prior (which is itself infinite dimensional). In case it is true, does not have such a distribution positive semidefinite kernel (otherwise it can be GP) ??? Fourth, What is the predictive posterior good for [3] ??. I consider useful only the predictive distribution [4], which give us the predictions on new values of y* [4]. It is however sometimes denoted also as the predictive posterior or posterior predictive, which is a bit confusing. Last, The posterior must be given by the product of prior and likelihood function. However, I  do read often that in the case of GP classification, GP prior is squeezed between <0,1> (using eg. logistic or probit function) rather than being multiplied by sigmoid likelihood function. I am so confused. Any help will be appreciated. Thanks a lot.",,"['probability', 'stochastic-processes', 'conditional-probability', 'bayes-theorem']"
54,Find a symmetric random walk on $\mathbb{Z}$ that is transient.,Find a symmetric random walk on  that is transient.,\mathbb{Z},"I wanted to know if it is possible find a symmetric random walk on $Z$ that is not recurrent. Let $X$ have the following distribution, with a probability $1/2^{i+1}$, $X=\pm b_i$. Let $$S_n=\sum_{k=1}^n X_k$$ be the random walk on $\mathbb{Z}$ with $X_k$ having the same distribution as $X$. The problem is to find $b_n$ such that $S_n$ is transient. We need to find $b_n$ large enough so that $E(X)$ is not defined (otherwise it would be 0 by symmetry and thus recurrent). I think $b_n=2^n$ is sufficient, but am having trouble showing transience. We know that a random walk is transient iff $$\int_{-\pi}^\pi \frac{1}{1-\varphi_X(t)}dt<\infty$$ I computed the characteristic function of $X$ as follows: $$\varphi_X(t)=\sum_{n=1}^\infty \frac{1}{2^{n+1}}e^{it2^n}+\frac{1}{2^{n+1}}e^{it2^{-n}}=\sum_{n=1}^\infty \frac{\cos(t2^n)}{2^n}$$ But I am unsure of how to proceed.","I wanted to know if it is possible find a symmetric random walk on $Z$ that is not recurrent. Let $X$ have the following distribution, with a probability $1/2^{i+1}$, $X=\pm b_i$. Let $$S_n=\sum_{k=1}^n X_k$$ be the random walk on $\mathbb{Z}$ with $X_k$ having the same distribution as $X$. The problem is to find $b_n$ such that $S_n$ is transient. We need to find $b_n$ large enough so that $E(X)$ is not defined (otherwise it would be 0 by symmetry and thus recurrent). I think $b_n=2^n$ is sufficient, but am having trouble showing transience. We know that a random walk is transient iff $$\int_{-\pi}^\pi \frac{1}{1-\varphi_X(t)}dt<\infty$$ I computed the characteristic function of $X$ as follows: $$\varphi_X(t)=\sum_{n=1}^\infty \frac{1}{2^{n+1}}e^{it2^n}+\frac{1}{2^{n+1}}e^{it2^{-n}}=\sum_{n=1}^\infty \frac{\cos(t2^n)}{2^n}$$ But I am unsure of how to proceed.",,"['probability', 'probability-theory', 'probability-distributions']"
55,Definition and derivation of conditional expectation/probability,Definition and derivation of conditional expectation/probability,,"I read quite a few books introducing the notion of conditional probabilities/expectation by putting a formula out there coming from what they call ""intuition"". Can someone provide me a good measure theoretic derivation of how to find the conditional expectation or probability of a random variable ? Or even a link or something. Basically knowing the basic in measure theory,let $(\Omega, Z,P)$ be a probability space. How do you derive finally that any function g s.t : \begin{align} \int_A YdP = \int_A gdP_{|G} \end{align} is the expectation of $Y$ knowing $G$. And also how do we go from there to find : \begin{align} P(A|B) = \frac{P(A,B)}{P(B)} \end{align} Any explanation/links/books is appreciated !!","I read quite a few books introducing the notion of conditional probabilities/expectation by putting a formula out there coming from what they call ""intuition"". Can someone provide me a good measure theoretic derivation of how to find the conditional expectation or probability of a random variable ? Or even a link or something. Basically knowing the basic in measure theory,let $(\Omega, Z,P)$ be a probability space. How do you derive finally that any function g s.t : \begin{align} \int_A YdP = \int_A gdP_{|G} \end{align} is the expectation of $Y$ knowing $G$. And also how do we go from there to find : \begin{align} P(A|B) = \frac{P(A,B)}{P(B)} \end{align} Any explanation/links/books is appreciated !!",,"['probability', 'measure-theory', 'probability-theory', 'expectation', 'conditional-expectation']"
56,"Conditional probability of picking particular second letters in Scrabble, given the first letter picked","Conditional probability of picking particular second letters in Scrabble, given the first letter picked",,"I was doing a question today and couldn't understand the answer. Here's the question, my attempt, and the answer: Question In a game of Scrabble, Dalene has the seven letters A, D, E, K, O, Q and S. She picks two of these letters at random. Given that she picks the letter Q first, what is the probability that she picks the letter D or the letter K second? My attempt A = letter Q, B = letter D or K $P(A) = \frac{1}{7}$ $P(B) = \frac{2}{6} = \frac{1}{3}$ So my answer is just $P(B) = \frac{1}{3}$ Official answer However, the Correct answer gives $\frac{2}{7}$. Why is this? Thanks in advance!","I was doing a question today and couldn't understand the answer. Here's the question, my attempt, and the answer: Question In a game of Scrabble, Dalene has the seven letters A, D, E, K, O, Q and S. She picks two of these letters at random. Given that she picks the letter Q first, what is the probability that she picks the letter D or the letter K second? My attempt A = letter Q, B = letter D or K $P(A) = \frac{1}{7}$ $P(B) = \frac{2}{6} = \frac{1}{3}$ So my answer is just $P(B) = \frac{1}{3}$ Official answer However, the Correct answer gives $\frac{2}{7}$. Why is this? Thanks in advance!",,"['probability', 'conditional-probability']"
57,Probability that a five is seen before any of the even numbers are seen,Probability that a five is seen before any of the even numbers are seen,,"A fair die is repeatedly tossed. What is the probability that a five is seen before any of the even numbers are seen? I have my own solution below and just want someone to verify it. According to the problem, it means that only 1's and 3's are allowed to appear before the first five. And now suppose there are $n$ rolls before the first five. For each of the $n$ rolls, there are two possibilities of 1 or 3. Therefore, there are totally $2^n$ different outcomes before the first five. And the probability for each outcome to happen is simply $\frac{1}{6}^n$. After all, the probability of getting the five at the last is 1/6, then the probability for one fixed $n$ is  $$ 2^n \left(\frac{1}{6}\right)^n\frac{1}{6} $$ Finally we sum up all the scenarios for each possible $n$ from $0$ to infinity to get $$ \sum_{n=0}^{\infty} 2^n \left(\frac{1}{6}\right)^n\frac{1}{6} = \sum_{n=0}^{\infty} \left(\frac{1}{3}\right)^n\frac{1}{6} = \frac{1}{6}\frac{1}{1-\frac{1}{3}}=\frac{1}{4} $$ Did I do this right? Thank you very much!","A fair die is repeatedly tossed. What is the probability that a five is seen before any of the even numbers are seen? I have my own solution below and just want someone to verify it. According to the problem, it means that only 1's and 3's are allowed to appear before the first five. And now suppose there are $n$ rolls before the first five. For each of the $n$ rolls, there are two possibilities of 1 or 3. Therefore, there are totally $2^n$ different outcomes before the first five. And the probability for each outcome to happen is simply $\frac{1}{6}^n$. After all, the probability of getting the five at the last is 1/6, then the probability for one fixed $n$ is  $$ 2^n \left(\frac{1}{6}\right)^n\frac{1}{6} $$ Finally we sum up all the scenarios for each possible $n$ from $0$ to infinity to get $$ \sum_{n=0}^{\infty} 2^n \left(\frac{1}{6}\right)^n\frac{1}{6} = \sum_{n=0}^{\infty} \left(\frac{1}{3}\right)^n\frac{1}{6} = \frac{1}{6}\frac{1}{1-\frac{1}{3}}=\frac{1}{4} $$ Did I do this right? Thank you very much!",,"['probability', 'probability-distributions', 'random-variables']"
58,Cramer's theorem reference request,Cramer's theorem reference request,,"I'm looking for a proof of Cramer's theorem that states the following: Let $X,Y$ two independent random variables such that $X+Y$ is normal distributed, then $X$ and $Y$ are normal distributed. Any help will be appreciated","I'm looking for a proof of Cramer's theorem that states the following: Let $X,Y$ two independent random variables such that $X+Y$ is normal distributed, then $X$ and $Y$ are normal distributed. Any help will be appreciated",,"['probability', 'reference-request']"
59,Cramér's Model - “The Prime Numbers and Their Distribution” - Part 4,Cramér's Model - “The Prime Numbers and Their Distribution” - Part 4,,"Following a previous question ( here you'll find an introduction): A paper by Maier which refutes Cramer's Model suggests we should replace the heuristic ""$\Bbb P(n\in\mathcal P)=1/\log n$"" with $$\Bbb P(n\in\mathcal P|P^-(n)\gt z)=\frac{1}{\log n}\prod_{p\le z\atop p\in\mathcal P}(1-1/p)^{-1}\quad (z\approx \log n)$$ where $P^{-}(n)$ denotes the least prime number that divides n. The book states that the new heuristic leads us to expects that the strong Cramér's conjecture $$\limsup_{n\to\infty}\frac{p_{n+1}-p_n}{(\log p_n)^2}=1$$ (which is derived in this paper by Cramer ) is false and should be replaced by $$\limsup_{n\to\infty}\frac{p_{n+1}-p_n}{(\log p_n)^2}=2\mathrm e^{-\gamma}$$ where $p_n$ denotes the $n^{th}$ prime number, and $\gamma$ denotes Euler's constant. For proving this last implication, I should mention Mertens' formula: $$\prod_{p\le z\atop p\in\mathcal P}(1-1/p)^{-1}=\mathrm e^\gamma\log z+O(1)\quad(z\ge 1)$$ I couldn't prove this.","Following a previous question ( here you'll find an introduction): A paper by Maier which refutes Cramer's Model suggests we should replace the heuristic ""$\Bbb P(n\in\mathcal P)=1/\log n$"" with $$\Bbb P(n\in\mathcal P|P^-(n)\gt z)=\frac{1}{\log n}\prod_{p\le z\atop p\in\mathcal P}(1-1/p)^{-1}\quad (z\approx \log n)$$ where $P^{-}(n)$ denotes the least prime number that divides n. The book states that the new heuristic leads us to expects that the strong Cramér's conjecture $$\limsup_{n\to\infty}\frac{p_{n+1}-p_n}{(\log p_n)^2}=1$$ (which is derived in this paper by Cramer ) is false and should be replaced by $$\limsup_{n\to\infty}\frac{p_{n+1}-p_n}{(\log p_n)^2}=2\mathrm e^{-\gamma}$$ where $p_n$ denotes the $n^{th}$ prime number, and $\gamma$ denotes Euler's constant. For proving this last implication, I should mention Mertens' formula: $$\prod_{p\le z\atop p\in\mathcal P}(1-1/p)^{-1}=\mathrm e^\gamma\log z+O(1)\quad(z\ge 1)$$ I couldn't prove this.",,"['probability', 'number-theory', 'prime-numbers', 'prime-gaps']"
60,Rate of convergence of a martingale,Rate of convergence of a martingale,,"I have a question related to convergence rates of martingales: Assume that there is a sequence of maximized likelihood ratios: $ \frac{f_{\hat{\theta}_{n}} \left ( Y_{1},Y_{2},\dots,Y_{n} \right ) }{f_{0} \left (Y_{1},Y_{2},...Y_{n} \right )} $ where $O$ denotes the true parameter and $\hat{\theta}_{n} \in \Theta - \{0\}$ is the maximum likelihood estimate of the unknown parameter at time $n$. Moreover the observation sequence $Y_{1},Y_{2},...Y_{n}$ can in general be dependent. An important information is that the parameter set $\Theta$ is finite. I.e. without loss of generality say $\Theta = \{0,1,2,...,K\} \Leftrightarrow |\Theta|=K+1$. The sequence of such ratios is known that forms a submartingale that converges to $0$ almost surely. I am trying to estimate the convergence rate (of the general dependent case) but I can't. Can anybody suggest something on that?","I have a question related to convergence rates of martingales: Assume that there is a sequence of maximized likelihood ratios: $ \frac{f_{\hat{\theta}_{n}} \left ( Y_{1},Y_{2},\dots,Y_{n} \right ) }{f_{0} \left (Y_{1},Y_{2},...Y_{n} \right )} $ where $O$ denotes the true parameter and $\hat{\theta}_{n} \in \Theta - \{0\}$ is the maximum likelihood estimate of the unknown parameter at time $n$. Moreover the observation sequence $Y_{1},Y_{2},...Y_{n}$ can in general be dependent. An important information is that the parameter set $\Theta$ is finite. I.e. without loss of generality say $\Theta = \{0,1,2,...,K\} \Leftrightarrow |\Theta|=K+1$. The sequence of such ratios is known that forms a submartingale that converges to $0$ almost surely. I am trying to estimate the convergence rate (of the general dependent case) but I can't. Can anybody suggest something on that?",,"['probability', 'statistics', 'probability-theory', 'martingales']"
61,Drawing previously undrawn cards from a deck,Drawing previously undrawn cards from a deck,,"Suppose you have a deck of $y$ cards. First, randomly select $y-x$ distinct cards and sign the face of each, then shuffle all the cards back in to the deck. Proceed as follows: Draw a card. If it is already signed, replace the card and shuffle the deck. If it is not yet signed, sign it, then replace the card and shuffle the deck. My question is what is the probability that you will draw an unsigned card as a function of time? For instance, when $t=1$, the probability is $x/y$. When $t=2$, the probability is $\frac{x(x-1)}{y^2}+\frac{(y-x)(x)}{y^2}$ and so on. I have written a Python program that will compute the probabilities for small values of $t$, but the run time is $O(2^t)$ and was wondering if there is a simpler way to solve this problem My solution: The number of summands for time $t$ is $2^{t-1}$, and each is of the form $\frac{a_1a_2\dots a_t}{y^t}$. The set of $t$-tuples $a_1a_2\dots a_t$ appearing in the sum can be put in bijection with the set of odd binary strings of length $t$ as follows: If the $i$-th digit of the string is 1, then $a_i$ is $x$ minus the sum of the previous digits of the string; otherwise, $a_i$ is equal to $y-x$ plus the sum of the previous digits. For instance, $1101$ corresponds with $x(x-1)(y-x+2)(x-2)$ and $1011$ corresponds with $x(y-x+1)(x-1)(x-2)$. It is pretty simple to write an algorithm that will loop over all such binary strings to find the probability.","Suppose you have a deck of $y$ cards. First, randomly select $y-x$ distinct cards and sign the face of each, then shuffle all the cards back in to the deck. Proceed as follows: Draw a card. If it is already signed, replace the card and shuffle the deck. If it is not yet signed, sign it, then replace the card and shuffle the deck. My question is what is the probability that you will draw an unsigned card as a function of time? For instance, when $t=1$, the probability is $x/y$. When $t=2$, the probability is $\frac{x(x-1)}{y^2}+\frac{(y-x)(x)}{y^2}$ and so on. I have written a Python program that will compute the probabilities for small values of $t$, but the run time is $O(2^t)$ and was wondering if there is a simpler way to solve this problem My solution: The number of summands for time $t$ is $2^{t-1}$, and each is of the form $\frac{a_1a_2\dots a_t}{y^t}$. The set of $t$-tuples $a_1a_2\dots a_t$ appearing in the sum can be put in bijection with the set of odd binary strings of length $t$ as follows: If the $i$-th digit of the string is 1, then $a_i$ is $x$ minus the sum of the previous digits of the string; otherwise, $a_i$ is equal to $y-x$ plus the sum of the previous digits. For instance, $1101$ corresponds with $x(x-1)(y-x+2)(x-2)$ and $1011$ corresponds with $x(y-x+1)(x-1)(x-2)$. It is pretty simple to write an algorithm that will loop over all such binary strings to find the probability.",,"['probability', 'combinatorics']"
62,Standard deviation of a quantum walk?,Standard deviation of a quantum walk?,,"The standard deviation of a classical random walk with $n$ steps is $\sqrt n$ - Standard deviation of a random walk . I have read in many places that the standard deviation of a quantum walk $n$ with a Hadamard transformation for the coin flip operation, and this makes sense intuitively because quantum constructive interference means the quantum walk diffuses more rapidly. However I have not seen any derivation of how this conclusion was arrived. What is the derivation for the standard deviation of a quantum walk?","The standard deviation of a classical random walk with $n$ steps is $\sqrt n$ - Standard deviation of a random walk . I have read in many places that the standard deviation of a quantum walk $n$ with a Hadamard transformation for the coin flip operation, and this makes sense intuitively because quantum constructive interference means the quantum walk diffuses more rapidly. However I have not seen any derivation of how this conclusion was arrived. What is the derivation for the standard deviation of a quantum walk?",,"['probability', 'markov-chains', 'random-walk', 'quantum-mechanics', 'quantum-computation']"
63,hat problem and probability [duplicate],hat problem and probability [duplicate],,This question already has answers here : A riddle about guessing hat colours (which is not among the commonly known ones) (2 answers) Closed 8 years ago . There are 7 prisoners in the room. In the entrance all of them get hat in one of random 2 colors: white or black. They are sitting in the circle and the light on. All of them see hat color of the rest but can't see himself. They will be free if at least one of them say the right color of himself hat color and no one is wrong (they could see the answer or no). The other prisoners can't hear the answer.  How to find the best strategy with the best probability?,This question already has answers here : A riddle about guessing hat colours (which is not among the commonly known ones) (2 answers) Closed 8 years ago . There are 7 prisoners in the room. In the entrance all of them get hat in one of random 2 colors: white or black. They are sitting in the circle and the light on. All of them see hat color of the rest but can't see himself. They will be free if at least one of them say the right color of himself hat color and no one is wrong (they could see the answer or no). The other prisoners can't hear the answer.  How to find the best strategy with the best probability?,,"['probability', 'probability-theory']"
64,Is there any version of Jensen's inequality for quasiconvex function,Is there any version of Jensen's inequality for quasiconvex function,,"I am looking for some generalization of Jensen's inequality for functions $g:\mathbb{R}^n \rightarrow \mathbb{R}$ where $g(x)$ is quasiconvex (or not convex). We known that for convex functions, $$\mathbb{E}\left[g(x)\right] \ge g\left(\mathbb{E}[x]\right).$$ Is there a generalization  of this result for quasiconvex functions or functions which are not convex. Thanks","I am looking for some generalization of Jensen's inequality for functions $g:\mathbb{R}^n \rightarrow \mathbb{R}$ where $g(x)$ is quasiconvex (or not convex). We known that for convex functions, $$\mathbb{E}\left[g(x)\right] \ge g\left(\mathbb{E}[x]\right).$$ Is there a generalization  of this result for quasiconvex functions or functions which are not convex. Thanks",,"['calculus', 'probability', 'inequality', 'convex-analysis']"
65,Skorohod representation theorem,Skorohod representation theorem,,"Assume $X_n$ are random variables such that $\mathbb{P}(X_n \leq B)=1$ for some random variable $B$. Assume also $X_n \Rightarrow X$ ($X_n$ converges weakly to $X$). By the skorohod representation theorem, there exists another probability space $(\Omega',\mathcal{F}',\mathbb{P}')$ and random variables $X_n'$, $X'$ on it (equal in distribution to the $X_n$'s, $X$) such that $X_n' \to X'$ almost everwhere. My question is: can I find a random variable $B'$ equal in distribution to $B$ such that $\mathbb{P}'(X_n' \leq B')=1$. My problem is the following : if it was true that $(X_n,B) \Rightarrow (X^*,B^*)$ with $X^*=X$ in distribution and $B^*=B$ in distribution, then the result would be proved by the Skorohod representation theorem. But it's not true in general, right? It's OK I found the answer: the sequence $(X_n,B)$ is tight. Take a converging subsequence $n_k$ such that $(X_{n_k},B) \Rightarrow (X^*,B^*)$. Then it implies $X^*=X$ and $B^*=B$ in distrib. Then apply the Skorohod repr. thm:  there exists another probability space $(\Omega',\mathcal{F}',\mathbb{P}')$ and random variables $X_{n_k}',B'$, $X_*'$, $B_*'$ on it (equal in distribution to the $X_{n_k}$'s, $X$,$B^*$,$X^*$) such that $(X_{n_k}',B') \to (X_*', B_*')$ almost everwhere. In particular $\mathbb{P}'(X_{n_k}' \leq B')=1$.","Assume $X_n$ are random variables such that $\mathbb{P}(X_n \leq B)=1$ for some random variable $B$. Assume also $X_n \Rightarrow X$ ($X_n$ converges weakly to $X$). By the skorohod representation theorem, there exists another probability space $(\Omega',\mathcal{F}',\mathbb{P}')$ and random variables $X_n'$, $X'$ on it (equal in distribution to the $X_n$'s, $X$) such that $X_n' \to X'$ almost everwhere. My question is: can I find a random variable $B'$ equal in distribution to $B$ such that $\mathbb{P}'(X_n' \leq B')=1$. My problem is the following : if it was true that $(X_n,B) \Rightarrow (X^*,B^*)$ with $X^*=X$ in distribution and $B^*=B$ in distribution, then the result would be proved by the Skorohod representation theorem. But it's not true in general, right? It's OK I found the answer: the sequence $(X_n,B)$ is tight. Take a converging subsequence $n_k$ such that $(X_{n_k},B) \Rightarrow (X^*,B^*)$. Then it implies $X^*=X$ and $B^*=B$ in distrib. Then apply the Skorohod repr. thm:  there exists another probability space $(\Omega',\mathcal{F}',\mathbb{P}')$ and random variables $X_{n_k}',B'$, $X_*'$, $B_*'$ on it (equal in distribution to the $X_{n_k}$'s, $X$,$B^*$,$X^*$) such that $(X_{n_k}',B') \to (X_*', B_*')$ almost everwhere. In particular $\mathbb{P}'(X_{n_k}' \leq B')=1$.",,"['probability', 'probability-theory', 'random-variables']"
66,A question on CLT from Durrett.,A question on CLT from Durrett.,,"I have a question from Durrett which I don't quite get the solution. The question is and the solution is I think I understand up to when $|S_n - n| \leq n^{2/3}$ is an event w.p.1, this is because $P(|S_n - n| \leq n^{2/3}) = P(|S_n - n|/\sigma n^{1/2} \leq n^{1/6}/\sigma)$. The former converges to a standard normal and the latter increases as n increases so the prob. converges to 1. However, I do not understand the first inequality below (where the integral is less than $n^{2/3}$ times the big bracket). I don't quite see why it holds. Please help, thanks.","I have a question from Durrett which I don't quite get the solution. The question is and the solution is I think I understand up to when $|S_n - n| \leq n^{2/3}$ is an event w.p.1, this is because $P(|S_n - n| \leq n^{2/3}) = P(|S_n - n|/\sigma n^{1/2} \leq n^{1/6}/\sigma)$. The former converges to a standard normal and the latter increases as n increases so the prob. converges to 1. However, I do not understand the first inequality below (where the integral is less than $n^{2/3}$ times the big bracket). I don't quite see why it holds. Please help, thanks.",,['probability']
67,Almost sure convergence + convergence in distribution implies joint convergence in distribution?,Almost sure convergence + convergence in distribution implies joint convergence in distribution?,,"I'm wondering, if I have two sequences of random variables $(X_n)$ and $(Y_n)$, defined on the same probability space, such that $X_n\stackrel{a.s.}{\rightarrow}X$ and $Y_n\stackrel{d}{\rightarrow}Y$, is it possible to conclude that they converge jointly in distribution, i.e. $$ (X_n,Y_n)\stackrel{d}{\rightarrow}(X,Y) $$ as $n\to\infty$? I believe that this question is closely related to the following: if $Y_n\stackrel{d}{\rightarrow}Y$, is it true that $$ (X,Y_n)\stackrel{d}{\rightarrow}(X,Y) $$ as $n\to \infty$? Thank you in advance for any thoughts, comments etc.!","I'm wondering, if I have two sequences of random variables $(X_n)$ and $(Y_n)$, defined on the same probability space, such that $X_n\stackrel{a.s.}{\rightarrow}X$ and $Y_n\stackrel{d}{\rightarrow}Y$, is it possible to conclude that they converge jointly in distribution, i.e. $$ (X_n,Y_n)\stackrel{d}{\rightarrow}(X,Y) $$ as $n\to\infty$? I believe that this question is closely related to the following: if $Y_n\stackrel{d}{\rightarrow}Y$, is it true that $$ (X,Y_n)\stackrel{d}{\rightarrow}(X,Y) $$ as $n\to \infty$? Thank you in advance for any thoughts, comments etc.!",,"['probability', 'probability-distributions', 'convergence-divergence', 'almost-everywhere']"
68,zarankiewicz problem lower bound,zarankiewicz problem lower bound,,"I was just reading through the following article: http://page.mi.fu-berlin.de/szabo/PDF/stoc96.pdf On page 2 they give an explicit formula for the lower bound of the size of the graph. Summary: We want to find a lower bound for the size of an $n\times n$ bipartite graph which does not contain the complete bipartite graph $K_{t,u}$ The size is at least $c' n^{2-\frac{t+u-2}{tu-1}}$. The article mentions it can be shown by some probabilistic method and refers to an article by Erdos but I cant find any proof in this paper, do you have some ideas?","I was just reading through the following article: http://page.mi.fu-berlin.de/szabo/PDF/stoc96.pdf On page 2 they give an explicit formula for the lower bound of the size of the graph. Summary: We want to find a lower bound for the size of an $n\times n$ bipartite graph which does not contain the complete bipartite graph $K_{t,u}$ The size is at least $c' n^{2-\frac{t+u-2}{tu-1}}$. The article mentions it can be shown by some probabilistic method and refers to an article by Erdos but I cant find any proof in this paper, do you have some ideas?",,"['probability', 'graph-theory', 'extremal-combinatorics']"
69,Is it reasonable to think of the expectation of an infinite-dimensional vector?,Is it reasonable to think of the expectation of an infinite-dimensional vector?,,"Given a probability space $(\Omega, \mathcal{F}, P)$, a random vector is an $\mathcal{F}$-measurable mapping $X: \Omega \rightarrow \mathbb{R}^{k}: X(\omega) = (X_{1}(\omega), X_{2}(\omega),\ldots,X_{k}(\omega))$ for finite $k$ and the expectation of $X$ can be defined as $E(X) = (E(X_{1}), E(X_{2}),\ldots,E(X_{k}))$. Now suppose $X: \Omega \rightarrow l_{p}$ is $\mathcal{F}$--measurable. Does it make sense to define the expectation of $X$ as $E(X) = (E(X_{1}), E(X_{2}),\ldots) \in l_{p}$? Are there any infinite-dimensional oddities that are going to occur when thinking about expectation of a random element of a $l_{p}$ in this way? What about if one wants to think about conditional expectation? Also, if the above definition does actually make sense, can you provide a reference or a field/subfield where you have seen it used so I can read more about it?","Given a probability space $(\Omega, \mathcal{F}, P)$, a random vector is an $\mathcal{F}$-measurable mapping $X: \Omega \rightarrow \mathbb{R}^{k}: X(\omega) = (X_{1}(\omega), X_{2}(\omega),\ldots,X_{k}(\omega))$ for finite $k$ and the expectation of $X$ can be defined as $E(X) = (E(X_{1}), E(X_{2}),\ldots,E(X_{k}))$. Now suppose $X: \Omega \rightarrow l_{p}$ is $\mathcal{F}$--measurable. Does it make sense to define the expectation of $X$ as $E(X) = (E(X_{1}), E(X_{2}),\ldots) \in l_{p}$? Are there any infinite-dimensional oddities that are going to occur when thinking about expectation of a random element of a $l_{p}$ in this way? What about if one wants to think about conditional expectation? Also, if the above definition does actually make sense, can you provide a reference or a field/subfield where you have seen it used so I can read more about it?",,"['probability', 'functional-analysis', 'probability-theory']"
70,95% confidence interval around sum of random variables,95% confidence interval around sum of random variables,,"Suppose I have two random variables, $X$ and $Y$.  Suppose $X$ is normally distributed, and therefore I know how to compute a 95% confidence interval (CI) estimator for $X$.  Suppose that $Y$ is not normally distributed, but that I have an unbiased 95% CI estimator for $Y$. Given that I know how to compute CIs for $X$ and $Y$ separately, how can I compute a 95% CI estimator for the quantity $$W = a \cdot X + b \cdot Y,$$ where $a$ and $b$ are real constants?","Suppose I have two random variables, $X$ and $Y$.  Suppose $X$ is normally distributed, and therefore I know how to compute a 95% confidence interval (CI) estimator for $X$.  Suppose that $Y$ is not normally distributed, but that I have an unbiased 95% CI estimator for $Y$. Given that I know how to compute CIs for $X$ and $Y$ separately, how can I compute a 95% CI estimator for the quantity $$W = a \cdot X + b \cdot Y,$$ where $a$ and $b$ are real constants?",,"['probability', 'statistics', 'estimation']"
71,Two Pairs Poker Hand Probability,Two Pairs Poker Hand Probability,,"The probability of getting a hand with two pairs in poker is $C^{13}_2 \cdot C^4_2 \cdot C^4_2 \cdot 11 \cdot C^4_1.$ When I first started calculating the probability, I thought it was: $$\binom{13}{1} \times \binom{4}{2} \times \binom{12}{1} \times \binom{4}{2} \times \binom{11}{1} \times \binom{4}{1}$$ Would someone please explain why my first thought is wrong?  Since in the second part, you have already picked a pair, and there are only 12 ranks left to choose from to make the second pair.","The probability of getting a hand with two pairs in poker is $C^{13}_2 \cdot C^4_2 \cdot C^4_2 \cdot 11 \cdot C^4_1.$ When I first started calculating the probability, I thought it was: $$\binom{13}{1} \times \binom{4}{2} \times \binom{12}{1} \times \binom{4}{2} \times \binom{11}{1} \times \binom{4}{1}$$ Would someone please explain why my first thought is wrong?  Since in the second part, you have already picked a pair, and there are only 12 ranks left to choose from to make the second pair.",,['probability']
72,A question on Regular Conditional Probability,A question on Regular Conditional Probability,,"Let $X$ & $Y$ be random vectors. Let $Z$=$f(X,Y)$ be a random variable. Then if $X$ & $Y$ are independent, the following is a well known result: $P(f(X,Y)\in B\mid Y=y)$  = $P(f(X,y)\in B)$ I need some help to 'formalize' what appears to be an intuitive generalization when $X$ & $Y$ are not assumed to be independent, namely that $P(f(X,Y)\in B\mid Y=y)$  = $P(f(X,y)\in B\mid Y=y)$ without assuming $Y$ is discrete. One 'immediate' problem for example is that, I am not sure that the RHS of the above equation is well defined. For in Breiman's text 'Probability' , by Definition 4.7 ,  the conditional probability $P(C\mid Y=y)$ is defined as a measurable function in $y$ satisfying $P(C, Y\in A )$=$\int_{A} P( C\mid Y=y) \, P_{Y}(dy)$ where $C$ is a measurable subset of the sample space and $A$ is an arbitrary Borel subset in the state space of $Y$. Notice that $C$ is held fixed in the definition, as $y$ is allowed to vary. Identifying $C$ with$[f(X,Y)\in B]$ in the LHS of our conjecture , allows us to make sense of $P(f(X,Y)\in B\mid Y=y)$. But the subset $[f(X,y)\in B]$ changes with changes in $y$, hence at least by the above definition $P(f(X,y)\in B\mid Y=y)$ is not well defined. As an aside, for similar reasons, $E(f(X,y)\in B\mid Y=y)$ is also problematic. Using the concept of Regular Conditional Probability, I think the statement above can be formalized as follows. Let $X$ & $Y$ be random vectors. Let $Z$=$f(X,Y)$ be a random variable, and $P_{Y}$ be the marginal distribution of $Y$. Let $Q_{X\mid Y}(\cdot\mid y)$ be a regular conditional distribution for $X$ given $Y=y$. Then $P(f(X,Y)\in B\mid Y=y)$  = $P(f(X,y)\in B\mid Y=y)$, is I believe an intuitive interpretation of the following 'unproven' statement $Q_{Z\mid Y}( B\mid y)$= $Q_{X\mid Y}(B_y \mid y)$ where $B$ is any arbitrary Borel subset of $R$, and  $B_y$ = $[x \mid f(x,y) \in B]$ and $Q_{Z\mid Y}(\cdot\mid y)$ the regular conditional distribution for $Z=f(X,Y)$ given $Y=y$. Written alternatively we need to prove that $P(Z\in B, Y\in A )$=$\int_{A} Q_{Z\mid Y}( B\mid y) \, P_{Y}(dy)$=$\int_{A} Q_{X\mid Y}( B_y\mid y) \, P_{Y}(dy)$ A related problem is to prove the following: Let $P_{(X,Y)}$ be the joint distribution of $(X,Y)$, $Q_{X\mid Y}(\cdot\mid y)$ be a regular conditional distribution for $X$ given $Y=y$, $P_{Y}$ be the marginal distribution of $Y$, and let $D$ be a given Borel subset in the state space of $(X,Y)$. Then $P_{(X,Y)}(D )$=$\int Q_{X\mid Y}( D_y\mid y) \, P_{Y}(dy)$ where $D_y$ = $[x \mid (x,y) \in D]$","Let $X$ & $Y$ be random vectors. Let $Z$=$f(X,Y)$ be a random variable. Then if $X$ & $Y$ are independent, the following is a well known result: $P(f(X,Y)\in B\mid Y=y)$  = $P(f(X,y)\in B)$ I need some help to 'formalize' what appears to be an intuitive generalization when $X$ & $Y$ are not assumed to be independent, namely that $P(f(X,Y)\in B\mid Y=y)$  = $P(f(X,y)\in B\mid Y=y)$ without assuming $Y$ is discrete. One 'immediate' problem for example is that, I am not sure that the RHS of the above equation is well defined. For in Breiman's text 'Probability' , by Definition 4.7 ,  the conditional probability $P(C\mid Y=y)$ is defined as a measurable function in $y$ satisfying $P(C, Y\in A )$=$\int_{A} P( C\mid Y=y) \, P_{Y}(dy)$ where $C$ is a measurable subset of the sample space and $A$ is an arbitrary Borel subset in the state space of $Y$. Notice that $C$ is held fixed in the definition, as $y$ is allowed to vary. Identifying $C$ with$[f(X,Y)\in B]$ in the LHS of our conjecture , allows us to make sense of $P(f(X,Y)\in B\mid Y=y)$. But the subset $[f(X,y)\in B]$ changes with changes in $y$, hence at least by the above definition $P(f(X,y)\in B\mid Y=y)$ is not well defined. As an aside, for similar reasons, $E(f(X,y)\in B\mid Y=y)$ is also problematic. Using the concept of Regular Conditional Probability, I think the statement above can be formalized as follows. Let $X$ & $Y$ be random vectors. Let $Z$=$f(X,Y)$ be a random variable, and $P_{Y}$ be the marginal distribution of $Y$. Let $Q_{X\mid Y}(\cdot\mid y)$ be a regular conditional distribution for $X$ given $Y=y$. Then $P(f(X,Y)\in B\mid Y=y)$  = $P(f(X,y)\in B\mid Y=y)$, is I believe an intuitive interpretation of the following 'unproven' statement $Q_{Z\mid Y}( B\mid y)$= $Q_{X\mid Y}(B_y \mid y)$ where $B$ is any arbitrary Borel subset of $R$, and  $B_y$ = $[x \mid f(x,y) \in B]$ and $Q_{Z\mid Y}(\cdot\mid y)$ the regular conditional distribution for $Z=f(X,Y)$ given $Y=y$. Written alternatively we need to prove that $P(Z\in B, Y\in A )$=$\int_{A} Q_{Z\mid Y}( B\mid y) \, P_{Y}(dy)$=$\int_{A} Q_{X\mid Y}( B_y\mid y) \, P_{Y}(dy)$ A related problem is to prove the following: Let $P_{(X,Y)}$ be the joint distribution of $(X,Y)$, $Q_{X\mid Y}(\cdot\mid y)$ be a regular conditional distribution for $X$ given $Y=y$, $P_{Y}$ be the marginal distribution of $Y$, and let $D$ be a given Borel subset in the state space of $(X,Y)$. Then $P_{(X,Y)}(D )$=$\int Q_{X\mid Y}( D_y\mid y) \, P_{Y}(dy)$ where $D_y$ = $[x \mid (x,y) \in D]$",,"['probability', 'probability-theory', 'conditional-probability']"
73,A tight lower bound for the entropy of the XOR of two random variables,A tight lower bound for the entropy of the XOR of two random variables,,"Let $U$ be the uniform random variable over $n$-bit binary strings, and let $X$ be another random variable that is dependent on $U$ and ranges over $n$-bit binary strings. Assuming $I(X;U) \le \epsilon$, can we find a tight lower bound on $H(X \oplus U)$? For instance, can we prove something like $H(X \oplus U) \ge n - \epsilon$? P.S.: The mutual information and entropy are denoted by $I$ and $H$, and $\oplus$ denotes the XOR operator.","Let $U$ be the uniform random variable over $n$-bit binary strings, and let $X$ be another random variable that is dependent on $U$ and ranges over $n$-bit binary strings. Assuming $I(X;U) \le \epsilon$, can we find a tight lower bound on $H(X \oplus U)$? For instance, can we prove something like $H(X \oplus U) \ge n - \epsilon$? P.S.: The mutual information and entropy are denoted by $I$ and $H$, and $\oplus$ denotes the XOR operator.",,"['probability', 'inequality', 'random-variables', 'information-theory', 'entropy']"
74,Potentials in Probability Theory,Potentials in Probability Theory,,Could someone give an intuitive interpretation of potentials in the field of probability theory. How do they link to the theory of stochastic processes. And maybe link this with SEP. References are appreciated as well. Thank you very much.,Could someone give an intuitive interpretation of potentials in the field of probability theory. How do they link to the theory of stochastic processes. And maybe link this with SEP. References are appreciated as well. Thank you very much.,,"['probability', 'probability-theory', 'stochastic-processes', 'potential-theory']"
75,The law of large numbers with dependent random variable,The law of large numbers with dependent random variable,,"Consider a sequence of i.i.d. random variables $\left\{X_i\right\}_i$, and let $Y$ be another random variable. Can we say something regard the convergence of the following series $$ \frac{1}{n}\sum_{i=1}^nf\left(X_iY\right) $$  as $n\to\infty$ (assume that $f$ is some ""nice"" function) ? If $Y$ was not there, then we obviously could use the law of large numbers to claim that the above sum converge in probability to $E(f(X))$. But, with $Y$, the summands are dependent and thus we cannot directly apply the law of large numbers, right? EDIT : The answer to this question is as follows (credit to Did): If one assumes that $Y$ is independent on the sequence $(X_i)$, then it can be shown that the series converges a.s. to the conditional expectation $E(f(X_1Y)\mid Y)$. This can be shown by using backwards martingale convergence theorem (actually a refined proof of the SLLN using the backwards martingale convergence theorem). If, however, $Y$ do depend on the the sequence $(X_i)$, then there is nothing much to say (in general).","Consider a sequence of i.i.d. random variables $\left\{X_i\right\}_i$, and let $Y$ be another random variable. Can we say something regard the convergence of the following series $$ \frac{1}{n}\sum_{i=1}^nf\left(X_iY\right) $$  as $n\to\infty$ (assume that $f$ is some ""nice"" function) ? If $Y$ was not there, then we obviously could use the law of large numbers to claim that the above sum converge in probability to $E(f(X))$. But, with $Y$, the summands are dependent and thus we cannot directly apply the law of large numbers, right? EDIT : The answer to this question is as follows (credit to Did): If one assumes that $Y$ is independent on the sequence $(X_i)$, then it can be shown that the series converges a.s. to the conditional expectation $E(f(X_1Y)\mid Y)$. This can be shown by using backwards martingale convergence theorem (actually a refined proof of the SLLN using the backwards martingale convergence theorem). If, however, $Y$ do depend on the the sequence $(X_i)$, then there is nothing much to say (in general).",,"['probability', 'probability-theory', 'probability-limit-theorems', 'law-of-large-numbers']"
76,Looking for first course textbooks on probability and statistics for math majors,Looking for first course textbooks on probability and statistics for math majors,,"I am taking a probability and statistics course soon and would like to find a text book that is targeted more towards math majors rather than engineers (which is what this class is). The book my class will officially be using is Probability and Statistics for Engineering and the Sciences by Devore (ISBN-13: 978-0538733526). To be clear I have not seen the book yet but for now I am assuming it is not what I am looking for. To give an idea of mathematical maturity and (possibly) relevant background by the time I plan to really start looking through whatever book I end up getting I will be through the integration chapter of Rudin's Principles (ch6), be through a course of point set topology (Topology and Groupoids), and I have familiarity with Linear Algebra (I have been learning from Course of linear algebra and multidimensional geometry by Ruslan Sharipov and getting exercises from various books). I don't have any real specification other than it doesn't focus on ad-hoc calculation skills and has a good amount of theory. Also the book should be a first course book, so it shouldn't assume previous probably and statistics classes. Also the book could be probability or statistics(not necessarily both).  Here are some things that would be nice to have but not necessary:  a book that covers a lot of the material from the book that will be used in the class; a book that has special topics or applications to things like number theory or combinatorics (basically anything separate from probability and statistics); not greater than 500 pages; reasonably priced. There are already many questions that have asked for probability or statistics books, but it is not obvious from the questions and answers (that I have seen) that any of those books fit what I am looking for. It looks like this answer might have what I am looking for but it is hard to tell and its expensive. Edit:  What am looking for is not necessarily a replacement for this class, I just figure that I might as well learn some probability or statistics at a more sophisticated level while I am taking the required class that I suspect is more focused on calculations.","I am taking a probability and statistics course soon and would like to find a text book that is targeted more towards math majors rather than engineers (which is what this class is). The book my class will officially be using is Probability and Statistics for Engineering and the Sciences by Devore (ISBN-13: 978-0538733526). To be clear I have not seen the book yet but for now I am assuming it is not what I am looking for. To give an idea of mathematical maturity and (possibly) relevant background by the time I plan to really start looking through whatever book I end up getting I will be through the integration chapter of Rudin's Principles (ch6), be through a course of point set topology (Topology and Groupoids), and I have familiarity with Linear Algebra (I have been learning from Course of linear algebra and multidimensional geometry by Ruslan Sharipov and getting exercises from various books). I don't have any real specification other than it doesn't focus on ad-hoc calculation skills and has a good amount of theory. Also the book should be a first course book, so it shouldn't assume previous probably and statistics classes. Also the book could be probability or statistics(not necessarily both).  Here are some things that would be nice to have but not necessary:  a book that covers a lot of the material from the book that will be used in the class; a book that has special topics or applications to things like number theory or combinatorics (basically anything separate from probability and statistics); not greater than 500 pages; reasonably priced. There are already many questions that have asked for probability or statistics books, but it is not obvious from the questions and answers (that I have seen) that any of those books fit what I am looking for. It looks like this answer might have what I am looking for but it is hard to tell and its expensive. Edit:  What am looking for is not necessarily a replacement for this class, I just figure that I might as well learn some probability or statistics at a more sophisticated level while I am taking the required class that I suspect is more focused on calculations.",,"['probability', 'statistics']"
77,A equivalent definition of the Feller Process.,A equivalent definition of the Feller Process.,,"I saw this on Liggett's Book (P.95). Let $S=% %TCIMACRO{\U{2115} }% %BeginExpansion \mathbb{N} %EndExpansion ,$ and suppose $\left( X_{t}\right) _{t\geq 0}$ is a continuous-time Markov process with state space $S$ and transition function $\left( p_{t}\right) _{t\geq 0}.$ Show that $\left( X_{t}\right) _{t\geq 0}$ is a Feller process if and only if $$ \lim_{t\downarrow 0}p\left( x,\left\{ x\right\} \right) =1\text{ for all }% x\in  %TCIMACRO{\U{2115} }% %BeginExpansion \mathbb{N} %EndExpansion $$ and $$ \lim_{x\rightarrow \infty }p_{t}\left( x,\left\{ y\right\} \right) =0\text{ for all }y\in  %TCIMACRO{\U{2115} }% %BeginExpansion \mathbb{N} %EndExpansion \text{ and }t>0. $$","I saw this on Liggett's Book (P.95). Let $S=% %TCIMACRO{\U{2115} }% %BeginExpansion \mathbb{N} %EndExpansion ,$ and suppose $\left( X_{t}\right) _{t\geq 0}$ is a continuous-time Markov process with state space $S$ and transition function $\left( p_{t}\right) _{t\geq 0}.$ Show that $\left( X_{t}\right) _{t\geq 0}$ is a Feller process if and only if $$ \lim_{t\downarrow 0}p\left( x,\left\{ x\right\} \right) =1\text{ for all }% x\in  %TCIMACRO{\U{2115} }% %BeginExpansion \mathbb{N} %EndExpansion $$ and $$ \lim_{x\rightarrow \infty }p_{t}\left( x,\left\{ y\right\} \right) =0\text{ for all }y\in  %TCIMACRO{\U{2115} }% %BeginExpansion \mathbb{N} %EndExpansion \text{ and }t>0. $$",,"['probability', 'statistics', 'probability-theory', 'stochastic-processes', 'markov-process']"
78,Minimum probability of biased coin to satisfy particular condition,Minimum probability of biased coin to satisfy particular condition,,"Take $n$ pairs of integers $(x_i,y_i)$,  $1\leq i\leq n$, selected independently as follows: Toss a fair coin $X$ and a biased coin $Y$ with $$\Pr[Y=\text{heads}] = p \neq 1$$ If $(X=\text{heads})$, then $x_i=-1$, otherwise $x_i=1$ If $(Y=\text{heads})$, then $y_i=0$, otherwise $y_i=1$ Find $\min(p)$ such that $\Pr[\sum_{i=1}^{n}x_iy_i=0]\geq a$ for some given $a$. (Note: not a homework question)","Take $n$ pairs of integers $(x_i,y_i)$,  $1\leq i\leq n$, selected independently as follows: Toss a fair coin $X$ and a biased coin $Y$ with $$\Pr[Y=\text{heads}] = p \neq 1$$ If $(X=\text{heads})$, then $x_i=-1$, otherwise $x_i=1$ If $(Y=\text{heads})$, then $y_i=0$, otherwise $y_i=1$ Find $\min(p)$ such that $\Pr[\sum_{i=1}^{n}x_iy_i=0]\geq a$ for some given $a$. (Note: not a homework question)",,['probability']
79,hint with Bayes rule problem,hint with Bayes rule problem,,"The pirate Captain Queequeg has a lazy crew and suspects they are planning to stage a mutiny. Captain Queequeg's solution is to have every member of the crew roll Queequeg's lucky die. If the roll is even, the crew member must walk the plank. If the roll is odd, the crew member must take a swig of truth serum and then reveal his or her true intent. If the crew member was planning on mutiny, he or she must walk the plank, and if not, is allowed to remain on the ship. Queequeg's die has been cursed so that evens are rolled 61% of the time. Crew members are either lazy or mutinous but not both. Of the crew members who take the serum, 18% are forced to reveal they were planning mutiny. 1) What is the chance a crew member will walk the plank, given he or she is mutinous? 2) What is the chance a crew member will walk the plank, given he or she is just plain lazy (but not mutinous)? 3) What is the chance a crew member is mutinous, given he or she walked the plank? This is what I have so far: The probability of walking the plank seems to be $P(\text{plank}) = P(\text{mutinous} \cap \text{odd})+P(\text{lazy} \cap \text{even})$ $P(\text{plank}) = P(\text{mutinous})P(\text{odd})+P(\text{lazy})P(\text{even})$ Rolling the die is independent of being mutinous or lazy. $P(\text{lazy})= P(\lnot \text{mutinous})$ 1) I tried applying Bayes rule, but then I need the posterior probability from part 3) first. $$P(\text{plank} \mid \text{mutinous}) = {P(\text{mutinous} \mid \text{plank})P(\text{plank}) \over P(mutinous)}$$ Expanding the likelihood using total probability $$P(\text{plank} \mid \text{mutinous}) = {P(\text{mutinous} \mid \text{plank})P(\text{plank}) \over P(\text{mutinous} \mid \text{plank})P(\text{plank}) + P(\text{mutinous} \mid \lnot \text{plank})P(\lnot \text{plank})}$$ Using conditional probability, substituting the joint probability for the posterior and prior $$P(\text{plank}\mid\text{mutinous}) = {P(\text{mutinous} \cap \text{plank}) \over P(\text{mutinous}\mid\text{plank})P(\text{plank}) + P(\text{mutinous}\mid\lnot\text{plank})P(\lnot\text{plank})}$$ Being mutinous and walking the plank aren't independent events, so I am not sure where to go from here.","The pirate Captain Queequeg has a lazy crew and suspects they are planning to stage a mutiny. Captain Queequeg's solution is to have every member of the crew roll Queequeg's lucky die. If the roll is even, the crew member must walk the plank. If the roll is odd, the crew member must take a swig of truth serum and then reveal his or her true intent. If the crew member was planning on mutiny, he or she must walk the plank, and if not, is allowed to remain on the ship. Queequeg's die has been cursed so that evens are rolled 61% of the time. Crew members are either lazy or mutinous but not both. Of the crew members who take the serum, 18% are forced to reveal they were planning mutiny. 1) What is the chance a crew member will walk the plank, given he or she is mutinous? 2) What is the chance a crew member will walk the plank, given he or she is just plain lazy (but not mutinous)? 3) What is the chance a crew member is mutinous, given he or she walked the plank? This is what I have so far: The probability of walking the plank seems to be $P(\text{plank}) = P(\text{mutinous} \cap \text{odd})+P(\text{lazy} \cap \text{even})$ $P(\text{plank}) = P(\text{mutinous})P(\text{odd})+P(\text{lazy})P(\text{even})$ Rolling the die is independent of being mutinous or lazy. $P(\text{lazy})= P(\lnot \text{mutinous})$ 1) I tried applying Bayes rule, but then I need the posterior probability from part 3) first. $$P(\text{plank} \mid \text{mutinous}) = {P(\text{mutinous} \mid \text{plank})P(\text{plank}) \over P(mutinous)}$$ Expanding the likelihood using total probability $$P(\text{plank} \mid \text{mutinous}) = {P(\text{mutinous} \mid \text{plank})P(\text{plank}) \over P(\text{mutinous} \mid \text{plank})P(\text{plank}) + P(\text{mutinous} \mid \lnot \text{plank})P(\lnot \text{plank})}$$ Using conditional probability, substituting the joint probability for the posterior and prior $$P(\text{plank}\mid\text{mutinous}) = {P(\text{mutinous} \cap \text{plank}) \over P(\text{mutinous}\mid\text{plank})P(\text{plank}) + P(\text{mutinous}\mid\lnot\text{plank})P(\lnot\text{plank})}$$ Being mutinous and walking the plank aren't independent events, so I am not sure where to go from here.",,"['probability', 'bayesian']"
80,What is the expected length of the longest increasing subsequence of a random permutation of the first n natural numbers?,What is the expected length of the longest increasing subsequence of a random permutation of the first n natural numbers?,,What is the average length of the longest increasing subsequence of a random permutation of the first n natural numbers?,What is the average length of the longest increasing subsequence of a random permutation of the first n natural numbers?,,"['probability', 'combinatorics']"
81,stochastic dominance,stochastic dominance,,"A group of people are choosing between two investments A and B. Both   have these payoff distributions: A: $$\langle.2, .1, .2, .4, .1 \mid 1, 2, 3, 4, 5\rangle$$ B: $$\langle.1, .3, .1, .3, .2 \mid 1, 2, 3, 4, 5\rangle$$ (IE for A, there is a .2 chance of getting 1, .1 chance of getting 2   etc. Sorry if my notation is bad). All the people are risk averse but may have different utility   functions. Can we say anything about any of the projects being   unanimously preferred using notions of first and second order   stochastic dominance? I believe I understand the basics of what first and second order stochastic dominance means but I'm having trouble applying it to this question. Please help on how one might approach this type of problem. Thanks","A group of people are choosing between two investments A and B. Both   have these payoff distributions: A: $$\langle.2, .1, .2, .4, .1 \mid 1, 2, 3, 4, 5\rangle$$ B: $$\langle.1, .3, .1, .3, .2 \mid 1, 2, 3, 4, 5\rangle$$ (IE for A, there is a .2 chance of getting 1, .1 chance of getting 2   etc. Sorry if my notation is bad). All the people are risk averse but may have different utility   functions. Can we say anything about any of the projects being   unanimously preferred using notions of first and second order   stochastic dominance? I believe I understand the basics of what first and second order stochastic dominance means but I'm having trouble applying it to this question. Please help on how one might approach this type of problem. Thanks",,"['probability', 'utility']"
82,Calculation of an expectation for the 'part' of a vector,Calculation of an expectation for the 'part' of a vector,,"Let $x$ be vector in $R^n$. Let $\pi(⋅)$  be a permutation on the set $\{1,\ldots,n\}$  with a uniform distribution.  Let $|m|\leq n, m \in Z$. Calculate $$ E\left|\sum_{i=1}^mx_{\pi(i)}\right|^q, \quad q\geq 2. $$ Thank you.","Let $x$ be vector in $R^n$. Let $\pi(⋅)$  be a permutation on the set $\{1,\ldots,n\}$  with a uniform distribution.  Let $|m|\leq n, m \in Z$. Calculate $$ E\left|\sum_{i=1}^mx_{\pi(i)}\right|^q, \quad q\geq 2. $$ Thank you.",,"['probability', 'statistics', 'probability-theory']"
83,Question on Conditional expectation [closed],Question on Conditional expectation [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question Let $X_1$ and $X_2$ be two random variables on $(\Omega,\mathcal{B},P)$. Suppose there is a function $g:\mathcal{B}\times\mathbb{R}\rightarrow[0,1]$ such that for any $x$, $g(\cdot,x)$ is a probability distribution over $\Omega$ and for any $B\in\mathcal{B}$, $P(X_2\in B|X_1)=g(B,X_1)$ a.s.. Then is it true that, for any measurable function $f$, \begin{align} \mathbb{E}[f(X_2)|X_1]=\mathbb{E}_{X_1}[f(X')], \end{align} where $\mathbb{E}_{x}$ denotes the expectation when $X'\sim g(\cdot,x)$? I think that the statement is true when $X_1$ and $X_2$ take values in a countable space. If it is not true in general, is $\mathbb{E}_{x}f(X')$ at-least a measurable function from $\mathbb{R}\rightarrow\mathbb{R}$?","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question Let $X_1$ and $X_2$ be two random variables on $(\Omega,\mathcal{B},P)$. Suppose there is a function $g:\mathcal{B}\times\mathbb{R}\rightarrow[0,1]$ such that for any $x$, $g(\cdot,x)$ is a probability distribution over $\Omega$ and for any $B\in\mathcal{B}$, $P(X_2\in B|X_1)=g(B,X_1)$ a.s.. Then is it true that, for any measurable function $f$, \begin{align} \mathbb{E}[f(X_2)|X_1]=\mathbb{E}_{X_1}[f(X')], \end{align} where $\mathbb{E}_{x}$ denotes the expectation when $X'\sim g(\cdot,x)$? I think that the statement is true when $X_1$ and $X_2$ take values in a countable space. If it is not true in general, is $\mathbb{E}_{x}f(X')$ at-least a measurable function from $\mathbb{R}\rightarrow\mathbb{R}$?",,"['probability', 'probability-theory', 'markov-process']"
84,Bayes analysis as used in a Presidential Election - with calculations shown,Bayes analysis as used in a Presidential Election - with calculations shown,,"Can you show all the steps needed for a Bayesian probability progression as new information is received.  As an example, initial estimate for Obama to win (popular vote) is 55%.  In state-A, Democrats usually get 51.5% of the vote but receive 57% of the vote after 10% of the ballots have been counted.  In state-B,Democrats receive 48% 0f the vote after 5% of of the ballots have been counted. 3 or 4 new pieces of data come in from other states (your choice of numbers). What I am interested in are the calculations one goes through to get the final probability estimate. I want to display this to 15 - 16 year old high school students. Thank you for any help that you can give me.","Can you show all the steps needed for a Bayesian probability progression as new information is received.  As an example, initial estimate for Obama to win (popular vote) is 55%.  In state-A, Democrats usually get 51.5% of the vote but receive 57% of the vote after 10% of the ballots have been counted.  In state-B,Democrats receive 48% 0f the vote after 5% of of the ballots have been counted. 3 or 4 new pieces of data come in from other states (your choice of numbers). What I am interested in are the calculations one goes through to get the final probability estimate. I want to display this to 15 - 16 year old high school students. Thank you for any help that you can give me.",,"['probability', 'education']"
85,Probability distribution for finding two values in stages,Probability distribution for finding two values in stages,,"Fix two arbitrary distinct values from $\{1,\dots,n\}$. Say for concreteness we choose the values $1$ and $2$. We sample uniformly with replacement from $\{1,\dots,n\}$ in a number of stages. Each stage runs until we either sample the same element twice (a duplicate) within the same stage , or we find both of the two values within the same stage . If the stage ends because we sampled the same element twice we just start again with a new stage. I would like to know the probability distribution for the number of samples we need to have in total before the two values are found within the same stage.","Fix two arbitrary distinct values from $\{1,\dots,n\}$. Say for concreteness we choose the values $1$ and $2$. We sample uniformly with replacement from $\{1,\dots,n\}$ in a number of stages. Each stage runs until we either sample the same element twice (a duplicate) within the same stage , or we find both of the two values within the same stage . If the stage ends because we sampled the same element twice we just start again with a new stage. I would like to know the probability distribution for the number of samples we need to have in total before the two values are found within the same stage.",,['probability']
86,Hellinger distance between Gaussians - multivariate and univariate forms,Hellinger distance between Gaussians - multivariate and univariate forms,,"On pages 46 and 51 of the book Statistical Inference based on divergence measures By Leandro Pardo Llorente there is a derivation for the Hellinger distance between two multivariate Gaussian distributions (the sample pages can be viewed on google books ). If  vectors of length 1 are used with this form and both distributions assumed to have zero means, then the exponential portion of the hellinger distance goes to unity. However this does not give the same result as the Hellinger distance between two univariate Gaussians under the same assumptions. Shouldn't these two forms be consistent under these assumptions? (with $u=(\mu_1-\mu_2)$ )","On pages 46 and 51 of the book Statistical Inference based on divergence measures By Leandro Pardo Llorente there is a derivation for the Hellinger distance between two multivariate Gaussian distributions (the sample pages can be viewed on google books ). If  vectors of length 1 are used with this form and both distributions assumed to have zero means, then the exponential portion of the hellinger distance goes to unity. However this does not give the same result as the Hellinger distance between two univariate Gaussians under the same assumptions. Shouldn't these two forms be consistent under these assumptions? (with )",u=(\mu_1-\mu_2),"['probability', 'probability-theory', 'probability-distributions']"
87,Concentration inequality for independent identically distributed variables,Concentration inequality for independent identically distributed variables,,"Let $x_1,..., x_n$ be independent identically distributed variables with means $M$ and variances $V$. Let $$\bar{x}=\frac{1}{n}\sum_{i=1}^{n}x_i.$$ Then we can say that: $$\mathbb{P}(|\bar{x}-M|>\varepsilon)\leq\frac{V}{n\varepsilon^2}. $$ Is this bound sharp for these conditions? If it is true can you hint me of an example that shows that it can't be tightened? Thanks!","Let $x_1,..., x_n$ be independent identically distributed variables with means $M$ and variances $V$. Let $$\bar{x}=\frac{1}{n}\sum_{i=1}^{n}x_i.$$ Then we can say that: $$\mathbb{P}(|\bar{x}-M|>\varepsilon)\leq\frac{V}{n\varepsilon^2}. $$ Is this bound sharp for these conditions? If it is true can you hint me of an example that shows that it can't be tightened? Thanks!",,"['probability', 'probability-theory']"
88,Median from probability generating function,Median from probability generating function,,"How can I find a median, or an approximation of a median, from the probability generating function?","How can I find a median, or an approximation of a median, from the probability generating function?",,['probability']
89,Probability that a random weight function on $K_n$ satisfies the triangle inequality,Probability that a random weight function on  satisfies the triangle inequality,K_n,"On a complete graph $K_n$, every edge is assigned a random real weight in $[0, 1]$. I am trying to calculate the probability that the weights satisfy the triangle inequality or even bounds on this probability. How about the discrete version where the weights are integers in $[0, k]$? EDIT : the question was asked and answered here .","On a complete graph $K_n$, every edge is assigned a random real weight in $[0, 1]$. I am trying to calculate the probability that the weights satisfy the triangle inequality or even bounds on this probability. How about the discrete version where the weights are integers in $[0, k]$? EDIT : the question was asked and answered here .",,"['probability', 'graph-theory']"
90,Singular measures - approximate characteristic function,Singular measures - approximate characteristic function,,"One can decompose a $\sigma$-finite measure $\mu$ on $\mathbb{R}$ in three parts: $\mu_{ac}$: absolutely continuous $\mu_{sc}$: singular continuous $\mu_{pp}$: pure point A common example for a singular continuous probability measure is Cantor's function as cdf. Such a cdf is continuous. I have two question: (1) Do singular cont. probability measures come up? E.g. as law of pure jump L\'evy processes, for which there are criteria to guarantee a density. What about semimartingales? (2) The characteristic function can be defined in two ways $\int e^{itx}dF(x)$ or $\int e^{itx}d\mu_{sc}(x)$. Though there is no density function, can $\int e^{itx}dF(x)$ be approximated by a sequence of, say a continuous $\mathcal{L}^1$ functions, such that $\int e^{itx}dF(x) = \lim_{n\rightarrow\infty}\int e^{itx}f_n(x)dx$? I can't find anything on that, so maybe singular measures are quite opaque objects in probability.","One can decompose a $\sigma$-finite measure $\mu$ on $\mathbb{R}$ in three parts: $\mu_{ac}$: absolutely continuous $\mu_{sc}$: singular continuous $\mu_{pp}$: pure point A common example for a singular continuous probability measure is Cantor's function as cdf. Such a cdf is continuous. I have two question: (1) Do singular cont. probability measures come up? E.g. as law of pure jump L\'evy processes, for which there are criteria to guarantee a density. What about semimartingales? (2) The characteristic function can be defined in two ways $\int e^{itx}dF(x)$ or $\int e^{itx}d\mu_{sc}(x)$. Though there is no density function, can $\int e^{itx}dF(x)$ be approximated by a sequence of, say a continuous $\mathcal{L}^1$ functions, such that $\int e^{itx}dF(x) = \lim_{n\rightarrow\infty}\int e^{itx}f_n(x)dx$? I can't find anything on that, so maybe singular measures are quite opaque objects in probability.",,"['probability', 'probability-theory', 'fourier-analysis', 'singular-measures']"
91,Intuition for Prokhorov metric and metrization of weak convergence,Intuition for Prokhorov metric and metrization of weak convergence,,"According to Billingsley, let $P$ and $Q$ be two probability measures. Then the Prokhorov metric $\pi (P,Q)$ is the infimum of those positive $ \epsilon $ for which the two inequalities $PA \le QA^{\epsilon}+ \epsilon$ and $QA \le PA^{\epsilon}+ \epsilon$ for all Borel sets $A$ . Then theorem 6.8 says that if the set on which probability measures is separable and complete, weak convergence is equivalent to $ \pi $ convergence. Then, Question: what is an intuitive explanation that the Prokhorov metric metrizes weak convergence? How can I explain it using an example?","According to Billingsley, let and be two probability measures. Then the Prokhorov metric is the infimum of those positive for which the two inequalities and for all Borel sets . Then theorem 6.8 says that if the set on which probability measures is separable and complete, weak convergence is equivalent to convergence. Then, Question: what is an intuitive explanation that the Prokhorov metric metrizes weak convergence? How can I explain it using an example?","P Q \pi (P,Q)  \epsilon  PA \le QA^{\epsilon}+ \epsilon QA \le PA^{\epsilon}+ \epsilon A  \pi ",['probability']
92,"If $X, Y \sim N(0,1)$, find the CDF of $\alpha X + \beta Y$ [duplicate]","If , find the CDF of  [duplicate]","X, Y \sim N(0,1) \alpha X + \beta Y","This question already has answers here : Closed 12 years ago . Possible Duplicate: Proof that the sum of two Gaussian variables is another Gaussian Let $X,Y$ be independent normally distributed $N(0,1)$ random variable, and $\alpha,\beta\in \mathbb{R}$. What is the cumulative distribution function of $\alpha X+\beta Y$? Thank you very much for your help.","This question already has answers here : Closed 12 years ago . Possible Duplicate: Proof that the sum of two Gaussian variables is another Gaussian Let $X,Y$ be independent normally distributed $N(0,1)$ random variable, and $\alpha,\beta\in \mathbb{R}$. What is the cumulative distribution function of $\alpha X+\beta Y$? Thank you very much for your help.",,"['probability', 'probability-distributions']"
93,Probability that a random walk in $2d$ has small local time at each vertex,Probability that a random walk in  has small local time at each vertex,2d,"Let $P_{n,k}$ be the probability that a  simple random walk of length $n$ in $\mathbb{Z}^2$ is such that each vertex of $\mathbb{Z}^2$ is visited at most $k$ times by the walk. Certainly this probability decays exponentially with $n$ ,  i.e, $P_{n,k} \sim e^{- c(k) n }$ , but how fast does the exponent $c(k)$ go to zero with $k$ in the limit of large $k$ ? It is reasonable to expect that $c(k) \rightarrow 0$ as $k \rightarrow \infty$ , but I am interested in the precise asymptotic behaviour. This question is related to this other question: Recurrent random walks with bounded local time at each vertex","Let be the probability that a  simple random walk of length in is such that each vertex of is visited at most times by the walk. Certainly this probability decays exponentially with ,  i.e, , but how fast does the exponent go to zero with in the limit of large ? It is reasonable to expect that as , but I am interested in the precise asymptotic behaviour. This question is related to this other question: Recurrent random walks with bounded local time at each vertex","P_{n,k} n \mathbb{Z}^2 \mathbb{Z}^2 k n P_{n,k} \sim e^{- c(k) n } c(k) k k c(k) \rightarrow 0 k \rightarrow \infty","['probability', 'combinatorics', 'stochastic-processes', 'random-walk', 'statistical-mechanics']"
94,"Probability $P(X>Y,X>Z)$ for independent normal random variables $X$, $Y$, $Z$","Probability  for independent normal random variables , ,","P(X>Y,X>Z) X Y Z","There are several answers already given for working out the probability of one random variable being greater than another, but I can't make the leap to working out the probability of one random variable being greater than several others. My random variables are independent and normally distributed. For example: Let $X$ , $Y$ and $Z$ be independent normal random variables. What is $P(X>Y,X>Z)$ ? The obvious (to me) answer, being to just multiply the two probabilities $P(X>Y)$ and $P(X>Z)$ does not work because the difference random variables $(X-Y)$ and $(X-Z)$ are not independent. Edit For $P(X>Y)$ the answer is: $$ {\rm P}(X  > Y )  = \Phi \left(\frac{\mu_X - \mu_Y }{\sqrt{\sigma_Y^2 + \sigma_Y^2}}\right). $$ I'm hoping for a way of adding a third normal random variable to the equation. If this is possible I presume the answer can be easily expanded to add further random variables.","There are several answers already given for working out the probability of one random variable being greater than another, but I can't make the leap to working out the probability of one random variable being greater than several others. My random variables are independent and normally distributed. For example: Let , and be independent normal random variables. What is ? The obvious (to me) answer, being to just multiply the two probabilities and does not work because the difference random variables and are not independent. Edit For the answer is: I'm hoping for a way of adding a third normal random variable to the equation. If this is possible I presume the answer can be easily expanded to add further random variables.","X Y Z P(X>Y,X>Z) P(X>Y) P(X>Z) (X-Y) (X-Z) P(X>Y) 
{\rm P}(X  > Y )  = \Phi \left(\frac{\mu_X - \mu_Y }{\sqrt{\sigma_Y^2 + \sigma_Y^2}}\right).
","['probability', 'normal-distribution']"
95,Concentration inequalities for product of gaussians,Concentration inequalities for product of gaussians,,Are there any concentration inequalities (i.e. probability bounds on how a random variable deviates from its expectation) for the product of $n$ independent gaussian random variables with zero means and equal variances? What about different means and variances?,Are there any concentration inequalities (i.e. probability bounds on how a random variable deviates from its expectation) for the product of independent gaussian random variables with zero means and equal variances? What about different means and variances?,n,"['probability', 'statistics', 'normal-distribution', 'concentration-of-measure']"
96,Coin Flips and Hypothesis Tests,Coin Flips and Hypothesis Tests,,"Here's a problem I thought of that I don't know how to approach: You have a fair coin that you keep on flipping. After every flip, you perform a hypothesis test based on all coin flips thus far, with significance level $\alpha$, where your null hypothesis is that the coin is fair and your alternative hypothesis is that the coin is not fair. In terms of $\alpha$, what is the expected number of flips before the first time that you reject the null hypothesis? Edit based on comment below: For what values of $\alpha$ is the answer to the question above finite? For those values for which it is infinite, what is the probability that the null hypothesis will ever be rejected, in terms of $\alpha$? Edit 2: My post was edited to say ""You believe that you have a fair coin."" The coin is in fact fair, and you know that. You do the hypothesis tests anyway. Otherwise the problem is unapproachable because you don't know the probability that any particular toss will come up a certain way.","Here's a problem I thought of that I don't know how to approach: You have a fair coin that you keep on flipping. After every flip, you perform a hypothesis test based on all coin flips thus far, with significance level $\alpha$, where your null hypothesis is that the coin is fair and your alternative hypothesis is that the coin is not fair. In terms of $\alpha$, what is the expected number of flips before the first time that you reject the null hypothesis? Edit based on comment below: For what values of $\alpha$ is the answer to the question above finite? For those values for which it is infinite, what is the probability that the null hypothesis will ever be rejected, in terms of $\alpha$? Edit 2: My post was edited to say ""You believe that you have a fair coin."" The coin is in fact fair, and you know that. You do the hypothesis tests anyway. Otherwise the problem is unapproachable because you don't know the probability that any particular toss will come up a certain way.",,"['probability', 'statistics', 'expectation', 'hypothesis-testing']"
97,What is the probability of someone else in a room having the same birthday as me and no other people sharing a birthday?,What is the probability of someone else in a room having the same birthday as me and no other people sharing a birthday?,,"What is the probability that at least one person in a room of $n$ people (excluding me) has the same birthday as I and that nobody else in the room shares a birthday on a different date? For practical purposes, pretend that leap years don't exist. As a follow-up, what is the expected number of people that have the same birthday as I in a room of $n$ people (excluding me) if nobody else in the room shares a birthday on a different day?","What is the probability that at least one person in a room of $n$ people (excluding me) has the same birthday as I and that nobody else in the room shares a birthday on a different date? For practical purposes, pretend that leap years don't exist. As a follow-up, what is the expected number of people that have the same birthday as I in a room of $n$ people (excluding me) if nobody else in the room shares a birthday on a different day?",,"['probability', 'combinatorics']"
98,Ergodic Process: Does it visit all state?,Ergodic Process: Does it visit all state?,,"I read in this article: "" Ludwig Boltzmann, coined ""ergodic"" as the name for a stronger but related property: starting from a random point in state space, orbits will typically pass through every point in state space. It is easy to show (with set theory) that this isn't doable,..."" Could someone please explain how this can be proved. I have seen other articles that define that an ergodic process will go to every state with some probability. Second part of my question is, what is the relationship between stationary process and an ergodic process?","I read in this article: "" Ludwig Boltzmann, coined ""ergodic"" as the name for a stronger but related property: starting from a random point in state space, orbits will typically pass through every point in state space. It is easy to show (with set theory) that this isn't doable,..."" Could someone please explain how this can be proved. I have seen other articles that define that an ergodic process will go to every state with some probability. Second part of my question is, what is the relationship between stationary process and an ergodic process?",,"['probability', 'stochastic-processes', 'ergodic-theory']"
99,Probability of Poisson event occuring at least 3 times in given interval,Probability of Poisson event occuring at least 3 times in given interval,,"At a parking garage, automobiles enter at a rate of $1$ car every $2$ minutes.  I need to find the probability that the number of automobiles entering the garage during any $2$ minute period exceeds $3$. I know $\Pr(x \gt 3)=  1-\Pr(x\le 3)$. So, $1-\left(e^{-1} + e^{-1} + \frac{1}{2} e^{-1} + \frac{1}{6} e^{-1}\right) \dots$.","At a parking garage, automobiles enter at a rate of $1$ car every $2$ minutes.  I need to find the probability that the number of automobiles entering the garage during any $2$ minute period exceeds $3$. I know $\Pr(x \gt 3)=  1-\Pr(x\le 3)$. So, $1-\left(e^{-1} + e^{-1} + \frac{1}{2} e^{-1} + \frac{1}{6} e^{-1}\right) \dots$.",,['probability']
