,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Why do we need two linearly independent solutions for 2nd order linear ODE,Why do we need two linearly independent solutions for 2nd order linear ODE,,Let we have a second-order homogeneous linear ODE with two initial conditions. $y''+ p(x)y'+q(x)y=0$ $y(x_0)=K_0$ and $y'(x_0)=K_1$ Why do we need two linearly independent solutions to satisfy the IVP. If we have only one solution what would happen? Could you please explain?,Let we have a second-order homogeneous linear ODE with two initial conditions. $y''+ p(x)y'+q(x)y=0$ $y(x_0)=K_0$ and $y'(x_0)=K_1$ Why do we need two linearly independent solutions to satisfy the IVP. If we have only one solution what would happen? Could you please explain?,,['ordinary-differential-equations']
1,"Teaching Dirac delta ""function"" $\delta(t)$","Teaching Dirac delta ""function""",\delta(t),"I am about to teach applied mathematics for engineering. So I will teach how to use Laplace Transform to solve differential equations. Some of these differential equations involve the Dirac delta ""function"" as a forcing term, for example: $$y''(t)+y'(t)-y(t)=3\delta(t-1).$$ The students have as prerequisite: Calculus II and Calculus III. I don't know how to introduce the Dirac delta function: 1) Should I first talk about the distributions as linear functionals acting on test functions. Then define Dirac delta function as $\langle\,\delta,\varphi\rangle=\varphi(0)$ . or 2) Define $\delta$ as ""something"" that has a Laplace Transform $\mathcal{L(\delta)}=1$ ,  that is $\delta$ is a limit (in some sense) of a sequence of functions $f_n$ having a tall spike at the origin with $\lim_{n\to \infty}\mathcal{L(f_n)}=1$ . That object can be viewed as a ""function"" defined by $$\delta(t)=\begin{cases} 0, & t\neq0\\ \infty, & t=0 \end{cases}$$","I am about to teach applied mathematics for engineering. So I will teach how to use Laplace Transform to solve differential equations. Some of these differential equations involve the Dirac delta ""function"" as a forcing term, for example: The students have as prerequisite: Calculus II and Calculus III. I don't know how to introduce the Dirac delta function: 1) Should I first talk about the distributions as linear functionals acting on test functions. Then define Dirac delta function as . or 2) Define as ""something"" that has a Laplace Transform ,  that is is a limit (in some sense) of a sequence of functions having a tall spike at the origin with . That object can be viewed as a ""function"" defined by","y''(t)+y'(t)-y(t)=3\delta(t-1). \langle\,\delta,\varphi\rangle=\varphi(0) \delta \mathcal{L(\delta)}=1 \delta f_n \lim_{n\to \infty}\mathcal{L(f_n)}=1 \delta(t)=\begin{cases}
0, & t\neq0\\
\infty, & t=0
\end{cases}","['ordinary-differential-equations', 'laplace-transform', 'education', 'distribution-theory', 'dirac-delta']"
2,Where does the $t$ come from in the solution to a critically damped differential equation?,Where does the  come from in the solution to a critically damped differential equation?,t,"When solving homogeneous second order constant coefficient linear ODEs ($ay''+by'+cy=0$), there are three 'cases' that solutions fall into, based on the roots $r_1$, $r_2$ of the characteristic equation $ar^2+br+c=0$: $r_1, r_2 \in \Bbb R,\; r_1 \ne r_2$, aka ""overdamped"", for which the general solution is of the form $y(t) = c_1 e^{r_1 t} + c_2 e^{r_2 t}$. $r_1, r_2 \in \Bbb C,\; r_1=\overline{r_2}$, aka ""underdamped"", for which the general solution is $y(t) = c_1 e^{\Re(r)\,t}    \cos\Im(r)\,t + c_2 e^{\Re(r)\,t} \sin\Im(r)\,t$. $r_1, r_2 \in \Bbb R,\; r_1 = r_2$, aka ""critically damped"", for which the general solution is $y(t) = c_1 e^{rt} + c_2 t e^{rt}$. I understand the underdamped solution as essentially the same thing as the overdamped solution, equivalent to picking complex $c_1, c_2$ such that $y(t)$ is always real. But the critically damped case doesn't seem to fit neatly as a generalization of the other cases - that extra 't' makes it at least appear like it's something else entirely. Where does the ""$t$"" actually come from in the critically damped case? Why don't the other cases require or allow this?","When solving homogeneous second order constant coefficient linear ODEs ($ay''+by'+cy=0$), there are three 'cases' that solutions fall into, based on the roots $r_1$, $r_2$ of the characteristic equation $ar^2+br+c=0$: $r_1, r_2 \in \Bbb R,\; r_1 \ne r_2$, aka ""overdamped"", for which the general solution is of the form $y(t) = c_1 e^{r_1 t} + c_2 e^{r_2 t}$. $r_1, r_2 \in \Bbb C,\; r_1=\overline{r_2}$, aka ""underdamped"", for which the general solution is $y(t) = c_1 e^{\Re(r)\,t}    \cos\Im(r)\,t + c_2 e^{\Re(r)\,t} \sin\Im(r)\,t$. $r_1, r_2 \in \Bbb R,\; r_1 = r_2$, aka ""critically damped"", for which the general solution is $y(t) = c_1 e^{rt} + c_2 t e^{rt}$. I understand the underdamped solution as essentially the same thing as the overdamped solution, equivalent to picking complex $c_1, c_2$ such that $y(t)$ is always real. But the critically damped case doesn't seem to fit neatly as a generalization of the other cases - that extra 't' makes it at least appear like it's something else entirely. Where does the ""$t$"" actually come from in the critically damped case? Why don't the other cases require or allow this?",,['ordinary-differential-equations']
3,Differential equation involving cross product.,Differential equation involving cross product.,,I have the differential equation $$f'=c \times f$$ for $f: \mathbb{R} \to \mathbb{R}^3$ and constants $c \in \mathbb{R}^3$.  How can I solve something like this?,I have the differential equation $$f'=c \times f$$ for $f: \mathbb{R} \to \mathbb{R}^3$ and constants $c \in \mathbb{R}^3$.  How can I solve something like this?,,"['ordinary-differential-equations', 'cross-product']"
4,"y'''+4y""+4y'=2 solution of non-homogenous differential equation","y'''+4y""+4y'=2 solution of non-homogenous differential equation",,"This is non-homogenous differential equation : $$y'''+4y''+4y'=2.$$ Of course, I started with characteristic polynomial of homogenous case: $$t^3+4t^2+4t=0$$ then $$t(t^2+4t+4)=0$$ we have: $$t_1=0; t_{2,3}=-2.$$ So, solution of homogenous case is: $$y_s(x)=c_1 + c_2e^{-2x}+c_3xe^{-2x}$$ Now, I want to continue from this point to solution of non-homogenous differential equation. Please give any hint or general solution!! Thanks in advance.","This is non-homogenous differential equation : $$y'''+4y''+4y'=2.$$ Of course, I started with characteristic polynomial of homogenous case: $$t^3+4t^2+4t=0$$ then $$t(t^2+4t+4)=0$$ we have: $$t_1=0; t_{2,3}=-2.$$ So, solution of homogenous case is: $$y_s(x)=c_1 + c_2e^{-2x}+c_3xe^{-2x}$$ Now, I want to continue from this point to solution of non-homogenous differential equation. Please give any hint or general solution!! Thanks in advance.",,['ordinary-differential-equations']
5,"$ \int_{-\infty}^{\infty} \frac{e^{2x}}{ae^{3x}+b} dx,$ where $a,b \gt 0$",where," \int_{-\infty}^{\infty} \frac{e^{2x}}{ae^{3x}+b} dx, a,b \gt 0","Evaluate $$ \int_{-\infty}^{\infty} \frac{e^{2x}}{ae^{3x}+b} dx,$$ where $a,b \gt 0$ I tried using $y=e^x$, but I still can't solve it. I get $\displaystyle\int_0^\infty \frac y{ay^3+b} \, dy.$ Is there any different method to solve it?","Evaluate $$ \int_{-\infty}^{\infty} \frac{e^{2x}}{ae^{3x}+b} dx,$$ where $a,b \gt 0$ I tried using $y=e^x$, but I still can't solve it. I get $\displaystyle\int_0^\infty \frac y{ay^3+b} \, dy.$ Is there any different method to solve it?",,['ordinary-differential-equations']
6,First derivative of Lagrange polynomial,First derivative of Lagrange polynomial,,"Given the Lagrange basis polynomial as: $L_i(x)= \prod_{m=0, m \neq i}^n \frac{x-x_m}{x_i-x_m} $ is there a generic equation for the first derivative ${L_i}'(x)$ for any order,t hat is for any $n$?","Given the Lagrange basis polynomial as: $L_i(x)= \prod_{m=0, m \neq i}^n \frac{x-x_m}{x_i-x_m} $ is there a generic equation for the first derivative ${L_i}'(x)$ for any order,t hat is for any $n$?",,"['ordinary-differential-equations', 'polynomials', 'derivatives', 'lagrange-multiplier']"
7,Pfaffian Equations,Pfaffian Equations,,"Solve the Pfaffian Equation below: $$(yz-1)dx+((z-x)x)dy+(1-xy)dz=0$$ I have tried to find the determinant, and it is $0$ . So it is integrable. I let $z$ to be a constant. After I'm having problems solving it.","Solve the Pfaffian Equation below: I have tried to find the determinant, and it is . So it is integrable. I let to be a constant. After I'm having problems solving it.",(yz-1)dx+((z-x)x)dy+(1-xy)dz=0 0 z,"['ordinary-differential-equations', 'partial-differential-equations']"
8,Nonuniqueness of Stochastic Differential Equation,Nonuniqueness of Stochastic Differential Equation,,"Let $B_t$ be the standard Brownian motion, $\mu(t,x)$ and $\sigma(t,x)\ne 0$ are real valued continuous functions where $|\mu(t,x)|+|\sigma(t,x)|$ is NOT Lipschitz continuous, and   $$dX_t = \mu(t,X(t))dt+\sigma(t,X(t))dB_t,\ X(0)=X_0.$$   Can one give an example where the solution $X(t)$ is not unique? Please note $\sigma(t,x)\ne 0$. I know for $\sigma=0, \mu(t,x)=\sqrt{x}, X_0=0$, this ordinary differential equation does not have a unique solution. I would think a nonzero $\sigma(t,x)$ would only make the nonuniqueness still worse. How does one show that?","Let $B_t$ be the standard Brownian motion, $\mu(t,x)$ and $\sigma(t,x)\ne 0$ are real valued continuous functions where $|\mu(t,x)|+|\sigma(t,x)|$ is NOT Lipschitz continuous, and   $$dX_t = \mu(t,X(t))dt+\sigma(t,X(t))dB_t,\ X(0)=X_0.$$   Can one give an example where the solution $X(t)$ is not unique? Please note $\sigma(t,x)\ne 0$. I know for $\sigma=0, \mu(t,x)=\sqrt{x}, X_0=0$, this ordinary differential equation does not have a unique solution. I would think a nonzero $\sigma(t,x)$ would only make the nonuniqueness still worse. How does one show that?",,"['ordinary-differential-equations', 'stochastic-processes', 'stochastic-analysis']"
9,Lyapunov equation for stability analysis - what's the point?,Lyapunov equation for stability analysis - what's the point?,,"Straight from Wikipedia : In the following theorem $A, P, Q \in \mathbb{R}^{n \times n}$ , and $P$ and $Q$ are symmetric. The notation $P>0$ means that the matrix $P$ is positive definite. Given any $Q>0$ , there exists a unique $P>0$ satisfying $A^T P + P A + Q = 0$ if and only if the linear system $\dot{x}=A x$ is globally asymptotically stable.  The quadratic function $V(z)=z^T P z$ is a Lyapunov function that can be used to verify stability. Recently I became quite familiar with tools to solve Lyapunov equation $A^T P + P A + Q = 0$ and obtain $P$ . However in the context of stability analysis I see one huge problem. It's that you still have to check that $P$ is positive definite or equivalently that its eigenvalues are positive. But for the linear system the fact that all eigenvalues of $A$ have negative real parts ensures that the system is globally asymptotically stable in the first place. So what's the point of going through all the hassle just to end up with looking for the eigenvalues ( $P$ and $A$ are of the same size!) anyway? Does the symmetric matrix make such a big difference?","Straight from Wikipedia : In the following theorem , and and are symmetric. The notation means that the matrix is positive definite. Given any , there exists a unique satisfying if and only if the linear system is globally asymptotically stable.  The quadratic function is a Lyapunov function that can be used to verify stability. Recently I became quite familiar with tools to solve Lyapunov equation and obtain . However in the context of stability analysis I see one huge problem. It's that you still have to check that is positive definite or equivalently that its eigenvalues are positive. But for the linear system the fact that all eigenvalues of have negative real parts ensures that the system is globally asymptotically stable in the first place. So what's the point of going through all the hassle just to end up with looking for the eigenvalues ( and are of the same size!) anyway? Does the symmetric matrix make such a big difference?","A, P, Q \in \mathbb{R}^{n \times n} P Q P>0 P Q>0 P>0 A^T P + P A + Q = 0 \dot{x}=A x V(z)=z^T P z A^T P + P A + Q = 0 P P A P A","['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'dynamical-systems', 'matrix-equations', 'stability-in-odes']"
10,Good book to study ODEs through geometric ideas,Good book to study ODEs through geometric ideas,,"When studying a subject, geometric intuition is important for me. The algebra books I know do not convey such intuition. Please recommend books on ordinary differential equations with an emphasis on geometric intuition.","When studying a subject, geometric intuition is important for me. The algebra books I know do not convey such intuition. Please recommend books on ordinary differential equations with an emphasis on geometric intuition.",,"['ordinary-differential-equations', 'reference-request', 'book-recommendation']"
11,How do I solve $y''+4y=0$?,How do I solve ?,y''+4y=0,"This problem is in Penney's Elementary Differential Equations, listed as a reducible, 2nd-order DE.  The chapter has taught two techniques to be used, which are for when either $x$ or $y(x)$ is missing.  It didn't show how to solve when $y'(x)$ is missing. I checked with WolframAlpha, and it suggests starting by assuming that $y$ is proportional to $e^{\lambda x}$. If I didn't know to assume this, how could I solve this otherwise?","This problem is in Penney's Elementary Differential Equations, listed as a reducible, 2nd-order DE.  The chapter has taught two techniques to be used, which are for when either $x$ or $y(x)$ is missing.  It didn't show how to solve when $y'(x)$ is missing. I checked with WolframAlpha, and it suggests starting by assuming that $y$ is proportional to $e^{\lambda x}$. If I didn't know to assume this, how could I solve this otherwise?",,['ordinary-differential-equations']
12,Do the endpoints of a catenary have to be horizontally aligned?,Do the endpoints of a catenary have to be horizontally aligned?,,"Nothing I see in the definition of a catenary says this must be the case, but every illustration I've seen draws it that way.  I'm assuming that a cable hanging from two points at different heights off the ground is still a catenary but how does that change the equation and properties?","Nothing I see in the definition of a catenary says this must be the case, but every illustration I've seen draws it that way.  I'm assuming that a cable hanging from two points at different heights off the ground is still a catenary but how does that change the equation and properties?",,['ordinary-differential-equations']
13,$2^{nd}$ order PDE: Solution,order PDE: Solution,2^{nd},"I am trying to solve the following equation: $$\frac{\partial F}{\partial t} = \alpha^2 \, \frac{\partial^2 F}{\partial x^2}-h \, F$$ subject to these conditions: $$F(x,0) = 0, \hspace{5mm} F(0,t) = F(L,t)=F_{0} \, e^{-ht}.$$ I know that I am suppose to simplify the equation with: $$F(x,t)=\phi(x,t)e^{-ht}$$ My initial guess is to divide by $$\alpha^2$$ and have this: $$\frac{d^2F}{dx^2}-\frac{1}{\alpha^2}\frac{dF}{dt}-\frac{h}{\alpha^2} \, F=0.$$ I am having trouble with the next steps. Should I assume a solution of the exponential form?","I am trying to solve the following equation: $$\frac{\partial F}{\partial t} = \alpha^2 \, \frac{\partial^2 F}{\partial x^2}-h \, F$$ subject to these conditions: $$F(x,0) = 0, \hspace{5mm} F(0,t) = F(L,t)=F_{0} \, e^{-ht}.$$ I know that I am suppose to simplify the equation with: $$F(x,t)=\phi(x,t)e^{-ht}$$ My initial guess is to divide by $$\alpha^2$$ and have this: $$\frac{d^2F}{dx^2}-\frac{1}{\alpha^2}\frac{dF}{dt}-\frac{h}{\alpha^2} \, F=0.$$ I am having trouble with the next steps. Should I assume a solution of the exponential form?",,"['ordinary-differential-equations', 'partial-differential-equations']"
14,Express in terms of Legendre polynomials,Express in terms of Legendre polynomials,,"Express the following functions as Legendre series (as a sum of Legendre polynomials) a) $-2x^2 + 7$ b) $3x^4+6x^2-2$ c) $\cos^5 \theta$ - the L.P. will be in functions of $\cos \theta$. If anyone could help me understand the setup process for Legendres, that would be highly appreciated. I have no exposure to Legendre series, and I am using Griffith's Intro. to Electrodynamics which doesn't provide the explanation of Legendre series in any detail. You may determine the coefficients in the series algebraically or using the orthogonality properties of the Legendre polynomials. Edit: It seems that the solutions for Legendre polynomial are set, such as the values of $P_0$, $P_1$, $P_2$, etc... But how do I incorporate the given function? Do I make them into a polynomial, then attempt to set that in series that is equal to the Legendre series? For (c), I am assuming to express that in complex terms.. The hint suggests to use orthogonality properties, in which the normalization would be 1?","Express the following functions as Legendre series (as a sum of Legendre polynomials) a) $-2x^2 + 7$ b) $3x^4+6x^2-2$ c) $\cos^5 \theta$ - the L.P. will be in functions of $\cos \theta$. If anyone could help me understand the setup process for Legendres, that would be highly appreciated. I have no exposure to Legendre series, and I am using Griffith's Intro. to Electrodynamics which doesn't provide the explanation of Legendre series in any detail. You may determine the coefficients in the series algebraically or using the orthogonality properties of the Legendre polynomials. Edit: It seems that the solutions for Legendre polynomial are set, such as the values of $P_0$, $P_1$, $P_2$, etc... But how do I incorporate the given function? Do I make them into a polynomial, then attempt to set that in series that is equal to the Legendre series? For (c), I am assuming to express that in complex terms.. The hint suggests to use orthogonality properties, in which the normalization would be 1?",,['ordinary-differential-equations']
15,An interesting pattern in solutions to differential equations,An interesting pattern in solutions to differential equations,,"OK, watch this: Suppose I have a weight on the end of a spring. Assuming the spring obeys Hooke's law, as the weight is displaced from its rest position, the spring exerts a restoring force in the opposite direction who's magnitude is equal to the displacement multiplied by the spring constant. Suppose that $f(t)$ represents the displacement of the weight at time $t$. If we assume that the spring constant and the mass of the weight are both unity, we have $$f''(t) = -f(t)$$ This is an equation involving both $f$ itself and its derivative $f''$, so this is presumably a ""differential equation"". I don't know how to deal with such a thing. But it is clear that this does not yet tell me what $f$ is , only that it must satisfy a specific property. Thinking about this for a moment, it is clear that $f(x) = 0$ has the requested property. This corresponds to the weight remaining stationary for all eternity - a physically valid, but rather ""boring"" result. Contemplating this further, it occurs to me that the derivative of $\sin$ is $\cos$, and the derivative of $\cos$ is $-\sin$. So if $f(t)=\sin(t)$ then $f''(t)=-\sin(t)$, which satisfies the equation. By nearly identical reasoning, $f(t)=\cos(t)$ would also work. In short, if you ping the weight, it oscillates around zero. Now suppose that, by some bizarre mechanism, the restoring force were to somehow be in the same direction as the displacement. Impossible, I know. But imagine. Now our equation becomes $$f''(t)=f(t)$$ Again $f(t)=0$ would work. But what else? Well, there is exactly one function who's derivative equals itself: $\exp$. This is a stronger property than we need, but still, if $f(t)=\exp(t)$ then every derivative of $f$ (including $f''$) would equal $f$. This describes the weight accelerating away exponentially - rather as you might expect. So far, we have two equations. The solution to one is $\sin$. The solution to the other is $\exp$. Two similar equations, two totally different solutions. Or at least, they look different. But now I'm thinking about something Euler once wrote: $$\exp(ix) = \cos(x) + i \sin(x)$$ Say that, and suddenly these solutions don't look so dissimilar at all. Now they suddenly look suspiciously similar! My question: Is this the result of some deep and meaningful connection? Or is it merely a coincidence? Holy smokes, you guys are right! I know, of course, of $\sinh$ and $\cosh$. (For real numbers, they look utterly unrelated. But in the complex plane, one is a trivially rotated version of the other.) What I didn't know, until I looked it up, was the derivatives of these functions. Since they're defined in terms of $\exp$, I was expecting some really complicated derivative. However, what I actually found (as you presumably all know) is that $\sinh'=\cosh$ and, unlike in the circular case, $\cosh'=\sinh$! So yes, for $f''=-f$ we have $f=\sin$ or $f=\cos$, and for $f''=f$ we have $f=\sinh$ or $f=\cosh$. So flipping the sign of the differential equation rotates the function in the complex plane. Physically, it doesn't look very meaningful to talk about complex-valued seconds, but mathematically it all looks vastly too perfect to be mere coincidence.","OK, watch this: Suppose I have a weight on the end of a spring. Assuming the spring obeys Hooke's law, as the weight is displaced from its rest position, the spring exerts a restoring force in the opposite direction who's magnitude is equal to the displacement multiplied by the spring constant. Suppose that $f(t)$ represents the displacement of the weight at time $t$. If we assume that the spring constant and the mass of the weight are both unity, we have $$f''(t) = -f(t)$$ This is an equation involving both $f$ itself and its derivative $f''$, so this is presumably a ""differential equation"". I don't know how to deal with such a thing. But it is clear that this does not yet tell me what $f$ is , only that it must satisfy a specific property. Thinking about this for a moment, it is clear that $f(x) = 0$ has the requested property. This corresponds to the weight remaining stationary for all eternity - a physically valid, but rather ""boring"" result. Contemplating this further, it occurs to me that the derivative of $\sin$ is $\cos$, and the derivative of $\cos$ is $-\sin$. So if $f(t)=\sin(t)$ then $f''(t)=-\sin(t)$, which satisfies the equation. By nearly identical reasoning, $f(t)=\cos(t)$ would also work. In short, if you ping the weight, it oscillates around zero. Now suppose that, by some bizarre mechanism, the restoring force were to somehow be in the same direction as the displacement. Impossible, I know. But imagine. Now our equation becomes $$f''(t)=f(t)$$ Again $f(t)=0$ would work. But what else? Well, there is exactly one function who's derivative equals itself: $\exp$. This is a stronger property than we need, but still, if $f(t)=\exp(t)$ then every derivative of $f$ (including $f''$) would equal $f$. This describes the weight accelerating away exponentially - rather as you might expect. So far, we have two equations. The solution to one is $\sin$. The solution to the other is $\exp$. Two similar equations, two totally different solutions. Or at least, they look different. But now I'm thinking about something Euler once wrote: $$\exp(ix) = \cos(x) + i \sin(x)$$ Say that, and suddenly these solutions don't look so dissimilar at all. Now they suddenly look suspiciously similar! My question: Is this the result of some deep and meaningful connection? Or is it merely a coincidence? Holy smokes, you guys are right! I know, of course, of $\sinh$ and $\cosh$. (For real numbers, they look utterly unrelated. But in the complex plane, one is a trivially rotated version of the other.) What I didn't know, until I looked it up, was the derivatives of these functions. Since they're defined in terms of $\exp$, I was expecting some really complicated derivative. However, what I actually found (as you presumably all know) is that $\sinh'=\cosh$ and, unlike in the circular case, $\cosh'=\sinh$! So yes, for $f''=-f$ we have $f=\sin$ or $f=\cos$, and for $f''=f$ we have $f=\sinh$ or $f=\cosh$. So flipping the sign of the differential equation rotates the function in the complex plane. Physically, it doesn't look very meaningful to talk about complex-valued seconds, but mathematically it all looks vastly too perfect to be mere coincidence.",,['ordinary-differential-equations']
16,How to find a Lyapunov function in this case?,How to find a Lyapunov function in this case?,,"We have the system of differential equations $$ \begin{aligned} \frac{dx}{dt} &= y + \sin{x}\\ \frac{dy}{dt} &= -5x-2y. \end{aligned} $$ It's necessary to prove that the system is stable using a Lyapunov function or else show that it's not, following Lyapunov's/Chetaev's theorem. The first thing I want to ask about is whether it's appropriate to solve the problem for using the fact that $\sin{x} \approx x$ around $x=0$ : $$ \begin{aligned} \frac{dx}{dt} &= y + x\\ \frac{dy}{dt} &= -5x-2y. \end{aligned} $$ If that is possible, I would usually check a few functions such as $V(x, y) = ax^2 + by^2$ or $V(x, y) = ax^4 + by^2$ , or $V(x, y) = ax^4 + by^4$ or even $V(x,y) = ax^2 + by^2 + cxy$ . The problem is that, unlike simpler problems I haven't yet managed to find such a function that the total derivative of $V$ is strictly positive/negative and the function itself is strictly negative/positive respectively for all pairs of $(x,y)$ except for $(0, 0)$ . Probably, I am trying to prove something that is not true and this is obvious from the beginning. I have tried a few simulation in Python to iterate over different suitable functions and values of $a, b$ to match the criterion, but there was no match.","We have the system of differential equations It's necessary to prove that the system is stable using a Lyapunov function or else show that it's not, following Lyapunov's/Chetaev's theorem. The first thing I want to ask about is whether it's appropriate to solve the problem for using the fact that around : If that is possible, I would usually check a few functions such as or , or or even . The problem is that, unlike simpler problems I haven't yet managed to find such a function that the total derivative of is strictly positive/negative and the function itself is strictly negative/positive respectively for all pairs of except for . Probably, I am trying to prove something that is not true and this is obvious from the beginning. I have tried a few simulation in Python to iterate over different suitable functions and values of to match the criterion, but there was no match.","
\begin{aligned}
\frac{dx}{dt} &= y + \sin{x}\\
\frac{dy}{dt} &= -5x-2y.
\end{aligned}
 \sin{x} \approx x x=0 
\begin{aligned}
\frac{dx}{dt} &= y + x\\
\frac{dy}{dt} &= -5x-2y.
\end{aligned}
 V(x, y) = ax^2 + by^2 V(x, y) = ax^4 + by^2 V(x, y) = ax^4 + by^4 V(x,y) = ax^2 + by^2 + cxy V (x,y) (0, 0) a, b","['ordinary-differential-equations', 'stability-theory', 'lyapunov-functions']"
17,hint on solving $1+y'^2-y\cdot y''=0$?,hint on solving ?,1+y'^2-y\cdot y''=0,"I have the following ODE: $$1+y'^2-y\cdot y''=0$$ I've never solved an ODE where two ""versions"" (don't know the term) of $y$ are multiplied with eachother, in this case $y$ and $y''$. Can I have a hint how to approach this, without too much of an answer?","I have the following ODE: $$1+y'^2-y\cdot y''=0$$ I've never solved an ODE where two ""versions"" (don't know the term) of $y$ are multiplied with eachother, in this case $y$ and $y''$. Can I have a hint how to approach this, without too much of an answer?",,"['ordinary-differential-equations', 'nonlinear-system']"
18,$f''+f \ge 0$ implies $f(x)+f(x+\pi) \ge 0$,implies,f''+f \ge 0 f(x)+f(x+\pi) \ge 0,"Let $f: \mathbb{R} \to \mathbb{R}$ be a function of class $C^2$ satisfying $f''(t)+f(t) \ge 0$ for all $t \in \mathbb{R}$. Show that $f(t)+f(t+\pi) \ge 0$. What I did: Set $f''(t)+f(t)=g(t)$. This is an LDE of order 2, and we denote this equation by (E), and the corresponding homegoneous equation by (H). and $f$ is of the form $A\cos(t)+B\sin(t)+y_0(t)$ where $y_0(t)$ is a particular solution of (E), $A,B$ are constants. The trigonometric part cancels in the evaluation of $f(t)+f(t+\pi)$, so the problem boils down to finding a $y_0(t)$ which is always nonnegative. Well let's search for such a $y_0(t)$ using the technique of reductin of order, i.e let's set $y_0(t)=\lambda y_h(t)$ where $y_h(t)$ is a particular solution of $(H)$. All solutions of $(H)$ are sinusoidal so if this method is going to work we might as well set $y_0(t)=\lambda \sin(t)$. Substituting, we find $\sin(t)\lambda''+2\cos(t)\lambda'=g$. So if $\sin(t)=0$, $\lambda'=g(t)/2$. Let $I_{2k}=(2k\pi,(2k+1)\pi)$, $I_{2k+1}=((2k+1)\pi,(2k+2)\pi)$. Define $L_I=2k\pi$ if $I=I_{2k}$, $L_I=(2k+1)*\pi$ if $I=I_{2k+1}$. If $I \in \{I_{2k},I_{2k+1}\}$ we have $(\frac{d}{dt} [\lambda'\sin(t)])/\sin^2(t)=g$ for all $t \in I$ $\lambda'\sin^2(t)=\int^t_{L_I} g(u)\sin(u)\,du+C_I$ for all $t \in I$ where $C_I$ is a constant of integration.Note that the integral is well-defined since $gsin(u)$ is continuous, and the integral is therefore itself continuous. Note that if $I=I_{2k}$, then the integrand is positive, and $\sin^2(t)$  is always positive, so if we choose $C_I$ correctly then $\lambda'$ is positive. The opposite holds if $I=I_{2k+1}$. This is good because we want $\lambda$ positive on $I_{2k}$ and negative on $I_{2k+1}$. Now note that $\lambda'$ is continuous on all the $I$'s. However if we impose that $\lambda'$ be continuous on $\mathbb{R}$ then we run into a problem because  $\lim_{t \to L_I, t>L_I}RHS=C_I$, which must be equal to $\lambda'(L_I)\sin^2(L_I)=0$, for all $I$. But then  $\lim_{t \to L_I, t<L_I}RHS=\int^{L_I}_{L_I'} g(u)\sin(u)\,du=0$, where $I'$ is the interval that precedes $I$. Of course in general $g$ doesn't have to satisfy this. So this method kind of breaks down when we consider continuity but I believe it gives a function $f$ which is continuous and differentiable everywhere (and nonnegative, if we choose $C_I=0$) except for the points $L_I$. Edit: Can you please tell me whether my method could work, or is the only possible solution the magical invaraint one given by achille?","Let $f: \mathbb{R} \to \mathbb{R}$ be a function of class $C^2$ satisfying $f''(t)+f(t) \ge 0$ for all $t \in \mathbb{R}$. Show that $f(t)+f(t+\pi) \ge 0$. What I did: Set $f''(t)+f(t)=g(t)$. This is an LDE of order 2, and we denote this equation by (E), and the corresponding homegoneous equation by (H). and $f$ is of the form $A\cos(t)+B\sin(t)+y_0(t)$ where $y_0(t)$ is a particular solution of (E), $A,B$ are constants. The trigonometric part cancels in the evaluation of $f(t)+f(t+\pi)$, so the problem boils down to finding a $y_0(t)$ which is always nonnegative. Well let's search for such a $y_0(t)$ using the technique of reductin of order, i.e let's set $y_0(t)=\lambda y_h(t)$ where $y_h(t)$ is a particular solution of $(H)$. All solutions of $(H)$ are sinusoidal so if this method is going to work we might as well set $y_0(t)=\lambda \sin(t)$. Substituting, we find $\sin(t)\lambda''+2\cos(t)\lambda'=g$. So if $\sin(t)=0$, $\lambda'=g(t)/2$. Let $I_{2k}=(2k\pi,(2k+1)\pi)$, $I_{2k+1}=((2k+1)\pi,(2k+2)\pi)$. Define $L_I=2k\pi$ if $I=I_{2k}$, $L_I=(2k+1)*\pi$ if $I=I_{2k+1}$. If $I \in \{I_{2k},I_{2k+1}\}$ we have $(\frac{d}{dt} [\lambda'\sin(t)])/\sin^2(t)=g$ for all $t \in I$ $\lambda'\sin^2(t)=\int^t_{L_I} g(u)\sin(u)\,du+C_I$ for all $t \in I$ where $C_I$ is a constant of integration.Note that the integral is well-defined since $gsin(u)$ is continuous, and the integral is therefore itself continuous. Note that if $I=I_{2k}$, then the integrand is positive, and $\sin^2(t)$  is always positive, so if we choose $C_I$ correctly then $\lambda'$ is positive. The opposite holds if $I=I_{2k+1}$. This is good because we want $\lambda$ positive on $I_{2k}$ and negative on $I_{2k+1}$. Now note that $\lambda'$ is continuous on all the $I$'s. However if we impose that $\lambda'$ be continuous on $\mathbb{R}$ then we run into a problem because  $\lim_{t \to L_I, t>L_I}RHS=C_I$, which must be equal to $\lambda'(L_I)\sin^2(L_I)=0$, for all $I$. But then  $\lim_{t \to L_I, t<L_I}RHS=\int^{L_I}_{L_I'} g(u)\sin(u)\,du=0$, where $I'$ is the interval that precedes $I$. Of course in general $g$ doesn't have to satisfy this. So this method kind of breaks down when we consider continuity but I believe it gives a function $f$ which is continuous and differentiable everywhere (and nonnegative, if we choose $C_I=0$) except for the points $L_I$. Edit: Can you please tell me whether my method could work, or is the only possible solution the magical invaraint one given by achille?",,['ordinary-differential-equations']
19,Fourier Series of $e^x$ [duplicate],Fourier Series of  [duplicate],e^x,"This question already has answers here : Fourier Series Representation $e^{ax}$ (3 answers) Closed 7 years ago . I am tying to integrate $a_n= \frac{1}{L} e^x\sin\left(\frac{n\pi x}{h}\right)dx$ and get the solution in sinh form.  I have gotten the long answer, but cannot figure out how to turn it into sinh. Can someone help me with the steps?","This question already has answers here : Fourier Series Representation $e^{ax}$ (3 answers) Closed 7 years ago . I am tying to integrate and get the solution in sinh form.  I have gotten the long answer, but cannot figure out how to turn it into sinh. Can someone help me with the steps?",a_n= \frac{1}{L} e^x\sin\left(\frac{n\pi x}{h}\right)dx,['ordinary-differential-equations']
20,Properties of sin(x) and cos(x) from definition as solution to differential equation y''=-y,Properties of sin(x) and cos(x) from definition as solution to differential equation y''=-y,,"I recently came across the interesting definition of the sine function as the unique solution to the Initial Value Problem $$y'' = -y$$ $$y(0) = 0, y'(0) = 1$$ (My first question would be why this solution is unique, but I found that proofs of uniqueness and existence are too complicated for me to understand) Taking existence and uniqueness for granted still leaves a lot of questions about the properties of sine like: $y$ is periodic with smallest period $T$ (defined to be $2\pi$) $ -1 \leq y(x) \leq 1 $ for all $x \in \mathbb{R}$ The value of sine for certain values related to the period: $ y(\frac{\pi}{6}) = \frac{1}{2}, y(\frac{\pi}{3}) = \frac{\sqrt3}{2}$ All the various trigonometric identities How would one go about proving these properties using only the differential equation definition? I have no idea where to start. Usually these properties (at least for me) are intuitively understood through the unit circle but that doesn't help here.","I recently came across the interesting definition of the sine function as the unique solution to the Initial Value Problem $$y'' = -y$$ $$y(0) = 0, y'(0) = 1$$ (My first question would be why this solution is unique, but I found that proofs of uniqueness and existence are too complicated for me to understand) Taking existence and uniqueness for granted still leaves a lot of questions about the properties of sine like: $y$ is periodic with smallest period $T$ (defined to be $2\pi$) $ -1 \leq y(x) \leq 1 $ for all $x \in \mathbb{R}$ The value of sine for certain values related to the period: $ y(\frac{\pi}{6}) = \frac{1}{2}, y(\frac{\pi}{3}) = \frac{\sqrt3}{2}$ All the various trigonometric identities How would one go about proving these properties using only the differential equation definition? I have no idea where to start. Usually these properties (at least for me) are intuitively understood through the unit circle but that doesn't help here.",,"['ordinary-differential-equations', 'trigonometry', 'definition']"
21,"What is the ""differential form"" of an ODE?","What is the ""differential form"" of an ODE?",,"I'm reading an introductory textbook on Ordinary Differential Equations and the author does something I think needs some justification. Here it goes: Consider an ODE $y' = f(x,y)$, so we can write $f(x,y)=\frac{-M(x,y)}{N(x,y)}$. Therefore $y'=\frac{-M(x,y)}{N(x,y)}$, which we can write in ""differential form"" as $M(x,y)dx+N(x,y)dy=0$. Question: What is exactly the ""differential form"" of an ODE? Is it just a purely formal reasoning or there is some elementary context in which we can interpret the $dx,dy$ in the above equation? Thank you.","I'm reading an introductory textbook on Ordinary Differential Equations and the author does something I think needs some justification. Here it goes: Consider an ODE $y' = f(x,y)$, so we can write $f(x,y)=\frac{-M(x,y)}{N(x,y)}$. Therefore $y'=\frac{-M(x,y)}{N(x,y)}$, which we can write in ""differential form"" as $M(x,y)dx+N(x,y)dy=0$. Question: What is exactly the ""differential form"" of an ODE? Is it just a purely formal reasoning or there is some elementary context in which we can interpret the $dx,dy$ in the above equation? Thank you.",,['ordinary-differential-equations']
22,What is the difference between an impulse response and a transferfunction?,What is the difference between an impulse response and a transferfunction?,,"An imupulse response, is the output you get when you apply an impulse, like a delta dirac function, to your system (only for LTI?). By knowing the impulse response you know the system. The transferfunction relates the input to the output. I.e. this is a representation of the system. So aren't both the same? Or Did I misunderstand something?","An imupulse response, is the output you get when you apply an impulse, like a delta dirac function, to your system (only for LTI?). By knowing the impulse response you know the system. The transferfunction relates the input to the output. I.e. this is a representation of the system. So aren't both the same? Or Did I misunderstand something?",,"['ordinary-differential-equations', 'laplace-transform', 'control-theory']"
23,"Show that the largest interval of existence of the solution predicted by Picard's Theorem is $[0,\frac{1}{2}]$",Show that the largest interval of existence of the solution predicted by Picard's Theorem is,"[0,\frac{1}{2}]","Let the $\operatorname{IVP}$ be given by: $\dfrac{\operatorname{dy}}{\operatorname{dx}}=y^2+\cos^2 x;x>0;y(0)=0$ Show that the largest interval of existence of the solution predicted by Picard's Theorem is $[0,\frac{1}{2}]$ By Picard's Existence and Uniqueness Theorem ;If $f$ is continuous on a domain $D$ and $f$ satisfies Lipschitz condition on $D$ .If $R=\{|x-x_0|\leq a;|y-y_0|\leq b\}$ lies in $D$ and $M=\sup |f(x,y)|,h=\min\{a,\frac{b}{M}\}$. Then the $\operatorname{IVP}$ has a unique solution on the interval $|x-x_0|\leq h$. Obviously the Lipscitz Condition is satisfied here.But I can't find the rectangle which is needed to apply the theorem.How should I do it?","Let the $\operatorname{IVP}$ be given by: $\dfrac{\operatorname{dy}}{\operatorname{dx}}=y^2+\cos^2 x;x>0;y(0)=0$ Show that the largest interval of existence of the solution predicted by Picard's Theorem is $[0,\frac{1}{2}]$ By Picard's Existence and Uniqueness Theorem ;If $f$ is continuous on a domain $D$ and $f$ satisfies Lipschitz condition on $D$ .If $R=\{|x-x_0|\leq a;|y-y_0|\leq b\}$ lies in $D$ and $M=\sup |f(x,y)|,h=\min\{a,\frac{b}{M}\}$. Then the $\operatorname{IVP}$ has a unique solution on the interval $|x-x_0|\leq h$. Obviously the Lipscitz Condition is satisfied here.But I can't find the rectangle which is needed to apply the theorem.How should I do it?",,"['ordinary-differential-equations', 'initial-value-problems']"
24,What really is a path-ordered exponential?,What really is a path-ordered exponential?,,"In some texts about gauge theories in Physics I've found one object called a path-ordered exponential which I'm not sure what it means. As I understood, the idea is as follows: let $G$ be a Lie group with Lie algebra $\mathfrak{g}$ and let $L_g : G\to G$ be the left translation by $g$, i.e. $L_g(g') = gg'$. If $\gamma : I\subset \mathbb{R}\to G$ is a curve in $G$ then by virtue of left translation we have the following: $$\gamma'(t) = (L_{\gamma(t)})_{\ast} \beta(t)$$ where $\beta : I\to \mathfrak{g}$ is defined by $$\beta(t) = (L_{\gamma(t)^{-1}})_\ast \gamma'(t).$$ So given $\gamma$ we can find $\beta$. Now, if someone gives $\beta$ and wants to find $\gamma$, then $\gamma$ is the curve satisfying the differential equation $$\gamma'(t) = (L_{\gamma(t)})_{\ast} \beta(t)$$ and then as I understand, the solution to this equation is the path-ordered exponential. If $G$ is the multiplicative group of real numbers, then $\mathfrak{g}$ is also the real numbers. In that case $\gamma(t) = f(t)$ is a real function and so is $\beta(t) = g(t)$. The differential equation is then $$f'(t) = f(t) g(t)$$ whose solution is just the usual exponential $$f(t) = C\exp\left(\int g(t)dt\right).$$ Now I simply can't understand what is this path-ordered exponential. What this path-ordered exponential really is, and how can one show that it is the solution to that differential equation?","In some texts about gauge theories in Physics I've found one object called a path-ordered exponential which I'm not sure what it means. As I understood, the idea is as follows: let $G$ be a Lie group with Lie algebra $\mathfrak{g}$ and let $L_g : G\to G$ be the left translation by $g$, i.e. $L_g(g') = gg'$. If $\gamma : I\subset \mathbb{R}\to G$ is a curve in $G$ then by virtue of left translation we have the following: $$\gamma'(t) = (L_{\gamma(t)})_{\ast} \beta(t)$$ where $\beta : I\to \mathfrak{g}$ is defined by $$\beta(t) = (L_{\gamma(t)^{-1}})_\ast \gamma'(t).$$ So given $\gamma$ we can find $\beta$. Now, if someone gives $\beta$ and wants to find $\gamma$, then $\gamma$ is the curve satisfying the differential equation $$\gamma'(t) = (L_{\gamma(t)})_{\ast} \beta(t)$$ and then as I understand, the solution to this equation is the path-ordered exponential. If $G$ is the multiplicative group of real numbers, then $\mathfrak{g}$ is also the real numbers. In that case $\gamma(t) = f(t)$ is a real function and so is $\beta(t) = g(t)$. The differential equation is then $$f'(t) = f(t) g(t)$$ whose solution is just the usual exponential $$f(t) = C\exp\left(\int g(t)dt\right).$$ Now I simply can't understand what is this path-ordered exponential. What this path-ordered exponential really is, and how can one show that it is the solution to that differential equation?",,"['ordinary-differential-equations', 'lie-groups', 'lie-algebras', 'mathematical-physics']"
25,Extension of Poincaré-Bendixson Theorem to $\mathbb{R}^3$,Extension of Poincaré-Bendixson Theorem to,\mathbb{R}^3,"Hartman mentioned in his ODE book (chapter 7) that Poincaré-Bendixson Theorem is limited to $\mathbb{R}^2$ or $2$-manifold because of Jordan Curve Theorem . Since there is generalization for Jordan Curve Theorem , why did no one extend Poincaré-Bendixson to higher dimensions?","Hartman mentioned in his ODE book (chapter 7) that Poincaré-Bendixson Theorem is limited to $\mathbb{R}^2$ or $2$-manifold because of Jordan Curve Theorem . Since there is generalization for Jordan Curve Theorem , why did no one extend Poincaré-Bendixson to higher dimensions?",,"['ordinary-differential-equations', 'dynamical-systems']"
26,"Properties of $y$ if $\frac{d^2y}{dx^2}+xy=0$, $x \in [a,b],$ and $y(a)=y(b)=0.$","Properties of  if ,  and","y \frac{d^2y}{dx^2}+xy=0 x \in [a,b], y(a)=y(b)=0.","Let $y$ be a nontrivial solution of the boundary value problem $$\frac{d^2y}{dx^2}+xy=0,\ x \in [a,b]$$ $$y(a)=y(b)=0$$ Then: $b>0$ $y$ is monotone in $(a,0)$ if $a<0<b$. $y'(a)=0$. $y$ has infinitely many zeros in $[a,b]$ I have no idea how to solve it and which options is/ are correct. please help me.","Let $y$ be a nontrivial solution of the boundary value problem $$\frac{d^2y}{dx^2}+xy=0,\ x \in [a,b]$$ $$y(a)=y(b)=0$$ Then: $b>0$ $y$ is monotone in $(a,0)$ if $a<0<b$. $y'(a)=0$. $y$ has infinitely many zeros in $[a,b]$ I have no idea how to solve it and which options is/ are correct. please help me.",,['ordinary-differential-equations']
27,Recommended book on modeling/differential equations,Recommended book on modeling/differential equations,,"I am soon attending a undergrad course named differential equations and modeling. I have dealt with differential equations before, but in that course just learned a bunch of methods for solving them. Is there any cool books with more 'modeling' view of this subject? Like given a problem A, you have to derive equations for solving it, then solve it. This is often a hard part in math problems in my view.","I am soon attending a undergrad course named differential equations and modeling. I have dealt with differential equations before, but in that course just learned a bunch of methods for solving them. Is there any cool books with more 'modeling' view of this subject? Like given a problem A, you have to derive equations for solving it, then solve it. This is often a hard part in math problems in my view.",,"['ordinary-differential-equations', 'mathematical-modeling']"
28,General solution for $y^{iv}+ 2y''+y=\cos x$,General solution for,y^{iv}+ 2y''+y=\cos x,"Here is another problem from Mathews and Walker that has given me some trouble. 1-18 . Find the general solution of   $y^{iv}+ 2y''+y=\cos x$. Note: Thanks, everyone, for clearing up the interpretation of $y^{iv}$ as the fourth derivative of $y$ and for the clear solutions.  I had interpreted $y^{iv}$ as $y^{\sqrt{-1} \ v}$ with $v\in \mathbb{C}$.  Of course, this is an awful nonlinear DE!","Here is another problem from Mathews and Walker that has given me some trouble. 1-18 . Find the general solution of   $y^{iv}+ 2y''+y=\cos x$. Note: Thanks, everyone, for clearing up the interpretation of $y^{iv}$ as the fourth derivative of $y$ and for the clear solutions.  I had interpreted $y^{iv}$ as $y^{\sqrt{-1} \ v}$ with $v\in \mathbb{C}$.  Of course, this is an awful nonlinear DE!",,"['ordinary-differential-equations', 'physics']"
29,General solution of $yy'' - (y')^2 + y' = 0$,General solution of,yy'' - (y')^2 + y' = 0,How do you derive the general solution of $$ y y'' - (y')^2 + y' = 0 $$ Thanks. ====== Never mind.  Solved. Let $u = y'$.  Then $y'' = u' = \frac{du}{dy}$ . $y' = \frac{du}{dy} . u$ Hence the equation is $$ yu \cdot\frac{du}{dy} = u^2 - u $$ which is separable.,How do you derive the general solution of $$ y y'' - (y')^2 + y' = 0 $$ Thanks. ====== Never mind.  Solved. Let $u = y'$.  Then $y'' = u' = \frac{du}{dy}$ . $y' = \frac{du}{dy} . u$ Hence the equation is $$ yu \cdot\frac{du}{dy} = u^2 - u $$ which is separable.,,['ordinary-differential-equations']
30,Differential equation involving Newton's law of gravitation,Differential equation involving Newton's law of gravitation,,"Newton's law of gravitation states that the acceleration of an object at a distance $r$ from the centre of an object of mass $M$ is given by $$\frac{d^2r}{dt^2}=-\frac{GM}{r^2},$$ where $G$ is the universal gravitational constant. (a) Use the identity $$\frac{d^2r}{dt^2}=\frac{d}{dr}\left(\frac12v^2\right),$$ combined with integration with respect to $r$ . Determine the resulting constant of integration using the condition $u=v(R)$ and show that $$v^2-u^2=\frac{2GM}{r}-\frac{2GM}{R}$$ (b) Now write $r=R+s$ where $s$ is the height of the object above the surface of the Earth, radius $R$ and mass $M$ . Use the binomial series to expand the factor $(1+s/R)^{-1}$ to show that, close to the surface of the Earth, $$v^2\approx u^2-2gs,$$ for some constant $g$ . Find the expression for $g$ . Reminder: The binomial series is $(1+x)^{-1}=1-x+x^2-x^3+\cdots$ , which converges for $|x|\lt 1$ . (In case it is unclear, I am assuming that $G$ and $M$ are constants, and $v$ is purely a function of $r$ , since that's what the question appears to mean.) So far I have done the following: I equated the two expressions for $\frac{d^2r}{dt^2}$ (which I'm not sure is correct, since if two functions have the same derivative, they may differ by a constant, so if their second derivatives are equal then I feel as though there should be two constants, but I haven't included any constants), then integrated both sides with respect to $r$ . $$\int \frac{d}{dr} \left(\frac12v^2\right) dr = -GM \int \frac{1}{r^2} dr$$ $$\implies\frac12v^2=\frac{GM}{r}+C$$ $$\implies v^2=\frac{2GM}{r}+C$$ $$\implies v=\sqrt{\frac{2GM}{r}+C}$$ $$u=v(R)=\sqrt{\frac{2GM}{R}+C}$$ $$\implies u^2=\frac{2GM}{R}+C$$ Since the constants are the same, they cancel when subtracted. $$v^2-u^2=\frac{2GM}{r}-\frac{2GM}{R}$$ I'm doubtful if this method is correct because of the problem with the constants I referred to before, and also because I couldn't ""determine the resulting constant of integration using the condition $u=v(R)$ "" as the question asked, rather I just cancelled the constants with subtraction. Is equating the two expressions for $\frac{d^2r}{dt^2}$ allowed? As for part (b), I think I need the constant of integration from (a) to get the full expression for $v^2$ , which I do not know how to find.","Newton's law of gravitation states that the acceleration of an object at a distance from the centre of an object of mass is given by where is the universal gravitational constant. (a) Use the identity combined with integration with respect to . Determine the resulting constant of integration using the condition and show that (b) Now write where is the height of the object above the surface of the Earth, radius and mass . Use the binomial series to expand the factor to show that, close to the surface of the Earth, for some constant . Find the expression for . Reminder: The binomial series is , which converges for . (In case it is unclear, I am assuming that and are constants, and is purely a function of , since that's what the question appears to mean.) So far I have done the following: I equated the two expressions for (which I'm not sure is correct, since if two functions have the same derivative, they may differ by a constant, so if their second derivatives are equal then I feel as though there should be two constants, but I haven't included any constants), then integrated both sides with respect to . Since the constants are the same, they cancel when subtracted. I'm doubtful if this method is correct because of the problem with the constants I referred to before, and also because I couldn't ""determine the resulting constant of integration using the condition "" as the question asked, rather I just cancelled the constants with subtraction. Is equating the two expressions for allowed? As for part (b), I think I need the constant of integration from (a) to get the full expression for , which I do not know how to find.","r M \frac{d^2r}{dt^2}=-\frac{GM}{r^2}, G \frac{d^2r}{dt^2}=\frac{d}{dr}\left(\frac12v^2\right), r u=v(R) v^2-u^2=\frac{2GM}{r}-\frac{2GM}{R} r=R+s s R M (1+s/R)^{-1} v^2\approx u^2-2gs, g g (1+x)^{-1}=1-x+x^2-x^3+\cdots |x|\lt 1 G M v r \frac{d^2r}{dt^2} r \int \frac{d}{dr} \left(\frac12v^2\right) dr = -GM \int \frac{1}{r^2} dr \implies\frac12v^2=\frac{GM}{r}+C \implies v^2=\frac{2GM}{r}+C \implies v=\sqrt{\frac{2GM}{r}+C} u=v(R)=\sqrt{\frac{2GM}{R}+C} \implies u^2=\frac{2GM}{R}+C v^2-u^2=\frac{2GM}{r}-\frac{2GM}{R} u=v(R) \frac{d^2r}{dt^2} v^2",['ordinary-differential-equations']
31,What type of differential equation is $f'(x) = f(x/2)$ and how do you solve it?,What type of differential equation is  and how do you solve it?,f'(x) = f(x/2),"I have the following different equation $$f'(x) = f(x/2)$$ with $f(0)=10$ . What type of DE is this, and how would you solve it? It seems $f(x)$ is likely to be some relative of $e^x$ , since $f'(x) = f(x)$ , which is close, but I don't even know what that type of DE is called with that $f(x/2)$ feature, so I'm not having any luck searching for a tutorial. The best candidate I've found was a ""delay differential equation"", but that seems more suited to $f(x-3)$ than $f(x/2)$ .","I have the following different equation with . What type of DE is this, and how would you solve it? It seems is likely to be some relative of , since , which is close, but I don't even know what that type of DE is called with that feature, so I'm not having any luck searching for a tutorial. The best candidate I've found was a ""delay differential equation"", but that seems more suited to than .",f'(x) = f(x/2) f(0)=10 f(x) e^x f'(x) = f(x) f(x/2) f(x-3) f(x/2),['ordinary-differential-equations']
32,How to remove this numerical artifact?,How to remove this numerical artifact?,,"I am trying to solve a differential equation: $$\frac{d f}{d\theta} = \frac{1}{c}(\text{max}(\sin\theta, 0) - f^4)~,$$ subject to periodic boundary condition, whic would imply $f(0)=f(2\pi)$ and $f'(0)= f'(2\pi)$ . To solve this numerically, I have set up an equation: $$f_i = f_{i-1}+\frac{1}{c}(\theta_i-\theta_{i-1})\left(\max(\sin\theta_i,0)-f_{i-1}^4\right)~.$$ Now, I want to solve this for specific grids. Suppose, I set up my grid points in $\theta = (0, 2\pi)$ to be $n$ equally spaced floats. Then I have small python program which would calculate $f$ for each grid points in $\theta$ . Here is the program: import numpy as np import matplotlib.pyplot as plt n=100 m = 500 a = np.linspace(0.01, 2*np.pi, n) b = np.linspace(0.01, 2*np.pi, m) arr = np.sin(a) arr1 = np.sin(b) index = np.where(arr<0) index1 = np.where(arr1<0) arr[index] = 0 arr1[index1] = 0 epsilon = 0.03 final_arr_l = np.ones(arr1.size) final_arr_n = np.ones(arr.size) for j in range(1,100*arr.size):     i = j%arr.size     step = final_arr_n[i-1]+ 1./epsilon*2*np.pi/n*(arr[i] - final_arr_n[i-1]**4)     if (step>=0):         final_arr_n[i] = step     else:         final_arr_n[i] = 0.2*final_arr_n[i-1] for j in range(1,100*arr1.size):     i = j%arr1.size     final_arr_l[i] = final_arr_l[i-1]+1./epsilon*2*np.pi/m*(arr1[i] - final_arr_l[i-1]**4)  plt.plot(b, final_arr_l) plt.plot(a, final_arr_n) plt.grid(); plt.show() My major problem is for small $c$ , in the above case when $c=0.03$ , the numerical equation does not converge to a reasonable value (it is highly oscillatory) if I choose $N$ to be not so large. The main reason for that is since $\frac{1}{c}*(\theta_i-\theta_{i-1})>1$ , $f$ tends to be driven to negative infinity when $N$ is not so large, i.e. $\theta_i-\theta_{i-1}$ is not so small. Here is an example with $c=0.03$ showing the behaviour when $N=100$ versus $N=500$ . In my code, I have applied some adhoc criteria for small $N$ to avoid divergences: step = final_arr_n[i-1]+ 1./epsilon*2*np.pi/n*(max(np.sin(a[i]), 0) - final_arr_n[i-1]**4) if (step>=0):     final_arr_n[i] = step else:     final_arr_n[i] = 0.2*final_arr_n[i-1] what I would like to know: is there any good mathematical trick to solve this numerical equation with not so large $N$ and still make it converge for small $c$ ?","I am trying to solve a differential equation: subject to periodic boundary condition, whic would imply and . To solve this numerically, I have set up an equation: Now, I want to solve this for specific grids. Suppose, I set up my grid points in to be equally spaced floats. Then I have small python program which would calculate for each grid points in . Here is the program: import numpy as np import matplotlib.pyplot as plt n=100 m = 500 a = np.linspace(0.01, 2*np.pi, n) b = np.linspace(0.01, 2*np.pi, m) arr = np.sin(a) arr1 = np.sin(b) index = np.where(arr<0) index1 = np.where(arr1<0) arr[index] = 0 arr1[index1] = 0 epsilon = 0.03 final_arr_l = np.ones(arr1.size) final_arr_n = np.ones(arr.size) for j in range(1,100*arr.size):     i = j%arr.size     step = final_arr_n[i-1]+ 1./epsilon*2*np.pi/n*(arr[i] - final_arr_n[i-1]**4)     if (step>=0):         final_arr_n[i] = step     else:         final_arr_n[i] = 0.2*final_arr_n[i-1] for j in range(1,100*arr1.size):     i = j%arr1.size     final_arr_l[i] = final_arr_l[i-1]+1./epsilon*2*np.pi/m*(arr1[i] - final_arr_l[i-1]**4)  plt.plot(b, final_arr_l) plt.plot(a, final_arr_n) plt.grid(); plt.show() My major problem is for small , in the above case when , the numerical equation does not converge to a reasonable value (it is highly oscillatory) if I choose to be not so large. The main reason for that is since , tends to be driven to negative infinity when is not so large, i.e. is not so small. Here is an example with showing the behaviour when versus . In my code, I have applied some adhoc criteria for small to avoid divergences: step = final_arr_n[i-1]+ 1./epsilon*2*np.pi/n*(max(np.sin(a[i]), 0) - final_arr_n[i-1]**4) if (step>=0):     final_arr_n[i] = step else:     final_arr_n[i] = 0.2*final_arr_n[i-1] what I would like to know: is there any good mathematical trick to solve this numerical equation with not so large and still make it converge for small ?","\frac{d f}{d\theta} = \frac{1}{c}(\text{max}(\sin\theta, 0) - f^4)~, f(0)=f(2\pi) f'(0)= f'(2\pi) f_i = f_{i-1}+\frac{1}{c}(\theta_i-\theta_{i-1})\left(\max(\sin\theta_i,0)-f_{i-1}^4\right)~. \theta = (0, 2\pi) n f \theta c c=0.03 N \frac{1}{c}*(\theta_i-\theta_{i-1})>1 f N \theta_i-\theta_{i-1} c=0.03 N=100 N=500 N N c","['ordinary-differential-equations', 'convergence-divergence', 'numerical-methods']"
33,First Order Differential equation - separable form,First Order Differential equation - separable form,,I am trying to evaluate the equation: $$y'=y\left(y^2-\frac12\right)$$ I multiplied the y over and tried to solve it in seperable form (M and N). The partial deritives did not work out to be equal to eachother so I am now stuck finding an integrating factor. Is this the right approach?,I am trying to evaluate the equation: I multiplied the y over and tried to solve it in seperable form (M and N). The partial deritives did not work out to be equal to eachother so I am now stuck finding an integrating factor. Is this the right approach?,y'=y\left(y^2-\frac12\right),['ordinary-differential-equations']
34,Checking Lyapunov stability of non linear system,Checking Lyapunov stability of non linear system,,"I need to check the stability of the equilibrium point of the following system, $n \in \Bbb N$: $$ \left\{  \begin{array} \dot \dot x_1=x_2 \\  \dot x_2=-x_1^n  \end{array} \right.  $$ I tried using linearization, but the eigenvalues are zero, which means it's not the way to go. I also searched for a Lyapunov function, but couldn't find one. Any ideas?","I need to check the stability of the equilibrium point of the following system, $n \in \Bbb N$: $$ \left\{  \begin{array} \dot \dot x_1=x_2 \\  \dot x_2=-x_1^n  \end{array} \right.  $$ I tried using linearization, but the eigenvalues are zero, which means it's not the way to go. I also searched for a Lyapunov function, but couldn't find one. Any ideas?",,"['ordinary-differential-equations', 'stability-in-odes', 'lyapunov-functions']"
35,How to solve $\dot x(t) = x(2t)$ [duplicate],How to solve  [duplicate],\dot x(t) = x(2t),This question already has an answer here : What function satisfies $F'(x) = F(2x)$? (1 answer) Closed 6 years ago . During my recent work I encountered a weird type of differential equation containing scalar factors. In order to understand them better I thought it would make sense to look at a simpler example like: $$ \dot x(t) = x(2t)$$ How does one solve this equation? Does it even have a solution? It seems kinda related to Functional Differential Equations which I am not familiar with.,This question already has an answer here : What function satisfies $F'(x) = F(2x)$? (1 answer) Closed 6 years ago . During my recent work I encountered a weird type of differential equation containing scalar factors. In order to understand them better I thought it would make sense to look at a simpler example like: $$ \dot x(t) = x(2t)$$ How does one solve this equation? Does it even have a solution? It seems kinda related to Functional Differential Equations which I am not familiar with.,,"['ordinary-differential-equations', 'functions', 'reference-request', 'functional-equations']"
36,"How do I get the $f(t_{n+1}, y_{n+1})$ needed to use the implicit Euler method?",How do I get the  needed to use the implicit Euler method?,"f(t_{n+1}, y_{n+1})","I have to solve a system of 1st order ODEs which are implicit. I know the formula for Explicit or forward Euler method is: $$y_{n+1}= y_n + hf(t_n, y_n),$$ whereas the formula for implicit or backward Euler method is $$y_{n+1}= y_n + hf(t_{n+1}, y_{n+1}).$$ In order to use the implicit Euler method, how can i get the value of $f(t_{n+1}, y_{n+1})$ ? Can I use the forward Euler method to get the value $y_{n+1}$ then substitute in backward Euler formula?","I have to solve a system of 1st order ODEs which are implicit. I know the formula for Explicit or forward Euler method is: whereas the formula for implicit or backward Euler method is In order to use the implicit Euler method, how can i get the value of ? Can I use the forward Euler method to get the value then substitute in backward Euler formula?","y_{n+1}= y_n + hf(t_n, y_n), y_{n+1}= y_n + hf(t_{n+1}, y_{n+1}). f(t_{n+1}, y_{n+1}) y_{n+1}","['ordinary-differential-equations', 'numerical-methods']"
37,Estimating Poincare constant for unit interval,Estimating Poincare constant for unit interval,,"I want to show that the Poincare constant for the $W^{1,2}_0(0,1)$ is smaller than $1$. More specifically, I want to show that there is a constant $C<1$ such that for any $f\in C^\infty_c(0,1)$ (compactly supported smooth) we have the inequality $$ \lVert f\rVert\leq C\lVert f'\lVert $$ where $\lVert\cdot\rVert$ is the $L^2$ norm. The proof of Poincare inequality that I know (using Cauchy-Schwarz) gives an estimate of $C=2$, while the Wikipedia article seems to say that optimally $C\leq \pi^{-1}$. I'm looking for a simple proof for this special case. I don't need a very sharp estimate, just smaller than $1$, and would appreciate a hint or a reference.","I want to show that the Poincare constant for the $W^{1,2}_0(0,1)$ is smaller than $1$. More specifically, I want to show that there is a constant $C<1$ such that for any $f\in C^\infty_c(0,1)$ (compactly supported smooth) we have the inequality $$ \lVert f\rVert\leq C\lVert f'\lVert $$ where $\lVert\cdot\rVert$ is the $L^2$ norm. The proof of Poincare inequality that I know (using Cauchy-Schwarz) gives an estimate of $C=2$, while the Wikipedia article seems to say that optimally $C\leq \pi^{-1}$. I'm looking for a simple proof for this special case. I don't need a very sharp estimate, just smaller than $1$, and would appreciate a hint or a reference.",,"['ordinary-differential-equations', 'sobolev-spaces']"
38,two identical point charges can't collide,two identical point charges can't collide,,"I've convinced myself intuitively that if you place two massless classical particles with the same charge in $\mathbb{R}^n$, with arbitrary initial velocities and (distinct) positions, they will never collide. However, I'm have a heck of a time trying to prove it, and would appreciate some help. Formally, consider $q_1, q_2: \mathbb{R} \rightarrow \mathbb{R}^n$ satisfying $$\ddot{q_i} = \frac{1}{\|q_i - q_j\|^3} (q_i - q_j)$$With $q_1(0) \neq q_2(0)$. The claim is that $q_1(t) \neq q_2(t)$ for all $t > 0$. So my questions are (i) is this true? (ii) what happens if we replace the exponent 3 in the denominator with say $\alpha > 0$ ? N.B. The question's already a bit long, but I'd be happy to post my thoughts so far. Edit All the answers were very helpful, thanks so much everyone!","I've convinced myself intuitively that if you place two massless classical particles with the same charge in $\mathbb{R}^n$, with arbitrary initial velocities and (distinct) positions, they will never collide. However, I'm have a heck of a time trying to prove it, and would appreciate some help. Formally, consider $q_1, q_2: \mathbb{R} \rightarrow \mathbb{R}^n$ satisfying $$\ddot{q_i} = \frac{1}{\|q_i - q_j\|^3} (q_i - q_j)$$With $q_1(0) \neq q_2(0)$. The claim is that $q_1(t) \neq q_2(t)$ for all $t > 0$. So my questions are (i) is this true? (ii) what happens if we replace the exponent 3 in the denominator with say $\alpha > 0$ ? N.B. The question's already a bit long, but I'd be happy to post my thoughts so far. Edit All the answers were very helpful, thanks so much everyone!",,"['ordinary-differential-equations', 'classical-mechanics']"
39,Is the Mellin transform useful to solve differential equations?,Is the Mellin transform useful to solve differential equations?,,"The Mellin transform is defined as: $$F(\mu)=\int_0^\infty f(x)x^{\mu-1}dx$$ The derivative of the Mellin transform is: $$F'(\mu)=-(\mu-1)F(\mu-1)$$ Applying this property, for example to the Bessel equation: $$x^2 y''+xy'+(x^2-\nu^2)y$$ we can transform it in the complex difference equation: $$Y(\mu + 2) = ±(\mu − \nu)(\mu + \nu)Y(\mu)$$ where $Y(\mu)$ is the Mellin transform of $y(x)$. Is this method useful in general to solve ODE? Thanks","The Mellin transform is defined as: $$F(\mu)=\int_0^\infty f(x)x^{\mu-1}dx$$ The derivative of the Mellin transform is: $$F'(\mu)=-(\mu-1)F(\mu-1)$$ Applying this property, for example to the Bessel equation: $$x^2 y''+xy'+(x^2-\nu^2)y$$ we can transform it in the complex difference equation: $$Y(\mu + 2) = ±(\mu − \nu)(\mu + \nu)Y(\mu)$$ where $Y(\mu)$ is the Mellin transform of $y(x)$. Is this method useful in general to solve ODE? Thanks",,"['ordinary-differential-equations', 'integral-transforms']"
40,Treacherous Euler-Lagrange equation,Treacherous Euler-Lagrange equation,,"If I have an Euler-Lagrange equation: $(y')^2 = 2 (1-\cos(y))$ where $y$ is a function of $x$ subjected to boundary conditions $y(x) \to 0$ as $x \to -\infty$ and $y(x) \to 2\pi$ as $x \to +\infty$, how might I find all its solutions? I can't seem to directly integrate the equation and sub in the conditions... Please help! Thanks.","If I have an Euler-Lagrange equation: $(y')^2 = 2 (1-\cos(y))$ where $y$ is a function of $x$ subjected to boundary conditions $y(x) \to 0$ as $x \to -\infty$ and $y(x) \to 2\pi$ as $x \to +\infty$, how might I find all its solutions? I can't seem to directly integrate the equation and sub in the conditions... Please help! Thanks.",,"['ordinary-differential-equations', 'calculus-of-variations']"
41,Understanding Finite element method,Understanding Finite element method,,"Suppose we have Poisson in 1D: $u'' = f(t)$ where $0<t<1$ and $u(0)=0$ and $u(1)=1$ We approximate the solution by $U(t) \approx \sum_{i=1}^n x_i \phi_i(t) $ where $\phi_i(t)$ are some basis functions. Once we put this into our equation we get some residual $r(x_i,t) = U''(t) - f(t) $ . The idea is to multiply by some ${\bf weight }$ function and solve $$ \int\limits_0^1 r(x_i,t) w(t) d t $$ In the Galerkin method, we take $w(t) = \phi_i(t)$ and use integration by parts to derive a system of equations where we find $x_1,...,x_n$ . The question is Suppose the hat function is used as the shape function (piecewise linear shape function) and the domain is decomposed into subintervals of lenght $h=0.2$ Compute the element stiffness matrix ${\bf K}$ If I understood correctly, we use $$ \phi_i(x) = \begin{cases} \frac{ x - x_{i-1} }{h} & x_{i-1} \leq x \leq x_i \\ \frac{x_{i+1}-x}{h} & x_i \leq x \leq x_{i+1} \end{cases} $$ as our basis functions. We are given that $$ K_{ij} = \sum_{e} K_{ij}^e $$ where $K_{ij}^e$ is the element stiffness matrix for element $\Omega_e$ . In our case, we have $5$ elements as $h=0.2$ . For example, for the first element: $[x_0,x_1]$ we have $$ K_{ij}^1 = \int\limits_{x_0}^{x_1} ( \phi_i' \phi_j' + \phi_i \phi_j) dx $$ Is this how finite element method works?","Suppose we have Poisson in 1D: where and and We approximate the solution by where are some basis functions. Once we put this into our equation we get some residual . The idea is to multiply by some function and solve In the Galerkin method, we take and use integration by parts to derive a system of equations where we find . The question is Suppose the hat function is used as the shape function (piecewise linear shape function) and the domain is decomposed into subintervals of lenght Compute the element stiffness matrix If I understood correctly, we use as our basis functions. We are given that where is the element stiffness matrix for element . In our case, we have elements as . For example, for the first element: we have Is this how finite element method works?","u'' = f(t) 0<t<1 u(0)=0 u(1)=1 U(t) \approx \sum_{i=1}^n x_i \phi_i(t)  \phi_i(t) r(x_i,t) = U''(t) - f(t)  {\bf weight }  \int\limits_0^1 r(x_i,t) w(t) d t  w(t) = \phi_i(t) x_1,...,x_n h=0.2 {\bf K}  \phi_i(x) = \begin{cases} \frac{ x - x_{i-1} }{h} & x_{i-1} \leq x \leq x_i \\ \frac{x_{i+1}-x}{h} & x_i \leq x \leq x_{i+1} \end{cases}   K_{ij} = \sum_{e} K_{ij}^e  K_{ij}^e \Omega_e 5 h=0.2 [x_0,x_1]  K_{ij}^1 = \int\limits_{x_0}^{x_1} ( \phi_i' \phi_j' + \phi_i \phi_j) dx ","['ordinary-differential-equations', 'numerical-methods', 'boundary-value-problem', 'finite-element-method']"
42,Sturm-Liouville differential equation eigenvalue problem,Sturm-Liouville differential equation eigenvalue problem,,If we have a Sturm-Liouville differential equation of the form $$ \frac{d}{dx}[p(x)\frac{dy}{dx}]+q(x)y=-\lambda w(x)y $$ and define the linear operator $L$ as $$L(u) = \frac{d}{dx}[p(x)\frac{du}{dx}]+q(x)u $$ then we get the equation $L(y)=-\lambda w(x)y$ which defines what is called the eigenvalue problem of the Sturm-Liouville differential equation. My question : why is it called that way despite the fact that there is still a function $w(x)$ in the equation? I thought an eigenvalue problem would have the form $L(y)=-\lambda y$ . What's happening here?,If we have a Sturm-Liouville differential equation of the form and define the linear operator as then we get the equation which defines what is called the eigenvalue problem of the Sturm-Liouville differential equation. My question : why is it called that way despite the fact that there is still a function in the equation? I thought an eigenvalue problem would have the form . What's happening here?,"
\frac{d}{dx}[p(x)\frac{dy}{dx}]+q(x)y=-\lambda w(x)y
 L L(u) = \frac{d}{dx}[p(x)\frac{du}{dx}]+q(x)u
 L(y)=-\lambda w(x)y w(x) L(y)=-\lambda y","['ordinary-differential-equations', 'analysis', 'eigenvalues-eigenvectors', 'sturm-liouville']"
43,How to solve ode of the form $ a_n(y')^n + a_{n-1}(y')^{n-1} + \cdots + a_1 y' + a_0 =0 $,How to solve ode of the form, a_n(y')^n + a_{n-1}(y')^{n-1} + \cdots + a_1 y' + a_0 =0 ,$ a_n(y')^n + a_{n-1}(y')^{n-1} + \cdots + a_1 y'+a_0 =0 $ I am also unclear on how to describe this as it is not nth order. The polynomial being in the derivative is not something that I think I have seen before.,I am also unclear on how to describe this as it is not nth order. The polynomial being in the derivative is not something that I think I have seen before.,"
a_n(y')^n + a_{n-1}(y')^{n-1} + \cdots + a_1 y'+a_0 =0
",['ordinary-differential-equations']
44,Solutions of a differential equation,Solutions of a differential equation,,"I'm trying to solve the following differential equation and I'm stuck at what it appears to be simple calculations. I'm terribly sorry if this turns out to be really simple. $(1)$ $X(f)=2f$ where $X=x_1^2 \frac \partial {\partial x_1}-x_2^2 \frac \partial {\partial x_2}$ in $\Bbb R^2$ with the identity chart $Id_{\Bbb R^2}=(x_1,x_2)$ and $f:\Bbb R^2 \to \Bbb R$, $(2)$ $f(cosθ,sinθ)=cosθ+sinθ$. Let $φ^Χ_t(p)=(\frac {x}{1-tx},\frac {y}{1+ty})$, where $p=(x,y)$, be the flow of $Χ$ and by denoting $h(t)=f(φ^Χ_t(p))$ we can make $(1)$ look like $h'(t)=2h(t)$ which can be easily solved to: $e^{2t}f(x,y)=f(\frac {x}{1-tx},\frac {y}{1+ty})$ Then by use of the initial condition $(2)$ we have $e^{2t}(cosθ+sinθ)=f(\frac {cosθ}{1-tcosθ},\frac {sinθ}{1+tsinθ})$ (this is as far as I can go) I tried setting $u = \frac {cosθ}{1-tcosθ}, v=\frac {sinθ}{1+tsinθ} $ but I haven't been able to isolate $u,v$ from $θ, t$ Can you give me any hints? Is there any trick I'm not thinking of?","I'm trying to solve the following differential equation and I'm stuck at what it appears to be simple calculations. I'm terribly sorry if this turns out to be really simple. $(1)$ $X(f)=2f$ where $X=x_1^2 \frac \partial {\partial x_1}-x_2^2 \frac \partial {\partial x_2}$ in $\Bbb R^2$ with the identity chart $Id_{\Bbb R^2}=(x_1,x_2)$ and $f:\Bbb R^2 \to \Bbb R$, $(2)$ $f(cosθ,sinθ)=cosθ+sinθ$. Let $φ^Χ_t(p)=(\frac {x}{1-tx},\frac {y}{1+ty})$, where $p=(x,y)$, be the flow of $Χ$ and by denoting $h(t)=f(φ^Χ_t(p))$ we can make $(1)$ look like $h'(t)=2h(t)$ which can be easily solved to: $e^{2t}f(x,y)=f(\frac {x}{1-tx},\frac {y}{1+ty})$ Then by use of the initial condition $(2)$ we have $e^{2t}(cosθ+sinθ)=f(\frac {cosθ}{1-tcosθ},\frac {sinθ}{1+tsinθ})$ (this is as far as I can go) I tried setting $u = \frac {cosθ}{1-tcosθ}, v=\frac {sinθ}{1+tsinθ} $ but I haven't been able to isolate $u,v$ from $θ, t$ Can you give me any hints? Is there any trick I'm not thinking of?",,"['ordinary-differential-equations', 'trigonometry', 'differential-geometry', 'partial-differential-equations']"
45,Differential Equation with Shifted Input,Differential Equation with Shifted Input,,"So I'm fairly new to differential equations, and while tinkering around with graphs, I came up with one that is confusing me quite a bit. I have both a simpler form and a more complicated form. The simpler one is: $$ f(x)=f'(x-c) $$ I see that when $c=0$, a solution is $f(x)=e^x,$ and that when $c=\frac{\pi}{2},$ solutions are $f(x) = \sin x$ and $f(x) = \cos x,$ but I don't see how to generalize it to any c. Conceptually, I view this equation as saying that when you take the derivative, the function is shifted over by $c.$ [ Edit : I couldn't get anywhere assuming the function was a sinusoidal, but I was able to get some results assuming an exponential: If we assume $f(x) = Ae^{bx}$, where $A$ and $b$ are real constants, we get the equation $$ Ae^{bx}=Abe^{b(x-c)} $$ Taking the natural log of both sides and simplifying yields the following: $$ c=\frac{\ln b}{b} $$ However, I can't figure out how to solve this for $b.$ ] The more complicated form is: $$ f(x)=f^{(n)}(x-nc) $$ This reduces to the simpler case when $n = 1,$ and when $c=\frac{\pi}{2},$ regardless of $n,$ solutions are again $f(x) = \sin x$ and $f(x) = \cos x$. However, again, I don't see how to generalize to both $n$ and $c,$ nor how to determine whether such a general solution exists. Conceptually, I view this equation as saying that each time you take the derivative of a function, it gets shifted $c$.","So I'm fairly new to differential equations, and while tinkering around with graphs, I came up with one that is confusing me quite a bit. I have both a simpler form and a more complicated form. The simpler one is: $$ f(x)=f'(x-c) $$ I see that when $c=0$, a solution is $f(x)=e^x,$ and that when $c=\frac{\pi}{2},$ solutions are $f(x) = \sin x$ and $f(x) = \cos x,$ but I don't see how to generalize it to any c. Conceptually, I view this equation as saying that when you take the derivative, the function is shifted over by $c.$ [ Edit : I couldn't get anywhere assuming the function was a sinusoidal, but I was able to get some results assuming an exponential: If we assume $f(x) = Ae^{bx}$, where $A$ and $b$ are real constants, we get the equation $$ Ae^{bx}=Abe^{b(x-c)} $$ Taking the natural log of both sides and simplifying yields the following: $$ c=\frac{\ln b}{b} $$ However, I can't figure out how to solve this for $b.$ ] The more complicated form is: $$ f(x)=f^{(n)}(x-nc) $$ This reduces to the simpler case when $n = 1,$ and when $c=\frac{\pi}{2},$ regardless of $n,$ solutions are again $f(x) = \sin x$ and $f(x) = \cos x$. However, again, I don't see how to generalize to both $n$ and $c,$ nor how to determine whether such a general solution exists. Conceptually, I view this equation as saying that each time you take the derivative of a function, it gets shifted $c$.",,['ordinary-differential-equations']
46,Showing that all solutions to the ODE $x''=4x^3$ cease to exist after a finite time,Showing that all solutions to the ODE  cease to exist after a finite time,x''=4x^3,"I am given the ""Newton equation"" $$x''(t)=4x^3(t)$$ The first part asks for finding a ""conservation of energy"" law. I'll spare the details: the conclusion is that every solution has a constant $C$ such that $\frac{y^2}{2}-x^4=C$ where $y(t)=x'(t)$. (That follows from writing the ODE as a system of two ODEs: $x'=y$ and $y'=4x^3$ which is exact). The second part asks to show that except the trivial solution $x\equiv0$, all solutions cease to exist (and therefore tend to infinity) after a finite time. Now, this is something that can't be inferred by knowing the relation between $x$ and $y$ because it relies on their dependence on $t$, so I suspect I don't have the means to solve it. I'd like to hear what is the technique for these questions so I can do the rest of the problems in my homework that contain similar questions (like, which solutions are bounded). Thanks!","I am given the ""Newton equation"" $$x''(t)=4x^3(t)$$ The first part asks for finding a ""conservation of energy"" law. I'll spare the details: the conclusion is that every solution has a constant $C$ such that $\frac{y^2}{2}-x^4=C$ where $y(t)=x'(t)$. (That follows from writing the ODE as a system of two ODEs: $x'=y$ and $y'=4x^3$ which is exact). The second part asks to show that except the trivial solution $x\equiv0$, all solutions cease to exist (and therefore tend to infinity) after a finite time. Now, this is something that can't be inferred by knowing the relation between $x$ and $y$ because it relies on their dependence on $t$, so I suspect I don't have the means to solve it. I'd like to hear what is the technique for these questions so I can do the rest of the problems in my homework that contain similar questions (like, which solutions are bounded). Thanks!",,['ordinary-differential-equations']
47,How do I solve $xy'= x^2+y^2$?,How do I solve ?,xy'= x^2+y^2,"I have: $$xy'= x^2+y^2$$ I tried to separate variables but it did not work, checked if it could be homogeneous equation but it is not (obiviously), all my transformations did not give me linear equation, so I have no clue how to approach it now :(","I have: $$xy'= x^2+y^2$$ I tried to separate variables but it did not work, checked if it could be homogeneous equation but it is not (obiviously), all my transformations did not give me linear equation, so I have no clue how to approach it now :(",,['ordinary-differential-equations']
48,Solving non-linear ordinary differential equation: $(y'')^2-y'y'''=\left(\frac{y'}{x}\right)^2$,Solving non-linear ordinary differential equation:,(y'')^2-y'y'''=\left(\frac{y'}{x}\right)^2,$$(y'')^2-y'y'''=\left(\frac{y'}{x}\right)^2$$ I have been struggling to solve this equation by doing following simplifications: $$y'''y'=(y'')^2-\left(\frac{y'}{x}\right)^2$$ Dividing by $y'$: $$y'''=\frac{(y'')^2}{y'}-\frac{y'}{x^2}$$ Dividing by $y''$: $$\frac{y'''}{y''}=\frac{y''}{y'}-\frac{y'}{y''x^2}$$ I can't figure out what goes next since I have never encountered with the last fraction. I would really appreciate any help on the matter.,$$(y'')^2-y'y'''=\left(\frac{y'}{x}\right)^2$$ I have been struggling to solve this equation by doing following simplifications: $$y'''y'=(y'')^2-\left(\frac{y'}{x}\right)^2$$ Dividing by $y'$: $$y'''=\frac{(y'')^2}{y'}-\frac{y'}{x^2}$$ Dividing by $y''$: $$\frac{y'''}{y''}=\frac{y''}{y'}-\frac{y'}{y''x^2}$$ I can't figure out what goes next since I have never encountered with the last fraction. I would really appreciate any help on the matter.,,['ordinary-differential-equations']
49,A solution of a second order homogenous ODE has infinite zeros on a closed interval is $y=0$,A solution of a second order homogenous ODE has infinite zeros on a closed interval is,y=0,"We have the ODE $y''+p(x)y'+q(x)y=0$, the functions $p(x)$ and $q(x)$   continuous on a closed interval $[a,b]$. Prove that if the solution   $y(x)$ has an infinite number of roots ($x$s such that $y(x)=0$) on   the interval $[a,b]$, then $y(x)=0$. I tried negating the claim, saying there is an $x_0$ such that $y(x_0)\neq 0$, but got nowhere. A hint or a direction of thought would be appreciated.","We have the ODE $y''+p(x)y'+q(x)y=0$, the functions $p(x)$ and $q(x)$   continuous on a closed interval $[a,b]$. Prove that if the solution   $y(x)$ has an infinite number of roots ($x$s such that $y(x)=0$) on   the interval $[a,b]$, then $y(x)=0$. I tried negating the claim, saying there is an $x_0$ such that $y(x_0)\neq 0$, but got nowhere. A hint or a direction of thought would be appreciated.",,['ordinary-differential-equations']
50,Euler Lagrange differential equation.,Euler Lagrange differential equation.,,"The Lagrange equation is a second order differential equation. However is it an ordinary or partial differential equation? Looking at wikipedia it says it is both, here it is a PDE and here it is an ODE. So which is it?  \begin{equation} \frac{d}{dt}\bigg(\frac{\partial \mathscr L}{\partial \dot q}\bigg)-\frac{\partial \mathscr L}{\partial q}=0 \end{equation} In classical mechanics?","The Lagrange equation is a second order differential equation. However is it an ordinary or partial differential equation? Looking at wikipedia it says it is both, here it is a PDE and here it is an ODE. So which is it?  \begin{equation} \frac{d}{dt}\bigg(\frac{\partial \mathscr L}{\partial \dot q}\bigg)-\frac{\partial \mathscr L}{\partial q}=0 \end{equation} In classical mechanics?",,"['ordinary-differential-equations', 'euler-lagrange-equation']"
51,"How to show $ y′(t) + y(t) = h(t) $ has a bounded solution, if $h(t)$ is bounded","How to show  has a bounded solution, if  is bounded", y′(t) + y(t) = h(t)  h(t),"I have a midterm tomorrow, and while studying for that I saw this question, however don't have any idea how to solve it. (I could not come up with a legitimate proof. All I could do was, by putting some functions, approving what the problem claims.) I will appreciate if you can help. Suppose that $ h(t)$ is continuous and bounded on $(-\infty,\infty)$ and $$|h(t)| \leq M, \forall t\in (-\infty,\infty)$$ Show that equation $$ y′(t) + y(t) = h(t) $$ has one solution that is bounded on $(-\infty,\infty)$. Also, show that if $h(t)$ is a periodic function, then $y(t)$ is also periodic. Regards, Amadeus","I have a midterm tomorrow, and while studying for that I saw this question, however don't have any idea how to solve it. (I could not come up with a legitimate proof. All I could do was, by putting some functions, approving what the problem claims.) I will appreciate if you can help. Suppose that $ h(t)$ is continuous and bounded on $(-\infty,\infty)$ and $$|h(t)| \leq M, \forall t\in (-\infty,\infty)$$ Show that equation $$ y′(t) + y(t) = h(t) $$ has one solution that is bounded on $(-\infty,\infty)$. Also, show that if $h(t)$ is a periodic function, then $y(t)$ is also periodic. Regards, Amadeus",,['ordinary-differential-equations']
52,"Solve the boundary value problem $y''+y= -1$, $\,y(0)=y(\pi/2)=0$ with the Green's function method","Solve the boundary value problem ,  with the Green's function method","y''+y= -1 \,y(0)=y(\pi/2)=0","Using the Green's Function method solve the boundary value problem: $$ y''+ y= -1,$$ with boundary conditions $$y(0)=0, \quad y(\pi/2)=0.$$ Verify the result by elementary technique.","Using the Green's Function method solve the boundary value problem: $$ y''+ y= -1,$$ with boundary conditions $$y(0)=0, \quad y(\pi/2)=0.$$ Verify the result by elementary technique.",,"['ordinary-differential-equations', 'boundary-value-problem']"
53,Solution to over-damped harmonic spring,Solution to over-damped harmonic spring,,"(A kind soul at physics.stackexchange suggested I post here as well, sorry if out of bounds.) I'm trying to programmatically model a damped harmonic spring for use in mobile UI animations ( physics mathematics isn't my background, please pardon any misconceptions). Having derived the parameters for the general case equation, I can iteratively calculate values until I reach a suitable threshold, though because this is bound to ""simple"" trigonometric and $e^{x}$ functions on the CPU, the 4000-some-odd steps can cause about 0.25 seconds lag on slow devices while it calculates. I'd like to speed this up using my platform's super-optimized vector and BLAS/LAPACK variants. The requirement for doing this is precalculating the number of steps necessary to reach my threshold value. In the underdamped case, where the roots of the characteristic function of the differential equation are non-real, I can use algebraic tricks to get my values: $$x(t) = c_{1}e^{r_{1}}\cos(i_{1}t) + c_{2}e^{r_{2}}\sin(i_{2}t)$$ (Given $r_{1}$, $i_{1}$, $r_{2}$, and $i_{2}$ are the real and irrational components of my two roots, respectively.) Knowing that $r_{1} = r_{2}$ and $i_{1} = -i_{2}$, I can simplify to: $$x(t) = c_{1}e^{r_{1}}\cos(i_{1}t)$$ And get my desired value of $t$ for my threshold $a$: $$t = \arccos(a / c_{1} / e^{r_{1}}) / i_{1}$$ When the roots are real, the equation looks a lot simpler: $$x(t) = c_{1}e^{r_{1}} + c_{2}e^{r_{2}}$$ However, I don't have my trig functions floating around to help me solve it (even if I did, the irrational components being 0 would cause problems, of course). Take the concrete example on pages 3-4 of this document (my bible during this process), since they at least solve cleanly: $$x(t) = 1.5e^{-t} - 0.5e^{-3t}$$ I know how I would solve for $t$ to get my for when $x(t) = a$ on paper, by setting $x=e^{t}$, solving, and back substituting, but I don't have that luxury here. I can make a few assumptions: the roots and constants are all real. I'm always going to be looking for the smallest, first, positive value of $t$. Obviously, the iterative solution is the simplest for this case, but in the end that would involve more steps and therefore be slower no matter what my other optimizations would be. How, then, would I go about solving for my threshold value algorithmically in this (supposedly) simplified case? Addendum The underdamped solution presents an extra requirement. The motion curve will oscillate back and forth a few times across the endpoint. Therefore, ""first and lowest"" $t$ requirement is not necessarily true. In my current, iterative code, the threshold value is both checked against the distance from the current $x(t)$ to the endpoint, as well as to the distance from the previous $x(t)$ as well to allow for a number of oscillations. This might make a more efficient solution nearly impossible.","(A kind soul at physics.stackexchange suggested I post here as well, sorry if out of bounds.) I'm trying to programmatically model a damped harmonic spring for use in mobile UI animations ( physics mathematics isn't my background, please pardon any misconceptions). Having derived the parameters for the general case equation, I can iteratively calculate values until I reach a suitable threshold, though because this is bound to ""simple"" trigonometric and $e^{x}$ functions on the CPU, the 4000-some-odd steps can cause about 0.25 seconds lag on slow devices while it calculates. I'd like to speed this up using my platform's super-optimized vector and BLAS/LAPACK variants. The requirement for doing this is precalculating the number of steps necessary to reach my threshold value. In the underdamped case, where the roots of the characteristic function of the differential equation are non-real, I can use algebraic tricks to get my values: $$x(t) = c_{1}e^{r_{1}}\cos(i_{1}t) + c_{2}e^{r_{2}}\sin(i_{2}t)$$ (Given $r_{1}$, $i_{1}$, $r_{2}$, and $i_{2}$ are the real and irrational components of my two roots, respectively.) Knowing that $r_{1} = r_{2}$ and $i_{1} = -i_{2}$, I can simplify to: $$x(t) = c_{1}e^{r_{1}}\cos(i_{1}t)$$ And get my desired value of $t$ for my threshold $a$: $$t = \arccos(a / c_{1} / e^{r_{1}}) / i_{1}$$ When the roots are real, the equation looks a lot simpler: $$x(t) = c_{1}e^{r_{1}} + c_{2}e^{r_{2}}$$ However, I don't have my trig functions floating around to help me solve it (even if I did, the irrational components being 0 would cause problems, of course). Take the concrete example on pages 3-4 of this document (my bible during this process), since they at least solve cleanly: $$x(t) = 1.5e^{-t} - 0.5e^{-3t}$$ I know how I would solve for $t$ to get my for when $x(t) = a$ on paper, by setting $x=e^{t}$, solving, and back substituting, but I don't have that luxury here. I can make a few assumptions: the roots and constants are all real. I'm always going to be looking for the smallest, first, positive value of $t$. Obviously, the iterative solution is the simplest for this case, but in the end that would involve more steps and therefore be slower no matter what my other optimizations would be. How, then, would I go about solving for my threshold value algorithmically in this (supposedly) simplified case? Addendum The underdamped solution presents an extra requirement. The motion curve will oscillate back and forth a few times across the endpoint. Therefore, ""first and lowest"" $t$ requirement is not necessarily true. In my current, iterative code, the threshold value is both checked against the distance from the current $x(t)$ to the endpoint, as well as to the distance from the previous $x(t)$ as well to allow for a number of oscillations. This might make a more efficient solution nearly impossible.",,"['ordinary-differential-equations', 'physics']"
54,Blow-up of ODE solution,Blow-up of ODE solution,,I am a newcomer to ODEs. The relevant theorem that I can think of is about the maximum open interval of existence of the solution. But I have not learned to find the interval on which the solution exists. Let $f : \mathbb{R}^{n}\to \mathbb{R}^{n}$ be $C^{1}$ and bounded on $\mathbb{R}^{n}$ . Is it possible to have a solution of $\dot{x}=f(x)$ that blows up in finite time?,I am a newcomer to ODEs. The relevant theorem that I can think of is about the maximum open interval of existence of the solution. But I have not learned to find the interval on which the solution exists. Let be and bounded on . Is it possible to have a solution of that blows up in finite time?,f : \mathbb{R}^{n}\to \mathbb{R}^{n} C^{1} \mathbb{R}^{n} \dot{x}=f(x),['ordinary-differential-equations']
55,"Simplifying trigonometric expressions, is there a unified theory?","Simplifying trigonometric expressions, is there a unified theory?",,"$\frac{1}{3}\cos^3 x \cos(2x)+\frac{1}{12}\sin(2x)(\sin(3x)+3\sin x)=\frac{1}{3} \cos x$ I got this as the result of a differential equation that I solved. The answer in the book is (1/3) cos(x), but after applying variation of parameters I got the expression on the left. To my delight Wolfram Alpha tells me that they are equal! (yay!!) But, without cheating and using the computer, how would I ever know that? Nothing about my expression screams ""simplify me"" unless I'm missing something. Perhaps I'd notice the graph looked like cosine if I happened to graph it. I know many trig identities, but I have never heard of a formal procedure that always works to simplify. Is there such a thing? How would you approach this messy expression? How can I get better at this important skill? I have more problems to solve and it feels cheap to keep plugging my answers in to W.alpha to see if they are right. Ps. Is there a widget to convert thing formatted for mathematical to latex and vice versa?","$\frac{1}{3}\cos^3 x \cos(2x)+\frac{1}{12}\sin(2x)(\sin(3x)+3\sin x)=\frac{1}{3} \cos x$ I got this as the result of a differential equation that I solved. The answer in the book is (1/3) cos(x), but after applying variation of parameters I got the expression on the left. To my delight Wolfram Alpha tells me that they are equal! (yay!!) But, without cheating and using the computer, how would I ever know that? Nothing about my expression screams ""simplify me"" unless I'm missing something. Perhaps I'd notice the graph looked like cosine if I happened to graph it. I know many trig identities, but I have never heard of a formal procedure that always works to simplify. Is there such a thing? How would you approach this messy expression? How can I get better at this important skill? I have more problems to solve and it feels cheap to keep plugging my answers in to W.alpha to see if they are right. Ps. Is there a widget to convert thing formatted for mathematical to latex and vice versa?",,"['trigonometry', 'ordinary-differential-equations']"
56,ODE introduction textbook,ODE introduction textbook,,Unfortunately I have reached the maximum number of math classes I can take for my undergraduate degree.  I still wish to study basic ODEs and basic number theory.  What is a good textbook with an introduction to these?  I would prefer a textbook that is not super rigorous or formal since I will be studying it on my own time. Thank you.,Unfortunately I have reached the maximum number of math classes I can take for my undergraduate degree.  I still wish to study basic ODEs and basic number theory.  What is a good textbook with an introduction to these?  I would prefer a textbook that is not super rigorous or formal since I will be studying it on my own time. Thank you.,,"['ordinary-differential-equations', 'reference-request', 'book-recommendation']"
57,"Differential equation, quite weird task","Differential equation, quite weird task",,"I'm having some trouble while trying to understand one task.. The task is as follows: $$\ddot{x}(t) + \dot{x}(t) + 2x(t) = \sin(\omega t)$$ where $x(0) = 7, t\geq 0$ The solution is in the following form: $$x(t) = f(t) + A\sin(\omega t + \varphi)$$ And the task is: find $\omega$ so that $A$ is max. My understanding of this is that $f(t)$ is the solution of the homogeneous differential equation and the rest is the special solution of the nonhomogeneous  equation. Still that does not give me any clue about how to evaluate the relationship between $A$ and $\omega$. Any clues?","I'm having some trouble while trying to understand one task.. The task is as follows: $$\ddot{x}(t) + \dot{x}(t) + 2x(t) = \sin(\omega t)$$ where $x(0) = 7, t\geq 0$ The solution is in the following form: $$x(t) = f(t) + A\sin(\omega t + \varphi)$$ And the task is: find $\omega$ so that $A$ is max. My understanding of this is that $f(t)$ is the solution of the homogeneous differential equation and the rest is the special solution of the nonhomogeneous  equation. Still that does not give me any clue about how to evaluate the relationship between $A$ and $\omega$. Any clues?",,['ordinary-differential-equations']
58,"Are there general solutions to quadratic, 2D, continuous, time-invariant dynamical systems?","Are there general solutions to quadratic, 2D, continuous, time-invariant dynamical systems?",,"I am a bit new to dynamical systems and don't know my way around terminology, so have had a hard time answering this for myself. I know the basics of theory for 2D linear, time-invariant systems, i.e., $$ \dot{x}=a_1x+a_2y \\ \dot{y}=b_1x+b_2y $$ I know that there are explicit exponential solutions, and a ton of theory about fixed points and stability. I'm wondering if there is any equivalent theory out there for the next higher-degree systems, i.e., $$ \dot{x}=a_1x+a_2y+a_3x^2+a_4y^2+a_5xy \\ \dot{y}=b_1x+b_2y+b_3x^2+b_4y^2+b_5xy $$ I get that this is non-linear, so not nearly as simple. But are there any general solutions or time-scales/eigenvalue-equivalents? Is there a standard approach to looking at this as pieced-together linear approximations? Is there a name for this kind of system or its study that I can look up? Thanks very much!","I am a bit new to dynamical systems and don't know my way around terminology, so have had a hard time answering this for myself. I know the basics of theory for 2D linear, time-invariant systems, i.e., I know that there are explicit exponential solutions, and a ton of theory about fixed points and stability. I'm wondering if there is any equivalent theory out there for the next higher-degree systems, i.e., I get that this is non-linear, so not nearly as simple. But are there any general solutions or time-scales/eigenvalue-equivalents? Is there a standard approach to looking at this as pieced-together linear approximations? Is there a name for this kind of system or its study that I can look up? Thanks very much!","
\dot{x}=a_1x+a_2y \\
\dot{y}=b_1x+b_2y
 
\dot{x}=a_1x+a_2y+a_3x^2+a_4y^2+a_5xy \\
\dot{y}=b_1x+b_2y+b_3x^2+b_4y^2+b_5xy
","['ordinary-differential-equations', 'dynamical-systems', 'quadratics', 'nonlinear-system', 'nonlinear-dynamics']"
59,Confused about Laplace Wave Equation Transformation,Confused about Laplace Wave Equation Transformation,,"$$\frac{\partial^2}{\partial t^2} u(x,t) -\frac{\partial^2}{\partial x^2} u(x,t) =f(x), \quad 0<x<1, \quad t>0\\ u(x,0)=0, \quad \frac{\partial}{\partial t} u(x,0)=0\\ u(0,t)=0, \quad u(1,t)=0$$   I am supposed to take the Laplace transform of the wave equation that yields a non-homogeneous ordinary differential equation in terms of $\mathcal{L} \{f(x)\}$ and $\mathcal{L} \{u(x,t)\}$ and its $x$-derivatives. Can someone please explain the relationship between the two functions, and how it's $x$-derivatives changes will be reflected in the function? This question was posed as a challenge question, and I am seeking some guidance. How does one setup this Laplace transformation? Why is there $t$ and $x$ in the same system?","$$\frac{\partial^2}{\partial t^2} u(x,t) -\frac{\partial^2}{\partial x^2} u(x,t) =f(x), \quad 0<x<1, \quad t>0\\ u(x,0)=0, \quad \frac{\partial}{\partial t} u(x,0)=0\\ u(0,t)=0, \quad u(1,t)=0$$   I am supposed to take the Laplace transform of the wave equation that yields a non-homogeneous ordinary differential equation in terms of $\mathcal{L} \{f(x)\}$ and $\mathcal{L} \{u(x,t)\}$ and its $x$-derivatives. Can someone please explain the relationship between the two functions, and how it's $x$-derivatives changes will be reflected in the function? This question was posed as a challenge question, and I am seeking some guidance. How does one setup this Laplace transformation? Why is there $t$ and $x$ in the same system?",,"['ordinary-differential-equations', 'partial-differential-equations', 'laplace-transform']"
60,Are integral curves of a vector field $X$ such that $\nabla_X X = 0$ geodesics?,Are integral curves of a vector field  such that  geodesics?,X \nabla_X X = 0,"Let $(M,g)$ be a riemannian manifold, $X$ a vector field of $M$ and $\gamma \colon M \to \mathbb{R}$ an integral curve of $X$ that is $\gamma'(t) = X(\gamma(t))$. Assume also that the Levi-Civita connection $\nabla_X X = 0$ where Are the integral curves of a $X$ geodesics? I believe is true. A curve $\gamma$ of $M$ is a geodesic if and only if the covariant derivative $\nabla_{\gamma'(t)}\gamma'(t) = 0$. But since $\gamma$ is an integral curve of $X$, just plugging $X(\gamma(t))$ in the connection and using the hypothesis that $\nabla_X X = 0$ gives the result. Is this correct?","Let $(M,g)$ be a riemannian manifold, $X$ a vector field of $M$ and $\gamma \colon M \to \mathbb{R}$ an integral curve of $X$ that is $\gamma'(t) = X(\gamma(t))$. Assume also that the Levi-Civita connection $\nabla_X X = 0$ where Are the integral curves of a $X$ geodesics? I believe is true. A curve $\gamma$ of $M$ is a geodesic if and only if the covariant derivative $\nabla_{\gamma'(t)}\gamma'(t) = 0$. But since $\gamma$ is an integral curve of $X$, just plugging $X(\gamma(t))$ in the connection and using the hypothesis that $\nabla_X X = 0$ gives the result. Is this correct?",,"['ordinary-differential-equations', 'riemannian-geometry', 'smooth-manifolds', 'vector-fields']"
61,On Vector fields as differential operators.,On Vector fields as differential operators.,,"I am having trouble with a simple statement in some lecture notes, they say: take a vector field $\sum_{i=1}^n a_i(x) \frac{\partial}{\partial x_i}$. I do not know how this differential operator (?) represents a vector field (or is this meant to be a differential form? if it is a differential form I have studied them but never made the connection ). My naive understanding of a vector field is a vector valued function that assigns to each point of a plane a value in $\mathbb{R}^2$, as an example I have seen them described in a physics course as $$F(x,y) = x i + yj$$ Where $i,j$ represent the standard basis in $\mathbb{R}^2$. This is a very simple object that I know how to draw while I have no idea how to represent this vector field in the form $\sum_{i=1}^n a_i(x) \frac{\partial}{\partial x_i}$(how would this be done?).","I am having trouble with a simple statement in some lecture notes, they say: take a vector field $\sum_{i=1}^n a_i(x) \frac{\partial}{\partial x_i}$. I do not know how this differential operator (?) represents a vector field (or is this meant to be a differential form? if it is a differential form I have studied them but never made the connection ). My naive understanding of a vector field is a vector valued function that assigns to each point of a plane a value in $\mathbb{R}^2$, as an example I have seen them described in a physics course as $$F(x,y) = x i + yj$$ Where $i,j$ represent the standard basis in $\mathbb{R}^2$. This is a very simple object that I know how to draw while I have no idea how to represent this vector field in the form $\sum_{i=1}^n a_i(x) \frac{\partial}{\partial x_i}$(how would this be done?).",,"['ordinary-differential-equations', 'differential-geometry', 'mathematical-physics']"
62,Is $y'(x) = y(y(x))$ an ODE?,Is  an ODE?,y'(x) = y(y(x)),I'm starting the course of ordinary differential equations and I'm not really sure if $y'(x) = y(y(x))$ is an ODE. More than the answer what I need is an explanation of the reason of it. Thanks!,I'm starting the course of ordinary differential equations and I'm not really sure if $y'(x) = y(y(x))$ is an ODE. More than the answer what I need is an explanation of the reason of it. Thanks!,,['ordinary-differential-equations']
63,Convert a differential equation into an algebraic equation?,Convert a differential equation into an algebraic equation?,,"The book I used (Calculus with Analytic Geometry by Thurman S. Peterson (printed in 1960)) says that: ""A relation among the variables which reduces a differential equation to an algebraic identity is called a solution of the equation."" Does it mean that we just convert a differential equation into its algebraic equation form and to verify that this algebraic equation comes from a certain differential equation, we called this algebraic equation the solution to a certain differential equation? Seems that the word ""solution"" in differential equations is not a traditional one. I wanna be enlightened. Thanks!","The book I used (Calculus with Analytic Geometry by Thurman S. Peterson (printed in 1960)) says that: ""A relation among the variables which reduces a differential equation to an algebraic identity is called a solution of the equation."" Does it mean that we just convert a differential equation into its algebraic equation form and to verify that this algebraic equation comes from a certain differential equation, we called this algebraic equation the solution to a certain differential equation? Seems that the word ""solution"" in differential equations is not a traditional one. I wanna be enlightened. Thanks!",,['ordinary-differential-equations']
64,Equations of motion for the n-body problem,Equations of motion for the n-body problem,,"The Lagrange function is defined as $\mathcal{L}(q,\dot{q}) = T(q,\dot{q}) - V(q,\dot{q})$ where $T$ defines the kinetic energy and $V$ the potential energy. The equations of motion are given by $\frac{\partial \mathcal{L}}{\partial q_i} - \frac{d}{dt} \frac{\partial \mathcal{L}}{\partial \dot{q_i}} = 0$. In the $n$-body problem we have $n$ planets with masses $m_1, \dots, m_n \in \mathbb{R}_+$. The kinetic and potential energy is given by $T = \sum_i \frac{1}{2} m_i  \Vert \dot{q_i} \Vert_2^2$ and $V = G \cdot \sum_{i<j} \frac{m_i m_j}{\Vert q_i - q_j \Vert_2}$ where $G$ denotes a gravitational constant. Furthermore, $q_i(t) \in \mathbb{R}^3$ decribes the position of the $i$-th planet at time $t$. Now I need to calculate the equations of motions. But now I do not understand how to deal with $\frac{\partial \mathcal{L}}{\partial q_i}$. The first thing which confuses me is that $q_i$ is a three-dimensional vector. The second thing would be the derivative of the norm because in calculus we have learned that the norm is not differentiable. Could anyone explain this problem to me? Any help is really appreciated.","The Lagrange function is defined as $\mathcal{L}(q,\dot{q}) = T(q,\dot{q}) - V(q,\dot{q})$ where $T$ defines the kinetic energy and $V$ the potential energy. The equations of motion are given by $\frac{\partial \mathcal{L}}{\partial q_i} - \frac{d}{dt} \frac{\partial \mathcal{L}}{\partial \dot{q_i}} = 0$. In the $n$-body problem we have $n$ planets with masses $m_1, \dots, m_n \in \mathbb{R}_+$. The kinetic and potential energy is given by $T = \sum_i \frac{1}{2} m_i  \Vert \dot{q_i} \Vert_2^2$ and $V = G \cdot \sum_{i<j} \frac{m_i m_j}{\Vert q_i - q_j \Vert_2}$ where $G$ denotes a gravitational constant. Furthermore, $q_i(t) \in \mathbb{R}^3$ decribes the position of the $i$-th planet at time $t$. Now I need to calculate the equations of motions. But now I do not understand how to deal with $\frac{\partial \mathcal{L}}{\partial q_i}$. The first thing which confuses me is that $q_i$ is a three-dimensional vector. The second thing would be the derivative of the norm because in calculus we have learned that the norm is not differentiable. Could anyone explain this problem to me? Any help is really appreciated.",,"['ordinary-differential-equations', 'physics', 'mathematical-physics', 'classical-mechanics', 'euler-lagrange-equation']"
65,"If $F=m\dfrac{dv}{dt}$ why is it incorrect to write $F\,dt=m\,dv$? [duplicate]",If  why is it incorrect to write ? [duplicate],"F=m\dfrac{dv}{dt} F\,dt=m\,dv","This question already has answers here : Is $\frac{\textrm{d}y}{\textrm{d}x}$ not a ratio? (27 answers) Closed 7 years ago . My university lecturer told me that: If $$F=m\dfrac{dv}{dt}$$ it's incorrect to write $$F\,dt=m\,dv\tag{1}$$ but it is okay to write $$\int F\,dt=\int m\,dv$$ for Newtons' second law. But never explained why $(1)$ is mathematically incorrect. My high school teacher told me that: Derivatives with respect to one independent variable can be treated as fractions. So this implies that $(1)$ is valid. This is clearly a contradiction as my high school teacher and university lecturer cannot both be correct. Or can they? Another example of this misuse of derivatives uses the specific heat capacity $c$ which is defined to be $$c=\frac{1}{m}\frac{\delta Q}{dT}\tag{2}$$ Now in the same vain another lecturer wrote that $$\delta Q=mc\,dT$$ by rearranging $(2)$. Another contraction to the first lecturer. I this really allowed or if it's invalid then which mathematical 'rule' has been violated here? EDIT: In my question here I have used formulae that belong to Physics but these were just simple examples to illustrate the point. My question is much more general and applies to any differential equation in mathematics involving the treatment of derivatives with respect to one independent variable as fractions. Specifically; Why is it 'strictly' incorrect to rearrange them without taking the integral of both sides?","This question already has answers here : Is $\frac{\textrm{d}y}{\textrm{d}x}$ not a ratio? (27 answers) Closed 7 years ago . My university lecturer told me that: If $$F=m\dfrac{dv}{dt}$$ it's incorrect to write $$F\,dt=m\,dv\tag{1}$$ but it is okay to write $$\int F\,dt=\int m\,dv$$ for Newtons' second law. But never explained why $(1)$ is mathematically incorrect. My high school teacher told me that: Derivatives with respect to one independent variable can be treated as fractions. So this implies that $(1)$ is valid. This is clearly a contradiction as my high school teacher and university lecturer cannot both be correct. Or can they? Another example of this misuse of derivatives uses the specific heat capacity $c$ which is defined to be $$c=\frac{1}{m}\frac{\delta Q}{dT}\tag{2}$$ Now in the same vain another lecturer wrote that $$\delta Q=mc\,dT$$ by rearranging $(2)$. Another contraction to the first lecturer. I this really allowed or if it's invalid then which mathematical 'rule' has been violated here? EDIT: In my question here I have used formulae that belong to Physics but these were just simple examples to illustrate the point. My question is much more general and applies to any differential equation in mathematics involving the treatment of derivatives with respect to one independent variable as fractions. Specifically; Why is it 'strictly' incorrect to rearrange them without taking the integral of both sides?",,"['ordinary-differential-equations', 'derivatives', 'infinitesimals']"
66,Can Laplace solve every lineair differential equation?,Can Laplace solve every lineair differential equation?,,I'm learning about laplace tranform method to solve lineair differential equations but i'm wondering if laplace transformations can be used to solve every linear differential equations there is. Or are there some limitations? I know that for the operator method the equation has to be from the form: $e^at$ / $sin(bt)$ / $cos(bt)$ and Polynomial.,I'm learning about laplace tranform method to solve lineair differential equations but i'm wondering if laplace transformations can be used to solve every linear differential equations there is. Or are there some limitations? I know that for the operator method the equation has to be from the form: $e^at$ / $sin(bt)$ / $cos(bt)$ and Polynomial.,,"['ordinary-differential-equations', 'laplace-transform']"
67,The real equation of a pendulum,The real equation of a pendulum,,"In physics I never solve the equation $\ddot\theta = \sin(\theta)$. Instead, we used the approximation $\theta = \sin(\theta)$ for small angles and then it was easy to solve. I didn't do any physics since a while but I was interested by the original equation. I tried to solve it with formal series but it was quite ugly and I give up quickly. But maybe it's well known so I'm asking the question here : What are the solutions to the equation $\ddot\theta = \sin(\theta)$ ?","In physics I never solve the equation $\ddot\theta = \sin(\theta)$. Instead, we used the approximation $\theta = \sin(\theta)$ for small angles and then it was easy to solve. I didn't do any physics since a while but I was interested by the original equation. I tried to solve it with formal series but it was quite ugly and I give up quickly. But maybe it's well known so I'm asking the question here : What are the solutions to the equation $\ddot\theta = \sin(\theta)$ ?",,['ordinary-differential-equations']
68,Uniqueness of solutions to linear recurrence relations,Uniqueness of solutions to linear recurrence relations,,"I understand that if I have a linear homogeneous recurrence relation of the form $q_n = c_1 q_{n-1} + c_2 q_{n-2} + \cdots + c_d q_{n-d}$, I can construct the characteristic polynomial $p(t) = t^d - c_1 t^{d-1} - \cdots - c_{d-1} t - c_d$, and if the roots are $r_1, \ldots, r_d$ (say distinct, for simplicity) I can be assured that $q_n = k_1 r_1^n + \cdots k_d r_d^n$ is a solution for any choice of coefficients $k_i$.  But are these the only solutions?  Is there a clean way to show this?","I understand that if I have a linear homogeneous recurrence relation of the form $q_n = c_1 q_{n-1} + c_2 q_{n-2} + \cdots + c_d q_{n-d}$, I can construct the characteristic polynomial $p(t) = t^d - c_1 t^{d-1} - \cdots - c_{d-1} t - c_d$, and if the roots are $r_1, \ldots, r_d$ (say distinct, for simplicity) I can be assured that $q_n = k_1 r_1^n + \cdots k_d r_d^n$ is a solution for any choice of coefficients $k_i$.  But are these the only solutions?  Is there a clean way to show this?",,"['ordinary-differential-equations', 'recurrence-relations']"
69,Solving a matrix as a differential equation,Solving a matrix as a differential equation,,"If I have a matrix $$A = \begin{bmatrix}5 & 4 & -6\\-2 & -1 & 2\\2 & 0 & -3\end{bmatrix}$$ how do I solve $x'=Ax$ as a differential equation? My text book explains this in a rather confusing way and I am really not getting it - what if I just found the eigen values - would the solution be anything to do with that or could I just use some kind of gaussian elimination? Any help would be much appreciated, many thanks. :)","If I have a matrix $$A = \begin{bmatrix}5 & 4 & -6\\-2 & -1 & 2\\2 & 0 & -3\end{bmatrix}$$ how do I solve $x'=Ax$ as a differential equation? My text book explains this in a rather confusing way and I am really not getting it - what if I just found the eigen values - would the solution be anything to do with that or could I just use some kind of gaussian elimination? Any help would be much appreciated, many thanks. :)",,['ordinary-differential-equations']
70,"How to solve $y(y'+3)=ax^2+bx+c, \quad a,b,c \in \mathbb{R}$",How to solve,"y(y'+3)=ax^2+bx+c, \quad a,b,c \in \mathbb{R}","How could we solve this differential equation $$y(y'+3)=ax^2+bx+c, \quad a,b,c \in \mathbb{R}$$ I really don't know how start. I am not familiar with this sort of differential equations (I know it is non-linear fist order, but don't see way to solve it). I passed this exam, so this is just for fun (not homework or something like that). Thanks.","How could we solve this differential equation $$y(y'+3)=ax^2+bx+c, \quad a,b,c \in \mathbb{R}$$ I really don't know how start. I am not familiar with this sort of differential equations (I know it is non-linear fist order, but don't see way to solve it). I passed this exam, so this is just for fun (not homework or something like that). Thanks.",,['ordinary-differential-equations']
71,Is $f'(x)=f(1/x)$ solvable?,Is  solvable?,f'(x)=f(1/x),"So recently I have been scrolling through Youtube (mainly to find math videos for entertainment, I'll attempt a question on my own every now and then) when I came across this video by Michael Penn solving the differential and functional equation $$f'(x)=f(1/x)$$ which I thought might be a nice challenge for me. Here is my attempt: Right away we can see that $f'(1)=f(1)$ from plugging in $x=1$ . But, this didn't really tell me anything, however I decided to keep this in mind for later. To make some actual progress we can see that $$\begin{align}f''(x)=\dfrac d{dx}f'(x)=\dfrac d{dx}f(1/x)\\=f'(1/x)\cdot\dfrac d{dx}\dfrac1x=-\dfrac{f(x)}{x^2}\end{align}$$ Therefore $$f''(x)=-\dfrac{f(x)}{x^2}$$ Now, we can set up 2 differential equations that our function must satisfy: $$x^2y''+y=0$$ and $$y'(1)=y(1)$$ I'll try solving our first one first: Plugging in $y=x^r$ ,we have $$x^2x^{r-2}(r^2-r)+x^r=0\\\implies x^r(r^2-r)+x^r=0\\x^r(r^2-r+1)=0\\\implies r^2-r+1=0$$ So solving our quadratic equation gets us $$r=\dfrac{1\pm i\sqrt3}2$$ We now let $$r_1=\dfrac{1+i\sqrt3}2,r_2=\dfrac{1-i\sqrt3}2\implies y=c_1x^{r_1}+c_2x^{r_2}$$ which we can write as [1] $$x^{r_1}=\sqrt x\left(\cos\left(\dfrac{\sqrt3}2\ln x\right)+i\sin\left(\dfrac{\sqrt3}2\ln x\right)\right)=z\sqrt x\\x^{r_2}=\sqrt x\left(\cos\left(\dfrac{\sqrt3}2\ln x\right)-i\sin\left(\dfrac{\sqrt3}2\ln x\right)\right)=\overline z\sqrt x$$ which is where I am currently stuck because I am unsure about what to do from here. So, my question is: Is $f'(x)=f(1/x)$ solvable, or am I doing math manipulations with no real meaning? [1] I decided to write everything that was being multiplied by $\sqrt x$ as $z$ and $\overline z$ for $r_1$ and $r_2$ respectively just so I didn't have to deal with the huge mess that the expression was. Also note that I am leaving how I expanded $x^{r_1}$ and $x^{r_2}$ as an exercise for the reader.","So recently I have been scrolling through Youtube (mainly to find math videos for entertainment, I'll attempt a question on my own every now and then) when I came across this video by Michael Penn solving the differential and functional equation which I thought might be a nice challenge for me. Here is my attempt: Right away we can see that from plugging in . But, this didn't really tell me anything, however I decided to keep this in mind for later. To make some actual progress we can see that Therefore Now, we can set up 2 differential equations that our function must satisfy: and I'll try solving our first one first: Plugging in ,we have So solving our quadratic equation gets us We now let which we can write as [1] which is where I am currently stuck because I am unsure about what to do from here. So, my question is: Is solvable, or am I doing math manipulations with no real meaning? [1] I decided to write everything that was being multiplied by as and for and respectively just so I didn't have to deal with the huge mess that the expression was. Also note that I am leaving how I expanded and as an exercise for the reader.","f'(x)=f(1/x) f'(1)=f(1) x=1 \begin{align}f''(x)=\dfrac d{dx}f'(x)=\dfrac d{dx}f(1/x)\\=f'(1/x)\cdot\dfrac d{dx}\dfrac1x=-\dfrac{f(x)}{x^2}\end{align} f''(x)=-\dfrac{f(x)}{x^2} x^2y''+y=0 y'(1)=y(1) y=x^r x^2x^{r-2}(r^2-r)+x^r=0\\\implies x^r(r^2-r)+x^r=0\\x^r(r^2-r+1)=0\\\implies r^2-r+1=0 r=\dfrac{1\pm i\sqrt3}2 r_1=\dfrac{1+i\sqrt3}2,r_2=\dfrac{1-i\sqrt3}2\implies y=c_1x^{r_1}+c_2x^{r_2} x^{r_1}=\sqrt x\left(\cos\left(\dfrac{\sqrt3}2\ln x\right)+i\sin\left(\dfrac{\sqrt3}2\ln x\right)\right)=z\sqrt x\\x^{r_2}=\sqrt x\left(\cos\left(\dfrac{\sqrt3}2\ln x\right)-i\sin\left(\dfrac{\sqrt3}2\ln x\right)\right)=\overline z\sqrt x f'(x)=f(1/x) \sqrt x z \overline z r_1 r_2 x^{r_1} x^{r_2}","['ordinary-differential-equations', 'functional-equations']"
72,"Given a self-map $h$ of a (closed?) manifold, is there a vector field $\xi$ with flow $\Phi_t$ such that $h = \Phi_1$?","Given a self-map  of a (closed?) manifold, is there a vector field  with flow  such that ?",h \xi \Phi_t h = \Phi_1,"Given a (closed?) connected Riemannian manifold $M^n$ and a self-diffeomorphism $h: M \to M$ , is it necessarily the case that there is a differential equation/smooth, tangent vector field $\xi$ on $M$ so that the flow $\Phi_t$ of $\xi$ has $h = \Phi_1$ ? That is, can we always extend a discrete-time dynamical system to a continuous-time one? Thanks in advance. EDIT: Here is a follow-up question: Follow-Up to given a self-map $h$ of a (closed?) manifold, is there a vector field $\xi$ with flow $\Phi_t$ such that $h=\Phi_1$?","Given a (closed?) connected Riemannian manifold and a self-diffeomorphism , is it necessarily the case that there is a differential equation/smooth, tangent vector field on so that the flow of has ? That is, can we always extend a discrete-time dynamical system to a continuous-time one? Thanks in advance. EDIT: Here is a follow-up question: Follow-Up to given a self-map $h$ of a (closed?) manifold, is there a vector field $\xi$ with flow $\Phi_t$ such that $h=\Phi_1$?",M^n h: M \to M \xi M \Phi_t \xi h = \Phi_1,"['ordinary-differential-equations', 'differential-topology', 'dynamical-systems', 'vector-fields']"
73,An approximate solution to a differential equation $f'(x)^2 - \omega^2 f(x)^2 = G(x)$ for small $x$,An approximate solution to a differential equation  for small,f'(x)^2 - \omega^2 f(x)^2 = G(x) x,"Suppose I have an ODE of the form $$ \left( \frac{df}{dx} \right)^{2} - \omega^2 f(x)^2 = G(x) $$ with $\omega>0$ and the initial condition $f(0)=0$ and where $G(x)$ is a very complicated function (which you might not even have an analytic expression for). I seek the solution for $x>0$ . Even though I don't have the full expression for $G(x)$ , I do know the series expansion for $G(x)$ for $0< x \ll 1$ where I have $$ G(x) \simeq \omega^2 - \alpha x^4 + \mathcal{O}(x^6) $$ for some number $\alpha >0$ . If I am interested in the solution for small $x$ , then I have approximately $$ \left( \frac{df}{dx} \right)^2 - \omega^2 f(x)^2 \simeq \omega^2  $$ under the condition that $\alpha x^4 \ll 1$ . There is a simple solution to the above DE: does it make sense to say the following? $$ f(x) \simeq \sinh(\omega x) \ \ \ \ \ \ \mathrm{when\ }\alpha x^4 \ll 1 $$ The above seems to work numerically quite well, but I am confused because I am taking a series expansion in the ODE and dropping terms $\mathcal{O}(x^4)$ there in the DE: however, my approximate solution has terms higher order than this in it. So in this sense my solution is not really a series in $x\ll 1$ . Does it make sense what I've done? Maybe the condition $\alpha x^4 \ll 1$ doesn't make sense here? Is there any literature on approximating DE's in this manner? An Example With a Plot I have cooked up a function (involving Bessel functions) $$ G_0(x) := \omega^2 J_0\left(\left[ \frac{192\alpha}{\omega^2} \right]^{1/4} x\right) + 2 \omega^2 J_{2}\left(\left[ \frac{192\alpha}{\omega^2} \right]^{1/4} x\right) $$ This has the $x\ll 1$ expansion $G_0(x) \simeq \omega^2 - \alpha x^4 + \mathcal{O}(x^6)$ (I've cooked up the parameters in the arguments so this is so). The smaller you make $\alpha$ , for larger values of $x$ the function looks approximately constant $G_0(x) \sim \omega^2$ . Below I do some numerical plotting for $\omega = 0.4$ and $\alpha=0.001$ . In the first curve I numerically solve for the exact curve $f(x)$ for the choice $G_0$ . The second curve I plot the approximation $\sinh(\omega x)$ . In the last curve I use the first three terms of the series expansion of $\sinh(\omega x) \sim \omega x + \ldots$ The Point: $\sinh(\omega x)$ is a much better approximation for the solution $f$ (than the simple series expansion $\omega x + \ldots$ ). This is because $\alpha$ is chosen to be small here. Why can you neglect terms $\mathcal{O}(x^4)$ in the ODE (presumably this would mean your solution needs to be a series in $x$ too?), and yet the better approximation is a function $\sinh(\omega x)$ which has every order (higher than 4!) contributions? How to understand the error in $\sinh(\omega x)$ when using such an approximation? (an error which seems to be smaller than the error introduced by a simple series solution to $f$ .)","Suppose I have an ODE of the form with and the initial condition and where is a very complicated function (which you might not even have an analytic expression for). I seek the solution for . Even though I don't have the full expression for , I do know the series expansion for for where I have for some number . If I am interested in the solution for small , then I have approximately under the condition that . There is a simple solution to the above DE: does it make sense to say the following? The above seems to work numerically quite well, but I am confused because I am taking a series expansion in the ODE and dropping terms there in the DE: however, my approximate solution has terms higher order than this in it. So in this sense my solution is not really a series in . Does it make sense what I've done? Maybe the condition doesn't make sense here? Is there any literature on approximating DE's in this manner? An Example With a Plot I have cooked up a function (involving Bessel functions) This has the expansion (I've cooked up the parameters in the arguments so this is so). The smaller you make , for larger values of the function looks approximately constant . Below I do some numerical plotting for and . In the first curve I numerically solve for the exact curve for the choice . The second curve I plot the approximation . In the last curve I use the first three terms of the series expansion of The Point: is a much better approximation for the solution (than the simple series expansion ). This is because is chosen to be small here. Why can you neglect terms in the ODE (presumably this would mean your solution needs to be a series in too?), and yet the better approximation is a function which has every order (higher than 4!) contributions? How to understand the error in when using such an approximation? (an error which seems to be smaller than the error introduced by a simple series solution to .)","
\left( \frac{df}{dx} \right)^{2} - \omega^2 f(x)^2 = G(x)
 \omega>0 f(0)=0 G(x) x>0 G(x) G(x) 0< x \ll 1 
G(x) \simeq \omega^2 - \alpha x^4 + \mathcal{O}(x^6)
 \alpha >0 x 
\left( \frac{df}{dx} \right)^2 - \omega^2 f(x)^2 \simeq \omega^2 
 \alpha x^4 \ll 1 
f(x) \simeq \sinh(\omega x) \ \ \ \ \ \ \mathrm{when\ }\alpha x^4 \ll 1
 \mathcal{O}(x^4) x\ll 1 \alpha x^4 \ll 1 
G_0(x) := \omega^2 J_0\left(\left[ \frac{192\alpha}{\omega^2} \right]^{1/4} x\right) + 2 \omega^2 J_{2}\left(\left[ \frac{192\alpha}{\omega^2} \right]^{1/4} x\right)
 x\ll 1 G_0(x) \simeq \omega^2 - \alpha x^4 + \mathcal{O}(x^6) \alpha x G_0(x) \sim \omega^2 \omega = 0.4 \alpha=0.001 f(x) G_0 \sinh(\omega x) \sinh(\omega x) \sim \omega x + \ldots \sinh(\omega x) f \omega x + \ldots \alpha \mathcal{O}(x^4) x \sinh(\omega x) \sinh(\omega x) f","['ordinary-differential-equations', 'approximation']"
74,Limiting distance of a chase of two points?,Limiting distance of a chase of two points?,,"Gil Kalai's blog post https://gilkalai.wordpress.com/2018/06/29/test-your-intuition-35-what-is-the-limiting-distance/ poses a riddle about a chase of two points. Point A chases point B at unit velocity. Point B heads right at unit velocity. At $t = 0$, $A(0) = (0,1)$ and $B(0) = (0,0)$. Question: what is the limiting distance $||B(t) - A(t)||$ as $t$ goes to infinity?","Gil Kalai's blog post https://gilkalai.wordpress.com/2018/06/29/test-your-intuition-35-what-is-the-limiting-distance/ poses a riddle about a chase of two points. Point A chases point B at unit velocity. Point B heads right at unit velocity. At $t = 0$, $A(0) = (0,1)$ and $B(0) = (0,0)$. Question: what is the limiting distance $||B(t) - A(t)||$ as $t$ goes to infinity?",,"['ordinary-differential-equations', 'analytic-geometry', 'dynamical-systems']"
75,Error in classical RK method. The result went to infinity.,Error in classical RK method. The result went to infinity.,,"I used classical ($4^\text{th}$ order) Runge-Kutta method to solve the ODE $$y'=5e^{5t}(y-t)^2+1,0\leq t\leq 1, \ y(0)=-1.$$ $h$ is the step size. When $h=0.2$, I got a good approximation of the solution. However, when $h=0.25$, the result went to infinity. What's the reason for this? What's restriction of the step size and convergent condition for classical RK method?","I used classical ($4^\text{th}$ order) Runge-Kutta method to solve the ODE $$y'=5e^{5t}(y-t)^2+1,0\leq t\leq 1, \ y(0)=-1.$$ $h$ is the step size. When $h=0.2$, I got a good approximation of the solution. However, when $h=0.25$, the result went to infinity. What's the reason for this? What's restriction of the step size and convergent condition for classical RK method?",,"['ordinary-differential-equations', 'numerical-methods', 'computational-mathematics', 'runge-kutta-methods']"
76,Single approach to solving differential equations,Single approach to solving differential equations,,"I am an engineer who uses mathematics for applications. I have learnt how to solve differential equations, both ordinary and partial. My impression has been that solving differential equations is all about knowing a bag of diverse tricks: separation of variables, reduction in order, power series method, etc. I would like to know if there is a single approach that would work for differential equations. I don't mind if the approach is tedious or if it involves successive approximations. All I wish for is that the procedure of solving differential equations be mechanical in nature, and applicable to widest possible variety of differential equations. I first thought that writing unknown function as Taylor series and successively finding the unknown coefficients is a very general, although tedious (which is alright with me), approach to solving differential equations. However I later learnt that it works only if the expansion is carried about a regular point, otherwise it gives nonsensical answer. Recently I have begun studying one-parameter group theoretic method for solving differential equations, and the author of a book promises it is a very general method. I wished to ask your opinion regarding this and whether there are any other general approaches which could be learnt with minimum prerequisites. Thanks in advance for any advice.","I am an engineer who uses mathematics for applications. I have learnt how to solve differential equations, both ordinary and partial. My impression has been that solving differential equations is all about knowing a bag of diverse tricks: separation of variables, reduction in order, power series method, etc. I would like to know if there is a single approach that would work for differential equations. I don't mind if the approach is tedious or if it involves successive approximations. All I wish for is that the procedure of solving differential equations be mechanical in nature, and applicable to widest possible variety of differential equations. I first thought that writing unknown function as Taylor series and successively finding the unknown coefficients is a very general, although tedious (which is alright with me), approach to solving differential equations. However I later learnt that it works only if the expansion is carried about a regular point, otherwise it gives nonsensical answer. Recently I have begun studying one-parameter group theoretic method for solving differential equations, and the author of a book promises it is a very general method. I wished to ask your opinion regarding this and whether there are any other general approaches which could be learnt with minimum prerequisites. Thanks in advance for any advice.",,"['ordinary-differential-equations', 'partial-differential-equations', 'online-resources']"
77,Dynamical system defined with a non-abelian group,Dynamical system defined with a non-abelian group,,"Soft question. I'm taking an introductory mini-course in dynamical systems, and the professor defined a continuous dynamical system in a topological space $M $ (or metric space, smooth manifold, or whatever) as a map (continuous, smooth, etc) as a map  $\Phi:M\times \Bbb R \to M$ satisfying (i) $\Phi(\cdot,0)={\rm Id}_M$ and (ii) $\Phi(\Phi(x,t),s)=\Phi(x,t+s)$ for all  $x\in M$ and $t,s\in \Bbb R$. He also defined a discrete dynamical system as same as above with $\Bbb Z$ instead of $\Bbb R$, and made a remark saying that we could use a group instead of $\Bbb R$ or $\Bbb Z$, which I guess it would be like this: let $G$ be a group and ask $\Phi:M\times G\to M$ to satisfy (i') $\Phi(\cdot,e_G) ={\rm Id}_M$ and (ii') $\Phi(\Phi(x,g),h)=\Phi(x,g\cdot h)$ for all $x\in M$ and $g,h\in G$. Although we'd probably want $G$ to be a nice topological group or Lie group (we could start talking about continuity or smoothness of $\Phi$), what I'd like to know is: what interesting stuff can be modeled using a dynamical system like this? I'm curious about the usefulness of non-abelian groups in this context, since we'd probably want the parameter space $G$ to represent instants in time.","Soft question. I'm taking an introductory mini-course in dynamical systems, and the professor defined a continuous dynamical system in a topological space $M $ (or metric space, smooth manifold, or whatever) as a map (continuous, smooth, etc) as a map  $\Phi:M\times \Bbb R \to M$ satisfying (i) $\Phi(\cdot,0)={\rm Id}_M$ and (ii) $\Phi(\Phi(x,t),s)=\Phi(x,t+s)$ for all  $x\in M$ and $t,s\in \Bbb R$. He also defined a discrete dynamical system as same as above with $\Bbb Z$ instead of $\Bbb R$, and made a remark saying that we could use a group instead of $\Bbb R$ or $\Bbb Z$, which I guess it would be like this: let $G$ be a group and ask $\Phi:M\times G\to M$ to satisfy (i') $\Phi(\cdot,e_G) ={\rm Id}_M$ and (ii') $\Phi(\Phi(x,g),h)=\Phi(x,g\cdot h)$ for all $x\in M$ and $g,h\in G$. Although we'd probably want $G$ to be a nice topological group or Lie group (we could start talking about continuity or smoothness of $\Phi$), what I'd like to know is: what interesting stuff can be modeled using a dynamical system like this? I'm curious about the usefulness of non-abelian groups in this context, since we'd probably want the parameter space $G$ to represent instants in time.",,"['ordinary-differential-equations', 'dynamical-systems', 'mathematical-physics', 'applications']"
78,Is this an ODE?,Is this an ODE?,,"A friend and I are discussing whether this is an ODE: $$y'(x)+y(-x)=e^x$$ My friend claims it is not because of the $-x$. IMHO, the differential equation can be written as $F(x,y,y')=0$ with: $$F=y' +  y\circ (-\mathrm{Id}) -\exp$$ and is an ODE, hence. Could you please confirm?","A friend and I are discussing whether this is an ODE: $$y'(x)+y(-x)=e^x$$ My friend claims it is not because of the $-x$. IMHO, the differential equation can be written as $F(x,y,y')=0$ with: $$F=y' +  y\circ (-\mathrm{Id}) -\exp$$ and is an ODE, hence. Could you please confirm?",,"['ordinary-differential-equations', 'terminology', 'definition']"
79,General Existence and Uniqueness of ODE,General Existence and Uniqueness of ODE,,"I am trying to make sure that I understand the following question. As well as I am having a bit of trouble understand the partial solutions given. The question is basically, what can we say about the following IVP ? $$ y'=y^{1/3}$$ with initial value, $y(0)=0$ for $t \ge 0$ I know that the important theorem to be aware of is the theorem about unique solutions depending on continuity of the partial derivative w.r.t to y and existence depending on the continuity of f(t,y) itself. So what I am seeing initially is that the partial derivative (w.r.t to y) is continuous everywhere expect at $y=0$. and  $f(t,y)$ itself is continuous for all values of $t$. So does this Imply that we have a solution for all values of t, but we will only have a unique solution when $y \neq 0 $ ? Solving, using the fact that we have a separable equation gives me, $$y^{-1/3}dy=dt$$ Integrating both sides, $$\frac{3}{2}y^{2/3}=t+c$$ $$y^{2/3}=\frac{2}{3}(t+c)$$ $$y^2=[(2/3)(t+c)]^{3}$$ $$y= \pm [(2/3)(t+c)]^{3/2}$$ From our initial conditions we can solve easily for c, to get that $c=0$ that is $y=\pm (2/3)t^{3/2}$ Now here is my first point of confusion, the solution says for both these answers, $t \ge 0$  I think I am just lost in it and even though I understood what the theorem stated, I don't understand its implications. As well, from initial observation we know $y=\psi(t)=0$ is also a solution. And the final answer is that $y= \lambda (t)= 0$ if $0 \le t \le t_{o}$ and $\pm[(2/3)(t-t_{o})]^{3/2}$ if $t \ge t_{o}$ Is the reason we have that the second part is only for $t \ge t_{0}$ just to avoid taking square root of negatives? I am still confused about the final answer. I understand how we found that we could have $\pm [(2/3)(t+c)]^{3/2}$ and y=0 but what I am not understand is how from that we can obtain y=0 if $0 \le t \lt t_{o}$ and $\pm [(2/3)(t-t_{o})]^{3/2}$  if $t \ge t_{o}$? How was this done? It just directly stated this. Mainly, how is it that it went directly to saying  Can someone please help tie this together for me? Thanks a lot","I am trying to make sure that I understand the following question. As well as I am having a bit of trouble understand the partial solutions given. The question is basically, what can we say about the following IVP ? $$ y'=y^{1/3}$$ with initial value, $y(0)=0$ for $t \ge 0$ I know that the important theorem to be aware of is the theorem about unique solutions depending on continuity of the partial derivative w.r.t to y and existence depending on the continuity of f(t,y) itself. So what I am seeing initially is that the partial derivative (w.r.t to y) is continuous everywhere expect at $y=0$. and  $f(t,y)$ itself is continuous for all values of $t$. So does this Imply that we have a solution for all values of t, but we will only have a unique solution when $y \neq 0 $ ? Solving, using the fact that we have a separable equation gives me, $$y^{-1/3}dy=dt$$ Integrating both sides, $$\frac{3}{2}y^{2/3}=t+c$$ $$y^{2/3}=\frac{2}{3}(t+c)$$ $$y^2=[(2/3)(t+c)]^{3}$$ $$y= \pm [(2/3)(t+c)]^{3/2}$$ From our initial conditions we can solve easily for c, to get that $c=0$ that is $y=\pm (2/3)t^{3/2}$ Now here is my first point of confusion, the solution says for both these answers, $t \ge 0$  I think I am just lost in it and even though I understood what the theorem stated, I don't understand its implications. As well, from initial observation we know $y=\psi(t)=0$ is also a solution. And the final answer is that $y= \lambda (t)= 0$ if $0 \le t \le t_{o}$ and $\pm[(2/3)(t-t_{o})]^{3/2}$ if $t \ge t_{o}$ Is the reason we have that the second part is only for $t \ge t_{0}$ just to avoid taking square root of negatives? I am still confused about the final answer. I understand how we found that we could have $\pm [(2/3)(t+c)]^{3/2}$ and y=0 but what I am not understand is how from that we can obtain y=0 if $0 \le t \lt t_{o}$ and $\pm [(2/3)(t-t_{o})]^{3/2}$  if $t \ge t_{o}$? How was this done? It just directly stated this. Mainly, how is it that it went directly to saying  Can someone please help tie this together for me? Thanks a lot",,['ordinary-differential-equations']
80,Does this technique for solving an ODE generalize?,Does this technique for solving an ODE generalize?,,"Apologies for what's probably a silly question to anyone who knows this stuff. I was looking at a question earlier and realized that $\sin(x)$ and $\cos(x)$ satisfy two different differential equations: $$u'' + u = 0$$ on the one hand, and $$u^2 + (u')^2 = 1$$ on the other.  Wondering if these were in any way connected, I tried differentiating the second equation, giving $$2 u u' + 2 u' u''= 0;$$ Factoring, $$2 u' (u + u'') = 0$$ so we have $$u' = 0 \qquad \text{or} \qquad u + u'' = 0$$ The former equation corresponds to the solution $u = 1$ and the latter corresponds to $\sin$ and $\cos$. Applying this to the differential equation $$(u')^2 = 4 u^3 - g_2 u - g_3$$ we obtain $$u' = 0 \qquad \text{or} \qquad 2 u'' - 12 u^2 + g_2 = 0.$$ Not having any intuition for the subject, I don't know if $2 u'' - 12 u^2 + g_2 = 0$ is in any sense ""better"" or ""worse"" than the original equation, though.  I can tell that differentiating both sides again doesn't appear to help. Question : is this technique part of a bigger picture?  Or is it simply an ad-hoc trick that happens to work in this particular situation but is, for reasons obvious to others but not to me, unlikely to be useful in other situations?","Apologies for what's probably a silly question to anyone who knows this stuff. I was looking at a question earlier and realized that $\sin(x)$ and $\cos(x)$ satisfy two different differential equations: $$u'' + u = 0$$ on the one hand, and $$u^2 + (u')^2 = 1$$ on the other.  Wondering if these were in any way connected, I tried differentiating the second equation, giving $$2 u u' + 2 u' u''= 0;$$ Factoring, $$2 u' (u + u'') = 0$$ so we have $$u' = 0 \qquad \text{or} \qquad u + u'' = 0$$ The former equation corresponds to the solution $u = 1$ and the latter corresponds to $\sin$ and $\cos$. Applying this to the differential equation $$(u')^2 = 4 u^3 - g_2 u - g_3$$ we obtain $$u' = 0 \qquad \text{or} \qquad 2 u'' - 12 u^2 + g_2 = 0.$$ Not having any intuition for the subject, I don't know if $2 u'' - 12 u^2 + g_2 = 0$ is in any sense ""better"" or ""worse"" than the original equation, though.  I can tell that differentiating both sides again doesn't appear to help. Question : is this technique part of a bigger picture?  Or is it simply an ad-hoc trick that happens to work in this particular situation but is, for reasons obvious to others but not to me, unlikely to be useful in other situations?",,['ordinary-differential-equations']
81,What does it mean to extend curves to the projective line,What does it mean to extend curves to the projective line,,"The question is (from Arnol'd 's book : Ordinary Differential Equations): ""Which of the differential equations $ \dot x = x^ n$ determine on an affine line a phase velocity field that can be extended without singular points to the projective line?"" The answers that are given are n = 0,1,2. In the book from where I got this question, phase velocity field is when you plot the function $ v (x) = \dot x = x^n $ against $x$. $x$ is on the vertical axis and $v(x)$ on the horizontal one. For $\dot x = x$ and $\dot x = 1 $, the phase velocity fields are a diagonal line and a vertical line, respectively. So it makes sense that these are solutions. On the other hand, $\dot x = x^2$ has the phase velocity field that looks like $ y = \sqrt x $, which is what the phase velocity fields look like for higher values of n (although they get flatter faster). So what I don't understand is why is $ n = 2 $ a solution, but not $ n > 2 $ So where am I going wrong? Any help is appreciated. Thanks in advance.","The question is (from Arnol'd 's book : Ordinary Differential Equations): ""Which of the differential equations $ \dot x = x^ n$ determine on an affine line a phase velocity field that can be extended without singular points to the projective line?"" The answers that are given are n = 0,1,2. In the book from where I got this question, phase velocity field is when you plot the function $ v (x) = \dot x = x^n $ against $x$. $x$ is on the vertical axis and $v(x)$ on the horizontal one. For $\dot x = x$ and $\dot x = 1 $, the phase velocity fields are a diagonal line and a vertical line, respectively. So it makes sense that these are solutions. On the other hand, $\dot x = x^2$ has the phase velocity field that looks like $ y = \sqrt x $, which is what the phase velocity fields look like for higher values of n (although they get flatter faster). So what I don't understand is why is $ n = 2 $ a solution, but not $ n > 2 $ So where am I going wrong? Any help is appreciated. Thanks in advance.",,['ordinary-differential-equations']
82,To solve $y'=\frac{\cos y+y\cos x}{x\sin y-\sin x}$,To solve,y'=\frac{\cos y+y\cos x}{x\sin y-\sin x},"$$y'(x)=\frac{\cos (y(x))+y(x) \cos (x)}{x \sin (y(x))-\sin (x)}$$ I am self-learning the differential equation from a textbook and I need some help with above equation. I learned I-factor (for 1st order linear ODE), Bernoulli's, and Riccati's. So the problem should be solved by them. The book doesn't have any examples using trig functions. I tried wolfram alpha but the website could not give me ""step-by-step"". Does anyone know why? Wolfram Alpha never had this problem where it could not give ""step-by-step"".","$$y'(x)=\frac{\cos (y(x))+y(x) \cos (x)}{x \sin (y(x))-\sin (x)}$$ I am self-learning the differential equation from a textbook and I need some help with above equation. I learned I-factor (for 1st order linear ODE), Bernoulli's, and Riccati's. So the problem should be solved by them. The book doesn't have any examples using trig functions. I tried wolfram alpha but the website could not give me ""step-by-step"". Does anyone know why? Wolfram Alpha never had this problem where it could not give ""step-by-step"".",,['ordinary-differential-equations']
83,Relation between Heaviside step function to Dirac Delta function,Relation between Heaviside step function to Dirac Delta function,,"I understand that ""delta function"" is a distribution, not a function, as in it acts on another integrand, picking out the value of that integrand at a specific point. The discontinuous function is first defined as  $$\delta_\epsilon(x) = \begin{cases} 0&x<-\epsilon\\ 1/2\epsilon & -\epsilon\le x\le\epsilon\\0 & x\gt-\epsilon\end{cases}$$ Because $$\delta(x) = \delta_{\epsilon\rightarrow0}(x)$$ Therefore in the limit $$\delta(x) = \begin{cases} \infty & x=0 \\ 0 & x\neq0 \end{cases}$$ and that  $$\int_{-\alpha}^\beta \delta(x)dx=1 \;\; \alpha\gt0, \; \beta\gt 0$$ Given the above conclusions, how can the Heaviside step function $H(x)$ be related to the Dirac Delta Function $\delta(x)$ in the following way?$$H(x)=\int_{-\infty}^x\delta(\xi)d\xi$$","I understand that ""delta function"" is a distribution, not a function, as in it acts on another integrand, picking out the value of that integrand at a specific point. The discontinuous function is first defined as  $$\delta_\epsilon(x) = \begin{cases} 0&x<-\epsilon\\ 1/2\epsilon & -\epsilon\le x\le\epsilon\\0 & x\gt-\epsilon\end{cases}$$ Because $$\delta(x) = \delta_{\epsilon\rightarrow0}(x)$$ Therefore in the limit $$\delta(x) = \begin{cases} \infty & x=0 \\ 0 & x\neq0 \end{cases}$$ and that  $$\int_{-\alpha}^\beta \delta(x)dx=1 \;\; \alpha\gt0, \; \beta\gt 0$$ Given the above conclusions, how can the Heaviside step function $H(x)$ be related to the Dirac Delta Function $\delta(x)$ in the following way?$$H(x)=\int_{-\infty}^x\delta(\xi)d\xi$$",,"['ordinary-differential-equations', 'fourier-analysis']"
84,Advection Diffusion Equation on Semi-Infinite Domain,Advection Diffusion Equation on Semi-Infinite Domain,,"Regarding the BVP $$u_t(x,t) - v\, u_x(x,t) = k\, u_{xx}(x,t),\qquad x\geq0$$ with BC $u_x(0, t)=0$ for $t\geq 0$ , and parameters $v,k>0$ , I have some questions. Does an expression for the Green's function in some relatively nice form exist? If so, what is it? There's the obvious change of variables to convert the above to the heat equation, but with non-standard boundary conditions. I could not think of any nice image singularity solution. We also have the problem of the domain being semi-infinite, making it difficult for there to be any series solutions. If such a solution exists, how would I derive such a Green's function? As another note, I would also be happy with a fundamental solution for the BC $u(0,t)=0$ . A related question is how does one go about solving the heat equation with moving boundary conditions.","Regarding the BVP with BC for , and parameters , I have some questions. Does an expression for the Green's function in some relatively nice form exist? If so, what is it? There's the obvious change of variables to convert the above to the heat equation, but with non-standard boundary conditions. I could not think of any nice image singularity solution. We also have the problem of the domain being semi-infinite, making it difficult for there to be any series solutions. If such a solution exists, how would I derive such a Green's function? As another note, I would also be happy with a fundamental solution for the BC . A related question is how does one go about solving the heat equation with moving boundary conditions.","u_t(x,t) - v\, u_x(x,t) = k\, u_{xx}(x,t),\qquad x\geq0 u_x(0, t)=0 t\geq 0 v,k>0 u(0,t)=0","['ordinary-differential-equations', 'partial-differential-equations']"
85,Solving $(y+x^4y^2)dx+xdy=0$,Solving,(y+x^4y^2)dx+xdy=0,"How to solve (in terms of $y$) $(y+x^4y^2)dx+xdy=0$. I know I'm supposed to multiply by an integrating factor to turn this equation into an exact equation. In the previous exercise I proved that in $Mdx+Ndy$ the functions: $\frac{1}{N}\left(\frac{\partial M}{\partial y}-\frac{\partial N}{\partial x}\right)$ $\frac{1}{M}\left(\frac{\partial M}{\partial y}-\frac{\partial N}{\partial x}\right)$ If $M=yf(xy)$ and $N=xg(xy)$ then $\frac{1}{xM-yN}$ is an integrating factor I've tried using 1. and 2., but the calculations are horrible and do not seem to work. I don't think that's the way to go. About 3. I don't know what $f$ and $g$ I should choose. how can I go about solving this differential equation? this isn't homework.","How to solve (in terms of $y$) $(y+x^4y^2)dx+xdy=0$. I know I'm supposed to multiply by an integrating factor to turn this equation into an exact equation. In the previous exercise I proved that in $Mdx+Ndy$ the functions: $\frac{1}{N}\left(\frac{\partial M}{\partial y}-\frac{\partial N}{\partial x}\right)$ $\frac{1}{M}\left(\frac{\partial M}{\partial y}-\frac{\partial N}{\partial x}\right)$ If $M=yf(xy)$ and $N=xg(xy)$ then $\frac{1}{xM-yN}$ is an integrating factor I've tried using 1. and 2., but the calculations are horrible and do not seem to work. I don't think that's the way to go. About 3. I don't know what $f$ and $g$ I should choose. how can I go about solving this differential equation? this isn't homework.",,['ordinary-differential-equations']
86,Clairaut's Equation Singular and General Solutions,Clairaut's Equation Singular and General Solutions,,"I want to know how one how one would prove that the singular solutions to Clairaut's equation are tangent to the General solutions. so I have here: $$y(x) = xy' - e ^{y'}$$ Differentiating $$y' = y' +xy'' - y''e^{y'}$$ $$0 =y''(x-e^{y'})$$ Therefore for the general solution, I have $y'' = 0 \implies y' = c_1 \implies y_g(x) = c_1x + c_2$ Okay thats all well and good. As for the singular solution. I'm still not quite sure how the singular solution differs from the general solution. All I know is it must envelope the family of general solutions as well as be tangent to them (For reasons unknown to me If someone could explain it I would be eternally grateful). The singular solution is found by: let us create some parameter $$y' = p \implies 0 = x-e^p \implies ln|x| = p$$ Plug this back into the original DE to get the singular solution of y: $$y_s(x) = xln|x| - x$$ I want to show that one is tangent to the other at some point (By the way a kind of side note, If the two equations exist at the same point, doesn't that mean that the solutions are NOT unique at that point?) $$y_g(x) = y_s(x) = c_1x + c_2 = xln|x| - x$$ Now what? I solve for some point (x,y)? Then how would I prove that they are tangent? I don't quite understand this part, assuming this is all correct. Thank you anyone for looking at this!","I want to know how one how one would prove that the singular solutions to Clairaut's equation are tangent to the General solutions. so I have here: $$y(x) = xy' - e ^{y'}$$ Differentiating $$y' = y' +xy'' - y''e^{y'}$$ $$0 =y''(x-e^{y'})$$ Therefore for the general solution, I have $y'' = 0 \implies y' = c_1 \implies y_g(x) = c_1x + c_2$ Okay thats all well and good. As for the singular solution. I'm still not quite sure how the singular solution differs from the general solution. All I know is it must envelope the family of general solutions as well as be tangent to them (For reasons unknown to me If someone could explain it I would be eternally grateful). The singular solution is found by: let us create some parameter $$y' = p \implies 0 = x-e^p \implies ln|x| = p$$ Plug this back into the original DE to get the singular solution of y: $$y_s(x) = xln|x| - x$$ I want to show that one is tangent to the other at some point (By the way a kind of side note, If the two equations exist at the same point, doesn't that mean that the solutions are NOT unique at that point?) $$y_g(x) = y_s(x) = c_1x + c_2 = xln|x| - x$$ Now what? I solve for some point (x,y)? Then how would I prove that they are tangent? I don't quite understand this part, assuming this is all correct. Thank you anyone for looking at this!",,"['ordinary-differential-equations', 'singular-solution']"
87,nonlinear first order differential equation,nonlinear first order differential equation,,How can I find an exact solution for this problem ? Is there any technique for cubic nonlinearity as in the case of Bernoulli differential equation? $y'=x^{3}y^{3}-1\\$,How can I find an exact solution for this problem ? Is there any technique for cubic nonlinearity as in the case of Bernoulli differential equation? $y'=x^{3}y^{3}-1\\$,,['ordinary-differential-equations']
88,An existence of global solution of differential equation of first order,An existence of global solution of differential equation of first order,,"Let $f: (a,b) \times \mathbb{R} \rightarrow \mathbb{R}$ be of class $C^1$ in $D:=(a,b) \times \mathbb{R}$ and satisfies condition $$| f(t,x)| \leq A+B|x|  \textrm{ for }  (t,x) \in D,$$ where $A,B$ are fixed real constants and let $t_0 \in (a,b)$. How to prove using the fixed point method that for arbitrary $x_0\in \mathbb{R}$ there exist exactly one solution $x: (a,b)\rightarrow \mathbb{R}$ of differential equation $$\frac{dx}{dt}=f(t,x) $$ with condition $x(t_0)=x_0$ ? Thanks. Added. Maybe it would be. Let $X=\{x:(a,b) \rightarrow \mathbb{R}: \sup_{t\in (a,b)}  e^{-B\gamma|t-t_0|} |x(t)|   <\infty, x(t_0)=x_0 \}$, $d(x,y)=\sup_{t\in (a,b)} e^{-B\gamma|t-t_0|} |x(t)-y(t)|$ for $x,y \in X$, where $\gamma$ is a suitable positive constant. Then $(X,d)$ is a complete metric  space and $Tx(t):=x_0+\int_{t_0}^t f(s,x(s))ds$, for $x \in X$ and $t\in (a,b)$, maps X into itself (because  $|f(s,x(s))|\leq A+Be^{B\gamma|t-t_0|}\cdot sup_{t\in (a,b)} |x(s)|e^{-B\gamma|t-t_0|} |x(t)|$ and      $| \int_{t_0}^t e^{B \gamma |s-t_0|} ds| \leq \frac{1}{B \gamma} e^{B\gamma|t-t_0|}$). However I don't know is it $T$ a contraction with some $\gamma>0$ and whether or not each solution of the differential equation belongs to $X$.","Let $f: (a,b) \times \mathbb{R} \rightarrow \mathbb{R}$ be of class $C^1$ in $D:=(a,b) \times \mathbb{R}$ and satisfies condition $$| f(t,x)| \leq A+B|x|  \textrm{ for }  (t,x) \in D,$$ where $A,B$ are fixed real constants and let $t_0 \in (a,b)$. How to prove using the fixed point method that for arbitrary $x_0\in \mathbb{R}$ there exist exactly one solution $x: (a,b)\rightarrow \mathbb{R}$ of differential equation $$\frac{dx}{dt}=f(t,x) $$ with condition $x(t_0)=x_0$ ? Thanks. Added. Maybe it would be. Let $X=\{x:(a,b) \rightarrow \mathbb{R}: \sup_{t\in (a,b)}  e^{-B\gamma|t-t_0|} |x(t)|   <\infty, x(t_0)=x_0 \}$, $d(x,y)=\sup_{t\in (a,b)} e^{-B\gamma|t-t_0|} |x(t)-y(t)|$ for $x,y \in X$, where $\gamma$ is a suitable positive constant. Then $(X,d)$ is a complete metric  space and $Tx(t):=x_0+\int_{t_0}^t f(s,x(s))ds$, for $x \in X$ and $t\in (a,b)$, maps X into itself (because  $|f(s,x(s))|\leq A+Be^{B\gamma|t-t_0|}\cdot sup_{t\in (a,b)} |x(s)|e^{-B\gamma|t-t_0|} |x(t)|$ and      $| \int_{t_0}^t e^{B \gamma |s-t_0|} ds| \leq \frac{1}{B \gamma} e^{B\gamma|t-t_0|}$). However I don't know is it $T$ a contraction with some $\gamma>0$ and whether or not each solution of the differential equation belongs to $X$.",,"['analysis', 'ordinary-differential-equations']"
89,A series solution of the differential equation: $\frac{d^2u(x)}{dx^2}+ u(x)^n = 0.$,A series solution of the differential equation:,\frac{d^2u(x)}{dx^2}+ u(x)^n = 0.,"Consider the differential equation $\frac{d^2u(x)}{dx^2}+ u(x)^n = 0.$ Let the solution be $u(x) = u_0(x) + p u_1(x) + p^2u_2(x) + \cdots +p^m u_m(x).$ Now we are interested in substituting the above solution into the original differential equation and collecting the coefficients of $p$. Here, we may assume that $m$ and $n$ are positive integers. How can we program this with a computer algebra system such as Maple/Mathematica? Thank you.","Consider the differential equation $\frac{d^2u(x)}{dx^2}+ u(x)^n = 0.$ Let the solution be $u(x) = u_0(x) + p u_1(x) + p^2u_2(x) + \cdots +p^m u_m(x).$ Now we are interested in substituting the above solution into the original differential equation and collecting the coefficients of $p$. Here, we may assume that $m$ and $n$ are positive integers. How can we program this with a computer algebra system such as Maple/Mathematica? Thank you.",,"['ordinary-differential-equations', 'mathematica', 'power-series', 'maple']"
90,prove or disprove: if $y'=y^2-\cos(x)$ then any solution diverges in a finite time,prove or disprove: if  then any solution diverges in a finite time,y'=y^2-\cos(x),"given the following ode, prove or disprove: if $y'=y^2-\cos(x)$ then any solution diverges in a finite time. ""diverge"" means, that there is a point $x$ where the solution isn't continuous (there and thereafter). if $y(0)>1$ or $y(0)<-1$ it's clear how to show it. for instance, take $y(0)>1$ , and let say we have $x$ where for the first time $y(x)=1$ . then according to the mean value theorem there is $t$ where $y'(t)<0$ and then it applies that $y(t)<1$ , thus $x$ is not the first time we reached $y(x)=1$ , therefore there isn't such point. The problem is showing that if $-1<y(0)<1$ it must go out from this range in a finite time. Edit I thought about a solution $y'=y^2-\cos(x)>y^2-1$ , which its solution is $\frac{1-e^{2c+2x}}{e^{2c+2x}+1}$ , which might have a pole. depending on y(0)","given the following ode, prove or disprove: if then any solution diverges in a finite time. ""diverge"" means, that there is a point where the solution isn't continuous (there and thereafter). if or it's clear how to show it. for instance, take , and let say we have where for the first time . then according to the mean value theorem there is where and then it applies that , thus is not the first time we reached , therefore there isn't such point. The problem is showing that if it must go out from this range in a finite time. Edit I thought about a solution , which its solution is , which might have a pole. depending on y(0)",y'=y^2-\cos(x) x y(0)>1 y(0)<-1 y(0)>1 x y(x)=1 t y'(t)<0 y(t)<1 x y(x)=1 -1<y(0)<1 y'=y^2-\cos(x)>y^2-1 \frac{1-e^{2c+2x}}{e^{2c+2x}+1},['ordinary-differential-equations']
91,Strange Lipschitz ODE with two solutions,Strange Lipschitz ODE with two solutions,,"Why does the ODE $$ \theta^{\prime} = (1-\theta^2)^{3/2}, \quad \theta(0) = 0,  $$ have the two (different) solutions $$\theta_1(t) = \frac{t}{\sqrt{1+t^{2}}}, \quad \theta_2(t) = \frac{-t}{\sqrt{1+t^{2}}}, $$ for $t \in (-\delta, \delta)$ , whereby $0<\delta <1$ ? How is this possible, since the right-hand side of the ODE is locally Lipschitz in a small neighbourhood around $0$ ? Please make the universe work again.","Why does the ODE have the two (different) solutions for , whereby ? How is this possible, since the right-hand side of the ODE is locally Lipschitz in a small neighbourhood around ? Please make the universe work again."," \theta^{\prime} = (1-\theta^2)^{3/2}, \quad \theta(0) = 0,   \theta_1(t) = \frac{t}{\sqrt{1+t^{2}}}, \quad \theta_2(t) = \frac{-t}{\sqrt{1+t^{2}}},  t \in (-\delta, \delta) 0<\delta <1 0","['ordinary-differential-equations', 'lipschitz-functions']"
92,Analytical solution to system of linear ODE's,Analytical solution to system of linear ODE's,,"Given $x'=Ax$ with initial data $x(0)=\xi$ , eigenvalues $\lambda_j=0,0,3$ and $$\begin{bmatrix}2&1&0\\0&2&4\\1&0&-1\end{bmatrix},$$ find the general   solution to the system and a canonical Jordan form of the matrix $A$ .   Find all initial conditions such that solutions to the IVP will be   bounded. To do this, the book says I need to use the following: Let the matrix $A$ have $s$ distinct eigenvalues $\lambda_1,\dots,\lambda_s$ with corresponding generalised eigenspaces $E(\lambda_j)$ . Represent the initial data $x(0)=\xi$ for the solution $x(t)$ as a sum of its components from different generalised   eigenspaces: $$\xi=\sum_{j=1}^{s}x^{0,j},\quad x^{0,j}\in > E(\lambda_j).$$ Here $x^{0,j}\in E(\lambda_j)$ - are components of $\xi$ in the generalized eigenspaces $E(\lambda_j)=\ker(A-\lambda_j)^{m_j}$ of the matrix $A$ . These   subspaces intersect only in the origin and are invariant with respect   to $A$ and $\exp(At)$ . It implies that for the solution $x_z(t)$ with   initial data $z\in E(\lambda_j)$ , we have $x_z(t)\in E(\lambda_j)$ for   all $t\in\Bbb R$ . Let $m_j$ be the algebraic multiplicity of the eigenvalue $\lambda_j$ .   We apply the formula $(11)$ to this representation and derive the an   expression for solutions for arbitrary initial data as a finite sum   (intead of series): $$\begin{equation}x(t)=e^{At}x_0=\sum_{j=1}^{s}\left(e^{\lambda_jt}\left[\sum_{k=0}^{m_j-1}(A-\lambda_jI)^{k}\frac{t^k}{k!}\right]x^{0,j}\right)\tag{13}\end{equation}.$$ Series expressing $\exp(At)x^{0,j}$ terminates on each of the   generalised eigenspaces $E(\lambda_j)$ . (Image that replaced text). Let me try: An eigenvector corresponding to $\lambda = 0$ can be taken as $v_1=\begin{bmatrix}1&-2&1\end{bmatrix}^T$ . For $\lambda=3$ we can take eigenvector $v_2=\begin{bmatrix}4&4&1\end{bmatrix}^T$ . Since the eigenvalue of zero is of algebraic multiplicity $2$ we need to find an extra generalized eigenvector $v_1^{(1)}$ such that we can express $\xi$ as a linear combination of these. We solve the equation $(A-\lambda I)v_1^{(1)}=v_1$ . Solving this we get that $v_1^{(1)}=\begin{bmatrix}1&-1&0\end{bmatrix}^T$ . Since we are in $\mathbb{R}^3$ we know that there can not be more than three generalized eigenvectors. In my case, $s=2$ since the multiplicity of the zero eigenvalue is $2$ . Thus expression $(13)$ can be written as \begin{align} x(t)&=\sum_{j=1}^2\left(e^{\lambda_jt}x^{0,j}\left[\sum_{k=0}^{1}(A-\lambda_jI)\frac{t^k}{k!}\right]\right)\\ &= \sum_{j=1}^2\left(e^{\lambda_jt}x^{0,j}\left[1+(A-\lambda_jI)\right]\right) \end{align} The problem I have here is that I don't understand how to express the $x^{0,j}$ .","Given with initial data , eigenvalues and find the general   solution to the system and a canonical Jordan form of the matrix .   Find all initial conditions such that solutions to the IVP will be   bounded. To do this, the book says I need to use the following: Let the matrix have distinct eigenvalues with corresponding generalised eigenspaces . Represent the initial data for the solution as a sum of its components from different generalised   eigenspaces: Here - are components of in the generalized eigenspaces of the matrix . These   subspaces intersect only in the origin and are invariant with respect   to and . It implies that for the solution with   initial data , we have for   all . Let be the algebraic multiplicity of the eigenvalue .   We apply the formula to this representation and derive the an   expression for solutions for arbitrary initial data as a finite sum   (intead of series): Series expressing terminates on each of the   generalised eigenspaces . (Image that replaced text). Let me try: An eigenvector corresponding to can be taken as . For we can take eigenvector . Since the eigenvalue of zero is of algebraic multiplicity we need to find an extra generalized eigenvector such that we can express as a linear combination of these. We solve the equation . Solving this we get that . Since we are in we know that there can not be more than three generalized eigenvectors. In my case, since the multiplicity of the zero eigenvalue is . Thus expression can be written as The problem I have here is that I don't understand how to express the .","x'=Ax x(0)=\xi \lambda_j=0,0,3 \begin{bmatrix}2&1&0\\0&2&4\\1&0&-1\end{bmatrix}, A A s \lambda_1,\dots,\lambda_s E(\lambda_j) x(0)=\xi x(t) \xi=\sum_{j=1}^{s}x^{0,j},\quad x^{0,j}\in
> E(\lambda_j). x^{0,j}\in E(\lambda_j) \xi E(\lambda_j)=\ker(A-\lambda_j)^{m_j} A A \exp(At) x_z(t) z\in E(\lambda_j) x_z(t)\in E(\lambda_j) t\in\Bbb R m_j \lambda_j (11) \begin{equation}x(t)=e^{At}x_0=\sum_{j=1}^{s}\left(e^{\lambda_jt}\left[\sum_{k=0}^{m_j-1}(A-\lambda_jI)^{k}\frac{t^k}{k!}\right]x^{0,j}\right)\tag{13}\end{equation}. \exp(At)x^{0,j} E(\lambda_j) \lambda = 0 v_1=\begin{bmatrix}1&-2&1\end{bmatrix}^T \lambda=3 v_2=\begin{bmatrix}4&4&1\end{bmatrix}^T 2 v_1^{(1)} \xi (A-\lambda I)v_1^{(1)}=v_1 v_1^{(1)}=\begin{bmatrix}1&-1&0\end{bmatrix}^T \mathbb{R}^3 s=2 2 (13) \begin{align}
x(t)&=\sum_{j=1}^2\left(e^{\lambda_jt}x^{0,j}\left[\sum_{k=0}^{1}(A-\lambda_jI)\frac{t^k}{k!}\right]\right)\\
&= \sum_{j=1}^2\left(e^{\lambda_jt}x^{0,j}\left[1+(A-\lambda_jI)\right]\right)
\end{align} x^{0,j}",['ordinary-differential-equations']
93,Solving a Second Order Order linear ODE when one solution is known,Solving a Second Order Order linear ODE when one solution is known,,"The following problem is from the book ""Introduction to Ordinary Differential Equations"" by Shepley L. Ross. Problem: Given that $y = x$ is a solution of $$ (x^2 -2x + 2)y'' - x^2y' + xy = 0$$ find a linearly independent solution by reducing the order. Write the general solution. Answer: Let $f(x)$ represent the solution we have. \begin{align*} f(x) &= x \\ y &= f(x) v = xv \\ y' &= x v' + v \\ y'' &= x v'' + v' + v' = xv'' + 2v' \\ \end{align*} \begin{align*} (x^2 -2x + 2)( xv'' + 2v') - x^2(x v' + v ) + x^2 v  &= 0 \\ (x^2 -2x + 2)( xv'' + 2v') - x^3 v'  &= 0 \\ (x^3 -2x^2 + 2x)xv'' + (-x^3 + 2x^2 -4x + 4)v'  &= 0 \\ (x^4 -2x^3 + 2x^2)v'' + (-x^3 + 2x^2 -4x + 4)v'  &= 0 \\ \end{align*} Now we let $w = \frac{dv}{dx}$ and this gives us a separable differential equation. \begin{align*} (x^4 -2x^3 + 2x^2)w' + (-x^3 + 2x^2 -4x + 4)w  &= 0 \\ (x^4 -2x^3 + 2x^2)w' &= (x^3 - 2x^2 + 4x - 4)w  \\ (x^4 -2x^3 + 2x^2) \,\, dw &= (x^3 - 2x^2 + 4x - 4)w \,\, dx  \\ \frac{dw}{w} &= \frac{ (x^3 - 2x^2 + 4x - 4) \, dx }{ x^4 -2x^3 + 2x^2 } \\ \end{align*} Now we perform the following integration using an online integral calculator: $$ \int \frac{ x^3 - 2x^2 + 4x - 4 }{x^4 -2x^3 + 2x^2} \,\, dx  = 	\frac{\ln{| x^2 - 2x + 2 |}}{2}  + \arctan{ \frac{ 2x - 2 }{2 } } + \frac{2}{x} + C_1 $$ \begin{align*} \ln{|w|} &= \frac{\ln{| x^2 - 2x + 2 |}}{2}  + \arctan{ \frac{ 2x - 2 }{2 } } + \frac{2}{x} + C_1 \\ \end{align*} At this point, I am confident that my attempt to solve the problem is wrong. The book's answer is: $$ y = (x-2)e^{x} $$ I would expect the book's answer to have at least one constant if not two in the answer since there were no initial conditions given. Thanks, Bob Here is my second attempt to solve the problem. I think I go wrong in the last step but I am not sure where. Answer: Let $f(x)$ represent the solution we have. \begin{align*} f(x) &= x \\ y &= f(x) v = xv \\ y' &= x v' + v \\ y'' &= x v'' + v' + v' = xv'' + 2v' \\ \end{align*} \begin{align*} (x^2 -2x + 2)( xv'' + 2v') - x^2(x v' + v ) + x^2 v  &= 0 \\ (x^2 -2x + 2)( xv'' + 2v') - x^3 v'  &= 0 \\ (x^3-2x^2+2x)v'' + ( -x^3 + 2x^2 -4x +4)v' &= 0 \\  \end{align*} Now we let $w = \frac{dv}{dx}$ and this gives us a separable differential equation. \begin{align*} (x^3-2x^2+2x)w' + ( -x^3 + 2x^2 -4x +4)w  &= 0 \\ \frac{dw}{w} &= \frac{x^3 - 2x^2 + 4x - 4}{x^3-2x^2+2x} \, dx \\ \end{align*} Now we need to integrate the right hand side. We perform long division on the right hand side and get: \begin{align*}  \frac{x^3 - 2x^2 + 4x - 4}{x^3-2x^2+2x} &= 1 + \frac{2x - 4}{ x^3-2x^2+2x } \\ \end{align*} Now we use the technique of partial fractions: \begin{align*} \frac{4x - 6}{ x^3-2x^2+2x } &= \frac{2x - 4}{ x(x^2-2x+2  ) } = \frac{A}{x} + \frac{Bx + C}{x^2-2x+2} \\ 2x - 4 &= A(x^2-2x + 2) + (Bx+C)x \\ \text{ We set  $x = 0$ and find } A &= -2 \\ 2x - 4 &= -2(x^2-2x + 2) + (Bx+C)x  = -2x^2 + 4x - 4 + Bx^2 + Cx \\ 0 &= -2 + B \\ B &= 2 \\ 2x - 4 &= -2(x^2-2x + 2) + (Bx+C)x  = -2x^2 + 4x - 4 + 2x^2 + Cx \\ 2x &= -2(x^2-2x + 2) + (Bx+C)x  = -2x^2 + 4x + 2x^2 + Cx \\ C + 4 &= 2 \\ C &= -2 \\ \frac{x^3 - 2x^2 + 4x - 4}{x^3-2x^2+2x} &= 1 - \frac{2}{x} + \frac{2x - 2}{x^2-2x+2}  \end{align*} We need to integrate the following: $$ \frac{2x - 2}{x^2-2x+2} $$ This can be done with the substitution $u = x^2  - 2x + 2$ which gives us $du = (2x - 2) dx$ . \begin{align*} \ln{|w|} &= x - 2 \ln{|x|} + \ln{|x^2-2x+2|} + C_1 \\ \ln{|w|} &= \ln{e^x} - 2 \ln{|x|} + \ln{|x^2-2x+2|} + \ln{ e^{C_1} } \\ w &= C_2 \left( \frac{e^x(x^2-2x+2)}{x^2} \right) \\ \frac{dv}{dx} &= C_2 \left( \frac{e^x(x^2-2x+2)}{x^2} \right) \\ v &= \int \,\, C_2 \frac{e^x(x^2-2x+2)}{x^2} dx \\ \end{align*} At this point, I am confident that my attempt to solve the problem is wrong. The book's answer is: $$ y = (x-2)e^{x} $$","The following problem is from the book ""Introduction to Ordinary Differential Equations"" by Shepley L. Ross. Problem: Given that is a solution of find a linearly independent solution by reducing the order. Write the general solution. Answer: Let represent the solution we have. Now we let and this gives us a separable differential equation. Now we perform the following integration using an online integral calculator: At this point, I am confident that my attempt to solve the problem is wrong. The book's answer is: I would expect the book's answer to have at least one constant if not two in the answer since there were no initial conditions given. Thanks, Bob Here is my second attempt to solve the problem. I think I go wrong in the last step but I am not sure where. Answer: Let represent the solution we have. Now we let and this gives us a separable differential equation. Now we need to integrate the right hand side. We perform long division on the right hand side and get: Now we use the technique of partial fractions: We need to integrate the following: This can be done with the substitution which gives us . At this point, I am confident that my attempt to solve the problem is wrong. The book's answer is:","y = x  (x^2 -2x + 2)y'' - x^2y' + xy = 0 f(x) \begin{align*}
f(x) &= x \\
y &= f(x) v = xv \\
y' &= x v' + v \\
y'' &= x v'' + v' + v' = xv'' + 2v' \\
\end{align*} \begin{align*}
(x^2 -2x + 2)( xv'' + 2v') - x^2(x v' + v ) + x^2 v  &= 0 \\
(x^2 -2x + 2)( xv'' + 2v') - x^3 v'  &= 0 \\
(x^3 -2x^2 + 2x)xv'' + (-x^3 + 2x^2 -4x + 4)v'  &= 0 \\
(x^4 -2x^3 + 2x^2)v'' + (-x^3 + 2x^2 -4x + 4)v'  &= 0 \\
\end{align*} w = \frac{dv}{dx} \begin{align*}
(x^4 -2x^3 + 2x^2)w' + (-x^3 + 2x^2 -4x + 4)w  &= 0 \\
(x^4 -2x^3 + 2x^2)w' &= (x^3 - 2x^2 + 4x - 4)w  \\
(x^4 -2x^3 + 2x^2) \,\, dw &= (x^3 - 2x^2 + 4x - 4)w \,\, dx  \\
\frac{dw}{w} &= \frac{ (x^3 - 2x^2 + 4x - 4) \, dx }{ x^4 -2x^3 + 2x^2 } \\
\end{align*}  \int \frac{ x^3 - 2x^2 + 4x - 4 }{x^4 -2x^3 + 2x^2} \,\, dx  =
	\frac{\ln{| x^2 - 2x + 2 |}}{2}  + \arctan{ \frac{ 2x - 2 }{2 } } + \frac{2}{x} + C_1  \begin{align*}
\ln{|w|} &= \frac{\ln{| x^2 - 2x + 2 |}}{2}  + \arctan{ \frac{ 2x - 2 }{2 } } + \frac{2}{x} + C_1 \\
\end{align*}  y = (x-2)e^{x}  f(x) \begin{align*}
f(x) &= x \\
y &= f(x) v = xv \\
y' &= x v' + v \\
y'' &= x v'' + v' + v' = xv'' + 2v' \\
\end{align*} \begin{align*}
(x^2 -2x + 2)( xv'' + 2v') - x^2(x v' + v ) + x^2 v  &= 0 \\
(x^2 -2x + 2)( xv'' + 2v') - x^3 v'  &= 0 \\
(x^3-2x^2+2x)v'' + ( -x^3 + 2x^2 -4x +4)v' &= 0 \\ 
\end{align*} w = \frac{dv}{dx} \begin{align*}
(x^3-2x^2+2x)w' + ( -x^3 + 2x^2 -4x +4)w  &= 0 \\
\frac{dw}{w} &= \frac{x^3 - 2x^2 + 4x - 4}{x^3-2x^2+2x} \, dx \\
\end{align*} \begin{align*}
 \frac{x^3 - 2x^2 + 4x - 4}{x^3-2x^2+2x} &= 1 + \frac{2x - 4}{ x^3-2x^2+2x } \\
\end{align*} \begin{align*}
\frac{4x - 6}{ x^3-2x^2+2x } &= \frac{2x - 4}{ x(x^2-2x+2  ) } = \frac{A}{x} + \frac{Bx + C}{x^2-2x+2} \\
2x - 4 &= A(x^2-2x + 2) + (Bx+C)x \\
\text{ We set  x = 0 and find } A &= -2 \\
2x - 4 &= -2(x^2-2x + 2) + (Bx+C)x  = -2x^2 + 4x - 4 + Bx^2 + Cx \\
0 &= -2 + B \\
B &= 2 \\
2x - 4 &= -2(x^2-2x + 2) + (Bx+C)x  = -2x^2 + 4x - 4 + 2x^2 + Cx \\
2x &= -2(x^2-2x + 2) + (Bx+C)x  = -2x^2 + 4x + 2x^2 + Cx \\
C + 4 &= 2 \\
C &= -2 \\
\frac{x^3 - 2x^2 + 4x - 4}{x^3-2x^2+2x} &= 1 - \frac{2}{x} + \frac{2x - 2}{x^2-2x+2} 
\end{align*}  \frac{2x - 2}{x^2-2x+2}  u = x^2  - 2x + 2 du = (2x - 2) dx \begin{align*}
\ln{|w|} &= x - 2 \ln{|x|} + \ln{|x^2-2x+2|} + C_1 \\
\ln{|w|} &= \ln{e^x} - 2 \ln{|x|} + \ln{|x^2-2x+2|} + \ln{ e^{C_1} } \\
w &= C_2 \left( \frac{e^x(x^2-2x+2)}{x^2} \right) \\
\frac{dv}{dx} &= C_2 \left( \frac{e^x(x^2-2x+2)}{x^2} \right) \\
v &= \int \,\, C_2 \frac{e^x(x^2-2x+2)}{x^2} dx \\
\end{align*}  y = (x-2)e^{x} ",['ordinary-differential-equations']
94,How to solve $\ddot{y} = t^2$,How to solve,\ddot{y} = t^2,"I am having trouble to solve $\ddot{y} = t^2$. Step 1: Find the homogenous solution: (this part is simple) $$y_H = c_1+c_2t$$ Step 2: Find the particular solution:  Since the nonhomogenous part is a polynomial of degree $2$, so $$y_P = At^2+Bt+C$$ Step 3:  $y = y_H+y_P$ and then plug in: We have $$2A = t^2$$ which is not correct. How should I modify this?  please advise, thanks!","I am having trouble to solve $\ddot{y} = t^2$. Step 1: Find the homogenous solution: (this part is simple) $$y_H = c_1+c_2t$$ Step 2: Find the particular solution:  Since the nonhomogenous part is a polynomial of degree $2$, so $$y_P = At^2+Bt+C$$ Step 3:  $y = y_H+y_P$ and then plug in: We have $$2A = t^2$$ which is not correct. How should I modify this?  please advise, thanks!",,"['ordinary-differential-equations', 'derivatives']"
95,Intuition of the Wronskian,Intuition of the Wronskian,,"I've got a question regarding the intuition of a Wronskian, in the following sense: The intuition for the determinant of a square $n \times n $-matrix is that it represents the area/(hyper-)volume between vectors. But what is the intuition behind the Wronskian of let's say two linear functions $f_1 = 0.5x+4$ and $f_2 = -2x+4$ which is $-10$? What is the (geometric/graphical? )intuition of the value $-10$? Is there any intuition in such a sense possible? Thank you in advance for hints. Best regards","I've got a question regarding the intuition of a Wronskian, in the following sense: The intuition for the determinant of a square $n \times n $-matrix is that it represents the area/(hyper-)volume between vectors. But what is the intuition behind the Wronskian of let's say two linear functions $f_1 = 0.5x+4$ and $f_2 = -2x+4$ which is $-10$? What is the (geometric/graphical? )intuition of the value $-10$? Is there any intuition in such a sense possible? Thank you in advance for hints. Best regards",,"['ordinary-differential-equations', 'determinant', 'wronskian']"
96,Elementary properties of gradient systems,Elementary properties of gradient systems,,"Consider $x_0\in\mathbb{R}^n$ and a $C^{1,1}$ function $f:\mathbb{R}^n\rightarrow\mathbb{R}$ (that is, a differentiable function whose gradient is Lipschitz function). Consider the system $$ \begin{cases} \dot{x}(t)=-\nabla f(x(t)),\\ x(0)=x_0. \end{cases} $$ Let $\gamma_{x_0}(t)$ be a trajectory (orbit) of the above system which starts at the point $x_0$ is defined for all $t$ in the maximal interval $[0,T_{x_0})$ ( $T_{x_0}\in(0,+\infty]$ ). I am interested in the following elementary properties of the trajectory (orbit) $\gamma_{x_0}(t)$ : Consider the mapping $t\mapsto\rho(t):=f(\gamma_{x_0}(t))$ . Prove that if $\nabla f(x_0)\ne 0$ then $\rho(t)$ is strictly deacreasing. The $\omega-$ limit set of $\gamma_{x_0}$ is defined by $$ \Omega(\gamma_{x_0}):=\{\gamma_\infty:\exists (t_n)\rightarrow\infty, \lim\gamma_{x_0}(t_n)=\gamma_\infty\}. $$ Prove that if $\gamma_{x_0}$ is bounded (that is, there exists a bounded set $B$ such that $\gamma_{x_0}(t)\in B$ for all $t$ ) then: $\Omega(\gamma_{x_0})$ is either singleton or infinite; $\nabla f(\gamma_\infty)=0$ for all $\gamma_\infty\in\Omega(\gamma_{x_0})$ ; $f$ is constant on $\Omega(\gamma_{x_0})$ ; $\text{dist}(\gamma_{x_0}(t),\Omega(\gamma_{x_0}))\leq\text{dist}(\gamma_{x_0}(t), S)$ , where $$ S:=\{x:\nabla f(x)=0\}. $$ Moreover, $\text{dist}(\gamma_{x_0}(t), S)\rightarrow 0$ as $t\rightarrow\infty$ . If $\gamma_{x_0}$ has finite length, that is $$ \text{length}(\gamma):=\int_{0}^{T_{x_0}}\|\dot{\gamma}(t)\| dt<+\infty$$ then $\Omega(\gamma_{x_0})$ is a singleton which is denoted by $\gamma_\infty$ and $$ \lim_{t\rightarrow\infty}\gamma_{x_0}(t)=\gamma_\infty. $$ My attempt The derivative of $\rho$ is given by $$ \rho^{\prime}(t)=\langle\nabla f(\gamma_{x_0}(t)),\dot{\gamma}(t)\rangle=-\|\nabla f(\gamma_{x_0}(t)\|^2=-\|\dot{\gamma}(t)\|^2\leq 0. $$ Hence $\rho(t)$ is decreasing. But I cannot know how to prove $\rho(t)$ is strictly decreasing. I have tried to prove these properties but it is uneasy to prove them. I would be grateful if someone could help me to make clear all the properties. Thank you for all kind help.","Consider and a function (that is, a differentiable function whose gradient is Lipschitz function). Consider the system Let be a trajectory (orbit) of the above system which starts at the point is defined for all in the maximal interval ( ). I am interested in the following elementary properties of the trajectory (orbit) : Consider the mapping . Prove that if then is strictly deacreasing. The limit set of is defined by Prove that if is bounded (that is, there exists a bounded set such that for all ) then: is either singleton or infinite; for all ; is constant on ; , where Moreover, as . If has finite length, that is then is a singleton which is denoted by and My attempt The derivative of is given by Hence is decreasing. But I cannot know how to prove is strictly decreasing. I have tried to prove these properties but it is uneasy to prove them. I would be grateful if someone could help me to make clear all the properties. Thank you for all kind help.","x_0\in\mathbb{R}^n C^{1,1} f:\mathbb{R}^n\rightarrow\mathbb{R} 
\begin{cases}
\dot{x}(t)=-\nabla f(x(t)),\\
x(0)=x_0.
\end{cases}
 \gamma_{x_0}(t) x_0 t [0,T_{x_0}) T_{x_0}\in(0,+\infty] \gamma_{x_0}(t) t\mapsto\rho(t):=f(\gamma_{x_0}(t)) \nabla f(x_0)\ne 0 \rho(t) \omega- \gamma_{x_0} 
\Omega(\gamma_{x_0}):=\{\gamma_\infty:\exists (t_n)\rightarrow\infty, \lim\gamma_{x_0}(t_n)=\gamma_\infty\}.
 \gamma_{x_0} B \gamma_{x_0}(t)\in B t \Omega(\gamma_{x_0}) \nabla f(\gamma_\infty)=0 \gamma_\infty\in\Omega(\gamma_{x_0}) f \Omega(\gamma_{x_0}) \text{dist}(\gamma_{x_0}(t),\Omega(\gamma_{x_0}))\leq\text{dist}(\gamma_{x_0}(t), S) 
S:=\{x:\nabla f(x)=0\}.
 \text{dist}(\gamma_{x_0}(t), S)\rightarrow 0 t\rightarrow\infty \gamma_{x_0} 
\text{length}(\gamma):=\int_{0}^{T_{x_0}}\|\dot{\gamma}(t)\|
dt<+\infty \Omega(\gamma_{x_0}) \gamma_\infty 
\lim_{t\rightarrow\infty}\gamma_{x_0}(t)=\gamma_\infty.
 \rho 
\rho^{\prime}(t)=\langle\nabla f(\gamma_{x_0}(t)),\dot{\gamma}(t)\rangle=-\|\nabla f(\gamma_{x_0}(t)\|^2=-\|\dot{\gamma}(t)\|^2\leq 0.
 \rho(t) \rho(t)","['ordinary-differential-equations', 'differential-geometry', 'dynamical-systems', 'gradient-flows']"
97,Positiveness of energy of differential equation,Positiveness of energy of differential equation,,"Let $x(t) : [0,T] \rightarrow \mathbb{R}^n$ be a solution of a differential equation   $$ \frac{d}{dt} x(t) = f(x(t),t). $$   In addition we have functions $E :\mathbb{R}^n \rightarrow \mathbb{R}$ and  $h:\mathbb{R}^{n+1}\rightarrow \mathbb{R}$  such that   $$ \frac{d}{dt}{E(x(t))} = h(x(t),t) E(x(t)) $$   for any solution $x(t)$ of the differential equation. Can we show that if $E$ is positive at the time $0$ then it is positive at all times? i.e.   $$ E(x(0))>0 \Longrightarrow E(x(t)) \qquad 0<t\leq T $$   Assume that all functions are at least one continuously differentiable. It is easy to show that it is true if $h$ does not depend on $x$ or can be written in the form $h(x(t),t) =\hat h(E(x(t)),t)$. Then $E$ satisfy following differential equation $$ \frac{d}{dt}{E(t)} = \hat h(E(t),t) E(t) $$ Thanks to the uniqueness of the solution, any solution $E(t)$ cannot cross the trivial solution $E(t)=0$ and therefore it has to have the same sign at all times. Application: Let's define matrix $A(t)$ via differential equation $$ \frac{d}{dt}{A(t)} = B(A(t),t) A(t) \qquad A(0) = I $$ where $B$ is matrix valued function with arguments $A$ and $t$. Is $A(t)$ invertible for all $t\geq 0$? We have  $$ \frac{d}{dt}{\det{A(t)}} = Tr(B(A(t),t)) \det{A(t)}  $$ If $B$ is just a function of $t$ and $\det{A(t)}$ then the answer is yes, in general I do not know if $A(t)$ is invertible or not.","Let $x(t) : [0,T] \rightarrow \mathbb{R}^n$ be a solution of a differential equation   $$ \frac{d}{dt} x(t) = f(x(t),t). $$   In addition we have functions $E :\mathbb{R}^n \rightarrow \mathbb{R}$ and  $h:\mathbb{R}^{n+1}\rightarrow \mathbb{R}$  such that   $$ \frac{d}{dt}{E(x(t))} = h(x(t),t) E(x(t)) $$   for any solution $x(t)$ of the differential equation. Can we show that if $E$ is positive at the time $0$ then it is positive at all times? i.e.   $$ E(x(0))>0 \Longrightarrow E(x(t)) \qquad 0<t\leq T $$   Assume that all functions are at least one continuously differentiable. It is easy to show that it is true if $h$ does not depend on $x$ or can be written in the form $h(x(t),t) =\hat h(E(x(t)),t)$. Then $E$ satisfy following differential equation $$ \frac{d}{dt}{E(t)} = \hat h(E(t),t) E(t) $$ Thanks to the uniqueness of the solution, any solution $E(t)$ cannot cross the trivial solution $E(t)=0$ and therefore it has to have the same sign at all times. Application: Let's define matrix $A(t)$ via differential equation $$ \frac{d}{dt}{A(t)} = B(A(t),t) A(t) \qquad A(0) = I $$ where $B$ is matrix valued function with arguments $A$ and $t$. Is $A(t)$ invertible for all $t\geq 0$? We have  $$ \frac{d}{dt}{\det{A(t)}} = Tr(B(A(t),t)) \det{A(t)}  $$ If $B$ is just a function of $t$ and $\det{A(t)}$ then the answer is yes, in general I do not know if $A(t)$ is invertible or not.",,['ordinary-differential-equations']
98,Power series method for differential equation $x^2y''+y=0$,Power series method for differential equation,x^2y''+y=0,"I tried to solve $(x^2)y''+y=0$ using power series, but I cannot get the general solution or the relation at least $$(x^2)y''+y=0 $$ $$ \sum_{n=2}^\infty c_n n(n-1) x^n + \sum_{n=0}^\infty c_n x^n$$ $$ \sum_{k=2}^\infty c_k k(k-1) x^k + \sum_{k=0}^\infty c_k x^k$$ $$ c_o+c_1+\sum_{k=2}^\infty c_k (k^2-k+1) x^k$$ From here $C_0=0$, $C_1=0$, $ C_k(k^2 -k+1)=0$. Is this right? How can I get a recurrence relation for coefficients or the general solution of the series?","I tried to solve $(x^2)y''+y=0$ using power series, but I cannot get the general solution or the relation at least $$(x^2)y''+y=0 $$ $$ \sum_{n=2}^\infty c_n n(n-1) x^n + \sum_{n=0}^\infty c_n x^n$$ $$ \sum_{k=2}^\infty c_k k(k-1) x^k + \sum_{k=0}^\infty c_k x^k$$ $$ c_o+c_1+\sum_{k=2}^\infty c_k (k^2-k+1) x^k$$ From here $C_0=0$, $C_1=0$, $ C_k(k^2 -k+1)=0$. Is this right? How can I get a recurrence relation for coefficients or the general solution of the series?",,"['ordinary-differential-equations', 'power-series']"
99,"How should a DE course be re-written, if Gian-Carlo Rota is correct?","How should a DE course be re-written, if Gian-Carlo Rota is correct?",,"Admittedly a soft question but an important one, I think.  The questions I've asked below are questions that can be answered, and not just discussed. I read this essay yesterday by Gian-Carlo Rota denouncing the structure of most differential equations courses.  In the essay, he says that the structure of these courses hasn't changed since the 19th century, almost word for word in some cases, and is filled with redundant lessons. In particular, he has a problem with the teaching of exact equations, integrating factors, homogeneous differential equations, and existence and uniqueness of solutions, saying that all of these topics and techniques are of no use whatsoever. He believes the bulk of DE courses should be linear DEs with constant coefficients (not variable coefficients), linear algebra, and Laplace transforms. Are most people here in agreement with this?  If so, how would a better DE course be built from scratch? EDIT: I just looked at MIT's syllabus for Differential Equations, and it looks like it reflects what Rota said.  So there's truth in it, though I haven't checked yet whether or not the course text by Edwards & Penney follows suit.","Admittedly a soft question but an important one, I think.  The questions I've asked below are questions that can be answered, and not just discussed. I read this essay yesterday by Gian-Carlo Rota denouncing the structure of most differential equations courses.  In the essay, he says that the structure of these courses hasn't changed since the 19th century, almost word for word in some cases, and is filled with redundant lessons. In particular, he has a problem with the teaching of exact equations, integrating factors, homogeneous differential equations, and existence and uniqueness of solutions, saying that all of these topics and techniques are of no use whatsoever. He believes the bulk of DE courses should be linear DEs with constant coefficients (not variable coefficients), linear algebra, and Laplace transforms. Are most people here in agreement with this?  If so, how would a better DE course be built from scratch? EDIT: I just looked at MIT's syllabus for Differential Equations, and it looks like it reflects what Rota said.  So there's truth in it, though I haven't checked yet whether or not the course text by Edwards & Penney follows suit.",,"['ordinary-differential-equations', 'education']"
