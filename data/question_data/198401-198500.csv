,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Is integral curve a embedded 1 dimensional submanifold of the given manifold?,Is integral curve a embedded 1 dimensional submanifold of the given manifold?,,I can easily see a proof that shows its going to be an immersed submanifold . (I am removing the case if the vector field at that point is 0). I am not able to see if it's a embedded submanifold or not? Thank you.,I can easily see a proof that shows its going to be an immersed submanifold . (I am removing the case if the vector field at that point is 0). I am not able to see if it's a embedded submanifold or not? Thank you.,,['differential-geometry']
1,"The limit of a uniform convergent sequence of isometries is an isometry (problem 6-3 of Lee's ""Riemannian manifolds"")","The limit of a uniform convergent sequence of isometries is an isometry (problem 6-3 of Lee's ""Riemannian manifolds"")",,"I'm trying to prove the following theorem: let $f_n : M \to N $ a sequence of isometries of Riemannian manifolds that converges uniformly to a function $f:M \to N$: prove that $f$ is an isometry too. What I did already: It is enough to prove the theorem for $M,N$ connected, so we can use the fact that $f$ is a Riemannian isometry if and only if it is a metric isometry, with respect to Riemannian distance. Now, $f$ is continuous because it is the limit of a uniformly convergent sequence of continuous maps, and preserves the distance because the distance function is continuous. So, $f$ is also injective, and then it is open by the invariance of domain theorem. Now, it suffices to show that $f$ is closed or surjective. Any help?","I'm trying to prove the following theorem: let $f_n : M \to N $ a sequence of isometries of Riemannian manifolds that converges uniformly to a function $f:M \to N$: prove that $f$ is an isometry too. What I did already: It is enough to prove the theorem for $M,N$ connected, so we can use the fact that $f$ is a Riemannian isometry if and only if it is a metric isometry, with respect to Riemannian distance. Now, $f$ is continuous because it is the limit of a uniformly convergent sequence of continuous maps, and preserves the distance because the distance function is continuous. So, $f$ is also injective, and then it is open by the invariance of domain theorem. Now, it suffices to show that $f$ is closed or surjective. Any help?",,"['differential-geometry', 'riemannian-geometry', 'uniform-convergence', 'smooth-manifolds']"
2,How to visualize differential forms geometrically,How to visualize differential forms geometrically,,"I've been attempting to teach myself differential geometry and I have heard that one can visualise them geometrically and that this can sometimes be helpful for an intuitive understanding of them. For example, I've heard that one can visualise a one-form in 3-d as a collection of surfaces and that when one integrates such a one-form over a path it can be thought of loosely as counting the number of these surfaces that this path ""pierces"" as one ""moves"" along it. What is the reasoning behind this? Why can one (at least heuristically) think of differential forms in this way?","I've been attempting to teach myself differential geometry and I have heard that one can visualise them geometrically and that this can sometimes be helpful for an intuitive understanding of them. For example, I've heard that one can visualise a one-form in 3-d as a collection of surfaces and that when one integrates such a one-form over a path it can be thought of loosely as counting the number of these surfaces that this path ""pierces"" as one ""moves"" along it. What is the reasoning behind this? Why can one (at least heuristically) think of differential forms in this way?",,"['differential-geometry', 'intuition', 'differential-forms']"
3,"Is it meaningful to take ""exterior products"" of vector fields?","Is it meaningful to take ""exterior products"" of vector fields?",,"Let $M$ denote a smooth manifold. I've read that a differential $k$-form is a smooth section of the $k$th exterior power of the cotangent bundle of $M$. However I barely understand what this means, and I'm trying to understand it better by tinkering with the definition. It seems that there is a notion of ""$k$-vectorfield"" obtained by putting the tangent bundle in place of the cotangent bundle. As in: Potentially Silly Definition . A $k$-vectorfield is a smooth section of the $k$th exterior power of the tangent bundle of $M$. Following this line of thought, it seems that we can take wedge products of vector fields. As in: $$f \frac{\partial}{\partial x} \wedge \frac{\partial}{\partial y} + g\frac{\partial}{\partial y} \wedge \frac{\partial}{\partial z}$$ Question. Is this, like, a thing? If not, why is it only the cotangent bundle whose exterior powers make sense and/or matter?","Let $M$ denote a smooth manifold. I've read that a differential $k$-form is a smooth section of the $k$th exterior power of the cotangent bundle of $M$. However I barely understand what this means, and I'm trying to understand it better by tinkering with the definition. It seems that there is a notion of ""$k$-vectorfield"" obtained by putting the tangent bundle in place of the cotangent bundle. As in: Potentially Silly Definition . A $k$-vectorfield is a smooth section of the $k$th exterior power of the tangent bundle of $M$. Following this line of thought, it seems that we can take wedge products of vector fields. As in: $$f \frac{\partial}{\partial x} \wedge \frac{\partial}{\partial y} + g\frac{\partial}{\partial y} \wedge \frac{\partial}{\partial z}$$ Question. Is this, like, a thing? If not, why is it only the cotangent bundle whose exterior powers make sense and/or matter?",,['differential-geometry']
4,The Product map of a Lie Group is a Submersion.,The Product map of a Lie Group is a Submersion.,,"Problem 7.1 of Lee's Introduction to Smooth Manifolds (2nd Edition) reads: Show that for a Lie group $G$, the multiplication map $\mu:G\times G\to G$ is a submersion (Hint: Use Local Sections). I did the following: Fix $g, h\in G$. Then since $T_{(g, h)}(G\times G)\cong T_g G\oplus T_h G$, we have $$ d\mu_{(g, h)}(X, Y)= d(\mu\circ i^h)_gX+d(\mu\circ j^g)_h Y $$ for $X\in T_gG, Y\in T_hG$, where $i^h:G\to G\times G$ is the map defined as $i^h(x)=(x, h)$ for all $x\in G$ and similarly for $y^g$. Thus we have $$ d\mu_{(g, h)}(X, Y)= dR_h|_gX+dL_g|_hY $$ Since $dR_h|_g:T_gG\to T_{gh}G$ is a linear isomorphism, we see that the rank of $\mu$ is full. So we are done. I do not see how to do it using the hint Lee has given Can somebody please do it using the hint?","Problem 7.1 of Lee's Introduction to Smooth Manifolds (2nd Edition) reads: Show that for a Lie group $G$, the multiplication map $\mu:G\times G\to G$ is a submersion (Hint: Use Local Sections). I did the following: Fix $g, h\in G$. Then since $T_{(g, h)}(G\times G)\cong T_g G\oplus T_h G$, we have $$ d\mu_{(g, h)}(X, Y)= d(\mu\circ i^h)_gX+d(\mu\circ j^g)_h Y $$ for $X\in T_gG, Y\in T_hG$, where $i^h:G\to G\times G$ is the map defined as $i^h(x)=(x, h)$ for all $x\in G$ and similarly for $y^g$. Thus we have $$ d\mu_{(g, h)}(X, Y)= dR_h|_gX+dL_g|_hY $$ Since $dR_h|_g:T_gG\to T_{gh}G$ is a linear isomorphism, we see that the rank of $\mu$ is full. So we are done. I do not see how to do it using the hint Lee has given Can somebody please do it using the hint?",,"['differential-geometry', 'lie-groups']"
5,Why can't that be an uncountable union?,Why can't that be an uncountable union?,,"I'm reading part of Lee's Introduction to manifolds. I have come to the following proposition. $\textbf{Proposition 14.6 (Local Structure of Integral Manifolds).}$ Let $D$ be an involutive $k$ -dimensional distribution on a smooth manifold $M$ , and let $(U,\varphi)$ be a flat chart for $D$ . If $N$ is any integral manifold of $D$ , then $N\cap U$ is a countable disjoint union of open subsets of $k$ -dimensional slices of $U$ , each of which is open in $N$ and embedded in $M$ . Proof. Because the inclusion map $\eta:N\hookrightarrow M$ is continuous, $N\cap U=\eta^{-1}(U)$ is open in $N$ , and thus consists of a countable disjoint union of connected components, each of which is open in $N$ . The proof then continues, and I will read the rest shortly. I was just wondering: why can't $\iota^{-1}(U)$ be an uncountable union of disjoint connected open components?","I'm reading part of Lee's Introduction to manifolds. I have come to the following proposition. Let be an involutive -dimensional distribution on a smooth manifold , and let be a flat chart for . If is any integral manifold of , then is a countable disjoint union of open subsets of -dimensional slices of , each of which is open in and embedded in . Proof. Because the inclusion map is continuous, is open in , and thus consists of a countable disjoint union of connected components, each of which is open in . The proof then continues, and I will read the rest shortly. I was just wondering: why can't be an uncountable union of disjoint connected open components?","\textbf{Proposition 14.6 (Local Structure of Integral Manifolds).} D k M (U,\varphi) D N D N\cap U k U N M \eta:N\hookrightarrow M N\cap U=\eta^{-1}(U) N N \iota^{-1}(U)","['differential-geometry', 'manifolds']"
6,Is a principal bundle automorphism locally given by a left action?,Is a principal bundle automorphism locally given by a left action?,,"Let $G\hookrightarrow P \xrightarrow{\pi} M$ be a principal bundle, denote by $\cdot$ the right action of $G$ on $P$. Let $f:P\rightarrow P$ a bundle automorphism (i.e. $f$ is a diffeo, $f(p \cdot g) = f(p)\cdot g$, $\pi\circ f=\pi$). Is it true that with respect to a local trivialisation the action of $f$ is given by the left multiplication on the fibres? The reason why I think so is the following one. Let $(U,\Phi)$ be a local trivialisation, $\Phi:\pi^{-1}(U) \rightarrow U\times G$, $\Phi(p)=(\pi(p),\phi(p))$. Since $f$ does not move base points, $\Phi\circ f\circ \Phi^{-1}(x,e) = (x,h(x)) $ for some $h(x)=\phi(f(\Phi^{-1}(x,g))\in G$. Then $\Phi\circ f\circ \Phi^{-1}(x,g) = \Phi( f(\Phi^{-1}(x,e)\cdot g)=(x,\phi(\Phi^{-1}(x,e)) g)=(x,h(x) g) $ and with respect to the local trivialisation the action of $f$ is given by left multiplication on the fibres by $h:U\rightarrow G$. Is the above correct?","Let $G\hookrightarrow P \xrightarrow{\pi} M$ be a principal bundle, denote by $\cdot$ the right action of $G$ on $P$. Let $f:P\rightarrow P$ a bundle automorphism (i.e. $f$ is a diffeo, $f(p \cdot g) = f(p)\cdot g$, $\pi\circ f=\pi$). Is it true that with respect to a local trivialisation the action of $f$ is given by the left multiplication on the fibres? The reason why I think so is the following one. Let $(U,\Phi)$ be a local trivialisation, $\Phi:\pi^{-1}(U) \rightarrow U\times G$, $\Phi(p)=(\pi(p),\phi(p))$. Since $f$ does not move base points, $\Phi\circ f\circ \Phi^{-1}(x,e) = (x,h(x)) $ for some $h(x)=\phi(f(\Phi^{-1}(x,g))\in G$. Then $\Phi\circ f\circ \Phi^{-1}(x,g) = \Phi( f(\Phi^{-1}(x,e)\cdot g)=(x,\phi(\Phi^{-1}(x,e)) g)=(x,h(x) g) $ and with respect to the local trivialisation the action of $f$ is given by left multiplication on the fibres by $h:U\rightarrow G$. Is the above correct?",,"['differential-geometry', 'fiber-bundles', 'principal-bundles']"
7,Making a bijection into a diffeomorphism,Making a bijection into a diffeomorphism,,"Given a set $M$, one that can be made into a smooth manifold, and a bijection $f:M\to M$, does there exist a differentiable structure on $M$ such that $f$ is a diffeomorphism? In case it's not always true, what should $M$ and $f$ satisfy in order to make that possible? If such an structure does exist, is it unique in some sense, in general, are any two differentiable structures on $M$ sharing the disired property diffeomorphic to each other?","Given a set $M$, one that can be made into a smooth manifold, and a bijection $f:M\to M$, does there exist a differentiable structure on $M$ such that $f$ is a diffeomorphism? In case it's not always true, what should $M$ and $f$ satisfy in order to make that possible? If such an structure does exist, is it unique in some sense, in general, are any two differentiable structures on $M$ sharing the disired property diffeomorphic to each other?",,['differential-geometry']
8,Sphere curvature as calculated from Liouville's equation,Sphere curvature as calculated from Liouville's equation,,"Liouville's equation for Gauss curvature tells us, that when Riemannian metric has the form $f^2(du^2+dv^2)$, then its Gauss curvature $K$ is expressed by the following equation: $$-Kf^2=\Delta_{0}log(f)$$ where $\Delta_{0}=\frac{\partial^2 }{\partial u^2}+\frac{\partial^2 }{\partial v^2}$. Consider a sphere with metric (due to stereographic projection): $$\frac{du^2+dv^2}{(1+u^2+v^2)^2}$$ Therefore we can apply Liouville's equation to find the curvature of the sphere with $f=\frac{1}{1+u^2+v^2}$.However, calculating the curvature of the sphere in this way I get $K=4$, which is not true. And I cann't spot my mistake.  Here are some calculations: $$log(f)=-log(1+u^2+v^2)$$ $$\frac{\partial^2 }{\partial u^2}log(f)=-\frac{2(1-u^2+v^2)}{(1+u^2+v^2)^2}$$ $$\frac{\partial^2 }{\partial u^2}log(f)=-\frac{2(1+u^2-v^2)}{(1+u^2+v^2)^2}$$ $$\Delta_{0}log(f)=-\frac{2(1-u^2+v^2)}{(1+u^2+v^2)^2}-\frac{2(1+u^2-v^2)}{(1+u^2+v^2)^2}=\frac{-4}{(1+u^2+v^2)^2}$$ Therefore: $$-K\frac{1}{(1+u^2+v^2)^2}=\frac{-4}{(1+u^2+v^2)^2}$$ Hence Gauss curvature of the sphere is (not): $$K=4$$ My question: where is that mistake?","Liouville's equation for Gauss curvature tells us, that when Riemannian metric has the form $f^2(du^2+dv^2)$, then its Gauss curvature $K$ is expressed by the following equation: $$-Kf^2=\Delta_{0}log(f)$$ where $\Delta_{0}=\frac{\partial^2 }{\partial u^2}+\frac{\partial^2 }{\partial v^2}$. Consider a sphere with metric (due to stereographic projection): $$\frac{du^2+dv^2}{(1+u^2+v^2)^2}$$ Therefore we can apply Liouville's equation to find the curvature of the sphere with $f=\frac{1}{1+u^2+v^2}$.However, calculating the curvature of the sphere in this way I get $K=4$, which is not true. And I cann't spot my mistake.  Here are some calculations: $$log(f)=-log(1+u^2+v^2)$$ $$\frac{\partial^2 }{\partial u^2}log(f)=-\frac{2(1-u^2+v^2)}{(1+u^2+v^2)^2}$$ $$\frac{\partial^2 }{\partial u^2}log(f)=-\frac{2(1+u^2-v^2)}{(1+u^2+v^2)^2}$$ $$\Delta_{0}log(f)=-\frac{2(1-u^2+v^2)}{(1+u^2+v^2)^2}-\frac{2(1+u^2-v^2)}{(1+u^2+v^2)^2}=\frac{-4}{(1+u^2+v^2)^2}$$ Therefore: $$-K\frac{1}{(1+u^2+v^2)^2}=\frac{-4}{(1+u^2+v^2)^2}$$ Hence Gauss curvature of the sphere is (not): $$K=4$$ My question: where is that mistake?",,"['differential-geometry', 'curvature']"
9,Need help understanding a relation between the fundamental forms,Need help understanding a relation between the fundamental forms,,"The book I am reading briefly mentions this relation between the fundamental forms but gives no explanation of how they got it. Take the following as the Weingarten Map/Shape Operator where $\nu$ is the gauss map and f is a surface element. $$L =  - D\nu  \circ {(Df)^{ - 1}}$$ Then it defines the fundamental forms as... $$I(X,Y) = \left\langle {X,Y} \right\rangle $$ $$II(X,Y) = I(LX,Y)$$ $$III(X,Y) = I(LX,LY)$$ It goes through all of this and explains it but then it gives me the following relation with no support... $$III - tr(L)II + \det (L)I = 0$$ Can someone show me how we would go about verifying it?","The book I am reading briefly mentions this relation between the fundamental forms but gives no explanation of how they got it. Take the following as the Weingarten Map/Shape Operator where $\nu$ is the gauss map and f is a surface element. $$L =  - D\nu  \circ {(Df)^{ - 1}}$$ Then it defines the fundamental forms as... $$I(X,Y) = \left\langle {X,Y} \right\rangle $$ $$II(X,Y) = I(LX,Y)$$ $$III(X,Y) = I(LX,LY)$$ It goes through all of this and explains it but then it gives me the following relation with no support... $$III - tr(L)II + \det (L)I = 0$$ Can someone show me how we would go about verifying it?",,['differential-geometry']
10,Second Chern class of $TS^2$,Second Chern class of,TS^2,"The induced metric on a sphere may be given by, $$ds^2 = d\theta^2 + \sin^2\theta\, d\phi^2$$ By using Cartan's method of moving frames , one can compute the curvature 2-form in an orthonormal basis ($e^\theta = d\theta$ and $e^\phi = \sin\theta \, d\phi$), that is,$^\dagger$ $$\Omega = \left(  \begin{array}{cc} 0 & \sin\theta \\ -\sin\theta & 0 \end{array} \right) (d\theta \wedge d\phi)$$ Now, the second Chern class of the tangent bundle is given by, $$c_2 = \frac{1}{8\pi^2} (\mathrm{Tr} \Omega^2 - \mathrm{Tr}^2 \Omega) = -\frac{\sin^2 \theta}{4\pi^2} (d\theta \wedge d\phi)^2$$ What disturbs me is the factor of $(d\theta \wedge d\phi)^2$ which arises from the matrix multiplication to obtain $\Omega^2$. How does it make sense to integrate, $$\int_{S^2}c_2 \sim \int_{S^2} \sin^2 \theta \, (d\theta \wedge d\phi)^2$$ Are we suppose to read this as $(\dots)^2 = (\dots) \wedge (\dots)$? Also, isn't $c_2$ a $4$-form then? I feel this may be a silly question, and I'm just misinterpreting something. $\dagger$ From Cartan's second equation, $\Omega^a_b = d\omega^a_b + \omega^a_c \wedge \omega^c_b$, with $\omega^a_b$ the spin connection. I double checked the curvature form to make sure by plugging it into $\int_{S^2} \mathrm{Pf}[\Omega]$ to get the right Euler characteristic for a sphere.","The induced metric on a sphere may be given by, $$ds^2 = d\theta^2 + \sin^2\theta\, d\phi^2$$ By using Cartan's method of moving frames , one can compute the curvature 2-form in an orthonormal basis ($e^\theta = d\theta$ and $e^\phi = \sin\theta \, d\phi$), that is,$^\dagger$ $$\Omega = \left(  \begin{array}{cc} 0 & \sin\theta \\ -\sin\theta & 0 \end{array} \right) (d\theta \wedge d\phi)$$ Now, the second Chern class of the tangent bundle is given by, $$c_2 = \frac{1}{8\pi^2} (\mathrm{Tr} \Omega^2 - \mathrm{Tr}^2 \Omega) = -\frac{\sin^2 \theta}{4\pi^2} (d\theta \wedge d\phi)^2$$ What disturbs me is the factor of $(d\theta \wedge d\phi)^2$ which arises from the matrix multiplication to obtain $\Omega^2$. How does it make sense to integrate, $$\int_{S^2}c_2 \sim \int_{S^2} \sin^2 \theta \, (d\theta \wedge d\phi)^2$$ Are we suppose to read this as $(\dots)^2 = (\dots) \wedge (\dots)$? Also, isn't $c_2$ a $4$-form then? I feel this may be a silly question, and I'm just misinterpreting something. $\dagger$ From Cartan's second equation, $\Omega^a_b = d\omega^a_b + \omega^a_c \wedge \omega^c_b$, with $\omega^a_b$ the spin connection. I double checked the curvature form to make sure by plugging it into $\int_{S^2} \mathrm{Pf}[\Omega]$ to get the right Euler characteristic for a sphere.",,"['algebraic-geometry', 'differential-geometry', 'differential-forms', 'characteristic-classes']"
11,Why are derivations useful for defining tangent vectors?,Why are derivations useful for defining tangent vectors?,,"On page 54 in his book Introduction to Smooth Manifolds , John Lee says the following: A linear map $v: C^\infty (M) \rightarrow \Bbb{R}$ is called a derivation at p if it satisfies \begin{equation} v(fg) = f(p)vg + g(p)vf  \end{equation} for all $f,g \in C^\infty (M)$. My Question: Why is this useful for defining tangent vectors? This just looks to me like a fancy way of characterizing a set of smooth, linear maps who also happen to satisfy the product rule. What relevance does that have to tangent vectors?","On page 54 in his book Introduction to Smooth Manifolds , John Lee says the following: A linear map $v: C^\infty (M) \rightarrow \Bbb{R}$ is called a derivation at p if it satisfies \begin{equation} v(fg) = f(p)vg + g(p)vf  \end{equation} for all $f,g \in C^\infty (M)$. My Question: Why is this useful for defining tangent vectors? This just looks to me like a fancy way of characterizing a set of smooth, linear maps who also happen to satisfy the product rule. What relevance does that have to tangent vectors?",,['differential-geometry']
12,Manifold Orientability Definition,Manifold Orientability Definition,,"In Shigeyuki Morita's Geometry of Differential Forms , orientability is defined in the following way: If we can assign an orientation to each point on a manifold $M$ in such a way that the orientations as any two sufficiently near points on $M$ are coherent, we say that $M$ is orientable . (Page 48) However, leading up to this definition, it is never explicitly defined what it means that two points have coherent orientations. The only explanation is in the discussion about surfaces: When an orientation is specified at a point, the ""same"" orientation is specified at an arbitrary point in a neighborhood of the point. This is called the coherent orientation . We specify an orientation at a point on a surface, and choose the coherent orientation at each point on a curve starting at the point. If the curve goes back to the starting point, the original orientation may or may not coincide with the orientation propagated along the curve. Now a surface is orientable if the orientation propagated along any curve always comes back to the starting orientation. In this case we can assign an orientation to all points on the surface in such a way that near points have mutually coherent orientations. (Page 46) Page 47 then involves defining orientations at a point $p\in M$ by choosing a basis on the tangent space $T_{p}M$ along with the standard ""right-hand orientation"" of $\mathbb{R}^{3}$ example, but never is it said what it means for orientations to be the ""same"" or coherent between points, or how the orientation of one point specifies one on points in a neighborhood of that point. The closest thing I could think of would be that the existence of an atlas so that the Jacobians of the transition maps between two local charts is positive, as is the definition in do Carmo's Differential Forms and Applications , Page 50. However, the equivalence of these statements is given as a proposition in Morita's text right after the definition of orientability without proof, which does not allow for me to see how the definition is used. Any help with providing an explicit definition of orientations between two points being coherent would be greatly appreciated. Maybe I'm just missing something obvious, and if this is the case, I would very much like to be shown where it is.","In Shigeyuki Morita's Geometry of Differential Forms , orientability is defined in the following way: If we can assign an orientation to each point on a manifold $M$ in such a way that the orientations as any two sufficiently near points on $M$ are coherent, we say that $M$ is orientable . (Page 48) However, leading up to this definition, it is never explicitly defined what it means that two points have coherent orientations. The only explanation is in the discussion about surfaces: When an orientation is specified at a point, the ""same"" orientation is specified at an arbitrary point in a neighborhood of the point. This is called the coherent orientation . We specify an orientation at a point on a surface, and choose the coherent orientation at each point on a curve starting at the point. If the curve goes back to the starting point, the original orientation may or may not coincide with the orientation propagated along the curve. Now a surface is orientable if the orientation propagated along any curve always comes back to the starting orientation. In this case we can assign an orientation to all points on the surface in such a way that near points have mutually coherent orientations. (Page 46) Page 47 then involves defining orientations at a point $p\in M$ by choosing a basis on the tangent space $T_{p}M$ along with the standard ""right-hand orientation"" of $\mathbb{R}^{3}$ example, but never is it said what it means for orientations to be the ""same"" or coherent between points, or how the orientation of one point specifies one on points in a neighborhood of that point. The closest thing I could think of would be that the existence of an atlas so that the Jacobians of the transition maps between two local charts is positive, as is the definition in do Carmo's Differential Forms and Applications , Page 50. However, the equivalence of these statements is given as a proposition in Morita's text right after the definition of orientability without proof, which does not allow for me to see how the definition is used. Any help with providing an explicit definition of orientations between two points being coherent would be greatly appreciated. Maybe I'm just missing something obvious, and if this is the case, I would very much like to be shown where it is.",,"['differential-geometry', 'differential-topology', 'definition']"
13,relation between first fundamental form for different parametrization,relation between first fundamental form for different parametrization,,"The sphere has a parameterization map for a surface patch $\phi(u,v)=(u,v,\sqrt{1-u^2-v^2})$. It has another parametrization map for a surface patch $\beta (x,y)=(\sin x \cos y,\sin x \sin y,\cos x)$. For the first the first fundamental form comes here. $E= \frac{1-v^2}{1-v^2-u^2}$ , $F=\frac{uv}{1-v^2-u^2}$ and $G=\frac{1-u^2}{1-v^2-u^2}$. If we calculate the first fundamental form then second case we have. $ E= 1 , F=0 , G= \sin^2 x$. My question is how can we relate them in the common domain.","The sphere has a parameterization map for a surface patch $\phi(u,v)=(u,v,\sqrt{1-u^2-v^2})$. It has another parametrization map for a surface patch $\beta (x,y)=(\sin x \cos y,\sin x \sin y,\cos x)$. For the first the first fundamental form comes here. $E= \frac{1-v^2}{1-v^2-u^2}$ , $F=\frac{uv}{1-v^2-u^2}$ and $G=\frac{1-u^2}{1-v^2-u^2}$. If we calculate the first fundamental form then second case we have. $ E= 1 , F=0 , G= \sin^2 x$. My question is how can we relate them in the common domain.",,['differential-geometry']
14,Reference request: Introduction to Applied Differential Geometry for Physicists and Engineers,Reference request: Introduction to Applied Differential Geometry for Physicists and Engineers,,"I'm looking for a book on differential geometry or differential topology that is comprehensive and reads at the level of someone with engineering background (i.e. Boyce's ODE, Stewart's Calculus, Axler's Linear algebra). The book should motivate the idea of manifold as it is used in physics and engineering and move up to stuff like vector bundle, wedge products, Poincaré–Hopf theorem and maybe at the very very end some Clifford algebra (helpful with application to electromagnetism or general relativity). The book I've surveyed which includes Janich's Intro to Differential Topology, Isham's Differential Geometry for Physicists, Differential Manifold by Serge Lang, Introduction to Manifolds by Tu L.W. unfortunately all reads like books written by mathematicians for mathematicians and has a dearth of physical examples and visual aids. Tu L.W.'s Intro to Manifold is surprisingly soft handed and perhaps would be good for a first book. The book nonetheless lacks motivating examples and illuminating graphs. Can someone who has taught differential geometry to engineers or physicists or perhaps know a good introductory book on this subject recommend a book that covers about half semester worth of undergrad? Thanks!","I'm looking for a book on differential geometry or differential topology that is comprehensive and reads at the level of someone with engineering background (i.e. Boyce's ODE, Stewart's Calculus, Axler's Linear algebra). The book should motivate the idea of manifold as it is used in physics and engineering and move up to stuff like vector bundle, wedge products, Poincaré–Hopf theorem and maybe at the very very end some Clifford algebra (helpful with application to electromagnetism or general relativity). The book I've surveyed which includes Janich's Intro to Differential Topology, Isham's Differential Geometry for Physicists, Differential Manifold by Serge Lang, Introduction to Manifolds by Tu L.W. unfortunately all reads like books written by mathematicians for mathematicians and has a dearth of physical examples and visual aids. Tu L.W.'s Intro to Manifold is surprisingly soft handed and perhaps would be good for a first book. The book nonetheless lacks motivating examples and illuminating graphs. Can someone who has taught differential geometry to engineers or physicists or perhaps know a good introductory book on this subject recommend a book that covers about half semester worth of undergrad? Thanks!",,"['reference-request', 'differential-geometry']"
15,Canonical connection on $CP^n$,Canonical connection on,CP^n,"I have heard something along the lines of ""There is a canonical $U(1)$ connection on $CP^n$"" and I am trying to understand what that means. First I suppose that the sentence refers to a line bundle over $CP^n$, possibly the tautological line bundle. Second why is there a preferred connection? I was thinking that $CP^n$ can be realised as the homogenous space $\frac{U(n+1)}{U(1)\times U(n)}$. Hence the Maurer-Cartan form on $U(n+1)$ descends to a unique $U(n+1)$-invariant connection form on $CP^n$ which however takes values in $U(1)\times U(n)$. I was thinking that one way to get a $U(1)$ connection would be to follow the prescription described in https://math.stackexchange.com/questions/875705/a-construction-on-principal-bundles . Namely one constructs the bundle $U(n+1) \times_{\rho} U(1)$ where the action $\rho: U(1)\times U(n) \rightarrow U(1)$ is simply projection onto the first factor followed ny $U(1)$ multiplication. Would such a construction work? Is there a simpler way of thinking of the canonical connection on the tautological (?) line bundle over $CP^n$?","I have heard something along the lines of ""There is a canonical $U(1)$ connection on $CP^n$"" and I am trying to understand what that means. First I suppose that the sentence refers to a line bundle over $CP^n$, possibly the tautological line bundle. Second why is there a preferred connection? I was thinking that $CP^n$ can be realised as the homogenous space $\frac{U(n+1)}{U(1)\times U(n)}$. Hence the Maurer-Cartan form on $U(n+1)$ descends to a unique $U(n+1)$-invariant connection form on $CP^n$ which however takes values in $U(1)\times U(n)$. I was thinking that one way to get a $U(1)$ connection would be to follow the prescription described in https://math.stackexchange.com/questions/875705/a-construction-on-principal-bundles . Namely one constructs the bundle $U(n+1) \times_{\rho} U(1)$ where the action $\rho: U(1)\times U(n) \rightarrow U(1)$ is simply projection onto the first factor followed ny $U(1)$ multiplication. Would such a construction work? Is there a simpler way of thinking of the canonical connection on the tautological (?) line bundle over $CP^n$?",,"['differential-geometry', 'fiber-bundles', 'principal-bundles', 'connections', 'homogeneous-spaces']"
16,Prove the existence (or well-definedness) of the induced connection in tensor bundle,Prove the existence (or well-definedness) of the induced connection in tensor bundle,,"Given a connection $\nabla$ on a vector bundle $E$ over a smooth manifold $M$, we know there is a unique extension of $\nabla$ to all tensor bundles of $E$ that satisfies Leibniz rule and contraction. I am going to prove this. We can first define the connection on the dual space of $E$ using the formula forced by the axioms above. Then the rest is essentially just proving one lemma: Given two bundles $E$, $F$ on $M$ and connections on $E$ and $F$, both denoted by $\nabla$, then there is a unique connection $\nabla$ on $E\otimes F$ such that  $$\nabla_X(s_E\otimes s_F)=\nabla_X s_E\otimes s_F+s_E\otimes \nabla_X s_F$$ I tried to invoke the universal property of tensor product as usual, to use bilinearity to prove well-definedness. However this tensor product of sections is not the strict tensor product in linear algebra tense (it takes tensor product pointwise, and it cannot be understood as a tensor product of two $\mathbb R$-vector spaces. It can be tensor product of $C^\infty(M)$ modules though, but $\nabla_X$ is not $C^\infty$ linear). So I get in problem here, and I ask for a conceptual way to show why $\nabla_X$ is a well defined map from the space of sections of $E\otimes F$ to itself.","Given a connection $\nabla$ on a vector bundle $E$ over a smooth manifold $M$, we know there is a unique extension of $\nabla$ to all tensor bundles of $E$ that satisfies Leibniz rule and contraction. I am going to prove this. We can first define the connection on the dual space of $E$ using the formula forced by the axioms above. Then the rest is essentially just proving one lemma: Given two bundles $E$, $F$ on $M$ and connections on $E$ and $F$, both denoted by $\nabla$, then there is a unique connection $\nabla$ on $E\otimes F$ such that  $$\nabla_X(s_E\otimes s_F)=\nabla_X s_E\otimes s_F+s_E\otimes \nabla_X s_F$$ I tried to invoke the universal property of tensor product as usual, to use bilinearity to prove well-definedness. However this tensor product of sections is not the strict tensor product in linear algebra tense (it takes tensor product pointwise, and it cannot be understood as a tensor product of two $\mathbb R$-vector spaces. It can be tensor product of $C^\infty(M)$ modules though, but $\nabla_X$ is not $C^\infty$ linear). So I get in problem here, and I ask for a conceptual way to show why $\nabla_X$ is a well defined map from the space of sections of $E\otimes F$ to itself.",,"['differential-geometry', 'vector-bundles', 'smooth-manifolds', 'connections']"
17,Transverse intersection in a compact manifold,Transverse intersection in a compact manifold,,"Is it true that if $M$ is a compact manifold and $X,Y$ are submanifolds of $M$ which intersect transversely that the intersection $X\cap Y$ consists of finitely many points? I'm trying to understand a proof of the Lefschetz fixed point theorem which, so far as I can tell, makes implicit use of this fact. My immediate impression is that the answer is yes because otherwise what good is compactness here? I'm trying to prove it by arguing that if there were infinitely many points of intersection then there would be an open cover with no finite subcover and I keep getting stuck. Is there a better way to proceed or is the result simply wrong? I don't necessarily need a full proof; a yes or no and if yes a push in the right direction would be more than appreciated already.","Is it true that if $M$ is a compact manifold and $X,Y$ are submanifolds of $M$ which intersect transversely that the intersection $X\cap Y$ consists of finitely many points? I'm trying to understand a proof of the Lefschetz fixed point theorem which, so far as I can tell, makes implicit use of this fact. My immediate impression is that the answer is yes because otherwise what good is compactness here? I'm trying to prove it by arguing that if there were infinitely many points of intersection then there would be an open cover with no finite subcover and I keep getting stuck. Is there a better way to proceed or is the result simply wrong? I don't necessarily need a full proof; a yes or no and if yes a push in the right direction would be more than appreciated already.",,"['differential-geometry', 'differential-topology', 'compactness', 'intersection-theory']"
18,Relation between different definitions of degree in complex geometry,Relation between different definitions of degree in complex geometry,,"Consider a holomoprhic map from a Riemann surface $$ f: \Sigma_g \to \mathbb{CP}^n. $$ This is given by some homogeneous polynomials in some variables. How can we show that the homogeneous degree $d$ of these polynomials coincides with the definition of degree from fundamental homology class, namely $d = f_* ([\Sigma_g]) \in H_2(\mathbb{CP}^n;\mathbb{Z})$?","Consider a holomoprhic map from a Riemann surface $$ f: \Sigma_g \to \mathbb{CP}^n. $$ This is given by some homogeneous polynomials in some variables. How can we show that the homogeneous degree $d$ of these polynomials coincides with the definition of degree from fundamental homology class, namely $d = f_* ([\Sigma_g]) \in H_2(\mathbb{CP}^n;\mathbb{Z})$?",,"['algebraic-geometry', 'differential-geometry', 'algebraic-topology', 'complex-geometry']"
19,(Determinant of) Hessian in local coordinates,(Determinant of) Hessian in local coordinates,,"Let $f\colon M\to \mathbb{R}$ be a smooth function on a manifold $M$ with a critical point $p$. We define its Hessian at $p$ via $H(u, v)=(UVf)(p)$ where $u, v\in T_pM$ and $U$ and $V$ are vector fields with $U_p=u, V_p=v$. I wonder if there is any way of computing the the matrix of Hessian other than using local coordinates.  To make my question more concrete how do you go about computing the Hessian matrix of a real valued function defined on, say, a sphere? My other question is about the determinant of the Hessian. Hessian is a bilinear map and its matrices in different coordinates are congruent (not similar, in general). So, how can one make sense of the determinant of Hessian in a well-defined way? All I can say is that a distinguished inner product should be required on the tangent space at the critical point to make the determinant well defined. Any thoughts on this would be appreciated.","Let $f\colon M\to \mathbb{R}$ be a smooth function on a manifold $M$ with a critical point $p$. We define its Hessian at $p$ via $H(u, v)=(UVf)(p)$ where $u, v\in T_pM$ and $U$ and $V$ are vector fields with $U_p=u, V_p=v$. I wonder if there is any way of computing the the matrix of Hessian other than using local coordinates.  To make my question more concrete how do you go about computing the Hessian matrix of a real valued function defined on, say, a sphere? My other question is about the determinant of the Hessian. Hessian is a bilinear map and its matrices in different coordinates are congruent (not similar, in general). So, how can one make sense of the determinant of Hessian in a well-defined way? All I can say is that a distinguished inner product should be required on the tangent space at the critical point to make the determinant well defined. Any thoughts on this would be appreciated.",,['differential-geometry']
20,Divergence in spherical coordinates problem,Divergence in spherical coordinates problem,,I have this formula for the divergence of a vector field: $$\nabla_m V^m = \frac{1}{\sqrt{|g|}} \frac{\partial (V^m\sqrt{|g|})}{\partial x^m}$$ The metric tensor in spherical coordinates: $$ g=\begin{pmatrix} 1 & 0 & 0\\  0 & r^2\sin^2(\theta) & 0\\  0 & 0 & r^2 \end{pmatrix} $$ $$\sqrt{|g|}=r^2\sin(\theta)$$ So the divergence in spherical coordinates should be: $$\nabla_m V^m =\frac{1}{r^2\sin(\theta)}\frac{\partial}{\partial r}(r^2\sin(\theta)V^r)+\frac{1}{r^2\sin(\theta)}\frac{\partial}{\partial \phi}(r^2\sin(\theta)V^\phi)+\frac{1}{r^2\sin(\theta)}\frac{\partial}{\partial \theta}(r^2\sin(\theta)V^\theta)$$ Some things simplify: $$\nabla_m V^m =\frac{1}{r^2}\frac{\partial}{\partial r}(r^2V^r)+\frac{\partial V^\phi}{\partial \phi}+\frac{1}{\sin(\theta)}\frac{\partial}{\partial \theta}(\sin(\theta)V^\theta)$$ What am I doing wrong??,I have this formula for the divergence of a vector field: $$\nabla_m V^m = \frac{1}{\sqrt{|g|}} \frac{\partial (V^m\sqrt{|g|})}{\partial x^m}$$ The metric tensor in spherical coordinates: $$ g=\begin{pmatrix} 1 & 0 & 0\\  0 & r^2\sin^2(\theta) & 0\\  0 & 0 & r^2 \end{pmatrix} $$ $$\sqrt{|g|}=r^2\sin(\theta)$$ So the divergence in spherical coordinates should be: $$\nabla_m V^m =\frac{1}{r^2\sin(\theta)}\frac{\partial}{\partial r}(r^2\sin(\theta)V^r)+\frac{1}{r^2\sin(\theta)}\frac{\partial}{\partial \phi}(r^2\sin(\theta)V^\phi)+\frac{1}{r^2\sin(\theta)}\frac{\partial}{\partial \theta}(r^2\sin(\theta)V^\theta)$$ Some things simplify: $$\nabla_m V^m =\frac{1}{r^2}\frac{\partial}{\partial r}(r^2V^r)+\frac{\partial V^\phi}{\partial \phi}+\frac{1}{\sin(\theta)}\frac{\partial}{\partial \theta}(\sin(\theta)V^\theta)$$ What am I doing wrong??,,['differential-geometry']
21,A specific example of $F$-related vector fields,A specific example of -related vector fields,F,"I need to prove the following: Let $F:\Bbb{R}\to\Bbb{R}^2$ be the smooth map $F(t)=(\cos t,\sin t)$. Then $d/dt\in\mathcal{T}(\Bbb{R})$ is $F$-related to the vector field $Z\in\mathcal{T}(\Bbb{R}^2)$ defined by $$Z=x\frac{∂}{∂y}-y\frac{∂}{∂x}.$$ So for any smooth $f$ defined on an open subset of $\Bbb{R}^2$ we want $$\frac{d}{dt}(f\circ F)=(x\frac{∂}{∂y}f-y\frac{∂}{∂x}f)\circ F.$$ That is,$$\frac{∂}{∂x}f\frac{d}{dt}F+\frac{∂}{∂y}f\frac{d}{dt}F=(x\frac{∂}{∂y}f-y\frac{∂}{∂x}f)\circ F.$$ What should I do next?","I need to prove the following: Let $F:\Bbb{R}\to\Bbb{R}^2$ be the smooth map $F(t)=(\cos t,\sin t)$. Then $d/dt\in\mathcal{T}(\Bbb{R})$ is $F$-related to the vector field $Z\in\mathcal{T}(\Bbb{R}^2)$ defined by $$Z=x\frac{∂}{∂y}-y\frac{∂}{∂x}.$$ So for any smooth $f$ defined on an open subset of $\Bbb{R}^2$ we want $$\frac{d}{dt}(f\circ F)=(x\frac{∂}{∂y}f-y\frac{∂}{∂x}f)\circ F.$$ That is,$$\frac{∂}{∂x}f\frac{d}{dt}F+\frac{∂}{∂y}f\frac{d}{dt}F=(x\frac{∂}{∂y}f-y\frac{∂}{∂x}f)\circ F.$$ What should I do next?",,"['differential-geometry', 'differential-topology']"
22,Translating French paper into English,Translating French paper into English,,"I am currently studying a French paper on Einstein manifolds by Berard Bergery and I have doubts that my translation of the following sentence is correct: De plus, puisque $G$ agit par isometries, $M/G$ herite par quotient d'une metrique, qui est enfait une metrique riemannienne (""a bord"" eventuellement). My translation: Furthermore, since $G$ acts by isometries, the quotient $M/G$ inherits a metric, which is actually a Riemannian metric (possibly  (..)). Is this correct so far? I don't know how to translate the last two words, apologies for leaving out the accents!","I am currently studying a French paper on Einstein manifolds by Berard Bergery and I have doubts that my translation of the following sentence is correct: De plus, puisque agit par isometries, herite par quotient d'une metrique, qui est enfait une metrique riemannienne (""a bord"" eventuellement). My translation: Furthermore, since acts by isometries, the quotient inherits a metric, which is actually a Riemannian metric (possibly  (..)). Is this correct so far? I don't know how to translate the last two words, apologies for leaving out the accents!",G M/G G M/G,"['differential-geometry', 'manifolds', 'lie-groups', 'translation-request', 'mathematical-french']"
23,O'Neil's problem 2.10 on Tensor Derivations. Literature on the subject.,O'Neil's problem 2.10 on Tensor Derivations. Literature on the subject.,,"I am having real trouble trying to understand a problem in O'Neil's ""Semi-Riemannian Geometry"" and I can't find much literature on the subject. I will expose the problem and I will be grateful to accept hints, solutions and bibliographic sources on tensor derivations. Problem 2.10 (p.53) Prove that a tensor derivation $\mathfrak{D}$ has $\mathfrak{D}_0 ^0 = 0$ if and only if $\mathfrak{D}_0^1$  is $\mathfrak{F}(M)$-    linear (here, $\mathfrak{F}(M)$ stands for all smooth real-valued functions on $M$).  Then by interpretation, $\mathfrak{D}_0 ^1  = B \in \mathfrak{T}_1^1(M)$ (where $\mathfrak{T}_s^r(M)$ stands for all (r,s)-tensors on $M$), and we write $\mathfrak{D} = \mathfrak{D}_B$. Here is the definition of a tensor derivation according to the book: Definition (Tensor derivation). A tensor derivation $\mathfrak{D}$ is a set of $\mathbb{R}$-linear functions on a smooth manifold $M$   $$ \mathfrak{D} = \mathfrak{D}^r_s:\mathfrak{T}_s^r \to \mathfrak{T}_s^r \quad\quad (r\geq 0, s\geq 0) $$   such that for any tensors $A$ and $B$: (1) $\quad \mathfrak{D}(A \otimes  B)= \mathfrak{D}A \otimes B+ A \otimes \mathfrak{D}B $ (2) $\quad \mathfrak{D}(\mathbf{C} A) = \mathbf{C}(\mathfrak{D} A )$ for any contraction $\mathbf{C}$. My first problem is that I cannot see the intuition behind the definition whose notation is already cumbersome and I cannot find additional literature on the subject. Anyway, I believe a solution to the problem is given by the identity (1), for if $\theta$ is a $(1,0)$-tensor and $f$ a smooth real-valued function then $\mathfrak{D}(f)=\mathfrak{D}_0^0(f)=0$ by hypothesis and (1) becomes (hope this makes some sense): $$ \mathfrak{D}(f \theta) = \mathfrak{D}(f) \otimes \theta + f \mathfrak{D}(\theta) = f \mathfrak{D}(\theta)$$ which I hope is enough to prove the necessity and sufficiency of $\mathfrak{F}(M)$-linearity. Yet I cannot derive how it can be interpreted that $\mathfrak{D}_0^1$ is a (1,1)-tensor! Can anyone explain it? Does someone know a more comprehensive and introductory reference? .","I am having real trouble trying to understand a problem in O'Neil's ""Semi-Riemannian Geometry"" and I can't find much literature on the subject. I will expose the problem and I will be grateful to accept hints, solutions and bibliographic sources on tensor derivations. Problem 2.10 (p.53) Prove that a tensor derivation $\mathfrak{D}$ has $\mathfrak{D}_0 ^0 = 0$ if and only if $\mathfrak{D}_0^1$  is $\mathfrak{F}(M)$-    linear (here, $\mathfrak{F}(M)$ stands for all smooth real-valued functions on $M$).  Then by interpretation, $\mathfrak{D}_0 ^1  = B \in \mathfrak{T}_1^1(M)$ (where $\mathfrak{T}_s^r(M)$ stands for all (r,s)-tensors on $M$), and we write $\mathfrak{D} = \mathfrak{D}_B$. Here is the definition of a tensor derivation according to the book: Definition (Tensor derivation). A tensor derivation $\mathfrak{D}$ is a set of $\mathbb{R}$-linear functions on a smooth manifold $M$   $$ \mathfrak{D} = \mathfrak{D}^r_s:\mathfrak{T}_s^r \to \mathfrak{T}_s^r \quad\quad (r\geq 0, s\geq 0) $$   such that for any tensors $A$ and $B$: (1) $\quad \mathfrak{D}(A \otimes  B)= \mathfrak{D}A \otimes B+ A \otimes \mathfrak{D}B $ (2) $\quad \mathfrak{D}(\mathbf{C} A) = \mathbf{C}(\mathfrak{D} A )$ for any contraction $\mathbf{C}$. My first problem is that I cannot see the intuition behind the definition whose notation is already cumbersome and I cannot find additional literature on the subject. Anyway, I believe a solution to the problem is given by the identity (1), for if $\theta$ is a $(1,0)$-tensor and $f$ a smooth real-valued function then $\mathfrak{D}(f)=\mathfrak{D}_0^0(f)=0$ by hypothesis and (1) becomes (hope this makes some sense): $$ \mathfrak{D}(f \theta) = \mathfrak{D}(f) \otimes \theta + f \mathfrak{D}(\theta) = f \mathfrak{D}(\theta)$$ which I hope is enough to prove the necessity and sufficiency of $\mathfrak{F}(M)$-linearity. Yet I cannot derive how it can be interpreted that $\mathfrak{D}_0^1$ is a (1,1)-tensor! Can anyone explain it? Does someone know a more comprehensive and introductory reference? .",,"['differential-geometry', 'tensor-products', 'tensors']"
24,Geodesic of a Surface in $\mathbb{R}^3$,Geodesic of a Surface in,\mathbb{R}^3,"I'm not familiar with geodesics. How can I show that a curve $c$ given by  $c(t)=(t,f(t)\cos{\alpha},f(t)\sin{\alpha})$ for $\alpha$ constant is a geodesic on $M$ where $M=\left\{(x,y,z) \in \Bbb{R}^3 \mid f(x)=y^2+z^2\right\}$?","I'm not familiar with geodesics. How can I show that a curve $c$ given by  $c(t)=(t,f(t)\cos{\alpha},f(t)\sin{\alpha})$ for $\alpha$ constant is a geodesic on $M$ where $M=\left\{(x,y,z) \in \Bbb{R}^3 \mid f(x)=y^2+z^2\right\}$?",,"['differential-geometry', 'surfaces', 'geodesic']"
25,Showing directly that the principal directions are going to be orthogonal,Showing directly that the principal directions are going to be orthogonal,,"So first of all, I know that the Weingarten map (which from now on I shall denote by $L$) is a symmetric linear operator, so there is an orthonormal basis of eigenvalues (Spectral Theorem). I have been trying this concrete example for a while but I am just stuck and I would appreciate an extra pairs of eyes. I will use the result that claims $L=G^{-1}H$ where $G$ is the matrix for the first fundamental form and $H$ is the matrix for the second fundamental form. Our surface is given by $f(u,v)=(u\cos v, u\sin v , v)$. Then: $f_u=(\cos v, \sin v, 0)$, $f_v=(-u\sin v, u \cos v, 1)$, $f_{uu}=(0,0,0)$, $f_{uv}=f_{vu}=(-\sin v, \cos v, 0)$, and $f_{vv}=(-u\cos v, -u \sin v, 0)$. Thus, $g_{11}=f_u\cdot f_u=1$, $g_{12}=g_{21}=f_u\cdot f_v=0$, and $g_{22}=f_v\cdot f_v =u^2+1$. Then, $f_u\times f_v=(\sin v, -\cos v, u)$, so $n=\frac{1}{\sqrt{1+u^2}}(\sin v, -\cos v, u)$. Then, $h_{11}=n\cdot f_{uu}=0$, $h_{12}=h_{21}=n\cdot f_{uv}=\frac{-1}{\sqrt{u^2+1}}$, and $h_{22}=0$. Then as $G^{_1}=Diag[1,\frac{1}{1+u^2}]$, we have that $L=G^{-1}H$, is: $L_{11}=0$, $L_{12}=\frac{-1}{\sqrt{u^2+1}}$, $L_{21}=\frac{-1}{(u^2+1)^{3/2}}$, and $L_{22}=0$ (here is where I start to doubt because I thought I would always wind up with a symmetric matrix). I get that the eigenvalues of this matrix are given by $\det(\lambda I-L)=\lambda^2-\frac{1}{(1+u^2)^2}$, so $k_1=\frac{1}{u^2+1}$, and $k_2=-k_1$. Then to find the first principal direction, we have to find the eigenvector correspoding to $k_1$, which turned out to be $(\frac{-1}{\sqrt{u^2+1}},1)$, and the other principal direction turned out to be $(\frac{1}{\sqrt{u^2+1}},1)$, which are not always orthogonal, so I dont know where I went wrong.","So first of all, I know that the Weingarten map (which from now on I shall denote by $L$) is a symmetric linear operator, so there is an orthonormal basis of eigenvalues (Spectral Theorem). I have been trying this concrete example for a while but I am just stuck and I would appreciate an extra pairs of eyes. I will use the result that claims $L=G^{-1}H$ where $G$ is the matrix for the first fundamental form and $H$ is the matrix for the second fundamental form. Our surface is given by $f(u,v)=(u\cos v, u\sin v , v)$. Then: $f_u=(\cos v, \sin v, 0)$, $f_v=(-u\sin v, u \cos v, 1)$, $f_{uu}=(0,0,0)$, $f_{uv}=f_{vu}=(-\sin v, \cos v, 0)$, and $f_{vv}=(-u\cos v, -u \sin v, 0)$. Thus, $g_{11}=f_u\cdot f_u=1$, $g_{12}=g_{21}=f_u\cdot f_v=0$, and $g_{22}=f_v\cdot f_v =u^2+1$. Then, $f_u\times f_v=(\sin v, -\cos v, u)$, so $n=\frac{1}{\sqrt{1+u^2}}(\sin v, -\cos v, u)$. Then, $h_{11}=n\cdot f_{uu}=0$, $h_{12}=h_{21}=n\cdot f_{uv}=\frac{-1}{\sqrt{u^2+1}}$, and $h_{22}=0$. Then as $G^{_1}=Diag[1,\frac{1}{1+u^2}]$, we have that $L=G^{-1}H$, is: $L_{11}=0$, $L_{12}=\frac{-1}{\sqrt{u^2+1}}$, $L_{21}=\frac{-1}{(u^2+1)^{3/2}}$, and $L_{22}=0$ (here is where I start to doubt because I thought I would always wind up with a symmetric matrix). I get that the eigenvalues of this matrix are given by $\det(\lambda I-L)=\lambda^2-\frac{1}{(1+u^2)^2}$, so $k_1=\frac{1}{u^2+1}$, and $k_2=-k_1$. Then to find the first principal direction, we have to find the eigenvector correspoding to $k_1$, which turned out to be $(\frac{-1}{\sqrt{u^2+1}},1)$, and the other principal direction turned out to be $(\frac{1}{\sqrt{u^2+1}},1)$, which are not always orthogonal, so I dont know where I went wrong.",,['differential-geometry']
26,characterizing semi-Riemannian spaces of constant curvature,characterizing semi-Riemannian spaces of constant curvature,,"How does one characterize $n$-dimensional semi-Riemannian spaces of constant curvature? By ""characterize,"" I mean giving both a definition and some insight into how the possibilities work out in low-dimensional spaces with signature $(1,n-1)$, which are the ones of interest in relativity. Googling is giving me lots of information on the Riemannian case, but not the semi-Riemannian one. I'm also having some trouble interpreting the info I find online for the Riemannian case because a lot of it is written in index-free notation, but I'm only really familiar with index-gymnastics notation. The sources that I'm finding give some criteria, but don't explain why they're valid or whether they're both necessary and sufficient. Is the correct criterion the vanishing of the covariant derivative of the Riemann tensor, $\nabla_a R_{bcde}=0$? Is it sufficient for the covariant derivative of the Ricci tensor to vanish, $\nabla_a R_{bc}=0$? Why? Are these conditions equivalent to simply counting Killing vectors and getting $n(n+1)/2$ of them? (The WP article on the Riemannian case http://en.wikipedia.org/wiki/Constant_curvature seems to be saying this, but doesn't say why it's valid, or why $n(n+1)/2$ is the magic number.) What is the lowest $n$ for which there are constant-curvature spaces with signature $(1,n-1)$ that are not flat, and what do the possibilities look like for this $n$?","How does one characterize $n$-dimensional semi-Riemannian spaces of constant curvature? By ""characterize,"" I mean giving both a definition and some insight into how the possibilities work out in low-dimensional spaces with signature $(1,n-1)$, which are the ones of interest in relativity. Googling is giving me lots of information on the Riemannian case, but not the semi-Riemannian one. I'm also having some trouble interpreting the info I find online for the Riemannian case because a lot of it is written in index-free notation, but I'm only really familiar with index-gymnastics notation. The sources that I'm finding give some criteria, but don't explain why they're valid or whether they're both necessary and sufficient. Is the correct criterion the vanishing of the covariant derivative of the Riemann tensor, $\nabla_a R_{bcde}=0$? Is it sufficient for the covariant derivative of the Ricci tensor to vanish, $\nabla_a R_{bc}=0$? Why? Are these conditions equivalent to simply counting Killing vectors and getting $n(n+1)/2$ of them? (The WP article on the Riemannian case http://en.wikipedia.org/wiki/Constant_curvature seems to be saying this, but doesn't say why it's valid, or why $n(n+1)/2$ is the magic number.) What is the lowest $n$ for which there are constant-curvature spaces with signature $(1,n-1)$ that are not flat, and what do the possibilities look like for this $n$?",,['differential-geometry']
27,Question about Lee's Introduction to Topological Manifolds,Question about Lee's Introduction to Topological Manifolds,,"From page 2 in Lee's Introduction to topological manifolds: Question 1: What does ""describe parametrically"" exactly mean? Is it a synonym for   ""global coordinate chart""? (that is, an atlas consisting of only one   element, $(M,f)$ where $M$ is the entire manifold and $f$ is a homeomorphism $M \to \mathbb R^n$?) Question 2 : Can you give me an example of a $1$-manifold that does not admit a global coordinate chart? (that is, of a non-orientable curve?) (Is the answer that there cannot be such a manifold since non-orientable means that we can embed a Moebius band in it which we can't do in dimension $1$?) Question 3: Is this definition compatible with this   one ? What's the domain of   the map mentioned in the definition on Wolfram Alpha? Any one   dimensional space? I doubt it since we also want $[0,1]$ to be the   domain sometimes. Is this Wolfram entry incorrect? Thanks for help!","From page 2 in Lee's Introduction to topological manifolds: Question 1: What does ""describe parametrically"" exactly mean? Is it a synonym for   ""global coordinate chart""? (that is, an atlas consisting of only one   element, $(M,f)$ where $M$ is the entire manifold and $f$ is a homeomorphism $M \to \mathbb R^n$?) Question 2 : Can you give me an example of a $1$-manifold that does not admit a global coordinate chart? (that is, of a non-orientable curve?) (Is the answer that there cannot be such a manifold since non-orientable means that we can embed a Moebius band in it which we can't do in dimension $1$?) Question 3: Is this definition compatible with this   one ? What's the domain of   the map mentioned in the definition on Wolfram Alpha? Any one   dimensional space? I doubt it since we also want $[0,1]$ to be the   domain sometimes. Is this Wolfram entry incorrect? Thanks for help!",,"['differential-geometry', 'manifolds']"
28,Existence of Complex Structures on Complex Vector Bundles,Existence of Complex Structures on Complex Vector Bundles,,"Let $E$ be a real vector bundle on a smooth manifold $X$. Let $J : E \to E$ be a vector bundle morphism (i.e. $\pi \circ J = \pi$, where $\pi : E \to X$ is the projection map) with $J^2 = -\textrm{id}$. Then $E$ is a complex vector bundle ($E_x$ has a complex structure given by $(a + bi)v = av + bJ_x(v)$). I will call $J$ an almost complex structure on $E$ - for $E = TX$, this is standard. Is there any relationship between an almost complex structure on a vector bundle $E$ and the existence of a complex structure on $E$ as a manifold? That is, is there anything we can say about $J$ that will help us to determine whether $E$ is a complex manifold? Another way of viewing this question is the following: If you have a smooth manifold, which happens to be a complex vector bundle over some other manifold, is it any easier to determine whether or not it has a complex structure?","Let $E$ be a real vector bundle on a smooth manifold $X$. Let $J : E \to E$ be a vector bundle morphism (i.e. $\pi \circ J = \pi$, where $\pi : E \to X$ is the projection map) with $J^2 = -\textrm{id}$. Then $E$ is a complex vector bundle ($E_x$ has a complex structure given by $(a + bi)v = av + bJ_x(v)$). I will call $J$ an almost complex structure on $E$ - for $E = TX$, this is standard. Is there any relationship between an almost complex structure on a vector bundle $E$ and the existence of a complex structure on $E$ as a manifold? That is, is there anything we can say about $J$ that will help us to determine whether $E$ is a complex manifold? Another way of viewing this question is the following: If you have a smooth manifold, which happens to be a complex vector bundle over some other manifold, is it any easier to determine whether or not it has a complex structure?",,"['differential-geometry', 'complex-geometry', 'vector-bundles', 'almost-complex']"
29,Stokes theorem for Lorentz manifolds,Stokes theorem for Lorentz manifolds,,"Reading Tao's book: Nonlinear Dispersive Equations I came upon an identity (the energy flux identity for the wave equation, page 90) for which the proof uses the Stokes theorem. In this case he uses the Stokes theorem on a truncated backward lightcone: $\{(t,x):0\leq t \leq t_1, |x| \leq T_* -t\}$ The problem here is that, when integrating along the boundary of the cone; the curved part (i.e., the mantle): $\{(t,x) : 0 < t < t_1, |x| = T_* -t\}$ is a null hypersurface with respect to the usual Minkowski metric, which he uses. When you restrict the Minkowski metric to a null hypersurface you get a degenerate metric. He explains that we can fix this apparent burden in a footnote: Strictly speaking, $\Sigma_1$ [which is this boundary] is not quite   spacelike, whic causes dS [the induced area form] to degenrate to zero   and $n_\beta$ [the normal] to elongate to infinity. But the area from   $n_\beta dS$ remains well defined in the limit; we omit the standard   details. I am aware that Stokes theorem is oblivious of the metric (that is, it works on any manifold, with or without metric). Furthermore, in this particular case the volume form in the lightcone coincides with the volume form given by the usual Euclidean metric, which we can use, and thus interpret the integral as an integral in Euclidean space; and then the induced metric is perfectly valid and I obtain the identity verbatim. It, however, worries me that this procedure probably doesn't extend to general Lorentz manifolds. Also, in the explanation given in the footnote he describes what seems to be another way to fix the problem, which apparently is standard but for which I haven't been able to find any reference. I guess I could integrate in a stretched out ""lightcone"" with a non-lightlike boundary and take limits as the boundary becomes lightlike; and this will probably give me the same answer, it however doesn't seem to be what he's describing (right?). Is there anything where I could learn how to do this more generally?","Reading Tao's book: Nonlinear Dispersive Equations I came upon an identity (the energy flux identity for the wave equation, page 90) for which the proof uses the Stokes theorem. In this case he uses the Stokes theorem on a truncated backward lightcone: $\{(t,x):0\leq t \leq t_1, |x| \leq T_* -t\}$ The problem here is that, when integrating along the boundary of the cone; the curved part (i.e., the mantle): $\{(t,x) : 0 < t < t_1, |x| = T_* -t\}$ is a null hypersurface with respect to the usual Minkowski metric, which he uses. When you restrict the Minkowski metric to a null hypersurface you get a degenerate metric. He explains that we can fix this apparent burden in a footnote: Strictly speaking, $\Sigma_1$ [which is this boundary] is not quite   spacelike, whic causes dS [the induced area form] to degenrate to zero   and $n_\beta$ [the normal] to elongate to infinity. But the area from   $n_\beta dS$ remains well defined in the limit; we omit the standard   details. I am aware that Stokes theorem is oblivious of the metric (that is, it works on any manifold, with or without metric). Furthermore, in this particular case the volume form in the lightcone coincides with the volume form given by the usual Euclidean metric, which we can use, and thus interpret the integral as an integral in Euclidean space; and then the induced metric is perfectly valid and I obtain the identity verbatim. It, however, worries me that this procedure probably doesn't extend to general Lorentz manifolds. Also, in the explanation given in the footnote he describes what seems to be another way to fix the problem, which apparently is standard but for which I haven't been able to find any reference. I guess I could integrate in a stretched out ""lightcone"" with a non-lightlike boundary and take limits as the boundary becomes lightlike; and this will probably give me the same answer, it however doesn't seem to be what he's describing (right?). Is there anything where I could learn how to do this more generally?",,['differential-geometry']
30,Adjoint action smooth (in Tapp's book)?,Adjoint action smooth (in Tapp's book)?,,"I'm reading ""Matrix Groups for Undergraduates"" by Tapp with a student.  A ""matrix group"" means a subgroup $G$ of $GL_n(\mathbb R)$ which is (relatively) closed-- so if $(A_n)\subseteq G$ with $A_n\rightarrow A$, and $A$ is invertible, then $A\in G$. In the book, Manifolds are treated in an adhoc way.  Given $X\subseteq\mathbb R^m$ a map $f:X\rightarrow \mathbb R^n$ is ""smooth"" if for each $x\in X$ there is an open set $U\subseteq\mathbb R^m$ containing $x$, and a smooth map $g:U\rightarrow\mathbb R^m$ which agrees with $f$ on $U\cap X$. Then a manifold is defined in the obvious way. We then have the adjoint action of a lie group $G$ on its lie algebra $\mathfrak g$.  If $\mathfrak g$ is $d$ dimensional, then by taking a basis $A_1,\cdots,A_d$ of $\mathfrak g$, we can regard the adjoint action as a homomorphism $Ad:G\rightarrow GL_d(\mathbb R)$.  So for each $g\in G$ there is a matrix $(X_{ij}(g))$ with $$ g A_j g^{-1} =\sum_i X_{ij}(g) A_i. $$ How do we show that $Ad$ is smooth? If we follow the definition from the book then we'd need to show that $G\rightarrow \mathbb R^{d\times d}; g \mapsto (X_{ij}(g))$ is smooth.  So for each $g\in G$ I need an open set $U$ in $GL_n(\mathbb R)\subseteq\mathbb R^{n\times n}$ and a smooth function $f:U\rightarrow\mathbb R^{d\times d}$ such that $f(g) = (X_{ij}(g))$ for $g\in G\cap U$.  This seems intractable...? (If one has more Manifold theory, then this becomes sort of obvious, as $Ad$ is just the derivative of the conjugation action, which is smooth, and the derivative of a smooth map is smooth.  But I want to stick to what the book has told us...) Edit: Maybe I can actually argue as follows.  The map $(A,B)\mapsto \operatorname{Tr}(AB)$ is an inner-product on $\mathbb M_n(\mathbb R)$; so I can find $B_1,\cdots,B_n\in\mathbb M_n(\mathbb R)$ with $\operatorname{Tr}(A_iB_j)=\delta_{i,j}$.  Thus $$ X_{ij}(g) = \operatorname{Tr}(g A_j g^{-1} B_i). $$ Thus I could take as my map $$ f(h) = \big( \operatorname{Tr}(h A_j h^{-1} B_i) \big)_{i,j}. $$ This is now a composition of matrix inverse and multiplication, and trace, all smooth maps, hence $f$ is smooth. Does this seem reasonable?  Is this the easiest approach?","I'm reading ""Matrix Groups for Undergraduates"" by Tapp with a student.  A ""matrix group"" means a subgroup $G$ of $GL_n(\mathbb R)$ which is (relatively) closed-- so if $(A_n)\subseteq G$ with $A_n\rightarrow A$, and $A$ is invertible, then $A\in G$. In the book, Manifolds are treated in an adhoc way.  Given $X\subseteq\mathbb R^m$ a map $f:X\rightarrow \mathbb R^n$ is ""smooth"" if for each $x\in X$ there is an open set $U\subseteq\mathbb R^m$ containing $x$, and a smooth map $g:U\rightarrow\mathbb R^m$ which agrees with $f$ on $U\cap X$. Then a manifold is defined in the obvious way. We then have the adjoint action of a lie group $G$ on its lie algebra $\mathfrak g$.  If $\mathfrak g$ is $d$ dimensional, then by taking a basis $A_1,\cdots,A_d$ of $\mathfrak g$, we can regard the adjoint action as a homomorphism $Ad:G\rightarrow GL_d(\mathbb R)$.  So for each $g\in G$ there is a matrix $(X_{ij}(g))$ with $$ g A_j g^{-1} =\sum_i X_{ij}(g) A_i. $$ How do we show that $Ad$ is smooth? If we follow the definition from the book then we'd need to show that $G\rightarrow \mathbb R^{d\times d}; g \mapsto (X_{ij}(g))$ is smooth.  So for each $g\in G$ I need an open set $U$ in $GL_n(\mathbb R)\subseteq\mathbb R^{n\times n}$ and a smooth function $f:U\rightarrow\mathbb R^{d\times d}$ such that $f(g) = (X_{ij}(g))$ for $g\in G\cap U$.  This seems intractable...? (If one has more Manifold theory, then this becomes sort of obvious, as $Ad$ is just the derivative of the conjugation action, which is smooth, and the derivative of a smooth map is smooth.  But I want to stick to what the book has told us...) Edit: Maybe I can actually argue as follows.  The map $(A,B)\mapsto \operatorname{Tr}(AB)$ is an inner-product on $\mathbb M_n(\mathbb R)$; so I can find $B_1,\cdots,B_n\in\mathbb M_n(\mathbb R)$ with $\operatorname{Tr}(A_iB_j)=\delta_{i,j}$.  Thus $$ X_{ij}(g) = \operatorname{Tr}(g A_j g^{-1} B_i). $$ Thus I could take as my map $$ f(h) = \big( \operatorname{Tr}(h A_j h^{-1} B_i) \big)_{i,j}. $$ This is now a composition of matrix inverse and multiplication, and trace, all smooth maps, hence $f$ is smooth. Does this seem reasonable?  Is this the easiest approach?",,"['differential-geometry', 'lie-groups']"
31,How to Prove this Map is an Embedding of a Smooth Manifold?,How to Prove this Map is an Embedding of a Smooth Manifold?,,"I have run into a problem in my differential geometry book. Let $M$ be a smooth manifold and $F={C^\infty }(M,\mathbb R)$. Define a mapping $i:M \to {\mathbb R^F}$ by ${i_f}(x) = f(x)$ for $x > \in M,f \in F$, then $i$ is an embedding. ($\mathbb R^F$ has product topology and $i_f$ means the component.) The injectivity and continuity are not hard. How can I prove that the mapping is an embedding?","I have run into a problem in my differential geometry book. Let $M$ be a smooth manifold and $F={C^\infty }(M,\mathbb R)$. Define a mapping $i:M \to {\mathbb R^F}$ by ${i_f}(x) = f(x)$ for $x > \in M,f \in F$, then $i$ is an embedding. ($\mathbb R^F$ has product topology and $i_f$ means the component.) The injectivity and continuity are not hard. How can I prove that the mapping is an embedding?",,"['differential-geometry', 'manifolds', 'smooth-manifolds']"
32,Image of smooth vector bundle morphism,Image of smooth vector bundle morphism,,"Let $\pi_1:V_1 \rightarrow B_1$ and $\pi_2:V_2 \rightarrow B_2$ be smooth vector bundles and write $\phi_V: V_1 \rightarrow V_2$ as well as $\phi_B:B_1 \rightarrow B_2$ respectively for the total and base part of a smooth vector bundle morphism. The Question is, what additional properties must we assume on $\phi_V$ / $\phi_B$ such that: 1.) The image is a smooth vector bundle? 2.) The image is a smooth sub(vector)bundle? 3.) The preimage $\phi_V^{-1}(W) \rightarrow \phi_B^{-1}(A)$ of a smooth sub(vector)bundle $W \subset V_2 \rightarrow A \subset B_2$ is a smooth sub(vector)bundle of $\pi_1$?","Let $\pi_1:V_1 \rightarrow B_1$ and $\pi_2:V_2 \rightarrow B_2$ be smooth vector bundles and write $\phi_V: V_1 \rightarrow V_2$ as well as $\phi_B:B_1 \rightarrow B_2$ respectively for the total and base part of a smooth vector bundle morphism. The Question is, what additional properties must we assume on $\phi_V$ / $\phi_B$ such that: 1.) The image is a smooth vector bundle? 2.) The image is a smooth sub(vector)bundle? 3.) The preimage $\phi_V^{-1}(W) \rightarrow \phi_B^{-1}(A)$ of a smooth sub(vector)bundle $W \subset V_2 \rightarrow A \subset B_2$ is a smooth sub(vector)bundle of $\pi_1$?",,"['differential-geometry', 'differential-topology', 'vector-bundles']"
33,Gauge transformations in differential forms,Gauge transformations in differential forms,,"I am aware of gauge transformations and covariant derivatives as understood in Quantum Field Theory and I am also familiar with deRham derivative for vector valued differential forms. I thinking of the gauge field A of the gauge group G as a Lie(G) valued 1-form on the manifold. But I can't see why under a gauge transformation on A by an element $g\in G$ amounts to the following change, $A \mapsto A^g = gAg^{-1} -dgg^{-1}$ (if G is thought of as a matrix Lie Group) or in general $A_g = Ad(g)A + g^* \omega$  (where $\omega$ is the left invariant Maurer-Cartan form on G and I guess $g^*$ is pull-back of $\omega$ along left translation map by $g$). Curvature is defined as $F = dA + \frac{1}{2}[A,A]$ and using this one wants to now see why does $F \mapsto F_g = gFg^{-1}$. Firstly is the expression for $A_g$ a definition or is there a derivation for that? When I try proving this (assuming matrix Lie groups) I am getting stuck in multiple places like what is $dA_g$ ? I would be happy if someone can explain the explicit calculations and/or give a reference where such things are explained. Usual books which explain differential forms or connections on principal bundles don't seem to help with such calculations.","I am aware of gauge transformations and covariant derivatives as understood in Quantum Field Theory and I am also familiar with deRham derivative for vector valued differential forms. I thinking of the gauge field A of the gauge group G as a Lie(G) valued 1-form on the manifold. But I can't see why under a gauge transformation on A by an element $g\in G$ amounts to the following change, $A \mapsto A^g = gAg^{-1} -dgg^{-1}$ (if G is thought of as a matrix Lie Group) or in general $A_g = Ad(g)A + g^* \omega$  (where $\omega$ is the left invariant Maurer-Cartan form on G and I guess $g^*$ is pull-back of $\omega$ along left translation map by $g$). Curvature is defined as $F = dA + \frac{1}{2}[A,A]$ and using this one wants to now see why does $F \mapsto F_g = gFg^{-1}$. Firstly is the expression for $A_g$ a definition or is there a derivation for that? When I try proving this (assuming matrix Lie groups) I am getting stuck in multiple places like what is $dA_g$ ? I would be happy if someone can explain the explicit calculations and/or give a reference where such things are explained. Usual books which explain differential forms or connections on principal bundles don't seem to help with such calculations.",,['differential-geometry']
34,"Uncertain about the statement ""equivalent condition for an isometry of Riemannian manifolds"" in Lee's Intro to Riemannian Manifolds","Uncertain about the statement ""equivalent condition for an isometry of Riemannian manifolds"" in Lee's Intro to Riemannian Manifolds",,"In Professor Lee's Introduction to Riemannian Manifolds, second edition on page 12, the first paragraph on Isometries reads Suppose $(M,g)$ and $(\tilde{M},\tilde{g})$ are Riemannian manifolds with or without boundary. An isometry from $(M,g)$ to $(\tilde{M},\tilde{g})$ is a diffeomorphism $\phi\colon M\to\tilde{M}$ such that $\phi^*\tilde{g}=g$ . Unwinding the definitions shows that this is equivalent to the requirement that $\phi$ be a smooth bijection and each $d\phi_p\colon T_pM\to T_{\phi(p)}\tilde{M}$ be a linear isometry. Only one part of a proof eludes me. That is, given a smooth bijection $\phi$ such that each $d\phi_p\colon T_pM\to T_{\phi(p)}\tilde{M}$ is a linear isometry, I haven't been able to prove that $\phi^{-1}$ is smooth if $M$ has nonempty boundary. When $M$ has no boundary, I can use his Introduction to Smooth Manifolds, second edition (ISM) Proposition 4.8(a) and ISM Exercise 4.9 to get that $\phi$ is a local diffeomorphism and ISM Proposition 4.6(f) to get that $\phi$ is a diffeomorphism. ISM Proposition 4.8(a) relies on the Inverse Function Theorem for Manifolds (ISM Theorem 4.5) which Professor Lee warns us ""can fail for a map whose domain has nonempty boundary."" Is a map with invertible differential that maps boundary to boundary a local diffeomorphism? might be part of an answer if I could prove that $\phi$ mapped the boundary of $M$ into the boundary of $\tilde{M}$ . (I can already prove that it maps the interior of $M$ into the interior of $\tilde{M}$ using ISM Problem 4-2.) Of course, such a requirement is necessary for $\phi$ to be a diffeomorphism by the Diffeomorphism Invariance of the Boundary Theorem (ISM Theorem 2.18), but I haven't been able to prove that $\phi$ maps boundary to boundary either. So, how can $\phi^{-1}$ be proven to be smooth, or is the statement in the book incorrect?","In Professor Lee's Introduction to Riemannian Manifolds, second edition on page 12, the first paragraph on Isometries reads Suppose and are Riemannian manifolds with or without boundary. An isometry from to is a diffeomorphism such that . Unwinding the definitions shows that this is equivalent to the requirement that be a smooth bijection and each be a linear isometry. Only one part of a proof eludes me. That is, given a smooth bijection such that each is a linear isometry, I haven't been able to prove that is smooth if has nonempty boundary. When has no boundary, I can use his Introduction to Smooth Manifolds, second edition (ISM) Proposition 4.8(a) and ISM Exercise 4.9 to get that is a local diffeomorphism and ISM Proposition 4.6(f) to get that is a diffeomorphism. ISM Proposition 4.8(a) relies on the Inverse Function Theorem for Manifolds (ISM Theorem 4.5) which Professor Lee warns us ""can fail for a map whose domain has nonempty boundary."" Is a map with invertible differential that maps boundary to boundary a local diffeomorphism? might be part of an answer if I could prove that mapped the boundary of into the boundary of . (I can already prove that it maps the interior of into the interior of using ISM Problem 4-2.) Of course, such a requirement is necessary for to be a diffeomorphism by the Diffeomorphism Invariance of the Boundary Theorem (ISM Theorem 2.18), but I haven't been able to prove that maps boundary to boundary either. So, how can be proven to be smooth, or is the statement in the book incorrect?","(M,g) (\tilde{M},\tilde{g}) (M,g) (\tilde{M},\tilde{g}) \phi\colon M\to\tilde{M} \phi^*\tilde{g}=g \phi d\phi_p\colon T_pM\to T_{\phi(p)}\tilde{M} \phi d\phi_p\colon T_pM\to T_{\phi(p)}\tilde{M} \phi^{-1} M M \phi \phi \phi M \tilde{M} M \tilde{M} \phi \phi \phi^{-1}","['differential-geometry', 'riemannian-geometry', 'smooth-manifolds', 'isometry', 'diffeomorphism']"
35,Reconstructing a sheaf from its global sections,Reconstructing a sheaf from its global sections,,Let $\mathcal{F}$ be a sheaf on a smooth manifold $M$ with the property that $\mathcal{F}(U)$ is a $C^{\infty}(U)$ -module for every open subset $U\subseteq M$ . I wonder if/when you can reconstruct $\mathcal{F}$ from its global sections $\mathcal{F}(M)$ . Maybe you can define something like $\tilde{\mathcal{F}}(U):=C^{\infty}(U)\otimes_{C^{\infty}(M)}\mathcal{F}(M)$ ?,Let be a sheaf on a smooth manifold with the property that is a -module for every open subset . I wonder if/when you can reconstruct from its global sections . Maybe you can define something like ?,\mathcal{F} M \mathcal{F}(U) C^{\infty}(U) U\subseteq M \mathcal{F} \mathcal{F}(M) \tilde{\mathcal{F}}(U):=C^{\infty}(U)\otimes_{C^{\infty}(M)}\mathcal{F}(M),"['differential-geometry', 'modules', 'sheaf-theory']"
36,Introductory Holography Resources,Introductory Holography Resources,,"I'm looking for mathematically rigorous introductory resources (as rigorous as an introduction can be) on the subject of Holographic Duality in physics. I require something that adequately covers Anti-de Sitter geometry and the conjectured correspondence with Conformal Field Theory with good physical explanations. I have a background in Differential Geometry at the level of O'Neill's Semi-Riemannian Geometry . My background in Algebra is upto Gorodentsev's Algebra I and II . Lots of diagrams and visualisations to help capture the intuition would be welcome! It would also be great if the resource was comprehensive and covers advanced topics as well. Any help will be greatly appreciated. I do not mind a bunch of different sources as well, so long as they're not all too lengthy.","I'm looking for mathematically rigorous introductory resources (as rigorous as an introduction can be) on the subject of Holographic Duality in physics. I require something that adequately covers Anti-de Sitter geometry and the conjectured correspondence with Conformal Field Theory with good physical explanations. I have a background in Differential Geometry at the level of O'Neill's Semi-Riemannian Geometry . My background in Algebra is upto Gorodentsev's Algebra I and II . Lots of diagrams and visualisations to help capture the intuition would be welcome! It would also be great if the resource was comprehensive and covers advanced topics as well. Any help will be greatly appreciated. I do not mind a bunch of different sources as well, so long as they're not all too lengthy.",,"['differential-geometry', 'reference-request', 'mathematical-physics', 'conformal-geometry', 'conformal-field-theory']"
37,Constructing a contact form for which a given contact vector field is Reeb,Constructing a contact form for which a given contact vector field is Reeb,,"Given a contact manifold $(M, H)$ and a smooth vector field $X$ on $M$ , I'm trying to show that $X$ is the Reeb field of some contact form for $H$ if and only if it's a contact vector field that's nowhere tangent to $H$ . First, the definitions as I understand them: A Reeb field for some contact form $\theta$ is a smooth vector field $T$ such that $\theta(T) = 1$ and $i_T d\theta = 0$ A contact vector field is a smooth vector field $X$ such that its flow preserves the contact structure $H$ . Explicitly, if $\psi$ is the flow of $X$ , then this means $\psi_{t*}Y \in \Gamma(H)$ for any $Y \in \Gamma(H)$ and any $t \in \mathbb{R}$ . Proving that Reeb $\Rightarrow$ contact is a straightforward application of the definitions, but proving contact $\Rightarrow$ Reeb has been more difficult. My attempt: If $X$ is nowhere tangent to $H$ , then for any contact form $\theta$ , $\theta(X)$ is nowhere zero. Thus we can define a contact form $\phi = \frac{1}{\theta(X)}\theta$ which clearly satisfies $\phi(X) = 1$ , and it remains only to show that $i_Xd\phi = 0$ . Applying Cartan's magic formula, we can write $$ \require{cancel} i_X d\phi = \mathcal{L}_X \phi - \cancelto{0}{d(\theta(X))} = \mathcal{L}_X \phi. $$ This Lie derivative vanishes if and only if $\phi$ is invariant under the flow of $X$ . Because $X$ is contact, the pullback of any contact form by $\psi_t$ will still annihilate $H$ for any $t$ , and so $\psi_t^* \phi = f\phi$ for some $f \in C^\infty(M)$ . Thus, we will be done if we can show that $f = 1$ , which will in turn follow if we can show that $\psi_t^*\phi(X) = \phi(X) = 1$ . \begin{align} (\psi_t^* \phi)(X) &= \left(\psi_t^* \left( \frac{1}{\theta(X)} \theta \right)\right)(X) \\\ &= \left( \frac{1}{\theta(X)} \circ \psi_t\right)(\psi_t^* \theta)(X) \\\ &= \frac{\theta(\psi_{t*} X)}{\theta(X) \circ \psi_t} \\\ &= \frac{\theta(X)}{\psi_t^* (\theta(X))}. \end{align} (On the last line, we should be careful to note that the pullback $\psi_t^*$ is acting on the function $\theta(X)$ , not on $\theta$ itself.) Here is where I find myself in a corner. The result will follow if I can show that $\theta(X)$ is constant on every integral curve of $X$ , which seems like it ought to be true, but I don't know how to show it. Should this be obvious? Or should I perhaps modify my definition of $\phi$ ?","Given a contact manifold and a smooth vector field on , I'm trying to show that is the Reeb field of some contact form for if and only if it's a contact vector field that's nowhere tangent to . First, the definitions as I understand them: A Reeb field for some contact form is a smooth vector field such that and A contact vector field is a smooth vector field such that its flow preserves the contact structure . Explicitly, if is the flow of , then this means for any and any . Proving that Reeb contact is a straightforward application of the definitions, but proving contact Reeb has been more difficult. My attempt: If is nowhere tangent to , then for any contact form , is nowhere zero. Thus we can define a contact form which clearly satisfies , and it remains only to show that . Applying Cartan's magic formula, we can write This Lie derivative vanishes if and only if is invariant under the flow of . Because is contact, the pullback of any contact form by will still annihilate for any , and so for some . Thus, we will be done if we can show that , which will in turn follow if we can show that . (On the last line, we should be careful to note that the pullback is acting on the function , not on itself.) Here is where I find myself in a corner. The result will follow if I can show that is constant on every integral curve of , which seems like it ought to be true, but I don't know how to show it. Should this be obvious? Or should I perhaps modify my definition of ?","(M, H) X M X H H \theta T \theta(T) = 1 i_T d\theta = 0 X H \psi X \psi_{t*}Y \in \Gamma(H) Y \in \Gamma(H) t \in \mathbb{R} \Rightarrow \Rightarrow X H \theta \theta(X) \phi = \frac{1}{\theta(X)}\theta \phi(X) = 1 i_Xd\phi = 0 
\require{cancel} i_X d\phi = \mathcal{L}_X \phi - \cancelto{0}{d(\theta(X))} = \mathcal{L}_X \phi.
 \phi X X \psi_t H t \psi_t^* \phi = f\phi f \in C^\infty(M) f = 1 \psi_t^*\phi(X) = \phi(X) = 1 \begin{align}
(\psi_t^* \phi)(X) &= \left(\psi_t^* \left( \frac{1}{\theta(X)} \theta \right)\right)(X) \\\
&= \left( \frac{1}{\theta(X)} \circ \psi_t\right)(\psi_t^* \theta)(X) \\\
&= \frac{\theta(\psi_{t*} X)}{\theta(X) \circ \psi_t} \\\
&= \frac{\theta(X)}{\psi_t^* (\theta(X))}.
\end{align} \psi_t^* \theta(X) \theta \theta(X) X \phi","['differential-geometry', 'contact-geometry']"
38,Riemann surface with transition functions of the form $z \mapsto az+b$,Riemann surface with transition functions of the form,z \mapsto az+b,"Let $\Sigma$ be a closed Riemann surface and let $\{U_\alpha,\phi_\alpha:U_\alpha\rightarrow\mathbb C\}$ be a family of holomorphic coordinate charts. If the transition maps $$\Phi_{\alpha\beta}:\phi_\alpha(U_\alpha\cap U_\beta)\rightarrow\phi_\beta(U_\alpha\cap U_\beta)$$ are of the form $z\mapsto az+b$ where $a,b$ are complex numbers (of course determined by the charts $U_\alpha$ and $U_\beta$ ). Show that $\Sigma$ must be a genus one Riemann surface. I can prove that $\Sigma$ must not be genus zero because a holomorphic developing map $F:\hat\Sigma\rightarrow \mathbb C$ can be well-defined using the data of the coordinate charts, where $\hat\Sigma$ is the universal covering of $\Sigma$ . The surface $\Sigma$ must not be genus zero since there are no nonconstant holomorphic functions on compact Riemann surfaces. I know this problem can be solved by introducing flat connections in differential geometry, and is related to a famous open problem (Chern's conjecture), but I believe there is a more elementary solution using theory of Riemann surfaces. Thanks for any help!","Let be a closed Riemann surface and let be a family of holomorphic coordinate charts. If the transition maps are of the form where are complex numbers (of course determined by the charts and ). Show that must be a genus one Riemann surface. I can prove that must not be genus zero because a holomorphic developing map can be well-defined using the data of the coordinate charts, where is the universal covering of . The surface must not be genus zero since there are no nonconstant holomorphic functions on compact Riemann surfaces. I know this problem can be solved by introducing flat connections in differential geometry, and is related to a famous open problem (Chern's conjecture), but I believe there is a more elementary solution using theory of Riemann surfaces. Thanks for any help!","\Sigma \{U_\alpha,\phi_\alpha:U_\alpha\rightarrow\mathbb C\} \Phi_{\alpha\beta}:\phi_\alpha(U_\alpha\cap U_\beta)\rightarrow\phi_\beta(U_\alpha\cap U_\beta) z\mapsto az+b a,b U_\alpha U_\beta \Sigma \Sigma F:\hat\Sigma\rightarrow \mathbb C \hat\Sigma \Sigma \Sigma","['differential-geometry', 'complex-geometry', 'riemann-surfaces']"
39,Piecewise smooth vector field along a one-parameter family of curves,Piecewise smooth vector field along a one-parameter family of curves,,"Let $(M,g)$ be a Riemannian manifold. According to Lee's book on Riemannian manifolds, a one-parameter family of curves is defined as a continuous map $\Gamma:J\times I\to M$ , where $I,J$ are intervals on the real line. This map got its name since we can thus obtain two collection of curves in $M$ : the main curves $\Gamma_s(t)=\Gamma(s,t)$ defined by holding $s$ constant, and the transverse curves $\Gamma^{(t)}(s)=\Gamma(s,t)$ defined by holding $t$ constant. Now we can introduce the notion of a vector field along such family of curves. A vector field along $\Gamma$ is a continuous map $V:J\times I\to TM$ such that each $(s,t)\in J\times I$ is assigned a vector $V(s,t)\in T_{\Gamma(s,t)}M$ . One example of such is the velocity vector field of a transverse curve, denoted by $$(\partial_s\Gamma)(s_0,t_0)={\Gamma^{(t_0)}}'(s_0).$$ I'm sorry that still another definition needs to be introduced. $\Gamma$ is said to be admissible if: (i) The domain of $\Gamma$ is of the form $J\times[a,b]$ for some open interval $J$ . (ii) There is a partition $(a_0,\ldots,a_k)$ of $[a,b]$ such that $\Gamma$ is smooth on each rectangle $J\times[a_{i-1},a_i]$ . Partitions like this are called admissible. (iii) Every main curve is a piecewise regular curve segment. A curve is said to be regular if it has non-vanishing velocity. Given an admissible family $\Gamma$ , we describe a continuous vector field along $\Gamma$ as piecewise smooth if the restriction of the vector field to each $J\times[a_{i-1},a_i]$ for some admissible partition $(a_0,\ldots,a_k)$ . Lee claims that $\partial_s\Gamma$ is one such vector field, and his argument about continuity of $\partial_s\Gamma$ on the whole $J\times[a,b]$ is confusing me: To see that is continuous on the whole domain $J\times[a,b]$ , note on the one hand that for each $i=1,\ldots,k-1$ , the values of $\partial_s\Gamma$ along the set $J\times\{a_i\}$ depend only on the values of $\Gamma$ on that set, since the derivative is taken only with respect to the $s$ variable; on the other hand, $\partial_s\Gamma$ is continuous (in fact smooth) on each sub-rectangle $J\times[a_{i-1},a_i]$ and $J\times[a_{i},a_{i+1}]$ , so the right-hand and left-hand limits at $t=a_i$ must be equal. Why is Lee concerned about the values of $\partial_s\Gamma$ along $J\times\{a_i\}$ ? Is he doing something like $$\lim_{x\to a}f(x)=f(a)?$$ Thank you so much for your patience. Thank you.","Let be a Riemannian manifold. According to Lee's book on Riemannian manifolds, a one-parameter family of curves is defined as a continuous map , where are intervals on the real line. This map got its name since we can thus obtain two collection of curves in : the main curves defined by holding constant, and the transverse curves defined by holding constant. Now we can introduce the notion of a vector field along such family of curves. A vector field along is a continuous map such that each is assigned a vector . One example of such is the velocity vector field of a transverse curve, denoted by I'm sorry that still another definition needs to be introduced. is said to be admissible if: (i) The domain of is of the form for some open interval . (ii) There is a partition of such that is smooth on each rectangle . Partitions like this are called admissible. (iii) Every main curve is a piecewise regular curve segment. A curve is said to be regular if it has non-vanishing velocity. Given an admissible family , we describe a continuous vector field along as piecewise smooth if the restriction of the vector field to each for some admissible partition . Lee claims that is one such vector field, and his argument about continuity of on the whole is confusing me: To see that is continuous on the whole domain , note on the one hand that for each , the values of along the set depend only on the values of on that set, since the derivative is taken only with respect to the variable; on the other hand, is continuous (in fact smooth) on each sub-rectangle and , so the right-hand and left-hand limits at must be equal. Why is Lee concerned about the values of along ? Is he doing something like Thank you so much for your patience. Thank you.","(M,g) \Gamma:J\times I\to M I,J M \Gamma_s(t)=\Gamma(s,t) s \Gamma^{(t)}(s)=\Gamma(s,t) t \Gamma V:J\times I\to TM (s,t)\in J\times I V(s,t)\in T_{\Gamma(s,t)}M (\partial_s\Gamma)(s_0,t_0)={\Gamma^{(t_0)}}'(s_0). \Gamma \Gamma J\times[a,b] J (a_0,\ldots,a_k) [a,b] \Gamma J\times[a_{i-1},a_i] \Gamma \Gamma J\times[a_{i-1},a_i] (a_0,\ldots,a_k) \partial_s\Gamma \partial_s\Gamma J\times[a,b] J\times[a,b] i=1,\ldots,k-1 \partial_s\Gamma J\times\{a_i\} \Gamma s \partial_s\Gamma J\times[a_{i-1},a_i] J\times[a_{i},a_{i+1}] t=a_i \partial_s\Gamma J\times\{a_i\} \lim_{x\to a}f(x)=f(a)?","['differential-geometry', 'riemannian-geometry']"
40,On $C^1$ convex domain,On  convex domain,C^1,"Let $D$ be a $C^1$ domain of $\mathbb{R}^d$ . Then we know that there exists a $C^1$ function $\rho:\mathbb R^d\rightarrow \mathbb R$ such that $$ D=\{x\in \mathbb R^d, \rho(x)<0\}, \quad \partial D=\{x\in \mathbb R^d, \rho(x)=0\}, $$ and $ x\in \partial D\Longrightarrow d\rho(x)\not=0. $ Assume now that $D$ is convex. Can we choose $\rho$ to be convex with the same properties? I know that the first claim follows by the local definition and use of partition of unity, but I can't manage to prove existence of such convex function.","Let be a domain of . Then we know that there exists a function such that and Assume now that is convex. Can we choose to be convex with the same properties? I know that the first claim follows by the local definition and use of partition of unity, but I can't manage to prove existence of such convex function.","D C^1 \mathbb{R}^d C^1 \rho:\mathbb R^d\rightarrow \mathbb R 
D=\{x\in \mathbb R^d, \rho(x)<0\}, \quad \partial D=\{x\in \mathbb R^d, \rho(x)=0\},
 
x\in \partial D\Longrightarrow d\rho(x)\not=0.
 D \rho","['differential-geometry', 'convex-analysis', 'euclidean-domain']"
41,Riemann and Ricci tensor,Riemann and Ricci tensor,,"Let $(M,g)$ be a smooth manifold and $f:M \rightarrow \mathbb{R}$ a smooth function. Can we get some expression of: $$ \partial_k f g^{jh} R^k_{hij}$$ in terms of the Ricci tensor? (I use the notations $R^a_{cdb} \partial_a = R(\partial_c, \partial_d) \partial_b$ and $R_{bd}= R^c_{bcd}$ ) In a paper, I have seen that $$\partial_k f g^{jh} R^k_{jih}= \partial_j fR^j_i$$ I have the following ansatz: $$\partial_k f g^{jh} R^k_{hij} =  \partial_k f  g^{jh} g^{lk} R_{hijl} =- \partial_k f  g^{jh} g^{lk} R_{ihjl}=-\partial_k f  g^{jh} g^{lk}  g_{hl}R^h_{ihj} \\ =-\partial_k f  g^{jh} \underbrace{g^{lk}  g_{hl}}_{= \delta_{kh}}R_{ij} = \partial_h f  g^{jh} R_{ij} = \partial_h f   R^j_i $$ I pretty sure that the first $=$ is correct but the third $=$ just looks wrong to me, because there are three $h$ s. Can someone help me here? Thanks in advance!","Let be a smooth manifold and a smooth function. Can we get some expression of: in terms of the Ricci tensor? (I use the notations and ) In a paper, I have seen that I have the following ansatz: I pretty sure that the first is correct but the third just looks wrong to me, because there are three s. Can someone help me here? Thanks in advance!","(M,g) f:M \rightarrow \mathbb{R}  \partial_k f g^{jh} R^k_{hij} R^a_{cdb} \partial_a = R(\partial_c, \partial_d) \partial_b R_{bd}= R^c_{bcd} \partial_k f g^{jh} R^k_{jih}= \partial_j fR^j_i \partial_k f g^{jh} R^k_{hij} =  \partial_k f  g^{jh} g^{lk} R_{hijl}
=- \partial_k f  g^{jh} g^{lk} R_{ihjl}=-\partial_k f  g^{jh} g^{lk}  g_{hl}R^h_{ihj} \\
=-\partial_k f  g^{jh} \underbrace{g^{lk}  g_{hl}}_{= \delta_{kh}}R_{ij} = \partial_h f  g^{jh} R_{ij} = \partial_h f   R^j_i
 = = h","['differential-geometry', 'tensors', 'semi-riemannian-geometry']"
42,Decomposition of the Riemann curvature operator in $4$ dimensions,Decomposition of the Riemann curvature operator in  dimensions,4,"Let $(M,g)$ be a $4$ -dimensional Riemannian manifold. The Riemann curvature tensor can be viewed as an operator $\mathcal{R}:\Lambda^2(T^{\star}M)\longrightarrow \Lambda^2(T^{\star}M)$ defined in this way (I'm using Einstein's notation): $$(\mathcal{R}(\omega))_{ij}=\frac{1}{2}R_{klij}\omega_{kl}$$ where $\omega_{ij}$ are the components of the $2$ -form $\omega$ with respect to an orthonormal basis $\{e^i \wedge e^j\}_{i,j=1,..,4}$ and $R_{ijkt}$ are the components of the Riemann curvature tensor. Because of the splitting $\Lambda^2 (T^{\star}M)=\Lambda_{+}\oplus \Lambda_{-}$ , the operator $\mathcal{R}$ can be written in a block form $$\begin{pmatrix} A & B \\ ^{t}B & C \end{pmatrix}$$ where $^{t}A=A$ , $^{t}C=C$ and $trA=trC=\frac{S}{4}$ , with $S$ the scalar curvature of $M$ . So basically, if we write a $2$ -form $\omega=\omega_{+} + \omega_{-}$ according to the splitting, we have that we can also write $\mathcal{R}\omega=(\mathcal{R}\omega)_{+}+(\mathcal{R}\omega)_{-}$ : in particular, $\mathcal{R}(\omega_{\pm})=\mathcal{R}(\omega_{\pm})_{+}+\mathcal{R}(\omega_{\pm})_{-}$ and, for example, $A$ ""sends"" $\omega_+$ in $\mathcal{R}(\omega_{+})_{+}$ . Now, I read in a few papers that, if we call $A_{ij}$ the components of the matrix $A$ , with $i,j=1,2,3$ , we have that $A_{12}=A_{13}=0$ if and only if $A$ is a multiple of the identity matrix. How is that possible? I tried to work directly with the components of the Weyl tensor $W$ of the manifold, since its self-dual part is strictly related to the matrix $A$ , but I couldn't get anything. I tried also to exploit the fact that a matrix is a multiple of the identity matrix if and only if it commutes with any other matrix, but I don't know if it's useful. EDIT: In these days I read also that this fact may be related to the property of the curvature form $\Omega$ , i.e. $R_g ^{\star}\Omega = g^{-1}\Omega g$ , where $R_g$ denotes the right multiplication by $g\in SO(4)$ in the bundle of orthonormal frames of $N$ . How could it help?","Let be a -dimensional Riemannian manifold. The Riemann curvature tensor can be viewed as an operator defined in this way (I'm using Einstein's notation): where are the components of the -form with respect to an orthonormal basis and are the components of the Riemann curvature tensor. Because of the splitting , the operator can be written in a block form where , and , with the scalar curvature of . So basically, if we write a -form according to the splitting, we have that we can also write : in particular, and, for example, ""sends"" in . Now, I read in a few papers that, if we call the components of the matrix , with , we have that if and only if is a multiple of the identity matrix. How is that possible? I tried to work directly with the components of the Weyl tensor of the manifold, since its self-dual part is strictly related to the matrix , but I couldn't get anything. I tried also to exploit the fact that a matrix is a multiple of the identity matrix if and only if it commutes with any other matrix, but I don't know if it's useful. EDIT: In these days I read also that this fact may be related to the property of the curvature form , i.e. , where denotes the right multiplication by in the bundle of orthonormal frames of . How could it help?","(M,g) 4 \mathcal{R}:\Lambda^2(T^{\star}M)\longrightarrow \Lambda^2(T^{\star}M) (\mathcal{R}(\omega))_{ij}=\frac{1}{2}R_{klij}\omega_{kl} \omega_{ij} 2 \omega \{e^i \wedge e^j\}_{i,j=1,..,4} R_{ijkt} \Lambda^2 (T^{\star}M)=\Lambda_{+}\oplus \Lambda_{-} \mathcal{R} \begin{pmatrix} A & B \\ ^{t}B & C \end{pmatrix} ^{t}A=A ^{t}C=C trA=trC=\frac{S}{4} S M 2 \omega=\omega_{+} + \omega_{-} \mathcal{R}\omega=(\mathcal{R}\omega)_{+}+(\mathcal{R}\omega)_{-} \mathcal{R}(\omega_{\pm})=\mathcal{R}(\omega_{\pm})_{+}+\mathcal{R}(\omega_{\pm})_{-} A \omega_+ \mathcal{R}(\omega_{+})_{+} A_{ij} A i,j=1,2,3 A_{12}=A_{13}=0 A W A \Omega R_g ^{\star}\Omega = g^{-1}\Omega g R_g g\in SO(4) N","['differential-geometry', 'riemannian-geometry', 'differential-forms', 'curvature']"
43,4th Stiefel whitney class of a 7-dimensional Spin manifold,4th Stiefel whitney class of a 7-dimensional Spin manifold,,"In Massey's paper ""On the Stiefel Whitney classes of a manifold I"" he shows that manifolds of dimension n = 4s + 3 have $w_n = w_{n-1} = w_{n-2} = 0$ . Where $w_i$ is a mod 2 Stiefel-Whitney class Also, the first non-zero Stiefel-Whitney class must be $w_{2^k}$ for some k. That means that if $M$ is a 7-dimensinal Spin manifold, the only class which has any hope of not being zero is $w_4$ . In general, it doesn't seem like $w_4$ has any ""special"" meaning (like $w_2$ tells about spin and $w_1$ orientability) but maybe in this specific case it does. Is there a condition that tells us when $w_4$ must be nonzero for 7-d Spin manifolds? Here is an idea: Suppose that $w_4 \neq 0$ . The Poincare duality tells us that there is a nonzero class in $H^3(M)$ . Do we know anything about that three class? Maybe it has to be zero, implying $w_4 = 0$ ?","In Massey's paper ""On the Stiefel Whitney classes of a manifold I"" he shows that manifolds of dimension n = 4s + 3 have . Where is a mod 2 Stiefel-Whitney class Also, the first non-zero Stiefel-Whitney class must be for some k. That means that if is a 7-dimensinal Spin manifold, the only class which has any hope of not being zero is . In general, it doesn't seem like has any ""special"" meaning (like tells about spin and orientability) but maybe in this specific case it does. Is there a condition that tells us when must be nonzero for 7-d Spin manifolds? Here is an idea: Suppose that . The Poincare duality tells us that there is a nonzero class in . Do we know anything about that three class? Maybe it has to be zero, implying ?",w_n = w_{n-1} = w_{n-2} = 0 w_i w_{2^k} M w_4 w_4 w_2 w_1 w_4 w_4 \neq 0 H^3(M) w_4 = 0,"['differential-geometry', 'algebraic-topology']"
44,What can I do with Gauss Bonnet Theorem ( Undergraduate Project ),What can I do with Gauss Bonnet Theorem ( Undergraduate Project ),,"I have selected Gauss-Bonnet Theorem as my undergraduate project topic ( suggested by the lecturer ). I have briefly gone through the book by do Carmo until the Gauss-Bonnet Theorem but I still have about 3 months before I had to start to write my report on this topic. Is there any particular suggestion (book, paper) that I can further my study related to this theorem? (For example, its extension or its special case, etc.)","I have selected Gauss-Bonnet Theorem as my undergraduate project topic ( suggested by the lecturer ). I have briefly gone through the book by do Carmo until the Gauss-Bonnet Theorem but I still have about 3 months before I had to start to write my report on this topic. Is there any particular suggestion (book, paper) that I can further my study related to this theorem? (For example, its extension or its special case, etc.)",,"['differential-geometry', 'riemannian-geometry']"
45,Difference between two connections,Difference between two connections,,"I have the following exercise: Let $E \rightarrow M$ be a vector bundle, $\nabla, \tilde{\nabla}$ two connections on $E$ . Show that there exists $A \in \Omega^1(M,End(E)) := \Gamma(T^{*}M \otimes End(E))$ , s.t. $\tilde{\nabla}=\nabla+A$ . So far, I have that $\Gamma(T^{*}M \otimes End(E))\cong Mult_{C^{\infty}(M)}(X(M) \times \Gamma(E) \times \Gamma(E^{*}))$ . Now it's obvius to define $A:= \tilde{\nabla}-\nabla$ and to show that $\tilde{\nabla}-\nabla \in Mult_{C^{\infty}(M)}(X(M) \times \Gamma(E) \times \Gamma(E^{*}))$ . Is that correct? Now I know that $A(X,s) \in Hom_{C^{\infty}(M)}(\Gamma(E^{*}, C^{\infty}(M)) \cong \Gamma(E)$ but I'm not sure how to shows multilinearity here. Can anybody give me a hint?","I have the following exercise: Let be a vector bundle, two connections on . Show that there exists , s.t. . So far, I have that . Now it's obvius to define and to show that . Is that correct? Now I know that but I'm not sure how to shows multilinearity here. Can anybody give me a hint?","E \rightarrow M \nabla, \tilde{\nabla} E A \in \Omega^1(M,End(E)) := \Gamma(T^{*}M \otimes End(E)) \tilde{\nabla}=\nabla+A \Gamma(T^{*}M \otimes End(E))\cong Mult_{C^{\infty}(M)}(X(M) \times \Gamma(E) \times \Gamma(E^{*})) A:= \tilde{\nabla}-\nabla \tilde{\nabla}-\nabla \in Mult_{C^{\infty}(M)}(X(M) \times \Gamma(E) \times \Gamma(E^{*})) A(X,s) \in Hom_{C^{\infty}(M)}(\Gamma(E^{*}, C^{\infty}(M)) \cong \Gamma(E)","['differential-geometry', 'connections']"
46,Complex structures on $TM$ and $T^*M$,Complex structures on  and,TM T^*M,"There we go. I'm asking this question to know the different complex structures can be defined on $TM$ and $T^*M$ (I don't mind because my manifold will be Kähler). I know there are related questions, for example those ones in MO (the former cites the latter ) and also others in M.SE, although I can't find them. However, none of them clarify my doubts. Suppose $M$ is a manifold. It is well known that $T^*M$ is a symplectic manifold in a natural way. It seems that doesn't happen if we are interested in complex structures, i.e., $T^*M$ isn't a (an almost) complex manifold in a natural way. Now, suppose that $M$ is a Riemannian manifold. Róbert Szöke, in his paper Complex structures on tangent bundles of Riemannian manifolds (you should be able to find the paper here ) says that there are a natural complex structure on $TM$ . Namely, that the metric gives rise to a direct sum decomposition of the bundle $T(TM)$ into the vertical and the horizontal subbsendles, so for any $p\in TM$ we have the isomorphism $T_p(TM)\cong T_{\pi(p)}M\oplus T_{\pi(p)}M$ . Question 1. Im' not sure the metric give you that split. I guess that the split is actually given by the Levi-Civita connection associated with the Riemannian metric of $M$ . Am I right? Furthermore,  imagine that $M$ is also complex (in my case it will be Kähler). If $J$ is the complex structure on $M$ , I have checked that $TJ$ (the differential map of $J$ ) is an almost complex structure on $TM$ . I don't know if it will be integrable, it should be though (call it poetic justice). Question 2. I can't find the question here in M.SE, but there, there was a comment saying that if $M$ is complex, then $T^*M$ is too. Is $TJ$ the complex structure that person referred to? Or can we define (in a more or less natural way) other complex structures using $J$ ? In that case, does it matter if I consider $TM$ or $T*M$ , suposseing $M$ is still Riemannian? Let me add some context for my question. I want to reproduce Hitchin's result about the c-Map but defining all the structures globally. Because he points that it is possible, but he doesn't prove the result and works only locally. Here is the paper I say and here you can find another paper studying the c-map globally in the lagange of principal bundles, which I want to avoid.","There we go. I'm asking this question to know the different complex structures can be defined on and (I don't mind because my manifold will be Kähler). I know there are related questions, for example those ones in MO (the former cites the latter ) and also others in M.SE, although I can't find them. However, none of them clarify my doubts. Suppose is a manifold. It is well known that is a symplectic manifold in a natural way. It seems that doesn't happen if we are interested in complex structures, i.e., isn't a (an almost) complex manifold in a natural way. Now, suppose that is a Riemannian manifold. Róbert Szöke, in his paper Complex structures on tangent bundles of Riemannian manifolds (you should be able to find the paper here ) says that there are a natural complex structure on . Namely, that the metric gives rise to a direct sum decomposition of the bundle into the vertical and the horizontal subbsendles, so for any we have the isomorphism . Question 1. Im' not sure the metric give you that split. I guess that the split is actually given by the Levi-Civita connection associated with the Riemannian metric of . Am I right? Furthermore,  imagine that is also complex (in my case it will be Kähler). If is the complex structure on , I have checked that (the differential map of ) is an almost complex structure on . I don't know if it will be integrable, it should be though (call it poetic justice). Question 2. I can't find the question here in M.SE, but there, there was a comment saying that if is complex, then is too. Is the complex structure that person referred to? Or can we define (in a more or less natural way) other complex structures using ? In that case, does it matter if I consider or , suposseing is still Riemannian? Let me add some context for my question. I want to reproduce Hitchin's result about the c-Map but defining all the structures globally. Because he points that it is possible, but he doesn't prove the result and works only locally. Here is the paper I say and here you can find another paper studying the c-map globally in the lagange of principal bundles, which I want to avoid.",TM T^*M M T^*M T^*M M TM T(TM) p\in TM T_p(TM)\cong T_{\pi(p)}M\oplus T_{\pi(p)}M M M J M TJ J TM M T^*M TJ J TM T*M M,"['differential-geometry', 'complex-geometry', 'vector-bundles']"
47,Prove that this class of curves has constant speed and curvature,Prove that this class of curves has constant speed and curvature,,"Let $\gamma: (a,b) \rightarrow \mathbb{R}^2$ be a smooth regular curve such that $\forall s,t \in (a,b)$ , $||\gamma(s)-\gamma(t)||$ is a non-negative real valued function which depends only on $|t-s|$ . Show that $\gamma(t)$ has speed and curvature both constant. Here is my attempt: I somehow have to use the constraint given of the function $||\gamma(t)- \gamma(s)||$ . Since this acts on $(a,b)^2$ , I want to handle a nicer funcion defined as follows: $$f(h):= ||\gamma(s+h)-\gamma(s)||^2$$ for $s\in(a,b)$ fixed. Taking the derivative with respect to $h$ , denoting with $\langle \cdot , \cdot \rangle$ the Euclidean inner product on $\mathbb{R}^2$ $f'(h)= \frac{d}{dh}\langle \gamma(s+h) - \gamma(s) , \gamma(s+h) - \gamma(s) \rangle = \frac{d}{dh} [\langle \gamma(s+h), \gamma(s+h) \rangle + \langle \gamma(s), \gamma(s) \rangle -2 \langle \gamma(s+h), \gamma(s) \rangle ] = 2[\langle \frac{d}{dh}\gamma(s+h), \gamma(s+h) \rangle - \langle \frac{d}{dh}\gamma(s+h), \gamma(s) \rangle] = 2[\langle \frac{d}{dh}\gamma(s+h), \gamma(s+h) - \gamma(s) \rangle ]$ So $f'(0)=0 \quad \forall s$ I think this is somehow useful, but I don't see how to continue from here.","Let be a smooth regular curve such that , is a non-negative real valued function which depends only on . Show that has speed and curvature both constant. Here is my attempt: I somehow have to use the constraint given of the function . Since this acts on , I want to handle a nicer funcion defined as follows: for fixed. Taking the derivative with respect to , denoting with the Euclidean inner product on So I think this is somehow useful, but I don't see how to continue from here.","\gamma: (a,b) \rightarrow \mathbb{R}^2 \forall s,t \in (a,b) ||\gamma(s)-\gamma(t)|| |t-s| \gamma(t) ||\gamma(t)- \gamma(s)|| (a,b)^2 f(h):= ||\gamma(s+h)-\gamma(s)||^2 s\in(a,b) h \langle \cdot , \cdot \rangle \mathbb{R}^2 f'(h)= \frac{d}{dh}\langle \gamma(s+h) - \gamma(s) , \gamma(s+h) - \gamma(s) \rangle = \frac{d}{dh} [\langle \gamma(s+h), \gamma(s+h) \rangle + \langle \gamma(s), \gamma(s) \rangle -2 \langle \gamma(s+h), \gamma(s) \rangle ] = 2[\langle \frac{d}{dh}\gamma(s+h), \gamma(s+h) \rangle - \langle \frac{d}{dh}\gamma(s+h), \gamma(s) \rangle] = 2[\langle \frac{d}{dh}\gamma(s+h), \gamma(s+h) - \gamma(s) \rangle ] f'(0)=0 \quad \forall s","['differential-geometry', 'curves', 'plane-curves']"
48,Pullback of differential forms and determinant,Pullback of differential forms and determinant,,"I'm studying differential geometry using the book ""Godinho Natàrio - An introduction to Riemannian Geometry"". These are the definitions and theorems I'm working with: Definition 1 (Pullback of a linear map) Let $V,W$ be finite dimensional real vector spaces, $F : V → W$ be a linear map. Then for every $k$ positive integer we define the pullback of $F$ as $$ F^* : \mathcal{T}^k(W^*) \to \mathcal{T}^k(V^*) \quad \quad (F^*T)(v_1, \dots, v_k) = T(F(v_1), \dots, F(v_k)) $$ for any $v_1, \dots, v_k \in V$ . Here $\mathcal{T}^k(W^*)$ is the space of $k$ -covariant tensors on $W$ . Theorem 1.13 Let $V$ be a $n$ -dimensional real vector space, $F : V → V$ be a linear map and let $T \in 􏰁\Lambda^n(V^*)$ (the space of $n$ -covariant alternating tensors on $V$ ). Then $F^*T = (\det A)T$ , where $A$ is any matrix representing $F$ . Definition 2 (Pullback of a tensor field) Let $M, N$ be smooth manifolds, $f : M \to N$ be a differentiable map. Then, each differentiable $k$ -covariant tensor field $T$ on $N$ defines a $k$ -covariant tensor field $f^*T$ on $M$ in the following way: $$ (f^*T)_p(v_1,...,v_k) = T_{f(p)}((df)_p(v_1),...,(df)_p(v_k)) $$ for any $ v_1, \dots,v_k \in T_pM$ . Then this last definition applies also to a differential form (being it a $k$ -covariant differentiable alternating tensor field). What I can't understand is the following remark at page 73 : Let $M,N$ be smooth manifolds and let $f: M \to N$ be a differentiable map s.t. $\dim(M)=\dim(N)=n$ . Let $p \in M$ and consider a coordinate systems $x = (x^1, \dots, x^n)$ around $p$ s.t. $x: V \to \mathbb{R}^n$ and $y=(y^1, \dots, y^n)$ around $f(p)$ s.t. $y: W \to \mathbb{R}^n$ . Let $\hat{f}:= y \circ f \circ x^{-1}$ be the local representation of $f$ . Then from Theorem 1.13 : $$(f^*(dy^1 \wedge \dots \wedge dy^n))_p = \det(d \hat{f})_{x(p)}(dx^1\wedge \dots \wedge dx^n)_p$$ How can I apply Theorem 1.13 in this situation? I mean, ""translating"" Definition 2 into Definition 1, I have the pullback of the linear map $dF_p : T_pM \to T_{f(p)}N$ applied to the element $dy^1 \wedge \dots \wedge dy^n$ of $\Lambda^n(T_{f(p)}N^*)$ .  But those vector spaces are not the same (as in the hypothesis of the Theorem)! Edit I think it can be fixed in this way: Let $I_1 : \mathbb{R}^n \to T_pM$ and $I_2 : \mathbb{R}^n \to T_{f(p)}N$ be two isomorphisms s.t.: $$ I_1(e^i) = \frac{\partial}{\partial x^i} \quad \quad I_2(e^i) = \frac{\partial}{\partial y^i} \quad \forall \, i = 1, \dots, n $$ where $ \{ e^1, \dots, e^n \}$ is the standard basis of $\mathbb{R}^n$ . Then $$ F :=  I_2^{-1} \circ df_p \circ I_1 : \mathbb{R}^n \to \mathbb{R}^n$$ is an endomorphism in $\mathbb{R}^n$ . By Theorem 1.13 $F^* = \det(A) \cdot$ being $A$ the matrix representing $F$ . By pullback's properties we have $$\det(A) \cdot = F^* = (I_2^{-1} \circ df_p \circ I_1)^* = I_1^* \circ (df_p^*) \circ (I_2^*)^{-1} \Rightarrow df_p^* = (I_1^*)^{-1} \circ (\det(A) \cdot) \circ I_2^*$$ Then $$ (f^*(dy^1 \wedge \dots \wedge dy^n))_p = df_p^* (dy^1 \wedge \dots \wedge dy^n)=  ((I_1^*)^{-1} \circ (\det(A) \cdot) \circ I_2^*) (dy^1 \wedge \dots \wedge dy^n)= \det(A) (I_2 \circ I_1^{-1})^*(dy^1 \wedge \dots \wedge dy^n) $$ Moreover $$I_2 \circ I_1^{-1} : T_p(M) \to T_{f(p)}N \quad \quad \frac{\partial}{\partial x^i} \mapsto \frac{\partial}{\partial y^i} $$ and then $$(I_2 \circ I_1^{-1})^*(dy^i) \biggl (\frac{\partial}{\partial x^j} \biggr) = dy^i \biggl ( (I_2 \circ I_1^{-1}) \biggl (\frac{\partial}{\partial x^j} \biggr) \biggr ) = dy^i \biggl ( \frac{\partial}{\partial y^j} \biggr ) = \delta_{ij} = dx^i \biggl (\frac{\partial}{\partial x^j} \biggr)  $$ i.e. $$(I_2 \circ I_1^{-1})^*(dy^i) =dx^i $$ and then since $ (I_2 \circ I_1^{-1})^* ( dy^i \wedge dy^j) = ((I_2 \circ I_1^{-1})^*dy^i) \wedge ((I_2 \circ I_1^{-1})^*dy^j)$ we have $$(f^*(dy^1 \wedge \dots \wedge dy^n))_p = \det(A)(dx^1 \wedge \dots dx^n) $$ and it is easy to show that $A = [d\hat{f}]_{ij}$ . Is it ok?","I'm studying differential geometry using the book ""Godinho Natàrio - An introduction to Riemannian Geometry"". These are the definitions and theorems I'm working with: Definition 1 (Pullback of a linear map) Let be finite dimensional real vector spaces, be a linear map. Then for every positive integer we define the pullback of as for any . Here is the space of -covariant tensors on . Theorem 1.13 Let be a -dimensional real vector space, be a linear map and let (the space of -covariant alternating tensors on ). Then , where is any matrix representing . Definition 2 (Pullback of a tensor field) Let be smooth manifolds, be a differentiable map. Then, each differentiable -covariant tensor field on defines a -covariant tensor field on in the following way: for any . Then this last definition applies also to a differential form (being it a -covariant differentiable alternating tensor field). What I can't understand is the following remark at page 73 : Let be smooth manifolds and let be a differentiable map s.t. . Let and consider a coordinate systems around s.t. and around s.t. . Let be the local representation of . Then from Theorem 1.13 : How can I apply Theorem 1.13 in this situation? I mean, ""translating"" Definition 2 into Definition 1, I have the pullback of the linear map applied to the element of .  But those vector spaces are not the same (as in the hypothesis of the Theorem)! Edit I think it can be fixed in this way: Let and be two isomorphisms s.t.: where is the standard basis of . Then is an endomorphism in . By Theorem 1.13 being the matrix representing . By pullback's properties we have Then Moreover and then i.e. and then since we have and it is easy to show that . Is it ok?","V,W F : V → W k F  F^* : \mathcal{T}^k(W^*) \to \mathcal{T}^k(V^*) \quad \quad (F^*T)(v_1, \dots, v_k) = T(F(v_1), \dots, F(v_k))  v_1, \dots, v_k \in V \mathcal{T}^k(W^*) k W V n F : V → V T \in 􏰁\Lambda^n(V^*) n V F^*T = (\det A)T A F M, N f : M \to N k T N k f^*T M  (f^*T)_p(v_1,...,v_k) = T_{f(p)}((df)_p(v_1),...,(df)_p(v_k))   v_1, \dots,v_k \in T_pM k M,N f: M \to N \dim(M)=\dim(N)=n p \in M x = (x^1, \dots, x^n) p x: V \to \mathbb{R}^n y=(y^1, \dots, y^n) f(p) y: W \to \mathbb{R}^n \hat{f}:= y \circ f \circ x^{-1} f (f^*(dy^1 \wedge \dots \wedge dy^n))_p = \det(d \hat{f})_{x(p)}(dx^1\wedge \dots \wedge dx^n)_p dF_p : T_pM \to T_{f(p)}N dy^1 \wedge \dots \wedge dy^n \Lambda^n(T_{f(p)}N^*) I_1 : \mathbb{R}^n \to T_pM I_2 : \mathbb{R}^n \to T_{f(p)}N  I_1(e^i) = \frac{\partial}{\partial x^i} \quad \quad I_2(e^i) = \frac{\partial}{\partial y^i} \quad \forall \, i = 1, \dots, n   \{ e^1, \dots, e^n \} \mathbb{R}^n  F :=  I_2^{-1} \circ df_p \circ I_1 : \mathbb{R}^n \to \mathbb{R}^n \mathbb{R}^n F^* = \det(A) \cdot A F \det(A) \cdot = F^* = (I_2^{-1} \circ df_p \circ I_1)^* = I_1^* \circ (df_p^*) \circ (I_2^*)^{-1} \Rightarrow df_p^* = (I_1^*)^{-1} \circ (\det(A) \cdot) \circ I_2^*  (f^*(dy^1 \wedge \dots \wedge dy^n))_p = df_p^* (dy^1 \wedge \dots \wedge dy^n)=  ((I_1^*)^{-1} \circ (\det(A) \cdot) \circ I_2^*) (dy^1 \wedge \dots \wedge dy^n)= \det(A) (I_2 \circ I_1^{-1})^*(dy^1 \wedge \dots \wedge dy^n)  I_2 \circ I_1^{-1} : T_p(M) \to T_{f(p)}N \quad \quad \frac{\partial}{\partial x^i} \mapsto \frac{\partial}{\partial y^i}  (I_2 \circ I_1^{-1})^*(dy^i) \biggl (\frac{\partial}{\partial x^j} \biggr) = dy^i \biggl ( (I_2 \circ I_1^{-1}) \biggl (\frac{\partial}{\partial x^j} \biggr) \biggr ) = dy^i \biggl ( \frac{\partial}{\partial y^j} \biggr ) = \delta_{ij} = dx^i \biggl (\frac{\partial}{\partial x^j} \biggr)   (I_2 \circ I_1^{-1})^*(dy^i) =dx^i   (I_2 \circ I_1^{-1})^* ( dy^i \wedge dy^j) = ((I_2 \circ I_1^{-1})^*dy^i) \wedge ((I_2 \circ I_1^{-1})^*dy^j) (f^*(dy^1 \wedge \dots \wedge dy^n))_p = \det(A)(dx^1 \wedge \dots dx^n)  A = [d\hat{f}]_{ij}","['differential-geometry', 'differential-forms', 'pullback']"
49,A Hodge dual computation on a $4$-dimensional Riemannian manifold,A Hodge dual computation on a -dimensional Riemannian manifold,4,"Let $(M,g)$ be a $4$-dimensional smooth Riemannian manifold.  I am trying to understand the following exterior algebra computation: Let $x^1,x^2,x^3,x^4$ be local coordinates on $M$ such that the Riemannian volume form of $g$ is $\mathrm{d}x^1\wedge\mathrm{d}x^2\wedge\mathrm{d}x^3\wedge\mathrm{d}x^4$. Then there exist a local frame $\alpha_1,\alpha_2,\alpha_3,\alpha_4$ of $1$-forms such that the following identities hold: $$ \begin{aligned} \ast_g(\mathrm{d}x^1\wedge\mathrm{d}x^2)&= \alpha_3\wedge\alpha_4\\ \ast_g(\mathrm{d}x^1\wedge\mathrm{d}x^3)&= \alpha_4\wedge\alpha_2\\ \ast_g(\mathrm{d}x^1\wedge\mathrm{d}x^4)&= \alpha_2\wedge\alpha_3\\ \end{aligned}\qquad \begin{aligned} \ast_g(\mathrm{d}x^3\wedge\mathrm{d}x^4)&= \alpha_1\wedge\alpha_2\\ \ast_g(\mathrm{d}x^4\wedge\mathrm{d}x^2)&= \alpha_1\wedge\alpha_3\\ \ast_g(\mathrm{d}x^2\wedge\mathrm{d}x^3)&= \alpha_1\wedge\alpha_4\\ \end{aligned} \tag1 $$ (The pattern is that the indices on both sides of every equation are complementary.) In fact, the solution of these equations (which is unique up to replacing each $\alpha_i$ by $-\alpha_i$) is given by  $$ \alpha_i = g_{ij}\,\mathrm{d}x^j\qquad\qquad\text{where}\quad  g = g_{ij}\,\mathrm{d}x^i\mathrm{d}x^j. $$ Question: Why does the formula $\alpha_i = g_{ij}\,\mathrm{d}x^j$ solve the equations? Here is what I understood: $\mathrm{d}x^1,\mathrm{d}x^2$ are $g$-orthogonal to $\alpha_3,\alpha_4$. Since $\ast_g(\mathrm{d}x^1\wedge\mathrm{d}x^2)$ is decomposable*, we can write  $$ \ast_g(\mathrm{d}x^1\wedge\mathrm{d}x^2)=\beta_1 \wedge \beta_2$$  for some one-forms $\beta_i$, which implies $\mathrm{d}x^1,\mathrm{d}x^2$ are $g$-orthogonal to $\beta_1,\beta_2$. Thus $\text{span}\{\beta_1,\beta_2\}=\text{span}\{\alpha_3,\alpha_4\}$, so $\beta_1 \wedge \beta_2=f\alpha_3 \wedge \alpha_4$ for some function $f$. We now need to prove that $f=1$, which is (up to a sign) equivalent to the statement $$ \| \ast_g(\mathrm{d}x^1\wedge\mathrm{d}x^2)\|=\|\alpha_3 \wedge \alpha_4\|. $$ Since the Hodge dual operator is an isometry, this is equivalent to $$ \| \mathrm{d}x^1\wedge\mathrm{d}x^2\|=\|\alpha_3 \wedge \alpha_4\|. $$ This is where the assumption $\det(g_{ij})=1$ is supposed to enter. I tried to expand both sides in terms of the $g_{ij}$, but so far I don't see how the result follows. Perhaps there is another easier way to see this. For the interested, this computation came up in a question about the existence of ""higher-order"" harmonic coordinates. Comment: Of course, everything here is ""pointwise"", i.e. this is really a result about $4$-dim inner product spaces. I have kept the manifold notation since it might be more familiar. *The Hodge star operator preserve decomposability of elements.","Let $(M,g)$ be a $4$-dimensional smooth Riemannian manifold.  I am trying to understand the following exterior algebra computation: Let $x^1,x^2,x^3,x^4$ be local coordinates on $M$ such that the Riemannian volume form of $g$ is $\mathrm{d}x^1\wedge\mathrm{d}x^2\wedge\mathrm{d}x^3\wedge\mathrm{d}x^4$. Then there exist a local frame $\alpha_1,\alpha_2,\alpha_3,\alpha_4$ of $1$-forms such that the following identities hold: $$ \begin{aligned} \ast_g(\mathrm{d}x^1\wedge\mathrm{d}x^2)&= \alpha_3\wedge\alpha_4\\ \ast_g(\mathrm{d}x^1\wedge\mathrm{d}x^3)&= \alpha_4\wedge\alpha_2\\ \ast_g(\mathrm{d}x^1\wedge\mathrm{d}x^4)&= \alpha_2\wedge\alpha_3\\ \end{aligned}\qquad \begin{aligned} \ast_g(\mathrm{d}x^3\wedge\mathrm{d}x^4)&= \alpha_1\wedge\alpha_2\\ \ast_g(\mathrm{d}x^4\wedge\mathrm{d}x^2)&= \alpha_1\wedge\alpha_3\\ \ast_g(\mathrm{d}x^2\wedge\mathrm{d}x^3)&= \alpha_1\wedge\alpha_4\\ \end{aligned} \tag1 $$ (The pattern is that the indices on both sides of every equation are complementary.) In fact, the solution of these equations (which is unique up to replacing each $\alpha_i$ by $-\alpha_i$) is given by  $$ \alpha_i = g_{ij}\,\mathrm{d}x^j\qquad\qquad\text{where}\quad  g = g_{ij}\,\mathrm{d}x^i\mathrm{d}x^j. $$ Question: Why does the formula $\alpha_i = g_{ij}\,\mathrm{d}x^j$ solve the equations? Here is what I understood: $\mathrm{d}x^1,\mathrm{d}x^2$ are $g$-orthogonal to $\alpha_3,\alpha_4$. Since $\ast_g(\mathrm{d}x^1\wedge\mathrm{d}x^2)$ is decomposable*, we can write  $$ \ast_g(\mathrm{d}x^1\wedge\mathrm{d}x^2)=\beta_1 \wedge \beta_2$$  for some one-forms $\beta_i$, which implies $\mathrm{d}x^1,\mathrm{d}x^2$ are $g$-orthogonal to $\beta_1,\beta_2$. Thus $\text{span}\{\beta_1,\beta_2\}=\text{span}\{\alpha_3,\alpha_4\}$, so $\beta_1 \wedge \beta_2=f\alpha_3 \wedge \alpha_4$ for some function $f$. We now need to prove that $f=1$, which is (up to a sign) equivalent to the statement $$ \| \ast_g(\mathrm{d}x^1\wedge\mathrm{d}x^2)\|=\|\alpha_3 \wedge \alpha_4\|. $$ Since the Hodge dual operator is an isometry, this is equivalent to $$ \| \mathrm{d}x^1\wedge\mathrm{d}x^2\|=\|\alpha_3 \wedge \alpha_4\|. $$ This is where the assumption $\det(g_{ij})=1$ is supposed to enter. I tried to expand both sides in terms of the $g_{ij}$, but so far I don't see how the result follows. Perhaps there is another easier way to see this. For the interested, this computation came up in a question about the existence of ""higher-order"" harmonic coordinates. Comment: Of course, everything here is ""pointwise"", i.e. this is really a result about $4$-dim inner product spaces. I have kept the manifold notation since it might be more familiar. *The Hodge star operator preserve decomposability of elements.",,"['differential-geometry', 'riemannian-geometry', 'differential-forms', 'exterior-algebra', '4-manifolds']"
50,Conormal bundle and lagrangian submanifold,Conormal bundle and lagrangian submanifold,,"Let $Q_1^{n_1},Q_2^{n_2}$ be smooth manifolds, $\phi:Q_1\to Q_2$ a smooth map and:   $$R_\phi:=\{(x, \xi, y,\eta)\mid y=\phi(x), \xi=(d\phi)^*\eta\}\subset T^*Q_1\times T^*Q_2$$   $$\text{graph}(\phi)=\{(q,\phi(q))\mid q\in Q_1\}\subset Q_1\times Q_2$$   verifiy that $R_\phi$ is a Lagrangian submanifold and describe the relation between $R_\phi$ and the conormal bundle $N^*\text{graph}(\phi)$. I was able prove that $R_\phi$ is Lagrangian, but I don't know where the conormal bundle fits into the picture. Since $R_\phi$ is Lagrangian, it has dimension $n_1+n_2$, while $N^*\text{graph}(\phi)$ has dimension $n_2$. Is the conormal bundle embbeded in $R_\phi$, maybe? If so,  why is this interesting?","Let $Q_1^{n_1},Q_2^{n_2}$ be smooth manifolds, $\phi:Q_1\to Q_2$ a smooth map and:   $$R_\phi:=\{(x, \xi, y,\eta)\mid y=\phi(x), \xi=(d\phi)^*\eta\}\subset T^*Q_1\times T^*Q_2$$   $$\text{graph}(\phi)=\{(q,\phi(q))\mid q\in Q_1\}\subset Q_1\times Q_2$$   verifiy that $R_\phi$ is a Lagrangian submanifold and describe the relation between $R_\phi$ and the conormal bundle $N^*\text{graph}(\phi)$. I was able prove that $R_\phi$ is Lagrangian, but I don't know where the conormal bundle fits into the picture. Since $R_\phi$ is Lagrangian, it has dimension $n_1+n_2$, while $N^*\text{graph}(\phi)$ has dimension $n_2$. Is the conormal bundle embbeded in $R_\phi$, maybe? If so,  why is this interesting?",,"['differential-geometry', 'symplectic-geometry', 'tangent-bundle']"
51,Proof verification: An asymptotic & geodesic curve is a straight line.,Proof verification: An asymptotic & geodesic curve is a straight line.,,"I am trying to prove that a curve $C\subset S$ is both an asymptotic curve and a geodesic if and only if $C$ is a (segment of) a straight line. $\Rightarrow$ NTS: If $C$ is both asymptotic and a geodesic, then it is a straight line. By definition asymptotic curves have normal curvature of zero. Hence $k_n = 0$. The relationship of normal, geodesic, and space curvature of $C$ gives: $(k_n)^2 + (k_g)^2=k^2$. Hence $\mid k_g \mid = k.$ A curve is geodesic if and only if $k_g = 0$ at each point of the curve. So $k=0$. Since $k=\vert \alpha ''(x) \vert \equiv 0$, then by integration $\alpha(s) = cs + d$, so the curve is a (segment of) a straight line. $\Leftarrow$ NTS: If $C$ is a straight line, then it is both an asymptotic curve and a geodesic. If $C$ is a straight line, the curvature of $C$ is zero, $k=0$, but from the relation $(k_n)^2+(k_g)^2=k^2$, $(k_n)^2 \geq 0$, and $(k_g)^2 \geq 0$, so $k_n$ and $k_g$ must both be zero. $\therefore$ $C \subset S$ is asymptotic and geodesic curve if and only if $C$ is a (segment of) a straight line. Is this correct? Thanks [Edited]","I am trying to prove that a curve $C\subset S$ is both an asymptotic curve and a geodesic if and only if $C$ is a (segment of) a straight line. $\Rightarrow$ NTS: If $C$ is both asymptotic and a geodesic, then it is a straight line. By definition asymptotic curves have normal curvature of zero. Hence $k_n = 0$. The relationship of normal, geodesic, and space curvature of $C$ gives: $(k_n)^2 + (k_g)^2=k^2$. Hence $\mid k_g \mid = k.$ A curve is geodesic if and only if $k_g = 0$ at each point of the curve. So $k=0$. Since $k=\vert \alpha ''(x) \vert \equiv 0$, then by integration $\alpha(s) = cs + d$, so the curve is a (segment of) a straight line. $\Leftarrow$ NTS: If $C$ is a straight line, then it is both an asymptotic curve and a geodesic. If $C$ is a straight line, the curvature of $C$ is zero, $k=0$, but from the relation $(k_n)^2+(k_g)^2=k^2$, $(k_n)^2 \geq 0$, and $(k_g)^2 \geq 0$, so $k_n$ and $k_g$ must both be zero. $\therefore$ $C \subset S$ is asymptotic and geodesic curve if and only if $C$ is a (segment of) a straight line. Is this correct? Thanks [Edited]",,"['differential-geometry', 'curvature']"
52,"Intrinsic, coordinate-free interpretation of ""inverse"" of a metric tensor","Intrinsic, coordinate-free interpretation of ""inverse"" of a metric tensor",,"I am taking a course on general relativity. There is some part where I think they are abusing terminology and I would like to interpret more properly. Given an indefinite metric tensor $g$ on a smooth manifold $M$, write $g$ in component form in local coordinates as $g_{\mu\nu}$. We define the following contravariant tensor in component form in local coordinates as $g^{\mu\nu}$, where $g^{\mu\nu}g_{\nu\lambda}=g_{\lambda\nu}g^{\nu\mu}=\delta_\lambda^\mu$, i.e. $g^{\mu\nu}$ is (the components of) the ""inverse"" of the metric tensor. The above is how they defined $g^{\mu\nu}$. I find it problematic to me. Firstly, the definition relies on coordinates. Secondly, metric tensors are type $(0,2)$ tensors instead of type $(1,1)$, i.e. they are not linear transformations. Without reference to coordinates, there is no such thing as ""inverse"" of type $(0,2)$ tensor. I propose two ways to define and interpret $g^{\mu\nu}$ in an as-coordinate-free-as-possible way: While $g$ is nondegenerate, it induces an isomorphism between a tangent space $T_pM$ and its dual $T_p^*M$ by $v\mapsto g_p(v,\cdot)$. This isomorphism transfers the indefinite inner product $g_p$ from $T_pM$ to $T_p^*M$. This inner product on $T_p^*M$ is type $(2,0)$ on the manifold, and $g^{\mu\nu}$ would be the local coordinates expression of this inner product. While $g$ is nondegenerate, hopefully there exists a unique type $(2,0)$ tensor on $M$, denoted $g^{-1}$ (this is NOT an inverse of $g$. It is just a symbol) for which forming the tensor products $g^{-1}\otimes g$ and $g\otimes g^{-1}$ and then do a tensor contraction on each of them would both give the identity linear transformation on each tangent space. Then $g^{\mu\nu}$ would be the local expression of $g^{-1}$. Are the above interpretations of $g^{\mu\nu}$ correct, i.e. can we recover the equation $g^{\mu\nu}g_{\nu\lambda}=g_{\lambda\nu}g^{\nu\mu}=\delta_\lambda^\mu$? I am quite sure interpretation 2 is correct, because multiplying the two components is corresponds to tensor product, and making an upper index and a lower index equal and sum over that index corresponds to tensor contraction. But I am not very sure about interpretation 1.","I am taking a course on general relativity. There is some part where I think they are abusing terminology and I would like to interpret more properly. Given an indefinite metric tensor $g$ on a smooth manifold $M$, write $g$ in component form in local coordinates as $g_{\mu\nu}$. We define the following contravariant tensor in component form in local coordinates as $g^{\mu\nu}$, where $g^{\mu\nu}g_{\nu\lambda}=g_{\lambda\nu}g^{\nu\mu}=\delta_\lambda^\mu$, i.e. $g^{\mu\nu}$ is (the components of) the ""inverse"" of the metric tensor. The above is how they defined $g^{\mu\nu}$. I find it problematic to me. Firstly, the definition relies on coordinates. Secondly, metric tensors are type $(0,2)$ tensors instead of type $(1,1)$, i.e. they are not linear transformations. Without reference to coordinates, there is no such thing as ""inverse"" of type $(0,2)$ tensor. I propose two ways to define and interpret $g^{\mu\nu}$ in an as-coordinate-free-as-possible way: While $g$ is nondegenerate, it induces an isomorphism between a tangent space $T_pM$ and its dual $T_p^*M$ by $v\mapsto g_p(v,\cdot)$. This isomorphism transfers the indefinite inner product $g_p$ from $T_pM$ to $T_p^*M$. This inner product on $T_p^*M$ is type $(2,0)$ on the manifold, and $g^{\mu\nu}$ would be the local coordinates expression of this inner product. While $g$ is nondegenerate, hopefully there exists a unique type $(2,0)$ tensor on $M$, denoted $g^{-1}$ (this is NOT an inverse of $g$. It is just a symbol) for which forming the tensor products $g^{-1}\otimes g$ and $g\otimes g^{-1}$ and then do a tensor contraction on each of them would both give the identity linear transformation on each tangent space. Then $g^{\mu\nu}$ would be the local expression of $g^{-1}$. Are the above interpretations of $g^{\mu\nu}$ correct, i.e. can we recover the equation $g^{\mu\nu}g_{\nu\lambda}=g_{\lambda\nu}g^{\nu\mu}=\delta_\lambda^\mu$? I am quite sure interpretation 2 is correct, because multiplying the two components is corresponds to tensor product, and making an upper index and a lower index equal and sum over that index corresponds to tensor contraction. But I am not very sure about interpretation 1.",,"['differential-geometry', 'tensors']"
53,Hessian and metric tensors on riemannian manifolds,Hessian and metric tensors on riemannian manifolds,,"First, a bit of notation (I'm assuming Einstein's convention on summation of index): $\text{Hessian}(f):=\nabla^2(f):=\nabla\nabla(f)=\left(\partial_i\partial_j(f)-\Gamma_{ij}^k\partial_k(f)\right)dx^i\otimes dx^j$ Where $\nabla$ is the Levi-Civita connection on a riemannian manifold $(M,g)$ and $\Gamma$ are the Christoffel symbols. By definition and a bit of computation, it is easy to note that $\nabla^2(f)\in\ \text{TM}^*\otimes\text{TM}^*$, as $g$, the metric tensor. My questions are: As it never been analyzed the function $f:\nabla^2(f)=g$ for a general manifold (that is: we can associate to a function $f$ a metric, but can we do the opposite in general?) How? Has this $f$ any kind of application? If so, where? I would really appreciate any kind of suggestion, expecially books or articles (I've found something, like https://arxiv.org/pdf/1312.1103.pdf , but is a bit too advanced for me, talking about transformations I haven't studied yet). Thanks in advance","First, a bit of notation (I'm assuming Einstein's convention on summation of index): $\text{Hessian}(f):=\nabla^2(f):=\nabla\nabla(f)=\left(\partial_i\partial_j(f)-\Gamma_{ij}^k\partial_k(f)\right)dx^i\otimes dx^j$ Where $\nabla$ is the Levi-Civita connection on a riemannian manifold $(M,g)$ and $\Gamma$ are the Christoffel symbols. By definition and a bit of computation, it is easy to note that $\nabla^2(f)\in\ \text{TM}^*\otimes\text{TM}^*$, as $g$, the metric tensor. My questions are: As it never been analyzed the function $f:\nabla^2(f)=g$ for a general manifold (that is: we can associate to a function $f$ a metric, but can we do the opposite in general?) How? Has this $f$ any kind of application? If so, where? I would really appreciate any kind of suggestion, expecially books or articles (I've found something, like https://arxiv.org/pdf/1312.1103.pdf , but is a bit too advanced for me, talking about transformations I haven't studied yet). Thanks in advance",,['differential-geometry']
54,Question about Hopf invariant from Milnor,Question about Hopf invariant from Milnor,,"In Milnor's book Topology from the Differentiable Viewpoint , there is a problem concerning the definition of the Hopf invariant.  Let $y\neq z$ be regular values of a smooth map $f:S^{2p-1}\to S^p$, then we want to show that the linking number $\ell(f^{-1}(y),f^{-1}(z))$ is locally constant as a function of $y$.  The Hopf invariant of $f$ is then defined as $\ell(f^{-1}(y),f^{-1}(z))$, after several more parts of this exercise showing that this quantity only depends on the homotopy class of $f$. Here, the linking number $\ell(M,N)$ for compact, oriented, boundaryless submanifolds $M^m,N^n$ of $S^{m+n+1}$ is defined by picking some $p\in S^{m+n+1}\setminus(M\cup N)$, identifying $S^{m+n+1}$ with $\mathbb R^{m+n+1}$.  The linking number is then defined by the degree of the map $\lambda:M\times N\to S^{m+n}$ given by $$\lambda(x,y)=\frac{x-y}{\|x-y\|}.$$ My idea was to use the framed cobordism theory outlined in $\S7$.  For if we choose a neighborhood $U$ of $y$ consisting of regular values of $f$ with $z\in U$, and if $y_0\in U$, then $f^{-1}(y)$ is framed cobordant to $f^{-1}(y_0)$.  But I don't know where to go from here. Any hints about how I should proceed would be greatly appreciated.","In Milnor's book Topology from the Differentiable Viewpoint , there is a problem concerning the definition of the Hopf invariant.  Let $y\neq z$ be regular values of a smooth map $f:S^{2p-1}\to S^p$, then we want to show that the linking number $\ell(f^{-1}(y),f^{-1}(z))$ is locally constant as a function of $y$.  The Hopf invariant of $f$ is then defined as $\ell(f^{-1}(y),f^{-1}(z))$, after several more parts of this exercise showing that this quantity only depends on the homotopy class of $f$. Here, the linking number $\ell(M,N)$ for compact, oriented, boundaryless submanifolds $M^m,N^n$ of $S^{m+n+1}$ is defined by picking some $p\in S^{m+n+1}\setminus(M\cup N)$, identifying $S^{m+n+1}$ with $\mathbb R^{m+n+1}$.  The linking number is then defined by the degree of the map $\lambda:M\times N\to S^{m+n}$ given by $$\lambda(x,y)=\frac{x-y}{\|x-y\|}.$$ My idea was to use the framed cobordism theory outlined in $\S7$.  For if we choose a neighborhood $U$ of $y$ consisting of regular values of $f$ with $z\in U$, and if $y_0\in U$, then $f^{-1}(y)$ is framed cobordant to $f^{-1}(y_0)$.  But I don't know where to go from here. Any hints about how I should proceed would be greatly appreciated.",,"['differential-geometry', 'algebraic-topology', 'differential-topology']"
55,Proving a few properties of Bertrand curves,Proving a few properties of Bertrand curves,,"Here's what I've got so far (and I'm assuming $\alpha$ is a unit speed curve): a) The fact that $\beta(s) = \alpha(s) + r(s)N(s)$ for some scalar function $r$ follows trivially because of the fact that the normal lines of $\beta$ and $\alpha$ are equal (so $\beta(s)$ is a point on the normal line of $\alpha(s)$, which is precisely what the equality states). Then: $$\ r(s) = (\beta(s) - \alpha(s))N(s) \Rightarrow r'(s) = (\beta'(s) -     \alpha'(s))N(s) + (\beta(s) - \alpha(s))N'(s)  $$ $$r'(s) = (\beta'(s) -\alpha'(s))N(s) + r(s)N(s)N'(s) = (\beta'(s) -     \alpha'(s))N(s)= \beta'(s)N(s) = 0$$ as desired. b) Let $\theta$ be the angle between the unit tangent vectors mentioned. Then: $$\cos(\theta) = \frac{\beta'(s)\alpha'(s)}{||\beta'(s)||} = \frac{1-r(s)k(s)}{||\beta'(s)||}$$ but $$||\beta'(s)||^2 = ||\alpha'(s)||^2 + r^2(s)||N(s)||^2 = 1 + r^2(s)$$ so $$||\beta'(s)|| = \sqrt{1+r^2(s)}$$ $$\cos(\theta) = \frac{1-r(s)k(s)}{\sqrt{1+r^2(s)}}$$ EDIT: The formula for $||\beta'(s)||^2$ above is wrong. Actually, $||\beta'(s)||^2 = ||\alpha'(s)||^2 + r^2(s)||N'(s)||^2$, which is not as neat as the above. But the hint in the comments is much better to solve this. c) Assuming I had done b) , then the following would be true: $$(\cos(\theta))' = \left( \frac{\beta'(s)\alpha'(s)}{||\beta'(s)||} \right)' = 0 \Rightarrow (\beta'(s)\alpha'(s))' = 0 \Rightarrow \beta''(s)\alpha'(s) + \beta'(s)\alpha''(s) = 0$$ but continuing on this path only leads to $k'(s) = 0$, which, even if I had done b), would not be useful (and I'm aware using this to prove b) is circular). Beyond these (b) and c)), I would also like some help solving d) and $21$ (I don't know where to start on those). Update : A lot of what I wrote above is wrong, but my doubts here are almost completely solved. I just have to finish $21$ and I'll be done. Update 2 : For $20$, see the discussion in comments. For $21$: Since $T_\beta$ is orthogonal to $N_\alpha$, we can write it as a linear combination of $T_\alpha$ and $B_\alpha$. In particular: $$T_\beta = \pm (\cos(\theta)T_\alpha + \sin(\theta)B_\alpha)$$ where $\theta$ is the angle between the unit tangent vectors of $\alpha$ and $\beta$. Then: $$B_\beta = T_\beta \times N_\beta = \pm T_\beta \times N_\alpha = \pm (\cos(\theta)T_\alpha + \sin(\theta)B_\alpha) N_\alpha= \pm(\cos(\theta)B_\alpha - \sin(\theta)T_\alpha)$$ Differentiating and using the fact that $\cos(\theta)$ and $\sin(\theta)$ are constant, we have: $$B_\beta' = \pm(\cos(\theta)B_\alpha' -\sin(\theta) T_\alpha') = \pm(-\cos(\theta)\tau_\alpha N_\alpha - \sin(\theta)\kappa_\alpha N_\alpha)$$ $$B_\beta' = \pm(-\cos(\theta)\tau_\alpha - \sin(\theta)\kappa_\alpha)N_\alpha$$ By the Frenet frame, we also have: $$B_\beta' = -\tau_\beta N_\beta = \pm(-\tau_\beta N_\alpha) $$ So: $$\pm(-\cos(\theta)\tau_\alpha - \sin(\theta)\kappa_\alpha)N_\alpha =  \pm(-\tau_\beta N_\alpha) \Rightarrow (\cos(\theta)\tau_\alpha + \sin(\theta)\kappa_\alpha) = \pm \tau_b $$ By previous work: $$\cot(\theta) = \frac{1-r(s)\kappa_\alpha(s)}{r(s) \tau_\alpha(s)}$$ which is equivalent to: $$\kappa_\alpha(s) + \cot(\theta) \tau_\alpha = \frac{1}{r(s)} \Rightarrow \kappa_\alpha(s) \sin(\theta) + \tau_\alpha(s) \cos(\theta) = \frac{\sin(\theta)}{r(s)}$$ Then: $$\pm (\cos(\theta)\tau_\alpha + \sin(\theta)\kappa_\alpha) = \pm \frac{\sin(\theta)}{r(s)} = \tau_\beta(s)$$ $\bar{s}$ being the arclength paramater, we also have: $$T_\beta = \frac{d \beta}{ds} \frac{ds}{d\bar{s}}$$ so: $$T_\beta = \left( ( 1 - r(s) \kappa_\alpha(s))T_\alpha + r(s) \tau_\alpha(s) b_\alpha(s) \right ) \frac{ds}{d\bar{s}}$$ comparing the very first expression for the coefficients, we see that: $$\pm \sin(\theta) = r(s) \tau_\alpha(s) \frac{ds}{d \bar{s}}$$ so that: $$\frac{d \bar{s}}{ds} = \frac{r(s)\tau_\alpha(s)}{\sin(\theta)}$$ and then, by previous work: $$\pm \frac{\tau_\alpha(s) \tau_\beta(s) r(s)}{\sin(\theta)} = \pm \frac{\sin(\theta)}{r(s)} \Rightarrow \tau_\alpha(s) \tau_\beta(s) = \frac{\sin^2{\theta}}{r^2(s)}$$ as desired.","Here's what I've got so far (and I'm assuming $\alpha$ is a unit speed curve): a) The fact that $\beta(s) = \alpha(s) + r(s)N(s)$ for some scalar function $r$ follows trivially because of the fact that the normal lines of $\beta$ and $\alpha$ are equal (so $\beta(s)$ is a point on the normal line of $\alpha(s)$, which is precisely what the equality states). Then: $$\ r(s) = (\beta(s) - \alpha(s))N(s) \Rightarrow r'(s) = (\beta'(s) -     \alpha'(s))N(s) + (\beta(s) - \alpha(s))N'(s)  $$ $$r'(s) = (\beta'(s) -\alpha'(s))N(s) + r(s)N(s)N'(s) = (\beta'(s) -     \alpha'(s))N(s)= \beta'(s)N(s) = 0$$ as desired. b) Let $\theta$ be the angle between the unit tangent vectors mentioned. Then: $$\cos(\theta) = \frac{\beta'(s)\alpha'(s)}{||\beta'(s)||} = \frac{1-r(s)k(s)}{||\beta'(s)||}$$ but $$||\beta'(s)||^2 = ||\alpha'(s)||^2 + r^2(s)||N(s)||^2 = 1 + r^2(s)$$ so $$||\beta'(s)|| = \sqrt{1+r^2(s)}$$ $$\cos(\theta) = \frac{1-r(s)k(s)}{\sqrt{1+r^2(s)}}$$ EDIT: The formula for $||\beta'(s)||^2$ above is wrong. Actually, $||\beta'(s)||^2 = ||\alpha'(s)||^2 + r^2(s)||N'(s)||^2$, which is not as neat as the above. But the hint in the comments is much better to solve this. c) Assuming I had done b) , then the following would be true: $$(\cos(\theta))' = \left( \frac{\beta'(s)\alpha'(s)}{||\beta'(s)||} \right)' = 0 \Rightarrow (\beta'(s)\alpha'(s))' = 0 \Rightarrow \beta''(s)\alpha'(s) + \beta'(s)\alpha''(s) = 0$$ but continuing on this path only leads to $k'(s) = 0$, which, even if I had done b), would not be useful (and I'm aware using this to prove b) is circular). Beyond these (b) and c)), I would also like some help solving d) and $21$ (I don't know where to start on those). Update : A lot of what I wrote above is wrong, but my doubts here are almost completely solved. I just have to finish $21$ and I'll be done. Update 2 : For $20$, see the discussion in comments. For $21$: Since $T_\beta$ is orthogonal to $N_\alpha$, we can write it as a linear combination of $T_\alpha$ and $B_\alpha$. In particular: $$T_\beta = \pm (\cos(\theta)T_\alpha + \sin(\theta)B_\alpha)$$ where $\theta$ is the angle between the unit tangent vectors of $\alpha$ and $\beta$. Then: $$B_\beta = T_\beta \times N_\beta = \pm T_\beta \times N_\alpha = \pm (\cos(\theta)T_\alpha + \sin(\theta)B_\alpha) N_\alpha= \pm(\cos(\theta)B_\alpha - \sin(\theta)T_\alpha)$$ Differentiating and using the fact that $\cos(\theta)$ and $\sin(\theta)$ are constant, we have: $$B_\beta' = \pm(\cos(\theta)B_\alpha' -\sin(\theta) T_\alpha') = \pm(-\cos(\theta)\tau_\alpha N_\alpha - \sin(\theta)\kappa_\alpha N_\alpha)$$ $$B_\beta' = \pm(-\cos(\theta)\tau_\alpha - \sin(\theta)\kappa_\alpha)N_\alpha$$ By the Frenet frame, we also have: $$B_\beta' = -\tau_\beta N_\beta = \pm(-\tau_\beta N_\alpha) $$ So: $$\pm(-\cos(\theta)\tau_\alpha - \sin(\theta)\kappa_\alpha)N_\alpha =  \pm(-\tau_\beta N_\alpha) \Rightarrow (\cos(\theta)\tau_\alpha + \sin(\theta)\kappa_\alpha) = \pm \tau_b $$ By previous work: $$\cot(\theta) = \frac{1-r(s)\kappa_\alpha(s)}{r(s) \tau_\alpha(s)}$$ which is equivalent to: $$\kappa_\alpha(s) + \cot(\theta) \tau_\alpha = \frac{1}{r(s)} \Rightarrow \kappa_\alpha(s) \sin(\theta) + \tau_\alpha(s) \cos(\theta) = \frac{\sin(\theta)}{r(s)}$$ Then: $$\pm (\cos(\theta)\tau_\alpha + \sin(\theta)\kappa_\alpha) = \pm \frac{\sin(\theta)}{r(s)} = \tau_\beta(s)$$ $\bar{s}$ being the arclength paramater, we also have: $$T_\beta = \frac{d \beta}{ds} \frac{ds}{d\bar{s}}$$ so: $$T_\beta = \left( ( 1 - r(s) \kappa_\alpha(s))T_\alpha + r(s) \tau_\alpha(s) b_\alpha(s) \right ) \frac{ds}{d\bar{s}}$$ comparing the very first expression for the coefficients, we see that: $$\pm \sin(\theta) = r(s) \tau_\alpha(s) \frac{ds}{d \bar{s}}$$ so that: $$\frac{d \bar{s}}{ds} = \frac{r(s)\tau_\alpha(s)}{\sin(\theta)}$$ and then, by previous work: $$\pm \frac{\tau_\alpha(s) \tau_\beta(s) r(s)}{\sin(\theta)} = \pm \frac{\sin(\theta)}{r(s)} \Rightarrow \tau_\alpha(s) \tau_\beta(s) = \frac{\sin^2{\theta}}{r^2(s)}$$ as desired.",,"['differential-geometry', 'curves', 'frenet-frame']"
56,Chern-Weil homomorphism and Chern/Pontryagin/Euler class,Chern-Weil homomorphism and Chern/Pontryagin/Euler class,,"I am reading Chapter on characteristic classes from Foundations of Differential geometry by Kobayashi and Nomizu. This chapter starts with concept of Chern-Weil homomorphism. Given a Lie algebra $G$ with $\mathfrak{g}$ as its Lie algebra and a principal $G$ bundle $P(M,G)$ chern Weil homomorphism is a map from $I(G)\rightarrow H^*(M,\mathbb{R})$ where $I(G)$ is the algebra of symmetric multilinear mappings on $\mathfrak{g}$ invariant by $G$ and $H^*(M;\mathbb{R})$ is the deRham cohomology algebra on $M$.  This they define fixing a connection and then proves this map is independent of choice of connection. I am able to understand this. Can some one help me to understand how this Chern-Weil homomorphism is involved in understanding about chern/Pontryagin/Euler classes? Any reference that explains motivation on these characteristic classes is also welcome. I am aware of Milnor’s book.","I am reading Chapter on characteristic classes from Foundations of Differential geometry by Kobayashi and Nomizu. This chapter starts with concept of Chern-Weil homomorphism. Given a Lie algebra $G$ with $\mathfrak{g}$ as its Lie algebra and a principal $G$ bundle $P(M,G)$ chern Weil homomorphism is a map from $I(G)\rightarrow H^*(M,\mathbb{R})$ where $I(G)$ is the algebra of symmetric multilinear mappings on $\mathfrak{g}$ invariant by $G$ and $H^*(M;\mathbb{R})$ is the deRham cohomology algebra on $M$.  This they define fixing a connection and then proves this map is independent of choice of connection. I am able to understand this. Can some one help me to understand how this Chern-Weil homomorphism is involved in understanding about chern/Pontryagin/Euler classes? Any reference that explains motivation on these characteristic classes is also welcome. I am aware of Milnor’s book.",,['differential-geometry']
57,How to see that $\mathbb{C}\mathbb{P}^3\cong\mathrm{SO}(5)/\mathrm{U}(2)$?,How to see that ?,\mathbb{C}\mathbb{P}^3\cong\mathrm{SO}(5)/\mathrm{U}(2),"I stumbled on this ismorphism in the context of twistor fibrations. See for example 'Twistors in Mathematics and Physics' by Bailey and Baston, p.58. Can anybody provide a construction of this isomorphism?","I stumbled on this ismorphism in the context of twistor fibrations. See for example 'Twistors in Mathematics and Physics' by Bailey and Baston, p.58. Can anybody provide a construction of this isomorphism?",,"['differential-geometry', 'homogeneous-spaces', 'twistor-theory']"
58,About differential of volume and area and length,About differential of volume and area and length,,"The volume $V$ of a sphere with radius $r$ is given by $$V=\frac43\pi r^3,$$ and its surface area is  $$S=4\pi r^2.$$ So, $$\frac{dV}{dr}=S.$$ I thought this means very thin volume of outer skin of sphere is area of the sphere. I thought that it can be applied to cylinder and other three-dimensional figures. For example, in the case of the cylinder, $$ \begin{split} V &= \pi r^2h\\ S &= 2\pi rh+2πr^2    = \frac{\partial V}{\partial r}+2\frac{\partial V}{\partial h} \end{split}$$ Similar principle can be applied to cube. But it can not be applied to cone tetrahedron.  For example, in cone,$$V=\frac{1}{3}πr^2h$$ $$S=πr^2+πr\sqrt{r^2+h^2}$$ It seems like V and S have no interaction. I'm curious about my understanding is correct and why cone and tetrahedron cannot be applied to this principle.","The volume $V$ of a sphere with radius $r$ is given by $$V=\frac43\pi r^3,$$ and its surface area is  $$S=4\pi r^2.$$ So, $$\frac{dV}{dr}=S.$$ I thought this means very thin volume of outer skin of sphere is area of the sphere. I thought that it can be applied to cylinder and other three-dimensional figures. For example, in the case of the cylinder, $$ \begin{split} V &= \pi r^2h\\ S &= 2\pi rh+2πr^2    = \frac{\partial V}{\partial r}+2\frac{\partial V}{\partial h} \end{split}$$ Similar principle can be applied to cube. But it can not be applied to cone tetrahedron.  For example, in cone,$$V=\frac{1}{3}πr^2h$$ $$S=πr^2+πr\sqrt{r^2+h^2}$$ It seems like V and S have no interaction. I'm curious about my understanding is correct and why cone and tetrahedron cannot be applied to this principle.",,['differential-geometry']
59,What is the connection between a row vector and covariant vector (or column and contravariant)?,What is the connection between a row vector and covariant vector (or column and contravariant)?,,"This youtube video by Eugene Khutoryansky makes the distinction clear between the covariant coordinates of a vector (dot product of the vector with each of the basis vectors or vector projection ), and the contravariant coordinates ( parallelogram law ): Is there a way you could explain to a lay person that this somehow underpins the following fact: Covariant vectors are representable as row vectors. Contravariant vectors are representable as column vectors. ? I would like to know, for example, if the idea carries beyond being able to calculate the length of a vector in non-Cartesian coordinates as the dot product of its covariant and contr$avariant expressions: $$ \lVert V\rVert ^2=\begin{bmatrix} V_X & V_Y & V_Z\end{bmatrix}\cdot \begin{bmatrix} V^X \\ V^Y\\ V^Z\end{bmatrix}.$$ In a curvilinear system presumably the contravariant basis vectors would be tangential, whereas the covariant basis vectors would be orthogonal to the coordinates: Apropos of the first comment, and if it can be confirmed (as a bonus), covariant vectors are covectors or dual vectors , while contravariant vectors are just vectors .","This youtube video by Eugene Khutoryansky makes the distinction clear between the covariant coordinates of a vector (dot product of the vector with each of the basis vectors or vector projection ), and the contravariant coordinates ( parallelogram law ): Is there a way you could explain to a lay person that this somehow underpins the following fact: Covariant vectors are representable as row vectors. Contravariant vectors are representable as column vectors. ? I would like to know, for example, if the idea carries beyond being able to calculate the length of a vector in non-Cartesian coordinates as the dot product of its covariant and contr$avariant expressions: $$ \lVert V\rVert ^2=\begin{bmatrix} V_X & V_Y & V_Z\end{bmatrix}\cdot \begin{bmatrix} V^X \\ V^Y\\ V^Z\end{bmatrix}.$$ In a curvilinear system presumably the contravariant basis vectors would be tangential, whereas the covariant basis vectors would be orthogonal to the coordinates: Apropos of the first comment, and if it can be confirmed (as a bonus), covariant vectors are covectors or dual vectors , while contravariant vectors are just vectors .",,"['differential-geometry', 'vectors', 'tensors']"
60,Counterexample to Gunther Theorem when assuming only a Ricci curvature upper bound,Counterexample to Gunther Theorem when assuming only a Ricci curvature upper bound,,"One of Gunther's comparison theorems states that if $(M,g)$ is a complete riemannian manifold, $m\in M$ , $\mathbb{B}_r(m)$ is a (geodetic) ball which does not touch Cut$_m$ and there exists a $b\in\mathbb{R}$ such that $K\le b$ (where $K$ is the sectional curvature) we can say that $$ vol_g(\mathbb{B}_r(m))\ge V^b(r) $$ where $V^b(r)$ is the volume of a ball of radius $r$ in the space form of curvature $b$. I would like to find a counterexample as to why this statement cannot be true if we were to assume the same hypothesis but the one about sectional curvature, replacing it with one about Ricci curvature ($Ric_g \le (n-1)bg$ ). I am aware that it can be shown that every riemannian manifold (under suitable assumptions) admits a metric whose Ricci curvature is strictly negative, though I have not yet managed to see how it can be useful in this context. The problem which I encounter is that volume is a Riemannian notion, while I assume the existence of metrics of negative curvature is useful to gather differential or topological informations.","One of Gunther's comparison theorems states that if $(M,g)$ is a complete riemannian manifold, $m\in M$ , $\mathbb{B}_r(m)$ is a (geodetic) ball which does not touch Cut$_m$ and there exists a $b\in\mathbb{R}$ such that $K\le b$ (where $K$ is the sectional curvature) we can say that $$ vol_g(\mathbb{B}_r(m))\ge V^b(r) $$ where $V^b(r)$ is the volume of a ball of radius $r$ in the space form of curvature $b$. I would like to find a counterexample as to why this statement cannot be true if we were to assume the same hypothesis but the one about sectional curvature, replacing it with one about Ricci curvature ($Ric_g \le (n-1)bg$ ). I am aware that it can be shown that every riemannian manifold (under suitable assumptions) admits a metric whose Ricci curvature is strictly negative, though I have not yet managed to see how it can be useful in this context. The problem which I encounter is that volume is a Riemannian notion, while I assume the existence of metrics of negative curvature is useful to gather differential or topological informations.",,"['differential-geometry', 'riemannian-geometry']"
61,Killing Fields on Euclidean Spaces,Killing Fields on Euclidean Spaces,,"Let $K$ be a set of all Killing vector fields on $\mathbf R^n$ (with the Euclidean metric $\bar g$) which vanish at the origin. (A vector field $V$ on a Riemannian manifold $(M, g)$ is said to be a Killing vector field if the flow of $V$ acts by isometries of $M$. This is equivalent to saying that $\mathcal L_Vg=0$). If $V\in K$, then by using $\mathcal L_V\bar g=0$, we get that the matrix $[\partial V^i/\partial x^j]$ is anti-symmetric, where $V^i$ are the components of $V$ in the standard coordinates. Define a map $T:K\to \mathfrak o(n)$ as $$T(V)= \left[\frac{\partial V^i}{\partial x^j}(0)\right]$$ where $\mathfrak o(n)$ is the Lie algebra of $O(n)$, which is ""same"" as the space of $n\times n$ real anti-symmetric matrices. Problem. To show that $T$ is injective. I am quite lost here.","Let $K$ be a set of all Killing vector fields on $\mathbf R^n$ (with the Euclidean metric $\bar g$) which vanish at the origin. (A vector field $V$ on a Riemannian manifold $(M, g)$ is said to be a Killing vector field if the flow of $V$ acts by isometries of $M$. This is equivalent to saying that $\mathcal L_Vg=0$). If $V\in K$, then by using $\mathcal L_V\bar g=0$, we get that the matrix $[\partial V^i/\partial x^j]$ is anti-symmetric, where $V^i$ are the components of $V$ in the standard coordinates. Define a map $T:K\to \mathfrak o(n)$ as $$T(V)= \left[\frac{\partial V^i}{\partial x^j}(0)\right]$$ where $\mathfrak o(n)$ is the Lie algebra of $O(n)$, which is ""same"" as the space of $n\times n$ real anti-symmetric matrices. Problem. To show that $T$ is injective. I am quite lost here.",,"['differential-geometry', 'riemannian-geometry']"
62,$G$-invariant tangent bundle,-invariant tangent bundle,G,"We are given a Lie group $G$ and a smooth manifold $M$ equipped with a smooth group action $\Phi : G \times M \to M$, where we denote $\Phi_g := \Phi(g, \cdot)$ for $g \in G$. Consider the following equivalence relation on the tangent bundle $TM$: For $X_x \in T_x M$ and $Y_y \in T_y M$, define $$ X_x \sim Y_y : \iff \exists g \in G : X_x = T_y \Phi_g Y_y, $$ denoting by $T_y \Phi_g : T_y M \to T_ {\Phi_g(y)} M$ the usual tangent map. Then, define $TM_G := TM/ \sim$ as some kind of reduction of the tangent bundle. We can equip this with a surjective map $\pi : TM_G \to M/G$ by setting $\pi([X_x]) = [x]$, and we find a vector space structure on every preimage $\pi^{-1}([x])$. Also, since $T\Phi : G \times TM \to TM$ can be regarded as a group action on the manifold $TM$, if we have a free and proper $\Phi$, we also have a free and proper $T \Phi$, and as such a smooth manifold structure on both $M/G$ and $TM_G$ (I'm not 100% certain whether properness of $\Phi$ implies properness of $T \Phi$, this I still need to check in detail, but my gut tells me it will work out). My question now is: Does this structure end up being a vector bundle over $M/G$ if we assume free and proper $\Phi$? Or do we need stronger conditions? With what I've got, I can't seem to write down meaningful local trivializations. Or is there a simpler construction for this bundle where the vector bundle structure comes more naturally? This thing we constructed here is in some sense ""larger"" than the usual tangent bundle: If well defined, every fiber in $T(M/G)$ is of dimension $\dim M - \dim G$, and every fiber in $TM_G$ is of dimension $\dim M$, so this is definitely something different from $T(M/G)$. I hope the question and the setting are clear, it might be that the whole idea is bollocks, but it seemed like a very natural structure to me. All help appreciated!","We are given a Lie group $G$ and a smooth manifold $M$ equipped with a smooth group action $\Phi : G \times M \to M$, where we denote $\Phi_g := \Phi(g, \cdot)$ for $g \in G$. Consider the following equivalence relation on the tangent bundle $TM$: For $X_x \in T_x M$ and $Y_y \in T_y M$, define $$ X_x \sim Y_y : \iff \exists g \in G : X_x = T_y \Phi_g Y_y, $$ denoting by $T_y \Phi_g : T_y M \to T_ {\Phi_g(y)} M$ the usual tangent map. Then, define $TM_G := TM/ \sim$ as some kind of reduction of the tangent bundle. We can equip this with a surjective map $\pi : TM_G \to M/G$ by setting $\pi([X_x]) = [x]$, and we find a vector space structure on every preimage $\pi^{-1}([x])$. Also, since $T\Phi : G \times TM \to TM$ can be regarded as a group action on the manifold $TM$, if we have a free and proper $\Phi$, we also have a free and proper $T \Phi$, and as such a smooth manifold structure on both $M/G$ and $TM_G$ (I'm not 100% certain whether properness of $\Phi$ implies properness of $T \Phi$, this I still need to check in detail, but my gut tells me it will work out). My question now is: Does this structure end up being a vector bundle over $M/G$ if we assume free and proper $\Phi$? Or do we need stronger conditions? With what I've got, I can't seem to write down meaningful local trivializations. Or is there a simpler construction for this bundle where the vector bundle structure comes more naturally? This thing we constructed here is in some sense ""larger"" than the usual tangent bundle: If well defined, every fiber in $T(M/G)$ is of dimension $\dim M - \dim G$, and every fiber in $TM_G$ is of dimension $\dim M$, so this is definitely something different from $T(M/G)$. I hope the question and the setting are clear, it might be that the whole idea is bollocks, but it seemed like a very natural structure to me. All help appreciated!",,"['differential-geometry', 'vector-bundles', 'group-actions']"
63,How to show that planetary orbits are conic sections,How to show that planetary orbits are conic sections,,"It's all very fun to use differential equations to show that the path of a particle follows a parabola under constant acceleration, a circle when the angular momentum is just enough to allow for zero radial acceleration and an ellipse when it is in a general orbit. (still haven't done the hyperbola). But a question that has always bugged me is why do these objects all follow conic sections under gravity? Intuitively, it seems like the inverse square law in polar coordinates is responsible. But an object accelerating under constant force still takes on a parabola. Is this related to the fact that angular momentum is constant under gravity due to zero torque? (Kepler's second law)","It's all very fun to use differential equations to show that the path of a particle follows a parabola under constant acceleration, a circle when the angular momentum is just enough to allow for zero radial acceleration and an ellipse when it is in a general orbit. (still haven't done the hyperbola). But a question that has always bugged me is why do these objects all follow conic sections under gravity? Intuitively, it seems like the inverse square law in polar coordinates is responsible. But an object accelerating under constant force still takes on a parabola. Is this related to the fact that angular momentum is constant under gravity due to zero torque? (Kepler's second law)",,"['differential-geometry', 'algebraic-geometry', 'conic-sections', 'mathematical-physics']"
64,Why are differential forms built up on alternating forms?,Why are differential forms built up on alternating forms?,,What properties make alternating forms so desirable in differential geometry?,What properties make alternating forms so desirable in differential geometry?,,"['differential-geometry', 'differential-forms']"
65,Why the codifferential depends only on the connection (and not the metric)?,Why the codifferential depends only on the connection (and not the metric)?,,"Let $E$ be a vector bundle over a smooth Riemannian manifold $(M,g)$. Suppose $E$ is equipped with a metric $\eta$ and a metric connection $\nabla$. Denote by $\Omega^k(M,E)$ the space of $E$-valued forms of degree $k$. $\nabla$ induces a covariant exterior derivative $d:\Omega^k(M,E) \to \Omega^{k+1}(M,E)$, which (together with the metrics $g,\eta$) induce the codifferential operator: $\delta:\Omega^k(M,E) \to \Omega^{k-1}(M,E)$. Even though the metric $\eta$ on $E$ plays a part in the definition of $\delta$, it turns out that $\delta$ is actually independent of it, and depends only on the connection $\nabla$ (there is an implicit dependence since $\nabla$ is required to be metric). Indeed, one formula for $\delta$ is $\delta =\star  d \star$ (up to sign), where $\star$ is the Hodge-dual $\Omega^k(M,E) \to \Omega^{d-k}(M,E)$ which is defined without any reference to the metric on $E$. Is there any way to see why the codifferential depends only on the connection $\nabla$ and the metric on $M$, and not on the metric on $E$? Of course, the derivation of the formula above for $\delta$  shows this, but I would like to find an argument without relying on this computation. (I am looking for a more ""conceptual explanation""). Note that it's easy to see from the definition of $\delta$ that it's invariant under scaling of the metric on $E$, however I do not see immediately why it's completely independent of it.","Let $E$ be a vector bundle over a smooth Riemannian manifold $(M,g)$. Suppose $E$ is equipped with a metric $\eta$ and a metric connection $\nabla$. Denote by $\Omega^k(M,E)$ the space of $E$-valued forms of degree $k$. $\nabla$ induces a covariant exterior derivative $d:\Omega^k(M,E) \to \Omega^{k+1}(M,E)$, which (together with the metrics $g,\eta$) induce the codifferential operator: $\delta:\Omega^k(M,E) \to \Omega^{k-1}(M,E)$. Even though the metric $\eta$ on $E$ plays a part in the definition of $\delta$, it turns out that $\delta$ is actually independent of it, and depends only on the connection $\nabla$ (there is an implicit dependence since $\nabla$ is required to be metric). Indeed, one formula for $\delta$ is $\delta =\star  d \star$ (up to sign), where $\star$ is the Hodge-dual $\Omega^k(M,E) \to \Omega^{d-k}(M,E)$ which is defined without any reference to the metric on $E$. Is there any way to see why the codifferential depends only on the connection $\nabla$ and the metric on $M$, and not on the metric on $E$? Of course, the derivation of the formula above for $\delta$  shows this, but I would like to find an argument without relying on this computation. (I am looking for a more ""conceptual explanation""). Note that it's easy to see from the definition of $\delta$ that it's invariant under scaling of the metric on $E$, however I do not see immediately why it's completely independent of it.",,"['differential-geometry', 'riemannian-geometry', 'differential-forms', 'vector-bundles', 'connections']"
66,$k$-jet transitivity of diffeomorphism group,-jet transitivity of diffeomorphism group,k,"Given a connected smooth manifold $M$ and an invertible jet $\xi \in {\rm inv} J^k_p(M,M)_q$, what are the required conditions for the existence of a diffeomorphism $\phi \in {\rm Diff}(M)$ such that $j^k_p \phi = \xi$? What about if we want $\phi$ to be isotopic to the identity? Given a smooth family $\xi_\epsilon \in {\rm inv} J^k_p(M,M)_q$ can we get a smooth family $\phi_\epsilon \in {\rm Diff}_0 (M)$? It's clear that we can find such a diffeomorphism from a neighbourhood of $p$ to a neighbourhood of $q$, and also that there must be some smooth map $M \to M$ contacting $\xi$, but I have no idea how to extend the former or make the latter a diffeomorphism. At least in low dimension it feels like the only condition should be that $\xi$ is orientation-preserving, but I have no idea how to establish this.","Given a connected smooth manifold $M$ and an invertible jet $\xi \in {\rm inv} J^k_p(M,M)_q$, what are the required conditions for the existence of a diffeomorphism $\phi \in {\rm Diff}(M)$ such that $j^k_p \phi = \xi$? What about if we want $\phi$ to be isotopic to the identity? Given a smooth family $\xi_\epsilon \in {\rm inv} J^k_p(M,M)_q$ can we get a smooth family $\phi_\epsilon \in {\rm Diff}_0 (M)$? It's clear that we can find such a diffeomorphism from a neighbourhood of $p$ to a neighbourhood of $q$, and also that there must be some smooth map $M \to M$ contacting $\xi$, but I have no idea how to extend the former or make the latter a diffeomorphism. At least in low dimension it feels like the only condition should be that $\xi$ is orientation-preserving, but I have no idea how to establish this.",,"['differential-geometry', 'differential-topology', 'smooth-manifolds', 'jet-bundles']"
67,Closed 1-form on a Riemann surface,Closed 1-form on a Riemann surface,,"Let $R$ be a Riemann surface of genus $g\ge 2$ and $p\in R$ a point. I need to find a way to produce a closed 1-form $\omega$ on $R$ which satisfies the following conditions: $\omega$ is not exact, not holomorphic and not anti-holomorphic $\omega$ is zero in $p$ $\int_R\omega\wedge \overline{\omega}>0$ Can you help me? The answer of this question Constructing one-forms on a Riemann surface using the uniformization theorem suggests to use the uniformization theorem, but they do it for meromorphic quadratic forms and I don't know how to apply it to my case.","Let $R$ be a Riemann surface of genus $g\ge 2$ and $p\in R$ a point. I need to find a way to produce a closed 1-form $\omega$ on $R$ which satisfies the following conditions: $\omega$ is not exact, not holomorphic and not anti-holomorphic $\omega$ is zero in $p$ $\int_R\omega\wedge \overline{\omega}>0$ Can you help me? The answer of this question Constructing one-forms on a Riemann surface using the uniformization theorem suggests to use the uniformization theorem, but they do it for meromorphic quadratic forms and I don't know how to apply it to my case.",,"['differential-geometry', 'differential-forms', 'complex-geometry', 'riemann-surfaces']"
68,Triangles on a Torus,Triangles on a Torus,,"This is a really basic question, which draws as its source two of the pictures from the Wikipedia article about Gaussian curvature . If it is true that the sum of the angles of a triangle on a surface of negative Gaussian curvature is less than 180 degrees (as it says on Wikipedia), and that the sum of the angles of a triangle on a surface of positive Gaussian curvature is more than 180 degrees (which I believe is the case for a sphere, I think the sum is 270 degrees), then: Since the inside of a torus supposedly has negative Gaussian curvature, and the outside supposedly has positive Gaussian curvature, does a triangle inscribed on the inside of a torus have a sum of angles less than 180 degrees while a triangle inscribed on the outside of a torus have a sum of angles greater than 180 degrees? By ""inside"" I mean ""closer to the donut hole"" and by ""outside"" I mean ""away from the donut hole"". Thus an ""ant walking on a torus"" could tell precisely when it arrived ""at the highest point"" of the torus (the circle on the torus where every point has zero Gaussian curvature which divides the above two mentioned regions of positive and negative curvature) when the triangles it is drawing on the ground have a sum of angles of precisely 180 degrees? Also does this mean that the torus essentially has both elliptic and hyperbolic geometries, depending on which side the ant is walking on? I was thinking about this question because I was trying to think of examples of closed surfaces which have negative Gaussian curvature over an extended region, because it is not possible for the surface to be closed and have negative curvature everywhere and be embeddable in $\mathbb{R}^3$ , see: existence of closed surface having only negative Gaussian curvature. https://mathoverflow.net/questions/111101/surfaces-in-mathbb-r3-with-negative-curvature-bounded-away-from-zero https://mathoverflow.net/questions/32597/compact-surfaces-of-negative-curvature","This is a really basic question, which draws as its source two of the pictures from the Wikipedia article about Gaussian curvature . If it is true that the sum of the angles of a triangle on a surface of negative Gaussian curvature is less than 180 degrees (as it says on Wikipedia), and that the sum of the angles of a triangle on a surface of positive Gaussian curvature is more than 180 degrees (which I believe is the case for a sphere, I think the sum is 270 degrees), then: Since the inside of a torus supposedly has negative Gaussian curvature, and the outside supposedly has positive Gaussian curvature, does a triangle inscribed on the inside of a torus have a sum of angles less than 180 degrees while a triangle inscribed on the outside of a torus have a sum of angles greater than 180 degrees? By ""inside"" I mean ""closer to the donut hole"" and by ""outside"" I mean ""away from the donut hole"". Thus an ""ant walking on a torus"" could tell precisely when it arrived ""at the highest point"" of the torus (the circle on the torus where every point has zero Gaussian curvature which divides the above two mentioned regions of positive and negative curvature) when the triangles it is drawing on the ground have a sum of angles of precisely 180 degrees? Also does this mean that the torus essentially has both elliptic and hyperbolic geometries, depending on which side the ant is walking on? I was thinking about this question because I was trying to think of examples of closed surfaces which have negative Gaussian curvature over an extended region, because it is not possible for the surface to be closed and have negative curvature everywhere and be embeddable in , see: existence of closed surface having only negative Gaussian curvature. https://mathoverflow.net/questions/111101/surfaces-in-mathbb-r3-with-negative-curvature-bounded-away-from-zero https://mathoverflow.net/questions/32597/compact-surfaces-of-negative-curvature",\mathbb{R}^3,"['differential-geometry', 'soft-question']"
69,Is the flow of an analytic vector field also analytic?,Is the flow of an analytic vector field also analytic?,,Let $X$ be an analytic vector field on a smooth manifold. Is it true that the flow $\Phi_t:M\to M$ associated to that vector field is also analytic?,Let $X$ be an analytic vector field on a smooth manifold. Is it true that the flow $\Phi_t:M\to M$ associated to that vector field is also analytic?,,"['differential-geometry', 'analytic-geometry', 'analytic-functions']"
70,Left-invariant vector fields on the circle $S^1$,Left-invariant vector fields on the circle,S^1,"I'm trying to find the left-invariant vector fields on the circle $S^1$. If I understand correctly, $S^1$ is given the group structure of the multiplicative group of complex numbers on the unit circle in the complex plane, and $S^1$ is made into a $C^\infty$ manifold by forming the maximal atlas from the four projection maps from open semi-circles of $S^1$ onto the $x$- and $y$-axes. Given a point $p\in S^1$, if $p$ is on the upper or lower open semicircles, then a basis for $T_pS^1$ is given by $\{\partial/\partial \bar x \rvert_p\}$, where $\bar x$ is the projection onto the $x$-axis, while if $p$ is on the left or right open semicircles, then a basis for $T_pS^1$ is given by $\{\partial/\partial \bar y \rvert_p\}$. Given a vector field $X$ on $S^1$, $X$ is left-invariant provided that for all points $g,h\in S^1$, $$ (\ell_g)_{*,h}(X_h) = X_{gh} \text, $$ where $\ell_g\colon S^1 \to S^1$ is left-multiplication by $g$. To obtain an arbitrary left-invariant vector field $X$ on $S^1$, I should be able to just pick an arbitrary tangent vector $A \in T_1S^1$ and see what it generates. Let $A = a(\partial/\partial \bar y\rvert_1) \in T_1S^1$. Then given a point $p\in S^1$, I can define $$ X_p = (\ell_p)_{*,1}(A). $$ However, I don't see how this is getting me any closer to concretely writing down what $X$ looks like. Any suggestions?","I'm trying to find the left-invariant vector fields on the circle $S^1$. If I understand correctly, $S^1$ is given the group structure of the multiplicative group of complex numbers on the unit circle in the complex plane, and $S^1$ is made into a $C^\infty$ manifold by forming the maximal atlas from the four projection maps from open semi-circles of $S^1$ onto the $x$- and $y$-axes. Given a point $p\in S^1$, if $p$ is on the upper or lower open semicircles, then a basis for $T_pS^1$ is given by $\{\partial/\partial \bar x \rvert_p\}$, where $\bar x$ is the projection onto the $x$-axis, while if $p$ is on the left or right open semicircles, then a basis for $T_pS^1$ is given by $\{\partial/\partial \bar y \rvert_p\}$. Given a vector field $X$ on $S^1$, $X$ is left-invariant provided that for all points $g,h\in S^1$, $$ (\ell_g)_{*,h}(X_h) = X_{gh} \text, $$ where $\ell_g\colon S^1 \to S^1$ is left-multiplication by $g$. To obtain an arbitrary left-invariant vector field $X$ on $S^1$, I should be able to just pick an arbitrary tangent vector $A \in T_1S^1$ and see what it generates. Let $A = a(\partial/\partial \bar y\rvert_1) \in T_1S^1$. Then given a point $p\in S^1$, I can define $$ X_p = (\ell_p)_{*,1}(A). $$ However, I don't see how this is getting me any closer to concretely writing down what $X$ looks like. Any suggestions?",,"['differential-geometry', 'manifolds', 'lie-groups', 'smooth-manifolds']"
71,Interpretation of $d\phi(z)$ in differential geometry,Interpretation of  in differential geometry,d\phi(z),"In ""Exercises and Solutions in Mathematics"", Ta-Tsien, 2nd Edition, exercise 3343. Statement of the exercise Let $(\mathbb{H}, g)$ be the two-dimensional hyperbolic space, where \begin{equation} \mathbb{H} = \{(x, y) \in \mathbb{R}^2 : y > 0\} \end{equation} is the upper half plane of $\mathbb{R}^2 = \mathbb{C}$ and the metric $g$ is given by \begin{equation} g = \frac{dx^2 + dy^2}{y^2} \end{equation} Suppose $a$, $b$, $c$ and $d$ are real numbers such that $ad - bc = 1$. Define \begin{equation} \phi(z) = \frac{az + b}{cz + d} \end{equation} for any $z = x + \sqrt{-1} y$. Prove that $\phi$ is an isometry for $(\mathbb{H}^2, g)$. Statement of the answer To prove that $\phi$ is an isometry, the authors compute: \begin{equation} d\phi = \frac{a(dz)(cz+d) - c(dz)(az+b)}{(cz + d)^2} \end{equation} and after some computations, concludes that since: \begin{equation} \Vert d\phi(z) \Vert^2 = \frac{d\phi(z) d\overline{\phi(z)}}{[Im \phi(z)]^2} = \frac{dx^2 + dy^2}{y^2} = \Vert dz \Vert^2 \end{equation} then $\phi$ is an isometry. My question What is the mathematical nature of the operator $d$ in this context ? It seems to me that in order to prove that $\phi$ is an isometry, one has to prove that: $g = \phi^* g$ i.e. that the pullback of $g$ by $\phi$ is $g$. In the context of differential geometry, I have only seen $d\phi$ standing for the exterior derivative of $\phi$ or for the differential map associated to $\phi$. In a more ""intuitive"" manner, considering small variations of $\phi(z)$ and $z$, I understand that an isomorphism maps a small increment $dz$ to a small increment $d\phi(z)$. But I would like to understand the differential geometry meaning. Is $dz$ a differential form in the context of this exercise ? Then what is the precise meaning of $\Vert dz \Vert^2$ ? Is $d\phi(z)$ of the same nature than $dz$ ? Is it a vector-valued differential form ?","In ""Exercises and Solutions in Mathematics"", Ta-Tsien, 2nd Edition, exercise 3343. Statement of the exercise Let $(\mathbb{H}, g)$ be the two-dimensional hyperbolic space, where \begin{equation} \mathbb{H} = \{(x, y) \in \mathbb{R}^2 : y > 0\} \end{equation} is the upper half plane of $\mathbb{R}^2 = \mathbb{C}$ and the metric $g$ is given by \begin{equation} g = \frac{dx^2 + dy^2}{y^2} \end{equation} Suppose $a$, $b$, $c$ and $d$ are real numbers such that $ad - bc = 1$. Define \begin{equation} \phi(z) = \frac{az + b}{cz + d} \end{equation} for any $z = x + \sqrt{-1} y$. Prove that $\phi$ is an isometry for $(\mathbb{H}^2, g)$. Statement of the answer To prove that $\phi$ is an isometry, the authors compute: \begin{equation} d\phi = \frac{a(dz)(cz+d) - c(dz)(az+b)}{(cz + d)^2} \end{equation} and after some computations, concludes that since: \begin{equation} \Vert d\phi(z) \Vert^2 = \frac{d\phi(z) d\overline{\phi(z)}}{[Im \phi(z)]^2} = \frac{dx^2 + dy^2}{y^2} = \Vert dz \Vert^2 \end{equation} then $\phi$ is an isometry. My question What is the mathematical nature of the operator $d$ in this context ? It seems to me that in order to prove that $\phi$ is an isometry, one has to prove that: $g = \phi^* g$ i.e. that the pullback of $g$ by $\phi$ is $g$. In the context of differential geometry, I have only seen $d\phi$ standing for the exterior derivative of $\phi$ or for the differential map associated to $\phi$. In a more ""intuitive"" manner, considering small variations of $\phi(z)$ and $z$, I understand that an isomorphism maps a small increment $dz$ to a small increment $d\phi(z)$. But I would like to understand the differential geometry meaning. Is $dz$ a differential form in the context of this exercise ? Then what is the precise meaning of $\Vert dz \Vert^2$ ? Is $d\phi(z)$ of the same nature than $dz$ ? Is it a vector-valued differential form ?",,"['differential-geometry', 'differential-forms', 'infinitesimals']"
72,Intrinsic Riemannian distance vs. Euclidean distance,Intrinsic Riemannian distance vs. Euclidean distance,,"The intrinsic metric $ d_i(x,y) $ on a Riemannian manifold is the infimum over all curves joining $ x $ and $ y $, of the arc length of these curves. For a surface $ S $ embedded in $ \mathbb{R}^3 $, we also have the usual Euclidean metric $ d_e $ induced on $ S $. It's clear that $ d_e(x,y) \leq d_i(x,y) $ so that if $ p_n $ converges to $ p $ in the intrinsic distance, we get convergence in the Euclidean distance. It's an exercise in do Carmo to prove that these metrics induce the same topology, but I can't see how to get the reverse implication; why does convergence in Euclidean distance imply convergence in intrinsic distance? I know that length $ \alpha = \int_0^1 {\alpha'(t)} \; dt $ but I can't seem to relate the norm of $ p_n-p $ to the norm of $ \alpha'(t) $ where $ \alpha $ is a curve joining $ p_n $ to $ p $.","The intrinsic metric $ d_i(x,y) $ on a Riemannian manifold is the infimum over all curves joining $ x $ and $ y $, of the arc length of these curves. For a surface $ S $ embedded in $ \mathbb{R}^3 $, we also have the usual Euclidean metric $ d_e $ induced on $ S $. It's clear that $ d_e(x,y) \leq d_i(x,y) $ so that if $ p_n $ converges to $ p $ in the intrinsic distance, we get convergence in the Euclidean distance. It's an exercise in do Carmo to prove that these metrics induce the same topology, but I can't see how to get the reverse implication; why does convergence in Euclidean distance imply convergence in intrinsic distance? I know that length $ \alpha = \int_0^1 {\alpha'(t)} \; dt $ but I can't seem to relate the norm of $ p_n-p $ to the norm of $ \alpha'(t) $ where $ \alpha $ is a curve joining $ p_n $ to $ p $.",,"['differential-geometry', 'riemannian-geometry']"
73,The complex structure of a complex torus,The complex structure of a complex torus,,"A complex torus $X$ of complex dimension $1$ is just a quotient of $\mathbb{C}$ by a lattice $\Lambda=\mathbb{Z}\oplus \tau\mathbb{Z}$ where $\tau=a+ib$ is a complex number with $b>0$. The complex structure is determined by this $\tau$. I want to undestand this complex structure as an endomorphism $J:TX \to TX$ such that $J^2=-1$ (an almost complex structure). I feel like, regarding X as a Lie group, it is a invariant complex structure. If so, $J$ is determined by a $2 \times 2$ matrix. Now, for a $2\times 2$ matrix $J$ satisfy $J^2=-1$, it must be something like $$J=\left(\begin{array}{cc} \frac{\alpha}{\beta} & \frac{1}{\beta}\\ -\frac{\alpha^2}{\beta}-\beta & -\frac{\alpha}{\beta} \end{array}\right). $$ What are the relations between $\tau=a+ib$ and the coefficients $\alpha, \beta$?","A complex torus $X$ of complex dimension $1$ is just a quotient of $\mathbb{C}$ by a lattice $\Lambda=\mathbb{Z}\oplus \tau\mathbb{Z}$ where $\tau=a+ib$ is a complex number with $b>0$. The complex structure is determined by this $\tau$. I want to undestand this complex structure as an endomorphism $J:TX \to TX$ such that $J^2=-1$ (an almost complex structure). I feel like, regarding X as a Lie group, it is a invariant complex structure. If so, $J$ is determined by a $2 \times 2$ matrix. Now, for a $2\times 2$ matrix $J$ satisfy $J^2=-1$, it must be something like $$J=\left(\begin{array}{cc} \frac{\alpha}{\beta} & \frac{1}{\beta}\\ -\frac{\alpha^2}{\beta}-\beta & -\frac{\alpha}{\beta} \end{array}\right). $$ What are the relations between $\tau=a+ib$ and the coefficients $\alpha, \beta$?",,"['differential-geometry', 'lie-groups', 'complex-geometry']"
74,Tangent space manifold,Tangent space manifold,,"Let M be a differentiable manifold of dimension m and also let $\{\xi_1,\dots,\xi_m\}\subset \text{T}_pM$ be an linearly independent set of the tangent bundle of M at a certain point $p\in M$. I have to proof that there is a chart $(\varphi=(u_1,\dots,u_m), U)$ with $\varphi(p)=0$ and:$$ \xi_i =\left( \frac{d}{du_i}\right)_p$$ -My approach: Let $(\varphi, U)$ be a chart with $\varphi(p)=0$, there is an unique linear endomorphism $F$ of $\text{T}_pM$ that sends (since both sets are bases), $$\left( \frac{d}{du_i}\right)_p\longrightarrow \xi_i$$ from this point I supose you should be able to define a diffeomorphism $f:M\rightarrow M$ with $df(p)=F$ wich would end the argument by taking the chart $(\varphi \circ f,U)$. I really don't know how to define such a map or if it even exists.","Let M be a differentiable manifold of dimension m and also let $\{\xi_1,\dots,\xi_m\}\subset \text{T}_pM$ be an linearly independent set of the tangent bundle of M at a certain point $p\in M$. I have to proof that there is a chart $(\varphi=(u_1,\dots,u_m), U)$ with $\varphi(p)=0$ and:$$ \xi_i =\left( \frac{d}{du_i}\right)_p$$ -My approach: Let $(\varphi, U)$ be a chart with $\varphi(p)=0$, there is an unique linear endomorphism $F$ of $\text{T}_pM$ that sends (since both sets are bases), $$\left( \frac{d}{du_i}\right)_p\longrightarrow \xi_i$$ from this point I supose you should be able to define a diffeomorphism $f:M\rightarrow M$ with $df(p)=F$ wich would end the argument by taking the chart $(\varphi \circ f,U)$. I really don't know how to define such a map or if it even exists.",,"['differential-geometry', 'manifolds', 'smooth-manifolds']"
75,Confused (disoriented?) by questions about orientation,Confused (disoriented?) by questions about orientation,,"I believe I have a reasonable basic understanding of orientation. Yet, i'm finding myself utterly confused when facing a specific question. Here are several examples: Exhibit an ordered basis of positive orientation for $S^2$ (as a boundary of $B^3$) at an arbitrary point point $p=(a,b,c)$. Let $f: S^2 \to (-1,1)$, be gicen by $f:(x,y,z) \mapsto z$. Exhibit an ordered basis of positive orientation for a typical point   on $f^{-1}(t)$. Show that the boundary orientation of $S^k = \partial B^{k+1}$ is the same as its preimage orientation given by: $$g:\mathbb{R}^{k+1} \to \mathbb{R}, g(x)= |x|^2$$ (The questions are taken from the book ""differential topology"" by Guillemin and Pollack). My issues: I don't quite understand how an answer is supposed to look like. Couldn't i just say $\{v_p,w_p\}$ is a basis for $T_p S^2$ so (if $n_p$ is the outward pointing normal) according to whether $sign(n_p,v_p,w_p)$ is positive or negative i can switch $v_p$ and $w_p$ and obtain a positive orientation? Same problem with (1). Only now there's another issue. I'm not sure what's the correct formal way of computing an orientation of a preimage like $f^{-1}(t)$. Would it be right to look at $(T_p f o \pi_p)^{-1}(1)$ where $\pi_p: T_p S^2 \to T_p (f^{-1}(t))$ is the projection? Same as (2), plus some general unfocused non-specific confusion. I must say that I didn't have any problems with the chapter itself, everything was done in a coordinate-free way that didn't raise these problems. Thanks for the help. EDIT: In the book an orinetation is defined as a continuous choice of equivalence class on the tangent space at each point where the relation is: $$v \sim w \iff Av=w \text{ for some $A$ with } |A| > 0$$ I do feel comfortable with the definition of an atlas with positive determinant transition functions as well.","I believe I have a reasonable basic understanding of orientation. Yet, i'm finding myself utterly confused when facing a specific question. Here are several examples: Exhibit an ordered basis of positive orientation for $S^2$ (as a boundary of $B^3$) at an arbitrary point point $p=(a,b,c)$. Let $f: S^2 \to (-1,1)$, be gicen by $f:(x,y,z) \mapsto z$. Exhibit an ordered basis of positive orientation for a typical point   on $f^{-1}(t)$. Show that the boundary orientation of $S^k = \partial B^{k+1}$ is the same as its preimage orientation given by: $$g:\mathbb{R}^{k+1} \to \mathbb{R}, g(x)= |x|^2$$ (The questions are taken from the book ""differential topology"" by Guillemin and Pollack). My issues: I don't quite understand how an answer is supposed to look like. Couldn't i just say $\{v_p,w_p\}$ is a basis for $T_p S^2$ so (if $n_p$ is the outward pointing normal) according to whether $sign(n_p,v_p,w_p)$ is positive or negative i can switch $v_p$ and $w_p$ and obtain a positive orientation? Same problem with (1). Only now there's another issue. I'm not sure what's the correct formal way of computing an orientation of a preimage like $f^{-1}(t)$. Would it be right to look at $(T_p f o \pi_p)^{-1}(1)$ where $\pi_p: T_p S^2 \to T_p (f^{-1}(t))$ is the projection? Same as (2), plus some general unfocused non-specific confusion. I must say that I didn't have any problems with the chapter itself, everything was done in a coordinate-free way that didn't raise these problems. Thanks for the help. EDIT: In the book an orinetation is defined as a continuous choice of equivalence class on the tangent space at each point where the relation is: $$v \sim w \iff Av=w \text{ for some $A$ with } |A| > 0$$ I do feel comfortable with the definition of an atlas with positive determinant transition functions as well.",,"['differential-geometry', 'manifolds', 'differential-topology', 'orientation']"
76,Coordinate systems on manifolds,Coordinate systems on manifolds,,"I am fairly new to differential geometry and something I can't get my head around is, if an $n$-dimensional manifold is locally homeomorphic to $\mathbb{R}^{n}$, i.e. Euclidean space, then isn't it possible to cover any manifold with a collection of coordinate charts whose coordinates are just the usual Cartesian coordinates of Euclidean space? Why does one need to even consider more general, cuvilinear coordinate systems, other than that they may simplify the problem at hand? For example, the 2-sphere $S^{2}$ can be locally described (perhaps most easily) by spherical polar coordinates  $(\theta , \phi)$ that can be mapped to local Cartesian coordinates, $x^{1}=\sin (\theta)\cos (\phi),\; x^{2}=\sin (\theta)\sin (\phi),\; x^{3}=\cos (\theta)$. Couldn't one equally just start from the definition of $S^{2}=\lbrace (x^{1},x^{2},x^{3})\in\mathbb{R}^{3}\;\vert\; (x^{1})^{2}+(x^{2})^{2}+(x^{3})^{2}=1\rbrace$ and just use Cartesian coordinates (forgoing curvilinear coordinates altogether)? However, I have read that, in general, curved manifolds cannot be described even locally by Cartesian coordinates. I'm confused how this is the case when supposedly all manifolds are locally homeomorphic to Euclidean space?","I am fairly new to differential geometry and something I can't get my head around is, if an $n$-dimensional manifold is locally homeomorphic to $\mathbb{R}^{n}$, i.e. Euclidean space, then isn't it possible to cover any manifold with a collection of coordinate charts whose coordinates are just the usual Cartesian coordinates of Euclidean space? Why does one need to even consider more general, cuvilinear coordinate systems, other than that they may simplify the problem at hand? For example, the 2-sphere $S^{2}$ can be locally described (perhaps most easily) by spherical polar coordinates  $(\theta , \phi)$ that can be mapped to local Cartesian coordinates, $x^{1}=\sin (\theta)\cos (\phi),\; x^{2}=\sin (\theta)\sin (\phi),\; x^{3}=\cos (\theta)$. Couldn't one equally just start from the definition of $S^{2}=\lbrace (x^{1},x^{2},x^{3})\in\mathbb{R}^{3}\;\vert\; (x^{1})^{2}+(x^{2})^{2}+(x^{3})^{2}=1\rbrace$ and just use Cartesian coordinates (forgoing curvilinear coordinates altogether)? However, I have read that, in general, curved manifolds cannot be described even locally by Cartesian coordinates. I'm confused how this is the case when supposedly all manifolds are locally homeomorphic to Euclidean space?",,"['differential-geometry', 'manifolds', 'coordinate-systems']"
77,maximal linear subspaces contained in the cone over the Clifford torus.,maximal linear subspaces contained in the cone over the Clifford torus.,,"Forgot: this is about Find a subspace of $\mathbb{R}^4$ for which $x^T*A*x$ = 0 I was a little surprised to find that, in the cone $x^2 + y^2 = z^2 + w^2$ in $\mathbb R^4,$ there are infinitely many 2-planes passing through the origin and completely contained in the cone. Indeed, take any real vector $(A,B,C,D)$ with $A^2 + B^2 = C^2 + D^2,$ we can make a 2-plne in the cone from the linear span of $$ (A,B,C,D) \; \; \; \rm{and} \; \; (-B,A,-D,C).  $$ We get a different 2-plane (I think) from the sapn of $$ (A,B,C,D) \; \; \; \rm{and} \; \; (-B,A,D,-C).  $$ I also think that is it, for each nonzero vector in the cone, two 2-planes containing it. So far, I do not see anything in my many quadratic forms books that predicts this, but I could be looking in the wrong places. I would like confirmation of all that. The cone over the Clifford torus is an important object in the differential geometry of minimal and constant mean curvature submanifolds. An early reference is Blaine Lawson (1970) in the Annals. Meanwhile, given positive integers $(p,q)$ and the cone in $\mathbb R^{p+q}$ given by $$ x_1^2 + \cdots + x_p^2 \; = \;   x_{p+1}^2 + \cdots + x_{p+q}^2,   $$ what is the highest dimension of a linear subspace (through the origin) that is entirely contained in the cone? Finally, is there some finiteness result such as I got above, given this many independent vectors in the cone, these complete to a maximal linear subspace in exactly two(?) ways? A few hours later: managed to relate this to something familiar. In $\mathbb R^3,$ the hyperboloid of one sheet $x^2 + y^2 = z^2 + 1$ is doubly ruled, two families of straight lines. If we intersect the cone $x^2 + y^2 = z^2 + w^2$ with the 3-plane $w=1,$ we get that doubly ruled hyperboloid. Furthermore, the original cone contains the cone over each of those straight lines, giving two $2$-planes for each point in the hyperboloid. http://en.wikipedia.org/wiki/Hyperboloid and http://en.wikipedia.org/wiki/Ruled_surface","Forgot: this is about Find a subspace of $\mathbb{R}^4$ for which $x^T*A*x$ = 0 I was a little surprised to find that, in the cone $x^2 + y^2 = z^2 + w^2$ in $\mathbb R^4,$ there are infinitely many 2-planes passing through the origin and completely contained in the cone. Indeed, take any real vector $(A,B,C,D)$ with $A^2 + B^2 = C^2 + D^2,$ we can make a 2-plne in the cone from the linear span of $$ (A,B,C,D) \; \; \; \rm{and} \; \; (-B,A,-D,C).  $$ We get a different 2-plane (I think) from the sapn of $$ (A,B,C,D) \; \; \; \rm{and} \; \; (-B,A,D,-C).  $$ I also think that is it, for each nonzero vector in the cone, two 2-planes containing it. So far, I do not see anything in my many quadratic forms books that predicts this, but I could be looking in the wrong places. I would like confirmation of all that. The cone over the Clifford torus is an important object in the differential geometry of minimal and constant mean curvature submanifolds. An early reference is Blaine Lawson (1970) in the Annals. Meanwhile, given positive integers $(p,q)$ and the cone in $\mathbb R^{p+q}$ given by $$ x_1^2 + \cdots + x_p^2 \; = \;   x_{p+1}^2 + \cdots + x_{p+q}^2,   $$ what is the highest dimension of a linear subspace (through the origin) that is entirely contained in the cone? Finally, is there some finiteness result such as I got above, given this many independent vectors in the cone, these complete to a maximal linear subspace in exactly two(?) ways? A few hours later: managed to relate this to something familiar. In $\mathbb R^3,$ the hyperboloid of one sheet $x^2 + y^2 = z^2 + 1$ is doubly ruled, two families of straight lines. If we intersect the cone $x^2 + y^2 = z^2 + w^2$ with the 3-plane $w=1,$ we get that doubly ruled hyperboloid. Furthermore, the original cone contains the cone over each of those straight lines, giving two $2$-planes for each point in the hyperboloid. http://en.wikipedia.org/wiki/Hyperboloid and http://en.wikipedia.org/wiki/Ruled_surface",,"['algebraic-geometry', 'differential-geometry', 'quadratic-forms']"
78,Real Manifold ... Complex Coordinates?,Real Manifold ... Complex Coordinates?,,"I'm working in an earlier edition of John Lee's book on smooth manifolds , and he has a number of problems where he represents a real manifold using complex variables.  For instance in chapter 3 problem 5: Consider $\mathbb{S}^3$ as a subset of $\mathbb{C}^2$ under the usual identification of $\mathbb{C}^2$ with $\mathbb{R}^4$.  For each $z = (z^1, z^2) \in \mathbb{S}^3$ define a curve $\gamma_z: \mathbb{R} \to \mathbb{S}^3$ by $$ \gamma_z(t) = (e^{it}z^1, e^{it}z^2).$$ He subsequently asks for us to compute the coordinate representation of $\gamma_z(t)$ using stereographic projection and then also compute $\gamma_z'(t)$ under the same coordinate transformation.  I'm really confused about to do this using complex variables when we're working in a real manifold.  Out of stubbornness I computed what I thought would be the real representation of $\gamma_z(t)$: $$ \gamma_x(t) \;\; =\;\; (x^1 \cos t - x^2 \sin t, x^1 \sin t + x^2 \cos t, x^3 \cos t - x^4 \sin t, x^3 \sin t + x^4 \cos t) $$ but this can't possibly be right.  Letting $\sigma:\mathbb{S}^3/\{N\} \to \mathbb{R}^3$ be the stereographic projection from the neighborhood omitting the point $N = (0,0,0,1)$, we then obtain $$ (\sigma\circ \gamma_x)(t) \;\; =\;\; \frac{(x^1 \cos t - x^2 \sin t, x^1 \sin t + x^2 \cos t, x^3 \cos t - x^4 \sin t)}{1 - x^3\sin t - x^4 \cos t} $$ which can't possibly be right since it's not defined at the point $(0,0,1,0)$ for all $t \in \mathbb{R}$. I'm lost as to how to tackle this kind of problem using complex variables.  Thanks in advance! P.S. For reference, my copy is a Chinese edition but I don't think there's any substantive difference between my copy and the one posted in the link.  I definitely do not have the second edition.","I'm working in an earlier edition of John Lee's book on smooth manifolds , and he has a number of problems where he represents a real manifold using complex variables.  For instance in chapter 3 problem 5: Consider $\mathbb{S}^3$ as a subset of $\mathbb{C}^2$ under the usual identification of $\mathbb{C}^2$ with $\mathbb{R}^4$.  For each $z = (z^1, z^2) \in \mathbb{S}^3$ define a curve $\gamma_z: \mathbb{R} \to \mathbb{S}^3$ by $$ \gamma_z(t) = (e^{it}z^1, e^{it}z^2).$$ He subsequently asks for us to compute the coordinate representation of $\gamma_z(t)$ using stereographic projection and then also compute $\gamma_z'(t)$ under the same coordinate transformation.  I'm really confused about to do this using complex variables when we're working in a real manifold.  Out of stubbornness I computed what I thought would be the real representation of $\gamma_z(t)$: $$ \gamma_x(t) \;\; =\;\; (x^1 \cos t - x^2 \sin t, x^1 \sin t + x^2 \cos t, x^3 \cos t - x^4 \sin t, x^3 \sin t + x^4 \cos t) $$ but this can't possibly be right.  Letting $\sigma:\mathbb{S}^3/\{N\} \to \mathbb{R}^3$ be the stereographic projection from the neighborhood omitting the point $N = (0,0,0,1)$, we then obtain $$ (\sigma\circ \gamma_x)(t) \;\; =\;\; \frac{(x^1 \cos t - x^2 \sin t, x^1 \sin t + x^2 \cos t, x^3 \cos t - x^4 \sin t)}{1 - x^3\sin t - x^4 \cos t} $$ which can't possibly be right since it's not defined at the point $(0,0,1,0)$ for all $t \in \mathbb{R}$. I'm lost as to how to tackle this kind of problem using complex variables.  Thanks in advance! P.S. For reference, my copy is a Chinese edition but I don't think there's any substantive difference between my copy and the one posted in the link.  I definitely do not have the second edition.",,"['differential-geometry', 'complex-numbers', 'coordinate-systems', 'smooth-manifolds']"
79,Parallel Transport on a Cone,Parallel Transport on a Cone,,"Suppose we have a cone and we wish to parallel transport a vector $w=(0,1,0)$ from  along the curve $\alpha(s)=(\sqrt{2}/2 \cos(v\sqrt{2}),\sqrt{2}/2 \sin(v\sqrt{2}),\sqrt{2}/2)$ from $p=\alpha(0)$ to $q=\alpha(\frac{\pi\sqrt{2}}{4})$ I take polar coordinates on the plane and construct the isometry $F:(\rho,\theta)\in (0,\infty)\times(0,\sqrt{2}\pi) \longrightarrow F(\rho,\theta)=\left(\frac{\sqrt{2}}{2} \rho\cos(\theta\sqrt{2}),\frac{\sqrt{2}}{2} \rho\sin(\theta\sqrt{2}),\rho\frac{\sqrt{2}}{2}\right)$. $F$ is a local isometry between the plane and the cone, so it is equivalent if we do parallel transport on the plane or on the cone. See this image from Do Carmo's Differential Geometry of Curves and Surfaces. So now I know the angle between the parallel transport of $w$ and the $\alpha'(\frac{\pi\sqrt{2}}{4})$. But how do I finish? How do I get the parallel transport of $w$?","Suppose we have a cone and we wish to parallel transport a vector $w=(0,1,0)$ from  along the curve $\alpha(s)=(\sqrt{2}/2 \cos(v\sqrt{2}),\sqrt{2}/2 \sin(v\sqrt{2}),\sqrt{2}/2)$ from $p=\alpha(0)$ to $q=\alpha(\frac{\pi\sqrt{2}}{4})$ I take polar coordinates on the plane and construct the isometry $F:(\rho,\theta)\in (0,\infty)\times(0,\sqrt{2}\pi) \longrightarrow F(\rho,\theta)=\left(\frac{\sqrt{2}}{2} \rho\cos(\theta\sqrt{2}),\frac{\sqrt{2}}{2} \rho\sin(\theta\sqrt{2}),\rho\frac{\sqrt{2}}{2}\right)$. $F$ is a local isometry between the plane and the cone, so it is equivalent if we do parallel transport on the plane or on the cone. See this image from Do Carmo's Differential Geometry of Curves and Surfaces. So now I know the angle between the parallel transport of $w$ and the $\alpha'(\frac{\pi\sqrt{2}}{4})$. But how do I finish? How do I get the parallel transport of $w$?",,"['differential-geometry', 'riemannian-geometry']"
80,"If a Subset Admits a Smooth Structure Which Makes it into a Submanifold, Then it is a Unique One.","If a Subset Admits a Smooth Structure Which Makes it into a Submanifold, Then it is a Unique One.",,"$$ \newcommand{\wh}{\widehat} \newcommand{\R}{\mathbf R} \newcommand{\mr}{\mathscr} \newcommand{\set}[1]{\{#1\}} \newcommand{\inclusion}{\hookrightarrow} \newcommand{\vp}{\varphi} $$ I am trying to understand the following theorem: Theorem. Let $M$ be a smooth manifold and $S$ be a subset of $M$.   There is a unique smooth structure on $S$, if one exists, which makes it into a smooth manifold such that $i:S\inclusion M$ is a smooth embedding. The only way I could prove this is via the lemma proved below whose proof is very long. I think the proof of the above theorem should be fairly straightforward and clear and should not require to do what I have done. Does somebody have a short proof? Lemma. Let $S$ be a $k$-dimensional embedded submanifold of a smooth manifold $M$.   Then for each $p\in S$, there exist a smooth charts $(U_p, \vp_p)$ and $(V_p, \psi_p)$ about $p$ on $S$ and $M$ respectively such that $U_p=V_p\cap S$, and $\psi_p\circ\vp_p^{-1}:\wh U_p\to \wh V_p$ (the `hat' denotes the image of the open set under the corresponding chart)   \begin{equation*} \psi_p\circ\vp_p^{-1}(x_1 , \ldots, x_k) = (x_1 , \ldots, x_k, 0 , \ldots, 0) \end{equation*}   for all $(x_1 , \ldots, x_k)\in \wh U_p$.   Therefore $\vp_p=(\pi\circ \psi_p\circ i)|_{V_p\cap S}$, where $\pi:\R^n\to \R^k$ is the projection on the first $k$ coordinates, and the collection of smooth charts $\mr U=\set{V_p\cap S,\ (\pi\circ \psi_p\circ i)|_{V_p\cap S}}_{p\in S}$ is a smooth atlas on $S$. Proof: Since $i:S\inclusion M$ is an immersion (it's more than that), by the Constant Rank Theorem, we know that there exists a smooth chart $(U,\vp)$ on $S$ containing the point $p$, and a smooth chart $(V,\psi)$ on $M$, again containing $p$, such that $U\subseteq V$ and $\psi\circ\vp^{-1}(x_1,\ldots,x_k)=(x_1,\ldots,x_k,0,\ldots,0)$ for all $(x_1,\ldots,x_k)\in \wh U$. Since $i:S\inclusion M$ is in particular a topological embedding, we know that $U$ is open in $M$ and thus we may WLOG assume that $U=V\cap S$. We now show that $\vp=(\pi\circ \psi\circ i)|_{V\cap S}$. For take $q\in V\cap S$, and say $\vp(q)=(x_1 , \ldots, x_k)$. Now we have $\psi\circ \vp^{-1}(x_1 , \ldots, x_k)=(x_1, , \ldots, x_k, 0 , \ldots, 0)$, giving $(\pi\circ \psi\circ i)(q)=(x_1 , \ldots, x_k)$. So we have shown that for each $p\in S$, there exist a smooth charts $(U_p, \vp_p)$ and $(V_p, \psi_p)$ about $p$ on $S$ and $M$ respectively such that $U_p=V_p\cap S$, and $\psi_p\circ\vp_p^{-1}:\wh U_p\to \wh V_p$ \begin{equation*} \psi_p\circ\vp_p^{-1}(x_1 , \ldots, x_k) = (x_1 , \ldots, x_k, 0 , \ldots, 0) \end{equation*} for all $(x_1 , \ldots, x_k)\in \wh U_p$. It remains to show that $\mr U=\set{U_p, \vp_p}_{p\in S}$ is a smooth atlas on $S$. To see this, consider note that \begin{equation*} \vp_q\circ \vp_p^{-1} = \vp_q\circ \psi_p^{-1} \circ \psi_p\circ \vp_p^{-1} \end{equation*} and this map sends $(x_1 , \ldots, x_k)$ to $\pi\circ\psi_q^{-1}\circ\psi_p(x_1 , \ldots, x_k, 0 , \ldots, 0)$ for all $(x_1 , \ldots, x_k)\in \vp_p(U_p\cap U_q)$, and hence is smooth.","$$ \newcommand{\wh}{\widehat} \newcommand{\R}{\mathbf R} \newcommand{\mr}{\mathscr} \newcommand{\set}[1]{\{#1\}} \newcommand{\inclusion}{\hookrightarrow} \newcommand{\vp}{\varphi} $$ I am trying to understand the following theorem: Theorem. Let $M$ be a smooth manifold and $S$ be a subset of $M$.   There is a unique smooth structure on $S$, if one exists, which makes it into a smooth manifold such that $i:S\inclusion M$ is a smooth embedding. The only way I could prove this is via the lemma proved below whose proof is very long. I think the proof of the above theorem should be fairly straightforward and clear and should not require to do what I have done. Does somebody have a short proof? Lemma. Let $S$ be a $k$-dimensional embedded submanifold of a smooth manifold $M$.   Then for each $p\in S$, there exist a smooth charts $(U_p, \vp_p)$ and $(V_p, \psi_p)$ about $p$ on $S$ and $M$ respectively such that $U_p=V_p\cap S$, and $\psi_p\circ\vp_p^{-1}:\wh U_p\to \wh V_p$ (the `hat' denotes the image of the open set under the corresponding chart)   \begin{equation*} \psi_p\circ\vp_p^{-1}(x_1 , \ldots, x_k) = (x_1 , \ldots, x_k, 0 , \ldots, 0) \end{equation*}   for all $(x_1 , \ldots, x_k)\in \wh U_p$.   Therefore $\vp_p=(\pi\circ \psi_p\circ i)|_{V_p\cap S}$, where $\pi:\R^n\to \R^k$ is the projection on the first $k$ coordinates, and the collection of smooth charts $\mr U=\set{V_p\cap S,\ (\pi\circ \psi_p\circ i)|_{V_p\cap S}}_{p\in S}$ is a smooth atlas on $S$. Proof: Since $i:S\inclusion M$ is an immersion (it's more than that), by the Constant Rank Theorem, we know that there exists a smooth chart $(U,\vp)$ on $S$ containing the point $p$, and a smooth chart $(V,\psi)$ on $M$, again containing $p$, such that $U\subseteq V$ and $\psi\circ\vp^{-1}(x_1,\ldots,x_k)=(x_1,\ldots,x_k,0,\ldots,0)$ for all $(x_1,\ldots,x_k)\in \wh U$. Since $i:S\inclusion M$ is in particular a topological embedding, we know that $U$ is open in $M$ and thus we may WLOG assume that $U=V\cap S$. We now show that $\vp=(\pi\circ \psi\circ i)|_{V\cap S}$. For take $q\in V\cap S$, and say $\vp(q)=(x_1 , \ldots, x_k)$. Now we have $\psi\circ \vp^{-1}(x_1 , \ldots, x_k)=(x_1, , \ldots, x_k, 0 , \ldots, 0)$, giving $(\pi\circ \psi\circ i)(q)=(x_1 , \ldots, x_k)$. So we have shown that for each $p\in S$, there exist a smooth charts $(U_p, \vp_p)$ and $(V_p, \psi_p)$ about $p$ on $S$ and $M$ respectively such that $U_p=V_p\cap S$, and $\psi_p\circ\vp_p^{-1}:\wh U_p\to \wh V_p$ \begin{equation*} \psi_p\circ\vp_p^{-1}(x_1 , \ldots, x_k) = (x_1 , \ldots, x_k, 0 , \ldots, 0) \end{equation*} for all $(x_1 , \ldots, x_k)\in \wh U_p$. It remains to show that $\mr U=\set{U_p, \vp_p}_{p\in S}$ is a smooth atlas on $S$. To see this, consider note that \begin{equation*} \vp_q\circ \vp_p^{-1} = \vp_q\circ \psi_p^{-1} \circ \psi_p\circ \vp_p^{-1} \end{equation*} and this map sends $(x_1 , \ldots, x_k)$ to $\pi\circ\psi_q^{-1}\circ\psi_p(x_1 , \ldots, x_k, 0 , \ldots, 0)$ for all $(x_1 , \ldots, x_k)\in \vp_p(U_p\cap U_q)$, and hence is smooth.",,"['differential-geometry', 'differential-topology', 'smooth-manifolds']"
81,basic question about holonomy,basic question about holonomy,,"I'm struggling to understand how conditions on the metric put conditions on the holonomy group and vice-versa. My understanding is that the holonomy principle says that there's a one-to-one correspondence between parallel tensors, constant tensors (i.e. with $\nabla S=0$), and elements of the fibre $E_p$ preserved under the holonomy group. I'm trying to understand the 2 following examples: If $(M,g)$ is a manifold and we have a complex structure $J_p:T_pM\to T_pM$ such that $J_p$ is invariant under the holonomy group, I've read that in order to extend $J$ to the whole manifold we need Hol$(g)\subset U(n)$. Why? Isn't all we need that $P_\gamma(J_p)=J_p$. I know that $U(n)=GL(n,\mathbb{C})\cap O(2n)$. So why is the condition $P_\gamma(J_p)=J_p$ equivalent to $P_\gamma$ commuting with $J$ and preserving $g$? The other example is showing that Hol$(g)\subset SO(n)$ if and only if $M$ is orientable. So to use the holonomy principle, start with non-zero $\alpha_p\in \Lambda^n(T^\ast_pM)$. We want $\alpha_p$ to be preserved under Hol$(g)$ so that it can be extended, by the holonomy principle, to a nowhere vanishing form. Again, we just need $\alpha_p$ to be fixed by the holonomy group. How does this happen if and only if the group is contained in SO$(n)$. I apologize for asking 2 questions but I feel the concept I am missing is the same in both. Thanks very much.","I'm struggling to understand how conditions on the metric put conditions on the holonomy group and vice-versa. My understanding is that the holonomy principle says that there's a one-to-one correspondence between parallel tensors, constant tensors (i.e. with $\nabla S=0$), and elements of the fibre $E_p$ preserved under the holonomy group. I'm trying to understand the 2 following examples: If $(M,g)$ is a manifold and we have a complex structure $J_p:T_pM\to T_pM$ such that $J_p$ is invariant under the holonomy group, I've read that in order to extend $J$ to the whole manifold we need Hol$(g)\subset U(n)$. Why? Isn't all we need that $P_\gamma(J_p)=J_p$. I know that $U(n)=GL(n,\mathbb{C})\cap O(2n)$. So why is the condition $P_\gamma(J_p)=J_p$ equivalent to $P_\gamma$ commuting with $J$ and preserving $g$? The other example is showing that Hol$(g)\subset SO(n)$ if and only if $M$ is orientable. So to use the holonomy principle, start with non-zero $\alpha_p\in \Lambda^n(T^\ast_pM)$. We want $\alpha_p$ to be preserved under Hol$(g)$ so that it can be extended, by the holonomy principle, to a nowhere vanishing form. Again, we just need $\alpha_p$ to be fixed by the holonomy group. How does this happen if and only if the group is contained in SO$(n)$. I apologize for asking 2 questions but I feel the concept I am missing is the same in both. Thanks very much.",,"['differential-geometry', 'kahler-manifolds', 'holonomy']"
82,Associated bundles: isomorphism between spaces of differential forms.,Associated bundles: isomorphism between spaces of differential forms.,,"I think this will be an easy question for numerous people. Let $\pi:P\rightarrow M$ be a principal bundle and $\rho:G\rightarrow GL(V)$ a representation. The space of $k$ forms on $M$ with values in $P\times_G V$ (denote as $\Omega^k(M;P\times_G V)$  can be identified with with the space of horizontal, right invariant $k$-forms on $M$ (denote as $\Omega^k_G(P;V))$. Ie, there is an isomorphism: $\Omega^k_G(P;V)\cong \Omega^k(M;P\times_G V)$. I am reading through some lecture notes which say Let $\overline{\zeta}\in \Omega^k_G(P;V)$. Define $\zeta_{\alpha}=s_{\alpha}^*\overline{\zeta}\in \Omega^k(U_{\alpha};V)$. ($s_{\alpha}$ is the local section $s_{\alpha}:U_{\alpha}\rightarrow P$). It then asks to show that $\{\zeta_{\alpha}\}$ define a form in $\Omega^k(M;P\times_G V)$ by showing that the 'gluing' equation is satisfied $\zeta_{\alpha}=\rho(g_{\alpha\beta})\circ \zeta_{\beta}$. Here $g_{\alpha\beta}$ is the transition functions related to the local trivialisations which satisfy $s_{\beta}(m)=s_{\alpha}(m)g_{\alpha\beta}(m)$. I have managed to show that the required equation holds. My question is - why do the constructed $\zeta_{\alpha}\in \Omega^k(U_{\alpha};V)$ define forms in $\Omega^k(M;P\times_G V)$? I understand that $P\times_G V$ has the structure of a fibre bundle with typical fibre $V$, but I am not sure why the gluing equation is important. I am guessing it has something to do with the fact that because the equation holds, one is able to extend the local definition go a global one. I'm not sure. If someone can help that would be great.","I think this will be an easy question for numerous people. Let $\pi:P\rightarrow M$ be a principal bundle and $\rho:G\rightarrow GL(V)$ a representation. The space of $k$ forms on $M$ with values in $P\times_G V$ (denote as $\Omega^k(M;P\times_G V)$  can be identified with with the space of horizontal, right invariant $k$-forms on $M$ (denote as $\Omega^k_G(P;V))$. Ie, there is an isomorphism: $\Omega^k_G(P;V)\cong \Omega^k(M;P\times_G V)$. I am reading through some lecture notes which say Let $\overline{\zeta}\in \Omega^k_G(P;V)$. Define $\zeta_{\alpha}=s_{\alpha}^*\overline{\zeta}\in \Omega^k(U_{\alpha};V)$. ($s_{\alpha}$ is the local section $s_{\alpha}:U_{\alpha}\rightarrow P$). It then asks to show that $\{\zeta_{\alpha}\}$ define a form in $\Omega^k(M;P\times_G V)$ by showing that the 'gluing' equation is satisfied $\zeta_{\alpha}=\rho(g_{\alpha\beta})\circ \zeta_{\beta}$. Here $g_{\alpha\beta}$ is the transition functions related to the local trivialisations which satisfy $s_{\beta}(m)=s_{\alpha}(m)g_{\alpha\beta}(m)$. I have managed to show that the required equation holds. My question is - why do the constructed $\zeta_{\alpha}\in \Omega^k(U_{\alpha};V)$ define forms in $\Omega^k(M;P\times_G V)$? I understand that $P\times_G V$ has the structure of a fibre bundle with typical fibre $V$, but I am not sure why the gluing equation is important. I am guessing it has something to do with the fact that because the equation holds, one is able to extend the local definition go a global one. I'm not sure. If someone can help that would be great.",,"['differential-geometry', 'manifolds', 'lie-groups', 'differential-forms', 'principal-bundles']"
83,Differentiation on Manifolds Basics,Differentiation on Manifolds Basics,,"I'm having some real trouble comprehending integral curves and Lie derivatives on a Manifold. I will write out my understanding and ask the questions below. For a vector field $X$ on smooth manifold $M$ there is a unique curve $\gamma (t):I\rightarrow M$ through each point $p\in M $ such that $\gamma (0)=p$ and such that the tangent vector to the curve $\gamma$ at $\gamma (t)$ is $X_{\gamma (t)}$. This is the integral curve of $X$ through $p$. (apparently) we view $I$ as a manifold (why, what is $I$ precisely?) with coordinates $t$ then $d/dt$ is a tangent vector field on $I$ which acts on functions $f:I\rightarrow \mathbb R$. By definition the tangent vector to the curve is given by $\gamma '(t):= \gamma _* (d/dt)_{\gamma (t)}=X_{\gamma(t)}$. For local coordinates about $p\in M$ \begin{equation} X_{\gamma(t)}(x^i)=\frac{d}{dt}\gamma ^*(x^i)=\frac{d}{dt}(x^i\circ \gamma)=\frac{d}{dt}\gamma^i(t)=\frac{d}{dt}x^i(t) \end{equation}using $X=X^j\partial /\partial x^j$ then, \begin{equation} X_{\gamma(t)}(x^i)=X^j\frac{\partial x^i}{\partial x^j}\bigg|_{\gamma(t)}=X^i(\gamma (t))=X^i\{x^i(t)\} \end{equation} We can equate the final results of each line to get a set of differential equations. \begin{equation} \frac{d}{dt}x^i(t)=X^i\{x^i(t)\} \end{equation} We integrate these to get the curve $\gamma$. At each point $p\in M$ we move a parameter a distance $t$ along the integral curve $\gamma$ of $X$ through $p$. Doing this for all points of $M$ we get the flow along $X$. The vector field generates the flow. My question is can anyone explain why this is different to a set of tangent vectors in the tangent space to $M$? Is the integral curve a function on $M$? Also It looks as if we are using the pullback of the coordinate system? Thanks for your help !! I am hoping that if I understand this I will not need to ask my next question regarding the Lie derivative of a tensor field on $M$.","I'm having some real trouble comprehending integral curves and Lie derivatives on a Manifold. I will write out my understanding and ask the questions below. For a vector field $X$ on smooth manifold $M$ there is a unique curve $\gamma (t):I\rightarrow M$ through each point $p\in M $ such that $\gamma (0)=p$ and such that the tangent vector to the curve $\gamma$ at $\gamma (t)$ is $X_{\gamma (t)}$. This is the integral curve of $X$ through $p$. (apparently) we view $I$ as a manifold (why, what is $I$ precisely?) with coordinates $t$ then $d/dt$ is a tangent vector field on $I$ which acts on functions $f:I\rightarrow \mathbb R$. By definition the tangent vector to the curve is given by $\gamma '(t):= \gamma _* (d/dt)_{\gamma (t)}=X_{\gamma(t)}$. For local coordinates about $p\in M$ \begin{equation} X_{\gamma(t)}(x^i)=\frac{d}{dt}\gamma ^*(x^i)=\frac{d}{dt}(x^i\circ \gamma)=\frac{d}{dt}\gamma^i(t)=\frac{d}{dt}x^i(t) \end{equation}using $X=X^j\partial /\partial x^j$ then, \begin{equation} X_{\gamma(t)}(x^i)=X^j\frac{\partial x^i}{\partial x^j}\bigg|_{\gamma(t)}=X^i(\gamma (t))=X^i\{x^i(t)\} \end{equation} We can equate the final results of each line to get a set of differential equations. \begin{equation} \frac{d}{dt}x^i(t)=X^i\{x^i(t)\} \end{equation} We integrate these to get the curve $\gamma$. At each point $p\in M$ we move a parameter a distance $t$ along the integral curve $\gamma$ of $X$ through $p$. Doing this for all points of $M$ we get the flow along $X$. The vector field generates the flow. My question is can anyone explain why this is different to a set of tangent vectors in the tangent space to $M$? Is the integral curve a function on $M$? Also It looks as if we are using the pullback of the coordinate system? Thanks for your help !! I am hoping that if I understand this I will not need to ask my next question regarding the Lie derivative of a tensor field on $M$.",,"['differential-geometry', 'multilinear-algebra', 'smooth-manifolds', 'lie-derivative']"
84,Reference for principal bundles and related concepts,Reference for principal bundles and related concepts,,"I am looking for a good reference for fibre bundles on differential manifolds, Ehresmann connections, principal $G$-bundles and principal Ehresmann connections (the $G$-equivariant version of Ehresmann connections). Could anyone advise me on this? I have looked at the book Fibre Bundles by Hausmöller, but it isn't quite what I want. Thanks in advance.","I am looking for a good reference for fibre bundles on differential manifolds, Ehresmann connections, principal $G$-bundles and principal Ehresmann connections (the $G$-equivariant version of Ehresmann connections). Could anyone advise me on this? I have looked at the book Fibre Bundles by Hausmöller, but it isn't quite what I want. Thanks in advance.",,"['differential-geometry', 'reference-request', 'principal-bundles', 'connections', 'equivariant-maps']"
85,Space of Alternating $k$-Tensors Notation,Space of Alternating -Tensors Notation,k,"I will be taking a Differential Geometry class in the Fall, so I decided to get somewhat of a head start by going through Spivak's ""Calculus on Manifolds."" Before reading, though, I saw the Addenda at the end, which stated that his notation $\Lambda^{k}\left(V\right)$ for the space of alternating $k$-tensors was incorrect, although it is naturally isomorphic to $\Lambda^{k}\left(V^{*}\right)$ for fin. dim. $V$ (and, after a little more digging around on Wikipedia , is naturally isomorphic to $\left(\Lambda^{k}\left(V\right)\right)^{*}$ in general). Is the notation suggested by Spivak, $\Omega^{k}\left(V\right)$, standard or is there some other notation that is typically used? EDIT: To quote Spivak: ""Finally, the notation $\Lambda^{k}\left(V\right)$ appearing in this book is incorrect, since it conflicts with the standard definition of $\Lambda^{k}\left(V\right)$ (as a certain quotient of the tensor algebra of $V$). For the vector space in question (which is naturally isomorphic to $\Lambda^{k}\left(V^{*}\right)$ for finite dimensional vector spaces $V$) the notation $\Omega^{k}\left(V\right)$ is probably on the way to becoming standard."" I don't know if this is still the case, though, or if his use of $\Lambda^{k}\left(V\right)$ became the standard.","I will be taking a Differential Geometry class in the Fall, so I decided to get somewhat of a head start by going through Spivak's ""Calculus on Manifolds."" Before reading, though, I saw the Addenda at the end, which stated that his notation $\Lambda^{k}\left(V\right)$ for the space of alternating $k$-tensors was incorrect, although it is naturally isomorphic to $\Lambda^{k}\left(V^{*}\right)$ for fin. dim. $V$ (and, after a little more digging around on Wikipedia , is naturally isomorphic to $\left(\Lambda^{k}\left(V\right)\right)^{*}$ in general). Is the notation suggested by Spivak, $\Omega^{k}\left(V\right)$, standard or is there some other notation that is typically used? EDIT: To quote Spivak: ""Finally, the notation $\Lambda^{k}\left(V\right)$ appearing in this book is incorrect, since it conflicts with the standard definition of $\Lambda^{k}\left(V\right)$ (as a certain quotient of the tensor algebra of $V$). For the vector space in question (which is naturally isomorphic to $\Lambda^{k}\left(V^{*}\right)$ for finite dimensional vector spaces $V$) the notation $\Omega^{k}\left(V\right)$ is probably on the way to becoming standard."" I don't know if this is still the case, though, or if his use of $\Lambda^{k}\left(V\right)$ became the standard.",,"['differential-geometry', 'notation', 'tensors']"
86,Perelman's F-functional and its analysis,Perelman's F-functional and its analysis,,"While going through the Kleiner and Lott notes ""Notes on Perelman's papers"" , I encountered an argument that seems wrong to me, or (more likely) I do not understand something. It is about the $F$-funtional. It is defined as follows: $$F(g,f)=\int_{M}(R+\left | \bigtriangledown f \right |^2)e^{-f}dV,$$ where $g$ is Riemannian metric, $f$ is a smooth function over manifold $M$. It is stated, that first variation of the functional can be expressed as follows: $$\delta F(v_{ij},h)=\int_{M}e^{-f}\left [ -v_{ij}(R_{ij}+\bigtriangledown_{i}\bigtriangledown _{j}f)+(\frac{v}{2}-h)(2\Delta f-\left | \bigtriangledown f \right |^2+R) \right ]dV,$$ where $v_{ij}=\delta g_{ij}$ is a symmetric covariant 2-tensor on $M$ (more specifically, it is an element of tangent space of infinite dimensional manifold of all smooth Riemannian metrics on $M$), $h=\delta f$ is a smooth function over $M$ (f is a smooth function as well), $v=g^{ij}v_{ij}$. Part of the proof is as follows: $$\delta R=-\Delta v+\triangledown_{i}\triangledown_{j}v_{ij}-R_{ij}v_{ij}$$ $$\delta (e^{-f}dV)=(\frac{v}{2}-h)e^{-f}dV$$ $$\delta \left | \triangledown f \right |^2=-v^{ij}\triangledown_{i}f\triangledown_{j}f+2<\triangledown f,\triangledown h>$$ I understand these three lines very clearly - it is not very difficult. Putting all together I get $$\delta F=\int_{M}\left [-\Delta v +\triangledown_{i}\triangledown_{j}v_{ij}-R_{ij}v_{ij} +(R+\left | \triangledown f \right |^2)(\frac{v}{2}-h) +2<\triangledown f,\triangledown h>-v^{ij}\triangledown_{i}f\triangledown_{j}f \right ]e^{-f}dV$$. And that is were my results do not match Kleiners and Lotts result. The difference is that in their derivation they have $v_{ij}\triangledown_{i}f\triangledown_{j}f$ instead of $v^{ij}\triangledown_{i}f\triangledown_{j}f$. But this part is just direct plug-in of those three variation formulas above. And those are exactly the same in my derivation and in authors. So, what I am missing? What I'm not seeing?","While going through the Kleiner and Lott notes ""Notes on Perelman's papers"" , I encountered an argument that seems wrong to me, or (more likely) I do not understand something. It is about the $F$-funtional. It is defined as follows: $$F(g,f)=\int_{M}(R+\left | \bigtriangledown f \right |^2)e^{-f}dV,$$ where $g$ is Riemannian metric, $f$ is a smooth function over manifold $M$. It is stated, that first variation of the functional can be expressed as follows: $$\delta F(v_{ij},h)=\int_{M}e^{-f}\left [ -v_{ij}(R_{ij}+\bigtriangledown_{i}\bigtriangledown _{j}f)+(\frac{v}{2}-h)(2\Delta f-\left | \bigtriangledown f \right |^2+R) \right ]dV,$$ where $v_{ij}=\delta g_{ij}$ is a symmetric covariant 2-tensor on $M$ (more specifically, it is an element of tangent space of infinite dimensional manifold of all smooth Riemannian metrics on $M$), $h=\delta f$ is a smooth function over $M$ (f is a smooth function as well), $v=g^{ij}v_{ij}$. Part of the proof is as follows: $$\delta R=-\Delta v+\triangledown_{i}\triangledown_{j}v_{ij}-R_{ij}v_{ij}$$ $$\delta (e^{-f}dV)=(\frac{v}{2}-h)e^{-f}dV$$ $$\delta \left | \triangledown f \right |^2=-v^{ij}\triangledown_{i}f\triangledown_{j}f+2<\triangledown f,\triangledown h>$$ I understand these three lines very clearly - it is not very difficult. Putting all together I get $$\delta F=\int_{M}\left [-\Delta v +\triangledown_{i}\triangledown_{j}v_{ij}-R_{ij}v_{ij} +(R+\left | \triangledown f \right |^2)(\frac{v}{2}-h) +2<\triangledown f,\triangledown h>-v^{ij}\triangledown_{i}f\triangledown_{j}f \right ]e^{-f}dV$$. And that is were my results do not match Kleiners and Lotts result. The difference is that in their derivation they have $v_{ij}\triangledown_{i}f\triangledown_{j}f$ instead of $v^{ij}\triangledown_{i}f\triangledown_{j}f$. But this part is just direct plug-in of those three variation formulas above. And those are exactly the same in my derivation and in authors. So, what I am missing? What I'm not seeing?",,"['differential-geometry', 'ricci-flow']"
87,Geodesics of this metric,Geodesics of this metric,,"I have to calculate the geodesics of the metric: $$\left(\matrix {1 &0\\0& x^2 }\right)$$ I've been able to derive its equations, which are: $$\ddot x -x\dot y ^2=0$$ $$\ddot y+\frac{2}{x}\dot x\dot y=0$$ It's easy to check that lines of constant $y$ are solutions of that equation: all $(v_0t+x_0,y_0)$ satisfy the above, so geodesics between two points of the form $(x_1,y_1)$, $(x_2,y_1)$ are straight lines, but I can't get the general solutions. Any help? Thanks in advance. BTW, To avoid calculations on your side, the Riemann tensor, and therefore the Ricci tensor and scalar curvature, vanish., so $R^2$ with that metric is flat","I have to calculate the geodesics of the metric: $$\left(\matrix {1 &0\\0& x^2 }\right)$$ I've been able to derive its equations, which are: $$\ddot x -x\dot y ^2=0$$ $$\ddot y+\frac{2}{x}\dot x\dot y=0$$ It's easy to check that lines of constant $y$ are solutions of that equation: all $(v_0t+x_0,y_0)$ satisfy the above, so geodesics between two points of the form $(x_1,y_1)$, $(x_2,y_1)$ are straight lines, but I can't get the general solutions. Any help? Thanks in advance. BTW, To avoid calculations on your side, the Riemann tensor, and therefore the Ricci tensor and scalar curvature, vanish., so $R^2$ with that metric is flat",,['differential-geometry']
88,Most probable path of diffusion process,Most probable path of diffusion process,,"Suppose we have an Ito diffusion $X_{t}$ on $\mathbb{R}$ given by \begin{align*} dX_{t} = A(X_{t})dt + B(X_{t}) dW_{t} \qquad (1) \end{align*} where $W_{t}$ is a standard Brownian motion. If $B = 1$, it is well-known that for a twice continuously differentiable curve $u:[0,T]\rightarrow \mathbb{R}$ it is true that \begin{align*} P\bigl( \sup_{  0 \leq t \leq T } | X_{t} - u(t) |  < \epsilon \bigr) \underset{\epsilon \rightarrow 0^{+}}{\sim}  e^{-\frac{1}{2} \int_{0}^{T} \mathcal{L}(u(t),u'(t)) dt } \end{align*} where \begin{align*} \mathcal{L}(u,u')  = \bigl( A( u ) - u' \bigr)^{2} + A'(u) \end{align*} is the Onsager-Machlup function. Thus, minimization of $\mathcal{L}(u(t),u'(t))$ using the Euler-Lagrange equation will yield the most probable path. Now, if $B = B(X_{t})$ is state-dependent, is has been shown by Dürr and Bach (Commun. Math. Phys. 60: 153–170, 1978) that, loosely speaking, the Onsager-Machlup function cannot be defined as a Lagrangian for the most probable path. On the other hand, Y. Takahashi and S. Watanabe (Springer Lecture Notes in Math. 851: 432–463, 1980) have proven that, if $M$ is a Riemannian manifold, $X_{t}$ is a diffusion process with generator $\frac{1}{2} \Delta + f$ (where $\Delta$ is the Laplace–Beltrami operator and $f$ is a vector field) and $u:[0,T] \rightarrow M$ is a smooth curve then \begin{align*} P\bigl( \rho( X_{t} , u(t) )  < \epsilon \text{ for all } t \in [0,T] \bigr)\underset{\epsilon \rightarrow 0^{+}}{\sim} e^{-\frac{1}{2} \int_{0}^{T} \mathcal{L}(u(t),u'(t)) dt } \end{align*} where $\rho$ is the Riemannian distance and $\mathcal{L}$ is a function on the tangent bundle $TM$ given by \begin{align*} \mathcal{L}(u,u') = \lVert f(u) - u' \rVert^{2} + \text{div } f(u) - \frac{1}{6} R(u) \end{align*} Here $\lVert \cdot \rVert$ is the Riemannian norm on the tangent space $T_{u}(M)$ and $R(u)$ is the scalar curvature. But how does this general result relate to (1)? That is, what are $\rho$ and $\mathcal{L}$ (and in particular $\lVert \cdot \rVert$, $f$, $\text{div }f$, and $R$) in this case? As far as I understand (and I am not very familiar with manifolds or diffusions on them) $\rho$ would depend on $B$.","Suppose we have an Ito diffusion $X_{t}$ on $\mathbb{R}$ given by \begin{align*} dX_{t} = A(X_{t})dt + B(X_{t}) dW_{t} \qquad (1) \end{align*} where $W_{t}$ is a standard Brownian motion. If $B = 1$, it is well-known that for a twice continuously differentiable curve $u:[0,T]\rightarrow \mathbb{R}$ it is true that \begin{align*} P\bigl( \sup_{  0 \leq t \leq T } | X_{t} - u(t) |  < \epsilon \bigr) \underset{\epsilon \rightarrow 0^{+}}{\sim}  e^{-\frac{1}{2} \int_{0}^{T} \mathcal{L}(u(t),u'(t)) dt } \end{align*} where \begin{align*} \mathcal{L}(u,u')  = \bigl( A( u ) - u' \bigr)^{2} + A'(u) \end{align*} is the Onsager-Machlup function. Thus, minimization of $\mathcal{L}(u(t),u'(t))$ using the Euler-Lagrange equation will yield the most probable path. Now, if $B = B(X_{t})$ is state-dependent, is has been shown by Dürr and Bach (Commun. Math. Phys. 60: 153–170, 1978) that, loosely speaking, the Onsager-Machlup function cannot be defined as a Lagrangian for the most probable path. On the other hand, Y. Takahashi and S. Watanabe (Springer Lecture Notes in Math. 851: 432–463, 1980) have proven that, if $M$ is a Riemannian manifold, $X_{t}$ is a diffusion process with generator $\frac{1}{2} \Delta + f$ (where $\Delta$ is the Laplace–Beltrami operator and $f$ is a vector field) and $u:[0,T] \rightarrow M$ is a smooth curve then \begin{align*} P\bigl( \rho( X_{t} , u(t) )  < \epsilon \text{ for all } t \in [0,T] \bigr)\underset{\epsilon \rightarrow 0^{+}}{\sim} e^{-\frac{1}{2} \int_{0}^{T} \mathcal{L}(u(t),u'(t)) dt } \end{align*} where $\rho$ is the Riemannian distance and $\mathcal{L}$ is a function on the tangent bundle $TM$ given by \begin{align*} \mathcal{L}(u,u') = \lVert f(u) - u' \rVert^{2} + \text{div } f(u) - \frac{1}{6} R(u) \end{align*} Here $\lVert \cdot \rVert$ is the Riemannian norm on the tangent space $T_{u}(M)$ and $R(u)$ is the scalar curvature. But how does this general result relate to (1)? That is, what are $\rho$ and $\mathcal{L}$ (and in particular $\lVert \cdot \rVert$, $f$, $\text{div }f$, and $R$) in this case? As far as I understand (and I am not very familiar with manifolds or diffusions on them) $\rho$ would depend on $B$.",,"['differential-geometry', 'manifolds', 'riemannian-geometry', 'stochastic-calculus', 'stochastic-analysis']"
89,The Definition of the Second Fundamental Form,The Definition of the Second Fundamental Form,,"Let $r:M\rightarrow{\mathbb{R}^{n+1}}$ be an isometric immersion and $M$ is an $n$ -dimensional Riemannian Manifold. That is to say, $M$ is the hypersurface in $\mathbb{{R}^{n+1}}$ . Then we can introduce a normal vector field: $N:M\rightarrow{T\mathbb{{R}^{n+1}}}=\mathbb{R}^{n+1}\times{\mathbb{R}^{n+1}}$ satisfies $N_p\in{T_{r(p)}\mathbb{R}^{n+1}}=\{r(p)\}\times\mathbb{R}^{n+1}$ . So we will have $T_{r(p)}\mathbb{R}^{n+1}=r_*(T_pM)\oplus{span\{N_p\}}$ . Before we talk about this problem, we look at the connection on $\mathbb{R}^{n+1}$ . Let $\bigtriangledown$ be its connection and $X=\sum_{i=1}^{n+1}x_ie_i$ , $Y=\sum_{i=1}^{n+1}y_ie_i$ . So $\bigtriangledown_XY=\sum_{i=1}^{n+1}\sum_{j=1}^{n+1}x_je_j(y_i)e_i$ . When I read book, I find two different definitions of The Second Fundamental Form . I want to verify that they are the same. Let $\bar{n}$ denote the Guass Map which is actually: $n=\pi_2\circ{N}:M\rightarrow{\mathbb{R}^{n+1}}$ . The first definition is: $II|_U=\sum_{i=1}^{n}\sum_{j=1}^{n}<\bigtriangledown_{\frac{\partial{r}}{\partial{x_i}}}N,\frac{\partial{r}}{\partial{x_j}}>dx_i\otimes{dx_j}$ The second definition is: $II|_U=\sum_{i=1}^{n}\sum_{j=1}^{n}<\frac{\partial{n}}{\partial{x_i}},\frac{\partial{r}}{\partial{x_j}}>dx_i\otimes{dx_j}$ So I think they are the same. Then I try to prove it. But I am failed. I want to prove $\bigtriangledown_{\frac{\partial{r}}{\partial{x_i}}}N=\frac{\partial{n}}{\partial{x_i}}$ . Proof. Let $\frac{\partial{r}}{\partial{x_i}}=\sum_{k=1}^{n+1}\frac{\partial{r_k}}{\partial{x_i}}e_k$ and $N=\sum_{k=1}^{n+1}n_ke_k$ . Then I have no idea. How to prove??","Let be an isometric immersion and is an -dimensional Riemannian Manifold. That is to say, is the hypersurface in . Then we can introduce a normal vector field: satisfies . So we will have . Before we talk about this problem, we look at the connection on . Let be its connection and , . So . When I read book, I find two different definitions of The Second Fundamental Form . I want to verify that they are the same. Let denote the Guass Map which is actually: . The first definition is: The second definition is: So I think they are the same. Then I try to prove it. But I am failed. I want to prove . Proof. Let and . Then I have no idea. How to prove??","r:M\rightarrow{\mathbb{R}^{n+1}} M n M \mathbb{{R}^{n+1}} N:M\rightarrow{T\mathbb{{R}^{n+1}}}=\mathbb{R}^{n+1}\times{\mathbb{R}^{n+1}} N_p\in{T_{r(p)}\mathbb{R}^{n+1}}=\{r(p)\}\times\mathbb{R}^{n+1} T_{r(p)}\mathbb{R}^{n+1}=r_*(T_pM)\oplus{span\{N_p\}} \mathbb{R}^{n+1} \bigtriangledown X=\sum_{i=1}^{n+1}x_ie_i Y=\sum_{i=1}^{n+1}y_ie_i \bigtriangledown_XY=\sum_{i=1}^{n+1}\sum_{j=1}^{n+1}x_je_j(y_i)e_i \bar{n} n=\pi_2\circ{N}:M\rightarrow{\mathbb{R}^{n+1}} II|_U=\sum_{i=1}^{n}\sum_{j=1}^{n}<\bigtriangledown_{\frac{\partial{r}}{\partial{x_i}}}N,\frac{\partial{r}}{\partial{x_j}}>dx_i\otimes{dx_j} II|_U=\sum_{i=1}^{n}\sum_{j=1}^{n}<\frac{\partial{n}}{\partial{x_i}},\frac{\partial{r}}{\partial{x_j}}>dx_i\otimes{dx_j} \bigtriangledown_{\frac{\partial{r}}{\partial{x_i}}}N=\frac{\partial{n}}{\partial{x_i}} \frac{\partial{r}}{\partial{x_i}}=\sum_{k=1}^{n+1}\frac{\partial{r_k}}{\partial{x_i}}e_k N=\sum_{k=1}^{n+1}n_ke_k","['differential-geometry', 'riemannian-geometry', 'surfaces']"
90,Submanifold given by an immersion open onto its image,Submanifold given by an immersion open onto its image,,"I was wondering if the following is true: Let $M,N$ be two manifolds such that $\dim M\leq \dim N$ and $f:M\rightarrow N$ an smooth immersion. Assume that for any open set $U\subset M$ , $f(U)$ is open in $f(M)$ , does it imply that $f(M)$ is a submanifold of $N$ ? I know that if we also ask $f$ to be injective, then it is an embedding and $f(M)$ is automatically a submanifold of $N$ . But without this assumption, I am not sure that the result holds. Being an open map on its image somehow tells us that there is no bad self-intersection in $f(M)$ but I am not sure this is enough to have a submanifold.","I was wondering if the following is true: Let be two manifolds such that and an smooth immersion. Assume that for any open set , is open in , does it imply that is a submanifold of ? I know that if we also ask to be injective, then it is an embedding and is automatically a submanifold of . But without this assumption, I am not sure that the result holds. Being an open map on its image somehow tells us that there is no bad self-intersection in but I am not sure this is enough to have a submanifold.","M,N \dim M\leq \dim N f:M\rightarrow N U\subset M f(U) f(M) f(M) N f f(M) N f(M)","['differential-geometry', 'manifolds', 'smooth-manifolds', 'submanifold']"
91,Prove that $g^{-1}(0)$ is a $n$-dimensional manifold.,Prove that  is a -dimensional manifold.,g^{-1}(0) n,"Let $A\subset \mathbb R ^n$ be open and let $g:A\to \mathbb R ^p$ be a differentiable function such that $g'(x)$ has rank $p$ whenever $g(x)=0$. Then $g^{-1}(0)$ is an $(n-p)$-dimensional manifold. This is Theorem 5-1 in Spivak's ""Calculus on Manifolds"" and the proof is: ""It follows immediately from Theorem 2-13.[]"" Theorem 2-13 says practically that (under the same hypothesis) if $x\in A$, then there is an open set $U$ containing $x$, an open set $V\subset \mathbb R ^n$ and a diffeomorphism $h:U\to V$ such that $$g(h(x))=(x_1,...,x_p).$$ My bad, I can't see how does the theorem follow immediately from this. To prove that $g^{-1}(0)$ is a $(n-p)$ manifold, I should find open sets $U'$ with $x\in U'$, $V'$ and a diffeomorphism $h':U'\to V'$ such that $$h'(g^{-1}(0)\cap U')=V'\cap [\mathbb R ^{(n-p)}\times \{0\}],$$ where $0$ is the $\mathbb R ^p$'s zero. How do I construct $U'$, $V'$, $h'$ starting from the given $U,V,h$? The only ideas that come to my mind are silly things like $$f(x)=I(x)-(0,g\circ h(x)),$$ where $I$ is the identity and $0$ is the $n-p$ one. I'm very unfamiliar with this stuff, so I'm probably missing some pedantic application of the definition, can you give me some hint?","Let $A\subset \mathbb R ^n$ be open and let $g:A\to \mathbb R ^p$ be a differentiable function such that $g'(x)$ has rank $p$ whenever $g(x)=0$. Then $g^{-1}(0)$ is an $(n-p)$-dimensional manifold. This is Theorem 5-1 in Spivak's ""Calculus on Manifolds"" and the proof is: ""It follows immediately from Theorem 2-13.[]"" Theorem 2-13 says practically that (under the same hypothesis) if $x\in A$, then there is an open set $U$ containing $x$, an open set $V\subset \mathbb R ^n$ and a diffeomorphism $h:U\to V$ such that $$g(h(x))=(x_1,...,x_p).$$ My bad, I can't see how does the theorem follow immediately from this. To prove that $g^{-1}(0)$ is a $(n-p)$ manifold, I should find open sets $U'$ with $x\in U'$, $V'$ and a diffeomorphism $h':U'\to V'$ such that $$h'(g^{-1}(0)\cap U')=V'\cap [\mathbb R ^{(n-p)}\times \{0\}],$$ where $0$ is the $\mathbb R ^p$'s zero. How do I construct $U'$, $V'$, $h'$ starting from the given $U,V,h$? The only ideas that come to my mind are silly things like $$f(x)=I(x)-(0,g\circ h(x)),$$ where $I$ is the identity and $0$ is the $n-p$ one. I'm very unfamiliar with this stuff, so I'm probably missing some pedantic application of the definition, can you give me some hint?",,"['differential-geometry', 'manifolds']"
92,Gauss-Bonnet Theorem in dimension four,Gauss-Bonnet Theorem in dimension four,,"I've read that the generalized Gauss-Bonnet theorem states that $$\int\limits_{M}Pf(\Omega)=(2\pi)^n\chi(M)$$ where, $M$ is a 2n-dimensional compact orientable Riemannian manifold without boundary $\Omega$ is the curvature form and $Pf(\Omega)$ is the Pfaffian of $\Omega$, $Pf(\Omega)$ is a 2n-form. How can I prove that in dimension four is valid: $$\chi(M)=\frac{1}{32\pi^2}\int\limits_M(|Rm|^2-4|Ric|^2+R^2)\,d\mu$$ where, $Rm$ is the Riemannian curvature tensor, $Ric$ is the Ricci curvature tensor ans $R$ is then scalar curvature. Tahnks in advice.","I've read that the generalized Gauss-Bonnet theorem states that $$\int\limits_{M}Pf(\Omega)=(2\pi)^n\chi(M)$$ where, $M$ is a 2n-dimensional compact orientable Riemannian manifold without boundary $\Omega$ is the curvature form and $Pf(\Omega)$ is the Pfaffian of $\Omega$, $Pf(\Omega)$ is a 2n-form. How can I prove that in dimension four is valid: $$\chi(M)=\frac{1}{32\pi^2}\int\limits_M(|Rm|^2-4|Ric|^2+R^2)\,d\mu$$ where, $Rm$ is the Riemannian curvature tensor, $Ric$ is the Ricci curvature tensor ans $R$ is then scalar curvature. Tahnks in advice.",,"['differential-geometry', 'riemannian-geometry', 'differential-topology', 'pfaffian']"
93,What is nonhomogeneous linear mapping?,What is nonhomogeneous linear mapping?,,"In Milnor's Topology from the differentiable viewpoint , page 3, he said: One thinks of the nonhomogeneous linear mapping from the tangent hyperplane at $x$ to the tangent hyperplane at $y$ which best approximates $f$. Translating both hyperplanes to the origin, one obtains $df_x$. What is nonhomogeneous linear mapping? As far as I know, linear maps are all homogeneous.","In Milnor's Topology from the differentiable viewpoint , page 3, he said: One thinks of the nonhomogeneous linear mapping from the tangent hyperplane at $x$ to the tangent hyperplane at $y$ which best approximates $f$. Translating both hyperplanes to the origin, one obtains $df_x$. What is nonhomogeneous linear mapping? As far as I know, linear maps are all homogeneous.",,"['differential-geometry', 'differential-topology', 'manifolds']"
94,local isometry for riemannian manifolds is not transitive,local isometry for riemannian manifolds is not transitive,,"Let $(M_1,g_1)$ and $(M_2,g_2)$ be Riemannian manifolds of the same dimension, and let $\phi: M_1 \to M_2$ be a smooth map. We say that $\phi$ is a local isometry if $g_2 (\phi_* X, \phi_* Y ) = g_1 (X, Y )$ for all $m \in M_1$ and $X, Y \in T_m M_1,$ where $\phi_* : T_m M_1 \to T_{\phi(m)} M_2$ is the derivative of the map $\phi$ at $m.$ The relation of being locally isometric for Riemannian manifolds is not symmetric, it is of course reflexive: is it transitive?","Let $(M_1,g_1)$ and $(M_2,g_2)$ be Riemannian manifolds of the same dimension, and let $\phi: M_1 \to M_2$ be a smooth map. We say that $\phi$ is a local isometry if $g_2 (\phi_* X, \phi_* Y ) = g_1 (X, Y )$ for all $m \in M_1$ and $X, Y \in T_m M_1,$ where $\phi_* : T_m M_1 \to T_{\phi(m)} M_2$ is the derivative of the map $\phi$ at $m.$ The relation of being locally isometric for Riemannian manifolds is not symmetric, it is of course reflexive: is it transitive?",,"['differential-geometry', 'riemannian-geometry']"
95,Exterior Algebra of smooth differential forms,Exterior Algebra of smooth differential forms,,"I'm a little bit confused about the exterior algebra of smooth differential forms $\Omega(M)$ on a manifold M. The definition of k-forms is clear to me, but I don't understand how to put them together, s.t. they form $\Omega(M)$ so to speak. Maybe you can help me to get rid of my problems: First of all there is some confusion about the definition of $\Omega(M)$. Some people write $\Omega(M):=\bigoplus\limits_{k=0}^{\dim(M)}\Omega^k(M)$ and some $\Omega(M):=\sum\limits_{k=0}^{\dim(M)}\Omega^k(M)$. I never saw the second notation before, are they both the same by definition or is there a different meaning by the second one? Furthermore, if we accept the definition $\Omega(M):=\bigoplus\limits_{k=0}^{\dim(M)}\Omega^k(M)$ then the elements of $\Omega(M)$ will consist of tuple like $(\omega_0,...,\omega_{\dim(M)})$, whereas $\omega_k\in\Omega^k(M)$. How do I extend the definition of the wedge product of single forms, i.e. $w_i\wedge\omega_j$, to elemts of the algebra, i.e. $(\omega_0,...,\omega_{\dim(M)})\wedge (\alpha_0,...,\alpha_{\dim(M)})=?$ I suggest that one writes the elements as ""formal sums"" like $(\omega_0,...,\omega_{\dim(M)})=:\omega_0+...+\omega_{dim(M)}$ and then extend the wedge product bilinearly. Is that right? I hope someone can help me by answering my two questions. Regards","I'm a little bit confused about the exterior algebra of smooth differential forms $\Omega(M)$ on a manifold M. The definition of k-forms is clear to me, but I don't understand how to put them together, s.t. they form $\Omega(M)$ so to speak. Maybe you can help me to get rid of my problems: First of all there is some confusion about the definition of $\Omega(M)$. Some people write $\Omega(M):=\bigoplus\limits_{k=0}^{\dim(M)}\Omega^k(M)$ and some $\Omega(M):=\sum\limits_{k=0}^{\dim(M)}\Omega^k(M)$. I never saw the second notation before, are they both the same by definition or is there a different meaning by the second one? Furthermore, if we accept the definition $\Omega(M):=\bigoplus\limits_{k=0}^{\dim(M)}\Omega^k(M)$ then the elements of $\Omega(M)$ will consist of tuple like $(\omega_0,...,\omega_{\dim(M)})$, whereas $\omega_k\in\Omega^k(M)$. How do I extend the definition of the wedge product of single forms, i.e. $w_i\wedge\omega_j$, to elemts of the algebra, i.e. $(\omega_0,...,\omega_{\dim(M)})\wedge (\alpha_0,...,\alpha_{\dim(M)})=?$ I suggest that one writes the elements as ""formal sums"" like $(\omega_0,...,\omega_{\dim(M)})=:\omega_0+...+\omega_{dim(M)}$ and then extend the wedge product bilinearly. Is that right? I hope someone can help me by answering my two questions. Regards",,['differential-geometry']
96,Exponential map and connection,Exponential map and connection,,"Suppose you have a Riemannian manifold $(M,g)$ and a point $p\in M$ fixed. Let $v: s\mapsto v(s)$ be a curve in $T_pM$. Now consider the map $f(s):=\exp_p(v(s))$. Can one get an explicit formula for $f'(s)$? Maybe that is confusing but I thought of something like $f'(s)=d(\exp_p)_{v(s)}(\nabla_sv(s))$, where $\nabla_s$ denotes the covariant derivative. Formally, $d(\exp_p)_{v(s)}$ is a map from $T_{v(s)}T_pM$ to $T_{\exp_p(v(s))}M$ and one can identify $T_{v(s)}T_pM\cong T_pM$. I would like to know if the formula above is true and how this identification is related to the Levi-Civita-connection on $M$. Thanks for any explanation.","Suppose you have a Riemannian manifold $(M,g)$ and a point $p\in M$ fixed. Let $v: s\mapsto v(s)$ be a curve in $T_pM$. Now consider the map $f(s):=\exp_p(v(s))$. Can one get an explicit formula for $f'(s)$? Maybe that is confusing but I thought of something like $f'(s)=d(\exp_p)_{v(s)}(\nabla_sv(s))$, where $\nabla_s$ denotes the covariant derivative. Formally, $d(\exp_p)_{v(s)}$ is a map from $T_{v(s)}T_pM$ to $T_{\exp_p(v(s))}M$ and one can identify $T_{v(s)}T_pM\cong T_pM$. I would like to know if the formula above is true and how this identification is related to the Levi-Civita-connection on $M$. Thanks for any explanation.",,"['differential-geometry', 'riemannian-geometry']"
97,Space of Jordan curves,Space of Jordan curves,,"The space of square-integrable functions $f:[0,1]\rightarrow\mathbb{R}$ is well conceivable: it's essentially an $\infty$-dimensional Euclidean space (the Hilbert space $L^2$) with well interpretable dimensions (= frequencies, modes). But is there something like the space of Jordan curves $f:[0,1]\rightarrow\mathbb{R}^2$? One might guess it's $L^2 \times L^2$, but that's not true, since being a Jordan curve imposes stronger restrictions on $f$, so the sought space $\mathcal{J}$ is only a subset of $L^2 \times L^2$. But what kind of subset? How is it shaped? Is it a subspace? Of which dimension? [For me this a general problem: to imagine and understand ""spaces of shapes"": what is a sensible metric (→ metric space), how - eventually - to add and scale shapes (→ vector space), what's the dimension, and how to interpret the dimensions?]","The space of square-integrable functions $f:[0,1]\rightarrow\mathbb{R}$ is well conceivable: it's essentially an $\infty$-dimensional Euclidean space (the Hilbert space $L^2$) with well interpretable dimensions (= frequencies, modes). But is there something like the space of Jordan curves $f:[0,1]\rightarrow\mathbb{R}^2$? One might guess it's $L^2 \times L^2$, but that's not true, since being a Jordan curve imposes stronger restrictions on $f$, so the sought space $\mathcal{J}$ is only a subset of $L^2 \times L^2$. But what kind of subset? How is it shaped? Is it a subspace? Of which dimension? [For me this a general problem: to imagine and understand ""spaces of shapes"": what is a sensible metric (→ metric space), how - eventually - to add and scale shapes (→ vector space), what's the dimension, and how to interpret the dimensions?]",,"['differential-geometry', 'hilbert-spaces']"
98,Prove that a curve is spherical iff it satisfies the relation,Prove that a curve is spherical iff it satisfies the relation,,"I couldn't prove that a regular curve, such that the torsion and curvature never equal zero, satisfies the relation $$\frac{\tau}{\kappa}+(\frac{1}{\tau}(\frac{1}{\kappa})´)´=0$$ iff it's spherical, i.e, it lies entirely on a sphere.","I couldn't prove that a regular curve, such that the torsion and curvature never equal zero, satisfies the relation $$\frac{\tau}{\kappa}+(\frac{1}{\tau}(\frac{1}{\kappa})´)´=0$$ iff it's spherical, i.e, it lies entirely on a sphere.",,['differential-geometry']
99,germ finitely determined,germ finitely determined,,"Does anyone know any result on finitely determined germs to help me prove that the germ $f(x,y)=x^3+ xy^3$ is $4$- determined? I tried using the definition of germs finitely determined, which is:$f: \mathbb{R}^n \rightarrow \mathbb{R}$ is $k$-determined if for any other germ $g: \mathbb{R}^n \rightarrow \mathbb{R}$ such that the $k$-jet of g is equal to $k$-jet of the $f$, then $f$ and $g$ are right equivalents, i. e., exist a difeomorfism $h$ such that $f=g\circ h$, but not getting success. I think there should be some results to help me prove it. Thanks!","Does anyone know any result on finitely determined germs to help me prove that the germ $f(x,y)=x^3+ xy^3$ is $4$- determined? I tried using the definition of germs finitely determined, which is:$f: \mathbb{R}^n \rightarrow \mathbb{R}$ is $k$-determined if for any other germ $g: \mathbb{R}^n \rightarrow \mathbb{R}$ such that the $k$-jet of g is equal to $k$-jet of the $f$, then $f$ and $g$ are right equivalents, i. e., exist a difeomorfism $h$ such that $f=g\circ h$, but not getting success. I think there should be some results to help me prove it. Thanks!",,"['differential-geometry', 'germs', 'singularity-theory']"
