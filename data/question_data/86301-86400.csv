,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Prove there exists an element $x\in X$, such that $\|x\|=1$ and $d(x,M)=1$.","Prove there exists an element , such that  and .","x\in X \|x\|=1 d(x,M)=1","If $M$ is a finite dimensional proper subspace of a normed linear space $X$, prove there exists an element $x\in X$, such that $\|x\|=1$ and $d(x,M)=1$. My Try and Problems: If $M$ were closed then I can apply Hahn Banach Theorem. But being finite dimensional implies that $M$ is closed?","If $M$ is a finite dimensional proper subspace of a normed linear space $X$, prove there exists an element $x\in X$, such that $\|x\|=1$ and $d(x,M)=1$. My Try and Problems: If $M$ were closed then I can apply Hahn Banach Theorem. But being finite dimensional implies that $M$ is closed?",,['functional-analysis']
1,"Show that bilinear form is $H^1(0,l)$-elliptic/coercive",Show that bilinear form is -elliptic/coercive,"H^1(0,l)","Let $$a(u,v) := \int_0^l \partial u(x) \partial v(x) + cu(x)v(x)\, \mathrm{d}x + \alpha u(l)v(l)$$ Show that $a(\cdot,\cdot)$ is $H^1(0,l)$-elliptic if either $c > 0$ or $\alpha > 0$. My attempt at a solution: The case where $\alpha$ is $0$ is straightforward. I am stuck on the $c =0$ and $\alpha > 0$ case. I need to show that  \begin{align} \int_0^l (\partial v(x))^2 \, \mathrm{d}x + \alpha u(l)v(l) \ge \int^l_0 (\partial v(x))^2 + v(x)^2 \, \mathrm{d}x. \end{align}  I've been comparing the terms of the $||v||_{H^1}$ term by term to the bilinear form - clearly, $a(v,v) \ge \int_0^l (\partial v(x))^2$, so that term is fine. But I can't figure out why $a(v,v) \ge c_0 \int_0^lv(x)^2$. The only estimate that I found to work with is that $|u(l)| \le 2l^{1/2} ||\partial u||_{L^2} + 2l^{-1/2}||u||_{L^2}$, and that's not getting me anywhere.","Let $$a(u,v) := \int_0^l \partial u(x) \partial v(x) + cu(x)v(x)\, \mathrm{d}x + \alpha u(l)v(l)$$ Show that $a(\cdot,\cdot)$ is $H^1(0,l)$-elliptic if either $c > 0$ or $\alpha > 0$. My attempt at a solution: The case where $\alpha$ is $0$ is straightforward. I am stuck on the $c =0$ and $\alpha > 0$ case. I need to show that  \begin{align} \int_0^l (\partial v(x))^2 \, \mathrm{d}x + \alpha u(l)v(l) \ge \int^l_0 (\partial v(x))^2 + v(x)^2 \, \mathrm{d}x. \end{align}  I've been comparing the terms of the $||v||_{H^1}$ term by term to the bilinear form - clearly, $a(v,v) \ge \int_0^l (\partial v(x))^2$, so that term is fine. But I can't figure out why $a(v,v) \ge c_0 \int_0^lv(x)^2$. The only estimate that I found to work with is that $|u(l)| \le 2l^{1/2} ||\partial u||_{L^2} + 2l^{-1/2}||u||_{L^2}$, and that's not getting me anywhere.",,"['functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
2,"If $f^2$ and $f^3$ are smooth, does it follow that $f$ is smooth?","If  and  are smooth, does it follow that  is smooth?",f^2 f^3 f,"Let $f: \mathbb{R} \to \mathbb{R}$ be given. Assume that the square and cube of $f$ are smooth. Is $f$ smooth? That is if $f \cdot f \in C^{\infty}$ and $f \cdot f \cdot f \in C^{\infty}$, does it follow that: $f \in C^{\infty}$ I got this from another question on SE. So, $f(x)^2$ is infinitely differentiable, and so is: $f(x)^3$ Also, realize: $$f(x) = \frac{f(x)^3}{f(x)^2}$$ But what can I do?","Let $f: \mathbb{R} \to \mathbb{R}$ be given. Assume that the square and cube of $f$ are smooth. Is $f$ smooth? That is if $f \cdot f \in C^{\infty}$ and $f \cdot f \cdot f \in C^{\infty}$, does it follow that: $f \in C^{\infty}$ I got this from another question on SE. So, $f(x)^2$ is infinitely differentiable, and so is: $f(x)^3$ Also, realize: $$f(x) = \frac{f(x)^3}{f(x)^2}$$ But what can I do?",,"['calculus', 'real-analysis', 'functional-analysis', 'derivatives', 'proof-writing']"
3,weak* convergence for sequence in $ L^\infty$,weak* convergence for sequence in, L^\infty,"Let $ \Omega \subset \mathbb{R}^d $ be a bounded and open set. Suppose $ \{f_n\} \subset L^{\infty} (\Omega) , f \in L^{\infty} (\Omega) $.  Prove that $ f_n \rightharpoonup^* f \ \ \text{in} \ \  L_{\infty} (\Omega)  \Longleftrightarrow \left(   \sup\limits_{n \in \mathbb{N} } \| f_n \|_{L^{\infty}} < + \infty \ \ \text{and} \ \ { \displaystyle \forall_{ V \subset \Omega} \ \lim\limits_{n \to \infty} \ \int\limits_V \ (f_n(x) - f(x)) =0} \right)   \\ \text{where } V= \Pi^d_{i=1}(a_i,b_i). $ Can anyone give me any clue? I would appreciate any help.","Let $ \Omega \subset \mathbb{R}^d $ be a bounded and open set. Suppose $ \{f_n\} \subset L^{\infty} (\Omega) , f \in L^{\infty} (\Omega) $.  Prove that $ f_n \rightharpoonup^* f \ \ \text{in} \ \  L_{\infty} (\Omega)  \Longleftrightarrow \left(   \sup\limits_{n \in \mathbb{N} } \| f_n \|_{L^{\infty}} < + \infty \ \ \text{and} \ \ { \displaystyle \forall_{ V \subset \Omega} \ \lim\limits_{n \to \infty} \ \int\limits_V \ (f_n(x) - f(x)) =0} \right)   \\ \text{where } V= \Pi^d_{i=1}(a_i,b_i). $ Can anyone give me any clue? I would appreciate any help.",,"['functional-analysis', 'weak-convergence']"
4,Continuous extension of the limit functional,Continuous extension of the limit functional,,"Let $(\ell^{\infty})'$ be the $\mathbb{F}$-vector space of linear and continuous (bounded) functionals $\ell^{\infty}\rightarrow \mathbb{F}$, where $\mathbb{F}$ is either $\mathbb{R}$ or $\mathbb{C}$ (but we can assume $\mathbb{F}=\mathbb{R}$, if needed) and $\ell^{\infty}$ has the sup norm $\parallel\cdot\parallel_{\infty}$. Let also $c$ be the subspace of $\ell^{\infty}$ consisting of convergent sequences. Then the limit functional $\lim\colon c\rightarrow \mathbb{F}$ sending a convergent sequence to its limit is a continuous, linear functional with operator norm $1$ ($c$ has the sup norm as well). I am asked to prove or disprove that there exist distinct elements $f,g\in(\ell^{\infty})'$ which extend the limit functional on $c$. I think the claim is true, but, up to now, I have been able to prove only the following fact (for $\mathbb{F}=\mathbb{R}$), using Hahn-Banach's extension Theorem: for every real number $\lambda$ with $-1\leq \lambda \leq 1$, there exists a linear extension $h_{\lambda}$ of the limit functional to the whole $\ell^{\infty}$ such that, for all $\alpha\in\mathbb{R}$ and any convergent sequence $x\in c$, if $y$ is the sequence $((-1)^{n})_{n\in\mathbb{N}}$, then $$h_{\lambda}(ay+x)=a\lambda +\lim(x)\leq \limsup(ay+x).$$ In particular, there are uncountably many linear extension of the limit functional. I can not prove that at least two of these are continuous though. Can someone help me solving this problem with a worked solution? (I have looked for Banach limits around, but I have not found an explicit proof of non uniqueness of such continuous extensions of the limit extension). Thanks in advance.","Let $(\ell^{\infty})'$ be the $\mathbb{F}$-vector space of linear and continuous (bounded) functionals $\ell^{\infty}\rightarrow \mathbb{F}$, where $\mathbb{F}$ is either $\mathbb{R}$ or $\mathbb{C}$ (but we can assume $\mathbb{F}=\mathbb{R}$, if needed) and $\ell^{\infty}$ has the sup norm $\parallel\cdot\parallel_{\infty}$. Let also $c$ be the subspace of $\ell^{\infty}$ consisting of convergent sequences. Then the limit functional $\lim\colon c\rightarrow \mathbb{F}$ sending a convergent sequence to its limit is a continuous, linear functional with operator norm $1$ ($c$ has the sup norm as well). I am asked to prove or disprove that there exist distinct elements $f,g\in(\ell^{\infty})'$ which extend the limit functional on $c$. I think the claim is true, but, up to now, I have been able to prove only the following fact (for $\mathbb{F}=\mathbb{R}$), using Hahn-Banach's extension Theorem: for every real number $\lambda$ with $-1\leq \lambda \leq 1$, there exists a linear extension $h_{\lambda}$ of the limit functional to the whole $\ell^{\infty}$ such that, for all $\alpha\in\mathbb{R}$ and any convergent sequence $x\in c$, if $y$ is the sequence $((-1)^{n})_{n\in\mathbb{N}}$, then $$h_{\lambda}(ay+x)=a\lambda +\lim(x)\leq \limsup(ay+x).$$ In particular, there are uncountably many linear extension of the limit functional. I can not prove that at least two of these are continuous though. Can someone help me solving this problem with a worked solution? (I have looked for Banach limits around, but I have not found an explicit proof of non uniqueness of such continuous extensions of the limit extension). Thanks in advance.",,"['functional-analysis', 'limits']"
5,Graph of weakly continuous linear operator,Graph of weakly continuous linear operator,,"I have a few questions regarding the graph of an operator. Consider the operator $T:X \rightarrow Y$ between Banach spaces $X,Y$. Assume that $T$ is a linear operator which is (weak, weak)-continuous, so $T$ is continuous when $X$ and $Y$ are endowed with the weak topology. Consider the graph $G(T) := \{(x,y) \in X \times Y: ~~Tx = y \}$. The two questions I have are: Can we conclude that $G(T)$ is weakly closed subspace of $X \times Y$? Also, since $G(T)$ is convex can we then use Mazur's Theorem to conclude that $G(T)$ is strongly closed in $X \times Y$? Thanks.","I have a few questions regarding the graph of an operator. Consider the operator $T:X \rightarrow Y$ between Banach spaces $X,Y$. Assume that $T$ is a linear operator which is (weak, weak)-continuous, so $T$ is continuous when $X$ and $Y$ are endowed with the weak topology. Consider the graph $G(T) := \{(x,y) \in X \times Y: ~~Tx = y \}$. The two questions I have are: Can we conclude that $G(T)$ is weakly closed subspace of $X \times Y$? Also, since $G(T)$ is convex can we then use Mazur's Theorem to conclude that $G(T)$ is strongly closed in $X \times Y$? Thanks.",,"['real-analysis', 'functional-analysis']"
6,Bounds on the line for entire functions of exponential type,Bounds on the line for entire functions of exponential type,,"Let $f$ be an entire function on the complex plane $\mathbb C$, assume that $$|f(z)|\le e^{|z|}.$$ Does the property $$|f(x)|\le e^{-|x|}, \qquad x\in\mathbb R,$$ imply $f\equiv 0$? More generally, characterize the class of majorants $w$ on the real line, for which $|f(x)|\le w(x)$ yields $f\equiv 0$.","Let $f$ be an entire function on the complex plane $\mathbb C$, assume that $$|f(z)|\le e^{|z|}.$$ Does the property $$|f(x)|\le e^{-|x|}, \qquad x\in\mathbb R,$$ imply $f\equiv 0$? More generally, characterize the class of majorants $w$ on the real line, for which $|f(x)|\le w(x)$ yields $f\equiv 0$.",,"['complex-analysis', 'analysis', 'functional-analysis', 'reference-request', 'fourier-analysis']"
7,"How to show $e^{-x}$ is a cyclic vector for $-\frac{d^{2}}{dx^{2}}$ in $L^{2}[0,\infty)$?",How to show  is a cyclic vector for  in ?,"e^{-x} -\frac{d^{2}}{dx^{2}} L^{2}[0,\infty)","Let $\mathcal{H}=L^{2}[0,\infty)$. How can one easily show that $e^{-x}$ is a cyclic vector under the $C^{\star}$ subalgebra of operators on $\mathcal{L}(H)$ generated by all resolvents $(L-\lambda I)^{-1}$ of the selfadjoint operator $$       Lf=-\frac{d^{2}}{dx^{2}}f $$ whose domain $\mathcal{D}(L)$ consists of all twice absolutely continuous functions $f\in L^{2}[0,\infty)$ for which $f'' \in L^{2}[0,\infty)$ and $f(0)=0$. What does the cyclic representation look like?","Let $\mathcal{H}=L^{2}[0,\infty)$. How can one easily show that $e^{-x}$ is a cyclic vector under the $C^{\star}$ subalgebra of operators on $\mathcal{L}(H)$ generated by all resolvents $(L-\lambda I)^{-1}$ of the selfadjoint operator $$       Lf=-\frac{d^{2}}{dx^{2}}f $$ whose domain $\mathcal{D}(L)$ consists of all twice absolutely continuous functions $f\in L^{2}[0,\infty)$ for which $f'' \in L^{2}[0,\infty)$ and $f(0)=0$. What does the cyclic representation look like?",,"['functional-analysis', 'operator-theory', 'spectral-theory']"
8,"Is $\mathcal{B}(H)$ complemented in $\ell_\infty(I, H)$",Is  complemented in,"\mathcal{B}(H) \ell_\infty(I, H)","Let $H$ be an infinite diensional Hilbert space. Consider unit ball of $H$ as index set, denote it by $I$, then we have an isometric embedding $$ j:\mathcal{B}(H)\to\ell_\infty(I,H):T\mapsto(T(i))_{i\in I} $$ So we have a copy of $\mathcal{B}(H)$ in $\ell_\infty(I,H)$. Is this copy of $\mathcal{B}(H)$ complemented in $\ell_\infty(I,H)$? I believe it is not, but I can't prove.","Let $H$ be an infinite diensional Hilbert space. Consider unit ball of $H$ as index set, denote it by $I$, then we have an isometric embedding $$ j:\mathcal{B}(H)\to\ell_\infty(I,H):T\mapsto(T(i))_{i\in I} $$ So we have a copy of $\mathcal{B}(H)$ in $\ell_\infty(I,H)$. Is this copy of $\mathcal{B}(H)$ complemented in $\ell_\infty(I,H)$? I believe it is not, but I can't prove.",,"['functional-analysis', 'banach-spaces']"
9,If any two norms on a vector space are equivalent then the space is finite-dimensional [duplicate],If any two norms on a vector space are equivalent then the space is finite-dimensional [duplicate],,This question already has answers here : Understanding of the theorem that all norms are equivalent in finite dimensional vector spaces (8 answers) Closed 9 years ago . I need to prove: If any two norms on a vector space are equivalent then the space is finite-dimensional. I am aware of the converse of this result that on a finite dimensional vector space any two norms are equivalent. Any kind of hint is appreciated. Thanks in advance!,This question already has answers here : Understanding of the theorem that all norms are equivalent in finite dimensional vector spaces (8 answers) Closed 9 years ago . I need to prove: If any two norms on a vector space are equivalent then the space is finite-dimensional. I am aware of the converse of this result that on a finite dimensional vector space any two norms are equivalent. Any kind of hint is appreciated. Thanks in advance!,,"['functional-analysis', 'normed-spaces']"
10,"For an inductive limit $X = \bigcup X_n$ of vector spaces, show that $X$ is complete if $X_n$ is complete for all $n$","For an inductive limit  of vector spaces, show that  is complete if  is complete for all",X = \bigcup X_n X X_n n,"Let $X$ be a vector space. Suppose that $\{X_n\}_{n=1}^\infty$ is a sequence of vector subspaces such that $X_n \subseteq X_{n+1}$ for all $n$, Each $X_n$ is a locally convex topological vector space, and the topology on $X_n$ coincides exactly with the subspace topology that it inherits for $X_{n+1}$. Then we can definie the so-called inductive limit topology on $X$, which is the strongest locally convex, topological vector space topology for which the injections $X_n \to X$ are all continuous. I would like to prove that if $X_n$ is complete for all $n$, then $X$ is complete. This fact is stated in Reed and Simon's Methods of Modern Mathematical Physics, Vol.1 . Reed and Simon give a reference for the proof: A. and W. Robertson's Topological Vector Spaces . Unfortunately, this text is not available at my University library. Here's what I have so far. Suppose that $\{x_n\}_{n=1}^\infty$ is Cauchy in $X$. First of all, consider the case when there exists some index $N$ so that $X_N \cap \{x_n\}$ is infinite. Then we can choose some subsequence $\{x_{n_k}\}_{k=1}^\infty$ so that$\{x_{n_k}\} \subseteq X_N$. Next, Let $U$ be an arbitrary balanced, convex, basis-neighborhood of $0$ in $X_N$. Using this lemma, it is possible to lift $U$ to a balanced, convex neighborhood $\tilde{U}$ of $0$ in $X$, with the property that $X_N \cap\tilde{U} = U$. Because $\{x_n\}$ is Cauchy in $X$, there is an $M \in \mathbb{N}$ so that $$n,m \ge M \implies x_n -x_m \in \tilde{U}.$$ This in turn implies that $$k,j \ge M \implies x_{n_k} -x_{n_j} \in \tilde{U} \cap X_N = U.$$ Therefore $\{x_{n_k}\}$ is Cauchy in $X_N$. By the completeness of $X_N$, there exists some $x \in X_n$ so that $x_{n_k} \to x$. To finish this case, we notice that: $$x_n - x = (x_n - x_{n_k}) + (x_{n_k} - x)$$ So if $n$ and $k$ are made large enough, $x_n - x$ will reside inside any neighborhood $U$ of $0$ (to make this last step totally clear, we do need to recall the fact that, within any open any $V$ containing zero in $X$, we can find an open neighborhood $\tilde{V}$ of $0$ so that $\tilde{V} +\tilde{V} \subseteq V$). I'm not sure how to complete the proof in the case where $\{x_n\} \cap X_N$ is finite for all $N$. In that case, I'm not able to extract the subsequence $x_{n_k}$, which was the key to the proof above. Hints or solutions are greatly appreciated.","Let $X$ be a vector space. Suppose that $\{X_n\}_{n=1}^\infty$ is a sequence of vector subspaces such that $X_n \subseteq X_{n+1}$ for all $n$, Each $X_n$ is a locally convex topological vector space, and the topology on $X_n$ coincides exactly with the subspace topology that it inherits for $X_{n+1}$. Then we can definie the so-called inductive limit topology on $X$, which is the strongest locally convex, topological vector space topology for which the injections $X_n \to X$ are all continuous. I would like to prove that if $X_n$ is complete for all $n$, then $X$ is complete. This fact is stated in Reed and Simon's Methods of Modern Mathematical Physics, Vol.1 . Reed and Simon give a reference for the proof: A. and W. Robertson's Topological Vector Spaces . Unfortunately, this text is not available at my University library. Here's what I have so far. Suppose that $\{x_n\}_{n=1}^\infty$ is Cauchy in $X$. First of all, consider the case when there exists some index $N$ so that $X_N \cap \{x_n\}$ is infinite. Then we can choose some subsequence $\{x_{n_k}\}_{k=1}^\infty$ so that$\{x_{n_k}\} \subseteq X_N$. Next, Let $U$ be an arbitrary balanced, convex, basis-neighborhood of $0$ in $X_N$. Using this lemma, it is possible to lift $U$ to a balanced, convex neighborhood $\tilde{U}$ of $0$ in $X$, with the property that $X_N \cap\tilde{U} = U$. Because $\{x_n\}$ is Cauchy in $X$, there is an $M \in \mathbb{N}$ so that $$n,m \ge M \implies x_n -x_m \in \tilde{U}.$$ This in turn implies that $$k,j \ge M \implies x_{n_k} -x_{n_j} \in \tilde{U} \cap X_N = U.$$ Therefore $\{x_{n_k}\}$ is Cauchy in $X_N$. By the completeness of $X_N$, there exists some $x \in X_n$ so that $x_{n_k} \to x$. To finish this case, we notice that: $$x_n - x = (x_n - x_{n_k}) + (x_{n_k} - x)$$ So if $n$ and $k$ are made large enough, $x_n - x$ will reside inside any neighborhood $U$ of $0$ (to make this last step totally clear, we do need to recall the fact that, within any open any $V$ containing zero in $X$, we can find an open neighborhood $\tilde{V}$ of $0$ so that $\tilde{V} +\tilde{V} \subseteq V$). I'm not sure how to complete the proof in the case where $\{x_n\} \cap X_N$ is finite for all $N$. In that case, I'm not able to extract the subsequence $x_{n_k}$, which was the key to the proof above. Hints or solutions are greatly appreciated.",,"['functional-analysis', 'topological-vector-spaces']"
11,"On the spectrum of a product in a Banach algebra, in specific case","On the spectrum of a product in a Banach algebra, in specific case",,"Let $A$ be a Banach algebra, and suppose that $a,b\in A$ have spectra that satisfy:  $\sigma(a) \subset U$, and $\sigma(b)\subset U$,  where $U$ is the open right half-plane of complex numbers with positive real part. Is is true that $\sigma(ab)$ does not contain any element of the form $-r$ for $r\geq 0$ ? That's obviously the case when $a$ and $b$ commute, but I can't find a counterexample in the noncommutative case.","Let $A$ be a Banach algebra, and suppose that $a,b\in A$ have spectra that satisfy:  $\sigma(a) \subset U$, and $\sigma(b)\subset U$,  where $U$ is the open right half-plane of complex numbers with positive real part. Is is true that $\sigma(ab)$ does not contain any element of the form $-r$ for $r\geq 0$ ? That's obviously the case when $a$ and $b$ commute, but I can't find a counterexample in the noncommutative case.",,"['functional-analysis', 'banach-algebras']"
12,An inequality for inner product space: $\|x-z\|.\|y-t\|\leq \|x-y\|.\|z-t\|+\|y-z\|.\|x-t\|$,An inequality for inner product space:,\|x-z\|.\|y-t\|\leq \|x-y\|.\|z-t\|+\|y-z\|.\|x-t\|,In a inner product space show that the following inequality holds. $\|x-z\|.\|y-t\|\leq \|x-y\|.\|z-t\|+\|y-z\|.\|x-t\|$ I am stuck in proving this inequality,In a inner product space show that the following inequality holds. $\|x-z\|.\|y-t\|\leq \|x-y\|.\|z-t\|+\|y-z\|.\|x-t\|$ I am stuck in proving this inequality,,"['functional-analysis', 'inequality', 'inner-products']"
13,"If $P$ has marginals $P_1, P_2$, is $L^1(P_1) + L^1(P_2)$ closed in $L^1(P)$?","If  has marginals , is  closed in ?","P P_1, P_2 L^1(P_1) + L^1(P_2) L^1(P)","Suppose that  $\mathbb{X}=\mathbb{X}_1\times \mathbb{X}_2$ and suppose that $ P$ is a probability measure on  $\mathbb{X}$ with marginals $ P_i$ on $\mathbb{X}_i, i=1,2$, i.e., $$\int f_i(x_i)\, dP=\int f_i(x_i)\, dP_i \,\,\,\,\text{for  }i=1,2,$$ for $P_i$ intergrable functions $f_i(x_i)$, $i=1,2$ (looking at them as functions of $(x_1,x_2)$). Is the linear space spanned by $L^1(P_1)\cup L^1(P_2)$ ($L^1(P_1) + L^1(P_2)$) closed in $L^1(P)$?","Suppose that  $\mathbb{X}=\mathbb{X}_1\times \mathbb{X}_2$ and suppose that $ P$ is a probability measure on  $\mathbb{X}$ with marginals $ P_i$ on $\mathbb{X}_i, i=1,2$, i.e., $$\int f_i(x_i)\, dP=\int f_i(x_i)\, dP_i \,\,\,\,\text{for  }i=1,2,$$ for $P_i$ intergrable functions $f_i(x_i)$, $i=1,2$ (looking at them as functions of $(x_1,x_2)$). Is the linear space spanned by $L^1(P_1)\cup L^1(P_2)$ ($L^1(P_1) + L^1(P_2)$) closed in $L^1(P)$?",,"['functional-analysis', 'banach-spaces', 'lp-spaces']"
14,restriction of unitary operator is unitary?,restriction of unitary operator is unitary?,,"Let $\mathcal{U}: \mathcal{H} \rightarrow \mathcal{H}$ be a unitary operator on a Hilbert space $\mathcal{H}$.  If $\mathcal{K}\subset \mathcal{H}$ is a closed subspace such that $\mathcal{U}(\mathcal{K})\subset \mathcal{K}$, is it necessarily the case that $\mathcal{U}(\mathcal{K})=\mathcal{K}$?    Thank you.","Let $\mathcal{U}: \mathcal{H} \rightarrow \mathcal{H}$ be a unitary operator on a Hilbert space $\mathcal{H}$.  If $\mathcal{K}\subset \mathcal{H}$ is a closed subspace such that $\mathcal{U}(\mathcal{K})\subset \mathcal{K}$, is it necessarily the case that $\mathcal{U}(\mathcal{K})=\mathcal{K}$?    Thank you.",,"['analysis', 'functional-analysis', 'representation-theory', 'harmonic-analysis']"
15,Caught in the net,Caught in the net,,"I'm reading through some notes one locally convex spaces (""lcs"" from now on) analysis and there the following version of the Banach-Steinhaus theorem is given Theorem (Banach-Steinhaus) $\quad$ The pointwise limit  of a sequence of continuous, linear mappings from a barrelled lcs $U$ to a lcs $V$ is again a continuous, linear mapping. followed by the remark If we replace ""sequence"" with ""net"" this needn't be the case: For a   discontinuous functional $f:U\rightarrow \mathbb{K}$ we can construct   for each subspace $W\subseteq U$ a continuous, linear functional $F_W$   such that $f\big|_W=F_W\big|_W$. Can someone explain, or give me a hint, how to make this construction from the last sentence above explicit ? I'm also not sure how to use this to obtain a counterexample to the theorem above ? I somehow can't think of a way to make use of a point of discontinuity in $x_0\in U$ of $f$ to show that the net $(F_W)_W$ doesn't converge at all at $x_0$ (at least I intuitively think that this is the case - opposed to that the net indeed converges everywhere, but not to a continuous, linear functional).","I'm reading through some notes one locally convex spaces (""lcs"" from now on) analysis and there the following version of the Banach-Steinhaus theorem is given Theorem (Banach-Steinhaus) $\quad$ The pointwise limit  of a sequence of continuous, linear mappings from a barrelled lcs $U$ to a lcs $V$ is again a continuous, linear mapping. followed by the remark If we replace ""sequence"" with ""net"" this needn't be the case: For a   discontinuous functional $f:U\rightarrow \mathbb{K}$ we can construct   for each subspace $W\subseteq U$ a continuous, linear functional $F_W$   such that $f\big|_W=F_W\big|_W$. Can someone explain, or give me a hint, how to make this construction from the last sentence above explicit ? I'm also not sure how to use this to obtain a counterexample to the theorem above ? I somehow can't think of a way to make use of a point of discontinuity in $x_0\in U$ of $f$ to show that the net $(F_W)_W$ doesn't converge at all at $x_0$ (at least I intuitively think that this is the case - opposed to that the net indeed converges everywhere, but not to a continuous, linear functional).",,"['functional-analysis', 'examples-counterexamples']"
16,First eigenvalue of Laplacian and Poincaré inequality,First eigenvalue of Laplacian and Poincaré inequality,,"Any idea on how to solve: $\int_{\Omega} |\nabla u|^2 d^n x=\lambda_1\int_{\Omega}u^2 d^n x$, with $u\in H^1_0(\Omega)$ and $\Omega\subset\mathbb{R}^n$, and $\lambda_1$ the first eigenvalue of the negative Laplacian with Dirichlet boundary conditions on $\Omega$ - I am suspecting $u$ is the eigenfunction corresponding to $\lambda_1$, but I am looking for a rigorous proof.","Any idea on how to solve: $\int_{\Omega} |\nabla u|^2 d^n x=\lambda_1\int_{\Omega}u^2 d^n x$, with $u\in H^1_0(\Omega)$ and $\Omega\subset\mathbb{R}^n$, and $\lambda_1$ the first eigenvalue of the negative Laplacian with Dirichlet boundary conditions on $\Omega$ - I am suspecting $u$ is the eigenfunction corresponding to $\lambda_1$, but I am looking for a rigorous proof.",,"['functional-analysis', 'ordinary-differential-equations', 'sobolev-spaces']"
17,Countable separating set of a von Neumann algebra,Countable separating set of a von Neumann algebra,,"I studied the following proposition , but I have a question in the proof of this proposition. Let $A$ be a von Neumann algebra. If $A$ possesses a countable separating set then show that $A$ is $\sigma- $ finite. Proof: Let $M$ be a countable separating set for $A$. Let $\{E_i\}_{i\in I}$ be a pairwise disjoint family of projections of $A$. Fr every $x\in M$, we have $E_ix=0$ for all but countably many of the indices $i$... My question: How can we say for countably many of the indices $i$,   $E_ix\neq 0$ ?","I studied the following proposition , but I have a question in the proof of this proposition. Let $A$ be a von Neumann algebra. If $A$ possesses a countable separating set then show that $A$ is $\sigma- $ finite. Proof: Let $M$ be a countable separating set for $A$. Let $\{E_i\}_{i\in I}$ be a pairwise disjoint family of projections of $A$. Fr every $x\in M$, we have $E_ix=0$ for all but countably many of the indices $i$... My question: How can we say for countably many of the indices $i$,   $E_ix\neq 0$ ?",,"['functional-analysis', 'von-neumann-algebras']"
18,"norm of differential operator on $P^n[0,1]$",norm of differential operator on,"P^n[0,1]","Consider the space $P^n[0,1]$ of polynomials of degree $\leq n$ on $[0,1]$, equipped with the sup norm. Now, this is a finite dimensional space, so all linear operators have to be continuous, hence bounded. My question is: what is the norm of the differential operator $d/dx : P^n[0,1] \rightarrow P^{n-1}[0,1]$? I can't see the relation between the supremum of a polynomial $p(x)=a_0+\dots+a_nx^n$ and the supremum of $p'(x)=a_1 + 2a_2 x +\dots+n a_n x^{n-1}$.","Consider the space $P^n[0,1]$ of polynomials of degree $\leq n$ on $[0,1]$, equipped with the sup norm. Now, this is a finite dimensional space, so all linear operators have to be continuous, hence bounded. My question is: what is the norm of the differential operator $d/dx : P^n[0,1] \rightarrow P^{n-1}[0,1]$? I can't see the relation between the supremum of a polynomial $p(x)=a_0+\dots+a_nx^n$ and the supremum of $p'(x)=a_1 + 2a_2 x +\dots+n a_n x^{n-1}$.",,"['functional-analysis', 'polynomials']"
19,Doubt about Proposition 2.39 in Dana Williams' crossed product book,Doubt about Proposition 2.39 in Dana Williams' crossed product book,,"You can see the proposition in a google books preview here . First and foremost, my question is: Question: Am I correct to interpret Proposition 2.39 as setting up a bijective correspondence between The set of nondegenerate covariant homomorphisms  $(\pi,u) : (A,G,\alpha) \to \mathcal{L}(X)$. The set of nondegenerate homomorphisms $L : A \rtimes_\alpha G \to \mathcal{L}(X)$. by sending each $(\pi,u)$ to $L = \pi \rtimes u$ and sending each $L$ to the $(\pi,u)$ defined by $\pi(a) = \overline{L}(i_A(a)), u_s = \overline{L}(i_G(s))$ for all $a \in A, s \in G$. I don't really have a mathematical reason to doubt this reading is correct. My reasons are slightly meta, so I hope I am making myself understood. The immediately following Proposition 2.40 would seem, in part, to be a corollary, where the Hilbert $B$-module $X$ is taken to be a Hilbert space. Now, in Proposition 2.40 Williams states explicitly that a bijective correspondence is being set up, whereas in Proposition 2.39 this is only implicit. That in and of itself would not be enough to raise my eyebrows, but, have a look at the 1st paragraph of the proof of Proposition 2.40. ""Proposition 2.39 on the facing page shows that the map $(\pi,U) \mapsto \pi \rtimes U$ is a surjection. It's one-to-one in view of Equations (2.21) and (2.27)."" I don't understand the need for the reference to equations (2.21) and (2.27). Does Proposition 2.40 not already show we have a bijection?","You can see the proposition in a google books preview here . First and foremost, my question is: Question: Am I correct to interpret Proposition 2.39 as setting up a bijective correspondence between The set of nondegenerate covariant homomorphisms  $(\pi,u) : (A,G,\alpha) \to \mathcal{L}(X)$. The set of nondegenerate homomorphisms $L : A \rtimes_\alpha G \to \mathcal{L}(X)$. by sending each $(\pi,u)$ to $L = \pi \rtimes u$ and sending each $L$ to the $(\pi,u)$ defined by $\pi(a) = \overline{L}(i_A(a)), u_s = \overline{L}(i_G(s))$ for all $a \in A, s \in G$. I don't really have a mathematical reason to doubt this reading is correct. My reasons are slightly meta, so I hope I am making myself understood. The immediately following Proposition 2.40 would seem, in part, to be a corollary, where the Hilbert $B$-module $X$ is taken to be a Hilbert space. Now, in Proposition 2.40 Williams states explicitly that a bijective correspondence is being set up, whereas in Proposition 2.39 this is only implicit. That in and of itself would not be enough to raise my eyebrows, but, have a look at the 1st paragraph of the proof of Proposition 2.40. ""Proposition 2.39 on the facing page shows that the map $(\pi,U) \mapsto \pi \rtimes U$ is a surjection. It's one-to-one in view of Equations (2.21) and (2.27)."" I don't understand the need for the reference to equations (2.21) and (2.27). Does Proposition 2.40 not already show we have a bijection?",,"['functional-analysis', 'representation-theory', 'operator-theory', 'c-star-algebras']"
20,$L^1(\mathbb{R}^n)$ functions not in $\mathcal{H}^1(\mathbb{R}^n)$,functions not in,L^1(\mathbb{R}^n) \mathcal{H}^1(\mathbb{R}^n),"I am wondering how to imagine the Hardy space on $\mathcal{H}^1(\mathbb{R}^n)$ and in particular what sort of functions are in $L^1(\mathbb{R}^n)\backslash\mathcal{H}^1(\mathbb{R}^n)$. Furthermore, is it possible to find explicit examples of functions in $\partial (L^1(\mathbb{R}^n)\backslash\mathcal{H}^1(\mathbb{R}^n))$? $\mathcal{H}^p(\mathbb{R}^n)=L^p(\mathbb{R}^n)$ for $p>1$, but what is the ""problem""/ difference when $p=1$ ( or $p\leq 1)$? Thanks!","I am wondering how to imagine the Hardy space on $\mathcal{H}^1(\mathbb{R}^n)$ and in particular what sort of functions are in $L^1(\mathbb{R}^n)\backslash\mathcal{H}^1(\mathbb{R}^n)$. Furthermore, is it possible to find explicit examples of functions in $\partial (L^1(\mathbb{R}^n)\backslash\mathcal{H}^1(\mathbb{R}^n))$? $\mathcal{H}^p(\mathbb{R}^n)=L^p(\mathbb{R}^n)$ for $p>1$, but what is the ""problem""/ difference when $p=1$ ( or $p\leq 1)$? Thanks!",,"['functional-analysis', 'examples-counterexamples', 'lp-spaces', 'hardy-spaces']"
21,When is it possible to construct ladder operators for a given Hamiltonian?,When is it possible to construct ladder operators for a given Hamiltonian?,,"It is pretty cool (in my opinion) that one can solve Schrödinger's equation for the harmonic oscillator by using ladder operators, rather than just integrating it. In particular, it is possible to extract the spectrum of the hamiltonian without almost any calculus at all ( => this method is called: algebraic method). My question is now mathematical: When is it possible to find such ladder operators for a given Hamiltonian and how do I find them? Cause so far, the operators where always given in advance and just used for the harmonic oscillator(or very similar potentials). Probably one needs a positive and somehow well-behaving spectrum of the Hamiltonian, but what else? Any hint is interesting!","It is pretty cool (in my opinion) that one can solve Schrödinger's equation for the harmonic oscillator by using ladder operators, rather than just integrating it. In particular, it is possible to extract the spectrum of the hamiltonian without almost any calculus at all ( => this method is called: algebraic method). My question is now mathematical: When is it possible to find such ladder operators for a given Hamiltonian and how do I find them? Cause so far, the operators where always given in advance and just used for the harmonic oscillator(or very similar potentials). Probably one needs a positive and somehow well-behaving spectrum of the Hamiltonian, but what else? Any hint is interesting!",,"['real-analysis', 'functional-analysis']"
22,Non linear compact map,Non linear compact map,,"Suppose to have two Banach spaces $E$ and $F$, with $E$ reflexive. Suppose to have a continuous map $T:E \to F$ which maps bounded subsets into precompact subsets. $T$ is not assumed to be linear. Finally, suppose to have a sequence $(e_n)_n\subset E$ weakly convergent to $e\in E$. Then, up to subsequences, $T(e_n)$ converges strongly to $T(e)$. Is it correct? How can I prove it? Can I drop the reflexivity assumption?","Suppose to have two Banach spaces $E$ and $F$, with $E$ reflexive. Suppose to have a continuous map $T:E \to F$ which maps bounded subsets into precompact subsets. $T$ is not assumed to be linear. Finally, suppose to have a sequence $(e_n)_n\subset E$ weakly convergent to $e\in E$. Then, up to subsequences, $T(e_n)$ converges strongly to $T(e)$. Is it correct? How can I prove it? Can I drop the reflexivity assumption?",,"['functional-analysis', 'banach-spaces', 'compact-operators']"
23,"In a normed space, the sum of a Closed Operator and a Bounded Operator is a Closed Operator.","In a normed space, the sum of a Closed Operator and a Bounded Operator is a Closed Operator.",,"The book "" Introductory Functional Analysis with Applications "" (Kreyszig) presents the following lemma Let $T:\mathcal{D}(T)\to Y$ be a bounded linear operator with domain $\mathcal{D}(T)\subset X$ , where $X$ and $Y$ are normed spaces. Then: (a) If $\mathcal{D}(T)$ is a closed subset of $X$ , then $T$ is closed. (b) If $T$ is closed and $Y$ is complete, then $\mathcal{D}(T)$ is a closed subset of $X$ . (page 295) and the following problem Let $X$ and $Y$ be normed spaces. If $T_1:X\to Y$ is a closed linear operator and $T_2\in\mathcal{B}(X,Y)$ , show that $T_1 + T_2$ is a closed linear operator. (page 296) If $X$ and $Y$ are Banach spaces, then we can conclude (by Closed Graph Theorem) that $T_1$ is bounded. So, $(T_1+T_2):X\to Y$ is bounded and thus it's closed (by lemma above). My question is: how to solve it when $X$ and $Y$ are not necessarily complete spaces? Thanks.","The book "" Introductory Functional Analysis with Applications "" (Kreyszig) presents the following lemma Let be a bounded linear operator with domain , where and are normed spaces. Then: (a) If is a closed subset of , then is closed. (b) If is closed and is complete, then is a closed subset of . (page 295) and the following problem Let and be normed spaces. If is a closed linear operator and , show that is a closed linear operator. (page 296) If and are Banach spaces, then we can conclude (by Closed Graph Theorem) that is bounded. So, is bounded and thus it's closed (by lemma above). My question is: how to solve it when and are not necessarily complete spaces? Thanks.","T:\mathcal{D}(T)\to Y \mathcal{D}(T)\subset X X Y \mathcal{D}(T) X T T Y \mathcal{D}(T) X X Y T_1:X\to Y T_2\in\mathcal{B}(X,Y) T_1 + T_2 X Y T_1 (T_1+T_2):X\to Y X Y","['functional-analysis', 'banach-spaces', 'normed-spaces']"
24,What is Newton's theorem?,What is Newton's theorem?,,I'm reading a paper about mathematical physics at the moment and am wondering about the following: Let $w\colon\mathbb{R}^2\to\mathbb{R}$ be defined by $w(x)=-\log|x|$ and $\mu\colon\mathbb{R}^2\to\mathbb{R}$ be some non-negative integrable function radially symmetric about $x_0\in\mathbb{R}^2$ such that $\int\mu(x)~dx=1$. Then by Newton's theorem $$(\mu\ast w)(x)\leq w(x)\text{ for a.e. }x.$$ What are they referring to?,I'm reading a paper about mathematical physics at the moment and am wondering about the following: Let $w\colon\mathbb{R}^2\to\mathbb{R}$ be defined by $w(x)=-\log|x|$ and $\mu\colon\mathbb{R}^2\to\mathbb{R}$ be some non-negative integrable function radially symmetric about $x_0\in\mathbb{R}^2$ such that $\int\mu(x)~dx=1$. Then by Newton's theorem $$(\mu\ast w)(x)\leq w(x)\text{ for a.e. }x.$$ What are they referring to?,,"['functional-analysis', 'reference-request', 'mathematical-physics']"
25,About measurability of operators,About measurability of operators,,"I'm triyng without success, to find some examples of functions that: $\bullet$Are WOT-measurable, but not SOT-measurable. $\bullet$Are SOT-measurable, but not $||\cdot||$-measurable. I give the definitions I'm dealing with. We have: $X=\cal{L}$($E_1$,$E_2$) $f:\Omega\to X$$\;,\;\;$$(X,\Sigma,\mu)$ measure space. $\bullet\; ||\cdot||_{X}$-measurable: $\exists s_n:\Omega\to X$, simple, and $\exists A\in\Sigma,\;\mu(A)=0$, with $||s_n(w)-f(w)||_{X}\xrightarrow[n\to\infty]{}0\;\;\forall w\notin A.$ $\bullet\;$ SOT-measurable: $\forall e_1\in E_1$, the function $w\to f(w)(e_1)\;\;$ ($\Omega\to E_2$) is $||\cdot||_{E_2}$-measurable. $\bullet\;$ WOT-measurable: $\forall e_1\in E_1$, and $\forall e_2^{}*\in E_2^{*}$, the function $w\to <f(w)(e_1),e_2^{*}>\;\;$ ($\Omega\to \mathbb{K}$) is measurable (in the usual sense). Thanks a lot for any help finding those examples.","I'm triyng without success, to find some examples of functions that: $\bullet$Are WOT-measurable, but not SOT-measurable. $\bullet$Are SOT-measurable, but not $||\cdot||$-measurable. I give the definitions I'm dealing with. We have: $X=\cal{L}$($E_1$,$E_2$) $f:\Omega\to X$$\;,\;\;$$(X,\Sigma,\mu)$ measure space. $\bullet\; ||\cdot||_{X}$-measurable: $\exists s_n:\Omega\to X$, simple, and $\exists A\in\Sigma,\;\mu(A)=0$, with $||s_n(w)-f(w)||_{X}\xrightarrow[n\to\infty]{}0\;\;\forall w\notin A.$ $\bullet\;$ SOT-measurable: $\forall e_1\in E_1$, the function $w\to f(w)(e_1)\;\;$ ($\Omega\to E_2$) is $||\cdot||_{E_2}$-measurable. $\bullet\;$ WOT-measurable: $\forall e_1\in E_1$, and $\forall e_2^{}*\in E_2^{*}$, the function $w\to <f(w)(e_1),e_2^{*}>\;\;$ ($\Omega\to \mathbb{K}$) is measurable (in the usual sense). Thanks a lot for any help finding those examples.",,"['functional-analysis', 'operator-theory', 'banach-spaces']"
26,Equicontinuity and Uniform Boundedness,Equicontinuity and Uniform Boundedness,,"If we have a sequence of smooth functions $\{f_{n}\}_{n}$ where $f_{n}: U \rightarrow \mathbb{R}$, where $U \subset \mathbb{R}^{n}$. We are given the following two results: For $x \in U$ we have $|f_{n}(x)| < \infty$ for all $n=1,2,...$ and also similarly $|Df_{n}(x)| < \infty$ for all $n=1,2,...$ then how does it follow that $\{f_{n}\}_{n}$ is uniformly bounded and equicontinuous. Note that $Df_{n}$ is the gradient vector. The uniform boundedness seems to follow directly from $|f_{n}(x)| < \infty$ for all $n=1,2,...$ and all $x$, but I can't see how equicontinuity follows, maybe I'm missing some result that is used? Thanks for any assistance, let me know if something is unclear.","If we have a sequence of smooth functions $\{f_{n}\}_{n}$ where $f_{n}: U \rightarrow \mathbb{R}$, where $U \subset \mathbb{R}^{n}$. We are given the following two results: For $x \in U$ we have $|f_{n}(x)| < \infty$ for all $n=1,2,...$ and also similarly $|Df_{n}(x)| < \infty$ for all $n=1,2,...$ then how does it follow that $\{f_{n}\}_{n}$ is uniformly bounded and equicontinuous. Note that $Df_{n}$ is the gradient vector. The uniform boundedness seems to follow directly from $|f_{n}(x)| < \infty$ for all $n=1,2,...$ and all $x$, but I can't see how equicontinuity follows, maybe I'm missing some result that is used? Thanks for any assistance, let me know if something is unclear.",,['functional-analysis']
27,Two question on a lemma about C*-algebra,Two question on a lemma about C*-algebra,,"I am reading Lin Hua xin's book ""An introduction to the classification of amenable C*-algebras"" and i am confused with the lemma 1.7.12 in this book. Lemma 1.7.12 Let $A$ be a C*-algebra and $f\in A^{\ast}$ be self-adjoint. Then there are vectors $\xi$, $\eta\in H_{U}$ with $||\xi||^{2} ||\eta||^{2}\leq||f||$ such that $f(a)=\langle\pi_{U}(a)(\xi), \eta\rangle$ for all $a\in A$. (Here, $\pi_{U}(a):A\rightarrow B(H_{U})$ is the universal representation.) Proof. To save notation, we may assume that $||f||=1$. By non-commutative Jordan decomposition theorem, there are positive linear functionals $f_{j}$, $j=1, 2$, such that $f=f_{1}-f_{2}$ and $||f||=||f_{1}||+||f_{2}||$. Each $f_{j}$ is a positive scalar multiple of state on $A$. Therefore there are mutually orthogonal vectors $\xi_{j}\in H_{U} (j=1,2)$ such that $||\xi_{j}||^{2}=||f_{j}||$ and $f_{j}(a)=\langle \pi_{U}(a)(\xi_{j}), \xi_{j}\rangle, j=1,2$. Set $\xi=\xi_{1}\oplus\xi_{2}$ and $\eta=\xi_{1}\oplus(-\xi_{2})$. Then $f(a)=\langle \pi_{U}(a)\xi, \eta\rangle$ for all $a\in A$. My question are: Why there exist mutually orthogonal vectors (why are they orthogonal?) $\xi_{j}\in H_{U} (j=1,2)$ such that $||\xi_{j}||^{2}=||f_{j}||$(why this normed equation hold?) and $f_{j}(a)=\langle \pi_{U}(a)(\xi_{j}), \xi_{j}\rangle, j=1,2$? If we set $\xi=\xi_{1}\oplus\xi_{2}$ and $\eta=\xi_{1}\oplus(-\xi_{2})$, how to conclude that $f(a)=\langle \pi_{U}(a)\xi, \eta\rangle$ for all $a\in A$?","I am reading Lin Hua xin's book ""An introduction to the classification of amenable C*-algebras"" and i am confused with the lemma 1.7.12 in this book. Lemma 1.7.12 Let $A$ be a C*-algebra and $f\in A^{\ast}$ be self-adjoint. Then there are vectors $\xi$, $\eta\in H_{U}$ with $||\xi||^{2} ||\eta||^{2}\leq||f||$ such that $f(a)=\langle\pi_{U}(a)(\xi), \eta\rangle$ for all $a\in A$. (Here, $\pi_{U}(a):A\rightarrow B(H_{U})$ is the universal representation.) Proof. To save notation, we may assume that $||f||=1$. By non-commutative Jordan decomposition theorem, there are positive linear functionals $f_{j}$, $j=1, 2$, such that $f=f_{1}-f_{2}$ and $||f||=||f_{1}||+||f_{2}||$. Each $f_{j}$ is a positive scalar multiple of state on $A$. Therefore there are mutually orthogonal vectors $\xi_{j}\in H_{U} (j=1,2)$ such that $||\xi_{j}||^{2}=||f_{j}||$ and $f_{j}(a)=\langle \pi_{U}(a)(\xi_{j}), \xi_{j}\rangle, j=1,2$. Set $\xi=\xi_{1}\oplus\xi_{2}$ and $\eta=\xi_{1}\oplus(-\xi_{2})$. Then $f(a)=\langle \pi_{U}(a)\xi, \eta\rangle$ for all $a\in A$. My question are: Why there exist mutually orthogonal vectors (why are they orthogonal?) $\xi_{j}\in H_{U} (j=1,2)$ such that $||\xi_{j}||^{2}=||f_{j}||$(why this normed equation hold?) and $f_{j}(a)=\langle \pi_{U}(a)(\xi_{j}), \xi_{j}\rangle, j=1,2$? If we set $\xi=\xi_{1}\oplus\xi_{2}$ and $\eta=\xi_{1}\oplus(-\xi_{2})$, how to conclude that $f(a)=\langle \pi_{U}(a)\xi, \eta\rangle$ for all $a\in A$?",,"['functional-analysis', 'operator-algebras', 'c-star-algebras', 'von-neumann-algebras']"
28,"g continuous on [a,b] using intermediate value theorem","g continuous on [a,b] using intermediate value theorem",,"Suppose that g is continuous on an interval [a,b] and that  $g(x) ∈ [a,b]$ for all $x ∈ [a,b]$. (a) Use the intermediate value theorem to prove that is at least one number $c ∈ [a,b]$ with $g(c) = c$. Here is my attempt: Define $G: [a,b] \to $$\mathbb R$ by $G(x) = x - g(x).$  Then $G$ is continuous on $[a,b].$ Since $a  \leq g(a)\leq b$ and  $a  \leq g(b)\leq b$ we find $G(a) = a -g(a) \leq 0 $ and $G(b) = b -g(b) \geq 0 $. By the intermediate value theorem, there is a $c ∈ [a,b]$ such that $G(c) = 0$ or $c - g(c) = 0.$ Thus, $g(c) = c.$ (b) Suppose further that $g$ is differentiable and that there exists a $\lambda <1$ with  $|g'(x)|\leq \lambda$ for all $x ∈ [a,b]$. Prove that there is exactly one number $c ∈ [a,b]$ with $g(c) = c$. Here is my attempt: Since there exists two different fixed points $\xi<\xi'$. Then as stated we can use Lagrange's theorem on $[\xi,\xi']$: We get $$1=\frac{g(\xi)-g(\xi')}{\xi-\xi'}=f'(\nu)$$ for some  $\nu$, which contradicts $|f'(x)|<1$. (c) For any initial value $x_{0} ∈ [a,b]$, define a sequence ${x_{n}} = x_{0}, x_{2}, x_{3} ...$ by $x_n = g(x_{n-1})$ for $n \geq 0$, define $E_{n} = |x_{n}-c|$ and $D_{n} = |x_{n+1} - x_{n}|$. Here is my attempt: I suppose that we can say set $g(c)=c$ and $g'(c)=0$. Then let the sequence $x_{n} = g(x_{n-1}) = g(c) + g'(c)(x_{n-1} - c) +$ $ \dfrac{g''(Thi)}{2} (x_{n-1}-c)^{2}$ for Thi between c and x. For E_{n} $E_{n} = |x_{n}-c| = |g(x_{n-1} - g(c)| = $ $ \dfrac{g''(Thi)}{2} |x_{n-1}-c|^{2} =$ $\dfrac{g''(Thi)}{2} {E_{n-1}}^{2} $ For D_{n} $D_{n} = |x_{n+1}-X_{n}| = |g(x_{n} - g(X_{n})| = $ $ \dfrac{g''(Thi)}{2} |x_{n}-X_{n}|^{2} =$ $\dfrac{g''(Thi)}{2} {E_{n}}^{2} $ Is this how we would define this? If not, can you please show me? Out of curiosity from a question I just thought of. From question (c): Is it possible that we can prove that $E_{n} < \lambda^{n} E_{0}$ which would make $(x_{n})$ converge to $c$ and $D_{n} < \lambda^{n} D_{0}$. If so can you please show me how to? It seems good to know. (ii) Prove that for $n < k$, $|x_k - x_n|\leq $ $$ \dfrac{\lambda^{n} - \lambda^{k}}{1-\lambda}(D_{0}) $$ (iii) Prove that for $E_{n} \leq $ $$ \dfrac{\lambda^{n}}{1-\lambda}(D_{0}) $$ As you can probably tell, I learned most of this by myself. I am a self taught person who is trying to show some effort. I am sorry if this is not enough, but still can you show me the proofs to each section. I will be able to learn and understand them. Thanks to all the people that do help!","Suppose that g is continuous on an interval [a,b] and that  $g(x) ∈ [a,b]$ for all $x ∈ [a,b]$. (a) Use the intermediate value theorem to prove that is at least one number $c ∈ [a,b]$ with $g(c) = c$. Here is my attempt: Define $G: [a,b] \to $$\mathbb R$ by $G(x) = x - g(x).$  Then $G$ is continuous on $[a,b].$ Since $a  \leq g(a)\leq b$ and  $a  \leq g(b)\leq b$ we find $G(a) = a -g(a) \leq 0 $ and $G(b) = b -g(b) \geq 0 $. By the intermediate value theorem, there is a $c ∈ [a,b]$ such that $G(c) = 0$ or $c - g(c) = 0.$ Thus, $g(c) = c.$ (b) Suppose further that $g$ is differentiable and that there exists a $\lambda <1$ with  $|g'(x)|\leq \lambda$ for all $x ∈ [a,b]$. Prove that there is exactly one number $c ∈ [a,b]$ with $g(c) = c$. Here is my attempt: Since there exists two different fixed points $\xi<\xi'$. Then as stated we can use Lagrange's theorem on $[\xi,\xi']$: We get $$1=\frac{g(\xi)-g(\xi')}{\xi-\xi'}=f'(\nu)$$ for some  $\nu$, which contradicts $|f'(x)|<1$. (c) For any initial value $x_{0} ∈ [a,b]$, define a sequence ${x_{n}} = x_{0}, x_{2}, x_{3} ...$ by $x_n = g(x_{n-1})$ for $n \geq 0$, define $E_{n} = |x_{n}-c|$ and $D_{n} = |x_{n+1} - x_{n}|$. Here is my attempt: I suppose that we can say set $g(c)=c$ and $g'(c)=0$. Then let the sequence $x_{n} = g(x_{n-1}) = g(c) + g'(c)(x_{n-1} - c) +$ $ \dfrac{g''(Thi)}{2} (x_{n-1}-c)^{2}$ for Thi between c and x. For E_{n} $E_{n} = |x_{n}-c| = |g(x_{n-1} - g(c)| = $ $ \dfrac{g''(Thi)}{2} |x_{n-1}-c|^{2} =$ $\dfrac{g''(Thi)}{2} {E_{n-1}}^{2} $ For D_{n} $D_{n} = |x_{n+1}-X_{n}| = |g(x_{n} - g(X_{n})| = $ $ \dfrac{g''(Thi)}{2} |x_{n}-X_{n}|^{2} =$ $\dfrac{g''(Thi)}{2} {E_{n}}^{2} $ Is this how we would define this? If not, can you please show me? Out of curiosity from a question I just thought of. From question (c): Is it possible that we can prove that $E_{n} < \lambda^{n} E_{0}$ which would make $(x_{n})$ converge to $c$ and $D_{n} < \lambda^{n} D_{0}$. If so can you please show me how to? It seems good to know. (ii) Prove that for $n < k$, $|x_k - x_n|\leq $ $$ \dfrac{\lambda^{n} - \lambda^{k}}{1-\lambda}(D_{0}) $$ (iii) Prove that for $E_{n} \leq $ $$ \dfrac{\lambda^{n}}{1-\lambda}(D_{0}) $$ As you can probably tell, I learned most of this by myself. I am a self taught person who is trying to show some effort. I am sorry if this is not enough, but still can you show me the proofs to each section. I will be able to learn and understand them. Thanks to all the people that do help!",,"['calculus', 'real-analysis', 'functional-analysis', 'numerical-methods']"
29,Inequality in Schwartz space,Inequality in Schwartz space,,"I am trying to prove theorem 9.2 from book ""Lectures on Linear Partial Differential Equations"" wtitten by G. Eskin. In proof of this theorem is inequality which makes problems for me. Firstly I remind designation. Multi-index: $\alpha=(\alpha_{1}, \ldots , \alpha_{n}) \in \mathbb{Z}_{+}^{n}$, length of multi-index: $| \alpha | = \sum_{i=1}^{n} \alpha_{i}$, partial differential operator: $$D^{\alpha} = \frac{\partial ^{|\alpha|}}{\partial x_{1}^{\alpha_{1}} \ldots \partial x_{n}^{\alpha_{n}}}.$$ Moreover $|x|=\sqrt{x_1^2+ \ldots + x_n^2}$, Schwartz space: $$\mathcal{S}(\mathbb{R}^n) = \{f \in C^{\infty}(\mathbb{R}^n) \colon \sup_{x \in \mathbb{R}^{n}} (1+|x|)^{m}|D^{\beta}f(x)| < \infty, \: m \in \mathbb{N}, \: \beta \in \mathbb{Z}_{+}^{n} \}.$$  Seminorm in Schwartz space: $$ \|f\|_{m,\mathcal{S}} = \sup_{x \in \mathbb{R}^{n}} (1+|x|)^{m} \sum_{|\beta| = 0}^{m} |D^{\beta}f(x)|. $$ For every function $f \in \mathcal{S}(\mathbb{R}^n)$, for every multi-index $\beta \in \mathbb{Z}_{+}^{n}$ and for every $m \in \mathbb{N}$ we have \begin{equation*} (1+|x|)^{m}D^{\beta}f(x) = \int \limits_{-\infty}^{x_1} \cdots \int \limits_{-\infty}^{x_n} D^{(1,\ldots,1)}[(1+|y|)^{m}D^{\beta}f(y)] \; dy_n \ldots dy_1. \end{equation*} We need to show inequality. \begin{equation} \| f \|_{m,\mathcal{S}} \leq C_{1} \sum_{|\beta|=0}^{n+m} \int _{\mathbb{R}^n} (1+|y|)^m |D^{\beta} f(y)| \; dy, \quad f \in \mathcal{S}(\mathbb{R}^n). \end{equation} I used above properties, used Liebniz and I've got this: \begin{eqnarray*} \|f\|_{m,\mathcal{S}} &=& \sup_{x \in \mathbb{R}^{n}} (1+|x|)^{m} \sum_{|\beta| = 0}^{m} |D^{\beta}f(x)| = \sup_{x \in \mathbb{R}^{n}} \sum_{|\beta| = 0}^{m} \left| (1+|x|)^{m} D^{\beta}f(x) \right|\\ &=& \sup_{x \in \mathbb{R}^{n}} \sum_{|\beta| = 0}^{m} \left| \int \limits_{-\infty}^{x_1} \cdots \int \limits_{-\infty}^{x_n} D^{(1,\ldots,1)}[(1+|y|)^{m}D^{\beta}f(y)] \; dy_n \ldots dy_1  \right|\\ &\leq & \sup_{x \in \mathbb{R}^{n}} \sum_{|\beta| = 0}^{m} \int \limits_{-\infty}^{x_1} \cdots \int \limits_{-\infty}^{x_n} \left| D^{(1,\ldots,1)}[(1+|y|)^{m}D^{\beta}f(y)] \right| \; dy_n \ldots dy_1 \\ & \leq & \sum_{|\beta| = 0}^{m} \int _{\mathbb{R}^n} \left| D^{(1,\ldots,1)}[(1+|y|)^{m}D^{\beta}f(y)] \right| \; dy = \\ & = & \sum_{|\beta| = 0}^{m} \int _{\mathbb{R}^n} \left| \sum _{\alpha + \gamma = (1,\ldots ,1)} \binom{n}{|\gamma|} D^{\alpha}\left( (1+|y|)^{m} \right) \cdot D^{\gamma} \left( D^{\beta}f(y) \right) \right| \; dy \\ &=& \sum_{|\beta| = 0}^{m} \int _{\mathbb{R}^n} \left| \sum _{\alpha + \gamma = (1,\ldots ,1)} \binom{n}{|\gamma|} c_0 \cdot y^{\alpha} |y|^{-2|\alpha|} (1+|y|)^{m} \cdot D^{\gamma} \left( D^{\beta}f(y) \right) \right| \; dy \\ & \leq & \sum_{|\beta| = 0}^{m} \int _{\mathbb{R}^n} \sum _{\alpha + \gamma = (1,\ldots ,1)} \binom{n}{|\gamma|} \left| c_0 \cdot y^{\alpha} |y|^{-2|\alpha|} (1+|y|)^{m} \cdot D^{\gamma} \left( D^{\beta}f(y) \right) \right| \; dy \\ & \leq & \sum_{|\beta| = 0}^{m} \int _{\mathbb{R}^n} \sum _{\alpha + \gamma = (1,\ldots ,1)} \binom{n}{|\gamma|} c_0 \cdot |y|^{|\alpha|} |y|^{-2|\alpha|} (1+|y|)^{m} \cdot \left| D^{\gamma} \left( D^{\beta}f(y) \right) \right| \; dy \\ &=&\sum_{|\beta| = 0}^{m} \int _{\mathbb{R}^n} \sum _{\alpha + \gamma = (1,\ldots ,1)} \binom{n}{|\gamma|} c_0 \cdot |y|^{-|\alpha|} (1+|y|)^{m} \cdot \left| D^{\gamma} \left( D^{\beta}f(y) \right) \right| \; dy \\ &=&\sum_{|\beta| = 0}^{m}\sum _{\alpha + \gamma = (1,\ldots ,1)} \binom{n}{|\gamma|} c_0 \cdot  \int _{\mathbb{R}^n} |y|^{-|\alpha|} (1+|y|)^{m} \cdot \left| D^{\gamma} \left( D^{\beta}f(y) \right) \right| \; dy \\ \end{eqnarray*} It is almost what I need - problem is with factor $|y|^{-|\alpha|}$. Has someone idea how to prove it? Thanks in advance.","I am trying to prove theorem 9.2 from book ""Lectures on Linear Partial Differential Equations"" wtitten by G. Eskin. In proof of this theorem is inequality which makes problems for me. Firstly I remind designation. Multi-index: $\alpha=(\alpha_{1}, \ldots , \alpha_{n}) \in \mathbb{Z}_{+}^{n}$, length of multi-index: $| \alpha | = \sum_{i=1}^{n} \alpha_{i}$, partial differential operator: $$D^{\alpha} = \frac{\partial ^{|\alpha|}}{\partial x_{1}^{\alpha_{1}} \ldots \partial x_{n}^{\alpha_{n}}}.$$ Moreover $|x|=\sqrt{x_1^2+ \ldots + x_n^2}$, Schwartz space: $$\mathcal{S}(\mathbb{R}^n) = \{f \in C^{\infty}(\mathbb{R}^n) \colon \sup_{x \in \mathbb{R}^{n}} (1+|x|)^{m}|D^{\beta}f(x)| < \infty, \: m \in \mathbb{N}, \: \beta \in \mathbb{Z}_{+}^{n} \}.$$  Seminorm in Schwartz space: $$ \|f\|_{m,\mathcal{S}} = \sup_{x \in \mathbb{R}^{n}} (1+|x|)^{m} \sum_{|\beta| = 0}^{m} |D^{\beta}f(x)|. $$ For every function $f \in \mathcal{S}(\mathbb{R}^n)$, for every multi-index $\beta \in \mathbb{Z}_{+}^{n}$ and for every $m \in \mathbb{N}$ we have \begin{equation*} (1+|x|)^{m}D^{\beta}f(x) = \int \limits_{-\infty}^{x_1} \cdots \int \limits_{-\infty}^{x_n} D^{(1,\ldots,1)}[(1+|y|)^{m}D^{\beta}f(y)] \; dy_n \ldots dy_1. \end{equation*} We need to show inequality. \begin{equation} \| f \|_{m,\mathcal{S}} \leq C_{1} \sum_{|\beta|=0}^{n+m} \int _{\mathbb{R}^n} (1+|y|)^m |D^{\beta} f(y)| \; dy, \quad f \in \mathcal{S}(\mathbb{R}^n). \end{equation} I used above properties, used Liebniz and I've got this: \begin{eqnarray*} \|f\|_{m,\mathcal{S}} &=& \sup_{x \in \mathbb{R}^{n}} (1+|x|)^{m} \sum_{|\beta| = 0}^{m} |D^{\beta}f(x)| = \sup_{x \in \mathbb{R}^{n}} \sum_{|\beta| = 0}^{m} \left| (1+|x|)^{m} D^{\beta}f(x) \right|\\ &=& \sup_{x \in \mathbb{R}^{n}} \sum_{|\beta| = 0}^{m} \left| \int \limits_{-\infty}^{x_1} \cdots \int \limits_{-\infty}^{x_n} D^{(1,\ldots,1)}[(1+|y|)^{m}D^{\beta}f(y)] \; dy_n \ldots dy_1  \right|\\ &\leq & \sup_{x \in \mathbb{R}^{n}} \sum_{|\beta| = 0}^{m} \int \limits_{-\infty}^{x_1} \cdots \int \limits_{-\infty}^{x_n} \left| D^{(1,\ldots,1)}[(1+|y|)^{m}D^{\beta}f(y)] \right| \; dy_n \ldots dy_1 \\ & \leq & \sum_{|\beta| = 0}^{m} \int _{\mathbb{R}^n} \left| D^{(1,\ldots,1)}[(1+|y|)^{m}D^{\beta}f(y)] \right| \; dy = \\ & = & \sum_{|\beta| = 0}^{m} \int _{\mathbb{R}^n} \left| \sum _{\alpha + \gamma = (1,\ldots ,1)} \binom{n}{|\gamma|} D^{\alpha}\left( (1+|y|)^{m} \right) \cdot D^{\gamma} \left( D^{\beta}f(y) \right) \right| \; dy \\ &=& \sum_{|\beta| = 0}^{m} \int _{\mathbb{R}^n} \left| \sum _{\alpha + \gamma = (1,\ldots ,1)} \binom{n}{|\gamma|} c_0 \cdot y^{\alpha} |y|^{-2|\alpha|} (1+|y|)^{m} \cdot D^{\gamma} \left( D^{\beta}f(y) \right) \right| \; dy \\ & \leq & \sum_{|\beta| = 0}^{m} \int _{\mathbb{R}^n} \sum _{\alpha + \gamma = (1,\ldots ,1)} \binom{n}{|\gamma|} \left| c_0 \cdot y^{\alpha} |y|^{-2|\alpha|} (1+|y|)^{m} \cdot D^{\gamma} \left( D^{\beta}f(y) \right) \right| \; dy \\ & \leq & \sum_{|\beta| = 0}^{m} \int _{\mathbb{R}^n} \sum _{\alpha + \gamma = (1,\ldots ,1)} \binom{n}{|\gamma|} c_0 \cdot |y|^{|\alpha|} |y|^{-2|\alpha|} (1+|y|)^{m} \cdot \left| D^{\gamma} \left( D^{\beta}f(y) \right) \right| \; dy \\ &=&\sum_{|\beta| = 0}^{m} \int _{\mathbb{R}^n} \sum _{\alpha + \gamma = (1,\ldots ,1)} \binom{n}{|\gamma|} c_0 \cdot |y|^{-|\alpha|} (1+|y|)^{m} \cdot \left| D^{\gamma} \left( D^{\beta}f(y) \right) \right| \; dy \\ &=&\sum_{|\beta| = 0}^{m}\sum _{\alpha + \gamma = (1,\ldots ,1)} \binom{n}{|\gamma|} c_0 \cdot  \int _{\mathbb{R}^n} |y|^{-|\alpha|} (1+|y|)^{m} \cdot \left| D^{\gamma} \left( D^{\beta}f(y) \right) \right| \; dy \\ \end{eqnarray*} It is almost what I need - problem is with factor $|y|^{-|\alpha|}$. Has someone idea how to prove it? Thanks in advance.",,"['functional-analysis', 'partial-differential-equations', 'integral-inequality']"
30,On the extension of distribution,On the extension of distribution,,"Define a distribution on $(0,+\infty)$ by $$u(\varphi):=\sum_{k=1}^{\infty} {1 \over {k!}}\partial^k \varphi(1/k)$$ how can I show it cannot be extended to any distribution defined globally on $\mathbb{R}$?","Define a distribution on $(0,+\infty)$ by $$u(\varphi):=\sum_{k=1}^{\infty} {1 \over {k!}}\partial^k \varphi(1/k)$$ how can I show it cannot be extended to any distribution defined globally on $\mathbb{R}$?",,"['functional-analysis', 'distribution-theory']"
31,Need help understanding this proof of regularity of traveling wave solutions to the Gross-Pitaevskii equation,Need help understanding this proof of regularity of traveling wave solutions to the Gross-Pitaevskii equation,,"These are actually 4 question about a proof given in this paper . Any hint to solutions for any of these questions would be much appreciated! Lemma 1. Assume $v$ is a solution to the equation \begin{equation} ic \partial_1 v + \Delta v + v(1-\vert v \vert^2) =0 \tag{GP} \end{equation} in $L^1_{loc}(\mathbb{R}^3)$ of finite energy, then $v$ is regular, bounded and its gradient belongs to all the spaces $W^{k,p}(\mathbb{R}^3)$ for $k \in \mathbb{3}$ and $p \in [2,\infty]$. Here ""finite energy"" means, that \begin{equation} \frac{1}{2} \int_{\mathbb{R}^N}\vert \nabla v \vert^2+\frac{1}{4}\int_{\mathbb{R}^N}(1-\vert v \vert^2)^2 < \infty. \end{equation} Proof. The authors consider a point $z_0 \in \mathbb{R}^3$ and denote by $\Omega$ the unit ball with center $z_0$. Then the consider the solutions $v_1$ and $v_2$ of \begin{equation} \begin{cases} \Delta v_1=0 & \mbox{on } \Omega  \\ v_1 = v & \mbox{on } \partial \Omega\end{cases} \tag{1} \end{equation} and \begin{equation} \begin{cases} \Delta v_2=g(v):=v(1-\vert v\vert^2) + ic \partial_1 v & \mbox{on } \Omega  \\ v_2 = 0 & \mbox{on } \partial \Omega\end{cases} \tag{2} \end{equation} Question 1. It seams odd to consider $v \in L^1_{loc}(\mathbb{R}^3)$. A function in this space can not even be a weak solution to (GP), can it? Also equation (1) does not need have a solution if $v$ is only $L^1$, does it? The authors go on and show that $g(v)$ is uniformly bounded in $L^{\frac{4}{3}}(\Omega)$, i.e. $\Vert g(v) \Vert_{L^{\frac{4}{3}}(\Omega)}$ is bounded by a constant not depending on $z_0$. The infer that $v_1$ is unformly bounded in $L^4(\Omega)$ and $v_2$ in $W^{2,\frac{4}{3}}(\Omega)$. I do already understand this part (some elliptic arguments and Sobolev embeddings). But now they denote by $\omega$ the ball with center $z_0$ and radius $\frac{1}{2}$ and tell me to use Cacioppoli inequalities to show that $v_1$ is uniformly bounded on $W^{2,\frac{4}{3}}(\omega)$ and $W^{3,\frac{12}{11}}(\omega)$. This would show that $v$ is uniformly bounded in $W^{2,\frac{4}{3}}(\Omega)$. Question 2. How do you do that? By Caccioppoli inequalities for the Laplace equation I can show that $\Vert Dv_1 \Vert_{L^2(\omega)} \Vert \leq C \Vert v_1 \Vert_{L^2(\omega)}$ but nothing else. How do they do this? Why do they even need the second statement (i.e. $W^{3,\frac{12}{11}}$)? They go on and compute \begin{equation} \nabla g(v) = \nabla v(1-\vert v\vert^2) - 2 (v.\nabla v)v + ic \partial_1 \nabla v \end{equation} which is easy. But from this they infer that $g(v)$ is uniformly bounded in $L^\frac{12}{11}(\omega)$ and finally $v$ is uniformly bounded in $C^{0,\frac{1}{12}}(\omega)$. Question 3. I don't understand any of these steps. For the first one I try to estimate $\int \vert \nabla v - \nabla v \vert v \vert^2 \vert^\frac{12}{11} \leq \int \vert \nabla v \vert^\frac{12}{11} + \int \vert \nabla v \vert^\frac{12}{11} \vert v \vert^\frac{24}{11}$ which I can't manage. For the second one I don't understand why they pick the $\frac{1}{12}$-Hölder space of all Hölder spaces. Last but not least they set \begin{equation} h(w)=w(1-\vert v \vert^2) - 2 (v.w)v + \left(\frac{c^2}{2}+2\right)w \end{equation} where $w=\nabla v$ and state that $h(w)$ belongs to $L^2(\mathbb{R}^N)$ and $w$ to $H^2(\mathbb{R}^N)$. Question 4. Why is $\nabla v$ even differentiable enough to be plugged in $h$? How do they infer their two results? I have absolutely no idea. Thank you so much in advance!","These are actually 4 question about a proof given in this paper . Any hint to solutions for any of these questions would be much appreciated! Lemma 1. Assume $v$ is a solution to the equation \begin{equation} ic \partial_1 v + \Delta v + v(1-\vert v \vert^2) =0 \tag{GP} \end{equation} in $L^1_{loc}(\mathbb{R}^3)$ of finite energy, then $v$ is regular, bounded and its gradient belongs to all the spaces $W^{k,p}(\mathbb{R}^3)$ for $k \in \mathbb{3}$ and $p \in [2,\infty]$. Here ""finite energy"" means, that \begin{equation} \frac{1}{2} \int_{\mathbb{R}^N}\vert \nabla v \vert^2+\frac{1}{4}\int_{\mathbb{R}^N}(1-\vert v \vert^2)^2 < \infty. \end{equation} Proof. The authors consider a point $z_0 \in \mathbb{R}^3$ and denote by $\Omega$ the unit ball with center $z_0$. Then the consider the solutions $v_1$ and $v_2$ of \begin{equation} \begin{cases} \Delta v_1=0 & \mbox{on } \Omega  \\ v_1 = v & \mbox{on } \partial \Omega\end{cases} \tag{1} \end{equation} and \begin{equation} \begin{cases} \Delta v_2=g(v):=v(1-\vert v\vert^2) + ic \partial_1 v & \mbox{on } \Omega  \\ v_2 = 0 & \mbox{on } \partial \Omega\end{cases} \tag{2} \end{equation} Question 1. It seams odd to consider $v \in L^1_{loc}(\mathbb{R}^3)$. A function in this space can not even be a weak solution to (GP), can it? Also equation (1) does not need have a solution if $v$ is only $L^1$, does it? The authors go on and show that $g(v)$ is uniformly bounded in $L^{\frac{4}{3}}(\Omega)$, i.e. $\Vert g(v) \Vert_{L^{\frac{4}{3}}(\Omega)}$ is bounded by a constant not depending on $z_0$. The infer that $v_1$ is unformly bounded in $L^4(\Omega)$ and $v_2$ in $W^{2,\frac{4}{3}}(\Omega)$. I do already understand this part (some elliptic arguments and Sobolev embeddings). But now they denote by $\omega$ the ball with center $z_0$ and radius $\frac{1}{2}$ and tell me to use Cacioppoli inequalities to show that $v_1$ is uniformly bounded on $W^{2,\frac{4}{3}}(\omega)$ and $W^{3,\frac{12}{11}}(\omega)$. This would show that $v$ is uniformly bounded in $W^{2,\frac{4}{3}}(\Omega)$. Question 2. How do you do that? By Caccioppoli inequalities for the Laplace equation I can show that $\Vert Dv_1 \Vert_{L^2(\omega)} \Vert \leq C \Vert v_1 \Vert_{L^2(\omega)}$ but nothing else. How do they do this? Why do they even need the second statement (i.e. $W^{3,\frac{12}{11}}$)? They go on and compute \begin{equation} \nabla g(v) = \nabla v(1-\vert v\vert^2) - 2 (v.\nabla v)v + ic \partial_1 \nabla v \end{equation} which is easy. But from this they infer that $g(v)$ is uniformly bounded in $L^\frac{12}{11}(\omega)$ and finally $v$ is uniformly bounded in $C^{0,\frac{1}{12}}(\omega)$. Question 3. I don't understand any of these steps. For the first one I try to estimate $\int \vert \nabla v - \nabla v \vert v \vert^2 \vert^\frac{12}{11} \leq \int \vert \nabla v \vert^\frac{12}{11} + \int \vert \nabla v \vert^\frac{12}{11} \vert v \vert^\frac{24}{11}$ which I can't manage. For the second one I don't understand why they pick the $\frac{1}{12}$-Hölder space of all Hölder spaces. Last but not least they set \begin{equation} h(w)=w(1-\vert v \vert^2) - 2 (v.w)v + \left(\frac{c^2}{2}+2\right)w \end{equation} where $w=\nabla v$ and state that $h(w)$ belongs to $L^2(\mathbb{R}^N)$ and $w$ to $H^2(\mathbb{R}^N)$. Question 4. Why is $\nabla v$ even differentiable enough to be plugged in $h$? How do they infer their two results? I have absolutely no idea. Thank you so much in advance!",,"['functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
32,Simple spectrum and the spectral theorem for bounded symmetric operators,Simple spectrum and the spectral theorem for bounded symmetric operators,,"I have a question regarding the spectral theorem for bounded self-adjoint operators. The book ""Functional Analysis, an Introduction"" by Eidelman, Milman, and Tsolomitis says that if an operator $T$ has a simple spectrum then it is unitarily equivalent to the operator $T\varphi(\lambda)=\lambda\cdot \varphi(\lambda)$ acting on $L^2$ equipped with the measure generated by the right-continuous function $\lambda\mapsto\langle E_\lambda x_0, x_0\rangle$. Here, $\{E_{\lambda}\}$ is our resolution of identity. Hence, is this unitary equivalence to the multiplication operator only possible when $T$ has a simple spectrum, or is it possible in the general self-adjoint case? Wikipedia says that it's always true, but my functional book along with the book ""Treastie on the Shift Operator"" say that it's only possible when $T$ has simple spectrum. Also, the definition of simple spectrum is when there exists an $x_0\in H$ such that $$\{ (E_{\lambda_1}-E_{\lambda_2})x_0\ : \ \lambda_2<\lambda_1\}$$ is a dense subset of our Hilbert Space. Can someone give me some intuition as to why this definition is natural.","I have a question regarding the spectral theorem for bounded self-adjoint operators. The book ""Functional Analysis, an Introduction"" by Eidelman, Milman, and Tsolomitis says that if an operator $T$ has a simple spectrum then it is unitarily equivalent to the operator $T\varphi(\lambda)=\lambda\cdot \varphi(\lambda)$ acting on $L^2$ equipped with the measure generated by the right-continuous function $\lambda\mapsto\langle E_\lambda x_0, x_0\rangle$. Here, $\{E_{\lambda}\}$ is our resolution of identity. Hence, is this unitary equivalence to the multiplication operator only possible when $T$ has a simple spectrum, or is it possible in the general self-adjoint case? Wikipedia says that it's always true, but my functional book along with the book ""Treastie on the Shift Operator"" say that it's only possible when $T$ has simple spectrum. Also, the definition of simple spectrum is when there exists an $x_0\in H$ such that $$\{ (E_{\lambda_1}-E_{\lambda_2})x_0\ : \ \lambda_2<\lambda_1\}$$ is a dense subset of our Hilbert Space. Can someone give me some intuition as to why this definition is natural.",,"['analysis', 'functional-analysis', 'operator-theory', 'spectral-theory']"
33,Dense subspace of Zygmund space or Hölder space?,Dense subspace of Zygmund space or Hölder space?,,"Do we know any function spaces dense in  Zygmund space $C_*^s$(a special case of Besov space , i.e. $C_*^s = B^s_{\infty,\infty}$) or Hölder space$C^{k,r}$, with underlying field $\mathbb{R}^d$? Will $C^\infty$ do the job?","Do we know any function spaces dense in  Zygmund space $C_*^s$(a special case of Besov space , i.e. $C_*^s = B^s_{\infty,\infty}$) or Hölder space$C^{k,r}$, with underlying field $\mathbb{R}^d$? Will $C^\infty$ do the job?",,"['functional-analysis', 'besov-space']"
34,I need help understanding the proof of the Lifting Theorem for $H^1$-functions.,I need help understanding the proof of the Lifting Theorem for -functions.,H^1,"The Lifiting Theorem by Béthuel [1,Lem.4.2] states Theorem Assume $\mathbb{T}^3 \simeq \Omega=[-\pi n, \pi n]$ is the 3-dimensional torus obtained by identifying opposing edges. Assume further $v \in H^1(\mathbb{T}^3,\mathbb{C}) \simeq H^1_{per}(\Omega, \mathbb{C})$ with \begin{equation} \vert v(x) \vert \geq \frac{1}{2} ~~~~~ \forall x \in \mathbb{T}^3. \end{equation} Then one can rewrite $v=\vert v \vert \exp(i \varphi)$ with $\varphi \in H^1(\mathbb{T}^3,\mathbb{R})$. Proof As $\vert v \vert \geq \frac{1}{2}$ we can write $v= \vert v \vert w$ with $\vert w \vert=1$. Question 1 Why is $\vert v \vert \geq \frac{1}{2}$ necessary in order to obtain $v= \vert v \vert w$? Can't I just do that for every function that maps to $\mathbb{C}$? Then we calculate $d(w \times dw) =0 $ and therefore the Hodge-de-Rham decomposition is written as \begin{equation} w \times dw = d \varphi + \sum_{j=1}^3 \alpha_j d x_j \end{equation} where $\alpha_j \in \mathbb{R}$ and $\varphi \in H^1(\mathbb{T}^3,\mathbb{C})$. Question 2 How does he calculate $d(w \times dw)$? Why is that even possible? We have no regularity of $w$ so far and I thought the differential form \begin{equation}  dw(x) = \sum_{i=1}^3 w_{x_i}(x) dx_i(x) \in (\mathbb{R}^n)^\ast\end{equation} is not defined for non-smooth functions $w$. Everything else then follows from properties of the Hodge-de-Rham decomposition. [1] Béthuel, F., P. Gravejat und J. C. Saut: Travelling waves for the Gross- Pitaevskii equation. II. Comm. Math. Phys., 285(2):567–651, 2009.","The Lifiting Theorem by Béthuel [1,Lem.4.2] states Theorem Assume $\mathbb{T}^3 \simeq \Omega=[-\pi n, \pi n]$ is the 3-dimensional torus obtained by identifying opposing edges. Assume further $v \in H^1(\mathbb{T}^3,\mathbb{C}) \simeq H^1_{per}(\Omega, \mathbb{C})$ with \begin{equation} \vert v(x) \vert \geq \frac{1}{2} ~~~~~ \forall x \in \mathbb{T}^3. \end{equation} Then one can rewrite $v=\vert v \vert \exp(i \varphi)$ with $\varphi \in H^1(\mathbb{T}^3,\mathbb{R})$. Proof As $\vert v \vert \geq \frac{1}{2}$ we can write $v= \vert v \vert w$ with $\vert w \vert=1$. Question 1 Why is $\vert v \vert \geq \frac{1}{2}$ necessary in order to obtain $v= \vert v \vert w$? Can't I just do that for every function that maps to $\mathbb{C}$? Then we calculate $d(w \times dw) =0 $ and therefore the Hodge-de-Rham decomposition is written as \begin{equation} w \times dw = d \varphi + \sum_{j=1}^3 \alpha_j d x_j \end{equation} where $\alpha_j \in \mathbb{R}$ and $\varphi \in H^1(\mathbb{T}^3,\mathbb{C})$. Question 2 How does he calculate $d(w \times dw)$? Why is that even possible? We have no regularity of $w$ so far and I thought the differential form \begin{equation}  dw(x) = \sum_{i=1}^3 w_{x_i}(x) dx_i(x) \in (\mathbb{R}^n)^\ast\end{equation} is not defined for non-smooth functions $w$. Everything else then follows from properties of the Hodge-de-Rham decomposition. [1] Béthuel, F., P. Gravejat und J. C. Saut: Travelling waves for the Gross- Pitaevskii equation. II. Comm. Math. Phys., 285(2):567–651, 2009.",,"['functional-analysis', 'sobolev-spaces', 'hodge-theory']"
35,Distribution with singularities.,Distribution with singularities.,,"I need some help to prove that $f$ defined by  $\langle f,\psi\rangle:= \sum_{n=0} ^\infty  \psi^{(n)}(n)$ is a distribution which has singularities of infinite  order. Here $\psi$ is a test function that belongs to $ \mathcal D(\Bbb R)$. Thanks.","I need some help to prove that $f$ defined by  $\langle f,\psi\rangle:= \sum_{n=0} ^\infty  \psi^{(n)}(n)$ is a distribution which has singularities of infinite  order. Here $\psi$ is a test function that belongs to $ \mathcal D(\Bbb R)$. Thanks.",,"['functional-analysis', 'distribution-theory', 'singularity-theory']"
36,Calculus on surfaces and chain rule,Calculus on surfaces and chain rule,,Define the surface gradient operator on any surface $S$ as $$\nabla_S f = \nabla f - \nabla f \cdot \nu_S \nu_S$$ where $\nu_S$ is the outward unit normal on $S$. Let $T:S_1 \to S_2$ be a $C^2$ diffeomorphism between hypersurfaces. Consider $v:S_2 \to \mathbb{R}$ a function. Is is true that $$\nabla_{S_1}(v \circ T(x)) = (\mathbf{D}T)^T(\nabla_{S_2}v)|_{T(x)}?$$ i.e. does the chain rule hold? Can someone prove it? Here $\mathbf{D}T$ denotes the Jacobian of $T$ wrt. the orthogonal basis of tangent space.,Define the surface gradient operator on any surface $S$ as $$\nabla_S f = \nabla f - \nabla f \cdot \nu_S \nu_S$$ where $\nu_S$ is the outward unit normal on $S$. Let $T:S_1 \to S_2$ be a $C^2$ diffeomorphism between hypersurfaces. Consider $v:S_2 \to \mathbb{R}$ a function. Is is true that $$\nabla_{S_1}(v \circ T(x)) = (\mathbf{D}T)^T(\nabla_{S_2}v)|_{T(x)}?$$ i.e. does the chain rule hold? Can someone prove it? Here $\mathbf{D}T$ denotes the Jacobian of $T$ wrt. the orthogonal basis of tangent space.,,"['calculus', 'functional-analysis', 'differential-geometry', 'partial-differential-equations']"
37,Example of an unbounded operator,Example of an unbounded operator,,"Can somebody give me an easy example of a linear operator which maps $L^1(\mathbb{R}^n)$ to $L^1(\mathbb{R}^n)$ and $L^\infty(\mathbb{R}^n)$ to $L^\infty(\mathbb{R}^n)$ (but not boundedly) but does not admit a bounded extension from $L^2(\mathbb{R}^n)$ to $L^2(\mathbb{R}^n)$ (or any other $L^p$, $1<p<\infty$) ?","Can somebody give me an easy example of a linear operator which maps $L^1(\mathbb{R}^n)$ to $L^1(\mathbb{R}^n)$ and $L^\infty(\mathbb{R}^n)$ to $L^\infty(\mathbb{R}^n)$ (but not boundedly) but does not admit a bounded extension from $L^2(\mathbb{R}^n)$ to $L^2(\mathbb{R}^n)$ (or any other $L^p$, $1<p<\infty$) ?",,"['functional-analysis', 'banach-spaces', 'harmonic-analysis']"
38,$AB - BA = I$ in Hilbert Space [duplicate],in Hilbert Space [duplicate],AB - BA = I,"This question already has answers here : The identity cannot be a commutator in a Banach algebra? (3 answers) Closed 11 years ago . Let H be a Hilbert space and $A$ and $B$ be bounded operators in $H$. How can I prove that $AB - BA = I$ is not possible ? Probably this is as easy as in the matrices case, but I couldn't prove it. Can you help me please ? Thank you :)","This question already has answers here : The identity cannot be a commutator in a Banach algebra? (3 answers) Closed 11 years ago . Let H be a Hilbert space and $A$ and $B$ be bounded operators in $H$. How can I prove that $AB - BA = I$ is not possible ? Probably this is as easy as in the matrices case, but I couldn't prove it. Can you help me please ? Thank you :)",,"['functional-analysis', 'operator-theory']"
39,Is this a correct question?,Is this a correct question?,,"This is an exam question in functional analysis which for me doesn't make sense the way it is written. I am asking you if you agree with me on the modifications that needs to be done in the question. Question: Let $X$ and $Y$ be normed linear spaces and $T: X\rightarrow Y$ a linear operator. Recall that the dual $X^*$ of $X$ is the space of bounded linear functionals on $X$ with the norm $||\phi ||=sup\{|\phi (x) | :x\in X \textrm{ and } ||x||\leq 1 \}$. (Similarly for $Y$). Define $T^*:Y^*\rightarrow X^*$ by $[T^*(\phi )](x)=\phi (Tx)$ for $\phi \in Y^*$. Show that $||T^*||\leq ||T||$. My comments to the question: Define $T^*:Y^*\rightarrow X^*$ by $[T^*(\phi )](x)=\phi (Tx)$ for $\phi \in Y^*$. For me this doesn't make sense, but I understand it if one drops the $x$, i.e. $T^*:Y^*\rightarrow X^*$ by $T^*(\phi )=\phi T$ since then the domain and codomain makes sense, what worries me with the notation above is that $[T^*(\phi )](x)$ has no meaning since $\phi$ is not defined on $X$ and furthermore $\phi (Tx)$ is a scalar and not an element of $X^*$. Furthermore, I feel a bit ackward about talking about $||T||$ when $T$ is not assumed to be bounded, so I have assumed that. Is this correct or could we talk about $||T||$ in some general way when $T$ is not bounded? With this change of the question I understand the solution to it, I have added the indexes on the norms, so it is possible to follow my argument: $|| T^*(\phi )||_{X^*}=||\phi T||_{X^*}$ $=sup \{|\phi T x| : ||x||_{X}\leq 1 \}$ $\leq sup \{||\phi ||_{Y^*} || T ||_{B(X,Y)}||x||_{X} : ||x||\leq 1 \}$ $= ||\phi||_{Y^*} || T ||_{B(X,Y)}.$ Hence $|| T^*(\phi )||_{X^*}$ $\leq ||\phi||_{Y^*} || T ||_{B(X,Y)}$. Now, $||T^*||=sup \{|| T^*\phi ||_{X^*} : ||\phi ||_{Y^*}\leq 1 \}$ and hence letting $||\phi ||_{Y^*}\leq 1$ in the inequality $|| T^*(\phi )||_{X^*}$ $\leq ||\phi||_{Y^*} || T ||_{B(X,Y)}$ showes that  $||T^*||\leq ||T||$. What do you think about the question and the suggested modifications to it?","This is an exam question in functional analysis which for me doesn't make sense the way it is written. I am asking you if you agree with me on the modifications that needs to be done in the question. Question: Let $X$ and $Y$ be normed linear spaces and $T: X\rightarrow Y$ a linear operator. Recall that the dual $X^*$ of $X$ is the space of bounded linear functionals on $X$ with the norm $||\phi ||=sup\{|\phi (x) | :x\in X \textrm{ and } ||x||\leq 1 \}$. (Similarly for $Y$). Define $T^*:Y^*\rightarrow X^*$ by $[T^*(\phi )](x)=\phi (Tx)$ for $\phi \in Y^*$. Show that $||T^*||\leq ||T||$. My comments to the question: Define $T^*:Y^*\rightarrow X^*$ by $[T^*(\phi )](x)=\phi (Tx)$ for $\phi \in Y^*$. For me this doesn't make sense, but I understand it if one drops the $x$, i.e. $T^*:Y^*\rightarrow X^*$ by $T^*(\phi )=\phi T$ since then the domain and codomain makes sense, what worries me with the notation above is that $[T^*(\phi )](x)$ has no meaning since $\phi$ is not defined on $X$ and furthermore $\phi (Tx)$ is a scalar and not an element of $X^*$. Furthermore, I feel a bit ackward about talking about $||T||$ when $T$ is not assumed to be bounded, so I have assumed that. Is this correct or could we talk about $||T||$ in some general way when $T$ is not bounded? With this change of the question I understand the solution to it, I have added the indexes on the norms, so it is possible to follow my argument: $|| T^*(\phi )||_{X^*}=||\phi T||_{X^*}$ $=sup \{|\phi T x| : ||x||_{X}\leq 1 \}$ $\leq sup \{||\phi ||_{Y^*} || T ||_{B(X,Y)}||x||_{X} : ||x||\leq 1 \}$ $= ||\phi||_{Y^*} || T ||_{B(X,Y)}.$ Hence $|| T^*(\phi )||_{X^*}$ $\leq ||\phi||_{Y^*} || T ||_{B(X,Y)}$. Now, $||T^*||=sup \{|| T^*\phi ||_{X^*} : ||\phi ||_{Y^*}\leq 1 \}$ and hence letting $||\phi ||_{Y^*}\leq 1$ in the inequality $|| T^*(\phi )||_{X^*}$ $\leq ||\phi||_{Y^*} || T ||_{B(X,Y)}$ showes that  $||T^*||\leq ||T||$. What do you think about the question and the suggested modifications to it?",,"['functional-analysis', 'normed-spaces']"
40,"The weak $\sigma$-$(V,V')$ topology on a normed space V is Hausdorff?",The weak - topology on a normed space V is Hausdorff?,"\sigma (V,V')","The weak $\sigma$-$(V,V')$ topology on a normed space V is Hausdorff? So it is claimed that The weak $\sigma$-$(V,V')$ topology on a BANACH space V is Hausdorff? In my proof I used the Hahn-Banach theorem to find the existence of linear function that separates any two arbitrarily chosen points. Then it easily follows that V is Hausdorff w.r.t the weak topology. Nowhere can I see the necessity of V being a Banach space. So it holds true when V is just a normed space?","The weak $\sigma$-$(V,V')$ topology on a normed space V is Hausdorff? So it is claimed that The weak $\sigma$-$(V,V')$ topology on a BANACH space V is Hausdorff? In my proof I used the Hahn-Banach theorem to find the existence of linear function that separates any two arbitrarily chosen points. Then it easily follows that V is Hausdorff w.r.t the weak topology. Nowhere can I see the necessity of V being a Banach space. So it holds true when V is just a normed space?",,[]
41,Finite representability of $\ell_1$ and $c_0$,Finite representability of  and,\ell_1 c_0,"It follows from Krivine's theorem for a given Banach space $X$, either some $\ell_p$ or $c_0$ is finitely represented in $X$. Since finite dimensional $\ell_1$ and $\ell_\infty$-spaces are dual to each other, can we expect that $\ell_1$ is finitely representable in $X$ if and only if $c_0$ is finitely representable in $X^*$?","It follows from Krivine's theorem for a given Banach space $X$, either some $\ell_p$ or $c_0$ is finitely represented in $X$. Since finite dimensional $\ell_1$ and $\ell_\infty$-spaces are dual to each other, can we expect that $\ell_1$ is finitely representable in $X$ if and only if $c_0$ is finitely representable in $X^*$?",,"['functional-analysis', 'banach-spaces']"
42,"Distributional differential equation, somehow related to compact support distributions","Distributional differential equation, somehow related to compact support distributions",,"I've been mulling over a problem from Friedlander's Introduction to Distribution Theory for a few days now: in Chapter 3 (on distributions with compact support), it asks to solve the differential equation $x\partial u-\lambda u=0$ for arbitrary $\lambda\in\mathbb{C}$. I've seen it stated that a distribution $u\in\mathbb{R}^n$ is homogeneous of degree $\lambda$ iff it satisfies Euler's equation, i.e. $\sum x_i\partial_i u=\lambda u$, but these just refer to Gel'fand and Shilov's book which I don't have access to. Moreover, this approach doesn't seem the author's intended one, given the chapter this problem is in. My method was basically to try and use ""integrating factors"", but of course we have only defined multiplication by smooth functions, which $x_+^\lambda$ is generally not. I've basically seen it without proof via online searching that $u=Ax_+^\lambda+Bx_-^\lambda$ is the general answer, but I can't seem to crack it, or find anything in this problem with compact support. Any help would be much appreciated.","I've been mulling over a problem from Friedlander's Introduction to Distribution Theory for a few days now: in Chapter 3 (on distributions with compact support), it asks to solve the differential equation $x\partial u-\lambda u=0$ for arbitrary $\lambda\in\mathbb{C}$. I've seen it stated that a distribution $u\in\mathbb{R}^n$ is homogeneous of degree $\lambda$ iff it satisfies Euler's equation, i.e. $\sum x_i\partial_i u=\lambda u$, but these just refer to Gel'fand and Shilov's book which I don't have access to. Moreover, this approach doesn't seem the author's intended one, given the chapter this problem is in. My method was basically to try and use ""integrating factors"", but of course we have only defined multiplication by smooth functions, which $x_+^\lambda$ is generally not. I've basically seen it without proof via online searching that $u=Ax_+^\lambda+Bx_-^\lambda$ is the general answer, but I can't seem to crack it, or find anything in this problem with compact support. Any help would be much appreciated.",,['functional-analysis']
43,Spectrum of a right shift operator.,Spectrum of a right shift operator.,,"I have some doubts on the following problem : Let us consider $T : \ell^1(\mathbb N) \to \ell^1(\mathbb N) $by $(x_1,x_2..... ) \to (x_2, x_3 ........) $. I want to find the eigen values and spectrum of T and also of $T' : \ell^\infty (\mathbb N)\to \ell^\infty(\mathbb N)$ let us consider $\lambda $ to be the eigen value , then $Tx=\lambda x$ for a $x \in \ell^1$  then we get $(x_2,x_3,......)=(\lambda x_1, \lambda x_2 ........)$ which holds equality if $x_1=x_2=.....=0$ , which means there is no eigen value for $T$ . How do i find the spectrum of $T$ and $T'$ ?  Thank you for your help.","I have some doubts on the following problem : Let us consider $T : \ell^1(\mathbb N) \to \ell^1(\mathbb N) $by $(x_1,x_2..... ) \to (x_2, x_3 ........) $. I want to find the eigen values and spectrum of T and also of $T' : \ell^\infty (\mathbb N)\to \ell^\infty(\mathbb N)$ let us consider $\lambda $ to be the eigen value , then $Tx=\lambda x$ for a $x \in \ell^1$  then we get $(x_2,x_3,......)=(\lambda x_1, \lambda x_2 ........)$ which holds equality if $x_1=x_2=.....=0$ , which means there is no eigen value for $T$ . How do i find the spectrum of $T$ and $T'$ ?  Thank you for your help.",,['functional-analysis']
44,Spectrum of a weakly compact operator,Spectrum of a weakly compact operator,,It is well known that the power of a weakly compact operator is compact. Is the spectrum of a weakly compact operator is the same as a compact operator?,It is well known that the power of a weakly compact operator is compact. Is the spectrum of a weakly compact operator is the same as a compact operator?,,"['functional-analysis', 'spectral-theory']"
45,Linear functional and convergent series in $\ell^\infty$,Linear functional and convergent series in,\ell^\infty,"Let $\ell^\infty$ be the Banach space of bounded sequences with the usual norm and let $c,c_0$ be the subspaces of sequences that are convergent, resp. convergent to zero. Show that: The linear functional $\ell_0\colon c\rightarrow \mathbb{C}$ defined for $x = (x_n) \in c$ by $$ \ell_0(x) = \lim_{n\rightarrow \infty} x_n$$ extends to a continuous functional on $\ell^\infty$ if $L$ denotes the set of all continuous extensions of the functional $\ell_0$ from (1), then a sequence $x = (x_n) \in  \ell^\infty$ belongs to $c_0$ iff $$\ell(x) = 0 \;\; \forall \ell \in L$$ Describe $c$ in a similar way My try: (1): This follows by Banach limits. (2): $(\Rightarrow)$ follows by extension $(\Leftarrow)$ Here Im a bit unsure, assume $x \not \in c_0$ if $x \in c$ we get an contradiction. But if $x\not \in c$ what happens then, can we use a subsequence? since we have bounded functionals? can we use $\ell x = \lim_{k \rightarrow \infty} x_{n_k}$ or something like that, would $\ell \in L$? (3): same as two I suppose, can we use subseqeunces? Please correct what I'm missed","Let $\ell^\infty$ be the Banach space of bounded sequences with the usual norm and let $c,c_0$ be the subspaces of sequences that are convergent, resp. convergent to zero. Show that: The linear functional $\ell_0\colon c\rightarrow \mathbb{C}$ defined for $x = (x_n) \in c$ by $$ \ell_0(x) = \lim_{n\rightarrow \infty} x_n$$ extends to a continuous functional on $\ell^\infty$ if $L$ denotes the set of all continuous extensions of the functional $\ell_0$ from (1), then a sequence $x = (x_n) \in  \ell^\infty$ belongs to $c_0$ iff $$\ell(x) = 0 \;\; \forall \ell \in L$$ Describe $c$ in a similar way My try: (1): This follows by Banach limits. (2): $(\Rightarrow)$ follows by extension $(\Leftarrow)$ Here Im a bit unsure, assume $x \not \in c_0$ if $x \in c$ we get an contradiction. But if $x\not \in c$ what happens then, can we use a subsequence? since we have bounded functionals? can we use $\ell x = \lim_{k \rightarrow \infty} x_{n_k}$ or something like that, would $\ell \in L$? (3): same as two I suppose, can we use subseqeunces? Please correct what I'm missed",,"['functional-analysis', 'banach-spaces']"
46,Norm in a dual space,Norm in a dual space,,"If $f \in X^*$, with $X^*$ the dual space consisting of all linear bounded functionals on a linear normed space $X$. With the norm defined as $||f||_{X^{*}} = \sup_{||x|| \leqslant 1} |f(x)|$. Why does $|f(x)| \leqslant ||f||_{X^{*}} ||x||_{X}$ hold?","If $f \in X^*$, with $X^*$ the dual space consisting of all linear bounded functionals on a linear normed space $X$. With the norm defined as $||f||_{X^{*}} = \sup_{||x|| \leqslant 1} |f(x)|$. Why does $|f(x)| \leqslant ||f||_{X^{*}} ||x||_{X}$ hold?",,"['functional-analysis', 'inequality', 'normed-spaces']"
47,"Equicontinuity and uniform boundedness for ""distributions""","Equicontinuity and uniform boundedness for ""distributions""",,"Exercise. (Rudin, Functional Analysis , chapter 2, pag. 53). Let us consider the space $$ \mathcal D :=\{f \in C^{\infty}(\mathbb R), \, \text{supp}f\subseteq [-1,1] \} $$ with the topology induced by the usual topology of $C^{\infty}(\Omega)$. Consider the linear functionals  $$ \mathcal D \ni \phi \mapsto \Lambda_n \phi :=\int_{[-1,1]}f_n(t)\phi(t)dt \in \mathbb R $$ where $\{f_n\}$ is a sequence of Lebesgue integrable functions s.t. $\lim_n \Lambda_n\phi$ esixts for every $\phi \in \mathcal D$. Using the facts that each $\Lambda_n$ is continuous and, moreover, that $\{\Lambda_n\}_{n \in \mathbb N}$ is equicontinuous I would like to prove that : There exist two numbers $p \in \mathbb N$, $M \in \mathbb R^+$ s.t.   $$ \left\vert \int_{[-1,1]}f_n(t)\phi(t)dt \right\vert \le M \Vert D^p\phi \Vert_{\infty}  $$   for every $n$, for every $\phi \in \mathcal D$. I think that this is a simple matter of uniform boundedness : we kwow that equicontinuity implies uniform boundedness so we can say $$ \forall E \subset \mathcal D \text{ bounded, }\, \exists M > 0 \text{ s.t. } \Lambda_nE \subset [-M,M], \quad \forall n \in \mathbb N. $$ I think that this fact is all we need to solve Rudin'exercise, but I do not kwow how to identify bounded sets in $\mathcal D$... Thanks in advance.","Exercise. (Rudin, Functional Analysis , chapter 2, pag. 53). Let us consider the space $$ \mathcal D :=\{f \in C^{\infty}(\mathbb R), \, \text{supp}f\subseteq [-1,1] \} $$ with the topology induced by the usual topology of $C^{\infty}(\Omega)$. Consider the linear functionals  $$ \mathcal D \ni \phi \mapsto \Lambda_n \phi :=\int_{[-1,1]}f_n(t)\phi(t)dt \in \mathbb R $$ where $\{f_n\}$ is a sequence of Lebesgue integrable functions s.t. $\lim_n \Lambda_n\phi$ esixts for every $\phi \in \mathcal D$. Using the facts that each $\Lambda_n$ is continuous and, moreover, that $\{\Lambda_n\}_{n \in \mathbb N}$ is equicontinuous I would like to prove that : There exist two numbers $p \in \mathbb N$, $M \in \mathbb R^+$ s.t.   $$ \left\vert \int_{[-1,1]}f_n(t)\phi(t)dt \right\vert \le M \Vert D^p\phi \Vert_{\infty}  $$   for every $n$, for every $\phi \in \mathcal D$. I think that this is a simple matter of uniform boundedness : we kwow that equicontinuity implies uniform boundedness so we can say $$ \forall E \subset \mathcal D \text{ bounded, }\, \exists M > 0 \text{ s.t. } \Lambda_nE \subset [-M,M], \quad \forall n \in \mathbb N. $$ I think that this fact is all we need to solve Rudin'exercise, but I do not kwow how to identify bounded sets in $\mathcal D$... Thanks in advance.",,"['functional-analysis', 'distribution-theory']"
48,Exercise: Application of The Uniform Boundedness Principle,Exercise: Application of The Uniform Boundedness Principle,,"I'm working on this exercise (not homework) and I would gladly welcome some hints for how to solve it! Exercise: Let $1 \leq p,q \leq \infty$ be conjugate exponents. Let $a=(a_1,a_2,...)$ be a sequence such that $\sum_1^\infty a_n x_n$ converges for all $x=(x_1,x_2,...) \in l^p$. Prove that $a \in l^q$. My idea is like this: It's sufficient to show that $a \in l^1$ since $l^1 \subset l^2 \subset... $ I define a family of operators $\{T_n \}_{n=1}^\infty$ by $T_n(x) = \sum_{k=1}^n a_k x_k$. It is clear that each $T_n$ is linear, bounded and that $\sup_{n}|T_n(x)| < \infty$ so by the Uniform Boundedness principle we get $\sup_n \| T_n \| < \infty$. Is this a good approach? If so, I'd be grateful for some guidance on how to proceed to get to the conclusion: $$\sum_1^\infty |a_k| < \infty$$ Otherwise, steer me in a better direction :) Thanks in advance","I'm working on this exercise (not homework) and I would gladly welcome some hints for how to solve it! Exercise: Let $1 \leq p,q \leq \infty$ be conjugate exponents. Let $a=(a_1,a_2,...)$ be a sequence such that $\sum_1^\infty a_n x_n$ converges for all $x=(x_1,x_2,...) \in l^p$. Prove that $a \in l^q$. My idea is like this: It's sufficient to show that $a \in l^1$ since $l^1 \subset l^2 \subset... $ I define a family of operators $\{T_n \}_{n=1}^\infty$ by $T_n(x) = \sum_{k=1}^n a_k x_k$. It is clear that each $T_n$ is linear, bounded and that $\sup_{n}|T_n(x)| < \infty$ so by the Uniform Boundedness principle we get $\sup_n \| T_n \| < \infty$. Is this a good approach? If so, I'd be grateful for some guidance on how to proceed to get to the conclusion: $$\sum_1^\infty |a_k| < \infty$$ Otherwise, steer me in a better direction :) Thanks in advance",,['functional-analysis']
49,Open mapping theorem and second category,Open mapping theorem and second category,,This seems like a fundamental result but I can not solve it of find an solution: Let $M:X\rightarrow U$ be a bounded linear map between Banach spaces. Show that if the  range of M is a set of second category of U; then the range is all of U.,This seems like a fundamental result but I can not solve it of find an solution: Let $M:X\rightarrow U$ be a bounded linear map between Banach spaces. Show that if the  range of M is a set of second category of U; then the range is all of U.,,"['functional-analysis', 'operator-theory']"
50,When multiplication operator is an bounded below/ open mapping/isometry/quotient map?,When multiplication operator is an bounded below/ open mapping/isometry/quotient map?,,"Let $(\Omega,\Sigma)$ be a $\sigma$-finite measurable space. Let $\mu,\nu\in \mathcal{M}(\Omega)$ be $\sigma$-additive measures on $\Omega$. Assume we are given $p,q\in[1,+\infty]$ and a measurable fnction $g:\Omega\to\mathbb{C}$. Which conditions on $p,q,g,\mu,\nu$ are necessary and sufficient for multiplication operator  $$ T:L_p(X,\mu)\to L_q(X,\nu): f\mapsto g\cdot f $$ to be $\langle$ bounded below/open mapping/isometry/quotient map$\rangle$ ?","Let $(\Omega,\Sigma)$ be a $\sigma$-finite measurable space. Let $\mu,\nu\in \mathcal{M}(\Omega)$ be $\sigma$-additive measures on $\Omega$. Assume we are given $p,q\in[1,+\infty]$ and a measurable fnction $g:\Omega\to\mathbb{C}$. Which conditions on $p,q,g,\mu,\nu$ are necessary and sufficient for multiplication operator  $$ T:L_p(X,\mu)\to L_q(X,\nu): f\mapsto g\cdot f $$ to be $\langle$ bounded below/open mapping/isometry/quotient map$\rangle$ ?",,"['functional-analysis', 'lp-spaces']"
51,One problem about complemented subspace,One problem about complemented subspace,,"Question: For every Banach space $X$ and its subspace $Y$, is there a complemented subspace $Z$ in $X$ such that $Y \subset Z \subset X $ and $\operatorname{card}(Y)=\operatorname{card}(Z)$ i.e., $Y$ and $Z$ have the same cardinality? The answer is yes if $\operatorname{card}(X)=\mathfrak c$ (i.e., Continuum), the proof is easy by taking a hyperplane $H$ such that $Y \subset H \subset X$, then $\operatorname{card}(Y)=\operatorname{card}(H)=\mathfrak c$. But I don't know whether it holds for general spaces.","Question: For every Banach space $X$ and its subspace $Y$, is there a complemented subspace $Z$ in $X$ such that $Y \subset Z \subset X $ and $\operatorname{card}(Y)=\operatorname{card}(Z)$ i.e., $Y$ and $Z$ have the same cardinality? The answer is yes if $\operatorname{card}(X)=\mathfrak c$ (i.e., Continuum), the proof is easy by taking a hyperplane $H$ such that $Y \subset H \subset X$, then $\operatorname{card}(Y)=\operatorname{card}(H)=\mathfrak c$. But I don't know whether it holds for general spaces.",,"['functional-analysis', 'banach-spaces', 'cardinals']"
52,Help for Divergence operator,Help for Divergence operator,,I'm a newbie and may be this question is bit simple for you but pardon me if it's too simple. Can some one tell me some reference to study about the invertibility of Divergence operator  $\operatorname{div}\colon C^1(\omega)\to G$ where $G$ is a space of  real valued function and $\omega$ is a subset of $\Bbb R^2$. Here I assume a Dirichlet type condition on the boundary of $\omega$ is specified and all boundary and domain have nice smoothness. In above context can someone give me some reference on the Null space structure of the divergence operator operating on differentiable maps defined on $\Bbb R^2$ ? Ariwn,I'm a newbie and may be this question is bit simple for you but pardon me if it's too simple. Can some one tell me some reference to study about the invertibility of Divergence operator  $\operatorname{div}\colon C^1(\omega)\to G$ where $G$ is a space of  real valued function and $\omega$ is a subset of $\Bbb R^2$. Here I assume a Dirichlet type condition on the boundary of $\omega$ is specified and all boundary and domain have nice smoothness. In above context can someone give me some reference on the Null space structure of the divergence operator operating on differentiable maps defined on $\Bbb R^2$ ? Ariwn,,"['analysis', 'functional-analysis', 'operator-theory']"
53,Existence of smooth function $f(x)$ satisfying partial summation,Existence of smooth function  satisfying partial summation,f(x),"We know that $\sum_{i=1}^{n}i=\frac{n(n+1)}{2}$, $\sum_{i=1}^{n}\frac{1}{i}=\psi_{0} (n+1)-\psi_{0} (1)$, where $\psi_{0}(x)$ is the digamma function. My problem is, (1).Is there a transformation such that it maps $x \to \frac{x(x+1)}{2}$ and $\frac{1}{x} \to \psi_{0}(x+1)$, and map a smooth $f(x)$ into another smooth function $g(x)$, such that $g(x)-g(x-1)=f(x)$ ? When I mention transformation, I mean an operator or algorithm for me to get $g(x)$ from $f(x)$. (2). Surely $g(x)$ if exists, it is not unique because $g(x)+C$ also satisfy the condition. Let's take $g(x)+C$ and $g(x)$ as the same case. Is there another smooth $h(x)\not = g(x)+C$ satisfying this condition? The problem came when I tried to evaluate $\sum_{i=1}^{n} \sqrt{i}$, I'd like to represent it by integral form. Thanks for attention!","We know that $\sum_{i=1}^{n}i=\frac{n(n+1)}{2}$, $\sum_{i=1}^{n}\frac{1}{i}=\psi_{0} (n+1)-\psi_{0} (1)$, where $\psi_{0}(x)$ is the digamma function. My problem is, (1).Is there a transformation such that it maps $x \to \frac{x(x+1)}{2}$ and $\frac{1}{x} \to \psi_{0}(x+1)$, and map a smooth $f(x)$ into another smooth function $g(x)$, such that $g(x)-g(x-1)=f(x)$ ? When I mention transformation, I mean an operator or algorithm for me to get $g(x)$ from $f(x)$. (2). Surely $g(x)$ if exists, it is not unique because $g(x)+C$ also satisfy the condition. Let's take $g(x)+C$ and $g(x)$ as the same case. Is there another smooth $h(x)\not = g(x)+C$ satisfying this condition? The problem came when I tried to evaluate $\sum_{i=1}^{n} \sqrt{i}$, I'd like to represent it by integral form. Thanks for attention!",,"['analysis', 'functional-analysis']"
54,Difficulty of functional analysis exam,Difficulty of functional analysis exam,,"I've just written the final exam in my introductory course to functional analysis (2nd year bachelor degree). I felt quite well prepared but nevertheless found the exam pretty challenging in the timeframe of two hours. I'd appreciate any comments about what you think about it! The exam can be found here: http://www.math.lmu.de/~michel/SS12_FA_Final_Test_01.pdf In particular I'd appreciate any hints how to solve 6 ii), which states: Use the fourier-series of $$f(x)=\begin{cases}x^2 &\mbox{ for } x\in[0,\pi] \\ (2\pi-x)^2 &\mbox{ for } x\in (\pi,2\pi ]\end{cases} $$ to calculate $\sum_{n\in\mathbb{N}}\frac{(-1)^{n-1}}{n^2}$","I've just written the final exam in my introductory course to functional analysis (2nd year bachelor degree). I felt quite well prepared but nevertheless found the exam pretty challenging in the timeframe of two hours. I'd appreciate any comments about what you think about it! The exam can be found here: http://www.math.lmu.de/~michel/SS12_FA_Final_Test_01.pdf In particular I'd appreciate any hints how to solve 6 ii), which states: Use the fourier-series of $$f(x)=\begin{cases}x^2 &\mbox{ for } x\in[0,\pi] \\ (2\pi-x)^2 &\mbox{ for } x\in (\pi,2\pi ]\end{cases} $$ to calculate $\sum_{n\in\mathbb{N}}\frac{(-1)^{n-1}}{n^2}$",,"['functional-analysis', 'soft-question']"
55,$L^p$ norms of Fourier transform of solutions of hyperbolic Burgers' equation at the time of first blow-up,norms of Fourier transform of solutions of hyperbolic Burgers' equation at the time of first blow-up,L^p,"I am struggling to understand the behavior of the Fourier transform (in the $x$ variable) of initially smooth solutions of the hyperbolic Burgers' equation in 1-D, $ \partial_t u + u~ \partial_x u =0$ . I start with a smooth and rapidly decaying initial condition $u(x,t)=u_0(x)$ on $\Bbb R$ . This solution evolves in time until it breaks down. At the time of first breakdown $t=T$ I look at the Fourier transform $\hat u(k,T)$ of the solution $u(x,T)$ . In particular, I am trying hard to understand how and why the $L^p$ norms of the Fourier transform $\hat u$ remain finite at the time of first blow-up for $p>1$ . I think that if one uses weak (or Lorentz) norms, then this non-blow-up extends even to the weak $L^1$ norm. The only way I have been able to understand this property is via the convervation law for the $L^\infty$ norm of $u$ .   For the $\|u \|_{L^\infty} $ norm to be defined at the time of first blow-up, the Fourier transform needs to remain in a weak $L^1$ space.  Interpolation explains the rest. My question is whether there is a way to understand the non-blow-up of the said $L^p$ norms of the Fourier transform $\hat u$ without invoking the conservation law for the $L^\infty$ norm of $u$ . What I seek is some kind of direct Fourier-analytic way to see what is going on. I have reached an impasse. I will be very grateful for any insight or advice.","I am struggling to understand the behavior of the Fourier transform (in the variable) of initially smooth solutions of the hyperbolic Burgers' equation in 1-D, . I start with a smooth and rapidly decaying initial condition on . This solution evolves in time until it breaks down. At the time of first breakdown I look at the Fourier transform of the solution . In particular, I am trying hard to understand how and why the norms of the Fourier transform remain finite at the time of first blow-up for . I think that if one uses weak (or Lorentz) norms, then this non-blow-up extends even to the weak norm. The only way I have been able to understand this property is via the convervation law for the norm of .   For the norm to be defined at the time of first blow-up, the Fourier transform needs to remain in a weak space.  Interpolation explains the rest. My question is whether there is a way to understand the non-blow-up of the said norms of the Fourier transform without invoking the conservation law for the norm of . What I seek is some kind of direct Fourier-analytic way to see what is going on. I have reached an impasse. I will be very grateful for any insight or advice.","x  \partial_t u + u~ \partial_x u =0 u(x,t)=u_0(x) \Bbb R t=T \hat u(k,T) u(x,T) L^p \hat u p>1 L^1 L^\infty u \|u \|_{L^\infty}  L^1 L^p \hat u L^\infty u","['functional-analysis', 'fourier-analysis', 'partial-differential-equations']"
56,Spectral radii and norms of similar elements in a C*-algebra: $\|bab^{-1}\|<1$ if $b=(\sum_{n=0}^\infty (a^*)^n a^n)^{1/2}$,Spectral radii and norms of similar elements in a C*-algebra:  if,\|bab^{-1}\|<1 b=(\sum_{n=0}^\infty (a^*)^n a^n)^{1/2},"Let $A$ be a unital $C^*$-algebra and $a\in A$ such that $r(a) < 1$. Define b  = $(\sum_{n=0}^\infty (a^*)^n a^n)^{1/2}$. We can prove that $b\geq e$ and that $b$ is invertible. I want to show $\| b a b^{-1} \| < 1$. From the definition of $b$ we see that $a^* b^2 a = b^2-e$ and we know $r(bab^{-1}) = r(a) <1$. So it suffices to prove $r(b a b^{-1}) = \| b a b^{-1} \|$. It can follow from the fact $c = ba b^{-1}$ is a normal element... I don't know how to prove it (I have tried to compare $c^* c$ and $c c^*$...). Context: The question appears in Murphy's book, page 74.   I have managed to prove the first part. The second part of the question is to prove $$r(a)= \inf_{c\in Inv(A)}\{\|cac^{-1} \| \}$$ It's easy to see $$r(a) \leq \inf_{c\in Inv(A)}\{\|cac^{-1} \| \}$$ But I can't prove $$r(a) \geq \inf_{c\in Inv(A)}\{\|cac^{-1} \| \}$$ If we have had $r(a) = \|b a b^{-1} \|$ then it was obvious... But this is not true, so how we can prove this inequality?","Let $A$ be a unital $C^*$-algebra and $a\in A$ such that $r(a) < 1$. Define b  = $(\sum_{n=0}^\infty (a^*)^n a^n)^{1/2}$. We can prove that $b\geq e$ and that $b$ is invertible. I want to show $\| b a b^{-1} \| < 1$. From the definition of $b$ we see that $a^* b^2 a = b^2-e$ and we know $r(bab^{-1}) = r(a) <1$. So it suffices to prove $r(b a b^{-1}) = \| b a b^{-1} \|$. It can follow from the fact $c = ba b^{-1}$ is a normal element... I don't know how to prove it (I have tried to compare $c^* c$ and $c c^*$...). Context: The question appears in Murphy's book, page 74.   I have managed to prove the first part. The second part of the question is to prove $$r(a)= \inf_{c\in Inv(A)}\{\|cac^{-1} \| \}$$ It's easy to see $$r(a) \leq \inf_{c\in Inv(A)}\{\|cac^{-1} \| \}$$ But I can't prove $$r(a) \geq \inf_{c\in Inv(A)}\{\|cac^{-1} \| \}$$ If we have had $r(a) = \|b a b^{-1} \|$ then it was obvious... But this is not true, so how we can prove this inequality?",,"['functional-analysis', 'c-star-algebras']"
57,$L^{2}$ functions,functions,L^{2},"Let $f(x)$ be a continuous function for all $x\in \mathbb R$, such that $f\in L^{2}(\mathbb R)$ (i.e., $\int_{-\infty}^{\infty}|f(x)|^{2}dx<\infty$), and define $$f_{o}(x):=\sup_{|x-y|\leq 1}|f(y)|$$ How to prove that $f_{o}\in L^{2}(\mathbb R)$, and $\|f_{o}\|_{L^{2}}\leq A\|f\|_{L^{2}}$, for some constant $A>0$? My progress is the follwoing, so correct me if I'm wrong, and advise me if I'm missing something: We can construct a function $g\in S(\mathbb R)$ (Schwartz class) with $\hat{g}=1$, so $\hat{f}=\hat{f}\hat{g}$, hence $f=f*g$ (convolution), then $$f_{o}(x)\leq (|f|*g_{o})(x)$$  which implies that $\|f_{o}\|_{L^{2}}\leq \|(|f|*g_{o})\|_{L^{2}}\leq \|f\|_{L^{2}} \|g_{o}\|_{L^{1}}$.","Let $f(x)$ be a continuous function for all $x\in \mathbb R$, such that $f\in L^{2}(\mathbb R)$ (i.e., $\int_{-\infty}^{\infty}|f(x)|^{2}dx<\infty$), and define $$f_{o}(x):=\sup_{|x-y|\leq 1}|f(y)|$$ How to prove that $f_{o}\in L^{2}(\mathbb R)$, and $\|f_{o}\|_{L^{2}}\leq A\|f\|_{L^{2}}$, for some constant $A>0$? My progress is the follwoing, so correct me if I'm wrong, and advise me if I'm missing something: We can construct a function $g\in S(\mathbb R)$ (Schwartz class) with $\hat{g}=1$, so $\hat{f}=\hat{f}\hat{g}$, hence $f=f*g$ (convolution), then $$f_{o}(x)\leq (|f|*g_{o})(x)$$  which implies that $\|f_{o}\|_{L^{2}}\leq \|(|f|*g_{o})\|_{L^{2}}\leq \|f\|_{L^{2}} \|g_{o}\|_{L^{1}}$.",,"['functional-analysis', 'hilbert-spaces']"
58,$\ell_p$ sums of Banach spaces,sums of Banach spaces,\ell_p,"Let $p\in (1,\infty)$ and let $(E_\alpha)_{\alpha<\omega_1}$ be a family of Banach spaces. Set $E=\left(\bigoplus_{\alpha<\omega_1}E_\alpha\right)_{\ell_p(\omega_1)}$. Must $E$ be isomorphic to $\ell_p(E)$?","Let $p\in (1,\infty)$ and let $(E_\alpha)_{\alpha<\omega_1}$ be a family of Banach spaces. Set $E=\left(\bigoplus_{\alpha<\omega_1}E_\alpha\right)_{\ell_p(\omega_1)}$. Must $E$ be isomorphic to $\ell_p(E)$?",,"['functional-analysis', 'banach-spaces']"
59,"If $T$ has finite rank, then: $I-T$ is injective if and only if $I-T$ is surjective?","If  has finite rank, then:  is injective if and only if  is surjective?",T I-T I-T,"I have a Banach Space $X$ and an linear continuous operator $T\colon X\to X$ that has finite rank (i.e. $\dim {T(X)}<\infty$). Then, $I-T$ is injective if and only if $I-T$ is surjective?","I have a Banach Space $X$ and an linear continuous operator $T\colon X\to X$ that has finite rank (i.e. $\dim {T(X)}<\infty$). Then, $I-T$ is injective if and only if $I-T$ is surjective?",,"['functional-analysis', 'banach-spaces', 'operator-theory']"
60,Tempered distribution concentrated in a lower dimensional manifold,Tempered distribution concentrated in a lower dimensional manifold,,"Question: What can you conclude about a tempered distribution $G\ \in\ S'(R^n)$ that is concentrated in some k-dimensional manifold $M\ \subset\ R^n$ (for k < n)?  More specifically, is there a result analogous to the following n=1 result? $n=1$ result (hope I remember it correctly): Let $\ S(R)\ $ be the set of Schwartz functions ($C^\infty$ functions $f:\ R\ \to\ C\ $ s.t. $\ f^{(n)}$ goes to 0 at infinity faster than any inverse power of x (for n=0, 1, ...)).  Let $\ S'(R)\ $ be the set of tempered distributions.  $G\ \in\ S'(R)$ is said to be concentrated in a set $A\ \subset\ R\ $ iff $\forall\ \phi\ \in\ S(R)$ that vanishes on some open set $B\ \supset\ A\ $, $G(\phi)\ =\ 0$. Suppose $\ G\ $ is concentrated in {$\ x\ $}, for some $x\ \in\ R$.  Then $\exists\ c_0,\ ...\ c_L\ \in\ C\ $ s.t. $\ G\ $ = $\sum_{j=0}^L\ c_j\ \delta_x^{(j)}$. [where $\delta_x^{(j)}\ (\phi)\ \equiv\ (-1)^j\ \phi^{(j)}(x)\ $]","Question: What can you conclude about a tempered distribution $G\ \in\ S'(R^n)$ that is concentrated in some k-dimensional manifold $M\ \subset\ R^n$ (for k < n)?  More specifically, is there a result analogous to the following n=1 result? $n=1$ result (hope I remember it correctly): Let $\ S(R)\ $ be the set of Schwartz functions ($C^\infty$ functions $f:\ R\ \to\ C\ $ s.t. $\ f^{(n)}$ goes to 0 at infinity faster than any inverse power of x (for n=0, 1, ...)).  Let $\ S'(R)\ $ be the set of tempered distributions.  $G\ \in\ S'(R)$ is said to be concentrated in a set $A\ \subset\ R\ $ iff $\forall\ \phi\ \in\ S(R)$ that vanishes on some open set $B\ \supset\ A\ $, $G(\phi)\ =\ 0$. Suppose $\ G\ $ is concentrated in {$\ x\ $}, for some $x\ \in\ R$.  Then $\exists\ c_0,\ ...\ c_L\ \in\ C\ $ s.t. $\ G\ $ = $\sum_{j=0}^L\ c_j\ \delta_x^{(j)}$. [where $\delta_x^{(j)}\ (\phi)\ \equiv\ (-1)^j\ \phi^{(j)}(x)\ $]",,['functional-analysis']
61,"Simultaneous orthogonal basis for $L^2$, $H^1_0$, ... $H^k_0$","Simultaneous orthogonal basis for , , ...",L^2 H^1_0 H^k_0,"Let $\Omega \in R^n$ be open, bounded and with smooth boundary. Can you prove the existence of a system of vectors that simultaneously forms an orthogonal basis both in $L^2(\Omega)$ and $H^1_0(\Omega)$? Can you generalize this construction to obtain a simultaneous orthogonal basis for $H^0(\Omega),\cdots,H^k_0(\Omega)$?","Let $\Omega \in R^n$ be open, bounded and with smooth boundary. Can you prove the existence of a system of vectors that simultaneously forms an orthogonal basis both in $L^2(\Omega)$ and $H^1_0(\Omega)$? Can you generalize this construction to obtain a simultaneous orthogonal basis for $H^0(\Omega),\cdots,H^k_0(\Omega)$?",,['functional-analysis']
62,"Extension of previous problem, involving $\ell^p$ norm circles","Extension of previous problem, involving  norm circles",\ell^p,"If you look at this previous problem , I asked how to find the sum of all the areas between two taxicab geometry circles. However, upon learning about $\ell^p$ norms , I thought it would be pretty interesting to extend the problem to all $\ell^p$ norm circles, not just $\ell^1$ (taxicab). If $p=1$, then the result has already been found (the total area is $\frac{5k^2-k-4}{2}$). If $p = \infty$, then each ""circle"" is just a square, and the area is also easily found (I'm too tired to think about it, but I think it would just be $4(k^2-1)$). Is there, however, a general formula for the area of each circle and the total area of the regions between circles in terms of $k$ and $p$; that is, what is the equation for the area of each overlapping region? The area of an individual circle, if I did it correctly, is the area of a Lamé curve with $r = p$ and a radius of $k-n$ (see the linked problem ), which equals $\displaystyle 4(k-n)^2\frac{(\Gamma(1+\frac{1}{p})^2)}{\Gamma(1+\frac{2}{p})}$. This can be reduced to $\displaystyle 2(k-n)^2 \frac{\Gamma(\frac{1}{p})^2}{p \Gamma(\frac{2}{p})}$ (see equations 41 and 42 here ). Here are some explanatory pictures: $k=5, p=1$ $k=5, p=2$ $k=5, p=3$","If you look at this previous problem , I asked how to find the sum of all the areas between two taxicab geometry circles. However, upon learning about $\ell^p$ norms , I thought it would be pretty interesting to extend the problem to all $\ell^p$ norm circles, not just $\ell^1$ (taxicab). If $p=1$, then the result has already been found (the total area is $\frac{5k^2-k-4}{2}$). If $p = \infty$, then each ""circle"" is just a square, and the area is also easily found (I'm too tired to think about it, but I think it would just be $4(k^2-1)$). Is there, however, a general formula for the area of each circle and the total area of the regions between circles in terms of $k$ and $p$; that is, what is the equation for the area of each overlapping region? The area of an individual circle, if I did it correctly, is the area of a Lamé curve with $r = p$ and a radius of $k-n$ (see the linked problem ), which equals $\displaystyle 4(k-n)^2\frac{(\Gamma(1+\frac{1}{p})^2)}{\Gamma(1+\frac{2}{p})}$. This can be reduced to $\displaystyle 2(k-n)^2 \frac{\Gamma(\frac{1}{p})^2}{p \Gamma(\frac{2}{p})}$ (see equations 41 and 42 here ). Here are some explanatory pictures: $k=5, p=1$ $k=5, p=2$ $k=5, p=3$",,"['calculus', 'real-analysis', 'geometry', 'functional-analysis']"
63,Bound on convolution: $ | (h * f^2) (x)| \leq \| f\|^2_2 g(h)$,Bound on convolution:, | (h * f^2) (x)| \leq \| f\|^2_2 g(h),"I am trying to find bounds for the following quantity. Take two functions $f,h \in L^{1} \cap L^2$ but $\|h \|_{\infty} = \infty$ . Is there a way to obtain a bound of the following type: $$ | (h * f^2) (x)| \leq \| f\|^2_2 g(h)$$ where $g$ is some function of $h$ ? This would be the case if we could take the sup norm of $h$ out of the integral by using Holder's inequality, but in this case is not allowed as $\|h \|_{\infty} = \infty$ . PS: you can also assume that $(h * f^2) (x)$ is everywhere well-defined.","I am trying to find bounds for the following quantity. Take two functions but . Is there a way to obtain a bound of the following type: where is some function of ? This would be the case if we could take the sup norm of out of the integral by using Holder's inequality, but in this case is not allowed as . PS: you can also assume that is everywhere well-defined.","f,h \in L^{1} \cap L^2 \|h \|_{\infty} = \infty  | (h * f^2) (x)| \leq \| f\|^2_2 g(h) g h h \|h \|_{\infty} = \infty (h * f^2) (x)","['real-analysis', 'functional-analysis', 'measure-theory', 'convolution', 'holder-inequality']"
64,Inequality related to function $f(x)=x\ln x-\frac{x^2}e+ax$,Inequality related to function,f(x)=x\ln x-\frac{x^2}e+ax,"I'm trying to solve a hard question about function and here is the question below: For the given function $f(x)=x\ln x-\frac{x^2}e+ax$ (where $a$ is a parameter), we have such property: $$\exists\;x_1,x_2\,(x_1\not=x_2)\quad s.t.\quad f(x_1)=f(x_2)\quad f'(x_1)=f'(x_2)$$ In this case, prove the inequality $$(a+1)^2x_1x_2-\sqrt{x_1x_2}\lt\frac34$$ To work this one out, I have tried a few things and finally got this $$x_1x_2=\frac1{e^{2a}}\lt\frac{e^2}4$$ Can anyone help me continue this? (I'll put my work below.) My work of proving $x_1x_2=\frac1{e^{2a}}\lt\frac{e^2}4$ First, I calculate the derivative. $$f'(x)=\ln x-\frac{2x}e+a+1$$ Because of the property of $f(x)$ , we have the following equations: $$\begin{cases} f(x_1)=f(x_2)\Rightarrow x_1\ln x_1-\frac{{x_1}^2}e+ax_1=x_2\ln x_2-\frac{{x_2}^2}e+ax_2\\ f'(x_1)=f'(x_2)\Rightarrow\ln x_1-\frac{2x_1}e=\ln x_2-\frac{2x_2}e \end{cases}$$ Simplify the second equation, we get $$x_1-x_2=\frac e2(\ln x_1-\ln x_2)\tag{*}\label{*}$$ Simplify the first one, we have $$x_1\ln x_1-x_2\ln x_2=\frac1e(x_1^2-x_2^2)-a(x_1-x_2)$$ $$\Rightarrow x_1\ln x_1-x_2\ln x_2=\frac1e(x_1-x_2)(x_1+x_2)-a(x_1-x_2)$$ $$\Rightarrow x_1\ln x_1-x_2\ln x_2\stackrel{\eqref{*}}=\frac12(\ln x_1-\ln x_2)(x_1+x_2)-\frac{ae}2(\ln x_1-\ln x_2)$$ $$\Rightarrow ea(\ln x_1-\ln x_2)=-(x_1-x_2)(\ln x_1+\ln x_2)$$ $$\Rightarrow 2a(\ln x_1-\ln x_2)\stackrel{\eqref{*}}=-(\ln x_1-\ln x_2)(\ln x_1+\ln x_2)$$ $$\Rightarrow \ln x_1x_2=-2a$$ $$\Rightarrow x_1x_2=\frac1{e^{2a}}$$ Then, let's calculate a vague range of a. $$f''(x)=\frac1x-\frac2e$$ So at $x=\frac e2$ , $f'(x)$ has a maximum. $$[f'(x)]_{max}=f'(\frac e2)=-\ln2+1+a$$ In order to satisfy the property of $f(x)$ , it is obvious that there must be an interval where $f'(x)\gt0$ . So we have $$[f'(x)]_{max}=-\ln2+1+a\gt0$$ $$\Rightarrow a\gt\ln2-1$$ $$\Rightarrow x_1x_2=\frac1{e^{2a}}\lt\frac{e^2}4$$","I'm trying to solve a hard question about function and here is the question below: For the given function (where is a parameter), we have such property: In this case, prove the inequality To work this one out, I have tried a few things and finally got this Can anyone help me continue this? (I'll put my work below.) My work of proving First, I calculate the derivative. Because of the property of , we have the following equations: Simplify the second equation, we get Simplify the first one, we have Then, let's calculate a vague range of a. So at , has a maximum. In order to satisfy the property of , it is obvious that there must be an interval where . So we have","f(x)=x\ln x-\frac{x^2}e+ax a \exists\;x_1,x_2\,(x_1\not=x_2)\quad s.t.\quad f(x_1)=f(x_2)\quad f'(x_1)=f'(x_2) (a+1)^2x_1x_2-\sqrt{x_1x_2}\lt\frac34 x_1x_2=\frac1{e^{2a}}\lt\frac{e^2}4 x_1x_2=\frac1{e^{2a}}\lt\frac{e^2}4 f'(x)=\ln x-\frac{2x}e+a+1 f(x) \begin{cases}
f(x_1)=f(x_2)\Rightarrow x_1\ln x_1-\frac{{x_1}^2}e+ax_1=x_2\ln x_2-\frac{{x_2}^2}e+ax_2\\
f'(x_1)=f'(x_2)\Rightarrow\ln x_1-\frac{2x_1}e=\ln x_2-\frac{2x_2}e
\end{cases} x_1-x_2=\frac e2(\ln x_1-\ln x_2)\tag{*}\label{*} x_1\ln x_1-x_2\ln x_2=\frac1e(x_1^2-x_2^2)-a(x_1-x_2) \Rightarrow x_1\ln x_1-x_2\ln x_2=\frac1e(x_1-x_2)(x_1+x_2)-a(x_1-x_2) \Rightarrow x_1\ln x_1-x_2\ln x_2\stackrel{\eqref{*}}=\frac12(\ln x_1-\ln x_2)(x_1+x_2)-\frac{ae}2(\ln x_1-\ln x_2) \Rightarrow ea(\ln x_1-\ln x_2)=-(x_1-x_2)(\ln x_1+\ln x_2) \Rightarrow 2a(\ln x_1-\ln x_2)\stackrel{\eqref{*}}=-(\ln x_1-\ln x_2)(\ln x_1+\ln x_2) \Rightarrow \ln x_1x_2=-2a \Rightarrow x_1x_2=\frac1{e^{2a}} f''(x)=\frac1x-\frac2e x=\frac e2 f'(x) [f'(x)]_{max}=f'(\frac e2)=-\ln2+1+a f(x) f'(x)\gt0 [f'(x)]_{max}=-\ln2+1+a\gt0 \Rightarrow a\gt\ln2-1 \Rightarrow x_1x_2=\frac1{e^{2a}}\lt\frac{e^2}4","['real-analysis', 'calculus', 'functional-analysis', 'derivatives', 'inequality']"
65,Uniform Integrability and relative compactness,Uniform Integrability and relative compactness,,"I am trying to proof relative compactness in L2(0,1) for a specific set of functions $(\phi_n)_{n \in \mathbb{N}}$ with following properties: $\int_0^1 \phi(x) dx = 0 $ $||\phi_n^2||_{L1(0,1)} = 1 $ $(\phi_n)_{n \in \mathbb{N}}$ is uniformly integrable such that for each $\epsilon >0$ there exist a $\delta> 0$ fulfilling: $$\sup_{n \in \mathbb{N}} \int_0^1 \phi_n(x)^2*\mathbb{1}\left(|\phi_n(x)| > \delta\right) dx < \epsilon $$ Here, $\mathbb{1}$ is the indicator function resulting in 1 if the inner statement is true. Each $\phi$ is non decreasing and has at most countable many discontinuity points. (Otherwise it is continuous) I tried to use the theorem of Kolmogorow-Riesz, where I have to proof boundedness in the norm (which is already given above) and equicontinuity: $$ \lim_{h \rightarrow 0} \sup_{n \in \mathbb{N}} \int_0^1 \left(\phi_n(x+h) - \phi_n(x)\right)^2 dx$$ But I have difficulties to show the equicontinuity, since I do not find a possibility to use the given uniform integrability. I plotted many examples and I think that this property is necessary in this scenario. Without this property I think there could be a $\phi_n$ that has all its mass at the border of 0 and 1 so that the equicontinuity can not hold anymore. I Imagine that the uniform integrability should lead to distributing the mass uniformly so that the actual difference of $\phi_n(x+h)-\phi_n(x)$ is bounded in terms of h. Does anyone have a source where something like this is done or is there a more elegant way than Kolmogorow-Riesz for showing the relative compactness?","I am trying to proof relative compactness in L2(0,1) for a specific set of functions with following properties: is uniformly integrable such that for each there exist a fulfilling: Here, is the indicator function resulting in 1 if the inner statement is true. Each is non decreasing and has at most countable many discontinuity points. (Otherwise it is continuous) I tried to use the theorem of Kolmogorow-Riesz, where I have to proof boundedness in the norm (which is already given above) and equicontinuity: But I have difficulties to show the equicontinuity, since I do not find a possibility to use the given uniform integrability. I plotted many examples and I think that this property is necessary in this scenario. Without this property I think there could be a that has all its mass at the border of 0 and 1 so that the equicontinuity can not hold anymore. I Imagine that the uniform integrability should lead to distributing the mass uniformly so that the actual difference of is bounded in terms of h. Does anyone have a source where something like this is done or is there a more elegant way than Kolmogorow-Riesz for showing the relative compactness?","(\phi_n)_{n \in \mathbb{N}} \int_0^1 \phi(x) dx = 0  ||\phi_n^2||_{L1(0,1)} = 1  (\phi_n)_{n \in \mathbb{N}} \epsilon >0 \delta> 0 \sup_{n \in \mathbb{N}} \int_0^1 \phi_n(x)^2*\mathbb{1}\left(|\phi_n(x)| > \delta\right) dx < \epsilon  \mathbb{1} \phi  \lim_{h \rightarrow 0} \sup_{n \in \mathbb{N}} \int_0^1 \left(\phi_n(x+h) - \phi_n(x)\right)^2 dx \phi_n \phi_n(x+h)-\phi_n(x)","['real-analysis', 'functional-analysis', 'compactness', 'uniform-integrability', 'equicontinuity']"
66,Integration by parts for distributions (generalized function),Integration by parts for distributions (generalized function),,"The following question comes from a research paper. I have simplified some statements and backgrounds. For any smooth compactly supported scalar function $u(x,t)$ and $v(x,t)$ defined on $(x,t) \in \mathbb{R}^2,$ and given a first of operator $L=\partial_t+\partial_x$ , we denote its adjoint operator is $L^*=-\partial_t-\partial_x$ , the classical integration by parts told us that $$ \int_{t_1}^{t_2} \int_{\mathbb{R}}  Lu \cdot v \ dx dt=\int_{t_1}^{t_2} \int_{\mathbb{R}}  u \cdot L^*v \ dx dt+\int_{\mathbb{R}} (uv)|_{t=t_2}dx-\int_{\mathbb{R}} (uv)|_{t=t_1}dx. $$ Now I am stating some properties we have already deduced. If $u \in L^2(\mathbb{R}^2), Lu \in L^2(\mathbb{R}^2)$ ( $Lu$ is understood as a distribution), then for any $t_0 \in \mathbb{R},$ we have $u|_{t=t_0} \in H^{-1/2}(\mathbb{R})$ and the following estimate holds $$ \| u|_{t=t_0}\|_{H^{-1/2}(\mathbb{R})} \leq C(\| u\|_{L^2(\mathbb{R}^2)}+\| Lu \|_{L^2(\mathbb{R}^2)}). $$ Moreover, $\mathcal{D}(\mathbb{R}^2)$ (compactly supported functions) are dense in $\{(u,Lu): u \in L^2(\mathbb{R}^2),Lu \in L^2(\mathbb{R}^2) \}$ with respect to the graph norm $\| u\|_{L^2(\mathbb{R}^2)}+\| Lu\|_{L^2(\mathbb{R}^2)}.$ Hence, for $u,Lu \in L^2$ , we know that there is a sequence of functions $u_n \in \mathcal{D}(\mathbb{R}^2)$ so that $u_n,L u_n$ converge to $u,Lu$ in $L^2(\mathbb{R}^2),$ and $u_n|_{t=t_0}$ converges to $u|_{t=t_0}$ in $H^{-1/2}(\mathbb{R})$ for any $t_0 \in \mathbb{R}.$ Clearly, for any compactly supported function $v$ and $u_n,$ the classical integration by parts formula holds: \begin{equation} \begin{aligned} \int_{t_1}^{t_2} \int_{\mathbb{R}}  Lu_n \cdot v dt&=\int_{t_1}^{t_2} \int_{\mathbb{R}}  u_n \cdot L^*v dt+\int_{\mathbb{R}} (u_nv)|_{t=t_2}dx-\int_{\mathbb{R}} (u_nv)|_{t=t_1}dx \\ &=\int_{t_1}^{t_2} \int_{\mathbb{R}}  u_n \cdot L^*v dt+\langle u_n|_{t=t_2},v|_{t=t_2}\rangle_{\mathcal{D}',\mathcal{D}}-\langle u_n|_{t=t_1},v|_{t=t_1}\rangle_{\mathcal{D}',\mathcal{D}}. \end{aligned} \end{equation} The paper said that if we let $n\rightarrow +\infty$ , then we have \begin{equation} \begin{aligned} \int_{t_1}^{t_2} \int_{\mathbb{R}}  Lu \cdot v dt=-\int_{t_1}^{t_2} \int_{\mathbb{R}}  u \cdot L^*v dt+\langle u|_{t=t_2-},v|_{t=t_2}\rangle_{\mathcal{D}',\mathcal{D}}-\langle u|_{t=t_1+},v|_{t=t_1}\rangle_{\mathcal{D}',\mathcal{D}}. \end{aligned} \end{equation} I am confused about the strict meaning of $u|_{t=t_1+}$ and $u|_{t=t_2-}$ as distribution here. It looks like the Lebesgue-Stieltjes integral, but I don't know how to deduce the expression. Can anyone help me? Any references are welcomed.","The following question comes from a research paper. I have simplified some statements and backgrounds. For any smooth compactly supported scalar function and defined on and given a first of operator , we denote its adjoint operator is , the classical integration by parts told us that Now I am stating some properties we have already deduced. If ( is understood as a distribution), then for any we have and the following estimate holds Moreover, (compactly supported functions) are dense in with respect to the graph norm Hence, for , we know that there is a sequence of functions so that converge to in and converges to in for any Clearly, for any compactly supported function and the classical integration by parts formula holds: The paper said that if we let , then we have I am confused about the strict meaning of and as distribution here. It looks like the Lebesgue-Stieltjes integral, but I don't know how to deduce the expression. Can anyone help me? Any references are welcomed.","u(x,t) v(x,t) (x,t) \in \mathbb{R}^2, L=\partial_t+\partial_x L^*=-\partial_t-\partial_x 
\int_{t_1}^{t_2} \int_{\mathbb{R}}  Lu \cdot v \ dx dt=\int_{t_1}^{t_2} \int_{\mathbb{R}}  u \cdot L^*v \ dx dt+\int_{\mathbb{R}} (uv)|_{t=t_2}dx-\int_{\mathbb{R}} (uv)|_{t=t_1}dx.
 u \in L^2(\mathbb{R}^2), Lu \in L^2(\mathbb{R}^2) Lu t_0 \in \mathbb{R}, u|_{t=t_0} \in H^{-1/2}(\mathbb{R}) 
\| u|_{t=t_0}\|_{H^{-1/2}(\mathbb{R})} \leq C(\| u\|_{L^2(\mathbb{R}^2)}+\| Lu \|_{L^2(\mathbb{R}^2)}).
 \mathcal{D}(\mathbb{R}^2) \{(u,Lu): u \in L^2(\mathbb{R}^2),Lu \in L^2(\mathbb{R}^2) \} \| u\|_{L^2(\mathbb{R}^2)}+\| Lu\|_{L^2(\mathbb{R}^2)}. u,Lu \in L^2 u_n \in \mathcal{D}(\mathbb{R}^2) u_n,L u_n u,Lu L^2(\mathbb{R}^2), u_n|_{t=t_0} u|_{t=t_0} H^{-1/2}(\mathbb{R}) t_0 \in \mathbb{R}. v u_n, \begin{equation}
\begin{aligned}
\int_{t_1}^{t_2} \int_{\mathbb{R}}  Lu_n \cdot v dt&=\int_{t_1}^{t_2} \int_{\mathbb{R}}  u_n \cdot L^*v dt+\int_{\mathbb{R}} (u_nv)|_{t=t_2}dx-\int_{\mathbb{R}} (u_nv)|_{t=t_1}dx \\
&=\int_{t_1}^{t_2} \int_{\mathbb{R}}  u_n \cdot L^*v dt+\langle u_n|_{t=t_2},v|_{t=t_2}\rangle_{\mathcal{D}',\mathcal{D}}-\langle u_n|_{t=t_1},v|_{t=t_1}\rangle_{\mathcal{D}',\mathcal{D}}.
\end{aligned}
\end{equation} n\rightarrow +\infty \begin{equation}
\begin{aligned}
\int_{t_1}^{t_2} \int_{\mathbb{R}}  Lu \cdot v dt=-\int_{t_1}^{t_2} \int_{\mathbb{R}}  u \cdot L^*v dt+\langle u|_{t=t_2-},v|_{t=t_2}\rangle_{\mathcal{D}',\mathcal{D}}-\langle u|_{t=t_1+},v|_{t=t_1}\rangle_{\mathcal{D}',\mathcal{D}}.
\end{aligned}
\end{equation} u|_{t=t_1+} u|_{t=t_2-}","['functional-analysis', 'distribution-theory']"
67,Converse of existence of minimizers.,Converse of existence of minimizers.,,"Given $(V, \|\cdot\|)$ a real normed linear space, suppose it has the property that given any closed convex set $K$ , there exists a $u_0 \in K$ such that $\|u_0\| \leq \|u\|$ for any $u \in K$ . Does it imply the space is complete? What if we have a real inner product space instead? Does it imply our space is actually a Hilbert Space? The proof in establishing the existence of the minimizers in Hilbert space uses completeness at a crucial step but what I would like to know if that is actually necessary in general. Edit . I think the following argument can show that if $(V,\langle \cdot , \cdot \rangle)$ is an inner product space, then existence of these minimizers implies that the space is complete: Given any closed subspace $M \subset V$ , We can still define the orthogonal projection operator $P: V \rightarrow M$ such that $P(x)$ is the closest point to $x$ in that subspace. This will tell us that the decomposition of $V$ into $M \bigoplus M^{\perp}$ still goes through, and thus Riesz Representation Theorem still holds for any such space. Thus as $V^* \cong V$ , the space is complete. Does there exist anything for general Banach Spaces? (Perhaps it is necessary to add the existence of a unique minimizer)","Given a real normed linear space, suppose it has the property that given any closed convex set , there exists a such that for any . Does it imply the space is complete? What if we have a real inner product space instead? Does it imply our space is actually a Hilbert Space? The proof in establishing the existence of the minimizers in Hilbert space uses completeness at a crucial step but what I would like to know if that is actually necessary in general. Edit . I think the following argument can show that if is an inner product space, then existence of these minimizers implies that the space is complete: Given any closed subspace , We can still define the orthogonal projection operator such that is the closest point to in that subspace. This will tell us that the decomposition of into still goes through, and thus Riesz Representation Theorem still holds for any such space. Thus as , the space is complete. Does there exist anything for general Banach Spaces? (Perhaps it is necessary to add the existence of a unique minimizer)","(V, \|\cdot\|) K u_0 \in K \|u_0\| \leq \|u\| u \in K (V,\langle \cdot , \cdot \rangle) M \subset V P: V \rightarrow M P(x) x V M \bigoplus M^{\perp} V^* \cong V","['real-analysis', 'functional-analysis']"
68,Friedrich's second inequality for functions with zero mean value,Friedrich's second inequality for functions with zero mean value,,"Friedrich's second inequality (or Maxwell Estimate or Gaffney’s inequality in the literature) is referred as follows: For all $\mathbf{u} \in H^1(\Omega)^2$ satisfying either $\mathbf{n} \cdot \mathbf{u}=0$ or $\mathbf{n} \times \mathbf{u}=\mathbf{0}$ on $\partial \Omega$ where $\Omega$ is a simply connected domain with Lipchitz boundary, then $$ \|\mathbf{u}\|_{L^2(\Omega)} \leq C_1\left(\|\nabla \cdot \mathbf{u}\|_{L^2(\Omega)}+\|\nabla \times \mathbf{u}\|_{L^2(\Omega)}\right). $$ My question is that if the boundary condition is satisfied, we only know the mean value of $\mathbf{u}$ is $0$ or simply replace the LHS with $\|\mathbf{u}-\frac{1}{|\Omega|}\int_{\Omega}\mathbf{u}\|_{L^2(\Omega)}$ . Does the inequality above hold? Or at least, particularly, if $\mathbf{u}$ is divergence-free, do we have $$ \|\mathbf{u}-\frac{1}{|\Omega|}\int_{\Omega}\mathbf{u}\|_{L^2(\Omega)} \leq C_1\|\nabla \times \mathbf{u}\|_{L^2(\Omega)}. $$ Thank you in advance!","Friedrich's second inequality (or Maxwell Estimate or Gaffney’s inequality in the literature) is referred as follows: For all satisfying either or on where is a simply connected domain with Lipchitz boundary, then My question is that if the boundary condition is satisfied, we only know the mean value of is or simply replace the LHS with . Does the inequality above hold? Or at least, particularly, if is divergence-free, do we have Thank you in advance!","\mathbf{u} \in H^1(\Omega)^2 \mathbf{n} \cdot \mathbf{u}=0 \mathbf{n} \times \mathbf{u}=\mathbf{0} \partial \Omega \Omega 
\|\mathbf{u}\|_{L^2(\Omega)} \leq C_1\left(\|\nabla \cdot \mathbf{u}\|_{L^2(\Omega)}+\|\nabla \times \mathbf{u}\|_{L^2(\Omega)}\right).
 \mathbf{u} 0 \|\mathbf{u}-\frac{1}{|\Omega|}\int_{\Omega}\mathbf{u}\|_{L^2(\Omega)} \mathbf{u} 
\|\mathbf{u}-\frac{1}{|\Omega|}\int_{\Omega}\mathbf{u}\|_{L^2(\Omega)} \leq C_1\|\nabla \times \mathbf{u}\|_{L^2(\Omega)}.
","['functional-analysis', 'inequality', 'sobolev-spaces']"
69,A possible characterization of WSC spaces,A possible characterization of WSC spaces,,"A Banach space $X$ is weakly sequentially complete (WSC) if every weakly Cauchy sequence in $X$ is weakly convergent. I will use the following classical result: Rosenthal's $\ell_1$ theorem : Every bounded sequence in a Banach space $X$ either contains a weakly Cauchy subsequence, or a subsequence equivalent to the $\ell_1$ basis. It follows from the fact that a closed subspace of a WSC space is WSC, that every subpace of a WSC space is either reflexive or contains $\ell_1$ . This observation, if localized, actually gives us a characterization of WSC spaces, namely: Proposition. For a Banach space $X$ TFAE: $X$ is WSC; Every bounded sequence in $X$ has either a weakly convergent subsequence, or a subsequence equivalent to the $\ell_1$ basis; Every weakly precompact (i.e. not containing a sequence equivalent to the $\ell_1$ basis) bounded set in $X$ is relatively weakly compact. It would thus feel plausible that the ""nonlocalized"" version of this Proposition could be true as well. Namely, I would like to know if: Question: Let $X$ be a Banach space such that every closed subspace $Y$ of $X$ is either reflexive or contains $\ell_1$ , is $X$ then weakly sequentially complete? As mentioned above, the converse implication is true. I have, however, not managet to prove/disprove the Question above. Any help will be appreciated.","A Banach space is weakly sequentially complete (WSC) if every weakly Cauchy sequence in is weakly convergent. I will use the following classical result: Rosenthal's theorem : Every bounded sequence in a Banach space either contains a weakly Cauchy subsequence, or a subsequence equivalent to the basis. It follows from the fact that a closed subspace of a WSC space is WSC, that every subpace of a WSC space is either reflexive or contains . This observation, if localized, actually gives us a characterization of WSC spaces, namely: Proposition. For a Banach space TFAE: is WSC; Every bounded sequence in has either a weakly convergent subsequence, or a subsequence equivalent to the basis; Every weakly precompact (i.e. not containing a sequence equivalent to the basis) bounded set in is relatively weakly compact. It would thus feel plausible that the ""nonlocalized"" version of this Proposition could be true as well. Namely, I would like to know if: Question: Let be a Banach space such that every closed subspace of is either reflexive or contains , is then weakly sequentially complete? As mentioned above, the converse implication is true. I have, however, not managet to prove/disprove the Question above. Any help will be appreciated.",X X \ell_1 X \ell_1 \ell_1 X X X \ell_1 \ell_1 X X Y X \ell_1 X,"['functional-analysis', 'banach-spaces', 'weakly-cauchy-sequences']"
70,A singular average integral,A singular average integral,,"Let $f(\theta, \epsilon)$ be continuous on the unit circle for $\epsilon>0$ , $f(\theta, \epsilon)>0$ for $\epsilon>0$ , and $f(\theta, 0)=(\theta -\theta^*)^2h(\theta)$ with $h(\theta)>0$ for all $\theta \in [0,2\pi]$ . Is the following true? $$\lim _{\epsilon \rightarrow 0}\big(\int_0^{2\pi}\frac{\sin \theta}{f(\theta, \epsilon)}d\theta\big)\big(\int_0^{2\pi}\frac{1}{f(\theta, \epsilon) } d \theta\big)^{-1}=\sin (\theta^*).$$ Intuitively, the limit should give us the weighted average, and if we have a singularity the average should be $\sin \theta^*$ . I am not sure how to prove this.","Let be continuous on the unit circle for , for , and with for all . Is the following true? Intuitively, the limit should give us the weighted average, and if we have a singularity the average should be . I am not sure how to prove this.","f(\theta, \epsilon) \epsilon>0 f(\theta, \epsilon)>0 \epsilon>0 f(\theta, 0)=(\theta -\theta^*)^2h(\theta) h(\theta)>0 \theta \in [0,2\pi] \lim _{\epsilon \rightarrow 0}\big(\int_0^{2\pi}\frac{\sin \theta}{f(\theta, \epsilon)}d\theta\big)\big(\int_0^{2\pi}\frac{1}{f(\theta, \epsilon) } d \theta\big)^{-1}=\sin (\theta^*). \sin \theta^*","['real-analysis', 'calculus', 'functional-analysis', 'analysis', 'multivariable-calculus']"
71,Category theory textbook for Banach algebras and Banach spaces,Category theory textbook for Banach algebras and Banach spaces,,"Do you know of a textbook at the graduate level about the common categories of functional analysis (such as the 2 categories of Banach spaces, the 2 categories of Banach algebras, the category of Banach modules, the category of C $^*$ -algebras, the category of W $^*$ -algebras) which is suitable for a student who has taken a basic course in category theory? The ideal book covers the basic themes such as injective, projective, flat objects; limits & colimits; tensor products; injective envelopes; etc. of each category in the category theoretical context, with their context-specific equivalent definitions. It would be instructive if the book contains examples for the (existence and) non-existence results. I can't blame if you think that I have way too many criteria for the ideal book, so I can't match up with a great book that would fill up the gap in my life and my bookshelf.","Do you know of a textbook at the graduate level about the common categories of functional analysis (such as the 2 categories of Banach spaces, the 2 categories of Banach algebras, the category of Banach modules, the category of C -algebras, the category of W -algebras) which is suitable for a student who has taken a basic course in category theory? The ideal book covers the basic themes such as injective, projective, flat objects; limits & colimits; tensor products; injective envelopes; etc. of each category in the category theoretical context, with their context-specific equivalent definitions. It would be instructive if the book contains examples for the (existence and) non-existence results. I can't blame if you think that I have way too many criteria for the ideal book, so I can't match up with a great book that would fill up the gap in my life and my bookshelf.",^* ^*,"['functional-analysis', 'reference-request', 'category-theory', 'operator-algebras', 'book-recommendation']"
72,Making dense subset of continuous functions using a single continuous function,Making dense subset of continuous functions using a single continuous function,,"I have a question about set of continuous function on the compact interval. Denote the set of all continuous $n$ -dimensional real functions on $[0,L]$ as $\mathcal{C}_{[0,L]}$ ( $\mathcal{C}_{[0,L]} = \left\{ u: [0,L] \rightarrow \mathbb{R}^n\right\}$ ) Is there any $T>L$ and a continuous function $f: [0,T] \rightarrow \mathbb{R}^n$ such that finite combination of $L$ -length segments of $f$ is dense in $\mathcal{C}_{[0,L]}$ with respect to supremum norm? I mean if we define $f_{[t,t+L]}:[0,L] \rightarrow \mathbb{R}^n$ as $f_{[t,t+L]}(\cdot) = f(\cdot + t)$ for $t \in [0,T-L]$ , is it possible to find continuous function $f: [0,T] \rightarrow \mathbb{R}^n$ such that any finite linear combination of $f_{[t, t+L]}$ , is dense subset of $\mathcal{C}_{[0,L]}$ with respect to sup-norm. If not, is there any space of functions that satisfies a similar property? (e.g., rather $L^2$ space satisfies such a condition etc.) Thank you for your attention.","I have a question about set of continuous function on the compact interval. Denote the set of all continuous -dimensional real functions on as ( ) Is there any and a continuous function such that finite combination of -length segments of is dense in with respect to supremum norm? I mean if we define as for , is it possible to find continuous function such that any finite linear combination of , is dense subset of with respect to sup-norm. If not, is there any space of functions that satisfies a similar property? (e.g., rather space satisfies such a condition etc.) Thank you for your attention.","n [0,L] \mathcal{C}_{[0,L]} \mathcal{C}_{[0,L]} = \left\{ u: [0,L] \rightarrow \mathbb{R}^n\right\} T>L f: [0,T] \rightarrow \mathbb{R}^n L f \mathcal{C}_{[0,L]} f_{[t,t+L]}:[0,L] \rightarrow \mathbb{R}^n f_{[t,t+L]}(\cdot) = f(\cdot + t) t \in [0,T-L] f: [0,T] \rightarrow \mathbb{R}^n f_{[t, t+L]} \mathcal{C}_{[0,L]} L^2","['functional-analysis', 'dense-subspaces']"
73,Proof of relationship between Dirac Delta and Co-Area formula,Proof of relationship between Dirac Delta and Co-Area formula,,"In the Wikipedia page for the Dirac Delta function this formula appears under ""Properties in $n$ dimension"". $$ \int f(x) \delta(g(x)) dx = \int_{g^{-1}(0)} \frac{f(x)}{|\nabla g(x)|} d\sigma(x) $$ It is said that this is a consequence of the Co-Area formula but no proof is given and the only reference (""Hörmander (1983), The analysis of linear partial differential operators I"") doesn't seem to have this formula in it. I have a few questions, in order of importance. What is a proof of this statement? What other references are there about this statement and its generalizations to a function $g:\mathbb{R}^n\to\mathbb{R}^m$ with $n > m > 1$ ? In the above the author uses $\delta (g(x)) dx$ as if $\delta$ was a function, where in fact it is a Schwartz distribution or a measure. What did they mean? Especially because now it is concatenated with another function. Definition of Dirac Distribution It's a linear functional that maps test functions $\varphi$ to $$ \delta_x[\varphi] = \int \varphi(y) \delta_x^{\text{measure}}(dy) = \varphi(y) $$ where $\delta_x^{\text{measure}}$ is the Dirac Measure which for any measurable set $A$ is defined as $$ \delta_x^{\text{measure}}(A) = \begin{cases}     1 & x\in A \\     0 & x\notin A \end{cases} $$ Co-Area Formula for Lipschitz Functions If $g:\mathbb{R}^n\to\mathbb{R}^m$ with $n > m$ then $$ \int_{\mathbb{R}^n} f(x) dx = \int_{\mathbb{R}^m} \left[\int_{g^{-1}(y)} f(x) |J_g(x) J_g(x)^\top|^{-1/2} \mathcal{H}^{n-m}(dx) \right]dy  $$ where $J_g(x)$ is the Jacobian matrix of $g$ .","In the Wikipedia page for the Dirac Delta function this formula appears under ""Properties in dimension"". It is said that this is a consequence of the Co-Area formula but no proof is given and the only reference (""Hörmander (1983), The analysis of linear partial differential operators I"") doesn't seem to have this formula in it. I have a few questions, in order of importance. What is a proof of this statement? What other references are there about this statement and its generalizations to a function with ? In the above the author uses as if was a function, where in fact it is a Schwartz distribution or a measure. What did they mean? Especially because now it is concatenated with another function. Definition of Dirac Distribution It's a linear functional that maps test functions to where is the Dirac Measure which for any measurable set is defined as Co-Area Formula for Lipschitz Functions If with then where is the Jacobian matrix of .","n 
\int f(x) \delta(g(x)) dx = \int_{g^{-1}(0)} \frac{f(x)}{|\nabla g(x)|} d\sigma(x)
 g:\mathbb{R}^n\to\mathbb{R}^m n > m > 1 \delta (g(x)) dx \delta \varphi 
\delta_x[\varphi] = \int \varphi(y) \delta_x^{\text{measure}}(dy) = \varphi(y)
 \delta_x^{\text{measure}} A 
\delta_x^{\text{measure}}(A) = \begin{cases}
    1 & x\in A \\
    0 & x\notin A
\end{cases}
 g:\mathbb{R}^n\to\mathbb{R}^m n > m 
\int_{\mathbb{R}^n} f(x) dx = \int_{\mathbb{R}^m} \left[\int_{g^{-1}(y)} f(x) |J_g(x) J_g(x)^\top|^{-1/2} \mathcal{H}^{n-m}(dx) \right]dy 
 J_g(x) g","['functional-analysis', 'measure-theory', 'distribution-theory', 'geometric-measure-theory']"
74,"Proof of the Meyers–Serrin theorem (the ""$H=W$"" theorem)","Proof of the Meyers–Serrin theorem (the """" theorem)",H=W,"I'm working on the proof of the "" $H=W$ "" theorem in the book SOBOLEV SPACES by Adams and Fournier, and there is an argument that seems quite strange to me. Let me introduce some conventions first. $H^{m,p}(\Omega)$ is defined as the completion of $$\{u\in C^m(\Omega):\lVert u\rVert_{m,p}<\infty\}$$ w.r.t. the norm $$\lVert u\rVert_{m,p}=\left(\sum_{|\alpha|\leq m}\lVert D^\alpha u\rVert_p^p\right)^\frac{1}{p},$$ where $D^\alpha$ is understood in the weak sense, while $W^{m,p}(\Omega)$ is defined by weak derivatives in the usual way. As to completions, the authors mentioned in the preliminary chapter that every normed space $X$ is either a Banach space or a dense subset of a Banach space $Y$ called the completion of $X$ whose norm satisfies $$\lVert x\rVert_Y=\lVert x\rVert_X$$ for every $x\in X$ . Finally, throughout our discussion, $\Omega$ denotes a nonempty open subset of the Euclidean space. Now let us see the proof to be understood. The argument underlined with red is much annoying. I don't see any relevance between completeness of $W^{m,p}(\Omega)$ and the extension to the asserted isometric isomorphism. If the identity operator on $S$ means the inclusion of $S$ in $W^{m,p}(\Omega)$ , how could I extend it to the isometric isomorphism? Before digging into the proof, I have reviewed the preliminary chapter, so I know the normed space $S$ is isometrically isomorphic to a dense subspace of a Banach space $H^{m,p}(\Omega)$ ? But this doesn't seem to be any helpful. How could I use this fact to build the extension. Help is much needed. Thank you for your precious time. Update 1: I remember that if the codomain $T$ of a uniformly continuous map $f:A\subseteq M\to T$ is complete, then we can uniquely extend $f$ to the closure $\bar{A}$ , preserving uniform continuity. But this seems to give nothing if I extend the inclusion of $S$ in $W$ . Update 2: Though it may be obvious to people who know about completions of normed spaces, I'd like to emphasize for once that every normed space $X$ is in fact isometrically isomorphic to a dense subspace of a Banach space and it is this Banach space that is defined as the completion of $X$ . Then, by identifying isomorphic spaces, the authors conclude that every normed space is a dense subset of its completion, I guess.","I'm working on the proof of the "" "" theorem in the book SOBOLEV SPACES by Adams and Fournier, and there is an argument that seems quite strange to me. Let me introduce some conventions first. is defined as the completion of w.r.t. the norm where is understood in the weak sense, while is defined by weak derivatives in the usual way. As to completions, the authors mentioned in the preliminary chapter that every normed space is either a Banach space or a dense subset of a Banach space called the completion of whose norm satisfies for every . Finally, throughout our discussion, denotes a nonempty open subset of the Euclidean space. Now let us see the proof to be understood. The argument underlined with red is much annoying. I don't see any relevance between completeness of and the extension to the asserted isometric isomorphism. If the identity operator on means the inclusion of in , how could I extend it to the isometric isomorphism? Before digging into the proof, I have reviewed the preliminary chapter, so I know the normed space is isometrically isomorphic to a dense subspace of a Banach space ? But this doesn't seem to be any helpful. How could I use this fact to build the extension. Help is much needed. Thank you for your precious time. Update 1: I remember that if the codomain of a uniformly continuous map is complete, then we can uniquely extend to the closure , preserving uniform continuity. But this seems to give nothing if I extend the inclusion of in . Update 2: Though it may be obvious to people who know about completions of normed spaces, I'd like to emphasize for once that every normed space is in fact isometrically isomorphic to a dense subspace of a Banach space and it is this Banach space that is defined as the completion of . Then, by identifying isomorphic spaces, the authors conclude that every normed space is a dense subset of its completion, I guess.","H=W H^{m,p}(\Omega) \{u\in C^m(\Omega):\lVert u\rVert_{m,p}<\infty\} \lVert u\rVert_{m,p}=\left(\sum_{|\alpha|\leq m}\lVert D^\alpha u\rVert_p^p\right)^\frac{1}{p}, D^\alpha W^{m,p}(\Omega) X Y X \lVert x\rVert_Y=\lVert x\rVert_X x\in X \Omega W^{m,p}(\Omega) S S W^{m,p}(\Omega) S H^{m,p}(\Omega) T f:A\subseteq M\to T f \bar{A} S W X X","['real-analysis', 'functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
75,Can Sum Of Exponentially Increasing sinusoids be square integrable?,Can Sum Of Exponentially Increasing sinusoids be square integrable?,,"Apologies if this is a trivial question.... im still learning Suppose we have an exponentially increasing sinusoidal signal $$f(x)=e^{\alpha x}\cos(\beta x)+e^{\alpha x}\sin(\beta x)$$ with $0<\alpha<1$ and $\beta\in\mathbb{R}$ Is it ever possible to dampen the signal with countably (perhaps infinitely) many other potentially exponentially increasing sinusoids such that the sum is square integrable? $$\int_\mathbb{R} [f(x)+\sum[\omega_i e^{\alpha_i x}\cos(\beta_i x)+\omega_i e^{\alpha_i x}\sin(\beta_i x)]]^2<\infty$$ where, for the elements in the summation, $\omega_i$ is the amplitude of the i'th element, $0\leq\alpha_i<1$ and $\beta_i\neq \beta$ . my instinct says it is not possible, but im not sure. edit: to make the question a bit more general, i'll add the possibility of both sin and cos.","Apologies if this is a trivial question.... im still learning Suppose we have an exponentially increasing sinusoidal signal with and Is it ever possible to dampen the signal with countably (perhaps infinitely) many other potentially exponentially increasing sinusoids such that the sum is square integrable? where, for the elements in the summation, is the amplitude of the i'th element, and . my instinct says it is not possible, but im not sure. edit: to make the question a bit more general, i'll add the possibility of both sin and cos.",f(x)=e^{\alpha x}\cos(\beta x)+e^{\alpha x}\sin(\beta x) 0<\alpha<1 \beta\in\mathbb{R} \int_\mathbb{R} [f(x)+\sum[\omega_i e^{\alpha_i x}\cos(\beta_i x)+\omega_i e^{\alpha_i x}\sin(\beta_i x)]]^2<\infty \omega_i 0\leq\alpha_i<1 \beta_i\neq \beta,"['real-analysis', 'calculus', 'functional-analysis', 'trigonometry', 'exponential-function']"
76,Let $f_n$ be a$f_{n+1}\left(x\right)=\frac{1}{x}\int _0^xf_n\left(t\right)dt\:$,Let  be a,f_n f_{n+1}\left(x\right)=\frac{1}{x}\int _0^xf_n\left(t\right)dt\:,"Let be $f_n $ be a sequence of functions and $f_0$ an continuous arbitrary function derivable in $0$ such that: $$f_{n+1}\left(x\right)=\frac{1}{x}\int _0^xf_n\left(t\right)dt$$ for every $n$ positive integer. The domain of $f_n$ is $[0,1]$ I was wondering if the following statement is true or not: $f'_n $ is uniformly convergent to the function: $g(x)=0$ for every $x \in [0,1]$ I was thinking that I need to prove that: $$sup_{x\in [0,1]}|f'_n(x)-f(x)|=0$$ that is to say for every $\epsilon>0$ there is a positive integer $N$ such that for every $n>N$ and ${x\in [0,1]}$ we have: $$|f'_n(x)-f(x)|<\epsilon$$ How should I proceed?",Let be be a sequence of functions and an continuous arbitrary function derivable in such that: for every positive integer. The domain of is I was wondering if the following statement is true or not: is uniformly convergent to the function: for every I was thinking that I need to prove that: that is to say for every there is a positive integer such that for every and we have: How should I proceed?,"f_n  f_0 0 f_{n+1}\left(x\right)=\frac{1}{x}\int _0^xf_n\left(t\right)dt n f_n [0,1] f'_n  g(x)=0 x \in [0,1] sup_{x\in [0,1]}|f'_n(x)-f(x)|=0 \epsilon>0 N n>N {x\in [0,1]} |f'_n(x)-f(x)|<\epsilon",['functional-analysis']
77,Continuity of orthogonal projection in Hilbert space with respect to different inner products,Continuity of orthogonal projection in Hilbert space with respect to different inner products,,"Suppose we are given a Hilbert space A, an infinite dimensional closed subspace B and an inner product $<\cdot,\cdot>$ . A sequence of inner products $<\cdot,\cdot>_i$ converge to $<\cdot,\cdot>$ in the sense that $C^{-1}||v||\le||v||_i\le C ||v||,\forall i,v$ and for any $v,w\in A$ , $<v,w>_i\rightarrow <v,w>$ as $i\rightarrow \infty$ . Can we prove that for any $v\in A$ , the orthogonal projection to B with respect to $<\cdot,\cdot>_i$ , $P_i(v)$ converges to $P(v)$ (defined similarly)? If not, what additional conditions are required?","Suppose we are given a Hilbert space A, an infinite dimensional closed subspace B and an inner product . A sequence of inner products converge to in the sense that and for any , as . Can we prove that for any , the orthogonal projection to B with respect to , converges to (defined similarly)? If not, what additional conditions are required?","<\cdot,\cdot> <\cdot,\cdot>_i <\cdot,\cdot> C^{-1}||v||\le||v||_i\le C ||v||,\forall i,v v,w\in A <v,w>_i\rightarrow <v,w> i\rightarrow \infty v\in A <\cdot,\cdot>_i P_i(v) P(v)",['functional-analysis']
78,Find $p$ such that the integral is finite,Find  such that the integral is finite,p,"Let $X \subset \mathbb{R}^{n}$ . For which $p \in[1, \infty)$ it holds that $f \in L^{p}(X)$ when $f(x)=|x|^{-1}$ and $X=B(0,1)$ $X=\mathbb{R}^{n} \backslash B(0,1)$ $X=\mathbb{R}^{n}$ . My attempt : Suppose $0\leq a<b\leq \infty$ and we consider the annulus $E_{a,b}:=\{x\in\Bbb{R}^n\,:\, a<|x|<b\}$ . Then, for any $p\in\Bbb{R}, $ we have $\int_{E_{a,b}}\frac{1}{|x|^{p}}\,dx=\int_a^b\frac{1}{r^{p}}A_{n-1}r^{n-1}\,dr=A_{n-1}\int_a^b\frac{1}{r^{p+1-n}}\,dr$ , where $A_{n-1}$ is the surface area of the unit sphere $S^{n-1}\subset\Bbb{R}^n$ $A_{n-1} \frac{-(p+1-n)}{r^{p+2-n}}\bigr\vert_{a}^{b}$ In the first case, $a = 0$ and $b = 1$ , in the second case $a = 1$ and $b = \infty$ , and in the third case $a = 0$ and $b = \infty$ . In the first case the integral is finite when $p +2 ≤ n$ , in the second case the integral is finite when $p + 2 ≥ n$ , and in the third case $p + 2 = n$ . Is my attempt correct?","Let . For which it holds that when and . My attempt : Suppose and we consider the annulus . Then, for any we have , where is the surface area of the unit sphere In the first case, and , in the second case and , and in the third case and . In the first case the integral is finite when , in the second case the integral is finite when , and in the third case . Is my attempt correct?","X \subset \mathbb{R}^{n} p \in[1, \infty) f \in L^{p}(X) f(x)=|x|^{-1} X=B(0,1) X=\mathbb{R}^{n} \backslash B(0,1) X=\mathbb{R}^{n} 0\leq a<b\leq \infty E_{a,b}:=\{x\in\Bbb{R}^n\,:\, a<|x|<b\} p\in\Bbb{R},  \int_{E_{a,b}}\frac{1}{|x|^{p}}\,dx=\int_a^b\frac{1}{r^{p}}A_{n-1}r^{n-1}\,dr=A_{n-1}\int_a^b\frac{1}{r^{p+1-n}}\,dr A_{n-1} S^{n-1}\subset\Bbb{R}^n A_{n-1} \frac{-(p+1-n)}{r^{p+2-n}}\bigr\vert_{a}^{b} a = 0 b = 1 a = 1 b = \infty a = 0 b = \infty p +2 ≤ n p + 2 ≥ n p + 2 = n","['functional-analysis', 'lp-spaces', 'sobolev-spaces']"
79,Is the function $f : \mathbb C^{*} \rightarrow \mathbb C^{*} $ given by $f(z)=ze^{z}$ a closed map?,Is the function  given by  a closed map?,f : \mathbb C^{*} \rightarrow \mathbb C^{*}  f(z)=ze^{z},"I came across a problem where I was asked to find that, the same map is closed or not when seen as from $\mathbb C^{*}$ to $\mathbb C$ . So i solved it by showing that image of real line is not closed in the later. After then I was thinking about the closeness of the same function with co-domain non-zero complex number, but I wasn't able to do it as in the last case zero was playing a crucial role. Any hint please?","I came across a problem where I was asked to find that, the same map is closed or not when seen as from to . So i solved it by showing that image of real line is not closed in the later. After then I was thinking about the closeness of the same function with co-domain non-zero complex number, but I wasn't able to do it as in the last case zero was playing a crucial role. Any hint please?",\mathbb C^{*} \mathbb C,"['real-analysis', 'complex-analysis', 'functional-analysis', 'analysis']"
80,Isomorphic Hilbert spaces and mutual singularity,Isomorphic Hilbert spaces and mutual singularity,,"Let $(X, \Omega)$ a measurable space, $\mu_1, \mu_2$ a $\sigma-$ finite measures on $X$ , and $\mu = \mu_1 + \mu_2$ . Define $$V : L^2(\mu) \longrightarrow L^2(\mu_1) \oplus L^2(\mu_2) $$ defined by $Vf = (f, f)$ . We easily see that it is well-defined. Furthermore, a moment's consideration shows that $V$ is linear, injective mapping. What I want to show is if $V$ is isomorphism of Hilbert spaces, then $\mu_1$ and $\mu_2$ are mutually singular. What I tried is that if $\mu_1$ is concentrated on $A \subseteq X$ and $X = A \cup B$ where $A, B$ disjoint, then by the surjectivity, there exist $f \in L^2(\mu)$ such that $f = 1_B$ , with $\mu_1 - a.e$ , and $f = 1_A$ , with $\mu_2 - a.e$ . And Im stucked. Can anyone help this problem? It is from Conway's functional analysis, page 25 #2. Thanks.","Let a measurable space, a finite measures on , and . Define defined by . We easily see that it is well-defined. Furthermore, a moment's consideration shows that is linear, injective mapping. What I want to show is if is isomorphism of Hilbert spaces, then and are mutually singular. What I tried is that if is concentrated on and where disjoint, then by the surjectivity, there exist such that , with , and , with . And Im stucked. Can anyone help this problem? It is from Conway's functional analysis, page 25 #2. Thanks.","(X, \Omega) \mu_1, \mu_2 \sigma- X \mu = \mu_1 + \mu_2 V : L^2(\mu) \longrightarrow L^2(\mu_1) \oplus L^2(\mu_2)  Vf = (f, f) V V \mu_1 \mu_2 \mu_1 A \subseteq X X = A \cup B A, B f \in L^2(\mu) f = 1_B \mu_1 - a.e f = 1_A \mu_2 - a.e","['real-analysis', 'functional-analysis', 'measure-theory']"
81,How does differential equations in the space of operator valued functions work? [closed],How does differential equations in the space of operator valued functions work? [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 2 years ago . Improve this question In my Quantum Mechanics class, we talked about Schrödinger Equation for the time evolution operator $\hat U(t)$ , $$i\hbar\frac{d\, \hat U(t)}{dt} = \hat H(t)\hat{U}(t),\qquad \hat U(0)=\mathrm{id},$$ with $\hat H(t)$ being some self adjoint operator on a Hilbert space $\mathcal H$ , with the solution $\hat U(t)$ being some unitary operators. And if $\hat H$ is independent of time, then we are given the solution $$\hat U(t) = e^{-i\hat H t/\hbar}.$$ We were told to just plug in and check that this is a solution, and were given no further explanation. I'm just basically not sure how most of this works. More specifically, I have the following questions. How is the derivative defined? My guess is that, we are using the operator norm for the operator, and define the derivative the normal way? If there's an integral, how is it defined? I've only seen lebesgue integral for real and complex valued function, and I couldn't think of a way it could work for operators. How do we know that the solution $\hat U (t)$ will remain unitary, provided the only thing we know is $\hat H(t)$ is self adjoint and $\hat U(0) = \mathrm{id}$ . If $\hat H(t_1)$ commutes with $\hat H(t_2)$ , is it true that the solution is $\hat U(t) = e^{-i\int \hat H(t) \,dt /\hbar}$ ? My intuition says this should be true, as $\hat U(t)$ should always commute with $\hat H(t)$ in this case. How much does the theory of normal differential equation carries over? Do we know that the solution exist? Is it unique? etc. The most general differential equation I've seen is for smooth functions on manifolds, and the one here is obviously very different (the functions aren't even commutative). What are some good references for these (if there are any)? Note: I'm an undergraduate student double majoring in math and physics, and I have taken several graduate courses. So don't expect me to know any of the advanced results, but I'd love to learn about them :)","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 2 years ago . Improve this question In my Quantum Mechanics class, we talked about Schrödinger Equation for the time evolution operator , with being some self adjoint operator on a Hilbert space , with the solution being some unitary operators. And if is independent of time, then we are given the solution We were told to just plug in and check that this is a solution, and were given no further explanation. I'm just basically not sure how most of this works. More specifically, I have the following questions. How is the derivative defined? My guess is that, we are using the operator norm for the operator, and define the derivative the normal way? If there's an integral, how is it defined? I've only seen lebesgue integral for real and complex valued function, and I couldn't think of a way it could work for operators. How do we know that the solution will remain unitary, provided the only thing we know is is self adjoint and . If commutes with , is it true that the solution is ? My intuition says this should be true, as should always commute with in this case. How much does the theory of normal differential equation carries over? Do we know that the solution exist? Is it unique? etc. The most general differential equation I've seen is for smooth functions on manifolds, and the one here is obviously very different (the functions aren't even commutative). What are some good references for these (if there are any)? Note: I'm an undergraduate student double majoring in math and physics, and I have taken several graduate courses. So don't expect me to know any of the advanced results, but I'd love to learn about them :)","\hat U(t) i\hbar\frac{d\, \hat U(t)}{dt} = \hat H(t)\hat{U}(t),\qquad \hat U(0)=\mathrm{id}, \hat H(t) \mathcal H \hat U(t) \hat H \hat U(t) = e^{-i\hat H t/\hbar}. \hat U (t) \hat H(t) \hat U(0) = \mathrm{id} \hat H(t_1) \hat H(t_2) \hat U(t) = e^{-i\int \hat H(t) \,dt /\hbar} \hat U(t) \hat H(t)","['functional-analysis', 'ordinary-differential-equations', 'physics', 'mathematical-physics', 'quantum-mechanics']"
82,"Coercivity of $B[u,v] = \int_\Omega a^{ij}\partial u_{x_i} \partial u_{x_j} + \int_\Omega c(x)uv $, with no sign assumption on $c(x)$","Coercivity of , with no sign assumption on","B[u,v] = \int_\Omega a^{ij}\partial u_{x_i} \partial u_{x_j} + \int_\Omega c(x)uv  c(x)","I am reviewing previous qualifying exams on PDE's and here is a problem that I am not confident in my solution. Assume that $\Omega \subset \mathbb{R}^n$ is open, bounded, connected, and $\partial\Omega \in C^1$ . Assume that $a^{ij} \in L^\infty(\Omega)$ is a uniformly elliptic matrix in the sense that there exists two constants $\lambda>0$ and $\Lambda>0$ so that $$ \forall \xi \in \mathbb{R}^n, \,\,\, \lambda|\xi|^2 \leq a^{ij}(x)\xi_i\xi_j \leq \Lambda|\xi|^2 $$ and that $c\in L^\infty(\Omega)$ is given. The bilinear form associated to $a^{ij}$ and $c$ is: $$ B[u,v] = \int_\Omega a^{ij}\frac{\partial u}{\partial x_i}\frac{\partial u}{\partial x_j} \,dx + \int_\Omega c(x)uv \,dx $$ Assume no assumption on the sign of $c(x)$ . Prove there exists a choice of $\lambda$ depending on $\Omega, n$ , and $\|c\|_{L^\infty}$ , so the that the bilinear form, $B$ is coercive on $H^1_0(\Omega)$ . Here is my thinking and work so far. Using Poincare inequality, we find that $$  \|u\|^2_{H^1_0(\Omega)} = \int_\Omega u^2 \,dx+\int_\Omega|\nabla u|^2\,dx \leq (C_p+1)\int_\Omega |\nabla u|^2\,dx. $$ Manipulating the bilinear form, $$ B[u,u] - \int_\Omega c(x) u^2\,dx = \int_\Omega a^{ij}\frac{\partial u}{\partial x_i}\frac{\partial u}{\partial x_j} \,dx $$ and using the estimate $$ \int_\Omega c(x) u^2 \,dx \leq \|c\|_{L^\infty}\int_\Omega u^2 \, dx $$ we get $$ B[u,u] +\|c\|_{L^\infty}\int_\Omega u^2 \, dx \geq B[u,u] -\int_\Omega c(x) u^2 \,dx = \int_\Omega a^{ij}\frac{\partial u}{\partial x_i}\frac{\partial u}{\partial x_j} \,dx \geq \lambda\int_\Omega |\nabla u|^2\,dx. $$ Combining this with the Poincare inequality $$ B[u,u] + \|c\|_{L^\infty} \int_\Omega u^2 \, dx \geq \frac{\lambda}{C_p +1}\left( \int_\Omega u^2 \,dx+\int_\Omega|\nabla u|^2\,dx \right). $$ then $$ B[u,u] \geq \frac{\lambda}{C_p +1}\int_\Omega|\nabla u|^2\,dx + \left( \frac{\lambda}{C_p +1}-\|c\|_{L^\infty} \right) \int_\Omega u^2 \, dx.  $$ From here we could could find a bound on $\lambda$ so the coefficient on the $\int u^2 $ term is some fraction. I personally have difficulty with these types of inequalities. Is my process and logic sound?","I am reviewing previous qualifying exams on PDE's and here is a problem that I am not confident in my solution. Assume that is open, bounded, connected, and . Assume that is a uniformly elliptic matrix in the sense that there exists two constants and so that and that is given. The bilinear form associated to and is: Assume no assumption on the sign of . Prove there exists a choice of depending on , and , so the that the bilinear form, is coercive on . Here is my thinking and work so far. Using Poincare inequality, we find that Manipulating the bilinear form, and using the estimate we get Combining this with the Poincare inequality then From here we could could find a bound on so the coefficient on the term is some fraction. I personally have difficulty with these types of inequalities. Is my process and logic sound?","\Omega \subset \mathbb{R}^n \partial\Omega \in C^1 a^{ij} \in L^\infty(\Omega) \lambda>0 \Lambda>0 
\forall \xi \in \mathbb{R}^n, \,\,\, \lambda|\xi|^2 \leq a^{ij}(x)\xi_i\xi_j \leq \Lambda|\xi|^2
 c\in L^\infty(\Omega) a^{ij} c 
B[u,v] = \int_\Omega a^{ij}\frac{\partial u}{\partial x_i}\frac{\partial u}{\partial x_j} \,dx + \int_\Omega c(x)uv \,dx
 c(x) \lambda \Omega, n \|c\|_{L^\infty} B H^1_0(\Omega)  
\|u\|^2_{H^1_0(\Omega)} = \int_\Omega u^2 \,dx+\int_\Omega|\nabla u|^2\,dx \leq (C_p+1)\int_\Omega |\nabla u|^2\,dx.
 
B[u,u] - \int_\Omega c(x) u^2\,dx = \int_\Omega a^{ij}\frac{\partial u}{\partial x_i}\frac{\partial u}{\partial x_j} \,dx
 
\int_\Omega c(x) u^2 \,dx \leq \|c\|_{L^\infty}\int_\Omega u^2 \, dx
 
B[u,u] +\|c\|_{L^\infty}\int_\Omega u^2 \, dx \geq B[u,u] -\int_\Omega c(x) u^2 \,dx = \int_\Omega a^{ij}\frac{\partial u}{\partial x_i}\frac{\partial u}{\partial x_j} \,dx \geq \lambda\int_\Omega |\nabla u|^2\,dx.
 
B[u,u] + \|c\|_{L^\infty} \int_\Omega u^2 \, dx \geq \frac{\lambda}{C_p +1}\left( \int_\Omega u^2 \,dx+\int_\Omega|\nabla u|^2\,dx \right).
 
B[u,u] \geq \frac{\lambda}{C_p +1}\int_\Omega|\nabla u|^2\,dx +
\left( \frac{\lambda}{C_p +1}-\|c\|_{L^\infty} \right) \int_\Omega u^2 \, dx. 
 \lambda \int u^2 ","['functional-analysis', 'partial-differential-equations', 'solution-verification', 'elliptic-equations']"
83,Lax-Milgram and the existence of solution to parabolic equation,Lax-Milgram and the existence of solution to parabolic equation,,"I think it is standard and common to use Lax-Milgram theorem to prove the existence of solution to elliptic equation. However, can we use it to establish the existence of parabolic equation? I do not find some examples in standard PDE textbooks. Suppose I have a parabolic equation $$ \partial_t u - \partial_{x_j}(a_{ij} \partial_{x_i} u) + b_i \partial_{x_i} u + c u =f(x,t)$$ on $\Omega \times [0,T]$ . Then the weak formulation should be $$ \int_{\Omega} \partial_{t} u \varphi + a_{ij} \partial_{x_i} u \partial_{x_j} \varphi + b_i \partial_{x_i} u \varphi + c u \varphi-f\varphi=0,$$ for all $\varphi(x) \in H^1_0(\Omega)$ and a.e. $t\in[0,T]$ . But I do not know how can we define the bilinear mapping in this way. May I get some help?","I think it is standard and common to use Lax-Milgram theorem to prove the existence of solution to elliptic equation. However, can we use it to establish the existence of parabolic equation? I do not find some examples in standard PDE textbooks. Suppose I have a parabolic equation on . Then the weak formulation should be for all and a.e. . But I do not know how can we define the bilinear mapping in this way. May I get some help?"," \partial_t u - \partial_{x_j}(a_{ij} \partial_{x_i} u) + b_i \partial_{x_i} u + c u =f(x,t) \Omega \times [0,T]  \int_{\Omega} \partial_{t} u \varphi + a_{ij} \partial_{x_i} u \partial_{x_j} \varphi + b_i \partial_{x_i} u \varphi + c u \varphi-f\varphi=0, \varphi(x) \in H^1_0(\Omega) t\in[0,T]","['functional-analysis', 'partial-differential-equations', 'parabolic-pde']"
84,Ways to introduce B-splines,Ways to introduce B-splines,,"I asked this on overflow, but it hasn't gotten many responses so I'll try here as well. I have the option of mentoring some undergrads in a topic lying within approximation theory and I really want to do $B$ -splines. Mostly because I have recently found applications of them in my own research and I think it's a good opportunity for me to further learn the material. (And to show them cool stuff as well of course.) Suppose we are given a sequence of knots t $= (t_i)_{i \in \mathbb{Z}} \subset \mathbb{R}$ . I am aware of two ways in which $B$ -splines can be defined. Method 1: First define the $B$ -splines of order $1$ (or degree $0$ ) to be the characteristic functions $B_{i1} = \chi_{[t_i,t_{i+1})}$ . Then we define the $B$ -splines of higher order by the recurrence relation $$B_{ik} =  \lambda_{ik}B_{i,k-1} + (1 - \lambda_{i+1,k})B_{i+1,k-1}$$ where \begin{equation*} \lambda_{ik}(t) =  \left\{         \begin{array}{ll}             \frac{t - t_i}{t_{i+k-1} - t_i} &  \quad  \text{if} \ \ \ t_i \neq t_{i+k-1} \\             0 & \quad \text{otherwise}         \end{array}     \right. \end{equation*} I understand that this is a computationally practical way of defining $B$ -splines and that many of the early theorems about $B$ -splines have simple proof when given this definition. However, I am of the opinion that you wouldn't introduce $B$ -splines this way unless you really want to bore your audience as this recurrence relation is highly unmotivated and, until you begin to actually prove theorems with it, it simply doesn't look interesting. The next way is longer but the idea is more natural (at first at least). I don't want to make this post too long so I will skip details. I include some details for completion, but I suspect someone with an answer to this post is likely familiar with everything I mention below. Method 2: Suppose we are investigating the problem of finding a basis for the space of piece-wise polynomials of order $k$ (or degree $k-1$ ) with breakpoints at $(t_i)$ with some specified smoothness condition at each $t_i$ . To find this basis we first make the problem easier by finding a basis for the space of piece-wise polynomials on $\mathbb{R}$ with a finite set of breakpoints and some specified smoothness condition at these breakpoints. If this finite knot sequence is $\{x_1, \dots x_n\} $ , we get led to the truncated power basis $\{(t - x_i)_+^{j} | 1 \leq i \leq n , 0 \leq j \leq k-1\}$ . We then find some linear combination of these truncated powers to begin constructing compactly supported piece-wise polynomials supported on closed intervals with end points belonging to our knot sequence t . (I have skipped many details here). But in order to actually define the $B$ -splines, we need to find out the coefficients of the truncated powers that yield them. This takes us to the divided difference operators and I am not a fan of these operators either. I also find considering them to be somewhat unmotivated (albeit not as unmotivated as the first idea). My question: Are there other ways to introduce $B$ -splines aside from the $2$ methods I have given? I suspect the solution to my dilemna is to understand these divided difference operators more in depth, but I want to know if there are other ways. I've been reading through a book on Box Splines which are defined as distributions. It seems interesting, but I've only begun and don't yet fully see how they generalize $B$ -splines. Even if this approach would work as well, I am unsure whether it would be accessible to undergrads.","I asked this on overflow, but it hasn't gotten many responses so I'll try here as well. I have the option of mentoring some undergrads in a topic lying within approximation theory and I really want to do -splines. Mostly because I have recently found applications of them in my own research and I think it's a good opportunity for me to further learn the material. (And to show them cool stuff as well of course.) Suppose we are given a sequence of knots t . I am aware of two ways in which -splines can be defined. Method 1: First define the -splines of order (or degree ) to be the characteristic functions . Then we define the -splines of higher order by the recurrence relation where I understand that this is a computationally practical way of defining -splines and that many of the early theorems about -splines have simple proof when given this definition. However, I am of the opinion that you wouldn't introduce -splines this way unless you really want to bore your audience as this recurrence relation is highly unmotivated and, until you begin to actually prove theorems with it, it simply doesn't look interesting. The next way is longer but the idea is more natural (at first at least). I don't want to make this post too long so I will skip details. I include some details for completion, but I suspect someone with an answer to this post is likely familiar with everything I mention below. Method 2: Suppose we are investigating the problem of finding a basis for the space of piece-wise polynomials of order (or degree ) with breakpoints at with some specified smoothness condition at each . To find this basis we first make the problem easier by finding a basis for the space of piece-wise polynomials on with a finite set of breakpoints and some specified smoothness condition at these breakpoints. If this finite knot sequence is , we get led to the truncated power basis . We then find some linear combination of these truncated powers to begin constructing compactly supported piece-wise polynomials supported on closed intervals with end points belonging to our knot sequence t . (I have skipped many details here). But in order to actually define the -splines, we need to find out the coefficients of the truncated powers that yield them. This takes us to the divided difference operators and I am not a fan of these operators either. I also find considering them to be somewhat unmotivated (albeit not as unmotivated as the first idea). My question: Are there other ways to introduce -splines aside from the methods I have given? I suspect the solution to my dilemna is to understand these divided difference operators more in depth, but I want to know if there are other ways. I've been reading through a book on Box Splines which are defined as distributions. It seems interesting, but I've only begun and don't yet fully see how they generalize -splines. Even if this approach would work as well, I am unsure whether it would be accessible to undergrads.","B = (t_i)_{i \in \mathbb{Z}} \subset \mathbb{R} B B 1 0 B_{i1} = \chi_{[t_i,t_{i+1})} B B_{ik} =  \lambda_{ik}B_{i,k-1} + (1 - \lambda_{i+1,k})B_{i+1,k-1} \begin{equation*}
\lambda_{ik}(t) =  \left\{
        \begin{array}{ll}
            \frac{t - t_i}{t_{i+k-1} - t_i} &  \quad  \text{if} \ \ \ t_i \neq t_{i+k-1} \\
            0 & \quad \text{otherwise}
        \end{array}
    \right.
\end{equation*} B B B k k-1 (t_i) t_i \mathbb{R} \{x_1, \dots x_n\}  \{(t - x_i)_+^{j} | 1 \leq i \leq n , 0 \leq j \leq k-1\} B B 2 B","['functional-analysis', 'polynomials', 'fourier-transform', 'approximation-theory', 'spline']"
85,If $\rho(z)<1$ then exists some $N$ such that $\lVert R(z)^N \rVert <3/4$ for all $z\in U$,If  then exists some  such that  for all,\rho(z)<1 N \lVert R(z)^N \rVert <3/4 z\in U,"Let $\rho(z)$ denote the spectral radius of $R(z)$ , where $R:U \to \text{Hom}(\mathcal{L},\mathcal{L})$ and $U$ is a closed subset of $\mathbb{C}$ , here $\mathcal{L}$ can be understood as a Banach subspace of functions. Let $\lVert\,\cdot\,\rVert $ denote the operator norm in $\text{Hom}(\mathcal{L},\mathcal{L})$ then $R(z)=\sum_{n\ge 1}z^nR_n$ where $R_n\in \text{Hom}(\mathcal{L},\mathcal{L})$ and, $$\rho(z)=\lim_{n\to\infty} \lVert R(z)^n \rVert^{1/n}$$ And by hypothesis the spectral radius is less than $1$ , and as $U$ is compact the maximum will be reached. $$\max_{z\in U} \rho(z)<1 \tag{1}\label{eq}$$ The step I didn't understand is this, For every $z\in U$ there exists $n=n(z)$ such that $\lVert R(z)^n \rVert<1/2$ . Since $z \mapsto \lVert R(z) \rVert$ is continuous in $U$ , there is some $r=r(z)$ such that $\lVert R(w)^n \rVert<3/4$ for all $w\in B_r(z)$ . Sinze $U$ is compact, there exists some $N$ such that for all $z\in U,\, \lVert R(z)^N \rVert <3/4$ . So, my doubt is in this choice of $N$ . I've detailed a little more what was done. I believe this is the way, but I'm not sure as I couldn't finish the proof. This is my attempt to understand what was done: Since $\max_{z\in U} \rho(z)<1$ , we have that $\lVert R(z)^n \rVert\to 0$ , therefore for every $z\in U$ exists $n_z$ (that depends on $z$ ), $\lVert R(z)^n \rVert<1/2$ for all $n\ge n_z$ . I understood that now fixing the $z$ and $n_z$ , there is $r$ such that $\lVert R(w)^{n_z} \rVert<3/4$ for all $w\in B_r(z)$ . As $U$ is compact, there is a finite subcover of these balls, ie, $$U\subseteq \bigcup_{i\in\Lambda} B_{r_i}(z_i)\quad \text{where for all } w\in B_{r_i}(z_i),\, \lVert R(w)^{n_i} \rVert<3/4$$ and $\Lambda=\{1,2,\cdots,k\}$ . First I had thought of taking $N$ as the maximum of these $n_i$ , but I think taking the sum would be better. Let $N=n_1+\cdots + n_k$ and $w\in U$ , so $$\lVert R(w)^{N} \rVert \le \lVert R(w)^{n_1} \rVert\cdot \lVert R(w)^{n_2} \rVert\cdots \lVert R(w)^{n_k} \rVert $$ Without loss of generality we can assume that $w\in B_{r_1}(z_1)$ , so $$\lVert R(w)^{N} \rVert \le \lVert R(w)^{n_1} \rVert\cdot \lVert R(w)^{n_2} \rVert\cdots \lVert R(w)^{n_k} \rVert \le \frac{3}{4} \cdot \lVert R(w)^{n_2} \rVert\cdots \lVert R(w)^{n_k} \rVert$$ So what I'm having trouble with is how to ensure that the remainder is at least less than 1, to get the 3/4 of the proof. Because note that $n_i$ depends on $z$ and it might happen that $n(w)$ is greater than all $n_i$ so I can't guarantee that $\lVert R(w)^{n_i} \rVert<1/2$ . I don't know if this is the best choice for $N$ , but I couldn't think of another one. Does anyone suggest something? Another thing I thought was to see if it was possible to change the order of maximum and limit in \eqref{eq}, but for that I would need the sequence to be decreasing and I don't think I can guarantee that. Edit: It seems to me that this is a standard result, maybe someone would also have a reference where to find it? Reference: If reference is needed, this is the ""preliminaries"" of Lemma 4 of this article .","Let denote the spectral radius of , where and is a closed subset of , here can be understood as a Banach subspace of functions. Let denote the operator norm in then where and, And by hypothesis the spectral radius is less than , and as is compact the maximum will be reached. The step I didn't understand is this, For every there exists such that . Since is continuous in , there is some such that for all . Sinze is compact, there exists some such that for all . So, my doubt is in this choice of . I've detailed a little more what was done. I believe this is the way, but I'm not sure as I couldn't finish the proof. This is my attempt to understand what was done: Since , we have that , therefore for every exists (that depends on ), for all . I understood that now fixing the and , there is such that for all . As is compact, there is a finite subcover of these balls, ie, and . First I had thought of taking as the maximum of these , but I think taking the sum would be better. Let and , so Without loss of generality we can assume that , so So what I'm having trouble with is how to ensure that the remainder is at least less than 1, to get the 3/4 of the proof. Because note that depends on and it might happen that is greater than all so I can't guarantee that . I don't know if this is the best choice for , but I couldn't think of another one. Does anyone suggest something? Another thing I thought was to see if it was possible to change the order of maximum and limit in \eqref{eq}, but for that I would need the sequence to be decreasing and I don't think I can guarantee that. Edit: It seems to me that this is a standard result, maybe someone would also have a reference where to find it? Reference: If reference is needed, this is the ""preliminaries"" of Lemma 4 of this article .","\rho(z) R(z) R:U \to \text{Hom}(\mathcal{L},\mathcal{L}) U \mathbb{C} \mathcal{L} \lVert\,\cdot\,\rVert  \text{Hom}(\mathcal{L},\mathcal{L}) R(z)=\sum_{n\ge 1}z^nR_n R_n\in \text{Hom}(\mathcal{L},\mathcal{L}) \rho(z)=\lim_{n\to\infty} \lVert R(z)^n \rVert^{1/n} 1 U \max_{z\in U} \rho(z)<1 \tag{1}\label{eq} z\in U n=n(z) \lVert R(z)^n \rVert<1/2 z \mapsto \lVert R(z) \rVert U r=r(z) \lVert R(w)^n \rVert<3/4 w\in B_r(z) U N z\in U,\, \lVert R(z)^N \rVert <3/4 N \max_{z\in U} \rho(z)<1 \lVert R(z)^n \rVert\to 0 z\in U n_z z \lVert R(z)^n \rVert<1/2 n\ge n_z z n_z r \lVert R(w)^{n_z} \rVert<3/4 w\in B_r(z) U U\subseteq \bigcup_{i\in\Lambda} B_{r_i}(z_i)\quad \text{where for all } w\in B_{r_i}(z_i),\, \lVert R(w)^{n_i} \rVert<3/4 \Lambda=\{1,2,\cdots,k\} N n_i N=n_1+\cdots + n_k w\in U \lVert R(w)^{N} \rVert \le \lVert R(w)^{n_1} \rVert\cdot \lVert R(w)^{n_2} \rVert\cdots \lVert R(w)^{n_k} \rVert  w\in B_{r_1}(z_1) \lVert R(w)^{N} \rVert \le \lVert R(w)^{n_1} \rVert\cdot \lVert R(w)^{n_2} \rVert\cdots \lVert R(w)^{n_k} \rVert \le \frac{3}{4} \cdot \lVert R(w)^{n_2} \rVert\cdots \lVert R(w)^{n_k} \rVert n_i z n(w) n_i \lVert R(w)^{n_i} \rVert<1/2 N","['complex-analysis', 'functional-analysis', 'analysis', 'banach-spaces', 'spectral-theory']"
86,Extending a linear operator satisfying an order condition,Extending a linear operator satisfying an order condition,,"Let $\ell^\infty$ be the usual space of bounded sequences, and consider the subspace $V_1 ⊂ \ell^\infty$ consisting of vectors with finite $1$ -norm. (That is, $V_1$ contains those $x ∈ \ell^\infty$ such that $\sum_i |x_i|$ is finite. I'm not calling this space $\ell^1$ because it inherits the $\infty$ -norm instead. I don't know what terminology would be standard.) Let $\ell^\infty$ be ordered such that $x \ge y$ iff $x_i \ge y_i$ for each $i$ . Let $$ P = \{ x ∈ \ell^\infty ∣ x ≥ y \text{ for some } y ∈ V_1 \} $$ Does there exist a linear operator $f : \ell^\infty \to V_1$ that extends the identity map on $V_1$ with the property that $x \ge f(x)$ for all $x ∈ P$ ? This seems like it should follow from some version of the Hahn-Banach theorem, maybe?","Let be the usual space of bounded sequences, and consider the subspace consisting of vectors with finite -norm. (That is, contains those such that is finite. I'm not calling this space because it inherits the -norm instead. I don't know what terminology would be standard.) Let be ordered such that iff for each . Let Does there exist a linear operator that extends the identity map on with the property that for all ? This seems like it should follow from some version of the Hahn-Banach theorem, maybe?","\ell^\infty V_1 ⊂ \ell^\infty 1 V_1 x ∈ \ell^\infty \sum_i |x_i| \ell^1 \infty \ell^\infty x \ge y x_i \ge y_i i 
P = \{ x ∈ \ell^\infty ∣ x ≥ y \text{ for some } y ∈ V_1 \}
 f : \ell^\infty \to V_1 V_1 x \ge f(x) x ∈ P","['functional-analysis', 'banach-spaces', 'banach-lattices']"
87,Proof of Brezis Theorem 9.17.,Proof of Brezis Theorem 9.17.,,"Theorem 9.17 of Brezis, ""Functional Analysis, Sobolev Spaces, and Partial Differential Equations,"" states the following claim. Suppose that $\Omega$ is of class $C^1$ . Let $u \in W^{1,p}(\Omega) \cap C(\overline{\Omega})$ with $1 \leq p < \infty$ . Then the following properties equivalent: (i) $u = 0$ on $\Gamma = \partial \Omega$ . (ii) $u \in W^{1,p}_0 (\Omega)$ . I need the proof of (i) $\Rightarrow$ (ii). The Brezis' book just prove this when $\textrm{supp} \, u$ is a compact subset of $\Omega$ . And, the Brezis' book says ""in the general case in which $\textrm{supp} \, u$ is not bounded, consider the sequence $(\zeta_n u)$ (where $(\zeta_n)$ is a sequence of cut-off functions)."" However, I cannot prove the general case. I want to show that $\|\nabla u - \nabla u_n\|_{L^p} \to 0$ , but if I try to do it, $\nabla \zeta_n$ appear and I cannot handle it.","Theorem 9.17 of Brezis, ""Functional Analysis, Sobolev Spaces, and Partial Differential Equations,"" states the following claim. Suppose that is of class . Let with . Then the following properties equivalent: (i) on . (ii) . I need the proof of (i) (ii). The Brezis' book just prove this when is a compact subset of . And, the Brezis' book says ""in the general case in which is not bounded, consider the sequence (where is a sequence of cut-off functions)."" However, I cannot prove the general case. I want to show that , but if I try to do it, appear and I cannot handle it.","\Omega C^1 u \in W^{1,p}(\Omega) \cap C(\overline{\Omega}) 1 \leq p < \infty u = 0 \Gamma = \partial \Omega u \in W^{1,p}_0 (\Omega) \Rightarrow \textrm{supp} \, u \Omega \textrm{supp} \, u (\zeta_n u) (\zeta_n) \|\nabla u - \nabla u_n\|_{L^p} \to 0 \nabla \zeta_n","['real-analysis', 'calculus', 'functional-analysis', 'analysis', 'partial-differential-equations']"
88,Understanding the role of Hilbert spaces in signal processing,Understanding the role of Hilbert spaces in signal processing,,"I'm taking a signal processing class and I've become quite confused about the nature of the subject. We began the course by elaborately developing the theory of function spaces (infinite dimensional Hilbert spaces), but only after this did I realize that signals are finite discrete functions. So it's always necessarily to embed these finite discrete-time signals in the function space in ways that seem far-fetched and rather pointless. Take the DFT for example. We have a signal $x[n]$ of finite length, $0\leq n < N-1$ and we embed this in $L^2([0,2 \pi])$ by interpreting the components as sample values for some periodic function on $[0,2\pi]$ . Then we apply the Fourier transform to get the coefficients of the Fourier series. This eventually leads to defining the space of $P_N$ of N-periodic functions with basis of the sampled complex exponentials. The story is similar for the Haar wavelet basis. The decomposition is defined on the entire continuous space $L^2(\mathbb{R})$ , and after all the effort of embedding we get a discrete transform. My question is: why not just define the discrete transforms independently, without this continuous Hilbert space machinery?","I'm taking a signal processing class and I've become quite confused about the nature of the subject. We began the course by elaborately developing the theory of function spaces (infinite dimensional Hilbert spaces), but only after this did I realize that signals are finite discrete functions. So it's always necessarily to embed these finite discrete-time signals in the function space in ways that seem far-fetched and rather pointless. Take the DFT for example. We have a signal of finite length, and we embed this in by interpreting the components as sample values for some periodic function on . Then we apply the Fourier transform to get the coefficients of the Fourier series. This eventually leads to defining the space of of N-periodic functions with basis of the sampled complex exponentials. The story is similar for the Haar wavelet basis. The decomposition is defined on the entire continuous space , and after all the effort of embedding we get a discrete transform. My question is: why not just define the discrete transforms independently, without this continuous Hilbert space machinery?","x[n] 0\leq n < N-1 L^2([0,2 \pi]) [0,2\pi] P_N L^2(\mathbb{R})","['functional-analysis', 'signal-processing']"
89,"Element of minimum norm in a closed subset of $L^2[-\pi,\pi]$",Element of minimum norm in a closed subset of,"L^2[-\pi,\pi]","Let $$Y=\Bigg\{ f\in L^2[-\pi,\pi]\, :\, \int_{-\pi}^\pi xf(x)\, dx=1,\,\, \int_{-\pi}^\pi (\sin x)f(x)\, dx=2 \Bigg\}.$$ My goal is to calculate the minimum norm element in $Y$ . The indication that appears to me is: to calculate $f\in Y$ of minimum norm, find a function of the type $g(x)=sx+t\sin x$ such that $f\in Y$ is fulfilled if and only if $f-g\in Z^\perp$ where $Z=\textrm{Span}[x,\sin x]$ , that is, that $Y=g+Z^\perp$ is fulfilled. Then $f=g$ . But I don't understand why with this procedure I will get an $f$ element of minimum norm...","Let My goal is to calculate the minimum norm element in . The indication that appears to me is: to calculate of minimum norm, find a function of the type such that is fulfilled if and only if where , that is, that is fulfilled. Then . But I don't understand why with this procedure I will get an element of minimum norm...","Y=\Bigg\{ f\in L^2[-\pi,\pi]\, :\, \int_{-\pi}^\pi xf(x)\, dx=1,\,\, \int_{-\pi}^\pi (\sin x)f(x)\, dx=2 \Bigg\}. Y f\in Y g(x)=sx+t\sin x f\in Y f-g\in Z^\perp Z=\textrm{Span}[x,\sin x] Y=g+Z^\perp f=g f",['functional-analysis']
90,Sequence of shifts in Hilbert Space,Sequence of shifts in Hilbert Space,,"I'd like someone to check my work on this 2-part problem: Let $(V_n )_{n \in \mathbb{N}}$ be a sequence of operators on $\ell^2(\mathbb{N})$ defined by $(V_n x)(k) = x(n + k)$ . Prove that $(V_n )_{n \in \mathbb{N}}$ converges in strong operator topology and identify its limit. Prove that the sequence of adjoints $(V^*_n )_{n \in \mathbb{N}}$ does not converge in the strong operator topology. My approach: Consider the limit of the square norm of the image: $lim_{n\to \infty}||V_n x||^2 =lim_{n\to \infty} \sum_{k=n}^\infty{x_k^2} = ||x||^2 - lim_{n\to \infty}\sum_{k=1}^n{x_k^2} = 0$ . Therefore $||V_nx|| \to 0$ for all $x$ , and $V_n$ converges to $0$ in SOT. For the second part, i'm quite sure that the adjoint of $V_n$ is $(V^*_nx)(k)= 0$ if $k\leq n$ , and $(V^*x)(k)=x(k-n)$ otherwise. It is then enough to mention an $x$ for which the limit $V^*_nx$ does not exist. $x_0=1,0,0...$ will do, since the sequence $V^*_nx_0$ is not Cauchy and therefore not convergent. Therefore, $V_n^*$ does not converge in SOT. any corrections and remarks are greatly appreciated, thank you!","I'd like someone to check my work on this 2-part problem: Let be a sequence of operators on defined by . Prove that converges in strong operator topology and identify its limit. Prove that the sequence of adjoints does not converge in the strong operator topology. My approach: Consider the limit of the square norm of the image: . Therefore for all , and converges to in SOT. For the second part, i'm quite sure that the adjoint of is if , and otherwise. It is then enough to mention an for which the limit does not exist. will do, since the sequence is not Cauchy and therefore not convergent. Therefore, does not converge in SOT. any corrections and remarks are greatly appreciated, thank you!","(V_n )_{n \in \mathbb{N}} \ell^2(\mathbb{N}) (V_n x)(k) = x(n + k) (V_n )_{n \in \mathbb{N}} (V^*_n )_{n \in \mathbb{N}} lim_{n\to \infty}||V_n x||^2 =lim_{n\to \infty} \sum_{k=n}^\infty{x_k^2} = ||x||^2 - lim_{n\to \infty}\sum_{k=1}^n{x_k^2} = 0 ||V_nx|| \to 0 x V_n 0 V_n (V^*_nx)(k)= 0 k\leq n (V^*x)(k)=x(k-n) x V^*_nx x_0=1,0,0... V^*_nx_0 V_n^*","['functional-analysis', 'solution-verification', 'hilbert-spaces']"
91,Has this energy a maximum in $B$?,Has this energy a maximum in ?,B,"Suppose $\mathcal{H}$ is an Hilbert space and $\langle ., . \rangle$ his scalar product. Suppose $T:\mathcal{H} \to \mathcal{H}$ is linear, continuos, compact and self-adjoint. If we define $J:\mathcal{H} \to \mathbb{R}$ as $J(x)=\langle Tx, x \rangle$ for all $x \in \mathcal{H}$ , can we conclude $J$ has a maximum in $B=\{x \in \mathcal{H}: \|x\| \leq 1\}$ ? I know that if $\mathcal{H}$ is separable I can use the fact that $B$ is sequentially compact and I can work with a maximizing sequence. But If I do not know the separability of $\mathcal{H}$ ? Does the same statement hold? I tried proving that $J_{|B}:B \to \mathbb{R}$ is weakly superior semi-continuos. In fact if $u_n \rightharpoonup u$ then $J(u_n) \to j(u)$ . Is it enough to conclude?","Suppose is an Hilbert space and his scalar product. Suppose is linear, continuos, compact and self-adjoint. If we define as for all , can we conclude has a maximum in ? I know that if is separable I can use the fact that is sequentially compact and I can work with a maximizing sequence. But If I do not know the separability of ? Does the same statement hold? I tried proving that is weakly superior semi-continuos. In fact if then . Is it enough to conclude?","\mathcal{H} \langle ., . \rangle T:\mathcal{H} \to \mathcal{H} J:\mathcal{H} \to \mathbb{R} J(x)=\langle Tx, x \rangle x \in \mathcal{H} J B=\{x \in \mathcal{H}: \|x\| \leq 1\} \mathcal{H} B \mathcal{H} J_{|B}:B \to \mathbb{R} u_n \rightharpoonup u J(u_n) \to j(u)","['functional-analysis', 'hilbert-spaces', 'banach-spaces', 'spectral-theory']"
92,"In ""Functional Analysis,"" is the word ""Functional"" an adjective, or a noun?","In ""Functional Analysis,"" is the word ""Functional"" an adjective, or a noun?",,"Does 'functional' mean ""relating to functions"" or ""a mapping from a space to its scalars""?","Does 'functional' mean ""relating to functions"" or ""a mapping from a space to its scalars""?",,"['functional-analysis', 'soft-question']"
93,Every metric space can be isometrically embedded in a Banach space.,Every metric space can be isometrically embedded in a Banach space.,,"(I have already read the explanations given for the suggested similar questions.) I approximate that I only understand about 40% of this solution.  I have been told that this follows from Kuratowski's Embedding Theorem but I can't find much more than a statement of the theorem.  I would like references on where I can get the information needed to comprehend this proof or other elementary proofs of the statement. Let $X$ be a set and $B(X, \mathbb R)$ the set of bounded functions $f:X\to \mathbb R$ with norm $||f||=\sup\{|f(x)|:x\in X\}$ . Claim: Every metric space $(X,d)$ can be embedded isometrically into the Banach Space $E=B(X,\mathbb R)$ . Proof of claim: Assume $X\neq \varnothing$ . Fix a point $a_0\in X$ and for every $a\in X$ define a function $f_a:X\to\mathbb R$ by $f_a(x)=d(x,a)-d(x,a_0)$ . The $|f_a(x)|\leq d(a,a_0)$ for every $x\in X$ so $f_a$ is bounded. By setting $\rho : X\to E$ , $\rho(a)=f_a$ we have the mapping $\rho : X \to E$ . Claim: $\rho$ is an isometry. Proof: Let $a,b\in X$ . As $x \in X$ we have that $|f_a(x)-f_b(x)|=|d(x,a)-d(x,b)|\leq d(a,b) \therefore ||f_a-f_b||\leq d(a,b)$ . On the otherhand (this specifically is a big part I dont understand, why are we looking at $f$ at $a$ now instead of $f$ at $x$ ?) $|f_a(a)-f_b(a)|=|d(a,a)-d(a,a_0)-d(a,b)+d(a,a_o)|=d(a,b) \therefore ||\rho(a)-\rho(b)||=||f_a-f_b||=d(a,b).$","(I have already read the explanations given for the suggested similar questions.) I approximate that I only understand about 40% of this solution.  I have been told that this follows from Kuratowski's Embedding Theorem but I can't find much more than a statement of the theorem.  I would like references on where I can get the information needed to comprehend this proof or other elementary proofs of the statement. Let be a set and the set of bounded functions with norm . Claim: Every metric space can be embedded isometrically into the Banach Space . Proof of claim: Assume . Fix a point and for every define a function by . The for every so is bounded. By setting , we have the mapping . Claim: is an isometry. Proof: Let . As we have that . On the otherhand (this specifically is a big part I dont understand, why are we looking at at now instead of at ?)","X B(X, \mathbb R) f:X\to \mathbb R ||f||=\sup\{|f(x)|:x\in X\} (X,d) E=B(X,\mathbb R) X\neq \varnothing a_0\in X a\in X f_a:X\to\mathbb R f_a(x)=d(x,a)-d(x,a_0) |f_a(x)|\leq d(a,a_0) x\in X f_a \rho : X\to E \rho(a)=f_a \rho : X \to E \rho a,b\in X x \in X |f_a(x)-f_b(x)|=|d(x,a)-d(x,b)|\leq d(a,b) \therefore ||f_a-f_b||\leq d(a,b) f a f x |f_a(a)-f_b(a)|=|d(a,a)-d(a,a_0)-d(a,b)+d(a,a_o)|=d(a,b) \therefore ||\rho(a)-\rho(b)||=||f_a-f_b||=d(a,b).","['functional-analysis', 'banach-spaces']"
94,Fixed point theorem involving nonexpansive mapping on uniformly convex Banach space,Fixed point theorem involving nonexpansive mapping on uniformly convex Banach space,,"This is an exercise from the Dirk Werner's book about fixed point theorem: Let $X$ be a uniformly convex Banach space. Let $F:B_X\to X$ be a nonexpansive mapping, i.e. $$ \forall x,y\in B_X: \|F(x)-F(y)\|\leq \|x-y\|.$$ Here denotes $B_X$ the closed unit ball in $X$ . Then Either $F$ has a fixed point, or there exists some $x\in S_X$ and some $\lambda>1$ such that $F(x)=\lambda x$ . Here $S_X$ denotes the unit sphere of $X$ . If $F(S_X)\subset B_X$ , then $F$ has a fixed point. I am still working with 1. What I tried is to assume that $F$ has no fixed point. In this case, the range of $F$ is contained in the ball $B_{1+\|F(0)\|}$ , and using rescaling one easily concludes that $F(x)=\lambda x$ for some $x\in B_X$ and $\lambda>1$ . However, I am not able to conclude that the point $x$ is exactly a point on the sphere. Any advice and suggestion is very welcome! Some thoughts: To show the statement (assuming no fixed points) it is equivalent to show that the function $(F(z)-z)/\|F(z)-z\|$ has a fixed point. Notice that this function has image in $S_X$ , so one might want to apply the Browder's FPT to this function stated in the Werner's textbook. However, it is no more nonexpansive now, so my goal is to construct a new function, derived from this one, which has image in $B_X$ and nonexpansive.","This is an exercise from the Dirk Werner's book about fixed point theorem: Let be a uniformly convex Banach space. Let be a nonexpansive mapping, i.e. Here denotes the closed unit ball in . Then Either has a fixed point, or there exists some and some such that . Here denotes the unit sphere of . If , then has a fixed point. I am still working with 1. What I tried is to assume that has no fixed point. In this case, the range of is contained in the ball , and using rescaling one easily concludes that for some and . However, I am not able to conclude that the point is exactly a point on the sphere. Any advice and suggestion is very welcome! Some thoughts: To show the statement (assuming no fixed points) it is equivalent to show that the function has a fixed point. Notice that this function has image in , so one might want to apply the Browder's FPT to this function stated in the Werner's textbook. However, it is no more nonexpansive now, so my goal is to construct a new function, derived from this one, which has image in and nonexpansive.","X F:B_X\to X  \forall x,y\in B_X: \|F(x)-F(y)\|\leq \|x-y\|. B_X X F x\in S_X \lambda>1 F(x)=\lambda x S_X X F(S_X)\subset B_X F F F B_{1+\|F(0)\|} F(x)=\lambda x x\in B_X \lambda>1 x (F(z)-z)/\|F(z)-z\| S_X B_X","['real-analysis', 'functional-analysis', 'fixed-point-theorems']"
95,Interpretation of lemma: $f\in L^{1}_{loc}(\Omega)\ st\ \int f\phi=0\: \forall\phi\in\mathcal{D}(\Omega)\ \Rightarrow f=0\ in\ L^{1}_{loc}(\Omega)$,Interpretation of lemma:,f\in L^{1}_{loc}(\Omega)\ st\ \int f\phi=0\: \forall\phi\in\mathcal{D}(\Omega)\ \Rightarrow f=0\ in\ L^{1}_{loc}(\Omega),"As in the title, consider the following lemma in the theory of distributions: $$ f\in L^{1}_{loc}(\Omega)\;\;\text{ s.t. }\int f\phi=0\quad \forall\phi\in\mathcal{D}(\Omega)\ \implies f=0\;\text{ in }\; L^{1}_{loc}(\Omega) $$ where $\mathcal{D}(\Omega)$ is the space of compactly supported $C^{\infty}$ functions, also known as test functions. The proof I know is based on mollifiers and convolution: the result follows by the uniform convergence of the regularized $f_\epsilon=f\ast\rho_\epsilon$ to $f$ , as $\epsilon\rightarrow 0$ where $\rho_\epsilon$ are a family of mollifiers. Now, we can see the same result written as $\langle f,\phi\rangle=0\; \forall\phi\in\mathcal{D}(\Omega)\ \Rightarrow\ f=0\ in\ L^1_{loc}(\Omega)$ where we view $f$ as an element of the dual $\mathcal{D}'(\Omega)$ . This may be interpreted as $f\in\mathcal{D}(\Omega)^{\perp}\subset\mathcal{D}'(\Omega)\ \Rightarrow f=0$ . If we where in a (seprable) Hilbert space, where the inner product allows us to 'internalize' the notion of dual (through Riesz theorem) and as a consequence that of orthogonal (as the most natural one) this consition is known to be necessary and sufficient for the set we are treating to be a orthogonal basis. I wonder if: we can have an analogous in this case (maybe through the notion of density of $\mathcal{D}(\Omega)$ in $L^1$ ?) where the setting is that of general Topological Vector Spaces It is in general possible to define and charachterize a subspace $M\subset E$ such that $f\in E\ st\ \langle f,\phi\rangle =0\ \forall\phi\in M\ \Rightarrow f=0\ \in E'$ If yes, can we in some sense think of M as a ''basis''? More in general are there relevant generalization of the concept in arbitary TVS?","As in the title, consider the following lemma in the theory of distributions: where is the space of compactly supported functions, also known as test functions. The proof I know is based on mollifiers and convolution: the result follows by the uniform convergence of the regularized to , as where are a family of mollifiers. Now, we can see the same result written as where we view as an element of the dual . This may be interpreted as . If we where in a (seprable) Hilbert space, where the inner product allows us to 'internalize' the notion of dual (through Riesz theorem) and as a consequence that of orthogonal (as the most natural one) this consition is known to be necessary and sufficient for the set we are treating to be a orthogonal basis. I wonder if: we can have an analogous in this case (maybe through the notion of density of in ?) where the setting is that of general Topological Vector Spaces It is in general possible to define and charachterize a subspace such that If yes, can we in some sense think of M as a ''basis''? More in general are there relevant generalization of the concept in arbitary TVS?","
f\in L^{1}_{loc}(\Omega)\;\;\text{ s.t. }\int f\phi=0\quad \forall\phi\in\mathcal{D}(\Omega)\ \implies f=0\;\text{ in }\; L^{1}_{loc}(\Omega)
 \mathcal{D}(\Omega) C^{\infty} f_\epsilon=f\ast\rho_\epsilon f \epsilon\rightarrow 0 \rho_\epsilon \langle f,\phi\rangle=0\; \forall\phi\in\mathcal{D}(\Omega)\ \Rightarrow\ f=0\ in\ L^1_{loc}(\Omega) f \mathcal{D}'(\Omega) f\in\mathcal{D}(\Omega)^{\perp}\subset\mathcal{D}'(\Omega)\ \Rightarrow f=0 \mathcal{D}(\Omega) L^1 M\subset E f\in E\ st\ \langle f,\phi\rangle =0\ \forall\phi\in M\ \Rightarrow f=0\ \in E'","['functional-analysis', 'distribution-theory', 'convolution', 'topological-vector-spaces']"
96,Sum of closed subspaces of Banach space is closed,Sum of closed subspaces of Banach space is closed,,"I am currently struggling with the following exercise: Let $B$ be a Banach space and $C, D \subset B$ closed subspaces of $B$ . There is a $M \in ]0, \infty[$ such that $\forall  x \in D : \operatorname{dist}(x, C \cap D) \leq M \cdot \operatorname{dist}(x, C)$ holds. Show that $C + D$ is closed.",I am currently struggling with the following exercise: Let be a Banach space and closed subspaces of . There is a such that holds. Show that is closed.,"B C, D \subset B B M \in ]0, \infty[ \forall  x \in D : \operatorname{dist}(x, C \cap D) \leq M \cdot \operatorname{dist}(x, C) C + D","['functional-analysis', 'banach-spaces']"
97,Calculus of Variations Problem with Convolution Constraint,Calculus of Variations Problem with Convolution Constraint,,"Consider the following problem: \begin{eqnarray} &\inf_{f} {\int\limits_0^1 g(x)f(x)   dx}\\ &\textrm{s.t.} \quad \int\limits_0^{2} g(x)(f\ast f(x))   dx=c\\ & \quad f:[0,1] \rightarrow [0,1] \end{eqnarray} where $f\ast f(x)$ is the convolution of $f$ with itself, $c>0$ is low enough so that the feasible set is non-empty. I plan to start working with the first variation, but wanted to pause and check here if there is a standard method of dealing with this kind of problem (maybe convolution suggests a move to the frequency domain?). Thank you. Note : Original problem had $g(x)=xe^{-x}$ , but the solution is more general.","Consider the following problem: where is the convolution of with itself, is low enough so that the feasible set is non-empty. I plan to start working with the first variation, but wanted to pause and check here if there is a standard method of dealing with this kind of problem (maybe convolution suggests a move to the frequency domain?). Thank you. Note : Original problem had , but the solution is more general.","\begin{eqnarray}
&\inf_{f} {\int\limits_0^1 g(x)f(x)   dx}\\
&\textrm{s.t.} \quad \int\limits_0^{2} g(x)(f\ast f(x))   dx=c\\
& \quad f:[0,1] \rightarrow [0,1]
\end{eqnarray} f\ast f(x) f c>0 g(x)=xe^{-x}","['functional-analysis', 'convolution', 'calculus-of-variations']"
98,"Compact operators on $L_1[0,1]$",Compact operators on,"L_1[0,1]","Let $(T_Kx)(t)=\int_0^1 K(t, s)x(s)ds$ be linear operator on $L_1[0,1]$ . Here $K(t, s)$ is measurable function on $[0,1]\times[0,1]$ with $\sup\{\int_0^1|K(t,s)|dt ,s\in[0,1]\}<\infty$ . Prove that $T_K:L_1[0,1]\to L_1[0,1]$ is compact. My thoughts: It is easy to show that $T$ is bounded. Next I want to show that $T$ is compact, i.e. for any bounded set $M\subset L_1[0,1]$ the image $T(M)$ is precompact. We have a criterion for precompactness in $L_1[0,1]:$ $A\subset L_1[0,1]$ is precompact $\Leftrightarrow$ $A$ is bounded and for any $\varepsilon>0$ there is $\delta>0$ such that for any $h\in[0,\delta]$ and $x\in A$ we have $\int_0^1|x(t+h)-x(t)|dt<\varepsilon$ , where $x(t+h)=0$ when $t+h\notin[0,1]$ . I want to check whether it is true for $T(M)$ , where $M$ is a bounded set. We want \begin{align} \int^1_0|(Tx)(t+h)-(Tx)(t)|dt&=\int^1_0\Bigl|\int_0^1 \Bigl(K(t+h,s)-K(t,s)\Bigr)x(s)ds\Bigr|dt\\ &\leq \int^1_0\int_0^1 \Bigl|K(t+h,s)-K(t,s)\Bigr||x(s)|dsdt \end{align} to be small for all $x\in M$ and small $h$ . To show that it is small I try to approximate $K(t, s)$ with continuous function. Using Luzin's theorem we can find continuous on $[0,1]\times[0,1]$ functions $g(t,s)$ so that $K(t,s)$ and $g(t, s)$ are not equal only on set of arbitrarily small measure. We have \begin{align} \int^1_0\int_0^1 \Bigl|K(t+h,s)-K(t,s)\Bigr||x(s)|dsdt&=\iint_A \Bigl|K(t+h,s)-K(t,s)\Bigr||x(s)|dsdt \,+\\ &\quad \iint_B \Bigl|g(t+h,s)-g(t,s)\Bigr||x(s)|dsdt=:I_1+I_2 \end{align} where $B=\{(t,s)\in [0, 1]\times [0,1]: g(t+h,s)=K(t+h,s), g(t,s)=K(t,s)\}$ , $A=[0,1]\times[0,1]-B$ . If $g(t,s)$ is fixed, second integral $I_2$ will be small for small $h$ due to uniform continuity of $g(t,s)$ and estimate can be made independent of $x$ . My problems began when I tried to estimate first integral $I_1$ . Due to absolute continuity of Lebesgue's integral, $I_1$ is small when measure of $A$ is small, but this estimate depends on $x$ . How can the problem be solved?","Let be linear operator on . Here is measurable function on with . Prove that is compact. My thoughts: It is easy to show that is bounded. Next I want to show that is compact, i.e. for any bounded set the image is precompact. We have a criterion for precompactness in is precompact is bounded and for any there is such that for any and we have , where when . I want to check whether it is true for , where is a bounded set. We want to be small for all and small . To show that it is small I try to approximate with continuous function. Using Luzin's theorem we can find continuous on functions so that and are not equal only on set of arbitrarily small measure. We have where , . If is fixed, second integral will be small for small due to uniform continuity of and estimate can be made independent of . My problems began when I tried to estimate first integral . Due to absolute continuity of Lebesgue's integral, is small when measure of is small, but this estimate depends on . How can the problem be solved?","(T_Kx)(t)=\int_0^1 K(t, s)x(s)ds L_1[0,1] K(t, s) [0,1]\times[0,1] \sup\{\int_0^1|K(t,s)|dt ,s\in[0,1]\}<\infty T_K:L_1[0,1]\to L_1[0,1] T T M\subset L_1[0,1] T(M) L_1[0,1]: A\subset L_1[0,1] \Leftrightarrow A \varepsilon>0 \delta>0 h\in[0,\delta] x\in A \int_0^1|x(t+h)-x(t)|dt<\varepsilon x(t+h)=0 t+h\notin[0,1] T(M) M \begin{align}
\int^1_0|(Tx)(t+h)-(Tx)(t)|dt&=\int^1_0\Bigl|\int_0^1 \Bigl(K(t+h,s)-K(t,s)\Bigr)x(s)ds\Bigr|dt\\
&\leq \int^1_0\int_0^1 \Bigl|K(t+h,s)-K(t,s)\Bigr||x(s)|dsdt
\end{align} x\in M h K(t, s) [0,1]\times[0,1] g(t,s) K(t,s) g(t, s) \begin{align}
\int^1_0\int_0^1 \Bigl|K(t+h,s)-K(t,s)\Bigr||x(s)|dsdt&=\iint_A \Bigl|K(t+h,s)-K(t,s)\Bigr||x(s)|dsdt \,+\\
&\quad \iint_B \Bigl|g(t+h,s)-g(t,s)\Bigr||x(s)|dsdt=:I_1+I_2
\end{align} B=\{(t,s)\in [0, 1]\times [0,1]: g(t+h,s)=K(t+h,s), g(t,s)=K(t,s)\} A=[0,1]\times[0,1]-B g(t,s) I_2 h g(t,s) x I_1 I_1 A x","['functional-analysis', 'lp-spaces', 'compact-operators']"
99,Confusions in Evans book regarding weak derivatives in Banach spaces,Confusions in Evans book regarding weak derivatives in Banach spaces,,"I am studying PDE using Evans' book and I have two main confusions (probably stupid questions to experts) regarding weak derivatives in Banach spaces. First confusion: $\def\u{\mathbf u}$ $\def\v{\mathbf v}$ DEFINITION. Let $\u \in L^1(0, T; X)$ . We say $\v \in L^1(0, T; X)$ is the weak derivative of $\u$ , written $$\u'=\v,$$ provided $$\int_0^T \phi'(t) \u(t) \, dt = -\int_0^T \phi(t) \v(t) \, dt$$ for all scalar test functions $\phi \in C_c^\infty (0, T)$ . However, a subsequent theorem begins with an assumption of THEOREM $\mathbf 3$ (More calculus). Suppose $\u \in L^2(0, T; H_0^1(U))$ , with $\u' \in L^2(0, T; H^{-1}(U))$ . Now, the main confusion here is that $H^{-1}$ is only the dual space of $H^1_0$ , not a subset nor a superset of $H^1_0$ , whereas in the definition, both the function and its weak derivative take values in the same Banach space $X$ . Is there some kind of hidden identification of spaces? Second confusion: THEOREM $\mathbf 2$ (Calculus in an abstract space). Let $\u \in W^{1,p} (0, T; X)$ for some $1 \leq p \leq \infty$ . Then (i) $\u \in C([0,T]; X)$ (after possibly being redefined on a set of measure zero), and (ii) $\u(t) = \u(s) +\displaystyle\int_s^t \u'(\tau) \, d\tau$ for all $0 \leq s \leq t \leq T$ . (iii) Furthermore, we have the estimate $$\max_{0 \leq t \leq T} \|\u(t)\| \leq C\|\u\|_{W^{1,p} (0,T;X)},  \tag{7}$$ the constant $C$ depending only on $T$ . Proof. $1$ . Extend $\u$ to be $\mathbf 0$ on $(-\infty, 0)$ and $(T, \infty)$ , and then set $\u^\varepsilon = \eta_\varepsilon * \u$ , $\eta_\varepsilon$ denoting the usual mollifier on $\mathbb R^1$ . We check as in the proof of Theorem $1$ in $\S5.3.1$ that $\u^{\varepsilon'} = \eta_\varepsilon * \u'$ on $(\varepsilon, T-\varepsilon)$ . Then as $\varepsilon \to 0$ , $$\begin{cases} \u^\varepsilon \to \u & \text{in } L^p(0, T; X), \\ (\u^\varepsilon)' \to \u' & \text{in } L^p (0, T; X). \end{cases}  \tag{8}$$ Fixing $0<s<t<T$ , we compute $$\boxed{\u^\varepsilon (t) = \u^\varepsilon (s) +\int_s^t \u^\varepsilon {}' (\tau) \, d\tau.}$$ Some kind of ""fundamental theorem of calculus for Bochner integrals"" seems to be used here. Am I correct that the functions are $C^1$ after mollification, so some version of the ""fundamental theorem of calculus for Bochner integrals"" can be applied? Here, at least the weak derivative and the function belong to the same space, as in the definition. But this is not the case in the proof of the subsequent theorem. Dual functions suddenly appear as derivatives: Proof. $1$ . Extend $\u$ to the larger interval $[-\sigma, T+\sigma]$ for $\sigma>0$ , and define the regularizations $\u^\varepsilon = \eta_\varepsilon * \u$ , as in the earlier proof. Then for $\varepsilon$ , $\delta>0$ , $$\frac{d}{dt} \|\u^\varepsilon (t) -\u^\delta (t)\|_{L^2(U)}^2 = 2 \bigl(\u^{\varepsilon'} (t) -\u^{\delta'} (t), \u^\varepsilon (t) -\u^\delta (t)\bigr)_{L^2(U)}.$$ Thus \begin{align} \|\u^\varepsilon (t) -\u^\delta (t)\|_{L^2(U)}^2 &= \|\u^\varepsilon (s) -\u^\delta (s)\|_{L^2(U)}^2 \\ &{}+2\int_s^t \langle \u^{\varepsilon'} (\tau) -\u^{\delta'} (\tau), \u^\varepsilon (\tau) -\u^\delta (\tau)\rangle \, d\tau  \tag{11} \end{align} None of these two ""fundamental theorems of calculus"" are derived explicitly and I am really confused!","I am studying PDE using Evans' book and I have two main confusions (probably stupid questions to experts) regarding weak derivatives in Banach spaces. First confusion: DEFINITION. Let . We say is the weak derivative of , written provided for all scalar test functions . However, a subsequent theorem begins with an assumption of THEOREM (More calculus). Suppose , with . Now, the main confusion here is that is only the dual space of , not a subset nor a superset of , whereas in the definition, both the function and its weak derivative take values in the same Banach space . Is there some kind of hidden identification of spaces? Second confusion: THEOREM (Calculus in an abstract space). Let for some . Then (i) (after possibly being redefined on a set of measure zero), and (ii) for all . (iii) Furthermore, we have the estimate the constant depending only on . Proof. . Extend to be on and , and then set , denoting the usual mollifier on . We check as in the proof of Theorem in that on . Then as , Fixing , we compute Some kind of ""fundamental theorem of calculus for Bochner integrals"" seems to be used here. Am I correct that the functions are after mollification, so some version of the ""fundamental theorem of calculus for Bochner integrals"" can be applied? Here, at least the weak derivative and the function belong to the same space, as in the definition. But this is not the case in the proof of the subsequent theorem. Dual functions suddenly appear as derivatives: Proof. . Extend to the larger interval for , and define the regularizations , as in the earlier proof. Then for , , Thus None of these two ""fundamental theorems of calculus"" are derived explicitly and I am really confused!","\def\u{\mathbf u} \def\v{\mathbf v} \u \in L^1(0, T; X) \v \in L^1(0, T; X) \u \u'=\v, \int_0^T \phi'(t) \u(t) \, dt = -\int_0^T \phi(t) \v(t) \, dt \phi \in C_c^\infty (0, T) \mathbf 3 \u \in L^2(0, T; H_0^1(U)) \u' \in L^2(0, T; H^{-1}(U)) H^{-1} H^1_0 H^1_0 X \mathbf 2 \u \in W^{1,p} (0, T; X) 1 \leq p \leq \infty \u \in C([0,T]; X) \u(t) = \u(s) +\displaystyle\int_s^t \u'(\tau) \, d\tau 0 \leq s \leq t \leq T \max_{0 \leq t \leq T} \|\u(t)\| \leq C\|\u\|_{W^{1,p} (0,T;X)},  \tag{7} C T 1 \u \mathbf 0 (-\infty, 0) (T, \infty) \u^\varepsilon = \eta_\varepsilon * \u \eta_\varepsilon \mathbb R^1 1 \S5.3.1 \u^{\varepsilon'} = \eta_\varepsilon * \u' (\varepsilon, T-\varepsilon) \varepsilon \to 0 \begin{cases} \u^\varepsilon \to \u & \text{in } L^p(0, T; X), \\ (\u^\varepsilon)' \to \u' & \text{in } L^p (0, T; X). \end{cases}  \tag{8} 0<s<t<T \boxed{\u^\varepsilon (t) = \u^\varepsilon (s) +\int_s^t \u^\varepsilon {}' (\tau) \, d\tau.} C^1 1 \u [-\sigma, T+\sigma] \sigma>0 \u^\varepsilon = \eta_\varepsilon * \u \varepsilon \delta>0 \frac{d}{dt} \|\u^\varepsilon (t) -\u^\delta (t)\|_{L^2(U)}^2 = 2 \bigl(\u^{\varepsilon'} (t) -\u^{\delta'} (t), \u^\varepsilon (t) -\u^\delta (t)\bigr)_{L^2(U)}. \begin{align} \|\u^\varepsilon (t) -\u^\delta (t)\|_{L^2(U)}^2 &= \|\u^\varepsilon (s) -\u^\delta (s)\|_{L^2(U)}^2 \\ &{}+2\int_s^t \langle \u^{\varepsilon'} (\tau) -\u^{\delta'} (\tau), \u^\varepsilon (\tau) -\u^\delta (\tau)\rangle \, d\tau  \tag{11} \end{align}","['functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
