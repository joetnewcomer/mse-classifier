,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,The tangent space of a manifold at a point given as the kernel of the jacobian of a submersion,The tangent space of a manifold at a point given as the kernel of the jacobian of a submersion,,"Let $\phi:M\to N$ is a smooth map, $q\in N$ a regular value, and $V=\phi^{-1}(q)$. I want to show that, for each $p\in V$, $T_p(V)= \mathrm{ker}(\phi_*)\subseteq T_p(M)$ (where $\phi_*$ is the differential of $\phi$). Since $q$ is regular, I know $\phi_*(T_p(M))=T_q(N)$. I believe I can say $\phi_*(T_p(V))=T_q(\{q\})=0$. But I don't know why $\phi_*^{-1}(0)=T_p(V)$.  Any input would be greatly appreciated.","Let $\phi:M\to N$ is a smooth map, $q\in N$ a regular value, and $V=\phi^{-1}(q)$. I want to show that, for each $p\in V$, $T_p(V)= \mathrm{ker}(\phi_*)\subseteq T_p(M)$ (where $\phi_*$ is the differential of $\phi$). Since $q$ is regular, I know $\phi_*(T_p(M))=T_q(N)$. I believe I can say $\phi_*(T_p(V))=T_q(\{q\})=0$. But I don't know why $\phi_*^{-1}(0)=T_p(V)$.  Any input would be greatly appreciated.",,"['linear-algebra', 'differential-geometry', 'manifolds']"
1,Linear Dependence Lemma,Linear Dependence Lemma,,"This is out of my textbook, Axler's ""Linear Algebra Done Right"" which I am self-studying from. ( I organized my thoughts in which I would like some sort of response with Roman Numerals ). Linear Dependence Lemma : If $(v_{1},\ldots,v_{m})$ is linearly dependent in $V$ and $v_{1} \neq 0$, then there exists $j \in \{2,\ldots,m\}$ such that the following hold: (a) $v_{j} \in span(v_{1},\ldots,v_{j-1})$; ( I. Why does this need to be justified? Is it because $v_{j}$ is an ""extra"" vector, which would make this arbitrary set of linear combinations dependent?). (b) If the $j^{th}$ term is removed from $(v_{1},\ldots,v_{m})$, the span of the remaining list equal $span(v_{1},\ldots,v_{m})$. ( II. My assumption is that this basically means that if we remove this extra vector, then we still have the same list of linear combinations). (I also found the following proof a bit confusing and need some clarification). PROOF: Suppose $(v_{1},\ldots,v_{m})$ is linearly dependent in $V$ and $v_{1} \neq 0$. Then there exists $a_{1},\ldots,a_{m} \in \mathbb{F}$, not all $0$ such that $$a_{1}v_{1}+\cdots+a_{m}v_{m} = 0$$. (So far so good, from what I know, this is just stating the opposite of Linear Independence, where the only choice of $a_{1},\ldots,a_{m} \in \mathbb{F}$ that satisfies $a_{1}v_{1}+\cdots+a_{m}v_{m} = 0$  is $a_1 =\cdots= a_{m} = 0$) CONT: Not all of $a_{2},a_{3},\ldots,a_{m}$ can be $0$ (because $v_1 \neq 0)$. Let $j$ be the largest element of $\{2,\ldots.,m\}$ such that $a_{j} \neq 0$. Then  $$ v_{j} = -\frac{a_1}{a_j}v_1 - \cdots - \frac{a_{j-1}}{a_j}v_{j-1} ,$$ proving (a). ( III. I will fill in the extra steps here because I feel that I may have the right idea). $Span(v_{1},\ldots,v_{m}) = 0$ for $j \in \{2,\ldots,m\} = a_{1}v_{1} + \cdots + a_{j}v_{j} = 0$. Here I just solved for $v_j$, and got the result  $v_{j} = -\frac{a_1}{a_j}v_1 - \cdots - \frac{a_{j-1}}{a_j}v_{j-1} ,$ which corresponds to the above. $a_{j} \neq 0$ because we have $a_j^{-1}$ for each term, and $v_1 \neq 0$ because if we have $a_{1}v_{1}+\cdots+a_{j}v_{j} = 0$ then all the scalars $a_{2},\ldots,a_{m} \in \mathbb{F}$ could be equal to $0$, if that was the case. I think I have an idea, but how exactly does this prove that $v_j$ is contained in the span of $(v_{1},\ldots,v_{j-1})$? Is it because $ -\frac{a_1}{a_j}v_1 - \cdots - \frac{a_{j-1}}{a_j}v_{j-1}$, is just a linear combination of vectors that is equal to $v_j$? CONT: to prove (b), suppose that $u \in span(v_{1},\ldots,v_{m})$. Then there exists $c_{1},\ldots,c_m \in \mathbb{F}$ such that $$u = c_1v_1 + \cdots + c_mv_m$$. In the equation above, we replace $v_j$ with the right side of 2.5, which shows that $u$ is in the span of the list obtained by removing the $j^{th}$ term from $(v_1,\ldots,v_m)$. Thus (b) holds. $\Box$ ( IV. So how exactly does this work? I find this part the most confusing). Sorry that this is such a long list, but I really want to fully understand everything I am learning, and I am pretty new to proving stuff, so I want to make sure that I improve that skill as well.","This is out of my textbook, Axler's ""Linear Algebra Done Right"" which I am self-studying from. ( I organized my thoughts in which I would like some sort of response with Roman Numerals ). Linear Dependence Lemma : If $(v_{1},\ldots,v_{m})$ is linearly dependent in $V$ and $v_{1} \neq 0$, then there exists $j \in \{2,\ldots,m\}$ such that the following hold: (a) $v_{j} \in span(v_{1},\ldots,v_{j-1})$; ( I. Why does this need to be justified? Is it because $v_{j}$ is an ""extra"" vector, which would make this arbitrary set of linear combinations dependent?). (b) If the $j^{th}$ term is removed from $(v_{1},\ldots,v_{m})$, the span of the remaining list equal $span(v_{1},\ldots,v_{m})$. ( II. My assumption is that this basically means that if we remove this extra vector, then we still have the same list of linear combinations). (I also found the following proof a bit confusing and need some clarification). PROOF: Suppose $(v_{1},\ldots,v_{m})$ is linearly dependent in $V$ and $v_{1} \neq 0$. Then there exists $a_{1},\ldots,a_{m} \in \mathbb{F}$, not all $0$ such that $$a_{1}v_{1}+\cdots+a_{m}v_{m} = 0$$. (So far so good, from what I know, this is just stating the opposite of Linear Independence, where the only choice of $a_{1},\ldots,a_{m} \in \mathbb{F}$ that satisfies $a_{1}v_{1}+\cdots+a_{m}v_{m} = 0$  is $a_1 =\cdots= a_{m} = 0$) CONT: Not all of $a_{2},a_{3},\ldots,a_{m}$ can be $0$ (because $v_1 \neq 0)$. Let $j$ be the largest element of $\{2,\ldots.,m\}$ such that $a_{j} \neq 0$. Then  $$ v_{j} = -\frac{a_1}{a_j}v_1 - \cdots - \frac{a_{j-1}}{a_j}v_{j-1} ,$$ proving (a). ( III. I will fill in the extra steps here because I feel that I may have the right idea). $Span(v_{1},\ldots,v_{m}) = 0$ for $j \in \{2,\ldots,m\} = a_{1}v_{1} + \cdots + a_{j}v_{j} = 0$. Here I just solved for $v_j$, and got the result  $v_{j} = -\frac{a_1}{a_j}v_1 - \cdots - \frac{a_{j-1}}{a_j}v_{j-1} ,$ which corresponds to the above. $a_{j} \neq 0$ because we have $a_j^{-1}$ for each term, and $v_1 \neq 0$ because if we have $a_{1}v_{1}+\cdots+a_{j}v_{j} = 0$ then all the scalars $a_{2},\ldots,a_{m} \in \mathbb{F}$ could be equal to $0$, if that was the case. I think I have an idea, but how exactly does this prove that $v_j$ is contained in the span of $(v_{1},\ldots,v_{j-1})$? Is it because $ -\frac{a_1}{a_j}v_1 - \cdots - \frac{a_{j-1}}{a_j}v_{j-1}$, is just a linear combination of vectors that is equal to $v_j$? CONT: to prove (b), suppose that $u \in span(v_{1},\ldots,v_{m})$. Then there exists $c_{1},\ldots,c_m \in \mathbb{F}$ such that $$u = c_1v_1 + \cdots + c_mv_m$$. In the equation above, we replace $v_j$ with the right side of 2.5, which shows that $u$ is in the span of the list obtained by removing the $j^{th}$ term from $(v_1,\ldots,v_m)$. Thus (b) holds. $\Box$ ( IV. So how exactly does this work? I find this part the most confusing). Sorry that this is such a long list, but I really want to fully understand everything I am learning, and I am pretty new to proving stuff, so I want to make sure that I improve that skill as well.",,['linear-algebra']
2,Relationship between the rows and columns of a matrix,Relationship between the rows and columns of a matrix,,"I am having trouble understanding the relatioship between rows and columns of a matrix. Say, the following homogeneous system has a nontrivial solution. $$ 3x_1 + 5x_2 − 4x_3 = 0  \\ −3x_1 − 2x_2 + 4x_3 = 0  \\ 6x_1 + x_2 − 8x_3 = 0\\ $$ Let A be the coefficient matrix and row reduce  $\begin{bmatrix} A & \mathbf 0 \end{bmatrix}$  to row-echelon form: $\begin{bmatrix}3&5&-4&0\\-3&-2&4&0\\6&1&-8&0\\ \end{bmatrix} \rightarrow \begin{bmatrix}3&5&-4&0\\0&3&0&0\\0&0&0&0\\ \end{bmatrix}$ $\quad a1 \quad a2 \quad \,a3$ Here, we see $x_3$ is a free variable and thus we can say 3rd column,$\,a_3$, is in $\text{span}(a_1, a_2)$ But what does it mean for an echelon form of a matrix to have a row of $0$'s? Does that mean 3rd row can be generated by 1st & 2nd rows? just like 3rd column can be generated by 1st & 2nd columns? And this raises another question for me, why do we mostly focus on columns of a matrix? because I get the impression that ,for vectors and other concepts, our only concern is whether the columns span $\mathbb R^n$ or the columns are linearly independent and so on. I thought linear algebra is all about solving a system of linear equations, and linear equations are rows of a matrix, thus i think it'd be logical to focus more on rows than columns. But why?","I am having trouble understanding the relatioship between rows and columns of a matrix. Say, the following homogeneous system has a nontrivial solution. $$ 3x_1 + 5x_2 − 4x_3 = 0  \\ −3x_1 − 2x_2 + 4x_3 = 0  \\ 6x_1 + x_2 − 8x_3 = 0\\ $$ Let A be the coefficient matrix and row reduce  $\begin{bmatrix} A & \mathbf 0 \end{bmatrix}$  to row-echelon form: $\begin{bmatrix}3&5&-4&0\\-3&-2&4&0\\6&1&-8&0\\ \end{bmatrix} \rightarrow \begin{bmatrix}3&5&-4&0\\0&3&0&0\\0&0&0&0\\ \end{bmatrix}$ $\quad a1 \quad a2 \quad \,a3$ Here, we see $x_3$ is a free variable and thus we can say 3rd column,$\,a_3$, is in $\text{span}(a_1, a_2)$ But what does it mean for an echelon form of a matrix to have a row of $0$'s? Does that mean 3rd row can be generated by 1st & 2nd rows? just like 3rd column can be generated by 1st & 2nd columns? And this raises another question for me, why do we mostly focus on columns of a matrix? because I get the impression that ,for vectors and other concepts, our only concern is whether the columns span $\mathbb R^n$ or the columns are linearly independent and so on. I thought linear algebra is all about solving a system of linear equations, and linear equations are rows of a matrix, thus i think it'd be logical to focus more on rows than columns. But why?",,"['linear-algebra', 'matrices']"
3,Positive definiteness of Fubini-Study metric,Positive definiteness of Fubini-Study metric,,"Define the Fubini-Study metric $$g_{i\overline{j}} = \frac{\delta_{i\overline{j}}(1+|\boldsymbol{z}|^2)-\overline{z}^jz^i}{(1+|\boldsymbol{z}|^2)^2} $$ for $i,j=1,\ldots,n$ and $z_i$ complex variables and $|\boldsymbol{z}|^2=\sum_{i=1}^n |z^i|^2.$ My GOAL is to show that, for every $k=1,\ldots,n,$ $$ \det \left( g_{i\overline{j}} \right)_{1 \leq i,\overline{j}\leq k} =  \frac{1+\sum_{i=k+1}^n |z^i|^2}{(1+|\boldsymbol{z}|^2)^{k+1}}.$$","Define the Fubini-Study metric $$g_{i\overline{j}} = \frac{\delta_{i\overline{j}}(1+|\boldsymbol{z}|^2)-\overline{z}^jz^i}{(1+|\boldsymbol{z}|^2)^2} $$ for $i,j=1,\ldots,n$ and $z_i$ complex variables and $|\boldsymbol{z}|^2=\sum_{i=1}^n |z^i|^2.$ My GOAL is to show that, for every $k=1,\ldots,n,$ $$ \det \left( g_{i\overline{j}} \right)_{1 \leq i,\overline{j}\leq k} =  \frac{1+\sum_{i=k+1}^n |z^i|^2}{(1+|\boldsymbol{z}|^2)^{k+1}}.$$",,"['linear-algebra', 'differential-geometry', 'riemannian-geometry']"
4,What can we say about $(I-AD)^{-1}$ if $D$ is a diagonal matrix?,What can we say about  if  is a diagonal matrix?,(I-AD)^{-1} D,"Assume we know that square  matrices $A$ and $(I-AD)^{-1}$ are invertible and also $D$ is a diagonal matrix. Also assume that $A$ is a symmetric matrix. My question is when we can express $(I-AD)^{-1}$ as a function of $A, D, A^{-1}, D^{-1}, (I-A)^{-1}$ (without terms $(A-D)^{-1}$ or $(A^{-1}-D)^{-1}$) ? For example when matrix A is rank 1, then we have: $(I-AD)^{-1}=I+\frac{1}{1-tr(AD)} AD$. As you can see if A is rank 1 then we can do this easily. The only related paper I found is a paper by Kenneth S. Miller, but it is not useful for higher rank matrices. I know it might be very hard for general matrix $A$ but can it be done for special cases where for instance matrix A is positive semidefinite? Any comment is highly appreciated.","Assume we know that square  matrices $A$ and $(I-AD)^{-1}$ are invertible and also $D$ is a diagonal matrix. Also assume that $A$ is a symmetric matrix. My question is when we can express $(I-AD)^{-1}$ as a function of $A, D, A^{-1}, D^{-1}, (I-A)^{-1}$ (without terms $(A-D)^{-1}$ or $(A^{-1}-D)^{-1}$) ? For example when matrix A is rank 1, then we have: $(I-AD)^{-1}=I+\frac{1}{1-tr(AD)} AD$. As you can see if A is rank 1 then we can do this easily. The only related paper I found is a paper by Kenneth S. Miller, but it is not useful for higher rank matrices. I know it might be very hard for general matrix $A$ but can it be done for special cases where for instance matrix A is positive semidefinite? Any comment is highly appreciated.",,"['linear-algebra', 'matrices']"
5,How do I compute the eigenfunctions of the Fourier Transform?,How do I compute the eigenfunctions of the Fourier Transform?,,"In Andy's answer to the question "" What are fixed points of the Fourier Transform "" on Math Overflow, he shows that the Fourier Transform has eigenvalues $\{+1, +i, -1, -i \}$ and that the projections of any function onto the corresponding four eigenspaces may be found through some simple linear algebra. I would like to get a better feeling for these four eigenspaces of the fourier transform. How can I find some interesting members of each of these eigenspaces? How can I show that Hermite-Gaussians are in one (or more?) of the eigenspaces? How can one define usable projection operators onto these eigenspaces? The wikipedia article on the Fourier Transform mentions that Wiener defined the Fourier Transform via these projections.  What exactly was Wiener's approach?","In Andy's answer to the question "" What are fixed points of the Fourier Transform "" on Math Overflow, he shows that the Fourier Transform has eigenvalues $\{+1, +i, -1, -i \}$ and that the projections of any function onto the corresponding four eigenspaces may be found through some simple linear algebra. I would like to get a better feeling for these four eigenspaces of the fourier transform. How can I find some interesting members of each of these eigenspaces? How can I show that Hermite-Gaussians are in one (or more?) of the eigenspaces? How can one define usable projection operators onto these eigenspaces? The wikipedia article on the Fourier Transform mentions that Wiener defined the Fourier Transform via these projections.  What exactly was Wiener's approach?",,"['linear-algebra', 'fourier-analysis']"
6,Dimension of subspace defined by $a^2+b^2=c^2+d^2 \Rightarrow f(a)+f(b)=f(c)+f(d)$,Dimension of subspace defined by,a^2+b^2=c^2+d^2 \Rightarrow f(a)+f(b)=f(c)+f(d),"Let $V={\mathbb Q}^{\mathbb N}$ be the space of all functions ${\mathbb N}\to {\mathbb Q}$ where ${\mathbb N}=\lbrace 1,2,3,\ldots \rbrace$ is the set of all positive integers. Let $W$ be the subspace of $W$ defined by $$ W=\lbrace f\in V \ | \ f(a)+f(b)=f(c)+f(d) \ \textrm{if} \ a^2+b^2=c^2+d^2, \ a,b,c,d \in {\mathbb N}\rbrace $$ My question : What is the dimension of $W$ ? What I did : I have shown that $\dim(W) \leq 6$ ; in fact, any $f(n)(n\in{\mathbb N})$ is a linear combination of $f(1),f(2),\ldots,f(6)$ . Indeed, the identities $1^2+7^2=5^2+5^2$ , $1^2+8^2=4^2+7^2$ , $2^2+9^2=6^2+7^2$ , $3^2+11^2=7^2+9^2$ , $5^2+10^2=2^2+11^2$ , yield succesively that this property is true for $7,8,9,11$ , and $10$ . Next, it is a simple enough arithmetical lemma that for $n\geq 12$ , there are $a,b,c$ with $1\leq a,b,c \lt n$ such that $n^2+a^2=b^2+c^2$ (this is shown in different ways in different answers of an older MSE question ), so that we are then done. Numerical experiments suggest that the dimension is exactly $6$ . UPDATE 10/04/2020 : The dimension of $W$ is at least four because $W$ contains the constants, the square function, the characteristic function of the numbers divisible by $4$ ( $f(n)=1$ if $n$ is divisible by 4, and $f(n)=0$ otherwise), and the characteristic function of the odd numbers. Here's what the beginning of the sequence corresponding to $f(1)=-1$ , $f(k)=0$ for $2\leq k \leq 6$ looks like : $-1, 0, 0, 0, 0, 0, 1, 2, 1, 2, 2, 4, 4, 4, 5, 6, 7, 8, 8, 10, 10, 12, 13, 14, 15, 16, 18, 20, 20, 22, 23, 26, 27, 28, 30, 32, 34, 36, 37, 40, 41, 44, 46, 48, 50, 52, 55, 58, 59, 62, 64, 68, 70, 72, 75, 78, 81, 84, 86, 90, 92, 96, 99, 102, 105, 108, 112, 116$ The sequence seems to be nondecreasing (from the ninth term on)  and nonnegative (except for the first term).","Let be the space of all functions where is the set of all positive integers. Let be the subspace of defined by My question : What is the dimension of ? What I did : I have shown that ; in fact, any is a linear combination of . Indeed, the identities , , , , , yield succesively that this property is true for , and . Next, it is a simple enough arithmetical lemma that for , there are with such that (this is shown in different ways in different answers of an older MSE question ), so that we are then done. Numerical experiments suggest that the dimension is exactly . UPDATE 10/04/2020 : The dimension of is at least four because contains the constants, the square function, the characteristic function of the numbers divisible by ( if is divisible by 4, and otherwise), and the characteristic function of the odd numbers. Here's what the beginning of the sequence corresponding to , for looks like : The sequence seems to be nondecreasing (from the ninth term on)  and nonnegative (except for the first term).","V={\mathbb Q}^{\mathbb N} {\mathbb N}\to {\mathbb Q} {\mathbb N}=\lbrace 1,2,3,\ldots \rbrace W W 
W=\lbrace f\in V \ | \ f(a)+f(b)=f(c)+f(d) \ \textrm{if} \ a^2+b^2=c^2+d^2, \ a,b,c,d \in {\mathbb N}\rbrace
 W \dim(W) \leq 6 f(n)(n\in{\mathbb N}) f(1),f(2),\ldots,f(6) 1^2+7^2=5^2+5^2 1^2+8^2=4^2+7^2 2^2+9^2=6^2+7^2 3^2+11^2=7^2+9^2 5^2+10^2=2^2+11^2 7,8,9,11 10 n\geq 12 a,b,c 1\leq a,b,c \lt n n^2+a^2=b^2+c^2 6 W W 4 f(n)=1 n f(n)=0 f(1)=-1 f(k)=0 2\leq k \leq 6 -1, 0, 0, 0, 0, 0, 1, 2, 1, 2, 2, 4, 4, 4, 5, 6, 7, 8, 8, 10, 10, 12, 13, 14, 15, 16, 18, 20, 20, 22, 23, 26, 27, 28, 30, 32, 34, 36, 37, 40, 41, 44, 46, 48, 50, 52, 55, 58, 59, 62, 64, 68, 70, 72, 75, 78, 81, 84, 86, 90, 92, 96, 99, 102, 105, 108, 112, 116","['linear-algebra', 'arithmetic', 'functional-equations']"
7,What did Emil Artin mean when he said this?,What did Emil Artin mean when he said this?,,"What does Emil Artin mean when he says: It is my experience that proofs involving matrices can be shortened by 50% if one throws the matrices out. I mean I do understand that matrices are really just Linear Transformations in a vector space and this also makes for cool visualizations associated with all of Linear Algebra. But for the sake of performing manipulations and thinking analytically about Linear Algebra, isn't it essential to have Matrices. If we throw them out, what else can take its place?","What does Emil Artin mean when he says: It is my experience that proofs involving matrices can be shortened by 50% if one throws the matrices out. I mean I do understand that matrices are really just Linear Transformations in a vector space and this also makes for cool visualizations associated with all of Linear Algebra. But for the sake of performing manipulations and thinking analytically about Linear Algebra, isn't it essential to have Matrices. If we throw them out, what else can take its place?",,"['linear-algebra', 'matrices', 'soft-question', 'notation']"
8,Jacobian matrix vs. Transformation matrix,Jacobian matrix vs. Transformation matrix,,"Given is a coordinate system $\{x_1,x_2,...,x_n\}$ and another, second coordinate system $\{y_1,y_2,...,y_n\}$, where $x_1=x_1(y_1,y_2,...,y_n)$ $x_2=x_2(y_1,y_2,...,y_n)$ ... $x_n=x_n(y_1,y_2,...,y_n)$ Then the Jacobian matrix is $${\mathbf J}=\begin{pmatrix} \frac{\partial x_1}{\partial y_1} &  \frac{\partial x_1}{\partial y_2} & ... & \frac{\partial x_1}{\partial y_n}\\ \frac{\partial x_2}{\partial y_1} &  \frac{\partial x_2}{\partial y_2} & ... & \frac{\partial x_2}{\partial y_n}\\ ... \\ \frac{\partial x_n}{\partial y_1} &  \frac{\partial x_n}{\partial y_2} & ... & \frac{\partial x_n}{\partial y_n}\\ \end{pmatrix}$$ Now, consider vector $\vec{u}$. Its coordinates in $\{x_k\}$ are $\vec{u}=(u_{x1},u_{x2},...,u_{xn})$, while its coordinates in $\{y_k\}$ are  $\vec{u}=(u_{y1},u_{y2},...,u_{yn})$, where $$\begin{pmatrix} u_{x1}\\ u_{x2}\\ ... \\ u_{xn}\\ \end{pmatrix}= {\mathbf A} \cdot  \begin{pmatrix} u_{y1}\\ u_{y2}\\ ... \\ u_{yn}\\ \end{pmatrix}$$ What is the difference between the Jacobian matrix ${\mathbf J}$ and the Transformation matrix ${\mathbf A}$? How are they related? Please, write the expression that connects them. ------------EXAMPLE----------- Cartesian and cylindrical coordinates are related via $x=r\cos\theta$ $y=r\sin\theta$ $z=z$ Then the Jacobian is $${\mathbf J}=\begin{pmatrix} \frac{\partial x}{\partial r} &  \frac{\partial x}{\partial \theta} &  \frac{\partial x}{\partial z}\\ \frac{\partial y}{\partial r} &  \frac{\partial y}{\partial \theta} & \frac{\partial y}{\partial z}\\ \frac{\partial z}{\partial r} &  \frac{\partial z}{\partial \theta} &  \frac{\partial z}{\partial z}\\ \end{pmatrix}= \begin{pmatrix} \cos\theta &  -r\sin\theta &  0\\ \sin\theta &  r\cos\theta & 0\\ 0 &  0 &  1\\ \end{pmatrix}$$ and $\det({\mathbf J})=r$. Because ${\mathbf J}$ is orthogonal when $r=1$, ${\mathbf J}^{-1}={\mathbf J}^{T}=\begin{pmatrix} \cos\theta &  \sin\theta &  0\\ -\sin\theta &  \cos\theta & 0\\ 0 &  0 &  1\\ \end{pmatrix}$ The Cartesian basis vectors are $\hat{\mathbf i} = \begin{pmatrix} 1\\ 0\\ 0\\ \end{pmatrix}$;  $\hat{\mathbf j} = \begin{pmatrix} 0\\ 1\\ 0\\ \end{pmatrix}$;  $\hat{\mathbf k} = \begin{pmatrix} 0\\ 0\\ 1\\ \end{pmatrix}$; The cylindrical basis vectors are $\hat{\mathbf r}= \begin{pmatrix} \cos\theta\\ \sin\theta\\ 0\\ \end{pmatrix}$;  $\hat{\mathbf \theta}= \begin{pmatrix} -\sin\theta\\ \cos\theta\\ 0\\ \end{pmatrix}$;  $\hat{\mathbf z} = \begin{pmatrix} 0\\ 0\\ 1\\ \end{pmatrix}$. It seems that $\hat{\mathbf r}={\mathbf A}\cdot \hat{\mathbf i}$ $\hat{\mathbf \theta}={\mathbf A}\cdot \hat{\mathbf j}$ $\hat{\mathbf z}={\mathbf A}\cdot \hat{\mathbf k}$ if the transformation matrix ${\mathbf A}$ is given by ${\mathbf A}=\frac{1}{\det({\mathbf J})}{\mathbf J}^{-1}$. Is this conclusion true? Is this the relationship between ${\mathbf A}$ and ${\mathbf J}$?","Given is a coordinate system $\{x_1,x_2,...,x_n\}$ and another, second coordinate system $\{y_1,y_2,...,y_n\}$, where $x_1=x_1(y_1,y_2,...,y_n)$ $x_2=x_2(y_1,y_2,...,y_n)$ ... $x_n=x_n(y_1,y_2,...,y_n)$ Then the Jacobian matrix is $${\mathbf J}=\begin{pmatrix} \frac{\partial x_1}{\partial y_1} &  \frac{\partial x_1}{\partial y_2} & ... & \frac{\partial x_1}{\partial y_n}\\ \frac{\partial x_2}{\partial y_1} &  \frac{\partial x_2}{\partial y_2} & ... & \frac{\partial x_2}{\partial y_n}\\ ... \\ \frac{\partial x_n}{\partial y_1} &  \frac{\partial x_n}{\partial y_2} & ... & \frac{\partial x_n}{\partial y_n}\\ \end{pmatrix}$$ Now, consider vector $\vec{u}$. Its coordinates in $\{x_k\}$ are $\vec{u}=(u_{x1},u_{x2},...,u_{xn})$, while its coordinates in $\{y_k\}$ are  $\vec{u}=(u_{y1},u_{y2},...,u_{yn})$, where $$\begin{pmatrix} u_{x1}\\ u_{x2}\\ ... \\ u_{xn}\\ \end{pmatrix}= {\mathbf A} \cdot  \begin{pmatrix} u_{y1}\\ u_{y2}\\ ... \\ u_{yn}\\ \end{pmatrix}$$ What is the difference between the Jacobian matrix ${\mathbf J}$ and the Transformation matrix ${\mathbf A}$? How are they related? Please, write the expression that connects them. ------------EXAMPLE----------- Cartesian and cylindrical coordinates are related via $x=r\cos\theta$ $y=r\sin\theta$ $z=z$ Then the Jacobian is $${\mathbf J}=\begin{pmatrix} \frac{\partial x}{\partial r} &  \frac{\partial x}{\partial \theta} &  \frac{\partial x}{\partial z}\\ \frac{\partial y}{\partial r} &  \frac{\partial y}{\partial \theta} & \frac{\partial y}{\partial z}\\ \frac{\partial z}{\partial r} &  \frac{\partial z}{\partial \theta} &  \frac{\partial z}{\partial z}\\ \end{pmatrix}= \begin{pmatrix} \cos\theta &  -r\sin\theta &  0\\ \sin\theta &  r\cos\theta & 0\\ 0 &  0 &  1\\ \end{pmatrix}$$ and $\det({\mathbf J})=r$. Because ${\mathbf J}$ is orthogonal when $r=1$, ${\mathbf J}^{-1}={\mathbf J}^{T}=\begin{pmatrix} \cos\theta &  \sin\theta &  0\\ -\sin\theta &  \cos\theta & 0\\ 0 &  0 &  1\\ \end{pmatrix}$ The Cartesian basis vectors are $\hat{\mathbf i} = \begin{pmatrix} 1\\ 0\\ 0\\ \end{pmatrix}$;  $\hat{\mathbf j} = \begin{pmatrix} 0\\ 1\\ 0\\ \end{pmatrix}$;  $\hat{\mathbf k} = \begin{pmatrix} 0\\ 0\\ 1\\ \end{pmatrix}$; The cylindrical basis vectors are $\hat{\mathbf r}= \begin{pmatrix} \cos\theta\\ \sin\theta\\ 0\\ \end{pmatrix}$;  $\hat{\mathbf \theta}= \begin{pmatrix} -\sin\theta\\ \cos\theta\\ 0\\ \end{pmatrix}$;  $\hat{\mathbf z} = \begin{pmatrix} 0\\ 0\\ 1\\ \end{pmatrix}$. It seems that $\hat{\mathbf r}={\mathbf A}\cdot \hat{\mathbf i}$ $\hat{\mathbf \theta}={\mathbf A}\cdot \hat{\mathbf j}$ $\hat{\mathbf z}={\mathbf A}\cdot \hat{\mathbf k}$ if the transformation matrix ${\mathbf A}$ is given by ${\mathbf A}=\frac{1}{\det({\mathbf J})}{\mathbf J}^{-1}$. Is this conclusion true? Is this the relationship between ${\mathbf A}$ and ${\mathbf J}$?",,"['linear-algebra', 'multivariable-calculus', 'vectors', 'vector-analysis', 'coordinate-systems']"
9,Number of matrices on $\mathbb Z_{p}$ with a given characteristic polynomial,Number of matrices on  with a given characteristic polynomial,\mathbb Z_{p},"How can I find the number of n×n matrices on $\mathbb Z_{p}$ with a given characteristic polynomial? for example: If $p$ is a prime number s.t. $p \equiv 3 \pmod 4$, then number of $2\times 2$ matrices on $\mathbb Z_{p}$ that their characteristic polynomial is $x^2+1$ would be equal to $p(p-1)$. I don't have the proof of above example.","How can I find the number of n×n matrices on $\mathbb Z_{p}$ with a given characteristic polynomial? for example: If $p$ is a prime number s.t. $p \equiv 3 \pmod 4$, then number of $2\times 2$ matrices on $\mathbb Z_{p}$ that their characteristic polynomial is $x^2+1$ would be equal to $p(p-1)$. I don't have the proof of above example.",,['linear-algebra']
10,$A^2+B^2=AB$ and $BA-AB$ is non-singular [duplicate],and  is non-singular [duplicate],A^2+B^2=AB BA-AB,"This question already has answers here : Square matrices satisfying certain relations must have dimension divisible by $3$ (3 answers) Closed 3 years ago . The question is: Are there square matrices $A,B$ over $\mathbb{C}$ s.t. $A^2+B^2=AB$ and $BA-AB$ is non-singular? From $A^2+B^2=AB$ one could obtain $A^3+B^3=0$. Can we get something from this? Edit: $$A^2+B^2=AB\implies\\ A(A^2+B^2)=A^2B \implies\\ A^3+AB^2=A^2B \implies \\ A^3+(A^2+B^2)B=A^2B \implies \\  A^3+A^2B+B^3=A^2B \implies \\ A^3+B^3=0 $$","This question already has answers here : Square matrices satisfying certain relations must have dimension divisible by $3$ (3 answers) Closed 3 years ago . The question is: Are there square matrices $A,B$ over $\mathbb{C}$ s.t. $A^2+B^2=AB$ and $BA-AB$ is non-singular? From $A^2+B^2=AB$ one could obtain $A^3+B^3=0$. Can we get something from this? Edit: $$A^2+B^2=AB\implies\\ A(A^2+B^2)=A^2B \implies\\ A^3+AB^2=A^2B \implies \\ A^3+(A^2+B^2)B=A^2B \implies \\  A^3+A^2B+B^3=A^2B \implies \\ A^3+B^3=0 $$",,"['linear-algebra', 'matrices']"
11,Which matrices are conjugate to an integer valued matrix?,Which matrices are conjugate to an integer valued matrix?,,"If I have a matrix $A \in M_{n\times n}(\mathbb{C})$, when does there exist a change of basis $B \in Gl_n(\mathbb{C})$ so that $BAB^{-1} \in M_{n\times n}(\mathbb{Z})$? Case $n=1$ is obvious (in this case, $A$ is a number, so $A$ must be an integer). Case $n=2$ already seems impossible to do by brute force. I do know it's not trivial though. Is there a nice characterization of such matrices?","If I have a matrix $A \in M_{n\times n}(\mathbb{C})$, when does there exist a change of basis $B \in Gl_n(\mathbb{C})$ so that $BAB^{-1} \in M_{n\times n}(\mathbb{Z})$? Case $n=1$ is obvious (in this case, $A$ is a number, so $A$ must be an integer). Case $n=2$ already seems impossible to do by brute force. I do know it's not trivial though. Is there a nice characterization of such matrices?",,['linear-algebra']
12,Prove a $n \times n $ matrix has rank 3,Prove a  matrix has rank 3,n \times n ,"I have been examining a problem dealing with finding the rank of a  $n \times n $ matrix $M$ as follows: \begin{bmatrix}  0&1&4&9&16&\cdots &(n-1)^2\\ 1&0&1&4&9&\cdots&(n-2)^2\\ 4&1&0&1&4&\cdots&(n-3)^2\\ 9&4&1&0&1&\cdots&(n-4)^2\\ 16&9&4&1&0&\cdots&(n-5)^2\\ \vdots&\vdots&\vdots&\vdots&\vdots&\ddots&\vdots\\ (n-1)^2&(n-2)^2&\cdots&(n-4)^2&\cdots&\cdots&0 \end{bmatrix} That is, the matrix with $k^2$ on its $k^{th}$ super and subdiagonals. One of my colleagues claims the matrix is of rank 3.  I have tried to factor this matrix somehow, but have not really gotten very far. Can we develop a proof of the claim that the rank of $M$ is indeed 3 (assume that $n\geq3$).","I have been examining a problem dealing with finding the rank of a  $n \times n $ matrix $M$ as follows: \begin{bmatrix}  0&1&4&9&16&\cdots &(n-1)^2\\ 1&0&1&4&9&\cdots&(n-2)^2\\ 4&1&0&1&4&\cdots&(n-3)^2\\ 9&4&1&0&1&\cdots&(n-4)^2\\ 16&9&4&1&0&\cdots&(n-5)^2\\ \vdots&\vdots&\vdots&\vdots&\vdots&\ddots&\vdots\\ (n-1)^2&(n-2)^2&\cdots&(n-4)^2&\cdots&\cdots&0 \end{bmatrix} That is, the matrix with $k^2$ on its $k^{th}$ super and subdiagonals. One of my colleagues claims the matrix is of rank 3.  I have tried to factor this matrix somehow, but have not really gotten very far. Can we develop a proof of the claim that the rank of $M$ is indeed 3 (assume that $n\geq3$).",,"['linear-algebra', 'matrices']"
13,Cholesky decomposition in positive semi-definite matrix,Cholesky decomposition in positive semi-definite matrix,,"While trying to apply the algorithm described in the article: Robust adaptative metropolis algorithm with coerced acceptance rate (2011), Matti Vihola I used the a Cholesky decomposition to find $S_n$: $S_n S_n^T = S_{n-1} D_n S_{n-1}^T$ The article ensures that the (known) matrix in the right hand side will always be positive definite. However, after a lot of iterations, it seems that floating point imprecision is causing it to be only positive semi-definite. I would like to know if it is possible (and how?) to get $S_n$ using a robust Cholesky decomposition of a matrix with pivoting which works when standard Cholesky fails. The problem is when using it, the result is obviously not the $S_n$ matrix, but rather an $X_n$ of the form: $P^T X_n Y X_n^T P = S_{n-1} D_n S_{n-1}^T$ From what @J.M. has said here , it seems possible: The Cholesky decomposition might fail in floating point when given a   symmetric positive semidefinite matrix. However, one can modify   Cholesky to do symmetric pivoting so that the matrix is factored for   ""as long as the matrix seems positive definite"". You'll have to modify   your Kalman formula if you adopt this, though. So, it seems that I need to do some algebra here to get $S_n$, I just don't know how since I'm pretty n00b in such subject.","While trying to apply the algorithm described in the article: Robust adaptative metropolis algorithm with coerced acceptance rate (2011), Matti Vihola I used the a Cholesky decomposition to find $S_n$: $S_n S_n^T = S_{n-1} D_n S_{n-1}^T$ The article ensures that the (known) matrix in the right hand side will always be positive definite. However, after a lot of iterations, it seems that floating point imprecision is causing it to be only positive semi-definite. I would like to know if it is possible (and how?) to get $S_n$ using a robust Cholesky decomposition of a matrix with pivoting which works when standard Cholesky fails. The problem is when using it, the result is obviously not the $S_n$ matrix, but rather an $X_n$ of the form: $P^T X_n Y X_n^T P = S_{n-1} D_n S_{n-1}^T$ From what @J.M. has said here , it seems possible: The Cholesky decomposition might fail in floating point when given a   symmetric positive semidefinite matrix. However, one can modify   Cholesky to do symmetric pivoting so that the matrix is factored for   ""as long as the matrix seems positive definite"". You'll have to modify   your Kalman formula if you adopt this, though. So, it seems that I need to do some algebra here to get $S_n$, I just don't know how since I'm pretty n00b in such subject.",,"['linear-algebra', 'matrices', 'positive-semidefinite', 'cholesky-decomposition']"
14,How is $\mathrm{PGL}(V)$ a subgroup of $\mathrm{P\Gamma L}(V)$?,How is  a subgroup of ?,\mathrm{PGL}(V) \mathrm{P\Gamma L}(V),"I've stumbled upon a strange exercise while reading ""Notes on Infinite Permutation Groups"" by Bhattacharjee, Möller, Macpherson and Neumann. If you have the book, the exercise is 7(ix) on page 66. Let me start with the definitions. Let $F$ be an arbitrary field, and $V$ a finite-dimensional vector space over $F$. Definition 1. A linear transformation on $V$ is a mapping $\varphi :\  V \to V$ such that $$ (\lambda x +\mu y)\varphi = \lambda (x \varphi) + \mu (y \varphi) $$ for all $\lambda, \mu \in F$ and all $x, y \in V$. Note that we use notation suitable for right actions here, i.e. we write $x \varphi$ instead of the probably more usual $\varphi(x)$. Also, the composition of transformations is defined accordingly, i.e. $x(\varphi \circ \psi) = (x \varphi)\psi$. The general linear group $\mathrm{GL}(V)$ consists of all the invertible linear transformations on $V$. Taking the quotient by the centre, we obtain the projective linear group $\mathrm{PGL}(V) = \mathrm{GL}(V) / Z(\mathrm{GL}(V))$. Definition 2. A semilinear transformation on $V$ is a mapping $\varphi :\ V \to V$ such that there exists an automorphism $\sigma$ of $F$, for which $$ (\lambda x +\mu y)\varphi = (\lambda^\sigma) (x \varphi) + (\mu^\sigma) (y \varphi) $$ for all $\lambda, \mu \in F$ and all $x, y \in V$. Not surprisingly, the semilinear group $\mathrm{\Gamma L}(V)$ consists of all the invertible semilinear transformations on $V$, and its quotient by the centre is the projective semilinear group $\mathrm{P\Gamma L}(V) = \mathrm{\Gamma L}(V) / Z(\mathrm{\Gamma L}(V))$. Here is the exercise that gives me trouble: Show that the group $\mathrm{GL}(V)$ is normal in $\mathrm{\Gamma L}(V)$, and that $\mathrm{PGL}(V)$ is normal in $\mathrm{P\Gamma L}(V)$. Work done. I don't have any trouble proving that $\mathrm{GL}(V) \lhd \mathrm{\Gamma L}(V)$, it follows straight from the definitions above. But I'm confused by the second part of the question, the one about projective groups. This is what I don't understand: how can $\mathrm{PGL}(V)$ be a normal subgroup of $\mathrm{P\Gamma L}(V)$ if it is not its subgroup in the first place? I don't see a natural way to build an injection from $\mathrm{PGL}(V)$ to $\mathrm{P\Gamma L}(V)$. It would be easy if this inclusion were true: $Z(\mathrm{GL}(V)) \leq Z(\mathrm{\Gamma L}(V))$. Then I would easily build a map $\alpha: \mathrm{PGL}(V) \to \mathrm{P\Gamma L}(V)$ that would make the diagram commute: $$ \newcommand{\ra}[1]{\kern-1.5ex\xrightarrow{\ \ #1\ \ }\phantom{}\kern-1.5ex} \newcommand{\ras}[1]{\kern-1.5ex\xrightarrow{\ \ \smash{#1}\ \ }\phantom{}\kern-1.5ex} \newcommand{\da}[1]{\bigg\downarrow\raise.5ex\rlap{\scriptstyle#1}} \begin{array}{c} \mathrm{GL}(V) & \ra{i} & \mathrm{\Gamma L}(V) \\ \da{ } & & \da{ } \\ \mathrm{PGL}(V) & \ra{\alpha} & \mathrm{P \Gamma L}(V) \end{array} $$ where $i$ is inclusion and vertical arrows are factorization. The problem is, $Z(\mathrm{GL}(V)) \not\leq Z(\mathrm{\Gamma L}(V))$ in the general case. To see this, let $V=F$ be a one-dimensional space. Let $\sigma$ be a non-trivial automorphism of the field $F$. Note that $\sigma$ is automatically a semilinear transformation of $V$. Now take $\varphi:\ V \to V$ to be multiplication by $\lambda \in F$, where $\lambda^\sigma \neq \lambda$. Then $\varphi$ commutes with every linear transformation on $V$, but it does not commute with the semilinear $\sigma$. So $\varphi$ belongs to $Z(\mathrm{GL}(V))$, but not to $Z(\mathrm{\Gamma L}(V))$. Is my approach naive? Where should the map $\mathrm{PGL}(V) \to \mathrm{P\Gamma L}(V)$ come from?","I've stumbled upon a strange exercise while reading ""Notes on Infinite Permutation Groups"" by Bhattacharjee, Möller, Macpherson and Neumann. If you have the book, the exercise is 7(ix) on page 66. Let me start with the definitions. Let $F$ be an arbitrary field, and $V$ a finite-dimensional vector space over $F$. Definition 1. A linear transformation on $V$ is a mapping $\varphi :\  V \to V$ such that $$ (\lambda x +\mu y)\varphi = \lambda (x \varphi) + \mu (y \varphi) $$ for all $\lambda, \mu \in F$ and all $x, y \in V$. Note that we use notation suitable for right actions here, i.e. we write $x \varphi$ instead of the probably more usual $\varphi(x)$. Also, the composition of transformations is defined accordingly, i.e. $x(\varphi \circ \psi) = (x \varphi)\psi$. The general linear group $\mathrm{GL}(V)$ consists of all the invertible linear transformations on $V$. Taking the quotient by the centre, we obtain the projective linear group $\mathrm{PGL}(V) = \mathrm{GL}(V) / Z(\mathrm{GL}(V))$. Definition 2. A semilinear transformation on $V$ is a mapping $\varphi :\ V \to V$ such that there exists an automorphism $\sigma$ of $F$, for which $$ (\lambda x +\mu y)\varphi = (\lambda^\sigma) (x \varphi) + (\mu^\sigma) (y \varphi) $$ for all $\lambda, \mu \in F$ and all $x, y \in V$. Not surprisingly, the semilinear group $\mathrm{\Gamma L}(V)$ consists of all the invertible semilinear transformations on $V$, and its quotient by the centre is the projective semilinear group $\mathrm{P\Gamma L}(V) = \mathrm{\Gamma L}(V) / Z(\mathrm{\Gamma L}(V))$. Here is the exercise that gives me trouble: Show that the group $\mathrm{GL}(V)$ is normal in $\mathrm{\Gamma L}(V)$, and that $\mathrm{PGL}(V)$ is normal in $\mathrm{P\Gamma L}(V)$. Work done. I don't have any trouble proving that $\mathrm{GL}(V) \lhd \mathrm{\Gamma L}(V)$, it follows straight from the definitions above. But I'm confused by the second part of the question, the one about projective groups. This is what I don't understand: how can $\mathrm{PGL}(V)$ be a normal subgroup of $\mathrm{P\Gamma L}(V)$ if it is not its subgroup in the first place? I don't see a natural way to build an injection from $\mathrm{PGL}(V)$ to $\mathrm{P\Gamma L}(V)$. It would be easy if this inclusion were true: $Z(\mathrm{GL}(V)) \leq Z(\mathrm{\Gamma L}(V))$. Then I would easily build a map $\alpha: \mathrm{PGL}(V) \to \mathrm{P\Gamma L}(V)$ that would make the diagram commute: $$ \newcommand{\ra}[1]{\kern-1.5ex\xrightarrow{\ \ #1\ \ }\phantom{}\kern-1.5ex} \newcommand{\ras}[1]{\kern-1.5ex\xrightarrow{\ \ \smash{#1}\ \ }\phantom{}\kern-1.5ex} \newcommand{\da}[1]{\bigg\downarrow\raise.5ex\rlap{\scriptstyle#1}} \begin{array}{c} \mathrm{GL}(V) & \ra{i} & \mathrm{\Gamma L}(V) \\ \da{ } & & \da{ } \\ \mathrm{PGL}(V) & \ra{\alpha} & \mathrm{P \Gamma L}(V) \end{array} $$ where $i$ is inclusion and vertical arrows are factorization. The problem is, $Z(\mathrm{GL}(V)) \not\leq Z(\mathrm{\Gamma L}(V))$ in the general case. To see this, let $V=F$ be a one-dimensional space. Let $\sigma$ be a non-trivial automorphism of the field $F$. Note that $\sigma$ is automatically a semilinear transformation of $V$. Now take $\varphi:\ V \to V$ to be multiplication by $\lambda \in F$, where $\lambda^\sigma \neq \lambda$. Then $\varphi$ commutes with every linear transformation on $V$, but it does not commute with the semilinear $\sigma$. So $\varphi$ belongs to $Z(\mathrm{GL}(V))$, but not to $Z(\mathrm{\Gamma L}(V))$. Is my approach naive? Where should the map $\mathrm{PGL}(V) \to \mathrm{P\Gamma L}(V)$ come from?",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'permutations', 'linear-groups']"
15,Why composite transformations are multiplied to the right side?,Why composite transformations are multiplied to the right side?,,"I have seen that many composite transformations have the later transformation multiplied to the right side of the matrix. Say I have matrix an existing transformation matrix $\mathbf{M}$ and then other transformation matrices $S$ for scaling, $T$ for translation, $R$ for rotation. I want to do a rotation first then translation then scaling, I have to do this: $\mathbf{M}(R)(T)(S)=\mathbf{M'}$ . Now the final matrix $\mathbf{M'}$ has the composite transformations in the sequence that I could use it to transform vectors with $\mathbf{M'}\vec{x}=\vec{x'}$ . But the question is why are transformations multiplied to the right side of the matrices and how can I prove that it has to be multiplied to the right side? What would the meaning be if I had multiplied them to the left side instead as $(S)(T)(R)\mathbf{M}=\mathbf{M'}$ ? Sometimes, I have this tendency to multiply to the left because in row elementary operations, the matrices are multiplied to the left.","I have seen that many composite transformations have the later transformation multiplied to the right side of the matrix. Say I have matrix an existing transformation matrix and then other transformation matrices for scaling, for translation, for rotation. I want to do a rotation first then translation then scaling, I have to do this: . Now the final matrix has the composite transformations in the sequence that I could use it to transform vectors with . But the question is why are transformations multiplied to the right side of the matrices and how can I prove that it has to be multiplied to the right side? What would the meaning be if I had multiplied them to the left side instead as ? Sometimes, I have this tendency to multiply to the left because in row elementary operations, the matrices are multiplied to the left.",\mathbf{M} S T R \mathbf{M}(R)(T)(S)=\mathbf{M'} \mathbf{M'} \mathbf{M'}\vec{x}=\vec{x'} (S)(T)(R)\mathbf{M}=\mathbf{M'},"['linear-algebra', 'matrices', 'transformation']"
16,Multiplication of block matrices,Multiplication of block matrices,,My textbook says that multiplying block matrices works the same as regular matrix multiplication (granted the dimensions of the submatrices are appropriate). Wikipedia also has an example saying so. It seems like proving it is purely technical and yet I'm having trouble putting it into words. What would be a good way to go?,My textbook says that multiplying block matrices works the same as regular matrix multiplication (granted the dimensions of the submatrices are appropriate). Wikipedia also has an example saying so. It seems like proving it is purely technical and yet I'm having trouble putting it into words. What would be a good way to go?,,"['linear-algebra', 'matrices', 'block-matrices']"
17,Is this matrix positive semidefinite? $M_{ij} = \sqrt{|x_i+x_j|} - \sqrt{|x_i-x_j|}$ where $x_i$'s are reals,Is this matrix positive semidefinite?  where 's are reals,M_{ij} = \sqrt{|x_i+x_j|} - \sqrt{|x_i-x_j|} x_i,"Let finite number of $x_i$ 's be reals. Define matrix $M_{ij} = \sqrt{|x_i+x_j|} - \sqrt{|x_i-x_j|}$ . Is this matrix positive semidefinite? I am reading this year's IMO problem number 2, which would be trivially true if we prove that $M_{ij}$ is positive semidefinite. Problem $\boldsymbol2$ . Show that the inequality $$\sum_{i=1}^{n}\sum_{j=1}^{n}\sqrt{\left|x_i - x_j\right|} \leqslant \sum_{i=1}^{n}\sum_{j=1}^n\sqrt{\left|x_i + x_j\right|}$$ holds for all real numbers $x_1,...,x_n$ .","Let finite number of 's be reals. Define matrix . Is this matrix positive semidefinite? I am reading this year's IMO problem number 2, which would be trivially true if we prove that is positive semidefinite. Problem . Show that the inequality holds for all real numbers .","x_i M_{ij} = \sqrt{|x_i+x_j|} - \sqrt{|x_i-x_j|} M_{ij} \boldsymbol2 \sum_{i=1}^{n}\sum_{j=1}^{n}\sqrt{\left|x_i - x_j\right|} \leqslant \sum_{i=1}^{n}\sum_{j=1}^n\sqrt{\left|x_i + x_j\right|} x_1,...,x_n","['linear-algebra', 'matrices', 'inequality', 'positive-semidefinite']"
18,Maximizing trace of mixed products of two real symmetric matrices,Maximizing trace of mixed products of two real symmetric matrices,,"Let $A$ , $B$ be two $N \times N$ real symmetric matrices whose entries i.i.d.r.v. from a mean 0, variance 1 distribution. Let $I, J$ be even positive integers, and let $i_k, j_k$ for $k = 1,\ldots,n$ be arbitrary finite sequences of positive integers such that $\sum i_k = I$ and $\sum j_k = J$ . Is it true that $\text{Tr} (A^I B^J) \geq \text{Tr}\left( A^{i_1} B^{j_1} \cdots A^{i_n} B^{j_n} \right) $ ? I have run extensive numerical experiments in Mathematica on large (10K x 10K) random real symmetric matrices, and have been unable to find a counterexample; I am wondering if this might be established in a theoretical sense.","Let , be two real symmetric matrices whose entries i.i.d.r.v. from a mean 0, variance 1 distribution. Let be even positive integers, and let for be arbitrary finite sequences of positive integers such that and . Is it true that ? I have run extensive numerical experiments in Mathematica on large (10K x 10K) random real symmetric matrices, and have been unable to find a counterexample; I am wondering if this might be established in a theoretical sense.","A B N \times N I, J i_k, j_k k = 1,\ldots,n \sum i_k = I \sum j_k = J \text{Tr} (A^I B^J) \geq \text{Tr}\left( A^{i_1} B^{j_1} \cdots A^{i_n} B^{j_n} \right) ","['linear-algebra', 'functional-analysis', 'trace', 'symmetric-matrices', 'random-matrices']"
19,How many connected components for the intersection $S \cap GL_n(\mathbb R)$ where $S \subset M_n(\mathbb R)$ is a linear subspace?,How many connected components for the intersection  where  is a linear subspace?,S \cap GL_n(\mathbb R) S \subset M_n(\mathbb R),Let $S \subset M_n(\mathbb R^n)$ be a linear subspace. Is there a way to determine how many connected components there are for $S \cap GL_n(\mathbb R)$? Let us assume the intersection is nonempty. $GL_n(\mathbb R)$ has two connected components. Does this intersection have two connected components or possibly more?,Let $S \subset M_n(\mathbb R^n)$ be a linear subspace. Is there a way to determine how many connected components there are for $S \cap GL_n(\mathbb R)$? Let us assume the intersection is nonempty. $GL_n(\mathbb R)$ has two connected components. Does this intersection have two connected components or possibly more?,,"['linear-algebra', 'abstract-algebra', 'general-topology', 'connectedness']"
20,Is there a way to know if a row reduction of a matrix has been done correctly?,Is there a way to know if a row reduction of a matrix has been done correctly?,,"I'm an undergrad taking the class of ""Linear algebra 1"". I came across a problem: sometimes we need to apply Gaussian elimination for matrices. Very quickly this skill is not much necessary as it's not a thinking skill but purely Technic. Yet, often in exams there's a question that requires you to apply row reduction  to a matrix. I am looking for a way to know if the Gaussian elimination has been done properly, meaning - with no Calculation mistakes, other than going over all the steps and check that the Calculation has been done correctly. as this processes will double the time I will spend on a given question, and due to the lack of time in a big course exam - which is also very stressful - such method could be much helpful for me. note: we're allowed to use a simple scientific calculator (not a graph calculator)","I'm an undergrad taking the class of ""Linear algebra 1"". I came across a problem: sometimes we need to apply Gaussian elimination for matrices. Very quickly this skill is not much necessary as it's not a thinking skill but purely Technic. Yet, often in exams there's a question that requires you to apply row reduction  to a matrix. I am looking for a way to know if the Gaussian elimination has been done properly, meaning - with no Calculation mistakes, other than going over all the steps and check that the Calculation has been done correctly. as this processes will double the time I will spend on a given question, and due to the lack of time in a big course exam - which is also very stressful - such method could be much helpful for me. note: we're allowed to use a simple scientific calculator (not a graph calculator)",,"['linear-algebra', 'matrices', 'reference-request', 'soft-question', 'gaussian-elimination']"
21,Matrix version of Pythagoras theorem,Matrix version of Pythagoras theorem,,"Can I find a solution for $C_{n\times n}$, explicitly, for the given $A_{n\times n}$ and $B_{n\times n}$ such that $AA^{T} + BB^{T} = CC^{T}$? Here $A^{T}$ denotes the transpose of $A$ and all the matrices ($A$, $B$, and $C$) are real. I think there is a solution for $C$ as there are $n^2$ unknowns (the $n^2$ components $C$) and $n^2$ equations (comparing the components of LHS and RHS). Note: If it can not be solved (explicitly), then one can assume mild conditions (like positive definiteness, symmetry) on $A$ and $B$.","Can I find a solution for $C_{n\times n}$, explicitly, for the given $A_{n\times n}$ and $B_{n\times n}$ such that $AA^{T} + BB^{T} = CC^{T}$? Here $A^{T}$ denotes the transpose of $A$ and all the matrices ($A$, $B$, and $C$) are real. I think there is a solution for $C$ as there are $n^2$ unknowns (the $n^2$ components $C$) and $n^2$ equations (comparing the components of LHS and RHS). Note: If it can not be solved (explicitly), then one can assume mild conditions (like positive definiteness, symmetry) on $A$ and $B$.",,"['linear-algebra', 'matrices', 'matrix-equations', 'pythagorean-triples']"
22,Product rule for vector-valued functions,Product rule for vector-valued functions,,"I'm trying to wrap my head around how to apply the product rule for matrix-valued or vector-valued matrix functions. Specifically, I'm trying to work through how to apply the product rule to $$x^TAx = f(x)g(x)$$ where $f(x) = x^T$ , $g(x)=Ax$ , $x\in\mathbb{R}^N$ , and $A\in \mathbb{R}^{NxN}$ I know that $\nabla_x x^TAx = (A + A^T)x$ or $x^T(A + A^T)$ depending on the layout, however I'm just trying to use this as an example to see if I can get the same result with the product rule. This question explains it for scalar-valued functions as $$f(x)\nabla_x g(x)+g(x)\nabla_x f(x).$$ However things don't have the correct dimensions when I plug in the values in the above, namely. As Travis wrote in the comment below, we should have: $$ \nabla_x(x^TAx) = (\nabla_x x^T)Ax + x^T\nabla_x(Ax) $$ however that still leaves you with at least an $x$ in the first expression and an $x^T$ in the second. I don't see how that can conform and how it leaves you with $(A + A^T)x$ or $x^T(A + A^T)$ This question is essentially asking the same thing, but the answer doesn't really involve the product rule above. I figure there must be some general formula to apply, as with scalar-valued functions. Am I writing the product rule correctly in this case? Is there somethign I'm missing or doing incorrectly? EDIT: Building off of Algabraic Pavel's answer... I think the problem is that you have to formulate the functions $f(x)$ and $f(x)$ so their in the same space. That is, for $f,g:\mathbb{R}^N\rightarrow \mathbb{R}^M$ , the product rule is: $$\nabla_x (f(x)^Tg(x)) = f(x)^T\nabla_x g(x) + g(x)^T \nabla f(x)$$ So in the example above, if we let $f(x) = x$ , $g(x)=Ax$ , then the formula holds. As another example, consider $$Axx^T$$ and let $f(x) = x^T A^T$ and $g(x) = x^T$ . We have both $f,g:\mathbb{R}^{Nx1} \rightarrow \mathbb{R}^{1xN}$ and $$\nabla_x (f(x)^Tg(x)) = \nabla_x (Axx^T) = Ax + xA^T$$ which holds, notice that if we made $f(x) = Ax$ and not $f(x) = (Ax)^T$ , the rule falls apart. I still don't know if this holds in all instances though. Any counter examples?","I'm trying to wrap my head around how to apply the product rule for matrix-valued or vector-valued matrix functions. Specifically, I'm trying to work through how to apply the product rule to where , , , and I know that or depending on the layout, however I'm just trying to use this as an example to see if I can get the same result with the product rule. This question explains it for scalar-valued functions as However things don't have the correct dimensions when I plug in the values in the above, namely. As Travis wrote in the comment below, we should have: however that still leaves you with at least an in the first expression and an in the second. I don't see how that can conform and how it leaves you with or This question is essentially asking the same thing, but the answer doesn't really involve the product rule above. I figure there must be some general formula to apply, as with scalar-valued functions. Am I writing the product rule correctly in this case? Is there somethign I'm missing or doing incorrectly? EDIT: Building off of Algabraic Pavel's answer... I think the problem is that you have to formulate the functions and so their in the same space. That is, for , the product rule is: So in the example above, if we let , , then the formula holds. As another example, consider and let and . We have both and which holds, notice that if we made and not , the rule falls apart. I still don't know if this holds in all instances though. Any counter examples?","x^TAx = f(x)g(x) f(x) = x^T g(x)=Ax x\in\mathbb{R}^N A\in \mathbb{R}^{NxN} \nabla_x x^TAx = (A + A^T)x x^T(A + A^T) f(x)\nabla_x g(x)+g(x)\nabla_x f(x).  \nabla_x(x^TAx) = (\nabla_x x^T)Ax + x^T\nabla_x(Ax)  x x^T (A + A^T)x x^T(A + A^T) f(x) f(x) f,g:\mathbb{R}^N\rightarrow \mathbb{R}^M \nabla_x (f(x)^Tg(x)) = f(x)^T\nabla_x g(x) + g(x)^T \nabla f(x) f(x) = x g(x)=Ax Axx^T f(x) = x^T A^T g(x) = x^T f,g:\mathbb{R}^{Nx1} \rightarrow \mathbb{R}^{1xN} \nabla_x (f(x)^Tg(x)) = \nabla_x (Axx^T) = Ax + xA^T f(x) = Ax f(x) = (Ax)^T","['linear-algebra', 'matrices', 'multivariable-calculus', 'derivatives', 'matrix-calculus']"
23,Finding null space of matrix.,Finding null space of matrix.,,"I need to make sure I'm understanding this correctly. I skipped a few steps to reduce typing, but let me know if I need to clarify something. Question asks: Find $N(A)$ for $A$ = \begin{bmatrix}         -3 & 6 & -1 & 1 & -7 \\         1 & -2 & 2 & 3 & -1\\         2 & -4 & 5 & 8 & -4 \\         \end{bmatrix} First thing I did was put the augmented matrix into reduced echelon row: $\begin{bmatrix}         1 & -2 & 0 & -1 & 3 & 0 \\         0 & 0 & 1 & 2 & -2 & 0\\         0 & 0 & 0 & 0 & 0 & 0 \\         \end{bmatrix}$ $(1)$ So then... $x=\begin{bmatrix} x_1\\ x_2 \\ x_3\\ x_4\\ x_5\\ \end{bmatrix}  =  \begin{bmatrix} 2x_2 + x_4 - 3x_5\\ x_2 \\ -2x_4 + 2x_5\\ x_4\\ x_5\\ \end{bmatrix}$ $(2) $ Since $x_2, x_4$ and $x_5$ are free variables.. $ x_2 \begin{bmatrix} 2\\ 1 \\ 0\\ 0\\ 0\\ \end{bmatrix} + x_4 \begin{bmatrix} 1\\ 0 \\ -2\\ 1\\ 0\\ \end{bmatrix} + x_5 \begin{bmatrix} -3\\ 0 \\ 2\\ 0\\ 1\\ \end{bmatrix}$ $(3)$ Resulting in.. $N(A)= \left( \begin{bmatrix} 2\\ 1 \\ 0\\ 0\\ 0\\ \end{bmatrix} , \begin{bmatrix} 1\\ 0 \\ -2\\ 1\\ 0\\ \end{bmatrix} , \begin{bmatrix} -3\\ 0 \\ 2\\ 0\\ 1\\ \end{bmatrix} \right)$ $(4)$","I need to make sure I'm understanding this correctly. I skipped a few steps to reduce typing, but let me know if I need to clarify something. Question asks: Find for = First thing I did was put the augmented matrix into reduced echelon row: So then... Since and are free variables.. Resulting in..","N(A) A \begin{bmatrix}
        -3 & 6 & -1 & 1 & -7 \\
        1 & -2 & 2 & 3 & -1\\
        2 & -4 & 5 & 8 & -4 \\
        \end{bmatrix} \begin{bmatrix}
        1 & -2 & 0 & -1 & 3 & 0 \\
        0 & 0 & 1 & 2 & -2 & 0\\
        0 & 0 & 0 & 0 & 0 & 0 \\
        \end{bmatrix} (1) x=\begin{bmatrix}
x_1\\
x_2 \\
x_3\\
x_4\\
x_5\\
\end{bmatrix} 
= 
\begin{bmatrix}
2x_2 + x_4 - 3x_5\\
x_2 \\
-2x_4 + 2x_5\\
x_4\\
x_5\\
\end{bmatrix} (2)  x_2, x_4 x_5 
x_2
\begin{bmatrix}
2\\
1 \\
0\\
0\\
0\\
\end{bmatrix}
+
x_4
\begin{bmatrix}
1\\
0 \\
-2\\
1\\
0\\
\end{bmatrix}
+
x_5
\begin{bmatrix}
-3\\
0 \\
2\\
0\\
1\\
\end{bmatrix} (3) N(A)= \left(
\begin{bmatrix}
2\\
1 \\
0\\
0\\
0\\
\end{bmatrix}
,
\begin{bmatrix}
1\\
0 \\
-2\\
1\\
0\\
\end{bmatrix}
,
\begin{bmatrix}
-3\\
0 \\
2\\
0\\
1\\
\end{bmatrix}
\right) (4)",['linear-algebra']
24,Is there a formula for the expansion coefficients of powers of an inner product?,Is there a formula for the expansion coefficients of powers of an inner product?,,"I would like to expand the following expression $$\left(\sum_{i,j=1}^N \,x_i A_{ij} x_j\right)^n$$ where $\mathbf A$ is a symmetric $N\times N$ matrix, $\mathbf {x}$ is an $N$-component vector, and $n$ is a non-negative integer power.  The expansion of this expression yields a homogeneous polynomial of order $2n$ in the $x_k$. What is the coefficient of the term $x_1^{p_1} x_2^{p_2} \cdots x_N^{p_N}$ for $p_1 + p_2 + \cdots + p_N = 2n$ in the expansion of this expression? Has the formula been worked out before?","I would like to expand the following expression $$\left(\sum_{i,j=1}^N \,x_i A_{ij} x_j\right)^n$$ where $\mathbf A$ is a symmetric $N\times N$ matrix, $\mathbf {x}$ is an $N$-component vector, and $n$ is a non-negative integer power.  The expansion of this expression yields a homogeneous polynomial of order $2n$ in the $x_k$. What is the coefficient of the term $x_1^{p_1} x_2^{p_2} \cdots x_N^{p_N}$ for $p_1 + p_2 + \cdots + p_N = 2n$ in the expansion of this expression? Has the formula been worked out before?",,"['linear-algebra', 'polynomials', 'symmetric-matrices', 'multinomial-theorem']"
25,Left and right multiplying of matrices,Left and right multiplying of matrices,,I am new to matrix multiplication and trying to understand something. Suppose you have a matrix equation $ A x=b $. I know to solve for $x$ you should left multiply by the inverse of A. But what is the reason you can't solve for $b$ like this: $ A A^{-1} x=b A^{-1} $ so that  $ x=b A^{-1} $? I tried with an example to see that it doesn't work but I don't have a good understanding of the mechanics why not. What if you had something like this instead: $ A x = By $ could you solve like this? $ AA^{-1} x = BA^{-1}y $ and then get the solution  $  x = BA^{-1}y $ or do you have to solve like this $ A^{-1}A x = A^{-1}By $ to get the solution $  x = A^{-1}By $,I am new to matrix multiplication and trying to understand something. Suppose you have a matrix equation $ A x=b $. I know to solve for $x$ you should left multiply by the inverse of A. But what is the reason you can't solve for $b$ like this: $ A A^{-1} x=b A^{-1} $ so that  $ x=b A^{-1} $? I tried with an example to see that it doesn't work but I don't have a good understanding of the mechanics why not. What if you had something like this instead: $ A x = By $ could you solve like this? $ AA^{-1} x = BA^{-1}y $ and then get the solution  $  x = BA^{-1}y $ or do you have to solve like this $ A^{-1}A x = A^{-1}By $ to get the solution $  x = A^{-1}By $,,['linear-algebra']
26,Why we wonder to know all derivations of an algebra?,Why we wonder to know all derivations of an algebra?,,"It is well-known that the space of all derivations of algebra of smooth functions on a manifold is its space of sections (vector fields on underlying manifold). But, I wonder to know why finding the space of all algebra derivations is an important problem? What aspects of its underlying  algebra is this space capable to show ?","It is well-known that the space of all derivations of algebra of smooth functions on a manifold is its space of sections (vector fields on underlying manifold). But, I wonder to know why finding the space of all algebra derivations is an important problem? What aspects of its underlying  algebra is this space capable to show ?",,"['linear-algebra', 'differential-geometry']"
27,What is the intuition behind tensors?,What is the intuition behind tensors?,,Can someone please explain the intuition behind tensors? Like an example or something of the similar kind that I should keep in mind reading the theorems about it? I can't visualize it Thanks in advance!,Can someone please explain the intuition behind tensors? Like an example or something of the similar kind that I should keep in mind reading the theorems about it? I can't visualize it Thanks in advance!,,"['linear-algebra', 'tensors']"
28,Is it possible to define an inner product such that an arbitrary operator is self adjoint?,Is it possible to define an inner product such that an arbitrary operator is self adjoint?,,"Given a vector space $V$ (possibly infinite dimensional) with inner product $(.,.)$. We say an operator $A$ is self adjoint if $(Af,g)=(f,Ag)$. The definition as stated require us to start with an inner product $(.,.)$ in $V$ and check if the operator $A$ satisfies the equality. My question is: If we start with an operator $B$ on a vector space $W$ what are the necessary and sufficient conditions such that we can define an inner product such that $B$ is self adjoint with that inner product?","Given a vector space $V$ (possibly infinite dimensional) with inner product $(.,.)$. We say an operator $A$ is self adjoint if $(Af,g)=(f,Ag)$. The definition as stated require us to start with an inner product $(.,.)$ in $V$ and check if the operator $A$ satisfies the equality. My question is: If we start with an operator $B$ on a vector space $W$ what are the necessary and sufficient conditions such that we can define an inner product such that $B$ is self adjoint with that inner product?",,"['linear-algebra', 'functional-analysis', 'vector-spaces', 'inner-products', 'adjoint-operators']"
29,Geometry of the Cayley Transform,Geometry of the Cayley Transform,,"I'm trying to understand the geometry of the Cayley transform. Suppose I have a $3 \times 3$ rotation matrix $R$ (i.e an orthogonal matrix with determinant equal to $1$ ). Let's ignore the corner case where $-1$ is an eigenvalue of $R$ (in other words, we assume that the rotation angle is not $\pi$ ). Then, according to a result of Cayley , I can find a skew symmetric matrix $S$ such that $$ R = (I - S)(I + S)^{-1} $$ In other words, I can find two other transformations $A = I - S$ and $B= (I + S)^{-1}$ whose combined effect, when applied one after the other, is the same as the original rotation. My question is: Can we find some geometric interpretation of the transforms $S$ and $A$ and $B$ , so that we can see how they combine to produce a rotation. I know that a rotation can be written as a product of two reflections. Is that related to the Cayley decomposition $R = AB$ ? Are $A$ and $B$ reflections? The 3-dimensional case is the only one that's of interest to me. Edit: Some Progress I made some progress on the algebra, but not the geometry. Suppose our matrix $R$ corresponds to a rotation through an angle $\theta$ around the unit vector $\mathbf{n} = (u,v,w)$ . Let $t = \tan\tfrac12\theta$ . Then I managed to show that  the Cayley decomposition is given by $R = A \cdot B$ , where $$ S =  \left[ \begin{matrix}  0 & t w & -t v \\  -t w & 0 & t u \\  t v & -t u & 0   \end{matrix} \right] $$ $$ A = I - S = \left[ \begin{matrix}  1 & -t w & t v \\  t w & 1 & -t u \\  -t v & t u & 1   \end{matrix} \right] $$ $$ B = (I + S)^{-1} = \frac{1}{1+t^2} \left[ \begin{matrix}  t^2 u^2+1 & t (t u v-w) & t (v+t u w) \\  t (t u v+w) & t^2 v^2+1 & t (t v w-u) \\  t (t u w-v) & t (u+t v w) & t^2 w^2+1   \end{matrix} \right] $$ We have $\det(A) = 1+ t^2$ and $\det(B) = 1/(1+t^2)$ , so neither $A$ nor $B$ is a rotation or a reflection. I still don't see the geometry of $A$ and $B$ , though. That's the puzzle.","I'm trying to understand the geometry of the Cayley transform. Suppose I have a rotation matrix (i.e an orthogonal matrix with determinant equal to ). Let's ignore the corner case where is an eigenvalue of (in other words, we assume that the rotation angle is not ). Then, according to a result of Cayley , I can find a skew symmetric matrix such that In other words, I can find two other transformations and whose combined effect, when applied one after the other, is the same as the original rotation. My question is: Can we find some geometric interpretation of the transforms and and , so that we can see how they combine to produce a rotation. I know that a rotation can be written as a product of two reflections. Is that related to the Cayley decomposition ? Are and reflections? The 3-dimensional case is the only one that's of interest to me. Edit: Some Progress I made some progress on the algebra, but not the geometry. Suppose our matrix corresponds to a rotation through an angle around the unit vector . Let . Then I managed to show that  the Cayley decomposition is given by , where We have and , so neither nor is a rotation or a reflection. I still don't see the geometry of and , though. That's the puzzle.","3 \times 3 R 1 -1 R \pi S 
R = (I - S)(I + S)^{-1}
 A = I - S B= (I + S)^{-1} S A B R = AB A B R \theta \mathbf{n} = (u,v,w) t = \tan\tfrac12\theta R = A \cdot B 
S = 
\left[
\begin{matrix}
 0 & t w & -t v \\
 -t w & 0 & t u \\
 t v & -t u & 0  
\end{matrix}
\right]
 
A = I - S =
\left[
\begin{matrix}
 1 & -t w & t v \\
 t w & 1 & -t u \\
 -t v & t u & 1  
\end{matrix}
\right]
 
B = (I + S)^{-1} = \frac{1}{1+t^2}
\left[
\begin{matrix}
 t^2 u^2+1 & t (t u v-w) & t (v+t u w) \\
 t (t u v+w) & t^2 v^2+1 & t (t v w-u) \\
 t (t u w-v) & t (u+t v w) & t^2 w^2+1  
\end{matrix}
\right]
 \det(A) = 1+ t^2 \det(B) = 1/(1+t^2) A B A B","['linear-algebra', 'geometry', 'matrices', 'rotations']"
30,Linear combination of symmetric matrices,Linear combination of symmetric matrices,,"Let $A, B, C, D$ be four linearly independent symmetric 3 x 3-matrices over $\mathbb K$ . Show that some linear combination of these matrices is a matrix of rank 1. I know it is supposed to be a question in Algebraic Geometry, so it must be interesting :)","Let be four linearly independent symmetric 3 x 3-matrices over . Show that some linear combination of these matrices is a matrix of rank 1. I know it is supposed to be a question in Algebraic Geometry, so it must be interesting :)","A, B, C, D \mathbb K","['linear-algebra', 'matrices', 'algebraic-geometry']"
31,Dimension of the space of algebraic Riemann curvature tensors,Dimension of the space of algebraic Riemann curvature tensors,,"Given $n\in \mathbb N$, consider the vector space $\mathbb R^{n^4}$ whose elements I will denote by $(R_{abcd})$ with indices $a,b,c,d \in \{1, \dots, n\}$. This vector space is $n^4$-dimensional. The space of algebraic Riemann curvature tensors is the subspace $V$ of $\mathbb R^4$ consisting of all $(R_{abcd})$ satisfying the following symmetries: $R_{abcd} = - R_{bacd}$ $R_{abcd} = - R_{abdc}$ $R_{abcd} + R_{cabd} + R_{bcad} = 0$ How would one go about calculating $\dim V$? Some thoughts: The three symmetries above imply another symmetry. Namely $R_{abcd} = R_{cdab}$. Combining this with the first two symmetries, it follows that $R_{abcd}$ can be viewed as a symmetric matrix of antisymmetric matrices. Since the space of antisymmetric matrices is $\binom n2$-dimensional, it follows that the first two symmetries together with the new one will cut out a space of dimension $$\frac{\binom n2 \left(\binom n2 +1\right)}{2}.$$ We still need to take into account the third symmetry. It is not quite clear to me which of the resulting additional equations are independent of each other and the old ones. But I believe there should certainly be some dependencies... I noticed that if I only take into account those equations corresponding to values $a<b<c<d$ (of which there are $\binom n4$) and if I assume that these are all independent of each other, then I obtain that  $$\dim V = \frac{\binom n2 \left(\binom n2 +1\right)}{2} - \binom n4 = \frac{n^2(n^2-1)}{12}.$$ (modulo miscalculation). This seems to be the right answer. However, I'm neither sure whether these equations indeed are independent nor am I sure whether they form a maximal set of independent equations. I would appreciate your input! Thanks. =)","Given $n\in \mathbb N$, consider the vector space $\mathbb R^{n^4}$ whose elements I will denote by $(R_{abcd})$ with indices $a,b,c,d \in \{1, \dots, n\}$. This vector space is $n^4$-dimensional. The space of algebraic Riemann curvature tensors is the subspace $V$ of $\mathbb R^4$ consisting of all $(R_{abcd})$ satisfying the following symmetries: $R_{abcd} = - R_{bacd}$ $R_{abcd} = - R_{abdc}$ $R_{abcd} + R_{cabd} + R_{bcad} = 0$ How would one go about calculating $\dim V$? Some thoughts: The three symmetries above imply another symmetry. Namely $R_{abcd} = R_{cdab}$. Combining this with the first two symmetries, it follows that $R_{abcd}$ can be viewed as a symmetric matrix of antisymmetric matrices. Since the space of antisymmetric matrices is $\binom n2$-dimensional, it follows that the first two symmetries together with the new one will cut out a space of dimension $$\frac{\binom n2 \left(\binom n2 +1\right)}{2}.$$ We still need to take into account the third symmetry. It is not quite clear to me which of the resulting additional equations are independent of each other and the old ones. But I believe there should certainly be some dependencies... I noticed that if I only take into account those equations corresponding to values $a<b<c<d$ (of which there are $\binom n4$) and if I assume that these are all independent of each other, then I obtain that  $$\dim V = \frac{\binom n2 \left(\binom n2 +1\right)}{2} - \binom n4 = \frac{n^2(n^2-1)}{12}.$$ (modulo miscalculation). This seems to be the right answer. However, I'm neither sure whether these equations indeed are independent nor am I sure whether they form a maximal set of independent equations. I would appreciate your input! Thanks. =)",,"['linear-algebra', 'combinatorics', 'differential-geometry', 'representation-theory']"
32,"Eigenvalues and column space, nullspace","Eigenvalues and column space, nullspace",,"Is there a way to relate Eigenvalues to the column space and nullspace of a matrix? I believe a matrices with different eigenvalues would have a different column spaces and/or nullspace. Is this correct? I am wondering if you can prove that the Eigenvalues of $A$ and $A^T$ are equal using properties of column spaces and nullspaces. My thinking is: If you transform a matrix $A$ into $B$, if the row space of $B$ is orthogonal to the nullspace of $A$, and the column space of $B$ is orthogonal to the left nullspace of $A$, then matrices $A$ and $B$ have the same eigenvalues.","Is there a way to relate Eigenvalues to the column space and nullspace of a matrix? I believe a matrices with different eigenvalues would have a different column spaces and/or nullspace. Is this correct? I am wondering if you can prove that the Eigenvalues of $A$ and $A^T$ are equal using properties of column spaces and nullspaces. My thinking is: If you transform a matrix $A$ into $B$, if the row space of $B$ is orthogonal to the nullspace of $A$, and the column space of $B$ is orthogonal to the left nullspace of $A$, then matrices $A$ and $B$ have the same eigenvalues.",,['linear-algebra']
33,Which matrices can be embedded in flows?,Which matrices can be embedded in flows?,,"I've been thinking about matrices, recently, and wondering about a fairly simple but maybe hard-to-answer question. A real $n\times n$ matrix $A$ yields a discrete dynamical system on $\mathbb{R}^n$ , where from time $n$ to time $n + 1$ the vector $\mathbf{x}$ jumps to the vector $A\mathbf{x}$ . Under what conditions on $A$ can we ""interpolate"" this to a continuous-time dynamical system, i.e., find some vector field $\mathbf{X}$ such that the time-one flow of $\mathbf{X}$ takes any vector $\mathbf{x}$ to $A\mathbf{x}$ ? Here is what I think I know (but please correct me if I'm wrong). A smooth homomorphism $\phi\colon \mathbb{R} \to \text{GL}_n(\mathbb{R})$ always takes the form $t \mapsto e^{tB}$ for some $B \in \text{M}_{n\times n}(\mathbb{R})$ . Thus, there exists a flow through matrices whose time-one map equals $A$ if and only if $A = e^B$ for some matrix $B \in \text{M}_{n\times n}(\mathbb{R})$ . There are known conditions specifying when this is true; for instance, a paper by Walter Culver titled ""On the Existence and Uniqueness of the Real Logarithm of a Matrix"" says that such a $B$ exists if and only if $A$ is nonsingular, and each Jordan block of $A$ belonging to a negative eigenvalue occurs an even number of times. But do there exist matrices $A$ which are not the exponential of some other real matrix, but nevertheless are the time-one map of some more complicated, non-linear, flow? Thus, we are looking for a smooth homomorphism $\phi\colon \mathbb{R} \to \text{Diff}(\mathbb{R}^n)$ such that $\phi(1) = A$ , or equivalently (I think), a not-necessarily-linear vector field $\mathbf{X}$ whose time-one flow equals $A$ . By the way, I don't need this for any work I'm doing; I just got curious. In terms of my level, I went to grad school for math, but it was years ago and I don't remember things as well as I should. Thanks in advance for your help! Edit: As Alp Uzman points out in his answer, if $A$ embeds in a nonlinear flow, then clearly $A$ must commute with nonlinear diffeomorphisms. By Kopell’s Theorem (again, see Uzman’s answer), this is not possible if $A$ is a contraction and certain conditions on its eigenvalues hold. In dimension 1, the possibilities for diffeomorphisms commuting with any smooth contraction $f$ (not necessarily a linear map) are quite restricted, thanks to Kopell’s Lemma. It’s natural to ask whether this result can be generalized to higher dimensions. I posed this question on Mathoverflow years ago, and have not yet gotten an answer: https://mathoverflow.net/questions/168550/kopells-lemma-in-higher-dimensions","I've been thinking about matrices, recently, and wondering about a fairly simple but maybe hard-to-answer question. A real matrix yields a discrete dynamical system on , where from time to time the vector jumps to the vector . Under what conditions on can we ""interpolate"" this to a continuous-time dynamical system, i.e., find some vector field such that the time-one flow of takes any vector to ? Here is what I think I know (but please correct me if I'm wrong). A smooth homomorphism always takes the form for some . Thus, there exists a flow through matrices whose time-one map equals if and only if for some matrix . There are known conditions specifying when this is true; for instance, a paper by Walter Culver titled ""On the Existence and Uniqueness of the Real Logarithm of a Matrix"" says that such a exists if and only if is nonsingular, and each Jordan block of belonging to a negative eigenvalue occurs an even number of times. But do there exist matrices which are not the exponential of some other real matrix, but nevertheless are the time-one map of some more complicated, non-linear, flow? Thus, we are looking for a smooth homomorphism such that , or equivalently (I think), a not-necessarily-linear vector field whose time-one flow equals . By the way, I don't need this for any work I'm doing; I just got curious. In terms of my level, I went to grad school for math, but it was years ago and I don't remember things as well as I should. Thanks in advance for your help! Edit: As Alp Uzman points out in his answer, if embeds in a nonlinear flow, then clearly must commute with nonlinear diffeomorphisms. By Kopell’s Theorem (again, see Uzman’s answer), this is not possible if is a contraction and certain conditions on its eigenvalues hold. In dimension 1, the possibilities for diffeomorphisms commuting with any smooth contraction (not necessarily a linear map) are quite restricted, thanks to Kopell’s Lemma. It’s natural to ask whether this result can be generalized to higher dimensions. I posed this question on Mathoverflow years ago, and have not yet gotten an answer: https://mathoverflow.net/questions/168550/kopells-lemma-in-higher-dimensions",n\times n A \mathbb{R}^n n n + 1 \mathbf{x} A\mathbf{x} A \mathbf{X} \mathbf{X} \mathbf{x} A\mathbf{x} \phi\colon \mathbb{R} \to \text{GL}_n(\mathbb{R}) t \mapsto e^{tB} B \in \text{M}_{n\times n}(\mathbb{R}) A A = e^B B \in \text{M}_{n\times n}(\mathbb{R}) B A A A \phi\colon \mathbb{R} \to \text{Diff}(\mathbb{R}^n) \phi(1) = A \mathbf{X} A A A A f,"['linear-algebra', 'lie-groups', 'differential-topology', 'dynamical-systems', 'vector-fields']"
34,On the complex matrix equation $AX-XA=B$,On the complex matrix equation,AX-XA=B,"I want to show that there exists solution to the matrix equation $AX-XA=B$ if and only if $$ \begin{pmatrix} A&0\\ 0&A \end{pmatrix}, \begin{pmatrix} A&B\\ 0&A \end{pmatrix} $$ are similar, where all matrices ( $A,B,X$ ) are complex and $A,B$ are $n\times n$ . The sufficiency is easy to show, while I could only manage to show the necessity in some special cases. I also noticed that this is in fact a special case of a much more general Roth's theorem, however I am more interested in this particular question (i.e. how is this equation special). Is there a proof to show the necessity without (over) generalizing?","I want to show that there exists solution to the matrix equation if and only if are similar, where all matrices ( ) are complex and are . The sufficiency is easy to show, while I could only manage to show the necessity in some special cases. I also noticed that this is in fact a special case of a much more general Roth's theorem, however I am more interested in this particular question (i.e. how is this equation special). Is there a proof to show the necessity without (over) generalizing?","AX-XA=B 
\begin{pmatrix}
A&0\\
0&A
\end{pmatrix},
\begin{pmatrix}
A&B\\
0&A
\end{pmatrix}
 A,B,X A,B n\times n","['linear-algebra', 'matrices', 'matrix-equations', 'jordan-normal-form']"
35,"For a unitary matrix $U$, what's the minimal value of the real part of $\det(U^*)\prod_i U_{ii}$?","For a unitary matrix , what's the minimal value of the real part of ?",U \det(U^*)\prod_i U_{ii},"For an $n$ -by- $n$ unitary matrix $U$ , what's the minimal value of the real part of $\Delta(U)=\det(U^*)\prod_i U_{ii}$ ? Let $V$ be the orthogonal matrix with diagonal entries equal to $1-2/n$ and all other entries equal to $-2/n$ . This achieves $\Delta(V)=-(1-2/n)^n$ , which computer experiments suggest is optimal. Interestingly this would mean that the large $n$ limit is $-e^{-2}$ . For $n=2$ the minimum is $0$ , which can be proven by writing $U$ in the form $$\begin{pmatrix}\alpha & \beta \\ -e^{-i\theta}\bar\beta & e^{-i\theta}\bar\alpha\end{pmatrix}.$$ The average value of $\Delta(U)$ across the unitary group is $1/n!$ . Indeed, for any permutation $\sigma$ with permutation matrix $P_\sigma$ , $\Delta_\sigma(U)=(-1)^\sigma\det(U^*)\prod_i U_{i,\sigma(i)}$ equals $\Delta(UP_\sigma)$ . The sum $\sum_\sigma \Delta_\sigma(U)$ equals $\det(U^*)\det(U)=1$ , and each $\int_{U(n)}\Delta_\sigma(U)dU$ is equal because multiplication by $P_\sigma$ preserves the Haar measure.","For an -by- unitary matrix , what's the minimal value of the real part of ? Let be the orthogonal matrix with diagonal entries equal to and all other entries equal to . This achieves , which computer experiments suggest is optimal. Interestingly this would mean that the large limit is . For the minimum is , which can be proven by writing in the form The average value of across the unitary group is . Indeed, for any permutation with permutation matrix , equals . The sum equals , and each is equal because multiplication by preserves the Haar measure.","n n U \Delta(U)=\det(U^*)\prod_i U_{ii} V 1-2/n -2/n \Delta(V)=-(1-2/n)^n n -e^{-2} n=2 0 U \begin{pmatrix}\alpha & \beta \\ -e^{-i\theta}\bar\beta & e^{-i\theta}\bar\alpha\end{pmatrix}. \Delta(U) 1/n! \sigma P_\sigma \Delta_\sigma(U)=(-1)^\sigma\det(U^*)\prod_i U_{i,\sigma(i)} \Delta(UP_\sigma) \sum_\sigma \Delta_\sigma(U) \det(U^*)\det(U)=1 \int_{U(n)}\Delta_\sigma(U)dU P_\sigma","['linear-algebra', 'matrices', 'inequality']"
36,$\operatorname{rank}(A^2)+\operatorname{rank}(B^2)\geq2\operatorname{rank}(AB)$ whenever $AB=BA$?,whenever ?,\operatorname{rank}(A^2)+\operatorname{rank}(B^2)\geq2\operatorname{rank}(AB) AB=BA,"Let $A,B$ be $n\times n$ matrices. If $AB=BA$ , then $\operatorname{rank}(A^2)+\operatorname{rank}(B^2)\geq2\operatorname{rank}(AB)$ . Is this rank inequality correct? No counterexample seems to exist. Here's what I've done. When $A$ is a Jordan block this is easily proved. I tried to bring $A$ into Jordan form for $n=2,3,4$ and found no counterexample. So currently I think it is true. By Fitting's lemma it suffices to consider the case in which $A$ is nilpotent. We can bring $A$ into Jordan form. Then the blocks in $B$ are upper triangular. However, even in this case $\operatorname{rank}(B^2)$ is not tractable. Any hints will be appreciated!","Let be matrices. If , then . Is this rank inequality correct? No counterexample seems to exist. Here's what I've done. When is a Jordan block this is easily proved. I tried to bring into Jordan form for and found no counterexample. So currently I think it is true. By Fitting's lemma it suffices to consider the case in which is nilpotent. We can bring into Jordan form. Then the blocks in are upper triangular. However, even in this case is not tractable. Any hints will be appreciated!","A,B n\times n AB=BA \operatorname{rank}(A^2)+\operatorname{rank}(B^2)\geq2\operatorname{rank}(AB) A A n=2,3,4 A A B \operatorname{rank}(B^2)","['linear-algebra', 'matrices', 'inequality', 'matrix-rank']"
37,Prove that $\begin{vmatrix} xa&yb&zc\\ yc&za&xb\\ zb&xc&ya\\ \end{vmatrix}=xyz\begin{vmatrix} a&b&c\\ c&a&b\\ b&c&a\\ \end{vmatrix}$ if $x+y+z=0$,Prove that  if,\begin{vmatrix} xa&yb&zc\\ yc&za&xb\\ zb&xc&ya\\ \end{vmatrix}=xyz\begin{vmatrix} a&b&c\\ c&a&b\\ b&c&a\\ \end{vmatrix} x+y+z=0,"If $x+y+z=0$, then prove that   $$ \begin{vmatrix} xa&yb&zc\\ yc&za&xb\\ zb&xc&ya\\ \end{vmatrix}=xyz\begin{vmatrix} a&b&c\\ c&a&b\\ b&c&a\\ \end{vmatrix} $$ I can do it by Sarrus' law but how can I prove it by matrix operations without actually expanding the determinant ? My Attempt $$ \begin{vmatrix} xa&yb&zc\\ yc&za&xb\\ zb&xc&ya\\ \end{vmatrix}=xyz\begin{vmatrix} a&\frac{yb}{x}&\frac{zc}{x}\\ c&\frac{za}{y}&\frac{xb}{y}\\ b&\frac{xc}{z}&\frac{ya}{z}\\ \end{vmatrix}=xyz\begin{vmatrix} a&-b-\frac{zb}{x}&-c-\frac{yc}{x}\\ c&-a-\frac{xa}{y}&-b-\frac{zb}{y}\\ b&-c-\frac{yc}{z}&-a-\frac{xa}{z}\\ \end{vmatrix}=xyz\begin{vmatrix} a&b+\frac{zb}{x}&c+\frac{yc}{x}\\ c&a+\frac{xa}{y}&b+\frac{zb}{y}\\ b&c+\frac{yc}{z}&a+\frac{xa}{z}\\ \end{vmatrix}=xyz\begin{vmatrix} a&b&c+\frac{yc}{x}\\ c&a&b+\frac{zb}{y}\\ b&c&a+\frac{xa}{z}\\ \end{vmatrix}+xyz\begin{vmatrix} a&\frac{zb}{x}&c+\frac{yc}{x}\\ c&\frac{xa}{y}&b+\frac{zb}{y}\\ b&\frac{yc}{z}&a+\frac{xa}{z}\\ \end{vmatrix}\\ =xyz\bigg(\begin{vmatrix} a&b&c\\ c&a&b\\ b&c&a\\ \end{vmatrix}+\begin{vmatrix} a&b&\frac{yc}{x}\\ c&a&\frac{zb}{y}\\ b&c&\frac{xa}{z}\\ \end{vmatrix}+\begin{vmatrix} a&\frac{zb}{x}&c\\ c&\frac{xa}{y}&b\\ b&\frac{yc}{z}&a\\ \end{vmatrix}+\begin{vmatrix} a&\frac{zb}{x}&\frac{yc}{x}\\ c&\frac{xa}{y}&\frac{zb}{y}\\ b&\frac{yc}{z}&\frac{xa}{z}\\ \end{vmatrix}\bigg)\\ $$ I need to prove that the sum of last three terms is zero. $$ \begin{vmatrix} a&b&\frac{yc}{x}\\ c&a&\frac{zb}{y}\\ b&c&\frac{xa}{z}\\ \end{vmatrix}+\begin{vmatrix} a&\frac{zb}{x}&c\\ c&\frac{xa}{y}&b\\ b&\frac{yc}{z}&a\\ \end{vmatrix}+\begin{vmatrix} a&\frac{zb}{x}&\frac{yc}{x}\\ c&\frac{xa}{y}&\frac{zb}{y}\\ b&\frac{yc}{z}&\frac{xa}{z}\\ \end{vmatrix}=\begin{vmatrix} a&b&\frac{yc}{x}\\ c&a&\frac{zb}{y}\\ b&c&\frac{xa}{z}\\ \end{vmatrix}+\begin{vmatrix} a&\frac{zb}{x}&\frac{-zc}{x}\\ c&\frac{xa}{y}&\frac{-xb}{y}\\ b&\frac{yc}{z}&\frac{-ya}{z}\\ \end{vmatrix}\\ $$ Solution by expansion $$ \Delta=\begin{matrix} xa&yb&zc&xa&yb\\ yc&za&xb&yc&za\\ zb&xc&ya&zb&xc\\ \end{matrix}=xyz(a^3+b^3+c^3)-abc(x^3+y^3+z^3) $$ We have $x^3+y^3+z^3-3xyz=(x+y+z)(x^2+y^2+z^2-xy-yz-zx)=0$ as $x+y+z=0$. Thus, $x^3+y^3+z^3=3xyz$ $$ \Delta=xyz(a^3+b^3+c^3)-abc(3xyz)=xyz(a^3+b^3+c^3-3abc)\\ =xyz\bigg[ a\big(a^2-bc\big)-b\big(ac-b^2\big)+c\big(c^2-ab\big) \bigg]\\ =xyz.\bigg[a\begin{vmatrix} a&b\\ c&a\\ \end{vmatrix}-b\begin{vmatrix} c&b\\ b&a\\ \end{vmatrix}+c\begin{vmatrix} c&a\\ b&c\\ \end{vmatrix}\bigg] =xyz\begin{vmatrix} a&b&c\\ c&a&b\\ b&c&a\\ \end{vmatrix} $$","If $x+y+z=0$, then prove that   $$ \begin{vmatrix} xa&yb&zc\\ yc&za&xb\\ zb&xc&ya\\ \end{vmatrix}=xyz\begin{vmatrix} a&b&c\\ c&a&b\\ b&c&a\\ \end{vmatrix} $$ I can do it by Sarrus' law but how can I prove it by matrix operations without actually expanding the determinant ? My Attempt $$ \begin{vmatrix} xa&yb&zc\\ yc&za&xb\\ zb&xc&ya\\ \end{vmatrix}=xyz\begin{vmatrix} a&\frac{yb}{x}&\frac{zc}{x}\\ c&\frac{za}{y}&\frac{xb}{y}\\ b&\frac{xc}{z}&\frac{ya}{z}\\ \end{vmatrix}=xyz\begin{vmatrix} a&-b-\frac{zb}{x}&-c-\frac{yc}{x}\\ c&-a-\frac{xa}{y}&-b-\frac{zb}{y}\\ b&-c-\frac{yc}{z}&-a-\frac{xa}{z}\\ \end{vmatrix}=xyz\begin{vmatrix} a&b+\frac{zb}{x}&c+\frac{yc}{x}\\ c&a+\frac{xa}{y}&b+\frac{zb}{y}\\ b&c+\frac{yc}{z}&a+\frac{xa}{z}\\ \end{vmatrix}=xyz\begin{vmatrix} a&b&c+\frac{yc}{x}\\ c&a&b+\frac{zb}{y}\\ b&c&a+\frac{xa}{z}\\ \end{vmatrix}+xyz\begin{vmatrix} a&\frac{zb}{x}&c+\frac{yc}{x}\\ c&\frac{xa}{y}&b+\frac{zb}{y}\\ b&\frac{yc}{z}&a+\frac{xa}{z}\\ \end{vmatrix}\\ =xyz\bigg(\begin{vmatrix} a&b&c\\ c&a&b\\ b&c&a\\ \end{vmatrix}+\begin{vmatrix} a&b&\frac{yc}{x}\\ c&a&\frac{zb}{y}\\ b&c&\frac{xa}{z}\\ \end{vmatrix}+\begin{vmatrix} a&\frac{zb}{x}&c\\ c&\frac{xa}{y}&b\\ b&\frac{yc}{z}&a\\ \end{vmatrix}+\begin{vmatrix} a&\frac{zb}{x}&\frac{yc}{x}\\ c&\frac{xa}{y}&\frac{zb}{y}\\ b&\frac{yc}{z}&\frac{xa}{z}\\ \end{vmatrix}\bigg)\\ $$ I need to prove that the sum of last three terms is zero. $$ \begin{vmatrix} a&b&\frac{yc}{x}\\ c&a&\frac{zb}{y}\\ b&c&\frac{xa}{z}\\ \end{vmatrix}+\begin{vmatrix} a&\frac{zb}{x}&c\\ c&\frac{xa}{y}&b\\ b&\frac{yc}{z}&a\\ \end{vmatrix}+\begin{vmatrix} a&\frac{zb}{x}&\frac{yc}{x}\\ c&\frac{xa}{y}&\frac{zb}{y}\\ b&\frac{yc}{z}&\frac{xa}{z}\\ \end{vmatrix}=\begin{vmatrix} a&b&\frac{yc}{x}\\ c&a&\frac{zb}{y}\\ b&c&\frac{xa}{z}\\ \end{vmatrix}+\begin{vmatrix} a&\frac{zb}{x}&\frac{-zc}{x}\\ c&\frac{xa}{y}&\frac{-xb}{y}\\ b&\frac{yc}{z}&\frac{-ya}{z}\\ \end{vmatrix}\\ $$ Solution by expansion $$ \Delta=\begin{matrix} xa&yb&zc&xa&yb\\ yc&za&xb&yc&za\\ zb&xc&ya&zb&xc\\ \end{matrix}=xyz(a^3+b^3+c^3)-abc(x^3+y^3+z^3) $$ We have $x^3+y^3+z^3-3xyz=(x+y+z)(x^2+y^2+z^2-xy-yz-zx)=0$ as $x+y+z=0$. Thus, $x^3+y^3+z^3=3xyz$ $$ \Delta=xyz(a^3+b^3+c^3)-abc(3xyz)=xyz(a^3+b^3+c^3-3abc)\\ =xyz\bigg[ a\big(a^2-bc\big)-b\big(ac-b^2\big)+c\big(c^2-ab\big) \bigg]\\ =xyz.\bigg[a\begin{vmatrix} a&b\\ c&a\\ \end{vmatrix}-b\begin{vmatrix} c&b\\ b&a\\ \end{vmatrix}+c\begin{vmatrix} c&a\\ b&c\\ \end{vmatrix}\bigg] =xyz\begin{vmatrix} a&b&c\\ c&a&b\\ b&c&a\\ \end{vmatrix} $$",,"['linear-algebra', 'matrices', 'determinant']"
38,Question on linear independence of particular vectors of $\mathbb{R}^8$,Question on linear independence of particular vectors of,\mathbb{R}^8,"Given any four points $z_1,z_2,z_3,z_4\in \mathbb{C}$, all different from zero, consider the following seven vectors of $\mathbb{C}^4$: $$v_1:=(0,z_2-0,0,0)$$ $$v_2:=(0,0,z_3-0,0)$$ $$v_3:=(z_1-z_2,z_2-z_1,0,0)$$ $$v_4:=(0,z_2-z_4,0,z_4-z_2)$$ $$v_5:=(0,0,z_3-z_4,z_4-z_3)$$ $$v_6:=(z_1-z_3,0,z_3-z_1,0)$$ $$v_7:=(z_1-z_4,0,0,z_4-z_1)$$ Under what conditions on $z_1,z_2,z_3,z_4$ are these seven vectors $v_1,\dots,v_7$ linearly independent as vectors of $\mathbb{R}^8$? Of course I know how to consider the vectors $v_i$ as vectors of $\mathbb{R}^8$. The difficult part (which is the aim of the question) is to deduce all sufficient conditions for their linear independence. Edit: User MvG posted an answer which involves the use of a computer algebra system, but I am actually interested in a method which could be solved by hand (without a computer).","Given any four points $z_1,z_2,z_3,z_4\in \mathbb{C}$, all different from zero, consider the following seven vectors of $\mathbb{C}^4$: $$v_1:=(0,z_2-0,0,0)$$ $$v_2:=(0,0,z_3-0,0)$$ $$v_3:=(z_1-z_2,z_2-z_1,0,0)$$ $$v_4:=(0,z_2-z_4,0,z_4-z_2)$$ $$v_5:=(0,0,z_3-z_4,z_4-z_3)$$ $$v_6:=(z_1-z_3,0,z_3-z_1,0)$$ $$v_7:=(z_1-z_4,0,0,z_4-z_1)$$ Under what conditions on $z_1,z_2,z_3,z_4$ are these seven vectors $v_1,\dots,v_7$ linearly independent as vectors of $\mathbb{R}^8$? Of course I know how to consider the vectors $v_i$ as vectors of $\mathbb{R}^8$. The difficult part (which is the aim of the question) is to deduce all sufficient conditions for their linear independence. Edit: User MvG posted an answer which involves the use of a computer algebra system, but I am actually interested in a method which could be solved by hand (without a computer).",,"['linear-algebra', 'geometry', 'algebraic-geometry']"
39,Surjective polynomial map from $M_n(F)$ to $M_n(F)$,Surjective polynomial map from  to,M_n(F) M_n(F),"Assume that $F$ is a field, and that $f \in F[x]$ is a polynomial. If $f$ is surjective for every sufficient large $n$ when we regard $f$ as a map from $M_n(F)$ to $M_n(F)$, must $f$ be linear?","Assume that $F$ is a field, and that $f \in F[x]$ is a polynomial. If $f$ is surjective for every sufficient large $n$ when we regard $f$ as a map from $M_n(F)$ to $M_n(F)$, must $f$ be linear?",,['linear-algebra']
40,Matrix inequality after taking inverse,Matrix inequality after taking inverse,,Let A and B be Positive definite Matrices with $ A\leq B$ in the sense that $B-A$ is positive definite. Is it true that $A^{-1} \geq B^{-1} $?,Let A and B be Positive definite Matrices with $ A\leq B$ in the sense that $B-A$ is positive definite. Is it true that $A^{-1} \geq B^{-1} $?,,"['linear-algebra', 'matrices', 'positive-definite']"
41,Number of solutions of a matrix algebraic equation,Number of solutions of a matrix algebraic equation,,"Reading that , I found that: given a matrix algebraic equation $$ X^n+A_1X^{n-1}+\cdots + A_n=0 $$    where the cofficients $A_1\cdots,A_n$ as well as solutions $X$ are   supposed to be square complex matrices of some order $k$.......   generically has $\binom {nk}{k}$ rather $n$ solutions. Someone know how can we prove this statement, or where I can find a proof?","Reading that , I found that: given a matrix algebraic equation $$ X^n+A_1X^{n-1}+\cdots + A_n=0 $$    where the cofficients $A_1\cdots,A_n$ as well as solutions $X$ are   supposed to be square complex matrices of some order $k$.......   generically has $\binom {nk}{k}$ rather $n$ solutions. Someone know how can we prove this statement, or where I can find a proof?",,"['linear-algebra', 'matrix-equations']"
42,Characterizing $\text{PGL}_2(\mathbb F_p)$,Characterizing,\text{PGL}_2(\mathbb F_p),"Where can I find a description and proof of the basic structure of $\text{PGL}_2(\mathbb{F}_p)$ (number of elements with each order, conjugacy classes, etc.) which is understandable by an undergraduate who has taken a few semesters of group theory? In places such as here there is a description of the distribution of orders and a reference to the conjugacy classes, but no proof.","Where can I find a description and proof of the basic structure of $\text{PGL}_2(\mathbb{F}_p)$ (number of elements with each order, conjugacy classes, etc.) which is understandable by an undergraduate who has taken a few semesters of group theory? In places such as here there is a description of the distribution of orders and a reference to the conjugacy classes, but no proof.",,"['linear-algebra', 'group-theory', 'projective-geometry']"
43,Prove that $V = \ker(\phi^n) \oplus \text{image}(\phi^n)$,Prove that,V = \ker(\phi^n) \oplus \text{image}(\phi^n),"Let $V$ be a $n$-dimensional complex vector space and $\phi:V\to V$ a linear mapping. Prove that $$V = \ker(\phi^n) \oplus \text{image}(\phi^n)$$ Here is my attempt: Since $\phi^n$ is also a linear mapping of $V$ into $V$, we have that $$\dim V = \dim \ker(\phi^n) + \dim \text{image}(\phi^n).$$ We only need only to show that this sum is direct, in other words, that $$\ker(\phi^n) \cap \text{image}(\phi^n) = \{0\}.$$ since this would imply $$V = \ker(\phi^n) + \text{image}(\phi^n)$$ We let $v \in \ker(\phi^n) \cap \text{image}(\phi^n)$ be arbitrary and aim to show that $v=0$. $\ker(\phi^n)$ is the generalized eigenspace of $\phi$ for the eigenvalue $0$, so there is a $k \leq n$ such that $\phi^k(v) = 0$. This is where I'm stuck. How do I proceed from here? Is there a different way to do this?","Let $V$ be a $n$-dimensional complex vector space and $\phi:V\to V$ a linear mapping. Prove that $$V = \ker(\phi^n) \oplus \text{image}(\phi^n)$$ Here is my attempt: Since $\phi^n$ is also a linear mapping of $V$ into $V$, we have that $$\dim V = \dim \ker(\phi^n) + \dim \text{image}(\phi^n).$$ We only need only to show that this sum is direct, in other words, that $$\ker(\phi^n) \cap \text{image}(\phi^n) = \{0\}.$$ since this would imply $$V = \ker(\phi^n) + \text{image}(\phi^n)$$ We let $v \in \ker(\phi^n) \cap \text{image}(\phi^n)$ be arbitrary and aim to show that $v=0$. $\ker(\phi^n)$ is the generalized eigenspace of $\phi$ for the eigenvalue $0$, so there is a $k \leq n$ such that $\phi^k(v) = 0$. This is where I'm stuck. How do I proceed from here? Is there a different way to do this?",,"['linear-algebra', 'jordan-normal-form']"
44,Showing that a matrix is positive (semi-)definite,Showing that a matrix is positive (semi-)definite,,"Let $G = (V,E)$ be a connected graph and $T$ one of its spanning trees. Let $w \in[0,1]^{|V|-1}$ be a weight for the spanning tree, i.e. we assign to each of the spanning tree's edges a number in $[0,1]$. For $(i,j) \in V \times V$ let $x_{ij}^T(w)$ be the minimum of the weights on the unique path from $i$ to $j$ in $T$. For $i=j$ we assign $x_{ij}^T(w) = 1$. Theorem: The $|V| \times |V|$ real symmetric matrix $x_{ij}^T(w)$ is positive semidefinite. If we assume $w \in [0,1[^{|V|-1}$ it is even positive definite. This positivity property is necessary for the absolute convergence of the reshuffled sum using the Loop Vertex Expansion. It is the central property of the forest formula, however I am wondering: Is there a simple proof not related to the forest formula at all? //edit: The proof I already know is in Rivasseau's and Wang's ""How To Resum Feynman Graphs"".","Let $G = (V,E)$ be a connected graph and $T$ one of its spanning trees. Let $w \in[0,1]^{|V|-1}$ be a weight for the spanning tree, i.e. we assign to each of the spanning tree's edges a number in $[0,1]$. For $(i,j) \in V \times V$ let $x_{ij}^T(w)$ be the minimum of the weights on the unique path from $i$ to $j$ in $T$. For $i=j$ we assign $x_{ij}^T(w) = 1$. Theorem: The $|V| \times |V|$ real symmetric matrix $x_{ij}^T(w)$ is positive semidefinite. If we assume $w \in [0,1[^{|V|-1}$ it is even positive definite. This positivity property is necessary for the absolute convergence of the reshuffled sum using the Loop Vertex Expansion. It is the central property of the forest formula, however I am wondering: Is there a simple proof not related to the forest formula at all? //edit: The proof I already know is in Rivasseau's and Wang's ""How To Resum Feynman Graphs"".",,"['linear-algebra', 'graph-theory']"
45,"Given three vectors, how to find an orthonormal basis closest to them?","Given three vectors, how to find an orthonormal basis closest to them?",,"I know Gram-Schmidt process but that is not what I am looking for. Given three vectors in $\mathbb{R}^3$, $\{v_1,v_2,v_3\}$, I want to find three vectors $\{w_1,w_2,w_3\} \subset \mathbb{R}^3$ such that $$(w_i,w_j) = \delta_{ij}, \quad \forall i,j\in\{1,2,3\},$$ $$D=\sum_{j=1}^3 \|v_j-w_j\|^2\ \mbox{is minimal.}$$ The drawback of the GS process is that it assumes a preferred order in the 3-tuple and only changes the length of the first vector, resulting in the change being done to the second and third too large and the distance $D$ suboptimal. I want to deform all three in a ""fair"" manner.","I know Gram-Schmidt process but that is not what I am looking for. Given three vectors in $\mathbb{R}^3$, $\{v_1,v_2,v_3\}$, I want to find three vectors $\{w_1,w_2,w_3\} \subset \mathbb{R}^3$ such that $$(w_i,w_j) = \delta_{ij}, \quad \forall i,j\in\{1,2,3\},$$ $$D=\sum_{j=1}^3 \|v_j-w_j\|^2\ \mbox{is minimal.}$$ The drawback of the GS process is that it assumes a preferred order in the 3-tuple and only changes the length of the first vector, resulting in the change being done to the second and third too large and the distance $D$ suboptimal. I want to deform all three in a ""fair"" manner.",,"['linear-algebra', 'orthonormal']"
46,Quick question: tensor product and dual of vector space,Quick question: tensor product and dual of vector space,,"Recall that for a finite dimensional vector space $V$ we have the natural isomorphism $\phi :V^{*} \otimes V \rightarrow Hom(V,V)$ given by $\alpha \otimes v \mapsto (x \mapsto \alpha (x)v)$. Is there a coordinate-free way to express $\phi^{-1}(id_{V})$?","Recall that for a finite dimensional vector space $V$ we have the natural isomorphism $\phi :V^{*} \otimes V \rightarrow Hom(V,V)$ given by $\alpha \otimes v \mapsto (x \mapsto \alpha (x)v)$. Is there a coordinate-free way to express $\phi^{-1}(id_{V})$?",,"['linear-algebra', 'abstract-algebra', 'tensor-products', 'multilinear-algebra']"
47,*-homomorphisms $M_m(\mathbb{C})\rightarrow M_n(\mathbb{C})$,*-homomorphisms,M_m(\mathbb{C})\rightarrow M_n(\mathbb{C}),"I've heard that every *-homomorphism $\phi:M_m(\mathbb{C})\rightarrow M_n(\mathbb{C})$ is unitarily equivalent to some *-homomorphism of the form $A\in M_m(\mathbb{C})\mapsto\left(\oplus_{k}A\right)\oplus0_{n-km}\in M_n(\mathbb{C})$ for some $k$, that is, the matrix $A$ is copied $k$ times in the diagonal as below: $$A\in M_m(\mathbb{C})\mapsto\left[\begin{array}{} A\\ &\ddots\\ &&A\\ &&&0_{n-km} \end{array}\right]\in M_n(\mathbb{C})$$ Do someone have a (maybe not so) simple proof of this fact?","I've heard that every *-homomorphism $\phi:M_m(\mathbb{C})\rightarrow M_n(\mathbb{C})$ is unitarily equivalent to some *-homomorphism of the form $A\in M_m(\mathbb{C})\mapsto\left(\oplus_{k}A\right)\oplus0_{n-km}\in M_n(\mathbb{C})$ for some $k$, that is, the matrix $A$ is copied $k$ times in the diagonal as below: $$A\in M_m(\mathbb{C})\mapsto\left[\begin{array}{} A\\ &\ddots\\ &&A\\ &&&0_{n-km} \end{array}\right]\in M_n(\mathbb{C})$$ Do someone have a (maybe not so) simple proof of this fact?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'functional-analysis']"
48,To Interpret Solving Systems of Linear Equations Geometrically in Terms of Linear Algebra,To Interpret Solving Systems of Linear Equations Geometrically in Terms of Linear Algebra,,"I never really understood basic Gaussian elimination & solving systems of equations once I learned some actual linear algebra. I thought this was due to me missing some fundamental aspect of the subject that some book would eventually illuminate for me or that things would just click but no they haven't & I can't stand being told something along the lines of Kaplansky quote ""we think basis-free, we write basis-free, but when the chips are down we close the office door and compute with matrices like fury"" as a rationale for the apparent disconnect between the theory & application of linear algebra when I view things as I'll describe below. Lets say I have this square system: $ax + by = e$ $cx + dy = f$ I think there are four ways we can geometrically understand this picture, & I have questions about all of them (note that nothing will be said about bigger or non-square systems in this post). $01:$ VECTORS & LINEAR MAPS If I want to understand this exclusively in terms of vectors & linear maps I can write this system as a linear combination: $x(a,c) + y(b,d) = (e,f)$ $xT(\hat{e_{1}}) + yT(\hat{e_{2}}) = (e,f)$ $T(x\hat{e_{1}} + y\hat{e_{2}}) = (e,f)$ $T(\vec{v}) = \vec{z}$ Now we can see that solving this system of linear equations is equivalent to determining which vector in the domain of $T$ is mapped to the vector $(e,f)$. Furthermore, using the fact that a linear map on a FDVS is completely determined by it's action on a basis, if we arrange things such that T acts on the standard basis then we can use linearity to determine the scalar multiples x & y. I think that's the general gist of what's going on (this is all correct so far, right?), & from a distance this is very geometric & conceptually intuitive. In the best case scenario (unique solution to the system) this is the image I think most people have. The thing I don't like about this perspective is how divorced it is from all computations that I know of, it basically has nothing to do with Gaussian or Gauss-Jordan elimination as far as I can tell. My first question is whether or not you can use this interpretation, i.e. linear maps, in a computational sense because it seems to me you have to revert to another interpretation I'll outline below & I'm wondering whether the concepts are actually so apparently divorced or whether I'm missing something, maybe I just don't see how all of this is actually related to basic linear algebra. Also it just seems strange to me to whip out new vectors that, while admittedly contain something from both equations, geometrically has no obvious connection with the lines. 02: NORMAL VECTORS This interpretation uses the fact that the vector $(a,b)$ is the normal vector to $ax + by = e$ (i.e. $(a,b)\cdot(x - x_0,y - y_0) = 0$ such that $ax_0 + by_0 = e$) & is basically a geometric interpretation of (every step of) both Gaussian & Gauss-Jordan elimination, giving some soul & feeling to the algebraic computations. Here you're using the second most obvious vectors associated with the lines (the normal, with the first most obvious vector being that one parallel to the line). Thus when you have $ax + by = e$ $cx + dy = f$ & you add a scalar multiple of one to the other you get $(a + \lambda c)x + (b + \lambda d)y = e + \lambda f$, you can interpret this as nothing other than adding normal vectors to end up with a new 'normal vector' $(a + \lambda c,b + \lambda d)$ (what it is 'normal' to I don't know but I think it just a convenient vector we use as a means to eliminate coefficients, as done next) & end up with: $(a + \lambda c,b + \lambda d)\cdot(x - x_0,y - y_0) = 0$ s.t. $(a + \lambda c)(x - x_0) + (b + \lambda d)(y - y_0) = 0$ $a(x - x_0) + \lambda c(x - x_0) + b(y - y_0) + \lambda d(y - y_0) = 0$ $ax + \lambda cx + by + \lambda dy - ax_0  - \lambda cx_0  - by_0 - \lambda dy_0 = 0$ $(a + \lambda c)x + (b + \lambda d)y = ax_0  + \lambda cx_0  + by_0 + \lambda dy_0 $ Thus as long as $(a,b)$ & $(c,d)$ are not linearly dependent you can't choose $\lambda$ such that the above becomes $(0,0)\cdot(x - x_0,y - y_0) = 0$. Now the standard route is to choose $\lambda$ such that you eliminate one of the variables & solve for the other, say $\lambda = - \frac{a}{c}$, gives $(a + \lambda c,b + \lambda d)\cdot(x - x_0,y - y_0) = 0$ $(a  - \frac{a}{c} c,b - \frac{a}{c} d)\cdot(x - x_0,y - y_0) = 0$ $(0,b - \frac{ad}{c})\cdot(x - x_0,y - y_0) = 0$ $(b - \frac{ad}{c})(y - y_0) = 0$ $bc(y - y_0) - ad(y - y_0) = 0$ $bcy - bcy_0 - ady + ady_0 = 0$ $(ad - bc)y_0 = (ad - bc)y$ $y_0 = y$ which can also be done using: $(a + \lambda c)x + (b + \lambda d)y = ax_0  + \lambda cx_0  + by_0 + \lambda dy_0 $ since you get $(a - \frac{a}{c} c)x + (b - \frac{a}{c}d)y = ax_0  - \frac{a}{c}(cx_0)  + by_0 - \frac{a}{c} dy_0 $ $(b - \frac{a}{c}d)y = (b - \frac{a}{c} d)y_0 $ $y = y_0 $ Similarly for finding $x = x_0$, however we want to understand this geometrically. My second question is as to whether it right to interpret the above as saying that we're going to take $(x_0,y_0)$ as the hypothetical point of intersection of the two lines & in the situation that no $\lambda$ can be chosen such that the dot product will contain a zero vector (i.e. if we can be sure the normal vectors are linearly independent) we know it uniquely exists & from then on we are doing nothing other than choosing $\lambda$ such that, say when we're solving for $y = y_0 $, the vector $(a + \lambda c,b + \lambda d)$ points in the y axis direction, i.e. it's a vertical vector in the cartesian plane, of the form $(0,y_0)$, i.e. pointing to the y component of the intersection of the two lines?  Similarly for finding the $x_0$ term, we just use vector addition to eliminate a coefficient then find $(x_0,0)$, then through finding both $(x_0,0)$ & $(0,y_0)$ we simultaneously find $(x_0,y_0)$. Unless I'm deluded I'm pretty sure all of the above is a geometric way to understand every step of those furious computations with matrices so I don't see how this can be wrong... My third question is to how any of this discussion relates to linear maps? It seems to me that interpreting a system of linear equations in terms of normal vectors is far superior to interpreting them in terms of linear maps, at least in the square $n x n$ case. Am I missing something? 03: DETERMINANTS & LINEAR MAPS: Let $\Psi$ be an alternating bilinear form such that $\Psi(e_1,e_2) = 1$. For an operator $T$ we note the number $\lambda$ such that $\Psi(T(e_1),T(e_2)) = \lambda\Psi(e_1,e_2)$ is known as the determinant, i.e. $\Psi(T(e_1),T(e_2)) = det(T)\Psi(e_1,e_2)$. Again this way of looking at things is very intuitive from a distance, the determinant of an operator is nothing but the number such that the area between $T(e_1)$ & $T(e_2)$, i.e. $\Psi(T(e_1),T(e_2))$, is just a multiple of the area between $e_1$ & $e_2$, i.e.$\Psi(e_1,e_2)$ (disregarding signs). In fact we have no problem in more generally writing $\Psi(T(u),T(v)) = det(T)\Psi(u,v)$ for arbitrary vectors $u$ & $v$. Note that $\Psi$ has nothing to do with normal vectors here, it's exploiting the properties of the first way of looking at this system (in terms of matrices we're dealing with the determinant as a linear function of the columns basically). The reason I bring this topic up here is to find out about how to relate these concepts to the geometry of the situation. Again we are introducing seemingly arbitrary vectors $T(e_1)$ & $T(e_2)$ that don't relate to the geometry of the lines (though of course the vectors contain algebraic information). With that said my fourth question comes from solution's determined via Cramer's rule. If you use this notation, $\Psi(T(e_1),T(e_2)) = det(T)\Psi(e_1,e_2)$, you see  $\Psi(\vec{z},T(e_2)) = \Psi(xT(e_1) + yT(e_2),e_2)$ implies $x = \frac{\Psi(\vec{z},T(e_2))}{\Psi(T(e_1),T(e_2))}$. This term simply must have some fascinating interpretation... I would love to know what it means to say that the $x$ component of the point of intersection of two lines is the ratio of the area between the vectors whose components are the solutions to both of the equations (I can't see a nice way to talk about or interpret this) & the vector $T(e_1)$ (whatever this vector is supposed to be interpreted as) to the area contained within $T(e_1)$ & $T(e_2)$. My fifth question is almost the same as the above except it modifies the interpretaton of the last sentence ""to the area contained within $T(e_1)$ & $T(e_2)$"". If we exploit the fact that for matrices: $det(T) = det(T^t)$ we can interpret the determinant in a whole new manner intimately related to the geometry of the lines , we can now interpret the determinant as containing the (signed) area between the normal vectors to the lines (which immediately gives meaning to the situations of either a zero or non-zero determinant). To restate the question I would love to know what it means to say that the $x$ component of the point of intersection of two lines is the ratio of the area between the vectors whose components are the solutions to both of the equations & the vector $T(e_1)$ (whatever this vector is supposed to be interpreted as) to the area contained within the normal vectors to the two lines. My sixth question is whether I'm right to make all these distinctions.  I don't know whether I should be going so far as to even delineate between two separate interpretations of the denominator in the solution to cramer's rule & asking for two different interpretations but it really seems like you have to be able to think about this in two different ways, one extremely geometric on every level (normal vectors), the other geometric only at the start. I am just not sure, I think you just have no intuitive geometric interpretation in terms of linear maps, you have to use these almost arbitrary vectors $T(e_1)$ divorced from the geometry of the lines if you think in terms of linear maps whereas when you do it in terms of normal vectors you get something nice. 04 LINEAR FUNCTIONALS & LINEAR MAPS My seventh & final question is about the relationship of linear functionals to solving systems of linear equations. Given the system: $ax + by = e$ $cx + dy = f$ i.e. $xT(\hat{e_{1}}) + yT(\hat{e_{2}}) = (e,f)$` we ask how linear functionals interact with this setup.  By introducing $\psi_1(xe_1 + ye_2) = e$ & $\psi_2(xe_1 + ye_2) = f$ we see $\psi_1(xe_1 + ye_2) = x\psi_1(e_1) + y\psi_1(e_2) = ax + by = e$ $\psi_2(xe_1 + ye_2) = x\psi_2(e_1) + y\psi_2(e_2) = cx + dy = f$ I really don't know how to interpret this or fit it into the general scheme of things. It seems to be saying that a linear functional maps the solution vector to a line, & that the action of a linear functional on a basis results in coefficients of the normal vectors (i.e. in some way you're mapping the solution of the system to the normal vectors) but I don't know what you're supposed to do with this & would appreciate any help on how to interpret this in light of everything I've asked. I really appreciate any help with this, I know it's a long post but the questions are, in my mind, all tied together so I sincerely appreciate any help.","I never really understood basic Gaussian elimination & solving systems of equations once I learned some actual linear algebra. I thought this was due to me missing some fundamental aspect of the subject that some book would eventually illuminate for me or that things would just click but no they haven't & I can't stand being told something along the lines of Kaplansky quote ""we think basis-free, we write basis-free, but when the chips are down we close the office door and compute with matrices like fury"" as a rationale for the apparent disconnect between the theory & application of linear algebra when I view things as I'll describe below. Lets say I have this square system: $ax + by = e$ $cx + dy = f$ I think there are four ways we can geometrically understand this picture, & I have questions about all of them (note that nothing will be said about bigger or non-square systems in this post). $01:$ VECTORS & LINEAR MAPS If I want to understand this exclusively in terms of vectors & linear maps I can write this system as a linear combination: $x(a,c) + y(b,d) = (e,f)$ $xT(\hat{e_{1}}) + yT(\hat{e_{2}}) = (e,f)$ $T(x\hat{e_{1}} + y\hat{e_{2}}) = (e,f)$ $T(\vec{v}) = \vec{z}$ Now we can see that solving this system of linear equations is equivalent to determining which vector in the domain of $T$ is mapped to the vector $(e,f)$. Furthermore, using the fact that a linear map on a FDVS is completely determined by it's action on a basis, if we arrange things such that T acts on the standard basis then we can use linearity to determine the scalar multiples x & y. I think that's the general gist of what's going on (this is all correct so far, right?), & from a distance this is very geometric & conceptually intuitive. In the best case scenario (unique solution to the system) this is the image I think most people have. The thing I don't like about this perspective is how divorced it is from all computations that I know of, it basically has nothing to do with Gaussian or Gauss-Jordan elimination as far as I can tell. My first question is whether or not you can use this interpretation, i.e. linear maps, in a computational sense because it seems to me you have to revert to another interpretation I'll outline below & I'm wondering whether the concepts are actually so apparently divorced or whether I'm missing something, maybe I just don't see how all of this is actually related to basic linear algebra. Also it just seems strange to me to whip out new vectors that, while admittedly contain something from both equations, geometrically has no obvious connection with the lines. 02: NORMAL VECTORS This interpretation uses the fact that the vector $(a,b)$ is the normal vector to $ax + by = e$ (i.e. $(a,b)\cdot(x - x_0,y - y_0) = 0$ such that $ax_0 + by_0 = e$) & is basically a geometric interpretation of (every step of) both Gaussian & Gauss-Jordan elimination, giving some soul & feeling to the algebraic computations. Here you're using the second most obvious vectors associated with the lines (the normal, with the first most obvious vector being that one parallel to the line). Thus when you have $ax + by = e$ $cx + dy = f$ & you add a scalar multiple of one to the other you get $(a + \lambda c)x + (b + \lambda d)y = e + \lambda f$, you can interpret this as nothing other than adding normal vectors to end up with a new 'normal vector' $(a + \lambda c,b + \lambda d)$ (what it is 'normal' to I don't know but I think it just a convenient vector we use as a means to eliminate coefficients, as done next) & end up with: $(a + \lambda c,b + \lambda d)\cdot(x - x_0,y - y_0) = 0$ s.t. $(a + \lambda c)(x - x_0) + (b + \lambda d)(y - y_0) = 0$ $a(x - x_0) + \lambda c(x - x_0) + b(y - y_0) + \lambda d(y - y_0) = 0$ $ax + \lambda cx + by + \lambda dy - ax_0  - \lambda cx_0  - by_0 - \lambda dy_0 = 0$ $(a + \lambda c)x + (b + \lambda d)y = ax_0  + \lambda cx_0  + by_0 + \lambda dy_0 $ Thus as long as $(a,b)$ & $(c,d)$ are not linearly dependent you can't choose $\lambda$ such that the above becomes $(0,0)\cdot(x - x_0,y - y_0) = 0$. Now the standard route is to choose $\lambda$ such that you eliminate one of the variables & solve for the other, say $\lambda = - \frac{a}{c}$, gives $(a + \lambda c,b + \lambda d)\cdot(x - x_0,y - y_0) = 0$ $(a  - \frac{a}{c} c,b - \frac{a}{c} d)\cdot(x - x_0,y - y_0) = 0$ $(0,b - \frac{ad}{c})\cdot(x - x_0,y - y_0) = 0$ $(b - \frac{ad}{c})(y - y_0) = 0$ $bc(y - y_0) - ad(y - y_0) = 0$ $bcy - bcy_0 - ady + ady_0 = 0$ $(ad - bc)y_0 = (ad - bc)y$ $y_0 = y$ which can also be done using: $(a + \lambda c)x + (b + \lambda d)y = ax_0  + \lambda cx_0  + by_0 + \lambda dy_0 $ since you get $(a - \frac{a}{c} c)x + (b - \frac{a}{c}d)y = ax_0  - \frac{a}{c}(cx_0)  + by_0 - \frac{a}{c} dy_0 $ $(b - \frac{a}{c}d)y = (b - \frac{a}{c} d)y_0 $ $y = y_0 $ Similarly for finding $x = x_0$, however we want to understand this geometrically. My second question is as to whether it right to interpret the above as saying that we're going to take $(x_0,y_0)$ as the hypothetical point of intersection of the two lines & in the situation that no $\lambda$ can be chosen such that the dot product will contain a zero vector (i.e. if we can be sure the normal vectors are linearly independent) we know it uniquely exists & from then on we are doing nothing other than choosing $\lambda$ such that, say when we're solving for $y = y_0 $, the vector $(a + \lambda c,b + \lambda d)$ points in the y axis direction, i.e. it's a vertical vector in the cartesian plane, of the form $(0,y_0)$, i.e. pointing to the y component of the intersection of the two lines?  Similarly for finding the $x_0$ term, we just use vector addition to eliminate a coefficient then find $(x_0,0)$, then through finding both $(x_0,0)$ & $(0,y_0)$ we simultaneously find $(x_0,y_0)$. Unless I'm deluded I'm pretty sure all of the above is a geometric way to understand every step of those furious computations with matrices so I don't see how this can be wrong... My third question is to how any of this discussion relates to linear maps? It seems to me that interpreting a system of linear equations in terms of normal vectors is far superior to interpreting them in terms of linear maps, at least in the square $n x n$ case. Am I missing something? 03: DETERMINANTS & LINEAR MAPS: Let $\Psi$ be an alternating bilinear form such that $\Psi(e_1,e_2) = 1$. For an operator $T$ we note the number $\lambda$ such that $\Psi(T(e_1),T(e_2)) = \lambda\Psi(e_1,e_2)$ is known as the determinant, i.e. $\Psi(T(e_1),T(e_2)) = det(T)\Psi(e_1,e_2)$. Again this way of looking at things is very intuitive from a distance, the determinant of an operator is nothing but the number such that the area between $T(e_1)$ & $T(e_2)$, i.e. $\Psi(T(e_1),T(e_2))$, is just a multiple of the area between $e_1$ & $e_2$, i.e.$\Psi(e_1,e_2)$ (disregarding signs). In fact we have no problem in more generally writing $\Psi(T(u),T(v)) = det(T)\Psi(u,v)$ for arbitrary vectors $u$ & $v$. Note that $\Psi$ has nothing to do with normal vectors here, it's exploiting the properties of the first way of looking at this system (in terms of matrices we're dealing with the determinant as a linear function of the columns basically). The reason I bring this topic up here is to find out about how to relate these concepts to the geometry of the situation. Again we are introducing seemingly arbitrary vectors $T(e_1)$ & $T(e_2)$ that don't relate to the geometry of the lines (though of course the vectors contain algebraic information). With that said my fourth question comes from solution's determined via Cramer's rule. If you use this notation, $\Psi(T(e_1),T(e_2)) = det(T)\Psi(e_1,e_2)$, you see  $\Psi(\vec{z},T(e_2)) = \Psi(xT(e_1) + yT(e_2),e_2)$ implies $x = \frac{\Psi(\vec{z},T(e_2))}{\Psi(T(e_1),T(e_2))}$. This term simply must have some fascinating interpretation... I would love to know what it means to say that the $x$ component of the point of intersection of two lines is the ratio of the area between the vectors whose components are the solutions to both of the equations (I can't see a nice way to talk about or interpret this) & the vector $T(e_1)$ (whatever this vector is supposed to be interpreted as) to the area contained within $T(e_1)$ & $T(e_2)$. My fifth question is almost the same as the above except it modifies the interpretaton of the last sentence ""to the area contained within $T(e_1)$ & $T(e_2)$"". If we exploit the fact that for matrices: $det(T) = det(T^t)$ we can interpret the determinant in a whole new manner intimately related to the geometry of the lines , we can now interpret the determinant as containing the (signed) area between the normal vectors to the lines (which immediately gives meaning to the situations of either a zero or non-zero determinant). To restate the question I would love to know what it means to say that the $x$ component of the point of intersection of two lines is the ratio of the area between the vectors whose components are the solutions to both of the equations & the vector $T(e_1)$ (whatever this vector is supposed to be interpreted as) to the area contained within the normal vectors to the two lines. My sixth question is whether I'm right to make all these distinctions.  I don't know whether I should be going so far as to even delineate between two separate interpretations of the denominator in the solution to cramer's rule & asking for two different interpretations but it really seems like you have to be able to think about this in two different ways, one extremely geometric on every level (normal vectors), the other geometric only at the start. I am just not sure, I think you just have no intuitive geometric interpretation in terms of linear maps, you have to use these almost arbitrary vectors $T(e_1)$ divorced from the geometry of the lines if you think in terms of linear maps whereas when you do it in terms of normal vectors you get something nice. 04 LINEAR FUNCTIONALS & LINEAR MAPS My seventh & final question is about the relationship of linear functionals to solving systems of linear equations. Given the system: $ax + by = e$ $cx + dy = f$ i.e. $xT(\hat{e_{1}}) + yT(\hat{e_{2}}) = (e,f)$` we ask how linear functionals interact with this setup.  By introducing $\psi_1(xe_1 + ye_2) = e$ & $\psi_2(xe_1 + ye_2) = f$ we see $\psi_1(xe_1 + ye_2) = x\psi_1(e_1) + y\psi_1(e_2) = ax + by = e$ $\psi_2(xe_1 + ye_2) = x\psi_2(e_1) + y\psi_2(e_2) = cx + dy = f$ I really don't know how to interpret this or fit it into the general scheme of things. It seems to be saying that a linear functional maps the solution vector to a line, & that the action of a linear functional on a basis results in coefficients of the normal vectors (i.e. in some way you're mapping the solution of the system to the normal vectors) but I don't know what you're supposed to do with this & would appreciate any help on how to interpret this in light of everything I've asked. I really appreciate any help with this, I know it's a long post but the questions are, in my mind, all tied together so I sincerely appreciate any help.",,['linear-algebra']
49,Expanding a basis of a subspace to a basis for the vector space,Expanding a basis of a subspace to a basis for the vector space,,"I'm not really sure how to extend a basis. I'm trying to do the following question. Consider the subspace $ W = \{(x_1, x_2, x_3, x_4) \in \mathbb{R}^4 : x_1 = -x_4, x_2 = x_3\}$ of $ \mathbb{R}^4$. Extend the basis $\{(0,2,2,0),(1,0,0,-1)\}$ of $W$ to a basis of $ \mathbb{R}^4$. I know I need to add another two vectors for it to be a basis of $ \mathbb{R}^4$ but I'm not sure how to pick the vectors. In general, how do you expand a basis?","I'm not really sure how to extend a basis. I'm trying to do the following question. Consider the subspace $ W = \{(x_1, x_2, x_3, x_4) \in \mathbb{R}^4 : x_1 = -x_4, x_2 = x_3\}$ of $ \mathbb{R}^4$. Extend the basis $\{(0,2,2,0),(1,0,0,-1)\}$ of $W$ to a basis of $ \mathbb{R}^4$. I know I need to add another two vectors for it to be a basis of $ \mathbb{R}^4$ but I'm not sure how to pick the vectors. In general, how do you expand a basis?",,['linear-algebra']
50,diagonalizing a matrix over the $\ell$-adics,diagonalizing a matrix over the -adics,\ell,"Let $M$ be a $2 \times 2$ matrix with coefficients in $\mathbb{Z}_{\ell}$ whose characteristical polynomial is $$ P(T) = T^2- (a+d) T + (ad-bc).  $$ I've encountered the following assertion: If $P(T)$ factors over $\mathbb{Z}/\ell \mathbb{Z}$ as $$ P(T) = (T-\lambda_1)(T-\lambda_2) $$ with $\lambda_1 \not\equiv \lambda_2 \pmod{\ell}$, then in fact $M$ is diagonalizable over $\mathbb{Z}_{\ell}$. I was able to prove this using a straightforward argument by diagonalizing over $\mathbb{Z}/\ell \mathbb{Z}$ and then explicitly lifting. However, I was wondering if there was a more conceptual way to explain this in terms of Hensel's Lemma or something (i.e. a more conceptual way to package this lifting argument).","Let $M$ be a $2 \times 2$ matrix with coefficients in $\mathbb{Z}_{\ell}$ whose characteristical polynomial is $$ P(T) = T^2- (a+d) T + (ad-bc).  $$ I've encountered the following assertion: If $P(T)$ factors over $\mathbb{Z}/\ell \mathbb{Z}$ as $$ P(T) = (T-\lambda_1)(T-\lambda_2) $$ with $\lambda_1 \not\equiv \lambda_2 \pmod{\ell}$, then in fact $M$ is diagonalizable over $\mathbb{Z}_{\ell}$. I was able to prove this using a straightforward argument by diagonalizing over $\mathbb{Z}/\ell \mathbb{Z}$ and then explicitly lifting. However, I was wondering if there was a more conceptual way to explain this in terms of Hensel's Lemma or something (i.e. a more conceptual way to package this lifting argument).",,"['linear-algebra', 'commutative-algebra', 'modules', 'p-adic-number-theory']"
51,Looking for a counterexample when dropping one of the constraints (Linear algebra),Looking for a counterexample when dropping one of the constraints (Linear algebra),,"I want to find a counterexample for the following ""Theorem"": Let $V \neq 0$ be a finite dimensional $K$ - vector space and $L \subset \mathfrak{gl}(V)$ a linear subspace. If all $x \in L$ are nilpotent as maps $V \rightarrow V$ then there is a $v \in V, v \neq 0$ such that $\forall x \in L: x(v) = 0$ . When $L$ is a Lie subalgebra of $\mathfrak{gl}(V)$ this statement is supposedly true, according to my lecture notes, but not true if it's just a linear subspace of $\mathfrak{gl}(V)$ . I couldn't come up with a counterexample though, so that's why I am asking for help. Certainly counterexamples can't be abelian, as then they'd automatically be Lie subalgebras. So they also, in particular, can not be one dimensional subspaces. I tried to construct something with two nilpotent matrices where their anti-commutator vanishes so that any power of their linear combinations consists of only powers of themselves again, but so far to no avail. There was always a vector in V mapped to 0 by all elements of $L$ Thank you in advance :)","I want to find a counterexample for the following ""Theorem"": Let be a finite dimensional - vector space and a linear subspace. If all are nilpotent as maps then there is a such that . When is a Lie subalgebra of this statement is supposedly true, according to my lecture notes, but not true if it's just a linear subspace of . I couldn't come up with a counterexample though, so that's why I am asking for help. Certainly counterexamples can't be abelian, as then they'd automatically be Lie subalgebras. So they also, in particular, can not be one dimensional subspaces. I tried to construct something with two nilpotent matrices where their anti-commutator vanishes so that any power of their linear combinations consists of only powers of themselves again, but so far to no avail. There was always a vector in V mapped to 0 by all elements of Thank you in advance :)","V \neq 0 K L \subset \mathfrak{gl}(V) x \in L V \rightarrow V v \in V, v \neq 0 \forall x \in L: x(v) = 0 L \mathfrak{gl}(V) \mathfrak{gl}(V) L","['linear-algebra', 'lie-algebras', 'nilpotence']"
52,$\operatorname{SO}(n)$ is an (abstractly) maximal subgroup of $\operatorname{SL}(n)$,is an (abstractly) maximal subgroup of,\operatorname{SO}(n) \operatorname{SL}(n),"Can somebody explain to me a proof that $\operatorname{SO}(n)$ is a maximal subgroup of $\operatorname{SL}(n,\mathbb{R})$ meaning, that if you add one element to $\operatorname{SO}(n)$ , you generate the whole $\operatorname{SL}(n,\mathbb{R})$ ? I am particularly interested in an inductive proof, and the induction step therein.","Can somebody explain to me a proof that is a maximal subgroup of meaning, that if you add one element to , you generate the whole ? I am particularly interested in an inductive proof, and the induction step therein.","\operatorname{SO}(n) \operatorname{SL}(n,\mathbb{R}) \operatorname{SO}(n) \operatorname{SL}(n,\mathbb{R})","['linear-algebra', 'group-theory', 'geometry', 'lie-groups']"
53,Every matrix of the centralizer of the centralizer of a matrix is a polynomial in that matrix,Every matrix of the centralizer of the centralizer of a matrix is a polynomial in that matrix,,"Let $V=M(n,\mathbb C)$ . For a subset $S \subseteq V$ , let $C(S):=\{A \in V | AB=BA, \forall B \in S \}$ . How to prove that for every $A\in V$ , we have $C(C (\{A\})) \subseteq \{ p(A) | p(t) \in \mathbb C[t] \}$ ? My thoughts (motivated by Omnomnomnom's comment ): the reverse inclusion holds. Since both the sets are vector subspaces of $V$ , enough to show their dimensions are equal. Now the dimension of the polynomial set is the degree of the minimal polynomial of $A$ . So enough to show that dim $C(C(A)) \le $ degree of minimal polynomial of $A$ .","Let . For a subset , let . How to prove that for every , we have ? My thoughts (motivated by Omnomnomnom's comment ): the reverse inclusion holds. Since both the sets are vector subspaces of , enough to show their dimensions are equal. Now the dimension of the polynomial set is the degree of the minimal polynomial of . So enough to show that dim degree of minimal polynomial of .","V=M(n,\mathbb C) S \subseteq V C(S):=\{A \in V | AB=BA, \forall B \in S \} A\in V C(C (\{A\})) \subseteq \{ p(A) | p(t) \in \mathbb C[t] \} V A C(C(A)) \le  A","['linear-algebra', 'matrices', 'polynomials', 'ring-theory']"
54,"Is the the $n \times n$ matrix $A_{ij} = (ij + 1)^m$, $m \geq n$ invertible?","Is the the  matrix ,  invertible?",n \times n A_{ij} = (ij + 1)^m m \geq n,"Consider the $n \times n$ symmetric matrix A, whose $ij$-th entry is defined by $A_{ij} = (ij + 1)^m$ and $m \geq n$. Is this matrix invertible? Approaches I've tried: Numeric attempts to find a counterexample over a range of $n$ and $m$ have failed. These experiments did suggest that $A$ may be positive definite. A unisolvence theorem approach does not work, because the $n$ polynomials defining the rows or columns are of order $m$ and evaluated at $n \leq m$ points.","Consider the $n \times n$ symmetric matrix A, whose $ij$-th entry is defined by $A_{ij} = (ij + 1)^m$ and $m \geq n$. Is this matrix invertible? Approaches I've tried: Numeric attempts to find a counterexample over a range of $n$ and $m$ have failed. These experiments did suggest that $A$ may be positive definite. A unisolvence theorem approach does not work, because the $n$ polynomials defining the rows or columns are of order $m$ and evaluated at $n \leq m$ points.",,"['linear-algebra', 'matrices', 'inverse']"
55,Subspaces of matrices whose determinant is $0$,Subspaces of matrices whose determinant is,0,Consider matrices of size $n\times n$ over finite field $\mathbb{F}_2$. It is linear space of dimension $n^2$. \begin{bmatrix}     x_{11}       & x_{12}  & \dots & x_{1n} \\     x_{21}       & x_{22}  & \dots & x_{2n} \\      \\     x_{n1}       & x_{n2}  & \dots & x_{nn} \end{bmatrix} Now consider set of matrices for which some fixed rows are linearly dependent. For example: let $L$ is set of matrices for which first two rows are the same. It is clear that $L$ is a linear subspace of dimension $n^2-n$ and for all matrices of $L$ determinant is equal to $0$. I am interested if the opposite is true. Let $L$ is subspace of dimension $n^2-n$ and determinant is $0$ for all matrices of $L$. Can we say that we can fix some set of rows which are linearly dependant for all matrices of $L$. One can see that there are $2^n-1$ linear combination of rows. So the question is if there are any other subspaces of dimension $n^2-n$ whose determinant is $0$ or each of such subspaces is identified by some linear combination of rows.,Consider matrices of size $n\times n$ over finite field $\mathbb{F}_2$. It is linear space of dimension $n^2$. \begin{bmatrix}     x_{11}       & x_{12}  & \dots & x_{1n} \\     x_{21}       & x_{22}  & \dots & x_{2n} \\      \\     x_{n1}       & x_{n2}  & \dots & x_{nn} \end{bmatrix} Now consider set of matrices for which some fixed rows are linearly dependent. For example: let $L$ is set of matrices for which first two rows are the same. It is clear that $L$ is a linear subspace of dimension $n^2-n$ and for all matrices of $L$ determinant is equal to $0$. I am interested if the opposite is true. Let $L$ is subspace of dimension $n^2-n$ and determinant is $0$ for all matrices of $L$. Can we say that we can fix some set of rows which are linearly dependant for all matrices of $L$. One can see that there are $2^n-1$ linear combination of rows. So the question is if there are any other subspaces of dimension $n^2-n$ whose determinant is $0$ or each of such subspaces is identified by some linear combination of rows.,,"['linear-algebra', 'matrices', 'determinant']"
56,Barycentric coordinates in a triangle - proof,Barycentric coordinates in a triangle - proof,,"I want to prove that the barycentric coordinates of a point $P$ inside the triangle with vertices in $(1,0,0), (0,1,0), (0,0,1)$ are distances from $P$ to the sides of the triangle. Let's denote the triangle by $ABC, \ A = (1,0,0), B=(0,1,0), C= (0,0,1)$. We consider triangles $ABP, \ BCP, \ CAP$. The barycentric coordinates of $P$ will then be $(h_1, h_2, h_3)$ where $h_1$ is the height of $ABP$, $h_2 \rightarrow BCP$, $h_3 \rightarrow CAP$ I know that $h_1 = \frac{S_{ABP}}{S_{ABC}}$ and similarly for $h_2, \ h_3$ My problem is that I don't know how to prove that if $P= (p_1, p_2, p_3)$ then $(h_1 + h_2 + h_3)P = h_1 (1,0,0) + h_2 (0,1,0) + h_3 (0,0,1)$ Could you tell me what to do about it? Thank you!","I want to prove that the barycentric coordinates of a point $P$ inside the triangle with vertices in $(1,0,0), (0,1,0), (0,0,1)$ are distances from $P$ to the sides of the triangle. Let's denote the triangle by $ABC, \ A = (1,0,0), B=(0,1,0), C= (0,0,1)$. We consider triangles $ABP, \ BCP, \ CAP$. The barycentric coordinates of $P$ will then be $(h_1, h_2, h_3)$ where $h_1$ is the height of $ABP$, $h_2 \rightarrow BCP$, $h_3 \rightarrow CAP$ I know that $h_1 = \frac{S_{ABP}}{S_{ABC}}$ and similarly for $h_2, \ h_3$ My problem is that I don't know how to prove that if $P= (p_1, p_2, p_3)$ then $(h_1 + h_2 + h_3)P = h_1 (1,0,0) + h_2 (0,1,0) + h_3 (0,0,1)$ Could you tell me what to do about it? Thank you!",,"['linear-algebra', 'geometry']"
57,On two special kind of invertible similar matrices with rational entries,On two special kind of invertible similar matrices with rational entries,,"Let $A,B \in GL(n, \mathbb Q)$ be two similar matrices i.e. there exists $X \in GL(n, \mathbb Q)$ with $XAX^{-1}=B.$ If there is an integer $s$ such that $A^{s+1}B=BA^s$ , then how to prove that $A,B$ are identity matrices?","Let be two similar matrices i.e. there exists with If there is an integer such that , then how to prove that are identity matrices?","A,B \in GL(n, \mathbb Q) X \in GL(n, \mathbb Q) XAX^{-1}=B. s A^{s+1}B=BA^s A,B","['linear-algebra', 'matrices', 'linear-transformations', 'similar-matrices']"
58,Motivation/intuition behind using linear algebra behind these combinatorics problem,Motivation/intuition behind using linear algebra behind these combinatorics problem,,"What is the motivation behind using linear algebra in these three problems ? A pair $(m,n)$ is called nice if there is a directed graph with (self edge are allowed, but multiple edge are not allowed) $n$ vertices such that for every pair of vertices is connected by exactly $m$ paths of lenght $2$ . Prove $(m,n)$ is nice iff $m \leq n$ and $\sqrt{mn} \in \mathbb{Z}$ . Let $S$ be a finite subset of $[0,1]$ containing $0$ and $1$ and such that every distance that occurs betwen paris of elements occurs at least twice, except for the distance $1$ . Prove that $S$ contains only rational numbers. Prove that $n$ distnict points, not all of them lying on a line, determine atleast $n$ distnict lines ? By ""motivation"", I mean suppose you are good in linear algebra, but you don't know how to use linear algebra in combinatorics. What structure in the problem would trigger you that linear algebra is a good tool in using that ? I understand the solutions using linear algebra to all these above three problems, but I have a hard time figuring out how one could have think of them in the first place - I mean, linear transformation between vector spaces and directed graph are two completely different things, how can one see the connection between them ? As an example of what I mean, consider this problem: Let $n$ be an even integer. How many subsets of the set $\{1,2,\dots,n\}$ can you pick if they all have to have odd size but the intersection of any two of them has to have even size? For this problem, this by Fields Medallist Timothy Gowers gives a really good motivation of how one can think of using linear algebra (I'm wanting this kind of answer) If you continue experimenting in this way, you will find the same thing every time: if you have chosen fewer than $n$ sets then you can choose more, but once you get to n sets then you get stuck. But there seems to be a great deal of freedom about how you choose the sets. This contrasts with many problems in extremal combinatorics, where the best possible examples are often unique, or unique up to some obvious symmetry. (A good example of the latter: how many subsets of $\{1,2,\dots,n\}$ can you choose if none of your subsets is contained in any other? The unique best possible collection of sets is all sets of size n/2.) Are there any other circumstances where you have a collection of objects, and a condition on pairs $(x,y)$ of those objects, such that (i) whenever you have chosen objects $x_1,x_2,\dots,x_m$ with $m<n$ there are many different ways of choosing y such that the condition holds for each pair $(x_i,y)$ , and (ii) it is impossible to choose more than n of them if any two have to satisfy the condition? Yes there are: a common one is when the objects are vectors in $\mathbb{R}^n$ and the condition on a pair $(x,y)$ is that x and y should be orthogonal. Since orthogonality implies independence, we cannot choose more than n non-zero vectors that are all orthogonal to each other. Can we relate these two situations?","What is the motivation behind using linear algebra in these three problems ? A pair is called nice if there is a directed graph with (self edge are allowed, but multiple edge are not allowed) vertices such that for every pair of vertices is connected by exactly paths of lenght . Prove is nice iff and . Let be a finite subset of containing and and such that every distance that occurs betwen paris of elements occurs at least twice, except for the distance . Prove that contains only rational numbers. Prove that distnict points, not all of them lying on a line, determine atleast distnict lines ? By ""motivation"", I mean suppose you are good in linear algebra, but you don't know how to use linear algebra in combinatorics. What structure in the problem would trigger you that linear algebra is a good tool in using that ? I understand the solutions using linear algebra to all these above three problems, but I have a hard time figuring out how one could have think of them in the first place - I mean, linear transformation between vector spaces and directed graph are two completely different things, how can one see the connection between them ? As an example of what I mean, consider this problem: Let be an even integer. How many subsets of the set can you pick if they all have to have odd size but the intersection of any two of them has to have even size? For this problem, this by Fields Medallist Timothy Gowers gives a really good motivation of how one can think of using linear algebra (I'm wanting this kind of answer) If you continue experimenting in this way, you will find the same thing every time: if you have chosen fewer than sets then you can choose more, but once you get to n sets then you get stuck. But there seems to be a great deal of freedom about how you choose the sets. This contrasts with many problems in extremal combinatorics, where the best possible examples are often unique, or unique up to some obvious symmetry. (A good example of the latter: how many subsets of can you choose if none of your subsets is contained in any other? The unique best possible collection of sets is all sets of size n/2.) Are there any other circumstances where you have a collection of objects, and a condition on pairs of those objects, such that (i) whenever you have chosen objects with there are many different ways of choosing y such that the condition holds for each pair , and (ii) it is impossible to choose more than n of them if any two have to satisfy the condition? Yes there are: a common one is when the objects are vectors in and the condition on a pair is that x and y should be orthogonal. Since orthogonality implies independence, we cannot choose more than n non-zero vectors that are all orthogonal to each other. Can we relate these two situations?","(m,n) n m 2 (m,n) m \leq n \sqrt{mn} \in \mathbb{Z} S [0,1] 0 1 1 S n n n \{1,2,\dots,n\} n \{1,2,\dots,n\} (x,y) x_1,x_2,\dots,x_m m<n (x_i,y) \mathbb{R}^n (x,y)","['linear-algebra', 'combinatorics', 'algebraic-combinatorics']"
59,"Prove that the group $\mathrm{GL}(n, \mathbb{Z})$ is finitely generated [duplicate]",Prove that the group  is finitely generated [duplicate],"\mathrm{GL}(n, \mathbb{Z})","This question already has answers here : What is the easiest way to generate $\mathrm{GL}(n,\mathbb Z)$? (2 answers) Closed 4 years ago . Knowing that for $n \geq 2$, $\mathrm{GL}(n, \mathbb{Z}) = \big\{ A \in \mathrm{M}_{n,n}(\mathbb{Z}) \mid \det(A) \in \{ 1, −1 \} \big\}$ is a group with respect to matrix multiplication, prove that for every integer $n \geq 2$ the group $\mathrm{GL}(n, \mathbb{Z})$ is finitely generated. If I prove that  $\mathrm{GL}(n, \mathbb{Z})$ has finite subgroups does that mean it has a finite set of generators so that it is finitely generated?","This question already has answers here : What is the easiest way to generate $\mathrm{GL}(n,\mathbb Z)$? (2 answers) Closed 4 years ago . Knowing that for $n \geq 2$, $\mathrm{GL}(n, \mathbb{Z}) = \big\{ A \in \mathrm{M}_{n,n}(\mathbb{Z}) \mid \det(A) \in \{ 1, −1 \} \big\}$ is a group with respect to matrix multiplication, prove that for every integer $n \geq 2$ the group $\mathrm{GL}(n, \mathbb{Z})$ is finitely generated. If I prove that  $\mathrm{GL}(n, \mathbb{Z})$ has finite subgroups does that mean it has a finite set of generators so that it is finitely generated?",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'finitely-generated']"
60,How to solve linear system of form $(A \otimes B + C^{T}C)x = b$ when $A \otimes B$ is too large to compute?,How to solve linear system of form  when  is too large to compute?,(A \otimes B + C^{T}C)x = b A \otimes B,"For the given linear system: $$(A \otimes B + C^{T}C)x = b$$ where $\otimes$ is the Kronecker product , $A$ and $B$ are dense and symmetric positive-definite, and $C^{T}C$ is a sparse symmetric block diagonal, is there a way I can determine $x$ in a way that takes advantage of the symmetric block structure of $A \otimes B$ and $C^{T}C$ without requiring explicit computation of those terms? I'm particularly interested in the case where the sizes of the diagonal blocks in $C^{T}C$ are the same size as $B$. I have attempted to implement the conjugate gradient method but have found it converges far too slowly for my purposes, so was looking to see if there are any alternative techniques I have overlooked. Many thanks for any assistance.","For the given linear system: $$(A \otimes B + C^{T}C)x = b$$ where $\otimes$ is the Kronecker product , $A$ and $B$ are dense and symmetric positive-definite, and $C^{T}C$ is a sparse symmetric block diagonal, is there a way I can determine $x$ in a way that takes advantage of the symmetric block structure of $A \otimes B$ and $C^{T}C$ without requiring explicit computation of those terms? I'm particularly interested in the case where the sizes of the diagonal blocks in $C^{T}C$ are the same size as $B$. I have attempted to implement the conjugate gradient method but have found it converges far too slowly for my purposes, so was looking to see if there are any alternative techniques I have overlooked. Many thanks for any assistance.",,"['linear-algebra', 'matrices', 'numerical-linear-algebra', 'kronecker-product']"
61,Exercise on representations,Exercise on representations,,"I am stuck on an exercise in Serre, Abelian $\ell$-adic representations (first exercise of chapter 1). Let $V$ be a vector space of dimension $2$, and $H$ a subgroup of $GL(V)$ such that $\det(1-h)=0$ for all $h \in H$. Show that in some basis $H$ is a subgroup of either $\begin{pmatrix} 1 & * \\ 0 &* \\ \end{pmatrix}$ or $\begin{pmatrix} 1 & 0 \\ * &* \\ \end{pmatrix}$. I know that this means that there is a subspace or a quotient of $V$ on which $H$ acts trivially, and I know it is enough to show that $V$ is not irreducible as representation of $H$, but I don't know how to do it.","I am stuck on an exercise in Serre, Abelian $\ell$-adic representations (first exercise of chapter 1). Let $V$ be a vector space of dimension $2$, and $H$ a subgroup of $GL(V)$ such that $\det(1-h)=0$ for all $h \in H$. Show that in some basis $H$ is a subgroup of either $\begin{pmatrix} 1 & * \\ 0 &* \\ \end{pmatrix}$ or $\begin{pmatrix} 1 & 0 \\ * &* \\ \end{pmatrix}$. I know that this means that there is a subspace or a quotient of $V$ on which $H$ acts trivially, and I know it is enough to show that $V$ is not irreducible as representation of $H$, but I don't know how to do it.",,"['linear-algebra', 'representation-theory']"
62,how prove the following statment for this matrix.,how prove the following statment for this matrix.,,"Let $A:=[a_{ij}]_{n×n}$ , $a_{ij}=0$  or $a_{ij}=1$ and  $\exists m \in\mathbb N$ such that $A^m=J-I$, where $I$ is the identity matrix and $J=[1]_{n×n}$ (each entry is $1$). How to prove: $\exists a \in\mathbb N$  such that $n=a^m+1$, and $m$ is odd. Thanks in advance.","Let $A:=[a_{ij}]_{n×n}$ , $a_{ij}=0$  or $a_{ij}=1$ and  $\exists m \in\mathbb N$ such that $A^m=J-I$, where $I$ is the identity matrix and $J=[1]_{n×n}$ (each entry is $1$). How to prove: $\exists a \in\mathbb N$  such that $n=a^m+1$, and $m$ is odd. Thanks in advance.",,"['linear-algebra', 'matrices']"
63,Understanding the absolute value of a matrix.,Understanding the absolute value of a matrix.,,"I believe that the absolute value of a matrix is defined as  $$ |A|=\sqrt{A^{\dagger}A} \ . $$ But the square root of a matrix is not unique wikipedia gives a list of examples to illustrate this. To understand this, how does one work out the absolute value of: $$ A=\begin{pmatrix}1 & 0\\0 & -1\end{pmatrix} $$ Clearly $A^{\dagger}=A$ so $|A|=\sqrt{A^2}$, but this is not necessarily $A$. I want to pick the identity in this case, since then the eigenvalues of $|A|$ are both 1 (and they were $\pm1$ for $A$). But mathematics is not about what I want. So what is $|A|$? Is it well-defined? And how do I do this operation in general, since my application for this is of course far more complex.","I believe that the absolute value of a matrix is defined as  $$ |A|=\sqrt{A^{\dagger}A} \ . $$ But the square root of a matrix is not unique wikipedia gives a list of examples to illustrate this. To understand this, how does one work out the absolute value of: $$ A=\begin{pmatrix}1 & 0\\0 & -1\end{pmatrix} $$ Clearly $A^{\dagger}=A$ so $|A|=\sqrt{A^2}$, but this is not necessarily $A$. I want to pick the identity in this case, since then the eigenvalues of $|A|$ are both 1 (and they were $\pm1$ for $A$). But mathematics is not about what I want. So what is $|A|$? Is it well-defined? And how do I do this operation in general, since my application for this is of course far more complex.",,"['linear-algebra', 'matrices', 'absolute-value']"
64,"Let $A\in M_n(\Bbb R)$ prove that, $\|A^n\|\le \frac{n}{\ln 2}\|A\|^{n-1}$ when $\lambda_i<1.$","Let  prove that,  when",A\in M_n(\Bbb R) \|A^n\|\le \frac{n}{\ln 2}\|A\|^{n-1} \lambda_i<1.,Let $n\in \Bbb N$ be fixed and $A$  be a $n\times n$  complex matrix whose eigenvalues $(\lambda_i)$ satisfy $|\lambda_i|\lt 1$. Prove that:   $$ \|A^n\|\le \frac{n}{\ln 2}\|A\|^{n-1} $$ where for any matrix $B$  $$ \|B\|:= \sup_{\|x\|= 1}\|Bx\| ~~~\text{with}~~\|x\|^2 = \sum^{n}_{i=1} |x_i|^2 $$ Edit: Here $n$ is fixed and represent the dimension of our space and matrix as well. I also thought it could be done by induction but this was  a wrong way. l don't know how to proceed.,Let $n\in \Bbb N$ be fixed and $A$  be a $n\times n$  complex matrix whose eigenvalues $(\lambda_i)$ satisfy $|\lambda_i|\lt 1$. Prove that:   $$ \|A^n\|\le \frac{n}{\ln 2}\|A\|^{n-1} $$ where for any matrix $B$  $$ \|B\|:= \sup_{\|x\|= 1}\|Bx\| ~~~\text{with}~~\|x\|^2 = \sum^{n}_{i=1} |x_i|^2 $$ Edit: Here $n$ is fixed and represent the dimension of our space and matrix as well. I also thought it could be done by induction but this was  a wrong way. l don't know how to proceed.,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'contest-math']"
65,Matrix multiplication of columns times rows instead of rows times columns,Matrix multiplication of columns times rows instead of rows times columns,,"In ordinary matrix multiplication $AB$ where we multiply each column $b_{i}$ by $A$, each resulting column of $AB$ can be viewed as a linear combination of $A$. If however if we decided to multiply each column of $A$ by each row of $B$, we  get an entire matrix for each column-row multiply. My question is: Does each matrix resulting from an outer product have any known meaning aside from being a part of the sum(a summand?) of the final $AB$? Edit: Say we have $AB$ $$ \left( \begin{array}{ccc} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{array} \right) \left( \begin{array}{ccc} 10 & 11 & 12 \\ 13 & 14 & 15 \\ 15 & 16 & 17 \end{array} \right) $$ Normally we would multiply each column of $B$ by A and get a linear combination of A , e.g. $$10\left( \begin{array}{c} 1 \\ 4\\ 7 \end{array} \right)+ 13\left( \begin{array}{c} 2 \\ 5\\ 8 \end{array} \right)+ 15\left( \begin{array}{c} 3 \\ 6\\ 9 \end{array} \right)$$ which is one column of $AB$. If however we multiply each column of $A$ by each row of $B$, e.g. $$\left( \begin{array}{c} 1 \\ 4\\ 7 \end{array} \right)\left( \begin{array}{ccc} 10 & 11 & 12 \end{array} \right)$$ we get a matrix. Each of the 3 matrices $a_{i}b_{i}^{T}$ summed together gives us $AB$. I was wondering if each individual matrix that sums to $AB$ has any sort of special meaning. This second way of performing multiplication also seems to be called column-row expansion. ( http://www.math.nyu.edu/~neylon/linalgfall04/project1/dj/crexpansion.htm ). I actually read about it in I believe section 2.4 of Strang's Introduction to Linear Algebra book. He mentions that not everybody is aware that matrix multiplication can be performed in this way.","In ordinary matrix multiplication $AB$ where we multiply each column $b_{i}$ by $A$, each resulting column of $AB$ can be viewed as a linear combination of $A$. If however if we decided to multiply each column of $A$ by each row of $B$, we  get an entire matrix for each column-row multiply. My question is: Does each matrix resulting from an outer product have any known meaning aside from being a part of the sum(a summand?) of the final $AB$? Edit: Say we have $AB$ $$ \left( \begin{array}{ccc} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{array} \right) \left( \begin{array}{ccc} 10 & 11 & 12 \\ 13 & 14 & 15 \\ 15 & 16 & 17 \end{array} \right) $$ Normally we would multiply each column of $B$ by A and get a linear combination of A , e.g. $$10\left( \begin{array}{c} 1 \\ 4\\ 7 \end{array} \right)+ 13\left( \begin{array}{c} 2 \\ 5\\ 8 \end{array} \right)+ 15\left( \begin{array}{c} 3 \\ 6\\ 9 \end{array} \right)$$ which is one column of $AB$. If however we multiply each column of $A$ by each row of $B$, e.g. $$\left( \begin{array}{c} 1 \\ 4\\ 7 \end{array} \right)\left( \begin{array}{ccc} 10 & 11 & 12 \end{array} \right)$$ we get a matrix. Each of the 3 matrices $a_{i}b_{i}^{T}$ summed together gives us $AB$. I was wondering if each individual matrix that sums to $AB$ has any sort of special meaning. This second way of performing multiplication also seems to be called column-row expansion. ( http://www.math.nyu.edu/~neylon/linalgfall04/project1/dj/crexpansion.htm ). I actually read about it in I believe section 2.4 of Strang's Introduction to Linear Algebra book. He mentions that not everybody is aware that matrix multiplication can be performed in this way.",,"['linear-algebra', 'matrices']"
66,Eigenvalues of doubly stochastic matrices,Eigenvalues of doubly stochastic matrices,,"There was a long standing conjecture stating that the geometric location of eigenvalues of doubly stochastic matrices of order $n$ is exactly the union of regular $k$-gons anchored at $1$ in the unit disc for $2 \leq k \leq n$. Mashreghi and Rivard showed that this conjecture is wrong for $n = 5$, cf. Linear and Multilinear Algebra, Volume 55, Number 5, September 2007 , pp. 491-498. Have we made progress since then, beyond $n=5$, or for $n=4$? ($n=2,3$ is pretty simple).","There was a long standing conjecture stating that the geometric location of eigenvalues of doubly stochastic matrices of order $n$ is exactly the union of regular $k$-gons anchored at $1$ in the unit disc for $2 \leq k \leq n$. Mashreghi and Rivard showed that this conjecture is wrong for $n = 5$, cf. Linear and Multilinear Algebra, Volume 55, Number 5, September 2007 , pp. 491-498. Have we made progress since then, beyond $n=5$, or for $n=4$? ($n=2,3$ is pretty simple).",,"['linear-algebra', 'spectral-theory']"
67,"If square matrices $A^2 + B^2 = 2AB$, then prove that $p_A(x) = p_B(x)$","If square matrices , then prove that",A^2 + B^2 = 2AB p_A(x) = p_B(x),"Original problem statement: Let $A, B \in M_n(\mathbb{C})$ such that $A^2 + B^2 = 2AB$ . Prove that for any $x \in \mathbb{C}$ : $$det(A - xI_n) = det(B-xI_n)$$ Now the first observation, the equality that has to be proven is the definition of the characteristic polynomials of both matrices being equivalent, i.e. $p_A(x) = p_B(x)$ . My first thought seeing this was that I had to prove that $A \sim B$ (i.e. $A$ is similar to $B$ ), which in turn would imply equivalent characteristic polynomials. I tried some algebraic manipulations to the equality given in order to utilize Jordan canonical form and somehow reach similarity, but I didn't get anywhere, so maybe that's futile. I've got a hunch that it's something way simpler, but clever. One can also observe that we can express the commutator of both matrices: $$[A, B] := AB - BA = \stackrel{\text{from the given}}{\dots} = (A - B)^2$$ but I'm not that knowledgeable in terms of commutators, so I couldn't find anything useful following from that. Any hints are welcome (and I'd be glad to have a hint only, not a full solution... I need to figure it out for myself a bit, too :D)! Thank you in advance.","Original problem statement: Let such that . Prove that for any : Now the first observation, the equality that has to be proven is the definition of the characteristic polynomials of both matrices being equivalent, i.e. . My first thought seeing this was that I had to prove that (i.e. is similar to ), which in turn would imply equivalent characteristic polynomials. I tried some algebraic manipulations to the equality given in order to utilize Jordan canonical form and somehow reach similarity, but I didn't get anywhere, so maybe that's futile. I've got a hunch that it's something way simpler, but clever. One can also observe that we can express the commutator of both matrices: but I'm not that knowledgeable in terms of commutators, so I couldn't find anything useful following from that. Any hints are welcome (and I'd be glad to have a hint only, not a full solution... I need to figure it out for myself a bit, too :D)! Thank you in advance.","A, B \in M_n(\mathbb{C}) A^2 + B^2 = 2AB x \in \mathbb{C} det(A - xI_n) = det(B-xI_n) p_A(x) = p_B(x) A \sim B A B [A, B] := AB - BA = \stackrel{\text{from the given}}{\dots} = (A - B)^2","['linear-algebra', 'matrices', 'matrix-equations', 'characteristic-polynomial']"
68,"Given $2^{n-1}$ subsets of a set with $n$ elements with the property that any three have nonempty intersection, prove that ....","Given  subsets of a set with  elements with the property that any three have nonempty intersection, prove that ....",2^{n-1} n,"Given a family of $2^{n-1}$ subsets of a set $S=\{1,2,3,...,n-1,n\}$ with the property that any three have nonempty intersection. Prove that the intersection of all the sets in this family is nonempty. I'm interested if the finishing of the solution here can be done like this: As there was constructed $W$ is $n-1$ dimensional set in $\mathbb{F}_2^n$ . Thus we have $W^{\bot}\oplus W =\mathbb{F}_2^n$ and $\dim W^{\bot} =1$ , so $W^{\bot} = \{0,v\}$ for some vector $v\ne 0$ . We can assume WLOG $$v = (\underbrace{1,1,1,...,1}_k,0,0,...,0)$$ Now each vector can be uniquely expressed with $v$ and some $w\in W$ , in particulary we have $$e_1=(1,0,0,...0) =av+w$$ for some $w\in W$ and $a \in \mathbb{F}_2$ . So if $a=0$ then $e_1\in W$ so $0=v\cdot e_1 =1$ which is impossible. Thus $a=1$ then $e_1-v \in W$ so $0=v\cdot (e_1-v) =1-k$ which means $k$ is odd. So each vector not in $W$ is generated with odd number of vectors in $\{e_1,e_2,...e_k\}$ and an arbitrary number of vectors in $\{e_{k+1},e_{k+2},...e_n\}$ where $e_1,e_2,...,e_n$ represents the standard basis of $\mathbb{F}_2^n$ . Since the number of vectors not in $W$ is $2^{k-1} \cdot 2^{n-k} = 2^{n-1}$ , we see if $k\geq 3$ then we have in $e_1$ and $e_2$ in $W^C$ so sets $\{1\}$ and $\{2\}$ are in $\mathcal{F}$ which is impossible. So $k=1$ and all sets in $\mathcal{F}$ have $1$ in common. Non linear algebra solution: Prove that the intersection of all the sets is nonempty.","Given a family of subsets of a set with the property that any three have nonempty intersection. Prove that the intersection of all the sets in this family is nonempty. I'm interested if the finishing of the solution here can be done like this: As there was constructed is dimensional set in . Thus we have and , so for some vector . We can assume WLOG Now each vector can be uniquely expressed with and some , in particulary we have for some and . So if then so which is impossible. Thus then so which means is odd. So each vector not in is generated with odd number of vectors in and an arbitrary number of vectors in where represents the standard basis of . Since the number of vectors not in is , we see if then we have in and in so sets and are in which is impossible. So and all sets in have in common. Non linear algebra solution: Prove that the intersection of all the sets is nonempty.","2^{n-1} S=\{1,2,3,...,n-1,n\} W n-1 \mathbb{F}_2^n W^{\bot}\oplus W =\mathbb{F}_2^n \dim W^{\bot} =1 W^{\bot} = \{0,v\} v\ne 0 v =
(\underbrace{1,1,1,...,1}_k,0,0,...,0) v w\in W e_1=(1,0,0,...0) =av+w w\in W a \in
\mathbb{F}_2 a=0 e_1\in W 0=v\cdot e_1 =1 a=1 e_1-v \in W 0=v\cdot
(e_1-v) =1-k k W \{e_1,e_2,...e_k\} \{e_{k+1},e_{k+2},...e_n\} e_1,e_2,...,e_n \mathbb{F}_2^n W 2^{k-1} \cdot 2^{n-k} =
2^{n-1} k\geq 3 e_1 e_2 W^C \{1\} \{2\} \mathcal{F} k=1 \mathcal{F} 1","['linear-algebra', 'combinatorics', 'solution-verification', 'contest-math', 'extremal-combinatorics']"
69,"Self-adjoint operators, distributions and spectral theorem","Self-adjoint operators, distributions and spectral theorem",,"Suppose we have a self-adjoint operator, $L$ ,  which maps elements $u \in C^2 \to C^2$ on $(0,\infty)$ using the standard inner product. Furthermore, we have a trigonometric system over an infinite set of real numbers, $\theta_i$ , which converges to $L$ as a distribution- when integrated against a test function. What i mean by this is that suppose, $ Lu=v$ , then we have, for example, a cosine series over all $\theta_i$ , which takes in $u$ and outputs a distribution which converges to $v$ . Perhaps something like $$\sum_{i=1}^{\infty}\omega_i\cos(\theta_i) \approx{Lu}$$ $$ \omega_i=\langle\cos(\theta_i),u\rangle$$ where $\approx$ is only achieved after integrating the LHS against a test function. Here are my questions: is it possible to use the spectral theorem to draw conclusions about the set of numbers, $\theta_i$ ? Specifically, can we say that the elements in the set are unique? while the cosine series is orthogonal, i am unsure if we can consider them eigenfunctions as the convergence is only as a distribution and is only achieved after we integrate against a test function. is it ever possible to have a self-adjoint operator which sends distributions to distributions? my instinct says no, because we can not take inner product of two distributions. EDIT (based off of comments from paul garrett): Let's assume that $C^2$ is the space of all twice-continuously-differentiable functions and the 'standard inner product' is $\langle f,g\rangle=\int_{0}^{\infty}f(x)g(x)dx$ . Furthermore, $L$ is not necessarily bounded. paul garrett also pointed out that the distinct consines in the example are problematic because they are not in $L^2$ . The essence of my confusion is whether or not these $\theta$ 's are unique, or if a collection of functions can be regarded as eigenfunctions if they only converge as a distribution- eg. the cosines above will not converge to $Lu$ , but we can prove that they will as a distribution.","Suppose we have a self-adjoint operator, ,  which maps elements on using the standard inner product. Furthermore, we have a trigonometric system over an infinite set of real numbers, , which converges to as a distribution- when integrated against a test function. What i mean by this is that suppose, , then we have, for example, a cosine series over all , which takes in and outputs a distribution which converges to . Perhaps something like where is only achieved after integrating the LHS against a test function. Here are my questions: is it possible to use the spectral theorem to draw conclusions about the set of numbers, ? Specifically, can we say that the elements in the set are unique? while the cosine series is orthogonal, i am unsure if we can consider them eigenfunctions as the convergence is only as a distribution and is only achieved after we integrate against a test function. is it ever possible to have a self-adjoint operator which sends distributions to distributions? my instinct says no, because we can not take inner product of two distributions. EDIT (based off of comments from paul garrett): Let's assume that is the space of all twice-continuously-differentiable functions and the 'standard inner product' is . Furthermore, is not necessarily bounded. paul garrett also pointed out that the distinct consines in the example are problematic because they are not in . The essence of my confusion is whether or not these 's are unique, or if a collection of functions can be regarded as eigenfunctions if they only converge as a distribution- eg. the cosines above will not converge to , but we can prove that they will as a distribution.","L u \in C^2 \to C^2 (0,\infty) \theta_i L  Lu=v \theta_i u v \sum_{i=1}^{\infty}\omega_i\cos(\theta_i) \approx{Lu}  \omega_i=\langle\cos(\theta_i),u\rangle \approx \theta_i C^2 \langle f,g\rangle=\int_{0}^{\infty}f(x)g(x)dx L L^2 \theta Lu","['linear-algebra', 'functional-analysis', 'eigenvalues-eigenvectors', 'linear-transformations', 'distribution-theory']"
70,Minimal polynomial of $T(X)=A^{-1}XA$,Minimal polynomial of,T(X)=A^{-1}XA,"Let $A$ be an invertible, diagonalizable matrix and let $V$ be the space of $n \times n$ matrices. Define $T\colon V\to V$ be $T(X)=A^{-1}XA$. Find the eigenvalues, minimal, and characteristic polynomial of $T$. I think I have what the eigenvalues could be. If we let $A=QDQ^{-1}$, where $D$ is diagonal with entries $\lambda_1,\dots,\lambda_n$ , then $T(X)=\lambda X$ is equivalent to $D^{-1}ZD=\lambda Z$, where $Z=Q^{-1}XQ$. Hence, comparing entries on the left and right, I get that $\lambda z_{ii}=z_{ii}$ for each $i$, and if $i\not=j$, $\lambda z_{ij}=\frac{\lambda_j}{\lambda_i}z_{ij}$. So the possible eigenvalues for $T$ are $\lambda=1$ or $\lambda=\frac{\lambda_j}{\lambda_i}$, $i\not=j$. From here though, I don't see how to find the minimal or characteristic polynomial of $T$.","Let $A$ be an invertible, diagonalizable matrix and let $V$ be the space of $n \times n$ matrices. Define $T\colon V\to V$ be $T(X)=A^{-1}XA$. Find the eigenvalues, minimal, and characteristic polynomial of $T$. I think I have what the eigenvalues could be. If we let $A=QDQ^{-1}$, where $D$ is diagonal with entries $\lambda_1,\dots,\lambda_n$ , then $T(X)=\lambda X$ is equivalent to $D^{-1}ZD=\lambda Z$, where $Z=Q^{-1}XQ$. Hence, comparing entries on the left and right, I get that $\lambda z_{ii}=z_{ii}$ for each $i$, and if $i\not=j$, $\lambda z_{ij}=\frac{\lambda_j}{\lambda_i}z_{ij}$. So the possible eigenvalues for $T$ are $\lambda=1$ or $\lambda=\frac{\lambda_j}{\lambda_i}$, $i\not=j$. From here though, I don't see how to find the minimal or characteristic polynomial of $T$.",,"['linear-algebra', 'vector-spaces', 'eigenvalues-eigenvectors', 'minimal-polynomials']"
71,Generalisation of prime numbers to matrices?,Generalisation of prime numbers to matrices?,,"Is it possible to generalise prime numbers to matrices? I'm trying to solve a Rubix cube in the minimum number of steps and I think this would be useful. I think it's possible to represent Rubix cube operations in the language of linear algebra or matrices. From there, maybe I can represent a solution of the Rubix cube as a product of matrices. Transforming a product of matrices into its minimum decomposition (this is where the prime version of matrices come in) should provide a 'minimum' solution. Disclosure: this is just my intuition and I understand completely if what I just wrote doesn't make much sense).","Is it possible to generalise prime numbers to matrices? I'm trying to solve a Rubix cube in the minimum number of steps and I think this would be useful. I think it's possible to represent Rubix cube operations in the language of linear algebra or matrices. From there, maybe I can represent a solution of the Rubix cube as a product of matrices. Transforming a product of matrices into its minimum decomposition (this is where the prime version of matrices come in) should provide a 'minimum' solution. Disclosure: this is just my intuition and I understand completely if what I just wrote doesn't make much sense).",,"['linear-algebra', 'matrices', 'elementary-number-theory', 'prime-numbers']"
72,Adjoint functors and the classical adjoint,Adjoint functors and the classical adjoint,,"Is there any relationship between adjoint functors seen in category theory, and the classical adjoint (as in adjoint matrices)?","Is there any relationship between adjoint functors seen in category theory, and the classical adjoint (as in adjoint matrices)?",,"['linear-algebra', 'category-theory', 'adjoint-functors']"
73,"If $A$ has eigenvalues $\lambda_1,...,\lambda_n$, is there a relationship between the eigenvalues of $A$ and $\hat{A}$","If  has eigenvalues , is there a relationship between the eigenvalues of  and","A \lambda_1,...,\lambda_n A \hat{A}","Suppose a square matrix $A$ has eigenvalues $\lambda_1,...,\lambda_n$. Note that A is a $n\times n$ matrix where $n$ is even. Let $\widehat{A}$ be a matrix that is obtained from $A$ only by multiplying every second row in $A$ by $-1$. Is there a relationship between the eigenvalues of $A$ and $\widehat{A}$? I don't think there is, but I am not sure.","Suppose a square matrix $A$ has eigenvalues $\lambda_1,...,\lambda_n$. Note that A is a $n\times n$ matrix where $n$ is even. Let $\widehat{A}$ be a matrix that is obtained from $A$ only by multiplying every second row in $A$ by $-1$. Is there a relationship between the eigenvalues of $A$ and $\widehat{A}$? I don't think there is, but I am not sure.",,['linear-algebra']
74,What is the precise mathematical definition of what a wavelet is and what is its relation to linear algebra?,What is the precise mathematical definition of what a wavelet is and what is its relation to linear algebra?,,"I was reading on wavelets and it seems that its hard to find a precise mathematical definition of what this concept is. My confusion first arose due to Gilbert Stang's linear algebra book. In particular consider the following extract: It talks about how to change a vector from one basis to another but it never rigorously defines what a wavelet is (by the way, I did understand that extract I included, just not the concept of ""wavelet""). From my understanding, some special basis are called wavelets (for some special reason). But which basis are we allowed to call wavelets? I would assume that they have something to do with linear algebra and oscillation/sinusoidal functions but I don't really see what the relation between the two is. To look for an alternative explanation I went to wikipedia and the initial paragraph starts as follows: A wavelet is a wave-like oscillation with an amplitude that begins at   zero, increases, and then decreases back to zero. It can typically be   visualized as a ""brief oscillation"" like one might see recorded by a   seismograph or heart monitor. Generally, wavelets are purposefully   crafted to have specific properties that make them useful for signal   processing. Wavelets can be combined, using a ""reverse, shift,   multiply and integrate"" technique called convolution, with portions of   a known signal to extract information from the unknown signal. With that description it makes me feel that wavelets are actually functions. However, I've had difficulty understanding this precisely, specially when trying to relate it to linear algebra. I guess I am having a hard time connecting the three, wavelets, linear algebra and their relations to sinusoidal functions (if there is any relation to them).","I was reading on wavelets and it seems that its hard to find a precise mathematical definition of what this concept is. My confusion first arose due to Gilbert Stang's linear algebra book. In particular consider the following extract: It talks about how to change a vector from one basis to another but it never rigorously defines what a wavelet is (by the way, I did understand that extract I included, just not the concept of ""wavelet""). From my understanding, some special basis are called wavelets (for some special reason). But which basis are we allowed to call wavelets? I would assume that they have something to do with linear algebra and oscillation/sinusoidal functions but I don't really see what the relation between the two is. To look for an alternative explanation I went to wikipedia and the initial paragraph starts as follows: A wavelet is a wave-like oscillation with an amplitude that begins at   zero, increases, and then decreases back to zero. It can typically be   visualized as a ""brief oscillation"" like one might see recorded by a   seismograph or heart monitor. Generally, wavelets are purposefully   crafted to have specific properties that make them useful for signal   processing. Wavelets can be combined, using a ""reverse, shift,   multiply and integrate"" technique called convolution, with portions of   a known signal to extract information from the unknown signal. With that description it makes me feel that wavelets are actually functions. However, I've had difficulty understanding this precisely, specially when trying to relate it to linear algebra. I guess I am having a hard time connecting the three, wavelets, linear algebra and their relations to sinusoidal functions (if there is any relation to them).",,"['linear-algebra', 'functional-analysis', 'functions', 'fourier-analysis', 'wavelets']"
75,"Quick ways to _verify_ determinant, minimal polynomial, characteristic polynomial, eigenvalues, eigenvectors ...","Quick ways to _verify_ determinant, minimal polynomial, characteristic polynomial, eigenvalues, eigenvectors ...",,"What are easy and quick ways to verify determinant, minimal polynomial, characteristic polynomial, eigenvalues, eigenvectors after calculating them? So if I calculated determinant, minimal polynomial, characteristic polynomial, eigenvalues, eigenvectors, what are ways to be sure that I didn't do a major mistake? I don't want to verify my solutions all the way through, I just want a quick way which gives me that it is highly likely that the calculated determinant is right etc. Let $A$ be a matrix $A \in \operatorname{Mat}(n, \mathbb{C})$, let $\det(A)$ be the determinant of matrix $A$, let $v_1, v_2, ..., v_k$ be eigenvectors of matrix $A$, let $\lambda_1, \lambda_2, ..., \lambda_n$ be eigenvalues of matrix $A$, let $\chi_A(t) = t^n + a_{n-1}t^{n-1}+\cdots + a_0 = (t-\lambda_1)\cdots(t-\lambda_n)$ be the characteristic polynomial of matrix $A$, let $\mu_A(t)$ be the minimal polynomial of matrix $A$. Verifications suggested so far: eigenvectors / eigenvalues $\det(A) = \lambda_1^{m_1} \lambda_2^{m_2} \cdots \lambda_n^{m_l}$ where $m_i$ is the multiplicity of the corresponding eigenvalue $a_0 = (-1)^n\lambda_1\cdots\lambda_n$ eigenvectors can be verified by multiplying with the matrix; the eigenvalues can be verified at the same time; i.e. $A v_i = \lambda_i v_i$ determinant $\det(A) = \lambda_1^{m_1} \lambda_2^{m_2} \cdots \lambda_l^{m_l}$ where $m_i$ is the multiplicity of the corresponding eigenvalue characteristic / minimal polynomial $a_0 = (-1)^n\lambda_1\cdots\lambda_n$ $\mu_A(A) = 0$ and $\chi_A(A) = 0$ $\mu_A(t) \mid \chi_A(t)$","What are easy and quick ways to verify determinant, minimal polynomial, characteristic polynomial, eigenvalues, eigenvectors after calculating them? So if I calculated determinant, minimal polynomial, characteristic polynomial, eigenvalues, eigenvectors, what are ways to be sure that I didn't do a major mistake? I don't want to verify my solutions all the way through, I just want a quick way which gives me that it is highly likely that the calculated determinant is right etc. Let $A$ be a matrix $A \in \operatorname{Mat}(n, \mathbb{C})$, let $\det(A)$ be the determinant of matrix $A$, let $v_1, v_2, ..., v_k$ be eigenvectors of matrix $A$, let $\lambda_1, \lambda_2, ..., \lambda_n$ be eigenvalues of matrix $A$, let $\chi_A(t) = t^n + a_{n-1}t^{n-1}+\cdots + a_0 = (t-\lambda_1)\cdots(t-\lambda_n)$ be the characteristic polynomial of matrix $A$, let $\mu_A(t)$ be the minimal polynomial of matrix $A$. Verifications suggested so far: eigenvectors / eigenvalues $\det(A) = \lambda_1^{m_1} \lambda_2^{m_2} \cdots \lambda_n^{m_l}$ where $m_i$ is the multiplicity of the corresponding eigenvalue $a_0 = (-1)^n\lambda_1\cdots\lambda_n$ eigenvectors can be verified by multiplying with the matrix; the eigenvalues can be verified at the same time; i.e. $A v_i = \lambda_i v_i$ determinant $\det(A) = \lambda_1^{m_1} \lambda_2^{m_2} \cdots \lambda_l^{m_l}$ where $m_i$ is the multiplicity of the corresponding eigenvalue characteristic / minimal polynomial $a_0 = (-1)^n\lambda_1\cdots\lambda_n$ $\mu_A(A) = 0$ and $\chi_A(A) = 0$ $\mu_A(t) \mid \chi_A(t)$",,"['linear-algebra', 'matrices', 'polynomials', 'eigenvalues-eigenvectors', 'determinant']"
76,A direct proof of the Vandermonde decomposition of a nonsingular Hankel matrix?,A direct proof of the Vandermonde decomposition of a nonsingular Hankel matrix?,,"I have been doggedly searching for a direct proof of the following theorem: Theorem 1: Let $H$ be a complex nonsingular $n\times n$ Hankel matrix. Then $H$ can be factorized $H = V^\top DV$ where $V$ is a complex $n\times n$ Vandermonde matrix and $D$ is a complex $n\times n$ diagonal matrix. There exist numerous references stating that this result is well known either without proof or with a citation to one of the references below which I have found insufficient for one reason or another. Here is an abbreviated summary of my research: References (e.g., here , here , and here ) that are unavailable to me or in Russian References (e.g., here , here , and here ) for a confluent Vandermonde decomposition of a rank-deficient finite and/or finite-rank infinite Hankel matrices: These do no prove the theorem I am after since the Vandermonde decomposition involves confluent Vandermonde matrices. References for a Vandermonde decomposition of positive semidefinite or positive definite Hankel matrix: These don't address the case of a general square Hankel matrix. References ( here , here , and here ) where the theorem is proven as a corollary of a rather length theory: these results do prove the theorem that I want. However, the result follows as a corollary of several pages of a lengthy theory drawing connections to other structured matrices, polynomials, and rational functions. It seems like this basic and essential fact should have a more direct proof. I am looking for as simple and direct a proof of Theorem 1 as possible, a reference to one in the literature, or some insight as to why a simple proof does not exist. Below the fold, I have outlined my attempt to provide such a proof. My Attempt Consider the entries of the Hankel matrix $$ H = \begin{bmatrix} h_0 & h_1 & \cdots & h_{n-1} \\ h_1 & h_2 & \cdots & h_{n+1} \\ \vdots & \vdots &\ddots & \vdots \\ h_{n-1} & h_n & \cdots & h_{2n-2} \end{bmatrix}. $$ The Vandermonde decomposition in Theorem 1 is equivalent to finding a representation of the sequence $(h_k)_{0\le k\le 2n-2}$ of the following form: $$ h_k = \sum_{j=1}^n \alpha_j z_j^k, \quad k = 0,1,2,\ldots,2n-2. \tag{$\star$} $$ Observe that the sequence $(h_k)_{0\le k\le 2n-2}$ has fewer parameters than the parametrization ( $\star$ ). To rectify this, add $h_{2n-1}$ , value to be set later, to the end of the sequence and require it to obey the parametrization ( $\star$ ) for $k=2n-1$ . We seek to identify the poles $z_1,\ldots,z_n$ by Prony's method . The key observation is a sequence of the form ( $\star$ ) is a solution to an order- $n$ difference equation: $$ h_{n+k} = c_{n-1} h_{k+(n-1)} + \cdots + c_0 h_k. $$ We find the difference equation coefficients $c_0,\ldots,c_{n-1}$ by solving the following linear system of equations $$ \begin{bmatrix} h_n \\ h_{n+1} \\ \vdots \\ h_{2n-1} \end{bmatrix} = \begin{bmatrix} h_0 & h_1 & \cdots & h_{n-1} \\ h_1 & h_2 & \cdots & h_{n+1} \\ \vdots & \vdots &\ddots & \vdots \\ h_{n-1} & h_n & \cdots & h_{2n-2} \end{bmatrix} \begin{bmatrix} c_0 \\ c_1 \\ \vdots \\ c_{n-1}\end{bmatrix}. $$ The matrix in this equation is precisely the matrix $H$ which we know to be nonzero. Thus, this equation has a unique solution $c_0,\ldots,c_{n-1}$ . Provided the characteristic equation $$ z^n - c_{n-1}z^{n-1} - \cdots - c_0z^0 = 0 \quad \text{has distinct roots} \quad z_1,\ldots,z_n, \tag{$\dagger$} $$ there exist coefficients $\alpha_1,\ldots,\alpha_n$ such that ( $\star$ ) holds. The Vandermonde decomposition is proven under the assumption ( $\dagger$ ). The Vandermonde decomposition thus rests on choosing $h_{2n-1}$ such that ( $\dagger$ ) holds. Since $h_{2n-1}$ is arbitrary, the polynomial in ( $\dagger$ ) can be chosen to be any polynomial of the form $$ f(z) = g(z) + \alpha h(z) $$ where $$ g(z) = z^n - d_{n-1} z^{n-1} - \cdots - d_0 z^0, \quad h(z) = -e_{n-1} z^{n-1} - \cdots - e_0 z^0 \tag{$\blacktriangle$} $$ with $d_0,\ldots,d_{n-1}$ and $e_0,\ldots,e_{n-1}$ the solutions of $$ \begin{bmatrix} h_n \\ h_{n+1} \\ \vdots \\ 0 \end{bmatrix} = H \begin{bmatrix} d_0 \\ d_1 \\ \vdots \\ d_{n-1}\end{bmatrix}, \quad \begin{bmatrix} 0 \\ 0 \\ \vdots \\ h_{2n-1} \end{bmatrix} = H \begin{bmatrix} e_0 \\ e_1 \\ \vdots \\ e_{n-1}\end{bmatrix}. $$ To this end, we have a helpful lemma (see, e.g., Lemma 7.7 ): Lemma 2. If any polynomials $g$ and $h$ are coprime , then $g - \alpha h$ has simple roots except for finitely many values $\alpha$ . Thus, Theorem 1 would be proven if I could show that $g$ and $h$ as defined in ( $\blacktriangle$ ) are coprime.","I have been doggedly searching for a direct proof of the following theorem: Theorem 1: Let be a complex nonsingular Hankel matrix. Then can be factorized where is a complex Vandermonde matrix and is a complex diagonal matrix. There exist numerous references stating that this result is well known either without proof or with a citation to one of the references below which I have found insufficient for one reason or another. Here is an abbreviated summary of my research: References (e.g., here , here , and here ) that are unavailable to me or in Russian References (e.g., here , here , and here ) for a confluent Vandermonde decomposition of a rank-deficient finite and/or finite-rank infinite Hankel matrices: These do no prove the theorem I am after since the Vandermonde decomposition involves confluent Vandermonde matrices. References for a Vandermonde decomposition of positive semidefinite or positive definite Hankel matrix: These don't address the case of a general square Hankel matrix. References ( here , here , and here ) where the theorem is proven as a corollary of a rather length theory: these results do prove the theorem that I want. However, the result follows as a corollary of several pages of a lengthy theory drawing connections to other structured matrices, polynomials, and rational functions. It seems like this basic and essential fact should have a more direct proof. I am looking for as simple and direct a proof of Theorem 1 as possible, a reference to one in the literature, or some insight as to why a simple proof does not exist. Below the fold, I have outlined my attempt to provide such a proof. My Attempt Consider the entries of the Hankel matrix The Vandermonde decomposition in Theorem 1 is equivalent to finding a representation of the sequence of the following form: Observe that the sequence has fewer parameters than the parametrization ( ). To rectify this, add , value to be set later, to the end of the sequence and require it to obey the parametrization ( ) for . We seek to identify the poles by Prony's method . The key observation is a sequence of the form ( ) is a solution to an order- difference equation: We find the difference equation coefficients by solving the following linear system of equations The matrix in this equation is precisely the matrix which we know to be nonzero. Thus, this equation has a unique solution . Provided the characteristic equation there exist coefficients such that ( ) holds. The Vandermonde decomposition is proven under the assumption ( ). The Vandermonde decomposition thus rests on choosing such that ( ) holds. Since is arbitrary, the polynomial in ( ) can be chosen to be any polynomial of the form where with and the solutions of To this end, we have a helpful lemma (see, e.g., Lemma 7.7 ): Lemma 2. If any polynomials and are coprime , then has simple roots except for finitely many values . Thus, Theorem 1 would be proven if I could show that and as defined in ( ) are coprime.","H n\times n H H = V^\top DV V n\times n D n\times n 
H = \begin{bmatrix} h_0 & h_1 & \cdots & h_{n-1} \\
h_1 & h_2 & \cdots & h_{n+1} \\
\vdots & \vdots &\ddots & \vdots \\
h_{n-1} & h_n & \cdots & h_{2n-2} \end{bmatrix}.
 (h_k)_{0\le k\le 2n-2} 
h_k = \sum_{j=1}^n \alpha_j z_j^k, \quad k = 0,1,2,\ldots,2n-2. \tag{\star}
 (h_k)_{0\le k\le 2n-2} \star h_{2n-1} \star k=2n-1 z_1,\ldots,z_n \star n 
h_{n+k} = c_{n-1} h_{k+(n-1)} + \cdots + c_0 h_k.
 c_0,\ldots,c_{n-1} 
\begin{bmatrix}
h_n \\ h_{n+1} \\ \vdots \\ h_{2n-1}
\end{bmatrix} = \begin{bmatrix} h_0 & h_1 & \cdots & h_{n-1} \\
h_1 & h_2 & \cdots & h_{n+1} \\
\vdots & \vdots &\ddots & \vdots \\
h_{n-1} & h_n & \cdots & h_{2n-2} \end{bmatrix} \begin{bmatrix} c_0 \\ c_1 \\ \vdots \\ c_{n-1}\end{bmatrix}.
 H c_0,\ldots,c_{n-1} 
z^n - c_{n-1}z^{n-1} - \cdots - c_0z^0 = 0 \quad \text{has distinct roots} \quad z_1,\ldots,z_n, \tag{\dagger}
 \alpha_1,\ldots,\alpha_n \star \dagger h_{2n-1} \dagger h_{2n-1} \dagger 
f(z) = g(z) + \alpha h(z)
 
g(z) = z^n - d_{n-1} z^{n-1} - \cdots - d_0 z^0, \quad h(z) = -e_{n-1} z^{n-1} - \cdots - e_0 z^0 \tag{\blacktriangle}
 d_0,\ldots,d_{n-1} e_0,\ldots,e_{n-1} 
\begin{bmatrix}
h_n \\ h_{n+1} \\ \vdots \\ 0
\end{bmatrix} = H \begin{bmatrix} d_0 \\ d_1 \\ \vdots \\ d_{n-1}\end{bmatrix}, \quad \begin{bmatrix}
0 \\ 0 \\ \vdots \\ h_{2n-1}
\end{bmatrix} = H \begin{bmatrix} e_0 \\ e_1 \\ \vdots \\ e_{n-1}\end{bmatrix}.
 g h g - \alpha h \alpha g h \blacktriangle","['linear-algebra', 'reference-request', 'control-theory', 'hankel-matrices']"
77,Clubs whose intersections are multiples of six (Oddtown variant),Clubs whose intersections are multiples of six (Oddtown variant),,"This is a question about generalizing the famous ""Clubs in Oddtown"" problem. The original setup is that a town has $n$ people, and $m$ clubs each consisting of a subset of the population. Each club has an odd number of members, while any two distinct clubs have an even number of members in common. Given this, you the goal is to prove $m\le n$ . You can prove this using linear algebra over $\mathbb F_2$ . One might ask if the results still hold if we replace $2$ with another number $d$ . That is, if we have a town with $n$ people and $m$ clubs such that the number of people in any club is not a multiple of $d$ , while the number of people in common to any two clubs is a multiple of $d$ , what it is the greatest possible value of $m$ in terms of $n$ ? When $d=p$ for any prime $p$ , you can prove $m\le n$ using the same argument but with $\mathbb F_p$ -linear algebra. You can also prove that $m\le n$ when $d=p^k$ is any prime power, see Generalize Oddtown . This leads me to ask, can we also prove that $m\le n$ for all other $d$ ? For concreteness, I am asking about $d=6$ (I like to call this variant ""Clubs in Sioux City""). That is, Question: There is a town with $n$ people and $m$ clubs consisting of subsets of the population. Any club has a number of members which is not a multiple of $6$ , while the number of members in common to any two clubs is a multiple of $6$ . Can we conclude that $m\le n$ ? I know that you can prove $m\le 2n$ , by combining the $d=2$ and $d=3$ results (proving this result is what was asked at that question I linked before ). But from playing with small cases, it does not seem like it is possible to create a setup with $m>n$ , though I also cannot prove this. Does anyone know how to prove that $m\le n$ , or how to construct a set of $n+1$ clubs satisfying the requirements?","This is a question about generalizing the famous ""Clubs in Oddtown"" problem. The original setup is that a town has people, and clubs each consisting of a subset of the population. Each club has an odd number of members, while any two distinct clubs have an even number of members in common. Given this, you the goal is to prove . You can prove this using linear algebra over . One might ask if the results still hold if we replace with another number . That is, if we have a town with people and clubs such that the number of people in any club is not a multiple of , while the number of people in common to any two clubs is a multiple of , what it is the greatest possible value of in terms of ? When for any prime , you can prove using the same argument but with -linear algebra. You can also prove that when is any prime power, see Generalize Oddtown . This leads me to ask, can we also prove that for all other ? For concreteness, I am asking about (I like to call this variant ""Clubs in Sioux City""). That is, Question: There is a town with people and clubs consisting of subsets of the population. Any club has a number of members which is not a multiple of , while the number of members in common to any two clubs is a multiple of . Can we conclude that ? I know that you can prove , by combining the and results (proving this result is what was asked at that question I linked before ). But from playing with small cases, it does not seem like it is possible to create a setup with , though I also cannot prove this. Does anyone know how to prove that , or how to construct a set of clubs satisfying the requirements?",n m m\le n \mathbb F_2 2 d n m d d m n d=p p m\le n \mathbb F_p m\le n d=p^k m\le n d d=6 n m 6 6 m\le n m\le 2n d=2 d=3 m>n m\le n n+1,"['linear-algebra', 'combinatorics', 'finite-fields', 'extremal-combinatorics', 'algebraic-combinatorics']"
78,Why can't we define traces and determinants for non-square matrices?,Why can't we define traces and determinants for non-square matrices?,,"I'm reading a book that claims that determinants and traces are only defined for square matrices, but doesn’t really explain why.  From a calculation standpoint, this seems correct because I wouldn’t really know how to calculate the determinant of something like a column vector (with more than one row). However, the only justification I can think of is that using the meaning of a determinant, we’re trying to calculate the scaling factor of our unit cube, hypercube, whatever, while missing some “dimension” of it. I don’t really know if that makes sense, but it’s like if I were to ask you to find the volume of a box using only the dimensions of one of its faces. Is this a reasonable explanation, or is there a better reason why we can’t calculate the determinant of a non-square matrix ? And as for the trace, does that really just come down to the definition of the trace, or is there something more “meaningful” ?","I'm reading a book that claims that determinants and traces are only defined for square matrices, but doesn’t really explain why.  From a calculation standpoint, this seems correct because I wouldn’t really know how to calculate the determinant of something like a column vector (with more than one row). However, the only justification I can think of is that using the meaning of a determinant, we’re trying to calculate the scaling factor of our unit cube, hypercube, whatever, while missing some “dimension” of it. I don’t really know if that makes sense, but it’s like if I were to ask you to find the volume of a box using only the dimensions of one of its faces. Is this a reasonable explanation, or is there a better reason why we can’t calculate the determinant of a non-square matrix ? And as for the trace, does that really just come down to the definition of the trace, or is there something more “meaningful” ?",,"['linear-algebra', 'matrices']"
79,Identities for subspaces and linear maps,Identities for subspaces and linear maps,,"Wikipedia has a nice list of identities for how intersections, unions, and complements interact with images and preimages of set functions. But if $f:V \to W$ is a linear map, many of the identities for set functions can be refined. For example, if $A \subset V$ , then $f^{-1}(f(A)) = A + \ker f$ . There are also identities which involve sums, e.g. $f(A + B) = f(A) + f(B)$ . Can anyone point me to a reference which has a list of identities for subspaces and linear maps?","Wikipedia has a nice list of identities for how intersections, unions, and complements interact with images and preimages of set functions. But if is a linear map, many of the identities for set functions can be refined. For example, if , then . There are also identities which involve sums, e.g. . Can anyone point me to a reference which has a list of identities for subspaces and linear maps?",f:V \to W A \subset V f^{-1}(f(A)) = A + \ker f f(A + B) = f(A) + f(B),"['linear-algebra', 'reference-request', 'vector-spaces', 'linear-transformations']"
80,Studying representations of orthogonal group via symmetric group?,Studying representations of orthogonal group via symmetric group?,,"Let $V$ be the vector space of $n\times n$ symmetric matrices with real entries. On $V$ we have the natural action of the orthogonal group $\textrm{O}(n)$ defined by $g.A:=g\cdot A\cdot g^{t}$ for $g\in\textrm{O}(n)$ and $A\in V$ . We identify $\mathbb{R}^n$ with the set of diagonal matrices. Thus we get an inclusion $\textrm{diag}:\mathbb{R}^n\hookrightarrow V$ . Now let $G=\mathfrak{S}_n$ be the symmetric group on $n$ elements. Considering $G$ as the subgroup of $\textrm{O}(n)$ consisting of permutation matrices, we get an action of $G$ both on $\mathbb{R}^n$ and $V$ that makes $\textrm{diag}$ to a homomorphism of $G$ -modules. If we pass to the dual map, we get a surjective homomorphism of $G$ -modules $$\textrm{diag}^\vee:V\to\mathbb{R}^n.$$ The decomposition of the $\textrm{O}(n)$ -module $V$ into irreducibles is $(\mathbb{R}\cdot\textrm{I}_n)\oplus V_0$ where $\textrm{I}_n$ is the identity matrix and $V_0$ is the set of traceless matrices. We note that the image under $\textrm{diag}^\vee$ of both is an irreducible $G$ -module. My question is, whether this property is preserved under taking the symmetric power: Let $W$ be an irreducible $\textrm{O}(n)$ -submodule of $\textrm{Sym}^p(V)$ . Is it true that $\textrm{diag}^\vee(W)$ is irreducible as $G$ -module?","Let be the vector space of symmetric matrices with real entries. On we have the natural action of the orthogonal group defined by for and . We identify with the set of diagonal matrices. Thus we get an inclusion . Now let be the symmetric group on elements. Considering as the subgroup of consisting of permutation matrices, we get an action of both on and that makes to a homomorphism of -modules. If we pass to the dual map, we get a surjective homomorphism of -modules The decomposition of the -module into irreducibles is where is the identity matrix and is the set of traceless matrices. We note that the image under of both is an irreducible -module. My question is, whether this property is preserved under taking the symmetric power: Let be an irreducible -submodule of . Is it true that is irreducible as -module?",V n\times n V \textrm{O}(n) g.A:=g\cdot A\cdot g^{t} g\in\textrm{O}(n) A\in V \mathbb{R}^n \textrm{diag}:\mathbb{R}^n\hookrightarrow V G=\mathfrak{S}_n n G \textrm{O}(n) G \mathbb{R}^n V \textrm{diag} G G \textrm{diag}^\vee:V\to\mathbb{R}^n. \textrm{O}(n) V (\mathbb{R}\cdot\textrm{I}_n)\oplus V_0 \textrm{I}_n V_0 \textrm{diag}^\vee G W \textrm{O}(n) \textrm{Sym}^p(V) \textrm{diag}^\vee(W) G,"['linear-algebra', 'abstract-algebra', 'representation-theory']"
81,Prove that $\det(A)=p_1p_2-ba={bf(a)-af(b)\over b-a}$,Prove that,\det(A)=p_1p_2-ba={bf(a)-af(b)\over b-a},"Let $f(x)=(p_1-x)\cdots (p_n-x)$ $p_1,\ldots, p_n\in \mathbb R$ and let $a,b\in \mathbb R$ such that $a\neq b$ Prove that $\det A={bf(a)-af(b)\over b-a}$ where $A$ is the matrix: $$\begin{pmatrix}p_1 & a & a & \cdots & a \\ b & p_2 & a & \cdots & a \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ b & b & b & \cdots & p_n \end{pmatrix}$$ that is the entries $k_{ij}=a$ if $i<j$, $k_{ij}=p_i$ if $i=j$ and $k_{ij}=b$ if $i>j$ I tried to do it by induction over $n$. The base case for $n=2$ is easy $\det(A)=p_1p_2-ba={bf(a)-af(b)\over b-a}$ The induction step is where I don´t know what to do. I tried to solve the dterminant by brute force(applying my induction hypothesis for n and prove it for n+1) but I don´t know how to reduce it. It gets horrible. I would really appreciate if you can help me with this problem. Any comments, suggestions or hints would be highly appreciated","Let $f(x)=(p_1-x)\cdots (p_n-x)$ $p_1,\ldots, p_n\in \mathbb R$ and let $a,b\in \mathbb R$ such that $a\neq b$ Prove that $\det A={bf(a)-af(b)\over b-a}$ where $A$ is the matrix: $$\begin{pmatrix}p_1 & a & a & \cdots & a \\ b & p_2 & a & \cdots & a \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ b & b & b & \cdots & p_n \end{pmatrix}$$ that is the entries $k_{ij}=a$ if $i<j$, $k_{ij}=p_i$ if $i=j$ and $k_{ij}=b$ if $i>j$ I tried to do it by induction over $n$. The base case for $n=2$ is easy $\det(A)=p_1p_2-ba={bf(a)-af(b)\over b-a}$ The induction step is where I don´t know what to do. I tried to solve the dterminant by brute force(applying my induction hypothesis for n and prove it for n+1) but I don´t know how to reduce it. It gets horrible. I would really appreciate if you can help me with this problem. Any comments, suggestions or hints would be highly appreciated",,"['linear-algebra', 'matrices', 'polynomials', 'induction', 'determinant']"
82,Determinant of $n\times n$ matrix with parameter,Determinant of  matrix with parameter,n\times n,"Problem: Let $\delta \in \mathbb{R}^+$ and $n\in \mathbb{N}$. The matrix $A_n = (a_{i,j}) \in \mathbb{R}^{n\times n}$ is defined as $$ a_{i,j} = \prod_{k=0}^{i-2}\left((j-1)\delta +n-k\right) $$   Prove that   $$\det A_n = \delta ^{\frac{1}{2}n(n-1)}\prod^{n-1}_{k=0}k!$$ So there is $$ A_1 = \pmatrix{ 1\\ } $$ $$ A_2 = \pmatrix{ 1&1\\ 2&\delta+2\\ } $$ $$ A_3 = \pmatrix{ 1&1&1\\ 3&\delta+3&2\delta+3\\ 6&(\delta+3)(\delta+2)&(2\delta+3)(2\delta+2)\\ } $$ $$\vdots$$ I eventually managed to prove it by converting the matrix to upper triangular using elementary row operations, but the proof is just too complicated, involves things like $(k−2)\delta+n−(i−(k−1))+1)$-th multiples of certain rows (for that matter it is quite long, so I am not fully including it here). So it somehow feels like not the best possible way to do this. What are some another ways to prove this?","Problem: Let $\delta \in \mathbb{R}^+$ and $n\in \mathbb{N}$. The matrix $A_n = (a_{i,j}) \in \mathbb{R}^{n\times n}$ is defined as $$ a_{i,j} = \prod_{k=0}^{i-2}\left((j-1)\delta +n-k\right) $$   Prove that   $$\det A_n = \delta ^{\frac{1}{2}n(n-1)}\prod^{n-1}_{k=0}k!$$ So there is $$ A_1 = \pmatrix{ 1\\ } $$ $$ A_2 = \pmatrix{ 1&1\\ 2&\delta+2\\ } $$ $$ A_3 = \pmatrix{ 1&1&1\\ 3&\delta+3&2\delta+3\\ 6&(\delta+3)(\delta+2)&(2\delta+3)(2\delta+2)\\ } $$ $$\vdots$$ I eventually managed to prove it by converting the matrix to upper triangular using elementary row operations, but the proof is just too complicated, involves things like $(k−2)\delta+n−(i−(k−1))+1)$-th multiples of certain rows (for that matter it is quite long, so I am not fully including it here). So it somehow feels like not the best possible way to do this. What are some another ways to prove this?",,"['linear-algebra', 'matrices', 'reference-request', 'determinant', 'alternative-proof']"
83,Does the Hadamard product of two vectors have a geometric interpretation?,Does the Hadamard product of two vectors have a geometric interpretation?,,Pointwise multiplication (or element-wise) is also known as the Hadamard product. Is there a geometric interpretation similar to addition or subtraction in vector spaces?,Pointwise multiplication (or element-wise) is also known as the Hadamard product. Is there a geometric interpretation similar to addition or subtraction in vector spaces?,,['linear-algebra']
84,How can I construct a solution for this system of many inequalities?,How can I construct a solution for this system of many inequalities?,,"Let there be types $\omega\in\{0,1\}^n$ drawn according to some probability distribution. Suppose that these types are relayed through some imperfect message service. Specifically, any type $\omega$'s message, $m$, is always the product of some garbling so that $m\in\{0,1\}^n$, but $\sum_{i=1}^n m_i = k$. So there are $\binom{n}{k}$ different messages. A type $\omega$ will send message $m$ with probability $p_m^\omega$. I'd like to construct these distributions $p^\omega$ for each $\omega$ so that the message is ""believed"" according to a cost-weighted error. If $\omega_i=1$ and $m_i=0$, then the cost is 1. If $\omega_i=0$ and $m_i=1$, then the cost is $c$. Insofar as we want to minimize error, I think it's obvious that for any type $\omega$ it should send a message where $m_i=\omega_i$ when $\omega_i=1$, but for types where $\sum_{i=1}^n\omega_i>k$, then some ones must be turned off so to speak. Similarly, we have to turn on some false ones for types $\sum_{i=1}^n\omega_i<k$. Then, I think the problem is finding a distribution $p^\omega$ across messages for each $\omega$ so that for any $m$, $$\sum_{\omega\in \{\omega:\sum_i^n\omega_i<k\}} c(k-\sum_i\omega_i )\Pr(\omega)p_m^\omega + \sum_{\omega\in \{\omega:\sum_i^n\omega_i\geq k\}} (\sum_i\omega_i-k)\Pr(\omega)p_m^\omega$$ $$\leq \sum_{\Omega}\Pr(\omega)p_m^\omega(\sum_i\omega_i) .$$ This inequality says that the cost of errors made from believing $\omega_i=m_i$ when $\omega_i=0$ and $m_i=1$ is less than the number of errors made from disregarding the message and believing every $\omega_i=0$. I think we can always find these $p^\omega$s whenever $\Pr(\sum_i^n\omega_i < k)\leq \frac{1}{c+1}$, but I don't believe this is necessary and I don't see how to actually construct the distributions to guarantee the inequality above. Any help would be appreciated, and let me know if anything is unclear. Example: Let $n=3$, $c=1$, and $\Pr(\omega_i=1)=.4$, i.i.d. Then for $k=2$, we can have $p^{(0,1,1)}_{(0,1,1)}=1$,$p^{(1,0,1)}_{(1,0,1)}=1$, and $p^{(1,1,0)}_{(1,1,0)}=1$. Then for the low types, set $p^{(1,0,0)}_{(1,1,0)}=p^{(1,0,0)}_{(1,0,1)}=1/2$. Similarly, set $p^{(0,1,0)}_{(1,1,0)}=p^{(0,1,0)}_{(0,1,1)}=1/2$ and set $p^{(0,0,1)}_{(0,1,1)}=p^{(0,0,1)}_{(1,0,1)}=1/2$. Finally, let the remaining types, $\omega=(1,1,1)$ and $(0,0,0)$ have $p^\omega_{(0,1,1)}=1$. I think all of the coniditions are met, yet $\Pr(\sum_{i=1}^n\omega<2)=0.648$ is not less than $1/2=\frac{1}{c+1}$.","Let there be types $\omega\in\{0,1\}^n$ drawn according to some probability distribution. Suppose that these types are relayed through some imperfect message service. Specifically, any type $\omega$'s message, $m$, is always the product of some garbling so that $m\in\{0,1\}^n$, but $\sum_{i=1}^n m_i = k$. So there are $\binom{n}{k}$ different messages. A type $\omega$ will send message $m$ with probability $p_m^\omega$. I'd like to construct these distributions $p^\omega$ for each $\omega$ so that the message is ""believed"" according to a cost-weighted error. If $\omega_i=1$ and $m_i=0$, then the cost is 1. If $\omega_i=0$ and $m_i=1$, then the cost is $c$. Insofar as we want to minimize error, I think it's obvious that for any type $\omega$ it should send a message where $m_i=\omega_i$ when $\omega_i=1$, but for types where $\sum_{i=1}^n\omega_i>k$, then some ones must be turned off so to speak. Similarly, we have to turn on some false ones for types $\sum_{i=1}^n\omega_i<k$. Then, I think the problem is finding a distribution $p^\omega$ across messages for each $\omega$ so that for any $m$, $$\sum_{\omega\in \{\omega:\sum_i^n\omega_i<k\}} c(k-\sum_i\omega_i )\Pr(\omega)p_m^\omega + \sum_{\omega\in \{\omega:\sum_i^n\omega_i\geq k\}} (\sum_i\omega_i-k)\Pr(\omega)p_m^\omega$$ $$\leq \sum_{\Omega}\Pr(\omega)p_m^\omega(\sum_i\omega_i) .$$ This inequality says that the cost of errors made from believing $\omega_i=m_i$ when $\omega_i=0$ and $m_i=1$ is less than the number of errors made from disregarding the message and believing every $\omega_i=0$. I think we can always find these $p^\omega$s whenever $\Pr(\sum_i^n\omega_i < k)\leq \frac{1}{c+1}$, but I don't believe this is necessary and I don't see how to actually construct the distributions to guarantee the inequality above. Any help would be appreciated, and let me know if anything is unclear. Example: Let $n=3$, $c=1$, and $\Pr(\omega_i=1)=.4$, i.i.d. Then for $k=2$, we can have $p^{(0,1,1)}_{(0,1,1)}=1$,$p^{(1,0,1)}_{(1,0,1)}=1$, and $p^{(1,1,0)}_{(1,1,0)}=1$. Then for the low types, set $p^{(1,0,0)}_{(1,1,0)}=p^{(1,0,0)}_{(1,0,1)}=1/2$. Similarly, set $p^{(0,1,0)}_{(1,1,0)}=p^{(0,1,0)}_{(0,1,1)}=1/2$ and set $p^{(0,0,1)}_{(0,1,1)}=p^{(0,0,1)}_{(1,0,1)}=1/2$. Finally, let the remaining types, $\omega=(1,1,1)$ and $(0,0,0)$ have $p^\omega_{(0,1,1)}=1$. I think all of the coniditions are met, yet $\Pr(\sum_{i=1}^n\omega<2)=0.648$ is not less than $1/2=\frac{1}{c+1}$.",,"['linear-algebra', 'coding-theory']"
85,Linear operators on the functions $f:\mathbb{R}\rightarrow\mathbb{R}$ that distribute over multiplication,Linear operators on the functions  that distribute over multiplication,f:\mathbb{R}\rightarrow\mathbb{R},"Let $V$ denote the vector space of all functions $f:\mathbb{R}\rightarrow\mathbb{R}$. What are the linear operators $L:V\rightarrow V$ such that $L[fg]=L[f]L[g]$ for all $f,g\in V$? I made a bit of progress by considering the functions $$\chi_t(x) = \begin{cases} 1 & x=t \\ 0 & x\neq t \end{cases}.$$ For fixed $x\in\mathbb{R}$, the value of $L[\chi_t](x)$ is either $0$ or $1$ for each $t\in\mathbb{R}$. If there exists some $t$ such that $L[\chi_t](x)=1$, then $L[f](x)=f(t)$. I was unable to do the case in which $L[\chi_t](x)=0$ for all $t$.","Let $V$ denote the vector space of all functions $f:\mathbb{R}\rightarrow\mathbb{R}$. What are the linear operators $L:V\rightarrow V$ such that $L[fg]=L[f]L[g]$ for all $f,g\in V$? I made a bit of progress by considering the functions $$\chi_t(x) = \begin{cases} 1 & x=t \\ 0 & x\neq t \end{cases}.$$ For fixed $x\in\mathbb{R}$, the value of $L[\chi_t](x)$ is either $0$ or $1$ for each $t\in\mathbb{R}$. If there exists some $t$ such that $L[\chi_t](x)=1$, then $L[f](x)=f(t)$. I was unable to do the case in which $L[\chi_t](x)=0$ for all $t$.",,['linear-algebra']
86,Proof of rank-nullity via the first isomorphism theorem,Proof of rank-nullity via the first isomorphism theorem,,"I was thinking about the proof of the rank-nullity theorem and I thought about proving it as follows. I just wondered whether this proof worked? Lemma. If $V$ is a finite-dimensional $F$-vector space and $U\leq V$, then $V/U$ is finite dimensional. $\hspace{16.5mm}$ Moreover, we have that $\dim{V/U}=\dim{V}-\dim{U}$. Theorem (Rank-Nullity). If $\alpha:V\to W$ is linear with $V$ finite-dimensional, then $$\dim{V}=\dim(\text{im }\alpha)+\dim(\ker \alpha)$$ Proof. By the first isomorphism theorem we have $$V/\ker{\alpha} \cong \text{im }{\alpha}.$$ Taking dimensions and applying the lemma we get $$\dim V - \dim(\ker\alpha)=\dim(\text{im }\alpha)$$ which on rearrangement yields the result. //","I was thinking about the proof of the rank-nullity theorem and I thought about proving it as follows. I just wondered whether this proof worked? Lemma. If $V$ is a finite-dimensional $F$-vector space and $U\leq V$, then $V/U$ is finite dimensional. $\hspace{16.5mm}$ Moreover, we have that $\dim{V/U}=\dim{V}-\dim{U}$. Theorem (Rank-Nullity). If $\alpha:V\to W$ is linear with $V$ finite-dimensional, then $$\dim{V}=\dim(\text{im }\alpha)+\dim(\ker \alpha)$$ Proof. By the first isomorphism theorem we have $$V/\ker{\alpha} \cong \text{im }{\alpha}.$$ Taking dimensions and applying the lemma we get $$\dim V - \dim(\ker\alpha)=\dim(\text{im }\alpha)$$ which on rearrangement yields the result. //",,['linear-algebra']
87,A sharper bound for $\|\cos(kA)\|_{\infty}$ for symmetric stochastic matrices,A sharper bound for  for symmetric stochastic matrices,\|\cos(kA)\|_{\infty},"Given $A \in \mathbb{R}^{n \times n}$ that is symmetric, stochastic and diagonalizable, and $k \in \mathbb{N}$, I am interested in bounding $\|\cos(kA)\|_{\infty}$ from above. $\| \|_{\infty}$ is the induced $\ell_{\infty}$ norm, i.e. $\|A\|_{\infty} = \max_{i}\sum_j|A_{i,j}|$, and the cosine of a matrix can be defined, for instance, via its Taylor expansion, $\cos(A) = \sum_t \frac{(-1)^{t}}{(2t)!}A^{2t}$. The trivial bound is of course $\cosh(k)$. However, it seems highly untight for large $k$. For instance, we also have $\|\cos(kA)\|_{\infty} \le \sqrt{n}\|\cos(kA)\|_{2} = \sqrt{n}$ (since $\cos(kA)$ is normal and its largest eigenvalue is at most $1$). Can you see a tighter bound in the special case where $A$ is doubly stochastic? Thank you very much.","Given $A \in \mathbb{R}^{n \times n}$ that is symmetric, stochastic and diagonalizable, and $k \in \mathbb{N}$, I am interested in bounding $\|\cos(kA)\|_{\infty}$ from above. $\| \|_{\infty}$ is the induced $\ell_{\infty}$ norm, i.e. $\|A\|_{\infty} = \max_{i}\sum_j|A_{i,j}|$, and the cosine of a matrix can be defined, for instance, via its Taylor expansion, $\cos(A) = \sum_t \frac{(-1)^{t}}{(2t)!}A^{2t}$. The trivial bound is of course $\cosh(k)$. However, it seems highly untight for large $k$. For instance, we also have $\|\cos(kA)\|_{\infty} \le \sqrt{n}\|\cos(kA)\|_{2} = \sqrt{n}$ (since $\cos(kA)$ is normal and its largest eigenvalue is at most $1$). Can you see a tighter bound in the special case where $A$ is doubly stochastic? Thank you very much.",,"['linear-algebra', 'matrices', 'normed-spaces']"
88,What does the degree of a matrix minimal polynomial encode?,What does the degree of a matrix minimal polynomial encode?,,"Let $\mathsf{F}$ be any field. Let $A$ be an $n \times n$ matrix over $\mathsf{F},$ whose rank is $r \le n.$ Let $\mu \in \mathsf{F}[x]$ be the minimal polynomial of $A.$ What does $\deg(\mu)$ tell about $A$? Is it related to the rank of $A$? Edit : As noted by Qiaochu, $\deg(\mu)$ is not equal to rank. Are there known cases (or conditions) where $\deg(\mu)$ is actually equal to the rank of $A$?","Let $\mathsf{F}$ be any field. Let $A$ be an $n \times n$ matrix over $\mathsf{F},$ whose rank is $r \le n.$ Let $\mu \in \mathsf{F}[x]$ be the minimal polynomial of $A.$ What does $\deg(\mu)$ tell about $A$? Is it related to the rank of $A$? Edit : As noted by Qiaochu, $\deg(\mu)$ is not equal to rank. Are there known cases (or conditions) where $\deg(\mu)$ is actually equal to the rank of $A$?",,['linear-algebra']
89,"If a Bilinear Form is Non-Degenerate on a Subspace $W$, then $V=W\oplus W^\perp$.","If a Bilinear Form is Non-Degenerate on a Subspace , then .",W V=W\oplus W^\perp,"$\newcommand{\range}{\text{image}}\newcommand{\ann}{\text{Ann}}\newcommand{\set}[1]{\{#1\}}$ Problem: Let $V$ be a finite dimensional vector space over a field $F$ and $f$ be a symmetric bilinear form on $V$.   Let $W$ be a subspace of $V$.   If $f$ is non-degenerate on $W$, then $V=W\oplus W^\perp$. Below I provide a proof of this. My problem is that my proof doesn't seem to make use of the fact that $f$ is symmetric. Is the above theorem also true if we strike out the word symmetric? If yes, then one may not read my proof and just leave a comment. If not then can one check where have I used the symmetry of $f$ in the argument? Definitions and Notations: Given a bilinear form $f$ on a (finite dimensional) vector space $V$, we define $R_f:V\to V^*$ as $(R_fv)(u)=f(u, v)$ for all $u, v\in V$. Clearly $R_f$ is a linear map. For a subspace $W$ of $V$, we write $f|W$ to denotes the restriciton of $f$ to $W\times W$. It can be seen that $f|W$ is a bilinear form on $W$. We define $W^\perp$ as $\set{v\in V: f(w, v)=0 \ for \ all \ w\in W}$. Also, we write $\ann W$ to denote the annihilator of $W$. We say that $f$ is non-degenerate on a subspace $W$ of $V$ is $R_{f|W}:W\to W^*$ has rank $\dim W$. The Purported Proof: It is clear that $f$ is non-degenerate on $W$ if and only if $W\cap W^\perp=\set{0}$. So it suffices to show that $\dim W+\dim W^{\perp}=\dim V$. To this end, first we note the simple fact that given $v\in V$ we have \begin{equation*} v\in W^\perp\ \iff\ R_fv\in \ann W \tag{1} \end{equation*} Let $\pi:V^*\to V^*/\ann W$ be the canonical projection map and conider the liner map $\pi\circ R_f:V\to V^*/\ann W$. By the Rank-Nullity Theorem we have $$ \begin{array}{rcl} \dim V &=& \dim \ker (\pi\circ R_f) + \dim \range(\pi\circ R_f)\\ \\ &\leq& \dim \ker (\pi\circ R_f) + \dim (V^*/\ann W)\\ \\ &=& \dim \ker (\pi\circ R_f) + \dim W \end{array} $$ Note that from (1) we know that $\ker (\pi\circ R_f)=W^\perp$. Therefore $\dim V\leq \dim W^\perp+\dim W$. But since $W\cap W^\perp=\set{0}$, we also have $\dim V\geq \dim W+\dim W^\perp$. So we must have $\dim V=\dim W+\dim W^\perp$ and we are done.","$\newcommand{\range}{\text{image}}\newcommand{\ann}{\text{Ann}}\newcommand{\set}[1]{\{#1\}}$ Problem: Let $V$ be a finite dimensional vector space over a field $F$ and $f$ be a symmetric bilinear form on $V$.   Let $W$ be a subspace of $V$.   If $f$ is non-degenerate on $W$, then $V=W\oplus W^\perp$. Below I provide a proof of this. My problem is that my proof doesn't seem to make use of the fact that $f$ is symmetric. Is the above theorem also true if we strike out the word symmetric? If yes, then one may not read my proof and just leave a comment. If not then can one check where have I used the symmetry of $f$ in the argument? Definitions and Notations: Given a bilinear form $f$ on a (finite dimensional) vector space $V$, we define $R_f:V\to V^*$ as $(R_fv)(u)=f(u, v)$ for all $u, v\in V$. Clearly $R_f$ is a linear map. For a subspace $W$ of $V$, we write $f|W$ to denotes the restriciton of $f$ to $W\times W$. It can be seen that $f|W$ is a bilinear form on $W$. We define $W^\perp$ as $\set{v\in V: f(w, v)=0 \ for \ all \ w\in W}$. Also, we write $\ann W$ to denote the annihilator of $W$. We say that $f$ is non-degenerate on a subspace $W$ of $V$ is $R_{f|W}:W\to W^*$ has rank $\dim W$. The Purported Proof: It is clear that $f$ is non-degenerate on $W$ if and only if $W\cap W^\perp=\set{0}$. So it suffices to show that $\dim W+\dim W^{\perp}=\dim V$. To this end, first we note the simple fact that given $v\in V$ we have \begin{equation*} v\in W^\perp\ \iff\ R_fv\in \ann W \tag{1} \end{equation*} Let $\pi:V^*\to V^*/\ann W$ be the canonical projection map and conider the liner map $\pi\circ R_f:V\to V^*/\ann W$. By the Rank-Nullity Theorem we have $$ \begin{array}{rcl} \dim V &=& \dim \ker (\pi\circ R_f) + \dim \range(\pi\circ R_f)\\ \\ &\leq& \dim \ker (\pi\circ R_f) + \dim (V^*/\ann W)\\ \\ &=& \dim \ker (\pi\circ R_f) + \dim W \end{array} $$ Note that from (1) we know that $\ker (\pi\circ R_f)=W^\perp$. Therefore $\dim V\leq \dim W^\perp+\dim W$. But since $W\cap W^\perp=\set{0}$, we also have $\dim V\geq \dim W+\dim W^\perp$. So we must have $\dim V=\dim W+\dim W^\perp$ and we are done.",,"['linear-algebra', 'abstract-algebra', 'multilinear-algebra', 'bilinear-form']"
90,"If $A$ is singular, is $A^3+A^2+A$ singular?","If  is singular, is  singular?",A A^3+A^2+A,"Suppose that $A$ is singular, is $A^3 + A^2 + A$ singular as well?","Suppose that $A$ is singular, is $A^3 + A^2 + A$ singular as well?",,"['linear-algebra', 'matrices']"
91,Will $2$ linear equations with $2$ unknowns always have a solution?,Will  linear equations with  unknowns always have a solution?,2 2,"As I am working on a problem with 3 linear equations with 2 unknowns I discover when I use any two of the equations it seems I always find a solution ok. But when I plug it into the third equation with the same two variables  , the third may or may not cause a contradiction depending if it is a solution and I am OK with that BUT I am confused on when I pick the two equations with two unknowns it seems like it has no choice but to work.  Is there something about linear algebra that makes this so and are there any conditions where it won't be the case that I will find a consistent solution using only the two equations?  My linear algebra is rusty and I am getting up to speed. These are just equations of lines and maybe the geometry would explain it but I am not sure how. Thank you.","As I am working on a problem with 3 linear equations with 2 unknowns I discover when I use any two of the equations it seems I always find a solution ok. But when I plug it into the third equation with the same two variables  , the third may or may not cause a contradiction depending if it is a solution and I am OK with that BUT I am confused on when I pick the two equations with two unknowns it seems like it has no choice but to work.  Is there something about linear algebra that makes this so and are there any conditions where it won't be the case that I will find a consistent solution using only the two equations?  My linear algebra is rusty and I am getting up to speed. These are just equations of lines and maybe the geometry would explain it but I am not sure how. Thank you.",,"['linear-algebra', 'systems-of-equations']"
92,Is there a mathematical notation of indexing a matrix?,Is there a mathematical notation of indexing a matrix?,,"Do matrices in linear algebra support an operation of indexing them analogous to array indexing? For example: $$ A = \left [\begin{array}{cc}     1 & 2 \\     3 & 4 \end{array}\right] $$ In C, a fixed 2 by 2 array in 32 bit integer space could be described as: int32_t A[][2] = {     {1, 2},      {3, 4} }; and allows operations like: A[0][0] = 2; Is there a universally accepted equivalent to such an operation in linear algebra? I understand that a Matrix is not a C Array equivalent, but I am curious whether such an operation is supported.","Do matrices in linear algebra support an operation of indexing them analogous to array indexing? For example: $$ A = \left [\begin{array}{cc}     1 & 2 \\     3 & 4 \end{array}\right] $$ In C, a fixed 2 by 2 array in 32 bit integer space could be described as: int32_t A[][2] = {     {1, 2},      {3, 4} }; and allows operations like: A[0][0] = 2; Is there a universally accepted equivalent to such an operation in linear algebra? I understand that a Matrix is not a C Array equivalent, but I am curious whether such an operation is supported.",,"['linear-algebra', 'notation']"
93,How to determine if a set of five $2\times2$ matrices is independent,How to determine if a set of five  matrices is independent,2\times2,"$$S=\bigg\{\left[\begin{matrix}1&2\\2&1\end{matrix}\right], \left[\begin{matrix}2&1\\-1&2\end{matrix}\right], \left[\begin{matrix}0&1\\1&2\end{matrix}\right],\left[\begin{matrix}1&0\\1&1\end{matrix}\right],  \left[\begin{matrix}1&4\\0&3\end{matrix}\right]\bigg\}$$ How can I determine if a set of five $2\times2$ matrices are independent?","$$S=\bigg\{\left[\begin{matrix}1&2\\2&1\end{matrix}\right], \left[\begin{matrix}2&1\\-1&2\end{matrix}\right], \left[\begin{matrix}0&1\\1&2\end{matrix}\right],\left[\begin{matrix}1&0\\1&1\end{matrix}\right],  \left[\begin{matrix}1&4\\0&3\end{matrix}\right]\bigg\}$$ How can I determine if a set of five $2\times2$ matrices are independent?",,"['linear-algebra', 'matrices', 'vector-spaces']"
94,How to quickly check if vectors are an orthonormal basis of a vector space?,How to quickly check if vectors are an orthonormal basis of a vector space?,,"Let's say we got $3$ vectors given and we need to check if they are an orthonormal basis of some vector space. How would that be done quickly? I have read on several sites on the Internet and here is my summary, correct me if I'm wrong please: All vectors need to be linearly independent. Vectors are perpendicular aka orthogonal to each other ( if $3$ vectors given, I have to do it as pairs of $2$, right? ). Each vector has length $1$.","Let's say we got $3$ vectors given and we need to check if they are an orthonormal basis of some vector space. How would that be done quickly? I have read on several sites on the Internet and here is my summary, correct me if I'm wrong please: All vectors need to be linearly independent. Vectors are perpendicular aka orthogonal to each other ( if $3$ vectors given, I have to do it as pairs of $2$, right? ). Each vector has length $1$.",,"['linear-algebra', 'vector-spaces', 'orthogonality']"
95,To prove that a mathematical statement is false is it enough to find a counterexample?,To prove that a mathematical statement is false is it enough to find a counterexample?,,"I am trying to show if the following statements are true or false. Is it true that $|a + b| = |a| + |b|$ for general vectors $a$ and $b$? If $a \cdot b = a \cdot c$ for equally-sized non-zero vectors $a$, $b$, $c$, does it follow that $b = c$? For the first one I found a counterexample that shows that the statement is false. If vector $a=\langle 1,4,5\rangle$ and $b=\langle 2,2,2\rangle$ then $|a|+|b|=\sqrt{42}$+$2\cdot\sqrt{3}=9.945$, and then, $|a+b|=\sqrt{1^2+4^2+5^2+2^2+2^2+2^2}=7.348$, then we can conclude that $9.945 \ne7.348$ and the statement is false. Also by the triangle identity $|a + b| \le |a| + |b|$ For the second statement I also found an counterexample that proves that is false. If vector $a=\langle 1,0,0\rangle$, $b=\langle 0,1,0\rangle$ and $c=\langle 0,0,1\rangle$, we will obtain the following dot product: $a \cdot b = 0$ $a \cdot c = 0$ then $a \cdot b = a \cdot c = 0$ and $b \ne c$ My question is: To prove the two statements is it enough to find a counterexample and say if it is true or false. Or should I try to provide a more mathematical proof like induction or contradiction?","I am trying to show if the following statements are true or false. Is it true that $|a + b| = |a| + |b|$ for general vectors $a$ and $b$? If $a \cdot b = a \cdot c$ for equally-sized non-zero vectors $a$, $b$, $c$, does it follow that $b = c$? For the first one I found a counterexample that shows that the statement is false. If vector $a=\langle 1,4,5\rangle$ and $b=\langle 2,2,2\rangle$ then $|a|+|b|=\sqrt{42}$+$2\cdot\sqrt{3}=9.945$, and then, $|a+b|=\sqrt{1^2+4^2+5^2+2^2+2^2+2^2}=7.348$, then we can conclude that $9.945 \ne7.348$ and the statement is false. Also by the triangle identity $|a + b| \le |a| + |b|$ For the second statement I also found an counterexample that proves that is false. If vector $a=\langle 1,0,0\rangle$, $b=\langle 0,1,0\rangle$ and $c=\langle 0,0,1\rangle$, we will obtain the following dot product: $a \cdot b = 0$ $a \cdot c = 0$ then $a \cdot b = a \cdot c = 0$ and $b \ne c$ My question is: To prove the two statements is it enough to find a counterexample and say if it is true or false. Or should I try to provide a more mathematical proof like induction or contradiction?",,"['linear-algebra', 'proof-verification', 'induction']"
96,Help finding the determinant of a 4x4 matrix?,Help finding the determinant of a 4x4 matrix?,,"Sorry for the lack of notation but the work should be easy to follow if you know what you are doing. Okay my problem is that the book says it can be done by expanding across any column or row but the only way to get what the book does in their practice example is to choose the row that they chose. This bothers me. As I should be able to do it as I see fit. I will post my work and someone point out the problem in my work. The matrix is as follows: $$A = \left(         \begin{matrix}         5&-7&2&2\\         0&3&0&-4\\         -5&-8&0&3\\         0&5&0&-6\\         \end{matrix} \right) $$ I decided to expand across row one and cross out columns as I found the minors. For the first minor obtaining: $$         \begin{pmatrix}         3 & 0 & -4 \\         -8 & 0 & 3 \\         5 & 0 & -6 \\         \end{pmatrix}  $$ M1 being row one column one we attain $-1^2 = 1$. This is to be multiplied by the determinate of the minor. Now finding the determinant I did: 3 times $$         \begin{pmatrix}         0 & 3 \\         0 & -6  \\         \end{pmatrix} $$  giving $3(0-0)= 0$  then: 0 times $$         \begin{pmatrix}         -8 & 3\\          5 & -6\\         \end{pmatrix} $$ giving 0(48-15)=0 Then: 4 times $$         \begin{pmatrix}         -8 & 0 \\          5 & 0 \\         \end{pmatrix} $$ giving $4(0-0)=0$ adding the determinants we get $0+0+0=0$  So det M1 $= 0(1) = 0$ M2--> M(1,2)---> $-1^1+2= -1^3 = -1$ $$         \begin{pmatrix}         0 & 0 & -4 \\         -5 & 0 & 3 \\         0 & 0 & -6 \\         \end{pmatrix} $$ o*  $$         \begin{pmatrix}         0 & 3 \\         0 & -6 \\         \end{pmatrix} $$ giving $0(0-0)=0$ obviously the next matrix will look the same as the top term in column two is a zero so the determinant for that will be $0$. Now finally 4 times $$         \begin{pmatrix}         -8 & 0 \\         5 & 0 \\        \end{pmatrix} $$ giving 4(0-0)= 0 So the Determinant of Minor 2 is (0+0+0)(-1)= 0 Now on to Minor number 3 M3 --> $-1^4 = 1$ $$         \begin{pmatrix}         0 & 3 & -4 \\         -5 & -8 & 3 \\         0 & 5 & -6 \\         \end{pmatrix} $$ for the determinant: 0 times $$         \begin{pmatrix}         -8 & 3 \\          5 & -6 \\         \end{pmatrix} $$ which gives $0(48-15)=0$ -3 times $$         \begin{pmatrix}         -5& 3 \\         0 & -6 \\           \end{pmatrix} $$ which gives $-3(30-0)= -90$ it is redundant to go on from here because after the final computation for this minor I get -100 and as a result get det M3 = -190 and get determinant of zeros for the following determinant of M4.  which gives: $0(5)+ 0(-7) + (-90)(2) + (0)(2)$ giving  Det Ax $= -380.$ The book says its $20$ and when I did it in a calculator it got 20 but the problem is that both the book and calculator expand across the row with the most zeros but theoretically speaking NO MATTER WHICH row or column you choose to expand across you should get the same answer. So what is it? Is my computation wrong or is my assumption that you can expand across any row or column wrong? Isn't it only important if the determinant doesn't equal zero? or does the exact value matter in more advanced cases?","Sorry for the lack of notation but the work should be easy to follow if you know what you are doing. Okay my problem is that the book says it can be done by expanding across any column or row but the only way to get what the book does in their practice example is to choose the row that they chose. This bothers me. As I should be able to do it as I see fit. I will post my work and someone point out the problem in my work. The matrix is as follows: $$A = \left(         \begin{matrix}         5&-7&2&2\\         0&3&0&-4\\         -5&-8&0&3\\         0&5&0&-6\\         \end{matrix} \right) $$ I decided to expand across row one and cross out columns as I found the minors. For the first minor obtaining: $$         \begin{pmatrix}         3 & 0 & -4 \\         -8 & 0 & 3 \\         5 & 0 & -6 \\         \end{pmatrix}  $$ M1 being row one column one we attain $-1^2 = 1$. This is to be multiplied by the determinate of the minor. Now finding the determinant I did: 3 times $$         \begin{pmatrix}         0 & 3 \\         0 & -6  \\         \end{pmatrix} $$  giving $3(0-0)= 0$  then: 0 times $$         \begin{pmatrix}         -8 & 3\\          5 & -6\\         \end{pmatrix} $$ giving 0(48-15)=0 Then: 4 times $$         \begin{pmatrix}         -8 & 0 \\          5 & 0 \\         \end{pmatrix} $$ giving $4(0-0)=0$ adding the determinants we get $0+0+0=0$  So det M1 $= 0(1) = 0$ M2--> M(1,2)---> $-1^1+2= -1^3 = -1$ $$         \begin{pmatrix}         0 & 0 & -4 \\         -5 & 0 & 3 \\         0 & 0 & -6 \\         \end{pmatrix} $$ o*  $$         \begin{pmatrix}         0 & 3 \\         0 & -6 \\         \end{pmatrix} $$ giving $0(0-0)=0$ obviously the next matrix will look the same as the top term in column two is a zero so the determinant for that will be $0$. Now finally 4 times $$         \begin{pmatrix}         -8 & 0 \\         5 & 0 \\        \end{pmatrix} $$ giving 4(0-0)= 0 So the Determinant of Minor 2 is (0+0+0)(-1)= 0 Now on to Minor number 3 M3 --> $-1^4 = 1$ $$         \begin{pmatrix}         0 & 3 & -4 \\         -5 & -8 & 3 \\         0 & 5 & -6 \\         \end{pmatrix} $$ for the determinant: 0 times $$         \begin{pmatrix}         -8 & 3 \\          5 & -6 \\         \end{pmatrix} $$ which gives $0(48-15)=0$ -3 times $$         \begin{pmatrix}         -5& 3 \\         0 & -6 \\           \end{pmatrix} $$ which gives $-3(30-0)= -90$ it is redundant to go on from here because after the final computation for this minor I get -100 and as a result get det M3 = -190 and get determinant of zeros for the following determinant of M4.  which gives: $0(5)+ 0(-7) + (-90)(2) + (0)(2)$ giving  Det Ax $= -380.$ The book says its $20$ and when I did it in a calculator it got 20 but the problem is that both the book and calculator expand across the row with the most zeros but theoretically speaking NO MATTER WHICH row or column you choose to expand across you should get the same answer. So what is it? Is my computation wrong or is my assumption that you can expand across any row or column wrong? Isn't it only important if the determinant doesn't equal zero? or does the exact value matter in more advanced cases?",,['linear-algebra']
97,Is it legitimate to use a delta function as a composite function?,Is it legitimate to use a delta function as a composite function?,,"I am used to $\int f(x) \delta(x) \, dx = f(0) $ and the idea that this represents the action of a dual vector on $f(x)$ as an element of a vector space of functions, mapping it to the real number $f(0)$ .  Advanced physics books on electromagnetism use the composite $\delta[g(x)] = \delta(x-r)/g'(r)$ , $r$ a root of $g$ , and integrate $f(x) \delta[g(x)] \, dx$ in deriving the Lienard-Wiechert potential of a moving charge using a Green function.  I am skeptical of the delta function, not actually a function, as a composite function.  (The result for the potential is correct - I just don't think the derivation is)  I wonder if $g(x)$ represents a change of basis and the integral should be of $ f[g(x)] \delta[g(x)] \, dg$ with the same real number output $f(0)$ .","I am used to and the idea that this represents the action of a dual vector on as an element of a vector space of functions, mapping it to the real number .  Advanced physics books on electromagnetism use the composite , a root of , and integrate in deriving the Lienard-Wiechert potential of a moving charge using a Green function.  I am skeptical of the delta function, not actually a function, as a composite function.  (The result for the potential is correct - I just don't think the derivation is)  I wonder if represents a change of basis and the integral should be of with the same real number output .","\int f(x) \delta(x) \, dx = f(0)  f(x) f(0) \delta[g(x)] = \delta(x-r)/g'(r) r g f(x) \delta[g(x)] \, dx g(x)  f[g(x)] \delta[g(x)] \, dg f(0)",['linear-algebra']
98,Rudin's equivalent in Linear Algebra,Rudin's equivalent in Linear Algebra,,"I am looking for a linear algebra book which is as abstract as possible. But not an abstract algebra book. Something that is what Rudin's is for (beginning) analysis, that is, terse, rigorous and with beautiful exercises. For example: Intro to LA, by Curtis, and LA by Hoffman-Kunze. Opinions? (I know there are already very similar questions around, but I felt they are not quite the same.)","I am looking for a linear algebra book which is as abstract as possible. But not an abstract algebra book. Something that is what Rudin's is for (beginning) analysis, that is, terse, rigorous and with beautiful exercises. For example: Intro to LA, by Curtis, and LA by Hoffman-Kunze. Opinions? (I know there are already very similar questions around, but I felt they are not quite the same.)",,"['linear-algebra', 'reference-request', 'book-recommendation']"
99,"Limit of matrix $A$ raised to power of $n$, as $n$ approaches infinity.","Limit of matrix  raised to power of , as  approaches infinity.",A n n,"I understand that the limit of $n$ approaching infinity of a matrix $A^n$ , can be computed, in some cases, by looking at the diagonalization of that matrix, and then looking at the limit of $n$ going to infinity of the resulting diagonal matrix, $D$ , whose elements are raised to the power $n$ . What I do not understand is when we do not raise the matrix, call it $P$ , consisting of the eigenvectors of $A$ , and its inverse, to the power of $n$ as well? So: $ P^{-1}AP = D    $ $A = PDP^{-1}  $ $A^n = (PDP^{-1})^n$ $A^n = P^nD^n(P^{-1})^n$ Why do the matrices $P^n$ and $(P^{-1})^n$ not have to be taken into account when looking at the limit of $n$ going to infinity?","I understand that the limit of approaching infinity of a matrix , can be computed, in some cases, by looking at the diagonalization of that matrix, and then looking at the limit of going to infinity of the resulting diagonal matrix, , whose elements are raised to the power . What I do not understand is when we do not raise the matrix, call it , consisting of the eigenvectors of , and its inverse, to the power of as well? So: Why do the matrices and not have to be taken into account when looking at the limit of going to infinity?",n A^n n D n P A n  P^{-1}AP = D     A = PDP^{-1}   A^n = (PDP^{-1})^n A^n = P^nD^n(P^{-1})^n P^n (P^{-1})^n n,"['linear-algebra', 'matrices', 'limits']"
