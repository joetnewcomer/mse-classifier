,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Functional equation + differential equation = way of finding solution?,Functional equation + differential equation = way of finding solution?,,"Question I was wondering about the following: Let's say there is a differential equation whose solution is $f$ And $f$ also satisfies a functional equation. Can anyone construct an (non-trivial) example where knowing the functional equation gives some sort of advantage in solving the differential equation or visa-versa? And if the functional equation does not help can you please give your reasoning on why so? My attempt For example, take $$\frac {\mathrm d f}{\mathrm d x} = f$$ And the functional  equation is of the form: $$ A f(x+y) = f(x)f(y)$$ where, $A$ is a constant. I can't see any sort manipulation where knowing the functional equation has given be an advantage in solving the differential equation.","Question I was wondering about the following: Let's say there is a differential equation whose solution is And also satisfies a functional equation. Can anyone construct an (non-trivial) example where knowing the functional equation gives some sort of advantage in solving the differential equation or visa-versa? And if the functional equation does not help can you please give your reasoning on why so? My attempt For example, take And the functional  equation is of the form: where, is a constant. I can't see any sort manipulation where knowing the functional equation has given be an advantage in solving the differential equation.",f f \frac {\mathrm d f}{\mathrm d x} = f  A f(x+y) = f(x)f(y) A,"['ordinary-differential-equations', 'functional-equations']"
1,The inverse Laplace transformation of $e^s$,The inverse Laplace transformation of,e^s,"I am solving the differential equation: $$y'' + 3xy' -6y = 1, \ y(0) = y'(0) = 0$$ Using Laplace transformations. I arrived at: $$L(y)(s) = \frac{c}{s^3} e^{s^2 / 6} + \frac1{s^3}$$ Where $c$ is an arbitrary constant. I doubt that this is what it really is, though I ran through my calculations again and nothing seemed to be wrong. So I did: $$L(y - \frac{x^2}2 )(s) = \frac{c}{s^3} e^{s^2/6} = L(g \star h)$$ Where: $$L(g)(s) = \frac{c}{s^3}$$ $$L(h)(s) = e^{s^2/6}$$ Now, all I have to do is find $h$, which seems not easy at all (if possible). Not to mention that I'll have to find the convolution afterwards. Is there an inverse Laplace transformation for the function $u \rightarrow e^u$? How could it be found? Thank you.","I am solving the differential equation: $$y'' + 3xy' -6y = 1, \ y(0) = y'(0) = 0$$ Using Laplace transformations. I arrived at: $$L(y)(s) = \frac{c}{s^3} e^{s^2 / 6} + \frac1{s^3}$$ Where $c$ is an arbitrary constant. I doubt that this is what it really is, though I ran through my calculations again and nothing seemed to be wrong. So I did: $$L(y - \frac{x^2}2 )(s) = \frac{c}{s^3} e^{s^2/6} = L(g \star h)$$ Where: $$L(g)(s) = \frac{c}{s^3}$$ $$L(h)(s) = e^{s^2/6}$$ Now, all I have to do is find $h$, which seems not easy at all (if possible). Not to mention that I'll have to find the convolution afterwards. Is there an inverse Laplace transformation for the function $u \rightarrow e^u$? How could it be found? Thank you.",,['ordinary-differential-equations']
2,Differential equation for the logistic map,Differential equation for the logistic map,,"From the Wikipedia article on the logistic map I find the following definition as a recurrence relation: $$x_{n+1} = rx_n(1 - x_n) \tag{1} $$ Then, in another article , I see how to derive from this the closely related logistic function. This begins by rewriting the recurrence in $(1)$ as a differential equation: $$x' = rx(1 - x) \tag{2} $$ And the solution to $(2)$ is the function $$x(t) = \frac{1}{1 + e^{-rt}} \tag{3} $$ When I compare the graphs of $(1)$ and $(3)$ I see that they are not the same, and this does not appear to be a problem with the scaling of the graph. Why does evaluating $(3)$ at $n$ not give me $x_n$ (the value of $x$ after $n$ iterations) in $(1)$? Is solving the recurrence different to solving it's differential equation?","From the Wikipedia article on the logistic map I find the following definition as a recurrence relation: $$x_{n+1} = rx_n(1 - x_n) \tag{1} $$ Then, in another article , I see how to derive from this the closely related logistic function. This begins by rewriting the recurrence in $(1)$ as a differential equation: $$x' = rx(1 - x) \tag{2} $$ And the solution to $(2)$ is the function $$x(t) = \frac{1}{1 + e^{-rt}} \tag{3} $$ When I compare the graphs of $(1)$ and $(3)$ I see that they are not the same, and this does not appear to be a problem with the scaling of the graph. Why does evaluating $(3)$ at $n$ not give me $x_n$ (the value of $x$ after $n$ iterations) in $(1)$? Is solving the recurrence different to solving it's differential equation?",,[]
3,How can I solve this ODE with nonconstant coefficient?,How can I solve this ODE with nonconstant coefficient?,,"$x(1-x)f''(x) - \lambda f(x) = 0$, where $\lambda$ is just any constant. So far, I've just tried guessing certain functional forms, but none of them seem to work.","$x(1-x)f''(x) - \lambda f(x) = 0$, where $\lambda$ is just any constant. So far, I've just tried guessing certain functional forms, but none of them seem to work.",,['ordinary-differential-equations']
4,Doing algebra with differential operators.,Doing algebra with differential operators.,,"I was thinking about, of the derivative as an operator, like $\frac{dy}{dx}$, and i am having trouble thinking on the things you do in courses of differential equations, with the $dx, dy$, like passing them around from one side of the equation, and to the other, with apparently no problem, i don't worry much about it, because in a lot of physics texts they do that, and it works, But when i ask about WHY?, they say, don't worry for now, if it works, don't worry. Once i ask a mathematician, why and he refereed to ""high level math"" subjects and books, and i couldn't get all he said, so i am looking for a ""formal"" simplest answer to  ""Why can we do Algebra with the Differential operators, why does it work, and until what point does it works""","I was thinking about, of the derivative as an operator, like $\frac{dy}{dx}$, and i am having trouble thinking on the things you do in courses of differential equations, with the $dx, dy$, like passing them around from one side of the equation, and to the other, with apparently no problem, i don't worry much about it, because in a lot of physics texts they do that, and it works, But when i ask about WHY?, they say, don't worry for now, if it works, don't worry. Once i ask a mathematician, why and he refereed to ""high level math"" subjects and books, and i couldn't get all he said, so i am looking for a ""formal"" simplest answer to  ""Why can we do Algebra with the Differential operators, why does it work, and until what point does it works""",,['ordinary-differential-equations']
5,What are the equations modelling a vertical spring system with two masses?,What are the equations modelling a vertical spring system with two masses?,,"Modeling a vertical spring system with one mass is a pretty common problem. I looked around online and found some horizontal spring systems with two masses, but no examples of a vertical one. I'm curious, how would you set up equations modeling a vertical spring system like this: $$ ----\\ \wedge \\ \vee \\ \wedge \\ \vee\\ (m_1)\\ \wedge\\ \vee\\ \wedge\\ \vee\\ (m_2) $$ Where the first spring has constant $k_1$, and the second $k_2$. I'll let $y_1(t)$ be the position of the top mass away from its equilibrium, and $y_2(t)$ the position of the bottom mass away from its equilibrium. I choose down to be the positive direction. For the bottom mass, there is an upward force of $k_2y_2$, and a downward gravitational force of $m_2g$. So one equation should be $$ m_2y_2''=-k_2y_2+m_2g $$ For the top mass, the first spring pulls up with force $-k_1y_1$ and a downward gravitational force of $m_1g$. I'm not sure how to account for the forces of the second spring and second mass acting of the first mass. Is the equation something like $$ m_1y_1''=-k_1y_1+m_1g+\text{ something?} $$ I'm just curious how you would correctly set up the equations for this system. Thanks.","Modeling a vertical spring system with one mass is a pretty common problem. I looked around online and found some horizontal spring systems with two masses, but no examples of a vertical one. I'm curious, how would you set up equations modeling a vertical spring system like this: $$ ----\\ \wedge \\ \vee \\ \wedge \\ \vee\\ (m_1)\\ \wedge\\ \vee\\ \wedge\\ \vee\\ (m_2) $$ Where the first spring has constant $k_1$, and the second $k_2$. I'll let $y_1(t)$ be the position of the top mass away from its equilibrium, and $y_2(t)$ the position of the bottom mass away from its equilibrium. I choose down to be the positive direction. For the bottom mass, there is an upward force of $k_2y_2$, and a downward gravitational force of $m_2g$. So one equation should be $$ m_2y_2''=-k_2y_2+m_2g $$ For the top mass, the first spring pulls up with force $-k_1y_1$ and a downward gravitational force of $m_1g$. I'm not sure how to account for the forces of the second spring and second mass acting of the first mass. Is the equation something like $$ m_1y_1''=-k_1y_1+m_1g+\text{ something?} $$ I'm just curious how you would correctly set up the equations for this system. Thanks.",,"['ordinary-differential-equations', 'physics']"
6,About an integral equation,About an integral equation,,"I would like to obtain $g$  by solving the following integral equation $$ \int_s^T R(u) dg(u) + f(s,T)\int_s^T g(u)du =0$$ where $f,R:\mathbb R _+ ^*\rightarrow \mathbb R _+ $and $g: \mathbb R _+    \rightarrow[0,1]$  is non-decreasing continuous function, $g(0)=1$, $g(T)<1$ $\lim _{s\to    \infty}g(s)=0$. One can think about g as being such that  $g= 1- F$ where $F$ is the law of an    absolute continuous random variable. If we assume that $g$ is differentiable then we have $$ \int_s^T \left(f(s,T)g(u)+R(u) g'(u)\right) ~du =0, \quad \forall s \in [0,T] $$ So I am tempted to conclude that $$ f(s,T)g(u)+R(u) g'(u)=0, \quad \forall u \in [s,T]$$  therefore  $$g(s) = \exp\left(-\int _0^s \frac{f(\tau,T)}{R(\tau)}~d\tau\right), \quad \forall u \in [0,T]$$ Is my approach correct or have I made a mistake when I assumed that the integrand is zero as the integral is zero for each $s$ ? I have no restrictions on $f$ and $R$ for the moment so we can assume any necessary condition about $f$ as necessary to solve it. Could anybody give me an opinion please? Please leave a comment.  All advices are appreciated. Edit A friend wisely advised me to take a look at Volterra integral   equation which is exactly what we have here after integrating by parts   the first integral and inverting the time as follows ( after that point I use   the notation abuse  $R(u): =R(-u), g(u):=g(-u) \text{ and } f(t,T) = f(-t,T) $): $$g(t) = \alpha + \int_{-T }^t K_T(t,u))g(u) du $$ where $\alpha := (Rg)(-T)$ and $$K_T(t,u):=\frac{R(u) +  f(t,T)}{R(t)}$$ Many thanks Since this question haven't received any answer or comment but some upvotes I posted it at mathoverflow too","I would like to obtain $g$  by solving the following integral equation $$ \int_s^T R(u) dg(u) + f(s,T)\int_s^T g(u)du =0$$ where $f,R:\mathbb R _+ ^*\rightarrow \mathbb R _+ $and $g: \mathbb R _+    \rightarrow[0,1]$  is non-decreasing continuous function, $g(0)=1$, $g(T)<1$ $\lim _{s\to    \infty}g(s)=0$. One can think about g as being such that  $g= 1- F$ where $F$ is the law of an    absolute continuous random variable. If we assume that $g$ is differentiable then we have $$ \int_s^T \left(f(s,T)g(u)+R(u) g'(u)\right) ~du =0, \quad \forall s \in [0,T] $$ So I am tempted to conclude that $$ f(s,T)g(u)+R(u) g'(u)=0, \quad \forall u \in [s,T]$$  therefore  $$g(s) = \exp\left(-\int _0^s \frac{f(\tau,T)}{R(\tau)}~d\tau\right), \quad \forall u \in [0,T]$$ Is my approach correct or have I made a mistake when I assumed that the integrand is zero as the integral is zero for each $s$ ? I have no restrictions on $f$ and $R$ for the moment so we can assume any necessary condition about $f$ as necessary to solve it. Could anybody give me an opinion please? Please leave a comment.  All advices are appreciated. Edit A friend wisely advised me to take a look at Volterra integral   equation which is exactly what we have here after integrating by parts   the first integral and inverting the time as follows ( after that point I use   the notation abuse  $R(u): =R(-u), g(u):=g(-u) \text{ and } f(t,T) = f(-t,T) $): $$g(t) = \alpha + \int_{-T }^t K_T(t,u))g(u) du $$ where $\alpha := (Rg)(-T)$ and $$K_T(t,u):=\frac{R(u) +  f(t,T)}{R(t)}$$ Many thanks Since this question haven't received any answer or comment but some upvotes I posted it at mathoverflow too",,"['ordinary-differential-equations', 'functional-equations', 'integral-equations']"
7,General Solution of $y'(x)+p(x)e^{r(x) y(x)}=q(x)$,General Solution of,y'(x)+p(x)e^{r(x) y(x)}=q(x),"I solved the case for the non-homogenous constant coefficients case and I wondered if there is a way to find a general solution to a non-constant coefficient case. I don't know how to approach this at all, the substitution $y(x)=\frac{\log (v(x))}{r(x)}$ gets problematic immediately.","I solved the case for the non-homogenous constant coefficients case and I wondered if there is a way to find a general solution to a non-constant coefficient case. I don't know how to approach this at all, the substitution $y(x)=\frac{\log (v(x))}{r(x)}$ gets problematic immediately.",,"['ordinary-differential-equations', 'closed-form']"
8,Recursive identity for elliptic lattice constants $\sum_{\lambda\in\Lambda\setminus0} \lambda^{-2k}$,Recursive identity for elliptic lattice constants,\sum_{\lambda\in\Lambda\setminus0} \lambda^{-2k},"I am stuck on Exercise 3 in these notes . To keep this question self-contained: we have $\displaystyle\Lambda=\langle\omega_1,\omega_2\rangle=\omega_1\Bbb Z+\omega_2\Bbb Z\subset\Bbb C,$ $\displaystyle \wp(z)  =\frac{1}{z^2}+\sum_{\lambda\in\Lambda\setminus0}\left(\frac{1}{(z-\lambda)^2}-\frac{1}{\lambda^2}\right)=\frac{1}{z^2}+\sum_{k=2}^\infty(2k-1)G_{2k}z^{2k-2} ~~ (z\approx0), $ $\displaystyle G_k=\sum_{\lambda\in\Lambda\setminus0}\frac{1}{\lambda^k}, \quad G_{\rm odd}=0,$ $\dot{\wp}^2=4\wp^3-40G_4\wp-160G_6.$ (See the link for proof of $\wp$'s Taylor expansion and the differential equation.) I want to prove $$(n-3)(2n+1)(2n-1)G_{2n}=\sum_{\substack{k+l=n \\ k,l\ge2}} (2k-1)(2l-1)G_{2k}G_{2l}$$ for $n\ge4$. Multiplying both sides by $z^{2n}$ and summing should result in a differential equation that will surely yield an unwieldy equation involving $z,\wp,\wp^2,\dot{\wp},\ddot{\wp}$. I have no how to derive this from the known differential equation $(4)$, or if perhaps I should try a route other than generating functions and differential equations to prove the identity. Perhaps I should augment the generating function because the identity is only being proven for $n\ge4$?","I am stuck on Exercise 3 in these notes . To keep this question self-contained: we have $\displaystyle\Lambda=\langle\omega_1,\omega_2\rangle=\omega_1\Bbb Z+\omega_2\Bbb Z\subset\Bbb C,$ $\displaystyle \wp(z)  =\frac{1}{z^2}+\sum_{\lambda\in\Lambda\setminus0}\left(\frac{1}{(z-\lambda)^2}-\frac{1}{\lambda^2}\right)=\frac{1}{z^2}+\sum_{k=2}^\infty(2k-1)G_{2k}z^{2k-2} ~~ (z\approx0), $ $\displaystyle G_k=\sum_{\lambda\in\Lambda\setminus0}\frac{1}{\lambda^k}, \quad G_{\rm odd}=0,$ $\dot{\wp}^2=4\wp^3-40G_4\wp-160G_6.$ (See the link for proof of $\wp$'s Taylor expansion and the differential equation.) I want to prove $$(n-3)(2n+1)(2n-1)G_{2n}=\sum_{\substack{k+l=n \\ k,l\ge2}} (2k-1)(2l-1)G_{2k}G_{2l}$$ for $n\ge4$. Multiplying both sides by $z^{2n}$ and summing should result in a differential equation that will surely yield an unwieldy equation involving $z,\wp,\wp^2,\dot{\wp},\ddot{\wp}$. I have no how to derive this from the known differential equation $(4)$, or if perhaps I should try a route other than generating functions and differential equations to prove the identity. Perhaps I should augment the generating function because the identity is only being proven for $n\ge4$?",,"['ordinary-differential-equations', 'recurrence-relations', 'modular-forms', 'elliptic-functions']"
9,Solving 2nd order ODE with Frobenius method - problems with summation symbol,Solving 2nd order ODE with Frobenius method - problems with summation symbol,,"I'm trying to solve the ODE: $$ y''(x) + \frac{2x}{(x-1)(2x-1)} y'(x) - \frac{2}{(x-1)(2x-1)} y(x) = 0 $$ I'm trying to find a solution by the Frobenius method, expanding a power series of the solution around $x = \frac 12$, that is in a series of terms $(x - 1/2)^{n}$. The indicial equation has two roots, $\alpha = 0$ and $\alpha=2$. For $\alpha= 2$ the solution will be $$ y(x) = \sum_{k=0}^{\infty} a_k \left( x - \frac 12 \right)^{k+2}$$ If I say that  $$ p(x) =\frac{1}{x-\frac12} \frac{x}{x-1} =\frac{1}{x-\frac12} \left(-1 + \sum_{i = 0}^{\infty} -2^{i+1} \left( x -\frac 12\right)^i \right) $$  and  $$ q(x) = -\frac{1}{\left( x -\frac12 \right)^2} \frac{x-1/2}{x-1} =\frac{1}{\left( x -\frac12 \right)^2} \left( -1 + \sum_{j=0}^{\infty} 2^{j+1} \left( x - \frac 12\right)^{j+1} \right) $$ and plug that and the power series expansion for $y, y', y''$ in the ODE, I get: $$\sum_{k=0}^{\infty} (k+2)(k+1) a_k \left( x- \frac 12\right)^k + \frac{1}{x-\frac12} \left(-1 + \sum_{i = 0}^{\infty} -2^{i+1} \left( x -\frac 12\right)^i \right) \left(\sum_{k=0}^{\infty} (k+2) a_k \left( x- \frac 12\right)^{k+1}\right) + \frac{1}{\left( x -\frac12 \right)^2} \left( -1 + \sum_{j=0}^{\infty} 2^{j+1} \left( x - \frac 12\right)^{j+1} \right) \left(\sum_{k=0}^{\infty} a_k \left( x - \frac 12 \right)^{k+2} \right)$$ I go through the math and get $$\sum_{k=0}^{\infty} (k+2)(k+1) a_k \left( x- \frac 12\right)^k +  \sum_{k=0}^{\infty} \left( \sum_{i=0}^{k} -a_i (i+2) 2^{k-i+1} \right) \left( x- \frac 12\right)^{k} +  \sum_{k=0}^{\infty} (k+2) a_k \left( x - \frac 12 \right)^k + \sum_{k=0}^{\infty} \left( \sum_{j=0}^{k} 2^{k-j} a_j \right) \left(x - \frac 12 \right)^k - \sum_{k=0}^{\infty} a_k \left( x - \frac 12 \right)^k = 0  $$ Now I equate the coefficients ofequal powers to 0: $$k = 0 , 2 a_0 - 4 a_ + 2 a_0 + a_0 - a_0 = 0 <=> 0 a_0 = 0 $$ $$k = 1 , 6 a_1 - 8 a_0 - 6 a_1 + 3 a_1 + 2 a_0 + a_1 - a_1 = 0 <=> a1 = (6/3) a_0$$ Am I now getting this right?","I'm trying to solve the ODE: $$ y''(x) + \frac{2x}{(x-1)(2x-1)} y'(x) - \frac{2}{(x-1)(2x-1)} y(x) = 0 $$ I'm trying to find a solution by the Frobenius method, expanding a power series of the solution around $x = \frac 12$, that is in a series of terms $(x - 1/2)^{n}$. The indicial equation has two roots, $\alpha = 0$ and $\alpha=2$. For $\alpha= 2$ the solution will be $$ y(x) = \sum_{k=0}^{\infty} a_k \left( x - \frac 12 \right)^{k+2}$$ If I say that  $$ p(x) =\frac{1}{x-\frac12} \frac{x}{x-1} =\frac{1}{x-\frac12} \left(-1 + \sum_{i = 0}^{\infty} -2^{i+1} \left( x -\frac 12\right)^i \right) $$  and  $$ q(x) = -\frac{1}{\left( x -\frac12 \right)^2} \frac{x-1/2}{x-1} =\frac{1}{\left( x -\frac12 \right)^2} \left( -1 + \sum_{j=0}^{\infty} 2^{j+1} \left( x - \frac 12\right)^{j+1} \right) $$ and plug that and the power series expansion for $y, y', y''$ in the ODE, I get: $$\sum_{k=0}^{\infty} (k+2)(k+1) a_k \left( x- \frac 12\right)^k + \frac{1}{x-\frac12} \left(-1 + \sum_{i = 0}^{\infty} -2^{i+1} \left( x -\frac 12\right)^i \right) \left(\sum_{k=0}^{\infty} (k+2) a_k \left( x- \frac 12\right)^{k+1}\right) + \frac{1}{\left( x -\frac12 \right)^2} \left( -1 + \sum_{j=0}^{\infty} 2^{j+1} \left( x - \frac 12\right)^{j+1} \right) \left(\sum_{k=0}^{\infty} a_k \left( x - \frac 12 \right)^{k+2} \right)$$ I go through the math and get $$\sum_{k=0}^{\infty} (k+2)(k+1) a_k \left( x- \frac 12\right)^k +  \sum_{k=0}^{\infty} \left( \sum_{i=0}^{k} -a_i (i+2) 2^{k-i+1} \right) \left( x- \frac 12\right)^{k} +  \sum_{k=0}^{\infty} (k+2) a_k \left( x - \frac 12 \right)^k + \sum_{k=0}^{\infty} \left( \sum_{j=0}^{k} 2^{k-j} a_j \right) \left(x - \frac 12 \right)^k - \sum_{k=0}^{\infty} a_k \left( x - \frac 12 \right)^k = 0  $$ Now I equate the coefficients ofequal powers to 0: $$k = 0 , 2 a_0 - 4 a_ + 2 a_0 + a_0 - a_0 = 0 <=> 0 a_0 = 0 $$ $$k = 1 , 6 a_1 - 8 a_0 - 6 a_1 + 3 a_1 + 2 a_0 + a_1 - a_1 = 0 <=> a1 = (6/3) a_0$$ Am I now getting this right?",,"['ordinary-differential-equations', 'summation', 'power-series']"
10,Properties of a Sturm-Liouville problem,Properties of a Sturm-Liouville problem,,"I want to show the following problem is regular. To show a Sturm-Liovulle problem is regular we need to demonstrate that $y''+\frac{b}{a}y'+\frac{1}{a}(c+\lambda)=0$ where $p(x)=e^{\int \frac{b}{a}\,dx}.$  We then have $[p(x)y']'+[q(x)+r(x)\lambda]y=0.$  Now this is regular if (i) $p,p',r,q$ are all real valued continuous on a finite interval $[a,b]$; (ii) $p(x)>0$ and $r(x)>0$ on $[a,b].$ Consider $x^2y''+xy'+\lambda y=0$ where $x>0$, $y(0)=y(e)=1$, and $y'(0)-2y'(e)=2.$ Dividing through by $x^2$ we get $y''+\frac{1}{x}y'+\frac{\lambda}{x^2}y=0.$  If we let $p(x)=x$  we then have $xy''+y'+\frac{\lambda}{x}y=0.$  This yields us $[xy']'+\frac{\lambda}{x}=0.$  If we let $p(x)=x$, $q(x)=0$. and $r(x)=1/x$, does this show that this problem is regular? Am I missing a step?","I want to show the following problem is regular. To show a Sturm-Liovulle problem is regular we need to demonstrate that $y''+\frac{b}{a}y'+\frac{1}{a}(c+\lambda)=0$ where $p(x)=e^{\int \frac{b}{a}\,dx}.$  We then have $[p(x)y']'+[q(x)+r(x)\lambda]y=0.$  Now this is regular if (i) $p,p',r,q$ are all real valued continuous on a finite interval $[a,b]$; (ii) $p(x)>0$ and $r(x)>0$ on $[a,b].$ Consider $x^2y''+xy'+\lambda y=0$ where $x>0$, $y(0)=y(e)=1$, and $y'(0)-2y'(e)=2.$ Dividing through by $x^2$ we get $y''+\frac{1}{x}y'+\frac{\lambda}{x^2}y=0.$  If we let $p(x)=x$  we then have $xy''+y'+\frac{\lambda}{x}y=0.$  This yields us $[xy']'+\frac{\lambda}{x}=0.$  If we let $p(x)=x$, $q(x)=0$. and $r(x)=1/x$, does this show that this problem is regular? Am I missing a step?",,['ordinary-differential-equations']
11,"The system $\dot{x}=x^2$, $\dot y=-y$, has infinitely many (local) center manifolds","The system , , has infinitely many (local) center manifolds",\dot{x}=x^2 \dot y=-y,"Consider the system, \begin{align} \dot{x}&=x^2 \\  \dot y&=-y \end{align} I am trying to show that this system has infinitely many local center manifolds. Here is what I have done so far : Clearly the system has a rest point at the origin. I linearized the system at the origin and got that $\lambda=0$ and $\lambda=-1$ to be the eigenvalues of the linearized operator. Turns out that the  corresponding eigenvectors are $(1,0)^T$ and $(0,1)^T$ respectively. So solutions of the linearized system can be written as follows. \begin{align} x&=c_1\\ y&=c_2e^{-t} \end{align} When $c_1=0$, $y\to0$ as $t\to \infty$. Thus, by definition of the stable manifold the $y$ axis must be a stable manifold. Also the unstable manifold is the trivial set $(0,0)$. Questions : How do I go about showing that there are infinitely many center manifolds?. Does solving the system explicitly have anything to do with answering this question?. In particular any orbit of the system not on the stable manifold $(0,y)$ satisfies an equation of the form $\displaystyle y=c_2e^{\frac{1}{x}-c_1}$. I feel like I am not seeing something simple. Can somebody give an explanation? Context : In the ODE book I am using ( C. Chicone ) this problem is given as an example to show that the center manifold need not be unique. (After proving the invariant manifold theorem).","Consider the system, \begin{align} \dot{x}&=x^2 \\  \dot y&=-y \end{align} I am trying to show that this system has infinitely many local center manifolds. Here is what I have done so far : Clearly the system has a rest point at the origin. I linearized the system at the origin and got that $\lambda=0$ and $\lambda=-1$ to be the eigenvalues of the linearized operator. Turns out that the  corresponding eigenvectors are $(1,0)^T$ and $(0,1)^T$ respectively. So solutions of the linearized system can be written as follows. \begin{align} x&=c_1\\ y&=c_2e^{-t} \end{align} When $c_1=0$, $y\to0$ as $t\to \infty$. Thus, by definition of the stable manifold the $y$ axis must be a stable manifold. Also the unstable manifold is the trivial set $(0,0)$. Questions : How do I go about showing that there are infinitely many center manifolds?. Does solving the system explicitly have anything to do with answering this question?. In particular any orbit of the system not on the stable manifold $(0,y)$ satisfies an equation of the form $\displaystyle y=c_2e^{\frac{1}{x}-c_1}$. I feel like I am not seeing something simple. Can somebody give an explanation? Context : In the ODE book I am using ( C. Chicone ) this problem is given as an example to show that the center manifold need not be unique. (After proving the invariant manifold theorem).",,"['ordinary-differential-equations', 'manifolds', 'dynamical-systems']"
12,First order ODE: $tx'(x'+2)=x$,First order ODE:,tx'(x'+2)=x,$$tx'(x'+2)=x$$ First I multiplied it: $$t(x')^2+2tx'=x$$ Then differentiated both sides: $$(x')^2+2tx'x''+2tx''+x'=0$$ substituted $p=x'$ and rewrote it as a multiplication $$(2p't+p)(p+1)=0$$ So either $(2p't+p)=0$ or $p+1=0$ The first one gives $p=\frac{C}{\sqrt{T}}$ The second one gives $p=-1$. My question is how do I take the antidervative of this in order to get the answer for the actual equation?,$$tx'(x'+2)=x$$ First I multiplied it: $$t(x')^2+2tx'=x$$ Then differentiated both sides: $$(x')^2+2tx'x''+2tx''+x'=0$$ substituted $p=x'$ and rewrote it as a multiplication $$(2p't+p)(p+1)=0$$ So either $(2p't+p)=0$ or $p+1=0$ The first one gives $p=\frac{C}{\sqrt{T}}$ The second one gives $p=-1$. My question is how do I take the antidervative of this in order to get the answer for the actual equation?,,['ordinary-differential-equations']
13,"Intuitive interpretation of $\frac{\partial S(a,t)}{\partial t} = -\frac{\partial S(a,t)}{\partial a}$",Intuitive interpretation of,"\frac{\partial S(a,t)}{\partial t} = -\frac{\partial S(a,t)}{\partial a}","I'm trying to visualize what the following equation is saying: $$\frac{\partial S(a,t)}{\partial t} = -\frac{\partial S(a,t)}{\partial a}$$ where $S$ is a probability-density, but I think you can assume any other quantity of interest (by the way, my variable of interest is the susceptibility of a person to a given disease), $a$ is a variable such as age and $t$ is time. I understand a simple solution for this equation is given by $S(a,t) = c (a - t)$. However, after plotting this plane, I can't intuitively grasp what the differential equation is actually doing. As I understand it, the left-hand side is describing the variation of $S(a,t)$ when we change a bit the time $t$. This should be equal to the variation of $S(a,t)$ when we change a bit the age $a$. In this particular example, I expect a reduction in $S(a,t)$ as we move through time (because here the susceptibility should decrease when people get older.) Therefore, I negative sign is appropriate. However, I don't see that intuition in the solution $S(a,t) = c (a - t)$ when I plot it. As a side note, the equation above doesn't describe the full dynamics of susceptibility. I dropped a few terms in order to simplify the analysis. UPDATE: I guess what is confusing me is that, in the left-hand side equation, we have the variation along $a$: $\displaystyle \frac{S(a + \Delta, t) - S(a,t)}{\Delta a}$, so we stay at the same time $t$ and move a bit in the $a$ direction . Similarly, the right side $\displaystyle \frac{S(a, t + \Delta) - S(a,t)}{\Delta t}$ means that we stay at the same (age) $a$ and move a bit in the $t$ direction. However, when I plot the solution $S(a,t)$, I can't see this solution expressing that a change in $t$ should be equal to a change in $a$ with opposite sign. Thanks in advance.","I'm trying to visualize what the following equation is saying: $$\frac{\partial S(a,t)}{\partial t} = -\frac{\partial S(a,t)}{\partial a}$$ where $S$ is a probability-density, but I think you can assume any other quantity of interest (by the way, my variable of interest is the susceptibility of a person to a given disease), $a$ is a variable such as age and $t$ is time. I understand a simple solution for this equation is given by $S(a,t) = c (a - t)$. However, after plotting this plane, I can't intuitively grasp what the differential equation is actually doing. As I understand it, the left-hand side is describing the variation of $S(a,t)$ when we change a bit the time $t$. This should be equal to the variation of $S(a,t)$ when we change a bit the age $a$. In this particular example, I expect a reduction in $S(a,t)$ as we move through time (because here the susceptibility should decrease when people get older.) Therefore, I negative sign is appropriate. However, I don't see that intuition in the solution $S(a,t) = c (a - t)$ when I plot it. As a side note, the equation above doesn't describe the full dynamics of susceptibility. I dropped a few terms in order to simplify the analysis. UPDATE: I guess what is confusing me is that, in the left-hand side equation, we have the variation along $a$: $\displaystyle \frac{S(a + \Delta, t) - S(a,t)}{\Delta a}$, so we stay at the same time $t$ and move a bit in the $a$ direction . Similarly, the right side $\displaystyle \frac{S(a, t + \Delta) - S(a,t)}{\Delta t}$ means that we stay at the same (age) $a$ and move a bit in the $t$ direction. However, when I plot the solution $S(a,t)$, I can't see this solution expressing that a change in $t$ should be equal to a change in $a$ with opposite sign. Thanks in advance.",,"['ordinary-differential-equations', 'partial-differential-equations', 'intuition']"
14,Differential equations books using lots of algebraic topology?,Differential equations books using lots of algebraic topology?,,"The wikipedia page on 'Algebraic Topology' contains the following sentence: One can use the differential structure of smooth manifolds via de Rham cohomology, or Čech or sheaf cohomology to investigate the solvability of differential equations defined on the manifold in question. Unfortunately, no concrete references are added. Are there good textbooks or other sources on applications of algebraic topology to solvability (and perhaps other qualitative aspects) of differential equations?","The wikipedia page on 'Algebraic Topology' contains the following sentence: One can use the differential structure of smooth manifolds via de Rham cohomology, or Čech or sheaf cohomology to investigate the solvability of differential equations defined on the manifold in question. Unfortunately, no concrete references are added. Are there good textbooks or other sources on applications of algebraic topology to solvability (and perhaps other qualitative aspects) of differential equations?",,"['ordinary-differential-equations', 'reference-request', 'algebraic-topology']"
15,Finding formula using tangentline information,Finding formula using tangentline information,,"I'm new on SE, but I hope that you guys will help me with this question I have. I'm currently not capable of using $\LaTeX$ or anything nice to set up my formulas, but I hope you will bear over with that - at least for now. I have the following information: A function $f$ solves the differential equation $\frac{dy}{dx} = 2x+5-y$ And the line with the equation $y=1$ (I notice the two $y$'s as well...) is a tangent line to $f$. And my question is then: How do I find the formula for $f$ using only this information? What I have done so far (basically only using separation of variables): $\frac{dy}{dx} = 2x+5-y$ $\frac{dy}{y} = (2x+5)dx$ $\int(1/y)dy = \int(2x+5)dx$ $\ln(y) = x^2 + 5x + k$ $y=e^{x^2 + 5x + k}$ Also I would be pleased if you would check if what I have done is correct, as I have not actually learned separation of variables yet... Thanks in advance","I'm new on SE, but I hope that you guys will help me with this question I have. I'm currently not capable of using $\LaTeX$ or anything nice to set up my formulas, but I hope you will bear over with that - at least for now. I have the following information: A function $f$ solves the differential equation $\frac{dy}{dx} = 2x+5-y$ And the line with the equation $y=1$ (I notice the two $y$'s as well...) is a tangent line to $f$. And my question is then: How do I find the formula for $f$ using only this information? What I have done so far (basically only using separation of variables): $\frac{dy}{dx} = 2x+5-y$ $\frac{dy}{y} = (2x+5)dx$ $\int(1/y)dy = \int(2x+5)dx$ $\ln(y) = x^2 + 5x + k$ $y=e^{x^2 + 5x + k}$ Also I would be pleased if you would check if what I have done is correct, as I have not actually learned separation of variables yet... Thanks in advance",,['ordinary-differential-equations']
16,Examples of potentials for which Schrödinger equation lacks discrete points in continuous spectrum,Examples of potentials for which Schrödinger equation lacks discrete points in continuous spectrum,,"In Landau, Lifshitz, ""Quantum Mechanics, non-relativistic theory"" in $\S18$ ""The fundamental properties of Schrödinger's equation"" the following is said in a footnote: it must be mentioned that, for some particular mathematical forms of the function $U(x,y,z)$ (which have no physical significance), a discrete set of values may be absent from the otherwise continuous spectrum. I wonder, what are the examples of such mathematical forms of potential?","In Landau, Lifshitz, ""Quantum Mechanics, non-relativistic theory"" in $\S18$ ""The fundamental properties of Schrödinger's equation"" the following is said in a footnote: it must be mentioned that, for some particular mathematical forms of the function $U(x,y,z)$ (which have no physical significance), a discrete set of values may be absent from the otherwise continuous spectrum. I wonder, what are the examples of such mathematical forms of potential?",,"['ordinary-differential-equations', 'eigenvalues-eigenvectors']"
17,Solving $y''+xy'+y=0$,Solving,y''+xy'+y=0,"So, as a homework question, I am trying to solve $y''+xy'+y=0$. I checked that this is exact and gives  $$(y'+xy)'=0$$ $$y'+xy = C_1$$ Using integrating factor $e^{\int xdx} = e^{x^2/2}$ : $$(ye^{x^2/2})' = C_1 e^{x^2/2}$$ At this point, integrating $e^{x^2/2}$ is needed which can't be done (without using the error function). I did go ahead to solve it and get $$y=C_1e^{-x^2/2}\int e^{x^2/2}dx + C_2e^{-x^2/2}$$ which doesn't seem to be the solution when subsituted back into the original equation. Am I doing something wrong here?","So, as a homework question, I am trying to solve $y''+xy'+y=0$. I checked that this is exact and gives  $$(y'+xy)'=0$$ $$y'+xy = C_1$$ Using integrating factor $e^{\int xdx} = e^{x^2/2}$ : $$(ye^{x^2/2})' = C_1 e^{x^2/2}$$ At this point, integrating $e^{x^2/2}$ is needed which can't be done (without using the error function). I did go ahead to solve it and get $$y=C_1e^{-x^2/2}\int e^{x^2/2}dx + C_2e^{-x^2/2}$$ which doesn't seem to be the solution when subsituted back into the original equation. Am I doing something wrong here?",,['ordinary-differential-equations']
18,Differential equations. Shortcut way to solve this problem?,Differential equations. Shortcut way to solve this problem?,,"Disclaimer: I am not a student trying to get free internet homework help. I am an adult who is learning Calculus from a textbook. I am deeply grateful to the members of this community for their time. Here is the problem I am trying to solve: The weight in pounds of a certain bear cub $t$ after birth is given by   $w(t)$. If $w(2)=36, w(7)=84,$ and $\frac{dw}{dt}$ was proportional to   the cub's weight for the first $15$ months of his life, how much did   the cub weigh when he was $11$ months old? A friend of mine emailed me his solution: No calculus involved. $\frac{dy}{dy} = k y$ That implies $y=\exp(kt+C)$ One $(t,y)$   pair gives $k$, another gives $C$. You don't have to know the   differentiation, just the result, hence, no Calculus. Can someone decipher what he's saying?  What is $\exp()$?  Exponent?  $y$ equals an exponent?  Huh?  I had no idea what he meant, and asked for clarification.    His alternate solution was If Diff. Eq. is of the form $\frac{dy}{dt} = ky$, then write solution as $y=$   $\exp(...)$ That's it! I don't understand how this problem can be solved in just 1 line.  It's clear that he's addressing this problem with a totally different approach than the traditional ""Calculus/DiffEq"" approach. Below is is how I did it.   Although I arrive at the correct answer after 10 mins. and an entire page of paper, I'd like to understand the 1-liner shortcut method above, as it seems a big time saver.  Can anyone explain his approach written out legibly in a photo scan?","Disclaimer: I am not a student trying to get free internet homework help. I am an adult who is learning Calculus from a textbook. I am deeply grateful to the members of this community for their time. Here is the problem I am trying to solve: The weight in pounds of a certain bear cub $t$ after birth is given by   $w(t)$. If $w(2)=36, w(7)=84,$ and $\frac{dw}{dt}$ was proportional to   the cub's weight for the first $15$ months of his life, how much did   the cub weigh when he was $11$ months old? A friend of mine emailed me his solution: No calculus involved. $\frac{dy}{dy} = k y$ That implies $y=\exp(kt+C)$ One $(t,y)$   pair gives $k$, another gives $C$. You don't have to know the   differentiation, just the result, hence, no Calculus. Can someone decipher what he's saying?  What is $\exp()$?  Exponent?  $y$ equals an exponent?  Huh?  I had no idea what he meant, and asked for clarification.    His alternate solution was If Diff. Eq. is of the form $\frac{dy}{dt} = ky$, then write solution as $y=$   $\exp(...)$ That's it! I don't understand how this problem can be solved in just 1 line.  It's clear that he's addressing this problem with a totally different approach than the traditional ""Calculus/DiffEq"" approach. Below is is how I did it.   Although I arrive at the correct answer after 10 mins. and an entire page of paper, I'd like to understand the 1-liner shortcut method above, as it seems a big time saver.  Can anyone explain his approach written out legibly in a photo scan?",,"['calculus', 'ordinary-differential-equations']"
19,Symmetry of the Riemannian curvature tensor,Symmetry of the Riemannian curvature tensor,,"The Riemannian curvature tensor, in local coordinates, $R_{ijkl}$, has the following symmetries: $$R_{ijkl}+R_{jikl}=0;$$ $$R_{ijkl}+R_{ijlk}=0;$$ $$R_{ijkl}=R_{klij};$$ $$R_{ijkl}+R_{jkil}+R_{kijl}=0.$$ These algebraic identities give the degree of the freedom of curvature to be $\frac{1}{12}n^2(n^2-1)$. where $n$ is the dimension of manifold. I believe, but have trouble to show, that these identities completely describe the pointwise symmetries of the curvature tensor, i.e. given any $\frac{1}{12}n^2(n^2-1)$ numbers, we can find a Riemannian manifold such that the curvature tensor $R_{ijkl}$ evaluated at certain point, in some local coordinate, is of these particular numbers.","The Riemannian curvature tensor, in local coordinates, $R_{ijkl}$, has the following symmetries: $$R_{ijkl}+R_{jikl}=0;$$ $$R_{ijkl}+R_{ijlk}=0;$$ $$R_{ijkl}=R_{klij};$$ $$R_{ijkl}+R_{jkil}+R_{kijl}=0.$$ These algebraic identities give the degree of the freedom of curvature to be $\frac{1}{12}n^2(n^2-1)$. where $n$ is the dimension of manifold. I believe, but have trouble to show, that these identities completely describe the pointwise symmetries of the curvature tensor, i.e. given any $\frac{1}{12}n^2(n^2-1)$ numbers, we can find a Riemannian manifold such that the curvature tensor $R_{ijkl}$ evaluated at certain point, in some local coordinate, is of these particular numbers.",,"['ordinary-differential-equations', 'riemannian-geometry', 'tensors']"
20,Solve a second order DEQ using Euler's method in MATLAB,Solve a second order DEQ using Euler's method in MATLAB,,"I need to solve the equation below with Euler's method: $$y''+ \pi ye^{x/3}(2y'  \sin(\pi x)+\pi y\cos (\pi x)) = \frac{y}{9}$$ for the initial conditions $y(0)=1$, $y'(0)=-1/3$ So I know I need to turn the problem into a system of two first order differential equations. Therefore $u_1=y'$ and $u_2=y''$ I can now write the system as: $$u_1=y' \\ u_2=\dfrac{y}{9}-\pi y e^{x/3}(2u_1  \sin(\pi x)-\pi y\cos (\pi x))$$ How do I proceed from here?","I need to solve the equation below with Euler's method: $$y''+ \pi ye^{x/3}(2y'  \sin(\pi x)+\pi y\cos (\pi x)) = \frac{y}{9}$$ for the initial conditions $y(0)=1$, $y'(0)=-1/3$ So I know I need to turn the problem into a system of two first order differential equations. Therefore $u_1=y'$ and $u_2=y''$ I can now write the system as: $$u_1=y' \\ u_2=\dfrac{y}{9}-\pi y e^{x/3}(2u_1  \sin(\pi x)-\pi y\cos (\pi x))$$ How do I proceed from here?",,"['ordinary-differential-equations', 'matlab']"
21,Getting the PDE using Laplace equations,Getting the PDE using Laplace equations,,"Hey guys I need help on one of the past midterm question that I came across. I am pretty sure I got (a) right. But if it is wrong could you please let me know. But its (b) and (c) that I got in trouble with. Any help or solution to those would be really helpful. I have a midterm in 2 days I just want to get confident in all this. Question: Let D be the inﬁnite vertical strip $D = (0 \leq  x \leq  1, -\infty < y < \infty)$: (a) show that the function $u(x, y) = sin(\alphax)sinh(\alphay)$ satisﬁes the (two-dimensional version of the) Laplace equation in D for any real value of the constant . For which value(s) of $\alpha$  does the above proposed solution satisfy the Dirichlet boundary conditions $u = 0$ on the boundary of D? b) consider the following PDE problem: $\bigtriangleup u = g(x, y)$ on D $u = h$      on the boundary of D where $\bigtriangleup$ denotes the two-dimensional Laplace operator $\frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2} $ Is the problem well-posed? Justify your answer using the results of item (a). (c) Taking inspiration for item (a), guess a solution of the Laplace equation in D which satisﬁes the Neumann boundary conditions $\frac{\partial u}{\partial n} = 0$ on the boundary of D. What can you conclude about the well-posedness ofthe following PDE problem? $\bigtriangleup u = g(x, y)$   on D $\frac{\partial u}{\partial n} = h$ on the boundary of D: My attempt: (a) So I know the laplace equation is $u_{xx} + u_{yy} = 0$ So I found $u_{xx} = - \alpha^2 sin(\alpha x)sinh(\alpha y)$ and $u_{yy} = \alpha^2 sin(\alpha x)sinh(\alpha y)$ So therefore $u_{xx} + u_{yy} = 0$ if I plug it into the equation. Now to find the values of $\alpha$ we know $u(0, y) = sin(0)sin(\alpha y) = 0$ and $u(1,y) = sin(\alpha)sinh(\alpha y)$ Therefore $sin(\alpha) = 0 \Rightarrow $ only when $\alpha = k \pi$ where $k \in \mathbb{R}$ now b and c I get confused. Please help out I would be really greatful thank you.","Hey guys I need help on one of the past midterm question that I came across. I am pretty sure I got (a) right. But if it is wrong could you please let me know. But its (b) and (c) that I got in trouble with. Any help or solution to those would be really helpful. I have a midterm in 2 days I just want to get confident in all this. Question: Let D be the inﬁnite vertical strip $D = (0 \leq  x \leq  1, -\infty < y < \infty)$: (a) show that the function $u(x, y) = sin(\alphax)sinh(\alphay)$ satisﬁes the (two-dimensional version of the) Laplace equation in D for any real value of the constant . For which value(s) of $\alpha$  does the above proposed solution satisfy the Dirichlet boundary conditions $u = 0$ on the boundary of D? b) consider the following PDE problem: $\bigtriangleup u = g(x, y)$ on D $u = h$      on the boundary of D where $\bigtriangleup$ denotes the two-dimensional Laplace operator $\frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2} $ Is the problem well-posed? Justify your answer using the results of item (a). (c) Taking inspiration for item (a), guess a solution of the Laplace equation in D which satisﬁes the Neumann boundary conditions $\frac{\partial u}{\partial n} = 0$ on the boundary of D. What can you conclude about the well-posedness ofthe following PDE problem? $\bigtriangleup u = g(x, y)$   on D $\frac{\partial u}{\partial n} = h$ on the boundary of D: My attempt: (a) So I know the laplace equation is $u_{xx} + u_{yy} = 0$ So I found $u_{xx} = - \alpha^2 sin(\alpha x)sinh(\alpha y)$ and $u_{yy} = \alpha^2 sin(\alpha x)sinh(\alpha y)$ So therefore $u_{xx} + u_{yy} = 0$ if I plug it into the equation. Now to find the values of $\alpha$ we know $u(0, y) = sin(0)sin(\alpha y) = 0$ and $u(1,y) = sin(\alpha)sinh(\alpha y)$ Therefore $sin(\alpha) = 0 \Rightarrow $ only when $\alpha = k \pi$ where $k \in \mathbb{R}$ now b and c I get confused. Please help out I would be really greatful thank you.",,"['ordinary-differential-equations', 'partial-differential-equations']"
22,"When solving a DE, what should our premises be?","When solving a DE, what should our premises be?",,"To solve a a DE rigorously, I'm not really sure how to set up the problem, in terms of definitions and premises. So as a simple example, suppose we're given a  function $f : \mathbb{R}^2 \rightarrow \mathbb{R},$ and we wish find the general solution to the first-order DE of the form $y'=f(x,y).$ My questions are as follows. Should we immediately define a function $S$ that takes a set $X$ and returns the set of all solutions on $X$, denoted $S_X$? In the sense of: $S_X = \{y : X \rightarrow \mathbb{R} \mid y'=f(x,y), \;y \mbox{ diff}\}.$ Should we assume that we have a set $X$ (on which we're looking for solutions) that is fixed but arbitrary? If so, should we furthermore assume that $X$ is an interval? That its open? Does it need to be non-empty and/or have two or more elements? If the answer to any of the above questions is 'yes,' should we furthermore assume that we have a function $y : X \rightarrow \mathbb{R}$ that is fixed but arbitrary? If so, should we furthermore assume that $y$ is differentiable? That $y$ satisfies the DE? Note that, if the answer to this final question is 'yes', we can write this more simply as ""Assume $y \in S_X$"" so long as we've defined $S_X$. Is there anything else that needs to be done at the outset?","To solve a a DE rigorously, I'm not really sure how to set up the problem, in terms of definitions and premises. So as a simple example, suppose we're given a  function $f : \mathbb{R}^2 \rightarrow \mathbb{R},$ and we wish find the general solution to the first-order DE of the form $y'=f(x,y).$ My questions are as follows. Should we immediately define a function $S$ that takes a set $X$ and returns the set of all solutions on $X$, denoted $S_X$? In the sense of: $S_X = \{y : X \rightarrow \mathbb{R} \mid y'=f(x,y), \;y \mbox{ diff}\}.$ Should we assume that we have a set $X$ (on which we're looking for solutions) that is fixed but arbitrary? If so, should we furthermore assume that $X$ is an interval? That its open? Does it need to be non-empty and/or have two or more elements? If the answer to any of the above questions is 'yes,' should we furthermore assume that we have a function $y : X \rightarrow \mathbb{R}$ that is fixed but arbitrary? If so, should we furthermore assume that $y$ is differentiable? That $y$ satisfies the DE? Note that, if the answer to this final question is 'yes', we can write this more simply as ""Assume $y \in S_X$"" so long as we've defined $S_X$. Is there anything else that needs to be done at the outset?",,['ordinary-differential-equations']
23,How to solve that ode,How to solve that ode,,I'm trying solve the following differential equation: $$ x\left(\frac{dx}{dy}\right)^2+y\frac{dx}{dy}=x $$ I tried to rewrite it this way: $$ y(x)=x\frac{dy}{dx}+f\left(\frac{dy}{dx}\right) $$,I'm trying solve the following differential equation: $$ x\left(\frac{dx}{dy}\right)^2+y\frac{dx}{dy}=x $$ I tried to rewrite it this way: $$ y(x)=x\frac{dy}{dx}+f\left(\frac{dy}{dx}\right) $$,,['ordinary-differential-equations']
24,Recurrence-differential equation [duplicate],Recurrence-differential equation [duplicate],,"This question already has answers here : A fierce differential-delay equation: df/dx = f(f(x)) (4 answers) Closed last year . In his book on differential equations, Arnold writes that $x'(t)=x(x(t))$ is not a differential equation. My question is: how can one solve it?","This question already has answers here : A fierce differential-delay equation: df/dx = f(f(x)) (4 answers) Closed last year . In his book on differential equations, Arnold writes that $x'(t)=x(x(t))$ is not a differential equation. My question is: how can one solve it?",,"['ordinary-differential-equations', 'recurrence-relations']"
25,Is this first-order nonlinear ordinary differential equation not explicitly or implicitly solvable?,Is this first-order nonlinear ordinary differential equation not explicitly or implicitly solvable?,,"Homework assignment suggests this: $$(- 3y-3e^x \sin (y)) dx + (-4 x - 6e^x\cos(y) + 4 y) dy = 0$$ To be solvable using the ""mixed partials"" method for exact differential equations.  However the test for 'exactness' disagrees with the graded response on the online assignment. With $M(x,y) = - 3y-3e^x \sin (y)$ and $N(x,y) = -4 x - 6e^x\cos(y) + 4 y$. $${\partial M\over \partial y}={\partial\over\partial y}(- 3y-3e^x \sin y)= -3-3e^x \cos y$$ and $${\partial N\over\partial x}={\partial\over\partial x}(-4 x - 6e^x\cos y  + 4 y)= -4 -6e^x\cos y$$ Is there some sort of fault with my partial differentiation, or is the programmed response on the homework in error? Tried finding an integrating factor $\mu(x,y)$ using  both $(M_y - N_x)\over N$ and $(N_x-M_y)\over M$ but end up being functions of both variables, despite my attempts to simplify them. Am I missing something?","Homework assignment suggests this: $$(- 3y-3e^x \sin (y)) dx + (-4 x - 6e^x\cos(y) + 4 y) dy = 0$$ To be solvable using the ""mixed partials"" method for exact differential equations.  However the test for 'exactness' disagrees with the graded response on the online assignment. With $M(x,y) = - 3y-3e^x \sin (y)$ and $N(x,y) = -4 x - 6e^x\cos(y) + 4 y$. $${\partial M\over \partial y}={\partial\over\partial y}(- 3y-3e^x \sin y)= -3-3e^x \cos y$$ and $${\partial N\over\partial x}={\partial\over\partial x}(-4 x - 6e^x\cos y  + 4 y)= -4 -6e^x\cos y$$ Is there some sort of fault with my partial differentiation, or is the programmed response on the homework in error? Tried finding an integrating factor $\mu(x,y)$ using  both $(M_y - N_x)\over N$ and $(N_x-M_y)\over M$ but end up being functions of both variables, despite my attempts to simplify them. Am I missing something?",,"['calculus', 'ordinary-differential-equations']"
26,Can a nice enough ODE always be extended to the complex plane?,Can a nice enough ODE always be extended to the complex plane?,,"Suppose I have a first-order ODE $y' = f(x, y)$, where $y: \mathbb{R} \to \mathbb{R}$, and $f \in \mathbb{R}[x, y]$. Consider $f^\mathbb{C} = i(f)$, where $i: \mathbb{R}[x, y] \to \mathbb{C}[x, y]$ is the usual embedding, and the equation $w' = f^\mathbb{C}(x, y)$, where $w: \mathbb{C} \to \mathbb{C}$ is meromorphic. Let $y$ be an analytic solution of $y' = f(x, y)$. Is its complex continuation $w$ necessarily a solution of $w' = f^\mathbb{C}(x, y)$? Sorry if it's a silly question, my complex analysis is very rusty :( My motivation is that I want to solve a Riccati equation $y' = y^2 + C^2$. If I could count $y(x) = iC$ as a solution, I would be able to solve the equation exactly.","Suppose I have a first-order ODE $y' = f(x, y)$, where $y: \mathbb{R} \to \mathbb{R}$, and $f \in \mathbb{R}[x, y]$. Consider $f^\mathbb{C} = i(f)$, where $i: \mathbb{R}[x, y] \to \mathbb{C}[x, y]$ is the usual embedding, and the equation $w' = f^\mathbb{C}(x, y)$, where $w: \mathbb{C} \to \mathbb{C}$ is meromorphic. Let $y$ be an analytic solution of $y' = f(x, y)$. Is its complex continuation $w$ necessarily a solution of $w' = f^\mathbb{C}(x, y)$? Sorry if it's a silly question, my complex analysis is very rusty :( My motivation is that I want to solve a Riccati equation $y' = y^2 + C^2$. If I could count $y(x) = iC$ as a solution, I would be able to solve the equation exactly.",,['ordinary-differential-equations']
27,"Let $\frac {dx}{dt}=Ax+By,\frac {dy}{dt}=x,$ where $B<-1<A<0.$",Let  where,"\frac {dx}{dt}=Ax+By,\frac {dy}{dt}=x, B<-1<A<0.","I came across the following problem that says: Consider the pair of first order ordinary differential equations $\frac {dx}{dt}=Ax+By,\frac {dy}{dt}=x,$ where $B<-1<A<0.$ Let $(x(t),y(t))$ be the solution of the above that satisfies  $(x(0),y(0))=(0,1).$ Then pick the correct statement: $1.(x(t),y(t))=(0,1) \forall t \in \mathbb R$ $2.x(t)$ is bounded on $\mathbb R$ $3.y(t)$ is bounded on $\mathbb R$ $4.x(t)$ is bounded on $[0,\infty )$. Utilizing the given equations,I see that $\frac {d^2x}{dt^2}-A\frac {dx}{dt}-Bx=0$ and $\frac {d^2y}{dt^2}-A\frac {dy}{dt}-By=0$.Do I have to solve it to reach a decision ? In that case ,it will be bit lengthy for a multiple choice question like it. Or are there any other better way to approach the problem? I am also not sure about the importance of $B<-1<A<0.$ Can someone point me in the right direction? Thanks in advance for your time.","I came across the following problem that says: Consider the pair of first order ordinary differential equations $\frac {dx}{dt}=Ax+By,\frac {dy}{dt}=x,$ where $B<-1<A<0.$ Let $(x(t),y(t))$ be the solution of the above that satisfies  $(x(0),y(0))=(0,1).$ Then pick the correct statement: $1.(x(t),y(t))=(0,1) \forall t \in \mathbb R$ $2.x(t)$ is bounded on $\mathbb R$ $3.y(t)$ is bounded on $\mathbb R$ $4.x(t)$ is bounded on $[0,\infty )$. Utilizing the given equations,I see that $\frac {d^2x}{dt^2}-A\frac {dx}{dt}-Bx=0$ and $\frac {d^2y}{dt^2}-A\frac {dy}{dt}-By=0$.Do I have to solve it to reach a decision ? In that case ,it will be bit lengthy for a multiple choice question like it. Or are there any other better way to approach the problem? I am also not sure about the importance of $B<-1<A<0.$ Can someone point me in the right direction? Thanks in advance for your time.",,[]
28,Find an autonomous differential equation with a given phase portrait,Find an autonomous differential equation with a given phase portrait,,"My DE course uses an online homework service to distribute and collect homework. One of the problems in this set is to furnish an autonomous DE consistent with the phase portrait below: I came up with $y' = (y-1)^2(y-3)^2(y-5)^2$, which the program rejected. Have I made a mistake, or has the program? Edit : Another problem was to find an autonomous DE consistent with the phase portrait For this, I came up with $(y-1)(y-3)^2(y-5)$. I expanded this into $y^4 - 12y^3 + 50y^2 - 84y + 45$ and got marked correct. When I expanded $(y-1)^2(y-3)^2(y-5)^2$ into $225-690 y +799 y^2 - 444 y^3+127$ $ y^4 -18 y^5 +y^6$ using Walpha, I got marked wrong. Is there some error I'm making? (And do problems like this actually arise in ""real life""?!)","My DE course uses an online homework service to distribute and collect homework. One of the problems in this set is to furnish an autonomous DE consistent with the phase portrait below: I came up with $y' = (y-1)^2(y-3)^2(y-5)^2$, which the program rejected. Have I made a mistake, or has the program? Edit : Another problem was to find an autonomous DE consistent with the phase portrait For this, I came up with $(y-1)(y-3)^2(y-5)$. I expanded this into $y^4 - 12y^3 + 50y^2 - 84y + 45$ and got marked correct. When I expanded $(y-1)^2(y-3)^2(y-5)^2$ into $225-690 y +799 y^2 - 444 y^3+127$ $ y^4 -18 y^5 +y^6$ using Walpha, I got marked wrong. Is there some error I'm making? (And do problems like this actually arise in ""real life""?!)",,['ordinary-differential-equations']
29,how to understand the differential operator acting on functions that are not scalar,how to understand the differential operator acting on functions that are not scalar,,"Quite often these days I find myself in a situation where I'd like to understand differential operators. One bit that is particularly subtle to me at the moment is how a differential operator is to be understood when it is supposed to act on vector - valued, or matrix - valued functions. For example, suppose we are given a general linear partial differential operator \begin{equation} D = \sum_{|\alpha| \leq m} a_\alpha(x)\partial^\alpha \end{equation} where $\alpha = (\alpha_1, \dots \alpha_n)$ denotes a multi-index, $m$ is some positive integer, $x \in \mathbb{R}^n$, $\partial^\alpha := \partial^{|\alpha|}/(\partial^{\alpha_1} x_1 \dots \partial^{\alpha_n} x_n)$ denotes a mixed partial derivative, and the functions $a_\alpha$ are smooth. In various contexts they might be vector- or matrix valued. This is already where I am having difficulties, because usually it is assumed the reader knows how to apply these operators, and from this I guess one could deduce what kind of functions these $a_\alpha$ are .. How is such an operator supposed to act on vector - valued or matrix valued functions $f : \mathbb{R}^m \to \mathbb{R}^k$ or $F: GL(n,\mathbb{R}) \to GL(k,\mathbb{R})$ ? Unfortunately my Calculus classes didn't cover much beyond the one - variable setting so I am shaky on these grounds. I am aware there are differential operators for non - scalar functions, such as div, curl, grad. All of these act in a specific way. But the operator above is none of these so I am a bit lost .. Sorry for being so confused about this - in case the question is unclear I am happy to try my best and improve the post, many thanks !","Quite often these days I find myself in a situation where I'd like to understand differential operators. One bit that is particularly subtle to me at the moment is how a differential operator is to be understood when it is supposed to act on vector - valued, or matrix - valued functions. For example, suppose we are given a general linear partial differential operator \begin{equation} D = \sum_{|\alpha| \leq m} a_\alpha(x)\partial^\alpha \end{equation} where $\alpha = (\alpha_1, \dots \alpha_n)$ denotes a multi-index, $m$ is some positive integer, $x \in \mathbb{R}^n$, $\partial^\alpha := \partial^{|\alpha|}/(\partial^{\alpha_1} x_1 \dots \partial^{\alpha_n} x_n)$ denotes a mixed partial derivative, and the functions $a_\alpha$ are smooth. In various contexts they might be vector- or matrix valued. This is already where I am having difficulties, because usually it is assumed the reader knows how to apply these operators, and from this I guess one could deduce what kind of functions these $a_\alpha$ are .. How is such an operator supposed to act on vector - valued or matrix valued functions $f : \mathbb{R}^m \to \mathbb{R}^k$ or $F: GL(n,\mathbb{R}) \to GL(k,\mathbb{R})$ ? Unfortunately my Calculus classes didn't cover much beyond the one - variable setting so I am shaky on these grounds. I am aware there are differential operators for non - scalar functions, such as div, curl, grad. All of these act in a specific way. But the operator above is none of these so I am a bit lost .. Sorry for being so confused about this - in case the question is unclear I am happy to try my best and improve the post, many thanks !",,"['ordinary-differential-equations', 'multivariable-calculus']"
30,Mathematics From Futurama,Mathematics From Futurama,,"We at D.O.O.P are trying to mathematically model a rocket ship fueled by your employee Leela's pet Nibbler's pooped Black matter. Obviously this rocket ship is fueled by black matter which along with the ship's combustion chamber has some special properties. Black matter Properties We have discovered that there are two kinds of black matter the one naturally made (by Nibbler's poop) and the other which is  synthetically made in our special rocket ship combustion engine. The Synthetically made Black matter exponentially decays at a rate of $r$, until all its mass eventually becomes nothing and is discarded. Rocket Ship Properties When any Black matter is used as fuel in the combustion chamber. It produces black matter (synthetic kind) according to following equation. $$\dfrac {dP_a} {dt}\leq \dfrac {dP_c} {dt}=\left( 1-\dfrac {\alpha } {100}\right)B$$ Here $B$ represents the amount of Black matter currently in the combustion chamber. $\alpha$ is a percentage, a controlling mechanism in the ship to control production. $P_c$ is an upper bound of the new Black matter production (capacity), but we discover there is an inefficiency in the system such that the actual rate of Black matter production is in fact $P_{a}$. Since we want to travel as fast as we can, as soon as any new Black matter $P_{a}$ is produced we add that to $B$ and we are able to do all of this in infinitesimally small amount of time(continuously). We invite you to scientifically examine and model the processes of this rocket ship along with say $B_0$ amount of initial natural black matter. How can we model or represent this system with the least amount of equations while capturing the essence of the whole problem ? From the desk of Zapp Brannigan ""And like all my plans, it's so simple an idiot could have devised it!"" Edit: Solution attempt Assuming $B_0$ to be the initial amount of black matter available. We start undertaking combustion with this initial amount $B_0$ we produce more black matter at the rate of $\dfrac {dP_a} {dt}\leq \dfrac {dP_c} {dt}=\left( 1-\dfrac {\alpha } {100}\right)B_{0}$. As $dt$ time period passes by we take the new $P_a$ amount produced and add it to $B_{0}$. We also observe that this newly created synthetic black matter $P_a$ is exponentially decaying. I am having trouble figuring out how to put these relations together so both of these processes can be carried out simultaneously.I'd be happy with if you wish, only consider the case when $P_a$ and $P_c$ are the same.","We at D.O.O.P are trying to mathematically model a rocket ship fueled by your employee Leela's pet Nibbler's pooped Black matter. Obviously this rocket ship is fueled by black matter which along with the ship's combustion chamber has some special properties. Black matter Properties We have discovered that there are two kinds of black matter the one naturally made (by Nibbler's poop) and the other which is  synthetically made in our special rocket ship combustion engine. The Synthetically made Black matter exponentially decays at a rate of $r$, until all its mass eventually becomes nothing and is discarded. Rocket Ship Properties When any Black matter is used as fuel in the combustion chamber. It produces black matter (synthetic kind) according to following equation. $$\dfrac {dP_a} {dt}\leq \dfrac {dP_c} {dt}=\left( 1-\dfrac {\alpha } {100}\right)B$$ Here $B$ represents the amount of Black matter currently in the combustion chamber. $\alpha$ is a percentage, a controlling mechanism in the ship to control production. $P_c$ is an upper bound of the new Black matter production (capacity), but we discover there is an inefficiency in the system such that the actual rate of Black matter production is in fact $P_{a}$. Since we want to travel as fast as we can, as soon as any new Black matter $P_{a}$ is produced we add that to $B$ and we are able to do all of this in infinitesimally small amount of time(continuously). We invite you to scientifically examine and model the processes of this rocket ship along with say $B_0$ amount of initial natural black matter. How can we model or represent this system with the least amount of equations while capturing the essence of the whole problem ? From the desk of Zapp Brannigan ""And like all my plans, it's so simple an idiot could have devised it!"" Edit: Solution attempt Assuming $B_0$ to be the initial amount of black matter available. We start undertaking combustion with this initial amount $B_0$ we produce more black matter at the rate of $\dfrac {dP_a} {dt}\leq \dfrac {dP_c} {dt}=\left( 1-\dfrac {\alpha } {100}\right)B_{0}$. As $dt$ time period passes by we take the new $P_a$ amount produced and add it to $B_{0}$. We also observe that this newly created synthetic black matter $P_a$ is exponentially decaying. I am having trouble figuring out how to put these relations together so both of these processes can be carried out simultaneously.I'd be happy with if you wish, only consider the case when $P_a$ and $P_c$ are the same.",,"['ordinary-differential-equations', 'puzzle', 'recreational-mathematics', 'mathematical-modeling']"
31,Please help me spot the mistake,Please help me spot the mistake,,"This is related to this question . If anyone could help be spot the mistake it would really make my day! I have redone the question. And found that the characteristics are indeed $x\pm 2\sqrt{-y}$ for the region $y<0$. And I know  that $F(x,y)=f_1(x+2\sqrt{-y})+f_2(x-2\sqrt{-y})$ for arbitrary $f_1,f_2$ is indeed the solution to the equation $F_{xx}+yF_{yy}+{1\over 2}F_y=0$. (Verifiable by direct substitution into the equation.) Now my problem is that I can't get the correct canonical form $F_{\alpha\beta}=0$ where $\alpha,\beta=x\pm 2\sqrt{-y}$ respectively. Here is what I have done: $M_\pm={1\over c}(-b\pm\sqrt{b^2-ac})=\mp (-y)^{-1/2}$ So the characteristics are given by $\alpha=x+2\sqrt{-y}$ and $\beta=x-2\sqrt{-y}$. We have $a(x,y)F_{xx}+2b(x,y)F_{xy}+c(x,y)F_{yy}+\cdots=H(x,y)$ and ${a\over c}\partial_{xx}+{2b\over c}\partial_{xy}+\partial_{yy}=(\partial_y-M_+\partial_x)(\partial_y-M_-\partial_x)+({\partial M_-\over \partial y}-M_+{\partial M_-\over \partial x})\partial_x$ where $(\partial_y-M_+\partial_x)(\partial_y-M_-\partial_x)=-\alpha_x\beta_x(M_+-M_-)^2\partial_{\alpha\beta}-\beta_x(M_--M_+)(\partial_\beta [\alpha_x(M_+-M_-)])\partial_\alpha$ For this particular question I calculated $(\partial_y-M_+\partial_x)(\partial_y-M_-\partial_x)=-4(-y)^{-1}\partial_{\alpha\beta}-2(-y)^{-3/2}\partial_\alpha$ $\implies(\partial_y-M_+\partial_x)(\partial_y-M_-\partial_x)+({\partial M_-\over \partial y}-M_+{\partial M_-\over \partial x})\partial_x=-4(-y)^{-1}\partial_{\alpha\beta}-{3\over 2}(-y)^{-3/2}\partial_\alpha+{1\over 2}(-y)^{-3/2}\partial_\beta$ $\implies 4F_{\alpha\beta}+{3\over 2}(-y)^{-1/2}F_\alpha-{1\over 2}(-y)^{-1/2}F_\beta-{1\over 2}(-y)^{-1/2}F_\alpha+{1\over 2}(-y)^{-1/2}F_\beta=0$ See my problem? I need the $F_\alpha, F_\beta$ terms to cancel. OK, the $F_\beta$ ones cancel, but not the $F_\alpha$ ones. Thanks in advance!","This is related to this question . If anyone could help be spot the mistake it would really make my day! I have redone the question. And found that the characteristics are indeed $x\pm 2\sqrt{-y}$ for the region $y<0$. And I know  that $F(x,y)=f_1(x+2\sqrt{-y})+f_2(x-2\sqrt{-y})$ for arbitrary $f_1,f_2$ is indeed the solution to the equation $F_{xx}+yF_{yy}+{1\over 2}F_y=0$. (Verifiable by direct substitution into the equation.) Now my problem is that I can't get the correct canonical form $F_{\alpha\beta}=0$ where $\alpha,\beta=x\pm 2\sqrt{-y}$ respectively. Here is what I have done: $M_\pm={1\over c}(-b\pm\sqrt{b^2-ac})=\mp (-y)^{-1/2}$ So the characteristics are given by $\alpha=x+2\sqrt{-y}$ and $\beta=x-2\sqrt{-y}$. We have $a(x,y)F_{xx}+2b(x,y)F_{xy}+c(x,y)F_{yy}+\cdots=H(x,y)$ and ${a\over c}\partial_{xx}+{2b\over c}\partial_{xy}+\partial_{yy}=(\partial_y-M_+\partial_x)(\partial_y-M_-\partial_x)+({\partial M_-\over \partial y}-M_+{\partial M_-\over \partial x})\partial_x$ where $(\partial_y-M_+\partial_x)(\partial_y-M_-\partial_x)=-\alpha_x\beta_x(M_+-M_-)^2\partial_{\alpha\beta}-\beta_x(M_--M_+)(\partial_\beta [\alpha_x(M_+-M_-)])\partial_\alpha$ For this particular question I calculated $(\partial_y-M_+\partial_x)(\partial_y-M_-\partial_x)=-4(-y)^{-1}\partial_{\alpha\beta}-2(-y)^{-3/2}\partial_\alpha$ $\implies(\partial_y-M_+\partial_x)(\partial_y-M_-\partial_x)+({\partial M_-\over \partial y}-M_+{\partial M_-\over \partial x})\partial_x=-4(-y)^{-1}\partial_{\alpha\beta}-{3\over 2}(-y)^{-3/2}\partial_\alpha+{1\over 2}(-y)^{-3/2}\partial_\beta$ $\implies 4F_{\alpha\beta}+{3\over 2}(-y)^{-1/2}F_\alpha-{1\over 2}(-y)^{-1/2}F_\beta-{1\over 2}(-y)^{-1/2}F_\alpha+{1\over 2}(-y)^{-1/2}F_\beta=0$ See my problem? I need the $F_\alpha, F_\beta$ terms to cancel. OK, the $F_\beta$ ones cancel, but not the $F_\alpha$ ones. Thanks in advance!",,['ordinary-differential-equations']
32,How to do a change of variable in an ODE,How to do a change of variable in an ODE,,"I asked something related to this and this was a secondary question, after some answers. So I prefer to ask this in a new post because this question is extensive. Sorry for ask this simple things. )= If I have an ODE, let's say of second order, and has the form  $$ \frac{d^2 y}{dx^2} = f \Big( {\frac{dy}{dx},y,x} \Big), $$ where clearly $y$ is a function that depends on $x$. There are two important kinds of change of variable. The second complains me, but I put i) also in case someone notes an error. In fact if someone know if this can be writted in a optional hide, but I don't know how to do it. i) If I use some change of the form $s=g(x)$, then I have to compute $\frac{d^2 y}{dx^2}, \frac{dy}{dx}$ with the new variable. I know how to do it. $$ \frac{dy}{dx} = \frac{dy}{ds}\frac{ds}{dx}. $$ Then for the second, we have  $$ \frac{d^2 y}{dx^2}  = \frac{d}{dx} \Big( \frac{dy}{dx} \Big)  = \frac{d}{dx} \left( \frac{dy}{ds} \frac{ds}{dx} \right)  = \frac{ds}{dx} \cdot \frac{d}{dx} \Big( \frac{dy}{ds} \Big) +  \frac{dy}{ds} \cdot \frac{d}{dx} \Big( \frac{ds}{dx} \Big) .$$ For the first term we have  $$ \frac{ds}{dx} \cdot \left( \frac{d}{dx} \Big( \frac{dy}{ds} \Big) \right)  = \frac{ds}{dx} \cdot \left( \frac{d}{ds} \Big( \frac{dy}{ds} \Big) \cdot \frac{ds}{dx} \right)  = \frac{d^2 y}{ds^2} \Big( \frac{ds}{dx} \Big)^2 , $$ and the second is obviously equal to  $$ \frac{dy}{ds} \cdot \frac{d^2 s}{dx^2}. $$ So we have that  $$ \frac{d^2 y}{dx^2}  = \frac{d^2 y}{ds^2} \Big( \frac{ds}{dx} \Big)^2   + \frac{dy}{ds} \cdot \frac{d^2 s}{dx^2} $$ ii) Here is my problem, a change of variable of the form $s= g(y)$. Here the technique is different because, in the other I start with $\frac{dy}{dx}$ and changed with the chain rule, but here is different because in a way it clearing in an indirect way. I start differentiating the equality with respect to $x$, I have  $$ \begin{align*} s &= g(y) \\  \frac{ds}{dx} &= \frac{dg}{dy} \cdot \frac{dy}{dx}  \end{align*}$$ But for the second, what can I do?","I asked something related to this and this was a secondary question, after some answers. So I prefer to ask this in a new post because this question is extensive. Sorry for ask this simple things. )= If I have an ODE, let's say of second order, and has the form  $$ \frac{d^2 y}{dx^2} = f \Big( {\frac{dy}{dx},y,x} \Big), $$ where clearly $y$ is a function that depends on $x$. There are two important kinds of change of variable. The second complains me, but I put i) also in case someone notes an error. In fact if someone know if this can be writted in a optional hide, but I don't know how to do it. i) If I use some change of the form $s=g(x)$, then I have to compute $\frac{d^2 y}{dx^2}, \frac{dy}{dx}$ with the new variable. I know how to do it. $$ \frac{dy}{dx} = \frac{dy}{ds}\frac{ds}{dx}. $$ Then for the second, we have  $$ \frac{d^2 y}{dx^2}  = \frac{d}{dx} \Big( \frac{dy}{dx} \Big)  = \frac{d}{dx} \left( \frac{dy}{ds} \frac{ds}{dx} \right)  = \frac{ds}{dx} \cdot \frac{d}{dx} \Big( \frac{dy}{ds} \Big) +  \frac{dy}{ds} \cdot \frac{d}{dx} \Big( \frac{ds}{dx} \Big) .$$ For the first term we have  $$ \frac{ds}{dx} \cdot \left( \frac{d}{dx} \Big( \frac{dy}{ds} \Big) \right)  = \frac{ds}{dx} \cdot \left( \frac{d}{ds} \Big( \frac{dy}{ds} \Big) \cdot \frac{ds}{dx} \right)  = \frac{d^2 y}{ds^2} \Big( \frac{ds}{dx} \Big)^2 , $$ and the second is obviously equal to  $$ \frac{dy}{ds} \cdot \frac{d^2 s}{dx^2}. $$ So we have that  $$ \frac{d^2 y}{dx^2}  = \frac{d^2 y}{ds^2} \Big( \frac{ds}{dx} \Big)^2   + \frac{dy}{ds} \cdot \frac{d^2 s}{dx^2} $$ ii) Here is my problem, a change of variable of the form $s= g(y)$. Here the technique is different because, in the other I start with $\frac{dy}{dx}$ and changed with the chain rule, but here is different because in a way it clearing in an indirect way. I start differentiating the equality with respect to $x$, I have  $$ \begin{align*} s &= g(y) \\  \frac{ds}{dx} &= \frac{dg}{dy} \cdot \frac{dy}{dx}  \end{align*}$$ But for the second, what can I do?",,['ordinary-differential-equations']
33,Differential Equation,Differential Equation,,"Suppose Ms. Lee is buying a new house and must borrow 150,000. She wants a  30-year mortgage and she has two choices. She can either borrow money at 7% per  year with no points, or she can borrow the money at 6.5% per year with a charge of  3 points. (A ""point"" is a fee of 1% of the loan amount that the borrower pays the  lender at the beginning of the loan. For example, a mortgage with 3 points requires  Ms. Lee to pay 4,500 extra to get the loan.) As an approximation, we assume that  interest is compounded and payments are made continuously. Let $$M(t) = \text{amount owed at time } t\ \left(\text{measured in years}\right)$$ $$r= \text{annual interest rate, and}$$ $$p= \text{annual payment}$$ Then the model for the amount owed is $$ \frac{dM}{dt}=rM-p$$ Q.How much does Ms Lee has to pay in each case? I have tried solving the DE, and i get $$ M(t)=C_1e^{rt} + \frac{p}{r}$$ Now what to do?","Suppose Ms. Lee is buying a new house and must borrow 150,000. She wants a  30-year mortgage and she has two choices. She can either borrow money at 7% per  year with no points, or she can borrow the money at 6.5% per year with a charge of  3 points. (A ""point"" is a fee of 1% of the loan amount that the borrower pays the  lender at the beginning of the loan. For example, a mortgage with 3 points requires  Ms. Lee to pay 4,500 extra to get the loan.) As an approximation, we assume that  interest is compounded and payments are made continuously. Let $$M(t) = \text{amount owed at time } t\ \left(\text{measured in years}\right)$$ $$r= \text{annual interest rate, and}$$ $$p= \text{annual payment}$$ Then the model for the amount owed is $$ \frac{dM}{dt}=rM-p$$ Q.How much does Ms Lee has to pay in each case? I have tried solving the DE, and i get $$ M(t)=C_1e^{rt} + \frac{p}{r}$$ Now what to do?",,['ordinary-differential-equations']
34,When could these be equal? Differentials,When could these be equal? Differentials,,"given $ax^2y''+bxy'+cy=0$ a,b, and c are real. x is positive. I want to show that  $$a \frac{d^2y}{dv^2} +(b-a)\frac{dy}{dv} +cy =0.$$ This is a problem in a elementary text, I have found a more rigours method online, but I'm trying to make sense of the last step in my own less rigours idea. Using the coefficients $a$, $b$, and $c$, I observe that if this is true then $$xy'=\frac{dy}{dv}\quad\text{and}\quad x^2y''= \frac{d^2y}{dv^2} - \frac{dy}{dv},$$ so if I show the two statements above are true I will have justified the subsitution. Let $\ln x = v$ $$\frac{1}{x} = \frac{dv}{dx}$$ $\ln$ is one to one so it has an inverse and  $$\begin{align*} x &= \frac{dx}{dv}\\ xy'&= \frac{dx}{dv} \frac{dy}{dx}\&&\text{by the chain rule}\\ xy'&= \frac{dy}{dv}. \end{align*}$$ Great one down one to go. Continuing with the above... $$x \frac{dy}{dx} = \frac{dy}{dv}.$$ I  take the derivative with respect to $x$ of both sides, using the product rule on the right. $$x \frac{d^2y}{dx^2} + \frac{dy}{dx} = \frac{d}{dx}  \left( \frac{dy}{dv} \right)$$ I dont feel so good about the right side, but I  keep going anyway. $$x \frac{d^2y}{dx^2} = \frac{d}{dx}  \left( \frac{dy}{dv} \right) - \frac{dy}{dx}$$ since $x$ is $e^v$, $x$ is its own derivative with respect to $v$. I multiply through by $x$: $$x^2 \frac{d^2y}{dx^2} = x \frac{d}{dx}  \left( \frac{dy}{dv} \right) - \frac{dy}{dx} \frac{dv}{dx}$$ chain rule on the far right. $$x^2 \frac{d^2y}{dx^2} = x \frac{d}{dx}  \left( \frac{dy}{dv} \right) - \frac{dy}{dv}$$ I'm so close but I'm stuck! all I want to say is: $$x^2y''= \frac{d^2y}{dv^2} - \frac{dy}{dv}$$ and it is very sugestive to write: $$x^2 \frac{d^2y}{dx^2} = \frac{dx}{dv} \frac{d}{dx}  \left( \frac{dy}{dv} \right) - \frac{dy}{dv}$$ but what could it mean to cross out the $dx/dx$? that is in my way? as I  was told before here such operations are ""dubious"" though I'm still trying to grasp why. But I  know this is true so it must be the case that for these functions $$\frac{dx}{dv} \frac{d}{dx}  \left( \frac{dy}{dv} \right) = \frac{d^2y}{dv^2}$$ perhaps I  can say that the kind of functions that work in the above are just the kind I'm working with and then I  would be done?","given $ax^2y''+bxy'+cy=0$ a,b, and c are real. x is positive. I want to show that  $$a \frac{d^2y}{dv^2} +(b-a)\frac{dy}{dv} +cy =0.$$ This is a problem in a elementary text, I have found a more rigours method online, but I'm trying to make sense of the last step in my own less rigours idea. Using the coefficients $a$, $b$, and $c$, I observe that if this is true then $$xy'=\frac{dy}{dv}\quad\text{and}\quad x^2y''= \frac{d^2y}{dv^2} - \frac{dy}{dv},$$ so if I show the two statements above are true I will have justified the subsitution. Let $\ln x = v$ $$\frac{1}{x} = \frac{dv}{dx}$$ $\ln$ is one to one so it has an inverse and  $$\begin{align*} x &= \frac{dx}{dv}\\ xy'&= \frac{dx}{dv} \frac{dy}{dx}\&&\text{by the chain rule}\\ xy'&= \frac{dy}{dv}. \end{align*}$$ Great one down one to go. Continuing with the above... $$x \frac{dy}{dx} = \frac{dy}{dv}.$$ I  take the derivative with respect to $x$ of both sides, using the product rule on the right. $$x \frac{d^2y}{dx^2} + \frac{dy}{dx} = \frac{d}{dx}  \left( \frac{dy}{dv} \right)$$ I dont feel so good about the right side, but I  keep going anyway. $$x \frac{d^2y}{dx^2} = \frac{d}{dx}  \left( \frac{dy}{dv} \right) - \frac{dy}{dx}$$ since $x$ is $e^v$, $x$ is its own derivative with respect to $v$. I multiply through by $x$: $$x^2 \frac{d^2y}{dx^2} = x \frac{d}{dx}  \left( \frac{dy}{dv} \right) - \frac{dy}{dx} \frac{dv}{dx}$$ chain rule on the far right. $$x^2 \frac{d^2y}{dx^2} = x \frac{d}{dx}  \left( \frac{dy}{dv} \right) - \frac{dy}{dv}$$ I'm so close but I'm stuck! all I want to say is: $$x^2y''= \frac{d^2y}{dv^2} - \frac{dy}{dv}$$ and it is very sugestive to write: $$x^2 \frac{d^2y}{dx^2} = \frac{dx}{dv} \frac{d}{dx}  \left( \frac{dy}{dv} \right) - \frac{dy}{dv}$$ but what could it mean to cross out the $dx/dx$? that is in my way? as I  was told before here such operations are ""dubious"" though I'm still trying to grasp why. But I  know this is true so it must be the case that for these functions $$\frac{dx}{dv} \frac{d}{dx}  \left( \frac{dy}{dv} \right) = \frac{d^2y}{dv^2}$$ perhaps I  can say that the kind of functions that work in the above are just the kind I'm working with and then I  would be done?",,['ordinary-differential-equations']
35,Nonlinear ODE $a^2u''+bu'+b^*(u')^2/u+cu=-1$,Nonlinear ODE,a^2u''+bu'+b^*(u')^2/u+cu=-1,"Consider the ODE $$[a(x)]^2u''+b(x)u'+b^*(x)(u')^2/u+c(x)u=-1$$ on $\mathbb{R}$ . I am trying to make it looks better by writing it as a variational problem ( $u$ is some minimizer of some energy functionals). Do anyone have ideas about that? Or any good change of variable to reduce the nonlinearity of the equation? My only thought is to write $u=e^v$ and $$[a(x)]^2v''+b(x)v'+\{b^*(x)+[a(x)]^2\}(v')^2+c(x)=-e^{-v},$$ but I do not know if it is useful.",Consider the ODE on . I am trying to make it looks better by writing it as a variational problem ( is some minimizer of some energy functionals). Do anyone have ideas about that? Or any good change of variable to reduce the nonlinearity of the equation? My only thought is to write and but I do not know if it is useful.,"[a(x)]^2u''+b(x)u'+b^*(x)(u')^2/u+c(x)u=-1 \mathbb{R} u u=e^v [a(x)]^2v''+b(x)v'+\{b^*(x)+[a(x)]^2\}(v')^2+c(x)=-e^{-v},","['ordinary-differential-equations', 'calculus-of-variations']"
36,Approximating the symplectic flow of Hamiltonian systems with $H = \frac{1}{2}p^TM^{-1}p + V(q)$,Approximating the symplectic flow of Hamiltonian systems with,H = \frac{1}{2}p^TM^{-1}p + V(q),"Consider a symplectic map $\phi_t(p,q)\in \mathbb{R}^{2n}$ , that solves or approximates the Hamiltonian system $$\dot{p} = - \nabla V(q),\quad\dot{q} = M^{-1}p,$$ for Hamiltonian $H = \frac{1}{2}p^TM^{-1}p + V(q)$ . That is, the Hamiltonian is separable, quadratic in $p$ , $V(q)$ is an arbitrary differentiable function and $M$ is symmetric and constant. What can we say about $\phi_t$ given this information and how can we leverage the structure of this Hamiltonian to find a better approximation to $\phi_h$ ? My thoughts so far: For example, given this particular form of the Hamiltonian, we can see that it is an even function of $p$ and therefore has time-reversal symmetry , so it's flow must also be time-reversible (i.e., invariant under the time-reversal operator $R:(p,q,t)\rightarrow(-p,q, -t)$ ). So the symplectic map must share this reversing symmetry $R\circ\phi_h = \phi_h^{-1}\circ R$ . To leverage the separability property of the Hamiltonian, we can use the Störmer-Verlet map to construct an explicit symplectic mapping (or any symplectic numerical method), given $V(q)$ . Such numerical methods can also have time-reversal symmetry, but are only accurate for short times $t<<1$ . To leverage the quadratic in $p$ propery , by defining $\phi_t(p,q)=(P, Q)$ , where $P$ and $Q$ can depend on $(p, q, t)$ , then using the definition of a symplectic transformation we require $$P_p^TQ_q - P_q^TQ_p = I,$$ where the subscripts denote a partial derivative. As $Q$ must satisfy $\dot{Q}=M^{-1}P$ , this becomes $$\dot{Q}_p^TQ_q - \dot{Q}_q^TQ_p = M^{-1}.$$ . (preserves the Poisson-bracket) we can then use the chain rule on $\dot{Q}$ to get a complex expression. But I think we need a good ansatz about the form of $Q$ that can solve/approximate the above for $Q$ . I would like to know more maps that can approximate $\phi_h$ by leveraging/preserving the structure of the Hamiltonian, and/or by combining the above ideas. Any references on this would be appreciated too :)","Consider a symplectic map , that solves or approximates the Hamiltonian system for Hamiltonian . That is, the Hamiltonian is separable, quadratic in , is an arbitrary differentiable function and is symmetric and constant. What can we say about given this information and how can we leverage the structure of this Hamiltonian to find a better approximation to ? My thoughts so far: For example, given this particular form of the Hamiltonian, we can see that it is an even function of and therefore has time-reversal symmetry , so it's flow must also be time-reversible (i.e., invariant under the time-reversal operator ). So the symplectic map must share this reversing symmetry . To leverage the separability property of the Hamiltonian, we can use the Störmer-Verlet map to construct an explicit symplectic mapping (or any symplectic numerical method), given . Such numerical methods can also have time-reversal symmetry, but are only accurate for short times . To leverage the quadratic in propery , by defining , where and can depend on , then using the definition of a symplectic transformation we require where the subscripts denote a partial derivative. As must satisfy , this becomes . (preserves the Poisson-bracket) we can then use the chain rule on to get a complex expression. But I think we need a good ansatz about the form of that can solve/approximate the above for . I would like to know more maps that can approximate by leveraging/preserving the structure of the Hamiltonian, and/or by combining the above ideas. Any references on this would be appreciated too :)","\phi_t(p,q)\in \mathbb{R}^{2n} \dot{p} = - \nabla V(q),\quad\dot{q} = M^{-1}p, H = \frac{1}{2}p^TM^{-1}p + V(q) p V(q) M \phi_t \phi_h p R:(p,q,t)\rightarrow(-p,q, -t) R\circ\phi_h = \phi_h^{-1}\circ R V(q) t<<1 p \phi_t(p,q)=(P, Q) P Q (p, q, t) P_p^TQ_q - P_q^TQ_p = I, Q \dot{Q}=M^{-1}P \dot{Q}_p^TQ_q - \dot{Q}_q^TQ_p = M^{-1}. \dot{Q} Q Q \phi_h","['ordinary-differential-equations', 'differential-geometry', 'dynamical-systems', 'symplectic-geometry']"
37,Existence and uniqueness of the solution of a control system,Existence and uniqueness of the solution of a control system,,"Let $T>0$ , $(U,d)$ be a metric space and $\mathcal{V}:=\{u:[0,T]\to U\,|\, u\text{ is measurable}\}$ . Consider the control system $$\begin{cases}\dot{x}(t)=f(t,x(t),u(t)),\quad \text{a.e. }t\in[0,T],\\x(0)=x_0,\end{cases}$$ where $f:[0,T]\times\mathbb{R}^n\times U\to \mathbb{R}^n$ is measurable. According to [1, P. 102], for any $u\in\mathcal{V}$ , the above control system has a unique solution $x:[0,T]\to\mathbb{R}^n$ if the following conditions hold: $U$ is a separable metric space. There exist a constant $L>0$ and a modulus of continuity $\omega: [0,\infty) \to [0,\infty)$ such that 2.1) $|f(t,x,u) - f(t,\hat{x},\hat{u})|\leq L|x-\hat{x}|+\omega(d(u,\hat{u})),\quad \forall t\in[0,T],x,\hat{x}\in \mathbb{R}^n,u,\hat{u}\in U,$ 2.2) $|f(t,0,u)|\leq L,\quad \forall (t,u)\in[0,T]\times U.$ My question is about the proof of this statement ([1] does not provide the proof). Can one extend the standard Picard–Lindelöf theorem to prove it? specially, since condition 2.1 is basically a Lipschitz condition for $f$ . I cannot also see what role separability of $U$ and the condition 2.2 play in the existence and uniqueness of the solution. Any help or hint with the proof is appreciated. [1] Yong, Jiongmin; Zhou, Xun Yu , Stochastic controls. Hamiltonian systems and HJB equations, Applications of Mathematics. 43. New York, NY: Springer. xx, 438 p. (1999). ZBL0943.93002 .","Let , be a metric space and . Consider the control system where is measurable. According to [1, P. 102], for any , the above control system has a unique solution if the following conditions hold: is a separable metric space. There exist a constant and a modulus of continuity such that 2.1) 2.2) My question is about the proof of this statement ([1] does not provide the proof). Can one extend the standard Picard–Lindelöf theorem to prove it? specially, since condition 2.1 is basically a Lipschitz condition for . I cannot also see what role separability of and the condition 2.2 play in the existence and uniqueness of the solution. Any help or hint with the proof is appreciated. [1] Yong, Jiongmin; Zhou, Xun Yu , Stochastic controls. Hamiltonian systems and HJB equations, Applications of Mathematics. 43. New York, NY: Springer. xx, 438 p. (1999). ZBL0943.93002 .","T>0 (U,d) \mathcal{V}:=\{u:[0,T]\to U\,|\, u\text{ is measurable}\} \begin{cases}\dot{x}(t)=f(t,x(t),u(t)),\quad \text{a.e. }t\in[0,T],\\x(0)=x_0,\end{cases} f:[0,T]\times\mathbb{R}^n\times U\to \mathbb{R}^n u\in\mathcal{V} x:[0,T]\to\mathbb{R}^n U L>0 \omega: [0,\infty) \to [0,\infty) |f(t,x,u) - f(t,\hat{x},\hat{u})|\leq L|x-\hat{x}|+\omega(d(u,\hat{u})),\quad \forall t\in[0,T],x,\hat{x}\in \mathbb{R}^n,u,\hat{u}\in U, |f(t,0,u)|\leq L,\quad \forall (t,u)\in[0,T]\times U. f U","['ordinary-differential-equations', 'dynamical-systems', 'control-theory', 'lipschitz-functions']"
38,"How to find all solutions of the ODE $x'=3x^{\frac{2}{3}}, x(0)=0$",How to find all solutions of the ODE,"x'=3x^{\frac{2}{3}}, x(0)=0","Problem: Find all the solutions of the IVP $$x'=3x^{\frac{2}{3}}, x(0)=0$$ for $t\geq 0$. Here $3x^{\frac{2}{3}}$ is not $C^1$, so the existence and uniqueness theorem does not apply here. My guess the solutions is $$x=\left\{\begin{matrix} 0 & \text{if }0\leq t< t_{0} \\   (t-t_{0})^3 &  \text{ if }t\geq t_{0}   \end{matrix}\right.$$ $t_{0}\in\mathbb{R^+}$ or $t_{0}\rightarrow+\infty$. But my professor told me that there are a lot more! My main question is: How can I find all the solutions, and then prove that they are all the solutions, rigorously?","Problem: Find all the solutions of the IVP $$x'=3x^{\frac{2}{3}}, x(0)=0$$ for $t\geq 0$. Here $3x^{\frac{2}{3}}$ is not $C^1$, so the existence and uniqueness theorem does not apply here. My guess the solutions is $$x=\left\{\begin{matrix} 0 & \text{if }0\leq t< t_{0} \\   (t-t_{0})^3 &  \text{ if }t\geq t_{0}   \end{matrix}\right.$$ $t_{0}\in\mathbb{R^+}$ or $t_{0}\rightarrow+\infty$. But my professor told me that there are a lot more! My main question is: How can I find all the solutions, and then prove that they are all the solutions, rigorously?",,"['real-analysis', 'ordinary-differential-equations', 'analysis']"
39,stability Sturm-Liouville equation,stability Sturm-Liouville equation,,"Consider the solutions $u_1,u_2$ of the Sturm Liouville equations $$ \begin{array}{ll} (p_1(x)u_1^\prime(x))^\prime+u_1(x)=f(x) && x\in (a,b)\\ u_1(x)=g(x) && x\in \{a,b\} \\ \end{array}$$ and $$\begin{array}{ll} (p_2(x)u_2^\prime(x))^\prime+u_2(x)=f(x) && x\in (a,b)\\ u_2(x)=g(x) && x\in \{a,b\} \\ \end{array}$$ for $p_1,p_2\in C^1[a,b],f\in C[a,b], p_1(x),p_2(x)\geq p_0>0.$ Show that there exists $C=C(p_0,f,g,(a,b))>0$ so that $$\int_a^b |u_1(x)-u_2(x)|^2+|u^\prime_1(x)-u^\prime_2(x)|^2dx\leq C\|p_1-p_2\|^2_\infty$$ Hint: You can use (without a prove) that the solution $u$ of $$ \begin{array}{ll} (p(x)u^\prime(x))^\prime+u(x)=0 && x\in (a,b)\\ u(x)=g(x)  && x\in \{a,b\} \\ \end{array}$$ satisfies $$\int_a^b |u(x)|^2+|u^\prime(x)|^2dx\leq C(g,p,(a,b))$$ I looked at $v(x)=u_1(x)-u_2(x)$ and tried to find a Sturm Liouville equation which is satisfied by $v$ to use the hint but this does not lead to the goal.",Consider the solutions of the Sturm Liouville equations and for Show that there exists so that Hint: You can use (without a prove) that the solution of satisfies I looked at and tried to find a Sturm Liouville equation which is satisfied by to use the hint but this does not lead to the goal.,"u_1,u_2 
\begin{array}{ll}
(p_1(x)u_1^\prime(x))^\prime+u_1(x)=f(x) && x\in (a,b)\\
u_1(x)=g(x) && x\in \{a,b\} \\
\end{array} \begin{array}{ll}
(p_2(x)u_2^\prime(x))^\prime+u_2(x)=f(x) && x\in (a,b)\\
u_2(x)=g(x) && x\in \{a,b\} \\
\end{array} p_1,p_2\in C^1[a,b],f\in C[a,b], p_1(x),p_2(x)\geq p_0>0. C=C(p_0,f,g,(a,b))>0 \int_a^b |u_1(x)-u_2(x)|^2+|u^\prime_1(x)-u^\prime_2(x)|^2dx\leq C\|p_1-p_2\|^2_\infty u 
\begin{array}{ll}
(p(x)u^\prime(x))^\prime+u(x)=0 && x\in (a,b)\\
u(x)=g(x)  && x\in \{a,b\} \\
\end{array} \int_a^b |u(x)|^2+|u^\prime(x)|^2dx\leq C(g,p,(a,b)) v(x)=u_1(x)-u_2(x) v","['calculus', 'ordinary-differential-equations', 'partial-differential-equations', 'stability-in-odes', 'sturm-liouville']"
40,Fitting a dampening coefficient and computing a Fourier transform at the same time,Fitting a dampening coefficient and computing a Fourier transform at the same time,,"I have a brief mono audio sample (a recording of a single note played on a guitar) that looks like this: As you can see in the image, the signal is losing intensity over time. My goal is to quantify the rate of decay of this note, i.e. how quickly it is losing intensity. I can do this heuristically, e.g. by taking the ratio between the first and last peak, but this approach is not very robust because it only uses two of the sample points and is thus highly sensitive to random noise. If possible, I'd like to ""divide out"" the oscillation component of the signal (probably using a Fourier transform) and then fit an exponential decay equation to the result. This approach would make use of the entire audio sample, and therefore be less sensitive to error in an individual sample point. However, I do not know if this approach is viable, and I don't understand the details of how to carry this out. My attempt We know from differential equations that the position of a damped harmonic oscillator is given by (after scaling and translation) $$x(t) = \exp(-\lambda t) \sin( \omega t),$$ where $\lambda$ is a damping coefficient and $\omega$ is the frequency. So, one idea is to fit the equation above to my input data (e.g. using least squares). But a guitar string doesn't oscillate at just one frequency $\omega$ —my input signal is really the composite of several frequencies, which depend on the physical properties of the string and guitar body. To get frequency components from time series data, we normally use a (discrete, in this case) Fourier transform. I'm stuck because it seems like I need to fit a damped harmonic oscillator model and perform a Fourier transform simultaneously. I know how to do these tasks individually, but not at the same time. My questions Is there a standard way to estimate the decay coefficient from time series data like this? Can it done programmatically? (That is, in a way that doesn't require manually labeling peaks, specifying the fundamental frequency, etc.) I have seen references online to something called RT60 estimation, used to quantify how much sound decays in a room, but this seems like a somewhat different problem: There, the problem is to integrate impulse response data sampled from many different places in the room, whereas I have just a single time series and no notion of place within the room.","I have a brief mono audio sample (a recording of a single note played on a guitar) that looks like this: As you can see in the image, the signal is losing intensity over time. My goal is to quantify the rate of decay of this note, i.e. how quickly it is losing intensity. I can do this heuristically, e.g. by taking the ratio between the first and last peak, but this approach is not very robust because it only uses two of the sample points and is thus highly sensitive to random noise. If possible, I'd like to ""divide out"" the oscillation component of the signal (probably using a Fourier transform) and then fit an exponential decay equation to the result. This approach would make use of the entire audio sample, and therefore be less sensitive to error in an individual sample point. However, I do not know if this approach is viable, and I don't understand the details of how to carry this out. My attempt We know from differential equations that the position of a damped harmonic oscillator is given by (after scaling and translation) where is a damping coefficient and is the frequency. So, one idea is to fit the equation above to my input data (e.g. using least squares). But a guitar string doesn't oscillate at just one frequency —my input signal is really the composite of several frequencies, which depend on the physical properties of the string and guitar body. To get frequency components from time series data, we normally use a (discrete, in this case) Fourier transform. I'm stuck because it seems like I need to fit a damped harmonic oscillator model and perform a Fourier transform simultaneously. I know how to do these tasks individually, but not at the same time. My questions Is there a standard way to estimate the decay coefficient from time series data like this? Can it done programmatically? (That is, in a way that doesn't require manually labeling peaks, specifying the fundamental frequency, etc.) I have seen references online to something called RT60 estimation, used to quantify how much sound decays in a room, but this seems like a somewhat different problem: There, the problem is to integrate impulse response data sampled from many different places in the room, whereas I have just a single time series and no notion of place within the room.","x(t) = \exp(-\lambda t) \sin( \omega t), \lambda \omega \omega","['ordinary-differential-equations', 'algorithms', 'fourier-series']"
41,Determine a function from a monotone condition,Determine a function from a monotone condition,,"Let $f\in C^\infty((0,1),\mathbb{R}_+)\cap C_0([0,1])$ and $$\lim_{t\to 0^+}f'(t)=+\infty,\lim_{t\to1^-}f'(t)=-\infty.$$ Furthermore, if $$F(t):=\frac{f''(t)}{f(t)(1+(f'(t))^2)^2}$$ is monotoinc in $(0,1)$ , find $f$ . The function $\sqrt{x-x^2}$ is obviously a solution. And $F$ is a constant at that time. But I am unable to determine whether it is the unique function that satisfies the conditions. The condition ""monotonic"" seems too weak for me. My attempts: The numerator of $F'$ is $$-f'f''(1+(f')^2+4ff'')+ff'''(1+(f')^2).$$ And we can suppose $F'\geq0$ since otherwise we can change $f(t)$ to $f(1-t)$ . I tried to analyze the behavior of $F'$ at the maximum point of $f$ and use the comparation theorems in ODE, but I didn't success, and I still don't know how to use the conditions of $f'$ . I also tried to let $g=f^2$ and wanted to show $g'''=0$ , but it didn't work as well. Any suggestion or counter-example will be appreciated. EDIT: I heard that $F$ is actually the Gaussian curvature of the rotational surface of $f$ about the $t$ -axis. And the conditions of $f'$ promises the surface to be $C^1$ . Will this help?","Let and Furthermore, if is monotoinc in , find . The function is obviously a solution. And is a constant at that time. But I am unable to determine whether it is the unique function that satisfies the conditions. The condition ""monotonic"" seems too weak for me. My attempts: The numerator of is And we can suppose since otherwise we can change to . I tried to analyze the behavior of at the maximum point of and use the comparation theorems in ODE, but I didn't success, and I still don't know how to use the conditions of . I also tried to let and wanted to show , but it didn't work as well. Any suggestion or counter-example will be appreciated. EDIT: I heard that is actually the Gaussian curvature of the rotational surface of about the -axis. And the conditions of promises the surface to be . Will this help?","f\in C^\infty((0,1),\mathbb{R}_+)\cap C_0([0,1]) \lim_{t\to 0^+}f'(t)=+\infty,\lim_{t\to1^-}f'(t)=-\infty. F(t):=\frac{f''(t)}{f(t)(1+(f'(t))^2)^2} (0,1) f \sqrt{x-x^2} F F' -f'f''(1+(f')^2+4ff'')+ff'''(1+(f')^2). F'\geq0 f(t) f(1-t) F' f f' g=f^2 g'''=0 F f t f' C^1","['ordinary-differential-equations', 'analysis', 'functions', 'derivatives']"
42,Localization of ground state Schrödinger eigenfunction,Localization of ground state Schrödinger eigenfunction,,"Principle: The ground state eigenfunction of a Schrodinger operator is localized near the minima of the potential. I learned this principle in basic quantum mechanics courses (when solving the Schrödinger equation for various potentials, e.g. double well, periodic box, etc.). My question concerns how to quantify this statement. Let $I=[-1,1]$ and $V:I\to \mathbb{R}$ be a smooth periodic potential and let $$ L=-\frac{d^2}{dx^2}+V(x) $$ be the corresponding Schrödinger operator on $I$ with periodic boundary conditions. Let $\lambda_1$ be the lowest eigenvalue of the eigenvalue problem $$ Lu=\lambda u $$ on $I$ with periodic boundary conditions and let $u_1$ be the corresponding eigenfunction (the ground state eigenfunction). By the nodal domain theorem, $u_1$ has no zeros, hence we may assume $u_1>0$ . Suppose also that $\lambda_1\leq 0$ , which implies that $V_{\min}\leq 0$ . Let $x_0\in I$ be the point at which the potential is maximized $$ V(x_0)=V_{\max}. $$ Question: Is it true that $x_0$ minimizes $u$ , i.e. that $$ u(x_0)=u_{\min}? $$ Numerical solutions I generated using Mathematica for various potentials indicate that the answer to the question is affirmative. How can one prove or disprove this. If the answer is negative, how can the ""principle"" written above be quantified? Any references would be appreciated. Thank you.","Principle: The ground state eigenfunction of a Schrodinger operator is localized near the minima of the potential. I learned this principle in basic quantum mechanics courses (when solving the Schrödinger equation for various potentials, e.g. double well, periodic box, etc.). My question concerns how to quantify this statement. Let and be a smooth periodic potential and let be the corresponding Schrödinger operator on with periodic boundary conditions. Let be the lowest eigenvalue of the eigenvalue problem on with periodic boundary conditions and let be the corresponding eigenfunction (the ground state eigenfunction). By the nodal domain theorem, has no zeros, hence we may assume . Suppose also that , which implies that . Let be the point at which the potential is maximized Question: Is it true that minimizes , i.e. that Numerical solutions I generated using Mathematica for various potentials indicate that the answer to the question is affirmative. How can one prove or disprove this. If the answer is negative, how can the ""principle"" written above be quantified? Any references would be appreciated. Thank you.","I=[-1,1] V:I\to \mathbb{R} 
L=-\frac{d^2}{dx^2}+V(x)
 I \lambda_1 
Lu=\lambda u
 I u_1 u_1 u_1>0 \lambda_1\leq 0 V_{\min}\leq 0 x_0\in I 
V(x_0)=V_{\max}.
 x_0 u 
u(x_0)=u_{\min}?
","['ordinary-differential-equations', 'partial-differential-equations', 'spectral-theory', 'eigenfunctions', 'sturm-liouville']"
43,Does this system of coupled second order differential equations have a closed form solution?,Does this system of coupled second order differential equations have a closed form solution?,,"I'm trying to evaluate the evolution of two scalar fields but their equations of motion are coupled via a potential term $$ V(\phi, \psi) \supset \frac{1}{2}\lambda \phi^{2}\psi^{2}.$$ From the lagrangian, the equations of motion are: $$ \ddot{\phi} - 3H\dot{\phi} + m^{2}\phi - \lambda \phi \psi^{2} = 0,  $$ $$ \ddot{\psi} - 3H\dot{\psi} + m^{2}\psi - \lambda \psi \phi^{2} = 0, $$ where $H$ is the Hubble constant (although it depends on time, for my purpose it can be set to a constant) and the dots denote a differentiation in respect to time. I'm looking for a closed solution or a way to decouple those equations avoiding the obvious $\lambda = 0$ situation.","I'm trying to evaluate the evolution of two scalar fields but their equations of motion are coupled via a potential term From the lagrangian, the equations of motion are: where is the Hubble constant (although it depends on time, for my purpose it can be set to a constant) and the dots denote a differentiation in respect to time. I'm looking for a closed solution or a way to decouple those equations avoiding the obvious situation."," V(\phi, \psi) \supset \frac{1}{2}\lambda \phi^{2}\psi^{2}.  \ddot{\phi} - 3H\dot{\phi} + m^{2}\phi - \lambda \phi \psi^{2} = 0,    \ddot{\psi} - 3H\dot{\psi} + m^{2}\psi - \lambda \psi \phi^{2} = 0,  H \lambda = 0",['ordinary-differential-equations']
44,Is this function an alternative solution to the nonlinear pendulum?,Is this function an alternative solution to the nonlinear pendulum?,,"Is this function an alternative solution to the nonlinear pendulum? Introduction I am working with the differential equation of the frictionless nonlinear pendulum: $$\ddot{\theta}(t) + b\,\sin(\theta(t)) = 0 \tag{Eq. 1}$$ Which it already have known exact solution : $$\theta(t)=2\sin^{-1}\left(\sin\left(\frac{\theta_0}{2}\right)\text{sn}\left[K\left(\textstyle{\sin^2\left(\frac{\theta_0}{2}\right)}\right)-\sqrt{b}\,t\,;\,\sin^2\left(\frac{\theta_0}{2}\right)\right] \right)\tag{Eq. 2}$$ with $\text{sn}[z;m]$ one of the Jacobi elliptic functions with mode $m$ , $K(z)$ the Complete elliptic integral of the first kind , and $\theta_0 = \theta(t=0)$ . Also, on Wolfram-alpha the solution to Eq. 1 is given by: $$\theta(t)=\pm\,\text{am}\left[\frac{1}{2}\sqrt{(2b+c_1)(t+c_2)^2}\,;\,\frac{4b}{2b+c_1}\right] \tag{Eq. 3}$$ with $\text{am}[z\,;\,m]$ the Jacobi amplitude function which fulfills $\text{sn}[z\,;\,m] = \sin\left(\text{am}[z\,;\,m]\right)$ , and $c_1,\,c_2$ are integration constants. If the initial conditions $\theta_0 = \frac{\pi}{2}$ and $\theta'(0)=0$ are introduced in Wolfram-Alpha the solution takes the form: $$\theta(t) = 2\,\sin^{-1}\left(\frac{\text{cd}\left[\sqrt{b}\,t\,;\,\frac{1}{2}\right]}{\sqrt{2}}\right)\tag{Eq. 4}$$ where $\text{cd}[z\,;\,m]$ is another secondary Jacobi elliptic function . Main text I was trying to apply the Weierstrass substitution $x(t)=\tan\left(\frac{\theta(t)}{2}\right)$ to Eq. 1 since it made possible to change the sine function into polynomials: $$ \begin{array}{c}  (i)\,\,\sin(\theta) = \frac{2x}{1+x^2}\,; & (ii)\,\,\dot{\theta} = \frac{2\dot{x}}{1+x^2}\,; & (iii)\,\,\ddot{\theta} = \frac{\ddot{x}(1+x^2)-2x(\dot{x})^2}{(1+x^2)^2} \end{array}$$ and since $1+x^2 > 0,\,\,\forall x \in \mathbb{R}$ with this change of variable Eq. 1 becomes: $$\ddot{x}+b\,x = \frac{2x(\dot{x})^2}{(1+x^2)} \tag{Eq. 5}$$ where it can be seen that Wolfram Alpha find the solution to Eq. 5 given by complex numbers: $$ x(t) = \pm i\,\text{sn}\left[i\sqrt{b+c_1}\,(t+c_2)\,;\,\frac{c_1}{b+c_1}\right] \tag{Eq. 6}$$ By a ""happy accident"" trying to solve Eq. 5, I found about the existence of the Lemniscate elliptic functions : lemniscate sine function $sl(x)$ and lemniscate cosine function $cl(x)$ , which haves only one variable and fulfills the following properties: $(iv)\quad sl' = cl\,(1+sl^2)$ $(v)\quad cl' = -sl\,(1+cl^2)$ $(vi)\quad (sl\,cl)' = cl^2-sl^2$ $(vii)\quad sl^2+sl\,cl+cl^2 = 1$ $(viii)\quad (sl')^2+sl^4 = 1$ $(ix)\quad sl^4+sl^4cl^2+sl^2cl^2 = sl^2$ $(x)\quad (1+sl^2)(1+cl^2)=2$ $(xi)\quad cl(t) = \text{cn}\left[\sqrt{2}\,t\,;\,\frac{1}{\sqrt{2}}\right]$ $(xii)\quad sl(t) = \displaystyle{\frac{\text{sd}\left[\sqrt{2}\,t\,;\,\frac{1}{\sqrt{2}}\right]}{\sqrt{2}}}$ Here is where the ""issue"" begins, since I found the following by accident, I don´t know if it is completely right, and maybe there is an ""initial constants issues"" that could made it an incomplete solution, as Eq. 4 is different from Eq. 3... but I believe that the following function is a solution to Eq. 5: $$ x(t) = cl\left(\sqrt{\frac{b}{2}}\,t-c_0\right) \tag{Eq. 7}$$ Differentiating Eq. 7 through property $(v)$ which lead to (I will be omitting the argument): $$ x'(t) = -sl\,(1+cl^2)\sqrt{\frac{b}{2}} \tag{Eq. 8}$$ And differentiating Eq. 8 through properties $(iv),\,(v),\,(x)$ which lead to: $$ \begin{array}{r c l}  x''(t) & = & -\sqrt{\frac{b}{2}}\left\{cl(1+sl^2)(1+cl^2)\sqrt{\frac{b}{2}}+sl\,(2\,cl)(-sl(1+cl^2))\sqrt{\frac{b}{2}} \right\} \\ & = & -\frac{b}{2}\left\{2\,cl-2\,cl\,sl^2(1+cl^2)) \right\} \\ & = & -b\left\{cl-cl\,sl^2(1+cl^2)) \right\} \tag{Eq. 9} \end{array}$$ Now, solving the Left-Hand-Side (LHS) and Right-Hand-Side (LHS) of Eq. 5 separately will lead to: $$\text{LHS} = x'' + b\,x = -b\,cl + b\,cl\,sl^2(1+cl^2)+b\,cl =  b\,cl\,sl^2(1+cl^2) \tag{Eq. 10}$$ $$\text{RHS} = \frac{2x(\dot{x})^2}{(1+x^2)} = \frac{2\,cl\left(-sl(1+cl^2)\sqrt{\frac{b}{2}}\right)^2}{1+cl^2} = \frac{b\,cl\,sl^2(1+cl^2)^2}{1+cl^2} = b\,cl\,sl^2(1+cl^2) \tag{Eq. 11}$$ If I didn´t make any mistakes, since Eq. 10 = Eq. 11 , I think that Eq. 7 is indeed a solution of Eq. 5, so a solution to Eq. 1 will be: $$ \theta(t) = 2\tan^{-1}\left(cl\left(\sqrt{\frac{b}{2}}\,t-c_0\right)\right) \tag{Eq. 12}$$ Where the integration constant will be given by (using their own inverse functions): $$c_0 = \frac{1}{\sqrt{2}}F\left[\cos^{-1}\left(\tan\left(\frac{\theta_0}{2}\right)\right)\,;\,\frac{1}{\sqrt{2}}\right] \tag{Eq. 13}$$ with $F[z,\,;\,m]$ the Incomplete elliptic integral of the first kind . Here since the constant is dependent of having well obtained Eq. 12, I am not so interested in it, being Eq. 12 the main topic from the questions. The Big Questions Is Eq. 12 a ""true"" solution for the frictionless nonlinear pendulum? ... Hope if there are calculation mistakes or conceptual ones, please show on your answer how to properly find the solution (if there is any by using the Weierstrass Substitution). Are all the presented solutions equivalent? ... I am not familiar with nonlinear differential equations, and I have read that sometimes don´t fulfill uniqueness of solutions, so I would like to know if they are the same solution or not (myself I tried to plot them but ""softwares"" have a bad time working with elliptic functions). Also, I have no formal training on elliptic functions (I discovered them on Wikipedia), so maybe equivalence among solutions could be ""easily"" proved (I hope) through Jacobi Theta Functions or Neville Theta Functions , or other elliptic functions, but unfortunately for me are too complicated and I don´t understand how to work with them from the Wiki web pages. There is a general solution in terms of lemniscatic sine/cosine functions? This for avoiding the second parameter of the Elliptic Functions. Here note that solution of Eq. 7 and 12 only depends of one integration constant!! which must be wrong for a second order ODE!! ... I believe the solution I found works only when starting from rest $\dot{\theta}(t)=0$ , but I am not really sure - Hope you can give the general form of this solution in terms of the lemniscatic function (I tried to work with Eq. 6 on Wolfram Alpha but a complex elliptic function was too heavy for the free version I think). Motivation The nonlinear pendulum with friction is known for being the simple and classical example of a mechanical system with friction included, by so far there is not known exact solution to its differential equation: $$\ddot{\theta}(t) + a\,\dot{\theta}+b\,\sin(\theta(t)) = 0 \tag{Eq. 14}$$ By using the Weierstrass Substitution $x(t)=\tan\left(\frac{\theta(t)}{2}\right)$ to Eq. 14 it will becomes: $$\ddot{x}+a\,\dot{x}+b\,x = \frac{2x(\dot{x})^2}{(1+x^2)} \tag{Eq. 15}$$ which is highly similar to Eq. 5, so I am trying to find if something of the form $x(t)=q(t)\,cl(\sqrt{b/2}\,t-c)$ could be a solution for a function $q(t)$ to be determined (but it rapidly becomes in a mess). Added Later I was trying to develop Eq. 6 in something similar to Eq. 7, but things are more confusing now... using what is said on Wikipedia I can use the following property: $$ (xiii)\quad \text{sn}[ix;\,m] = i\,\text{sc}[x;\,1-m]$$ where $\text{sc}[x;\,m]=\frac{\text{sn}[x;\,m]}{\text{cn}[x;\,m]}$ another Jacobi Elliptic function. But reading the reference of Wikipedia, it takes it from Page 504 of Wiki-Reference where the property is display as $\text{sn}[ix;\,k] = i\,\text{sc}[x;\,k']$ where in Page 493 is defined the complementary modulus as $k^2+k'^2=1$ , which is different from property $(xiii)$ - I don´t know if there is a mistake or both are right because of other properties of the Jacobi elliptic functions, but since in Wolfram-Alpha is implemented the Wikipedia version, I will use it as if it is correct. With this, Eq. 6 becomes: $$ x(t) = \pm \text{sc}\left[\sqrt{b+c_1}\,(t+c_2)\,;\,\frac{b}{b+c_1}\right] \tag{Eq. 16}$$ Which will imply that: $$\theta(t) = 2 \tan^{-1}\left(\text{sc}\left[\sqrt{b+c_1}\,(t+c_2)\,;\,\frac{b}{b+c_1}\right] \right) \tag{Eq. 17}$$ which should be the same solution as Eq. 2 by Uniqueness (as I get convinced from here - but for me is not trivial at all, I don´t know if after matching the integration constants will lead to some new properties, but is of reach of my current skills). At least, using property $(xi)$ in Eq. 7, one can see the proposed solution as: $$x(t) = \text{cn}\left[\sqrt{b}t-c_3;\,\frac{1}{\sqrt{2}}\right] \tag{Eq. 18}$$ but I have not been able to match the constants so Eq. 18 and Eq. 16 becomes equivalent, which should be the case at some pair of integrating constant due Uniqueness of solutions (I don't believe I accidentally found something that drop Uniqueness theorem of Lipschitz Diff. Eq... so If you found the corresponding constants, will be highly appreciated).","Is this function an alternative solution to the nonlinear pendulum? Introduction I am working with the differential equation of the frictionless nonlinear pendulum: Which it already have known exact solution : with one of the Jacobi elliptic functions with mode , the Complete elliptic integral of the first kind , and . Also, on Wolfram-alpha the solution to Eq. 1 is given by: with the Jacobi amplitude function which fulfills , and are integration constants. If the initial conditions and are introduced in Wolfram-Alpha the solution takes the form: where is another secondary Jacobi elliptic function . Main text I was trying to apply the Weierstrass substitution to Eq. 1 since it made possible to change the sine function into polynomials: and since with this change of variable Eq. 1 becomes: where it can be seen that Wolfram Alpha find the solution to Eq. 5 given by complex numbers: By a ""happy accident"" trying to solve Eq. 5, I found about the existence of the Lemniscate elliptic functions : lemniscate sine function and lemniscate cosine function , which haves only one variable and fulfills the following properties: Here is where the ""issue"" begins, since I found the following by accident, I don´t know if it is completely right, and maybe there is an ""initial constants issues"" that could made it an incomplete solution, as Eq. 4 is different from Eq. 3... but I believe that the following function is a solution to Eq. 5: Differentiating Eq. 7 through property which lead to (I will be omitting the argument): And differentiating Eq. 8 through properties which lead to: Now, solving the Left-Hand-Side (LHS) and Right-Hand-Side (LHS) of Eq. 5 separately will lead to: If I didn´t make any mistakes, since Eq. 10 = Eq. 11 , I think that Eq. 7 is indeed a solution of Eq. 5, so a solution to Eq. 1 will be: Where the integration constant will be given by (using their own inverse functions): with the Incomplete elliptic integral of the first kind . Here since the constant is dependent of having well obtained Eq. 12, I am not so interested in it, being Eq. 12 the main topic from the questions. The Big Questions Is Eq. 12 a ""true"" solution for the frictionless nonlinear pendulum? ... Hope if there are calculation mistakes or conceptual ones, please show on your answer how to properly find the solution (if there is any by using the Weierstrass Substitution). Are all the presented solutions equivalent? ... I am not familiar with nonlinear differential equations, and I have read that sometimes don´t fulfill uniqueness of solutions, so I would like to know if they are the same solution or not (myself I tried to plot them but ""softwares"" have a bad time working with elliptic functions). Also, I have no formal training on elliptic functions (I discovered them on Wikipedia), so maybe equivalence among solutions could be ""easily"" proved (I hope) through Jacobi Theta Functions or Neville Theta Functions , or other elliptic functions, but unfortunately for me are too complicated and I don´t understand how to work with them from the Wiki web pages. There is a general solution in terms of lemniscatic sine/cosine functions? This for avoiding the second parameter of the Elliptic Functions. Here note that solution of Eq. 7 and 12 only depends of one integration constant!! which must be wrong for a second order ODE!! ... I believe the solution I found works only when starting from rest , but I am not really sure - Hope you can give the general form of this solution in terms of the lemniscatic function (I tried to work with Eq. 6 on Wolfram Alpha but a complex elliptic function was too heavy for the free version I think). Motivation The nonlinear pendulum with friction is known for being the simple and classical example of a mechanical system with friction included, by so far there is not known exact solution to its differential equation: By using the Weierstrass Substitution to Eq. 14 it will becomes: which is highly similar to Eq. 5, so I am trying to find if something of the form could be a solution for a function to be determined (but it rapidly becomes in a mess). Added Later I was trying to develop Eq. 6 in something similar to Eq. 7, but things are more confusing now... using what is said on Wikipedia I can use the following property: where another Jacobi Elliptic function. But reading the reference of Wikipedia, it takes it from Page 504 of Wiki-Reference where the property is display as where in Page 493 is defined the complementary modulus as , which is different from property - I don´t know if there is a mistake or both are right because of other properties of the Jacobi elliptic functions, but since in Wolfram-Alpha is implemented the Wikipedia version, I will use it as if it is correct. With this, Eq. 6 becomes: Which will imply that: which should be the same solution as Eq. 2 by Uniqueness (as I get convinced from here - but for me is not trivial at all, I don´t know if after matching the integration constants will lead to some new properties, but is of reach of my current skills). At least, using property in Eq. 7, one can see the proposed solution as: but I have not been able to match the constants so Eq. 18 and Eq. 16 becomes equivalent, which should be the case at some pair of integrating constant due Uniqueness of solutions (I don't believe I accidentally found something that drop Uniqueness theorem of Lipschitz Diff. Eq... so If you found the corresponding constants, will be highly appreciated).","\ddot{\theta}(t) + b\,\sin(\theta(t)) = 0 \tag{Eq. 1} \theta(t)=2\sin^{-1}\left(\sin\left(\frac{\theta_0}{2}\right)\text{sn}\left[K\left(\textstyle{\sin^2\left(\frac{\theta_0}{2}\right)}\right)-\sqrt{b}\,t\,;\,\sin^2\left(\frac{\theta_0}{2}\right)\right] \right)\tag{Eq. 2} \text{sn}[z;m] m K(z) \theta_0 = \theta(t=0) \theta(t)=\pm\,\text{am}\left[\frac{1}{2}\sqrt{(2b+c_1)(t+c_2)^2}\,;\,\frac{4b}{2b+c_1}\right] \tag{Eq. 3} \text{am}[z\,;\,m] \text{sn}[z\,;\,m] = \sin\left(\text{am}[z\,;\,m]\right) c_1,\,c_2 \theta_0 = \frac{\pi}{2} \theta'(0)=0 \theta(t) = 2\,\sin^{-1}\left(\frac{\text{cd}\left[\sqrt{b}\,t\,;\,\frac{1}{2}\right]}{\sqrt{2}}\right)\tag{Eq. 4} \text{cd}[z\,;\,m] x(t)=\tan\left(\frac{\theta(t)}{2}\right)  \begin{array}{c} 
(i)\,\,\sin(\theta) = \frac{2x}{1+x^2}\,; & (ii)\,\,\dot{\theta} = \frac{2\dot{x}}{1+x^2}\,; & (iii)\,\,\ddot{\theta} = \frac{\ddot{x}(1+x^2)-2x(\dot{x})^2}{(1+x^2)^2}
\end{array} 1+x^2 > 0,\,\,\forall x \in \mathbb{R} \ddot{x}+b\,x = \frac{2x(\dot{x})^2}{(1+x^2)} \tag{Eq. 5}  x(t) = \pm i\,\text{sn}\left[i\sqrt{b+c_1}\,(t+c_2)\,;\,\frac{c_1}{b+c_1}\right] \tag{Eq. 6} sl(x) cl(x) (iv)\quad sl' = cl\,(1+sl^2) (v)\quad cl' = -sl\,(1+cl^2) (vi)\quad (sl\,cl)' = cl^2-sl^2 (vii)\quad sl^2+sl\,cl+cl^2 = 1 (viii)\quad (sl')^2+sl^4 = 1 (ix)\quad sl^4+sl^4cl^2+sl^2cl^2 = sl^2 (x)\quad (1+sl^2)(1+cl^2)=2 (xi)\quad cl(t) = \text{cn}\left[\sqrt{2}\,t\,;\,\frac{1}{\sqrt{2}}\right] (xii)\quad sl(t) = \displaystyle{\frac{\text{sd}\left[\sqrt{2}\,t\,;\,\frac{1}{\sqrt{2}}\right]}{\sqrt{2}}}  x(t) = cl\left(\sqrt{\frac{b}{2}}\,t-c_0\right) \tag{Eq. 7} (v)  x'(t) = -sl\,(1+cl^2)\sqrt{\frac{b}{2}} \tag{Eq. 8} (iv),\,(v),\,(x)  \begin{array}{r c l} 
x''(t) & = & -\sqrt{\frac{b}{2}}\left\{cl(1+sl^2)(1+cl^2)\sqrt{\frac{b}{2}}+sl\,(2\,cl)(-sl(1+cl^2))\sqrt{\frac{b}{2}} \right\} \\
& = & -\frac{b}{2}\left\{2\,cl-2\,cl\,sl^2(1+cl^2)) \right\} \\
& = & -b\left\{cl-cl\,sl^2(1+cl^2)) \right\} \tag{Eq. 9}
\end{array} \text{LHS} = x'' + b\,x = -b\,cl + b\,cl\,sl^2(1+cl^2)+b\,cl =  b\,cl\,sl^2(1+cl^2) \tag{Eq. 10} \text{RHS} = \frac{2x(\dot{x})^2}{(1+x^2)} = \frac{2\,cl\left(-sl(1+cl^2)\sqrt{\frac{b}{2}}\right)^2}{1+cl^2} = \frac{b\,cl\,sl^2(1+cl^2)^2}{1+cl^2} = b\,cl\,sl^2(1+cl^2) \tag{Eq. 11}  \theta(t) = 2\tan^{-1}\left(cl\left(\sqrt{\frac{b}{2}}\,t-c_0\right)\right) \tag{Eq. 12} c_0 = \frac{1}{\sqrt{2}}F\left[\cos^{-1}\left(\tan\left(\frac{\theta_0}{2}\right)\right)\,;\,\frac{1}{\sqrt{2}}\right] \tag{Eq. 13} F[z,\,;\,m] \dot{\theta}(t)=0 \ddot{\theta}(t) + a\,\dot{\theta}+b\,\sin(\theta(t)) = 0 \tag{Eq. 14} x(t)=\tan\left(\frac{\theta(t)}{2}\right) \ddot{x}+a\,\dot{x}+b\,x = \frac{2x(\dot{x})^2}{(1+x^2)} \tag{Eq. 15} x(t)=q(t)\,cl(\sqrt{b/2}\,t-c) q(t)  (xiii)\quad \text{sn}[ix;\,m] = i\,\text{sc}[x;\,1-m] \text{sc}[x;\,m]=\frac{\text{sn}[x;\,m]}{\text{cn}[x;\,m]} \text{sn}[ix;\,k] = i\,\text{sc}[x;\,k'] k^2+k'^2=1 (xiii)  x(t) = \pm \text{sc}\left[\sqrt{b+c_1}\,(t+c_2)\,;\,\frac{b}{b+c_1}\right] \tag{Eq. 16} \theta(t) = 2 \tan^{-1}\left(\text{sc}\left[\sqrt{b+c_1}\,(t+c_2)\,;\,\frac{b}{b+c_1}\right] \right) \tag{Eq. 17} (xi) x(t) = \text{cn}\left[\sqrt{b}t-c_3;\,\frac{1}{\sqrt{2}}\right] \tag{Eq. 18}","['ordinary-differential-equations', 'solution-verification', 'elliptic-integrals', 'elliptic-functions', 'theta-functions']"
45,What are the solutions for $y(t)\cdot\left(y'(t) + a\right)=-b\sin(t)$?,What are the solutions for ?,y(t)\cdot\left(y'(t) + a\right)=-b\sin(t),"What are the solutions for $y(t)\cdot\left(y'(t) + a\right)=-b\sin(t)$ ? It could be proben that there exists some solutions? Are these solutions unique? and obviously, which are these solutions? (Closed-form if possible) Actually the question is simple, and no other info is required, but to encourage you to participate I will share a motivation, but this intro is not needed to find the answer (just in case you don´t want to read it - but is quite interesting in my opinion). Motivation I am trying to figure out if the equation of the classic damped non-linear pendulum $$y''(t) + ay'(t)+b\sin\left(y(t)\right)=0$$ admits finite-duration solutions ( $\{a,\,b\}$ are constants): I believe it must to, because is the simplest realistic physical model I know, and experimentally it stops moving after some finite time. But along with being the simplest case of ""realistic"" dynamical system, it is also known to don´t have any known close form solution, and every solution it has through approximations are ""vanishing at infinite"", don´t really being a ""true"" finite-duration functions. Recently I have learned that every non-constant dynamical system, to be of finite-duration, it must be nonlinear , so its solutions won´t be analytical in the full domain... so everything I know as engineer through Taylor Series, Linear ODEs, and Power Series expansions are approximations: quite shocking at first... I know that equations are ""models"" tied to assumptions, but I never think before that every model I know are actually approximations since no finite-duration solution can be supported by them ... neither dynamical systems with stands uniqueness of solutions will be an accurate model for finite-duration phenomena. But also I know now that a finite-duration solution could been represented within its duration using known functions, as bump functions $\in C_c^\infty$ , like $f(t) = e^{t^2/(t^2-1)},\,|t|\leq 1; \,(f(t)\text{ = 0, else})$ where the exponential function is defined to represent the solution only within its compact support. And studying compact-supported function, I found a really interesting condition: if a function is continuous and compact-supported (as a finite-duration position-vs-time model where ""teleportation"" is forbidden), then it is also: (i) a bounded function, and (ii) its Fourier Transform is an analytic function, both actually quite huge restrictions, and I start to wonder if being of finite-duration could make same kind of restrictions to the maximum rate of change that dynamical system could achieve, like restrictions to fulfill causality. So now, I am trying to see if a finite-duration alternative could be found for the solution of the nonlinear damped pendulum without using approximations (here for start from the very beginning of dynamical systems I know). I have found recently two papers from the same author (V. T. Haimo / Vardia Haimo), that analyze finite-duration differential equations [ 1 ] and [ 2 ]. And in [1] on Theorem 2 point (i), it is said that, without losing generality by considering that the finishing time of the finite-duration solution happens at $t_F = 0$ , for a second order dynamical system described by $\ddot{x}(t) = g(x(t),\dot{x}(t))$ such $g(0,0)=0$ (the system dynamics ""die"" at $t_F = 0$ ), with $g \in C^1(\mathbb{R}\setminus \{0\})$ , then for the system to support finite-duration solutions, the following another differential equation must have solutions: $$q(z)\frac{dq(z)}{dz} = g(z,q(z)),\,q(0)=0$$ Honestly the papers are bit advanced to my mathematical skills, but if I didn´t made any mistakes, following the example given on the papers, the corresponding equation for finding if the nonlinear damped pendulum supports finite-duration solutions is: $$q(z)\cdot\left(q'(z) + a\right)=-b\sin(z)$$ where I have used and abuse of notation in the main question: the $y(t)$ of the main equation are not the same function $y(t)$ of the solutions of the nonlinear damped pendulum, neither their variables $t$ are the same (it just look more natural for asking as a differential equation dependent in time instead of an arbitrary variable $z$ ). But I get stack here, since I don´t know how to figure out if the equation of the main question have solutions. Hope you get interested in this as I am, unfortunately, I have found just a few papers on Google about continuous time finite-duration differential equations, and neither of them as a whole studied Theory, neither a Wikipedia page, so or it is a quite unexplored topic, or because of security reasons the publication are not published for general public (the papers where published in a corporation that works for the military, so it could be a feasible reason).","What are the solutions for ? It could be proben that there exists some solutions? Are these solutions unique? and obviously, which are these solutions? (Closed-form if possible) Actually the question is simple, and no other info is required, but to encourage you to participate I will share a motivation, but this intro is not needed to find the answer (just in case you don´t want to read it - but is quite interesting in my opinion). Motivation I am trying to figure out if the equation of the classic damped non-linear pendulum admits finite-duration solutions ( are constants): I believe it must to, because is the simplest realistic physical model I know, and experimentally it stops moving after some finite time. But along with being the simplest case of ""realistic"" dynamical system, it is also known to don´t have any known close form solution, and every solution it has through approximations are ""vanishing at infinite"", don´t really being a ""true"" finite-duration functions. Recently I have learned that every non-constant dynamical system, to be of finite-duration, it must be nonlinear , so its solutions won´t be analytical in the full domain... so everything I know as engineer through Taylor Series, Linear ODEs, and Power Series expansions are approximations: quite shocking at first... I know that equations are ""models"" tied to assumptions, but I never think before that every model I know are actually approximations since no finite-duration solution can be supported by them ... neither dynamical systems with stands uniqueness of solutions will be an accurate model for finite-duration phenomena. But also I know now that a finite-duration solution could been represented within its duration using known functions, as bump functions , like where the exponential function is defined to represent the solution only within its compact support. And studying compact-supported function, I found a really interesting condition: if a function is continuous and compact-supported (as a finite-duration position-vs-time model where ""teleportation"" is forbidden), then it is also: (i) a bounded function, and (ii) its Fourier Transform is an analytic function, both actually quite huge restrictions, and I start to wonder if being of finite-duration could make same kind of restrictions to the maximum rate of change that dynamical system could achieve, like restrictions to fulfill causality. So now, I am trying to see if a finite-duration alternative could be found for the solution of the nonlinear damped pendulum without using approximations (here for start from the very beginning of dynamical systems I know). I have found recently two papers from the same author (V. T. Haimo / Vardia Haimo), that analyze finite-duration differential equations [ 1 ] and [ 2 ]. And in [1] on Theorem 2 point (i), it is said that, without losing generality by considering that the finishing time of the finite-duration solution happens at , for a second order dynamical system described by such (the system dynamics ""die"" at ), with , then for the system to support finite-duration solutions, the following another differential equation must have solutions: Honestly the papers are bit advanced to my mathematical skills, but if I didn´t made any mistakes, following the example given on the papers, the corresponding equation for finding if the nonlinear damped pendulum supports finite-duration solutions is: where I have used and abuse of notation in the main question: the of the main equation are not the same function of the solutions of the nonlinear damped pendulum, neither their variables are the same (it just look more natural for asking as a differential equation dependent in time instead of an arbitrary variable ). But I get stack here, since I don´t know how to figure out if the equation of the main question have solutions. Hope you get interested in this as I am, unfortunately, I have found just a few papers on Google about continuous time finite-duration differential equations, and neither of them as a whole studied Theory, neither a Wikipedia page, so or it is a quite unexplored topic, or because of security reasons the publication are not published for general public (the papers where published in a corporation that works for the military, so it could be a feasible reason).","y(t)\cdot\left(y'(t) + a\right)=-b\sin(t) y''(t) + ay'(t)+b\sin\left(y(t)\right)=0 \{a,\,b\} \in C_c^\infty f(t) = e^{t^2/(t^2-1)},\,|t|\leq 1; \,(f(t)\text{ = 0, else}) t_F = 0 \ddot{x}(t) = g(x(t),\dot{x}(t)) g(0,0)=0 t_F = 0 g \in C^1(\mathbb{R}\setminus \{0\}) q(z)\frac{dq(z)}{dz} = g(z,q(z)),\,q(0)=0 q(z)\cdot\left(q'(z) + a\right)=-b\sin(z) y(t) y(t) t z","['real-analysis', 'ordinary-differential-equations', 'physics', 'nonlinear-dynamics', 'finite-duration']"
46,Solving infinite ladder of differential equations using generating functions.,Solving infinite ladder of differential equations using generating functions.,,"I am interested in solving the following infinite ladder of coupled differential equations. For any integer $k \geq 0$ , we have a real-valued function of a single real variable, $p_k (t)$ , which satisfies $$\dot{p}_k(t) = (k+1)p_{k+1}(t) - kp_k(t)$$ Here, $t \geq 0$ (""time""), and the dot denotes a derivative. The choice of notation $p_k$ is intentional, as these form a set of probabilities. That is, $$\forall t\geq 0, \quad  p_k(t) \geq 0 \, \,\text{and}\,\, \sum_{k=0}^\infty p_k(t)=1$$ (One can show that the differential equations conserve this sum.) To solve this problem, I attempted to introduce a generating function of the form $$g(z, t) \equiv \sum_{k=0}^\infty z^k p_k(t).$$ This function has the property that $g(0, t) = 0$ and $g(1,t) = 1$ . Moreover, by differentiating the equation with respect to $t$ , I found that it satisfies the following first-order, linear partial differential equation. $$ \partial_t g(z,t) + (z-1)\partial_z g(z,t) = 0$$ This seems promising to me, as I seem to have a well defined boundary value problem. Namely, letting $t \in [0,\infty)$ and $z \in [0,1]$ , I set the values of $g$ at the boundaries $z = 0, 1$ , and with the corresponding initial condition $g(z,0)$ . This seems like a well-posed problem. However, I'm having trouble finding the solution. I believe the general solution to the differential equation is $$ g(z,t) = f(e^{-t}(1-z)) $$ where $f$ is any differentiable function of a single variable. But when I try to satisfy the boundary conditions, I hit a snag. The $z = 1$ condition implies $f(0) = 1$ , but the $z = 0$ condition implies $f(e^{-t}) = 0$ . I'm pretty sure this breaks the camel's back: it seems to be saying $f = 0$ for all values! Am I missing something? Are there modifications to this process that can lead me to a solution? Thanks in advance!","I am interested in solving the following infinite ladder of coupled differential equations. For any integer , we have a real-valued function of a single real variable, , which satisfies Here, (""time""), and the dot denotes a derivative. The choice of notation is intentional, as these form a set of probabilities. That is, (One can show that the differential equations conserve this sum.) To solve this problem, I attempted to introduce a generating function of the form This function has the property that and . Moreover, by differentiating the equation with respect to , I found that it satisfies the following first-order, linear partial differential equation. This seems promising to me, as I seem to have a well defined boundary value problem. Namely, letting and , I set the values of at the boundaries , and with the corresponding initial condition . This seems like a well-posed problem. However, I'm having trouble finding the solution. I believe the general solution to the differential equation is where is any differentiable function of a single variable. But when I try to satisfy the boundary conditions, I hit a snag. The condition implies , but the condition implies . I'm pretty sure this breaks the camel's back: it seems to be saying for all values! Am I missing something? Are there modifications to this process that can lead me to a solution? Thanks in advance!","k \geq 0 p_k (t) \dot{p}_k(t) = (k+1)p_{k+1}(t) - kp_k(t) t \geq 0 p_k \forall t\geq 0, \quad  p_k(t) \geq 0 \, \,\text{and}\,\, \sum_{k=0}^\infty p_k(t)=1 g(z, t) \equiv \sum_{k=0}^\infty z^k p_k(t). g(0, t) = 0 g(1,t) = 1 t  \partial_t g(z,t) + (z-1)\partial_z g(z,t) = 0 t \in [0,\infty) z \in [0,1] g z = 0, 1 g(z,0)  g(z,t) = f(e^{-t}(1-z))  f z = 1 f(0) = 1 z = 0 f(e^{-t}) = 0 f = 0","['ordinary-differential-equations', 'partial-differential-equations', 'problem-solving', 'generating-functions']"
47,"Regularity of an ODE: series Ansatz, analytic and numerical solutions","Regularity of an ODE: series Ansatz, analytic and numerical solutions",,"Suppose I have an ODE that can be written as $$ E(y'(x),y(x),x)=0 $$ Suppose also that this equation has a closed-form solution for $y(x)$ , that is $$y(x) =y(x,C)$$ for an integration constant $C$ . If I know from ""physical arguments"" that this function ought to be even (say because it is defined on a 3-sphere where x is the third hyper-spherical coordinate), I might get the idea of substituting in a sort of power series ansatz $$ y(x)= \sum_{i=-1}^{m} y_i x^i$$ where $m$ is some truncation number. This yield an identically vanishing polynomial, from the coefficients of which I might verify that indeed, only the even components of $y_i$ are non-zero. My first problem is, and this already tells me that the integration constant $C$ in the analytic solution has to take a specific value (zero in my case), if its series expansion around $x=0$ coincides with the $y_i$ coefficients calculated from the polynomial. Is this because a regularity (even power-series) at $x=0$ acts a sort of boundary condition, hence determining the integration constant? My second problem is, that even though on paper this function should be regular at the $x=0$ origin, in numerical solutions there IS a singularity there. How can this be, if $y_{-1}=0$ ? Should the numeric solution not go as $$ y_\text{numeric} \propto y_{0}+y_{2} x^2+\mathcal{O}(x^4)   \;\;\;?$$","Suppose I have an ODE that can be written as Suppose also that this equation has a closed-form solution for , that is for an integration constant . If I know from ""physical arguments"" that this function ought to be even (say because it is defined on a 3-sphere where x is the third hyper-spherical coordinate), I might get the idea of substituting in a sort of power series ansatz where is some truncation number. This yield an identically vanishing polynomial, from the coefficients of which I might verify that indeed, only the even components of are non-zero. My first problem is, and this already tells me that the integration constant in the analytic solution has to take a specific value (zero in my case), if its series expansion around coincides with the coefficients calculated from the polynomial. Is this because a regularity (even power-series) at acts a sort of boundary condition, hence determining the integration constant? My second problem is, that even though on paper this function should be regular at the origin, in numerical solutions there IS a singularity there. How can this be, if ? Should the numeric solution not go as"," E(y'(x),y(x),x)=0  y(x) y(x) =y(x,C) C  y(x)= \sum_{i=-1}^{m} y_i x^i m y_i C x=0 y_i x=0 x=0 y_{-1}=0  y_\text{numeric} \propto y_{0}+y_{2} x^2+\mathcal{O}(x^4)   \;\;\;?","['calculus', 'ordinary-differential-equations', 'power-series']"
48,What is this differential equation? Why is it here?,What is this differential equation? Why is it here?,,"I was riding a train near Paris and I saw this fairly detailed differential equation plastered on the wall, and I have no idea what it means or what context it gets used in. Does anyone recognize it? What does it mean? Why is it on the wall of a subway station, without any explanation? Apologies for the image quality—I have also included a transcription: $$ C'_p = C_p \left[1 + \frac{L}{C_p} \cdot \frac{\partial}{\partial T} \left(\frac{0.622 he,(T)}{p - he,(T)}\right)\right], $$ EDIT: I understand now this equation has something to do with global warming, but not what it means on its own, or what it implies about global warming. I'm also curious about who put it up (or rather, who convinced the city of Paris to put it up) and why they thought this would specifically be a good idea.","I was riding a train near Paris and I saw this fairly detailed differential equation plastered on the wall, and I have no idea what it means or what context it gets used in. Does anyone recognize it? What does it mean? Why is it on the wall of a subway station, without any explanation? Apologies for the image quality—I have also included a transcription: EDIT: I understand now this equation has something to do with global warming, but not what it means on its own, or what it implies about global warming. I'm also curious about who put it up (or rather, who convinced the city of Paris to put it up) and why they thought this would specifically be a good idea.","
C'_p = C_p \left[1 + \frac{L}{C_p} \cdot \frac{\partial}{\partial T} \left(\frac{0.622 he,(T)}{p - he,(T)}\right)\right],
","['ordinary-differential-equations', 'partial-differential-equations', 'reference-request']"
49,Example for Carleman Linearization resulting in a linear system,Example for Carleman Linearization resulting in a linear system,,"The Carleman linearization came to my attention due to this article . I tried to understand this method but so far i wasn't succesful i tried to understand page 39 of this presentation however the example didn't make sense for me. Could someone demonstrate how to compute a Carleman linearization and demonstrate/argue that the resulting linear ODE behaves similar to the non linear system? I would prefer a demonstration that $\frac{dx}{dt} = f(x,t)$ has a multidimensional $x$ $f(x,t)$ is non linear in $x$ It would be nice if the nonlinear system is well understood.  Examples would be the single or double inverse pendulum, Dubins car, SIR model, Lotka-Volterra  but that is not a requirement shows what role initial conditions play If there is some visualization (for example a phase portrait) that shows how a finite approximation of the infinite dimensional linear equation breaks and how it gets better if a larger finite dimensional approximation is used please also add that. Are there some well understood conditions when a Carleman linearization will be non successful in reproducing the dynamics?","The Carleman linearization came to my attention due to this article . I tried to understand this method but so far i wasn't succesful i tried to understand page 39 of this presentation however the example didn't make sense for me. Could someone demonstrate how to compute a Carleman linearization and demonstrate/argue that the resulting linear ODE behaves similar to the non linear system? I would prefer a demonstration that has a multidimensional is non linear in It would be nice if the nonlinear system is well understood.  Examples would be the single or double inverse pendulum, Dubins car, SIR model, Lotka-Volterra  but that is not a requirement shows what role initial conditions play If there is some visualization (for example a phase portrait) that shows how a finite approximation of the infinite dimensional linear equation breaks and how it gets better if a larger finite dimensional approximation is used please also add that. Are there some well understood conditions when a Carleman linearization will be non successful in reproducing the dynamics?","\frac{dx}{dt} = f(x,t) x f(x,t) x","['ordinary-differential-equations', 'analysis', 'nonlinear-system', 'linearization']"
50,Omega Limit sets for a Gradient system,Omega Limit sets for a Gradient system,,"I am trying to do the following exercise Let $V: \mathbb{R}^2 \rightarrow \mathbb{R}$ be a $C^2$ function such that the set of equilibrium points is finite. Consider the diferential equation $x'=-\nabla V(x)$ . Show that $V$ has no periodic orbits and no homoclinic orbits, and that for every $x\in \mathbb{R}^n$ if we have that $\omega(x)\neq \emptyset$ then it consists of a single equilibrium point. The first two assertions I was able to do. The first by noticing that $V(x(t_0))-V(x(t_N))=\int_{t_0}^{t_N}\frac{d}{ds}V(x(s)) ds =\int_{t_0}^{t_N}-\nabla V(x(s))^2ds<0$ . The second one since we have that $\frac{d}{dt}V(x(t))<0$ for any solution starting in the homoclinic orbit and if we have that $x_0$ is the equilibrium point associated with the homoclinic orbit then $x_0=\lim_{t_n\rightarrow \infty}x(t_n)=\lim_{t_n\rightarrow -\infty}x(t_n)$ but since we have that $V(x(t))$ is decreasing with time we get that $V(x_0)< V(x(t_0))<V(x_0)$ and hence a contradiction . Now the one I am not being able to do is the third assertion . Let's suppose that $\omega(x)\neq \emptyset $ , and now I would like to see that for $y\in \omega(x)$ then $\nabla V(y)=0$ , and with this I would get that $\omega(x)$ is a compact set since there's a finite number of equilibrium points and hence connceted and so it can only be one. But I am not sure how to see that $\nabla V(y)=0$ . We have that $\nabla V(y)=\lim_{t_n\rightarrow \infty}\nabla  V(x(t_n))$ . Also another idea would be to try and use Poincare Bendixson theorem but I am not sure that $\omega(x)\neq \emptyset$ implies that it is bounded, since there are examples of unbounded $\omega$ -limit sets. Also I don't think we know that the solution is defined on the whole line but I guess we are assuming it, I think for this to be true could ask that $V(x)\geq 0$ and that $V(0)=0.$ Any help with this last is appreciated. Thanks in advance.","I am trying to do the following exercise Let be a function such that the set of equilibrium points is finite. Consider the diferential equation . Show that has no periodic orbits and no homoclinic orbits, and that for every if we have that then it consists of a single equilibrium point. The first two assertions I was able to do. The first by noticing that . The second one since we have that for any solution starting in the homoclinic orbit and if we have that is the equilibrium point associated with the homoclinic orbit then but since we have that is decreasing with time we get that and hence a contradiction . Now the one I am not being able to do is the third assertion . Let's suppose that , and now I would like to see that for then , and with this I would get that is a compact set since there's a finite number of equilibrium points and hence connceted and so it can only be one. But I am not sure how to see that . We have that . Also another idea would be to try and use Poincare Bendixson theorem but I am not sure that implies that it is bounded, since there are examples of unbounded -limit sets. Also I don't think we know that the solution is defined on the whole line but I guess we are assuming it, I think for this to be true could ask that and that Any help with this last is appreciated. Thanks in advance.",V: \mathbb{R}^2 \rightarrow \mathbb{R} C^2 x'=-\nabla V(x) V x\in \mathbb{R}^n \omega(x)\neq \emptyset V(x(t_0))-V(x(t_N))=\int_{t_0}^{t_N}\frac{d}{ds}V(x(s)) ds =\int_{t_0}^{t_N}-\nabla V(x(s))^2ds<0 \frac{d}{dt}V(x(t))<0 x_0 x_0=\lim_{t_n\rightarrow \infty}x(t_n)=\lim_{t_n\rightarrow -\infty}x(t_n) V(x(t)) V(x_0)< V(x(t_0))<V(x_0) \omega(x)\neq \emptyset  y\in \omega(x) \nabla V(y)=0 \omega(x) \nabla V(y)=0 \nabla V(y)=\lim_{t_n\rightarrow \infty}\nabla  V(x(t_n)) \omega(x)\neq \emptyset \omega V(x)\geq 0 V(0)=0.,"['ordinary-differential-equations', 'dynamical-systems']"
51,Distributional and weak solutions,Distributional and weak solutions,,"Consider a differential equation, $$L(T)=f,$$ where $L$ is a linear ordinary differential operator with smooth coefficients and $f$ is a $L_{loc}^1$ function. Does there always exist a distribution T such that above equation holds (if not what conditions on $f$ are sufficient to guarantee that)? What is an example s.t. distribution solution exists but any function is not a solution in the sense of distribution? I know examples of ODE's for which there are distributional solutions that are not functions (e.g. $x^2y’=0$ ), but I want an example in which no function is a solution but distributional solution exist. I am learning this stuff on my own so questions might be trivial. Thanks for any help!","Consider a differential equation, where is a linear ordinary differential operator with smooth coefficients and is a function. Does there always exist a distribution T such that above equation holds (if not what conditions on are sufficient to guarantee that)? What is an example s.t. distribution solution exists but any function is not a solution in the sense of distribution? I know examples of ODE's for which there are distributional solutions that are not functions (e.g. ), but I want an example in which no function is a solution but distributional solution exist. I am learning this stuff on my own so questions might be trivial. Thanks for any help!","L(T)=f, L f L_{loc}^1 f x^2y’=0","['ordinary-differential-equations', 'distribution-theory', 'regularity-theory-of-pdes']"
52,Finding the characteristic exponents (Floquet/Lyapunov exponents) of ODEs with periodic coefficients,Finding the characteristic exponents (Floquet/Lyapunov exponents) of ODEs with periodic coefficients,,"I am interested in finding the characteristic exponents $\lambda_i$ of the ODE $\pmb{x}'\left(t\right) = \pmb{A}\left(t\right)\pmb{x}\left(t\right)$ , where $\pmb{A}\left(t\right)$ is periodic with period $T$ . In particular, consider the following $2$ D case $\left[ {\begin{array}{cc}    x \\    y \\   \end{array} } \right]' = \left[ {\begin{array}{cc}    a_1 + b_1 \sin(\omega t) & a_2 + b_2 \sin(\omega t) \\    a_3 + b_3 \sin(\omega t) & a_4 + b_4 \sin(\omega t) \\   \end{array} } \right]\left[ {\begin{array}{cc}    x \\    y \\   \end{array} } \right]$ , where $a_i$ , $b_i$ , and $\omega$ are all constants. Although we have useful relations such as $$\lambda_1+\lambda_2= \frac{1}{T}\int_0^T {\mathrm tr}\boldsymbol{A}(\tau)\,\mathrm{d}\tau,$$ in order to find the values of individual $\lambda_1$ and $\lambda_2$ , we still need to calculate the principal fundamental matrix, which as far as I know requires solving the original ODE. I know for the $1$ D case $x' = \left(a_1 + b_1 \sin(\omega t)\right) x$ , the solution is $x(t) = c_1\mathrm{e}^{-\frac{b_1}{\omega}\cos(\omega t)+a_1 t}$ . But what is the solution to the $2$ D case above? I tried something like $$x(t) = c_1\mathrm{e}^{p_1\cos(\omega t)+q_1 t} + c_2\mathrm{e}^{p_2\cos(\omega t)+q_2 t},$$ $$y(t) = c_3\mathrm{e}^{p_1\cos(\omega t)+q_1 t} + c_4\mathrm{e}^{p_2\cos(\omega t)+q_2 t},$$ but it doesn't seem to work.","I am interested in finding the characteristic exponents of the ODE , where is periodic with period . In particular, consider the following D case , where , , and are all constants. Although we have useful relations such as in order to find the values of individual and , we still need to calculate the principal fundamental matrix, which as far as I know requires solving the original ODE. I know for the D case , the solution is . But what is the solution to the D case above? I tried something like but it doesn't seem to work.","\lambda_i \pmb{x}'\left(t\right) = \pmb{A}\left(t\right)\pmb{x}\left(t\right) \pmb{A}\left(t\right) T 2 \left[ {\begin{array}{cc}
   x \\
   y \\
  \end{array} } \right]' = \left[ {\begin{array}{cc}
   a_1 + b_1 \sin(\omega t) & a_2 + b_2 \sin(\omega t) \\
   a_3 + b_3 \sin(\omega t) & a_4 + b_4 \sin(\omega t) \\
  \end{array} } \right]\left[ {\begin{array}{cc}
   x \\
   y \\
  \end{array} } \right] a_i b_i \omega \lambda_1+\lambda_2= \frac{1}{T}\int_0^T {\mathrm tr}\boldsymbol{A}(\tau)\,\mathrm{d}\tau, \lambda_1 \lambda_2 1 x' = \left(a_1 + b_1 \sin(\omega t)\right) x x(t) = c_1\mathrm{e}^{-\frac{b_1}{\omega}\cos(\omega t)+a_1 t} 2 x(t) = c_1\mathrm{e}^{p_1\cos(\omega t)+q_1 t} + c_2\mathrm{e}^{p_2\cos(\omega t)+q_2 t}, y(t) = c_3\mathrm{e}^{p_1\cos(\omega t)+q_1 t} + c_4\mathrm{e}^{p_2\cos(\omega t)+q_2 t},","['ordinary-differential-equations', 'dynamical-systems', 'stability-theory']"
53,Hölder's theorem on gamma function,Hölder's theorem on gamma function,,"I was reading the proof of Hölder's theorem on wikipedia that the gamma function $\Gamma(z)$ does not satisfy any differential equation of form $P(z;\Gamma(z),\Gamma'(z),...,\Gamma^{(n)}(z))=0$ , where $P(X;Y_0,Y_1,...,Y_n)$ is a polynomial (I don't know why there is a semicolon, but see the last paragraph). One of the key idea is to define a total order on monomials, and therefore we may talk about the highest term or ""degree"" of a polynomial. However the article is kind of vague about that. According to this there are several orders on monomials. Which of them is being used here? More specifically, in the wiki article the following example is given: $\deg \left(-3 X^{10} Y_{0}^{2} Y_{1}^{4} + i X^{2}Y_{2} \right) < \deg \left( 2 X Y_{0}^{3} - Y_{1}^{4} \right)$ Below is part of the proof. Furthermore, if $X^{h} Y_{0}^{h_{0}} Y_{1}^{h_{1}} \cdots Y_{n}^{h_{n}}$ is the highest-degree monomial term in $P$ , then the highest-degree monomial term in $Q\stackrel{\text{df}}{=} ~ P(X + 1;X Y_{0},X Y_{1} + Y_{0},X Y_{2} + 2 Y_{1},\ldots,X Y_{n} + n Y_{n - 1})$ is $X^{h + h_{0} + h_{1} + \cdots + h_{n}}Y_{0}^{h_{0}}Y_{1}^{h_{1}} \cdots Y_{n}^{h_{n}}$ . Consequently, the polynomial $Q - X^{h_{0} + h_{1} + \cdots + h_{n}} P$ has a smaller overall degree than $P$ . I think for the argument to work we should use the lexicographic order $Y_n<Y_n-1<...<Y_0<X$ , i.e., given two monomials first compare the order of $Y_n$ , and then $Y_{n-1}$ , and finally $X$ . In this case the example given above should be false. Am I correct? Wikipedia gives two references, A Survey of Transcendentally Transcendental Functions by Lee A. Rubel and Irresistible Integrals by George Boros and Victor Moll. Both of their proofs are quite similar to wikipedia article, but the order they use seems to be more mysterious: they view $X$ as coefficient rather than indeterminate and the order does not involve $X$ , so the ""highest degree term"" is something like $q(X)Y_{0}^{h_{0}}Y_{1}^{h_{1}} \cdots Y_{n}^{h_{n}}$ . Then they use some ""Euclidean algorithm"", which I do not understand how to work for multivariate polynomials. Should the monomial order involve $X$ or not ?","I was reading the proof of Hölder's theorem on wikipedia that the gamma function does not satisfy any differential equation of form , where is a polynomial (I don't know why there is a semicolon, but see the last paragraph). One of the key idea is to define a total order on monomials, and therefore we may talk about the highest term or ""degree"" of a polynomial. However the article is kind of vague about that. According to this there are several orders on monomials. Which of them is being used here? More specifically, in the wiki article the following example is given: Below is part of the proof. Furthermore, if is the highest-degree monomial term in , then the highest-degree monomial term in is . Consequently, the polynomial has a smaller overall degree than . I think for the argument to work we should use the lexicographic order , i.e., given two monomials first compare the order of , and then , and finally . In this case the example given above should be false. Am I correct? Wikipedia gives two references, A Survey of Transcendentally Transcendental Functions by Lee A. Rubel and Irresistible Integrals by George Boros and Victor Moll. Both of their proofs are quite similar to wikipedia article, but the order they use seems to be more mysterious: they view as coefficient rather than indeterminate and the order does not involve , so the ""highest degree term"" is something like . Then they use some ""Euclidean algorithm"", which I do not understand how to work for multivariate polynomials. Should the monomial order involve or not ?","\Gamma(z) P(z;\Gamma(z),\Gamma'(z),...,\Gamma^{(n)}(z))=0 P(X;Y_0,Y_1,...,Y_n) \deg \left(-3 X^{10} Y_{0}^{2} Y_{1}^{4} + i X^{2}Y_{2} \right) < \deg \left( 2 X Y_{0}^{3} - Y_{1}^{4} \right) X^{h} Y_{0}^{h_{0}} Y_{1}^{h_{1}} \cdots Y_{n}^{h_{n}} P Q\stackrel{\text{df}}{=} ~ P(X + 1;X Y_{0},X Y_{1} + Y_{0},X Y_{2} + 2 Y_{1},\ldots,X Y_{n} + n Y_{n - 1}) X^{h + h_{0} + h_{1} + \cdots + h_{n}}Y_{0}^{h_{0}}Y_{1}^{h_{1}} \cdots Y_{n}^{h_{n}} Q - X^{h_{0} + h_{1} + \cdots + h_{n}} P P Y_n<Y_n-1<...<Y_0<X Y_n Y_{n-1} X X X q(X)Y_{0}^{h_{0}}Y_{1}^{h_{1}} \cdots Y_{n}^{h_{n}} X","['calculus', 'abstract-algebra', 'ordinary-differential-equations', 'polynomials', 'gamma-function']"
54,Homogeneous Linear Differential Equation with Constant Coefficientes,Homogeneous Linear Differential Equation with Constant Coefficientes,,"Consider the differential equation $$x^{(n)} + c_{n-1} x^{(n-1)} + \dots + c_1 x' + c_0 = 0,$$ with $c_0, c_1, \dots, c_{n-1} \in \mathbb{R}$ . Show that $x(t) = t^ke^{\beta t}$ is a solution if and only if $\beta$ is a root of $z^n + c_{n-1}z^{n-1} + \dots + c_1z + c_0 = 0$ with multiplicity greater than $k$ . Attempt: I tried to solve this exercise in several ways, but they all led me to calculate $$(t^k e^{\beta t})^{(n)} + c_{n-1} (t^ke^{\beta t})^{(n-1)} + \dots + c_1 (t^ke^{\beta t})' + c_0.$$ After a few tries, I got that $$(t^k e^{\beta t})^{(i)} = \sum_{j=0}^{i} i! \left( \begin{array}{c} k \\ k-j \end{array} \right) t^{k-j} \beta^{i-j} e^{\beta t}. $$ My initial idea was to use induction over $n$ . The case $n = 1$ is quite easy, but it is too tedious to work with this sum through the rest of the induction. My professor suggested that I use Jordan Canonical Form, but I am having a hard time to understand how to use it in order to prove the equivalence. Any help would be appreciated!","Consider the differential equation with . Show that is a solution if and only if is a root of with multiplicity greater than . Attempt: I tried to solve this exercise in several ways, but they all led me to calculate After a few tries, I got that My initial idea was to use induction over . The case is quite easy, but it is too tedious to work with this sum through the rest of the induction. My professor suggested that I use Jordan Canonical Form, but I am having a hard time to understand how to use it in order to prove the equivalence. Any help would be appreciated!","x^{(n)} + c_{n-1} x^{(n-1)} + \dots + c_1 x' + c_0 = 0, c_0, c_1, \dots, c_{n-1} \in \mathbb{R} x(t) = t^ke^{\beta t} \beta z^n + c_{n-1}z^{n-1} + \dots + c_1z + c_0 = 0 k (t^k e^{\beta t})^{(n)} + c_{n-1} (t^ke^{\beta t})^{(n-1)} + \dots + c_1 (t^ke^{\beta t})' + c_0. (t^k e^{\beta t})^{(i)} = \sum_{j=0}^{i} i! \left( \begin{array}{c} k \\ k-j \end{array} \right) t^{k-j} \beta^{i-j} e^{\beta t}.  n n = 1",['ordinary-differential-equations']
55,Verifying a solution for the differential equation $(D-m)^3y=0$,Verifying a solution for the differential equation,(D-m)^3y=0,"Verify that $y=c_1e^{mx}+c_2xe^{mx}+c_3x^2e^{mx}$ satisfies the differential equation $(D-m)^3y=0$ . $(D= {d\over dx})$ I solved this problem, but our professor said that it was wrong because my method is not general and it is for a specific case, but I failed to understand what is wrong here, can someone please explain why? Here is my professor's explanation: ""You need show that in general. This method is not general. It is for a specific case"" Here is what I did: I substituted given $y$ into the differential equation, then proved it is equal to zero. Consider, $$(D-m)^3(c_1e^{mx}+c_2xe^{mx}+c_3x^2e^{mx})=(D-m)^3c_1e^{mx}+(D-m)^3xc_2e^{mx}+(D-m)^3x^2c_3e^{mx}$$ then I used the fact that $F(D) e^{ax}V(x) =  e^{ax}F(D+a)V(x)$ : $$(D-m)^3(c_1e^{mx}+c_2xe^{mx}+c_3x^2e^{mx})=e^{mx}(D)^31+e^{mx}(D)^3x+e^{mx}(D)^3x^2$$ $$e^{mx}(D)^3+e^{mx}(D)^3x+e^{mx}(D)^3x^2  = e^{mx}(D^31)+e^{mx}(D^3x)+e^{mx}(D^3x^2)=0$$ Thus: $$(D-m)^3(c_1e^{mx}+c_2xe^{mx}+c_3x^2e^{mx})=0$$ Then I said that $y=c_1e^{mx}+c_2xe^{mx}+c_3x^2e^{mx}$ satisfies the differential equation $(D-m)^3y=0$ . Please explain, is it incorrect? Thank you!","Verify that satisfies the differential equation . I solved this problem, but our professor said that it was wrong because my method is not general and it is for a specific case, but I failed to understand what is wrong here, can someone please explain why? Here is my professor's explanation: ""You need show that in general. This method is not general. It is for a specific case"" Here is what I did: I substituted given into the differential equation, then proved it is equal to zero. Consider, then I used the fact that : Thus: Then I said that satisfies the differential equation . Please explain, is it incorrect? Thank you!",y=c_1e^{mx}+c_2xe^{mx}+c_3x^2e^{mx} (D-m)^3y=0 (D= {d\over dx}) y (D-m)^3(c_1e^{mx}+c_2xe^{mx}+c_3x^2e^{mx})=(D-m)^3c_1e^{mx}+(D-m)^3xc_2e^{mx}+(D-m)^3x^2c_3e^{mx} F(D) e^{ax}V(x) =  e^{ax}F(D+a)V(x) (D-m)^3(c_1e^{mx}+c_2xe^{mx}+c_3x^2e^{mx})=e^{mx}(D)^31+e^{mx}(D)^3x+e^{mx}(D)^3x^2 e^{mx}(D)^3+e^{mx}(D)^3x+e^{mx}(D)^3x^2  = e^{mx}(D^31)+e^{mx}(D^3x)+e^{mx}(D^3x^2)=0 (D-m)^3(c_1e^{mx}+c_2xe^{mx}+c_3x^2e^{mx})=0 y=c_1e^{mx}+c_2xe^{mx}+c_3x^2e^{mx} (D-m)^3y=0,['ordinary-differential-equations']
56,solution of $f'(x)+f(-x)=e^{-x^2}$,solution of,f'(x)+f(-x)=e^{-x^2},"let $f$ be a function such that $f:\mathbb{R}\to \mathbb{R}$ , I want to determine all functions of class $C^1$ such that $f'(x)+f(-x)=e^{-x^2}$ for all $x\in \mathbb{R}$ , Now we have $f'(x)+f(-x)=e^{-x^2}$ this implies that $f'(x)=e^{-x^2}-f(-x)$ , since $f'$ is of class $C^1$ this means that $f$ is of class $c^2$ , That equation equivalent to : $f''(x)=-2x e^{-x^2}-f'(-x)$ implies to $f''(x)-f(x)=-2x e^{-x^2}-e^{x^2}$ , if I take now $x\mapsto \lambda\cos(x)+\mu\sin(x),\lambda,\mu\in\mathbb R$ as a solution without second term in order to get particular solution it would be complicated and given by error function which forbide me to get a general solution, I ask now if there is any simple way to solve that functional ?","let be a function such that , I want to determine all functions of class such that for all , Now we have this implies that , since is of class this means that is of class , That equation equivalent to : implies to , if I take now as a solution without second term in order to get particular solution it would be complicated and given by error function which forbide me to get a general solution, I ask now if there is any simple way to solve that functional ?","f f:\mathbb{R}\to \mathbb{R} C^1 f'(x)+f(-x)=e^{-x^2} x\in \mathbb{R} f'(x)+f(-x)=e^{-x^2} f'(x)=e^{-x^2}-f(-x) f' C^1 f c^2 f''(x)=-2x e^{-x^2}-f'(-x) f''(x)-f(x)=-2x e^{-x^2}-e^{x^2} x\mapsto \lambda\cos(x)+\mu\sin(x),\lambda,\mu\in\mathbb R","['ordinary-differential-equations', 'mathematical-physics', 'error-function']"
57,Extension of the Continuous Dependence Theorem (ODE),Extension of the Continuous Dependence Theorem (ODE),,"Exercise: Let $(M, d_M)$ be a metric space and $F: \mathbb{R}^{1+d} \times M \rightarrow~\mathbb{R}^{d}$ a limited function, lipschitz on the second variable and uniformly continuous on the following distance: $$d((t, x, \lambda), (s, y, \mu)) = |t-s| + ||x-y|| + d_M(\lambda, \mu),$$ i.e, given $\epsilon > 0$ , there exists $\delta >0$ such that $$||F(t,x, \mu) - F(s,y,\lambda)|| < \epsilon, \text{whenever} \hspace{0.15cm} d((t,x,\mu), (s,y,\lambda)) < \delta.$$ Fixe $(t_0, x_0) \in \mathbb{R}^{1 + d}$ and let $t \mapsto \gamma_\lambda(t)$ be the maximal solution of $x'~=~F(t,x,\lambda)$ with initial condition $\gamma_\lambda(t_0) = x_0$ . Show that $\gamma_\lambda(t)$ is continuous on the parameter $\lambda$ : given $t \in \mathbb{R}$ and $\epsilon > 0$ , there exists $\delta > 0$ such that $$||\gamma_\lambda (t) - \gamma_\mu (t) || < \epsilon, \text{whenever} \hspace{0.15cm} d_M (\lambda, \mu) < \delta.$$ Attempt: Consider $(t_0, x_0) \in \mathbb{R}^{1+d}$ and let $t \mapsto \gamma_{\lambda}(t)$ be the maximal solution of the diferential equation $x' = F(t,x, \mu)$ , with initial condition $\gamma_{\lambda}(t_0) = x_0$ . Let $t \geq t_0$ and $\epsilon > 0$ . The following proof is completely analogous for $t \leq t_0$ . Considering $\lambda, \mu$ in the metric space $(M, d_M)$ , due to Picard's Theorem, we have $$\gamma_{\lambda} (t) = x_0 + \displaystyle\int_{t_0}^t F(s, \gamma_{\lambda}(s), \lambda) ds \hspace{0.15cm} \text{and} \hspace{0.15cm} \gamma_{\mu} (t) = x_0 + \displaystyle\int_{t_0}^t F(s, \gamma_{\mu}(s), \mu) ds,$$ with $\gamma_{\mu}(t)$ solution of $x' = F(t,x, \mu)$ , with initial condition $\gamma_{\mu} (t_0) = x_0$ . Therefore, $ \begin{array}{ccl} || \gamma_{\lambda}(t) - \gamma_{\mu}(t)|| & = & || \displaystyle\int_{t_0}^t \left[ F(s, \gamma_{\lambda}(s), \lambda) - F(s, \gamma_{\mu}(s), \mu) \right] ds || \\ & \leq & \displaystyle\int_{t_0}^t || F(s, \gamma_{\lambda}(s), \lambda) - F(s, \gamma_{\mu}(s), \mu) || ds \\ & = & \displaystyle\int_{t_0}^t ||F(s, \gamma_{\lambda}(s), \lambda) - F(s, \gamma_{\mu} (s), \lambda) +  F(s, \gamma_{\mu} (s), \lambda) - F(s, \gamma_{\mu}(s), \mu) || ds.          \end{array}$ It follows that $ \begin{array}{ccl} || \gamma_{\lambda}(t) - \gamma_{\mu}(t)|| & \leq & \displaystyle\int_{t_0}^t ||F(s, \gamma_{\lambda}(s), \lambda) - F(s, \gamma_{\mu} (s), \lambda) || ds + \displaystyle\int_{t_0}^t ||F(s, \gamma_{\mu} (s), \lambda) - F(s, \gamma_{\mu}(s), \mu) || ds.\end{array}$ Note that, since $F$ is lipschit on the second variable, there exists $C > 0$ such that $$ || F(t, x_1, \mu) - F(t, x_2, \mu)|| < C ||x_1 - x_2||, \hspace{0.1cm} \forall \hspace{0.1cm} (t, x_1, \mu), (t, x_2, \mu) \in \mathbb{R}^{1+d} \times M. $$ We also know that $F$ is uniformly continuous on the distance $d$ , so given $\epsilon'$ , there exists $\delta > 0$ such that $$||F(t,x, \mu) - F(s, y, \lambda)|| < \epsilon',$$ whenever $d((t,x,\mu), (s,y,\lambda)) < \delta$ . For $t=s$ e $x=y$ , by the definition of the distance $d$ , we get that $$d_M(\lambda, \mu) < \delta \Rightarrow ||F(t,x,\mu) - F(t,x,\lambda)|| < \epsilon'.$$ From the information above mentioned, whenever $d_M(\lambda, \mu) < \delta$ ,  we have $ \begin{array}{ccl} || \gamma_{\lambda}(t) - \gamma_{\mu}(t)|| & \leq & \displaystyle\int_{t_0}^t ||F(s, \gamma_{\lambda}(s), \lambda) - F(s, \gamma_{\mu} (s), \lambda) || ds +  \displaystyle\int_{t_0}^t ||F(s, \gamma_{\mu} (s), \lambda) - F(s, \gamma_{\mu}(s), \mu) || ds \\ & < & \displaystyle\int_{t_0}^t C || \gamma_{\lambda} (s) - \gamma_{\mu}(s) || ds + \displaystyle\int_{t_0}^t \epsilon' ds \\ & = & \displaystyle\int_{t_0}^t C ||\gamma_{\lambda}(s) - \gamma_{\mu}(s)|| ds + \epsilon' (t-t_0). \end{array}$ Since $t \geq t_0$ , the function $\epsilon' (t-t_0)$ is continuous and not decreasing. Therefore, we can apply Gronwall's Lemma for the function $||\gamma_{\lambda}(t) - \gamma_{\mu}(t)||$ . So, $$ || \gamma_{\lambda}(t) - \gamma_{\mu}(t)|| <  \epsilon' (t-t_0) e^{\displaystyle\int_{t_0}^t C ds}  =  \epsilon' (t-t_0)e^{C(t-t_0)}.$$ How do I take a proper $\epsilon'$ in order to make $||\gamma_{\lambda}(t) - \gamma_{\mu}(t)|| < \epsilon$ ? Does it somehow follow from the hypotesis that F is limited? I didn't use it so far. Any help would be appreciated!","Exercise: Let be a metric space and a limited function, lipschitz on the second variable and uniformly continuous on the following distance: i.e, given , there exists such that Fixe and let be the maximal solution of with initial condition . Show that is continuous on the parameter : given and , there exists such that Attempt: Consider and let be the maximal solution of the diferential equation , with initial condition . Let and . The following proof is completely analogous for . Considering in the metric space , due to Picard's Theorem, we have with solution of , with initial condition . Therefore, It follows that Note that, since is lipschit on the second variable, there exists such that We also know that is uniformly continuous on the distance , so given , there exists such that whenever . For e , by the definition of the distance , we get that From the information above mentioned, whenever ,  we have Since , the function is continuous and not decreasing. Therefore, we can apply Gronwall's Lemma for the function . So, How do I take a proper in order to make ? Does it somehow follow from the hypotesis that F is limited? I didn't use it so far. Any help would be appreciated!","(M, d_M) F: \mathbb{R}^{1+d} \times M \rightarrow~\mathbb{R}^{d} d((t, x, \lambda), (s, y, \mu)) = |t-s| + ||x-y|| + d_M(\lambda, \mu), \epsilon > 0 \delta >0 ||F(t,x, \mu) - F(s,y,\lambda)|| < \epsilon, \text{whenever} \hspace{0.15cm} d((t,x,\mu), (s,y,\lambda)) < \delta. (t_0, x_0) \in \mathbb{R}^{1 + d} t \mapsto \gamma_\lambda(t) x'~=~F(t,x,\lambda) \gamma_\lambda(t_0) = x_0 \gamma_\lambda(t) \lambda t \in \mathbb{R} \epsilon > 0 \delta > 0 ||\gamma_\lambda (t) - \gamma_\mu (t) || < \epsilon, \text{whenever} \hspace{0.15cm} d_M (\lambda, \mu) < \delta. (t_0, x_0) \in \mathbb{R}^{1+d} t \mapsto \gamma_{\lambda}(t) x' = F(t,x, \mu) \gamma_{\lambda}(t_0) = x_0 t \geq t_0 \epsilon > 0 t \leq t_0 \lambda, \mu (M, d_M) \gamma_{\lambda} (t) = x_0 + \displaystyle\int_{t_0}^t F(s, \gamma_{\lambda}(s), \lambda) ds \hspace{0.15cm} \text{and} \hspace{0.15cm} \gamma_{\mu} (t) = x_0 + \displaystyle\int_{t_0}^t F(s, \gamma_{\mu}(s), \mu) ds, \gamma_{\mu}(t) x' = F(t,x, \mu) \gamma_{\mu} (t_0) = x_0  \begin{array}{ccl} || \gamma_{\lambda}(t) - \gamma_{\mu}(t)|| & = & || \displaystyle\int_{t_0}^t \left[ F(s, \gamma_{\lambda}(s), \lambda) - F(s, \gamma_{\mu}(s), \mu) \right] ds || \\ & \leq & \displaystyle\int_{t_0}^t || F(s, \gamma_{\lambda}(s), \lambda) - F(s, \gamma_{\mu}(s), \mu) || ds \\ & = & \displaystyle\int_{t_0}^t ||F(s, \gamma_{\lambda}(s), \lambda) - F(s, \gamma_{\mu} (s), \lambda) +  F(s, \gamma_{\mu} (s), \lambda) - F(s, \gamma_{\mu}(s), \mu) || ds.          \end{array}  \begin{array}{ccl} || \gamma_{\lambda}(t) - \gamma_{\mu}(t)|| & \leq & \displaystyle\int_{t_0}^t ||F(s, \gamma_{\lambda}(s), \lambda) - F(s, \gamma_{\mu} (s), \lambda) || ds + \displaystyle\int_{t_0}^t ||F(s, \gamma_{\mu} (s), \lambda) - F(s, \gamma_{\mu}(s), \mu) || ds.\end{array} F C > 0  || F(t, x_1, \mu) - F(t, x_2, \mu)|| < C ||x_1 - x_2||, \hspace{0.1cm} \forall \hspace{0.1cm} (t, x_1, \mu), (t, x_2, \mu) \in \mathbb{R}^{1+d} \times M.  F d \epsilon' \delta > 0 ||F(t,x, \mu) - F(s, y, \lambda)|| < \epsilon', d((t,x,\mu), (s,y,\lambda)) < \delta t=s x=y d d_M(\lambda, \mu) < \delta \Rightarrow ||F(t,x,\mu) - F(t,x,\lambda)|| < \epsilon'. d_M(\lambda, \mu) < \delta  \begin{array}{ccl} || \gamma_{\lambda}(t) - \gamma_{\mu}(t)|| & \leq & \displaystyle\int_{t_0}^t ||F(s, \gamma_{\lambda}(s), \lambda) - F(s, \gamma_{\mu} (s), \lambda) || ds +  \displaystyle\int_{t_0}^t ||F(s, \gamma_{\mu} (s), \lambda) - F(s, \gamma_{\mu}(s), \mu) || ds \\ & < & \displaystyle\int_{t_0}^t C || \gamma_{\lambda} (s) - \gamma_{\mu}(s) || ds + \displaystyle\int_{t_0}^t \epsilon' ds \\ & = & \displaystyle\int_{t_0}^t C ||\gamma_{\lambda}(s) - \gamma_{\mu}(s)|| ds + \epsilon' (t-t_0). \end{array} t \geq t_0 \epsilon' (t-t_0) ||\gamma_{\lambda}(t) - \gamma_{\mu}(t)||  || \gamma_{\lambda}(t) - \gamma_{\mu}(t)|| <  \epsilon' (t-t_0) e^{\displaystyle\int_{t_0}^t C ds}  =  \epsilon' (t-t_0)e^{C(t-t_0)}. \epsilon' ||\gamma_{\lambda}(t) - \gamma_{\mu}(t)|| < \epsilon","['ordinary-differential-equations', 'analysis', 'dynamical-systems']"
58,"Creating a SIIR (susceptible, infected, isolated, recovered) model using differential equations.","Creating a SIIR (susceptible, infected, isolated, recovered) model using differential equations.",,"I wasn't too sure of where to post this since it's a mix of physics (dynamical systems), medicine, and mathematics but here it goes. I am trying to model the current outbreak of Covid 19 using a more sophisticated model than the simple SIR model, and so I added two categories: Isolated Sick people to represent sick people who self isolate after presenting symptoms, and Isolated Healthy people to represent people who isolate once isolation is ordered. I set the following set of differential equations and I could appreciate criticism or ways of improving the model, or just confirmation that it should be able to model a pandemic like the one today. I name $S(t)$ the susceptible people, $I(t)$ the infected people (not in quarantine), $Q_s(t)$ people who are sick in quarantine, $Q_h(t)$ people who are healthy in quarantine, and $R(t)$ recovered people. $$ \frac{dS}{dt}=-\beta \frac{S}{N}I-\alpha(t)\frac{S}{N}S+\alpha'(t)\frac{S}{N}Q_h-\beta c\frac{Q_s}{N}S $$ $$ \frac{dI}{dt}=\beta \frac{S}{N}I+\beta c\frac{S}{N}Q_s+\beta c\frac{I}{N}Q_h-\lambda I-\gamma I -\alpha(t)\frac{S}{N}I $$ $$ \frac{dQ_s}{dt}=\lambda I-\gamma Q_s+\alpha(t)\frac{S}{N}I $$ $$ \frac{dQ_h}{dt}=\alpha(t)\frac{S}{N}S-\alpha'(t)\frac{S}{N}Q_h-\beta c\frac{I}{N}Q_h $$ $$ \frac{dR}{dt}=\gamma Q_s+\gamma I $$ $$ N = S(t)+I(t)+Q_s(t)+Q_h(t)+R(t) = constant $$ The equations use the following parameters. $\beta$ is the transmission rate of the disease. $\alpha(t)$ is a time-dependent parameter that indicates the rate at which people go into quarantine once a compulsory quarantine is in effect. You can think of it being a pulse shaped function of a given duration (quarantine duration) and a given amplitude. $\alpha'(t)$ follows the same idea but instead describes how people get out of the quarantine once it's over. $c$ is a percentage multiplier that describes how reduced the infection rate for quarantine people is. In an ideal quarantine, it would be $0$ . $\lambda$ is the rate at which people self isolate once they get infected. $\gamma$ is the rate of recovery from the disease. Finally, $N$ should be the total population and it should be a constant since all the time derivatives should add up to $0$ (unless I mistyped). I am currently solving it using Python and Scipy, and the main question I have apart from how relevant the coefficients like $\frac{S}{N}$ or $\frac{I}{N}$ are is how come isolation doesn't seem to affect much the amplitude of the peak of infections. Instead what happens is that it only pushes it back. Furthermore, sometimes it seems like isolation at the right moment (once the peak starts to form) can help a lot more than isolation too early there's barely any cases. Indeed, early isolation just pushes it back, but isolation at the right time seems to reduce the future peek since a lot of individuals are by that time already immune and the susceptible population is a lot smaller than just pure isolation when the susceptible population just bounces back to pre isolation levels and since the disease is just as infectious and it hasn't been eliminated, the peak happens just later in the future. Is this effect normal? Is this effect truly what happens in a real pandemic? And if so isn't the current isolation a bit counterproductive in the sense that we don't allow any herd immunity to happen? Thank you! EDIT: I have updated the equations following a comment and removed a squared dependence on S. So now: $$ \frac{dS}{dt}=-\beta \frac{S}{N}I-\alpha(t)S+\alpha'(t)\frac{S}{N}Q_h-\beta c\frac{Q_s}{N}S $$ $$ \frac{dQ_h}{dt}=\alpha(t)S-\alpha'(t)\frac{S}{N}Q_h-\beta c\frac{I}{N}Q_h $$ Also, I have been running some simulations (You can find the code on my Github ) that I'd like to discuss. Both correspond to a run with N=40 million people (the size of California), with $\beta=\frac{1}{24*4.375} \text{h}^{-1}$ (the $\beta$ was obtained doing a fit to the cumulative cases in California, and getting the time constant in days). $\lambda=\frac{1}{24*5}=\alpha_0$ (so people take around five days to fully comply with a quarantine. $c=0.5$ . The initial conditions are $I_0=1190$ cases $R_0=20$ , $Q_{h_0}=10$ . The first picture shows a quarantine that is pronounced at $t=20$ days, with a duration of 60 days, with 30 days at full swing, and 15 for rise at full level, and another 15 to come back to pre isolation levels. The first picture has a peak at 199 days, with a total number of infected at 24.1 million people. The second one gives a peak at day 86 with 24.2 million people infected. So as you can see the peak is moved back significantly but the amplitude of said peak stays about the same. Now what's interesting is if I launch the isolation when a little outbreak is happening. I will delay the full quarantine by 50 days. And leave the rest the same. Now the peak on the right is ""only"" at 15.5 million and it was still pushed back to day 243. It seems more manageable. Since probably around 30% of those cases might require hospitalization. Finally, I wonder if repeating a quarantine while the second outbreak happens will work like the first one further helping to reduce the total number of infected. I could model that by changing $\alpha(t)$ and $\alpha'(t)$ .","I wasn't too sure of where to post this since it's a mix of physics (dynamical systems), medicine, and mathematics but here it goes. I am trying to model the current outbreak of Covid 19 using a more sophisticated model than the simple SIR model, and so I added two categories: Isolated Sick people to represent sick people who self isolate after presenting symptoms, and Isolated Healthy people to represent people who isolate once isolation is ordered. I set the following set of differential equations and I could appreciate criticism or ways of improving the model, or just confirmation that it should be able to model a pandemic like the one today. I name the susceptible people, the infected people (not in quarantine), people who are sick in quarantine, people who are healthy in quarantine, and recovered people. The equations use the following parameters. is the transmission rate of the disease. is a time-dependent parameter that indicates the rate at which people go into quarantine once a compulsory quarantine is in effect. You can think of it being a pulse shaped function of a given duration (quarantine duration) and a given amplitude. follows the same idea but instead describes how people get out of the quarantine once it's over. is a percentage multiplier that describes how reduced the infection rate for quarantine people is. In an ideal quarantine, it would be . is the rate at which people self isolate once they get infected. is the rate of recovery from the disease. Finally, should be the total population and it should be a constant since all the time derivatives should add up to (unless I mistyped). I am currently solving it using Python and Scipy, and the main question I have apart from how relevant the coefficients like or are is how come isolation doesn't seem to affect much the amplitude of the peak of infections. Instead what happens is that it only pushes it back. Furthermore, sometimes it seems like isolation at the right moment (once the peak starts to form) can help a lot more than isolation too early there's barely any cases. Indeed, early isolation just pushes it back, but isolation at the right time seems to reduce the future peek since a lot of individuals are by that time already immune and the susceptible population is a lot smaller than just pure isolation when the susceptible population just bounces back to pre isolation levels and since the disease is just as infectious and it hasn't been eliminated, the peak happens just later in the future. Is this effect normal? Is this effect truly what happens in a real pandemic? And if so isn't the current isolation a bit counterproductive in the sense that we don't allow any herd immunity to happen? Thank you! EDIT: I have updated the equations following a comment and removed a squared dependence on S. So now: Also, I have been running some simulations (You can find the code on my Github ) that I'd like to discuss. Both correspond to a run with N=40 million people (the size of California), with (the was obtained doing a fit to the cumulative cases in California, and getting the time constant in days). (so people take around five days to fully comply with a quarantine. . The initial conditions are cases , . The first picture shows a quarantine that is pronounced at days, with a duration of 60 days, with 30 days at full swing, and 15 for rise at full level, and another 15 to come back to pre isolation levels. The first picture has a peak at 199 days, with a total number of infected at 24.1 million people. The second one gives a peak at day 86 with 24.2 million people infected. So as you can see the peak is moved back significantly but the amplitude of said peak stays about the same. Now what's interesting is if I launch the isolation when a little outbreak is happening. I will delay the full quarantine by 50 days. And leave the rest the same. Now the peak on the right is ""only"" at 15.5 million and it was still pushed back to day 243. It seems more manageable. Since probably around 30% of those cases might require hospitalization. Finally, I wonder if repeating a quarantine while the second outbreak happens will work like the first one further helping to reduce the total number of infected. I could model that by changing and .","S(t) I(t) Q_s(t) Q_h(t) R(t) 
\frac{dS}{dt}=-\beta \frac{S}{N}I-\alpha(t)\frac{S}{N}S+\alpha'(t)\frac{S}{N}Q_h-\beta c\frac{Q_s}{N}S
 
\frac{dI}{dt}=\beta \frac{S}{N}I+\beta c\frac{S}{N}Q_s+\beta c\frac{I}{N}Q_h-\lambda I-\gamma I -\alpha(t)\frac{S}{N}I
 
\frac{dQ_s}{dt}=\lambda I-\gamma Q_s+\alpha(t)\frac{S}{N}I
 
\frac{dQ_h}{dt}=\alpha(t)\frac{S}{N}S-\alpha'(t)\frac{S}{N}Q_h-\beta c\frac{I}{N}Q_h
 
\frac{dR}{dt}=\gamma Q_s+\gamma I
 
N = S(t)+I(t)+Q_s(t)+Q_h(t)+R(t) = constant
 \beta \alpha(t) \alpha'(t) c 0 \lambda \gamma N 0 \frac{S}{N} \frac{I}{N} 
\frac{dS}{dt}=-\beta \frac{S}{N}I-\alpha(t)S+\alpha'(t)\frac{S}{N}Q_h-\beta c\frac{Q_s}{N}S
 
\frac{dQ_h}{dt}=\alpha(t)S-\alpha'(t)\frac{S}{N}Q_h-\beta c\frac{I}{N}Q_h
 \beta=\frac{1}{24*4.375} \text{h}^{-1} \beta \lambda=\frac{1}{24*5}=\alpha_0 c=0.5 I_0=1190 R_0=20 Q_{h_0}=10 t=20 \alpha(t) \alpha'(t)","['ordinary-differential-equations', 'dynamical-systems', 'nonlinear-dynamics']"
59,Non-linear ODE. Almost Abel Equation of first kind,Non-linear ODE. Almost Abel Equation of first kind,,"I'm trying to solve the following ODE: $$f'(x)=-f(x)^2+P(x)f(x)^3+Q(x)f(x)^4$$ where the functions $P(x)$ and $Q(x)$ are known. This nearly resembles Abel Equation of first kind. The power in $f$ 's is of 1 order too high. Maybe this equation has a 'canonical form' like in Abel equation of the first kind . Any hints on solving this? Maybe someone sees a way to reduce it to one of the forms in List of nonlinear ODE EDIT: Equivalently, for $f(x)=\frac{1}{g(x)}$ : $$g'(x)=\frac{Q(x)}{g(x)^2}-\frac{P(x)}{g(x)}+1$$","I'm trying to solve the following ODE: where the functions and are known. This nearly resembles Abel Equation of first kind. The power in 's is of 1 order too high. Maybe this equation has a 'canonical form' like in Abel equation of the first kind . Any hints on solving this? Maybe someone sees a way to reduce it to one of the forms in List of nonlinear ODE EDIT: Equivalently, for :",f'(x)=-f(x)^2+P(x)f(x)^3+Q(x)f(x)^4 P(x) Q(x) f f(x)=\frac{1}{g(x)} g'(x)=\frac{Q(x)}{g(x)^2}-\frac{P(x)}{g(x)}+1,['ordinary-differential-equations']
60,Is there a general stability result for the linear delay differential equation $x'(t)=Ax(t)+Bx(t-\tau)$?,Is there a general stability result for the linear delay differential equation ?,x'(t)=Ax(t)+Bx(t-\tau),"Is there a general stability result for the linear delay differential equation $x'(t)=Ax(t)+Bx(t-\tau)$ where $A$ and $B$ are $m\times m$ matrices?  If not, is there a current summary of the known stability conditions?  As of 2007, I believe the answer was no.  Matsunaga had a nice summary in a 2007 paper titled, ""Exact stability criteria for delay differential and difference equations"" that specified: I was unable to locate a more recent survey and didn't know if better criteria had been found since that publication.","Is there a general stability result for the linear delay differential equation where and are matrices?  If not, is there a current summary of the known stability conditions?  As of 2007, I believe the answer was no.  Matsunaga had a nice summary in a 2007 paper titled, ""Exact stability criteria for delay differential and difference equations"" that specified: I was unable to locate a more recent survey and didn't know if better criteria had been found since that publication.",x'(t)=Ax(t)+Bx(t-\tau) A B m\times m,"['ordinary-differential-equations', 'stability-in-odes', 'stability-theory', 'delay-differential-equations']"
61,What is Gian-Carlo-Rota really saying about DE courses? (For students like me),What is Gian-Carlo-Rota really saying about DE courses? (For students like me),,"I am a bit confused (and entertained) after reading TEN LESSONS I WISH I HAD LEARNED BEFORE I STARTED TEACHING DIFFERENTIAL EQUATIONS by Gian Carlo Rota For example, he writes The most preposterous items   are found at the beginning, when the text (any text) will list a number of disconnected tricks that   are passed off as useful, such as exact equations, integrating factors, homogeneous differential   equations, and similarly preposterous techniques. Since it is rare – to put it gently – to find a   differential equation of this kind ever occurring in engineering practice, the exercises provided   along with these topics are of limited scope: as a matter of fact, the same sets of exercises have   been coming down the pike with little change since Euler He goes on to mock many other so called ""tricks"" that he describes as jokes. As a student, these tricks are all I have. Can someone explain what he means. Is it that, in the real world (whatever that is), the equations are never suitable for these bags of tricks, and therefore we should be learning about numerical methods or analyzing phase portraits, or high performance computing? I must admit, I am struggling in general to understand how people create differential equations out in the wild. How does one transition from bags of tricks to understanding DE's out in the wild so to speak?","I am a bit confused (and entertained) after reading TEN LESSONS I WISH I HAD LEARNED BEFORE I STARTED TEACHING DIFFERENTIAL EQUATIONS by Gian Carlo Rota For example, he writes The most preposterous items   are found at the beginning, when the text (any text) will list a number of disconnected tricks that   are passed off as useful, such as exact equations, integrating factors, homogeneous differential   equations, and similarly preposterous techniques. Since it is rare – to put it gently – to find a   differential equation of this kind ever occurring in engineering practice, the exercises provided   along with these topics are of limited scope: as a matter of fact, the same sets of exercises have   been coming down the pike with little change since Euler He goes on to mock many other so called ""tricks"" that he describes as jokes. As a student, these tricks are all I have. Can someone explain what he means. Is it that, in the real world (whatever that is), the equations are never suitable for these bags of tricks, and therefore we should be learning about numerical methods or analyzing phase portraits, or high performance computing? I must admit, I am struggling in general to understand how people create differential equations out in the wild. How does one transition from bags of tricks to understanding DE's out in the wild so to speak?",,['ordinary-differential-equations']
62,Solving ODE - Space curve Frame,Solving ODE - Space curve Frame,,"I'm trying to calculate the parallel frame $\{T, U, V\}$ of a space curve $\alpha : I \mapsto \mathbb{R}^3$ . It's similar to Frenet frame, except we have instead the projection of torsion $\tau$ on $U$ : $\tau_U = 0$ It's described by the three equations, $$T' = \kappa_U U + \kappa_V V \quad \quad \quad \quad \ \ (1)$$ $$U' = -\kappa_U T = -\langle \kappa,U \rangle \ T \quad \quad (2)$$ $$V' = -\kappa_V T = -\langle \kappa,V \rangle \ T \quad \quad (3)$$ where $\kappa = T'$ is the curvature vector and $\kappa_U, \kappa_V$ are its components. I know $T$ , $\kappa$ and the initial conditions $\{T(0), U(0), V(0)\}$ at $\alpha(0)$ but I don't know how to solve the ODE $(2)$ and $(3)$ in order to get $U$ and $V$ . Any suggestion or a note on what to search for would be great!","I'm trying to calculate the parallel frame of a space curve . It's similar to Frenet frame, except we have instead the projection of torsion on : It's described by the three equations, where is the curvature vector and are its components. I know , and the initial conditions at but I don't know how to solve the ODE and in order to get and . Any suggestion or a note on what to search for would be great!","\{T, U, V\} \alpha : I \mapsto \mathbb{R}^3 \tau U \tau_U = 0 T' = \kappa_U U + \kappa_V V \quad \quad \quad \quad \ \ (1) U' = -\kappa_U T = -\langle \kappa,U \rangle \ T \quad \quad (2) V' = -\kappa_V T = -\langle \kappa,V \rangle \ T \quad \quad (3) \kappa = T' \kappa_U, \kappa_V T \kappa \{T(0), U(0), V(0)\} \alpha(0) (2) (3) U V","['ordinary-differential-equations', 'differential-geometry']"
63,How to obtain recursion relations from this,How to obtain recursion relations from this,,"I'm trying to solve a problem using  the power series solution. Finally (and after substitution of differentations) I have come up with $$ -\frac1{2\mu}\sum_{i=2}^p i(i-1)a_i r^{(i-1)}+\frac1{2\mu}\omega \sum_{i=1}^p i\,a_i \,r^{(i+1)}-\frac{(l+1)}{\mu}\sum_{i=1}^p i\,a_i\,r^{(i-1)}-\frac{(4\mu^2-1)}{8\mu}\omega^2\sum_{i=0}^p a_i\,r^{(i+3)}+\frac{(2l+3)}{4\mu}\omega\sum_{i=0}^p a_i\,r^{i+1}-\sum_{i=0}^p a_i\,r^{i}=0 $$ According the power series solutions method now it's time to equalize the summation limits and powers of $r$ , but here the powers include a variable called $l$ . For this reason I'm confused how to proceed this calculation and get recursion relations? Addendum: As a user said there is no need to include $l$ 's in the summations. So I divided out them. Now the problem is equalizing the powers and limits.","I'm trying to solve a problem using  the power series solution. Finally (and after substitution of differentations) I have come up with According the power series solutions method now it's time to equalize the summation limits and powers of , but here the powers include a variable called . For this reason I'm confused how to proceed this calculation and get recursion relations? Addendum: As a user said there is no need to include 's in the summations. So I divided out them. Now the problem is equalizing the powers and limits.","
-\frac1{2\mu}\sum_{i=2}^p i(i-1)a_i r^{(i-1)}+\frac1{2\mu}\omega \sum_{i=1}^p i\,a_i \,r^{(i+1)}-\frac{(l+1)}{\mu}\sum_{i=1}^p i\,a_i\,r^{(i-1)}-\frac{(4\mu^2-1)}{8\mu}\omega^2\sum_{i=0}^p a_i\,r^{(i+3)}+\frac{(2l+3)}{4\mu}\omega\sum_{i=0}^p a_i\,r^{i+1}-\sum_{i=0}^p a_i\,r^{i}=0
 r l l","['ordinary-differential-equations', 'recurrence-relations']"
64,"Efficient method to find $H$ given by $H(x)=\int_0^x f(x-u) f(x-au) e^u \, du$",Efficient method to find  given by,"H H(x)=\int_0^x f(x-u) f(x-au) e^u \, du","Question Let $f:[0,\infty) \rightarrow [0,\infty)$ be some continuously differentiable function and $a \in (0,1)$ then we define the function $H:[0,\infty) \rightarrow [0,\infty)$ by letting: $$H(x)=\int_0^x f(x-u) f(x-au) e^{-u} \, du.$$ Suppose we would like to compute $H$ numerically, then one way to do this would be to simply compute the integral in the right hand for each value of $x$ . I am however looking for a more efficient method as is possible for the special case $a=0$ . Special case $a=0$ In this case we define $K(x) = \int_0^x f(x-u) e^{-u} \, du$ and we see that $H(x)=f(x) K(x)$ . We now show that there is a simple method to compute K(x). We find by a simple change of variables $v=x-u$ that: $$ K(x)=\int_0^x f(v) e^{v-x} \, dv =  \int_0^x f(v) e^{v}\, dv \cdot e^{-x}. $$ Applying the product rule we obtain $K'(x)=f(x) e^x e^{-x} - \int_0^x f(v)\, e^{v} \, dv\, e^{-x}$ applying the definition of $H$ and some rewriting we obtain: $$ K'(x)=f(x)-K(x). $$ This is a differential equation which can be solved much more quickly and $H(x)$ is easily obtained from $K(x)$ .","Question Let be some continuously differentiable function and then we define the function by letting: Suppose we would like to compute numerically, then one way to do this would be to simply compute the integral in the right hand for each value of . I am however looking for a more efficient method as is possible for the special case . Special case In this case we define and we see that . We now show that there is a simple method to compute K(x). We find by a simple change of variables that: Applying the product rule we obtain applying the definition of and some rewriting we obtain: This is a differential equation which can be solved much more quickly and is easily obtained from .","f:[0,\infty) \rightarrow [0,\infty) a \in (0,1) H:[0,\infty) \rightarrow [0,\infty) H(x)=\int_0^x f(x-u) f(x-au) e^{-u} \, du. H x a=0 a=0 K(x) = \int_0^x f(x-u) e^{-u} \, du H(x)=f(x) K(x) v=x-u 
K(x)=\int_0^x f(v) e^{v-x} \, dv =  \int_0^x f(v) e^{v}\, dv \cdot e^{-x}.
 K'(x)=f(x) e^x e^{-x} - \int_0^x f(v)\, e^{v} \, dv\, e^{-x} H 
K'(x)=f(x)-K(x).
 H(x) K(x)","['real-analysis', 'calculus', 'ordinary-differential-equations', 'numerical-methods', 'integral-equations']"
65,error sensitivity analysis of Runge - Kutta method,error sensitivity analysis of Runge - Kutta method,,"In Runge - Kutta - Fehlberg methods, sometimes and in some cases the answer depends on the method we define the error and also on the magnitude of the error. In the case I am working on, there are several ""zero crossings""; which is problematic for 'scaling' in the definition of precision. Besides, the solution depends on the steplength; surprisingly, not its precision, but its behavior! So, the numerical method is not reliable at all. How can I define error and control the sensitivity of the numerical method to be reliable? I saw in the literature some different methods for defining the error and controlling the steplength. Kash and Carp happens to be the most famous. But the problem is the way all these methods 'scale' the precision. I need the step correction method to be 'robust' enough. The most weird case is that in some time-steps the error (i.e., the difference between the 4th order estimation and the 5th order solution) becomes zero! The zero error causes the corrected steplength to become Inf! How can I write the steplength correction to overcome this problem? Also, if anyone knows any useful reference please announce me. Thanks in advance","In Runge - Kutta - Fehlberg methods, sometimes and in some cases the answer depends on the method we define the error and also on the magnitude of the error. In the case I am working on, there are several ""zero crossings""; which is problematic for 'scaling' in the definition of precision. Besides, the solution depends on the steplength; surprisingly, not its precision, but its behavior! So, the numerical method is not reliable at all. How can I define error and control the sensitivity of the numerical method to be reliable? I saw in the literature some different methods for defining the error and controlling the steplength. Kash and Carp happens to be the most famous. But the problem is the way all these methods 'scale' the precision. I need the step correction method to be 'robust' enough. The most weird case is that in some time-steps the error (i.e., the difference between the 4th order estimation and the 5th order solution) becomes zero! The zero error causes the corrected steplength to become Inf! How can I write the steplength correction to overcome this problem? Also, if anyone knows any useful reference please announce me. Thanks in advance",,"['ordinary-differential-equations', 'numerical-methods', 'runge-kutta-methods']"
66,Closed-form solution of a linear time-varying system,Closed-form solution of a linear time-varying system,,"Consider the following three-dimensional linear time-varying system $$\label{eq1}\tag{$\star$} \dot{x}(t)=(A \cos(\omega_1 t) + B \cos(\omega_2 t) )x(t), \ \ x(0)\in\mathbb{R}^{3}, $$ where $\omega_1$ and $\omega_2$ are positive real numbers such that $\omega_1\ne \omega_2$ and $$ A=\begin{bmatrix}a & -a & 0 \\ -a & a & 0 \\ 0 & 0 & 0\end{bmatrix}, \quad B=\begin{bmatrix}0 & 0 & 0 \\ 0 & -b & b \\ 0 & b & -b\end{bmatrix} $$ with $a,b$ positive real numbers. Notice that $A$, $B$ do not commute. However, the commutator $$ [A,B]=AB-BA = \begin{bmatrix}0 & ab & -ab \\ -ab & 0 & ab \\ ab & -ab & 0\end{bmatrix} $$ is a skew-symmetric matrix (I don't know if this can be useful though). My questions are: Does there exist an explicit closed-form expression for the solution of \eqref{eq1}? If not, is it possible to find a bound on $\|x(t)\|$ which explicitly depends on $\omega_1$ and $\omega_2$? I've been stuck on this problem for a while now. So I would greatly appreciate any kind of comment or help. Thanks.","Consider the following three-dimensional linear time-varying system $$\label{eq1}\tag{$\star$} \dot{x}(t)=(A \cos(\omega_1 t) + B \cos(\omega_2 t) )x(t), \ \ x(0)\in\mathbb{R}^{3}, $$ where $\omega_1$ and $\omega_2$ are positive real numbers such that $\omega_1\ne \omega_2$ and $$ A=\begin{bmatrix}a & -a & 0 \\ -a & a & 0 \\ 0 & 0 & 0\end{bmatrix}, \quad B=\begin{bmatrix}0 & 0 & 0 \\ 0 & -b & b \\ 0 & b & -b\end{bmatrix} $$ with $a,b$ positive real numbers. Notice that $A$, $B$ do not commute. However, the commutator $$ [A,B]=AB-BA = \begin{bmatrix}0 & ab & -ab \\ -ab & 0 & ab \\ ab & -ab & 0\end{bmatrix} $$ is a skew-symmetric matrix (I don't know if this can be useful though). My questions are: Does there exist an explicit closed-form expression for the solution of \eqref{eq1}? If not, is it possible to find a bound on $\|x(t)\|$ which explicitly depends on $\omega_1$ and $\omega_2$? I've been stuck on this problem for a while now. So I would greatly appreciate any kind of comment or help. Thanks.",,"['ordinary-differential-equations', 'analysis', 'dynamical-systems', 'closed-form', 'control-theory']"
67,System of ODEs: does one solution dominates the other?,System of ODEs: does one solution dominates the other?,,"I am interested in the following non-linear system \begin{align*} \ddot{x}&=(\dot{x})^2+g(x-y+\nu)\\ \ddot{y}&=(\dot{y})^2+g(y-x)~~,\\ \end{align*} where $g$ is a monotonic, increasing, and positive function; and $\nu>0$ is a constant. We can assume $g$ is smoother if it helps. Initial conditions are $\dot{x}(0)=x(0)=\dot{y}(0)=y(0)=0$. Both $x(t)$ and $y(t)$ are functions from the positive reals to the positive reals. I think it is not too hard to prove they are growing functions as well. What I expect is that the answer to the question I pose in the title is ""yes"". That is, the initial extra ""kick"" on $x(t)$ will keep it above $y(t)$ at all times. Maybe $x(t)$ will blow-up before $y(t)$ (I am almost sure they both blow-up in finite time, at least when g is bounded). In fact, I think that $x(t)$ will be larger than the solution of $f''=(f')^2+g(\nu)$ and that $y(t)$ will be smaller than the solution of $f''=(f')^2+g(0)$, but I am not completly sure. This could be an approach to solve the question. How to prove all these? (the things that are true of course). I would be very thankful if whoever kind enough to answer or comment could mention  good references to study about similar subjects. Regards. Update (11 May 2018) I was able to solve the equation for the particular case $g(s)=e^s$. With the change of variables $x=-\ln(p)$ and $y=-\ln(q)$, the system becomes  \begin{align*} -\ddot{p}&=p(t)g\left(\ln\left(\frac{q(t)}{p(t)}\right)+\nu\right)=e^\nu q(t)\\ -\ddot{q}&=q(t)g\left(\ln\left(\frac{p(t)}{q(t)}\right)\right)=p(t)~~,\\ \end{align*} with initial conditions $p(0)=q(0)=1$ and $\dot{p}(0)=\dot{q}(0)=0$. This system is solvable, although the solutions are quite ugly so I will not put them explicitly. However, as expected, both $x$ and $y$ are increasing, and $p(t)$ becomes zero (that is, $x(t)$ blows-up) in finite time while $y(t)$ is still finite. What I postulate above about the inequalities that I expect x and y should fulfill it is also true in this particular case.","I am interested in the following non-linear system \begin{align*} \ddot{x}&=(\dot{x})^2+g(x-y+\nu)\\ \ddot{y}&=(\dot{y})^2+g(y-x)~~,\\ \end{align*} where $g$ is a monotonic, increasing, and positive function; and $\nu>0$ is a constant. We can assume $g$ is smoother if it helps. Initial conditions are $\dot{x}(0)=x(0)=\dot{y}(0)=y(0)=0$. Both $x(t)$ and $y(t)$ are functions from the positive reals to the positive reals. I think it is not too hard to prove they are growing functions as well. What I expect is that the answer to the question I pose in the title is ""yes"". That is, the initial extra ""kick"" on $x(t)$ will keep it above $y(t)$ at all times. Maybe $x(t)$ will blow-up before $y(t)$ (I am almost sure they both blow-up in finite time, at least when g is bounded). In fact, I think that $x(t)$ will be larger than the solution of $f''=(f')^2+g(\nu)$ and that $y(t)$ will be smaller than the solution of $f''=(f')^2+g(0)$, but I am not completly sure. This could be an approach to solve the question. How to prove all these? (the things that are true of course). I would be very thankful if whoever kind enough to answer or comment could mention  good references to study about similar subjects. Regards. Update (11 May 2018) I was able to solve the equation for the particular case $g(s)=e^s$. With the change of variables $x=-\ln(p)$ and $y=-\ln(q)$, the system becomes  \begin{align*} -\ddot{p}&=p(t)g\left(\ln\left(\frac{q(t)}{p(t)}\right)+\nu\right)=e^\nu q(t)\\ -\ddot{q}&=q(t)g\left(\ln\left(\frac{p(t)}{q(t)}\right)\right)=p(t)~~,\\ \end{align*} with initial conditions $p(0)=q(0)=1$ and $\dot{p}(0)=\dot{q}(0)=0$. This system is solvable, although the solutions are quite ugly so I will not put them explicitly. However, as expected, both $x$ and $y$ are increasing, and $p(t)$ becomes zero (that is, $x(t)$ blows-up) in finite time while $y(t)$ is still finite. What I postulate above about the inequalities that I expect x and y should fulfill it is also true in this particular case.",,"['ordinary-differential-equations', 'systems-of-equations', 'nonlinear-system']"
68,Solving a constrained inhomogeneous first order ODE.,Solving a constrained inhomogeneous first order ODE.,,"Suppose that $A$ is a symmetric $n \times n$ matrix and $b \in \mathbf{R}^n$. I want to solve the following ODE,  $$ \dot{x} + 2Ax +2b = 0, $$ with $x: \mathbf{R}_+ \to \mathbf{S}^{n-1}$, so the domain is the nonnegative reals and codomain is the the unit sphere. Is this possible with the implicit constraints on $x$? Are there good references that explain how to solve this type of ODE numerically and analyze it mathematically (i.e., stable equilibrium, etc.)?","Suppose that $A$ is a symmetric $n \times n$ matrix and $b \in \mathbf{R}^n$. I want to solve the following ODE,  $$ \dot{x} + 2Ax +2b = 0, $$ with $x: \mathbf{R}_+ \to \mathbf{S}^{n-1}$, so the domain is the nonnegative reals and codomain is the the unit sphere. Is this possible with the implicit constraints on $x$? Are there good references that explain how to solve this type of ODE numerically and analyze it mathematically (i.e., stable equilibrium, etc.)?",,"['ordinary-differential-equations', 'reference-request', 'numerical-methods']"
69,How to teach ordinary differential equations to good students? [closed],How to teach ordinary differential equations to good students? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 6 years ago . Improve this question I am TA-ing a introductory course on ODEs and PDEs this year. At my university most introductory math courses can be taken at ""basic"" and ""extended"" levels. This one is the extended one. My students seem to be not engaged in the subject. I don't blame them. They have already seen some abstract mathematical theories -- general topology, measure theory, abstract algebra. What I am teaching them is a lot of methods for solving specific ODEs. There are of course parts of general theory (existence theorems and some qualitative theory) and exercises to prove something, but mostly its calculation. They long for some abstraction, general framework. Unfortunately, i don't know how to provide it to them without functional analysis (which they haven't had). Please, give me any of your thoughts and advice on this problem.","Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 6 years ago . Improve this question I am TA-ing a introductory course on ODEs and PDEs this year. At my university most introductory math courses can be taken at ""basic"" and ""extended"" levels. This one is the extended one. My students seem to be not engaged in the subject. I don't blame them. They have already seen some abstract mathematical theories -- general topology, measure theory, abstract algebra. What I am teaching them is a lot of methods for solving specific ODEs. There are of course parts of general theory (existence theorems and some qualitative theory) and exercises to prove something, but mostly its calculation. They long for some abstraction, general framework. Unfortunately, i don't know how to provide it to them without functional analysis (which they haven't had). Please, give me any of your thoughts and advice on this problem.",,"['ordinary-differential-equations', 'education']"
70,"Omega limit set $\omega(x_0)$ for $r' = 2μr(5-r^2), \space θ' = -1$",Omega limit set  for,"\omega(x_0) r' = 2μr(5-r^2), \space θ' = -1","Exercise : Given the dynamical system :   $$x_1' = x_2+2μx_1(5-x_1^2-x_2^2)$$   $$x_2' = -x_1+2μx_2(5-x_1^2-x_2^2)$$   where $(x_1,x_2) \in \mathbb R^2$ and $μ>0$ a constant. Applying polar coordinates, determine the omega limit set $ω(x_0)$ for any given vector $(x_1,x_2)$. Discussion : I used the polar coordinates substitution : $$x_1 = r\cosθ$$ $$x_2 = r\sinθ$$ and via the expressions : $$rr' = x_1x_1' + x_2x_2'$$ $$θ' = \frac{x_1x_2' - x_2x_1'}{r^2}$$ I derived the system in polar coordinates : $$r' = 2μr(5-r^2)$$ $$θ' = -1$$ Now, what we can see is that the sign of $r'$ entirely depends on the factor $5-r^2$, since by it's definition $r>0$ and $μ>0$. This means that until $r=\sqrt5$ $r'>0$ and after that $r'<0$. Also, noting that $θ'=-1$ means that the direction of the phase portrait flow follows an anticlockwise flow. Other than that, defining an one dimension phase portrait for $r'$ and noting as a right arrow for where $r'>0$ and left for $r'<0$, we get : $$--- \quad 0 \quad \rightarrow \sqrt{5} \leftarrow$$ This means that there is a circle defined with $r=\sqrt{5}$. This implies that the omega limit set for any given values, will be : $$\text{For} \space x_0 \neq 0 \space \rightarrow \space ω(x_0) = S_{\sqrt5}$$ $$\text{For} \space x_0 = 0 \space \rightarrow \space ω(x_0) = \{0\}$$ Question : Is the above approach correct ?","Exercise : Given the dynamical system :   $$x_1' = x_2+2μx_1(5-x_1^2-x_2^2)$$   $$x_2' = -x_1+2μx_2(5-x_1^2-x_2^2)$$   where $(x_1,x_2) \in \mathbb R^2$ and $μ>0$ a constant. Applying polar coordinates, determine the omega limit set $ω(x_0)$ for any given vector $(x_1,x_2)$. Discussion : I used the polar coordinates substitution : $$x_1 = r\cosθ$$ $$x_2 = r\sinθ$$ and via the expressions : $$rr' = x_1x_1' + x_2x_2'$$ $$θ' = \frac{x_1x_2' - x_2x_1'}{r^2}$$ I derived the system in polar coordinates : $$r' = 2μr(5-r^2)$$ $$θ' = -1$$ Now, what we can see is that the sign of $r'$ entirely depends on the factor $5-r^2$, since by it's definition $r>0$ and $μ>0$. This means that until $r=\sqrt5$ $r'>0$ and after that $r'<0$. Also, noting that $θ'=-1$ means that the direction of the phase portrait flow follows an anticlockwise flow. Other than that, defining an one dimension phase portrait for $r'$ and noting as a right arrow for where $r'>0$ and left for $r'<0$, we get : $$--- \quad 0 \quad \rightarrow \sqrt{5} \leftarrow$$ This means that there is a circle defined with $r=\sqrt{5}$. This implies that the omega limit set for any given values, will be : $$\text{For} \space x_0 \neq 0 \space \rightarrow \space ω(x_0) = S_{\sqrt5}$$ $$\text{For} \space x_0 = 0 \space \rightarrow \space ω(x_0) = \{0\}$$ Question : Is the above approach correct ?",,"['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes', 'stability-theory']"
71,"The system $x' = h(y), \space y' = ay + g(x)$ has no periodic solutions",The system  has no periodic solutions,"x' = h(y), \space y' = ay + g(x)","Exercise : Show that the system of differential equations :   $$x' = h(y)$$   $$y'=ay+g(x)$$   where $a>0$ is a constant and $h,g : \mathbb R \to \mathbb R$ smooth functions, does not have any periodic solutions. Hint : Apply Bendixson's criteria. Discussion/Question : First of all, the exercise refers to Bendixson's Criteria but on our notes and previous lessons, we have explored two different theorems/criteria involving Bendixson : Poincare-Bendixson Theorem : Given a differentiable real dynamical system defined on an open subset of the plane, then every non-empty compact ω-limit set of an orbit, which contains only finitely many fixed points, is either : a fixed point, a periodic orbit, or a connected set composed of a finite number of fixed points together with homoclinic and heteroclinic orbits connecting these. Moreover, there is at most one orbit connecting different fixed points in the same direction. However, there could be countably many homoclinic orbits connecting one fixed point. Bendixson-Dulac Theorem : Ιf there exists a $C^{1}$ function ${\displaystyle \varphi (x,y)}$ (called the Dulac function) such that the expression :   $$\frac{\partial φf}{\partial x} + \frac{\partial φg}{\partial y}$$   has the same sign $( \neq 0)$ almost everywhere in a simply connected region of the plane, then the plane autonomous system :   $$\frac{dx}{dt} = f(x,y)$$   $$\frac{dy}{dt} = g(x,y)$$   has no nonconstant periodic solutions lying entirely within the region.""Almost everywhere"" means everywhere except possibly in a set of measure 0, such as a point or line. I wanted to clarify which of the $2$ criteria/theorems is implied to be used by the phrased used as a hint given by the exercise, as it is not clear enough. In my opinion, I think it refers to the Bendixson - Dulac one, since it's the one regarding the non-existence (point of the exercise as well). Would the first one though lead to something that won't hold with the hypothesis that a periodic orbit exists ? Now, applying the Bendixson-Dulac Theorem : First of all, we're given that $h,g : \mathbb R \to \mathbb R$ are smooth functions, which means that they indeed $h,g \in C^1(\mathbb R)$. Then, for  $φ(x,y) = 1 \in C^1(\mathbb R^2)$, we have : $$\frac{\partial φf}{\partial x} + \frac{\partial φg}{\partial y} = \frac{\partial h(y) }{\partial x} + \frac{\partial (ay + g(x))}{\partial y} = a> 0$$ since $h(y)$ is a function of $y$ and $g(x)$ a function of $x$. Then, since the expression required sustains the same sign everywhere, from the Bendixson-Dulac Theorem there exists none periodic solutions to the given system of differential equations. So, is my approach correct ? Would be letting $φ(x,y) = 1$ be correct ? It seems trivial in such case, still not sure about what exactly the hint wants us to use.","Exercise : Show that the system of differential equations :   $$x' = h(y)$$   $$y'=ay+g(x)$$   where $a>0$ is a constant and $h,g : \mathbb R \to \mathbb R$ smooth functions, does not have any periodic solutions. Hint : Apply Bendixson's criteria. Discussion/Question : First of all, the exercise refers to Bendixson's Criteria but on our notes and previous lessons, we have explored two different theorems/criteria involving Bendixson : Poincare-Bendixson Theorem : Given a differentiable real dynamical system defined on an open subset of the plane, then every non-empty compact ω-limit set of an orbit, which contains only finitely many fixed points, is either : a fixed point, a periodic orbit, or a connected set composed of a finite number of fixed points together with homoclinic and heteroclinic orbits connecting these. Moreover, there is at most one orbit connecting different fixed points in the same direction. However, there could be countably many homoclinic orbits connecting one fixed point. Bendixson-Dulac Theorem : Ιf there exists a $C^{1}$ function ${\displaystyle \varphi (x,y)}$ (called the Dulac function) such that the expression :   $$\frac{\partial φf}{\partial x} + \frac{\partial φg}{\partial y}$$   has the same sign $( \neq 0)$ almost everywhere in a simply connected region of the plane, then the plane autonomous system :   $$\frac{dx}{dt} = f(x,y)$$   $$\frac{dy}{dt} = g(x,y)$$   has no nonconstant periodic solutions lying entirely within the region.""Almost everywhere"" means everywhere except possibly in a set of measure 0, such as a point or line. I wanted to clarify which of the $2$ criteria/theorems is implied to be used by the phrased used as a hint given by the exercise, as it is not clear enough. In my opinion, I think it refers to the Bendixson - Dulac one, since it's the one regarding the non-existence (point of the exercise as well). Would the first one though lead to something that won't hold with the hypothesis that a periodic orbit exists ? Now, applying the Bendixson-Dulac Theorem : First of all, we're given that $h,g : \mathbb R \to \mathbb R$ are smooth functions, which means that they indeed $h,g \in C^1(\mathbb R)$. Then, for  $φ(x,y) = 1 \in C^1(\mathbb R^2)$, we have : $$\frac{\partial φf}{\partial x} + \frac{\partial φg}{\partial y} = \frac{\partial h(y) }{\partial x} + \frac{\partial (ay + g(x))}{\partial y} = a> 0$$ since $h(y)$ is a function of $y$ and $g(x)$ a function of $x$. Then, since the expression required sustains the same sign everywhere, from the Bendixson-Dulac Theorem there exists none periodic solutions to the given system of differential equations. So, is my approach correct ? Would be letting $φ(x,y) = 1$ be correct ? It seems trivial in such case, still not sure about what exactly the hint wants us to use.",,"['ordinary-differential-equations', 'dynamical-systems', 'periodic-functions', 'stability-in-odes']"
72,Behavior of the solution of the equation $f'' = -x f $ as $x\rightarrow \infty$,Behavior of the solution of the equation  as,f'' = -x f  x\rightarrow \infty,"This is the Airy equation, and its two independent solutions are $Ai(-x)$ and $Bi(-x)$. Both of which oscillate with a decaying amplitude but an increasing frequency as $x\rightarrow \infty $. That means a general solution has also similar behavior. Can anyone give a direct proof that a general solution of the equation does not diverge as $x\rightarrow \infty$?","This is the Airy equation, and its two independent solutions are $Ai(-x)$ and $Bi(-x)$. Both of which oscillate with a decaying amplitude but an increasing frequency as $x\rightarrow \infty $. That means a general solution has also similar behavior. Can anyone give a direct proof that a general solution of the equation does not diverge as $x\rightarrow \infty$?",,"['ordinary-differential-equations', 'asymptotics']"
73,Theory of ordinary differential equations - challenging problem (partially solved) - request for help,Theory of ordinary differential equations - challenging problem (partially solved) - request for help,,"Preliminary notation: $\wedge$ - logical ""and"", $J$ - bessel function of I kind, $Y$-bessel function of II kind Problem: a) Solve: $\frac{d^{2}y}{dx^{2}} - [\frac{p(p+1)}{x^{2}} + c]y = 0$ $(\star)$ $(c, p \in \mathbb{R}$, $x \in \mathbb{R} \setminus \{0\}$)  and distinguish bounded solutions for this equation in the neighbourhood of $x=0$. My partial solution(a.k.a. what I did by far): Let: $A:=c$ $\wedge$ $B:=p(p+1)$. Then: $(\star)$ is equivalent to:  $$ x^{2}\frac{d^{2}y}{dx^{2}}-(Ax^{2}+B)y=0 $$ This equation is recognised as Bessesl differential equation. We proceed as follows: Consideration of given algebraic form of coefficients of dependent variable $y$ implies existence of the solution in the form of general power series in the surrounding of $x=0$. Let's apply Frobenius method. Let: $y=y(x)=x^{\rho}\sum\limits_{n=0}^{+\infty}a_{n}x^{n}$ ($a_{0} \in \mathbb{R} \setminus \{ 0 \}$ $\wedge$ $\mid x \mid < +\infty$ $\wedge$ $\rho \in \mathbb{R}_{+}$). Then: $y^{'}(x) = \sum\limits_{n=0}^{+\infty}(n+\rho)a_{n}x^{n+\rho-1} = x^{\rho} \sum\limits_{n=0}^{+\infty}(n+\rho)a_{n}x^{n-1}$ $\implies$ $y^{''}(x) = \sum\limits_{n=0}^{+\infty}(n+\rho)(n+\rho-1)a_{n}x^{n+\rho-2}$   $\implies$ $x^{2}y^{''}(x) = x^{\rho}\sum\limits_{n=0}^{+\infty}(n+\rho)(n+\rho-1)a_{n}x^{n}$ $\implies$ $x^{2}\frac{d^{2}y}{dx^{2}}-(Ax^{2}+B)y = 0$ $\iff$ $x^{\rho}\sum\limits_{n=0}^{+\infty}(n+\rho)(n+\rho-1)a_{n}x^{n} - (Ax^{2}+B)x^{\rho}\sum\limits_{n=0}^{+\infty}a_{n}x^{n} = 0$ $\implies$ $\sum\limits_{n=0}^{+\infty}(n+\rho)(n+\rho-1)a_{n}x^{n} - A\sum\limits_{n=0}^{+\infty}a_{n}x^{n+2} - B\sum\limits_{n=0}^{+\infty}a_{n}x^{n} = 0$ $\implies$ $\sum\limits_{n=0}^{+\infty}(n+\rho)(n+\rho-1)a_{n}x^{n} - \sum\limits_{n=2}^{+\infty}Aa_{n}x^{n} - \sum\limits_{n=0}^{+\infty}Ba_{n}x^{n} = 0$ $\implies$  $[\rho(\rho - 1)-B]a_{0} + [(1 + \rho)\rho - B]a_{1}x + \sum\limits_{n=2}^{+\infty}\Big\{ [(n+\rho)(n+\rho-1)-B]a_{n} - Aa_{n-2}\Big\}x^{n} = 0$ $\implies$  $\implies$ $\begin{cases} \rho(\rho-1)-B=0 \\ [(1+\rho)\rho-B]a_{1}=0 \\ [(n+\rho)(n+\rho-1)-B]a_{n}-A a_{n-2} = 0 \end{cases}$ $\implies$  $\begin{cases} B=\rho(\rho-1) \\ [(\rho+\rho^{2})-(\rho^{2}-\rho)]a_{1}=0 \\ a_{n} = \frac{A}{[(n+\rho)(n+\rho-1)-B]}a_{n-2} \end{cases}$ $\implies$  $\begin{cases} B=\rho(\rho-1) \\ 2\rho a_{1} = 0 \\ a_{n} = \frac{A}{[(n+\rho)(n+\rho-1)-B]}a_{n-2} \end{cases}$ $\implies$  $\begin{cases} B=\rho(\rho-1) \\ a_{1} = 0 \\ a_{n} = \frac{A}{[(n+\rho)(n+\rho-1)-B]}a_{n-2} \end{cases}$ $\implies$  $\begin{cases} B=\rho(1-\rho) \\ \forall n \in \mathbb{N}_{0}: a_{2n+1}=0 \\ \forall n \in \mathbb{N}: a_{2n} = \frac{A}{[(2n+\rho)(2n+\rho-1)-B]}a_{2n-2} \end{cases}$ ; $\rho(\rho-1)-B=0$ $\iff$ $\rho^{2}-\rho-B = 0$. $\Delta = 1+4B$. $\rho = \frac{1\pm \sqrt{1+4B}}{2}$ $\implies$ $\rho = \frac{1+\sqrt{1+4B}}{2}$ $\implies$ $\forall n \in \mathbb{N}$: $a_{2n} = \frac{A}{[(n+\frac{1+\sqrt{1+4B}}{2})(n+\frac{1+\sqrt{1+4B}}{2}-1)-B]}a_{2n-2}$ $\implies$ $\forall n \in \mathbb{N}$: $a_{2n} = \frac{A^{n}}{\Big{[} \prod\limits_{k=0}^{n}(2k+\frac{1+\sqrt{1+4B}}{2})(2k+\frac{1+\sqrt{1+4B}}{2}-1)-B\Big{]}}a_{0}$  $\implies$ $y_{1}(x) = a_{0}\sum\limits_{n=0}^{+\infty}\frac{A^{n}x^{2n}}{ {\Big{[} \prod\limits_{k=0}^{n}(2k+\frac{1+\sqrt{1+4B}}{2})(2k+\frac{1+\sqrt{1+4B}}{2}-1)-B\Big{]}}} = \sqrt{x}J_{\frac{1}{2}\sqrt{1+4B}}(-i\sqrt{A}x)$  $\rho_{1} = \frac{1+\sqrt{1+4B}}{2}$ $\wedge$ $\rho_{2} = \frac{1-\sqrt{1+4B}}{2}$ $\implies$ $\rho_{1} - \rho_{2} = \sqrt{1+4B}$ $\implies$ $y_{2}(x) = b_{0}\sum\limits_{n=0}^{+\infty}\frac{(-1)^{n}A^{n}x^{2k-\sqrt{1+4B}}}{{\Big{[} \prod\limits_{k=0}^{n}(2k+\frac{1+\sqrt{1+4B}}{2})(2k+\frac{1+\sqrt{1+4B}}{2}-1)-B\Big{]}}} = \sqrt{x}Y_{\frac{1}{2}\sqrt{1+4B}}(-i\sqrt{A}x)$ \ $\implies$ $y(x) = C_{1}y_{1}(x) + C_{2}y_{2}(x)=C_{1}\sqrt{x}J_{\frac{1}{2}\sqrt{1+4B}}(-i\sqrt{A}x) + C_{2}\sqrt{x}Y_{\frac{\sqrt{1+4B}}{2}}(-i\sqrt{A}x)$  [$C_{1},C_{2} \in \mathbb{R}$ $\wedge$ $A:=c$ $\wedge$ $B:=p(p-1)$: $c,p \in \mathbb{R}$] I checked my solution with mathematical package Maple and it turned out that I obtained correct result. Note: I know that the solution can be expressed in more concise way by using Bessel function of III kind(a.k. Hankel function) What is my problem: I do not know how to distinguish bounded solutions in the neighbourhood of $x=0$. I got stuck here completely. My request Since I did a significant work, I do very request for help in the determination of the aforementioned bounded solution of the considered equation in the neighbourhood of x=0. Help very appreciated!","Preliminary notation: $\wedge$ - logical ""and"", $J$ - bessel function of I kind, $Y$-bessel function of II kind Problem: a) Solve: $\frac{d^{2}y}{dx^{2}} - [\frac{p(p+1)}{x^{2}} + c]y = 0$ $(\star)$ $(c, p \in \mathbb{R}$, $x \in \mathbb{R} \setminus \{0\}$)  and distinguish bounded solutions for this equation in the neighbourhood of $x=0$. My partial solution(a.k.a. what I did by far): Let: $A:=c$ $\wedge$ $B:=p(p+1)$. Then: $(\star)$ is equivalent to:  $$ x^{2}\frac{d^{2}y}{dx^{2}}-(Ax^{2}+B)y=0 $$ This equation is recognised as Bessesl differential equation. We proceed as follows: Consideration of given algebraic form of coefficients of dependent variable $y$ implies existence of the solution in the form of general power series in the surrounding of $x=0$. Let's apply Frobenius method. Let: $y=y(x)=x^{\rho}\sum\limits_{n=0}^{+\infty}a_{n}x^{n}$ ($a_{0} \in \mathbb{R} \setminus \{ 0 \}$ $\wedge$ $\mid x \mid < +\infty$ $\wedge$ $\rho \in \mathbb{R}_{+}$). Then: $y^{'}(x) = \sum\limits_{n=0}^{+\infty}(n+\rho)a_{n}x^{n+\rho-1} = x^{\rho} \sum\limits_{n=0}^{+\infty}(n+\rho)a_{n}x^{n-1}$ $\implies$ $y^{''}(x) = \sum\limits_{n=0}^{+\infty}(n+\rho)(n+\rho-1)a_{n}x^{n+\rho-2}$   $\implies$ $x^{2}y^{''}(x) = x^{\rho}\sum\limits_{n=0}^{+\infty}(n+\rho)(n+\rho-1)a_{n}x^{n}$ $\implies$ $x^{2}\frac{d^{2}y}{dx^{2}}-(Ax^{2}+B)y = 0$ $\iff$ $x^{\rho}\sum\limits_{n=0}^{+\infty}(n+\rho)(n+\rho-1)a_{n}x^{n} - (Ax^{2}+B)x^{\rho}\sum\limits_{n=0}^{+\infty}a_{n}x^{n} = 0$ $\implies$ $\sum\limits_{n=0}^{+\infty}(n+\rho)(n+\rho-1)a_{n}x^{n} - A\sum\limits_{n=0}^{+\infty}a_{n}x^{n+2} - B\sum\limits_{n=0}^{+\infty}a_{n}x^{n} = 0$ $\implies$ $\sum\limits_{n=0}^{+\infty}(n+\rho)(n+\rho-1)a_{n}x^{n} - \sum\limits_{n=2}^{+\infty}Aa_{n}x^{n} - \sum\limits_{n=0}^{+\infty}Ba_{n}x^{n} = 0$ $\implies$  $[\rho(\rho - 1)-B]a_{0} + [(1 + \rho)\rho - B]a_{1}x + \sum\limits_{n=2}^{+\infty}\Big\{ [(n+\rho)(n+\rho-1)-B]a_{n} - Aa_{n-2}\Big\}x^{n} = 0$ $\implies$  $\implies$ $\begin{cases} \rho(\rho-1)-B=0 \\ [(1+\rho)\rho-B]a_{1}=0 \\ [(n+\rho)(n+\rho-1)-B]a_{n}-A a_{n-2} = 0 \end{cases}$ $\implies$  $\begin{cases} B=\rho(\rho-1) \\ [(\rho+\rho^{2})-(\rho^{2}-\rho)]a_{1}=0 \\ a_{n} = \frac{A}{[(n+\rho)(n+\rho-1)-B]}a_{n-2} \end{cases}$ $\implies$  $\begin{cases} B=\rho(\rho-1) \\ 2\rho a_{1} = 0 \\ a_{n} = \frac{A}{[(n+\rho)(n+\rho-1)-B]}a_{n-2} \end{cases}$ $\implies$  $\begin{cases} B=\rho(\rho-1) \\ a_{1} = 0 \\ a_{n} = \frac{A}{[(n+\rho)(n+\rho-1)-B]}a_{n-2} \end{cases}$ $\implies$  $\begin{cases} B=\rho(1-\rho) \\ \forall n \in \mathbb{N}_{0}: a_{2n+1}=0 \\ \forall n \in \mathbb{N}: a_{2n} = \frac{A}{[(2n+\rho)(2n+\rho-1)-B]}a_{2n-2} \end{cases}$ ; $\rho(\rho-1)-B=0$ $\iff$ $\rho^{2}-\rho-B = 0$. $\Delta = 1+4B$. $\rho = \frac{1\pm \sqrt{1+4B}}{2}$ $\implies$ $\rho = \frac{1+\sqrt{1+4B}}{2}$ $\implies$ $\forall n \in \mathbb{N}$: $a_{2n} = \frac{A}{[(n+\frac{1+\sqrt{1+4B}}{2})(n+\frac{1+\sqrt{1+4B}}{2}-1)-B]}a_{2n-2}$ $\implies$ $\forall n \in \mathbb{N}$: $a_{2n} = \frac{A^{n}}{\Big{[} \prod\limits_{k=0}^{n}(2k+\frac{1+\sqrt{1+4B}}{2})(2k+\frac{1+\sqrt{1+4B}}{2}-1)-B\Big{]}}a_{0}$  $\implies$ $y_{1}(x) = a_{0}\sum\limits_{n=0}^{+\infty}\frac{A^{n}x^{2n}}{ {\Big{[} \prod\limits_{k=0}^{n}(2k+\frac{1+\sqrt{1+4B}}{2})(2k+\frac{1+\sqrt{1+4B}}{2}-1)-B\Big{]}}} = \sqrt{x}J_{\frac{1}{2}\sqrt{1+4B}}(-i\sqrt{A}x)$  $\rho_{1} = \frac{1+\sqrt{1+4B}}{2}$ $\wedge$ $\rho_{2} = \frac{1-\sqrt{1+4B}}{2}$ $\implies$ $\rho_{1} - \rho_{2} = \sqrt{1+4B}$ $\implies$ $y_{2}(x) = b_{0}\sum\limits_{n=0}^{+\infty}\frac{(-1)^{n}A^{n}x^{2k-\sqrt{1+4B}}}{{\Big{[} \prod\limits_{k=0}^{n}(2k+\frac{1+\sqrt{1+4B}}{2})(2k+\frac{1+\sqrt{1+4B}}{2}-1)-B\Big{]}}} = \sqrt{x}Y_{\frac{1}{2}\sqrt{1+4B}}(-i\sqrt{A}x)$ \ $\implies$ $y(x) = C_{1}y_{1}(x) + C_{2}y_{2}(x)=C_{1}\sqrt{x}J_{\frac{1}{2}\sqrt{1+4B}}(-i\sqrt{A}x) + C_{2}\sqrt{x}Y_{\frac{\sqrt{1+4B}}{2}}(-i\sqrt{A}x)$  [$C_{1},C_{2} \in \mathbb{R}$ $\wedge$ $A:=c$ $\wedge$ $B:=p(p-1)$: $c,p \in \mathbb{R}$] I checked my solution with mathematical package Maple and it turned out that I obtained correct result. Note: I know that the solution can be expressed in more concise way by using Bessel function of III kind(a.k. Hankel function) What is my problem: I do not know how to distinguish bounded solutions in the neighbourhood of $x=0$. I got stuck here completely. My request Since I did a significant work, I do very request for help in the determination of the aforementioned bounded solution of the considered equation in the neighbourhood of x=0. Help very appreciated!",,"['real-analysis', 'ordinary-differential-equations']"
74,Gronwall's inequality for higher order derivatives.,Gronwall's inequality for higher order derivatives.,,"Gronwall's inequality says that solutions to the initial value problem $u'(t) \leq \beta(t)u(t)$ with $u(0)=u_0$ are bounded by solutions to the problem with inequality replaced with equality for $t\in [0,\infty)$. Is there a way to generalize to higher order derivatives. That is, if $u''(t) \leq \alpha(t)u'(t) + \beta(t)u(t)$ with $u(0)=u_0$ and $u'(0)=u'_0$ can we say that the solutions to the corresponding differential equation dominates $u$?","Gronwall's inequality says that solutions to the initial value problem $u'(t) \leq \beta(t)u(t)$ with $u(0)=u_0$ are bounded by solutions to the problem with inequality replaced with equality for $t\in [0,\infty)$. Is there a way to generalize to higher order derivatives. That is, if $u''(t) \leq \alpha(t)u'(t) + \beta(t)u(t)$ with $u(0)=u_0$ and $u'(0)=u'_0$ can we say that the solutions to the corresponding differential equation dominates $u$?",,"['real-analysis', 'ordinary-differential-equations', 'inequality']"
75,Kalman decomposition using Hautus test,Kalman decomposition using Hautus test,,"In linear control theory, the Kalman decomposition is used as a similarity transformation to decompose a given linear time-invariant system $$\dot{x}(t)=Ax(t)+Bu(t)$$ $$y(t) = Cx(t) + Du(t)$$ into its controllable & observable, controllable & not observable, not controllable & observable and not controllable & not observable subsystems. A system is controllable if $\mathcal{C}=[B\quad AB \quad A^2B \ldots A^{n-1}B]^T$ has rank $n$ (number of rows/columns of the system matrix $A$). A system is observable if $\mathcal{O}=[C \quad CA \quad CA^2 \ldots CA^{n-1}]^T$ has rank $n$ (number of rows/columns of the system matrix $A$). It seems possible to construct the similarity transformation by using the eigenvectors of the Hautus test for controllability and observability. It states a system is controllable if for all eigenvalues $\lambda$ of $A$ the matrix $[A-\lambda I\quad B]$ has full rank $n$ for a $n \times n$ system matrix $A$. A similar statement reads: A system is observable if for all eigenvalues $\lambda$ of $A$ the matrix $[A^T - \lambda I \quad C^T]$ has full rank $n$ for a $n \times n$ system matrix $A$. The user fibonatic from Engineering Stack Exchange suggested that one could construct the similarity transformation for the Hautus test. I am wondering, how this is possible. This is the same example as in the post on Engineering Stack Exchange. $$\dot{x} = \begin{bmatrix}1 & 1 & 0 \\ 0 & 1 & 0 \\ 0 & 1 & 1 \end{bmatrix}x + \begin{bmatrix}0 & 1 \\ 1 & 0 \\ 0 & 1 \end{bmatrix} u$$ $$y = \begin{bmatrix}1 & 1 & 1 \end{bmatrix} x$$ The similarity transfromation is given by  $$ M =  \begin{bmatrix} 0 & 1 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & -1 \end{bmatrix}. $$ This is what I have tried. The three eigenvalues of $A$ are all $\lambda =1$ the eigenvectors are $v_1 = [1, 0,0]^T, v_2=[0,0,1]^T$ and $v_3=[1,0,1]^T$. The rank of the $[A-\lambda B]$ is 2. That means there is a deficit in rank for every eigenvector. If I include the eigenvectors to a ""similarity"" transformation $$\tilde{M} = \begin{bmatrix} 1 & 0 & 1\\ 0 & 0 & 0\\ 0 & 1 & 1\\  \end{bmatrix} $$ it turns out that it is singular and cannot be the similarity transformation that I need. So how can I use the Hautus test to construct the similarity   transformation for the Kalman decomposition?","In linear control theory, the Kalman decomposition is used as a similarity transformation to decompose a given linear time-invariant system $$\dot{x}(t)=Ax(t)+Bu(t)$$ $$y(t) = Cx(t) + Du(t)$$ into its controllable & observable, controllable & not observable, not controllable & observable and not controllable & not observable subsystems. A system is controllable if $\mathcal{C}=[B\quad AB \quad A^2B \ldots A^{n-1}B]^T$ has rank $n$ (number of rows/columns of the system matrix $A$). A system is observable if $\mathcal{O}=[C \quad CA \quad CA^2 \ldots CA^{n-1}]^T$ has rank $n$ (number of rows/columns of the system matrix $A$). It seems possible to construct the similarity transformation by using the eigenvectors of the Hautus test for controllability and observability. It states a system is controllable if for all eigenvalues $\lambda$ of $A$ the matrix $[A-\lambda I\quad B]$ has full rank $n$ for a $n \times n$ system matrix $A$. A similar statement reads: A system is observable if for all eigenvalues $\lambda$ of $A$ the matrix $[A^T - \lambda I \quad C^T]$ has full rank $n$ for a $n \times n$ system matrix $A$. The user fibonatic from Engineering Stack Exchange suggested that one could construct the similarity transformation for the Hautus test. I am wondering, how this is possible. This is the same example as in the post on Engineering Stack Exchange. $$\dot{x} = \begin{bmatrix}1 & 1 & 0 \\ 0 & 1 & 0 \\ 0 & 1 & 1 \end{bmatrix}x + \begin{bmatrix}0 & 1 \\ 1 & 0 \\ 0 & 1 \end{bmatrix} u$$ $$y = \begin{bmatrix}1 & 1 & 1 \end{bmatrix} x$$ The similarity transfromation is given by  $$ M =  \begin{bmatrix} 0 & 1 & 1 \\ 1 & 0 & 0 \\ 0 & 1 & -1 \end{bmatrix}. $$ This is what I have tried. The three eigenvalues of $A$ are all $\lambda =1$ the eigenvectors are $v_1 = [1, 0,0]^T, v_2=[0,0,1]^T$ and $v_3=[1,0,1]^T$. The rank of the $[A-\lambda B]$ is 2. That means there is a deficit in rank for every eigenvector. If I include the eigenvectors to a ""similarity"" transformation $$\tilde{M} = \begin{bmatrix} 1 & 0 & 1\\ 0 & 0 & 0\\ 0 & 1 & 1\\  \end{bmatrix} $$ it turns out that it is singular and cannot be the similarity transformation that I need. So how can I use the Hautus test to construct the similarity   transformation for the Kalman decomposition?",,"['ordinary-differential-equations', 'dynamical-systems', 'control-theory', 'linear-control']"
76,A set of coupled ODE,A set of coupled ODE,,"I've recently encountered this set of coupled ODE, $\partial_x\alpha(x)-iA(x)\beta(x)+B(x)\beta(x)=iC(x)\alpha(x),\\ \partial_x\beta(x)+iA(x)\alpha(x)+B(x)\alpha(x)=-iC(x)\beta(x),$ where $\alpha(x)$, $\beta(x)$, $A(x)$, $B(x)$ and $C(x)$ are real functions of $x$. This set of coupled ODE was arisen from solving a 2x2 matrix eigenvalue problem. Naively this should not be hard, but I got stucked for a while... I tried to use the matrix method $\mathbb{x}'=\mathbb{A}\mathbb{x}$ to solve it, but faced two difficulties: the complex entries and all matrix elements are functions which makes it hard to determine the eigenvalues. As a remark, I already generalized from the set of coupled ODE I'm dealing with and hope to find a general form of solutions by replacing original functions with $A(x), B(x), C(x)$. *I consulted with a few applied mathematicians and physicists about this problem and they concluded that there is no solution of analytic form (product of elementary functions) which I highly doubted. I believe this can be solved analytically and systematically. So, here I am. Please point out my flaws and enlight me. Thanks in advanced.","I've recently encountered this set of coupled ODE, $\partial_x\alpha(x)-iA(x)\beta(x)+B(x)\beta(x)=iC(x)\alpha(x),\\ \partial_x\beta(x)+iA(x)\alpha(x)+B(x)\alpha(x)=-iC(x)\beta(x),$ where $\alpha(x)$, $\beta(x)$, $A(x)$, $B(x)$ and $C(x)$ are real functions of $x$. This set of coupled ODE was arisen from solving a 2x2 matrix eigenvalue problem. Naively this should not be hard, but I got stucked for a while... I tried to use the matrix method $\mathbb{x}'=\mathbb{A}\mathbb{x}$ to solve it, but faced two difficulties: the complex entries and all matrix elements are functions which makes it hard to determine the eigenvalues. As a remark, I already generalized from the set of coupled ODE I'm dealing with and hope to find a general form of solutions by replacing original functions with $A(x), B(x), C(x)$. *I consulted with a few applied mathematicians and physicists about this problem and they concluded that there is no solution of analytic form (product of elementary functions) which I highly doubted. I believe this can be solved analytically and systematically. So, here I am. Please point out my flaws and enlight me. Thanks in advanced.",,['ordinary-differential-equations']
77,"Formulas involving the polynomials $\frac{1}{n!}x^n(a_n-b_nx)^n,$ where the coprime integers $a_n,b_n$ satisfy $H_n=\frac{a_n}{b_n}$ for $n\geq 1$",Formulas involving the polynomials  where the coprime integers  satisfy  for,"\frac{1}{n!}x^n(a_n-b_nx)^n, a_n,b_n H_n=\frac{a_n}{b_n} n\geq 1","For integers $n\geq 1$ let $$H_n=1+\frac{1}{2}+\ldots+\frac{1}{n}$$ the $nth$ harmonic number. After I've seen the form of the polynomials used by Niven in [1] I wanted to create a puzzle with a new definiton. Definition. For integers $n\geq 1$, I define the following polynomials    $$\operatorname{Niv}_{n}(x):=\frac{1}{n!}x^n(a_n-b_nx)^n,\tag{1}$$ where $a_n$ and $b_n$ are positive integers satisfying $\gcd(a_n,b_n)=1$ and $$H_n=\frac{a_n}{b_n}.\tag{2}$$ I would like to know if such polynomials satisfy some nice recurrence, or some nice ordinary differential equation. Since my definiton was a puzzle (I mean that there are no mathematical reason to define those as I did) isn't required that these have a special features. Question. Can you set a simple recurrence or a simple ordinary differential equation that satisfy the polynomials $\operatorname{Niv}_{n}(x)$? Only is required an aproach to set a recurrence* or a differential equation. Many thanks. Isn't required that the order of our equation to be $1$. Using the definiton one can write $$\left(a_n-b_nx\right)^n\operatorname{Niv}_{n+1}(x)=\frac{x^{n+1}}{n+1}\left(a_{n+1}-b_{n+1}x\right)^{n+1}\operatorname{Niv}_{n}(x).\tag{3}$$ But it does not explode $H_{n+1}=H_n+\frac{1}{n},$ that is $$\frac{a_{n+1}}{b_{n+1}}=\frac{a_n}{b_n}+\frac{1}{n}.\tag{4}$$  Thus I don't know if this recurrence, although it is simple,  isn't the best recurrence by capturing the information of the definition. So here is a compromise on the purpose of finding a simple recurrence or well a differential equation, but interesting enough in relation to our definition. Thus you can develop your reasoning to find a simple formula, understanding this word in a flexible way. *In the case of the recurrence if you prefer it you can write an asymptotic identity (as $n\to\infty$) involving our polynomials. References: [1] I. Niven, A simple proof that $\pi$ is irrational , Bull. Amer. Math. Soc. Volume 53, Number 6 (1947).","For integers $n\geq 1$ let $$H_n=1+\frac{1}{2}+\ldots+\frac{1}{n}$$ the $nth$ harmonic number. After I've seen the form of the polynomials used by Niven in [1] I wanted to create a puzzle with a new definiton. Definition. For integers $n\geq 1$, I define the following polynomials    $$\operatorname{Niv}_{n}(x):=\frac{1}{n!}x^n(a_n-b_nx)^n,\tag{1}$$ where $a_n$ and $b_n$ are positive integers satisfying $\gcd(a_n,b_n)=1$ and $$H_n=\frac{a_n}{b_n}.\tag{2}$$ I would like to know if such polynomials satisfy some nice recurrence, or some nice ordinary differential equation. Since my definiton was a puzzle (I mean that there are no mathematical reason to define those as I did) isn't required that these have a special features. Question. Can you set a simple recurrence or a simple ordinary differential equation that satisfy the polynomials $\operatorname{Niv}_{n}(x)$? Only is required an aproach to set a recurrence* or a differential equation. Many thanks. Isn't required that the order of our equation to be $1$. Using the definiton one can write $$\left(a_n-b_nx\right)^n\operatorname{Niv}_{n+1}(x)=\frac{x^{n+1}}{n+1}\left(a_{n+1}-b_{n+1}x\right)^{n+1}\operatorname{Niv}_{n}(x).\tag{3}$$ But it does not explode $H_{n+1}=H_n+\frac{1}{n},$ that is $$\frac{a_{n+1}}{b_{n+1}}=\frac{a_n}{b_n}+\frac{1}{n}.\tag{4}$$  Thus I don't know if this recurrence, although it is simple,  isn't the best recurrence by capturing the information of the definition. So here is a compromise on the purpose of finding a simple recurrence or well a differential equation, but interesting enough in relation to our definition. Thus you can develop your reasoning to find a simple formula, understanding this word in a flexible way. *In the case of the recurrence if you prefer it you can write an asymptotic identity (as $n\to\infty$) involving our polynomials. References: [1] I. Niven, A simple proof that $\pi$ is irrational , Bull. Amer. Math. Soc. Volume 53, Number 6 (1947).",,"['real-analysis', 'ordinary-differential-equations']"
78,Confusing result about stability of a limit cycle,Confusing result about stability of a limit cycle,,"I want to understand a confusing result about this ODE system from Arnold's book on dynamical systems. $$\dot{r}=[r^2-1][2 r \cos \phi - 1]$$  $$\dot{\phi}=1$$ The limit cycle is given for $r=1$ and $\phi = t$. My question : The limit cycle seems to be asymptotically stable (if I   didn't do any mistakes; see 1. Determining stability by Floquet analysis ) but the phase portrait seems to suggest, that   there are trajectories that start close to the limit cycle but drift   away from it (see: 2. Understanding stability in the phase portrait ). I would interpret the phase portrait of the original system as evidence that   the limit cycle is not stable because there are trajectories that start close to it but then drift away on the $x$-axis $r=1$. I would be glad if someone could help   me in resolving this confusion. 1. Determining stability by Floquet analysis One way of dealing with the stability of this limit cycle is by first rewriting the ODE such that the origin is the trivial solution to the ODE. In order to do this use the following substitution $r = 1 + x_1$ and $\phi = t + x_2$. The system then can be written as  $$\dot{x}_1=[x_1^2+2x_1]\left[2(x_1+1)\cos(t+x_2)-1 \right]$$ $$\dot{x}_2=0.$$ It is obvious that $x_1=0$ and $x_2=0$ is now the trivial solution, which corresponds to the limit cycle in the original equation. Now we can linearize the system at the origin to obtain: $$\Delta \dot{x}_1=-2\Delta x_1 +4\Delta x_1\cos(t) $$ $$\Delta \dot{x}_2=0.$$ Note, that the system in question is a linear time-variant system but has periodic coefficients with period $T = 2\pi$. Such systems can be investigated using Floquet theory . One will have to simulate the system for two pairs of initial conditions $x_{1,1}(0)=1\, \wedge \,x_{2,1}(0)=0$ and $x_{1,2}(0)=0 \,\wedge \, x_{2,2}(0)=1$ form $t=0$ to $t=2\pi$. One will obtain the states $x_{1,1}(2\pi),x_{2,1}(2\pi),x_{1,2}(2\pi),x_{2,2}(2\pi)$ which can be put together into the so called  monodromy matrix $$C=\begin{bmatrix} x_{1,1}(2\pi) & x_{1,2}(2\pi)\\ x_{2,1}(2\pi) & x_{2,2}(2\pi) \end{bmatrix}.$$ This can be as previously told by numerical integration or analytically like in this case. Note that the linearized equation is a decoupled first order linear equation. The general solution is given by $$\Delta x_1(t) = c_1\exp(4\sin(t)-2t)$$ $$\Delta x_2(t) = c_2.$$ Using the initial conditions and setting $t=2\pi$ we obtain: $$C = \begin{bmatrix}\exp(-4\pi) & 0 \\ 0 &1 \end{bmatrix}$$ The eigenvalues of this matrix, which can also be complex, are called Floquet multipliers $\lambda_i$. The eigenvalues are given by the entry on the diagonal, as we have an upper triangular matrix. The first eigenvalue is $\lambda_1= \exp(-4\pi)<1$ and the second one is given as $\lambda_2=1$. It can be shown that linearization of a limit cycle will always lead to one Floquet multiplier $\lambda = 1$, which can be excluded from the analysis. This eigenvalue corresponds to a disturbance along the limit cycle. If the remaining Floquet multipliers fulfill the following inequality $|\lambda_i| < 1$, then the limit cycle is asymptotically stable. If there is at least one remaining Floquet multiplier with $|\lambda_i|>1$ then the limit cycle is unstable. If there exist at least one remaining Floquet multipliers for which $|\lambda_i|=1$ but the absolute value of all other Floquet multipliers is smaller or equal to $1$ then this method is indecisive. As we can see that the only remaining eigenvalue is $0<\lambda_1=\exp(-4\pi)<1$, we can conclude that the limit cycle is asymptotically stable . 2. Understanding stability in the phase portrait In order to get a better picture of the limit cycle, I plotted some trajectories of the ODE system. Which you see in the following figure ($x$-axis: $r$ and $y$-axis: $\phi$).","I want to understand a confusing result about this ODE system from Arnold's book on dynamical systems. $$\dot{r}=[r^2-1][2 r \cos \phi - 1]$$  $$\dot{\phi}=1$$ The limit cycle is given for $r=1$ and $\phi = t$. My question : The limit cycle seems to be asymptotically stable (if I   didn't do any mistakes; see 1. Determining stability by Floquet analysis ) but the phase portrait seems to suggest, that   there are trajectories that start close to the limit cycle but drift   away from it (see: 2. Understanding stability in the phase portrait ). I would interpret the phase portrait of the original system as evidence that   the limit cycle is not stable because there are trajectories that start close to it but then drift away on the $x$-axis $r=1$. I would be glad if someone could help   me in resolving this confusion. 1. Determining stability by Floquet analysis One way of dealing with the stability of this limit cycle is by first rewriting the ODE such that the origin is the trivial solution to the ODE. In order to do this use the following substitution $r = 1 + x_1$ and $\phi = t + x_2$. The system then can be written as  $$\dot{x}_1=[x_1^2+2x_1]\left[2(x_1+1)\cos(t+x_2)-1 \right]$$ $$\dot{x}_2=0.$$ It is obvious that $x_1=0$ and $x_2=0$ is now the trivial solution, which corresponds to the limit cycle in the original equation. Now we can linearize the system at the origin to obtain: $$\Delta \dot{x}_1=-2\Delta x_1 +4\Delta x_1\cos(t) $$ $$\Delta \dot{x}_2=0.$$ Note, that the system in question is a linear time-variant system but has periodic coefficients with period $T = 2\pi$. Such systems can be investigated using Floquet theory . One will have to simulate the system for two pairs of initial conditions $x_{1,1}(0)=1\, \wedge \,x_{2,1}(0)=0$ and $x_{1,2}(0)=0 \,\wedge \, x_{2,2}(0)=1$ form $t=0$ to $t=2\pi$. One will obtain the states $x_{1,1}(2\pi),x_{2,1}(2\pi),x_{1,2}(2\pi),x_{2,2}(2\pi)$ which can be put together into the so called  monodromy matrix $$C=\begin{bmatrix} x_{1,1}(2\pi) & x_{1,2}(2\pi)\\ x_{2,1}(2\pi) & x_{2,2}(2\pi) \end{bmatrix}.$$ This can be as previously told by numerical integration or analytically like in this case. Note that the linearized equation is a decoupled first order linear equation. The general solution is given by $$\Delta x_1(t) = c_1\exp(4\sin(t)-2t)$$ $$\Delta x_2(t) = c_2.$$ Using the initial conditions and setting $t=2\pi$ we obtain: $$C = \begin{bmatrix}\exp(-4\pi) & 0 \\ 0 &1 \end{bmatrix}$$ The eigenvalues of this matrix, which can also be complex, are called Floquet multipliers $\lambda_i$. The eigenvalues are given by the entry on the diagonal, as we have an upper triangular matrix. The first eigenvalue is $\lambda_1= \exp(-4\pi)<1$ and the second one is given as $\lambda_2=1$. It can be shown that linearization of a limit cycle will always lead to one Floquet multiplier $\lambda = 1$, which can be excluded from the analysis. This eigenvalue corresponds to a disturbance along the limit cycle. If the remaining Floquet multipliers fulfill the following inequality $|\lambda_i| < 1$, then the limit cycle is asymptotically stable. If there is at least one remaining Floquet multiplier with $|\lambda_i|>1$ then the limit cycle is unstable. If there exist at least one remaining Floquet multipliers for which $|\lambda_i|=1$ but the absolute value of all other Floquet multipliers is smaller or equal to $1$ then this method is indecisive. As we can see that the only remaining eigenvalue is $0<\lambda_1=\exp(-4\pi)<1$, we can conclude that the limit cycle is asymptotically stable . 2. Understanding stability in the phase portrait In order to get a better picture of the limit cycle, I plotted some trajectories of the ODE system. Which you see in the following figure ($x$-axis: $r$ and $y$-axis: $\phi$).",,"['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes', 'stability-theory']"
79,"Is there any theory surrounding this ability to go back and forth between ""logarithms"" and ""differentiation""?","Is there any theory surrounding this ability to go back and forth between ""logarithms"" and ""differentiation""?",,"Given a real number $x>0$, define it's ""derivative"" as follows: $$x' = x \log(x).$$ We can easily show that $$(xy)' = x'y+xy'$$ for all $x,y > 0$. In particular: $$(xy)' = xy \log(xy) = xy(\log(x)+\log(y)) = xy \log(x)+xy\log(y) = x'y+xy'$$ So, we started with something, namely logarithms, that turns multiplication into addition, and we obtained something, namely ""derivatives"" in the above sense, satisfying the Leibniz law, and these are related by $x'/x = \log(x)$. This suggests that we try to go the other way. Given a function $f$ on the real line, define it's ""loogarithm"" to be $$\mathrm{loog}(f) = \frac{f'}{f},$$ where $f'$ is the ordinary derivative. If we're willing to assume $f$ is positive, then the loogarithm can be understood as the act of taking the ordinary logarithm and then differentiating. In more detail: $$\mathrm{loog}(f) = (\log(f))'.$$ So basically, $\mathrm{loog}$ is the well-known technique of logarithmic differentiation. It's straightforward to check that $$\mathrm{loog}(fg) = \mathrm{loog}(f)+\mathrm{loog}(g),$$ which is what makes that technique work, after all. The above observations generalize readily to derivations on commutative rings/fields. In particular we're given a function $\log : R \rightarrow R$ on a commutative ring satisfying $\log(xy) = \log(x)+\log(y)$ and $\log(1) = 0$, then we can get a notion of differentiation on $R$ via $x' = x \log(x)$, and it will always satisfy Leibniz. Conversely, the formula $\log(x) = x'/x$ allows us to turn anything satisfying Leibniz into a notion of logarithm that's defined for all units. Question. Is there any theory surrounding this ability to go back and forth between ""logarithms"" and ""differentiation"" in this way? It seems to be rather important. What follows is a long discussion about this kind of thing. We might try inverting $\mathrm{loog}$ to get some kind of exponentiation function. Unfortunately, unlike $\log$, the loogarithm is not invertible because differentiation does funny things. Oh well, let's just add the basepoint from which we're going to start integrating as an extra parameter $p$: $$a_p^f := a^{\int_p f}.$$ This exponentiation operator occurs in the method of integrating factors . Some of it's properties are familiar looking: $$a_p^0 = 1, \qquad a_p^{f+g} = a_p^f a_p^g, \qquad a_p^{-f} = \frac{1}{a_p^f}$$ Others look a bit strange, for example: $$a^1_p(x) = a^{x-p}$$ And I haven't been able to find anything worthwhile to say about $a_p^{fg}$. The method of integrating factors works because of the statement: $$(e_p^f)' = f \cdot e_p^f.$$ That means that we can transform the DE $$f' + pf = q$$ into $$f'e_0^p + pfe_0^p = qe_0^p$$ which becomes $$(f e_0^p)' = qe_0^p.$$ So $$fe_0^p = f(0)+\int_0 qe_0^p$$ which yields $$f = e_0^{-p}\left(f(0)+\int_0 qe_0^p\right)$$ Going back to the idea that numbers can be ""differentiated"" via $x' = x \log(x)$, the above technique can be applied to solve equations of the form $x \log(x)+px = q.$ In particular, multiply both sides by $e^p$. We obtain: $$x' e^p + pxe^p = qe^p$$ So $(xe^p)' = qe^p.$ To finish the problem, we can use the Lambert $W$ function and this , or just define our function and call it ""integration."" This post is getting pretty long so I might just leave it there.","Given a real number $x>0$, define it's ""derivative"" as follows: $$x' = x \log(x).$$ We can easily show that $$(xy)' = x'y+xy'$$ for all $x,y > 0$. In particular: $$(xy)' = xy \log(xy) = xy(\log(x)+\log(y)) = xy \log(x)+xy\log(y) = x'y+xy'$$ So, we started with something, namely logarithms, that turns multiplication into addition, and we obtained something, namely ""derivatives"" in the above sense, satisfying the Leibniz law, and these are related by $x'/x = \log(x)$. This suggests that we try to go the other way. Given a function $f$ on the real line, define it's ""loogarithm"" to be $$\mathrm{loog}(f) = \frac{f'}{f},$$ where $f'$ is the ordinary derivative. If we're willing to assume $f$ is positive, then the loogarithm can be understood as the act of taking the ordinary logarithm and then differentiating. In more detail: $$\mathrm{loog}(f) = (\log(f))'.$$ So basically, $\mathrm{loog}$ is the well-known technique of logarithmic differentiation. It's straightforward to check that $$\mathrm{loog}(fg) = \mathrm{loog}(f)+\mathrm{loog}(g),$$ which is what makes that technique work, after all. The above observations generalize readily to derivations on commutative rings/fields. In particular we're given a function $\log : R \rightarrow R$ on a commutative ring satisfying $\log(xy) = \log(x)+\log(y)$ and $\log(1) = 0$, then we can get a notion of differentiation on $R$ via $x' = x \log(x)$, and it will always satisfy Leibniz. Conversely, the formula $\log(x) = x'/x$ allows us to turn anything satisfying Leibniz into a notion of logarithm that's defined for all units. Question. Is there any theory surrounding this ability to go back and forth between ""logarithms"" and ""differentiation"" in this way? It seems to be rather important. What follows is a long discussion about this kind of thing. We might try inverting $\mathrm{loog}$ to get some kind of exponentiation function. Unfortunately, unlike $\log$, the loogarithm is not invertible because differentiation does funny things. Oh well, let's just add the basepoint from which we're going to start integrating as an extra parameter $p$: $$a_p^f := a^{\int_p f}.$$ This exponentiation operator occurs in the method of integrating factors . Some of it's properties are familiar looking: $$a_p^0 = 1, \qquad a_p^{f+g} = a_p^f a_p^g, \qquad a_p^{-f} = \frac{1}{a_p^f}$$ Others look a bit strange, for example: $$a^1_p(x) = a^{x-p}$$ And I haven't been able to find anything worthwhile to say about $a_p^{fg}$. The method of integrating factors works because of the statement: $$(e_p^f)' = f \cdot e_p^f.$$ That means that we can transform the DE $$f' + pf = q$$ into $$f'e_0^p + pfe_0^p = qe_0^p$$ which becomes $$(f e_0^p)' = qe_0^p.$$ So $$fe_0^p = f(0)+\int_0 qe_0^p$$ which yields $$f = e_0^{-p}\left(f(0)+\int_0 qe_0^p\right)$$ Going back to the idea that numbers can be ""differentiated"" via $x' = x \log(x)$, the above technique can be applied to solve equations of the form $x \log(x)+px = q.$ In particular, multiply both sides by $e^p$. We obtain: $$x' e^p + pxe^p = qe^p$$ So $(xe^p)' = qe^p.$ To finish the problem, we can use the Lambert $W$ function and this , or just define our function and call it ""integration."" This post is getting pretty long so I might just leave it there.",,"['calculus', 'abstract-algebra', 'ordinary-differential-equations', 'ring-theory', 'commutative-algebra']"
80,Henon's trick on ODEs without using differential notation,Henon's trick on ODEs without using differential notation,,"Henon's trick [1] is notationally easy to follow when using the following notation: $$ \begin{align} \frac{dx_1}{dt}&=f_1(x_1,\cdots,x_N) \\ &\vdots\\ \frac{dx_N}{dt}&=f_N(x_1,\cdots,x_n) \end{align} $$ For example, dividing through the first $N-1$ equation with the last equation, and taking the inverse of the last equation gives: $$ \begin{align} \frac{dx_1}{dx_N}&=\frac{f_1}{f_N}(x_1,\cdots,x_N) \\ &\vdots\\ \frac{dx_{N-1}}{dx_N}&=\frac{f_{N-1}}{f_N}(x_1,\cdots,x_n)\\ \frac{dt}{dx_N}&=\frac{1}{f_N(x_1,\cdots,x_n)} \end{align} $$ (where $\frac{f_1}{f_2}(x) = \frac{f_1(x)}{f_2(x)}$) The new system is another ODE, where the integration happens over $x_N$, and not $t$. Though it's cumbersome, I prefer to write an ODE as $$ \begin{align} x_1'(t)&=f_1(x_1(t),\cdots,x_N(t)) \\ &\vdots\\ x_n'(t)&=f_N(x_1(t),\cdots,x_n(t)) \end{align} $$ (where there is nothing that resembles a differential) Pattern matching, after applying Henon's trick we should get $$ \begin{align} \bar{x_1}'(\bar{x_N})&=\frac{f_1}{f_N}(\bar{x_1}(\bar{x_N}),\cdots,\bar{x_{N-1}}(\bar{x_N}),\bar{x_N}) \\ &\vdots\\ \bar{x_{N-1}}(\bar{x_N})&=\frac{f_{N-1}}{f_N}(\bar{x_1}(\bar{x_N}),\cdots,\bar{x_{N-1}}(\bar{x_N}),\bar{x_N})\\ \bar{t}'(\bar{x_N})&=\frac{1}{f_N(\bar{x_1}(\bar{x_N}),\cdots,\bar{x_{N-1}}(\bar{x_N}),\bar{x_N})} \end{align} $$ (Where there are bars, because originally $t$ was an independent variable, and now it became a function of the $x_N$, as did all the other functions that solve the ODE.) Clearly my ""preferred"" notation is cumbersome to write and read, but it is explicit and mechanical. How can I understand the manipulations on the ODEs (which as far as I can tell, in the traditional notation, use differentials) in this more cumbersome notation (and what is a good name for this notation)? I imagined the Inverse Function Theorem and Implicit Function Theorem might be needed, but didn't see how. [1] Henon, M. On the numerical computation of Poincaré maps http://www.sciencedirect.com/science/article/pii/0167278982900343","Henon's trick [1] is notationally easy to follow when using the following notation: $$ \begin{align} \frac{dx_1}{dt}&=f_1(x_1,\cdots,x_N) \\ &\vdots\\ \frac{dx_N}{dt}&=f_N(x_1,\cdots,x_n) \end{align} $$ For example, dividing through the first $N-1$ equation with the last equation, and taking the inverse of the last equation gives: $$ \begin{align} \frac{dx_1}{dx_N}&=\frac{f_1}{f_N}(x_1,\cdots,x_N) \\ &\vdots\\ \frac{dx_{N-1}}{dx_N}&=\frac{f_{N-1}}{f_N}(x_1,\cdots,x_n)\\ \frac{dt}{dx_N}&=\frac{1}{f_N(x_1,\cdots,x_n)} \end{align} $$ (where $\frac{f_1}{f_2}(x) = \frac{f_1(x)}{f_2(x)}$) The new system is another ODE, where the integration happens over $x_N$, and not $t$. Though it's cumbersome, I prefer to write an ODE as $$ \begin{align} x_1'(t)&=f_1(x_1(t),\cdots,x_N(t)) \\ &\vdots\\ x_n'(t)&=f_N(x_1(t),\cdots,x_n(t)) \end{align} $$ (where there is nothing that resembles a differential) Pattern matching, after applying Henon's trick we should get $$ \begin{align} \bar{x_1}'(\bar{x_N})&=\frac{f_1}{f_N}(\bar{x_1}(\bar{x_N}),\cdots,\bar{x_{N-1}}(\bar{x_N}),\bar{x_N}) \\ &\vdots\\ \bar{x_{N-1}}(\bar{x_N})&=\frac{f_{N-1}}{f_N}(\bar{x_1}(\bar{x_N}),\cdots,\bar{x_{N-1}}(\bar{x_N}),\bar{x_N})\\ \bar{t}'(\bar{x_N})&=\frac{1}{f_N(\bar{x_1}(\bar{x_N}),\cdots,\bar{x_{N-1}}(\bar{x_N}),\bar{x_N})} \end{align} $$ (Where there are bars, because originally $t$ was an independent variable, and now it became a function of the $x_N$, as did all the other functions that solve the ODE.) Clearly my ""preferred"" notation is cumbersome to write and read, but it is explicit and mechanical. How can I understand the manipulations on the ODEs (which as far as I can tell, in the traditional notation, use differentials) in this more cumbersome notation (and what is a good name for this notation)? I imagined the Inverse Function Theorem and Implicit Function Theorem might be needed, but didn't see how. [1] Henon, M. On the numerical computation of Poincaré maps http://www.sciencedirect.com/science/article/pii/0167278982900343",,"['calculus', 'ordinary-differential-equations', 'notation']"
81,"Consider the 1D wave equation: $u_{tt} - c^2 u_{xx} = q(x,t)$, Show $u(x,t)=0$","Consider the 1D wave equation: , Show","u_{tt} - c^2 u_{xx} = q(x,t) u(x,t)=0","Consider the 1D wave equation($x \in \mathbb R$ and $t>0$): $$u_{tt} - c^2 u_{xx} = q(x,t)$$ where $q(x,t) = (1-x^2)sin(t)$ when $|x|\leq 1$ and $q(x,t)=0$ if $|x|>1$. $u(x,0)=0$ and $u_t(x,0)=0$. Here $c$ is a positive constant. Prove: $u(x,t)=0$ for $|x|>ct+1$. By D'Alembert formula, we have $u(x,t)=\frac{1}{2c} \int_{0}^{t} \int_{x-c(t-s)}^{x+c(t-s)} q(y,s)dyds$. My understanding is if we want $u=0$, we need the absolute values of end points to be greater than 1. However I tried, both $t$ and $s$ are always included. Besides, in the question, the expression of $q$ when $|x|<1$ doesn't really matter, right?","Consider the 1D wave equation($x \in \mathbb R$ and $t>0$): $$u_{tt} - c^2 u_{xx} = q(x,t)$$ where $q(x,t) = (1-x^2)sin(t)$ when $|x|\leq 1$ and $q(x,t)=0$ if $|x|>1$. $u(x,0)=0$ and $u_t(x,0)=0$. Here $c$ is a positive constant. Prove: $u(x,t)=0$ for $|x|>ct+1$. By D'Alembert formula, we have $u(x,t)=\frac{1}{2c} \int_{0}^{t} \int_{x-c(t-s)}^{x+c(t-s)} q(y,s)dyds$. My understanding is if we want $u=0$, we need the absolute values of end points to be greater than 1. However I tried, both $t$ and $s$ are always included. Besides, in the question, the expression of $q$ when $|x|<1$ doesn't really matter, right?",,"['real-analysis', 'analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'wave-equation']"
82,Why is the Picard-Fuchs equation for elliptic curves of second order?,Why is the Picard-Fuchs equation for elliptic curves of second order?,,"Consider the elliptic curve defined by a polynomial $$ F(x,y,z) = t(x^3 + y^3 + z^3) - 3xyz $$ If we consider the period integral $$ \pi(t) = \int_{\gamma} \omega(t) $$ where $\gamma$ is a $1$-cycle defined at and around $F$ for a variation of $t$. We can associate a second-order differential equation $$ \frac{d^2 \pi(t)}{dt^2} + A(t)\frac{d\pi(t)}{dt} + B(t) = 0 $$ called the Picard-Fuchs equation. Why is this differential equation second order? Does this have something to do with the real dimension of the complex variety?","Consider the elliptic curve defined by a polynomial $$ F(x,y,z) = t(x^3 + y^3 + z^3) - 3xyz $$ If we consider the period integral $$ \pi(t) = \int_{\gamma} \omega(t) $$ where $\gamma$ is a $1$-cycle defined at and around $F$ for a variation of $t$. We can associate a second-order differential equation $$ \frac{d^2 \pi(t)}{dt^2} + A(t)\frac{d\pi(t)}{dt} + B(t) = 0 $$ called the Picard-Fuchs equation. Why is this differential equation second order? Does this have something to do with the real dimension of the complex variety?",,"['ordinary-differential-equations', 'algebraic-geometry', 'algebraic-topology', 'complex-geometry', 'modular-forms']"
83,Didactic examples in linear ordinary differential equations,Didactic examples in linear ordinary differential equations,,"I believe it is very common among beginners in differential equations to feel like this field is absolutely chaotic and that there is little to no theory and only techniques which may be applicable only in some rare cases. Recently however I have come to believe that for the case of linear ordinary differential equations one can do much better than just a a bag of tools. What are the most simple instructive examples in the theory of linear   ordinary differential equations? I use ""instructive"" in the sense that they contain an instance of a general qualitative phenomenon which occurs in many different equations. For instance here is a very partial list I made for myself. All will be of the form $Ly=0$ and so i will only specify $L$. The fractional monomial $L=x \partial + c$  - regular singularities at 0 and infinity. If $c \ne 0,1$ solutions have branch points. If $c \notin \mathbb{Q}$ solution is transcendental. The exponentiated monomial $L = x^{n}\partial + c, n \in \mathbb{Z} \text{\\}\{1\} $  - Irregular singularity at zero/infinity. The Legendre equation $L=(1-x^2)\partial^2 -2x\partial+c$ - Regualr singularities at $-1,1, \infty$. One solution will be entire, the other will have singularities at $-1,+1,\infty$, non trivial monodromy representation. The Airy equation $L=\partial^2 - x$ - Irregular singularity at $\infty$, stokes phenomenon, trivial monodromy. The Bessel equation $L=\partial^2+x^{-1}\partial +(1-n^2x^{-2})$ - regular singularity at $0$ and irregular singularity at $\infty$. Stokes phenomenon and non-trivial monodromy. I'm sure the above partial list is very naive and elementary i think a a list like this written by a specialist could be of a lot of help to anyone willing to put some effort in studying said examples.","I believe it is very common among beginners in differential equations to feel like this field is absolutely chaotic and that there is little to no theory and only techniques which may be applicable only in some rare cases. Recently however I have come to believe that for the case of linear ordinary differential equations one can do much better than just a a bag of tools. What are the most simple instructive examples in the theory of linear   ordinary differential equations? I use ""instructive"" in the sense that they contain an instance of a general qualitative phenomenon which occurs in many different equations. For instance here is a very partial list I made for myself. All will be of the form $Ly=0$ and so i will only specify $L$. The fractional monomial $L=x \partial + c$  - regular singularities at 0 and infinity. If $c \ne 0,1$ solutions have branch points. If $c \notin \mathbb{Q}$ solution is transcendental. The exponentiated monomial $L = x^{n}\partial + c, n \in \mathbb{Z} \text{\\}\{1\} $  - Irregular singularity at zero/infinity. The Legendre equation $L=(1-x^2)\partial^2 -2x\partial+c$ - Regualr singularities at $-1,1, \infty$. One solution will be entire, the other will have singularities at $-1,+1,\infty$, non trivial monodromy representation. The Airy equation $L=\partial^2 - x$ - Irregular singularity at $\infty$, stokes phenomenon, trivial monodromy. The Bessel equation $L=\partial^2+x^{-1}\partial +(1-n^2x^{-2})$ - regular singularity at $0$ and irregular singularity at $\infty$. Stokes phenomenon and non-trivial monodromy. I'm sure the above partial list is very naive and elementary i think a a list like this written by a specialist could be of a lot of help to anyone willing to put some effort in studying said examples.",,['ordinary-differential-equations']
84,Methods to approach the non-linear bvp $y''(x) - \lambda e^{y(x)} - \alpha=0$,Methods to approach the non-linear bvp,y''(x) - \lambda e^{y(x)} - \alpha=0,"I am searching for (semi)-analytical methods to approach the following non-linear boundary value problem \begin{align*} y''(x) - \lambda e^{y(x)} - \alpha&=0, \qquad \lambda,\alpha>0 \\ y(0)=y(1)=0  \end{align*} I am particularly interested in the case where $\lambda$ is large. I believe an explicit analytic solution does not exist. I can find an implicit solution by reducing the order via $y'(x) = \omega(y(x)) = \pm\sqrt{\lambda e^{y}+\alpha y+c_1}$ where $c_1$ is an integration constant, but it would be helpful to have an explicit approximation. I have tried approaching the problem with the Adomian decomposition, but since in the end I am interested on large values of $\lambda$, the approximation (a polynomial proportional to $\lambda$) does not seem to converge well to the solution. I am not sure if this helps, but the related problem with $\alpha=0$  (a.k.a Liouville problem) can be solved exactly by \begin{align*} y_{\alpha=0}(x) = 2\log\left[\frac{\cos{\frac{\theta}{4}}}{\cos\left(\frac{\theta}{2}\left(\frac{1}{2}-x\right)\right)}\right] \end{align*} where the parameter $\lambda$ is related to $\theta$ via the equation $\theta = \sqrt{2\lambda} \cos{\frac{\theta}{4}}$. Here $\theta=2\pi$ corresponds to the asymptotic limit of interest $\lambda \to \infty$. Related to my remark above, $y_{\alpha=0}(x)$ reaches its maximum at $x=1/2$ where $y_{\alpha=0}(1/2)=2\log{\cos{\frac{\theta}{4}}}$, diverging logarithmically as $\theta\to2\pi$, so an approximation of $y_{\alpha=0}$ given by a power series proportional to $\lambda$ diverges faster. Any ideas?","I am searching for (semi)-analytical methods to approach the following non-linear boundary value problem \begin{align*} y''(x) - \lambda e^{y(x)} - \alpha&=0, \qquad \lambda,\alpha>0 \\ y(0)=y(1)=0  \end{align*} I am particularly interested in the case where $\lambda$ is large. I believe an explicit analytic solution does not exist. I can find an implicit solution by reducing the order via $y'(x) = \omega(y(x)) = \pm\sqrt{\lambda e^{y}+\alpha y+c_1}$ where $c_1$ is an integration constant, but it would be helpful to have an explicit approximation. I have tried approaching the problem with the Adomian decomposition, but since in the end I am interested on large values of $\lambda$, the approximation (a polynomial proportional to $\lambda$) does not seem to converge well to the solution. I am not sure if this helps, but the related problem with $\alpha=0$  (a.k.a Liouville problem) can be solved exactly by \begin{align*} y_{\alpha=0}(x) = 2\log\left[\frac{\cos{\frac{\theta}{4}}}{\cos\left(\frac{\theta}{2}\left(\frac{1}{2}-x\right)\right)}\right] \end{align*} where the parameter $\lambda$ is related to $\theta$ via the equation $\theta = \sqrt{2\lambda} \cos{\frac{\theta}{4}}$. Here $\theta=2\pi$ corresponds to the asymptotic limit of interest $\lambda \to \infty$. Related to my remark above, $y_{\alpha=0}(x)$ reaches its maximum at $x=1/2$ where $y_{\alpha=0}(1/2)=2\log{\cos{\frac{\theta}{4}}}$, diverging logarithmically as $\theta\to2\pi$, so an approximation of $y_{\alpha=0}$ given by a power series proportional to $\lambda$ diverges faster. Any ideas?",,"['ordinary-differential-equations', 'approximation', 'nonlinear-system']"
85,Equation Similar to Lamé Differential Equation,Equation Similar to Lamé Differential Equation,,"I have an equation similar to the Lamé differential equation in the Jacobi form, defined as $$\frac{d^2y}{dx^2} + (a\,\mathrm{sn}(x)^2+b)y(x)=0$$ where the function $\mathrm{sn}(x)$ is one of the Jacobi elliptic functions.  My equation takes the form $$\frac{d^2y}{dx^2} = \left[k^2-\alpha_1\left(\frac{1-\gamma_1\mathrm{sn}^2(\gamma_2 x,\beta)}{r-\gamma_1\mathrm{sn}^2(\gamma_2 x,\beta)}\right)-\alpha_2\left(\frac{r-\gamma_1\mathrm{sn}^2(\gamma_2 x,\beta)}{1-\gamma_1\mathrm{sn}^2(\gamma_2 x,\beta)}\right)^2\right]y(x)$$ where $k,r,\alpha_1,\alpha_2,\beta,\gamma_1,\gamma_2$ are all real constants. I recognize this could also be viewed as Schrodinger's equation in one dimension with a tricky non-periodic potential, so I've looked at methods to solve that too. For background on the Lamé differential equation, check Chapter XV of Higher Transcendental Functions ( http://apps.nrbook.com/bateman/Vol3.pdf ) and the last chapter in A Course of Modern Analysis ( https://archive.org/details/courseofmodernan00whit ). The goal is to find an exact closed-form solution if possible.  I believe there is some transformation I might be able to use to put the equation in a nicer form, but I can't quite find it. EDIT : When I use the transformation $z=1-\gamma_1\mathrm{sn}^2(x)$ I get something that looks like Heun's differential equation, but with slightly different forms for the polynomials. Are there any known generalizations?","I have an equation similar to the Lamé differential equation in the Jacobi form, defined as $$\frac{d^2y}{dx^2} + (a\,\mathrm{sn}(x)^2+b)y(x)=0$$ where the function $\mathrm{sn}(x)$ is one of the Jacobi elliptic functions.  My equation takes the form $$\frac{d^2y}{dx^2} = \left[k^2-\alpha_1\left(\frac{1-\gamma_1\mathrm{sn}^2(\gamma_2 x,\beta)}{r-\gamma_1\mathrm{sn}^2(\gamma_2 x,\beta)}\right)-\alpha_2\left(\frac{r-\gamma_1\mathrm{sn}^2(\gamma_2 x,\beta)}{1-\gamma_1\mathrm{sn}^2(\gamma_2 x,\beta)}\right)^2\right]y(x)$$ where $k,r,\alpha_1,\alpha_2,\beta,\gamma_1,\gamma_2$ are all real constants. I recognize this could also be viewed as Schrodinger's equation in one dimension with a tricky non-periodic potential, so I've looked at methods to solve that too. For background on the Lamé differential equation, check Chapter XV of Higher Transcendental Functions ( http://apps.nrbook.com/bateman/Vol3.pdf ) and the last chapter in A Course of Modern Analysis ( https://archive.org/details/courseofmodernan00whit ). The goal is to find an exact closed-form solution if possible.  I believe there is some transformation I might be able to use to put the equation in a nicer form, but I can't quite find it. EDIT : When I use the transformation $z=1-\gamma_1\mathrm{sn}^2(x)$ I get something that looks like Heun's differential equation, but with slightly different forms for the polynomials. Are there any known generalizations?",,"['ordinary-differential-equations', 'elliptic-functions']"
86,Differential equations and categorical logic,Differential equations and categorical logic,,"In categorical logic, we usually think of each theory as being category $\mathbf{T}$. A model of $\mathbf{T}$ is then a functor $\mathbf{T} \rightarrow \mathbf{Set}$ satisfying some appropriate conditions. And, we can change $\mathbf{Set}$ to other sufficiently-nice categories to get other notions of model. I was wondering whether or not something similar can be done for differential equations/IVP's/integral equations (etc.); can we think of these as some kind of object such that morphisms out of this object ""are"" solutions to the problem?","In categorical logic, we usually think of each theory as being category $\mathbf{T}$. A model of $\mathbf{T}$ is then a functor $\mathbf{T} \rightarrow \mathbf{Set}$ satisfying some appropriate conditions. And, we can change $\mathbf{Set}$ to other sufficiently-nice categories to get other notions of model. I was wondering whether or not something similar can be done for differential equations/IVP's/integral equations (etc.); can we think of these as some kind of object such that morphisms out of this object ""are"" solutions to the problem?",,"['ordinary-differential-equations', 'differential-geometry', 'logic', 'category-theory', 'initial-value-problems']"
87,Why is Lagrangian important?,Why is Lagrangian important?,,"In physics Lagrangian is a formulation for dynamic systems and we can deduce equation of motion using the Lagrange equation. But, mathematicaly,  does lagrangian system means something? Which are the application of this? Is it linked somehow with variational formulations? Wikipedia , in fact, does not clarify anything.","In physics Lagrangian is a formulation for dynamic systems and we can deduce equation of motion using the Lagrange equation. But, mathematicaly,  does lagrangian system means something? Which are the application of this? Is it linked somehow with variational formulations? Wikipedia , in fact, does not clarify anything.",,"['analysis', 'ordinary-differential-equations']"
88,Solving $(x^{y+1}x\ln x - x^2 y^x)y' = y^{x+2}\ln y - x^y y^2$,Solving,(x^{y+1}x\ln x - x^2 y^x)y' = y^{x+2}\ln y - x^y y^2,I had the differential equation $$(x^{y+1}x\ln x - x^2 y^x)y' = y^{x+2}\ln y - x^y y^2$$  and simplified it to $$x(xx^y(y'\ln x)+yx^y (yx^{-1})))+y(yy^x(\ln y)+xy^x(xy^{-1}y'))=0$$ But what to do next ? Any hints?,I had the differential equation $$(x^{y+1}x\ln x - x^2 y^x)y' = y^{x+2}\ln y - x^y y^2$$  and simplified it to $$x(xx^y(y'\ln x)+yx^y (yx^{-1})))+y(yy^x(\ln y)+xy^x(xy^{-1}y'))=0$$ But what to do next ? Any hints?,,[]
89,Root-finding with following set of equations,Root-finding with following set of equations,,"Having the following set of equations; \begin{align} f_1(x) &= \frac{1}{f_2(x)}\\ f_1(x) &= \int^{x_1(x)}_0 \phi(t) dt\\ f_2(x) &= \int^{x_2(x)}_0 \phi(t) dt, \end{align} I'd like to solve for $\phi$. The functions $x_1(x)$ and $x_2(x)$ are known, continuous and strictly monotonic. I could use the fundamental theorem of calculus to achieve \begin{align} \frac{df_1}{dx} &= \phi(x_1) \frac{dx_1}{dx} &\frac{df_2}{dx} &= \phi(x_2) \frac{dx_2}{dx}, \end{align} and then take the derivate of the first equation like so \begin{equation} \frac{df_1}{dx} = -\frac{1}{f_2^2} \frac{df_2}{dx}. \end{equation} But I still cannot get rid of the $f_2$ and don't know if that makes the situation any better. Is this a well-known type of mathematical problem? And is there a way to solve this or similar problems computationally, e.g. with root-finding or the finite difference method? Many thanks in advance.","Having the following set of equations; \begin{align} f_1(x) &= \frac{1}{f_2(x)}\\ f_1(x) &= \int^{x_1(x)}_0 \phi(t) dt\\ f_2(x) &= \int^{x_2(x)}_0 \phi(t) dt, \end{align} I'd like to solve for $\phi$. The functions $x_1(x)$ and $x_2(x)$ are known, continuous and strictly monotonic. I could use the fundamental theorem of calculus to achieve \begin{align} \frac{df_1}{dx} &= \phi(x_1) \frac{dx_1}{dx} &\frac{df_2}{dx} &= \phi(x_2) \frac{dx_2}{dx}, \end{align} and then take the derivate of the first equation like so \begin{equation} \frac{df_1}{dx} = -\frac{1}{f_2^2} \frac{df_2}{dx}. \end{equation} But I still cannot get rid of the $f_2$ and don't know if that makes the situation any better. Is this a well-known type of mathematical problem? And is there a way to solve this or similar problems computationally, e.g. with root-finding or the finite difference method? Many thanks in advance.",,"['ordinary-differential-equations', 'transcendental-equations']"
90,Analytic solution of the $3\times3$ symmetrical ODE system $x'_i=-x_i\cdot(x_i-\bar{x})$,Analytic solution of the  symmetrical ODE system,3\times3 x'_i=-x_i\cdot(x_i-\bar{x}),"Consider the following system for $x_1$, $x_2$, $x_3$ positive:   $$\frac{dx_{i}}{dt}=-x_{i}\left(x_{i}-\bar{x}\right)\qquad\text{where}\ \bar{x}=\frac{x_1+x_2+x_3}{3}$$ Given a starting point such that $x_1x_2x_3=1$, it is easy to show that the system remains on the surface $x_1x_2x_3=1$ and heads towards the stationary point $x_1=x_2=x_3=1$. Is it possible to find an analytic solution, so that one could calculate where the system is at time $t$? I've noted that the system is in generalized Lotka-Volterra form, and I've been wondering whether the system travels on a geodesic, but I don't know how to show this, or whether it would be useful.","Consider the following system for $x_1$, $x_2$, $x_3$ positive:   $$\frac{dx_{i}}{dt}=-x_{i}\left(x_{i}-\bar{x}\right)\qquad\text{where}\ \bar{x}=\frac{x_1+x_2+x_3}{3}$$ Given a starting point such that $x_1x_2x_3=1$, it is easy to show that the system remains on the surface $x_1x_2x_3=1$ and heads towards the stationary point $x_1=x_2=x_3=1$. Is it possible to find an analytic solution, so that one could calculate where the system is at time $t$? I've noted that the system is in generalized Lotka-Volterra form, and I've been wondering whether the system travels on a geodesic, but I don't know how to show this, or whether it would be useful.",,"['ordinary-differential-equations', 'systems-of-equations', 'dynamical-systems', 'nonlinear-system', 'geodesic']"
91,Curious hyperbolic metric interesting?,Curious hyperbolic metric interesting?,,"Knowing that distance in polar coordinates is minimized by metric $ ds^2 = (r^2 + r^{\prime 2})\,d\theta^2 $ as representing a straight line geodesic in the plane, I was curious to know about the metric: $$ ds^2 = (r^2 - r^{\prime 2}) \, d\theta^2 \tag{0}$$ $$ s = \int \sqrt{ r^2 - r^{\prime 2}} d\theta  \tag{1}$$ (Primed with respect to $\theta$). With Euler-Lagrange equation in Calculus of variations  we find solutions $$ F = \sqrt{ r^2 - r^{\prime 2}} \tag{2}$$ $$ F - r^{\prime}\,\partial F/ \partial r^{\prime} = const \tag{3}$$ $$ \frac{r^2}{\sqrt{ r^2 - r^{\prime 2}}} = const. \tag{4} $$ $$ r^{\prime \prime }  = 2 r^{\prime 2}/r - r  \tag{5}$$ Please note that by a simple change of sign of above, the straight line  can be obtained by integration with constants $ (p,\alpha) $  in polar form as: $$ r^{\prime \prime }  = 2 r^{\prime 2}/r + r, \quad p =  r \cos(\theta- \alpha)  \tag{6}$$ Now (5) integrates to : $$ 1/r = e^\theta /2a + e^ {-\theta}/2b \tag{7}$$ $(a,b) $ are arbitrary constants. When $a=b,\, r = a\, sech \,\theta $ This is a spiral falling to the origin, the projection is exactly the same as the polar projection of the central (Beltrami) pseudosphere asymptotic line or geodesic. So the metric is fundamentally and qualitatively a hyperbolic metric . It remains to be shown that the lines could be hyperbolcally geodesic as well in $\mathbb R^3 $ in this particular case. An integrand of differential equation gives plot of the curve with B.C. $ r(0) =2, r^{\prime }(0) = 0 $ with plot: The condition $ a=b$ implies hyperbolic geodesics on Beltrami pseudosphere. When $ a\ne b$ typically meridian resembles a hypo pseudosphere containing the curious metric. $ \phi =\psi$ (slope and inclination to meridian) for the former and $ \phi +\psi = \pi/2  $ for meridian shown. I expected in a not too wild and off-tangent imagination to mirror a skewed Pythagorean relation ( hyperbolic?) $ c^2 = (a^2-b^2) $ of equation tagged (0) geometrical relation being valid and should show up somewhere, but I cannot recognize it anywhere. Sorry for the subjective nature of query, I am posting it despite whatever vagueness that is going with it ... and I do appreciate not down-voting as stimulus to such research inquiries..","Knowing that distance in polar coordinates is minimized by metric $ ds^2 = (r^2 + r^{\prime 2})\,d\theta^2 $ as representing a straight line geodesic in the plane, I was curious to know about the metric: $$ ds^2 = (r^2 - r^{\prime 2}) \, d\theta^2 \tag{0}$$ $$ s = \int \sqrt{ r^2 - r^{\prime 2}} d\theta  \tag{1}$$ (Primed with respect to $\theta$). With Euler-Lagrange equation in Calculus of variations  we find solutions $$ F = \sqrt{ r^2 - r^{\prime 2}} \tag{2}$$ $$ F - r^{\prime}\,\partial F/ \partial r^{\prime} = const \tag{3}$$ $$ \frac{r^2}{\sqrt{ r^2 - r^{\prime 2}}} = const. \tag{4} $$ $$ r^{\prime \prime }  = 2 r^{\prime 2}/r - r  \tag{5}$$ Please note that by a simple change of sign of above, the straight line  can be obtained by integration with constants $ (p,\alpha) $  in polar form as: $$ r^{\prime \prime }  = 2 r^{\prime 2}/r + r, \quad p =  r \cos(\theta- \alpha)  \tag{6}$$ Now (5) integrates to : $$ 1/r = e^\theta /2a + e^ {-\theta}/2b \tag{7}$$ $(a,b) $ are arbitrary constants. When $a=b,\, r = a\, sech \,\theta $ This is a spiral falling to the origin, the projection is exactly the same as the polar projection of the central (Beltrami) pseudosphere asymptotic line or geodesic. So the metric is fundamentally and qualitatively a hyperbolic metric . It remains to be shown that the lines could be hyperbolcally geodesic as well in $\mathbb R^3 $ in this particular case. An integrand of differential equation gives plot of the curve with B.C. $ r(0) =2, r^{\prime }(0) = 0 $ with plot: The condition $ a=b$ implies hyperbolic geodesics on Beltrami pseudosphere. When $ a\ne b$ typically meridian resembles a hypo pseudosphere containing the curious metric. $ \phi =\psi$ (slope and inclination to meridian) for the former and $ \phi +\psi = \pi/2  $ for meridian shown. I expected in a not too wild and off-tangent imagination to mirror a skewed Pythagorean relation ( hyperbolic?) $ c^2 = (a^2-b^2) $ of equation tagged (0) geometrical relation being valid and should show up somewhere, but I cannot recognize it anywhere. Sorry for the subjective nature of query, I am posting it despite whatever vagueness that is going with it ... and I do appreciate not down-voting as stimulus to such research inquiries..",,"['ordinary-differential-equations', 'calculus-of-variations', 'hyperbolic-geometry']"
92,When is the center manifold attractive and how does this dictate asymptotic behaviour?,When is the center manifold attractive and how does this dictate asymptotic behaviour?,,"Assuming some knowledge of center manifold theory, I would like more details on the following statement found on the Wikipedia page https://en.wikipedia.org/wiki/Center_manifold . ""The center manifold emergence theorem then says that the neighborhood may be chosen so that all solutions of the system staying in the neighborhood tend exponentially quickly to some solution $y(t)$ on the center manifold. That is, $x(t) = y(t) + O( e^{−\beta t} )$ as  $t \to \infty$ for some rate $\beta$."" I have never seen a statement of the center manifold theorem where the proof of this result is explicit. The attractivity part of the Theorem in Ioos and Adelmeyer (referenced by Wikipedia) is stated without proof. In Kuznetsov's book it states that a necessary condition for a center manifold to be attractive is that the unstable manifold vanishes, but not that this condition is sufficient. Guckenheimer + Holmes only state the Theorem of Henry and Carr that says if the unstable manifold vanishes and the origin is a stable solution of the reduced equation, then it is a stable solution of the whole system. In summary, I am looking for a proof of the intuitive fact that, if the unstable manifold is empty, for large time any trajectory starting in the neighborhood of the equilibrium point (taken w.l.o.g. to be the origin) approaches a solution of the reduced equation on the center manifold, regardless the nature of the origin as an equilibrium of the reduced equation. Motivating example is the system \begin{equation*} \dot{x} = A x + f(x,y), \quad \dot{y} = By +g(x,y)\end{equation*}  where $A \in \Bbb R^{2 \times 2}$ has an imaginary pair of eigenvalues and all eigenvalues of $B \in \Bbb R^{n \times n}$ have negative real part. Suppose the origin is a center of the reduced equation $\dot{x}=Ax+f(x,h(x))$ on the center manifold $y= h(x)$. Then my intuition says solutions approach a periodic solution of reduced equation that is realized as a limit cycle of the total system. When is this the case? What happens if the reduced equation admits a family of periodic orbits?","Assuming some knowledge of center manifold theory, I would like more details on the following statement found on the Wikipedia page https://en.wikipedia.org/wiki/Center_manifold . ""The center manifold emergence theorem then says that the neighborhood may be chosen so that all solutions of the system staying in the neighborhood tend exponentially quickly to some solution $y(t)$ on the center manifold. That is, $x(t) = y(t) + O( e^{−\beta t} )$ as  $t \to \infty$ for some rate $\beta$."" I have never seen a statement of the center manifold theorem where the proof of this result is explicit. The attractivity part of the Theorem in Ioos and Adelmeyer (referenced by Wikipedia) is stated without proof. In Kuznetsov's book it states that a necessary condition for a center manifold to be attractive is that the unstable manifold vanishes, but not that this condition is sufficient. Guckenheimer + Holmes only state the Theorem of Henry and Carr that says if the unstable manifold vanishes and the origin is a stable solution of the reduced equation, then it is a stable solution of the whole system. In summary, I am looking for a proof of the intuitive fact that, if the unstable manifold is empty, for large time any trajectory starting in the neighborhood of the equilibrium point (taken w.l.o.g. to be the origin) approaches a solution of the reduced equation on the center manifold, regardless the nature of the origin as an equilibrium of the reduced equation. Motivating example is the system \begin{equation*} \dot{x} = A x + f(x,y), \quad \dot{y} = By +g(x,y)\end{equation*}  where $A \in \Bbb R^{2 \times 2}$ has an imaginary pair of eigenvalues and all eigenvalues of $B \in \Bbb R^{n \times n}$ have negative real part. Suppose the origin is a center of the reduced equation $\dot{x}=Ax+f(x,h(x))$ on the center manifold $y= h(x)$. Then my intuition says solutions approach a periodic solution of reduced equation that is realized as a limit cycle of the total system. When is this the case? What happens if the reduced equation admits a family of periodic orbits?",,"['ordinary-differential-equations', 'asymptotics', 'dynamical-systems']"
93,differential-difference equation,differential-difference equation,,"In my work I've arrived at the equation: $x y'(x)+ay(x)+b y(cx-1)=0$, where $a,b,c$ are given constants ($c\ge1$). I suspect this equation has been studied before. Can anyone suggest a reference? In the special case of $c=1$, the equation reduces to $x y'(x)+ay(x)+b y(x-1)=0$ and this equation has already been studied extensively in the literature, especially on number theory.","In my work I've arrived at the equation: $x y'(x)+ay(x)+b y(cx-1)=0$, where $a,b,c$ are given constants ($c\ge1$). I suspect this equation has been studied before. Can anyone suggest a reference? In the special case of $c=1$, the equation reduces to $x y'(x)+ay(x)+b y(x-1)=0$ and this equation has already been studied extensively in the literature, especially on number theory.",,"['real-analysis', 'ordinary-differential-equations', 'recurrence-relations', 'delay-differential-equations']"
94,Sketching the global phase portrait for a version of the Lotka-Volterra system,Sketching the global phase portrait for a version of the Lotka-Volterra system,,"I'm trying to sketch the phase portrait for a version of Lotka-Volterra given by $$\begin{cases} \dot{x} = x(3-x-2y)\\ \dot{y} = y(2-x-y) \end{cases}.$$ I can sketch this just fine except for the brown curve (shown below). The fixed points are: $(0,0), (0,2), (3,0), (1,1)$. The linearised system is given by $Df(x,y) = \begin{pmatrix} 3-2x-2x & -2x \\  -y & 2-x-2y \end{pmatrix}$. So we have: $Df(0,0) = \begin{pmatrix}3 & 0 \\ 0 & 2 \end{pmatrix}$, $\lambda_1 = 3, \lambda_2 = 2, e_1 = \begin{pmatrix}1 \\ 0 \end{pmatrix}, e_2 = \begin{pmatrix}0 \\ 1 \end{pmatrix}$ $Df(0,2) = \begin{pmatrix}-1 & 0 \\ 2 & -2 \end{pmatrix}$, $\lambda_1 = -1, \lambda_2 = -2, e_1 = \begin{pmatrix}1 \\ -2 \end{pmatrix}, e_2 = \begin{pmatrix}0 \\ 1 \end{pmatrix}$ $Df(3,0) = \begin{pmatrix}3 & -6 \\ 0 & -1 \end{pmatrix}$, $\lambda_1 = -3, \lambda_2 = -1, e_1 = \begin{pmatrix}1 \\ 0 \end{pmatrix}, e_2 = \begin{pmatrix}3 \\ -1 \end{pmatrix}$ $Df(1,1) = \begin{pmatrix}-1 & -2 \\ -1 & -1 \end{pmatrix}$, $\lambda_1 = -1 + \sqrt{2}, \lambda_2 = -1-\sqrt{2}, e_1 = \begin{pmatrix}1 \\ -\frac{1}{\sqrt{2}} \end{pmatrix}, e_2 = \begin{pmatrix}1 \\ \frac{1}{\sqrt{2}} \end{pmatrix}$ So respectively we have an unstable node, a stable node, a stable node and a saddle point. I'm having difficulty sketching the stable manifold for the saddle point. I know that the manifold must contain $(1,1)$ and is tangent to $(1,1) + E^s$, where $E^s$ is the space spanned by the eigenvector $\begin{pmatrix}1 \\ \frac{1}{\sqrt{2}} \end{pmatrix}$, i.e. the manifold must be tangent to the line $y = \frac{1}{\sqrt{2}}(x-1)+1$. Here is a sketch of the phase portrait by the lecturer: Since the manifold needs to be tangent to $y = \frac{1}{\sqrt{2}}(x-1)+1$, why is the brown curve not concave downward then concave upward (rather than the way it's drawn, i.e. concave upward then concave downward)?","I'm trying to sketch the phase portrait for a version of Lotka-Volterra given by $$\begin{cases} \dot{x} = x(3-x-2y)\\ \dot{y} = y(2-x-y) \end{cases}.$$ I can sketch this just fine except for the brown curve (shown below). The fixed points are: $(0,0), (0,2), (3,0), (1,1)$. The linearised system is given by $Df(x,y) = \begin{pmatrix} 3-2x-2x & -2x \\  -y & 2-x-2y \end{pmatrix}$. So we have: $Df(0,0) = \begin{pmatrix}3 & 0 \\ 0 & 2 \end{pmatrix}$, $\lambda_1 = 3, \lambda_2 = 2, e_1 = \begin{pmatrix}1 \\ 0 \end{pmatrix}, e_2 = \begin{pmatrix}0 \\ 1 \end{pmatrix}$ $Df(0,2) = \begin{pmatrix}-1 & 0 \\ 2 & -2 \end{pmatrix}$, $\lambda_1 = -1, \lambda_2 = -2, e_1 = \begin{pmatrix}1 \\ -2 \end{pmatrix}, e_2 = \begin{pmatrix}0 \\ 1 \end{pmatrix}$ $Df(3,0) = \begin{pmatrix}3 & -6 \\ 0 & -1 \end{pmatrix}$, $\lambda_1 = -3, \lambda_2 = -1, e_1 = \begin{pmatrix}1 \\ 0 \end{pmatrix}, e_2 = \begin{pmatrix}3 \\ -1 \end{pmatrix}$ $Df(1,1) = \begin{pmatrix}-1 & -2 \\ -1 & -1 \end{pmatrix}$, $\lambda_1 = -1 + \sqrt{2}, \lambda_2 = -1-\sqrt{2}, e_1 = \begin{pmatrix}1 \\ -\frac{1}{\sqrt{2}} \end{pmatrix}, e_2 = \begin{pmatrix}1 \\ \frac{1}{\sqrt{2}} \end{pmatrix}$ So respectively we have an unstable node, a stable node, a stable node and a saddle point. I'm having difficulty sketching the stable manifold for the saddle point. I know that the manifold must contain $(1,1)$ and is tangent to $(1,1) + E^s$, where $E^s$ is the space spanned by the eigenvector $\begin{pmatrix}1 \\ \frac{1}{\sqrt{2}} \end{pmatrix}$, i.e. the manifold must be tangent to the line $y = \frac{1}{\sqrt{2}}(x-1)+1$. Here is a sketch of the phase portrait by the lecturer: Since the manifold needs to be tangent to $y = \frac{1}{\sqrt{2}}(x-1)+1$, why is the brown curve not concave downward then concave upward (rather than the way it's drawn, i.e. concave upward then concave downward)?",,"['ordinary-differential-equations', 'manifolds', 'dynamical-systems']"
95,Solution of Non-linear ODEs,Solution of Non-linear ODEs,,"I have a 3rd order non-linear differential equation.  $$ y''' + k (y \cdot y''- y\,'^2+1) =0 $$ with boundary conditions $$  \lim_{x \rightarrow \infty} \frac{y}{x}  =1 \\ y(0) = 0 \\ y'(0) = 0 $$ I know that it is difficult or may even be impossible to solve the ODE. But  is it correct to say these boundary conditions are sufficient ti solve the problem? Since, it is a 3rd order ODE and we have three boundary conditions, I think that these boundary conditions are sufficient (of course, assuming that boundary conditions are compatible with each other).","I have a 3rd order non-linear differential equation.  $$ y''' + k (y \cdot y''- y\,'^2+1) =0 $$ with boundary conditions $$  \lim_{x \rightarrow \infty} \frac{y}{x}  =1 \\ y(0) = 0 \\ y'(0) = 0 $$ I know that it is difficult or may even be impossible to solve the ODE. But  is it correct to say these boundary conditions are sufficient ti solve the problem? Since, it is a 3rd order ODE and we have three boundary conditions, I think that these boundary conditions are sufficient (of course, assuming that boundary conditions are compatible with each other).",,['ordinary-differential-equations']
96,Proofs from Ch. 1 of Arnold's ODEs,Proofs from Ch. 1 of Arnold's ODEs,,"I've started reading Vladimir Arnold's Ordinary Differential Equations on my own.  I like it so far, the only problem is that all of the exercises (as yet) are of the type ""prove $X$"" and without an instructor or TA to grade me, I have no idea if I'm proving these things correctly and rigorously.  I'm hoping you guys can let me know where I've made mistakes and where I can improve the language to sound a bit more professional. I provided relevant definitions at the bottom. Problem 2 (pg 4): Prove that a one-parameter group of transformations is a commutative group and that every mapping $g^t: M \to M$ is one-to-one. Proof: Let's start with proving that this is a commutative group: Closure: $g^rg^s=g^{r+s}$ by definition.  $r+s \in \Bbb R$, thus $g^{r+s} \in \{g^t \mid t \in \Bbb R\}$ for all $r,s \in \Bbb R$. Associativity: $(g^rg^s)g^t=g^{r+s}g^t=g^{(r+s)+t} = g^{r+(s+t)}=g^rg^{s+t}=g^r(g^sg^t)$. Identity Element: $g^0$ is the identity element because $g^tg^0 = g^{t+0} = g^t$ and $g^0g^t=g^{0+t}=g^t$. Inverse Element: Because $t\in \Bbb R$ and $\Bbb R$ is an abelian group wrt addition, there exists a $-t$ for every $t$.  Then I just show that $(g^t)^{-1}=g^{-t}$ because $g^tg^{-t} = g^{t+-t}=g^0$ and $g^{-t}g^t=g^{-t+t}=g^0$. Commutativity: $g^tg^s = g^{t+s} = g^{s+t} = g^sg^t$. Now I'll prove that every element of this group is one-to-one.  I never know if one-to-one means injective or bijective.  I'm pretty sure these functions are bijective, though, so that's what I'll try to prove. First injectivity: Because there exists an inverse element $g^{-t}$ for every mapping $g^t$ and because the image of $g^t$ is in $M$ (because the codomain is $M$), we can see that $$g^tx=g^ty \implies g^{-t}g^tx=g^{-t}g^ty \implies x=y$$ thus these functions are injective. Now surjectivity: Consider an arbitrary element $y \in M$.  I need to show that there exists an $x\in M$ s.t. $g^tx=y$.  I propose $x=g^{-t}y$ because then $g^t(g^{-t}y) = g^tg^{-t}y = g^0y=y$.  I know that $x=g^{-t}y$ can always be chosen because $g^{-t}$ exists (as proven above) and because $y\in M = \operatorname{domain}(g^{-t})$. Because $g^t$ is injective and surjective, it must be bijective.$\ \ \ \ \ \square$ Problem 3 (pg 5): Prove that there is one and only one phase curve passing through every point of phase space. Proof: To prove that there is one phase curve passing through every point of phase space means that there should exist a $t\in \Bbb R$ and an $x\in M$ s.t. $g^tx=y,\ \forall y \in M$.  But I already proved that $g^t$ is surjective on $M$ above so this is true. Next I prove that there is only one phase curve passing through every point of phase space.   I think what need to prove here is that if $y\in \{g^tx_1\}$ and $y\in \{g^tx_2\}$ then $x_2 \in \{g^tx_1\}$.  Because $y\in \{g^tx_1\}$ and $y\in \{g^tx_2\}$, there exists $t_1, t_2 \in \Bbb R$ s.t. $$g^{t_1}x_1=y=g^{t_2}x_2$$ This implies that $x_2 = g^{-t_2}g^{t_1}x_1 = g^{t_1-t_2}x_1$. Therefore $x_2 \in \{g^tx_1\}$.$\ \ \ \ \ \square$ Problem 4 (pg 5): Prove that there is one and only one integral curve passing through every point of extended phase space. Proof: First I prove that there exists an integral curve passing through every point $(t_0,x_0)\in \Bbb R \times M$.  Consider the arbitrary integral curve $\{(t,g^tx) \in \Bbb R \times M\}$.  Because $t$ is unrestricted, this really means that there exists an $x\in M$ s.t. $g^tx=x_0$ for every $x_0\in M$.  But this just means that the function $g^t$ is surjective on $M$ which was proven above. Next I prove that there is a unique integral curve passing through every point of extended phase space.  Not only is $g^t$ surjective on $M$, it is injective.  Meaning if $g^tx=x_0$ and $g^tx' = x_0$, then $x=x'$.  Thus for every $(t_0,x_0)\in \Bbb R\times M$, there is only one $x\in M$ s.t. $g^{t_0}x=x_0$.  This then means that there is only one integral curve $\{(t,g^tx)\}$ containing the point $(t_0,x_0)$.$\ \ \ \ \ \ \square$ Problem 5 (pg 5): Prove that the horizontal line $\Bbb R \times x,\ x\in M$ is an integral curve if and only if $x$ is an equilibrium position. Proof: $$\Bbb R\times x = \{(t,g^ty)\in \Bbb R\times M \mid t\in \Bbb R\ \&\ g^ty=x\}$$ But $g^ty=x$ for all $t$ means that $g^0y=x$.  Which implies $y=x$.  Which implies $g^tx=x$.  Which implies $x$ is an equilibrium position.$\ \ \ \ \ \square$ Problem 6 (pg 5): Prove that a shift $$\begin{matrix}h^s: (\Bbb R \times M) \to (\Bbb R \times M), & h^s(t,x)=(t+s,x)\end{matrix}$$ of extended phase space along the time axis carries integral curves into integral curves. Proof: Consider an arbitrary integral curve $A=\{(t,g^tx)\}$.  The image of $A$ under $h^s$ is $$\{h^s(t,g^tx)\} = \{(t+s,g^tx\} = \{(r,g^{r-s}x)\} = \{r,g^r(g^{-s}x)\} = \{r,g^ry\}$$ for $r=s+t$ and $y=g^{-s}x$.  But this is just an integral curve.  Thus $h^s$ takes integrals curves to integral curves.$\ \ \ \ \ \square$. Definition: A family $\{g^t\}$ of mappings of a set $M$ into itself, is called a one-parameter group of transformations of $M$ if $g^{t+s}=g^tg^s$ for all $s,t \in \Bbb R$ and $g^0$ is the identity mapping. Definition: A pair $(M,\{g^t\})$ consisting of a set $M$ and a one-parameter group $\{g^t\}$ of transformations of $M$ into itself is called a phase flow . Definition: The set $M$ is called the phase space of the flow. Definition: The image of $\Bbb R$ under mapping $$\begin{matrix}\varphi: \Bbb R \to M, & \varphi(t)=g^tx\end{matrix}$$ is called a phase curve of the flow $(M, \{g^t\})$. Definition: By the extended phase space of a flow $(M, \{g^t\})$ is meant the direct product $\Bbb R\times M$ (I think Arnold is using direct product to mean Cartesian product) of the real $t$-axis and the phase space $M$. Definition: The graph of the function $$\begin{matrix}\varphi: \Bbb R \to M, & \varphi(t)=g^tx\end{matrix}$$ is called an integral curve of the flow $(M, \{g^t\})$. Definition: By an equilibrium position $x\in M$ of a flow $(M,\{g^t\})$ is meant a phase point which is itself a phase curve: $$\begin{matrix}g^tx=x & \forall t\in\Bbb R\end{matrix}$$","I've started reading Vladimir Arnold's Ordinary Differential Equations on my own.  I like it so far, the only problem is that all of the exercises (as yet) are of the type ""prove $X$"" and without an instructor or TA to grade me, I have no idea if I'm proving these things correctly and rigorously.  I'm hoping you guys can let me know where I've made mistakes and where I can improve the language to sound a bit more professional. I provided relevant definitions at the bottom. Problem 2 (pg 4): Prove that a one-parameter group of transformations is a commutative group and that every mapping $g^t: M \to M$ is one-to-one. Proof: Let's start with proving that this is a commutative group: Closure: $g^rg^s=g^{r+s}$ by definition.  $r+s \in \Bbb R$, thus $g^{r+s} \in \{g^t \mid t \in \Bbb R\}$ for all $r,s \in \Bbb R$. Associativity: $(g^rg^s)g^t=g^{r+s}g^t=g^{(r+s)+t} = g^{r+(s+t)}=g^rg^{s+t}=g^r(g^sg^t)$. Identity Element: $g^0$ is the identity element because $g^tg^0 = g^{t+0} = g^t$ and $g^0g^t=g^{0+t}=g^t$. Inverse Element: Because $t\in \Bbb R$ and $\Bbb R$ is an abelian group wrt addition, there exists a $-t$ for every $t$.  Then I just show that $(g^t)^{-1}=g^{-t}$ because $g^tg^{-t} = g^{t+-t}=g^0$ and $g^{-t}g^t=g^{-t+t}=g^0$. Commutativity: $g^tg^s = g^{t+s} = g^{s+t} = g^sg^t$. Now I'll prove that every element of this group is one-to-one.  I never know if one-to-one means injective or bijective.  I'm pretty sure these functions are bijective, though, so that's what I'll try to prove. First injectivity: Because there exists an inverse element $g^{-t}$ for every mapping $g^t$ and because the image of $g^t$ is in $M$ (because the codomain is $M$), we can see that $$g^tx=g^ty \implies g^{-t}g^tx=g^{-t}g^ty \implies x=y$$ thus these functions are injective. Now surjectivity: Consider an arbitrary element $y \in M$.  I need to show that there exists an $x\in M$ s.t. $g^tx=y$.  I propose $x=g^{-t}y$ because then $g^t(g^{-t}y) = g^tg^{-t}y = g^0y=y$.  I know that $x=g^{-t}y$ can always be chosen because $g^{-t}$ exists (as proven above) and because $y\in M = \operatorname{domain}(g^{-t})$. Because $g^t$ is injective and surjective, it must be bijective.$\ \ \ \ \ \square$ Problem 3 (pg 5): Prove that there is one and only one phase curve passing through every point of phase space. Proof: To prove that there is one phase curve passing through every point of phase space means that there should exist a $t\in \Bbb R$ and an $x\in M$ s.t. $g^tx=y,\ \forall y \in M$.  But I already proved that $g^t$ is surjective on $M$ above so this is true. Next I prove that there is only one phase curve passing through every point of phase space.   I think what need to prove here is that if $y\in \{g^tx_1\}$ and $y\in \{g^tx_2\}$ then $x_2 \in \{g^tx_1\}$.  Because $y\in \{g^tx_1\}$ and $y\in \{g^tx_2\}$, there exists $t_1, t_2 \in \Bbb R$ s.t. $$g^{t_1}x_1=y=g^{t_2}x_2$$ This implies that $x_2 = g^{-t_2}g^{t_1}x_1 = g^{t_1-t_2}x_1$. Therefore $x_2 \in \{g^tx_1\}$.$\ \ \ \ \ \square$ Problem 4 (pg 5): Prove that there is one and only one integral curve passing through every point of extended phase space. Proof: First I prove that there exists an integral curve passing through every point $(t_0,x_0)\in \Bbb R \times M$.  Consider the arbitrary integral curve $\{(t,g^tx) \in \Bbb R \times M\}$.  Because $t$ is unrestricted, this really means that there exists an $x\in M$ s.t. $g^tx=x_0$ for every $x_0\in M$.  But this just means that the function $g^t$ is surjective on $M$ which was proven above. Next I prove that there is a unique integral curve passing through every point of extended phase space.  Not only is $g^t$ surjective on $M$, it is injective.  Meaning if $g^tx=x_0$ and $g^tx' = x_0$, then $x=x'$.  Thus for every $(t_0,x_0)\in \Bbb R\times M$, there is only one $x\in M$ s.t. $g^{t_0}x=x_0$.  This then means that there is only one integral curve $\{(t,g^tx)\}$ containing the point $(t_0,x_0)$.$\ \ \ \ \ \ \square$ Problem 5 (pg 5): Prove that the horizontal line $\Bbb R \times x,\ x\in M$ is an integral curve if and only if $x$ is an equilibrium position. Proof: $$\Bbb R\times x = \{(t,g^ty)\in \Bbb R\times M \mid t\in \Bbb R\ \&\ g^ty=x\}$$ But $g^ty=x$ for all $t$ means that $g^0y=x$.  Which implies $y=x$.  Which implies $g^tx=x$.  Which implies $x$ is an equilibrium position.$\ \ \ \ \ \square$ Problem 6 (pg 5): Prove that a shift $$\begin{matrix}h^s: (\Bbb R \times M) \to (\Bbb R \times M), & h^s(t,x)=(t+s,x)\end{matrix}$$ of extended phase space along the time axis carries integral curves into integral curves. Proof: Consider an arbitrary integral curve $A=\{(t,g^tx)\}$.  The image of $A$ under $h^s$ is $$\{h^s(t,g^tx)\} = \{(t+s,g^tx\} = \{(r,g^{r-s}x)\} = \{r,g^r(g^{-s}x)\} = \{r,g^ry\}$$ for $r=s+t$ and $y=g^{-s}x$.  But this is just an integral curve.  Thus $h^s$ takes integrals curves to integral curves.$\ \ \ \ \ \square$. Definition: A family $\{g^t\}$ of mappings of a set $M$ into itself, is called a one-parameter group of transformations of $M$ if $g^{t+s}=g^tg^s$ for all $s,t \in \Bbb R$ and $g^0$ is the identity mapping. Definition: A pair $(M,\{g^t\})$ consisting of a set $M$ and a one-parameter group $\{g^t\}$ of transformations of $M$ into itself is called a phase flow . Definition: The set $M$ is called the phase space of the flow. Definition: The image of $\Bbb R$ under mapping $$\begin{matrix}\varphi: \Bbb R \to M, & \varphi(t)=g^tx\end{matrix}$$ is called a phase curve of the flow $(M, \{g^t\})$. Definition: By the extended phase space of a flow $(M, \{g^t\})$ is meant the direct product $\Bbb R\times M$ (I think Arnold is using direct product to mean Cartesian product) of the real $t$-axis and the phase space $M$. Definition: The graph of the function $$\begin{matrix}\varphi: \Bbb R \to M, & \varphi(t)=g^tx\end{matrix}$$ is called an integral curve of the flow $(M, \{g^t\})$. Definition: By an equilibrium position $x\in M$ of a flow $(M,\{g^t\})$ is meant a phase point which is itself a phase curve: $$\begin{matrix}g^tx=x & \forall t\in\Bbb R\end{matrix}$$",,"['ordinary-differential-equations', 'proof-verification']"
97,Converse to Existence and Uniqueness Theorem of Differential equations,Converse to Existence and Uniqueness Theorem of Differential equations,,"One of the first big theorems you learn in ODEs is the following existence and uniqueness theorem: Let $I$ be a closed rectangle in $\mathbb{R} \times \mathbb{R}$, and let $f: I \rightarrow \mathbb{R}$ be differentiable on the interior, and also assume that $\frac{\partial f}{\partial y}$ is continuous on $I$.  Then for any point $(x_0,y_0)$ in the interior of $I$, there is a small interval $(x_0 - t, x_0 + t)$ about $x_0$ on which there exists a unique differentiable function $g: (x_0 - t, x_0 + t) \rightarrow \mathbb{R}$ such that $g'(x) = f(x,g(x))$ and $g(x_0) = y_0$. My question is, can any sort of converse to this be stated?  If not, are there any easy examples where you have a unique function $g$ satisfying the given properties, but without everything in the hypothesis holding?","One of the first big theorems you learn in ODEs is the following existence and uniqueness theorem: Let $I$ be a closed rectangle in $\mathbb{R} \times \mathbb{R}$, and let $f: I \rightarrow \mathbb{R}$ be differentiable on the interior, and also assume that $\frac{\partial f}{\partial y}$ is continuous on $I$.  Then for any point $(x_0,y_0)$ in the interior of $I$, there is a small interval $(x_0 - t, x_0 + t)$ about $x_0$ on which there exists a unique differentiable function $g: (x_0 - t, x_0 + t) \rightarrow \mathbb{R}$ such that $g'(x) = f(x,g(x))$ and $g(x_0) = y_0$. My question is, can any sort of converse to this be stated?  If not, are there any easy examples where you have a unique function $g$ satisfying the given properties, but without everything in the hypothesis holding?",,['ordinary-differential-equations']
98,Solving a Matrix DE involving the KL divergence,Solving a Matrix DE involving the KL divergence,,"If we let $U_\mu$ be a vector field that associates a direction vector $U_\mu(\pi)$ with each $\pi \in $ unit simplex. Each such vector field is associated with a system of ODEs: $$ \pi'(u) = U_\mu(\pi(u)), \quad \pi(0) = \pi \in \text{Unit Simplex} $$ I am working with the ODE: $$ U_\mu \propto \nabla H(\pi|\mu) =\pi_i'(u) = \log \frac{\pi_i(u)}{\mu_i} - \frac{1}{n} \sum_{j=1}^n \log \frac{\pi_j(u)}{\mu_j} ~~~~~i \in \{1,2,...,n \} $$ with condition: $\pi_i(0) = p_i \in$ Unit Simplex. Where $H(\pi|\mu) = \sum_{i=1}^n \pi_i \log \frac{\pi_i}{\mu_i}$. I am wondering how I could even show that a solution exists, and if it does, how I would tackle a problem like this given the dependencies of equation $i$ on all $j=1,2,..,n$? Or if this kind of equation can only be solved numerically?","If we let $U_\mu$ be a vector field that associates a direction vector $U_\mu(\pi)$ with each $\pi \in $ unit simplex. Each such vector field is associated with a system of ODEs: $$ \pi'(u) = U_\mu(\pi(u)), \quad \pi(0) = \pi \in \text{Unit Simplex} $$ I am working with the ODE: $$ U_\mu \propto \nabla H(\pi|\mu) =\pi_i'(u) = \log \frac{\pi_i(u)}{\mu_i} - \frac{1}{n} \sum_{j=1}^n \log \frac{\pi_j(u)}{\mu_j} ~~~~~i \in \{1,2,...,n \} $$ with condition: $\pi_i(0) = p_i \in$ Unit Simplex. Where $H(\pi|\mu) = \sum_{i=1}^n \pi_i \log \frac{\pi_i}{\mu_i}$. I am wondering how I could even show that a solution exists, and if it does, how I would tackle a problem like this given the dependencies of equation $i$ on all $j=1,2,..,n$? Or if this kind of equation can only be solved numerically?",,"['ordinary-differential-equations', 'information-theory']"
99,Is there some relationship between algebraic curves and partial differential equations that goes beyond classifying different PDE's,Is there some relationship between algebraic curves and partial differential equations that goes beyond classifying different PDE's,,"I ask primarily because despite not having taken that many math classes (up to two semesters of a PDE class in college), it would be very interesting if maybe we could gain intuition regarding nonlinear PDE's by looking at properties of their algebraic counterparts (i.e, from properties of parabolas for example, we could gain a better understanding of parabolic PDE's). This would extend to looking at other algebraic curves to study many other differential equations, at least, that is what I am thinking. Also, if this actually is a field of research currently, what would be a good way to get into it (books and papers to read, and possible people who are doing this kind of thing to look into)?","I ask primarily because despite not having taken that many math classes (up to two semesters of a PDE class in college), it would be very interesting if maybe we could gain intuition regarding nonlinear PDE's by looking at properties of their algebraic counterparts (i.e, from properties of parabolas for example, we could gain a better understanding of parabolic PDE's). This would extend to looking at other algebraic curves to study many other differential equations, at least, that is what I am thinking. Also, if this actually is a field of research currently, what would be a good way to get into it (books and papers to read, and possible people who are doing this kind of thing to look into)?",,"['ordinary-differential-equations', 'algebraic-geometry', 'differential-geometry', 'partial-differential-equations', 'algebraic-curves']"
