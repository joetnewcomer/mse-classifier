,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Radius of convergence continuous?,Radius of convergence continuous?,,"Let $ f: [0,1] \rightarrow \mathbb{R} $ be analytic. Let $ r_f(x) $ be the radius of convergence of $ f $ at $ x $. Is $ r_x(f) $ continuous? Alternatively, is there an $ r_{min} $ I can choose so that the power series of $ f $ about $ x $ converges in $ (x-r_{min},\;  x+ r_{min}) $ for all $ x $. Obviously if $ r_f(x) $ is continuous, then this will be true. Also does this hold in higher dimensions, ie $ f: [0,1] \times [0,1] \rightarrow \mathbb{R} $? Thank you for reading!","Let $ f: [0,1] \rightarrow \mathbb{R} $ be analytic. Let $ r_f(x) $ be the radius of convergence of $ f $ at $ x $. Is $ r_x(f) $ continuous? Alternatively, is there an $ r_{min} $ I can choose so that the power series of $ f $ about $ x $ converges in $ (x-r_{min},\;  x+ r_{min}) $ for all $ x $. Obviously if $ r_f(x) $ is continuous, then this will be true. Also does this hold in higher dimensions, ie $ f: [0,1] \times [0,1] \rightarrow \mathbb{R} $? Thank you for reading!",,"['calculus', 'real-analysis', 'analyticity']"
1,Jordan decomposition of linear functionals,Jordan decomposition of linear functionals,,"Let $X$ be a locally compact Hausdorff space. Also, let $C_0(X,\mathbb R)$ denote the vector space of such continuous functions $f:X\to\mathbb R$ that the set $\{x\in X\,|\,|f(x)|\geq\varepsilon\}$ is compact for all $\varepsilon>0$. Suppose that $I:C_0(X,\mathbb R)\to\mathbb R$ is a bounded, linear functional. Define, for any $f\in C_0(X,[0,\infty))$, $$I^+(f)\equiv\sup\{I(g)\,|\,g\in C_0(X,[0,\infty)),\,0\leq g\leq f\text{ pointwise}\}.$$ The following properties of $I^+$ are known. $I^+(f)$ is well-defined (finite) and non-negative for any $f\in C_0(X,[0,\infty))$; $I^+(cf)=cI^+(f)$ for any $f\in C_0(X,[0,\infty))$ and $c\geq0$; $I^+(f_1+f_2)=I^+(f_1)+I^+(f_2)$ for all $f_1,f_2\in C_0(X,[0,\infty))$. I am trying to prove the following Claim: If $f\in C_0(X,[0,\infty))$, then it is impossible to have both $I^+(f)>0$ and $I^+(f)>I(f)$. In other words, $I^+(f)\leq\max\{0,I(f)\}$ must hold. I tried to derive a contradiction using the supremum property, but to no avail so far. Any hint would be appreciated. UPDATE: The claim is false , as @ChristianRemling cogently pointed out. FYI, what I ultimately wanted to show is this: If the functionals $I^+$ and $I^-\equiv I^+-I$, which can be meaningfully extended to $C_0(X,\mathbb R)$ from $C_0(X,[0,\infty))$, are given by $I^+(f)=\int f\mathrm d\mu^+$ and $I^-(f)=\int f\mathrm d\mu^-$ $\forall f\in C_0(X,\mathbb R)$ for some finite Radon measures $\mu^+,\mu^-$ on $X$, then $\mu=\mu^+-\mu^-$ is precisely the Jordan decomposition of the signed measure $\mu$. While this claim happens to be true, I wanted to prove it using the false claim above in this question. The proof of the claim about the Jordan decomposition of $\mu$ is surprisingly and annoyingly elusive—for those interested, it is proved here (Proposition 13.42, pp. 279–280) .","Let $X$ be a locally compact Hausdorff space. Also, let $C_0(X,\mathbb R)$ denote the vector space of such continuous functions $f:X\to\mathbb R$ that the set $\{x\in X\,|\,|f(x)|\geq\varepsilon\}$ is compact for all $\varepsilon>0$. Suppose that $I:C_0(X,\mathbb R)\to\mathbb R$ is a bounded, linear functional. Define, for any $f\in C_0(X,[0,\infty))$, $$I^+(f)\equiv\sup\{I(g)\,|\,g\in C_0(X,[0,\infty)),\,0\leq g\leq f\text{ pointwise}\}.$$ The following properties of $I^+$ are known. $I^+(f)$ is well-defined (finite) and non-negative for any $f\in C_0(X,[0,\infty))$; $I^+(cf)=cI^+(f)$ for any $f\in C_0(X,[0,\infty))$ and $c\geq0$; $I^+(f_1+f_2)=I^+(f_1)+I^+(f_2)$ for all $f_1,f_2\in C_0(X,[0,\infty))$. I am trying to prove the following Claim: If $f\in C_0(X,[0,\infty))$, then it is impossible to have both $I^+(f)>0$ and $I^+(f)>I(f)$. In other words, $I^+(f)\leq\max\{0,I(f)\}$ must hold. I tried to derive a contradiction using the supremum property, but to no avail so far. Any hint would be appreciated. UPDATE: The claim is false , as @ChristianRemling cogently pointed out. FYI, what I ultimately wanted to show is this: If the functionals $I^+$ and $I^-\equiv I^+-I$, which can be meaningfully extended to $C_0(X,\mathbb R)$ from $C_0(X,[0,\infty))$, are given by $I^+(f)=\int f\mathrm d\mu^+$ and $I^-(f)=\int f\mathrm d\mu^-$ $\forall f\in C_0(X,\mathbb R)$ for some finite Radon measures $\mu^+,\mu^-$ on $X$, then $\mu=\mu^+-\mu^-$ is precisely the Jordan decomposition of the signed measure $\mu$. While this claim happens to be true, I wanted to prove it using the false claim above in this question. The proof of the claim about the Jordan decomposition of $\mu$ is surprisingly and annoyingly elusive—for those interested, it is proved here (Proposition 13.42, pp. 279–280) .",,"['real-analysis', 'functional-analysis', 'measure-theory']"
2,"The elementary methods to compute $\int_0^\pi\frac{e^{ix}}{x-\alpha e^{ix}}\,dx\quad;\quad\text{for}\, \alpha>0$",The elementary methods to compute,"\int_0^\pi\frac{e^{ix}}{x-\alpha e^{ix}}\,dx\quad;\quad\text{for}\, \alpha>0","How to compute the following integral using elementary methods (high school methods). \begin{equation}\int_0^\pi\frac{e^{ix}}{x-\alpha e^{ix}}\,dx\qquad;\qquad\text{for}\, \alpha>0\end{equation} Honestly, I don't know how to compute this integral. I have posted this problem in other forum and I only got a link direction to another problem but it didn't help me so that's why I post the problem here. So far I could manage to get \begin{equation} \frac{e^{ix}}{x-\alpha e^{ix}}=\frac{x\cos x-\alpha}{x^2-2\alpha x\cos x+\alpha^2}+i\frac{x\sin x}{x^2-2\alpha x\cos x+\alpha^2} \end{equation} or \begin{equation} \frac{e^{ix}}{x-\alpha e^{ix}}=\frac{1}{\alpha(\beta xe^{-ix}-1)}\qquad;\qquad\text{where}\, \beta=\frac{1}{\alpha} \end{equation} but none of them is easy to be computed. These are related questions that might help: [1] and [2] . Any help would be greatly appreciated. Thank you.","How to compute the following integral using elementary methods (high school methods). \begin{equation}\int_0^\pi\frac{e^{ix}}{x-\alpha e^{ix}}\,dx\qquad;\qquad\text{for}\, \alpha>0\end{equation} Honestly, I don't know how to compute this integral. I have posted this problem in other forum and I only got a link direction to another problem but it didn't help me so that's why I post the problem here. So far I could manage to get \begin{equation} \frac{e^{ix}}{x-\alpha e^{ix}}=\frac{x\cos x-\alpha}{x^2-2\alpha x\cos x+\alpha^2}+i\frac{x\sin x}{x^2-2\alpha x\cos x+\alpha^2} \end{equation} or \begin{equation} \frac{e^{ix}}{x-\alpha e^{ix}}=\frac{1}{\alpha(\beta xe^{-ix}-1)}\qquad;\qquad\text{where}\, \beta=\frac{1}{\alpha} \end{equation} but none of them is easy to be computed. These are related questions that might help: [1] and [2] . Any help would be greatly appreciated. Thank you.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
3,Taylor Expansions in Spherical Coordinates (Generator of Rotations),Taylor Expansions in Spherical Coordinates (Generator of Rotations),,"We can expand a smooth function $f:\mathbb{R}^3\to \mathbb{R}$ in a Taylor series: $$f((x^1,x^2,x^3)+(h^1,h^2,h^3))=f(x^1,x^2,x^3)+h_i\frac{\partial f}{\partial x^i}+h_ih_j\frac{\partial^2 f}{\partial x^i\,\partial x^j}+\cdots$$ Now suppose we write the same function in spherical coordinates: $\hat f\equiv f\circ g$, where $g:R_\theta^3\to \mathbb{R}^3$ is the spherical coordinate mapping. Why can't we then write $$\hat f((\theta^1,\theta^2,\theta^3)+(\phi^1,\phi^2,\phi^3))=\hat f(\theta^1,\theta^2,\theta^3)+\phi_i\frac{\partial \hat f}{\partial \theta^i}+\phi_i\phi_j\frac{\partial^2 \hat f}{\partial \theta^i\,\partial \theta^j}+\cdots\tag{1}$$ What's going on here? Perhaps it's the periodicity, but then shouldn't the formula be valid if $(\phi^1,\phi^2,\phi^3)$ is sufficiently small? What I'm really after is for a valid version of (1).","We can expand a smooth function $f:\mathbb{R}^3\to \mathbb{R}$ in a Taylor series: $$f((x^1,x^2,x^3)+(h^1,h^2,h^3))=f(x^1,x^2,x^3)+h_i\frac{\partial f}{\partial x^i}+h_ih_j\frac{\partial^2 f}{\partial x^i\,\partial x^j}+\cdots$$ Now suppose we write the same function in spherical coordinates: $\hat f\equiv f\circ g$, where $g:R_\theta^3\to \mathbb{R}^3$ is the spherical coordinate mapping. Why can't we then write $$\hat f((\theta^1,\theta^2,\theta^3)+(\phi^1,\phi^2,\phi^3))=\hat f(\theta^1,\theta^2,\theta^3)+\phi_i\frac{\partial \hat f}{\partial \theta^i}+\phi_i\phi_j\frac{\partial^2 \hat f}{\partial \theta^i\,\partial \theta^j}+\cdots\tag{1}$$ What's going on here? Perhaps it's the periodicity, but then shouldn't the formula be valid if $(\phi^1,\phi^2,\phi^3)$ is sufficiently small? What I'm really after is for a valid version of (1).",,"['real-analysis', 'derivatives']"
4,Uniqueness of the extension in Hahn-Kolmogorov extension theorem,Uniqueness of the extension in Hahn-Kolmogorov extension theorem,,"Statement: (Hahn-Kolmogorov theorem) Every pre-measure $\mu_0 : \mathcal{B}_0\rightarrow [0,\infty]$ on a Boolean algebra $\mathcal{B}_0$ in $X$ can be extended to a countably additive measure $\mu:\mathcal{B}\rightarrow [0,\infty]$. Outline for proof of the existence is the following, construct the outer measure $\mu^*:2^X\rightarrow[0,\infty]$ $$\mu^*(E) := \inf\{\sum_{n=1}^\infty \mu_0(E_n) : E\subset \cup_{n=1}^\infty E_n; E_n\in \mathcal{B}_0\},$$ then applying the Caratheodory extension theorem that the set of $\mu^*$-measurable sets forms a $\sigma$-algebra $\mathcal{B}$ and by restricting $\mu^*$ to $\mathcal{B}$ we get the countably additive measure $\mu:\mathcal{B} \rightarrow [0,\infty]$. At last, check that $\mathcal{B}$ contains $\mathcal{B}_0$ and $\mu(E) = \mu_0(E)$ for $E\in \mathcal{B}_0$, in another word $\mu$ extends $\mu_0$. For uniqueness , if $\mu_0$ is $\sigma$-finite, for two extensions of $\mu_0$, namely $\mu:\mathcal{B}\rightarrow [0,\infty]$ and $\mu':\mathcal{B}'\rightarrow [0,\infty]$, we have $\mu(E) = \mu'(E)$ for all $E\in \mathcal{B}\cap\mathcal{B}'$. My first question where does the non-uniqueness come from? From different outer measures? Or from the Caratheodory extension   theorem which extends the outer measure to a measure? This is not clear from the existence proof. If the outer measure is unique, is the $\sigma$-algebra generated by the $\mu^*$-measurable sets the smallest $\sigma$-algebra which the extension can be done? So all the extensions would agree on this smallest $\sigma$-algebra. Just like Lebesgue measure on Lebesgue measurable sets and Lebesgue measure on Borel sets are both extensions of the elementary measure on $\sigma$(elementary sets) , in this case, is the Borel $\sigma$-algebra the smallest $\sigma$-algebra that we can work with for the extension. To prove this, it suffices to show that given an extension $\mu:\mathcal{B}\rightarrow [0,\infty]$, for any other measure $\nu$ defined on $\mathcal{B}$ which agrees with $\mu_0$ on $\mathcal{B}_0$, we have $\nu(E) = \mu^*(E)$ for $E\in \mathcal{B}$, or to say $\nu = \mu$ on $\mathcal{B}$. Is the above argument sufficient? This is something that I read in a   note which hinted it would imply this kind of uniqueness. From what I understand, there is a measure on $\mathcal{B}\cap\mathcal{B}'$ which is an extension of $\mu_0$, since both $\mu$ and $\mu'$ can be restricted to this $\mathcal{B}\cap\mathcal{B}'$, we would get the desired result. For $\nu \leq \mu^*$, observe that for each collection $\{E_n\}$ such that $E\subset \cup_{n=1}^\infty E_n; E_n\in \mathcal{B}_0$, we have $$\sum_{n=1}^\infty \mu_0(E_n) = \sum_{n=1}^\infty \nu(E_n) \geq \nu(\cup_{n=1}^\infty E_n)\geq \nu(E),$$ since taking the infinum does not change the inequality, we have $\nu(E) \leq\mu^*(E)$. For $\nu\geq\mu^*$, first assume that $\mu^*(E) <\infty$ so that we can apply an ""$\epsilon$"" argument, then we generalize the result which uses the $\sigma$-finiteness of $\mu_0$. To prove $\nu(E)\geq\mu^*(E)$, it suffices to show $\nu(E)\geq\mu^*(E)-\epsilon$ for each $\epsilon>0$. Let $\epsilon$ be given, we know there exists disjoint $\{E_n\}, E_n \in \mathcal{B}_0$ such that  $$\sum_{n=1}^\infty \mu_0(E_n) \leq \mu^*(E) +\frac{\epsilon}{2},$$  using the fact that $\mu_0 (E_n) = \nu(E_n)$, $\nu$ is countability additive and (1), we have  $$\nu(\cup_{n=1}^\infty E_n / E) \leq \mu^*(\cup_{n=1}^\infty E_n / E)\leq \frac{\epsilon}{2}.$$ Also use the fact that  $$\nu(\cup_{n=1}^N E_n) =\mu^*(\cup_{n=1}^N E_n) $$ for each finite $N$ together with there exists $N$ such that  $$\mu^*(\cup_{n=1}^N E_n) \geq \mu^*(E) - \frac{\epsilon}{2},$$ we have $$\nu(E) \geq \nu (\cup_{n=1}^\infty E_n) -\frac{\epsilon}{2} \geq \nu (\cup_{n=1}^N E_n) -\frac{\epsilon}{2} = \mu^* (\cup_{n=1}^N E_n) -\frac{\epsilon}{2}\geq \mu^*(E) - \frac{\epsilon}{2} - \frac{\epsilon}{2}.$$ Now for the case when $\mu^*(E) = \infty$, cover $E$ with $A_n \in\mathcal{B}_0$ which all have finite $\mu_0$ measure and define $G_k = \cup_{n=1}^k A_k$, we have $$\nu (E\cap G_k) \geq \mu^*(E\cap G_k)$$ for all $k$. Letting $k$ tends to infinity, $\mu^*(E\cap G_k)$ goes to infinity, then $\nu (E\cap G_k)$ goes to infinity as well$. Last, could you guys check my proof please. Thank you very much!","Statement: (Hahn-Kolmogorov theorem) Every pre-measure $\mu_0 : \mathcal{B}_0\rightarrow [0,\infty]$ on a Boolean algebra $\mathcal{B}_0$ in $X$ can be extended to a countably additive measure $\mu:\mathcal{B}\rightarrow [0,\infty]$. Outline for proof of the existence is the following, construct the outer measure $\mu^*:2^X\rightarrow[0,\infty]$ $$\mu^*(E) := \inf\{\sum_{n=1}^\infty \mu_0(E_n) : E\subset \cup_{n=1}^\infty E_n; E_n\in \mathcal{B}_0\},$$ then applying the Caratheodory extension theorem that the set of $\mu^*$-measurable sets forms a $\sigma$-algebra $\mathcal{B}$ and by restricting $\mu^*$ to $\mathcal{B}$ we get the countably additive measure $\mu:\mathcal{B} \rightarrow [0,\infty]$. At last, check that $\mathcal{B}$ contains $\mathcal{B}_0$ and $\mu(E) = \mu_0(E)$ for $E\in \mathcal{B}_0$, in another word $\mu$ extends $\mu_0$. For uniqueness , if $\mu_0$ is $\sigma$-finite, for two extensions of $\mu_0$, namely $\mu:\mathcal{B}\rightarrow [0,\infty]$ and $\mu':\mathcal{B}'\rightarrow [0,\infty]$, we have $\mu(E) = \mu'(E)$ for all $E\in \mathcal{B}\cap\mathcal{B}'$. My first question where does the non-uniqueness come from? From different outer measures? Or from the Caratheodory extension   theorem which extends the outer measure to a measure? This is not clear from the existence proof. If the outer measure is unique, is the $\sigma$-algebra generated by the $\mu^*$-measurable sets the smallest $\sigma$-algebra which the extension can be done? So all the extensions would agree on this smallest $\sigma$-algebra. Just like Lebesgue measure on Lebesgue measurable sets and Lebesgue measure on Borel sets are both extensions of the elementary measure on $\sigma$(elementary sets) , in this case, is the Borel $\sigma$-algebra the smallest $\sigma$-algebra that we can work with for the extension. To prove this, it suffices to show that given an extension $\mu:\mathcal{B}\rightarrow [0,\infty]$, for any other measure $\nu$ defined on $\mathcal{B}$ which agrees with $\mu_0$ on $\mathcal{B}_0$, we have $\nu(E) = \mu^*(E)$ for $E\in \mathcal{B}$, or to say $\nu = \mu$ on $\mathcal{B}$. Is the above argument sufficient? This is something that I read in a   note which hinted it would imply this kind of uniqueness. From what I understand, there is a measure on $\mathcal{B}\cap\mathcal{B}'$ which is an extension of $\mu_0$, since both $\mu$ and $\mu'$ can be restricted to this $\mathcal{B}\cap\mathcal{B}'$, we would get the desired result. For $\nu \leq \mu^*$, observe that for each collection $\{E_n\}$ such that $E\subset \cup_{n=1}^\infty E_n; E_n\in \mathcal{B}_0$, we have $$\sum_{n=1}^\infty \mu_0(E_n) = \sum_{n=1}^\infty \nu(E_n) \geq \nu(\cup_{n=1}^\infty E_n)\geq \nu(E),$$ since taking the infinum does not change the inequality, we have $\nu(E) \leq\mu^*(E)$. For $\nu\geq\mu^*$, first assume that $\mu^*(E) <\infty$ so that we can apply an ""$\epsilon$"" argument, then we generalize the result which uses the $\sigma$-finiteness of $\mu_0$. To prove $\nu(E)\geq\mu^*(E)$, it suffices to show $\nu(E)\geq\mu^*(E)-\epsilon$ for each $\epsilon>0$. Let $\epsilon$ be given, we know there exists disjoint $\{E_n\}, E_n \in \mathcal{B}_0$ such that  $$\sum_{n=1}^\infty \mu_0(E_n) \leq \mu^*(E) +\frac{\epsilon}{2},$$  using the fact that $\mu_0 (E_n) = \nu(E_n)$, $\nu$ is countability additive and (1), we have  $$\nu(\cup_{n=1}^\infty E_n / E) \leq \mu^*(\cup_{n=1}^\infty E_n / E)\leq \frac{\epsilon}{2}.$$ Also use the fact that  $$\nu(\cup_{n=1}^N E_n) =\mu^*(\cup_{n=1}^N E_n) $$ for each finite $N$ together with there exists $N$ such that  $$\mu^*(\cup_{n=1}^N E_n) \geq \mu^*(E) - \frac{\epsilon}{2},$$ we have $$\nu(E) \geq \nu (\cup_{n=1}^\infty E_n) -\frac{\epsilon}{2} \geq \nu (\cup_{n=1}^N E_n) -\frac{\epsilon}{2} = \mu^* (\cup_{n=1}^N E_n) -\frac{\epsilon}{2}\geq \mu^*(E) - \frac{\epsilon}{2} - \frac{\epsilon}{2}.$$ Now for the case when $\mu^*(E) = \infty$, cover $E$ with $A_n \in\mathcal{B}_0$ which all have finite $\mu_0$ measure and define $G_k = \cup_{n=1}^k A_k$, we have $$\nu (E\cap G_k) \geq \mu^*(E\cap G_k)$$ for all $k$. Letting $k$ tends to infinity, $\mu^*(E\cap G_k)$ goes to infinity, then $\nu (E\cap G_k)$ goes to infinity as well$. Last, could you guys check my proof please. Thank you very much!",,"['real-analysis', 'measure-theory']"
5,Set of continuity points of a real function,Set of continuity points of a real function,,"I have a question about subsets $$ A \subseteq \mathbb R $$ for which there exists a function $$f : \mathbb R \to \mathbb R$$ such that the set of continuity points of $f$ is $A$. Can I characterize this kind of sets? In a topological,measurable or in some way? For example, does there exist a function continuous on $\mathbb Q$ and discontinuous on the irrationals?","I have a question about subsets $$ A \subseteq \mathbb R $$ for which there exists a function $$f : \mathbb R \to \mathbb R$$ such that the set of continuity points of $f$ is $A$. Can I characterize this kind of sets? In a topological,measurable or in some way? For example, does there exist a function continuous on $\mathbb Q$ and discontinuous on the irrationals?",,"['real-analysis', 'general-topology', 'measure-theory']"
6,"$x^x = y$, given $y$ solve for $x$ analytically",", given  solve for  analytically",x^x = y y x,"This question has been bugging me since high school where I was told ""not to be concerned with such matters"", but years later I still haven't found a satisfying answer. The question is really simple: $ x^x = y $ given $y$, where $y \in \mathbb{R}$ solve for $x$ (analytically)","This question has been bugging me since high school where I was told ""not to be concerned with such matters"", but years later I still haven't found a satisfying answer. The question is really simple: $ x^x = y $ given $y$, where $y \in \mathbb{R}$ solve for $x$ (analytically)",,['real-analysis']
7,Optimal assumptions for the differentiation of integrals,Optimal assumptions for the differentiation of integrals,,"Let us consider the following integral:  $$I(x)=\int_\Omega f(x, \omega)\, d\omega, $$ where $\Omega$ is a measure space and $f\colon \mathbb{R}\times \Omega \to \mathbb{R}$ is such that $f(x, \cdot)\in L^1(\Omega)$ for all $x$. When can we differentiate $I$? A dominated convergence argument gives the following result. Proposition . If For almost all $\omega\in \mathbb{\Omega}$, $$f(\cdot, \omega)\ \text{is differentiable;}$$ For all $x\in \mathbb{R}$, $$\frac{\partial f}{\partial x}(x, \cdot)\in L^1(\Omega);$$ There exists a function $\Theta\in L^1(\Omega)$ such that $$\left\lvert \frac{\partial f}{\partial x}(x, \omega)\right\rvert \le \Theta (\omega).$$ Then $I$ is differentiable and $$\frac{d I}{dx}=\int_\Omega\frac{\partial f}{\partial x}(x, \omega)\, d\omega.$$ (This has appeared here sometimes, for instance in this answer by Qiaochu Yuan .) My question is: Can you provide an example showing that condition 3 cannot be dropped?","Let us consider the following integral:  $$I(x)=\int_\Omega f(x, \omega)\, d\omega, $$ where $\Omega$ is a measure space and $f\colon \mathbb{R}\times \Omega \to \mathbb{R}$ is such that $f(x, \cdot)\in L^1(\Omega)$ for all $x$. When can we differentiate $I$? A dominated convergence argument gives the following result. Proposition . If For almost all $\omega\in \mathbb{\Omega}$, $$f(\cdot, \omega)\ \text{is differentiable;}$$ For all $x\in \mathbb{R}$, $$\frac{\partial f}{\partial x}(x, \cdot)\in L^1(\Omega);$$ There exists a function $\Theta\in L^1(\Omega)$ such that $$\left\lvert \frac{\partial f}{\partial x}(x, \omega)\right\rvert \le \Theta (\omega).$$ Then $I$ is differentiable and $$\frac{d I}{dx}=\int_\Omega\frac{\partial f}{\partial x}(x, \omega)\, d\omega.$$ (This has appeared here sometimes, for instance in this answer by Qiaochu Yuan .) My question is: Can you provide an example showing that condition 3 cannot be dropped?",,"['real-analysis', 'analysis', 'measure-theory']"
8,A basic question on symmetry of metric space,A basic question on symmetry of metric space,,"In the metric space definiton, the second condition for a metric i.e. symmetry (d(p,q)=d(q,p)) is present. But, I have not seen any example where this condition has been used. Can anyone give any such example Actually, in the analysis book of Rudin, all the theorems can be proved without using this property. I have not found any case where this property has been used. If anyone has any knowledge please share. What kind of results can't be proved using this property.","In the metric space definiton, the second condition for a metric i.e. symmetry (d(p,q)=d(q,p)) is present. But, I have not seen any example where this condition has been used. Can anyone give any such example Actually, in the analysis book of Rudin, all the theorems can be proved without using this property. I have not found any case where this property has been used. If anyone has any knowledge please share. What kind of results can't be proved using this property.",,['real-analysis']
9,Is there a subsequence of $a_n = n \sin(n)$ which tends to $0$? [closed],Is there a subsequence of  which tends to ? [closed],a_n = n \sin(n) 0,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question I know there is such a subsequence for $b_n = \sin(n)$. What about $a_n = n\sin(n)$?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question I know there is such a subsequence for $b_n = \sin(n)$. What about $a_n = n\sin(n)$?",,"['calculus', 'real-analysis']"
10,Integration for functions with values in a separable Banach space,Integration for functions with values in a separable Banach space,,"Let $(X,\mathcal{M},\mu)$ be a measure space, $Y$ a separable Banach space, and $L_{Y}$ the space of all $(\mathcal{M},\mathcal{B}_{Y})$-measurable maps from $X$ to $Y$ (where $\mathcal{B}$ denotes the Borel $\sigma$-algebra). Let $F_{Y}$ be the set of maps $f:X\rightarrow Y$ of the form $f(x)=\sum_{j=1}^{n}\chi_{E_{j}}(x)y_{j}$ where $n\in\mathbb{N},y_{j}\in Y,E_{j}\in\mathcal{M}$, and $\mu(E_{j})<\infty$. If $f\in L_{Y}$, since $y\mapsto||y||$ is continuous, $x\mapsto||f(x)||$ is $(\mathcal{M},\mathcal{B}_{\mathbb{R}})$-measurable, and we define $||f||_{1}=\int||f(x)||\;d\mu(x)$. FInally, let $L_{Y}^{1}=\{f\in L_{Y}:||f||_{1}<\infty\}$. This is a 6-part question in Folland's Real Analysis (problem 5.16). I've done the first two parts (which I will state below for reference) and I'm stuck with the third. (a) $L_{Y}$ is a vector space, $F_{Y}$ and $L_{Y}^{1}$ are subspaces of it, $F_{Y}\subset L_{Y}^{1}$, and $||\cdot||_{1}$ is a seminorm on $L_{Y}^{1}$ that becomes a norm if we identify two functions that are equal a.e. (b) Let $\{y_{n}\}_{n=1}^{\infty}$ be a countable dense set in $Y$. Given $\varepsilon>0$, let $B_{n}^{\varepsilon}=\{y\in Y:||y-y_{n}||<\varepsilon||y_{n}||\}$. Then $\bigcup_{n}B_{n}^{\varepsilon}\supset Y\setminus\{0\}$. (c) If $f\in L_{Y}^{1}$, there is a sequence $\{h_{n}\}\subset F_{Y}$ with $h_{n}\rightarrow f$ a.e. and $||h_{n}-f||_{1}\rightarrow 0$. (With notation as in (b), let $A_{nj}=B_{n}^{1/j}\setminus\bigcup_{m=1}^{n-1}B_{m}^{1/j}$ and $E_{nj}=f^{-1}(A_{nj})$, and consider $g_{j}=\sum_{n=1}^{\infty}y_{n}\chi_{E_{nj}}$.)","Let $(X,\mathcal{M},\mu)$ be a measure space, $Y$ a separable Banach space, and $L_{Y}$ the space of all $(\mathcal{M},\mathcal{B}_{Y})$-measurable maps from $X$ to $Y$ (where $\mathcal{B}$ denotes the Borel $\sigma$-algebra). Let $F_{Y}$ be the set of maps $f:X\rightarrow Y$ of the form $f(x)=\sum_{j=1}^{n}\chi_{E_{j}}(x)y_{j}$ where $n\in\mathbb{N},y_{j}\in Y,E_{j}\in\mathcal{M}$, and $\mu(E_{j})<\infty$. If $f\in L_{Y}$, since $y\mapsto||y||$ is continuous, $x\mapsto||f(x)||$ is $(\mathcal{M},\mathcal{B}_{\mathbb{R}})$-measurable, and we define $||f||_{1}=\int||f(x)||\;d\mu(x)$. FInally, let $L_{Y}^{1}=\{f\in L_{Y}:||f||_{1}<\infty\}$. This is a 6-part question in Folland's Real Analysis (problem 5.16). I've done the first two parts (which I will state below for reference) and I'm stuck with the third. (a) $L_{Y}$ is a vector space, $F_{Y}$ and $L_{Y}^{1}$ are subspaces of it, $F_{Y}\subset L_{Y}^{1}$, and $||\cdot||_{1}$ is a seminorm on $L_{Y}^{1}$ that becomes a norm if we identify two functions that are equal a.e. (b) Let $\{y_{n}\}_{n=1}^{\infty}$ be a countable dense set in $Y$. Given $\varepsilon>0$, let $B_{n}^{\varepsilon}=\{y\in Y:||y-y_{n}||<\varepsilon||y_{n}||\}$. Then $\bigcup_{n}B_{n}^{\varepsilon}\supset Y\setminus\{0\}$. (c) If $f\in L_{Y}^{1}$, there is a sequence $\{h_{n}\}\subset F_{Y}$ with $h_{n}\rightarrow f$ a.e. and $||h_{n}-f||_{1}\rightarrow 0$. (With notation as in (b), let $A_{nj}=B_{n}^{1/j}\setminus\bigcup_{m=1}^{n-1}B_{m}^{1/j}$ and $E_{nj}=f^{-1}(A_{nj})$, and consider $g_{j}=\sum_{n=1}^{\infty}y_{n}\chi_{E_{nj}}$.)",,"['real-analysis', 'measure-theory', 'banach-spaces']"
11,"If $\sum_{n=1}^\infty a_n$ is convergent, then $\sum_{n=1}^{\infty} a_n\sin(n)$","If  is convergent, then",\sum_{n=1}^\infty a_n \sum_{n=1}^{\infty} a_n\sin(n),"If the series $\sum_{n=1}^\infty a_n$ is convergent (absolutely convergent or conditionally convergent), then   $$ \sum_{n=1}^\infty a_n \sin(n) $$   is also convergent. Any hint? today I'm not at my best. Thanks in advance.","If the series $\sum_{n=1}^\infty a_n$ is convergent (absolutely convergent or conditionally convergent), then   $$ \sum_{n=1}^\infty a_n \sin(n) $$   is also convergent. Any hint? today I'm not at my best. Thanks in advance.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
12,"Evaluate $\int_{0}^{\infty}\frac{\alpha \sin x}{\alpha^2+x^2} \mathrm{dx},\space \alpha>0$",Evaluate,"\int_{0}^{\infty}\frac{\alpha \sin x}{\alpha^2+x^2} \mathrm{dx},\space \alpha>0","Evaluate  $$\int_{0}^{\infty}\frac{\alpha \sin x}{\alpha^2+x^2} \mathrm{dx},\space \alpha>0$$ I thought of using Feyman way, but it doesn't seem to help that much. Some hints, suggestions? Thanks.","Evaluate  $$\int_{0}^{\infty}\frac{\alpha \sin x}{\alpha^2+x^2} \mathrm{dx},\space \alpha>0$$ I thought of using Feyman way, but it doesn't seem to help that much. Some hints, suggestions? Thanks.",,"['calculus', 'real-analysis', 'integration', 'improper-integrals']"
13,"Convergence of indicator functions in $L^2[0,1]$ when $m(\limsup(E_n)\setminus \liminf(E_n)) = 0$",Convergence of indicator functions in  when,"L^2[0,1] m(\limsup(E_n)\setminus \liminf(E_n)) = 0","I am trying to solve a qualifying exam problem. I would like to know what can be said about the convergence of the indicator functions $I_{E_k}$ in $L^2[0, 1]$ when it's known that $m(\limsup E_k\setminus\liminf E_k) = 0$. My guess is that at least $I_{E_k}$ converges in $L^2$, and it seems like one way to show it is to show that $I_{E_k}$ is Cauchy, using the definitions of the lim sup and lim inf of a sequence of sets, but this is where I'm stuck. Any hints would be appreciated! Edit: Basically I'd like to make this rigorous: $$\|I_{E_k} - I_{E_l}\|^2_2 = m(E_k \Delta E_l) = m((E_k \cup E_l) \setminus (E_k \cap E_l)).$$ For $l > k$ and as $k\rightarrow\infty$, $$ \begin{align*} 0 &\leq m((E_k \cup E_l) \setminus (E_k \cap E_l)) = \lim\sup_k \int I_{(E_k \cup E_l) \setminus (E_k \cap E_l)}dm \\ &\leq \int \lim\sup_k I_{(E_k \cup E_l) \setminus (E_k \cap E_l)}dm \leq \int I_{(\lim\sup E_n \setminus \lim\sup E_n)}dm  \\ &= m(\lim\sup E_n \setminus \lim\inf E_n) = 0, \end{align*}$$ where we use Fatou's lemma when moving the lim sup inside the integral. Since $\{I_{E_k}\}$ is Cauchy, it converges.","I am trying to solve a qualifying exam problem. I would like to know what can be said about the convergence of the indicator functions $I_{E_k}$ in $L^2[0, 1]$ when it's known that $m(\limsup E_k\setminus\liminf E_k) = 0$. My guess is that at least $I_{E_k}$ converges in $L^2$, and it seems like one way to show it is to show that $I_{E_k}$ is Cauchy, using the definitions of the lim sup and lim inf of a sequence of sets, but this is where I'm stuck. Any hints would be appreciated! Edit: Basically I'd like to make this rigorous: $$\|I_{E_k} - I_{E_l}\|^2_2 = m(E_k \Delta E_l) = m((E_k \cup E_l) \setminus (E_k \cap E_l)).$$ For $l > k$ and as $k\rightarrow\infty$, $$ \begin{align*} 0 &\leq m((E_k \cup E_l) \setminus (E_k \cap E_l)) = \lim\sup_k \int I_{(E_k \cup E_l) \setminus (E_k \cap E_l)}dm \\ &\leq \int \lim\sup_k I_{(E_k \cup E_l) \setminus (E_k \cap E_l)}dm \leq \int I_{(\lim\sup E_n \setminus \lim\sup E_n)}dm  \\ &= m(\lim\sup E_n \setminus \lim\inf E_n) = 0, \end{align*}$$ where we use Fatou's lemma when moving the lim sup inside the integral. Since $\{I_{E_k}\}$ is Cauchy, it converges.",,"['real-analysis', 'analysis', 'measure-theory']"
14,Behaviour at infinity of a function in terms of first and second derivatives,Behaviour at infinity of a function in terms of first and second derivatives,,"In a paper (dealing with spectra of certain Schrodinger operators) I found the following assumption for a function $f\in C^\infty(\mathbb R^n;\mathbb R)$: there exists a constant $C>0$ and a compact set $K\subset\mathbb R^n$ such that  for $x\in\mathbb R^n\setminus K$ (i) $|\nabla f(x)|\geq \frac 1C$ (ii) $|\text{Hess}f(x)|\leq C|\nabla f(x)|^2$ I'm bit confused about this and don't find it intuitive. I understand that $f(x)= |x|^\alpha$ is ok for $\alpha\geq 1$ and it is not ok for $\alpha<1$ because (i) doesn't hold. I  would like to understand it better by finding (preferably simple,onedimensional) examples of functions that a) satisfy (i) but not (ii)     (or viceversa) b) satisfy  (i)' $|\nabla f(x)|\rightarrow  \infty$ for $|x|\rightarrow\infty$ instead of (i)  but not (ii) c)show the relation (if any) of (i) and (ii) with convexity/concavity properties. d) show the relation (if any) to $e^{-f} \in L^2$ e)  (i) and (ii) hold but not the following (ii)': there exists $\rho>0$ such that $\sup_{y\in B_{\rho}(x)}|\text{Hess}f(y)|\leq C|\nabla f(x)|^2$ with $B_\rho(x)$ the ball of radius $\rho$ centered in $x$. In general any kind of intuition and comments are appreciated, and of course also partial answers are more than welcome. Many thanks","In a paper (dealing with spectra of certain Schrodinger operators) I found the following assumption for a function $f\in C^\infty(\mathbb R^n;\mathbb R)$: there exists a constant $C>0$ and a compact set $K\subset\mathbb R^n$ such that  for $x\in\mathbb R^n\setminus K$ (i) $|\nabla f(x)|\geq \frac 1C$ (ii) $|\text{Hess}f(x)|\leq C|\nabla f(x)|^2$ I'm bit confused about this and don't find it intuitive. I understand that $f(x)= |x|^\alpha$ is ok for $\alpha\geq 1$ and it is not ok for $\alpha<1$ because (i) doesn't hold. I  would like to understand it better by finding (preferably simple,onedimensional) examples of functions that a) satisfy (i) but not (ii)     (or viceversa) b) satisfy  (i)' $|\nabla f(x)|\rightarrow  \infty$ for $|x|\rightarrow\infty$ instead of (i)  but not (ii) c)show the relation (if any) of (i) and (ii) with convexity/concavity properties. d) show the relation (if any) to $e^{-f} \in L^2$ e)  (i) and (ii) hold but not the following (ii)': there exists $\rho>0$ such that $\sup_{y\in B_{\rho}(x)}|\text{Hess}f(y)|\leq C|\nabla f(x)|^2$ with $B_\rho(x)$ the ball of radius $\rho$ centered in $x$. In general any kind of intuition and comments are appreciated, and of course also partial answers are more than welcome. Many thanks",,"['real-analysis', 'inequality', 'multivariable-calculus', 'intuition', 'convex-analysis']"
15,Compute $\int_{0}^{+\infty} e^{-y}y^a\mathrm{d} y$,Compute,\int_{0}^{+\infty} e^{-y}y^a\mathrm{d} y,Does the following integral have a finite value? How to compute it? $$\int_{0}^{+\infty} e^{-x^k}\mathrm{d} x$$ where $k$ is given and $0<k<1$. By substituting $x^k=y$ we may obtain an equivalent integral $$\int_{0}^{+\infty} e^{-y}y^a\mathrm{d} y$$ where $a>0$ is given.,Does the following integral have a finite value? How to compute it? $$\int_{0}^{+\infty} e^{-x^k}\mathrm{d} x$$ where $k$ is given and $0<k<1$. By substituting $x^k=y$ we may obtain an equivalent integral $$\int_{0}^{+\infty} e^{-y}y^a\mathrm{d} y$$ where $a>0$ is given.,,"['real-analysis', 'integration']"
16,Distinct eigenvalues for integral operator?,Distinct eigenvalues for integral operator?,,"Is there some sufficient condition on the kernel $K$ of a (say, finite rank, to simplify) integral operator $$ \mathcal K:f(x)\in L^2(\mathbb R)\mapsto \int K(x,y)f(y)dy $$ so that it has all its non-zero eigenvalues $\textbf{distinct}$ (i.e. with multiplicity one) ?","Is there some sufficient condition on the kernel $K$ of a (say, finite rank, to simplify) integral operator $$ \mathcal K:f(x)\in L^2(\mathbb R)\mapsto \int K(x,y)f(y)dy $$ so that it has all its non-zero eigenvalues $\textbf{distinct}$ (i.e. with multiplicity one) ?",,"['real-analysis', 'functional-analysis', 'eigenvalues-eigenvectors', 'operator-theory']"
17,Fourier dimension of sets of positive Lebesgue measure,Fourier dimension of sets of positive Lebesgue measure,,"Let $K$ be a compact set in $\mathbb{R}$ with positive Lebesgue measure. My question is whether there exists a probability measure $\mu$ supported on $K$ such that $\hat{\mu}(\xi)$, the Fourier-Stieltjes transform of $\mu$, has decay $O(|\xi|^{-1})$? Note that when $K=[0,1]$, we can simply take $\mu=\chi_{[0,1]}dt$, see this post . Generally, if $K$ contains an interior point, then by the same token such a probability measure trivially exists. But things become unclear to me when $K$ is a general set. Thanks!","Let $K$ be a compact set in $\mathbb{R}$ with positive Lebesgue measure. My question is whether there exists a probability measure $\mu$ supported on $K$ such that $\hat{\mu}(\xi)$, the Fourier-Stieltjes transform of $\mu$, has decay $O(|\xi|^{-1})$? Note that when $K=[0,1]$, we can simply take $\mu=\chi_{[0,1]}dt$, see this post . Generally, if $K$ contains an interior point, then by the same token such a probability measure trivially exists. But things become unclear to me when $K$ is a general set. Thanks!",,"['real-analysis', 'measure-theory', 'fourier-analysis', 'examples-counterexamples']"
18,Does the n'th derivative of a smooth transition have to have more than n+1 roots for some n? [duplicate],Does the n'th derivative of a smooth transition have to have more than n+1 roots for some n? [duplicate],,"This question already has answers here : Prove that there exists $n\in\mathbb{N}$ such that $f^{(n)}$ has at least n+1 zeros on $(-1,1)$ (3 answers) Closed 6 years ago . I'm wondering if the space $$S=\left\{f\in C^{\infty}([0,1])\ \ :\ \ f(0)=0\bigwedge f(1)=1\bigwedge\forall n\in\mathbb{N},\ f^{(n)}(0)=f^{(n)}(1)=0\right\}$$ contains an element $f_0$ such that $f_0^{(n)}$ has at most $n+1$ roots for all $n\in\mathbb{N}_0$. Intuitively I'm somewhat inclined to think it's false. Do anyone know if this holds, or how I should go on about finding out? If $f_0^{(n)}$ has $n+1$ roots, then by the mean value theorem $f_0^{(n+1)}$ has $n$ distinct roots in the interior of the support in addition to the boundary points being roots. Hence by induction $f_0^{(n)}$ must have $n+1$ roots for all $n\in\mathbb{N}$. As to whether the lower bound is non-strict, I think maybe the way to go is to assume that $(a_i)_{i\in\mathbb{N}}$ is a sequence with $a_i \in S$ and $a_i^{(i)}$ having at most $i+1$ roots. Because if those properties imply that not all derivatives of $a_i$ remain bounded as $i\to\infty$, then no such sequence can have an element of $S$ as limit, and so the lower bound must be strict. I made some numerically experiments with linear programs which indicated the existence of such a sequence where also $\sup_x a_i^{(1)}(x)<2$. So several derivatives must be considered simultaneously.","This question already has answers here : Prove that there exists $n\in\mathbb{N}$ such that $f^{(n)}$ has at least n+1 zeros on $(-1,1)$ (3 answers) Closed 6 years ago . I'm wondering if the space $$S=\left\{f\in C^{\infty}([0,1])\ \ :\ \ f(0)=0\bigwedge f(1)=1\bigwedge\forall n\in\mathbb{N},\ f^{(n)}(0)=f^{(n)}(1)=0\right\}$$ contains an element $f_0$ such that $f_0^{(n)}$ has at most $n+1$ roots for all $n\in\mathbb{N}_0$. Intuitively I'm somewhat inclined to think it's false. Do anyone know if this holds, or how I should go on about finding out? If $f_0^{(n)}$ has $n+1$ roots, then by the mean value theorem $f_0^{(n+1)}$ has $n$ distinct roots in the interior of the support in addition to the boundary points being roots. Hence by induction $f_0^{(n)}$ must have $n+1$ roots for all $n\in\mathbb{N}$. As to whether the lower bound is non-strict, I think maybe the way to go is to assume that $(a_i)_{i\in\mathbb{N}}$ is a sequence with $a_i \in S$ and $a_i^{(i)}$ having at most $i+1$ roots. Because if those properties imply that not all derivatives of $a_i$ remain bounded as $i\to\infty$, then no such sequence can have an element of $S$ as limit, and so the lower bound must be strict. I made some numerically experiments with linear programs which indicated the existence of such a sequence where also $\sup_x a_i^{(1)}(x)<2$. So several derivatives must be considered simultaneously.",,"['real-analysis', 'functions', 'derivatives']"
19,Minimum of $\frac{x^3}{x-6}$ for $x>6$ without using derivative?,Minimum of  for  without using derivative?,\frac{x^3}{x-6} x>6,"Find the minimum of $y=f(x)=\dfrac{x^3}{x-6}$ for $x>6$ . I can solve the question using derivatives but I have no any idea how to do it without them. Using derivatives, we find $x=9$ and $y_{min}=243$ . When looking for the minimum of a function, calculus is the default choice (with good reason), but it is a relatively new idea in Mathematics. Questions such as the one posed here are an interesting intellectual challenge because the obvious approach (calculus) is not the only approach. Without calculus there is no obvious approach though...","Find the minimum of for . I can solve the question using derivatives but I have no any idea how to do it without them. Using derivatives, we find and . When looking for the minimum of a function, calculus is the default choice (with good reason), but it is a relatively new idea in Mathematics. Questions such as the one posed here are an interesting intellectual challenge because the obvious approach (calculus) is not the only approach. Without calculus there is no obvious approach though...",y=f(x)=\dfrac{x^3}{x-6} x>6 x=9 y_{min}=243,"['real-analysis', 'algebra-precalculus', 'inequality', 'optimization']"
20,How to define the $0^0$? [duplicate],How to define the ? [duplicate],0^0,This question already has answers here : Closed 11 years ago . Possible Duplicate: Zero to zero power According to Wolfram Alpha : $0^0$ is indeterminate. According to google: $0^0=1$ According to my calculator: $0^0$ is undefined Is there consensus regarding $0^0$? And what makes $0^0$ so problematic?,This question already has answers here : Closed 11 years ago . Possible Duplicate: Zero to zero power According to Wolfram Alpha : $0^0$ is indeterminate. According to google: $0^0=1$ According to my calculator: $0^0$ is undefined Is there consensus regarding $0^0$? And what makes $0^0$ so problematic?,,['real-analysis']
21,Intuitive reason why $\sqrt[n]n\to 1$ as $n\to\infty$?,Intuitive reason why  as ?,\sqrt[n]n\to 1 n\to\infty,"We are aware of the limit $$ \lim_{n\to\infty}\sqrt[n]n = 1; $$ is there any geometric or otherwise intuitive reason to see why this limit holds? Edit: I am adding some context, since this question was previously put on-hold, and I think one of the main reasons was that it was poorly motivated. From theorem 8.1 of Baby Rudin, suppose the series $$ \sum_{n=0}^\infty c_nx^n $$ converges for $|x|<R$, and define $$ f(x) = \sum_{n=0}^\infty c_nx^n \qquad (|x|<R). \tag{1} $$ Among other conclusions, the function $f$ is differentiable in $(-R,R)$, and $$ f'(x) = \sum_{n=0}^\infty nc_n x^{n-1} \qquad (|x|<R). \tag{2} $$ Rudin uses the fact that $\sqrt[n]n\to 1$ as $n\to\infty$ to justify that the series in $(1)$ and the series in $(2)$ have the same radius of convergence. I recognized the limit, but it is just such a nice combination of $n$ and the $n$th-root, that I thought there ought to be some nice intuitive way to understand it, hence this question.","We are aware of the limit $$ \lim_{n\to\infty}\sqrt[n]n = 1; $$ is there any geometric or otherwise intuitive reason to see why this limit holds? Edit: I am adding some context, since this question was previously put on-hold, and I think one of the main reasons was that it was poorly motivated. From theorem 8.1 of Baby Rudin, suppose the series $$ \sum_{n=0}^\infty c_nx^n $$ converges for $|x|<R$, and define $$ f(x) = \sum_{n=0}^\infty c_nx^n \qquad (|x|<R). \tag{1} $$ Among other conclusions, the function $f$ is differentiable in $(-R,R)$, and $$ f'(x) = \sum_{n=0}^\infty nc_n x^{n-1} \qquad (|x|<R). \tag{2} $$ Rudin uses the fact that $\sqrt[n]n\to 1$ as $n\to\infty$ to justify that the series in $(1)$ and the series in $(2)$ have the same radius of convergence. I recognized the limit, but it is just such a nice combination of $n$ and the $n$th-root, that I thought there ought to be some nice intuitive way to understand it, hence this question.",,"['real-analysis', 'limits', 'intuition']"
22,Ways of showing $\sum_\limits{n=1}^{\infty}\ln(1+1/n)$ to be divergent,Ways of showing  to be divergent,\sum_\limits{n=1}^{\infty}\ln(1+1/n),"Show that the following sum is divergent   $$\sum_{n=1}^{\infty}\ln\left(1+\frac1n\right)$$ I thought to do this using Taylor series using the fact that $$ \ln\left(1+\frac1n\right)=\frac1n+O\left(\frac1{n^2}\right) $$ Which then makes it clear that $$ \sum_{n=1}^{\infty}\ln\left(1+\frac1n\right)\sim \sum_{n=1}^{\infty}\frac1n\longrightarrow \infty $$ But I feel like I overcomplicated the problem and would be interested to see some other solutions. Also, would taylor series be the way you would see that this diverges if you were not told?","Show that the following sum is divergent   $$\sum_{n=1}^{\infty}\ln\left(1+\frac1n\right)$$ I thought to do this using Taylor series using the fact that $$ \ln\left(1+\frac1n\right)=\frac1n+O\left(\frac1{n^2}\right) $$ Which then makes it clear that $$ \sum_{n=1}^{\infty}\ln\left(1+\frac1n\right)\sim \sum_{n=1}^{\infty}\frac1n\longrightarrow \infty $$ But I feel like I overcomplicated the problem and would be interested to see some other solutions. Also, would taylor series be the way you would see that this diverges if you were not told?",,"['real-analysis', 'calculus', 'sequences-and-series', 'convergence-divergence', 'logarithms']"
23,What is a natural number? [duplicate],What is a natural number? [duplicate],,"This question already has answers here : Why are integers subset of reals? (11 answers) Closed 2 years ago . According to page 25 of the book A First Course in Real Analysis , an inductive set is a set of real numbers such that $0$ is in the set and for every real number $x$ in the set, $x + 1$ is also in the set and a natural number is a real number that every inductive set contains. The problem with that definition is that it is circular because the real numbers are constructed from the natural numbers.","This question already has answers here : Why are integers subset of reals? (11 answers) Closed 2 years ago . According to page 25 of the book A First Course in Real Analysis , an inductive set is a set of real numbers such that $0$ is in the set and for every real number $x$ in the set, $x + 1$ is also in the set and a natural number is a real number that every inductive set contains. The problem with that definition is that it is circular because the real numbers are constructed from the natural numbers.",,"['real-analysis', 'elementary-number-theory', 'real-numbers', 'integers']"
24,Does a periodic function have to be bounded?,Does a periodic function have to be bounded?,,"Let a function $f$ satisfy the relation $f(x)=f(x+1)$ for all $x\in \Bbb{R}$. Should this function always be bounded? I think so, but the book doesn't. Any help will be greatly appreciated. Please note that the function need not be continuous.","Let a function $f$ satisfy the relation $f(x)=f(x+1)$ for all $x\in \Bbb{R}$. Should this function always be bounded? I think so, but the book doesn't. Any help will be greatly appreciated. Please note that the function need not be continuous.",,['real-analysis']
25,Find the limit $\lim _{n \to \infty}(n!)^{{\frac{1}{n^2}}}$ [duplicate],Find the limit  [duplicate],\lim _{n \to \infty}(n!)^{{\frac{1}{n^2}}},This question already has answers here : Using the squeeze theorem to determine a limit $\lim_{n\to\infty} (n!)^{\frac{1}{n^2}}$ (2 answers) Closed 5 years ago . Find the limit $$\lim _{n \to \infty}(n!)^{{\frac{1}{n^2}}}$$ I have tried it using Stirling's approximation and then using L'Hospital's Rule and get answer $1$ Is there any other easy method to find this limit ? Any hint. Thanks in advance.,This question already has answers here : Using the squeeze theorem to determine a limit $\lim_{n\to\infty} (n!)^{\frac{1}{n^2}}$ (2 answers) Closed 5 years ago . Find the limit I have tried it using Stirling's approximation and then using L'Hospital's Rule and get answer Is there any other easy method to find this limit ? Any hint. Thanks in advance.,\lim _{n \to \infty}(n!)^{{\frac{1}{n^2}}} 1,"['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'analysis']"
26,Is a connected set always an uncountably infinite set?,Is a connected set always an uncountably infinite set?,,"I'm trying to understand the concept of a connected set. The classic example which is presented is that of $\Bbb R$ or any interval of $\Bbb R$ with the usual topology. Moreover, I heard that to a certain extent connected sets can be considered opposite to discrete sets. They are sometimes indicated as representing the idea of a continuum. So, intuitively, shouldn't they always be uncountable sets?","I'm trying to understand the concept of a connected set. The classic example which is presented is that of or any interval of with the usual topology. Moreover, I heard that to a certain extent connected sets can be considered opposite to discrete sets. They are sometimes indicated as representing the idea of a continuum. So, intuitively, shouldn't they always be uncountable sets?",\Bbb R \Bbb R,"['real-analysis', 'general-topology', 'connectedness']"
27,Proof Check: Every Cauchy Sequence is Bounded,Proof Check: Every Cauchy Sequence is Bounded,,"Sorry if I keep asking for proof checks. I'll try to keep it to a minimum after this. I know this has a well-known proof. I understand that proof as well but I thought I'd do a proof that made sense to me and seemed, in some ways, simpler. Trouble is I'm not sure if it's totally correct. It's quite short though. I was just hoping someone could look it over and see if it is a valid proof. Thank you! Lemma : Every Cauchy sequence is bounded. Proof : Let $(a_{n})$ be Cauchy. We choose $ 0<\epsilon_{0}$. So $ \forall \; n>m\geq N_{0}$ we have that $\vert a_{n}-a_{m} \vert < \epsilon_{0}$. Therefore $(a_{n})$ is bounded for all $ m \geq N_{0} $ by $ \epsilon_{0} $. Since $ \mathbb{N}_{N_{0}}$ is finite, it is bounded. So, for all $ m<N_{0} $, $ (a_{n})$ is bounded. Therefore $(a_{n})$ is bounded. I realize I haven't said what the bounds are but I think that's sort of irrelevant. So long as we know it's bounded. Any help is much appreciated!","Sorry if I keep asking for proof checks. I'll try to keep it to a minimum after this. I know this has a well-known proof. I understand that proof as well but I thought I'd do a proof that made sense to me and seemed, in some ways, simpler. Trouble is I'm not sure if it's totally correct. It's quite short though. I was just hoping someone could look it over and see if it is a valid proof. Thank you! Lemma : Every Cauchy sequence is bounded. Proof : Let $(a_{n})$ be Cauchy. We choose $ 0<\epsilon_{0}$. So $ \forall \; n>m\geq N_{0}$ we have that $\vert a_{n}-a_{m} \vert < \epsilon_{0}$. Therefore $(a_{n})$ is bounded for all $ m \geq N_{0} $ by $ \epsilon_{0} $. Since $ \mathbb{N}_{N_{0}}$ is finite, it is bounded. So, for all $ m<N_{0} $, $ (a_{n})$ is bounded. Therefore $(a_{n})$ is bounded. I realize I haven't said what the bounds are but I think that's sort of irrelevant. So long as we know it's bounded. Any help is much appreciated!",,"['real-analysis', 'sequences-and-series', 'proof-verification']"
28,Limit of $f(x)$ given that $ f(x)/x$ is known,Limit of  given that  is known,f(x)  f(x)/x,"Given that $$ \lim_{x \to 0} \dfrac{f(x)}{x} $$ exists as a real number, I am trying to show that $\lim_{x\to0}f(x) = 0$. There is a similar question here: Limit of f(x) knowing limit of f(x)/x . But this question starts with the assumption of $$ \lim_{x \to 0} \dfrac{f(x)}{x} = 0, $$ and all I am assuming is that the limit is some real number. So the product rule for limits doesn't really work here. Or do I need to show that $$ \lim_{x \to 0} \frac{f(x)}{x} = 0 $$ and then apply the product rule?","Given that $$ \lim_{x \to 0} \dfrac{f(x)}{x} $$ exists as a real number, I am trying to show that $\lim_{x\to0}f(x) = 0$. There is a similar question here: Limit of f(x) knowing limit of f(x)/x . But this question starts with the assumption of $$ \lim_{x \to 0} \dfrac{f(x)}{x} = 0, $$ and all I am assuming is that the limit is some real number. So the product rule for limits doesn't really work here. Or do I need to show that $$ \lim_{x \to 0} \frac{f(x)}{x} = 0 $$ and then apply the product rule?",,"['calculus', 'real-analysis', 'limits', 'analysis']"
29,There exists a rational sequence that converges to $\sqrt3$,There exists a rational sequence that converges to,\sqrt3,"I got a proof of this but I am quite sure that it is not what was expected on the exam. Also, this proof seems really kludgy and non-kosher. Because of the density of the raitonals in the reals, there exists a $q\in\mathbb{Q}$ such that $\sqrt3-\frac1 n < q < \sqrt3$ For each n, let $a_n = q$.  ($\sqrt3-\frac1 n < a_n < \sqrt3$) So for n=1. There exists a $q$ such that $\sqrt3-1 < q < \sqrt3$. Choose this q for $a_1$ For n=2, choose $q$ such that $\sqrt3-\frac1 2 < q < \sqrt3$. Since $\sqrt3-\frac1 n$ converges to $\sqrt3$, $a_n$ must also converge to $\sqrt3$. So we are constructing a sequence out of things that we are only know the existence of. Also does this require the axiom of choice? Anyways, on the exam there was a hint ""consider $S = \{r\in\mathbb{Q}|r>0 \,\mathrm{and}\,r^2<3\}$"" And I am not exctly sure what to make of it other than the fact that $\sup S = \sqrt 3$ Edit: To be specific about my question, is this proof ok? And what is the standard proof (the proof that my prof was hinting at)?","I got a proof of this but I am quite sure that it is not what was expected on the exam. Also, this proof seems really kludgy and non-kosher. Because of the density of the raitonals in the reals, there exists a $q\in\mathbb{Q}$ such that $\sqrt3-\frac1 n < q < \sqrt3$ For each n, let $a_n = q$.  ($\sqrt3-\frac1 n < a_n < \sqrt3$) So for n=1. There exists a $q$ such that $\sqrt3-1 < q < \sqrt3$. Choose this q for $a_1$ For n=2, choose $q$ such that $\sqrt3-\frac1 2 < q < \sqrt3$. Since $\sqrt3-\frac1 n$ converges to $\sqrt3$, $a_n$ must also converge to $\sqrt3$. So we are constructing a sequence out of things that we are only know the existence of. Also does this require the axiom of choice? Anyways, on the exam there was a hint ""consider $S = \{r\in\mathbb{Q}|r>0 \,\mathrm{and}\,r^2<3\}$"" And I am not exctly sure what to make of it other than the fact that $\sup S = \sqrt 3$ Edit: To be specific about my question, is this proof ok? And what is the standard proof (the proof that my prof was hinting at)?",,['real-analysis']
30,Giving a specific example of a positive sequence increasing to 1 and with its partial products having a positive limit,Giving a specific example of a positive sequence increasing to 1 and with its partial products having a positive limit,,"In my real analysis class I was asked this which got me stick: Is there an example of a sequence of real positive numbers increasing to the limit 1 $ \{ a_n \}_{n=1}^{\infty} $ such that the partial products $ a_1 , a_1a_2,a_1a_2a_3,... $ converges to a positive limit? I thought about it and thought it might be true because I coud not disprove it generally but I cannot come up with an example. Woud someone please be able to provide an example if any? Thanks to all helpers.","In my real analysis class I was asked this which got me stick: Is there an example of a sequence of real positive numbers increasing to the limit 1 $ \{ a_n \}_{n=1}^{\infty} $ such that the partial products $ a_1 , a_1a_2,a_1a_2a_3,... $ converges to a positive limit? I thought about it and thought it might be true because I coud not disprove it generally but I cannot come up with an example. Woud someone please be able to provide an example if any? Thanks to all helpers.",,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
31,How to evaluate $\lim\limits_{x\to 0}\frac{1-\cos 7x}{3x^2}$?,How to evaluate ?,\lim\limits_{x\to 0}\frac{1-\cos 7x}{3x^2},Evaluate $$\lim\limits_{x\to 0}\frac{1-\cos 7x}{3x^2}.$$ I solved the problem with the Taylor series expansion of $\cos x$ . Here is my solution: $\lim\limits_{x\to 0}\frac{1-\cos 7x}{3x^2}\\ =\lim\limits_{x\to 0}\frac{1-\{1-\frac{(7x)^2}{2!}+\frac{(7x)^4}{4!}-\frac{(7x)^6}{6!}+\dots\}}{3x^2}\\ =\lim\limits_{x\to 0}\frac{x^2(\frac{7^2}{2!}-\frac{7^4x^2}{4!}+\frac{7^6x^4}{6!}-\dots)}{3x^2}\\ =\lim\limits_{x\to 0}\frac{1}{3}(\frac{7^2}{2!}-\frac{7^4x^2}{4!}+\frac{7^6x^4}{6!}-\dots)\\ =\frac {49}{6}$ Can this be solved without using the Taylor series?,Evaluate I solved the problem with the Taylor series expansion of . Here is my solution: Can this be solved without using the Taylor series?,\lim\limits_{x\to 0}\frac{1-\cos 7x}{3x^2}. \cos x \lim\limits_{x\to 0}\frac{1-\cos 7x}{3x^2}\\ =\lim\limits_{x\to 0}\frac{1-\{1-\frac{(7x)^2}{2!}+\frac{(7x)^4}{4!}-\frac{(7x)^6}{6!}+\dots\}}{3x^2}\\ =\lim\limits_{x\to 0}\frac{x^2(\frac{7^2}{2!}-\frac{7^4x^2}{4!}+\frac{7^6x^4}{6!}-\dots)}{3x^2}\\ =\lim\limits_{x\to 0}\frac{1}{3}(\frac{7^2}{2!}-\frac{7^4x^2}{4!}+\frac{7^6x^4}{6!}-\dots)\\ =\frac {49}{6},"['real-analysis', 'calculus', 'limits', 'taylor-expansion', 'limits-without-lhopital']"
32,"Where is the mistake in the argument in favor of the (erroneous) claim ""every Dedekind cut is a rational cut""?","Where is the mistake in the argument in favor of the (erroneous) claim ""every Dedekind cut is a rational cut""?",,"A cut is a set $C$ such that: (a) $C\subseteq \mathbb Q $ (b) $C \neq \emptyset $ (c) $C \neq \mathbb {Q} $ (d) for all $a, c \in \mathbb Q $ , if $c\in C$ and $a\lt c$ , then $a\in C $ (e) for all $c\in \mathbb Q$ , if $c\in C$ , then there exists $d\gt c$ such that $d\in C$ A rational cut is a cut of the form $C(r) = \{c | c\lt r, r\in \mathbb Q\}$ . I know that the statement ""every cut is a rational cut"" can be disproved by giving a counterexample. However, I can't prevent myself from finding an air of plausibility to the following argument aiming at justifying that every cut is of the form $C(r)$ . Hence my request: can you please show me where the following argument goes wrong? Step 1 : Suppose $C$ is a cut , and that $C$ is not of the form $C(r)$ for some $r \in \mathbb Q$ . This amounts to saying: (1) there is no $r$ such that for all $c$ if $c \in C$ then $c\lt r $ (2) implying that: for all $r$ it is not the case that for all $c$ if $c \in C$ then $c\lt r $ (3) implying that:   for all $ r$ there is some $c$ such that $c\in C$ and $c\geq r $ Step 2: Suppose $r\in \mathbb Q$ and $r\notin C $ . By (3) above there is some $c$ such that: $c\in C$ and $c\geq r$ . Now, if $c=r$ then $r\in C$ . Also, if $c\gt r$ then $ r\lt c$ and, by the ""cut"" definition, $r\in C$ . So the hypothesis $r \notin C$ leads to a contradiction; meaning that $\mathbb  Q\subseteq C$ , since $r$ was arbitrary. Finally we have $C\subset \mathbb Q$ and $\mathbb{Q} \subseteq C$ , a contradiction (since the first statement negates that all rationals belong to $C$ while the second asserts the same proposition). The conclusion seems to be that a Dedekind cut that is not a rational cut is not a Dedekind cut; in other words, every Dedekind cut has to be a rational cut. Note: I can see an objection to the conclusion of ""step 2"", namely that what the argument shows is that the first conjuction is false, namely $r\in \mathbb Q$ . But how to make sense of the negation of this conjunction since the set of rationals is the domain over which range all our number variables (in this argument)?","A cut is a set such that: (a) (b) (c) (d) for all , if and , then (e) for all , if , then there exists such that A rational cut is a cut of the form . I know that the statement ""every cut is a rational cut"" can be disproved by giving a counterexample. However, I can't prevent myself from finding an air of plausibility to the following argument aiming at justifying that every cut is of the form . Hence my request: can you please show me where the following argument goes wrong? Step 1 : Suppose is a cut , and that is not of the form for some . This amounts to saying: (1) there is no such that for all if then (2) implying that: for all it is not the case that for all if then (3) implying that:   for all there is some such that and Step 2: Suppose and . By (3) above there is some such that: and . Now, if then . Also, if then and, by the ""cut"" definition, . So the hypothesis leads to a contradiction; meaning that , since was arbitrary. Finally we have and , a contradiction (since the first statement negates that all rationals belong to while the second asserts the same proposition). The conclusion seems to be that a Dedekind cut that is not a rational cut is not a Dedekind cut; in other words, every Dedekind cut has to be a rational cut. Note: I can see an objection to the conclusion of ""step 2"", namely that what the argument shows is that the first conjuction is false, namely . But how to make sense of the negation of this conjunction since the set of rationals is the domain over which range all our number variables (in this argument)?","C C\subseteq \mathbb Q  C \neq \emptyset  C \neq \mathbb {Q}  a, c \in \mathbb Q  c\in C a\lt c a\in C  c\in \mathbb Q c\in C d\gt c d\in C C(r) = \{c | c\lt r, r\in \mathbb Q\} C(r) C C C(r) r \in \mathbb Q r c c \in C c\lt r  r c c \in C c\lt r   r c c\in C c\geq r  r\in \mathbb Q r\notin C  c c\in C c\geq r c=r r\in C c\gt r  r\lt c r\in C r \notin C \mathbb  Q\subseteq C r C\subset \mathbb Q \mathbb{Q} \subseteq C C r\in \mathbb Q","['real-analysis', 'proof-explanation', 'real-numbers', 'irrational-numbers', 'fake-proofs']"
33,Is this the right way to multiply series?,Is this the right way to multiply series?,,"Is $$\sum a_n \sum b_n = \sum a_nb_n$$ for finite sums? In particular, is $$\sum a_n \sum a_n = \sum a_n^2?$$ Thanks,","Is $$\sum a_n \sum b_n = \sum a_nb_n$$ for finite sums? In particular, is $$\sum a_n \sum a_n = \sum a_n^2?$$ Thanks,",,"['real-analysis', 'sequences-and-series', 'products']"
34,What does this series converge to? $\sum_{n=0}^\infty \beta(4n+1)-\beta(4n+3)$,What does this series converge to?,\sum_{n=0}^\infty \beta(4n+1)-\beta(4n+3),"I was investigating patterns in sums of Dirichlet beta function and I found out that if $$a_n=\beta(4n+1)-\beta(4n+3) $$ then the associated series seems to approach a number: $$\sum_{n=0}^\infty a_n=\beta(1)-\beta(3)+\beta(5)-\beta(7)+\beta(9)-\beta(11)+\dots=-0.18698991\dots $$ So my question is: what is the value of this series? I tried to use the integral representation of $\beta$ : $$\beta(s)=\frac{1}{\Gamma(s)}\int_0^\infty\frac{x^{s-1}e^{-x}}{1+e^{-2x}}dx $$ but then I couldn't interchange the sum and integral, since this would change the result. EDIT As @KStarGamer and @ClaudeLeibovici pointed out, result seems to be $$\sum_{n=0}^\infty a_n\stackrel{(?)}=\frac{\pi}{4}\text{sech}\left(\frac{\pi}{2}\right)-\frac12 $$ Now the question is: how to prove it?","I was investigating patterns in sums of Dirichlet beta function and I found out that if then the associated series seems to approach a number: So my question is: what is the value of this series? I tried to use the integral representation of : but then I couldn't interchange the sum and integral, since this would change the result. EDIT As @KStarGamer and @ClaudeLeibovici pointed out, result seems to be Now the question is: how to prove it?",a_n=\beta(4n+1)-\beta(4n+3)  \sum_{n=0}^\infty a_n=\beta(1)-\beta(3)+\beta(5)-\beta(7)+\beta(9)-\beta(11)+\dots=-0.18698991\dots  \beta \beta(s)=\frac{1}{\Gamma(s)}\int_0^\infty\frac{x^{s-1}e^{-x}}{1+e^{-2x}}dx  \sum_{n=0}^\infty a_n\stackrel{(?)}=\frac{\pi}{4}\text{sech}\left(\frac{\pi}{2}\right)-\frac12 ,"['real-analysis', 'calculus', 'integration', 'sequences-and-series', 'limits']"
35,"If $f'$ is bounded, then $f$ is uniformly continuous","If  is bounded, then  is uniformly continuous",f' f,"Let $f\colon(a,b) \to \mathbb R$ be differentiable and suppose that there exists an $M>0$ such that $|f'(x)| \leq M$ for all $x$ in $(a,b)$. Prove that $f$  is uniformly continuous.","Let $f\colon(a,b) \to \mathbb R$ be differentiable and suppose that there exists an $M>0$ such that $|f'(x)| \leq M$ for all $x$ in $(a,b)$. Prove that $f$  is uniformly continuous.",,"['real-analysis', 'uniform-continuity']"
36,One-sided inverse of a function,One-sided inverse of a function,,Is it possible to find an example of an one-sided inverse of a function?  other than matrix? I am trying to find such an example but having no luck. Anybody got an idea about it?,Is it possible to find an example of an one-sided inverse of a function?  other than matrix? I am trying to find such an example but having no luck. Anybody got an idea about it?,,"['real-analysis', 'functions', 'elementary-set-theory']"
37,Definition of Bilinear maps.,Definition of Bilinear maps.,,"Please I need a layman's definition of the bilinear map. Wikipedia has a brilliant one, I however don't seem to make anything of it.","Please I need a layman's definition of the bilinear map. Wikipedia has a brilliant one, I however don't seem to make anything of it.",,"['real-analysis', 'linear-algebra', 'bilinear-form']"
38,Showing that the square root is monotone,Showing that the square root is monotone,,"I've shown the existence of unique square roots of all positive rational numbers, so now I want to prove that the square root is monotone: $0<a<b$ if and only if $\sqrt{a} < \sqrt{b}$","I've shown the existence of unique square roots of all positive rational numbers, so now I want to prove that the square root is monotone: $0<a<b$ if and only if $\sqrt{a} < \sqrt{b}$",,"['real-analysis', 'analysis']"
39,"If $f$ and $g$ are integrable then is $\max\{f,g\}$? [duplicate]",If  and  are integrable then is ? [duplicate],"f g \max\{f,g\}","This question already has answers here : Closed 12 years ago . Possible Duplicate: Is the pointwise maximum of two Riemann integrable functions Riemann integrable? Let $f$ and $g$ be two integrable real functions. Is this leads that $\max\{f,g\}$ is integrable too? Any proof? Thanks",This question already has answers here : Closed 12 years ago . Possible Duplicate: Is the pointwise maximum of two Riemann integrable functions Riemann integrable? Let and be two integrable real functions. Is this leads that is integrable too? Any proof? Thanks,"f g \max\{f,g\}","['real-analysis', 'calculus', 'integration', 'analysis']"
40,Power Series with the coefficients $n!/(n^n)$,Power Series with the coefficients,n!/(n^n),"I'd be grateful if someone could tell me how to obtain the convergence radius of the aforementioned power series. Or, by Cauchy Hadamard, the limit of $(n!/(n^n))^{(1/n)}$ as n approaches infinity. Thanks, Paul I tried the quotient criteria, but since the quotient of two consecutive coefficients is not bounded by any value strictly smaller than 1, it didn't yield a result.","I'd be grateful if someone could tell me how to obtain the convergence radius of the aforementioned power series. Or, by Cauchy Hadamard, the limit of $(n!/(n^n))^{(1/n)}$ as n approaches infinity. Thanks, Paul I tried the quotient criteria, but since the quotient of two consecutive coefficients is not bounded by any value strictly smaller than 1, it didn't yield a result.",,"['real-analysis', 'convergence-divergence', 'power-series']"
41,$\lim\limits_{x\to0}\sin1/x$,,\lim\limits_{x\to0}\sin1/x,"What is $$\lim\limits_{x\rightarrow0}{\left( \sin{\frac{1}{x}}\right)} $$ ? Wolfram says ""-1 to 1"", but I don't know what that means. In fact, I thought this limit didn't exist, so what does ""-1 to 1"" mean in this context?","What is ? Wolfram says ""-1 to 1"", but I don't know what that means. In fact, I thought this limit didn't exist, so what does ""-1 to 1"" mean in this context?",\lim\limits_{x\rightarrow0}{\left( \sin{\frac{1}{x}}\right)} ,"['real-analysis', 'limits']"
42,Any neat proof that $0$ is the unique solution of the equation $4^x+9^x+25^x=6^x+10^x+15^x$?,Any neat proof that  is the unique solution of the equation ?,0 4^x+9^x+25^x=6^x+10^x+15^x,"It is obvious that both $f(x)= 4^x+9^x+25^x$ and $g(x)=6^x+10^x+15^x$ are strictly monotonic increasing functions. It is also easy to check that $0$ is a solution of the equation. Also I chart the functions, and it looks that for any $x$ , $f(x)>g(x),$ which can be somehow proof by studying the derivative of the $h(x)=f(x)-g(x)$ and showing that $(0,0)$ is an absolute minimum point for $h(x).$ However $h(x)$ is a function with a messy derivative, and is not looking easy (for me) to find the zeroes of this derivative. Does anyone know an elegant proof (maybe an elementary one, without derivatives) for this problem?","It is obvious that both and are strictly monotonic increasing functions. It is also easy to check that is a solution of the equation. Also I chart the functions, and it looks that for any , which can be somehow proof by studying the derivative of the and showing that is an absolute minimum point for However is a function with a messy derivative, and is not looking easy (for me) to find the zeroes of this derivative. Does anyone know an elegant proof (maybe an elementary one, without derivatives) for this problem?","f(x)= 4^x+9^x+25^x g(x)=6^x+10^x+15^x 0 x f(x)>g(x), h(x)=f(x)-g(x) (0,0) h(x). h(x)","['real-analysis', 'exponential-function']"
43,Question about the $\epsilon-\delta$ definition of continuity of function,Question about the  definition of continuity of function,\epsilon-\delta,"When we apply the $\epsilon -\delta$ definition of continuity of function, we normally assumed that $\epsilon$ to be small enough. However, how small should the $\epsilon$ be? Intuitively it is quite reasonable to just mention small imprecisely but if the function vibrate frequently at some point, we need to consider the $\epsilon$ really small but then i don't know how small should it be defined. What do the definition mean by small $\epsilon$?","When we apply the $\epsilon -\delta$ definition of continuity of function, we normally assumed that $\epsilon$ to be small enough. However, how small should the $\epsilon$ be? Intuitively it is quite reasonable to just mention small imprecisely but if the function vibrate frequently at some point, we need to consider the $\epsilon$ really small but then i don't know how small should it be defined. What do the definition mean by small $\epsilon$?",,"['real-analysis', 'limits']"
44,The convergence of $\sqrt {2+\sqrt {2+\sqrt {2+\ldots}}}$ [duplicate],The convergence of  [duplicate],\sqrt {2+\sqrt {2+\sqrt {2+\ldots}}},"This question already has answers here : Limit of the nested radical $x_{n+1} = \sqrt{c+x_n}$ (5 answers) Closed 9 years ago . I would like to know if this sequence converges $\sqrt {2+\sqrt {2+\sqrt {2+\ldots}}}$. I know this sequence is increasing monotone, but I couldn't prove it's bounded. Thanks","This question already has answers here : Limit of the nested radical $x_{n+1} = \sqrt{c+x_n}$ (5 answers) Closed 9 years ago . I would like to know if this sequence converges $\sqrt {2+\sqrt {2+\sqrt {2+\ldots}}}$. I know this sequence is increasing monotone, but I couldn't prove it's bounded. Thanks",,"['real-analysis', 'sequences-and-series', 'nested-radicals']"
45,How do I calculate the derivative of $x|x|$?,How do I calculate the derivative of ?,x|x|,I know that $$f(x)=x\cdot|x|$$ have no derivative at $$x=0$$ but how do I calculate it's derivative for the rest of the points? When I  calculate for $$x>0$$ I get that $$f'(x) = 2x $$ but for $$ x < 0 $$ I can't seem to find a way to solve the limit. As this is homework please don't put the answer straight forward.,I know that $$f(x)=x\cdot|x|$$ have no derivative at $$x=0$$ but how do I calculate it's derivative for the rest of the points? When I  calculate for $$x>0$$ I get that $$f'(x) = 2x $$ but for $$ x < 0 $$ I can't seem to find a way to solve the limit. As this is homework please don't put the answer straight forward.,,"['real-analysis', 'derivatives']"
46,Is there any significance to the logarithm of a sum?,Is there any significance to the logarithm of a sum?,,"Many years ago, while working as a computer programmer, I tracked down a subtle bug in the software that we were using. Management had dispaired of finding the bug, but I pursued it in odd moments over a period of a few days, and finally found that the problem was that, in computing the geometric mean, the program was taking the log of the sum instead of the sum of the logs. When thinking back on that, I always wonder whether there is any situation in which taking the log of a sum would be of interest. The only case I can think of is that it is often convenient to shift the logarithm to the left by 1 unit, which is done by adding 1 to the argument, that is, one often wishes to deal with log(1 + x) instead of log(x), so that one has the convenient situation of f(0) = 0. Let’s call this the trivial scenario. So, can anyone think of any non-trivial scenario in which taking the log of a sum is the thing to do?","Many years ago, while working as a computer programmer, I tracked down a subtle bug in the software that we were using. Management had dispaired of finding the bug, but I pursued it in odd moments over a period of a few days, and finally found that the problem was that, in computing the geometric mean, the program was taking the log of the sum instead of the sum of the logs. When thinking back on that, I always wonder whether there is any situation in which taking the log of a sum would be of interest. The only case I can think of is that it is often convenient to shift the logarithm to the left by 1 unit, which is done by adding 1 to the argument, that is, one often wishes to deal with log(1 + x) instead of log(x), so that one has the convenient situation of f(0) = 0. Let’s call this the trivial scenario. So, can anyone think of any non-trivial scenario in which taking the log of a sum is the thing to do?",,"['real-analysis', 'logarithms']"
47,Suppose $\sum_{n=1}^{\infty}\sqrt{{a_{n}}/{n}}$ is convergent. Prove that $\sum_{n=1}^{\infty}a_{n}$ is also convergent.,Suppose  is convergent. Prove that  is also convergent.,\sum_{n=1}^{\infty}\sqrt{{a_{n}}/{n}} \sum_{n=1}^{\infty}a_{n},Let $\{a_{n}\}$ be a decreasing sequence of non-negative real numbers. Suppose $\sum_{n=1}^{\infty}\sqrt{\frac{a_{n}}{n}}$ is convergent. Prove that $\sum_{n=1}^{\infty}a_{n}$ is also convergent. My thought is to use the direct comparison test but I think I'm struggling with showing that $a_{n}\leq\sqrt{\frac{a_{n}}{n}}$ $\forall$ n $\in\mathbb{N}$ . Any help would be great. Thank you!,Let be a decreasing sequence of non-negative real numbers. Suppose is convergent. Prove that is also convergent. My thought is to use the direct comparison test but I think I'm struggling with showing that n . Any help would be great. Thank you!,\{a_{n}\} \sum_{n=1}^{\infty}\sqrt{\frac{a_{n}}{n}} \sum_{n=1}^{\infty}a_{n} a_{n}\leq\sqrt{\frac{a_{n}}{n}} \forall \in\mathbb{N},"['real-analysis', 'calculus', 'convergence-divergence']"
48,Why does the distance between two members of a metric space map to a real number?,Why does the distance between two members of a metric space map to a real number?,,"Why does the distance between two members of a metric space map to a real number? The set of real numbers is itself (along with the Euclidean distance function) a metric space - in which case, real numbers are used to describe a property of real numbers (themselves).  How is that possible? (How can you use something to describe itself?)﻿","Why does the distance between two members of a metric space map to a real number? The set of real numbers is itself (along with the Euclidean distance function) a metric space - in which case, real numbers are used to describe a property of real numbers (themselves).  How is that possible? (How can you use something to describe itself?)﻿",,"['real-analysis', 'real-numbers']"
49,Deriving Taylor series without applying Taylor's theorem.,Deriving Taylor series without applying Taylor's theorem.,,"First, a neat little 'proof' of the Taylor series of $e^x$. Start by proving with L'Hospital's rule or similar that $$e^x=\lim_{n\to\infty}\left(1+\frac xn\right)^n$$ and then binomial expand into $$e^x=\lim_{n\to\infty}1+x+\frac{n-1}n\frac{x^2}2+\dots+\frac{(n-1)(n-2)(n-3)\dots(n-k+1))}{n^{k-1}}\frac{x^k}{k!}+\dots$$ Evaluating the limit, we are left with $$e^x=1+x+\frac{x^2}2+\dots+\frac{x^k}{k!}+\dots$$ which is our well known Taylor series of $e^x$. As dxiv mentions, we can exploit the geometric series: $$\frac1{1-x}=1+x+x^2+\dots$$ $$\ln(1+x)=x-\frac12x^2+\frac13x^3-\dots$$ $$\arctan(x)=x-\frac13x^3+\frac15x^5-\dots$$ which are found by integrating the geometric series and variants of it. I was wondering if other known Taylor series can be created without applying Taylor's theorem.  Specifically, can we derive the expansions of $\sin$ or $\cos$?","First, a neat little 'proof' of the Taylor series of $e^x$. Start by proving with L'Hospital's rule or similar that $$e^x=\lim_{n\to\infty}\left(1+\frac xn\right)^n$$ and then binomial expand into $$e^x=\lim_{n\to\infty}1+x+\frac{n-1}n\frac{x^2}2+\dots+\frac{(n-1)(n-2)(n-3)\dots(n-k+1))}{n^{k-1}}\frac{x^k}{k!}+\dots$$ Evaluating the limit, we are left with $$e^x=1+x+\frac{x^2}2+\dots+\frac{x^k}{k!}+\dots$$ which is our well known Taylor series of $e^x$. As dxiv mentions, we can exploit the geometric series: $$\frac1{1-x}=1+x+x^2+\dots$$ $$\ln(1+x)=x-\frac12x^2+\frac13x^3-\dots$$ $$\arctan(x)=x-\frac13x^3+\frac15x^5-\dots$$ which are found by integrating the geometric series and variants of it. I was wondering if other known Taylor series can be created without applying Taylor's theorem.  Specifically, can we derive the expansions of $\sin$ or $\cos$?",,"['calculus', 'real-analysis', 'taylor-expansion', 'recreational-mathematics']"
50,Integral of an increasing function is convex?,Integral of an increasing function is convex?,,"Let $f$ be a real valued differentiable function defined for all $x \geq a$. Consider a function F defined by $F(x) = \int_a^x  f(t) dt$. If f is increasing on any interval, then on that interval F is convex. I am not sure I intuitively understand this. What is the function is increasing at an increasing rate? In this figure, for example, isn't the set defined by the integral not convex? Since any line joining the two points of the set are lying outside the said set? Can someone explain what I'm missing?","Let $f$ be a real valued differentiable function defined for all $x \geq a$. Consider a function F defined by $F(x) = \int_a^x  f(t) dt$. If f is increasing on any interval, then on that interval F is convex. I am not sure I intuitively understand this. What is the function is increasing at an increasing rate? In this figure, for example, isn't the set defined by the integral not convex? Since any line joining the two points of the set are lying outside the said set? Can someone explain what I'm missing?",,"['real-analysis', 'integration', 'convex-analysis']"
51,"Undergraduate math competition problem: find $\lim \limits_{n \to \infty} \int \limits^{2006}_{1385}f(nx)\, \mathrm dx$ [closed]",Undergraduate math competition problem: find  [closed],"\lim \limits_{n \to \infty} \int \limits^{2006}_{1385}f(nx)\, \mathrm dx","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Suppose $f\colon [0, +\infty) \to \mathbb{R}$ is a continuous function and $\displaystyle \lim \limits_{x \to +\infty} f(x) = 1$. Find the following limit: $$\large\displaystyle \lim \limits_{n \to \infty} \int \limits^{2006}_{1385}f(nx)\, \mathrm dx$$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Suppose $f\colon [0, +\infty) \to \mathbb{R}$ is a continuous function and $\displaystyle \lim \limits_{x \to +\infty} f(x) = 1$. Find the following limit: $$\large\displaystyle \lim \limits_{n \to \infty} \int \limits^{2006}_{1385}f(nx)\, \mathrm dx$$",,['real-analysis']
52,Proof $\sum_{n=0}^\infty \frac{n^2+3n+2}{4^n} = \frac{128}{27}$,Proof,\sum_{n=0}^\infty \frac{n^2+3n+2}{4^n} = \frac{128}{27},$\sum_{n=0}^\infty \frac{n^2+3n+2}{4^n} =  \frac{128}{27}$ Given hint: $(n^2+3n+2) = (n+2)(n+1)$ I've tried converting the series to a geometric one but failed with that approach and don't know other methods for normal series that help determine the actual convergence value. Help and hints are both appreciated,$\sum_{n=0}^\infty \frac{n^2+3n+2}{4^n} =  \frac{128}{27}$ Given hint: $(n^2+3n+2) = (n+2)(n+1)$ I've tried converting the series to a geometric one but failed with that approach and don't know other methods for normal series that help determine the actual convergence value. Help and hints are both appreciated,,"['real-analysis', 'sequences-and-series', 'analysis', 'limits']"
53,Showing a recursive sequence isn't bounded $a_{n+1}=a_n+\frac 1 {a_n}$,Showing a recursive sequence isn't bounded,a_{n+1}=a_n+\frac 1 {a_n},"Show the sequence isn't bounded: $a_1=1$, $a_{n+1}=a_n+\frac 1 {a_n}$. Proof by contradiction: Let $M>0$ such that $\forall n: |a_n|< M$. Let $\epsilon >0 $ and for some $n=N, \epsilon: a_N=M-\epsilon<M $ pluging that in the recursion: $a_{N+1}=M-\epsilon+\frac 1 {M-\epsilon}>M>M-\epsilon$. Contradiction. I wondered if I could suppose about the boundary that $\forall n: |a_n|\le M$ ? The proof would basically be the same only I could drop the epsilon.","Show the sequence isn't bounded: $a_1=1$, $a_{n+1}=a_n+\frac 1 {a_n}$. Proof by contradiction: Let $M>0$ such that $\forall n: |a_n|< M$. Let $\epsilon >0 $ and for some $n=N, \epsilon: a_N=M-\epsilon<M $ pluging that in the recursion: $a_{N+1}=M-\epsilon+\frac 1 {M-\epsilon}>M>M-\epsilon$. Contradiction. I wondered if I could suppose about the boundary that $\forall n: |a_n|\le M$ ? The proof would basically be the same only I could drop the epsilon.",,"['real-analysis', 'sequences-and-series', 'proof-verification']"
54,"Terminology: What does ""less than 1 and 0 integral"" mean? (as in ""functions with Lipschitz constant less than one and zero integral"")","Terminology: What does ""less than 1 and 0 integral"" mean? (as in ""functions with Lipschitz constant less than one and zero integral"")",,"I am working through Applied Analysis by John K. Hunter. Problem 2.7 states: Prove that the set of Lipschitz continuous functions on $[0, 1]$ with Lipschitz constant less than one and zero integral is compact in $C([0, 1])$ . What does ""less than one and zero integral"" mean? $|C|<1$ ? $0\leq C\leq 1$ ?","I am working through Applied Analysis by John K. Hunter. Problem 2.7 states: Prove that the set of Lipschitz continuous functions on with Lipschitz constant less than one and zero integral is compact in . What does ""less than one and zero integral"" mean? ? ?","[0, 1] C([0, 1]) |C|<1 0\leq C\leq 1","['real-analysis', 'analysis', 'terminology', 'lipschitz-functions']"
55,How can I prove that this infinite product equals $0$?,How can I prove that this infinite product equals ?,0,"I am having trouble to prove that $$\prod_{n=1}^\infty\frac{2n-1}{2n}=0$$ I know that the sequence of partial products $(p_n)$ converges to a limit $L\ge0$, because $$p_n=\frac{(2n)!}{2^{2n}(n!)^2}$$ is decreasing and bounded below by zero. I believe that proving that the limit equals zero would involve finding a sequence $(a_n) \to 0$ such that, eventually, $p_n \le a_n$; however, I could not find such sequence. How can it be done?","I am having trouble to prove that $$\prod_{n=1}^\infty\frac{2n-1}{2n}=0$$ I know that the sequence of partial products $(p_n)$ converges to a limit $L\ge0$, because $$p_n=\frac{(2n)!}{2^{2n}(n!)^2}$$ is decreasing and bounded below by zero. I believe that proving that the limit equals zero would involve finding a sequence $(a_n) \to 0$ such that, eventually, $p_n \le a_n$; however, I could not find such sequence. How can it be done?",,['real-analysis']
56,Does there exist an exponential function not vanishing at $-\infty?$,Does there exist an exponential function not vanishing at,-\infty?,I am searching for a non-constant function $f:\mathbb{R}\rightarrow \mathbb{R}$ with the following properties: 1) $f(a+b)=f(a)f(b)$ 2) $\lim\limits_{x\rightarrow -\infty} f(x) = 1$. Is it possible to find such a function or is there a reason why such a function can not exist?,I am searching for a non-constant function $f:\mathbb{R}\rightarrow \mathbb{R}$ with the following properties: 1) $f(a+b)=f(a)f(b)$ 2) $\lim\limits_{x\rightarrow -\infty} f(x) = 1$. Is it possible to find such a function or is there a reason why such a function can not exist?,,"['real-analysis', 'analysis', 'functions', 'exponential-function']"
57,Difficult general integral definite 0 to 1,Difficult general integral definite 0 to 1,,"$$\int_{0}^{1} \log^2(x)\cdot x^{k+1} dx$$ I tried integration by parts but it leads to an extremely complicated computation, which didnt lead me anywhere. Then I tried differentiating the beta function. That was partly successful. But the problem was when I substituted $k+2$ in and then the digammas and trigamma acted out. Any help? Thanks! Series, and ANY type EXCEPT COMPLEX ANALYSIS is welcome.","$$\int_{0}^{1} \log^2(x)\cdot x^{k+1} dx$$ I tried integration by parts but it leads to an extremely complicated computation, which didnt lead me anywhere. Then I tried differentiating the beta function. That was partly successful. But the problem was when I substituted $k+2$ in and then the digammas and trigamma acted out. Any help? Thanks! Series, and ANY type EXCEPT COMPLEX ANALYSIS is welcome.",,"['calculus', 'real-analysis', 'integration', 'analysis', 'definite-integrals']"
58,Can a continuous real function take each value exactly 3 times? [duplicate],Can a continuous real function take each value exactly 3 times? [duplicate],,"This question already has answers here : Function $f: \mathbb{R} \to \mathbb{R}$ that takes each value in $\mathbb{R}$ three times (7 answers) Closed 9 years ago . Let $f: \mathbb{R} \to D \subseteq \mathbb{R}$ be a continuous function. Is there a function $f$ that satisfies the following property? $\forall y \in D$, there are exactly 3 $ x_1,x_2,x_3 \in \mathbb{R}$ such that $f(x_1) = f(x_2) = f(x_3) = y$.","This question already has answers here : Function $f: \mathbb{R} \to \mathbb{R}$ that takes each value in $\mathbb{R}$ three times (7 answers) Closed 9 years ago . Let $f: \mathbb{R} \to D \subseteq \mathbb{R}$ be a continuous function. Is there a function $f$ that satisfies the following property? $\forall y \in D$, there are exactly 3 $ x_1,x_2,x_3 \in \mathbb{R}$ such that $f(x_1) = f(x_2) = f(x_3) = y$.",,"['real-analysis', 'functions']"
59,Square Root Inequality [closed],Square Root Inequality [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question How can I prove the following inequality: Given $ a,b>0 $ and $a^2>b $, we have $a>\sqrt b$ Thank you.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question How can I prove the following inequality: Given $ a,b>0 $ and $a^2>b $, we have $a>\sqrt b$ Thank you.",,"['real-analysis', 'inequality']"
60,Mean Value Theorem: Continuous or Defined?,Mean Value Theorem: Continuous or Defined?,,"One of the requirements of the MVT is that the function has to be continuous at each point on the closed interval $a\leq x\leq b$ . If we call the function $f(x)$ , I am confused about how the function is continuous at points $(a, f(a))$ and $(b, f(b))$ . The definition of a function being continuous at a point (let's call that point $(c, f(c)))$ is that $ \lim_{x\to c} f(x) = f(c).$ If we take the point $(a, f(a))$ , in order for $f(x)$ to be continuous at $(a, f(a))$ , $ \lim_{x\to a} f(x) = f(a)$ must hold true. However, it can't because the two sided limit doesn't exist. $x$ can approach $a$ from the right but not from the left, and $x$ can approach $b$ from the left but not from the right. So shouldn't ""has to be continuous at each point"" actually be ""has to be defined at each point""?","One of the requirements of the MVT is that the function has to be continuous at each point on the closed interval . If we call the function , I am confused about how the function is continuous at points and . The definition of a function being continuous at a point (let's call that point is that If we take the point , in order for to be continuous at , must hold true. However, it can't because the two sided limit doesn't exist. can approach from the right but not from the left, and can approach from the left but not from the right. So shouldn't ""has to be continuous at each point"" actually be ""has to be defined at each point""?","a\leq x\leq b f(x) (a, f(a)) (b, f(b)) (c, f(c)))  \lim_{x\to c} f(x) = f(c). (a, f(a)) f(x) (a, f(a))  \lim_{x\to a} f(x) = f(a) x a x b",['real-analysis']
61,Does every circle in $\mathbb{R^2}$ contain a point with rational coordinates? [closed],Does every circle in  contain a point with rational coordinates? [closed],\mathbb{R^2},"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Is it true that any circle in $\mathbb{R^2}$ contains a point with rational coordinates? what about any simple closed curve? If it is, could you please help me with the proof?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Is it true that any circle in $\mathbb{R^2}$ contains a point with rational coordinates? what about any simple closed curve? If it is, could you please help me with the proof?",,"['real-analysis', 'general-topology']"
62,"Non differentiable, continuous functions in metric spaces.","Non differentiable, continuous functions in metric spaces.",,"Take the metric space $(C([0,1]),d)$ with $C([0,1])$ the set that contains all continuous functions $f: [0,1] \to \mathbb{R} $ and the metric $d(f,g) = \sup \{|f(x)-g(x)| : x \in [0,1]\}$. Now, I was wondering that if we take a non differentiable, continuous function $f$ in this metric space, are there any differentiable functions in the set $B(f,\varepsilon) = \{ g \in C([0,1]) : d(f,g) < \varepsilon \}$? I do think that this is true but that is just my intuition, can someone help me find a solution?","Take the metric space $(C([0,1]),d)$ with $C([0,1])$ the set that contains all continuous functions $f: [0,1] \to \mathbb{R} $ and the metric $d(f,g) = \sup \{|f(x)-g(x)| : x \in [0,1]\}$. Now, I was wondering that if we take a non differentiable, continuous function $f$ in this metric space, are there any differentiable functions in the set $B(f,\varepsilon) = \{ g \in C([0,1]) : d(f,g) < \varepsilon \}$? I do think that this is true but that is just my intuition, can someone help me find a solution?",,"['real-analysis', 'derivatives', 'metric-spaces', 'continuity']"
63,Baby Rudin Chapter 4 Exercise 1,Baby Rudin Chapter 4 Exercise 1,,"Suppose $f$ is a real function defined on $R^1$ which satisfies   $$\lim_{h\to 0}[f(x+h)-f(x-h)]=0$$   for every $x\in R^1$. Does this imply that $f$ is continuous? below is my solution, my answer is yes but I looked up the solution manual which says it doesn't and I am confused which step of my reasoning is incorrect. Thank you. Because $$\lim_{h\to 0}[f(x+h)-f(x-h)]=0$$ Let $$\lim_{h\to 0}f(x+h) = \lim_{h\to 0}f(x-h) = v = f(x)$$ Because $$\lim_{h\to 0}f(x+h) = v$$ thus $$\forall \epsilon>0, \exists h_1>0, \forall h < h_1, d(f(x+h),f(x)) < \epsilon$$ Similarly, for $f(x-h)$, I get    $$\forall \epsilon>0, \exists h_2>0, \forall h < h_2, d(f(x-h),f(x)) < \epsilon$$ Let $$H = min(h_1, h_2)$$ Then $$\forall \epsilon>0, \exists H > 0, \forall p\in R^1, \text{if } d(x,p)<H, \text{then } d(f(x), f(p))<\epsilon$$ Thus, $f$ is continuous.","Suppose $f$ is a real function defined on $R^1$ which satisfies   $$\lim_{h\to 0}[f(x+h)-f(x-h)]=0$$   for every $x\in R^1$. Does this imply that $f$ is continuous? below is my solution, my answer is yes but I looked up the solution manual which says it doesn't and I am confused which step of my reasoning is incorrect. Thank you. Because $$\lim_{h\to 0}[f(x+h)-f(x-h)]=0$$ Let $$\lim_{h\to 0}f(x+h) = \lim_{h\to 0}f(x-h) = v = f(x)$$ Because $$\lim_{h\to 0}f(x+h) = v$$ thus $$\forall \epsilon>0, \exists h_1>0, \forall h < h_1, d(f(x+h),f(x)) < \epsilon$$ Similarly, for $f(x-h)$, I get    $$\forall \epsilon>0, \exists h_2>0, \forall h < h_2, d(f(x-h),f(x)) < \epsilon$$ Let $$H = min(h_1, h_2)$$ Then $$\forall \epsilon>0, \exists H > 0, \forall p\in R^1, \text{if } d(x,p)<H, \text{then } d(f(x), f(p))<\epsilon$$ Thus, $f$ is continuous.",,"['real-analysis', 'proof-verification']"
64,Prove that $\sum_{n=2}^\infty (\ln n)^{- \ln n}$ converges,Prove that  converges,\sum_{n=2}^\infty (\ln n)^{- \ln n},"As the title suggests, I'd like to prove that the sum $$ \sum_{n=2}^\infty (\ln n)^{- \ln n} $$ is finite.  The root and ratio test both fail here, but WA suggests that there is a comparison that can be used to show convergence. The only thought I have is that it may help to write the terms as $e^{-\ln(n)\ln(\ln(n))}$, but this has not led me to any particular insight.  Any ideas are appreciated.","As the title suggests, I'd like to prove that the sum $$ \sum_{n=2}^\infty (\ln n)^{- \ln n} $$ is finite.  The root and ratio test both fail here, but WA suggests that there is a comparison that can be used to show convergence. The only thought I have is that it may help to write the terms as $e^{-\ln(n)\ln(\ln(n))}$, but this has not led me to any particular insight.  Any ideas are appreciated.",,"['real-analysis', 'sequences-and-series']"
65,Integral $\int_0^1\frac{dx}{\sqrt{\log \frac{1}{x}}}=\sqrt \pi$,Integral,\int_0^1\frac{dx}{\sqrt{\log \frac{1}{x}}}=\sqrt \pi,"Hi I am trying to prove this result below $$ \mathcal{J}:=\int_0^1\frac{dx}{\sqrt{\log \frac{1}{x}}}=\sqrt \pi. $$ The result is quite interesting however I realized I am not familiar with working square roots of log functions like this.  I am more stumped than usual because there isn't much to work with here.  The indefinite integral is given by $$ \int \frac{dx}{\sqrt{\log \frac{1}{x}}}=-\sqrt \pi\, \text{erf}\left(\sqrt {\log \frac{1}{x}}\right), $$ although I cannot prove this.  Possibly a proof to this will lead to the result of the definite integral. It seems the integral is somehow related to a Gaussian integral possibly, I notice the error function and the result $\sqrt \pi$.  A solution would be greatly appreciated and I hope could also be of use to the math community here. Thanks.  In case anybody likes this integral and is interested in a similar one, here is another one for you: $$ \int_0^1 \sqrt{\log \frac{1}{x}} \,dx=\frac{\sqrt \pi}{2} $$","Hi I am trying to prove this result below $$ \mathcal{J}:=\int_0^1\frac{dx}{\sqrt{\log \frac{1}{x}}}=\sqrt \pi. $$ The result is quite interesting however I realized I am not familiar with working square roots of log functions like this.  I am more stumped than usual because there isn't much to work with here.  The indefinite integral is given by $$ \int \frac{dx}{\sqrt{\log \frac{1}{x}}}=-\sqrt \pi\, \text{erf}\left(\sqrt {\log \frac{1}{x}}\right), $$ although I cannot prove this.  Possibly a proof to this will lead to the result of the definite integral. It seems the integral is somehow related to a Gaussian integral possibly, I notice the error function and the result $\sqrt \pi$.  A solution would be greatly appreciated and I hope could also be of use to the math community here. Thanks.  In case anybody likes this integral and is interested in a similar one, here is another one for you: $$ \int_0^1 \sqrt{\log \frac{1}{x}} \,dx=\frac{\sqrt \pi}{2} $$",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
66,"If two limits don't exist, are we sure the limit of sum doesn't exist?","If two limits don't exist, are we sure the limit of sum doesn't exist?",,"This is a true/false question. If the limits $$\lim_ {x\to a}\ f(x)\quad \text{ and }\quad  \lim_ {x\to a}\ g(x)$$ don't exist, limits $$\lim_ {x\to a}\ (f(x)+g(x))\quad \text{ and }\quad \lim_ {x\to a}\ (f(x)g(x))$$ don't exist also. My idea is, that, because $$\lim_ {x\to a}\ (f(x)+g(x))=\lim_ {x\to a}\ f(x)+\lim_ {x\to a} g(x)$$ then the limits doesn't exist, correct?","This is a true/false question. If the limits $$\lim_ {x\to a}\ f(x)\quad \text{ and }\quad  \lim_ {x\to a}\ g(x)$$ don't exist, limits $$\lim_ {x\to a}\ (f(x)+g(x))\quad \text{ and }\quad \lim_ {x\to a}\ (f(x)g(x))$$ don't exist also. My idea is, that, because $$\lim_ {x\to a}\ (f(x)+g(x))=\lim_ {x\to a}\ f(x)+\lim_ {x\to a} g(x)$$ then the limits doesn't exist, correct?",,"['calculus', 'real-analysis', 'limits']"
67,Prove that $||x|-|y|| \leq |x-y|$ [duplicate],Prove that  [duplicate],||x|-|y|| \leq |x-y|,"This question already has answers here : How to prove $\lvert \lVert x \rVert - \lVert y \rVert \rvert \overset{\heartsuit}{\leq} \lVert x-y \rVert$? (3 answers) Closed 10 years ago . $||x|-|y|| \leq |x-y|$  when $(x,y \in R^k)$ In Principles of MA(Rudin), the author said one sees easily that $||x|-|y|| \leq |x-y|$  when $(x,y \in R^k)$ (p.88, Rudin) from the triangle inequality. But I'm not sure how to use the triangle inequality to show this. Can you help me show this?","This question already has answers here : How to prove $\lvert \lVert x \rVert - \lVert y \rVert \rvert \overset{\heartsuit}{\leq} \lVert x-y \rVert$? (3 answers) Closed 10 years ago . $||x|-|y|| \leq |x-y|$  when $(x,y \in R^k)$ In Principles of MA(Rudin), the author said one sees easily that $||x|-|y|| \leq |x-y|$  when $(x,y \in R^k)$ (p.88, Rudin) from the triangle inequality. But I'm not sure how to use the triangle inequality to show this. Can you help me show this?",,"['real-analysis', 'inequality', 'normed-spaces', 'absolute-value']"
68,"what are all the open subgroups of $(\mathbb{R},+)$",what are all the open subgroups of,"(\mathbb{R},+)","I am not able to find out what are all the open subgroups of $(\mathbb{R},+)$, open as a set in usual topology and also subgroup.","I am not able to find out what are all the open subgroups of $(\mathbb{R},+)$, open as a set in usual topology and also subgroup.",,"['real-analysis', 'general-topology', 'group-theory', 'topological-groups']"
69,Calculating the following definite integral,Calculating the following definite integral,,"How may I approach and compute the following integral ? : $$ \int_{0}^{\pi/2}\frac{x}{\tan\left(x\right)}\,{\rm d}x $$ Could polylogarithm functions and complex numbers be Avoided ?.",How may I approach and compute the following integral ? : Could polylogarithm functions and complex numbers be Avoided ?.,"
\int_{0}^{\pi/2}\frac{x}{\tan\left(x\right)}\,{\rm d}x
","['calculus', 'real-analysis', 'integration', 'definite-integrals']"
70,Prove $\sum\limits_{n=1}^\infty \ln({1+\frac 1 {{n}}})$ is divergent. [duplicate],Prove  is divergent. [duplicate],\sum\limits_{n=1}^\infty \ln({1+\frac 1 {{n}}}),"This question already has answers here : Ways of showing $\sum_\limits{n=1}^{\infty}\ln(1+1/n)$ to be divergent (8 answers) Closed 6 years ago . Evaluate if the following series is convergent or divergent: $\sum\limits_{n=1}^\infty \ln({1+\frac 1 {{n}}})$. I tried to evaluate the divergence, applying the Weierstrass comparison test: $\sum\limits_{n=1}^\infty \ln({1+\frac 1 {{n}}})>\sum\limits_{n=1}^\infty \ln({\frac 1 {{n}}})$. Since the function is decreasing then as $1+\frac{1}{n}$ is closer to $0$ then the inequality follows. Obviously $\sum\limits_{n=1}^\infty \ln({\frac 1 {{n}}})$ diverges since $\lim_{n\to\infty}\ln({\frac 1 {{n}}})=-\infty$. Questions : 1) Is my answer right? If not why? 2) What other kind of approach do you propose? Thanks in advance!","This question already has answers here : Ways of showing $\sum_\limits{n=1}^{\infty}\ln(1+1/n)$ to be divergent (8 answers) Closed 6 years ago . Evaluate if the following series is convergent or divergent: $\sum\limits_{n=1}^\infty \ln({1+\frac 1 {{n}}})$. I tried to evaluate the divergence, applying the Weierstrass comparison test: $\sum\limits_{n=1}^\infty \ln({1+\frac 1 {{n}}})>\sum\limits_{n=1}^\infty \ln({\frac 1 {{n}}})$. Since the function is decreasing then as $1+\frac{1}{n}$ is closer to $0$ then the inequality follows. Obviously $\sum\limits_{n=1}^\infty \ln({\frac 1 {{n}}})$ diverges since $\lim_{n\to\infty}\ln({\frac 1 {{n}}})=-\infty$. Questions : 1) Is my answer right? If not why? 2) What other kind of approach do you propose? Thanks in advance!",,"['calculus', 'real-analysis', 'sequences-and-series']"
71,"Integral calculus sine functions: $\frac{1}{2\pi }\int_{-\pi }^{\pi }\frac{\sin\left((n+1/2)\,x\right)}{\sin\left(x/2\right)}\,dx = 1$",Integral calculus sine functions:,"\frac{1}{2\pi }\int_{-\pi }^{\pi }\frac{\sin\left((n+1/2)\,x\right)}{\sin\left(x/2\right)}\,dx = 1","For an integer, $n$, how do I show the following? $$ \frac{1}{2\pi }\int_{-\pi }^{\pi }\frac{\sin\left((n+1/2)\,x\right)}{\sin\left(x/2\right)}\,dx = 1. $$ Can I use induction?","For an integer, $n$, how do I show the following? $$ \frac{1}{2\pi }\int_{-\pi }^{\pi }\frac{\sin\left((n+1/2)\,x\right)}{\sin\left(x/2\right)}\,dx = 1. $$ Can I use induction?",,"['calculus', 'real-analysis', 'integration', 'analysis', 'definite-integrals']"
72,how to show strictly increasing function on an interval has continuous inverse [duplicate],how to show strictly increasing function on an interval has continuous inverse [duplicate],,This question already has an answer here : Prove inverse of strictly monotone increasing function is continuous over the range of original function (1 answer) Closed 3 years ago . my question is to show that a strictly increasing function that is defined on an interval has a continuous inverse. Thanks.,This question already has an answer here : Prove inverse of strictly monotone increasing function is continuous over the range of original function (1 answer) Closed 3 years ago . my question is to show that a strictly increasing function that is defined on an interval has a continuous inverse. Thanks.,,['real-analysis']
73,Does pointwise convergence against a continuous function imply uniform convergence?,Does pointwise convergence against a continuous function imply uniform convergence?,,"Let $(f_n)_{n\in\mathbb{N}}$ be a sequence of functions with $f_n:\mathbb{R}\rightarrow[0,1]$ for all $n\in\mathbb{N}$ and $f:\mathbb{R}\rightarrow[0,1]$ continuous such that $\lim_{n\rightarrow\infty}f_n(x)=f(x)$ for all $x\in\mathbb{R}$. Is it now possible to show that $f_n$ converges uniformly to $f$?","Let $(f_n)_{n\in\mathbb{N}}$ be a sequence of functions with $f_n:\mathbb{R}\rightarrow[0,1]$ for all $n\in\mathbb{N}$ and $f:\mathbb{R}\rightarrow[0,1]$ continuous such that $\lim_{n\rightarrow\infty}f_n(x)=f(x)$ for all $x\in\mathbb{R}$. Is it now possible to show that $f_n$ converges uniformly to $f$?",,['real-analysis']
74,Prove that f is differentiable in $\mathbb{R}$,Prove that f is differentiable in,\mathbb{R},Let $f: \mathbb{R} \rightarrow \mathbb{R}$ some function that all $x$ and $y$ in $\mathbb{R}$ satisfies: $$\left|f(x)-f(y)\right| \le (x-y)^2$$ Prove that f is differentiable in any point in $\mathbb{R}$. Prove that f is constant.,Let $f: \mathbb{R} \rightarrow \mathbb{R}$ some function that all $x$ and $y$ in $\mathbb{R}$ satisfies: $$\left|f(x)-f(y)\right| \le (x-y)^2$$ Prove that f is differentiable in any point in $\mathbb{R}$. Prove that f is constant.,,"['real-analysis', 'analysis']"
75,Recursive Sequence Limit,Recursive Sequence Limit,,I'm trying to show that the limit of the following recursive sequence is 4. $a_{n+1} = \frac{1}{2}a_{n} + 2$ and $a_{1} = \frac{1}{2}$. Could someone give me a hint as to how to start this problem? I've been stuck on this for a while.,I'm trying to show that the limit of the following recursive sequence is 4. $a_{n+1} = \frac{1}{2}a_{n} + 2$ and $a_{1} = \frac{1}{2}$. Could someone give me a hint as to how to start this problem? I've been stuck on this for a while.,,['real-analysis']
76,Differential equation in optics,Differential equation in optics,,"While playing with the eikonal equation (classical optics) I stumbled this differential equation : $$ y y'' = k\left((y')^2 + 1\right) $$ where $y=y(x)$ should be defined for $x>0$, $y>0$ and where $k\in \mathbb Z$. I don't know if there is a general solution $y(x)$ to that problem for a given $k$. What I know is that solving this for : $k=0$ is easy $k=1$ gives something like $y(x)=\cosh(x)$ Question : Is there a known solution for $k\in \mathbb Z \backslash \{0,1\}$ ? Remark 1 : The general solution for $k=1$ is, as asked, $y(x)=y_0 \cosh((x-x_0)/y_0)$ for some constants $x_0,y_0$. Remark 2 : The case $k=-1$ is solved below.","While playing with the eikonal equation (classical optics) I stumbled this differential equation : $$ y y'' = k\left((y')^2 + 1\right) $$ where $y=y(x)$ should be defined for $x>0$, $y>0$ and where $k\in \mathbb Z$. I don't know if there is a general solution $y(x)$ to that problem for a given $k$. What I know is that solving this for : $k=0$ is easy $k=1$ gives something like $y(x)=\cosh(x)$ Question : Is there a known solution for $k\in \mathbb Z \backslash \{0,1\}$ ? Remark 1 : The general solution for $k=1$ is, as asked, $y(x)=y_0 \cosh((x-x_0)/y_0)$ for some constants $x_0,y_0$. Remark 2 : The case $k=-1$ is solved below.",,"['real-analysis', 'ordinary-differential-equations']"
77,"(Proof) If $f$ and $g$ are continuous, then $\max\{f(x),g(x)\}$ is continuous","(Proof) If  and  are continuous, then  is continuous","f g \max\{f(x),g(x)\}","Consider the continuous functions $f,g:\mathbb{R}\rightarrow\mathbb{R}$. Show that $F:\mathbb{R}\rightarrow\mathbb{R}$ with $x\mapsto \max\{f(x),g(x)\}$ is continuous using the $\epsilon - \delta$ definition of continuity. I know there must be four cases. If $f(x)\leq g(x)$ and $f(x_0)\leq g(x_0)$ or if $g(x)\leq f(x)$ and $g(x_0)\leq f(x_0)$ it is easy. However , assuming $f(x_0)\neq g(x_0)$, what if $g(x)\leq f(x)$ and $f(x_0)\leq g(x_0)$ or $f(x)\leq g(x)$ and $g(x_0)\leq f(x_0)$? For example: $|f(x)-g(x_0)|$... how do I get from here to $|x-x_0|$?","Consider the continuous functions $f,g:\mathbb{R}\rightarrow\mathbb{R}$. Show that $F:\mathbb{R}\rightarrow\mathbb{R}$ with $x\mapsto \max\{f(x),g(x)\}$ is continuous using the $\epsilon - \delta$ definition of continuity. I know there must be four cases. If $f(x)\leq g(x)$ and $f(x_0)\leq g(x_0)$ or if $g(x)\leq f(x)$ and $g(x_0)\leq f(x_0)$ it is easy. However , assuming $f(x_0)\neq g(x_0)$, what if $g(x)\leq f(x)$ and $f(x_0)\leq g(x_0)$ or $f(x)\leq g(x)$ and $g(x_0)\leq f(x_0)$? For example: $|f(x)-g(x_0)|$... how do I get from here to $|x-x_0|$?",,"['real-analysis', 'continuity', 'proof-writing', 'epsilon-delta']"
78,Definition of sequential continuity: converse?,Definition of sequential continuity: converse?,,"A function $f: \mathbb R \to \mathbb R$ is called sequentially continuuous if $x_n \to x$ implies $f(x_n) \to f(x)$. Every continuous function is sequentially continuous. Let $f$ be a continuous function on $\mathbb R$. If $y_n = f(x_n)$ is a convergent sequence does it follow that $x_n$   is a convergent sequence? At first I thought that yes because: assume $f(x_n) \to f(x)$ but $x_n \not\to x$. Say, $x_n \to z$. Then since $f$ is sequentially continuous, $f(x_n) \to f(z)\neq f(y)$. Although $x_n \not\to x$ is not equivalent to $x_n$ converging to a different value it seems that the case where $x_n$ diverges can be treated similarly. But then I came up with an almost counterexample: Let $x_n =n$ and $f(x) = {1\over x}$. Then $f(x_n) \to 0$ but $x_n \to \infty$. The problem is, this $f$ is not defined on $\mathbb R$ and also, there is no $x$ with $f(x_n) \to f(x)$. Is it possible that $f(x_n) \to f(x)$ implies $x_n \to x$?","A function $f: \mathbb R \to \mathbb R$ is called sequentially continuuous if $x_n \to x$ implies $f(x_n) \to f(x)$. Every continuous function is sequentially continuous. Let $f$ be a continuous function on $\mathbb R$. If $y_n = f(x_n)$ is a convergent sequence does it follow that $x_n$   is a convergent sequence? At first I thought that yes because: assume $f(x_n) \to f(x)$ but $x_n \not\to x$. Say, $x_n \to z$. Then since $f$ is sequentially continuous, $f(x_n) \to f(z)\neq f(y)$. Although $x_n \not\to x$ is not equivalent to $x_n$ converging to a different value it seems that the case where $x_n$ diverges can be treated similarly. But then I came up with an almost counterexample: Let $x_n =n$ and $f(x) = {1\over x}$. Then $f(x_n) \to 0$ but $x_n \to \infty$. The problem is, this $f$ is not defined on $\mathbb R$ and also, there is no $x$ with $f(x_n) \to f(x)$. Is it possible that $f(x_n) \to f(x)$ implies $x_n \to x$?",,"['real-analysis', 'functional-analysis', 'continuity']"
79,Suppose $\sum{a_n}$ converges. How do I prove that $\sum{\frac{\sqrt{a_n}}{n}}$ converges [duplicate],Suppose  converges. How do I prove that  converges [duplicate],\sum{a_n} \sum{\frac{\sqrt{a_n}}{n}},"This question already has answers here : Convergence of the series $\sum a_n$ implies the convergence of $\sum \frac{\sqrt a_n}{n}$, if $a_n>0$ [duplicate] (1 answer) If $\sum_{n=1}^{\infty} a_n^{2}$ converges, then so does $\sum_{n=1}^{\infty} \frac {a_n}{n}$ (6 answers) Closed 5 years ago . Suppose $\sum{a_n}$ converges. How do I prove that $\sum{\frac{\sqrt{a_n}}{n}}$ converges. So I know that just because  $\sum{a_n}$ converges, do not mean I can say anything about the converges of its square root. So I know that I can prove that if  $\sum{\sqrt{a_n}}$ converges then $\sum{\frac{\sqrt{a_n}}{n}}$ converges by Abel's Test, but I do not know where to start for the case where  $\sum{\frac{\sqrt{a_n}}{n}}$ diverges","This question already has answers here : Convergence of the series $\sum a_n$ implies the convergence of $\sum \frac{\sqrt a_n}{n}$, if $a_n>0$ [duplicate] (1 answer) If $\sum_{n=1}^{\infty} a_n^{2}$ converges, then so does $\sum_{n=1}^{\infty} \frac {a_n}{n}$ (6 answers) Closed 5 years ago . Suppose $\sum{a_n}$ converges. How do I prove that $\sum{\frac{\sqrt{a_n}}{n}}$ converges. So I know that just because  $\sum{a_n}$ converges, do not mean I can say anything about the converges of its square root. So I know that I can prove that if  $\sum{\sqrt{a_n}}$ converges then $\sum{\frac{\sqrt{a_n}}{n}}$ converges by Abel's Test, but I do not know where to start for the case where  $\sum{\frac{\sqrt{a_n}}{n}}$ diverges",,"['real-analysis', 'sequences-and-series']"
80,$\displaystyle\lim_{n\to \infty} n^2(\sqrt[n]{2}-\sqrt[n+1]{2})$,,\displaystyle\lim_{n\to \infty} n^2(\sqrt[n]{2}-\sqrt[n+1]{2}),i cannot figure out a way to find this limit. $$\displaystyle\lim_{n\to \infty} n^2(\sqrt[n]{2}-\sqrt[n+1]{2})$$ Its undeterminate form $0\cdot\infty$ so i tried using $$\displaystyle\lim_{n\to \infty}\frac{a^{X_n}-1}{X_n}=\ln{a}$$ this leads to $0\cdot\infty$ again.  I then tried to transform it in $\frac{0}{0}$ $$\displaystyle\lim_{n\to \infty} \frac{\sqrt[n]{2}-\sqrt[n+1]{2}}{\frac{1}{n^2}}$$ here i dont know what i could do to find the limit.,i cannot figure out a way to find this limit. $$\displaystyle\lim_{n\to \infty} n^2(\sqrt[n]{2}-\sqrt[n+1]{2})$$ Its undeterminate form $0\cdot\infty$ so i tried using $$\displaystyle\lim_{n\to \infty}\frac{a^{X_n}-1}{X_n}=\ln{a}$$ this leads to $0\cdot\infty$ again.  I then tried to transform it in $\frac{0}{0}$ $$\displaystyle\lim_{n\to \infty} \frac{\sqrt[n]{2}-\sqrt[n+1]{2}}{\frac{1}{n^2}}$$ here i dont know what i could do to find the limit.,,"['calculus', 'real-analysis', 'limits']"
81,A function with a non-negative upper derivative must be increasing?,A function with a non-negative upper derivative must be increasing?,,"I am trying to show that if $f$ is continuous on the interval $[a,b]$ and its upper derivative $\overline{D}f$ is such that $ \overline{D}f \geq 0$ on $(a,b)$, then $f$ is increasing on the entire interval. Here $\overline{D}f$ is defined by $$ \overline{D}f(x) = \lim\limits_{h \to 0} \sup\limits_{h, 0 < |t| \leq h} \frac{f(x+t) - f(x)}{t}  $$ I am not sure where to begin, though. Letting $x,y \in [a,b]$ be such that $x \leq y$, suppose for contradiction that $f(x) > f(y)$, then continuity of $f$ means that there is some neighbourhood of $y$ such that $f$ takes on values strictly less than $f(x)$ on this neighbourhood. Now I think I would like to use this neighbourhood to argue that the upper derivative at $y$ is then negative, but I cannot see how to complete this argument. Any help is appreciated! :)","I am trying to show that if $f$ is continuous on the interval $[a,b]$ and its upper derivative $\overline{D}f$ is such that $ \overline{D}f \geq 0$ on $(a,b)$, then $f$ is increasing on the entire interval. Here $\overline{D}f$ is defined by $$ \overline{D}f(x) = \lim\limits_{h \to 0} \sup\limits_{h, 0 < |t| \leq h} \frac{f(x+t) - f(x)}{t}  $$ I am not sure where to begin, though. Letting $x,y \in [a,b]$ be such that $x \leq y$, suppose for contradiction that $f(x) > f(y)$, then continuity of $f$ means that there is some neighbourhood of $y$ such that $f$ takes on values strictly less than $f(x)$ on this neighbourhood. Now I think I would like to use this neighbourhood to argue that the upper derivative at $y$ is then negative, but I cannot see how to complete this argument. Any help is appreciated! :)",,"['real-analysis', 'derivatives', 'examples-counterexamples', 'monotone-functions']"
82,Continuity of the sign function,Continuity of the sign function,,"It is perhaps well known that the sign function is discontinuous, if defined for $f:\mathbb{R}\rightarrow \mathbb{R}$. However, if we were to define the sign function for $f:\mathbb{R} \setminus \left \{ 0 \right \}\rightarrow \mathbb{R}$, would the sign function still remain discontinuous? My belief is yes simply because at any given $x_{0}>0$ or $x_{0}<0$ the function will still not be continuous by virtue of the epsilon-delta proof (there exist a value of $\varepsilon$ where $\left |f(x)-f(x_{0})  \right | < \varepsilon $ is not satisfied.) Is my reasoning correct?","It is perhaps well known that the sign function is discontinuous, if defined for $f:\mathbb{R}\rightarrow \mathbb{R}$. However, if we were to define the sign function for $f:\mathbb{R} \setminus \left \{ 0 \right \}\rightarrow \mathbb{R}$, would the sign function still remain discontinuous? My belief is yes simply because at any given $x_{0}>0$ or $x_{0}<0$ the function will still not be continuous by virtue of the epsilon-delta proof (there exist a value of $\varepsilon$ where $\left |f(x)-f(x_{0})  \right | < \varepsilon $ is not satisfied.) Is my reasoning correct?",,"['real-analysis', 'continuity']"
83,is the smallest $\sigma$-algebra containing all compact sets the Borel $\sigma$-algebra,is the smallest -algebra containing all compact sets the Borel -algebra,\sigma \sigma,Let $R$ be the smallest $\sigma$-algebra containing all compact sets in $\mathbb R^n$. I know that based on definition the minimal $\sigma$-algebra containing the closed (or open) sets is the Borel $\sigma$-algebra. But how can I prove that $R$ is actually the Borel $\sigma$-algebra?,Let $R$ be the smallest $\sigma$-algebra containing all compact sets in $\mathbb R^n$. I know that based on definition the minimal $\sigma$-algebra containing the closed (or open) sets is the Borel $\sigma$-algebra. But how can I prove that $R$ is actually the Borel $\sigma$-algebra?,,"['real-analysis', 'measure-theory', 'compactness']"
84,Evaluating the limit: $\lim \limits_{n\to \infty}\sqrt[n]{4^n+9n^2}=4$,Evaluating the limit:,\lim \limits_{n\to \infty}\sqrt[n]{4^n+9n^2}=4,How do I prove that:  $\lim \limits_{n\to \infty}\sqrt[n]{4^n+9n^2}=4$ Thank you.,How do I prove that:  $\lim \limits_{n\to \infty}\sqrt[n]{4^n+9n^2}=4$ Thank you.,,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'radicals']"
85,Finding $\sum_{n=1}^{\infty}\frac{(-1)^n (H_{2n}-H_{n})}{n2^n \binom{2n}{n}}$,Finding,\sum_{n=1}^{\infty}\frac{(-1)^n (H_{2n}-H_{n})}{n2^n \binom{2n}{n}},"I want to find the closed form of: $\displaystyle \tag*{} \sum \limits _{n=1}^{\infty}\frac{(-1)^n (H_{2n}-H_{n})}{n2^n \binom{2n}{n}}$ Where $H_{k}$ is $k^{\text{th}}$ harmonic number I tried to expand the numerator (Harmonic numbers) in terms of integral, to get: $\displaystyle \tag*{} \sum \limits_{n=1}^{\infty} \frac{(-1)^n}{n2^n\binom{2n}{n}} \int _{0}^{1} \frac{x^n - x^{2n}}{1-x} \ \mathrm dx$ And I found that with the help of series expansion of $\sin^{-1}(x)$ and subsituting $x = i \sqrt{x} /8 $ where $i^2=1$ $\displaystyle \tag*{} -2(\sinh^{-1} (\sqrt{x}/8))^2 = \sum \limits_{n=1}^{\infty} \frac{(-1)^nx^n}{n^22^n \binom{2n}{n}} $ But this has $n^2$ in the denominator, which makes it complicated. EDIT: we can eliminate $n^2$ by differentiating and multiplying by $x$ as mentioned in the comments. But now, how can we solve our sum since $H_{2n}-H_n$ is numerator? And I have the general formula for generating sum: $ \displaystyle \tag*{} \sum \limits _{n=1}^{\infty} \frac{x^n}{n^y \binom{2n}{n}}$ And this doesn't have $n$ in the denominator and also it has closed-form $\forall \ y \geq 2$ Maybe if there is a way of expressing the denominator in the form integral, the sum can be changed in evaluating the double integral. I think there are other easy ways (such as using Hypergeometric functions)? Any help would be appreciated. EDIT 2: From the help of comments and a quora user, $\DeclareMathOperator{\arcsinh}{arcsinh}$ By @Bertrand87, we have: $\displaystyle \tag{1} H_{k} - H_{2k} + \ln (2) = \int _{0}^{1} \frac{x^{2k}}{1+x} \ \mathrm dx$ To make use of this, we express our sum as follows: $\displaystyle \tag*{} S = \sum \limits_{k=1}^{\infty} \frac{(-1)^k(H_{2k}-H_k - \ln2)}{k2^k \binom{2k}{k}}  + \sum \limits_{k=1}^{\infty} \frac{(-1)^k(\ln2)}{k2^k \binom{2k}{k}}$ We know $\displaystyle \tag*{} 2\arcsin^2(x) = \sum \limits_{k=1}^{\infty} \frac{(2x)^{2k}}{k^2 \binom{2k}{k}}$ We differentiate both sides w.r.t $x$ both sides, $\displaystyle \tag*{} \frac{2 \arcsin(x)}{\sqrt{1-x^2}} = \sum \limits_{k=1}^{\infty} \frac{(2x)^{2k-1}}{k \binom{2k}{k}}$ Now, we multiply both sides by $(2x)$ and define $x:= ix/ \sqrt{8}$ to get: $\displaystyle \tag{2} \frac{-2x \arcsinh (x/ \sqrt {8})}{\sqrt{8}\sqrt{1+x^2/8}} = \sum \limits_{k=1}^{\infty} \frac{(-1)^k x^{2k}}{k2^k \binom{2k}{k}}$ We now multiply both sides by $-1/(1+x)$ and integrate from $0$ to $1$ and arrive at: $\displaystyle \tag*{} \frac{2}{\sqrt {8}}\int_{0}^{1}\frac{x\arcsinh(x/ \sqrt{8})}{\sqrt{1+x^2/8} (1+x)} \ \mathrm dx = \sum \limits_{k=1}^{\infty} \frac{(-1)^k(H_{2k}-H_k - \ln2)}{k2^k \binom{2k}{k}}$ Similarly, from $(2)$ if we let $x=1$ and multiply both sides by $\ln 2$ , it yields: $\displaystyle \tag*{} \frac{-2 \arcsinh (1/ \sqrt{8}) \ln 2}{ \sqrt{8} \sqrt{1 + 1/8}} =\sum \limits_{k=1}^{\infty} \frac{(-1)^k(\ln2)}{k2^k \binom{2k}{k}} \approx -0.1601$ Now, our only problem is to evaluate the integral: $\displaystyle \tag*{} \boxed{\frac{2}{\sqrt {8}}\int_{0}^{1}\frac{x\arcsinh(x/ \sqrt{8})}{\sqrt{1+x^2/8} (1+x)} \ \mathrm dx} $ Can anyone help me with this integral?","I want to find the closed form of: Where is harmonic number I tried to expand the numerator (Harmonic numbers) in terms of integral, to get: And I found that with the help of series expansion of and subsituting where But this has in the denominator, which makes it complicated. EDIT: we can eliminate by differentiating and multiplying by as mentioned in the comments. But now, how can we solve our sum since is numerator? And I have the general formula for generating sum: And this doesn't have in the denominator and also it has closed-form Maybe if there is a way of expressing the denominator in the form integral, the sum can be changed in evaluating the double integral. I think there are other easy ways (such as using Hypergeometric functions)? Any help would be appreciated. EDIT 2: From the help of comments and a quora user, By @Bertrand87, we have: To make use of this, we express our sum as follows: We know We differentiate both sides w.r.t both sides, Now, we multiply both sides by and define to get: We now multiply both sides by and integrate from to and arrive at: Similarly, from if we let and multiply both sides by , it yields: Now, our only problem is to evaluate the integral: Can anyone help me with this integral?",\displaystyle \tag*{} \sum \limits _{n=1}^{\infty}\frac{(-1)^n (H_{2n}-H_{n})}{n2^n \binom{2n}{n}} H_{k} k^{\text{th}} \displaystyle \tag*{} \sum \limits_{n=1}^{\infty} \frac{(-1)^n}{n2^n\binom{2n}{n}} \int _{0}^{1} \frac{x^n - x^{2n}}{1-x} \ \mathrm dx \sin^{-1}(x) x = i \sqrt{x} /8  i^2=1 \displaystyle \tag*{} -2(\sinh^{-1} (\sqrt{x}/8))^2 = \sum \limits_{n=1}^{\infty} \frac{(-1)^nx^n}{n^22^n \binom{2n}{n}}  n^2 n^2 x H_{2n}-H_n  \displaystyle \tag*{} \sum \limits _{n=1}^{\infty} \frac{x^n}{n^y \binom{2n}{n}} n \forall \ y \geq 2 \DeclareMathOperator{\arcsinh}{arcsinh} \displaystyle \tag{1} H_{k} - H_{2k} + \ln (2) = \int _{0}^{1} \frac{x^{2k}}{1+x} \ \mathrm dx \displaystyle \tag*{} S = \sum \limits_{k=1}^{\infty} \frac{(-1)^k(H_{2k}-H_k - \ln2)}{k2^k \binom{2k}{k}}  + \sum \limits_{k=1}^{\infty} \frac{(-1)^k(\ln2)}{k2^k \binom{2k}{k}} \displaystyle \tag*{} 2\arcsin^2(x) = \sum \limits_{k=1}^{\infty} \frac{(2x)^{2k}}{k^2 \binom{2k}{k}} x \displaystyle \tag*{} \frac{2 \arcsin(x)}{\sqrt{1-x^2}} = \sum \limits_{k=1}^{\infty} \frac{(2x)^{2k-1}}{k \binom{2k}{k}} (2x) x:= ix/ \sqrt{8} \displaystyle \tag{2} \frac{-2x \arcsinh (x/ \sqrt {8})}{\sqrt{8}\sqrt{1+x^2/8}} = \sum \limits_{k=1}^{\infty} \frac{(-1)^k x^{2k}}{k2^k \binom{2k}{k}} -1/(1+x) 0 1 \displaystyle \tag*{} \frac{2}{\sqrt {8}}\int_{0}^{1}\frac{x\arcsinh(x/ \sqrt{8})}{\sqrt{1+x^2/8} (1+x)} \ \mathrm dx = \sum \limits_{k=1}^{\infty} \frac{(-1)^k(H_{2k}-H_k - \ln2)}{k2^k \binom{2k}{k}} (2) x=1 \ln 2 \displaystyle \tag*{} \frac{-2 \arcsinh (1/ \sqrt{8}) \ln 2}{ \sqrt{8} \sqrt{1 + 1/8}} =\sum \limits_{k=1}^{\infty} \frac{(-1)^k(\ln2)}{k2^k \binom{2k}{k}} \approx -0.1601 \displaystyle \tag*{} \boxed{\frac{2}{\sqrt {8}}\int_{0}^{1}\frac{x\arcsinh(x/ \sqrt{8})}{\sqrt{1+x^2/8} (1+x)} \ \mathrm dx} ,"['real-analysis', 'integration', 'sequences-and-series', 'summation', 'harmonic-numbers']"
86,$2^x$ is irrational if $x$ is irrational?,is irrational if  is irrational?,2^x x,"Prove/Disprove that if $x$ is irrational, then $2^x$ is also irrational. My attempt for the proof: Suppose $2^x>0$ is a rational number, then $2^x=\frac{a}{b}$ for some natural numbers $a$ and $b$ . Taking logarithm with base $2$ on both sides to get, $x=\log_2 \frac{a}{b}$ . Here I stuck! how to reach at $x$ is rational?","Prove/Disprove that if is irrational, then is also irrational. My attempt for the proof: Suppose is a rational number, then for some natural numbers and . Taking logarithm with base on both sides to get, . Here I stuck! how to reach at is rational?",x 2^x 2^x>0 2^x=\frac{a}{b} a b 2 x=\log_2 \frac{a}{b} x,['real-analysis']
87,"Prove or disprove: If $\sum a_n$ converges conditionally, then $\sum n^2 a_n$ diverges","Prove or disprove: If  converges conditionally, then  diverges",\sum a_n \sum n^2 a_n,"Intuitively my guess is that the statement is true but i've struggled to find a way to show it rigorously. The reason I believe it to be true is since $\sum n^2 a_n$ converging seems like a ""strong"" condition and for some obvious candidates that satisfy this (e.g. $\sum \frac{1}{n^4}$ ), $\sum n^2 a_n$ converging usually meant that that $\sum a_n$ converges absolutely. This line of thought led me to attempting to prove the contrapositive instead; that if $\sum n^2a_n$ converges, then $\sum a_n$ converges absolutely (and thus not conditionally). I've tried to show this using the Cauchy criterion, which eventually brought me to attempting to prove the following inequality: $$|a_{n+1}| + |a_{n+2}| + \cdots + |a_m| \le |(n+1)^2a_{n+1} + \cdots + m^2a_m|$$ For $m > n \ge N$ for some $N \in \mathbb{N}$ but i've struggled to make meaningful progress past this point. My main issue with the inequality above being that I don't have any guarantee that each of the terms $a_n$ have the same sign, which made it difficult to work with the expression on the right. Is there something fundamentally wrong with my line of thought above? Any hints on how I might better approach the problem would be really appreciated.","Intuitively my guess is that the statement is true but i've struggled to find a way to show it rigorously. The reason I believe it to be true is since converging seems like a ""strong"" condition and for some obvious candidates that satisfy this (e.g. ), converging usually meant that that converges absolutely. This line of thought led me to attempting to prove the contrapositive instead; that if converges, then converges absolutely (and thus not conditionally). I've tried to show this using the Cauchy criterion, which eventually brought me to attempting to prove the following inequality: For for some but i've struggled to make meaningful progress past this point. My main issue with the inequality above being that I don't have any guarantee that each of the terms have the same sign, which made it difficult to work with the expression on the right. Is there something fundamentally wrong with my line of thought above? Any hints on how I might better approach the problem would be really appreciated.",\sum n^2 a_n \sum \frac{1}{n^4} \sum n^2 a_n \sum a_n \sum n^2a_n \sum a_n |a_{n+1}| + |a_{n+2}| + \cdots + |a_m| \le |(n+1)^2a_{n+1} + \cdots + m^2a_m| m > n \ge N N \in \mathbb{N} a_n,['real-analysis']
88,"Does $\sum_{n=1}^\infty a_n^2 \text{ converges },\ a_n \geq 0 \implies \sum_{n=1}^\infty \frac{a_n}{n} \text{ converges}$? [duplicate]",Does ? [duplicate],"\sum_{n=1}^\infty a_n^2 \text{ converges },\ a_n \geq 0 \implies \sum_{n=1}^\infty \frac{a_n}{n} \text{ converges}","This question already has answers here : If $\sum_{n=1}^{\infty} a_n^{2}$ converges, then so does $\sum_{n=1}^{\infty} \frac {a_n}{n}$ (6 answers) Closed 4 years ago . Let $\sum_{n=1}^\infty a_n^2$ a converging series with $a_n \geq 0$ . Decide whether $\sum_{n=1}^\infty \frac{a_n}{n}$ is converging. Note: you may consider the fact that $\sum_{n=1}^\infty \frac{1}{n^2}$ is converging. I'm having problems with the above task. Given the hint in the note, I believe the latter series is indeed converging and am trying to prove it. Unfortunately, I fail to find a promising approach to do so. Obviously, $\sum_{n=1}^\infty a_n^2 \neq \left(\sum_{n=1}^\infty a_n\right) \cdot \left(\sum_{n=1}^\infty a_n \right)$ , which would be a rookie mistake. Similarly, $\sqrt{\sum_{n=1}^\infty a_n^2} \neq \sum_{n=1}^\infty a_n$ . Therefore, the square seems difficult to get rid of. I thought of arguing using the Cauchy product, however, I'm not sure whether it could be applied ""in reverse"" to show that if the series based on $a_n^2$ is converging, then the one based on $a_n$ is, too. What approach would you take to solve this problem?","This question already has answers here : If $\sum_{n=1}^{\infty} a_n^{2}$ converges, then so does $\sum_{n=1}^{\infty} \frac {a_n}{n}$ (6 answers) Closed 4 years ago . Let a converging series with . Decide whether is converging. Note: you may consider the fact that is converging. I'm having problems with the above task. Given the hint in the note, I believe the latter series is indeed converging and am trying to prove it. Unfortunately, I fail to find a promising approach to do so. Obviously, , which would be a rookie mistake. Similarly, . Therefore, the square seems difficult to get rid of. I thought of arguing using the Cauchy product, however, I'm not sure whether it could be applied ""in reverse"" to show that if the series based on is converging, then the one based on is, too. What approach would you take to solve this problem?",\sum_{n=1}^\infty a_n^2 a_n \geq 0 \sum_{n=1}^\infty \frac{a_n}{n} \sum_{n=1}^\infty \frac{1}{n^2} \sum_{n=1}^\infty a_n^2 \neq \left(\sum_{n=1}^\infty a_n\right) \cdot \left(\sum_{n=1}^\infty a_n \right) \sqrt{\sum_{n=1}^\infty a_n^2} \neq \sum_{n=1}^\infty a_n a_n^2 a_n,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
89,Is this set open or closed (or both?),Is this set open or closed (or both?),,"I'm trying to figure out whether or not the following set is open or closed. $$D=\{(x,y,z)\in\mathbb R^3\mid x\gt0,y\gt0,z=0\}$$ I've tried imagining it and to me, it seems like an open set, but maybe it is both open and closed. How would I determine that?","I'm trying to figure out whether or not the following set is open or closed. I've tried imagining it and to me, it seems like an open set, but maybe it is both open and closed. How would I determine that?","D=\{(x,y,z)\in\mathbb R^3\mid x\gt0,y\gt0,z=0\}","['real-analysis', 'general-topology']"
90,Evaluate $\lim_{x\to 0}\frac{x-\sin x}{x\sin x}$ without to use L'Hopital,Evaluate  without to use L'Hopital,\lim_{x\to 0}\frac{x-\sin x}{x\sin x},"Evaluate $$\lim_{x\to 0}\frac{x-\sin x}{x\sin x}$$ Without L'Hopital's Rule $$\lim_{x\to 0}\frac{x-\sin x}{x\sin x}=\lim_{x\to 0}\frac{x(1-\frac{\sin x}{x})}{x\sin x}=\lim_{x\to 0}\frac{1-\frac{\sin x}{x}}{\sin x}$$ But I can not find a way to deal with $\sin x$ that does not result with a limit of the type $""\frac{0}{0}""$","Evaluate $$\lim_{x\to 0}\frac{x-\sin x}{x\sin x}$$ Without L'Hopital's Rule $$\lim_{x\to 0}\frac{x-\sin x}{x\sin x}=\lim_{x\to 0}\frac{x(1-\frac{\sin x}{x})}{x\sin x}=\lim_{x\to 0}\frac{1-\frac{\sin x}{x}}{\sin x}$$ But I can not find a way to deal with $\sin x$ that does not result with a limit of the type $""\frac{0}{0}""$",,"['real-analysis', 'limits', 'limits-without-lhopital']"
91,"Need help unpacking definitions of $\limsup$, $\liminf$","Need help unpacking definitions of ,",\limsup \liminf,"I am reading a real analysis textbook, and need help clarifying/unpacking the definition of $\limsup$ and $\liminf$. Given a sequence $\{a_n\}$ of real numbers,$$\limsup_{n \to \infty} a_n = \inf_n \sup_{m \ge n} a_m, \quad \liminf_{n \to \infty} a_n = \sup_n \inf_{m \ge n} a_m.$$   We use analogous definitions when we take a limit along the real numbers. For example,$$\limsup_{y \to x} f(y) = \inf_{\delta > 0} \sup_{|y - x| < \delta} f(y).$$ Any help would be well-appreciated. Thanks!","I am reading a real analysis textbook, and need help clarifying/unpacking the definition of $\limsup$ and $\liminf$. Given a sequence $\{a_n\}$ of real numbers,$$\limsup_{n \to \infty} a_n = \inf_n \sup_{m \ge n} a_m, \quad \liminf_{n \to \infty} a_n = \sup_n \inf_{m \ge n} a_m.$$   We use analogous definitions when we take a limit along the real numbers. For example,$$\limsup_{y \to x} f(y) = \inf_{\delta > 0} \sup_{|y - x| < \delta} f(y).$$ Any help would be well-appreciated. Thanks!",,"['calculus', 'real-analysis']"
92,A curious proof of L'Hospital's rule,A curious proof of L'Hospital's rule,,"I quote P. Nahin When Least is Best (2004), pp. 173-174 ""Since $g(x)=R(x)h(x)$, then differentiation of both sides gives $$g'(x)=R(x)h'(x)+R'(x)h(x).$$ Since $\lim_{x \rightarrow 0} h(x)=0$, and we assume $R(x)$ really does have a limit as $x \rightarrow 0$, i.e., $\lim_{x \rightarrow 0} R(x)=R$, then $$\lim_{x \rightarrow 0} g'(x)=\lim_{x \rightarrow 0} R(x)h'(x)+\lim_{x \rightarrow 0} R'(x)h(x)$$$$=R \lim_{x \rightarrow 0} h'(x) + \lim_{x \rightarrow 0} R'(x) \lim_{x \rightarrow 0} h(x).$$ The last term is zero because $\lim_{x \rightarrow 0} h(x)=0$ and because the very fact that $\lim_{x \rightarrow 0} R(x)=R$ implies that $\lim_{x \rightarrow 0} R'(x)=0$, too (i.e. the $y=R(x)$ curve must approach the horizontal zero- slope line $y=R$ as $x \rightarrow 0)$. So, $$\lim_{x \rightarrow 0} g'(x)=R \lim_{x \rightarrow 0} h'(x).$$ and we have L'Hospital's rule."" Please, can someone criticize this proof ? Every argument about the limits of the popularization of mathematics is welcome.","I quote P. Nahin When Least is Best (2004), pp. 173-174 ""Since $g(x)=R(x)h(x)$, then differentiation of both sides gives $$g'(x)=R(x)h'(x)+R'(x)h(x).$$ Since $\lim_{x \rightarrow 0} h(x)=0$, and we assume $R(x)$ really does have a limit as $x \rightarrow 0$, i.e., $\lim_{x \rightarrow 0} R(x)=R$, then $$\lim_{x \rightarrow 0} g'(x)=\lim_{x \rightarrow 0} R(x)h'(x)+\lim_{x \rightarrow 0} R'(x)h(x)$$$$=R \lim_{x \rightarrow 0} h'(x) + \lim_{x \rightarrow 0} R'(x) \lim_{x \rightarrow 0} h(x).$$ The last term is zero because $\lim_{x \rightarrow 0} h(x)=0$ and because the very fact that $\lim_{x \rightarrow 0} R(x)=R$ implies that $\lim_{x \rightarrow 0} R'(x)=0$, too (i.e. the $y=R(x)$ curve must approach the horizontal zero- slope line $y=R$ as $x \rightarrow 0)$. So, $$\lim_{x \rightarrow 0} g'(x)=R \lim_{x \rightarrow 0} h'(x).$$ and we have L'Hospital's rule."" Please, can someone criticize this proof ? Every argument about the limits of the popularization of mathematics is welcome.",,"['calculus', 'real-analysis', 'limits', 'proof-verification']"
93,Find the limit of $\lim\limits_{x\rightarrow0}\frac{x}{\tan x}$.,Find the limit of .,\lim\limits_{x\rightarrow0}\frac{x}{\tan x},"Find the limit of $\lim\limits_{x\rightarrow0}\frac{x}{\tan x}$ Clearly, since the limit takes the form of $\frac{0}{0}$, one should try L'Hopital's Rule. If we apply L'Hopital's Rule, the problem is that $\frac{d(\tan x)}{dx}=\cot x$, and $\cot(0)$ is undefined. Thus, we cannot find a limit using L'Hopital's Rule. My problem set suggests that this question can be answered using little more than L'Hopital and Cauchy's Mean Value Theorem. Not sure how to proceed. Please help.","Find the limit of $\lim\limits_{x\rightarrow0}\frac{x}{\tan x}$ Clearly, since the limit takes the form of $\frac{0}{0}$, one should try L'Hopital's Rule. If we apply L'Hopital's Rule, the problem is that $\frac{d(\tan x)}{dx}=\cot x$, and $\cot(0)$ is undefined. Thus, we cannot find a limit using L'Hopital's Rule. My problem set suggests that this question can be answered using little more than L'Hopital and Cauchy's Mean Value Theorem. Not sure how to proceed. Please help.",,"['calculus', 'real-analysis', 'limits', 'derivatives']"
94,Proof of the substitution rule for integrals for the indefinite case,Proof of the substitution rule for integrals for the indefinite case,,"I know that the substitution rule works like this: By the chain rule: $$F(g(x))' = f(g(x))g'(x)$$ Then $$\begin{align}     \int_a^b F(g(x))'dx &= \int_a^b f(g(x))g'(x)dx\\     F(g(b)) - F(g(a)) &=  \int_a^b f(g(x))g'(x)dx\\     \int_{g(a)}^{g(b)} f(u)du &= \int_a^b f(g(x))g'(x)dx     \end{align}$$ Perfectly fine. The problem with this proof is that it uses the fact that $F(g(b))-F(g(a))$ is the same as the integral of a function $f(u)$ from $g(a)$ to $g(b)$ . However, if we just want to find the antiderivative, that's a problem. In [this][1] proof, he says: $$\displaystyle \int f \left({\phi \left({u}\right)}\right) \phi' \left({u}\right) \ \mathrm d u = \int f \left({x}\right) \ \mathrm d x$$ where $x = \phi \left({u}\right)$ I can't accept this. It is basically saying that we should integrate with respect to $\phi(u)$ . Not saying that this is not possible, but it requires further theory about integration that is not on the scope of this proof. I realy really can't understand this proof as valid. The other text books I've read also say the same thing...","I know that the substitution rule works like this: By the chain rule: Then Perfectly fine. The problem with this proof is that it uses the fact that is the same as the integral of a function from to . However, if we just want to find the antiderivative, that's a problem. In [this][1] proof, he says: where I can't accept this. It is basically saying that we should integrate with respect to . Not saying that this is not possible, but it requires further theory about integration that is not on the scope of this proof. I realy really can't understand this proof as valid. The other text books I've read also say the same thing...","F(g(x))' = f(g(x))g'(x) \begin{align}
    \int_a^b F(g(x))'dx &= \int_a^b f(g(x))g'(x)dx\\
    F(g(b)) - F(g(a)) &=  \int_a^b f(g(x))g'(x)dx\\
    \int_{g(a)}^{g(b)} f(u)du &= \int_a^b f(g(x))g'(x)dx
    \end{align} F(g(b))-F(g(a)) f(u) g(a) g(b) \displaystyle \int f \left({\phi \left({u}\right)}\right) \phi' \left({u}\right) \ \mathrm d u = \int f \left({x}\right) \ \mathrm d x x = \phi \left({u}\right) \phi(u)","['calculus', 'real-analysis', 'integration']"
95,"Assume that $ f ∈ L([a, b])$ and $\int x^nf(x)dx=0$ for $n=0,1,2...$.",Assume that  and  for .," f ∈ L([a, b]) \int x^nf(x)dx=0 n=0,1,2...","Assume that $ f ∈ L([a, b])$ and $\int x^nf(x)dx=0$ for $n=0,1,2...$.  Prove $f=0$ a.e.  Since there exist polynomials going to f almost everywhere all I would need to do is bring the limit in to prove that $ ||f||_2 = 0$ and then I'm done.  But no success with this.  Thanks for any help.","Assume that $ f ∈ L([a, b])$ and $\int x^nf(x)dx=0$ for $n=0,1,2...$.  Prove $f=0$ a.e.  Since there exist polynomials going to f almost everywhere all I would need to do is bring the limit in to prove that $ ||f||_2 = 0$ and then I'm done.  But no success with this.  Thanks for any help.",,"['real-analysis', 'measure-theory', 'lebesgue-integral']"
96,Incorrect logic in popular proof of the irrationality of $\sqrt2$?,Incorrect logic in popular proof of the irrationality of ?,\sqrt2,"A popular proof of the irrationality of $\sqrt2$ is to first assume that the number is rational. This means that $\sqrt2=a/b$ where $a$ and $b$ are integers. Another assumption is that $a$ and $b$ are coprime. It turns out that this leads to a contradiction. And the conclusion is that $\sqrt2$ is not rational (because the assumption of rationality led to a contradiction). But what about the second assumption? Logically, the conclusion could also be that $a$ and $b$ are not coprime.. How can this be resolved? Edit: Thank you all for your comments. It cleared up my mind. Three aspects of this kind of proof are: mathematical, logical, and didactical. Mathematically, it is the case that any rational number can be written as a fraction with coprime numerator and denominator. But this point is often described in a way that is either literally a logical assumption, or in a way that is natural to interpret as a hidden logical assumption. In this case, the proof will have a flawed logic, because any of the two assumptions could be the cause of the contradiction. Didactically, this can cause confusion in the mind of the student, who might not be so accepting of the proofs conclusion. I used to be that student. So, how to resolve it? Many of you provided explanations on the mathematical aspect, which are all correct. But my main point is not of not understanding the proof, but of the use of flawed logic which leads to not understanding the proof. I am sure that a lot of teachers think or say something along the lines of WLOG, but for many students, this makes the proof weaker, because it is not so convincing. When we present this proof to our students, we need to have this in mind. Thank you.","A popular proof of the irrationality of $\sqrt2$ is to first assume that the number is rational. This means that $\sqrt2=a/b$ where $a$ and $b$ are integers. Another assumption is that $a$ and $b$ are coprime. It turns out that this leads to a contradiction. And the conclusion is that $\sqrt2$ is not rational (because the assumption of rationality led to a contradiction). But what about the second assumption? Logically, the conclusion could also be that $a$ and $b$ are not coprime.. How can this be resolved? Edit: Thank you all for your comments. It cleared up my mind. Three aspects of this kind of proof are: mathematical, logical, and didactical. Mathematically, it is the case that any rational number can be written as a fraction with coprime numerator and denominator. But this point is often described in a way that is either literally a logical assumption, or in a way that is natural to interpret as a hidden logical assumption. In this case, the proof will have a flawed logic, because any of the two assumptions could be the cause of the contradiction. Didactically, this can cause confusion in the mind of the student, who might not be so accepting of the proofs conclusion. I used to be that student. So, how to resolve it? Many of you provided explanations on the mathematical aspect, which are all correct. But my main point is not of not understanding the proof, but of the use of flawed logic which leads to not understanding the proof. I am sure that a lot of teachers think or say something along the lines of WLOG, but for many students, this makes the proof weaker, because it is not so convincing. When we present this proof to our students, we need to have this in mind. Thank you.",,['real-analysis']
97,What books on analysis after someone has finished all 3 by Rudin?,What books on analysis after someone has finished all 3 by Rudin?,,"What books on analysis would people recommend after someone has finished all three by Rudin ( Principles of Mathematical Analysis , Real and Complex Analysis , and Functional Analysis )? I am looking for well-organised books which go deep: either 1-2 which are broad in scope, or, if no single book at this advanced level offers a lot of breadth, then a set which pack a lot of breadth when considered together.","What books on analysis would people recommend after someone has finished all three by Rudin ( Principles of Mathematical Analysis , Real and Complex Analysis , and Functional Analysis )? I am looking for well-organised books which go deep: either 1-2 which are broad in scope, or, if no single book at this advanced level offers a lot of breadth, then a set which pack a lot of breadth when considered together.",,"['real-analysis', 'complex-analysis', 'analysis', 'functional-analysis', 'reference-request']"
98,The set of rational numbers doesn't satisfy the least-upper-bound property,The set of rational numbers doesn't satisfy the least-upper-bound property,,"This exercise is from Tom Apostol's Calculus Vol.1 : I 3.12 Exercises (page 29) 12) The Archimedian property of the real-number system was deduced as a consequence of the least-upper-bound axiom. Prove that the set of rational numbers satisfies the Archimedian property but not the least-upper-bound property . This shows that the Archimedian property does not imply the least-upper-bound axiom. I don't get what I have to do, and what it means -- to satisfy the least-upper-bound-property. I know it is an axiom, I didn't know it is a property . My closest guess is that if I have two rational numbers $x < y$, I can find an integer $n$ such that $nx > y$. And I don't need to refer to that axiom, because I can find this $n$ by playing with the integers inside the rational numbers. Previously from the book: The Archimedian property (page 26): If $x > 0$ and if $y$ is an arbitrary real number there exists a positive integer $n$ such that $nx > y$. The least-upper-bound axiom (page 25): Every nonempty set $S$ of real numbers which is bounded above has a supremum; that is, there is a real number $B$ such that $B = sup S$ .","This exercise is from Tom Apostol's Calculus Vol.1 : I 3.12 Exercises (page 29) 12) The Archimedian property of the real-number system was deduced as a consequence of the least-upper-bound axiom. Prove that the set of rational numbers satisfies the Archimedian property but not the least-upper-bound property . This shows that the Archimedian property does not imply the least-upper-bound axiom. I don't get what I have to do, and what it means -- to satisfy the least-upper-bound-property. I know it is an axiom, I didn't know it is a property . My closest guess is that if I have two rational numbers $x < y$, I can find an integer $n$ such that $nx > y$. And I don't need to refer to that axiom, because I can find this $n$ by playing with the integers inside the rational numbers. Previously from the book: The Archimedian property (page 26): If $x > 0$ and if $y$ is an arbitrary real number there exists a positive integer $n$ such that $nx > y$. The least-upper-bound axiom (page 25): Every nonempty set $S$ of real numbers which is bounded above has a supremum; that is, there is a real number $B$ such that $B = sup S$ .",,"['calculus', 'real-analysis']"
99,How to show if $|a_{n+1} - a_{n}| \le \frac{1}{2^n}$ then the sequence is Cauchy.,How to show if  then the sequence is Cauchy.,|a_{n+1} - a_{n}| \le \frac{1}{2^n},"Let $\{a_n\}$ be a sequence of real numbers such that $|a_{n+1} - a_n| \le \dfrac{1}{2^n}$.   I would like to show that this sequence is Cauchy. Letting $\epsilon > 0$, I said choose $N$ such that $1/2^N \le \epsilon$. However, I'm not sure if this is the right $N$, since $m, n > N$ doesn't seem to necessarily imply $|a_n - a_m| \le \epsilon$. Can someone show me the light?","Let $\{a_n\}$ be a sequence of real numbers such that $|a_{n+1} - a_n| \le \dfrac{1}{2^n}$.   I would like to show that this sequence is Cauchy. Letting $\epsilon > 0$, I said choose $N$ such that $1/2^N \le \epsilon$. However, I'm not sure if this is the right $N$, since $m, n > N$ doesn't seem to necessarily imply $|a_n - a_m| \le \epsilon$. Can someone show me the light?",,['real-analysis']
