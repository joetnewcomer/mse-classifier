,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Logarithm real analytic on $(0,\infty)$",Logarithm real analytic on,"(0,\infty)","I am working on a Tao Analysis II question. I have to prove that $log$ the inverse function of $exp$ is real analytic on $(0,\infty)$. I have already  proven that  $$  \forall x \in(-1,1): ln(1-x) = - \sum_{n=1}^\infty \frac{x^n}{n} $$ and that $$ \forall x \in (0,2): ln(x)= \sum_{n=1}^\infty \frac{(-1)^{n+1}}n (x-1)^n $$ Does this help ? Further i may not make use of complex numbers.","I am working on a Tao Analysis II question. I have to prove that $log$ the inverse function of $exp$ is real analytic on $(0,\infty)$. I have already  proven that  $$  \forall x \in(-1,1): ln(1-x) = - \sum_{n=1}^\infty \frac{x^n}{n} $$ and that $$ \forall x \in (0,2): ln(x)= \sum_{n=1}^\infty \frac{(-1)^{n+1}}n (x-1)^n $$ Does this help ? Further i may not make use of complex numbers.",,[]
1,Special integrals,Special integrals,,"There are special integrals such as the logarithmic integral and exponential integrals. I want to know if there are primitives for such integrals. If not, why not?","There are special integrals such as the logarithmic integral and exponential integrals. I want to know if there are primitives for such integrals. If not, why not?",,['real-analysis']
2,Uniformly Continuous Like Property of the Integration on Measure Space,Uniformly Continuous Like Property of the Integration on Measure Space,,"This is the Excercise 1.12 of Rudin's Real and Complex Analysis : Suppose $f\in L^1(\mu)$. Prove that to each $\epsilon>0$ there exists a $\delta>0$ such that $\int_{E}|f|d\mu<\epsilon$ whenever $\mu(E)<\delta$. This problem likes the uniformly continuous property. I tried to prove it by making contradiction, but I can't figure it out. Thanks!","This is the Excercise 1.12 of Rudin's Real and Complex Analysis : Suppose $f\in L^1(\mu)$. Prove that to each $\epsilon>0$ there exists a $\delta>0$ such that $\int_{E}|f|d\mu<\epsilon$ whenever $\mu(E)<\delta$. This problem likes the uniformly continuous property. I tried to prove it by making contradiction, but I can't figure it out. Thanks!",,"['real-analysis', 'measure-theory']"
3,How to compute this integral $\int_0^1 x^2 \arctan (e^{-x}) dx$,How to compute this integral,\int_0^1 x^2 \arctan (e^{-x}) dx,"I have tried hard to estimate this definite integral $$I:=\int_0^1 x^2 \arctan(e^{-x}) dx,$$ but all in vain. This exercise is from Page 204, 4.115, of Giaquinta & Modica's Book Mathematical Analysis, Foundations of One Variable. Actually I have tried using substitution $t=arctan(e^{-x})$, or $s=e^{-x}$, and with the help of integration by parts, but can not get the result. Even by using Maple, what I got is just the following:  $$1/2\,i \left( -2\,{\it polylog} \left( 4,i \right) +2\,{\it polylog}  \left( 4,-i \right) +{\it polylog} \left( 2,{\frac {i}{e}} \right)   \left( \ln  \left( e \right)  \right) ^{2}+2\,{\it polylog} \left( 3, {\frac {i}{e}} \right) \ln  \left( e \right) +2\,{\it polylog} \left(  4,{\frac {i}{e}} \right) -{\it polylog} \left( 2,{\frac {-i}{e}}  \right)  \left( \ln  \left( e \right)  \right) ^{2}-2\,{\it polylog}  \left( 3,{\frac {-i}{e}} \right) \ln  \left( e \right) -2\,{\it  polylog} \left( 4,{\frac {-i}{e}} \right)  \right)  \left( \ln   \left( e \right)  \right) ^{-3}. $$ Can anyone help me? Thanks in advance.","I have tried hard to estimate this definite integral $$I:=\int_0^1 x^2 \arctan(e^{-x}) dx,$$ but all in vain. This exercise is from Page 204, 4.115, of Giaquinta & Modica's Book Mathematical Analysis, Foundations of One Variable. Actually I have tried using substitution $t=arctan(e^{-x})$, or $s=e^{-x}$, and with the help of integration by parts, but can not get the result. Even by using Maple, what I got is just the following:  $$1/2\,i \left( -2\,{\it polylog} \left( 4,i \right) +2\,{\it polylog}  \left( 4,-i \right) +{\it polylog} \left( 2,{\frac {i}{e}} \right)   \left( \ln  \left( e \right)  \right) ^{2}+2\,{\it polylog} \left( 3, {\frac {i}{e}} \right) \ln  \left( e \right) +2\,{\it polylog} \left(  4,{\frac {i}{e}} \right) -{\it polylog} \left( 2,{\frac {-i}{e}}  \right)  \left( \ln  \left( e \right)  \right) ^{2}-2\,{\it polylog}  \left( 3,{\frac {-i}{e}} \right) \ln  \left( e \right) -2\,{\it  polylog} \left( 4,{\frac {-i}{e}} \right)  \right)  \left( \ln   \left( e \right)  \right) ^{-3}. $$ Can anyone help me? Thanks in advance.",,"['calculus', 'real-analysis', 'integration']"
4,Partition of Unity question,Partition of Unity question,,"I am starting to read the book ""Differential Forms in Algebraic Topology"" by Bott and Tu. In the proof of the exactness of the Mayer - Vietoris sequence (Proposition 2.3, page 22 - 23) a partition of unity $\{\rho_U,\rho_V\}$ subordinate to an open cover of two open sets $U,V$ is applied to a function $f$ which is defined on the intersection $U \cap V$. Then the authors emphasize that $\rho_V \,f$ is a function on $U$. I struggle to see why this is true, I thought $\rho_V$ has support contained in $V$, and so from the picture in the book I have the impression that the function $\rho_U\,f$ is a function with support in $U$. I am aware this must be false, the authors even stress the fact that one has to multiply by the partition function of the other open set! Any help to understand this would be great, many thanks !!","I am starting to read the book ""Differential Forms in Algebraic Topology"" by Bott and Tu. In the proof of the exactness of the Mayer - Vietoris sequence (Proposition 2.3, page 22 - 23) a partition of unity $\{\rho_U,\rho_V\}$ subordinate to an open cover of two open sets $U,V$ is applied to a function $f$ which is defined on the intersection $U \cap V$. Then the authors emphasize that $\rho_V \,f$ is a function on $U$. I struggle to see why this is true, I thought $\rho_V$ has support contained in $V$, and so from the picture in the book I have the impression that the function $\rho_U\,f$ is a function with support in $U$. I am aware this must be false, the authors even stress the fact that one has to multiply by the partition function of the other open set! Any help to understand this would be great, many thanks !!",,"['real-analysis', 'algebraic-topology', 'differential-geometry']"
5,Conditions of Cauchy's Mean Value Theorem,Conditions of Cauchy's Mean Value Theorem,,"I sometimes see Cauchy's Mean Value Theorem stated as follows: Let $f,\ g:\mathbb{R}\rightarrow\mathbb{R}$ be continuous on $[a,\ b]$ and differentiable on $(a,\ b)$. Suppose that $g(b) \neq g(a)$. Then there exists $c\in(a,\ b)$ such that $g'(c)\neq 0$ and such that $$\frac{f(b) - f(a)}{g(b) - g(a)} = \frac{f'(c)}{g'(c)}$$ I have never once seen a proper proof of the bolded fact and I'm beginning to wonder about the validity of it. Is the assumption $g(b) \neq g(a)$ really enough to prove the existence of such a $c$? Edit: I think my question is being misunderstood. I am not asking for a standard proof of the Cauchy Mean Value Theorem. The proofs I see assume that $g'(x) \neq 0\ \forall\ x\in(a,\ b)$. This version also claims $g'(c) \neq 0$ when $g(b) \neq g(a)$ (along with the standard continuity/differentiably conditions of course). How can we guarentee there exists such a $c$?","I sometimes see Cauchy's Mean Value Theorem stated as follows: Let $f,\ g:\mathbb{R}\rightarrow\mathbb{R}$ be continuous on $[a,\ b]$ and differentiable on $(a,\ b)$. Suppose that $g(b) \neq g(a)$. Then there exists $c\in(a,\ b)$ such that $g'(c)\neq 0$ and such that $$\frac{f(b) - f(a)}{g(b) - g(a)} = \frac{f'(c)}{g'(c)}$$ I have never once seen a proper proof of the bolded fact and I'm beginning to wonder about the validity of it. Is the assumption $g(b) \neq g(a)$ really enough to prove the existence of such a $c$? Edit: I think my question is being misunderstood. I am not asking for a standard proof of the Cauchy Mean Value Theorem. The proofs I see assume that $g'(x) \neq 0\ \forall\ x\in(a,\ b)$. This version also claims $g'(c) \neq 0$ when $g(b) \neq g(a)$ (along with the standard continuity/differentiably conditions of course). How can we guarentee there exists such a $c$?",,"['calculus', 'real-analysis']"
6,Continuity/differentiability at a point and in some neighbourhood of the point,Continuity/differentiability at a point and in some neighbourhood of the point,,"For a function $f: U \to \mathbb{R}$ where $U$ is a subset of $\mathbb{R}$, it seems like that it being continuous at a point doesn't imply that there is a neighbourhood of the point where it can be continuous. Similarly, it seems like that it being differentiable at a point doesn't imply that there is a neighbourhood of the point where it can be differentiable. I was wondering if there are some counterexamples to confirm the above? Added: What are some necessary and/or sufficient conditions for continuity/differentiability at a point and in some neighbourhood of the point to be equivalent? Can the case of continuity be generalized to mappings between topological spaces? Thanks and regards!","For a function $f: U \to \mathbb{R}$ where $U$ is a subset of $\mathbb{R}$, it seems like that it being continuous at a point doesn't imply that there is a neighbourhood of the point where it can be continuous. Similarly, it seems like that it being differentiable at a point doesn't imply that there is a neighbourhood of the point where it can be differentiable. I was wondering if there are some counterexamples to confirm the above? Added: What are some necessary and/or sufficient conditions for continuity/differentiability at a point and in some neighbourhood of the point to be equivalent? Can the case of continuity be generalized to mappings between topological spaces? Thanks and regards!",,['real-analysis']
7,Subset of $l^\infty$ compact or not,Subset of  compact or not,l^\infty,"I would be grateful if you can give me some hints for the following homework problem. Let $C$ be a subset of $l^\infty$ (with uniform norm) such that $C = \left\{(x_n) \mid |x_n|\leq \frac1n \,\forall n\geq 1\right\}$   Is $C$ a compact set or not? Honestly, I am stuck. I tried to use sequentially compactness and attempted to construct different sequences which may serve as counter examples but all of them failed to do so. I also thought that if $y_n$ is a constant sequence equal to $0$ and if we take the open ball $B(y_n,2)$ with radius $2$, does that count as a finite open cover of $C$? Thanks in advance.","I would be grateful if you can give me some hints for the following homework problem. Let $C$ be a subset of $l^\infty$ (with uniform norm) such that $C = \left\{(x_n) \mid |x_n|\leq \frac1n \,\forall n\geq 1\right\}$   Is $C$ a compact set or not? Honestly, I am stuck. I tried to use sequentially compactness and attempted to construct different sequences which may serve as counter examples but all of them failed to do so. I also thought that if $y_n$ is a constant sequence equal to $0$ and if we take the open ball $B(y_n,2)$ with radius $2$, does that count as a finite open cover of $C$? Thanks in advance.",,['real-analysis']
8,"$f:[0,1] \rightarrow \mathbb{R}$ is absolutely continuous and $f' \in \mathcal{L}_{2}$",is absolutely continuous and,"f:[0,1] \rightarrow \mathbb{R} f' \in \mathcal{L}_{2}","I am studying for an exam and am stuck on this practice problem. Suppose $f:[0,1] \rightarrow \mathbb{R}$ is absolutely continuous and $f' \in \mathcal{L}_{2}$.   If $f(0)=0$ does it follow that $\lim_{x\rightarrow 0} f(x)x^{-1/2}=0$?","I am studying for an exam and am stuck on this practice problem. Suppose $f:[0,1] \rightarrow \mathbb{R}$ is absolutely continuous and $f' \in \mathcal{L}_{2}$.   If $f(0)=0$ does it follow that $\lim_{x\rightarrow 0} f(x)x^{-1/2}=0$?",,['real-analysis']
9,How to prove that $ f(x) = \sum_{k=1}^\infty \frac{\sin((k + 1)!\;x )}{k!}$ is nowhere differentiable,How to prove that  is nowhere differentiable, f(x) = \sum_{k=1}^\infty \frac{\sin((k + 1)!\;x )}{k!},"This function is continuous, it follows by M-Weierstrass Test. But proving non-differentiability, I think it's too hard. Does someone know how can I prove this? Or at least have a paper with the proof? The function is  $$ f(x) = \sum_{k=1}^\infty \frac{\sin((k + 1)!\;x )}{k!}$$ Thanks!","This function is continuous, it follows by M-Weierstrass Test. But proving non-differentiability, I think it's too hard. Does someone know how can I prove this? Or at least have a paper with the proof? The function is  $$ f(x) = \sum_{k=1}^\infty \frac{\sin((k + 1)!\;x )}{k!}$$ Thanks!",,"['calculus', 'real-analysis', 'analysis']"
10,Classification of a kind of compact sets,Classification of a kind of compact sets,,"We define a Cantor set in the real line as a set that is: compact perfect with empty interior Is it true that if we have 2 sets in the real line with this properties (and no countable because the singleton also has this) then they are homeomorphic? Please don't give me a solution, I want to do this problem, but i need some advice. Clearly if I have a continuous bijection then it'll be a homeomorphism. Because the closed sets are compact and $f$ continuous map preserve this property, and so the image is also closed, so this map is also homeomorphism. But I don't know how to use the property of empty interior in this problem.","We define a Cantor set in the real line as a set that is: compact perfect with empty interior Is it true that if we have 2 sets in the real line with this properties (and no countable because the singleton also has this) then they are homeomorphic? Please don't give me a solution, I want to do this problem, but i need some advice. Clearly if I have a continuous bijection then it'll be a homeomorphism. Because the closed sets are compact and $f$ continuous map preserve this property, and so the image is also closed, so this map is also homeomorphism. But I don't know how to use the property of empty interior in this problem.",,"['real-analysis', 'general-topology', 'descriptive-set-theory']"
11,Divergence of $\sum\frac{\cos(\sqrt{n}x)}{\sqrt{n}}$,Divergence of,\sum\frac{\cos(\sqrt{n}x)}{\sqrt{n}},I have difficulties in showing the series $f(x)=\sum_{n=1}^\infty \frac{\cos(\sqrt{n}x)}{\sqrt{n}}$ is divergent at every real numbers $x$. However I cannot find any elementary methods to do this. Can anyone help me on this? Thanks.,I have difficulties in showing the series $f(x)=\sum_{n=1}^\infty \frac{\cos(\sqrt{n}x)}{\sqrt{n}}$ is divergent at every real numbers $x$. However I cannot find any elementary methods to do this. Can anyone help me on this? Thanks.,,"['real-analysis', 'sequences-and-series', 'trigonometry', 'convergence-divergence']"
12,Concerning sets of (Lebesgue) measure zero,Concerning sets of (Lebesgue) measure zero,,"Perhaps this has a simple answer, but I don't know (I wouldn't be asking if I did). Every set of outer measure zero is Lebesgue measurable with Lebesgue measure zero. Is the converse true? That is, if a set has Lebesgue measure zero, does it necessarily have outer measure zero? And I suppose I should specify that I'm thinking in $\mathbb{R}^n$.","Perhaps this has a simple answer, but I don't know (I wouldn't be asking if I did). Every set of outer measure zero is Lebesgue measurable with Lebesgue measure zero. Is the converse true? That is, if a set has Lebesgue measure zero, does it necessarily have outer measure zero? And I suppose I should specify that I'm thinking in $\mathbb{R}^n$.",,"['real-analysis', 'measure-theory']"
13,"Is it always true that if $x_n\to0$, $y_n\to0$ there exist $\epsilon_n\in\{-1,1\}$ such that both $\sum\epsilon_nx_n$and $\sum\epsilon_ny_n$ converge? [duplicate]","Is it always true that if ,  there exist  such that both and  converge? [duplicate]","x_n\to0 y_n\to0 \epsilon_n\in\{-1,1\} \sum\epsilon_nx_n \sum\epsilon_ny_n","This question already has answers here : Must there be a sequence $(\epsilon_n)$ of signs such that $\sum\epsilon_nx_n$ and $\sum\epsilon_ny_n$ are both convergent? (3 answers) Closed 4 months ago . I saw this interesting problem: Let $x_n$ and $y_n$ be real sequences with $x_n \to 0$ and $y_n \to 0$ as $n \to \infty$ . Show that there is a sequence $\varepsilon_n $ of signs (i.e. $\varepsilon_n \in \{−1, +1\}$ for all $n$ ) such that $\sum \varepsilon_n x_n$ is convergent. Must there be a sequence $\varepsilon_n$ of signs such that $\sum \varepsilon_n x_n$ and $\sum \varepsilon_n y_n$ are both convergent? I solved the first question as follows: either $\sum  |x_n|$ converge or diverge if it is convergent we are done and if it is divergent the way to prove that there is a sequence $\varepsilon_n $ of signs (i.e. $\varepsilon_n \in \{−1, +1\}$ for all $n$ ) such that $\sum \varepsilon_n x_n$ is convergent is the same way the Riemann's rearrangement theorem is proved I mean choosing some limit say $0$ and adding a  negative term then positive terms until the partial sum exceeds $0$ then adding the next negative and so on the limit will be $0$ this way is analogous to the proof of Riemann's rearrangement theorem. Riemann's rearrangement theorem: if $\sum {x_n}$ converges, but not absolutely. Suppose $-\infty \le a \le b \le \infty $ Then there exists a rearrangement $\sum {x_n'}$ with partial sums $s_n'$ such that $\lim \inf s_n' = a$ , $\lim \sup s_n' =b$ . (I don't know if this proof is right or wrong btw. if it is wrong I ask for other proof) As the way the second question is written it seems that the answer is no but I  couldn't find any counterexamples or proof of this claim.","This question already has answers here : Must there be a sequence $(\epsilon_n)$ of signs such that $\sum\epsilon_nx_n$ and $\sum\epsilon_ny_n$ are both convergent? (3 answers) Closed 4 months ago . I saw this interesting problem: Let and be real sequences with and as . Show that there is a sequence of signs (i.e. for all ) such that is convergent. Must there be a sequence of signs such that and are both convergent? I solved the first question as follows: either converge or diverge if it is convergent we are done and if it is divergent the way to prove that there is a sequence of signs (i.e. for all ) such that is convergent is the same way the Riemann's rearrangement theorem is proved I mean choosing some limit say and adding a  negative term then positive terms until the partial sum exceeds then adding the next negative and so on the limit will be this way is analogous to the proof of Riemann's rearrangement theorem. Riemann's rearrangement theorem: if converges, but not absolutely. Suppose Then there exists a rearrangement with partial sums such that , . (I don't know if this proof is right or wrong btw. if it is wrong I ask for other proof) As the way the second question is written it seems that the answer is no but I  couldn't find any counterexamples or proof of this claim.","x_n y_n x_n \to 0 y_n \to 0 n \to \infty \varepsilon_n  \varepsilon_n \in \{−1, +1\} n \sum \varepsilon_n x_n \varepsilon_n \sum \varepsilon_n x_n \sum \varepsilon_n y_n \sum  |x_n| \varepsilon_n  \varepsilon_n \in \{−1, +1\} n \sum \varepsilon_n x_n 0 0 0 \sum {x_n} -\infty \le a \le b \le \infty  \sum {x_n'} s_n' \lim \inf s_n' = a \lim \sup s_n' =b","['real-analysis', 'sequences-and-series', 'convergence-divergence', 'solution-verification', 'summation']"
14,"Is there an opposite of a dirac delta function, a function that is infinitely wide and infinitesimally high?","Is there an opposite of a dirac delta function, a function that is infinitely wide and infinitesimally high?",,"Is there an opposite of a dirac delta function, a function that is infinitely wide and infinitesimally high? The dirac delta function is defined as: $$ \delta(x) = \begin{cases} 0, & \text{if } x \neq 0\\ \infty, & \text{if } x = 0 \end{cases} $$ where $$ \int_{-\infty}^{\infty} \delta(x) \, dx = 1 \\ \int_{-\infty}^{\infty} f(x) \delta(x - a) \, dx = f(a) $$ Is there a function: $$ \epsilon(x) = \lim_{\epsilon \to 0} \epsilon $$ where $$ \int_{-\infty}^{\infty} \epsilon(x) \, dx = 1 $$ The dirac delta function $\delta(x)$ is infinitely high but infinitesimally wide, this new function $\epsilon(x)$ is infinitesimally high but infinitely wide. Im guessing theres no such function/distribution given how limits work and it probably would not be useful if it did, but is there any credence to this idea?","Is there an opposite of a dirac delta function, a function that is infinitely wide and infinitesimally high? The dirac delta function is defined as: where Is there a function: where The dirac delta function is infinitely high but infinitesimally wide, this new function is infinitesimally high but infinitely wide. Im guessing theres no such function/distribution given how limits work and it probably would not be useful if it did, but is there any credence to this idea?","
\delta(x) =
\begin{cases}
0, & \text{if } x \neq 0\\
\infty, & \text{if } x = 0
\end{cases}
 
\int_{-\infty}^{\infty} \delta(x) \, dx = 1
\\
\int_{-\infty}^{\infty} f(x) \delta(x - a) \, dx = f(a)
 
\epsilon(x) = \lim_{\epsilon \to 0} \epsilon
 
\int_{-\infty}^{\infty} \epsilon(x) \, dx = 1
 \delta(x) \epsilon(x)","['real-analysis', 'limits', 'functions', 'dirac-delta']"
15,"Does there exist a polynomial $f(x, y, z)$ which is positive iff |x|, |y|, |z| are sides of a triangle?","Does there exist a polynomial  which is positive iff |x|, |y|, |z| are sides of a triangle?","f(x, y, z)","The following problem appeared in the Kömal (Hungarian mathematical journal) in year 1997. Official Source: Problem N 145 at the end of the page here. Problem. Does there exist a polynomial $f(x, y, z)$ of real coefficients such that $f(x,y,z)$ is positive if and only if $|x|, |y|, |z|$ are sides of a triangle? The given inequalities $|x|+|y|>|z|$ , $|y|+|z|>|x|$ , and $|x|+|z|>|y|$ impose an interesting geometric region in $\mathbb{R}^3$ : I am guessing that the question has a negative answer -- no such polynomial should exist. Any thoughts / comments / hints are appreciated!","The following problem appeared in the Kömal (Hungarian mathematical journal) in year 1997. Official Source: Problem N 145 at the end of the page here. Problem. Does there exist a polynomial of real coefficients such that is positive if and only if are sides of a triangle? The given inequalities , , and impose an interesting geometric region in : I am guessing that the question has a negative answer -- no such polynomial should exist. Any thoughts / comments / hints are appreciated!","f(x, y, z) f(x,y,z) |x|, |y|, |z| |x|+|y|>|z| |y|+|z|>|x| |x|+|z|>|y| \mathbb{R}^3","['real-analysis', 'polynomials', 'contest-math', 'polyhedra']"
16,Show that there is no sequence with infinitely many zero's that converges to a non-zero number.,Show that there is no sequence with infinitely many zero's that converges to a non-zero number.,,"I've already finished what I think is a valid proof by contradiction, but I would like to know if it's correct. Let $(a_n)$ be a sequence with infinetly many $0$ s, such that $\lim(a_n) = a \ , a\neq0$ . Then $\forall\epsilon>0,\exists N\in \mathbb{N}$ such that $|a_n-a| < \epsilon \ ,\forall n>N$ . Let's choose an epsilon such that $0<\epsilon<|a|$ . Knowing that $(a_n)$ has infinitely many $0$ s, then $\exists n>N$ such that $a_n=0$ . That way, for that particular $n$ , we have that $|0-a|=|a|<|a|$ , which is absurd.We can now conclude that only one of the following happens: $(a_n)$ doesn't converge to a non-zero number or $(a_n)$ has a finite number of $0$ s. Is this proof correct? Before getting to this result I was trying to achieve that $\forall n > N$ we necessarily have that $a_n > 0 \ \lor \ a_n < 0$ , but I was unable to do it. Is it possible to achieve something like this?","I've already finished what I think is a valid proof by contradiction, but I would like to know if it's correct. Let be a sequence with infinetly many s, such that . Then such that . Let's choose an epsilon such that . Knowing that has infinitely many s, then such that . That way, for that particular , we have that , which is absurd.We can now conclude that only one of the following happens: doesn't converge to a non-zero number or has a finite number of s. Is this proof correct? Before getting to this result I was trying to achieve that we necessarily have that , but I was unable to do it. Is it possible to achieve something like this?","(a_n) 0 \lim(a_n) = a \ , a\neq0 \forall\epsilon>0,\exists N\in \mathbb{N} |a_n-a| < \epsilon \ ,\forall n>N 0<\epsilon<|a| (a_n) 0 \exists n>N a_n=0 n |0-a|=|a|<|a| (a_n) (a_n) 0 \forall n > N a_n > 0 \ \lor \ a_n < 0","['real-analysis', 'sequences-and-series', 'solution-verification']"
17,"Evaluate $\int_{0}^{\pi/2}x\sin^a (x) dx$, $a>0$","Evaluate ,",\int_{0}^{\pi/2}x\sin^a (x) dx a>0,"I want to evaluate $$\int_{0}^{\pi/2}x\sin^a (x)\, dx$$ where $a>0$ is a real number. I tried: $$I(a)= \int_{0}^{\pi/2}x\sin^a(x)\,dx = \int_{0}^{1}\frac{\arcsin x}{\sqrt{1-x^2}}x^a\,dx$$ $$ I(a)=\sum_{m\geq 1}\frac{4^m}{2m\left(2m+a\right)\binom{2m}{m}}$$ $$I(a)=\frac{1}{a+2}\cdot\phantom{}_3 F_2\left(1,1,1+\tfrac{a}{2};\tfrac{3}{2},2+\tfrac{a}{2};1\right)$$ Any other method please. Any help will be appreciated. Thank you. edit The series expansion of $ \arcsin(x)^2$ is $$2\;\arcsin(x)^2=\sum_{n=1}^\infty \frac{(2x)^{2n}}{n^2\binom{2n}{n}}$$ Differentiating the above series we get the formula used in the question.",I want to evaluate where is a real number. I tried: Any other method please. Any help will be appreciated. Thank you. edit The series expansion of is Differentiating the above series we get the formula used in the question.,"\int_{0}^{\pi/2}x\sin^a (x)\, dx a>0 I(a)= \int_{0}^{\pi/2}x\sin^a(x)\,dx = \int_{0}^{1}\frac{\arcsin x}{\sqrt{1-x^2}}x^a\,dx  I(a)=\sum_{m\geq 1}\frac{4^m}{2m\left(2m+a\right)\binom{2m}{m}} I(a)=\frac{1}{a+2}\cdot\phantom{}_3 F_2\left(1,1,1+\tfrac{a}{2};\tfrac{3}{2},2+\tfrac{a}{2};1\right)  \arcsin(x)^2 2\;\arcsin(x)^2=\sum_{n=1}^\infty \frac{(2x)^{2n}}{n^2\binom{2n}{n}}","['real-analysis', 'calculus', 'integration', 'definite-integrals', 'hypergeometric-function']"
18,Is there an analysis book with a proper introduction to mathematical proofs?,Is there an analysis book with a proper introduction to mathematical proofs?,,"I am taking my first analysis course in university. Since I am studying in Germany, there is no real distinction between calculus and real analysis, meaning this is the first university course on anything analysis that can be taken here. Therefore, I am new to the concepts of mathematical proofs. However, they are regularly used in the course and I often have problems following them. Since I am taking classes on mathematical principles and logic as well, my initial idea was to buy a dedicated textbook on proofs, e.g. Hammack's Book of Proof , Velleman's How to Prove It or Chartrand's Mathematical Proofs: A Transition to Advanced Mathematics , which I could use for analysis as well as the other classes. However, some answers to this question imply that this might not be the best way to go and I should rather buy a book on a certain mathematical field, in my case analysis, that contains a rigorous introduction to proofs in itself and applies them. Additionally, since time is limited, I am not quite sure whether it makes sense to focus on a book on mathematical proofs (which sometimes have several hundred pages) rather than the course itself. Since I am missing an introductory text to analysis anyway, I thought about following the recommendations in the answers and getting a book that teaches introductory analysis with a focus on formal notation as well as writing and understanding mathematical proofs. From what I have read so far, Abbott's Understanding Analysis and Tao's Analysis I + II might be great options for that purpose, however, one answer on the question I was referring to earlier suggests there might be a specific book which is specifically designed that way. Should I get a separate book on mathematical proofs or would I be better off with one of the analysis books I mentioned (or a completely different one)?","I am taking my first analysis course in university. Since I am studying in Germany, there is no real distinction between calculus and real analysis, meaning this is the first university course on anything analysis that can be taken here. Therefore, I am new to the concepts of mathematical proofs. However, they are regularly used in the course and I often have problems following them. Since I am taking classes on mathematical principles and logic as well, my initial idea was to buy a dedicated textbook on proofs, e.g. Hammack's Book of Proof , Velleman's How to Prove It or Chartrand's Mathematical Proofs: A Transition to Advanced Mathematics , which I could use for analysis as well as the other classes. However, some answers to this question imply that this might not be the best way to go and I should rather buy a book on a certain mathematical field, in my case analysis, that contains a rigorous introduction to proofs in itself and applies them. Additionally, since time is limited, I am not quite sure whether it makes sense to focus on a book on mathematical proofs (which sometimes have several hundred pages) rather than the course itself. Since I am missing an introductory text to analysis anyway, I thought about following the recommendations in the answers and getting a book that teaches introductory analysis with a focus on formal notation as well as writing and understanding mathematical proofs. From what I have read so far, Abbott's Understanding Analysis and Tao's Analysis I + II might be great options for that purpose, however, one answer on the question I was referring to earlier suggests there might be a specific book which is specifically designed that way. Should I get a separate book on mathematical proofs or would I be better off with one of the analysis books I mentioned (or a completely different one)?",,"['real-analysis', 'calculus', 'reference-request', 'proof-writing', 'book-recommendation']"
19,An ODE confusion,An ODE confusion,,"I was thinking about a ODE problem recently when I was reading about dynamical system. In school we used to solve the ODE problem $\frac{dx}{dt}=\sqrt{1-x^2}, x=0, t=0$ as $x=\sin(t),$ which will have the graph Now in dynamical system we can see that the fixed points are $\pm 1$ so specifically we can observe if the solution hits $1$ or $-1$ it should not increase or decrease from there. Specifically if we draw the phase diagram we can conclude that the solution passing through $(0,0)$ should look like and it seems reasonable. So I am surprised that we were taught wrong for many days. Isn't it? Or, am I making any mistake?","I was thinking about a ODE problem recently when I was reading about dynamical system. In school we used to solve the ODE problem as which will have the graph Now in dynamical system we can see that the fixed points are so specifically we can observe if the solution hits or it should not increase or decrease from there. Specifically if we draw the phase diagram we can conclude that the solution passing through should look like and it seems reasonable. So I am surprised that we were taught wrong for many days. Isn't it? Or, am I making any mistake?","\frac{dx}{dt}=\sqrt{1-x^2}, x=0, t=0 x=\sin(t), \pm 1 1 -1 (0,0)","['real-analysis', 'ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes', 'nonlinear-dynamics']"
20,Does Cantor's set contain a copy of each finite set?,Does Cantor's set contain a copy of each finite set?,,"Let $C$ denote the (usual) Cantor's set in the interval $[0,1]$ . If $S=\{x_1,x_2,\cdots,x_n\}$ is a finite set of points in $\mathbb R$ , is it true that $aS+b\subset C$ for some $a\neq 0$ and $b\in\mathbb R$ ? (i.e. $C$ contains a copy of $S$ ) For the fat Cantor-type sets the answer is yes (from Steinhaus Theorem), since in this case the fat Cantor-type set has positive Lebesgue measure. Also it is known that there exists Lebesgue measure zero sets that contain a copy of each finite set.","Let denote the (usual) Cantor's set in the interval . If is a finite set of points in , is it true that for some and ? (i.e. contains a copy of ) For the fat Cantor-type sets the answer is yes (from Steinhaus Theorem), since in this case the fat Cantor-type set has positive Lebesgue measure. Also it is known that there exists Lebesgue measure zero sets that contain a copy of each finite set.","C [0,1] S=\{x_1,x_2,\cdots,x_n\} \mathbb R aS+b\subset C a\neq 0 b\in\mathbb R C S","['real-analysis', 'measure-theory', 'cantor-set']"
21,When is it necessary to define a new function?,When is it necessary to define a new function?,,"For example: Lambert $W$ is a non-elementary function that can be defined as a solution for $x$ to $x\cdot e^x$ , but $\int{\frac{1}{\ln(t)}dt}$ is also supposed to be nonelementary. How do we know that those are not related by known operations? When is it necessary to define a new second or third etc. function?","For example: Lambert is a non-elementary function that can be defined as a solution for to , but is also supposed to be nonelementary. How do we know that those are not related by known operations? When is it necessary to define a new second or third etc. function?",W x x\cdot e^x \int{\frac{1}{\ln(t)}dt},"['real-analysis', 'logarithms', 'closed-form', 'lambert-w', 'elementary-functions']"
22,A continuous function such that the inverse image of a bounded set is bounded,A continuous function such that the inverse image of a bounded set is bounded,,"Suppose $f:\mathbb{R}\longrightarrow \mathbb{R}$ be an arbitrary continuous fuction such that the inverse image of a bounded set is bounded. Then show that, $1$ ) The image under $f$ of a closed set is closed. $2$ ) $f$ is not necessarily a surjective function. My attempt ( $1$ ) : Say, $X\subset \mathbb{R}$ is an arbitrary closed set, such that $f(X)=Y\subset \mathbb{R}$ is not closed. Then I am trying to prove by contradiction. Case 1 : $X$ is bounded. Hence $X$ is closed and bounded $\implies$ compact. Since $f$ is continuous, $f(X)=Y$ is also compact $\implies Y$ is closed. (a contradiction) Case 2 : $X$ is not closed and not bounded. So $f(X)=Y$ is not bounded and not closed. If $Y$ is open and not bounded then $Y=\mathbb{R}$ , but $\mathbb{R}$ is clopen (closed and open). (a contradiction). Hence, $f(X)=Y$ where $X$ is closed and not bounded and $Y$ is neither open nor closed and also not bounded. Now I am confused that how to get a contradiction of this final case. My attempt ( $2$ ) : Here, I cannot understand the meaning of the inverse image of a bounded set. I think inverse function will exist only for the bijections. In this context how I can find a continuous map $f$ which is not a surjection, and inverse image of the bounded set is bounded.","Suppose be an arbitrary continuous fuction such that the inverse image of a bounded set is bounded. Then show that, ) The image under of a closed set is closed. ) is not necessarily a surjective function. My attempt ( ) : Say, is an arbitrary closed set, such that is not closed. Then I am trying to prove by contradiction. Case 1 : is bounded. Hence is closed and bounded compact. Since is continuous, is also compact is closed. (a contradiction) Case 2 : is not closed and not bounded. So is not bounded and not closed. If is open and not bounded then , but is clopen (closed and open). (a contradiction). Hence, where is closed and not bounded and is neither open nor closed and also not bounded. Now I am confused that how to get a contradiction of this final case. My attempt ( ) : Here, I cannot understand the meaning of the inverse image of a bounded set. I think inverse function will exist only for the bijections. In this context how I can find a continuous map which is not a surjection, and inverse image of the bounded set is bounded.",f:\mathbb{R}\longrightarrow \mathbb{R} 1 f 2 f 1 X\subset \mathbb{R} f(X)=Y\subset \mathbb{R} X X \implies f f(X)=Y \implies Y X f(X)=Y Y Y=\mathbb{R} \mathbb{R} f(X)=Y X Y 2 f,"['real-analysis', 'continuity', 'compactness', 'closed-map']"
23,Nowhere continuous functions $f: \mathbb{R} \to \mathbb{R}$ such that $f\bigl(f(x)\bigr) = \frac{f(2x)}{2}$ [closed],Nowhere continuous functions  such that  [closed],f: \mathbb{R} \to \mathbb{R} f\bigl(f(x)\bigr) = \frac{f(2x)}{2},"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question I'm looking for a function $f: \mathbb{R} \to \mathbb{R}$ that is continuous at no point and satisfies the identity $$f\bigl(f(x)\bigr) = \frac{f(2x)}{2}$$ for all $x \in \mathbb{R}$ . This is not a homework question, but rather a curiosity of mine. There is absolutely no context, I just thought of it. $f(x)=x$ is the example that made me think of this problem, but there are other solutions of the functional equation obviously, for example $f(x)=0$ for all $x$ . I was only able to notice that if $f$ is not identically $0$ then $f(\mathbb{R})$ is an infinite set.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question I'm looking for a function that is continuous at no point and satisfies the identity for all . This is not a homework question, but rather a curiosity of mine. There is absolutely no context, I just thought of it. is the example that made me think of this problem, but there are other solutions of the functional equation obviously, for example for all . I was only able to notice that if is not identically then is an infinite set.",f: \mathbb{R} \to \mathbb{R} f\bigl(f(x)\bigr) = \frac{f(2x)}{2} x \in \mathbb{R} f(x)=x f(x)=0 x f 0 f(\mathbb{R}),"['real-analysis', 'calculus', 'functions', 'continuity', 'functional-equations']"
24,Is the closed form of $\int_0^1 \frac{x\ln^a(1+x)}{1+x^2}dx$ known in the literature?,Is the closed form of  known in the literature?,\int_0^1 \frac{x\ln^a(1+x)}{1+x^2}dx,"We know how hard these integrals $$\int_0^1 \frac{x\ln(1+x)}{1+x^2}dx; \int_0^1 \frac{x\ln^2(1+x)}{1+x^2}dx; \int_0^1 \frac{x\ln^3(1+x)}{1+x^2}dx; ...$$ can be. So I decided to come up with a generalization and I succeeded: With $x=1/(1+y)$ , we have $$\int_0^1 \frac{y\ln^a(1+y)}{1+y^2}dy=(-1)^a\left[\int_{1/2}^1\frac{\ln^a(x)}{x}dx+\int_{1/2}^1\frac{1-2x}{x^2+(1-x)^2}\ln^a(x)dx\right]$$ $$=(-1)^a\left[\frac{(-1)^a}{a+1}\ln^{a+1}(2)+\Re\int_{1/2}^1\frac{1+i}{1-(1+i)x}\ln^a(x)dx\right].$$ For the remaining integral, we need to find $\int_{1/2}^1\frac{z}{1-zx}\ln^a(x)dx:$ $$\int_{1/2}^1\frac{z}{1-zx}\ln^a(x)dx=\int_{0}^1\frac{z}{1-zx}\ln^a(x)dx-\underbrace{\int_0^{1/2}\frac{z}{1-zx}\ln^a(x)dx}_{x=y/2}$$ $$=(-a)^aa!\,\text{Li}_{a+1}(z)-\int_0^{1}\frac{\frac{z}2}{1-\frac{z}2y}\ln^a(y/2)dy$$ $$=(-a)^aa!\,\text{Li}_{a+1}(z)-\sum_{k=0}^a{a\choose k}(-\ln(2))^{a-k}\int_0^1 \frac{\frac{z}2}{1-\frac{z}2y}\ln^k(y)dy$$ $$=(-a)^aa!\,\text{Li}_{a+1}(z)-\sum_{k=0}^a{a\choose k}(-\ln(2))^{a-k}(-1)^kk!\,\text{Li}_{k+1}\left(\frac{z}2\right)$$ $$=(-1)^aa!\,\left[\text{Li}_{a+1}(z)-\sum_{k=0}^a\frac{\ln^{a-k}(2)}{(a-k)!}\text{Li}_{k+1}\left(\frac{z}2\right)\right].\quad\quad (*)$$ Using this integral, we get $$\int_0^1\frac{x\ln^a(1+x)}{1+x^2}dx=\frac{\ln^{a+1}(2)}{a+1}+a!\,\Re\left[\text{Li}_{a+1}(1+i)-\sum_{k=0}^a\frac{\ln^{a-k}(2)}{(a-k)!}\text{Li}_{k+1}\left(\frac{1+i}2\right)\right].$$ Question : Is this generalization known in the mathematical literature? and if so, may I get the reference? Thanks, Bonus : Following the same approach above, using $\frac{1}{x^2+(1-x)^2}=\Im\,\frac{1+i}{1-(1+i)x}$ , also gives $$\int_0^1\frac{\ln^a(1+x)}{1+x^2}dx=a!\,\Im\left[\text{Li}_{a+1}(1+i)-\sum_{k=0}^a\frac{\ln^{a-k}(2)}{(a-k)!}\text{Li}_{k+1}\left(\frac{1+i}2\right)\right].$$ Also if we divide the identity in $(*)$ by $z$ then integrate $\int_0^1$ w.r.t $z$ , we obtain $$\int_{1/2}^1\frac{\ln(1-x)\ln^a(x)}{x}dx=(-1)^{a-1}a!\,\left[\zeta(a+2)-\sum_{k=0}^a\frac{\ln^{a-k}(2)}{(a-k)!}\text{Li}_{k+2}\left(\frac{1}2\right)\right].$$","We know how hard these integrals can be. So I decided to come up with a generalization and I succeeded: With , we have For the remaining integral, we need to find Using this integral, we get Question : Is this generalization known in the mathematical literature? and if so, may I get the reference? Thanks, Bonus : Following the same approach above, using , also gives Also if we divide the identity in by then integrate w.r.t , we obtain","\int_0^1 \frac{x\ln(1+x)}{1+x^2}dx;
\int_0^1 \frac{x\ln^2(1+x)}{1+x^2}dx;
\int_0^1 \frac{x\ln^3(1+x)}{1+x^2}dx;
... x=1/(1+y) \int_0^1 \frac{y\ln^a(1+y)}{1+y^2}dy=(-1)^a\left[\int_{1/2}^1\frac{\ln^a(x)}{x}dx+\int_{1/2}^1\frac{1-2x}{x^2+(1-x)^2}\ln^a(x)dx\right] =(-1)^a\left[\frac{(-1)^a}{a+1}\ln^{a+1}(2)+\Re\int_{1/2}^1\frac{1+i}{1-(1+i)x}\ln^a(x)dx\right]. \int_{1/2}^1\frac{z}{1-zx}\ln^a(x)dx: \int_{1/2}^1\frac{z}{1-zx}\ln^a(x)dx=\int_{0}^1\frac{z}{1-zx}\ln^a(x)dx-\underbrace{\int_0^{1/2}\frac{z}{1-zx}\ln^a(x)dx}_{x=y/2} =(-a)^aa!\,\text{Li}_{a+1}(z)-\int_0^{1}\frac{\frac{z}2}{1-\frac{z}2y}\ln^a(y/2)dy =(-a)^aa!\,\text{Li}_{a+1}(z)-\sum_{k=0}^a{a\choose k}(-\ln(2))^{a-k}\int_0^1 \frac{\frac{z}2}{1-\frac{z}2y}\ln^k(y)dy =(-a)^aa!\,\text{Li}_{a+1}(z)-\sum_{k=0}^a{a\choose k}(-\ln(2))^{a-k}(-1)^kk!\,\text{Li}_{k+1}\left(\frac{z}2\right) =(-1)^aa!\,\left[\text{Li}_{a+1}(z)-\sum_{k=0}^a\frac{\ln^{a-k}(2)}{(a-k)!}\text{Li}_{k+1}\left(\frac{z}2\right)\right].\quad\quad (*) \int_0^1\frac{x\ln^a(1+x)}{1+x^2}dx=\frac{\ln^{a+1}(2)}{a+1}+a!\,\Re\left[\text{Li}_{a+1}(1+i)-\sum_{k=0}^a\frac{\ln^{a-k}(2)}{(a-k)!}\text{Li}_{k+1}\left(\frac{1+i}2\right)\right]. \frac{1}{x^2+(1-x)^2}=\Im\,\frac{1+i}{1-(1+i)x} \int_0^1\frac{\ln^a(1+x)}{1+x^2}dx=a!\,\Im\left[\text{Li}_{a+1}(1+i)-\sum_{k=0}^a\frac{\ln^{a-k}(2)}{(a-k)!}\text{Li}_{k+1}\left(\frac{1+i}2\right)\right]. (*) z \int_0^1 z \int_{1/2}^1\frac{\ln(1-x)\ln^a(x)}{x}dx=(-1)^{a-1}a!\,\left[\zeta(a+2)-\sum_{k=0}^a\frac{\ln^{a-k}(2)}{(a-k)!}\text{Li}_{k+2}\left(\frac{1}2\right)\right].","['real-analysis', 'calculus', 'integration', 'reference-request', 'polylogarithm']"
25,"Uniformly continuous and integrable function $f:[0,\infty)\to (0,\infty)$ with $\lim_{t\to\infty}f(t)=C$ satisfies $\sum_{n=1}^\infty f(n)=\infty$",Uniformly continuous and integrable function  with  satisfies,"f:[0,\infty)\to (0,\infty) \lim_{t\to\infty}f(t)=C \sum_{n=1}^\infty f(n)=\infty","Let $f:[0,\infty)\to (0,\infty)$ be a uniformly continuous function with $\lim_{t\to\infty}f(t)=C.$ Prove that $\int_0^\infty f(t)dt=\infty$ $\Longleftrightarrow$ $\sum_{n=1}^\infty f(n)=\infty$ . My attempt For the case $C>0$ , I can prove that both $\int_0^\infty f(t)dt=\infty$ and $\sum_{n=1}^\infty f(n)=\infty$ hold. The difficulty is in the case that $C=0$ and I got stuck. Any helps would be highly appreciated!","Let be a uniformly continuous function with Prove that . My attempt For the case , I can prove that both and hold. The difficulty is in the case that and I got stuck. Any helps would be highly appreciated!","f:[0,\infty)\to (0,\infty) \lim_{t\to\infty}f(t)=C. \int_0^\infty f(t)dt=\infty \Longleftrightarrow \sum_{n=1}^\infty f(n)=\infty C>0 \int_0^\infty f(t)dt=\infty \sum_{n=1}^\infty f(n)=\infty C=0","['real-analysis', 'integration']"
26,A convergent sequence has either a maximum or a minimum or both.,A convergent sequence has either a maximum or a minimum or both.,,At odd times when I am not writing my Ph.d thesis I am solving some problems from Polya-Szego book which seems to me really interesting. Here is the one of the problems (Problem 106 from Volume 1) which I've solved and would be grateful if you can take a look. A convergent sequence has either a maximum or a minimum or both. Suppose $\{a_n\}$ be a convergent sequence but there is no $\min \limits_{n\in \mathbb{N}} a_n$ and there is no $\max \limits_{n\in \mathbb{N}}a_n$ . It is easy to show the following fact: If the sequence $\{a_n\}$ does not have a minimum then $\forall n\in \mathbb{N}$ $\exists k>n$ such that $a_k<a_n$ . In the same way you can show that if the sequence $\{a_n\}$ does not have a maximum then $\forall n\in \mathbb{N}$ $\exists m>n$ such that $a_m>a_n$ . Then using it we can construct two increasing subsequences $n_1<n_2<n_3<\dots$ and $m_1<m_2<m_3<\dots$ such that $n_1=m_1=1$ and $a_{n_1}>a_{n_2}>a_{n_3}>\dots $ and $a_{m_1}<a_{m_2}<a_{m_3}<\dots$ . Since $\{a_n\}$ is convergent then $a_{n_k}\to a$ and $a_{m_k}\to a$ as $k\to \infty$ . Then $a=\inf \limits_{k\in \mathbb{N}}a_{n_k}$ and $a=\sup \limits_{k\in \mathbb{N}}a_{m_k}$ . Hence $a_{n_1}>a$ and $a_{m_1}<a$ but this is a contradiction because $n_1=m_1=1$ . Please let me know is everything valid in this short proof?,At odd times when I am not writing my Ph.d thesis I am solving some problems from Polya-Szego book which seems to me really interesting. Here is the one of the problems (Problem 106 from Volume 1) which I've solved and would be grateful if you can take a look. A convergent sequence has either a maximum or a minimum or both. Suppose be a convergent sequence but there is no and there is no . It is easy to show the following fact: If the sequence does not have a minimum then such that . In the same way you can show that if the sequence does not have a maximum then such that . Then using it we can construct two increasing subsequences and such that and and . Since is convergent then and as . Then and . Hence and but this is a contradiction because . Please let me know is everything valid in this short proof?,\{a_n\} \min \limits_{n\in \mathbb{N}} a_n \max \limits_{n\in \mathbb{N}}a_n \{a_n\} \forall n\in \mathbb{N} \exists k>n a_k<a_n \{a_n\} \forall n\in \mathbb{N} \exists m>n a_m>a_n n_1<n_2<n_3<\dots m_1<m_2<m_3<\dots n_1=m_1=1 a_{n_1}>a_{n_2}>a_{n_3}>\dots  a_{m_1}<a_{m_2}<a_{m_3}<\dots \{a_n\} a_{n_k}\to a a_{m_k}\to a k\to \infty a=\inf \limits_{k\in \mathbb{N}}a_{n_k} a=\sup \limits_{k\in \mathbb{N}}a_{m_k} a_{n_1}>a a_{m_1}<a n_1=m_1=1,"['real-analysis', 'sequences-and-series']"
27,Does $(1 + f(x))^x \to e^k$ if $xf(x) \to k$?,Does  if ?,(1 + f(x))^x \to e^k xf(x) \to k,"It's well known that $\displaystyle{\lim_{x \to \infty} \left(1 + \frac{k}{x}\right)^x = e^k}$ . Suppose $xf(x) \to k$ as $x \to \infty$ . Do we necessarily have $(1 + f(x))^x \to e^k$ ? If $f$ happens to be differentiable, and if we also have $\displaystyle{\frac{f'(x)}{-x^{-2}} \to k}$ , then the answer to the above question is yes, as can be seen by an application of L'Hopital's rule. But without assuming this second limit exists, I don't think L'Hopital's rule is relevant. If it's any easier, I'd also be interested in the answer for the special case when $k = 1$ . This is a problem I thought of as I was solving problem 3.2.1(b) (page 72) from ""Problems in Mathematical Analysis I"" by Kaczor and Nowak.","It's well known that . Suppose as . Do we necessarily have ? If happens to be differentiable, and if we also have , then the answer to the above question is yes, as can be seen by an application of L'Hopital's rule. But without assuming this second limit exists, I don't think L'Hopital's rule is relevant. If it's any easier, I'd also be interested in the answer for the special case when . This is a problem I thought of as I was solving problem 3.2.1(b) (page 72) from ""Problems in Mathematical Analysis I"" by Kaczor and Nowak.",\displaystyle{\lim_{x \to \infty} \left(1 + \frac{k}{x}\right)^x = e^k} xf(x) \to k x \to \infty (1 + f(x))^x \to e^k f \displaystyle{\frac{f'(x)}{-x^{-2}} \to k} k = 1,"['real-analysis', 'limits']"
28,Prove that the set of measures on $\mathbb{R}$ isn't separable.,Prove that the set of measures on  isn't separable.,\mathbb{R},"I think I've got the main idea to prove this, but I'm stuck on a single step.  Here is the problem statement. Here, $\mathbb{F} = \mathbb{R}$ or $\mathbb{C}$ and $\mathcal{B} :=$ the Borel subsets of $\mathbb{R}$ .  This problem is from Axler's Measure and Integration Ch.9. Let $M_{\mathbb{F}}(\mathcal{B}) = \{\nu: \mathcal{B} \longrightarrow \mathbb{F} \: | \: \text{$\nu$ is a measure on the Borel subsets of $\mathbb{R}$}\}$ .  Prove that $(M_{\mathbb{F}}(\mathcal{B}),||\cdot||)$ isn't separable, where $||\cdot||$ is given by $||\nu|| = |\nu|(\mathbb{R})$ . My proof attempt: Let $X = \{\nu_1,\nu_2,\dots\}$ be a countable collection of measures $\nu_k \in M_{\mathbb{F}}(\mathcal{B})$ for $k \in \mathbb{N}$ . Define the sequence $(E_k) \subset \mathcal{B}$ such that $E_j \cap E_k = \emptyset$ if and only if $j \neq k$ .  Define $\nu: \mathcal{B} \longrightarrow \mathbb{F}$ such that $\nu(E_k)$ is given by: \begin{cases}                                  2 & \text{if $|\nu_k(E_k)| < 1$} \\                                  0 & \text{if $|\nu_k(E_k)| \geq 1$} \\   \end{cases} Then given any $j \in \mathbb{N}$ , notice that $$||\nu-\nu_j|| \geq \sum_{k=1}^{j} |(\nu-\nu_j)(E_k)| = \sum_{k=1}^{j} |\nu(E_k)-\nu_j(E_k)| \geq \sum_{k=1}^{j} \bigg||\nu(E_k)|-|\nu_j(E_k)| \bigg|$$ $$ = \left(\sum_{k=1}^{j-1} \bigg||\nu(E_k)|-|\nu_j(E_k)| \bigg|\right) + \bigg||\nu(E_j)|-|\nu_j(E_j)| \bigg|  \geq \bigg||\nu(E_j)|-|\nu_j(E_j)| \bigg| \geq 1.$$ Therefore, $||\nu-\nu_j|| \geq 1 \implies B(\frac{1}{2},\nu) \cap X = \emptyset \implies$ $X$ isn't dense in $M_{\mathbb{F}}(\mathcal{B})$ . The main issue here is that i don't know how to prove that the 'measure' $\nu$ i defined is actually a measure.  Any ideas? Thanks.","I think I've got the main idea to prove this, but I'm stuck on a single step.  Here is the problem statement. Here, or and the Borel subsets of .  This problem is from Axler's Measure and Integration Ch.9. Let .  Prove that isn't separable, where is given by . My proof attempt: Let be a countable collection of measures for . Define the sequence such that if and only if .  Define such that is given by: Then given any , notice that Therefore, isn't dense in . The main issue here is that i don't know how to prove that the 'measure' i defined is actually a measure.  Any ideas? Thanks.","\mathbb{F} = \mathbb{R} \mathbb{C} \mathcal{B} := \mathbb{R} M_{\mathbb{F}}(\mathcal{B}) = \{\nu: \mathcal{B} \longrightarrow \mathbb{F} \: | \: \text{\nu is a measure on the Borel subsets of \mathbb{R}}\} (M_{\mathbb{F}}(\mathcal{B}),||\cdot||) ||\cdot|| ||\nu|| = |\nu|(\mathbb{R}) X = \{\nu_1,\nu_2,\dots\} \nu_k \in M_{\mathbb{F}}(\mathcal{B}) k \in \mathbb{N} (E_k) \subset \mathcal{B} E_j \cap E_k = \emptyset j \neq k \nu: \mathcal{B} \longrightarrow \mathbb{F} \nu(E_k) \begin{cases}
                                 2 & \text{if |\nu_k(E_k)| < 1} \\
                                 0 & \text{if |\nu_k(E_k)| \geq 1} \\
  \end{cases} j \in \mathbb{N} ||\nu-\nu_j|| \geq \sum_{k=1}^{j} |(\nu-\nu_j)(E_k)| = \sum_{k=1}^{j} |\nu(E_k)-\nu_j(E_k)| \geq \sum_{k=1}^{j} \bigg||\nu(E_k)|-|\nu_j(E_k)| \bigg|  = \left(\sum_{k=1}^{j-1} \bigg||\nu(E_k)|-|\nu_j(E_k)| \bigg|\right) + \bigg||\nu(E_j)|-|\nu_j(E_j)| \bigg|  \geq \bigg||\nu(E_j)|-|\nu_j(E_j)| \bigg| \geq 1. ||\nu-\nu_j|| \geq 1 \implies B(\frac{1}{2},\nu) \cap X = \emptyset \implies X M_{\mathbb{F}}(\mathcal{B}) \nu","['real-analysis', 'analysis', 'measure-theory']"
29,Calculate integral $\int\limits_{0}^{2\pi}\frac{dx}{\left ( 1+n^2\sin^2 x \right )^2}$,Calculate integral,\int\limits_{0}^{2\pi}\frac{dx}{\left ( 1+n^2\sin^2 x \right )^2},I recently saw the integral problem $$\int\limits_{0}^{2\pi}\frac{dx}{\left ( 1+n^2\sin^2 x \right )^2}$$ and tried to solve it. Below is what I did. Interesting to look at other easier solutions. $$\int\limits_{0}^{2\pi}\frac{dx}{\left ( 1+n^2\sin^2 x \right )^2}=4\int_{0}^{\pi /2}\frac{dx}{\left ( 1+n^2\sin^2 x \right )^2}\\\overset{t=\operatorname{tg} x}{=}\int\limits_{0}^{\infty }\frac{1+t^2}{\left ( 1+\left ( 1+n^2 \right )t^2 \right )^2}dt\\  \overset{t=\frac{y}{\sqrt{1+n^2}}}{=}\frac{4}{\left ( 1+n^2 \right )\sqrt{1+n^2}}\int\limits_{0}^{\infty }\frac{1+n^2+y^2}{\left ( 1+y^2 \right )^2}dy\\ \overset{y=\operatorname{tg} \theta }{=}\frac{4}{\left ( 1+n^2 \right )\sqrt{1+n^2}}\int_{0}^{\pi /2}\left ( 1+n^2\cos^2 \theta  \right )d\theta \\ =\frac{\pi \left ( 2+n^2 \right )}{\left ( 1+n^2 \right )\sqrt{1+n^2}}$$,I recently saw the integral problem and tried to solve it. Below is what I did. Interesting to look at other easier solutions.,"\int\limits_{0}^{2\pi}\frac{dx}{\left ( 1+n^2\sin^2 x \right )^2} \int\limits_{0}^{2\pi}\frac{dx}{\left ( 1+n^2\sin^2 x \right )^2}=4\int_{0}^{\pi /2}\frac{dx}{\left ( 1+n^2\sin^2 x \right )^2}\\\overset{t=\operatorname{tg} x}{=}\int\limits_{0}^{\infty }\frac{1+t^2}{\left ( 1+\left ( 1+n^2 \right )t^2 \right )^2}dt\\ 
\overset{t=\frac{y}{\sqrt{1+n^2}}}{=}\frac{4}{\left ( 1+n^2 \right )\sqrt{1+n^2}}\int\limits_{0}^{\infty }\frac{1+n^2+y^2}{\left ( 1+y^2 \right )^2}dy\\ \overset{y=\operatorname{tg} \theta }{=}\frac{4}{\left ( 1+n^2 \right )\sqrt{1+n^2}}\int_{0}^{\pi /2}\left ( 1+n^2\cos^2 \theta  \right )d\theta \\
=\frac{\pi \left ( 2+n^2 \right )}{\left ( 1+n^2 \right )\sqrt{1+n^2}}","['real-analysis', 'calculus', 'integration']"
30,Find the maximum value of $f(\theta)=\frac{\sin(\theta-a)\sin(\theta-b)}{\sin(\theta-c)\sin(\theta-d)}$,Find the maximum value of,f(\theta)=\frac{\sin(\theta-a)\sin(\theta-b)}{\sin(\theta-c)\sin(\theta-d)},"Context : I am deriving the formula for active earth pressure for an inclined soil mass behind a retaining wall. The formula is given in a textbooks (page 25 of Theoretical Foundation Engineering by Braja M. Das ). I am restating the problem without any soil mechanics terminology. Problem : Find the maximum value of the following function $-$ $ f(\theta)=\dfrac{\sin(\theta-a)\sin(\theta-b)}{\sin(\theta-c)\sin(\theta-d)} $ Solution : Maximum value of the function, as given in the book, is $-$ $ f_{max}=\dfrac{\sin^{2}(a-b)}{\left\{\sqrt{\sin(d-b)\sin(a-c)}+\sqrt{\sin(d-a)\sin(b-c)}\right\}^{2}} $ My attempt : I tried deriving the above maxima by the following procedure $-$ Obtain the point of maxima, $\theta_{max}$ , by $\left.\dfrac{df(\theta)}{d\theta}\right|_{\theta=\theta_{max}}=0$ I get the following equation $-$ $$ \begin{aligned} \sin(2\theta_{max}-a-b)\cos(d-c)-\sin(2\theta_{max}-c-d)\cos(a-b)=\sin(c+d-a-b) \end{aligned} $$ Which can also be written as $-$ $$ \sin(2\theta_{max}-a-b-c-d)\left[\cos(d+c)\cos(d-c)-\cos(a+b)\cos(a-b)\right]+\cos(2\theta_{max}-a-b-c-d)\left[\sin(d+c)\cos(d-c)-\sin(a+b)\cos(a-b)\right]=\sin(c+d-a-b) $$ I can obtain $2\theta_{max}-a-b-c-d$ (by writing the above equation as a quadratic), but the equation becomes too complicated to simplify and put back in $f(\theta)$ . Please let me know if there is a better method or if I should go ahead with this method. Thanks PS: I have checked the accuracy of the solution by comparing with the optimum solution in Python. The solution is correct.","Context : I am deriving the formula for active earth pressure for an inclined soil mass behind a retaining wall. The formula is given in a textbooks (page 25 of Theoretical Foundation Engineering by Braja M. Das ). I am restating the problem without any soil mechanics terminology. Problem : Find the maximum value of the following function Solution : Maximum value of the function, as given in the book, is My attempt : I tried deriving the above maxima by the following procedure Obtain the point of maxima, , by I get the following equation Which can also be written as I can obtain (by writing the above equation as a quadratic), but the equation becomes too complicated to simplify and put back in . Please let me know if there is a better method or if I should go ahead with this method. Thanks PS: I have checked the accuracy of the solution by comparing with the optimum solution in Python. The solution is correct.","- 
f(\theta)=\dfrac{\sin(\theta-a)\sin(\theta-b)}{\sin(\theta-c)\sin(\theta-d)}
 - 
f_{max}=\dfrac{\sin^{2}(a-b)}{\left\{\sqrt{\sin(d-b)\sin(a-c)}+\sqrt{\sin(d-a)\sin(b-c)}\right\}^{2}}
 - \theta_{max} \left.\dfrac{df(\theta)}{d\theta}\right|_{\theta=\theta_{max}}=0 - 
\begin{aligned}
\sin(2\theta_{max}-a-b)\cos(d-c)-\sin(2\theta_{max}-c-d)\cos(a-b)=\sin(c+d-a-b)
\end{aligned}
 - 
\sin(2\theta_{max}-a-b-c-d)\left[\cos(d+c)\cos(d-c)-\cos(a+b)\cos(a-b)\right]+\cos(2\theta_{max}-a-b-c-d)\left[\sin(d+c)\cos(d-c)-\sin(a+b)\cos(a-b)\right]=\sin(c+d-a-b)
 2\theta_{max}-a-b-c-d f(\theta)","['real-analysis', 'calculus', 'trigonometry']"
31,How to prove this lower bound on $\log(1+x)$ for $x \geq 0$? How about $x > -1$?,How to prove this lower bound on  for ? How about ?,\log(1+x) x \geq 0 x > -1,"I want to show that for all real numbers $x \geq 0$ : $$ \log(1+x) \geq \frac{x(5x+6)}{2(x+3)(x+1)}. $$ I'd like to understand each step necessary to prove this so that I can apply it to future problems. Amazingly, a popular online calculator immediately spits out that this inequality is true for all $x > -1$ . Is there an easy way to prove this for $x \geq 0$ ? How about $x >-1$ ? Attempt (and more info on how I'm stuck) : My guess is to use the Taylor series expansion of both sides at $x=0$ (but why is that sufficient for all $x \geq 0$ ?). Expanding both sides: $$ \log(1+x) = x - \frac{x^2}{2} + \frac{x^3}{3} - \frac{x^4}{4} + \ldots \geq x - \frac{x^2}{2} + \frac{x^3}{3} - \frac{5x^4}{18} + \frac{7x^5}{27} - \ldots = \frac{x(5x+6)}{2(x+3)(x+1)}. $$ I see that the early order terms match and then they don't. Not sure how to proceed from there. The signs are alternating on both sides, which I'm not sure how to deal with. Though, it seems the coefficients are converging to $0$ on both sides. It also seems the magnitude of the coefficients on the right-hand side is larger than the left-hand side (if you go term-wise), but how do I prove that? And do I need that fact to prove the inequality?","I want to show that for all real numbers : I'd like to understand each step necessary to prove this so that I can apply it to future problems. Amazingly, a popular online calculator immediately spits out that this inequality is true for all . Is there an easy way to prove this for ? How about ? Attempt (and more info on how I'm stuck) : My guess is to use the Taylor series expansion of both sides at (but why is that sufficient for all ?). Expanding both sides: I see that the early order terms match and then they don't. Not sure how to proceed from there. The signs are alternating on both sides, which I'm not sure how to deal with. Though, it seems the coefficients are converging to on both sides. It also seems the magnitude of the coefficients on the right-hand side is larger than the left-hand side (if you go term-wise), but how do I prove that? And do I need that fact to prove the inequality?","x \geq 0 
\log(1+x) \geq \frac{x(5x+6)}{2(x+3)(x+1)}.
 x > -1 x \geq 0 x >-1 x=0 x \geq 0 
\log(1+x) = x - \frac{x^2}{2} + \frac{x^3}{3} - \frac{x^4}{4} + \ldots \geq x - \frac{x^2}{2} + \frac{x^3}{3} - \frac{5x^4}{18} + \frac{7x^5}{27} - \ldots = \frac{x(5x+6)}{2(x+3)(x+1)}.
 0","['real-analysis', 'calculus', 'inequality', 'logarithms', 'taylor-expansion']"
32,General term of a sequence and its limit.,General term of a sequence and its limit.,,"Having a sequence $\;\sqrt[3]5\;,\;\sqrt[3]{5\sqrt[3]5}\;,\;\sqrt[3]{5\sqrt[3]{5\sqrt[3]5}}\;,\;\ldots\;,\;$ what could be the general term and its limit ? Note that the second term is cube root of $5$ and inside that there is cube root of $5$ again and the third term is cube root of $5$ and two cube root of $5$ inside the outer cube root of $5$ . I tried to define it recursively as $c_{n+1}=\sqrt[3]{5c_n}\;$ where $\;c_1=\sqrt[3]5\;.$",Having a sequence what could be the general term and its limit ? Note that the second term is cube root of and inside that there is cube root of again and the third term is cube root of and two cube root of inside the outer cube root of . I tried to define it recursively as where,"\;\sqrt[3]5\;,\;\sqrt[3]{5\sqrt[3]5}\;,\;\sqrt[3]{5\sqrt[3]{5\sqrt[3]5}}\;,\;\ldots\;,\; 5 5 5 5 5 c_{n+1}=\sqrt[3]{5c_n}\; \;c_1=\sqrt[3]5\;.","['real-analysis', 'sequences-and-series', 'radicals']"
33,"A differentiable function whose derivative is $0$ almost everywhere, but not everywhere","A differentiable function whose derivative is  almost everywhere, but not everywhere",0,"In another thread, I remarked that if $$\int_0^1 \vert f'(x)\vert\mathrm dx=0,$$ then $f'(x)=0$ for all $x\in(0,1)$ can only be concluded if $f'$ is continuous. This would certainly be correct if we were talking about a general integrable function $(0,1)\to\mathbb R$ . Take the characteristic function of a singleton set as a counterexample where the integral vanishes, but the function doesn't. But such a function is not a valid derivative of any function, since it doesn't have the mean value property. And I couldn't find any functions where the integral of the absolute value vanishes and which have the mean value property. That's why I'm wondering: Is there a differentiable function $f:(0,1)\to\mathbb R$ such that $f'$ is not identically $0$ , but $$\int_0^1\vert f'(x)\vert\mathrm dx=0,$$ or equivalently, $f'$ is $0$ almost everywhere, but not everywhere.","In another thread, I remarked that if then for all can only be concluded if is continuous. This would certainly be correct if we were talking about a general integrable function . Take the characteristic function of a singleton set as a counterexample where the integral vanishes, but the function doesn't. But such a function is not a valid derivative of any function, since it doesn't have the mean value property. And I couldn't find any functions where the integral of the absolute value vanishes and which have the mean value property. That's why I'm wondering: Is there a differentiable function such that is not identically , but or equivalently, is almost everywhere, but not everywhere.","\int_0^1 \vert f'(x)\vert\mathrm dx=0, f'(x)=0 x\in(0,1) f' (0,1)\to\mathbb R f:(0,1)\to\mathbb R f' 0 \int_0^1\vert f'(x)\vert\mathrm dx=0, f' 0","['real-analysis', 'measure-theory']"
34,"Let $f$ be a real function and $a<b<c<d$. If $f$ is convex on $[a,c]$ and $[b,d]$, then can we say $f$ is convex on $[a,d]$?","Let  be a real function and . If  is convex on  and , then can we say  is convex on ?","f a<b<c<d f [a,c] [b,d] f [a,d]","Let $f$ be a real function and $a<b<c<d$ . If $f$ is convex on $[a,c]$ and $[b,d]$ , then can we say $f$ is convex on $[a,d]$ ? Below is my attempt. If we can show that when $x_1\in[a,b]$ , $x_2\in [c,d]$ , then $$f(\theta x_1+(1-\theta)x_2)\le \theta f(x_1)+(1-\theta)f(x_2),\forall \theta\in(0,1)$$ the proof is done. Let $x_3:=\theta x_1+(1-\theta)x_2$ . It is equivalent to prove $$\frac{f(x_3)-f(x_1)}{x_3-x_1}\le \frac{f(x_2)-f(x_3)}{x_2-x_3}.$$ When $x_3\in (b,c)$ , we have $$\frac{f(x_3)-f(x_1)}{x_3-x_1}\le \frac{f(c)-f(x_3)}{c-x_3}\le \frac{f(x_2)-f(x_3)}{x_2-x_3}.$$ However, when $x_3\in [a,b]\cup [c,d]$ , the approach fails.","Let be a real function and . If is convex on and , then can we say is convex on ? Below is my attempt. If we can show that when , , then the proof is done. Let . It is equivalent to prove When , we have However, when , the approach fails.","f a<b<c<d f [a,c] [b,d] f [a,d] x_1\in[a,b] x_2\in [c,d] f(\theta x_1+(1-\theta)x_2)\le \theta f(x_1)+(1-\theta)f(x_2),\forall \theta\in(0,1) x_3:=\theta x_1+(1-\theta)x_2 \frac{f(x_3)-f(x_1)}{x_3-x_1}\le \frac{f(x_2)-f(x_3)}{x_2-x_3}. x_3\in (b,c) \frac{f(x_3)-f(x_1)}{x_3-x_1}\le \frac{f(c)-f(x_3)}{c-x_3}\le \frac{f(x_2)-f(x_3)}{x_2-x_3}. x_3\in [a,b]\cup [c,d]","['real-analysis', 'convex-analysis']"
35,"If there is a linear function $g$ which is at least as good of an approximation as any other linear $h$, then $f$ is differentiable at $x_0$.","If there is a linear function  which is at least as good of an approximation as any other linear , then  is differentiable at .",g h f x_0,"This question is related to one in this question where the author asks what is the intuition behind saying that derivative is the best linear approximation. One of the answers by user ""Milo Brandt"" is that we have two theorems, one of which is: $f$ is differentiable at $x_0$ if and only if there is a linear function $g$ which is at least as good of an approximation as any other linear $h$ . I am struggling to prove one part of this theorem. First, I think that $g$ and $h$ are supposed to be affine and not linear in a sense that $g(x) = A + B(x-x_0)$ and $h(x) = C + D(x-x_0)$ where $A,C \in \mathbb{R}^m$ and $B,D : \mathbb{R}^n \to \mathbb{R}^m$ are linear functions. Assume that there is such function $g$ which is at least as good as an approximation as any other $h$ . By definition, this means that there exists $\delta > 0$ such that for all $x$ that have $|x - x_0 | < \delta$ we have $|f(x) - g(x)| \leq |h(x) - f(x)|$ . I would like to show that $f$ is differentiable at $x_0$ , in other words, that there exists a linear function $\lambda : \mathbb{R}^n \to \mathbb{R}^m$ such that: $$ \lim \limits_{x \to x_0} \frac{|f(x) - f(x_0) - \lambda(x-x_0)|}{|x-x_0|} = 0 $$ This translates to be able to find such $\lambda$ so that for each $\varepsilon > 0$ we can find $\delta > 0$ such that when $|x - x_0| < \delta$ , we have $|f(x) - f(x_0) - \lambda(x-x_0)| < \varepsilon |x-x_0|$ . I think intuitively, I would like to show that $\lambda = B$ is correct choice. From $g$ being as good as approximation as any $h$ , I have the following: there is $\delta > 0 $ so that for all $|x - x_0| < \delta$ I have $|f(x) - g(x)| \leq |C - f(x) + D(x-x_0)|$ . Now, I can use triangle inequality and also result that I have already proven which is that any linear function $D$ is bounded in the following way: $|D(x-x_0)| < M|x-x_0|$ . In this case, I can show that I can always find $\delta > 0$ such that for all $|x-x_0| <\delta$ I have $|f(x) - g(x) | \leq |C-f(x)+D(x-x_0)| \leq |C - f(x)| + |D(x-x_0)| < |C - f(x)| + M|x-x_0|$ . As $h$ is arbitrary, I could choose $M = \varepsilon$ as I also know that $M = \sqrt{mn}$ $ \mathrm{max}_{ij}|D_{ij}|$ . But then I would only get that $|f(x) - g(x)| < |C - f(x)| + \varepsilon |x-x_0|$ . How to get rid of the second term? Should I use continuity? Do I somehow use that $g$ is linear now? Any help would be appreciated - thanks!","This question is related to one in this question where the author asks what is the intuition behind saying that derivative is the best linear approximation. One of the answers by user ""Milo Brandt"" is that we have two theorems, one of which is: is differentiable at if and only if there is a linear function which is at least as good of an approximation as any other linear . I am struggling to prove one part of this theorem. First, I think that and are supposed to be affine and not linear in a sense that and where and are linear functions. Assume that there is such function which is at least as good as an approximation as any other . By definition, this means that there exists such that for all that have we have . I would like to show that is differentiable at , in other words, that there exists a linear function such that: This translates to be able to find such so that for each we can find such that when , we have . I think intuitively, I would like to show that is correct choice. From being as good as approximation as any , I have the following: there is so that for all I have . Now, I can use triangle inequality and also result that I have already proven which is that any linear function is bounded in the following way: . In this case, I can show that I can always find such that for all I have . As is arbitrary, I could choose as I also know that . But then I would only get that . How to get rid of the second term? Should I use continuity? Do I somehow use that is linear now? Any help would be appreciated - thanks!","f x_0 g h g h g(x) = A + B(x-x_0) h(x) = C + D(x-x_0) A,C \in \mathbb{R}^m B,D : \mathbb{R}^n \to \mathbb{R}^m g h \delta > 0 x |x - x_0 | < \delta |f(x) - g(x)| \leq |h(x) - f(x)| f x_0 \lambda : \mathbb{R}^n \to \mathbb{R}^m  \lim \limits_{x \to x_0} \frac{|f(x) - f(x_0) - \lambda(x-x_0)|}{|x-x_0|} = 0  \lambda \varepsilon > 0 \delta > 0 |x - x_0| < \delta |f(x) - f(x_0) - \lambda(x-x_0)| < \varepsilon |x-x_0| \lambda = B g h \delta > 0  |x - x_0| < \delta |f(x) - g(x)| \leq |C - f(x) + D(x-x_0)| D |D(x-x_0)| < M|x-x_0| \delta > 0 |x-x_0| <\delta |f(x) - g(x) | \leq |C-f(x)+D(x-x_0)| \leq |C - f(x)| + |D(x-x_0)| < |C - f(x)| + M|x-x_0| h M = \varepsilon M = \sqrt{mn}  \mathrm{max}_{ij}|D_{ij}| |f(x) - g(x)| < |C - f(x)| + \varepsilon |x-x_0| g","['real-analysis', 'multivariable-calculus', 'derivatives']"
36,"Solve the integral $\int_1^3\!\sqrt{x-\sqrt{x+\sqrt{x-...}}}\,\mathrm{d}x$",Solve the integral,"\int_1^3\!\sqrt{x-\sqrt{x+\sqrt{x-...}}}\,\mathrm{d}x","As an extension to my discussion in one of the answers to my previous question on simplifying the integrand, I'd like to evaluate the following integral: $$\int_1^3\!\sqrt{x-\sqrt{x+\sqrt{x-...}}}\,\mathrm{d}x$$ The above radical, when solved, yields 4 possible solutions: $$1) y=\frac{1}{2}(-\sqrt{4x-3}-1)\\2)y=\frac{1}{2}(\sqrt{4x-3}-1)\\3)y=\frac{1}{2}(1-\sqrt{4x+1})\\4)y=\frac{1}{2}(\sqrt{4x+1}+1)$$ Definitely, only one of these solutions has to be considered as an integrand. Since the limits of integration are positive(and square roots are involved), I suspect that the integrand must be positive as a whole. Thus, solutions $(1$ ) and $(3)$ are ruled out. However, I cannot decide which expression amongst $(2)$ and $(4)$ is legitimate. It was brought to my attention that this involves the notion of convergence, a concept I'm not yet completely familiar with(I have a naive understanding of convergence in infinite geometric series). Thus, I'd like to know: Which of the above 4 solutions to the radical is legitimate for solving this integral, and why?","As an extension to my discussion in one of the answers to my previous question on simplifying the integrand, I'd like to evaluate the following integral: The above radical, when solved, yields 4 possible solutions: Definitely, only one of these solutions has to be considered as an integrand. Since the limits of integration are positive(and square roots are involved), I suspect that the integrand must be positive as a whole. Thus, solutions ) and are ruled out. However, I cannot decide which expression amongst and is legitimate. It was brought to my attention that this involves the notion of convergence, a concept I'm not yet completely familiar with(I have a naive understanding of convergence in infinite geometric series). Thus, I'd like to know: Which of the above 4 solutions to the radical is legitimate for solving this integral, and why?","\int_1^3\!\sqrt{x-\sqrt{x+\sqrt{x-...}}}\,\mathrm{d}x 1) y=\frac{1}{2}(-\sqrt{4x-3}-1)\\2)y=\frac{1}{2}(\sqrt{4x-3}-1)\\3)y=\frac{1}{2}(1-\sqrt{4x+1})\\4)y=\frac{1}{2}(\sqrt{4x+1}+1) (1 (3) (2) (4)","['real-analysis', 'calculus', 'integration']"
37,Show that $\sqrt{1+x}<1+\frac{x}{2}$ for all $x>0$,Show that  for all,\sqrt{1+x}<1+\frac{x}{2} x>0,"I am a little stuck on this question and would appreciate some help. The question asks me to prove that $\sqrt{1+x}<1+\frac{x}{2}$ for all $x>0$ . I squared both sides of the question to get $1+x<\frac{x^2}{4}+x+1$ for all $x>0$ . Then, I multiplied both sides by $4$ to get $4+4x<x^2+4x+4$ for all $x>0$ . I am a little stuck and was wondering what to do after this step and how to actually provide sufficient proof to say that this statement is true.","I am a little stuck on this question and would appreciate some help. The question asks me to prove that for all . I squared both sides of the question to get for all . Then, I multiplied both sides by to get for all . I am a little stuck and was wondering what to do after this step and how to actually provide sufficient proof to say that this statement is true.",\sqrt{1+x}<1+\frac{x}{2} x>0 1+x<\frac{x^2}{4}+x+1 x>0 4 4+4x<x^2+4x+4 x>0,['real-analysis']
38,A question about the definition of a strictly increasing function,A question about the definition of a strictly increasing function,,"So a definition of a strictly increasing function is $~x_1 < x_2 \implies f(x_1) < f(x_2)$ . Can this be extended to be a two-way implication, namely, $~x_1 < x_2 \iff f(x_1) < f(x_2)$ ? Thanks!","So a definition of a strictly increasing function is . Can this be extended to be a two-way implication, namely, ? Thanks!",~x_1 < x_2 \implies f(x_1) < f(x_2) ~x_1 < x_2 \iff f(x_1) < f(x_2),"['real-analysis', 'definition', 'monotone-functions']"
39,Limit of an integral in the form of $\lim_{x\to 0} g(x) \int_{0}^{x} f(t) dt$,Limit of an integral in the form of,\lim_{x\to 0} g(x) \int_{0}^{x} f(t) dt,"while preparing my next exam I found this exercise in the exam of two years ago: $$\lim_{x\to 0} \frac{\sinh(x)}{\cos(x)-1}  \int_{0}^{x} \sqrt{e^t-t^4} dt$$ I first thought to use de L'Hopital rule, but it didn't feel right so I tried another way. I decided to try to expand the function inside the integral using McLaurin series. So the function became: $$\lim_{x\to 0} \frac{\sinh(x)}{\cos(x)-1}  \int_{0}^{x} (1+\frac t 2 + \frac {t^2} 4+ \frac {t^3} {12} + \frac {25t^4}{48}+\mathcal{o}(t^4)) dt$$ After expanding $\frac{\sinh(x)}{\cos(x)-1}$ and integrating and some other algebric steps, it came down to $$\frac{x^2} {\frac{-x^2} 2}$$ the result was -2. My problem is that I'm not sure I could actually do everything I did. I someone could explain to me whether I'm right or not, and maybe also explain to me how to approach this type of exercises I would be extremely thankful. I would like to apologize already for the spelling mistakes I made for sure, but I'm not a native English speaker.","while preparing my next exam I found this exercise in the exam of two years ago: I first thought to use de L'Hopital rule, but it didn't feel right so I tried another way. I decided to try to expand the function inside the integral using McLaurin series. So the function became: After expanding and integrating and some other algebric steps, it came down to the result was -2. My problem is that I'm not sure I could actually do everything I did. I someone could explain to me whether I'm right or not, and maybe also explain to me how to approach this type of exercises I would be extremely thankful. I would like to apologize already for the spelling mistakes I made for sure, but I'm not a native English speaker.",\lim_{x\to 0} \frac{\sinh(x)}{\cos(x)-1}  \int_{0}^{x} \sqrt{e^t-t^4} dt \lim_{x\to 0} \frac{\sinh(x)}{\cos(x)-1}  \int_{0}^{x} (1+\frac t 2 + \frac {t^2} 4+ \frac {t^3} {12} + \frac {25t^4}{48}+\mathcal{o}(t^4)) dt \frac{\sinh(x)}{\cos(x)-1} \frac{x^2} {\frac{-x^2} 2},"['real-analysis', 'calculus']"
40,Integrability of the derivative of an increasing function- MIT primes 2020 A2,Integrability of the derivative of an increasing function- MIT primes 2020 A2,,"Problem A2 from the MIT PRIMES Problem Set 2020 goes as following (discussion is already open): Let $f: [0,1]\to \mathbb{R}$ be a strictly increasing function which is differentiable in $(0,1)$ . Suppose that $f(0)=0$ and for every $x\in(0,1)$ we have $$\frac{f'(x)}{x}\ge f(x)^2+1.$$ How small can $f(1)$ be? I thought the solution idea was pretty straightforward; it follows pretty quickly from integrating the rearranged inequality $\frac{f'(x)}{f(x)^2+1}\ge x$ . However, it is somewhat difficult to prove the left side is integrable. We can show that $\arctan(f(x))$ is increasing, and the derivative of an increasing function satisfies using the Lebesgue integral $\int_a^bf'(x)dx\le f(b)-f(a)$ . I was wondering if it is necessarily true that the left hand side is Riemann integrable, and if not, what is a counterexample? If it were Riemann integrable, we could have $\int_a^bf'(x)dx= f(b)-f(a)$ , which is slightly stronger.","Problem A2 from the MIT PRIMES Problem Set 2020 goes as following (discussion is already open): Let be a strictly increasing function which is differentiable in . Suppose that and for every we have How small can be? I thought the solution idea was pretty straightforward; it follows pretty quickly from integrating the rearranged inequality . However, it is somewhat difficult to prove the left side is integrable. We can show that is increasing, and the derivative of an increasing function satisfies using the Lebesgue integral . I was wondering if it is necessarily true that the left hand side is Riemann integrable, and if not, what is a counterexample? If it were Riemann integrable, we could have , which is slightly stronger.","f: [0,1]\to \mathbb{R} (0,1) f(0)=0 x\in(0,1) \frac{f'(x)}{x}\ge f(x)^2+1. f(1) \frac{f'(x)}{f(x)^2+1}\ge x \arctan(f(x)) \int_a^bf'(x)dx\le f(b)-f(a) \int_a^bf'(x)dx= f(b)-f(a)","['real-analysis', 'calculus', 'integration', 'riemann-integration']"
41,Sum of the series $\sum_{n=0}^{\infty} \lfloor nr \rfloor x^n$ where $r$ is rational?,Sum of the series  where  is rational?,\sum_{n=0}^{\infty} \lfloor nr \rfloor x^n r,Can we find the exact sum of the series $\sum_{n=0}^{\infty} \lfloor nr \rfloor x^n$ where $r$ is rational? There is a special case given here but I don't know how to prove it and can we get the sum for the general case?,Can we find the exact sum of the series where is rational? There is a special case given here but I don't know how to prove it and can we get the sum for the general case?,\sum_{n=0}^{\infty} \lfloor nr \rfloor x^n r,"['real-analysis', 'analysis', 'functions', 'rational-numbers']"
42,How to show CES is not an algebra,How to show CES is not an algebra,,"We say that a subset $V$ of $\{1,2,3,\dots\}$ has Cesaro density $\gamma(V)$ and denote $V\in CES$ if the limit $$\gamma(V):=\lim_{n\to \infty} \frac{\mid V\cap \{1,2,3,\dots, n\}\mid}{n}$$ exists. Give an example of sets $V_1\in CES$ and $V_2\in CES$ for which $V_1\cap V_2\notin CES$ . How to find such an example? If I try to show CES is not algebra.",We say that a subset of has Cesaro density and denote if the limit exists. Give an example of sets and for which . How to find such an example? If I try to show CES is not algebra.,"V \{1,2,3,\dots\} \gamma(V) V\in CES \gamma(V):=\lim_{n\to \infty} \frac{\mid V\cap \{1,2,3,\dots, n\}\mid}{n} V_1\in CES V_2\in CES V_1\cap V_2\notin CES","['real-analysis', 'measure-theory']"
43,Is there a polynomial which detects when the two smallest roots of a given real polynomial are equal?,Is there a polynomial which detects when the two smallest roots of a given real polynomial are equal?,,"The discriminant of a polynomial over a field is a ""universal""* polynomial function of its coefficients , which is zero if and only if the polynomial has a multiple root in some field extension. Now, let's limit the discussion to polynomials $p(x) \in \mathbb{R}[X]$ with real coefficients, with all their roots real and non-negative. Is there a ""universal""* polynomial in the coefficients of such $p(x)$ , which is zero if and only if the two smallest roots of $p(x)$ are equal? (equivalently, the smallest root of $p$ has multiplicity greater than $1$ ). If not, is there such a universal real-analytic function of the coefficients? *By ""universal"", I mean that the coefficients of the discriminant are independent of $p$ .","The discriminant of a polynomial over a field is a ""universal""* polynomial function of its coefficients , which is zero if and only if the polynomial has a multiple root in some field extension. Now, let's limit the discussion to polynomials with real coefficients, with all their roots real and non-negative. Is there a ""universal""* polynomial in the coefficients of such , which is zero if and only if the two smallest roots of are equal? (equivalently, the smallest root of has multiplicity greater than ). If not, is there such a universal real-analytic function of the coefficients? *By ""universal"", I mean that the coefficients of the discriminant are independent of .",p(x) \in \mathbb{R}[X] p(x) p(x) p 1 p,"['real-analysis', 'algebraic-geometry', 'polynomials', 'field-theory', 'real-algebraic-geometry']"
44,Characteristic functions agree in a neighborhood of zero,Characteristic functions agree in a neighborhood of zero,,"Suppose I have two random variables $X,Y$ with finite mean . Let's say that their characteristic functions agree in a small neighborhood of zero (this means that there is some $\epsilon>0$ such that $\Bbb E[e^{itX}] = \Bbb E[e^{itY}]$ for all $|t|<\epsilon$ ). Can I conclude that $X \stackrel{d}{=} Y$ ? Remarks: If I remove the condition of finite mean then certainly the answer is no. This is a trivial consequence of Polya's criterion (see Theorem 3.3.22 in Durrett's book). On the other hand, if I instead impose the stronger moment condition that $\Bbb E[e^{\lambda|X|}]< \infty$ for some small $\lambda >0$ , then $X$ and $Y$ certainly must have the same distribution (because then the characteristic functions extend to analytic functions on some domain of $\Bbb C$ which contains the entire real line). Hence the real underlying question is: what is the minimal number of moments needed by $X,Y$ so that $X \stackrel{d}{=} Y$ under the given assumptions on characteristic functions? I suspect that the mgf condition is the minimal one (and tried to construct a counterexample using the lognormal distribution), but I could not prove it. If that's wrong then my next guess is that two moments or one moment would suffice, hence the original question.","Suppose I have two random variables with finite mean . Let's say that their characteristic functions agree in a small neighborhood of zero (this means that there is some such that for all ). Can I conclude that ? Remarks: If I remove the condition of finite mean then certainly the answer is no. This is a trivial consequence of Polya's criterion (see Theorem 3.3.22 in Durrett's book). On the other hand, if I instead impose the stronger moment condition that for some small , then and certainly must have the same distribution (because then the characteristic functions extend to analytic functions on some domain of which contains the entire real line). Hence the real underlying question is: what is the minimal number of moments needed by so that under the given assumptions on characteristic functions? I suspect that the mgf condition is the minimal one (and tried to construct a counterexample using the lognormal distribution), but I could not prove it. If that's wrong then my next guess is that two moments or one moment would suffice, hence the original question.","X,Y \epsilon>0 \Bbb E[e^{itX}] = \Bbb E[e^{itY}] |t|<\epsilon X \stackrel{d}{=} Y \Bbb E[e^{\lambda|X|}]< \infty \lambda >0 X Y \Bbb C X,Y X \stackrel{d}{=} Y","['real-analysis', 'probability-theory', 'probability-distributions', 'fourier-analysis', 'characteristic-functions']"
45,Is the space of maps which satisfy this vanishing condition finite-dimensional?,Is the space of maps which satisfy this vanishing condition finite-dimensional?,,"Let $\mathbb{D}^n \subseteq \mathbb{R}^n$ be the closed $n$ -dimensional unit ball. Let $h:\mathbb{D}^n \to \mathbb{R}^{k}$ be smooth, and suppose that $h(x) \neq 0$ a.e. on $\mathbb{D}^n$ . Set $$V_h=\{  \,\,f \in C^{\infty}(\mathbb{D}^n;\mathbb{R}^{k}) \, \,\,| \, \,  (df_x)^T\big(h(x)\big)=0 \, \text{ for every }\, x \in \mathbb{D}^n \, \} $$ $V_h$ is a real vector-space. Is it always finite-dimensional? Can it be infinite-dimensional for some $h$ ? Edit: Pozz showed nicely that when $k=1$ , $V_h$ always coincides with the space of constant functions, and that for $k>1$ , $V_h$ might be infinite-dimensional (e.g. if $h$ is a constant function). Is there ever a case where $V_h$ is finite-dimensional when $k>1$ ? I suspect that the answer is negative, but I don't know how to prove this.","Let be the closed -dimensional unit ball. Let be smooth, and suppose that a.e. on . Set is a real vector-space. Is it always finite-dimensional? Can it be infinite-dimensional for some ? Edit: Pozz showed nicely that when , always coincides with the space of constant functions, and that for , might be infinite-dimensional (e.g. if is a constant function). Is there ever a case where is finite-dimensional when ? I suspect that the answer is negative, but I don't know how to prove this.","\mathbb{D}^n \subseteq \mathbb{R}^n n h:\mathbb{D}^n \to \mathbb{R}^{k} h(x) \neq 0 \mathbb{D}^n V_h=\{  \,\,f \in C^{\infty}(\mathbb{D}^n;\mathbb{R}^{k}) \, \,\,| \, \,  (df_x)^T\big(h(x)\big)=0 \, \text{ for every }\, x \in \mathbb{D}^n \, \}  V_h h k=1 V_h k>1 V_h h V_h k>1","['real-analysis', 'linear-algebra', 'multivariable-calculus', 'transpose', 'smooth-functions']"
46,Continuous function between a lower semi-continuous function and an upper semi-continuous function.,Continuous function between a lower semi-continuous function and an upper semi-continuous function.,,"Let $X$ be a compact metric space, $u: X \to [0, 1]$ an upper semi-continuous function and $l: X \to [0, 1]$ a lower semi-continuous function such that $u(x) < l(x)$ for each $x \in X$ . Does there exist a continuous function $f: X \to [0, 1]$ such that $u(x) < f(x) < l(x)$ for each $x \in X$ ?","Let be a compact metric space, an upper semi-continuous function and a lower semi-continuous function such that for each . Does there exist a continuous function such that for each ?","X u: X \to [0, 1] l: X \to [0, 1] u(x) < l(x) x \in X f: X \to [0, 1] u(x) < f(x) < l(x) x \in X","['real-analysis', 'continuity', 'metric-spaces', 'semicontinuous-functions']"
47,How to find $\sum_{n=1}^{\infty}\frac{H_nH_{2n}}{n^2}$ using real analysis and in an elegant way?,How to find  using real analysis and in an elegant way?,\sum_{n=1}^{\infty}\frac{H_nH_{2n}}{n^2},I have already evaluated this sum: \begin{equation*} \sum_{n=1}^{\infty}\frac{H_nH_{2n}}{n^2}=4\operatorname{Li_4}\left( \frac12\right)+\frac{13}{8}\zeta(4)+\frac72\ln2\zeta(3)-\ln^22\zeta(2)+\frac16\ln^42  \end{equation*} using the identity $\displaystyle\frac{1}{1-x^2}\ln\left(\frac{1-x}{1+x}\right)=\sum_{n=1}^{\infty}\left(H_n-2H_{2n}\right)x^{2n-1}$ but kind of lengthy. any other approaches?,I have already evaluated this sum: using the identity but kind of lengthy. any other approaches?,"\begin{equation*}
\sum_{n=1}^{\infty}\frac{H_nH_{2n}}{n^2}=4\operatorname{Li_4}\left( \frac12\right)+\frac{13}{8}\zeta(4)+\frac72\ln2\zeta(3)-\ln^22\zeta(2)+\frac16\ln^42 
\end{equation*} \displaystyle\frac{1}{1-x^2}\ln\left(\frac{1-x}{1+x}\right)=\sum_{n=1}^{\infty}\left(H_n-2H_{2n}\right)x^{2n-1}","['real-analysis', 'alternative-proof', 'harmonic-numbers', 'polylogarithm', 'euler-sums']"
48,"Weak solutions to $\Delta u=f$ are in $W^{2,2}$",Weak solutions to  are in,"\Delta u=f W^{2,2}","I  believe  the following statement is true. Let $\Omega$ be a smoothly , bounded domain in $\mathbb{R}^{n}$ . The statement: Let $u\in H^{1}(\Omega)$ so that there exists $f\in L^{2}(\Omega)  \;s.t.\int_{\Omega}\nabla u\nabla \varphi=\int_{\Omega} f\varphi, \forall \varphi\in H^{1}(\Omega)$ . Then $u\in H^{2}(\Omega)$ . I have searched in the book by Evans and Brezis but not so certain. Could anyone provide a reference for that? It can be seen in Evans's book that $u\in H^{2}_{loc}(\Omega)$ . Thanks so much.","I  believe  the following statement is true. Let be a smoothly , bounded domain in . The statement: Let so that there exists . Then . I have searched in the book by Evans and Brezis but not so certain. Could anyone provide a reference for that? It can be seen in Evans's book that . Thanks so much.","\Omega \mathbb{R}^{n} u\in H^{1}(\Omega) f\in L^{2}(\Omega)  \;s.t.\int_{\Omega}\nabla u\nabla \varphi=\int_{\Omega} f\varphi, \forall \varphi\in H^{1}(\Omega) u\in H^{2}(\Omega) u\in H^{2}_{loc}(\Omega)","['real-analysis', 'functional-analysis', 'sobolev-spaces', 'regularity-theory-of-pdes']"
49,For which $a>0$ series is convergent?,For which  series is convergent?,a>0,"For which $a>0$ series $$\sum { \left(2-2 \cos\frac{1}{n} -\frac{1}{n}\cdot \sin\left( \sin\frac{1}{n}  \right) \right)^a } $$ $(n \in \mathbb N)$ is convergent? My try: From Taylor theorem I know that: $$a_{n}={ \left(2-2 \cos\frac{1}{n} -\frac{1}{n}\cdot \sin\left( \sin\frac{1}{n}  \right) \right)^a } = (\frac{1}{n^{4}}-\frac{7}{72n^{6}}+o(\frac{1}{n^{8}}))^{a}$$ Then I have: $$(\frac{1}{n^{4}}-\frac{7}{72n^{6}}+o(\frac{1}{n^{8}}))^{a} \le (\frac{1}{n^{4}}+o(\frac{1}{n^{8}}))^{a}$$ At this point, my problem is that if I had: $$(\frac{1}{n^{4}}-\frac{7}{72n^{6}}+o(\frac{1}{n^{8}}))^{a} \le (\frac{1}{n^{4}})^{a}$$ I could say that $0 \le a_{n} \le (\frac{1}{n^{4}})^{a}$ so for $a>\frac{1}{4}$ this series is convergent. However in this task I have also $o(\frac{1}{n^{8}}))^{a}$ and I don't know what I can do with it to finish my sollution. Can you help me?","For which series is convergent? My try: From Taylor theorem I know that: Then I have: At this point, my problem is that if I had: I could say that so for this series is convergent. However in this task I have also and I don't know what I can do with it to finish my sollution. Can you help me?",a>0 \sum { \left(2-2 \cos\frac{1}{n} -\frac{1}{n}\cdot \sin\left( \sin\frac{1}{n}  \right) \right)^a }  (n \in \mathbb N) a_{n}={ \left(2-2 \cos\frac{1}{n} -\frac{1}{n}\cdot \sin\left( \sin\frac{1}{n}  \right) \right)^a } = (\frac{1}{n^{4}}-\frac{7}{72n^{6}}+o(\frac{1}{n^{8}}))^{a} (\frac{1}{n^{4}}-\frac{7}{72n^{6}}+o(\frac{1}{n^{8}}))^{a} \le (\frac{1}{n^{4}}+o(\frac{1}{n^{8}}))^{a} (\frac{1}{n^{4}}-\frac{7}{72n^{6}}+o(\frac{1}{n^{8}}))^{a} \le (\frac{1}{n^{4}})^{a} 0 \le a_{n} \le (\frac{1}{n^{4}})^{a} a>\frac{1}{4} o(\frac{1}{n^{8}}))^{a},['real-analysis']
50,Diagonal argument applied to computable numbers [duplicate],Diagonal argument applied to computable numbers [duplicate],,"This question already has an answer here : A paradox related to computable reals? (1 answer) Closed 4 years ago . Upon applying the Cantor diagonal argument to the enumerated list of all computable numbers, we produce a number not in it, but seems to be computable too, and that seems paradoxical. For clarity, let me state the argument formally. It suffices to consider the interval [0,1] only. Consider $0 \leq a \leq 1$ , and let it's decimal representation be found as $$ a =\sum_{n=1}^\infty 10^{-n} a^{(n)} $$ We say that $a$ is computable, if there is a Turing machine $T_a$ which receives $n$ and outputs $a^{(n)}$ in finite time, i.e., it always halts. Let $A$ be the set of all computable numbers. A moment's reflection shows that $A$ is countably many. For example, we can implement $T_a$ in (say) Python, always choosing the program with the shortest string length, and we sort $T_a$ by their program according to the ASCII characters' dictionary order. If two programs, which both implement $T_a$ , have the same string length, choose the one with foremost dictionary order. This gives a sorting function of $A$ . Let $A$ be enumerated with $a_1, a_2, a_3 \dotsc$ . Now, construct a number $b$ so that: $$ b =\sum_{n=1}^\infty 10^{-n} b^{(n)} \\ b^{(n)} =\begin{cases} 4,\quad a_n^{(n)} =3 \\ 3,\quad \rm{otherwise} \end{cases} $$ We see $b \notin A$ . Moreover, $b^{(n)}$ is obtained within finite time, since we only need to know $a_n^{(n)}$ 's value. Thus $b \in A$ , but $b \neq a_n$ for all $n =1,2,3, \dotsc$ , a contradiction. This post is related, but I don't think it's a duplicate. The post applies the diagonal argument on the reals, rather than the computable numbers.","This question already has an answer here : A paradox related to computable reals? (1 answer) Closed 4 years ago . Upon applying the Cantor diagonal argument to the enumerated list of all computable numbers, we produce a number not in it, but seems to be computable too, and that seems paradoxical. For clarity, let me state the argument formally. It suffices to consider the interval [0,1] only. Consider , and let it's decimal representation be found as We say that is computable, if there is a Turing machine which receives and outputs in finite time, i.e., it always halts. Let be the set of all computable numbers. A moment's reflection shows that is countably many. For example, we can implement in (say) Python, always choosing the program with the shortest string length, and we sort by their program according to the ASCII characters' dictionary order. If two programs, which both implement , have the same string length, choose the one with foremost dictionary order. This gives a sorting function of . Let be enumerated with . Now, construct a number so that: We see . Moreover, is obtained within finite time, since we only need to know 's value. Thus , but for all , a contradiction. This post is related, but I don't think it's a duplicate. The post applies the diagonal argument on the reals, rather than the computable numbers.","0 \leq a \leq 1 
a =\sum_{n=1}^\infty 10^{-n} a^{(n)}
 a T_a n a^{(n)} A A T_a T_a T_a A A a_1, a_2, a_3 \dotsc b 
b =\sum_{n=1}^\infty 10^{-n} b^{(n)} \\
b^{(n)} =\begin{cases}
4,\quad a_n^{(n)} =3 \\
3,\quad \rm{otherwise}
\end{cases}
 b \notin A b^{(n)} a_n^{(n)} b \in A b \neq a_n n =1,2,3, \dotsc","['real-analysis', 'cardinals', 'computability', 'turing-machines']"
51,union of closed balls centered around points of a compact set,union of closed balls centered around points of a compact set,,"Let $S$ be a compact subset of $(\mathbb{R}^n, d_2)$ where $d_2$ is the Euclidean metric. For all $r > 0$ , we write $S_r = \bigcup\limits_{s \in S} \overline{B}_r(s)$ . I want to show that $S_r$ is a closed set but I am a little stuck. I know that a finite union of closed sets is closed, but in this case is it necessarily true that $S$ is finite?","Let be a compact subset of where is the Euclidean metric. For all , we write . I want to show that is a closed set but I am a little stuck. I know that a finite union of closed sets is closed, but in this case is it necessarily true that is finite?","S (\mathbb{R}^n, d_2) d_2 r > 0 S_r = \bigcup\limits_{s \in S} \overline{B}_r(s) S_r S","['real-analysis', 'general-topology']"
52,$f(x)/x \to l$ and $f''(x) = O(1/x)$,and,f(x)/x \to l f''(x) = O(1/x),"$f \in C^2(\mathbb{R}, \mathbb{R})$ such that $f(x)/x \to l \in \mathbb{R}$ as $x \to + \infty$ , and such that $f''(x) = O(1/x)$ at $+\infty$ . Find : $$\lim_{x \to +\infty} f'(x)$$ Some thoughts : It seems to me that the limit is $l$ , but I am unable to prove it. Moreover it seems that we have $f''(x) = o(1/x)$ . I think I need somehow to relate the derivatives to each other, maybe using  the series expansion of $f$ , but it doesn't seem to work. Thank you!","such that as , and such that at . Find : Some thoughts : It seems to me that the limit is , but I am unable to prove it. Moreover it seems that we have . I think I need somehow to relate the derivatives to each other, maybe using  the series expansion of , but it doesn't seem to work. Thank you!","f \in C^2(\mathbb{R}, \mathbb{R}) f(x)/x \to l \in \mathbb{R} x \to + \infty f''(x) = O(1/x) +\infty \lim_{x \to +\infty} f'(x) l f''(x) = o(1/x) f","['real-analysis', 'integration', 'sequences-and-series', 'limits']"
53,What does the symbol ∟ mean?,What does the symbol ∟ mean?,,"I encountered this symbol $\lefthalfcup$ ∟ in the statement of Besicovitch derivation theorem. It says that the Radon-Nykodym decomposition of the given measure $\nu$ is $\nu=f\mu+\nu^s$ , $\nu^s$ is singular with respect to $\mu$ and $$ \nu^s∟ E $$ where $E$ is a specific set. I don't understand what that symbol means. It also appears in Federer-Vol'pert theorem and it may have something to do with distributional derivatives as seen here , but I really can't figure it out. Any help is appreciated, thank you. Screenshot of page 24 of The Comprehensive $\LaTeX$ symbol list :","I encountered this symbol ∟ in the statement of Besicovitch derivation theorem. It says that the Radon-Nykodym decomposition of the given measure is , is singular with respect to and where is a specific set. I don't understand what that symbol means. It also appears in Federer-Vol'pert theorem and it may have something to do with distributional derivatives as seen here , but I really can't figure it out. Any help is appreciated, thank you. Screenshot of page 24 of The Comprehensive symbol list :","\lefthalfcup \nu \nu=f\mu+\nu^s \nu^s \mu 
\nu^s∟ E
 E \LaTeX","['real-analysis', 'measure-theory', 'notation', 'distribution-theory', 'radon-nikodym']"
54,A variant of Kronecker's approximation theorem?,A variant of Kronecker's approximation theorem?,,"Let $\tau,\sigma\in(0,\infty)$ with $\frac{\tau}{\sigma}\notin\mathbb Q$ . By Kronecker's approximation theorem, we know: (1) For each $x\in \mathbb R$ and $\epsilon>0$ , there are $m,n\in\mathbb N$ such that $|x+n\tau-m\sigma|<\epsilon$ . In other words, if you keep adding $\tau$ to $x$ , you will eventually come arbitratily close to the set $\sigma\mathbb N$ . But what happens if you keep adding values that are just approximately $\tau$ ? To make this a precise question, let $(\tau_n)_{n\in\mathbb N_0}\subset (0,\infty)$ with $$ \tau_{n+1}-\tau_n \xrightarrow{n\to\infty}\tau.$$ Then, the according conjecture is: (2) For each $x\in \mathbb R$ and $\epsilon>0$ , there are $m,n\in\mathbb N$ such that $|x+\tau_n-m\sigma|<\epsilon$ . If one assumes that $$ \sum_{n=0}^\infty \left((\tau_{n+1}-\tau_n)-\tau \right) \text{ converges in $\mathbb R$,}$$ it is indeed relatively easy to deduce (2) from (1). QUESTION: If $\sum_{n=0}^\infty \left((\tau_{n+1}-\tau_n)-\tau \right)$ diverges, does (2) still hold? My ad hoc ideas didn't quite work out and before I start to think deeper about it, I thought I might ask if anyone here knows of any result in this direction. Thanks a lot in advance!","Let with . By Kronecker's approximation theorem, we know: (1) For each and , there are such that . In other words, if you keep adding to , you will eventually come arbitratily close to the set . But what happens if you keep adding values that are just approximately ? To make this a precise question, let with Then, the according conjecture is: (2) For each and , there are such that . If one assumes that it is indeed relatively easy to deduce (2) from (1). QUESTION: If diverges, does (2) still hold? My ad hoc ideas didn't quite work out and before I start to think deeper about it, I thought I might ask if anyone here knows of any result in this direction. Thanks a lot in advance!","\tau,\sigma\in(0,\infty) \frac{\tau}{\sigma}\notin\mathbb Q x\in \mathbb R \epsilon>0 m,n\in\mathbb N |x+n\tau-m\sigma|<\epsilon \tau x \sigma\mathbb N \tau (\tau_n)_{n\in\mathbb N_0}\subset (0,\infty)  \tau_{n+1}-\tau_n \xrightarrow{n\to\infty}\tau. x\in \mathbb R \epsilon>0 m,n\in\mathbb N |x+\tau_n-m\sigma|<\epsilon  \sum_{n=0}^\infty \left((\tau_{n+1}-\tau_n)-\tau \right) \text{ converges in \mathbb R,} \sum_{n=0}^\infty \left((\tau_{n+1}-\tau_n)-\tau \right)","['real-analysis', 'sequences-and-series', 'approximation', 'real-numbers', 'diophantine-approximation']"
55,$f_n$ converge uniformly to $f$ then $\mathrm{d}f_n(x_n)$ converges to $\mathrm{d}f(x)$,converge uniformly to  then  converges to,f_n f \mathrm{d}f_n(x_n) \mathrm{d}f(x),"Let $f_n : \mathbb{R}^p \to \mathbb{R}$ such that the $f_n$ are $C^1$ and such that the sequence $(f_n)_{n \in \mathbb{N}}$ converges uniformly to a function $f : \mathbb{R}^p \to \mathbb{R}$ which is $C^1$ . Then prove that for all $x \in \mathbb{R}^n$ there is a sequence $(x_n)_{n \in \mathbb{N}}$ which converge to $x$ such that $\mathrm{d}f_n(x_n)$ converges to $\mathrm{d}f(x)$ . I must say that I don't know at all how to do and don't have any intuition of what is really going on here. So we might look at the case qhere $p= 1$ . So we can write : $$f(a+h) = f(a)+ f'(a)h +o(h)$$ $$\forall n \in \mathbb{N}, f_n(a+h) = f_n(a) + f'_n(a)h +o(h)$$ Hence we have : $$\mid f'(a)h - f'_n(a)h \mid \leq \mid f(a+h)-f(a) \mid +\mid f(a)-f_n(a) \mid + \mid o(h) \mid$$ Since the function $f_n$ converge uniformly to $f$ , we have : $$\mid f'(a)h - f'_{\infty}(a) \mid \leq \mid o(h) \mid$$ And now using we let $h \to 0$ so that : $$\mid f'(a) -f_\infty'(a) \mid = 0 $$ I don't know if this works, but it feels strange to me since in the case the sequence $x_n$ is just the constant sequence... and moreover if this is correct I don't see at all how to generalise to higher dimensions. Thank you !","Let such that the are and such that the sequence converges uniformly to a function which is . Then prove that for all there is a sequence which converge to such that converges to . I must say that I don't know at all how to do and don't have any intuition of what is really going on here. So we might look at the case qhere . So we can write : Hence we have : Since the function converge uniformly to , we have : And now using we let so that : I don't know if this works, but it feels strange to me since in the case the sequence is just the constant sequence... and moreover if this is correct I don't see at all how to generalise to higher dimensions. Thank you !","f_n : \mathbb{R}^p \to \mathbb{R} f_n C^1 (f_n)_{n \in \mathbb{N}} f : \mathbb{R}^p \to \mathbb{R} C^1 x \in \mathbb{R}^n (x_n)_{n \in \mathbb{N}} x \mathrm{d}f_n(x_n) \mathrm{d}f(x) p= 1 f(a+h) = f(a)+ f'(a)h +o(h) \forall n \in \mathbb{N}, f_n(a+h) = f_n(a) + f'_n(a)h +o(h) \mid f'(a)h - f'_n(a)h \mid \leq \mid f(a+h)-f(a) \mid +\mid f(a)-f_n(a) \mid + \mid o(h) \mid f_n f \mid f'(a)h - f'_{\infty}(a) \mid \leq \mid o(h) \mid h \to 0 \mid f'(a) -f_\infty'(a) \mid = 0  x_n","['real-analysis', 'calculus', 'sequences-and-series', 'multivariable-calculus', 'uniform-convergence']"
56,$f+f'+f''\geq0$ implies that $f$ has a lower bound,implies that  has a lower bound,f+f'+f''\geq0 f,"Let $f\in C^2(a,b)$ such that $f+f'+f''\geq0$ . Prove that $f$ has a lower bound. $My\quad Attempt$ $1.\quad$ Suppose that $f$ has no lower bound at x=b, so there is a sequence $\{x_n\}$ which converges to b( $\lim_{n\to\infty}{x_n}=b$ ), and $f'(x_n)<0,f(x_n)<-n$ . $2.\quad\forall k\in \mathbb N,\exists N\in\mathbb N,n>N,x_n>x_k$ , then prove that $$\int_{\{x|x\in(x_k,x_n)\land f(x)>0\}}{}f^2(x)dx\leq C.$$ $3.\quad$ Prove: $$0\leq\int_{x_k}^{x_n}{(f+f'+f'')}dx\leq0+f(x_k)-f(x_n)+C\to-\infty$$ $\quad\quad\quad\quad\quad\quad\quad$ This contradicts the problem So that's my idea, but I can't do it from step 2. And my idea might be wrong. Edit in 2019/2/16 I solve the question if $q=0$ . There are some Chinese characters in my answer. I hope it doesn't bother you.","Let such that . Prove that has a lower bound. Suppose that has no lower bound at x=b, so there is a sequence which converges to b( ), and . , then prove that Prove: This contradicts the problem So that's my idea, but I can't do it from step 2. And my idea might be wrong. Edit in 2019/2/16 I solve the question if . There are some Chinese characters in my answer. I hope it doesn't bother you.","f\in C^2(a,b) f+f'+f''\geq0 f My\quad Attempt 1.\quad f \{x_n\} \lim_{n\to\infty}{x_n}=b f'(x_n)<0,f(x_n)<-n 2.\quad\forall k\in \mathbb N,\exists N\in\mathbb N,n>N,x_n>x_k \int_{\{x|x\in(x_k,x_n)\land f(x)>0\}}{}f^2(x)dx\leq C. 3.\quad 0\leq\int_{x_k}^{x_n}{(f+f'+f'')}dx\leq0+f(x_k)-f(x_n)+C\to-\infty \quad\quad\quad\quad\quad\quad\quad q=0","['real-analysis', 'contest-math']"
57,Taylor expansion of imaginary part?-Doable or not?,Taylor expansion of imaginary part?-Doable or not?,,"I have a number $z = a+re^{i(\pi-\varepsilon)}$ and $\varepsilon>0$ is small, $a,r>0.$ You can assume furthermore that $r\le a+2.$ I then define the expressions $$z_{\pm}:=\frac{1}{2} \left(z\pm \sqrt{z^2-4} \right).$$ The question is: Can one find a Taylor expansion of the imaginary part of $z_{\pm}$ in terms of $\varepsilon$ . I would like to know at least what the leading order terms are for $\varepsilon$ small. Let me finish with a quote of encouragement: Mark Twain — 'They did not know it was impossible so they did it'","I have a number and is small, You can assume furthermore that I then define the expressions The question is: Can one find a Taylor expansion of the imaginary part of in terms of . I would like to know at least what the leading order terms are for small. Let me finish with a quote of encouragement: Mark Twain — 'They did not know it was impossible so they did it'","z = a+re^{i(\pi-\varepsilon)} \varepsilon>0 a,r>0. r\le a+2. z_{\pm}:=\frac{1}{2} \left(z\pm \sqrt{z^2-4} \right). z_{\pm} \varepsilon \varepsilon","['real-analysis', 'calculus']"
58,determinant differentiable at identity,determinant differentiable at identity,,"I would like to prove that the determinant $det:\mathbb{M_n}\rightarrow\mathbb{R}$ is differentiable at the identity matrix with $(Ddet(I))(H)=tr(H)$ . Using the definition of differentiability this boils down to showing that: $$\displaystyle{\lim_{H \to 0}}\frac{\det(I+H)-det(I)-tr(H)}{||H||}=0 \space \space (\bigstar)$$ Now I'm aware of the formula: $\det(I+H)=1+tr(H)+\frac{(tr(H)^2-tr(H^2))}{2!}+\frac{(tr(H)^3-3tr(H)tr(H^2)+2tr(H^3))}{3!}\dots$ but I don't see any way to show that: $$\displaystyle{\lim_{H \to 0}}\frac{\frac{(tr(H)^2-tr(H^2))}{2!}+\frac{(tr(H)^3-3tr(H)tr(H^2)+2tr(H^3))}{3!}+\dots}{||H||}=0$$ $1)$ I'm aware of the directional derivative aproach (see here for example link ) but that can be used only after we know that the derivative exists, to show that $(Ddet(I))(H)=tr(H)$ . $2)$ I'm also aware of the proof treating the determinant as a polynomial function of $n^2$ variables. But I would like to know if there exists a proof using just the definition of derivative $(\bigstar)$ . Thank you!","I would like to prove that the determinant is differentiable at the identity matrix with . Using the definition of differentiability this boils down to showing that: Now I'm aware of the formula: but I don't see any way to show that: I'm aware of the directional derivative aproach (see here for example link ) but that can be used only after we know that the derivative exists, to show that . I'm also aware of the proof treating the determinant as a polynomial function of variables. But I would like to know if there exists a proof using just the definition of derivative . Thank you!",det:\mathbb{M_n}\rightarrow\mathbb{R} (Ddet(I))(H)=tr(H) \displaystyle{\lim_{H \to 0}}\frac{\det(I+H)-det(I)-tr(H)}{||H||}=0 \space \space (\bigstar) \det(I+H)=1+tr(H)+\frac{(tr(H)^2-tr(H^2))}{2!}+\frac{(tr(H)^3-3tr(H)tr(H^2)+2tr(H^3))}{3!}\dots \displaystyle{\lim_{H \to 0}}\frac{\frac{(tr(H)^2-tr(H^2))}{2!}+\frac{(tr(H)^3-3tr(H)tr(H^2)+2tr(H^3))}{3!}+\dots}{||H||}=0 1) (Ddet(I))(H)=tr(H) 2) n^2 (\bigstar),"['real-analysis', 'linear-algebra']"
59,"Construction of an Open, Dense, Connected Set in the Plane","Construction of an Open, Dense, Connected Set in the Plane",,"I'm stumped with the following problem. Let $\varepsilon>0$ be given. Prove that there exists an open, dense, and connected set $G\subset \mathbb{R}^{2}$ such that $m_{2}(G)<\varepsilon$ , where $m_{2}$ is the Lebesgue measure on $\mathbb{R}^{2}$ . My thoughts: I'm thinking that I need to use some sort of construction with a Cantor-like set in $\mathbb{R}^{2}$ and then take a set complement. However, I haven't worked with Cantor sets outside of $\mathbb{R}$ , so I'm not sure what constitutes a ""Cantor-like set"" in higher dimensions (if this is even defined or a valid construction) Is this roughly what I should want to do? Otherwise, I'm not sure where I should start. Thanks in advance for any help!","I'm stumped with the following problem. Let be given. Prove that there exists an open, dense, and connected set such that , where is the Lebesgue measure on . My thoughts: I'm thinking that I need to use some sort of construction with a Cantor-like set in and then take a set complement. However, I haven't worked with Cantor sets outside of , so I'm not sure what constitutes a ""Cantor-like set"" in higher dimensions (if this is even defined or a valid construction) Is this roughly what I should want to do? Otherwise, I'm not sure where I should start. Thanks in advance for any help!",\varepsilon>0 G\subset \mathbb{R}^{2} m_{2}(G)<\varepsilon m_{2} \mathbb{R}^{2} \mathbb{R}^{2} \mathbb{R},"['real-analysis', 'measure-theory', 'lebesgue-measure']"
60,"Is this seemingly new proof that $[0, 1]$ is compact correct?",Is this seemingly new proof that  is compact correct?,"[0, 1]","Hi all I think I found a new proof that $[0, 1]$ is compact but I am not 100% if it is correct, could you help me check? In the usual proof we just take the sup{x in [0, 1] : [0,  x] is covered by finitely many intervals} etc. My proof goes like this: First we show that compactness for $[0, 1]$ is equivalent to countable compactness : if we have an uncountable open cover of $[0, 1]$ then let us first notice that if a point is covered by more than 2 intervals one of the intervals is contained in the union of the other two so we may as well assume that any point in $[0, 1]$ is covered by at most two intervals. Next, select a rational from inside every interval in the cover and note that since every rational is contained in at most two intervals by the above and since there are countably many rationals we get a countable cover. Finally, to show that $[0, 1]$ is countably compact, we can do the following : suppose $I_1, I_2, ..., I_n, ...$ is a countable cover such that $[0,1]$ is not covered by $I_1, ..., I_k$ for any natural k. Now $[0,1] - I_1$ is a union of at most two closed intervals. It must be the case that at least one of these two intervals is never covered by $I_1,...,I_k$ for any natural k. Select this interval, wlog let's call it $J_1$ . Next, $J_1-I_2$ is also a union of at most two closed intervals, one of which cannot be covered by $I_1,...,I_k$ for any natural k.Let's call this interval $J_2$ . Inductively we get $J_1 \supset J_2 \supset J_3$ ... a descending sequence of closed intervals whose intersection is non-empty and not covered by any $I_n$ , contradiction. Is this correct? I find this proof much more intuitive than the usual one, don't you agree? N.B. actually a student of mine found this proof","Hi all I think I found a new proof that is compact but I am not 100% if it is correct, could you help me check? In the usual proof we just take the sup{x in [0, 1] : [0,  x] is covered by finitely many intervals} etc. My proof goes like this: First we show that compactness for is equivalent to countable compactness : if we have an uncountable open cover of then let us first notice that if a point is covered by more than 2 intervals one of the intervals is contained in the union of the other two so we may as well assume that any point in is covered by at most two intervals. Next, select a rational from inside every interval in the cover and note that since every rational is contained in at most two intervals by the above and since there are countably many rationals we get a countable cover. Finally, to show that is countably compact, we can do the following : suppose is a countable cover such that is not covered by for any natural k. Now is a union of at most two closed intervals. It must be the case that at least one of these two intervals is never covered by for any natural k. Select this interval, wlog let's call it . Next, is also a union of at most two closed intervals, one of which cannot be covered by for any natural k.Let's call this interval . Inductively we get ... a descending sequence of closed intervals whose intersection is non-empty and not covered by any , contradiction. Is this correct? I find this proof much more intuitive than the usual one, don't you agree? N.B. actually a student of mine found this proof","[0, 1] [0, 1] [0, 1] [0, 1] [0, 1] I_1, I_2, ..., I_n, ... [0,1] I_1, ..., I_k [0,1] - I_1 I_1,...,I_k J_1 J_1-I_2 I_1,...,I_k J_2 J_1 \supset J_2 \supset J_3 I_n","['real-analysis', 'alternative-proof']"
61,"Is $f$ differentiable at $0$, where $f(x) = x$ if $x$ is rational and $f(x) = 0$ otherwise?","Is  differentiable at , where  if  is rational and  otherwise?",f 0 f(x) = x x f(x) = 0,"This is the function: $$f(x)=\begin{cases}x& \text{if $x$ is rational}\\0 &\text{if $x$ is irrational}\end{cases}$$ My attempt: It's easy to verify that $f$ is continuous at $x=0$ using the sequential definition of continuity. I claim that $f$ is not differentiable at $x=0$ . Assume the contrary and let $f'(0)=L$ . Now, we pick an $\varepsilon$ such that $0<\varepsilon < |L|$ . For this choice of $\varepsilon$ there is a $\delta >0$ such that if $0<|x-0|<\delta$ then we have $\left| \frac{f(x)-f(0)}{x-0} -L\right| < \varepsilon $ . Now, pick $x' \in \mathbb{R}\setminus\mathbb{Q}$ with $0<|x'| <\delta$ . Then we have $\left| \frac{f(x')-f(0)}{x'-0} -L\right| = |L| > \varepsilon$ . A contradiction! Is this proof correct?","This is the function: My attempt: It's easy to verify that is continuous at using the sequential definition of continuity. I claim that is not differentiable at . Assume the contrary and let . Now, we pick an such that . For this choice of there is a such that if then we have . Now, pick with . Then we have . A contradiction! Is this proof correct?",f(x)=\begin{cases}x& \text{if x is rational}\\0 &\text{if x is irrational}\end{cases} f x=0 f x=0 f'(0)=L \varepsilon 0<\varepsilon < |L| \varepsilon \delta >0 0<|x-0|<\delta \left| \frac{f(x)-f(0)}{x-0} -L\right| < \varepsilon  x' \in \mathbb{R}\setminus\mathbb{Q} 0<|x'| <\delta \left| \frac{f(x')-f(0)}{x'-0} -L\right| = |L| > \varepsilon,"['real-analysis', 'derivatives']"
62,Differentiating closed formulas in formal power series,Differentiating closed formulas in formal power series,,"In Herbert Wilfs' book gfology, the generating function is defined ""formally"" as If $\displaystyle f = \sum_{i \geq 0} a_i x^i$, and $\displaystyle g = \sum_{i \geq 0} b_i x^i $, we define $\displaystyle f+g := \sum_{i \geq 0} (a_i + b_i) x^i$ $\displaystyle fg := \sum_{i \geq 0}(\sum_{m+n = i} a_mb_n)x^i$. $\displaystyle f' := \sum_{i \geq 0} ia_{i+1} x^i$ (Call this $\star$) This is fine and I understand this and have no problem with this. But when we say something like this in a formal setting: $\displaystyle e^x = \sum_{i \geq 0} \frac{x^n}{n!}$ Or, $\displaystyle \frac{1}{\sqrt{1-4x}} = \sum_{k \geq 0} \binom{2k}{k} x^k  $ (Call this equation $\spadesuit$) My question is: How is $e^x$ or $\displaystyle \frac{1}{\sqrt{1-4x}}$ defined in a formal setting ? Are they defined as the generating functions of the sequences I mentioned ? If it's defined like that, then how can you proceed to ""differentiate"" both sides of $\spadesuit$ to obtain $\displaystyle \frac{2}{\sqrt{(1-4x)^3}} = \sum_{k \geq 0} k \binom{2(k+1)}{k+1} x^k $ I understand how you formally differntiate the right hand side of $\spadesuit$ since that's defined in $\star$, but how the formal differentiation of $\frac{1}{\sqrt{1-4x}}$ defined (especially when limits may not make much sense working on some rings)?","In Herbert Wilfs' book gfology, the generating function is defined ""formally"" as If $\displaystyle f = \sum_{i \geq 0} a_i x^i$, and $\displaystyle g = \sum_{i \geq 0} b_i x^i $, we define $\displaystyle f+g := \sum_{i \geq 0} (a_i + b_i) x^i$ $\displaystyle fg := \sum_{i \geq 0}(\sum_{m+n = i} a_mb_n)x^i$. $\displaystyle f' := \sum_{i \geq 0} ia_{i+1} x^i$ (Call this $\star$) This is fine and I understand this and have no problem with this. But when we say something like this in a formal setting: $\displaystyle e^x = \sum_{i \geq 0} \frac{x^n}{n!}$ Or, $\displaystyle \frac{1}{\sqrt{1-4x}} = \sum_{k \geq 0} \binom{2k}{k} x^k  $ (Call this equation $\spadesuit$) My question is: How is $e^x$ or $\displaystyle \frac{1}{\sqrt{1-4x}}$ defined in a formal setting ? Are they defined as the generating functions of the sequences I mentioned ? If it's defined like that, then how can you proceed to ""differentiate"" both sides of $\spadesuit$ to obtain $\displaystyle \frac{2}{\sqrt{(1-4x)^3}} = \sum_{k \geq 0} k \binom{2(k+1)}{k+1} x^k $ I understand how you formally differntiate the right hand side of $\spadesuit$ since that's defined in $\star$, but how the formal differentiation of $\frac{1}{\sqrt{1-4x}}$ defined (especially when limits may not make much sense working on some rings)?",,"['real-analysis', 'combinatorics', 'taylor-expansion', 'generating-functions']"
63,The convolution of two $L^2(\mathbb R)$ functions is continuous,The convolution of two  functions is continuous,L^2(\mathbb R),"Take $f$ and $g$ $\in L^2(\mathbb R)$, then I want to show that $\lim_{h \to 0} \int f(x-y-h)g(y)dy = f \ast g(x)$. My idea is to first take $f$ to be a continuous function with a compact support, then $f$ has to be uniformly continuous. Then we know that $f \ast g$ has to be continuous by exchanging the order of the limit and integration. Then use the fact that $C_{c}(\mathbb R)$ is dense in $L^2(\mathbb R)$ to conclude the proof. Is this the right idea? Is there any simpler arguments?","Take $f$ and $g$ $\in L^2(\mathbb R)$, then I want to show that $\lim_{h \to 0} \int f(x-y-h)g(y)dy = f \ast g(x)$. My idea is to first take $f$ to be a continuous function with a compact support, then $f$ has to be uniformly continuous. Then we know that $f \ast g$ has to be continuous by exchanging the order of the limit and integration. Then use the fact that $C_{c}(\mathbb R)$ is dense in $L^2(\mathbb R)$ to conclude the proof. Is this the right idea? Is there any simpler arguments?",,['real-analysis']
64,Suppose $b \in \mathbb{R}$ and $|b| < \frac{1}{n}$ for every positive integer n. Prove that $b = 0$.,Suppose  and  for every positive integer n. Prove that .,b \in \mathbb{R} |b| < \frac{1}{n} b = 0,"This comes from an exercise in Appendix C from Axler's Measure, Integration & Real Analysis . The following is my approach. Suppose $b \neq 0$. Let $|b| = \epsilon$. Then          by Archimedean Property (2)          $$\exists n^* \in \mathbb{Z}^+ \text{ such that} \frac{1}{n^*} < \epsilon$$         but $b < \frac{1}{n}$, $\forall n \in \mathbb{Z}^+$. Hence a contradiction. Am I approaching this correctly, if I am are there any other approaches?","This comes from an exercise in Appendix C from Axler's Measure, Integration & Real Analysis . The following is my approach. Suppose $b \neq 0$. Let $|b| = \epsilon$. Then          by Archimedean Property (2)          $$\exists n^* \in \mathbb{Z}^+ \text{ such that} \frac{1}{n^*} < \epsilon$$         but $b < \frac{1}{n}$, $\forall n \in \mathbb{Z}^+$. Hence a contradiction. Am I approaching this correctly, if I am are there any other approaches?",,['real-analysis']
65,Number of zeros of a weighted sum of exponentials,Number of zeros of a weighted sum of exponentials,,"Let be $n$ an integer. Let be $a_1, \ldots, a_n \in \mathbb{R}$ not all null and $b_1 < \ldots < b_n$ reals. Let be $f : x \mapsto \sum\limits_{i=1}^{n} a_i \exp(b_i x)$. I am trying to show that $f$ can be null over at most $n - 1$ points. What I tried: Considering derivatives / series expansion. Looking at $f$ as an solution of a differential equation. Looked the $n = 2$ trivial case but failed to see how to do even $n = 3$ (tried induction) I tried to suppose that there would be more than or $n$ zeros and tried to find contradictions with the derivatives.","Let be $n$ an integer. Let be $a_1, \ldots, a_n \in \mathbb{R}$ not all null and $b_1 < \ldots < b_n$ reals. Let be $f : x \mapsto \sum\limits_{i=1}^{n} a_i \exp(b_i x)$. I am trying to show that $f$ can be null over at most $n - 1$ points. What I tried: Considering derivatives / series expansion. Looking at $f$ as an solution of a differential equation. Looked the $n = 2$ trivial case but failed to see how to do even $n = 3$ (tried induction) I tried to suppose that there would be more than or $n$ zeros and tried to find contradictions with the derivatives.",,"['real-analysis', 'exponential-sum']"
66,Do continuous weak derivatives imply continuity?,Do continuous weak derivatives imply continuity?,,"Let $U \subseteq \mathbb{R}^n$ be an open set. Let $f \in W^{1,p}(U)$ and suppose all the weak derivatives of $f$ are continuous. Is $f$ itself continuous? It is a classic fact that if we assume a-priori $f$ is also continuous, then it is in $C^1$. Here, however I do not assume $f$ is continuous.","Let $U \subseteq \mathbb{R}^n$ be an open set. Let $f \in W^{1,p}(U)$ and suppose all the weak derivatives of $f$ are continuous. Is $f$ itself continuous? It is a classic fact that if we assume a-priori $f$ is also continuous, then it is in $C^1$. Here, however I do not assume $f$ is continuous.",,"['real-analysis', 'sobolev-spaces', 'weak-derivatives']"
67,"The set of injective linear transformations is dense in $\mathcal{L}(\mathbb{R}^{n},\mathbb{R}^{m})$",The set of injective linear transformations is dense in,"\mathcal{L}(\mathbb{R}^{n},\mathbb{R}^{m})","The set of injective (surjective) linear transformations is dense in $\mathcal{L}(\mathbb{R}^{n},\mathbb{R}^{m})$ if $n\leq m$ (if $n\geq m$). I don't know how to show this. If $A_{1}$ is the set of injective linear transformations and $A_{2}$ is the set of surjective linear transformations, I shown that $A_{1}$ and $A_{2}$ are open sets in $\mathcal{L}(\mathbb{R}^{n},\mathbb{R}^{m})$. This a previous questions so, I think that I should use it, but I don't have any idea. Thanks for any hint.","The set of injective (surjective) linear transformations is dense in $\mathcal{L}(\mathbb{R}^{n},\mathbb{R}^{m})$ if $n\leq m$ (if $n\geq m$). I don't know how to show this. If $A_{1}$ is the set of injective linear transformations and $A_{2}$ is the set of surjective linear transformations, I shown that $A_{1}$ and $A_{2}$ are open sets in $\mathcal{L}(\mathbb{R}^{n},\mathbb{R}^{m})$. This a previous questions so, I think that I should use it, but I don't have any idea. Thanks for any hint.",,"['real-analysis', 'linear-algebra', 'metric-spaces']"
68,Bounded sequence $\{a_n\}_n$ such that $a_n < \frac{a_{n−1} + a_{n+1}}{2}$. Is $\{a_n\}_n$ convergent?,Bounded sequence  such that . Is  convergent?,\{a_n\}_n a_n < \frac{a_{n−1} + a_{n+1}}{2} \{a_n\}_n,"Let $\{a_n\}_n$ be a sequence of numbers in the interval $(0, 1)$ with the property that   $$a_n < \frac{a_{n−1} + a_{n+1}}{2}$$   for all $n = 2, 3, 4,\dots$. Show that this sequence is convergent. My attempt: We can write the inequality as $a_n - a_{n-1} < a_{n+1} - a_n$ So, sequence {$s_n$} = $a_{n+1} - a_n$ is monotonic and since -1<$s_n$<1 , it is also bounded and hence convergent. Sequence {$a_n$} is bounded and by Bolzano-Weierstrass property has a convergent subsequence {$a_{n_k}$}. Applying Cauchy sequence property on this convergent subsequence, we have for every $\epsilon$ >0, there is $N_0$, such that $|a_{n_l} - a_{n_k}|< \epsilon$ for all $l>k>N_0$ I feel like, from here I should have been able to prove this, but unfortunately I am stuck. Please help.","Let $\{a_n\}_n$ be a sequence of numbers in the interval $(0, 1)$ with the property that   $$a_n < \frac{a_{n−1} + a_{n+1}}{2}$$   for all $n = 2, 3, 4,\dots$. Show that this sequence is convergent. My attempt: We can write the inequality as $a_n - a_{n-1} < a_{n+1} - a_n$ So, sequence {$s_n$} = $a_{n+1} - a_n$ is monotonic and since -1<$s_n$<1 , it is also bounded and hence convergent. Sequence {$a_n$} is bounded and by Bolzano-Weierstrass property has a convergent subsequence {$a_{n_k}$}. Applying Cauchy sequence property on this convergent subsequence, we have for every $\epsilon$ >0, there is $N_0$, such that $|a_{n_l} - a_{n_k}|< \epsilon$ for all $l>k>N_0$ I feel like, from here I should have been able to prove this, but unfortunately I am stuck. Please help.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'cauchy-sequences']"
69,Can a continuous functions intersect each horizontal line an uncountable amount of times,Can a continuous functions intersect each horizontal line an uncountable amount of times,,"I'm looking to find out whether or not there exists a continuous function $f \colon \mathbb{R}\to\mathbb{R}$ such that for $\textbf{any}$ $\alpha\in\mathbb{R}$ the equation \begin{equation*} f(x)=\alpha \end{equation*} has an uncountable number of solutions. I am tempted to believe that such a function doesn't exist, but I can't prove it. Despite this, I was able to find functions satisfying weaker conditions. For example for the function $f \colon \mathbb{R}\to\mathbb{R}$, $f(x)=x\sin x$ and for any $\alpha$ the equation  \begin{equation*} f(x)=\alpha \end{equation*} has a countable number of solutions. An idea to build the desired function would be to try to replicate the behaviour of $\sin\frac{1}{x}$ around $0$, especially at plus minus infinity, but I can't quite get my head around this.","I'm looking to find out whether or not there exists a continuous function $f \colon \mathbb{R}\to\mathbb{R}$ such that for $\textbf{any}$ $\alpha\in\mathbb{R}$ the equation \begin{equation*} f(x)=\alpha \end{equation*} has an uncountable number of solutions. I am tempted to believe that such a function doesn't exist, but I can't prove it. Despite this, I was able to find functions satisfying weaker conditions. For example for the function $f \colon \mathbb{R}\to\mathbb{R}$, $f(x)=x\sin x$ and for any $\alpha$ the equation  \begin{equation*} f(x)=\alpha \end{equation*} has a countable number of solutions. An idea to build the desired function would be to try to replicate the behaviour of $\sin\frac{1}{x}$ around $0$, especially at plus minus infinity, but I can't quite get my head around this.",,"['real-analysis', 'continuity', 'cardinals']"
70,Strictly positive Riemann integrable Function,Strictly positive Riemann integrable Function,,"I read a good number of posts which deal with this specific topic, but none of them seem to answer my question. If this is a repost, I apologize. Alright so here is the problem, followed by a theorem which I use, and then my attempted proof: Exercise 7.4.4. Show that if $f(x)>0$ for all $x\in [a,b]$ and $f$ is integrable, then $\int_{a}^{b} f>0$. The theorem (and exercise) is from Abbott's Understanding Analysis , and my proof uses Theorem 7.4.2(ii): Theorem 7.4.2.(ii) Assume $f$ is integrable on $[a,b]$. If $m\leq f(x)\leq M$ for all $x\in [a,b]$ then  $$m(b-a)\leq\int_a^b f(x) \leq M(b-a)$$ Now here is my proof: Proof . Let $f(x)>0$ for all $x\in [a,b]$ and assume $f$ is integrable. Since $f$ is integrable, then we can let ( but can we? ) $$m=\inf\{f(x_0):x_0 \in [a,b]\}\text{ and  }M=\sup\{f(x_0):x_0\in [a,b]\}$$  Where both $m$ and $M$ are greater than zero since $f(x)>0$.  Now by Theorem 7.4.2.(ii), we can write $$m(b-a)\leq \int_{a}^{b} f\leq M(b-a)$$ And since $0<m(b-a)\leq M(b-a)$, then the integral must be greater than zero as well. Q.E.D. I would just like some verification/correction. My doubt is whether or not the values $m$ and $M$ necessarily have to exist, since Theorem 7.4.2.(ii) states that the inequality is only true IF there exists such values for $m$ and $M$. But since $f$ is integrable and $[a,b]$ is compact, then the function should attain a maximum and a minimum, correct?","I read a good number of posts which deal with this specific topic, but none of them seem to answer my question. If this is a repost, I apologize. Alright so here is the problem, followed by a theorem which I use, and then my attempted proof: Exercise 7.4.4. Show that if $f(x)>0$ for all $x\in [a,b]$ and $f$ is integrable, then $\int_{a}^{b} f>0$. The theorem (and exercise) is from Abbott's Understanding Analysis , and my proof uses Theorem 7.4.2(ii): Theorem 7.4.2.(ii) Assume $f$ is integrable on $[a,b]$. If $m\leq f(x)\leq M$ for all $x\in [a,b]$ then  $$m(b-a)\leq\int_a^b f(x) \leq M(b-a)$$ Now here is my proof: Proof . Let $f(x)>0$ for all $x\in [a,b]$ and assume $f$ is integrable. Since $f$ is integrable, then we can let ( but can we? ) $$m=\inf\{f(x_0):x_0 \in [a,b]\}\text{ and  }M=\sup\{f(x_0):x_0\in [a,b]\}$$  Where both $m$ and $M$ are greater than zero since $f(x)>0$.  Now by Theorem 7.4.2.(ii), we can write $$m(b-a)\leq \int_{a}^{b} f\leq M(b-a)$$ And since $0<m(b-a)\leq M(b-a)$, then the integral must be greater than zero as well. Q.E.D. I would just like some verification/correction. My doubt is whether or not the values $m$ and $M$ necessarily have to exist, since Theorem 7.4.2.(ii) states that the inequality is only true IF there exists such values for $m$ and $M$. But since $f$ is integrable and $[a,b]$ is compact, then the function should attain a maximum and a minimum, correct?",,"['calculus', 'real-analysis', 'integration', 'proof-verification']"
71,Existence of Partial Derivatives Implies Differentiability [closed],Existence of Partial Derivatives Implies Differentiability [closed],,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 years ago . Improve this question Doing some self-study. My textbook has this Theorem (see below). I understand it but was hoping for something shorter and more intuitive. Any thoughts?,Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 years ago . Improve this question Doing some self-study. My textbook has this Theorem (see below). I understand it but was hoping for something shorter and more intuitive. Any thoughts?,,"['real-analysis', 'multivariable-calculus', 'vector-analysis', 'self-learning']"
72,"Among $2n-1$ irrationals there are $x_1,\dots,x_n$ such that for rationals $a_i\ge0$ with $\sum_{i=1}^{n}a_i>0$, $\sum_{i=1}^{n}a_ix_i$ is irrational","Among  irrationals there are  such that for rationals  with ,  is irrational","2n-1 x_1,\dots,x_n a_i\ge0 \sum_{i=1}^{n}a_i>0 \sum_{i=1}^{n}a_ix_i","Given a set $S=\{z_1,\cdots, z_{2n-1}\}\subset \Bbb Q^c,$ of $2n-1$ different irrational numbers, prove that there are $n$ different elements $x_1,\cdots,x_n\in S$ such that for all non-negative rational numbers $a_1,\cdots,a_n\in\Bbb Q$, with $a_1+\cdots+a_n>0 $ we have that  $$a_1x_1+\cdots+a_nx_n$$ is an irrational number. I have checked that this is true for $n=1,2,3,4$, but could not figure out how to use induction to argue in general. Any thoughts are greatly appreciated.","Given a set $S=\{z_1,\cdots, z_{2n-1}\}\subset \Bbb Q^c,$ of $2n-1$ different irrational numbers, prove that there are $n$ different elements $x_1,\cdots,x_n\in S$ such that for all non-negative rational numbers $a_1,\cdots,a_n\in\Bbb Q$, with $a_1+\cdots+a_n>0 $ we have that  $$a_1x_1+\cdots+a_nx_n$$ is an irrational number. I have checked that this is true for $n=1,2,3,4$, but could not figure out how to use induction to argue in general. Any thoughts are greatly appreciated.",,"['real-analysis', 'contest-math', 'irrational-numbers', 'rational-numbers']"
73,Connection between Riemann and Riemann-Stieltjes Integrals,Connection between Riemann and Riemann-Stieltjes Integrals,,"Let functions $f$ and $h$ be Riemann integrable and $H(x) = \int_a^x h(t)dt$.  Is it always true that the Riemann-Stieltjes integral $\int_a^bf(x)dH(x)$ exists and $\int_a^b f(x) dH(x) = \int_a^bf(x)h(x) dx$? I remember seeing this used in a reference without a proof. How is it proved? The closest I could find was the more restrictive Theorem 6.17 in Principles of Mathematical Analysis by Rudin. He proves $\int_a^b fd\alpha = \int_a^bf(x) \alpha'(x) dx$ when $\alpha$ is differentiable in $[a,b]$ and the derivative $\alpha'$ is Riemann integrable.","Let functions $f$ and $h$ be Riemann integrable and $H(x) = \int_a^x h(t)dt$.  Is it always true that the Riemann-Stieltjes integral $\int_a^bf(x)dH(x)$ exists and $\int_a^b f(x) dH(x) = \int_a^bf(x)h(x) dx$? I remember seeing this used in a reference without a proof. How is it proved? The closest I could find was the more restrictive Theorem 6.17 in Principles of Mathematical Analysis by Rudin. He proves $\int_a^b fd\alpha = \int_a^bf(x) \alpha'(x) dx$ when $\alpha$ is differentiable in $[a,b]$ and the derivative $\alpha'$ is Riemann integrable.",,"['real-analysis', 'integration', 'stieltjes-integral']"
74,How to show that Bernstein set is not Lebesgue measurable?,How to show that Bernstein set is not Lebesgue measurable?,,"A Bernstein set is a subset of the real line that meets every uncountable closed subset of the real line but that contains none of them. (They can be easily generalized to $\mathbb R^n$, but for the sake of simplicity we might stick with $\mathbb R$.) See also this post for some more details: What's application of Bernstein Set? (A proof of existence of Bernstein sets is given there - it is based on transfinite induction and well-ordering theorem. Also various properties of Bernstein sets are mentioned there, including the fact that existence of Bernstein sets cannot be shown in ZF.) My questions are: How can we show that a Bernstein set is not Lebesgue measurable? Can these proofs be generalized to other measures? By the latter I mean whether something like this can be said about some of the proofs: ""We have shown that Bernstein set is not Lebesgue measurable. But the same proof works for any translation-invariant measure such that all closed sets are measurable and bounded sets have finite measure."" (This is just a hypothetical example to make a bit clearer what I mean by the second question.) When I was thinking about this problem, I thought that one way to go would be using regularity of Lebesgue measure . Let $B$ be a Bernstein set. If $C\subseteq B$ is compact, then it has to be countable and thus $\mu(C)=0$. Similarly, if $B\subseteq U$ then $B$ does not intersect the closed set $\mathbb R\setminus U$, hence $U$ is complement of countable set and $\mu(U)=\infty$. So from regularity of Lebesgue measure we get that $B$ is not measurable. I have considered also posting my attempt sketched in the previous paragraph as an answer. But I decided not to do so - maybe somebody who knows more about this will be able to expand on this or add some other related results and useful observations.","A Bernstein set is a subset of the real line that meets every uncountable closed subset of the real line but that contains none of them. (They can be easily generalized to $\mathbb R^n$, but for the sake of simplicity we might stick with $\mathbb R$.) See also this post for some more details: What's application of Bernstein Set? (A proof of existence of Bernstein sets is given there - it is based on transfinite induction and well-ordering theorem. Also various properties of Bernstein sets are mentioned there, including the fact that existence of Bernstein sets cannot be shown in ZF.) My questions are: How can we show that a Bernstein set is not Lebesgue measurable? Can these proofs be generalized to other measures? By the latter I mean whether something like this can be said about some of the proofs: ""We have shown that Bernstein set is not Lebesgue measurable. But the same proof works for any translation-invariant measure such that all closed sets are measurable and bounded sets have finite measure."" (This is just a hypothetical example to make a bit clearer what I mean by the second question.) When I was thinking about this problem, I thought that one way to go would be using regularity of Lebesgue measure . Let $B$ be a Bernstein set. If $C\subseteq B$ is compact, then it has to be countable and thus $\mu(C)=0$. Similarly, if $B\subseteq U$ then $B$ does not intersect the closed set $\mathbb R\setminus U$, hence $U$ is complement of countable set and $\mu(U)=\infty$. So from regularity of Lebesgue measure we get that $B$ is not measurable. I have considered also posting my attempt sketched in the previous paragraph as an answer. But I decided not to do so - maybe somebody who knows more about this will be able to expand on this or add some other related results and useful observations.",,"['real-analysis', 'general-topology', 'measure-theory', 'lebesgue-measure', 'descriptive-set-theory']"
75,Proving the FKG Inequality : $\mathbb{E}[f(X)g(X)] \geq \mathbb{E}[f(X)]\mathbb{E}[g(X)]$ using the Chebychev Sum Inequality,Proving the FKG Inequality :  using the Chebychev Sum Inequality,\mathbb{E}[f(X)g(X)] \geq \mathbb{E}[f(X)]\mathbb{E}[g(X)],"$f$ and $g$ are nondecreasing functions, then for any random variable $X$, show that: $$\mathbb{E}[f(X)g(X)] \geq \mathbb{E}[f(X)]\mathbb{E}[g(X)]$$ I know this question has been addressed here before: Show $\mathbb{E}[f(X)g(X)] \geq \mathbb{E}[f(X)]\mathbb{E}[g(X)]$ for $f,g$ bounded, nondecreasing Inequality for Expected Value of Product However, what I am interested in attempting to do is prove it using the Chebychev Sum Inequality which states: Given $a_1 \geq a_2 \geq \cdots \geq a_n$ and $b_1 \geq b_2 \geq \cdots \geq b_n$ then $$ {1\over n} \sum_{k=1}^n a_k \cdot b_k \geq \left({1\over n}\sum_{k=1}^n a_k\right)\left({1\over n}\sum_{k=1}^n b_k\right).$$ Similarly, if $a_1 \leq a_2 \leq \cdots \leq a_n$ and $b_1 \geq b_2 \geq \cdots \geq b_n,$, then the opposite inequality holds. Logically, I feel such an argument should be possible since $f$ and $g$ being monotone non-decreasing are known to be measurable and $X$ being a random variable i.e. a measurable function, their respective compositions with it should also be measurable. If we take $F := f \circ X$ and $G:= g \circ X$ then the problem is asking us to show : $$\int FG d\mu \geq \int Fd\mu \int Gd\mu $$ It is also known that the product of two simple functions is also a simple functions. So, we can represent each of these integrals as a limits of integrals of simple functions (increasing limits of simple functions if we want to use MCT). What I want to do is prove the desired inequality using Chebychev for the integrals of each of the simple functions and use a limiting argument to extend the result generally. I am familiar with the other proofs (Coupling, Fubini-Tonelli, Covariance operator as guiding intuition etc), admittedly simpler, but this is something that I have been trying to do for a while -- knowing that it should technically be possible -- but haven't been able to.","$f$ and $g$ are nondecreasing functions, then for any random variable $X$, show that: $$\mathbb{E}[f(X)g(X)] \geq \mathbb{E}[f(X)]\mathbb{E}[g(X)]$$ I know this question has been addressed here before: Show $\mathbb{E}[f(X)g(X)] \geq \mathbb{E}[f(X)]\mathbb{E}[g(X)]$ for $f,g$ bounded, nondecreasing Inequality for Expected Value of Product However, what I am interested in attempting to do is prove it using the Chebychev Sum Inequality which states: Given $a_1 \geq a_2 \geq \cdots \geq a_n$ and $b_1 \geq b_2 \geq \cdots \geq b_n$ then $$ {1\over n} \sum_{k=1}^n a_k \cdot b_k \geq \left({1\over n}\sum_{k=1}^n a_k\right)\left({1\over n}\sum_{k=1}^n b_k\right).$$ Similarly, if $a_1 \leq a_2 \leq \cdots \leq a_n$ and $b_1 \geq b_2 \geq \cdots \geq b_n,$, then the opposite inequality holds. Logically, I feel such an argument should be possible since $f$ and $g$ being monotone non-decreasing are known to be measurable and $X$ being a random variable i.e. a measurable function, their respective compositions with it should also be measurable. If we take $F := f \circ X$ and $G:= g \circ X$ then the problem is asking us to show : $$\int FG d\mu \geq \int Fd\mu \int Gd\mu $$ It is also known that the product of two simple functions is also a simple functions. So, we can represent each of these integrals as a limits of integrals of simple functions (increasing limits of simple functions if we want to use MCT). What I want to do is prove the desired inequality using Chebychev for the integrals of each of the simple functions and use a limiting argument to extend the result generally. I am familiar with the other proofs (Coupling, Fubini-Tonelli, Covariance operator as guiding intuition etc), admittedly simpler, but this is something that I have been trying to do for a while -- knowing that it should technically be possible -- but haven't been able to.",,"['real-analysis', 'probability', 'analysis', 'probability-theory', 'measure-theory']"
76,Prove that $\sqrt {f(x)}$ is Lipschitz,Prove that  is Lipschitz,\sqrt {f(x)},"Let $f(x)\in C^2(\mathbb{R}), f(x)\geq0,f''(x)\leq1,$ prove that $\sqrt{f(x)}$ is a Lipschitz function . I can prove that $f(x)$ is uniformly continuous by the inequality without the condition of the $f''(x)$, so I want to ask someone for a better answer. Thanks in advance.","Let $f(x)\in C^2(\mathbb{R}), f(x)\geq0,f''(x)\leq1,$ prove that $\sqrt{f(x)}$ is a Lipschitz function . I can prove that $f(x)$ is uniformly continuous by the inequality without the condition of the $f''(x)$, so I want to ask someone for a better answer. Thanks in advance.",,"['real-analysis', 'analysis', 'lipschitz-functions']"
77,Compact preimage of a point by C¹ function,Compact preimage of a point by C¹ function,,"Let $f:\mathbb{R}^{m} \to \mathbb{R}$ ($m \geq 2$) be a $C^{1}$ function such that, for some $c \in \mathbb{R}$, $f^{-1}(c)$ is compact and non-empty. Show that $F=\{x \in \mathbb{R}^{m} | f(x) \leq c\}$ or $G=\{x \in \mathbb{R}^{m}| f(x) \geq c\}$ is compact. $F$ and $G$ are closed, then I only need show that one of them is bounded. I have tried to show by contradiction.","Let $f:\mathbb{R}^{m} \to \mathbb{R}$ ($m \geq 2$) be a $C^{1}$ function such that, for some $c \in \mathbb{R}$, $f^{-1}(c)$ is compact and non-empty. Show that $F=\{x \in \mathbb{R}^{m} | f(x) \leq c\}$ or $G=\{x \in \mathbb{R}^{m}| f(x) \geq c\}$ is compact. $F$ and $G$ are closed, then I only need show that one of them is bounded. I have tried to show by contradiction.",,"['real-analysis', 'analysis']"
78,$a_{n+1} = \cos(a_n)$ with $a_1 = x$,with,a_{n+1} = \cos(a_n) a_1 = x,Let  $(a_n)_{n=1}^{\infty}$ be a sequence defined by $a_1=x$  and $  a_{n+1}=\cos(a_n)$ then check the  the convergence of sequence . Here what Approached is like $-1\leq \cos x \leq +1$ so we get $a_2 = cos(\mathbb{I_1})$ in general $a_{n+1} = cos(\mathbb{I_n})$ and cos being an even function $\mathbb{I_2}$ onwards  it will contain elements $0 \leq x\leq c  $ and  $c<1$  hence it converges to $cos (0)=1$ It that a fair argument,Let  $(a_n)_{n=1}^{\infty}$ be a sequence defined by $a_1=x$  and $  a_{n+1}=\cos(a_n)$ then check the  the convergence of sequence . Here what Approached is like $-1\leq \cos x \leq +1$ so we get $a_2 = cos(\mathbb{I_1})$ in general $a_{n+1} = cos(\mathbb{I_n})$ and cos being an even function $\mathbb{I_2}$ onwards  it will contain elements $0 \leq x\leq c  $ and  $c<1$  hence it converges to $cos (0)=1$ It that a fair argument,,['real-analysis']
79,Prove the maximum of the function $\det x$ restrained to the sphere $\sum x_{ij}^2=n$ is $1$,Prove the maximum of the function  restrained to the sphere  is,\det x \sum x_{ij}^2=n 1,"Let $M(n\times n)=\mathbb{R}^{n^2}$. Prove the maximum of $f:\mathbb{R}^{n^2}\to\mathbb{R}$, given by $f(x)=\det x$, restrained by the sphere $\sum_{i,j} x_{ij}^2=n$, is reached in an orthogonal matrix, so its value is $1$. Attempt: define $\varphi=\sum_{i,j} x_{ij}^2=n$, so $\nabla\varphi(x)=2[x_{ij}]$ $\nabla f(x)=(-1)^{i+j}X_{ij}$, with $X_{ij}$ the $ij$-minor of $x$. Using the Lagrange Multipliers, with $M=\varphi^{-1}(n)$, the maximum of $f|_M$ is given when $(-1)^{i+j}X_{ij}=2\lambda x_{ij}\Rightarrow(-1)^{i+j}X_{ij}x_{ij}=2\lambda x_{ij}^2$. I sum in $i$ and $j$ to get: $n\det x=2\lambda\sum_{i,j} x_{ij}^2=2\lambda n\Rightarrow\det x = 2\lambda$. I couldn't go further, specially proving that the matrix is orthogonal. Any help?","Let $M(n\times n)=\mathbb{R}^{n^2}$. Prove the maximum of $f:\mathbb{R}^{n^2}\to\mathbb{R}$, given by $f(x)=\det x$, restrained by the sphere $\sum_{i,j} x_{ij}^2=n$, is reached in an orthogonal matrix, so its value is $1$. Attempt: define $\varphi=\sum_{i,j} x_{ij}^2=n$, so $\nabla\varphi(x)=2[x_{ij}]$ $\nabla f(x)=(-1)^{i+j}X_{ij}$, with $X_{ij}$ the $ij$-minor of $x$. Using the Lagrange Multipliers, with $M=\varphi^{-1}(n)$, the maximum of $f|_M$ is given when $(-1)^{i+j}X_{ij}=2\lambda x_{ij}\Rightarrow(-1)^{i+j}X_{ij}x_{ij}=2\lambda x_{ij}^2$. I sum in $i$ and $j$ to get: $n\det x=2\lambda\sum_{i,j} x_{ij}^2=2\lambda n\Rightarrow\det x = 2\lambda$. I couldn't go further, specially proving that the matrix is orthogonal. Any help?",,"['real-analysis', 'lagrange-multiplier']"
80,How many monotonic functions from $\mathbb{R}$ to $\mathbb{R}$ are there,How many monotonic functions from  to  are there,\mathbb{R} \mathbb{R},"from $\mathbb{N}$ to $\mathbb{N}$ there are $2^{\aleph_0}$ as you can define $2^{\aleph_0}$ functions as following: For every subset of $\mathbb{N}$ keep the numbers in the subset the same, and add 1 to the rest. What about the cardinality of all montonic functions from $\mathbb{R}$ to $\mathbb{R}$? Is it ${\mathfrak c}$ or $2^{\mathfrak c}$, ${\aleph_1}$ or ${\aleph_2}$ My intuition is to say that its cardinality is the same as the cardinality of the set of real continuous functions which is ${\mathfrak c}$, as monotonic functions are continuous except possibly at a countable number of points, but I couldn't come up with a concrete proof.","from $\mathbb{N}$ to $\mathbb{N}$ there are $2^{\aleph_0}$ as you can define $2^{\aleph_0}$ functions as following: For every subset of $\mathbb{N}$ keep the numbers in the subset the same, and add 1 to the rest. What about the cardinality of all montonic functions from $\mathbb{R}$ to $\mathbb{R}$? Is it ${\mathfrak c}$ or $2^{\mathfrak c}$, ${\aleph_1}$ or ${\aleph_2}$ My intuition is to say that its cardinality is the same as the cardinality of the set of real continuous functions which is ${\mathfrak c}$, as monotonic functions are continuous except possibly at a countable number of points, but I couldn't come up with a concrete proof.",,['real-analysis']
81,"Prove $f(x) = \sqrt x$ is uniformly continuous on $[0,\infty)$",Prove  is uniformly continuous on,"f(x) = \sqrt x [0,\infty)","I have seen a proof in $\sqrt x$ is uniformly continuous Below shows an alternative proof. Please correct me if im wrong. Proof: For any given $\varepsilon >0$, Let $\delta_1 = \frac{\varepsilon}{2}$, $\forall x,y \in [1,\infty)$ with $|x-y|<\delta_1$ Since $|\sqrt x + \sqrt y|\geq2$ $$|\sqrt x - \sqrt y|  = \frac{|x-y|}{|\sqrt x+\sqrt y|} < |x-y| < \delta_1 = \frac{\varepsilon}{2}$$ Hence, $\sqrt x$ is uniformly continuous on $[1,\infty)$. $\sqrt x$ is continuous on [0,1] , so $\sqrt x$ is uniformly continuous on [0,1]. So, there exist $\delta_2 > 0$ such that $\forall x,y \in [0,1], |x-y|<\delta_2$, $|\sqrt x -\sqrt y| <\frac{\varepsilon}{2}$ Let $\delta = \min{(\delta_1,\delta_2)}$ $\forall x,y \in [0,\infty)$ with $|x-y|<\delta$, Case 1: $x,y \in [0,1]$ Proven above as $|x-y| < \frac{\varepsilon}{2} < \varepsilon$ Case 2: $x,y \in [1,\infty)$ Proven above as $|x-y| < \frac{\varepsilon}{2} < \varepsilon$ Case 3: $x \in [0,1] , y \in [1,\infty]$ $$|\sqrt x-\sqrt y| = |\sqrt x -1+1-\sqrt y| \leq |\sqrt x-1| + |\sqrt y -1| < \frac{\varepsilon}{2} + \frac{\varepsilon}{2}= \varepsilon$$ by applying case 1 and case 2.","I have seen a proof in $\sqrt x$ is uniformly continuous Below shows an alternative proof. Please correct me if im wrong. Proof: For any given $\varepsilon >0$, Let $\delta_1 = \frac{\varepsilon}{2}$, $\forall x,y \in [1,\infty)$ with $|x-y|<\delta_1$ Since $|\sqrt x + \sqrt y|\geq2$ $$|\sqrt x - \sqrt y|  = \frac{|x-y|}{|\sqrt x+\sqrt y|} < |x-y| < \delta_1 = \frac{\varepsilon}{2}$$ Hence, $\sqrt x$ is uniformly continuous on $[1,\infty)$. $\sqrt x$ is continuous on [0,1] , so $\sqrt x$ is uniformly continuous on [0,1]. So, there exist $\delta_2 > 0$ such that $\forall x,y \in [0,1], |x-y|<\delta_2$, $|\sqrt x -\sqrt y| <\frac{\varepsilon}{2}$ Let $\delta = \min{(\delta_1,\delta_2)}$ $\forall x,y \in [0,\infty)$ with $|x-y|<\delta$, Case 1: $x,y \in [0,1]$ Proven above as $|x-y| < \frac{\varepsilon}{2} < \varepsilon$ Case 2: $x,y \in [1,\infty)$ Proven above as $|x-y| < \frac{\varepsilon}{2} < \varepsilon$ Case 3: $x \in [0,1] , y \in [1,\infty]$ $$|\sqrt x-\sqrt y| = |\sqrt x -1+1-\sqrt y| \leq |\sqrt x-1| + |\sqrt y -1| < \frac{\varepsilon}{2} + \frac{\varepsilon}{2}= \varepsilon$$ by applying case 1 and case 2.",,"['real-analysis', 'proof-verification', 'uniform-continuity']"
82,"If $f(a^+)$ and $f(a^-)$ exist, then $f$ is bounded.","If  and  exist, then  is bounded.",f(a^+) f(a^-) f,"Let $f:[0,1] \to \mathbb{R}$ be a function such that for every $a \in [0,1)$ and $b \in (0,1]$ the one-sided limits $$f(a^+)=\lim _{x\to a^+}f(x) \in \mathbb{R}$$ $$f(b^-)=\lim _{x \to b^-} f(x) \in \mathbb {R}$$ exist. A) Show that $f$ is bounded. B) Does $f$ necessarily achieve its maximum at some $x \in [0,1]$? C) Suppose further that $f$ is continuous at $0$ and $1$, and that $f(0) f(1)<0$. Prove that there exists  some point $p \in (0,1)$ such that $f(p^-)f(p^+) \leq 0$. Intuitively, I can see why part A is true, but I am not sure how to prove this formally. For part B, I think the answer is no, but I haven't yet come up with a counterexample. My initial thoughts on part C are to somehow apply the intermediate value theorem, but I am not sure if this is the correct approach or not.","Let $f:[0,1] \to \mathbb{R}$ be a function such that for every $a \in [0,1)$ and $b \in (0,1]$ the one-sided limits $$f(a^+)=\lim _{x\to a^+}f(x) \in \mathbb{R}$$ $$f(b^-)=\lim _{x \to b^-} f(x) \in \mathbb {R}$$ exist. A) Show that $f$ is bounded. B) Does $f$ necessarily achieve its maximum at some $x \in [0,1]$? C) Suppose further that $f$ is continuous at $0$ and $1$, and that $f(0) f(1)<0$. Prove that there exists  some point $p \in (0,1)$ such that $f(p^-)f(p^+) \leq 0$. Intuitively, I can see why part A is true, but I am not sure how to prove this formally. For part B, I think the answer is no, but I haven't yet come up with a counterexample. My initial thoughts on part C are to somehow apply the intermediate value theorem, but I am not sure if this is the correct approach or not.",,['real-analysis']
83,Construct series,Construct series,,"Are there two non-negative, monotone sequences ${\{a_n\}}$ and ${\{b_n\}}$, s.t. $\sum{a_n}$ and $\sum{b_n}$ diverge, but $\sum{min{(a_n,b_n)}}$ converges? I guess that the convergence speed must be carefully controlled, but I couldn't find such sequences. Thanks in advance for any help.","Are there two non-negative, monotone sequences ${\{a_n\}}$ and ${\{b_n\}}$, s.t. $\sum{a_n}$ and $\sum{b_n}$ diverge, but $\sum{min{(a_n,b_n)}}$ converges? I guess that the convergence speed must be carefully controlled, but I couldn't find such sequences. Thanks in advance for any help.",,['real-analysis']
84,"For $\{a,b\}\subset(0,1]$ prove that $a^{b-a}+b^{a-b}+(a-b)^2\leq2$",For  prove that,"\{a,b\}\subset(0,1] a^{b-a}+b^{a-b}+(a-b)^2\leq2","Let $\{a,b\}\subset(0,1]$. Prove that:   $$a^{b-a}+b^{a-b}+(a-b)^2\leq2$$ I don't see even how to begin the proof. We can rewrite our inequality in the following form $$\frac{a^b}{a^a}+\frac{b^a}{b^b}+(a-b)^2\leq2$$ or $$(ab)^b+(ab)^a\leq(2-(a-b)^2)a^ab^b.$$ By Jensen easy to show that  $a^ab^b\geq\left(\frac{a+b}{2}\right)^{a+b}$. Thus, it remains to prove that $$\left(\frac{a+b}{2}\right)^{a+b}(2-(a-b)^2)\geq(ab)^b+(ab)^a$$ and I not sure that the last inequality is true. Thank you!","Let $\{a,b\}\subset(0,1]$. Prove that:   $$a^{b-a}+b^{a-b}+(a-b)^2\leq2$$ I don't see even how to begin the proof. We can rewrite our inequality in the following form $$\frac{a^b}{a^a}+\frac{b^a}{b^b}+(a-b)^2\leq2$$ or $$(ab)^b+(ab)^a\leq(2-(a-b)^2)a^ab^b.$$ By Jensen easy to show that  $a^ab^b\geq\left(\frac{a+b}{2}\right)^{a+b}$. Thus, it remains to prove that $$\left(\frac{a+b}{2}\right)^{a+b}(2-(a-b)^2)\geq(ab)^b+(ab)^a$$ and I not sure that the last inequality is true. Thank you!",,"['calculus', 'real-analysis', 'inequality', 'contest-math', 'exponential-function']"
85,Prove linear combinations of logarithms of primes over $\mathbb{Q}$ is independent,Prove linear combinations of logarithms of primes over  is independent,\mathbb{Q},"Suppose we have a set of primes $p_1,\dots,p_t$. Prove that  $\log p_1,\dots,\log p_t$ is linear independent over $\mathbb{Q}$. Now, this implies $ \sum_{j=1}^{t}x_j\log(p_j)=0 \iff x_1=\dots=x_t=0$. I think I have to use that fact that every $q\in\mathbb{Q}$ can be written as $\prod_{\mathcal{P}}$, where $n_p$ is a unique sequence ($n_2$,$n_3$,$\dots$) with domain $\mathbb{Z}$. Here, $\mathcal{P}$ denotes the set of all integers. Now how can I use this to prove the linear independency?","Suppose we have a set of primes $p_1,\dots,p_t$. Prove that  $\log p_1,\dots,\log p_t$ is linear independent over $\mathbb{Q}$. Now, this implies $ \sum_{j=1}^{t}x_j\log(p_j)=0 \iff x_1=\dots=x_t=0$. I think I have to use that fact that every $q\in\mathbb{Q}$ can be written as $\prod_{\mathcal{P}}$, where $n_p$ is a unique sequence ($n_2$,$n_3$,$\dots$) with domain $\mathbb{Z}$. Here, $\mathcal{P}$ denotes the set of all integers. Now how can I use this to prove the linear independency?",,['real-analysis']
86,Asymptotic behavior of integral $\int_1^\infty \frac{e^{-xt}}{\sqrt{1+t^2}}dt$ as $x \to 0$,Asymptotic behavior of integral  as,\int_1^\infty \frac{e^{-xt}}{\sqrt{1+t^2}}dt x \to 0,"I wish to prove that: $$ \int_1^\infty \frac{e^{-xt}}{\sqrt{1+t^2}}dt \sim - \ln x \quad  \mathrm{as} \quad x \to 0^+$$ using the fact that: $$ f \underset{b}{\sim} g \Rightarrow \int_a^x f \underset{x \to b}{\sim} \int_a^x g$$ if $\int_a^x g \to \infty$ as $x \to b$ and $f$ and $g$ are integrable on every interval $[a,c]$ with $c < b$. Does anyone have an idea? Thank you!","I wish to prove that: $$ \int_1^\infty \frac{e^{-xt}}{\sqrt{1+t^2}}dt \sim - \ln x \quad  \mathrm{as} \quad x \to 0^+$$ using the fact that: $$ f \underset{b}{\sim} g \Rightarrow \int_a^x f \underset{x \to b}{\sim} \int_a^x g$$ if $\int_a^x g \to \infty$ as $x \to b$ and $f$ and $g$ are integrable on every interval $[a,c]$ with $c < b$. Does anyone have an idea? Thank you!",,"['real-analysis', 'asymptotics']"
87,"$(\mathbb R, \oplus)$ is a group. Define a multiplication with which we get a field. Where $a \oplus b = a + b +1$",is a group. Define a multiplication with which we get a field. Where,"(\mathbb R, \oplus) a \oplus b = a + b +1","I have this problem from the book Real Analysis of Miklos Laczkovich ( p. $35$ ): Let $\oplus : \mathbb R \times \mathbb R\to \mathbb R $, where $(a,b) \mapsto a + b +1$. Then,  $(\mathbb R, \oplus)$ is a group. Define a multiplication with which we get a field. But in the exercise there are no hints. Could anyone help me?","I have this problem from the book Real Analysis of Miklos Laczkovich ( p. $35$ ): Let $\oplus : \mathbb R \times \mathbb R\to \mathbb R $, where $(a,b) \mapsto a + b +1$. Then,  $(\mathbb R, \oplus)$ is a group. Define a multiplication with which we get a field. But in the exercise there are no hints. Could anyone help me?",,"['real-analysis', 'field-theory', 'binary-operations']"
88,Prove that $\underline{\lim} \{a_n\}+ \underline{\lim} \{b_n\} \leq \underline{\lim} (a_n + b_n)$,Prove that,\underline{\lim} \{a_n\}+ \underline{\lim} \{b_n\} \leq \underline{\lim} (a_n + b_n),"In my real analysis class, my instructor proved that $\underline{\lim}  \{a_n\}+ \underline{\lim} \{b_n\} \leq     \underline{\lim} (a_n + b_n)$. (Note that $\underline{\lim}$ is the limit inferior of a sequence.) but a lot of details were left out and I want to make sure that I have a correct proof and that my logic is correct. Note that $\underline{lim} \{a_n\} = lim_{n \rightarrow \infty} \inf\{a_k|k \geq n\}$ and in my class we define $t_n =\inf\{a_k|k \geq n\}$. My proof: Suppose that $\underline{\lim}  \{a_n\} = a$ and $\underline{\lim}  \{b_n\} = b$. First consider $\underline{\lim}  \{a_n\} = a$. This means that $ \displaystyle \lim_{n \rightarrow \infty} t_n = a$. So for all $\epsilon > 0$ there exists $N_1 \in \mathbb{N}$ such that when $n \geq N_1$ we have that $ | t_n - a | < \frac{ \epsilon } { 2}$. It follows that $a - \frac{\epsilon}{2} < t_n < a_n$. By a similar argument, there exists $N_2$ such that for all $n \geq N_2$, we have  $b - \frac{\epsilon}{2} < b_n$. So if we choose $N = \max \{N_1, N_2 \}$, for all $n \geq N$ we have $a - \frac{\epsilon}{2} < a_n$ and $b - \frac{\epsilon}{2} < b_n$. Combining these inequalities gives $a_n + b_n > a+b-\epsilon$ Then since $a+b - \epsilon$ is a lower bound for the set $\{ a_n + b_n | n \geq N \}$, we must have $\inf\{a_n + b_n | n \geq N \} \geq a+b- \epsilon$ and $ \displaystyle \lim_{N \rightarrow \infty} \inf\{a_n + b_n | n \geq N \} \geq a+b- \epsilon$      (because the ineq. was true for all $n \geq N$). Then since $\epsilon$ was arbitrary, $ \underline{ lim} (a_n + b_n) \geq a+b$","In my real analysis class, my instructor proved that $\underline{\lim}  \{a_n\}+ \underline{\lim} \{b_n\} \leq     \underline{\lim} (a_n + b_n)$. (Note that $\underline{\lim}$ is the limit inferior of a sequence.) but a lot of details were left out and I want to make sure that I have a correct proof and that my logic is correct. Note that $\underline{lim} \{a_n\} = lim_{n \rightarrow \infty} \inf\{a_k|k \geq n\}$ and in my class we define $t_n =\inf\{a_k|k \geq n\}$. My proof: Suppose that $\underline{\lim}  \{a_n\} = a$ and $\underline{\lim}  \{b_n\} = b$. First consider $\underline{\lim}  \{a_n\} = a$. This means that $ \displaystyle \lim_{n \rightarrow \infty} t_n = a$. So for all $\epsilon > 0$ there exists $N_1 \in \mathbb{N}$ such that when $n \geq N_1$ we have that $ | t_n - a | < \frac{ \epsilon } { 2}$. It follows that $a - \frac{\epsilon}{2} < t_n < a_n$. By a similar argument, there exists $N_2$ such that for all $n \geq N_2$, we have  $b - \frac{\epsilon}{2} < b_n$. So if we choose $N = \max \{N_1, N_2 \}$, for all $n \geq N$ we have $a - \frac{\epsilon}{2} < a_n$ and $b - \frac{\epsilon}{2} < b_n$. Combining these inequalities gives $a_n + b_n > a+b-\epsilon$ Then since $a+b - \epsilon$ is a lower bound for the set $\{ a_n + b_n | n \geq N \}$, we must have $\inf\{a_n + b_n | n \geq N \} \geq a+b- \epsilon$ and $ \displaystyle \lim_{N \rightarrow \infty} \inf\{a_n + b_n | n \geq N \} \geq a+b- \epsilon$      (because the ineq. was true for all $n \geq N$). Then since $\epsilon$ was arbitrary, $ \underline{ lim} (a_n + b_n) \geq a+b$",,"['real-analysis', 'proof-verification', 'inequality', 'limsup-and-liminf']"
89,"Prob. 17, Chap. 3 in Baby Rudin: For $\alpha > 1$, how to obtain these inequalities from this recurrence relation?","Prob. 17, Chap. 3 in Baby Rudin: For , how to obtain these inequalities from this recurrence relation?",\alpha > 1,"Here's Prob. 17, Chap. 3 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Fix $\alpha > 1$. Take $x_1 > \sqrt{\alpha}$, and define $$x_{n+1} = \frac{\alpha + x_n}{1+x_n} = x_n + \frac{\alpha - x_n^2}{1+x_n}.$$ (a) Prove that $x_1 > x_3 > x_5 > \cdots$. (b) Prove that $x_2 < x_4 < x_6 < \cdots$. (c) Prove that $\lim x_n = \sqrt{\alpha}$. My effort: From the recursion formula, we can obtain  $$ \begin{align} x_{n+1} &= \frac{ \alpha + x_n}{1+ x_n} \\ &= \frac{ \alpha + \frac{\alpha + x_{n-1}}{1+x_{n-1}} }{ 1 + \frac{\alpha + x_{n-1}}{1+x_{n-1}} } \\ &= \frac{ (\alpha + 1) x_{n-1} + 2 \alpha   }{ 2x_{n-1} + ( 1 + \alpha ) } \\ &= \frac{\alpha+1}{2} + \frac{2 \alpha - \frac{(\alpha+1)^2}{2} }{2x_{n-1} + ( 1 + \alpha ) } \\  &= \frac{\alpha+1}{2} + \frac{ \alpha - \frac{\alpha^2+1 }{2} }{2x_{n-1} + ( 1 + \alpha ) }.   \end{align} $$ What next?","Here's Prob. 17, Chap. 3 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Fix $\alpha > 1$. Take $x_1 > \sqrt{\alpha}$, and define $$x_{n+1} = \frac{\alpha + x_n}{1+x_n} = x_n + \frac{\alpha - x_n^2}{1+x_n}.$$ (a) Prove that $x_1 > x_3 > x_5 > \cdots$. (b) Prove that $x_2 < x_4 < x_6 < \cdots$. (c) Prove that $\lim x_n = \sqrt{\alpha}$. My effort: From the recursion formula, we can obtain  $$ \begin{align} x_{n+1} &= \frac{ \alpha + x_n}{1+ x_n} \\ &= \frac{ \alpha + \frac{\alpha + x_{n-1}}{1+x_{n-1}} }{ 1 + \frac{\alpha + x_{n-1}}{1+x_{n-1}} } \\ &= \frac{ (\alpha + 1) x_{n-1} + 2 \alpha   }{ 2x_{n-1} + ( 1 + \alpha ) } \\ &= \frac{\alpha+1}{2} + \frac{2 \alpha - \frac{(\alpha+1)^2}{2} }{2x_{n-1} + ( 1 + \alpha ) } \\  &= \frac{\alpha+1}{2} + \frac{ \alpha - \frac{\alpha^2+1 }{2} }{2x_{n-1} + ( 1 + \alpha ) }.   \end{align} $$ What next?",,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence', 'recurrence-relations']"
90,For continuous 1-periodic f. $\int_1^{\infty}\frac{f(x)}{x}dx $ converges.,For continuous 1-periodic f.  converges.,\int_1^{\infty}\frac{f(x)}{x}dx ,"Let $f:\mathbb{R} \rightarrow \mathbb{R}$ is continuous and 1 - periodic. If $ \int_0^1f(x)dx = 0$. Then $$\int_1^{\infty}\frac{f(x)}{x}dx $$ converges. My question is that does a bound on the $\int_1^{\infty}\frac{f(x)}{x}dx$ will imply convergence of the integral? Please justify as well. And if not then how does one show convergence. My Attempt Let $ G(x) = \int_0^xf(x)dx$ then $G'(x)=f(x)$ we have to show that  $\int_1^{\infty}\frac{G'(x)}{x}dx$ converges.  We note that $f(x)$ is bounded on $[0,1]$ say by $M$ and hence on $[0, \infty)$. Then so is $G(x) =\int_0^xf(x)dx= \int_{[x]}^xf(x)dx <M(x- [x])\leq M$. Using integration by parts we have  $$ \left.\frac{G(x)}{x}\right|_1^{\infty} + \int_1^{\infty}\frac{G(x)}{x^2}dx < M(2)$$","Let $f:\mathbb{R} \rightarrow \mathbb{R}$ is continuous and 1 - periodic. If $ \int_0^1f(x)dx = 0$. Then $$\int_1^{\infty}\frac{f(x)}{x}dx $$ converges. My question is that does a bound on the $\int_1^{\infty}\frac{f(x)}{x}dx$ will imply convergence of the integral? Please justify as well. And if not then how does one show convergence. My Attempt Let $ G(x) = \int_0^xf(x)dx$ then $G'(x)=f(x)$ we have to show that  $\int_1^{\infty}\frac{G'(x)}{x}dx$ converges.  We note that $f(x)$ is bounded on $[0,1]$ say by $M$ and hence on $[0, \infty)$. Then so is $G(x) =\int_0^xf(x)dx= \int_{[x]}^xf(x)dx <M(x- [x])\leq M$. Using integration by parts we have  $$ \left.\frac{G(x)}{x}\right|_1^{\infty} + \int_1^{\infty}\frac{G(x)}{x^2}dx < M(2)$$",,"['real-analysis', 'integration', 'convergence-divergence', 'improper-integrals', 'integration-by-parts']"
91,"Limit of recursive sequence defined by $a_0=0$, $a_{n+1}=\frac12\left(a_n+\sqrt{a_n^2+\frac{1}{4^n}}\right)$","Limit of recursive sequence defined by ,",a_0=0 a_{n+1}=\frac12\left(a_n+\sqrt{a_n^2+\frac{1}{4^n}}\right),"Given the following sequence: $a_0=0$, $$a_{n+1}=\frac12\left(a_n+\sqrt{a_n^2+\frac{1}{4^n}}\right),\ \forall n\ge 0.$$  Find $\lim\limits_{n\to\infty}a_n$.","Given the following sequence: $a_0=0$, $$a_{n+1}=\frac12\left(a_n+\sqrt{a_n^2+\frac{1}{4^n}}\right),\ \forall n\ge 0.$$  Find $\lim\limits_{n\to\infty}a_n$.",,"['real-analysis', 'sequences-and-series', 'limits']"
92,"Self Teaching Analysis, Topology and Differential Geometry","Self Teaching Analysis, Topology and Differential Geometry",,"I posted some questions similar to this one not long ago but I think I phrased them wrong and as such got valuable input but not really an answer to my questions. If reposting a similar question like this is against the rules of this forum please tell me! I am still new to this site and eager to learn. Basically I am in electrical engineering (3rd year) but I think I should have done an undergrad in math. I have a limited interest in application and am really much more interested in class when we do rigorous math. At this point I am considering either doing a masters degree in math or going into Control Theory (not control systems, but rather the mathematical theories behind nonlinear control which I guess is more applied math than engineering). What I want to do now is go through textbooks on my own to make up for the butchery of math that happened in my engineering classes. The great thing is that I will have a year-long internship starting in May during which I will have time to dedicate to this. I want to emphasize that although there is a strong chance I will go into Control Theory, I want to cover rigorous math and detach myself almost entirely from engineering. My real question/concern is what textbooks to use for each subject and what order to do it all in. Here is my current plan, I would very much like your input: Real Analysis by Chapman Pugh (I'm already quite deep into it and loving it) Topology by Munkres (Part I: General Topology) Abstract Algebra by Dummit and Foote (Group Theory) Topology by Munkres (Part II: Algebraic Topology) Smooth Manifolds by John M. Lee And then perhaps more of the Abstract Algebra textbook and/or an intro to Dynamical Systems, Chaos and Fractals. Please note I do have some experience in proofs despite engineering. Some of it is due to computer science courses behind quite rigorous and the rest is self practice. For example I am finding Chapman Pugh quite accessible.","I posted some questions similar to this one not long ago but I think I phrased them wrong and as such got valuable input but not really an answer to my questions. If reposting a similar question like this is against the rules of this forum please tell me! I am still new to this site and eager to learn. Basically I am in electrical engineering (3rd year) but I think I should have done an undergrad in math. I have a limited interest in application and am really much more interested in class when we do rigorous math. At this point I am considering either doing a masters degree in math or going into Control Theory (not control systems, but rather the mathematical theories behind nonlinear control which I guess is more applied math than engineering). What I want to do now is go through textbooks on my own to make up for the butchery of math that happened in my engineering classes. The great thing is that I will have a year-long internship starting in May during which I will have time to dedicate to this. I want to emphasize that although there is a strong chance I will go into Control Theory, I want to cover rigorous math and detach myself almost entirely from engineering. My real question/concern is what textbooks to use for each subject and what order to do it all in. Here is my current plan, I would very much like your input: Real Analysis by Chapman Pugh (I'm already quite deep into it and loving it) Topology by Munkres (Part I: General Topology) Abstract Algebra by Dummit and Foote (Group Theory) Topology by Munkres (Part II: Algebraic Topology) Smooth Manifolds by John M. Lee And then perhaps more of the Abstract Algebra textbook and/or an intro to Dynamical Systems, Chaos and Fractals. Please note I do have some experience in proofs despite engineering. Some of it is due to computer science courses behind quite rigorous and the rest is self practice. For example I am finding Chapman Pugh quite accessible.",,"['real-analysis', 'abstract-algebra', 'general-topology', 'differential-topology', 'self-learning']"
93,Why is it necessary to have convexity outside the feasible region?,Why is it necessary to have convexity outside the feasible region?,,"In the book ""convex optimization"" by Prof. Boyd and Prof. Vandenberghe, a convex problem is defined as, $$ \begin{equation}\label{op_prob_gen_equivalent} \begin{aligned} & {\text{minimise}} & &  \  f_0(x) \\ & \text{subject to} & & f_i(x) \leqslant 0 \ , \ \quad \forall i \in \{1,2,...,m\}\\ &&& a_i^Tx=b_i \ , \ \quad \forall i \in \{1,2,...,p\}\\ \end{aligned} \quad , \end{equation} $$ where $f_0(x) \ , \ f_1(x) \ , \ ... \ , \ f_m(x)$ are convex. Let $X$ be the feasible set defined as $X=\{x| x \in \mathbb{R}^n, \ f_i(x) \leqslant 0 \  \forall i =1,2,...,m , \ a_i^Tx=b_i \ , \ \forall i =1,2,...,p \}$. My question is, why is it necessary for $f_0(x) \ , \ f_1(x) \ , \ ... \ , \ f_m(x)$ to be convex outside $X$?","In the book ""convex optimization"" by Prof. Boyd and Prof. Vandenberghe, a convex problem is defined as, $$ \begin{equation}\label{op_prob_gen_equivalent} \begin{aligned} & {\text{minimise}} & &  \  f_0(x) \\ & \text{subject to} & & f_i(x) \leqslant 0 \ , \ \quad \forall i \in \{1,2,...,m\}\\ &&& a_i^Tx=b_i \ , \ \quad \forall i \in \{1,2,...,p\}\\ \end{aligned} \quad , \end{equation} $$ where $f_0(x) \ , \ f_1(x) \ , \ ... \ , \ f_m(x)$ are convex. Let $X$ be the feasible set defined as $X=\{x| x \in \mathbb{R}^n, \ f_i(x) \leqslant 0 \  \forall i =1,2,...,m , \ a_i^Tx=b_i \ , \ \forall i =1,2,...,p \}$. My question is, why is it necessary for $f_0(x) \ , \ f_1(x) \ , \ ... \ , \ f_m(x)$ to be convex outside $X$?",,"['real-analysis', 'convex-optimization']"
94,"Is $C[0,1]$ not complete with the $L^2$ distance?",Is  not complete with the  distance?,"C[0,1] L^2","$$X=C[0,1]\qquad d(f,g)=\left(\int_{0}^{1}\vert f-g \vert^{2}dx\right)^{1/2}$$ The complete metric space is defined that every Cauchy sequence should be convergent on my book, so I think that the main goal is to either find at least one Cauchy sequence that is not convergent or to prove that all Cauchy sequences are convergent. I have completely no idea how to deal with the given distance here, could anybody give me some idea?","$$X=C[0,1]\qquad d(f,g)=\left(\int_{0}^{1}\vert f-g \vert^{2}dx\right)^{1/2}$$ The complete metric space is defined that every Cauchy sequence should be convergent on my book, so I think that the main goal is to either find at least one Cauchy sequence that is not convergent or to prove that all Cauchy sequences are convergent. I have completely no idea how to deal with the given distance here, could anybody give me some idea?",,"['real-analysis', 'cauchy-sequences']"
95,"If $f$ is differentiable on $[1,2]$, then $\exists \alpha\in(1,2): f(2)-f(1) = \frac{\alpha^2}{2}f'(\alpha)$","If  is differentiable on , then","f [1,2] \exists \alpha\in(1,2): f(2)-f(1) = \frac{\alpha^2}{2}f'(\alpha)","If $f$ is differentiable on $[1,2]$, then $\exists \alpha\in(1,2) : f(2)-f(1) = \frac{\alpha^2}{2}f'(\alpha)$ I really would like some hint. I noticed that the equation can be written $$\int_1^2f(x)'\mathbb{d}x = f'(\alpha)\int_0^{\alpha} x\mathbb{d}x$$ EDIT: I confused the theorems. I guess I have to apply the Mean Value Theorem, but I don't know how.","If $f$ is differentiable on $[1,2]$, then $\exists \alpha\in(1,2) : f(2)-f(1) = \frac{\alpha^2}{2}f'(\alpha)$ I really would like some hint. I noticed that the equation can be written $$\int_1^2f(x)'\mathbb{d}x = f'(\alpha)\int_0^{\alpha} x\mathbb{d}x$$ EDIT: I confused the theorems. I guess I have to apply the Mean Value Theorem, but I don't know how.",,"['calculus', 'real-analysis']"
96,Proof about Conjugate and subgradient,Proof about Conjugate and subgradient,,"I am reading the proof on the 15th slides of this link regarding the conjugate and subgradient. $$y \in \partial f(x) \iff x \in \partial f^*(y)$$ http://www.seas.ucla.edu/~vandenbe/236C/lectures/conj.pdf However, I can't really understand the first line of the proof, that is $$\text{If } y \in \partial f(x)\\ f^*(x)=\sup_u(y^Tu-f(u))=y^Tx-f(x)$$ Also, at end the slide mentions that the proof follows from $f^{**}=f$, where did they plug in that property in the proof?","I am reading the proof on the 15th slides of this link regarding the conjugate and subgradient. $$y \in \partial f(x) \iff x \in \partial f^*(y)$$ http://www.seas.ucla.edu/~vandenbe/236C/lectures/conj.pdf However, I can't really understand the first line of the proof, that is $$\text{If } y \in \partial f(x)\\ f^*(x)=\sup_u(y^Tu-f(u))=y^Tx-f(x)$$ Also, at end the slide mentions that the proof follows from $f^{**}=f$, where did they plug in that property in the proof?",,"['real-analysis', 'convex-analysis', 'convex-optimization']"
97,"How does one prove $\int_0^\infty \frac{\log(x)}{1 + e^{ax}} \, dx = -\frac{\log(2)(2\log(a) + \log(2))}{2a}$ for $a > 0$?",How does one prove  for ?,"\int_0^\infty \frac{\log(x)}{1 + e^{ax}} \, dx = -\frac{\log(2)(2\log(a) + \log(2))}{2a} a > 0","Link to WolframAlpha's assertion . Here's my attempt. Using the substitution $t = ax$, we can show the integral is equal to  $$ \frac{1}{a} \int_0^\infty \frac{\log(t)}{1 + e^t}\, dt -\frac{\log(a)}{a} \int_0^\infty \frac{dt}{1 + e^{t} } .$$ The second integral is equal to $\log(2)$ using another substitution $v = e^t$ and partial fractions. So I'm left with the first integral. I'll switch to complex variables for notation. I make two observations: (I) The denominator has simple poles when $z = t = (2k-1)\cdot i \pi, \, k \in \mathbb{N}.$ (II) The numerator has a branch point $z = 0$.","Link to WolframAlpha's assertion . Here's my attempt. Using the substitution $t = ax$, we can show the integral is equal to  $$ \frac{1}{a} \int_0^\infty \frac{\log(t)}{1 + e^t}\, dt -\frac{\log(a)}{a} \int_0^\infty \frac{dt}{1 + e^{t} } .$$ The second integral is equal to $\log(2)$ using another substitution $v = e^t$ and partial fractions. So I'm left with the first integral. I'll switch to complex variables for notation. I make two observations: (I) The denominator has simple poles when $z = t = (2k-1)\cdot i \pi, \, k \in \mathbb{N}.$ (II) The numerator has a branch point $z = 0$.",,"['real-analysis', 'integration', 'complex-analysis', 'definite-integrals', 'power-series']"
98,Analytic solution to definite integral problem,Analytic solution to definite integral problem,,"I am trying to solve an equation including a definite integral. I have a real valued function: $f(a,b,x) = \frac{(1-x)^a}{x}\exp(-b/x)$ Given $a_1, b_1$ and $b_2$, I wish to find $a_2$ from the condition: $\int_0^1 dx\ f(a_1,b_1,x) = \int_0^1 dx\ f(a_2,b_2,x)$. If we are worried about undefined behavior at $x=0$, we can set $f(0) = 0$ always. My best attempt so far is to rewrite the condition as: $\int_0^1 dx\ \ln(f(a_1,b_1,x)) = \int_0^1 dx\ \ln(f(a_2,b_2,x))$, which gives me: $a_2 = a_1 + \int_0^1 dx\ \frac{b_1 - b_2}{x}$. That is divergent, so I will never get a number out of it. I can of course solve it numerically, which I in fact already do. It sits in an optimization routine. But it would be much faster if an analytic answer is possible.","I am trying to solve an equation including a definite integral. I have a real valued function: $f(a,b,x) = \frac{(1-x)^a}{x}\exp(-b/x)$ Given $a_1, b_1$ and $b_2$, I wish to find $a_2$ from the condition: $\int_0^1 dx\ f(a_1,b_1,x) = \int_0^1 dx\ f(a_2,b_2,x)$. If we are worried about undefined behavior at $x=0$, we can set $f(0) = 0$ always. My best attempt so far is to rewrite the condition as: $\int_0^1 dx\ \ln(f(a_1,b_1,x)) = \int_0^1 dx\ \ln(f(a_2,b_2,x))$, which gives me: $a_2 = a_1 + \int_0^1 dx\ \frac{b_1 - b_2}{x}$. That is divergent, so I will never get a number out of it. I can of course solve it numerically, which I in fact already do. It sits in an optimization routine. But it would be much faster if an analytic answer is possible.",,"['calculus', 'real-analysis', 'optimization', 'definite-integrals']"
99,$(a+2)^3+(b+2)^3+(c+2)^3 \ge 81$ while $a+b+c=3$,while,(a+2)^3+(b+2)^3+(c+2)^3 \ge 81 a+b+c=3,"I need to show that $(a+2)^3+(b+2)^3+(c+2)^3 \ge 81$ while $a+b+c=3$ and $a,b,c > 0$ using means of univariate Analysis. It is intuitively clear that $(a+2)^3+(b+2)^3+(c+2)^3$ is at its minimum (when $a+b+c=3$) if $a,b,c$ have ""equal weights"", i.e. $a=b=c=1$. To show that formally one can firstly fix $0<c \le 3$, introduce a variable $0\le\alpha\le1$ and express $a$ and $b$ through $\alpha$ $$a= \alpha(3-c)$$ $$b=(1-\alpha)(3-c)$$ Then we minimize $(a+2)^3+(b+2)^3+(c+2)^3$ w.r.t $\alpha$ treating $c$ as a parameter. We get $\alpha=\frac{1}{2}(3-c)$. Further we maximize $(a+2)^3+(b+2)^3+(c+2)^3$ one more time w.r.t. $c$. That is quite tedious. I am wondering whether their is a more elegant way. Probably using idea of norms. Basically we need to show that $\lVert(a,b,c)+(2,2,2)\rVert_3 \ge (81)^{1/3}$ while $\lVert(a,b,c)\rVert_1=3$ and $a,b,c>0$.","I need to show that $(a+2)^3+(b+2)^3+(c+2)^3 \ge 81$ while $a+b+c=3$ and $a,b,c > 0$ using means of univariate Analysis. It is intuitively clear that $(a+2)^3+(b+2)^3+(c+2)^3$ is at its minimum (when $a+b+c=3$) if $a,b,c$ have ""equal weights"", i.e. $a=b=c=1$. To show that formally one can firstly fix $0<c \le 3$, introduce a variable $0\le\alpha\le1$ and express $a$ and $b$ through $\alpha$ $$a= \alpha(3-c)$$ $$b=(1-\alpha)(3-c)$$ Then we minimize $(a+2)^3+(b+2)^3+(c+2)^3$ w.r.t $\alpha$ treating $c$ as a parameter. We get $\alpha=\frac{1}{2}(3-c)$. Further we maximize $(a+2)^3+(b+2)^3+(c+2)^3$ one more time w.r.t. $c$. That is quite tedious. I am wondering whether their is a more elegant way. Probably using idea of norms. Basically we need to show that $\lVert(a,b,c)+(2,2,2)\rVert_3 \ge (81)^{1/3}$ while $\lVert(a,b,c)\rVert_1=3$ and $a,b,c>0$.",,"['real-analysis', 'inequality', 'optimization', 'jensen-inequality']"
