,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Singular Value Decomposition for zero-diagonal symmetric matrix,Singular Value Decomposition for zero-diagonal symmetric matrix,,"Let's say a zero-diagonal $4\times4$ symmetric matrix,  $$ \begin{bmatrix} 0 & 1 & 3 & 3 \\ 1 & 0 & 3 & 3 \\ 3 & 3 & 0 & 1 \\ 3 & 3 & 1 & 0 \end{bmatrix} $$ Does anyone know how to obtain SVD from the above matrix mathematically? as $A = U W V^*$ Note: eigenvectors of $A^*A$ will make up $V$ with associate eigenvalues of the diagonal of $W^*W$. Similarly, $D^*D = U^*(WW^*)U$ Thank you very much!","Let's say a zero-diagonal $4\times4$ symmetric matrix,  $$ \begin{bmatrix} 0 & 1 & 3 & 3 \\ 1 & 0 & 3 & 3 \\ 3 & 3 & 0 & 1 \\ 3 & 3 & 1 & 0 \end{bmatrix} $$ Does anyone know how to obtain SVD from the above matrix mathematically? as $A = U W V^*$ Note: eigenvectors of $A^*A$ will make up $V$ with associate eigenvalues of the diagonal of $W^*W$. Similarly, $D^*D = U^*(WW^*)U$ Thank you very much!",,"['linear-algebra', 'matrix-decomposition', 'svd']"
1,Any easier and direct method to prove $\|P\|_2=1$ and $\|Px\|\le\|x\|$ for orthogonal projector $P$,Any easier and direct method to prove  and  for orthogonal projector,\|P\|_2=1 \|Px\|\le\|x\| P,I know complicated ways to prove them. But wonder if there is any easier and direct method to prove $\|P\|_2=1$ and $\|Px\|\le\|x\|$ for orthogonal projector $P$ just given the fact $P^T=P$ and $P^2=P$. Please help. Thank you.,I know complicated ways to prove them. But wonder if there is any easier and direct method to prove $\|P\|_2=1$ and $\|Px\|\le\|x\|$ for orthogonal projector $P$ just given the fact $P^T=P$ and $P^2=P$. Please help. Thank you.,,['linear-algebra']
2,How to find the sparsest vector in a given subspace of $\mathbb{F}_2^n$,How to find the sparsest vector in a given subspace of,\mathbb{F}_2^n,A subspace $C$ of $\mathbb{F}_2^n$ is given for some $n \geq 1$. The space $C$ is given by its basis. Is there a polynomial time algorithm to find the (nonzero) vector in $C$ of lowest hamming weight? Motivation : Finding the distance of a given linear code.,A subspace $C$ of $\mathbb{F}_2^n$ is given for some $n \geq 1$. The space $C$ is given by its basis. Is there a polynomial time algorithm to find the (nonzero) vector in $C$ of lowest hamming weight? Motivation : Finding the distance of a given linear code.,,"['linear-algebra', 'reference-request', 'algorithms', 'np-complete', 'coding-theory']"
3,Linear algebra question involving principal submatrices,Linear algebra question involving principal submatrices,,"Let $A$ $\in$ $M_n (C)$. For each $1 \leq i \leq n$, let $A_i$ be the $(n-1)\times(n-1)$ principal submatrix of $A$ resulting from deleting the $i^\text{th}$ row and $i^\text{th}$ column. Prove that for $1 \leq k \leq n-1$, $\sum\limits_{i=1}^n E_{k}(A_{i}) = (n-k)E_{k}(A)$. EDIT: The notation is consistent with Horn and Johnson's Matrix Analysis book. On page 40, it says that there are $\binom{n}{k}$ different k-by-k principal minors of the matrix $A=[a_{ij}]$, and the sum of these is denoted by $E_{k}(A)$ I have been trying different formulas involving the trace and the determinant, but I think I am missing some insight into this question. Any help will be appreciated, thanks.","Let $A$ $\in$ $M_n (C)$. For each $1 \leq i \leq n$, let $A_i$ be the $(n-1)\times(n-1)$ principal submatrix of $A$ resulting from deleting the $i^\text{th}$ row and $i^\text{th}$ column. Prove that for $1 \leq k \leq n-1$, $\sum\limits_{i=1}^n E_{k}(A_{i}) = (n-k)E_{k}(A)$. EDIT: The notation is consistent with Horn and Johnson's Matrix Analysis book. On page 40, it says that there are $\binom{n}{k}$ different k-by-k principal minors of the matrix $A=[a_{ij}]$, and the sum of these is denoted by $E_{k}(A)$ I have been trying different formulas involving the trace and the determinant, but I think I am missing some insight into this question. Any help will be appreciated, thanks.",,"['linear-algebra', 'matrices']"
4,Similar matrices and eigenvalues,Similar matrices and eigenvalues,,"Given two invertible matrices $A,B\in M_{2}(\mathbb R)$ such that $B^{-1}AB=A^{2}$, and $1$ is NOT an eigenvalue of $A$. (1) Find the eigenvalues of A, and (2) Find $A$ and $B$ satisfying the given conditions. What I tried is as follows: since  $B^{-1}AB=A^{2}$, then $A$ is similar to $A^{2}$, so the set of eigenvalues of $A$ and $A^{2}$ coincide. So, $\{\lambda_{1},\lambda_{2}\}=\{\lambda^{2}_{1},\lambda^{2}_{2}\}$, and we have two cases: $\lambda_{1}=\lambda^{2}_{1}, \lambda_{2}=\lambda^{2}_{2}$, so $\lambda_{1}=0,1$, but since $A$ is invertible then $\lambda_{1}\neq 0$, so $\lambda_{1}=1$ !! I'm getting confused here!! $\lambda_{1}=\lambda^{2}_{2}, \lambda_{2}=\lambda^{2}_{1}$, so $\lambda_{1}=\lambda^{4}_{1}$ is a 3rd primitive root of unity. I don't Know how to find such $A$ and $B$, any help?!","Given two invertible matrices $A,B\in M_{2}(\mathbb R)$ such that $B^{-1}AB=A^{2}$, and $1$ is NOT an eigenvalue of $A$. (1) Find the eigenvalues of A, and (2) Find $A$ and $B$ satisfying the given conditions. What I tried is as follows: since  $B^{-1}AB=A^{2}$, then $A$ is similar to $A^{2}$, so the set of eigenvalues of $A$ and $A^{2}$ coincide. So, $\{\lambda_{1},\lambda_{2}\}=\{\lambda^{2}_{1},\lambda^{2}_{2}\}$, and we have two cases: $\lambda_{1}=\lambda^{2}_{1}, \lambda_{2}=\lambda^{2}_{2}$, so $\lambda_{1}=0,1$, but since $A$ is invertible then $\lambda_{1}\neq 0$, so $\lambda_{1}=1$ !! I'm getting confused here!! $\lambda_{1}=\lambda^{2}_{2}, \lambda_{2}=\lambda^{2}_{1}$, so $\lambda_{1}=\lambda^{4}_{1}$ is a 3rd primitive root of unity. I don't Know how to find such $A$ and $B$, any help?!",,['linear-algebra']
5,The form of a solution in a linear system,The form of a solution in a linear system,,"I have this linear system: $\left\{\begin{array}{c} 2x + 3y - 4z = \ 1 \\ 3x -  y - 2z = 2 \\  x - 7y - 6z = 0 \end{array}\right.$ I found the following solution: $\begin{pmatrix} x \\ y \\ z \end{pmatrix} = \begin{pmatrix} \alpha \frac{10}{11}+\frac{7}{11} \\ \alpha \frac{8}{11}-\frac{1}{11} \\ \alpha \end{pmatrix} $ but the correct solution is $\begin{pmatrix} x \\ y \\ z \end{pmatrix} = \begin{pmatrix} 10t+7\\ 8t+5 \\ 11t+7 \end{pmatrix} $ I know that the two solution are equivalent (and correct). Assuming that i don't know the second form, how i can transform the first form into the last form (with only integer coefficents)?","I have this linear system: $\left\{\begin{array}{c} 2x + 3y - 4z = \ 1 \\ 3x -  y - 2z = 2 \\  x - 7y - 6z = 0 \end{array}\right.$ I found the following solution: $\begin{pmatrix} x \\ y \\ z \end{pmatrix} = \begin{pmatrix} \alpha \frac{10}{11}+\frac{7}{11} \\ \alpha \frac{8}{11}-\frac{1}{11} \\ \alpha \end{pmatrix} $ but the correct solution is $\begin{pmatrix} x \\ y \\ z \end{pmatrix} = \begin{pmatrix} 10t+7\\ 8t+5 \\ 11t+7 \end{pmatrix} $ I know that the two solution are equivalent (and correct). Assuming that i don't know the second form, how i can transform the first form into the last form (with only integer coefficents)?",,['linear-algebra']
6,"Linear functional on matrix space, nonnegative on positive semidefinite matrices","Linear functional on matrix space, nonnegative on positive semidefinite matrices",,"Let $f:M_n(\mathbb C) \to \mathbb C$ be a linear function such that $f(x^* x)\ge0$ for all  $x$ and $f(1)=1$. Show that there exist $\alpha_1,...,\alpha_k\in \mathbb C^n$ such that $f(x)=\sum_{i=1}^{k}\langle x\alpha_i,\alpha_i \rangle$ for all $x\in M_n(\mathbb C)$.","Let $f:M_n(\mathbb C) \to \mathbb C$ be a linear function such that $f(x^* x)\ge0$ for all  $x$ and $f(1)=1$. Show that there exist $\alpha_1,...,\alpha_k\in \mathbb C^n$ such that $f(x)=\sum_{i=1}^{k}\langle x\alpha_i,\alpha_i \rangle$ for all $x\in M_n(\mathbb C)$.",,['linear-algebra']
7,the relationship between eigenvectors and matrix multiplication,the relationship between eigenvectors and matrix multiplication,,"If A has eigenvector $\mathbf{v}_1$ so that $A\mathbf{v}_1=\lambda_1\mathbf{v}_1$and B has eignenvector  $\mathbf{v}_2$ so that $B\mathbf{v}_2=\lambda_2\mathbf{v}_2$, then what can you say about AB? can you say $AB\mathbf{v_3}=\lambda_3\mathbf{v}_3$? and what would be the relationship between $\mathbf{v}_1,\mathbf{v}_2,\mathbf{v}_3$ and what would be the relationship between $\lambda_1,\lambda_2,\lambda_3$? Edit $A,B$ are 3 by 3 matrices and $\lambda_1,\lambda_2,\lambda_3$ can be real or complex numbers and $\mathbf{v}_1,\mathbf{v}_2,\mathbf{v}_3$ is a triple.","If A has eigenvector $\mathbf{v}_1$ so that $A\mathbf{v}_1=\lambda_1\mathbf{v}_1$and B has eignenvector  $\mathbf{v}_2$ so that $B\mathbf{v}_2=\lambda_2\mathbf{v}_2$, then what can you say about AB? can you say $AB\mathbf{v_3}=\lambda_3\mathbf{v}_3$? and what would be the relationship between $\mathbf{v}_1,\mathbf{v}_2,\mathbf{v}_3$ and what would be the relationship between $\lambda_1,\lambda_2,\lambda_3$? Edit $A,B$ are 3 by 3 matrices and $\lambda_1,\lambda_2,\lambda_3$ can be real or complex numbers and $\mathbf{v}_1,\mathbf{v}_2,\mathbf{v}_3$ is a triple.",,['linear-algebra']
8,Finding points on two linear lines which are a particular distance apart,Finding points on two linear lines which are a particular distance apart,,"I have two linear, skew, 3D lines, and I was wondering how I could find a points on each of the lines whereby the distance between the two points are a particular distance apart? I'm not after the points where the lines are the closest distance apart, nor the furthest distance apart, nor the point at which the lines cross! I'm after the points on the lines where the two lines are a particular distance apart. I'd like to also be able to change this distance and find the new points. Thanks very much in advance! :)","I have two linear, skew, 3D lines, and I was wondering how I could find a points on each of the lines whereby the distance between the two points are a particular distance apart? I'm not after the points where the lines are the closest distance apart, nor the furthest distance apart, nor the point at which the lines cross! I'm after the points on the lines where the two lines are a particular distance apart. I'd like to also be able to change this distance and find the new points. Thanks very much in advance! :)",,"['linear-algebra', 'geometry']"
9,How can I tell which matrix decomposition to use for OLS?,How can I tell which matrix decomposition to use for OLS?,,"I want to find the least squares solution to $\boldsymbol{Ax}=\boldsymbol{b}$ where $\boldsymbol{A}$ is a highly sparse square matrix. I found two methods that look like they might lead me to a solution: QR factorization , and singular value decomposition . Unfortunately, I haven't taken linear algebra yet, so I can't really understand most of what those pages are saying. I can calculate both in Matlab though, and it looks like the SVD gave me a smaller squared error. Why did that happen? How can I know which one I should be using in the future?","I want to find the least squares solution to $\boldsymbol{Ax}=\boldsymbol{b}$ where $\boldsymbol{A}$ is a highly sparse square matrix. I found two methods that look like they might lead me to a solution: QR factorization , and singular value decomposition . Unfortunately, I haven't taken linear algebra yet, so I can't really understand most of what those pages are saying. I can calculate both in Matlab though, and it looks like the SVD gave me a smaller squared error. Why did that happen? How can I know which one I should be using in the future?",,"['linear-algebra', 'numerical-methods', 'numerical-linear-algebra']"
10,Defining matrix function through its diagonalization,Defining matrix function through its diagonalization,,"Suppose $f:\mathbb{R} \to \mathbb{R}$ is a smooth function. Let $Dg(n)$ be the set of diagonal $n \times n$ matrices, and define $\overline{f}:Dg(n) \to Dg(n)$ in the following way: first, for a diagonal matrix $D=diag(\lambda_1,\dots,\lambda_n)$ , $$\overline{f}(D)=diag(f(\lambda_1),\dots,f(\lambda_n)).$$ For a general diagonalizable matrix $A=PDP^{-1}$ , define $$\overline{f}(A)=P \overline{f}(D) P^{-1}.$$ What are the properties of $\overline{f}$ ? Is it still smooth? Motivation: the idea is to find a different way to show that the exponential matrix and logarithm are smooth functions, which would be a particular case of above when $f=\exp,\log$ respectively.","Suppose is a smooth function. Let be the set of diagonal matrices, and define in the following way: first, for a diagonal matrix , For a general diagonalizable matrix , define What are the properties of ? Is it still smooth? Motivation: the idea is to find a different way to show that the exponential matrix and logarithm are smooth functions, which would be a particular case of above when respectively.","f:\mathbb{R} \to \mathbb{R} Dg(n) n \times n \overline{f}:Dg(n) \to Dg(n) D=diag(\lambda_1,\dots,\lambda_n) \overline{f}(D)=diag(f(\lambda_1),\dots,f(\lambda_n)). A=PDP^{-1} \overline{f}(A)=P \overline{f}(D) P^{-1}. \overline{f} f=\exp,\log","['linear-algebra', 'matrices', 'derivatives', 'continuity']"
11,Rigorous Mathematical foundations of Machine Learning / Deep Learning / Neural Networks,Rigorous Mathematical foundations of Machine Learning / Deep Learning / Neural Networks,,"I am an Engineering Graduate (with a strong background in Probability/Measure Theory, Linear Algebra and Calculus) wanting to dig deep into Deep Learning and Neural Networks, and I'm looking for mathematically thorough and rigurous (almost pedantic) material and books to go through the material, but I don't seem to find something of my liking among all the myriad of hype-train books out there considering how popular Machine Learning has become in the last years. Even the rather non-hands-on books about it seem to be too hands-on to me. I have both Pattern Recognition and Machine Learning by Bishop , which I started working through (about 150 Pages), as well as Deep Learning by Goodfellow (which I only skimmed through), but I am not happy with either. They both seem to try to cover too much material, with little rigorousness and detail, getting lost in specificalities. I am only speaking out of intuition here, but I feel like due to the assumption that the average reader lacks real depth in the areas of Probability, Linear Algebra and Calculus (which is clear by the sloppiness of the use of them), I feel like there's the need for too much explanation. What I'm looking for is a more concise, but more mathematically precise book on the fundamentals of Deep Learning / Machine Learning that assumes a strong and formal knowledge of Linear Algebra, Probability Theory (Kolmogorov, Probability Spaces, etc.), and Calculus where I can draw my own (mathematical) conclusions and without too much specific gibberish (e.g. I don't need 5 examples, graphs and intuitive explanations on the difference between bias and variance, I just need a rigorous definition of both, and a few important Theorems around them which I can work with). My reasoning here is that the field is so complex already and expanding in an exponential manner on so many directions, and understanding concepts like regularization and optimization seems so daunting, because of the lack of mathematical abstractions behind it, so it seems that every application needs its own interpretation. As experience has showed me, this happens when there's a lack of fundamental mathematical knowledge that can help extrapolate into other examples without the explicit need to understand the details of it. An analogous example is say with systems of differential equations: You might need to understand a few examples (e.g. Romeo and Juliet/Love affairs https://ai.stanford.edu/~rajatr/articles/SS_love_dEq.pdf ) but then it suffices to be able to understand those systems as general mathematical constructs and how to solve them/approach them without having to understand intuitively every single example a priori. Thank you","I am an Engineering Graduate (with a strong background in Probability/Measure Theory, Linear Algebra and Calculus) wanting to dig deep into Deep Learning and Neural Networks, and I'm looking for mathematically thorough and rigurous (almost pedantic) material and books to go through the material, but I don't seem to find something of my liking among all the myriad of hype-train books out there considering how popular Machine Learning has become in the last years. Even the rather non-hands-on books about it seem to be too hands-on to me. I have both Pattern Recognition and Machine Learning by Bishop , which I started working through (about 150 Pages), as well as Deep Learning by Goodfellow (which I only skimmed through), but I am not happy with either. They both seem to try to cover too much material, with little rigorousness and detail, getting lost in specificalities. I am only speaking out of intuition here, but I feel like due to the assumption that the average reader lacks real depth in the areas of Probability, Linear Algebra and Calculus (which is clear by the sloppiness of the use of them), I feel like there's the need for too much explanation. What I'm looking for is a more concise, but more mathematically precise book on the fundamentals of Deep Learning / Machine Learning that assumes a strong and formal knowledge of Linear Algebra, Probability Theory (Kolmogorov, Probability Spaces, etc.), and Calculus where I can draw my own (mathematical) conclusions and without too much specific gibberish (e.g. I don't need 5 examples, graphs and intuitive explanations on the difference between bias and variance, I just need a rigorous definition of both, and a few important Theorems around them which I can work with). My reasoning here is that the field is so complex already and expanding in an exponential manner on so many directions, and understanding concepts like regularization and optimization seems so daunting, because of the lack of mathematical abstractions behind it, so it seems that every application needs its own interpretation. As experience has showed me, this happens when there's a lack of fundamental mathematical knowledge that can help extrapolate into other examples without the explicit need to understand the details of it. An analogous example is say with systems of differential equations: You might need to understand a few examples (e.g. Romeo and Juliet/Love affairs https://ai.stanford.edu/~rajatr/articles/SS_love_dEq.pdf ) but then it suffices to be able to understand those systems as general mathematical constructs and how to solve them/approach them without having to understand intuitively every single example a priori. Thank you",,"['linear-algebra', 'probability-theory', 'statistics', 'machine-learning', 'neural-networks']"
12,Coincidence of numbers of solutions of matrix equations $A^2=B^2$ and $AB=0$ of size 2 over finite fields,Coincidence of numbers of solutions of matrix equations  and  of size 2 over finite fields,A^2=B^2 AB=0,"Let ring $R:=M_2(\mathbb F_p)$ , where prime $p\ne 2$ . There are two interesting equations $A^2=B^2$ and $(A+B)(A-B)=0$ , and the numbers of solution pairs $(A,B)\in R^2$ of the equations are denoted $S_1(p),S_2(p)$ . The fun fact is that, I used a computer search and found $S_1(p)=S_2(p)$ for very small $p$ -s. Actually: $$ S_1(3)=S_2(3)=417,\ S_1(5)=S_2(5)=4705,\ S_1(7)=S_2(7)=23233,\ S_1(11)=S_2(11)=202081.  $$ And if we set $R:=M_2(\mathbb Z/p\mathbb Z)$ where $p$ may be a general odd number , we even have $S_1(9)=S_2(9)=123201$ . So $S_1=S_2$ seems to be true in some more general settings. But I can't find any patterns in these numbers $S_i$ by just looking at them or using an OEIS check. Let's name the equations (1) and (2) respectively. For (1), I've tried to use $A=X+Y,B=X-Y$ to rewrite it as follows, $$ (X+Y)^2=(X-Y)^2 \iff 2(XY+YX)=0, $$ since $R$ is not commutative. And by $p$ is odd, we eventually get $ab+ba=0$ . And for (2), a similar argument reduce it to $XY=0$ . But I still can't see how can one relate a solution of $XY+YX=0$ to a solution of $XY=0$ . Is there any magic behind this? Also, there would be no similar results for $M_3(\mathbb F_3)$ , in that case $S_1=221157$ and $S_2=496341$ . Update The above is the original problem. By a careful case-by-case discussion over the trace and rank of one of the matrix $X$ or $Y$ , as suggested by the answer or the comment, we find the two equations $XY+YX=0$ and $XY=0$ both have $p^5+3p^4-2p^3-2p^2+p$ solutions. But there are similar and more shocking phenomena: for a positive integer $n>0$ , the following equation $$ f_n(X,Y)=XY-YX-X^n=0 $$ always seems to have $2p^4-p^2$ solutions no matter what $n$ is, coprime to $p$ or not. This sequence is OEIS $\mathsf A2593$ . But there seems to be nothing related to our problem on that link page. Another Update Thanks to @user1551. For the new equation $f_n(X,Y)=0$ when $n\ge 2$ , there is now some shortcut to see the result by linear algebra. By the fact that $[X,Y]$ commutes with $X$ (since $[X,Y]=X^n$ ), Jacobson's Lemma implies that $[X,Y]$ is nilpotent and $X$ is nilpotent too under our field characteristic condition. Because $X\in R$ is a $2\times 2$ nilpotent matrix, we have $X^n=0$ when $n\ge 2$ . So solution sets for $n\ge 2$ are the same since $f_n(X,Y)=0$ iff $[X,Y]=0$ and $X$ is nilpotent. When $n=1$ , we know now $[X,Y]=X$ is nilpotent. Notice that this equation has at least one solution $Y_0$ for each given nilpotent $X$ (consider $X$ to be a Jordan form and construct $Y$ directly). After $Y\mapsto Y+Y_0$ , equation $[X,Y_0+Y]=X$ becomes $[X,Y]=0$ . This build a correspondence between the $n=1$ case and the $n\ge 2$ case.","Let ring , where prime . There are two interesting equations and , and the numbers of solution pairs of the equations are denoted . The fun fact is that, I used a computer search and found for very small -s. Actually: And if we set where may be a general odd number , we even have . So seems to be true in some more general settings. But I can't find any patterns in these numbers by just looking at them or using an OEIS check. Let's name the equations (1) and (2) respectively. For (1), I've tried to use to rewrite it as follows, since is not commutative. And by is odd, we eventually get . And for (2), a similar argument reduce it to . But I still can't see how can one relate a solution of to a solution of . Is there any magic behind this? Also, there would be no similar results for , in that case and . Update The above is the original problem. By a careful case-by-case discussion over the trace and rank of one of the matrix or , as suggested by the answer or the comment, we find the two equations and both have solutions. But there are similar and more shocking phenomena: for a positive integer , the following equation always seems to have solutions no matter what is, coprime to or not. This sequence is OEIS . But there seems to be nothing related to our problem on that link page. Another Update Thanks to @user1551. For the new equation when , there is now some shortcut to see the result by linear algebra. By the fact that commutes with (since ), Jacobson's Lemma implies that is nilpotent and is nilpotent too under our field characteristic condition. Because is a nilpotent matrix, we have when . So solution sets for are the same since iff and is nilpotent. When , we know now is nilpotent. Notice that this equation has at least one solution for each given nilpotent (consider to be a Jordan form and construct directly). After , equation becomes . This build a correspondence between the case and the case.","R:=M_2(\mathbb F_p) p\ne 2 A^2=B^2 (A+B)(A-B)=0 (A,B)\in R^2 S_1(p),S_2(p) S_1(p)=S_2(p) p 
S_1(3)=S_2(3)=417,\ S_1(5)=S_2(5)=4705,\ S_1(7)=S_2(7)=23233,\ S_1(11)=S_2(11)=202081. 
 R:=M_2(\mathbb Z/p\mathbb Z) p S_1(9)=S_2(9)=123201 S_1=S_2 S_i A=X+Y,B=X-Y  (X+Y)^2=(X-Y)^2 \iff 2(XY+YX)=0,  R p ab+ba=0 XY=0 XY+YX=0 XY=0 M_3(\mathbb F_3) S_1=221157 S_2=496341 X Y XY+YX=0 XY=0 p^5+3p^4-2p^3-2p^2+p n>0 
f_n(X,Y)=XY-YX-X^n=0
 2p^4-p^2 n p \mathsf A2593 f_n(X,Y)=0 n\ge 2 [X,Y] X [X,Y]=X^n [X,Y] X X\in R 2\times 2 X^n=0 n\ge 2 n\ge 2 f_n(X,Y)=0 [X,Y]=0 X n=1 [X,Y]=X Y_0 X X Y Y\mapsto Y+Y_0 [X,Y_0+Y]=X [X,Y]=0 n=1 n\ge 2","['linear-algebra', 'number-theory', 'finite-fields', 'matrix-equations']"
13,Finding orthogonal polynomials,Finding orthogonal polynomials,,"I have the vector space $C^0([0,1],ℝ)$ of continious functions from 0 to 1, with the inner product $$ \langle f , g \rangle =  \int_{0}^{1} f(x)g(x) \,dx	$$ I have to find all the first degree polynomials $g(x)=ax+b$ that are orthogonal on $f(x)=6x$ , which means that $\langle f , g \rangle = 0$ . I have solved it like this $$\langle f,g\rangle = \int_{0}^{1} f(x)g(x) \,dx = \int_{0}^{1} (6x)(ax+b) \,dx = 6a \int_{0}^{1} x^2 \,dx   + 6b\int_{0}^{1} x \,dx  $$ $$ = 6a \left[ \frac{x^3}{3} \right]^1_0 + 6b\left[ \frac{1}{2}\right]^1_0 = 6a \cdot \frac{1}{3} + 3b = 2a+3b $$ $$ 2a+3b = 0$$ $$ a =- \frac{3b}{2}$$ Therefore $g(x)=ax+b$ $$g(x) =- \frac{3b}{2}x+b = b(-\frac{3}{2}x+1)$$ Is this solution correct?","I have the vector space of continious functions from 0 to 1, with the inner product I have to find all the first degree polynomials that are orthogonal on , which means that . I have solved it like this Therefore Is this solution correct?","C^0([0,1],ℝ)  \langle f , g \rangle =  \int_{0}^{1} f(x)g(x) \,dx	 g(x)=ax+b f(x)=6x \langle f , g \rangle = 0 \langle f,g\rangle = \int_{0}^{1} f(x)g(x) \,dx = \int_{0}^{1} (6x)(ax+b) \,dx = 6a \int_{0}^{1} x^2 \,dx   + 6b\int_{0}^{1} x \,dx   
= 6a \left[ \frac{x^3}{3} \right]^1_0 + 6b\left[ \frac{1}{2}\right]^1_0 = 6a \cdot \frac{1}{3} + 3b = 2a+3b
  2a+3b = 0  a =- \frac{3b}{2} g(x)=ax+b g(x) =- \frac{3b}{2}x+b = b(-\frac{3}{2}x+1)","['linear-algebra', 'linear-transformations']"
14,"Show that if $A \in M_{2 \times 2}(\mathbb{R})$ then $A, A^T$ are similar.",Show that if  then  are similar.,"A \in M_{2 \times 2}(\mathbb{R}) A, A^T","Show that if $A \in M_{2 \times 2}(\mathbb{R})$ then $A, A^T$ are similar. We say that two matrices $A, B \in M_{n}$ are similar if there exists an invertible matrix $P \in M_{n}$ s.t $A = P^{-1}BP$ I was asked to decide wether this is true or false during an exam at algebra. Thing is, this exam was for algebra 1 and we still haven't started to learn about eigenvalues (so the notion of similar matrices was introduced very briefly so far). I know that this is true for $n$ , but I probably was expected to have enough knowledge to solve it for the case where $n=2$ . I'm struggling to see how this can be proved with basic knowledge about similarity. I hope to get some ideas, thanks","Show that if then are similar. We say that two matrices are similar if there exists an invertible matrix s.t I was asked to decide wether this is true or false during an exam at algebra. Thing is, this exam was for algebra 1 and we still haven't started to learn about eigenvalues (so the notion of similar matrices was introduced very briefly so far). I know that this is true for , but I probably was expected to have enough knowledge to solve it for the case where . I'm struggling to see how this can be proved with basic knowledge about similarity. I hope to get some ideas, thanks","A \in M_{2 \times 2}(\mathbb{R}) A, A^T A, B \in M_{n} P \in M_{n} A = P^{-1}BP n n=2","['linear-algebra', 'abstract-algebra', 'alternative-proof', 'similar-matrices']"
15,Trace convergence of finite dimensional truncations?,Trace convergence of finite dimensional truncations?,,"Suppose $P, Q$ are two positive, self-adjoint trace-class operators on $\ell^2$ . Let $V_k \colon \ell^2 \to \mathbb{R}^k$ denote the projection onto the first $k$ coordinates. Suppose also that $Q$ is strictly positive in the sense that $\langle Qx, x \rangle > 0$ for all $x \neq 0$ . Consider the sequence of operators: $$ P_k = V_k P V_k^\ast, \quad \mbox{and} \quad Q_k = V_k Q V_k^\ast,  \quad \mbox{for}~k \geq 1.   $$ I am wondering if it is true that $$ \lim_{k \to \infty} \mathrm{tr}(Q_k (P_k Q_k + I)^{-1}) = \mathrm{tr}(Q (P Q + I)^{-1}).  $$ The operators $I$ above are (slightly abusing notation) the identity on $\mathbb{R}^k$ and $\ell^2$ , respectively. In the case where $P, Q$ are diagonal I can see that this is true, but otherwise I am unsure.","Suppose are two positive, self-adjoint trace-class operators on . Let denote the projection onto the first coordinates. Suppose also that is strictly positive in the sense that for all . Consider the sequence of operators: I am wondering if it is true that The operators above are (slightly abusing notation) the identity on and , respectively. In the case where are diagonal I can see that this is true, but otherwise I am unsure.","P, Q \ell^2 V_k \colon \ell^2 \to \mathbb{R}^k k Q \langle Qx, x \rangle > 0 x \neq 0 
P_k = V_k P V_k^\ast, \quad \mbox{and} \quad Q_k = V_k Q V_k^\ast, 
\quad \mbox{for}~k \geq 1.  
 
\lim_{k \to \infty} \mathrm{tr}(Q_k (P_k Q_k + I)^{-1}) = \mathrm{tr}(Q (P Q + I)^{-1}). 
 I \mathbb{R}^k \ell^2 P, Q","['linear-algebra', 'operator-theory']"
16,Coefficients of a symmetric product of polynomials with root of unity,Coefficients of a symmetric product of polynomials with root of unity,,"For number $n\ge2$ , let $\xi$ be a primitive $n$ -th root of unity. The determinant of circulant matrix is a symmetric polynomial in $x_0,\dots,x_{n-1}$ $$f_n=\prod_{j=0}^{n-1}\sum_{i=0}^{n-1}ξ^{ij}x_i$$ so after expansion, all coefficients are integer. Is the following true? For prime number $n$ , all coefficients of $$f_n-\sum_{i=0}^{n-1}x_i^n$$ are divisible by $n$ . Using SageMath I verified it for $n=2,3,5,7$ . For $n=5$ : from sympy import symbols x = symbols('x_:5')  f5 = prod(sum(exp(I*2*pi/5*j*i) * x[i] for i in (0..4)) for j in (0..4)) poly = expand(f5- sum(x[i]**5 for i in (0..4))).maxima_methods().rootscontract() output: $-5x_{0} x_{1}^{3} x_{2} + 5x_{0}^{2} x_{1} x_{2}^{2} + 5x_{0}^{2} x_{1}^{2} x_{3} - 5x_{0}^{3} x_{2} x_{3} - 5x_{1} x_{2}^{3} x_{3} + 5x_{1}^{2} x_{2} x_{3}^{2} + 5x_{0} x_{2}^{2} x_{3}^{2} - 5x_{0} x_{1} x_{3}^{3} - 5x_{0}^{3} x_{1} x_{4} + 5x_{1}^{2} x_{2}^{2} x_{4} - 5x_{0} x_{2}^{3} x_{4} - 5x_{1}^{3} x_{3} x_{4} - 5x_{0} x_{1} x_{2} x_{3} x_{4} + 5x_{0}^{2} x_{3}^{2} x_{4} - 5x_{2} x_{3}^{3} x_{4} + 5x_{0} x_{1}^{2} x_{4}^{2} + 5x_{0}^{2} x_{2} x_{4}^{2} + 5x_{2}^{2} x_{3} x_{4}^{2} + 5x_{1} x_{3}^{2} x_{4}^{2} - 5x_{1} x_{2} x_{4}^{3} - 5x_{0} x_{3} x_{4}^{3}$ poly/5 in ZZ[x] output: True","For number , let be a primitive -th root of unity. The determinant of circulant matrix is a symmetric polynomial in so after expansion, all coefficients are integer. Is the following true? For prime number , all coefficients of are divisible by . Using SageMath I verified it for . For : from sympy import symbols x = symbols('x_:5')  f5 = prod(sum(exp(I*2*pi/5*j*i) * x[i] for i in (0..4)) for j in (0..4)) poly = expand(f5- sum(x[i]**5 for i in (0..4))).maxima_methods().rootscontract() output: $-5x_{0} x_{1}^{3} x_{2} + 5x_{0}^{2} x_{1} x_{2}^{2} + 5x_{0}^{2} x_{1}^{2} x_{3} - 5x_{0}^{3} x_{2} x_{3} - 5x_{1} x_{2}^{3} x_{3} + 5x_{1}^{2} x_{2} x_{3}^{2} + 5x_{0} x_{2}^{2} x_{3}^{2} - 5x_{0} x_{1} x_{3}^{3} - 5x_{0}^{3} x_{1} x_{4} + 5x_{1}^{2} x_{2}^{2} x_{4} - 5x_{0} x_{2}^{3} x_{4} - 5x_{1}^{3} x_{3} x_{4} - 5x_{0} x_{1} x_{2} x_{3} x_{4} + 5x_{0}^{2} x_{3}^{2} x_{4} - 5x_{2} x_{3}^{3} x_{4} + 5x_{0} x_{1}^{2} x_{4}^{2} + 5x_{0}^{2} x_{2} x_{4}^{2} + 5x_{2}^{2} x_{3} x_{4}^{2} + 5x_{1} x_{3}^{2} x_{4}^{2} - 5x_{1} x_{2} x_{4}^{3} - 5x_{0} x_{3} x_{4}^{3}$ poly/5 in ZZ[x] output: True","n\ge2 \xi n x_0,\dots,x_{n-1} f_n=\prod_{j=0}^{n-1}\sum_{i=0}^{n-1}ξ^{ij}x_i n f_n-\sum_{i=0}^{n-1}x_i^n n n=2,3,5,7 n=5","['linear-algebra', 'symmetric-polynomials', 'roots-of-unity', 'circulant-matrices']"
17,Spectral theorem for diagonal matrix in different inner product spaces,Spectral theorem for diagonal matrix in different inner product spaces,,"I learned a special case of the spectral theorem for finite dimensional inner product space. As I understand it states that a real matrix is orthogonally diagonalizable with real eigenvalues iff it equal its hermitian adjoint. However when I try to apply it to diagonal matrices, I think I have a problem. If I work with the regular inner product ( $\langle u,v\rangle \to u^tv)$ then I see how this works out ( $A^* = A^t = A$ ), but if I work in a more general inner product space: $\langle u,v\rangle = (Bu)^t(Bv)$ where $B$ is an invertiable matrix, then the hermitian adjoint of a matrix $A$ becomes $A^* = (B^tB)^{-1}A^t(B^tB)$ , and this may not commute with $A$ , if $A$ is some general diagonal matrix, for example let's define $A = \begin{pmatrix}1 & 0\\0 & -1\end{pmatrix}, B = \begin{pmatrix}1 & 1\\0 & 1\end{pmatrix}$ . However we still have that for the identity matrix: $I^* = I$ so a diagonal matrix, is still orthogonally diagonalizable. How this doesn't contradict the spectral theorem?","I learned a special case of the spectral theorem for finite dimensional inner product space. As I understand it states that a real matrix is orthogonally diagonalizable with real eigenvalues iff it equal its hermitian adjoint. However when I try to apply it to diagonal matrices, I think I have a problem. If I work with the regular inner product ( then I see how this works out ( ), but if I work in a more general inner product space: where is an invertiable matrix, then the hermitian adjoint of a matrix becomes , and this may not commute with , if is some general diagonal matrix, for example let's define . However we still have that for the identity matrix: so a diagonal matrix, is still orthogonally diagonalizable. How this doesn't contradict the spectral theorem?","\langle u,v\rangle \to u^tv) A^* = A^t = A \langle u,v\rangle = (Bu)^t(Bv) B A A^* = (B^tB)^{-1}A^t(B^tB) A A A = \begin{pmatrix}1 & 0\\0 & -1\end{pmatrix}, B = \begin{pmatrix}1 & 1\\0 & 1\end{pmatrix} I^* = I","['linear-algebra', 'spectral-theory', 'orthogonality']"
18,Find all idempotent matrices such that $(A-B)^2 = 0$,Find all idempotent matrices such that,(A-B)^2 = 0,"Find all idempotent matrices such that $(A-B)^2 = 0$ We can see that the hypotheses imply that $A+B=AB+BA$ , and if we multiply by $AB$ on the right, we get $AB+BAB=(AB)^2+BAB$ , which also implies that $AB$ is idempotent (and we can also see that $BA$ is idempotent). After that, I am not sure what to do next. I add a necessary condition that $\operatorname{rank} (A) = \operatorname{rank} (B)$ , since $A-B$ is nilpotent, then $\operatorname{Trace} (A-B) = 0$ , so $\operatorname{Trace} (A) = \operatorname{Trace} (B)$ . But $A$ and $B$ are projectors, so $\operatorname{rank} (A) = \operatorname{rank} (B)$ . additional sufficient condition: if $\text{im}(A) = \text{im}(B)$ , then $AB=B$ and $BA=A$ . This implies that $A+B=AB+BA$ , which further implies that $(A-B)^2=0$ .","Find all idempotent matrices such that We can see that the hypotheses imply that , and if we multiply by on the right, we get , which also implies that is idempotent (and we can also see that is idempotent). After that, I am not sure what to do next. I add a necessary condition that , since is nilpotent, then , so . But and are projectors, so . additional sufficient condition: if , then and . This implies that , which further implies that .",(A-B)^2 = 0 A+B=AB+BA AB AB+BAB=(AB)^2+BAB AB BA \operatorname{rank} (A) = \operatorname{rank} (B) A-B \operatorname{Trace} (A-B) = 0 \operatorname{Trace} (A) = \operatorname{Trace} (B) A B \operatorname{rank} (A) = \operatorname{rank} (B) \text{im}(A) = \text{im}(B) AB=B BA=A A+B=AB+BA (A-B)^2=0,"['linear-algebra', 'matrices', 'matrix-equations', 'idempotents']"
19,"$V=\left \{ C|\space C\in M_{q\times r}(R)\space such \space that\space ACB=O \right \} $, find the dimension of the vector space V",", find the dimension of the vector space V",V=\left \{ C|\space C\in M_{q\times r}(R)\space such \space that\space ACB=O \right \} ,"Let $A$ be a $p\times q$ matrix of rank $\alpha$ and $B$ a $r\times s$ matrix of rank $\beta$ .  Let $V=\left \{ C|\space C\in M_{q\times r}(\mathbb{R}) \text{ and } ACB=0_{p\times s}  \right \} $ , find the dimension of the vector space V. Can I solve this by the following...... (1)the range space of B is the domain of C, where rank(B)= $\beta$ (2)the range space of C covers the whole null space of A, where nullity(A)=q-rank(A)=q- $\alpha$ Hence the dimension of C need not to be q $\times r$ , it only takes (q- $\alpha)\times \beta$ in necessary. please give some opinion please!","Let be a matrix of rank and a matrix of rank .  Let , find the dimension of the vector space V. Can I solve this by the following...... (1)the range space of B is the domain of C, where rank(B)= (2)the range space of C covers the whole null space of A, where nullity(A)=q-rank(A)=q- Hence the dimension of C need not to be q , it only takes (q- in necessary. please give some opinion please!",A p\times q \alpha B r\times s \beta V=\left \{ C|\space C\in M_{q\times r}(\mathbb{R}) \text{ and } ACB=0_{p\times s}  \right \}  \beta \alpha \times r \alpha)\times \beta,"['linear-algebra', 'vector-spaces', 'matrix-rank']"
20,Are these Hankel matrices positive semidefinite?,Are these Hankel matrices positive semidefinite?,,"While working on a quantum information project, I encountered the following two Hankel matrices $$ a_{i,j} = (i+j)!(2n-(i+j))! ,\qquad b_{i,j} = (i+j+1)!(2n-(i+j))! $$ where $0 \le i,j \le n$ and $!$ denotes the factorial. I would like to know if the matrices are positive semidefinite for any integer $n \geq 2$ . Their positive semidefiniteness is related to the quantum entanglement of some systems. Computing the eigenvalues via Mathematica up to $n \approx 30$ seems to affirm that these matrices are indeed positive definite. How would I prove it for all $n$ ?","While working on a quantum information project, I encountered the following two Hankel matrices where and denotes the factorial. I would like to know if the matrices are positive semidefinite for any integer . Their positive semidefiniteness is related to the quantum entanglement of some systems. Computing the eigenvalues via Mathematica up to seems to affirm that these matrices are indeed positive definite. How would I prove it for all ?"," a_{i,j} = (i+j)!(2n-(i+j))! ,\qquad b_{i,j} = (i+j+1)!(2n-(i+j))!  0 \le i,j \le n ! n \geq 2 n \approx 30 n","['linear-algebra', 'combinatorics', 'matrices', 'positive-semidefinite', 'hankel-matrices']"
21,"Showing $f(t)=\text{Tr}[A(I-A)^{2t}]\approx\frac{\sqrt{\pi/2}\,\text{erfc}(\sqrt{2t}\,/d)}{2\sqrt{t}}$, where $A=\text{diag}(1,1/4,1/9,\ldots,1/d^2)$","Showing , where","f(t)=\text{Tr}[A(I-A)^{2t}]\approx\frac{\sqrt{\pi/2}\,\text{erfc}(\sqrt{2t}\,/d)}{2\sqrt{t}} A=\text{diag}(1,1/4,1/9,\ldots,1/d^2)","Suppose $h=\left(1,\frac{1}{4},\frac{1}{9},\ldots,\frac{1}{d^2}\right)$ and $A=\operatorname{diag}(h)$ What's the easiest way of deriving the following expression in terms of Erf function? $$f(t)=\operatorname{Tr}\left[A(I-A)^{2t}\right] \approx \frac{\sqrt{\frac{\pi }{2}} \left(1-\text{erf}\left(\frac{\sqrt{2} \sqrt{t}}{d}\right)\right)}{2 \sqrt{t}} $$ The following tests goodness of fit against true values of $f(t)$ for fixed $d$ or fixed $t$ Notebook Motivation: $f(t)$ gives loss after $t$ steps of gradient descent minimizing $y=x_1^2+\frac{1}{4}x_2^2+\frac{1}{9}x_3^2 + \ldots + \frac{1}{d^2}x_d^2$",Suppose and What's the easiest way of deriving the following expression in terms of Erf function? The following tests goodness of fit against true values of for fixed or fixed Notebook Motivation: gives loss after steps of gradient descent minimizing,"h=\left(1,\frac{1}{4},\frac{1}{9},\ldots,\frac{1}{d^2}\right) A=\operatorname{diag}(h) f(t)=\operatorname{Tr}\left[A(I-A)^{2t}\right] \approx \frac{\sqrt{\frac{\pi }{2}} \left(1-\text{erf}\left(\frac{\sqrt{2} \sqrt{t}}{d}\right)\right)}{2 \sqrt{t}}  f(t) d t f(t) t y=x_1^2+\frac{1}{4}x_2^2+\frac{1}{9}x_3^2 + \ldots + \frac{1}{d^2}x_d^2","['calculus', 'linear-algebra']"
22,The boundedness of $L_1$ norm $\|(I+A)^{-1}\|_1$ if both $\|A\|_1$ and $\|A^{-1}\|_1$ are bounded,The boundedness of  norm  if both  and  are bounded,L_1 \|(I+A)^{-1}\|_1 \|A\|_1 \|A^{-1}\|_1,"Assume that $A \in \mathbb{R}^{N \times N}$ is a positive semi-definite matrix, both $\|A\|_1$ and $\|A^{-1}\|_1$ are uniformly bounded as $N \to \infty.$ Here $\| \cdot \|_1$ is the induced $L_1$ Norm. Prove that there exists a constant $\kappa > 0$ such that $$\|(I_N+A)^{-1}\|_1 \leq \kappa $$ as $N \to \infty.$ Else show a counterexample. I attempted to demonstrate that the boundedness of $\|(I_N+A)^{-1}\|_1$ can be proven using the definition of $\| \cdot \|_1$ as follows $$ \|(I_N + A)^{-1} \|_1 = \sup_{x \neq 0} \frac{ \|(I_N + A)^{-1} x\|_1 }{\|x\|_1} = \sup_{z \neq 0} \frac{ \|z\|_1 }{\|(I_N+A)z\|_1} \leq \sup_{z \neq 0} \frac{ \|z\|_1 }{|\|Az\|_1 - \|z\|_1|} = \sup_{z \neq 0} \frac{1}{ \Big| \frac{\|Az\|_1}{\|z\|_1} - 1 \Big| } $$ However, it seems possible that $\inf_{z} \Big|\frac{\|Az\|_1}{\|z\|_1} - 1 \Big|=0$ , so I doubt this proof is wrong. But intuitively, $(I_N + A)^{-1}$ seems to be smaller than $A^{-1}$ , and when we consider $L_2$ norm, it is easy to find that $\|(I_N + A)^{-1}\|_2 \leq 1$ , so I guess the boundedness still holds when we consider $L_1$ norm. Any guidance on how to tackle this problem? Thanks a ton!","Assume that is a positive semi-definite matrix, both and are uniformly bounded as Here is the induced Norm. Prove that there exists a constant such that as Else show a counterexample. I attempted to demonstrate that the boundedness of can be proven using the definition of as follows However, it seems possible that , so I doubt this proof is wrong. But intuitively, seems to be smaller than , and when we consider norm, it is easy to find that , so I guess the boundedness still holds when we consider norm. Any guidance on how to tackle this problem? Thanks a ton!",A \in \mathbb{R}^{N \times N} \|A\|_1 \|A^{-1}\|_1 N \to \infty. \| \cdot \|_1 L_1 \kappa > 0 \|(I_N+A)^{-1}\|_1 \leq \kappa  N \to \infty. \|(I_N+A)^{-1}\|_1 \| \cdot \|_1  \|(I_N + A)^{-1} \|_1 = \sup_{x \neq 0} \frac{ \|(I_N + A)^{-1} x\|_1 }{\|x\|_1} = \sup_{z \neq 0} \frac{ \|z\|_1 }{\|(I_N+A)z\|_1} \leq \sup_{z \neq 0} \frac{ \|z\|_1 }{|\|Az\|_1 - \|z\|_1|} = \sup_{z \neq 0} \frac{1}{ \Big| \frac{\|Az\|_1}{\|z\|_1} - 1 \Big| }  \inf_{z} \Big|\frac{\|Az\|_1}{\|z\|_1} - 1 \Big|=0 (I_N + A)^{-1} A^{-1} L_2 \|(I_N + A)^{-1}\|_2 \leq 1 L_1,['linear-algebra']
23,"How do we get $ J_{f \circ \psi} (x) = J_\psi(x) J_f(y) $ for $f$ on a submanifold, and $\psi$ is local coordinates?","How do we get  for  on a submanifold, and  is local coordinates?", J_{f \circ \psi} (x) = J_\psi(x) J_f(y)  f \psi,"The following is taken from Leon Simon Geometric Measure Theory: Let $f: M \to \mathbb{R}^P$ for $P \geq n$ . Where $M$ is an $n$ dimensional smooth submanifold of $\mathbb{R}^{n+l}$ , and $f$ is locally Lipschitz. For a tangent $\tau \in T_y M$ , we define the directional derivative $D_\tau f \in \mathbb{R}^P$ by $$ D_\tau f = \frac{d}{dt} f(\gamma(t))|_{t=0}$$ $\gamma: (-1,1) \to M$ is a $C^1$ curve, with $\gamma(0) = y$ and $\dot{\gamma}(0) = \tau$ . We go to local coordinates. Fix $y \in M$ . Let $$\varphi : U \cap M \to V $$ where $U \in \mathbb{R}^{n+l}$ and $V \in \mathbb{R}^n$ . Take the inverse $$ \psi:V \to M \cap U.$$ Apply Radamacher's Theorem (which says a Lipschitz function is differentiable almost everywhere) to $f \circ \psi: V \subset \mathbb{R^n} \to \mathbb{R}^P$ , so that there is $E_0 \in V$ with $H^n(E_0) = 0$ and $f \circ \psi$ is differentiable in $V \setminus E_0$ . So for any $\eta \in \mathbb{R}^n$ , $x \in V \setminus E_0$ , $$ D_\eta(f \circ \psi)(x) = \frac{d}{dt} f ( \psi(x + t \eta))|_{t=0} $$ exists and is linear in $\eta$ . But $\psi(x + t \eta)$ is a curve as in the definition above, so letting (by chain rule) $\tau = \dot{\psi} = \sum_{j=1}^n \eta_j D_j \psi(x)$ , we have that $$ \tag{*}D_\tau f(\psi(x))) = D_\eta(f \circ \psi )(x) $$ exists for all $\psi(x) \in U\cap M \setminus \psi(E_0)$ . $H^n(\psi(E_0))= 0$ because $\psi $ is locally Lipschitz on $V$ . Letting $\eta = e_i$ , so that the curve is $\psi(x + te_i)$ in $(*)$ and $\tau_1, ... \tau_n$ an orthonormal basis for $T_y M$ , we have that $\tau = D_i \psi(x)$ , and $$ D_i \psi(x) = \sum_{l=1}^n (D_i \psi(x) \cdot \tau_j) \tau_l $$ $$ D_i(f \circ \psi) (x) = \sum_{k=1}^n D_{\tau_k} f(y)( D_i \psi(x) \cdot \tau_k ) $$ $$\tag{**} D_i(f \circ \psi) \cdot D_j(f \circ \psi) = \sum_{k,m = 1}^n (D_i \psi \cdot \tau_k)(D_j \psi \cdot \tau_m)   D_{\tau_k}f(y) \cdot D_{\tau_m} f(y) $$ I understand so far . I dont get how the next few lines follow from the previous computations: Since $\det AB = \det A \det B$ for square matrices $A,B$ and $$ \sum_{k=1}^n D_i \psi(x) \cdot \tau_k D_j \psi(x) \cdot \tau_k = D_i \psi(x) \cdot D_j \psi(x) $$ this implies $$ J_{f \circ \psi} (x) = J_\psi(x) J_f(y) .$$ where $J_{f \circ \psi}(x)= \sqrt{\det(D_i (f \circ \psi)(x) \cdot D_j (f \circ \psi)(x))}$ , $J_\psi(x) = \sqrt{\det(D_i(\psi(x)) \cdot D_j(\psi(x))}$ and $J_f(y) = \sqrt{\det G(y)}$ where $G(y)$ is the $n \times n$ matrix with $(D_{\tau_k} f(y) \cdot D_{\tau_m} f(y))$ in the $k$ th row and $m$ th column. END SIMON I understand that equation $(**)$ is supposed to somehow lead us to matrix multiplication, and then we can take determinants etc. But I really dont understand how it all fits together, how the statements lead to the final conlusion. Where does the double sum in $k,m$ go? and I dont see how the matrix multiplication comes about. Can someone spell out the details?","The following is taken from Leon Simon Geometric Measure Theory: Let for . Where is an dimensional smooth submanifold of , and is locally Lipschitz. For a tangent , we define the directional derivative by is a curve, with and . We go to local coordinates. Fix . Let where and . Take the inverse Apply Radamacher's Theorem (which says a Lipschitz function is differentiable almost everywhere) to , so that there is with and is differentiable in . So for any , , exists and is linear in . But is a curve as in the definition above, so letting (by chain rule) , we have that exists for all . because is locally Lipschitz on . Letting , so that the curve is in and an orthonormal basis for , we have that , and I understand so far . I dont get how the next few lines follow from the previous computations: Since for square matrices and this implies where , and where is the matrix with in the th row and th column. END SIMON I understand that equation is supposed to somehow lead us to matrix multiplication, and then we can take determinants etc. But I really dont understand how it all fits together, how the statements lead to the final conlusion. Where does the double sum in go? and I dont see how the matrix multiplication comes about. Can someone spell out the details?","f: M \to \mathbb{R}^P P \geq n M n \mathbb{R}^{n+l} f \tau \in T_y M D_\tau f \in \mathbb{R}^P  D_\tau f = \frac{d}{dt} f(\gamma(t))|_{t=0} \gamma: (-1,1) \to M C^1 \gamma(0) = y \dot{\gamma}(0) = \tau y \in M \varphi : U \cap M \to V  U \in \mathbb{R}^{n+l} V \in \mathbb{R}^n  \psi:V \to M \cap U. f \circ \psi: V \subset \mathbb{R^n} \to \mathbb{R}^P E_0 \in V H^n(E_0) = 0 f \circ \psi V \setminus E_0 \eta \in \mathbb{R}^n x \in V \setminus E_0  D_\eta(f \circ \psi)(x) = \frac{d}{dt} f ( \psi(x + t \eta))|_{t=0}  \eta \psi(x + t \eta) \tau = \dot{\psi} = \sum_{j=1}^n \eta_j D_j \psi(x)  \tag{*}D_\tau f(\psi(x))) = D_\eta(f \circ \psi )(x)  \psi(x) \in U\cap M \setminus \psi(E_0) H^n(\psi(E_0))= 0 \psi  V \eta = e_i \psi(x + te_i) (*) \tau_1, ... \tau_n T_y M \tau = D_i \psi(x)  D_i \psi(x) = \sum_{l=1}^n (D_i \psi(x) \cdot \tau_j) \tau_l   D_i(f \circ \psi) (x) = \sum_{k=1}^n D_{\tau_k} f(y)( D_i \psi(x) \cdot \tau_k )  \tag{**} D_i(f \circ \psi) \cdot D_j(f \circ \psi) = \sum_{k,m = 1}^n (D_i \psi \cdot \tau_k)(D_j \psi \cdot \tau_m)   D_{\tau_k}f(y) \cdot D_{\tau_m} f(y)  \det AB = \det A \det B A,B  \sum_{k=1}^n D_i \psi(x) \cdot \tau_k D_j \psi(x) \cdot \tau_k = D_i \psi(x) \cdot D_j \psi(x)   J_{f \circ \psi} (x) = J_\psi(x) J_f(y) . J_{f \circ \psi}(x)= \sqrt{\det(D_i (f \circ \psi)(x) \cdot D_j (f \circ \psi)(x))} J_\psi(x) = \sqrt{\det(D_i(\psi(x)) \cdot D_j(\psi(x))} J_f(y) = \sqrt{\det G(y)} G(y) n \times n (D_{\tau_k} f(y) \cdot D_{\tau_m} f(y)) k m (**) k,m","['linear-algebra', 'differential-geometry', 'geometric-measure-theory', 'submanifold']"
24,Best rank-$1$ approximation of matrix with condition.,Best rank- approximation of matrix with condition.,1,"Let $A\in\mathbb R^{m\times n}$ be a real matrix. For any $x,y\in\mathbb R^m$ , we write $x\leq y$ if $x_i\leq y_i$ for $i=1,\dots,m$ . For any matrix $B$ , $\| B \|_F$ is the Frobenius norm and is defined by $\| B \|_F^2 := \operatorname{Tr}\left(B^T B\right)$ , recall that the trace has the cyclic property $\operatorname{Tr}(AB) = \operatorname{Tr}(BA)$ and that the trace of a $1 \times 1$ matrix is the only value in that matrix. I am trying to find a numerical method to solve the following optimization problem \begin{align*} \min_{\substack{u\in\mathbb R^m,v\in\mathbb R^n\\ 0\leq Av}} \left\| A - u v^T \right\|_F^2 \end{align*} For practical purpose, we have $m\ll n$ , for instance $m$ would be of the order of hundreds and $n$ would typically be more than millions. I am expecting that, similarly to the PCA iterative computation procedure , we could get a runtime of order $O(m^2n)$ . To a lesser extent I am also interested in anything related to the case $n<m$ . This is trying to find the best rank- $1$ approximation of $A$ with some constraint on the rank- $1$ approximation. This problem is not convex but is biconvex on a convex subset of $\mathbb R^m\times \mathbb R^n$ , therefore my first idea was to alternate between optimization of $u$ and $v$ . Observe that we can write \begin{align*} \| A - uv^T \|_F^2 &= \|A\|_F^2 + \mathrm{Tr} (vu^Tuv^T) - 2 \mathrm{Tr}(v u^T A)\\ &= \|A\|_F^2 + v^Tvu^Tu - 2 u^T A v\\ &= \| A \|_F^2+\|u\|^2\cdot \|v\|^2 - 2 u^T A v \end{align*} Differentiating w.r.t. $u$ gives $2\|v\|^2u-2Av$ and w.r.t. $v$ gives $2\|u\|^2 v - 2 A^T u$ . For $v$ we need to compute the Lagragian and the KKT conditions (which are sufficient for a fixed $u$ but not in general), they are (with $\mu\in\mathbb R^m$ being the vector of Lagrange multipliers : \begin{cases} 2\|u\|^2 v - 2 A^T u-A^T\mu=0\\ 0\leq Av\\ 0\leq \mu\\ \mu^TAv = 0 \end{cases} So it is necessary that we have $\| v \|^2u=Av$ and the previous conditions. I am not able to solve the system of KKT conditions for $v$ , but solving it's Wolfe dual problem might be easier, it is given by \begin{align*} \max_{v,\mu} \| u \|^2\cdot \| v\|^2-2 u^TAv-\mu^TAv \end{align*} under the constraint $2\| u\|^2\cdot v-2A^Tu-A^T\mu=0$ and $\mu\geq 0$ , solving for $v$ and then replacing in the optimization expression gives after some simplifications \begin{align*} \max_{\mu\geq 0} -(2u+\mu)^TM(2u+\mu) \end{align*} Therefore solving for $\mu$ is equivalent to solving $\min_{\mu\geq 0} (2u+\mu)M(2u+\mu)$ . I made a second post about this here because it is much simpler and this post is begining to be dirty, I will clean up everything when a solution is found. Let $A^\dagger\in\mathbb R^{n\times m}$ be the Moore-Penrose pseudoinverse of $A$ , in particular $A^\dagger A$ and $A A^\dagger$ are the respective projections onto the span of $A^T$ and $A$ . Here are some facts I could obtain : Fact 1: In the solution $u,v$ to the original problem, $uv^T = 0$ if and only if $$\operatorname{span}(A) \cap [0,\infty[^m = \{ 0\}$$ Proof : If $uv^T\neq0$ , then by the KKT conditions $0\leq u=\frac{Av}{\|\ v\|^2}\in\mathrm{span}(A)\cap [0,\infty[^m$ , but $u\neq 0$ . If $0\neq a\in \mathrm{span}(A)\cap[0,\infty[^m$ , then $b=A^\dagger a \neq 0$ and \begin{align*} \left\| A-\frac{ab^T}{\|b\|^2} \right\|_F^2 &= \| A \|_F^2+\frac{\|a\|^2\cdot \|b\|^2}{\| b\|^4} - 2 \frac{a^T A A^\dagger a}{\| b\|^2} \\ &=\| A \|_F^2+\frac{\|a\|^2}{\| b\|^2} - 2 \frac{\| a\|^2}{\| b\|^2} \\ &=\| A \|_F^2 - \frac{\| a\|^2}{\| b\|^2} \\ &<\| A \|_F^2 \end{align*} Therefore $uv^T\neq 0$ . Fact 2: If $uv^T\neq 0$ then $u\in \mathrm{span}(A)$ and $v\in\mathrm{span}(A^T)$ , furthermore $u=A\frac{v}{\| v\|^2}$ and $v = A^T\frac{2u+\mu}{2\|u\|^2}$ . Proof : Trivial from KKT conditions. Fact 3: If $a$ , $\sigma$ , $b$ is a singular triple of $A$ with $0\leq a$ , then $u=a$ and $v=\sigma b$ satisfies all KKT conditions. Proof : $2\| v\|^2u-2Av = 2\sigma^2 a- 2 \sigma^2a=0$ , select $\mu=0$ to get $2\|u\|^2 v - 2A^Tu=2\sigma b-2\sigma b=0$ , $0\leq \sigma^2 u = Av$ .","Let be a real matrix. For any , we write if for . For any matrix , is the Frobenius norm and is defined by , recall that the trace has the cyclic property and that the trace of a matrix is the only value in that matrix. I am trying to find a numerical method to solve the following optimization problem For practical purpose, we have , for instance would be of the order of hundreds and would typically be more than millions. I am expecting that, similarly to the PCA iterative computation procedure , we could get a runtime of order . To a lesser extent I am also interested in anything related to the case . This is trying to find the best rank- approximation of with some constraint on the rank- approximation. This problem is not convex but is biconvex on a convex subset of , therefore my first idea was to alternate between optimization of and . Observe that we can write Differentiating w.r.t. gives and w.r.t. gives . For we need to compute the Lagragian and the KKT conditions (which are sufficient for a fixed but not in general), they are (with being the vector of Lagrange multipliers : So it is necessary that we have and the previous conditions. I am not able to solve the system of KKT conditions for , but solving it's Wolfe dual problem might be easier, it is given by under the constraint and , solving for and then replacing in the optimization expression gives after some simplifications Therefore solving for is equivalent to solving . I made a second post about this here because it is much simpler and this post is begining to be dirty, I will clean up everything when a solution is found. Let be the Moore-Penrose pseudoinverse of , in particular and are the respective projections onto the span of and . Here are some facts I could obtain : Fact 1: In the solution to the original problem, if and only if Proof : If , then by the KKT conditions , but . If , then and Therefore . Fact 2: If then and , furthermore and . Proof : Trivial from KKT conditions. Fact 3: If , , is a singular triple of with , then and satisfies all KKT conditions. Proof : , select to get , .","A\in\mathbb R^{m\times n} x,y\in\mathbb R^m x\leq y x_i\leq y_i i=1,\dots,m B \| B \|_F \| B \|_F^2 := \operatorname{Tr}\left(B^T B\right) \operatorname{Tr}(AB) = \operatorname{Tr}(BA) 1 \times 1 \begin{align*}
\min_{\substack{u\in\mathbb R^m,v\in\mathbb R^n\\ 0\leq Av}} \left\| A - u v^T \right\|_F^2
\end{align*} m\ll n m n O(m^2n) n<m 1 A 1 \mathbb R^m\times \mathbb R^n u v \begin{align*}
\| A - uv^T \|_F^2 &= \|A\|_F^2 + \mathrm{Tr} (vu^Tuv^T) - 2 \mathrm{Tr}(v u^T A)\\
&= \|A\|_F^2 + v^Tvu^Tu - 2 u^T A v\\
&= \| A \|_F^2+\|u\|^2\cdot \|v\|^2 - 2 u^T A v
\end{align*} u 2\|v\|^2u-2Av v 2\|u\|^2 v - 2 A^T u v u \mu\in\mathbb R^m \begin{cases}
2\|u\|^2 v - 2 A^T u-A^T\mu=0\\
0\leq Av\\
0\leq \mu\\
\mu^TAv = 0
\end{cases} \| v \|^2u=Av v \begin{align*}
\max_{v,\mu} \| u \|^2\cdot \| v\|^2-2 u^TAv-\mu^TAv
\end{align*} 2\| u\|^2\cdot v-2A^Tu-A^T\mu=0 \mu\geq 0 v \begin{align*}
\max_{\mu\geq 0} -(2u+\mu)^TM(2u+\mu)
\end{align*} \mu \min_{\mu\geq 0} (2u+\mu)M(2u+\mu) A^\dagger\in\mathbb R^{n\times m} A A^\dagger A A A^\dagger A^T A u,v uv^T = 0 \operatorname{span}(A) \cap [0,\infty[^m = \{ 0\} uv^T\neq0 0\leq u=\frac{Av}{\|\ v\|^2}\in\mathrm{span}(A)\cap [0,\infty[^m u\neq 0 0\neq a\in \mathrm{span}(A)\cap[0,\infty[^m b=A^\dagger a \neq 0 \begin{align*}
\left\| A-\frac{ab^T}{\|b\|^2} \right\|_F^2 &= \| A \|_F^2+\frac{\|a\|^2\cdot \|b\|^2}{\| b\|^4} - 2 \frac{a^T A A^\dagger a}{\| b\|^2} \\
&=\| A \|_F^2+\frac{\|a\|^2}{\| b\|^2} - 2 \frac{\| a\|^2}{\| b\|^2} \\
&=\| A \|_F^2 - \frac{\| a\|^2}{\| b\|^2} \\
&<\| A \|_F^2
\end{align*} uv^T\neq 0 uv^T\neq 0 u\in \mathrm{span}(A) v\in\mathrm{span}(A^T) u=A\frac{v}{\| v\|^2} v = A^T\frac{2u+\mu}{2\|u\|^2} a \sigma b A 0\leq a u=a v=\sigma b 2\| v\|^2u-2Av = 2\sigma^2 a- 2 \sigma^2a=0 \mu=0 2\|u\|^2 v - 2A^Tu=2\sigma b-2\sigma b=0 0\leq \sigma^2 u = Av","['linear-algebra', 'optimization', 'convex-optimization', 'matrix-rank', 'karush-kuhn-tucker']"
25,Gaussian Elimination - where did I go wrong?,Gaussian Elimination - where did I go wrong?,,"I have just learned about Gaussian Elimination and I decided to try an example question. I was trying to solve a question and I realised later that I had copied the question wrong but I still decided to proceed and solve the system of linear equations. However, when I checked my answer online using an online calculator it gave different values meaning my solution was wrong. I've checked my working out and I can't seem to figure out where I've gone wrong. I'd appreciate if someone could help me figure out my mistake. I've used an online tool to convert my writing into LATEX but it's kind of messed up. I have no idea how to write in LATEX.  As a result, I have attached images as well: Working Out Page 1 Working Out Page 2 Also, I'd appreciate any suggestions on how to improve my work. (Next time, I'm just going to pivot on the 3 to avoid fractions.) Here it is: Q) $$ \begin{aligned} & 3 x-2 y-4 z=3 \\ & 2 x+3 y+3 z=15 \\ & 5 x-3 y+z=14 \\ & {\left[\begin{array}{rrr|r} 3^* & -2 & -4 & 3 \\ 2 & 3 & 3 & 15 \\ 5 & -3 & 1 & 14 \end{array}\right] \quad R_1 \times \frac{1}{3}=R_1} \end{aligned} $$ $$ \begin{aligned} & {\left[\begin{array}{ccc|c} 1 & -\frac{2}{3} & -\frac{4}{3} & 1 \\ 2 & 3 & 3 & 15 \\ 5 & -3 & 1 & 14 \end{array}\right] R_2-2 R_1 \rightarrow R_2} \\ & {\left[\begin{array}{ccc|c} 1 & -\frac{2}{3} & -4 / 3 & 1 \\ 0 & 3 / 3 & 17 / 3 & 13 \\ 5 & -3 & 1 & 14 \end{array}\right] R_3-5 R_1 \rightarrow R_3} \end{aligned} $$ $$ \left[\begin{array}{ccc|c} 1 & -2 / 3 & -4 / 3 & 1 \\ 0 & 13 / 3 & 17 / 3 & 13 \\ 0 & 1 / 3 & 23 / 3 & 9 \end{array}\right] \text { } $$ \begin{aligned} & {\left[\begin{array}{ccc|c} 1 & -2 / 3 & -4 / 3 & 1 \\ 0 & 1 & 17 / 13 & 3 \\ 0 & 1 / 3 & 23 / 3 & 9 \end{array}\right] \quad \begin{array}{l} R_2 \times 3 / 13 \\ R_3-1 / 3 R_2 \rightarrow R_3 \end{array}} \\ & {\left[\begin{array}{ccc|c} 1 & 0 & -6 / 13 & 1 \\ 0 & 1 & 17 / 13 & 3 \\ 0 & 0 & 94 / 13 & 11 \end{array}\right] \quad R_1+2 / 3 R_2 \rightarrow R_1} \\ & {\left[\begin{array}{ccc|c|} 1 & 0 & -6 / 1 & 1 \\ 0 & 1 & 17 / 13 & 3 \\ 0 & 0 & 1 & 143 / 94 \end{array}\right] \quad \begin{array}{l} R_1+\frac{6 R_3}{13} \rightarrow R_1 \\ R_2-\frac{17 R_3}{3} \rightarrow R_2 \end{array}} \\ & {\left[\begin{array} { l l l | l }  { 1 } & { 0 } & { 0 } & { 8 0 / 4 7 } \\ { 0 } & { 1 } & { 0 } & { 9 5 / 9 4 } \\ { 0 } & { 0 } & { 1 } & { 1 4 3 / 9 4 } \end{array} \quad \left[x=\frac{165}{47} \quad y=\frac{73}{47} z=\frac{52}{47}\right.\right.} \\ & \end{aligned}","I have just learned about Gaussian Elimination and I decided to try an example question. I was trying to solve a question and I realised later that I had copied the question wrong but I still decided to proceed and solve the system of linear equations. However, when I checked my answer online using an online calculator it gave different values meaning my solution was wrong. I've checked my working out and I can't seem to figure out where I've gone wrong. I'd appreciate if someone could help me figure out my mistake. I've used an online tool to convert my writing into LATEX but it's kind of messed up. I have no idea how to write in LATEX.  As a result, I have attached images as well: Working Out Page 1 Working Out Page 2 Also, I'd appreciate any suggestions on how to improve my work. (Next time, I'm just going to pivot on the 3 to avoid fractions.) Here it is: Q)","
\begin{aligned}
& 3 x-2 y-4 z=3 \\
& 2 x+3 y+3 z=15 \\
& 5 x-3 y+z=14 \\
& {\left[\begin{array}{rrr|r}
3^* & -2 & -4 & 3 \\
2 & 3 & 3 & 15 \\
5 & -3 & 1 & 14
\end{array}\right] \quad R_1 \times \frac{1}{3}=R_1}
\end{aligned}
 
\begin{aligned}
& {\left[\begin{array}{ccc|c}
1 & -\frac{2}{3} & -\frac{4}{3} & 1 \\
2 & 3 & 3 & 15 \\
5 & -3 & 1 & 14
\end{array}\right] R_2-2 R_1 \rightarrow R_2} \\
& {\left[\begin{array}{ccc|c}
1 & -\frac{2}{3} & -4 / 3 & 1 \\
0 & 3 / 3 & 17 / 3 & 13 \\
5 & -3 & 1 & 14
\end{array}\right] R_3-5 R_1 \rightarrow R_3}
\end{aligned}
 
\left[\begin{array}{ccc|c}
1 & -2 / 3 & -4 / 3 & 1 \\
0 & 13 / 3 & 17 / 3 & 13 \\
0 & 1 / 3 & 23 / 3 & 9
\end{array}\right] \text { }
 \begin{aligned}
& {\left[\begin{array}{ccc|c}
1 & -2 / 3 & -4 / 3 & 1 \\
0 & 1 & 17 / 13 & 3 \\
0 & 1 / 3 & 23 / 3 & 9
\end{array}\right] \quad \begin{array}{l}
R_2 \times 3 / 13 \\
R_3-1 / 3 R_2 \rightarrow R_3
\end{array}} \\
& {\left[\begin{array}{ccc|c}
1 & 0 & -6 / 13 & 1 \\
0 & 1 & 17 / 13 & 3 \\
0 & 0 & 94 / 13 & 11
\end{array}\right] \quad R_1+2 / 3 R_2 \rightarrow R_1} \\
& {\left[\begin{array}{ccc|c|}
1 & 0 & -6 / 1 & 1 \\
0 & 1 & 17 / 13 & 3 \\
0 & 0 & 1 & 143 / 94
\end{array}\right] \quad \begin{array}{l}
R_1+\frac{6 R_3}{13} \rightarrow R_1 \\
R_2-\frac{17 R_3}{3} \rightarrow R_2
\end{array}} \\
& {\left[\begin{array} { l l l | l } 
{ 1 } & { 0 } & { 0 } & { 8 0 / 4 7 } \\
{ 0 } & { 1 } & { 0 } & { 9 5 / 9 4 } \\
{ 0 } & { 0 } & { 1 } & { 1 4 3 / 9 4 }
\end{array} \quad \left[x=\frac{165}{47} \quad y=\frac{73}{47} z=\frac{52}{47}\right.\right.} \\
&
\end{aligned}","['linear-algebra', 'matrices', 'gaussian-elimination']"
26,Inner products on $M_n(F)$ inducing the same norm on all matrices of rank one,Inner products on  inducing the same norm on all matrices of rank one,M_n(F),"This question is related to exercise 11 on page 310 of Hoffman and Kunze's Linear Algebra book. Let $F:=\mathbb R/\mathbb C$ . Let $\langle \ , \ \rangle_1,\langle \ , \ \rangle_2$ be two inner products on $M_n(F)$ with induced norms $\lVert \ \rVert_1,\lVert \ \rVert_2$ , respectively. If $\lVert A\rVert_1=\lVert A\rVert_2$ for all $n\times n$ matrices $A$ of rank one, does it follow that $\langle \ , \ \rangle_1=\langle \ , \ \rangle_2$ ?","This question is related to exercise 11 on page 310 of Hoffman and Kunze's Linear Algebra book. Let . Let be two inner products on with induced norms , respectively. If for all matrices of rank one, does it follow that ?","F:=\mathbb R/\mathbb C \langle \ , \ \rangle_1,\langle \ , \ \rangle_2 M_n(F) \lVert \ \rVert_1,\lVert \ \rVert_2 \lVert A\rVert_1=\lVert A\rVert_2 n\times n A \langle \ , \ \rangle_1=\langle \ , \ \rangle_2","['linear-algebra', 'matrices', 'inner-products']"
27,Can a self-map of a vector space preserve a nondegenerate bilinear form without being linear?,Can a self-map of a vector space preserve a nondegenerate bilinear form without being linear?,,"Let $V$ be an infinite-dimensional vector space over $\mathbb{R}$ . Let $B:V\times V\rightarrow \mathbb{R}$ be a nondegenerate bilinear form, i.e., a bilinear form satisfying $$ B(x,y)=0,\;\forall y\in V \Rightarrow x=0.$$ Suppose $\phi:V\rightarrow V$ is a self-map satisfying $B(x,y)=B(\phi x, \phi y)$ for all $x,y\in V$ . My question is this: Is $\phi$ necessarily linear? If ""no"", what's an example? The answer is yes if $\phi$ is surjective, by an argument I will share momentarily. What I want to know is if there exists a non-surjective nonlinear $\phi$ and a $B$ satisfying the above. The question is motivated by a passage in Peter Lax's book Functional Analysis , which gives a proof that relies on this assumption. On p. 59, Lax writes the following. (Hopefully the meaning of Lax's notation can be inferred from context, but just in case: $H$ is a Hilbert space and there is a fixed self-map of $H$ for which the image of $x\in H$ is denoted by $x'$ , and similarly for other variables.) ""We stated in chapter 5 that every isometry of a Banach space onto itself that maps $0$ into $0$ is linear. We give now a new proof of this in Hilbert space: [... here Lax verifies that an isometry fixing the origin preserves the Hilbert space inner product...] Now denote $x+y$ by $z$ , and let $u$ be any vector in $H$ . Using [the preservation of the inner product] we have $$ (z',u')=(z,u) = (x+y,u) = (x,u) + (y,u) = (x', u') + (y',u') = (x'+y',u'). $$ Thus $$(z'-x'-y',u')=0$$ for all $u'$ . This can be only if $z'=x'+y'$ . The virtue of this proof is that it applies even when the scalar product is not positive, as long as it is nondegenerate, meaning that no $u$ is orthogonal to all points. My question arose from making sense of Lax's final comment here. If the scalar product is not positive, then it doesn't induce a metric, so it becomes unclear to me what it should mean for the metric to be complete, and therefore unclear to me how to interpret the notion of ""Hilbert space"" in this broader context. On the other hand, the argument makes no use of completeness, and the metric structure only enters through the inner product, so presumably the scope of Lax's final comment above is really just spaces equipped with a nondegenerate bilinear form (as in the question above). This led me to a question about what Lax wants to replace the isometry with in this broader context. He defines an isometry as (distance-preserving and) surjective, and implicitly uses this in the argument above because prima facie, ""for all $u'$ "" means ""for all $\phi u$ with $u\in H$ ""; this only ranges across all of $H$ if $\phi$ surjects. So the claim that results from a straightforward generalization of the above argument is that if $\phi$ preserves $B$ and is surjective onto $V$ , then $\phi$ is linear. So this made me curious: what happens if $\phi$ is not surjective?","Let be an infinite-dimensional vector space over . Let be a nondegenerate bilinear form, i.e., a bilinear form satisfying Suppose is a self-map satisfying for all . My question is this: Is necessarily linear? If ""no"", what's an example? The answer is yes if is surjective, by an argument I will share momentarily. What I want to know is if there exists a non-surjective nonlinear and a satisfying the above. The question is motivated by a passage in Peter Lax's book Functional Analysis , which gives a proof that relies on this assumption. On p. 59, Lax writes the following. (Hopefully the meaning of Lax's notation can be inferred from context, but just in case: is a Hilbert space and there is a fixed self-map of for which the image of is denoted by , and similarly for other variables.) ""We stated in chapter 5 that every isometry of a Banach space onto itself that maps into is linear. We give now a new proof of this in Hilbert space: [... here Lax verifies that an isometry fixing the origin preserves the Hilbert space inner product...] Now denote by , and let be any vector in . Using [the preservation of the inner product] we have Thus for all . This can be only if . The virtue of this proof is that it applies even when the scalar product is not positive, as long as it is nondegenerate, meaning that no is orthogonal to all points. My question arose from making sense of Lax's final comment here. If the scalar product is not positive, then it doesn't induce a metric, so it becomes unclear to me what it should mean for the metric to be complete, and therefore unclear to me how to interpret the notion of ""Hilbert space"" in this broader context. On the other hand, the argument makes no use of completeness, and the metric structure only enters through the inner product, so presumably the scope of Lax's final comment above is really just spaces equipped with a nondegenerate bilinear form (as in the question above). This led me to a question about what Lax wants to replace the isometry with in this broader context. He defines an isometry as (distance-preserving and) surjective, and implicitly uses this in the argument above because prima facie, ""for all "" means ""for all with ""; this only ranges across all of if surjects. So the claim that results from a straightforward generalization of the above argument is that if preserves and is surjective onto , then is linear. So this made me curious: what happens if is not surjective?","V \mathbb{R} B:V\times V\rightarrow \mathbb{R}  B(x,y)=0,\;\forall y\in V \Rightarrow x=0. \phi:V\rightarrow V B(x,y)=B(\phi x, \phi y) x,y\in V \phi \phi \phi B H H x\in H x' 0 0 x+y z u H  (z',u')=(z,u) = (x+y,u) = (x,u) + (y,u) = (x', u') + (y',u') = (x'+y',u').  (z'-x'-y',u')=0 u' z'=x'+y' u u' \phi u u\in H H \phi \phi B V \phi \phi","['linear-algebra', 'functional-analysis']"
28,How can I construct a basis of $\mathbb{R}^n$ with this property?,How can I construct a basis of  with this property?,\mathbb{R}^n,"Given $\theta \in (0,\pi/2]$ I would like to find a basis of unit length vectors $\{v_1, \dots, v_n\}$ for $\mathbb{R}^n$ such that each $v_i$ has angle $\theta$ from the subspace spanned by the remaining vectors. Are there any references for something like this? A (recursive) family of examples would be great but even some algorithmic ideas would be helpful. For $n = 2$ it is trivial, take any unit length vector and rotate it by $\theta$ . For $n = 3$ I am already struggling. I have an ugly numeric approach that can find me one solution, however I am having a hard time generalizing it to dimensions $n > 3$ . (I'm happy to share more details but it's not clever at all). The properties of such a basis also means the pairwise angles between the $v_i$ will be the same, but for $n > 2$ it is larger than $\theta$ and (I imagine) continues to increase with the dimension. Knowing this angle in advance would also make constructing examples of these bases by hand much easier.","Given I would like to find a basis of unit length vectors for such that each has angle from the subspace spanned by the remaining vectors. Are there any references for something like this? A (recursive) family of examples would be great but even some algorithmic ideas would be helpful. For it is trivial, take any unit length vector and rotate it by . For I am already struggling. I have an ugly numeric approach that can find me one solution, however I am having a hard time generalizing it to dimensions . (I'm happy to share more details but it's not clever at all). The properties of such a basis also means the pairwise angles between the will be the same, but for it is larger than and (I imagine) continues to increase with the dimension. Knowing this angle in advance would also make constructing examples of these bases by hand much easier.","\theta \in (0,\pi/2] \{v_1, \dots, v_n\} \mathbb{R}^n v_i \theta n = 2 \theta n = 3 n > 3 v_i n > 2 \theta","['linear-algebra', 'vector-spaces']"
29,Solve for eigenvector as rational function of eigenvalue,Solve for eigenvector as rational function of eigenvalue,,"Let $A$ be an $n$ by $n$ matrix with entries in $\{0,1\}$ , and such that some power $A^k$ is strictly positive, so Perron-Frobenius stuff applies, i.e. there is a real top eigenvalue $\lambda$ with multiplicity $1$ . Let $v$ be a corresponding eigenvector, $Av = \lambda v$ . Q: Is it always possible to write $v$ as a quotient of polynomials in $\lambda?$ If so, is there a simple procedure to determine those rational functions? I'm also wondering if the tools to do this in Python (or Sage perhaps?) already exist, i.e. given the matrix $A$ , spit out an explicit expression for the eigenvector $v$ in terms of $\lambda$ . A bit of guessing works for small examples, e.g.: $A = \begin{bmatrix} 1 & 1 & 0 \\ 1 & 0 & 1 \\ 1 & 0 & 0 \end{bmatrix}$ has characteristic polynomial $x^3 - x^2 - x - 1$ , with top eigenvalue $\lambda \approx 1.839$ , and with a bit of elbow grease, $v = \begin{bmatrix} 1 \\ \lambda - 1 \\ \lambda^{-1} \end{bmatrix}$ is a corresponding (right) eigenvector. I want to be able to do the same thing in general. I'm particularly interested in the $8$ by $8$ matrix $\begin{bmatrix} 1& 1& 0& 0& 0& 0& 0& 0 \\ 0& 1& 1& 0& 0& 0& 0& 0 \\ 0& 1& 0& 1& 0& 0& 0& 0 \\ 1& 0& 0& 0& 1& 0& 0& 0 \\ 0& 0& 1& 0& 0& 1& 0& 0 \\ 0& 1& 0& 0& 0& 0& 1& 0 \\ 0& 0& 0& 1& 0& 0& 0& 1 \\ 0& 1& 0& 0& 0& 0& 0& 0 \end{bmatrix}$ . If someone can handle this guy, even with ad-hoc ideas, it would be helpful! Edit: Ok, the 8x8 matrix isn't too hard to do by hand. So it's not so enlightening regarding a general method. Edit 2: To clarify, I'm looking for a formula or method that's simpler than Gaussian elimination or the like. There are sometimes clever methods for these sorts of things -- the 'cover up' method for partial fractions comes to mind.","Let be an by matrix with entries in , and such that some power is strictly positive, so Perron-Frobenius stuff applies, i.e. there is a real top eigenvalue with multiplicity . Let be a corresponding eigenvector, . Q: Is it always possible to write as a quotient of polynomials in If so, is there a simple procedure to determine those rational functions? I'm also wondering if the tools to do this in Python (or Sage perhaps?) already exist, i.e. given the matrix , spit out an explicit expression for the eigenvector in terms of . A bit of guessing works for small examples, e.g.: has characteristic polynomial , with top eigenvalue , and with a bit of elbow grease, is a corresponding (right) eigenvector. I want to be able to do the same thing in general. I'm particularly interested in the by matrix . If someone can handle this guy, even with ad-hoc ideas, it would be helpful! Edit: Ok, the 8x8 matrix isn't too hard to do by hand. So it's not so enlightening regarding a general method. Edit 2: To clarify, I'm looking for a formula or method that's simpler than Gaussian elimination or the like. There are sometimes clever methods for these sorts of things -- the 'cover up' method for partial fractions comes to mind.","A n n \{0,1\} A^k \lambda 1 v Av = \lambda v v \lambda? A v \lambda A = \begin{bmatrix} 1 & 1 & 0 \\ 1 & 0 & 1 \\ 1 & 0 & 0 \end{bmatrix} x^3 - x^2 - x - 1 \lambda \approx 1.839 v = \begin{bmatrix} 1 \\ \lambda - 1 \\ \lambda^{-1} \end{bmatrix} 8 8 \begin{bmatrix} 1& 1& 0& 0& 0& 0& 0& 0 \\ 0& 1& 1& 0& 0& 0& 0& 0 \\ 0& 1& 0& 1& 0& 0& 0& 0 \\ 1& 0& 0& 0& 1& 0& 0& 0 \\ 0& 0& 1& 0& 0& 1& 0& 0 \\ 0& 1& 0& 0& 0& 0& 1& 0 \\ 0& 0& 0& 1& 0& 0& 0& 1 \\ 0& 1& 0& 0& 0& 0& 0& 0 \end{bmatrix}","['linear-algebra', 'eigenvalues-eigenvectors', 'rational-functions']"
30,Relation between uniform and operator norm,Relation between uniform and operator norm,,"Let $f:U\subset \mathbb{C}\to \mathbb{R}^n$ be a $\mathcal{C}^1$ function. I would like to know the relation between the uniform norm and the operator norm of the differential. For this question to make sense, I must make a few comments... Remembering that the differential of the function $f$ , is $f':U\to \mathcal{L}\left (\mathbb{C},\mathbb{R}^n\right )$ . But in this special case, we are talking about a differentiable curve, and we have a natural isomorphism between $\mathcal{L}\left (\mathbb{C},\mathbb{R}^n\right )$ and $\mathbb{R}^n$ so the derivative can be seen as a function $f':U\to \mathbb{R}^n$ . And so we have the uniform norm: $$\|f' \|_\infty =\sup \limits _{z\in U}\|f'(z) \|.$$ Furthermore, seeing the differential as $f'\in \mathcal{L}\left (\mathbb{C},\mathbb{R}^n\right )$ , we can apply the operator norm, $$\|f'(z)\|=\sup \limits _{|v|=1}\|f'(z)\cdot v\|.$$ So, even with this information, I don't know if my question makes much sense, but I would like to know if the two norms are related, more specifically, if $\|f'\|_\infty <\infty$ we will also have $\|f'(z)\|<\infty$ for all $z\in U$ . What did I do: \begin{align*}\|f'(z)\| & =\sup \limits _{|v|=1}\|f'(z)v\|_{\mathbb{R}^n}=\sup \limits _{|v|=1}\|v(f'(z)\cdot 1)\|_{\mathbb{R}^n} \\ & ""=""\sup \limits _{|v|=1}\|v f'(z)\|_{\mathbb{R}^n} \\ & =\sup \limits _{|v|=1}|v|\|f'(z))\|_{\mathbb{R}^n}<\infty. \end{align*} Where in the second line I used the identification I mentioned earlier... I would like to know if my reasoning is correct...","Let be a function. I would like to know the relation between the uniform norm and the operator norm of the differential. For this question to make sense, I must make a few comments... Remembering that the differential of the function , is . But in this special case, we are talking about a differentiable curve, and we have a natural isomorphism between and so the derivative can be seen as a function . And so we have the uniform norm: Furthermore, seeing the differential as , we can apply the operator norm, So, even with this information, I don't know if my question makes much sense, but I would like to know if the two norms are related, more specifically, if we will also have for all . What did I do: Where in the second line I used the identification I mentioned earlier... I would like to know if my reasoning is correct...","f:U\subset \mathbb{C}\to \mathbb{R}^n \mathcal{C}^1 f f':U\to \mathcal{L}\left (\mathbb{C},\mathbb{R}^n\right ) \mathcal{L}\left (\mathbb{C},\mathbb{R}^n\right ) \mathbb{R}^n f':U\to \mathbb{R}^n \|f' \|_\infty =\sup \limits _{z\in U}\|f'(z) \|. f'\in \mathcal{L}\left (\mathbb{C},\mathbb{R}^n\right ) \|f'(z)\|=\sup \limits _{|v|=1}\|f'(z)\cdot v\|. \|f'\|_\infty <\infty \|f'(z)\|<\infty z\in U \begin{align*}\|f'(z)\| & =\sup \limits _{|v|=1}\|f'(z)v\|_{\mathbb{R}^n}=\sup \limits _{|v|=1}\|v(f'(z)\cdot 1)\|_{\mathbb{R}^n} \\
& ""=""\sup \limits _{|v|=1}\|v f'(z)\|_{\mathbb{R}^n} \\
& =\sup \limits _{|v|=1}|v|\|f'(z))\|_{\mathbb{R}^n}<\infty.
\end{align*}","['linear-algebra', 'functional-analysis', 'analysis', 'normed-spaces']"
31,Minimize $\mathrm{tr}(B^TXB)$ subject to $X=A^TX(I+BB^TX)^{-1}A$,Minimize  subject to,\mathrm{tr}(B^TXB) X=A^TX(I+BB^TX)^{-1}A,"For a given $A=\begin{bmatrix}a_1&&&\\&a_2&1&\\&&a_2&\\&&&a_2\end{bmatrix}$ and $B=\begin{bmatrix}b_{11}&b_{12}\\b_{21}&b_{22}\\b_{31}&b_{32}\\0&b_{42}\end{bmatrix}$ , where $a_i>1$ , $a_1\neq a_2$ , $a_i$ and $b_{ij}$ are real, \begin{array}{ll} \underset{X \in \mathbb{R}^{4\times 4}}{\text{minimize}} & \mathrm{tr} \left( B^T X B \right)\\ \text{subject to} & X=A^TX(I+BB^TX)^{-1}A.\end{array} Here $X$ is the unique solution to DARE (discrete-time algebraic Riccati equation). For a specific $A$ and $B$ , I can use Matlab to solve the DARE and then insert the resulting $X$ to find $\mathrm{tr}(B^TXB)$ . Is it possible to get the general answer in terms of $A$ and $B$ ? I have asked this question for general $A$ and $B$ here , but now trying to solve it for simpler case. Currently I have the following bounds: $$a_1^2a_2^4+a_2^2-2\geq \mathrm{tr}(B^TXB) \geq 2a_1a_2^3-2.$$ Matlab code to calculate the $\mathrm{tr}(B^TXB)$ when $A$ is fixed for various $B'$ s: A=[5 0 0 0; 0 2 1 0; 0 0 2 0; 0 0 0 2]; Q=zeros(4,4); for i=1:100     B=2.*rand(4,2)-1*ones(4,2);     [X,K,L] = dare(A,B,Q);     t(i)=trace(B'*X*B); end","For a given and , where , , and are real, Here is the unique solution to DARE (discrete-time algebraic Riccati equation). For a specific and , I can use Matlab to solve the DARE and then insert the resulting to find . Is it possible to get the general answer in terms of and ? I have asked this question for general and here , but now trying to solve it for simpler case. Currently I have the following bounds: Matlab code to calculate the when is fixed for various s: A=[5 0 0 0; 0 2 1 0; 0 0 2 0; 0 0 0 2]; Q=zeros(4,4); for i=1:100     B=2.*rand(4,2)-1*ones(4,2);     [X,K,L] = dare(A,B,Q);     t(i)=trace(B'*X*B); end",A=\begin{bmatrix}a_1&&&\\&a_2&1&\\&&a_2&\\&&&a_2\end{bmatrix} B=\begin{bmatrix}b_{11}&b_{12}\\b_{21}&b_{22}\\b_{31}&b_{32}\\0&b_{42}\end{bmatrix} a_i>1 a_1\neq a_2 a_i b_{ij} \begin{array}{ll} \underset{X \in \mathbb{R}^{4\times 4}}{\text{minimize}} & \mathrm{tr} \left( B^T X B \right)\\ \text{subject to} & X=A^TX(I+BB^TX)^{-1}A.\end{array} X A B X \mathrm{tr}(B^TXB) A B A B a_1^2a_2^4+a_2^2-2\geq \mathrm{tr}(B^TXB) \geq 2a_1a_2^3-2. \mathrm{tr}(B^TXB) A B',"['linear-algebra', 'optimization', 'trace']"
32,"If $\lambda(\mathbf{A}+\mathbf{B})=\lambda(\mathbf{A})+\lambda(\mathbf{B})$, are HPD matrices $\mathbf{A}, \mathbf{B}$ simultaneously diagonalizable?","If , are HPD matrices  simultaneously diagonalizable?","\lambda(\mathbf{A}+\mathbf{B})=\lambda(\mathbf{A})+\lambda(\mathbf{B}) \mathbf{A}, \mathbf{B}","$\mathbf{A}, \mathbf{B} \in \mathbb{C}^{n \times n}$ are Hermitian and positive-definite (HPD) matrices. The following conditions are equivalent: $\mathbf{A}$ and $\mathbf{B}$ commute . $\mathbf{A}$ and $\mathbf{B}$ are simultaneously unitary diagonalizable . $\mathbf{A}$ and $\mathbf{B}$ have the same eigenspace . If either of the above equivalent conditions holds, then $$\lambda(\mathbf{A}+\mathbf{B})=\lambda(\mathbf{A})+\lambda(\mathbf{B}), \tag{1} $$ where $\lambda(\cdot) = (\lambda_1(\cdot), \dots, \lambda_n(\cdot))$ is the $n$ -tuple of the eigenvalues of the corresponding matrix in decreasing order. My question is the opposite of the above statement: If (1) holds, can we say $\mathbf{A}$ and $\mathbf{B}$ satisfy the above conditions, namely, they are simultaneously unitary diagonalizable? If this is true, I guess a place to start to prove might be Theorem 7.6.4 of [1] (see also A property of positive definite matrices ), but I am not sure how to proceed. [1] Horn, R. A., Johnson, C. R. (1990). Matrix Analysis .","are Hermitian and positive-definite (HPD) matrices. The following conditions are equivalent: and commute . and are simultaneously unitary diagonalizable . and have the same eigenspace . If either of the above equivalent conditions holds, then where is the -tuple of the eigenvalues of the corresponding matrix in decreasing order. My question is the opposite of the above statement: If (1) holds, can we say and satisfy the above conditions, namely, they are simultaneously unitary diagonalizable? If this is true, I guess a place to start to prove might be Theorem 7.6.4 of [1] (see also A property of positive definite matrices ), but I am not sure how to proceed. [1] Horn, R. A., Johnson, C. R. (1990). Matrix Analysis .","\mathbf{A}, \mathbf{B} \in \mathbb{C}^{n \times n} \mathbf{A} \mathbf{B} \mathbf{A} \mathbf{B} \mathbf{A} \mathbf{B} \lambda(\mathbf{A}+\mathbf{B})=\lambda(\mathbf{A})+\lambda(\mathbf{B}), \tag{1}  \lambda(\cdot) = (\lambda_1(\cdot), \dots, \lambda_n(\cdot)) n \mathbf{A} \mathbf{B}","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization', 'positive-definite']"
33,Spectral Theorem for Normal Operators - Help understanding proof,Spectral Theorem for Normal Operators - Help understanding proof,,"Theorem: Let $(V, \langle\,,\rangle)$ be a complex inner product space and $T: V \to V$ be a normal operator. Then for any eigenvalue $\lambda \in \textrm{spec}(T)$ , there exists an orthogonal projection $P_{\lambda}$ with $\sum_{\lambda \in \textrm{spec}(T)}P_{\lambda}=I$ and $T = \sum_{\lambda \in \textrm{spec}(T)} \lambda P_{\lambda} $ . Proof: Let $\textrm{spec}(T)=\{\lambda_1, ..., \lambda_k\}.$ It has been proved that $V = \bigoplus_{i=1}^kV_{\lambda_i}$ , where $V_{\lambda_i}$ is the eigenspace corresponding to eigenvalue $\lambda_i$ , $i=1, ..., k$ . This implies $\sum_{\lambda \in \textrm{spec}(T)}P_{\lambda}=I$ . Applying $T$ to this relation gives the second relation. Question: How does the decomposition of $V$ into eigenspaces imply the first relation? I can understand how the second is obtained. Could someone help me understand how the decomposition of $V$ into eigenspaces implies the first relation? Thank you for your help.","Theorem: Let be a complex inner product space and be a normal operator. Then for any eigenvalue , there exists an orthogonal projection with and . Proof: Let It has been proved that , where is the eigenspace corresponding to eigenvalue , . This implies . Applying to this relation gives the second relation. Question: How does the decomposition of into eigenspaces imply the first relation? I can understand how the second is obtained. Could someone help me understand how the decomposition of into eigenspaces implies the first relation? Thank you for your help.","(V, \langle\,,\rangle) T: V \to V \lambda \in \textrm{spec}(T) P_{\lambda} \sum_{\lambda \in \textrm{spec}(T)}P_{\lambda}=I T = \sum_{\lambda \in \textrm{spec}(T)} \lambda P_{\lambda}  \textrm{spec}(T)=\{\lambda_1, ..., \lambda_k\}. V = \bigoplus_{i=1}^kV_{\lambda_i} V_{\lambda_i} \lambda_i i=1, ..., k \sum_{\lambda \in \textrm{spec}(T)}P_{\lambda}=I T V V","['linear-algebra', 'eigenvalues-eigenvectors', 'inner-products']"
34,"$A \in M_{n\times n}(\mathbb{Q})$ with $A\neq I_n$, $A^p=I_n$ then $p\leq n+1$.","with ,  then .",A \in M_{n\times n}(\mathbb{Q}) A\neq I_n A^p=I_n p\leq n+1,"Let $A \in M_{n\times n}(\mathbb{Q})$ with $A\neq I_n$ . I want to prove the following: if $A^p =I_n$ for a prime $p$ , then $p\leq n+1$ . Hmm, At this moment, I have no good idea for this problem.  I just list some of my ideas (which seem not useful).... If $A^p=I_n$ , then $\det(A) \neq 0$ so $A$ is invertible. For matrix over $\mathbb{C}$ , $A^p-I=0$ implies $m_A(t)|t^p-1$ , and for this case since $\mathbb{C}$ is algebraically closed, minimal polynomial factors out to monic polynomial and hence $A$ is diagonalizable. So $\lambda^p=1$ ... (This approach seems not good.)","Let with . I want to prove the following: if for a prime , then . Hmm, At this moment, I have no good idea for this problem.  I just list some of my ideas (which seem not useful).... If , then so is invertible. For matrix over , implies , and for this case since is algebraically closed, minimal polynomial factors out to monic polynomial and hence is diagonalizable. So ... (This approach seems not good.)",A \in M_{n\times n}(\mathbb{Q}) A\neq I_n A^p =I_n p p\leq n+1 A^p=I_n \det(A) \neq 0 A \mathbb{C} A^p-I=0 m_A(t)|t^p-1 \mathbb{C} A \lambda^p=1,"['linear-algebra', 'matrices', 'matrix-equations']"
35,Path to proving partial fractions and the fundamental theorem of algebra,Path to proving partial fractions and the fundamental theorem of algebra,,"As I've learned Calculus, I've tried to follow along with proofs of the rules that I use.  In most cases, like say the Power Rule, I'm able to follow along with the proofs using concepts I understand, or things I've already proved like the Binomial Theorem.  In other cases, like the Extreme Value Theorem the concepts used to prove it seem to require math that's beyond me, so I save proving them for later. I recently learned Partial Fraction Decomposition in the context of Integrals, and I became enamored with this technique.  I searched for proofs of why it works, and most of them seem to rely on the Fundamental Theorem of Algebra.  Most of the proofs of the FTA I've seen rely on either Number Theory or Complex Analysis.  I'm interested in proving the FTA regardless of whether I need it to prove partial fractions. If I want to: Prove the Fundamental Theorem of Algebra Prove why Partial Fractions work What would be the recommended math path for me following Calc?  Would it be to study Number Theory concepts like Euclidean Division and GCD, or to focus on complex numbers to prove the FTA? Also, am I correct to assume that proving FTA should come before proving Partial Fractions always work because the latter proof will follow easily from the former?","As I've learned Calculus, I've tried to follow along with proofs of the rules that I use.  In most cases, like say the Power Rule, I'm able to follow along with the proofs using concepts I understand, or things I've already proved like the Binomial Theorem.  In other cases, like the Extreme Value Theorem the concepts used to prove it seem to require math that's beyond me, so I save proving them for later. I recently learned Partial Fraction Decomposition in the context of Integrals, and I became enamored with this technique.  I searched for proofs of why it works, and most of them seem to rely on the Fundamental Theorem of Algebra.  Most of the proofs of the FTA I've seen rely on either Number Theory or Complex Analysis.  I'm interested in proving the FTA regardless of whether I need it to prove partial fractions. If I want to: Prove the Fundamental Theorem of Algebra Prove why Partial Fractions work What would be the recommended math path for me following Calc?  Would it be to study Number Theory concepts like Euclidean Division and GCD, or to focus on complex numbers to prove the FTA? Also, am I correct to assume that proving FTA should come before proving Partial Fractions always work because the latter proof will follow easily from the former?",,"['linear-algebra', 'number-theory', 'polynomials', 'education', 'partial-fractions']"
36,Solve system of 2 equations with 3 unknowns,Solve system of 2 equations with 3 unknowns,,"We are given a triangle $ABC$ with sides $a, b, c$ respectively and for which the following relationships hold: $a^2+bc\sqrt 3  = b^2+c^2$ , $c^2+ba = a^2+b^2$ We want to prove that angle $B$ is right. I am trying to express sides $b$ and $c$ in relation to $a$ and then prove that they satisfy the Pythagorean theorem. By combining the two equations, I am getting: $b = \frac {c\sqrt 3+1}{2}$ Then I am plugging this expression into the second equation, in order to eliminate $b$ : $4a^2-2a(c\sqrt 3+1)-c^2+2c \sqrt 3 +1=0$ and I must now solve for $a$ in relation to $c$ but I am getting a complex expression, which, by no means, satisfies Pythagorean. I input the two initial equations in Wolfram and it gives as a solution (apart from the ones which are rejected): $b=2a$ and $c=a\sqrt 3$ which clearly satisfy Pythagorean, because $b^2 = 4a^2 = c^2+a^2$ . Any ideas? Thank you!!","We are given a triangle with sides respectively and for which the following relationships hold: , We want to prove that angle is right. I am trying to express sides and in relation to and then prove that they satisfy the Pythagorean theorem. By combining the two equations, I am getting: Then I am plugging this expression into the second equation, in order to eliminate : and I must now solve for in relation to but I am getting a complex expression, which, by no means, satisfies Pythagorean. I input the two initial equations in Wolfram and it gives as a solution (apart from the ones which are rejected): and which clearly satisfy Pythagorean, because . Any ideas? Thank you!!","ABC a, b, c a^2+bc\sqrt 3  = b^2+c^2 c^2+ba = a^2+b^2 B b c a b = \frac {c\sqrt 3+1}{2} b 4a^2-2a(c\sqrt 3+1)-c^2+2c \sqrt 3 +1=0 a c b=2a c=a\sqrt 3 b^2 = 4a^2 = c^2+a^2","['calculus', 'linear-algebra', 'geometry', 'systems-of-equations']"
37,Show that the minimal polynomial of $T:\mathbb{K}^n \mapsto \mathbb{K}^n$ remains the same over field extension,Show that the minimal polynomial of  remains the same over field extension,T:\mathbb{K}^n \mapsto \mathbb{K}^n,"Show that the minimal polynomial of linear transformation $T:\mathbb{K}^n \mapsto \mathbb{K}^n$ remains the same over field extension using the cyclic decomposition theorem. Let $\mathbb{C}$ be a field and let $\mathbb{K}$ be a subfield of $\mathbb{C}$ . Then we see that the minimal polynomial of $T$ over $\mathbb{K}$ (say $p_K(x)$ ) divides the minimal polynomial over $\mathbb{C}$ (say $p_C(x)$ ) Now assume that $p_C(x)= (x-c_1)p_K(x)$ I don't understand where do I use the cyclic decomposition theorem.Some hints rather than the  complete answer would be helpful Edit 1: Here's my attempt to the outline given in the answer: We try to show that $p_{\mathbb{C}}(x) | p_{\mathbb{K}}(x)$ . Now $p_{\mathbb{K}}(T)(e_i) = 0, 1 \le i \le n$ . [ As $e_i \in \mathbb{K}^n , \forall i   s.t. 1 \le i \le n]$ Now $v \in \mathbb{C}^n$ then $v = d_1.e_1 + \cdots + d_n.e_n$ where $d_i \in \mathbb{C}$ then $p_{\mathbb{K}}(T)(d_1.e_1 + \cdots d_n.e_n) = p_{\mathbb{K}}(T)(d_1.e_1) + \cdots + p_{\mathbb{K}}(T)(d_n.e_n) $ then $p_{\mathbb{K}}(T)(v) = 0$ for all $v \in \mathbb{C}$ . Then by the definition of minimal polynomial we see that $p_{\mathbb{C}}(x) | p_{\mathbb{K}}(x)$ . Now to show the converse we know that by the cyclic decomposition theorem we can write : $\mathbb{K}^n = Z(\alpha_1;T) \bigoplus \cdots \bigoplus Z(\alpha_k;T)$ Then $T_{Z(\alpha_i;T)} = T|_{Z(\alpha_i;T)}$ then as $Z(\alpha_i;T)$ is a cyclic subspace then we know that the minimal polynomial of $T_{Z(\alpha_i;T)}$ is the same as the c.p. of $T_{Z(\alpha_i;T)}$ . Now the c.p.( $T$ ) = m.p.( $T_{Z(\alpha_1;T)}$ ).mp.( $T_{Z(\alpha_i;T)}$ ).... m.p.( $T_{Z(\alpha_j;T)}$ Also the minimal polynomial of $T$ over $\mathbb{K}$ is the l.c.m.( m.p.( $T_{Z(\alpha_1;T)}$ ),mp.( $T_{Z(\alpha_i;T)}$ ),...., m.p.( $T_{Z(\alpha_j;T)})$ Thus $p_{\mathbb{K}}(x) = $ l.c.m.( m.p.( $T_{Z(\alpha_1;T)}$ ),mp.( $T_{Z(\alpha_i;T)}$ ),...., m.p.( $T_{Z(\alpha_j;T)})$ Now by the definition of the characteristic polynomial of $T$ it remains the same over any field. Now $p_{\mathbb{C}}(x) | $ c.p. $(x)$ then by the property of the l.c.m. we can claim that $p_{\mathbb{K}}(x) | p_{\mathbb{C}}(x)$ Hence $p_{\mathbb{K}}(x) = p_{\mathbb{C}}(x)$ .","Show that the minimal polynomial of linear transformation remains the same over field extension using the cyclic decomposition theorem. Let be a field and let be a subfield of . Then we see that the minimal polynomial of over (say ) divides the minimal polynomial over (say ) Now assume that I don't understand where do I use the cyclic decomposition theorem.Some hints rather than the  complete answer would be helpful Edit 1: Here's my attempt to the outline given in the answer: We try to show that . Now . [ As Now then where then then for all . Then by the definition of minimal polynomial we see that . Now to show the converse we know that by the cyclic decomposition theorem we can write : Then then as is a cyclic subspace then we know that the minimal polynomial of is the same as the c.p. of . Now the c.p.( ) = m.p.( ).mp.( ).... m.p.( Also the minimal polynomial of over is the l.c.m.( m.p.( ),mp.( ),...., m.p.( Thus l.c.m.( m.p.( ),mp.( ),...., m.p.( Now by the definition of the characteristic polynomial of it remains the same over any field. Now c.p. then by the property of the l.c.m. we can claim that Hence .","T:\mathbb{K}^n \mapsto \mathbb{K}^n \mathbb{C} \mathbb{K} \mathbb{C} T \mathbb{K} p_K(x) \mathbb{C} p_C(x) p_C(x)= (x-c_1)p_K(x) p_{\mathbb{C}}(x) | p_{\mathbb{K}}(x) p_{\mathbb{K}}(T)(e_i) = 0, 1 \le i \le n e_i \in \mathbb{K}^n , \forall i   s.t. 1 \le i \le n] v \in \mathbb{C}^n v = d_1.e_1 + \cdots + d_n.e_n d_i \in \mathbb{C} p_{\mathbb{K}}(T)(d_1.e_1 + \cdots d_n.e_n) = p_{\mathbb{K}}(T)(d_1.e_1) + \cdots + p_{\mathbb{K}}(T)(d_n.e_n)  p_{\mathbb{K}}(T)(v) = 0 v \in \mathbb{C} p_{\mathbb{C}}(x) | p_{\mathbb{K}}(x) \mathbb{K}^n = Z(\alpha_1;T) \bigoplus \cdots \bigoplus Z(\alpha_k;T) T_{Z(\alpha_i;T)} = T|_{Z(\alpha_i;T)} Z(\alpha_i;T) T_{Z(\alpha_i;T)} T_{Z(\alpha_i;T)} T T_{Z(\alpha_1;T)} T_{Z(\alpha_i;T)} T_{Z(\alpha_j;T)} T \mathbb{K} T_{Z(\alpha_1;T)} T_{Z(\alpha_i;T)} T_{Z(\alpha_j;T)}) p_{\mathbb{K}}(x) =  T_{Z(\alpha_1;T)} T_{Z(\alpha_i;T)} T_{Z(\alpha_j;T)}) T p_{\mathbb{C}}(x) |  (x) p_{\mathbb{K}}(x) | p_{\mathbb{C}}(x) p_{\mathbb{K}}(x) = p_{\mathbb{C}}(x)","['linear-algebra', 'linear-transformations', 'minimal-polynomials']"
38,Show that for all $X \in \mathscr{M}_n(\mathbb{C})$ we have that: $\lim\limits_{n \rightarrow \infty}{\left(I + \frac{1}{m}X\right)}^{m} = \exp{X}$,Show that for all  we have that:,X \in \mathscr{M}_n(\mathbb{C}) \lim\limits_{n \rightarrow \infty}{\left(I + \frac{1}{m}X\right)}^{m} = \exp{X},"Show that for all $X \in \mathscr{M}_n(\mathbb{C})$ we have that: $$\lim_{n \rightarrow \infty}{\left(I + \frac{1}{m}X\right)^{m}} = \exp{X}$$ Note that for matrices, the $e^X$ is defined by using the series expansion of the exponential function: $$\exp{X} = I + X + \frac{1}{2!}X + \frac{1}{3!}X + \cdots$$ I expanded out $\left(I + \frac{1}{m}X\right)^{m}$ , which I figured I could do since powers of $X$ commute with each other: $$\left(I + \frac{1}{m}X\right)^{m} = \sum_{k=0}^{m}{\binom{m}{k}\left(\frac{1}{m}X\right)^{k}} = \sum_{k=0}^{m}{\frac{m!}{k!\left(m-k\right)!}\left(\frac{1}{m^{k}}X^{k}\right)}$$ The $k$ 'th term in this expansion is can be simplified to: $$\frac{m\left(m-1\right)\left(m-2\right)\cdots\left(m-k+1\right)}{m^{k}}\frac{1}{k!}X^{k}$$ (edited) Thus, as $m \rightarrow \infty$ , the identity will hold since $\frac{m\left(m-1\right)\left(m-2\right)\cdots\left(m-k-1\right)}{m^{k}} \underset{m\to \infty}{\longrightarrow} 1$","Show that for all we have that: Note that for matrices, the is defined by using the series expansion of the exponential function: I expanded out , which I figured I could do since powers of commute with each other: The 'th term in this expansion is can be simplified to: (edited) Thus, as , the identity will hold since",X \in \mathscr{M}_n(\mathbb{C}) \lim_{n \rightarrow \infty}{\left(I + \frac{1}{m}X\right)^{m}} = \exp{X} e^X \exp{X} = I + X + \frac{1}{2!}X + \frac{1}{3!}X + \cdots \left(I + \frac{1}{m}X\right)^{m} X \left(I + \frac{1}{m}X\right)^{m} = \sum_{k=0}^{m}{\binom{m}{k}\left(\frac{1}{m}X\right)^{k}} = \sum_{k=0}^{m}{\frac{m!}{k!\left(m-k\right)!}\left(\frac{1}{m^{k}}X^{k}\right)} k \frac{m\left(m-1\right)\left(m-2\right)\cdots\left(m-k+1\right)}{m^{k}}\frac{1}{k!}X^{k} m \rightarrow \infty \frac{m\left(m-1\right)\left(m-2\right)\cdots\left(m-k-1\right)}{m^{k}} \underset{m\to \infty}{\longrightarrow} 1,"['linear-algebra', 'matrices']"
39,How to show that $D^{-1}A$ and $L^{T}D^{-1}L$ have same Eigenvalues,How to show that  and  have same Eigenvalues,D^{-1}A L^{T}D^{-1}L,"Let $A$ be a symmetric positive definite matrix, I want to prove that $D^{-1}A$ and $L^{T}D^{-1}L$ have same Eigenvalues where $D=\text{diag}(\text{diag}(A))$ and $L$ is a lower-triangular matrix such that $A=LL^{T}$ . I observed that $D^{-1}A=D^{-1}LL^{T}$ and thus we can write $L^{T}D^{-1}A=(L^{T}D^{-1}L)L^{T}$ .How can we proceed from here? I would hope for some hints. I noticed that $D^{-1}$ and $L^{T}D^{-1}L$ are similar given this form. Therefore, they both must have same eigenvalues but how can I show that $D^{-1}$ and $D^{-1}A$ have same eigenvalues to complete this proof by substitution as it appears? Update: As mentioned in the comments, this does not work since $L^{T}D^{-1}L$ and $D^{-1}$ are congruent and not similar. Therefore, I would really hope for some help in finding an approach to prove the claim. Remark: $D:=\text{diag}(\text{diag}(A))$ means that $D$ is a diagonal matrix whose diagonal entries are the diagonal entries of $A$ . This is based on MATLAB's notation.","Let be a symmetric positive definite matrix, I want to prove that and have same Eigenvalues where and is a lower-triangular matrix such that . I observed that and thus we can write .How can we proceed from here? I would hope for some hints. I noticed that and are similar given this form. Therefore, they both must have same eigenvalues but how can I show that and have same eigenvalues to complete this proof by substitution as it appears? Update: As mentioned in the comments, this does not work since and are congruent and not similar. Therefore, I would really hope for some help in finding an approach to prove the claim. Remark: means that is a diagonal matrix whose diagonal entries are the diagonal entries of . This is based on MATLAB's notation.",A D^{-1}A L^{T}D^{-1}L D=\text{diag}(\text{diag}(A)) L A=LL^{T} D^{-1}A=D^{-1}LL^{T} L^{T}D^{-1}A=(L^{T}D^{-1}L)L^{T} D^{-1} L^{T}D^{-1}L D^{-1} D^{-1}A L^{T}D^{-1}L D^{-1} D:=\text{diag}(\text{diag}(A)) D A,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
40,Why is a linear system of equations considered consistent when a solution contains 0/0?,Why is a linear system of equations considered consistent when a solution contains 0/0?,,"Suppose I have a system of equations as such, with $\alpha$ and $\beta$ being constants: $$ x_1+5x_2+x_3=1\\ x_1+6x_2-x_3=1\\ 2x_1+\alpha x_2-6x_3=\beta $$ Suppose I want to calculate for which values of $\alpha$ and $\beta$ my system is consistent. I can do that by making a coëfficiënt matrix and using Gaussian elimination to derive the solution space, it being: $$ x_1=1-\frac{11\beta -22}{2\alpha-28}\\ x_2=\frac{\beta -2}{\alpha -14}\\ x_3=\frac{\beta -2}{2\alpha -28} $$ Then I can see that for the value $\alpha=14$ the denominator = 0, which is not possible. So, I can conclude that the system is only consistent for $\alpha\neq14$ . This is clear to me. However, my answer sheet says that $\alpha=14$ and $\beta=2$ is also an answer. Plugging these values into $x_1$ , $x_2$ and $x_3$ gives these answers: $$ x_1=1-\frac{0}{0}\\ x_2=\frac{0}{0}\\ x_3=\frac{0}{0} $$ I thought $\frac{0}{0}$ was supposed to be undefined? How can it be then that using these values for $\alpha$ and $\beta$ , we get a consistent system? To try to figure it out, I checked the answer of $A\overrightarrow{x}=\overrightarrow{b}$ , which gives me this (I think): $$ \begin{bmatrix}1 & 5 & 1\\1 & 6 & -1\\2 & \alpha & -6\end{bmatrix} \begin{bmatrix}1-\frac{0}{0}\\\frac{0}{0}\\\frac{0}{0}\end{bmatrix} =  \begin{bmatrix}1-\frac{0}{0}+5(1-\frac{0}{0})+1-\frac{0}{0}\\\frac{0}{0}+6\frac{0}{0}-\frac{0}{0}\\2\frac{0}{0}+\alpha\frac{0}{0}-6\frac{0}{0}\end{bmatrix} $$ Which in turn implies that the equation below must be true, since the answer sheet says that the system is consistent: $$ \begin{bmatrix}1-\frac{0}{0}+5(1-\frac{0}{0})+1-\frac{0}{0}\\\frac{0}{0}+6\frac{0}{0}-\frac{0}{0}\\2\frac{0}{0}+\alpha\frac{0}{0}-6\frac{0}{0}\end{bmatrix} = \begin{bmatrix}1\\1\\\beta\end{bmatrix} $$ If I substitute $\frac{0}{0}$ for something else (lets say $\frac{0}{0}=p$ ), I get this: $$ \begin{bmatrix}1-p+5(1-p)+1-p\\p+6p-p\\2p+\alpha p-6p\end{bmatrix} = \begin{bmatrix}7-7p\\6p\\\alpha p-4p\end{bmatrix} = \begin{bmatrix}1\\1\\\beta\end{bmatrix} $$ I get this equation, which when looking at the first 2 components of the vectors already doesn't make any sense, since for $7-7p=1$ to make sense, $p$ would have to be $\frac{6}{7}$ . But for $6p=1$ , $p$ would have to be $\frac{1}{6}$ .","Suppose I have a system of equations as such, with and being constants: Suppose I want to calculate for which values of and my system is consistent. I can do that by making a coëfficiënt matrix and using Gaussian elimination to derive the solution space, it being: Then I can see that for the value the denominator = 0, which is not possible. So, I can conclude that the system is only consistent for . This is clear to me. However, my answer sheet says that and is also an answer. Plugging these values into , and gives these answers: I thought was supposed to be undefined? How can it be then that using these values for and , we get a consistent system? To try to figure it out, I checked the answer of , which gives me this (I think): Which in turn implies that the equation below must be true, since the answer sheet says that the system is consistent: If I substitute for something else (lets say ), I get this: I get this equation, which when looking at the first 2 components of the vectors already doesn't make any sense, since for to make sense, would have to be . But for , would have to be .","\alpha \beta 
x_1+5x_2+x_3=1\\
x_1+6x_2-x_3=1\\
2x_1+\alpha x_2-6x_3=\beta
 \alpha \beta 
x_1=1-\frac{11\beta -22}{2\alpha-28}\\
x_2=\frac{\beta -2}{\alpha -14}\\
x_3=\frac{\beta -2}{2\alpha -28}
 \alpha=14 \alpha\neq14 \alpha=14 \beta=2 x_1 x_2 x_3 
x_1=1-\frac{0}{0}\\
x_2=\frac{0}{0}\\
x_3=\frac{0}{0}
 \frac{0}{0} \alpha \beta A\overrightarrow{x}=\overrightarrow{b} 
\begin{bmatrix}1 & 5 & 1\\1 & 6 & -1\\2 & \alpha & -6\end{bmatrix}
\begin{bmatrix}1-\frac{0}{0}\\\frac{0}{0}\\\frac{0}{0}\end{bmatrix}
= 
\begin{bmatrix}1-\frac{0}{0}+5(1-\frac{0}{0})+1-\frac{0}{0}\\\frac{0}{0}+6\frac{0}{0}-\frac{0}{0}\\2\frac{0}{0}+\alpha\frac{0}{0}-6\frac{0}{0}\end{bmatrix}
 
\begin{bmatrix}1-\frac{0}{0}+5(1-\frac{0}{0})+1-\frac{0}{0}\\\frac{0}{0}+6\frac{0}{0}-\frac{0}{0}\\2\frac{0}{0}+\alpha\frac{0}{0}-6\frac{0}{0}\end{bmatrix}
=
\begin{bmatrix}1\\1\\\beta\end{bmatrix}
 \frac{0}{0} \frac{0}{0}=p 
\begin{bmatrix}1-p+5(1-p)+1-p\\p+6p-p\\2p+\alpha p-6p\end{bmatrix}
=
\begin{bmatrix}7-7p\\6p\\\alpha p-4p\end{bmatrix}
=
\begin{bmatrix}1\\1\\\beta\end{bmatrix}
 7-7p=1 p \frac{6}{7} 6p=1 p \frac{1}{6}",['linear-algebra']
41,"Verify that $x = −(I − A^{\dagger}A)c$ optimises $ \frac{1}{2}\|x\|^2 + \langle c,x \rangle$ under constraints",Verify that  optimises  under constraints,"x = −(I − A^{\dagger}A)c  \frac{1}{2}\|x\|^2 + \langle c,x \rangle","Under the constraint that $Ax = 0$ and $A$ is full row rank, how would you get the solution to the optimisation problem $$\text{min } \frac{1}{2}\|x\|^2 + \langle c,x \rangle$$ My text says that $x = −(I − A^{\dagger}A)c$ is a unique solution but I am unsure as to how to verify it as I am unsure what a feasible direction is under this constraint. $A^{\dagger}$ is the Moore Penrose inverse","Under the constraint that and is full row rank, how would you get the solution to the optimisation problem My text says that is a unique solution but I am unsure as to how to verify it as I am unsure what a feasible direction is under this constraint. is the Moore Penrose inverse","Ax = 0 A \text{min } \frac{1}{2}\|x\|^2 + \langle c,x \rangle x = −(I − A^{\dagger}A)c A^{\dagger}","['linear-algebra', 'optimization', 'numerical-linear-algebra', 'nonlinear-optimization', 'constraints']"
42,Find the matrix representation of $A$ with respect to basis given.,Find the matrix representation of  with respect to basis given.,A,"Let $A = \left[\begin{matrix} 2 & 3 & 4 \\ 8 & 5 & 1\end{matrix}\right]$ and consider $A$ as a linear transformation from $\mathbb{R}^3$ into $\mathbb{R}^2$ using the standard bases. Find the matrix representation of $A$ with resepect to the basis $\left\{\space\left[\begin{matrix}1 \\ 1 \\ 0\end{matrix}\right], \left[\begin{matrix}0 \\ 1 \\ 1\end{matrix}\right], \left[\begin{matrix}1 \\ 0 \\ 1\end{matrix}\right]\space\right\}$ for $\mathbb{R}^3$ and $\left\{ \left[\begin{matrix}3 \\ 1\end{matrix}\right], \left[\begin{matrix}2 \\ 1\end{matrix}\right] \right\}$ for $\mathbb{R}^2$ . Here is my attempt taking from these two posts post_1 and post_2 : Let $C = \left[\begin{matrix}\mathbf{c}_1 & \mathbf{c}_2  & \mathbf{c_3}\end{matrix}\right]$ , that is, the matrix representation of the vectors of the given basis for $\mathbb{R}^3$ . Similarly represent the basis vectors for $\mathbb{R}^2$ as $B = \left[\begin{matrix}\mathbf{b}_1 & \mathbf{b}_2 \end{matrix}\right]$ . Then, apply the transformation $T(\mathbf{c}_1) = A\mathbf{c}_1 = \mathbf{v}_1$ . We have a new vector now in $\mathbb{R}^2$ . Side note: formula for coordinate vector of $\mathbf{v}$ with respect to the basis B. $$(1)\quad\quad  \mathbf{v} = B[\mathbf{v}]_B \implies B^{-1}\mathbf{v} = [\mathbf{v}]_B$$ Now using formula (1) to find $\mathbf{v}_1$ in terms of our basis for $\mathbb{R}^2$ we have $B^{-1}\mathbf{v}_1$ . Thus, this is our first vector in our new matrix representation of A. Repeat this process for $\mathbf{c}_2$ and $\mathbf{c}_3$ . Generalizing this process with the help of post_2 we get $$(2)\quad\quad B^{-1}AC$$ This is our new matrix and our representation for matrix A. Is this correct? Is there something I am missing? Furthermore, my intuition for this is that we are given matrix $A$ that maps vectors $\mathbb{R}^3 \to \mathbb{R}^2$ . Then we need to map the given basis vectors (IDK ... thb) for $\mathbb{R}^3$ into $\mathbb{R}^2$ . But now we need to write these new vectors (hence new matrix) in terms of our given basis for $\mathbb{R}^2$ . So we take each vector $\mathbf{v}_i$ , $1 \leq i \leq 3$ , and do $\mathbf{v}_i = B[\mathbf{v}_i]_B \implies B^{-1}\mathbf{v}_i = [\mathbf{v}_i]_B$ . That is now we have each vector represented now in terms of the basis in $\mathbb{R}^2$ which gives us the matrix representation. Let me know if what I have done or my intuition needs correcting! I would love all the help!","Let and consider as a linear transformation from into using the standard bases. Find the matrix representation of with resepect to the basis for and for . Here is my attempt taking from these two posts post_1 and post_2 : Let , that is, the matrix representation of the vectors of the given basis for . Similarly represent the basis vectors for as . Then, apply the transformation . We have a new vector now in . Side note: formula for coordinate vector of with respect to the basis B. Now using formula (1) to find in terms of our basis for we have . Thus, this is our first vector in our new matrix representation of A. Repeat this process for and . Generalizing this process with the help of post_2 we get This is our new matrix and our representation for matrix A. Is this correct? Is there something I am missing? Furthermore, my intuition for this is that we are given matrix that maps vectors . Then we need to map the given basis vectors (IDK ... thb) for into . But now we need to write these new vectors (hence new matrix) in terms of our given basis for . So we take each vector , , and do . That is now we have each vector represented now in terms of the basis in which gives us the matrix representation. Let me know if what I have done or my intuition needs correcting! I would love all the help!","A = \left[\begin{matrix} 2 & 3 & 4 \\ 8 & 5 & 1\end{matrix}\right] A \mathbb{R}^3 \mathbb{R}^2 A \left\{\space\left[\begin{matrix}1 \\ 1 \\ 0\end{matrix}\right], \left[\begin{matrix}0 \\ 1 \\ 1\end{matrix}\right], \left[\begin{matrix}1 \\ 0 \\ 1\end{matrix}\right]\space\right\} \mathbb{R}^3 \left\{ \left[\begin{matrix}3 \\ 1\end{matrix}\right], \left[\begin{matrix}2 \\ 1\end{matrix}\right] \right\} \mathbb{R}^2 C = \left[\begin{matrix}\mathbf{c}_1 & \mathbf{c}_2  & \mathbf{c_3}\end{matrix}\right] \mathbb{R}^3 \mathbb{R}^2 B = \left[\begin{matrix}\mathbf{b}_1 & \mathbf{b}_2 \end{matrix}\right] T(\mathbf{c}_1) = A\mathbf{c}_1 = \mathbf{v}_1 \mathbb{R}^2 \mathbf{v} (1)\quad\quad  \mathbf{v} = B[\mathbf{v}]_B \implies B^{-1}\mathbf{v} = [\mathbf{v}]_B \mathbf{v}_1 \mathbb{R}^2 B^{-1}\mathbf{v}_1 \mathbf{c}_2 \mathbf{c}_3 (2)\quad\quad B^{-1}AC A \mathbb{R}^3 \to \mathbb{R}^2 \mathbb{R}^3 \mathbb{R}^2 \mathbb{R}^2 \mathbf{v}_i 1 \leq i \leq 3 \mathbf{v}_i = B[\mathbf{v}_i]_B \implies B^{-1}\mathbf{v}_i = [\mathbf{v}_i]_B \mathbb{R}^2","['linear-algebra', 'matrices', 'solution-verification', 'linear-transformations', 'change-of-basis']"
43,Constructive proof that a linear relation between columns implies a linear relation between rows,Constructive proof that a linear relation between columns implies a linear relation between rows,,"Let $M = \left(M_{ij}\right)$ be a square matrix and let $v = (v_{i}) \neq 0$ be such that $Mv = 0$ . This can be rephrased into requiring that $\sum_{i} v_{i} M_{i} = 0$ , where $\{M_{i}\}$ are the column vectors in $M$ , i.e. $(v_{j})$ gives a linear relation on the columns of $M$ . I can think of multiple proofs that the existence of $v$ implies the existence of a $w=(w_{j})$ such that $M^{t}w = 0$ , but none of them are constructive. How can we write done the coordinates of such a $w$ in terms of the coordinates of $v$ and $M$ ? In other words, how can we translate a linear relation of columns into a linear relation of rows? To be more explicit, I would like a non-trivial expression $w_j = f(v,M)$ such that when I compute $\sum_j w_j M_j$ (where $M_j$ are the row vectors in $M$ ) I obtain $0$ by using the fact that $\sum_{i} v_{i} M_{i} = 0$ . Does such an expression exist?","Let be a square matrix and let be such that . This can be rephrased into requiring that , where are the column vectors in , i.e. gives a linear relation on the columns of . I can think of multiple proofs that the existence of implies the existence of a such that , but none of them are constructive. How can we write done the coordinates of such a in terms of the coordinates of and ? In other words, how can we translate a linear relation of columns into a linear relation of rows? To be more explicit, I would like a non-trivial expression such that when I compute (where are the row vectors in ) I obtain by using the fact that . Does such an expression exist?","M = \left(M_{ij}\right) v = (v_{i}) \neq 0 Mv = 0 \sum_{i} v_{i} M_{i} = 0 \{M_{i}\} M (v_{j}) M v w=(w_{j}) M^{t}w = 0 w v M w_j = f(v,M) \sum_j w_j M_j M_j M 0 \sum_{i} v_{i} M_{i} = 0","['linear-algebra', 'matrices']"
44,Relation between eigenvalues and size of geometrically shaped data in PCA,Relation between eigenvalues and size of geometrically shaped data in PCA,,"I want to measure the orientation and size of objects in a binary image using PCA. As I have a picture the data points are gridded (pixels). The object can be either an ellipse or a rectangle that I do not know beforehand. For the centered data points I calculate the eigenvalues and eigenvectors of the covariance matrix and then want to derive the object size from the eigenvalues. The problem is that the scaling of the eigenvalues depends on the shape of the data points. Example Let's assume I have an ellipse with full axes lengths (from one end to other end) $a=100,b=50$ and a rectangle with same dimensions. Both objects are rotated by $\theta=45$ deg. from the axis aligned state, see figure. The normalized eigenvectors direct as expected in the principal directions: $e_1\approx\dfrac{1}{\sqrt{2}} \begin{align}      \begin{bmatrix}-1 \\ 1  \end{bmatrix}\end{align}$ , $e_2\approx\dfrac{1}{\sqrt{2}} \begin{align}      \begin{bmatrix}1 \\ 1  \end{bmatrix}\end{align}$ . The corresponding eigenvalues $\lambda_1,\lambda_2$ are related to object sizes by $a=k \sqrt{\lambda_1},b=k \sqrt{\lambda_2}$ . However $k$ depends on the shape. By testing many combinations of $a,b,\theta$ I can reproduce the  dimensions of the ellipse if $k\approx4$ and for the rectangle if $k\approx2\sqrt{3}\approx3.46$ . How can I derive $k$ from my original data as I do not know beforehand if my object is an ellipse or rectangle? For completeness I give the covariance matrices of the depicted example: ellipse (3927 data points): $\begin{align}  \begin{bmatrix} 390.84770    &  -234.51809 \\ -234.51809   &    390.84770   \end{bmatrix}\end{align}$ rectangle (4994 data points): $\begin{align}  \begin{bmatrix}  518.98143  &    -309.86476\\ -309.86476   &    518.98143   \end{bmatrix}\end{align}$","I want to measure the orientation and size of objects in a binary image using PCA. As I have a picture the data points are gridded (pixels). The object can be either an ellipse or a rectangle that I do not know beforehand. For the centered data points I calculate the eigenvalues and eigenvectors of the covariance matrix and then want to derive the object size from the eigenvalues. The problem is that the scaling of the eigenvalues depends on the shape of the data points. Example Let's assume I have an ellipse with full axes lengths (from one end to other end) and a rectangle with same dimensions. Both objects are rotated by deg. from the axis aligned state, see figure. The normalized eigenvectors direct as expected in the principal directions: , . The corresponding eigenvalues are related to object sizes by . However depends on the shape. By testing many combinations of I can reproduce the  dimensions of the ellipse if and for the rectangle if . How can I derive from my original data as I do not know beforehand if my object is an ellipse or rectangle? For completeness I give the covariance matrices of the depicted example: ellipse (3927 data points): rectangle (4994 data points):","a=100,b=50 \theta=45 e_1\approx\dfrac{1}{\sqrt{2}} \begin{align}
     \begin{bmatrix}-1 \\ 1  \end{bmatrix}\end{align} e_2\approx\dfrac{1}{\sqrt{2}} \begin{align}
     \begin{bmatrix}1 \\ 1  \end{bmatrix}\end{align} \lambda_1,\lambda_2 a=k \sqrt{\lambda_1},b=k \sqrt{\lambda_2} k a,b,\theta k\approx4 k\approx2\sqrt{3}\approx3.46 k \begin{align}  \begin{bmatrix}
390.84770    &  -234.51809 \\
-234.51809   &    390.84770
  \end{bmatrix}\end{align} \begin{align}  \begin{bmatrix}
 518.98143  &    -309.86476\\
-309.86476   &    518.98143
  \end{bmatrix}\end{align}","['linear-algebra', 'eigenvalues-eigenvectors', 'covariance', 'image-processing']"
45,Standard matrices to test low rank decomposition,Standard matrices to test low rank decomposition,,"I am working on a low rank decomposition technique that is robust to different types of noise (gaussian, salt and pepper, poisson). For starters, I simulated such low rank matrices and have successfully demonstrated that my technique works fairly well, and would like to test the method on more relevant matrices. As part of validation, I need to show how it performs on some commonly used low rank matrices. Is there such a repository of matrices? Here are some matrices that I have tried so far: Hyperspectral images (each column of the matrix is an image at a narrowband of wavelength) Eigenfaces (PCA of a covariance matrices constructed from images of faces) Videos (background subtraction) Thanks in advance for the help!","I am working on a low rank decomposition technique that is robust to different types of noise (gaussian, salt and pepper, poisson). For starters, I simulated such low rank matrices and have successfully demonstrated that my technique works fairly well, and would like to test the method on more relevant matrices. As part of validation, I need to show how it performs on some commonly used low rank matrices. Is there such a repository of matrices? Here are some matrices that I have tried so far: Hyperspectral images (each column of the matrix is an image at a narrowband of wavelength) Eigenfaces (PCA of a covariance matrices constructed from images of faces) Videos (background subtraction) Thanks in advance for the help!",,"['linear-algebra', 'matrices', 'numerical-linear-algebra', 'matrix-decomposition', 'applications']"
46,Covering of a polytope by balls with midpoints at the vertices,Covering of a polytope by balls with midpoints at the vertices,,"I have a polytope $P=\operatorname{conv}(v_1,\ldots,v_m)\subset \mathbb{R}^n$ and a ball $B_r(x)$ (with center $x\in \mathbb{R}^n$ and  radius $r>0$ ), such that $P\subset B_r(x)$ . Is the statement $$ P\subset \bigcup_{i=1}^mB_r(v_i)$$ true? Intuitively, I think it is true, but it is hard for me to prove it. For every point $w\in P$ I can show $d(w,v_i)\leq 2r$ , since $d(w,v_i)\leq d(w,x)+d(x,v_i)\leq 2r$ . But this only implies $$ P\subset B_{2r}(v_i)\quad\forall i\in\{1,\ldots,m\}.$$ Maybe it's better to formulate it without balls. I have $m$ points $v_1,\ldots, v_m$ and i know a point $x$ and a real number $r>0$ with $d(x,v_i)\leq r$ for all $v_i$ . I claim for every $w\in\operatorname{conv}\{v_1,\ldots, v_m\}$ there is an $i$ with $d(w, v_i)\leq r$ .","I have a polytope and a ball (with center and  radius ), such that . Is the statement true? Intuitively, I think it is true, but it is hard for me to prove it. For every point I can show , since . But this only implies Maybe it's better to formulate it without balls. I have points and i know a point and a real number with for all . I claim for every there is an with .","P=\operatorname{conv}(v_1,\ldots,v_m)\subset \mathbb{R}^n B_r(x) x\in \mathbb{R}^n r>0 P\subset B_r(x)  P\subset \bigcup_{i=1}^mB_r(v_i) w\in P d(w,v_i)\leq 2r d(w,v_i)\leq d(w,x)+d(x,v_i)\leq 2r  P\subset B_{2r}(v_i)\quad\forall i\in\{1,\ldots,m\}. m v_1,\ldots, v_m x r>0 d(x,v_i)\leq r v_i w\in\operatorname{conv}\{v_1,\ldots, v_m\} i d(w, v_i)\leq r","['linear-algebra', 'polygons', 'convex-hulls']"
47,Minimize $x^*(A+A^*)x$ such that $x^*A^*Ax=1$ and $x^*x=1$,Minimize  such that  and,x^*(A+A^*)x x^*A^*Ax=1 x^*x=1,"Given $A\in\mathbb{C}^{n\times n}$ , such that it has singular values larger than $1$ and smaller than $1$ , \begin{array}{ll} \underset{x\in\mathbb{C^n}}{\text{minimize}} & x^*(A+A^*)x.\\ \text{subject to} & x^*A^*Ax=1,\\&x^*x=1\end{array} My attempt: I couldn't get anything done for general $A$ . Assume $A$ is hermitian, then $A=U\Sigma U^*$ , where $U$ is unitary and $\Sigma$ is real diagonal. Let $y=U^*x$ , then \begin{array}{ll} \underset{y\in\mathbb{C^n}}{\text{minimize}} & 2y^*\Sigma y.\\ \text{subject to} & y^*\Sigma^2y=1,\\&y^*y=1\end{array} Using Lagrange multiplier, we can find: $L(y,\lambda_1,\lambda_2)=2y^*\Sigma y-\lambda_1(y^*\Sigma^2y-1)-\lambda_2(y^*y-1)$ . Then \begin{align} &\frac{\partial L(y,\lambda_1,\lambda_2)}{\partial y}=4\Sigma y-2\lambda_1\Sigma^2y-2\lambda_2y=0 \end{align} gives us $y_i=0$ or $\lambda_1\sigma_i^2-2\sigma_i+\lambda_2=0$ for all $i=1,2,\ldots,n.$ From $\lambda_1\sigma_i^2-2\sigma_i+\lambda_2=0$ , we can get $\sigma_i=\frac{1\pm\sqrt{1-\lambda_1\lambda_2}}{\lambda_1}$ , so here I think that if there are many distinct $\sigma_i$ 's, then we will get $\sigma_{j_1}=\frac{1+\sqrt{1-\lambda_1\lambda_2}}{\lambda_1}$ , $\sigma_{j_2}=\frac{1-\sqrt{1-\lambda_1\lambda_2}}{\lambda_1}$ (here $j_i$ is any number from $1$ to $n$ ), and $y_k=0$ for all $k\neq j_2$ and $k\neq j_1$ . From $\sigma_{j_1}=\frac{1+\sqrt{1-\lambda_1\lambda_2}}{\lambda_1}$ , $\sigma_{j_2}=\frac{1-\sqrt{1-\lambda_1\lambda_2}}{\lambda_1}$ , we can find that $\lambda_1=\frac{2}{\sigma_{j_1}+\sigma_{j_2}}$ and $\lambda_2=\frac{2\sigma_{j_1}\sigma_{j_2}}{\sigma_{j_1}+\sigma_{j_2}}$ . From this observations, I got impression that for Hermitian $A$ , original problem is equivalent to \begin{array}{ll} \underset{y\in\mathbb{C^2}}{\text{minimize}} & 2y^*\begin{bmatrix} \sigma_{j_1} & \\  & \sigma_{j_2} \end{bmatrix} y\quad=\quad\underset{\sigma_{j_1},\sigma_{j_2}}{\text{minimize}} &\sigma_{j_1}\sqrt{\frac{1-\sigma_{j_2}^2}{\sigma_{j_1}^2-\sigma_{j_2}^2}}+\sigma_{j_2}\sqrt{\frac{\sigma_{j_1}^2-1}{\sigma_{j_1}^2-\sigma_{j_2}^2}}\\ \text{subject to} & y^*\begin{bmatrix} \sigma_{j_1} & \\  & \sigma_{j_2} \end{bmatrix}^2y=1,&\sigma_{j_1}>1\\&y^*y=1&\sigma_{j_2}<1\end{array} when $y_1=\sqrt{\frac{1-\sigma_{j_2}^2}{\sigma_{j_1}^2-\sigma_{j_2}^2}}$ and $y_2=\sqrt{\frac{\sigma_{j_1}^2-1}{\sigma_{j_1}^2-\sigma_{j_2}^2}}$ . Can you please tell me if the above calculations make sense? Any help on solving (analytically or numerically) the original problem for general $A$ would be appreciated.","Given , such that it has singular values larger than and smaller than , My attempt: I couldn't get anything done for general . Assume is hermitian, then , where is unitary and is real diagonal. Let , then Using Lagrange multiplier, we can find: . Then gives us or for all From , we can get , so here I think that if there are many distinct 's, then we will get , (here is any number from to ), and for all and . From , , we can find that and . From this observations, I got impression that for Hermitian , original problem is equivalent to when and . Can you please tell me if the above calculations make sense? Any help on solving (analytically or numerically) the original problem for general would be appreciated.","A\in\mathbb{C}^{n\times n} 1 1 \begin{array}{ll} \underset{x\in\mathbb{C^n}}{\text{minimize}} & x^*(A+A^*)x.\\ \text{subject to} & x^*A^*Ax=1,\\&x^*x=1\end{array} A A A=U\Sigma U^* U \Sigma y=U^*x \begin{array}{ll} \underset{y\in\mathbb{C^n}}{\text{minimize}} & 2y^*\Sigma y.\\ \text{subject to} & y^*\Sigma^2y=1,\\&y^*y=1\end{array} L(y,\lambda_1,\lambda_2)=2y^*\Sigma y-\lambda_1(y^*\Sigma^2y-1)-\lambda_2(y^*y-1) \begin{align}
&\frac{\partial L(y,\lambda_1,\lambda_2)}{\partial y}=4\Sigma y-2\lambda_1\Sigma^2y-2\lambda_2y=0
\end{align} y_i=0 \lambda_1\sigma_i^2-2\sigma_i+\lambda_2=0 i=1,2,\ldots,n. \lambda_1\sigma_i^2-2\sigma_i+\lambda_2=0 \sigma_i=\frac{1\pm\sqrt{1-\lambda_1\lambda_2}}{\lambda_1} \sigma_i \sigma_{j_1}=\frac{1+\sqrt{1-\lambda_1\lambda_2}}{\lambda_1} \sigma_{j_2}=\frac{1-\sqrt{1-\lambda_1\lambda_2}}{\lambda_1} j_i 1 n y_k=0 k\neq j_2 k\neq j_1 \sigma_{j_1}=\frac{1+\sqrt{1-\lambda_1\lambda_2}}{\lambda_1} \sigma_{j_2}=\frac{1-\sqrt{1-\lambda_1\lambda_2}}{\lambda_1} \lambda_1=\frac{2}{\sigma_{j_1}+\sigma_{j_2}} \lambda_2=\frac{2\sigma_{j_1}\sigma_{j_2}}{\sigma_{j_1}+\sigma_{j_2}} A \begin{array}{ll} \underset{y\in\mathbb{C^2}}{\text{minimize}} & 2y^*\begin{bmatrix}
\sigma_{j_1} & \\
 & \sigma_{j_2}
\end{bmatrix} y\quad=\quad\underset{\sigma_{j_1},\sigma_{j_2}}{\text{minimize}} &\sigma_{j_1}\sqrt{\frac{1-\sigma_{j_2}^2}{\sigma_{j_1}^2-\sigma_{j_2}^2}}+\sigma_{j_2}\sqrt{\frac{\sigma_{j_1}^2-1}{\sigma_{j_1}^2-\sigma_{j_2}^2}}\\ \text{subject to} & y^*\begin{bmatrix}
\sigma_{j_1} & \\
 & \sigma_{j_2}
\end{bmatrix}^2y=1,&\sigma_{j_1}>1\\&y^*y=1&\sigma_{j_2}<1\end{array} y_1=\sqrt{\frac{1-\sigma_{j_2}^2}{\sigma_{j_1}^2-\sigma_{j_2}^2}} y_2=\sqrt{\frac{\sigma_{j_1}^2-1}{\sigma_{j_1}^2-\sigma_{j_2}^2}} A","['linear-algebra', 'optimization', 'normed-spaces', 'singular-values', 'qcqp']"
48,Gronwall lemma for system of linear differential inequalities,Gronwall lemma for system of linear differential inequalities,,"Let $u,v:[0,\infty)\to[0,\infty)$ satsfying the following system of differential inequalities: $$ u'(t)\leq a_1\,u(t) + a_2\,v(t) + a_0 \\[4pt] v'(t)\leq b_1\,u(t) + b_2\,v(t) + b_0 $$ for suitable coefficients $a_0,a_1,a_2,b_0,b_1,b_2\in\mathbb R\,.$ In particular I have $a_1,b_2<0\,$ and $\,0<a_2<|a_1|\,$ , $0<b_1<|b_2|\,$ . Is there a Gronwall lemma for this system of linear differential inqualities? Namely an (optimal) inequality of type $$ u(t) \leq F(t) \\ v(t) \leq G(t)$$ where the functions $F,G:[0,\infty)\to[0,\infty)$ depend on $u,v$ only through their initial values $u(0),v(0)$ ? I remind that for a single differential inequality $$ u'(t) \leq a_1\,u(t) + a_0 $$ the Gronwall lemma guarantees that $$u(t)\leq\, u(0)\,e^{a_1 t} + \frac{a_0}{a_1}\, (e^{a_1t}-1) $$ and it can be proven for example by bounding the derivative of $U(t)\equiv u(t) e^{-a_1t}$ and integrating on the inverval $[0,t]$ . Notice also that the bound is the solution of the differential equation $y'(t)=a_1\,y(t)+a_0\,$ , $y(0)=u(0)\,$ .","Let satsfying the following system of differential inequalities: for suitable coefficients In particular I have and , . Is there a Gronwall lemma for this system of linear differential inqualities? Namely an (optimal) inequality of type where the functions depend on only through their initial values ? I remind that for a single differential inequality the Gronwall lemma guarantees that and it can be proven for example by bounding the derivative of and integrating on the inverval . Notice also that the bound is the solution of the differential equation , .","u,v:[0,\infty)\to[0,\infty)  u'(t)\leq a_1\,u(t) + a_2\,v(t) + a_0 \\[4pt]
v'(t)\leq b_1\,u(t) + b_2\,v(t) + b_0  a_0,a_1,a_2,b_0,b_1,b_2\in\mathbb R\,. a_1,b_2<0\, \,0<a_2<|a_1|\, 0<b_1<|b_2|\,  u(t) \leq F(t) \\
v(t) \leq G(t) F,G:[0,\infty)\to[0,\infty) u,v u(0),v(0)  u'(t) \leq a_1\,u(t) + a_0  u(t)\leq\, u(0)\,e^{a_1 t} + \frac{a_0}{a_1}\, (e^{a_1t}-1)  U(t)\equiv u(t) e^{-a_1t} [0,t] y'(t)=a_1\,y(t)+a_0\, y(0)=u(0)\,","['linear-algebra', 'ordinary-differential-equations', 'systems-of-equations', 'functional-inequalities']"
49,complexification of the vector space $\Bbb R^+$,complexification of the vector space,\Bbb R^+,"Consider $\Bbb R$ and $\Bbb R^+$ as isomorphic vector spaces, with isomorphism $f(x)=\exp x.$ Is the complexification of $\Bbb R$ isomorphic to the complexification of $\Bbb R^+?$ What is the complexification of $\Bbb R^+?$ I understand that when going from $\Bbb R$ to $\Bbb R^+$ under $f$ we have $a+b:=ab$ and $kb:=b^k.$ Here is a relevant link: vector space .","Consider and as isomorphic vector spaces, with isomorphism Is the complexification of isomorphic to the complexification of What is the complexification of I understand that when going from to under we have and Here is a relevant link: vector space .",\Bbb R \Bbb R^+ f(x)=\exp x. \Bbb R \Bbb R^+? \Bbb R^+? \Bbb R \Bbb R^+ f a+b:=ab kb:=b^k.,"['linear-algebra', 'complex-analysis', 'complex-numbers', 'vector-spaces']"
50,Nullspace of a block matrix,Nullspace of a block matrix,,"Suppose I have $M \in \mathbb{R}^{2n\times 2n}$ such that $$M = \begin{bmatrix} A &B\\0  & C\end{bmatrix}$$ for some $A,B,C \in \mathbb{R}^{n \times n}$ . I wish to: (i) Find the nullspace of $M$ (ii) Prove that rank $(M) \geq \text{rank}(A) + \text{rank}(C)$ For (i) it is clear that for some vector $[x,  y]^T$ will be in the null space if and only if $y \in \text{null}(C)$ and that $Ax + By = 0$ , but I am unsure how to proceed from here. For (ii) I can see why this is true, but am not sure how to approach proving it. Thanks.","Suppose I have such that for some . I wish to: (i) Find the nullspace of (ii) Prove that rank For (i) it is clear that for some vector will be in the null space if and only if and that , but I am unsure how to proceed from here. For (ii) I can see why this is true, but am not sure how to approach proving it. Thanks.","M \in \mathbb{R}^{2n\times 2n} M = \begin{bmatrix} A &B\\0  & C\end{bmatrix} A,B,C \in \mathbb{R}^{n \times n} M (M) \geq \text{rank}(A) + \text{rank}(C) [x,  y]^T y \in \text{null}(C) Ax + By = 0","['linear-algebra', 'matrices', 'matrix-rank', 'block-matrices']"
51,PCA for image analysis - Eigendecomposition VS SVD,PCA for image analysis - Eigendecomposition VS SVD,,"I have learned about two different ways of doing PCA, one using Eigendecomposition and the other (IMO more intuitive) using SVD. Although both are ""just"" a change of basis, I struggle to see how one can use Eigendecomposition for dimensionality reduction in the context of object recognition. Situation: Given $n$ images $f_i$ , each written as an $m$ vector (training set) and centered around $0$ , we want to find a smaller representation using Eigenfaces (i.e. using principal components of a face images). Using SVD: Write the images as columns into a matrix $M := [f_1, f_2, ..., f_n]\in \mathbb{R}^{m \times n}$ . Apply (economical) SVD to get $U \in \mathbb{R}^{m \times n}, \Sigma \in \mathbb{R}^{n \times n}, V^T \in \mathbb{R}^{n \times n}$ . Note that $U$ holds orthonormal vectors in columns (our principal components), while $\Sigma \cdot V^T $ holds our weights for each such vector. To reduce dimension, we just truncate the matrices: keep first $k$ columns of $U$ , the upper left $k \times k$ submatrix of $\Sigma$ and the first $k$ rows of $V^T$ . Call them $U_k, \Sigma_k, V^T_k$ respectively and $W_k := \Sigma_k \cdot V^T_k$ . For any ""new"" input image $g$ centered around $0$ , to check how similar it is with any stored image $f_i$ , we can just compute $||(W_k)_{:,i} - U_k^T \cdot g||_2$ . Hence, we only now need to do $\mathcal{O}(n \cdot k)$ operations rather than $\mathcal{O}(n \cdot m)$ to compare $g$ with our dataset $M$ . This approach makes sense, as I can really see where the $W_k$ come from. I also find it intuitive how to reconstruct an (approximation of an) image given its weight vector $w$ , as all we need to do is multiply it from the left with $U_k$ . If we want to do an alternative approach using Eigendecomposition, I don't see how the above goal is achieved. Based on what I know, I would compute PCA using Eigendecomposition the following way: Same as SVD Do autocorrelation $C_M := \frac{1}{n} M M^T$ Diagonalize $C_M$ , get $\Phi, \Lambda$ out, where $\Phi$ holds the Eigenvectors in columns, $\Lambda$ is diagonal matrix of eigenvalues $\lambda_i$ . Problem: The issue I am having is that the PCA using Eigendecomp. is performed on the autocorrelation matrix and not on the original data. This indirection step makes me lose track of: How to compare similarity of a new image $g$ with the existing ones, similar to step (4) of SVD-based PCA. How to go back from PCA to reconstructed images. For that, I reckon we would need to reverse Autocorrelation, which I am not sure how to do. So far, all ressources I checked out only use Eigendecomposition as a didactic tool and then quickly switch over to the SVD based approach. They include but are not limited to: Math Stackexchange Lecture notes A Tutorial on Principal Component Analysis","I have learned about two different ways of doing PCA, one using Eigendecomposition and the other (IMO more intuitive) using SVD. Although both are ""just"" a change of basis, I struggle to see how one can use Eigendecomposition for dimensionality reduction in the context of object recognition. Situation: Given images , each written as an vector (training set) and centered around , we want to find a smaller representation using Eigenfaces (i.e. using principal components of a face images). Using SVD: Write the images as columns into a matrix . Apply (economical) SVD to get . Note that holds orthonormal vectors in columns (our principal components), while holds our weights for each such vector. To reduce dimension, we just truncate the matrices: keep first columns of , the upper left submatrix of and the first rows of . Call them respectively and . For any ""new"" input image centered around , to check how similar it is with any stored image , we can just compute . Hence, we only now need to do operations rather than to compare with our dataset . This approach makes sense, as I can really see where the come from. I also find it intuitive how to reconstruct an (approximation of an) image given its weight vector , as all we need to do is multiply it from the left with . If we want to do an alternative approach using Eigendecomposition, I don't see how the above goal is achieved. Based on what I know, I would compute PCA using Eigendecomposition the following way: Same as SVD Do autocorrelation Diagonalize , get out, where holds the Eigenvectors in columns, is diagonal matrix of eigenvalues . Problem: The issue I am having is that the PCA using Eigendecomp. is performed on the autocorrelation matrix and not on the original data. This indirection step makes me lose track of: How to compare similarity of a new image with the existing ones, similar to step (4) of SVD-based PCA. How to go back from PCA to reconstructed images. For that, I reckon we would need to reverse Autocorrelation, which I am not sure how to do. So far, all ressources I checked out only use Eigendecomposition as a didactic tool and then quickly switch over to the SVD based approach. They include but are not limited to: Math Stackexchange Lecture notes A Tutorial on Principal Component Analysis","n f_i m 0 M := [f_1, f_2, ..., f_n]\in \mathbb{R}^{m \times n} U \in \mathbb{R}^{m \times n}, \Sigma \in \mathbb{R}^{n \times n}, V^T \in \mathbb{R}^{n \times n} U \Sigma \cdot V^T  k U k \times k \Sigma k V^T U_k, \Sigma_k, V^T_k W_k := \Sigma_k \cdot V^T_k g 0 f_i ||(W_k)_{:,i} - U_k^T \cdot g||_2 \mathcal{O}(n \cdot k) \mathcal{O}(n \cdot m) g M W_k w U_k C_M := \frac{1}{n} M M^T C_M \Phi, \Lambda \Phi \Lambda \lambda_i g","['linear-algebra', 'statistics', 'eigenvalues-eigenvectors', 'matrix-decomposition', 'svd']"
52,The Convergence of Jacobi and Gauss-Seidel Iteration,The Convergence of Jacobi and Gauss-Seidel Iteration,,"I have a question, is there any example that the convergence of Jacobi is faster than Gauss-Seidel ?","I have a question, is there any example that the convergence of Jacobi is faster than Gauss-Seidel ?",,"['linear-algebra', 'numerical-linear-algebra']"
53,Cauchy-Schwarz-like inequality for the wedge product,Cauchy-Schwarz-like inequality for the wedge product,,"Summary of my question: Does $\| \mathbf a \wedge \mathbf b\| \leq \| \mathbf a \| \| \mathbf b\|$ hold for all $\mathbf a \in \Lambda^k(\mathbb R^n)$ and $\mathbf b \in \Lambda^\ell(\mathbb R^n)$ ? Some background: Given $\mathbb R^n$ with the standard inner product, we can define an inner product on the exterior powers $\Lambda^k(\mathbb R^n)$ by $\langle u_1 \wedge \cdots \wedge u_k, v_1 \wedge \cdots \wedge v_k\rangle = \det(\langle u_i, v_j\rangle)_{i,j=1}^k$ and then extending by linearity. As usual, for $\mathbf a \in \Lambda^k(\mathbb R^n)$ , let $\|\mathbf a\| = \sqrt{\langle \mathbf a, \mathbf a \rangle}$ . If $\mathbf a = u_1 \wedge \cdots \wedge u_k$ (i.e., if $\mathbf a$ is simple), then $\|\mathbf a \|$ is equal to the volume of the $k$ -dimensional parallelotope generated by $u_1, \ldots, u_k$ . When $\mathbf a$ and $\mathbf b$ are simple, then we can see that $\| \mathbf a \wedge \mathbf b\| \leq \| \mathbf a \| \| \mathbf b\|$ holds by using the geometric interpretation given above. When $\mathbf a = u_1 \wedge \cdots \wedge u_k + v_1 \wedge \cdots \wedge v_k$ and $\mathbf b$ is simple, then after some lengthy computations (expanding the inner product and Gram-Schmidt), I was able to show the inequality holds in this case as well. Is the inequality $\| \mathbf a \wedge \mathbf b\| \leq \| \mathbf a \| \| \mathbf b\|$ true in general? And is there a geometric interpretation of these quantities when $\mathbf a$ and $\mathbf b$ are not simple?","Summary of my question: Does hold for all and ? Some background: Given with the standard inner product, we can define an inner product on the exterior powers by and then extending by linearity. As usual, for , let . If (i.e., if is simple), then is equal to the volume of the -dimensional parallelotope generated by . When and are simple, then we can see that holds by using the geometric interpretation given above. When and is simple, then after some lengthy computations (expanding the inner product and Gram-Schmidt), I was able to show the inequality holds in this case as well. Is the inequality true in general? And is there a geometric interpretation of these quantities when and are not simple?","\| \mathbf a \wedge \mathbf b\| \leq \| \mathbf a \| \| \mathbf b\| \mathbf a \in \Lambda^k(\mathbb R^n) \mathbf b \in \Lambda^\ell(\mathbb R^n) \mathbb R^n \Lambda^k(\mathbb R^n) \langle u_1 \wedge \cdots \wedge u_k, v_1 \wedge \cdots \wedge v_k\rangle = \det(\langle u_i, v_j\rangle)_{i,j=1}^k \mathbf a \in \Lambda^k(\mathbb R^n) \|\mathbf a\| = \sqrt{\langle \mathbf a, \mathbf a \rangle} \mathbf a = u_1 \wedge \cdots \wedge u_k \mathbf a \|\mathbf a \| k u_1, \ldots, u_k \mathbf a \mathbf b \| \mathbf a \wedge \mathbf b\| \leq \| \mathbf a \| \| \mathbf b\| \mathbf a = u_1 \wedge \cdots \wedge u_k + v_1 \wedge \cdots \wedge v_k \mathbf b \| \mathbf a \wedge \mathbf b\| \leq \| \mathbf a \| \| \mathbf b\| \mathbf a \mathbf b","['linear-algebra', 'cauchy-schwarz-inequality', 'multilinear-algebra', 'exterior-algebra']"
54,"Prove: If two finite-dimensional vector spaces are isomorphic, then they have the same dimension.","Prove: If two finite-dimensional vector spaces are isomorphic, then they have the same dimension.",,"Please comment on the validity of my proof. Any tips and suggestions are appreciated. Prove: If two finite-dimensional vector spaces are isomorphic, then they have the same dimension. Let $B=$ { $b_1,...,b_n$ } be a basis for a finite-dimensional vector space $V$ . Let $W$ be a finite-dimensional vector space, and define $T: V→W$ as an isomorphism between them. Let $x∈V$ , then $T(x)=T(c_1b_1+...+c_nb_n)$ for some real scalars $c_1...c_n$ as $B$ is a basis for $V$ . Also, because $T$ is an isomorphism, $T$ is a linear transformation, onto, and one-to-one. Because $T$ is onto, for any $y∈W$ , $y=T(x)=T(c_1b_1+...+c_nb_n)=c_1T(b_1)+...+c_nT(b_n)$ . Thereofore, $span$ { $T(b_1),...T(b_n)$ }= $W$ . Let $c_1T(b_1)+...+c_nT(b_n)=0$ . Because $T$ is one-to-one and linear $c_1T(b_1)+...+c_nT(b_n)=T(c_1b_1+...+c_nb_n)=0$ implies that $c_1b_1+...+c_nb_n=0$ . But because $b_1...b_n$ is a basis, it is linearly independent and so $c_1=...=c_n=0$ . Therefore { $T(b_1),...T(b_n)$ } is a linearly independent set. So, { $T(b_1),...T(b_n)$ } is a basis for $W$ and is clearly $n$ dimensional. So, $dim(V)=dim(W)$ .","Please comment on the validity of my proof. Any tips and suggestions are appreciated. Prove: If two finite-dimensional vector spaces are isomorphic, then they have the same dimension. Let { } be a basis for a finite-dimensional vector space . Let be a finite-dimensional vector space, and define as an isomorphism between them. Let , then for some real scalars as is a basis for . Also, because is an isomorphism, is a linear transformation, onto, and one-to-one. Because is onto, for any , . Thereofore, { }= . Let . Because is one-to-one and linear implies that . But because is a basis, it is linearly independent and so . Therefore { } is a linearly independent set. So, { } is a basis for and is clearly dimensional. So, .","B= b_1,...,b_n V W T: V→W x∈V T(x)=T(c_1b_1+...+c_nb_n) c_1...c_n B V T T T y∈W y=T(x)=T(c_1b_1+...+c_nb_n)=c_1T(b_1)+...+c_nT(b_n) span T(b_1),...T(b_n) W c_1T(b_1)+...+c_nT(b_n)=0 T c_1T(b_1)+...+c_nT(b_n)=T(c_1b_1+...+c_nb_n)=0 c_1b_1+...+c_nb_n=0 b_1...b_n c_1=...=c_n=0 T(b_1),...T(b_n) T(b_1),...T(b_n) W n dim(V)=dim(W)","['linear-algebra', 'vector-spaces', 'proof-writing', 'solution-verification']"
55,"Spectral theorem, inner products and adjoints/transposes: what's the relation?","Spectral theorem, inner products and adjoints/transposes: what's the relation?",,"I only consider operators on finite-dimensional vector spaces. I'll write down my question and explain my motivation afterwards: The spectral theorem relates diagonalizability, which is a purely algebraic notion, with adjoints, which requires inner products. Why is that so? Is there a non-complex/real version of the spectral theorem? Consider the following version of the Spectral Theorem: Spectral Theorem : Every self-adjoint operator on a finite-dimensional (complex) inner product space admits a basis consisting of eigenvectors (""eigenbasis"") I am teaching Linear Algebra for the first time, so I am revisiting lots of concepts I studied several years ago, and just got to the Spectral Theorem. So this is a little funny, since ""diagonalizability""/existence of an eigenbasis does not relate directly to inner products in any obvious manner. Yes, I understand the point that self-adjointness of an operator $A$ takes in the proof: Every $A$ -invariant subspace has an $A$ -invariant complement. A similar statement holds for normal operators and eigenspaces. But the point seems to be that it just so happens that a condition related to inner product implies in a condition related to invariant subspaces, and it is not clear to me why that is so from a non-purely formal perspective. So that got me thinking: How would one be able to get a spectral theorem for non-complex/real spaces? The more I think the more I convince myself the condition needs to deal with some sort of transpose. But the problem is that transposes do not make much sense in general, and this is also another point: Inner products seem to just-so-happen allow us to define adjoints/transposes of operators. The point seems to be that, in general, transposes ""live"" on dual spaces, whereas Riesz representation allows us to go back to the level of the original spaces. So I would imagine the abstract version of the Spectral Theorem (if there is one) would need a condition relating an operator $T\colon V\to V$ and its formal transpose $T'\colon V'\to V'$ , perhaps passing to the bidual if necessary (as this is naturally isomorphic to the ground space), but I can't find any possible formula involving both $T$ and $T'$ .","I only consider operators on finite-dimensional vector spaces. I'll write down my question and explain my motivation afterwards: The spectral theorem relates diagonalizability, which is a purely algebraic notion, with adjoints, which requires inner products. Why is that so? Is there a non-complex/real version of the spectral theorem? Consider the following version of the Spectral Theorem: Spectral Theorem : Every self-adjoint operator on a finite-dimensional (complex) inner product space admits a basis consisting of eigenvectors (""eigenbasis"") I am teaching Linear Algebra for the first time, so I am revisiting lots of concepts I studied several years ago, and just got to the Spectral Theorem. So this is a little funny, since ""diagonalizability""/existence of an eigenbasis does not relate directly to inner products in any obvious manner. Yes, I understand the point that self-adjointness of an operator takes in the proof: Every -invariant subspace has an -invariant complement. A similar statement holds for normal operators and eigenspaces. But the point seems to be that it just so happens that a condition related to inner product implies in a condition related to invariant subspaces, and it is not clear to me why that is so from a non-purely formal perspective. So that got me thinking: How would one be able to get a spectral theorem for non-complex/real spaces? The more I think the more I convince myself the condition needs to deal with some sort of transpose. But the problem is that transposes do not make much sense in general, and this is also another point: Inner products seem to just-so-happen allow us to define adjoints/transposes of operators. The point seems to be that, in general, transposes ""live"" on dual spaces, whereas Riesz representation allows us to go back to the level of the original spaces. So I would imagine the abstract version of the Spectral Theorem (if there is one) would need a condition relating an operator and its formal transpose , perhaps passing to the bidual if necessary (as this is naturally isomorphic to the ground space), but I can't find any possible formula involving both and .",A A A T\colon V\to V T'\colon V'\to V' T T',"['linear-algebra', 'spectral-theory']"
56,"Salvaging $A-B$ and $A+B$ invertible imply $A,B$ invertible, for $A,B\in \mathbb{C}^{n\times n}$","Salvaging  and  invertible imply  invertible, for","A-B A+B A,B A,B\in \mathbb{C}^{n\times n}","Recently, a professor posed the following question on a midterm: If $A,B\in \mathbb{C}^{n\times n}$ and $A-B$ , $A+B$ are invertible, prove that $A,B$ are invertible. The question was meant to be, 'prove or disprove', as taking $A=\begin{bmatrix}0 & 1 \\ 0 & 0\end{bmatrix}$ and $B=A^T$ provides a counterexample. But I'm left wondering if we could add some condition to make the statement true, maybe with information about the Gershgorin discs or the spectrum of the matrix? In short, my question is: is there a condition $\mathcal{C}$ such that if $A-B$ and $A+B$ are invertible and satisfy $\mathcal{C}$ , then $A$ and $B$ are invertible as well?","Recently, a professor posed the following question on a midterm: If and , are invertible, prove that are invertible. The question was meant to be, 'prove or disprove', as taking and provides a counterexample. But I'm left wondering if we could add some condition to make the statement true, maybe with information about the Gershgorin discs or the spectrum of the matrix? In short, my question is: is there a condition such that if and are invertible and satisfy , then and are invertible as well?","A,B\in \mathbb{C}^{n\times n} A-B A+B A,B A=\begin{bmatrix}0 & 1 \\ 0 & 0\end{bmatrix} B=A^T \mathcal{C} A-B A+B \mathcal{C} A B","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'inverse', 'gershgorin-sets']"
57,every eigenvalue of $T$ has only one corresponding eigenvector up to a scalar multiplication,every eigenvalue of  has only one corresponding eigenvector up to a scalar multiplication,T,"For a linear transformation $T$ on a vector space $V$ of dimension $n .$ Suppose it is given that for some vector $\mathbf{v},$ the vectors $\mathbf{v}, T(\mathbf{v}), T^{2}(\mathbf{v}), \ldots, T^{n-1}(\mathbf{v})$ are linearly independent, then is it true that every eigenvalue of $T$ has only one corresponding eigenvector up to a scalar multiplication.","For a linear transformation on a vector space of dimension Suppose it is given that for some vector the vectors are linearly independent, then is it true that every eigenvalue of has only one corresponding eigenvector up to a scalar multiplication.","T V n . \mathbf{v}, \mathbf{v}, T(\mathbf{v}), T^{2}(\mathbf{v}), \ldots, T^{n-1}(\mathbf{v}) T","['linear-algebra', 'eigenvalues-eigenvectors', 'linear-transformations']"
58,Annihilator in infinite dimensional vector space,Annihilator in infinite dimensional vector space,,"I am working on the following question: Consider $U$ and $W$ subspaces of a finite-dimensional vector space $V$ . Show that $$(U \cap W)^{0} = U^{0} + W^{0}$$ Is it necessary for the $V$ dimension to be finite? I proved that $(U \cap W)^{0} = U^{0} + W^{0}$ and I know that it is necessary that the dimension of $V$ be finite so that $(U \cap W)^{0} \subset U^{0} + W^{0}$ , but I can't find a counterexample show this inclusion fails if the dimension of $V$ is infinite. Can someone help me? Thank you in advance.","I am working on the following question: Consider and subspaces of a finite-dimensional vector space . Show that Is it necessary for the dimension to be finite? I proved that and I know that it is necessary that the dimension of be finite so that , but I can't find a counterexample show this inclusion fails if the dimension of is infinite. Can someone help me? Thank you in advance.",U W V (U \cap W)^{0} = U^{0} + W^{0} V (U \cap W)^{0} = U^{0} + W^{0} V (U \cap W)^{0} \subset U^{0} + W^{0} V,"['linear-algebra', 'vector-spaces', 'dual-spaces']"
59,The natural map from the tensor algebra to the symmetric algebra has a section?,The natural map from the tensor algebra to the symmetric algebra has a section?,,"If $V$ is a finite-dimensional vector space over a field of characteristic zero, and $\pi$ is the projection from $T^n(V)$ (the $n$ -fold tensor product) to $S^n(V)$ the $n$ -fold symmetric product, I'm asked to show that $\pi$ has a section. My initial thought was that since $T^n(V)$ is a finite-dimensional vector space, and the projection $\pi$ is a surjective linear transformation, we have that there exists a right inverse of $\pi$ (i.e. a section). This seemed too simple to be true and didn't require any knowledge about tensor algebras. Am I missing something?","If is a finite-dimensional vector space over a field of characteristic zero, and is the projection from (the -fold tensor product) to the -fold symmetric product, I'm asked to show that has a section. My initial thought was that since is a finite-dimensional vector space, and the projection is a surjective linear transformation, we have that there exists a right inverse of (i.e. a section). This seemed too simple to be true and didn't require any knowledge about tensor algebras. Am I missing something?",V \pi T^n(V) n S^n(V) n \pi T^n(V) \pi \pi,"['linear-algebra', 'commutative-algebra', 'tensor-products']"
60,Dot product properties,Dot product properties,,"I want to prove or contradict the following claim: If we take two vectors $\mathbf{v}_1$ and $\mathbf{v}_2$ in $\mathbb{R}^{d}$ ( $d$ isn't neccesarily 2, so geometric proofs aren't available) and the angle between them, which is defined by $\cos(\alpha_{\mathbf{v}_1,\mathbf{v}_2}) = \frac{\mathbf{v}_1^T\mathbf{v}_2}{\Vert \mathbf{v}_1 \Vert \Vert \mathbf{v}_2 \Vert}$ the following holds: For any vector $\mathbf{u}$ s.t. $\text{sgn}(\mathbf{v}_1^T\mathbf{u}) = \text{sgn}(\mathbf{v}_2^T\mathbf{u}) = 1$ if we denote $\tilde{\mathbf{v}}_1 = \mathbf{v}_1+\mathbf{u}$ and $\tilde{\mathbf{v}}_2 = \mathbf{v}_2+\mathbf{u}$ we'll get $\alpha_{\tilde{\mathbf{v}}_1,\tilde{\mathbf{v}}_2}<\alpha_{\mathbf{v}_1,\mathbf{v}_2}$ For any vector $\mathbf{u}$ s.t. $\text{sgn}(\mathbf{v}_1^T\mathbf{u}) = \text{sgn}(\mathbf{v}_2^T\mathbf{u}) = -1$ if we denote $\tilde{\mathbf{v}}_1 = \mathbf{v}_1-\mathbf{u}$ and $\tilde{\mathbf{v}}_2 = \mathbf{v}_2-\mathbf{u}$ we'll get $\alpha_{\tilde{\mathbf{v}}_1,\tilde{\mathbf{v}}_2}<\alpha_{\mathbf{v}_1,\mathbf{v}_2}$ I am pretty confident the above holds, since I ran a lot of numerical simulations and it does seem to hold, i.e. I believe the claim needs to be proved and not contradicted. I attempted to use the algebraic definition of cosine with some algebraic tricks (triangle inequality etc) and it didn't work, same with the generalized cosine inequality (for vectors).","I want to prove or contradict the following claim: If we take two vectors and in ( isn't neccesarily 2, so geometric proofs aren't available) and the angle between them, which is defined by the following holds: For any vector s.t. if we denote and we'll get For any vector s.t. if we denote and we'll get I am pretty confident the above holds, since I ran a lot of numerical simulations and it does seem to hold, i.e. I believe the claim needs to be proved and not contradicted. I attempted to use the algebraic definition of cosine with some algebraic tricks (triangle inequality etc) and it didn't work, same with the generalized cosine inequality (for vectors).","\mathbf{v}_1 \mathbf{v}_2 \mathbb{R}^{d} d \cos(\alpha_{\mathbf{v}_1,\mathbf{v}_2}) = \frac{\mathbf{v}_1^T\mathbf{v}_2}{\Vert \mathbf{v}_1 \Vert \Vert \mathbf{v}_2 \Vert} \mathbf{u} \text{sgn}(\mathbf{v}_1^T\mathbf{u}) = \text{sgn}(\mathbf{v}_2^T\mathbf{u}) = 1 \tilde{\mathbf{v}}_1 = \mathbf{v}_1+\mathbf{u} \tilde{\mathbf{v}}_2 = \mathbf{v}_2+\mathbf{u} \alpha_{\tilde{\mathbf{v}}_1,\tilde{\mathbf{v}}_2}<\alpha_{\mathbf{v}_1,\mathbf{v}_2} \mathbf{u} \text{sgn}(\mathbf{v}_1^T\mathbf{u}) = \text{sgn}(\mathbf{v}_2^T\mathbf{u}) = -1 \tilde{\mathbf{v}}_1 = \mathbf{v}_1-\mathbf{u} \tilde{\mathbf{v}}_2 = \mathbf{v}_2-\mathbf{u} \alpha_{\tilde{\mathbf{v}}_1,\tilde{\mathbf{v}}_2}<\alpha_{\mathbf{v}_1,\mathbf{v}_2}",['linear-algebra']
61,Bound on difference of eigen projections of positive definite matrices,Bound on difference of eigen projections of positive definite matrices,,"Suppose I have two positive semidefinite matrices (and their eigendecompositions) $A = U \Lambda_A U'$ and $B = V \Lambda_B V'$ . I was wondering if $$ ||U_jU_j' - V_jV_j' ||  \leq C || A - B || $$ for some $C$ . Here, $U_j$ is the $j$ th column of $U$ . Hence, I'm asking if the eigenprojections are Lipschitz continuous with respect to the original matrices. I've went through Kato's praised book on perturbation theory, which is great by the way, but the theory there is mostly about when the matrices depend on a single parameter. Any help would be greatly appreciated.","Suppose I have two positive semidefinite matrices (and their eigendecompositions) and . I was wondering if for some . Here, is the th column of . Hence, I'm asking if the eigenprojections are Lipschitz continuous with respect to the original matrices. I've went through Kato's praised book on perturbation theory, which is great by the way, but the theory there is mostly about when the matrices depend on a single parameter. Any help would be greatly appreciated.","A = U \Lambda_A U' B = V \Lambda_B V' 
||U_jU_j' - V_jV_j' ||  \leq C || A - B ||
 C U_j j U","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'perturbation-theory', 'matrix-analysis']"
62,"Fourier-like family total in $L^2(-\pi,\pi)$",Fourier-like family total in,"L^2(-\pi,\pi)","Consider the Hilbert space $H=L^2(-\pi,\pi)$ . A subset of $H$ is said to be total in $H$ if the closure of its span is the whole $H$ . For instance, the Fourier basis $\{e^{in x}\}_{n\in\mathbb{Z}}$ is a Hilbert basis for $H$ and in particular it is total in $H$ . I am interested in families of the form $\{e^{is_n x}\}_{n\in\mathbb{Z}}$ , where $\mathcal{S}=\{s_n\}_{n\in\mathbb{Z}}$ is a strictly increasing sequence. Is there any easy condition on $\mathcal{S}$ which guarantees that such a family is total in $H$ ? I would guess that $S$ should at least be diverging, in order to capture arbitrarily high frequencies, but I'm not sure that it is actually a necessary condition. Of course not every increasing sequence $\mathcal{S}$ has this property. An easy counter example is any $\mathcal{S}\subsetneq \mathbb{Z}$ , since the obtained family would be a proper subset of the Fourier basis. However I guess there must be some easy and quite general criterion to check whether a sequence gives rise to a family total in $H$ .","Consider the Hilbert space . A subset of is said to be total in if the closure of its span is the whole . For instance, the Fourier basis is a Hilbert basis for and in particular it is total in . I am interested in families of the form , where is a strictly increasing sequence. Is there any easy condition on which guarantees that such a family is total in ? I would guess that should at least be diverging, in order to capture arbitrarily high frequencies, but I'm not sure that it is actually a necessary condition. Of course not every increasing sequence has this property. An easy counter example is any , since the obtained family would be a proper subset of the Fourier basis. However I guess there must be some easy and quite general criterion to check whether a sequence gives rise to a family total in .","H=L^2(-\pi,\pi) H H H \{e^{in x}\}_{n\in\mathbb{Z}} H H \{e^{is_n x}\}_{n\in\mathbb{Z}} \mathcal{S}=\{s_n\}_{n\in\mathbb{Z}} \mathcal{S} H S \mathcal{S} \mathcal{S}\subsetneq \mathbb{Z} H","['linear-algebra', 'functional-analysis', 'fourier-analysis', 'hilbert-spaces']"
63,A question on proving an equation to be an $n$-linear system in linear algebra,A question on proving an equation to be an -linear system in linear algebra,n,"While studying Determinants from text book Hoffman and Kunze, I have a in an argument in a theorem whose reasoning is not provided . Questions: 1st question is in underlined part of theorem. It's image : How did authors deduced that $A_{ij} D_{ij}(A) $ is n-linear function of A? Question 2:  How does author derived the last line which is ""Therefore $ E_{j} (A) $ = $(-1)^{k+j} $ .... . Can anyone please give some hints. Any help would be really appreciated","While studying Determinants from text book Hoffman and Kunze, I have a in an argument in a theorem whose reasoning is not provided . Questions: 1st question is in underlined part of theorem. It's image : How did authors deduced that is n-linear function of A? Question 2:  How does author derived the last line which is ""Therefore = .... . Can anyone please give some hints. Any help would be really appreciated",A_{ij} D_{ij}(A)   E_{j} (A)  (-1)^{k+j} ,[]
64,Dimension theorem (in infinite dimension) - all basis of any vector space have the same cardinality.,Dimension theorem (in infinite dimension) - all basis of any vector space have the same cardinality.,,"I am reading a note from author Justin Tatch Moore, in which he proves the Dimension Theorem, which says that all the basis of a vector space have the same cardinality (in infinite dimension), using Zorn's Lemma. The text can be found HERE! The idea is basically as follows: suppose that $A$ and $B$ are basis of the vector space $V$ , and define $P$ to be the set consisting of all linear transformations $T$ from a subspace $W$ of $V$ onto $W$ such that: $A \cap W$ and $B \cap W$ are each a basis for $W$ , and the restriction of $T$ to $A \cap W$ is a bijection between $A \cap W$ and $B \cap W.$ then we show using Zorn's Lemma that $P$ has a maximal element, which is a transformation $ T: W \rightarrow W $ with the properties (1) and (2). Note that if we show that $ W = V $ , the theorem ends due to the propertie (2). Hence to do this the argument starts as in the figure below My question is how to build two sets of sequences with the above four properties? The first two properties seem very reasonable, but the last two I have no idea how to do it. Can anyone help me with this?","I am reading a note from author Justin Tatch Moore, in which he proves the Dimension Theorem, which says that all the basis of a vector space have the same cardinality (in infinite dimension), using Zorn's Lemma. The text can be found HERE! The idea is basically as follows: suppose that and are basis of the vector space , and define to be the set consisting of all linear transformations from a subspace of onto such that: and are each a basis for , and the restriction of to is a bijection between and then we show using Zorn's Lemma that has a maximal element, which is a transformation with the properties (1) and (2). Note that if we show that , the theorem ends due to the propertie (2). Hence to do this the argument starts as in the figure below My question is how to build two sets of sequences with the above four properties? The first two properties seem very reasonable, but the last two I have no idea how to do it. Can anyone help me with this?",A B V P T W V W A \cap W B \cap W W T A \cap W A \cap W B \cap W. P  T: W \rightarrow W   W = V ,"['linear-algebra', 'vector-spaces', 'proof-explanation']"
65,"Function satisfying $f(v,w) = f(Uv,Uw)$ for any vectors $v,w$ and unitary $U$",Function satisfying  for any vectors  and unitary,"f(v,w) = f(Uv,Uw) v,w U","If I have a function $f: V\times V\to \mathbb{R}$ where $V$ is an inner product space over $\mathbb{R}$ and for all unitary matrices $U \in \mathcal{L}(V)$ I have $$f(v,w) = f(Uv,Uw)$$ can I conclude that $f$ is a function of the inner products $v\cdot w, v\cdot v, w\cdot w$ alone? I am motivated to ask this because I do not know of any other ways to combine two vectors into a real number which is invariant under $v\to U v$ (i.e. orthonormal basis independent). But I am failing to find a way to prove it. How about if the target space is not necessarily $\mathbb{R}$ ?",If I have a function where is an inner product space over and for all unitary matrices I have can I conclude that is a function of the inner products alone? I am motivated to ask this because I do not know of any other ways to combine two vectors into a real number which is invariant under (i.e. orthonormal basis independent). But I am failing to find a way to prove it. How about if the target space is not necessarily ?,"f: V\times V\to \mathbb{R} V \mathbb{R} U \in \mathcal{L}(V) f(v,w) = f(Uv,Uw) f v\cdot w, v\cdot v, w\cdot w v\to U v \mathbb{R}","['linear-algebra', 'functional-analysis', 'mathematical-physics']"
66,Show $A$ is Hermitian and find the orthonormal basis for $V$ in which $A$ is diagonalizable.,Show  is Hermitian and find the orthonormal basis for  in which  is diagonalizable.,A V A,"Let $\{e_1,e_2,e_4\}$ be an orthonormal basis for a complex unitary space $V$ . Let's define the vectors: $f_j=e_j-\frac14\sum\limits_{i=1}^4e_i, j\in\{1,2,3,4\}$ . Let $A\in\mathcal L(V), Ax:=\sum\limits_{j=1}^4\langle x,f_j\rangle f_j$ . Show $A$ is Hermitian and find the orthonormal basis for $V$ in which $A$ is diagonalizable. Note: typo  corrected. My attempt: Let's compute $f_1,f_2,f_3,f_4$ first. $\begin{aligned}f_j=e_j-\frac14\sum\limits_{i=1}^4, e_i\implies&f_1=\frac34e_1-\frac14(e_2+e_3+e_4)\\&f_2=\frac34e_2-\frac14(e_1+e_3+e_4)\\&f_3=\frac34e_3-\frac14(e_1+e_2+e_4)\\&f_4=\frac34e_4-\frac14(e_1+e_2+e_3)\end{aligned}$ $\begin{aligned}Ae_i&=\sum\limits_{j=1}^4\langle e_i,f_j\rangle f_j\implies Ae_1=\left\langle e_1,\frac34e_1-\frac14(e_2+e_3+e_4)\right\rangle f_1+\left\langle e_1,\frac34e_2-\frac14(e_1+e_3+e_4)\right\rangle f_2+\left\langle e_1,\frac34e_3-\frac14(e_1+e_2+e_4)\right\rangle f_3+\left\langle e_1,\frac34e_4-\frac14(e_1+e_2+e_3)\right\rangle f_4=\frac34f_1-\frac14(f_2+f_3+f_4)\end{aligned}$ $\ Ae_2=\frac34f_2-\frac14(f_1+f_3+f_4)\\Ae_3=\frac34f_3-\frac14(f_1+f_2+f_4)\\Ae_4=\frac34f_4-\frac14(f_1+f_2+f_3)$ Then, $$[A]_e^f=\begin{bmatrix}\frac34&-\frac14&-\frac14&-\frac14\\-\frac14&\frac34&-\frac14&-\frac14\\-\frac14&-\frac14&\frac34&-\frac14\\-\frac14&-\frac14&-\frac14&\frac34\end{bmatrix}$$ About the matrix representation of $A\in\mathcal L(V)$ : $A\in M_n(\Bbb R)\ \&\ A=A^\tau\ \implies A=A^*\iff A\ \text{is normal}\implies A\text{ is diagonalizable in some orthonormal basis}$ $\{a_1,a_2,a_3,a_4\}$ Let's find the eigenvalues and the corresponding eigenspaces using the formula derived here . According to the notation I used in the thread, $a_j=\frac34-\lambda\ \forall j\in\{1,2,3,4\}$ and $x=-\frac14$ . $$\det(A-\lambda I)=\begin{vmatrix}\frac34-\lambda&-\frac14&-\frac14&-\frac14\\-\frac14&\frac34-\lambda&-\frac14&-\frac14\\-\frac14&-\frac14&\frac34-\lambda&-\frac14\\-\frac14&-\frac14&-\frac14&\frac34-\lambda\end{vmatrix}=\left(\frac34-\lambda+\frac14\right)^4\left(1-\frac14\cdot 4\cdot\frac1{\frac34-\lambda+\frac14}\right)=-\lambda(1-\lambda)^3=\lambda(\lambda-1)(1-\lambda)^2\implies\sigma(A)=\{0,1\}$$ Let's use the fact $\Omega$ is the orthonormal complement of the row-space (explanation for the indeces orther I used) since $\boxed{E_A(0)\oplus E_A(1)=V}$ : Now, $E_A(0)=\ker(A)$ : $$\begin{bmatrix}\frac34&-\frac14&-\frac14&-\frac14\\-\frac14&\frac34&-\frac14&-\frac14\\-\frac14&-\frac14&\frac34&-\frac14\\-\frac14&-\frac14&-\frac14&\frac34\end{bmatrix}\sim\begin{bmatrix}1&1&1&-3\\1&1&-3&1\\1&-3&1&1\\-3&1&1&1\end{bmatrix}\sim\begin{bmatrix}1&1&1&-3\\0&0&-4&4\\0&-4&0&4\\0&4&4&-8\end{bmatrix}\sim\begin{bmatrix}1&1&1&-3\\0&0&-1&1\\0&-1&0&1\\0&0&0&0\end{bmatrix}\sim\begin{bmatrix}1&0&0&-1\\0&-1&0&1\\0&0&-1&1\\0&0&0&0\end{bmatrix}$$ $$\implies E_A(0)=\operatorname{span}\left\{\underbrace{\begin{bmatrix}1\\1\\1\\1\end{bmatrix}}_{v_4}\right\}$$ $E_A(1)=\ker(A-I)$ : $$\begin{bmatrix}-\frac14&-\frac14&-\frac14&-\frac14\\-\frac14&-\frac14&-\frac14&-\frac14\\-\frac14&-\frac14&-\frac14&-\frac14\\-\frac14&-\frac14&-\frac14&-\frac14\end{bmatrix}\sim\begin{bmatrix}1&1&1&1\\0&0&0&0\\0&0&0&0\\0&0&0&0\end{bmatrix}$$ $$\implies E_A(1)=\operatorname{span}\left\{\underbrace{\begin{bmatrix}1\\0\\0\\-1\end{bmatrix}}_{v_1},\underbrace{\begin{bmatrix}0\\-1\\0\\1\end{bmatrix}}_{v_2},\underbrace{\begin{bmatrix}0\\0\\-1\\1\end{bmatrix}}_{v_3}\right\}$$ Let's apply Gramm-Schmidt to the obtained basis for $V$ : $$a_1=\frac1{\|v_1\|}v_1=\frac1{\sqrt{2}}\begin{bmatrix}1\\0\\0\\-1\end{bmatrix}=b_1$$ $$\begin{aligned}b_2&=v_2-\langle v_2,a_1\rangle a_1\\&=\begin{bmatrix}0\\-1\\0\\1\end{bmatrix}-\frac1{\sqrt{2}}\left\langle\begin{bmatrix}0\\-1\\0\\1\end{bmatrix},\begin{bmatrix}1\\0\\0\\-1\end{bmatrix}\right\rangle\frac1{\sqrt{2}}\begin{bmatrix}0\\-1\\0\\1\end{bmatrix}\\&=\frac32\begin{bmatrix}0\\-1\\0\\1\end{bmatrix}\end{aligned}$$ $$a_2=\frac1{\|b_2\|}b_2=\frac1{\sqrt{2}}\begin{bmatrix}0\\-1\\0\\1\end{bmatrix}$$ $$\begin{aligned}b_3&=v_3-\langle v_3,a_1\rangle a_1-\langle v_3,a_2\rangle a_2\\&=\begin{bmatrix}0\\0\\-1\\1\end{bmatrix}-\frac1{\sqrt{2}}\left\langle\begin{bmatrix}0\\0\\-1\\1\end{bmatrix},\begin{bmatrix}1\\0\\0\\-1\end{bmatrix}\right\rangle\frac1{\sqrt{2}}\begin{bmatrix}1\\0\\0\\-1\end{bmatrix}-\frac1{\sqrt{2}}\left\langle\begin{bmatrix}0\\0\\-1\\1\end{bmatrix},\begin{bmatrix}0\\-1\\0\\1\end{bmatrix}\right\rangle\frac1{\sqrt{2}}\begin{bmatrix}0\\-1\\0\\1\end{bmatrix}\\&=\begin{bmatrix}0+\frac12\\0+\frac12\\-1\\1-\frac12-\frac12\end{bmatrix}=\begin{bmatrix}\frac12\\\frac12\\-1\\0\end{bmatrix}\end{aligned}$$ $$a_3=\frac1{\|b_3\|}b_3=\frac{\sqrt{6}}3\begin{bmatrix}\frac12\\\frac12\\-1\\0\end{bmatrix}$$ $$\begin{aligned}b_4&=v_4-\langle v_4,a_1\rangle a_1-\langle v_4,a_2\rangle a_2-\langle v_4,a_3\rangle a_3\\&=\begin{bmatrix}1\\1\\1\\1\end{bmatrix}-\frac1{\sqrt{2}}\left\langle\begin{bmatrix}1\\1\\1\\1\end{bmatrix},\begin{bmatrix}1\\0\\0\\-1\end{bmatrix}\right\rangle\frac1{\sqrt{2}}\begin{bmatrix}1\\0\\0\\-1\end{bmatrix}-\frac1{\sqrt{2}}\left\langle\begin{bmatrix}1\\1\\1\\1\end{bmatrix},\begin{bmatrix}0\\-1\\0\\1\end{bmatrix}\right\rangle\frac1{\sqrt{2}}\begin{bmatrix}0\\-1\\0\\1\end{bmatrix}-\frac{\sqrt{6}}3\left\langle\begin{bmatrix}1\\1\\1\\1\end{bmatrix},\begin{bmatrix}\frac12\\\frac12\\-1\\0\end{bmatrix}\right\rangle\frac{\sqrt{6}}3\begin{bmatrix}\frac12\\\frac12\\-1\\0\end{bmatrix}\\&=\begin{bmatrix}1\\1\\1\\1\end{bmatrix}\ \underline{\text{we can skip this step}}\end{aligned}$$ $$a_4=\frac1{\|b_4\|}b_4=\frac12\begin{bmatrix}1\\1\\1\\1\end{bmatrix}$$ Therefore, a Hermitian operator $A$ is diagonalizable in the orthonormal basis: $$\{a_1,a_2,a_3,a_4\}=\left\{\frac1{\sqrt{2}}\begin{bmatrix}1\\0\\0\\-1\end{bmatrix},\frac1{\sqrt{2}}\begin{bmatrix}0\\-1\\0\\1\end{bmatrix},\frac{\sqrt{6}}3\begin{bmatrix}\frac12\\\frac12\\-1\\0\end{bmatrix},\frac12\begin{bmatrix}1\\1\\1\\1\end{bmatrix}\right\}$$ May I ask if this is correct? If so, how could I improve my approach? Thank you in advance!","Let be an orthonormal basis for a complex unitary space . Let's define the vectors: . Let . Show is Hermitian and find the orthonormal basis for in which is diagonalizable. Note: typo  corrected. My attempt: Let's compute first. Then, About the matrix representation of : Let's find the eigenvalues and the corresponding eigenspaces using the formula derived here . According to the notation I used in the thread, and . Let's use the fact is the orthonormal complement of the row-space (explanation for the indeces orther I used) since : Now, : : Let's apply Gramm-Schmidt to the obtained basis for : Therefore, a Hermitian operator is diagonalizable in the orthonormal basis: May I ask if this is correct? If so, how could I improve my approach? Thank you in advance!","\{e_1,e_2,e_4\} V f_j=e_j-\frac14\sum\limits_{i=1}^4e_i, j\in\{1,2,3,4\} A\in\mathcal L(V), Ax:=\sum\limits_{j=1}^4\langle x,f_j\rangle f_j A V A f_1,f_2,f_3,f_4 \begin{aligned}f_j=e_j-\frac14\sum\limits_{i=1}^4, e_i\implies&f_1=\frac34e_1-\frac14(e_2+e_3+e_4)\\&f_2=\frac34e_2-\frac14(e_1+e_3+e_4)\\&f_3=\frac34e_3-\frac14(e_1+e_2+e_4)\\&f_4=\frac34e_4-\frac14(e_1+e_2+e_3)\end{aligned} \begin{aligned}Ae_i&=\sum\limits_{j=1}^4\langle e_i,f_j\rangle f_j\implies Ae_1=\left\langle e_1,\frac34e_1-\frac14(e_2+e_3+e_4)\right\rangle f_1+\left\langle e_1,\frac34e_2-\frac14(e_1+e_3+e_4)\right\rangle f_2+\left\langle e_1,\frac34e_3-\frac14(e_1+e_2+e_4)\right\rangle f_3+\left\langle e_1,\frac34e_4-\frac14(e_1+e_2+e_3)\right\rangle f_4=\frac34f_1-\frac14(f_2+f_3+f_4)\end{aligned} \ Ae_2=\frac34f_2-\frac14(f_1+f_3+f_4)\\Ae_3=\frac34f_3-\frac14(f_1+f_2+f_4)\\Ae_4=\frac34f_4-\frac14(f_1+f_2+f_3) [A]_e^f=\begin{bmatrix}\frac34&-\frac14&-\frac14&-\frac14\\-\frac14&\frac34&-\frac14&-\frac14\\-\frac14&-\frac14&\frac34&-\frac14\\-\frac14&-\frac14&-\frac14&\frac34\end{bmatrix} A\in\mathcal L(V) A\in M_n(\Bbb R)\ \&\ A=A^\tau\ \implies A=A^*\iff A\ \text{is normal}\implies A\text{ is diagonalizable in some orthonormal basis} \{a_1,a_2,a_3,a_4\} a_j=\frac34-\lambda\ \forall j\in\{1,2,3,4\} x=-\frac14 \det(A-\lambda I)=\begin{vmatrix}\frac34-\lambda&-\frac14&-\frac14&-\frac14\\-\frac14&\frac34-\lambda&-\frac14&-\frac14\\-\frac14&-\frac14&\frac34-\lambda&-\frac14\\-\frac14&-\frac14&-\frac14&\frac34-\lambda\end{vmatrix}=\left(\frac34-\lambda+\frac14\right)^4\left(1-\frac14\cdot 4\cdot\frac1{\frac34-\lambda+\frac14}\right)=-\lambda(1-\lambda)^3=\lambda(\lambda-1)(1-\lambda)^2\implies\sigma(A)=\{0,1\} \Omega \boxed{E_A(0)\oplus E_A(1)=V} E_A(0)=\ker(A) \begin{bmatrix}\frac34&-\frac14&-\frac14&-\frac14\\-\frac14&\frac34&-\frac14&-\frac14\\-\frac14&-\frac14&\frac34&-\frac14\\-\frac14&-\frac14&-\frac14&\frac34\end{bmatrix}\sim\begin{bmatrix}1&1&1&-3\\1&1&-3&1\\1&-3&1&1\\-3&1&1&1\end{bmatrix}\sim\begin{bmatrix}1&1&1&-3\\0&0&-4&4\\0&-4&0&4\\0&4&4&-8\end{bmatrix}\sim\begin{bmatrix}1&1&1&-3\\0&0&-1&1\\0&-1&0&1\\0&0&0&0\end{bmatrix}\sim\begin{bmatrix}1&0&0&-1\\0&-1&0&1\\0&0&-1&1\\0&0&0&0\end{bmatrix} \implies E_A(0)=\operatorname{span}\left\{\underbrace{\begin{bmatrix}1\\1\\1\\1\end{bmatrix}}_{v_4}\right\} E_A(1)=\ker(A-I) \begin{bmatrix}-\frac14&-\frac14&-\frac14&-\frac14\\-\frac14&-\frac14&-\frac14&-\frac14\\-\frac14&-\frac14&-\frac14&-\frac14\\-\frac14&-\frac14&-\frac14&-\frac14\end{bmatrix}\sim\begin{bmatrix}1&1&1&1\\0&0&0&0\\0&0&0&0\\0&0&0&0\end{bmatrix} \implies E_A(1)=\operatorname{span}\left\{\underbrace{\begin{bmatrix}1\\0\\0\\-1\end{bmatrix}}_{v_1},\underbrace{\begin{bmatrix}0\\-1\\0\\1\end{bmatrix}}_{v_2},\underbrace{\begin{bmatrix}0\\0\\-1\\1\end{bmatrix}}_{v_3}\right\} V a_1=\frac1{\|v_1\|}v_1=\frac1{\sqrt{2}}\begin{bmatrix}1\\0\\0\\-1\end{bmatrix}=b_1 \begin{aligned}b_2&=v_2-\langle v_2,a_1\rangle a_1\\&=\begin{bmatrix}0\\-1\\0\\1\end{bmatrix}-\frac1{\sqrt{2}}\left\langle\begin{bmatrix}0\\-1\\0\\1\end{bmatrix},\begin{bmatrix}1\\0\\0\\-1\end{bmatrix}\right\rangle\frac1{\sqrt{2}}\begin{bmatrix}0\\-1\\0\\1\end{bmatrix}\\&=\frac32\begin{bmatrix}0\\-1\\0\\1\end{bmatrix}\end{aligned} a_2=\frac1{\|b_2\|}b_2=\frac1{\sqrt{2}}\begin{bmatrix}0\\-1\\0\\1\end{bmatrix} \begin{aligned}b_3&=v_3-\langle v_3,a_1\rangle a_1-\langle v_3,a_2\rangle a_2\\&=\begin{bmatrix}0\\0\\-1\\1\end{bmatrix}-\frac1{\sqrt{2}}\left\langle\begin{bmatrix}0\\0\\-1\\1\end{bmatrix},\begin{bmatrix}1\\0\\0\\-1\end{bmatrix}\right\rangle\frac1{\sqrt{2}}\begin{bmatrix}1\\0\\0\\-1\end{bmatrix}-\frac1{\sqrt{2}}\left\langle\begin{bmatrix}0\\0\\-1\\1\end{bmatrix},\begin{bmatrix}0\\-1\\0\\1\end{bmatrix}\right\rangle\frac1{\sqrt{2}}\begin{bmatrix}0\\-1\\0\\1\end{bmatrix}\\&=\begin{bmatrix}0+\frac12\\0+\frac12\\-1\\1-\frac12-\frac12\end{bmatrix}=\begin{bmatrix}\frac12\\\frac12\\-1\\0\end{bmatrix}\end{aligned} a_3=\frac1{\|b_3\|}b_3=\frac{\sqrt{6}}3\begin{bmatrix}\frac12\\\frac12\\-1\\0\end{bmatrix} \begin{aligned}b_4&=v_4-\langle v_4,a_1\rangle a_1-\langle v_4,a_2\rangle a_2-\langle v_4,a_3\rangle a_3\\&=\begin{bmatrix}1\\1\\1\\1\end{bmatrix}-\frac1{\sqrt{2}}\left\langle\begin{bmatrix}1\\1\\1\\1\end{bmatrix},\begin{bmatrix}1\\0\\0\\-1\end{bmatrix}\right\rangle\frac1{\sqrt{2}}\begin{bmatrix}1\\0\\0\\-1\end{bmatrix}-\frac1{\sqrt{2}}\left\langle\begin{bmatrix}1\\1\\1\\1\end{bmatrix},\begin{bmatrix}0\\-1\\0\\1\end{bmatrix}\right\rangle\frac1{\sqrt{2}}\begin{bmatrix}0\\-1\\0\\1\end{bmatrix}-\frac{\sqrt{6}}3\left\langle\begin{bmatrix}1\\1\\1\\1\end{bmatrix},\begin{bmatrix}\frac12\\\frac12\\-1\\0\end{bmatrix}\right\rangle\frac{\sqrt{6}}3\begin{bmatrix}\frac12\\\frac12\\-1\\0\end{bmatrix}\\&=\begin{bmatrix}1\\1\\1\\1\end{bmatrix}\ \underline{\text{we can skip this step}}\end{aligned} a_4=\frac1{\|b_4\|}b_4=\frac12\begin{bmatrix}1\\1\\1\\1\end{bmatrix} A \{a_1,a_2,a_3,a_4\}=\left\{\frac1{\sqrt{2}}\begin{bmatrix}1\\0\\0\\-1\end{bmatrix},\frac1{\sqrt{2}}\begin{bmatrix}0\\-1\\0\\1\end{bmatrix},\frac{\sqrt{6}}3\begin{bmatrix}\frac12\\\frac12\\-1\\0\end{bmatrix},\frac12\begin{bmatrix}1\\1\\1\\1\end{bmatrix}\right\}","['linear-algebra', 'operator-theory', 'solution-verification', 'hermitian-matrices']"
67,Number of elements of permutation group,Number of elements of permutation group,,"Let $G$ be a permutation group of a set $X\neq \emptyset$ and let $x,y\in X$ . We define: \begin{align*}&G_x:=\{g\in G\mid g(x)=x\} \\ &G_{x\rightarrow y}:=\{g\in G\mid g(x)=y\} \\ &B:=\{y\in X\mid \exists g\in G: g(x)=y\}\end{align*} I have shown that $G_x$ is a subgroup of $G$ als that the set $\{G_{x\rightarrow y}\mid y\in B\}\subseteq 2^G$ is a partition of $G$ . Let $g\in G_{x\rightarrow y}$ and let $u\in G_x$ and $v\in G_{x\rightarrow y}$ and so it holds that $g\circ u\in G_{x\rightarrow y}$ and $g^{-1}\circ v\in G_x$ . The maps \begin{align*}\alpha:G_x\rightarrow G_{x\rightarrow y}, \ u\mapsto g\circ u \\ \beta: G_{x\rightarrow y}\rightarrow G_x, \ v\mapsto g^{-1}\circ v\end{align*} are to each other inverse bijections. Let $G$ be finite. I want to show that that $|G|=|B|\cdot |G_x|$ . Could you give a hint? We have that $\{G_{x\rightarrow y}\mid y\in B\}\subseteq 2^G$ is a partition of $G$ . Does this mean that $|G|=|\{G_{x\rightarrow y}\mid y\in B\}|$ ? If it is like that, then we have to show that $|\{G_{x\rightarrow y}\mid y\in B\}|=|B|\cdot |G_x|$ , right? It is also given a hint that we can use the idea that we use when we show that there are $48$ symmetries of a cube.","Let be a permutation group of a set and let . We define: I have shown that is a subgroup of als that the set is a partition of . Let and let and and so it holds that and . The maps are to each other inverse bijections. Let be finite. I want to show that that . Could you give a hint? We have that is a partition of . Does this mean that ? If it is like that, then we have to show that , right? It is also given a hint that we can use the idea that we use when we show that there are symmetries of a cube.","G X\neq \emptyset x,y\in X \begin{align*}&G_x:=\{g\in G\mid g(x)=x\} \\ &G_{x\rightarrow y}:=\{g\in G\mid g(x)=y\} \\ &B:=\{y\in X\mid \exists g\in G: g(x)=y\}\end{align*} G_x G \{G_{x\rightarrow y}\mid y\in B\}\subseteq 2^G G g\in G_{x\rightarrow y} u\in G_x v\in G_{x\rightarrow y} g\circ u\in G_{x\rightarrow y} g^{-1}\circ v\in G_x \begin{align*}\alpha:G_x\rightarrow G_{x\rightarrow y}, \ u\mapsto g\circ u \\ \beta: G_{x\rightarrow y}\rightarrow G_x, \ v\mapsto g^{-1}\circ v\end{align*} G |G|=|B|\cdot |G_x| \{G_{x\rightarrow y}\mid y\in B\}\subseteq 2^G G |G|=|\{G_{x\rightarrow y}\mid y\in B\}| |\{G_{x\rightarrow y}\mid y\in B\}|=|B|\cdot |G_x| 48","['linear-algebra', 'group-theory', 'permutations', 'set-partition']"
68,Matrix Eigenvalue/Positive definiteness optimization problem,Matrix Eigenvalue/Positive definiteness optimization problem,,"I have encountered a matrix optimization problem from a system of ODE's, however I am not very familiar with optimization. Let $A(\lambda)$ be a $4 \times 4$ matrix dependent on a value $\lambda >0$ (some entries are constants, others are scalar multiples of $\lambda$ ) where my goal is to maximize this value of $\lambda$ . However, there are some additional constraints: the symmetric part of matrix the matrix product $-CA(\lambda)$ is positive definite (or equiv. strictly positive eigenvalues). Where the symmetric part is defined as $M_{symmetric} = \frac{1}{2}(M+M^{T})$ $C$ symmetric $C$ preserves sign (Non-negative) Here $C$ is a $4 \times 4$ matrix, which is to be chosen freely as long as it satisfies these constraints (the goal is to find a smart choice of $C$ , maximizing this value of $\lambda$ ). I have made this work for $A$ a $2 \times 2$ matrix in Mathematica (only 3 variables in $C$ using NMaximize[]). However for large $C$ this doesn't work anymore. Is there any software to tackle this problem with? Do you know similar problems with known steps to take? Any guidance in direction would be very welcome!","I have encountered a matrix optimization problem from a system of ODE's, however I am not very familiar with optimization. Let be a matrix dependent on a value (some entries are constants, others are scalar multiples of ) where my goal is to maximize this value of . However, there are some additional constraints: the symmetric part of matrix the matrix product is positive definite (or equiv. strictly positive eigenvalues). Where the symmetric part is defined as symmetric preserves sign (Non-negative) Here is a matrix, which is to be chosen freely as long as it satisfies these constraints (the goal is to find a smart choice of , maximizing this value of ). I have made this work for a matrix in Mathematica (only 3 variables in using NMaximize[]). However for large this doesn't work anymore. Is there any software to tackle this problem with? Do you know similar problems with known steps to take? Any guidance in direction would be very welcome!",A(\lambda) 4 \times 4 \lambda >0 \lambda \lambda -CA(\lambda) M_{symmetric} = \frac{1}{2}(M+M^{T}) C C C 4 \times 4 C \lambda A 2 \times 2 C C,"['linear-algebra', 'matrices', 'optimization', 'math-software', 'positive-definite']"
69,On the matrix logarithm,On the matrix logarithm,,"Let $A\in M_{n\times n}(\mathbb{C})$ . We define $\displaystyle\ln A=\sum_{k=1}^\infty(-1)^{k+1}\frac{(A-I)^k}{k}$ This series is convergent in particular when $A-I$ is nilpotent. Is it true (and how do you prove it) that if $A-I$ is nilpotent, then $e^{\ln A}=A$ ? (Also what about $\ln e^A=A$ ?)","Let . We define This series is convergent in particular when is nilpotent. Is it true (and how do you prove it) that if is nilpotent, then ? (Also what about ?)",A\in M_{n\times n}(\mathbb{C}) \displaystyle\ln A=\sum_{k=1}^\infty(-1)^{k+1}\frac{(A-I)^k}{k} A-I A-I e^{\ln A}=A \ln e^A=A,"['linear-algebra', 'matrices', 'logarithms', 'power-series', 'analytic-functions']"
70,Prove $\sum_{i=1}^{n}v_i^t \cdot v_i = I$ for orthonormal base,Prove  for orthonormal base,\sum_{i=1}^{n}v_i^t \cdot v_i = I,"Let $$ \{v_1,..,v_n\}  $$ Be an orthonormal base in $R^n$ with the standard inner product. I need to prove that: $$ \sum_{i=1}^{n}v_i^t \cdot v_i = I $$ Where $v_i$ is a row vector. What i tried: I tried to look at an example but still - i dont feel its getting me somewhere. Lets take $R^2$ The base will be: $$ \{v_1,v_2\} $$ Let $v_1 = [a_1,a_2], v_2 = [b_1,b_2]$ As an orthonormal base we know that: $$ ||v_1|| = ||v_2|| = 1 $$ And: $$ <v_1,v_2> = 0 $$ Therefore: $$ ||v_1||^2 = a_1^2 + a_2^2 = 1, ||v_2||^2 = b_1^2 + b_2^2 = 1 $$ $$ <v_1,v_2> = a_1b_1 + a_2b_2 = 0 $$ $$ \sum_{i = 1}^{n = 2}v_i^t \cdot v_i =  \begin{bmatrix} a_1^2&a_2a_2 \\ a_2a_1&a_2^2\end{bmatrix} + \begin{bmatrix} b_1^2&b_2b_2 \\ b_2b_1&b_2^2\end{bmatrix} $$ But i dont see how i get here to $I_2$ ? Am i even in the right direction? what am i missing? I would prefer a hint than a full answer - as those are my homework. And thanks for the help.",Let Be an orthonormal base in with the standard inner product. I need to prove that: Where is a row vector. What i tried: I tried to look at an example but still - i dont feel its getting me somewhere. Lets take The base will be: Let As an orthonormal base we know that: And: Therefore: But i dont see how i get here to ? Am i even in the right direction? what am i missing? I would prefer a hint than a full answer - as those are my homework. And thanks for the help.,"
\{v_1,..,v_n\} 
 R^n 
\sum_{i=1}^{n}v_i^t \cdot v_i = I
 v_i R^2 
\{v_1,v_2\}
 v_1 = [a_1,a_2], v_2 = [b_1,b_2] 
||v_1|| = ||v_2|| = 1
 
<v_1,v_2> = 0
 
||v_1||^2 = a_1^2 + a_2^2 = 1, ||v_2||^2 = b_1^2 + b_2^2 = 1
 
<v_1,v_2> = a_1b_1 + a_2b_2 = 0
 
\sum_{i = 1}^{n = 2}v_i^t \cdot v_i = 
\begin{bmatrix} a_1^2&a_2a_2 \\ a_2a_1&a_2^2\end{bmatrix} + \begin{bmatrix} b_1^2&b_2b_2 \\ b_2b_1&b_2^2\end{bmatrix}
 I_2","['linear-algebra', 'inner-products']"
71,"Is there any way to visualize the inner product of two continuous functions on $[a,b]$.",Is there any way to visualize the inner product of two continuous functions on .,"[a,b]","Suppose we consider $C([a,b])$ and define an inner product on the vector space by $$\left<f,g\right>=\int_a^b f(t)g(t)dt$$ where $f,g$ are real valued functions on $[a,b]$ . My question is, can we visualize this inner product, i.e. how it graphically looks for two given continuous functions? I tried to visualize it as area under $f(t)g(t)$ curve between the two bounds but that does not help me much.I want a more clear visualization. A diagram would also help.","Suppose we consider and define an inner product on the vector space by where are real valued functions on . My question is, can we visualize this inner product, i.e. how it graphically looks for two given continuous functions? I tried to visualize it as area under curve between the two bounds but that does not help me much.I want a more clear visualization. A diagram would also help.","C([a,b]) \left<f,g\right>=\int_a^b f(t)g(t)dt f,g [a,b] f(t)g(t)","['linear-algebra', 'continuity', 'normed-spaces', 'inner-products', 'riemann-integration']"
72,When does multiplication by an orthogonal matrix preserves the eigenvalues?,When does multiplication by an orthogonal matrix preserves the eigenvalues?,,"Let $A$ be a real $n \times n$ matrix, with rank $\ge n-1$ . Suppose that the eigenvalues (counted with multiplicities) of $A$ are the same as the eigenvalues of $QA$ for some orthogonal matrix $Q$ . Must $Q$ be diagonal? The condition $\text{rank}(A)\ge n-1$ is necessary: If we allow $\text{rank}(A)< n-1$ , then one can take $A$ to be block diagonal with the $2 \times 2$ zero matrix as its first block. Then the entire $\text{O}(2) \times \text{Id}_{n-2}$ preserves the eigenvalues.","Let be a real matrix, with rank . Suppose that the eigenvalues (counted with multiplicities) of are the same as the eigenvalues of for some orthogonal matrix . Must be diagonal? The condition is necessary: If we allow , then one can take to be block diagonal with the zero matrix as its first block. Then the entire preserves the eigenvalues.",A n \times n \ge n-1 A QA Q Q \text{rank}(A)\ge n-1 \text{rank}(A)< n-1 A 2 \times 2 \text{O}(2) \times \text{Id}_{n-2},"['linear-algebra', 'eigenvalues-eigenvectors', 'matrix-equations', 'matrix-calculus', 'orthogonal-matrices']"
73,A property of a linear image of the cube,A property of a linear image of the cube,,"Let $[0,1]^3$ denote the unit cube in $\mathbb{R}^3$ . Let $L : \mathbb{R}^3 \to \mathbb{R}^2$ be a surjective linear map, and let $H := L([0,1]^3)$ (which is generically a hexagon). Can you provide a proof of this statement ? If $w_1$ , $w_2$ , and $w_1 + w_2$ are all in $H$ , then there exist $v_1$ , $v_2 \in [0,1]^3$ so that: $L(v_1)=w_1$ and $L(v_2)=w_2$ $v_1 + v_2 \in [0,1]^3$ Property 2. is the hard part.","Let denote the unit cube in . Let be a surjective linear map, and let (which is generically a hexagon). Can you provide a proof of this statement ? If , , and are all in , then there exist , so that: and Property 2. is the hard part.","[0,1]^3 \mathbb{R}^3 L : \mathbb{R}^3 \to \mathbb{R}^2 H := L([0,1]^3) w_1 w_2 w_1 + w_2 H v_1 v_2 \in [0,1]^3 L(v_1)=w_1 L(v_2)=w_2 v_1 + v_2 \in [0,1]^3","['linear-algebra', 'linear-programming', 'polyhedra']"
74,Converting inequalities into equalities by adding more variables,Converting inequalities into equalities by adding more variables,,"I was trying to solve a rather large system of equalities and inequalities and was stuck, until I realized that converting the inequalities into equalities by adding more variables showed that zero-vector was the only answer. But I'm not so sure if it's correct. So my question is, it is valid to do so? What I'm trying to do is to find the range of $x$ where the following is true: $$\mathbb{A} x\leq 0, x\geq 0$$ where $\mathbb{A}$ is a specific $m\times n$ coefficient matrix and $x$ is an $n\times 1$ vector. But is this the same as solving the following: $$\tilde{\mathbb{A}}\tilde{x}=0, \tilde{x}\geq 0$$ where $\tilde{\mathbb{A}}$ is an $m\times(n+m)$ matrix and $\tilde{x}$ is an $(n+m)\times 1$ vector. By solution, I mean writing the elements of the original $x$ in terms of the elements that are in $\tilde{x}$ but not $x$ .","I was trying to solve a rather large system of equalities and inequalities and was stuck, until I realized that converting the inequalities into equalities by adding more variables showed that zero-vector was the only answer. But I'm not so sure if it's correct. So my question is, it is valid to do so? What I'm trying to do is to find the range of where the following is true: where is a specific coefficient matrix and is an vector. But is this the same as solving the following: where is an matrix and is an vector. By solution, I mean writing the elements of the original in terms of the elements that are in but not .","x \mathbb{A} x\leq 0, x\geq 0 \mathbb{A} m\times n x n\times 1 \tilde{\mathbb{A}}\tilde{x}=0, \tilde{x}\geq 0 \tilde{\mathbb{A}} m\times(n+m) \tilde{x} (n+m)\times 1 x \tilde{x} x","['linear-algebra', 'inequality']"
75,Basis for span of tensor power of permutation matrices?,Basis for span of tensor power of permutation matrices?,,"Set $$ P^{(r)}=\{\sigma ^{\otimes r}\,|\,\sigma\,\, \textrm{ is an $n\times n$ permutation matrix}\} $$ We know that for $r=1$ $\mathbb{F}P^{(r)}$ is the space of all matrices over $\mathbb{F}$ with constant column and row sum. We can also show that this space is of dimension $(n-1)^2+1$ over any field, and compute an explixit basis for it. Namely $$ \{E_{11}+E_{ij}-E_{1j}-E_{i1}\,|\,1<i,\,j\leq n\}\cup\{\mathbb{I}\} $$ where $E_{ij}$ is the matrix with $ij$ entry $1$ and $0$ elsewhere and $\mathbb{I}$ is the identity matrix. Is there a similar result for $P^{(r)}$ defined as above, that is, an explicit closed form for its basis for any $r\in \mathbb{N}$ ? And is there a closed form for the dimension of $\mathbb{F}P^{(r)}$ ? I suppose one could use representation theoretic arguments by decomposing $\mathbb{F}P^{(r)}$ whenever it is semisimple, but I'm just wondering if things were as simple as the case where $r=1$ .","Set We know that for is the space of all matrices over with constant column and row sum. We can also show that this space is of dimension over any field, and compute an explixit basis for it. Namely where is the matrix with entry and elsewhere and is the identity matrix. Is there a similar result for defined as above, that is, an explicit closed form for its basis for any ? And is there a closed form for the dimension of ? I suppose one could use representation theoretic arguments by decomposing whenever it is semisimple, but I'm just wondering if things were as simple as the case where .","
P^{(r)}=\{\sigma ^{\otimes r}\,|\,\sigma\,\, \textrm{ is an n\times n permutation matrix}\}
 r=1 \mathbb{F}P^{(r)} \mathbb{F} (n-1)^2+1 
\{E_{11}+E_{ij}-E_{1j}-E_{i1}\,|\,1<i,\,j\leq n\}\cup\{\mathbb{I}\}
 E_{ij} ij 1 0 \mathbb{I} P^{(r)} r\in \mathbb{N} \mathbb{F}P^{(r)} \mathbb{F}P^{(r)} r=1","['linear-algebra', 'abstract-algebra', 'matrices', 'representation-theory', 'permutation-matrices']"
76,Eigenvalues of the hyperbolic Laplacian,Eigenvalues of the hyperbolic Laplacian,,"I am working on the following Linear Algebra problem: Let $k$ be a field, let $d$ be a positive integer, and let $P_d$ be the $k$ -vector space of polynomials of degree $\leq d$ in $k[x,y]$ . The hyperbolic Laplacian on polynomials of degree $d$ is the differential operator $D_d = -y^2(\frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2})$ . For each $d \leq 5$ , compute the eigenvalues of $D_d$ , and, for each eigenvalue, compute the dimension of the eigenspace. (Your answer will depend on $d$ .) Here is my work so far: Let $f(x,y) = \sum_{i = 0}^d \sum_{j = 0}^d a_{i,j}x^iy^j \in k[x,y]$ be a $d$ -dimensional polynomial, where $a_{i,j} \in k$ . Then $D_d(f) = \lambda f$ $\Rightarrow -y^2(\frac{\partial^2f}{\partial x^2} + \frac{\partial^2f}{\partial y^2}) = \lambda f$ $\Rightarrow \sum_{i = 0}^d \sum_{j = 0}^d i(1-i)a_{i,j}x^{i-2}y^{j+2} + \sum_{i = 0}^d \sum_{j = 0}^d j(1-j)a_{i,j}x^iy^j = \sum_{i = 0}^d \sum_{j = 0}^d a_{i,j} \lambda x^iy^j$ $\Rightarrow \sum_{i = 0}^d \sum_{j = 0}^d a_{i,j}[i(1-i)x^{i-2}y^{j+2} + (j(1-j)-\lambda)x^iy^j] = 0$ From the above equation, I'm struggling with finding the solutions for the eigenvalues $\lambda$ and their corresponding eigenspace dimensions. How do I deconstruct the above equation for my solutions for $\lambda$ ? It seems a bit overwhelming. Thanks!","I am working on the following Linear Algebra problem: Let be a field, let be a positive integer, and let be the -vector space of polynomials of degree in . The hyperbolic Laplacian on polynomials of degree is the differential operator . For each , compute the eigenvalues of , and, for each eigenvalue, compute the dimension of the eigenspace. (Your answer will depend on .) Here is my work so far: Let be a -dimensional polynomial, where . Then From the above equation, I'm struggling with finding the solutions for the eigenvalues and their corresponding eigenspace dimensions. How do I deconstruct the above equation for my solutions for ? It seems a bit overwhelming. Thanks!","k d P_d k \leq d k[x,y] d D_d = -y^2(\frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2}) d \leq 5 D_d d f(x,y) = \sum_{i = 0}^d \sum_{j = 0}^d a_{i,j}x^iy^j \in k[x,y] d a_{i,j} \in k D_d(f) = \lambda f \Rightarrow -y^2(\frac{\partial^2f}{\partial x^2} + \frac{\partial^2f}{\partial y^2}) = \lambda f \Rightarrow \sum_{i = 0}^d \sum_{j = 0}^d i(1-i)a_{i,j}x^{i-2}y^{j+2} + \sum_{i = 0}^d \sum_{j = 0}^d j(1-j)a_{i,j}x^iy^j = \sum_{i = 0}^d \sum_{j = 0}^d a_{i,j} \lambda x^iy^j \Rightarrow \sum_{i = 0}^d \sum_{j = 0}^d a_{i,j}[i(1-i)x^{i-2}y^{j+2} + (j(1-j)-\lambda)x^iy^j] = 0 \lambda \lambda","['linear-algebra', 'polynomials', 'vector-spaces', 'eigenvalues-eigenvectors']"
77,A question about the norm of an element in a field extension.,A question about the norm of an element in a field extension.,,"Background: Since $x^3\not\equiv2\pmod{7}, \forall x\in\mathbb{Z}$ , we can let $K=\mathbb{F_7}[\sqrt[3]{2}]$ so that $K$ is an extension of $\mathbb{F_7}$ , which can be think of as a finite-dimensional vector space over $\mathbb{F_7}$ with a basis $(1,\sqrt[3]{2},\sqrt[3]{4})$ . Then the norm of an element $\alpha=x+y\sqrt[3]{2}+z\sqrt[3]{4}$ is defined to be $N(\alpha)=x^3+2y^3+4y^3-6xyz$ , which is the determinant of the linear transformation $f_\alpha:K \rightarrow K$ defined as left multiplication by $\alpha$ in $K$ . My question is that if I have an element $\beta\in K$ having a unit norm, i.e. $N(\beta)=1$ , can I then conclude that $N(\beta\alpha)=N(\beta)N(\alpha)=N(\alpha)$ ? I.e., is $N$ a homomorphism? My attempt Observe that $f_{\beta\alpha}=f_\beta\circ f_\alpha$ , then $$N(\beta\alpha)=\det(f_{\beta\alpha})=\det(f_\beta)\det(f_\alpha)=N(\beta)N(\alpha).$$ My motivation: Then $x^3+2y^3+4z^3-6xyz=2$ has infinitely many integral solutions since we can just find an $\alpha$ s.t. $N(\alpha)=2$ and some non-identity $\beta$ s.t. $N(\beta)=1 \Rightarrow \forall n\in\mathbb{N}, N(\beta^n\alpha)=2$ , i.e. we have infinitely many solutions to the give cubic form.","Background: Since , we can let so that is an extension of , which can be think of as a finite-dimensional vector space over with a basis . Then the norm of an element is defined to be , which is the determinant of the linear transformation defined as left multiplication by in . My question is that if I have an element having a unit norm, i.e. , can I then conclude that ? I.e., is a homomorphism? My attempt Observe that , then My motivation: Then has infinitely many integral solutions since we can just find an s.t. and some non-identity s.t. , i.e. we have infinitely many solutions to the give cubic form.","x^3\not\equiv2\pmod{7}, \forall x\in\mathbb{Z} K=\mathbb{F_7}[\sqrt[3]{2}] K \mathbb{F_7} \mathbb{F_7} (1,\sqrt[3]{2},\sqrt[3]{4}) \alpha=x+y\sqrt[3]{2}+z\sqrt[3]{4} N(\alpha)=x^3+2y^3+4y^3-6xyz f_\alpha:K \rightarrow K \alpha K \beta\in K N(\beta)=1 N(\beta\alpha)=N(\beta)N(\alpha)=N(\alpha) N f_{\beta\alpha}=f_\beta\circ f_\alpha N(\beta\alpha)=\det(f_{\beta\alpha})=\det(f_\beta)\det(f_\alpha)=N(\beta)N(\alpha). x^3+2y^3+4z^3-6xyz=2 \alpha N(\alpha)=2 \beta N(\beta)=1 \Rightarrow \forall n\in\mathbb{N}, N(\beta^n\alpha)=2","['linear-algebra', 'field-theory']"
78,Is possible to show that the linear operator $T(\varphi)(x) = \int_{V_x\cap M} \varphi(y)\text{d}y$ has spectral radius $>0$.,Is possible to show that the linear operator  has spectral radius .,T(\varphi)(x) = \int_{V_x\cap M} \varphi(y)\text{d}y >0,"Fix some $σ>2/(3\sqrt{3})$ , let $M$ be the interval $[x_-,x_+]$ , where $$x_- = \text{unique real root of   $x^3 + \sigma = x$}$$ and $$x_+ = \text{unique real root of   $x^3 - \sigma = x$}.$$ Moreover, define the set $$V_x=\{z\in \mathbb{R};\ z = x^3+\omega \sigma,\ \mbox{for some $\omega\in[-1,1]$}\}.$$ Consider the Banach Space $\left(\mathcal C^0(M),\|\cdot\|_\infty\right)$ . Where $$\mathcal C^0(M):=\{f:M\to\mathbb{R};f\ \text{is continuous}\}\ \mbox{and }\|\phi\|_\infty=\sup_{x\in M}|\phi|.$$ Now, define the continuous linear map \begin{align*} T: \mathcal C^0(M)&\to \mathcal{C}^0 (M)\\ \varphi&\mapsto \left(x\mapsto\int_{V_x\cap M}\varphi(y)\text{d} y\right). \end{align*} My Question: Is it possible to guarantee that the continuous linear operator $T$ has a positive spectral radius? Remark: It is possible to show that $T$ is a compact operator.","Fix some , let be the interval , where and Moreover, define the set Consider the Banach Space . Where Now, define the continuous linear map My Question: Is it possible to guarantee that the continuous linear operator has a positive spectral radius? Remark: It is possible to show that is a compact operator.","σ>2/(3\sqrt{3}) M [x_-,x_+] x_- = \text{unique real root of 
 x^3 + \sigma = x} x_+ = \text{unique real root of 
 x^3 - \sigma = x}. V_x=\{z\in \mathbb{R};\ z = x^3+\omega \sigma,\ \mbox{for some \omega\in[-1,1]}\}. \left(\mathcal C^0(M),\|\cdot\|_\infty\right) \mathcal C^0(M):=\{f:M\to\mathbb{R};f\ \text{is continuous}\}\ \mbox{and }\|\phi\|_\infty=\sup_{x\in M}|\phi|. \begin{align*}
T: \mathcal C^0(M)&\to \mathcal{C}^0 (M)\\
\varphi&\mapsto \left(x\mapsto\int_{V_x\cap M}\varphi(y)\text{d} y\right).
\end{align*} T T","['linear-algebra', 'functional-analysis', 'banach-spaces', 'spectral-radius', 'function-spaces']"
79,"Given $W=ULV^T$ and a vector $\mathbf{x}$, can we compute $UL^kV^T\mathbf{x}$ without doing the SVD, for any integer k?","Given  and a vector , can we compute  without doing the SVD, for any integer k?",W=ULV^T \mathbf{x} UL^kV^T\mathbf{x},"Consider a matrix $W \in \mathbb{R}^{n\times m}$ with corresponding singular value decomposition, $W = ULV^T$ ,  and a vector $\mathbf{x} \in \mathbb{R}^{m}$ . Is it possible to compute matrix-vector-products of the form $U L^k V^T \mathbf{x}$ without explicitly computing the SVD? If $k$ is odd, this is easy. For example, $W W^{T} W\mathbf{x} = UL^3V^T\mathbf{x}$ . But I can't see an obvious way to solve this for even powers. For context, I want to avoid the SVD as computing the matrix vector products will typically be much cheaper (e.g. for $k \ll n)$ . Ideally, a working algorithm would have complexity $O(kmn)$ instead of $O(mn^2 + nm^2)$ .","Consider a matrix with corresponding singular value decomposition, ,  and a vector . Is it possible to compute matrix-vector-products of the form without explicitly computing the SVD? If is odd, this is easy. For example, . But I can't see an obvious way to solve this for even powers. For context, I want to avoid the SVD as computing the matrix vector products will typically be much cheaper (e.g. for . Ideally, a working algorithm would have complexity instead of .",W \in \mathbb{R}^{n\times m} W = ULV^T \mathbf{x} \in \mathbb{R}^{m} U L^k V^T \mathbf{x} k W W^{T} W\mathbf{x} = UL^3V^T\mathbf{x} k \ll n) O(kmn) O(mn^2 + nm^2),"['linear-algebra', 'numerical-linear-algebra', 'svd']"
80,Proof Verification: Showing a function is affine if its convex and concave,Proof Verification: Showing a function is affine if its convex and concave,,"I know this question has been asked several times but the answers don't really make sense to me (I'll explain misunderstandings:) Question: Suppose that a function $f: \mathbb R^n \rightarrow \mathbb{R}$ is both concave and convex. Prove that $f$ is an affine function. My solution uses: How to prove convex+concave=affine? as inspiration. However, I'm not sure if I did it correctly especially for the negative cases and I'm not exactly sure if I have showed that $g$ is linear in all cases. et $g(x)=f(x)-a$ , where $a=f(0)$ . Thus $g(0)=0$ . Also since $f$ is convex and concave, $g$ is convex and concave as well. Thus for $x,y \in \mathbb R^n $ , $0 \leq \lambda \leq 1$ , by inequalities resulting from convexity and concavity we have: $g(\lambda x +(1-\lambda) y)$ = $\lambda g(x) +(1-\lambda)g(y)$ . Does this mean $g$ is linear (why?, I don't get this from looking at the other stack-exchange posts) and hence $f$ is affine. Case 2: $\lambda >1$ . Note $x = (1/\lambda ) (\lambda x) + (1 - 1/\lambda) (0)$ . Note then $1/ \lambda \in [0,1]$ .  Thus $g(x)=g((1/\lambda) (\lambda x) + (1 - 1/\lambda) (0))$ = $1/\lambda \cdot g(\lambda x)+$$(1- 1/\lambda) \cdot g(0) $ . This means $g(x)=1/\lambda * g(\lambda x)$ . Hence $g(\lambda x)=\lambda g(x)$ . Case 3: $\lambda <0$ . Not sure what to do now.... Perhaps I could do : Case : $\lambda \leq -1$ . $$x=(-1/\lambda)(-\lambda x)+(1+1/\lambda)(0)$$ Note that $-1/\lambda \in  [0,1]$ $g(x)=g((-1/\lambda)(-\lambda x) + (1 + 1/\lambda) (0))$ = $-1/\lambda \cdot g(-\lambda x)+$$(1+1/\lambda) \cdot g(0) $ . This means $g(x)=-1/\lambda * g(-\lambda x)$ . Hence $g(-\lambda x)=-\lambda g(x)$ . Case: $-1< \lambda <0$ Note $-\lambda \in [0,1]$ . Thus $x=(-\lambda)(-1/\lambda \cdot x)+(1+\lambda)(0)$ $g(x)=g((-\lambda) (-1/\lambda \cdot x) + (1 + \lambda) (0))$ = $-\lambda \cdot g(-1/\lambda \cdot x)+$$(1+\lambda) \cdot g(0) $ . This means $g(x)=-\lambda * g(-1/\lambda \cdot x)$ . Hence $g(-1/\lambda \cdot x)=-1/ \lambda \cdot g(x)$ . Any help would much appreciated. Thanks. Thus, linear in all cases...not exactly sure if this is correct at all.","I know this question has been asked several times but the answers don't really make sense to me (I'll explain misunderstandings:) Question: Suppose that a function is both concave and convex. Prove that is an affine function. My solution uses: How to prove convex+concave=affine? as inspiration. However, I'm not sure if I did it correctly especially for the negative cases and I'm not exactly sure if I have showed that is linear in all cases. et , where . Thus . Also since is convex and concave, is convex and concave as well. Thus for , , by inequalities resulting from convexity and concavity we have: = . Does this mean is linear (why?, I don't get this from looking at the other stack-exchange posts) and hence is affine. Case 2: . Note . Note then .  Thus = . This means . Hence . Case 3: . Not sure what to do now.... Perhaps I could do : Case : . Note that = . This means . Hence . Case: Note . Thus = . This means . Hence . Any help would much appreciated. Thanks. Thus, linear in all cases...not exactly sure if this is correct at all.","f: \mathbb R^n \rightarrow \mathbb{R} f g g(x)=f(x)-a a=f(0) g(0)=0 f g x,y \in \mathbb R^n  0 \leq \lambda \leq 1 g(\lambda x +(1-\lambda) y) \lambda g(x) +(1-\lambda)g(y) g f \lambda >1 x = (1/\lambda ) (\lambda x) + (1 - 1/\lambda) (0) 1/ \lambda \in [0,1] g(x)=g((1/\lambda) (\lambda x) + (1 - 1/\lambda) (0)) 1/\lambda \cdot g(\lambda x)+(1- 1/\lambda) \cdot g(0)  g(x)=1/\lambda * g(\lambda x) g(\lambda x)=\lambda g(x) \lambda <0 \lambda \leq -1 x=(-1/\lambda)(-\lambda x)+(1+1/\lambda)(0) -1/\lambda \in  [0,1] g(x)=g((-1/\lambda)(-\lambda x) + (1 + 1/\lambda) (0)) -1/\lambda \cdot g(-\lambda x)+(1+1/\lambda) \cdot g(0)  g(x)=-1/\lambda * g(-\lambda x) g(-\lambda x)=-\lambda g(x) -1< \lambda <0 -\lambda \in [0,1] x=(-\lambda)(-1/\lambda \cdot x)+(1+\lambda)(0) g(x)=g((-\lambda) (-1/\lambda \cdot x) + (1 + \lambda) (0)) -\lambda \cdot g(-1/\lambda \cdot x)+(1+\lambda) \cdot g(0)  g(x)=-\lambda * g(-1/\lambda \cdot x) g(-1/\lambda \cdot x)=-1/ \lambda \cdot g(x)","['real-analysis', 'linear-algebra', 'analysis', 'optimization', 'convex-analysis']"
81,Intuitive understanding behind exterior algebra construction,Intuitive understanding behind exterior algebra construction,,"I'm trying to get deeper intuition into the exterior algebra construction on a finite dimensional $\mathbb{R}$ -vector space. Our accustomed notion of volume given by measure is neither multi-linear nor anti-symmetric, so I don't buy construction of a 'volume function' as an a priori motivation for an exterior algebra. It's great that $v_1 \wedge \cdots \wedge v_n = \alpha \ e_1 \wedge \cdots \wedge e_n$ computes the (signed) volume $\alpha$ of the parallelotope spanned by these vectors. But this fact seems rather arbitrary and a priori unexpected. It would be nice to have a narrative as to why constructing an exterior algebra on a vector space is just the natural thing to do. For instance, generalizing from metric spaces to topological spaces is very natural once we realize that metrics just generate open sets, and that continuity of functions can be characterized by their behavior on open sets alone. Is there any reason why one would intuitively anticipate beforehand that constructing an alternating algebra on a vector space would give a device to compute volumes, detect linear dependence etc.? Or should the recognition of these facts just be considered a random encounter in the process of experimentation with mathematical constructs?","I'm trying to get deeper intuition into the exterior algebra construction on a finite dimensional -vector space. Our accustomed notion of volume given by measure is neither multi-linear nor anti-symmetric, so I don't buy construction of a 'volume function' as an a priori motivation for an exterior algebra. It's great that computes the (signed) volume of the parallelotope spanned by these vectors. But this fact seems rather arbitrary and a priori unexpected. It would be nice to have a narrative as to why constructing an exterior algebra on a vector space is just the natural thing to do. For instance, generalizing from metric spaces to topological spaces is very natural once we realize that metrics just generate open sets, and that continuity of functions can be characterized by their behavior on open sets alone. Is there any reason why one would intuitively anticipate beforehand that constructing an alternating algebra on a vector space would give a device to compute volumes, detect linear dependence etc.? Or should the recognition of these facts just be considered a random encounter in the process of experimentation with mathematical constructs?",\mathbb{R} v_1 \wedge \cdots \wedge v_n = \alpha \ e_1 \wedge \cdots \wedge e_n \alpha,"['linear-algebra', 'abstract-algebra', 'vector-spaces', 'soft-question', 'multilinear-algebra']"
82,Matrix Multiplication - Undefined Product,Matrix Multiplication - Undefined Product,,"I am learning linear algebra for a machine learning class and have a question about matrix multiplication. The product of two matrices is undefined whenever the rows of the first matrix (reading right to left) do not match the column of the second matrix. However, say that I would need (for some reason) to multiply a 3x3 matrix by a 2x2 one. Couldn't I just complete the operation by adding the ""missing row"" with coordinates [0,0]? I am thinking about it because if matrices represent linear transformations of space, then a 2x2 matrix represent a two dimensional transformation. However, isn't a two dimensional transformation simply a transformation where every other dimension is equal to 0? To illustrate that, let's say I want to apply a linear transformation [-1,0;0,1] to vector [3,3]. The resulting vector would be a two dimensional vector [-3,3]. Now let's say that after this transformation I want to apply another transformation to the same vector, but this time in three dimensions. To keep the example as simple as possible let's use the identity transformation for this: [1,0,0;0,1,0;0,0,1]. Doing this would require to multiply the 3x3 matrix [1,0,0;0,1,0;0,0,1] by the 2x3 one [-1,0;0,1], which is technically not possible. However, if I apply the method above (i.e. adding a third row with all 0 to the 2x3 matrix) I would still be able to compute the transformation and get the result [-1,0,0;0,1,0]. I can then multiply this to my original vector and get [-3,3,0]. The only difference I can see between [-3,3,0] and [-3,3] is that in the first one I am just ""explicitly showing"" the third dimension as having coordinate 0, whilst in the second I am keeping this implicit. What am I missing here? Regards, Federico","I am learning linear algebra for a machine learning class and have a question about matrix multiplication. The product of two matrices is undefined whenever the rows of the first matrix (reading right to left) do not match the column of the second matrix. However, say that I would need (for some reason) to multiply a 3x3 matrix by a 2x2 one. Couldn't I just complete the operation by adding the ""missing row"" with coordinates [0,0]? I am thinking about it because if matrices represent linear transformations of space, then a 2x2 matrix represent a two dimensional transformation. However, isn't a two dimensional transformation simply a transformation where every other dimension is equal to 0? To illustrate that, let's say I want to apply a linear transformation [-1,0;0,1] to vector [3,3]. The resulting vector would be a two dimensional vector [-3,3]. Now let's say that after this transformation I want to apply another transformation to the same vector, but this time in three dimensions. To keep the example as simple as possible let's use the identity transformation for this: [1,0,0;0,1,0;0,0,1]. Doing this would require to multiply the 3x3 matrix [1,0,0;0,1,0;0,0,1] by the 2x3 one [-1,0;0,1], which is technically not possible. However, if I apply the method above (i.e. adding a third row with all 0 to the 2x3 matrix) I would still be able to compute the transformation and get the result [-1,0,0;0,1,0]. I can then multiply this to my original vector and get [-3,3,0]. The only difference I can see between [-3,3,0] and [-3,3] is that in the first one I am just ""explicitly showing"" the third dimension as having coordinate 0, whilst in the second I am keeping this implicit. What am I missing here? Regards, Federico",,"['linear-algebra', 'matrices', 'linear-transformations']"
83,Proof verification: if $W_1 \subseteq W_2$ then $\dim(W_1) \le \dim(W_2)$,Proof verification: if  then,W_1 \subseteq W_2 \dim(W_1) \le \dim(W_2),"Let $W_1$ and $W_2$ be subspaces of vector space $V$ . Prove that   If $W_1 \subseteq W_2$ then $\dim(W_1) \le \dim(W_2)$ . My proof: Let $v_1, ...,v_n$ be base vectors of vector space $W_1$ and $W_1 \subseteq W_2$ . Then $\dim(W_1) = n$ . From the assumptions we have that $v_1, ..., v_n \in W_2$ and because they are base vectors in $W_1$ they are lineary independent. Therefore  if $\operatorname{span}(v_1, ...,v_n) = W_2$ then $\dim(W_2) = n = \dim(W_1)$ . Otherwise there exists vector $v_{n+1} \in W_2$ such that $v_{n+1}$ is not linear combination of $v_1, ...,v_2$ and therefore $\dim(W_2) \ge n+1$ . Hence $\dim(W_1) \le \dim(W_2)$ .",Let and be subspaces of vector space . Prove that   If then . My proof: Let be base vectors of vector space and . Then . From the assumptions we have that and because they are base vectors in they are lineary independent. Therefore  if then . Otherwise there exists vector such that is not linear combination of and therefore . Hence .,"W_1 W_2 V W_1 \subseteq W_2 \dim(W_1) \le \dim(W_2) v_1, ...,v_n W_1 W_1 \subseteq W_2 \dim(W_1) = n v_1, ..., v_n \in W_2 W_1 \operatorname{span}(v_1, ...,v_n) = W_2 \dim(W_2) = n = \dim(W_1) v_{n+1} \in W_2 v_{n+1} v_1, ...,v_2 \dim(W_2) \ge n+1 \dim(W_1) \le \dim(W_2)","['linear-algebra', 'proof-verification', 'vector-spaces']"
84,Calculate Gram matrix from distance matrix,Calculate Gram matrix from distance matrix,,"I've been trying to wrap my head around this homework question we've gotten for a few days but made little progress. We've been instructed, given a euclidean distance matrix of $n$ coordinates in $\mathbb R^2$ to calculate the Gram matrix of the coordinates, namely, $X^TX$ if $X$ is a $2 \times n$ matrix whose columns are the coordinates (note we don't have $X$ ). We've also been told that the center of mass (namely the average of the coordinates) is exactly (0,0). As the second part of the question, we've been given a $312 \times 312$ distance matrix and the first 5 coordinates and have been asked to find the full coordinate matrix. I've tried to use eigenvalue decomposition and calculate $U\sqrt D$ assuming it'd satisfy the demands but I am not sure how to use the coordinates we have been given. It is important to note that for the second part it is not known what the center of mass is. I tried searching on here but only found solutions for when (0,0) is $x_1$ and I am not sure how to move from there. Thank you in advance!","I've been trying to wrap my head around this homework question we've gotten for a few days but made little progress. We've been instructed, given a euclidean distance matrix of coordinates in to calculate the Gram matrix of the coordinates, namely, if is a matrix whose columns are the coordinates (note we don't have ). We've also been told that the center of mass (namely the average of the coordinates) is exactly (0,0). As the second part of the question, we've been given a distance matrix and the first 5 coordinates and have been asked to find the full coordinate matrix. I've tried to use eigenvalue decomposition and calculate assuming it'd satisfy the demands but I am not sure how to use the coordinates we have been given. It is important to note that for the second part it is not known what the center of mass is. I tried searching on here but only found solutions for when (0,0) is and I am not sure how to move from there. Thank you in advance!",n \mathbb R^2 X^TX X 2 \times n X 312 \times 312 U\sqrt D x_1,"['linear-algebra', 'matrices']"
85,How to show the following problem of determinant,How to show the following problem of determinant,,"If the matrices $A,B\in M_{3}(\Bbb{Z})$ are singular and $AB=BA$ , show that the number $$\det(A^3+B^3)+\det(A^3-B^3)$$ is the double of a perfect cube. I have considered the polynomial $$\det(A+xB)=\det A+mx+nx^2+x^3\det B$$ From this how I can show","If the matrices are singular and , show that the number is the double of a perfect cube. I have considered the polynomial From this how I can show","A,B\in M_{3}(\Bbb{Z}) AB=BA \det(A^3+B^3)+\det(A^3-B^3) \det(A+xB)=\det A+mx+nx^2+x^3\det B","['linear-algebra', 'matrices', 'determinant']"
86,Request for info about Real Analysis to a beginner [closed],Request for info about Real Analysis to a beginner [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 5 years ago . Improve this question So I'm a junior HS student very interested in mathematics and I think I have pretty good chances to get admitted into Stanford online HS and take a university level real analysis course that's worth 5 credits. It also says that a course on linear algebra and multi variable calculus is strongly recommended, but I have limited knowledge in those fields. What should I expect? What work should I do in order not to be overwhelmed and fail the course? Should I just switch to Elementary Number Theory? Thanks in advance and sorry if that was too long :)","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 5 years ago . Improve this question So I'm a junior HS student very interested in mathematics and I think I have pretty good chances to get admitted into Stanford online HS and take a university level real analysis course that's worth 5 credits. It also says that a course on linear algebra and multi variable calculus is strongly recommended, but I have limited knowledge in those fields. What should I expect? What work should I do in order not to be overwhelmed and fail the course? Should I just switch to Elementary Number Theory? Thanks in advance and sorry if that was too long :)",,"['real-analysis', 'calculus', 'linear-algebra', 'number-theory', 'education']"
87,The number of elements in a set of matrices with some properties,The number of elements in a set of matrices with some properties,,"Given $M$ comprised of $n\times n$ matrices, which satisfies $I_n \in M$ and $0_{n} \not\in M$ If $A,B \in M$ ,  then $AB \in M$ or $-AB \in M$ If $A,B \in M$ ,  then $AB = BA $ or $AB = -BA$ If $A\in M$ and $A\ne I_n$ , then there exists $B \in M$ such that $AB=-BA$ Prove that the number of elements in $M$ in less than $2 n^2$ . Some thoughts For the condition 4 , we can say the corresponding $B\ne I_n,A$ . Because if $B=I_n$ , we get $A=0$ , a contradiction. If $B= A$ then $AB=0$ , which contradicts the condition 2. Thus we can consider $M$ as the set of such pairs $(A,B)$ . But how to move on? Any hints? Thank you in advance! Added It's easy to see for all $A \in M$ , $A^2$ and $-A^2$ commute with all the matrices in $M$ . Thus from conditions 2 and 4 we get $A^2 = I$ or $-I$ , which might help.","Given comprised of matrices, which satisfies and If ,  then or If ,  then or If and , then there exists such that Prove that the number of elements in in less than . Some thoughts For the condition 4 , we can say the corresponding . Because if , we get , a contradiction. If then , which contradicts the condition 2. Thus we can consider as the set of such pairs . But how to move on? Any hints? Thank you in advance! Added It's easy to see for all , and commute with all the matrices in . Thus from conditions 2 and 4 we get or , which might help.","M n\times n I_n \in M 0_{n} \not\in M A,B \in M AB \in M -AB \in M A,B \in M AB = BA  AB = -BA A\in M A\ne I_n B \in M AB=-BA M 2 n^2 B\ne I_n,A B=I_n A=0 B= A AB=0 M (A,B) A \in M A^2 -A^2 M A^2 = I -I","['linear-algebra', 'abstract-algebra', 'matrices']"
88,How do the eigenvalues change if we change the diagonal entries of the matrix?,How do the eigenvalues change if we change the diagonal entries of the matrix?,,"Suppose $A \in M_n(\mathbb R)$ is stable. By stable, we mean the eigenvalues are all on the left open half plane of $\mathbb C$ . Now if we decrease the value of $A_{11}$ , does the matrix remain stable? I first thought in terms of Gershgorin Disks. If we decrease the entry $A_{11}$ , the center of corresponding disk would move to the left of the real axis. But then I realized this is not enough since we only know the eigenvalues are contained in the union of all disks. However, I could not see a counterexample. Alternatively, the question is a perturbation with rank-one matrix, i.e., we want to know whether $A-t e_1e_1^T$ remains stable for $t > 0$ where $e_1 = (1, 0, \dots, 0)^T$ .","Suppose is stable. By stable, we mean the eigenvalues are all on the left open half plane of . Now if we decrease the value of , does the matrix remain stable? I first thought in terms of Gershgorin Disks. If we decrease the entry , the center of corresponding disk would move to the left of the real axis. But then I realized this is not enough since we only know the eigenvalues are contained in the union of all disks. However, I could not see a counterexample. Alternatively, the question is a perturbation with rank-one matrix, i.e., we want to know whether remains stable for where .","A \in M_n(\mathbb R) \mathbb C A_{11} A_{11} A-t e_1e_1^T t > 0 e_1 = (1, 0, \dots, 0)^T","['linear-algebra', 'eigenvalues-eigenvectors', 'perturbation-theory', 'matrix-analysis', 'gershgorin-sets']"
89,"$M_n(R)/[M_n(R), M_n(R)] \cong R/[R,R]$",,"M_n(R)/[M_n(R), M_n(R)] \cong R/[R,R]","If $R$ is an associative ring then $[R,R]$ is the subgroup generated by the elements $[r,s]= rs-sr,$ for $r,s\in R$ . Show that $Trace : M_n(R)\longrightarrow R/[R,R]$ induces an isomorphism $$ M_n(R)/[M_n(R),M_n(R)] \longrightarrow R/[R,R]$$ What I have tried so far is this: I have a map $trace : M_n(R) \longrightarrow R$ the usual trace which is surjective Then $j : R\longrightarrow R/[R,R]$ which is also surjective. So I have a surjection $Trace$ ( $(j\circ trace))$ form $ M_n(R)\longrightarrow R/[R,R]$ Now I need to show that Kernel(Trace) is my $[M_n(R),M_n(R)] $ Since $trace [AB-BA] \in [R,R]$ I have one way containment which is: $$ [M_n(R),M_n(R)] \subset Kernel(Trace)$$ I am having trouble in proving $$Kernel(Trace)\subset [M_n(R),M_n(R)]$$ What I mistakenly proved is that for any $r\in [R,R]$ there exists $A,B \in M_n(R)$ such that $trace(AB-BA) = r$ . Which doesn't help me. So if you guys could help me out I will be delighted. Thank you.",If is an associative ring then is the subgroup generated by the elements for . Show that induces an isomorphism What I have tried so far is this: I have a map the usual trace which is surjective Then which is also surjective. So I have a surjection ( form Now I need to show that Kernel(Trace) is my Since I have one way containment which is: I am having trouble in proving What I mistakenly proved is that for any there exists such that . Which doesn't help me. So if you guys could help me out I will be delighted. Thank you.,"R [R,R] [r,s]= rs-sr, r,s\in R Trace : M_n(R)\longrightarrow R/[R,R]  M_n(R)/[M_n(R),M_n(R)] \longrightarrow R/[R,R] trace : M_n(R) \longrightarrow R j : R\longrightarrow R/[R,R] Trace (j\circ trace))  M_n(R)\longrightarrow R/[R,R] [M_n(R),M_n(R)]  trace [AB-BA] \in [R,R]  [M_n(R),M_n(R)] \subset Kernel(Trace) Kernel(Trace)\subset [M_n(R),M_n(R)] r\in [R,R] A,B \in M_n(R) trace(AB-BA) = r","['linear-algebra', 'abstract-algebra', 'lie-algebras']"
90,How can I show that this matrix has no inverse?,How can I show that this matrix has no inverse?,,"Let $A = [a_{ij}]$ be an $n \times n$ matrix with entries in $\mathbb{R}$ . Suppose there exists an $m$ with $a_{ij} = 0$ for $i \ge m$ and $j \le m$ , and $a_{i,i} \ne 0$ for $1 \le i \lt m$ . Show that $A$ has no inverse. From my understanding no diagonal value is zero, but all the non-diagonal values more than some arbitrary $m$ is zero. And, I know I must show that the determinant is zero, but I'm not sure how to do this.","Let be an matrix with entries in . Suppose there exists an with for and , and for . Show that has no inverse. From my understanding no diagonal value is zero, but all the non-diagonal values more than some arbitrary is zero. And, I know I must show that the determinant is zero, but I'm not sure how to do this.","A = [a_{ij}] n \times n \mathbb{R} m a_{ij} = 0 i \ge m j \le m a_{i,i} \ne 0 1 \le i \lt m A m","['linear-algebra', 'matrices', 'inverse']"
91,An inequation about real pairwise commuting matrices,An inequation about real pairwise commuting matrices,,"Let A and B be two real $n\times n$ matrices such that $AB=BA$ . It’s known that $\det(A^2+B^2)\geq 0$ . I wonder if it’s true that: For $k$ pairwise commuting real matrices $A_1,\cdots,A_k$ ,we have: $\det(\sum_{i=1}^{k}A_{i}^2)\geq0$ .","Let A and B be two real matrices such that . It’s known that . I wonder if it’s true that: For pairwise commuting real matrices ,we have: .","n\times n AB=BA \det(A^2+B^2)\geq 0 k A_1,\cdots,A_k \det(\sum_{i=1}^{k}A_{i}^2)\geq0","['linear-algebra', 'matrices', 'determinant']"
92,A question about a generalization of covariance,A question about a generalization of covariance,,"Suppose, $H$ is a Hilbert space over $\mathbb{R}$ . Suppose, $X$ and $Y$ are random vectors in $H$ . Let’s define Hilbert expectation of a random vector $X$ in a Hilbert space $H$ as a vector $v \in H$ , such that $\forall u \in H \text{ } (E\langle X, u \rangle = \langle v, u \rangle)$ . If a Hilbert expectation exists, then it is unique, due to the fact that every Hilbert space admits an orthonormal basis. Let’s denote Hilbert expectation of a random vector $X$ as $E_HX$ . Now let’s define scalar covariance of two random vectors $X$ and $Y$ as $Cov_H(X, Y) = E\langle X, Y\rangle - \langle E_HX, E_HY \rangle$ Is it always true, that if $X$ and $Y$ are independent, then $Cov_H(X, Y) = 0$ ? If $H$ is $l_2$ or any its subspace, and $X = (X_n)_{n = 1}^\infty$ and $Y = (Y_n)_{n = 1}^\infty$ . Then $Cov_H(X, Y) = \Sigma_{n = 1}^{\infty} EX_nY_n - \Sigma_{n = 1}^{\infty} EX_nEY_n = \Sigma_{n = 1}^{\infty} Cov(X_n, Y_n)$ . Thus, if $X$ and $Y$ are independent, then $Cov_H(X, Y) = 0$ . And because every separable Hilbert space is isometrically isomorphic to a subspace of $l_2$ , the statement is proven for any separable Hilbert space. However, I do not know, what to do in case, when $H$ is not separable.","Suppose, is a Hilbert space over . Suppose, and are random vectors in . Let’s define Hilbert expectation of a random vector in a Hilbert space as a vector , such that . If a Hilbert expectation exists, then it is unique, due to the fact that every Hilbert space admits an orthonormal basis. Let’s denote Hilbert expectation of a random vector as . Now let’s define scalar covariance of two random vectors and as Is it always true, that if and are independent, then ? If is or any its subspace, and and . Then . Thus, if and are independent, then . And because every separable Hilbert space is isometrically isomorphic to a subspace of , the statement is proven for any separable Hilbert space. However, I do not know, what to do in case, when is not separable.","H \mathbb{R} X Y H X H v \in H \forall u \in H \text{ } (E\langle X, u \rangle = \langle v, u \rangle) X E_HX X Y Cov_H(X, Y) = E\langle X, Y\rangle - \langle E_HX, E_HY \rangle X Y Cov_H(X, Y) = 0 H l_2 X = (X_n)_{n = 1}^\infty Y = (Y_n)_{n = 1}^\infty Cov_H(X, Y) = \Sigma_{n = 1}^{\infty} EX_nY_n - \Sigma_{n = 1}^{\infty} EX_nEY_n = \Sigma_{n = 1}^{\infty} Cov(X_n, Y_n) X Y Cov_H(X, Y) = 0 l_2 H","['linear-algebra', 'probability', 'probability-theory', 'hilbert-spaces', 'covariance']"
93,"Operator norm of $T:l^{2}\rightarrow l^{1}$ where $Tx=(x_{1},x_{2}/2,x_{3}/3,x_{4}/4,...)$",Operator norm of  where,"T:l^{2}\rightarrow l^{1} Tx=(x_{1},x_{2}/2,x_{3}/3,x_{4}/4,...)","As the title states, I need to compute the operator norm of a linear operator $T:l^{2}\rightarrow l^{1}$ , where $$Tx=\left(x_{1},\frac{x_{2}}{2},\frac{x_{3}}{3},\frac{x_{4}}{4},... \right)$$ Using Holder's inequality for any sequence $(x_{i})_{i\geq 1}\in l^{2}$ , we can show \begin{align} |Tx|_{1}&=\sum_{i=1}^{\infty}=|x_{1}|+\left|\frac{x_{2}}{2}\right|+\left|\frac{x_{3}}{3}\right|+\cdots\\ &=|x_{1}||1|+|x_{2}|\left|\frac{1}{2}\right|+|x_{3}|\left|\frac{1}{3}\right|+\cdots \\ &\leq \left|x_{i}\right|_{2}\left|\frac{1}{i}\right|_{2} \\ &=\frac{\pi}{\sqrt{6}}|(x_{i})|_{2} \end{align} Hence $$\displaystyle |Tx|_{1}\leq\frac{\pi}{\sqrt{6}}|(x_{i})|_{2}\implies||T||\leq\frac{\pi}{\sqrt{6}}$$ However, I am unable to find a sequence in $l_{2}$ which has norm $|(x_{i})|\leq 1$ so that I may use the property $||T||=\text{sup}_{|(x_{i})|_{2}=1}|Tx|_{1}$ . Any help is appreciated. Thank you.","As the title states, I need to compute the operator norm of a linear operator , where Using Holder's inequality for any sequence , we can show Hence However, I am unable to find a sequence in which has norm so that I may use the property . Any help is appreciated. Thank you.","T:l^{2}\rightarrow l^{1} Tx=\left(x_{1},\frac{x_{2}}{2},\frac{x_{3}}{3},\frac{x_{4}}{4},... \right) (x_{i})_{i\geq 1}\in l^{2} \begin{align}
|Tx|_{1}&=\sum_{i=1}^{\infty}=|x_{1}|+\left|\frac{x_{2}}{2}\right|+\left|\frac{x_{3}}{3}\right|+\cdots\\
&=|x_{1}||1|+|x_{2}|\left|\frac{1}{2}\right|+|x_{3}|\left|\frac{1}{3}\right|+\cdots \\
&\leq \left|x_{i}\right|_{2}\left|\frac{1}{i}\right|_{2} \\
&=\frac{\pi}{\sqrt{6}}|(x_{i})|_{2}
\end{align} \displaystyle |Tx|_{1}\leq\frac{\pi}{\sqrt{6}}|(x_{i})|_{2}\implies||T||\leq\frac{\pi}{\sqrt{6}} l_{2} |(x_{i})|\leq 1 ||T||=\text{sup}_{|(x_{i})|_{2}=1}|Tx|_{1}","['linear-algebra', 'functional-analysis', 'operator-theory', 'normed-spaces']"
94,Find $\det A$ and $\operatorname{Tr} A$ if $\det(A-\sqrt[n]{3}I_n)=0$,Find  and  if,\det A \operatorname{Tr} A \det(A-\sqrt[n]{3}I_n)=0,"$A \in M_{n}(\mathbb{Q})$ and I have to find $\det A$ and $\operatorname{Tr} A$ if $\det(A+\sqrt[n]{3}I_n)=0$ . I observed that $\sqrt[n]{3}$ is an eigenvalue of $A$ ,but I don't know how to continue. EDIT : My bad,the matrix has rational entries.","and I have to find and if . I observed that is an eigenvalue of ,but I don't know how to continue. EDIT : My bad,the matrix has rational entries.",A \in M_{n}(\mathbb{Q}) \det A \operatorname{Tr} A \det(A+\sqrt[n]{3}I_n)=0 \sqrt[n]{3} A,"['linear-algebra', 'matrices']"
95,On the size of a semigroup generated by 5x5 matrices,On the size of a semigroup generated by 5x5 matrices,,"I was working on a problem with a friend where we were given two 5x5 matrices $A$ and $B$ with entries in $\{0, 1, -1\}$ , which must generate a semigroup (under matrix multiplication) of order $>800$ . On the theory side of things, the principle is that we want $$ f(i_1, i_2, i_3, \cdots) = A^{i_1}B^{i_2}A^{i_3}\cdots = \prod_{i=1}^\infty A^{i_{2n-1}}A^{i_{2n}} $$ to have finite range, mapping $\mathbb{N}^\mathbb{N}\to \mathbb{M}_{5\times 5}(\mathbb Z)$ (there is no restriction on the entries of matrices in the semigroup, only on the entries of the generators $A$ and $B$ ). We tried toying arround with the ideas of nilpotent matrices, idempotent matrices, relationships between $A$ and $B$ , but couldn't come up with any explicit theory or any guiding principles for how we should choose $A$ and $B$ . There are some minor facts we know. Clearly, $\det(A),\det(B)\in\{0,1,-1\}$ otherwise there will be an infinite number of distinct powers $A^n$ and $B^n$ . Also, there clearly need to be a finite number of powers of $A$ and $B$ , so they're either nilpotent, eventually idempotent, or something else, but then we're not sure what to do to check combinations of $A$ and $B$ . EDIT: As it turns out, the only reason our code below terminated for those $A$ and $B$ was because of an integer overflow error. As far as we can tell, it seems like those $A$ and $B$ actually generate an infinitely large semigroup, though I haven't rigorously proven it. In this section, the semigroup generated by $A$ and $B$ is NOT finite. Then we plugged in the following $A$ and $B$ into a python program that calculated the size of the semigroup generated by them: $$ A = \left[\begin{matrix} 1 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 \\ 0 & -1 & 0 & -1 & 0 \\ 0 & 0 & 1 & -1 & 1 \\ 0 & 0 & 0 & 0 & 0 \end{matrix}\right]\quad B = \left[\begin{matrix} 0 & -1 & 0 & -1 & 0 \\ -1 & 0 & 0 & 0 & 0\\ 0 & 0 & 1 & 0 & -1 \\ 1 & 0 & 0 & 0 & 0 \\ 1 & 0 & 0 & 0 & 0 \end{matrix}\right] $$ I'm not sure how he came up with these bad boys, he didn't really have a theory for why he chose them, but apparently they generate a semigroup with 12427 elements. Now, we can observe a few things about these matrices: $A^4 = A$ $B^3 = B^4$ (so $B^3 = B^4 = B^5 = \cdots$ ) $(AB)^3 = 0$ $(BA)^4 = 0$ You can keep going generating interesting properties of these two matrices, but bottom line we're not sure why in general there are only a finite number of products of $A$ and $B$ . It seems like you'd have to check a bunch of different edge cases. For example, we may know the nilpotency/idempotency of $A^i B^j$ for $i < 4, j < 3$ , but how would you determine that $$ f(i_1, j_1, i_2, j_2, i_3, j_3, ...) $$ where $(i_k, j_k)_{k=1}^\infty \in \{1, 2, 3\}\times\{1, 2\}$ is an arbitrary sequence, has a finite range? I suppose you could check something how, if you restrict the sequences $(i_k, j_k)$ such that a specific pair $(i_{k'}, j_{k'})$ doesn't repeat more than the nilpotency/idempotency of $A^{i_{k'}}B^{j_{k'}}$ , then there's only a finite number of sequences you can choose, but this seems like a really difficult/roundabout way of proving this. Also, this doesn't give any insight into how you choose $A$ and $B$ . Can anyone provide any insight into this? Why do the matrices $A$ and $B$ work, and how would you choose other $A$ and $B$ in general? EDIT 2: I've determined the following identity involving $A$ and $B$ . Let $Z = B^2A^2BA$ , and $C = B^3A^2$ . Then $ZC^k$ is zero except the middle row (third row) is $$ \text{Row}_3(ZC^k) = \left[\begin{matrix}-(-2)^{k+1} & (-2)^k & -(-2)^{k+1} & -(-2)^k & -(-2)^{k+1}\end{matrix}\right] $$ which is unbounded. Of course, there are probably other simpler relationships, this is just the first one we found via computer. EDIT 3: Another friend showed us a different approach. We can fairly easily brute force search for $A, B\in \mathbb{M}_{2\times 2}(\{0, 1, -1\})$ (there are only 6561 = (3^4)^2 choices) such that the semigroup generated by $A$ and $B$ is finite, then extend these to $5\times 5$ matrices in block format: $$ A_{5\times 5} = \left[\begin{matrix}A & O \\ O & I\end{matrix}\right],\qquad B_{5\times 5} = \left[\begin{matrix}B & O \\ O & I\end{matrix}\right] $$ Though this works to generate such a semigroup, my problem with it is that it doesn't give us any insight into the choice of $A$ and $B$ either; it still requires us to brute force search. If, for example, we were forced to find $A$ and $B$ , $5\times 5$ , that generate a semigroup with order greater than any finite semigroup of $2\times 2$ or $3\times 3$ matrices, we would still be lost. Hence, I'm still curious about the theory behind choosing $A$ and $B$ .","I was working on a problem with a friend where we were given two 5x5 matrices and with entries in , which must generate a semigroup (under matrix multiplication) of order . On the theory side of things, the principle is that we want to have finite range, mapping (there is no restriction on the entries of matrices in the semigroup, only on the entries of the generators and ). We tried toying arround with the ideas of nilpotent matrices, idempotent matrices, relationships between and , but couldn't come up with any explicit theory or any guiding principles for how we should choose and . There are some minor facts we know. Clearly, otherwise there will be an infinite number of distinct powers and . Also, there clearly need to be a finite number of powers of and , so they're either nilpotent, eventually idempotent, or something else, but then we're not sure what to do to check combinations of and . EDIT: As it turns out, the only reason our code below terminated for those and was because of an integer overflow error. As far as we can tell, it seems like those and actually generate an infinitely large semigroup, though I haven't rigorously proven it. In this section, the semigroup generated by and is NOT finite. Then we plugged in the following and into a python program that calculated the size of the semigroup generated by them: I'm not sure how he came up with these bad boys, he didn't really have a theory for why he chose them, but apparently they generate a semigroup with 12427 elements. Now, we can observe a few things about these matrices: (so ) You can keep going generating interesting properties of these two matrices, but bottom line we're not sure why in general there are only a finite number of products of and . It seems like you'd have to check a bunch of different edge cases. For example, we may know the nilpotency/idempotency of for , but how would you determine that where is an arbitrary sequence, has a finite range? I suppose you could check something how, if you restrict the sequences such that a specific pair doesn't repeat more than the nilpotency/idempotency of , then there's only a finite number of sequences you can choose, but this seems like a really difficult/roundabout way of proving this. Also, this doesn't give any insight into how you choose and . Can anyone provide any insight into this? Why do the matrices and work, and how would you choose other and in general? EDIT 2: I've determined the following identity involving and . Let , and . Then is zero except the middle row (third row) is which is unbounded. Of course, there are probably other simpler relationships, this is just the first one we found via computer. EDIT 3: Another friend showed us a different approach. We can fairly easily brute force search for (there are only 6561 = (3^4)^2 choices) such that the semigroup generated by and is finite, then extend these to matrices in block format: Though this works to generate such a semigroup, my problem with it is that it doesn't give us any insight into the choice of and either; it still requires us to brute force search. If, for example, we were forced to find and , , that generate a semigroup with order greater than any finite semigroup of or matrices, we would still be lost. Hence, I'm still curious about the theory behind choosing and .","A B \{0, 1, -1\} >800 
f(i_1, i_2, i_3, \cdots) = A^{i_1}B^{i_2}A^{i_3}\cdots = \prod_{i=1}^\infty A^{i_{2n-1}}A^{i_{2n}}
 \mathbb{N}^\mathbb{N}\to \mathbb{M}_{5\times 5}(\mathbb Z) A B A B A B \det(A),\det(B)\in\{0,1,-1\} A^n B^n A B A B A B A B A B A B 
A = \left[\begin{matrix}
1 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 \\
0 & -1 & 0 & -1 & 0 \\
0 & 0 & 1 & -1 & 1 \\
0 & 0 & 0 & 0 & 0
\end{matrix}\right]\quad B = \left[\begin{matrix}
0 & -1 & 0 & -1 & 0 \\
-1 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & -1 \\
1 & 0 & 0 & 0 & 0 \\
1 & 0 & 0 & 0 & 0
\end{matrix}\right]
 A^4 = A B^3 = B^4 B^3 = B^4 = B^5 = \cdots (AB)^3 = 0 (BA)^4 = 0 A B A^i B^j i < 4, j < 3 
f(i_1, j_1, i_2, j_2, i_3, j_3, ...)
 (i_k, j_k)_{k=1}^\infty \in \{1, 2, 3\}\times\{1, 2\} (i_k, j_k) (i_{k'}, j_{k'}) A^{i_{k'}}B^{j_{k'}} A B A B A B A B Z = B^2A^2BA C = B^3A^2 ZC^k 
\text{Row}_3(ZC^k) = \left[\begin{matrix}-(-2)^{k+1} & (-2)^k & -(-2)^{k+1} & -(-2)^k & -(-2)^{k+1}\end{matrix}\right]
 A, B\in \mathbb{M}_{2\times 2}(\{0, 1, -1\}) A B 5\times 5 
A_{5\times 5} = \left[\begin{matrix}A & O \\ O & I\end{matrix}\right],\qquad B_{5\times 5} = \left[\begin{matrix}B & O \\ O & I\end{matrix}\right]
 A B A B 5\times 5 2\times 2 3\times 3 A B","['linear-algebra', 'group-theory', 'semigroups']"
96,7-Dimensional Curvature and Curl,7-Dimensional Curvature and Curl,,"The cross product from linear algebra is most often used in conjunction with 2-dimensional or 3-dimensional vectors.  However, it is also said to work with 7-dimensional vectors, although this version appears to live a bit deeper in the sea of mathematical prerequisites. In multivariable calculus, 2D-Curl and 3D-Curl are very much connected to the 2D cross product and 3D cross product, respectively. Curvature, with respect to arclength, has a loosely similar geometry to Curl (both involve some measure of straightness or lack thereof), and also connects with the cross product $(k = \frac{\|S'(t)\ \times\ S''(t)\|}{\|S'(t)\|^3}$ ). This raises the question, does the 7-dimensional version of the cross product lead to a 7D-Curl, 7D curvature, or 7-dimensional versions of any other topics in multivariable calculus tied to the cross product which don't exist in every dimension?","The cross product from linear algebra is most often used in conjunction with 2-dimensional or 3-dimensional vectors.  However, it is also said to work with 7-dimensional vectors, although this version appears to live a bit deeper in the sea of mathematical prerequisites. In multivariable calculus, 2D-Curl and 3D-Curl are very much connected to the 2D cross product and 3D cross product, respectively. Curvature, with respect to arclength, has a loosely similar geometry to Curl (both involve some measure of straightness or lack thereof), and also connects with the cross product ). This raises the question, does the 7-dimensional version of the cross product lead to a 7D-Curl, 7D curvature, or 7-dimensional versions of any other topics in multivariable calculus tied to the cross product which don't exist in every dimension?",(k = \frac{\|S'(t)\ \times\ S''(t)\|}{\|S'(t)\|^3},"['linear-algebra', 'geometry', 'multivariable-calculus', 'cross-product']"
97,Finding the amount partitions with gives sizes of a multiset,Finding the amount partitions with gives sizes of a multiset,,"A multiset $A$ contains $E$ positive integers. The multiplicity of each element is $r_i \; i=1,\ldots ,N$. $A$ is partitioned in $M$ (we do not necessary have $M=N$) ordinary sets (where elements are therefore not repeated). The size of the $i$-th set $C_i$ is given and equal to $c_i$. Is there a way to count the number of possible partitions knowing $N, M, r_i, c_i$ with or without consider sets order? I think the answer to this question may be found starting from here and maybe by employing the partial Bells polynomials . Another way to interpret the problem can be the following: find the number of binary $N \times M$ matrices with fixed rows and columns sums with no column and row full of zeros (a similar problem has been answered here ) .","A multiset $A$ contains $E$ positive integers. The multiplicity of each element is $r_i \; i=1,\ldots ,N$. $A$ is partitioned in $M$ (we do not necessary have $M=N$) ordinary sets (where elements are therefore not repeated). The size of the $i$-th set $C_i$ is given and equal to $c_i$. Is there a way to count the number of possible partitions knowing $N, M, r_i, c_i$ with or without consider sets order? I think the answer to this question may be found starting from here and maybe by employing the partial Bells polynomials . Another way to interpret the problem can be the following: find the number of binary $N \times M$ matrices with fixed rows and columns sums with no column and row full of zeros (a similar problem has been answered here ) .",,"['linear-algebra', 'combinatorics', 'combinations', 'set-partition', 'multisets']"
98,Is Whitehead lemma true for super Lie algebras?,Is Whitehead lemma true for super Lie algebras?,,"Classical Whitehead lemma states that if $\mathfrak g$ is a finite-dimensional complex Lie algebra and $M$ is a finite-dimensional $\mathfrak g$-module, then first cohomology group $H^1(\mathfrak g, M)$ (defined for example as cohomology of the Chevalley-Eilenberg complex) is trivial. I need to know if this is also true for super Lie algebras.","Classical Whitehead lemma states that if $\mathfrak g$ is a finite-dimensional complex Lie algebra and $M$ is a finite-dimensional $\mathfrak g$-module, then first cohomology group $H^1(\mathfrak g, M)$ (defined for example as cohomology of the Chevalley-Eilenberg complex) is trivial. I need to know if this is also true for super Lie algebras.",,"['linear-algebra', 'lie-algebras', 'homological-algebra', 'superalgebra', 'lie-superalgebras']"
99,Minimal rotation matrix for two vectors in $\mathbb{R}^n$,Minimal rotation matrix for two vectors in,\mathbb{R}^n,"Suppose we have two normal vectors $v, u \in \mathbb{R}^n, \|u\|_2=1, \|v\|_2=1$ . We would like to find a rotation matrix $R\in\mathbb{R}^{n \times n}$ that satisfies $R u = v$ . This clearly does not uniquely determine matrix $R$ because there are only $n$ equations while there are $n(n-1)/2$ degrees of freedom for $R$ . However we can make it unique by forcing it to only rotate vectors in the $Span\{u,v\}$ . In other words: $$\forall w \in Span\{u,v\}^\perp : R w = w$$ Intuitively, it only rotates vectors in the $Span\{u,v\}$ plane while leaving every other direction untouched. In this sense, the rotation matrix is minimal. In the very simple case that $Span\{u,v\} = Span\{e_i,e_j\}$ , i.e., the linear space spanned by $u,v$ is the same as the one spanned by $e_i, e_j$ we can compute $R$ as follows: $R = I_n - M$ , in which $M\in \mathbb{R}^{n\times n}$ is only zeros except for its $i$ -th and $j$ -th rows and columns: $$M_{i, j\times i, j} = \begin{bmatrix}     1-\cos \theta       &  \sin \theta\\     -\sin\theta       & 1-\cos\theta \\ \end{bmatrix}$$ in which $\theta = \cos^{-1} \langle u, v \rangle$ Is there a way to compute the rotation matrix for general normal vectors $u, v$ ?","Suppose we have two normal vectors . We would like to find a rotation matrix that satisfies . This clearly does not uniquely determine matrix because there are only equations while there are degrees of freedom for . However we can make it unique by forcing it to only rotate vectors in the . In other words: Intuitively, it only rotates vectors in the plane while leaving every other direction untouched. In this sense, the rotation matrix is minimal. In the very simple case that , i.e., the linear space spanned by is the same as the one spanned by we can compute as follows: , in which is only zeros except for its -th and -th rows and columns: in which Is there a way to compute the rotation matrix for general normal vectors ?","v, u \in \mathbb{R}^n, \|u\|_2=1, \|v\|_2=1 R\in\mathbb{R}^{n \times n} R u = v R n n(n-1)/2 R Span\{u,v\} \forall w \in Span\{u,v\}^\perp : R w = w Span\{u,v\} Span\{u,v\} = Span\{e_i,e_j\} u,v e_i, e_j R R = I_n - M M\in \mathbb{R}^{n\times n} i j M_{i, j\times i, j} = \begin{bmatrix}
    1-\cos \theta       &  \sin \theta\\
    -\sin\theta       & 1-\cos\theta \\
\end{bmatrix} \theta = \cos^{-1} \langle u, v \rangle u, v","['linear-algebra', 'rotations']"
