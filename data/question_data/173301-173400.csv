,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,In how many ways can $8$ appointments be scheduled for a physician visiting a nursing home with $20$ patients? [closed],In how many ways can  appointments be scheduled for a physician visiting a nursing home with  patients? [closed],8 20,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question A physician routinely visits a local nursing home on Thursday mornings to examine patients. Suppose the facility has $20$ residents, but the physician only has time to check $8$. The supervisor places $8$ random patients on an order list and presents the schedule to the physician. a. How many different schedules are possible? b. If there are $15$ women and $5$ men in the facility, what is the probability that all appointments will be with women? All the help is much appreciated. Also, if you can explain how you got the answers.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question A physician routinely visits a local nursing home on Thursday mornings to examine patients. Suppose the facility has $20$ residents, but the physician only has time to check $8$. The supervisor places $8$ random patients on an order list and presents the schedule to the physician. a. How many different schedules are possible? b. If there are $15$ women and $5$ men in the facility, what is the probability that all appointments will be with women? All the help is much appreciated. Also, if you can explain how you got the answers.",,"['probability', 'combinatorics', 'statistics']"
1,Proof of the existence of a reversible stationary distribution,Proof of the existence of a reversible stationary distribution,,"$p$ is a finite Markov chain where $p(i,j)>0$ for all $i,j$. Prove a reversible stationary distribution exists for $p$ if $p(i,j)p(j,k)p(k,i)=p(i,k)p(k,j)p(j,i)$ for all $i,j,k$ This question is from Durrett's Essentials of Stochastic Processes. There is a hint provided that says: Hint: fix $i$ and take $π_j = cp(i,j)/p(j,i)$ where $c$ is some constant. I am struggling to see how to apply this hint. I want to show $c$ is $π_i$ but don't know how to get there.","$p$ is a finite Markov chain where $p(i,j)>0$ for all $i,j$. Prove a reversible stationary distribution exists for $p$ if $p(i,j)p(j,k)p(k,i)=p(i,k)p(k,j)p(j,i)$ for all $i,j,k$ This question is from Durrett's Essentials of Stochastic Processes. There is a hint provided that says: Hint: fix $i$ and take $π_j = cp(i,j)/p(j,i)$ where $c$ is some constant. I am struggling to see how to apply this hint. I want to show $c$ is $π_i$ but don't know how to get there.",,"['statistics', 'stochastic-processes', 'markov-chains']"
2,The concatenation of two independent normal vectors is multivariate normal.,The concatenation of two independent normal vectors is multivariate normal.,,"I've already read this question . By the definition I have,  $$\mathbf{z} = \begin{bmatrix} z_1 & z_2 & \cdots & z_n  \end{bmatrix}^{T}$$ is a multivariate standard normal vector if each $z_i \sim \mathcal{N}(0, 1)$ is iid. Furthermore, $\mathbf{Az+b} \sim \mathcal{N}(\mathbf{b}, \mathbf{A}\mathbf{A}^{T})$ (assuming that $\mathbf{A}$, $\mathbf{b}$ are obviously conformable) is multivariate normal. Suppose that I have two vectors $\mathbf{a} \sim \mathcal{N}(\boldsymbol{\mu}_1, \mathbf{V}_1)$ and $\mathbf{b} \sim \mathcal{N}(\boldsymbol{\mu}_2, \mathbf{V}_2)$ which are both $n$-dimensional and independent. How do I show that $$\begin{bmatrix} \mathbf{a} \\ \mathbf{b} \end{bmatrix} \sim \mathcal{N}\left(\begin{bmatrix}  \boldsymbol{\mu}_1\\ \boldsymbol{\mu}_2  \end{bmatrix}, \begin{bmatrix} \mathbf{V}_1 & \mathbf{0}_{n \times n} \\ \mathbf{0}_{n \times n} & \mathbf{V}_2 \end{bmatrix} \right)\text{?}$$ The mean vector and variance-covariance matrix are easy to derive, but I am stuck as to how to show joint normality.","I've already read this question . By the definition I have,  $$\mathbf{z} = \begin{bmatrix} z_1 & z_2 & \cdots & z_n  \end{bmatrix}^{T}$$ is a multivariate standard normal vector if each $z_i \sim \mathcal{N}(0, 1)$ is iid. Furthermore, $\mathbf{Az+b} \sim \mathcal{N}(\mathbf{b}, \mathbf{A}\mathbf{A}^{T})$ (assuming that $\mathbf{A}$, $\mathbf{b}$ are obviously conformable) is multivariate normal. Suppose that I have two vectors $\mathbf{a} \sim \mathcal{N}(\boldsymbol{\mu}_1, \mathbf{V}_1)$ and $\mathbf{b} \sim \mathcal{N}(\boldsymbol{\mu}_2, \mathbf{V}_2)$ which are both $n$-dimensional and independent. How do I show that $$\begin{bmatrix} \mathbf{a} \\ \mathbf{b} \end{bmatrix} \sim \mathcal{N}\left(\begin{bmatrix}  \boldsymbol{\mu}_1\\ \boldsymbol{\mu}_2  \end{bmatrix}, \begin{bmatrix} \mathbf{V}_1 & \mathbf{0}_{n \times n} \\ \mathbf{0}_{n \times n} & \mathbf{V}_2 \end{bmatrix} \right)\text{?}$$ The mean vector and variance-covariance matrix are easy to derive, but I am stuck as to how to show joint normality.",,"['probability', 'statistics', 'probability-distributions']"
3,What is this matrix notation and how is it solved?,What is this matrix notation and how is it solved?,,"I've never taken a stats class, or linear algebra or much of anything that involves matrices.  In one of my books they give me this as part of an example and it states, $$\binom{6}{4} = 15 \text{ combinations}$$ I don't understand how the math is done for that statement. I don't get where $15$ comes from. I'm assuming it is some sort of a probability formula, but I am not sure.","I've never taken a stats class, or linear algebra or much of anything that involves matrices.  In one of my books they give me this as part of an example and it states, $$\binom{6}{4} = 15 \text{ combinations}$$ I don't understand how the math is done for that statement. I don't get where $15$ comes from. I'm assuming it is some sort of a probability formula, but I am not sure.",,"['combinatorics', 'statistics', 'binomial-coefficients']"
4,Maximum Likelihood Estimation with 2 parameters for a Poisson distribution,Maximum Likelihood Estimation with 2 parameters for a Poisson distribution,,"I have two observations from a Poisson distribution. The first one ($N_r$) come with a Poisson distribution with mean $k_1$. For the second one ($N_e$) I know that $N_e - M$ also come from the same Poisson distribution with mean $k_1$, but the parameter $M$ is unknown. My observations are $N_r$ and $N_e$. I want to do the MLE for $M$ and $k_1$. If I write the equations:  $$ f(N_r, N_e|k_1, M) = \frac{e^{-k_1}k_1^{N_r}}{N_r!}\frac{e^{-k_1}k_1^{N_e-M}}{(N_e-M)!}=\frac{e^{-2k_1}k_1^{N_r+N_e-M}}{N_r!(N_e-M)!}$$ Taking logarithms: $$ ln(f) = -2k_1 + (N_r+N_e-M)ln(k_1)-ln(N_r!(N_e-M)!)$$ I can estimate the mean parameter $k_1$ as $$ k_1 = \frac{N_r+N_e-M}{2}$$ But I don't know how to estimate the $M$ parameter as I have to deal with the factorial in the derivative. Can anybody help me? Thank you very much!!","I have two observations from a Poisson distribution. The first one ($N_r$) come with a Poisson distribution with mean $k_1$. For the second one ($N_e$) I know that $N_e - M$ also come from the same Poisson distribution with mean $k_1$, but the parameter $M$ is unknown. My observations are $N_r$ and $N_e$. I want to do the MLE for $M$ and $k_1$. If I write the equations:  $$ f(N_r, N_e|k_1, M) = \frac{e^{-k_1}k_1^{N_r}}{N_r!}\frac{e^{-k_1}k_1^{N_e-M}}{(N_e-M)!}=\frac{e^{-2k_1}k_1^{N_r+N_e-M}}{N_r!(N_e-M)!}$$ Taking logarithms: $$ ln(f) = -2k_1 + (N_r+N_e-M)ln(k_1)-ln(N_r!(N_e-M)!)$$ I can estimate the mean parameter $k_1$ as $$ k_1 = \frac{N_r+N_e-M}{2}$$ But I don't know how to estimate the $M$ parameter as I have to deal with the factorial in the derivative. Can anybody help me? Thank you very much!!",,"['probability', 'statistics', 'estimation', 'poisson-distribution', 'maximum-likelihood']"
5,Intuition behid $P(A\mid B)$. [duplicate],Intuition behid . [duplicate],P(A\mid B),"This question already has answers here : Intuition behind the Definition of Conditional Probability (for 2 Events) (10 answers) Closed 8 years ago . What is the intuition behind the formula $$P(A\mid B)=\frac{P(A\cap B)}{P(B)}$$ I have seen this formula around, but every site/book I look at does not really have a clear & cut explanation behind this formula.","This question already has answers here : Intuition behind the Definition of Conditional Probability (for 2 Events) (10 answers) Closed 8 years ago . What is the intuition behind the formula $$P(A\mid B)=\frac{P(A\cap B)}{P(B)}$$ I have seen this formula around, but every site/book I look at does not really have a clear & cut explanation behind this formula.",,"['probability', 'statistics']"
6,Expectation of a function of a normally distributed random variable,Expectation of a function of a normally distributed random variable,,"Consider that I have to produce this result: $$E[u(W_0+r(\theta))] = u(W_0)+\theta-\frac 12\rho\sigma^2$$ From this: $$ E[u(W_0+r(\theta))]     = \int_{-\infty}^\infty u(w_0+r) \frac{1}{\sigma \sqrt{2\pi}}                             \exp\left(-\frac{(r-\theta)^2}{2 \sigma^2}\right) dr $$ And: $$\int_{-\infty}^\infty           \frac{1}{\sigma \sqrt{2\pi}}                             \exp\left(-\frac{(r-\theta)^2}{2 \sigma^2}\right) dr = 1 $$ Also: $u(w) = -\exp(-\rho w) $ And: $r(\theta) \sim N(\theta, \sigma^2)$ I have reached the point where I removed the constants, and realize that I have both a positive exponent and negative exponent, and I believe no way to combine them. Help is much appreciated. Thanks.","Consider that I have to produce this result: $$E[u(W_0+r(\theta))] = u(W_0)+\theta-\frac 12\rho\sigma^2$$ From this: $$ E[u(W_0+r(\theta))]     = \int_{-\infty}^\infty u(w_0+r) \frac{1}{\sigma \sqrt{2\pi}}                             \exp\left(-\frac{(r-\theta)^2}{2 \sigma^2}\right) dr $$ And: $$\int_{-\infty}^\infty           \frac{1}{\sigma \sqrt{2\pi}}                             \exp\left(-\frac{(r-\theta)^2}{2 \sigma^2}\right) dr = 1 $$ Also: $u(w) = -\exp(-\rho w) $ And: $r(\theta) \sim N(\theta, \sigma^2)$ I have reached the point where I removed the constants, and realize that I have both a positive exponent and negative exponent, and I believe no way to combine them. Help is much appreciated. Thanks.",,"['calculus', 'statistics', 'probability-distributions', 'normal-distribution']"
7,How to pick the same color spheres out of two boxes,How to pick the same color spheres out of two boxes,,"For the following problem: There are two boxes $A$ and $B$. Box $A$ contains $3$ red, $8$ white and $13$ green spheres, while box $B$ contains $5$ red, $7$ white and $6$ green spheres. If we pick one sphere from each box at random, what is the probability that they have the same color? I followed this way of thinking: I assume the following events: $A_1:=\{ \text{Sphere from box A is red} \}, \quad \quad B_1:=\{ \text{Sphere from box B is red} \} $ $A_2:=\{ \text{Sphere from box A is white} \}, \quad B_2:=\{ \text{Sphere from box B is white} \} $ $A_3:=\{ \text{Sphere from box A is green} \}, \quad B_3:=\{ \text{Sphere from box B is green} \} $ Then the probability that they have the same color will be: \begin{equation} P[(A_1 \cap B_1)\cup (A_2 \cap B_2)\cup (A_3 \cap B_3)] \end{equation} But those events are both independent and mutually exclusive therefore for two events $A$ and $B$ it generally holds: \begin{equation} P(A\cup B)=P(A)+P(B), \quad P(A \cap B)=P(A)P(B) \end{equation} So our equation above is rewritten as: \begin{equation} P[(A_1 \cap B_1)\cup (A_2 \cap B_2)\cup (A_3 \cap B_3)]=P(A_1)P(B_1)+P(A_2)P(B_2)+P(A_3)P(B_3) \end{equation} which now is just a matter of arithmetics. Anyway, my question is the following: Is my thinking above correct? And if yes, I am pretty sure there is a faster way to go with this one. Any ideas? Thank you!","For the following problem: There are two boxes $A$ and $B$. Box $A$ contains $3$ red, $8$ white and $13$ green spheres, while box $B$ contains $5$ red, $7$ white and $6$ green spheres. If we pick one sphere from each box at random, what is the probability that they have the same color? I followed this way of thinking: I assume the following events: $A_1:=\{ \text{Sphere from box A is red} \}, \quad \quad B_1:=\{ \text{Sphere from box B is red} \} $ $A_2:=\{ \text{Sphere from box A is white} \}, \quad B_2:=\{ \text{Sphere from box B is white} \} $ $A_3:=\{ \text{Sphere from box A is green} \}, \quad B_3:=\{ \text{Sphere from box B is green} \} $ Then the probability that they have the same color will be: \begin{equation} P[(A_1 \cap B_1)\cup (A_2 \cap B_2)\cup (A_3 \cap B_3)] \end{equation} But those events are both independent and mutually exclusive therefore for two events $A$ and $B$ it generally holds: \begin{equation} P(A\cup B)=P(A)+P(B), \quad P(A \cap B)=P(A)P(B) \end{equation} So our equation above is rewritten as: \begin{equation} P[(A_1 \cap B_1)\cup (A_2 \cap B_2)\cup (A_3 \cap B_3)]=P(A_1)P(B_1)+P(A_2)P(B_2)+P(A_3)P(B_3) \end{equation} which now is just a matter of arithmetics. Anyway, my question is the following: Is my thinking above correct? And if yes, I am pretty sure there is a faster way to go with this one. Any ideas? Thank you!",,"['probability', 'statistics', 'probability-distributions']"
8,Upper bound for probability,Upper bound for probability,,"Given two i.i.d. random variables $X,Y$ that satisfy the following condition: $P(|X|<0.5)<a$ and $P(|Y|<0.5)<b$ How can I derive an upper bound for the following probability $$P\left(\middle|\frac{X+Y}{2}\middle|<0.5\right)$$ as  a function of $a$ and $b$? Thanks.","Given two i.i.d. random variables $X,Y$ that satisfy the following condition: $P(|X|<0.5)<a$ and $P(|Y|<0.5)<b$ How can I derive an upper bound for the following probability $$P\left(\middle|\frac{X+Y}{2}\middle|<0.5\right)$$ as  a function of $a$ and $b$? Thanks.",,"['probability', 'statistics', 'means']"
9,Random Samples and Sample Variance Bound,Random Samples and Sample Variance Bound,,"Let $X_{1}, X_{2}, \dots, X_{n}$ be a random sample from a population. Show that: $$\max_{1 \leq i \leq n}|X_{i}-\bar{X}|<\frac{(n-1)}{\sqrt{n}}S$$ Where we have the sample variance $S^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\bar{X})^{2}$, unless all $n$ observations are equal or $n-1$ of the $X_{i}$'s are equal. I'm not sure on how to begin with the exercise. Maybe one could use one of the convergence theorems (weak law of large numbers or central limit theorem) and obtain a bound depending on $n$? What would be a good way to get started?","Let $X_{1}, X_{2}, \dots, X_{n}$ be a random sample from a population. Show that: $$\max_{1 \leq i \leq n}|X_{i}-\bar{X}|<\frac{(n-1)}{\sqrt{n}}S$$ Where we have the sample variance $S^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\bar{X})^{2}$, unless all $n$ observations are equal or $n-1$ of the $X_{i}$'s are equal. I'm not sure on how to begin with the exercise. Maybe one could use one of the convergence theorems (weak law of large numbers or central limit theorem) and obtain a bound depending on $n$? What would be a good way to get started?",,"['probability', 'statistics', 'convergence-divergence', 'random-variables']"
10,Why is there no unbiased estimator for $\frac{1}{\theta}$ for Poisson Distribution?,Why is there no unbiased estimator for  for Poisson Distribution?,\frac{1}{\theta},"Suppose that $X_1,\dots,X_n$ is an iid random sample from a Poisson distribution with mean $\theta$. I would like to prove that there exists no unbiased estimator of $\frac{1}{\theta}$. To do so, I will let $\bar\theta(X)$ be an estimator of $\frac{1}{\theta}$. Then, I'd like to equate the expectation of $\bar\theta(X)$ and $\frac{1}{\theta}$ $$E\left[\bar\theta(X)\right] = \sum_{x=0}^{\infty}\bar\theta(x)P(X=x)$$ Now, my problem is that I don't know what to do next. I can write the probability $P(X=x)$ out but how do I continue from there? I have no information of $\bar\theta$ which is inside the sum. Any ideas?","Suppose that $X_1,\dots,X_n$ is an iid random sample from a Poisson distribution with mean $\theta$. I would like to prove that there exists no unbiased estimator of $\frac{1}{\theta}$. To do so, I will let $\bar\theta(X)$ be an estimator of $\frac{1}{\theta}$. Then, I'd like to equate the expectation of $\bar\theta(X)$ and $\frac{1}{\theta}$ $$E\left[\bar\theta(X)\right] = \sum_{x=0}^{\infty}\bar\theta(x)P(X=x)$$ Now, my problem is that I don't know what to do next. I can write the probability $P(X=x)$ out but how do I continue from there? I have no information of $\bar\theta$ which is inside the sum. Any ideas?",,"['probability', 'statistics', 'estimation', 'poisson-distribution']"
11,Distribution of discrete function of continuous random variable?,Distribution of discrete function of continuous random variable?,,"It has been quite some time that I did statistics, and I am not sure how to figure out the distribution of a function of a random variable if the function itself discretizes (if that is a word) the random variable. So I hope you can help me remember / figure out how to solve it. Let $x$ be a normally distributed random variable. Now, consider the random variable $y = g(x)$, where $g(x) = 1$ if $x \leq a$, $g(x) = 2$ if $a< x \leq b$ and $g(x) = 3$ if $x > b$, and $a,b$ are some constants in $R$. My goal is to see if the final distribution belongs to the exponential family, and knowing the distribution would be a necessary first step. If the function was continuous, I would know what to do, but I am somewhat stuck since it is not. Any help including links to theorems is appreciated!","It has been quite some time that I did statistics, and I am not sure how to figure out the distribution of a function of a random variable if the function itself discretizes (if that is a word) the random variable. So I hope you can help me remember / figure out how to solve it. Let $x$ be a normally distributed random variable. Now, consider the random variable $y = g(x)$, where $g(x) = 1$ if $x \leq a$, $g(x) = 2$ if $a< x \leq b$ and $g(x) = 3$ if $x > b$, and $a,b$ are some constants in $R$. My goal is to see if the final distribution belongs to the exponential family, and knowing the distribution would be a necessary first step. If the function was continuous, I would know what to do, but I am somewhat stuck since it is not. Any help including links to theorems is appreciated!",,"['statistics', 'probability-distributions', 'normal-distribution', 'transformation']"
12,Probability density function rules [closed],Probability density function rules [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question Probability distribution function (pdf):   \begin{align*} &P(-\infty<X<+\infty)=1\\ &P(x_0\leq X\leq x_1)\geq 0\\ &p(x_0)=-9.0? \end{align*} Source. Why does the third one make sense? I know that $p(x)$ cannot be negative, yet supposedly the third one can occur. Why is that?","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question Probability distribution function (pdf):   \begin{align*} &P(-\infty<X<+\infty)=1\\ &P(x_0\leq X\leq x_1)\geq 0\\ &p(x_0)=-9.0? \end{align*} Source. Why does the third one make sense? I know that $p(x)$ cannot be negative, yet supposedly the third one can occur. Why is that?",,"['probability', 'statistics']"
13,Why is Method of Moments not the MLE for Beta Distribution?,Why is Method of Moments not the MLE for Beta Distribution?,,"This is a question from a statistics textbook: Suppose that $X_1, \ldots, X_n$ form a random sample from the beta distribution with parameters $\alpha$ and $\beta$. Let $\theta=(\alpha,\beta)$ be the vector parameter. Show that the method of moments estimator is not the M.L.E. Denote $\alpha$ and $\beta$ in terms of method of moments, which is $\alpha = {m_1(m_1-m_2) \over m_2-m_1^2}$ and $\beta = {(1-m_1)(m_1-m_2) \over m_2-m_1^2}$, where $m_j = {1 \over n} \sum_{i=1}^n X^j_i$ for $j \ge 1$. How to prove that the method of moments is not the MLE?","This is a question from a statistics textbook: Suppose that $X_1, \ldots, X_n$ form a random sample from the beta distribution with parameters $\alpha$ and $\beta$. Let $\theta=(\alpha,\beta)$ be the vector parameter. Show that the method of moments estimator is not the M.L.E. Denote $\alpha$ and $\beta$ in terms of method of moments, which is $\alpha = {m_1(m_1-m_2) \over m_2-m_1^2}$ and $\beta = {(1-m_1)(m_1-m_2) \over m_2-m_1^2}$, where $m_j = {1 \over n} \sum_{i=1}^n X^j_i$ for $j \ge 1$. How to prove that the method of moments is not the MLE?",,"['statistics', 'maximum-likelihood']"
14,solve the integral equation 2,solve the integral equation 2,,I want to solve the integral .Solve is difficult. I want to use statistical methods to solve them. $$\int_{0}^{+\infty}x \exp\{ ax-b x^2\}d x=\int _{0}^{+\infty} x\exp\{-b(x^2-\frac{a}{b}x)\}dx=\\ exp\{\frac{a^2}{4b^2}\}\int_{0}^{+\infty}x\exp\{-b(x-\frac{a}{b})^2\}dx=\sqrt{\frac{\pi}{4}}\exp{\frac{a^2}{4b^2}}\int_{0}^{+\infty}\frac{\sqrt{2}}{\pi(1\2)}x \exp\frac{-b}{2(1\2)}(x-\frac{a}{b})^2dx=\frac{\pi}{4}\exp\{\frac{a^2}{4b^2}\}.E(HN) $$ Wherever a and b are fixed and HN half normal distribution. What is the final answer,I want to solve the integral .Solve is difficult. I want to use statistical methods to solve them. $$\int_{0}^{+\infty}x \exp\{ ax-b x^2\}d x=\int _{0}^{+\infty} x\exp\{-b(x^2-\frac{a}{b}x)\}dx=\\ exp\{\frac{a^2}{4b^2}\}\int_{0}^{+\infty}x\exp\{-b(x-\frac{a}{b})^2\}dx=\sqrt{\frac{\pi}{4}}\exp{\frac{a^2}{4b^2}}\int_{0}^{+\infty}\frac{\sqrt{2}}{\pi(1\2)}x \exp\frac{-b}{2(1\2)}(x-\frac{a}{b})^2dx=\frac{\pi}{4}\exp\{\frac{a^2}{4b^2}\}.E(HN) $$ Wherever a and b are fixed and HN half normal distribution. What is the final answer,,"['statistics', 'statistical-inference']"
15,Bounds on the eigenvalues of a covariance matrix,Bounds on the eigenvalues of a covariance matrix,,"Consider the (symmetric) covariance matrix $\Sigma_{22}(k,k)= \begin{bmatrix}     b(0)       & b(1)& b(2) & \cdots & b(k-1) \\     b(1)      & b(0) & b(1) & \cdots & b(k-2) \\     \vdots  && \vdots \\     b(k-1)      & b(k-2)& b(k-3)& \cdots & b(0) \end{bmatrix}, $ where $\sum_{i=1}^n\sum_{j=1}^nb(|i-j|)=O(n^{2H})$ and $0<H<1$.  Can we give an estimate of the bounds of the eigenvalues, $\min \lambda_i$ and $\max \lambda_i$? I know that $\sum \lambda_i=trace(\Sigma_{22})=kb(0)$, so $b(0)\leq\max \lambda_i\leq kb(0)$, but can we give a better estimate? Thank you in advance.","Consider the (symmetric) covariance matrix $\Sigma_{22}(k,k)= \begin{bmatrix}     b(0)       & b(1)& b(2) & \cdots & b(k-1) \\     b(1)      & b(0) & b(1) & \cdots & b(k-2) \\     \vdots  && \vdots \\     b(k-1)      & b(k-2)& b(k-3)& \cdots & b(0) \end{bmatrix}, $ where $\sum_{i=1}^n\sum_{j=1}^nb(|i-j|)=O(n^{2H})$ and $0<H<1$.  Can we give an estimate of the bounds of the eigenvalues, $\min \lambda_i$ and $\max \lambda_i$? I know that $\sum \lambda_i=trace(\Sigma_{22})=kb(0)$, so $b(0)\leq\max \lambda_i\leq kb(0)$, but can we give a better estimate? Thank you in advance.",,"['linear-algebra', 'probability', 'statistics']"
16,Need help creating a single formula to find the probability of successfully rolling multiple 6-sided dice with conditions.,Need help creating a single formula to find the probability of successfully rolling multiple 6-sided dice with conditions.,,"Example question: If you roll 7 dice, what is the probability of successfully rolling a 5 or higher , at least 4 times? Let a=7 b=5 c=4. My goal is to plug a,b,c into a long formula and have the probability of success for the example above.","Example question: If you roll 7 dice, what is the probability of successfully rolling a 5 or higher , at least 4 times? Let a=7 b=5 c=4. My goal is to plug a,b,c into a long formula and have the probability of success for the example above.",,"['probability', 'statistics']"
17,Bounds on probability of sample mean in a small neighborhood rather than the tail,Bounds on probability of sample mean in a small neighborhood rather than the tail,,"Say we have i.i.d random variables $x_i$ whose mean and variance are $1$. Then the sample $s_n=\frac{1}{n}\sum_{i=1}^n x_i$ has mean $1$ and variance $\frac{1}{n}$. If we are given a small enough positive $\epsilon$, e.g., $\epsilon\in(0,0.5)$, what bound can we get for $P(0<s_n<\epsilon)$ for all $n$? For example, if we consider the Chebyshev inequality, we have \begin{align} P(0<s_n<\epsilon)&\leq P(|s_n-1|\geq 1-\epsilon)\\ &=P(|s_n-1|\geq \sqrt{n}(1-\epsilon)\cdot\frac{1}{\sqrt{n}})\\ &\leq\frac{1}{n(1-\epsilon)^2} \end{align} This bound is rather loose because of the first inequality. I also checked other inequalities, like Hoeffding's, yet they are all bounds for the form of $P(|s_n-1|\geq a)$. If we think about $x\sim\mathcal{N}(1,1)$, then $s_n\sim\mathcal{N}(1,\frac{1}{n})$. Thus,$$P(0<s_n<\epsilon)<\epsilon\frac{\sqrt{n}}{\sqrt{2\pi}}\exp\left(-\frac{n(1-\epsilon)^2}{2}\right)<t_0\frac{\sqrt{n}}{\exp(n)}\epsilon,$$ where $t_0$ is the constant coefficient. I wonder what bounds, similar to the above that decreases to $0$ as $\epsilon\to 0$, can be obtained? Perhaps some assumptions shall be placed. Thanks.","Say we have i.i.d random variables $x_i$ whose mean and variance are $1$. Then the sample $s_n=\frac{1}{n}\sum_{i=1}^n x_i$ has mean $1$ and variance $\frac{1}{n}$. If we are given a small enough positive $\epsilon$, e.g., $\epsilon\in(0,0.5)$, what bound can we get for $P(0<s_n<\epsilon)$ for all $n$? For example, if we consider the Chebyshev inequality, we have \begin{align} P(0<s_n<\epsilon)&\leq P(|s_n-1|\geq 1-\epsilon)\\ &=P(|s_n-1|\geq \sqrt{n}(1-\epsilon)\cdot\frac{1}{\sqrt{n}})\\ &\leq\frac{1}{n(1-\epsilon)^2} \end{align} This bound is rather loose because of the first inequality. I also checked other inequalities, like Hoeffding's, yet they are all bounds for the form of $P(|s_n-1|\geq a)$. If we think about $x\sim\mathcal{N}(1,1)$, then $s_n\sim\mathcal{N}(1,\frac{1}{n})$. Thus,$$P(0<s_n<\epsilon)<\epsilon\frac{\sqrt{n}}{\sqrt{2\pi}}\exp\left(-\frac{n(1-\epsilon)^2}{2}\right)<t_0\frac{\sqrt{n}}{\exp(n)}\epsilon,$$ where $t_0$ is the constant coefficient. I wonder what bounds, similar to the above that decreases to $0$ as $\epsilon\to 0$, can be obtained? Perhaps some assumptions shall be placed. Thanks.",,"['probability', 'statistics', 'law-of-large-numbers']"
18,Integral inequality. Useful for optimization,Integral inequality. Useful for optimization,,"Let us introduce the following notation: \begin{eqnarray} && f_\alpha (z)=\frac{1}{2\pi}\int_{-\infty}^\infty \cos(tz)e^{-|t|^\alpha} \, dt \\ && F_{\alpha}(x)=\int_{-\infty}^x f_{\alpha}(z)\,dz \end{eqnarray} with $\alpha \in (1,2]$. I am trying to prove mathematically my empirical result that if $\alpha_1<\alpha_2$ then $$\int_{-\infty}^x F_{\alpha_1}(z)dz\geq\int_{-\infty}^x F_{\alpha_2}(z)dz, \quad \forall x \in \mathbb{R}\quad \forall \alpha_1,\alpha_2 \in (1,2]$$ which according to Ruszczynski and Dencheva (2003) is equivalent to $$\int_{-\infty}^x (x-z)f_{\alpha_1} (z) \, dz\geq\int_{-\infty}^x (x-z) f_{\alpha_2} (z)  dz, \quad \forall x \in \mathbb{R}\quad \forall \alpha_1,\alpha_2 \in (1,2].$$ To solve the problem, it is enough to prove one of them.  When solving the first inequality, we end up with tripple integrals while we will end up with double integrals if we solve the second inequality. $$$$ $$$$ PS1. We denote $\int_{-\infty}^x (x-z)f_{\alpha}(z)\,dz$ with $F_{\alpha}^{(2)}(x)$, i.e. $$F_{\alpha}^{(2)}(x)=\int_{-\infty}^x F_{\alpha}(z)\,dz=\int_{-\infty}^x (x-z)f_{\alpha}(z)\,dz$$ PS2. $f_{\alpha}(x)$ and $F_{\alpha}(x)$ are the density and distribution function of a stable random variable $X \sim S_{\alpha}(1,0,0)$ respectively. See  this link: https://en.wikipedia.org/wiki/Stable_distribution","Let us introduce the following notation: \begin{eqnarray} && f_\alpha (z)=\frac{1}{2\pi}\int_{-\infty}^\infty \cos(tz)e^{-|t|^\alpha} \, dt \\ && F_{\alpha}(x)=\int_{-\infty}^x f_{\alpha}(z)\,dz \end{eqnarray} with $\alpha \in (1,2]$. I am trying to prove mathematically my empirical result that if $\alpha_1<\alpha_2$ then $$\int_{-\infty}^x F_{\alpha_1}(z)dz\geq\int_{-\infty}^x F_{\alpha_2}(z)dz, \quad \forall x \in \mathbb{R}\quad \forall \alpha_1,\alpha_2 \in (1,2]$$ which according to Ruszczynski and Dencheva (2003) is equivalent to $$\int_{-\infty}^x (x-z)f_{\alpha_1} (z) \, dz\geq\int_{-\infty}^x (x-z) f_{\alpha_2} (z)  dz, \quad \forall x \in \mathbb{R}\quad \forall \alpha_1,\alpha_2 \in (1,2].$$ To solve the problem, it is enough to prove one of them.  When solving the first inequality, we end up with tripple integrals while we will end up with double integrals if we solve the second inequality. $$$$ $$$$ PS1. We denote $\int_{-\infty}^x (x-z)f_{\alpha}(z)\,dz$ with $F_{\alpha}^{(2)}(x)$, i.e. $$F_{\alpha}^{(2)}(x)=\int_{-\infty}^x F_{\alpha}(z)\,dz=\int_{-\infty}^x (x-z)f_{\alpha}(z)\,dz$$ PS2. $f_{\alpha}(x)$ and $F_{\alpha}(x)$ are the density and distribution function of a stable random variable $X \sim S_{\alpha}(1,0,0)$ respectively. See  this link: https://en.wikipedia.org/wiki/Stable_distribution",,"['integration', 'statistics']"
19,"Time series analysis, moving-average model, ARMA model","Time series analysis, moving-average model, ARMA model",,"I'm reading documents about time series analysis, including Autoregressive–moving-average model , Moving average model , Wold's theorem , etc. The notation MA(q) refers to the moving average model of order q: $$X_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \cdots + \theta_q \varepsilon_{t-q} \qquad (*)$$ where μ is the mean of the series, the $θ_1$, ..., $θ_q$ are the parameters of the model and the $ε_t$, $ε_{t−1}$,..., $ε_{t−q}$ are white noise error terms. 1) I would understand if we would want to express $X_t$ as a linear combination of past values $X_{t-1}$, $X_{t-2}$, $X_{t-3}$, etc. i.e. : $$X_t = a_1 X_{t-1} + a_2 X_{t-2} + ... + a_q X_{t-q} + \epsilon_t$$ and if we would want to find optimal values for $(a_i)_{i \leq q}$. This would make sense to me (by the way, does this approach exist, what's its name?). 2) But here I don't understand why we try to express $X_t$ in terms of past values of something which is totally uncontrolled and has nothing to do with $(X_t)$ : $\epsilon_t$, i.e. some random white noise! Why do we do this in the moving average model ? (By the way, how are exactly defined the $\epsilon_t$ ?)","I'm reading documents about time series analysis, including Autoregressive–moving-average model , Moving average model , Wold's theorem , etc. The notation MA(q) refers to the moving average model of order q: $$X_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \cdots + \theta_q \varepsilon_{t-q} \qquad (*)$$ where μ is the mean of the series, the $θ_1$, ..., $θ_q$ are the parameters of the model and the $ε_t$, $ε_{t−1}$,..., $ε_{t−q}$ are white noise error terms. 1) I would understand if we would want to express $X_t$ as a linear combination of past values $X_{t-1}$, $X_{t-2}$, $X_{t-3}$, etc. i.e. : $$X_t = a_1 X_{t-1} + a_2 X_{t-2} + ... + a_q X_{t-q} + \epsilon_t$$ and if we would want to find optimal values for $(a_i)_{i \leq q}$. This would make sense to me (by the way, does this approach exist, what's its name?). 2) But here I don't understand why we try to express $X_t$ in terms of past values of something which is totally uncontrolled and has nothing to do with $(X_t)$ : $\epsilon_t$, i.e. some random white noise! Why do we do this in the moving average model ? (By the way, how are exactly defined the $\epsilon_t$ ?)",,"['real-analysis', 'statistics', 'average', 'time-series']"
20,Distribution MSR,Distribution MSR,,"We have $Y_i = \beta_0 +\beta_1(X_i -\bar X )+\epsilon_i$ for i=1,...,n $$\epsilon_i \sim N(0,\sigma^2)$$ We know that $SSR= Y^T P_xY - n\bar Y^2=Y^T (P_x -n^{-1} J_nJ_n^T)Y$ $$J_n=\big[\begin{matrix}1 \\ ... \\ 1\end{matrix}\big]_{nxn}$$ $$MSR=SSR/(r-1)$$ How can I find the distribution of MSR? Suppose $Y\sim N_p(\mu, V)$ and Rank(V)=p If AV is idempotent of ranks then $Y^TAY \sim \chi_s^2(\lambda)$ where $\lambda =1/2 (\mu^TA\mu)$ Will I use this result? I have done for $SSR=Y^TP_xY$ Uncorrected model. By using this result and find $$\frac{Y^TP_xY}{\sigma^2}=Y^TBY\sim \chi_s^2(\lambda)$$ where $\lambda =1/2 (\mu^TB\mu)$ and$B= \sigma^{-2 }P_x$ But I couldn't find for corrected model MSR.","We have $Y_i = \beta_0 +\beta_1(X_i -\bar X )+\epsilon_i$ for i=1,...,n $$\epsilon_i \sim N(0,\sigma^2)$$ We know that $SSR= Y^T P_xY - n\bar Y^2=Y^T (P_x -n^{-1} J_nJ_n^T)Y$ $$J_n=\big[\begin{matrix}1 \\ ... \\ 1\end{matrix}\big]_{nxn}$$ $$MSR=SSR/(r-1)$$ How can I find the distribution of MSR? Suppose $Y\sim N_p(\mu, V)$ and Rank(V)=p If AV is idempotent of ranks then $Y^TAY \sim \chi_s^2(\lambda)$ where $\lambda =1/2 (\mu^TA\mu)$ Will I use this result? I have done for $SSR=Y^TP_xY$ Uncorrected model. By using this result and find $$\frac{Y^TP_xY}{\sigma^2}=Y^TBY\sim \chi_s^2(\lambda)$$ where $\lambda =1/2 (\mu^TB\mu)$ and$B= \sigma^{-2 }P_x$ But I couldn't find for corrected model MSR.",,"['calculus', 'probability-theory', 'statistics', 'stochastic-processes', 'stochastic-calculus']"
21,"Let $\theta >0$ be unknown and suppose that $(X,Y)$ is uniform over the triangular region with vertices at $(0,0)$,$(\theta, 0)$, and $(0,\theta)$.","Let  be unknown and suppose that  is uniform over the triangular region with vertices at ,, and .","\theta >0 (X,Y) (0,0) (\theta, 0) (0,\theta)","Let $\theta >0$ be unknown and suppose that $(X,Y)$ is uniform over the triangular region with vertices at $(0,0)$,$(\theta, 0)$, and $(0,\theta)$. Let $(X_i,Y_i)$ be iid as $(X,Y)$. Find a one dimensional sufficient statistic $T$ for $\theta$, and prove it's sufficient. My thoughts: I want to find the likelihood function then try to separate as two functions so that I can get a statistic, but in this case, I have difficulty finding the density function. Any hints would be appreciated.","Let $\theta >0$ be unknown and suppose that $(X,Y)$ is uniform over the triangular region with vertices at $(0,0)$,$(\theta, 0)$, and $(0,\theta)$. Let $(X_i,Y_i)$ be iid as $(X,Y)$. Find a one dimensional sufficient statistic $T$ for $\theta$, and prove it's sufficient. My thoughts: I want to find the likelihood function then try to separate as two functions so that I can get a statistic, but in this case, I have difficulty finding the density function. Any hints would be appreciated.",,"['probability', 'statistics', 'sufficient-statistics']"
22,Compute the probability of a joint event involving two independent standard normals,Compute the probability of a joint event involving two independent standard normals,,"Suppose $X$ and $Y$ are independent, standard normal random variables. I'm trying to compute the probability of the event $$ \{X \leq x, Y \leq kX\} $$ where $k$ is a positive constant. The probability is given by the integral $$ \int_{-\infty}^x \int_{-\infty}^{ks} \phi(t)\,\phi(s)\, dt \, ds $$ where $$ \phi(s) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}s^2} $$ I'm only able to find a closed form solution when $k = 1$. Is it possible to obtain a closed form solution for arbitrary $k > 0$? I've tried the usual integration tools but seem to be coming up empty. Thanks!","Suppose $X$ and $Y$ are independent, standard normal random variables. I'm trying to compute the probability of the event $$ \{X \leq x, Y \leq kX\} $$ where $k$ is a positive constant. The probability is given by the integral $$ \int_{-\infty}^x \int_{-\infty}^{ks} \phi(t)\,\phi(s)\, dt \, ds $$ where $$ \phi(s) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}s^2} $$ I'm only able to find a closed form solution when $k = 1$. Is it possible to obtain a closed form solution for arbitrary $k > 0$? I've tried the usual integration tools but seem to be coming up empty. Thanks!",,"['probability', 'integration', 'statistics', 'normal-distribution']"
23,How does Restricted Boltzmann Machine (RBM) try to model the distribution of data?,How does Restricted Boltzmann Machine (RBM) try to model the distribution of data?,,"An RBM is a non-directed graphical model that defines the distribution over some input vector X. I know it's going to model the distribution of that those vectors in my training data X using a layer of binary hidden units, but i don't understand if it's an unsupervised method, then what kind of information about the labeling of the data gives us the distribution? I would appreciate if someone could give a brief intuitive explanation about this.","An RBM is a non-directed graphical model that defines the distribution over some input vector X. I know it's going to model the distribution of that those vectors in my training data X using a layer of binary hidden units, but i don't understand if it's an unsupervised method, then what kind of information about the labeling of the data gives us the distribution? I would appreciate if someone could give a brief intuitive explanation about this.",,"['statistics', 'machine-learning', 'neural-networks']"
24,Do i get the right MLE and 90% confidence interval of normal distribution?,Do i get the right MLE and 90% confidence interval of normal distribution?,,I think i do right in step1 above. But i wonder whether i get the right confidence interval of mu and sigma in step2?,I think i do right in step1 above. But i wonder whether i get the right confidence interval of mu and sigma in step2?,,"['statistics', 'statistical-inference', 'statistical-mechanics']"
25,Does linear regression form a subspace?,Does linear regression form a subspace?,,"The author writes Given a vector of inputs $X^T = (X_1, \dots ,X_p)$, we can predict an output $Y$ via $$ \hat{Y} = \beta_0 + \sum_{j = 1}^p X_j \beta_j$$ He goes on to note that if we include a 1 in the vector $X$, making it a $p+1$ dimensional vector, we can write the regression as an inner product $\hat{Y} = X^T\beta$, and this is a subspace.  If the constant is not included, it forms an affine set. I'm not convinced that $X^T\beta$ forms a subspace of $X^T \in \mathbb{R}^{p+1}$, mostly because $\vec{0} \not \in X^T\beta$.  Unless the author thinks that $X^T\beta$ is a subspace in beta, in which case I agree. Maybe someone could clarify for me.  The book is Elements of Statistical Learning , page 12.","The author writes Given a vector of inputs $X^T = (X_1, \dots ,X_p)$, we can predict an output $Y$ via $$ \hat{Y} = \beta_0 + \sum_{j = 1}^p X_j \beta_j$$ He goes on to note that if we include a 1 in the vector $X$, making it a $p+1$ dimensional vector, we can write the regression as an inner product $\hat{Y} = X^T\beta$, and this is a subspace.  If the constant is not included, it forms an affine set. I'm not convinced that $X^T\beta$ forms a subspace of $X^T \in \mathbb{R}^{p+1}$, mostly because $\vec{0} \not \in X^T\beta$.  Unless the author thinks that $X^T\beta$ is a subspace in beta, in which case I agree. Maybe someone could clarify for me.  The book is Elements of Statistical Learning , page 12.",,"['linear-algebra', 'statistics', 'regression', 'machine-learning']"
26,Hypothesis testing (parameters),Hypothesis testing (parameters),,"I am new to statistics, but familiar with basic concepts. The lecture that confuses me is Hypothesis testing. Although the idea behind it seems ok, I couldn't understand the first exercise I found. Exercise : Average time of execution of computer program is 45s. We want to buy a better PC so we tested that program on 30 other PCs and got average time of 44.5s and standard deviation 2. Based on these assumptions, should we buy a new PC? Result from the book : $$H_0:m=45$$   $$H_1:m<45$$   $$n=30$$   $$\overline{X_n}=44.5$$   $$\overline{S_n}=2$$   $$T_n=\frac{\overline{X_n}-m_0}{\sqrt{\overline{S_n^2}}}=\frac{44.5-45}{2}=\sqrt{30}=-1.3639$$   $$W=\{T_n<c\}$$   $$p=P\{T_n<\overline{T_n}\}=0.0907$$   p is greater then alpha so we accept hypothesis null hypothesis My understanding :  We want to prove that we shouldn't buy new PC because it runs as long on the others average, and alternative hypothesis says that we should buy a new one. So I don't understand what the last three rows, from Tn to p. Can you please tell me what is the idea behind this?","I am new to statistics, but familiar with basic concepts. The lecture that confuses me is Hypothesis testing. Although the idea behind it seems ok, I couldn't understand the first exercise I found. Exercise : Average time of execution of computer program is 45s. We want to buy a better PC so we tested that program on 30 other PCs and got average time of 44.5s and standard deviation 2. Based on these assumptions, should we buy a new PC? Result from the book : $$H_0:m=45$$   $$H_1:m<45$$   $$n=30$$   $$\overline{X_n}=44.5$$   $$\overline{S_n}=2$$   $$T_n=\frac{\overline{X_n}-m_0}{\sqrt{\overline{S_n^2}}}=\frac{44.5-45}{2}=\sqrt{30}=-1.3639$$   $$W=\{T_n<c\}$$   $$p=P\{T_n<\overline{T_n}\}=0.0907$$   p is greater then alpha so we accept hypothesis null hypothesis My understanding :  We want to prove that we shouldn't buy new PC because it runs as long on the others average, and alternative hypothesis says that we should buy a new one. So I don't understand what the last three rows, from Tn to p. Can you please tell me what is the idea behind this?",,"['statistics', 'normal-distribution', 'hypothesis-testing']"
27,Calculate SE from a Given Margin of Error,Calculate SE from a Given Margin of Error,,"I have data from the Census bureau that has means and margins of error that are defined at 90% confidence.  For example, a zip-code population estimate of 17042 with a 90% moe of 278.  I'd like to do some inference based off of this data, but I need to reverse engineer to get the standard error of the estimate.  Given the mean and this margin of error, how can I come up with a standard error?","I have data from the Census bureau that has means and margins of error that are defined at 90% confidence.  For example, a zip-code population estimate of 17042 with a 90% moe of 278.  I'd like to do some inference based off of this data, but I need to reverse engineer to get the standard error of the estimate.  Given the mean and this margin of error, how can I come up with a standard error?",,['statistics']
28,"For what $r,s$ exist unbiased estimation of $f(p) = p^{r}(1 - p)^{s}$ for binomial distribution?",For what  exist unbiased estimation of  for binomial distribution?,"r,s f(p) = p^{r}(1 - p)^{s}","We have sample $x_1, ..., x_n$ generated by independent binomial random variables $\xi_1, ..., \xi_n$. We know parameter $k$ but don't know probability $p$. k is number of tests: $\xi_i \sim Binomial(k,p)$ The task is to find numbers $r,s$, that there is exist unbiased estimation for $$f(p) = p^{r}(1 - p)^{s}$$ The problem is that I don't understand the general approach how to test existence of unbiased estimation. Could you help me please?","We have sample $x_1, ..., x_n$ generated by independent binomial random variables $\xi_1, ..., \xi_n$. We know parameter $k$ but don't know probability $p$. k is number of tests: $\xi_i \sim Binomial(k,p)$ The task is to find numbers $r,s$, that there is exist unbiased estimation for $$f(p) = p^{r}(1 - p)^{s}$$ The problem is that I don't understand the general approach how to test existence of unbiased estimation. Could you help me please?",,"['probability', 'statistics', 'estimation', 'binomial-distribution']"
29,standard deviation for regression,standard deviation for regression,,"The first slide is the denifition of simple linear regression model, the second slides is an example I have two questions:  1.Did I get the right calculation of the standard deviation?  2.I still have difficulties in understanding Confidence interval for mean and Confidence interval and Prediction interval. For the example above, if I was correct,the standard deviation would be 0.0634 then y3 would be :y3~(10,0.0634^2), so we can say we are 95% sure y3 would be from 10-2*0.0634 to 10+2*0.0634, why we still need Confidence interval for mean and and Prediction interval?","The first slide is the denifition of simple linear regression model, the second slides is an example I have two questions:  1.Did I get the right calculation of the standard deviation?  2.I still have difficulties in understanding Confidence interval for mean and Confidence interval and Prediction interval. For the example above, if I was correct,the standard deviation would be 0.0634 then y3 would be :y3~(10,0.0634^2), so we can say we are 95% sure y3 would be from 10-2*0.0634 to 10+2*0.0634, why we still need Confidence interval for mean and and Prediction interval?",,"['statistics', 'regression']"
30,Finding conditional probability distribution (X|Y) from (Y|X),Finding conditional probability distribution (X|Y) from (Y|X),,"$(Y,X)$ has a joint distribution where the marginal of $X$ is a standard normal and $Y|X \sim U \left[|X|-\frac{1}{2},|X|+\frac{1}{2}\right] $ where $U[a,b] $ means uniform in the interval [a,b]. How can I find the distribution of $X$ given $Y$ and conditional expectation function $E(X|Y)$? I tried to multiply p.d.f. of $Y|X$ and $X$ to find joint distribution first, but support of $Y|X $ confused me. Thanks in advance :)","$(Y,X)$ has a joint distribution where the marginal of $X$ is a standard normal and $Y|X \sim U \left[|X|-\frac{1}{2},|X|+\frac{1}{2}\right] $ where $U[a,b] $ means uniform in the interval [a,b]. How can I find the distribution of $X$ given $Y$ and conditional expectation function $E(X|Y)$? I tried to multiply p.d.f. of $Y|X$ and $X$ to find joint distribution first, but support of $Y|X $ confused me. Thanks in advance :)",,"['statistics', 'normal-distribution', 'conditional-expectation', 'uniform-distribution']"
31,Need help understanding central limit theorem,Need help understanding central limit theorem,,"I am very confused about CLT and have searched on the internet but found nothing that solved my confusion. How can I solve a problem like this with CLT? Let $Y =\operatorname{Pois}(n)$.  Using Normal approximation, aka the CLT, give an estimate of the probability $$p\Big[|Y-n| \geq 2\sqrt{n}\Big].$$","I am very confused about CLT and have searched on the internet but found nothing that solved my confusion. How can I solve a problem like this with CLT? Let $Y =\operatorname{Pois}(n)$.  Using Normal approximation, aka the CLT, give an estimate of the probability $$p\Big[|Y-n| \geq 2\sqrt{n}\Big].$$",,"['statistics', 'central-limit-theorem']"
32,What Counts as Statistically Significant?,What Counts as Statistically Significant?,,"Problem: In clinical trails of Nasonex, $3774$ adult and adolescent allergy patients were randomly divided into two groups. The patients in Group $1$ ( experimental group ) received $200$ mcg of Nasonex, while the patients in Group $2$ ( control group ) received a placebo. Of the $2013$ patients in the experimental group, $547$ reported headaches as a side effect. Of the $1671$ patients in the control group, $368$ reported headaches as side effect. Is there   sufficient evidence to support the claim that users of Nasonex experience   headaches? Partial Solution Let $p_1$ and $p_2$ be the ratio of patients in the two control groups that experience headaches. \begin{eqnarray*} p_1 &=& \frac{547}{2013} = 0.27173 \\ p_2 &=& \frac{368}{3774-2013} = 0.2089722 \\ \end{eqnarray*} Is this statistically significant? How do I tell?","Problem: In clinical trails of Nasonex, $3774$ adult and adolescent allergy patients were randomly divided into two groups. The patients in Group $1$ ( experimental group ) received $200$ mcg of Nasonex, while the patients in Group $2$ ( control group ) received a placebo. Of the $2013$ patients in the experimental group, $547$ reported headaches as a side effect. Of the $1671$ patients in the control group, $368$ reported headaches as side effect. Is there   sufficient evidence to support the claim that users of Nasonex experience   headaches? Partial Solution Let $p_1$ and $p_2$ be the ratio of patients in the two control groups that experience headaches. \begin{eqnarray*} p_1 &=& \frac{547}{2013} = 0.27173 \\ p_2 &=& \frac{368}{3774-2013} = 0.2089722 \\ \end{eqnarray*} Is this statistically significant? How do I tell?",,"['probability', 'statistics', 'probability-distributions']"
33,Find the joint probability function of X and Y.,Find the joint probability function of X and Y.,,"The number of defects per yard $Y$ for a certain fabric is known to have a Poisson distribution with parameter $x$, i.e. $f(Y|X = x)$ has a Poisson distribution with parameter $x$. However, $X$ itself is a random variable with probability density function given by: $$f(x) = e^{-x}, \quad \text{ for } x \geq 0$$ Find the joint probability function for $X$ and $Y$. It confuses me a bit since $Y$ is a discrete random variable whereas $X$ is continuous as given by the variable. Apologies for the lack of attempt since I don't know where to begin in this case.","The number of defects per yard $Y$ for a certain fabric is known to have a Poisson distribution with parameter $x$, i.e. $f(Y|X = x)$ has a Poisson distribution with parameter $x$. However, $X$ itself is a random variable with probability density function given by: $$f(x) = e^{-x}, \quad \text{ for } x \geq 0$$ Find the joint probability function for $X$ and $Y$. It confuses me a bit since $Y$ is a discrete random variable whereas $X$ is continuous as given by the variable. Apologies for the lack of attempt since I don't know where to begin in this case.",,"['statistics', 'probability-distributions', 'poisson-distribution', 'bivariate-distributions']"
34,"If I have that $X \sim \chi^2_{1}$ and $Y \sim \chi^2_{2}$ are independent, how can I show that $4XY \sim Y^2$?","If I have that  and  are independent, how can I show that ?",X \sim \chi^2_{1} Y \sim \chi^2_{2} 4XY \sim Y^2,"If I have that $X \sim \chi^2_{1}$ and $Y \sim \chi^2_{2}$ are independent, how can I show that $4XY \sim Y^2$? Here, I have that $X \sim \chi^2_{1}$ means that $X$ is a chi-square random variable with 1 degree of freedom and that $Y \sim \chi^2_{2}$ has two degrees of freedom. Is there a way to do this without integration and more by representation? I have thought about moment generating functions, or taking the expectations, but am not sure how to do it. I've also thought about taking logs of both sides to induce additivity. Finally, the $4XY$ looks suspiciously set up as I know that $(X+Y)^2 - (X-Y)^2 = 4XY$. Does anyone have any ideas?","If I have that $X \sim \chi^2_{1}$ and $Y \sim \chi^2_{2}$ are independent, how can I show that $4XY \sim Y^2$? Here, I have that $X \sim \chi^2_{1}$ means that $X$ is a chi-square random variable with 1 degree of freedom and that $Y \sim \chi^2_{2}$ has two degrees of freedom. Is there a way to do this without integration and more by representation? I have thought about moment generating functions, or taking the expectations, but am not sure how to do it. I've also thought about taking logs of both sides to induce additivity. Finally, the $4XY$ looks suspiciously set up as I know that $(X+Y)^2 - (X-Y)^2 = 4XY$. Does anyone have any ideas?",,"['probability', 'statistics']"
35,Would you ever stop rolling the die? [duplicate],Would you ever stop rolling the die? [duplicate],,"This question already has an answer here : Toss a fair die until the cumulative sum is a perfect square-Expected Value (1 answer) Closed 9 years ago . You have a six-sided die. You keep a cumulative total of your dice rolls. (E.g. if you roll a 3, then a 5, then a 2, your cumulative total is 10.) If your cumulative total is ever equal to a perfect square, then you lose, and you go home with nothing. Otherwise, you can choose to go home with a payout of your cumulative total, or to roll the die again. My question is about the optimal strategy for this game. In particular, this means that I am looking for an answer to this question: if my cumulative total is $n$, do I choose to roll or not to roll in order to maximize my cumulative total? Is there some integer $N$ after which the answer to this question is always to roll? I think that there is such an integer, and I conjecture that this integer is $4$. My reasoning is that the square numbers become sufficiently sparse for the expected value to always be in increased by rolling the die again. As an example, suppose your cumulative total is $35$. Rolling a $1$ and hitting 36 means we go home with nothing, so the expected value of rolling once is: $$E(Roll|35) = \frac 0 6 + \frac {37} 6 + \frac {38} 6 + \frac{39} 6 + \frac {40} {6} + \frac{41}{6} = 32.5$$ i.e. $$E(Roll|35) = \frac 1 6 \cdot (37 + 38 + 39 + 40 + 41) = 32.5$$ But the next square after $35$ is $49$. So in the event that we don't roll a $36$, we get to keep rolling the die at no risk as long as the cumulative total is less than $42$. For the sake of simplification, let's say that if we roll and don't hit $36$, then we will roll once more. That die-roll has an expected value of $3.5$. This means the expected value of rolling on $35$ is: $$E(Roll|35) = \frac 1 6 \cdot (40.5 + 41.5 + 42.5 + 43.5 + 44.5) = 35.42$$ And since $35.42 > 35$, the profit-maximizing choice is to roll again. And this strategy can be applied for every total. I don't see when this would cease to be the reasonable move, though I haven't attempted to verify it computationally. I intuitively think about this in terms of diverging sequences. I recently had this question in a job interview, and thought it was quite interesting. (And counter-intuitive, since this profit-maximizing strategy invariably results in going home with nothing.)","This question already has an answer here : Toss a fair die until the cumulative sum is a perfect square-Expected Value (1 answer) Closed 9 years ago . You have a six-sided die. You keep a cumulative total of your dice rolls. (E.g. if you roll a 3, then a 5, then a 2, your cumulative total is 10.) If your cumulative total is ever equal to a perfect square, then you lose, and you go home with nothing. Otherwise, you can choose to go home with a payout of your cumulative total, or to roll the die again. My question is about the optimal strategy for this game. In particular, this means that I am looking for an answer to this question: if my cumulative total is $n$, do I choose to roll or not to roll in order to maximize my cumulative total? Is there some integer $N$ after which the answer to this question is always to roll? I think that there is such an integer, and I conjecture that this integer is $4$. My reasoning is that the square numbers become sufficiently sparse for the expected value to always be in increased by rolling the die again. As an example, suppose your cumulative total is $35$. Rolling a $1$ and hitting 36 means we go home with nothing, so the expected value of rolling once is: $$E(Roll|35) = \frac 0 6 + \frac {37} 6 + \frac {38} 6 + \frac{39} 6 + \frac {40} {6} + \frac{41}{6} = 32.5$$ i.e. $$E(Roll|35) = \frac 1 6 \cdot (37 + 38 + 39 + 40 + 41) = 32.5$$ But the next square after $35$ is $49$. So in the event that we don't roll a $36$, we get to keep rolling the die at no risk as long as the cumulative total is less than $42$. For the sake of simplification, let's say that if we roll and don't hit $36$, then we will roll once more. That die-roll has an expected value of $3.5$. This means the expected value of rolling on $35$ is: $$E(Roll|35) = \frac 1 6 \cdot (40.5 + 41.5 + 42.5 + 43.5 + 44.5) = 35.42$$ And since $35.42 > 35$, the profit-maximizing choice is to roll again. And this strategy can be applied for every total. I don't see when this would cease to be the reasonable move, though I haven't attempted to verify it computationally. I intuitively think about this in terms of diverging sequences. I recently had this question in a job interview, and thought it was quite interesting. (And counter-intuitive, since this profit-maximizing strategy invariably results in going home with nothing.)",,"['probability', 'discrete-mathematics', 'expectation', 'gambling']"
36,"Given 100 coin tosses, the largest string of same results in a row is...?","Given 100 coin tosses, the largest string of same results in a row is...?",,"My question is, if someone tossed a fair coin 100 times, what is the most number of times that a result will likely present itself in a row. Alternatively put, what is the largest string of consecutive flips of the same result that has a probability of occurring >50% in 100 coin flips. Conversely, I think this question can be answered by giving the equation for the expected number of flips of a fair coin before X number of consecutive flips  are the same results (without constraining that the results be heads or tails).","My question is, if someone tossed a fair coin 100 times, what is the most number of times that a result will likely present itself in a row. Alternatively put, what is the largest string of consecutive flips of the same result that has a probability of occurring >50% in 100 coin flips. Conversely, I think this question can be answered by giving the equation for the expected number of flips of a fair coin before X number of consecutive flips  are the same results (without constraining that the results be heads or tails).",,"['probability', 'statistics']"
37,Bias of two estimators,Bias of two estimators,,"I hope someone can help me. I have some trouble calculating the bias of two estimators.Unluckily it is really urgent because I hold a presentation next week. The topic is nonparametric local regression. In order to compare kernel estimators I have to calculate the bias of the following two estimators: $m_1(x,h)=n^{-1}f(x)^{-1}\sum_{i=0}^nK_{h}(X_i-x)Y_i$ and $m_2(x,h)=n^{-1}\sum_{i=0}^nf(X_i)^{-1}K_{h}(X_i-x)Y_i$. For the second estimator I had until now: $$E[m_2(x,h)]=\int K_{h}(z-x)m(z)\frac{f(z)}{f(z)}dz=\int K(\frac{z-x}{h})m(z)dz=h\int K(u)m(x+uh)du$$ $$= hm(x)\int K(u)du+\frac{1}{2}h^3m´´(x)\int u^2K(u)du=hm(x)+\frac{1}{2}h^3m´´(x)\int u^2K(u)du+o(h^2)$$ Somehow I´m stuck here. I don´t know what to do. Please could someone help me!","I hope someone can help me. I have some trouble calculating the bias of two estimators.Unluckily it is really urgent because I hold a presentation next week. The topic is nonparametric local regression. In order to compare kernel estimators I have to calculate the bias of the following two estimators: $m_1(x,h)=n^{-1}f(x)^{-1}\sum_{i=0}^nK_{h}(X_i-x)Y_i$ and $m_2(x,h)=n^{-1}\sum_{i=0}^nf(X_i)^{-1}K_{h}(X_i-x)Y_i$. For the second estimator I had until now: $$E[m_2(x,h)]=\int K_{h}(z-x)m(z)\frac{f(z)}{f(z)}dz=\int K(\frac{z-x}{h})m(z)dz=h\int K(u)m(x+uh)du$$ $$= hm(x)\int K(u)du+\frac{1}{2}h^3m´´(x)\int u^2K(u)du=hm(x)+\frac{1}{2}h^3m´´(x)\int u^2K(u)du+o(h^2)$$ Somehow I´m stuck here. I don´t know what to do. Please could someone help me!",,['statistics']
38,Independence of time series data,Independence of time series data,,"I have a time series data with $52$ observations and I would like to check for the independence between observations. The ACF for correlation and covariance of my data look I am aware that $covariance = 0$ does not imply independence, except for Gaussian process. I wonder if I can use ACF to show the independence of my data or there are ways to justify it? Many thanks.","I have a time series data with $52$ observations and I would like to check for the independence between observations. The ACF for correlation and covariance of my data look I am aware that $covariance = 0$ does not imply independence, except for Gaussian process. I wonder if I can use ACF to show the independence of my data or there are ways to justify it? Many thanks.",,"['probability', 'statistics', 'stochastic-processes', 'time-series']"
39,An exponential family problem,An exponential family problem,,"I don't know how to express the problem to the form $f(x|\theta)=h(x)c(\theta)(\sum_{i=1}^{k}\omega_i(\theta)t_i(x))$ Let $X$ have pdf $f(x)=\frac{1}{\beta}e^{-(x-\alpha)/\beta}$, $x>\alpha$ Determine whether $f(x)$ is an exponential a. if both $\alpha$ and $\beta$ are unknown. b. if only $\beta$ is unknown I thought those questions are same but it is not.","I don't know how to express the problem to the form $f(x|\theta)=h(x)c(\theta)(\sum_{i=1}^{k}\omega_i(\theta)t_i(x))$ Let $X$ have pdf $f(x)=\frac{1}{\beta}e^{-(x-\alpha)/\beta}$, $x>\alpha$ Determine whether $f(x)$ is an exponential a. if both $\alpha$ and $\beta$ are unknown. b. if only $\beta$ is unknown I thought those questions are same but it is not.",,"['statistics', 'exponential-distribution']"
40,How can I explain my logic? - Related to Herfindahl index,How can I explain my logic? - Related to Herfindahl index,,"I've tried to measure something that I have in mind. My problem is as following: Let's assume that there is a group with 8 members. There are two cases: First, A group consists of 4 subgroups each with 1,1,2, and 4 members.(1+1+2+4=8) Second, A group consists of 4 subgroups each with 2 members (2+2+2+2=8) I want to calculate the relative size of a subgroup with 2 members. If I calculated it as 2/8, it does not reflect the size of the other subgroups. In the first case, a subgroup with 2 members is likely to be a minor subgroups because of a major subgroup with 4 members. But in the latter case, a subgroup with 2 members is not either minor or major subgroup because other subgroups have 2 members. To reflect this concern, I try to adopt Herfindahl index. First of all, divide the number of members of subgroups by total members(=8). For example, in the group with 1,1,2,4 members, the value is 0.125, 0.125, 0.25, 0.5. Then, I calculate the square value (0.0156,0.0156,0.0625,0.25). The sum of it is Herfindahl index as you know. The important things, here, is that I divide the square value by sum of square value.  In the end, the 2-member subgroup in the group with 1,1,2,4 members have the value-0.181818. Contrary to it, the 2-member subgroup in the group with 2,2,2,2 members have the value-0.25 I think it can adequately reflect my thought because the 2 member subgroup in the former case (=0.1818) has relatively smaller size than in the latter group (=0.25). I can understand it intuitively but I cannot explain it logically. Maybe one reason might be I cannot find the reference. Calculating Herfindahl index is common, but dividing the fraction by the index is unusual to me. Does anyone know the similar situation? Does it make sense? If you know any reference or have any recommendation to my logic, please let me know. Thanks a lot!","I've tried to measure something that I have in mind. My problem is as following: Let's assume that there is a group with 8 members. There are two cases: First, A group consists of 4 subgroups each with 1,1,2, and 4 members.(1+1+2+4=8) Second, A group consists of 4 subgroups each with 2 members (2+2+2+2=8) I want to calculate the relative size of a subgroup with 2 members. If I calculated it as 2/8, it does not reflect the size of the other subgroups. In the first case, a subgroup with 2 members is likely to be a minor subgroups because of a major subgroup with 4 members. But in the latter case, a subgroup with 2 members is not either minor or major subgroup because other subgroups have 2 members. To reflect this concern, I try to adopt Herfindahl index. First of all, divide the number of members of subgroups by total members(=8). For example, in the group with 1,1,2,4 members, the value is 0.125, 0.125, 0.25, 0.5. Then, I calculate the square value (0.0156,0.0156,0.0625,0.25). The sum of it is Herfindahl index as you know. The important things, here, is that I divide the square value by sum of square value.  In the end, the 2-member subgroup in the group with 1,1,2,4 members have the value-0.181818. Contrary to it, the 2-member subgroup in the group with 2,2,2,2 members have the value-0.25 I think it can adequately reflect my thought because the 2 member subgroup in the former case (=0.1818) has relatively smaller size than in the latter group (=0.25). I can understand it intuitively but I cannot explain it logically. Maybe one reason might be I cannot find the reference. Calculating Herfindahl index is common, but dividing the fraction by the index is unusual to me. Does anyone know the similar situation? Does it make sense? If you know any reference or have any recommendation to my logic, please let me know. Thanks a lot!",,"['calculus', 'probability', 'statistics', 'probability-distributions']"
41,Gumbel distribution convergence,Gumbel distribution convergence,,"$X_1,X_2,\ldots$ IID, so that $X_i\sim E(\lambda)$. Therefore $\mathbf{P}(X_i>t)=\exp(-\lambda t)$ if $t\geqslant 0$. We denote $M_n=\max\{X_1,\ldots ,X_n\}$ and I want to prove that $$M_n-\frac{\ln n}\lambda \overset{D}\to Y,$$ where $Y\sim G_\lambda$, $G_\lambda (t)=\exp(-e^{-\lambda t}), \ t\in \mathbb{R}, \ \lambda >0$. I don't know how to deal with $M_n$. Advice would be appreciated.","$X_1,X_2,\ldots$ IID, so that $X_i\sim E(\lambda)$. Therefore $\mathbf{P}(X_i>t)=\exp(-\lambda t)$ if $t\geqslant 0$. We denote $M_n=\max\{X_1,\ldots ,X_n\}$ and I want to prove that $$M_n-\frac{\ln n}\lambda \overset{D}\to Y,$$ where $Y\sim G_\lambda$, $G_\lambda (t)=\exp(-e^{-\lambda t}), \ t\in \mathbb{R}, \ \lambda >0$. I don't know how to deal with $M_n$. Advice would be appreciated.",,"['probability', 'statistics']"
42,Showing truncated Poisson process has no unbiased estimator,Showing truncated Poisson process has no unbiased estimator,,"Suppose that $X$ has the Poisson distribution truncated on the right at $a$, so that it has the conditional distribution of $Y$ given $Y \leq a$, where $Y$ is distributed as $P(\lambda)$. Show that $\lambda$ does not have an unbiased estimator. I'm trying to prove this using a proof by contradiction. Suppose $\delta(X)$ is an unbiased estimator for $g(\lambda) = \lambda$. Then: $$\sum_{k=0}^{\infty} \delta(k)\frac{e^{-\lambda} \lambda^k}{k!} 1_{k \leq a} = \lambda $$ Rearranging I get: $$\sum_{k=0}^{\infty} \delta(k)\frac{\lambda^{k-1}}{k!} 1_{k \leq a} = e^{\lambda}  $$ Is this a contradiction because the finite sum will never equal $e^\lambda$? Or do I need another argument?","Suppose that $X$ has the Poisson distribution truncated on the right at $a$, so that it has the conditional distribution of $Y$ given $Y \leq a$, where $Y$ is distributed as $P(\lambda)$. Show that $\lambda$ does not have an unbiased estimator. I'm trying to prove this using a proof by contradiction. Suppose $\delta(X)$ is an unbiased estimator for $g(\lambda) = \lambda$. Then: $$\sum_{k=0}^{\infty} \delta(k)\frac{e^{-\lambda} \lambda^k}{k!} 1_{k \leq a} = \lambda $$ Rearranging I get: $$\sum_{k=0}^{\infty} \delta(k)\frac{\lambda^{k-1}}{k!} 1_{k \leq a} = e^{\lambda}  $$ Is this a contradiction because the finite sum will never equal $e^\lambda$? Or do I need another argument?",,"['statistics', 'poisson-distribution', 'parameter-estimation']"
43,Number of samples needed to distinguish between two Bernoulli distributions,Number of samples needed to distinguish between two Bernoulli distributions,,"In a paper I am reading, they make the following claim offhandedly (paraphrased): Suppose there are two Bernoulli distributions with means $p_0$ and $p_1$ where $\delta=p_1-p_0$. One distribution is chosen (by a fair coin flip) and then $n$ i.i.d. samples are drawn from it. The probability of error (incorrectly identifying the correct distribution based on the i.i.d. samples) is $\Omega(1)$ unless the number of samples is $n=\Omega(1/\delta^2)$. What is the intuition/reasoning behind this claim? I am also confused about the setup (this claim was stated rather hastily in the paper), such as whether the two means are known or just their difference $\delta$ is known, and also what statistic (sample mean?) and procedure (pick the distribution whose mean is closer to sample mean?) is being used to infer the distribution. Any references would also be appreciated.","In a paper I am reading, they make the following claim offhandedly (paraphrased): Suppose there are two Bernoulli distributions with means $p_0$ and $p_1$ where $\delta=p_1-p_0$. One distribution is chosen (by a fair coin flip) and then $n$ i.i.d. samples are drawn from it. The probability of error (incorrectly identifying the correct distribution based on the i.i.d. samples) is $\Omega(1)$ unless the number of samples is $n=\Omega(1/\delta^2)$. What is the intuition/reasoning behind this claim? I am also confused about the setup (this claim was stated rather hastily in the paper), such as whether the two means are known or just their difference $\delta$ is known, and also what statistic (sample mean?) and procedure (pick the distribution whose mean is closer to sample mean?) is being used to infer the distribution. Any references would also be appreciated.",,"['probability', 'statistics', 'hypothesis-testing']"
44,Understanding the Normal Distribution?,Understanding the Normal Distribution?,,"If a sample is normal with observations independent and identically distributed: $\mu|\sigma^2 \propto N(\beta \,,\,\sigma^2/\, n_0)$ How can I show that  $\mu\,|\,x_1,x_2,....x_n\,,\,\sigma^2 \sim N(\frac {n\bar{x} + n_o\beta}{ n + n_o} \, , \frac {\sigma^2}{n + n_o})$ ? I have been trying to figure this out for days.  Originally I assumed both the mean and $x_1,....x_n$ were normally distributed and the the variance as chi squared distributed but I have don't know how to incorporate all three in a manner to get a normal distribution.","If a sample is normal with observations independent and identically distributed: $\mu|\sigma^2 \propto N(\beta \,,\,\sigma^2/\, n_0)$ How can I show that  $\mu\,|\,x_1,x_2,....x_n\,,\,\sigma^2 \sim N(\frac {n\bar{x} + n_o\beta}{ n + n_o} \, , \frac {\sigma^2}{n + n_o})$ ? I have been trying to figure this out for days.  Originally I assumed both the mean and $x_1,....x_n$ were normally distributed and the the variance as chi squared distributed but I have don't know how to incorporate all three in a manner to get a normal distribution.",,"['probability-theory', 'statistics', 'random-variables', 'normal-distribution', 'standard-deviation']"
45,Proof that correlation coefficient squared equals the coefficient of determination,Proof that correlation coefficient squared equals the coefficient of determination,,"Hi I as the title says I'm looking at the proof that $r^2$ = $R^2$ in the case of simple linear regression, but I don't understand one part. There are different versions of the proof, but in most of them they do a step I don't understand. You may look at slide 9 here for instance. Particularly I don't understand how $(Y-\hat{Y})(\hat{Y}-\bar{Y}) = 0$. Could anyone please explain to me why that is? Thank you","Hi I as the title says I'm looking at the proof that $r^2$ = $R^2$ in the case of simple linear regression, but I don't understand one part. There are different versions of the proof, but in most of them they do a step I don't understand. You may look at slide 9 here for instance. Particularly I don't understand how $(Y-\hat{Y})(\hat{Y}-\bar{Y}) = 0$. Could anyone please explain to me why that is? Thank you",,"['statistics', 'regression', 'correlation']"
46,Finding the limiting distribution for a Binomial-related random variable,Finding the limiting distribution for a Binomial-related random variable,,"A foreword that this isn't homework, rather exam review without solutions. So the question asks that given $X_{n} \sim \text{Bin}(n,\theta)$, find the limiting distribution of $U_{n} = \frac{X_{n}}{n}\left(1-\frac{X_{n}}{n}\right)$. I figured out that I can rewrite $X_{n}$ as $\sum \limits_{k=1}^{n}Z_{k}$ where $Z_{k}$ is a sequence of i.i.d Bernouilli trials, and then $\lim \limits_{n \rightarrow \infty} \dfrac{\sum \limits_{k=1}^{n}Z_{k}}{n} = \theta$ by the Weak Law of Large Numbers, so I have $\frac{X_{n}}{n}$ figured out. But with this more complicated form, I'm not sure what to do.","A foreword that this isn't homework, rather exam review without solutions. So the question asks that given $X_{n} \sim \text{Bin}(n,\theta)$, find the limiting distribution of $U_{n} = \frac{X_{n}}{n}\left(1-\frac{X_{n}}{n}\right)$. I figured out that I can rewrite $X_{n}$ as $\sum \limits_{k=1}^{n}Z_{k}$ where $Z_{k}$ is a sequence of i.i.d Bernouilli trials, and then $\lim \limits_{n \rightarrow \infty} \dfrac{\sum \limits_{k=1}^{n}Z_{k}}{n} = \theta$ by the Weak Law of Large Numbers, so I have $\frac{X_{n}}{n}$ figured out. But with this more complicated form, I'm not sure what to do.",,"['limits', 'probability-theory', 'statistics', 'weak-convergence']"
47,Joint density of exponential and sum of exponentials / general RV,Joint density of exponential and sum of exponentials / general RV,,"I need to find the density $f_{x,z}$ where $x \sim exp(\lambda_1)$ and $z = x + y$, where $y \sim exp(\lambda_2)$. $x$ and $y$ are independent. Finding the density of $z$ is not too hard (and has also been answered on here several times). I am stumped by trying to find the joint density of $x$ and $z$ - they are clearly not independent. Add in complications such as that surely $f_{x, z} = 0$ if $z < x$, and I am thoroughly confused.","I need to find the density $f_{x,z}$ where $x \sim exp(\lambda_1)$ and $z = x + y$, where $y \sim exp(\lambda_2)$. $x$ and $y$ are independent. Finding the density of $z$ is not too hard (and has also been answered on here several times). I am stumped by trying to find the joint density of $x$ and $z$ - they are clearly not independent. Add in complications such as that surely $f_{x, z} = 0$ if $z < x$, and I am thoroughly confused.",,"['probability', 'statistics']"
48,Variance and covariance,Variance and covariance,,"I'm practicing for an exam and a mock question has me completely stumped. If someone could show me the steps I would be very grateful! There are two random variables, $A$ and $B$. $Var(A) = 9$, and $Var(B) = 4$, and $Cov(A, B) = 2$. a) What is $Var(2A - 3B + 10)$? b) $U = A + B$, and $V = A + aB$, where $a$ is decided so $Cov(U, V) = 0$. What are the possible values of $a$? c) $A$ and $B$ are simultaneously distributed with the Normal distribution, where $U$ and $V$ are the same as in the previous problem. What is $Cov(e^{sU}, e^{rV})$?","I'm practicing for an exam and a mock question has me completely stumped. If someone could show me the steps I would be very grateful! There are two random variables, $A$ and $B$. $Var(A) = 9$, and $Var(B) = 4$, and $Cov(A, B) = 2$. a) What is $Var(2A - 3B + 10)$? b) $U = A + B$, and $V = A + aB$, where $a$ is decided so $Cov(U, V) = 0$. What are the possible values of $a$? c) $A$ and $B$ are simultaneously distributed with the Normal distribution, where $U$ and $V$ are the same as in the previous problem. What is $Cov(e^{sU}, e^{rV})$?",,"['statistics', 'covariance']"
49,Ammunition Depot: Monte Carlo Method,Ammunition Depot: Monte Carlo Method,,"I was given the following question from a friend of mine and I can't seem to understand it to well: A squadron of 10 bombers attempts to destroy an ammunition depot. The fighter jet flies in the horizontal direction. The aiming point is the center of the depot. The point of impact is assumed to be a normally distributed around the aiming point $(60,0)$ with a standard deviation $\sigma$ of $200$ yards in the horizontal direction and $100$ yards in the vertical direction. Simulate the operation and estimate the number of bombs on target. The middle two corners of the following ammunition depot are $(-150,0)$ and $(270,0)$. $X$~Normal (mean $= 60, \sigma = 200$) and $Y$~Normal (mean $=0,\sigma = 100$). The picture above is a diagram of the ammunition depot. I don't have a strong background in statistics to fully understand how the normal distribution is being applied here. I am somewhat familiar with Monte Carlo Method, but I have only used it as an alternative to solving integrals numerically. My idea was to just define the region above using the following equations:  \begin{array} ` y_1 = 120 &  & -210\leq x \leq 210\\ y_2 = -120 &  & -210\leq x \leq 210\\ y_3 = 2x-540 &  & 210\leq x \leq 270\\ y_4 = -2x+540 &  & 210\leq x \leq 270\\ y_5 = 2x+300 &  & -210\leq x \leq -150\\ y_6 = -2x-300 &  & -210\leq x \leq -150\\ \end{array} Then randomly generate points that represent where the projectiles land. If the projectiles (coordinates in this case) landed within the boundary I specified, then I would mark it as a hit; otherwise, a miss. However, I do not know if this is sufficient since it does not incorporate the distribution (at least not that I know of). Thank you for your time and have a wonderful day.","I was given the following question from a friend of mine and I can't seem to understand it to well: A squadron of 10 bombers attempts to destroy an ammunition depot. The fighter jet flies in the horizontal direction. The aiming point is the center of the depot. The point of impact is assumed to be a normally distributed around the aiming point $(60,0)$ with a standard deviation $\sigma$ of $200$ yards in the horizontal direction and $100$ yards in the vertical direction. Simulate the operation and estimate the number of bombs on target. The middle two corners of the following ammunition depot are $(-150,0)$ and $(270,0)$. $X$~Normal (mean $= 60, \sigma = 200$) and $Y$~Normal (mean $=0,\sigma = 100$). The picture above is a diagram of the ammunition depot. I don't have a strong background in statistics to fully understand how the normal distribution is being applied here. I am somewhat familiar with Monte Carlo Method, but I have only used it as an alternative to solving integrals numerically. My idea was to just define the region above using the following equations:  \begin{array} ` y_1 = 120 &  & -210\leq x \leq 210\\ y_2 = -120 &  & -210\leq x \leq 210\\ y_3 = 2x-540 &  & 210\leq x \leq 270\\ y_4 = -2x+540 &  & 210\leq x \leq 270\\ y_5 = 2x+300 &  & -210\leq x \leq -150\\ y_6 = -2x-300 &  & -210\leq x \leq -150\\ \end{array} Then randomly generate points that represent where the projectiles land. If the projectiles (coordinates in this case) landed within the boundary I specified, then I would mark it as a hit; otherwise, a miss. However, I do not know if this is sufficient since it does not incorporate the distribution (at least not that I know of). Thank you for your time and have a wonderful day.",,"['statistics', 'normal-distribution', 'monte-carlo']"
50,P-value of a test of binomial success probability,P-value of a test of binomial success probability,,"How do I get the $p$-value for tossing a coin 100 times with the result 30 Tails and 70 Heads for the null hypotheses $H_0=0.5$? At best, a command in Matlab should be given to obtain this number, with a brief explanation of its derivation. Together with the result, i.e. whether we reject $H_0$. Moreover, what is the power (= the parameter $\beta$) of the test?","How do I get the $p$-value for tossing a coin 100 times with the result 30 Tails and 70 Heads for the null hypotheses $H_0=0.5$? At best, a command in Matlab should be given to obtain this number, with a brief explanation of its derivation. Together with the result, i.e. whether we reject $H_0$. Moreover, what is the power (= the parameter $\beta$) of the test?",,"['statistics', 'hypothesis-testing']"
51,Probability of guessing a list,Probability of guessing a list,,"The question is ""A history quiz has one question where the students are asked to arranged the first ten presidents in correct chronological order. If a student is totally unprepared and makes a random list, what is the probability of getting the incorrect order? The correct order?"" I tried to just do 10! (10 factorial) and got 3,628,800 So I figured probability of guessing correctly is 1/3,628,800 = 2.76 E^-7 And incorrectly is 3,628,799/3,628,800 = .9999997244 Did I do this correctly or is this wrong? Thank you!","The question is ""A history quiz has one question where the students are asked to arranged the first ten presidents in correct chronological order. If a student is totally unprepared and makes a random list, what is the probability of getting the incorrect order? The correct order?"" I tried to just do 10! (10 factorial) and got 3,628,800 So I figured probability of guessing correctly is 1/3,628,800 = 2.76 E^-7 And incorrectly is 3,628,799/3,628,800 = .9999997244 Did I do this correctly or is this wrong? Thank you!",,"['probability', 'statistics']"
52,Variance estimation for uncorrelated variables,Variance estimation for uncorrelated variables,,"Consider a sequence of uncorrelated variables $(X_n)$. If these variables are bounded in $L^2$ and have common mean $\mu$, a well-known variant of the law of large numbers states that $$   \frac{1}{n}\sum_{i=1}^n X_i \stackrel{\mathrm{a.s.}}\longrightarrow \mu. $$ This allows for estimation of parameters when observations are not i.i.d., a situation occurring for example for non-uniformly spaced observations of continuous-time processes. My question is about the variance analogue of the above. Again, assume that $(X_n)$ is a sequence of uncorrelated variables. Assume that the variables have common variance $\sigma^2$. We do not assume that the variables have common mean. Is there an asymptotic result similar to the above yielding $\sigma^2$ as an almost sure limit? I would imagine something like $$   \frac{1}{n}\sum_{i=1}^n (X_i - \bar{X}_n)^2 \stackrel{\mathrm{a.s.}}\longrightarrow \sigma^2, $$ where $\bar{X}_n$ is the average of $X_1,\ldots,X_n$, but this result is not obvious to me. Assume boundedness in $L^p$ for any $p\ge 1$ necessary. For completeness, here is an explicit motivation for the above example. Assume that $S_t = \exp(X_t)$ for some Levy process $X$ and $t\ge0$. Here, $S$ denotes an asset price, and this is then an instance of the exponential Levy market model. In this model, continuous returns of period $\Delta$ are given by $\log S_{t+\Delta} / S_t$ and have mean $\Delta \alpha$ and variance $\Delta \beta$ for some $\alpha$ and $\beta$. Making observations $S_{t_1},\ldots,S_{t_n}$, I'm interested in estimating $\beta$. Here, it holds that the variables $$   \frac{1}{\sqrt{t_i-t_{i-1}}}\log\frac{S_{t_i}}{S_{t_{i-1}}} $$ are uncorrelated (in fact, independent) and have the same variance $\beta$. If something like the above almost sure result for convergence to the variance holds, I would then be able to use the above transformed variables to estimate $\beta$.","Consider a sequence of uncorrelated variables $(X_n)$. If these variables are bounded in $L^2$ and have common mean $\mu$, a well-known variant of the law of large numbers states that $$   \frac{1}{n}\sum_{i=1}^n X_i \stackrel{\mathrm{a.s.}}\longrightarrow \mu. $$ This allows for estimation of parameters when observations are not i.i.d., a situation occurring for example for non-uniformly spaced observations of continuous-time processes. My question is about the variance analogue of the above. Again, assume that $(X_n)$ is a sequence of uncorrelated variables. Assume that the variables have common variance $\sigma^2$. We do not assume that the variables have common mean. Is there an asymptotic result similar to the above yielding $\sigma^2$ as an almost sure limit? I would imagine something like $$   \frac{1}{n}\sum_{i=1}^n (X_i - \bar{X}_n)^2 \stackrel{\mathrm{a.s.}}\longrightarrow \sigma^2, $$ where $\bar{X}_n$ is the average of $X_1,\ldots,X_n$, but this result is not obvious to me. Assume boundedness in $L^p$ for any $p\ge 1$ necessary. For completeness, here is an explicit motivation for the above example. Assume that $S_t = \exp(X_t)$ for some Levy process $X$ and $t\ge0$. Here, $S$ denotes an asset price, and this is then an instance of the exponential Levy market model. In this model, continuous returns of period $\Delta$ are given by $\log S_{t+\Delta} / S_t$ and have mean $\Delta \alpha$ and variance $\Delta \beta$ for some $\alpha$ and $\beta$. Making observations $S_{t_1},\ldots,S_{t_n}$, I'm interested in estimating $\beta$. Here, it holds that the variables $$   \frac{1}{\sqrt{t_i-t_{i-1}}}\log\frac{S_{t_i}}{S_{t_{i-1}}} $$ are uncorrelated (in fact, independent) and have the same variance $\beta$. If something like the above almost sure result for convergence to the variance holds, I would then be able to use the above transformed variables to estimate $\beta$.",,"['probability-theory', 'statistics', 'stochastic-processes', 'parameter-estimation', 'almost-everywhere']"
53,Find the test statistic,Find the test statistic,,"A large orthodontist practice is investigating a new supplier for a dental bonding agent. One important   consideration is the breaking strength of the agent. It is well known from a very large amount of past   experience that the agent currently in use has a mean breaking strength of 6 Mpa. The new supplier is offering a new type of agent, one that would result in very large savings. One factor the orthodontist   wants to investigate is the breaking strength of the new agent. He uses the agent to affix a dental   appliance on each of 12 extracted teeth, and then measures the breaking strength. He finds that the   breaking strength has a sample mean of 4.8 Mpa with a sample standard deviation of 1.1 Mpa.   For such a small sample size (n = 12), it is a bit dubious to use the t procedures, as we cannot rely on   the Central Limit Theorem. But let's assume a normally distributed population here. Let's also assume that these observations can be thought of as a random sample from the population of interest. That's also a bit dubious, but let's go with it. Test the null hypothesis that the population mean breaking strength of the new agent is equal to 6.0   Mpa, against a two-sided alternative hypothesis. What is the value of the appropriate test statistic? I am confused. I know that the population mean is 6 and the standard deviation is 1.1 and the sample size is 12 but what is the sample mean.  So I used the formula of (sample mean - population mean/ standard deviation/square root of sample size) Can someone please help me out?","A large orthodontist practice is investigating a new supplier for a dental bonding agent. One important   consideration is the breaking strength of the agent. It is well known from a very large amount of past   experience that the agent currently in use has a mean breaking strength of 6 Mpa. The new supplier is offering a new type of agent, one that would result in very large savings. One factor the orthodontist   wants to investigate is the breaking strength of the new agent. He uses the agent to affix a dental   appliance on each of 12 extracted teeth, and then measures the breaking strength. He finds that the   breaking strength has a sample mean of 4.8 Mpa with a sample standard deviation of 1.1 Mpa.   For such a small sample size (n = 12), it is a bit dubious to use the t procedures, as we cannot rely on   the Central Limit Theorem. But let's assume a normally distributed population here. Let's also assume that these observations can be thought of as a random sample from the population of interest. That's also a bit dubious, but let's go with it. Test the null hypothesis that the population mean breaking strength of the new agent is equal to 6.0   Mpa, against a two-sided alternative hypothesis. What is the value of the appropriate test statistic? I am confused. I know that the population mean is 6 and the standard deviation is 1.1 and the sample size is 12 but what is the sample mean.  So I used the formula of (sample mean - population mean/ standard deviation/square root of sample size) Can someone please help me out?",,"['probability', 'statistics']"
54,Latent Dirichlet Allocation Derivation,Latent Dirichlet Allocation Derivation,,"I am exploring different derivations for the the LDA and was a bit surprised about a step I found in the following paper : https://cxwangyi.files.wordpress.com/2012/01/llt.pdf My question is about the transition between step 2.19 -> 2.20 which goes as follow : $$ p(W|Z,\beta) = \prod_{k=1}^{K} \left ( \frac{1}{B(\beta )} \int \prod_{v=1}^{V} \phi_{k,v}^{\psi_{k,v} + \beta_{v}-1} d\phi_k \right ) $$ to $$ p(W|Z,\beta) = \prod_{k=1}^{K} \frac{B(\psi_k + \beta)}{B(\beta )} $$ The precise question is : If we integrate out the right part of the equation it should sum to 1 and shouldn't be left with the Beta numerator... Or should we ? Thx in advance for any details. Keep.","I am exploring different derivations for the the LDA and was a bit surprised about a step I found in the following paper : https://cxwangyi.files.wordpress.com/2012/01/llt.pdf My question is about the transition between step 2.19 -> 2.20 which goes as follow : $$ p(W|Z,\beta) = \prod_{k=1}^{K} \left ( \frac{1}{B(\beta )} \int \prod_{v=1}^{V} \phi_{k,v}^{\psi_{k,v} + \beta_{v}-1} d\phi_k \right ) $$ to $$ p(W|Z,\beta) = \prod_{k=1}^{K} \frac{B(\psi_k + \beta)}{B(\beta )} $$ The precise question is : If we integrate out the right part of the equation it should sum to 1 and shouldn't be left with the Beta numerator... Or should we ? Thx in advance for any details. Keep.",,"['calculus', 'probability', 'integration', 'statistics']"
55,Joint exponential distribution seems ok until I take derivitive,Joint exponential distribution seems ok until I take derivitive,,"In studying joint distribution and density functions I have become confused by this exponential distrobution: $$F_{X,Y}(x,y)=(1-e^{-(x+y)})u(x)u(y)$$ It seems this should be a proper distribution as it is: monotonically increasing $F_{X,Y}(0,0)=F_{X,Y}(-\infty,y)=F_{X,Y}(x,-\infty)=F_{X,Y}(-\infty,-\infty)=0$ $F_{X,Y}(\infty,\infty)=1$ $0\le F_{X,Y}(x,y)\le1$ However if I take the partial derivatives with respect to X and Y to derive the density I get a function that is not a valid density: $$\frac{\partial}{\partial x \partial y} F_{X,Y}(x,y)=\delta(x) \delta(y)-e^{-(x+y)}u(x)u(y)+e^{-(x+y)}u(x)\delta(y)+e^{-(x+y)}\delta(x)u(y)-e^{-(x+y)}\delta(x)\delta(y)$$ Which basically reduces to $-e^{-(x+y)}u(x)u(y)$ for non-zero values of X and Y.  But this is not a valid density function.  Can anyone offer insight on how I am going astray?","In studying joint distribution and density functions I have become confused by this exponential distrobution: $$F_{X,Y}(x,y)=(1-e^{-(x+y)})u(x)u(y)$$ It seems this should be a proper distribution as it is: monotonically increasing $F_{X,Y}(0,0)=F_{X,Y}(-\infty,y)=F_{X,Y}(x,-\infty)=F_{X,Y}(-\infty,-\infty)=0$ $F_{X,Y}(\infty,\infty)=1$ $0\le F_{X,Y}(x,y)\le1$ However if I take the partial derivatives with respect to X and Y to derive the density I get a function that is not a valid density: $$\frac{\partial}{\partial x \partial y} F_{X,Y}(x,y)=\delta(x) \delta(y)-e^{-(x+y)}u(x)u(y)+e^{-(x+y)}u(x)\delta(y)+e^{-(x+y)}\delta(x)u(y)-e^{-(x+y)}\delta(x)\delta(y)$$ Which basically reduces to $-e^{-(x+y)}u(x)u(y)$ for non-zero values of X and Y.  But this is not a valid density function.  Can anyone offer insight on how I am going astray?",,"['probability', 'statistics', 'probability-distributions', 'density-function']"
56,How can you show that $E(Y\mid E(Y\mid X)) = E(Y\mid X)$?,How can you show that ?,E(Y\mid E(Y\mid X)) = E(Y\mid X),"I am trying to show that  $E(Y\mid E(Y\mid X)) = E(Y\mid X)$. However, I cannot see how to do this without integrals.","I am trying to show that  $E(Y\mid E(Y\mid X)) = E(Y\mid X)$. However, I cannot see how to do this without integrals.",,"['probability', 'statistics']"
57,Ratio estimator in sampling,Ratio estimator in sampling,,"Let the population $U=(1,2,3)$ . We want to estimate $R=\frac{\mu_y}{\mu_x}$ .Consider the estimators $$\hat{R_1}=\frac{\overline{y}}{\overline{x}},\hat{R_2}=\frac{\overline{y}}{\mu_x}$$ where $Y=(9,42,53)$ and $X=(1,4,5)$ . Find the distributions of the estimators and their bias. Consider the sample size $n=2$ Considering a simple random sample without replacement, we have that $S=[(1,2),(1,3),(2,3)]$ $$\begin{bmatrix}s:&12&13&23\\\hat{R_1}: &10.2&10.33&10.55\\ p:&1/3&1/3&1/3 \end{bmatrix}$$ Just to ilustrate, taking $s=(1,2)$ I did $$\frac{\overline{y}}{\overline{x}}=\frac{(\frac{9+42}{2})}{(\frac{1+4}{2})}=10.2$$ but solutions say that the distribution is $$\begin{bmatrix}s:&12&13&23\\\hat{R_1}: &0.098&0.097&0.095\\ p:&1/3&1/3&1/3 \end{bmatrix}$$ It is as if they had done $$\hat{R_1}=\frac{\overline{X}}{\overline{Y}}$$ Could anyone help me?","Let the population . We want to estimate .Consider the estimators where and . Find the distributions of the estimators and their bias. Consider the sample size Considering a simple random sample without replacement, we have that Just to ilustrate, taking I did but solutions say that the distribution is It is as if they had done Could anyone help me?","U=(1,2,3) R=\frac{\mu_y}{\mu_x} \hat{R_1}=\frac{\overline{y}}{\overline{x}},\hat{R_2}=\frac{\overline{y}}{\mu_x} Y=(9,42,53) X=(1,4,5) n=2 S=[(1,2),(1,3),(2,3)] \begin{bmatrix}s:&12&13&23\\\hat{R_1}: &10.2&10.33&10.55\\ p:&1/3&1/3&1/3 \end{bmatrix} s=(1,2) \frac{\overline{y}}{\overline{x}}=\frac{(\frac{9+42}{2})}{(\frac{1+4}{2})}=10.2 \begin{bmatrix}s:&12&13&23\\\hat{R_1}: &0.098&0.097&0.095\\ p:&1/3&1/3&1/3 \end{bmatrix} \hat{R_1}=\frac{\overline{X}}{\overline{Y}}","['statistics', 'estimation', 'sampling']"
58,Finding the p.d.f. of T=XY using the C.D.F. Technique (Functions of Random Variable),Finding the p.d.f. of T=XY using the C.D.F. Technique (Functions of Random Variable),,"Suppose X and Y are continuous r.v. with join p.d.f. $$f(x,y)=3y, 0 \le x \le y \le 1$$ Find the p.d.f. of $T=xy$ So I know that we can consider for $ t < 0, P(T \le t)=0$ and if $t \ge 1, P(T \le t)=1$ Therefore we consider $t \in (0,1)$. But I don't know how to set up the limits of integration. Please help!","Suppose X and Y are continuous r.v. with join p.d.f. $$f(x,y)=3y, 0 \le x \le y \le 1$$ Find the p.d.f. of $T=xy$ So I know that we can consider for $ t < 0, P(T \le t)=0$ and if $t \ge 1, P(T \le t)=1$ Therefore we consider $t \in (0,1)$. But I don't know how to set up the limits of integration. Please help!",,['statistics']
59,"Singular covariance matrix, understanding the beginning of a proof","Singular covariance matrix, understanding the beginning of a proof",,"Let $\left\{X_t,t\in T\right\}$ be a stationary process such that $\text{Var}(X_t)<\infty$ for each $t\in T$. The autocovariance function $\gamma_X(\cdot)=\gamma(\cdot)$ of $\left\{X_t\right\}$ is defined to be $$ \gamma(h)=\text{Cov}(X_{h+t},X_t)~\forall h,t\in\mathbb{Z}. $$ Moreover, assume that $EX_t=0$ for each $t\in T$. There is the following statement: If $\gamma(0)>0$ and $\gamma(h)\to 0$ as $h\to\infty$, then the covariance matrix $\Gamma_n=[\gamma(i-j)]_{i,j=1,\ldots,n}$ of the column vector $(X_1,\ldots,X_n)'$ is non-singular for every $n$. The proof of this starts as follows: Suppose that $\Gamma_n$ is singular for some $n$. Then since $EX_t=0$ there exists an integer $r\geq 1$ and real constants $a_1,\ldots,a_r$ such that $\Gamma_r$ is non-singular and $$ X_{r+1}=\sum_{j=1}^r a_j X_j.~~~~~(*) $$ I do not completely see this argument. In particular, there are two inaccuricies to my opinion. Here's how I do understand it: Suppose $\Gamma_n$ is singular. This means, its determinant is zero. So there is at least one column (one row) that is a linear combinations of the other columns (rows). If we delete this column (row), we get $\Gamma_{n-1}$, if its determinant is again zero, we repeat this. Eventually, we get $\Gamma_r$, for some $r\geq 1$, such that all columns (rows) are independent, meaning that $\Gamma_r$ has positive determinant, i.e. is non-singular. So far so good. It remains to argue, how to get $(*)$. In the proof, there is said to look at the following statement that one probably should use for my problem: If $X=(X_1,\ldots,X_n)'$ is a random vector with covariance matrix $\Sigma$, then $\Sigma$ is singular if and only if there exists a non-zero vector $b=(b_1,\ldots,b_n)'\in\mathbb{R}^n$ such that $\text{Var}(b'X)=0$. So, I apply this as follows: As done above, let $\Gamma_r$ be non-singular. Since $r$ was the smallest $r\geq 1$ such that all columns are linear independent, the covariance matrix $\Gamma_{r+1}$ of $X=(X_1,X_2,\ldots,X_{r+1})'$ is singular. By the cited statement, there is some $b=(b_1,b_2,\ldots,b_{r+1})'\in\mathbb{R}^{r+1}$ such that $$ \text{Var}(b'X)=0. $$ But this means that $$ b'X=\sum_{i=1}^{r+1}b_iX_i=E(b'X)=0\text{ almost surely }, $$ implying that $$ b_{r+1}X_{r+1}=-\sum_{i=1}^r b_iX_i. $$ Imho, we can suppose that $b_{r+1}\neq 0$ since if $b_{r+1}=0$, we have that $0=\text{Var}(b_1X_1+b_2X_2+\ldots + b_rX_r+b_{r+1}X_{r+1})=\text{Var}(b_1X_1+b_2X_2+\ldots + b_rX_r)$, meaning by the cited statement that $\Gamma_r$ is singular. But $\Gamma_r$ is non-singular. Hence $$ X_{r+1}=\sum_{i=1}^r a_iX_i,~~~a_i:=-\frac{b_i}{b_{r+1}} $$ or, more precisely, this identity holds almost surely. I don't know why the ""almost surely"" is omitted in the proof. Is this okay?","Let $\left\{X_t,t\in T\right\}$ be a stationary process such that $\text{Var}(X_t)<\infty$ for each $t\in T$. The autocovariance function $\gamma_X(\cdot)=\gamma(\cdot)$ of $\left\{X_t\right\}$ is defined to be $$ \gamma(h)=\text{Cov}(X_{h+t},X_t)~\forall h,t\in\mathbb{Z}. $$ Moreover, assume that $EX_t=0$ for each $t\in T$. There is the following statement: If $\gamma(0)>0$ and $\gamma(h)\to 0$ as $h\to\infty$, then the covariance matrix $\Gamma_n=[\gamma(i-j)]_{i,j=1,\ldots,n}$ of the column vector $(X_1,\ldots,X_n)'$ is non-singular for every $n$. The proof of this starts as follows: Suppose that $\Gamma_n$ is singular for some $n$. Then since $EX_t=0$ there exists an integer $r\geq 1$ and real constants $a_1,\ldots,a_r$ such that $\Gamma_r$ is non-singular and $$ X_{r+1}=\sum_{j=1}^r a_j X_j.~~~~~(*) $$ I do not completely see this argument. In particular, there are two inaccuricies to my opinion. Here's how I do understand it: Suppose $\Gamma_n$ is singular. This means, its determinant is zero. So there is at least one column (one row) that is a linear combinations of the other columns (rows). If we delete this column (row), we get $\Gamma_{n-1}$, if its determinant is again zero, we repeat this. Eventually, we get $\Gamma_r$, for some $r\geq 1$, such that all columns (rows) are independent, meaning that $\Gamma_r$ has positive determinant, i.e. is non-singular. So far so good. It remains to argue, how to get $(*)$. In the proof, there is said to look at the following statement that one probably should use for my problem: If $X=(X_1,\ldots,X_n)'$ is a random vector with covariance matrix $\Sigma$, then $\Sigma$ is singular if and only if there exists a non-zero vector $b=(b_1,\ldots,b_n)'\in\mathbb{R}^n$ such that $\text{Var}(b'X)=0$. So, I apply this as follows: As done above, let $\Gamma_r$ be non-singular. Since $r$ was the smallest $r\geq 1$ such that all columns are linear independent, the covariance matrix $\Gamma_{r+1}$ of $X=(X_1,X_2,\ldots,X_{r+1})'$ is singular. By the cited statement, there is some $b=(b_1,b_2,\ldots,b_{r+1})'\in\mathbb{R}^{r+1}$ such that $$ \text{Var}(b'X)=0. $$ But this means that $$ b'X=\sum_{i=1}^{r+1}b_iX_i=E(b'X)=0\text{ almost surely }, $$ implying that $$ b_{r+1}X_{r+1}=-\sum_{i=1}^r b_iX_i. $$ Imho, we can suppose that $b_{r+1}\neq 0$ since if $b_{r+1}=0$, we have that $0=\text{Var}(b_1X_1+b_2X_2+\ldots + b_rX_r+b_{r+1}X_{r+1})=\text{Var}(b_1X_1+b_2X_2+\ldots + b_rX_r)$, meaning by the cited statement that $\Gamma_r$ is singular. But $\Gamma_r$ is non-singular. Hence $$ X_{r+1}=\sum_{i=1}^r a_iX_i,~~~a_i:=-\frac{b_i}{b_{r+1}} $$ or, more precisely, this identity holds almost surely. I don't know why the ""almost surely"" is omitted in the proof. Is this okay?",,"['probability-theory', 'statistics', 'stochastic-processes', 'covariance', 'time-series']"
60,Finding the moment generating function for an absolute normal distribution,Finding the moment generating function for an absolute normal distribution,,"Another exam review question. Suppose I know $Z \sim \text{N}(0,1)$, $Y=|Z|$. And I want to find the moment generating function $M_{Y}(t)$. In our notes it's just given to us as $2 N(t) e^{\frac{t^{2}}{2}}$ where $N(t)$ is the c.d.f. of a $N(0,1)$ random variable, but I felt like for practicing mgfs it could be good to derive it. That being said, I don't even know where to start. I feel like starting with the normal p.d.f., but from there I don't know where to simplify, if that's the right approach. Maybe MacLaurin series?","Another exam review question. Suppose I know $Z \sim \text{N}(0,1)$, $Y=|Z|$. And I want to find the moment generating function $M_{Y}(t)$. In our notes it's just given to us as $2 N(t) e^{\frac{t^{2}}{2}}$ where $N(t)$ is the c.d.f. of a $N(0,1)$ random variable, but I felt like for practicing mgfs it could be good to derive it. That being said, I don't even know where to start. I feel like starting with the normal p.d.f., but from there I don't know where to simplify, if that's the right approach. Maybe MacLaurin series?",,"['statistics', 'moment-generating-functions']"
61,Are relative uncertainties additive?,Are relative uncertainties additive?,,"I'm given a quantity which is defined as $$a=\frac{bcd}{ef}$$ and I know the relative uncertainties in each of $b,c,d,e,f$. I'm supposed to find relative uncertainty in $a$. As far as I have learnt, we have $$\%\frac{\Delta a}{a}=\%\frac{\Delta b}{b}+\%\frac{\Delta c}{c}+\%\frac{\Delta d}{d}+\%\frac{\Delta e}{e}+\%\frac{\Delta f}{f}$$ The $\%$ sign indicates that the relative uncertainties are in percentage. However, is this actually correct, that is, are relative uncertainties really additive? I couldn't find a good reference which explicitly states so.","I'm given a quantity which is defined as $$a=\frac{bcd}{ef}$$ and I know the relative uncertainties in each of $b,c,d,e,f$. I'm supposed to find relative uncertainty in $a$. As far as I have learnt, we have $$\%\frac{\Delta a}{a}=\%\frac{\Delta b}{b}+\%\frac{\Delta c}{c}+\%\frac{\Delta d}{d}+\%\frac{\Delta e}{e}+\%\frac{\Delta f}{f}$$ The $\%$ sign indicates that the relative uncertainties are in percentage. However, is this actually correct, that is, are relative uncertainties really additive? I couldn't find a good reference which explicitly states so.",,"['algebra-precalculus', 'statistics', 'percentages']"
62,The distribution of fourier coefficients of a Rademacher sequence,The distribution of fourier coefficients of a Rademacher sequence,,"Assume that $x$ is a Rademacher vector or sequence with $x_i\in \{-1,+1\},i=1,2,...,N$ and $\Pr(x_i=+1)=\Pr(x_i=-1)=0.5$. What's the distribution of $\text{fft}(x)$? Is that a gaussian distribution? What's the mean and variance of it?","Assume that $x$ is a Rademacher vector or sequence with $x_i\in \{-1,+1\},i=1,2,...,N$ and $\Pr(x_i=+1)=\Pr(x_i=-1)=0.5$. What's the distribution of $\text{fft}(x)$? Is that a gaussian distribution? What's the mean and variance of it?",,"['statistics', 'fourier-analysis']"
63,Evaluate a game skill benefit,Evaluate a game skill benefit,,"I ask a same problem yesterday, but my post has so many grammar error. I am very sorry about that! Sorry for my poor English. So now I try to use more simple sentences for me to describe this problem again: A player can fire magic ball, which has 4 second CD time Magic ball will hit to an enemy, the hit rate depend on player skill A magic ball has 15% chance to make a buff to the enemy, this buff will cause the enemy loses his 0.1% MAX-HP per second An enemy can be made only one buff, and buff exist time, 10 second~40 second,  is depend on enemy's defend value An enemy has a special skill, called ""RB"", can remove this buff immediately, this special skill has 60 second CD time My question is that Now I have a skill, called ""MA"", which can increase 5% chance of buff making, it means that 15% => 20%. How about the expected damage of skill ""MA""? ================================================================== I have some preliminary idea and I will try to describe them in follow: I think this problem can make a math form - (Expected Damage) = (Magic ball hit number) (Magic ball hit rate) (Buff chance) (0.1% Enemy Max HP) (Buff exist time) Where (Magic ball hit number) is a random variable depend on fire CD time, maybe a Poisson distribution (Magic ball hit rate) is a random variable depend on player skill, maybe a logarithmic distribution, according to ELO evaluation. (0.1% Enemy Max HP) is a random variable depend on average enemy's MAX HP (Buff exist time) is a random variable depend on average enemy's defend value, a continuous random variable I think this is Multiple-Random-Variable-Expected-Value problem, but I do not have any idea to evaluate it. Please give me some tips or research direction. Sorry for my poor English, and thank you very much! PS: I do not know how to delete the old article, please help me to delete it, thanks a lot.","I ask a same problem yesterday, but my post has so many grammar error. I am very sorry about that! Sorry for my poor English. So now I try to use more simple sentences for me to describe this problem again: A player can fire magic ball, which has 4 second CD time Magic ball will hit to an enemy, the hit rate depend on player skill A magic ball has 15% chance to make a buff to the enemy, this buff will cause the enemy loses his 0.1% MAX-HP per second An enemy can be made only one buff, and buff exist time, 10 second~40 second,  is depend on enemy's defend value An enemy has a special skill, called ""RB"", can remove this buff immediately, this special skill has 60 second CD time My question is that Now I have a skill, called ""MA"", which can increase 5% chance of buff making, it means that 15% => 20%. How about the expected damage of skill ""MA""? ================================================================== I have some preliminary idea and I will try to describe them in follow: I think this problem can make a math form - (Expected Damage) = (Magic ball hit number) (Magic ball hit rate) (Buff chance) (0.1% Enemy Max HP) (Buff exist time) Where (Magic ball hit number) is a random variable depend on fire CD time, maybe a Poisson distribution (Magic ball hit rate) is a random variable depend on player skill, maybe a logarithmic distribution, according to ELO evaluation. (0.1% Enemy Max HP) is a random variable depend on average enemy's MAX HP (Buff exist time) is a random variable depend on average enemy's defend value, a continuous random variable I think this is Multiple-Random-Variable-Expected-Value problem, but I do not have any idea to evaluate it. Please give me some tips or research direction. Sorry for my poor English, and thank you very much! PS: I do not know how to delete the old article, please help me to delete it, thanks a lot.",,"['probability', 'statistics', 'probability-distributions', 'random-variables']"
64,Different notation when using integration by parts?,Different notation when using integration by parts?,,"I am trying to evaluate three integrals and I found this solution, but it uses some notation I'm unfamiliar with. The first result I solved a little differently using integration by parts once and then you need to figure out $\int_0^{\infty}e^{-x^2 / \beta ^2}$. But I don't really understand the second two integrals (part b, different just by power of x) especially since I find the notation very confusing. In the second step, how he moves the dx inside and it turns into d, I have never seen that, could someone explain maybe with more intermediate steps? Also, in the next step, I don't understand what that $dx^2$ is. I can see this is just repeated integration by parts, but I cannot seem to figure this out. Thanks a lot! Apparently, it won't let me post images, so here is the link: https://i.sstatic.net/h8yyU.png","I am trying to evaluate three integrals and I found this solution, but it uses some notation I'm unfamiliar with. The first result I solved a little differently using integration by parts once and then you need to figure out $\int_0^{\infty}e^{-x^2 / \beta ^2}$. But I don't really understand the second two integrals (part b, different just by power of x) especially since I find the notation very confusing. In the second step, how he moves the dx inside and it turns into d, I have never seen that, could someone explain maybe with more intermediate steps? Also, in the next step, I don't understand what that $dx^2$ is. I can see this is just repeated integration by parts, but I cannot seem to figure this out. Thanks a lot! Apparently, it won't let me post images, so here is the link: https://i.sstatic.net/h8yyU.png",,"['calculus', 'statistics']"
65,Probability Distribution of Infinity Norm of a Vector of Independent Random Variables,Probability Distribution of Infinity Norm of a Vector of Independent Random Variables,,"Assuming $V\in \mathbb{R}^k$ to be a vector of independent random variables each with $\sim\mathcal{N}(0,1)$ (Or even more general $\sim\mathcal{N}(a,b)$). I was wondering how I can calculate the distribution (so both $\mathbb{E}$ and $\sigma$) of infinity norm of my vector; i.e. $\|V\|_\infty=\max_{i\in \{1,2, \ldots, k\}} |V_i|$ ?","Assuming $V\in \mathbb{R}^k$ to be a vector of independent random variables each with $\sim\mathcal{N}(0,1)$ (Or even more general $\sim\mathcal{N}(a,b)$). I was wondering how I can calculate the distribution (so both $\mathbb{E}$ and $\sigma$) of infinity norm of my vector; i.e. $\|V\|_\infty=\max_{i\in \{1,2, \ldots, k\}} |V_i|$ ?",,"['probability', 'analysis', 'statistics', 'vector-spaces']"
66,Help with understanding Rating Systems,Help with understanding Rating Systems,,"Hey guys so I'm trying to come with a rating system from 1-10 that will be determined based on how close a user is to an average value that I have. I don't have any data(other than the average value) to work with, so standard deviation is not possible. So lets say I want to give a number to a household based off of how many children they have(assume average is 2), if a house has higher than two its value will be a bit higher than 5. Is there any documentation/articles/books I could read to get a better grasp of where to look? Sorry for being a bit vague, but I don't know where the right places to look are in this matter, hope you guys can help!","Hey guys so I'm trying to come with a rating system from 1-10 that will be determined based on how close a user is to an average value that I have. I don't have any data(other than the average value) to work with, so standard deviation is not possible. So lets say I want to give a number to a household based off of how many children they have(assume average is 2), if a house has higher than two its value will be a bit higher than 5. Is there any documentation/articles/books I could read to get a better grasp of where to look? Sorry for being a bit vague, but I don't know where the right places to look are in this matter, hope you guys can help!",,"['statistics', 'recreational-mathematics']"
67,poisson distribution,poisson distribution,,An ice-cream vendor’s sales follow a Poisson distribution with an average rate of 10 per hour. How much should his stock of ice-cream be at any point in time if he wants to be at least 95% sure that he does not run out of ice-cream in the following hour?,An ice-cream vendor’s sales follow a Poisson distribution with an average rate of 10 per hour. How much should his stock of ice-cream be at any point in time if he wants to be at least 95% sure that he does not run out of ice-cream in the following hour?,,"['statistics', 'poisson-distribution']"
68,Probability of less than 10 % from the sampled are truck owners,Probability of less than 10 % from the sampled are truck owners,,"A simple random sample n = 25 is being drawn from a population from 320 members, exactly 30% of whom own a truck. Provide answers to the following to three decimal places. What is the probability of less than 10 % from the sampled are truck owners? My thoughts: Should I use the given condition to find the probability ? Or shouldd I use the combinatoric approach?","A simple random sample n = 25 is being drawn from a population from 320 members, exactly 30% of whom own a truck. Provide answers to the following to three decimal places. What is the probability of less than 10 % from the sampled are truck owners? My thoughts: Should I use the given condition to find the probability ? Or shouldd I use the combinatoric approach?",,"['probability', 'statistics']"
69,"Basic statistics: outliers, mean, median","Basic statistics: outliers, mean, median",,"For a sample of data, I got that the mean = 4.31 and the median = 4.42. Also, 36% of the values from the sample are less than the mean. Does this illustrate that the data is skewed left?","For a sample of data, I got that the mean = 4.31 and the median = 4.42. Also, 36% of the values from the sample are less than the mean. Does this illustrate that the data is skewed left?",,['statistics']
70,What is the conditional distribution of a variable with a Dirichlet distribution? [closed],What is the conditional distribution of a variable with a Dirichlet distribution? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question More specifically: if $X$ is a $k-$dimensional vector with a Dirichlet distribution, what is the probability that $X_i>a$, given $X_j>b$, where $a$ and $b$ are both in $(0,1)$ and $i\neq j$? Thanks!","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question More specifically: if $X$ is a $k-$dimensional vector with a Dirichlet distribution, what is the probability that $X_i>a$, given $X_j>b$, where $a$ and $b$ are both in $(0,1)$ and $i\neq j$? Thanks!",,"['probability', 'statistics']"
71,What is the probability that I will be dealt 2 or more diamonds when 5 cards are dealt?,What is the probability that I will be dealt 2 or more diamonds when 5 cards are dealt?,,"So assume we have a standard 52 cards deck that is shuffled fairly. If I am dealt 5 cards what is the probability that I will have 2 or more diamonds? My initial thought was to add the probability of getting exactly 2, 3, 4, and 5 diamonds. So I did that as follows: C(4,1) * C(13, 5) + C(4,1) * C(13, 4) + C(4,1) * C(13, 3) + C(4,1) * C(13, 2) But I know that this can't be right because I feel like I am duplicating possibilities.  Isn't C(4,1) * C(13, 4) also including the possibility of getting all 5 diamonds? Any help would be appreciated.","So assume we have a standard 52 cards deck that is shuffled fairly. If I am dealt 5 cards what is the probability that I will have 2 or more diamonds? My initial thought was to add the probability of getting exactly 2, 3, 4, and 5 diamonds. So I did that as follows: C(4,1) * C(13, 5) + C(4,1) * C(13, 4) + C(4,1) * C(13, 3) + C(4,1) * C(13, 2) But I know that this can't be right because I feel like I am duplicating possibilities.  Isn't C(4,1) * C(13, 4) also including the possibility of getting all 5 diamonds? Any help would be appreciated.",,"['probability', 'statistics']"
72,Hearthstone Pack Expected Dust Value,Hearthstone Pack Expected Dust Value,,"In the video game Hearthstone, you acquire card by opening packs. Each pack contains 5 cards of varying rarities: common, rare, epic and legendary. There is a different percentage chance of a card being of each rarity. A pack can contain any combination of rarities except 5 commons. I'd like to calculate the expected value of a pack using the game's currency ""Arcane Dust,"" which I'll refer to as ""Dust"" for the remainder of the post. If I do not have the card, then the card is worth the full value for the rarity. If I do have the card, it's worth its D/E (disenchant) value. I'm trying to figure out what should go in the ""?"" cell. Ignore the empty cell at the bottom of the D/E Value column. Table Explanation: The yellow colored cells contain data on the chance of a card being of each rarity, as well as their full and D/E values. The red cells contain, in order, the total amount of card of that rarity, how many are missing from my collection, and the expected value of a card of that rarity. For example, the expected Dust value for a Rare card is $$\frac5{162} \times 100 + \frac{162-5}{162} \times 40 = 22.47$$ I can calculate the expected Dust value for a single card by taking a weighted average of the values in the column on the far right, which comes out to 6.32. Multiplying this value by 5 doesn't work, because we are guaranteed not to get 5 commons. I'm really struggling figuring out how to include that requirement in the formula for expected Dust value of a pack. Given this information and the table, what is the formula for calculating the expected value of a pack?","In the video game Hearthstone, you acquire card by opening packs. Each pack contains 5 cards of varying rarities: common, rare, epic and legendary. There is a different percentage chance of a card being of each rarity. A pack can contain any combination of rarities except 5 commons. I'd like to calculate the expected value of a pack using the game's currency ""Arcane Dust,"" which I'll refer to as ""Dust"" for the remainder of the post. If I do not have the card, then the card is worth the full value for the rarity. If I do have the card, it's worth its D/E (disenchant) value. I'm trying to figure out what should go in the ""?"" cell. Ignore the empty cell at the bottom of the D/E Value column. Table Explanation: The yellow colored cells contain data on the chance of a card being of each rarity, as well as their full and D/E values. The red cells contain, in order, the total amount of card of that rarity, how many are missing from my collection, and the expected value of a card of that rarity. For example, the expected Dust value for a Rare card is $$\frac5{162} \times 100 + \frac{162-5}{162} \times 40 = 22.47$$ I can calculate the expected Dust value for a single card by taking a weighted average of the values in the column on the far right, which comes out to 6.32. Multiplying this value by 5 doesn't work, because we are guaranteed not to get 5 commons. I'm really struggling figuring out how to include that requirement in the formula for expected Dust value of a pack. Given this information and the table, what is the formula for calculating the expected value of a pack?",,"['probability', 'statistics']"
73,How can we measure the accuracy of prediction algorithm?,How can we measure the accuracy of prediction algorithm?,,"We have created a prediction algorithm, which predicts the chances of confirmation of ticket based on some parameters, and gives the prediction in percent. Now, how do I measure how close the prediction is to reality. Like 70% chances actually mean 70% chance. I would like to have some information, any information to measure the accuracy of this algorithm. Edit: To clarify, the way our algorithm works is, a user enters his ticket number for any flight or train, and based on historical data with similar situation, we predict the chances of confirmation. So, it's not like there is a fixed number of tickets, but yes we have historical data of various predictions and final result.","We have created a prediction algorithm, which predicts the chances of confirmation of ticket based on some parameters, and gives the prediction in percent. Now, how do I measure how close the prediction is to reality. Like 70% chances actually mean 70% chance. I would like to have some information, any information to measure the accuracy of this algorithm. Edit: To clarify, the way our algorithm works is, a user enters his ticket number for any flight or train, and based on historical data with similar situation, we predict the chances of confirmation. So, it's not like there is a fixed number of tickets, but yes we have historical data of various predictions and final result.",,"['probability', 'statistics', 'algorithms']"
74,Nice applications of estimation theory and hypothesis testing,Nice applications of estimation theory and hypothesis testing,,"As a mathematics professor in an engineeer school, I want to write some lab work for students in Statistics. This work should last four hours and will be made in a language such as Matlab or Python. Therefore, I am looking for nice applications of Statistics, in particular estimation theory, or hypothesis testing (potentially regression). I thought of: restoration of a blurred image with noise (based on the method of maximum likelihood) problems in signal detection. Are you aware of other applications?","As a mathematics professor in an engineeer school, I want to write some lab work for students in Statistics. This work should last four hours and will be made in a language such as Matlab or Python. Therefore, I am looking for nice applications of Statistics, in particular estimation theory, or hypothesis testing (potentially regression). I thought of: restoration of a blurred image with noise (based on the method of maximum likelihood) problems in signal detection. Are you aware of other applications?",,"['statistics', 'statistical-inference', 'applications', 'parameter-estimation', 'hypothesis-testing']"
75,Statistical bias and the probability of an outcome.,Statistical bias and the probability of an outcome.,,"A town referendum has occurred. The question posed to voters was YES or NO on a local law. There were 3 methods of voting: Electronic machine (voting booths), absentee ballot, and affidavit ballot. The results are as follows: A) The machine vote tallied 13,891 “YES” vs. 13,526 “NO” votes.  B) The absentee ballot vote tallied 377 “YES” vs. 201 “NO” votes.  C) The affidavit ballot vote tallied 419 “YES” vs. 1,854 “NO” votes. Questions: 1) What is the probability, given the results of the machine and absentee votes, of the affidavit ballot results occurring randomly? 2) What is the probability of getting anywhere from 1,854 ""NO"" votes to 2,273 ""NO"" votes randomly? Note: this is a real life application. More info regarding the scenario can be made available upon request.","A town referendum has occurred. The question posed to voters was YES or NO on a local law. There were 3 methods of voting: Electronic machine (voting booths), absentee ballot, and affidavit ballot. The results are as follows: A) The machine vote tallied 13,891 “YES” vs. 13,526 “NO” votes.  B) The absentee ballot vote tallied 377 “YES” vs. 201 “NO” votes.  C) The affidavit ballot vote tallied 419 “YES” vs. 1,854 “NO” votes. Questions: 1) What is the probability, given the results of the machine and absentee votes, of the affidavit ballot results occurring randomly? 2) What is the probability of getting anywhere from 1,854 ""NO"" votes to 2,273 ""NO"" votes randomly? Note: this is a real life application. More info regarding the scenario can be made available upon request.",,"['probability', 'statistics']"
76,"Difference between the ""Hazard Rate"" and the ""Killing Function"" of a diffusion model?","Difference between the ""Hazard Rate"" and the ""Killing Function"" of a diffusion model?",,"I posted this question on Cross Validated - but I think it applies here too. Also, it increases the chances of the question being answered. Link here If this is not acceptable - administrators please delete, and anybody else please do not take points away from me for this reason. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ What is the difference between the ""Hazard Rate"" and the ""Killing Function"" of a diffusion model? Some Definitions: The Killing Function The function k(t,x) is interpreted as the killing rate. Informally, this means that if, at time t , the particle is alive and is situated at the point x , then the probability that it dies in the next h units of time is approximately k(t,x)h when h is small. \begin{gather*} Pr(\rho  \leq t+h \mid \rho  > t, X(t) = x)\approx k(t, x)h & (1) \end{gather*} And, \begin{gather*} dX(t) = \mu dt+\sigma dW\ & (2) \end{gather*} Hazard Rate \begin{gather*} Pr(t \leq T \leq t+h \mid T > t) \approx \lambda (t)h & (3) \end{gather*} That is, λ(t)h represents the instantaneous chance that an individual will die in the interval (t, t + h) given that this individual is alive at age t . Lastly, to put it in perspective here is a picture of a diffusion with arbitrary Killing Function k(x) = a + Sqrt(t/b) , where a, and b are some constants. I added the lines for later reference. So, these results raise a lot more question. How do I interpret ""rho"" in Equation (1) for example - if I am modeling a type of bird population for with X(t) ? How do I relate the Killing Function with the Hazard Rate? Is it OK to say that if the f(t) is the density distribution of the First-Passage-Times (Refer to Fig-2), then the Hazard rate for the diffusion (2) is: \begin{gather*} \lambda (t) = \frac{f(t)}{1-F(t)} & (4) \end{gather*} If I do not know the killing function - but I observe the first passage time distribution as in Fig-2: Is it possible to solve for the Killing Function? Lastly, in the definition k(t,x) is a function of both variables { x,t }. In the literature, most of the time is referred as k(x) , which is really k(X(t)) since X() is a function of t . But if one was to actually apply it - as I did in Fig-1, say: \begin{gather*} k(x) = b[(x(t)-a)^{2}]\ & (5) \end{gather*} I would have to express it in terms of X(t) : \begin{gather*} X(t) = a+\sqrt{\frac{t}{b}} & (6) \end{gather*} But X(t) is reserved for the diffusion model (2) so it makes it extra confusing. Note: Assume OP (original poster) is very unintelligent; hence, be very specific, use simple words, do not leave any algebra out, and do not hesitate to curse me out if I wrote something stupid above. Thank you so much in advance! ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~","I posted this question on Cross Validated - but I think it applies here too. Also, it increases the chances of the question being answered. Link here If this is not acceptable - administrators please delete, and anybody else please do not take points away from me for this reason. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ What is the difference between the ""Hazard Rate"" and the ""Killing Function"" of a diffusion model? Some Definitions: The Killing Function The function k(t,x) is interpreted as the killing rate. Informally, this means that if, at time t , the particle is alive and is situated at the point x , then the probability that it dies in the next h units of time is approximately k(t,x)h when h is small. \begin{gather*} Pr(\rho  \leq t+h \mid \rho  > t, X(t) = x)\approx k(t, x)h & (1) \end{gather*} And, \begin{gather*} dX(t) = \mu dt+\sigma dW\ & (2) \end{gather*} Hazard Rate \begin{gather*} Pr(t \leq T \leq t+h \mid T > t) \approx \lambda (t)h & (3) \end{gather*} That is, λ(t)h represents the instantaneous chance that an individual will die in the interval (t, t + h) given that this individual is alive at age t . Lastly, to put it in perspective here is a picture of a diffusion with arbitrary Killing Function k(x) = a + Sqrt(t/b) , where a, and b are some constants. I added the lines for later reference. So, these results raise a lot more question. How do I interpret ""rho"" in Equation (1) for example - if I am modeling a type of bird population for with X(t) ? How do I relate the Killing Function with the Hazard Rate? Is it OK to say that if the f(t) is the density distribution of the First-Passage-Times (Refer to Fig-2), then the Hazard rate for the diffusion (2) is: \begin{gather*} \lambda (t) = \frac{f(t)}{1-F(t)} & (4) \end{gather*} If I do not know the killing function - but I observe the first passage time distribution as in Fig-2: Is it possible to solve for the Killing Function? Lastly, in the definition k(t,x) is a function of both variables { x,t }. In the literature, most of the time is referred as k(x) , which is really k(X(t)) since X() is a function of t . But if one was to actually apply it - as I did in Fig-1, say: \begin{gather*} k(x) = b[(x(t)-a)^{2}]\ & (5) \end{gather*} I would have to express it in terms of X(t) : \begin{gather*} X(t) = a+\sqrt{\frac{t}{b}} & (6) \end{gather*} But X(t) is reserved for the diffusion model (2) so it makes it extra confusing. Note: Assume OP (original poster) is very unintelligent; hence, be very specific, use simple words, do not leave any algebra out, and do not hesitate to curse me out if I wrote something stupid above. Thank you so much in advance! ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~",,"['probability', 'statistics', 'stochastic-processes', 'wiener-measure']"
77,How to solve this confusing iterative limit equation,How to solve this confusing iterative limit equation,,"I have this equation which is intended to be used on a data set of integers. I am attempting to create a software algorithm using this equation, but I am very confused about how to implement it. Things that confuse me about this equation: I am supposed to be getting a single value result from the equation, which confuses me because it seems to be some kind of iterative function. I.e. how to know what the first J value is, what to do with all the J values, etc.. I am also confused by the limit being as Delta J approaches 0. Why is the limit wrapped inside a min function? Why would the limit return more than one value? Finally, why does this equation involve imaginary numbers and how do you deal with that? $$   \operatorname{Re}(J_{i+1}) = \min \left(\lim_{\Delta J\to 0} \left(\frac{\log({R}/{\sigma NJ_i})}{\log(1/N)}\right)\right) $$ Where $R$ is the range of the data, $\sigma$ is the standard deviation, and $N$ is the number of values in data set. The purpose of the equation is to collect some kind of fractal data modeling statistic (don't know what this means) to use for machine learning purposes. The data set is comprised of 3-digit values that are taken from reading in binary file data.","I have this equation which is intended to be used on a data set of integers. I am attempting to create a software algorithm using this equation, but I am very confused about how to implement it. Things that confuse me about this equation: I am supposed to be getting a single value result from the equation, which confuses me because it seems to be some kind of iterative function. I.e. how to know what the first J value is, what to do with all the J values, etc.. I am also confused by the limit being as Delta J approaches 0. Why is the limit wrapped inside a min function? Why would the limit return more than one value? Finally, why does this equation involve imaginary numbers and how do you deal with that? $$   \operatorname{Re}(J_{i+1}) = \min \left(\lim_{\Delta J\to 0} \left(\frac{\log({R}/{\sigma NJ_i})}{\log(1/N)}\right)\right) $$ Where $R$ is the range of the data, $\sigma$ is the standard deviation, and $N$ is the number of values in data set. The purpose of the equation is to collect some kind of fractal data modeling statistic (don't know what this means) to use for machine learning purposes. The data set is comprised of 3-digit values that are taken from reading in binary file data.",,"['statistics', 'machine-learning']"
78,Optimal place to measure for simple linear regression/fitting,Optimal place to measure for simple linear regression/fitting,,"Suppose I have a linear model $y_i=a\cdot x_i+b+\epsilon_i$, where $y_i,\epsilon_i,a,b\in\mathbb{R}$, $x_i\in[-1,1]$. I can take n measurements of $y_i$ at $x_i$, where $n\in\mathbb{N}$. $\epsilon_i$ denote the errors of measurement. I use $x_i$ and $y_i$ to fit $a$ and $b$, with simple linear regression. My goal is to get the most out of my limitted measurements. Which essentially means, I want the standard error of the estimator $\hat{a}$ and $\hat{b}$ as small as possible, see https://en.wikipedia.org/wiki/Simple_linear_regression#Normality_assumption . (Suppose the normality assumptions hold). But the only thing I can choose are the values of $x_i$. How should I choose them? My thoughts up to now: Naive I would place them equispaced in my interval. Looking at  Wikipedia's $s_\hat{\beta} = \sqrt{ \frac{\tfrac{1}{n-2}\sum_{i=1}^n \hat{\varepsilon}_i^{\,2}} {\sum_{i=1}^n (x_i -\bar{x})^2} }$ I would make $\sum_{i=1}^n (x_i -\bar{x})^2$ as big as possible, which means $x_i=(-1)^i$. But then i would measure the same $x_i$ multiple times, which feels wrong. I remember from a lecture (10 years ago), that you preferably choose $x_i=cos(\frac{i⋅π}{n})$. But I can't find any references to that any more. It turns out, my memory was wrong. Any idea where to investigate or read up?","Suppose I have a linear model $y_i=a\cdot x_i+b+\epsilon_i$, where $y_i,\epsilon_i,a,b\in\mathbb{R}$, $x_i\in[-1,1]$. I can take n measurements of $y_i$ at $x_i$, where $n\in\mathbb{N}$. $\epsilon_i$ denote the errors of measurement. I use $x_i$ and $y_i$ to fit $a$ and $b$, with simple linear regression. My goal is to get the most out of my limitted measurements. Which essentially means, I want the standard error of the estimator $\hat{a}$ and $\hat{b}$ as small as possible, see https://en.wikipedia.org/wiki/Simple_linear_regression#Normality_assumption . (Suppose the normality assumptions hold). But the only thing I can choose are the values of $x_i$. How should I choose them? My thoughts up to now: Naive I would place them equispaced in my interval. Looking at  Wikipedia's $s_\hat{\beta} = \sqrt{ \frac{\tfrac{1}{n-2}\sum_{i=1}^n \hat{\varepsilon}_i^{\,2}} {\sum_{i=1}^n (x_i -\bar{x})^2} }$ I would make $\sum_{i=1}^n (x_i -\bar{x})^2$ as big as possible, which means $x_i=(-1)^i$. But then i would measure the same $x_i$ multiple times, which feels wrong. I remember from a lecture (10 years ago), that you preferably choose $x_i=cos(\frac{i⋅π}{n})$. But I can't find any references to that any more. It turns out, my memory was wrong. Any idea where to investigate or read up?",,"['statistics', 'regression']"
79,What is the probability that TEAM A will win in both of the following scenarios?,What is the probability that TEAM A will win in both of the following scenarios?,,"Two teams are in the same league and just finished a season. Assume both teams had played the exact same schedule. TEAM A had a winrate of 90% and TEAM B had a winrate of 70%. If both teams then meet each other head to head during a tournament, with your only knowledge of the two teams being their winrate, what are the odds of TEAM A winning in a Best of One match, and what are the odds of TEAM A winning in a Best of Three match. I know logic tells us that due to TEAM A having the better record competing against the same opponents they should have a higher probability of winning. Additionally, I believe the probability of TEAM A winning is higher in a best of 3 than in a best of 1. My understanding of probability is that the larger the sample size of games, the more likely the results will reflect what the probability suggested, where in a smaller sample size, it is easier for an upset. My curiosity is at what rate do the odds of an upset get smaller between a BO1 and a BO3. The probability of me being all over the place and not making any sense is high, so ask questions for clarification. Thanks for the help!","Two teams are in the same league and just finished a season. Assume both teams had played the exact same schedule. TEAM A had a winrate of 90% and TEAM B had a winrate of 70%. If both teams then meet each other head to head during a tournament, with your only knowledge of the two teams being their winrate, what are the odds of TEAM A winning in a Best of One match, and what are the odds of TEAM A winning in a Best of Three match. I know logic tells us that due to TEAM A having the better record competing against the same opponents they should have a higher probability of winning. Additionally, I believe the probability of TEAM A winning is higher in a best of 3 than in a best of 1. My understanding of probability is that the larger the sample size of games, the more likely the results will reflect what the probability suggested, where in a smaller sample size, it is easier for an upset. My curiosity is at what rate do the odds of an upset get smaller between a BO1 and a BO3. The probability of me being all over the place and not making any sense is high, so ask questions for clarification. Thanks for the help!",,"['probability', 'statistics']"
80,# of people that go to a clinic follows a poisson distribution of 4 per day......,# of people that go to a clinic follows a poisson distribution of 4 per day......,,I just had an exam and I wanted to discuss a specific question on it. I will do my best to recall the question. Suppose the number of people that go to a clinic follows a poisson distribution of 4 per day. What is the probability that no one will show up in the next 5 days? (Assume each day is independent). My answer was an extremely low probability.,I just had an exam and I wanted to discuss a specific question on it. I will do my best to recall the question. Suppose the number of people that go to a clinic follows a poisson distribution of 4 per day. What is the probability that no one will show up in the next 5 days? (Assume each day is independent). My answer was an extremely low probability.,,"['probability', 'statistics', 'poisson-distribution']"
81,Creating a minimal sufficient statistic with Likelihood function,Creating a minimal sufficient statistic with Likelihood function,,"To find a minimal sufficient statistic you can take the likelihood ratio and find a function $T$ so that the ratio does not depend on the parameter $\theta$ , as page 18 here http://sites.stat.psu.edu/~mga/514/Ch6.pdf does it. Since I do not understand the proof, I wonder if I can understand this via some logic or intuition.","To find a minimal sufficient statistic you can take the likelihood ratio and find a function $T$ so that the ratio does not depend on the parameter $\theta$ , as page 18 here http://sites.stat.psu.edu/~mga/514/Ch6.pdf does it. Since I do not understand the proof, I wonder if I can understand this via some logic or intuition.",,"['statistics', 'statistical-inference']"
82,Probability generating function of bivariate Poisson distribution!,Probability generating function of bivariate Poisson distribution!,,"Problem setup: $X_1=Y_1+Y_0,X_2=Y_2+Y_0$ where $Y_1, Y_2\text{ and }Y_0$ are independent Poisson random variables with parameters $θ_1, θ_2\text{ and }θ_0$, respectively. I know that the joint probability function of bivariate Poisson distribution is given by: $P(X_1 = x_1, X_2 = x_2) = e^{-(\theta_{1}+\theta_{2}+\theta_{0})} \displaystyle\frac{\theta_{1}^{x_1}}{x_1!}\frac{\theta_{2}^{x_2}}{x_2!} \sum_{i=0}^{min(x_1,x_2)}\binom{x_1}{i}\binom{x_2}{i}i!\left(\frac{\theta_{0}}{\theta_{1}\theta_{2}}\right)^{i}$ Also, the joint probability generating function is: $exp⁡{(θ_1 (t_1-1)+θ_2(t_2-1)+θ_0(t_1 t_2-1))}$ My question is: how can we derive this probability generating function from the joint probability function? I tried but I couldn't find this result! If anyone knows, please give me a helping hand. Also, how can I derive the probability generating function in general for the multivariate case? Thanks in advanced.","Problem setup: $X_1=Y_1+Y_0,X_2=Y_2+Y_0$ where $Y_1, Y_2\text{ and }Y_0$ are independent Poisson random variables with parameters $θ_1, θ_2\text{ and }θ_0$, respectively. I know that the joint probability function of bivariate Poisson distribution is given by: $P(X_1 = x_1, X_2 = x_2) = e^{-(\theta_{1}+\theta_{2}+\theta_{0})} \displaystyle\frac{\theta_{1}^{x_1}}{x_1!}\frac{\theta_{2}^{x_2}}{x_2!} \sum_{i=0}^{min(x_1,x_2)}\binom{x_1}{i}\binom{x_2}{i}i!\left(\frac{\theta_{0}}{\theta_{1}\theta_{2}}\right)^{i}$ Also, the joint probability generating function is: $exp⁡{(θ_1 (t_1-1)+θ_2(t_2-1)+θ_0(t_1 t_2-1))}$ My question is: how can we derive this probability generating function from the joint probability function? I tried but I couldn't find this result! If anyone knows, please give me a helping hand. Also, how can I derive the probability generating function in general for the multivariate case? Thanks in advanced.",,"['probability', 'statistics', 'discrete-mathematics', 'probability-distributions']"
83,How can dimension reduction lead to better results?,How can dimension reduction lead to better results?,,Can someone please explain why a model fitted using a linear combination of the parameters can have better results (lower error) than a plain vanilla one with all the parameters? Can I think about this like adding a bunch of interaction variables to the model to get to better $R^2$?,Can someone please explain why a model fitted using a linear combination of the parameters can have better results (lower error) than a plain vanilla one with all the parameters? Can I think about this like adding a bunch of interaction variables to the model to get to better $R^2$?,,"['statistics', 'machine-learning']"
84,How does one find the Fisher Information of a MLE?,How does one find the Fisher Information of a MLE?,,"The Statement of the Problem: Suppose that $Y_1, Y_2, \ldots, Y_n$ constitute a random sample of size $n$ from an exponential distribution with mean $\theta$. Find a $100(1-\alpha)\%$ confidence interval for $\theta$. Where I Am: I suppose that what I am to do here is construct a CI of the form $$ \hat \theta \pm z_{\frac{\alpha}{2}} \frac{1}{\sqrt{nI(\hat \theta_\text{mle})}}.$$ Well, I found $\hat \theta_\text{mle}$ (where $\theta = 1/\lambda \implies \lambda = 1/\theta)$: $$ \hat \theta_\text{mle} = n\sum_{i=1}^n Y_i.$$ But how do I find the Fisher Information of this MLE? I have in my notes that $$ \hat \theta_\text{mle} \approx N\left(\theta_0, \frac{1}{nI(\theta_0)}\right) $$ which I assume I'm supposed to use... but I'm not sure what this is saying. Do I have to find the Fisher information of a normal distribution with these parameters? That... doesn't make sense to me. If anyone could shed some light on this, I'd appreciate it. Thanks.","The Statement of the Problem: Suppose that $Y_1, Y_2, \ldots, Y_n$ constitute a random sample of size $n$ from an exponential distribution with mean $\theta$. Find a $100(1-\alpha)\%$ confidence interval for $\theta$. Where I Am: I suppose that what I am to do here is construct a CI of the form $$ \hat \theta \pm z_{\frac{\alpha}{2}} \frac{1}{\sqrt{nI(\hat \theta_\text{mle})}}.$$ Well, I found $\hat \theta_\text{mle}$ (where $\theta = 1/\lambda \implies \lambda = 1/\theta)$: $$ \hat \theta_\text{mle} = n\sum_{i=1}^n Y_i.$$ But how do I find the Fisher Information of this MLE? I have in my notes that $$ \hat \theta_\text{mle} \approx N\left(\theta_0, \frac{1}{nI(\theta_0)}\right) $$ which I assume I'm supposed to use... but I'm not sure what this is saying. Do I have to find the Fisher information of a normal distribution with these parameters? That... doesn't make sense to me. If anyone could shed some light on this, I'd appreciate it. Thanks.",,"['statistics', 'statistical-inference']"
85,Is $X(T) = A \sin(\omega_0 t + \Phi)$ mean ergodic?,Is  mean ergodic?,X(T) = A \sin(\omega_0 t + \Phi),"This is an example of a tutorial but I think has not been solved properly. Please help me! $X(T) = A \sin(\omega_0 t + \Phi)$ $A$ and $\phi$ are independent $A$ is uniformly distributed over $(0,1)$ $\Phi$ is uniformly   distributed over $(0, 2\pi)$ Is it mean-ergodic? It is solved as follows: $E[X(t)]=E[A]E[\sin(\omega_0t+\Phi)]=0$ $R_X(\tau)=E[X(t)X(t+\tau)]=E[A]E[\sin(\omega_0t+\Phi)\sin(\omega_0t+\omega_0\tau+\Phi)]=\frac{1}{2}E[A^2]\cos(\omega_0\tau)$ so it is stationary. Since its mean value is zero $C_X(\tau)=\frac{1}{2}E[A^2]\cos(\omega_0\tau)$ $\sigma^2_{\hat\mu_X}=\frac{E[A^2]}{2T}\{\int_{-2T}^{2T}[1-\frac{|\tau|}{2T}]\frac{\cos(\omega_0\tau)}{2}d\tau\}$ The next step is where I have problem with. I don't know whether we can substitute the fourier transform of $f$ when computing $\int_a^b f$ or are the fourier transforms computed here correct? Even I don't know the integration has been done correctly? The solution continues as follows: We can use the result from Fourier transform that time multiplication   is frequency convolution and evaluate the integral shown above. $$[1-\frac{|\tau|}{2T}]\Longleftrightarrow 2T[\frac{\sin(\omega T)}{\omega T}]^2\quad and\quad\frac{\cos(\omega_0\tau)}{2}\Longleftrightarrow \frac{\pi}{2}[\delta(\omega+\omega_0)+\delta(\omega-\omega_0)]$$ Substituting the Fourier transforms of the triangular and the cosine functions in the equation for vairance, we obtain $$\begin{align} \sigma^2_{\hat\mu_X}&=\frac{1}{2\pi}\frac{E[A^2]}{2T}\int_{-\infty}^{\infty}2T[\frac{\sin(\omega T)}{\omega T}]^2\frac{\pi}{2}[\delta(\omega+\omega_0)+\delta(\omega-\omega_0)]d\omega\\ &=\frac{E[A^2]}{2\pi}[\frac{\sin(\omega_0T)}{\omega_0T}]^2 \end{align}$$   $T\to\infty\,,\sigma^2_{\hat\mu_X}\to 0\,,for\;\omega_0\neq 0$ Thus $X(t)$ is mean-ergodic. 1- I don't know if the fourier transform calculation $[1-\frac{|\tau|}{2T}]\Longleftrightarrow 2T[\frac{\sin(\omega T)}{\omega T}]^2$ is true? 2- I don't know whether we can compute the integral of the fourier transform of a function instead of the original function? 3- Regarding that multiplication in time is equivalent to the convolution in frequency space I don't know whether substituting $[1-\frac{|\tau|}{2T}]\frac{\cos(\omega_0\tau)}{2}$ in $\sigma^2_{\hat\mu_X}=\frac{E[A^2]}{2T}\{\int_{-2T}^{2T}[1-\frac{|\tau|}{2T}]\frac{\cos(\omega_0\tau)}{2}d\tau\}$ by $2T[\frac{\sin(\omega T)}{\omega T}]^2\frac{\pi}{2}[\delta(\omega+\omega_0)+\delta(\omega-\omega_0)]$ in $\sigma^2_{\hat\mu_X}=\frac{1}{2\pi}\frac{E[A^2]}{2T}\int_{-\infty}^{\infty}2T[\frac{\sin(\omega T)}{\omega T}]^2\frac{\pi}{2}[\delta(\omega+\omega_0)+\delta(\omega-\omega_0)]d\omega$ is done correctly?","This is an example of a tutorial but I think has not been solved properly. Please help me! $X(T) = A \sin(\omega_0 t + \Phi)$ $A$ and $\phi$ are independent $A$ is uniformly distributed over $(0,1)$ $\Phi$ is uniformly   distributed over $(0, 2\pi)$ Is it mean-ergodic? It is solved as follows: $E[X(t)]=E[A]E[\sin(\omega_0t+\Phi)]=0$ $R_X(\tau)=E[X(t)X(t+\tau)]=E[A]E[\sin(\omega_0t+\Phi)\sin(\omega_0t+\omega_0\tau+\Phi)]=\frac{1}{2}E[A^2]\cos(\omega_0\tau)$ so it is stationary. Since its mean value is zero $C_X(\tau)=\frac{1}{2}E[A^2]\cos(\omega_0\tau)$ $\sigma^2_{\hat\mu_X}=\frac{E[A^2]}{2T}\{\int_{-2T}^{2T}[1-\frac{|\tau|}{2T}]\frac{\cos(\omega_0\tau)}{2}d\tau\}$ The next step is where I have problem with. I don't know whether we can substitute the fourier transform of $f$ when computing $\int_a^b f$ or are the fourier transforms computed here correct? Even I don't know the integration has been done correctly? The solution continues as follows: We can use the result from Fourier transform that time multiplication   is frequency convolution and evaluate the integral shown above. $$[1-\frac{|\tau|}{2T}]\Longleftrightarrow 2T[\frac{\sin(\omega T)}{\omega T}]^2\quad and\quad\frac{\cos(\omega_0\tau)}{2}\Longleftrightarrow \frac{\pi}{2}[\delta(\omega+\omega_0)+\delta(\omega-\omega_0)]$$ Substituting the Fourier transforms of the triangular and the cosine functions in the equation for vairance, we obtain $$\begin{align} \sigma^2_{\hat\mu_X}&=\frac{1}{2\pi}\frac{E[A^2]}{2T}\int_{-\infty}^{\infty}2T[\frac{\sin(\omega T)}{\omega T}]^2\frac{\pi}{2}[\delta(\omega+\omega_0)+\delta(\omega-\omega_0)]d\omega\\ &=\frac{E[A^2]}{2\pi}[\frac{\sin(\omega_0T)}{\omega_0T}]^2 \end{align}$$   $T\to\infty\,,\sigma^2_{\hat\mu_X}\to 0\,,for\;\omega_0\neq 0$ Thus $X(t)$ is mean-ergodic. 1- I don't know if the fourier transform calculation $[1-\frac{|\tau|}{2T}]\Longleftrightarrow 2T[\frac{\sin(\omega T)}{\omega T}]^2$ is true? 2- I don't know whether we can compute the integral of the fourier transform of a function instead of the original function? 3- Regarding that multiplication in time is equivalent to the convolution in frequency space I don't know whether substituting $[1-\frac{|\tau|}{2T}]\frac{\cos(\omega_0\tau)}{2}$ in $\sigma^2_{\hat\mu_X}=\frac{E[A^2]}{2T}\{\int_{-2T}^{2T}[1-\frac{|\tau|}{2T}]\frac{\cos(\omega_0\tau)}{2}d\tau\}$ by $2T[\frac{\sin(\omega T)}{\omega T}]^2\frac{\pi}{2}[\delta(\omega+\omega_0)+\delta(\omega-\omega_0)]$ in $\sigma^2_{\hat\mu_X}=\frac{1}{2\pi}\frac{E[A^2]}{2T}\int_{-\infty}^{\infty}2T[\frac{\sin(\omega T)}{\omega T}]^2\frac{\pi}{2}[\delta(\omega+\omega_0)+\delta(\omega-\omega_0)]d\omega$ is done correctly?",,"['integration', 'statistics', 'fourier-analysis', 'ergodic-theory', 'means']"
86,Estimating the Average and Standard Deviation of a Population based on a Sample with Missing Data with Known Ranks,Estimating the Average and Standard Deviation of a Population based on a Sample with Missing Data with Known Ranks,,"I need a way to shows me how the parameters of PDF, log-normal in this case, can be estimated based on a set with missing data points at the tail end of a sample. For example, Consider we had 20 numbers with specified μ and σ , and then missed two largest number of them. How do we can estimate the mean ( μ ) and standard deviation ( σ ) of these 20 numbers with only 18 available numbers, if we know that all of them obey a log-normal distribution?! (this is obvious that these 2 missing numbers are in the tail end of log-normal distribution) *I am a structural engineering student. So please don't present complex solutions :) I want to code a program in Matlab to do this for me.","I need a way to shows me how the parameters of PDF, log-normal in this case, can be estimated based on a set with missing data points at the tail end of a sample. For example, Consider we had 20 numbers with specified μ and σ , and then missed two largest number of them. How do we can estimate the mean ( μ ) and standard deviation ( σ ) of these 20 numbers with only 18 available numbers, if we know that all of them obey a log-normal distribution?! (this is obvious that these 2 missing numbers are in the tail end of log-normal distribution) *I am a structural engineering student. So please don't present complex solutions :) I want to code a program in Matlab to do this for me.",,"['probability', 'statistics', 'estimation', 'parameter-estimation']"
87,Departure from uniformity in a continuous (time) distribution,Departure from uniformity in a continuous (time) distribution,,"I know how to quantify the departure from uniformity ( or a uniform distribution) for discrete distributions. Assume you have a distribution set of P: P={P1, P2, P3, ....., PN} The departure from uniformity for this discrete distribution is defined as follow: D= SUM(Pi * log (Pi / (1/N))) Now my question is how to quantify the departure from uniformity in a time distribution which is actually a continuous distribution. I have a family of time distribution functions in the form of: (B is a parameter ) P(t;B) = - (t)^(B) Clearly, the linear distribution is resulted once B=1. How can I quantify the departure from uniformity for this family of time distributions.  Thanks in advance and I do appreciate your help. Best, HRJ","I know how to quantify the departure from uniformity ( or a uniform distribution) for discrete distributions. Assume you have a distribution set of P: P={P1, P2, P3, ....., PN} The departure from uniformity for this discrete distribution is defined as follow: D= SUM(Pi * log (Pi / (1/N))) Now my question is how to quantify the departure from uniformity in a time distribution which is actually a continuous distribution. I have a family of time distribution functions in the form of: (B is a parameter ) P(t;B) = - (t)^(B) Clearly, the linear distribution is resulted once B=1. How can I quantify the departure from uniformity for this family of time distributions.  Thanks in advance and I do appreciate your help. Best, HRJ",,"['statistics', 'information-theory', 'uniform-distribution']"
88,Instrumental Variables and orthogonality conditions,Instrumental Variables and orthogonality conditions,,"To obtain the method of moment estimate for instrument variables, we use the moment condition $z'\varepsilon=0$ in the exact identified case (number of endogenous variables = number of instruments) or $\hat{x}'\varepsilon=0$ in the overidentified case where the hat represents ttted value of the endogenous variable in a first stage regression on the instrumets. We use the sample counterpart and replace the true errors by the residuals. In the former case, we obtain $\hat{\beta}_{IV}=(z'x)^{-1}(z'y)$ and in the latter $\hat{\beta}_{2\mathrm{SLS}}=(\hat{x}'\hat{x})^{-1}(\hat{x}'y)$. The intuition for the usual Sargan test of overidentifying restrictions is that we regress the endogenous variable on the instruments in the first stage, obtain the predicted value of the endogenous variables, use these predicted values in the second stage on the RHS and obtain the (possibly) unbiased and conistent estimate of our parameters. We then use this estimate of the paramaters to obtain unbiased estimates of the true errors (the residuals in the second stage) and run the following regression: $e=\alpha_0+\alpha_1 z_1+\cdots+\alpha_k z_k+u_k$ where $e$ represent the residuals (unbiased estimates of the true errors) and $z$ represent the various instruments. We then run a usual $NR^2$ test to test the joint significance of the instruments. If instruments are valid, then we should not be able to reject the null of orthogonality. My question is that shouldnt this always be the case by construction? After all, we use this specific moment condition in the first place! Any help is much appreciated.","To obtain the method of moment estimate for instrument variables, we use the moment condition $z'\varepsilon=0$ in the exact identified case (number of endogenous variables = number of instruments) or $\hat{x}'\varepsilon=0$ in the overidentified case where the hat represents ttted value of the endogenous variable in a first stage regression on the instrumets. We use the sample counterpart and replace the true errors by the residuals. In the former case, we obtain $\hat{\beta}_{IV}=(z'x)^{-1}(z'y)$ and in the latter $\hat{\beta}_{2\mathrm{SLS}}=(\hat{x}'\hat{x})^{-1}(\hat{x}'y)$. The intuition for the usual Sargan test of overidentifying restrictions is that we regress the endogenous variable on the instruments in the first stage, obtain the predicted value of the endogenous variables, use these predicted values in the second stage on the RHS and obtain the (possibly) unbiased and conistent estimate of our parameters. We then use this estimate of the paramaters to obtain unbiased estimates of the true errors (the residuals in the second stage) and run the following regression: $e=\alpha_0+\alpha_1 z_1+\cdots+\alpha_k z_k+u_k$ where $e$ represent the residuals (unbiased estimates of the true errors) and $z$ represent the various instruments. We then run a usual $NR^2$ test to test the joint significance of the instruments. If instruments are valid, then we should not be able to reject the null of orthogonality. My question is that shouldnt this always be the case by construction? After all, we use this specific moment condition in the first place! Any help is much appreciated.",,['statistics']
89,Statistical Significance,Statistical Significance,,"I'm attempting to compute statistical significance for two data sets. I have the mean and the number of data points, but I don't think I can compute std deviation, because I don't have the individual data points. Under previous teacher, 1,072 tests given with average score of 27.7 correct. New teacher, 179 tests given with average score of 28.6. Seems self-evident to me that combination of small uptick and tiny sample size means that conclusion ""new teacher responsible for improvement"" is untenable, but I'd like to be able to say that the improvement is not statistically significant. All help appreciated.","I'm attempting to compute statistical significance for two data sets. I have the mean and the number of data points, but I don't think I can compute std deviation, because I don't have the individual data points. Under previous teacher, 1,072 tests given with average score of 27.7 correct. New teacher, 179 tests given with average score of 28.6. Seems self-evident to me that combination of small uptick and tiny sample size means that conclusion ""new teacher responsible for improvement"" is untenable, but I'd like to be able to say that the improvement is not statistically significant. All help appreciated.",,['statistics']
90,Mutual information as a fraction of entropy?,Mutual information as a fraction of entropy?,,"Suppose I have two (discrete) random variables $(X,Y)$ with some joint distribution $P$. The mutual information $I(X;Y)$ is informally defined as the reduction is the remaining entropy in $X$ once the realization of $Y$ is known, ie $$I(X;Y) = H(X) - H(X|Y)$$ The units of all the information quantities (entropy and mutual information) are typically bits. I am interested in computing a normalized version of the mutual information which takes a value in $[0,1]$ and is defined as the proportion of reduction in entropy of $X$ once the realization of $Y$ is known, i.e. $$I'(X;Y) = \frac{I(X;Y)}{H(X)}$$ Are there any references in the literature which use this normalized version $I'(X;Y)$ (which is obviously not symmetric)? The reason I am interested in this question is to use mutual information to compare the strength of dependencies between random variables (parallel to how correlation is a normalization of covariance for measuring the strength of linear dependence). If $I(X,Y) = 4$ and $I(W,Z)$ = 1000, this alone does not facilitate a comparison about the relative strength of dependency (are $(X,Y)$ or $(W,Z)$ more strongly dependent?) without knowing the entropies of $X,Y,W,Z$.","Suppose I have two (discrete) random variables $(X,Y)$ with some joint distribution $P$. The mutual information $I(X;Y)$ is informally defined as the reduction is the remaining entropy in $X$ once the realization of $Y$ is known, ie $$I(X;Y) = H(X) - H(X|Y)$$ The units of all the information quantities (entropy and mutual information) are typically bits. I am interested in computing a normalized version of the mutual information which takes a value in $[0,1]$ and is defined as the proportion of reduction in entropy of $X$ once the realization of $Y$ is known, i.e. $$I'(X;Y) = \frac{I(X;Y)}{H(X)}$$ Are there any references in the literature which use this normalized version $I'(X;Y)$ (which is obviously not symmetric)? The reason I am interested in this question is to use mutual information to compare the strength of dependencies between random variables (parallel to how correlation is a normalization of covariance for measuring the strength of linear dependence). If $I(X,Y) = 4$ and $I(W,Z)$ = 1000, this alone does not facilitate a comparison about the relative strength of dependency (are $(X,Y)$ or $(W,Z)$ more strongly dependent?) without knowing the entropies of $X,Y,W,Z$.",,"['probability', 'probability-theory', 'statistics', 'information-theory']"
91,The uniqueness of solution of an equation that involves CDFs,The uniqueness of solution of an equation that involves CDFs,,"I have two monotone CDFs $F(x)$ and $G(x)$. The functions are symmetric in a sense that $F(x)=1-G(1-x)$, $f(x)=g(1-x)$. I am trying to show that equation $xF(2x)+(1-x)G(2x)=1/n$, $n\geq2$ has a unique solution on $[0, 0.5]$. I have not found a counterexample, and simulations show that this is likely to be correct. I would appreciate any ideas how to show this generally. ps: $xF(2x)+(1-x)G(2x)$ is generally not monotone, and it seems that when it is not, there are 1 local maximum and 1 local minimum, which are above 0.5.","I have two monotone CDFs $F(x)$ and $G(x)$. The functions are symmetric in a sense that $F(x)=1-G(1-x)$, $f(x)=g(1-x)$. I am trying to show that equation $xF(2x)+(1-x)G(2x)=1/n$, $n\geq2$ has a unique solution on $[0, 0.5]$. I have not found a counterexample, and simulations show that this is likely to be correct. I would appreciate any ideas how to show this generally. ps: $xF(2x)+(1-x)G(2x)$ is generally not monotone, and it seems that when it is not, there are 1 local maximum and 1 local minimum, which are above 0.5.",,"['statistics', 'probability-distributions']"
92,Likelihood of large difference in averages,Likelihood of large difference in averages,,"Note that I updated question because initial example grades did not satisfy the overall average that followed from the numbers given in the article. A newspaper artticle in Norway made me curious about this one. https://translate.google.com/translate?sl=no&tl=en&js=y&prev=_t&hl=en&ie=UTF-8&u=http%3A%2F%2Fwww.dn.no%2Ftalent%2F2015%2F06%2F30%2F1025%2FEksamen%2Fda-elevene-sorterte-eksamenskarakterene-alfabetisk-etter-navn-gikk-det-opp-for-dem&edit-text= In a school class there were 25 students who had the same exam, graded from 1 to 6. After the the exams had been graded a lot of the students felt something was wrong. So one them collected the names and grades of all the students and put them into an excel sheet. When he sorted the names alphabetically he noticed that the first 13 had on average better grades than the last 12. In fact, the average of the first 12 was 3.4 and the average of the last 13 was 1.8. When confronting the school it was confirmed that the class had been split in half, by name, and one pair of evaluators graded the first 13 students, and a different pair of evaluators graded the last 12 students. (there are always 2 persons evaluating each exam submitted) Now we are debating the probablity that this difference in averages occured randomly because stuff just happens, and the probablity that the different evaluators was the cause of this. Unfortunately I don't know the grades. But suppose the grades i the entire class were: 1,1,1,1,1,1,1 2,2,2,2,2,2 3,3,3,3,3,3 4,4,4 5,5 6  Group A: 1 2,2 3,3,3,3 4,4,4 5,5 6  Group B: 1,1,1,1,1,1 2,2,2 3,3 4 These numbers gives close to the averages mentioned. (but the distribution feels kinds of wrong, but overall average was very low...) I'm not sure of the math here, so I ran a simulation and found that there is about 16% chance that the difference in averages between 2 randomly created groups of sizes 12 and 13 is 3.4-1.8=1.6 or larger. 16% is not that small a number. This will happen many times each year in classes at many schools. But in this case that we also know that there were different pairs of evaluators in the 2 groups, how should I interpret this? Is it correct to say it is 84% chance that the large difference was beacuse of different evaluators? Are there good reasons for the group with the worst grades to demand new evaluations? I think the 16% number is correct, but doing the math would be better than  running a sim. So some pointers on how to do that would also be appreciated.","Note that I updated question because initial example grades did not satisfy the overall average that followed from the numbers given in the article. A newspaper artticle in Norway made me curious about this one. https://translate.google.com/translate?sl=no&tl=en&js=y&prev=_t&hl=en&ie=UTF-8&u=http%3A%2F%2Fwww.dn.no%2Ftalent%2F2015%2F06%2F30%2F1025%2FEksamen%2Fda-elevene-sorterte-eksamenskarakterene-alfabetisk-etter-navn-gikk-det-opp-for-dem&edit-text= In a school class there were 25 students who had the same exam, graded from 1 to 6. After the the exams had been graded a lot of the students felt something was wrong. So one them collected the names and grades of all the students and put them into an excel sheet. When he sorted the names alphabetically he noticed that the first 13 had on average better grades than the last 12. In fact, the average of the first 12 was 3.4 and the average of the last 13 was 1.8. When confronting the school it was confirmed that the class had been split in half, by name, and one pair of evaluators graded the first 13 students, and a different pair of evaluators graded the last 12 students. (there are always 2 persons evaluating each exam submitted) Now we are debating the probablity that this difference in averages occured randomly because stuff just happens, and the probablity that the different evaluators was the cause of this. Unfortunately I don't know the grades. But suppose the grades i the entire class were: 1,1,1,1,1,1,1 2,2,2,2,2,2 3,3,3,3,3,3 4,4,4 5,5 6  Group A: 1 2,2 3,3,3,3 4,4,4 5,5 6  Group B: 1,1,1,1,1,1 2,2,2 3,3 4 These numbers gives close to the averages mentioned. (but the distribution feels kinds of wrong, but overall average was very low...) I'm not sure of the math here, so I ran a simulation and found that there is about 16% chance that the difference in averages between 2 randomly created groups of sizes 12 and 13 is 3.4-1.8=1.6 or larger. 16% is not that small a number. This will happen many times each year in classes at many schools. But in this case that we also know that there were different pairs of evaluators in the 2 groups, how should I interpret this? Is it correct to say it is 84% chance that the large difference was beacuse of different evaluators? Are there good reasons for the group with the worst grades to demand new evaluations? I think the 16% number is correct, but doing the math would be better than  running a sim. So some pointers on how to do that would also be appreciated.",,['statistics']
93,How to measure the stability of datas,How to measure the stability of datas,,"The background: I have a server handling $n$ kinds of requests, denoted by $k_1, ..., k_n$, at a certain time, many requests has been processed, the average time it takes to process $k_i$ is $t_i$, for $i \neq j$, $\left|t_i - t_j\right|$ can be very large, or very small. The question: Is there a function, $f$, it takes a list of numbers, (in this specific case, the numbers are the time consumed by processing $k_i$), and yields a number, so that, for $j \in \{1,...n\}$, I get n numbers, $f_1, ..., f_n$, which allows me to know, by the magnitude of these numbers, among these $n$ kinds requests, which one is most unstable to process?","The background: I have a server handling $n$ kinds of requests, denoted by $k_1, ..., k_n$, at a certain time, many requests has been processed, the average time it takes to process $k_i$ is $t_i$, for $i \neq j$, $\left|t_i - t_j\right|$ can be very large, or very small. The question: Is there a function, $f$, it takes a list of numbers, (in this specific case, the numbers are the time consumed by processing $k_i$), and yields a number, so that, for $j \in \{1,...n\}$, I get n numbers, $f_1, ..., f_n$, which allows me to know, by the magnitude of these numbers, among these $n$ kinds requests, which one is most unstable to process?",,"['statistics', 'statistical-inference']"
94,Formula to represent 'equality',Formula to represent 'equality',,"I am trying to find the appropriate formula in order to calculate and represent the 'equality' between a set of values. Let me explain with an example: Imagine 3 people speaking on an a TV Show that lasts 45 minutes: Person 1 -> 30 minutes spoken Person 2 -> 5 minutes spoken Person 3 -> 10 minutes spoken I want to find a number that expresses how ""equal"" was this discussion in matters of time spoken per person. The ideal would be to speak 15 minutes each (100% equality) and the worst case scenario would be to speak only one person for 45 minutes (0% equality). My first thought, is to use the standard deviation. When the standard deviation is 0 we have perfect equality. As the standard deviation gets larger, the equality is reduced. The problem with standard deviation, is that is not easily readable for a person who is not familiar with statistics. Can you think of a formula that can help me represent the standard deviation (maybe in conjunction with the mean) as a percentage between 0% and 100% ?","I am trying to find the appropriate formula in order to calculate and represent the 'equality' between a set of values. Let me explain with an example: Imagine 3 people speaking on an a TV Show that lasts 45 minutes: Person 1 -> 30 minutes spoken Person 2 -> 5 minutes spoken Person 3 -> 10 minutes spoken I want to find a number that expresses how ""equal"" was this discussion in matters of time spoken per person. The ideal would be to speak 15 minutes each (100% equality) and the worst case scenario would be to speak only one person for 45 minutes (0% equality). My first thought, is to use the standard deviation. When the standard deviation is 0 we have perfect equality. As the standard deviation gets larger, the equality is reduced. The problem with standard deviation, is that is not easily readable for a person who is not familiar with statistics. Can you think of a formula that can help me represent the standard deviation (maybe in conjunction with the mean) as a percentage between 0% and 100% ?",,['statistics']
95,Why is this the solution?,Why is this the solution?,,"I have this exercise with the solution. Let $X\sim N(0,1)$.  Show that $P(2X = 3Y + 1) = 0$ if $Y\sim \text{Poisson}(\lambda)$. I have this solution $P(2X=3Y+1)= P(\bigcup_{k=0}^{\infty}(2X = 3Y+1, Y=k)) = \sum_{k=0}^\infty P(2X=3Y+1, Y=k) = \sum_{k=0}^\infty P(2X=3k+1, Y=k)< \sum_{k=0}^\infty P(2X=3k+1) = \sum_{k=0}^\infty P(X = \frac{3k+1}{2})=0$. but I don't understand because $\sum_{k=0}^\infty P(X = \frac{3k+1}{2})$ is zero. Thanks to all!","I have this exercise with the solution. Let $X\sim N(0,1)$.  Show that $P(2X = 3Y + 1) = 0$ if $Y\sim \text{Poisson}(\lambda)$. I have this solution $P(2X=3Y+1)= P(\bigcup_{k=0}^{\infty}(2X = 3Y+1, Y=k)) = \sum_{k=0}^\infty P(2X=3Y+1, Y=k) = \sum_{k=0}^\infty P(2X=3k+1, Y=k)< \sum_{k=0}^\infty P(2X=3k+1) = \sum_{k=0}^\infty P(X = \frac{3k+1}{2})=0$. but I don't understand because $\sum_{k=0}^\infty P(X = \frac{3k+1}{2})$ is zero. Thanks to all!",,"['probability', 'statistics']"
96,What is the asymptotic value of the smoothed probability in a HMM model?,What is the asymptotic value of the smoothed probability in a HMM model?,,"If I have a HMM model with a hidden markov chain $(S_t)_t$ with 3 states and if I assume that the distribution of the observation knowing in which state it is, is a normal. Do I know what is the value asymptotic of the smoothed probabilities? In other words what is $P(S_t=s|Y_1,..,Y_{\infty})$?","If I have a HMM model with a hidden markov chain $(S_t)_t$ with 3 states and if I assume that the distribution of the observation knowing in which state it is, is a normal. Do I know what is the value asymptotic of the smoothed probabilities? In other words what is $P(S_t=s|Y_1,..,Y_{\infty})$?",,"['probability', 'probability-theory', 'statistics', 'statistical-inference', 'bayesian-network']"
97,Expectation of an integral with a random variable on its borne,Expectation of an integral with a random variable on its borne,,"Having a random variable $T$ with a known distribution: Is it correct to say that $E\left[\displaystyle\int_{0}^{T}a\,dt\right]=a\times E[T]$ ? How can we calculate $E\left[\displaystyle \int_{0}^{T}f(t)\,dt\right]$  with $f(t)$ a known function of $t$ ?","Having a random variable $T$ with a known distribution: Is it correct to say that $E\left[\displaystyle\int_{0}^{T}a\,dt\right]=a\times E[T]$ ? How can we calculate $E\left[\displaystyle \int_{0}^{T}f(t)\,dt\right]$  with $f(t)$ a known function of $t$ ?",,"['probability', 'statistics']"
98,Distance of a test point from the center of an ellipsoid,Distance of a test point from the center of an ellipsoid,,"I'm trying to learn about Mahanalobis distance and I'm pretty close to getting the idea. I've learned that the distance has got a lot to do with the properties of an ellipsoid . I have understood so far that: The Mahalanobis distance is simply the distance of the test point $\textbf{x}$ from the center of mass $\textbf{y}$ divided by the width of the ellipsoid in the direction of the test point and is given by the formula: $$D(\textbf{x},\textbf{y})=\sqrt{ (\textbf{x}-\textbf{y})^TC^{-1}(\textbf{x}-\textbf{y})} $$ Now my question is: ""Why does this formula give us the distance of a point $\textbf{x}$ from the center of mass $\textbf{y}$ divided by the width of the ellipsoid in the direction of the test point?"" =) I don't understand or see how this formula describes that distance, could someone help explaining this distance more? How is the plain distance of a point $\textbf{x}$ from the ellipsoid's center of mass $\textbf{y}$ in the direction of the test point found and why? =) I hope my question is clear enough. My question could be analogous with for example ""Why does $c^2 = a^2 + b^2$"" Then you would prove this to me with a geometric proof or something =) Thank you for any help =) P.S. $C$ is the covariance matrix of vector $\textbf{x} = (x_1, ..., x_n)$","I'm trying to learn about Mahanalobis distance and I'm pretty close to getting the idea. I've learned that the distance has got a lot to do with the properties of an ellipsoid . I have understood so far that: The Mahalanobis distance is simply the distance of the test point $\textbf{x}$ from the center of mass $\textbf{y}$ divided by the width of the ellipsoid in the direction of the test point and is given by the formula: $$D(\textbf{x},\textbf{y})=\sqrt{ (\textbf{x}-\textbf{y})^TC^{-1}(\textbf{x}-\textbf{y})} $$ Now my question is: ""Why does this formula give us the distance of a point $\textbf{x}$ from the center of mass $\textbf{y}$ divided by the width of the ellipsoid in the direction of the test point?"" =) I don't understand or see how this formula describes that distance, could someone help explaining this distance more? How is the plain distance of a point $\textbf{x}$ from the ellipsoid's center of mass $\textbf{y}$ in the direction of the test point found and why? =) I hope my question is clear enough. My question could be analogous with for example ""Why does $c^2 = a^2 + b^2$"" Then you would prove this to me with a geometric proof or something =) Thank you for any help =) P.S. $C$ is the covariance matrix of vector $\textbf{x} = (x_1, ..., x_n)$",,"['linear-algebra', 'probability', 'geometry', 'statistics', 'multivariable-calculus']"
99,Can I adjust linear growth of a a subpopulation to a linear decay of the general population?,Can I adjust linear growth of a a subpopulation to a linear decay of the general population?,,"I need to estimate the amount of CF patients in Poland in the next four years. I have: estimations of the Polish population for the future years a CF patients' register for the last couple of years in the USA estimates for the USA population in the next few years. The catch is: Polish population will be decreasing, but the CF population should be growing, with more awareness, better diagnostics etc. The question is: can I (assuming that the CF subpopulation's relative growth in the USA is the same as in Poland) use the data about the CF subpopulation in the USA, adjust it for the USA growing population, as well as Polish diminishing population to get a statistically justified estimate of the future CF subpopulation's size in Poland? All those data seem to have a linear trend for the last couple of years, so I thought I could just make linear regressions for all three data sets and then assume that the polish CF subpopulation will grow according to the slope $\alpha_{\textrm{CF in Poland}} = \alpha_{\textrm{CF in USA}} - \alpha_{\textrm{USA}} + \alpha_{\textrm{Poland}}$, where $\alpha$ are slopes for the respective reggresions. Is there any statistical merit to this approach? EDIT: After beginning my calculations I see that my method is wrong (the slopes for Poland's and USA population growth differ greatly in size).","I need to estimate the amount of CF patients in Poland in the next four years. I have: estimations of the Polish population for the future years a CF patients' register for the last couple of years in the USA estimates for the USA population in the next few years. The catch is: Polish population will be decreasing, but the CF population should be growing, with more awareness, better diagnostics etc. The question is: can I (assuming that the CF subpopulation's relative growth in the USA is the same as in Poland) use the data about the CF subpopulation in the USA, adjust it for the USA growing population, as well as Polish diminishing population to get a statistically justified estimate of the future CF subpopulation's size in Poland? All those data seem to have a linear trend for the last couple of years, so I thought I could just make linear regressions for all three data sets and then assume that the polish CF subpopulation will grow according to the slope $\alpha_{\textrm{CF in Poland}} = \alpha_{\textrm{CF in USA}} - \alpha_{\textrm{USA}} + \alpha_{\textrm{Poland}}$, where $\alpha$ are slopes for the respective reggresions. Is there any statistical merit to this approach? EDIT: After beginning my calculations I see that my method is wrong (the slopes for Poland's and USA population growth differ greatly in size).",,"['statistics', 'regression', 'applications', 'estimation']"
