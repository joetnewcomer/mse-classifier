,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,$R$ with an upper bound for degrees of irreducibles in $R[x]$,with an upper bound for degrees of irreducibles in,R R[x],"One very convenient property of $\mathbb{R}$ as a ring is that there is an upper bound for the degree of irreducible polynomials in $\mathbb{R}[x]$, as If $f\in\mathbb{R}[x]$ has degree larger than $2$, then $f$ is reducible. However, the proof as I know depends highly on the fact that $\mathbb{C}$ is algebraically closed, and the very nice property: $z\in\mathbb{C}$ is in $\mathbb{R}$ if and only if $z=\bar{z}$. This makes a generalization to other integral domains rather difficult. So the problem is For what kind of integral domain $R$, we have a finite upper bound on the degree of irreducible elements in $R[x]$? Some most familiar examples are ruled out: in $\mathbb{Z}[x]$, $\mathbb{Q}[x]$ and $\mathbb{F}_{p}[x]$, there is not such a bound. Unfortunately these exhaust all integral domains about which I have a working knowledge. Another result might help is Eisenstein's criterion and its generalized form , which says if we can find a prime ideal $\mathfrak{p}$ in $R$ such that $\mathfrak{p}^2\neq\mathfrak{p}$, then by picking $a\in\mathfrak{p}^2\backslash\mathfrak{p}$ we have an irreducible $a+x^d$, where $d$ can be arbitrary, and hence the upper bound is not possible. So we only need to focus on domains where $\mathfrak{p}^2=\mathfrak{p}$ for all prime ideals. This seems to be a quite strong restriction but I am not sure what to make of it. Can someone give a hint? Thanks!","One very convenient property of $\mathbb{R}$ as a ring is that there is an upper bound for the degree of irreducible polynomials in $\mathbb{R}[x]$, as If $f\in\mathbb{R}[x]$ has degree larger than $2$, then $f$ is reducible. However, the proof as I know depends highly on the fact that $\mathbb{C}$ is algebraically closed, and the very nice property: $z\in\mathbb{C}$ is in $\mathbb{R}$ if and only if $z=\bar{z}$. This makes a generalization to other integral domains rather difficult. So the problem is For what kind of integral domain $R$, we have a finite upper bound on the degree of irreducible elements in $R[x]$? Some most familiar examples are ruled out: in $\mathbb{Z}[x]$, $\mathbb{Q}[x]$ and $\mathbb{F}_{p}[x]$, there is not such a bound. Unfortunately these exhaust all integral domains about which I have a working knowledge. Another result might help is Eisenstein's criterion and its generalized form , which says if we can find a prime ideal $\mathfrak{p}$ in $R$ such that $\mathfrak{p}^2\neq\mathfrak{p}$, then by picking $a\in\mathfrak{p}^2\backslash\mathfrak{p}$ we have an irreducible $a+x^d$, where $d$ can be arbitrary, and hence the upper bound is not possible. So we only need to focus on domains where $\mathfrak{p}^2=\mathfrak{p}$ for all prime ideals. This seems to be a quite strong restriction but I am not sure what to make of it. Can someone give a hint? Thanks!",,"['abstract-algebra', 'polynomials', 'ring-theory', 'prime-numbers', 'modules']"
1,Which algebraic structure captures the ordinal arithmetic?,Which algebraic structure captures the ordinal arithmetic?,,"Consider the set class $\mathrm{Ord}$ of all (finite and infinite) ordinal numbers , equipped with ordinal arithmetic operations: addition, multiplication, and exponentiation. It is closed under these operations. Addition is non-commutative and there are no additive or multiplicative inverses. Is $(\mathrm{Ord}, +)$ a magma ? What algebraic structure does $\mathrm{Ord}$ posses (under either/both $+, \times$ operations)?","Consider the set class $\mathrm{Ord}$ of all (finite and infinite) ordinal numbers , equipped with ordinal arithmetic operations: addition, multiplication, and exponentiation. It is closed under these operations. Addition is non-commutative and there are no additive or multiplicative inverses. Is $(\mathrm{Ord}, +)$ a magma ? What algebraic structure does $\mathrm{Ord}$ posses (under either/both $+, \times$ operations)?",,"['abstract-algebra', 'elementary-set-theory']"
2,"How can I work with (i.e. add, multiply) algebraic numbers in practice?","How can I work with (i.e. add, multiply) algebraic numbers in practice?",,"I know that the real algebraic numbers $\mathbb A \subset \mathbb R$ form a field. I've seen this as a more theoretical result, but it's also seems nice idea to implement algebraic numbers for the computer in order to make computations more exact. Now I wonder how could this be done? I thought of representing an algebraic number $\alpha$ as a tuple $(P,(a,b)) \in \mathbb Q[T]\times \mathbb Q^2$ such that $\alpha$ is the unique root of $P$ in $[a,b]$. Now take for example the numbers $\varphi = (T^2-T-1,(\frac 3 2,2))$ $\sqrt 2 = (T^2-2,(1,2))$ But how can I actually perform computations on this numbers? Surely $\varphi + \sqrt 2$ lies in $(\frac 5 2, 4)$, but what polynomial's root is it? What about $\varphi \cdot \sqrt 2$? In general, given polynomials $P,Q \in \mathbb Q[T]$ with $P(x)=Q(y)=0$, how do I find some polynomial $S$ such that $S(x+y)=0$?","I know that the real algebraic numbers $\mathbb A \subset \mathbb R$ form a field. I've seen this as a more theoretical result, but it's also seems nice idea to implement algebraic numbers for the computer in order to make computations more exact. Now I wonder how could this be done? I thought of representing an algebraic number $\alpha$ as a tuple $(P,(a,b)) \in \mathbb Q[T]\times \mathbb Q^2$ such that $\alpha$ is the unique root of $P$ in $[a,b]$. Now take for example the numbers $\varphi = (T^2-T-1,(\frac 3 2,2))$ $\sqrt 2 = (T^2-2,(1,2))$ But how can I actually perform computations on this numbers? Surely $\varphi + \sqrt 2$ lies in $(\frac 5 2, 4)$, but what polynomial's root is it? What about $\varphi \cdot \sqrt 2$? In general, given polynomials $P,Q \in \mathbb Q[T]$ with $P(x)=Q(y)=0$, how do I find some polynomial $S$ such that $S(x+y)=0$?",,['abstract-algebra']
3,An example of prime ideal $P$ in an integral domain such that $\bigcap_{n=1}^{\infty}P^n$ is not prime,An example of prime ideal  in an integral domain such that  is not prime,P \bigcap_{n=1}^{\infty}P^n,I am looking for an example of prime ideal $P$ in an integral domain such that the ideal $\bigcap_{n=1}^{\infty}P^n$ is not a prime ideal. This is a followup to this question where the ring was not assumed to be an integral domain.,I am looking for an example of prime ideal $P$ in an integral domain such that the ideal $\bigcap_{n=1}^{\infty}P^n$ is not a prime ideal. This is a followup to this question where the ring was not assumed to be an integral domain.,,"['abstract-algebra', 'ring-theory', 'commutative-algebra', 'maximal-and-prime-ideals', 'dedekind-domain']"
4,Show that there is no subgroup of $S_n$ of order $(n-1)!/n$.,Show that there is no subgroup of  of order .,S_n (n-1)!/n,"I am trying to show that $S_n$ does not have a subgroup of order $(n-1)!/n$ for any $n$ other than $6$. I have checked it to be true up to $S_{13}$. Any ideas? Of course, if $n$ is prime then that order isn't an integer, so obviously there can't be a subgroup of that order. But what about composite $n$?","I am trying to show that $S_n$ does not have a subgroup of order $(n-1)!/n$ for any $n$ other than $6$. I have checked it to be true up to $S_{13}$. Any ideas? Of course, if $n$ is prime then that order isn't an integer, so obviously there can't be a subgroup of that order. But what about composite $n$?",,"['abstract-algebra', 'group-theory', 'permutations', 'symmetric-groups']"
5,Morita equivalent rings have homeomorphic spectra?,Morita equivalent rings have homeomorphic spectra?,,"For two unital rings $R,S$, let $R\sim S$ denote that $R$ and $S$ are Morita equivalent , i.e. the categories $\mathrm{Mod}_R$ and $\mathrm{Mod}_S$ are equivalent. Examples include: Matrix rings: $R\sim M_n(R)$ for any $n\geq 1$, Full corners: $R\sim eRe$ whenever $e$ is a full idempotent in $R$, i.e. $e=e^2$ and $ReR=R$. The former is an example of the latter, since $R\simeq \varepsilon_{11}M_n(R)\varepsilon_{11}$ where $\varepsilon_{11}\in M_n(R)$ is the idempotent with $1$ in the $(1,1)$ entry and $0$'s everywhere else. It turns out these are essentially the only ways to construct Morita equivalent rings. Theorem. $R\sim S$ if and only if $S\simeq eM_n(R)e$ for some $n\geq 1$ and some full idempotent $e\in M_n(R)$. If $R$ is a unital ring, a proper ideal $P\subseteq R$ is prime if $IJ\subseteq P$ implies $I\subseteq P$ or $J\subseteq P$ for all ideals $I,J$. Let $\mathrm{Spec}(R)$ denote the set of prime ideals of $R$, called the prime spectrum of $R$, with the usual Zariski topology : the closed sets are $$V(I):=\{ P\in\mathrm{Spec}(R) : P\supseteq I\}$$ for $I$ an ideal of $R$. My question is: if $R$ and $S$ are Morita equivalent rings, then is $\mathrm{Spec}(R)$ homeomorphic to $\mathrm{Spec}(S)$? I'm also wondering if Morita equivalence determines the poset of ideals. By the above Theorem, we just have to investigate the cases where $S=M_n(R)$ or $S=eRe$. It's not hard to see that $I\mapsto M_n(I)$ is an order-preserving bijection between ideals of $R$ and ideals of $M_n(R)$, which is multiplicative in the sense that $M_n(IJ)=M_n(I)M_n(J)$, and this multiplicativity implies that it takes prime ideals to prime ideals. It's also easily seen to be continuous when restricted to $\mathrm{Spec}(R)$. So we can conclude that $$\mathrm{Spec}(R) \simeq \mathrm{Spec}(M_n(R)).$$ Now how about full corners? If $eRe$ is not full then it's easy to see that it's false: for example $S=\mathbf{Z}$ is a corner of $R=\mathbf{Z}\oplus \mathbf{Z}$ where $e=(1,0)\in R$, and yet $\mathrm{Spec}(\mathbf{Z})\not\simeq \mathrm{Spec}(\mathbf{Z}\oplus\mathbf{Z})$. This corner is not full though. Here's my idea: we have a map $$\Phi:\{\text{ideals of } R\} \rightarrow \{\text{ideals of } eRe\}$$ $$I\longmapsto eIe$$ which is surjective --- a right inverse is given by $\Psi(J):=RJR$. Is $\Phi$ injective if $eRe$ is a full corner? Does $\Phi$ preserve prime ideals? Would it be Zariski-continuous? EDIT: Clearly order-preserving maps are Zariski-continuous, so $\Phi$ is definitely continuous.","For two unital rings $R,S$, let $R\sim S$ denote that $R$ and $S$ are Morita equivalent , i.e. the categories $\mathrm{Mod}_R$ and $\mathrm{Mod}_S$ are equivalent. Examples include: Matrix rings: $R\sim M_n(R)$ for any $n\geq 1$, Full corners: $R\sim eRe$ whenever $e$ is a full idempotent in $R$, i.e. $e=e^2$ and $ReR=R$. The former is an example of the latter, since $R\simeq \varepsilon_{11}M_n(R)\varepsilon_{11}$ where $\varepsilon_{11}\in M_n(R)$ is the idempotent with $1$ in the $(1,1)$ entry and $0$'s everywhere else. It turns out these are essentially the only ways to construct Morita equivalent rings. Theorem. $R\sim S$ if and only if $S\simeq eM_n(R)e$ for some $n\geq 1$ and some full idempotent $e\in M_n(R)$. If $R$ is a unital ring, a proper ideal $P\subseteq R$ is prime if $IJ\subseteq P$ implies $I\subseteq P$ or $J\subseteq P$ for all ideals $I,J$. Let $\mathrm{Spec}(R)$ denote the set of prime ideals of $R$, called the prime spectrum of $R$, with the usual Zariski topology : the closed sets are $$V(I):=\{ P\in\mathrm{Spec}(R) : P\supseteq I\}$$ for $I$ an ideal of $R$. My question is: if $R$ and $S$ are Morita equivalent rings, then is $\mathrm{Spec}(R)$ homeomorphic to $\mathrm{Spec}(S)$? I'm also wondering if Morita equivalence determines the poset of ideals. By the above Theorem, we just have to investigate the cases where $S=M_n(R)$ or $S=eRe$. It's not hard to see that $I\mapsto M_n(I)$ is an order-preserving bijection between ideals of $R$ and ideals of $M_n(R)$, which is multiplicative in the sense that $M_n(IJ)=M_n(I)M_n(J)$, and this multiplicativity implies that it takes prime ideals to prime ideals. It's also easily seen to be continuous when restricted to $\mathrm{Spec}(R)$. So we can conclude that $$\mathrm{Spec}(R) \simeq \mathrm{Spec}(M_n(R)).$$ Now how about full corners? If $eRe$ is not full then it's easy to see that it's false: for example $S=\mathbf{Z}$ is a corner of $R=\mathbf{Z}\oplus \mathbf{Z}$ where $e=(1,0)\in R$, and yet $\mathrm{Spec}(\mathbf{Z})\not\simeq \mathrm{Spec}(\mathbf{Z}\oplus\mathbf{Z})$. This corner is not full though. Here's my idea: we have a map $$\Phi:\{\text{ideals of } R\} \rightarrow \{\text{ideals of } eRe\}$$ $$I\longmapsto eIe$$ which is surjective --- a right inverse is given by $\Psi(J):=RJR$. Is $\Phi$ injective if $eRe$ is a full corner? Does $\Phi$ preserve prime ideals? Would it be Zariski-continuous? EDIT: Clearly order-preserving maps are Zariski-continuous, so $\Phi$ is definitely continuous.",,"['abstract-algebra', 'ring-theory', 'maximal-and-prime-ideals']"
6,Every subring of a field is a domain. Is this reciprocal?,Every subring of a field is a domain. Is this reciprocal?,,"I'm reading my notes on ring theory, and we proved on class that every subring of a field is a domain. Proof: Let $S \subseteq K$ be a subring of $K$, with $K$ a field. Let $x,y \in S$. If $xy=0$, then $xy=0$ in $K$ too, so $x=0$ or $y=0$ (because $K$ is a field). Automatically the question that came to my mind was: Is the reciprocal true? Can we say that every domain is a subring of a field? Thank you.","I'm reading my notes on ring theory, and we proved on class that every subring of a field is a domain. Proof: Let $S \subseteq K$ be a subring of $K$, with $K$ a field. Let $x,y \in S$. If $xy=0$, then $xy=0$ in $K$ too, so $x=0$ or $y=0$ (because $K$ is a field). Automatically the question that came to my mind was: Is the reciprocal true? Can we say that every domain is a subring of a field? Thank you.",,"['abstract-algebra', 'ring-theory', 'field-theory']"
7,Algebraic independence via the Jacobian,Algebraic independence via the Jacobian,,I have seen being mentioned that algebraic independence of polynomials can be tested by the so called Jacobian Criterion (Apparently one takes the Jacobian matrix of these polynomials and inspects the rank of the matrix (or the rank of its minors)). Where can i find the precise statement and its proof?,I have seen being mentioned that algebraic independence of polynomials can be tested by the so called Jacobian Criterion (Apparently one takes the Jacobian matrix of these polynomials and inspects the rank of the matrix (or the rank of its minors)). Where can i find the precise statement and its proof?,,"['abstract-algebra', 'reference-request', 'polynomials', 'commutative-algebra']"
8,Is the sum of an algebraic and transcendental complex number transcendental?,Is the sum of an algebraic and transcendental complex number transcendental?,,"I was wondering if the sum of an algebraic and transcendental complex number is transcendental. I was thinking if a is algebraic, and b is transcendental, then if a+b is algebraic, then a+b-a is also algebraic since algebraic numbers are closed under additive inverses, but b is transcendental. Is this a correct approach?","I was wondering if the sum of an algebraic and transcendental complex number is transcendental. I was thinking if a is algebraic, and b is transcendental, then if a+b is algebraic, then a+b-a is also algebraic since algebraic numbers are closed under additive inverses, but b is transcendental. Is this a correct approach?",,"['abstract-algebra', 'field-theory', 'galois-theory', 'transcendental-numbers']"
9,Calculate the number of Sylow $p$-subgroups of $A_5$,Calculate the number of Sylow -subgroups of,p A_5,"Calculate the number of Sylow $p$-subgroups of $A_5$ We have $|G|=60=2^2\cdot 3\cdot 5$ Let $n_p$ be the number of Sylow $p$-subgroups of $G$. By Sylow's third theorem, we have $n_3\in\{1,4,10\}$. But $G$ contains $20$ elements of order $3$, each of which generates a Sylow $3$-subgroup, so $n_3=10$ Similarly $n_5\in\{1,6\}$ and contains $30$ elements each generating a Sylow $5$-subgroup so $n_5=6$ Consider $n_2$. By Sylow's third theorem, we have $n_2=\{1,3,5,15\}$ (1) Clearly, $n_2\in\{5,15\}$ since $G$ contains $15$ elements of order $2$ (2) If $n_2=15$ then $|G:N_G(H)|=15$ thus $H=N_G(H)$ (3) but this is false since $(1,2,3)\in N_G(H)\backslash H$ Therefore $n_2=5$ Can someone clarrify any of the three lines in bold (lines (1),(2),(3)) Line (1), why is $3$ excluded from the list for $n_2$ Line (2), $N_G(H)$ is the normalizer of $H$ in $G$ Please comment if you can clarify any of the lines, any help would be greatly appreciated.","Calculate the number of Sylow $p$-subgroups of $A_5$ We have $|G|=60=2^2\cdot 3\cdot 5$ Let $n_p$ be the number of Sylow $p$-subgroups of $G$. By Sylow's third theorem, we have $n_3\in\{1,4,10\}$. But $G$ contains $20$ elements of order $3$, each of which generates a Sylow $3$-subgroup, so $n_3=10$ Similarly $n_5\in\{1,6\}$ and contains $30$ elements each generating a Sylow $5$-subgroup so $n_5=6$ Consider $n_2$. By Sylow's third theorem, we have $n_2=\{1,3,5,15\}$ (1) Clearly, $n_2\in\{5,15\}$ since $G$ contains $15$ elements of order $2$ (2) If $n_2=15$ then $|G:N_G(H)|=15$ thus $H=N_G(H)$ (3) but this is false since $(1,2,3)\in N_G(H)\backslash H$ Therefore $n_2=5$ Can someone clarrify any of the three lines in bold (lines (1),(2),(3)) Line (1), why is $3$ excluded from the list for $n_2$ Line (2), $N_G(H)$ is the normalizer of $H$ in $G$ Please comment if you can clarify any of the lines, any help would be greatly appreciated.",,"['abstract-algebra', 'group-theory', 'finite-groups', 'symmetric-groups']"
10,Checking if $\langle 2 \rangle$ is a maximal ideal in $\mathbb{Z}[i]$,Checking if  is a maximal ideal in,\langle 2 \rangle \mathbb{Z}[i],"Is $\langle 2 \rangle$ a maximal ideal in $\mathbb Z[i]$? Solution: We know that $\mathbb Z[i]$ is an Euclidean domain and hence a principal ideal domain. Consider $2 \in \mathbb Z[i]$. Then $N(2) = 2^2 = 4$. (NOTE: $N$ is norm). Since $N(2)$ is not prime, this implies that $2$ is reducible in $\mathbb Z[i]$. And this implies that $\langle 2 \rangle$ is not maximal. I want to know is there any mistake in this? Thank you.","Is $\langle 2 \rangle$ a maximal ideal in $\mathbb Z[i]$? Solution: We know that $\mathbb Z[i]$ is an Euclidean domain and hence a principal ideal domain. Consider $2 \in \mathbb Z[i]$. Then $N(2) = 2^2 = 4$. (NOTE: $N$ is norm). Since $N(2)$ is not prime, this implies that $2$ is reducible in $\mathbb Z[i]$. And this implies that $\langle 2 \rangle$ is not maximal. I want to know is there any mistake in this? Thank you.",,"['abstract-algebra', 'proof-verification', 'ring-theory', 'ideals']"
11,Checking irreducibility,Checking irreducibility,,"I have the polynomial $f(X)=X^{2n}-2X^{n}+1-p$ where $p$ is a prime number and $n\in\mathbb{N}$. I want to check whether it is irreducible or not over $\mathbb{Q}[X]$. If $2^{2}\nmid1-p$ then $f(X)$ is irreducible by Eisenstein's Criterion. However, I can't make any progress when I consider the polynomial $f(X)=X^{2n}-2X^{n}+4r, r\in\mathbb{Z}$. Any hints?","I have the polynomial $f(X)=X^{2n}-2X^{n}+1-p$ where $p$ is a prime number and $n\in\mathbb{N}$. I want to check whether it is irreducible or not over $\mathbb{Q}[X]$. If $2^{2}\nmid1-p$ then $f(X)$ is irreducible by Eisenstein's Criterion. However, I can't make any progress when I consider the polynomial $f(X)=X^{2n}-2X^{n}+4r, r\in\mathbb{Z}$. Any hints?",,"['abstract-algebra', 'polynomials', 'galois-theory', 'irreducible-polynomials']"
12,Set of elements of degree $2^n$ over a base field is itself a field,Set of elements of degree  over a base field is itself a field,2^n,"Let $F \subset L$ be two fields, and define $K = \{\alpha \in L\mid [F(\alpha): F] \text{ is a power of 2} \}$.  Our problem is to prove that $K$ is a field. Closure under reciprocation is easy (because $F(\alpha) = F(\alpha^{-1})$).  We ran into trouble proving closure under addition and multiplication.  Our initial attempt was to prove that for any $\alpha, \beta \in L$, $[F(\alpha, \beta): F]$ divides $[F(\alpha): F] [F(\beta): F]$, but this relies on a result that $[F(\alpha, \beta): F(\alpha)] = \deg_{F(\alpha)} (\beta)$ divides $[F(\beta), F] = \deg_F (\beta)$, which (as far as we can tell) is not necessarily true. EDIT: As pointed out below, the problem is unsolvable; the professor who assigned it agrees.","Let $F \subset L$ be two fields, and define $K = \{\alpha \in L\mid [F(\alpha): F] \text{ is a power of 2} \}$.  Our problem is to prove that $K$ is a field. Closure under reciprocation is easy (because $F(\alpha) = F(\alpha^{-1})$).  We ran into trouble proving closure under addition and multiplication.  Our initial attempt was to prove that for any $\alpha, \beta \in L$, $[F(\alpha, \beta): F]$ divides $[F(\alpha): F] [F(\beta): F]$, but this relies on a result that $[F(\alpha, \beta): F(\alpha)] = \deg_{F(\alpha)} (\beta)$ divides $[F(\beta), F] = \deg_F (\beta)$, which (as far as we can tell) is not necessarily true. EDIT: As pointed out below, the problem is unsolvable; the professor who assigned it agrees.",,"['abstract-algebra', 'field-theory', 'extension-field']"
13,Extending Herstein's Challenging Exercise to Modules,Extending Herstein's Challenging Exercise to Modules,,"Anybody who has worked through Herstein's Topics in Algebra might remember Exercise 26 of Section 2.5 (in second edition): If $G$ is an abelian group containing subgroups of order $m$ and $n$, then $G$ contains a subgroup whose order is the least common multiple of $m$ and $n$. Robert Beals has given a very nice solution in [1]. My question is whether or not we can extend the result of this exercise to modules (and if we can, to which extent). More concretely, Suppose $R$ is a commutative ring. If $M$ is an $R$-module that contains submodules of cardinality $m$ and   $n$, then must $M$ necessarily contain a submodule whose cardinality is the least   common multiple of $m$ and $n$? Since abelian groups can be viewed as $\mathbb{Z}$-modules, this generalizes the above situation. Now, I am guessing this fails for arbitrary rings. I would be especially interested to know the conclusion if $R$ is Noetherian. (I think if $R$ is a PID, the conclusion follows, but I have no proof for this. Beals' proof uses purely group-theoretic concepts some of which I cannot generalize to modules). Thanks for your time :) [1] Beals, Robert. ""On Orders of Subgroups in Abelian Groups: An Elementary Solution of an Exercise of Herstein."" The American Mathematical Monthly, Vol. 116, No. 10 (Dec., 2009), pp. 923-926","Anybody who has worked through Herstein's Topics in Algebra might remember Exercise 26 of Section 2.5 (in second edition): If $G$ is an abelian group containing subgroups of order $m$ and $n$, then $G$ contains a subgroup whose order is the least common multiple of $m$ and $n$. Robert Beals has given a very nice solution in [1]. My question is whether or not we can extend the result of this exercise to modules (and if we can, to which extent). More concretely, Suppose $R$ is a commutative ring. If $M$ is an $R$-module that contains submodules of cardinality $m$ and   $n$, then must $M$ necessarily contain a submodule whose cardinality is the least   common multiple of $m$ and $n$? Since abelian groups can be viewed as $\mathbb{Z}$-modules, this generalizes the above situation. Now, I am guessing this fails for arbitrary rings. I would be especially interested to know the conclusion if $R$ is Noetherian. (I think if $R$ is a PID, the conclusion follows, but I have no proof for this. Beals' proof uses purely group-theoretic concepts some of which I cannot generalize to modules). Thanks for your time :) [1] Beals, Robert. ""On Orders of Subgroups in Abelian Groups: An Elementary Solution of an Exercise of Herstein."" The American Mathematical Monthly, Vol. 116, No. 10 (Dec., 2009), pp. 923-926",,"['abstract-algebra', 'group-theory', 'commutative-algebra', 'examples-counterexamples']"
14,Establishing equality between two functions $\mathbb{Z}_5[x]$; $a(x) = x^5 + 1$ and $b(x) = x-4$,Establishing equality between two functions ;  and,\mathbb{Z}_5[x] a(x) = x^5 + 1 b(x) = x-4,"Consider the following two polynomials in $\mathbb{Z}_5[x]$ :* $$ a(x) = x^5 + 1 $$ $$ b(x) = x - 4 $$ You may check that $a(0) = b(0), a(1) = b(1), \ldots, a(4) = b(4)$ , hence $a(x)$ and $b(x)$ are equal functions from $\mathbb{Z}_5$ to $\mathbb{Z}_5$ . Why and how $a(0)=b(0)$ for the above two functions?","Consider the following two polynomials in :* You may check that , hence and are equal functions from to . Why and how for the above two functions?","\mathbb{Z}_5[x] 
a(x) = x^5 + 1
 
b(x) = x - 4
 a(0) = b(0), a(1) = b(1), \ldots, a(4) = b(4) a(x) b(x) \mathbb{Z}_5 \mathbb{Z}_5 a(0)=b(0)",['abstract-algebra']
15,Toric Varieties: gluing of affine varieties (blow-up example),Toric Varieties: gluing of affine varieties (blow-up example),,"Let $\Delta$ be a fan, consisting of cones $\sigma_0=conv(e_1,e_1+e_2)$ and $\sigma_2=conv(e_1+e_2,e_2)$ and $\tau=\sigma_0\cap\sigma_1=conv(e_1+e_2)$. The dual cones are $\sigma_0^\vee= conv(e_2,e_1\!-\!e_2)$ and $\sigma_1^\vee= conv(e_2\!-\!e_1,e_1)$ and $\tau^\vee= conv(e_2\!-\!e_1,e_1\!-\!e_2,e_1,e_2)= conv(e_2\!-\!e_1,e_1\!-\!e_2,e_1)= conv(e_2\!-\!e_1,e_1\!-\!e_2,e_2)$. The corresponding semigroup algebras are $S_{\sigma_0}= \mathbb{C}[\mathbf{x}^{e_2},\mathbf{x}^{e_1\!-\!e_2}] =\mathbb{C}[y,xy^{-1}]$ and $S_{\sigma_1}= \mathbb{C}[\mathbf{x}^{e_2\!-\!e_1},\mathbf{x}^{e_1}]= \mathbb{C}[x^{-1}y,x]$ and $S_\tau= \mathbb{C}[\mathbf{x}^{e_2\!-\!e_1}, \mathbf{x}^{e_1\!-\!e_2}, \mathbf{x}^{e_1},\mathbf{x}^{e_2}]$ $=$ $\mathbb{C}[x^{-1}y,y^{-1}x,x,y]$ $=$ $\mathbb{C}[x^{-1}y,y^{-1}x,x]= \mathbb{C}[x^{-1}y,y^{-1}x,y]$ inside $\mathbb{C}[x^{\pm1},y^{\pm1}]$. The three associated affine schemes $U_{\sigma_0}, U_{\sigma_1}, U_\tau$ are their spectrums. The homomorphism of $\mathbb{C}$-algebras $\beta_0\!: \mathbb{C}[X,Y]\rightarrow\mathbb{C}[x/y,y]$ that sends $X\!\mapsto\!x/y,Y\!\mapsto\!y$ is an isomorphism. The homomorphism of $\mathbb{C}$-algebras $\beta_1\!: \mathbb{C}[X,Y]\rightarrow\mathbb{C}[x,y/x]$ that sends $X\!\mapsto\!x,Y\!\mapsto\!y/x$ is an isomorphism. Thus $U_{\sigma_0}\!=\!\mathbb{C}^2\!=\!U_{\sigma_1}$ via $\overset{\beta_0^\ast}{\longleftarrow}\!\ldots\!\overset{\beta_1^\ast}{\longrightarrow}$. Furthermore, $\mathbb{C}[X,x]_X\!=\!\mathbb{C}[X,x,X^{-1}]\!\cong\!\mathbb{C}[x/y,x,y/x] \!=\! \mathbb{C}[x/y,y,y/x]\!\cong\!\mathbb{C}[Y^{-1}\!,y,Y]\!=\!\mathbb{C}[y,Y]_Y$, so $\mathbb{C}^\ast\!\!\times\!\mathbb{C} \!=\!U_\tau\!=\! \mathbb{C}\!\times\!\mathbb{C}^\ast$ and $X\!=\!Y^{-1}$. We have inclusions $\sigma_0\!\hookleftarrow\!\tau\!\hookrightarrow\!\sigma_1$, hence inclusions $\sigma_0^\vee\!\hookrightarrow\!\tau^\vee\!\hookleftarrow\!\sigma_1^\vee$, thus morphisms $\mathbb{C}[\sigma_0^\vee]\!\overset{\iota_0}{\hookrightarrow}\!\mathbb{C}[\tau^\vee]\!\overset{\iota_1}{\hookleftarrow}\!\mathbb{C}[\sigma_1^\vee]$, hence morphisms $Spec\,\mathbb{C}[\sigma_0^\vee]\!\overset{\iota_0^\ast}{\leftarrow}\!Spec\,\mathbb{C}[\tau^\vee]\!\overset{\iota_1^\ast}{\rightarrow}\!Spec\,\mathbb{C}[\sigma_1^\vee]$. Our morphisms $\mathbb{C}[x/y,y]\!\overset{\iota_0}{\hookrightarrow}\!\mathbb{C}[x/y,x,y/x]\!=\!\mathbb{C}[x/y,y,y/x]\!\overset{\iota_1}{\hookleftarrow}\!\mathbb{C}[x,y/x]$ send $(x/y,y)\!\rightarrow\!(x/y,y\!=\!x\frac{y}{x})$ and $(y\frac{x}{y}\!=\!x,y/x)\!\leftarrow\!(x,y/x)$, i.e. morphisms $\mathbb{C}[X,Y]\!\overset{\iota_0}{\hookrightarrow}\!\mathbb{C}[X,x,X^{-1}]\!=\!\mathbb{C}[Y^{-1},y,Y]\!\overset{\iota_1}{\hookleftarrow}\!\mathbb{C}[X,Y]$ send $(X,Y)\!\rightarrow\!(X,x/X)$ and $(y/Y,Y)\!\leftarrow\!(X,Y)$. On ideals, they map $\langle X\!-\!u,Y\!-\!v\rangle \!\overset{\iota_0}{\rightarrow}\! \langle X\!-\!u,x/X\!-\!v\rangle \!=\! \langle X\!-\!u,x\!-\!vX\rangle$ and $\langle y\!-\!uY,Y\!-\!v\rangle \!=\! \langle y/Y\!-\!u,Y\!-\!v\rangle\!\overset{\iota_0}{\leftarrow}\!\langle X\!-\!u,Y\!-\!v\rangle$, so preimage morphisms $\mathbb{C}^2\!\overset{\iota_0^\ast}{\leftarrow}\!\mathbb{C}^\ast\!\!\times\!\mathbb{C}\!=\!\mathbb{C}\!\times\!\mathbb{C}^\ast\!\overset{\iota_1^\ast}{\rightarrow}\!\mathbb{C}^2$ send $\langle X\!-\!u,Y\!-\!???\rangle\!\leftarrow\!\langle X\!-\!u,x\!-\!v\rangle$ and $\langle y\!-\!u,Y\!-\!v\rangle\!\rightarrow\!\langle X\!-\!???,Y\!-\!v\rangle$, i.e. $(u,???)\!\leftarrow\!(u,v)$ and $(u,v)\!\rightarrow\!(???,v)$. Question 1: Is everything so far correct? Question 2: The usual identification is that $Max\,\mathbb{C}[x,y]=\{\langle x\!-\!u,y\!-\!v\rangle;\, u,v\!\in\!\mathbb{C}\}\equiv\mathbb{C}^2$. However, in the literature, $Spec\,\mathbb{C}[x,y]$ is identified with $\mathbb{C}^2$ (I'm not sure this is the right thing to do). I would like to glue together $U_{\sigma_0}$ and $U_{\sigma_1}$ via $U_\tau$ to obtain a scheme. How can I obtain a gluing isomorphism? The problem is that for $\iota_0,\iota_1$, the preimage of a maximal ideal is not a maximal ideal. How does this gluing morphism map points $\mathbb{C}^\ast\!\!\times\!\mathbb{C}\rightarrow\mathbb{C}\!\times\!\mathbb{C}^\ast$? I'd like to see that is sends $(u,v)\mapsto(uv,u^{-1})$. Are we gluing two copies of $\mathbb{C}^2$ along $\mathbb{C}\!\times\!\mathbb{C}^\ast$ or along $\mathbb{C}^\ast\!\times\!\mathbb{C}^\ast$? How are $\mathbb{C}^\ast\!\!\times\!\mathbb{C}$ and $\mathbb{C}\!\times\!\mathbb{C}^\ast$ via $\iota_0^\ast$ and $\iota_1^\ast$ embedded in $\mathbb{C}^2$?","Let $\Delta$ be a fan, consisting of cones $\sigma_0=conv(e_1,e_1+e_2)$ and $\sigma_2=conv(e_1+e_2,e_2)$ and $\tau=\sigma_0\cap\sigma_1=conv(e_1+e_2)$. The dual cones are $\sigma_0^\vee= conv(e_2,e_1\!-\!e_2)$ and $\sigma_1^\vee= conv(e_2\!-\!e_1,e_1)$ and $\tau^\vee= conv(e_2\!-\!e_1,e_1\!-\!e_2,e_1,e_2)= conv(e_2\!-\!e_1,e_1\!-\!e_2,e_1)= conv(e_2\!-\!e_1,e_1\!-\!e_2,e_2)$. The corresponding semigroup algebras are $S_{\sigma_0}= \mathbb{C}[\mathbf{x}^{e_2},\mathbf{x}^{e_1\!-\!e_2}] =\mathbb{C}[y,xy^{-1}]$ and $S_{\sigma_1}= \mathbb{C}[\mathbf{x}^{e_2\!-\!e_1},\mathbf{x}^{e_1}]= \mathbb{C}[x^{-1}y,x]$ and $S_\tau= \mathbb{C}[\mathbf{x}^{e_2\!-\!e_1}, \mathbf{x}^{e_1\!-\!e_2}, \mathbf{x}^{e_1},\mathbf{x}^{e_2}]$ $=$ $\mathbb{C}[x^{-1}y,y^{-1}x,x,y]$ $=$ $\mathbb{C}[x^{-1}y,y^{-1}x,x]= \mathbb{C}[x^{-1}y,y^{-1}x,y]$ inside $\mathbb{C}[x^{\pm1},y^{\pm1}]$. The three associated affine schemes $U_{\sigma_0}, U_{\sigma_1}, U_\tau$ are their spectrums. The homomorphism of $\mathbb{C}$-algebras $\beta_0\!: \mathbb{C}[X,Y]\rightarrow\mathbb{C}[x/y,y]$ that sends $X\!\mapsto\!x/y,Y\!\mapsto\!y$ is an isomorphism. The homomorphism of $\mathbb{C}$-algebras $\beta_1\!: \mathbb{C}[X,Y]\rightarrow\mathbb{C}[x,y/x]$ that sends $X\!\mapsto\!x,Y\!\mapsto\!y/x$ is an isomorphism. Thus $U_{\sigma_0}\!=\!\mathbb{C}^2\!=\!U_{\sigma_1}$ via $\overset{\beta_0^\ast}{\longleftarrow}\!\ldots\!\overset{\beta_1^\ast}{\longrightarrow}$. Furthermore, $\mathbb{C}[X,x]_X\!=\!\mathbb{C}[X,x,X^{-1}]\!\cong\!\mathbb{C}[x/y,x,y/x] \!=\! \mathbb{C}[x/y,y,y/x]\!\cong\!\mathbb{C}[Y^{-1}\!,y,Y]\!=\!\mathbb{C}[y,Y]_Y$, so $\mathbb{C}^\ast\!\!\times\!\mathbb{C} \!=\!U_\tau\!=\! \mathbb{C}\!\times\!\mathbb{C}^\ast$ and $X\!=\!Y^{-1}$. We have inclusions $\sigma_0\!\hookleftarrow\!\tau\!\hookrightarrow\!\sigma_1$, hence inclusions $\sigma_0^\vee\!\hookrightarrow\!\tau^\vee\!\hookleftarrow\!\sigma_1^\vee$, thus morphisms $\mathbb{C}[\sigma_0^\vee]\!\overset{\iota_0}{\hookrightarrow}\!\mathbb{C}[\tau^\vee]\!\overset{\iota_1}{\hookleftarrow}\!\mathbb{C}[\sigma_1^\vee]$, hence morphisms $Spec\,\mathbb{C}[\sigma_0^\vee]\!\overset{\iota_0^\ast}{\leftarrow}\!Spec\,\mathbb{C}[\tau^\vee]\!\overset{\iota_1^\ast}{\rightarrow}\!Spec\,\mathbb{C}[\sigma_1^\vee]$. Our morphisms $\mathbb{C}[x/y,y]\!\overset{\iota_0}{\hookrightarrow}\!\mathbb{C}[x/y,x,y/x]\!=\!\mathbb{C}[x/y,y,y/x]\!\overset{\iota_1}{\hookleftarrow}\!\mathbb{C}[x,y/x]$ send $(x/y,y)\!\rightarrow\!(x/y,y\!=\!x\frac{y}{x})$ and $(y\frac{x}{y}\!=\!x,y/x)\!\leftarrow\!(x,y/x)$, i.e. morphisms $\mathbb{C}[X,Y]\!\overset{\iota_0}{\hookrightarrow}\!\mathbb{C}[X,x,X^{-1}]\!=\!\mathbb{C}[Y^{-1},y,Y]\!\overset{\iota_1}{\hookleftarrow}\!\mathbb{C}[X,Y]$ send $(X,Y)\!\rightarrow\!(X,x/X)$ and $(y/Y,Y)\!\leftarrow\!(X,Y)$. On ideals, they map $\langle X\!-\!u,Y\!-\!v\rangle \!\overset{\iota_0}{\rightarrow}\! \langle X\!-\!u,x/X\!-\!v\rangle \!=\! \langle X\!-\!u,x\!-\!vX\rangle$ and $\langle y\!-\!uY,Y\!-\!v\rangle \!=\! \langle y/Y\!-\!u,Y\!-\!v\rangle\!\overset{\iota_0}{\leftarrow}\!\langle X\!-\!u,Y\!-\!v\rangle$, so preimage morphisms $\mathbb{C}^2\!\overset{\iota_0^\ast}{\leftarrow}\!\mathbb{C}^\ast\!\!\times\!\mathbb{C}\!=\!\mathbb{C}\!\times\!\mathbb{C}^\ast\!\overset{\iota_1^\ast}{\rightarrow}\!\mathbb{C}^2$ send $\langle X\!-\!u,Y\!-\!???\rangle\!\leftarrow\!\langle X\!-\!u,x\!-\!v\rangle$ and $\langle y\!-\!u,Y\!-\!v\rangle\!\rightarrow\!\langle X\!-\!???,Y\!-\!v\rangle$, i.e. $(u,???)\!\leftarrow\!(u,v)$ and $(u,v)\!\rightarrow\!(???,v)$. Question 1: Is everything so far correct? Question 2: The usual identification is that $Max\,\mathbb{C}[x,y]=\{\langle x\!-\!u,y\!-\!v\rangle;\, u,v\!\in\!\mathbb{C}\}\equiv\mathbb{C}^2$. However, in the literature, $Spec\,\mathbb{C}[x,y]$ is identified with $\mathbb{C}^2$ (I'm not sure this is the right thing to do). I would like to glue together $U_{\sigma_0}$ and $U_{\sigma_1}$ via $U_\tau$ to obtain a scheme. How can I obtain a gluing isomorphism? The problem is that for $\iota_0,\iota_1$, the preimage of a maximal ideal is not a maximal ideal. How does this gluing morphism map points $\mathbb{C}^\ast\!\!\times\!\mathbb{C}\rightarrow\mathbb{C}\!\times\!\mathbb{C}^\ast$? I'd like to see that is sends $(u,v)\mapsto(uv,u^{-1})$. Are we gluing two copies of $\mathbb{C}^2$ along $\mathbb{C}\!\times\!\mathbb{C}^\ast$ or along $\mathbb{C}^\ast\!\times\!\mathbb{C}^\ast$? How are $\mathbb{C}^\ast\!\!\times\!\mathbb{C}$ and $\mathbb{C}\!\times\!\mathbb{C}^\ast$ via $\iota_0^\ast$ and $\iota_1^\ast$ embedded in $\mathbb{C}^2$?",,"['abstract-algebra', 'algebraic-geometry', 'schemes', 'simplicial-stuff', 'toric-geometry']"
16,Semiring between measure theory and abstract algebra,Semiring between measure theory and abstract algebra,,What is the relation between semirings in measure theory and semirings in abstract algebra? Why are they called the same? You can see : http://en.wikipedia.org/wiki/Semiring,What is the relation between semirings in measure theory and semirings in abstract algebra? Why are they called the same? You can see : http://en.wikipedia.org/wiki/Semiring,,"['abstract-algebra', 'measure-theory', 'terminology']"
17,Galois Groups of Finite Extensions of Fixed Fields,Galois Groups of Finite Extensions of Fixed Fields,,"I am trying to prove the following proposition: Let $L$ be an algebraically closed field, $g \in Aut(L)$ and $K=\{x \in L \; | \; g(x)=x\}$. Show that every finite extensions $E/K$ is a cyclic Galois extension. So far my thoughts have led me to the following: If $E=K$, then clearly $E/K$ is Galois and $Gal(E/K)=Aut(K/K)=i$, where $i$ is the identity automorphism, so it is trivially cyclic. Assume $[E:K]=n$ for some $n>1$. Since $E/K$ is a finite extension, it is algebraic. Note that if $\beta \in E\setminus F$ then we have the minimal polynomial $m_{\beta}(X)$ over $K[x]$. Since $K$ is fixed by $g$, we know that $g(\beta)$ is also a root of $m_{\beta}(X)$. We can also conclude that $g(\beta)\not \in K$ since $\beta \not \in K$. Where to take any of these observations, I have yet to determine. I feel I must be missing something elementary, for I cannot see how to conclude this is a Galois extension from here. Any tips would be greatly appreciated, especially over full solutions.","I am trying to prove the following proposition: Let $L$ be an algebraically closed field, $g \in Aut(L)$ and $K=\{x \in L \; | \; g(x)=x\}$. Show that every finite extensions $E/K$ is a cyclic Galois extension. So far my thoughts have led me to the following: If $E=K$, then clearly $E/K$ is Galois and $Gal(E/K)=Aut(K/K)=i$, where $i$ is the identity automorphism, so it is trivially cyclic. Assume $[E:K]=n$ for some $n>1$. Since $E/K$ is a finite extension, it is algebraic. Note that if $\beta \in E\setminus F$ then we have the minimal polynomial $m_{\beta}(X)$ over $K[x]$. Since $K$ is fixed by $g$, we know that $g(\beta)$ is also a root of $m_{\beta}(X)$. We can also conclude that $g(\beta)\not \in K$ since $\beta \not \in K$. Where to take any of these observations, I have yet to determine. I feel I must be missing something elementary, for I cannot see how to conclude this is a Galois extension from here. Any tips would be greatly appreciated, especially over full solutions.",,"['abstract-algebra', 'field-theory', 'galois-theory', 'galois-extensions']"
18,Ring homomorphisms which yield same endomorphisms on modules,Ring homomorphisms which yield same endomorphisms on modules,,"Let $\varphi: R \rightarrow S$ be a (unital) ring homomorphism. So every left $S$-module $M$ has also a left $R$-module structure via $\varphi$ and in general we have $$ \text{End}_S(M) \subseteq \text{End}_R(M)$$ My question is: Is there a necessary and sufficient condition on $\varphi$ such that the above inclusion becomes an equality for every $M$? Note that $\varphi$ being surjective is sufficient but not necessary as $\mathbb{Z} \hookrightarrow \mathbb{Q}$ also has the desired property. This led me to believe $\varphi$ being an epimorphism in the category $\mathsf{Ring}$ is the condition I want but I couldn't show that or find a counterexample. Edit: We can generalize the situation with $\mathbb{Z} \hookrightarrow \mathbb{Q}$: If $R$ is commutative and $D$ is a multiplicatively closed subset of $R$, the localization $R \rightarrow D^{-1}R$ has the desired property. Moreover ring homomorphisms with this ""equal endomorphisms"" property are closed under composition. So any composition of surjections and localizations also works. Unfortunately there are commutative ring epimorphisms which cannot be factored to surjections and localizations, as can be seen here . (Some time I will try to work out some of the examples given there to see whether they fail to equalize the endomorphisms or not) Edit: In general a necessary condition is that the centralizer of the image $\varphi(R)$ in $S$ should be equal to the center of $S$: We always have $\mathbf{Z}(S) \subseteq \mathbf{C}_{S}(\varphi (R))$. For the reverse inclusion, let $u \in \mathbf{C}_S(\varphi(R))$. Then letting M be the left regular $S$-module $_S S$, the map $\alpha: S \rightarrow S$ given by $\alpha(s) = us$ lies in $\text{End}_R(M)$. Now by assumption $\alpha$ is also an $S$-endomorphism. We know that $S$-endomorphisms of $M$ are right multiplications by elements of $S$. Say $\alpha$ is given by right multiplication by $t \in S$. Then evaluating at $1$, we get $u = t$. Thus left and right multiplications by $u$ coincide, that is $u \in \mathbf{Z}(S)$. Note that by above, when $R$ is commutative we get $$\varphi(R) \subseteq \mathbf{C}_S(\varphi(R)) = \mathbf{Z}(S)$$ This means $S$ is an $R$-algebra via $\varphi$. So in this case the question can be rephrased as ""Let $k$ be a commutative ring and $S$ be a $k$-algebra. When can we say that $k$-linear endomorphisms of $S$-modules are always $S$-linear?"" Maybe this has an easy answer when $k$ is a field. A relevant (perhaps more natural) question to mine is when all homomorphisms are the same, not just endomorphisms. That is, can we find a (nice) condition on $\varphi$ such that for every pair of $S$-modules $M$ and $N$ the inclusion $$\text{Hom}_S(M,N) \subseteq \text{Hom}_R(M,N)$$ becomes an equality?","Let $\varphi: R \rightarrow S$ be a (unital) ring homomorphism. So every left $S$-module $M$ has also a left $R$-module structure via $\varphi$ and in general we have $$ \text{End}_S(M) \subseteq \text{End}_R(M)$$ My question is: Is there a necessary and sufficient condition on $\varphi$ such that the above inclusion becomes an equality for every $M$? Note that $\varphi$ being surjective is sufficient but not necessary as $\mathbb{Z} \hookrightarrow \mathbb{Q}$ also has the desired property. This led me to believe $\varphi$ being an epimorphism in the category $\mathsf{Ring}$ is the condition I want but I couldn't show that or find a counterexample. Edit: We can generalize the situation with $\mathbb{Z} \hookrightarrow \mathbb{Q}$: If $R$ is commutative and $D$ is a multiplicatively closed subset of $R$, the localization $R \rightarrow D^{-1}R$ has the desired property. Moreover ring homomorphisms with this ""equal endomorphisms"" property are closed under composition. So any composition of surjections and localizations also works. Unfortunately there are commutative ring epimorphisms which cannot be factored to surjections and localizations, as can be seen here . (Some time I will try to work out some of the examples given there to see whether they fail to equalize the endomorphisms or not) Edit: In general a necessary condition is that the centralizer of the image $\varphi(R)$ in $S$ should be equal to the center of $S$: We always have $\mathbf{Z}(S) \subseteq \mathbf{C}_{S}(\varphi (R))$. For the reverse inclusion, let $u \in \mathbf{C}_S(\varphi(R))$. Then letting M be the left regular $S$-module $_S S$, the map $\alpha: S \rightarrow S$ given by $\alpha(s) = us$ lies in $\text{End}_R(M)$. Now by assumption $\alpha$ is also an $S$-endomorphism. We know that $S$-endomorphisms of $M$ are right multiplications by elements of $S$. Say $\alpha$ is given by right multiplication by $t \in S$. Then evaluating at $1$, we get $u = t$. Thus left and right multiplications by $u$ coincide, that is $u \in \mathbf{Z}(S)$. Note that by above, when $R$ is commutative we get $$\varphi(R) \subseteq \mathbf{C}_S(\varphi(R)) = \mathbf{Z}(S)$$ This means $S$ is an $R$-algebra via $\varphi$. So in this case the question can be rephrased as ""Let $k$ be a commutative ring and $S$ be a $k$-algebra. When can we say that $k$-linear endomorphisms of $S$-modules are always $S$-linear?"" Maybe this has an easy answer when $k$ is a field. A relevant (perhaps more natural) question to mine is when all homomorphisms are the same, not just endomorphisms. That is, can we find a (nice) condition on $\varphi$ such that for every pair of $S$-modules $M$ and $N$ the inclusion $$\text{Hom}_S(M,N) \subseteq \text{Hom}_R(M,N)$$ becomes an equality?",,"['abstract-algebra', 'ring-theory', 'modules']"
19,Why are inverse images much better behaved than forward images?,Why are inverse images much better behaved than forward images?,,"If $f:X\to Y$ is a function, and $A\subseteq X$ , then we define the (forward) image of $A$ under $f$ as the set $f(A)=\{y\in B:(\exists x\in A)(y=f(x))\}$ ; similarly, if $B\subseteq Y$ , then the inverse image of $B$ under $f$ is defined as $f^{-1}(B)=\{x\in A:f(x)\in B\}$ . Despite the apparent duality between these two concepts, it is well known that inverse images are in general much better behaved than forward images. While unions are preserved under images (i.e. $f\left(\bigcup_i A_i\right)=\bigcup_if(A_i)$ ), the corresponding equation for intersections is false, and taking the forward image does not interact well with taking complements. By contrast, pretty much everything that you would hope that inverse images preserve is preserved. (This phenomenon is not limited to set theory. For instance, if $\phi :G\to H$ is a group homomorphism, and $N$ is a normal subgroup of $H$ , then $\phi^{-1}(N)$ is a normal subgroup of $G$ . Again, the corresponding statement for forward images is false.) Is there a deeper explanation, say using category theory, for why inverse images are in general much better behaved than forward images?","If is a function, and , then we define the (forward) image of under as the set ; similarly, if , then the inverse image of under is defined as . Despite the apparent duality between these two concepts, it is well known that inverse images are in general much better behaved than forward images. While unions are preserved under images (i.e. ), the corresponding equation for intersections is false, and taking the forward image does not interact well with taking complements. By contrast, pretty much everything that you would hope that inverse images preserve is preserved. (This phenomenon is not limited to set theory. For instance, if is a group homomorphism, and is a normal subgroup of , then is a normal subgroup of . Again, the corresponding statement for forward images is false.) Is there a deeper explanation, say using category theory, for why inverse images are in general much better behaved than forward images?",f:X\to Y A\subseteq X A f f(A)=\{y\in B:(\exists x\in A)(y=f(x))\} B\subseteq Y B f f^{-1}(B)=\{x\in A:f(x)\in B\} f\left(\bigcup_i A_i\right)=\bigcup_if(A_i) \phi :G\to H N H \phi^{-1}(N) G,"['abstract-algebra', 'elementary-set-theory', 'category-theory']"
20,Galois group of $x^5-x-1$ over $\Bbb Q$,Galois group of  over,x^5-x-1 \Bbb Q,"I am trying to compute the Galois group of $x^5-x-1$ over $ \Bbb Q$ . I've shown that this polynomial is irreducible over $\Bbb Q$ , by showing that it is irreducible over $\Bbb Z_5$ . Let $F$ be the splitting field of $x^5-x-1$ over $\Bbb Q$ . This polynomial has $1$ real root and $4$ complex (non-real) roots. If $\alpha \in F$ is the real root of $x^5-x-1$ , then $[\Bbb Q(\alpha):\Bbb Q]=5$ , and $\Bbb Q(\alpha)\subset \Bbb R$ . Since $F \not\subset \Bbb R$ , from this we conclude that $[F:\Bbb Q]$ is strictly bigger than $5$ , and that the Galois group $G$ has a subgroup of order $5$ , i.e., contains a $5$ -cycle. But I got stuck here. Any hints?","I am trying to compute the Galois group of over . I've shown that this polynomial is irreducible over , by showing that it is irreducible over . Let be the splitting field of over . This polynomial has real root and complex (non-real) roots. If is the real root of , then , and . Since , from this we conclude that is strictly bigger than , and that the Galois group has a subgroup of order , i.e., contains a -cycle. But I got stuck here. Any hints?","x^5-x-1 
\Bbb Q \Bbb Q \Bbb Z_5 F x^5-x-1 \Bbb Q 1 4 \alpha \in F x^5-x-1 [\Bbb Q(\alpha):\Bbb Q]=5 \Bbb Q(\alpha)\subset \Bbb R F \not\subset \Bbb R [F:\Bbb Q] 5 G 5 5","['abstract-algebra', 'polynomials', 'field-theory', 'galois-theory', 'galois-extensions']"
21,Is this module free?,Is this module free?,,"Recently I was trying to amuse myself by inventing unusual examples of modules which are projective but not free. One of the candidates I came up with led to a question which I don't know how to answer. Consider the following: Let $R$ be the ring of continuous real-valued functions defined on $[-1,1]$. Let $E\subseteq R$ be the set of even functions and let $O\subseteq R$ be the set of odd functions. Since $E$ is a subring of $R$, we can view $R$ as an $E$-module. Unless I am mistaken, $O$ is an $E$-submodule of $R$ and $R$ is isomorphic to $E \oplus_E O$. I can show that $O$ is not a free $E$-module (details below), but I cannot show the same for $R$. So that's my question: is $R$ a free $E$-module? Here's my argument for why $O$ is not a free $E$-module: First of all, notice that no two $f,g\in O$ can be linearly independent over $E$. For if $f,g$ are odd, then $fg$ and $-f^2$ are even, and $(fg)f + (-f^2)g=0$. So if $O$ were free, there would exist a single $f\in O$ such that any $g\in O$ can be written uniquely as $g=hf$, where $h\in E$. In particular, if $g(x)=x$ for all $x$, we see that $f(x)\neq 0$ for $x\neq 0$. Since $f$ is odd, so is $f^{1/3}$. Therefore there exists a continuous even function $h$ such that $f^{1/3}=hf$. But then we have $h(x)=\frac{1}{f(x)^{2/3}}$ for $x\neq 0$. Since $f$ is odd, $f(0)=0$. This contradicts the fact that $h$ is continuous at $0$.","Recently I was trying to amuse myself by inventing unusual examples of modules which are projective but not free. One of the candidates I came up with led to a question which I don't know how to answer. Consider the following: Let $R$ be the ring of continuous real-valued functions defined on $[-1,1]$. Let $E\subseteq R$ be the set of even functions and let $O\subseteq R$ be the set of odd functions. Since $E$ is a subring of $R$, we can view $R$ as an $E$-module. Unless I am mistaken, $O$ is an $E$-submodule of $R$ and $R$ is isomorphic to $E \oplus_E O$. I can show that $O$ is not a free $E$-module (details below), but I cannot show the same for $R$. So that's my question: is $R$ a free $E$-module? Here's my argument for why $O$ is not a free $E$-module: First of all, notice that no two $f,g\in O$ can be linearly independent over $E$. For if $f,g$ are odd, then $fg$ and $-f^2$ are even, and $(fg)f + (-f^2)g=0$. So if $O$ were free, there would exist a single $f\in O$ such that any $g\in O$ can be written uniquely as $g=hf$, where $h\in E$. In particular, if $g(x)=x$ for all $x$, we see that $f(x)\neq 0$ for $x\neq 0$. Since $f$ is odd, so is $f^{1/3}$. Therefore there exists a continuous even function $h$ such that $f^{1/3}=hf$. But then we have $h(x)=\frac{1}{f(x)^{2/3}}$ for $x\neq 0$. Since $f$ is odd, $f(0)=0$. This contradicts the fact that $h$ is continuous at $0$.",,"['abstract-algebra', 'modules']"
22,How does restriction of scalars interact with tensor products?,How does restriction of scalars interact with tensor products?,,"Say that we have a morphism of commutative rings $f: R \to S$. Does the restriction of scalars functor $f^*: S \text{Mod} \to R \text{Mod}$ commute with tensor products? In other words, I would like to know if we have an isomorphism of $R$-modules: $$f^*(V \otimes_S V') \cong f^*V \otimes_R f^*V'.$$  If it does not hold in general, then under what conditions is it true? The reason I am wondering is that it seems that an $S$-bilinear map $\phi: V \times V' \to V''$ determines an $R$-bilinear map under restriction of scalars. I get the feeling that it is not quite right to say that $f^*$ is a monoidal functor, but I am not sure what the correct relationship to tensor products should be.","Say that we have a morphism of commutative rings $f: R \to S$. Does the restriction of scalars functor $f^*: S \text{Mod} \to R \text{Mod}$ commute with tensor products? In other words, I would like to know if we have an isomorphism of $R$-modules: $$f^*(V \otimes_S V') \cong f^*V \otimes_R f^*V'.$$  If it does not hold in general, then under what conditions is it true? The reason I am wondering is that it seems that an $S$-bilinear map $\phi: V \times V' \to V''$ determines an $R$-bilinear map under restriction of scalars. I get the feeling that it is not quite right to say that $f^*$ is a monoidal functor, but I am not sure what the correct relationship to tensor products should be.",,"['abstract-algebra', 'ring-theory', 'modules']"
23,Subfield of $\mathbb{Q}(\sqrt[n]{a})$ [duplicate],Subfield of  [duplicate],\mathbb{Q}(\sqrt[n]{a}),"This question already has answers here : Subfields of $\mathbb{Q}(\sqrt[n]a)$ (2 answers) Closed 4 months ago . Exercise 14.7.4 from Dummit and Foote Let $K=\mathbb{Q}(\sqrt[n]{a})$ , where $a\in \mathbb{Q}$ , $a>0$ and suppose $[K:\mathbb{Q}]=n$ (i.e., $x^n-a$ is irreducible over $\mathbb Q$ ). Let $E$ be any subfield of $K$ and let $[E:\mathbb{Q}]=d$ . Prove that $E=\mathbb{Q}(\sqrt[d]{a})$ . [Consider $ N_{K/E}(\sqrt[n]a)\in E$ ] I think I know how to solve it by considering $K=\mathbb{Q}(\sqrt[n]{a})$ as a subfield of $L=K(\sqrt[n]{a},\xi_n=e^{2\pi i/n})$ , splitting field of $x^n-a$ . $L$ over $\mathbb{Q}$ has Galois group $\left<\sigma,\tau\right>$ where $\sigma: \sqrt[n]{a}\rightarrow \sqrt[n]{a}\xi_n$ , $\xi_n\rightarrow\xi_n$ and $\tau:\xi_n\rightarrow\xi_n^{m}$ ( $\gcd(m,n)=1$ , $m\neq 1$ ), $\sqrt[n]{a}\rightarrow \sqrt[n]{a}$ . $K$ is the fixed field of $\left<\tau\right>$ , therefore a subfield of $K$ must be fixed by a group containing $\left<\tau\right>$ , and it's not hard to check that $\left<\tau,\sigma^{d}\right>$ fixes $K=\mathbb{Q}(\sqrt[d]{a})$ . However what I don't understand the given hint. Can anyone explain to me what that notation mean (I couldn't find it in the corresponding section in the book) and how does that (potentially) generate a easier solution of this problem?","This question already has answers here : Subfields of $\mathbb{Q}(\sqrt[n]a)$ (2 answers) Closed 4 months ago . Exercise 14.7.4 from Dummit and Foote Let , where , and suppose (i.e., is irreducible over ). Let be any subfield of and let . Prove that . [Consider ] I think I know how to solve it by considering as a subfield of , splitting field of . over has Galois group where , and ( , ), . is the fixed field of , therefore a subfield of must be fixed by a group containing , and it's not hard to check that fixes . However what I don't understand the given hint. Can anyone explain to me what that notation mean (I couldn't find it in the corresponding section in the book) and how does that (potentially) generate a easier solution of this problem?","K=\mathbb{Q}(\sqrt[n]{a}) a\in \mathbb{Q} a>0 [K:\mathbb{Q}]=n x^n-a \mathbb Q E K [E:\mathbb{Q}]=d E=\mathbb{Q}(\sqrt[d]{a})  N_{K/E}(\sqrt[n]a)\in E K=\mathbb{Q}(\sqrt[n]{a}) L=K(\sqrt[n]{a},\xi_n=e^{2\pi i/n}) x^n-a L \mathbb{Q} \left<\sigma,\tau\right> \sigma: \sqrt[n]{a}\rightarrow \sqrt[n]{a}\xi_n \xi_n\rightarrow\xi_n \tau:\xi_n\rightarrow\xi_n^{m} \gcd(m,n)=1 m\neq 1 \sqrt[n]{a}\rightarrow \sqrt[n]{a} K \left<\tau\right> K \left<\tau\right> \left<\tau,\sigma^{d}\right> K=\mathbb{Q}(\sqrt[d]{a})","['abstract-algebra', 'galois-theory']"
24,The cubic equation $x^3 - 4 x^2 + x + 1 =0$,The cubic equation,x^3 - 4 x^2 + x + 1 =0,"The cubic polynomial $P(x) = x^{3} - 4x^{2} + x + 1$ has discriminant $\Delta = 169 = 13^{2}$ which tells us that the extension $\mathbb{Q}(a)/\mathbb{Q}$ is normal, where $a$ is any root of the equation $P(x) = 0$ . Therefore, given one root $a$ , one can find the other as polynomial expressions in $a$ . For instance, in this case it is not hard to check  that the other roots are $a^{2} - 4a + 2$ and $-a^{2} + 3 a + 2$ . But what if we didn't know these expressions? Is there a way to get them?","The cubic polynomial has discriminant which tells us that the extension is normal, where is any root of the equation . Therefore, given one root , one can find the other as polynomial expressions in . For instance, in this case it is not hard to check  that the other roots are and . But what if we didn't know these expressions? Is there a way to get them?",P(x) = x^{3} - 4x^{2} + x + 1 \Delta = 169 = 13^{2} \mathbb{Q}(a)/\mathbb{Q} a P(x) = 0 a a a^{2} - 4a + 2 -a^{2} + 3 a + 2,"['abstract-algebra', 'polynomials', 'galois-theory', 'cubics']"
25,Is Kaplansky's theorem for hereditary rings a characterization?,Is Kaplansky's theorem for hereditary rings a characterization?,,"This question came up during a first course on rings and modules I TA'd at. Kaplansky's Theorem for hereditary rings states that If $A$ is a hereditary ring, and $F$ is a free left $A$-module, then every submodule $M \subset F$ is isomorphic to a direct sum $\bigoplus_{i \in I} J_i$, where every $J_i$ is a left ideal of $A$. See for example Lam's Lectures on modules and rings , (2.24). Recently a student asked me for an example of a submodule of a free module that was not a direct sum of ideals, and the best I could come up with was the following: Let $A = \mathbb Z_4[X]/(X^2)$, and let $M = \langle (2,X) \rangle \subset A^2$; then $M$ is not isomorphic to a direct sum of ideals. My proof is long and tedious, and besides $A$ is very very far from being hereditary, since it has infinite global dimension. Hence the question: Can we find a simpler example of a submodule of a free module that is not isomorphic to a direct sum of ideals (say, over $\mathbb Z[X]$)? In fact, I was wondering if there are examples for all non-hereditary rings. is the converse of Kaplansky's theorem true? if a ring $A$ is such that all submodules of a free module are isomorphic to a direct sum of ideals, does it follow that $A$ is hereditary?","This question came up during a first course on rings and modules I TA'd at. Kaplansky's Theorem for hereditary rings states that If $A$ is a hereditary ring, and $F$ is a free left $A$-module, then every submodule $M \subset F$ is isomorphic to a direct sum $\bigoplus_{i \in I} J_i$, where every $J_i$ is a left ideal of $A$. See for example Lam's Lectures on modules and rings , (2.24). Recently a student asked me for an example of a submodule of a free module that was not a direct sum of ideals, and the best I could come up with was the following: Let $A = \mathbb Z_4[X]/(X^2)$, and let $M = \langle (2,X) \rangle \subset A^2$; then $M$ is not isomorphic to a direct sum of ideals. My proof is long and tedious, and besides $A$ is very very far from being hereditary, since it has infinite global dimension. Hence the question: Can we find a simpler example of a submodule of a free module that is not isomorphic to a direct sum of ideals (say, over $\mathbb Z[X]$)? In fact, I was wondering if there are examples for all non-hereditary rings. is the converse of Kaplansky's theorem true? if a ring $A$ is such that all submodules of a free module are isomorphic to a direct sum of ideals, does it follow that $A$ is hereditary?",,"['abstract-algebra', 'ring-theory']"
26,Basis for proper rational functions,Basis for proper rational functions,,"Suppose $F$ is a field, and let $F(x)$ denote the $F$ -vector space of all rational functions $\frac{f(x)}{g(x)}$ , where $f,g\in F[x]$ are polynomials, with $g$ different from zero. Let $F(x)_p$ denote the subspace of F(x) of all proper fractions, i.e. all $\frac{f(x)}{g(x)}$ where degree( $f$ ) $<$ degree( $g$ ). Show that: (a) $F(x)$ is isomorphic to $F[x]\oplus F(x)_p $ as an $F$ -vector space. (b) If $\mathcal{I}=\{p(x)\in F[x] \mid p \text { is a monic irreducible polynomial}\}$ , then $$\beta= \{\frac{x^{j}}{p(x)^{k}} \mid p(x)\in \mathcal{I}, 0\leq j< \text{degree}(p); k\geq 1 \}  $$ is a basis for $F(x)_p$ as an $F$ -vector space. I am done with part (a), but I don't know how to proceed in part (b). Can anyone help me please?","Suppose is a field, and let denote the -vector space of all rational functions , where are polynomials, with different from zero. Let denote the subspace of F(x) of all proper fractions, i.e. all where degree( ) degree( ). Show that: (a) is isomorphic to as an -vector space. (b) If , then is a basis for as an -vector space. I am done with part (a), but I don't know how to proceed in part (b). Can anyone help me please?","F F(x) F \frac{f(x)}{g(x)} f,g\in F[x] g F(x)_p \frac{f(x)}{g(x)} f < g F(x) F[x]\oplus F(x)_p  F \mathcal{I}=\{p(x)\in F[x] \mid p \text { is a monic irreducible polynomial}\} \beta= \{\frac{x^{j}}{p(x)^{k}} \mid p(x)\in \mathcal{I}, 0\leq j< \text{degree}(p); k\geq 1 \}   F(x)_p F","['abstract-algebra', 'vector-spaces', 'modules']"
27,Errata for Hungerford's Algebra,Errata for Hungerford's Algebra,,"Currently reading the category theory section of Hungerford's Algebra (the GTM), and I am noticing an egregious amount of typos. However, I have been at a loss to find an errata for this text, despite it seemingly being rather well known. Could anyone suggest an errata?","Currently reading the category theory section of Hungerford's Algebra (the GTM), and I am noticing an egregious amount of typos. However, I have been at a loss to find an errata for this text, despite it seemingly being rather well known. Could anyone suggest an errata?",,"['abstract-algebra', 'reference-request']"
28,Applications of Banach Algebras and Operator Algebras,Applications of Banach Algebras and Operator Algebras,,"I am trying to learn operator algebra theory (I am tempted to start with Douglas' ""Banach Algebra Techniques in Operator Theory"" ). One aspect that I am curious about is whether there are significant applications of that theory. (Especially in applied mathematics or any area that heavily relies on applied mathematics).","I am trying to learn operator algebra theory (I am tempted to start with Douglas' ""Banach Algebra Techniques in Operator Theory"" ). One aspect that I am curious about is whether there are significant applications of that theory. (Especially in applied mathematics or any area that heavily relies on applied mathematics).",,"['abstract-algebra', 'functional-analysis', 'reference-request', 'operator-algebras', 'applications']"
29,Galois group of $X^5 - X^3 - 2X^2 - 2X - 1$ over $\mathbb{Q}$.,Galois group of  over .,X^5 - X^3 - 2X^2 - 2X - 1 \mathbb{Q},"So far I have found that this polynomial is irreducible, and its discriminant is a square. Therefore the Galois group it a transitive subgroup of $A_5$. I found out that the only transitive subgroups in $A_5$ are $A_5$ and $D_{10}$. Is there a way to tell the Galois group from these two possibilities?","So far I have found that this polynomial is irreducible, and its discriminant is a square. Therefore the Galois group it a transitive subgroup of $A_5$. I found out that the only transitive subgroups in $A_5$ are $A_5$ and $D_{10}$. Is there a way to tell the Galois group from these two possibilities?",,"['abstract-algebra', 'galois-theory']"
30,Counting roots of a multivariate polynomial over a finite field,Counting roots of a multivariate polynomial over a finite field,,"How many roots can there be of a polynomial $f \in K[x_1, x_2, \dots , x_n]$ where $K$ is a finite field and the maximum exponent of $x_i$ in any term is $m$ for all $i$, assuming not all coefficients are zero? I found this related question , but I'm really interested in the worst case; I won't have any ""singularity condition"", necessarily. Here's an idea: View $f$ as a polynomial over $(K(x_2, \dots , x_n))[x_1]$, where $K(x_2, \dots , x_n)$ is the field of fractions of $K[x_2, \dots , x_n]$. At least one coefficient of $f$ when viewed this way is non-zero. Now $f$ has at most $m$ roots, since its single indeterminate, $x_1$, has exponent at most $m$. For each of these $m$ roots $y_{1,1}, y_{1,2}, \dots, y_{1,m}$ and any $x_2, \dots, x_n$, $f(y_{1,j}, x_2, \dots, x_n) = 0$. There are $m k^{n-1}$ tuples of this form, where $k$ is the size of $K$. Additionally, for each $v \in K$, $f(v, x_2, \dots, x_n)$ is a polynomial in $n-1$ variables. For the $v$s that are not roots of $f$ when viewed as above, $f(v, x_2, \dots, x_n)$ has at least one non-zero coefficient. This sets up an inductive equation: $$ c(n) \leq  \begin{cases} m & n = 1\\ m k^{n-1} + k c(n-1) & n > 1 \end{cases} $$ With solution $$ c(n) \leq n m k^{n-1} $$ On the other hand, let $f_i$ be a polynomial of degree $m$ in $K[x_i]$ with $m$ distinct roots. Then $\prod f_i$ has at least $n m (k-m)^{n-1}$ distinct roots. Is this right? Can we get tighter bounds?","How many roots can there be of a polynomial $f \in K[x_1, x_2, \dots , x_n]$ where $K$ is a finite field and the maximum exponent of $x_i$ in any term is $m$ for all $i$, assuming not all coefficients are zero? I found this related question , but I'm really interested in the worst case; I won't have any ""singularity condition"", necessarily. Here's an idea: View $f$ as a polynomial over $(K(x_2, \dots , x_n))[x_1]$, where $K(x_2, \dots , x_n)$ is the field of fractions of $K[x_2, \dots , x_n]$. At least one coefficient of $f$ when viewed this way is non-zero. Now $f$ has at most $m$ roots, since its single indeterminate, $x_1$, has exponent at most $m$. For each of these $m$ roots $y_{1,1}, y_{1,2}, \dots, y_{1,m}$ and any $x_2, \dots, x_n$, $f(y_{1,j}, x_2, \dots, x_n) = 0$. There are $m k^{n-1}$ tuples of this form, where $k$ is the size of $K$. Additionally, for each $v \in K$, $f(v, x_2, \dots, x_n)$ is a polynomial in $n-1$ variables. For the $v$s that are not roots of $f$ when viewed as above, $f(v, x_2, \dots, x_n)$ has at least one non-zero coefficient. This sets up an inductive equation: $$ c(n) \leq  \begin{cases} m & n = 1\\ m k^{n-1} + k c(n-1) & n > 1 \end{cases} $$ With solution $$ c(n) \leq n m k^{n-1} $$ On the other hand, let $f_i$ be a polynomial of degree $m$ in $K[x_i]$ with $m$ distinct roots. Then $\prod f_i$ has at least $n m (k-m)^{n-1}$ distinct roots. Is this right? Can we get tighter bounds?",,"['abstract-algebra', 'polynomials', 'finite-fields']"
31,"If $B$ is finitely generated as a $k$-algebra, and $\phi:A\to B$ is a $k$-algebra map, is $\phi^{-1}(M)$ maximal for any maximal $M\subset B$? [duplicate]","If  is finitely generated as a -algebra, and  is a -algebra map, is  maximal for any maximal ? [duplicate]",B k \phi:A\to B k \phi^{-1}(M) M\subset B,"This question already has answers here : In an extension of finitely generated $k$-algebras the contraction of a maximal ideal is also maximal (2 answers) Closed 5 years ago . Suppose that $A$ and $B$ are commutative rings containing a field $k$, and $B$ is finitely generated $k$-algebra. Let $\phi: A\rightarrow B$ be a ring homomorphism with $\phi|_k =\mathrm{Id}$. I am trying to prove that if $M\subset B$ is a maximal ideal, then $\phi^{-1}(M)$ is a maximal ideal of $A$. The case when $A \subset B$ is an integral extension of rings is well-known. I think I can also prove the result when $\phi$ is surjective. Inverse Image of Maximal Ideals discusses the case when $B$ is a finitely generated $\mathbb{Z}$-algebra but I am not sure how to generalize this.","This question already has answers here : In an extension of finitely generated $k$-algebras the contraction of a maximal ideal is also maximal (2 answers) Closed 5 years ago . Suppose that $A$ and $B$ are commutative rings containing a field $k$, and $B$ is finitely generated $k$-algebra. Let $\phi: A\rightarrow B$ be a ring homomorphism with $\phi|_k =\mathrm{Id}$. I am trying to prove that if $M\subset B$ is a maximal ideal, then $\phi^{-1}(M)$ is a maximal ideal of $A$. The case when $A \subset B$ is an integral extension of rings is well-known. I think I can also prove the result when $\phi$ is surjective. Inverse Image of Maximal Ideals discusses the case when $B$ is a finitely generated $\mathbb{Z}$-algebra but I am not sure how to generalize this.",,"['abstract-algebra', 'commutative-algebra', 'ring-theory']"
32,Classifing groups of order 56: problems with the semidirect product,Classifing groups of order 56: problems with the semidirect product,,"While I was doing an exercise about the classification of groups of order 56, I had some problems concerning the semidirect product. Let $G$ a group of order 56 and let us suppose that the 7-Sylow is normal (let's call it $H$). Then we want to construct the non abelian group whose 2-Sylow $S$ is $S \cong \mathbb Z_8$. First of all, we have to determine the homomorphism $\phi \colon \mathbb Z_8 \to \text{Aut}(\mathbb Z_7)$.  It is known that $\text{Aut}(\mathbb Z_7) \cong \mathbb Z_6$ and the isomorphism is given by $$ \begin{split} & \mathbb Z_6 \to \text{Aut}(\mathbb Z_7) \\ & a \mapsto \psi_a \colon \mathbb Z_7 \ni n \mapsto an \in \mathbb Z_7 \end{split} $$ So we can start by finding the homomorphism $\mathbb Z_8 \to \mathbb Z_6$. There are exacly $(6,8)=2$ such homomorphism. Who are they? Simply the one who sends everything to $0$ and the multiplication by $3$ (which is the only element in $\mathbb Z_6$ whose order - 2 - divides 8). In multiplicative terms, they are the homomorphism which sends everything to $1$ and the homomorphism which sends $n \mapsto 6^n=(-1)^n$. So, by composition, we have the two homomorphism  $$ \begin{split} \phi_1 \colon & \mathbb Z_8 \to \text{Aut}(\mathbb Z_7)\\ & n \mapsto \text{id} \end{split} $$ and $$ \begin{split} \phi_2 \colon & \mathbb Z_8 \to \text{Aut}(\mathbb Z_7)\\ & n \mapsto \psi_n \colon \mathbb Z_7 \ni x \mapsto 6^nx = (-1)^nx \in \mathbb Z_7  \end{split} $$ Am I right? Now, if we take $\phi_1$ we simply get the direct product. What if we take $\phi_2$? For sake of simplicity, let's assume additive notation (this is stupid, I know but it has helped me somehow to understand). If I'm not wrong, we obtain that $H \rtimes_{\phi_2} \mathbb Z_8$ is the set $H \times \mathbb Z_8$ with the operation given by  $$ (a,b) + (c,d) = (a+(-1)^bc,b+d) $$ Now if I do $(0,k)+(h,0)-(0,k) = ((-1)^k h, 0) = \phi_k(h)$ which is exactly what I want. Now I must pass to the much more confortable multiplicative notation: so let's $C_7=\langle s \rangle$ and $C_8=\langle r \rangle$ be the cyclic groups of order 7 and 8. Then we define the automorphisms  $$ \begin{split} \phi_n \colon & C_7 \to C_7 \\ & x \mapsto x^{(-1)^n} \end{split} $$ and the homomorphism  $$ \begin{split} \psi \colon & C_8 \to \text{Aut}(C_7) \\ & n \mapsto \phi_n \end{split} $$ In other words, we can simply say that $\psi$ is the homomorphism which sends the generator $r$ to the inversion $x^{-1}$. Am I right so far? Well, now $C_7 \rtimes_{\psi} C_8$ is the set $C_7 \times C_8$ with the operation given by  $$ (a,b)(c,d) = (ac^{(-1)^b},bd) $$ I do again the calculation $(1,k)(h,1)(1,k)^{-1}=(h^{(-1)^k},1) = \phi_k(h)$ which is exactly what I want (also according to ineff's answer). Are there any mistakes?  May I ask one more question? Who is this mysterious group I've built up? Is it isomorphic to some other (simpler) group? How can I do to write down a presentation? I thank you in advance for your kind help.","While I was doing an exercise about the classification of groups of order 56, I had some problems concerning the semidirect product. Let $G$ a group of order 56 and let us suppose that the 7-Sylow is normal (let's call it $H$). Then we want to construct the non abelian group whose 2-Sylow $S$ is $S \cong \mathbb Z_8$. First of all, we have to determine the homomorphism $\phi \colon \mathbb Z_8 \to \text{Aut}(\mathbb Z_7)$.  It is known that $\text{Aut}(\mathbb Z_7) \cong \mathbb Z_6$ and the isomorphism is given by $$ \begin{split} & \mathbb Z_6 \to \text{Aut}(\mathbb Z_7) \\ & a \mapsto \psi_a \colon \mathbb Z_7 \ni n \mapsto an \in \mathbb Z_7 \end{split} $$ So we can start by finding the homomorphism $\mathbb Z_8 \to \mathbb Z_6$. There are exacly $(6,8)=2$ such homomorphism. Who are they? Simply the one who sends everything to $0$ and the multiplication by $3$ (which is the only element in $\mathbb Z_6$ whose order - 2 - divides 8). In multiplicative terms, they are the homomorphism which sends everything to $1$ and the homomorphism which sends $n \mapsto 6^n=(-1)^n$. So, by composition, we have the two homomorphism  $$ \begin{split} \phi_1 \colon & \mathbb Z_8 \to \text{Aut}(\mathbb Z_7)\\ & n \mapsto \text{id} \end{split} $$ and $$ \begin{split} \phi_2 \colon & \mathbb Z_8 \to \text{Aut}(\mathbb Z_7)\\ & n \mapsto \psi_n \colon \mathbb Z_7 \ni x \mapsto 6^nx = (-1)^nx \in \mathbb Z_7  \end{split} $$ Am I right? Now, if we take $\phi_1$ we simply get the direct product. What if we take $\phi_2$? For sake of simplicity, let's assume additive notation (this is stupid, I know but it has helped me somehow to understand). If I'm not wrong, we obtain that $H \rtimes_{\phi_2} \mathbb Z_8$ is the set $H \times \mathbb Z_8$ with the operation given by  $$ (a,b) + (c,d) = (a+(-1)^bc,b+d) $$ Now if I do $(0,k)+(h,0)-(0,k) = ((-1)^k h, 0) = \phi_k(h)$ which is exactly what I want. Now I must pass to the much more confortable multiplicative notation: so let's $C_7=\langle s \rangle$ and $C_8=\langle r \rangle$ be the cyclic groups of order 7 and 8. Then we define the automorphisms  $$ \begin{split} \phi_n \colon & C_7 \to C_7 \\ & x \mapsto x^{(-1)^n} \end{split} $$ and the homomorphism  $$ \begin{split} \psi \colon & C_8 \to \text{Aut}(C_7) \\ & n \mapsto \phi_n \end{split} $$ In other words, we can simply say that $\psi$ is the homomorphism which sends the generator $r$ to the inversion $x^{-1}$. Am I right so far? Well, now $C_7 \rtimes_{\psi} C_8$ is the set $C_7 \times C_8$ with the operation given by  $$ (a,b)(c,d) = (ac^{(-1)^b},bd) $$ I do again the calculation $(1,k)(h,1)(1,k)^{-1}=(h^{(-1)^k},1) = \phi_k(h)$ which is exactly what I want (also according to ineff's answer). Are there any mistakes?  May I ask one more question? Who is this mysterious group I've built up? Is it isomorphic to some other (simpler) group? How can I do to write down a presentation? I thank you in advance for your kind help.",,"['abstract-algebra', 'group-theory', 'finite-groups']"
33,Azumaya algebras,Azumaya algebras,,"I have seen a few different definitions of an Azumaya algebra in the literature- for example, Wikipedia prefers the following one: An Azumaya algebra over a commutative local ring $R$ is an $R$-algebra $A$ that is free and of finite rank $r$ as an $R$-module, such that the tensor product $A\otimes_R A^{op}$  (where $A^{op}$ is the opposite algebra) is isomorphic to the matrix algebra $\mathrm{End}_R(A) \sim M_r(R)$ via the map sending $a\otimes b$ to the endomorphism $x \mapsto axb$ of A. On the other hand, for purposes of checking this definition, I have seen the following characterization used: $A$ is a finitely generated projective $R$-module and for all maximal ideals $\mathfrak{m}\subset R$, $A/\mathfrak{m}A$ is a central simple $R/\mathfrak{m}$ algebra. Is there an obvious way to show that these are equivalent? EDIT : It has been noticed in the comments that the first definition specified ""local ring""- this is too specific for the two definitions to be equivalent. I'm interested in what can be said when one removes local (and as Mariano points out, replaces free by projective) from the first definition and allows $R$ to be just a commutative ring.","I have seen a few different definitions of an Azumaya algebra in the literature- for example, Wikipedia prefers the following one: An Azumaya algebra over a commutative local ring $R$ is an $R$-algebra $A$ that is free and of finite rank $r$ as an $R$-module, such that the tensor product $A\otimes_R A^{op}$  (where $A^{op}$ is the opposite algebra) is isomorphic to the matrix algebra $\mathrm{End}_R(A) \sim M_r(R)$ via the map sending $a\otimes b$ to the endomorphism $x \mapsto axb$ of A. On the other hand, for purposes of checking this definition, I have seen the following characterization used: $A$ is a finitely generated projective $R$-module and for all maximal ideals $\mathfrak{m}\subset R$, $A/\mathfrak{m}A$ is a central simple $R/\mathfrak{m}$ algebra. Is there an obvious way to show that these are equivalent? EDIT : It has been noticed in the comments that the first definition specified ""local ring""- this is too specific for the two definitions to be equivalent. I'm interested in what can be said when one removes local (and as Mariano points out, replaces free by projective) from the first definition and allows $R$ to be just a commutative ring.",,['abstract-algebra']
34,Is turtle graphics a group?,Is turtle graphics a group?,,"I'm trying to construct examples of combinatorial group theory flavor to introduce the ideas to young students. More specifically, I'm trying to introduce the idea that you can have strings of letters that have some replacement rules (such as ""you can delete $\texttt{XX}$ "" or ""you can replace $\texttt{XY}$ with $\texttt{YX}$ ""), and that you can just play with these things as formal symbols. But it would be nice to also have real examples that the students can act/draw out to verify their computations with the abstract pencil-and-paper manipulations. It struck me that turtle graphics could be one such example. Here's the idea: Let's say that the turtle graphics group $\mathcal{T}$ is generated by elements $L,R,F,B$ . In the typical setting of a turtle with a pen moving in the standard integer lattice in the plane, $L$ means to make a quarter turn left, $R$ a quarter turn right, $F$ move forward one step drawing e.g. a black line, $B$ move backward one step drawing e.g. a blue line. We'll keep track of the multiplicity of lines between the lattice paints, and let's say that black lines and blue lines along the same edge cancel. Then $L=R^{-1}$ and $B=F^{-1}$ , and I think it follows that this is a group. Can anyone give a presentation for this group? I.e. what are the relators? It's clear that $R$ has order $4$ and $F$ has infinite order, so a presentation might start at $\mathcal{T}=\langle R,F \mid R^4,\dotsc \rangle$ . But it's unclear to me whether $R$ and $F$ have any relation. Is the group in fact just the free product $\mathbb{Z}/4 * \mathbb Z$ ? Edit: to make the question more rigorous along the lines of the comment from @Kyle Miller, there are a few things at play here. There is a monoid generated by the symbols $R$ and $F$ . There is a set $D$ (for drawings?) that indexes possible states of the turtle graphics system. This set can be described as follows: Let $V=\mathbb{Z^2}$ denote the set of integer points in the plane ( $V$ for vertices). This is the set of possible positions of the turtle. Let $E$ denote the set of edges in the standard square grid. If you like, the elements of this set are all intervals of the forms $\{x\} \times [y,y+1]$ and $[x,x+1] \times \{y\}$ , where $x,y \in \mathbb Z$ . Then $D$ is the set $V \times \{0,1,2,3\} \times \mathbb{Z}^E$ where the first coordinate is the position of the turtle, the second is the turtle's orientation, and the third tells you how many times each edge has been drawn with multiplicity (positive for a forward step, negative for a backwards one). Then there is an action $\langle R,F \rangle \to \operatorname{Sym}(D)$ , and I suppose $\mathcal{T}$ is the group that is the image of this representation.","I'm trying to construct examples of combinatorial group theory flavor to introduce the ideas to young students. More specifically, I'm trying to introduce the idea that you can have strings of letters that have some replacement rules (such as ""you can delete "" or ""you can replace with ""), and that you can just play with these things as formal symbols. But it would be nice to also have real examples that the students can act/draw out to verify their computations with the abstract pencil-and-paper manipulations. It struck me that turtle graphics could be one such example. Here's the idea: Let's say that the turtle graphics group is generated by elements . In the typical setting of a turtle with a pen moving in the standard integer lattice in the plane, means to make a quarter turn left, a quarter turn right, move forward one step drawing e.g. a black line, move backward one step drawing e.g. a blue line. We'll keep track of the multiplicity of lines between the lattice paints, and let's say that black lines and blue lines along the same edge cancel. Then and , and I think it follows that this is a group. Can anyone give a presentation for this group? I.e. what are the relators? It's clear that has order and has infinite order, so a presentation might start at . But it's unclear to me whether and have any relation. Is the group in fact just the free product ? Edit: to make the question more rigorous along the lines of the comment from @Kyle Miller, there are a few things at play here. There is a monoid generated by the symbols and . There is a set (for drawings?) that indexes possible states of the turtle graphics system. This set can be described as follows: Let denote the set of integer points in the plane ( for vertices). This is the set of possible positions of the turtle. Let denote the set of edges in the standard square grid. If you like, the elements of this set are all intervals of the forms and , where . Then is the set where the first coordinate is the position of the turtle, the second is the turtle's orientation, and the third tells you how many times each edge has been drawn with multiplicity (positive for a forward step, negative for a backwards one). Then there is an action , and I suppose is the group that is the image of this representation.","\texttt{XX} \texttt{XY} \texttt{YX} \mathcal{T} L,R,F,B L R F B L=R^{-1} B=F^{-1} R 4 F \mathcal{T}=\langle R,F \mid R^4,\dotsc \rangle R F \mathbb{Z}/4 * \mathbb Z R F D V=\mathbb{Z^2} V E \{x\} \times [y,y+1] [x,x+1] \times \{y\} x,y \in \mathbb Z D V \times \{0,1,2,3\} \times \mathbb{Z}^E \langle R,F \rangle \to \operatorname{Sym}(D) \mathcal{T}","['abstract-algebra', 'group-theory', 'group-presentation', 'combinatorial-group-theory']"
35,Every prime ideal in $\mathbb{Z}[x]$ is generated by at most two elements [duplicate],Every prime ideal in  is generated by at most two elements [duplicate],\mathbb{Z}[x],"This question already has an answer here : Classification of prime ideals of $\mathbb{Z}[X]$ (1 answer) Closed 2 years ago . I am trying to prove Every prime ideal in $\mathbb{Z}[x]$ can be generated by at most two elements. Since I have not seen any prime ideal generated by 3 elements, this statement seems true to me. But I cannot find a way to prove it. Thanks for any help in advance!","This question already has an answer here : Classification of prime ideals of $\mathbb{Z}[X]$ (1 answer) Closed 2 years ago . I am trying to prove Every prime ideal in can be generated by at most two elements. Since I have not seen any prime ideal generated by 3 elements, this statement seems true to me. But I cannot find a way to prove it. Thanks for any help in advance!",\mathbb{Z}[x],"['abstract-algebra', 'ring-theory', 'ideals', 'maximal-and-prime-ideals']"
36,Why does this ring have rank $k!$?,Why does this ring have rank ?,k!,"Let $R$ be any ring free of rank $k$ over $\mathbb{Z}$ having non-zero discriminant.  Let $R^{\otimes k} = R \otimes_{\mathbb{Z}} \cdots \otimes_{\mathbb{Z}} R$. Then $R^{\otimes k}$ is a ring of rank $k^k$ in which $\mathbb{Z}$ lies naturally as a subring via $n \rightarrow n(1 \otimes  \cdots \otimes 1)$. Denote by $I_R$ the ideal in $R^{\otimes k}$ generated by elements of the form  $$ x \otimes  \cdots \otimes 1 + 1 \otimes x \cdots \otimes 1 + \cdots + 1 \otimes  \cdots \otimes x - \text{Tr} (x) $$ for $x \in R$. Let  $$ J_R = \{ r \in R^{\otimes k} :  nr \in I_R \text{ for some } n \in \mathbb{Z} \}. $$ Then the ring $$ R^{\otimes k} / J_R $$ has rank $k!$. I have been trying to work out why this ring has rank $k!$, but I am not quite seeing it yet. I would appreciate any explanation. Thank you very much! PS This is a ring that comes up in ``Higher composition laws III'' by Manjul Bhargava.","Let $R$ be any ring free of rank $k$ over $\mathbb{Z}$ having non-zero discriminant.  Let $R^{\otimes k} = R \otimes_{\mathbb{Z}} \cdots \otimes_{\mathbb{Z}} R$. Then $R^{\otimes k}$ is a ring of rank $k^k$ in which $\mathbb{Z}$ lies naturally as a subring via $n \rightarrow n(1 \otimes  \cdots \otimes 1)$. Denote by $I_R$ the ideal in $R^{\otimes k}$ generated by elements of the form  $$ x \otimes  \cdots \otimes 1 + 1 \otimes x \cdots \otimes 1 + \cdots + 1 \otimes  \cdots \otimes x - \text{Tr} (x) $$ for $x \in R$. Let  $$ J_R = \{ r \in R^{\otimes k} :  nr \in I_R \text{ for some } n \in \mathbb{Z} \}. $$ Then the ring $$ R^{\otimes k} / J_R $$ has rank $k!$. I have been trying to work out why this ring has rank $k!$, but I am not quite seeing it yet. I would appreciate any explanation. Thank you very much! PS This is a ring that comes up in ``Higher composition laws III'' by Manjul Bhargava.",,"['abstract-algebra', 'ring-theory', 'modules', 'tensor-products']"
37,What's an Isomorphism?,What's an Isomorphism?,,"I'm familiar with the definition (inverses and bijections, preserving operations) in the context of groups and vector spaces, the hoeomorphism of topological spaces, and have some feeling for the definition in category theory. What I'm looking for is a mathematical justification: for statements like ""....two isomorphic objects cannot be distinguished by using only the properties used to define morphisms; thus isomorphic objects may be considered the same as long as one considers only these properties and their consequences"" ( https://en.wikipedia.org/wiki/Isomorphism ). for the reliance on isomorphism in proofs. For example, the internal direct sum of subspaces of a vector space is isomorphic to the external direct sum of these subspaces. One can prove that the internal direct sum is associative and commutative and then call on isomorphism to say the same applies to the external direct sum. I would imagine somewhere in category theory there is some result along the lines that if $\phi$ is an isomorphism between two objects $O_1, O_2$ in a category, and $P$ is some logic statement about $O_1$ then $\phi(P) = P$, i.e. the logic statement about the corresponding entities in $O_2$ is true or false in accordance with the statement in $O_1$. Maybe my imagination is running ahead of the facts, but I would appreciate some feedback on the formalisation of ""...B  is isomorphic to A and therefore since P is true in A ..."" Addendum : thanks for comments and answer. It seems that an easily accessible answer applicable across all categories may be too much to aim for. What about answers for specific categories ? If one takes for example the category of topological spaces it appears (from what I've read) that ""properties which can be defined in terms of open sets are preserved by homeomorphism"". Can this statement be proved as such , or must one execute specific proofs for compactness, connectedness, convergence, etc ?","I'm familiar with the definition (inverses and bijections, preserving operations) in the context of groups and vector spaces, the hoeomorphism of topological spaces, and have some feeling for the definition in category theory. What I'm looking for is a mathematical justification: for statements like ""....two isomorphic objects cannot be distinguished by using only the properties used to define morphisms; thus isomorphic objects may be considered the same as long as one considers only these properties and their consequences"" ( https://en.wikipedia.org/wiki/Isomorphism ). for the reliance on isomorphism in proofs. For example, the internal direct sum of subspaces of a vector space is isomorphic to the external direct sum of these subspaces. One can prove that the internal direct sum is associative and commutative and then call on isomorphism to say the same applies to the external direct sum. I would imagine somewhere in category theory there is some result along the lines that if $\phi$ is an isomorphism between two objects $O_1, O_2$ in a category, and $P$ is some logic statement about $O_1$ then $\phi(P) = P$, i.e. the logic statement about the corresponding entities in $O_2$ is true or false in accordance with the statement in $O_1$. Maybe my imagination is running ahead of the facts, but I would appreciate some feedback on the formalisation of ""...B  is isomorphic to A and therefore since P is true in A ..."" Addendum : thanks for comments and answer. It seems that an easily accessible answer applicable across all categories may be too much to aim for. What about answers for specific categories ? If one takes for example the category of topological spaces it appears (from what I've read) that ""properties which can be defined in terms of open sets are preserved by homeomorphism"". Can this statement be proved as such , or must one execute specific proofs for compactness, connectedness, convergence, etc ?",,"['abstract-algebra', 'logic', 'category-theory']"
38,"Is there an algebraic non-rational extension of the integers, whose set of prime elements contains the prime integers?","Is there an algebraic non-rational extension of the integers, whose set of prime elements contains the prime integers?",,Let the ring $\mathbb{Z}[\alpha]$ with $\alpha$ an algebraic number. Let $P(\mathbb{Z}[\alpha])$ be the set of all the prime elements of $\mathbb{Z}[\alpha]$. Question : Is there $\alpha$ algebraic and non-rational such that $P(\mathbb{Z}) \subset P(\mathbb{Z}[\alpha])$?,Let the ring $\mathbb{Z}[\alpha]$ with $\alpha$ an algebraic number. Let $P(\mathbb{Z}[\alpha])$ be the set of all the prime elements of $\mathbb{Z}[\alpha]$. Question : Is there $\alpha$ algebraic and non-rational such that $P(\mathbb{Z}) \subset P(\mathbb{Z}[\alpha])$?,,"['abstract-algebra', 'ring-theory', 'commutative-algebra', 'prime-numbers']"
39,On conjugacy class size of finite groups.,On conjugacy class size of finite groups.,,"Suppose $G$ is a finite group such that the set of all the conjugacy class size is $\{1,2,\dots,n\}$, where $n$ is a natural number. Is it true that $n\leq 3$? Thanks in advance.","Suppose $G$ is a finite group such that the set of all the conjugacy class size is $\{1,2,\dots,n\}$, where $n$ is a natural number. Is it true that $n\leq 3$? Thanks in advance.",,"['abstract-algebra', 'group-theory', 'finite-groups']"
40,"Intermediate field, normal closure and Galois group","Intermediate field, normal closure and Galois group",,"Let $K/F$ be Galois with $G=Gal(K/F)$ and let $L$ be an intermediate field. Let $N\subseteq K$ be the normal closure of $L/F$. If $H=Gal(K/L)$ show that $Gal(K/N)=\cap_{\sigma\in G}\sigma H\sigma^{-1}$. (Exercise $8$, page $60$, Field and Galois Theory , Patrick Morandi.) Help me a hint to prove it. Thanks a lot.","Let $K/F$ be Galois with $G=Gal(K/F)$ and let $L$ be an intermediate field. Let $N\subseteq K$ be the normal closure of $L/F$. If $H=Gal(K/L)$ show that $Gal(K/N)=\cap_{\sigma\in G}\sigma H\sigma^{-1}$. (Exercise $8$, page $60$, Field and Galois Theory , Patrick Morandi.) Help me a hint to prove it. Thanks a lot.",,"['abstract-algebra', 'field-theory', 'galois-theory']"
41,Ideal defining the nilpotent cone of $\mathfrak{gl}_n(k)$,Ideal defining the nilpotent cone of,\mathfrak{gl}_n(k),"Let $k$ be an algebraically closed field, and let $\mathfrak{g}=\mathfrak{gl}_n(k)$.  Let $\mathcal{N}\subset\mathfrak{g}$ be the nilpotent cone, that is: $$\mathcal{N}=\{A\in\mathfrak{g}\mid A^n=0\}=\{A\in\mathfrak{g}\mid \text{ch}(A)=x^n\}$$ where ch$(A)$ is the characteristic polynomial of the matrix $A$.  Let $X=(x_{ij})$ be a matrix of indeterminates, and notice that as an affine variety $\mathcal{N}\subset\mathbb{A}_k^{n^2}$ can be defined in two ways.  If $I$ is the ideal generated by the $n^2$ homogeneous polynomial equations of degree $n$ given by $X^n=0$, then $\mathcal{N}=Z(I)$.  Also, if $J$ is the ideal generated by the $n$ homogeneous equations of degrees $1,2,\ldots,n$ which define the non-leading coefficients of ch$(X)$, then $\mathcal{N}=Z(J)$.  Now it follows that $\sqrt{I}=\sqrt{J}$. It is clear to me that $I$ is not radical because $x_{11}+x_{22}+\ldots+x_{nn}\in \sqrt I\setminus I$.  Is $J$ a radical ideal?  I believe it is, based on the comments in this question , but I can't seem to prove it.","Let $k$ be an algebraically closed field, and let $\mathfrak{g}=\mathfrak{gl}_n(k)$.  Let $\mathcal{N}\subset\mathfrak{g}$ be the nilpotent cone, that is: $$\mathcal{N}=\{A\in\mathfrak{g}\mid A^n=0\}=\{A\in\mathfrak{g}\mid \text{ch}(A)=x^n\}$$ where ch$(A)$ is the characteristic polynomial of the matrix $A$.  Let $X=(x_{ij})$ be a matrix of indeterminates, and notice that as an affine variety $\mathcal{N}\subset\mathbb{A}_k^{n^2}$ can be defined in two ways.  If $I$ is the ideal generated by the $n^2$ homogeneous polynomial equations of degree $n$ given by $X^n=0$, then $\mathcal{N}=Z(I)$.  Also, if $J$ is the ideal generated by the $n$ homogeneous equations of degrees $1,2,\ldots,n$ which define the non-leading coefficients of ch$(X)$, then $\mathcal{N}=Z(J)$.  Now it follows that $\sqrt{I}=\sqrt{J}$. It is clear to me that $I$ is not radical because $x_{11}+x_{22}+\ldots+x_{nn}\in \sqrt I\setminus I$.  Is $J$ a radical ideal?  I believe it is, based on the comments in this question , but I can't seem to prove it.",,"['abstract-algebra', 'algebraic-geometry', 'commutative-algebra', 'lie-algebras']"
42,Isomorphic rings or not?,Isomorphic rings or not?,,"Prove that the ring $\mathbb F_2(T^2)[X]/((X^2+T^2)^2)$ is (or is not) isomorphic to $\mathbb F_2(T)[Y]/(Y^2)$. Remark. The above question is related to this topic , where it's proved that for $p(X)=X^2+T^2$ such an isomorphism holds provided $p'\neq 0$, i.e., over fields of characteristic $\neq 2$.","Prove that the ring $\mathbb F_2(T^2)[X]/((X^2+T^2)^2)$ is (or is not) isomorphic to $\mathbb F_2(T)[Y]/(Y^2)$. Remark. The above question is related to this topic , where it's proved that for $p(X)=X^2+T^2$ such an isomorphism holds provided $p'\neq 0$, i.e., over fields of characteristic $\neq 2$.",,['abstract-algebra']
43,Path Connectedness and continuous bijections,Path Connectedness and continuous bijections,,"Mathoverflow . Are there any two topological spaces $X$ and $Y$ such that they are path connected and such that there exist continuous bijections $X\rightarrow Y$ and $Y\rightarrow X$, but  and yet they are not homeomorphic? Without Path-Connectedness requirement, this is easily fulfilled as the examples in the cited post. If it indeed implies homeomorphism, how can I prove it?","Mathoverflow . Are there any two topological spaces $X$ and $Y$ such that they are path connected and such that there exist continuous bijections $X\rightarrow Y$ and $Y\rightarrow X$, but  and yet they are not homeomorphic? Without Path-Connectedness requirement, this is easily fulfilled as the examples in the cited post. If it indeed implies homeomorphism, how can I prove it?",,"['general-topology', 'continuity']"
44,Opposite Clifford-Algebra,Opposite Clifford-Algebra,,"for a symmetric bilinearform $\beta$ on a $\mathbb{K}$-vectorspace $V$ the associated Clifford Algebra $Cl(\beta)$ is the associative algebra with unit subject to the relations $$v\cdot v=\beta(v,v)\cdot 1\qquad\forall v\in V.$$It is then often said that $Cl(-\beta)$ is isomorphic to the opposite Algebra $Cl(\beta)^\text{op}$. Why is that? Cheers, Robert","for a symmetric bilinearform $\beta$ on a $\mathbb{K}$-vectorspace $V$ the associated Clifford Algebra $Cl(\beta)$ is the associative algebra with unit subject to the relations $$v\cdot v=\beta(v,v)\cdot 1\qquad\forall v\in V.$$It is then often said that $Cl(-\beta)$ is isomorphic to the opposite Algebra $Cl(\beta)^\text{op}$. Why is that? Cheers, Robert",,"['abstract-algebra', 'clifford-algebras']"
45,"Computing the ""lying over"", ""going up"", ""going down"" ideals.","Computing the ""lying over"", ""going up"", ""going down"" ideals.",,"For any commutative unital ring $R$ and an ideal $\mathfrak{a}$ of $R$, we shall denote  $$\begin{align*} \mathrm{Spec}(R)&:=\{\text{prime ideals of }R\},\\  \mathrm{Max}(R)&:=\{\text{maximal ideals of }R\},\text{ and}\\  \mathrm{minAss}(\mathfrak{a})&:=\{\mathfrak{p}\in \mathrm{Spec}(R);\, \mathfrak{a}\subseteq\mathfrak{p}, \nexists\mathfrak{p}' \in\mathrm{Spec}(R): \mathfrak{a}\subsetneq\mathfrak{p}'\subsetneq\!\mathfrak{p}\}\\ &\;=\{\text{minimal prime ideals over }\mathfrak{a}\}. \end{align*}$$ The Krull dimension of $R$ is  $\dim(R)\!:=\!\mathrm{max} \{n\!\in\!\mathbb{N}_0;\, \exists\mathfrak{p}_0,\ldots,\mathfrak{p}_n\!\in\!\mathrm{Spec}(R)\!: \mathfrak{p}_0\!\subsetneq\!\ldots\!\subsetneq\!\mathfrak{p}_n\}=$ length of the longest chain of prime ideals. I'm trying to understand the following excerpt from A SINGULAR Introduction to Commutative Algebra (Greuel & Pfister - 2008), p.242-243: Questions: (1) What is a ring of finite type over $K$? The term is never defined in the book, even though ring and other elementary notions are. I'm guessing it's a ring of the form $K[x_1,\ldots,x_n]/I$ for some $I\unlhd K[\mathbf{x}]$. But are there any restrictions on $I$? (2) If I am not mistaken, in general (i.e. for any commutative unital ring $R$ and $\mathfrak{a}\unlhd R$), we have $\mathrm{Spec}(R/\mathfrak{a})=\{\mathfrak{p}/\mathfrak{a};\;\mathfrak{p}\in\mathrm{Spec(R),\; \mathfrak{a}\subseteq\mathfrak{p}}\}$ and  $\mathrm{Max}(R/\mathfrak{a})=\{\mathfrak{m}/\mathfrak{a};\;\mathfrak{m}\in\mathrm{Max(R),\; \mathfrak{a}\subseteq\mathfrak{m}}\}$. Correct? (3) I'm trying to formulate Remark 3.5.14 more precisely. Is the following correct: Computing the ""Lying Over"", ""Going Up"", ""Going Down"" Ideals: Suppose we have $\{x_1,\ldots,x_m\}\subseteq\{y_1,\ldots,y_n\}$, $\;I\unlhd K[x_1,\ldots,x_m] =K[\mathbf{x}]$, $\;A=K[\mathbf{x}]/I$, $\;J\unlhd K[y_1,\ldots,y_n]=K[\mathbf{y}]$, $\;B=K[\mathbf{y}]/J$, and $A\leq B$ via the identification $f(\mathbf{x})\!+\!I\mapsto f(\mathbf{x})\!+\!J$ (this map is injective iff $J\cap K[\mathbf{x}]\subseteq I$, which we assume). For any $\mathfrak{a}\!\unlhd\!A$, let $\mathfrak{a}B$ denote the ideal of $B$, generated by $\mathfrak{a}$. We investigate the situation where we have $\mathfrak{p}_0\subseteq\mathfrak{p}_1\subseteq\mathfrak{p}_2$, $\;\mathfrak{p}_i\!\in\!\mathrm{Spec}(A)$, $\;\mathfrak{q}_0\subseteq\mathfrak{q}_1\subseteq\mathfrak{q}_2$, $\;\mathfrak{q}_i\!\in\! \mathrm{Spec}(B)$, and $\mathfrak{q}_i\!\cap\!A=\mathfrak{p}_i$, for $i\!=\!0,1,2$. $$\begin{array}{c @{\hspace{1mm}} c @{\hspace{1mm}} c @{\hspace{1mm}} c @{\hspace{1mm}} c @{\hspace{1mm}} c @{\hspace{1mm}} c} \frak{q}_0 & \subseteq & \frak{q}_1 & \subseteq & \frak{q}_2 \\ \downarrow &           & \downarrow &           & \downarrow \\ \frak{p}_0 & \subseteq & \frak{p}_1 & \subseteq & \frak{p}_2\\ \end{array}$$ Lemma 3.5.13 says that if $A\leq B$  is a finite extension of affine rings, $\mathfrak{p}\in\mathrm{Spec}(A)$, $\;\mathfrak{q}\in\mathrm{Spec}(B)$, $\;\mathfrak{p}B\subseteq\mathfrak{q}$, $\;\dim(B/\mathfrak{p}B)=\dim(B/\mathfrak{q})$, then we have $\mathfrak{q}\cap A=\mathfrak{p}$. (3.1) Does the converse of 3.5.13 hold: if $\mathfrak{q}\!\cap\!A\!=\!\mathfrak{p}$, then $\dim(B/\mathfrak{p}B)\!=\!\dim(B/\mathfrak{q})$? I think so. It suffices to show that $\mathfrak{p}B\!\subseteq\!\mathfrak{q}'\!\subseteq\!\mathfrak{q}$ and $\mathfrak{q}'\!\in\!\mathrm{Spec}(B)$ imply $\mathfrak{q}'\!=\!\mathfrak{q}$. Indeed, we have: $$\begin{array}{r @{\hspace{1mm}} c @{\hspace{1mm}} c @{\hspace{1mm}} c @{\hspace{1mm}} c @{\hspace{1mm}} c @{\hspace{1mm}} c} \mathfrak{p}B & \subseteq & \mathfrak{q}' & \subseteq & \mathfrak{q} \\ \downarrow &           & \downarrow &           & \downarrow \\ \mathfrak{p}\!\subseteq\!\mathfrak{p}B\!\cap\!A & \subseteq & \mathfrak{q}'\!\cap\!A & \subseteq & \mathfrak{p}\\ \end{array},$$ so $\mathfrak{q}'\!\cap\!A\!=\!\mathfrak{p}$, which by the ""incomparable"" theorem implies $\mathfrak{q}'\!=\!\mathfrak{q}$. Correct? This means that the only candidates for the ""lying over $\mathfrak{p}$"" ideals are among $\mathrm{minAss}(\mathfrak{p}B)$. (3.2) If I see correctly, with the hypotheses $\mathfrak{p}\in\mathrm{Spec}(A)$, $\;\mathfrak{q}\in\mathrm{Spec}(B)$, $\;\mathfrak{p}B\subseteq\mathfrak{q}$, the condition $\dim(B/\mathfrak{p}B)=\dim(B/\mathfrak{q})$ is equivalent to $\mathfrak{q}\in\mathrm{minAss}(\mathfrak{p}B)$, by (2) and the definition of $\dim$ and $\mathrm{minAss}$. Yes?  Why do we then have to check in 3.5.14 that the dimension is right?","For any commutative unital ring $R$ and an ideal $\mathfrak{a}$ of $R$, we shall denote  $$\begin{align*} \mathrm{Spec}(R)&:=\{\text{prime ideals of }R\},\\  \mathrm{Max}(R)&:=\{\text{maximal ideals of }R\},\text{ and}\\  \mathrm{minAss}(\mathfrak{a})&:=\{\mathfrak{p}\in \mathrm{Spec}(R);\, \mathfrak{a}\subseteq\mathfrak{p}, \nexists\mathfrak{p}' \in\mathrm{Spec}(R): \mathfrak{a}\subsetneq\mathfrak{p}'\subsetneq\!\mathfrak{p}\}\\ &\;=\{\text{minimal prime ideals over }\mathfrak{a}\}. \end{align*}$$ The Krull dimension of $R$ is  $\dim(R)\!:=\!\mathrm{max} \{n\!\in\!\mathbb{N}_0;\, \exists\mathfrak{p}_0,\ldots,\mathfrak{p}_n\!\in\!\mathrm{Spec}(R)\!: \mathfrak{p}_0\!\subsetneq\!\ldots\!\subsetneq\!\mathfrak{p}_n\}=$ length of the longest chain of prime ideals. I'm trying to understand the following excerpt from A SINGULAR Introduction to Commutative Algebra (Greuel & Pfister - 2008), p.242-243: Questions: (1) What is a ring of finite type over $K$? The term is never defined in the book, even though ring and other elementary notions are. I'm guessing it's a ring of the form $K[x_1,\ldots,x_n]/I$ for some $I\unlhd K[\mathbf{x}]$. But are there any restrictions on $I$? (2) If I am not mistaken, in general (i.e. for any commutative unital ring $R$ and $\mathfrak{a}\unlhd R$), we have $\mathrm{Spec}(R/\mathfrak{a})=\{\mathfrak{p}/\mathfrak{a};\;\mathfrak{p}\in\mathrm{Spec(R),\; \mathfrak{a}\subseteq\mathfrak{p}}\}$ and  $\mathrm{Max}(R/\mathfrak{a})=\{\mathfrak{m}/\mathfrak{a};\;\mathfrak{m}\in\mathrm{Max(R),\; \mathfrak{a}\subseteq\mathfrak{m}}\}$. Correct? (3) I'm trying to formulate Remark 3.5.14 more precisely. Is the following correct: Computing the ""Lying Over"", ""Going Up"", ""Going Down"" Ideals: Suppose we have $\{x_1,\ldots,x_m\}\subseteq\{y_1,\ldots,y_n\}$, $\;I\unlhd K[x_1,\ldots,x_m] =K[\mathbf{x}]$, $\;A=K[\mathbf{x}]/I$, $\;J\unlhd K[y_1,\ldots,y_n]=K[\mathbf{y}]$, $\;B=K[\mathbf{y}]/J$, and $A\leq B$ via the identification $f(\mathbf{x})\!+\!I\mapsto f(\mathbf{x})\!+\!J$ (this map is injective iff $J\cap K[\mathbf{x}]\subseteq I$, which we assume). For any $\mathfrak{a}\!\unlhd\!A$, let $\mathfrak{a}B$ denote the ideal of $B$, generated by $\mathfrak{a}$. We investigate the situation where we have $\mathfrak{p}_0\subseteq\mathfrak{p}_1\subseteq\mathfrak{p}_2$, $\;\mathfrak{p}_i\!\in\!\mathrm{Spec}(A)$, $\;\mathfrak{q}_0\subseteq\mathfrak{q}_1\subseteq\mathfrak{q}_2$, $\;\mathfrak{q}_i\!\in\! \mathrm{Spec}(B)$, and $\mathfrak{q}_i\!\cap\!A=\mathfrak{p}_i$, for $i\!=\!0,1,2$. $$\begin{array}{c @{\hspace{1mm}} c @{\hspace{1mm}} c @{\hspace{1mm}} c @{\hspace{1mm}} c @{\hspace{1mm}} c @{\hspace{1mm}} c} \frak{q}_0 & \subseteq & \frak{q}_1 & \subseteq & \frak{q}_2 \\ \downarrow &           & \downarrow &           & \downarrow \\ \frak{p}_0 & \subseteq & \frak{p}_1 & \subseteq & \frak{p}_2\\ \end{array}$$ Lemma 3.5.13 says that if $A\leq B$  is a finite extension of affine rings, $\mathfrak{p}\in\mathrm{Spec}(A)$, $\;\mathfrak{q}\in\mathrm{Spec}(B)$, $\;\mathfrak{p}B\subseteq\mathfrak{q}$, $\;\dim(B/\mathfrak{p}B)=\dim(B/\mathfrak{q})$, then we have $\mathfrak{q}\cap A=\mathfrak{p}$. (3.1) Does the converse of 3.5.13 hold: if $\mathfrak{q}\!\cap\!A\!=\!\mathfrak{p}$, then $\dim(B/\mathfrak{p}B)\!=\!\dim(B/\mathfrak{q})$? I think so. It suffices to show that $\mathfrak{p}B\!\subseteq\!\mathfrak{q}'\!\subseteq\!\mathfrak{q}$ and $\mathfrak{q}'\!\in\!\mathrm{Spec}(B)$ imply $\mathfrak{q}'\!=\!\mathfrak{q}$. Indeed, we have: $$\begin{array}{r @{\hspace{1mm}} c @{\hspace{1mm}} c @{\hspace{1mm}} c @{\hspace{1mm}} c @{\hspace{1mm}} c @{\hspace{1mm}} c} \mathfrak{p}B & \subseteq & \mathfrak{q}' & \subseteq & \mathfrak{q} \\ \downarrow &           & \downarrow &           & \downarrow \\ \mathfrak{p}\!\subseteq\!\mathfrak{p}B\!\cap\!A & \subseteq & \mathfrak{q}'\!\cap\!A & \subseteq & \mathfrak{p}\\ \end{array},$$ so $\mathfrak{q}'\!\cap\!A\!=\!\mathfrak{p}$, which by the ""incomparable"" theorem implies $\mathfrak{q}'\!=\!\mathfrak{q}$. Correct? This means that the only candidates for the ""lying over $\mathfrak{p}$"" ideals are among $\mathrm{minAss}(\mathfrak{p}B)$. (3.2) If I see correctly, with the hypotheses $\mathfrak{p}\in\mathrm{Spec}(A)$, $\;\mathfrak{q}\in\mathrm{Spec}(B)$, $\;\mathfrak{p}B\subseteq\mathfrak{q}$, the condition $\dim(B/\mathfrak{p}B)=\dim(B/\mathfrak{q})$ is equivalent to $\mathfrak{q}\in\mathrm{minAss}(\mathfrak{p}B)$, by (2) and the definition of $\dim$ and $\mathrm{minAss}$. Yes?  Why do we then have to check in 3.5.14 that the dimension is right?",,"['abstract-algebra', 'algebraic-geometry', 'commutative-algebra', 'computational-mathematics']"
46,Why do elements of coprime orders commute in nilpotent groups?,Why do elements of coprime orders commute in nilpotent groups?,,"I want to show the following statement: Let $G$ be a nilpotent group and $a,b\in G$ such that there exist $m,n\in\mathbf{N}_{>0}$ such that $\text{gcd}(m,n)=1$ and $a^m=b^n=1$. Then $ab=ba$. If $G$ is finite, this is clear to me, since I know that then $G$ is the direct product of its Sylow subgroups. I have found a sketch of a proof in Hall's Theory of groups : If $G=G_1,G_2,\ldots$ is the lower central series of $G$, show that $[a,b]\in G_i$ for any $i$, which then implies $[a,b]=e$. Hall even gives another hint: If $[a,b]\in G_i$, show that $[a,b]^m\in G_{i+1}$ and $[a,b]^n\in G_{i+1}$. Unfortunately I can't do that. I managed the case $m=2$: $$a^{-1}b^{-1}aba^{-1}b^{-1}ab=a^{-1}(b^{-1}a^{-1}ba)a(a^{-1}b^{-1}ab)=[a,[a,b]],$$ but I don't see how to generalize this. Another hint would be nice.","I want to show the following statement: Let $G$ be a nilpotent group and $a,b\in G$ such that there exist $m,n\in\mathbf{N}_{>0}$ such that $\text{gcd}(m,n)=1$ and $a^m=b^n=1$. Then $ab=ba$. If $G$ is finite, this is clear to me, since I know that then $G$ is the direct product of its Sylow subgroups. I have found a sketch of a proof in Hall's Theory of groups : If $G=G_1,G_2,\ldots$ is the lower central series of $G$, show that $[a,b]\in G_i$ for any $i$, which then implies $[a,b]=e$. Hall even gives another hint: If $[a,b]\in G_i$, show that $[a,b]^m\in G_{i+1}$ and $[a,b]^n\in G_{i+1}$. Unfortunately I can't do that. I managed the case $m=2$: $$a^{-1}b^{-1}aba^{-1}b^{-1}ab=a^{-1}(b^{-1}a^{-1}ba)a(a^{-1}b^{-1}ab)=[a,[a,b]],$$ but I don't see how to generalize this. Another hint would be nice.",,"['abstract-algebra', 'group-theory']"
47,Singular points of orders of a number field,Singular points of orders of a number field,,"Let $K$ be a number field (finite field extension of $\mathbb{Q}$) and let $\mathcal{O}_{K}$ be its ring of integers (integral closure of $\mathbb{Z}$ in $K$). We know that $\mathcal{O}_{K}$ is a free $\mathbb{Z}$-module of rank $n=[K:\mathbb{Q}]$, and we know that it is a Dedekind domain. Hence $\operatorname{Spec}(\mathcal{O}_{K})$ is a regular curve. Let $\beta_{1},\ldots,\beta_{n}\in \mathcal{O}_{K}$ be a basis of $K$ over $\mathbb{Q}$. Then $\mathcal{O}=\mathbb{Z}[\beta_{1},\ldots,\beta_{n}]\subseteq \mathcal{O}_{K}$ is by definition an order of $K$ [ Remark (thanks to Hurkyl): we really want the $\beta_{i}$ to be a basis of $\mathcal{O}$ as a free $\mathbb{Z}$-module, see the comments below]. We also know that orders of $K$ are one-dimensional noetherian domains. So the only thing that stops $\mathcal{O}$ from being a Dedekind domain (and hence being equal to the maximal order $\mathcal{O}_{K}$) is that $\mathcal{O}$ may not be integrally closed. This means that $\operatorname{Spec}(\mathcal{O})$ is a curve which may have a bunch of singular points, and $\mathcal{O}=\mathcal{O}_{K}$ if and only if $\operatorname{Spec}(\mathcal{O})$ is a regular curve (we go from $\operatorname{Spec}(\mathcal{O})$ to $\operatorname{Spec}(\mathcal{O}_{K})$ by normalizing). We want to determine whether $\mathcal{O}=\mathcal{O}_{K}$ or not, and for this we have the following proposition: Equality holds if and only if for every prime $p$ such that   $$ p^{2} | \operatorname{disc}(\beta_{1},\ldots,\beta_{n})=\det(\operatorname{Tr}_{K/\mathbb{Q}}(\beta_{i}\beta_{j}))$$   we have that $\bar{\beta_{1}},\ldots,\bar{\beta_{n}}\in \mathcal{O}_{K}/p\mathcal{O}_{K}$ are linearly independent over $\mathbb{F}_{p}$. Given the previous geometric interpretation, I was hoping that what this proposition really does is ""looking for singularities"" in the curve $\operatorname{Spec}(\mathcal{O})$. So I tried to show that if the projections of the $\beta_{i}$ are linearly dependent over $\mathbb{F}_{p}$, then there is a prime $\mathfrak{p}$ above $p$ in $\mathcal{O}_{K}$ such that the localization $\mathcal{O}_{K,\mathfrak{p}}$ is not a discrete valuation ring. But I don't really see how to relate both things, and the proof of the proposition also doesn't help me see any connection. Any hints? And in particular is there any geometric interpretation of this proposition, say, in terms of Zariski (co)tangent spaces? Sketch the proof I know, in case it helps: We know an integral basis exists, so we may write our basis in terms of this integral basis. Since the discriminant of our basis is the discriminant of the integral basis times the square of the determinant of the transition matrix, equality holds if and only if the determinant of the transition matrix is plus or minus one. So if our basis is not an integral basis, some prime $p$ divides the determinant of the transition matrix (which implies that its square divides the discriminant of our basis). This means that the reduction modulo $p$ of the matrix has non-trivial kernel, and then any non-trivial element in the kernel gives us a non-trivial linear combination of the $\bar{\beta_{i}}$ which is equal to zero. So the $\bar{\beta_{i}}$ are not linearly independent over $\mathbb{F}_{p}$. Conversely, suppose there exists a $p$ such that we can find a non-trivial linear combination of the $\bar{\beta_{i}}$ equal to zero, hence a non-trivial element in the kernel of the reduction modulo $p$ of the transition matrix. Then the determinant of the transition matrix is divisible by $p$, and hence our basis is not an integral basis.","Let $K$ be a number field (finite field extension of $\mathbb{Q}$) and let $\mathcal{O}_{K}$ be its ring of integers (integral closure of $\mathbb{Z}$ in $K$). We know that $\mathcal{O}_{K}$ is a free $\mathbb{Z}$-module of rank $n=[K:\mathbb{Q}]$, and we know that it is a Dedekind domain. Hence $\operatorname{Spec}(\mathcal{O}_{K})$ is a regular curve. Let $\beta_{1},\ldots,\beta_{n}\in \mathcal{O}_{K}$ be a basis of $K$ over $\mathbb{Q}$. Then $\mathcal{O}=\mathbb{Z}[\beta_{1},\ldots,\beta_{n}]\subseteq \mathcal{O}_{K}$ is by definition an order of $K$ [ Remark (thanks to Hurkyl): we really want the $\beta_{i}$ to be a basis of $\mathcal{O}$ as a free $\mathbb{Z}$-module, see the comments below]. We also know that orders of $K$ are one-dimensional noetherian domains. So the only thing that stops $\mathcal{O}$ from being a Dedekind domain (and hence being equal to the maximal order $\mathcal{O}_{K}$) is that $\mathcal{O}$ may not be integrally closed. This means that $\operatorname{Spec}(\mathcal{O})$ is a curve which may have a bunch of singular points, and $\mathcal{O}=\mathcal{O}_{K}$ if and only if $\operatorname{Spec}(\mathcal{O})$ is a regular curve (we go from $\operatorname{Spec}(\mathcal{O})$ to $\operatorname{Spec}(\mathcal{O}_{K})$ by normalizing). We want to determine whether $\mathcal{O}=\mathcal{O}_{K}$ or not, and for this we have the following proposition: Equality holds if and only if for every prime $p$ such that   $$ p^{2} | \operatorname{disc}(\beta_{1},\ldots,\beta_{n})=\det(\operatorname{Tr}_{K/\mathbb{Q}}(\beta_{i}\beta_{j}))$$   we have that $\bar{\beta_{1}},\ldots,\bar{\beta_{n}}\in \mathcal{O}_{K}/p\mathcal{O}_{K}$ are linearly independent over $\mathbb{F}_{p}$. Given the previous geometric interpretation, I was hoping that what this proposition really does is ""looking for singularities"" in the curve $\operatorname{Spec}(\mathcal{O})$. So I tried to show that if the projections of the $\beta_{i}$ are linearly dependent over $\mathbb{F}_{p}$, then there is a prime $\mathfrak{p}$ above $p$ in $\mathcal{O}_{K}$ such that the localization $\mathcal{O}_{K,\mathfrak{p}}$ is not a discrete valuation ring. But I don't really see how to relate both things, and the proof of the proposition also doesn't help me see any connection. Any hints? And in particular is there any geometric interpretation of this proposition, say, in terms of Zariski (co)tangent spaces? Sketch the proof I know, in case it helps: We know an integral basis exists, so we may write our basis in terms of this integral basis. Since the discriminant of our basis is the discriminant of the integral basis times the square of the determinant of the transition matrix, equality holds if and only if the determinant of the transition matrix is plus or minus one. So if our basis is not an integral basis, some prime $p$ divides the determinant of the transition matrix (which implies that its square divides the discriminant of our basis). This means that the reduction modulo $p$ of the matrix has non-trivial kernel, and then any non-trivial element in the kernel gives us a non-trivial linear combination of the $\bar{\beta_{i}}$ which is equal to zero. So the $\bar{\beta_{i}}$ are not linearly independent over $\mathbb{F}_{p}$. Conversely, suppose there exists a $p$ such that we can find a non-trivial linear combination of the $\bar{\beta_{i}}$ equal to zero, hence a non-trivial element in the kernel of the reduction modulo $p$ of the transition matrix. Then the determinant of the transition matrix is divisible by $p$, and hence our basis is not an integral basis.",,"['abstract-algebra', 'number-theory', 'algebraic-geometry', 'algebraic-number-theory', 'arithmetic-geometry']"
48,"A problem of central simple algebras: why $(E,s,\gamma)\cong M_n(F)$ only if $\gamma$ is the norm of an element of $E$?",A problem of central simple algebras: why  only if  is the norm of an element of ?,"(E,s,\gamma)\cong M_n(F) \gamma E","I am stuck in the following problem, which is the exercise 6 of section 4.6 of N. Jacobson's Basic Algebra II : Problem. Prove that $(E,s,\gamma)\cong M_n(F)$ if and only if $\gamma$ is the norm of an element of $E$. Here $F$ is a field, and $E/F$ is a cyclic extension of degree $n$ with $\mathrm{Gal}(E/F)=\langle s\rangle$. $(E,s,\gamma)$ is an $n^2$-dimensional central simple algebra over $F$ generated by all matrices $\overline b=\mathrm{diag}\{b, s(b),\cdots,s^{n-1}(b)\}$ ( $\forall b\in E$ ) and  $$ z=\left(\begin{matrix} 0&1& & &\\ 0&0&1& &\\ \vdots& &\ddots&\ddots &\\ 0& \cdots&\cdots& 0& 1\\ \gamma&\cdots&\cdots & 0& 0 \end{matrix}\right) $$ for a fixed $\gamma\in F^\times$. Actually the ""if"" part is quite obvious as a concrete isomorphism can be formulated (Indeed, the correspondence $\lambda_b\mapsto\overline b$, $\lambda_as\mapsto z$ gives an isomorphism from $\mathrm{End}_FE$ to $(E,s,\gamma)$, where $\gamma=N_{E/F}(a)$ and $\lambda_b$ is the $\require{enclose}\enclose{horizontalstrike}F$-isomorphism  $\enclose{horizontalstrike}{x=\sum_{i=0}^{n-1}c_i\alpha^{i}\mapsto \sum_{i=0}^{n-1}c_is^{i}(b)\alpha^i}$, in which $\enclose{horizontalstrike}\alpha$ is a primitive element over $\enclose{horizontalstrike}F$). is the $F$-linear homomorphism $x\mapsto bx$. Where I am stuck is the ""only if"" part of this problem. Although the book gives a hint to use Skolem-Noether theorem, I can hardly find where to apply it. So I would like to ask how to obtain the implication $(E,s,\gamma)\cong M_n(F)$ $\Longrightarrow$ $\gamma=N_{E/F}(a)$ (i.e., $\gamma=as(a)\cdots s^{n-1}(a)$) for some $a\in E$. Any help would be greatly appreciated. Here are some attempts (I will continue to update until a solution is come up): I guess this is one probable way. If we denote the isomorphism by $f\colon (E,s,\gamma)\to M_n(F)$ , and let $\enclose{horizontalstrike}g$ be the monomorphism from the algebra generated by all $\enclose{horizontalstrike}{\overline b}$ to $\enclose{horizontalstrike}{M_n(F)}$ that sends $\enclose{horizontalstrike}{\overline b}$ to the matrix of $\enclose{horizontalstrike}{\lambda_b}$ under a basis of $\enclose{horizontalstrike}{E/F}$, then the Skolem-Noether theorem tells us that there is a matrix $\enclose{horizontalstrike}{M\in \mathrm{GL}_n(E)}$ such that $\enclose{horizontalstrike}{f(\cdot)=M^{-1}g(\cdot)M}$. Then  we can define $\enclose{horizontalstrike}{g(z)}$ and it remains to show that $\enclose{horizontalstrike}{g(z)}$ is the matrix of $\enclose{horizontalstrike}{as}$ for some $\enclose{horizontalstrike}{a\in E}$. However, this way seems not working and the use of inner automorphism is still unclear... A previous exercise of this section asserts that for each $x\in(E,s,\gamma)$, it can be written in a unique way as $$ x=\overline b_0+\overline b_1 z+\cdots+\overline b_{n-1}z^{n-1}. $$ If we can prove that $\enclose{horizontalstrike}{M\in \mathrm{GL}_n(F)}$, then $\enclose{horizontalstrike}{g(z)}$ corresponds a homomorphism $\enclose{horizontalstrike}{\zeta\in\mathrm{End}_FE}$. Moreover, since if so, $\enclose{horizontalstrike}{g\colon(E,s,\gamma)\to M_n(F)}$ is an $\enclose{horizontalstrike}F$-linear isomorphism, and thus $\enclose{horizontalstrike}{s\in\mathrm{End}_FE}$ can be written in a unique manner as follows $$ \enclose{horizontalstrike}{s=\lambda_{b_0}+\lambda_{b_1}\zeta+\cdots+\lambda_{b_{n-1}}\zeta^{n-1}} $$ where $\enclose{horizontalstrike}{b_0,\cdots,b_{n-1}\in E}$. Hopefully by $\enclose{horizontalstrike}{s^n=1}$ we perhaps can get that all $\enclose{horizontalstrike}{b_i=0}$ but $\enclose{horizontalstrike}{b_1}$ (at least this holds for $\enclose{horizontalstrike}{n=2}$), and then $\enclose{horizontalstrike}{\zeta=\lambda_{b_1^{-1}}s}$, which indicates $\enclose{horizontalstrike}{\lambda_\gamma=\zeta^n=(\lambda_{b_1^{-1}}s)^n}$, namely $\enclose{horizontalstrike}{\gamma=N_{E/F}(b_1^{-1})}$. Let us denote by $\iota\colon(E,s,\gamma)\hookrightarrow M_n(E)$ the natural embedding. Then by Skolem-Noether theorem, $\iota(\cdot)=N^{-1}f(\cdot)N$ for some $N\in\mathrm{GL}_n(E)$. Since $f$ is isomorphic, each $x\in M_n(F)$ can be written as $$ x=f(\overline b_0)+f(\overline b_1)f(z)+\cdots+f(\overline b_{n-1})f(z)^{n-1} $$ in a unique manner with $b_i\in E$. Note that $s\in \mathrm{End}_FE$ corresponds to a matrix  $$x_s=f(\overline b_0)+\cdots+f(\overline b_{n-1})f(z)^{n-1}\in M_n(F)$$  and $x_s^n=I\in M_n(F)$ (the identity matrix). If we can deduce that $f(\overline b_i)=0$ but $f(\overline b_1)$ (again, this holds at least for $n=2$), then it follows that $\enclose{horizontalstrike}{\det(f(\overline b_1z))^n=1}$, and thus $\enclose{horizontalstrike}{\det(\overline b_1z)^n=1\Longrightarrow N_{E/F}(b_1)^n\gamma^n=1}$. $x_s=f(b_1)f(z)$ and hence $\gamma I=N_{E/F}(b_1^{-1})I$. If it can be shown that $x_sf(\overline b)=f(s(\overline b))x_s$, then that $f(\overline b_i)=0$ but $f(\overline b_1)$ can be deduced, and the arguments before all make sense. A failed attempt: I tried to prove that the $m_b$ defined as follows corresponds to the matrix $f(\overline b)$ under some $F$-basis of $E$ but at last found that I made several fatal mistakes, having no idea how to fix them for the moment. I think this is very likely to be a working way. Assume that $f\colon(E,s,\gamma)\to M_n(F)\subset M_n(E)$. For the inclusion mapping $\iota\colon(E,s,\gamma)\hookrightarrow M_n(E)$, the Skolem-Noether theorem implies that  $$ f(\cdot)=N^{-1}\iota(\cdot)N $$ for some $N\in\mathrm{GL}_n(E)$. For a primitive element $\alpha$ of $E/F$, $B:=\{1,\alpha,\cdots,\alpha^{n-1}\}$ is an $F$-basis of $E$. Define for each $b\in E$, \begin{align} m_b\colon E&\to E\\ x=\sum_{i=0}^{n-1}c_i\alpha^i&\mapsto\sum_{i=0}^{n-1}c_is^i(b)\alpha^i \end{align} and it follows that $m_b$ is $F$-linear, i.e., $m_b\in\mathrm{End}_FE$. We assert that there is an $F$-basis $B'$ of $E$, under which the matrix of $m_b\ (\forall b\in E)$ is $f(\overline b)$. Indeed, for each $b\in E$, \begin{align} m_b(1,\alpha,\cdots,\alpha^{n-1})N=&(b,s(b)\alpha,\cdots,s^{n-1}(b)\alpha^{n-1})N\\ =&(1,\alpha,\cdots,\alpha^{n-1})\overline bN\\ =&(1,\alpha,\cdots,\alpha^{n-1})NN^{-1}\iota(\overline b)N\\ =&(1,\alpha,\cdots,\alpha^{n-1})Nf(\overline b). \end{align} Thus it remains to show that $B':=(1,\alpha,\cdots,\alpha^{n-1})N$ is an $F$-basis of $E$. Let us consider the matrix $$ V=\left( \begin{matrix} 1&\alpha&\cdots&\alpha^{n-1}\\ 1&s(\alpha)&\cdots&s(\alpha^{n-1})\\ \vdots&\vdots&&\vdots\\ 1&s^{n-1}(\alpha)&\cdots&s^{n-1}(\alpha^{n-1}) \end{matrix}\right)\in M_n(E). $$ Since $\mathrm{Gal}(E/F)=\langle s\rangle$ and $E=F(\alpha)$, $s^i(\alpha)\neq s^j(\alpha)$ for $i\neq j$ ($i, j=1,\cdots,n$). Therefore the Vandermonde matrix $V\in\mathrm{GL}_n(E)$. If $B'=(1,\alpha,\cdots,\alpha^{n-1})N$ is $F$-linear dependent, then the columns of $\enclose{horizontalstrike}{VN}$ are $\enclose{horizontalstrike}{F}$-(and of course $\enclose{horizontalstrike}{E}$-)linear dependent (since $\enclose{horizontalstrike}{s^i}$ fixes elements of $\enclose{horizontalstrike}{F}$). As $\enclose{horizontalstrike}{N}$ is invertible, it leads to a contradiction that $\enclose{horizontalstrike}{V\notin\mathrm{GL}_n(E)}$. Therefore $\enclose{horizontalstrike}{B'}$ is an $\enclose{horizontalstrike}{F}$-basis of $\enclose{horizontalstrike}{E}$. Since $M_n(F)\cong \mathrm{End}_FE$, under the basis $B'$, $s$ corresponds to a matrix $x_s\in M_n(F)$. Then $$ x_s=f(\overline b_0)+f(\overline b_1)f(z)+\cdots+f(\overline b_{n-1})f(z^{n-1}) $$ for some $b_0,\cdots,b_{n-1}\in E$. Note that $\enclose{horizontalstrike}{sm_b=m_{s(b)}s}$ and we have for each $\enclose{horizontalstrike}{b\in E}$, $\enclose{horizontalstrike}{x_sf(\overline b)=f(s(\overline b))x_s}$, while \begin{align} x_sf(\overline b)=&f(\overline b_0)f(\overline b)+f(\overline b_1)f(z)f(\overline b)+\cdots+f(\overline b_{n-1})f(z)^{n-1}f(\overline b),\\ f(s(\overline b))x_s=&f(s(\overline b))f(\overline b_0)+f(s(\overline b))f(\overline b_1)f(z)+\cdots+f(s(\overline b))f(\overline b_{n-1})f(z)^{n-1}. \end{align} The exercise 4 of section 4.6 of N. Jacobson's Basic Algebra II asserts that Each $x\in (E,s,\gamma)$ can be written in a unique way as $$x=\overline b_0+\overline b_1z+\cdots+\overline b_{n-1}z^{n-1},$$ where $b_i\in E$. Thus it follows that \begin{align} &f(\overline b_0)f(\overline b)=f(s(\overline b))f(\overline b_0),\\ &f(\overline b_1)f(s(\overline b))=f(s(\overline b))f(\overline b_1),\\ &\quad\vdots\\ &f(b_{n-1})f(s^{n-1}(\overline b))=f(s(\overline b))f(\overline b_{n-1}). \end{align} The arbitrariness of $b\in E$ forces that $b_0=b_2=\cdots=b_{n-1}=0$ (precisely speaking,  for each $i=0,2,\cdots,n-1$, pick a $b\in E\setminus F$ which is not fixed by $s^{i-1}$, and then $f(\overline b_i)f(s^i(\overline b))=f(s(\overline b))f(\overline b_i)=f(\overline b_i)f(s(\overline b))$; if $b_i\neq 0$, $f$ being isomorphic entails that $f(s^i(\overline b))=f(s(\overline b))$ and thence, $b=s^{i-1}(b)$, which is a contradiction). Hence $x_s=f(\overline b_1)f(z)$, and the invertibility of $x_s$ implies that $b_1\neq 0$. We thus have \begin{align} \gamma I=&f(z)^n=(f(\overline b_1)^{-1}x_s)^n=(f(\overline{b_1^{-1}})x_s)^n\\ =&f(\overline{b_1^{-1}})f(s(\overline{b_1^{-1}}))\cdots f(s^{n-1}(\overline{b_1^{-1}}))x_s^n\\ =&N_{E/F}(b_1^{-1})I, \end{align} i.e., $\gamma=N_{E/F}(a)$, where $a=b_1^{-1}\in E^\times$.","I am stuck in the following problem, which is the exercise 6 of section 4.6 of N. Jacobson's Basic Algebra II : Problem. Prove that $(E,s,\gamma)\cong M_n(F)$ if and only if $\gamma$ is the norm of an element of $E$. Here $F$ is a field, and $E/F$ is a cyclic extension of degree $n$ with $\mathrm{Gal}(E/F)=\langle s\rangle$. $(E,s,\gamma)$ is an $n^2$-dimensional central simple algebra over $F$ generated by all matrices $\overline b=\mathrm{diag}\{b, s(b),\cdots,s^{n-1}(b)\}$ ( $\forall b\in E$ ) and  $$ z=\left(\begin{matrix} 0&1& & &\\ 0&0&1& &\\ \vdots& &\ddots&\ddots &\\ 0& \cdots&\cdots& 0& 1\\ \gamma&\cdots&\cdots & 0& 0 \end{matrix}\right) $$ for a fixed $\gamma\in F^\times$. Actually the ""if"" part is quite obvious as a concrete isomorphism can be formulated (Indeed, the correspondence $\lambda_b\mapsto\overline b$, $\lambda_as\mapsto z$ gives an isomorphism from $\mathrm{End}_FE$ to $(E,s,\gamma)$, where $\gamma=N_{E/F}(a)$ and $\lambda_b$ is the $\require{enclose}\enclose{horizontalstrike}F$-isomorphism  $\enclose{horizontalstrike}{x=\sum_{i=0}^{n-1}c_i\alpha^{i}\mapsto \sum_{i=0}^{n-1}c_is^{i}(b)\alpha^i}$, in which $\enclose{horizontalstrike}\alpha$ is a primitive element over $\enclose{horizontalstrike}F$). is the $F$-linear homomorphism $x\mapsto bx$. Where I am stuck is the ""only if"" part of this problem. Although the book gives a hint to use Skolem-Noether theorem, I can hardly find where to apply it. So I would like to ask how to obtain the implication $(E,s,\gamma)\cong M_n(F)$ $\Longrightarrow$ $\gamma=N_{E/F}(a)$ (i.e., $\gamma=as(a)\cdots s^{n-1}(a)$) for some $a\in E$. Any help would be greatly appreciated. Here are some attempts (I will continue to update until a solution is come up): I guess this is one probable way. If we denote the isomorphism by $f\colon (E,s,\gamma)\to M_n(F)$ , and let $\enclose{horizontalstrike}g$ be the monomorphism from the algebra generated by all $\enclose{horizontalstrike}{\overline b}$ to $\enclose{horizontalstrike}{M_n(F)}$ that sends $\enclose{horizontalstrike}{\overline b}$ to the matrix of $\enclose{horizontalstrike}{\lambda_b}$ under a basis of $\enclose{horizontalstrike}{E/F}$, then the Skolem-Noether theorem tells us that there is a matrix $\enclose{horizontalstrike}{M\in \mathrm{GL}_n(E)}$ such that $\enclose{horizontalstrike}{f(\cdot)=M^{-1}g(\cdot)M}$. Then  we can define $\enclose{horizontalstrike}{g(z)}$ and it remains to show that $\enclose{horizontalstrike}{g(z)}$ is the matrix of $\enclose{horizontalstrike}{as}$ for some $\enclose{horizontalstrike}{a\in E}$. However, this way seems not working and the use of inner automorphism is still unclear... A previous exercise of this section asserts that for each $x\in(E,s,\gamma)$, it can be written in a unique way as $$ x=\overline b_0+\overline b_1 z+\cdots+\overline b_{n-1}z^{n-1}. $$ If we can prove that $\enclose{horizontalstrike}{M\in \mathrm{GL}_n(F)}$, then $\enclose{horizontalstrike}{g(z)}$ corresponds a homomorphism $\enclose{horizontalstrike}{\zeta\in\mathrm{End}_FE}$. Moreover, since if so, $\enclose{horizontalstrike}{g\colon(E,s,\gamma)\to M_n(F)}$ is an $\enclose{horizontalstrike}F$-linear isomorphism, and thus $\enclose{horizontalstrike}{s\in\mathrm{End}_FE}$ can be written in a unique manner as follows $$ \enclose{horizontalstrike}{s=\lambda_{b_0}+\lambda_{b_1}\zeta+\cdots+\lambda_{b_{n-1}}\zeta^{n-1}} $$ where $\enclose{horizontalstrike}{b_0,\cdots,b_{n-1}\in E}$. Hopefully by $\enclose{horizontalstrike}{s^n=1}$ we perhaps can get that all $\enclose{horizontalstrike}{b_i=0}$ but $\enclose{horizontalstrike}{b_1}$ (at least this holds for $\enclose{horizontalstrike}{n=2}$), and then $\enclose{horizontalstrike}{\zeta=\lambda_{b_1^{-1}}s}$, which indicates $\enclose{horizontalstrike}{\lambda_\gamma=\zeta^n=(\lambda_{b_1^{-1}}s)^n}$, namely $\enclose{horizontalstrike}{\gamma=N_{E/F}(b_1^{-1})}$. Let us denote by $\iota\colon(E,s,\gamma)\hookrightarrow M_n(E)$ the natural embedding. Then by Skolem-Noether theorem, $\iota(\cdot)=N^{-1}f(\cdot)N$ for some $N\in\mathrm{GL}_n(E)$. Since $f$ is isomorphic, each $x\in M_n(F)$ can be written as $$ x=f(\overline b_0)+f(\overline b_1)f(z)+\cdots+f(\overline b_{n-1})f(z)^{n-1} $$ in a unique manner with $b_i\in E$. Note that $s\in \mathrm{End}_FE$ corresponds to a matrix  $$x_s=f(\overline b_0)+\cdots+f(\overline b_{n-1})f(z)^{n-1}\in M_n(F)$$  and $x_s^n=I\in M_n(F)$ (the identity matrix). If we can deduce that $f(\overline b_i)=0$ but $f(\overline b_1)$ (again, this holds at least for $n=2$), then it follows that $\enclose{horizontalstrike}{\det(f(\overline b_1z))^n=1}$, and thus $\enclose{horizontalstrike}{\det(\overline b_1z)^n=1\Longrightarrow N_{E/F}(b_1)^n\gamma^n=1}$. $x_s=f(b_1)f(z)$ and hence $\gamma I=N_{E/F}(b_1^{-1})I$. If it can be shown that $x_sf(\overline b)=f(s(\overline b))x_s$, then that $f(\overline b_i)=0$ but $f(\overline b_1)$ can be deduced, and the arguments before all make sense. A failed attempt: I tried to prove that the $m_b$ defined as follows corresponds to the matrix $f(\overline b)$ under some $F$-basis of $E$ but at last found that I made several fatal mistakes, having no idea how to fix them for the moment. I think this is very likely to be a working way. Assume that $f\colon(E,s,\gamma)\to M_n(F)\subset M_n(E)$. For the inclusion mapping $\iota\colon(E,s,\gamma)\hookrightarrow M_n(E)$, the Skolem-Noether theorem implies that  $$ f(\cdot)=N^{-1}\iota(\cdot)N $$ for some $N\in\mathrm{GL}_n(E)$. For a primitive element $\alpha$ of $E/F$, $B:=\{1,\alpha,\cdots,\alpha^{n-1}\}$ is an $F$-basis of $E$. Define for each $b\in E$, \begin{align} m_b\colon E&\to E\\ x=\sum_{i=0}^{n-1}c_i\alpha^i&\mapsto\sum_{i=0}^{n-1}c_is^i(b)\alpha^i \end{align} and it follows that $m_b$ is $F$-linear, i.e., $m_b\in\mathrm{End}_FE$. We assert that there is an $F$-basis $B'$ of $E$, under which the matrix of $m_b\ (\forall b\in E)$ is $f(\overline b)$. Indeed, for each $b\in E$, \begin{align} m_b(1,\alpha,\cdots,\alpha^{n-1})N=&(b,s(b)\alpha,\cdots,s^{n-1}(b)\alpha^{n-1})N\\ =&(1,\alpha,\cdots,\alpha^{n-1})\overline bN\\ =&(1,\alpha,\cdots,\alpha^{n-1})NN^{-1}\iota(\overline b)N\\ =&(1,\alpha,\cdots,\alpha^{n-1})Nf(\overline b). \end{align} Thus it remains to show that $B':=(1,\alpha,\cdots,\alpha^{n-1})N$ is an $F$-basis of $E$. Let us consider the matrix $$ V=\left( \begin{matrix} 1&\alpha&\cdots&\alpha^{n-1}\\ 1&s(\alpha)&\cdots&s(\alpha^{n-1})\\ \vdots&\vdots&&\vdots\\ 1&s^{n-1}(\alpha)&\cdots&s^{n-1}(\alpha^{n-1}) \end{matrix}\right)\in M_n(E). $$ Since $\mathrm{Gal}(E/F)=\langle s\rangle$ and $E=F(\alpha)$, $s^i(\alpha)\neq s^j(\alpha)$ for $i\neq j$ ($i, j=1,\cdots,n$). Therefore the Vandermonde matrix $V\in\mathrm{GL}_n(E)$. If $B'=(1,\alpha,\cdots,\alpha^{n-1})N$ is $F$-linear dependent, then the columns of $\enclose{horizontalstrike}{VN}$ are $\enclose{horizontalstrike}{F}$-(and of course $\enclose{horizontalstrike}{E}$-)linear dependent (since $\enclose{horizontalstrike}{s^i}$ fixes elements of $\enclose{horizontalstrike}{F}$). As $\enclose{horizontalstrike}{N}$ is invertible, it leads to a contradiction that $\enclose{horizontalstrike}{V\notin\mathrm{GL}_n(E)}$. Therefore $\enclose{horizontalstrike}{B'}$ is an $\enclose{horizontalstrike}{F}$-basis of $\enclose{horizontalstrike}{E}$. Since $M_n(F)\cong \mathrm{End}_FE$, under the basis $B'$, $s$ corresponds to a matrix $x_s\in M_n(F)$. Then $$ x_s=f(\overline b_0)+f(\overline b_1)f(z)+\cdots+f(\overline b_{n-1})f(z^{n-1}) $$ for some $b_0,\cdots,b_{n-1}\in E$. Note that $\enclose{horizontalstrike}{sm_b=m_{s(b)}s}$ and we have for each $\enclose{horizontalstrike}{b\in E}$, $\enclose{horizontalstrike}{x_sf(\overline b)=f(s(\overline b))x_s}$, while \begin{align} x_sf(\overline b)=&f(\overline b_0)f(\overline b)+f(\overline b_1)f(z)f(\overline b)+\cdots+f(\overline b_{n-1})f(z)^{n-1}f(\overline b),\\ f(s(\overline b))x_s=&f(s(\overline b))f(\overline b_0)+f(s(\overline b))f(\overline b_1)f(z)+\cdots+f(s(\overline b))f(\overline b_{n-1})f(z)^{n-1}. \end{align} The exercise 4 of section 4.6 of N. Jacobson's Basic Algebra II asserts that Each $x\in (E,s,\gamma)$ can be written in a unique way as $$x=\overline b_0+\overline b_1z+\cdots+\overline b_{n-1}z^{n-1},$$ where $b_i\in E$. Thus it follows that \begin{align} &f(\overline b_0)f(\overline b)=f(s(\overline b))f(\overline b_0),\\ &f(\overline b_1)f(s(\overline b))=f(s(\overline b))f(\overline b_1),\\ &\quad\vdots\\ &f(b_{n-1})f(s^{n-1}(\overline b))=f(s(\overline b))f(\overline b_{n-1}). \end{align} The arbitrariness of $b\in E$ forces that $b_0=b_2=\cdots=b_{n-1}=0$ (precisely speaking,  for each $i=0,2,\cdots,n-1$, pick a $b\in E\setminus F$ which is not fixed by $s^{i-1}$, and then $f(\overline b_i)f(s^i(\overline b))=f(s(\overline b))f(\overline b_i)=f(\overline b_i)f(s(\overline b))$; if $b_i\neq 0$, $f$ being isomorphic entails that $f(s^i(\overline b))=f(s(\overline b))$ and thence, $b=s^{i-1}(b)$, which is a contradiction). Hence $x_s=f(\overline b_1)f(z)$, and the invertibility of $x_s$ implies that $b_1\neq 0$. We thus have \begin{align} \gamma I=&f(z)^n=(f(\overline b_1)^{-1}x_s)^n=(f(\overline{b_1^{-1}})x_s)^n\\ =&f(\overline{b_1^{-1}})f(s(\overline{b_1^{-1}}))\cdots f(s^{n-1}(\overline{b_1^{-1}}))x_s^n\\ =&N_{E/F}(b_1^{-1})I, \end{align} i.e., $\gamma=N_{E/F}(a)$, where $a=b_1^{-1}\in E^\times$.",,"['abstract-algebra', 'ring-theory', 'representation-theory', 'noncommutative-algebra', 'semi-simple-rings']"
49,Prove that the given polynomial is irreducible in $\mathbb{Z}[X]$.,Prove that the given polynomial is irreducible in .,\mathbb{Z}[X],"Show that if $p$ is a prime and  $\gcd(n_1,...,n_p)=d$, then $$\dfrac{(\sum_{k=1}^px^{n_k})-p}{x^d-1}$$ is irreducible in $\mathbb{Z}[X]$. All I can do is to write the numerator as $\sum(x^{n_k}-1)$ to prove it is really in $\mathbb{Z}[X]$ by doing the factorization, but I have no idea in this example how to use that ""p is prime"" after the factorization. Would someone give me a hint?","Show that if $p$ is a prime and  $\gcd(n_1,...,n_p)=d$, then $$\dfrac{(\sum_{k=1}^px^{n_k})-p}{x^d-1}$$ is irreducible in $\mathbb{Z}[X]$. All I can do is to write the numerator as $\sum(x^{n_k}-1)$ to prove it is really in $\mathbb{Z}[X]$ by doing the factorization, but I have no idea in this example how to use that ""p is prime"" after the factorization. Would someone give me a hint?",,"['abstract-algebra', 'polynomials', 'commutative-algebra', 'irreducible-polynomials']"
50,"Pedagogical examples of distinguishing ""types of symmetry""","Pedagogical examples of distinguishing ""types of symmetry""",,"When speaking to interested parties lacking formal mathematical background, I've illustrated how different objects can have the same number of symmetries and yet have different types of symmetry with the example of an oriented hexagon (obtained by putting arrows on the edges in cyclic fashion) versus an unadorned triangle. These have cyclic and dihedral symmetry groups, respectively, each with six symmetries. There are multiple reasons these are distinct types of symmetry: (a) the rotations of the hexagon can all be generated by a single symmetry (unlike with the triangle, where flips and rotations cannot be generated by a single symmetry); (b) any two rotations of the oriented hexagon (by $x$ and $y$ degrees) can be performed in either order, but applying a flip and a rotation of the triangle in different orders yield distinct symmetries (they permute the vertices differently); (c) one can tabulate how many applications of each symmetry is required to achieve an initial state, and notice we obtain a different list of numbers for the two symmetry groups. In other words, cyclicity, commutativity, and order information. However, when I've asked people to identify a reason the triangle and oriented hexagon have different types of symmetry, they've consistently (and understandably) identified the fact the triangle has reflectional symmetry while the oriented hexagon doesn't. Which is a perfectly valid answer, because my question is ambiguous: they are identifying why the individual symmetries are different types of transformations, instead of looking at the internal structure of the symmetry group. When I explain I am interested in answers that look instead at how the symmetries interact with each other, I am met with confusion, although when I explain (a), (b) and (c) above it is more clear what I intend. Question : What are some pedagogical examples of distinguishing basic symmetry groups that avoid this pitfall? To be specific, two types of situations could fit the bill: Different symmetry groups where the individual symmetries are the same types of transformations of some figure or basic mathematical object. Different figures or mathematical objects with the same symmetry group but where some individual symmetries do different types of things. Examples of type (1) would be more powerful in my opinion, but for added effect examples of type (2) may be explored first. Preferably examples should be basic, close to napkin-level math. Here's the simplest case of (2) which I thought of: (i) on the one hand, consider a kind of ""sunset"" picture that has reflectional symmetry across the horizon, but say there's a seagull on the right or left side that precludes sliding or vertical reflections from being symmetries, and (ii) on the other hand, the outline of the yin-yang symbol (so, not painting it in), which has no reflectional symmetries but has a single nontrivial $180^{\circ}$ rotational symmetry.","When speaking to interested parties lacking formal mathematical background, I've illustrated how different objects can have the same number of symmetries and yet have different types of symmetry with the example of an oriented hexagon (obtained by putting arrows on the edges in cyclic fashion) versus an unadorned triangle. These have cyclic and dihedral symmetry groups, respectively, each with six symmetries. There are multiple reasons these are distinct types of symmetry: (a) the rotations of the hexagon can all be generated by a single symmetry (unlike with the triangle, where flips and rotations cannot be generated by a single symmetry); (b) any two rotations of the oriented hexagon (by $x$ and $y$ degrees) can be performed in either order, but applying a flip and a rotation of the triangle in different orders yield distinct symmetries (they permute the vertices differently); (c) one can tabulate how many applications of each symmetry is required to achieve an initial state, and notice we obtain a different list of numbers for the two symmetry groups. In other words, cyclicity, commutativity, and order information. However, when I've asked people to identify a reason the triangle and oriented hexagon have different types of symmetry, they've consistently (and understandably) identified the fact the triangle has reflectional symmetry while the oriented hexagon doesn't. Which is a perfectly valid answer, because my question is ambiguous: they are identifying why the individual symmetries are different types of transformations, instead of looking at the internal structure of the symmetry group. When I explain I am interested in answers that look instead at how the symmetries interact with each other, I am met with confusion, although when I explain (a), (b) and (c) above it is more clear what I intend. Question : What are some pedagogical examples of distinguishing basic symmetry groups that avoid this pitfall? To be specific, two types of situations could fit the bill: Different symmetry groups where the individual symmetries are the same types of transformations of some figure or basic mathematical object. Different figures or mathematical objects with the same symmetry group but where some individual symmetries do different types of things. Examples of type (1) would be more powerful in my opinion, but for added effect examples of type (2) may be explored first. Preferably examples should be basic, close to napkin-level math. Here's the simplest case of (2) which I thought of: (i) on the one hand, consider a kind of ""sunset"" picture that has reflectional symmetry across the horizon, but say there's a seagull on the right or left side that precludes sliding or vertical reflections from being symmetries, and (ii) on the other hand, the outline of the yin-yang symbol (so, not painting it in), which has no reflectional symmetries but has a single nontrivial $180^{\circ}$ rotational symmetry.",,"['abstract-algebra', 'group-theory', 'education', 'symmetry']"
51,Group Presentation of the Direct Product.,Group Presentation of the Direct Product.,,"This is Exercise 1.2.5 and Exercise 1.2.6 of ""Combinatorial Group Theory: Presentations of Groups in Terms of Generators and Relations,"" by Magnus et al. The Details: Definition 1: Let $$\langle a, b ,c, \dots \mid P, Q, R, \dots \rangle$$ be a group presentation, where $P, Q, R, \dots$ are relators, not relations ( i.e. , words, not equations). We say the words $W_1$ and $W_2$ in $a, b, c, \dots$ are equivalent , denoted $W_1\sim W_2$, if the following operations applied a finite number of times, change $W_1$ into $W_2$: (i) Insertion of one of the words $P, P^{-1}, Q, Q^{-1}, R, R^{-1}, \dots$ or one of the trivial relators between any two consecutive symbols of $W_1$, or before $W_1$, or after $W_1$. (ii) Deletion of one of the words $P, P^{-1}, Q, Q^{-1}, R, R^{-1}, \dots$ or one of the trivial relators, if it forms a block of consecutive symbols in $W_1$. The Question(s): Exercise 1.2.5 Suppose $G=\langle a, b\mid P(a, b), Q(a, b)\rangle$ and $H=\langle x, y\mid S(x, y), T(x, y)\rangle$. Then show that the direct product $G\times H$ has the presentation   $$\langle a, b, x, y\mid P(a, b), Q(a, b), S(x, y), T(x, y), ax=xa, ay=ya, bx=xb, by=yb\rangle.$$ [$\color{red}{\text{Hint}}$: If $G$ is presented under the mapping $\theta: a\mapsto g, b\mapsto g'$, and $H$ is presented under the mapping $\phi: x\mapsto h, y\mapsto h'$, then show that the combined mapping $\theta\times \phi:a\mapsto (g, 1), b\mapsto (g', 1), x\mapsto (1, h), b\mapsto (1, h')$ determines a homomorphism of the alleged presentation for $G\times H$ onto $G\times H$. Next show that each element of the alleged presentation can be defined by a word $U(a, b)V(x, y)$. Show that if $U(a, b)V(x, y)\sim U'(a, b)V'(x, y)$, then $U(g, g')=U'(g, g')$ and $V(h, h')=V'(h, h')$ by mapping the alleged presentation for $G\times H$ into $G$ under $a\mapsto g, b\mapsto g', x\mapsto 1, y\mapsto 1$, and into $H$ under $a\mapsto 1, b\mapsto 1, x\mapsto h, y\mapsto h'$.] Exercise 1.2.6: Generalise Exercise 1.2.5 to arbitrary presentations $G$ and $H$. Generalise Exercise 1.2.5 to an arbitrary number of groups $G, H, \dots$. My Attempt: The hint for Exercise 1.2.5 is very detailed. The map $\theta\times\phi$ clearly defines a homomorphism from the presentation to $G\times H$ because on the generators, for example, $$\begin{align}(\theta\times\phi)(ab)&=(gg', 1) \\ &=(g\times g', 1\times 1) \\ &=(g, 1)\times (g', 1) \\ &=(\theta\times\phi)(a)(\theta\times\phi)(b). \end{align}$$ That the generators of $G$ commute with the generators of $H$ in the alleged presentation for $G\times H$ means we can clearly move blocks in $\{a, b\}$ in a word past and to the left of $x, y$, resulting in a word of the form $U(a, b)V(x, y)$. I have no idea how to show that if $U(a, b)V(x, y)\sim U'(a, b)V'(x, y)$, then $U(g, g')=U'(g, g')$ and $V(h, h')=V'(h, h')$. As for Exercise 1.2.6 , I have found that, for $G=\langle \mathcal G_G\mid \mathcal R_G\rangle$ and $H=\langle \mathcal G_H\mid \mathcal R_H\rangle$, $G\times H$ has the presentation $$\langle \mathcal G_G\cup \mathcal G_H\mid \mathcal R_G\cup\mathcal R_H\cup\{xy=yx : x\in \mathcal G_G, y\in \mathcal G_H\}\rangle,$$ though I don't know how to prove it. I have no idea how to generalise it to an arbitrary number of groups. Please help :)","This is Exercise 1.2.5 and Exercise 1.2.6 of ""Combinatorial Group Theory: Presentations of Groups in Terms of Generators and Relations,"" by Magnus et al. The Details: Definition 1: Let $$\langle a, b ,c, \dots \mid P, Q, R, \dots \rangle$$ be a group presentation, where $P, Q, R, \dots$ are relators, not relations ( i.e. , words, not equations). We say the words $W_1$ and $W_2$ in $a, b, c, \dots$ are equivalent , denoted $W_1\sim W_2$, if the following operations applied a finite number of times, change $W_1$ into $W_2$: (i) Insertion of one of the words $P, P^{-1}, Q, Q^{-1}, R, R^{-1}, \dots$ or one of the trivial relators between any two consecutive symbols of $W_1$, or before $W_1$, or after $W_1$. (ii) Deletion of one of the words $P, P^{-1}, Q, Q^{-1}, R, R^{-1}, \dots$ or one of the trivial relators, if it forms a block of consecutive symbols in $W_1$. The Question(s): Exercise 1.2.5 Suppose $G=\langle a, b\mid P(a, b), Q(a, b)\rangle$ and $H=\langle x, y\mid S(x, y), T(x, y)\rangle$. Then show that the direct product $G\times H$ has the presentation   $$\langle a, b, x, y\mid P(a, b), Q(a, b), S(x, y), T(x, y), ax=xa, ay=ya, bx=xb, by=yb\rangle.$$ [$\color{red}{\text{Hint}}$: If $G$ is presented under the mapping $\theta: a\mapsto g, b\mapsto g'$, and $H$ is presented under the mapping $\phi: x\mapsto h, y\mapsto h'$, then show that the combined mapping $\theta\times \phi:a\mapsto (g, 1), b\mapsto (g', 1), x\mapsto (1, h), b\mapsto (1, h')$ determines a homomorphism of the alleged presentation for $G\times H$ onto $G\times H$. Next show that each element of the alleged presentation can be defined by a word $U(a, b)V(x, y)$. Show that if $U(a, b)V(x, y)\sim U'(a, b)V'(x, y)$, then $U(g, g')=U'(g, g')$ and $V(h, h')=V'(h, h')$ by mapping the alleged presentation for $G\times H$ into $G$ under $a\mapsto g, b\mapsto g', x\mapsto 1, y\mapsto 1$, and into $H$ under $a\mapsto 1, b\mapsto 1, x\mapsto h, y\mapsto h'$.] Exercise 1.2.6: Generalise Exercise 1.2.5 to arbitrary presentations $G$ and $H$. Generalise Exercise 1.2.5 to an arbitrary number of groups $G, H, \dots$. My Attempt: The hint for Exercise 1.2.5 is very detailed. The map $\theta\times\phi$ clearly defines a homomorphism from the presentation to $G\times H$ because on the generators, for example, $$\begin{align}(\theta\times\phi)(ab)&=(gg', 1) \\ &=(g\times g', 1\times 1) \\ &=(g, 1)\times (g', 1) \\ &=(\theta\times\phi)(a)(\theta\times\phi)(b). \end{align}$$ That the generators of $G$ commute with the generators of $H$ in the alleged presentation for $G\times H$ means we can clearly move blocks in $\{a, b\}$ in a word past and to the left of $x, y$, resulting in a word of the form $U(a, b)V(x, y)$. I have no idea how to show that if $U(a, b)V(x, y)\sim U'(a, b)V'(x, y)$, then $U(g, g')=U'(g, g')$ and $V(h, h')=V'(h, h')$. As for Exercise 1.2.6 , I have found that, for $G=\langle \mathcal G_G\mid \mathcal R_G\rangle$ and $H=\langle \mathcal G_H\mid \mathcal R_H\rangle$, $G\times H$ has the presentation $$\langle \mathcal G_G\cup \mathcal G_H\mid \mathcal R_G\cup\mathcal R_H\cup\{xy=yx : x\in \mathcal G_G, y\in \mathcal G_H\}\rangle,$$ though I don't know how to prove it. I have no idea how to generalise it to an arbitrary number of groups. Please help :)",,"['abstract-algebra', 'group-theory', 'group-presentation', 'direct-product', 'combinatorial-group-theory']"
52,"Given two algebraic conjugates $\alpha,\beta$ and their minimal polynomial, find a polynomial that vanishes at $\alpha\beta$ in a efficient way","Given two algebraic conjugates  and their minimal polynomial, find a polynomial that vanishes at  in a efficient way","\alpha,\beta \alpha\beta","Inspired by this question , I was wondering about the following problem. $\alpha,\beta,\gamma,\ldots$ are the roots of an irreducible   polynomial over $\mathbb{Q}$. How to compute the coefficients of $$  (x-\alpha\beta)(x-\alpha\gamma)\cdot \ldots\cdot (x-\beta\gamma)\cdot\ldots, $$ i.e. the candidate minimal polynomial   of $\alpha\beta$, in the most efficient way? My approach is based on a simple lemma and a general fact. If $p_m$ is the power sum $\alpha^m+\beta^m+\gamma^m+\ldots$ and $e_i$ is the $i$-th elementary symmetric polynomial , the identity $$ \exp\left(-\sum_{m\geq 1}\frac{p_m}{m}x^m\right) = \sum_{r\geq 0}(-1)^r e_r\,x^r$$ gives a way to compute $p_1,p_2,p_3,\ldots$ from $e_1,e_2,e_3,\ldots$ through the Taylor series of a logarithm ($\text{LOG}$) as well as $e_1,e_2,e_3,\ldots$ from $p_1,p_2,p_3,\ldots$ through the Taylor series of an exponential ($\text{EXP}$). Moreover, the characteristic polynomial of the sequence $\{p_k\}_{k\geq 0}$ is exactly the minimal polynomial of $\alpha$, hence we may compute $p_{n+1},p_{n+2},\ldots,p_{n+m}$ from $p_1,p_2,\ldots,p_n$ with a simple recursive approach (I will call this procedure $\text{CH}$, from Cayley-Hamilton). $\text{L}$ will be my previously mentioned lemma, i.e. $$ p_k(\alpha\beta,\alpha\gamma,\ldots) = \frac{1}{2}\left(p_k^2-p_{2k}\right).$$ Now my algorithm goes as follows: (1). $$ e_i(\alpha,\beta,\gamma,\ldots)_{i\leq n}\quad\xrightarrow{\text{LOG}}\quad p_i(\alpha,\beta,\gamma,\ldots)_{i\leq n}$$  (2). $$ p_i(\alpha,\beta,\gamma,\ldots)_{i\leq n}\quad\xrightarrow{\text{CH}}\quad p_i(\alpha,\beta,\gamma,\ldots)_{i\leq n(n-1)}$$  (3). $$ p_i(\alpha,\beta,\gamma,\ldots)_{i\leq n(n-1)}\quad\xrightarrow{\text{L}}\quad p_i(\alpha\beta,\alpha\gamma,\ldots)_{i\leq \binom{n}{2}}$$  (4). $$ p_i(\alpha\beta,\alpha\gamma,\ldots)_{i\leq \binom{n}{2}}\quad\xrightarrow{\text{EXP}}\quad e_i(\alpha\beta,\alpha\gamma,\ldots)_{i\leq \binom{n}{2}}.$$","Inspired by this question , I was wondering about the following problem. $\alpha,\beta,\gamma,\ldots$ are the roots of an irreducible   polynomial over $\mathbb{Q}$. How to compute the coefficients of $$  (x-\alpha\beta)(x-\alpha\gamma)\cdot \ldots\cdot (x-\beta\gamma)\cdot\ldots, $$ i.e. the candidate minimal polynomial   of $\alpha\beta$, in the most efficient way? My approach is based on a simple lemma and a general fact. If $p_m$ is the power sum $\alpha^m+\beta^m+\gamma^m+\ldots$ and $e_i$ is the $i$-th elementary symmetric polynomial , the identity $$ \exp\left(-\sum_{m\geq 1}\frac{p_m}{m}x^m\right) = \sum_{r\geq 0}(-1)^r e_r\,x^r$$ gives a way to compute $p_1,p_2,p_3,\ldots$ from $e_1,e_2,e_3,\ldots$ through the Taylor series of a logarithm ($\text{LOG}$) as well as $e_1,e_2,e_3,\ldots$ from $p_1,p_2,p_3,\ldots$ through the Taylor series of an exponential ($\text{EXP}$). Moreover, the characteristic polynomial of the sequence $\{p_k\}_{k\geq 0}$ is exactly the minimal polynomial of $\alpha$, hence we may compute $p_{n+1},p_{n+2},\ldots,p_{n+m}$ from $p_1,p_2,\ldots,p_n$ with a simple recursive approach (I will call this procedure $\text{CH}$, from Cayley-Hamilton). $\text{L}$ will be my previously mentioned lemma, i.e. $$ p_k(\alpha\beta,\alpha\gamma,\ldots) = \frac{1}{2}\left(p_k^2-p_{2k}\right).$$ Now my algorithm goes as follows: (1). $$ e_i(\alpha,\beta,\gamma,\ldots)_{i\leq n}\quad\xrightarrow{\text{LOG}}\quad p_i(\alpha,\beta,\gamma,\ldots)_{i\leq n}$$  (2). $$ p_i(\alpha,\beta,\gamma,\ldots)_{i\leq n}\quad\xrightarrow{\text{CH}}\quad p_i(\alpha,\beta,\gamma,\ldots)_{i\leq n(n-1)}$$  (3). $$ p_i(\alpha,\beta,\gamma,\ldots)_{i\leq n(n-1)}\quad\xrightarrow{\text{L}}\quad p_i(\alpha\beta,\alpha\gamma,\ldots)_{i\leq \binom{n}{2}}$$  (4). $$ p_i(\alpha\beta,\alpha\gamma,\ldots)_{i\leq \binom{n}{2}}\quad\xrightarrow{\text{EXP}}\quad e_i(\alpha\beta,\alpha\gamma,\ldots)_{i\leq \binom{n}{2}}.$$",,"['abstract-algebra', 'ring-theory', 'computational-mathematics']"
53,"Is $\mathbb{R}[x,y,z]/(x^2+y^2+z^2)$ a UFD?",Is  a UFD?,"\mathbb{R}[x,y,z]/(x^2+y^2+z^2)","As the title says, I am curious as to whether $A =\mathbb{R}[x,y,z]/(x^2+y^2+z^2)$ is a UFD. I believe the answer is yes. A thought I had was to apply Nagata's criterion, say by localizing at $z,$ to get $A_z = \mathbb{R}[x,y,z,z^{-1}]/((x/z)^2+(y/z)^2+1).$ Further, from that, I was hoping to maybe show that $A_z$ is isomorphic to $\mathbb{R}[x',y',z,z^{-1}]/(x'^2+y'^2+1)$ by sending $x/z$ to $x'$ and $y/z$ to $y'.$ However, for this I would still need to show that $\mathbb{R}[x',y']/(x^2+y^2+1)$ is an UFD, something I'm not quite sure how to do. Any help would be welcome.","As the title says, I am curious as to whether $A =\mathbb{R}[x,y,z]/(x^2+y^2+z^2)$ is a UFD. I believe the answer is yes. A thought I had was to apply Nagata's criterion, say by localizing at $z,$ to get $A_z = \mathbb{R}[x,y,z,z^{-1}]/((x/z)^2+(y/z)^2+1).$ Further, from that, I was hoping to maybe show that $A_z$ is isomorphic to $\mathbb{R}[x',y',z,z^{-1}]/(x'^2+y'^2+1)$ by sending $x/z$ to $x'$ and $y/z$ to $y'.$ However, for this I would still need to show that $\mathbb{R}[x',y']/(x^2+y^2+1)$ is an UFD, something I'm not quite sure how to do. Any help would be welcome.",,"['abstract-algebra', 'commutative-algebra', 'unique-factorization-domains']"
54,Proportion of nonabelian $2$-groups of a certain order whose exponent is $4$,Proportion of nonabelian -groups of a certain order whose exponent is,2 4,"Let $$\displaystyle A(n)=\frac{\text{number of nonabelian 2-groups of order $n$ whose exponent is }4}{\text{total number of nonabelian 2-groups of order $n$}}.$$ Using GAP, I could observe the following: $$A(16)=\frac{5}{9}=0.5556, A(32)=\frac{21}{44}=0.4773, A(64)=\frac{93}{256}=0.3633, A(128)=\frac{820}{2313}=0.3545, A(256)=\frac{30446}{56070}=0.5430 \text{   and } A(512)=\frac{8791058}{10494183}=0.8377.$$ Can one prove that if $n>4$, then $A(n)>\frac{1}{3}$?","Let $$\displaystyle A(n)=\frac{\text{number of nonabelian 2-groups of order $n$ whose exponent is }4}{\text{total number of nonabelian 2-groups of order $n$}}.$$ Using GAP, I could observe the following: $$A(16)=\frac{5}{9}=0.5556, A(32)=\frac{21}{44}=0.4773, A(64)=\frac{93}{256}=0.3633, A(128)=\frac{820}{2313}=0.3545, A(256)=\frac{30446}{56070}=0.5430 \text{   and } A(512)=\frac{8791058}{10494183}=0.8377.$$ Can one prove that if $n>4$, then $A(n)>\frac{1}{3}$?",,"['abstract-algebra', 'finite-groups', 'gap', '2-groups', 'groups-enumeration']"
55,"Showing that $\mathbb{Q}\left(\sqrt{2},\sqrt{3}, \sqrt{(9 - 5\sqrt{3})(2-\sqrt{2})}\right)$ is normal over $\mathbb{Q}$, and finding its Galois group","Showing that  is normal over , and finding its Galois group","\mathbb{Q}\left(\sqrt{2},\sqrt{3}, \sqrt{(9 - 5\sqrt{3})(2-\sqrt{2})}\right) \mathbb{Q}","If $K=\mathbb{Q}\left(\sqrt{2},\sqrt{3}, u\right)$ , where $u^2 = (9 - 5\sqrt{3})(2-\sqrt{2})$ , show that $K/\mathbb{Q}$ is normal, and find $\operatorname{Gal}(K/\mathbb{Q})$ . I found that the minimal polynomial of $u^2$ over $\mathbb{Q}$ is $f = t^4 - 72t^3 + 720t^2 - 864t + 144$ , so that $u$ satisfies $g = t^8 - 72t^6 + 720t^4 - 864t^2 + 144$ , but I'm not sure how to show that $K$ is a splitting field of $g$ . Any help would be greatly appreciated!","If , where , show that is normal, and find . I found that the minimal polynomial of over is , so that satisfies , but I'm not sure how to show that is a splitting field of . Any help would be greatly appreciated!","K=\mathbb{Q}\left(\sqrt{2},\sqrt{3}, u\right) u^2 = (9 - 5\sqrt{3})(2-\sqrt{2}) K/\mathbb{Q} \operatorname{Gal}(K/\mathbb{Q}) u^2 \mathbb{Q} f = t^4 - 72t^3 + 720t^2 - 864t + 144 u g = t^8 - 72t^6 + 720t^4 - 864t^2 + 144 K g","['abstract-algebra', 'field-theory', 'galois-theory', 'extension-field']"
56,Local Artinian rings with a principal maximal ideal,Local Artinian rings with a principal maximal ideal,,"I would be very grateful if someone would check my proof of the following result (this is not homework). All rings are commutative and unital. Proposition: If $(A,\mathfrak{m})$ is a local Artinian ring and $\mathfrak{m}$ is principal, then every non-zero ideal of $A$ is a power of $\mathfrak{m}$. Proof: I assume the following two facts: Artinian rings are Noetherian. The Jacobson radical of an Artinian ring is nilpotent. By facts 1 and 2 combined, we see that $\mathfrak{m}$ is nilpotent. Hence, given a proper, non-zero ideal $\mathfrak{a}$ of $A$, there is some $r \geq 1$ such that $\mathfrak{a}\subseteq\mathfrak{m}^r$ (we have $\mathfrak{a}\subseteq \mathfrak{m}$ as $A$ is local) but $\mathfrak{a}\nsubseteq\mathfrak{m}^{r+1}.$ We will show that $\mathfrak{a}=\mathfrak{m}^r.$ Choose $y \in \mathfrak{a}$ such that $y \notin \mathfrak{m}^{r+1}.$ As $\mathfrak{m}$ is principal, we have $y=ax^r$ for some $a \in A$ and $x \in A$ such that $\mathfrak{m}=(x)$. But as $\mathfrak{a}\nsubseteq(x^{r+1})$ we have $a \notin (x)$ and so, as $A$ is local, we have that $a$ is a unit in $A$. Therefore $x^r=a^{-1}y \in \mathfrak{a}$ and so $\mathfrak{m}^r\subseteq\mathfrak{a}$. Q.E.D. Many thanks!","I would be very grateful if someone would check my proof of the following result (this is not homework). All rings are commutative and unital. Proposition: If $(A,\mathfrak{m})$ is a local Artinian ring and $\mathfrak{m}$ is principal, then every non-zero ideal of $A$ is a power of $\mathfrak{m}$. Proof: I assume the following two facts: Artinian rings are Noetherian. The Jacobson radical of an Artinian ring is nilpotent. By facts 1 and 2 combined, we see that $\mathfrak{m}$ is nilpotent. Hence, given a proper, non-zero ideal $\mathfrak{a}$ of $A$, there is some $r \geq 1$ such that $\mathfrak{a}\subseteq\mathfrak{m}^r$ (we have $\mathfrak{a}\subseteq \mathfrak{m}$ as $A$ is local) but $\mathfrak{a}\nsubseteq\mathfrak{m}^{r+1}.$ We will show that $\mathfrak{a}=\mathfrak{m}^r.$ Choose $y \in \mathfrak{a}$ such that $y \notin \mathfrak{m}^{r+1}.$ As $\mathfrak{m}$ is principal, we have $y=ax^r$ for some $a \in A$ and $x \in A$ such that $\mathfrak{m}=(x)$. But as $\mathfrak{a}\nsubseteq(x^{r+1})$ we have $a \notin (x)$ and so, as $A$ is local, we have that $a$ is a unit in $A$. Therefore $x^r=a^{-1}y \in \mathfrak{a}$ and so $\mathfrak{m}^r\subseteq\mathfrak{a}$. Q.E.D. Many thanks!",,"['abstract-algebra', 'ring-theory', 'commutative-algebra', 'proof-verification']"
57,How to recover the integral group ring?,How to recover the integral group ring?,,"I would like to solve the following exercise: Suppose $R$ is a commutative semisimple ring of characteristic $p^t, t\geq1$, and we have two finite groups $G_1=H_1 \times A_1$ and $G_2=H_2 \times A_2$. Now $H_1$ and $H_2$ are finite $p$-groups, where $p \nmid \vert A_i \vert, i=1,2$. Prove that $RG_1 \cong RG_2$ iff $\mathbb Z A_{1} \cong \mathbb Z A_2$ and $\mathbb{Z} H_1 \cong \mathbb{Z} H_2$. Having no idea how to start, I first tried solving a different exercise, also related to direct products: If $G_1=H_1 \times A_1$ is a finite group, where $(\vert H_1 \vert,\vert A_1 \vert)=1$, and $G_2$ is any other group, then: $\mathbb Z G_1 \cong \mathbb Z G_2$ iff $G_2=H_2 \times A_2$, $\mathbb{Z}A_1 \cong \mathbb Z A_2$ and $\mathbb Z H_1 \cong \mathbb Z H_2$. Proof: One direction is clear, since $\mathbb Z (G \times H) \cong \mathbb Z G \otimes \mathbb Z H$. For the other direction, since $H_1 \trianglelefteq G_1$, by the normal subgroup correspondence, there exists a unique normal subgroup $H_2 \trianglelefteq G_2$  such that $\mathbb Z (G_2 / H_2) \cong \mathbb Z (G_1/H_1) \cong \mathbb Z A_1$. Doing the same for $A_1 \trianglelefteq G_1$ gives $\mathbb{Z}(G_2/A_2) \cong \mathbb Z H_1$. The correspondence respects intersections, so $A_2 \cap H_2= \emptyset$. Since $\vert A_2 \times H_2 \vert= \vert A_2 \vert \cdot \vert H_2 \vert=\vert A_1 \vert \cdot \vert H_1 \vert =\vert G_1 \vert = \vert G_2 \vert$ ($\mathbb Z G_1$ and $\mathbb Z G_2$ have the same rank over $\mathbb Z$), I think we can conclude that $G_2=A_2 \times H_2$. $\square$ EDIT: Does anyone know why the orders are supposed to be coprime? This doesn't really seem to help for the first exercise though. Since $R=k_1 \times k_2 \times \ldots \times k_n$, where the $k_i$ are fields of characteristic $p$, it is probably helpful to first consider $R=k$. Any help would be appreciated.","I would like to solve the following exercise: Suppose $R$ is a commutative semisimple ring of characteristic $p^t, t\geq1$, and we have two finite groups $G_1=H_1 \times A_1$ and $G_2=H_2 \times A_2$. Now $H_1$ and $H_2$ are finite $p$-groups, where $p \nmid \vert A_i \vert, i=1,2$. Prove that $RG_1 \cong RG_2$ iff $\mathbb Z A_{1} \cong \mathbb Z A_2$ and $\mathbb{Z} H_1 \cong \mathbb{Z} H_2$. Having no idea how to start, I first tried solving a different exercise, also related to direct products: If $G_1=H_1 \times A_1$ is a finite group, where $(\vert H_1 \vert,\vert A_1 \vert)=1$, and $G_2$ is any other group, then: $\mathbb Z G_1 \cong \mathbb Z G_2$ iff $G_2=H_2 \times A_2$, $\mathbb{Z}A_1 \cong \mathbb Z A_2$ and $\mathbb Z H_1 \cong \mathbb Z H_2$. Proof: One direction is clear, since $\mathbb Z (G \times H) \cong \mathbb Z G \otimes \mathbb Z H$. For the other direction, since $H_1 \trianglelefteq G_1$, by the normal subgroup correspondence, there exists a unique normal subgroup $H_2 \trianglelefteq G_2$  such that $\mathbb Z (G_2 / H_2) \cong \mathbb Z (G_1/H_1) \cong \mathbb Z A_1$. Doing the same for $A_1 \trianglelefteq G_1$ gives $\mathbb{Z}(G_2/A_2) \cong \mathbb Z H_1$. The correspondence respects intersections, so $A_2 \cap H_2= \emptyset$. Since $\vert A_2 \times H_2 \vert= \vert A_2 \vert \cdot \vert H_2 \vert=\vert A_1 \vert \cdot \vert H_1 \vert =\vert G_1 \vert = \vert G_2 \vert$ ($\mathbb Z G_1$ and $\mathbb Z G_2$ have the same rank over $\mathbb Z$), I think we can conclude that $G_2=A_2 \times H_2$. $\square$ EDIT: Does anyone know why the orders are supposed to be coprime? This doesn't really seem to help for the first exercise though. Since $R=k_1 \times k_2 \times \ldots \times k_n$, where the $k_i$ are fields of characteristic $p$, it is probably helpful to first consider $R=k$. Any help would be appreciated.",,"['abstract-algebra', 'group-theory', 'finite-groups', 'group-rings']"
58,Normal closure of a radical extension is radical,Normal closure of a radical extension is radical,,"I'm struggling to understand the proof that the normal closure of a radical extension of fields is also a radical extension, which is crucial since it allows us to work with radical and normal extensions (and thus, in characteristic 0, with radical Galois extensions). The proof I'm reading is lemma 4.17 in Rotman's Advanced Modern Algebra, pp. 211-212, which I attach at the end of this message. The definition Rotman uses is the following: an extension $F\subset F(u)$ is pure if there exists $m \geq 1$ such that $u^m\in F$. An extension $F\subset K$ is radical if there exists a tower (which I like to call radical tower ) $$F=K_0\subset K_1\subset \dots \subset K_t=K$$ such that $K_i\subset K_{i+1}$ is pure for all $i$. What I'm struggling to prove is: if $F\subset K$ is a radical extension, and $N$ is the normal closure of the extension, then $F\subset N$ is a radical extension. Since I find Rotman's proof confusing (and not convincing), I have rewritten the proof. It might seem longer in appearance, but the first part of the proof (the one that troubles me) was unsatisfying. Let $m_{\alpha,F}$ denote the minimal polynomial for an element $\alpha$ over $F$. What I ask is: 1) Is my rewriting of the proof correct? 2) Is this what's behind the argument of Rotman, or is there another, more simple way of reading it? Rotman's proof seems shorter, but I don't understand when he says ""it follows that $E=k(\sigma(u_1),\dots,\sigma(u_t):\sigma \in G)$"": what are those $u_i$? Here's my rewriting: Let $F\subset K$ be a radical extension. There exist $u_1,\dots,u_t$ such that $$F\subset F(u_1)\subset F(u_1,u_2)\subset \dots \subset F(u_1,\dots,u_t) = K$$ is a radical tower. Let $E$ be the normal closure of $F\subset K$. I claim that$$E=F\left(\\{\sigma(u_i): \sigma \in \text{Gal}^E_F, i=1,\dots,t\\}\right)$$ Indeed, $$E=\text{Split}_F\left(m_{u_1,F} \dots m_{u_t,F}\right)=F(\{\text{roots of }m_{u_1,F} \dots m_{u_t,F}\})$$ Since the Galois group of a polynomial acts transitively on its roots, $\{\text{roots of }m_{u_i,F}\}=\{\sigma(u_i):\sigma \in \text{Gal}_F(m_{u_i,F})\}$. Let us see that is also equals $\{\sigma(u_i):\sigma \in \text{Gal}^E_F\}$. $(\subset)$: We can extend every $\sigma\in \text{Gal}_F(m_{u_i,F})$ to $\tilde{\sigma}\in \text{Gal}_F^E$ by the theorem of extension of a polynomial to its splitting field, by taking the polynomial $m_{u_1,F} \dots m_{u_t,F}\in F[X]$. $(\supset)$: If $\sigma\in \text{Gal}^E_F$, then $\sigma$ restricts to an $F$-isomorphism $F(u_i)\to F(\sigma(u_i))$, then (corollary 1.9 p.236 of Hungerford)  $u_i$ and $\sigma(u_i)$ must be roots of the same irreducible polynomial in $F[X]$. This proves that $\sigma(u_i)$ is a root of $m_{u_i,F}$. Let $B_i=F\left(\{\sigma(u_j):1\leq j\leq i,\sigma \in\text{Gal}^E_F\}\right)$. What we just proved is that $E=B_t$. To see that $F\subset E= B_t$ es radical, we use induction on $t$, and this part of Rotman's proof is satisfying to me. Below is Rotman's proof.","I'm struggling to understand the proof that the normal closure of a radical extension of fields is also a radical extension, which is crucial since it allows us to work with radical and normal extensions (and thus, in characteristic 0, with radical Galois extensions). The proof I'm reading is lemma 4.17 in Rotman's Advanced Modern Algebra, pp. 211-212, which I attach at the end of this message. The definition Rotman uses is the following: an extension $F\subset F(u)$ is pure if there exists $m \geq 1$ such that $u^m\in F$. An extension $F\subset K$ is radical if there exists a tower (which I like to call radical tower ) $$F=K_0\subset K_1\subset \dots \subset K_t=K$$ such that $K_i\subset K_{i+1}$ is pure for all $i$. What I'm struggling to prove is: if $F\subset K$ is a radical extension, and $N$ is the normal closure of the extension, then $F\subset N$ is a radical extension. Since I find Rotman's proof confusing (and not convincing), I have rewritten the proof. It might seem longer in appearance, but the first part of the proof (the one that troubles me) was unsatisfying. Let $m_{\alpha,F}$ denote the minimal polynomial for an element $\alpha$ over $F$. What I ask is: 1) Is my rewriting of the proof correct? 2) Is this what's behind the argument of Rotman, or is there another, more simple way of reading it? Rotman's proof seems shorter, but I don't understand when he says ""it follows that $E=k(\sigma(u_1),\dots,\sigma(u_t):\sigma \in G)$"": what are those $u_i$? Here's my rewriting: Let $F\subset K$ be a radical extension. There exist $u_1,\dots,u_t$ such that $$F\subset F(u_1)\subset F(u_1,u_2)\subset \dots \subset F(u_1,\dots,u_t) = K$$ is a radical tower. Let $E$ be the normal closure of $F\subset K$. I claim that$$E=F\left(\\{\sigma(u_i): \sigma \in \text{Gal}^E_F, i=1,\dots,t\\}\right)$$ Indeed, $$E=\text{Split}_F\left(m_{u_1,F} \dots m_{u_t,F}\right)=F(\{\text{roots of }m_{u_1,F} \dots m_{u_t,F}\})$$ Since the Galois group of a polynomial acts transitively on its roots, $\{\text{roots of }m_{u_i,F}\}=\{\sigma(u_i):\sigma \in \text{Gal}_F(m_{u_i,F})\}$. Let us see that is also equals $\{\sigma(u_i):\sigma \in \text{Gal}^E_F\}$. $(\subset)$: We can extend every $\sigma\in \text{Gal}_F(m_{u_i,F})$ to $\tilde{\sigma}\in \text{Gal}_F^E$ by the theorem of extension of a polynomial to its splitting field, by taking the polynomial $m_{u_1,F} \dots m_{u_t,F}\in F[X]$. $(\supset)$: If $\sigma\in \text{Gal}^E_F$, then $\sigma$ restricts to an $F$-isomorphism $F(u_i)\to F(\sigma(u_i))$, then (corollary 1.9 p.236 of Hungerford)  $u_i$ and $\sigma(u_i)$ must be roots of the same irreducible polynomial in $F[X]$. This proves that $\sigma(u_i)$ is a root of $m_{u_i,F}$. Let $B_i=F\left(\{\sigma(u_j):1\leq j\leq i,\sigma \in\text{Gal}^E_F\}\right)$. What we just proved is that $E=B_t$. To see that $F\subset E= B_t$ es radical, we use induction on $t$, and this part of Rotman's proof is satisfying to me. Below is Rotman's proof.",,"['abstract-algebra', 'field-theory', 'galois-theory']"
59,Determine whether a polynomial is irreducible,Determine whether a polynomial is irreducible,,"Consider the polynomial $P=X^5-X-1\in\Bbb{F}_3[X]$ . I want to show that $P$ is irreducible. We can easily check it has no roots, so the only way it could not be irreducible is by being a product of two polynomials of degree $2$ and $3$ respectively. So I determine all the irreducible polynomials of degree $2$ in $\Bbb{F}_3[X]$ . These are $$X^2+1;{~~} X^2+X-1 {~~}\text{and}{~~}X^2-X-1.$$ Finally I check using the Euclidean division algorithm that none of those polynomials divides $P$ , which concludes the proof. My question is this:  is there a more efficient way to do this? I had to first determine all the polynomials of a given degree ( $2$ ) and then apply Euclidean algorithm to each of them. That is quite some work, and we're still considering polynomials of relatively small degree, and fields of small cardinal. I can't imagine applying the same reasoning to determine whether $Q=X^9-X^2-1\in\Bbb{F}_{17}[X]$ is irreducible. So, is there a more efficient method to determine irreducibility, at least in the case of polynomials of small degree over small fields?","Consider the polynomial . I want to show that is irreducible. We can easily check it has no roots, so the only way it could not be irreducible is by being a product of two polynomials of degree and respectively. So I determine all the irreducible polynomials of degree in . These are Finally I check using the Euclidean division algorithm that none of those polynomials divides , which concludes the proof. My question is this:  is there a more efficient way to do this? I had to first determine all the polynomials of a given degree ( ) and then apply Euclidean algorithm to each of them. That is quite some work, and we're still considering polynomials of relatively small degree, and fields of small cardinal. I can't imagine applying the same reasoning to determine whether is irreducible. So, is there a more efficient method to determine irreducibility, at least in the case of polynomials of small degree over small fields?",P=X^5-X-1\in\Bbb{F}_3[X] P 2 3 2 \Bbb{F}_3[X] X^2+1;{~~} X^2+X-1 {~~}\text{and}{~~}X^2-X-1. P 2 Q=X^9-X^2-1\in\Bbb{F}_{17}[X],"['abstract-algebra', 'polynomials', 'ring-theory', 'finite-fields', 'irreducible-polynomials']"
60,Mathematics and cinema,Mathematics and cinema,,"I wander if anyone of you have some knowledge about relations between abstract algebra and cinema. I'm not searching for movies about mathematics or algebra; I'm searching for some kind of application of algebra in the technical or aesthetic aspect of cinema itself. I would prefer an application of abstract algebra but applications of logic, analysis, topology or set theory also will be appreciated. Have a nice day!","I wander if anyone of you have some knowledge about relations between abstract algebra and cinema. I'm not searching for movies about mathematics or algebra; I'm searching for some kind of application of algebra in the technical or aesthetic aspect of cinema itself. I would prefer an application of abstract algebra but applications of logic, analysis, topology or set theory also will be appreciated. Have a nice day!",,"['abstract-algebra', 'computer-science', 'applications', 'art']"
61,Is the de Rham complex a free (commutative?) differential graded algebra?,Is the de Rham complex a free (commutative?) differential graded algebra?,,"A differential graded algebra (dg-algebra) is a monoid object in the category of chain complexes with respect to the usual tensor product of complexes. A (graded) commutative dg-algebra is simply a commutative monoid object. The de Rham complex for a smooth manifold is easily seen to be a commutative dg-algebra with respect to the wedge product. I could have sworn that I saw somewhere that the de Rham complex is the free dg-algebra (or free commutative dg-algebra?) on some appropriate base category, i.e. left adjoint to a forgetful functor. I can't seem to find a reference for this, however. With regards to my background I'm familiar with category theory and differential geometry, but my algebra is a bit weak.","A differential graded algebra (dg-algebra) is a monoid object in the category of chain complexes with respect to the usual tensor product of complexes. A (graded) commutative dg-algebra is simply a commutative monoid object. The de Rham complex for a smooth manifold is easily seen to be a commutative dg-algebra with respect to the wedge product. I could have sworn that I saw somewhere that the de Rham complex is the free dg-algebra (or free commutative dg-algebra?) on some appropriate base category, i.e. left adjoint to a forgetful functor. I can't seem to find a reference for this, however. With regards to my background I'm familiar with category theory and differential geometry, but my algebra is a bit weak.",,"['abstract-algebra', 'differential-geometry', 'algebraic-topology', 'homology-cohomology', 'differential-forms']"
62,How to compute Ext over an exterior algebra,How to compute Ext over an exterior algebra,,"I found this question in several places (even on mathoverflow and mathstackexchange), but I never found a satisfying answer. Let $k$ be a field and $V$ a finite dimensional $k$-vectorspace. I would like to compute $\mathrm{Ext}_{\bigwedge V}(k,k)$. By looking online it seems that it should be the dual of the symmetric algebra. The first step it's computing a free resolution of $k$ over $\bigwedge V$. Always by looking online it seems that this resolution should look like $$K_.:\cdots\rightarrow S^{k+1}V\otimes_k\bigwedge V\xrightarrow{d^{k+1}}S^kV\otimes_k\bigwedge V\rightarrow\cdots,$$ where by $SV$ I'm denoting the symmetric algebra. I think the differential should be $$d^{k+1}(v_1\cdots v_{k+1}\otimes w)=\sum_i (-1)^{i+1}v_1\cdots\hat{v}_i\cdots v_{k+1}\otimes v_i\wedge w,$$ but I'm not entirely sure. In fact, it is not obvious to me that this complex is acyclic. Could you tell me how to prove it? And once we know that this is a resolution how do we compute the Ext's? It seems that the differentials of $\mathrm{Hom}_{\bigwedge V}(K_.,k)$ are zero but I can't understand why. Any help, please? (I'm also using commutative-algebra and modules as tags since this question is related to the Koszul complex.)","I found this question in several places (even on mathoverflow and mathstackexchange), but I never found a satisfying answer. Let $k$ be a field and $V$ a finite dimensional $k$-vectorspace. I would like to compute $\mathrm{Ext}_{\bigwedge V}(k,k)$. By looking online it seems that it should be the dual of the symmetric algebra. The first step it's computing a free resolution of $k$ over $\bigwedge V$. Always by looking online it seems that this resolution should look like $$K_.:\cdots\rightarrow S^{k+1}V\otimes_k\bigwedge V\xrightarrow{d^{k+1}}S^kV\otimes_k\bigwedge V\rightarrow\cdots,$$ where by $SV$ I'm denoting the symmetric algebra. I think the differential should be $$d^{k+1}(v_1\cdots v_{k+1}\otimes w)=\sum_i (-1)^{i+1}v_1\cdots\hat{v}_i\cdots v_{k+1}\otimes v_i\wedge w,$$ but I'm not entirely sure. In fact, it is not obvious to me that this complex is acyclic. Could you tell me how to prove it? And once we know that this is a resolution how do we compute the Ext's? It seems that the differentials of $\mathrm{Hom}_{\bigwedge V}(K_.,k)$ are zero but I can't understand why. Any help, please? (I'm also using commutative-algebra and modules as tags since this question is related to the Koszul complex.)",,"['abstract-algebra', 'commutative-algebra', 'modules', 'homological-algebra', 'exterior-algebra']"
63,Does the Trace product in a semigroup have any relation with Trace of a matrix / matrix product,Does the Trace product in a semigroup have any relation with Trace of a matrix / matrix product,,"I recently read an article on generalized inverses and Green's relations (by X.Mary). The framework is semigroups, but obviously it has a lot of application within matrix theory. In the article mention is made of the ""trace product"" - it is defined as: if $a,b \in S$, then $ab$ is a trace product if $ab \in \mathcal{R}_a \cap \mathcal{L}_b$. Here $\mathcal{R}_a$ and $\mathcal{L_b}$ are the relevant classes from Green's relations. So I traced (no pun intended) this product back to two articles, one by Rees and one by Miller and Clifford (Regular D-classes in semigroups). So here the explanation is given of a semigroup $D^0$, constructed from a $D$ class and called the trace of $D$. The definition of the binary operation of this semigroup then includes this trace product. So this semigroup is used to prove that $D$ is partially isomorphic to a regular matrix semigroup. I am not so familiar with the abstract algebra of semigroups - does anyone know if this trace product mentioned here bears any relation to the trace of a matrix? or perhaps trace of a matrix product? as it is usually referred to in the context of matrix theory. Perhaps in a semigroup of matrices (say square matrices of the same size) you get some classes as per Green's definitions that can be characterized in terms of the trace of a matrix, and it is related to this trace product (it's a long shot I know...)? NB: You can read it online if you register - and the section on the trace construction is on page 276/277.","I recently read an article on generalized inverses and Green's relations (by X.Mary). The framework is semigroups, but obviously it has a lot of application within matrix theory. In the article mention is made of the ""trace product"" - it is defined as: if $a,b \in S$, then $ab$ is a trace product if $ab \in \mathcal{R}_a \cap \mathcal{L}_b$. Here $\mathcal{R}_a$ and $\mathcal{L_b}$ are the relevant classes from Green's relations. So I traced (no pun intended) this product back to two articles, one by Rees and one by Miller and Clifford (Regular D-classes in semigroups). So here the explanation is given of a semigroup $D^0$, constructed from a $D$ class and called the trace of $D$. The definition of the binary operation of this semigroup then includes this trace product. So this semigroup is used to prove that $D$ is partially isomorphic to a regular matrix semigroup. I am not so familiar with the abstract algebra of semigroups - does anyone know if this trace product mentioned here bears any relation to the trace of a matrix? or perhaps trace of a matrix product? as it is usually referred to in the context of matrix theory. Perhaps in a semigroup of matrices (say square matrices of the same size) you get some classes as per Green's definitions that can be characterized in terms of the trace of a matrix, and it is related to this trace product (it's a long shot I know...)? NB: You can read it online if you register - and the section on the trace construction is on page 276/277.",,"['abstract-algebra', 'matrices', 'equivalence-relations', 'semigroups']"
64,Homogeneous polynomials in two variables taking integer values,Homogeneous polynomials in two variables taking integer values,,"It is known that ${x\choose 0},{x\choose 1},\ldots,{x\choose n}\in\mathbb{Q}[x]$ is a $\mathbb{Z}$-basis for set of polynomials of degree at most $n$ which map $\mathbb{Z}$ into itself. For fixed positive integer $n$, let $M_n\subset \mathbb{Q}[x,y]$ be the set of homogeneous polynomials of degree $n$, which map $\mathbb{Z}\times\mathbb{Z}$ into $\mathbb{Z}$.  My question: is there an explicit description of a $\mathbb{Z}$-basis of $M_n$? Certainly we have $M_n\subset y^n{x/y\choose 0}\mathbb{Z}+y^n{x/y\choose 1}\mathbb{Z}+\ldots+y^n{x/y\choose n}\mathbb{Z}$, but we don't have equality.","It is known that ${x\choose 0},{x\choose 1},\ldots,{x\choose n}\in\mathbb{Q}[x]$ is a $\mathbb{Z}$-basis for set of polynomials of degree at most $n$ which map $\mathbb{Z}$ into itself. For fixed positive integer $n$, let $M_n\subset \mathbb{Q}[x,y]$ be the set of homogeneous polynomials of degree $n$, which map $\mathbb{Z}\times\mathbb{Z}$ into $\mathbb{Z}$.  My question: is there an explicit description of a $\mathbb{Z}$-basis of $M_n$? Certainly we have $M_n\subset y^n{x/y\choose 0}\mathbb{Z}+y^n{x/y\choose 1}\mathbb{Z}+\ldots+y^n{x/y\choose n}\mathbb{Z}$, but we don't have equality.",,"['abstract-algebra', 'polynomials']"
65,How does the theory of the quadratic number fields relate to the quadratic forms?,How does the theory of the quadratic number fields relate to the quadratic forms?,,"As every one knows, the quadratic number fields shares a deep connection with the binary quadratic forms; I have been told this relation when I was a senior high, and now I, learning some difficult theories like the theory of fields of classes, know some basic facts about the algebraic number fields of degree two but I know nothing about what is going on in the Disquisitiones Arithmeticae by Gauss even though I have read it twice; it seems to be a kind of a magic there; he makes things like ideals happen with only elementary and lengthy processes. My question is the Is it necessary to verify all identities in the book by Gauss to understand his theory of quadratic forms? I suppose it is not; then what could be said about the connection between them so that one can easily have a sight on what he is doing by modern algebraic tools? If it was possible, please say something assertive like ""The forms are just ..."" Thank you very much.","As every one knows, the quadratic number fields shares a deep connection with the binary quadratic forms; I have been told this relation when I was a senior high, and now I, learning some difficult theories like the theory of fields of classes, know some basic facts about the algebraic number fields of degree two but I know nothing about what is going on in the Disquisitiones Arithmeticae by Gauss even though I have read it twice; it seems to be a kind of a magic there; he makes things like ideals happen with only elementary and lengthy processes. My question is the Is it necessary to verify all identities in the book by Gauss to understand his theory of quadratic forms? I suppose it is not; then what could be said about the connection between them so that one can easily have a sight on what he is doing by modern algebraic tools? If it was possible, please say something assertive like ""The forms are just ..."" Thank you very much.",,"['abstract-algebra', 'elementary-number-theory', 'quadratic-forms']"
66,Existence of maximal free submodules,Existence of maximal free submodules,,"Let $A$ be a domain and $M$ be a finitely generated module. Is there a free submodule which is maximal among free submodules? Answer is yes for Noetherian rings, obviously. Also the rank of any such submodule is determined by $M$ by tensoring up with $Q(A)$ . I haven't got much further than this.","Let be a domain and be a finitely generated module. Is there a free submodule which is maximal among free submodules? Answer is yes for Noetherian rings, obviously. Also the rank of any such submodule is determined by by tensoring up with . I haven't got much further than this.",A M M Q(A),"['abstract-algebra', 'modules', 'free-modules']"
67,"Why do complex analysis and complex geometry exhibit a sort of ""arithmetic"" behavior?","Why do complex analysis and complex geometry exhibit a sort of ""arithmetic"" behavior?",,"That is going to be a rather vague question, but as I stated in the title, why do complex analysis and complex geometry seem to exhibit a sort of ""arithmetic"" behavior, contrary to real analysis and real differential geometry? I am referring to facts like complex tori having as moduli space $\mathbb H/PSL(2, \mathbb{Z})$ while real tori are all diffeomorphic, or the relation between complex analysis and number theory in general. Is there a deeper reason for that? After all, at a first glance real and complex analysis are built on the derivative, which is constructed in the same way in $\mathbb{R}$ and $\mathbb{C}$ , only relying on completeness as metric spaces and field operations (as far as I know). Only difference (although a relevant one) being that one of them is algebraically closed, while the other one is not; again, as far as I know. Apologies again for this not-so-well-asked question.","That is going to be a rather vague question, but as I stated in the title, why do complex analysis and complex geometry seem to exhibit a sort of ""arithmetic"" behavior, contrary to real analysis and real differential geometry? I am referring to facts like complex tori having as moduli space while real tori are all diffeomorphic, or the relation between complex analysis and number theory in general. Is there a deeper reason for that? After all, at a first glance real and complex analysis are built on the derivative, which is constructed in the same way in and , only relying on completeness as metric spaces and field operations (as far as I know). Only difference (although a relevant one) being that one of them is algebraically closed, while the other one is not; again, as far as I know. Apologies again for this not-so-well-asked question.","\mathbb H/PSL(2, \mathbb{Z}) \mathbb{R} \mathbb{C}","['abstract-algebra', 'complex-analysis', 'number-theory', 'differential-geometry', 'complex-geometry']"
68,What is the inverse of the $q$-exponential?,What is the inverse of the -exponential?,q,"The $q$ -exponential function is given by the power series $$ e_q(x) = \sum_{n=0}^\infty \frac{x^n}{[n]!} $$ using the $q$ -integers $[k]:=q^{k-1}+\cdots+q+1=(q^k-1)/(q-1)$ and $q$ -factorials $[n]!=[n][n-1]\cdots[2][1]$ . What is the inverse function $\ln_q$ satisfying $\ln_q(e_q(x))=x$ and $e_q(\ln_q(1+x))=1+x$ ? I am treating $x$ and $q$ as either indeterminates for formal power series or complex variables in whatever domain yields an answer. By hand I calculated the first few terms $$ \ln_q(1+x)=x-\frac{1}{[2]}x^2+ \left(\frac{2}{\,[2]^2}-\frac{1}{[3][2]}\right)x^3- \left(\frac{5}{\,[2]^3}-\frac{5}{[3][2]^2}+\frac{1}{[4][3][2]}\right)x^4+\cdots $$ A vast generalization might be to treat $[1],[2],[3],\cdots$ as independent formal variables and treat these coefficients above as rational functions of them. At worst, the series for $\ln_q$ is no simpler than this generalization. Curiously, this seems to be in no literature at all. Like, are these coefficients a special kind of polynomial? Is $\ln_q$ the $q$ -integral of something? Is there an analogous identity for the fact $e_q(x+y)=e_q(x)e_q(y)$ when $(xy)=q(yx)$ ? All mentions of $q$ -logarithms in search results seem to be talking about Tsallis statistics, which is an unrelated situation with a unrelated notion of $q$ -exponential.","The -exponential function is given by the power series using the -integers and -factorials . What is the inverse function satisfying and ? I am treating and as either indeterminates for formal power series or complex variables in whatever domain yields an answer. By hand I calculated the first few terms A vast generalization might be to treat as independent formal variables and treat these coefficients above as rational functions of them. At worst, the series for is no simpler than this generalization. Curiously, this seems to be in no literature at all. Like, are these coefficients a special kind of polynomial? Is the -integral of something? Is there an analogous identity for the fact when ? All mentions of -logarithms in search results seem to be talking about Tsallis statistics, which is an unrelated situation with a unrelated notion of -exponential.","q  e_q(x) = \sum_{n=0}^\infty \frac{x^n}{[n]!}  q [k]:=q^{k-1}+\cdots+q+1=(q^k-1)/(q-1) q [n]!=[n][n-1]\cdots[2][1] \ln_q \ln_q(e_q(x))=x e_q(\ln_q(1+x))=1+x x q  \ln_q(1+x)=x-\frac{1}{[2]}x^2+
\left(\frac{2}{\,[2]^2}-\frac{1}{[3][2]}\right)x^3-
\left(\frac{5}{\,[2]^3}-\frac{5}{[3][2]^2}+\frac{1}{[4][3][2]}\right)x^4+\cdots  [1],[2],[3],\cdots \ln_q \ln_q q e_q(x+y)=e_q(x)e_q(y) (xy)=q(yx) q q","['abstract-algebra', 'combinatorics', 'generating-functions', 'q-analogs']"
69,Algebra & Artificial Intelligence (AI),Algebra & Artificial Intelligence (AI),,"Artificial intelligence , especially deep learning & neural networks for image processing and classfication, are related to statistics and physics e.g. as decribed in below papers. Statistics and AI Simoncelli + Olshausen.Natural images statistics and neural representation Field.What the statistics of natural images tell us about visual coding Physics and AI A relation of deep learning and physics (renormalization group) is elaborated in: Mehta + Schwab.An exact mapping between the Variational Renormalization Group and Deep Learning Also category theory is used to describe neural systems and single aspects like e.g. back propagation. Category theory and AI MSE - Category Theory & Artificial Intelligence (AI) Question . Are there interesting formulations and generalizations of (convolutional) neural networks (CNNs) / deep learning (or to machine learning in general) concepts in an algebraic context? Fernando Martin-Maroto and Gonzalo G. de Polavieja published in 2018 Martin-Maroto and Polavieja.Algebraic Machine Learning See also the following pages Martin-Maroto and Polavieja et al.Algebraic AI YouTube.ALMA - Human Centric Algebraic Machine Learning European Commission.ALMA: Human Centric Algebraic Machine Learning Clifford Neural Networks based on Clifford Algebras are subject in Hitzer et al.Applications of Clifford’s Geometric Algebra mentioning Clifford algebra neural computing . However as far as I understand, this is more a generalization of neuron's in- and output values rather than an algebraic formulation of neural network related concepts? In a similar direction(?) goes e.g. Bronstein et al.Geometric deep learning: going beyond Euclidean data Remark. I am aware that this question could also be posted at stack overflow or some other stack exchange site. However my idea is that it could be better to ask it on the mathematics site, since it might rather be mathematicians that have some knowledge about such connections than e.g. computer scientists themselves.","Artificial intelligence , especially deep learning & neural networks for image processing and classfication, are related to statistics and physics e.g. as decribed in below papers. Statistics and AI Simoncelli + Olshausen.Natural images statistics and neural representation Field.What the statistics of natural images tell us about visual coding Physics and AI A relation of deep learning and physics (renormalization group) is elaborated in: Mehta + Schwab.An exact mapping between the Variational Renormalization Group and Deep Learning Also category theory is used to describe neural systems and single aspects like e.g. back propagation. Category theory and AI MSE - Category Theory & Artificial Intelligence (AI) Question . Are there interesting formulations and generalizations of (convolutional) neural networks (CNNs) / deep learning (or to machine learning in general) concepts in an algebraic context? Fernando Martin-Maroto and Gonzalo G. de Polavieja published in 2018 Martin-Maroto and Polavieja.Algebraic Machine Learning See also the following pages Martin-Maroto and Polavieja et al.Algebraic AI YouTube.ALMA - Human Centric Algebraic Machine Learning European Commission.ALMA: Human Centric Algebraic Machine Learning Clifford Neural Networks based on Clifford Algebras are subject in Hitzer et al.Applications of Clifford’s Geometric Algebra mentioning Clifford algebra neural computing . However as far as I understand, this is more a generalization of neuron's in- and output values rather than an algebraic formulation of neural network related concepts? In a similar direction(?) goes e.g. Bronstein et al.Geometric deep learning: going beyond Euclidean data Remark. I am aware that this question could also be posted at stack overflow or some other stack exchange site. However my idea is that it could be better to ask it on the mathematics site, since it might rather be mathematicians that have some knowledge about such connections than e.g. computer scientists themselves.",,"['abstract-algebra', 'machine-learning', 'big-list', 'neural-networks', 'artificial-intelligence']"
70,"""Universal"" differential identities","""Universal"" differential identities",,"I now asked this at MO . Let $f:\mathbb{R}^d \to \mathbb{R}$ be smooth. The mixed derivatives commute: $f_{xy}=f_{yx}$. This  identity is "" universal "" in the sense that it holds for any smooth map. Question: Are there any universal identities which are not consequences of the commutation of the mixed derivatives? More explicitly, let $D_i$ be the differential operator which takes the partial derivative with respect to $x_i$. The symmetry can be written as an algebraic statement $$    D_i \circ D_j = D_j \circ D_i \tag{1}.$$ (When we choose the domain for these operators to be the space of smooth maps $\mathbb{R}^d \to \mathbb{R}^d$, so we can compose operators). Consider the subset $A$ of differential operators (which map $C^{\infty}(\mathbb{R}^d) \to C^{\infty}(\mathbb{R}^d)$) that is ""generated"" by the $D_i$ via addition, composition and multiplication*. Note that this algebraic structure $A$ has $3$ binary operations. (I am not sure if there is a term for such an ""algebraic creature"", $A$ is a ring w.r.t both operations $(+,\cdot)$ and $(+,\circ)$, but this two operations have relations, namely $$(f \cdot g) \circ h = (f \circ h) \cdot (g \circ h),$$ they ""commute"". Does such a structure have a name? ""Concrete Question:"": Are there relations in $A$ which are not consequences of the fundamental relation $(1)$?. *By multiplication (as opposed to composition ) of operators I mean the following: $$D_x \times D_y(f)=f_x \cdot f_y \, , \,  D_x∘D_y(f)=f_{xy} \, , \, (D_x \circ D_x) \times D_y(f)=f_{xx}f_y$$ etc. (Here $f$ is a scalar function, to extend the operations to $\mathbb{R}^d$-valued maps, jut act on each component separately.  I am also allowing for the $i$-th component of output to depend on partial derivatives of all components of $f:\mathbb{R}^d \to \mathbb{R}^d$). The multiplication is needed in order to talk about relations of the form of $f_x f_y=f_yf_x$ (trivially true) or $f_{xx}=f_yf_{xy}$ (clearly not universal).  (Without multiplication, there are no additional relations as observed in this answer ). In particular, I am interested to know whether the ""Cofactor Lemma"" (divergence-free rows, see below) is a ""consequence"" of the commutation of mixed derivatives (for dimension $d>2$ this involves multiplication, as well as addition and composition). The Cofactor Lemma: Let $f:\mathbb{R}^d \to \mathbb{R}^d$ be smooth. Then the Cofactor of $df$ has divergence-free rows : $$\sum_{j=1}^n \frac{\partial(Cof(Du))_{kj}}{\partial x_j} = 0, k=1,...,d.$$ In dimension $d=2$, it reduces to relation $(1)$: Given $A= \begin{pmatrix} a & b \\\ c & d \end{pmatrix}$, $\operatorname{Cof}A= \begin{pmatrix} d & -c \\\ -b & a \end{pmatrix}$, so $$ df= \begin{pmatrix} (f_1)_x & (f_1)_y \\\ (f_2)_x & (f_2)_y \end{pmatrix}, \operatorname{Cof}df= \begin{pmatrix} (f_2)_y & -(f_2)_x \\\ -(f_1)_y & (f_1)_x \end{pmatrix} .$$ We see that $\operatorname{div} (\operatorname{Cof}df)=0$ is equivalent to $(f_1)_{xy}=(f_1)_{yx},(f_2)_{xy}=(f_2)_{yx}$. As stated above, for dimension $d>2$ we need multiplication to even phrase the question properly.","I now asked this at MO . Let $f:\mathbb{R}^d \to \mathbb{R}$ be smooth. The mixed derivatives commute: $f_{xy}=f_{yx}$. This  identity is "" universal "" in the sense that it holds for any smooth map. Question: Are there any universal identities which are not consequences of the commutation of the mixed derivatives? More explicitly, let $D_i$ be the differential operator which takes the partial derivative with respect to $x_i$. The symmetry can be written as an algebraic statement $$    D_i \circ D_j = D_j \circ D_i \tag{1}.$$ (When we choose the domain for these operators to be the space of smooth maps $\mathbb{R}^d \to \mathbb{R}^d$, so we can compose operators). Consider the subset $A$ of differential operators (which map $C^{\infty}(\mathbb{R}^d) \to C^{\infty}(\mathbb{R}^d)$) that is ""generated"" by the $D_i$ via addition, composition and multiplication*. Note that this algebraic structure $A$ has $3$ binary operations. (I am not sure if there is a term for such an ""algebraic creature"", $A$ is a ring w.r.t both operations $(+,\cdot)$ and $(+,\circ)$, but this two operations have relations, namely $$(f \cdot g) \circ h = (f \circ h) \cdot (g \circ h),$$ they ""commute"". Does such a structure have a name? ""Concrete Question:"": Are there relations in $A$ which are not consequences of the fundamental relation $(1)$?. *By multiplication (as opposed to composition ) of operators I mean the following: $$D_x \times D_y(f)=f_x \cdot f_y \, , \,  D_x∘D_y(f)=f_{xy} \, , \, (D_x \circ D_x) \times D_y(f)=f_{xx}f_y$$ etc. (Here $f$ is a scalar function, to extend the operations to $\mathbb{R}^d$-valued maps, jut act on each component separately.  I am also allowing for the $i$-th component of output to depend on partial derivatives of all components of $f:\mathbb{R}^d \to \mathbb{R}^d$). The multiplication is needed in order to talk about relations of the form of $f_x f_y=f_yf_x$ (trivially true) or $f_{xx}=f_yf_{xy}$ (clearly not universal).  (Without multiplication, there are no additional relations as observed in this answer ). In particular, I am interested to know whether the ""Cofactor Lemma"" (divergence-free rows, see below) is a ""consequence"" of the commutation of mixed derivatives (for dimension $d>2$ this involves multiplication, as well as addition and composition). The Cofactor Lemma: Let $f:\mathbb{R}^d \to \mathbb{R}^d$ be smooth. Then the Cofactor of $df$ has divergence-free rows : $$\sum_{j=1}^n \frac{\partial(Cof(Du))_{kj}}{\partial x_j} = 0, k=1,...,d.$$ In dimension $d=2$, it reduces to relation $(1)$: Given $A= \begin{pmatrix} a & b \\\ c & d \end{pmatrix}$, $\operatorname{Cof}A= \begin{pmatrix} d & -c \\\ -b & a \end{pmatrix}$, so $$ df= \begin{pmatrix} (f_1)_x & (f_1)_y \\\ (f_2)_x & (f_2)_y \end{pmatrix}, \operatorname{Cof}df= \begin{pmatrix} (f_2)_y & -(f_2)_x \\\ -(f_1)_y & (f_1)_x \end{pmatrix} .$$ We see that $\operatorname{div} (\operatorname{Cof}df)=0$ is equivalent to $(f_1)_{xy}=(f_1)_{yx},(f_2)_{xy}=(f_2)_{yx}$. As stated above, for dimension $d>2$ we need multiplication to even phrase the question properly.",,"['abstract-algebra', 'differential-operators', 'universal-property', 'congruence-relations']"
71,"UCT and Künneth, Hom-Tensor adjunction","UCT and Künneth, Hom-Tensor adjunction",,"A few days ago, there was a similar question in an other context. Künneth: Consider the tensor product of modules $H_i(X;\mathbb{Z})\otimes A$, where $A$ is an abelian group, $X$ is a topological space and $H_*$ denotes singular homology.  The theorem states there is a short exact sequence $$0 \to H_i(X; \mathbf{Z})\otimes A \, \to \, H_i(X;A) \to \operatorname{Tor}(H_{i-1}(X; \mathbf{Z}),A)\to 0.$$ UCT: There is also a short exact sequence in singular cohomology $H^*$ $$0 \to \operatorname{Ext}_\mathbf{Z}^1(\operatorname{H}_{i-1}(X; \mathbf{Z}), A) \to H^i(X; A) \,  \to \, \operatorname{Hom}_\mathbf{Z}(H_i(X; \mathbf{Z}), A)\to 0.$$ What I want to know: I know a proof of both theorems with projective and injective resolutions respectively and Eilenberg-Zilber for Künneth. Nevertheless, is it possible to deduce the theorems from each other using the fact that Hom and Tensor are adjoint functors ? (A paper ""Adjoint Functors and Equivalences"" brought me to the idea)","A few days ago, there was a similar question in an other context. Künneth: Consider the tensor product of modules $H_i(X;\mathbb{Z})\otimes A$, where $A$ is an abelian group, $X$ is a topological space and $H_*$ denotes singular homology.  The theorem states there is a short exact sequence $$0 \to H_i(X; \mathbf{Z})\otimes A \, \to \, H_i(X;A) \to \operatorname{Tor}(H_{i-1}(X; \mathbf{Z}),A)\to 0.$$ UCT: There is also a short exact sequence in singular cohomology $H^*$ $$0 \to \operatorname{Ext}_\mathbf{Z}^1(\operatorname{H}_{i-1}(X; \mathbf{Z}), A) \to H^i(X; A) \,  \to \, \operatorname{Hom}_\mathbf{Z}(H_i(X; \mathbf{Z}), A)\to 0.$$ What I want to know: I know a proof of both theorems with projective and injective resolutions respectively and Eilenberg-Zilber for Künneth. Nevertheless, is it possible to deduce the theorems from each other using the fact that Hom and Tensor are adjoint functors ? (A paper ""Adjoint Functors and Equivalences"" brought me to the idea)",,"['abstract-algebra', 'algebraic-topology', 'category-theory', 'adjoint-functors']"
72,"Liars, adjunctions, and functions $f : S \rightarrow UFS$. Does this lead anywhere interesting?","Liars, adjunctions, and functions . Does this lead anywhere interesting?",f : S \rightarrow UFS,"A student of mine was recently given the following question: ""At least one of us is lying,"" said Andrew. ""Only one of us is lying,"" said Bertas. ""Squeak, two of us are lying,"" said the Chipmunk. ""Either three or four of us are lying,"" said Daisy. ""Elmo thinks that everybody is lying,"" said Elmo. How many liars are there altogether? We can rehash this into language that is more easy to analyse as follows: Andrew: the number of truthspeakers is an element of $\{0,1,2,3,4\}.$ Bertas: the number of truthspeakers is an element of $\{4\}.$ Chipmunk: the number of truthspeakers is an element of $\{3\}.$ Daisy: the number of truthspeakers is an element of $\{1,2\}.$ Elmo: the number of truthspeakers is an element of $\{0\}.$ With a bit of thought, we see that the answer is ""$2$ truthspeakers."" It can't be $0$, because then Andrew and Elmo would be truthspeakers. It can't be $1$, because then both Andrew and Daisy would be truthspeakers. It can be $2$, with Andrew and Daisy being the only truthspeakers. It can't be $3$ or more, because the intersection of any three distinct sets listed above is empty. Anyway, I thought this question was pretty cool, enough so that I tried throwing some abstract nonsense at it. Here's what I came up with. Definition 0. Consider a category concrete over $\mathbf{Set}.$ Lets refer to its objects as algebras , and lets assume there's a free algebra on every set. Write $U$ for the underlying set functor, $F$ for the free functor, and $\Phi$ for the adjunction $F \dashv U$. Given a set $S$ equipped with a function $f : S \rightarrow UFS$ an algebra $X$, a realization of $f$ in $X$ is a function $g : S \rightarrow UX$ such that $g=U(\Phi_{S,X}(g)) \circ f$ For instance: Work over the concrete category of Boolean algebras; so in particular, $F$ is the free Boolean algebra functor. Define $S = \{A,B,C,D,E\}$. Define $f : S \rightarrow UFS$ so that it expresses whom the truthspeakers are conjectured to be: For example: $$f(A) = \neg(\neg A \wedge \neg B \wedge \neg C \wedge \neg D \wedge \neg E), \qquad f(E) = \bot$$ Then the unique realization of $f$ in the boolean algebra $\{0,1\}$ is $$g : A,D \mapsto 1, \qquad g : B,C,E \mapsto 0.$$ Here's a more simple example: Liar's paradox. The function $f : \{x\} \rightarrow F(\{x\})$ given by $x \mapsto \neg x$ has no realizations in $\{0,1\}$. What I'd like to know is: Question. Does this train of thought lead to any interesting mathematics?","A student of mine was recently given the following question: ""At least one of us is lying,"" said Andrew. ""Only one of us is lying,"" said Bertas. ""Squeak, two of us are lying,"" said the Chipmunk. ""Either three or four of us are lying,"" said Daisy. ""Elmo thinks that everybody is lying,"" said Elmo. How many liars are there altogether? We can rehash this into language that is more easy to analyse as follows: Andrew: the number of truthspeakers is an element of $\{0,1,2,3,4\}.$ Bertas: the number of truthspeakers is an element of $\{4\}.$ Chipmunk: the number of truthspeakers is an element of $\{3\}.$ Daisy: the number of truthspeakers is an element of $\{1,2\}.$ Elmo: the number of truthspeakers is an element of $\{0\}.$ With a bit of thought, we see that the answer is ""$2$ truthspeakers."" It can't be $0$, because then Andrew and Elmo would be truthspeakers. It can't be $1$, because then both Andrew and Daisy would be truthspeakers. It can be $2$, with Andrew and Daisy being the only truthspeakers. It can't be $3$ or more, because the intersection of any three distinct sets listed above is empty. Anyway, I thought this question was pretty cool, enough so that I tried throwing some abstract nonsense at it. Here's what I came up with. Definition 0. Consider a category concrete over $\mathbf{Set}.$ Lets refer to its objects as algebras , and lets assume there's a free algebra on every set. Write $U$ for the underlying set functor, $F$ for the free functor, and $\Phi$ for the adjunction $F \dashv U$. Given a set $S$ equipped with a function $f : S \rightarrow UFS$ an algebra $X$, a realization of $f$ in $X$ is a function $g : S \rightarrow UX$ such that $g=U(\Phi_{S,X}(g)) \circ f$ For instance: Work over the concrete category of Boolean algebras; so in particular, $F$ is the free Boolean algebra functor. Define $S = \{A,B,C,D,E\}$. Define $f : S \rightarrow UFS$ so that it expresses whom the truthspeakers are conjectured to be: For example: $$f(A) = \neg(\neg A \wedge \neg B \wedge \neg C \wedge \neg D \wedge \neg E), \qquad f(E) = \bot$$ Then the unique realization of $f$ in the boolean algebra $\{0,1\}$ is $$g : A,D \mapsto 1, \qquad g : B,C,E \mapsto 0.$$ Here's a more simple example: Liar's paradox. The function $f : \{x\} \rightarrow F(\{x\})$ given by $x \mapsto \neg x$ has no realizations in $\{0,1\}$. What I'd like to know is: Question. Does this train of thought lead to any interesting mathematics?",,"['abstract-algebra', 'logic', 'category-theory', 'universal-algebra', 'monads']"
73,Proof of Cohn's Irreducibility Criterion,Proof of Cohn's Irreducibility Criterion,,"I was looking for an elementary (or involving introductory level abstract algebra/analysis) proof of Cohn's Irreduciblity Criterion: If   $$ a_0, a_1, \dots, a_n \in \Bbb{Z} $$    and    $$ 0 \le a_i \le t$$    and   $$ a_0 + a_1t + a_2t^2 +\cdots + a_nt^n \in \{ \text{Primes} \} $$    then the polynomial   $$ a_0 + a_1 x + a_2 x^2 +\cdots+ a_nx^n $$    is irreducible. I began to plot out a proof. Assume that we have a polynomial that does evaluate to a prime for some $t$ satisfying the inequalities above. Then it follows, that if this polynomial is factorable, it would be factored into $$ (b_0 + b_1x + b_2x^2 + \cdots+ b_rx^r )(c_0 + c_1x + c_2x^2 + \cdots+ c_jx^j) $$ Whereas Without loss of generality we can assume $$ c_0 + c_1t + c_2t^2 + \cdots+ c_jt^j = \pm P$$ (the prime in question is $P$) $$ b_0 + b_1t + b_2t^2 + \cdots+ b_rt^r = \pm 1 $$ From here I am not clear how to proceed. Another Idea: We can try to do something with induction. Lets start with the base case of the polynomial $b_0 + b_1x +\cdots $ being linear $$ b_0 + b_1 t = 1 $$ Tells us that  $$ t = \frac{1 - b_0}{b_1} $$ But since $b_0 \ge 0, b_1 \ge 0 , t \ge 0$ it follows that $b_0 = 0$ (but then the value P never could have been prime since it would be divisible by t) So we conclude that the polynomial has no linear factors that way. On the flip side it could be that $$ b_0 + b_1 t = -1$$ $$ t = \frac{-1-b_0}{b_1} $$ Theres no $b_0 > 0$ that can make this expression greater than 0. So this case is covered. Thus we conclude there are NO linear factors. But I have no idea how to generalize this technique in a way that knocks out other polynomials too. especially given that from degree $5$ onwards there isn't even an algebraic formula for me to work with, expressing $t$.","I was looking for an elementary (or involving introductory level abstract algebra/analysis) proof of Cohn's Irreduciblity Criterion: If   $$ a_0, a_1, \dots, a_n \in \Bbb{Z} $$    and    $$ 0 \le a_i \le t$$    and   $$ a_0 + a_1t + a_2t^2 +\cdots + a_nt^n \in \{ \text{Primes} \} $$    then the polynomial   $$ a_0 + a_1 x + a_2 x^2 +\cdots+ a_nx^n $$    is irreducible. I began to plot out a proof. Assume that we have a polynomial that does evaluate to a prime for some $t$ satisfying the inequalities above. Then it follows, that if this polynomial is factorable, it would be factored into $$ (b_0 + b_1x + b_2x^2 + \cdots+ b_rx^r )(c_0 + c_1x + c_2x^2 + \cdots+ c_jx^j) $$ Whereas Without loss of generality we can assume $$ c_0 + c_1t + c_2t^2 + \cdots+ c_jt^j = \pm P$$ (the prime in question is $P$) $$ b_0 + b_1t + b_2t^2 + \cdots+ b_rt^r = \pm 1 $$ From here I am not clear how to proceed. Another Idea: We can try to do something with induction. Lets start with the base case of the polynomial $b_0 + b_1x +\cdots $ being linear $$ b_0 + b_1 t = 1 $$ Tells us that  $$ t = \frac{1 - b_0}{b_1} $$ But since $b_0 \ge 0, b_1 \ge 0 , t \ge 0$ it follows that $b_0 = 0$ (but then the value P never could have been prime since it would be divisible by t) So we conclude that the polynomial has no linear factors that way. On the flip side it could be that $$ b_0 + b_1 t = -1$$ $$ t = \frac{-1-b_0}{b_1} $$ Theres no $b_0 > 0$ that can make this expression greater than 0. So this case is covered. Thus we conclude there are NO linear factors. But I have no idea how to generalize this technique in a way that knocks out other polynomials too. especially given that from degree $5$ onwards there isn't even an algebraic formula for me to work with, expressing $t$.",,"['abstract-algebra', 'number-theory', 'elementary-number-theory', 'polynomials', 'irreducible-polynomials']"
74,Reduction modulo p of a linear group over the rational numbers,Reduction modulo p of a linear group over the rational numbers,,"A paper ( http://arxiv.org/pdf/1407.3158v2.pdf ) contains the following theorem: Suppose $\mathbb{G}$ is a connected, simply connected, semisimple algebraic group defined over $\mathbb{Q}$, and let $\Gamma \leq \mathbb{G}(\mathbb{Q})$ be a finitely generated Zariski-dense subgroup. Denote by $\mathbb{G}_p$ the smooth reduction of $\mathbb{G}$ over $\mathbb{F}_p$. Then for all sufficiently large prime numbers $p$, the reduction $\Gamma_p$ of $\Gamma$ is equal to $\mathbb{G}_p (\mathbb{F}_p)$. What confuses me is the so-called smooth reduction happening. I would understand this if $\mathbb{G}$ were defined over $\mathbb{Z}$ because then we can just take all the matrix entries modulo $p$, but what can it mean for a rational number to be reduced in this way, if that is indeed what is happening? EDIT It would help to have also an example of how this reduction would apply to an element of $\operatorname{GL}_n(\mathbb{Q})$. EDIT 2 I received the following answer to this question in a chatroom, but it is far too complicated for me to understand. In particular, I know little to no algebraic geometry, and I am only beginning to learn category theory. $\mathbb{G}$ is defined over $\operatorname{Spec} \mathbb{Q}$, so it's of finite type (over $\operatorname{Spec} \mathbb{Q}$), and so you can take the ideal sheaf locally and restrict it to $\mathbb{Z}$. -- hodgeclass","A paper ( http://arxiv.org/pdf/1407.3158v2.pdf ) contains the following theorem: Suppose $\mathbb{G}$ is a connected, simply connected, semisimple algebraic group defined over $\mathbb{Q}$, and let $\Gamma \leq \mathbb{G}(\mathbb{Q})$ be a finitely generated Zariski-dense subgroup. Denote by $\mathbb{G}_p$ the smooth reduction of $\mathbb{G}$ over $\mathbb{F}_p$. Then for all sufficiently large prime numbers $p$, the reduction $\Gamma_p$ of $\Gamma$ is equal to $\mathbb{G}_p (\mathbb{F}_p)$. What confuses me is the so-called smooth reduction happening. I would understand this if $\mathbb{G}$ were defined over $\mathbb{Z}$ because then we can just take all the matrix entries modulo $p$, but what can it mean for a rational number to be reduced in this way, if that is indeed what is happening? EDIT It would help to have also an example of how this reduction would apply to an element of $\operatorname{GL}_n(\mathbb{Q})$. EDIT 2 I received the following answer to this question in a chatroom, but it is far too complicated for me to understand. In particular, I know little to no algebraic geometry, and I am only beginning to learn category theory. $\mathbb{G}$ is defined over $\operatorname{Spec} \mathbb{Q}$, so it's of finite type (over $\operatorname{Spec} \mathbb{Q}$), and so you can take the ideal sheaf locally and restrict it to $\mathbb{Z}$. -- hodgeclass",,"['abstract-algebra', 'algebraic-geometry', 'modular-arithmetic', 'finite-fields', 'algebraic-groups']"
75,Objects whose limiting behaviour resembles a group,Objects whose limiting behaviour resembles a group,,"Is there a name for a structure that isn't a group, but that begins to behave like a group the more operations are performed? I'm trying to take the idea of an attractor from dynamical systems and impose some algebraic structure on the limiting set. Here's a tentative definition. Consider a set $S$ with a proper subset $G$ and a binary operation $\circ: S \times S\rightarrow S$, such that: $–$ $(G,\circ)$ forms a group $–$ $S$ is closed under $\circ$ $–$ if $a \in G$ or $b \in G$, then  $a\circ b \in G$ In addition, define a function $\|\cdot\|:S\rightarrow \mathbb{R}$ which satisfies: $–$ $\|g\|=0, \forall g \in G$ $–$ $\|a\|>0, \forall a \in S\backslash G$ $–$ $\|a\circ b\|< \|a\|$, and $\|a\circ b\|< \|b\|, \forall a,b \in S\backslash G$ The function $\|\cdot\|$ is meant to be a measure of distance from $G$. Here's an example. Consider $\mathbb{R}^2\backslash\{0\}$, with coordinates expressed in polar form. Define $\|(r,\theta)\|=|r-1|$, and $(r_1,\theta_1)\circ(r_2,\theta_2)=(1+\frac{r_\min}{2},\theta_1+\theta_2)$, where $r_\min=\min(\|(r_1,\theta_1)\|,\|(r_2,\theta_2)\|)$. The group in this case is the set where $r=1$. Does this kind of object (or something that behaves like it) exist in any field of study?","Is there a name for a structure that isn't a group, but that begins to behave like a group the more operations are performed? I'm trying to take the idea of an attractor from dynamical systems and impose some algebraic structure on the limiting set. Here's a tentative definition. Consider a set $S$ with a proper subset $G$ and a binary operation $\circ: S \times S\rightarrow S$, such that: $–$ $(G,\circ)$ forms a group $–$ $S$ is closed under $\circ$ $–$ if $a \in G$ or $b \in G$, then  $a\circ b \in G$ In addition, define a function $\|\cdot\|:S\rightarrow \mathbb{R}$ which satisfies: $–$ $\|g\|=0, \forall g \in G$ $–$ $\|a\|>0, \forall a \in S\backslash G$ $–$ $\|a\circ b\|< \|a\|$, and $\|a\circ b\|< \|b\|, \forall a,b \in S\backslash G$ The function $\|\cdot\|$ is meant to be a measure of distance from $G$. Here's an example. Consider $\mathbb{R}^2\backslash\{0\}$, with coordinates expressed in polar form. Define $\|(r,\theta)\|=|r-1|$, and $(r_1,\theta_1)\circ(r_2,\theta_2)=(1+\frac{r_\min}{2},\theta_1+\theta_2)$, where $r_\min=\min(\|(r_1,\theta_1)\|,\|(r_2,\theta_2)\|)$. The group in this case is the set where $r=1$. Does this kind of object (or something that behaves like it) exist in any field of study?",,"['abstract-algebra', 'limits']"
76,On a Proof in Galois Theory,On a Proof in Galois Theory,,"We have the following lemma (used in the proof of Abel's Theorem): If $\text{Char}(F)=0$, $E/F$ is a radical extension, and $K/E$ is the Galois closure of $E/F$, then $K/F$ is also a radical extension. Proof: We have $$ F=:E_0\subset E_1\subset E_2\subset\cdots\subset E_n:=E $$ such that $E_{i+1}=E_i(\alpha_i)$ where $\alpha_i^m\in E_i$. Define $K_{i+1}:=K_i(\sigma(\alpha_i),\sigma\in\text{Gal}(K/F))$ and $K_0:=E_0=F$. Note that $E_i\subset K_i$ for all $i$. Since $ (\sigma(\alpha_i)))^m=\sigma(\alpha_i^m)$ where $\alpha_i^m\in E_i$, we have $(\sigma(\alpha_i)))^m\in K_i$ and therefore $K_{i+1}/K_i$ is a radical extension. But $K_n=K$. Indeed, since $K$ is the Galois closure of $E/F$ and we have the tower of fields $K/K_n/E/F$, it suffices to show $K_n/F$ is a Galois extension. But this is the case because $K_{i+1}$ is the splitting field of the minimal polynomial of $\alpha_i$ over $F$ for all $i$. Hence the result. $\blacksquare$ Questions: $E/F$ must be finite, right? Where is used the hypothesis $\text{Char}(F)=0$ ? To even talk about the Galois closure, we must verify that $E/F$ is a separable extension. Why is it in this case? Why is $K_{i+1}$ the splitting field of $\alpha_i$ for all $i$ ? (Why) was it essential to consider a new tower of fields $K_i$ ? I thought that $K$ must be the splitting field of the minimal polynomial of $\alpha_{n-1}$ because of Proposition: If $E/F$ is a finite separable extension, then its Galois closure exists. Proof: We know that if $E/F$ is a finite separable extension, then $E=F(\alpha)$ for a certain $\alpha\in E$. Let $m\in F[x]$ be the minimal polynomial of $\alpha$. Then by hypothesis $m$  is separable. Let $E'$ denote the splitting field of $m$ over $F$. We know $E'/F$ is a Galois extension. Also, suppose $E\subset\tilde{E}\subset E'$ is such that $\tilde{E}/F$ is a Galois extension. Since $E'/F$ is finite, we know $\tilde{E}/F$ must be finite and we know that a finite Galois extension is, in particular, normal. Therefore we have $\alpha\in\tilde{E}$ with $m$ splitting over $\tilde{E}$. By unicity of the splitting field, $\tilde{E}=E'$. $\blacksquare$ So, does the proof actually amount to show that this splitting field is part of a radical extension of $F$ ? Thoughts: I think so because I only know that the Galois closure exists if $E/F$ is finite. I think $\text{Char}(F)=0$ tells us that there exists an extension of $F$ in which there is a primitive root of unity $\zeta$. In turn I think this shows that $x^m-\alpha_i^m\in F[x]$ has $m$ different roots, namely $\zeta^k\alpha_i$ for $k=1,2,\ldots,m$. Hence $x^m-\alpha_i^m$ is separable and this implies that the minimal polynomial of $\alpha_i$ over $F$ is separable. Hence this would show that $E/F$ is separable because $E=F(\alpha_0,\alpha_1,\ldots,\alpha_{n-1})$ and we know that an extension generated by separable elements is separable. See 2. I understand that every $\sigma(\alpha_i)$ must be a root of the minimal polynomial of $\alpha_i$ over $F$. However I don't see why we get all the roots. I see $\#\text{Gal}(K_{i+1})=[K_{i+1}:F]\geq [E_{i+1}:F]=\deg(m_{\alpha_i})$ where $m_{\alpha_i}\in F[x]$ is the minimal polynomial of $\alpha_i$, but maybe some automorphisms in $\text{Gal}(K/F)$ send $\alpha_i$ to a same image...?","We have the following lemma (used in the proof of Abel's Theorem): If $\text{Char}(F)=0$, $E/F$ is a radical extension, and $K/E$ is the Galois closure of $E/F$, then $K/F$ is also a radical extension. Proof: We have $$ F=:E_0\subset E_1\subset E_2\subset\cdots\subset E_n:=E $$ such that $E_{i+1}=E_i(\alpha_i)$ where $\alpha_i^m\in E_i$. Define $K_{i+1}:=K_i(\sigma(\alpha_i),\sigma\in\text{Gal}(K/F))$ and $K_0:=E_0=F$. Note that $E_i\subset K_i$ for all $i$. Since $ (\sigma(\alpha_i)))^m=\sigma(\alpha_i^m)$ where $\alpha_i^m\in E_i$, we have $(\sigma(\alpha_i)))^m\in K_i$ and therefore $K_{i+1}/K_i$ is a radical extension. But $K_n=K$. Indeed, since $K$ is the Galois closure of $E/F$ and we have the tower of fields $K/K_n/E/F$, it suffices to show $K_n/F$ is a Galois extension. But this is the case because $K_{i+1}$ is the splitting field of the minimal polynomial of $\alpha_i$ over $F$ for all $i$. Hence the result. $\blacksquare$ Questions: $E/F$ must be finite, right? Where is used the hypothesis $\text{Char}(F)=0$ ? To even talk about the Galois closure, we must verify that $E/F$ is a separable extension. Why is it in this case? Why is $K_{i+1}$ the splitting field of $\alpha_i$ for all $i$ ? (Why) was it essential to consider a new tower of fields $K_i$ ? I thought that $K$ must be the splitting field of the minimal polynomial of $\alpha_{n-1}$ because of Proposition: If $E/F$ is a finite separable extension, then its Galois closure exists. Proof: We know that if $E/F$ is a finite separable extension, then $E=F(\alpha)$ for a certain $\alpha\in E$. Let $m\in F[x]$ be the minimal polynomial of $\alpha$. Then by hypothesis $m$  is separable. Let $E'$ denote the splitting field of $m$ over $F$. We know $E'/F$ is a Galois extension. Also, suppose $E\subset\tilde{E}\subset E'$ is such that $\tilde{E}/F$ is a Galois extension. Since $E'/F$ is finite, we know $\tilde{E}/F$ must be finite and we know that a finite Galois extension is, in particular, normal. Therefore we have $\alpha\in\tilde{E}$ with $m$ splitting over $\tilde{E}$. By unicity of the splitting field, $\tilde{E}=E'$. $\blacksquare$ So, does the proof actually amount to show that this splitting field is part of a radical extension of $F$ ? Thoughts: I think so because I only know that the Galois closure exists if $E/F$ is finite. I think $\text{Char}(F)=0$ tells us that there exists an extension of $F$ in which there is a primitive root of unity $\zeta$. In turn I think this shows that $x^m-\alpha_i^m\in F[x]$ has $m$ different roots, namely $\zeta^k\alpha_i$ for $k=1,2,\ldots,m$. Hence $x^m-\alpha_i^m$ is separable and this implies that the minimal polynomial of $\alpha_i$ over $F$ is separable. Hence this would show that $E/F$ is separable because $E=F(\alpha_0,\alpha_1,\ldots,\alpha_{n-1})$ and we know that an extension generated by separable elements is separable. See 2. I understand that every $\sigma(\alpha_i)$ must be a root of the minimal polynomial of $\alpha_i$ over $F$. However I don't see why we get all the roots. I see $\#\text{Gal}(K_{i+1})=[K_{i+1}:F]\geq [E_{i+1}:F]=\deg(m_{\alpha_i})$ where $m_{\alpha_i}\in F[x]$ is the minimal polynomial of $\alpha_i$, but maybe some automorphisms in $\text{Gal}(K/F)$ send $\alpha_i$ to a same image...?",,"['abstract-algebra', 'field-theory', 'galois-theory', 'extension-field']"
77,When does a polynomial fixing a subring imply its coefficients are in that subring?,When does a polynomial fixing a subring imply its coefficients are in that subring?,,"Let $S$ be a subring of $R$.  If $p$ is a polynomial with coefficients in $S$, then $p$ fixes $S$ (as a function, that is, $p(s)\in S$ for all $s\in S$).  A converse statement is: If $p$ is a polynomial with coefficients in $R$, and $p$ fixes $S$, then the coefficients of $p$ are in $S$. My question: For which pairs $(S,R)$ does this converse hold? Some basic observations: It does not hold for $(\mathbb{Z},\mathbb{Q})$.  For example, $p(x)=\frac12 x(x+1)$ fixes $\mathbb{Z}$ but has non-integer coefficients. A direct generalization is that it does not hold if there exists $s\in S$ which is uninvertible in $S$ but invertible in $R$, and such that $S/sS$ is finite.  For example, it does not hold for $(\mathbb{Z}[\sqrt2],\mathbb{Q}[\sqrt2])$ (taking $s=2$, say, or $s=\sqrt2$), nor for $(\mathbb{Z}[\frac12],\mathbb{Q})$ (taking, say, $s=3$).  [Note: There are rings in which every uninvertible element gives rise to infinitely many congruence classes, e.g., $\mathbb{Q}[x]$.] It does hold if $S$ and $R$ are fields of characteristic zero.  (This can be shown using the inverse of a Vandermonde matrix to express the coefficients of $p$ in terms of some of its values; I'll give the details if there is interest.)","Let $S$ be a subring of $R$.  If $p$ is a polynomial with coefficients in $S$, then $p$ fixes $S$ (as a function, that is, $p(s)\in S$ for all $s\in S$).  A converse statement is: If $p$ is a polynomial with coefficients in $R$, and $p$ fixes $S$, then the coefficients of $p$ are in $S$. My question: For which pairs $(S,R)$ does this converse hold? Some basic observations: It does not hold for $(\mathbb{Z},\mathbb{Q})$.  For example, $p(x)=\frac12 x(x+1)$ fixes $\mathbb{Z}$ but has non-integer coefficients. A direct generalization is that it does not hold if there exists $s\in S$ which is uninvertible in $S$ but invertible in $R$, and such that $S/sS$ is finite.  For example, it does not hold for $(\mathbb{Z}[\sqrt2],\mathbb{Q}[\sqrt2])$ (taking $s=2$, say, or $s=\sqrt2$), nor for $(\mathbb{Z}[\frac12],\mathbb{Q})$ (taking, say, $s=3$).  [Note: There are rings in which every uninvertible element gives rise to infinitely many congruence classes, e.g., $\mathbb{Q}[x]$.] It does hold if $S$ and $R$ are fields of characteristic zero.  (This can be shown using the inverse of a Vandermonde matrix to express the coefficients of $p$ in terms of some of its values; I'll give the details if there is interest.)",,['abstract-algebra']
78,Morita-invariance of Hochschild (co)homology.,Morita-invariance of Hochschild (co)homology.,,"Ok, I’m reading the paper Homology and cohomology of associative algebras. A concise introduction to cyclic homology by Christian Kassel, and on page 19 he says that Hochschild homology is Morita-invariant, by which he means that if $R$ and $S$ are two Morita equivalent rings then $$   H_*(R,M) \cong H_*(S, Q \otimes_R M \otimes_R P) $$ (here $H_*$ denotes Hochshild homology). He then shows how $$   H_*(M,M) \cong H_*(S,S) \,, $$ and states that Hochschild cohomology groups are Morita-invariant in a similar way. I wanted to find a proof or reference for this? Just wanted to be completely sure about this and avoid any confusion, in what similar way are Hochschild cohomology groups Morita-invariant? Please be nice lol.","Ok, I’m reading the paper Homology and cohomology of associative algebras. A concise introduction to cyclic homology by Christian Kassel, and on page 19 he says that Hochschild homology is Morita-invariant, by which he means that if and are two Morita equivalent rings then (here denotes Hochshild homology). He then shows how and states that Hochschild cohomology groups are Morita-invariant in a similar way. I wanted to find a proof or reference for this? Just wanted to be completely sure about this and avoid any confusion, in what similar way are Hochschild cohomology groups Morita-invariant? Please be nice lol.","R S 
  H_*(R,M) \cong H_*(S, Q \otimes_R M \otimes_R P)
 H_* 
  H_*(M,M) \cong H_*(S,S) \,,
","['abstract-algebra', 'category-theory', 'homology-cohomology', 'homological-algebra', 'hochschild-cohomology']"
79,Number of subgroups of prime order,Number of subgroups of prime order,,"I've been doing some exercises from my introductory algebra text and came across a problem which I reduced to proving that: The number of distinct subgroups of prime order $p$ of a finite group $G$ is either $0$ or congruent to $1\pmod{p} $. With my little experience I was unable to overcome this (all I was able to conclude is that these groups are disjoint short of the identity), and also did not find any solution with a search on google (except for stronger theorems which I am not interested in because of my novice level). I remember that a similar result is widely known as one of Sylow Theorems. This result was proven by the use of group actions. But can my problem be proved without using the concept of group actions? Can this be proven WITH the use of that concept? EDIT: With help from comments I came up with this: The action Derek proposed is well-defined largely because in a group if $ab = e$ (the identity), then certainly $ba = e$. By Orbit-Stabilizer Theorem we can see that all orbits are either of size 1 or $p$ (here I had most problems, and found out cyclic group of order $p$ acts on the set of solutions in the same way). The orbits of size 1 contain precisely the elements $(x,x,x....,x)$ for some element x in G. In addition, orders of all orbits add up to $|G|^{(p-1)}$ because the orbits are equivalence classes of an equivalence relation. But certainly $(e,e,e....,e)$ is in an orbit of size 1, and that means there has to be more orbits of exactly one element, actually $p-1 + np$ more for some integer $n$. These elements form the disjoint groups I am looking for. if $p-1$ divides $(p-1 + np)$, it's easy to check the result is 1 mod p. Could someone check if I understood this correctly?","I've been doing some exercises from my introductory algebra text and came across a problem which I reduced to proving that: The number of distinct subgroups of prime order $p$ of a finite group $G$ is either $0$ or congruent to $1\pmod{p} $. With my little experience I was unable to overcome this (all I was able to conclude is that these groups are disjoint short of the identity), and also did not find any solution with a search on google (except for stronger theorems which I am not interested in because of my novice level). I remember that a similar result is widely known as one of Sylow Theorems. This result was proven by the use of group actions. But can my problem be proved without using the concept of group actions? Can this be proven WITH the use of that concept? EDIT: With help from comments I came up with this: The action Derek proposed is well-defined largely because in a group if $ab = e$ (the identity), then certainly $ba = e$. By Orbit-Stabilizer Theorem we can see that all orbits are either of size 1 or $p$ (here I had most problems, and found out cyclic group of order $p$ acts on the set of solutions in the same way). The orbits of size 1 contain precisely the elements $(x,x,x....,x)$ for some element x in G. In addition, orders of all orbits add up to $|G|^{(p-1)}$ because the orbits are equivalence classes of an equivalence relation. But certainly $(e,e,e....,e)$ is in an orbit of size 1, and that means there has to be more orbits of exactly one element, actually $p-1 + np$ more for some integer $n$. These elements form the disjoint groups I am looking for. if $p-1$ divides $(p-1 + np)$, it's easy to check the result is 1 mod p. Could someone check if I understood this correctly?",,"['abstract-algebra', 'group-theory', 'finite-groups', 'self-learning']"
80,An example of a compact multiplicatively unbounded ring,An example of a compact multiplicatively unbounded ring,,"My teacher asked me to build an associative topological Hausdorff compact ring $R$ with $1$, which is multiplicatively unbounded. That means there is a neighborhood $U\ni 1$ such that $FU\not=R$ for each finite subset $F$ of $R$. I am somewhat stuck, because I have a small stock of topological rings, and I see only two main ways to build such an example: to endow a compact topological group with a multiplication  or to endow a ring with a compact ring topology. Both of these ways require a concordance of many conditions and therefore it seems to me that my success of the construction “depends on luck, but not on method”.","My teacher asked me to build an associative topological Hausdorff compact ring $R$ with $1$, which is multiplicatively unbounded. That means there is a neighborhood $U\ni 1$ such that $FU\not=R$ for each finite subset $F$ of $R$. I am somewhat stuck, because I have a small stock of topological rings, and I see only two main ways to build such an example: to endow a compact topological group with a multiplication  or to endow a ring with a compact ring topology. Both of these ways require a concordance of many conditions and therefore it seems to me that my success of the construction “depends on luck, but not on method”.",,"['abstract-algebra', 'ring-theory', 'compactness', 'topological-groups']"
81,Kernel of the product homomorphism $A\otimes A\rightarrow A$,Kernel of the product homomorphism,A\otimes A\rightarrow A,"Let $R$ be a commutative ring with $1$ and let $A$ be a commutative $R$-algebra. We view $A\otimes A$ also as $R$-algebra, the multiplication on generators being given by $(a\otimes b)(a'\otimes b')= aa'\otimes bb'$. Denote by $m: A\otimes A\rightarrow A$ the product homomorphism, i.e. the map that maps a generator $a\otimes b$ to $ab$. My question is: Why is the kernel of $m$ generated by the elements $a\otimes 1 - 1\otimes a$ where $a$ runs through $A$? The answer seems to be so easy that I could not find it anywhere in books. Alas, it is not clear to me :( Thank you for your help!","Let $R$ be a commutative ring with $1$ and let $A$ be a commutative $R$-algebra. We view $A\otimes A$ also as $R$-algebra, the multiplication on generators being given by $(a\otimes b)(a'\otimes b')= aa'\otimes bb'$. Denote by $m: A\otimes A\rightarrow A$ the product homomorphism, i.e. the map that maps a generator $a\otimes b$ to $ab$. My question is: Why is the kernel of $m$ generated by the elements $a\otimes 1 - 1\otimes a$ where $a$ runs through $A$? The answer seems to be so easy that I could not find it anywhere in books. Alas, it is not clear to me :( Thank you for your help!",,"['abstract-algebra', 'tensor-products']"
82,Generating function for characters of representations,Generating function for characters of representations,,"One example of such a generating function that I know how to derive is for $SU(2)$, $\frac{1}{(1-tx)(1-\frac{t}{x})}$. The coefficient of $t^n$ in the above function is the character in the $n+1$ dimensional representation of $SU(2)$ for elements conjugate to the element ""x"" . Similarly I have seen that for $SU(3)$ the function is,  $$\frac{1-t_1 t_2}{(1-t_1Y_2)(1-t_1 \frac{Y_2}{Y_1})(1-t_2Y_2)(1-\frac{t_1}{Y_2})(1-\frac{t_2}{Y_1})(1-t_2 \frac{Y_1}{Y_2})}$$ The coefficient of $t_1^a t_2^b$ in the above function gives the character of the element in the conjugacy class of $Y_1^{T_3}Y_2^{T_8}$ of $SU(3)$ in the irreducible representation whose highest weight is $(a,b)$. I would like to know of the proof of the above. I would like to know of any general method of computing these functions (..like I have seen such a function in literature for $O(5)$ but again I don't know the derivation..)","One example of such a generating function that I know how to derive is for $SU(2)$, $\frac{1}{(1-tx)(1-\frac{t}{x})}$. The coefficient of $t^n$ in the above function is the character in the $n+1$ dimensional representation of $SU(2)$ for elements conjugate to the element ""x"" . Similarly I have seen that for $SU(3)$ the function is,  $$\frac{1-t_1 t_2}{(1-t_1Y_2)(1-t_1 \frac{Y_2}{Y_1})(1-t_2Y_2)(1-\frac{t_1}{Y_2})(1-\frac{t_2}{Y_1})(1-t_2 \frac{Y_1}{Y_2})}$$ The coefficient of $t_1^a t_2^b$ in the above function gives the character of the element in the conjugacy class of $Y_1^{T_3}Y_2^{T_8}$ of $SU(3)$ in the irreducible representation whose highest weight is $(a,b)$. I would like to know of the proof of the above. I would like to know of any general method of computing these functions (..like I have seen such a function in literature for $O(5)$ but again I don't know the derivation..)",,"['abstract-algebra', 'group-theory', 'representation-theory', 'lie-groups', 'lie-algebras']"
83,Computation of a Hilbert Samuel function,Computation of a Hilbert Samuel function,,"I am trying to solve the following exercise from Eisenbud's Commutative Algebra with a View Toward Algebraic Geometry: Exercise 12.1: Let $f\in R = k[x,y,z]_{(x,y,z)}$ be a homogeneous form of degree $d$ , monic in $x$ . Show that $(y,z)$ is an ideal of finite colength on $M = R/(f)$ . Compute the corresponding Hilbert-Samuel functions. I am aware that a similar question has been asked here: Compute the Hilbert-Samuel function . Nonetheless, since there are no answers or comments at the linked post, I'm posting it again here. Here is what I have tried: I have been able to show that $(y,z)$ has finite colength on $M$ , as it's quite easy to see that some large power of $(x,y,z)$ is contained in $(y,z) + \text{ann } M$ . For the second part however, I think I am stuck. To find the Hilbert Samuel polynomial, I need to compute the length of the module $M_n = (y,z)^nM/(y,z)^{n+1}M$ . I am tempted to think that this length is $d(n+1)$ , because as far as I can see the $M_n$ is a finite dimensional $k$ vector space, with basis $\{x^ay^bz^c |0\le a\le d - 1, b + c = n\}$ . However, I'm not sure if this is useful, or how I can translate this to a statement about the length of $M_n$ . I would be glad if someone could point out how to proceed.","I am trying to solve the following exercise from Eisenbud's Commutative Algebra with a View Toward Algebraic Geometry: Exercise 12.1: Let be a homogeneous form of degree , monic in . Show that is an ideal of finite colength on . Compute the corresponding Hilbert-Samuel functions. I am aware that a similar question has been asked here: Compute the Hilbert-Samuel function . Nonetheless, since there are no answers or comments at the linked post, I'm posting it again here. Here is what I have tried: I have been able to show that has finite colength on , as it's quite easy to see that some large power of is contained in . For the second part however, I think I am stuck. To find the Hilbert Samuel polynomial, I need to compute the length of the module . I am tempted to think that this length is , because as far as I can see the is a finite dimensional vector space, with basis . However, I'm not sure if this is useful, or how I can translate this to a statement about the length of . I would be glad if someone could point out how to proceed.","f\in R = k[x,y,z]_{(x,y,z)} d x (y,z) M = R/(f) (y,z) M (x,y,z) (y,z) + \text{ann } M M_n = (y,z)^nM/(y,z)^{n+1}M d(n+1) M_n k \{x^ay^bz^c |0\le a\le d - 1, b + c = n\} M_n","['abstract-algebra', 'ring-theory', 'commutative-algebra', 'modules', 'hilbert-polynomial']"
84,Does there exist a field which has infinitely many subfields?,Does there exist a field which has infinitely many subfields?,,Does there exist a field which has infinitely many subfields? Does there exist an enormous supply of such fields? I don't know how to begin.,Does there exist a field which has infinitely many subfields? Does there exist an enormous supply of such fields? I don't know how to begin.,,"['abstract-algebra', 'field-theory']"
85,"Why doesn't the definition ""$p$ is called 'prime' if $p\mid ab\implies p\mid a\,\text{ or }\,p\mid b$"" hold up when we square numbers?","Why doesn't the definition "" is called 'prime' if "" hold up when we square numbers?","p p\mid ab\implies p\mid a\,\text{ or }\,p\mid b","So, I've been given the actual definition of a prime for the first time, as opposed to the definition of an irreducible which I was previously taught (as is customary), it goes as follows: An element $p$ of a ring $R$ is called ""prime"" if $a,b\in R$ and $p|ab\rightarrow p|a$ or $p|b$ $4^2 = 16$ . So inversely $4|16$ . Now let's look at all the combinations of two integers $a,b\in R$ where $ab = 16$ and whether $p|a$ or $p|b$ : $1 * 16$ ( $4|16$ ) $2 * 8$ ( $4|8$ ) $4 * 4$ ( $4|4$ ) By my logic, $4$ is prime. However, as we all know $4 = 2^2$ . But $2$ is not a unit so $4$ is an irreducible. Since the ring of integers is an integral domain $4$ must, therefore, be prime as well. Why does this happen? Edit Thanks for your help everyone. I understand your answers logically and experimentally but not really conceptually. The reason this definition works (from my perspective) because your saying that A number is prime if it necessary to represent its multiples in   factorisations. Whereby factorisations, I mean $8 = 2^3, 6 = 2*3$ and relevantly $16 = 2^4$ . The answer I had kind-of expected from this question (and was going to include in my original query until I forgot) was that more generally if there exists the product of any amount of numbers that is equivalent to $ab$ and for each of those numbers $n$ , $n$ suffices $p|n$ then $p$ is composite. This makes much more sense to me logically. In the case of $16$ you saying that you don't need $4$ to express $16$ since you can instead use $2*2*2*2$ or $2^4$ to represent the same thing. For this reason, $4$ is composite. By this logic, you don't need to prove it for every multiple of 4, only 1. Have I completely confused myself? Can someone provide a counter-example? Also, could you provide an explanation of why my logic doesn't work? Edit 2 Yes everyone, the correct definition of includes checking the factors of every $ab$ such $p|ab$ . However, as @Vincent and I showed in our respective answers (the former much better) the definition I used is equivalent to the definition everyone else is using in integral domains. I highly suggest you read @Vincent answer as he makes this very clear. Regardless, I'm not accepting any answer that simply says that my definition is wrong when it is actually equivalent.","So, I've been given the actual definition of a prime for the first time, as opposed to the definition of an irreducible which I was previously taught (as is customary), it goes as follows: An element of a ring is called ""prime"" if and or . So inversely . Now let's look at all the combinations of two integers where and whether or : ( ) ( ) ( ) By my logic, is prime. However, as we all know . But is not a unit so is an irreducible. Since the ring of integers is an integral domain must, therefore, be prime as well. Why does this happen? Edit Thanks for your help everyone. I understand your answers logically and experimentally but not really conceptually. The reason this definition works (from my perspective) because your saying that A number is prime if it necessary to represent its multiples in   factorisations. Whereby factorisations, I mean and relevantly . The answer I had kind-of expected from this question (and was going to include in my original query until I forgot) was that more generally if there exists the product of any amount of numbers that is equivalent to and for each of those numbers , suffices then is composite. This makes much more sense to me logically. In the case of you saying that you don't need to express since you can instead use or to represent the same thing. For this reason, is composite. By this logic, you don't need to prove it for every multiple of 4, only 1. Have I completely confused myself? Can someone provide a counter-example? Also, could you provide an explanation of why my logic doesn't work? Edit 2 Yes everyone, the correct definition of includes checking the factors of every such . However, as @Vincent and I showed in our respective answers (the former much better) the definition I used is equivalent to the definition everyone else is using in integral domains. I highly suggest you read @Vincent answer as he makes this very clear. Regardless, I'm not accepting any answer that simply says that my definition is wrong when it is actually equivalent.","p R a,b\in R p|ab\rightarrow p|a p|b 4^2 = 16 4|16 a,b\in R ab = 16 p|a p|b 1 * 16 4|16 2 * 8 4|8 4 * 4 4|4 4 4 = 2^2 2 4 4 8 = 2^3, 6 = 2*3 16 = 2^4 ab n n p|n p 16 4 16 2*2*2*2 2^4 4 ab p|ab","['abstract-algebra', 'group-theory', 'ring-theory', 'prime-numbers', 'examples-counterexamples']"
86,"Non-associative, non-commutative binary operation with a identity","Non-associative, non-commutative binary operation with a identity",,"Can you give me few examples of binary operation that it is not associative, not commutative but has an identity element?","Can you give me few examples of binary operation that it is not associative, not commutative but has an identity element?",,"['abstract-algebra', 'examples-counterexamples', 'binary-operations', 'associativity']"
87,What does it even mean to say 'preserve structure'? [duplicate],What does it even mean to say 'preserve structure'? [duplicate],,"This question already has answers here : In what sense of ""structure"" do group homomorphisms ""preserve structure""? (7 answers) Closed 8 years ago . Could somebody give a concrete example of a group structure being preserved in a isomorphism, et cetera? I always hear this 'preserve structure' thing. Ok, could somebody give me a rigorous definition of what it means to preserve structure? I'm tired of hearing things like 'they behave the same way under some operation bla bla bla'","This question already has answers here : In what sense of ""structure"" do group homomorphisms ""preserve structure""? (7 answers) Closed 8 years ago . Could somebody give a concrete example of a group structure being preserved in a isomorphism, et cetera? I always hear this 'preserve structure' thing. Ok, could somebody give me a rigorous definition of what it means to preserve structure? I'm tired of hearing things like 'they behave the same way under some operation bla bla bla'",,"['abstract-algebra', 'general-topology', 'group-theory']"
88,Inconsistencies in the definition of derivative of a polynomial over a field,Inconsistencies in the definition of derivative of a polynomial over a field,,"A problem I came across defines a particular differentiation operator $D$ over the set of polynomials $\{P\}$ over a field $F$ with ""the normal formula; that is $D(\sum_{i=0}^n a_nx^i) = \sum_{i=1}^n na_nx^{i-1}$."" However, there seem to be some problems with this definition. First, we cannot assume that $F$ contains the natural numbers as a subset. For example, what is $D(x^2)$ if $F = \mathbb{Z_2}$? Furthermore, the derivative may not be well-defined, depending on the definition of equality for polynomials. For example, if $F = \mathbb{Z_3}$, then $2x^2 + x = 0$ for $x \in \{0, 1, 2\}$, but $D(2x^2 + x) = 2(2)x+1 = x + 1 \neq 0 = D(0)$. The problem then defines the derivative of rational functions $P/Q$ with the standard quotient rule formula $D(P/Q) = \frac{P'Q - PQ'}{Q^2}$ and then asks to prove that that definition is well-defined, but I can't do that without knowing that the derivative of polynomials exists and is well-defined. Can anyone shed some light on how to interpret the definition given, or what restrictions need to be made to make it work?","A problem I came across defines a particular differentiation operator $D$ over the set of polynomials $\{P\}$ over a field $F$ with ""the normal formula; that is $D(\sum_{i=0}^n a_nx^i) = \sum_{i=1}^n na_nx^{i-1}$."" However, there seem to be some problems with this definition. First, we cannot assume that $F$ contains the natural numbers as a subset. For example, what is $D(x^2)$ if $F = \mathbb{Z_2}$? Furthermore, the derivative may not be well-defined, depending on the definition of equality for polynomials. For example, if $F = \mathbb{Z_3}$, then $2x^2 + x = 0$ for $x \in \{0, 1, 2\}$, but $D(2x^2 + x) = 2(2)x+1 = x + 1 \neq 0 = D(0)$. The problem then defines the derivative of rational functions $P/Q$ with the standard quotient rule formula $D(P/Q) = \frac{P'Q - PQ'}{Q^2}$ and then asks to prove that that definition is well-defined, but I can't do that without knowing that the derivative of polynomials exists and is well-defined. Can anyone shed some light on how to interpret the definition given, or what restrictions need to be made to make it work?",,"['abstract-algebra', 'analysis', 'differential-algebra']"
89,"If $f: \mathbb Q\to \mathbb Q$ is a homomorphism, prove that $f(x)=0$ for all $x\in\mathbb Q$ or $f(x)=x$ for all $x$ in $\mathbb Q$.","If  is a homomorphism, prove that  for all  or  for all  in .",f: \mathbb Q\to \mathbb Q f(x)=0 x\in\mathbb Q f(x)=x x \mathbb Q,"If $f: \mathbb Q\to \mathbb Q$ is a homomorphism, prove that $f(x)=0$ for all $x\in\mathbb Q$ or $f(x)=x$ for all $x$ in $\mathbb Q$. I'm wondering if you can help me with this one?","If $f: \mathbb Q\to \mathbb Q$ is a homomorphism, prove that $f(x)=0$ for all $x\in\mathbb Q$ or $f(x)=x$ for all $x$ in $\mathbb Q$. I'm wondering if you can help me with this one?",,"['abstract-algebra', 'ring-theory']"
90,Closure under matrix multiplication for 2x2 matrices,Closure under matrix multiplication for 2x2 matrices,,"I need to show that this set is closed under matrix multiplication, is there a better way than doing it via a Cayley table? Or rather I assume there is and I just can't get my head around it. Any help would be greatly appreciated. $\begin{gather*} M= \left\{ \begin{bmatrix} 0 & 1\\ 1 & 0 \end{bmatrix}, \begin{bmatrix} 0 & -1\\ -1 & 0 \end{bmatrix}, \begin{bmatrix} 1 & 0\\ 0 & 1 \end{bmatrix}, \begin{bmatrix} -1 & 0\\ 0 & -1 \end{bmatrix}, \begin{bmatrix} 0 & 1\\ -1 & 0 \end{bmatrix}, \begin{bmatrix} 0 & -1\\ 1 & 0 \end{bmatrix}, \begin{bmatrix} 1 & 0\\ 0 & -1 \end{bmatrix}, \begin{bmatrix} -1 & 0\\ 0 & 1 \end{bmatrix} \right\}\\ \end{gather*}$","I need to show that this set is closed under matrix multiplication, is there a better way than doing it via a Cayley table? Or rather I assume there is and I just can't get my head around it. Any help would be greatly appreciated. $\begin{gather*} M= \left\{ \begin{bmatrix} 0 & 1\\ 1 & 0 \end{bmatrix}, \begin{bmatrix} 0 & -1\\ -1 & 0 \end{bmatrix}, \begin{bmatrix} 1 & 0\\ 0 & 1 \end{bmatrix}, \begin{bmatrix} -1 & 0\\ 0 & -1 \end{bmatrix}, \begin{bmatrix} 0 & 1\\ -1 & 0 \end{bmatrix}, \begin{bmatrix} 0 & -1\\ 1 & 0 \end{bmatrix}, \begin{bmatrix} 1 & 0\\ 0 & -1 \end{bmatrix}, \begin{bmatrix} -1 & 0\\ 0 & 1 \end{bmatrix} \right\}\\ \end{gather*}$",,"['abstract-algebra', 'matrices', 'proof-writing', 'cayley-table']"
91,Morphisms in the category of rings,Morphisms in the category of rings,,"We know that in the category of (unitary) rings, $\mathbb{Z}$ is the itinial object, i.e. it is the only ring such that for each ring $A,$ there exists a unique ring homomorphism $f:\mathbb{Z} \to A$. This means, in particular, that $\mathbb{R}$ does not satisfy this property, so for a certain ring $B$, we can construct $g:\mathbb{R} \to B$ and $h:\mathbb{R} \to B$ such that $g \ne h$. So far, I have proven that $B$ cannot be an ordered ring and cannot be $\mathbb{R}^n \ (n\in\mathbb{N})$. Can you help me finding such a ring $B$?","We know that in the category of (unitary) rings, $\mathbb{Z}$ is the itinial object, i.e. it is the only ring such that for each ring $A,$ there exists a unique ring homomorphism $f:\mathbb{Z} \to A$. This means, in particular, that $\mathbb{R}$ does not satisfy this property, so for a certain ring $B$, we can construct $g:\mathbb{R} \to B$ and $h:\mathbb{R} \to B$ such that $g \ne h$. So far, I have proven that $B$ cannot be an ordered ring and cannot be $\mathbb{R}^n \ (n\in\mathbb{N})$. Can you help me finding such a ring $B$?",,"['abstract-algebra', 'ring-theory', 'category-theory']"
92,$\sqrt[31]{12} +\sqrt[12]{31}$ is irrational,is irrational,\sqrt[31]{12} +\sqrt[12]{31},"Prove that $\sqrt[31]{12} +\sqrt[12]{31}$ is irrational. I would assume that $\sqrt[31]{12} +\sqrt[12]{31}$ is rational and try to find a contradiction. However, I don't know where to start. Can someone give me a tip on how to approach this problem?","Prove that $\sqrt[31]{12} +\sqrt[12]{31}$ is irrational. I would assume that $\sqrt[31]{12} +\sqrt[12]{31}$ is rational and try to find a contradiction. However, I don't know where to start. Can someone give me a tip on how to approach this problem?",,"['abstract-algebra', 'algebra-precalculus', 'roots', 'extension-field', 'irrational-numbers']"
93,Irreducibility of $x^5 -x -1$ by reduction mod 5,Irreducibility of  by reduction mod 5,x^5 -x -1,"Is there a quick way of deducing that $x^5-x-1 \in \mathbb{Z}[x]$ is irreducible by reducing it mod 5, other than verifying that it has no roots in $\mathbb{Z}_5$ and no factorization as the product of a factor of order 2 and a factor of order 3?","Is there a quick way of deducing that $x^5-x-1 \in \mathbb{Z}[x]$ is irreducible by reducing it mod 5, other than verifying that it has no roots in $\mathbb{Z}_5$ and no factorization as the product of a factor of order 2 and a factor of order 3?",,"['abstract-algebra', 'polynomials']"
94,$S_6$ can't have a subgroup isomorphic to $S_3\times S_4$,can't have a subgroup isomorphic to,S_6 S_3\times S_4,How to show that $S_6$ can't have a subgroup isomorphic to $S_3\times S_4$? Please help me. I'm clueless.,How to show that $S_6$ can't have a subgroup isomorphic to $S_3\times S_4$? Please help me. I'm clueless.,,"['abstract-algebra', 'group-theory']"
95,Is a submodule of a flat module flat?,Is a submodule of a flat module flat?,,"This is probably obvious but I am getting stuck thinking about it: Let $A$ be a commutative ring with unity, $M$ a flat $A$-module, and $N\subset M$ a submodule. Is $N$ necessarily flat over $A$? I haven't found a simple counterexample, but I also haven't found more than a vague intuitive reason to think so. Thanks in advance.","This is probably obvious but I am getting stuck thinking about it: Let $A$ be a commutative ring with unity, $M$ a flat $A$-module, and $N\subset M$ a submodule. Is $N$ necessarily flat over $A$? I haven't found a simple counterexample, but I also haven't found more than a vague intuitive reason to think so. Thanks in advance.",,"['abstract-algebra', 'commutative-algebra', 'flatness']"
96,Show that $x^{3}-3$ irreducible over $\mathbb{Q}(\sqrt{-3})$,Show that  irreducible over,x^{3}-3 \mathbb{Q}(\sqrt{-3}),Is there a slick way to show that $x^{3}-3$ is irreducible over $F= \mathbb{Q}(\sqrt{-3})$? What I did seems kind of convoluted (showing directly that there is no root in F). Thanks,Is there a slick way to show that $x^{3}-3$ is irreducible over $F= \mathbb{Q}(\sqrt{-3})$? What I did seems kind of convoluted (showing directly that there is no root in F). Thanks,,"['abstract-algebra', 'irreducible-polynomials']"
97,Are there any Symmetric Groups that are cyclic?,Are there any Symmetric Groups that are cyclic?,,"Are there any Symmetric Groups that are cyclic? Because I have been doing some problems and I tend to notice that the problems I do that involve the symmetric group are not cyclic meaning they do not have a generator which generates the set. So are there any cases in which any of the symmetric group is cyclic? If not, then why are none of them cyclic?","Are there any Symmetric Groups that are cyclic? Because I have been doing some problems and I tend to notice that the problems I do that involve the symmetric group are not cyclic meaning they do not have a generator which generates the set. So are there any cases in which any of the symmetric group is cyclic? If not, then why are none of them cyclic?",,"['abstract-algebra', 'group-theory', 'permutations', 'symmetric-groups', 'cyclic-groups']"
98,book with lot of examples on abstract algebra and topology,book with lot of examples on abstract algebra and topology,,"I have read a lot of blogs and forums to find out the best of the books on Abstract algebra and Topology but on going through the books I realized that they are full of proofs and all kind of theorems and corollaries. But the disappointing point is that there are either very small number of examples or none at all to support that particular theorem or proof. My idea of example is practically calculating using a set with contents rather than just saying ""a set of integers"" and the calculations should go to the end, for example calculating index in subgroups. Everything should be step by step. I went through ""Topics in Algebra"" by I.N.Herstein but it's full of rigor. I faced similar problems with topology. Please help me by suggesting books where I can find lot and lot of simple examples on these topics. Please don't suggest any advanced book but something that keeps me near the base with lot of permutations of ideas. Even if you have something to suggest other than these, you are most welcome.","I have read a lot of blogs and forums to find out the best of the books on Abstract algebra and Topology but on going through the books I realized that they are full of proofs and all kind of theorems and corollaries. But the disappointing point is that there are either very small number of examples or none at all to support that particular theorem or proof. My idea of example is practically calculating using a set with contents rather than just saying ""a set of integers"" and the calculations should go to the end, for example calculating index in subgroups. Everything should be step by step. I went through ""Topics in Algebra"" by I.N.Herstein but it's full of rigor. I faced similar problems with topology. Please help me by suggesting books where I can find lot and lot of simple examples on these topics. Please don't suggest any advanced book but something that keeps me near the base with lot of permutations of ideas. Even if you have something to suggest other than these, you are most welcome.",,"['abstract-algebra', 'general-topology', 'reference-request', 'book-recommendation']"
99,"If a group is $3$-abelian and $5$-abelian, then it is abelian","If a group is -abelian and -abelian, then it is abelian",3 5,"In a group $G$ , $(ab)^{5}=a^{5}b^{5},\forall a,b\in G$ and $(ab)^{3}=a^{3}b^{3}$ then prove that $G$ is abelian. I know that for three consecutive integer if $(ab)^{i}=a^{i}b^{i},\forall a,b\in G$ holds then $G$ is abelian. I know I have to use this property and I have to use three consecutive integer $3,4$ and $5$ . But I am stuck.","In a group , and then prove that is abelian. I know that for three consecutive integer if holds then is abelian. I know I have to use this property and I have to use three consecutive integer and . But I am stuck.","G (ab)^{5}=a^{5}b^{5},\forall a,b\in G (ab)^{3}=a^{3}b^{3} G (ab)^{i}=a^{i}b^{i},\forall a,b\in G G 3,4 5","['abstract-algebra', 'group-theory', 'abelian-groups']"
