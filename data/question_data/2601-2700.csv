,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Prove that every such $f$ is $=0$ everywhere,Prove that every such  is  everywhere,f =0,"Let $f: \mathbb{R} \to \mathbb{R}$ be differentiable;   let $0 \leq f'(x) \leq f(x)$ for all $x \in \mathbb{R}$;   and let $f$ vanish at some point. Prove that $f = 0$ on $\mathbb{R}$. Since there is some $c \in \mathbb{R}$ such that $f(c) = 0$, by the mean-value theorem for derivatives for every $x < c$ there is some $x_{1} \in ]x,c[$ such that $f(c) - f(x) = -f(x) = f'(x_{1})(c-x) \geq 0$. Since $f \geq f'\geq 0$ on $\mathbb{R}$, we conclude that $f(x) = 0$ for all $x < c$. But what about $x > c$? The mean-value theorem in this case seems not to be helpful?","Let $f: \mathbb{R} \to \mathbb{R}$ be differentiable;   let $0 \leq f'(x) \leq f(x)$ for all $x \in \mathbb{R}$;   and let $f$ vanish at some point. Prove that $f = 0$ on $\mathbb{R}$. Since there is some $c \in \mathbb{R}$ such that $f(c) = 0$, by the mean-value theorem for derivatives for every $x < c$ there is some $x_{1} \in ]x,c[$ such that $f(c) - f(x) = -f(x) = f'(x_{1})(c-x) \geq 0$. Since $f \geq f'\geq 0$ on $\mathbb{R}$, we conclude that $f(x) = 0$ for all $x < c$. But what about $x > c$? The mean-value theorem in this case seems not to be helpful?",,"['real-analysis', 'derivatives']"
1,"Find $f:C\to\mathbb{R}^2$ continuous and bijective but not open, $C\subset\mathbb{R}^2$ is closed","Find  continuous and bijective but not open,  is closed",f:C\to\mathbb{R}^2 C\subset\mathbb{R}^2,"Are there a closed subspace  $C$ of $\mathbb{R}^2$ and a continuous, bijective function  $f:C\to\mathbb{R}^2$ that is not open? I cannot think about a continuous and bijective function $f:C\to\mathbb{R}^2$. My best candidate was the set $C=[0,+\infty)\times[0,+\infty)$ and the function  $$f(s,t)=\Bigg(s\cos\bigg(2\pi\frac{t}{t+1}\bigg),s\sin\bigg(2\pi\frac{t}{t+1}\bigg)\Bigg),$$ but this is not injective in $\{0\}\times[0,+\infty)$. The only hint that I can think about is that $C$ cannot be compact, because $f$ is bijective. Therefore, $C$ cannot be bounded. An idea that I've been trying is to work in the sphere $S^2$. There, we can think of $C$ as a subset of $S^2$ with N=(0,0,1) as a limit point such that $C\cup\{N\}$ is compact. Then the function $f$ takes $C$ onto $S^2-\{N\}$. But in this way I only see open functions. Any suggestions?","Are there a closed subspace  $C$ of $\mathbb{R}^2$ and a continuous, bijective function  $f:C\to\mathbb{R}^2$ that is not open? I cannot think about a continuous and bijective function $f:C\to\mathbb{R}^2$. My best candidate was the set $C=[0,+\infty)\times[0,+\infty)$ and the function  $$f(s,t)=\Bigg(s\cos\bigg(2\pi\frac{t}{t+1}\bigg),s\sin\bigg(2\pi\frac{t}{t+1}\bigg)\Bigg),$$ but this is not injective in $\{0\}\times[0,+\infty)$. The only hint that I can think about is that $C$ cannot be compact, because $f$ is bijective. Therefore, $C$ cannot be bounded. An idea that I've been trying is to work in the sphere $S^2$. There, we can think of $C$ as a subset of $S^2$ with N=(0,0,1) as a limit point such that $C\cup\{N\}$ is compact. Then the function $f$ takes $C$ onto $S^2-\{N\}$. But in this way I only see open functions. Any suggestions?",,"['real-analysis', 'general-topology']"
2,Functions that satisfy $f(x+y)=f(x)f(y)$ and $f(1)=e$,Functions that satisfy  and,f(x+y)=f(x)f(y) f(1)=e,"My real analysis professor mentioned in passing that there exist functions $f:\mathbb{R}\rightarrow\mathbb{R}$ that satisfy all of the following conditions for all $a,b \in \mathbb{R}$: $$f(1)=e$$ $$f(a+b)=f(a)f(b)$$ $$\exists\, x \in \mathbb{R} \,\,\,\,\text{ such that } \,\,\,f(x)\neq e^{x}$$ I was trying to find such a function and I encountered some difficulty. I tried finding out which values of $x$ necessitated $f(x) = e^x$ ... I started with $0$ and got to all of the integers, then to the rationals. From property $2$ we can let $a=0$ to find $f(0)$ as follows: $$f(b) = f(0+b) = f(0)f(b)$$ $$f(0)=1$$ For any positive integer $n \in \mathbb{N}$ we can find $f(n)$ as $$f(n) = f(1+(n-1)) = f(1)f(n-1) =\cdots=f(1)^n = e^n$$ We can find $f(-1)$ as follows: $$1 = f(0) = f(-1+1) = f(-1)f(1) = f(-1) \cdot e$$ $$f(-1) = \frac{1}{e} = e^{-1}$$ This lets us find $f(-n)$ for any $n \in \mathbb{N}$: $$f(-n) = f(-1 + -(n-1)) = f(-1)f(-(n-1)) = \cdots = f(-1)^n = e^{-n}$$ Thus, for all $m \in \mathbb{Z}$, $f(m)=e^m$. We can find $f(1/2)$ as follows: $$e = f(1) = f\left(\frac{1}{2}+\frac{1}{2}\right) = f\left(\frac{1}{2}\right)^2$$ $$f\left(\frac{1}{2}\right) = \pm \sqrt{e} = \pm \,e^{1/2}$$ Similarly, $$f\left(\frac{1}{3}\right) = \sqrt[3]{e} = e^{1/3}$$ We can also get $$f\left(\frac{2}{3}\right) = e^{2/3}$$ It follows quite quickly that for all $p/q \in \mathbb{Q}$ with $p \in \mathbb{Z}$, $q \in \mathbb{N}$ (so that $q > 0$), and $\gcd(p,q)=1$, we have $$f\left(\frac{p}{q}\right) = \pm \,e^{p/q}$$ when $q$ is even, and $$f\left(\frac{p}{q}\right) = e^{p/q}$$ when $q$ is odd. Since these work for all rationals, we can say that if $f$ is continuous, then $f(x) = e^x$ for all $x \in \mathbb{R}$. But we don't know that $f$ is continuous, so there can definitely still be values of $x$ for which $f(x) \neq e^x$. Namely, $x \in \mathbb{R}$ with $x \not\in\mathbb{Q}$. Let's take the first example of an irrational which comes to mind: $\sqrt{2}$ Consider $f\left(\sqrt{2}\right) \in \mathbb{R}$. Just as we used $f(1) = e$ above, for any rational $p/q$ we can derive $$f\left(\frac{p}{q}\sqrt{2}\right) = e^{p/q}f\left(\sqrt{2}\right)$$ This could have a $\pm$ in front if $q$ is even. In any case, if we ignore the possible negative values, we can look at the graph of this function, plotting the points we know so far. At any rational $x$, it is $e^x$. At $\sqrt{2}$ it is some real number, call it $k$. At any rational multiple of $\sqrt{2}$, $x\sqrt{2}$, it is $ke^x$. So if we only look at rational multiples of $\sqrt{2}$, the function looks like a stretched copy of $e^x$, where the extent of stretching depends on the value of $f\left(\sqrt{2}\right)$. At this point, I thought I'd found a function that satisfied all three properties, and that I could explicitly write its values. Suppose $f\left(\sqrt{2}\right) = 5$ and $f\left(\frac{p}{q}\sqrt{2}\right) = 5e^{p/q}$ for any rational $p/q$. For any other $x \in \mathbb{R}$, we let $f(x) = e^x$. Unfortunately, this falls apart quickly. Consider $$a=\frac{\sqrt{2}+\sqrt{3}}{2}$$ $$b = \frac{\sqrt{2}-\sqrt{3}}{2}$$ Neither are multiples of $\sqrt{2}$ but their sum is. $f(a+b) = f\left(\sqrt{2}\right)$ has to simultaneously be $e^\sqrt{2}$ and $5$, which is a problem. My subsequent efforts all resulted in failure. I may have found some functions that satisfied the three conditions, exploiting the $\pm$ that appears with even-denominator fractions, but if I added a fourth condition for $f(x)$ to be positive, I couldn't write out any functions that worked. I asked my professor about this, and he said that it wasn't possible to write out a function, and that was why I wasn't able to. He said that mathematicians only know they exist because of something called a Hamel basis. In a fortunate turn of events, the textbook for my set theory class had a page or two on the existence of a Hamel basis, and it turns out that the proof of their existence requires the Axiom of Choice. We haven't gotten close to talking about the Axiom of Choice in my set theory class, so I'm not too clear on it, and I didn't really understand it. This brings me, at long last, to my question(s). If such functions are impossible to write out explicitly, how do we know they exist? It's not just that we've proven that they exist and simply haven't managed to find one yet, it's that we've proven (somehow!?) that they can't be explicitly written out. Is the proof of the existence of a Hamel basis uncontroversial? Is the assumption of the Axiom of Choice something that every mathematician thinks is a good one? How do we know that it's impossible to write out a function that has positive values and satisfies properites $1$, $2$ and $3$?","My real analysis professor mentioned in passing that there exist functions $f:\mathbb{R}\rightarrow\mathbb{R}$ that satisfy all of the following conditions for all $a,b \in \mathbb{R}$: $$f(1)=e$$ $$f(a+b)=f(a)f(b)$$ $$\exists\, x \in \mathbb{R} \,\,\,\,\text{ such that } \,\,\,f(x)\neq e^{x}$$ I was trying to find such a function and I encountered some difficulty. I tried finding out which values of $x$ necessitated $f(x) = e^x$ ... I started with $0$ and got to all of the integers, then to the rationals. From property $2$ we can let $a=0$ to find $f(0)$ as follows: $$f(b) = f(0+b) = f(0)f(b)$$ $$f(0)=1$$ For any positive integer $n \in \mathbb{N}$ we can find $f(n)$ as $$f(n) = f(1+(n-1)) = f(1)f(n-1) =\cdots=f(1)^n = e^n$$ We can find $f(-1)$ as follows: $$1 = f(0) = f(-1+1) = f(-1)f(1) = f(-1) \cdot e$$ $$f(-1) = \frac{1}{e} = e^{-1}$$ This lets us find $f(-n)$ for any $n \in \mathbb{N}$: $$f(-n) = f(-1 + -(n-1)) = f(-1)f(-(n-1)) = \cdots = f(-1)^n = e^{-n}$$ Thus, for all $m \in \mathbb{Z}$, $f(m)=e^m$. We can find $f(1/2)$ as follows: $$e = f(1) = f\left(\frac{1}{2}+\frac{1}{2}\right) = f\left(\frac{1}{2}\right)^2$$ $$f\left(\frac{1}{2}\right) = \pm \sqrt{e} = \pm \,e^{1/2}$$ Similarly, $$f\left(\frac{1}{3}\right) = \sqrt[3]{e} = e^{1/3}$$ We can also get $$f\left(\frac{2}{3}\right) = e^{2/3}$$ It follows quite quickly that for all $p/q \in \mathbb{Q}$ with $p \in \mathbb{Z}$, $q \in \mathbb{N}$ (so that $q > 0$), and $\gcd(p,q)=1$, we have $$f\left(\frac{p}{q}\right) = \pm \,e^{p/q}$$ when $q$ is even, and $$f\left(\frac{p}{q}\right) = e^{p/q}$$ when $q$ is odd. Since these work for all rationals, we can say that if $f$ is continuous, then $f(x) = e^x$ for all $x \in \mathbb{R}$. But we don't know that $f$ is continuous, so there can definitely still be values of $x$ for which $f(x) \neq e^x$. Namely, $x \in \mathbb{R}$ with $x \not\in\mathbb{Q}$. Let's take the first example of an irrational which comes to mind: $\sqrt{2}$ Consider $f\left(\sqrt{2}\right) \in \mathbb{R}$. Just as we used $f(1) = e$ above, for any rational $p/q$ we can derive $$f\left(\frac{p}{q}\sqrt{2}\right) = e^{p/q}f\left(\sqrt{2}\right)$$ This could have a $\pm$ in front if $q$ is even. In any case, if we ignore the possible negative values, we can look at the graph of this function, plotting the points we know so far. At any rational $x$, it is $e^x$. At $\sqrt{2}$ it is some real number, call it $k$. At any rational multiple of $\sqrt{2}$, $x\sqrt{2}$, it is $ke^x$. So if we only look at rational multiples of $\sqrt{2}$, the function looks like a stretched copy of $e^x$, where the extent of stretching depends on the value of $f\left(\sqrt{2}\right)$. At this point, I thought I'd found a function that satisfied all three properties, and that I could explicitly write its values. Suppose $f\left(\sqrt{2}\right) = 5$ and $f\left(\frac{p}{q}\sqrt{2}\right) = 5e^{p/q}$ for any rational $p/q$. For any other $x \in \mathbb{R}$, we let $f(x) = e^x$. Unfortunately, this falls apart quickly. Consider $$a=\frac{\sqrt{2}+\sqrt{3}}{2}$$ $$b = \frac{\sqrt{2}-\sqrt{3}}{2}$$ Neither are multiples of $\sqrt{2}$ but their sum is. $f(a+b) = f\left(\sqrt{2}\right)$ has to simultaneously be $e^\sqrt{2}$ and $5$, which is a problem. My subsequent efforts all resulted in failure. I may have found some functions that satisfied the three conditions, exploiting the $\pm$ that appears with even-denominator fractions, but if I added a fourth condition for $f(x)$ to be positive, I couldn't write out any functions that worked. I asked my professor about this, and he said that it wasn't possible to write out a function, and that was why I wasn't able to. He said that mathematicians only know they exist because of something called a Hamel basis. In a fortunate turn of events, the textbook for my set theory class had a page or two on the existence of a Hamel basis, and it turns out that the proof of their existence requires the Axiom of Choice. We haven't gotten close to talking about the Axiom of Choice in my set theory class, so I'm not too clear on it, and I didn't really understand it. This brings me, at long last, to my question(s). If such functions are impossible to write out explicitly, how do we know they exist? It's not just that we've proven that they exist and simply haven't managed to find one yet, it's that we've proven (somehow!?) that they can't be explicitly written out. Is the proof of the existence of a Hamel basis uncontroversial? Is the assumption of the Axiom of Choice something that every mathematician thinks is a good one? How do we know that it's impossible to write out a function that has positive values and satisfies properites $1$, $2$ and $3$?",,"['real-analysis', 'elementary-set-theory', 'functional-equations', 'axiom-of-choice']"
3,A derivative which is not Lebesgue integrable on any interval?,A derivative which is not Lebesgue integrable on any interval?,,"If $f=x^2\sin(x^{-2})$, then $f'$ exists everywhere (including $x=0$) but $f'$ is not Lebesgue integrable on $[0,1]$ (precisely because of the singularity at $x=0).$ I'm trying to find a function $f$ such that $f'$ exists everywhere but $f'$ is not Lebesgue integrable on any interval. Perhaps someone can recall a standard example?","If $f=x^2\sin(x^{-2})$, then $f'$ exists everywhere (including $x=0$) but $f'$ is not Lebesgue integrable on $[0,1]$ (precisely because of the singularity at $x=0).$ I'm trying to find a function $f$ such that $f'$ exists everywhere but $f'$ is not Lebesgue integrable on any interval. Perhaps someone can recall a standard example?",,"['real-analysis', 'derivatives', 'examples-counterexamples', 'lebesgue-integral']"
4,Proving that a uniformly cauchy sequence of functions $f_n:\mathbb{R}\to\mathbb{R}$ is uniformly convergent.,Proving that a uniformly cauchy sequence of functions  is uniformly convergent.,f_n:\mathbb{R}\to\mathbb{R},"Before asking this question, I have scoured the stackexchange for a satisfactory answer, but could not find one. Some answers mentioned using the $\epsilon/3$ trick. I have attempted a proof but could not find a use for this trick. Below is my proof: Let $f_n(x)$ be a sequence such that $\forall\epsilon>0\exists N>0,\forall m,n>N,|f_m(x)-f_n(x)|<\epsilon$. Prove uniform convergence for this sequence. Define $f(x)=lim_{n\rightarrow\infty}f_n(x)$. This is well defined as $f_n(x)$ is a cauchy sequence for all x. For fixed m > N and a given $\epsilon>0$, $\forall n>N, |f_m(x)-f_n(x)|<\epsilon$. Or $f_m(x)-\epsilon<f_n(x)<f_m(x)+\epsilon$. $f_m(x)-\epsilon<\lim_{n\rightarrow\infty}f_n(x)<f_m(x)+\epsilon$. Or $f_m(x)-\epsilon<f(x)<f_m(x)+\epsilon$. i.e, $|f_m(x)-f(x)|<\epsilon$. Since m was chosen arbitrarily, we have $\forall m>N, |f_m(x)-f(x)|<\epsilon$ Which proves uniform convergence. Is there any mistake in above reasoning. I would be grateful for anyone to point it out.","Before asking this question, I have scoured the stackexchange for a satisfactory answer, but could not find one. Some answers mentioned using the $\epsilon/3$ trick. I have attempted a proof but could not find a use for this trick. Below is my proof: Let $f_n(x)$ be a sequence such that $\forall\epsilon>0\exists N>0,\forall m,n>N,|f_m(x)-f_n(x)|<\epsilon$. Prove uniform convergence for this sequence. Define $f(x)=lim_{n\rightarrow\infty}f_n(x)$. This is well defined as $f_n(x)$ is a cauchy sequence for all x. For fixed m > N and a given $\epsilon>0$, $\forall n>N, |f_m(x)-f_n(x)|<\epsilon$. Or $f_m(x)-\epsilon<f_n(x)<f_m(x)+\epsilon$. $f_m(x)-\epsilon<\lim_{n\rightarrow\infty}f_n(x)<f_m(x)+\epsilon$. Or $f_m(x)-\epsilon<f(x)<f_m(x)+\epsilon$. i.e, $|f_m(x)-f(x)|<\epsilon$. Since m was chosen arbitrarily, we have $\forall m>N, |f_m(x)-f(x)|<\epsilon$ Which proves uniform convergence. Is there any mistake in above reasoning. I would be grateful for anyone to point it out.",,['real-analysis']
5,Forcing series convergence,Forcing series convergence,,"I am trying to figure this out: $\mathscr{S}=\big\{(a_n),(b_n),\dots \big\}$ is a finite set of real, null sequences. Does there exist a sequence $(\epsilon_n)$ , where $\epsilon_k=\pm 1$ for each $k$ , such that: $$\forall\;(x_n)\in\mathscr{S}:\quad \sum_n \epsilon_n x_n<\infty\;?$$ A special case of this problem was posed by one of my lecturers: does every null sequence in $\mathbb{C}$ admit a sequence $(\epsilon_n)$ of signs such that $\sum\epsilon_nz_n$ converges? The answer is yes. We can always choose signs so that $|\epsilon_1z_1+\cdots\epsilon_nz_n|\leq \sqrt{3}$ for all $n$ , with some assumptions on $|z_n|$ . The geometric nature of the proof prevents me from generalising though. Any ideas how to deal with the general case?","I am trying to figure this out: is a finite set of real, null sequences. Does there exist a sequence , where for each , such that: A special case of this problem was posed by one of my lecturers: does every null sequence in admit a sequence of signs such that converges? The answer is yes. We can always choose signs so that for all , with some assumptions on . The geometric nature of the proof prevents me from generalising though. Any ideas how to deal with the general case?","\mathscr{S}=\big\{(a_n),(b_n),\dots \big\} (\epsilon_n) \epsilon_k=\pm 1 k \forall\;(x_n)\in\mathscr{S}:\quad \sum_n \epsilon_n x_n<\infty\;? \mathbb{C} (\epsilon_n) \sum\epsilon_nz_n |\epsilon_1z_1+\cdots\epsilon_nz_n|\leq \sqrt{3} n |z_n|","['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
6,Approximate Holder continuous functions by smooth functions,Approximate Holder continuous functions by smooth functions,,"Let $g \in C^{\alpha} (B_1)$ be given. Can we find a sequence $(f_n) \subset C^{\infty} (B_1)$ such that $f_n \rightarrow g$ in $C^{\alpha}(\overline{B_1})$? If so, how can it be done? I have tried using the method to show that given $g \in C_0(B_1)$, there exists $(f_n) \subset C^{\infty} (B_1)$ such that $f_n \rightarrow g$ in $C(\overline{B_1})$, that is, I make use of the modllifier $\varphi_\varepsilon$ and consider $g \star \varphi_\varepsilon$, then $$|g \star \varphi_\varepsilon (x) - g(x)| \leq \int_{B_1} \varphi(z)|g(x - \varepsilon z) - g(x)|dz \rightarrow 0$$ uniformly since $g$ is uniformly continuous. Using the approach, the only fact I need to show is $[g \star \varphi_\varepsilon - g]_\alpha \rightarrow 0$ as well. However, this is where I got stuck. If I try to calculate the semi-norm directly, then we have $$[g \star \varphi_\varepsilon - g]_\alpha = \sup_{x \neq y} |\int_{B_1} \varphi(z) \frac{(g(x - \varepsilon z) - g(x)) - (g(y - \varepsilon z) - g(y))}{|x - y|^{\alpha}}dz|$$ and I have no idea how can I proceed. I believe this is a standard and elementary approximation problem, but I have yet to find any text that give light to this problem and spent quite some time on it. So, could anyone give me some kind of directions or answers? It would then be a great help to my studies and thanks in advance.","Let $g \in C^{\alpha} (B_1)$ be given. Can we find a sequence $(f_n) \subset C^{\infty} (B_1)$ such that $f_n \rightarrow g$ in $C^{\alpha}(\overline{B_1})$? If so, how can it be done? I have tried using the method to show that given $g \in C_0(B_1)$, there exists $(f_n) \subset C^{\infty} (B_1)$ such that $f_n \rightarrow g$ in $C(\overline{B_1})$, that is, I make use of the modllifier $\varphi_\varepsilon$ and consider $g \star \varphi_\varepsilon$, then $$|g \star \varphi_\varepsilon (x) - g(x)| \leq \int_{B_1} \varphi(z)|g(x - \varepsilon z) - g(x)|dz \rightarrow 0$$ uniformly since $g$ is uniformly continuous. Using the approach, the only fact I need to show is $[g \star \varphi_\varepsilon - g]_\alpha \rightarrow 0$ as well. However, this is where I got stuck. If I try to calculate the semi-norm directly, then we have $$[g \star \varphi_\varepsilon - g]_\alpha = \sup_{x \neq y} |\int_{B_1} \varphi(z) \frac{(g(x - \varepsilon z) - g(x)) - (g(y - \varepsilon z) - g(y))}{|x - y|^{\alpha}}dz|$$ and I have no idea how can I proceed. I believe this is a standard and elementary approximation problem, but I have yet to find any text that give light to this problem and spent quite some time on it. So, could anyone give me some kind of directions or answers? It would then be a great help to my studies and thanks in advance.",,"['real-analysis', 'partial-differential-equations', 'approximation', 'holder-spaces']"
7,Show that $L^1$ convergence implies uniform integrability.,Show that  convergence implies uniform integrability.,L^1,"Let $(X,\mathscr{M}, \mu)$ be a measure space and suppose $(f_n)$ is a sequence of $L^1$ functions ($L^1$ being the space of equivalence classes of absolutely integrable functions) converging to $f$ in $L^1$ ( that is $\displaystyle\lim\limits_{n \rightarrow \infty} \|f-f_{n} \|_{1}=\displaystyle\lim\limits_{n \rightarrow \infty} \int_{X} \left |f-f_{n} \right | d\mu=0$). Prove that the sequence $(f_n)$ is uniformly integrable, that is the sequence satisfies the following: i) $\displaystyle\sup\limits_{n} \|f_{n} \|_{1} < +\infty$ ii)  $\displaystyle\sup\limits_{n} \int_{|f_{n}| >M} \left | f_n \right | d \mu \rightarrow 0$ as $M \rightarrow \infty$ iii) $\displaystyle\sup\limits_{n} \int_{|f_{n}| < \delta} \left | f \right | d \mu \rightarrow 0$ as $\delta \rightarrow 0$. I'm working on this problem and am having some difficulty. Part (i) is straight forward from $\left |f_n \right |< \left |f_n -f \right | +\left |f \right |$. Since the sequence is in $L^1$ it follows that $f$ is, so we have that $\int \left | f \right | =A <+\infty$. Let $\epsilon >0$, then we can find an $N$ so that for $n \geq N$ we have $\|f-f_{n} \|_{1} < \epsilon$. If $B = \max\{\|f-f_{1} \|_{1},\|f-f_{2} \|_{1},\ldots, \|f-f_{N-1} \|_{1},\epsilon\}$, then $\| f_n \| < A + B < +\infty$ for all $n$. Part (ii) I'm thinking that we should somehow use the fact that $\{{f_n} >M\} \subseteq \{ \left | f_n-f \right | > \frac{M}{2}\} \cup  \{\left | f \right | > \frac{M}{2}\}$ to break up the integral into pieces which we can control. Part(iii) I'm thinking that we can again use $\left |f \right |< \left |f -f_n \right | +\left |f_n \right |$. For a fixed $n$ we can show that $\int_{|f_n| < \delta} |f_n| \rightarrow 0$ as $\delta \rightarrow 0$ so if we can somehow bound $\int_{|f_n| < \delta} |f-f_n|$ we will be done. Any help/advice/direction would be greatly appreciated. Thanks!","Let $(X,\mathscr{M}, \mu)$ be a measure space and suppose $(f_n)$ is a sequence of $L^1$ functions ($L^1$ being the space of equivalence classes of absolutely integrable functions) converging to $f$ in $L^1$ ( that is $\displaystyle\lim\limits_{n \rightarrow \infty} \|f-f_{n} \|_{1}=\displaystyle\lim\limits_{n \rightarrow \infty} \int_{X} \left |f-f_{n} \right | d\mu=0$). Prove that the sequence $(f_n)$ is uniformly integrable, that is the sequence satisfies the following: i) $\displaystyle\sup\limits_{n} \|f_{n} \|_{1} < +\infty$ ii)  $\displaystyle\sup\limits_{n} \int_{|f_{n}| >M} \left | f_n \right | d \mu \rightarrow 0$ as $M \rightarrow \infty$ iii) $\displaystyle\sup\limits_{n} \int_{|f_{n}| < \delta} \left | f \right | d \mu \rightarrow 0$ as $\delta \rightarrow 0$. I'm working on this problem and am having some difficulty. Part (i) is straight forward from $\left |f_n \right |< \left |f_n -f \right | +\left |f \right |$. Since the sequence is in $L^1$ it follows that $f$ is, so we have that $\int \left | f \right | =A <+\infty$. Let $\epsilon >0$, then we can find an $N$ so that for $n \geq N$ we have $\|f-f_{n} \|_{1} < \epsilon$. If $B = \max\{\|f-f_{1} \|_{1},\|f-f_{2} \|_{1},\ldots, \|f-f_{N-1} \|_{1},\epsilon\}$, then $\| f_n \| < A + B < +\infty$ for all $n$. Part (ii) I'm thinking that we should somehow use the fact that $\{{f_n} >M\} \subseteq \{ \left | f_n-f \right | > \frac{M}{2}\} \cup  \{\left | f \right | > \frac{M}{2}\}$ to break up the integral into pieces which we can control. Part(iii) I'm thinking that we can again use $\left |f \right |< \left |f -f_n \right | +\left |f_n \right |$. For a fixed $n$ we can show that $\int_{|f_n| < \delta} |f_n| \rightarrow 0$ as $\delta \rightarrow 0$ so if we can somehow bound $\int_{|f_n| < \delta} |f-f_n|$ we will be done. Any help/advice/direction would be greatly appreciated. Thanks!",,"['real-analysis', 'measure-theory']"
8,Cauchy condition for functions,Cauchy condition for functions,,"Prove that $f$ has a limit at $a$ if and only if for every $\epsilon > 0$, there exists $\delta>0$ such that if $0<|x-a|<\delta$ and $0<|y-a|<\delta$, then $|f(x)-f(y)|<\epsilon$. Forward direction: Suppose $f$ has  a limit $L$ at $a$. Fix $\epsilon$. Then for some $\delta$ we have $|f(x)-L|<\epsilon/2$ whenever $|x-a|<\delta$. Then for $|x-a|,|y-a|<\delta$, we have $|f(x)-f(y)|\le|f(x)-L|+|f(y)-L|<\epsilon/2+\epsilon/2=\epsilon$. Backward direction: Suppose there exists $\delta>0$ such that if $0<|x-a|<\delta$ and $0<|y-a|<\delta$, then $|f(x)-f(y)|<\epsilon$. We want to show $f$ has limit at $a$, which means that for some $L$, any sequence of $x_i$'s converging to $a$ has $f(x_i)$'s converging to $L$. How to proceed?","Prove that $f$ has a limit at $a$ if and only if for every $\epsilon > 0$, there exists $\delta>0$ such that if $0<|x-a|<\delta$ and $0<|y-a|<\delta$, then $|f(x)-f(y)|<\epsilon$. Forward direction: Suppose $f$ has  a limit $L$ at $a$. Fix $\epsilon$. Then for some $\delta$ we have $|f(x)-L|<\epsilon/2$ whenever $|x-a|<\delta$. Then for $|x-a|,|y-a|<\delta$, we have $|f(x)-f(y)|\le|f(x)-L|+|f(y)-L|<\epsilon/2+\epsilon/2=\epsilon$. Backward direction: Suppose there exists $\delta>0$ such that if $0<|x-a|<\delta$ and $0<|y-a|<\delta$, then $|f(x)-f(y)|<\epsilon$. We want to show $f$ has limit at $a$, which means that for some $L$, any sequence of $x_i$'s converging to $a$ has $f(x_i)$'s converging to $L$. How to proceed?",,"['real-analysis', 'convergence-divergence']"
9,Can all points in the plane be represented like this?,Can all points in the plane be represented like this?,,"Solving a task regaring affine geometry, I've come across a problem: Is it  true that, for every point $(x,y)\in \mathbb{R}^2$, there  exist $t\in \mathbb{R}, \alpha\in[0,2\pi]$, such that $$x = t\cos\alpha + \sin \alpha \\ y = t\sin\alpha + \cos \alpha?$$ I don't have any idea how to prove this or find a counterexample. All hints appreciated!","Solving a task regaring affine geometry, I've come across a problem: Is it  true that, for every point $(x,y)\in \mathbb{R}^2$, there  exist $t\in \mathbb{R}, \alpha\in[0,2\pi]$, such that $$x = t\cos\alpha + \sin \alpha \\ y = t\sin\alpha + \cos \alpha?$$ I don't have any idea how to prove this or find a counterexample. All hints appreciated!",,"['real-analysis', 'geometry', 'ordinary-differential-equations', 'trigonometry']"
10,Proof that a linear transformation is continuous,Proof that a linear transformation is continuous,,"I got started recently on proofs about continuity and so on. So to start working with this on $n$-spaces I've selected to prove that every linear function $f: \mathbb{R}^n \to \mathbb{R}^m$ is continuous at every $a \in \mathbb{R}^n$. Since I'm just getting started with this kind of proof I just want to know if my proof is okay or if there's any inconsistency. My proof is as follows: Since $f$ is linear, we know that there's some $k\in \mathbb{R}$ such that $|f(x)|\leq k|x|$ for every $x\in \mathbb{R}^n$, in that case let $a\in \mathbb{R}^n$ and let $\varepsilon >0$. Consider $\delta = \varepsilon /k$ and suppose $|x-a|<\delta$, in that case we have: $$|f(x)-f(a)|=|f(x-a)|\leq k |x-a|<k \frac{\varepsilon}{k}=\varepsilon$$ And since $|x-a|<\delta$ implies $|f(x)-f(a)|<\varepsilon$ we have that $f$ is continuous at $a \in \mathbb{R}^n$. Since $a$ was arbitrary, $f$ is continous in $\mathbb{R}^n$. Is this proof fine? Or there was something I've missed on the way?","I got started recently on proofs about continuity and so on. So to start working with this on $n$-spaces I've selected to prove that every linear function $f: \mathbb{R}^n \to \mathbb{R}^m$ is continuous at every $a \in \mathbb{R}^n$. Since I'm just getting started with this kind of proof I just want to know if my proof is okay or if there's any inconsistency. My proof is as follows: Since $f$ is linear, we know that there's some $k\in \mathbb{R}$ such that $|f(x)|\leq k|x|$ for every $x\in \mathbb{R}^n$, in that case let $a\in \mathbb{R}^n$ and let $\varepsilon >0$. Consider $\delta = \varepsilon /k$ and suppose $|x-a|<\delta$, in that case we have: $$|f(x)-f(a)|=|f(x-a)|\leq k |x-a|<k \frac{\varepsilon}{k}=\varepsilon$$ And since $|x-a|<\delta$ implies $|f(x)-f(a)|<\varepsilon$ we have that $f$ is continuous at $a \in \mathbb{R}^n$. Since $a$ was arbitrary, $f$ is continous in $\mathbb{R}^n$. Is this proof fine? Or there was something I've missed on the way?",,"['real-analysis', 'proof-verification', 'continuity']"
11,Sum of $\sum_{n=1}^\infty \frac{(-1)^n}{n^2+(n^2-1)^2}$,Sum of,\sum_{n=1}^\infty \frac{(-1)^n}{n^2+(n^2-1)^2},I came across these two series $$\sum_{n=1}^\infty \frac{(-1)^n}{n^2+(n^2-1)^2}$$ and $$\sum_{n=1}^\infty \frac{(-1)^n(n^2-1)}{n^2+(n^2-1)^2}.$$ Does anyone recognize the sums of these series?,I came across these two series $$\sum_{n=1}^\infty \frac{(-1)^n}{n^2+(n^2-1)^2}$$ and $$\sum_{n=1}^\infty \frac{(-1)^n(n^2-1)}{n^2+(n^2-1)^2}.$$ Does anyone recognize the sums of these series?,,"['real-analysis', 'sequences-and-series']"
12,"If $A$ is compact, is then $f(A)$ compact?","If  is compact, is then  compact?",A f(A),"I just got my exam back, and I still cannot understand this question: Given a continuous function $f:A\subseteq\mathbb{R}\to\mathbb{R}$, show that if $A$ is a compact set, then its image, $f(A)$, is also compact. I know that a set $A\subseteq\mathbb{R}$ is compact if every sequence in $A$ has a subsequence that converges to a limit that is also in $A$, and I know that a function $f$ is continuous on $A$ if for every $(x_n)\subseteq A$ such that $x_n\to c\in A$, it follows that $f(x_n)\to f(c)$. Therefore, all that I need to do is show that for every $(y_n)\subseteq f(A)$, there is a subsequence $(y_{n_k})$ such that $y_{n_k}\to y\in f(A)$. Can I then make the assumption that for any sequence $(y_n)\subseteq f(A)$, there is a sequence $(x_n)\subseteq A$ such that $y_n=f(x_n)$? If so, I could then continue by stating that since $A$ is compact, there is a subsequence $(x_{n_k})$ such that $x_{n_k}\to x\in A$, and since $f$ is continuous, $f(x_{n_k})\to f(x)$. I believe that this yields the required subsequence $(y_{n_k})$ of $(y_n)$ such that $y_{n_k}=f(x_{n_k})\to f(x)=y\in f(A)$. What do you guys think? Is this a sound approach? Thanks in advance.","I just got my exam back, and I still cannot understand this question: Given a continuous function $f:A\subseteq\mathbb{R}\to\mathbb{R}$, show that if $A$ is a compact set, then its image, $f(A)$, is also compact. I know that a set $A\subseteq\mathbb{R}$ is compact if every sequence in $A$ has a subsequence that converges to a limit that is also in $A$, and I know that a function $f$ is continuous on $A$ if for every $(x_n)\subseteq A$ such that $x_n\to c\in A$, it follows that $f(x_n)\to f(c)$. Therefore, all that I need to do is show that for every $(y_n)\subseteq f(A)$, there is a subsequence $(y_{n_k})$ such that $y_{n_k}\to y\in f(A)$. Can I then make the assumption that for any sequence $(y_n)\subseteq f(A)$, there is a sequence $(x_n)\subseteq A$ such that $y_n=f(x_n)$? If so, I could then continue by stating that since $A$ is compact, there is a subsequence $(x_{n_k})$ such that $x_{n_k}\to x\in A$, and since $f$ is continuous, $f(x_{n_k})\to f(x)$. I believe that this yields the required subsequence $(y_{n_k})$ of $(y_n)$ such that $y_{n_k}=f(x_{n_k})\to f(x)=y\in f(A)$. What do you guys think? Is this a sound approach? Thanks in advance.",,"['real-analysis', 'compactness']"
13,A locally injective but non globally injective function?,A locally injective but non globally injective function?,,"A continuous function $f : U \subset \mathbb{R}^n \to \mathbb{R}^n$, is said to be locally injective at $x_0 \in U$ if exist a neighborhood $V \subset U$ of $x_0$ s.t. $f|_V$ is injective. $f$ is said to be locally injective on $U$ if is locally injective at all points of $U$. If $n=1$, clearly $f$ is locally injective on $U$ iff it's injective on $U$. If $n >1$, this is false (at least is what I believe). Does anybody know a counterexample i.e. a continuous function which is locally injective on an open $U$ set but it's not injective on $U$? [ observation ] if $f$ is locally injective, by the invariance domain theorem is a local homeomorphism. if $f$ is also proper (i.e. the counter-image of a compact set is compact), then it's also a global homeomorphism (Caccioppoli theorem), and consequently injective. This means the counterexample cannot be a proper map. [ edit ] I forgot to specify that I was interested in continuous functions.","A continuous function $f : U \subset \mathbb{R}^n \to \mathbb{R}^n$, is said to be locally injective at $x_0 \in U$ if exist a neighborhood $V \subset U$ of $x_0$ s.t. $f|_V$ is injective. $f$ is said to be locally injective on $U$ if is locally injective at all points of $U$. If $n=1$, clearly $f$ is locally injective on $U$ iff it's injective on $U$. If $n >1$, this is false (at least is what I believe). Does anybody know a counterexample i.e. a continuous function which is locally injective on an open $U$ set but it's not injective on $U$? [ observation ] if $f$ is locally injective, by the invariance domain theorem is a local homeomorphism. if $f$ is also proper (i.e. the counter-image of a compact set is compact), then it's also a global homeomorphism (Caccioppoli theorem), and consequently injective. This means the counterexample cannot be a proper map. [ edit ] I forgot to specify that I was interested in continuous functions.",,"['real-analysis', 'general-topology', 'examples-counterexamples']"
14,"Writing $f(x,y)$ as $\Phi(g(x) + h(y))$",Writing  as,"f(x,y) \Phi(g(x) + h(y))","Could you prove or disprove the following statement? Let    $f\colon[0,1]^2\rightarrow \mathbb R$    be a continuous function.  Then   there are continuous functions    $g,\ h\colon [0,1]\rightarrow \mathbb R$ and   $\Phi\colon \mathbb R \to \mathbb R$   such that  $$ f(x,y) = \Phi(g(x) + h(y)).$$ (This problem popped up in my mind while I was thinking about this related one on MO.  I couldn't find an easy proof or a disproof. This version is much weaker than the one asked at MO, since $g$ and $h$ do depend on $f$ here.)","Could you prove or disprove the following statement? Let    $f\colon[0,1]^2\rightarrow \mathbb R$    be a continuous function.  Then   there are continuous functions    $g,\ h\colon [0,1]\rightarrow \mathbb R$ and   $\Phi\colon \mathbb R \to \mathbb R$   such that  $$ f(x,y) = \Phi(g(x) + h(y)).$$ (This problem popped up in my mind while I was thinking about this related one on MO.  I couldn't find an easy proof or a disproof. This version is much weaker than the one asked at MO, since $g$ and $h$ do depend on $f$ here.)",,['real-analysis']
15,"Proving that $\lim_{s \to 0^-} \int_{0}^{\infty} \frac{\arcsin(\sin x)}{x} x^{s} \, dx = \int_{0}^{\infty} \frac{\arcsin(\sin x)}{x} \, dx $",Proving that,"\lim_{s \to 0^-} \int_{0}^{\infty} \frac{\arcsin(\sin x)}{x} x^{s} \, dx = \int_{0}^{\infty} \frac{\arcsin(\sin x)}{x} \, dx ","In an attempt to show that $$\int_0^\infty \frac{\arcsin(\sin x)}{x} \, \mathrm dx =2G,$$ where $G$ is Catalan's constant,  I first evaluated the Mellin transform $$I(s)=\int_0^\infty \frac{\arcsin(\sin x)}{x}  x^s\, \mathrm dx, \quad -1 < s< 0,$$ by using the fact that $\arcsin(\sin x)$ can be represented by the Fourier series $$\arcsin(\sin x) = \frac{4}{\pi} \sum_{n=0}^\infty \frac{(-1)^n \sin \left[(2n+1)x \right]}{(2n+1)^2}. $$ Replacing $\arcsin(\sin x)$ with this representation and then switching the order of summation and integration, which is justified by Fubini's theorem, I got $$ I(s) = \frac{4}{\pi} \beta(2+s) \Gamma(s) \sin \left(\frac{\pi s}{2} \right),$$ where $\beta(s)$ is the Dirichlet beta function . Now I want to argue that $$\lim_{s \to 0^-} I(s) = \int_{0}^{\infty} \frac{\arcsin(\sin x)}{x} \, \mathrm dx  $$ so that I can conclude that $$\begin{align} \int_0^\infty \frac{\arcsin(\sin x)}{x} \, \mathrm dx &= \frac{4}{\pi} \lim_{s \to 0^-} \beta(2+s) \Gamma(s) \sin \left(\frac{\pi s}{2} \right) \\ &= \frac{4}{\pi} \beta(2) \lim_{s \to 0^-} \left( \frac{1}{s} + \mathcal{O}(1) \right) \sin \left(\frac{\pi s}{2} \right) \\ &= 2 G. \end{align} $$ The issue is that $\int_{0}^{\infty} \frac{\arcsin(\sin x)}{x} \, \mathrm dx$ doesn't converge absolutely, so we can't appeal immediately to the dominated convergence theorem. In the case of the Laplace transform,  it turns out that $$\lim_{s \to 0^+} \int_0^\infty \frac{f(x)}{x} e^{-sx} \, \mathrm dx = \int_0^\infty \frac{f(x)}{x} \, \mathrm dx$$ if $\int_0^\infty \frac{f(x)}{x} \, \mathrm dx$ exists as an improper Riemann integral.  This has to do with the fact that the improper integrals $\int_0^\infty \frac{f(x)}{x} e^{-sx} \, \mathrm dx$ converge uniformly on $[0, \infty)$ . (See here and here .) I don't know if the Mellin transform has a similar property, but I imagine it does since the two transforms are related.","In an attempt to show that where is Catalan's constant,  I first evaluated the Mellin transform by using the fact that can be represented by the Fourier series Replacing with this representation and then switching the order of summation and integration, which is justified by Fubini's theorem, I got where is the Dirichlet beta function . Now I want to argue that so that I can conclude that The issue is that doesn't converge absolutely, so we can't appeal immediately to the dominated convergence theorem. In the case of the Laplace transform,  it turns out that if exists as an improper Riemann integral.  This has to do with the fact that the improper integrals converge uniformly on . (See here and here .) I don't know if the Mellin transform has a similar property, but I imagine it does since the two transforms are related.","\int_0^\infty \frac{\arcsin(\sin x)}{x} \, \mathrm dx =2G, G I(s)=\int_0^\infty \frac{\arcsin(\sin x)}{x}  x^s\, \mathrm dx, \quad -1 < s< 0, \arcsin(\sin x) \arcsin(\sin x) = \frac{4}{\pi} \sum_{n=0}^\infty \frac{(-1)^n \sin \left[(2n+1)x \right]}{(2n+1)^2}.  \arcsin(\sin x)  I(s) = \frac{4}{\pi} \beta(2+s) \Gamma(s) \sin \left(\frac{\pi s}{2} \right), \beta(s) \lim_{s \to 0^-} I(s) = \int_{0}^{\infty} \frac{\arcsin(\sin x)}{x} \, \mathrm dx   \begin{align} \int_0^\infty \frac{\arcsin(\sin x)}{x} \, \mathrm dx &= \frac{4}{\pi} \lim_{s \to 0^-} \beta(2+s) \Gamma(s) \sin \left(\frac{\pi s}{2} \right) \\ &= \frac{4}{\pi} \beta(2) \lim_{s \to 0^-} \left( \frac{1}{s} + \mathcal{O}(1) \right) \sin \left(\frac{\pi s}{2} \right) \\ &= 2 G. \end{align}  \int_{0}^{\infty} \frac{\arcsin(\sin x)}{x} \, \mathrm dx \lim_{s \to 0^+} \int_0^\infty \frac{f(x)}{x} e^{-sx} \, \mathrm dx = \int_0^\infty \frac{f(x)}{x} \, \mathrm dx \int_0^\infty \frac{f(x)}{x} \, \mathrm dx \int_0^\infty \frac{f(x)}{x} e^{-sx} \, \mathrm dx [0, \infty)","['real-analysis', 'integration', 'limits', 'fourier-series', 'mellin-transform']"
16,"Does $f$ have a critical point if $f(x, y) \to +\infty$ on all horizontal lines and $f(x, y) \to -\infty$ on all vertical lines?",Does  have a critical point if  on all horizontal lines and  on all vertical lines?,"f f(x, y) \to +\infty f(x, y) \to -\infty","Given a function $f:\Bbb R^2 \to \Bbb R, f \in C^\infty$ with the property that $$\lim_{x\to+\infty}f(x,y_0) = \lim_{x\to-\infty}f(x,y_0) = +\infty \qquad \forall y_0\in \Bbb R, \\[2ex] \lim_{y\to+\infty}f(x_0,y) = \lim_{y\to-\infty}f(x_0,y) = -\infty \qquad \forall x_0\in \Bbb R.$$ Determine whether $f(x,y)$ necessarily has at least one critic point. My attempt: I suppose that such a function could be something just like: $(x^{2n}-y^{2m})$ ; anyway what I mean is that both $f(x,y_0)$ and $f(x_0,y)$ have to assume eventually the shape of a sort of ""parabola"". Because of $f\in C^{\infty}$ then both $f(x,y_0)$ and $f(x_0,y)$ are continuous, thus: 1. $f(x,y_0)=g(x),$ has a global minimum and it means: $\forall y_0 \in \Bbb R$ there is at least one $x^*$ such that $f_x(x^*,y_0)=0;$ 2. $f(x_0,y)=h(y),$ has a global maximum and it means: $\forall x_0 \in \Bbb R$ there is at least one $y^*$ such that $f_y(x_0,y^*)=0.$ If my attempt is correct until now, the last thing I need to do is observe that there is at least a couple $(x^*,y^*)$ such that $f_x(x^*,y^*)=0$ and $f_y(x^*,y^*)=0$ . This last step is the one I stuck in. Is there someone who can handle this? (or can propose another path to follow)","Given a function with the property that Determine whether necessarily has at least one critic point. My attempt: I suppose that such a function could be something just like: ; anyway what I mean is that both and have to assume eventually the shape of a sort of ""parabola"". Because of then both and are continuous, thus: 1. has a global minimum and it means: there is at least one such that 2. has a global maximum and it means: there is at least one such that If my attempt is correct until now, the last thing I need to do is observe that there is at least a couple such that and . This last step is the one I stuck in. Is there someone who can handle this? (or can propose another path to follow)","f:\Bbb R^2 \to \Bbb R, f \in C^\infty \lim_{x\to+\infty}f(x,y_0) = \lim_{x\to-\infty}f(x,y_0) = +\infty \qquad \forall y_0\in \Bbb R, \\[2ex]
\lim_{y\to+\infty}f(x_0,y) = \lim_{y\to-\infty}f(x_0,y) = -\infty \qquad \forall x_0\in \Bbb R. f(x,y) (x^{2n}-y^{2m}) f(x,y_0) f(x_0,y) f\in C^{\infty} f(x,y_0) f(x_0,y) f(x,y_0)=g(x), \forall y_0 \in \Bbb R x^* f_x(x^*,y_0)=0; f(x_0,y)=h(y), \forall x_0 \in \Bbb R y^* f_y(x_0,y^*)=0. (x^*,y^*) f_x(x^*,y^*)=0 f_y(x^*,y^*)=0","['real-analysis', 'calculus', 'multivariable-calculus', 'examples-counterexamples', 'maxima-minima']"
17,Prove that $f'(x) \to 0$ as $x \to \infty$ when $f'+f''$ is bounded abve.,Prove that  as  when  is bounded abve.,f'(x) \to 0 x \to \infty f'+f'',"I am given $f \in C^2([0,\infty))$ and $\lim_{x \to \infty}f(x) = L  \in \mathbb{R}$ . I am also given there is a real number $M$ such that $f'(x) + f''(x) < M$ for all $x \in [0,\infty)$ . Prove that $\lim_{x  \to \infty}f'(x) = 0$ . My thoughts This is similar to a frequently asked question: If $\lim_{x \to \infty}f(x) = L$ then if $\lim_{x \to \infty} f'(x)= L'$ exists it is necessary that $L' = 0$ . Possible approaches are: (1) Originally I thought to show upper bound on $f' + f''$ along with $f(x) \to L$ guarantees that the limit of $f'$ exists. but I just realized the function $f(x) = \sin(x^2)/x$ is a counterexample. (2) Somehow use the Taylor expansion $f(y) = f(x) + f'(x)(y-x) + \frac{1}{2} f''(\xi)(x-y)^2$ ( $\xi \in (x,y)$ ). Here I am having difficulty tying $f'$ and $f''$ together to use the bound.",I am given and . I am also given there is a real number such that for all . Prove that . My thoughts This is similar to a frequently asked question: If then if exists it is necessary that . Possible approaches are: (1) Originally I thought to show upper bound on along with guarantees that the limit of exists. but I just realized the function is a counterexample. (2) Somehow use the Taylor expansion ( ). Here I am having difficulty tying and together to use the bound.,"f \in C^2([0,\infty)) \lim_{x \to \infty}f(x) = L
 \in \mathbb{R} M f'(x) + f''(x) < M x \in [0,\infty) \lim_{x
 \to \infty}f'(x) = 0 \lim_{x \to \infty}f(x) = L \lim_{x \to \infty} f'(x)= L' L' = 0 f' + f'' f(x) \to L f' f(x) = \sin(x^2)/x f(y) = f(x) + f'(x)(y-x) + \frac{1}{2} f''(\xi)(x-y)^2 \xi \in (x,y) f' f''",['real-analysis']
18,Integrals of the Bessel function $J_0(x)$ over the intervals between its zeros,Integrals of the Bessel function  over the intervals between its zeros,J_0(x),"Let $J_0(x)$ be the Bessel function of the first kind . It has an infinite number of zeros on the positive real semi-axis. Let's denote them as $j_{0,n}$ : $$j_{0,1}=2.40482...,\quad j_{0,2}=5.52007...,\quad j_{0,3}=8.65372...,\quad\small...\tag1$$ We are interested in absolute values of the integrals of $J_0(x)$ over the intervals between its consecutive zeros: $$\sigma_n=(-1)^n\int_{j_{0,n}}^{j_{0,n+1}}\!\!J_0(x)\,dx.\tag2$$ Their values are: $$\sigma_1=0.80145...,\quad\sigma_2=0.59932...,\quad\sigma_3=0.49904...,\quad\small...\tag3$$ We are interested in the asymptotic behavior of this sequence. Empirically, it seems that $$\sigma_n\,\stackrel{\color{#a0a0a0}?}\sim\,\frac{2\sqrt2}{\pi\sqrt n}\left(1-\frac1{8n}+O\!\left(\frac1{n^2}\right)\right)\!.\tag4$$ Can we prove this? Can we find next coefficients in this expansion?","Let be the Bessel function of the first kind . It has an infinite number of zeros on the positive real semi-axis. Let's denote them as : We are interested in absolute values of the integrals of over the intervals between its consecutive zeros: Their values are: We are interested in the asymptotic behavior of this sequence. Empirically, it seems that Can we prove this? Can we find next coefficients in this expansion?","J_0(x) j_{0,n} j_{0,1}=2.40482...,\quad j_{0,2}=5.52007...,\quad j_{0,3}=8.65372...,\quad\small...\tag1 J_0(x) \sigma_n=(-1)^n\int_{j_{0,n}}^{j_{0,n+1}}\!\!J_0(x)\,dx.\tag2 \sigma_1=0.80145...,\quad\sigma_2=0.59932...,\quad\sigma_3=0.49904...,\quad\small...\tag3 \sigma_n\,\stackrel{\color{#a0a0a0}?}\sim\,\frac{2\sqrt2}{\pi\sqrt n}\left(1-\frac1{8n}+O\!\left(\frac1{n^2}\right)\right)\!.\tag4","['real-analysis', 'integration', 'asymptotics', 'bessel-functions', 'conjectures']"
19,Step Function vs Simple Function,Step Function vs Simple Function,,"Unfortunately, my book (Royden-Fitzpatrick's Real Analysis) offers no definition of a step function but merely compares its role in Riemann integration to the role played by linear combinations of characteristic functions in Lebesgue integration. From what I gather, these are simple function in which he defines as follows: A real-valued function $\varphi$ is defined on a measurable set $E$ is simple provided it is measurable and takes only a finite number of values. This means that $\varphi = \sum_{i=1}^n c_k \chi_{E_k}$ where $E_k = \{x \in E \mid \varphi(x) = c_k\}$ However, here and here the step function is defined in the very same way, the only difference being that the $E_k$ are taken to be intervals.  But I have read elsewhere that the set of step functions and simple functions do not coincide. What am I misunderstanding? The reason I ask is because the following problem I am working could be rendered trivial depending on how one interprets these terms (at least it seems it could be): Let $I$ be a closed, bounded interval and $E$ a measurable subset of $I$. Let $\epsilon > 0$. Show that there is a step function $h$ on $I$ and a measurable subset $F$ of $I$ for which $h = \chi_E$ on $F$ and $m(I-F) < \epsilon$. I suppose my question comes down to: how am I to understand how the author is using the term ""step function"" so that the problem makes sense but isn't triviaL?","Unfortunately, my book (Royden-Fitzpatrick's Real Analysis) offers no definition of a step function but merely compares its role in Riemann integration to the role played by linear combinations of characteristic functions in Lebesgue integration. From what I gather, these are simple function in which he defines as follows: A real-valued function $\varphi$ is defined on a measurable set $E$ is simple provided it is measurable and takes only a finite number of values. This means that $\varphi = \sum_{i=1}^n c_k \chi_{E_k}$ where $E_k = \{x \in E \mid \varphi(x) = c_k\}$ However, here and here the step function is defined in the very same way, the only difference being that the $E_k$ are taken to be intervals.  But I have read elsewhere that the set of step functions and simple functions do not coincide. What am I misunderstanding? The reason I ask is because the following problem I am working could be rendered trivial depending on how one interprets these terms (at least it seems it could be): Let $I$ be a closed, bounded interval and $E$ a measurable subset of $I$. Let $\epsilon > 0$. Show that there is a step function $h$ on $I$ and a measurable subset $F$ of $I$ for which $h = \chi_E$ on $F$ and $m(I-F) < \epsilon$. I suppose my question comes down to: how am I to understand how the author is using the term ""step function"" so that the problem makes sense but isn't triviaL?",,"['real-analysis', 'measure-theory']"
20,$fg\in L^1$ for all $g\in L^q$ $\Longrightarrow f\in L^p$,for all,fg\in L^1 g\in L^q \Longrightarrow f\in L^p,"Let $(X,\mathscr{M},\mu)$ be a $\sigma$-finite measure space and $f$ be a $\mathscr{M}$-measurable function. Let $p,q$ be Hölder conjugates of each other where $1\leq p\leq\infty$. Then is it true that $fg\in L^1$ for all $g\in L^q$ $\Longrightarrow f\in L^p$ ? I've managed to prove this for $1\leq p<\infty$. I argued as follows: First, it is easy to check that $f$ must be finite almost everywhere. Now define $\nu(A)=\int_A |f|^p \,d\mu$ for each $A\in\mathscr{M}$. Then it is also easy to check that $(X,\mathscr{M},\nu)$ is a $\sigma$-finite measure space, and the assertion is that this is actually a finite measure space. Assume $\nu(X)=\infty$ for contradiction. Then there exist a measurable function $h$ such that $$h\in L^p(X,\mathscr{M},\nu)\text{ for all }p>1,\text{ but }h\notin L^1(X,\mathscr{M},\nu).$$ Then the function $g=h|f|^{p-1}$ is in $L^q(X,\mathscr{M},\mu)$ but we have $fg\notin L^1(X,\mathscr{M},\nu)$, a contradiction. The case $p=\infty$ must be handled separately, but I don't see how I should proceed... This is an exercise from Jones' Lebesgue Integration on Euclidean Space , and the book does not contain the theory of Banach spaces, dual spaces, Riesz representation theorem, etc. The proof I'm looking for is therefore the one avoiding such advanced theories. Can someone show me how to handle the case $p=\infty$ in a (relatively) elementary manner? Any advice is welcome. Please enlighten me.","Let $(X,\mathscr{M},\mu)$ be a $\sigma$-finite measure space and $f$ be a $\mathscr{M}$-measurable function. Let $p,q$ be Hölder conjugates of each other where $1\leq p\leq\infty$. Then is it true that $fg\in L^1$ for all $g\in L^q$ $\Longrightarrow f\in L^p$ ? I've managed to prove this for $1\leq p<\infty$. I argued as follows: First, it is easy to check that $f$ must be finite almost everywhere. Now define $\nu(A)=\int_A |f|^p \,d\mu$ for each $A\in\mathscr{M}$. Then it is also easy to check that $(X,\mathscr{M},\nu)$ is a $\sigma$-finite measure space, and the assertion is that this is actually a finite measure space. Assume $\nu(X)=\infty$ for contradiction. Then there exist a measurable function $h$ such that $$h\in L^p(X,\mathscr{M},\nu)\text{ for all }p>1,\text{ but }h\notin L^1(X,\mathscr{M},\nu).$$ Then the function $g=h|f|^{p-1}$ is in $L^q(X,\mathscr{M},\mu)$ but we have $fg\notin L^1(X,\mathscr{M},\nu)$, a contradiction. The case $p=\infty$ must be handled separately, but I don't see how I should proceed... This is an exercise from Jones' Lebesgue Integration on Euclidean Space , and the book does not contain the theory of Banach spaces, dual spaces, Riesz representation theorem, etc. The proof I'm looking for is therefore the one avoiding such advanced theories. Can someone show me how to handle the case $p=\infty$ in a (relatively) elementary manner? Any advice is welcome. Please enlighten me.",,"['real-analysis', 'functional-analysis', 'measure-theory', 'lebesgue-integral', 'lp-spaces']"
21,The set of all limits of sub-series of an absolute convergent series,The set of all limits of sub-series of an absolute convergent series,,"Let $\sum_{i=1}^\infty a_i$ be an absolute convergent series and let $\sum_{i=1}^\infty b_i$ be a sub-series of it, i.e. $$b_j=c_j\cdot a_j\quad (c_j\in\{0,1\}),\qquad\forall j\in\mathbb N$$ We can say that $\sum_{i=1}^\infty b_i$ is absolutely convergent as well. So its limit, say $L$, is a real number. Now the question is, what can be said about the set of all possible values of $L$? Is it connected? Closed maybe? Neither closed / connected / compact? I have no idea. Can we determine whether a specific number belongs to this set or not? I was specially interested in finding a sub-series of $\sum \frac 1{n^2}$ whose limit is, say $\frac{\pi}6$, and then I ended up with this question.","Let $\sum_{i=1}^\infty a_i$ be an absolute convergent series and let $\sum_{i=1}^\infty b_i$ be a sub-series of it, i.e. $$b_j=c_j\cdot a_j\quad (c_j\in\{0,1\}),\qquad\forall j\in\mathbb N$$ We can say that $\sum_{i=1}^\infty b_i$ is absolutely convergent as well. So its limit, say $L$, is a real number. Now the question is, what can be said about the set of all possible values of $L$? Is it connected? Closed maybe? Neither closed / connected / compact? I have no idea. Can we determine whether a specific number belongs to this set or not? I was specially interested in finding a sub-series of $\sum \frac 1{n^2}$ whose limit is, say $\frac{\pi}6$, and then I ended up with this question.",,"['real-analysis', 'sequences-and-series', 'limits']"
22,Conditionally convergent power sums,Conditionally convergent power sums,,"I'm struggling on the following question: Let $S$ be a (possibly infinite) set of odd positive integers. Prove that   there exists a real sequence $(x_n)$ such that, for each positive integer $k$, the   series $\sum x_n^k$ converges iff $k \in S$. I'm completely lost on this one. How can we even form a sequence such that the series converges for $k = 3, 7$ but not $5$? The series are all conditionally convergent, perhaps some clever rearrangement of the alternating harmonic series could do it.","I'm struggling on the following question: Let $S$ be a (possibly infinite) set of odd positive integers. Prove that   there exists a real sequence $(x_n)$ such that, for each positive integer $k$, the   series $\sum x_n^k$ converges iff $k \in S$. I'm completely lost on this one. How can we even form a sequence such that the series converges for $k = 3, 7$ but not $5$? The series are all conditionally convergent, perhaps some clever rearrangement of the alternating harmonic series could do it.",,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence', 'conditional-convergence']"
23,Rudin assumes $(x^a)^b=x^{ab}$(for real $a$ and $b$) without proof?,Rudin assumes (for real  and ) without proof?,(x^a)^b=x^{ab} a b,"I am currently self studying Baby Rudin and I'm having some problems with his proof of Theorem 3.20(a) on page 58. I have read all previous chapters and I can't find any mention of real exponents besides its definition in the exercise section of Chapter 1 (but nothing about order/this identity). He defines $x^a$ for rational $a$ and $x>1$ then this is used to extend it to all real $a$ by $x^a = \sup \{x^t : t \leq a,~ t \in Q\}$. Am I missing something or does he expect the reader to fill this huge gap in the proof? Prefatory note on parts for Theorem 3.20: We shall now compute the limits of some sequences which occur frequently. The proofs will all be based on the following remark: If $0\leq x_n\leq s_n$ for $n\geq N$, where $N$ is some fixed number, and if $s_n\to 0$, then $x_n\to 0$. Theorem 3.20 (a): If $p>0$, then $\lim_{n\to\infty}\frac{1}{n^p}=0$. Proof. Take $n>(1/\varepsilon)^{1/p}$. (Note that the archimedean property of the real number system is used here.)","I am currently self studying Baby Rudin and I'm having some problems with his proof of Theorem 3.20(a) on page 58. I have read all previous chapters and I can't find any mention of real exponents besides its definition in the exercise section of Chapter 1 (but nothing about order/this identity). He defines $x^a$ for rational $a$ and $x>1$ then this is used to extend it to all real $a$ by $x^a = \sup \{x^t : t \leq a,~ t \in Q\}$. Am I missing something or does he expect the reader to fill this huge gap in the proof? Prefatory note on parts for Theorem 3.20: We shall now compute the limits of some sequences which occur frequently. The proofs will all be based on the following remark: If $0\leq x_n\leq s_n$ for $n\geq N$, where $N$ is some fixed number, and if $s_n\to 0$, then $x_n\to 0$. Theorem 3.20 (a): If $p>0$, then $\lim_{n\to\infty}\frac{1}{n^p}=0$. Proof. Take $n>(1/\varepsilon)^{1/p}$. (Note that the archimedean property of the real number system is used here.)",,"['real-analysis', 'analysis']"
24,Why is $\int\int f(x)f(y) |x-y|dxdy$ negative?,Why is  negative?,\int\int f(x)f(y) |x-y|dxdy,"The Setup Let $f:\mathbb{R} \to\mathbb{R}$ be a smooth function with support in the  interval $[-R,R]$ and satisfying $\int f = 0$.  By manipulating some integrals, I found the surprising inequality $$ \int\int f(x)f(y) |x-y| \,dxdy \leq 0. $$ My questions are Is this inequality true? Is my derivation below correct? Is there a different reason why this is true? Are there other nontrivial functions $g(x,y)$ for which  $$ \int \int f(x)f(y)g(x,y)\,dxdy \leq 0? $$ What properties should I expect of these functions $g(x,y)$? The Derivation Let $H:\mathbb{R}\to\mathbb{R}$ be the Heaviside function, so $H(x) = 1$ for $x>0$, and $H(x) = 0$ for $x\leq 0$.  I was interested in the convolution $H\ast f(x)$ defined by  $$ H\ast f(x) = \int f(y) H(x-y)\,dy = \int_{-\infty}^x f(y)\,dy. $$ Notice that since $f$ has support in $[-R,R]$ and $\int f = 0$,  $H\ast f$ also has support in $[-R,R]$. Now we're prepared to start the derivation of the inequality, beginning with the simple observation $$ 0 \leq \int |H\ast f(x)|^2\,dx = \int_{-R}^R |H\ast f(x)|^2\,dx. $$ First expand the square and the convolution, and then rearrange the order of the integrals: \begin{align*} \int_{-R}^R |H\ast f(x)|^2\,dx  &= \int_{-R}^R \left(\int_{-R}^R f(y)H(x-y)\,dy\right) \left(\int_{-R}^R f(z)H(x-z)\,dz\right) \,dx \\ &= \int_{-R}^R\int_{-R}^R f(y)f(z) \left(\int_{-R}^R H(x-y)H(x-z)\,dx\right)\,dydz \end{align*} The integrand $H(x-y)H(x-z)$ is $1$ when both $x>y$ and $x>z$, and $0$ otherwise.  Thus the integral comes out to $\min\{R-y,R-z\}$. We plug this back into the integral, and use again the fact that $\int f = 0$: \begin{align*} \int_{-R}^R\int_{-R}^R f(y)f(z) \min\{R-y,R-z\} \,dydz &= \int_{-R}^R \int_{-R}^R f(y)f(z)(\min\{-y,-z\} - R)\,dydz \\&= \int_{-R}^R\int_{-R}^R f(y)f(z)\min\{-y,-z\}\,dydz. \end{align*} Now split up the domain according to which of $-y$ or $-z$ is smaller: \begin{align*} \int_{-R}^R\int_{-R}^R f(y)f(z)\min\{-y,-z\}\,dydz &= -\int_{y=-R}^R f(y)\left(y\int_{z=-R}^y f(z)\,dz + \int_{z=y}^R zf(z)\,dz\right)\,dy.  \end{align*} Since $\int_{-R}^R f(z)\,dz = 0$, $\int_{-R}^yf(z)\,dz = -\int_y^R f(z)\,dz$, so we can combine the integrals to conclude that  \begin{align*} \int |H\ast f(x)|^2\,dx &= \int_{-R}^R f(y) \int_y^R f(z) (y-z)\,dz\,dy\\ &=  - \int_{-R}^R f(y) \int_y^R f(z) |y-z|\,dz\,dy  \end{align*} To get to the integral above, swap the names of the dummy variables and then swap the order of the integrals: \begin{align*} \int |H\ast f(x)|^2\,dx &= -\int_{-R}^R f(z) \int_z^R f(y) |z-y|\,dy\,dz\\ &= -\int_{-R}^R f(y) \int_{-R}^y f(z) |z-y|\,dy\,dz. \end{align*} In conclusion, adding both of these formulas together, we obtain $$ 0\leq 2\int |H\ast f(x)|^2\,dx = -\int\int f(y)f(z)|y-z|\,dy\,dz. $$ The derivation is quite long so there's a good chance I've made a mistake.","The Setup Let $f:\mathbb{R} \to\mathbb{R}$ be a smooth function with support in the  interval $[-R,R]$ and satisfying $\int f = 0$.  By manipulating some integrals, I found the surprising inequality $$ \int\int f(x)f(y) |x-y| \,dxdy \leq 0. $$ My questions are Is this inequality true? Is my derivation below correct? Is there a different reason why this is true? Are there other nontrivial functions $g(x,y)$ for which  $$ \int \int f(x)f(y)g(x,y)\,dxdy \leq 0? $$ What properties should I expect of these functions $g(x,y)$? The Derivation Let $H:\mathbb{R}\to\mathbb{R}$ be the Heaviside function, so $H(x) = 1$ for $x>0$, and $H(x) = 0$ for $x\leq 0$.  I was interested in the convolution $H\ast f(x)$ defined by  $$ H\ast f(x) = \int f(y) H(x-y)\,dy = \int_{-\infty}^x f(y)\,dy. $$ Notice that since $f$ has support in $[-R,R]$ and $\int f = 0$,  $H\ast f$ also has support in $[-R,R]$. Now we're prepared to start the derivation of the inequality, beginning with the simple observation $$ 0 \leq \int |H\ast f(x)|^2\,dx = \int_{-R}^R |H\ast f(x)|^2\,dx. $$ First expand the square and the convolution, and then rearrange the order of the integrals: \begin{align*} \int_{-R}^R |H\ast f(x)|^2\,dx  &= \int_{-R}^R \left(\int_{-R}^R f(y)H(x-y)\,dy\right) \left(\int_{-R}^R f(z)H(x-z)\,dz\right) \,dx \\ &= \int_{-R}^R\int_{-R}^R f(y)f(z) \left(\int_{-R}^R H(x-y)H(x-z)\,dx\right)\,dydz \end{align*} The integrand $H(x-y)H(x-z)$ is $1$ when both $x>y$ and $x>z$, and $0$ otherwise.  Thus the integral comes out to $\min\{R-y,R-z\}$. We plug this back into the integral, and use again the fact that $\int f = 0$: \begin{align*} \int_{-R}^R\int_{-R}^R f(y)f(z) \min\{R-y,R-z\} \,dydz &= \int_{-R}^R \int_{-R}^R f(y)f(z)(\min\{-y,-z\} - R)\,dydz \\&= \int_{-R}^R\int_{-R}^R f(y)f(z)\min\{-y,-z\}\,dydz. \end{align*} Now split up the domain according to which of $-y$ or $-z$ is smaller: \begin{align*} \int_{-R}^R\int_{-R}^R f(y)f(z)\min\{-y,-z\}\,dydz &= -\int_{y=-R}^R f(y)\left(y\int_{z=-R}^y f(z)\,dz + \int_{z=y}^R zf(z)\,dz\right)\,dy.  \end{align*} Since $\int_{-R}^R f(z)\,dz = 0$, $\int_{-R}^yf(z)\,dz = -\int_y^R f(z)\,dz$, so we can combine the integrals to conclude that  \begin{align*} \int |H\ast f(x)|^2\,dx &= \int_{-R}^R f(y) \int_y^R f(z) (y-z)\,dz\,dy\\ &=  - \int_{-R}^R f(y) \int_y^R f(z) |y-z|\,dz\,dy  \end{align*} To get to the integral above, swap the names of the dummy variables and then swap the order of the integrals: \begin{align*} \int |H\ast f(x)|^2\,dx &= -\int_{-R}^R f(z) \int_z^R f(y) |z-y|\,dy\,dz\\ &= -\int_{-R}^R f(y) \int_{-R}^y f(z) |z-y|\,dy\,dz. \end{align*} In conclusion, adding both of these formulas together, we obtain $$ 0\leq 2\int |H\ast f(x)|^2\,dx = -\int\int f(y)f(z)|y-z|\,dy\,dz. $$ The derivation is quite long so there's a good chance I've made a mistake.",,"['real-analysis', 'integration', 'inequality']"
25,When is this sum of perfect powers bounded,When is this sum of perfect powers bounded,,"For any positive integers $n,d$, let $$ A_d(n)=\frac{\sum_{k=1}^n k^{2d}}{n(n+1)(2n+1)} $$ It is easy to see (and well-known) that for fixed $d$, $A_d(.)$ is a polynomial of degree $2d-2$. Writing $A_d(x)$ then makes sense for any $x\in{\mathbb R}$, not just the positive integers. Is it known for which real numbers $x$ the sequence $(A_d(x))_{d\geq 1}$ is bounded ? UPDATE : this question was also asked on MO","For any positive integers $n,d$, let $$ A_d(n)=\frac{\sum_{k=1}^n k^{2d}}{n(n+1)(2n+1)} $$ It is easy to see (and well-known) that for fixed $d$, $A_d(.)$ is a polynomial of degree $2d-2$. Writing $A_d(x)$ then makes sense for any $x\in{\mathbb R}$, not just the positive integers. Is it known for which real numbers $x$ the sequence $(A_d(x))_{d\geq 1}$ is bounded ? UPDATE : this question was also asked on MO",,"['real-analysis', 'polynomials', 'asymptotics']"
26,Existence of a sequence of positive continuous functions on $\mathbb R$ which is unbounded precisely on $\mathbb Q$,Existence of a sequence of positive continuous functions on  which is unbounded precisely on,\mathbb R \mathbb Q,Is it possible to find a sequence of positive continuous functions $g_n$ on the real numbers such that $( g_n(x) )$ is unbounded if and only if $x \in \mathbb{Q}$ ?,Is it possible to find a sequence of positive continuous functions $g_n$ on the real numbers such that $( g_n(x) )$ is unbounded if and only if $x \in \mathbb{Q}$ ?,,['real-analysis']
27,Bernoulli's inequality and an unexpected limit,Bernoulli's inequality and an unexpected limit,,"This question is inspired by What would happen to Bernoulli's inequality if $x<-1$? . Let $x_n=\min\{x\in{\bf R}:(1+x)^n\geq 1+nx\}$, where $n$ is natural and odd (my mistake in the first version, observed by @mfl). Is it true, that  $$ \lim_{n\to\infty}x_n=-2? $$ Numerical computations suggest the positive answer.","This question is inspired by What would happen to Bernoulli's inequality if $x<-1$? . Let $x_n=\min\{x\in{\bf R}:(1+x)^n\geq 1+nx\}$, where $n$ is natural and odd (my mistake in the first version, observed by @mfl). Is it true, that  $$ \lim_{n\to\infty}x_n=-2? $$ Numerical computations suggest the positive answer.",,"['real-analysis', 'inequality']"
28,Under which conditions a solution of an ODE is analytic function?,Under which conditions a solution of an ODE is analytic function?,,"If I'm not wrong there is a theorem that says that if the conditions for Picard's theorem are satisfied, for an ode $\dot x=f(x,t)$, then the solution of the ode is as smooth as $f$. I think I'm not wrong with this fact. So if $f$ is $\mathcal{C}^k$ then $x(t)$ will be also $\mathcal{C}^k$. I wonder if the fact that $f$ is analytic implies also that $x$ is analytic or if there is another condition that implies so.","If I'm not wrong there is a theorem that says that if the conditions for Picard's theorem are satisfied, for an ode $\dot x=f(x,t)$, then the solution of the ode is as smooth as $f$. I think I'm not wrong with this fact. So if $f$ is $\mathcal{C}^k$ then $x(t)$ will be also $\mathcal{C}^k$. I wonder if the fact that $f$ is analytic implies also that $x$ is analytic or if there is another condition that implies so.",,"['real-analysis', 'ordinary-differential-equations']"
29,$A$ is open iff it is union of open balls,is open iff it is union of open balls,A,"Suppose $(X,d)$ is metric space. I want to show that $A \subseteq X$ is open iff $A$ is union of open balls. My attempt. suppose $A$ is open, then for every $x \in A$, there exists $r>0$ such that $B(x,r) \subset A$ by definition. We claim that $A = \bigcup_{x\in A} B(x,r) $. To see this, pick $x \in A$, then can find $r>0$ such that $x \in B(x,r) \subseteq \bigcup B(x,r)$. Conversely, suppose $y \in \bigcup B(x,r) \implies y \in B(x,r) $ for some $x$. But $B(x,r) \subseteq A$ for some $x$, hence $y \in A$. So, our claim is proved. For the other direction, suppose $A = \bigcup_{\alpha} O_{\alpha} $ where $O_{\alpha}$ is open ball. Take $x \in A$, then $x \in O_{\alpha} $ for some $\alpha$. But $O_{\alpha} \subseteq \bigcup O_{\alpha} = A $. So, we have found an open ball inside $A$, and since $x$ was chosen arbitrary, then $A$ must be an open set by definition. Is this correct? Any feedback would be greatly appreciated. thanks","Suppose $(X,d)$ is metric space. I want to show that $A \subseteq X$ is open iff $A$ is union of open balls. My attempt. suppose $A$ is open, then for every $x \in A$, there exists $r>0$ such that $B(x,r) \subset A$ by definition. We claim that $A = \bigcup_{x\in A} B(x,r) $. To see this, pick $x \in A$, then can find $r>0$ such that $x \in B(x,r) \subseteq \bigcup B(x,r)$. Conversely, suppose $y \in \bigcup B(x,r) \implies y \in B(x,r) $ for some $x$. But $B(x,r) \subseteq A$ for some $x$, hence $y \in A$. So, our claim is proved. For the other direction, suppose $A = \bigcup_{\alpha} O_{\alpha} $ where $O_{\alpha}$ is open ball. Take $x \in A$, then $x \in O_{\alpha} $ for some $\alpha$. But $O_{\alpha} \subseteq \bigcup O_{\alpha} = A $. So, we have found an open ball inside $A$, and since $x$ was chosen arbitrary, then $A$ must be an open set by definition. Is this correct? Any feedback would be greatly appreciated. thanks",,['real-analysis']
30,When is the moment of inertia of a smooth plane curve is maximum?,When is the moment of inertia of a smooth plane curve is maximum?,,"Given a smooth plane curve $(x(s),y(s))$, parameterized in arc length $s$, of fixed finite length $L$, its moment of inertia about its center of mass (axis perpendicular to the plane) is given as $$MI = \int_0^L ((x(s)-x_{cm})^2 + (y(s)-y_{cm})^2)  ds$$. What I predict from earlier discussions, and almost convinced is that if we fix length $L$, $MI$ is maximum when the curve is a straight line. I lack the faculty of mathematical machinery (I guess calculus of variations) to prove it, hence is my gentle request to help me out in proving it and thoroughly understanding the situation and all the corollaries and nuances. This not just the result I need, but I want to do more with it and hence would like understand all the things that are making this result and even more general ones (only to plane curves though).","Given a smooth plane curve $(x(s),y(s))$, parameterized in arc length $s$, of fixed finite length $L$, its moment of inertia about its center of mass (axis perpendicular to the plane) is given as $$MI = \int_0^L ((x(s)-x_{cm})^2 + (y(s)-y_{cm})^2)  ds$$. What I predict from earlier discussions, and almost convinced is that if we fix length $L$, $MI$ is maximum when the curve is a straight line. I lack the faculty of mathematical machinery (I guess calculus of variations) to prove it, hence is my gentle request to help me out in proving it and thoroughly understanding the situation and all the corollaries and nuances. This not just the result I need, but I want to do more with it and hence would like understand all the things that are making this result and even more general ones (only to plane curves though).",,"['calculus', 'real-analysis', 'calculus-of-variations', 'plane-curves']"
31,Nowhere dense set with positive Lebesgue measure,Nowhere dense set with positive Lebesgue measure,,I'm looking for a subset $A$ of $\mathbb R$ such that $A$ is a nowhere dense set with positive Lebesgue measure.,I'm looking for a subset $A$ of $\mathbb R$ such that $A$ is a nowhere dense set with positive Lebesgue measure.,,"['real-analysis', 'measure-theory']"
32,"How to prove $f \in L^p(0,1)$",How to prove,"f \in L^p(0,1)","Let $f \ge 0$ , $f\in L^1(0,1)$ , monotone decrease. Suppose $\forall a,x \in (0,1) $ such that $0<x-a<x+a<1$ , $$\int_{x-a}^x f(t)dt<\frac{5}{4} \int_{x}^{x+a}f(t)dt.$$ Prove $f\in L^p(0,1)$ , $\forall 1\le p\le \frac{\ln 2}{\ln 3-\ln 2}$ . I can get a weaker bound $1\le p<\frac{\ln 2}{\ln25 - \ln16}$ by dividing the interval of integration by $[2^{-n},2^{-n+1}]$ and getting $f(2^{-n})<\frac{25}{16}f(2^{-n+1})$ , but I don't know how to get a finer bound. My attempt, $$ 2^{-n}f(2^{-n})<\int_0^{2^{-n}} f(t)dt<\frac{5}{4}\int_{2^{-n}} ^{2^{-n+1}} f(t)dt<\frac{25}{16}\int_{2^{-n+1}} ^{2^{-n+2}} f(t)dt<2^{-n}f(2^{-n+1}), $$ thus, $$f(2^{-n})<\frac{25}{16}f(2^{-n+1})<\left(\frac{25}{16}\right)^nf(1).$$ Then, $$ \int_0^1 f(t)^p<\sum_{n=0}^{+\infty}2^{-n}f(2^{-n})^p<f(1)^p\sum_{n=0}^{+\infty}\left(\frac{25}{16}\right)^{np}\cdot2^{-n}, $$ if $\frac{1}{2}\left(\frac{25}{16}\right)^p<1$ , we can get $\int_0^1 f(t)^p<+\infty$ .","Let , , monotone decrease. Suppose such that , Prove , . I can get a weaker bound by dividing the interval of integration by and getting , but I don't know how to get a finer bound. My attempt, thus, Then, if , we can get .","f \ge 0 f\in L^1(0,1) \forall a,x \in (0,1)  0<x-a<x+a<1 \int_{x-a}^x f(t)dt<\frac{5}{4} \int_{x}^{x+a}f(t)dt. f\in L^p(0,1) \forall 1\le p\le \frac{\ln 2}{\ln 3-\ln 2} 1\le p<\frac{\ln 2}{\ln25 - \ln16} [2^{-n},2^{-n+1}] f(2^{-n})<\frac{25}{16}f(2^{-n+1}) 
2^{-n}f(2^{-n})<\int_0^{2^{-n}} f(t)dt<\frac{5}{4}\int_{2^{-n}} ^{2^{-n+1}} f(t)dt<\frac{25}{16}\int_{2^{-n+1}} ^{2^{-n+2}} f(t)dt<2^{-n}f(2^{-n+1}),
 f(2^{-n})<\frac{25}{16}f(2^{-n+1})<\left(\frac{25}{16}\right)^nf(1). 
\int_0^1 f(t)^p<\sum_{n=0}^{+\infty}2^{-n}f(2^{-n})^p<f(1)^p\sum_{n=0}^{+\infty}\left(\frac{25}{16}\right)^{np}\cdot2^{-n},
 \frac{1}{2}\left(\frac{25}{16}\right)^p<1 \int_0^1 f(t)^p<+\infty","['real-analysis', 'analysis']"
33,"Conjecture about the mean value of an almost periodic function (the ""Mountains of Guilin"")","Conjecture about the mean value of an almost periodic function (the ""Mountains of Guilin"")",,"Consider the function $f_{p_j,n}(x)=|\sin (p_1x)+\sin (p_2x)+\dots+\sin (p_nx)|$ where $p_j$ is any sequence such that no two $p$ are rational multiples of each other where the $p_j$ are linearly independent over $\mathbb{Q}$ (so the function is almost periodic). (After reading @mollyerin's answer, I changed the condition on $p_j$ .) For example, here is the graph of $f_{2^{1/j},\color{red}{5}}(x)$ : And here is the graph of $f_{2^{1/j},\color{red}{10}}(x)$ : I call this kind of function $f_{p_j,n}(x)$ , the "" Mountains of Guilin "". Now consider the mean value of $f_{p_j,n}(x)$ . I have evidence that suggests that: For any given value of $n$ , the mean value of $f_{p_j,n}(x)$ is independent of the choice of sequence $p_j$ (as long as the $p_j$ are linearly independent over $\mathbb{Q}$ ). The mean value of $f_{p_j,\color{red}{2}}(x)$ is $8/\pi^2$ . I have not been able to find a general expression for the mean value of $f_{p_j,n}(x)$ in terms of $n$ , but I have the following conjecture : $$\color{red}{\lim_{n\to\infty}\frac{\text{mean value of $f_{p_j,n}(x)$}}{\sqrt n}=\frac{1}{\sqrt \pi}}$$ Is my conjecture true? Numerical evidence for my conjecture: Using desmos, I approximated the mean value of $f_{2^{1/j},n}(x)$ as $M_n=\frac{1}{10^7}\sum\limits_{k=1}^{10^7}|\sin (2^1k)+\sin (2^{1/2}k)+\dots+\sin (2^{1/n}k)|$ Letting $L(n)=\dfrac{M_n}{\sqrt n}$ , we have: $L(1)\approx 1.128379\left(\frac{1}{\sqrt \pi}\right)$ $L(2)\approx 1.015898\left(\frac{1}{\sqrt \pi}\right)$ $L(10)\approx 1.006373\left(\frac{1}{\sqrt \pi}\right)$ $L(100)\approx 1.000334\left(\frac{1}{\sqrt \pi}\right)$ This suggests that my conjecture may be true, but I don't know how to prove it. Visual representation of the mean value: If my conjecture is true, then there is a nice way to visually represent the mean value of the function. Draw a circle of area $n$ with its centre on the $x$ -axis. A horizontal line through the top of the circle represents, approximately, the average value of the function.","Consider the function where is any sequence such that no two are rational multiples of each other where the are linearly independent over (so the function is almost periodic). (After reading @mollyerin's answer, I changed the condition on .) For example, here is the graph of : And here is the graph of : I call this kind of function , the "" Mountains of Guilin "". Now consider the mean value of . I have evidence that suggests that: For any given value of , the mean value of is independent of the choice of sequence (as long as the are linearly independent over ). The mean value of is . I have not been able to find a general expression for the mean value of in terms of , but I have the following conjecture : Is my conjecture true? Numerical evidence for my conjecture: Using desmos, I approximated the mean value of as Letting , we have: This suggests that my conjecture may be true, but I don't know how to prove it. Visual representation of the mean value: If my conjecture is true, then there is a nice way to visually represent the mean value of the function. Draw a circle of area with its centre on the -axis. A horizontal line through the top of the circle represents, approximately, the average value of the function.","f_{p_j,n}(x)=|\sin (p_1x)+\sin (p_2x)+\dots+\sin (p_nx)| p_j p p_j \mathbb{Q} p_j f_{2^{1/j},\color{red}{5}}(x) f_{2^{1/j},\color{red}{10}}(x) f_{p_j,n}(x) f_{p_j,n}(x) n f_{p_j,n}(x) p_j p_j \mathbb{Q} f_{p_j,\color{red}{2}}(x) 8/\pi^2 f_{p_j,n}(x) n \color{red}{\lim_{n\to\infty}\frac{\text{mean value of f_{p_j,n}(x)}}{\sqrt n}=\frac{1}{\sqrt \pi}} f_{2^{1/j},n}(x) M_n=\frac{1}{10^7}\sum\limits_{k=1}^{10^7}|\sin (2^1k)+\sin (2^{1/2}k)+\dots+\sin (2^{1/n}k)| L(n)=\dfrac{M_n}{\sqrt n} L(1)\approx 1.128379\left(\frac{1}{\sqrt \pi}\right) L(2)\approx 1.015898\left(\frac{1}{\sqrt \pi}\right) L(10)\approx 1.006373\left(\frac{1}{\sqrt \pi}\right) L(100)\approx 1.000334\left(\frac{1}{\sqrt \pi}\right) n x","['real-analysis', 'integration', 'ergodic-theory', 'means', 'conjectures']"
34,"Proof (without use of differential calculus) that $e^{\sqrt{x}}$ is convex on $[1,+\infty)$.",Proof (without use of differential calculus) that  is convex on .,"e^{\sqrt{x}} [1,+\infty)","Prove (without use of  differentiation) that $f(x)=e^{\sqrt{x}}$ is convex on $[1,+\infty)$ . Attempt . Function $x\mapsto e^x$ is convex and increasing, but $x\mapsto \sqrt{x}$ is concave, so we cannot use the proposition of composition: $$(convex~\&~increasing)\circ convex=convex.$$ Definition would require to prove for all $x,~y\geqslant 1$ and $\lambda \in [0,1]$ : $$e^{\sqrt{\lambda x+(1-\lambda) y}}\leqslant \lambda e^{\sqrt{ x}}+(1-\lambda)e^{\sqrt{y}}$$ but squaring doesn't work here. If we used continuity in order to prove mid-convexity, the problem would go like: $$e^{\sqrt{\frac{x+y}{2}}}\leqslant \frac{e^{\sqrt{x}}+e^{\sqrt{y}}}{2},$$ equivalently: $$e^{\sqrt{\frac{x+y}{2}}}-e^{\sqrt{x}} \leqslant  e^{\sqrt{y}}-e^{\sqrt{\frac{x+y}{2}}}$$ (but without MVT what would we do with these differences?) Thanks in advance for the help.","Prove (without use of  differentiation) that is convex on . Attempt . Function is convex and increasing, but is concave, so we cannot use the proposition of composition: Definition would require to prove for all and : but squaring doesn't work here. If we used continuity in order to prove mid-convexity, the problem would go like: equivalently: (but without MVT what would we do with these differences?) Thanks in advance for the help.","f(x)=e^{\sqrt{x}} [1,+\infty) x\mapsto e^x x\mapsto \sqrt{x} (convex~\&~increasing)\circ convex=convex. x,~y\geqslant 1 \lambda \in [0,1] e^{\sqrt{\lambda x+(1-\lambda) y}}\leqslant \lambda e^{\sqrt{ x}}+(1-\lambda)e^{\sqrt{y}} e^{\sqrt{\frac{x+y}{2}}}\leqslant \frac{e^{\sqrt{x}}+e^{\sqrt{y}}}{2}, e^{\sqrt{\frac{x+y}{2}}}-e^{\sqrt{x}} \leqslant  e^{\sqrt{y}}-e^{\sqrt{\frac{x+y}{2}}}","['real-analysis', 'analysis', 'convex-analysis']"
35,A minimization problem in function fitting setup,A minimization problem in function fitting setup,,"Let $\Omega$ be a convex, closed, compact set in $\mathbb{R}^d$ with a smooth boundary. Given a data $(x_i,d_i)$ , $x_i \in \Omega$ , $d_i \in \mathbb{R}$ , $i = 1,2,3...N$ , $N>d$ and $\sum\limits_{i=1}^N d_i = 0$ . Also given that, there are always $d$ vectors in $\{x_i\}$ which are linearly independent. Let $A = \int_{\Omega}dx$ I want to find a continuous function $f:\Omega \to \mathbb{R}$ , such that, $\int_{\Omega}f(x)dx = 0$ and $C(f)$ is minimum, where $$C(f) = \frac{A^{1/d}}{N}\bigg(\sum\limits_{i=1}^N |f(x_i)-d_i|^d\bigg)^{1/d} +\|f\|_{L^d}+ A^{1/d} \||\nabla f|\|_{L^d}$$ Does the solution exist? Is the solution unique? I mean any two solutions are equal almost everywhere?","Let be a convex, closed, compact set in with a smooth boundary. Given a data , , , , and . Also given that, there are always vectors in which are linearly independent. Let I want to find a continuous function , such that, and is minimum, where Does the solution exist? Is the solution unique? I mean any two solutions are equal almost everywhere?","\Omega \mathbb{R}^d (x_i,d_i) x_i \in \Omega d_i \in \mathbb{R} i = 1,2,3...N N>d \sum\limits_{i=1}^N d_i = 0 d \{x_i\} A = \int_{\Omega}dx f:\Omega \to \mathbb{R} \int_{\Omega}f(x)dx = 0 C(f) C(f) = \frac{A^{1/d}}{N}\bigg(\sum\limits_{i=1}^N |f(x_i)-d_i|^d\bigg)^{1/d} +\|f\|_{L^d}+ A^{1/d} \||\nabla f|\|_{L^d}","['functional-analysis', 'real-analysis', 'convex-optimization']"
36,Can it be that $f$ and $g$ are everywhere continuous but nowhere differentiable but that $f \circ g$ is differentiable?,Can it be that  and  are everywhere continuous but nowhere differentiable but that  is differentiable?,f g f \circ g,"So, I was just asking myself can something like this happen? I was thinking about some everywhere continuous but nowhere differentiable functions $f$ and $g$ and the natural question arose on can the composition $f \circ g$ be differentiable, in other words, can the operation of composition somehow ""smoothen"" the irregularities of $f$ and $g$ which make them non-differentiable in such a way that composition becomes differentiable? So here is the question again: Suppose that $f$ and $g$ are everywhere continuous but nowhere differentiable functions. Can $f \circ g$ be differentiable? If such an example exists it would be interesting because the rule $(f(g(x))'=f'(g(x)) \cdot g'(x)$ would not hold, and not only that it would not hold, it would not make any sense because $f$ and $g$ are not differentiable.","So, I was just asking myself can something like this happen? I was thinking about some everywhere continuous but nowhere differentiable functions $f$ and $g$ and the natural question arose on can the composition $f \circ g$ be differentiable, in other words, can the operation of composition somehow ""smoothen"" the irregularities of $f$ and $g$ which make them non-differentiable in such a way that composition becomes differentiable? So here is the question again: Suppose that $f$ and $g$ are everywhere continuous but nowhere differentiable functions. Can $f \circ g$ be differentiable? If such an example exists it would be interesting because the rule $(f(g(x))'=f'(g(x)) \cdot g'(x)$ would not hold, and not only that it would not hold, it would not make any sense because $f$ and $g$ are not differentiable.",,"['real-analysis', 'derivatives', 'continuity', 'function-and-relation-composition']"
37,How to show that the series of $\frac{\sin(n)}{\log(n)}$ converges?,How to show that the series of  converges?,\frac{\sin(n)}{\log(n)},"Edit:  I am seeking a solution that uses only calculus and real analysis methods -- not complex analysis.  This is an old advanced calculus exam question, and I think we are not allowed to use any complex analysis that could make the problem statement a triviality. Show that the series $$\sum_{n=2}^{\infty} \frac{\sin(n)}{\log(n)}$$ converges. Any hints or suggestions are welcome. Some thoughts: The integral test is not applicable here, since the summands are not positive. The Dirichlet test does seem applicable either, since if I let 1/log(n) be the decreasing sequence, then the series of sin(n) does not have bounded partial sums for every interval. Thanks,","Edit:  I am seeking a solution that uses only calculus and real analysis methods -- not complex analysis.  This is an old advanced calculus exam question, and I think we are not allowed to use any complex analysis that could make the problem statement a triviality. Show that the series $$\sum_{n=2}^{\infty} \frac{\sin(n)}{\log(n)}$$ converges. Any hints or suggestions are welcome. Some thoughts: The integral test is not applicable here, since the summands are not positive. The Dirichlet test does seem applicable either, since if I let 1/log(n) be the decreasing sequence, then the series of sin(n) does not have bounded partial sums for every interval. Thanks,",,"['calculus', 'real-analysis', 'sequences-and-series', 'convergence-divergence']"
38,Dominated convergence theorem for complex-valued functions?,Dominated convergence theorem for complex-valued functions?,,"Suppose there is a sequence $\{f_n(x)\}$ such that $\lim_{n\rightarrow\infty}f_n(x)=f(x)$. I've previously used the dominated convergence theorem for interchanging the limit and the integral in $\lim_{n\rightarrow\infty}\int_{-\infty}^{\infty} f_n(x)dx$ when $f_n(x)$ was a real-valued function $f_n:\mathbb{R}\rightarrow\mathbb{R}$.  Per the usual steps, I would find an integrable function $g:\mathbb{R}\rightarrow\mathbb{R}$ such that $|f_n(x)|<g(x)$ for all $n$ in the index set and $x\in\mathbb{R}$.  I would thus justify $\lim_{n\rightarrow\infty}\int_{-\infty}^{\infty} f_n(x)dx=\int_{-\infty}^{\infty} \lim_{n\rightarrow\infty}f_n(x)dx=\int_{-\infty}^{\infty}f(x)dx$. I am wondering what happens when $f_n(x)$ as well as the limiting function $f(x)$ are complex-valued, i.e., $f_n:\mathbb{R}\rightarrow\mathbb{C}$, but their integral is real-valued. How do I safely interchange the limit and the integral? Specific example The semi-classical theory of optical homodyne detection (see section on homodyne detection, for example, here ) involves subtracting two independent Poisson random variables.  The resulting random variable, when appropriately normalized, converges to a Gaussian random variable in distribution.  I am wondering if a stronger result holds, where the density function converges pointwise to the Gaussina density as well. Consider Poisson random variables $N_-$ and $N_+$ with respective means $a^2_-=\frac{1}{2}(a_S-a_L)^2$ and $a^2_+=\frac{1}{2}(a_S+a_L)^2$.  Here, $a_S$ is the amplitude of the signal field, and $a_L$ is the amplitude of the much-stronger local oscillator field.  $N_-$ and $N_+$ are the photon counts at the two arms of the homodyne detector.  To recover the signal at the output, we subtract the two counts (and normalize), which effectively cancels the local oscillator field. Thus, consider the random variable $A=\frac{N_+-N_-}{2a_L}$.  Its characteristic function is just the product of the characteristic functions of the Poisson random variables $\frac{N_+}{2a_L}$ and $-\frac{N_-}{2a_L}$: $$\phi_A(t)=\exp\left[a_+^2(e^{it/2a_L}-1)+a_-^2(e^{-it/2a_L}-1)\right],$$ where $i=\sqrt{-1}$. Now, taking the limit as $a_L\rightarrow\infty$ yields: $$\begin{align}\lim_{a_L\rightarrow\infty}\phi_A(t)&=\lim_{a_L\rightarrow\infty}\exp\left[a_+^2(e^{it/2a_L}-1)+a_-^2(e^{-it/2a_L}-1)\right]\\ &=\lim_{a_L\rightarrow\infty}\exp\left[a_+^2\left(\frac{it}{2a_L}-\frac{t^2}{8a_L^2}+\mathcal{O}(a_L^{-3})\right)+a_-^2\left(-\frac{it}{2a_L}-\frac{t^2}{8a_L^2}+\mathcal{O}(a_L^{-3})\right)\right]\\ &\begin{aligned}=\lim_{a_L\rightarrow\infty}\exp\left[\frac{1}{2}(a_S^2+2a_Sa_L+a_L^2)\left(\frac{it}{2a_L}-\frac{t^2}{8a_L^2}+\mathcal{O}(a_L^{-3})\right)\\\qquad\qquad\qquad\qquad\qquad+\frac{1}{2}(a_S^2-2a_Sa_L+a_L^2)\left(-\frac{it}{2a_L}-\frac{t^2}{8a_L^2}+\mathcal{O}(a_L^{-3})\right)\right]\end{aligned}\\ &=\lim_{a_L\rightarrow\infty}\exp\left[ita_S-\frac{t^2}{8}-\frac{t^2a_S^2}{8a_L^2}+\mathcal{O}(a_L^{-1})\right]\\ &=\exp\left[ita_S-\frac{t^2}{8}\right], \end{align}$$ which is the characteristic function of Gaussian random variable with mean $a_S$ and variance $\frac{1}{4}$, thus proving convergence of $A$ to Gaussian in distribution. Now, applying the inverse Fourier transform to $\phi_A(t)$ yields the density function of $A$. I am wondering if it's Gaussian in the limit of large $a_L$.  It would be if: $$\begin{align}\lim_{a_L\rightarrow\infty}\int_{-\infty}^{\infty}\exp\left[-it(x-a_S)-\frac{t^2}{8}-\frac{t^2a_S^2}{8a_L^2}+\mathcal{O}(a_L^{-1})\right]dt=\\ \qquad\int_{-\infty}^{\infty}\lim_{a_L\rightarrow\infty}\exp\left[-it(x-a_S)-\frac{t^2}{8}-\frac{t^2a_S^2}{8a_L^2}+\mathcal{O}(a_L^{-1})\right]dt.\end{align}$$ Can someone help?","Suppose there is a sequence $\{f_n(x)\}$ such that $\lim_{n\rightarrow\infty}f_n(x)=f(x)$. I've previously used the dominated convergence theorem for interchanging the limit and the integral in $\lim_{n\rightarrow\infty}\int_{-\infty}^{\infty} f_n(x)dx$ when $f_n(x)$ was a real-valued function $f_n:\mathbb{R}\rightarrow\mathbb{R}$.  Per the usual steps, I would find an integrable function $g:\mathbb{R}\rightarrow\mathbb{R}$ such that $|f_n(x)|<g(x)$ for all $n$ in the index set and $x\in\mathbb{R}$.  I would thus justify $\lim_{n\rightarrow\infty}\int_{-\infty}^{\infty} f_n(x)dx=\int_{-\infty}^{\infty} \lim_{n\rightarrow\infty}f_n(x)dx=\int_{-\infty}^{\infty}f(x)dx$. I am wondering what happens when $f_n(x)$ as well as the limiting function $f(x)$ are complex-valued, i.e., $f_n:\mathbb{R}\rightarrow\mathbb{C}$, but their integral is real-valued. How do I safely interchange the limit and the integral? Specific example The semi-classical theory of optical homodyne detection (see section on homodyne detection, for example, here ) involves subtracting two independent Poisson random variables.  The resulting random variable, when appropriately normalized, converges to a Gaussian random variable in distribution.  I am wondering if a stronger result holds, where the density function converges pointwise to the Gaussina density as well. Consider Poisson random variables $N_-$ and $N_+$ with respective means $a^2_-=\frac{1}{2}(a_S-a_L)^2$ and $a^2_+=\frac{1}{2}(a_S+a_L)^2$.  Here, $a_S$ is the amplitude of the signal field, and $a_L$ is the amplitude of the much-stronger local oscillator field.  $N_-$ and $N_+$ are the photon counts at the two arms of the homodyne detector.  To recover the signal at the output, we subtract the two counts (and normalize), which effectively cancels the local oscillator field. Thus, consider the random variable $A=\frac{N_+-N_-}{2a_L}$.  Its characteristic function is just the product of the characteristic functions of the Poisson random variables $\frac{N_+}{2a_L}$ and $-\frac{N_-}{2a_L}$: $$\phi_A(t)=\exp\left[a_+^2(e^{it/2a_L}-1)+a_-^2(e^{-it/2a_L}-1)\right],$$ where $i=\sqrt{-1}$. Now, taking the limit as $a_L\rightarrow\infty$ yields: $$\begin{align}\lim_{a_L\rightarrow\infty}\phi_A(t)&=\lim_{a_L\rightarrow\infty}\exp\left[a_+^2(e^{it/2a_L}-1)+a_-^2(e^{-it/2a_L}-1)\right]\\ &=\lim_{a_L\rightarrow\infty}\exp\left[a_+^2\left(\frac{it}{2a_L}-\frac{t^2}{8a_L^2}+\mathcal{O}(a_L^{-3})\right)+a_-^2\left(-\frac{it}{2a_L}-\frac{t^2}{8a_L^2}+\mathcal{O}(a_L^{-3})\right)\right]\\ &\begin{aligned}=\lim_{a_L\rightarrow\infty}\exp\left[\frac{1}{2}(a_S^2+2a_Sa_L+a_L^2)\left(\frac{it}{2a_L}-\frac{t^2}{8a_L^2}+\mathcal{O}(a_L^{-3})\right)\\\qquad\qquad\qquad\qquad\qquad+\frac{1}{2}(a_S^2-2a_Sa_L+a_L^2)\left(-\frac{it}{2a_L}-\frac{t^2}{8a_L^2}+\mathcal{O}(a_L^{-3})\right)\right]\end{aligned}\\ &=\lim_{a_L\rightarrow\infty}\exp\left[ita_S-\frac{t^2}{8}-\frac{t^2a_S^2}{8a_L^2}+\mathcal{O}(a_L^{-1})\right]\\ &=\exp\left[ita_S-\frac{t^2}{8}\right], \end{align}$$ which is the characteristic function of Gaussian random variable with mean $a_S$ and variance $\frac{1}{4}$, thus proving convergence of $A$ to Gaussian in distribution. Now, applying the inverse Fourier transform to $\phi_A(t)$ yields the density function of $A$. I am wondering if it's Gaussian in the limit of large $a_L$.  It would be if: $$\begin{align}\lim_{a_L\rightarrow\infty}\int_{-\infty}^{\infty}\exp\left[-it(x-a_S)-\frac{t^2}{8}-\frac{t^2a_S^2}{8a_L^2}+\mathcal{O}(a_L^{-1})\right]dt=\\ \qquad\int_{-\infty}^{\infty}\lim_{a_L\rightarrow\infty}\exp\left[-it(x-a_S)-\frac{t^2}{8}-\frac{t^2a_S^2}{8a_L^2}+\mathcal{O}(a_L^{-1})\right]dt.\end{align}$$ Can someone help?",,"['real-analysis', 'integration', 'limits', 'probability-theory', 'convergence-divergence']"
39,"There is no continuous mapping from $L^1([0,1])$ onto $L^\infty([0,1])$",There is no continuous mapping from  onto,"L^1([0,1]) L^\infty([0,1])","There is no continuous mapping from $L^1([0,1])$ onto $L^\infty([0,1])$. Proof: suppose $T:L^1 \rightarrow L^\infty$ continuous and onto. $L^1$ is separable, let $\{f_n\}$  be a countable dense subset. If $T$ is onto, for each $g\in L^\infty$, $g = Tf$ for some $f\in L^1$, and we can approximate $g$ with $Tf_{n_k}$ since $T$ is continuous. We get the contradiction that $\{Tf_n\}$ is a countable dense subset of $L^\infty$. Is this okay? thank you very much!","There is no continuous mapping from $L^1([0,1])$ onto $L^\infty([0,1])$. Proof: suppose $T:L^1 \rightarrow L^\infty$ continuous and onto. $L^1$ is separable, let $\{f_n\}$  be a countable dense subset. If $T$ is onto, for each $g\in L^\infty$, $g = Tf$ for some $f\in L^1$, and we can approximate $g$ with $Tf_{n_k}$ since $T$ is continuous. We get the contradiction that $\{Tf_n\}$ is a countable dense subset of $L^\infty$. Is this okay? thank you very much!",,"['real-analysis', 'functional-analysis', 'proof-verification', 'lp-spaces']"
40,A real continuous periodic function with two incommensurate periods is constant.,A real continuous periodic function with two incommensurate periods is constant.,,"I think I have a proof for the statement, but I can't think of a counter-example when $f: \mathbb{R} \to \mathbb{R}$ is not continous. Here's the problem: Let $f: \mathbb{R} \to \mathbb{R}$ be a continuous periodic function with two incommensurate periods $T_1$ and $T_2$; that is $\displaystyle \frac{T_1}{T_2}$ is irrational. Prove that $f$ is a constant function. Give an example of a nonconstant periodic function with two incommensurate periods. Consider the set $G= \{n_1T_1+n_2T_2 : n_1,n_2 \in \mathbb{Z} \}$. It's straight forward to verify that this set forms a subgroup of $\mathbb{R}$ under addition. Since $\displaystyle \frac{T_1}{T_2}$ is irrational, $G$ will not be cyclic, because if it's cyclic, then there exists an element $m_1T_1+m_2T_2 \in G$ such that for any $n_1T_1+n_2T_2 \in G$ there exists a $p \in \mathbb{Z}$ such that: $$n_1T_1 + n_2T_2 = p (m_1T_1+m_2T_2)$$ Rearranging the terms we obtain: $$\frac{T_1}{T_2} = \frac{n_2-pm_2}{pm_1 -n_1} \in \mathbb{Q}$$ Which is contradiction. Therefore $G$ is not cyclic. Since $G$ is a subgroup of $\mathbb{R}$ which is not cylic, we conclude that $G$ is dense in $\mathbb{R}$. Assume that $f(0)=C$. I'm going to show that $f(x)=C$. Since $G$ is dense in $\mathbb{R}$, every point $x \in \mathbb{R}$ can be approached by elements of $G$ of the form $n_1T_1+n_2T_2$. In particular, for any $\delta>0$ there exists $c_1,c_2 \in \mathbb{Z}$ such that: $$ |x - (c_1T_1+c_2T2)| < \delta$$ Since $f$ is assumed to be continuous, this means that for any $\epsilon>0$ we have: $$|f(x) - f(c_1T_1+c_2T_2)|< \epsilon$$ but $f(c_1T_1+c_2T_2)=f(c_2T_2)=f(0)=C$. Therefore, for any $x \in \mathbb{R}$ we have shown that $\forall \epsilon>0: |f(x)-C|< \epsilon$ which implies $f(x)=C$. I can't think of a counter-example for when $f$ is not continuous. Can someone suggest a counter-example?","I think I have a proof for the statement, but I can't think of a counter-example when $f: \mathbb{R} \to \mathbb{R}$ is not continous. Here's the problem: Let $f: \mathbb{R} \to \mathbb{R}$ be a continuous periodic function with two incommensurate periods $T_1$ and $T_2$; that is $\displaystyle \frac{T_1}{T_2}$ is irrational. Prove that $f$ is a constant function. Give an example of a nonconstant periodic function with two incommensurate periods. Consider the set $G= \{n_1T_1+n_2T_2 : n_1,n_2 \in \mathbb{Z} \}$. It's straight forward to verify that this set forms a subgroup of $\mathbb{R}$ under addition. Since $\displaystyle \frac{T_1}{T_2}$ is irrational, $G$ will not be cyclic, because if it's cyclic, then there exists an element $m_1T_1+m_2T_2 \in G$ such that for any $n_1T_1+n_2T_2 \in G$ there exists a $p \in \mathbb{Z}$ such that: $$n_1T_1 + n_2T_2 = p (m_1T_1+m_2T_2)$$ Rearranging the terms we obtain: $$\frac{T_1}{T_2} = \frac{n_2-pm_2}{pm_1 -n_1} \in \mathbb{Q}$$ Which is contradiction. Therefore $G$ is not cyclic. Since $G$ is a subgroup of $\mathbb{R}$ which is not cylic, we conclude that $G$ is dense in $\mathbb{R}$. Assume that $f(0)=C$. I'm going to show that $f(x)=C$. Since $G$ is dense in $\mathbb{R}$, every point $x \in \mathbb{R}$ can be approached by elements of $G$ of the form $n_1T_1+n_2T_2$. In particular, for any $\delta>0$ there exists $c_1,c_2 \in \mathbb{Z}$ such that: $$ |x - (c_1T_1+c_2T2)| < \delta$$ Since $f$ is assumed to be continuous, this means that for any $\epsilon>0$ we have: $$|f(x) - f(c_1T_1+c_2T_2)|< \epsilon$$ but $f(c_1T_1+c_2T_2)=f(c_2T_2)=f(0)=C$. Therefore, for any $x \in \mathbb{R}$ we have shown that $\forall \epsilon>0: |f(x)-C|< \epsilon$ which implies $f(x)=C$. I can't think of a counter-example for when $f$ is not continuous. Can someone suggest a counter-example?",,"['calculus', 'real-analysis', 'proof-verification', 'examples-counterexamples']"
41,Let $f$ be a cont. on $\mathbb{R}$ and define $G(x)=\int_0^{\sin (x)}f(t) dt $. Show that $G$ is differentiable on $\Bbb{R}$ and compute $G'$.,Let  be a cont. on  and define . Show that  is differentiable on  and compute .,f \mathbb{R} G(x)=\int_0^{\sin (x)}f(t) dt  G \Bbb{R} G',"Let $f$ be a continuous function on $\mathbb{R}$ and define $$G(x)=\int_0^{\sin (x)}f(t) dt $$ Show that $G$ is differentiable on $\mathbb{R}$ and compute $G'$ . This is an exercise from Elementary Analysis: The Theory of Calculus by Kenneth A. Ross . My first idea is the following. Let $x\in \Bbb R$ . Define $F(x):=\displaystyle\int_0^{x}f(t) dt $ . And prove that $G:=F\circ \sin$ is differentiable at $x$ using: It is clear that $\sin(x)$ is differentiable at $x$ . So I need to proof that $F$ is differentiable at $y:=\sin(x)$ . I was trying to prove that using: 34.3 Fundamental Theorem of Calculus II. Let $f$ be an integrable function on $[a, b] .$ For $x$ in $[a, b],$ let $$ F(x)=\int_{a}^{x} f(t) d t $$ Then $F$ is continuous on $[a, b] .$ If $f$ is continuous at $x_{0}$ in $(a, b),$ then $F$ is differentiable at $x_{0}$ and $$ F^{\prime}\left(x_{0}\right)=f\left(x_{0}\right) $$ I'm not sure if I can use this theorem. Obviously, $f$ sattisfies the conditions. But as $y=\sin(x) \in [-1,1]$ , I dont see how I could choose $a,b$ in theorem 34.3, such that it fits this case. It seems clear that I should choose $a:=0$ , and if $y>0$ I could choose $b:=1$ , but if $y<0$ , I can't find any $b$ such that $y\in[a,b]$ . I wanted to know if I'm heading in the right direction, so I looked at the official solution manual. What they are doing there seems complete nonsense to me: I don't see why they are doing what they are doing. They end the proof with $G$ is continuous. Why is that result needed ? And isn't $f$ confused for (some undefined) $F$ in the beginning? As this proof didn't satisfy me, I looked further, and I found this proof: This proof seems completely solid to me. But it is much less intuitive (for me). I wouldn't have come with this proof myself. My question are: Was I heading in the right direction with my first idea ? Or is it not possible to apply theorem 34.3 in this kind of way ? Am I right that the official solution manual is complete nonsense, or am I missing something ? Is this last proof I found correct, and do you think the author of the book has this kind of proof in mind when he wrote this exercise ? Or do you think the author was expecting some other kind of proof ? Edit: For the bounty I would like to see a rigorous prove of why $G$ is differentiable on all $x\in \Bbb R$ using only theorems that are proven in Elementary Analysis: The Theory of Calculus by Kenneth A. Ross .","Let be a continuous function on and define Show that is differentiable on and compute . This is an exercise from Elementary Analysis: The Theory of Calculus by Kenneth A. Ross . My first idea is the following. Let . Define . And prove that is differentiable at using: It is clear that is differentiable at . So I need to proof that is differentiable at . I was trying to prove that using: 34.3 Fundamental Theorem of Calculus II. Let be an integrable function on For in let Then is continuous on If is continuous at in then is differentiable at and I'm not sure if I can use this theorem. Obviously, sattisfies the conditions. But as , I dont see how I could choose in theorem 34.3, such that it fits this case. It seems clear that I should choose , and if I could choose , but if , I can't find any such that . I wanted to know if I'm heading in the right direction, so I looked at the official solution manual. What they are doing there seems complete nonsense to me: I don't see why they are doing what they are doing. They end the proof with is continuous. Why is that result needed ? And isn't confused for (some undefined) in the beginning? As this proof didn't satisfy me, I looked further, and I found this proof: This proof seems completely solid to me. But it is much less intuitive (for me). I wouldn't have come with this proof myself. My question are: Was I heading in the right direction with my first idea ? Or is it not possible to apply theorem 34.3 in this kind of way ? Am I right that the official solution manual is complete nonsense, or am I missing something ? Is this last proof I found correct, and do you think the author of the book has this kind of proof in mind when he wrote this exercise ? Or do you think the author was expecting some other kind of proof ? Edit: For the bounty I would like to see a rigorous prove of why is differentiable on all using only theorems that are proven in Elementary Analysis: The Theory of Calculus by Kenneth A. Ross .","f \mathbb{R} G(x)=\int_0^{\sin (x)}f(t) dt  G \mathbb{R} G' x\in \Bbb R F(x):=\displaystyle\int_0^{x}f(t) dt  G:=F\circ \sin x \sin(x) x F y:=\sin(x) f [a, b] . x [a, b], 
F(x)=\int_{a}^{x} f(t) d t
 F [a, b] . f x_{0} (a, b), F x_{0} 
F^{\prime}\left(x_{0}\right)=f\left(x_{0}\right)
 f y=\sin(x) \in [-1,1] a,b a:=0 y>0 b:=1 y<0 b y\in[a,b] G f F G x\in \Bbb R","['real-analysis', 'solution-verification']"
42,Showing a set is countable or not.,Showing a set is countable or not.,,"Let $A=\{x\in\mathbb{R}:\forall n\in\mathbb{Z}^+,\lfloor x^n \rfloor \text{ is odd} \}$ where $\lfloor x \rfloor$ is the largest integer equal or less than $x$. Is $A$ countable?","Let $A=\{x\in\mathbb{R}:\forall n\in\mathbb{Z}^+,\lfloor x^n \rfloor \text{ is odd} \}$ where $\lfloor x \rfloor$ is the largest integer equal or less than $x$. Is $A$ countable?",,['real-analysis']
43,Schwartz Class Functions on Integers,Schwartz Class Functions on Integers,,"On $\mathbb{R}$, we define the Schwartz class functions as infinitely differentiable functions such that $$ \lim\limits_{|x|\to \infty} | x^{m}f^{(n)}(x) | = 0 $$ for all $m, n \in \mathbb{N}$ and $f^{(n)}$ denotes the $n^{th}$ derivative of $f$. However, I was asked to define Schwartz class on integers, and I came up with the following: $$ \mathcal{S}(\mathbb{Z}) = \{ f : \mathbb{Z} \to \mathbb{C} \;\; | \;\; \lim\limits_{|n| \to \infty} n^{k}f(n) = 0 \;\; \forall\,\, k \in \mathbb{N} \}$$ The first examples that come to my mind are the restrictions of Schwartz functions on $\mathbb{R}$ to $\mathbb{Z}$. For example, $f(n) = e^{-n^{2}}$ being the restriction of the Gaussian function. I was wondering if the converse is true. That is, given a function in $\mathcal{S}(\mathbb{Z})$, can it be extended to a function in $\mathcal{S}(\mathbb{R})$? I was thinking that we could smoothly interpolate the function in between the integers, where it is already defined. The question remains about the rapid decay of this function. However, since original function goes to zero rather faster, I expect this to go to zero fast enough as well, since the slopes cannot change erratically in this case. So, is the statement true? And if so, what is the justification?","On $\mathbb{R}$, we define the Schwartz class functions as infinitely differentiable functions such that $$ \lim\limits_{|x|\to \infty} | x^{m}f^{(n)}(x) | = 0 $$ for all $m, n \in \mathbb{N}$ and $f^{(n)}$ denotes the $n^{th}$ derivative of $f$. However, I was asked to define Schwartz class on integers, and I came up with the following: $$ \mathcal{S}(\mathbb{Z}) = \{ f : \mathbb{Z} \to \mathbb{C} \;\; | \;\; \lim\limits_{|n| \to \infty} n^{k}f(n) = 0 \;\; \forall\,\, k \in \mathbb{N} \}$$ The first examples that come to my mind are the restrictions of Schwartz functions on $\mathbb{R}$ to $\mathbb{Z}$. For example, $f(n) = e^{-n^{2}}$ being the restriction of the Gaussian function. I was wondering if the converse is true. That is, given a function in $\mathcal{S}(\mathbb{Z})$, can it be extended to a function in $\mathcal{S}(\mathbb{R})$? I was thinking that we could smoothly interpolate the function in between the integers, where it is already defined. The question remains about the rapid decay of this function. However, since original function goes to zero rather faster, I expect this to go to zero fast enough as well, since the slopes cannot change erratically in this case. So, is the statement true? And if so, what is the justification?",,['real-analysis']
44,Uniformly continuous function acts almost like an Lipschitz function?,Uniformly continuous function acts almost like an Lipschitz function?,,"Can anyone help?  Let $g:I \to \mathbb{R}$ be an uniformly continuous function, where $I$ is an interval. Prove that exists an constant $c$ that satisfies: $$\lvert g(x)-g(y)\rvert < 1 + c \lvert x-y \rvert, \forall x,y \in I$$","Can anyone help?  Let $g:I \to \mathbb{R}$ be an uniformly continuous function, where $I$ is an interval. Prove that exists an constant $c$ that satisfies: $$\lvert g(x)-g(y)\rvert < 1 + c \lvert x-y \rvert, \forall x,y \in I$$",,"['real-analysis', 'continuity']"
45,"Prove that there exists $t$ such that $0\le t\le T$ and $\int_0^Te^{-x}y'y''\,dx=\int_0^ty'y''\,dx$.",Prove that there exists  such that  and .,"t 0\le t\le T \int_0^Te^{-x}y'y''\,dx=\int_0^ty'y''\,dx","Let $y(x)$ be a solution to $y''+e^xy=0$. Prove that there exists $t$ such that $0\le t\le T$ and $$\int_0^Te^{-x} y'y'' \, dx=\int_0^ty'y''\,dx.$$","Let $y(x)$ be a solution to $y''+e^xy=0$. Prove that there exists $t$ such that $0\le t\le T$ and $$\int_0^Te^{-x} y'y'' \, dx=\int_0^ty'y''\,dx.$$",,"['calculus', 'real-analysis', 'integration', 'ordinary-differential-equations', 'contest-math']"
46,Outer measure of a union of 2 subsets of disjoint measurable sets of real numbers.,Outer measure of a union of 2 subsets of disjoint measurable sets of real numbers.,,"Every set mentioned is a subset of the real numbers. Let $m^*(C)$ denote the outer measure of a set $C$. Let $E$ be $any$ set and $A,B$ be measurable, disjoint sets. I'm trying to show that $$m^*(E\cap (A\cup B))=m^*(E\cap A)+m^*(E\cap B).$$ Proof:  ($\le$) follows by the countable subadditivity of the outer measure since $$E\cap (A\cup B)= (E\cap A)\cup (E\cap B).$$ Here's where I get stuck: ($\ge$) My attempts have reduced to something of the form: There are bounded open sets $G_1, G_2$ containing $E\cap A, E\cap     B$, respectively, such that $$m(G_1)\ge m^*(E\cap A),\quad m(G_2)\ge     m^*(E\cap B).$$ Hence $$m^*(E\cap A)+m^*(E\cap B)\le m(G_1)+m(G_2).$$ And I would like to extend this inequality to $m(G_1\cup G_2)$ but I know that's not even true, especially since the sets $G_1, G_2$ may not even be disjoint. I also tried de la Vallée-Poussin Criterion: Let $\epsilon>0$. Since $A, B$ are measurable, there are closed subsets $F_1, F_2$ of $A,B$ respectively, such that $m^*(A\cap E - F_1)+ m^*(B\cap     E-F_2)<\epsilon$. Even if I could show, $$|m^*(E\cap (A\cup     B)-[(F_1\cup F_2))+ m^*(A\cap E - F_1)+ m^*(B\cap     E-F_2)]|<\epsilon.$$ I'm not sure what that would mean. What I know: Measure has only been defined for bounded sets. A bounded set $A$ is $measurable$ if its outer and inner measures     are equal; if so, the measure of $A$ is the common value of these     measures. Differences, countable unions, countable intersections of measurable sets are measurable. The union of a set of pairwise disjoint measurable sets is measurable, with the measure of the union equal to the sum of the measures of the sets in the union. Outer and inner measures are monotone increasing functions. Countable subadditivity for outer measure, which states that if $A$ is a countable or finite union of sets $A_i$ then $m^*(A)\le \sum    m^*(A_i)$. De la Vallée-Poussin Criterion, which states that a bounded set $A$ is measurable iff for every $\epsilon >0$ there is a closed set $B\subset A$ such that $m^*(A-B)< \epsilon$. For any bounded set $B$, I can always find a set $C$ that is a countable intersection of open sets for which $B \subset C$ and $m^*(B)=m^*(C)$. If $A$ and $B$ are measurable sets, then $m(A\cup B) + m(A\cap B) =    m(A) + m(B)$. If $A$ is bounded and $I$ is an open interval containing $E$, then $m^*(E) + m_*(I-E) = m(I)$.","Every set mentioned is a subset of the real numbers. Let $m^*(C)$ denote the outer measure of a set $C$. Let $E$ be $any$ set and $A,B$ be measurable, disjoint sets. I'm trying to show that $$m^*(E\cap (A\cup B))=m^*(E\cap A)+m^*(E\cap B).$$ Proof:  ($\le$) follows by the countable subadditivity of the outer measure since $$E\cap (A\cup B)= (E\cap A)\cup (E\cap B).$$ Here's where I get stuck: ($\ge$) My attempts have reduced to something of the form: There are bounded open sets $G_1, G_2$ containing $E\cap A, E\cap     B$, respectively, such that $$m(G_1)\ge m^*(E\cap A),\quad m(G_2)\ge     m^*(E\cap B).$$ Hence $$m^*(E\cap A)+m^*(E\cap B)\le m(G_1)+m(G_2).$$ And I would like to extend this inequality to $m(G_1\cup G_2)$ but I know that's not even true, especially since the sets $G_1, G_2$ may not even be disjoint. I also tried de la Vallée-Poussin Criterion: Let $\epsilon>0$. Since $A, B$ are measurable, there are closed subsets $F_1, F_2$ of $A,B$ respectively, such that $m^*(A\cap E - F_1)+ m^*(B\cap     E-F_2)<\epsilon$. Even if I could show, $$|m^*(E\cap (A\cup     B)-[(F_1\cup F_2))+ m^*(A\cap E - F_1)+ m^*(B\cap     E-F_2)]|<\epsilon.$$ I'm not sure what that would mean. What I know: Measure has only been defined for bounded sets. A bounded set $A$ is $measurable$ if its outer and inner measures     are equal; if so, the measure of $A$ is the common value of these     measures. Differences, countable unions, countable intersections of measurable sets are measurable. The union of a set of pairwise disjoint measurable sets is measurable, with the measure of the union equal to the sum of the measures of the sets in the union. Outer and inner measures are monotone increasing functions. Countable subadditivity for outer measure, which states that if $A$ is a countable or finite union of sets $A_i$ then $m^*(A)\le \sum    m^*(A_i)$. De la Vallée-Poussin Criterion, which states that a bounded set $A$ is measurable iff for every $\epsilon >0$ there is a closed set $B\subset A$ such that $m^*(A-B)< \epsilon$. For any bounded set $B$, I can always find a set $C$ that is a countable intersection of open sets for which $B \subset C$ and $m^*(B)=m^*(C)$. If $A$ and $B$ are measurable sets, then $m(A\cup B) + m(A\cap B) =    m(A) + m(B)$. If $A$ is bounded and $I$ is an open interval containing $E$, then $m^*(E) + m_*(I-E) = m(I)$.",,"['real-analysis', 'measure-theory']"
47,Is the natural map $L^p(X) \otimes L^p(Y) \to L^p(X \times Y)$ injective?,Is the natural map  injective?,L^p(X) \otimes L^p(Y) \to L^p(X \times Y),"Let $X,Y$ be $\sigma$-finite measure spaces, and let $L^p(X) \otimes L^p(Y)$ be the algebraic tensor product.  The product has a natural map into $L^p(X \times Y)$ which takes $\sum a_{ij} f_i \otimes g_j$ to the function $F(x,y) = \sum a_{ij} f_i(x) g_j(y)$.  A moment's thought shows that this map is well-defined.  Is it also injective? It seems that this should be true, but I can't see how to prove it.  Intuitively, one needs to show that if $\sum a_{ij} f_i(x) g_j(y) = 0$ a.e., then one should be able to cancel all the terms in the sum using bilinearity.  It is not quite clear how to do this without knowing anything about the terms.","Let $X,Y$ be $\sigma$-finite measure spaces, and let $L^p(X) \otimes L^p(Y)$ be the algebraic tensor product.  The product has a natural map into $L^p(X \times Y)$ which takes $\sum a_{ij} f_i \otimes g_j$ to the function $F(x,y) = \sum a_{ij} f_i(x) g_j(y)$.  A moment's thought shows that this map is well-defined.  Is it also injective? It seems that this should be true, but I can't see how to prove it.  Intuitively, one needs to show that if $\sum a_{ij} f_i(x) g_j(y) = 0$ a.e., then one should be able to cancel all the terms in the sum using bilinearity.  It is not quite clear how to do this without knowing anything about the terms.",,"['linear-algebra', 'real-analysis', 'functional-analysis']"
48,Bauer's series for $\frac{1}{\pi}$,Bauer's series for,\frac{1}{\pi},"Recently, someone asked a question involving the expression $$ \sum_{n=0}^{\infty} (-1)^n (4n+1) \left(\frac{(2n-1)!!}{(2n)!!}\right)^3 $$ At first glance, I knew that the expression was the value of a hypergeometric function, and anticipated difficulty in determining its exact value. Surprisingly, the value obtained using Mathematica was $\frac{2}{\pi}$ . He couldn't recall the context in which the formula appeared well. (He mentioned seeing the expression in a comic book!) However, at this point, I suspect this may be a series studied by Ramanujan. Interestingly, when variables are introduced into the expression, as the following $$ \sum_{n=0}^{\infty} x^n (4n+1) \left(\frac{(2n-1)!!}{(2n)!!}\right)^3 $$ a very complicated hypergeometric function emerges, Nonetheless, Mathematica was unable to simplify the expression obtained by substituting $x=-1$ to revert to the original form. Therefore, I believe that the result might be derived from something like a complex contour integral or a similar method, and might not be readily analyzed as a special value of a hypergeometric function. Please help in understanding the reasoning behind the evaluation of this series.","Recently, someone asked a question involving the expression At first glance, I knew that the expression was the value of a hypergeometric function, and anticipated difficulty in determining its exact value. Surprisingly, the value obtained using Mathematica was . He couldn't recall the context in which the formula appeared well. (He mentioned seeing the expression in a comic book!) However, at this point, I suspect this may be a series studied by Ramanujan. Interestingly, when variables are introduced into the expression, as the following a very complicated hypergeometric function emerges, Nonetheless, Mathematica was unable to simplify the expression obtained by substituting to revert to the original form. Therefore, I believe that the result might be derived from something like a complex contour integral or a similar method, and might not be readily analyzed as a special value of a hypergeometric function. Please help in understanding the reasoning behind the evaluation of this series.", \sum_{n=0}^{\infty} (-1)^n (4n+1) \left(\frac{(2n-1)!!}{(2n)!!}\right)^3  \frac{2}{\pi}  \sum_{n=0}^{\infty} x^n (4n+1) \left(\frac{(2n-1)!!}{(2n)!!}\right)^3  x=-1,"['real-analysis', 'calculus', 'sequences-and-series', 'pi', 'elliptic-integrals']"
49,To what extent will Radon-Nikodym's theorem hold if we do not admit axiom of choice?,To what extent will Radon-Nikodym's theorem hold if we do not admit axiom of choice?,,"In the proof of the Radon-Nikodym theorem, the function $f = \frac{\mathrm{d} \nu}{\mathrm{d} \mu}$ is constructed as the supremum of measurable functions satisfying $\int_A f \mathrm{d} \mu \leq \int_A \mathrm{d}\nu$ , and the existence of this supremum is by Zorn's lemma, an equivalent statement of the axiom of choice (AC), which is not admitted by many mathematicians. So, does Radon-Nikodym's theorem still hold if we don't admit AC? And if it doesn't, then can we get a similar statement like Radon-Nikodym's theorem (probably weaker)?","In the proof of the Radon-Nikodym theorem, the function is constructed as the supremum of measurable functions satisfying , and the existence of this supremum is by Zorn's lemma, an equivalent statement of the axiom of choice (AC), which is not admitted by many mathematicians. So, does Radon-Nikodym's theorem still hold if we don't admit AC? And if it doesn't, then can we get a similar statement like Radon-Nikodym's theorem (probably weaker)?",f = \frac{\mathrm{d} \nu}{\mathrm{d} \mu} \int_A f \mathrm{d} \mu \leq \int_A \mathrm{d}\nu,"['real-analysis', 'probability-theory', 'measure-theory', 'axiom-of-choice']"
50,Upper bounds for $\frac{x_1}{1+x_1^2} + \frac{x_2}{1 + x_1^2 + x_2^2} + \cdots + \frac{x_n}{1 + x_1^2 + x_2^2 + \cdots + x_n^2}$,Upper bounds for,\frac{x_1}{1+x_1^2} + \frac{x_2}{1 + x_1^2 + x_2^2} + \cdots + \frac{x_n}{1 + x_1^2 + x_2^2 + \cdots + x_n^2},"Problem : Let $x_1, x_2, \cdots, x_n$ ( $n\ge 2$ ) be reals. Find upper bounds for $$\frac{x_1}{1+x_1^2} + \frac{x_2}{1 + x_1^2 + x_2^2} + \cdots + \frac{x_n}{1 + x_1^2 + x_2^2 + \cdots +  x_n^2}. $$ There is also the following Ji Chen's estimation (mentioned in the link below): $$\frac{x_1}{1+x_1^2}+\frac{x_2}{1+x_1^2+x_2^2}+\dotsb+\frac{x_n}{1+x_1^2+x_2^2+\dotsb+x_n^2}<\sqrt{n}-\dfrac{\ln{n}}{2\sqrt{n}}.\tag{1}$$ This is the follow up of Prove that $\frac{a}{1+a^2}+\frac{b}{1+a^2+b^2}+\frac{c}{1+a^2+b^2+c^2}+\frac{d}{1+a^2+b^2+c^2+d^2}\leq\frac{3}{2}$ . Question : How to prove the bound (1)? Can we obtain better upper bounds? Any comments and solutions are welcome and appreciated. Edit (2022/02/22): The problem can be rephrased as follows: Let $c_1 = 1/2$ and $c_{k + 1} = g(c_k), k \ge 1$ where $$g(c) = \frac18\sqrt{-2c^4 + 40c^2 + 16 + 2c(c^2 + 8)\sqrt{c^2 + 8}}.$$ Find the upper bounds of $c_n$ . Some bounds : IMO ShortList 2001, algebra problem 3, see: https://artofproblemsolving.com/community/c6h17449p119163 $$\frac{x_1}{1+x_1^2}+\frac{x_2}{1+x_1^2+x_2^2}+\dotsb+\frac{x_n}{1+x_1^2+x_2^2+\dotsb+x_n^2}<\sqrt{n}. \tag{2}$$ zhaobin@AoPS gave a very nice proof for (2): \begin{align*} &\mathrm{LHS}^2\\ \le\ & n\left(\frac{x_1^2}{(1+x_1^2)^2}+\frac{x_2^2}{(1+x_1^2+x_2^2)^2}+\dotsb+\frac{x_n^2}{(1+x_1^2+x_2^2+\dotsb+x_n^2)^2}\right)\\ \le\ & n\Big(\frac{x_1^2}{1\cdot (1+x_1^2)}+\frac{x_2^2}{(1+x_1^2)(1+x_1^2+x_2^2)}+ \frac{x_3^2}{(1+x_1^2+x_2^2)(1+x_1^2+x_2^2+x_3^2)}\dotsb\Big)\\ \le\ & n\Big(1 - \frac{1}{1+x_1^2} + \frac{1}{1+x_1^2} - \frac{1}{1+x_1^2+x_2^2} + \frac{1}{1+x_1^2+x_2^2} - \frac{1}{1+x_1^2+x_2^2+x_3^2}\cdots\Big)\\ \le\ & n\left(1 - \frac{1}{1+x_1^2+x_2^2 + \cdots + x_n^2}\right)\\ <\ & n. \end{align*} My attempt : I found the following relation: Let $y_k = \frac{x_{k+1}}{\sqrt{1+x_1^2}}, k = 1, 2, \cdots, n-1$ and we have $$\sum_{m=1}^n \frac{x_m}{1 + \sum_{k=1}^m x_k^2} = \frac{x_1}{1 + x_1^2} + \left(\sum_{m=1}^{n-1} \frac{y_m}{1 + \sum_{k=1}^m y_k^2}\right)\frac{1}{\sqrt{1+x_1^2}}.$$ By this, if we have $\sum_{m=1}^{n-1} \frac{x_m}{1 + \sum_{k=1}^m x_k^2} \le F(n-1)$ on $\mathbb{R}^{n-1}$ for some function $F(\cdot)$ , then we have $\sum_{m=1}^n \frac{x_m}{1 + \sum_{k=1}^m x_k^2} \le g(F(n-1))$ on $\mathbb{R}^n$ where $$g(c) \triangleq \max_{x\in \mathbb{R}} \frac{x}{1+x^2} + c \frac{1}{\sqrt{1+x^2}}.$$ Remark: $g(c)$ admits a closed form: $$g(c) = f\left(\sqrt{\tfrac{2}{c^2 + 2 + c\sqrt{c^2 + 8}}},\ c\right) = \frac{\sqrt{\frac{2}{c\, \sqrt{c^2 + 8} + c^2 + 2}}}{1 + \frac{2}{c\, \sqrt{c^2 + 8} + c^2 + 2}} + \frac{c}{\sqrt{1 + \frac{2}{c\, \sqrt{c^2 + 8} + c^2 + 2}}}$$ where $f(x, y) = \frac{x}{1+x^2} + \frac{y}{\sqrt{1+x^2}}$ . We immediately have the following results: The maximum of $\sum_{m=1}^n \frac{x_m}{1 + \sum_{k=1}^m x_k^2}$ is given by $$\underbrace{g\circ g \circ \cdots \circ g}_{n-1} \left(\frac{1}{2}\right).$$ Indeed, denote the maximum of $\sum_{m=1}^n \frac{x_m}{1 + \sum_{k=1}^m x_k^2}$ by $M(n)$ , and we have $M(n) = g(M(n-1))$ . Also, $M(1)$ is equal to the maximum of $\frac{x_1}{1+x_1^2}$ which is $1/2$ . The desired result follows. A upper bound for $\sum_{m=1}^n \frac{x_m}{1 + \sum_{k=1}^m x_k^2}$ is $\sqrt{n}$ . We use mathematical induction. When $n=1$ , it is true. Assume $\sum_{m=1}^n \frac{x_m}{1 + \sum_{k=1}^m x_k^2} < \sqrt{n}$ . We need to prove that $g(\sqrt{n}) < \sqrt{n+1}$ . It is true. Similarly, we can obtain $\sum_{m=1}^n \frac{x_m}{1 + \sum_{k=1}^m x_k^2} \le \sqrt{n} - \frac{1}{2\sqrt{n}}$ . However, this does not work for $\sum_{m=1}^n \frac{x_m}{1 + \sum_{k=1}^m x_k^2} < \sqrt{n}-\frac{\ln{n}}{2\sqrt{n}}$ since $g(\sqrt{n}-\frac{\ln{n}}{2\sqrt{n}}) < \sqrt{n+1}-\frac{\ln{n+1}}{2\sqrt{n+1}}$ is not true.","Problem : Let ( ) be reals. Find upper bounds for There is also the following Ji Chen's estimation (mentioned in the link below): This is the follow up of Prove that $\frac{a}{1+a^2}+\frac{b}{1+a^2+b^2}+\frac{c}{1+a^2+b^2+c^2}+\frac{d}{1+a^2+b^2+c^2+d^2}\leq\frac{3}{2}$ . Question : How to prove the bound (1)? Can we obtain better upper bounds? Any comments and solutions are welcome and appreciated. Edit (2022/02/22): The problem can be rephrased as follows: Let and where Find the upper bounds of . Some bounds : IMO ShortList 2001, algebra problem 3, see: https://artofproblemsolving.com/community/c6h17449p119163 zhaobin@AoPS gave a very nice proof for (2): My attempt : I found the following relation: Let and we have By this, if we have on for some function , then we have on where Remark: admits a closed form: where . We immediately have the following results: The maximum of is given by Indeed, denote the maximum of by , and we have . Also, is equal to the maximum of which is . The desired result follows. A upper bound for is . We use mathematical induction. When , it is true. Assume . We need to prove that . It is true. Similarly, we can obtain . However, this does not work for since is not true.","x_1, x_2, \cdots, x_n n\ge 2 \frac{x_1}{1+x_1^2} + \frac{x_2}{1 + x_1^2 + x_2^2} + \cdots + \frac{x_n}{1 + x_1^2 + x_2^2 + \cdots +  x_n^2}.  \frac{x_1}{1+x_1^2}+\frac{x_2}{1+x_1^2+x_2^2}+\dotsb+\frac{x_n}{1+x_1^2+x_2^2+\dotsb+x_n^2}<\sqrt{n}-\dfrac{\ln{n}}{2\sqrt{n}}.\tag{1} c_1 = 1/2 c_{k + 1} = g(c_k), k \ge 1 g(c) = \frac18\sqrt{-2c^4 + 40c^2 + 16 + 2c(c^2 + 8)\sqrt{c^2 + 8}}. c_n \frac{x_1}{1+x_1^2}+\frac{x_2}{1+x_1^2+x_2^2}+\dotsb+\frac{x_n}{1+x_1^2+x_2^2+\dotsb+x_n^2}<\sqrt{n}. \tag{2} \begin{align*}
&\mathrm{LHS}^2\\
\le\ & n\left(\frac{x_1^2}{(1+x_1^2)^2}+\frac{x_2^2}{(1+x_1^2+x_2^2)^2}+\dotsb+\frac{x_n^2}{(1+x_1^2+x_2^2+\dotsb+x_n^2)^2}\right)\\
\le\ & n\Big(\frac{x_1^2}{1\cdot (1+x_1^2)}+\frac{x_2^2}{(1+x_1^2)(1+x_1^2+x_2^2)}+ \frac{x_3^2}{(1+x_1^2+x_2^2)(1+x_1^2+x_2^2+x_3^2)}\dotsb\Big)\\
\le\ & n\Big(1 - \frac{1}{1+x_1^2} + \frac{1}{1+x_1^2} - \frac{1}{1+x_1^2+x_2^2} + \frac{1}{1+x_1^2+x_2^2} - \frac{1}{1+x_1^2+x_2^2+x_3^2}\cdots\Big)\\
\le\ & n\left(1 - \frac{1}{1+x_1^2+x_2^2 + \cdots + x_n^2}\right)\\
<\ & n.
\end{align*} y_k = \frac{x_{k+1}}{\sqrt{1+x_1^2}}, k = 1, 2, \cdots, n-1 \sum_{m=1}^n \frac{x_m}{1 + \sum_{k=1}^m x_k^2} = \frac{x_1}{1 + x_1^2} + \left(\sum_{m=1}^{n-1} \frac{y_m}{1 + \sum_{k=1}^m y_k^2}\right)\frac{1}{\sqrt{1+x_1^2}}. \sum_{m=1}^{n-1} \frac{x_m}{1 + \sum_{k=1}^m x_k^2} \le F(n-1) \mathbb{R}^{n-1} F(\cdot) \sum_{m=1}^n \frac{x_m}{1 + \sum_{k=1}^m x_k^2} \le g(F(n-1)) \mathbb{R}^n g(c) \triangleq \max_{x\in \mathbb{R}} \frac{x}{1+x^2} + c \frac{1}{\sqrt{1+x^2}}. g(c) g(c) = f\left(\sqrt{\tfrac{2}{c^2 + 2 + c\sqrt{c^2 + 8}}},\ c\right)
= \frac{\sqrt{\frac{2}{c\, \sqrt{c^2 + 8} + c^2 + 2}}}{1 + \frac{2}{c\, \sqrt{c^2 + 8} + c^2 + 2}} + \frac{c}{\sqrt{1 + \frac{2}{c\, \sqrt{c^2 + 8} + c^2 + 2}}} f(x, y) = \frac{x}{1+x^2} + \frac{y}{\sqrt{1+x^2}} \sum_{m=1}^n \frac{x_m}{1 + \sum_{k=1}^m x_k^2} \underbrace{g\circ g \circ \cdots \circ g}_{n-1} \left(\frac{1}{2}\right). \sum_{m=1}^n \frac{x_m}{1 + \sum_{k=1}^m x_k^2} M(n) M(n) = g(M(n-1)) M(1) \frac{x_1}{1+x_1^2} 1/2 \sum_{m=1}^n \frac{x_m}{1 + \sum_{k=1}^m x_k^2} \sqrt{n} n=1 \sum_{m=1}^n \frac{x_m}{1 + \sum_{k=1}^m x_k^2} < \sqrt{n} g(\sqrt{n}) < \sqrt{n+1} \sum_{m=1}^n \frac{x_m}{1 + \sum_{k=1}^m x_k^2} \le \sqrt{n} - \frac{1}{2\sqrt{n}} \sum_{m=1}^n \frac{x_m}{1 + \sum_{k=1}^m x_k^2} < \sqrt{n}-\frac{\ln{n}}{2\sqrt{n}} g(\sqrt{n}-\frac{\ln{n}}{2\sqrt{n}}) < \sqrt{n+1}-\frac{\ln{n+1}}{2\sqrt{n+1}}","['real-analysis', 'inequality', 'summation', 'upper-lower-bounds']"
51,"Prove that $e^n\bmod 1$ is dense in $[0,1]$",Prove that  is dense in,"e^n\bmod 1 [0,1]","I just noticed that I have left unanswered one part of an old multi-part question and so decided to re-ask it separately: Consider the sequence $e^n\bmod 1$ , $n\in\Bbb N$ . Show that it is dense in $[0,1]$ . This apparently does require specific (approximation?) properties of $e$ , as for example replacing $e$ with any integer leads to a non-dense sequence. On the other hand, for every sequence of numbers $a_n\in(0,1)$ , it is not hard to find $\alpha$ such that $|\alpha^{2^n}\bmod 1- a_n|<\frac1n$ for all $n$ , or $\beta$ such that $|\beta^n\bmod 1-a_n|<\frac1{1000}$ . Hence there exist (irrational) bases that lead to a dense sequence and others that lead to a non-dense sequence. Other than that I'm a bit at a dead end.","I just noticed that I have left unanswered one part of an old multi-part question and so decided to re-ask it separately: Consider the sequence , . Show that it is dense in . This apparently does require specific (approximation?) properties of , as for example replacing with any integer leads to a non-dense sequence. On the other hand, for every sequence of numbers , it is not hard to find such that for all , or such that . Hence there exist (irrational) bases that lead to a dense sequence and others that lead to a non-dense sequence. Other than that I'm a bit at a dead end.","e^n\bmod 1 n\in\Bbb N [0,1] e e a_n\in(0,1) \alpha |\alpha^{2^n}\bmod 1- a_n|<\frac1n n \beta |\beta^n\bmod 1-a_n|<\frac1{1000}",['real-analysis']
52,Lebesgue - Radon - Nikodym Theorem: Question about $\sigma$-finite case,Lebesgue - Radon - Nikodym Theorem: Question about -finite case,\sigma,"Lebesgue Radon Nikodym Theorem Let $\nu$ be a $\sigma$ -finite signed measure on $(X,\mathcal{A})$ and $\mu$ a $\sigma$ -finite positive measure on $(X,\mathcal{A})$ There exist unique $\sigma$ -finite signed measures $\rho,\lambda$ on $(X,\mathcal{A})$ such that $$\nu=\rho+\lambda\qquad \rho \ll\mu,\qquad\lambda \perp \mu.$$ There exists an extended $\mu$ -integrable function $f$ such that $d\rho=f\,d\mu$ i.e. $$\nu=f\,d\mu+\lambda$$ If we also have $\nu=\tilde{f}\,d\mu+\lambda$ where $\tilde{f}$ is an extended $\mu$ -integrable function, then $$\tilde{f}=f\quad\mu\text{-a.e}$$ Proof. Case 1 Suppose first that $\mu$ and $\nu$ are both finite, positive measures. On this first step I have no problems Case 2 Suppose that $\mu$ , $\nu$ are both $\sigma$ -finite positive measure. We can write $$X=\bigcup_l E_j\quad\text{and}\quad X=\bigcup_k F_k,$$ with $\mu(E_j)<\infty$ , $\nu(F_k)<\infty.$ Then $$X=\bigcup_{j,k}(E_j\cap F_k)=\bigcup_l A_l$$ disjointly with $\mu(A_l), \nu(A_l)<\infty.$ Define $$\mu_k(E)=\mu(E\cap A_k)\quad \nu_k(E)=\nu(E\cap A_k),$$ so by case 1. we can write $\nu_k=\rho_k+\lambda_k$ for some unique measures with $\rho_k \ll \mu_k$ and $\lambda_k\perp \mu_k.$ Note that $$\mu_k(A_k^c)=\mu(A_k^c\cap A_k)=0,$$ so $A_k^c$ is a $\mu_k-$ null set. Therefore $$f^{'}_k=f_k\chi_{A_k}$$ equals $f_k$ $\mu_k-$ a.e, so we can replace $f_k$ with $f^{'}_k$ without changing $\lambda_k$ or $\rho_k.$ In other words, we  can assume that $f_k(x)=0$ $\forall x\notin A_k.$ Since the $A_k$ are disjoint, we can therefore define $$f=\sum_{k=1}^\infty f_k.$$ Since $f\ge 0$ , $$d\rho=f\,d\mu$$ defines a positive measure. Also, $$\lambda=\sum_{k=1}^\infty \lambda_k$$ is a positive measure, since each $\lambda_k\ge 0$ Question I'm trying and trying again to show the following but I can't: $\lambda, \rho$ are $\sigma$ -finite; $\nu=\rho+\lambda$ ; $\rho \ll \mu$ ; $\lambda\perp \mu$ ; The uniqueness statements hold. Could you please give me some suggestions on the basis of what I have already shown?","Lebesgue Radon Nikodym Theorem Let be a -finite signed measure on and a -finite positive measure on There exist unique -finite signed measures on such that There exists an extended -integrable function such that i.e. If we also have where is an extended -integrable function, then Proof. Case 1 Suppose first that and are both finite, positive measures. On this first step I have no problems Case 2 Suppose that , are both -finite positive measure. We can write with , Then disjointly with Define so by case 1. we can write for some unique measures with and Note that so is a null set. Therefore equals a.e, so we can replace with without changing or In other words, we  can assume that Since the are disjoint, we can therefore define Since , defines a positive measure. Also, is a positive measure, since each Question I'm trying and trying again to show the following but I can't: are -finite; ; ; ; The uniqueness statements hold. Could you please give me some suggestions on the basis of what I have already shown?","\nu \sigma (X,\mathcal{A}) \mu \sigma (X,\mathcal{A}) \sigma \rho,\lambda (X,\mathcal{A}) \nu=\rho+\lambda\qquad \rho \ll\mu,\qquad\lambda \perp \mu. \mu f d\rho=f\,d\mu \nu=f\,d\mu+\lambda \nu=\tilde{f}\,d\mu+\lambda \tilde{f} \mu \tilde{f}=f\quad\mu\text{-a.e} \mu \nu \mu \nu \sigma X=\bigcup_l E_j\quad\text{and}\quad X=\bigcup_k F_k, \mu(E_j)<\infty \nu(F_k)<\infty. X=\bigcup_{j,k}(E_j\cap F_k)=\bigcup_l A_l \mu(A_l), \nu(A_l)<\infty. \mu_k(E)=\mu(E\cap A_k)\quad \nu_k(E)=\nu(E\cap A_k), \nu_k=\rho_k+\lambda_k \rho_k \ll \mu_k \lambda_k\perp \mu_k. \mu_k(A_k^c)=\mu(A_k^c\cap A_k)=0, A_k^c \mu_k- f^{'}_k=f_k\chi_{A_k} f_k \mu_k- f_k f^{'}_k \lambda_k \rho_k. f_k(x)=0 \forall x\notin A_k. A_k f=\sum_{k=1}^\infty f_k. f\ge 0 d\rho=f\,d\mu \lambda=\sum_{k=1}^\infty \lambda_k \lambda_k\ge 0 \lambda, \rho \sigma \nu=\rho+\lambda \rho \ll \mu \lambda\perp \mu","['real-analysis', 'measure-theory', 'proof-writing', 'solution-verification']"
53,"Can a function $f:\mathbb{R} \rightarrow \mathbb{R}$ be ""infinitely steep"" on a set with non-zero Lebesgue outer measure?","Can a function  be ""infinitely steep"" on a set with non-zero Lebesgue outer measure?",f:\mathbb{R} \rightarrow \mathbb{R},"By being ""infinitely steep"" on a set I mean that for each point $x$ in the set we have $\sup\limits_{\delta>0}\text{ }\inf\limits_{y\in(x-\delta,\text{ }x+\delta)\backslash\{x\}}\frac{f(y)-f(x)}{y-x}=\infty$ . As a remark, the Cantor function is infinitely steep on the Cantor-Set which is uncountable, but of course has Lebesgue measure $0$ .","By being ""infinitely steep"" on a set I mean that for each point in the set we have . As a remark, the Cantor function is infinitely steep on the Cantor-Set which is uncountable, but of course has Lebesgue measure .","x \sup\limits_{\delta>0}\text{ }\inf\limits_{y\in(x-\delta,\text{ }x+\delta)\backslash\{x\}}\frac{f(y)-f(x)}{y-x}=\infty 0",['real-analysis']
54,Is the condition sufficient?,Is the condition sufficient?,,"I am stuck on the following problem: Let $f:\mathbb{R} \to \mathbb{R}$ be continuous and $2\pi$-periodic and $n$ be a positive integer. If for any integer $p \in [0,n-1]$, $$\int_{0}^{2\pi} f(t) \cos(pt)\,\mathrm{d}t=\int_{0}^{2\pi} f(t) \sin(pt)\,\mathrm{d}t=0,$$ then is it true that $f$ has at least $2n$ roots on $[0,2\pi]$? I tried to prove the problem using induction. $p=0$ is easy , but $p=1$ is giving me a hard time. Edit: After the comment of Dear Dunham , it's seems that we need both integral to be equal to zero , in other to work , in that case  we have the form :  $$ \int_{0}^{2\pi} f(t) e^{ipt} \mathrm{d}t=0$$   ,  Now it looks like the trigonometric version of Prove that $f$ has $m+1$ zeros if $\int_{a}^{b} x^nf(x)dx=0$ for all $n\le m$ Edit: I made some progress with the new version of the question in the case $f$ is not identicaly $0$ , and proved $M \geq n$ , where $M$ is the number of zeros of $f$ as follow : Let $(T_{n})$ the sequence of Tchebychev polynomial , $\cos(nt)=T_{n}(\cos(t))$ and $\deg(T_{n})=n$ . hence $(T_{0},...,T_{n-1})$ is a basis of $R_{n-1}[X]$ ,$0\leq a_{1}<a_{2}<... <a_{m}\leq \pi \leq a_{m+1}<..<a_{p}$ the zeros of $f$ and suppose that $p\leq n-1$  . For $i \in [1,m-1]$ such that $a_{i}<x<a_{i+1}$ since cosinus is decreasing in  $[0,\pi[$ then $b_{i+1}=\cos(a_{i+1}) < \cos(x) < b_{i}=\cos(a_{i})$  , on the other hand, for $j \in [m ,p-1]$ we have $c_{j}=\cos(a_{j}) < \cos(x) < c_{j+1}=\cos(a_{j+1})$  since here cosine is increasing in $[\pi ,2 \pi]$.  We have then $(d_{n})$ constructed from $(b_{n})$ and $(c_{n})$ such that :  $\forall x \in [0,2\pi]-J :  f(x)  \Pi_{i=1}^{p} (\cos(x)-d_{i}) > 0$ where  $J=\{a_{1},a_{2},....,a_{p}, d_{1},...,d_{p}\}$ , the polynomial $P(x)=\Pi_{i=1}^{p}(x-d_{i})$ is of degree at most $n-1$ , hence we have $(t_{i})$ such that:  $P(\cos(t))=\sum_{i=1}^{n-1} t_{i}T_{i}(\cos(t))$ by linearity of the integral, we conclude that : $\int_{0}^{2\pi} f(t)P(t)=0$ hence $f(x)=0$ For any  $x \in [0,2\pi]-J$ , hence by continuity $f=0$ every where, contradiction . I feel that I can improve this solution , by a better choice of the sequence of polynomial , one that also include the orthoganility with the familly $\sin(pt)$ $p \in [[0,n-1]]$ this information is critical to improve the bound , any help ? thank you a lot .","I am stuck on the following problem: Let $f:\mathbb{R} \to \mathbb{R}$ be continuous and $2\pi$-periodic and $n$ be a positive integer. If for any integer $p \in [0,n-1]$, $$\int_{0}^{2\pi} f(t) \cos(pt)\,\mathrm{d}t=\int_{0}^{2\pi} f(t) \sin(pt)\,\mathrm{d}t=0,$$ then is it true that $f$ has at least $2n$ roots on $[0,2\pi]$? I tried to prove the problem using induction. $p=0$ is easy , but $p=1$ is giving me a hard time. Edit: After the comment of Dear Dunham , it's seems that we need both integral to be equal to zero , in other to work , in that case  we have the form :  $$ \int_{0}^{2\pi} f(t) e^{ipt} \mathrm{d}t=0$$   ,  Now it looks like the trigonometric version of Prove that $f$ has $m+1$ zeros if $\int_{a}^{b} x^nf(x)dx=0$ for all $n\le m$ Edit: I made some progress with the new version of the question in the case $f$ is not identicaly $0$ , and proved $M \geq n$ , where $M$ is the number of zeros of $f$ as follow : Let $(T_{n})$ the sequence of Tchebychev polynomial , $\cos(nt)=T_{n}(\cos(t))$ and $\deg(T_{n})=n$ . hence $(T_{0},...,T_{n-1})$ is a basis of $R_{n-1}[X]$ ,$0\leq a_{1}<a_{2}<... <a_{m}\leq \pi \leq a_{m+1}<..<a_{p}$ the zeros of $f$ and suppose that $p\leq n-1$  . For $i \in [1,m-1]$ such that $a_{i}<x<a_{i+1}$ since cosinus is decreasing in  $[0,\pi[$ then $b_{i+1}=\cos(a_{i+1}) < \cos(x) < b_{i}=\cos(a_{i})$  , on the other hand, for $j \in [m ,p-1]$ we have $c_{j}=\cos(a_{j}) < \cos(x) < c_{j+1}=\cos(a_{j+1})$  since here cosine is increasing in $[\pi ,2 \pi]$.  We have then $(d_{n})$ constructed from $(b_{n})$ and $(c_{n})$ such that :  $\forall x \in [0,2\pi]-J :  f(x)  \Pi_{i=1}^{p} (\cos(x)-d_{i}) > 0$ where  $J=\{a_{1},a_{2},....,a_{p}, d_{1},...,d_{p}\}$ , the polynomial $P(x)=\Pi_{i=1}^{p}(x-d_{i})$ is of degree at most $n-1$ , hence we have $(t_{i})$ such that:  $P(\cos(t))=\sum_{i=1}^{n-1} t_{i}T_{i}(\cos(t))$ by linearity of the integral, we conclude that : $\int_{0}^{2\pi} f(t)P(t)=0$ hence $f(x)=0$ For any  $x \in [0,2\pi]-J$ , hence by continuity $f=0$ every where, contradiction . I feel that I can improve this solution , by a better choice of the sequence of polynomial , one that also include the orthoganility with the familly $\sin(pt)$ $p \in [[0,n-1]]$ this information is critical to improve the bound , any help ? thank you a lot .",,"['real-analysis', 'integration', 'periodic-functions']"
55,Generalization of Dirichlet integral,Generalization of Dirichlet integral,,"Is it correct that $$\lim_{L\to\infty} \int_0^L d x \int_0^L dy \int_0^L d z \frac{\sin(x+y)}{x+y}\frac{\sin(y+z)}{y+z}\frac{\sin(z+x)}{z+x} =\frac {\pi^3}{16}. $$ or does anybody has a reference for this? The value $\frac{\pi^3}{16}$ comes from numerics.  The identity $$ \lim_{L\to\infty}\int_0^L \frac{\sin(x)}{x} = \frac \pi 2 $$ (which is sometimes also called Dirichlet integral) is well known and there are many ways to prove it. However, I need the slight generalization of this above.","Is it correct that $$\lim_{L\to\infty} \int_0^L d x \int_0^L dy \int_0^L d z \frac{\sin(x+y)}{x+y}\frac{\sin(y+z)}{y+z}\frac{\sin(z+x)}{z+x} =\frac {\pi^3}{16}. $$ or does anybody has a reference for this? The value $\frac{\pi^3}{16}$ comes from numerics.  The identity $$ \lim_{L\to\infty}\int_0^L \frac{\sin(x)}{x} = \frac \pi 2 $$ (which is sometimes also called Dirichlet integral) is well known and there are many ways to prove it. However, I need the slight generalization of this above.",,"['real-analysis', 'integration']"
56,"The sequence $a_1 = \frac{1}{2}, a_2 = 1, a_{n+1} = \frac{na_n+1}{a_{n-1}+n}$ is decreasing",The sequence  is decreasing,"a_1 = \frac{1}{2}, a_2 = 1, a_{n+1} = \frac{na_n+1}{a_{n-1}+n}","Consider the sequence $\{a_n\}$ defined by $$a_1 = \frac{1}{2}, a_2 = 1, a_{n+1} = \frac{na_n+1}{a_{n-1}+n}, \forall n\ge 2.$$   Prove that $\{a_n\}_{n\ge 3}$ is decreasing. I get the first $200$ values of $\{a_n\}$ and recognize this fact, but I cannot prove it. Thank you very much.","Consider the sequence $\{a_n\}$ defined by $$a_1 = \frac{1}{2}, a_2 = 1, a_{n+1} = \frac{na_n+1}{a_{n-1}+n}, \forall n\ge 2.$$   Prove that $\{a_n\}_{n\ge 3}$ is decreasing. I get the first $200$ values of $\{a_n\}$ and recognize this fact, but I cannot prove it. Thank you very much.",,"['real-analysis', 'sequences-and-series']"
57,Regarding the sum $\sum_{p \ \text{prime}} \sin p$,Regarding the sum,\sum_{p \ \text{prime}} \sin p,"I'm very confident that  $$\sum_{p \ \text{prime}} \sin p $$ diverges. Of course, it suffices to show that there are arbitrarily large primes which are not in the set $\bigcup_{n \geq 1} (\pi n - \epsilon, \pi n + \epsilon)$ for sufficiently small $\epsilon$. More strongly, it seems that $\sin p$ for prime $p$ is dense in $[-1,1]$. This problem doesn't seem that hard though. Here's something that (to me) seems harder. If $p_n$ is the nth prime, what is $$\limsup_{n \to +\infty} \sum_{p \ \text{prime} \leq p_n} \sin p?$$   What is $$\sup_{n \in \mathbb{N}} \sum_{p \ \text{prime} \leq p_n} \sin p? $$ Of course, we can ask analogous questions for $\inf$. I'm happy with partial answers or ideas. For example, merely an upper bound.","I'm very confident that  $$\sum_{p \ \text{prime}} \sin p $$ diverges. Of course, it suffices to show that there are arbitrarily large primes which are not in the set $\bigcup_{n \geq 1} (\pi n - \epsilon, \pi n + \epsilon)$ for sufficiently small $\epsilon$. More strongly, it seems that $\sin p$ for prime $p$ is dense in $[-1,1]$. This problem doesn't seem that hard though. Here's something that (to me) seems harder. If $p_n$ is the nth prime, what is $$\limsup_{n \to +\infty} \sum_{p \ \text{prime} \leq p_n} \sin p?$$   What is $$\sup_{n \in \mathbb{N}} \sum_{p \ \text{prime} \leq p_n} \sin p? $$ Of course, we can ask analogous questions for $\inf$. I'm happy with partial answers or ideas. For example, merely an upper bound.",,"['calculus', 'real-analysis', 'sequences-and-series']"
58,Reverse Stolz-Cesaro Theorem,Reverse Stolz-Cesaro Theorem,,"One form of Stolz-Cesaro that is well known states: If $(a_n)$ and $(b_n)$ are real sequences such that $b_n \uparrow  \infty$ and $(a_{n+1} - a_n)/(b_{n+1} - b_n)$ converges as $n \to  \infty$, then $$ \lim_{n \to \infty} \frac{a_{n+1} - a_n}{b_{n+1} - b_n} = L \in  \mathbb{R} \cup\{\pm \infty\} \,\, \implies \,\, \lim_{n \to  \infty}\frac{a_n}{b_n} =L$$ It is perhaps not so well known that a reverse implication is true when an additional condition is imposed: $$ \lim_{n \to \infty}\frac{b_n}{b_{n+1}} = B \in \mathbb{R} \setminus \{1\} , \,\,\,\,\, \lim_{n \to \infty}\frac{a_n}{b_n} = L \,\, \implies \,\,\lim_{n \to \infty} \frac{a_{n+1} - a_n}{b_{n+1} - b_n} = L $$ For a proof note that $$\frac{a_{n+1} - a_n}{b_{n+1} - b_n} \left(1 - \frac{b_n}{b_{n+1}} \right) = \frac{a_{n+1}}{b_{n+1}} - \frac{a_n}{b_{n+1}} = \frac{a_{n+1}}{b_{n+1}} - \frac{a_n}{b_{n}} \frac{b_n}{b_{n+1}}$$ and, hence, $$\frac{a_{n+1} - a_n}{b_{n+1} - b_n} =  \left(1 - \frac{b_n}{b_{n+1}} \right)^{-1} \left(\frac{a_{n+1}}{b_{n+1}} - \frac{a_n}{b_{n}} \frac{b_n}{b_{n+1}} \right)$$ If $B \neq 1$, the limit on the RHS exists and $$\lim_{n \to \infty} \frac{a_{n+1} - a_n}{b_{n+1} - b_n} = (1- B)^{-1}(L - L B) = L$$ If $B = 1$, then counterexamples to the reverse implication are readily found. The question is what additional conditions are needed for the reverse Stolz -Cesaro theorem to hold in the case where $B = 1$?","One form of Stolz-Cesaro that is well known states: If $(a_n)$ and $(b_n)$ are real sequences such that $b_n \uparrow  \infty$ and $(a_{n+1} - a_n)/(b_{n+1} - b_n)$ converges as $n \to  \infty$, then $$ \lim_{n \to \infty} \frac{a_{n+1} - a_n}{b_{n+1} - b_n} = L \in  \mathbb{R} \cup\{\pm \infty\} \,\, \implies \,\, \lim_{n \to  \infty}\frac{a_n}{b_n} =L$$ It is perhaps not so well known that a reverse implication is true when an additional condition is imposed: $$ \lim_{n \to \infty}\frac{b_n}{b_{n+1}} = B \in \mathbb{R} \setminus \{1\} , \,\,\,\,\, \lim_{n \to \infty}\frac{a_n}{b_n} = L \,\, \implies \,\,\lim_{n \to \infty} \frac{a_{n+1} - a_n}{b_{n+1} - b_n} = L $$ For a proof note that $$\frac{a_{n+1} - a_n}{b_{n+1} - b_n} \left(1 - \frac{b_n}{b_{n+1}} \right) = \frac{a_{n+1}}{b_{n+1}} - \frac{a_n}{b_{n+1}} = \frac{a_{n+1}}{b_{n+1}} - \frac{a_n}{b_{n}} \frac{b_n}{b_{n+1}}$$ and, hence, $$\frac{a_{n+1} - a_n}{b_{n+1} - b_n} =  \left(1 - \frac{b_n}{b_{n+1}} \right)^{-1} \left(\frac{a_{n+1}}{b_{n+1}} - \frac{a_n}{b_{n}} \frac{b_n}{b_{n+1}} \right)$$ If $B \neq 1$, the limit on the RHS exists and $$\lim_{n \to \infty} \frac{a_{n+1} - a_n}{b_{n+1} - b_n} = (1- B)^{-1}(L - L B) = L$$ If $B = 1$, then counterexamples to the reverse implication are readily found. The question is what additional conditions are needed for the reverse Stolz -Cesaro theorem to hold in the case where $B = 1$?",,"['real-analysis', 'sequences-and-series', 'limits']"
59,How are these definitions of the limit superior and limit inferior equivalent?,How are these definitions of the limit superior and limit inferior equivalent?,,"I have come across these three definitions of the limit superior (or upper limit) and the limit inferior (or lower limit) of a sequence of real numbers and I wonder how to establish the equivalence of these. Walter Rudin: PRINCIPLES OF MATHEMATICAL ANALYSIS, 3rd edition:  Definition 3.16: Given a sequence $\{s_n\}$ of real numbers, let $E$ be the set of numbers $x$ (in the extended real number system) such that $s_{n_k} \to x$ for some subsequence $\{s_{n_k}\}$. This set contains all subsequential limits ..., plus possibly the numbers $+\infty$, $-\infty$. We now ... put  $$s^{*} = \sup E,$$ $$s_{*} = \inf E.$$ The numbers $s^{*}$, $s_{*}$ are called the upper and lower limits of $\{s_n\}$; we use the notation  $$ \lim_{n \to \infty} \sup s_n = s^{*}, \, \, \, \lim_{n \to \infty} \inf s_n = s_{*}.$$ Tom M. Apostol: MATHEMATICAL ANALYSIS, 2nd edition:  Sec. 8.3: Definition 8.2: Let $\{a_n\}$ be a sequence of real numbers. Suppose there is a real number $U$ satisfying the following conditions: i} For every $\epsilon > 0$ there exists an integer $N$ such that $n > N$ implies $$a_n < U + \epsilon.$$ ii) Given $\epsilon > 0$ and given $m > 0$, there exists an integer $n> m$ such that $$a_n > U - \epsilon.$$ Then $U$ is called the limit superior (or upper limit) of $\{a_n\}$, and we write $$U = \lim_{n \to \infty} \sup a_n.$$ Statement (i) implies that the set $\{a_1, a_2, a_3, \ldots \}$ is bounded above. If this set is not bounded above, we define $$\lim_{n\to\infty}\sup a_n = +\infty.$$ If the set is bounded above but not bounded below and if $\{a_n\}$ has no finite limit superior, then we say $\lim \sup_{n\to\infty} a_n = -\infty$. The limit inferior (or lower limit) of $\{a_n\}$ is defined as follows: $$\lim_{n\to\infty}\inf a_n = -\lim_{n\to\infty}\sup b_n,$$ where $b_n = -a_n$ for $n= 1, 2, 3, \ldots$. Robert G. Bartle and Donald R. Sherbert: INTRODUCTION TO REAL ANALYSIS, 3rd edition: Exercises for Section 3.3: Problem 10: Let $(x_n)$ be a bounded sequence of real numbers, and for each $n\in \mathbb{N}$ let $s_n \colon= \sup \{x_k \colon k \geq n\}$ and let $t_n \colon= \inf \{x_k \colon k \geq n\}$. Prove that $(s_n)$ and $(t_n)$ are monotone and convergent. Also prove that if $\lim (s_n) = \lim (t_n)$, then $(x_n)$ is convergent. [One calls $\lim (s_n)$ the limit superior of $(x_n)$ and $\lim (t_n)$ the limit inferior of $(x_n)$.] Now how can one show that the above three definitions are equivalent (i.e. these three definitions are of the same pair of numbers)?","I have come across these three definitions of the limit superior (or upper limit) and the limit inferior (or lower limit) of a sequence of real numbers and I wonder how to establish the equivalence of these. Walter Rudin: PRINCIPLES OF MATHEMATICAL ANALYSIS, 3rd edition:  Definition 3.16: Given a sequence $\{s_n\}$ of real numbers, let $E$ be the set of numbers $x$ (in the extended real number system) such that $s_{n_k} \to x$ for some subsequence $\{s_{n_k}\}$. This set contains all subsequential limits ..., plus possibly the numbers $+\infty$, $-\infty$. We now ... put  $$s^{*} = \sup E,$$ $$s_{*} = \inf E.$$ The numbers $s^{*}$, $s_{*}$ are called the upper and lower limits of $\{s_n\}$; we use the notation  $$ \lim_{n \to \infty} \sup s_n = s^{*}, \, \, \, \lim_{n \to \infty} \inf s_n = s_{*}.$$ Tom M. Apostol: MATHEMATICAL ANALYSIS, 2nd edition:  Sec. 8.3: Definition 8.2: Let $\{a_n\}$ be a sequence of real numbers. Suppose there is a real number $U$ satisfying the following conditions: i} For every $\epsilon > 0$ there exists an integer $N$ such that $n > N$ implies $$a_n < U + \epsilon.$$ ii) Given $\epsilon > 0$ and given $m > 0$, there exists an integer $n> m$ such that $$a_n > U - \epsilon.$$ Then $U$ is called the limit superior (or upper limit) of $\{a_n\}$, and we write $$U = \lim_{n \to \infty} \sup a_n.$$ Statement (i) implies that the set $\{a_1, a_2, a_3, \ldots \}$ is bounded above. If this set is not bounded above, we define $$\lim_{n\to\infty}\sup a_n = +\infty.$$ If the set is bounded above but not bounded below and if $\{a_n\}$ has no finite limit superior, then we say $\lim \sup_{n\to\infty} a_n = -\infty$. The limit inferior (or lower limit) of $\{a_n\}$ is defined as follows: $$\lim_{n\to\infty}\inf a_n = -\lim_{n\to\infty}\sup b_n,$$ where $b_n = -a_n$ for $n= 1, 2, 3, \ldots$. Robert G. Bartle and Donald R. Sherbert: INTRODUCTION TO REAL ANALYSIS, 3rd edition: Exercises for Section 3.3: Problem 10: Let $(x_n)$ be a bounded sequence of real numbers, and for each $n\in \mathbb{N}$ let $s_n \colon= \sup \{x_k \colon k \geq n\}$ and let $t_n \colon= \inf \{x_k \colon k \geq n\}$. Prove that $(s_n)$ and $(t_n)$ are monotone and convergent. Also prove that if $\lim (s_n) = \lim (t_n)$, then $(x_n)$ is convergent. [One calls $\lim (s_n)$ the limit superior of $(x_n)$ and $\lim (t_n)$ the limit inferior of $(x_n)$.] Now how can one show that the above three definitions are equivalent (i.e. these three definitions are of the same pair of numbers)?",,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis', 'limits']"
60,Second derivative of $f(f(\cdots f(x)\cdots )?$,Second derivative of,f(f(\cdots f(x)\cdots )?,"For convenience, let's write $f_n(x)=f(f(\cdots f(x)\cdots )$ where   $f$ is iterated $n$ times. Suppose: $$f(0)=0,\quad f'(0)=\alpha,\quad f''(0)=\beta$$   What is $f''_n(0)?$ I've found $f'_n(0),$ using the chain rule: $$f'_n(x)=\prod_{k=1}^n f'(f_{k-1}(x))\Rightarrow f'_n(0)=\prod_{k=1}^n f'(0)=\alpha^n$$ But I'm stuck on $f_n''(0)$. I am sure there is a pattern, but I can't spot it.","For convenience, let's write $f_n(x)=f(f(\cdots f(x)\cdots )$ where   $f$ is iterated $n$ times. Suppose: $$f(0)=0,\quad f'(0)=\alpha,\quad f''(0)=\beta$$   What is $f''_n(0)?$ I've found $f'_n(0),$ using the chain rule: $$f'_n(x)=\prod_{k=1}^n f'(f_{k-1}(x))\Rightarrow f'_n(0)=\prod_{k=1}^n f'(0)=\alpha^n$$ But I'm stuck on $f_n''(0)$. I am sure there is a pattern, but I can't spot it.",,"['calculus', 'real-analysis', 'analysis', 'derivatives']"
61,Integral $I=\int_0^\infty \frac{\ln(1+x) \operatorname{Li}_2 (-x)}{x^{3/2}} dx$,Integral,I=\int_0^\infty \frac{\ln(1+x) \operatorname{Li}_2 (-x)}{x^{3/2}} dx,"Hello can you please help me solve this integral $$ \int_0^\infty \frac{\ln(1+x) \operatorname{Li}_2 (-x)}{x^{3/2}} dx=-\frac{2\pi}{3}(\pi^2+24\ln 2). $$ I am trying to work through all logarithmic integrals. Note, the Polylogarithm function is given by $\operatorname{Li}_2(-x)$ and is defined by $$ \operatorname{Li}_2(-x)=\sum_{k=1}^\infty \frac{(-x)^k}{k^2}, \ |-x|<1 $$ and can be extended using analytical continuation for $|-x|>1$ .  We also know that $$ \frac{d}{dx} \operatorname{Li}_2(-x)=-\frac{\ln(1+x)}{x}. $$ Thanks!","Hello can you please help me solve this integral I am trying to work through all logarithmic integrals. Note, the Polylogarithm function is given by and is defined by and can be extended using analytical continuation for .  We also know that Thanks!","
\int_0^\infty \frac{\ln(1+x) \operatorname{Li}_2 (-x)}{x^{3/2}} dx=-\frac{2\pi}{3}(\pi^2+24\ln 2).
 \operatorname{Li}_2(-x) 
\operatorname{Li}_2(-x)=\sum_{k=1}^\infty \frac{(-x)^k}{k^2}, \ |-x|<1
 |-x|>1 
\frac{d}{dx} \operatorname{Li}_2(-x)=-\frac{\ln(1+x)}{x}.
","['real-analysis', 'integration', 'definite-integrals', 'special-functions', 'contour-integration']"
62,Show that convolution of two measurable functions is well-defined,Show that convolution of two measurable functions is well-defined,,"Question : Recall the definition of the convolution of $f$ and $g$ given by   $$(f*g)(x)=\int_{\mathbb{R}^d}f(x-y)g(y)dy.$$ If  we only know that   $f$ and $g$ are measurable, can we show that $f*g$ is well defined for   a.e. $x$, that is, $f(x-y)g(y)$ is integrable? (Exercise 2.5.21(c) in 'Real Analysis', by Stein and Shakarchi) Actually, the book writes like this: Suppose that $f$ and $g$ are measurable functions on $\mathbb{R}^d$. (a)Prove that $f(x-y)g(y)$ is measurable on $\mathbb{R}^{2d}$. (b)Show that if $f$ and $g$ are integrable on $\mathbb{R}^d$, then   $f(x-y)g(y)$ is integrable on $\mathbb{R}^{2d}$. (c)Recall the definition of the convolution of $f$ and $g$ given by   $$(f*g)(x)=\int_{\mathbb{R}^d}f(x-y)g(y)dy$$ Show that $f*g$ is well   defined for a.e. $x$, that is, $f(x-y)g(y)$ is integrable. Can we use the assumption that $f,g$ are integrable in (c)?","Question : Recall the definition of the convolution of $f$ and $g$ given by   $$(f*g)(x)=\int_{\mathbb{R}^d}f(x-y)g(y)dy.$$ If  we only know that   $f$ and $g$ are measurable, can we show that $f*g$ is well defined for   a.e. $x$, that is, $f(x-y)g(y)$ is integrable? (Exercise 2.5.21(c) in 'Real Analysis', by Stein and Shakarchi) Actually, the book writes like this: Suppose that $f$ and $g$ are measurable functions on $\mathbb{R}^d$. (a)Prove that $f(x-y)g(y)$ is measurable on $\mathbb{R}^{2d}$. (b)Show that if $f$ and $g$ are integrable on $\mathbb{R}^d$, then   $f(x-y)g(y)$ is integrable on $\mathbb{R}^{2d}$. (c)Recall the definition of the convolution of $f$ and $g$ given by   $$(f*g)(x)=\int_{\mathbb{R}^d}f(x-y)g(y)dy$$ Show that $f*g$ is well   defined for a.e. $x$, that is, $f(x-y)g(y)$ is integrable. Can we use the assumption that $f,g$ are integrable in (c)?",,"['real-analysis', 'measure-theory']"
63,Continuous unbounded but integrable functions,Continuous unbounded but integrable functions,,"Many tricky exercises concern the quest for functions that satisfy particular conditions. For example, let us consider the spaces $C_p( \mathbb R), 1 \leq p < \infty$, of continuous functions on $\mathbb R$ such that $\int_{\mathbb R} \lvert f(x) \rvert^p \mbox{d}x < \infty$. I found the following exercises rather demanding: Find a function $f\in C_1(\mathbb R)$ such that $f$ is unbounded; Find a function $f \in C_1(\mathbb R) \backslash C_2(\mathbb R)$. so one can deduce that $C_p \not\subset C_q$ if $p < q$. I'm quite sure that a function defined to be zero everywhere except for triangular peaks of height $k$ and base $1/k^3$ centered on positive integers $k$ is a solution for both the exercises. (it's a simple matter to give to this function an explicit form, but I think it would be rather unclear.) Is it correct? Can anyone give other examples? (for the one or the other exercise, not necessary a solution of both of them!)","Many tricky exercises concern the quest for functions that satisfy particular conditions. For example, let us consider the spaces $C_p( \mathbb R), 1 \leq p < \infty$, of continuous functions on $\mathbb R$ such that $\int_{\mathbb R} \lvert f(x) \rvert^p \mbox{d}x < \infty$. I found the following exercises rather demanding: Find a function $f\in C_1(\mathbb R)$ such that $f$ is unbounded; Find a function $f \in C_1(\mathbb R) \backslash C_2(\mathbb R)$. so one can deduce that $C_p \not\subset C_q$ if $p < q$. I'm quite sure that a function defined to be zero everywhere except for triangular peaks of height $k$ and base $1/k^3$ centered on positive integers $k$ is a solution for both the exercises. (it's a simple matter to give to this function an explicit form, but I think it would be rather unclear.) Is it correct? Can anyone give other examples? (for the one or the other exercise, not necessary a solution of both of them!)",,"['real-analysis', 'functional-analysis', 'functions']"
64,Is every positive real number the limit of a sequence of ratios obtained from a double partition of $ \mathbb{N} $?,Is every positive real number the limit of a sequence of ratios obtained from a double partition of ?, \mathbb{N} ,"Let $ \{ A,B \} $ be a partition of $ \mathbb{N} $ into two infinite subsets. For every $ r \in \mathbb{R}_{> 0} $, can we find an increasing sequence $ (a_{n})_{n \in \mathbb{N}} $ in $ A $ and an increasing sequence $ (b_{n})_{n \in \mathbb{N}} $ in $ B $ such that $ \displaystyle \lim_{n \rightarrow \infty} \frac{a_{n}}{b_{n}} = r $?","Let $ \{ A,B \} $ be a partition of $ \mathbb{N} $ into two infinite subsets. For every $ r \in \mathbb{R}_{> 0} $, can we find an increasing sequence $ (a_{n})_{n \in \mathbb{N}} $ in $ A $ and an increasing sequence $ (b_{n})_{n \in \mathbb{N}} $ in $ B $ such that $ \displaystyle \lim_{n \rightarrow \infty} \frac{a_{n}}{b_{n}} = r $?",,['real-analysis']
65,Subset of $\mathbb{Q}$,Subset of,\mathbb{Q},"Let $S= \{x_0,\dots,x_n\}$ be a finite subset of $[0,1]$ , $x_0=0$ and $x_1=1$ such that every distance between pair of elements of $S$ occurs at least twice, except for the distance $1$, then we are to show that $S$ is a subset of $\mathbb{Q}$.","Let $S= \{x_0,\dots,x_n\}$ be a finite subset of $[0,1]$ , $x_0=0$ and $x_1=1$ such that every distance between pair of elements of $S$ occurs at least twice, except for the distance $1$, then we are to show that $S$ is a subset of $\mathbb{Q}$.",,['real-analysis']
66,How to prove that the Cantor ternary function is not weakly differentiable?,How to prove that the Cantor ternary function is not weakly differentiable?,,"I am using the standard cantor ternary function $f$ here, as cited in this Wikipedia page . It is an example of continuous, monotone increasing, but not strictly monotone increasing function with zero derivative almost everywhere. But how should I prove that its weak/distributional derivatives do not exist? I guess I start of with assuming that there exist $ g \in L^1_\text{loc}(R)$ such that $\int_R {f\phi'} = - \int_R{g\phi}$ for all $\phi\in C_c^\infty (R)$. And then I have to probably choose appropriate mollifiers $\phi_\epsilon$ and let $\epsilon \to 0$. But I am kind of stuck here; could you give me a detailed proof? Also, is the derivative of $f$ a measure in the distributional sense? Thank you !","I am using the standard cantor ternary function $f$ here, as cited in this Wikipedia page . It is an example of continuous, monotone increasing, but not strictly monotone increasing function with zero derivative almost everywhere. But how should I prove that its weak/distributional derivatives do not exist? I guess I start of with assuming that there exist $ g \in L^1_\text{loc}(R)$ such that $\int_R {f\phi'} = - \int_R{g\phi}$ for all $\phi\in C_c^\infty (R)$. And then I have to probably choose appropriate mollifiers $\phi_\epsilon$ and let $\epsilon \to 0$. But I am kind of stuck here; could you give me a detailed proof? Also, is the derivative of $f$ a measure in the distributional sense? Thank you !",,"['real-analysis', 'distribution-theory']"
67,Complex analysis or real analysis books that have these special functions.,Complex analysis or real analysis books that have these special functions.,,"I saw an integral question that involved the digamma function, which I know nothing about, and I want to learn more about it, its properties, and other functions like the polylogarithm function and the hypergeometric function. So, I searched through all of my analysis books (Differential Equations, Real Analysis, Complex Analysis) to find any book that mentions the digamma function, but none of them do. The only books I have that contain the digamma function are problem books, which assume the reader is familiar with it and present hard problems involving it, making them not useful for me. The other type is special functions books, which don’t contain any proofs, so they are also not useful. I know that I can search for the digamma function on Wikipedia, but I am more interested in finding an analysis book that contains the digamma function and its properties because such a book might have more interesting theories and functions that I know nothing about, like the polylogarithm function, the hypergeometric function, or even functions and theorems I didn't know existed. If there are no analysis books with these functions and the other types of books (problem books, special functions books) cannot be relied on, then how did many people learn about these functions anyway? I don't think the people learnt these functions from articles so they must have used some analysis book to learn them. So, I want to ask for analysis books (complex or real, but preferably complex analysis) that include the digamma function, the polylogarithm function, and the hypergeometric function, along with their properties and proofs. Additionally, I am looking for rigorous books that do not contain any real-world applications or physics.","I saw an integral question that involved the digamma function, which I know nothing about, and I want to learn more about it, its properties, and other functions like the polylogarithm function and the hypergeometric function. So, I searched through all of my analysis books (Differential Equations, Real Analysis, Complex Analysis) to find any book that mentions the digamma function, but none of them do. The only books I have that contain the digamma function are problem books, which assume the reader is familiar with it and present hard problems involving it, making them not useful for me. The other type is special functions books, which don’t contain any proofs, so they are also not useful. I know that I can search for the digamma function on Wikipedia, but I am more interested in finding an analysis book that contains the digamma function and its properties because such a book might have more interesting theories and functions that I know nothing about, like the polylogarithm function, the hypergeometric function, or even functions and theorems I didn't know existed. If there are no analysis books with these functions and the other types of books (problem books, special functions books) cannot be relied on, then how did many people learn about these functions anyway? I don't think the people learnt these functions from articles so they must have used some analysis book to learn them. So, I want to ask for analysis books (complex or real, but preferably complex analysis) that include the digamma function, the polylogarithm function, and the hypergeometric function, along with their properties and proofs. Additionally, I am looking for rigorous books that do not contain any real-world applications or physics.",,"['real-analysis', 'complex-analysis', 'analysis', 'special-functions', 'book-recommendation']"
68,"Evaluating $\int_{0}^{1}\frac{\arctan(x)}{x^2+x+1}\,dx$",Evaluating,"\int_{0}^{1}\frac{\arctan(x)}{x^2+x+1}\,dx","As in the title I was evaluating the integral, $$I:=\int_{0}^{1}\frac{\arctan(x)}{x^2+x+1}\,dx$$ But numerically speaking, I seemed to have gotten an incorrect answer. What have I done wrong? Here is my work. Let $$x\longrightarrow{\frac{1-x}{1+x}}$$ $$I=2\int_{0}^{1}\frac{\frac{\pi}{4}-\arctan(x)}{3+x^2}\,dx$$ $I.B.P$ $$I=\frac{\pi^2}{12}\left(1-\frac{1}{\sqrt{3}}\right)+\frac{1}{\sqrt{3}}\int_{0}^{1}\frac{\arctan\left(\frac{x}{\sqrt{3}}\right)}{1+x^2}\,dx$$ Let $$x\longrightarrow{\tan(x)}$$ $$I=\frac{\pi^2}{12}\left(1-\frac{1}{\sqrt{3}}\right)+\frac{1}{\sqrt{3}}J$$ $$J:=\int_{0}^{\frac{\pi}{4}}\arctan\left(\frac{\tan(x)}{\sqrt{3}}\right)\,dx$$ To evaluate $J$ define: $$J(t):=\int_{0}^{\frac{\pi}{4}}\arctan\left(t\tan(x)\right)\,dx$$ We can see that: $$J(0)=0,J\left(\frac{1}{\sqrt{3}}\right)=J$$ Now it follows that from the $F.T.C.$ $$J=\int_{0}^{\frac{1}{\sqrt{3}}}J’(t)\,dt$$ And so: $$J’(t)=\int_{0}^{\frac{\pi}{4}}\frac{\tan(x)}{t^2\tan^2(x)+1}\,dx$$ Multiply the numerator and denominator by $\sec(x)$ then let: $$\sec(x)\longrightarrow{x}$$ $$J’(t)=\int_{1}^{\sqrt{2}}\frac{1}{x(t^2x^2+1-t^2)}\,dx$$ $$=\frac{1}{1-t^2}\left[\int_{1}^{\sqrt{2}}\frac{1}{x}\,dx-\int_{1}^{\sqrt{2}}\frac{t^2x}{t^2x^2+1-t^2}\,dx\right]$$ $$J’(t)=\frac{1}{1-t^2}\frac{1}{2}\left[\log(2)-\log(1+t^2)\right]$$ And so $$J=\frac{1}{2}\int_{0}^{\frac{1}{\sqrt{3}}}\frac{1}{1-t^2}\left[\log(2)-\log(1+t^2)\right]\,dt$$ Let $$t=\frac{1-x}{1+x}$$ $$J=\frac{1}{4}\int_{2-\sqrt{3}}^{1}\frac{\log\left(\frac{(1+x)^2}{1+x^2}\right)}{x}\,dx$$ $$=\frac{1}{4}\left[2\int_{2-\sqrt{3}}^{1}\frac{\log(1+x)}{x}\,dx-\int_{2-\sqrt{3}}^{1}\frac{\log(x+i)}{x}\,dx-\int_{2-\sqrt{3}}^{1}\frac{\log(x-i)}{x}\,dx\right]$$ Now using $$\int\frac{\log(x+a)}{x}\,dx=\log(a)\log(x)-Li_{2}\left(-\frac{x}{a}\right)$$ $$=-\frac{1}{4}\left[2Li_{2}(-x)+Li_{2}(ix)+Li_{2}(-ix)\right]_{2-\sqrt{3}}^{1}$$ Using: $$Li_{2}(z)+Li_{2}(-z)=\frac{1}{2}Li_{2}(z^2)$$ We have $$=-\frac{1}{4}\left[2Li_{2}(-x)+\frac{1}{2}Li_{2}(-x^2)\right]_{2-\sqrt{3}}^{1}$$ And so $$J=\frac{5\pi^2}{96}+\frac{1}{4}\left[2Li_{2}(\sqrt{3}-2)+\frac{1}{2}Li_{2}(4\sqrt{3}-7)\right]$$ And so we have that $$I=\frac{\pi^2}{12}-\frac{7\pi^2}{96\sqrt{3}}+\frac{1}{4\sqrt{3}}\left[2Li_{2}(\sqrt{3}-3)+\frac{1}{2}Li_{2}(4\sqrt{3}-7)\right]$$ Where did I how wrong? Any help is appreciated.","As in the title I was evaluating the integral, But numerically speaking, I seemed to have gotten an incorrect answer. What have I done wrong? Here is my work. Let Let To evaluate define: We can see that: Now it follows that from the And so: Multiply the numerator and denominator by then let: And so Let Now using Using: We have And so And so we have that Where did I how wrong? Any help is appreciated.","I:=\int_{0}^{1}\frac{\arctan(x)}{x^2+x+1}\,dx x\longrightarrow{\frac{1-x}{1+x}} I=2\int_{0}^{1}\frac{\frac{\pi}{4}-\arctan(x)}{3+x^2}\,dx I.B.P I=\frac{\pi^2}{12}\left(1-\frac{1}{\sqrt{3}}\right)+\frac{1}{\sqrt{3}}\int_{0}^{1}\frac{\arctan\left(\frac{x}{\sqrt{3}}\right)}{1+x^2}\,dx x\longrightarrow{\tan(x)} I=\frac{\pi^2}{12}\left(1-\frac{1}{\sqrt{3}}\right)+\frac{1}{\sqrt{3}}J J:=\int_{0}^{\frac{\pi}{4}}\arctan\left(\frac{\tan(x)}{\sqrt{3}}\right)\,dx J J(t):=\int_{0}^{\frac{\pi}{4}}\arctan\left(t\tan(x)\right)\,dx J(0)=0,J\left(\frac{1}{\sqrt{3}}\right)=J F.T.C. J=\int_{0}^{\frac{1}{\sqrt{3}}}J’(t)\,dt J’(t)=\int_{0}^{\frac{\pi}{4}}\frac{\tan(x)}{t^2\tan^2(x)+1}\,dx \sec(x) \sec(x)\longrightarrow{x} J’(t)=\int_{1}^{\sqrt{2}}\frac{1}{x(t^2x^2+1-t^2)}\,dx =\frac{1}{1-t^2}\left[\int_{1}^{\sqrt{2}}\frac{1}{x}\,dx-\int_{1}^{\sqrt{2}}\frac{t^2x}{t^2x^2+1-t^2}\,dx\right] J’(t)=\frac{1}{1-t^2}\frac{1}{2}\left[\log(2)-\log(1+t^2)\right] J=\frac{1}{2}\int_{0}^{\frac{1}{\sqrt{3}}}\frac{1}{1-t^2}\left[\log(2)-\log(1+t^2)\right]\,dt t=\frac{1-x}{1+x} J=\frac{1}{4}\int_{2-\sqrt{3}}^{1}\frac{\log\left(\frac{(1+x)^2}{1+x^2}\right)}{x}\,dx =\frac{1}{4}\left[2\int_{2-\sqrt{3}}^{1}\frac{\log(1+x)}{x}\,dx-\int_{2-\sqrt{3}}^{1}\frac{\log(x+i)}{x}\,dx-\int_{2-\sqrt{3}}^{1}\frac{\log(x-i)}{x}\,dx\right] \int\frac{\log(x+a)}{x}\,dx=\log(a)\log(x)-Li_{2}\left(-\frac{x}{a}\right) =-\frac{1}{4}\left[2Li_{2}(-x)+Li_{2}(ix)+Li_{2}(-ix)\right]_{2-\sqrt{3}}^{1} Li_{2}(z)+Li_{2}(-z)=\frac{1}{2}Li_{2}(z^2) =-\frac{1}{4}\left[2Li_{2}(-x)+\frac{1}{2}Li_{2}(-x^2)\right]_{2-\sqrt{3}}^{1} J=\frac{5\pi^2}{96}+\frac{1}{4}\left[2Li_{2}(\sqrt{3}-2)+\frac{1}{2}Li_{2}(4\sqrt{3}-7)\right] I=\frac{\pi^2}{12}-\frac{7\pi^2}{96\sqrt{3}}+\frac{1}{4\sqrt{3}}\left[2Li_{2}(\sqrt{3}-3)+\frac{1}{2}Li_{2}(4\sqrt{3}-7)\right]","['real-analysis', 'integration', 'solution-verification', 'definite-integrals']"
69,A unit speed differentiable curve which avoids a null set of directions,A unit speed differentiable curve which avoids a null set of directions,,"For each $x \in \mathbb{R}^2$ , let $N_x$ be a null set of the unit circle $S^1$ (with respect to $1$ -dimensional Lebesgue measure). Further given that for each $u \in S^1$ , we have that $u \not \in N_x$ for almost all $x \in \mathbb{R}^2$ . Does there necessarily exist a unit speed differentiable curve $\gamma : (-\varepsilon, \varepsilon) \rightarrow \mathbb{R}^2$ such that: $\gamma'(t) \not \in N_{\gamma(t)}$ for all $t \in (-\varepsilon,\varepsilon)$ ?","For each , let be a null set of the unit circle (with respect to -dimensional Lebesgue measure). Further given that for each , we have that for almost all . Does there necessarily exist a unit speed differentiable curve such that: for all ?","x \in \mathbb{R}^2 N_x S^1 1 u \in S^1 u \not \in N_x x \in \mathbb{R}^2 \gamma : (-\varepsilon, \varepsilon) \rightarrow \mathbb{R}^2 \gamma'(t) \not \in N_{\gamma(t)} t \in (-\varepsilon,\varepsilon)","['real-analysis', 'measure-theory', 'differential-geometry']"
70,Proving $\lim_{n\rightarrow\infty}\sum_{k=1}^n (-1)^{k-1}{n\choose k}\frac{k}{2^k-1}=\frac{1}{\ln2}$,Proving,\lim_{n\rightarrow\infty}\sum_{k=1}^n (-1)^{k-1}{n\choose k}\frac{k}{2^k-1}=\frac{1}{\ln2},"Can someone help with this sum? Prove $$S=\lim_{n\rightarrow\infty}\sum_{k=1}^n (-1)^{k-1}{n\choose k}\frac{k}{2^k-1}=\frac{1}{\ln2}$$ I have tried to break down the $\frac{k}{2^k-1}=\sum_{j=1}^\infty \frac{k}{2^{kj}}$ and tried writing it as $$\lim_{n\rightarrow\infty}\sum_{j=1}^\infty\sum_{k=1}^n (-1)^{k-1}{n\choose k}k\left(\frac{1}{2^{j}}\right)^k,$$ then using the binomial summation as such, note $\left(x=\frac{1}{2^j}\right)$ $$\lim_{n\rightarrow\infty}\sum_{j=1}^\infty\frac{1}{2^j}\frac{\mathrm{d}}{\mathrm{d}x}\left(\sum_{k=1}^n (-1)^{k-1}{n\choose k}x^{k}\right)=\lim_{n\rightarrow\infty}\sum_{j=1}^\infty\frac{n(1-x)^{n-1}}{2^j}.$$ Giving us the final sum to be $$\lim_{n\rightarrow\infty}\sum_{j=1}^\infty\frac{n\left(1-\frac{1}{2^j}\right)^{n-1}}{2^j},$$ the limit of which seems to be $0$ . Which is not what we want, can someone tell me where my mistakes lie, and also point towards the solution or present it themselves. Thanks in Advance.","Can someone help with this sum? Prove I have tried to break down the and tried writing it as then using the binomial summation as such, note Giving us the final sum to be the limit of which seems to be . Which is not what we want, can someone tell me where my mistakes lie, and also point towards the solution or present it themselves. Thanks in Advance.","S=\lim_{n\rightarrow\infty}\sum_{k=1}^n (-1)^{k-1}{n\choose k}\frac{k}{2^k-1}=\frac{1}{\ln2} \frac{k}{2^k-1}=\sum_{j=1}^\infty \frac{k}{2^{kj}} \lim_{n\rightarrow\infty}\sum_{j=1}^\infty\sum_{k=1}^n (-1)^{k-1}{n\choose k}k\left(\frac{1}{2^{j}}\right)^k, \left(x=\frac{1}{2^j}\right) \lim_{n\rightarrow\infty}\sum_{j=1}^\infty\frac{1}{2^j}\frac{\mathrm{d}}{\mathrm{d}x}\left(\sum_{k=1}^n (-1)^{k-1}{n\choose k}x^{k}\right)=\lim_{n\rightarrow\infty}\sum_{j=1}^\infty\frac{n(1-x)^{n-1}}{2^j}. \lim_{n\rightarrow\infty}\sum_{j=1}^\infty\frac{n\left(1-\frac{1}{2^j}\right)^{n-1}}{2^j}, 0","['real-analysis', 'sequences-and-series']"
71,Length of a union of intervals,Length of a union of intervals,,"Let $X$ be a subset of $[0,1]$ with length (Lebesgue measure) $1$ . For each $x\in X$ , there is some $\epsilon_x > 0$ . Define $I_x$ as the open interval $(x, x+\epsilon_x)$ . I am interested in the union of all these intervals: $$U := \bigcup_{x\in X} I_x$$ Initially I thought that $U$ contains the entire unit interval, but this is not true. For example, it is possible that for each $x<0.5$ , $\epsilon_X := (0.5 - x)/2$ . Then, $0.5 \not \in U$ . My quesion is: is the length of $U$ always at least $1$ ?","Let be a subset of with length (Lebesgue measure) . For each , there is some . Define as the open interval . I am interested in the union of all these intervals: Initially I thought that contains the entire unit interval, but this is not true. For example, it is possible that for each , . Then, . My quesion is: is the length of always at least ?","X [0,1] 1 x\in X \epsilon_x > 0 I_x (x, x+\epsilon_x) U := \bigcup_{x\in X} I_x U x<0.5 \epsilon_X := (0.5 - x)/2 0.5 \not \in U U 1","['real-analysis', 'measure-theory']"
72,How to show the derivative by using the limit definition?,How to show the derivative by using the limit definition?,,"Let $(\Omega, \mathcal{F}, \mu)$ be a probability space. For the sake of simplicity, let $\Omega$ be $\mathbb{R}$. In what follows, the measurability always refers to Borel measurability. Let $f \colon \mathbb{R}_+ \times \Omega \to \mathbb{R}_+$ be a function such that: (i) For each $z \in \Omega$, the function $k \mapsto f(k,z)$ is concave, increasing, and continuously differentiable, while $z \mapsto f(k,z)$ is Borel measurable for each $k \in \mathbb{R}_+$; (ii) $\lim_{k \downarrow 0} f'(k,z) >0$ for each $z \in \Omega$. Here and below, $f'(k,z)$ denotes the partial derivative of $f$ with respect to $k$; and (iii) $f(0,z)=0$ for all $z \in \Omega$. Let $v \colon \mathbb{R}_+ \to \mathbb{R}_+$ be a bounded, strictly concave and strictly increasing function, and be continuously differentiable on $(0, \infty)$. Define a function $g $  by \begin{align*} g(k) := \left( \int_{\Omega} \left[ v\left( f( k, z ) \right) \right]^\alpha \mu (\mathrm{d}z) \right)^{1/\alpha}, \qquad (0<\alpha <1). \end{align*} Question: In fact, since $g$ is concave (it has been proved), we know that the right-hand and the left-hand derivatives of $g$ exist. I aim to show that $g$ is differentiable on $(0, \eta)$ for any fixed constant $\eta >0$, and to show the derivative of $g$ which I conjecture is   \begin{align*} g'(k) =  \left( \int_{\Omega} \left[ v\left( f( k, z ) \right) \right]^\alpha \mu (\mathrm{d}z) \right)^{\frac{1}{\alpha} -1 }   \int_{\Omega} \left[ v\left( f( k, z ) \right) \right]^{\alpha -1}  v'\left( f( k, z ) \right) f'( k, z ) \mu( \mathrm{d} z) \end{align*}   for all $0 < k < \eta$. Here, $g'(k) = \dfrac{\mathrm{d}}{\mathrm{d} k} g(k)$, $v'\left( f( k, z ) \right) := \dfrac{\mathrm{d}}{\mathrm{d} f }v( f( k, z ))$, and $f'(k, z) := \dfrac{\partial}{\partial k} f(k,z)$. My attempt: The above stated derivative of $g$ is just my conjecture and I am not sure if the right-hand derivative of $g$ is equal to its left-hand derivative, thus I wish to verify it.   My attempt is making use of the limit definition to show the left-hand and right-hand derivatives of $g$ are the same and equal to the above stated formula. In fact, I even got stuck in finding the left-hand side and the right-hand side derivatives of $g$.  But I thought it might suffice to show that the left-hand side derivative $g’_-(k) := \lim_{h \to 0^-} \dfrac{g(k+h)-g(k)}{h}$ is less than the conjecture formulation that stated above, and to show that the right-hand side derivative $g’_+(k) := \lim_{h \to 0^+}\dfrac{g(k+h)-g(k)}{h}$  is greater than the conjecture formulation. Then, by concavity of $g$, we have $g’_-(k) \geq g’_+(k)$ and hence, $g’_-(k)= g’_+(k)=g’(k)$ as desired. In this connection, I think the problem becomes how to establish the relation between the right-hand derivative and conjecture formula, and relation between the left-hand derivative and conjecture formula. Could anyone give me some guidance and help me out please? Thank you very much in advance!","Let $(\Omega, \mathcal{F}, \mu)$ be a probability space. For the sake of simplicity, let $\Omega$ be $\mathbb{R}$. In what follows, the measurability always refers to Borel measurability. Let $f \colon \mathbb{R}_+ \times \Omega \to \mathbb{R}_+$ be a function such that: (i) For each $z \in \Omega$, the function $k \mapsto f(k,z)$ is concave, increasing, and continuously differentiable, while $z \mapsto f(k,z)$ is Borel measurable for each $k \in \mathbb{R}_+$; (ii) $\lim_{k \downarrow 0} f'(k,z) >0$ for each $z \in \Omega$. Here and below, $f'(k,z)$ denotes the partial derivative of $f$ with respect to $k$; and (iii) $f(0,z)=0$ for all $z \in \Omega$. Let $v \colon \mathbb{R}_+ \to \mathbb{R}_+$ be a bounded, strictly concave and strictly increasing function, and be continuously differentiable on $(0, \infty)$. Define a function $g $  by \begin{align*} g(k) := \left( \int_{\Omega} \left[ v\left( f( k, z ) \right) \right]^\alpha \mu (\mathrm{d}z) \right)^{1/\alpha}, \qquad (0<\alpha <1). \end{align*} Question: In fact, since $g$ is concave (it has been proved), we know that the right-hand and the left-hand derivatives of $g$ exist. I aim to show that $g$ is differentiable on $(0, \eta)$ for any fixed constant $\eta >0$, and to show the derivative of $g$ which I conjecture is   \begin{align*} g'(k) =  \left( \int_{\Omega} \left[ v\left( f( k, z ) \right) \right]^\alpha \mu (\mathrm{d}z) \right)^{\frac{1}{\alpha} -1 }   \int_{\Omega} \left[ v\left( f( k, z ) \right) \right]^{\alpha -1}  v'\left( f( k, z ) \right) f'( k, z ) \mu( \mathrm{d} z) \end{align*}   for all $0 < k < \eta$. Here, $g'(k) = \dfrac{\mathrm{d}}{\mathrm{d} k} g(k)$, $v'\left( f( k, z ) \right) := \dfrac{\mathrm{d}}{\mathrm{d} f }v( f( k, z ))$, and $f'(k, z) := \dfrac{\partial}{\partial k} f(k,z)$. My attempt: The above stated derivative of $g$ is just my conjecture and I am not sure if the right-hand derivative of $g$ is equal to its left-hand derivative, thus I wish to verify it.   My attempt is making use of the limit definition to show the left-hand and right-hand derivatives of $g$ are the same and equal to the above stated formula. In fact, I even got stuck in finding the left-hand side and the right-hand side derivatives of $g$.  But I thought it might suffice to show that the left-hand side derivative $g’_-(k) := \lim_{h \to 0^-} \dfrac{g(k+h)-g(k)}{h}$ is less than the conjecture formulation that stated above, and to show that the right-hand side derivative $g’_+(k) := \lim_{h \to 0^+}\dfrac{g(k+h)-g(k)}{h}$  is greater than the conjecture formulation. Then, by concavity of $g$, we have $g’_-(k) \geq g’_+(k)$ and hence, $g’_-(k)= g’_+(k)=g’(k)$ as desired. In this connection, I think the problem becomes how to establish the relation between the right-hand derivative and conjecture formula, and relation between the left-hand derivative and conjecture formula. Could anyone give me some guidance and help me out please? Thank you very much in advance!",,"['calculus', 'real-analysis', 'integration', 'ordinary-differential-equations', 'derivatives']"
73,Union of countable sets contained in each other is not necessarily countable,Union of countable sets contained in each other is not necessarily countable,,"It is obvious that there doesn't exist a maximal enumerable set in $\mathbb{R}$ ( i.e a set $A$ that if $A \subset B \subset \mathbb{R}$ , and $B$ is contable $\Rightarrow$ $A=B$ ). I am searching for an example satisfying these conditions: A family $\{A_i\}_{i\in I}$ , such that each $A_i$ is a countable set contained in $\mathbb{R}$ , and $\forall$ $i,j$ $\in$ $I$ $$A_i \subset A_j \quad\mbox{or} \quad A_j \subset A_i. $$ But $\bigcup\limits_{i\in I} A_i$ is uncountable. Using Zorn's lemma, it is easy to see that it must exist a family that satisfies the above conditions, otherwise would exist a maximal enumerable set in $\mathbb{R}$ . Is it possible to find an explicit example, or is it one of those cases in which the Axiom of Choice generates sets that exist but are impossible to construct?","It is obvious that there doesn't exist a maximal enumerable set in ( i.e a set that if , and is contable ). I am searching for an example satisfying these conditions: A family , such that each is a countable set contained in , and But is uncountable. Using Zorn's lemma, it is easy to see that it must exist a family that satisfies the above conditions, otherwise would exist a maximal enumerable set in . Is it possible to find an explicit example, or is it one of those cases in which the Axiom of Choice generates sets that exist but are impossible to construct?","\mathbb{R} A A \subset B \subset \mathbb{R} B \Rightarrow A=B \{A_i\}_{i\in I} A_i \mathbb{R} \forall i,j \in I A_i \subset A_j \quad\mbox{or} \quad A_j \subset A_i.  \bigcup\limits_{i\in I} A_i \mathbb{R}","['real-analysis', 'set-theory', 'axiom-of-choice']"
74,Multiplier for the space of functions which have a primitive,Multiplier for the space of functions which have a primitive,,"Let us denote by $Pr([a,b])$ the set of functions $f:[a,b]\to\mathbb R$, which have a primitive, i.e. an everywhere differentiable function $F:[a,b]\to\mathbb R$ such that $F'(x)=f(x)$ for all $x\in[a,b]$ (Limits at the boundaries are considered to be one-sided). Now, suppose a function $g:[a,b]\to\mathbb R$ has the following property: For each $f\in Pr([a,b])$ we have $f\cdot g\in Pr([a,b])$. I'd like to show (or disprove), that $g\in L^\infty([a,b])$ (i.e. that $g$ is essentially bounded). I was able to show that $g$ has the following properties: $g\in L^p([a,b])$ for each $p\in[1,\infty)$. $p\circ g\in Pr([a,b])$ for each polynomial $p$, in particular, $g\in Pr([a,b])$. The property $g\in Pr([a,b])$ alone is, of course, too weak to guarantee $g\in L^\infty([a,b])$, since there are well-known examples of derivatives which are essentially unbounded. Any help is highly appreciated. Thanks in advance!","Let us denote by $Pr([a,b])$ the set of functions $f:[a,b]\to\mathbb R$, which have a primitive, i.e. an everywhere differentiable function $F:[a,b]\to\mathbb R$ such that $F'(x)=f(x)$ for all $x\in[a,b]$ (Limits at the boundaries are considered to be one-sided). Now, suppose a function $g:[a,b]\to\mathbb R$ has the following property: For each $f\in Pr([a,b])$ we have $f\cdot g\in Pr([a,b])$. I'd like to show (or disprove), that $g\in L^\infty([a,b])$ (i.e. that $g$ is essentially bounded). I was able to show that $g$ has the following properties: $g\in L^p([a,b])$ for each $p\in[1,\infty)$. $p\circ g\in Pr([a,b])$ for each polynomial $p$, in particular, $g\in Pr([a,b])$. The property $g\in Pr([a,b])$ alone is, of course, too weak to guarantee $g\in L^\infty([a,b])$, since there are well-known examples of derivatives which are essentially unbounded. Any help is highly appreciated. Thanks in advance!",,"['real-analysis', 'integration', 'derivatives']"
75,How to get the idea of the formula for the mean value property for the heat equation,How to get the idea of the formula for the mean value property for the heat equation,,"From the mean-value property of the Laplace's equation, we have the following mean-value property: $$ u(x)=\frac{1}{a(n)r^n}\int_{B(x,r)}u\,dy. $$ But for the mean-value property of the Heat equation, Evans' book defines a heat ball: $$ E(x,t,r)=\left\{(y,s)\in R^{n+1}\bigg|s\leq t, \Phi(x-y,t-s)\geq \frac{1}{r^n}\right\}. $$ Then, the theorem claims that if $u\in C^2_1(U_T)$ solves the heat equation. Then, $$ u(x,t)=\frac{1}{4r^n}\iint\limits_{E(x,t,r)}u(y,s)\frac{|x-y|^2}{(t-s)^2}\,dy\,ds. $$ My question is: is there any explanation (or a guessed one) about the discover of this theorem? The mean-value property is intuitive. But how can we know that we can achieve the goal by making the integrand as the multiplication of $u(y,s)$ with such a strange factor, $\frac{|x-y|^2}{(t-s)^2}$, and a nonintuitive heat ball?","From the mean-value property of the Laplace's equation, we have the following mean-value property: $$ u(x)=\frac{1}{a(n)r^n}\int_{B(x,r)}u\,dy. $$ But for the mean-value property of the Heat equation, Evans' book defines a heat ball: $$ E(x,t,r)=\left\{(y,s)\in R^{n+1}\bigg|s\leq t, \Phi(x-y,t-s)\geq \frac{1}{r^n}\right\}. $$ Then, the theorem claims that if $u\in C^2_1(U_T)$ solves the heat equation. Then, $$ u(x,t)=\frac{1}{4r^n}\iint\limits_{E(x,t,r)}u(y,s)\frac{|x-y|^2}{(t-s)^2}\,dy\,ds. $$ My question is: is there any explanation (or a guessed one) about the discover of this theorem? The mean-value property is intuitive. But how can we know that we can achieve the goal by making the integrand as the multiplication of $u(y,s)$ with such a strange factor, $\frac{|x-y|^2}{(t-s)^2}$, and a nonintuitive heat ball?",,"['real-analysis', 'functional-analysis', 'partial-differential-equations']"
76,Is there a “nice” “constructive” field of numbers?,Is there a “nice” “constructive” field of numbers?,,"I am wondering about this. I've had some interest in “constructive” mathematics, although also some rather strong opinions against those who want to insist that everything else is “wrong” in favor of it. One constructive object is the “computable real number line” as an alternative to the usual Cantor-Dedekind real number line. The computable real number line is such that the numbers on it are all “computable”, meaning you can give an algorithm (perhaps extremely long and inefficient) which will compute the number and which can be run by a Turing computer. In particular, noncomputable constants, like Chaitin's Omega, simply don't exist. The approach is appealing, since it means you can in principle , even if not in practice, write down a finite procedure involving finite (if perhaps unbounded in the accuracy you want) resources to obtain an arbitrarily-good approximation of every real number and thus they are in some sense “explicit”. However, to me it suffers from a few drawbacks. One is that various styles of completeness do not hold – even if we apply the “constructive” restriction to the things within them. For example, the bounded monotone convergence theorem – which is something that looks really intuitive when you draw illustrations on a piece of paper – fails for the computable reals even if we require sequences to themselves be computable , in the sense that we can give an algorithm which approximates the nth term arbitrarily well. (Note that a sequence of computable numbers can itself be noncomputable, e.g. the sequence of digits, taken as real numbers, of Chaitin's Omega.) Another problem is that we cannot computably test for equality – that is, there is no procedure that will let us decide if two arbitrary computable reals are equal, even though we have explicit procedures for computing them . These two results seem somewhat unsatisfactory to me. To do math on the computable reals, then, we have to introduce caveats that make things seem less natural, and I don't like that. Which makes me wonder, is there some way around these difficulties, perhaps by suitably restricting the definition of “computable"" or otherwise “constructibility”? For example, what if we limited our constructibility further to only be numbers and sequences computable by algorithms we can actually prove halt algorithmically? (They all have to be halting algorithms anyways, but here we are saying we should be able to prove they halt by an algorithm . That is, take a subset of all number and sequence and function and whatever other object-computing algorithms out there that is “nice and big enough for our use” but small enough that everything in it has its halting properties decidable by some algorithm which is valid on the entire set.) Failing that, is there some other way to achieve this? Is there any notion of constructive which, while it may not be a “computer” definition but could be something else as long as it is, at least on small scales, practically realizable (so it cannot involve any mandatory infinities or anything like that), that results in a constructively-complete subset of real numbers with decidable equality relation and so over which we can do analysis? If so, what is it, and if not, what is the proof of impossibility?","I am wondering about this. I've had some interest in “constructive” mathematics, although also some rather strong opinions against those who want to insist that everything else is “wrong” in favor of it. One constructive object is the “computable real number line” as an alternative to the usual Cantor-Dedekind real number line. The computable real number line is such that the numbers on it are all “computable”, meaning you can give an algorithm (perhaps extremely long and inefficient) which will compute the number and which can be run by a Turing computer. In particular, noncomputable constants, like Chaitin's Omega, simply don't exist. The approach is appealing, since it means you can in principle , even if not in practice, write down a finite procedure involving finite (if perhaps unbounded in the accuracy you want) resources to obtain an arbitrarily-good approximation of every real number and thus they are in some sense “explicit”. However, to me it suffers from a few drawbacks. One is that various styles of completeness do not hold – even if we apply the “constructive” restriction to the things within them. For example, the bounded monotone convergence theorem – which is something that looks really intuitive when you draw illustrations on a piece of paper – fails for the computable reals even if we require sequences to themselves be computable , in the sense that we can give an algorithm which approximates the nth term arbitrarily well. (Note that a sequence of computable numbers can itself be noncomputable, e.g. the sequence of digits, taken as real numbers, of Chaitin's Omega.) Another problem is that we cannot computably test for equality – that is, there is no procedure that will let us decide if two arbitrary computable reals are equal, even though we have explicit procedures for computing them . These two results seem somewhat unsatisfactory to me. To do math on the computable reals, then, we have to introduce caveats that make things seem less natural, and I don't like that. Which makes me wonder, is there some way around these difficulties, perhaps by suitably restricting the definition of “computable"" or otherwise “constructibility”? For example, what if we limited our constructibility further to only be numbers and sequences computable by algorithms we can actually prove halt algorithmically? (They all have to be halting algorithms anyways, but here we are saying we should be able to prove they halt by an algorithm . That is, take a subset of all number and sequence and function and whatever other object-computing algorithms out there that is “nice and big enough for our use” but small enough that everything in it has its halting properties decidable by some algorithm which is valid on the entire set.) Failing that, is there some other way to achieve this? Is there any notion of constructive which, while it may not be a “computer” definition but could be something else as long as it is, at least on small scales, practically realizable (so it cannot involve any mandatory infinities or anything like that), that results in a constructively-complete subset of real numbers with decidable equality relation and so over which we can do analysis? If so, what is it, and if not, what is the proof of impossibility?",,"['real-analysis', 'computability', 'constructive-mathematics']"
77,Convergence of Euler-transformed zeta series,Convergence of Euler-transformed zeta series,,"I am trying to prove that the expression $$(1-2^{1-s})\zeta(s)=\sum_{n=0}^\infty\sum_{m=0}^n\frac{(-1)^m}{2^{n+1}}{n\choose m}(m+1)^{-s}$$ converges to an analytic continuation of the alternating zeta function for all $s\in\Bbb C\setminus\{1\}$, following the paper by Sondow . My strategy is to show that this ""Euler transformation"" of the original series, $$(1-2^{1-s})\zeta(s)=\sum_{n=1}^\infty(-1)^{n-1}n^{-s},$$ which is convergent for $\Re s>0$, can be achieved by repeatedly applying the transformation $$a_1-a_2+a_3-\dots=\frac12a_1+\frac12[(a_1-a_2)-(a_2-a_3)+(a_3-a_4)-\cdots],$$ which after $k$ transformations extends the region of convergence to $\Re s>-k$, so that the limit sequence is convergent everywhere. The general term after $k$ transformations of the alternating zeta function is: $$(1-2^{1-s})\zeta(s)=\sum_{n=0}^{k-1}\sum_{m=0}^n\frac{(-1)^m}{2^{n+1}}{n\choose m}(m+1)^{-s}+\frac1{2^k}\sum_{n=1}^\infty\sum_{m=0}^k(-1)^{n+m-1}{k\choose m}(m+n)^{-s}$$ Now it follows from the alternating series test that for $k=0$, the original series is convergent for $\Re s>0$, but in order to apply the alternating series test on the transformed series, I need to know that $$\sum_{m=0}^k(-1)^{m-1}{k\choose m}(m+n)^{-s}\ge0\tag{1}$$ $$\sum_{m=0}^k(-1)^{m-1}{k\choose m}(m+n+1)^{-s}\le\sum_{m=0}^k(-1)^{m-1}{k\choose m}(m+n)^{-s}.\tag{2}$$ Now $(1)$ for $k$ follows from $(2)$ for $k-1$, but verifying $(2)$ is not at all obvious to me. Am I going about this the right way?","I am trying to prove that the expression $$(1-2^{1-s})\zeta(s)=\sum_{n=0}^\infty\sum_{m=0}^n\frac{(-1)^m}{2^{n+1}}{n\choose m}(m+1)^{-s}$$ converges to an analytic continuation of the alternating zeta function for all $s\in\Bbb C\setminus\{1\}$, following the paper by Sondow . My strategy is to show that this ""Euler transformation"" of the original series, $$(1-2^{1-s})\zeta(s)=\sum_{n=1}^\infty(-1)^{n-1}n^{-s},$$ which is convergent for $\Re s>0$, can be achieved by repeatedly applying the transformation $$a_1-a_2+a_3-\dots=\frac12a_1+\frac12[(a_1-a_2)-(a_2-a_3)+(a_3-a_4)-\cdots],$$ which after $k$ transformations extends the region of convergence to $\Re s>-k$, so that the limit sequence is convergent everywhere. The general term after $k$ transformations of the alternating zeta function is: $$(1-2^{1-s})\zeta(s)=\sum_{n=0}^{k-1}\sum_{m=0}^n\frac{(-1)^m}{2^{n+1}}{n\choose m}(m+1)^{-s}+\frac1{2^k}\sum_{n=1}^\infty\sum_{m=0}^k(-1)^{n+m-1}{k\choose m}(m+n)^{-s}$$ Now it follows from the alternating series test that for $k=0$, the original series is convergent for $\Re s>0$, but in order to apply the alternating series test on the transformed series, I need to know that $$\sum_{m=0}^k(-1)^{m-1}{k\choose m}(m+n)^{-s}\ge0\tag{1}$$ $$\sum_{m=0}^k(-1)^{m-1}{k\choose m}(m+n+1)^{-s}\le\sum_{m=0}^k(-1)^{m-1}{k\choose m}(m+n)^{-s}.\tag{2}$$ Now $(1)$ for $k$ follows from $(2)$ for $k-1$, but verifying $(2)$ is not at all obvious to me. Am I going about this the right way?",,"['real-analysis', 'convergence-divergence', 'riemann-zeta']"
78,"Derivatives of the Struve functions $H_\nu(x)$, $L_\nu(x)$ and other related functions w.r.t. their index $\nu$","Derivatives of the Struve functions ,  and other related functions w.r.t. their index",H_\nu(x) L_\nu(x) \nu,"There are some known formulae for derivatives of the Bessel functions $J_\nu(x),\,$$Y_\nu(x),\,$$K_\nu(x),\,$$I_\nu(x)\,$with respect to their index $\nu$ for certain values of $\nu$, e.g. $$\left[\frac{\partial J_\nu(x)}{\partial\nu}\right]_{\nu=1/2}=\sqrt{\frac2{\pi\,x}}\Big(\operatorname{Ci}(2\,x)\sin x -\operatorname{Si}(2\,x)\cos x\Big).$$ In this question I am particularly interested in the case $\nu=0$: $$\begin{array}{} &\left[\frac{\partial J_\nu(x)}{\partial\nu}\right]_{\nu=0}=\frac\pi2Y_0(x), &\left[\frac{\partial Y_\nu(x)}{\partial\nu}\right]_{\nu=0}=-\frac\pi2J_0(x),\\ &\left[\frac{\partial K_\nu(x)}{\partial\nu}\right]_{\nu=0}=0, &\left[\frac{\partial I_\nu(x)}{\partial\nu}\right]_{\nu=0}=-K_0(x).\end{array}$$ Are there similar formulae for the Struve functions $\mathbf H_\nu(x)$ , $\mathbf L_\nu(x)$ , Anger function ${\bf{J}}_\nu(x)$ and Weber function ${\bf{E}}_\nu(x)$ ?","There are some known formulae for derivatives of the Bessel functions $J_\nu(x),\,$$Y_\nu(x),\,$$K_\nu(x),\,$$I_\nu(x)\,$with respect to their index $\nu$ for certain values of $\nu$, e.g. $$\left[\frac{\partial J_\nu(x)}{\partial\nu}\right]_{\nu=1/2}=\sqrt{\frac2{\pi\,x}}\Big(\operatorname{Ci}(2\,x)\sin x -\operatorname{Si}(2\,x)\cos x\Big).$$ In this question I am particularly interested in the case $\nu=0$: $$\begin{array}{} &\left[\frac{\partial J_\nu(x)}{\partial\nu}\right]_{\nu=0}=\frac\pi2Y_0(x), &\left[\frac{\partial Y_\nu(x)}{\partial\nu}\right]_{\nu=0}=-\frac\pi2J_0(x),\\ &\left[\frac{\partial K_\nu(x)}{\partial\nu}\right]_{\nu=0}=0, &\left[\frac{\partial I_\nu(x)}{\partial\nu}\right]_{\nu=0}=-K_0(x).\end{array}$$ Are there similar formulae for the Struve functions $\mathbf H_\nu(x)$ , $\mathbf L_\nu(x)$ , Anger function ${\bf{J}}_\nu(x)$ and Weber function ${\bf{E}}_\nu(x)$ ?",,"['calculus', 'real-analysis', 'derivatives', 'special-functions', 'closed-form']"
79,Are there some strategies to prove a set has measure zero?,Are there some strategies to prove a set has measure zero?,,"I'm still confused with subsets of $\mathbb{R}^n$ with measure zero. I mean, I know the definition very well: a subset $A$ of $\mathbb{R}^n$ has measure zero if for every $\epsilon > 0$ given there's an enumerable cover $\{U_i : i \in \Bbb N\}$ of $A$ by closed or open rectangles such that $\sum_{i=1}^\infty \operatorname{vol}(U_i)<\epsilon$. That's fine, but finding this collection depending on $\epsilon$ seem to be tricky. The sequence of the partial sums $\sum_{i=1}^k\operatorname{vol}(U_i)$ should converge, so that this already restricts how should we pick the collection. And besides that, in general I still couldn't get how to do it. When proving continuity, limits, integrability I have a strategy. For continuity (in $\Bbb R^n$) of a function $f$ for example at a point $a$, I write $|f(x)-f(a)|$ and try to bound this in terms of $|x-a|$ and things that I know I can bound without refering to $x$. For integrability, I do the same for the difference $U(f,P)-L(f,P)$. Well, these are just examples of cases I have an idea of a path to start, obviously each case requires a different thought, but I have an starting point. What about proving a set has measure zero? What should be a good starting point? Is there some strategy we can use in these cases? I think there aren't any strategies, but I think it's valid asking. Thanks very much in advance!","I'm still confused with subsets of $\mathbb{R}^n$ with measure zero. I mean, I know the definition very well: a subset $A$ of $\mathbb{R}^n$ has measure zero if for every $\epsilon > 0$ given there's an enumerable cover $\{U_i : i \in \Bbb N\}$ of $A$ by closed or open rectangles such that $\sum_{i=1}^\infty \operatorname{vol}(U_i)<\epsilon$. That's fine, but finding this collection depending on $\epsilon$ seem to be tricky. The sequence of the partial sums $\sum_{i=1}^k\operatorname{vol}(U_i)$ should converge, so that this already restricts how should we pick the collection. And besides that, in general I still couldn't get how to do it. When proving continuity, limits, integrability I have a strategy. For continuity (in $\Bbb R^n$) of a function $f$ for example at a point $a$, I write $|f(x)-f(a)|$ and try to bound this in terms of $|x-a|$ and things that I know I can bound without refering to $x$. For integrability, I do the same for the difference $U(f,P)-L(f,P)$. Well, these are just examples of cases I have an idea of a path to start, obviously each case requires a different thought, but I have an starting point. What about proving a set has measure zero? What should be a good starting point? Is there some strategy we can use in these cases? I think there aren't any strategies, but I think it's valid asking. Thanks very much in advance!",,"['real-analysis', 'measure-theory']"
80,Do uniformly continuous functions map complete sets to complete sets?,Do uniformly continuous functions map complete sets to complete sets?,,"Let $f: (M, d) \rightarrow (N, \rho)$ be uniformly continuous. Prove or disprove that if M is complete, then $f(M)$ is complete. If I am asking a previously posted question, please accept my apologies and tell me to bugger off.  I saw a similar problem but the solution was dealing with a Bi-Lipschitz function or some such business. I believe this statement to be true and here is a rough sketch of my reasoning: Since $f$ is uniformly continuous, then $f$ maps Cauchy to Cauchy. Let $(x_n)$ be a Cauchy sequence in $M$.  Since $M$ is complete, $x_n \rightarrow x \in M$.  Again, because of $f$'s uniform continuity, we now have $(f(x_n))$ is Cauchy in $N$ and $f(x_n) \rightarrow f(x) \in N$.  Thus $N$ is complete. By the way, I am studying for an exam.  This is certainly not homework.  I gladly accept your criticisms.  Thank you in advance for your help.","Let $f: (M, d) \rightarrow (N, \rho)$ be uniformly continuous. Prove or disprove that if M is complete, then $f(M)$ is complete. If I am asking a previously posted question, please accept my apologies and tell me to bugger off.  I saw a similar problem but the solution was dealing with a Bi-Lipschitz function or some such business. I believe this statement to be true and here is a rough sketch of my reasoning: Since $f$ is uniformly continuous, then $f$ maps Cauchy to Cauchy. Let $(x_n)$ be a Cauchy sequence in $M$.  Since $M$ is complete, $x_n \rightarrow x \in M$.  Again, because of $f$'s uniform continuity, we now have $(f(x_n))$ is Cauchy in $N$ and $f(x_n) \rightarrow f(x) \in N$.  Thus $N$ is complete. By the way, I am studying for an exam.  This is certainly not homework.  I gladly accept your criticisms.  Thank you in advance for your help.",,"['real-analysis', 'cauchy-sequences']"
81,Measurable functions with values in Banach spaces,Measurable functions with values in Banach spaces,,"My question refers to functions with values in Banach spaces and under what conditions the limit of a sequence of measurable functions is also measurable. But first, let me recall some well-known results from real analysis. Let $(X,\mathcal{F})$ be a measurable space, $\bar{\mathbb{R}}$ the extended real line, and $\mathcal{B}(\bar{\mathbb{R}})$ the standard Borel $\sigma$-algebra generated by the open sets of $\bar{\mathbb{R}}$. (1) If a sequence of measurable functions $f_n:(X,\mathcal{F})\rightarrow(\bar{\mathbb{R}},\mathcal{B}(\bar{\mathbb{R}}))$ converges to a function $f$ pointwise in $X$, then $f$ is also measurable w.r.t. $\mathcal{F}$ and $\mathcal{B}(\bar{\mathbb{R}})$. (See, e.g., Rudin, Real and Complex Analysis, Theorem 1.14.) (2) This can be generalized to the case of a complete measure space $(X,\mathcal{F},\mu)$ and convergence of $f_n$ to $f$ pointwise-$\mu$-a.e. in $X$. (This is stated without proof in Hunter and Nachtergaele, Applied Analysis, Theorem 12.24. Can anyone point to a proof of this, or is it trivial?) Dudley (Real Analysis and Probability, Theorem 4.2.2) generalizes part (1) above to functions with values in metric spaces. Therefore (1) also holds for functions with values in a Banach space $(Y,\Vert\cdot\Vert)$, with corresponding Borel $\sigma$-algebra $\mathcal{B}(Y)$. My question is: can part (2) be also generalized? In other words, does the following result hold: Let $(X,\mathcal{F},\mu)$ be a complete measure space and $(Y,\Vert\cdot\Vert)$ be a Banach space. Let $f_n:(X,\mathcal{F})\rightarrow(Y,\mathcal{B}(Y))$  be a sequence of measurable functions that converges to a function $f$ pointwise-$\mu$-a.e. in $X$ (i.e., $\Vert f_n(x) - f(x)\Vert \rightarrow 0$ as $n\rightarrow\infty$, for $x$ $\mu$-a.e. in $X$), then $f$ is also measurable w.r.t. $\mathcal{F}$ and $\mathcal{B}(Y)$. Note, I use ""measurable"" in the standard sense: if $U\in\mathcal{B}(Y)$ then $f^{-1}(U)\in\mathcal{F}$. It seems to me that the result should hold but I haven't been able to find a proof in the standard references. Is it trivial? Also, why should the measure space $(X,\mathcal{F},\mu)$ be complete? What fails if it is not complete. Finally, if the result holds, does this mean that standard measurability is equivalent to so-called strong or Bochner measurability? (Maybe this last question should be the topic of another post.) Thanks in advance!","My question refers to functions with values in Banach spaces and under what conditions the limit of a sequence of measurable functions is also measurable. But first, let me recall some well-known results from real analysis. Let $(X,\mathcal{F})$ be a measurable space, $\bar{\mathbb{R}}$ the extended real line, and $\mathcal{B}(\bar{\mathbb{R}})$ the standard Borel $\sigma$-algebra generated by the open sets of $\bar{\mathbb{R}}$. (1) If a sequence of measurable functions $f_n:(X,\mathcal{F})\rightarrow(\bar{\mathbb{R}},\mathcal{B}(\bar{\mathbb{R}}))$ converges to a function $f$ pointwise in $X$, then $f$ is also measurable w.r.t. $\mathcal{F}$ and $\mathcal{B}(\bar{\mathbb{R}})$. (See, e.g., Rudin, Real and Complex Analysis, Theorem 1.14.) (2) This can be generalized to the case of a complete measure space $(X,\mathcal{F},\mu)$ and convergence of $f_n$ to $f$ pointwise-$\mu$-a.e. in $X$. (This is stated without proof in Hunter and Nachtergaele, Applied Analysis, Theorem 12.24. Can anyone point to a proof of this, or is it trivial?) Dudley (Real Analysis and Probability, Theorem 4.2.2) generalizes part (1) above to functions with values in metric spaces. Therefore (1) also holds for functions with values in a Banach space $(Y,\Vert\cdot\Vert)$, with corresponding Borel $\sigma$-algebra $\mathcal{B}(Y)$. My question is: can part (2) be also generalized? In other words, does the following result hold: Let $(X,\mathcal{F},\mu)$ be a complete measure space and $(Y,\Vert\cdot\Vert)$ be a Banach space. Let $f_n:(X,\mathcal{F})\rightarrow(Y,\mathcal{B}(Y))$  be a sequence of measurable functions that converges to a function $f$ pointwise-$\mu$-a.e. in $X$ (i.e., $\Vert f_n(x) - f(x)\Vert \rightarrow 0$ as $n\rightarrow\infty$, for $x$ $\mu$-a.e. in $X$), then $f$ is also measurable w.r.t. $\mathcal{F}$ and $\mathcal{B}(Y)$. Note, I use ""measurable"" in the standard sense: if $U\in\mathcal{B}(Y)$ then $f^{-1}(U)\in\mathcal{F}$. It seems to me that the result should hold but I haven't been able to find a proof in the standard references. Is it trivial? Also, why should the measure space $(X,\mathcal{F},\mu)$ be complete? What fails if it is not complete. Finally, if the result holds, does this mean that standard measurability is equivalent to so-called strong or Bochner measurability? (Maybe this last question should be the topic of another post.) Thanks in advance!",,"['real-analysis', 'measure-theory']"
82,Integrate product of Dirac delta and discontinuous function?,Integrate product of Dirac delta and discontinuous function?,,"Consider the piecewise constant function $\psi:I=[-1,1] \rightarrow \mathbb{R}$ given by $$\psi(x) = \begin{cases} \psi_1 & x \leq 0, \ \psi_2 & x > 0 \end{cases}$$ for some constants $\psi_1, \psi_2 \in \mathbb{R}$.  I would like to evaluate the integral $$\int_I \delta(x) \psi(x) dx$$ where $\delta$ is the Dirac delta distribution centered at $x=0$.  Of course, if we think about distributions in the usual way then you might say that $\delta(\psi) = \langle \delta, \psi \rangle = \psi(0) = \psi_1.$  But then the result depends on a fairly arbitrary choice when defining $\psi$: should the left- or right- half of the interval be closed?  This question doesn't seem to have a meaningful answer when dealing with a problem that arises from a physical system (say). Instead, consider a family of distributions $\phi_\epsilon(x)$ such that $\phi_\epsilon(x)=\phi_\epsilon(-x)$, $\int_I \phi_\epsilon(x)=1$ for all $\epsilon$, and $\lim_{\epsilon \rightarrow 0} \phi_\epsilon = \delta$, i.e., any family of even distributions with unit mass that approaches the Dirac delta distribution as $\epsilon$ approaches zero.  (For instance, you could use the family of Gaussians $\phi_\epsilon(x) = \frac{1}{\epsilon\sqrt{\pi}}e^{-x^2/\epsilon^2}$.) I can now think of my integral as $$\lim_{\epsilon \rightarrow 0} \int_I \phi_\epsilon(x) \psi(x) dx.$$ For some $\epsilon > 0$, the integral inside the limit can be expressed as $$u(\epsilon) = \psi_1 \int_{-1}^0 \phi_\epsilon(x) dx + \psi_2 \int_0^1 \phi_\epsilon(x) dx = \frac{1}{2}\left( \psi_1 + \psi_2 \right) = \bar{\psi},$$ where $\bar{\psi}$ is the mean of the constant values.  Can we say, then, that $\lim_{\epsilon \rightarrow 0} u(\epsilon) = \bar{\psi}$?  It would seem so: for any $\mu > 0$ there exists an $\epsilon_0$ such that $\epsilon < \epsilon_0$ implies $|u(\epsilon) - \bar{\psi}|<\mu$ (namely, $\epsilon$ is any positive constant!).  But clearly I've got a problem somewhere, because $\bar{\psi} \ne \psi_1$, i.e., this result does not agree with my earlier interpretation. So what's the right thing to do here?  The latter answer ($\bar{\psi}$) agrees more with my ""physical"" intuition (because it's invariant with respect to deciding which half-interval is open), but I'm concerned about rigor. Edit: Since the problem as stated is not well-posed ($\delta$ cannot be evaluated on discontinuous functions), let me give some motivation.  Imagine that I have a pair of piecewise linear functions $f,g:I^2 \rightarrow \mathbb{R}$, which are again discontinuous only at $x=0$.  I would like to integrate the wedge product of $df$ and $dg$ over the domain: $$\int_{I^2} df \wedge dg = \int_I \int_I \frac{\partial f}{\partial x} \frac{\partial g}{\partial y} - \frac{\partial f}{\partial y}\frac{\partial g}{\partial x} dx dy.$$ Consider just the first term $(\partial f/\partial x)(\partial g/\partial y)$ and consider just the inner integral $\int_I \cdot dx$.  We now have (almost) the original problem: $\partial f/\partial x$ can be thought of as a $\delta$ (plus a piecewise constant), and $\partial g/\partial y$ is simply piecewise constant along the $x$-direction. So, the problem could be restated as: how do I integrate the wedge product $df \wedge dg$ of piecewise linear 0-forms $f$ and $g$ defined over a planar region?  Formally this problem may again be ill-posed, yet it is a real problem that comes up in the context of finite element analysis where basis functions are nonsmooth or even discontinuous.","Consider the piecewise constant function $\psi:I=[-1,1] \rightarrow \mathbb{R}$ given by $$\psi(x) = \begin{cases} \psi_1 & x \leq 0, \ \psi_2 & x > 0 \end{cases}$$ for some constants $\psi_1, \psi_2 \in \mathbb{R}$.  I would like to evaluate the integral $$\int_I \delta(x) \psi(x) dx$$ where $\delta$ is the Dirac delta distribution centered at $x=0$.  Of course, if we think about distributions in the usual way then you might say that $\delta(\psi) = \langle \delta, \psi \rangle = \psi(0) = \psi_1.$  But then the result depends on a fairly arbitrary choice when defining $\psi$: should the left- or right- half of the interval be closed?  This question doesn't seem to have a meaningful answer when dealing with a problem that arises from a physical system (say). Instead, consider a family of distributions $\phi_\epsilon(x)$ such that $\phi_\epsilon(x)=\phi_\epsilon(-x)$, $\int_I \phi_\epsilon(x)=1$ for all $\epsilon$, and $\lim_{\epsilon \rightarrow 0} \phi_\epsilon = \delta$, i.e., any family of even distributions with unit mass that approaches the Dirac delta distribution as $\epsilon$ approaches zero.  (For instance, you could use the family of Gaussians $\phi_\epsilon(x) = \frac{1}{\epsilon\sqrt{\pi}}e^{-x^2/\epsilon^2}$.) I can now think of my integral as $$\lim_{\epsilon \rightarrow 0} \int_I \phi_\epsilon(x) \psi(x) dx.$$ For some $\epsilon > 0$, the integral inside the limit can be expressed as $$u(\epsilon) = \psi_1 \int_{-1}^0 \phi_\epsilon(x) dx + \psi_2 \int_0^1 \phi_\epsilon(x) dx = \frac{1}{2}\left( \psi_1 + \psi_2 \right) = \bar{\psi},$$ where $\bar{\psi}$ is the mean of the constant values.  Can we say, then, that $\lim_{\epsilon \rightarrow 0} u(\epsilon) = \bar{\psi}$?  It would seem so: for any $\mu > 0$ there exists an $\epsilon_0$ such that $\epsilon < \epsilon_0$ implies $|u(\epsilon) - \bar{\psi}|<\mu$ (namely, $\epsilon$ is any positive constant!).  But clearly I've got a problem somewhere, because $\bar{\psi} \ne \psi_1$, i.e., this result does not agree with my earlier interpretation. So what's the right thing to do here?  The latter answer ($\bar{\psi}$) agrees more with my ""physical"" intuition (because it's invariant with respect to deciding which half-interval is open), but I'm concerned about rigor. Edit: Since the problem as stated is not well-posed ($\delta$ cannot be evaluated on discontinuous functions), let me give some motivation.  Imagine that I have a pair of piecewise linear functions $f,g:I^2 \rightarrow \mathbb{R}$, which are again discontinuous only at $x=0$.  I would like to integrate the wedge product of $df$ and $dg$ over the domain: $$\int_{I^2} df \wedge dg = \int_I \int_I \frac{\partial f}{\partial x} \frac{\partial g}{\partial y} - \frac{\partial f}{\partial y}\frac{\partial g}{\partial x} dx dy.$$ Consider just the first term $(\partial f/\partial x)(\partial g/\partial y)$ and consider just the inner integral $\int_I \cdot dx$.  We now have (almost) the original problem: $\partial f/\partial x$ can be thought of as a $\delta$ (plus a piecewise constant), and $\partial g/\partial y$ is simply piecewise constant along the $x$-direction. So, the problem could be restated as: how do I integrate the wedge product $df \wedge dg$ of piecewise linear 0-forms $f$ and $g$ defined over a planar region?  Formally this problem may again be ill-posed, yet it is a real problem that comes up in the context of finite element analysis where basis functions are nonsmooth or even discontinuous.",,"['calculus', 'real-analysis', 'analysis']"
83,A somewhat intriguing dilogarithmic integral,A somewhat intriguing dilogarithmic integral,,"The following problem has been recently proposed by C.I. Valean , $$\int_0^1 \frac{\displaystyle \left(\operatorname{Li}_2\left(\frac{1-x}{2}\right)\right)^2}{1+x} \textrm{d}x$$ $$=\frac{3}{16}\zeta(5)-\frac{1}{4}\zeta(2)\zeta(3)+\frac{5}{8}\log(2)\zeta(4)-\frac{1}{6}\log^3(2)\zeta(2)+\frac{1}{20}\log^5(2).$$ One of the author's solutions makes use of the following ( very useful ) Cauchy product, $$\frac{(\operatorname{Li_2}(x))^2}{1-x}=\sum_{n=1}^{\infty}x^n\left((H_n^{(2)})^2-5 H_n^{(4)}+4\sum_{k=1}^n\frac{H_k}{k^3}\right),$$ which appears in More (Almost) Impossible Integrals, Sums, and Series: A New Collection of Fiendish Problems and Surprising Solutions (2023) , in Sect. 4.5 , page 398 (see the top of the page), which is the sequel of (Almost) Impossible Integrals, Sums, and Series (2019) . We observe the main integral, after the variable change $x\mapsto 1-x$ , may be put in the form $$\frac{1}{2}\int_0^1 \frac{\displaystyle \left(\operatorname{Li}_2\left(\frac{x}{2}\right)\right)^2}{1-x/2} \textrm{d}x,$$ and from here we get manageable results, all easily derived with results from the mentioned books. For example, at this point we might find helpful to know and use generating functions like $$\sum_{n=1}^{\infty} x^n (H_n^{(2)})^2$$ $$=\frac{1}{1-x}\biggr(-4\zeta(4)-4\zeta(3)\log(1-x)-2\zeta(2)\log^2(1-x)-\frac{1}{6}\log^4(1-x)$$ $$+\frac{2}{3}\log(x)\log^3(1-x)+(\operatorname{Li_2}(x))^2+4\log(1-x)\operatorname{Li_3}(x)-3\operatorname{Li_4}(x)$$ $$+4\operatorname{Li_4}(1-x)-4\operatorname{Li_4}\left(\frac{x}{x-1}\right)\biggr),$$ which appears in the first-mentioned book (and visiting further its Sect. 3.51 is definitely illuminating for the next steps to do). Another solution idea: we could start with letting $x\mapsto 2x-1$ and then exploiting the Dilogarithm reflection formula, in an effort to reduce everything to manageable (known) results. The interesting question: Now, as previously seen, the path described above involves the use of harmonic series. May we design ways that avoid the use of harmonic series and then elegantly get the desired closed form?","The following problem has been recently proposed by C.I. Valean , One of the author's solutions makes use of the following ( very useful ) Cauchy product, which appears in More (Almost) Impossible Integrals, Sums, and Series: A New Collection of Fiendish Problems and Surprising Solutions (2023) , in Sect. 4.5 , page 398 (see the top of the page), which is the sequel of (Almost) Impossible Integrals, Sums, and Series (2019) . We observe the main integral, after the variable change , may be put in the form and from here we get manageable results, all easily derived with results from the mentioned books. For example, at this point we might find helpful to know and use generating functions like which appears in the first-mentioned book (and visiting further its Sect. 3.51 is definitely illuminating for the next steps to do). Another solution idea: we could start with letting and then exploiting the Dilogarithm reflection formula, in an effort to reduce everything to manageable (known) results. The interesting question: Now, as previously seen, the path described above involves the use of harmonic series. May we design ways that avoid the use of harmonic series and then elegantly get the desired closed form?","\int_0^1 \frac{\displaystyle \left(\operatorname{Li}_2\left(\frac{1-x}{2}\right)\right)^2}{1+x} \textrm{d}x =\frac{3}{16}\zeta(5)-\frac{1}{4}\zeta(2)\zeta(3)+\frac{5}{8}\log(2)\zeta(4)-\frac{1}{6}\log^3(2)\zeta(2)+\frac{1}{20}\log^5(2). \frac{(\operatorname{Li_2}(x))^2}{1-x}=\sum_{n=1}^{\infty}x^n\left((H_n^{(2)})^2-5 H_n^{(4)}+4\sum_{k=1}^n\frac{H_k}{k^3}\right), x\mapsto 1-x \frac{1}{2}\int_0^1 \frac{\displaystyle \left(\operatorname{Li}_2\left(\frac{x}{2}\right)\right)^2}{1-x/2} \textrm{d}x, \sum_{n=1}^{\infty} x^n (H_n^{(2)})^2 =\frac{1}{1-x}\biggr(-4\zeta(4)-4\zeta(3)\log(1-x)-2\zeta(2)\log^2(1-x)-\frac{1}{6}\log^4(1-x) +\frac{2}{3}\log(x)\log^3(1-x)+(\operatorname{Li_2}(x))^2+4\log(1-x)\operatorname{Li_3}(x)-3\operatorname{Li_4}(x) +4\operatorname{Li_4}(1-x)-4\operatorname{Li_4}\left(\frac{x}{x-1}\right)\biggr), x\mapsto 2x-1","['real-analysis', 'calculus', 'integration', 'sequences-and-series', 'definite-integrals']"
84,How far can we push the Fundamental Theorem of Calculus for Riemann integral?,How far can we push the Fundamental Theorem of Calculus for Riemann integral?,,"Let $f:[a,b] \rightarrow \mathbb{R}$ be a continuous function such that it is differentiable everywhere unless in a null set $S$ . Suppose that there is a function $g$ , which is bounded and Riemann integrable in $[a,b]$ , such that $g(x) = f'(x)$ for every $x\in [a,b] -S$ . Then, $$ f(b)-f(a) = \int^b_a g  $$ is true? If it is false, provide a counterexample and consider the case in which we switch ""null set"" by ""countable set"". In this case, it will be true? If not, provides a counterexample. I know that it would be true if we have finite set instead of null set. Remark: I am asking this question because I am studying a Brazilian book about Fourier Analysis (""Análise de fourier e equações diferenciais parcias"" whose author is Djairo), in which he just uses Riemann integral and uses many times integration by parts. However, he just say ""let $f$ be continuous in a closed and bounded interval such that $f'$ is integrable in the same interval"" and then he uses integration by parts (the other function is $\cos$ or $\sin$ usually). And he doesn't define which he means by the ""derivative"" of a function, because if $f$ is differentiable everywhere, it would redundant to say that it is continuous. Would it be differentiable everywhere unless in a null set, countable set, finite set? Idk. This is why i am asking how far I can push the fundamental theorem of calculus, which is used to prove integration by parts. Edits :  Thanks for the comments, the case in which $S$ is a null set is already solved. It's false . The counterexample is the Cantor function. It remains the case in which $S$ is countable . Here: "" A Fundamental Theorem of Calculus "" there's a similar problem, but I m not sure if it is equivalent. Anyway, I would really appreciate if my problem were solved not using Lebesgue theory, which i haven't studied yet. I had to fix the statement of the problem because it was wrong, as pointed out in the comments. Originally, i thought that it was enough that $f$ was differentiable in $[a,b]-S$ and one can extend $f'$ in any way, but then $\int_a^b f'$ will not exist necessarily, and we won't obviously have $ f(b)-f(a) = \int^b_a f'$ .","Let be a continuous function such that it is differentiable everywhere unless in a null set . Suppose that there is a function , which is bounded and Riemann integrable in , such that for every . Then, is true? If it is false, provide a counterexample and consider the case in which we switch ""null set"" by ""countable set"". In this case, it will be true? If not, provides a counterexample. I know that it would be true if we have finite set instead of null set. Remark: I am asking this question because I am studying a Brazilian book about Fourier Analysis (""Análise de fourier e equações diferenciais parcias"" whose author is Djairo), in which he just uses Riemann integral and uses many times integration by parts. However, he just say ""let be continuous in a closed and bounded interval such that is integrable in the same interval"" and then he uses integration by parts (the other function is or usually). And he doesn't define which he means by the ""derivative"" of a function, because if is differentiable everywhere, it would redundant to say that it is continuous. Would it be differentiable everywhere unless in a null set, countable set, finite set? Idk. This is why i am asking how far I can push the fundamental theorem of calculus, which is used to prove integration by parts. Edits :  Thanks for the comments, the case in which is a null set is already solved. It's false . The counterexample is the Cantor function. It remains the case in which is countable . Here: "" A Fundamental Theorem of Calculus "" there's a similar problem, but I m not sure if it is equivalent. Anyway, I would really appreciate if my problem were solved not using Lebesgue theory, which i haven't studied yet. I had to fix the statement of the problem because it was wrong, as pointed out in the comments. Originally, i thought that it was enough that was differentiable in and one can extend in any way, but then will not exist necessarily, and we won't obviously have .","f:[a,b] \rightarrow \mathbb{R} S g [a,b] g(x) = f'(x) x\in [a,b] -S  f(b)-f(a) = \int^b_a g   f f' \cos \sin f S S f [a,b]-S f' \int_a^b f'  f(b)-f(a) = \int^b_a f'","['real-analysis', 'riemann-integration']"
85,If a sequence $\{x_{n}\}$ is a Cauchy sequence and the sequence has a limit point $x_{0}$ then $x_{n} \rightarrow x_{0}$,If a sequence  is a Cauchy sequence and the sequence has a limit point  then,\{x_{n}\} x_{0} x_{n} \rightarrow x_{0},"I am trying to prove that if a sequence $\{x_{n}\}$ in a set $M\in X$ ( $X$ is a metric space) is a Cauchy sequence and the sequence has a limit point $x_{0}$ then $x_{n} \rightarrow x_{0}$ . I wish to understand the details in such a proof whence I will try to explain my thoughts. I know that the sequence $\{x_{n}\}_{N\in \mathbb{N}}$ is a Cauchy sequence which means that for all $\epsilon >0$ I can find an $N \in \mathbb{N}$ beyond which the distance between any to elements $x_{n}$ and $x_{m}$ of the sequence will be smaller than $\epsilon$ . This means that I can make the distance between any two element of the sequence arbitrarily small. Furthermore we know that $x_{0}$ is a limit point which means that for every $\epsilon>0$ there is a point $m \in M$ such that $m \in B(x_{0},\epsilon)$ . I wish to argue as follows: 1) I would like to argue that $x_{m}=m \in B(x_{0},\frac{\epsilon}{2})$ (I can freely choose $r=\frac{\epsilon}{2}$ since $x_{0}$ is a limit point. 2) Since $\{x_{n}\}$ is a Cauchy sequence $d(x_{n},x_{m})$ will be smaller than any $\epsilon$ I choose, so I choose $\frac{\epsilon}{2}$ . Therefore $d(x_{n},x_{m})<\frac{\epsilon}{2}$ . 3) Since $x_{m}=m \in B(x_{0},\frac{\epsilon}{2})$ then $d(x_{m},x_{0})<\frac{\epsilon}{2}$ . 4) Using the triangle inequality we have $d(x_{n},x_{0}) \leq d(x_{n},x_{m}) + d(x_{m},x_{0}) \leq \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon$ 5) We can conclude that $x_{n} \rightarrow x_{0}$ Can someone please comment on the proof and the arguments presented? How can I convince myself of 1) ? How do I convince myself of the choice of epsilon (I start my choice of epsilon from the fact that $x_{0}$ is a limit point and from this I choose my epsilon for the distance between $x_{n}$ and $x_{m}$ ). Many thanks","I am trying to prove that if a sequence in a set ( is a metric space) is a Cauchy sequence and the sequence has a limit point then . I wish to understand the details in such a proof whence I will try to explain my thoughts. I know that the sequence is a Cauchy sequence which means that for all I can find an beyond which the distance between any to elements and of the sequence will be smaller than . This means that I can make the distance between any two element of the sequence arbitrarily small. Furthermore we know that is a limit point which means that for every there is a point such that . I wish to argue as follows: 1) I would like to argue that (I can freely choose since is a limit point. 2) Since is a Cauchy sequence will be smaller than any I choose, so I choose . Therefore . 3) Since then . 4) Using the triangle inequality we have 5) We can conclude that Can someone please comment on the proof and the arguments presented? How can I convince myself of 1) ? How do I convince myself of the choice of epsilon (I start my choice of epsilon from the fact that is a limit point and from this I choose my epsilon for the distance between and ). Many thanks","\{x_{n}\} M\in X X x_{0} x_{n} \rightarrow x_{0} \{x_{n}\}_{N\in \mathbb{N}} \epsilon >0 N \in \mathbb{N} x_{n} x_{m} \epsilon x_{0} \epsilon>0 m \in M m \in B(x_{0},\epsilon) x_{m}=m \in B(x_{0},\frac{\epsilon}{2}) r=\frac{\epsilon}{2} x_{0} \{x_{n}\} d(x_{n},x_{m}) \epsilon \frac{\epsilon}{2} d(x_{n},x_{m})<\frac{\epsilon}{2} x_{m}=m \in B(x_{0},\frac{\epsilon}{2}) d(x_{m},x_{0})<\frac{\epsilon}{2} d(x_{n},x_{0}) \leq d(x_{n},x_{m}) + d(x_{m},x_{0}) \leq \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon x_{n} \rightarrow x_{0} x_{0} x_{n} x_{m}","['real-analysis', 'proof-verification', 'convergence-divergence', 'metric-spaces', 'cauchy-sequences']"
86,On an asymptotic improvement of AMM problem 11145 (April 2005),On an asymptotic improvement of AMM problem 11145 (April 2005),,"Motivation Motivated by this question , I tried improve the inequality $$\sum_{k=1}^{n}\dfrac{k}{a_{1}+a_{2}+\cdots+a_{k}}\le2\sum_{k=1}^{n}\dfrac{1}{a_{k}}$$ asymptotically. In other words, with support of some numerical evidence, I want to find the value of the following limit. Question Numerical experiment indicates that $$\lim_{n\to \infty}\ln n\left( 2-\sup_{a_k>0 (k=1\ldots n)}\frac{\sum_{k=1}^n{\frac{k}{a_1+a_2+\cdots +a_k}}}{\sum_{k=1}^n{1/a_k}} \right) $$ exists. The limit seems to be approximately $1.5$ . How can we prove it and find its value? Some Trivial Results From this answer we can see that this limit is of form $\infty\cdot0$ . One can apply $\frac{\partial}{\partial a_k}$ to the formula in the $\sup$ and get a simultaneous equation, which is extremely complex and hence almost unsolvable with unknown $n$ .","Motivation Motivated by this question , I tried improve the inequality asymptotically. In other words, with support of some numerical evidence, I want to find the value of the following limit. Question Numerical experiment indicates that exists. The limit seems to be approximately . How can we prove it and find its value? Some Trivial Results From this answer we can see that this limit is of form . One can apply to the formula in the and get a simultaneous equation, which is extremely complex and hence almost unsolvable with unknown .",\sum_{k=1}^{n}\dfrac{k}{a_{1}+a_{2}+\cdots+a_{k}}\le2\sum_{k=1}^{n}\dfrac{1}{a_{k}} \lim_{n\to \infty}\ln n\left( 2-\sup_{a_k>0 (k=1\ldots n)}\frac{\sum_{k=1}^n{\frac{k}{a_1+a_2+\cdots +a_k}}}{\sum_{k=1}^n{1/a_k}} \right)  1.5 \infty\cdot0 \frac{\partial}{\partial a_k} \sup n,"['real-analysis', 'limits', 'inequality', 'summation', 'supremum-and-infimum']"
87,What is Tarski’s definition of real number multiplication?,What is Tarski’s definition of real number multiplication?,,"Alfred Tarski came up with the following axiomatization of the real numbers, which only references the notions of “less than” and addition: If $x < y$ , then not $y < x$ . That is,  “ $<$ "" is an asymmetric relation. If $x < z$ , there exists a $y$ such that $x < y$ and $y < z$ . In other words, "" $<$ "" is dense in $\mathbb{R}$ . "" $<$ "" is Dedekind-complete. More formally, for all $X,Y \subseteq \mathbb{R}$ , if for all $x \in X$ and $y \in Y$ , $x < y$ , then there exists a $z$ such that for all $x \in X$ and $y \in Y$ , if $z \neq x$ and $z \neq y$ , then $x < z$ and $z < y$ . $x + (y + z) = (x + z) + y$ . For all $x$ , $y$ , there exists a $z$ such that $x + z = y$ . If $x + y < z + w$ , then $x < z$ or $y < w$ . $1\in\mathbb{R}$ $1 < 1 + 1$ . But it’s still equivalent to the usual axiomatization of the real numbers, which includes axioms for multiplication. Here is what Wikipedia says: Tarski sketched the (nontrivial) proof of how these axioms and primitives imply the existence of a binary operation called multiplication and having the expected properties, so that $\mathbb{R}$ is a complete ordered field under addition and multiplication. This proof builds crucially on the integers with addition being an abelian group and has its origins in Eudoxus' definition of magnitude. My question is, what is Tarski’s definition of multiplication in this system? I skimmed Tarski’s book “Introduction to Logic and to the Methodology of Deductive Sciences”, and I found the above axioms, but I couldn’t find a definition of multiplication or a proof that multiplication satisfies the usual properties.","Alfred Tarski came up with the following axiomatization of the real numbers, which only references the notions of “less than” and addition: If , then not . That is,  “ "" is an asymmetric relation. If , there exists a such that and . In other words, "" "" is dense in . "" "" is Dedekind-complete. More formally, for all , if for all and , , then there exists a such that for all and , if and , then and . . For all , , there exists a such that . If , then or . . But it’s still equivalent to the usual axiomatization of the real numbers, which includes axioms for multiplication. Here is what Wikipedia says: Tarski sketched the (nontrivial) proof of how these axioms and primitives imply the existence of a binary operation called multiplication and having the expected properties, so that is a complete ordered field under addition and multiplication. This proof builds crucially on the integers with addition being an abelian group and has its origins in Eudoxus' definition of magnitude. My question is, what is Tarski’s definition of multiplication in this system? I skimmed Tarski’s book “Introduction to Logic and to the Methodology of Deductive Sciences”, and I found the above axioms, but I couldn’t find a definition of multiplication or a proof that multiplication satisfies the usual properties.","x < y y < x < x < z y x < y y < z < \mathbb{R} < X,Y \subseteq \mathbb{R} x \in X y \in Y x < y z x \in X y \in Y z \neq x z \neq y x < z z < y x + (y + z) = (x + z) + y x y z x + z = y x + y < z + w x < z y < w 1\in\mathbb{R} 1 < 1 + 1 \mathbb{R}","['real-analysis', 'logic', 'real-numbers', 'ordered-fields', 'second-order-logic']"
88,Hahn Decomposition Theorem In Folland,Hahn Decomposition Theorem In Folland,,"I was reading the proof of Hahn Decomposition theorem from the textbook of Folland: precisely I was looking at the following text I have the following question: As Highlighted in the text above, why $m$ is finite? It may be infinite as there is no restriction on $X$ . Why does the author consider it finite? Again why $\nu(A)<\infty$ ? I do not understand also this. I understand that even for $A$ also we get some $B$ with the property that $\nu(B)>\nu(A)+1/n$ but I do not understand how this leads to a contradiction. I would be really thankful if someone could help me. Any help will be appreciated.","I was reading the proof of Hahn Decomposition theorem from the textbook of Folland: precisely I was looking at the following text I have the following question: As Highlighted in the text above, why is finite? It may be infinite as there is no restriction on . Why does the author consider it finite? Again why ? I do not understand also this. I understand that even for also we get some with the property that but I do not understand how this leads to a contradiction. I would be really thankful if someone could help me. Any help will be appreciated.",m X \nu(A)<\infty A B \nu(B)>\nu(A)+1/n,"['real-analysis', 'measure-theory', 'proof-explanation', 'lebesgue-measure']"
89,Explicit value for $\sum_{n=1}^{\infty} \left(\frac{1}{\sqrt{1}+\sqrt{2}+\dots+\sqrt{n}}\right)$,Explicit value for,\sum_{n=1}^{\infty} \left(\frac{1}{\sqrt{1}+\sqrt{2}+\dots+\sqrt{n}}\right),"This question came out from this other one : Is there an explicit value for this series? $$\sum_{n=1}^{\infty}\frac{1}{\sqrt{1}+\sqrt{2}+\dots+\sqrt{n}}= \sum_{n=1}^{\infty} \frac{1}{\sum_{i=1}^{n}\sqrt{i}}=\sum_{n=1}^{\infty}\frac{1}{H_{n,-\frac{1}{2}}}$$ As someone pointed out in comments to the other question, this value is a little bit lower than $3.167830$ and it's not far away from $\frac{63}{20}+\frac{7\sqrt{3}}{680}$. Can we go further on this? Can we reach an exact value or a better approximation?","This question came out from this other one : Is there an explicit value for this series? $$\sum_{n=1}^{\infty}\frac{1}{\sqrt{1}+\sqrt{2}+\dots+\sqrt{n}}= \sum_{n=1}^{\infty} \frac{1}{\sum_{i=1}^{n}\sqrt{i}}=\sum_{n=1}^{\infty}\frac{1}{H_{n,-\frac{1}{2}}}$$ As someone pointed out in comments to the other question, this value is a little bit lower than $3.167830$ and it's not far away from $\frac{63}{20}+\frac{7\sqrt{3}}{680}$. Can we go further on this? Can we reach an exact value or a better approximation?",,"['calculus', 'real-analysis', 'sequences-and-series', 'convergence-divergence']"
90,Theorem 6.16 in Baby Rudin: $\int_a^b f d \alpha = \sum_{n=1}^\infty c_n f\left(s_n\right)$,Theorem 6.16 in Baby Rudin:,\int_a^b f d \alpha = \sum_{n=1}^\infty c_n f\left(s_n\right),"Here is Theorem 6.16 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $c_n \geq 0$ for $n = 1, 2, 3, \ldots$, $\sum c_n$ converges, $\left\{ s_n \right\}$ is a sequence of distinct points in $(a, b)$, and    $$\tag{22} \alpha(x) = \sum_{n=1}^\infty c_n I \left(x-s_n \right). $$   Let $f$ be continuous on $[a, b]$. Then    $$\tag{23}  \int_a^b f d \alpha = \sum_{n=1}^\infty c_n f \left( s_n \right). $$ And, here is Rudin's proof: The comparison test shows that the series (22) converges for every $x$. Its sum $\alpha(x)$ is evidently monotonic, and $\alpha(a) = 0$, $\alpha(b) = \sum c_n$. Let $\varepsilon > 0$ be given, and choose $N$ so that    $$ \sum_{N+1}^\infty c_n < \varepsilon. $$   Put    $$ \alpha_1(x) = \sum_{n=1}^N c_n I \left( x-s_n \right), \qquad \alpha_2(x) = \sum_{N+1}^\infty c_n I \left( x - s_n \right). $$   By Theorems 6.12 and 6.15,    $$\tag{24} \int_a^b f d \alpha_1 = \sum_{n=1}^N c_n f \left( s_n \right). $$   Since $\alpha_2(b) - \alpha_2(a) < \varepsilon$,    $$ \tag{25} \left\lvert \int_a^b f d \alpha_2 \right\rvert \leq M \varepsilon, $$   where $M = \sup \lvert f(x) \rvert$. Since $\alpha = \alpha_1 + \alpha_2$, it follows from (24) and (25) that    $$\tag{26} \left\lvert \int_a^b f d\alpha - \sum_{n=1}^N c_n f \left( s_n \right) \right\rvert \leq M \varepsilon.$$    If we let $N \to \infty$, we obtain (23). Here are the links to my earlier posts here on Math SE on Theorems 6.12 and 6.15: Theorem 6.12: Theorem 6.12 (a) in Baby Rudin: $\int_a^b \left( f_1 + f_2 \right) d \alpha=\int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha$ Theorem 6.12 (a) in Baby Rudin: If $f\in\mathscr{R}(\alpha)$ on $[a,b]$, then $cf\in\mathscr{R}(\alpha)$ for every constant $c$ Theorem 6.12 (b) in Baby Rudin: If $f_1 \leq f_2$ on $[a, b]$, then $\int_a^b f_1 d\alpha \leq \int_a^b f_2 d\alpha$ Theorem 6.12 (c) in Baby Rudin: If $f\in\mathscr{R}(\alpha)$ on $[a, b]$ and $a<c<b$, then $f\in\mathscr{R}(\alpha)$ on $[a, c]$ and $[c, b]$ Theorem 6.12 (d) in Baby Rudin: If $\lvert f(x) \rvert \leq M$ on $[a, b]$, then $\lvert \int_a^b f d\alpha \rvert \leq \ldots$ Theorem 6.12 (e) in Baby Rudin: If $f \in \mathscr{R}\left(\alpha_1\right)$ and $f \in \mathscr{R}\left(\alpha_2\right)$, then $\ldots$ Theorem 6.12 (e) in Baby Rudin: If $f \in \mathscr{R}(\alpha)$ and $c > 0$, then $\ldots$ Theorem 6.15: Theorem 6.15 in Baby Rudin: If $a<s<b$, $f$ is bounded on $[a,b]$, $f$ is continuous at $s$, and $\alpha(x)=I(x-s)$, then . . . Finally, here  is Theorem 6.8 in Baby Rudin, 3rd edition: If $f$ is continuous on $[a, b]$, then $f \in \mathscr{R}(\alpha)$ on $[a, b]$. Now here is my account of Rudin's proof: As $c_n \geq 0$, $\sum c_n$ converges, and $0 \leq I \left( x-s_n \right) \leq 1$ for each $n= 1, 2, 3, \ldots$, so the series $\sum c_n I \left( x- s_n \right)$ converges as well. As $a < s_n < b$, so $I \left( a - s_n \right) = 0$ and $I \left( b-s_n \right) =1$ for every $n$ and therefore $\alpha(a) = \sum_{n=1}^\infty c_n I \left( a - s_n \right) = 0$ and $\alpha(b) = \sum_{n=1}^\infty c_n I \left( b-s_n \right) = \sum_{n=1}^\infty c_n$. Moreover, if $a \leq x < y \leq b$, then, for every $n \in \mathbb{N}$, if $s_n < x$, then $s_n < y$ and so if $I \left( x-s_n \right) = 1$, then  $I \left( y-s_n \right) = 1$ also; that is,    $$ I \left( x - s_n \right) \leq I \left( y - s_n \right) \tag{0} $$   for every natural number $n$. And, for every $n \in \mathbb{N}$, since $c_n \geq 0$, therefore    $$ \sum_{k=1}^n c_k I \left( x-s_k \right) \leq  \sum_{k=1}^n c_k I \left( y-s_k \right) $$   for every $n \in \mathbb{N}$, which implies that $$ \begin{align} \alpha(x) &= \sum_{n=1}^\infty c_n I \left( x-s_n \right) \\ &= \lim_{n \to \infty} \sum_{k=1}^n c_k I \left( x-s_k \right) \\ &\leq \lim_{n \to \infty}  \sum_{k=1}^n c_k I \left( y-s_k \right)  \\ &= \sum_{n=1}^\infty c_n I \left( y - s_n \right) \\ &= \alpha(y). \end{align} $$   Thus we have shown that $\alpha$ is a monotonically increasing function defined on $[a, b]$, with $\alpha(a) = 0$ and $\alpha(b) = \sum_{n=1}^\infty c_n$. As $f$ is continuous on $[a, b]$, so $f$ is integrable with respect to $\alpha$ over $[a, b]$, that is,    $ \int_a^b f d \alpha$ exists (in the set $\mathbb{R}$ of real numbers). Now we determine the value of  $\int_a^b f d \alpha$ as follows: Now as $f$ is continuous on the compact set $[a, b]$, $f$ is bounded on $[a, b]$ and so there exists a positive real number $M$ such that    $\lvert f(x) \rvert \leq M$ for all $x \in [a, b]$. Let $\varepsilon > 0$ be given. Let's put $$c \colon= \sum_{n=1}^\infty c_n = \lim_{n \to \infty} \sum_{k=1}^n c_k. $$   Then there is a natural number $N$ such that    $$\tag{1} \left\lvert \sum_{k=1}^n c_k - c \right\rvert < { \varepsilon \over M } $$   for every natural number $n \geq N$. But $c_k \geq 0$ for all $k \in \mathbb{N}$, so the sequence $\left\{ \sum_{k=1}^n c_k \right\}_{n \in \mathbb{N}}$ of the partial sums of the series  $\sum c_n$ is monotonically increasing and therefore    $$ c = \sup \left\{ \ \sum_{k=1}^n c_k \ \colon \ n \in \mathbb{N} \ \right\},$$   which implies that $ \sum_{k=1}^n c_k \leq c$ for every natural number $n$. So (1) takes the form    $$ c - \sum_{k=1}^n c_k < {\varepsilon \over M }$$   for every natural number $n \geq  N$,    which we can write  as    $$ \sum_{k = n+1 }^\infty c_k < { \varepsilon \over M }.  \tag{2} $$    for every natural number $n \geq  N$. Let us fix a natural number $n \geq N$. Now we put    $$ \alpha_1(x) = \sum_{k=1}^n c_k I \left( x - s_k \right), \qquad \alpha_2(x) = \sum_{k= n+1}^\infty c_k I \left( x - s_k \right) $$   for all $x \in [a, b]$. Then    $$ \alpha = \alpha_1 + \alpha_2, \tag{3} $$   and from (0) we can conclude both $\alpha_1$ and $\alpha_2$ are monotonically increasing; furthermore , as $a < s_n < b$ for every natural number $n$, so $I \left( a-s_n \right) = 0$ and $I \left( b - s_n \right) = 1$ and therefore    $$ \begin{align} \alpha_2 (b) - \alpha_2 (a) &= \sum_{k = n+1}^\infty c_k I \left( b - s_k \right) - \sum_{k = n+1 }^\infty c_k I \left( a - s_k \right) \\  &= \sum_{k= n+1 }^\infty c_k I \left( b - s_k \right) - 0 \\ &\leq \sum_{k= n+1}^\infty c_k \\ &< { \varepsilon \over M }. \qquad \mbox{ [ using (2) ] }. \tag{4} \end{align}   $$ Now    $$ \begin{align} \int_a^b f(x) d\alpha_1(x) &= \int_a^b f(x) d\left( \sum_{k=1}^n c_k I \left( x-s_k \right) \right) \\ &= \sum_{k=1}^n \int_a^b f(x) d \left( c_k I \left( x- s_k \right) \right) \qquad \mbox{ [ using Theorem 6.12 (e) ] } \\ &= \sum_{k=1}^n c_k \int_a^b f(x) d \left( I \left( x-s_k \right) \right) \qquad \mbox{ [ using Theorem 6.12 (e) again; $c_n \geq 0$ ] } \\ &= \sum_{k=1}^n c_k f \left( s_k \right). \qquad \mbox{ [ using Theorem 6.15 ] } \end{align} $$   Thus $$ \int_a^b f d \alpha_1 = \sum_{k=1}^n c_k f \left( s_k \right).  \tag{5} $$ Now as $M > 0$ by our assumption and as $\lvert f(x) \rvert \leq M$ for all $x \in [a, b]$, so by Theorem 6.12 (d) in Baby Rudin and by (4) above, we see that    $$ \left\lvert  \int_a^b f d \alpha_2 \right\rvert \leq M \left[ \alpha_2 (b) - \alpha_2(a) \right] < M { \varepsilon \over M } = \varepsilon. \tag{6} $$ Now using (3) and Theorem 6.12 (a) in Baby Rudin, we have    $$ \begin{align} \int_a^b f d \alpha &= \int_a^b f d \left( \alpha_1 + \alpha_2 \right) \\ &=  \int_a^b f d \alpha_1 + \int_a^b f d \alpha_2 \\ &= \sum_{k=1}^n c_k f \left( s_k \right) +  \int_a^b f d \alpha_2, \qquad \mbox{ [ using (5) above ] }    \end{align} $$   Therefore    $$ \int_a^b f d \alpha - \sum_{k=1}^n c_k f \left( s_k \right) = \int_a^b f d \alpha_2,$$   amd hence from (6) we conclude that    $$  \left\lvert  \sum_{k = 1 }^n c_k f \left( s_k \right) \ - \ \int_a^b f d \alpha  \right\rvert = \left\lvert \int_a^b f d \alpha - \sum_{k = 1 }^n c_k f \left( s_k \right) \right\rvert = \left\lvert \int_a^b f d \alpha_2 \right\rvert < \varepsilon. \tag{7}$$ Thus, we have shown that, corresponding to every real number $\varepsilon > 0$, we can find a natural number $N$ such that (7) holds for every natural number $n \geq N$. So the sequence $\left\{ \sum_{k=1}^n c_k f \left( s_k \right) \right\}_{n \in \mathbb{N} }$ of the partial sums of the series $\sum c_n f \left( s_n \right)$    converges. Hence the series $\sum c_n f \left( s_n \right)$ converges, with the sum    $$ \sum_{n=1}^\infty c_n f \left( s_n \right) = \int_a^b f d \alpha,$$   that is,    $$  \int_a^b f d \alpha = \sum_{n=1}^\infty c_n f \left( s_n \right),$$    as required. Is my understanding of Rudin's proof correct and clear enough? If not, then where have I still left the ambiguities? Is it essential that $\left\{ s_n \right\}$ be a sequence of distinct points for the conclusion of this theorem to hold? Apparently, this assumption has not been used, has it?","Here is Theorem 6.16 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $c_n \geq 0$ for $n = 1, 2, 3, \ldots$, $\sum c_n$ converges, $\left\{ s_n \right\}$ is a sequence of distinct points in $(a, b)$, and    $$\tag{22} \alpha(x) = \sum_{n=1}^\infty c_n I \left(x-s_n \right). $$   Let $f$ be continuous on $[a, b]$. Then    $$\tag{23}  \int_a^b f d \alpha = \sum_{n=1}^\infty c_n f \left( s_n \right). $$ And, here is Rudin's proof: The comparison test shows that the series (22) converges for every $x$. Its sum $\alpha(x)$ is evidently monotonic, and $\alpha(a) = 0$, $\alpha(b) = \sum c_n$. Let $\varepsilon > 0$ be given, and choose $N$ so that    $$ \sum_{N+1}^\infty c_n < \varepsilon. $$   Put    $$ \alpha_1(x) = \sum_{n=1}^N c_n I \left( x-s_n \right), \qquad \alpha_2(x) = \sum_{N+1}^\infty c_n I \left( x - s_n \right). $$   By Theorems 6.12 and 6.15,    $$\tag{24} \int_a^b f d \alpha_1 = \sum_{n=1}^N c_n f \left( s_n \right). $$   Since $\alpha_2(b) - \alpha_2(a) < \varepsilon$,    $$ \tag{25} \left\lvert \int_a^b f d \alpha_2 \right\rvert \leq M \varepsilon, $$   where $M = \sup \lvert f(x) \rvert$. Since $\alpha = \alpha_1 + \alpha_2$, it follows from (24) and (25) that    $$\tag{26} \left\lvert \int_a^b f d\alpha - \sum_{n=1}^N c_n f \left( s_n \right) \right\rvert \leq M \varepsilon.$$    If we let $N \to \infty$, we obtain (23). Here are the links to my earlier posts here on Math SE on Theorems 6.12 and 6.15: Theorem 6.12: Theorem 6.12 (a) in Baby Rudin: $\int_a^b \left( f_1 + f_2 \right) d \alpha=\int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha$ Theorem 6.12 (a) in Baby Rudin: If $f\in\mathscr{R}(\alpha)$ on $[a,b]$, then $cf\in\mathscr{R}(\alpha)$ for every constant $c$ Theorem 6.12 (b) in Baby Rudin: If $f_1 \leq f_2$ on $[a, b]$, then $\int_a^b f_1 d\alpha \leq \int_a^b f_2 d\alpha$ Theorem 6.12 (c) in Baby Rudin: If $f\in\mathscr{R}(\alpha)$ on $[a, b]$ and $a<c<b$, then $f\in\mathscr{R}(\alpha)$ on $[a, c]$ and $[c, b]$ Theorem 6.12 (d) in Baby Rudin: If $\lvert f(x) \rvert \leq M$ on $[a, b]$, then $\lvert \int_a^b f d\alpha \rvert \leq \ldots$ Theorem 6.12 (e) in Baby Rudin: If $f \in \mathscr{R}\left(\alpha_1\right)$ and $f \in \mathscr{R}\left(\alpha_2\right)$, then $\ldots$ Theorem 6.12 (e) in Baby Rudin: If $f \in \mathscr{R}(\alpha)$ and $c > 0$, then $\ldots$ Theorem 6.15: Theorem 6.15 in Baby Rudin: If $a<s<b$, $f$ is bounded on $[a,b]$, $f$ is continuous at $s$, and $\alpha(x)=I(x-s)$, then . . . Finally, here  is Theorem 6.8 in Baby Rudin, 3rd edition: If $f$ is continuous on $[a, b]$, then $f \in \mathscr{R}(\alpha)$ on $[a, b]$. Now here is my account of Rudin's proof: As $c_n \geq 0$, $\sum c_n$ converges, and $0 \leq I \left( x-s_n \right) \leq 1$ for each $n= 1, 2, 3, \ldots$, so the series $\sum c_n I \left( x- s_n \right)$ converges as well. As $a < s_n < b$, so $I \left( a - s_n \right) = 0$ and $I \left( b-s_n \right) =1$ for every $n$ and therefore $\alpha(a) = \sum_{n=1}^\infty c_n I \left( a - s_n \right) = 0$ and $\alpha(b) = \sum_{n=1}^\infty c_n I \left( b-s_n \right) = \sum_{n=1}^\infty c_n$. Moreover, if $a \leq x < y \leq b$, then, for every $n \in \mathbb{N}$, if $s_n < x$, then $s_n < y$ and so if $I \left( x-s_n \right) = 1$, then  $I \left( y-s_n \right) = 1$ also; that is,    $$ I \left( x - s_n \right) \leq I \left( y - s_n \right) \tag{0} $$   for every natural number $n$. And, for every $n \in \mathbb{N}$, since $c_n \geq 0$, therefore    $$ \sum_{k=1}^n c_k I \left( x-s_k \right) \leq  \sum_{k=1}^n c_k I \left( y-s_k \right) $$   for every $n \in \mathbb{N}$, which implies that $$ \begin{align} \alpha(x) &= \sum_{n=1}^\infty c_n I \left( x-s_n \right) \\ &= \lim_{n \to \infty} \sum_{k=1}^n c_k I \left( x-s_k \right) \\ &\leq \lim_{n \to \infty}  \sum_{k=1}^n c_k I \left( y-s_k \right)  \\ &= \sum_{n=1}^\infty c_n I \left( y - s_n \right) \\ &= \alpha(y). \end{align} $$   Thus we have shown that $\alpha$ is a monotonically increasing function defined on $[a, b]$, with $\alpha(a) = 0$ and $\alpha(b) = \sum_{n=1}^\infty c_n$. As $f$ is continuous on $[a, b]$, so $f$ is integrable with respect to $\alpha$ over $[a, b]$, that is,    $ \int_a^b f d \alpha$ exists (in the set $\mathbb{R}$ of real numbers). Now we determine the value of  $\int_a^b f d \alpha$ as follows: Now as $f$ is continuous on the compact set $[a, b]$, $f$ is bounded on $[a, b]$ and so there exists a positive real number $M$ such that    $\lvert f(x) \rvert \leq M$ for all $x \in [a, b]$. Let $\varepsilon > 0$ be given. Let's put $$c \colon= \sum_{n=1}^\infty c_n = \lim_{n \to \infty} \sum_{k=1}^n c_k. $$   Then there is a natural number $N$ such that    $$\tag{1} \left\lvert \sum_{k=1}^n c_k - c \right\rvert < { \varepsilon \over M } $$   for every natural number $n \geq N$. But $c_k \geq 0$ for all $k \in \mathbb{N}$, so the sequence $\left\{ \sum_{k=1}^n c_k \right\}_{n \in \mathbb{N}}$ of the partial sums of the series  $\sum c_n$ is monotonically increasing and therefore    $$ c = \sup \left\{ \ \sum_{k=1}^n c_k \ \colon \ n \in \mathbb{N} \ \right\},$$   which implies that $ \sum_{k=1}^n c_k \leq c$ for every natural number $n$. So (1) takes the form    $$ c - \sum_{k=1}^n c_k < {\varepsilon \over M }$$   for every natural number $n \geq  N$,    which we can write  as    $$ \sum_{k = n+1 }^\infty c_k < { \varepsilon \over M }.  \tag{2} $$    for every natural number $n \geq  N$. Let us fix a natural number $n \geq N$. Now we put    $$ \alpha_1(x) = \sum_{k=1}^n c_k I \left( x - s_k \right), \qquad \alpha_2(x) = \sum_{k= n+1}^\infty c_k I \left( x - s_k \right) $$   for all $x \in [a, b]$. Then    $$ \alpha = \alpha_1 + \alpha_2, \tag{3} $$   and from (0) we can conclude both $\alpha_1$ and $\alpha_2$ are monotonically increasing; furthermore , as $a < s_n < b$ for every natural number $n$, so $I \left( a-s_n \right) = 0$ and $I \left( b - s_n \right) = 1$ and therefore    $$ \begin{align} \alpha_2 (b) - \alpha_2 (a) &= \sum_{k = n+1}^\infty c_k I \left( b - s_k \right) - \sum_{k = n+1 }^\infty c_k I \left( a - s_k \right) \\  &= \sum_{k= n+1 }^\infty c_k I \left( b - s_k \right) - 0 \\ &\leq \sum_{k= n+1}^\infty c_k \\ &< { \varepsilon \over M }. \qquad \mbox{ [ using (2) ] }. \tag{4} \end{align}   $$ Now    $$ \begin{align} \int_a^b f(x) d\alpha_1(x) &= \int_a^b f(x) d\left( \sum_{k=1}^n c_k I \left( x-s_k \right) \right) \\ &= \sum_{k=1}^n \int_a^b f(x) d \left( c_k I \left( x- s_k \right) \right) \qquad \mbox{ [ using Theorem 6.12 (e) ] } \\ &= \sum_{k=1}^n c_k \int_a^b f(x) d \left( I \left( x-s_k \right) \right) \qquad \mbox{ [ using Theorem 6.12 (e) again; $c_n \geq 0$ ] } \\ &= \sum_{k=1}^n c_k f \left( s_k \right). \qquad \mbox{ [ using Theorem 6.15 ] } \end{align} $$   Thus $$ \int_a^b f d \alpha_1 = \sum_{k=1}^n c_k f \left( s_k \right).  \tag{5} $$ Now as $M > 0$ by our assumption and as $\lvert f(x) \rvert \leq M$ for all $x \in [a, b]$, so by Theorem 6.12 (d) in Baby Rudin and by (4) above, we see that    $$ \left\lvert  \int_a^b f d \alpha_2 \right\rvert \leq M \left[ \alpha_2 (b) - \alpha_2(a) \right] < M { \varepsilon \over M } = \varepsilon. \tag{6} $$ Now using (3) and Theorem 6.12 (a) in Baby Rudin, we have    $$ \begin{align} \int_a^b f d \alpha &= \int_a^b f d \left( \alpha_1 + \alpha_2 \right) \\ &=  \int_a^b f d \alpha_1 + \int_a^b f d \alpha_2 \\ &= \sum_{k=1}^n c_k f \left( s_k \right) +  \int_a^b f d \alpha_2, \qquad \mbox{ [ using (5) above ] }    \end{align} $$   Therefore    $$ \int_a^b f d \alpha - \sum_{k=1}^n c_k f \left( s_k \right) = \int_a^b f d \alpha_2,$$   amd hence from (6) we conclude that    $$  \left\lvert  \sum_{k = 1 }^n c_k f \left( s_k \right) \ - \ \int_a^b f d \alpha  \right\rvert = \left\lvert \int_a^b f d \alpha - \sum_{k = 1 }^n c_k f \left( s_k \right) \right\rvert = \left\lvert \int_a^b f d \alpha_2 \right\rvert < \varepsilon. \tag{7}$$ Thus, we have shown that, corresponding to every real number $\varepsilon > 0$, we can find a natural number $N$ such that (7) holds for every natural number $n \geq N$. So the sequence $\left\{ \sum_{k=1}^n c_k f \left( s_k \right) \right\}_{n \in \mathbb{N} }$ of the partial sums of the series $\sum c_n f \left( s_n \right)$    converges. Hence the series $\sum c_n f \left( s_n \right)$ converges, with the sum    $$ \sum_{n=1}^\infty c_n f \left( s_n \right) = \int_a^b f d \alpha,$$   that is,    $$  \int_a^b f d \alpha = \sum_{n=1}^\infty c_n f \left( s_n \right),$$    as required. Is my understanding of Rudin's proof correct and clear enough? If not, then where have I still left the ambiguities? Is it essential that $\left\{ s_n \right\}$ be a sequence of distinct points for the conclusion of this theorem to hold? Apparently, this assumption has not been used, has it?",,"['real-analysis', 'integration', 'analysis', 'proof-verification', 'definite-integrals']"
91,Normed vector space inequality $|\|x\|^2 - \|y\|^2| \le \|x-y\|\|x+y\|$,Normed vector space inequality,|\|x\|^2 - \|y\|^2| \le \|x-y\|\|x+y\|,"I'm looking at an old qualifying exam, and one question is to prove the following inequality in any normed vector space: $$ |\|x\|^2 - \|y\|^2| \le \|x-y\|\|x+y\| $$ My initial thought was that $$ |\|x\|^2 - \|y\|^2| = |(\|x\|+\|y\|)(\|x\|-\|y\|)|=\left|(\|x\|+\|y\|)\right||(\|x\|-\|y\|)|,$$ and it's easy to show $|\|x\|-\|y\||$ is less than both $\|x-y\|$ and $\|x+y\|$, but it isn't true that $\|x\|+\|y\|$ is less than either in general (by the triangle inequality it's 'usually' larger than the latter), so I'm unsure what to do. Any guidance is appreciated.","I'm looking at an old qualifying exam, and one question is to prove the following inequality in any normed vector space: $$ |\|x\|^2 - \|y\|^2| \le \|x-y\|\|x+y\| $$ My initial thought was that $$ |\|x\|^2 - \|y\|^2| = |(\|x\|+\|y\|)(\|x\|-\|y\|)|=\left|(\|x\|+\|y\|)\right||(\|x\|-\|y\|)|,$$ and it's easy to show $|\|x\|-\|y\||$ is less than both $\|x-y\|$ and $\|x+y\|$, but it isn't true that $\|x\|+\|y\|$ is less than either in general (by the triangle inequality it's 'usually' larger than the latter), so I'm unsure what to do. Any guidance is appreciated.",,"['real-analysis', 'normed-spaces']"
92,Curve of length $L=1$ contained in a semicircle of diameter $2R=1$.,Curve of length  contained in a semicircle of diameter .,L=1 2R=1,"How prove  that for any curve $\alpha(s)$ of length $L=1$ in the real plane, there is a semicircle of diameter $2R=1$ that contains it. Any hints would be appreciated.","How prove  that for any curve $\alpha(s)$ of length $L=1$ in the real plane, there is a semicircle of diameter $2R=1$ that contains it. Any hints would be appreciated.",,"['real-analysis', 'differential-geometry']"
93,Prove that a countable subset of $\mathbb{R}$ has Lebesgue outer measure zero,Prove that a countable subset of  has Lebesgue outer measure zero,\mathbb{R},"Prove that a countable subset of $\mathbb{R}$ has Lebesgue outer measure zero. I believe I am on the right track but can use some help with this one. Here is my proof thus far: Let $a \in \mathbb{R}$. Then, $\{a\} \subset [a-\epsilon, a+\epsilon]$ holds $\forall \epsilon>0$ and so $\lambda^*(\{a\}) \le \lambda^*([a-\epsilon, a+\epsilon])=2\epsilon \; \forall \epsilon >0$. Therefore, $\lambda^*(\{a\})=0$ holds $\forall a \in \mathbb{R}$. If $A = \{ a_1, a_2,... \} = \bigcup_{n=1}^{\infty} \{a_n\}$ is a countable set, then note that $\lambda^*(A) \le \sum_{n=1}^{\infty} \lambda^*(\{a_n\})=0$ so that $\lambda^*(A)=0$. Is this along the right lines?","Prove that a countable subset of $\mathbb{R}$ has Lebesgue outer measure zero. I believe I am on the right track but can use some help with this one. Here is my proof thus far: Let $a \in \mathbb{R}$. Then, $\{a\} \subset [a-\epsilon, a+\epsilon]$ holds $\forall \epsilon>0$ and so $\lambda^*(\{a\}) \le \lambda^*([a-\epsilon, a+\epsilon])=2\epsilon \; \forall \epsilon >0$. Therefore, $\lambda^*(\{a\})=0$ holds $\forall a \in \mathbb{R}$. If $A = \{ a_1, a_2,... \} = \bigcup_{n=1}^{\infty} \{a_n\}$ is a countable set, then note that $\lambda^*(A) \le \sum_{n=1}^{\infty} \lambda^*(\{a_n\})=0$ so that $\lambda^*(A)=0$. Is this along the right lines?",,"['real-analysis', 'lebesgue-measure']"
94,Why don't fractals have more differentiable symmetries?,Why don't fractals have more differentiable symmetries?,,"Some sets tend not to ""look"" very homogeneous, such as self-similar fractals. I'd like to know why! And there's a particular class of statements that I'm hoping can be made... Definition Let $A$ be a subset of $\mathbb R^n$. Consider two points of $A$ to be equivalent if they have similar neighborhoods up to a differentiable map. Concretely, for $x,y\in A$, write $x\sim y$ if there exists a map $f:A\to A$ and an invertible matrix $T\in\mathbb R^{n\times n}$ such that $f(x)=y$ and $$\lim_{\Delta x\to 0}\frac{f(x+\Delta x) - f(x)-T\Delta x}{|\Delta x|}=0.$$ I require $T$ to be invertible so that infinitesimal neighborhoods of $x$ and $y$ look affinely equivalent. (If we allowed $T$ to be singular, then $f$ could be a constant map, which wouldn't be interesting.) For that matter, let's go ahead and require that $f$ itself is a bijection between a neighborhood of $x$ and a neighborhood of $y$. Example Consider the standard middle-third Cantor set $C\subset[0,1]\subset\mathbb R^1$. This set is topologically homogeneous in the sense that for all $x,y\in C$, there is a homeomorphism $h:C\to C$ such that $h(x)=y$. In fact, if we consider $C$ to be a subset of $\mathbb R^2$, we can find an $h$ that extends to a homeomorphism of all of $\mathbb R^2$, and we can even find an isotopy from the identity on $\mathbb R^2$. So topological methods alone don't seem to be enough to distinguish between points in $C$. Once we start considering differentiable maps, $C$ starts looking less homogeneous. In particular, $0\sim x$ if and only if $x$ has a finite ternary expansion. The equivalence class of $0$ is countable, whereas $C$ is uncountable, so there are relatively few points equivalent to $0$. I suspect that every equivalence class in $C$ is countable, although I've only proven the restricted case of this latter statement when $f$ is required to be an isometry with respect to the natural ultrametric on $C$. I'm looking for a more general theory... Questions Under what conditions on $A$ are we assured that: All points in $A$ are equivalent? For example, it suffices that $A$ is an open set, or that $A$ is an embedded differentiable manifold. All equivalence classes in $A$ are at most countable? I can prove this for sufficiently nice maps $f$ where $A$ is a sufficiently nice embedding of the $p$-adic integers, which is the set that I'm most interested in, but again I'm looking for more general results. Does it suffice that $A$ doesn't contain any differentiable curves? Or that $A$ is nowhere a differentiable image of a product $\mathbb R^k\times B$? At least one equivalence class in $A$ is at most countable? For example, it suffices for $A$ to contain a corner point, as it can only be equivalent to other corner points, and there are at most countably many corners in any subset of $\mathbb R^n$. This generalizes the example of $0\in C$ given above. But some sets in $\mathbb R^n$ for $n\geq2$ don't have corners; what about them? If necessary, you can assume that $A$ is closed, or even that $A$ is a Cantor set. You can also assume that the maps $f$ under consideration must be differentiable on a neighborhood of $x$, or even $C^1$, or that $f$ must be a bijection. A statement like ""If $A$ is a Cantor set then every equivalence class is at most countable"" would be a home run! Is this an easy problem or a hard problem? I don't know what kind of theory the question belongs to, hence the reference-request tag. I've heard of the study of ""analysis on Foo"" where Foo = closed sets, metric spaces, or fractals, but these topics seem to focus on different sorts of questions. Of course, if this is a hard problem, I can also consult MathOverflow. But for all I know, the problem is an easy consequence of Theorem-I've-Never-Heard-Of. (edited inline)","Some sets tend not to ""look"" very homogeneous, such as self-similar fractals. I'd like to know why! And there's a particular class of statements that I'm hoping can be made... Definition Let $A$ be a subset of $\mathbb R^n$. Consider two points of $A$ to be equivalent if they have similar neighborhoods up to a differentiable map. Concretely, for $x,y\in A$, write $x\sim y$ if there exists a map $f:A\to A$ and an invertible matrix $T\in\mathbb R^{n\times n}$ such that $f(x)=y$ and $$\lim_{\Delta x\to 0}\frac{f(x+\Delta x) - f(x)-T\Delta x}{|\Delta x|}=0.$$ I require $T$ to be invertible so that infinitesimal neighborhoods of $x$ and $y$ look affinely equivalent. (If we allowed $T$ to be singular, then $f$ could be a constant map, which wouldn't be interesting.) For that matter, let's go ahead and require that $f$ itself is a bijection between a neighborhood of $x$ and a neighborhood of $y$. Example Consider the standard middle-third Cantor set $C\subset[0,1]\subset\mathbb R^1$. This set is topologically homogeneous in the sense that for all $x,y\in C$, there is a homeomorphism $h:C\to C$ such that $h(x)=y$. In fact, if we consider $C$ to be a subset of $\mathbb R^2$, we can find an $h$ that extends to a homeomorphism of all of $\mathbb R^2$, and we can even find an isotopy from the identity on $\mathbb R^2$. So topological methods alone don't seem to be enough to distinguish between points in $C$. Once we start considering differentiable maps, $C$ starts looking less homogeneous. In particular, $0\sim x$ if and only if $x$ has a finite ternary expansion. The equivalence class of $0$ is countable, whereas $C$ is uncountable, so there are relatively few points equivalent to $0$. I suspect that every equivalence class in $C$ is countable, although I've only proven the restricted case of this latter statement when $f$ is required to be an isometry with respect to the natural ultrametric on $C$. I'm looking for a more general theory... Questions Under what conditions on $A$ are we assured that: All points in $A$ are equivalent? For example, it suffices that $A$ is an open set, or that $A$ is an embedded differentiable manifold. All equivalence classes in $A$ are at most countable? I can prove this for sufficiently nice maps $f$ where $A$ is a sufficiently nice embedding of the $p$-adic integers, which is the set that I'm most interested in, but again I'm looking for more general results. Does it suffice that $A$ doesn't contain any differentiable curves? Or that $A$ is nowhere a differentiable image of a product $\mathbb R^k\times B$? At least one equivalence class in $A$ is at most countable? For example, it suffices for $A$ to contain a corner point, as it can only be equivalent to other corner points, and there are at most countably many corners in any subset of $\mathbb R^n$. This generalizes the example of $0\in C$ given above. But some sets in $\mathbb R^n$ for $n\geq2$ don't have corners; what about them? If necessary, you can assume that $A$ is closed, or even that $A$ is a Cantor set. You can also assume that the maps $f$ under consideration must be differentiable on a neighborhood of $x$, or even $C^1$, or that $f$ must be a bijection. A statement like ""If $A$ is a Cantor set then every equivalence class is at most countable"" would be a home run! Is this an easy problem or a hard problem? I don't know what kind of theory the question belongs to, hence the reference-request tag. I've heard of the study of ""analysis on Foo"" where Foo = closed sets, metric spaces, or fractals, but these topics seem to focus on different sorts of questions. Of course, if this is a hard problem, I can also consult MathOverflow. But for all I know, the problem is an easy consequence of Theorem-I've-Never-Heard-Of. (edited inline)",,"['real-analysis', 'reference-request']"
95,Convergence/Divergence of infinite product,Convergence/Divergence of infinite product,,"General condition: $(P_n)_{n\in\mathbb{N}}$ is a sequence of non-zero real numbers. If $\prod_{n=0}^\infty P_n$ exists in reals and is non-zero , then call this infinite product convergent. Otherwise divergent. I have proven that: If all $(P_n)_{n\in\mathbb{N}}$ are positive, then its convergence is equivalent to convergence of $\sum_{n = 0}^\infty \log(P_n)$. If further $(P_n)_{n\in\mathbb{N}}$ are further greater or equal to 1, then the convergence of the product is equivalent to the convergence of $\sum_{n = 0}^\infty (P_n-1)$ Now I am looking for two examples where a)  $(P_n)_{n\in\mathbb{N}}$ are reals. $\sum_{n = 0}^\infty (P_n-1)$ converges but $\prod_{n=0}^\infty P_n$ diverges b) $(P_n)_{n\in\mathbb{N}}$ are reals. $\prod_{n=0}^\infty P_n$ converges but $\sum_{n = 0}^\infty (P_n-1)$ diverges I have spent a long time on this but failed to find any. I guess it requires complex analysis technique? (Which I don't know) Please help me out. Thank you.","General condition: $(P_n)_{n\in\mathbb{N}}$ is a sequence of non-zero real numbers. If $\prod_{n=0}^\infty P_n$ exists in reals and is non-zero , then call this infinite product convergent. Otherwise divergent. I have proven that: If all $(P_n)_{n\in\mathbb{N}}$ are positive, then its convergence is equivalent to convergence of $\sum_{n = 0}^\infty \log(P_n)$. If further $(P_n)_{n\in\mathbb{N}}$ are further greater or equal to 1, then the convergence of the product is equivalent to the convergence of $\sum_{n = 0}^\infty (P_n-1)$ Now I am looking for two examples where a)  $(P_n)_{n\in\mathbb{N}}$ are reals. $\sum_{n = 0}^\infty (P_n-1)$ converges but $\prod_{n=0}^\infty P_n$ diverges b) $(P_n)_{n\in\mathbb{N}}$ are reals. $\prod_{n=0}^\infty P_n$ converges but $\sum_{n = 0}^\infty (P_n-1)$ diverges I have spent a long time on this but failed to find any. I guess it requires complex analysis technique? (Which I don't know) Please help me out. Thank you.",,"['real-analysis', 'sequences-and-series', 'analysis', 'infinite-product']"
96,"Practical applications of the $L^p$ norm when $p \neq 1,2,\infty$",Practical applications of the  norm when,"L^p p \neq 1,2,\infty","I'm roughly familiar with the concept of $L^p$ norms -- what they represent and how they are computed -- though I am far from educated in functional analysis in general. For reasons that are more or less obvious, in many practical applications, the $L^2$ norm is used (ie, optimization, etc). One also sees the $L^1$ and $L^{\infty}$ norms used from time to time. However, rarely do I see any mention of, say, the $L^3$ norm, or really any $L^p$ norm where $p$ is something other than 1, 2 or $\infty$ (or, in some interpretations, 0). Are there any algorithms or applications that have exploited the other norms? Does convergence in an $L^p$ sense mean something different for these values of $p$? Is there any value of $p$ that can lead to a property not obtainable by $p=1,2,\infty$?","I'm roughly familiar with the concept of $L^p$ norms -- what they represent and how they are computed -- though I am far from educated in functional analysis in general. For reasons that are more or less obvious, in many practical applications, the $L^2$ norm is used (ie, optimization, etc). One also sees the $L^1$ and $L^{\infty}$ norms used from time to time. However, rarely do I see any mention of, say, the $L^3$ norm, or really any $L^p$ norm where $p$ is something other than 1, 2 or $\infty$ (or, in some interpretations, 0). Are there any algorithms or applications that have exploited the other norms? Does convergence in an $L^p$ sense mean something different for these values of $p$? Is there any value of $p$ that can lead to a property not obtainable by $p=1,2,\infty$?",,"['real-analysis', 'functional-analysis', 'optimization', 'numerical-methods']"
97,Asymptotic behavior of iterative sequences,Asymptotic behavior of iterative sequences,,"Suppose you have a sequence $$ a_1<a_2<\cdots $$ with $$ a_{n+1}=a_n+f(a_n) $$ where $f$ is a sufficiently nice nondecreasing function. What can be determined about the asymptotic behavior of $a_n$? For example, suppose $f(x)=\log x+O(1)$ with $f(a_1)>0$. Can we conclude that $a_n\sim Cn\log n$ for some $C$?","Suppose you have a sequence $$ a_1<a_2<\cdots $$ with $$ a_{n+1}=a_n+f(a_n) $$ where $f$ is a sufficiently nice nondecreasing function. What can be determined about the asymptotic behavior of $a_n$? For example, suppose $f(x)=\log x+O(1)$ with $f(a_1)>0$. Can we conclude that $a_n\sim Cn\log n$ for some $C$?",,"['real-analysis', 'sequences-and-series']"
98,Largest $\sigma$-algebra on which an outer measure is countably additive,Largest -algebra on which an outer measure is countably additive,\sigma,"If $m$ is an outer measure on a set $X$, a subset $E$ of $X$ is called $m$-measurable iff  $$ m(A) = m(A \cap E) + m(A \cap E^c) $$ for all subsets $A$ of $X$. The collection $M$ of all $m$-measurable subsets of $X$ forms a $\sigma$-algebra and $m$ is a complete measure when restricted to $M$. Is $M$ the largest $\sigma$-algebra on $X$ on which $m$ is a measure (i.e., on which $m$ is countably additive)?  If not, what is? Is $M$ the largest $\sigma$-algebra on $X$ on which $m$ is a complete measure?  If not, what is? I am especially interested in the case when $X$ is $\mathbb{R}$ or $\mathbb{R}^n$ and $m$ is the Lebesgue outer measure.  In this case $M$ is the Lebesgue $\sigma$-algebra. ADDED: Julián Aguirre (thanks!) has shown in his response below that the answer to the first question is yes when $X$ is $\mathbb{R}^n$ and $m$ is the Lebesgue outer measure.  Hence the answer to the second question in this situation is also yes.","If $m$ is an outer measure on a set $X$, a subset $E$ of $X$ is called $m$-measurable iff  $$ m(A) = m(A \cap E) + m(A \cap E^c) $$ for all subsets $A$ of $X$. The collection $M$ of all $m$-measurable subsets of $X$ forms a $\sigma$-algebra and $m$ is a complete measure when restricted to $M$. Is $M$ the largest $\sigma$-algebra on $X$ on which $m$ is a measure (i.e., on which $m$ is countably additive)?  If not, what is? Is $M$ the largest $\sigma$-algebra on $X$ on which $m$ is a complete measure?  If not, what is? I am especially interested in the case when $X$ is $\mathbb{R}$ or $\mathbb{R}^n$ and $m$ is the Lebesgue outer measure.  In this case $M$ is the Lebesgue $\sigma$-algebra. ADDED: Julián Aguirre (thanks!) has shown in his response below that the answer to the first question is yes when $X$ is $\mathbb{R}^n$ and $m$ is the Lebesgue outer measure.  Hence the answer to the second question in this situation is also yes.",,"['real-analysis', 'analysis', 'measure-theory']"
99,$\infty$-multifactorial: $\displaystyle z!_{(\infty)}:=\lim_{\alpha\to\infty}z!_{(\alpha)}$,-multifactorial:,\infty \displaystyle z!_{(\infty)}:=\lim_{\alpha\to\infty}z!_{(\alpha)},"Introduction Let $z!_{(\alpha)}$ the $\alpha$ -multifactorial. $$z!_{(\alpha)}=\alpha^{\frac{z}{\alpha}}\Gamma\left(1+\frac{z}{\alpha}\right)\prod_{j=1}^{\alpha-1}\left(\frac{\alpha^{\frac{\alpha-j}{\alpha}}}{\Gamma\left(\frac{j}{\alpha}\right)}\right)^{C_{\alpha}(z-j)}$$ Where $$C_{\alpha}(z)=\frac{1}{\alpha}\left(1+2\sum_{k=1}^{\left\lfloor\frac{\alpha-1}{2}\right\rfloor}\cos\left(\frac{2k\pi}{\alpha}z\right)+\text{mod}(\alpha-1,2)\cos(\pi z)\right)$$ My intent I would like to calculate the limit of the multifactorial formula for $\alpha$ which approaches infinity. Leaving aside the various steps that are long and irrelevant to the question I arrived at this formula: $$z!_{(\infty)}=\exp\left(\sum_{j=1}^{\infty}\text{sinc}(z-j)\ln(j)\right)$$ Where $\text{sinc}(x):=\begin{cases}\dfrac{\sin(\pi x)}{\pi x}&\text{if }x\neq0\\1&\text{if }x=0\end{cases}$ Unfortunately I can't move forward with this last sum :/, does anyone have any suggestions? (It would also be acceptable to write it as an integral) At the end of everything, a function with the following characteristics must emerge: $n!_{(\infty)}=n$ for $n\in\mathbb{N}^{+}$ $n!_{(\infty)}=1$ for $n=-1,-2,-3,...$ $\displaystyle\prod_{i=0}^{\infty}(z-i)!_{(\infty)}=z!$ This is the graph: I'll also leave this link which leads to the desmos graph I made in case anyone wants to try their hand at trying to solve it. Update 1 The series is equal to $$z!_{(\infty)}=\exp\left(\frac{\sin(\pi z)}{\pi}\sum_{n=1}^{\infty}(-1)^n\frac{\ln(n)}{z-n}\right)\qquad \text{for }z\not\in\mathbb{N}^{+}$$ I take inspiration from an answer to make a clarification: It would be useful to be able to use a result like this : $$\sum_{k=-\infty}^{\infty}\frac{(-1)^k}{z-k}f(k)=\pi\frac{f(z)}{\sin(\pi z)}$$ Applying this result with $f(n)=\begin{cases}\ln(n)&\text{if }n>1\\0&\text{if }n\leq 1\end{cases}$ the result would be: $$z!_{(\infty)}=\begin{cases}z&\text{if }z>1\\1&\text{if }z\leq 1\end{cases}$$ Unfortunately: In this case it is not applicable because one of the hypotheses requires that $f(n)$ is entire and the logarithm is not (and it can not be continued analytically to an entire function). $z!_{(\infty)}$ is a complex variable function, $\mathbb{C}$ is not ordered so it's not possible to say things like "" $z>1$ "" or "" $z\leq 1$ "" According to this solution for "" $z\leq 1$ "" the series should ALWAYS be $0$ . Update 2 $$x!_{(\infty)}=\exp\left(-\text{sinc}(x)\sum_{n=1}^{\infty}\eta'(n)x^n\right)$$ Where $\eta'(s)$ is the derivative of the Dirichlet Eta function.","Introduction Let the -multifactorial. Where My intent I would like to calculate the limit of the multifactorial formula for which approaches infinity. Leaving aside the various steps that are long and irrelevant to the question I arrived at this formula: Where Unfortunately I can't move forward with this last sum :/, does anyone have any suggestions? (It would also be acceptable to write it as an integral) At the end of everything, a function with the following characteristics must emerge: for for This is the graph: I'll also leave this link which leads to the desmos graph I made in case anyone wants to try their hand at trying to solve it. Update 1 The series is equal to I take inspiration from an answer to make a clarification: It would be useful to be able to use a result like this : Applying this result with the result would be: Unfortunately: In this case it is not applicable because one of the hypotheses requires that is entire and the logarithm is not (and it can not be continued analytically to an entire function). is a complex variable function, is not ordered so it's not possible to say things like "" "" or "" "" According to this solution for "" "" the series should ALWAYS be . Update 2 Where is the derivative of the Dirichlet Eta function.","z!_{(\alpha)} \alpha z!_{(\alpha)}=\alpha^{\frac{z}{\alpha}}\Gamma\left(1+\frac{z}{\alpha}\right)\prod_{j=1}^{\alpha-1}\left(\frac{\alpha^{\frac{\alpha-j}{\alpha}}}{\Gamma\left(\frac{j}{\alpha}\right)}\right)^{C_{\alpha}(z-j)} C_{\alpha}(z)=\frac{1}{\alpha}\left(1+2\sum_{k=1}^{\left\lfloor\frac{\alpha-1}{2}\right\rfloor}\cos\left(\frac{2k\pi}{\alpha}z\right)+\text{mod}(\alpha-1,2)\cos(\pi z)\right) \alpha z!_{(\infty)}=\exp\left(\sum_{j=1}^{\infty}\text{sinc}(z-j)\ln(j)\right) \text{sinc}(x):=\begin{cases}\dfrac{\sin(\pi x)}{\pi x}&\text{if }x\neq0\\1&\text{if }x=0\end{cases} n!_{(\infty)}=n n\in\mathbb{N}^{+} n!_{(\infty)}=1 n=-1,-2,-3,... \displaystyle\prod_{i=0}^{\infty}(z-i)!_{(\infty)}=z! z!_{(\infty)}=\exp\left(\frac{\sin(\pi z)}{\pi}\sum_{n=1}^{\infty}(-1)^n\frac{\ln(n)}{z-n}\right)\qquad \text{for }z\not\in\mathbb{N}^{+} \sum_{k=-\infty}^{\infty}\frac{(-1)^k}{z-k}f(k)=\pi\frac{f(z)}{\sin(\pi z)} f(n)=\begin{cases}\ln(n)&\text{if }n>1\\0&\text{if }n\leq 1\end{cases} z!_{(\infty)}=\begin{cases}z&\text{if }z>1\\1&\text{if }z\leq 1\end{cases} f(n) z!_{(\infty)} \mathbb{C} z>1 z\leq 1 z\leq 1 0 x!_{(\infty)}=\exp\left(-\text{sinc}(x)\sum_{n=1}^{\infty}\eta'(n)x^n\right) \eta'(s)","['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'gamma-function']"
