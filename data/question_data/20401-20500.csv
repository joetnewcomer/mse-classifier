,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"""Rank-K Correction"" of a matrix and significance?","""Rank-K Correction"" of a matrix and significance?",,"Today my studies led me to read about the matrix inversion lemma , which Wikipedia introduces as follows: In mathematics (specifically linear algebra), the Woodbury matrix identity, named after Max A. Woodbury[1][2] says that the inverse of a rank-k correction of some matrix can be computed by doing a rank-k correction to the inverse of the original matrix. After some searching I couldn't find any explanation of the terminology rank-$k$ correction , but I did find a Wikipedia article about the BFGS algorithm that uses the same term: Instead, the Hessian matrix is approximated using rank-one updates specified by gradient evaluations...  the approximate Hessian at stage $k$ is updated by the addition of two matrices $$B_{k+1} = B_k + U_k + V_k$$ Both $U_k$ and $V_k$ are symmetric rank-one matrices... [and together] construct a rank-two update matrix... As far as I can tell from these sources, a rank-$k$ correction of matrix $A$ consists of the addition of a matrix $B$ of rank $k$ to $A$. However, it is unclear to me what exactly the reason is for describing matrix addition in this way.  Can someone help me understand the implications and applications of this terminology at a basic level, as well as its importance to optimization problems?","Today my studies led me to read about the matrix inversion lemma , which Wikipedia introduces as follows: In mathematics (specifically linear algebra), the Woodbury matrix identity, named after Max A. Woodbury[1][2] says that the inverse of a rank-k correction of some matrix can be computed by doing a rank-k correction to the inverse of the original matrix. After some searching I couldn't find any explanation of the terminology rank-$k$ correction , but I did find a Wikipedia article about the BFGS algorithm that uses the same term: Instead, the Hessian matrix is approximated using rank-one updates specified by gradient evaluations...  the approximate Hessian at stage $k$ is updated by the addition of two matrices $$B_{k+1} = B_k + U_k + V_k$$ Both $U_k$ and $V_k$ are symmetric rank-one matrices... [and together] construct a rank-two update matrix... As far as I can tell from these sources, a rank-$k$ correction of matrix $A$ consists of the addition of a matrix $B$ of rank $k$ to $A$. However, it is unclear to me what exactly the reason is for describing matrix addition in this way.  Can someone help me understand the implications and applications of this terminology at a basic level, as well as its importance to optimization problems?",,"['linear-algebra', 'optimization', 'matrix-rank']"
1,"Show that $A$ is symmetric, with $A \in M_n(\mathbb R)$","Show that  is symmetric, with",A A \in M_n(\mathbb R),"Let $A \in M_n(\mathbb R)$. Show that if $A(\,{}^t\!A)A$  is symmetric, then $A$ is also symmetric. My attempt: If $A \in Gl_n(\mathbb{R})$, We have : ${}^t(A^{-1})=(\,{}^t\!A)^{-1}$ Then : $${}^t\!AA\,{}^t\!A= A{}^tAA A\,{}^t\!A=(\,{}^\!tA)^{-1}A\,{}^t\!AA$$ This does not allow me to continue ... I started with $A$ invertible for use after density. However, it may be a bad idea, what do you think? Thank you Edit : ${}^tA$ is the transpose of A.","Let $A \in M_n(\mathbb R)$. Show that if $A(\,{}^t\!A)A$  is symmetric, then $A$ is also symmetric. My attempt: If $A \in Gl_n(\mathbb{R})$, We have : ${}^t(A^{-1})=(\,{}^t\!A)^{-1}$ Then : $${}^t\!AA\,{}^t\!A= A{}^tAA A\,{}^t\!A=(\,{}^\!tA)^{-1}A\,{}^t\!AA$$ This does not allow me to continue ... I started with $A$ invertible for use after density. However, it may be a bad idea, what do you think? Thank you Edit : ${}^tA$ is the transpose of A.",,['linear-algebra']
2,"Vectors, Basis, Dual Vectors, Dual Basis and Tensors","Vectors, Basis, Dual Vectors, Dual Basis and Tensors",,"I'm trying to understand tensors and I know they have something to do with the basis and the dual basis of a vector space and a dual space. First I will give a concrete example to make clear what I want to understand. Let  $$v_1=\left(\begin{array}{cc} 2 \\ 3 \\ \end{array}\right)$$ $$v_2= \left(\begin{array}{cc} 1 \\ 2 \\ \end{array}\right) $$ be a basis for some vectorspace V(over the reals) and let v be in V. $$v=\left(\begin{array}{cc} 4 \\ 2 \\ \end{array}\right) $$ Then calculate: $$\left(\begin{array}{cc} 4 \\ 2 \\ \end{array}\right) = v_1\left(\begin{array}{cc} 2 \\ 3 \\ \end{array}\right)+ v_2\left(\begin{array}{cc} 1 \\ 2 \\ \end{array}\right) $$ In matrix-form: $$\left(\begin{array}{cc} 4 \\ 2 \\ \end{array}\right)= \left(\begin{array}{cc}2&1\\3&2\end{array}\right)\left(\begin{array}{cc} v_1 \\ v_2 \\ \end{array}\right) $$ With solution: $$\left(\begin{array}{cc} v_1 \\ v_2 \\ \end{array}\right)= \left(\begin{array}{cc} 6 \\ -8 \\ \end{array}\right)$$ So I have two transformations: 1. $$\left(\begin{array}{cc} 4 \\ 2 \\ \end{array}\right)= \left(\begin{array}{cc}2&1\\3&2\end{array}\right)\left(\begin{array}{cc} 6 \\ -8 \\ \end{array}\right)$$ 2.With the inverse matrix ! $$\left(\begin{array}{cc}2&-1\\-3&2\end{array}\right)\left(\begin{array}{cc} 4 \\ 2 \\ \end{array}\right)= \left(\begin{array}{cc} 6 \\ -8 \\ \end{array}\right)$$ Now I know that the dual basis is just the rows of the inverse matrix: $$ B^* = (2, -1), (-3, 2)$$ which gives the Kronecker delta when multiplied in matrix form with it's ""not inverse"" matrix. For simplicity let's write: $$S = \left(\begin{array}{cc}2&1\\3&2\end{array}\right)$$ $$S^{-1} = \left(\begin{array}{cc}2&-1\\-3&2\end{array}\right) = T $$ So we can write it as this: 1. $$\left(\begin{array}{cc} 4 \\ 2 \\ \end{array}\right)= S\left(\begin{array}{cc} 6 \\ -8 \\ \end{array}\right)$$ 2. $$T\left(\begin{array}{cc} 4 \\ 2 \\ \end{array}\right)= \left(\begin{array}{cc} 6 \\ -8 \\ \end{array}\right)$$ Now I'm almost there to get a tensor I think. But I'm confused with all the bases and transformations. What role does the dual basis play in this? And how can I construct a tensor out of these bases? Just glue all the bases together, then dual bases indices up and normal bases indices down ? I think what would help me is when someone could show me how to do this procedure above, but with covectors(other name for dual vectors) instead of vectors. For example, let's have a dual basis and some covector and then find the coefficients with the inverse of the dual basis and so on. And I would appreciate a concrete example similar to mine above.Thanks for response ! Happy Holidays","I'm trying to understand tensors and I know they have something to do with the basis and the dual basis of a vector space and a dual space. First I will give a concrete example to make clear what I want to understand. Let  $$v_1=\left(\begin{array}{cc} 2 \\ 3 \\ \end{array}\right)$$ $$v_2= \left(\begin{array}{cc} 1 \\ 2 \\ \end{array}\right) $$ be a basis for some vectorspace V(over the reals) and let v be in V. $$v=\left(\begin{array}{cc} 4 \\ 2 \\ \end{array}\right) $$ Then calculate: $$\left(\begin{array}{cc} 4 \\ 2 \\ \end{array}\right) = v_1\left(\begin{array}{cc} 2 \\ 3 \\ \end{array}\right)+ v_2\left(\begin{array}{cc} 1 \\ 2 \\ \end{array}\right) $$ In matrix-form: $$\left(\begin{array}{cc} 4 \\ 2 \\ \end{array}\right)= \left(\begin{array}{cc}2&1\\3&2\end{array}\right)\left(\begin{array}{cc} v_1 \\ v_2 \\ \end{array}\right) $$ With solution: $$\left(\begin{array}{cc} v_1 \\ v_2 \\ \end{array}\right)= \left(\begin{array}{cc} 6 \\ -8 \\ \end{array}\right)$$ So I have two transformations: 1. $$\left(\begin{array}{cc} 4 \\ 2 \\ \end{array}\right)= \left(\begin{array}{cc}2&1\\3&2\end{array}\right)\left(\begin{array}{cc} 6 \\ -8 \\ \end{array}\right)$$ 2.With the inverse matrix ! $$\left(\begin{array}{cc}2&-1\\-3&2\end{array}\right)\left(\begin{array}{cc} 4 \\ 2 \\ \end{array}\right)= \left(\begin{array}{cc} 6 \\ -8 \\ \end{array}\right)$$ Now I know that the dual basis is just the rows of the inverse matrix: $$ B^* = (2, -1), (-3, 2)$$ which gives the Kronecker delta when multiplied in matrix form with it's ""not inverse"" matrix. For simplicity let's write: $$S = \left(\begin{array}{cc}2&1\\3&2\end{array}\right)$$ $$S^{-1} = \left(\begin{array}{cc}2&-1\\-3&2\end{array}\right) = T $$ So we can write it as this: 1. $$\left(\begin{array}{cc} 4 \\ 2 \\ \end{array}\right)= S\left(\begin{array}{cc} 6 \\ -8 \\ \end{array}\right)$$ 2. $$T\left(\begin{array}{cc} 4 \\ 2 \\ \end{array}\right)= \left(\begin{array}{cc} 6 \\ -8 \\ \end{array}\right)$$ Now I'm almost there to get a tensor I think. But I'm confused with all the bases and transformations. What role does the dual basis play in this? And how can I construct a tensor out of these bases? Just glue all the bases together, then dual bases indices up and normal bases indices down ? I think what would help me is when someone could show me how to do this procedure above, but with covectors(other name for dual vectors) instead of vectors. For example, let's have a dual basis and some covector and then find the coefficients with the inverse of the dual basis and so on. And I would appreciate a concrete example similar to mine above.Thanks for response ! Happy Holidays",,"['linear-algebra', 'functional-analysis', 'differential-geometry', 'vector-spaces', 'tensors']"
3,"If $u_i$ are affinely independent, are they also linearly independent?","If  are affinely independent, are they also linearly independent?",u_i,"I am wondering about affinely independent and just linearly independent. On Wikipedia it is explained that $u_i$ are affinely independent if $u_1 - u_0, ...,u_k -u_0$ are linearly independent. It is clear that if $u_i$ are linearly independent then $u_1 - u_0, ...,u_k -u_0$ are linearly independent. Is the other implication not also true? Then what is the difference between the two definitions?","I am wondering about affinely independent and just linearly independent. On Wikipedia it is explained that $u_i$ are affinely independent if $u_1 - u_0, ...,u_k -u_0$ are linearly independent. It is clear that if $u_i$ are linearly independent then $u_1 - u_0, ...,u_k -u_0$ are linearly independent. Is the other implication not also true? Then what is the difference between the two definitions?",,"['linear-algebra', 'affine-geometry']"
4,Choosing a linear map $(\mathbb{Z}/2\mathbb{Z})^n \rightarrow \mathbb{Z}/2\mathbb{Z}$ which is nonzero on half of a sequence of vectors,Choosing a linear map  which is nonzero on half of a sequence of vectors,(\mathbb{Z}/2\mathbb{Z})^n \rightarrow \mathbb{Z}/2\mathbb{Z},"Let $v_1,\ldots,v_m \in (\mathbb{Z}/2\mathbb{Z})^n$ be nonzero vectors.  Is it always possible to choose a linear map $f : (\mathbb{Z}/2\mathbb{Z})^n \rightarrow \mathbb{Z}/2\mathbb{Z}$ such that $f$ is nonzero on at least half of the $v_i$, i.e. such that $$|\{\text{$i$ $|$ $1 \leq i \leq m$ and $f(v_i) \neq 0$}\}| \geq \frac{1}{2}m?$$ My guess is that the answer is yes; at the very least, it is true for $m=1$ and when the $v_i$ enumerate all of the nonzero vectors.","Let $v_1,\ldots,v_m \in (\mathbb{Z}/2\mathbb{Z})^n$ be nonzero vectors.  Is it always possible to choose a linear map $f : (\mathbb{Z}/2\mathbb{Z})^n \rightarrow \mathbb{Z}/2\mathbb{Z}$ such that $f$ is nonzero on at least half of the $v_i$, i.e. such that $$|\{\text{$i$ $|$ $1 \leq i \leq m$ and $f(v_i) \neq 0$}\}| \geq \frac{1}{2}m?$$ My guess is that the answer is yes; at the very least, it is true for $m=1$ and when the $v_i$ enumerate all of the nonzero vectors.",,"['linear-algebra', 'combinatorics']"
5,What's the point of duality?,What's the point of duality?,,"I'm taking a second course in linear algebra. Duality was discussed in the early part of the course. But I don't see any significance of it. It seems to be an isolated topic, and it hasn't been mentioned anymore. So what's exactly the point of duality?","I'm taking a second course in linear algebra. Duality was discussed in the early part of the course. But I don't see any significance of it. It seems to be an isolated topic, and it hasn't been mentioned anymore. So what's exactly the point of duality?",,['linear-algebra']
6,Find an equation of the plane passing through 2 points and perpendicular to another plane,Find an equation of the plane passing through 2 points and perpendicular to another plane,,"Find an equation of the plane that passes through the points $(0-2,5)$ and $(-1,3,1)$ and is perpendicular to the plane $2z = 5x + 4y$. Here's what I have so far: The plane through $(0,-2,5)$ is $ax + b(y+z) + c(z-5) = 0$.    And the plane also passes through $(-1,3,1)$ so I get: $$-a + 5b - 4c = 0 \tag{1}.$$ When I looked at the explanation it says: Now we know that the plane is perpendicular to $5x + 4y - 2z = 0$ and then it replaces $(x,y,z)$ with $(a,b,c)$ to get $$5a + 4b - 2c = 0. \tag{2}$$ It continues from there saying to solve the two equations to get $\frac{a}{6} = \frac{b}{-22} = \frac{c}{-29}$. I know how to solve it once it gets to this but I have absolutely no idea how they got to this step.","Find an equation of the plane that passes through the points $(0-2,5)$ and $(-1,3,1)$ and is perpendicular to the plane $2z = 5x + 4y$. Here's what I have so far: The plane through $(0,-2,5)$ is $ax + b(y+z) + c(z-5) = 0$.    And the plane also passes through $(-1,3,1)$ so I get: $$-a + 5b - 4c = 0 \tag{1}.$$ When I looked at the explanation it says: Now we know that the plane is perpendicular to $5x + 4y - 2z = 0$ and then it replaces $(x,y,z)$ with $(a,b,c)$ to get $$5a + 4b - 2c = 0. \tag{2}$$ It continues from there saying to solve the two equations to get $\frac{a}{6} = \frac{b}{-22} = \frac{c}{-29}$. I know how to solve it once it gets to this but I have absolutely no idea how they got to this step.",,['linear-algebra']
7,Closedness of sets under linear transformation,Closedness of sets under linear transformation,,"Let $Y$ be a closed subset of $\mathbb{R}^m$ (in fact $Y$ is convex and compact, but I think the extra assumptions are irrelevant). Let $A \in \mathbb{R}^{n \times n}$ be a non-singular matrix (so $A^{-1}$ exists). Let $C \in \mathbb{R}^{m \times n}$ be any matrix. Is the set $$ Y' = \{ C A x \in \mathbb{R}^m \, : \, x \in \mathbb{R}^n, C x \in Y\} $$ also closed? Note: just to be clear, the definition of $Y'$ means $Y' = C A X = \{ C A x \in \mathbb{R}^m \, : \, x \in X \}$ where $X = \{ x \in \mathbb{R}^n \, : \, C x \in Y \}$.","Let $Y$ be a closed subset of $\mathbb{R}^m$ (in fact $Y$ is convex and compact, but I think the extra assumptions are irrelevant). Let $A \in \mathbb{R}^{n \times n}$ be a non-singular matrix (so $A^{-1}$ exists). Let $C \in \mathbb{R}^{m \times n}$ be any matrix. Is the set $$ Y' = \{ C A x \in \mathbb{R}^m \, : \, x \in \mathbb{R}^n, C x \in Y\} $$ also closed? Note: just to be clear, the definition of $Y'$ means $Y' = C A X = \{ C A x \in \mathbb{R}^m \, : \, x \in X \}$ where $X = \{ x \in \mathbb{R}^n \, : \, C x \in Y \}$.",,"['linear-algebra', 'general-topology']"
8,Finding the Dual Basis,Finding the Dual Basis,,"Define the four vectors in $\mathbb{R}^4$ by $$v_1=\left( \begin{array}{ccc} 1 \\ 0 \\ 0 \\ 0 \end{array} \right), v_2=\left( \begin{array}{ccc} 1 \\ 1 \\ 0 \\ 0 \end{array} \right), v_3=\left( \begin{array}{ccc} 1 \\ 1 \\ 1 \\ 0 \end{array} \right), v_4=\left( \begin{array}{ccc} 1 \\ 1 \\ 1 \\ 1 \end{array} \right). $$ I'm now asked to find the basis dual to $\{v_1,v_2,v_3,v_4 \}$ in $\mathbb{R}^4$, wth each vector expressed as a linear combination of the standard basis in $\mathbb{R}^4$. Now, this is one of those situations where I 'know' all of the bookwork regarding dual bases etc. however, what seems like a simple application presents quite a hurdle. Any explanation of how to progress would be very appreciated.","Define the four vectors in $\mathbb{R}^4$ by $$v_1=\left( \begin{array}{ccc} 1 \\ 0 \\ 0 \\ 0 \end{array} \right), v_2=\left( \begin{array}{ccc} 1 \\ 1 \\ 0 \\ 0 \end{array} \right), v_3=\left( \begin{array}{ccc} 1 \\ 1 \\ 1 \\ 0 \end{array} \right), v_4=\left( \begin{array}{ccc} 1 \\ 1 \\ 1 \\ 1 \end{array} \right). $$ I'm now asked to find the basis dual to $\{v_1,v_2,v_3,v_4 \}$ in $\mathbb{R}^4$, wth each vector expressed as a linear combination of the standard basis in $\mathbb{R}^4$. Now, this is one of those situations where I 'know' all of the bookwork regarding dual bases etc. however, what seems like a simple application presents quite a hurdle. Any explanation of how to progress would be very appreciated.",,"['linear-algebra', 'transformation']"
9,Basis-free formulation of Jordan normal form theorem,Basis-free formulation of Jordan normal form theorem,,"Is there a basis-free formulation of Jordan normal form theorem? From some search I did in Google, the answer is apparently yes. But I didn't find any article that I could understand. (I've only taken two semester course in linear algebra.) My curiosity comes from the question whether the theorem can be generalized to infinite dimensional situation. If it's a separable Hilbert space, we can still represent the linear operator as a matrix, but does the theorem remain true? In case of non-separable space, I think there's no way to put the linear operator in matrix form. So we need to find a basis-free formulation. Wikipedia says that there is an analogue of Jordan normal form theorem for compact operators in Banach space. What is this analogous result?","Is there a basis-free formulation of Jordan normal form theorem? From some search I did in Google, the answer is apparently yes. But I didn't find any article that I could understand. (I've only taken two semester course in linear algebra.) My curiosity comes from the question whether the theorem can be generalized to infinite dimensional situation. If it's a separable Hilbert space, we can still represent the linear operator as a matrix, but does the theorem remain true? In case of non-separable space, I think there's no way to put the linear operator in matrix form. So we need to find a basis-free formulation. Wikipedia says that there is an analogue of Jordan normal form theorem for compact operators in Banach space. What is this analogous result?",,['linear-algebra']
10,annihilator linear algebra,annihilator linear algebra,,"I have a problem as follows: $W_1$ and $W_2$ are subspaces of a finite-dimensional vector space $V$. $W^0$ is the annihilator of $W$. (a) Prove $(W_1 + W_2)^0 = W_1^0 \cap W_2^0$. (b) Prove $(W_1 \cap W_2)^0 = W_1^0 + W_2^0$. Thoughts so far: By definition of Annihilator,  $W^0$ is the set of linear functionals that vanish on $W$. I feel that I also should use dual, but not sure how to put things together.","I have a problem as follows: $W_1$ and $W_2$ are subspaces of a finite-dimensional vector space $V$. $W^0$ is the annihilator of $W$. (a) Prove $(W_1 + W_2)^0 = W_1^0 \cap W_2^0$. (b) Prove $(W_1 \cap W_2)^0 = W_1^0 + W_2^0$. Thoughts so far: By definition of Annihilator,  $W^0$ is the set of linear functionals that vanish on $W$. I feel that I also should use dual, but not sure how to put things together.",,['linear-algebra']
11,Orthogonal planes in n-dimensions,Orthogonal planes in n-dimensions,,"In 3D two planes are orthogonal when their normal vectors are orthogonal (their inner product is zero). For example, planes $xy$ and $xz$ are orthogonal because their normal vectors $\hat{z}$ and $\hat{y}$ respectively are orthogonal, i.e $\hat{z}\cdot \hat{y}=0$ . How we define orthogonality of planes in $n$ dimensions? I am talking about 2d planes through the origin, in n-dimensional Euclidean space, that are specified by orthonormal vectors $\hat{x}_1, \hat{x}_2,.., \hat{x}_n$ . In 4D we have four orthogonal axes x,y,z,w defined by normal vectors $\hat{x}, \hat{y}, \hat{z}, \hat{w}$ . However these axes make six planes: $xy, xz, xw, yz, yw, zw$ . Are these planes orthogonal to each other? For example, the normal vectors $\hat{z}$ and $\hat{w}$ are perpendicular to the plane $xy$ , but they are orthogonal, i.e $\hat{z}\cdot \hat{w}=0$ , not parallel. How it is possible that they are not parallel when they are perpendicular to the same plane and how we check if the plane $xy$ is orthogonal to the plane $wz$ ?","In 3D two planes are orthogonal when their normal vectors are orthogonal (their inner product is zero). For example, planes and are orthogonal because their normal vectors and respectively are orthogonal, i.e . How we define orthogonality of planes in dimensions? I am talking about 2d planes through the origin, in n-dimensional Euclidean space, that are specified by orthonormal vectors . In 4D we have four orthogonal axes x,y,z,w defined by normal vectors . However these axes make six planes: . Are these planes orthogonal to each other? For example, the normal vectors and are perpendicular to the plane , but they are orthogonal, i.e , not parallel. How it is possible that they are not parallel when they are perpendicular to the same plane and how we check if the plane is orthogonal to the plane ?","xy xz \hat{z} \hat{y} \hat{z}\cdot \hat{y}=0 n \hat{x}_1, \hat{x}_2,.., \hat{x}_n \hat{x}, \hat{y}, \hat{z}, \hat{w} xy, xz, xw, yz, yw, zw \hat{z} \hat{w} xy \hat{z}\cdot \hat{w}=0 xy wz","['linear-algebra', 'geometry']"
12,A pen-and-paper proof for a matrix implication.,A pen-and-paper proof for a matrix implication.,,"Suppose $A = \begin{bmatrix} x & 1\\ y & 0\end{bmatrix}, B = \begin{bmatrix} z & 1\\ w & 0\end{bmatrix}$ , for $x,y,z,w \in \Bbb{R}$ . I have observed by considering many  examples of $x,y,z,w$ that: If all the eigen values of $A^2B$ and $AB^2$ are less than one in absolute value $\implies$ $\det(AB+A+I)<0$ and $\det(BA+B+I)<0$ is not possible. OR alternatively, If all the eigen values of $A^2B$ and $AB^2$ are less than one in absolute value $\implies$ $\det(AB+A+I)\ge 0$ OR $\det(BA+B+I)\ge 0$ I wonder how to prove it actually? A computational proof using computer package was shown in https://mathoverflow.net/questions/435267/proof-of-a-matrix-implication/435689#435689 But I am wondering about formal or analytical proof for this question which can be done using pen paper. EDIT The case of $y=x$ or $z=w$ is covered by Andreas as an answer below. The only case that remains to show is whether the conjecture still holds for $y\neq x$ or $w \neq z$ .","Suppose , for . I have observed by considering many  examples of that: If all the eigen values of and are less than one in absolute value and is not possible. OR alternatively, If all the eigen values of and are less than one in absolute value OR I wonder how to prove it actually? A computational proof using computer package was shown in https://mathoverflow.net/questions/435267/proof-of-a-matrix-implication/435689#435689 But I am wondering about formal or analytical proof for this question which can be done using pen paper. EDIT The case of or is covered by Andreas as an answer below. The only case that remains to show is whether the conjecture still holds for or .","A = \begin{bmatrix} x & 1\\ y & 0\end{bmatrix}, B = \begin{bmatrix} z & 1\\ w & 0\end{bmatrix} x,y,z,w \in \Bbb{R} x,y,z,w A^2B AB^2 \implies \det(AB+A+I)<0 \det(BA+B+I)<0 A^2B AB^2 \implies \det(AB+A+I)\ge 0 \det(BA+B+I)\ge 0 y=x z=w y\neq x w \neq z","['linear-algebra', 'inequality', 'contest-math', 'determinant']"
13,When is the projection of an ellipsoid a circle?,When is the projection of an ellipsoid a circle?,,"Consider an ellipsoid in the three dimensional Euclidean space, say $$\frac{x^2}{a^2}+\frac{y^2}{b^2} + \frac{z^2}{c^2} =1 $$ where $a$ , $b$ , $c$ are positive reals. I'm counting the number of planes through the origin so that the image is a perfect circle. There may be divergent cases if we consider the case that some of $a$ , $b$ , $c$ are coincide. But at first, let us focus on the case that $a$ , $b$ , $c$ are all different, say $a>b>c$ . I guess the answer would be $4$ . I have made many efforts but failed. What I have observed is the that at least two such planes exists and the radius of the circle is $b$ . Just consider rotating plane possesses $y$ axis and apply intermediate value theorem. Causion! We are concerning projection, not intersection. PS. Now I guess there are infinitely many... PS2. According to one suggested answer, there are just two such planes for the non-degenerate case. I'm checking if it is correct. PS3. Another opinion appeared that the selected answer may have fault. And it seems making sense. I think somewhat stronger analysis is required. PS4. The above PS3 is about another answer which now have disappeared.","Consider an ellipsoid in the three dimensional Euclidean space, say where , , are positive reals. I'm counting the number of planes through the origin so that the image is a perfect circle. There may be divergent cases if we consider the case that some of , , are coincide. But at first, let us focus on the case that , , are all different, say . I guess the answer would be . I have made many efforts but failed. What I have observed is the that at least two such planes exists and the radius of the circle is . Just consider rotating plane possesses axis and apply intermediate value theorem. Causion! We are concerning projection, not intersection. PS. Now I guess there are infinitely many... PS2. According to one suggested answer, there are just two such planes for the non-degenerate case. I'm checking if it is correct. PS3. Another opinion appeared that the selected answer may have fault. And it seems making sense. I think somewhat stronger analysis is required. PS4. The above PS3 is about another answer which now have disappeared.",\frac{x^2}{a^2}+\frac{y^2}{b^2} + \frac{z^2}{c^2} =1  a b c a b c a b c a>b>c 4 b y,"['linear-algebra', 'euclidean-geometry']"
14,Does in plane exist $22$ points and $22$ such circles that each circle contains at least $7$ points and each point is on at least $7$ circles.,Does in plane exist  points and  such circles that each circle contains at least  points and each point is on at least  circles.,22 22 7 7,"Does in plane exist $22$ points and $22$ such circles that each circle contains at least $7$ points and each point is on at least $7$ circles. I have solved this one but now I can't remember how I did it. I just remember that I used some linear algebra and double counting. Suppose each point $P_i\in \{P_1,P_2,...P_{22}\}$ is on $p_i\geq 7$ circles among circles $C_1,C_2,...,C_{22}$ . Since each pair $\{C_i,C_j\}$ share at most $2$ points and each point is on $\displaystyle{p_i\choose 2}$ pair of circles, we have: $$ 2\cdot {22\choose 2} \geq \sum _{i=1}^{22} {p_i\choose 2} \geq 22 {7\choose 2} $$ Since we must have all equalities we deduce that $p_i = 7$ for all $i$ and each pair of circles intersect in exactly two points. Now since $$ 22\cdot 7 \leq \sum _{i=1}^{22} |C_i| =  \sum _{i=1}^{22}p_i = 22\cdot 7$$ so each circle contains exactly $7$ points.","Does in plane exist points and such circles that each circle contains at least points and each point is on at least circles. I have solved this one but now I can't remember how I did it. I just remember that I used some linear algebra and double counting. Suppose each point is on circles among circles . Since each pair share at most points and each point is on pair of circles, we have: Since we must have all equalities we deduce that for all and each pair of circles intersect in exactly two points. Now since so each circle contains exactly points.","22 22 7 7 P_i\in \{P_1,P_2,...P_{22}\} p_i\geq 7 C_1,C_2,...,C_{22} \{C_i,C_j\} 2 \displaystyle{p_i\choose 2}  2\cdot {22\choose 2} \geq \sum _{i=1}^{22} {p_i\choose 2} \geq 22 {7\choose 2}  p_i = 7 i  22\cdot 7 \leq \sum _{i=1}^{22} |C_i| =  \sum _{i=1}^{22}p_i = 22\cdot 7 7","['linear-algebra', 'combinatorics', 'matrices', 'contest-math', 'algebraic-combinatorics']"
15,What's the equation for a line segment?,What's the equation for a line segment?,,"I already know that the standard equation for a line is $y=mx+b$, but what if I want the line to have specific endpoints and not go on forever? For example, the equation for a line beginning at $(3, 1)$ and ending at $(7, 2)$. Can you help me? What's the standard equation for this?","I already know that the standard equation for a line is $y=mx+b$, but what if I want the line to have specific endpoints and not go on forever? For example, the equation for a line beginning at $(3, 1)$ and ending at $(7, 2)$. Can you help me? What's the standard equation for this?",,"['linear-algebra', 'geometry']"
16,$P+Q-PQ$ is a projection if and only if $PQ=QP$.,is a projection if and only if .,P+Q-PQ PQ=QP,"Let $\mathcal H$ is a Hilbert space and $P,Q:\mathcal H \to \mathcal H$ are projections. I want to show that $P+Q-PQ$ is a projection if and only if $PQ=QP$. If $PQ=QP$ clearly $P+Q-PQ$ is a projection. But when $P+Q-PQ$ is a projection, how I can show that $PQ=QP$? I tried to use $$(P+Q-PQ)^2=P+Q-PQ$$ And I got: $$QP-QPQ-PQP+(PQ)^2=0$$ And from here: $$(Q-PQ)(P-PQ)=0$$ Can somebody give me a hint? Thanx in advance.","Let $\mathcal H$ is a Hilbert space and $P,Q:\mathcal H \to \mathcal H$ are projections. I want to show that $P+Q-PQ$ is a projection if and only if $PQ=QP$. If $PQ=QP$ clearly $P+Q-PQ$ is a projection. But when $P+Q-PQ$ is a projection, how I can show that $PQ=QP$? I tried to use $$(P+Q-PQ)^2=P+Q-PQ$$ And I got: $$QP-QPQ-PQP+(PQ)^2=0$$ And from here: $$(Q-PQ)(P-PQ)=0$$ Can somebody give me a hint? Thanx in advance.",,"['linear-algebra', 'functional-analysis', 'operator-theory', 'hilbert-spaces']"
17,Why are inner product spaces only defined on $\Bbb R$ or $\Bbb C$?,Why are inner product spaces only defined on  or ?,\Bbb R \Bbb C,"A vector space $V$ makes sense over any field $F$, or even a division ring. So why does adding an inner product suddenly not make sense without taking the $F=\Bbb R$ or $\Bbb C$? What are the primary properties we want in our scalar field that forces it to be one of these two? (In particular I don't see why completeness of the scalar field is necessary in a pre -Hilbert space, but even in a Hilbert space completeness of the vector space does not imply completeness of the scalar field.) Do any problems arise when taking $F$ to be an involutive field with an absolute value satifying $|x^*|=|x|$ and defining $||x||=\sqrt{|\langle x,x\rangle|}$? Or, if we stick to the standard definition $||x||=\sqrt{\langle x,x\rangle}$ (which only makes sense when $F$ has a subfield $K$, identified with a subfield of $\Bbb R$, such that $\langle x,x\rangle\in K$ for all $x\in V$), what problems arise if $F$ is not complete, or at least quadratically complete? (Note that the expression $\sqrt{\langle x,x\rangle}$ is evaluated in $\Bbb R$, not $K$.)","A vector space $V$ makes sense over any field $F$, or even a division ring. So why does adding an inner product suddenly not make sense without taking the $F=\Bbb R$ or $\Bbb C$? What are the primary properties we want in our scalar field that forces it to be one of these two? (In particular I don't see why completeness of the scalar field is necessary in a pre -Hilbert space, but even in a Hilbert space completeness of the vector space does not imply completeness of the scalar field.) Do any problems arise when taking $F$ to be an involutive field with an absolute value satifying $|x^*|=|x|$ and defining $||x||=\sqrt{|\langle x,x\rangle|}$? Or, if we stick to the standard definition $||x||=\sqrt{\langle x,x\rangle}$ (which only makes sense when $F$ has a subfield $K$, identified with a subfield of $\Bbb R$, such that $\langle x,x\rangle\in K$ for all $x\in V$), what problems arise if $F$ is not complete, or at least quadratically complete? (Note that the expression $\sqrt{\langle x,x\rangle}$ is evaluated in $\Bbb R$, not $K$.)",,"['linear-algebra', 'abstract-algebra', 'vector-spaces', 'hilbert-spaces', 'inner-products']"
18,Convergence of QR algorithm to upper triangular matrix,Convergence of QR algorithm to upper triangular matrix,,"Sorry for asking really silly question. I guess the answer will be very simple. The question I am doing is: Does QR method always converge to a upper triangular matrix? I think the answer is not. And I guess $$A= \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}$$ And I have $Q_k R_k =A_k$ (QR factorization) and $A_{k+1}=R_k Q_k$. But if my calculations are not wrong, I have all $A_k=A$, which is not upper trianguler. But my professor had a theorem (and he didn't prove it) that any real symmetric non-singular matrix $A$ will converge to upper triangular form and the diagonal entries converges to eigenvalues. And in my case, $A$ doesn't converge, and the entries (even not the diagonal ones) don't match the eigenvalues ($1$ and $-1$) of $A$. So, have I made any silly mistakes? Thank you.","Sorry for asking really silly question. I guess the answer will be very simple. The question I am doing is: Does QR method always converge to a upper triangular matrix? I think the answer is not. And I guess $$A= \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}$$ And I have $Q_k R_k =A_k$ (QR factorization) and $A_{k+1}=R_k Q_k$. But if my calculations are not wrong, I have all $A_k=A$, which is not upper trianguler. But my professor had a theorem (and he didn't prove it) that any real symmetric non-singular matrix $A$ will converge to upper triangular form and the diagonal entries converges to eigenvalues. And in my case, $A$ doesn't converge, and the entries (even not the diagonal ones) don't match the eigenvalues ($1$ and $-1$) of $A$. So, have I made any silly mistakes? Thank you.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
19,Singular values of a matrix written in controllable canonical form,Singular values of a matrix written in controllable canonical form,,"Let the following equation represent a stable (marginally) dynamical system in discrete time domain \begin{equation} \mathbf{x}_{k+1} = \mathbf{A}\mathbf{x}_k + \mathbf{B}\mathbf{u}_k \end{equation} where $\mathbf{A}\in\mathbb{R}^{m\times m}$, $\mathbf{x}_k\in\mathbb{R}^m$, $\mathbf{B}\in\mathbb{R}^{m\times n}$ and $\mathbf{u}_k\in\mathbb{R}^{n}$. $\mathbf{A}$ is written in controllable canonical form. Since the system is stable (marginally), all the eigenvalues of $\mathbf{A}$ lie inside or on the unit circle. Given the system is stable, is it correct to say that at least $m-1$ singular values of $\mathbf{A}$ are less than or equal to unity and at most one singular value can be greater than $1$?","Let the following equation represent a stable (marginally) dynamical system in discrete time domain \begin{equation} \mathbf{x}_{k+1} = \mathbf{A}\mathbf{x}_k + \mathbf{B}\mathbf{u}_k \end{equation} where $\mathbf{A}\in\mathbb{R}^{m\times m}$, $\mathbf{x}_k\in\mathbb{R}^m$, $\mathbf{B}\in\mathbb{R}^{m\times n}$ and $\mathbf{u}_k\in\mathbb{R}^{n}$. $\mathbf{A}$ is written in controllable canonical form. Since the system is stable (marginally), all the eigenvalues of $\mathbf{A}$ lie inside or on the unit circle. Given the system is stable, is it correct to say that at least $m-1$ singular values of $\mathbf{A}$ are less than or equal to unity and at most one singular value can be greater than $1$?",,"['linear-algebra', 'matrices', 'control-theory', 'linear-control']"
20,Inner Product Space vs. Vector Space,Inner Product Space vs. Vector Space,,"I had no trouble understanding what a vector space is: a constraint on the type of vectors you can create, such that certain operations could be performed with them. For example, a vector space of  $$\left( \begin{array}{ccc} a  \\ a\\ a\end{array} \right)$$ fulfills all the requirements of vector spaces and is represented by a straight line through the origin. However I cannot understand what an Inner Product Space is. My first thought was that it's a set of vectors that fulfill certain requirements like Symmetry, Positivity, Multiplicity etc, similar to the concept of vector space. However, upon further observation, I realized that all vector spaces have these properties. If it is a vector space, it seems it is already an Inner Product Space. So what exactly does the Inner Product Space do for us? What does it add? Is there a visual representation of an Inner Product Space?","I had no trouble understanding what a vector space is: a constraint on the type of vectors you can create, such that certain operations could be performed with them. For example, a vector space of  $$\left( \begin{array}{ccc} a  \\ a\\ a\end{array} \right)$$ fulfills all the requirements of vector spaces and is represented by a straight line through the origin. However I cannot understand what an Inner Product Space is. My first thought was that it's a set of vectors that fulfill certain requirements like Symmetry, Positivity, Multiplicity etc, similar to the concept of vector space. However, upon further observation, I realized that all vector spaces have these properties. If it is a vector space, it seems it is already an Inner Product Space. So what exactly does the Inner Product Space do for us? What does it add? Is there a visual representation of an Inner Product Space?",,"['linear-algebra', 'inner-products', 'machine-learning']"
21,dot product between vector and matrix,dot product between vector and matrix,,"In my book on fluid mechanics there is an expression $$ \boldsymbol{\nabla}\cdot \boldsymbol{\tau}_{ij} $$ where $\boldsymbol{\tau}_{ij}$ is a rank-2 tensor (=matrix). Given that $\boldsymbol\nabla=(\partial_x, \partial_y, \partial_z)$, a vector, what do I get when I dot it with a matrix? If I was to write $\boldsymbol{\nabla}\cdot \boldsymbol{\tau}_{ij}$ in Einstein notation, then how would it look? The tensor $\boldsymbol{\tau}_{ij}$ is given by $$  \begin{pmatrix}   \tau_{xx} & \tau_{yx} & \tau_{zx} \\   \tau_{xy} & \tau_{yy} & \tau_{zy} \\   \tau_{xz} & \tau_{yz} & \tau_{zz}  \end{pmatrix} $$ and the dot-product yields (by comparison with later expressions in the chapter) $$ \boldsymbol{\nabla}\cdot \boldsymbol{\tau}_{ij} = \mathbf{i}(\partial_x \tau_{xx} + \partial_y \tau_{yx} + \partial_z \tau_{zx})+\mathbf{j}(\partial_x \tau_{xy} + \partial_y \tau_{yy} + \partial_z \tau_{zy}) + \mathbf{k}(\partial_x \tau_{xz} + \partial_y \tau_{yz} + \partial_z \tau_{zz}) $$ However, I don't see how this last expression comes about.","In my book on fluid mechanics there is an expression $$ \boldsymbol{\nabla}\cdot \boldsymbol{\tau}_{ij} $$ where $\boldsymbol{\tau}_{ij}$ is a rank-2 tensor (=matrix). Given that $\boldsymbol\nabla=(\partial_x, \partial_y, \partial_z)$, a vector, what do I get when I dot it with a matrix? If I was to write $\boldsymbol{\nabla}\cdot \boldsymbol{\tau}_{ij}$ in Einstein notation, then how would it look? The tensor $\boldsymbol{\tau}_{ij}$ is given by $$  \begin{pmatrix}   \tau_{xx} & \tau_{yx} & \tau_{zx} \\   \tau_{xy} & \tau_{yy} & \tau_{zy} \\   \tau_{xz} & \tau_{yz} & \tau_{zz}  \end{pmatrix} $$ and the dot-product yields (by comparison with later expressions in the chapter) $$ \boldsymbol{\nabla}\cdot \boldsymbol{\tau}_{ij} = \mathbf{i}(\partial_x \tau_{xx} + \partial_y \tau_{yx} + \partial_z \tau_{zx})+\mathbf{j}(\partial_x \tau_{xy} + \partial_y \tau_{yy} + \partial_z \tau_{zy}) + \mathbf{k}(\partial_x \tau_{xz} + \partial_y \tau_{yz} + \partial_z \tau_{zz}) $$ However, I don't see how this last expression comes about.",,"['linear-algebra', 'tensors']"
22,What is Volume?,What is Volume?,,"What i know is that ""Volume"" is a undefined notion in general, and the best approach to define volume is by Lebesgue measure. I'm studying linear-algebra right now and the text says that ""absolute value of determinant of n vectors is equal to n-dimensional volume formed by vectors"". (Note that it is not defined to be, but it is equal to) What is the definition of $n$-dimensional volume? and where can i see the proof that the Lebesgue measure of $n$-dimensional parallepiped is equal to the determinant of vectors forming the parallepiped? It was not that easy to me to prove that the Lebesgue measure of $n$-dimensional rectagular parallelepiped is equal to the intuitive Volume i.e.$ \prod |I_k|$. So i guess the proof for equality of the Lebesgue measure and determinant would be really tedious and not that easy.. Where can i see this proof if there is, or what is the definition of volume in linear-algebra?","What i know is that ""Volume"" is a undefined notion in general, and the best approach to define volume is by Lebesgue measure. I'm studying linear-algebra right now and the text says that ""absolute value of determinant of n vectors is equal to n-dimensional volume formed by vectors"". (Note that it is not defined to be, but it is equal to) What is the definition of $n$-dimensional volume? and where can i see the proof that the Lebesgue measure of $n$-dimensional parallepiped is equal to the determinant of vectors forming the parallepiped? It was not that easy to me to prove that the Lebesgue measure of $n$-dimensional rectagular parallelepiped is equal to the intuitive Volume i.e.$ \prod |I_k|$. So i guess the proof for equality of the Lebesgue measure and determinant would be really tedious and not that easy.. Where can i see this proof if there is, or what is the definition of volume in linear-algebra?",,"['linear-algebra', 'measure-theory', 'volume']"
23,Isomorphism of rings implies isomorphism of vector spaces?,Isomorphism of rings implies isomorphism of vector spaces?,,"Let $A$ and $B$ be isomorphic unitary rings. Suppose that both of them admit a structure of (maybe finite dimensional) vector space over some field $k$. I would like to know if then $A$ and $B$ are isomorphic as vector spaces over $k$ (if they are forced to have the same dimension). Notice that in general I am not requiring $A$ and $B$ to be $k$-algebras, i.e. I am not requiring any kind of compatibility between the multiplicative structure and the product with scalars from the field. My guess is that in this generality the answer is no, but I can't provide nor find any example. Here I gather some things I can prove: 1) if the field is $k=\mathbb{Q}$ and $A$ and $B$ are $k$-algebras, then the answer is yes. 2) if the field is $k=\mathbb{R}$, $A$ and $B$ are $k$-algebras and they are fields, then the answer is yes again. 3) if $A$ and $B$ are finite $k$-algebras (and so $k$ is finite too) the answer is yes again. Unfortunately these rule out most of the examples from a first course in ring theory, so I suspect the answer would be more exotic than this, but I can't find anything. Maybe the answer is yes even in the general setting (or maybe just for $k$-algebras), and in this case I'd like to see a proof. Thanks in advance.","Let $A$ and $B$ be isomorphic unitary rings. Suppose that both of them admit a structure of (maybe finite dimensional) vector space over some field $k$. I would like to know if then $A$ and $B$ are isomorphic as vector spaces over $k$ (if they are forced to have the same dimension). Notice that in general I am not requiring $A$ and $B$ to be $k$-algebras, i.e. I am not requiring any kind of compatibility between the multiplicative structure and the product with scalars from the field. My guess is that in this generality the answer is no, but I can't provide nor find any example. Here I gather some things I can prove: 1) if the field is $k=\mathbb{Q}$ and $A$ and $B$ are $k$-algebras, then the answer is yes. 2) if the field is $k=\mathbb{R}$, $A$ and $B$ are $k$-algebras and they are fields, then the answer is yes again. 3) if $A$ and $B$ are finite $k$-algebras (and so $k$ is finite too) the answer is yes again. Unfortunately these rule out most of the examples from a first course in ring theory, so I suspect the answer would be more exotic than this, but I can't find anything. Maybe the answer is yes even in the general setting (or maybe just for $k$-algebras), and in this case I'd like to see a proof. Thanks in advance.",,"['linear-algebra', 'abstract-algebra', 'commutative-algebra', 'ring-theory', 'vector-spaces']"
24,How to prove that exponential kernel is positive definite?,How to prove that exponential kernel is positive definite?,,"The exponential kernel is defined by: $$k(x,z) = e^{-\alpha\|x-z\|}$$ where $\alpha>0$, $x,z\in \Bbb{R}^d$, $\|x\|$ is the 2-norm. The kernel matrix is defined by $K_{ij} = k(x_i,x_j)$, $i,j\in[1\ldots n]$. How to prove that $K$ is a positive (semi-positive) definite matrix?","The exponential kernel is defined by: $$k(x,z) = e^{-\alpha\|x-z\|}$$ where $\alpha>0$, $x,z\in \Bbb{R}^d$, $\|x\|$ is the 2-norm. The kernel matrix is defined by $K_{ij} = k(x_i,x_j)$, $i,j\in[1\ldots n]$. How to prove that $K$ is a positive (semi-positive) definite matrix?",,"['linear-algebra', 'matrices']"
25,"How to prove that, for $U$ unitary, $|\det U| = 1$ but $\det U\neq \det U^H$?","How to prove that, for  unitary,  but ?",U |\det U| = 1 \det U\neq \det U^H,"Prove that unitary matrix $U$ satisfies $|\det U| = 1$, but $\det U$ is different from $\det U^{H}$. How can I prove these two statements? I guess I should use the fact that every column of unitary matirx is orthonormal, but I'm not sure where to put that...","Prove that unitary matrix $U$ satisfies $|\det U| = 1$, but $\det U$ is different from $\det U^{H}$. How can I prove these two statements? I guess I should use the fact that every column of unitary matirx is orthonormal, but I'm not sure where to put that...",,"['linear-algebra', 'matrices', 'determinant', 'unitary-matrices']"
26,Nilpotent Matrix,Nilpotent Matrix,,"So I saw this problem: Is there an upper triangular matrix $A$ such that $A^n\neq 0$ but $A^{n+1}=0$? Prove or disprove. I said no, and my reasoning was that the matrix must have a zero diagonal since $(A^k)_{ii}=a_{ii}^k$. Then the matrix must be srtictly upper diagonal, and when we multiply it the diagonal of zeros starts ""moving up"" and eventually the matrix is zero and it follows that it must be in at most $n$ steps. Is there a neater way to do this? Like looking at characteristic polynomials or something? I was thinking that the minimal polynomial should be of the form $t^k$ and thus the characteristic polynomial ought to be $t^n$ as the minimal divides the characteristic, and by Cayley Hamilton we ought to have $A^n=0$, but I cannot see why the minimal polynomial must be $t^k$. If I was working over a complex (or alg closed field then yes since it could only have zero eigenvalues). Any thoughts? Thanks,","So I saw this problem: Is there an upper triangular matrix $A$ such that $A^n\neq 0$ but $A^{n+1}=0$? Prove or disprove. I said no, and my reasoning was that the matrix must have a zero diagonal since $(A^k)_{ii}=a_{ii}^k$. Then the matrix must be srtictly upper diagonal, and when we multiply it the diagonal of zeros starts ""moving up"" and eventually the matrix is zero and it follows that it must be in at most $n$ steps. Is there a neater way to do this? Like looking at characteristic polynomials or something? I was thinking that the minimal polynomial should be of the form $t^k$ and thus the characteristic polynomial ought to be $t^n$ as the minimal divides the characteristic, and by Cayley Hamilton we ought to have $A^n=0$, but I cannot see why the minimal polynomial must be $t^k$. If I was working over a complex (or alg closed field then yes since it could only have zero eigenvalues). Any thoughts? Thanks,",,"['linear-algebra', 'matrices']"
27,Question about the converse of a well known result from Linear Algebra,Question about the converse of a well known result from Linear Algebra,,"I am a graduate student studying for a Linear Algebra qualifying exam and I have been going over sample problems from previous exams.  The recommended text for these problems are Hoffman and Kunze ""Linear Algebra"", Chapter three of Jacobson ""Algebra I"" and The module theory section of Dummit and Foote.   Most likely this problem is a variation of an exercise from one of the texts I just mentioned but I have had trouble placing it. I think the background for the problem comes from a converse to the following lemma on page 186 of Hoffman and Kunze. Lemma:  Let $V$ be a vector space over the field $F$ and let $T$ be a linear operator on $V$.  Suppose that $T \alpha = c \alpha$ for some vector $\alpha \in V$ and scalar $c \in F$.  Then if $f$ is any polynomial, $f(T) \alpha = f(c) \alpha$. Finally here is the question I am having problems with: Let $T: \mathbb{C}^5 \rightarrow \mathbb{C}^5$  be a linear operator and let $g(x)$ be a polynomial in $\mathbb{C}[x]$.  If $c$ is a characteristic value for $g(T)$, must there exist a characteristic value $a$ for $T$ such that $g(a) = c$?  Explain why or why not My guess is that the question is not true but I am having trouble constructing an example.  Thank you for any advice you can give.","I am a graduate student studying for a Linear Algebra qualifying exam and I have been going over sample problems from previous exams.  The recommended text for these problems are Hoffman and Kunze ""Linear Algebra"", Chapter three of Jacobson ""Algebra I"" and The module theory section of Dummit and Foote.   Most likely this problem is a variation of an exercise from one of the texts I just mentioned but I have had trouble placing it. I think the background for the problem comes from a converse to the following lemma on page 186 of Hoffman and Kunze. Lemma:  Let $V$ be a vector space over the field $F$ and let $T$ be a linear operator on $V$.  Suppose that $T \alpha = c \alpha$ for some vector $\alpha \in V$ and scalar $c \in F$.  Then if $f$ is any polynomial, $f(T) \alpha = f(c) \alpha$. Finally here is the question I am having problems with: Let $T: \mathbb{C}^5 \rightarrow \mathbb{C}^5$  be a linear operator and let $g(x)$ be a polynomial in $\mathbb{C}[x]$.  If $c$ is a characteristic value for $g(T)$, must there exist a characteristic value $a$ for $T$ such that $g(a) = c$?  Explain why or why not My guess is that the question is not true but I am having trouble constructing an example.  Thank you for any advice you can give.",,['linear-algebra']
28,"Inner Product, definite positive?","Inner Product, definite positive?",,"While reading through my textbook it says ""the most important example of an inner-product space is $F^n$"", where $F$ denotes $\mathbb{C}$ or $\mathbb{R}$ . Our definition of an inner product on a vector space $V$ is as follows: 1) Positive definite: $\langle v,v \rangle \ge 0$ with equality if and only if $v=0$ 2) Linearity in the first arguement: $\langle a_1v_1+a_2v_2,w \rangle = a_1 \langle v_1,w \rangle + a_2\langle v_2,w \rangle$ 3) Conjugate symmetric: $\langle u,v\rangle = \overline{\langle v,u\rangle}$ Let $$\displaystyle w=(w_1\ldots,w_n) , z=(z_1,\ldots,z_n)$$ Then: $$\displaystyle \langle w,z\rangle =w_1\overline{z_1}+\cdots+w_n\overline{z_n}$$ I'm trying to verify that this is indeed true.  So first I want to check that $\langle w,z\rangle$ satisfies condition (1). Say that $w,z\in \mathbb{C}$. Just looking at say $w_1=a+bi$ and $z_1=c+di$, how can we guarantee that $w_1\overline{z_1}\geq 0$? If we can observe this, it would need to hold true for the other coordinates as well.  So my question is, how do we know that $w_1\overline{z_1}\geq 0$?","While reading through my textbook it says ""the most important example of an inner-product space is $F^n$"", where $F$ denotes $\mathbb{C}$ or $\mathbb{R}$ . Our definition of an inner product on a vector space $V$ is as follows: 1) Positive definite: $\langle v,v \rangle \ge 0$ with equality if and only if $v=0$ 2) Linearity in the first arguement: $\langle a_1v_1+a_2v_2,w \rangle = a_1 \langle v_1,w \rangle + a_2\langle v_2,w \rangle$ 3) Conjugate symmetric: $\langle u,v\rangle = \overline{\langle v,u\rangle}$ Let $$\displaystyle w=(w_1\ldots,w_n) , z=(z_1,\ldots,z_n)$$ Then: $$\displaystyle \langle w,z\rangle =w_1\overline{z_1}+\cdots+w_n\overline{z_n}$$ I'm trying to verify that this is indeed true.  So first I want to check that $\langle w,z\rangle$ satisfies condition (1). Say that $w,z\in \mathbb{C}$. Just looking at say $w_1=a+bi$ and $z_1=c+di$, how can we guarantee that $w_1\overline{z_1}\geq 0$? If we can observe this, it would need to hold true for the other coordinates as well.  So my question is, how do we know that $w_1\overline{z_1}\geq 0$?",,"['linear-algebra', 'inner-products']"
29,What are the conditions on $\text{tr}(AB) \leq \text{tr(A)} \text{tr(B)}$ to be true?,What are the conditions on  to be true?,\text{tr}(AB) \leq \text{tr(A)} \text{tr(B)},Let $A$ and $B$ be two arbitrary matrix with proper dimension for multiplication. Consider this trace inequlaty which is trace of multiplication of two matrices versus their individual traces $$\text{tr}(AB) \leq \text{tr(A)}  \text{tr(B)}$$ 1- Do we have result for rectangular matrix that satisfy this inequality? 2- If they were square matrices what are the conditions? 3- Is there any specific name for this inequality?,Let and be two arbitrary matrix with proper dimension for multiplication. Consider this trace inequlaty which is trace of multiplication of two matrices versus their individual traces 1- Do we have result for rectangular matrix that satisfy this inequality? 2- If they were square matrices what are the conditions? 3- Is there any specific name for this inequality?,A B \text{tr}(AB) \leq \text{tr(A)}  \text{tr(B)},['linear-algebra']
30,proving the determinant of a product of matrices is the product of their determinants in suffix / index notation,proving the determinant of a product of matrices is the product of their determinants in suffix / index notation,,"I'm trying to learn suffix notation (both to prove results in linear algebra and for application in vector calculus). As an exercise, I wanted to use it to prove that the determinant of a product of two matrices is equal to the product of their determinants, i.e. for $\underline{A},\, \underline{B},\, \underline{C}$, $3\times 3$ matrices where $C_{pq} = A_{pr}B_{rq}$, I'd like to show that: $$\det(\underline{A}\,\underline{B}) = \det(\underline{A}) \det(\underline{B})$$ $\color{blue}{\textbf{Here is what I have tried so far}}$: $$\det{(\underline{C})} = \det(\underline{A}\,\underline{B}),$$ Now, the determinant in suffix notation would be: $$\begin{align*} \epsilon_{ijk} C_{1i} C_{2j} C_{3k}     & = \epsilon_{ijk} \,  A_{1p}B_{pi} \, A_{2q}B_{qj} \, A_{3r}B_{rk}\\     & = \epsilon_{ijk} \, A_{1p}A_{2q}A_{3r} \, B_{pi}B_{qj}B_{rk}    \end{align*}$$ $\color{blue}{\textbf{At this point, I couldn't find any way forward}}$. I tried reading up on tensors, to build my intuition, but after hours spent with several different texts, I was only more confused than before, so I tried to focus on the formalism, itself. $\color{green}{\textbf{Eventually, I thought to try it from the other side}}$, i.e. starting with $\det(\underline{A}) \det(\underline{B})$, and (after spending more time than I care to admit) I managed this: $$ \begin{align*}     \det(\underline{A}) \det(\underline{B}) %%     = &  \epsilon_{ijk} \, A_{1i}A_{2j}A_{3k}        \, \epsilon_{pqr}B_{1p}B_{2q}B_{3r} \\[5pt] %%     = &   \epsilon_{ijk} \epsilon_{pqr} \, A_{1i}A_{2j}A_{3k}        \, B_{1p}B_{2q}B_{3r} \\[5pt] %%     = &  \bigl(               \delta_{ip}(\delta_{jq}\delta_{kr} - \delta_{jr}\delta_{kq}) %           + \delta_{iq}(\delta_{jr}\delta_{kp} - \delta_{jp}\delta_{kr}) %          +  \delta_{ir}(\delta_{jp}\delta_{kq} - \delta_{jq}\delta_{kp})        \bigr)              A_{1i}A_{2j}A_{3k} \, B_{1p}B_{2q}B_{3r} \\[5pt] %%     = &  (\delta_{jq}\delta_{kr} - \delta_{jr}\delta_{kq})         A_{1i}A_{2j}A_{3k} \, B_{1{\color{red}i}}B_{2q}B_{3r}\\ %    & \quad + (\delta_{jr}\delta_{kp} - \delta_{jp}\delta_{kr})                A_{1i}A_{2j}A_{3k} \, B_{1p}B_{2{\color{red}i}}B_{3r}\\ %    & \qquad + (\delta_{jp}\delta_{kq} - \delta_{jq}\delta_{kp})                  A_{1i}A_{2j}A_{3k} \, B_{1p}B_{2q}B_{3{\color{red}i}}\\[5pt] %%   = &  \quad A_{1i}A_{2j}A_{3k} B_{1{\color{red}i}} (B_{2j}B_{3k} - B_{2k}B_{3j}) \\   &  + A_{1i}A_{2j}A_{3k} B_{2{\color{red}i}} (B_{1k}B_{3j} - B_{1j}B_{3k}) \\   &  + A_{1i}A_{2j}A_{3k} B_{3{\color{red}i}} (B_{1j}B_{2k} - B_{1k}B_{2j}) \end{align*} $$ $\color{green}{\textbf{I } think \textbf{ this last expression can be written as}}$: $$\begin{align*} \det(\underline{A} \, \underline{B})  %%     & =  A_{1i}A_{2j}A_{3k} \, \epsilon_{pqr}B_{pi}B_{qj}B_{rk} \\[5pt] %%     & = \epsilon_{pqr} \, A_{1i}B_{pi}\,  A_{2j}B_{qj} \, A_{3k}B_{rk}; \end{align*}$$ however, it's possible my reasoning is incorrect (I'm still very uncertain about the formalism...). $\color{red}{If \textbf{ the above is correct}}$, it seems to look like $\det(\underline{A} \underline{B}^T)$, which suggests that if I had started off by noting that: $$ \det(\underline{A})\, \det(\underline{B}) = \det(\underline{A}) \, \det(\underline{B}^T)$$ $\color{red}{\textbf{I would end up with the correct final expression}}$. However, (if all my reasoning to date is correct) my question(s) are : is it possible to see that $$\epsilon_{pqr} \, A_{1i}B_{{\color{blue}p}i}\,  A_{2j}B_{{\color{blue}q}j} \, A_{3k}B_{{\color{blue}r}k}   = \epsilon_{pqr} \, A_{1i}B_{i{\color{blue}p}}\,  A_{2j}B_{j{\color{blue}q}} \, A_{3k}B_{k{\color{blue}r}}$$ directly (i.e. without having to think about the interpretation as the product of determinants etc; simply via identities or application of the definitions)? If so, how? is there some way of doing this (using suffix notation) without having to expand the epsilon product $\epsilon_{ijk}\, \epsilon_{pqr}$, explicitly (all the resources I have read so far have only offered the identity in terms of the Kronecker deltas, nothing that relates the product to another epsilon)? This is painfully long to write out, so would equally appreciate mere direction towards a good ""workbook"" with lots of problems (& answers or solutions) related to suffix notation. My hope is that if I first become comfortable with the formalism, I might later have a more success trying to understand the math.","I'm trying to learn suffix notation (both to prove results in linear algebra and for application in vector calculus). As an exercise, I wanted to use it to prove that the determinant of a product of two matrices is equal to the product of their determinants, i.e. for $\underline{A},\, \underline{B},\, \underline{C}$, $3\times 3$ matrices where $C_{pq} = A_{pr}B_{rq}$, I'd like to show that: $$\det(\underline{A}\,\underline{B}) = \det(\underline{A}) \det(\underline{B})$$ $\color{blue}{\textbf{Here is what I have tried so far}}$: $$\det{(\underline{C})} = \det(\underline{A}\,\underline{B}),$$ Now, the determinant in suffix notation would be: $$\begin{align*} \epsilon_{ijk} C_{1i} C_{2j} C_{3k}     & = \epsilon_{ijk} \,  A_{1p}B_{pi} \, A_{2q}B_{qj} \, A_{3r}B_{rk}\\     & = \epsilon_{ijk} \, A_{1p}A_{2q}A_{3r} \, B_{pi}B_{qj}B_{rk}    \end{align*}$$ $\color{blue}{\textbf{At this point, I couldn't find any way forward}}$. I tried reading up on tensors, to build my intuition, but after hours spent with several different texts, I was only more confused than before, so I tried to focus on the formalism, itself. $\color{green}{\textbf{Eventually, I thought to try it from the other side}}$, i.e. starting with $\det(\underline{A}) \det(\underline{B})$, and (after spending more time than I care to admit) I managed this: $$ \begin{align*}     \det(\underline{A}) \det(\underline{B}) %%     = &  \epsilon_{ijk} \, A_{1i}A_{2j}A_{3k}        \, \epsilon_{pqr}B_{1p}B_{2q}B_{3r} \\[5pt] %%     = &   \epsilon_{ijk} \epsilon_{pqr} \, A_{1i}A_{2j}A_{3k}        \, B_{1p}B_{2q}B_{3r} \\[5pt] %%     = &  \bigl(               \delta_{ip}(\delta_{jq}\delta_{kr} - \delta_{jr}\delta_{kq}) %           + \delta_{iq}(\delta_{jr}\delta_{kp} - \delta_{jp}\delta_{kr}) %          +  \delta_{ir}(\delta_{jp}\delta_{kq} - \delta_{jq}\delta_{kp})        \bigr)              A_{1i}A_{2j}A_{3k} \, B_{1p}B_{2q}B_{3r} \\[5pt] %%     = &  (\delta_{jq}\delta_{kr} - \delta_{jr}\delta_{kq})         A_{1i}A_{2j}A_{3k} \, B_{1{\color{red}i}}B_{2q}B_{3r}\\ %    & \quad + (\delta_{jr}\delta_{kp} - \delta_{jp}\delta_{kr})                A_{1i}A_{2j}A_{3k} \, B_{1p}B_{2{\color{red}i}}B_{3r}\\ %    & \qquad + (\delta_{jp}\delta_{kq} - \delta_{jq}\delta_{kp})                  A_{1i}A_{2j}A_{3k} \, B_{1p}B_{2q}B_{3{\color{red}i}}\\[5pt] %%   = &  \quad A_{1i}A_{2j}A_{3k} B_{1{\color{red}i}} (B_{2j}B_{3k} - B_{2k}B_{3j}) \\   &  + A_{1i}A_{2j}A_{3k} B_{2{\color{red}i}} (B_{1k}B_{3j} - B_{1j}B_{3k}) \\   &  + A_{1i}A_{2j}A_{3k} B_{3{\color{red}i}} (B_{1j}B_{2k} - B_{1k}B_{2j}) \end{align*} $$ $\color{green}{\textbf{I } think \textbf{ this last expression can be written as}}$: $$\begin{align*} \det(\underline{A} \, \underline{B})  %%     & =  A_{1i}A_{2j}A_{3k} \, \epsilon_{pqr}B_{pi}B_{qj}B_{rk} \\[5pt] %%     & = \epsilon_{pqr} \, A_{1i}B_{pi}\,  A_{2j}B_{qj} \, A_{3k}B_{rk}; \end{align*}$$ however, it's possible my reasoning is incorrect (I'm still very uncertain about the formalism...). $\color{red}{If \textbf{ the above is correct}}$, it seems to look like $\det(\underline{A} \underline{B}^T)$, which suggests that if I had started off by noting that: $$ \det(\underline{A})\, \det(\underline{B}) = \det(\underline{A}) \, \det(\underline{B}^T)$$ $\color{red}{\textbf{I would end up with the correct final expression}}$. However, (if all my reasoning to date is correct) my question(s) are : is it possible to see that $$\epsilon_{pqr} \, A_{1i}B_{{\color{blue}p}i}\,  A_{2j}B_{{\color{blue}q}j} \, A_{3k}B_{{\color{blue}r}k}   = \epsilon_{pqr} \, A_{1i}B_{i{\color{blue}p}}\,  A_{2j}B_{j{\color{blue}q}} \, A_{3k}B_{k{\color{blue}r}}$$ directly (i.e. without having to think about the interpretation as the product of determinants etc; simply via identities or application of the definitions)? If so, how? is there some way of doing this (using suffix notation) without having to expand the epsilon product $\epsilon_{ijk}\, \epsilon_{pqr}$, explicitly (all the resources I have read so far have only offered the identity in terms of the Kronecker deltas, nothing that relates the product to another epsilon)? This is painfully long to write out, so would equally appreciate mere direction towards a good ""workbook"" with lots of problems (& answers or solutions) related to suffix notation. My hope is that if I first become comfortable with the formalism, I might later have a more success trying to understand the math.",,"['linear-algebra', 'proof-verification', 'determinant', 'index-notation']"
31,Mazur's Weak Basis Theorem,Mazur's Weak Basis Theorem,,"It is the Exercise 1.1 in Topics in Banach Space Theory by Albiac and Kalton to prove Mazur's Weak Basis Theorem, which states that every weak basis in a Banach space $X$ is a Schauder basis, where weak basis is defined as follows: A sequence $(e_n)_n$ in a Banach space $X$ is called a weak basis for $X$ if for every $x\in X$ there is an unique sequence of scalars $(a_n)$ such that $x=\sum_{k=1}^\infty a_ke_k$ in the weak topology. I tried to show, to no avail, that $\lVert\sum^n a_kx_k\rVert\rightarrow\lVert x\rVert$ so that it would immediately follow that $\sum^n a_ne_n\overset{\lVert\cdot\rVert}{\rightarrow}x$. I'd like suggestions on how to proceed.","It is the Exercise 1.1 in Topics in Banach Space Theory by Albiac and Kalton to prove Mazur's Weak Basis Theorem, which states that every weak basis in a Banach space $X$ is a Schauder basis, where weak basis is defined as follows: A sequence $(e_n)_n$ in a Banach space $X$ is called a weak basis for $X$ if for every $x\in X$ there is an unique sequence of scalars $(a_n)$ such that $x=\sum_{k=1}^\infty a_ke_k$ in the weak topology. I tried to show, to no avail, that $\lVert\sum^n a_kx_k\rVert\rightarrow\lVert x\rVert$ so that it would immediately follow that $\sum^n a_ne_n\overset{\lVert\cdot\rVert}{\rightarrow}x$. I'd like suggestions on how to proceed.",,"['linear-algebra', 'functional-analysis', 'schauder-basis']"
32,Equivalence of skew-symmetric matrices,Equivalence of skew-symmetric matrices,,"Let $N=\{1,\dots,n\}$ and $A,B$ be $n\times n$ skew symmetric matrices such that it is possible to permute some rows and some columns from $A$ to get $B$. In other words, for some permutations $g,h: N\rightarrow N$,  $$A_{i,j}=B_{g(i),h(j)}$$  for all $1\leq i,j\leq n$. Must there exist a permutation $f:N\rightarrow N$ such that $$A_{i,j}=B_{f(i),f(j)}$$for all $1\leq i,j\leq n?$ For example, let  $$A=\begin{pmatrix}   0 & 3  \\   -3 & 0   \end{pmatrix} , B=\begin{pmatrix}   0 & -3  \\   3 & 0   \end{pmatrix} $$ If we switch the rows and also switch the columns, we get from $A$ to $B$. And there exists a permutation $f$ with $f(1)=2,f(2)=1$ such that $A_{i,j}=B_{f(i),f(j)}$ for all $1\leq i,j\leq 2$. There exists an example with $g\neq h$. Let  $$A=\begin{pmatrix}   0 & 0 & 2 & -2  \\   0 & 0 & -2 & 2 \\   -2 & 2 & 0 & 0 \\   2 & -2 & 0 & 0  \end{pmatrix} , B=\begin{pmatrix}   0 & 0 & -2 & 2  \\   0 & 0 & 2 & -2 \\   2 & -2 & 0 & 0 \\   -2 & 2 & 0 & 0  \end{pmatrix} $$ One possibility for $g,h$ is $g(i)=i$ for all $i$, $h(1)=2,h(2)=1,h(3)=4,h(4)=3$. In this case we can let $f(1)=2,f(2)=1,f(3)=3,f(4)=4$.","Let $N=\{1,\dots,n\}$ and $A,B$ be $n\times n$ skew symmetric matrices such that it is possible to permute some rows and some columns from $A$ to get $B$. In other words, for some permutations $g,h: N\rightarrow N$,  $$A_{i,j}=B_{g(i),h(j)}$$  for all $1\leq i,j\leq n$. Must there exist a permutation $f:N\rightarrow N$ such that $$A_{i,j}=B_{f(i),f(j)}$$for all $1\leq i,j\leq n?$ For example, let  $$A=\begin{pmatrix}   0 & 3  \\   -3 & 0   \end{pmatrix} , B=\begin{pmatrix}   0 & -3  \\   3 & 0   \end{pmatrix} $$ If we switch the rows and also switch the columns, we get from $A$ to $B$. And there exists a permutation $f$ with $f(1)=2,f(2)=1$ such that $A_{i,j}=B_{f(i),f(j)}$ for all $1\leq i,j\leq 2$. There exists an example with $g\neq h$. Let  $$A=\begin{pmatrix}   0 & 0 & 2 & -2  \\   0 & 0 & -2 & 2 \\   -2 & 2 & 0 & 0 \\   2 & -2 & 0 & 0  \end{pmatrix} , B=\begin{pmatrix}   0 & 0 & -2 & 2  \\   0 & 0 & 2 & -2 \\   2 & -2 & 0 & 0 \\   -2 & 2 & 0 & 0  \end{pmatrix} $$ One possibility for $g,h$ is $g(i)=i$ for all $i$, $h(1)=2,h(2)=1,h(3)=4,h(4)=3$. In this case we can let $f(1)=2,f(2)=1,f(3)=3,f(4)=4$.",,"['linear-algebra', 'matrices', 'permutations', 'skew-symmetric-matrices']"
33,Determinant inequality and positive definite matrix,Determinant inequality and positive definite matrix,,"Let $B$ and $C$ be $n \times n$ Hermitian matrices, with $B$ positive definite and $C$ positive semidefinite. Show that $B+C$ is positive definite Show that $\det(B) \leq \det(B+C)$ . What is the equality case? Show that $B^{-1}-(B+C)^{-1}$ is positive semidefinite. I proved (1). And for (2), I tried using Cholesky decomposition but does not gain anything. Any hints, ideas? Thanks.","Let and be Hermitian matrices, with positive definite and positive semidefinite. Show that is positive definite Show that . What is the equality case? Show that is positive semidefinite. I proved (1). And for (2), I tried using Cholesky decomposition but does not gain anything. Any hints, ideas? Thanks.",B C n \times n B C B+C \det(B) \leq \det(B+C) B^{-1}-(B+C)^{-1},"['linear-algebra', 'matrices', 'determinant', 'positive-definite']"
34,Tetrahedron volume relation to parallelepiped and pyramid,Tetrahedron volume relation to parallelepiped and pyramid,,"Reading at Mathworld , I came across the subject of tetrahedrons. Particularly calculating the volume with four known vertices. There's a formula which uses the triple product to calculate the volume of a parallelepiped. I'm very aware of why the triple product represents the volume of a parallelepiped. However, I don't understand how to derive the relation that the volume of a tetrahedrons is one-sixth of a parallelepiped. I can accept it, sure. And graphically, I'm sure that it is true. Is there any way to prove it using linear algebra? Wikipedia provides me with one proof. However, I don't understand how they went from pyramid volume being $1/3\,A_0h$ to tetrahedrons being $1/6\,A_0h$. It must mean that the volume of a tetrahedron is half that of a pyramid, must it not?","Reading at Mathworld , I came across the subject of tetrahedrons. Particularly calculating the volume with four known vertices. There's a formula which uses the triple product to calculate the volume of a parallelepiped. I'm very aware of why the triple product represents the volume of a parallelepiped. However, I don't understand how to derive the relation that the volume of a tetrahedrons is one-sixth of a parallelepiped. I can accept it, sure. And graphically, I'm sure that it is true. Is there any way to prove it using linear algebra? Wikipedia provides me with one proof. However, I don't understand how they went from pyramid volume being $1/3\,A_0h$ to tetrahedrons being $1/6\,A_0h$. It must mean that the volume of a tetrahedron is half that of a pyramid, must it not?",,"['linear-algebra', 'geometry', 'vector-spaces']"
35,Prerequisites for Linear Algebra Done Right by Sheldon Axler.,Prerequisites for Linear Algebra Done Right by Sheldon Axler.,,"I've read some notes online and I learned so far: $\{\overset{\displaystyle\ldots}\ldots$ Systems of Two Linear Equations $\rlap{\require{cancel}{\rlap{\Huge\times}\cancel{\color{white}h}}}{\begin{bmatrix}\quad\end{bmatrix}}\,\,$ Gaussian Elimination $\left[\vdots\,\vdots\,\vdots\right]$ Matrices $(\square^{-1})$ The Inverse of a Square Matrix (and also how to solve $\sf Ax=B$) $\left|\,\overset{\overset{\displaystyle\cdot}{}}.\,\overset{\overset{\displaystyle\cdot}{}}.\right|$ Determinants and Cramers Rule Is this enough to start reading the celebrated book Linear Algebra Done Right by the author Sheldon Axler? I will in the future study topics in physics like advanced electrodynamics, classical field theory, general relativity, quantum mechanics and a lot of particle physics. (i heard that there were a lot of ""matrix"" stuff there but idk), so will Axler's book be sufficient for those? Thanks for your answer.","I've read some notes online and I learned so far: $\{\overset{\displaystyle\ldots}\ldots$ Systems of Two Linear Equations $\rlap{\require{cancel}{\rlap{\Huge\times}\cancel{\color{white}h}}}{\begin{bmatrix}\quad\end{bmatrix}}\,\,$ Gaussian Elimination $\left[\vdots\,\vdots\,\vdots\right]$ Matrices $(\square^{-1})$ The Inverse of a Square Matrix (and also how to solve $\sf Ax=B$) $\left|\,\overset{\overset{\displaystyle\cdot}{}}.\,\overset{\overset{\displaystyle\cdot}{}}.\right|$ Determinants and Cramers Rule Is this enough to start reading the celebrated book Linear Algebra Done Right by the author Sheldon Axler? I will in the future study topics in physics like advanced electrodynamics, classical field theory, general relativity, quantum mechanics and a lot of particle physics. (i heard that there were a lot of ""matrix"" stuff there but idk), so will Axler's book be sufficient for those? Thanks for your answer.",,"['linear-algebra', 'reference-request']"
36,Symmetry group of hypercube in $\mathbb{R}^4$ [closed],Symmetry group of hypercube in  [closed],\mathbb{R}^4,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Let $ T= \{(x, y, z, w) \in \mathbb{R}^4 \text{ }|\text{ }|x|,\, |y|,\,|z|,\,|w| \le 1\}$ be the hypercube in $\mathbb{R}^4$ of side length $2$ centered at the origin. Identify the symmetry group of $T$.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Let $ T= \{(x, y, z, w) \in \mathbb{R}^4 \text{ }|\text{ }|x|,\, |y|,\,|z|,\,|w| \le 1\}$ be the hypercube in $\mathbb{R}^4$ of side length $2$ centered at the origin. Identify the symmetry group of $T$.",,"['linear-algebra', 'abstract-algebra']"
37,Does the shift operator on $\ell^2(\mathbb{Z})$ have a logarithm?,Does the shift operator on  have a logarithm?,\ell^2(\mathbb{Z}),"Consider the Hilbert space $\ell^2(\mathbb{Z})$, i.e., the space of all sequences $\ldots,a_{-2},a_{-1},a_0,a_1,a_2,\ldots$ of complex numbers such that $\sum_n |a_n|^2 < \infty$ with the usual inner product. Let $S$ be the shift operator: $$ (S a)_n = a_{n-1}. $$ If there a linear operator $A:\ell^2\to\ell^2$ such that $S=e^A$? I really doubt there is, but I'm not sure. Context: I'm thinking about the momentum operator as a generator of the translation operator in quantum mechanics. One could think of $\ell^2(\mathbb{Z})$ as a discrete version of a particle on the line.","Consider the Hilbert space $\ell^2(\mathbb{Z})$, i.e., the space of all sequences $\ldots,a_{-2},a_{-1},a_0,a_1,a_2,\ldots$ of complex numbers such that $\sum_n |a_n|^2 < \infty$ with the usual inner product. Let $S$ be the shift operator: $$ (S a)_n = a_{n-1}. $$ If there a linear operator $A:\ell^2\to\ell^2$ such that $S=e^A$? I really doubt there is, but I'm not sure. Context: I'm thinking about the momentum operator as a generator of the translation operator in quantum mechanics. One could think of $\ell^2(\mathbb{Z})$ as a discrete version of a particle on the line.",,"['linear-algebra', 'functional-analysis', 'hilbert-spaces', 'quantum-mechanics']"
38,Quaternions vs Axis angle,Quaternions vs Axis angle,,Whats the use of representing rotation with quaternions compared to normal axis angle representation? I've been trying to learn quaternions and they make enough sense but as far as I can tell quaternions are just axis angle with a transformed axis and angle of rotation. How exactly does transforming the axis and angle of rotation affect the matrix instead of just representing it normally through axis angle?,Whats the use of representing rotation with quaternions compared to normal axis angle representation? I've been trying to learn quaternions and they make enough sense but as far as I can tell quaternions are just axis angle with a transformed axis and angle of rotation. How exactly does transforming the axis and angle of rotation affect the matrix instead of just representing it normally through axis angle?,,"['linear-algebra', 'matrices', 'rotations', 'quaternions', 'motivation']"
39,"solving linear system ""by inspection""?","solving linear system ""by inspection""?",,"A text question is asking to solve some linear systems by inspection . My interpretation of ""by inspection"" is ""by looking"". For a linear system like $$ \begin{cases} 3x + 4y = 28\\ 3x + 4y = 83 \end{cases} $$ you could say that by inspection there is no solution because ""two (of the same) numbers can't have different sums."" or given the system $$ \begin{cases} y = 3x + 5\\ y = 2x + 5 \end{cases} $$ you could say that by inspection, the solution is (0, 5) because both lines have the same y-intercept but different slopes. But the exercises given in my text don't seem (to me) to lend themselves to solution by inspection (hence my question here). The four systems are: $$ \begin{cases} x + y = 6\\ 2x + y = 8 \end{cases} $$ $$ \begin{cases} x - y = 1\\ 5x + 2y = 5 \end{cases} $$ $$ \begin{cases} x + y = 8\\ 2x + y = -11 \end{cases} $$ $$ \begin{cases} 2x + y = 13\\ x + 2y = 7 \end{cases} $$ For each one, it seems that at the very least some mental calculation is required  which goes beyond the concept of ""by inspection"" in my opinion. Or does it? Or is there something else I'm missing? Thank you.","A text question is asking to solve some linear systems by inspection . My interpretation of ""by inspection"" is ""by looking"". For a linear system like $$ \begin{cases} 3x + 4y = 28\\ 3x + 4y = 83 \end{cases} $$ you could say that by inspection there is no solution because ""two (of the same) numbers can't have different sums."" or given the system $$ \begin{cases} y = 3x + 5\\ y = 2x + 5 \end{cases} $$ you could say that by inspection, the solution is (0, 5) because both lines have the same y-intercept but different slopes. But the exercises given in my text don't seem (to me) to lend themselves to solution by inspection (hence my question here). The four systems are: $$ \begin{cases} x + y = 6\\ 2x + y = 8 \end{cases} $$ $$ \begin{cases} x - y = 1\\ 5x + 2y = 5 \end{cases} $$ $$ \begin{cases} x + y = 8\\ 2x + y = -11 \end{cases} $$ $$ \begin{cases} 2x + y = 13\\ x + 2y = 7 \end{cases} $$ For each one, it seems that at the very least some mental calculation is required  which goes beyond the concept of ""by inspection"" in my opinion. Or does it? Or is there something else I'm missing? Thank you.",,"['linear-algebra', 'terminology']"
40,Skew-triangular (?) matrices and their properties,Skew-triangular (?) matrices and their properties,,"By skew-triangular matrices, I mean matrices with the following sparsity patterns. $$\begin{bmatrix} \times & \times & \times & \times \\   \times & \times & \times \\   \times & \times \\   \times \\ \end{bmatrix} \qquad \text{or}\qquad \begin{bmatrix}  &  &  & \times \\  &  & \times & \times \\  & \times & \times & \times \\ \times & \times & \times & \times \\ \end{bmatrix}$$ Simple experiments with Mathematica show that the inverse of the first type is a matrix of the second type (and vice versa, of course). Do these matrices have their real name and where do they occur? What other interesting properties do they possess? I am asking this just out of curiosity because a brief googling failed to give me the answer.","By skew-triangular matrices, I mean matrices with the following sparsity patterns. Simple experiments with Mathematica show that the inverse of the first type is a matrix of the second type (and vice versa, of course). Do these matrices have their real name and where do they occur? What other interesting properties do they possess? I am asking this just out of curiosity because a brief googling failed to give me the answer.","\begin{bmatrix}
\times & \times & \times & \times \\
  \times & \times & \times \\
  \times & \times \\
  \times \\
\end{bmatrix} \qquad \text{or}\qquad
\begin{bmatrix}
 &  &  & \times \\
 &  & \times & \times \\
 & \times & \times & \times \\
\times & \times & \times & \times \\
\end{bmatrix}","['linear-algebra', 'matrices', 'reference-request', 'terminology']"
41,What is the significance of theoretical linear algebra in machine learning/computer vision research?,What is the significance of theoretical linear algebra in machine learning/computer vision research?,,"I am a computer science research student working in application of Machine Learning to solve Computer Vision problems. Since, lot of linear algebra(eigenvalues, SVD etc.) comes up when reading Machine Learning/Vision literature, I decided to take a linear algebra course this semester. Much to my surprise, the course didn't look at all like Gilbert Strang's Applied Linear algebra(on OCW) I had started taking earlier. The course textbook is Linear Algebra by Hoffman and Kunze . We started with concepts of Abstract algebra like groups, fields, rings, isomorphism, quotient groups etc. And then moved on to study ""theoretical"" linear algebra over finite fields, where we cover proofs for important theorms/lemmas in the following topics: Vector spaces, linear span, linear independence, existence of basis.   Linear transformations. Solutions of linear equations, row reduced   echelon form, complete echelon form,rank. Minimal polynomial of a   linear transformation. Jordan canonical form. Determinants.   Characteristic polynomial, eigenvalues and eigenvectors. Inner product   space. Gram Schmidt orthogonalization. Unitary and Hermitian   transformations. Diagonalization of Hermitian transformations. I wanted to understand if there is any significance/application of understanding these proofs in machine learning/computer vision research or should I be better off focusing on the applied Linear Algebra?","I am a computer science research student working in application of Machine Learning to solve Computer Vision problems. Since, lot of linear algebra(eigenvalues, SVD etc.) comes up when reading Machine Learning/Vision literature, I decided to take a linear algebra course this semester. Much to my surprise, the course didn't look at all like Gilbert Strang's Applied Linear algebra(on OCW) I had started taking earlier. The course textbook is Linear Algebra by Hoffman and Kunze . We started with concepts of Abstract algebra like groups, fields, rings, isomorphism, quotient groups etc. And then moved on to study ""theoretical"" linear algebra over finite fields, where we cover proofs for important theorms/lemmas in the following topics: Vector spaces, linear span, linear independence, existence of basis.   Linear transformations. Solutions of linear equations, row reduced   echelon form, complete echelon form,rank. Minimal polynomial of a   linear transformation. Jordan canonical form. Determinants.   Characteristic polynomial, eigenvalues and eigenvectors. Inner product   space. Gram Schmidt orthogonalization. Unitary and Hermitian   transformations. Diagonalization of Hermitian transformations. I wanted to understand if there is any significance/application of understanding these proofs in machine learning/computer vision research or should I be better off focusing on the applied Linear Algebra?",,"['linear-algebra', 'machine-learning']"
42,Is there a similarity transformation rendering all diagonal elements of a matrix equal?,Is there a similarity transformation rendering all diagonal elements of a matrix equal?,,"I'm especially interested in SL$(2,\mathbb C)$, i.e. $2\times2$ matrices with determinant one, in which case I'm looking for a transformation from $\begin{pmatrix}a&b\\c&d\end{pmatrix}$ to $\begin{pmatrix}\frac{a+d}2&x\\ y&\frac{a+d}2\end{pmatrix}$ (the trace is conserved). Does such a similarity transformation exist? What about general $n\times n$ matrices? Bonus points for an analytical formula (even if only for the 2x2 case).","I'm especially interested in SL$(2,\mathbb C)$, i.e. $2\times2$ matrices with determinant one, in which case I'm looking for a transformation from $\begin{pmatrix}a&b\\c&d\end{pmatrix}$ to $\begin{pmatrix}\frac{a+d}2&x\\ y&\frac{a+d}2\end{pmatrix}$ (the trace is conserved). Does such a similarity transformation exist? What about general $n\times n$ matrices? Bonus points for an analytical formula (even if only for the 2x2 case).",,['linear-algebra']
43,$k[x]$-module and cyclic module over a finite dimensional vector space,-module and cyclic module over a finite dimensional vector space,k[x],"Given a finite dimensional vector space $V$ over a field $k$ and a linear transformation $T: V \rightarrow V$ we can make $V$ a $k[x]$-module via the map: $$(a_{0}+a_{1}x+\cdots+a_{n}x^{n}) \cdot v \mapsto \sum a_{i}T^{i}(v).$$ On page 10 of this file I was reading the following result: ""$V$ is a cyclic $k[x]$-module iff the minimal polynomial and the characteristic polynomial coincide"" Can anyone please show me how to prove this? Or any reference? Thanks.","Given a finite dimensional vector space $V$ over a field $k$ and a linear transformation $T: V \rightarrow V$ we can make $V$ a $k[x]$-module via the map: $$(a_{0}+a_{1}x+\cdots+a_{n}x^{n}) \cdot v \mapsto \sum a_{i}T^{i}(v).$$ On page 10 of this file I was reading the following result: ""$V$ is a cyclic $k[x]$-module iff the minimal polynomial and the characteristic polynomial coincide"" Can anyone please show me how to prove this? Or any reference? Thanks.",,"['linear-algebra', 'vector-spaces']"
44,Expressing the product Ax as a linear combination of the column vectors of A,Expressing the product Ax as a linear combination of the column vectors of A,,"Expressing the product Ax as a linear combination of the column vectors of $A$= $\begin{bmatrix} 4 & 0 & -1\\  3 & 6 & 2\\  0 & -1 & 4 \end{bmatrix}$ $\vec{x}$=$\begin{bmatrix} -2\\  3\\  5 \end{bmatrix}$ I get it now. They just want me to multiply the two vectors together. I end up with $\begin{bmatrix} -13\\\  22\\\  17 \end{bmatrix}$","Expressing the product Ax as a linear combination of the column vectors of $A$= $\begin{bmatrix} 4 & 0 & -1\\  3 & 6 & 2\\  0 & -1 & 4 \end{bmatrix}$ $\vec{x}$=$\begin{bmatrix} -2\\  3\\  5 \end{bmatrix}$ I get it now. They just want me to multiply the two vectors together. I end up with $\begin{bmatrix} -13\\\  22\\\  17 \end{bmatrix}$",,[]
45,"For $2 \times 2$-matrix, $\|A^2\|=\|A\|^2$ implies that A is normal","For -matrix,  implies that A is normal",2 \times 2 \|A^2\|=\|A\|^2,"I am reading book ""A Hilbert space problem book"", written by Halmos and getting some trouble. In problem 205, he claim that For two-by-two matrices an unpleasant computation proves a strong converse: if $\|A^2\|=\|A\|^2$, then $A$ is normal. I tried to use the determinant as the norm of $A$, but it seems to be useless. I also found a similar question on Mathstackexchange. Someone said that we should only consider matrix $\begin{pmatrix} \lambda & 1\\  0 & \lambda \end{pmatrix}$. I, however, do not understand why. Could you please give me some hint to solve this claim? Thank you for your time.","I am reading book ""A Hilbert space problem book"", written by Halmos and getting some trouble. In problem 205, he claim that For two-by-two matrices an unpleasant computation proves a strong converse: if $\|A^2\|=\|A\|^2$, then $A$ is normal. I tried to use the determinant as the norm of $A$, but it seems to be useless. I also found a similar question on Mathstackexchange. Someone said that we should only consider matrix $\begin{pmatrix} \lambda & 1\\  0 & \lambda \end{pmatrix}$. I, however, do not understand why. Could you please give me some hint to solve this claim? Thank you for your time.",,"['linear-algebra', 'matrices', 'normal-operator']"
46,Probability that a random matrix will have full column rank?,Probability that a random matrix will have full column rank?,,"It is known that for a random matrix $A\in \mathbb{C}^{M\times K}$ with $M>K$, if each element is i.i.d. Gaussian generated, then $A$ has full column rank with probability one. Now, assume that each element of $A$ is in the form of $e^{j\theta}$, where $\theta$'s are i.i.d. uniformly distributed in $(0,2\pi)$. Can I prove that $A$ has full column rank, with probability one? Thanks.","It is known that for a random matrix $A\in \mathbb{C}^{M\times K}$ with $M>K$, if each element is i.i.d. Gaussian generated, then $A$ has full column rank with probability one. Now, assume that each element of $A$ is in the form of $e^{j\theta}$, where $\theta$'s are i.i.d. uniformly distributed in $(0,2\pi)$. Can I prove that $A$ has full column rank, with probability one? Thanks.",,"['linear-algebra', 'random-matrices']"
47,On an operator norm inequality for two triangular matrices,On an operator norm inequality for two triangular matrices,,"Let $n\ge3$. Create two copies $A$ and $B$ of the $n\times n$ lower triangular matrices of ones. Then set $A(2,1)=A(n,n-1)=B(n,n-1)=B(n,n-2)=0$ (with all other entries in the lower triangular parts still equal to $1$). I want to show that the largest eigenvalue of $AA^T$ is greater than or equal to that of $BB^T$. Any hint to prove it?","Let $n\ge3$. Create two copies $A$ and $B$ of the $n\times n$ lower triangular matrices of ones. Then set $A(2,1)=A(n,n-1)=B(n,n-1)=B(n,n-2)=0$ (with all other entries in the lower triangular parts still equal to $1$). I want to show that the largest eigenvalue of $AA^T$ is greater than or equal to that of $BB^T$. Any hint to prove it?",,"['linear-algebra', 'matrices']"
48,Trace operators on modules,Trace operators on modules,,"This question is motivated by this other one. A classical result of linear algebra states what follows Up to scalar, trace is the only linear operator $\text{M}(n,k) \stackrel{t}{\to} k $ such that $t(AB) = t(BA)$ . So there are two natural generalization of the problem. The first one is in the direction of replacing $k$ with a ring $A$ . The other is to ignore the restriction to finite dimensional vector spaces. Consider a finitely generated module $M$ on a commutative unital ring $A$ . End( $M$ ) has a natural module structure. A trace operator is a morphism of modules End( $ M) \stackrel{tr}{\to} A$ such that $tr(fg)=tr(gf)$ . Tr, the set of trace operators is a submodule of $\text{Hom}_A(\text{End(M)}, A)$ . In the special case of vector spaces, its dimension is $1$ . What happens for general rings? Can we recover a partial result? How many trace operators are there? What should be the trace for a morphism on modules? Sum of eigenvalues looks to me naive. Examples In the case of $M = \mathbb{Z}_6, A = \mathbb{Z}$ . There are no trace operators. Attempts In the case of vector spaces the submodule genetated by $fg-gf$ has codimension 1. What's its codimension in the case of free modules? Maybe this question makes much more sense when $A$ is a PID.","This question is motivated by this other one. A classical result of linear algebra states what follows Up to scalar, trace is the only linear operator such that . So there are two natural generalization of the problem. The first one is in the direction of replacing with a ring . The other is to ignore the restriction to finite dimensional vector spaces. Consider a finitely generated module on a commutative unital ring . End( ) has a natural module structure. A trace operator is a morphism of modules End( such that . Tr, the set of trace operators is a submodule of . In the special case of vector spaces, its dimension is . What happens for general rings? Can we recover a partial result? How many trace operators are there? What should be the trace for a morphism on modules? Sum of eigenvalues looks to me naive. Examples In the case of . There are no trace operators. Attempts In the case of vector spaces the submodule genetated by has codimension 1. What's its codimension in the case of free modules? Maybe this question makes much more sense when is a PID.","\text{M}(n,k) \stackrel{t}{\to} k  t(AB) = t(BA) k A M A M  M) \stackrel{tr}{\to} A tr(fg)=tr(gf) \text{Hom}_A(\text{End(M)}, A) 1 M = \mathbb{Z}_6, A = \mathbb{Z} fg-gf A","['linear-algebra', 'abstract-algebra', 'modules', 'representation-theory']"
49,Matrix given by $a_{ij} = 1/(i+j)$ is non-singular.,Matrix given by  is non-singular.,a_{ij} = 1/(i+j),"What is a smart way to see that $$A\stackrel{\cdot}{=} \begin{bmatrix} 1/2 & 1/3 & \cdots & 1/(n+1) \\ 1/3 & 1/4 & \cdots & 1/(n+2) \\ \vdots & \vdots & \ddots & \vdots \\ 1/(n+1) & 1/(n+2) & \cdots & 1/(2n) \end{bmatrix} $$is non-singular? I computed $\det A$ for $n=1,2$ and $3$ but I failed to see the pattern. The context is as follows: in $\Bbb R[x] $ with inner product given by $\langle p(x),q(x)\rangle \stackrel{\cdot}{=}\int_0^1p(x)q(x)\,{\rm d}x$, I want to see that if $U\stackrel{\cdot}{=} \{p(x) \in \Bbb R[x]\mid p(0)=0\}$, then $U^{\perp} =\{0\}$ (and hence $U^{\perp\perp}=\Bbb R[x]\neq U$). I took $f(x) \in U^\perp$ and computed  $\langle f(x),x^k\rangle$ for $k\geq 1$. I obtained a homogeneous system for the coefficients of $f(x)$ which has $A $ as the matrix associated. So if $A$ is non-singular I'm done.","What is a smart way to see that $$A\stackrel{\cdot}{=} \begin{bmatrix} 1/2 & 1/3 & \cdots & 1/(n+1) \\ 1/3 & 1/4 & \cdots & 1/(n+2) \\ \vdots & \vdots & \ddots & \vdots \\ 1/(n+1) & 1/(n+2) & \cdots & 1/(2n) \end{bmatrix} $$is non-singular? I computed $\det A$ for $n=1,2$ and $3$ but I failed to see the pattern. The context is as follows: in $\Bbb R[x] $ with inner product given by $\langle p(x),q(x)\rangle \stackrel{\cdot}{=}\int_0^1p(x)q(x)\,{\rm d}x$, I want to see that if $U\stackrel{\cdot}{=} \{p(x) \in \Bbb R[x]\mid p(0)=0\}$, then $U^{\perp} =\{0\}$ (and hence $U^{\perp\perp}=\Bbb R[x]\neq U$). I took $f(x) \in U^\perp$ and computed  $\langle f(x),x^k\rangle$ for $k\geq 1$. I obtained a homogeneous system for the coefficients of $f(x)$ which has $A $ as the matrix associated. So if $A$ is non-singular I'm done.",,"['linear-algebra', 'determinant', 'inner-products']"
50,Find basis vectors of the vector space of all $4 \times 4$ magic squares,Find basis vectors of the vector space of all  magic squares,4 \times 4,"I'm taking a course in linear algebra and I need to solve this problem: Let's define a magic square as a matrix whose sums of all the numbers on a line, a column and on both the main diagonal and the main anti-diagonal are the same. Prove that $4 \times 4$ magic squares form a vector space. Find the basis vectors of this vector space. There are more questions in the exercise, but I guess these are the most important ones that it will help me solve other questions. I have already searched almost the whole Internet, but I'm not able to find the answer. Thank you!","I'm taking a course in linear algebra and I need to solve this problem: Let's define a magic square as a matrix whose sums of all the numbers on a line, a column and on both the main diagonal and the main anti-diagonal are the same. Prove that magic squares form a vector space. Find the basis vectors of this vector space. There are more questions in the exercise, but I guess these are the most important ones that it will help me solve other questions. I have already searched almost the whole Internet, but I'm not able to find the answer. Thank you!",4 \times 4,"['linear-algebra', 'matrices', 'vector-spaces', 'magic-square']"
51,For every *non-square* matrix prove that $AA^t$ or/and $A^tA$ is singular,For every *non-square* matrix prove that  or/and  is singular,AA^t A^tA,"For every non-square matrix prove that $AA^t$ or/and $A^tA$ is singular. Like the title, I want to prove this and I tried to think of ways to prove it but I couldn't think of some.. I know by this answer that $AA^t$ is symmetric but I cant make the connection. If someone has at least a hint to that It'll be great if you could write it in the comment section so I could give it a shot! Thanks in advance.","For every non-square matrix prove that $AA^t$ or/and $A^tA$ is singular. Like the title, I want to prove this and I tried to think of ways to prove it but I couldn't think of some.. I know by this answer that $AA^t$ is symmetric but I cant make the connection. If someone has at least a hint to that It'll be great if you could write it in the comment section so I could give it a shot! Thanks in advance.",,"['linear-algebra', 'matrices', 'transpose']"
52,What is the maximum number of quadrants in $n$ dimensional space that a $k$ dimensional hyperplane can pass through?,What is the maximum number of quadrants in  dimensional space that a  dimensional hyperplane can pass through?,n k,"Assume that the hyperplane passes through the origin i.e. it is the span of some $k$ linearly independent vectors in $n$-space. For example, for $n,k = 2,1,$ we have a line in the 2d plane passing through the origin, so the answer is 2. For $n,k=3,2$, it is 6. In fact, I have proven that $F(n,n-1)=F(n-1,n-2)+2^{n-1}$. This follows by considering the $(n-1)$-subspace for which one of the coordinates is 0. The intersection of this subspace with the hyperplane is an $n-2$ dimensional hyperplane which passes through the origin and $F(n-1,n-2)$ quadrants of the subspace. For each quadrant, the original hyperplane will at maximum pass through two corresponding quadrants of the $n$-space. And for each quadrant it does not pass through, the original can pass through at most one quadrant. The recurrence follows. But I am unable to extend this proof to general $k$, or even $n-2$. Any help is appreciated.","Assume that the hyperplane passes through the origin i.e. it is the span of some $k$ linearly independent vectors in $n$-space. For example, for $n,k = 2,1,$ we have a line in the 2d plane passing through the origin, so the answer is 2. For $n,k=3,2$, it is 6. In fact, I have proven that $F(n,n-1)=F(n-1,n-2)+2^{n-1}$. This follows by considering the $(n-1)$-subspace for which one of the coordinates is 0. The intersection of this subspace with the hyperplane is an $n-2$ dimensional hyperplane which passes through the origin and $F(n-1,n-2)$ quadrants of the subspace. For each quadrant, the original hyperplane will at maximum pass through two corresponding quadrants of the $n$-space. And for each quadrant it does not pass through, the original can pass through at most one quadrant. The recurrence follows. But I am unable to extend this proof to general $k$, or even $n-2$. Any help is appreciated.",,"['linear-algebra', 'geometry']"
53,Does non-degenerate and positive semi-definite imply positive definite?,Does non-degenerate and positive semi-definite imply positive definite?,,"Let $V$ be a finite-dimensional real vector space and $$B:V\times V\to\Bbb R$$ a bilinear form. Suppose that $B$ is non-degenerate and positive semi-definite, i.e. $B(X,Y)=0,\forall Y\in V\implies X=0$ $B(X,X)\ge0,\forall X\in V$. Does that imply that $B$ is positive definite? First I thought it would, but then I couldn't prove it. If $B(X,X)=0$ then I would like to prove that $B(X,Y)=0$ for all $Y\in V$ so that $X=0$. But maybe this is not necessarily true?","Let $V$ be a finite-dimensional real vector space and $$B:V\times V\to\Bbb R$$ a bilinear form. Suppose that $B$ is non-degenerate and positive semi-definite, i.e. $B(X,Y)=0,\forall Y\in V\implies X=0$ $B(X,X)\ge0,\forall X\in V$. Does that imply that $B$ is positive definite? First I thought it would, but then I couldn't prove it. If $B(X,X)=0$ then I would like to prove that $B(X,Y)=0$ for all $Y\in V$ so that $X=0$. But maybe this is not necessarily true?",,['linear-algebra']
54,"If a matrix's eigenvalues are all $1$, is the matrix the identity?","If a matrix's eigenvalues are all , is the matrix the identity?",1,"It's that time of night when my girl and I bicker about matrices. Tonight we ponder whether a square matrix of dimension $d$ which has a spectrum of $1$'s with multiplicity $d$ must be $I$. If such a matrix is diagonalizable, then it must be $I$. But we're not sure what would happen if it is not diagonalizable or whether that is even a possibility. Relatedly, we're having a tough time thinking up a matrix other than $I$ whose spectrum is all $1$'s.","It's that time of night when my girl and I bicker about matrices. Tonight we ponder whether a square matrix of dimension $d$ which has a spectrum of $1$'s with multiplicity $d$ must be $I$. If such a matrix is diagonalizable, then it must be $I$. But we're not sure what would happen if it is not diagonalizable or whether that is even a possibility. Relatedly, we're having a tough time thinking up a matrix other than $I$ whose spectrum is all $1$'s.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
55,The sum of subspaces is the smallest subspace containing all the summands,The sum of subspaces is the smallest subspace containing all the summands,,"In Axler's Linear Algebra Done Right the theorem given is Suppose $U_1,\ldots, U_m$ are subspaces of $V$. Then $U_1+\cdots+ U_m$ is the smallest subspace of $V$ containing $U_1,\ldots, U_m.$ I can see that the sum will be a subspace of $V$. What I don't understand is the following paragraph: ""Clearly $U_1,\ldots, U_m$ are all contained in $U_1+\cdots+ U_m$ (to see this consider sums $u_1 + \cdots+u_m$ where all except one of the $u$s is $0$). Conversely every subspace of V containing $U_1,\ldots, U_m$ must contain $U_1+\cdots+ U_m$ (because subspaces must contain all finite sums of their elements). Thus, $U_1+\cdots+ U_m$ is the smallest subspace of $V$ containing $U_1,\cdots, U_m.""$ This question was addressed here: Misunderstanding in the proof that the sum of subspaces is the smallest containing subspace. but the answer wasn't clear enough for me. I can see how the first part is true, that  $U_1,\ldots, U_m$ are all contained in $U_1+\cdots+ U_m$ and the second part, $U_1,\ldots, U_m$ must contain $U_1+\cdots+ U_m$ but I don't understand how together they prove the theorem. Can someone please take the trouble of giving an intuitive explanation, or a geometric one if it is possible? Thanks a lot.","In Axler's Linear Algebra Done Right the theorem given is Suppose $U_1,\ldots, U_m$ are subspaces of $V$. Then $U_1+\cdots+ U_m$ is the smallest subspace of $V$ containing $U_1,\ldots, U_m.$ I can see that the sum will be a subspace of $V$. What I don't understand is the following paragraph: ""Clearly $U_1,\ldots, U_m$ are all contained in $U_1+\cdots+ U_m$ (to see this consider sums $u_1 + \cdots+u_m$ where all except one of the $u$s is $0$). Conversely every subspace of V containing $U_1,\ldots, U_m$ must contain $U_1+\cdots+ U_m$ (because subspaces must contain all finite sums of their elements). Thus, $U_1+\cdots+ U_m$ is the smallest subspace of $V$ containing $U_1,\cdots, U_m.""$ This question was addressed here: Misunderstanding in the proof that the sum of subspaces is the smallest containing subspace. but the answer wasn't clear enough for me. I can see how the first part is true, that  $U_1,\ldots, U_m$ are all contained in $U_1+\cdots+ U_m$ and the second part, $U_1,\ldots, U_m$ must contain $U_1+\cdots+ U_m$ but I don't understand how together they prove the theorem. Can someone please take the trouble of giving an intuitive explanation, or a geometric one if it is possible? Thanks a lot.",,"['linear-algebra', 'proof-explanation']"
56,How to calculate expected value of matrix norms of $A^TA$?,How to calculate expected value of matrix norms of ?,A^TA,"Let $A$ be a random $m$ by $n$ rectangular sign matrix, chosen uniformly at random, with $m < n$. To be clear,  $A$ is a matrix whose entries are chosen from $\{1,-1\}$. Let $B = A^T A$.  We know, for example, that $B$ is a square and symmetric $n$ by $n$ matrix with all its  diagonal entries equal to $m$ exactly.   I am trying to learn how to calculate the expected Frobenius and spectral norm of $B$.  We can assume both $m$ and $n$ are large if that helps give a good bound or estimate. How can you calculate $\mathbb{E}(||B||_F)$ and   $\mathbb{E}(||B||_2)$? The expected Frobenius norm of $B$ is defined to be $$ \mathbb{E}(||B||_F)=\mathbb{E}\left(\sqrt{\sum_{i=1}^n\sum_{j=1}^n |b_{ij}|^2}\right) $$ where $b_{ij}$ are the elements of $B$. The expected spectral norm of $B$ is defined to be $$ \mathbb{E}(||B||_2)= \mathbb{E}\left(\max_{|x|_2 \ne 0}\frac{|Bx|_2}{|x|_2}\right). $$ Cross-posted to https://mathoverflow.net/questions/222994/how-to-calculate-expected-value-of-matrix-norms-of-ata now.  I will update this question if anything substantive is added there.","Let $A$ be a random $m$ by $n$ rectangular sign matrix, chosen uniformly at random, with $m < n$. To be clear,  $A$ is a matrix whose entries are chosen from $\{1,-1\}$. Let $B = A^T A$.  We know, for example, that $B$ is a square and symmetric $n$ by $n$ matrix with all its  diagonal entries equal to $m$ exactly.   I am trying to learn how to calculate the expected Frobenius and spectral norm of $B$.  We can assume both $m$ and $n$ are large if that helps give a good bound or estimate. How can you calculate $\mathbb{E}(||B||_F)$ and   $\mathbb{E}(||B||_2)$? The expected Frobenius norm of $B$ is defined to be $$ \mathbb{E}(||B||_F)=\mathbb{E}\left(\sqrt{\sum_{i=1}^n\sum_{j=1}^n |b_{ij}|^2}\right) $$ where $b_{ij}$ are the elements of $B$. The expected spectral norm of $B$ is defined to be $$ \mathbb{E}(||B||_2)= \mathbb{E}\left(\max_{|x|_2 \ne 0}\frac{|Bx|_2}{|x|_2}\right). $$ Cross-posted to https://mathoverflow.net/questions/222994/how-to-calculate-expected-value-of-matrix-norms-of-ata now.  I will update this question if anything substantive is added there.",,"['linear-algebra', 'probability']"
57,Why is the the double dual functor on finite-dimensional vector spaces naturally isomorphic to the identity?,Why is the the double dual functor on finite-dimensional vector spaces naturally isomorphic to the identity?,,"$\require{AMScd}$ Note: I have already seen this question, which asks about a specific aspect of the construction - here I am trying to construct this functor and failing at a very different stage. We are trying to show that the two functors $\sf Id:FDVec_k\to FDVec_k$ and $(\cdot)^{\vee\vee}:\sf FDVec_k\to FDVec_k$ are naturally isomorphic. I've never seen such an argument before, and working through the linear algebra is giving me some problems. An element of $V^{\vee\vee}$ is defined to be a linear map from $V^{\vee}$ to $k$, so it takes linear maps $V\to k$ to elements of $k$. For a natural isomorphism we need isomorphisms $$m_V:V\to V^{\vee\vee}.$$ We input a vector and output a map $\operatorname{Hom}(V,k)\to k$: $$m_V(v)(f:V\to k)=f(v)\in k.$$ We then need to show that diagrams $$ \begin{CD} V @>\phi>> W\\ @V{m_V}VV @VV{m_W}V\\ V^{\vee\vee} @>>\phi^{\vee\vee}> W^{\vee\vee} \end{CD} $$ commute if $\phi$ is a linear map of finite-dimensional vector spaces, so $$m_W\circ\phi=\phi^{\vee\vee}\circ m_V.$$ However, my problem ultimately ends up being that I can't interpret $\phi^{\vee\vee}$ sufficiently well. It should take linear maps $\operatorname{Hom}(V,k)\to k$ to linear maps $\operatorname{Hom}(W,k)\to k$ in a way that is somehow induced by $\phi$, but I'm not sure what this way is. My attempt: $$[\phi^{\vee\vee}(f:V^{\vee}\to k)](g:W\to k)\in k$$ but I get no further than this. I ask purely for clarification as to what a double dual map really is (the several layers of abstraction present in the problem are quite hard for my beginner's mind to handle).","$\require{AMScd}$ Note: I have already seen this question, which asks about a specific aspect of the construction - here I am trying to construct this functor and failing at a very different stage. We are trying to show that the two functors $\sf Id:FDVec_k\to FDVec_k$ and $(\cdot)^{\vee\vee}:\sf FDVec_k\to FDVec_k$ are naturally isomorphic. I've never seen such an argument before, and working through the linear algebra is giving me some problems. An element of $V^{\vee\vee}$ is defined to be a linear map from $V^{\vee}$ to $k$, so it takes linear maps $V\to k$ to elements of $k$. For a natural isomorphism we need isomorphisms $$m_V:V\to V^{\vee\vee}.$$ We input a vector and output a map $\operatorname{Hom}(V,k)\to k$: $$m_V(v)(f:V\to k)=f(v)\in k.$$ We then need to show that diagrams $$ \begin{CD} V @>\phi>> W\\ @V{m_V}VV @VV{m_W}V\\ V^{\vee\vee} @>>\phi^{\vee\vee}> W^{\vee\vee} \end{CD} $$ commute if $\phi$ is a linear map of finite-dimensional vector spaces, so $$m_W\circ\phi=\phi^{\vee\vee}\circ m_V.$$ However, my problem ultimately ends up being that I can't interpret $\phi^{\vee\vee}$ sufficiently well. It should take linear maps $\operatorname{Hom}(V,k)\to k$ to linear maps $\operatorname{Hom}(W,k)\to k$ in a way that is somehow induced by $\phi$, but I'm not sure what this way is. My attempt: $$[\phi^{\vee\vee}(f:V^{\vee}\to k)](g:W\to k)\in k$$ but I get no further than this. I ask purely for clarification as to what a double dual map really is (the several layers of abstraction present in the problem are quite hard for my beginner's mind to handle).",,"['linear-algebra', 'category-theory']"
58,Is every complex Lie algebra a complexification?,Is every complex Lie algebra a complexification?,,"I'm wondering if every finite-dimensional complex Lie algebra can be written as a complexification of a real Lie algebra. At the vector space level, clearly every $\mathbb{C}^n$ is a complexification of $\mathbb{R}^n$, but at the level of bracket structure, it's not clear to me if we can consistently ""decomplexify"" the Lie algebra. Putting it another way, for any given $n$-dimensional complex Lie algebra, can we find a basis $\{e_1,\ldots,e_n\}$ such that the structure constants are all real?","I'm wondering if every finite-dimensional complex Lie algebra can be written as a complexification of a real Lie algebra. At the vector space level, clearly every $\mathbb{C}^n$ is a complexification of $\mathbb{R}^n$, but at the level of bracket structure, it's not clear to me if we can consistently ""decomplexify"" the Lie algebra. Putting it another way, for any given $n$-dimensional complex Lie algebra, can we find a basis $\{e_1,\ldots,e_n\}$ such that the structure constants are all real?",,"['linear-algebra', 'lie-groups', 'lie-algebras']"
59,"""All positive solution"" to system of linear equations","""All positive solution"" to system of linear equations",,"My question is: Are there any criteria to decide whether a system of linear equations allows a solution where all variables are greater than 0? Clearly, I could compute the solution space and check if there is a solution that satisfies the condition. But is there any better way?","My question is: Are there any criteria to decide whether a system of linear equations allows a solution where all variables are greater than 0? Clearly, I could compute the solution space and check if there is a solution that satisfies the condition. But is there any better way?",,['linear-algebra']
60,Non-examples for the Kato-Rellich Theorem,Non-examples for the Kato-Rellich Theorem,,"The Kato-Rellich Theorem is a classical result stating that if $A,B$ are unbounded operators on a Hilbert space with $A$ self-adjoint, $B$ symmetric, $\mathcal D (A)\subset \mathcal D(B)$ and $$ \|Bx\|\leq a\|A x\|+b\|x\|,\;\;x\in\mathcal D(A) $$ for positive constants $a,b$, with $a<1$, then $A+B$ is self-adjoint on the domain of $A$. The infimum of those $a$ for which the above inequality holds is called the relative bound of $B$. Thus, $A+B$ is self-adjoint if $B$ is relatively bounded with bound $a<1$. I'm interested in non-examples where $A+B$ is not essentially self-adjoint even though $A$ is self-adjoint, $B$ is symmetric and $\mathcal D(A)\subset\mathcal D(B)$. I have tried considering multiplication operators to no avail. Let $M_\phi$ denote the multiplication operator $$ M_\phi h=\phi h,\;\;\mathcal D(M_{\phi})=\{ h|\phi h\in L_2\}, $$ and let $A=M_f$, $B=M_g$ with $f,g$ real functions, finite almost everywhere. Since $A$ and $B$ are closed, the assumption $\mathcal D(A)\subset\mathcal D(B)$ implies the bound $$ \|B h\|\leq C(\|A h\| +\|h\|) $$ for some $C>0$. But then, for $h$ non-zero only on the set $\{x||f(x)|\leq M\}$, one obtains $$ \|B  h\|\leq C(1+M)\|h\|, $$ which implies that $|g(x)|\leq C(1+M)$ almost everywhere on $\{x||f(x)|\leq M\}$. This in turn implies $|f(x)+g(x)|\leq (M+ CM + C)$ on that same set. But then $\mathcal D(A)$ is a core for $\mathcal D(M_{f+g})$, which implies that $A+B$ is essentially self-adjoint. Next up were Schrdinger operators. 'Unfortunately', Theorem XIII.96 in Methods of Modern Mathematical Physics vol 4 by Reed and Simon implies that (on $\mathbb R^d$, with $d\leq 3$) the inclusion $\mathcal D(-\Delta)\subset\mathcal D(V)$ is sufficient in order to conclude that $V$ is relatively $(-\Delta)$-bounded with bound $0$.","The Kato-Rellich Theorem is a classical result stating that if $A,B$ are unbounded operators on a Hilbert space with $A$ self-adjoint, $B$ symmetric, $\mathcal D (A)\subset \mathcal D(B)$ and $$ \|Bx\|\leq a\|A x\|+b\|x\|,\;\;x\in\mathcal D(A) $$ for positive constants $a,b$, with $a<1$, then $A+B$ is self-adjoint on the domain of $A$. The infimum of those $a$ for which the above inequality holds is called the relative bound of $B$. Thus, $A+B$ is self-adjoint if $B$ is relatively bounded with bound $a<1$. I'm interested in non-examples where $A+B$ is not essentially self-adjoint even though $A$ is self-adjoint, $B$ is symmetric and $\mathcal D(A)\subset\mathcal D(B)$. I have tried considering multiplication operators to no avail. Let $M_\phi$ denote the multiplication operator $$ M_\phi h=\phi h,\;\;\mathcal D(M_{\phi})=\{ h|\phi h\in L_2\}, $$ and let $A=M_f$, $B=M_g$ with $f,g$ real functions, finite almost everywhere. Since $A$ and $B$ are closed, the assumption $\mathcal D(A)\subset\mathcal D(B)$ implies the bound $$ \|B h\|\leq C(\|A h\| +\|h\|) $$ for some $C>0$. But then, for $h$ non-zero only on the set $\{x||f(x)|\leq M\}$, one obtains $$ \|B  h\|\leq C(1+M)\|h\|, $$ which implies that $|g(x)|\leq C(1+M)$ almost everywhere on $\{x||f(x)|\leq M\}$. This in turn implies $|f(x)+g(x)|\leq (M+ CM + C)$ on that same set. But then $\mathcal D(A)$ is a core for $\mathcal D(M_{f+g})$, which implies that $A+B$ is essentially self-adjoint. Next up were Schrdinger operators. 'Unfortunately', Theorem XIII.96 in Methods of Modern Mathematical Physics vol 4 by Reed and Simon implies that (on $\mathbb R^d$, with $d\leq 3$) the inclusion $\mathcal D(-\Delta)\subset\mathcal D(V)$ is sufficient in order to conclude that $V$ is relatively $(-\Delta)$-bounded with bound $0$.",,"['linear-algebra', 'functional-analysis', 'partial-differential-equations', 'hilbert-spaces', 'examples-counterexamples']"
61,All linear combinations diagonalizable over $\mathbb{C}$ implies commuting.,All linear combinations diagonalizable over  implies commuting.,\mathbb{C},"Let $A, B \in \mathcal{M}_n(\mathbb{C})$ such that for all $x,y \in \mathbb{C}, xA+yB$ is diagonalizable. Show that $AB=BA$ . My idea (not really an attempt) : It suffices to show that $A$ and $B$ are simultaneous diagonalizable. Then one can use the theorem that says $A$ and $B$ are diagonalizable at the same time if and only if they commute. Does anyone has a way to prove that $A$ and $B$ are simultaneous diagonalizable ?",Let such that for all is diagonalizable. Show that . My idea (not really an attempt) : It suffices to show that and are simultaneous diagonalizable. Then one can use the theorem that says and are diagonalizable at the same time if and only if they commute. Does anyone has a way to prove that and are simultaneous diagonalizable ?,"A, B \in \mathcal{M}_n(\mathbb{C}) x,y \in \mathbb{C}, xA+yB AB=BA A B A B A B",[]
62,Generalisation of the trace cyclic theorem to partial traces.,Generalisation of the trace cyclic theorem to partial traces.,,"The trace-cyclic theorem says that linear operators commute (or cycle) within the trace, that is $${\rm Tr}(XY) = {\rm Tr}(YX).$$ Now if $X$ and $Y$ operate on a product space $H_A \times H_B  \times ...$, it is also possible to take the partial trace over any combination of those spaces.  But for various reasons, the cyclic theorem will not hold in that case. $${\rm Tr_A}(XY) \ne {\rm Tr_A}(YX)  \hspace{4em} \mbox{(except special cases).}$$ My general question is whether there is still some constraint on the commuted trace.  That is, does the trace-cyclic theorem generalise in some weakened form. My specific question involves lots of details: We are working in complex Hilbert space (quantum). $X =   \left|R\rangle\langle{E}\right|$ is rank-1 i.e. an outer product of arbitrarily entangled Hilbert-space vectors. $Y = U$ is unitary, but it freely entangles the subspaces. There can be more than two subspaces and it is given that $$    {\rm Tr_{\bar S}}(XU) = 0.  $$ Where $\bar{S}$ means ""all subspaces other than $S$"", and $S$ is arbitrary.  That is, $X$ is chosen such that when you trace over $XU$ and leave any one subspace alive, you always end up with zero.  So what I want to know is, whether: $$    {\rm Tr_{\bar S}}(UX) = 0.  $$ If this is true, I would like to know why.  If it isn't true, I would like to know if any related thing is true.","The trace-cyclic theorem says that linear operators commute (or cycle) within the trace, that is $${\rm Tr}(XY) = {\rm Tr}(YX).$$ Now if $X$ and $Y$ operate on a product space $H_A \times H_B  \times ...$, it is also possible to take the partial trace over any combination of those spaces.  But for various reasons, the cyclic theorem will not hold in that case. $${\rm Tr_A}(XY) \ne {\rm Tr_A}(YX)  \hspace{4em} \mbox{(except special cases).}$$ My general question is whether there is still some constraint on the commuted trace.  That is, does the trace-cyclic theorem generalise in some weakened form. My specific question involves lots of details: We are working in complex Hilbert space (quantum). $X =   \left|R\rangle\langle{E}\right|$ is rank-1 i.e. an outer product of arbitrarily entangled Hilbert-space vectors. $Y = U$ is unitary, but it freely entangles the subspaces. There can be more than two subspaces and it is given that $$    {\rm Tr_{\bar S}}(XU) = 0.  $$ Where $\bar{S}$ means ""all subspaces other than $S$"", and $S$ is arbitrary.  That is, $X$ is chosen such that when you trace over $XU$ and leave any one subspace alive, you always end up with zero.  So what I want to know is, whether: $$    {\rm Tr_{\bar S}}(UX) = 0.  $$ If this is true, I would like to know why.  If it isn't true, I would like to know if any related thing is true.",,"['linear-algebra', 'operator-algebras', 'trace']"
63,Normal operator + only real eigenvalues implies self-adjoint operator?,Normal operator + only real eigenvalues implies self-adjoint operator?,,"Let say we are in a complex vector space, is there an example of a normal operator with only real eigenvalues(or without eigenvalues) that is not a self-adjoint operator? Cause of the spectral theorem it is impossible for the finite dimensional case. I have no idea in the infinite case. I would appreciate any help! Thanks!","Let say we are in a complex vector space, is there an example of a normal operator with only real eigenvalues(or without eigenvalues) that is not a self-adjoint operator? Cause of the spectral theorem it is impossible for the finite dimensional case. I have no idea in the infinite case. I would appreciate any help! Thanks!",,"['linear-algebra', 'operator-theory', 'eigenvalues-eigenvectors']"
64,Prove that Anosov Automorphisms are chaotic,Prove that Anosov Automorphisms are chaotic,,"Let me add some detail first. An Anosov automorphism on $R^2$ is a mapping from the unit square $S$ onto $S$ of the form $\begin{bmatrix}x \\y\end{bmatrix}\rightarrow\begin{bmatrix}a && b \\c && d \end{bmatrix}\begin{bmatrix}x \\y\end{bmatrix}mod \hspace{1mm}1$ in which (i) $a, b, c, and \hspace{1mm} d$ are integers, (ii) the determinant of the matrix is $\pm$1, and (iii) the eigenvalues of the matrix do not have magnitude $1$. It is easy to show that Arnold's cat map is an Anosov automorphism, and that it is chaotic. To define ""chaotic"" in this context, A mapping $T$ of $S$ onto itself is said to be chaotic if: (i) $S$ contains a dense set of periodic points of the mapping $T$ (ii) There is a point in $S$ whose iterates under $T$ are dense in $S$. That said, it is said that all Anosov automorphisms are chaotic mappings. Based on the definition of chaotic, how can one prove that statement? Any feedback will be appreciated.","Let me add some detail first. An Anosov automorphism on $R^2$ is a mapping from the unit square $S$ onto $S$ of the form $\begin{bmatrix}x \\y\end{bmatrix}\rightarrow\begin{bmatrix}a && b \\c && d \end{bmatrix}\begin{bmatrix}x \\y\end{bmatrix}mod \hspace{1mm}1$ in which (i) $a, b, c, and \hspace{1mm} d$ are integers, (ii) the determinant of the matrix is $\pm$1, and (iii) the eigenvalues of the matrix do not have magnitude $1$. It is easy to show that Arnold's cat map is an Anosov automorphism, and that it is chaotic. To define ""chaotic"" in this context, A mapping $T$ of $S$ onto itself is said to be chaotic if: (i) $S$ contains a dense set of periodic points of the mapping $T$ (ii) There is a point in $S$ whose iterates under $T$ are dense in $S$. That said, it is said that all Anosov automorphisms are chaotic mappings. Based on the definition of chaotic, how can one prove that statement? Any feedback will be appreciated.",,"['linear-algebra', 'dynamical-systems', 'chaos-theory']"
65,On certain decomposition of unitary symmetric matrices,On certain decomposition of unitary symmetric matrices,,"It is well known that a symmetric matrix over field $\Bbb F$ is congruent to a diagonal matrix, i.e., there exists some A s.t. $A^TUA=D$ with $U$ symmetric and $D$ diagonal. If $\Bbb F=\Bbb C$ then we can make $D=I$. Recently I learned that if $U$ is unitary that we can do one step further by requiring $A$ to be unitary too. A similar result holds for unitary skew matrices. But I fail to figure out a proof myself. Can anyone provide a proof of this or at least help me to locate some references? Many thanks!","It is well known that a symmetric matrix over field $\Bbb F$ is congruent to a diagonal matrix, i.e., there exists some A s.t. $A^TUA=D$ with $U$ symmetric and $D$ diagonal. If $\Bbb F=\Bbb C$ then we can make $D=I$. Recently I learned that if $U$ is unitary that we can do one step further by requiring $A$ to be unitary too. A similar result holds for unitary skew matrices. But I fail to figure out a proof myself. Can anyone provide a proof of this or at least help me to locate some references? Many thanks!",,"['linear-algebra', 'reference-request']"
66,Do multiplicative maps of matrices factor through determinants?,Do multiplicative maps of matrices factor through determinants?,,"Given a map $f:M_n(k)\to k$ (with $k$ some field) such that $f(AB)=f(A)f(B)$ for all matrices $A$ and $B$, is it necessarily the case that $f$ factors through the determinant, i.e. does there exist a multiplicative map $g:k\to k$ such that $f=g\circ\det\,$? Are constraints on $k$ necessary? A simple corollary would be that nonzero multiplicative maps on subgroups of the general linear group $GL_n(k)$ factor through multiplicative maps on the units $k^\times\to k^\times$. Two definitions of $\det$ I'm aware of (written with our setting in mind): The unique alternating mulilinear map (of column vectors in a matrix) sending $I$ to $1_k$. The trace of the map induced by $A$ on the $n$th exterior power $\mathrm{Alt}^nk^n$. By Gaussian elemination, any multiplicative map $f$ from matrices to the base field is determined by its values on the matrices representing elementary row operations and upper triangular matrices. One stumbling block is that it seems hard, in general, to fully characterize the multiplicative maps on the base field $k$. With a finite field it would just be integer powers and the zero map, but with the reals you get all (positive) real powers too, and some funky stuff may occur with other fields.","Given a map $f:M_n(k)\to k$ (with $k$ some field) such that $f(AB)=f(A)f(B)$ for all matrices $A$ and $B$, is it necessarily the case that $f$ factors through the determinant, i.e. does there exist a multiplicative map $g:k\to k$ such that $f=g\circ\det\,$? Are constraints on $k$ necessary? A simple corollary would be that nonzero multiplicative maps on subgroups of the general linear group $GL_n(k)$ factor through multiplicative maps on the units $k^\times\to k^\times$. Two definitions of $\det$ I'm aware of (written with our setting in mind): The unique alternating mulilinear map (of column vectors in a matrix) sending $I$ to $1_k$. The trace of the map induced by $A$ on the $n$th exterior power $\mathrm{Alt}^nk^n$. By Gaussian elemination, any multiplicative map $f$ from matrices to the base field is determined by its values on the matrices representing elementary row operations and upper triangular matrices. One stumbling block is that it seems hard, in general, to fully characterize the multiplicative maps on the base field $k$. With a finite field it would just be integer powers and the zero map, but with the reals you get all (positive) real powers too, and some funky stuff may occur with other fields.",,"['linear-algebra', 'abstract-algebra', 'determinant']"
67,"Are a $2\times 2$ matrix in $\text{SL}(2,\mathbb{Z})$ and its transpose conjugate in $\text{GL}(2,\mathbb{Z})$?",Are a  matrix in  and its transpose conjugate in ?,"2\times 2 \text{SL}(2,\mathbb{Z}) \text{GL}(2,\mathbb{Z})","I've been studying some math by myself this summer, and have recently been doing some reading about the groups $\text{GL}(2,\mathbb{Z})$ , $\text{SL}(2,\mathbb{Z})$ , etc. I've been trying to get a better grasp of conjugation in these groups, but have been unable to figure out something relatively simple: Are the following two matrices conjugate in $\text{SL}(2,\mathbb{Z})$ ? If not maybe in $\text{GL}(2,\mathbb{Z})$ ? $$\begin{bmatrix} a & b\\ c & d \end{bmatrix},  \begin{bmatrix} a & c\\ b & d \end{bmatrix}\in\text{SL}(\mathbb{Z}[t,t^{-1}]) $$ If so is there a nice conjugating element? Thanks in advance! I don't have the greatest linear algebra background, but am loving group theory, which might be why I am not getting this.","I've been studying some math by myself this summer, and have recently been doing some reading about the groups , , etc. I've been trying to get a better grasp of conjugation in these groups, but have been unable to figure out something relatively simple: Are the following two matrices conjugate in ? If not maybe in ? If so is there a nice conjugating element? Thanks in advance! I don't have the greatest linear algebra background, but am loving group theory, which might be why I am not getting this.","\text{GL}(2,\mathbb{Z}) \text{SL}(2,\mathbb{Z}) \text{SL}(2,\mathbb{Z}) \text{GL}(2,\mathbb{Z}) \begin{bmatrix}
a & b\\
c & d
\end{bmatrix}, 
\begin{bmatrix}
a & c\\
b & d
\end{bmatrix}\in\text{SL}(\mathbb{Z}[t,t^{-1}])
","['linear-algebra', 'matrices', 'group-theory', 'general-linear-group']"
68,"Let $A,B\in M_2(\mathbb{C})$ such that $A^2+B^2=3AB$. Prove or disprove that $ \det(AB+BA)=\det(2AB). $",Let  such that . Prove or disprove that,"A,B\in M_2(\mathbb{C}) A^2+B^2=3AB  \det(AB+BA)=\det(2AB). ","Let $A,B\in M_2(\mathbb{C})$ such that $A^2+B^2=3AB$ . Prove or disprove that $$ \det(AB+BA)=\det(2AB). $$ I try to start with $\det(2AB)$ and write $$  \det(2AB)=4\det(AB)=\frac{4}{9}\det(3AB)=\frac{4}{9}\det(A^2+B^2). $$ After that it seems to me that something is missing. How can I continue from here? Thank you.",Let such that . Prove or disprove that I try to start with and write After that it seems to me that something is missing. How can I continue from here? Thank you.,"A,B\in M_2(\mathbb{C}) A^2+B^2=3AB 
\det(AB+BA)=\det(2AB).
 \det(2AB) 
 \det(2AB)=4\det(AB)=\frac{4}{9}\det(3AB)=\frac{4}{9}\det(A^2+B^2).
","['linear-algebra', 'matrices']"
69,Are there matrices such that $(AB-BA)^{71}=I_{69}$?,Are there matrices such that ?,(AB-BA)^{71}=I_{69},"Are there matrices $A,B \in \mathcal{M}_{69}(\mathbb{C})$ such that $$(AB-BA)^{71}=I_{69}?$$   Here $I_{69}$ denotes the $69 \times 69$ matrix with $1$ on its main diagonal and $0$ everywhere else. My strong guess is that there are not. Let $C=AB-BA$. Then $\text{tr }C=0$ and $\text{tr }C^{71}=69$. If $\lambda$ is an eigenvalue of $C$, then $\lambda^{71}=1$. So \begin{align*} \lambda_1+\lambda_2+\dots+\lambda_{69}=0 \\ \lambda_1^{71}+\lambda_2^{71}+\dots+\lambda_{69}^{71}=69 \end{align*} I think some contradiction may come from here, but I don't see it yet. I also tried working with polynomials. If $p=X^{71}-1$, then $p(C)=0$. If we worked in $\mathcal{M}_{69}(\mathbb{Q})$ it would have been easier because the minimal polynomial of $C$ divides $p$ and since $(X-1)$ is the only irreductible factor of $p$ with its degree less or equal than $69$, we would have that the minimal polynomial of $C$ is $(X-1)$ and so $C=I_{69}$, which leads to a contradiction after applying trace.","Are there matrices $A,B \in \mathcal{M}_{69}(\mathbb{C})$ such that $$(AB-BA)^{71}=I_{69}?$$   Here $I_{69}$ denotes the $69 \times 69$ matrix with $1$ on its main diagonal and $0$ everywhere else. My strong guess is that there are not. Let $C=AB-BA$. Then $\text{tr }C=0$ and $\text{tr }C^{71}=69$. If $\lambda$ is an eigenvalue of $C$, then $\lambda^{71}=1$. So \begin{align*} \lambda_1+\lambda_2+\dots+\lambda_{69}=0 \\ \lambda_1^{71}+\lambda_2^{71}+\dots+\lambda_{69}^{71}=69 \end{align*} I think some contradiction may come from here, but I don't see it yet. I also tried working with polynomials. If $p=X^{71}-1$, then $p(C)=0$. If we worked in $\mathcal{M}_{69}(\mathbb{Q})$ it would have been easier because the minimal polynomial of $C$ divides $p$ and since $(X-1)$ is the only irreductible factor of $p$ with its degree less or equal than $69$, we would have that the minimal polynomial of $C$ is $(X-1)$ and so $C=I_{69}$, which leads to a contradiction after applying trace.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'trace', 'minimal-polynomials']"
70,Determinant of matrix with binomial coefficients entries,Determinant of matrix with binomial coefficients entries,,"I try that find the way for calculate the determinant of the following matrix of size $n\times n$. The determinant is $\displaystyle\binom{k+n}{k}x^{n}$. I wait that you can help me. THE OMITTED ENTRIES ARE ZERO $$A=\left(   \begin{array}{ccccc}     \binom{k+1}{k}x & \binom{k+1}{k+1} &  &  &  \\\\     \binom{k+2}{k}x^2 & \binom{k+2}{k+1}x & \binom{k+2}{k+2} &  &  \\\\     \vdots & \vdots & \cdots &  &  \\\\     \binom{k+n-1}{k}x^{n-1} & \binom{k+n-1}{k+1}x^{n-2} & \binom{k+n-1}{k+2}x^{n-3} &\cdots  &\,\,\,\,\,\,\,\,\,\,\binom{k+n-1}{k+n-1}  \\\\     \binom{k+n}{k}x^{n} & \binom{k+n}{k+1}x^{n-1} & \binom{k+n}{k+2}x^{n-2} &\cdots  & \binom{k+n}{k+n-1}x \\   \end{array} \right)$$ In other words, the $\left(i, j\right)$-th entry of the matrix is $\dbinom{k+i}{k+j-1} x^{i-j+1}$ when $j \leq i+1$, and otherwise is $0$.","I try that find the way for calculate the determinant of the following matrix of size $n\times n$. The determinant is $\displaystyle\binom{k+n}{k}x^{n}$. I wait that you can help me. THE OMITTED ENTRIES ARE ZERO $$A=\left(   \begin{array}{ccccc}     \binom{k+1}{k}x & \binom{k+1}{k+1} &  &  &  \\\\     \binom{k+2}{k}x^2 & \binom{k+2}{k+1}x & \binom{k+2}{k+2} &  &  \\\\     \vdots & \vdots & \cdots &  &  \\\\     \binom{k+n-1}{k}x^{n-1} & \binom{k+n-1}{k+1}x^{n-2} & \binom{k+n-1}{k+2}x^{n-3} &\cdots  &\,\,\,\,\,\,\,\,\,\,\binom{k+n-1}{k+n-1}  \\\\     \binom{k+n}{k}x^{n} & \binom{k+n}{k+1}x^{n-1} & \binom{k+n}{k+2}x^{n-2} &\cdots  & \binom{k+n}{k+n-1}x \\   \end{array} \right)$$ In other words, the $\left(i, j\right)$-th entry of the matrix is $\dbinom{k+i}{k+j-1} x^{i-j+1}$ when $j \leq i+1$, and otherwise is $0$.",,"['linear-algebra', 'matrices', 'determinant', 'matrix-calculus', 'matrix-decomposition']"
71,Find a matrix with determinant equals to $\det{(A)}\det{(D)}-\det{(B)}\det{(C)}$,Find a matrix with determinant equals to,\det{(A)}\det{(D)}-\det{(B)}\det{(C)},"Assume I have 4 matrices $A,B,C,D\in\Bbb{R}^{n\times n}$. I want to build a matrix $E\in\Bbb{R}^{m\times m}$ such that: $$\det{(E)}=\det{(A)}\det{(D)}-\det{(B)}\det{(C)}$$ under the following assumptions: $m$ can be any number we want, but I prefer $2n$. $E$ should not contain the terms $\det{(A)},\det{(B)},\det{(C)},\det{(D)}$. that means that the matrix  $\begin{pmatrix} \det{(A)} & \det{(B)} \\ \det{(C)} & \det{(D)} \end{pmatrix} $ is not the case... There can't be any further assumptions on $A,B,C,D$ I've already checked the matrix  $\begin{pmatrix} A & B \\ C & D \end{pmatrix} $ but it's not that... Does anyone have an idea what $E$ can be?","Assume I have 4 matrices $A,B,C,D\in\Bbb{R}^{n\times n}$. I want to build a matrix $E\in\Bbb{R}^{m\times m}$ such that: $$\det{(E)}=\det{(A)}\det{(D)}-\det{(B)}\det{(C)}$$ under the following assumptions: $m$ can be any number we want, but I prefer $2n$. $E$ should not contain the terms $\det{(A)},\det{(B)},\det{(C)},\det{(D)}$. that means that the matrix  $\begin{pmatrix} \det{(A)} & \det{(B)} \\ \det{(C)} & \det{(D)} \end{pmatrix} $ is not the case... There can't be any further assumptions on $A,B,C,D$ I've already checked the matrix  $\begin{pmatrix} A & B \\ C & D \end{pmatrix} $ but it's not that... Does anyone have an idea what $E$ can be?",,"['linear-algebra', 'matrices', 'determinant']"
72,"Universal Enveloping Algebra of $\mathfrak{gl}(n,\Bbb R)$",Universal Enveloping Algebra of,"\mathfrak{gl}(n,\Bbb R)","I am just learning about universal enveloping algebras, and I am wondering about the following. Question: Is the universal enveloping algebras of $\mathfrak{gl}(n,\Bbb R)$ just $\mathfrak{gl}(n,\Bbb R)$ itself? It is an associative algebra with matrix multiplication. But I am not sure that it satisfies the universal property. Given an $\Bbb R$-algebra $A$ and a Lie algebra homomorphism $$\varphi:\mathfrak{gl}(n,\Bbb R)\to A$$ we must show that it is also an $\Bbb R$-algebra homomorphism. That is $$\varphi([X,Y])=\varphi(X)\varphi(Y)-\varphi(Y)\varphi(X),\quad\forall X,Y\quad\implies\quad \varphi(XY)=\varphi(X)\varphi(Y),\quad\forall X,Y.$$ I am not sure this is true.","I am just learning about universal enveloping algebras, and I am wondering about the following. Question: Is the universal enveloping algebras of $\mathfrak{gl}(n,\Bbb R)$ just $\mathfrak{gl}(n,\Bbb R)$ itself? It is an associative algebra with matrix multiplication. But I am not sure that it satisfies the universal property. Given an $\Bbb R$-algebra $A$ and a Lie algebra homomorphism $$\varphi:\mathfrak{gl}(n,\Bbb R)\to A$$ we must show that it is also an $\Bbb R$-algebra homomorphism. That is $$\varphi([X,Y])=\varphi(X)\varphi(Y)-\varphi(Y)\varphi(X),\quad\forall X,Y\quad\implies\quad \varphi(XY)=\varphi(X)\varphi(Y),\quad\forall X,Y.$$ I am not sure this is true.",,"['linear-algebra', 'abstract-algebra', 'lie-groups', 'lie-algebras']"
73,"If a real symmetric matrix has repeated eigenvalues, why does it still have n linearly independent eigenvectors?","If a real symmetric matrix has repeated eigenvalues, why does it still have n linearly independent eigenvectors?",,"I know that if $A=A^T$ is real, then the eigenvalues are real, and that eigenvectors corresponding to different eigenvalues are orthogonal. Is there an easy way to see what happens in the case of a repeated eigenvalue, or does this require some sophisticated arguments (that I guess will come later in my book)?","I know that if $A=A^T$ is real, then the eigenvalues are real, and that eigenvectors corresponding to different eigenvalues are orthogonal. Is there an easy way to see what happens in the case of a repeated eigenvalue, or does this require some sophisticated arguments (that I guess will come later in my book)?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
74,"Proving that $T$:$(x_1,...,x_n) \rightarrow (\frac {x_1+x_2}{2},\frac {x_2+x_3}{2},...,\frac {x_n+x_1}{2})$ leads to nonintegral components",Proving that : leads to nonintegral components,"T (x_1,...,x_n) \rightarrow (\frac {x_1+x_2}{2},\frac {x_2+x_3}{2},...,\frac {x_n+x_1}{2})","Start with $n$ paiwise different integers $x_1,x_2,...,x_n,(n>2)$ and repeat the following step: $T$:$(x_1,...,x_n) \rightarrow (\frac {x_1+x_2}{2},\frac {x_2+x_3}{2},...,\frac {x_n+x_1}{2})$ Show that $T,T^2,...$ finally leads to nonintegral components. I have solved this problem with the approach written below, but is there a more elegant elementary way to solve this problem using the fact that the sum of the components is invariant ? Denote $y_k =(z(k)_1,....,z(k)_n) = T^k(x)$ where $x=(x_1,...,x_n)$. Then $||y_{k+1}||^2=(\frac {z(k)_1+z(k)_2}{2})^2+(\frac {z(k)_2+z(k)_3}{2})^2+...+(\frac {z(k)_n+z(k)_1}{2})^2$ $=\frac {1} {2}(z(k)_1^2+....+z(k)_n^2)+\frac {1} {4}(2z(k)_1z(k)_2+2z(k)_2z(k)_3+...+2z(k)_nz(k)_1)$ $\le z(k)_1^2+...+z(k)_n^2$ ,Since $2xy\le x^2+y^2$ Thus $||y_{n+1}|| \le ||y_n||$. The equality holds if and only if $z(k)_i=z(k)_{i+1}$ for $i=1,2..,n-1$   and $z(k)_n=z(k)_{1} \rightarrow z(k)_1=z(k)_2=....=z(k)_n$ If the components remain integers then the sum of squares are strictly decreasing until all components are equal.Suppose $T^k(x)$ consists of equal components and $T^{k-1}(x)$ consists of components such that not all of them are equal. Then $$T^k(x)=(\frac {z(k-1)_1+z(k-1)_2} {2},...,\frac {z(k-1)_n+z(k-1)_1} {2})$$ with  $$\frac {z(k-1)_1+z(k-1)_2} {2}=\frac {z(k-1)_2+z(k-1)_3} {2}=...=\frac {z(k-1)_n+z(k-1)_1}{2}$$ $\rightarrow z(k-1)_1=z(k-1)_3=z(k-1)_5... $ , $z(k-1)_2=z(k-1)_4=z(k-1)_6...$ and $z(k-1)_n=z(k-1)_2$. If $n$ is odd it follows that $z(k-1)_1=z(k-1)_2=...=z(k-1)_n$ which is a contradiction. If $n$ is even we have that $T^{k-1}(x)=(r,s,r,s,....,r,s)$ $$\frac {z(k-2)_1+z(k-2)_2} {2}=\frac {z(k-2)_3+z(k-2)_4} {2}=...=\frac {z(k-2)_{n-1}+z(k-2)_n} {2}=r $$ and $$ \frac {z(k-2)_2+z(k-2)_3} {2}=\frac {z(k-2)_4+z(k-2)_5} {2}=...=\frac {z(k-2)_{n}+z(k-2)_1} {2}=s $$ It follows that $$ \frac {z(k-2)_1+z(k-2)_2} {2}+\frac {z(k-2)_3+z(k-2)_4} {2}+...+\frac {z(k-2)_{n-1}+z(k-2)_n} {2}=\frac {n} {2} r $$ and  $$ \frac {z(k-2)_2+z(k-2)_3} {2}+\frac {z(k-2)_4+z(k-2)_5} {2}+...+\frac {z(k-2)_{n}+z(k-2)_1} {2}= \frac {n} {2} s $$ $ \rightarrow r=s $.Thus we have arrived to a contradiction. It follows that $T,T^2,...$ finally leads to nonintegral values.","Start with $n$ paiwise different integers $x_1,x_2,...,x_n,(n>2)$ and repeat the following step: $T$:$(x_1,...,x_n) \rightarrow (\frac {x_1+x_2}{2},\frac {x_2+x_3}{2},...,\frac {x_n+x_1}{2})$ Show that $T,T^2,...$ finally leads to nonintegral components. I have solved this problem with the approach written below, but is there a more elegant elementary way to solve this problem using the fact that the sum of the components is invariant ? Denote $y_k =(z(k)_1,....,z(k)_n) = T^k(x)$ where $x=(x_1,...,x_n)$. Then $||y_{k+1}||^2=(\frac {z(k)_1+z(k)_2}{2})^2+(\frac {z(k)_2+z(k)_3}{2})^2+...+(\frac {z(k)_n+z(k)_1}{2})^2$ $=\frac {1} {2}(z(k)_1^2+....+z(k)_n^2)+\frac {1} {4}(2z(k)_1z(k)_2+2z(k)_2z(k)_3+...+2z(k)_nz(k)_1)$ $\le z(k)_1^2+...+z(k)_n^2$ ,Since $2xy\le x^2+y^2$ Thus $||y_{n+1}|| \le ||y_n||$. The equality holds if and only if $z(k)_i=z(k)_{i+1}$ for $i=1,2..,n-1$   and $z(k)_n=z(k)_{1} \rightarrow z(k)_1=z(k)_2=....=z(k)_n$ If the components remain integers then the sum of squares are strictly decreasing until all components are equal.Suppose $T^k(x)$ consists of equal components and $T^{k-1}(x)$ consists of components such that not all of them are equal. Then $$T^k(x)=(\frac {z(k-1)_1+z(k-1)_2} {2},...,\frac {z(k-1)_n+z(k-1)_1} {2})$$ with  $$\frac {z(k-1)_1+z(k-1)_2} {2}=\frac {z(k-1)_2+z(k-1)_3} {2}=...=\frac {z(k-1)_n+z(k-1)_1}{2}$$ $\rightarrow z(k-1)_1=z(k-1)_3=z(k-1)_5... $ , $z(k-1)_2=z(k-1)_4=z(k-1)_6...$ and $z(k-1)_n=z(k-1)_2$. If $n$ is odd it follows that $z(k-1)_1=z(k-1)_2=...=z(k-1)_n$ which is a contradiction. If $n$ is even we have that $T^{k-1}(x)=(r,s,r,s,....,r,s)$ $$\frac {z(k-2)_1+z(k-2)_2} {2}=\frac {z(k-2)_3+z(k-2)_4} {2}=...=\frac {z(k-2)_{n-1}+z(k-2)_n} {2}=r $$ and $$ \frac {z(k-2)_2+z(k-2)_3} {2}=\frac {z(k-2)_4+z(k-2)_5} {2}=...=\frac {z(k-2)_{n}+z(k-2)_1} {2}=s $$ It follows that $$ \frac {z(k-2)_1+z(k-2)_2} {2}+\frac {z(k-2)_3+z(k-2)_4} {2}+...+\frac {z(k-2)_{n-1}+z(k-2)_n} {2}=\frac {n} {2} r $$ and  $$ \frac {z(k-2)_2+z(k-2)_3} {2}+\frac {z(k-2)_4+z(k-2)_5} {2}+...+\frac {z(k-2)_{n}+z(k-2)_1} {2}= \frac {n} {2} s $$ $ \rightarrow r=s $.Thus we have arrived to a contradiction. It follows that $T,T^2,...$ finally leads to nonintegral values.",,"['linear-algebra', 'number-theory', 'algorithms', 'problem-solving', 'integer-lattices']"
75,covariant and contravariant components and change of basis,covariant and contravariant components and change of basis,,"I encountered the following in reading about covariant and contravariant: In those discussions, you may see words to the effect that covariant    components transform in the same way as basis vectors (co  with),   and contravariant components transform in the opposite way to basis    vectors (contra  against). As youll see later in this chapter,   theres plenty of truth in that description, but theres also a major   pitfall. Thats because the transformation of basis vectors usually   refers to the conversion of the basis vectors in the original   (non-rotated) coordinate system to the different basis vectors which   point along the coordinate axes in the new (rotated) system, whereas   the transformation of vector components refers to the change in the   components of the same vector referred to two different sets of   coordinate axes. Later on it shows the following: $$\begin{pmatrix} \text{Components of}  \\                   \text{same vector} \\                   \text{in new system} \end{pmatrix} =  \begin{pmatrix} \text{Inverse}  \\                   \text{transformation} \\                   \text{matrix} \end{pmatrix} \begin{pmatrix} \text{Components of}  \\                   \text{vector in} \\                   \text{original system} \end{pmatrix}$$ $$\begin{pmatrix} \text{New basis}  \\                   \text{vectors} \end{pmatrix} =  \begin{pmatrix} \text{Direct}  \\                   \text{transformation} \\                   \text{matrix} \end{pmatrix} \begin{pmatrix} \text{Original basis}  \\                   \text{vectors}\end{pmatrix}$$ These confuse me because from change of basis we have $B'= BP$ and $[v]_{B'}=P[v]_B$. The $[v]_{B'}=P[v]_B$ is the first of the aforementioned equations. But the second of them I don't understand since the book as shown above has the direct transformation matrix on the left side and not the right side. We could write $B'= BP = (BPB^{-1})B$ but if $BPB^{-1}$ is the direct transformation matrix then the $P$ in $[v]_{B'}=P[v]_B$ does not make sense as the inverse transformation matrix as the inverse of $BPB^{-1}$ is not $P$. So then later on when I read the following: you can combine superscripted (contravariant) components with   subscripted (covariant) basis vectors I don't know how to reconcile it with what I already know about change of basis.","I encountered the following in reading about covariant and contravariant: In those discussions, you may see words to the effect that covariant    components transform in the same way as basis vectors (co  with),   and contravariant components transform in the opposite way to basis    vectors (contra  against). As youll see later in this chapter,   theres plenty of truth in that description, but theres also a major   pitfall. Thats because the transformation of basis vectors usually   refers to the conversion of the basis vectors in the original   (non-rotated) coordinate system to the different basis vectors which   point along the coordinate axes in the new (rotated) system, whereas   the transformation of vector components refers to the change in the   components of the same vector referred to two different sets of   coordinate axes. Later on it shows the following: $$\begin{pmatrix} \text{Components of}  \\                   \text{same vector} \\                   \text{in new system} \end{pmatrix} =  \begin{pmatrix} \text{Inverse}  \\                   \text{transformation} \\                   \text{matrix} \end{pmatrix} \begin{pmatrix} \text{Components of}  \\                   \text{vector in} \\                   \text{original system} \end{pmatrix}$$ $$\begin{pmatrix} \text{New basis}  \\                   \text{vectors} \end{pmatrix} =  \begin{pmatrix} \text{Direct}  \\                   \text{transformation} \\                   \text{matrix} \end{pmatrix} \begin{pmatrix} \text{Original basis}  \\                   \text{vectors}\end{pmatrix}$$ These confuse me because from change of basis we have $B'= BP$ and $[v]_{B'}=P[v]_B$. The $[v]_{B'}=P[v]_B$ is the first of the aforementioned equations. But the second of them I don't understand since the book as shown above has the direct transformation matrix on the left side and not the right side. We could write $B'= BP = (BPB^{-1})B$ but if $BPB^{-1}$ is the direct transformation matrix then the $P$ in $[v]_{B'}=P[v]_B$ does not make sense as the inverse transformation matrix as the inverse of $BPB^{-1}$ is not $P$. So then later on when I read the following: you can combine superscripted (contravariant) components with   subscripted (covariant) basis vectors I don't know how to reconcile it with what I already know about change of basis.",,"['linear-algebra', 'vectors', 'tensor-products', 'multilinear-algebra']"
76,"Decompose invertible matrix $A$ as $A = LPU$. (Artin, Chapter 2, Exercise M.11)","Decompose invertible matrix  as . (Artin, Chapter 2, Exercise M.11)",A A = LPU,"Decompose matrix $A$ as $A = LPU$ , where $A \in Gl_n( \mathbb{R}^n)$ , $L$ is lower triangular, $U$ is upper triangular with diagonal elements of $1$ , and $P$ is a permutation matrix. It is fairly easy to decompose any invertible such $A$ as $PA = LU$ (1) (more or less Gaussian elimination with pivoting). But what is a constructive proof to get the above decomposition? The usual way I prove (1) is applying elementary matrices to the left of the pivoted $PA$ , and this would (would it?) change $PA$ , not only $A$ (if it only changed $A$ , we could commute $P$ ). This is an exercise in Artin, 2nd edition, 2nd chapter, M* exercises. Thanks.","Decompose matrix as , where , is lower triangular, is upper triangular with diagonal elements of , and is a permutation matrix. It is fairly easy to decompose any invertible such as (1) (more or less Gaussian elimination with pivoting). But what is a constructive proof to get the above decomposition? The usual way I prove (1) is applying elementary matrices to the left of the pivoted , and this would (would it?) change , not only (if it only changed , we could commute ). This is an exercise in Artin, 2nd edition, 2nd chapter, M* exercises. Thanks.",A A = LPU A \in Gl_n( \mathbb{R}^n) L U 1 P A PA = LU PA PA A A P,"['linear-algebra', 'matrices', 'numerical-linear-algebra', 'gaussian-elimination', 'lu-decomposition']"
77,Prove Farkas Lemma using the Fourier-Motzkin elimination algorithm,Prove Farkas Lemma using the Fourier-Motzkin elimination algorithm,,"I am trying to prove the Farkas Lemma using the Fourier-Motzkin elimination algorithm. From Wikipedia: Let A be an $m \times n$ matrix and $b$ an $m$-dimensional vector. Then, exactly one of the following two statements is true: There exists an $x \in \Bbb R^n$ such that $Ax = b$ and $x \ge 0$. There exists a $y \in \Bbb R^m$ such that $A^Ty \ge 0$ and $b^Ty < 0$. The first direction is quite easy. I assume that there is a vector $y$ and I found a contradiction. To the other direction I have used the fourier-motzkin elimination to reduce the number of variables. I assume that $Ax \le b$ and I do one step from the algorithm. I create a new system $A'x' \le b'$. I know that there exist a non-negative matrix $M$ that is a linear combination of the new system to the original. I have followed the direction of repeating the algorithm $n$ times to eliminate all the variables and create the system: $0 \le b''$. Now in order this system to be infeasible it must be that: $b''<0$. So I can assume that exist vector $y''$ such that $y''A''=0$ and also $y''b''<0$ because $b'<0$ and $y\ge 0$. Now I can prove that also there is a vector $y$ for the original system. But the repetition of $n$ steps seems to me a bit arbitrary. If I just do one step and create the system $A'x'\le b'$ how I can use it is infeasible?","I am trying to prove the Farkas Lemma using the Fourier-Motzkin elimination algorithm. From Wikipedia: Let A be an $m \times n$ matrix and $b$ an $m$-dimensional vector. Then, exactly one of the following two statements is true: There exists an $x \in \Bbb R^n$ such that $Ax = b$ and $x \ge 0$. There exists a $y \in \Bbb R^m$ such that $A^Ty \ge 0$ and $b^Ty < 0$. The first direction is quite easy. I assume that there is a vector $y$ and I found a contradiction. To the other direction I have used the fourier-motzkin elimination to reduce the number of variables. I assume that $Ax \le b$ and I do one step from the algorithm. I create a new system $A'x' \le b'$. I know that there exist a non-negative matrix $M$ that is a linear combination of the new system to the original. I have followed the direction of repeating the algorithm $n$ times to eliminate all the variables and create the system: $0 \le b''$. Now in order this system to be infeasible it must be that: $b''<0$. So I can assume that exist vector $y''$ such that $y''A''=0$ and also $y''b''<0$ because $b'<0$ and $y\ge 0$. Now I can prove that also there is a vector $y$ for the original system. But the repetition of $n$ steps seems to me a bit arbitrary. If I just do one step and create the system $A'x'\le b'$ how I can use it is infeasible?",,"['linear-algebra', 'inequality', 'convex-analysis', 'linear-programming', 'fourier-motzkin']"
78,How do you find all solutions to the matrix equation $XAX=A^T$?,How do you find all solutions to the matrix equation ?,XAX=A^T,"I was recently asked to solve a problem in a programming interview involving word squares , and on further reflection I realized it could be recast as a linear algebra question. Since my solution has a worst-case time complexity of $O(n!)$ if $n$ is the width of $A$, out of personal curiosity I'm trying to find a better solution, as measured by worst-case time complexity. Given an $n$ by $n$ square matrix $A$ over the reals with $n\geq 2$, is there an $n$ by $n$ permutation matrix $X$ satisfying $$XA=(XA)^T?$$ Since $(XA)^T=A^TX^T$ and $X$ is a permutation matrix, we have $X^T=X^{-1}$, hence this is equivalent to asking if there are any solutions to $$XAX=A^T$$ which are permutation matrices. Naively, I was hoping there would be a unique solution, but numeric techniques strongly indicate a wide variety of solutions, even when a satisfactory permutation exists. Unfortunately, it doesn't seem that solutions form a linear subspace. For example, if $X$ is a solution, then $(aX)A(aX)=a^2A^T$ for any real $a$, so most multiples of $X$ are not solutions for most matrices $A$. Similar problems exist for sums of solutions. With this in mind, I'm not quite sure which method one might use to enumerate solutions. If they don't form a linear subspace, we can't just enumerate a basis. Any partial solutions, theorems, pointers in the right direction, or related problems would be helpful. I'm not familiar with what seems to be a nonlinear matrix equation (if that's the right terminology). Update This seems to be a special case of something called the algebraic Riccati equation , as indicated in another question . The techniques I have found so far emphasize one of two approaches: Algorithms for finding any solution from a given starting guess. Algorithms for finding the unique stabilizing solution, should one exist. Given the control theoretic background for those approaches, they typically assume that $A^T$ is symmetric. In my case, if $A^T$ were symmetric I could simply take $X$ to be the identity and be done, and that assumption oversimplifies the problem. Additionally, those solutions do not seem to have a clean way to enumerate all solutions, inhibiting one from checking if any happen to be permutation matrices. Does anyone have more tips from a Riccati equation standpoint? I would like to add that strictly speaking one can answer the question in finite time by simply enumerating all permutation matrices of the appropriate size and checking if they satisfy $XA=(XA)^T$. For some reason I'm having trouble describing, I don't think this captures the spirit of the question. Update @GCab points out that $A$ would need a very peculiar structure for $X$ to be a permutation matrix. This is indeed the case, and $A$ would need to be symmetric on some row or column permutation. This does yield some insights. If $X$ is any permutation matrix, then $$(XAX)_{ij}=A_{ji}=(A^T)_{ij}$$ for any $i$, $j$ so that $X_{ij}=1$, and this does not hold when $X_{ij}=0$. In other words, the elements of $A$ come in pairs except for possibly the main diagonal of $XA$. Using this, one can rule out many possible permutations, since any element appearing only once in $A^T$ must give a location of a $1$ in $X$. There are many other such combinatorial tricks that can greatly improve the average runtime. One can similarly note that for a permutation $X$ to exist as a solution, there must exist a bijection $f$ between the rows and columns of $A$ so that if $r$ is a row of $A$ then the count of each element of $r$ is the same as its count in $f(r)$. Such a heuristic has the potential to easily classify a choice of $A$ as not having a satisfactory solution, but it could still take an enormous amount of time to confirm that a given $A$ does have a satisfactory $X$.","I was recently asked to solve a problem in a programming interview involving word squares , and on further reflection I realized it could be recast as a linear algebra question. Since my solution has a worst-case time complexity of $O(n!)$ if $n$ is the width of $A$, out of personal curiosity I'm trying to find a better solution, as measured by worst-case time complexity. Given an $n$ by $n$ square matrix $A$ over the reals with $n\geq 2$, is there an $n$ by $n$ permutation matrix $X$ satisfying $$XA=(XA)^T?$$ Since $(XA)^T=A^TX^T$ and $X$ is a permutation matrix, we have $X^T=X^{-1}$, hence this is equivalent to asking if there are any solutions to $$XAX=A^T$$ which are permutation matrices. Naively, I was hoping there would be a unique solution, but numeric techniques strongly indicate a wide variety of solutions, even when a satisfactory permutation exists. Unfortunately, it doesn't seem that solutions form a linear subspace. For example, if $X$ is a solution, then $(aX)A(aX)=a^2A^T$ for any real $a$, so most multiples of $X$ are not solutions for most matrices $A$. Similar problems exist for sums of solutions. With this in mind, I'm not quite sure which method one might use to enumerate solutions. If they don't form a linear subspace, we can't just enumerate a basis. Any partial solutions, theorems, pointers in the right direction, or related problems would be helpful. I'm not familiar with what seems to be a nonlinear matrix equation (if that's the right terminology). Update This seems to be a special case of something called the algebraic Riccati equation , as indicated in another question . The techniques I have found so far emphasize one of two approaches: Algorithms for finding any solution from a given starting guess. Algorithms for finding the unique stabilizing solution, should one exist. Given the control theoretic background for those approaches, they typically assume that $A^T$ is symmetric. In my case, if $A^T$ were symmetric I could simply take $X$ to be the identity and be done, and that assumption oversimplifies the problem. Additionally, those solutions do not seem to have a clean way to enumerate all solutions, inhibiting one from checking if any happen to be permutation matrices. Does anyone have more tips from a Riccati equation standpoint? I would like to add that strictly speaking one can answer the question in finite time by simply enumerating all permutation matrices of the appropriate size and checking if they satisfy $XA=(XA)^T$. For some reason I'm having trouble describing, I don't think this captures the spirit of the question. Update @GCab points out that $A$ would need a very peculiar structure for $X$ to be a permutation matrix. This is indeed the case, and $A$ would need to be symmetric on some row or column permutation. This does yield some insights. If $X$ is any permutation matrix, then $$(XAX)_{ij}=A_{ji}=(A^T)_{ij}$$ for any $i$, $j$ so that $X_{ij}=1$, and this does not hold when $X_{ij}=0$. In other words, the elements of $A$ come in pairs except for possibly the main diagonal of $XA$. Using this, one can rule out many possible permutations, since any element appearing only once in $A^T$ must give a location of a $1$ in $X$. There are many other such combinatorial tricks that can greatly improve the average runtime. One can similarly note that for a permutation $X$ to exist as a solution, there must exist a bijection $f$ between the rows and columns of $A$ so that if $r$ is a row of $A$ then the count of each element of $r$ is the same as its count in $f(r)$. Such a heuristic has the potential to easily classify a choice of $A$ as not having a satisfactory solution, but it could still take an enormous amount of time to confirm that a given $A$ does have a satisfactory $X$.",,"['linear-algebra', 'matrices', 'systems-of-equations', 'matrix-equations', 'permutation-matrices']"
79,Is the function T $\mathbb R$-linear?,Is the function T -linear?,\mathbb R,"Let $T:\mathbb R^2\to \mathbb R^2$ be a mapping such that $T(C)$ is a convex set in $\mathbb R^2$ whenever $C$ is convex set in $\mathbb R^2$ and $T(0,0)=(0,0).$ Is $T$ $\mathbb R$-linear? We have to show here that $T(ax+by)=aT(x)+bT(y)$ for all $a,b\in \mathbb R$ and $x,y\in \mathbb R^2.$ Any help is appreciated. Thank you.","Let $T:\mathbb R^2\to \mathbb R^2$ be a mapping such that $T(C)$ is a convex set in $\mathbb R^2$ whenever $C$ is convex set in $\mathbb R^2$ and $T(0,0)=(0,0).$ Is $T$ $\mathbb R$-linear? We have to show here that $T(ax+by)=aT(x)+bT(y)$ for all $a,b\in \mathbb R$ and $x,y\in \mathbb R^2.$ Any help is appreciated. Thank you.",,['linear-algebra']
80,The number of esquares of idempotents in the rank 2 $\mathcal{D}$-class of $M_n(\mathbb{Z}_2)$.,The number of esquares of idempotents in the rank 2 -class of .,\mathcal{D} M_n(\mathbb{Z}_2),"Here's a variation of a question I was given during a research internship. Some Definitions: Definition 1: Let $S$ be a semigroup. For any $a, b\in S$, define Green's $\mathcal{L}$-relation by $a\mathcal{L}b$ if and only if $S^1a=S^1b$ and define Green's $\mathcal{R}$-relation by $a\mathcal{R}b$ if and only if $aS^1=bS^1$, where $S^1$ is $S$ with a one adjoined if necessary. Then Green's $\mathcal{D}$-relation is given by $a\mathcal{D}b$ if and only if $a(\mathcal{L}\circ\mathcal{R})b$ (which is equivalent to $a(\mathcal{R}\circ\mathcal{L})b$); that is, there exists a $c\in S$ such that $a\mathcal{L}c\mathcal{R}b$ Definition 2: The rank of a matrix is the number of linearly independent columns it has. The Question: Let $S=M_n(\mathbb{Z}_2)$. Let $D^{(n)}_2$ be the matrices in the rank $2$ $\mathcal{D}$-class of $S$. Find $$N_n=\left\lvert\left\{\left.\begin{array} \, e & \mathcal{L} & f \\ \mathcal{R} & \, & \mathcal{R} \\ h & \mathcal{L} & g \end{array}\right\vert e, f, g, h\in E\left(D^{(n)}_2\right)\right\}\right\rvert;$$ that is, find $N_n$,  the number of quadruples $(e, f, g,h)\in E\left(D_2^{(n)}\right)^4$ such that $e\mathcal{L}f\mathcal{R}g\mathcal{L}h\mathcal{R}e$. Here $E(T)$ is the set of idempotents of the semigroup $T$. Background: I did most of the cases when $n=4$ and $n=6$ using the programming language GAP . For the $6\times 6$ case, I sorted matrices of the $D^{(6)}_2$ into certain types (that I can't produce from memory as my notes are missing) then had GAP do an iterated procedure to find $N_6$.","Here's a variation of a question I was given during a research internship. Some Definitions: Definition 1: Let $S$ be a semigroup. For any $a, b\in S$, define Green's $\mathcal{L}$-relation by $a\mathcal{L}b$ if and only if $S^1a=S^1b$ and define Green's $\mathcal{R}$-relation by $a\mathcal{R}b$ if and only if $aS^1=bS^1$, where $S^1$ is $S$ with a one adjoined if necessary. Then Green's $\mathcal{D}$-relation is given by $a\mathcal{D}b$ if and only if $a(\mathcal{L}\circ\mathcal{R})b$ (which is equivalent to $a(\mathcal{R}\circ\mathcal{L})b$); that is, there exists a $c\in S$ such that $a\mathcal{L}c\mathcal{R}b$ Definition 2: The rank of a matrix is the number of linearly independent columns it has. The Question: Let $S=M_n(\mathbb{Z}_2)$. Let $D^{(n)}_2$ be the matrices in the rank $2$ $\mathcal{D}$-class of $S$. Find $$N_n=\left\lvert\left\{\left.\begin{array} \, e & \mathcal{L} & f \\ \mathcal{R} & \, & \mathcal{R} \\ h & \mathcal{L} & g \end{array}\right\vert e, f, g, h\in E\left(D^{(n)}_2\right)\right\}\right\rvert;$$ that is, find $N_n$,  the number of quadruples $(e, f, g,h)\in E\left(D_2^{(n)}\right)^4$ such that $e\mathcal{L}f\mathcal{R}g\mathcal{L}h\mathcal{R}e$. Here $E(T)$ is the set of idempotents of the semigroup $T$. Background: I did most of the cases when $n=4$ and $n=6$ using the programming language GAP . For the $6\times 6$ case, I sorted matrices of the $D^{(6)}_2$ into certain types (that I can't produce from memory as my notes are missing) then had GAP do an iterated procedure to find $N_6$.",,"['linear-algebra', 'abstract-algebra', 'combinatorics', 'semigroups', 'idempotents']"
81,"Why vectors in Linear Algebra start from point (0,0)?","Why vectors in Linear Algebra start from point (0,0)?",,"I learn linear algebra in university and I was wondering why vectors in linear algebra always start from the point $(0,0)$? how many kinds of mathematical vectors out there? Is it legit to use other kind of vectors in linear algebra apart from that vectors that start at $(0,0)$?","I learn linear algebra in university and I was wondering why vectors in linear algebra always start from the point $(0,0)$? how many kinds of mathematical vectors out there? Is it legit to use other kind of vectors in linear algebra apart from that vectors that start at $(0,0)$?",,['linear-algebra']
82,$\det(I+A)$= sum of all principal minors of $A$,= sum of all principal minors of,\det(I+A) A,"I'm having a hard time proving or finding a proof for the following result. It should follow from an application of the Laplace expansion. Let $n\in\mathbb{N}$, $[n]=\{1,\dots,n\}$, and $A\in\mathbb{R}^{n\times n}$. Then,   $$\det\left(  I_{n}+A\right)  =\sum\limits_{G\subseteq\left[  n\right]  } \det\left(  A_{G}\right).$$   where $A_{G}$ is the matrix $A$ with all columns and rows not in $G$ removed. For instance, here is the proof for $n=3$: $\det\left(  I_{n}+A\right)  =\det\left( \begin{array} [c]{ccc} 1+a_{1,1} & a_{1,2} & a_{1,3}\\ a_{2,1} & 1+a_{2,2} & a_{2,3}\\ a_{3,1} & a_{3,2} & 1+a_{3,3} \end{array} \right)  $ $\;\;\;\;=\left(  1+a_{1,1}\right)  \left(  1+a_{2,2}\right)  \left(  1+a_{3,3} \right)  +a_{1,2}a_{2,3}a_{3,1}+a_{1,3}a_{2,1}a_{3,2}$ $\;\;\;\;\;\;\;\; -\left(  1+a_{1,1}\right)  a_{2,3}a_{3,2}-a_{1,2}a_{2,1}\left( 1+a_{3,3}\right)  -a_{1,3}\left(  1+a_{2,2}\right)  a_{3,1}$ $\;\;\;\;=a_{1,1}a_{2,2}-a_{1,2}a_{2,1}+a_{1,1}a_{3,3}-a_{1,3}a_{3,1}+a_{2,2}a_{3,3}$ $\;\;\;\;\;\;\;\; -a_{2,3}a_{3,2}+a_{1,1}a_{2,2}a_{3,3}-a_{1,1}a_{2,3}a_{3,2}-a_{1,2} a_{2,1}a_{3,3}$ $\;\;\;\;\;\;\;\; +a_{1,2}a_{3,1}a_{2,3}+a_{2,1}a_{1,3}a_{3,2}-a_{1,3}a_{2,2}a_{3,1} +a_{1,1}+a_{2,2}+a_{3,3}+1$ $\;\;\;\;=\underbrace{a_{1,1}a_{2,2}a_{3,3}+a_{1,2}a_{2,3}a_{3,1}+a_{1,3} a_{2,1}a_{3,2}-a_{1,1}a_{2,3}a_{3,2}-a_{1,2}a_{2,1}a_{3,3}-a_{1,3} a_{2,2}a_{3,1}}_{=\det A=\det\left(  A_{\left\{  1,2,3\right\} }\right)  }$ $\;\;\;\;\;\;\;\;+\underbrace{a_{1,1}a_{2,2}-a_{1,2}a_{2,1}}_{=\det\left(  A_{\left\{ 1,2\right\}  }\right)  }+\underbrace{a_{1,1} a_{3,3}-a_{1,3}a_{3,1}}_{=\det\left(  A_{\left\{  1,3\right\} }\right)  }+\underbrace{a_{2,2}a_{3,3}-a_{2,3}a_{3,2}}_{=\det\left(  A_{\left\{ 2,3\right\}    }\right)  }$ $\;\;\;\;\;\;\;\;+\underbrace{a_{1,1}} _{=\det\left(  A_{\left\{  1\right\}  }\right)  }+\underbrace{a_{2,2}}_{=\det\left(  A_{\left\{  2\right\} }\right)  }+\underbrace{a_{3,3}}_{=\det\left(  A_{\left\{ 3\right\}   }\right)  }+\underbrace{1}_{=\det\left( A_{\varnothing}\right)  }$ $\;\;\;\;=\sum\limits_{G\subseteq\left[  n\right]  }\det\left(  A_{G}\right)  $.","I'm having a hard time proving or finding a proof for the following result. It should follow from an application of the Laplace expansion. Let $n\in\mathbb{N}$, $[n]=\{1,\dots,n\}$, and $A\in\mathbb{R}^{n\times n}$. Then,   $$\det\left(  I_{n}+A\right)  =\sum\limits_{G\subseteq\left[  n\right]  } \det\left(  A_{G}\right).$$   where $A_{G}$ is the matrix $A$ with all columns and rows not in $G$ removed. For instance, here is the proof for $n=3$: $\det\left(  I_{n}+A\right)  =\det\left( \begin{array} [c]{ccc} 1+a_{1,1} & a_{1,2} & a_{1,3}\\ a_{2,1} & 1+a_{2,2} & a_{2,3}\\ a_{3,1} & a_{3,2} & 1+a_{3,3} \end{array} \right)  $ $\;\;\;\;=\left(  1+a_{1,1}\right)  \left(  1+a_{2,2}\right)  \left(  1+a_{3,3} \right)  +a_{1,2}a_{2,3}a_{3,1}+a_{1,3}a_{2,1}a_{3,2}$ $\;\;\;\;\;\;\;\; -\left(  1+a_{1,1}\right)  a_{2,3}a_{3,2}-a_{1,2}a_{2,1}\left( 1+a_{3,3}\right)  -a_{1,3}\left(  1+a_{2,2}\right)  a_{3,1}$ $\;\;\;\;=a_{1,1}a_{2,2}-a_{1,2}a_{2,1}+a_{1,1}a_{3,3}-a_{1,3}a_{3,1}+a_{2,2}a_{3,3}$ $\;\;\;\;\;\;\;\; -a_{2,3}a_{3,2}+a_{1,1}a_{2,2}a_{3,3}-a_{1,1}a_{2,3}a_{3,2}-a_{1,2} a_{2,1}a_{3,3}$ $\;\;\;\;\;\;\;\; +a_{1,2}a_{3,1}a_{2,3}+a_{2,1}a_{1,3}a_{3,2}-a_{1,3}a_{2,2}a_{3,1} +a_{1,1}+a_{2,2}+a_{3,3}+1$ $\;\;\;\;=\underbrace{a_{1,1}a_{2,2}a_{3,3}+a_{1,2}a_{2,3}a_{3,1}+a_{1,3} a_{2,1}a_{3,2}-a_{1,1}a_{2,3}a_{3,2}-a_{1,2}a_{2,1}a_{3,3}-a_{1,3} a_{2,2}a_{3,1}}_{=\det A=\det\left(  A_{\left\{  1,2,3\right\} }\right)  }$ $\;\;\;\;\;\;\;\;+\underbrace{a_{1,1}a_{2,2}-a_{1,2}a_{2,1}}_{=\det\left(  A_{\left\{ 1,2\right\}  }\right)  }+\underbrace{a_{1,1} a_{3,3}-a_{1,3}a_{3,1}}_{=\det\left(  A_{\left\{  1,3\right\} }\right)  }+\underbrace{a_{2,2}a_{3,3}-a_{2,3}a_{3,2}}_{=\det\left(  A_{\left\{ 2,3\right\}    }\right)  }$ $\;\;\;\;\;\;\;\;+\underbrace{a_{1,1}} _{=\det\left(  A_{\left\{  1\right\}  }\right)  }+\underbrace{a_{2,2}}_{=\det\left(  A_{\left\{  2\right\} }\right)  }+\underbrace{a_{3,3}}_{=\det\left(  A_{\left\{ 3\right\}   }\right)  }+\underbrace{1}_{=\det\left( A_{\varnothing}\right)  }$ $\;\;\;\;=\sum\limits_{G\subseteq\left[  n\right]  }\det\left(  A_{G}\right)  $.",,"['linear-algebra', 'matrices', 'determinant']"
83,Eigenvalues of $MA$ versus eigenvalues of $A$ for orthogonal projection $M$,Eigenvalues of  versus eigenvalues of  for orthogonal projection,MA A M,"Suppose that $M$ is symmetric idempotent $n\times n$ and has rank $n-k$. Suppose that $A$ is $n\times n$ and positive definite. Let $0<\nu_1\leq\nu_2\leq\ldots\nu_{n-k}$ be the nonzero eigenvalues of $MA$ and $0<\lambda_1\leq\lambda_2\leq\cdots\leq\lambda_n$ be the eigenvalues of $A$. I'm trying to show that   $$ \forall i=1,\ldots,n-k:\quad 0<\lambda_i\leq\nu_i\leq\lambda_{i+k}\tag{$*$} $$ There will be a 300 bounty for the accepted answer. Can someone also please make all the (attempted) proofs below as spoilers? I can only do that for the first proof. Attempt : I have an attempt here using Durbin and Watson (1950) but I don't fully understand the authors' argument so the attempt is incomplete. Nonetheless, I'll present the attempt here. Step 3 below is where I am stuck. Step 1 : One can write $M$ as $M_kM_{k-1}\cdots M_1$ where $M_i=I_n-p_ip_i'$ and $\{p_1,\ldots,p_k\}$ is a set of $n\times 1$ mutually orthogonal vectors s.t. $||p_i||=1$. Proof. $M$, by assumption, can be written as $M=I_n-X(X'X)^{-1}X'$ where $X$ is $n\times k$ with full column rank. Let $P=(p_1,\ldots,p_k)$ (dimension $n\times k$) be the $Q$ bit of the QR decomposition of $X$. Step 2 : Let $T=(T_1,\ldots,T_n)$ be an $n\times n$ matrix of orthonormal eigenvectors of $A$ that corresponds to eigenvalues $\lambda_1,\ldots,\lambda_n$. Let $l_{1i}=T_i'p_1$. Then any nonzero eigenvalue $\theta$ of $M_1A$ satisfies $$ \sum_{i=1}^nl_{1i}^2\prod_{j\neq i}(\theta-\lambda_j)=0.\tag{$**$} $$ Proof. For any eigenvalue (possibly $0$) $\theta$ of $M_1A$, we have   $$ 0=|I_n\theta-M_1A|=|I_n\theta-(I_n-p_1p_1')A|=|I_n\theta-(I_n-l_1l_1')\Lambda| $$   Here, $l_1$ is the $n\times 1$ column vector with entries $l_{1i}$ and $\Lambda=\text{diag}(\lambda_1,\ldots,\lambda_n)$. Write out $I_n\theta-(I_n-l_1l_1')\Lambda$ in full. Subtract $l_2/l_1$ times the first row from the second row, $l_3/l_1$ times the first row from the third row, and so on, and then execute the Laplace expansion along the first row. The result is   $$ 0=|I_n\theta-(I_n-l_1l_1')\Lambda|=\prod_{j=1}^n(\theta-\lambda_j)+\sum_{i=1}^nl_{1i}^2\lambda_{i}\prod_{j\neq i}(\theta-\lambda_j). $$   Plugging $\theta=0$ in the rightmost expression above gives $\sum_{i=1}^2l_{1i}^2=1$. Thus,    \begin{align*} 0&=\sum_{i=1}^nl_{1i}^2\prod_{j=1}^n(\theta-\lambda_j)+\sum_{i=1}^nl_i^2\lambda_{i}\prod_{j\neq i}(\theta-\lambda_j)\\ &=\sum_{i=1}^nl_{1i}^2(\theta-\lambda_i)\prod_{j\neq i}(\theta-\lambda_j)+\sum_{i=1}^nl_i^2\lambda_{i}\prod_{j\neq i}(\theta-\lambda_j) \end{align*}   which can be simplified and, for $\theta\neq 0$, divided by $\theta$ to get ($**$). Step 3 : Let $0=\cdots =0<\theta_1^{(s)}\leq \theta_2^{(s)}\leq \theta_{n-s}^{(s)}$ be the eigenvalues of $M_sM_{s-1}\cdots M_1A$. Then, $$ \forall s=1,\ldots,k:\quad \theta_i^{(s-1)}\leq\theta_i^{(s)}\leq \theta_{i+1}^{(s-1)},\quad i=1,\ldots,n-s.\tag{$***$} $$ Here the $\lambda_i$'s are taken to be the $\theta_i^{(0)}$'s. Proof. Let's build the first step for the case $s=1$. Consider   $$ f(\theta)=\sum_{i=1}^nl_{1i}^2\prod_{j\neq i}(\theta-\lambda_j) $$   and consider $[\lambda_r,\lambda_{r+1}]$ for $r=1,\ldots,n-1$. Either $f(\lambda_r)=0$ or $f(\lambda_{r+1})=0$ or $f(\lambda_r)f(\lambda_{r+1})\neq 0$. It's easy to show that in general $f(\lambda_r)f(\lambda_{r+1})\leq 0$ and so if $f(\lambda_r)f(\lambda_{r+1})\neq 0$ then $f(\lambda_r)f(\lambda_{r+1})< 0$ and so by the Intermediate Value Theorem, there is an zero of $f$ between $(\lambda_r,\lambda_{r+1})$. In sum, there is a zero of $f$ in each $[\lambda_r,\lambda_{r+1}]$ for each $r=1,\ldots,n-1$. It follows that   $$ 0<\lambda_1\leq\theta_1^{(1)}\leq\lambda_2\leq \theta_2^{(1)}\leq\cdots\leq \theta_{n-1}^{(1)}\leq\lambda_n. $$   This proves ($***$) for $s=1$. Proceed with $M_2M_1A$ as $M_2(M_1A)$ to get ($***$) for $s=2$. And so on. Step 4 : ($*$) holds. Proof. By Step 3, for $i=1,\ldots,n-k$,   $$ \nu_i=\theta_i^{(k)}\geq \theta_i^{(k-1)}\geq  \cdots \geq \theta_i^{(1)} \geq \theta_i^{(0)}=\lambda_i. $$   Similarly,   $$ \nu_i=\theta_i^{(k)}\geq \theta_{i+1}^{(k-1)}\geq  \cdots \geq \theta_{i+k-1}^{(1)} \geq \theta_{i+k}^{(0)}=\lambda_{i+k}. $$ Problem with Step 3 . The case for $s=1$ and $M_1A$ relies on $A$ being diagonalizable. I don't think the same argument works for $M_2(M_1A)$ because we don't know the diagonalizability of $M_1A$. So I don't think the induction step in Step 3 works (Durbin and Watson (1950) claim it does.). Moreover, while I'm confident in the Intermediate Value Theorem argument, I'm not confident about the subsequent claim: It follows that $$ 0<\lambda_1\leq\theta_1^{(1)}\leq\lambda_2\leq \theta_2^{(1)}\leq\cdots\leq \theta_{n-1}^{(1)}\leq\lambda_n. $$","Suppose that $M$ is symmetric idempotent $n\times n$ and has rank $n-k$. Suppose that $A$ is $n\times n$ and positive definite. Let $0<\nu_1\leq\nu_2\leq\ldots\nu_{n-k}$ be the nonzero eigenvalues of $MA$ and $0<\lambda_1\leq\lambda_2\leq\cdots\leq\lambda_n$ be the eigenvalues of $A$. I'm trying to show that   $$ \forall i=1,\ldots,n-k:\quad 0<\lambda_i\leq\nu_i\leq\lambda_{i+k}\tag{$*$} $$ There will be a 300 bounty for the accepted answer. Can someone also please make all the (attempted) proofs below as spoilers? I can only do that for the first proof. Attempt : I have an attempt here using Durbin and Watson (1950) but I don't fully understand the authors' argument so the attempt is incomplete. Nonetheless, I'll present the attempt here. Step 3 below is where I am stuck. Step 1 : One can write $M$ as $M_kM_{k-1}\cdots M_1$ where $M_i=I_n-p_ip_i'$ and $\{p_1,\ldots,p_k\}$ is a set of $n\times 1$ mutually orthogonal vectors s.t. $||p_i||=1$. Proof. $M$, by assumption, can be written as $M=I_n-X(X'X)^{-1}X'$ where $X$ is $n\times k$ with full column rank. Let $P=(p_1,\ldots,p_k)$ (dimension $n\times k$) be the $Q$ bit of the QR decomposition of $X$. Step 2 : Let $T=(T_1,\ldots,T_n)$ be an $n\times n$ matrix of orthonormal eigenvectors of $A$ that corresponds to eigenvalues $\lambda_1,\ldots,\lambda_n$. Let $l_{1i}=T_i'p_1$. Then any nonzero eigenvalue $\theta$ of $M_1A$ satisfies $$ \sum_{i=1}^nl_{1i}^2\prod_{j\neq i}(\theta-\lambda_j)=0.\tag{$**$} $$ Proof. For any eigenvalue (possibly $0$) $\theta$ of $M_1A$, we have   $$ 0=|I_n\theta-M_1A|=|I_n\theta-(I_n-p_1p_1')A|=|I_n\theta-(I_n-l_1l_1')\Lambda| $$   Here, $l_1$ is the $n\times 1$ column vector with entries $l_{1i}$ and $\Lambda=\text{diag}(\lambda_1,\ldots,\lambda_n)$. Write out $I_n\theta-(I_n-l_1l_1')\Lambda$ in full. Subtract $l_2/l_1$ times the first row from the second row, $l_3/l_1$ times the first row from the third row, and so on, and then execute the Laplace expansion along the first row. The result is   $$ 0=|I_n\theta-(I_n-l_1l_1')\Lambda|=\prod_{j=1}^n(\theta-\lambda_j)+\sum_{i=1}^nl_{1i}^2\lambda_{i}\prod_{j\neq i}(\theta-\lambda_j). $$   Plugging $\theta=0$ in the rightmost expression above gives $\sum_{i=1}^2l_{1i}^2=1$. Thus,    \begin{align*} 0&=\sum_{i=1}^nl_{1i}^2\prod_{j=1}^n(\theta-\lambda_j)+\sum_{i=1}^nl_i^2\lambda_{i}\prod_{j\neq i}(\theta-\lambda_j)\\ &=\sum_{i=1}^nl_{1i}^2(\theta-\lambda_i)\prod_{j\neq i}(\theta-\lambda_j)+\sum_{i=1}^nl_i^2\lambda_{i}\prod_{j\neq i}(\theta-\lambda_j) \end{align*}   which can be simplified and, for $\theta\neq 0$, divided by $\theta$ to get ($**$). Step 3 : Let $0=\cdots =0<\theta_1^{(s)}\leq \theta_2^{(s)}\leq \theta_{n-s}^{(s)}$ be the eigenvalues of $M_sM_{s-1}\cdots M_1A$. Then, $$ \forall s=1,\ldots,k:\quad \theta_i^{(s-1)}\leq\theta_i^{(s)}\leq \theta_{i+1}^{(s-1)},\quad i=1,\ldots,n-s.\tag{$***$} $$ Here the $\lambda_i$'s are taken to be the $\theta_i^{(0)}$'s. Proof. Let's build the first step for the case $s=1$. Consider   $$ f(\theta)=\sum_{i=1}^nl_{1i}^2\prod_{j\neq i}(\theta-\lambda_j) $$   and consider $[\lambda_r,\lambda_{r+1}]$ for $r=1,\ldots,n-1$. Either $f(\lambda_r)=0$ or $f(\lambda_{r+1})=0$ or $f(\lambda_r)f(\lambda_{r+1})\neq 0$. It's easy to show that in general $f(\lambda_r)f(\lambda_{r+1})\leq 0$ and so if $f(\lambda_r)f(\lambda_{r+1})\neq 0$ then $f(\lambda_r)f(\lambda_{r+1})< 0$ and so by the Intermediate Value Theorem, there is an zero of $f$ between $(\lambda_r,\lambda_{r+1})$. In sum, there is a zero of $f$ in each $[\lambda_r,\lambda_{r+1}]$ for each $r=1,\ldots,n-1$. It follows that   $$ 0<\lambda_1\leq\theta_1^{(1)}\leq\lambda_2\leq \theta_2^{(1)}\leq\cdots\leq \theta_{n-1}^{(1)}\leq\lambda_n. $$   This proves ($***$) for $s=1$. Proceed with $M_2M_1A$ as $M_2(M_1A)$ to get ($***$) for $s=2$. And so on. Step 4 : ($*$) holds. Proof. By Step 3, for $i=1,\ldots,n-k$,   $$ \nu_i=\theta_i^{(k)}\geq \theta_i^{(k-1)}\geq  \cdots \geq \theta_i^{(1)} \geq \theta_i^{(0)}=\lambda_i. $$   Similarly,   $$ \nu_i=\theta_i^{(k)}\geq \theta_{i+1}^{(k-1)}\geq  \cdots \geq \theta_{i+k-1}^{(1)} \geq \theta_{i+k}^{(0)}=\lambda_{i+k}. $$ Problem with Step 3 . The case for $s=1$ and $M_1A$ relies on $A$ being diagonalizable. I don't think the same argument works for $M_2(M_1A)$ because we don't know the diagonalizability of $M_1A$. So I don't think the induction step in Step 3 works (Durbin and Watson (1950) claim it does.). Moreover, while I'm confident in the Intermediate Value Theorem argument, I'm not confident about the subsequent claim: It follows that $$ 0<\lambda_1\leq\theta_1^{(1)}\leq\lambda_2\leq \theta_2^{(1)}\leq\cdots\leq \theta_{n-1}^{(1)}\leq\lambda_n. $$",,"['linear-algebra', 'matrices', 'inequality', 'proof-verification', 'eigenvalues-eigenvectors']"
84,"For $T\in \mathcal L(V)$, we have $\text{adj}(T)T=(\det T)I$.","For , we have .",T\in \mathcal L(V) \text{adj}(T)T=(\det T)I,"Let $V$ be an $n$-dimensional vector space over a field of characteristic $0$. For a linear operator $T\in \mathcal L(V)$, we know that $\bigwedge^n T=(\det T)I$, where $I:V\to V$ is the identity map. Further, from this answer we can define the adjugate of $T$ as $\bigwedge^{n-1}T^t:\bigwedge^{n-1}V^*\to \bigwedge^{n-1}V^*$, where $T^t$ is the transpose of $T$. We write $T^\sharp$ as a shorthand for $\bigwedge^{n-1}T^t$. The Question: It is a well-known formula that if $M$ is an $n\times n$ matrix with entries from a field $F$, then  $$\text{adj}(M)M=M(\text{adj}(M))=(\det M)I_n$$ where $\text{adj}(M)$ is the adjugate of $M$. I am trying to formulate this fact in the language of linear maps rather than matrices. The problem is that it does not mean anything to take the product of $T^\sharp$ with $T$. We just need to make a connection between $T^\sharp$ , $T$, and $\bigwedge^n T^t$.","Let $V$ be an $n$-dimensional vector space over a field of characteristic $0$. For a linear operator $T\in \mathcal L(V)$, we know that $\bigwedge^n T=(\det T)I$, where $I:V\to V$ is the identity map. Further, from this answer we can define the adjugate of $T$ as $\bigwedge^{n-1}T^t:\bigwedge^{n-1}V^*\to \bigwedge^{n-1}V^*$, where $T^t$ is the transpose of $T$. We write $T^\sharp$ as a shorthand for $\bigwedge^{n-1}T^t$. The Question: It is a well-known formula that if $M$ is an $n\times n$ matrix with entries from a field $F$, then  $$\text{adj}(M)M=M(\text{adj}(M))=(\det M)I_n$$ where $\text{adj}(M)$ is the adjugate of $M$. I am trying to formulate this fact in the language of linear maps rather than matrices. The problem is that it does not mean anything to take the product of $T^\sharp$ with $T$. We just need to make a connection between $T^\sharp$ , $T$, and $\bigwedge^n T^t$.",,"['linear-algebra', 'matrices', 'multilinear-algebra', 'exterior-algebra']"
85,Is this dual pairing the same as the inner product?,Is this dual pairing the same as the inner product?,,"If $(V, \langle \cdot$ , $ \cdot\rangle)$ is an inner product space with dual $V^*$ then there is a natural dual pairing $\langle \cdot$ , $ \cdot \rangle ^*: V^* \times V \rightarrow \mathbb K$ given by $\langle f,v \rangle ^*= f(v)$. Also any inner product space can be put in a dual pairing with itself via the inner product on it. What I want to know is if $V$ is its own dual (that is $V^* = V$) then is this natural pairing the same as the inner product on $V$? That is for all  $u, v \in V$ is $\langle u, v \rangle = \langle u, v \rangle ^*$ Thanks!","If $(V, \langle \cdot$ , $ \cdot\rangle)$ is an inner product space with dual $V^*$ then there is a natural dual pairing $\langle \cdot$ , $ \cdot \rangle ^*: V^* \times V \rightarrow \mathbb K$ given by $\langle f,v \rangle ^*= f(v)$. Also any inner product space can be put in a dual pairing with itself via the inner product on it. What I want to know is if $V$ is its own dual (that is $V^* = V$) then is this natural pairing the same as the inner product on $V$? That is for all  $u, v \in V$ is $\langle u, v \rangle = \langle u, v \rangle ^*$ Thanks!",,"['linear-algebra', 'vector-spaces', 'inner-products']"
86,A polynomial that annihilates two other,A polynomial that annihilates two other,,"While studying, I found the following problem: Let $f, g \in F[t]$. Prove that $\exists p \in F[x, y], p \neq 0 : p(f(t), g(t)) = 0$ I'd thank any hints that point me in the right direction.","While studying, I found the following problem: Let $f, g \in F[t]$. Prove that $\exists p \in F[x, y], p \neq 0 : p(f(t), g(t)) = 0$ I'd thank any hints that point me in the right direction.",,"['linear-algebra', 'polynomials']"
87,Proof - Inverse of linear function is linear,Proof - Inverse of linear function is linear,,"This is my first proof related to linear functions. It refers to the linear-algebra-$\textit{linear}$ (not the calculus-$\textit{linear}$). Please comment. Theorem The inverse of a linear bijection is linear. Proof Let $X,Y$ be vector spaces over a common field. Let $f : X \rightarrow Y$ be a linear bijection. We denote by $f^{-1}$ the inverse of $f$. It remains to prove that $f^{-1}$ is linear, i.e. both $\textit{additive}$ and $\textit{homogeneous}$. Additivity Let $y_1, y_2 \in Y$. We prove that $$f^{-1}(y_1 + y_2) = f^{-1}(y_1) + f^{-1}(y_2).$$ \begin{equation*} 	\begin{split} 		f^{-1}(y_1) + f^{-1}(y_2) 			&=	f^{-1}\Big( f\big( f^{-1}(y_1) + f^{-1}(y_2) \big) \Big) && \quad \text{by bijectivity} \\ 			&=	f^{-1}\Big(	f\big( f^{-1}(y_1) \big) + 												f\big( f^{-1}(y_2) \big) \Big) && \quad \text{by linearity of } f \\ 			&=	f^{-1}\Big(	y_1 + f\big( f^{-1}(y_2) \big) \Big) && \quad \text{by bijectivity} \\ 			&= f^{-1}(y_1 + y_2) && \quad \text{by bijectivity}\phantom{\Big(\Big)} \\ 	\end{split} \end{equation*} Homogeneity Let $y \in Y$ and let $s$ be a scalar. We prove that $$f^{-1}(sy) = sf^{-1}(y).$$ \begin{equation*} 	\begin{split} 		sf^{-1}(y) 			&= f^{-1}\Big( f\big( sf^{-1}(y) \big) \Big) && \quad \text{by bijectivity} \\ 			&= f^{-1}\Big( sf\big( f^{-1}(y) \big) \Big) && \quad \text{by linearity of } f \\ 			&= f^{-1}(sy) && \quad \text{by bijectivity}\phantom{\Big(\Big)} \\ 	\end{split} \end{equation*} QED","This is my first proof related to linear functions. It refers to the linear-algebra-$\textit{linear}$ (not the calculus-$\textit{linear}$). Please comment. Theorem The inverse of a linear bijection is linear. Proof Let $X,Y$ be vector spaces over a common field. Let $f : X \rightarrow Y$ be a linear bijection. We denote by $f^{-1}$ the inverse of $f$. It remains to prove that $f^{-1}$ is linear, i.e. both $\textit{additive}$ and $\textit{homogeneous}$. Additivity Let $y_1, y_2 \in Y$. We prove that $$f^{-1}(y_1 + y_2) = f^{-1}(y_1) + f^{-1}(y_2).$$ \begin{equation*} 	\begin{split} 		f^{-1}(y_1) + f^{-1}(y_2) 			&=	f^{-1}\Big( f\big( f^{-1}(y_1) + f^{-1}(y_2) \big) \Big) && \quad \text{by bijectivity} \\ 			&=	f^{-1}\Big(	f\big( f^{-1}(y_1) \big) + 												f\big( f^{-1}(y_2) \big) \Big) && \quad \text{by linearity of } f \\ 			&=	f^{-1}\Big(	y_1 + f\big( f^{-1}(y_2) \big) \Big) && \quad \text{by bijectivity} \\ 			&= f^{-1}(y_1 + y_2) && \quad \text{by bijectivity}\phantom{\Big(\Big)} \\ 	\end{split} \end{equation*} Homogeneity Let $y \in Y$ and let $s$ be a scalar. We prove that $$f^{-1}(sy) = sf^{-1}(y).$$ \begin{equation*} 	\begin{split} 		sf^{-1}(y) 			&= f^{-1}\Big( f\big( sf^{-1}(y) \big) \Big) && \quad \text{by bijectivity} \\ 			&= f^{-1}\Big( sf\big( f^{-1}(y) \big) \Big) && \quad \text{by linearity of } f \\ 			&= f^{-1}(sy) && \quad \text{by bijectivity}\phantom{\Big(\Big)} \\ 	\end{split} \end{equation*} QED",,"['linear-algebra', 'functions', 'soft-question', 'proof-verification', 'proof-writing']"
88,Geometric meaning of a matrix decomposed into its symmetric and skew-symmetric parts,Geometric meaning of a matrix decomposed into its symmetric and skew-symmetric parts,,"What's the geometric meaning of a matrix decomposed into its symmetric and skew-symmetric parts?  For example, a skew-symmetric matrix on its own can be interpreted as an infinitesimal rotation.  As an another example, the polar decomposition of a matrix A=UP=QU for a unitary matrix U and symmetric positive definite matrices P and Q means that we can interpret a matrix as a stretching (the positive definite matrix) followed by a rotation (the unitary matrix) or vice versa. Basically, the decomposition of a matrix into its symmetric and skew symmetric parts $A=(1/2)(A+A^T)+(1/2)(A-A^T)$ is causing me problems because I don't know how to geometrically interpret the symmetric part (if it was positive definite it would be a stretch) nor the sum (if it was a product then one action would follow the other.) Edit 1 The answer from @user_of_math helped me figure the rest of this out. Assume that we have a matrix $B$ thats symmetric.  Then, we know that it has a spectral decomposition $B=VDV^T$.  What this says is that the action of $B$ on a vector $x$ occurs in three steps We determine how much we will project $x$ onto the columns of $V$, $V^T x$ We scale this projection by the eigenvalues of $A$, $D V^T x$ Then, we put everything back into the original coordinate axis, $V D V^T x$ In other words, a symmetric operator scales the space along the orthogonal axes defined by $V$.  When $B$ is positive definite, the elements of $D$ are positive and this is a stretch.  When $B$ is indefinite or negative definite, we can potentially flip or collapse an axis.  In any case, provides one explanation of the geometric the meaning of a symmetric operator. Now, let $C$ be a skew-symmetric operator.  Then, $exp(C)$ is orthonormal matrix, which corresponds to a rotation. At this point, we have an idea of what happens with a symmetric operator and a skew-symmetric operator, but there's a disconnect because our explanation assumed the skew-symmetric operator corresponds to something infinitesimal and the symmetric operator does not.  As such, consider an infinitesimal shift of a vector x by a symmetric operator B $$ \lim\limits_{\alpha\rightarrow 0} (I+\alpha B)x $$ More specifically, we want to do an infinite number of these transformations, so we have $$ \lim\limits_{n\rightarrow\infty} \left(I+\frac{1}{n} B\right)^n x = exp(B)x $$ Now, since $B$ is symmetric, we have the spectral decomposition, $B=VDV^T$.  Hence, $$ exp(B)x=exp(VDV^T)x=Vexp(D)V^Tx, $$ which is interesting because this says that $exp(B)$ is a positive definite operator.  In other words, and infinite number of shifts by a symmetric operator eventually yields a single stretch by a positive definite operator. In any case, let's look at the general situation now.  Let $B=(A+A^T)/2$ be the symmetric part of $A$ and $B=(A-A^T)/2$ be the skew-symmetric part.  Then, consider an infinite number of shifts by $A$, $$ \lim\limits_{n\rightarrow\infty} \left(I+\frac{1}{n} A\right)^n  = exp(A) = exp(B+C) = \lim\limits_{n\rightarrow \infty}(exp(B/n)exp(C/n))^n $$ where the last equality comes from the Lie product formula.  This says that the meaning of $exp(A)$ is an infinite, alternating application of infinitesimally small stretches, defined by $exp((A+A^T)/2)$, and rotations, defined by $exp((A-A^T)/2)$.  In other words, the geometric meaning of exp(A), when $A$ is decomposed into its symmetric and skew-symmetric components, is an alternating application of stretches and rotations defined by these symmetric and skew-symmetric components, respectively.","What's the geometric meaning of a matrix decomposed into its symmetric and skew-symmetric parts?  For example, a skew-symmetric matrix on its own can be interpreted as an infinitesimal rotation.  As an another example, the polar decomposition of a matrix A=UP=QU for a unitary matrix U and symmetric positive definite matrices P and Q means that we can interpret a matrix as a stretching (the positive definite matrix) followed by a rotation (the unitary matrix) or vice versa. Basically, the decomposition of a matrix into its symmetric and skew symmetric parts $A=(1/2)(A+A^T)+(1/2)(A-A^T)$ is causing me problems because I don't know how to geometrically interpret the symmetric part (if it was positive definite it would be a stretch) nor the sum (if it was a product then one action would follow the other.) Edit 1 The answer from @user_of_math helped me figure the rest of this out. Assume that we have a matrix $B$ thats symmetric.  Then, we know that it has a spectral decomposition $B=VDV^T$.  What this says is that the action of $B$ on a vector $x$ occurs in three steps We determine how much we will project $x$ onto the columns of $V$, $V^T x$ We scale this projection by the eigenvalues of $A$, $D V^T x$ Then, we put everything back into the original coordinate axis, $V D V^T x$ In other words, a symmetric operator scales the space along the orthogonal axes defined by $V$.  When $B$ is positive definite, the elements of $D$ are positive and this is a stretch.  When $B$ is indefinite or negative definite, we can potentially flip or collapse an axis.  In any case, provides one explanation of the geometric the meaning of a symmetric operator. Now, let $C$ be a skew-symmetric operator.  Then, $exp(C)$ is orthonormal matrix, which corresponds to a rotation. At this point, we have an idea of what happens with a symmetric operator and a skew-symmetric operator, but there's a disconnect because our explanation assumed the skew-symmetric operator corresponds to something infinitesimal and the symmetric operator does not.  As such, consider an infinitesimal shift of a vector x by a symmetric operator B $$ \lim\limits_{\alpha\rightarrow 0} (I+\alpha B)x $$ More specifically, we want to do an infinite number of these transformations, so we have $$ \lim\limits_{n\rightarrow\infty} \left(I+\frac{1}{n} B\right)^n x = exp(B)x $$ Now, since $B$ is symmetric, we have the spectral decomposition, $B=VDV^T$.  Hence, $$ exp(B)x=exp(VDV^T)x=Vexp(D)V^Tx, $$ which is interesting because this says that $exp(B)$ is a positive definite operator.  In other words, and infinite number of shifts by a symmetric operator eventually yields a single stretch by a positive definite operator. In any case, let's look at the general situation now.  Let $B=(A+A^T)/2$ be the symmetric part of $A$ and $B=(A-A^T)/2$ be the skew-symmetric part.  Then, consider an infinite number of shifts by $A$, $$ \lim\limits_{n\rightarrow\infty} \left(I+\frac{1}{n} A\right)^n  = exp(A) = exp(B+C) = \lim\limits_{n\rightarrow \infty}(exp(B/n)exp(C/n))^n $$ where the last equality comes from the Lie product formula.  This says that the meaning of $exp(A)$ is an infinite, alternating application of infinitesimally small stretches, defined by $exp((A+A^T)/2)$, and rotations, defined by $exp((A-A^T)/2)$.  In other words, the geometric meaning of exp(A), when $A$ is decomposed into its symmetric and skew-symmetric components, is an alternating application of stretches and rotations defined by these symmetric and skew-symmetric components, respectively.",,"['linear-algebra', 'geometry', 'classical-mechanics']"
89,Generalization of Cayley-Hamilton,Generalization of Cayley-Hamilton,,"I'm having trouble following a proof of this generalization of the Cayley-Hamilton theorem: Suppose that $M$ is an $A$-module generated by $n$ elements, and that $\varphi \in \operatorname{Hom}_A(M,M)$; let $I$ be an ideal of $A$ such that $\varphi(M) \subset IM$. Then there is a relation of the form $$(**)\quad \varphi^n + a_1 \varphi^{n-1} + \cdots + a_{n-1}\varphi + a_n = 0,$$ with $a_i \in I^i$ for $1 \leqslant i \leqslant n$ (where both sides are considered as endomorphisms of $M$). This is theorem 2.1 in Matsumura's Commutative Ring Theory . The proof there goes as follows: Let $M = A\omega_1 + \dots + A\omega_n$; by the assumption $\varphi(M) \subset IM$ there exist $a_{ij} \in I$ such that $\varphi(\omega_i) = \sum_{j=1}^n a_{ij}\omega_j$. This can be rewritten $$\sum_{j=1}^n(\varphi\delta_{ij} - a_{ij})\omega_j = 0 \quad (\text{for }\  1\leqslant i \leqslant n),$$ where $\delta_{ij}$ is the Kronecker symbol. The coefficients of this system of linear equations can be viewed as a square matrix $(\varphi\delta_{ij} - a_{ij})$ of elements of $A'[\varphi]$, the commutative subring of the endomorphism ring $E$ of $M$ generated by the image $A'$ of $A$ together with $\varphi$; let $b_{ij}$ denote its $(i,j)$th cofactor, and $d$ its determinant. By multiplying the above equation through by $b_{ik}$ and summing over $i$, we get $d\omega_k = 0$ for $1 \leqslant k \leqslant n$. Hence $d\cdot M = 0$, so that $d = 0$ as an element of $E$. Expanding the determinant $d$ gives a relation of the form $(**)$. I would like to understand this particular proof, but I'm having trouble concluding $d\omega_k = 0$ for $1 \leqslant k \leqslant n$ from the multiplication of the above equation by $b_{ik}$ and summing over $i$. It's clear that the right hand side remains zero, but I'm having trouble expanding the left hand side into a useful form. I'm pretty sure I have to use the fact that $d = \sum_{i=1}^n b_{ik} (\varphi\delta_{ik} - a_{ik})$ by the cofactor expansion of the determinant along the $k$th column, but I can only seem conclude $d\omega_k - d\omega_k = 0$, which is not very enlightening. Thanks in advance.","I'm having trouble following a proof of this generalization of the Cayley-Hamilton theorem: Suppose that $M$ is an $A$-module generated by $n$ elements, and that $\varphi \in \operatorname{Hom}_A(M,M)$; let $I$ be an ideal of $A$ such that $\varphi(M) \subset IM$. Then there is a relation of the form $$(**)\quad \varphi^n + a_1 \varphi^{n-1} + \cdots + a_{n-1}\varphi + a_n = 0,$$ with $a_i \in I^i$ for $1 \leqslant i \leqslant n$ (where both sides are considered as endomorphisms of $M$). This is theorem 2.1 in Matsumura's Commutative Ring Theory . The proof there goes as follows: Let $M = A\omega_1 + \dots + A\omega_n$; by the assumption $\varphi(M) \subset IM$ there exist $a_{ij} \in I$ such that $\varphi(\omega_i) = \sum_{j=1}^n a_{ij}\omega_j$. This can be rewritten $$\sum_{j=1}^n(\varphi\delta_{ij} - a_{ij})\omega_j = 0 \quad (\text{for }\  1\leqslant i \leqslant n),$$ where $\delta_{ij}$ is the Kronecker symbol. The coefficients of this system of linear equations can be viewed as a square matrix $(\varphi\delta_{ij} - a_{ij})$ of elements of $A'[\varphi]$, the commutative subring of the endomorphism ring $E$ of $M$ generated by the image $A'$ of $A$ together with $\varphi$; let $b_{ij}$ denote its $(i,j)$th cofactor, and $d$ its determinant. By multiplying the above equation through by $b_{ik}$ and summing over $i$, we get $d\omega_k = 0$ for $1 \leqslant k \leqslant n$. Hence $d\cdot M = 0$, so that $d = 0$ as an element of $E$. Expanding the determinant $d$ gives a relation of the form $(**)$. I would like to understand this particular proof, but I'm having trouble concluding $d\omega_k = 0$ for $1 \leqslant k \leqslant n$ from the multiplication of the above equation by $b_{ik}$ and summing over $i$. It's clear that the right hand side remains zero, but I'm having trouble expanding the left hand side into a useful form. I'm pretty sure I have to use the fact that $d = \sum_{i=1}^n b_{ik} (\varphi\delta_{ik} - a_{ik})$ by the cofactor expansion of the determinant along the $k$th column, but I can only seem conclude $d\omega_k - d\omega_k = 0$, which is not very enlightening. Thanks in advance.",,"['linear-algebra', 'commutative-algebra', 'ring-theory', 'modules']"
90,Positive semidefiniteness of a block matrix of positive semidefinite matrices,Positive semidefiniteness of a block matrix of positive semidefinite matrices,,"Given any symmetric matrix $\mathbf{M} = \begin{pmatrix} \mathbf{A} & \mathbf{B}\\   \mathbf{B}^\mathrm{T}& \mathbf{C}  \end{pmatrix}$, the following conditions are equivalent: (1) $\mathbf{M}\succeq 0$ ($\mathbf{M}$ is positive semidefinite) (2) $\mathbf{A}\succeq 0$, $\;\;(\mathbf{I}-\mathbf{A}\mathbf{A}^T )\mathbf{B}= 0$, $\;\;\mathbf{C} - \mathbf{B}^T\mathbf{A}^{\dagger }\mathbf{B} \succeq 0$. (3) $\mathbf{C}\succeq 0$, $\;\;(\mathbf{I}-\mathbf{C}\mathbf{C}^T )\mathbf{B}= 0$, $\;\;\mathbf{A} - \mathbf{B}\mathbf{C}^{\dagger }\mathbf{B}^T \succeq 0$. Then, if we are given that $\mathbf{A}\succeq 0$, $\mathbf{B}\succeq 0$ and $\mathbf{C}\succeq 0$, is there any way to verify (2) or (3) and therefore conclude that $\mathbf{M}$ will be positive semidefinite as well?","Given any symmetric matrix $\mathbf{M} = \begin{pmatrix} \mathbf{A} & \mathbf{B}\\   \mathbf{B}^\mathrm{T}& \mathbf{C}  \end{pmatrix}$, the following conditions are equivalent: (1) $\mathbf{M}\succeq 0$ ($\mathbf{M}$ is positive semidefinite) (2) $\mathbf{A}\succeq 0$, $\;\;(\mathbf{I}-\mathbf{A}\mathbf{A}^T )\mathbf{B}= 0$, $\;\;\mathbf{C} - \mathbf{B}^T\mathbf{A}^{\dagger }\mathbf{B} \succeq 0$. (3) $\mathbf{C}\succeq 0$, $\;\;(\mathbf{I}-\mathbf{C}\mathbf{C}^T )\mathbf{B}= 0$, $\;\;\mathbf{A} - \mathbf{B}\mathbf{C}^{\dagger }\mathbf{B}^T \succeq 0$. Then, if we are given that $\mathbf{A}\succeq 0$, $\mathbf{B}\succeq 0$ and $\mathbf{C}\succeq 0$, is there any way to verify (2) or (3) and therefore conclude that $\mathbf{M}$ will be positive semidefinite as well?",,"['linear-algebra', 'matrices']"
91,Is there an example of an orthogonal matrix which is not diagonalizable,Is there an example of an orthogonal matrix which is not diagonalizable,,"Based off the theory I can't see any reason that an example would not exist.  Specifically, the fact that A matrix is orthogonal only implies that the possible eigenvalues are $\pm 1$.  However, we don't know anything about the sizes of the eigenspaces. Nonetheless, it is not hard to show that a 2x2 orthogonal matrix must in fact be diagonalizable.  So an example has to at least be 3x3.","Based off the theory I can't see any reason that an example would not exist.  Specifically, the fact that A matrix is orthogonal only implies that the possible eigenvalues are $\pm 1$.  However, we don't know anything about the sizes of the eigenspaces. Nonetheless, it is not hard to show that a 2x2 orthogonal matrix must in fact be diagonalizable.  So an example has to at least be 3x3.",,['linear-algebra']
92,Is there a quick way to generate the characteristic polynomial of a Vandermonde matrix?,Is there a quick way to generate the characteristic polynomial of a Vandermonde matrix?,,"This came up on an exam recently as extra credit. The first part was to find the characteristic polynomial, $f_A = \text{det(}A - xI_n)$ where $I_n$ is the n by n identity matrix, of $A = \left| \begin{array}{ccc} 1 & 1 & 1 \\ a & b & c \\ a^2 & b^2 & c^2 \end{array} \right|.$ and that's easy enough with some basic calculations and brute force. But the second part asks for a general n x n Vandermonde matrix like $$B = \left| \begin{array}{ccc} 1 &  & 1 \\ a_1 & ... & a_n \\ ... &  & ... \\ a_1^{n-1} &  & a_n^{n-1} \end{array} \right|.$$ I cant find anything in our book or the internet about the second part. Is there a ""good"" way to find the characteristic polynomial of B? I know nothing of the values $a_i$ in the matrix if that makes any difference at all. Thanks in Advance for any assistance. EDIT: It turns out that this question was put on the exam last minute and the Professor thought it should be easy because the determinant is easy; Its not. So I don't even know if this has an answer.","This came up on an exam recently as extra credit. The first part was to find the characteristic polynomial, $f_A = \text{det(}A - xI_n)$ where $I_n$ is the n by n identity matrix, of $A = \left| \begin{array}{ccc} 1 & 1 & 1 \\ a & b & c \\ a^2 & b^2 & c^2 \end{array} \right|.$ and that's easy enough with some basic calculations and brute force. But the second part asks for a general n x n Vandermonde matrix like $$B = \left| \begin{array}{ccc} 1 &  & 1 \\ a_1 & ... & a_n \\ ... &  & ... \\ a_1^{n-1} &  & a_n^{n-1} \end{array} \right|.$$ I cant find anything in our book or the internet about the second part. Is there a ""good"" way to find the characteristic polynomial of B? I know nothing of the values $a_i$ in the matrix if that makes any difference at all. Thanks in Advance for any assistance. EDIT: It turns out that this question was put on the exam last minute and the Professor thought it should be easy because the determinant is easy; Its not. So I don't even know if this has an answer.",,['linear-algebra']
93,Sparse basis for linear subspace,Sparse basis for linear subspace,,"Suppose I have a linear subspace of some vector space, e.g., described as the column space of some big matrix. How would I algorithmically find a basis of that same subspace where the basis matrix is sparse, i.e., most entries in most basis vectors are zero? I understand that this will depend on the structure of the matrix, so you might prefer to interpret this as as many entries as possible are zero instead of most entries are zero. I'm interested in good practical solutions, even if the results are not optimal.","Suppose I have a linear subspace of some vector space, e.g., described as the column space of some big matrix. How would I algorithmically find a basis of that same subspace where the basis matrix is sparse, i.e., most entries in most basis vectors are zero? I understand that this will depend on the structure of the matrix, so you might prefer to interpret this as as many entries as possible are zero instead of most entries are zero. I'm interested in good practical solutions, even if the results are not optimal.",,"['linear-algebra', 'matrices', 'vector-spaces', 'numerical-linear-algebra', 'sparse-matrices']"
94,Two definitions of tensor product: when are they equivalent?,Two definitions of tensor product: when are they equivalent?,,"Take vector spaces $V, W$ over the field $\mathbb{K}$ . I've come across two different definitions of tensor product $V \otimes W$ and was wondering whether they are the same thing. Definition 1 . $V \otimes W$ is the quotient space $F(V\times W) / \sim$ where $F(V\times W)$ is the free vector space over $V \times W$ and $\sim$ denotes the following relations: $$(v_1+v_2, w)\sim (v_1, w)+(v_2,w), \qquad (v, w_1+w_2)\sim (v, w_1) + (v, w_2), $$ $$(\lambda v, w) \sim (v, \lambda w)\sim \lambda (v, w).$$ In other words $V \otimes W$ is the space of formal linear combinations $$\sum_{j=1}^n \alpha_j v_j \otimes w_j, \qquad \alpha_j \in \mathbb{K}, v_j \in V, w_j \in W$$ where we explicitly require $\otimes$ to be bilinear. Definition 2 (When $V, W$ are finite-dimensional.) $V \otimes W$ is the space of mappings from $V^\star\times W^\star$ into $\mathbb{K}$ that are linear in each variable. Question . Under what circumstances do those two definitions coincide (up to canonical isomorphism)? I guess that this happens if and only if both $V$ and $W$ are finite-dimensional.",Take vector spaces over the field . I've come across two different definitions of tensor product and was wondering whether they are the same thing. Definition 1 . is the quotient space where is the free vector space over and denotes the following relations: In other words is the space of formal linear combinations where we explicitly require to be bilinear. Definition 2 (When are finite-dimensional.) is the space of mappings from into that are linear in each variable. Question . Under what circumstances do those two definitions coincide (up to canonical isomorphism)? I guess that this happens if and only if both and are finite-dimensional.,"V, W \mathbb{K} V \otimes W V \otimes W F(V\times W) / \sim F(V\times W) V \times W \sim (v_1+v_2, w)\sim (v_1, w)+(v_2,w), \qquad (v, w_1+w_2)\sim (v, w_1) + (v, w_2),  (\lambda v, w) \sim (v, \lambda w)\sim \lambda (v, w). V \otimes W \sum_{j=1}^n \alpha_j v_j \otimes w_j, \qquad \alpha_j \in \mathbb{K}, v_j \in V, w_j \in W \otimes V, W V \otimes W V^\star\times W^\star \mathbb{K} V W","['linear-algebra', 'abstract-algebra', 'tensor-products']"
95,$3\times 3$ matrices over $GF(3)$ which satisfy $A^{-1}=(A^T)^2$,matrices over  which satisfy,3\times 3 GF(3) A^{-1}=(A^T)^2,"I came across this question on Quora : How many $3\times 3$ matrices over the finite field $\{0,1,2\}$ satisfy the condition $A^{-1}=(A^T)^2$ ? Thanks to NumPy, with sheer brute-force , I found that there are $33$ such matrices. (You may go through the code and the output.) I believe that there is a more elegant/algebraic approach to this problem because of the following observations I made: $A^{-1}=(A^T)^2\implies \det A= 1$ so there can be at most $\dfrac{(3^3-1)(3^3-3)(3^3-3^2)}{3-1}=5616\tag*{}$ such matrices. ( Order of special linear group .) All of them are symmetric i.e., $A^T=A$ . (I don't know how to prove this but it's supported by observation.) All of them satisfy $A^3=I$ . (As a consequence of 2) All of them have zero trace i.e., sum of the diagonal elements is $0$ . (I don't know how to prove this either.) There is only one such matrix which has $(1,1,1)$ on the diagonal. It is the identity matrix. There are four matrices each which have $(2,2,2)$ or $(0,0,0)$ on their diagonal. For each permutation $\sigma$ of $0$ , $1$ , $2$ , there are four matrices such that $\text{diag}(A)=\sigma$ . This accounts for the $1+4+4+3!\cdot 4=33$ such matrices. I would want to know why $(2)$ and $(4)$ are true. Here's a short summary: For a matrix $A$ over the field of three elements, it looks like: $$A^{-1}=(A^T)^2 \iff  \begin{array}\ A^T=A\\ \text{det }A=1\\  A^3=I \end{array}\implies \text{tr}(A)=0$$ However, I don't have a proof, it's just observations. Update: After a long discussion in comments, it is clear to me why $(2)$ and $(4)$ are true. Why (2) is true? You can rewrite the given condition as $A^T=(A^TA)^{-1}$ ; now take transpose on both sides to get $A=(AA^T)^{-1}$ and now, use these facts: The product of a matrix and its transpose is always symmetric. The inverse (if exists) of a symmetric matrix is symmetric. Why (4) is true? $1$ is the eigen value of $A^3$ with algebraic multiplicity of $3$ ... This means that eigenvalues of $A$ are of the form cube-root of unity. There is only one cube root of unity in this field. It is $1$ iteself. So the eigenvalues of $A$ are $1,1,1$ . Thus, $\text{tr}(A)=1+1+1=0$ . Account for 33 solutions : I think I have also got an idea for a more elegant approach which accounts for the $33$ solutions. I don't know if there is any fallacy in my logic, it is not at all rigorous :( The properties $A^3=I$ and $A^T=A$ are preserved under conjugation by a non-singular diagonal matrix in this field . What I mean is that: Let $D$ be a non-singular diagonal matrix (there are $2^3$ choices because $0$ can't be diagonal entry). If $A$ is a solution, $DAD^{-1}$ is also a solution. Reason : $\mathbb Z_3^*$ is a cyclic group of order two and the group of diagonal matrices is isomorphic to $\mathbb Z_3^*\times \mathbb Z_3^*\times \mathbb Z_3^*$ which is Boolean group. Hence, $D^2=I$ . This implies that $D^{-1}=D=D^{T}$ . Thus, the conjugate $DAD^{-1}=DAD^T=(DAD^{-1})^T$ is symmetric. Also, notice that $(DAD^{-1})^3=DA^3D^{-1}=I$ . Another point to note is that Conjugation by a diagonal matrix preserves the diagonal I mean that $\text{diag}(DAD^{-1})=\text{diag}(A)$ . Reason: When you left multiply $A$ by a diagonal matrix $D$ , each row $i$ is scaled by $D_{ii}$ . Following that right multiplication by $D^{-1}$ , scales each column $i$ by $1/D_{ii}$ . As a result, the diagonal elements remain unscaled and intact. Given a solution $A$ with $\text{diag}(A)=(1,2,0)$ , how many $D$ -conjugates of $A$ exist? $D$ and $-D$ yield the same conjugate because $DAD^{-1}=(-D)A(-D)^{-1}$  We have $8/2=4$ unique diagonal matrices upto sign, each of which yield a unique $D$ -conjugate of $A$ . These conjugates have the same diagonal as $A$ and also satisfy our condition. Likewise, for every solution $A$ with $\text{diag}(A)=(2,2,2)$ or $(2,1,0)$ , etc. (we make sure the trace is zero), we have $4$ solutions each by conjugating $A$ with diagonal matrices. $A=I$ is an exception because $DID^{-1}=I$ so we dont get any new solution. Leaving $(1,1,1)$ aside, there are $8$ other arrangements for the diagonal entries which make the trace zero: $(0,0,0)$ , $(2,2,2)$ or some permutation of $(1,2,0)$ . For each of these types of arrangements along the diagonal, it is observed that there is at least one solution, which can be $D$ -conjugated to generate new solutions. \begin{array}{|c|c|}\hline \text{diag}(A) & \text{No. of $D$-conjugates}\\ \hline (1,1,1)&1\\ \hline(0,0,0)&4\\ \hline (2,2,2)&4\\ \hline (1,2,0)&4\\ \hline (1,0,2)&4\\ \hline (0,1,2)&4\\ \hline (0,2,1)&4\\ \hline (2,1,0)&4\\ \hline (2,0,1)&4\\ \hline \end{array} That makes the count $8\times 4+1=33$ .","I came across this question on Quora : How many matrices over the finite field satisfy the condition ? Thanks to NumPy, with sheer brute-force , I found that there are such matrices. (You may go through the code and the output.) I believe that there is a more elegant/algebraic approach to this problem because of the following observations I made: so there can be at most such matrices. ( Order of special linear group .) All of them are symmetric i.e., . (I don't know how to prove this but it's supported by observation.) All of them satisfy . (As a consequence of 2) All of them have zero trace i.e., sum of the diagonal elements is . (I don't know how to prove this either.) There is only one such matrix which has on the diagonal. It is the identity matrix. There are four matrices each which have or on their diagonal. For each permutation of , , , there are four matrices such that . This accounts for the such matrices. I would want to know why and are true. Here's a short summary: For a matrix over the field of three elements, it looks like: However, I don't have a proof, it's just observations. Update: After a long discussion in comments, it is clear to me why and are true. Why (2) is true? You can rewrite the given condition as ; now take transpose on both sides to get and now, use these facts: The product of a matrix and its transpose is always symmetric. The inverse (if exists) of a symmetric matrix is symmetric. Why (4) is true? is the eigen value of with algebraic multiplicity of ... This means that eigenvalues of are of the form cube-root of unity. There is only one cube root of unity in this field. It is iteself. So the eigenvalues of are . Thus, . Account for 33 solutions : I think I have also got an idea for a more elegant approach which accounts for the solutions. I don't know if there is any fallacy in my logic, it is not at all rigorous :( The properties and are preserved under conjugation by a non-singular diagonal matrix in this field . What I mean is that: Let be a non-singular diagonal matrix (there are choices because can't be diagonal entry). If is a solution, is also a solution. Reason : is a cyclic group of order two and the group of diagonal matrices is isomorphic to which is Boolean group. Hence, . This implies that . Thus, the conjugate is symmetric. Also, notice that . Another point to note is that Conjugation by a diagonal matrix preserves the diagonal I mean that . Reason: When you left multiply by a diagonal matrix , each row is scaled by . Following that right multiplication by , scales each column by . As a result, the diagonal elements remain unscaled and intact. Given a solution with , how many -conjugates of exist? and yield the same conjugate because  We have unique diagonal matrices upto sign, each of which yield a unique -conjugate of . These conjugates have the same diagonal as and also satisfy our condition. Likewise, for every solution with or , etc. (we make sure the trace is zero), we have solutions each by conjugating with diagonal matrices. is an exception because so we dont get any new solution. Leaving aside, there are other arrangements for the diagonal entries which make the trace zero: , or some permutation of . For each of these types of arrangements along the diagonal, it is observed that there is at least one solution, which can be -conjugated to generate new solutions. That makes the count .","3\times 3 \{0,1,2\} A^{-1}=(A^T)^2 33 A^{-1}=(A^T)^2\implies \det A= 1 \dfrac{(3^3-1)(3^3-3)(3^3-3^2)}{3-1}=5616\tag*{} A^T=A A^3=I 0 (1,1,1) (2,2,2) (0,0,0) \sigma 0 1 2 \text{diag}(A)=\sigma 1+4+4+3!\cdot 4=33 (2) (4) A A^{-1}=(A^T)^2 \iff  \begin{array}\ A^T=A\\ \text{det }A=1\\  A^3=I \end{array}\implies \text{tr}(A)=0 (2) (4) A^T=(A^TA)^{-1} A=(AA^T)^{-1} 1 A^3 3 A 1 A 1,1,1 \text{tr}(A)=1+1+1=0 33 A^3=I A^T=A D 2^3 0 A DAD^{-1} \mathbb Z_3^* \mathbb Z_3^*\times \mathbb Z_3^*\times \mathbb Z_3^* D^2=I D^{-1}=D=D^{T} DAD^{-1}=DAD^T=(DAD^{-1})^T (DAD^{-1})^3=DA^3D^{-1}=I \text{diag}(DAD^{-1})=\text{diag}(A) A D i D_{ii} D^{-1} i 1/D_{ii} A \text{diag}(A)=(1,2,0) D A D -D DAD^{-1}=(-D)A(-D)^{-1} 8/2=4 D A A A \text{diag}(A)=(2,2,2) (2,1,0) 4 A A=I DID^{-1}=I (1,1,1) 8 (0,0,0) (2,2,2) (1,2,0) D \begin{array}{|c|c|}\hline \text{diag}(A) & \text{No. of D-conjugates}\\ \hline (1,1,1)&1\\ \hline(0,0,0)&4\\ \hline (2,2,2)&4\\ \hline (1,2,0)&4\\ \hline (1,0,2)&4\\ \hline (0,1,2)&4\\ \hline (0,2,1)&4\\ \hline (2,1,0)&4\\ \hline (2,0,1)&4\\ \hline \end{array} 8\times 4+1=33","['linear-algebra', 'matrices', 'finite-fields']"
96,"if $F(x,y)>0$ is convex and $1/F$ is convex show that up to a rotation $F(x,y) = F(x)$",if  is convex and  is convex show that up to a rotation,"F(x,y)>0 1/F F(x,y) = F(x)","Let $$F \; : \; \mathbb{R}^2 \to \mathbb{R}$$ be a strictly positive function such that $F, \frac{1}{F}$ are both convex. Show that there is a function $$g \; : \; \mathbb{R} \to \mathbb{R}$$ and a vector $$v \in \mathbb{R}^2$$ such that $$F(x) = g(v\cdot x) \;\; \text{ for all } x \in \mathbb{R}^2$$ I have found a solution to this exercise but I'm not satisfied because it uses the extra assumption that $F \in C^2$ and $\nabla F$ is never $\underline{0}$ , I would like to find an easier solution that doesn't rely on these extra assumptions. This is my solution : I will assume $F \in C^2$ and $\nabla F$ is always non-zero. First a bit of notation : I use the euclidean metric on $\mathbb{R}^2$ thus I identify matrices with biliniear forms, if $A$ is a ( $2 \times 2$ ) matrix by $A \geq 0$ I mean $$(Ax,x) \geq 0 \;\; \forall x \in \mathbb{R}^2$$ by $A \leq B$ I mean $B - A \geq 0$ , or equivalently $$(Ax,x) \leq (Bx,x) \;\; \forall x \in \mathbb{R}^2$$ I claim that if $A$ is a symmetric matrix and $B$ is any matrix such that $0 \leq A \leq B$ then $\ker(B) \subset \ker(A)$ To show it observe that, since $A$ is symmetric, up to a rotation I can assume it is diagonal,so there are $\lambda,\mu \geq 0$ so that $$ 		A= 		\left( {\begin{array}{cc} 				\lambda & 0 \\ 				0 & \mu \\ 		\end{array} } \right) 		$$ then if $B v = 0$ and $v = (v_1,v_2)$ I find $0 \leq \lambda (v_1)^2 + \mu(v_2)^2 \leq 0$ which clearly implies $\lambda v_1 = \mu v_2 = 0$ so $A v = 0$ which shows the claim Now back to the exercise I assume $F \in C^2$ , thus $1/F \in C^2$ and so convexity implies $\nabla^2 F \geq 0$ , $\nabla^2 1/F \geq 0$ . I calculate $\nabla^2 \frac{1}{F}$ \begin{equation}\nabla^2 \frac{1}{F} = \frac{2}{F^3}\nabla F \otimes \nabla F - \frac{1}{F^2}\nabla^2 F \geq 0\end{equation} from this I easily find \begin{equation} 0 \leq \nabla^2 F \leq \frac{2}{F} (\nabla F \otimes \nabla F) \end{equation} so using the previous claim $\ker(\nabla F \otimes \nabla F) \subset \ker(\nabla^2 F)$ but clearly $\ker(\nabla F) \subset \ker(\nabla F \otimes \nabla F)$ . Now let $$\mathcal{N}(x,y,\underline{v}) =  \left( {\begin{array}{cc} 		\nabla F(x,y)\cdot \underline{v}  \\ 		\underline{v} \cdot \underline{v} - 1 \\ \end{array} } \right) $$ I want to use implicit differentiation theorem to find $v = v(x,y)$ , so I differentiate $\mathcal{N}$ with respect to $v$ $$\nabla_{\underline{v}}{\mathcal{N}} =  \left( {\begin{array}{cc} 		\nabla F(x,y)  \\ 		2\underline{v} \\ \end{array} } \right) $$ by definition if $\mathcal{N}(x,y,\underline{v}) = 0$ then the two vectors are orthogonal and they are both non-zero thus $\nabla_{\underline{v}}{\mathcal{N}}$ is invertible and $$\nabla_{x,y}{\underline{v}} = (\nabla_{\underline{v}}{\mathcal{N}})^{-1}( \nabla_{x,y}{\mathcal{N}})$$ but I have that $$\nabla_{x,y}{\mathcal{N}} = 	\left( {\begin{array}{cc} 		\nabla^2F(x,y) \cdot v  \\ 		\underline{0} \\ \end{array} }  \right) = \left( {\begin{array}{cc} 		\underline{0}  \\ 		\underline{0} \\ \end{array} }  \right) $$ and here I'm using $\underline{v} \in \ker{\nabla F} \subset \ker{\nabla^2 F}$ thus $\nabla_{x,y}{\underline{v}} = 0$ . Now fix $\underline{w}$ so that it has norm $1$ and $\nabla F(0,0) \cdot \underline{w} = 0$ , Let $U := \{ (x,y) \in \mathbb{R}^2 \; : \; \mathcal{N}(x,y,w) = 0\}$ .\ The set is nonempty since $(0,0) \in U$ , and by what we have shown the set is open, therefore it's $U =\mathbb{R}^2$ , but this shows that $\nabla F(x,y) \cdot w = 0$ for all $(x,y)$ and this concludes the proof.","Let be a strictly positive function such that are both convex. Show that there is a function and a vector such that I have found a solution to this exercise but I'm not satisfied because it uses the extra assumption that and is never , I would like to find an easier solution that doesn't rely on these extra assumptions. This is my solution : I will assume and is always non-zero. First a bit of notation : I use the euclidean metric on thus I identify matrices with biliniear forms, if is a ( ) matrix by I mean by I mean , or equivalently I claim that if is a symmetric matrix and is any matrix such that then To show it observe that, since is symmetric, up to a rotation I can assume it is diagonal,so there are so that then if and I find which clearly implies so which shows the claim Now back to the exercise I assume , thus and so convexity implies , . I calculate from this I easily find so using the previous claim but clearly . Now let I want to use implicit differentiation theorem to find , so I differentiate with respect to by definition if then the two vectors are orthogonal and they are both non-zero thus is invertible and but I have that and here I'm using thus . Now fix so that it has norm and , Let .\ The set is nonempty since , and by what we have shown the set is open, therefore it's , but this shows that for all and this concludes the proof.","F \; : \; \mathbb{R}^2 \to \mathbb{R} F, \frac{1}{F} g \; : \; \mathbb{R} \to \mathbb{R} v \in \mathbb{R}^2 F(x) = g(v\cdot x) \;\; \text{ for all } x \in \mathbb{R}^2 F \in C^2 \nabla F \underline{0} F \in C^2 \nabla F \mathbb{R}^2 A 2 \times 2 A \geq 0 (Ax,x) \geq 0 \;\; \forall x \in \mathbb{R}^2 A \leq B B - A \geq 0 (Ax,x) \leq (Bx,x) \;\; \forall x \in \mathbb{R}^2 A B 0 \leq A \leq B \ker(B) \subset \ker(A) A \lambda,\mu \geq 0 
		A=
		\left( {\begin{array}{cc}
				\lambda & 0 \\
				0 & \mu \\
		\end{array} } \right)
		 B v = 0 v = (v_1,v_2) 0 \leq \lambda (v_1)^2 + \mu(v_2)^2 \leq 0 \lambda v_1 = \mu v_2 = 0 A v = 0 F \in C^2 1/F \in C^2 \nabla^2 F \geq 0 \nabla^2 1/F \geq 0 \nabla^2 \frac{1}{F} \begin{equation}\nabla^2 \frac{1}{F} = \frac{2}{F^3}\nabla F \otimes \nabla F - \frac{1}{F^2}\nabla^2 F \geq 0\end{equation} \begin{equation} 0 \leq \nabla^2 F \leq \frac{2}{F} (\nabla F \otimes \nabla F) \end{equation} \ker(\nabla F \otimes \nabla F) \subset \ker(\nabla^2 F) \ker(\nabla F) \subset \ker(\nabla F \otimes \nabla F) \mathcal{N}(x,y,\underline{v}) = 
\left( {\begin{array}{cc}
		\nabla F(x,y)\cdot \underline{v}  \\
		\underline{v} \cdot \underline{v} - 1 \\
\end{array} } \right)  v = v(x,y) \mathcal{N} v \nabla_{\underline{v}}{\mathcal{N}} = 
\left( {\begin{array}{cc}
		\nabla F(x,y)  \\
		2\underline{v} \\
\end{array} } \right)  \mathcal{N}(x,y,\underline{v}) = 0 \nabla_{\underline{v}}{\mathcal{N}} \nabla_{x,y}{\underline{v}} = (\nabla_{\underline{v}}{\mathcal{N}})^{-1}( \nabla_{x,y}{\mathcal{N}}) \nabla_{x,y}{\mathcal{N}} = 	\left( {\begin{array}{cc}
		\nabla^2F(x,y) \cdot v  \\
		\underline{0} \\
\end{array} }  \right) = \left( {\begin{array}{cc}
		\underline{0}  \\
		\underline{0} \\
\end{array} }  \right)  \underline{v} \in \ker{\nabla F} \subset \ker{\nabla^2 F} \nabla_{x,y}{\underline{v}} = 0 \underline{w} 1 \nabla F(0,0) \cdot \underline{w} = 0 U := \{ (x,y) \in \mathbb{R}^2 \; : \; \mathcal{N}(x,y,w) = 0\} (0,0) \in U U =\mathbb{R}^2 \nabla F(x,y) \cdot w = 0 (x,y)","['linear-algebra', 'analysis', 'solution-verification', 'convex-analysis']"
97,How to construct abstractly and symmetrically the map $^{n+k}M^kN^nM$ given a pairing $MNR$?,How to construct abstractly and symmetrically the map  given a pairing ?,^{n+k}M^kN^nM MNR,"Let $R$ be a commutative ring and let $M,N$ be $R$ -modules. Let $\langle-,-\rangle$ be a pairing $MNR$ . We can construct a pairing $\Lambda^kM\Lambda^kNR$ by sending $(a_1a_k)(b_1b_k)$ to $\det(\langle a_i, b_j \rangle)_{ij}$ . We can more generally construct a map $\Lambda^{n+k} M  \Lambda^n N  \Lambda^k M$ that can be defined recursively by $$ \langle a_1a_{n+k}, b_1b_k \rangle = \sum_{i=1}^{n+k} (-1)^{i-1} \langle a_i,b_1\rangle \langle a_1\widehat{a_i}a_{n+k}, b_2b_k \rangle $$ where $\widehat{a_i}$ means that $a_i$ is omitted. The result is a sum over all injections $f : [\![1,k]\!]  [\![1,n+k]\!]$ of $$ \bigg(\prod_{i=1}^k \langle a_{f(i)}, b_i \rangle \bigg) \bigwedge_{j \not\in \operatorname{image}(f)} a_j.$$ Bourbaki gives an abstract construction of these maps as follows, but it is not symmetrical and I don't know how to prove the symmetry of the definition without referring to the formula. Here is the construction of Bourbaki (Algbre III): We start by defining a coalgebra structure on $\Lambda M$ by applying $\Lambda$ to the coalgebra structure on $M$ given by the diagonal $MMM$ and the zero map $M0$ . We get a comultiplication $\Lambda M  \Lambda(MM) = \Lambda M  \Lambda M$ and a counit $\Lambda M0=R$ . Dualizing degreewise this structure, we get an algebra structure on the graded dual $$(\Lambda M)' = \bigoplus_{k=0}^ (\Lambda^k M)^*.$$ This is an anticommutative alternating graded algebra (which can be deduced from the fact that $\Lambda M  \Lambda (MM)$ is a morphism of algebras, without computation), so by the universal property of $\Lambda (M^*)$ , the morphism $M^*  (\Lambda M)'$ gives an algebra morphism $\Lambda (M^*)  (\Lambda M)'$ and thus an action on $\Lambda M$ . So this construction gives the pairing $\Lambda^k M  \Lambda^k M^*  R$ and if we have a morphism $NM^*$ , we can deduce the pairing $\Lambda^k M  \Lambda^k N  \Lambda^k M  \Lambda^k M^*  R$ . Is there an other abstract construction that is more symmetrical than Bourbaki's? PS: I include the tag ""category theory"" because perhaps people watching this tag know an answer.","Let be a commutative ring and let be -modules. Let be a pairing . We can construct a pairing by sending to . We can more generally construct a map that can be defined recursively by where means that is omitted. The result is a sum over all injections of Bourbaki gives an abstract construction of these maps as follows, but it is not symmetrical and I don't know how to prove the symmetry of the definition without referring to the formula. Here is the construction of Bourbaki (Algbre III): We start by defining a coalgebra structure on by applying to the coalgebra structure on given by the diagonal and the zero map . We get a comultiplication and a counit . Dualizing degreewise this structure, we get an algebra structure on the graded dual This is an anticommutative alternating graded algebra (which can be deduced from the fact that is a morphism of algebras, without computation), so by the universal property of , the morphism gives an algebra morphism and thus an action on . So this construction gives the pairing and if we have a morphism , we can deduce the pairing . Is there an other abstract construction that is more symmetrical than Bourbaki's? PS: I include the tag ""category theory"" because perhaps people watching this tag know an answer.","R M,N R \langle-,-\rangle MNR \Lambda^kM\Lambda^kNR (a_1a_k)(b_1b_k) \det(\langle a_i, b_j \rangle)_{ij} \Lambda^{n+k} M  \Lambda^n N  \Lambda^k M  \langle a_1a_{n+k}, b_1b_k \rangle = \sum_{i=1}^{n+k} (-1)^{i-1} \langle a_i,b_1\rangle \langle a_1\widehat{a_i}a_{n+k}, b_2b_k \rangle  \widehat{a_i} a_i f : [\![1,k]\!]  [\![1,n+k]\!]  \bigg(\prod_{i=1}^k \langle a_{f(i)}, b_i \rangle \bigg) \bigwedge_{j \not\in \operatorname{image}(f)} a_j. \Lambda M \Lambda M MMM M0 \Lambda M  \Lambda(MM) = \Lambda M  \Lambda M \Lambda M0=R (\Lambda M)' = \bigoplus_{k=0}^ (\Lambda^k M)^*. \Lambda M  \Lambda (MM) \Lambda (M^*) M^*  (\Lambda M)' \Lambda (M^*)  (\Lambda M)' \Lambda M \Lambda^k M  \Lambda^k M^*  R NM^* \Lambda^k M  \Lambda^k N  \Lambda^k M  \Lambda^k M^*  R","['linear-algebra', 'abstract-algebra', 'category-theory', 'tensor-products', 'multilinear-algebra']"
98,Eigenvalue bound for quadratic maximization with linear constraint,Eigenvalue bound for quadratic maximization with linear constraint,,"This builds on my earlier questions here and here . Let $B$ be a symmetric positive definite matrix in $\mathbb{R}^{k\times k}$ and consider the problem $$\begin{array}{ll} \text{maximize} & x^\top B x\\ \text{subject to} & \|x\|=1 \\ & b^\top x = a\end{array}$$ where $b$ is an arbitrary unit vector and $a > 0$ is a small positive number. Let $$\lambda_1 > \lambda_2 \geq \cdots \geq \lambda_k > 0$$ be the eigenvalues of $B$ with corresponding eigenvectors $z_1,...,z_k$ . I conjecture that the optimal value of the problem is bounded below by $a^2 \lambda_1 + \left(1-a^2\right)\lambda_2$ , at least if $a$ is small enough. To motivate this conjecture, let us consider two special cases. First, suppose that $a= 0$ . Then, as was explained to me in one of my previous posts, the optimal value is between $\lambda_1$ and $\lambda_2$ by the Courant-Fischer theorem. Thus, $\lambda_2$ is a lower bound, and it also coincides with my conjectured lower bound in this special case. Second, let $a > 0$ but suppose that $b = z_i$ for some $i = 1,...,k$ . Any feasible $x$ can be written as $$x = ab + \sqrt{1-a^2} \cdot \hat{b}$$ where $\hat{b}\perp b$ . If $b = z_1$ , I can take $\hat{b} = z_2$ , and if $b = z_i$ for $i \neq 1$ , I can take $\hat{b} = z_1$ . Either way, the objective value of $x$ is bounded below by $a^2 \lambda_1 + \left(1-a^2\right)\lambda_2$ as long as $a$ is small enough (note that this requires $\lambda_1 > \lambda_2$ ). The difficulty is showing that it holds in the case where $b$ is not one of the eigenvectors of $B$ (perhaps with additional restrictions on how large $a$ can be). My intuition is that, if $b$ is not required to be orthogonal to $x$ , but only ""almost"" orthogonal (meaning that $a$ may be required to be sufficiently small), you should be able to go a bit further in the direction of the principal eigenvector than in the case where $a = 0$ . Here is the most up-to-date work on this problem. In the answer below, it was found that the optimal value $v$ of the problem is a generalized eigenvalue of the system $$PBx = vPx,$$ which in turn was derived from the system $$PBPy + aPBb = v Py.$$ Any pair $\left(y,v\right)$ that solves these equations then leads to a feasible $x = ab+Py$ , with $v$ being the objective value. We can write $$\left(vI - PB\right)Py = aPBb.$$ Note that, for any $v$ that is not an eigenvalue of $PB$ , the matrix $vI-PB$ is invertible, whence $$Py = a\left(vI-PB\right)^{-1}PBb.$$ The normalization $x^\top x = 1$ then becomes $y^\top P y = 1-a^2$ , leading to the equation $$\frac{1-a^2}{a^2} = b^\top BP\left(vI-PB\right)^{-2} PBb.$$ The largest root of this equation is the optimal value of the problem. Perhaps, as suggested, it can be found numerically.","This builds on my earlier questions here and here . Let be a symmetric positive definite matrix in and consider the problem where is an arbitrary unit vector and is a small positive number. Let be the eigenvalues of with corresponding eigenvectors . I conjecture that the optimal value of the problem is bounded below by , at least if is small enough. To motivate this conjecture, let us consider two special cases. First, suppose that . Then, as was explained to me in one of my previous posts, the optimal value is between and by the Courant-Fischer theorem. Thus, is a lower bound, and it also coincides with my conjectured lower bound in this special case. Second, let but suppose that for some . Any feasible can be written as where . If , I can take , and if for , I can take . Either way, the objective value of is bounded below by as long as is small enough (note that this requires ). The difficulty is showing that it holds in the case where is not one of the eigenvectors of (perhaps with additional restrictions on how large can be). My intuition is that, if is not required to be orthogonal to , but only ""almost"" orthogonal (meaning that may be required to be sufficiently small), you should be able to go a bit further in the direction of the principal eigenvector than in the case where . Here is the most up-to-date work on this problem. In the answer below, it was found that the optimal value of the problem is a generalized eigenvalue of the system which in turn was derived from the system Any pair that solves these equations then leads to a feasible , with being the objective value. We can write Note that, for any that is not an eigenvalue of , the matrix is invertible, whence The normalization then becomes , leading to the equation The largest root of this equation is the optimal value of the problem. Perhaps, as suggested, it can be found numerically.","B \mathbb{R}^{k\times k} \begin{array}{ll} \text{maximize} & x^\top B x\\ \text{subject to} & \|x\|=1 \\ & b^\top x = a\end{array} b a > 0 \lambda_1 > \lambda_2 \geq \cdots \geq \lambda_k > 0 B z_1,...,z_k a^2 \lambda_1 + \left(1-a^2\right)\lambda_2 a a= 0 \lambda_1 \lambda_2 \lambda_2 a > 0 b = z_i i = 1,...,k x x = ab + \sqrt{1-a^2} \cdot \hat{b} \hat{b}\perp b b = z_1 \hat{b} = z_2 b = z_i i \neq 1 \hat{b} = z_1 x a^2 \lambda_1 + \left(1-a^2\right)\lambda_2 a \lambda_1 > \lambda_2 b B a b x a a = 0 v PBx = vPx, PBPy + aPBb = v Py. \left(y,v\right) x = ab+Py v \left(vI - PB\right)Py = aPBb. v PB vI-PB Py = a\left(vI-PB\right)^{-1}PBb. x^\top x = 1 y^\top P y = 1-a^2 \frac{1-a^2}{a^2} = b^\top BP\left(vI-PB\right)^{-2} PBb.","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'positive-definite', 'qcqp']"
99,"Dimension of a Subspace of $\text{Hom}_\mathbb{K}(\mathcal{V},\mathcal{W})$ Consisting of Only Linear Transformations of Rank $\leq r$",Dimension of a Subspace of  Consisting of Only Linear Transformations of Rank,"\text{Hom}_\mathbb{K}(\mathcal{V},\mathcal{W}) \leq r","Background. I have a conjecture (stated in two ways below), which I would like to see whether it is true (both the inequality part and the part involving the equality cases).  The motivation comes from this thread .  The conjecture (both the inequality part and the equality cases) is known to be true when the base field $\mathbb{K}$ has the property that, for any positive integer $l$ , the only solution $(x_1,x_2,\ldots,x_l)\in\mathbb{K}^l$ to $$x_1^2+x_2^2+\ldots+x_l^2=0$$ is $x_1=x_2=\ldots=x_l=0$ .  Subfields of $\mathbb{R}$ are examples, but there are other fields with this property such as pure transcendental extensions of $\mathbb{R}$ and ordered fields.  (They are all fields of characteristic $0$ , by the way.) Conjecture (Version I). Let $\mathbb{K}$ be a field.  Let $\mathcal{V}$ and $\mathcal{W}$ be two finite-dimensional vector spaces over $\mathbb{K}$ with dimensions $n$ and $m$ , respectively.  For an integer $r$ such that $$0\leq r\leq \min\{m,n\}\,,$$ suppose that $\mathcal{T}$ is a subspace of the $\mathbb{K}$ -vector space $\text{Hom}_{\mathbb{K}}(\mathcal{V},\mathcal{W})$ such that every linear transformation $\varphi:\mathcal{V}\to \mathcal{W}$ in $\mathcal{T}$ is of rank at most $r$ .  Then, $$\dim_\mathbb{K}(\mathcal{T})\leq r\,\max\{m,n\}\,.$$ Here are the equality cases. (a) For $m< n$ , there exists an $r$ -dimensional subspace $\mathcal{I}$ of $\mathcal{W}$ such that $\mathcal{T}$ consists of linear transformations $\varphi$ such that $\text{im}(\varphi)\subseteq \mathcal{I}$ . (b) For $m>n$ , there exists an $(n-r)$ -dimensional subspace $\mathcal{K}$ of $\mathcal{V}$ such that $\mathcal{T}$ consists of linear transformations $\varphi$ such that $\ker(\varphi)\supseteq \mathcal{K}$ . (c) For $m=n$ , there are two possibilities. There exists an $r$ -dimensional subspace $\mathcal{I}$ of $\mathcal{W}$ such that $\mathcal{T}$ consists of linear transformations $\varphi$ such that $\text{im}(\varphi)\subseteq \mathcal{I}$ . There exists an $(n-r)$ -dimensional subspace $\mathcal{K}$ of $\mathcal{V}$ such that $\mathcal{T}$ consists of linear transformations $\varphi$ such that $\ker(\varphi)\supseteq \mathcal{K}$ . Conjecture (Version II). Let $\mathbb{K}$ be a field.  For integers $m,n>0$ and $r$ such that $$0\leq r\leq \min\{m,n\}\,,$$ suppose that $\mathcal{M}$ is a $\mathbb{K}$ -vector subspace of the $\mathbb{K}$ -vector space $\text{Mat}_{m\times n}(\mathbb{K})$ such that each matrix in $\mathcal{M}$ is of rank at most $r$ .  Then, $$\dim_\mathbb{K}(\mathcal{M})\leq r\,\max\{m,n\}\,.$$ Here are the equality cases. (a) For $m< n$ , there exists an $r$ -dimensional subspace $\mathcal{C}$ of $\mathbb{K}^m$ such that $\mathcal{M}$ consists of matrices whose column spaces are contained in $\mathcal{C}$ . (b) For $m> n$ , there exists an $r$ -dimensional subspace $\mathcal{R}$ of $\mathbb{K}^n$ such that $\mathcal{M}$ consists of matrices whose row spaces are contained in $\mathcal{R}$ . (c) For $m=n$ , there are two possibilites. There exists an $r$ -dimensional subspace $\mathcal{C}$ of $\mathbb{K}^m$ such that $\mathcal{M}$ consists of matrices whose column spaces are contained in $\mathcal{C}$ . There exists an $r$ -dimensional subspace $\mathcal{R}$ of $\mathbb{K}^n$ such that $\mathcal{M}$ consists of matrices whose row spaces are contained in $\mathcal{R}$ . Warning. While I am quite confident that the inequality is true, I am not very sure that I got the equality cases right.  If there are counterexamples, I would love to see. Remark. I just noticed that joriki posted this reference under the question here .  I have not yet digested it, but it seems says that the inequality holds when $m=n$ for any field $\mathbb{K}$ .  Maybe, the proof can be modified to allow $m\neq n$ . There does not seem to be any mention of the equality cases in this paper. Theorem 3 of the paper gives the equality condition. Edit. The conjecture (both the inequality part and the equality cases) is true when $m=n$ .  Now, we have to find out what to do when $m\neq n$ .  By the way, using duality or transpose, we may assume that $m<n$ .  (Perhaps, we can even allow $\mathbb{K}$ to be a division ring.)","Background. I have a conjecture (stated in two ways below), which I would like to see whether it is true (both the inequality part and the part involving the equality cases).  The motivation comes from this thread .  The conjecture (both the inequality part and the equality cases) is known to be true when the base field has the property that, for any positive integer , the only solution to is .  Subfields of are examples, but there are other fields with this property such as pure transcendental extensions of and ordered fields.  (They are all fields of characteristic , by the way.) Conjecture (Version I). Let be a field.  Let and be two finite-dimensional vector spaces over with dimensions and , respectively.  For an integer such that suppose that is a subspace of the -vector space such that every linear transformation in is of rank at most .  Then, Here are the equality cases. (a) For , there exists an -dimensional subspace of such that consists of linear transformations such that . (b) For , there exists an -dimensional subspace of such that consists of linear transformations such that . (c) For , there are two possibilities. There exists an -dimensional subspace of such that consists of linear transformations such that . There exists an -dimensional subspace of such that consists of linear transformations such that . Conjecture (Version II). Let be a field.  For integers and such that suppose that is a -vector subspace of the -vector space such that each matrix in is of rank at most .  Then, Here are the equality cases. (a) For , there exists an -dimensional subspace of such that consists of matrices whose column spaces are contained in . (b) For , there exists an -dimensional subspace of such that consists of matrices whose row spaces are contained in . (c) For , there are two possibilites. There exists an -dimensional subspace of such that consists of matrices whose column spaces are contained in . There exists an -dimensional subspace of such that consists of matrices whose row spaces are contained in . Warning. While I am quite confident that the inequality is true, I am not very sure that I got the equality cases right.  If there are counterexamples, I would love to see. Remark. I just noticed that joriki posted this reference under the question here .  I have not yet digested it, but it seems says that the inequality holds when for any field .  Maybe, the proof can be modified to allow . There does not seem to be any mention of the equality cases in this paper. Theorem 3 of the paper gives the equality condition. Edit. The conjecture (both the inequality part and the equality cases) is true when .  Now, we have to find out what to do when .  By the way, using duality or transpose, we may assume that .  (Perhaps, we can even allow to be a division ring.)","\mathbb{K} l (x_1,x_2,\ldots,x_l)\in\mathbb{K}^l x_1^2+x_2^2+\ldots+x_l^2=0 x_1=x_2=\ldots=x_l=0 \mathbb{R} \mathbb{R} 0 \mathbb{K} \mathcal{V} \mathcal{W} \mathbb{K} n m r 0\leq r\leq \min\{m,n\}\,, \mathcal{T} \mathbb{K} \text{Hom}_{\mathbb{K}}(\mathcal{V},\mathcal{W}) \varphi:\mathcal{V}\to \mathcal{W} \mathcal{T} r \dim_\mathbb{K}(\mathcal{T})\leq r\,\max\{m,n\}\,. m< n r \mathcal{I} \mathcal{W} \mathcal{T} \varphi \text{im}(\varphi)\subseteq \mathcal{I} m>n (n-r) \mathcal{K} \mathcal{V} \mathcal{T} \varphi \ker(\varphi)\supseteq \mathcal{K} m=n r \mathcal{I} \mathcal{W} \mathcal{T} \varphi \text{im}(\varphi)\subseteq \mathcal{I} (n-r) \mathcal{K} \mathcal{V} \mathcal{T} \varphi \ker(\varphi)\supseteq \mathcal{K} \mathbb{K} m,n>0 r 0\leq r\leq \min\{m,n\}\,, \mathcal{M} \mathbb{K} \mathbb{K} \text{Mat}_{m\times n}(\mathbb{K}) \mathcal{M} r \dim_\mathbb{K}(\mathcal{M})\leq r\,\max\{m,n\}\,. m< n r \mathcal{C} \mathbb{K}^m \mathcal{M} \mathcal{C} m> n r \mathcal{R} \mathbb{K}^n \mathcal{M} \mathcal{R} m=n r \mathcal{C} \mathbb{K}^m \mathcal{M} \mathcal{C} r \mathcal{R} \mathbb{K}^n \mathcal{M} \mathcal{R} m=n \mathbb{K} m\neq n m=n m\neq n m<n \mathbb{K}","['linear-algebra', 'matrices', 'vector-spaces', 'linear-transformations', 'matrix-rank']"
