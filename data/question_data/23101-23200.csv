,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Are vector spaces and their double duals in fact equal?,Are vector spaces and their double duals in fact equal?,,"Excuse me, in a course of linear algebra, our assistant stated that, if $\mathbb{V}$ is a finite-dimensional vector space, and $\mathbb{W}$ its double dual, $\mathbb{V}$ and $\mathbb{W}$ are actually equal to each other; I am wondering if this has anything to do with the viewpoint in algebraic number theory that realizes elements, in algebraic number fields, as functions? In any case, thank you very much.","Excuse me, in a course of linear algebra, our assistant stated that, if $\mathbb{V}$ is a finite-dimensional vector space, and $\mathbb{W}$ its double dual, $\mathbb{V}$ and $\mathbb{W}$ are actually equal to each other; I am wondering if this has anything to do with the viewpoint in algebraic number theory that realizes elements, in algebraic number fields, as functions? In any case, thank you very much.",,['linear-algebra']
1,Pivoting in LU decomposition,Pivoting in LU decomposition,,I tried to find some reference on the net but couldn't find a good one. What is the advantage of pivoting in LU decomposition over regular LU decomposition? Is it something to do with matrix singular or not?,I tried to find some reference on the net but couldn't find a good one. What is the advantage of pivoting in LU decomposition over regular LU decomposition? Is it something to do with matrix singular or not?,,"['linear-algebra', 'matrices']"
2,Are there non-commuting matrices for which $\mathrm{tr}(ABC)=\mathrm{tr}(BAC)$?,Are there non-commuting matrices for which ?,\mathrm{tr}(ABC)=\mathrm{tr}(BAC),"For linear operators $A,B$ acting on $\mathbb{C}^n$ the fact that $\text{tr}(AB)=\text{tr}(BA)$ follows from the cyclic property of the trace, which says more generally that if $A,B,C$ are linear operators acting on $\mathbb{C}^n$ we have that $$\text{tr}(ABC)=\text{tr}(CAB)=\text{tr}(BCA)=\text{tr}(ABC).$$ My question is whether there are specific conditions on $A$ , $B$ , and $C$ such that $$\text{tr}(ABC)=\text{tr}(BAC)?$$ Clearly, its true if $A$ and $B$ commute, but its also sufficient for either $A$ or $B$ to commute with $C$ , but are these necessary for this property to hold? I should mention that I am primarily interested in the case that $A,B,C$ are invertible, since clearly if $A,B,C$ have zero divisors the property can be satisfied in a trivial sort of way.","For linear operators acting on the fact that follows from the cyclic property of the trace, which says more generally that if are linear operators acting on we have that My question is whether there are specific conditions on , , and such that Clearly, its true if and commute, but its also sufficient for either or to commute with , but are these necessary for this property to hold? I should mention that I am primarily interested in the case that are invertible, since clearly if have zero divisors the property can be satisfied in a trivial sort of way.","A,B \mathbb{C}^n \text{tr}(AB)=\text{tr}(BA) A,B,C \mathbb{C}^n \text{tr}(ABC)=\text{tr}(CAB)=\text{tr}(BCA)=\text{tr}(ABC). A B C \text{tr}(ABC)=\text{tr}(BAC)? A B A B C A,B,C A,B,C","['linear-algebra', 'matrices', 'trace']"
3,Trace of a Matrix Product.,Trace of a Matrix Product.,,"Let $A \in \mathbb{R}^{n\times m}$ , and $B\in \mathbb{R}^{m\times m}$ . Let $A'$ denote the transpose of $A$ . From this we know that : $A'\in \mathbb{R}^{m\times n}$ and $AB\in \mathbb{R}^{n\times m}$ and hence $ABA' \in \mathbb{R}^{n\times n}$ . The Trace of a matrix is defined as the sum of its diagonal entries. $\textbf{My Question :}$ Does anyone know bounds (using any usual matrix norm) for $\text{Trace}(ABA')$ Or more generally $\text{Trace}(AB)$","Let , and . Let denote the transpose of . From this we know that : and and hence . The Trace of a matrix is defined as the sum of its diagonal entries. Does anyone know bounds (using any usual matrix norm) for Or more generally",A \in \mathbb{R}^{n\times m} B\in \mathbb{R}^{m\times m} A' A A'\in \mathbb{R}^{m\times n} AB\in \mathbb{R}^{n\times m} ABA' \in \mathbb{R}^{n\times n} \textbf{My Question :} \text{Trace}(ABA') \text{Trace}(AB),"['linear-algebra', 'matrices', 'analysis', 'vectors', 'trace']"
4,"If A commutes with both of these matrices, then A must be a scalar multiple of the identity matrix","If A commutes with both of these matrices, then A must be a scalar multiple of the identity matrix",,"I am working on the following problem: Let $A$ be a $4 \times 4$ matrix with entries in a field of characteristic zero. Suppose that $A$ commutes with both $\begin{pmatrix} 1 & 0 & 0 & 0\\ 0 & 2 & 0 & 0\\ 0 & 0 & 3 & 0\\ 0 & 0 & 0 & 4 \end{pmatrix}$ and $\begin{pmatrix} 0 & 0 & 0 & 1\\ 1 & 0 & 0 & 0\\ 0 & 1 & 0 & 0\\ 0 & 0 & 1 & 0 \end{pmatrix}$ . Prove that $A$ is a scalar multiple of the identity matrix. I know that $A$ is a scalar multiple of the identity matrix if and only if $AB = BA$ for all other possible $4 \times 4$ matrices $B$ with entries in a field of characteristic $0$ . However, I'm struggling with deducing here that $A$ commuting with these specific matrices forces $A$ to be a scalar multiple of the identity matrix. Does commuting with these specific matrices force $A$ to commute with all $4 \times 4$ matrices with entries in a field of characteristic $0$ ? If so, how can I deduce this? Thanks!","I am working on the following problem: Let be a matrix with entries in a field of characteristic zero. Suppose that commutes with both and . Prove that is a scalar multiple of the identity matrix. I know that is a scalar multiple of the identity matrix if and only if for all other possible matrices with entries in a field of characteristic . However, I'm struggling with deducing here that commuting with these specific matrices forces to be a scalar multiple of the identity matrix. Does commuting with these specific matrices force to commute with all matrices with entries in a field of characteristic ? If so, how can I deduce this? Thanks!",A 4 \times 4 A \begin{pmatrix} 1 & 0 & 0 & 0\\ 0 & 2 & 0 & 0\\ 0 & 0 & 3 & 0\\ 0 & 0 & 0 & 4 \end{pmatrix} \begin{pmatrix} 0 & 0 & 0 & 1\\ 1 & 0 & 0 & 0\\ 0 & 1 & 0 & 0\\ 0 & 0 & 1 & 0 \end{pmatrix} A A AB = BA 4 \times 4 B 0 A A A 4 \times 4 0,"['linear-algebra', 'matrices']"
5,proving that $a + b \sqrt {2} + c \sqrt{3} + d \sqrt{6} $ is a subfield of $\mathbb{R}$,proving that  is a subfield of,a + b \sqrt {2} + c \sqrt{3} + d \sqrt{6}  \mathbb{R},"The question is given below: My questions are: 1- How can I find the general form of the multiplicative inverse of each element? 2-How can I find the multiplicative identity? 3-Is the only difference between the field and the subfield definition is that in the case of a subfield every nonzero element has an additive identity but in the field every element not only nonzero ones? Could anyone help me in understanding these questions, please? EDIT: I have found this solution on the internet: My question : is that a fully acceptable answer to the question? I guess yes.","The question is given below: My questions are: 1- How can I find the general form of the multiplicative inverse of each element? 2-How can I find the multiplicative identity? 3-Is the only difference between the field and the subfield definition is that in the case of a subfield every nonzero element has an additive identity but in the field every element not only nonzero ones? Could anyone help me in understanding these questions, please? EDIT: I have found this solution on the internet: My question : is that a fully acceptable answer to the question? I guess yes.",,"['linear-algebra', 'abstract-algebra']"
6,Why does a $2×3$ matrix multiplied by a vector in $\Bbb{R}^3$ give a vector in $\Bbb{R}^2$?,Why does a  matrix multiplied by a vector in  give a vector in ?,2×3 \Bbb{R}^3 \Bbb{R}^2,"I'm so confused on how we can have a 2x3 matrix A, multiply it by a vector in $\Bbb R^3$ and then end up with a vector in $\Bbb R^2$ . Is it possible to visualize this at all or do I need to sort of blindly accept this concept as facts that I'll accept and use?  Can someone give a very brief summarization on why this makes sense? Because I just see it as, in a world (dimension) in $\Bbb R^3$ , we multiply it by a vector in $\Bbb R^3$ , and out pops a vector in $\Bbb R^2$ . Thanks!","I'm so confused on how we can have a 2x3 matrix A, multiply it by a vector in and then end up with a vector in . Is it possible to visualize this at all or do I need to sort of blindly accept this concept as facts that I'll accept and use?  Can someone give a very brief summarization on why this makes sense? Because I just see it as, in a world (dimension) in , we multiply it by a vector in , and out pops a vector in . Thanks!",\Bbb R^3 \Bbb R^2 \Bbb R^3 \Bbb R^3 \Bbb R^2,"['linear-algebra', 'matrices', 'vector-spaces', 'vectors']"
7,Atypical way to find angle between unit vectors: $\theta = 2 \sin^{-1}\left(\frac{1}{2}\left\|\hat{A}-\hat{B}\right\|\right)$,Atypical way to find angle between unit vectors:,\theta = 2 \sin^{-1}\left(\frac{1}{2}\left\|\hat{A}-\hat{B}\right\|\right),"At my work, I have come across code with the following way of calculating the angle between two vectors. $$\theta = 2 \sin^{-1}\left(\frac{1}{2}\left\|\hat{A}-\hat{B} \right\|\right)$$ (Note the physics convention: $\hat{v}$ indicates the normalization of $v$ ; ie, $\hat{v}:=v/\|v\|$ ). I've spent some time, but I can't think of how this was derived using typical methodologies (law of sines, law of cosines, dot product, cross product). It is pretty different. So my questions is, How could this have been derived?","At my work, I have come across code with the following way of calculating the angle between two vectors. (Note the physics convention: indicates the normalization of ; ie, ). I've spent some time, but I can't think of how this was derived using typical methodologies (law of sines, law of cosines, dot product, cross product). It is pretty different. So my questions is, How could this have been derived?",\theta = 2 \sin^{-1}\left(\frac{1}{2}\left\|\hat{A}-\hat{B} \right\|\right) \hat{v} v \hat{v}:=v/\|v\|,"['linear-algebra', 'trigonometry', 'vectors']"
8,"If two rotation matrices commute, do their infinitesimal generators commute too?","If two rotation matrices commute, do their infinitesimal generators commute too?",,"Suppose that $e^A$ and $e^B$ are two rotations in $\mathrm{SO}(n)$ . If $e^{A}e^{B} = e^{B}e^{A}$ , can we conclude that $e^{A+B}=e^Ae^B$ ? More importantly, can we say that $AB=BA$ ? I'm particularly interested in the cases when $n=2,3,4$ because I was working on a physics problem that this question was brought up. $n=3$ is the most important case for me.","Suppose that and are two rotations in . If , can we conclude that ? More importantly, can we say that ? I'm particularly interested in the cases when because I was working on a physics problem that this question was brought up. is the most important case for me.","e^A e^B \mathrm{SO}(n) e^{A}e^{B} = e^{B}e^{A} e^{A+B}=e^Ae^B AB=BA n=2,3,4 n=3","['linear-algebra', 'lie-groups', 'lie-algebras', 'rotations', 'matrix-exponential']"
9,Find locus of points by finding eigenvalues,Find locus of points by finding eigenvalues,,"Let $\boldsymbol{x}=\left(\begin{matrix}x\\ y\end{matrix}\right)$ be a vector in two-dimensional real space. By finding the eigenvalues and eigenvectors of $\boldsymbol{M}$ , sketch the locus of points $\boldsymbol{x}$ that satisfy $$ \boldsymbol{x^TMx}=4$$ given that $$\boldsymbol{M}=\left(\begin{matrix}&5 &\sqrt{3}\\ &\sqrt{3} &3\end{matrix}\right). $$ I found two eigenvalues to be $\lambda_1 = 6$ and $\lambda_2=2$ , and the corresponding eigenvectors are $$ \boldsymbol{v}_1=\left(\begin{matrix}\sqrt{3}\\ 1\end{matrix}\right)\quad\text{ and }\quad \boldsymbol{v}_2=\left(\begin{matrix}1\\ -\sqrt{3}\end{matrix}\right)$$ (if I'm not mistaken :) ), but... what now? Frankly, I can't figure out how to make this helpful to find $\boldsymbol{x^TMx}=4$ . Any hints?","Let be a vector in two-dimensional real space. By finding the eigenvalues and eigenvectors of , sketch the locus of points that satisfy given that I found two eigenvalues to be and , and the corresponding eigenvectors are (if I'm not mistaken :) ), but... what now? Frankly, I can't figure out how to make this helpful to find . Any hints?",\boldsymbol{x}=\left(\begin{matrix}x\\ y\end{matrix}\right) \boldsymbol{M} \boldsymbol{x}  \boldsymbol{x^TMx}=4 \boldsymbol{M}=\left(\begin{matrix}&5 &\sqrt{3}\\ &\sqrt{3} &3\end{matrix}\right).  \lambda_1 = 6 \lambda_2=2  \boldsymbol{v}_1=\left(\begin{matrix}\sqrt{3}\\ 1\end{matrix}\right)\quad\text{ and }\quad \boldsymbol{v}_2=\left(\begin{matrix}1\\ -\sqrt{3}\end{matrix}\right) \boldsymbol{x^TMx}=4,['linear-algebra']
10,Necessary and sufficient condition for diagonalizable matrix,Necessary and sufficient condition for diagonalizable matrix,,"Let $A\in M_n(\mathbb R)$ and $B=\begin{pmatrix}A & A\\0 & A\end{pmatrix}$. Prove that $B$ is diagonalizable if and only $A = 0$. We see that for any polynomial $P$, $P(B) = \begin{pmatrix}P(A) & AP'(A)\\0 & P(A)\end{pmatrix}$ where $P'$ is the derivative of P $B$ is diagonalizable $\iff \exists P$ a polynomial with single roots which annihilates B Then B diagonalizable if and only if A diagonalizable because $P(A)=0$.  A annihilates the polynomial $XP'$ too. From there, I don't see how to prove that $A=0$","Let $A\in M_n(\mathbb R)$ and $B=\begin{pmatrix}A & A\\0 & A\end{pmatrix}$. Prove that $B$ is diagonalizable if and only $A = 0$. We see that for any polynomial $P$, $P(B) = \begin{pmatrix}P(A) & AP'(A)\\0 & P(A)\end{pmatrix}$ where $P'$ is the derivative of P $B$ is diagonalizable $\iff \exists P$ a polynomial with single roots which annihilates B Then B diagonalizable if and only if A diagonalizable because $P(A)=0$.  A annihilates the polynomial $XP'$ too. From there, I don't see how to prove that $A=0$",,"['linear-algebra', 'matrices', 'diagonalization', 'block-matrices']"
11,British Maths Olympiad (BMO) 2004 Round 1 Question 1 alternative approaches?,British Maths Olympiad (BMO) 2004 Round 1 Question 1 alternative approaches?,,"The questions states: Solve the simultaneous equations (which I respectively label as $ > \ref{1}, \ref{2}, \ref{3}, \ref{4}$) $$\begin{align} ab + c + d &= 3 \tag{1} \label{1} \\ bc + d + a &= 5  \tag{2} \label{2} \\  cd + a + b &= 2 \tag{3} \label{3} \\  da + b + c  &= 6 \tag{4} \label{4} \end{align}$$ where $a,b,c,d$ are real numbers. I solved this system after quite a while by taking $eqns$ 1 - 3 = $eqns$ 4 - 2 which yields $a + c = 2$ You can then substitute that in and find the other variables I also noticed that $(a+1)(b+1) + (a+1)(d+1) + (c+1)(b+1) + (c+1)(d+1) = 20$ but that line didnt really help me. I'm interested in seeing the other approaches people can take with this system. Additionally, is there a sufficient enough hint to take another route? Did I miss an easy solution?","The questions states: Solve the simultaneous equations (which I respectively label as $ > \ref{1}, \ref{2}, \ref{3}, \ref{4}$) $$\begin{align} ab + c + d &= 3 \tag{1} \label{1} \\ bc + d + a &= 5  \tag{2} \label{2} \\  cd + a + b &= 2 \tag{3} \label{3} \\  da + b + c  &= 6 \tag{4} \label{4} \end{align}$$ where $a,b,c,d$ are real numbers. I solved this system after quite a while by taking $eqns$ 1 - 3 = $eqns$ 4 - 2 which yields $a + c = 2$ You can then substitute that in and find the other variables I also noticed that $(a+1)(b+1) + (a+1)(d+1) + (c+1)(b+1) + (c+1)(d+1) = 20$ but that line didnt really help me. I'm interested in seeing the other approaches people can take with this system. Additionally, is there a sufficient enough hint to take another route? Did I miss an easy solution?",,"['linear-algebra', 'algebra-precalculus', 'contest-math', 'systems-of-equations']"
12,"Is $\operatorname{tr}\left(\int A(t)\,dt\right) = \int \operatorname{tr}(A(t))\,dt$ true?",Is  true?,"\operatorname{tr}\left(\int A(t)\,dt\right) = \int \operatorname{tr}(A(t))\,dt","Is it true that the trace of a matrix integration equals the integration of the trace of such matrix? $$\operatorname{tr}\left(\int A(t)\,dt\right) = \int \operatorname{tr}(A(t))\,dt$$","Is it true that the trace of a matrix integration equals the integration of the trace of such matrix? $$\operatorname{tr}\left(\int A(t)\,dt\right) = \int \operatorname{tr}(A(t))\,dt$$",,"['linear-algebra', 'ordinary-differential-equations']"
13,How to find Jordan normal form and basis?,How to find Jordan normal form and basis?,,"I have a matrix: $$A = \begin{pmatrix}-1&5&4\\ 4&-6&-6\\ -8&16&14 \end{pmatrix}$$ simplifying: $$A = \begin{pmatrix}-1&5&4\\ 2&-3&-3\\ -4&8&7 \end{pmatrix}$$ What steps should I reproduce to find Jordan form and Jordan basis, I am really really confuswd without good step by step example! What I really understand that I have to find eigenvalues, so I did: $$\det|A-I\lambda| = \begin{vmatrix}-1 - \lambda&5&4\\ 2&-3-\lambda&-3\\ -4&8&7-\lambda \end{vmatrix}  $$ so I found cubic equation which is: $$-\lambda^3+3\lambda^2-5\lambda+3 = 0$$ $$-(\lambda-1)^3-2(\lambda-1)$$ So as I can judge Jordan normal form will have three eigenvalues that equal to $\lambda = 1$ on its main diagonal, below main diagonal all the values should be ""$0$"". But I bet it is not all. I do not know how to proceed (If I am right at all), and do not know how to find Jordan's basis after :(","I have a matrix: $$A = \begin{pmatrix}-1&5&4\\ 4&-6&-6\\ -8&16&14 \end{pmatrix}$$ simplifying: $$A = \begin{pmatrix}-1&5&4\\ 2&-3&-3\\ -4&8&7 \end{pmatrix}$$ What steps should I reproduce to find Jordan form and Jordan basis, I am really really confuswd without good step by step example! What I really understand that I have to find eigenvalues, so I did: $$\det|A-I\lambda| = \begin{vmatrix}-1 - \lambda&5&4\\ 2&-3-\lambda&-3\\ -4&8&7-\lambda \end{vmatrix}  $$ so I found cubic equation which is: $$-\lambda^3+3\lambda^2-5\lambda+3 = 0$$ $$-(\lambda-1)^3-2(\lambda-1)$$ So as I can judge Jordan normal form will have three eigenvalues that equal to $\lambda = 1$ on its main diagonal, below main diagonal all the values should be ""$0$"". But I bet it is not all. I do not know how to proceed (If I am right at all), and do not know how to find Jordan's basis after :(",,['linear-algebra']
14,Closed linear operators vs Continuous linear operators,Closed linear operators vs Continuous linear operators,,"Suppose we have two real Banach spaces $X, Y$, and a linear operator $A:X \rightarrow Y$. We say that $A$ is closed if whenever $u_k \rightarrow u$ in $X$ and $A u_k \rightarrow v$ in $Y$, then $Au = v$. This definition is very reminiscent of the sequential criterion for continuity of real-valued functions. I assume that there are operators that are closed but not continuous, because otherwise, there'd be no point in having a different word. If my assumption is correct, is this because operators behave differently when we consider more general spaces, or because there is some subtle difference between this definition and the sequential criterion that I'm missing? Also, could someone provide an example of such a function?","Suppose we have two real Banach spaces $X, Y$, and a linear operator $A:X \rightarrow Y$. We say that $A$ is closed if whenever $u_k \rightarrow u$ in $X$ and $A u_k \rightarrow v$ in $Y$, then $Au = v$. This definition is very reminiscent of the sequential criterion for continuity of real-valued functions. I assume that there are operators that are closed but not continuous, because otherwise, there'd be no point in having a different word. If my assumption is correct, is this because operators behave differently when we consider more general spaces, or because there is some subtle difference between this definition and the sequential criterion that I'm missing? Also, could someone provide an example of such a function?",,"['linear-algebra', 'analysis', 'continuity', 'banach-spaces']"
15,"Given any two non zero vectors $x,y$ does there exist symmetric matrix $A$ such that $y = Ax$?",Given any two non zero vectors  does there exist symmetric matrix  such that ?,"x,y A y = Ax","Let $x,y$ be non-zero vectors from $\mathbb{R}^n$. Is it true, that there exists symmetric matrix $A$ such that $y = Ax$. I was reasoning the following way. Having an equation $y = Ax$ for some symmetric matrix $A$ is equivalent to being able to solve a system of $n + \frac{n^2-n}{2}$ linear equations: $$\begin{cases} a_{11}x_1 + ... + a_{1n}x_n = y_1 \\ \vdots \\ a_{n1}x_1 + ... + a_{nn}x_n = y_n \\ a_{12} = a_{21} \\ \vdots \\ a_{n,n-1} = a_{n-1,n} \end{cases}$$ First $n$ equations are our constraints for identity $y = Ax$ to be true and the following $\frac{n^2-n}{2}$ for symmetricity of $A$. So, in our system we have $n^2$ variables and $\frac{n^2-n}{2} + n = \frac{n^2+n}{2}$ equations. As $n^2 \geq \frac{n^2+n}{2}$ such system is always solvable. That's why such matrix $A$ always exists. This solution was marked as wrong on exam, where did I make a mistake?","Let $x,y$ be non-zero vectors from $\mathbb{R}^n$. Is it true, that there exists symmetric matrix $A$ such that $y = Ax$. I was reasoning the following way. Having an equation $y = Ax$ for some symmetric matrix $A$ is equivalent to being able to solve a system of $n + \frac{n^2-n}{2}$ linear equations: $$\begin{cases} a_{11}x_1 + ... + a_{1n}x_n = y_1 \\ \vdots \\ a_{n1}x_1 + ... + a_{nn}x_n = y_n \\ a_{12} = a_{21} \\ \vdots \\ a_{n,n-1} = a_{n-1,n} \end{cases}$$ First $n$ equations are our constraints for identity $y = Ax$ to be true and the following $\frac{n^2-n}{2}$ for symmetricity of $A$. So, in our system we have $n^2$ variables and $\frac{n^2-n}{2} + n = \frac{n^2+n}{2}$ equations. As $n^2 \geq \frac{n^2+n}{2}$ such system is always solvable. That's why such matrix $A$ always exists. This solution was marked as wrong on exam, where did I make a mistake?",,"['linear-algebra', 'matrices', 'proof-verification']"
16,What is the mechanism of Eigenvector? [closed],What is the mechanism of Eigenvector? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question I have studied EigenValues and EigenVectors but still what I can't see is that how EigenVectors become transformed or rotated vectors.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question I have studied EigenValues and EigenVectors but still what I can't see is that how EigenVectors become transformed or rotated vectors.",,"['linear-algebra', 'linear-transformations', 'vector-analysis']"
17,Multiplication of two upper triangular matrices,Multiplication of two upper triangular matrices,,"Consider the product $C=(C_{ij})$ of two upper triangular matrices $A=(a_{ij})$ and $B=(b_{ij})$, with $n=m$ (rows=columns). Deduce the expression of $C=(C_{ij})$. I'm trying to do this proof but can't complete it.","Consider the product $C=(C_{ij})$ of two upper triangular matrices $A=(a_{ij})$ and $B=(b_{ij})$, with $n=m$ (rows=columns). Deduce the expression of $C=(C_{ij})$. I'm trying to do this proof but can't complete it.",,"['linear-algebra', 'matrices']"
18,Why a complex symmetric matrix is not diagonalizible?,Why a complex symmetric matrix is not diagonalizible?,,"I know an Hermitian matrix is diagonalizable, and similarly a real symmetric matrix is diagonalizable, but what's wrong in a complex symmetric matrix. Why does the Gram-Schmidt process fail?","I know an Hermitian matrix is diagonalizable, and similarly a real symmetric matrix is diagonalizable, but what's wrong in a complex symmetric matrix. Why does the Gram-Schmidt process fail?",,"['linear-algebra', 'matrices']"
19,Postive-semidefiniteness of matrix with entries $1/(a_i+a_j)$,Postive-semidefiniteness of matrix with entries,1/(a_i+a_j),"Let $a_1, \ldots, a_n$ be a set of positive numbers. Define a matrix $M_{ij} = \frac{1}{a_i+a_j}$. I'm trying to prove that $M$ is positive-semidefinite. The hint says to use the fact that $\int_{0}^{\infty} e^{-sx}\; dx = \frac{1}{s}$ if $s > 0$. However I don't know how this hint is useful. I've tried choosing an arbitrary vector $x$ and substituting $x^{\intercal}Mx = \sum_{i}\sum_{j} \frac{x_ix_j}{a_i+a_j}$ into $s$ and using properties of exponents to simplify the equation into something that is clearly positive, but without any luck. The denominator $\frac{1}{a_i+a_j}$ is simply too difficult to work with. At this point I think I'm just missing some trick that I don't know. Any help would be appreciated.","Let $a_1, \ldots, a_n$ be a set of positive numbers. Define a matrix $M_{ij} = \frac{1}{a_i+a_j}$. I'm trying to prove that $M$ is positive-semidefinite. The hint says to use the fact that $\int_{0}^{\infty} e^{-sx}\; dx = \frac{1}{s}$ if $s > 0$. However I don't know how this hint is useful. I've tried choosing an arbitrary vector $x$ and substituting $x^{\intercal}Mx = \sum_{i}\sum_{j} \frac{x_ix_j}{a_i+a_j}$ into $s$ and using properties of exponents to simplify the equation into something that is clearly positive, but without any luck. The denominator $\frac{1}{a_i+a_j}$ is simply too difficult to work with. At this point I think I'm just missing some trick that I don't know. Any help would be appreciated.",,"['linear-algebra', 'positive-definite']"
20,how to find null space basis directly by matrix calculation,how to find null space basis directly by matrix calculation,,"The problem of finding the basis for the null space of an $m \times n$ matrix $A$ is a well-known problem of linear algebra. We solve $Ax=0$ by Gaussian elimination. Either the solution is unique and $x=0$ is the only solution, or, there are infinitely many solutions which can be parametrized by the non-pivotal variables. Traditionally, my advice has been to calculate $\text{rref}(A)$ then read from that the dependence of pivotal on non-pivotal variables. Next, I put those linear dependencies into $x = (x_1, \dots , x_n)$ and if $x_{i_1}, \dots , x_{i_k}$ are the non-pivotal variables we can write: $$ x = x_{i_1}v_1+ \cdots + x_{i_k}v_k \qquad \star$$ where $v_1, \dots, v_k$ are linearly independent solutions of $Ax=0$. In fact, $\text{Null}(A) = \text{span}\{ v_1, \dots, v_k \}$ and $k = \text{nullity}(A) = \text{dim}(\text{Null}(a))$. In contrast, to read off the basis of the column space I need only calculate $\text{rref}(A)$ to identify the pivot columns ( I suppose $\text{ref}(A)$ or less might suffice for this task). Then by the column correspondence property it follows that the pivot columns of $A$ serve as a basis for the column space of $A$. My question is this: What is the nice way to calculate the basis for the null space of $A$ without need for non-matrix calculation? In particular, I'd like an algorithm where the basis for $\text{Null}(A)$ appears explicitly. I'd like avoid the step I outline at $\star$. When I took graduate linear algebra the professor gave a handout which explained how to do this, but, I'd like a more standard reference. I'm primarily interested in the characteristic zero case, but, I would be delighted by a more general answer. Thanks in advance for your insight. The ideal answer outlines the method and points to a standard reference on this calculation.","The problem of finding the basis for the null space of an $m \times n$ matrix $A$ is a well-known problem of linear algebra. We solve $Ax=0$ by Gaussian elimination. Either the solution is unique and $x=0$ is the only solution, or, there are infinitely many solutions which can be parametrized by the non-pivotal variables. Traditionally, my advice has been to calculate $\text{rref}(A)$ then read from that the dependence of pivotal on non-pivotal variables. Next, I put those linear dependencies into $x = (x_1, \dots , x_n)$ and if $x_{i_1}, \dots , x_{i_k}$ are the non-pivotal variables we can write: $$ x = x_{i_1}v_1+ \cdots + x_{i_k}v_k \qquad \star$$ where $v_1, \dots, v_k$ are linearly independent solutions of $Ax=0$. In fact, $\text{Null}(A) = \text{span}\{ v_1, \dots, v_k \}$ and $k = \text{nullity}(A) = \text{dim}(\text{Null}(a))$. In contrast, to read off the basis of the column space I need only calculate $\text{rref}(A)$ to identify the pivot columns ( I suppose $\text{ref}(A)$ or less might suffice for this task). Then by the column correspondence property it follows that the pivot columns of $A$ serve as a basis for the column space of $A$. My question is this: What is the nice way to calculate the basis for the null space of $A$ without need for non-matrix calculation? In particular, I'd like an algorithm where the basis for $\text{Null}(A)$ appears explicitly. I'd like avoid the step I outline at $\star$. When I took graduate linear algebra the professor gave a handout which explained how to do this, but, I'd like a more standard reference. I'm primarily interested in the characteristic zero case, but, I would be delighted by a more general answer. Thanks in advance for your insight. The ideal answer outlines the method and points to a standard reference on this calculation.",,"['linear-algebra', 'matrices', 'reference-request', 'gaussian-elimination']"
21,What does 'vanishing' mean in the context of Linear Algebra?,What does 'vanishing' mean in the context of Linear Algebra?,,"https://en.wikipedia.org/wiki/Wronskian The line under 'Wronskian and linear independence' is what I'm talking about. When you take the determinant of a matrix with linearly dependent vectors, doesn't it make the determinant $0$? Is going to $0$ what they are talking about?","https://en.wikipedia.org/wiki/Wronskian The line under 'Wronskian and linear independence' is what I'm talking about. When you take the determinant of a matrix with linearly dependent vectors, doesn't it make the determinant $0$? Is going to $0$ what they are talking about?",,"['linear-algebra', 'ordinary-differential-equations', 'terminology']"
22,"Does the inner product $\langle \cdot, \cdot \rangle$ induce any other norms other than the 2 norm?",Does the inner product  induce any other norms other than the 2 norm?,"\langle \cdot, \cdot \rangle","In the lecture my professor wrote that the standard inner product on $R^n$ is given by $\langle x, y \rangle = x^Ty = \sum\limits_{i=1}^n x_i y_i$ which induces a norm $\sqrt{\langle x,x \rangle} = \|x\|_2$ My question is do inner products induce other types of norms...or rather are norms such as the 1-norm or the $\infty$-norm induces by some inner product?","In the lecture my professor wrote that the standard inner product on $R^n$ is given by $\langle x, y \rangle = x^Ty = \sum\limits_{i=1}^n x_i y_i$ which induces a norm $\sqrt{\langle x,x \rangle} = \|x\|_2$ My question is do inner products induce other types of norms...or rather are norms such as the 1-norm or the $\infty$-norm induces by some inner product?",,"['linear-algebra', 'vector-spaces', 'normed-spaces', 'inner-products']"
23,Showing when a permutation matrix is diagonizable over $\mathbb R$ and over $\mathbb C$,Showing when a permutation matrix is diagonizable over  and over,\mathbb R \mathbb C,"For a permutation $\sigma$ of the set $\{1,...,n\}$, and consider the $n \times n$ matrix $A_\sigma$, where the $i^{\text{th}}$ column is the standard vector $e_{\sigma (i)}$.  For which $\sigma$ is $A_\sigma$ digonizable over $\mathbb{C}$, and which values for $\mathbb{R}$? I don't exactly know what I should do.  I think I should use the fact that a map is diagnizable iff the characteristic polynomial of t factors in linear factors over the field I'm in, and if $m_g(\lambda)=m_a(\lambda)$ for all eigenvalues If anyone could start me up or help me out or do one of the examples it would be appreciated, thanks! Small edit: I don't even know what the matrix looks like, maybe it's just me but I don't find it's clear","For a permutation $\sigma$ of the set $\{1,...,n\}$, and consider the $n \times n$ matrix $A_\sigma$, where the $i^{\text{th}}$ column is the standard vector $e_{\sigma (i)}$.  For which $\sigma$ is $A_\sigma$ digonizable over $\mathbb{C}$, and which values for $\mathbb{R}$? I don't exactly know what I should do.  I think I should use the fact that a map is diagnizable iff the characteristic polynomial of t factors in linear factors over the field I'm in, and if $m_g(\lambda)=m_a(\lambda)$ for all eigenvalues If anyone could start me up or help me out or do one of the examples it would be appreciated, thanks! Small edit: I don't even know what the matrix looks like, maybe it's just me but I don't find it's clear",,"['linear-algebra', 'matrices', 'diagonalization']"
24,Find the determinant of the following matrix,Find the determinant of the following matrix,,"Find the determinant of the following matrix: $$A = \begin{bmatrix} 1+x_1^2 &x_1x_2  & ... & x_1x_n \\   x_2x_1&1+x_2^2  &...  & x_2x_n\\   ...& ... & ... &... \\   x_nx_1& x_nx_2  &...  & 1+x_n^2 \end{bmatrix}$$ I computed for the case $n=2$, and $n=3$ and guessed that $\det(A)$ should be $ 1+\sum_{i=1}^n x_i^2 $  but not sure how to proceed for any $n$.","Find the determinant of the following matrix: $$A = \begin{bmatrix} 1+x_1^2 &x_1x_2  & ... & x_1x_n \\   x_2x_1&1+x_2^2  &...  & x_2x_n\\   ...& ... & ... &... \\   x_nx_1& x_nx_2  &...  & 1+x_n^2 \end{bmatrix}$$ I computed for the case $n=2$, and $n=3$ and guessed that $\det(A)$ should be $ 1+\sum_{i=1}^n x_i^2 $  but not sure how to proceed for any $n$.",,"['linear-algebra', 'matrices', 'determinant']"
25,Finding a value from 5 systems of equations of 5 variables(CHMMC 2014),Finding a value from 5 systems of equations of 5 variables(CHMMC 2014),,"$$\text{For } a_1\cdots a_5\in \mathbb{R},$$ $$\frac{a_1}{k^2+1}+\cdots+\frac{a_5}{k^2+5}=\frac{1}{k^2}$$ $$\forall k=\{2,3,4,5,6\}$$ $$\text{Find }\frac{a_1}2+\cdots+\frac{a_5}6$$ The Provided explanation/official solution was $$\text{Solution }1:\large{\frac{65}{72}}$$ Please don't mark this as off-topic, I have no clue where to start. Clearly it would not be feasible to compute $a_1\cdots a_5$.","$$\text{For } a_1\cdots a_5\in \mathbb{R},$$ $$\frac{a_1}{k^2+1}+\cdots+\frac{a_5}{k^2+5}=\frac{1}{k^2}$$ $$\forall k=\{2,3,4,5,6\}$$ $$\text{Find }\frac{a_1}2+\cdots+\frac{a_5}6$$ The Provided explanation/official solution was $$\text{Solution }1:\large{\frac{65}{72}}$$ Please don't mark this as off-topic, I have no clue where to start. Clearly it would not be feasible to compute $a_1\cdots a_5$.",,"['linear-algebra', 'contest-math', 'systems-of-equations']"
26,"Find an equation of the plane spanned by $v_1$ and $v_2$, find a vector $v_3$ that can be added to produce a basis for $\mathbb{R}^3$","Find an equation of the plane spanned by  and , find a vector  that can be added to produce a basis for",v_1 v_2 v_3 \mathbb{R}^3,"Let $v_1=(-1,2,3)$ and $v_2=(5,3,-1)$. Find the equation of the plane spanned by $v_1$ and $v_2$. Also find a vector $v_3$ that can be added to the set $\{v_1,v_2\}$ to produce a basis for $\mathbb{R}^3$. I'm stuck on both parts because everywhere I read it says I need a point in addition to these two vectors to find the equation. For the second part I'm not sure of the method to find a vector to produce a basis, we haven't been told how to do that... I know a basis for $\mathbb{R}^3$ would have to have $3$ vectors that span $\mathbb{R}^3$ and must be L.I., so I know that $kv_1 + kv_2 + kv_3 = 0$ must be $k_1=k_2=k_3 = 0$ ...but what is a concrete method to find a third vector $v_3$? Forgive me if I missed somewhere I could have found this out, I couldn't seem to find the method.","Let $v_1=(-1,2,3)$ and $v_2=(5,3,-1)$. Find the equation of the plane spanned by $v_1$ and $v_2$. Also find a vector $v_3$ that can be added to the set $\{v_1,v_2\}$ to produce a basis for $\mathbb{R}^3$. I'm stuck on both parts because everywhere I read it says I need a point in addition to these two vectors to find the equation. For the second part I'm not sure of the method to find a vector to produce a basis, we haven't been told how to do that... I know a basis for $\mathbb{R}^3$ would have to have $3$ vectors that span $\mathbb{R}^3$ and must be L.I., so I know that $kv_1 + kv_2 + kv_3 = 0$ must be $k_1=k_2=k_3 = 0$ ...but what is a concrete method to find a third vector $v_3$? Forgive me if I missed somewhere I could have found this out, I couldn't seem to find the method.",,"['linear-algebra', 'vector-spaces']"
27,Unique solution comes out to be trace,Unique solution comes out to be trace,,"Let $f: \mathcal{M}_n(F)\to F$ be a linear functional satisfying $f(\mathbf{AB})=f(\mathbf{BA})\;\forall \mathbf{A,B}\in \mathcal{M}_n(F)$. Also it satisfies $f(\mathbf{I})=n$. Prove that $f(\mathbf{A})=\operatorname{tr}(\mathbf{A})$. Can someone tell me how to prove this? I have no idea about how to start it. My first idea was to showing this for the canonical basis but I failed. Can someone help me? Thanks.","Let $f: \mathcal{M}_n(F)\to F$ be a linear functional satisfying $f(\mathbf{AB})=f(\mathbf{BA})\;\forall \mathbf{A,B}\in \mathcal{M}_n(F)$. Also it satisfies $f(\mathbf{I})=n$. Prove that $f(\mathbf{A})=\operatorname{tr}(\mathbf{A})$. Can someone tell me how to prove this? I have no idea about how to start it. My first idea was to showing this for the canonical basis but I failed. Can someone help me? Thanks.",,['linear-algebra']
28,"If $A^n = I$, $n$ odd, $A$ a square integer matrix, does $A = I$?","If ,  odd,  a square integer matrix, does ?",A^n = I n A A = I,"Edit: Crap, even my hypothesis was wrong. If you put $A = \left[ \begin{array}{cc} 1&-1\\3&-2 \end{array} \right]$, then $A^3 = I$ but no eigenvalue is $1$. (What's true is that all eigenvalues are $n$th roots of unity.) I ask this because I believe it might resolve a trickier problem I'm working on in a simple way. Obviously this is false if $n$ is even, but I have no counterexample if $n$ is odd. Indeed, if $x$ is an eigenvector of $A$, $$A^nx = \lambda^nx = Ix = x \implies \lambda = 1 $$ so $\lambda = 1$ is the only eigenvalue of the matrix. This says $\operatorname{trace}(A) = n$ and $p(\lambda) = (1 - \lambda)^n$. (Trivially, $\det(A) = 1$ from the multiplicativity of the determinant.) (Incidentally, I am aware that there is a subtlety here in assuming a real eigenvector exists, since the rotation-by-$90^{\circ}$ matrix satisfies $A^4 = I$ and has none; but it seems that the condition that $A^n = I$ for odd $n$ may take care of this problem.) How can we prove this, in a fairly simple way? (If the proof is complicated, I'm still game, but it's probably not the intended solution to my original problem.)","Edit: Crap, even my hypothesis was wrong. If you put $A = \left[ \begin{array}{cc} 1&-1\\3&-2 \end{array} \right]$, then $A^3 = I$ but no eigenvalue is $1$. (What's true is that all eigenvalues are $n$th roots of unity.) I ask this because I believe it might resolve a trickier problem I'm working on in a simple way. Obviously this is false if $n$ is even, but I have no counterexample if $n$ is odd. Indeed, if $x$ is an eigenvector of $A$, $$A^nx = \lambda^nx = Ix = x \implies \lambda = 1 $$ so $\lambda = 1$ is the only eigenvalue of the matrix. This says $\operatorname{trace}(A) = n$ and $p(\lambda) = (1 - \lambda)^n$. (Trivially, $\det(A) = 1$ from the multiplicativity of the determinant.) (Incidentally, I am aware that there is a subtlety here in assuming a real eigenvector exists, since the rotation-by-$90^{\circ}$ matrix satisfies $A^4 = I$ and has none; but it seems that the condition that $A^n = I$ for odd $n$ may take care of this problem.) How can we prove this, in a fairly simple way? (If the proof is complicated, I'm still game, but it's probably not the intended solution to my original problem.)",,"['linear-algebra', 'matrices', 'determinant']"
29,Non-symmetric matrix with orthogonal eigenvectors,Non-symmetric matrix with orthogonal eigenvectors,,"Given that a symmetric matrix with real entries has orthogonal eigenvectors, is the converse true? That is, if a matrix has orthogonal eigenvectors, does it have to be symmetrical and real?","Given that a symmetric matrix with real entries has orthogonal eigenvectors, is the converse true? That is, if a matrix has orthogonal eigenvectors, does it have to be symmetrical and real?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization']"
30,Square root of a Hermitian operator exists,Square root of a Hermitian operator exists,,"There are a lot of questions here about square root operators, but none of them addresses the basic question of existence, and I didn't find a very beefy section in Wikipedia talking about this, so I'll ask it here. Let $A$ be a bounded positive-semidefinite Hermitian operator on Hilbert space. The claim is that there exists a positive-semidefinite Hermitian operator $B$ such that $B\circ B=A$. I've got no idea how you would prove this. Any tips or references would be appreciated.","There are a lot of questions here about square root operators, but none of them addresses the basic question of existence, and I didn't find a very beefy section in Wikipedia talking about this, so I'll ask it here. Let $A$ be a bounded positive-semidefinite Hermitian operator on Hilbert space. The claim is that there exists a positive-semidefinite Hermitian operator $B$ such that $B\circ B=A$. I've got no idea how you would prove this. Any tips or references would be appreciated.",,"['linear-algebra', 'functional-analysis', 'operator-theory', 'hilbert-spaces']"
31,How to prove that $A^{-1} + B^{-1}$ is invertible given the conditions,How to prove that  is invertible given the conditions,A^{-1} + B^{-1},"If $A$ and $B$ be two invertible $n \times n$ real matrices and $A + B$ is invertible, how to prove that $A^{-1} + B^{-1}$ is also invertible?","If $A$ and $B$ be two invertible $n \times n$ real matrices and $A + B$ is invertible, how to prove that $A^{-1} + B^{-1}$ is also invertible?",,"['linear-algebra', 'matrices']"
32,Show that $Q_8$ can't be embedded in $M_{2 \times 2}(\mathbb{R})$ as a group.,Show that  can't be embedded in  as a group.,Q_8 M_{2 \times 2}(\mathbb{R}),"So, suppose that we're working in a field $F$. Consider the ring $M_{n \times n} (F)$ which is the set all $n \times n$ matrices with entries in $F$. Is it possible to determine whether a matrix polynomial equation has solutions? If yes, is it possible to find the solutions for low degree polynomials? My knowledge of mathematics is very limited as an undergraduate. I'm sure that there are many aspects of this question that are possibly beyond my knowledge at this level, but my interest in this question comes from this problem: Show that the group $Q_8 = \{ \pm 1, \pm i, \pm j,\pm k\}$ under multiplication defined on Quaternions can not be embedded in the group of invertible elements of $M_{2\times 2}(\mathbb{R})$ under matrix multiplication. So, because the cardinality of $M_{2 \times 2}(\mathbb{R})$ is infinite, I thought it might be very difficult to try group homomorphisms from $Q_8$ to $M_{2 \times 2}(\mathbb{R})$ and show that they can't have trivial kernel. Especially because if we replace $\mathbb{R}$ by $\mathbb{C}$ the problem becomes false. So, I thought that I should carefully study the number of solutions in each group and find some contradictions. For example, is it possible to show that the equation $A^4=I$ has less than $8$ solutions in $M_{2 \times 2}(\mathbb{R})$?","So, suppose that we're working in a field $F$. Consider the ring $M_{n \times n} (F)$ which is the set all $n \times n$ matrices with entries in $F$. Is it possible to determine whether a matrix polynomial equation has solutions? If yes, is it possible to find the solutions for low degree polynomials? My knowledge of mathematics is very limited as an undergraduate. I'm sure that there are many aspects of this question that are possibly beyond my knowledge at this level, but my interest in this question comes from this problem: Show that the group $Q_8 = \{ \pm 1, \pm i, \pm j,\pm k\}$ under multiplication defined on Quaternions can not be embedded in the group of invertible elements of $M_{2\times 2}(\mathbb{R})$ under matrix multiplication. So, because the cardinality of $M_{2 \times 2}(\mathbb{R})$ is infinite, I thought it might be very difficult to try group homomorphisms from $Q_8$ to $M_{2 \times 2}(\mathbb{R})$ and show that they can't have trivial kernel. Especially because if we replace $\mathbb{R}$ by $\mathbb{C}$ the problem becomes false. So, I thought that I should carefully study the number of solutions in each group and find some contradictions. For example, is it possible to show that the equation $A^4=I$ has less than $8$ solutions in $M_{2 \times 2}(\mathbb{R})$?",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'finite-groups', 'quaternions']"
33,Origin in vector space?,Origin in vector space?,,"In the wikipedia article about vector space I do not understand this sentence Roughly, affine spaces are vector spaces whose origin is not specified. A vector space does not need an origin. When one writes: $\vec{v} = \pmatrix{v_x\\v_y\\v_z} = \pmatrix{1\\-4\\7}$ it only needs a basis, not an origin. Am I wrong, or what am I misunderstanding ?","In the wikipedia article about vector space I do not understand this sentence Roughly, affine spaces are vector spaces whose origin is not specified. A vector space does not need an origin. When one writes: $\vec{v} = \pmatrix{v_x\\v_y\\v_z} = \pmatrix{1\\-4\\7}$ it only needs a basis, not an origin. Am I wrong, or what am I misunderstanding ?",,"['linear-algebra', 'vector-spaces', 'coordinate-systems', 'affine-geometry']"
34,Gram Determinant equals volume?,Gram Determinant equals volume?,,"I have been trying to solve this problem of finding the 'n-volume' of a paralleletope spanned by m vectors, where clearly m =< n. In general, for computational purposes, what I have managed to do is define volume as the product of absolute values of vectors obtained by gram-schmidt orthogonalizationn. (Makes sense right? That's the natural interpretation when we say volume) I had to do two things, firstly to show that this definition of volume is a well defined one (i.e. any set of orthogonal vectors obtained by the process will give the same volume), and secondly to find a quick way to do this. I managed to prove the first one by induction, but the second part is a little bit of a problem. I managed to obtain formulae for small dimensions as 2,3 or even 4 but this process is impractical for any bigger dimensions as the substitutions for smaller dimensions into the formula for the next dimension becomes exponentially complicated How does one prove that the gram determinant is equal to the volume of a paralleletope spanned by a set of vectors?","I have been trying to solve this problem of finding the 'n-volume' of a paralleletope spanned by m vectors, where clearly m =< n. In general, for computational purposes, what I have managed to do is define volume as the product of absolute values of vectors obtained by gram-schmidt orthogonalizationn. (Makes sense right? That's the natural interpretation when we say volume) I had to do two things, firstly to show that this definition of volume is a well defined one (i.e. any set of orthogonal vectors obtained by the process will give the same volume), and secondly to find a quick way to do this. I managed to prove the first one by induction, but the second part is a little bit of a problem. I managed to obtain formulae for small dimensions as 2,3 or even 4 but this process is impractical for any bigger dimensions as the substitutions for smaller dimensions into the formula for the next dimension becomes exponentially complicated How does one prove that the gram determinant is equal to the volume of a paralleletope spanned by a set of vectors?",,"['linear-algebra', 'combinatorics', 'determinant', 'volume']"
35,Find a basis for the vector space of symmetric matrices with an order of $n \times n$ [duplicate],Find a basis for the vector space of symmetric matrices with an order of  [duplicate],n \times n,"This question already has answers here : Describe a basis for the vector space of symmetric $n \times n$ matrices [closed] (2 answers) Closed 10 years ago . Find a basis for the vector space of symmetric matrices with an order of $n \times n$ This is my thought: by definition of symmetry, $a_{i,j}=a_{j,i}$. Therefore, the basis should consist ${n^2-n} \over 2$ matrices to determine each symmetric pair. In addition, it should also consist $n$ matrices to determine each term in the diagonal. Therefore, the dimension of the vector space is ${n^2+n} \over 2$. It's not hard to write down the above mathematically (in case it's true). Two questions: Am I right? Is that the desired basis? Is there a more efficent alternative to reprsent the basis? Thanks!","This question already has answers here : Describe a basis for the vector space of symmetric $n \times n$ matrices [closed] (2 answers) Closed 10 years ago . Find a basis for the vector space of symmetric matrices with an order of $n \times n$ This is my thought: by definition of symmetry, $a_{i,j}=a_{j,i}$. Therefore, the basis should consist ${n^2-n} \over 2$ matrices to determine each symmetric pair. In addition, it should also consist $n$ matrices to determine each term in the diagonal. Therefore, the dimension of the vector space is ${n^2+n} \over 2$. It's not hard to write down the above mathematically (in case it's true). Two questions: Am I right? Is that the desired basis? Is there a more efficent alternative to reprsent the basis? Thanks!",,"['linear-algebra', 'matrices', 'vector-spaces']"
36,How to express a matrix as a product of two symmetric matrices?,How to express a matrix as a product of two symmetric matrices?,,Let $A$ be a matrix and $J$ its Jordan canonical form. How can one express $A$ as a product of two symmetric matrices? I expressed $J$ as a product of two symmetric matrices: block by block in the following manner: $$\begin{bmatrix}\lambda&1 & \\ &\lambda&1 \\ & &\lambda \end{bmatrix} =\begin{bmatrix} & & 1\\ &1& \\1& & \end{bmatrix}\begin{bmatrix} & & \lambda\\ &\lambda&1 \\\lambda& 1& \end{bmatrix}$$ and then I was trying to make use of the fact that $A$ and $A^t$ are similar. Yet I got stuck. Any hints are hugely appreciated.,Let $A$ be a matrix and $J$ its Jordan canonical form. How can one express $A$ as a product of two symmetric matrices? I expressed $J$ as a product of two symmetric matrices: block by block in the following manner: $$\begin{bmatrix}\lambda&1 & \\ &\lambda&1 \\ & &\lambda \end{bmatrix} =\begin{bmatrix} & & 1\\ &1& \\1& & \end{bmatrix}\begin{bmatrix} & & \lambda\\ &\lambda&1 \\\lambda& 1& \end{bmatrix}$$ and then I was trying to make use of the fact that $A$ and $A^t$ are similar. Yet I got stuck. Any hints are hugely appreciated.,,"['linear-algebra', 'matrices', 'block-matrices', 'jordan-normal-form']"
37,"Given a linear map $T:V\to V$, is it true that $V=\ker(T) \oplus \mathrm{im}(T)$?","Given a linear map , is it true that ?",T:V\to V V=\ker(T) \oplus \mathrm{im}(T),I was wondering if $V=\ker(T) \oplus \mathrm{im}(T)$ if $T:V \to V$. I know the theorem that if $T:V\to W$ is linear then $\dim(V) = \dim(\ker(T)) + \dim(\mathrm{im}(T))$. This should imply $V=\ker(T) \oplus \mathrm{im}(T)$ because if $\dim(V) = \dim(\ker(T)) + \dim(\mathrm{im}(T))$ it mean that a basis of $\ker(T)$ plus a basis of $\mathrm{im}(T)$ forms a basis of $V$ which is the same like  $V=\ker(T) \oplus \mathrm{im}(T)$. Is it correct?,I was wondering if $V=\ker(T) \oplus \mathrm{im}(T)$ if $T:V \to V$. I know the theorem that if $T:V\to W$ is linear then $\dim(V) = \dim(\ker(T)) + \dim(\mathrm{im}(T))$. This should imply $V=\ker(T) \oplus \mathrm{im}(T)$ because if $\dim(V) = \dim(\ker(T)) + \dim(\mathrm{im}(T))$ it mean that a basis of $\ker(T)$ plus a basis of $\mathrm{im}(T)$ forms a basis of $V$ which is the same like  $V=\ker(T) \oplus \mathrm{im}(T)$. Is it correct?,,['linear-algebra']
38,How to find 3 x 3 matrix inverses,How to find 3 x 3 matrix inverses,,"Is there a way of finding the inverse of a $3 \times 3$ matrix without forming an augmented matrix with the identity matrix? Also, is there a quick way of checking that a  $3 \times 3$ matrix's inverse exists, without trying to compute it? Thanks.","Is there a way of finding the inverse of a $3 \times 3$ matrix without forming an augmented matrix with the identity matrix? Also, is there a quick way of checking that a  $3 \times 3$ matrix's inverse exists, without trying to compute it? Thanks.",,"['linear-algebra', 'matrices']"
39,What are some relationships between a matrix and its transpose?,What are some relationships between a matrix and its transpose?,,"All I can think of are that If symmetric, they're equivalent If A is orthogonal, then its transpose is equivalent to its inverse. They have the same rank and determinant. Is there any relationship between their images/kernels or even eigenvalues?","All I can think of are that If symmetric, they're equivalent If A is orthogonal, then its transpose is equivalent to its inverse. They have the same rank and determinant. Is there any relationship between their images/kernels or even eigenvalues?",,['linear-algebra']
40,Intersection of affine subspaces is affine,Intersection of affine subspaces is affine,,"If I have two affine subspaces, each is a translation (or coset) of some linear subspace.  I want to show that the intersection of such affine subspaces is also affine, particularly in $\mathbb{R}^d$ .  My intuition suggests that the resulting space is just a coset of the intersection of the two linear subspaces, but I'm having some trouble arguing this precisely.","If I have two affine subspaces, each is a translation (or coset) of some linear subspace.  I want to show that the intersection of such affine subspaces is also affine, particularly in .  My intuition suggests that the resulting space is just a coset of the intersection of the two linear subspaces, but I'm having some trouble arguing this precisely.",\mathbb{R}^d,"['linear-algebra', 'affine-geometry']"
41,"Eigenvalues of operator $p(T)$ in terms of the eigenvalues of $T$, where $p$ is a polynomial","Eigenvalues of operator  in terms of the eigenvalues of , where  is a polynomial",p(T) T p,"Let $T$ be a linear operator on a finite dimensional vector space over an algebraically closed field $F.$ Let $f$ be a polynomial over $F.$ Prove that $c$ is a characteristic value of $f(T)$ iff $c=f(t)$, where $t$ is a characteristic value of $T.$ I have proved that $c$ is a characteristic value of $f(T)$, where $c=f(t)$ and $t$ is a characteristic value of $T$, but I cannot show rigorously that all characteristic values of $f(T)$ are of the form $f(t)$ where $t$ is the characteristic value of $T$.By intuition I know it is so $K$ is an algebraically closed field and because if the space $V$ has dimension $n$,then $T$ has $n$ eigenvalues (counted with multiplicity) and $f(T)$ also has also $n$ eigenvalues (counted with multiplicity). So, since for all $n$ eigenvalues $t$ of $T$, I have $n$ eigenvalues $f(t)$ of $f(T)$ then I exhaust the eigenvalues of $f(T)$ but I feel that this way of proving is not rigourous.","Let $T$ be a linear operator on a finite dimensional vector space over an algebraically closed field $F.$ Let $f$ be a polynomial over $F.$ Prove that $c$ is a characteristic value of $f(T)$ iff $c=f(t)$, where $t$ is a characteristic value of $T.$ I have proved that $c$ is a characteristic value of $f(T)$, where $c=f(t)$ and $t$ is a characteristic value of $T$, but I cannot show rigorously that all characteristic values of $f(T)$ are of the form $f(t)$ where $t$ is the characteristic value of $T$.By intuition I know it is so $K$ is an algebraically closed field and because if the space $V$ has dimension $n$,then $T$ has $n$ eigenvalues (counted with multiplicity) and $f(T)$ also has also $n$ eigenvalues (counted with multiplicity). So, since for all $n$ eigenvalues $t$ of $T$, I have $n$ eigenvalues $f(t)$ of $f(T)$ then I exhaust the eigenvalues of $f(T)$ but I feel that this way of proving is not rigourous.",,"['linear-algebra', 'polynomials', 'eigenvalues-eigenvectors']"
42,Factorize a Symmetric matrix as an 'Approximation' with an outer product.,Factorize a Symmetric matrix as an 'Approximation' with an outer product.,,"(deprecated-taken back based on discussion(OLD)) What is a good way to factor a symmetric matrix $X$ as an outer product of two vectors $u$ and $v$. i.e, Find two vectors $u$ and $v$ such that $X=uv^T$, where $X$ is a symmetric matrix. (Updated/ New Question of interest(NEW)) Given a symmetric matrix $X$, what is a way to figure out the best possible vectors $u$ and $v$ such that the error under say an l2 loss over $X-uv^T$ is minimum.  Feel free to  make notes about any optimality conditions/ assumptions that might go around this problem.","(deprecated-taken back based on discussion(OLD)) What is a good way to factor a symmetric matrix $X$ as an outer product of two vectors $u$ and $v$. i.e, Find two vectors $u$ and $v$ such that $X=uv^T$, where $X$ is a symmetric matrix. (Updated/ New Question of interest(NEW)) Given a symmetric matrix $X$, what is a way to figure out the best possible vectors $u$ and $v$ such that the error under say an l2 loss over $X-uv^T$ is minimum.  Feel free to  make notes about any optimality conditions/ assumptions that might go around this problem.",,['linear-algebra']
43,What are the rules for basic algebra when modulo real numbers are involved,What are the rules for basic algebra when modulo real numbers are involved,,"That is, real numbers modulo an integer. I'm just interested in shuffling around the $+$ , $-$ , $*$ , and $/$ operations. In case a concrete example helps, here's my current problem. (I'm from a programming background so there's probably a notation disconnect, sorry about that.) $$ LI(x) = (LI_0 + x / PI) \bmod 1 $$ $$ LF(x) = (LF_0 + x / PF) \bmod 1 $$ $$ LI(s) + 0.5 = LF(f) $$ $$ f = s + PT / 2 $$ I need to find a solution for $s$ , given $LI_0$ , $LF_0$ , $PI$ , $PF$ and $PT$ . Also I think I might have a solution by dropping the "" $\bmod 1$ ""s, solving for $s$ and then modding that by: $$\frac{1}{ \left| \frac{1}{PI} - \frac{1}{PF} \right| }$$ But I can't tell if that actually works because it introduces an enormous rounding error. Also, while this is the problem at hand and solving it is my immediate goal, I really want to understand how to generate that solution, for next time.","That is, real numbers modulo an integer. I'm just interested in shuffling around the , , , and operations. In case a concrete example helps, here's my current problem. (I'm from a programming background so there's probably a notation disconnect, sorry about that.) I need to find a solution for , given , , , and . Also I think I might have a solution by dropping the "" ""s, solving for and then modding that by: But I can't tell if that actually works because it introduces an enormous rounding error. Also, while this is the problem at hand and solving it is my immediate goal, I really want to understand how to generate that solution, for next time.",+ - * /  LI(x) = (LI_0 + x / PI) \bmod 1   LF(x) = (LF_0 + x / PF) \bmod 1   LI(s) + 0.5 = LF(f)   f = s + PT / 2  s LI_0 LF_0 PI PF PT \bmod 1 s \frac{1}{ \left| \frac{1}{PI} - \frac{1}{PF} \right| },"['linear-algebra', 'modular-arithmetic']"
44,knapsack algorithm that looks too good to be true,knapsack algorithm that looks too good to be true,,"I have an idea for solving the knapsack problem, but it looks too good to be true. I would like someone to explain potential problems with this approach. I'll give an example: I want to find a subset of {2,7,11} which has elements that sum to 13. Here is the algorithm for solving this: In binary notation 2=0010, 7=0111, 11=1011, 13=1101. Suppose that 2x+7y+11z=13, where x,y,z are {0,1}. Then y+z=1 (mod 2), since the last bits of 7, 11, and 13 are 1 and the last bit of 2 is 0. And (y+z-1)+2*(x+y+z)=0 (mod 4), since the second to last bits of 2, 7, and 11 are 1 and the second to last bit of 13 is 0. (The y+z-1 is the carry bit from before.) And ((y+z-1)+2*(x+y+z)-0)+4y=4 (mod 8), since the third to last bits of 7 and 13 are 1 and the third to last bit of 2 and 11 are 0. (The (y+z-1)+2*(x+y+z)-0 is the carry bit from before.) And finally ((y+z-1)+2*(x+y+z)-0)+4y-4)+8z=8 (mod 16), since the first bits of 11 and 13 are 1 and the first bit of 2 and 7 are 0. (The ((y+z-1)+2*(x+y+z)-0)+4y-4 is the carry bit from before.) So we have four linear equations over a finite ring, if we change each one to mod 16. 8y+8z=8 (mod 16) 8x+12y+12z=4 (mod 16) 4x+14y+6z=10 (mod 16) 2x+7y+11z=13 (mod 16) As we can see, a solution is x=1, y=0, z=1, as one would expect, since the set {2,11} has elements that sum to 13. We could have gotten this through Gaussian elimination (according to answers to my last question on this site). By request, here is the general algorithm that I just wrote (which can be induced from the above example): We want to solve a_1*x_1 + ... a_n*x_n = b, where each a_j and b is an integer and each x_j is in {0,1}. Putting each a_j and b in binary, we have a_j=(a_{mj} ... a_{1j}) and b=(b_m ... b_1), for suitable m. For instance, a_j=5 in binary is (a_{4j} ... a_{1j})=(0 1 0 1) - highest bit on the left side, lowest on the right side. Here is the simple algorithm for constructing a new matrix A=(a_{ij}) from this original (a_{ij}) and a new b=(b_i) from this original b: `Let b_1 = 2^{m-1}*b_1; For j=1 to n   Let a_{1j} = 2^{m-1}*a_{1j}; For i=2 to m {  Let b_i = 2^{m-1}*b_i + b_{i-1}/2;  For j=1 to n   Let a_{ij} = 2^{m-1}*a_{ij} + a_{(i-1),j}/2; }` Next solve the matrix equation Ax=b (mod 2^m), for the new A=(a_{ij}) and b=(b_i). If there is a solution to the knapsack problem, there should be a solution for x in {0,1} in Ax=b (mod 2^m) and vice versa (since the last row is just the original a_j and b). I'm curious to see if it works for large m and n. Anyone want to give it a shot? What's wrong with this approach? It shouldn't work, but I don't know why.","I have an idea for solving the knapsack problem, but it looks too good to be true. I would like someone to explain potential problems with this approach. I'll give an example: I want to find a subset of {2,7,11} which has elements that sum to 13. Here is the algorithm for solving this: In binary notation 2=0010, 7=0111, 11=1011, 13=1101. Suppose that 2x+7y+11z=13, where x,y,z are {0,1}. Then y+z=1 (mod 2), since the last bits of 7, 11, and 13 are 1 and the last bit of 2 is 0. And (y+z-1)+2*(x+y+z)=0 (mod 4), since the second to last bits of 2, 7, and 11 are 1 and the second to last bit of 13 is 0. (The y+z-1 is the carry bit from before.) And ((y+z-1)+2*(x+y+z)-0)+4y=4 (mod 8), since the third to last bits of 7 and 13 are 1 and the third to last bit of 2 and 11 are 0. (The (y+z-1)+2*(x+y+z)-0 is the carry bit from before.) And finally ((y+z-1)+2*(x+y+z)-0)+4y-4)+8z=8 (mod 16), since the first bits of 11 and 13 are 1 and the first bit of 2 and 7 are 0. (The ((y+z-1)+2*(x+y+z)-0)+4y-4 is the carry bit from before.) So we have four linear equations over a finite ring, if we change each one to mod 16. 8y+8z=8 (mod 16) 8x+12y+12z=4 (mod 16) 4x+14y+6z=10 (mod 16) 2x+7y+11z=13 (mod 16) As we can see, a solution is x=1, y=0, z=1, as one would expect, since the set {2,11} has elements that sum to 13. We could have gotten this through Gaussian elimination (according to answers to my last question on this site). By request, here is the general algorithm that I just wrote (which can be induced from the above example): We want to solve a_1*x_1 + ... a_n*x_n = b, where each a_j and b is an integer and each x_j is in {0,1}. Putting each a_j and b in binary, we have a_j=(a_{mj} ... a_{1j}) and b=(b_m ... b_1), for suitable m. For instance, a_j=5 in binary is (a_{4j} ... a_{1j})=(0 1 0 1) - highest bit on the left side, lowest on the right side. Here is the simple algorithm for constructing a new matrix A=(a_{ij}) from this original (a_{ij}) and a new b=(b_i) from this original b: `Let b_1 = 2^{m-1}*b_1; For j=1 to n   Let a_{1j} = 2^{m-1}*a_{1j}; For i=2 to m {  Let b_i = 2^{m-1}*b_i + b_{i-1}/2;  For j=1 to n   Let a_{ij} = 2^{m-1}*a_{ij} + a_{(i-1),j}/2; }` Next solve the matrix equation Ax=b (mod 2^m), for the new A=(a_{ij}) and b=(b_i). If there is a solution to the knapsack problem, there should be a solution for x in {0,1} in Ax=b (mod 2^m) and vice versa (since the last row is just the original a_j and b). I'm curious to see if it works for large m and n. Anyone want to give it a shot? What's wrong with this approach? It shouldn't work, but I don't know why.",,"['linear-algebra', 'algorithms', 'computer-science']"
45,What is the difference between matrix theory and linear algebra?,What is the difference between matrix theory and linear algebra?,,"I have lifted this from Mathoverflow since it belongs here. Hi, Currently, I'm taking matrix theory, and our textbook is Strang's Linear Algebra. Besides matrix theory, which all engineers must take, there exists linear algebra I and II for math majors. What is the difference, if any, between matrix theory and linear algebra? Thanks! kolistivra","I have lifted this from Mathoverflow since it belongs here. Hi, Currently, I'm taking matrix theory, and our textbook is Strang's Linear Algebra. Besides matrix theory, which all engineers must take, there exists linear algebra I and II for math majors. What is the difference, if any, between matrix theory and linear algebra? Thanks! kolistivra",,[]
46,Axler Proof of Real vs. Complex Spectral Theorem,Axler Proof of Real vs. Complex Spectral Theorem,,"In Professor Axler's Linear Algebra Done Right , he separates the proof of the spectral theorem for normal operators on a complex v.s. from the spectral theorem for self-adjoint operators over a real v.s., and I am wondering if each proof also works for the other case. To my understanding, sketches of the proofs are as follows: Normal operator $T$ on a complex v.s.: $T$ has an eigenvalue $\lambda$ . Schur's theorem says we can express $T$ as an upper triangular matrix (apply the inductive hypothesis of Schur's theorem to the range of $T - \lambda I$ and complete the basis). Result that $||Tx|| = ||T^*x||$ for normal operators gives the result. Self-adjoint operator $T$ on a real v.s.: $T$ has an eigenvalue $\lambda$ . Apply the inductive hypothesis of the spectral theorem to the orthogonal complement of the span of an eigenvector (allowed to do this because this is a $T$ -invariant subspace) to get result. It seems to me like the key result is showing the existence of eigenvalues, and each proof would work for the other case (to apply (2) to normal operators, we need to verify that the complement of an eigenspace is $T$ -invariant if $T$ is normal). I just want to make sure I'm not missing some subtlety that explains why Professor Axler separated these 2 cases. Thank you!","In Professor Axler's Linear Algebra Done Right , he separates the proof of the spectral theorem for normal operators on a complex v.s. from the spectral theorem for self-adjoint operators over a real v.s., and I am wondering if each proof also works for the other case. To my understanding, sketches of the proofs are as follows: Normal operator on a complex v.s.: has an eigenvalue . Schur's theorem says we can express as an upper triangular matrix (apply the inductive hypothesis of Schur's theorem to the range of and complete the basis). Result that for normal operators gives the result. Self-adjoint operator on a real v.s.: has an eigenvalue . Apply the inductive hypothesis of the spectral theorem to the orthogonal complement of the span of an eigenvector (allowed to do this because this is a -invariant subspace) to get result. It seems to me like the key result is showing the existence of eigenvalues, and each proof would work for the other case (to apply (2) to normal operators, we need to verify that the complement of an eigenspace is -invariant if is normal). I just want to make sure I'm not missing some subtlety that explains why Professor Axler separated these 2 cases. Thank you!",T T \lambda T T - \lambda I ||Tx|| = ||T^*x|| T T \lambda T T T,"['linear-algebra', 'spectral-theory']"
47,What does distance of a point from line being negative signify?,What does distance of a point from line being negative signify?,,"When we take distance from the line, we take $$ d = \frac{ Ax_o + By_o + C}{ \sqrt{A^2 +B^2}}$$ usually with a modulus on top, now my question is if I evaluate this distance as negative what does it mean? Can I decide on which half-plane a point using this?","When we take distance from the line, we take usually with a modulus on top, now my question is if I evaluate this distance as negative what does it mean? Can I decide on which half-plane a point using this?", d = \frac{ Ax_o + By_o + C}{ \sqrt{A^2 +B^2}},['linear-algebra']
48,Determinant of a Toeplitz matrix,Determinant of a Toeplitz matrix,,How can I calculate the determinant of the following Toeplitz matrix? \begin{bmatrix} 1&2&3&4&5&6&7&8&9&10\\ 2&1&2&3&4&5&6&7&8&9 \\ 3&2&1&2&3&4&5&6&7&8 \\ 4&3&2&1&2&3&4&5&6&7 \\ 5&4&3&2&1&2&3&4&5&6 \\ 6&5&4&3&2&1&2&3&4&5 \\ 7&6&5&4&3&2&1&2&3&4 \\ 8&7&6&5&4&3&2&1&2&3 \\ 9&8&7&6&5&4&3&2&1&2 \\ 10&9&8&7&6&5&4&3&2&1 \\ \end{bmatrix},How can I calculate the determinant of the following Toeplitz matrix?,"\begin{bmatrix}
1&2&3&4&5&6&7&8&9&10\\
2&1&2&3&4&5&6&7&8&9 \\
3&2&1&2&3&4&5&6&7&8 \\
4&3&2&1&2&3&4&5&6&7 \\
5&4&3&2&1&2&3&4&5&6 \\
6&5&4&3&2&1&2&3&4&5 \\
7&6&5&4&3&2&1&2&3&4 \\
8&7&6&5&4&3&2&1&2&3 \\
9&8&7&6&5&4&3&2&1&2 \\
10&9&8&7&6&5&4&3&2&1 \\
\end{bmatrix}","['linear-algebra', 'matrices', 'determinant', 'toeplitz-matrices']"
49,do all matrices with $\det(A)=\pm 1$ form a group under multiplication?,do all matrices with  form a group under multiplication?,\det(A)=\pm 1,"all matrices with determinant one form the special linear group. it is explained that because $\det(A) \det(B)=\det(AB)$ it is closed as $1*1=1$ and because the general linear group is a group, and special linear group is a part of the general one, and because all of the inverses must have determinant 1 and also be in the special linear group, the inversion axiom holds. doesn't this also hold for the group of matrices defined by $\det(A)= \pm 1$ ? $$(1)(1)=1,(-1)1=-1,1(-1)=-1,(-1)(-1)=1$$ so it is closed.can a similar argument to the special linear group prove this is a group? can anyone provide a counter example or prove this?","all matrices with determinant one form the special linear group. it is explained that because it is closed as and because the general linear group is a group, and special linear group is a part of the general one, and because all of the inverses must have determinant 1 and also be in the special linear group, the inversion axiom holds. doesn't this also hold for the group of matrices defined by ? so it is closed.can a similar argument to the special linear group prove this is a group? can anyone provide a counter example or prove this?","\det(A) \det(B)=\det(AB) 1*1=1 \det(A)= \pm 1 (1)(1)=1,(-1)1=-1,1(-1)=-1,(-1)(-1)=1","['linear-algebra', 'group-theory']"
50,"Prove $R(a \times b) = Ra \times Rb$ given $R \in \mathcal{SO}(3)$ and $a,b \in \mathbb{R}^3$.",Prove  given  and .,"R(a \times b) = Ra \times Rb R \in \mathcal{SO}(3) a,b \in \mathbb{R}^3","Here, $R$ is a proper rotation matrix, and $(\times)$ is the cross-product. I already found 3 ways of proving this, all have problems, and I am requesting an elegant approach. (1) Direct calculation, $R = R_{\mathbf{u},\theta}$ has an explicit expression, where $\mathbf{u}$ is the vector about which the rotation of $\theta$ is carried out. It is essentially possible to calculate both sides and compare. Inelegant. (2) Using antisymmetric matrices: $Ra \times Rb=S(Ra)Rb=RS(a)R^\top Rb=RS(a)b=R(a\times b)$ . My issue with this is that the equality I am trying to prove, is used to prove $RS(a)R^\top=S(Ra)$ . And so using this feels circular. (3) $Ra \times Rb=\|Ra\|\|Rb\|\sin(Ra,Rb)\mathbb{\hat{u}_1}$ and $a \times b=\|a\|\|b\|\sin(a,b)\mathbb{\hat{u}_2}$ . Here, essentially $\|Ra\|$ should equal $\|a\|$ since $R$ only affects orientation. Because the relative orientation does not change, the $\sin(\cdot)$ term should be equal. Likewise, $\mathbb{\hat{u}_1}$ and $\mathbb{\hat{u}_2}$ intuitively I know are equal but I am having a hard time expressing it. Lastly, I have no idea how to bridge $(a \times b)$ to $R(a \times b)$ . I intuitively see it, and perhaps $\det R = 1$ might be useful, but I feel it is hard to write. Please, a fourth approach is welcome, and insight is always appreciated.","Here, is a proper rotation matrix, and is the cross-product. I already found 3 ways of proving this, all have problems, and I am requesting an elegant approach. (1) Direct calculation, has an explicit expression, where is the vector about which the rotation of is carried out. It is essentially possible to calculate both sides and compare. Inelegant. (2) Using antisymmetric matrices: . My issue with this is that the equality I am trying to prove, is used to prove . And so using this feels circular. (3) and . Here, essentially should equal since only affects orientation. Because the relative orientation does not change, the term should be equal. Likewise, and intuitively I know are equal but I am having a hard time expressing it. Lastly, I have no idea how to bridge to . I intuitively see it, and perhaps might be useful, but I feel it is hard to write. Please, a fourth approach is welcome, and insight is always appreciated.","R (\times) R = R_{\mathbf{u},\theta} \mathbf{u} \theta Ra \times Rb=S(Ra)Rb=RS(a)R^\top Rb=RS(a)b=R(a\times b) RS(a)R^\top=S(Ra) Ra \times Rb=\|Ra\|\|Rb\|\sin(Ra,Rb)\mathbb{\hat{u}_1} a \times b=\|a\|\|b\|\sin(a,b)\mathbb{\hat{u}_2} \|Ra\| \|a\| R \sin(\cdot) \mathbb{\hat{u}_1} \mathbb{\hat{u}_2} (a \times b) R(a \times b) \det R = 1","['linear-algebra', 'matrices', 'vector-spaces', 'solution-verification', 'rotations']"
51,Confusion about order of rotations for Euler Angles,Confusion about order of rotations for Euler Angles,,"I'm taking a robotics class and trying to understand Euler angles. My understanding is, matrices are applied to to the vector from right to left (the first transformation applied is the one closest to $\overrightarrow{\boldsymbol{x}} $ ) My book defines Euler angles as: If we are rotating about the Z axis first , then shouldn't the rightmost matrix be for the Z rotation and not the X rotation? Shouldn't we have the transformation $${_a^b}R_{{Z^\prime}{Y^\prime}{X^\prime}}=R_X(\gamma)R_Y(\beta)R_Z(\alpha) $$ rather than $${_a^b}R_{{Z^\prime}{Y^\prime}{X^\prime}}=R_Z(\alpha)R_Y(\beta)R_X(\gamma) $$ I've read the definition from my textbook in several places and am aware it is correct, but","I'm taking a robotics class and trying to understand Euler angles. My understanding is, matrices are applied to to the vector from right to left (the first transformation applied is the one closest to ) My book defines Euler angles as: If we are rotating about the Z axis first , then shouldn't the rightmost matrix be for the Z rotation and not the X rotation? Shouldn't we have the transformation rather than I've read the definition from my textbook in several places and am aware it is correct, but",\overrightarrow{\boldsymbol{x}}  {_a^b}R_{{Z^\prime}{Y^\prime}{X^\prime}}=R_X(\gamma)R_Y(\beta)R_Z(\alpha)  {_a^b}R_{{Z^\prime}{Y^\prime}{X^\prime}}=R_Z(\alpha)R_Y(\beta)R_X(\gamma) ,"['linear-algebra', 'linear-transformations', 'rotations']"
52,Showing that every rational eigenvalue of a graph is integral,Showing that every rational eigenvalue of a graph is integral,,"(This is taken from the exercises in Bondy and Murty's Graph Theory.) Let the adjacency matrix of a graph $G$ be denoted by $\mathbf{A}$. The eigenvalues $\lambda$ of $G$ are defined as the roots of the characteristic polynomial of $\mathbf{A}$, $\lvert \mathbf A-\lambda\mathbf I\rvert=0$. I am asked to show that, if $\lambda\in\mathbb Q$, then $\lambda\in\mathbb Z$. I have already managed to show that $\lambda$ is real. This is because $\mathbf A$ is symmetric by definition, then it becomes a simple linear algebra fact that its eigenvalues are real. Then, if $G$ is simple, all diagonal entries of $\mathbf A$ are $0$, and since the eigenvalues of a matrix have the same signs as the diagonal entries (I hope!) then all the eigenvalues are $0$. However, I'm not given that $G$ is simple and so this line of reasoning doesn't work, and I have no idea where to use the fact that $\lambda$ is rational in the proof anyway. Any help is appreciated!","(This is taken from the exercises in Bondy and Murty's Graph Theory.) Let the adjacency matrix of a graph $G$ be denoted by $\mathbf{A}$. The eigenvalues $\lambda$ of $G$ are defined as the roots of the characteristic polynomial of $\mathbf{A}$, $\lvert \mathbf A-\lambda\mathbf I\rvert=0$. I am asked to show that, if $\lambda\in\mathbb Q$, then $\lambda\in\mathbb Z$. I have already managed to show that $\lambda$ is real. This is because $\mathbf A$ is symmetric by definition, then it becomes a simple linear algebra fact that its eigenvalues are real. Then, if $G$ is simple, all diagonal entries of $\mathbf A$ are $0$, and since the eigenvalues of a matrix have the same signs as the diagonal entries (I hope!) then all the eigenvalues are $0$. However, I'm not given that $G$ is simple and so this line of reasoning doesn't work, and I have no idea where to use the fact that $\lambda$ is rational in the proof anyway. Any help is appreciated!",,"['linear-algebra', 'matrices', 'graph-theory', 'eigenvalues-eigenvectors', 'symmetric-matrices']"
53,$A^TA=B^TB$. Is $A=QB$ for some orthogonal $Q$?,. Is  for some orthogonal ?,A^TA=B^TB A=QB Q,"Suppose that $A$ and $B$ are two real square matrices and $A^TA=B^TB$ . Can we say that $A=QB$ for some orthogonal matrix $Q$ ? If they are vectors we have $\|a\|^2=a^Ta=b^Tb=\|b\|^2$ , so intuitively clear, since we just have to rotate. But it is hard to picture the matrix case but I have not been able to show.","Suppose that and are two real square matrices and . Can we say that for some orthogonal matrix ? If they are vectors we have , so intuitively clear, since we just have to rotate. But it is hard to picture the matrix case but I have not been able to show.",A B A^TA=B^TB A=QB Q \|a\|^2=a^Ta=b^Tb=\|b\|^2,"['linear-algebra', 'orthogonal-matrices']"
54,Upper bounds on the approximate inverse of a singular matrix,Upper bounds on the approximate inverse of a singular matrix,,"Let $A$ be a singular matrix and $\zeta>0$ such that $A+\zeta I$ is nonsingular. Soft question: Is it true that if $\zeta$ is sufficiently small, then $$(A+\zeta I)^{-1}A \approx I$$ (or $A(A+\zeta I)^{-1} \approx I$)? More precisely: Let $E:=(A+\zeta I)^{-1}A - I$. If the answer to the above (soft) question is ""yes"", what are some known bounds on $E$? Note. Using the condition number $\kappa(A)$, a well-known bound is  $$ \frac{\Vert(A+B)^{-1} - A^{-1}\Vert}{\Vert A^{-1}\Vert} \le \kappa(A)\frac{\Vert B\Vert}{\Vert A\Vert }, $$ but this bound requires that both $A$ and $A+B$ are nonsingular. In my question, $A$ is singular while $A+B$ is nonsingular.","Let $A$ be a singular matrix and $\zeta>0$ such that $A+\zeta I$ is nonsingular. Soft question: Is it true that if $\zeta$ is sufficiently small, then $$(A+\zeta I)^{-1}A \approx I$$ (or $A(A+\zeta I)^{-1} \approx I$)? More precisely: Let $E:=(A+\zeta I)^{-1}A - I$. If the answer to the above (soft) question is ""yes"", what are some known bounds on $E$? Note. Using the condition number $\kappa(A)$, a well-known bound is  $$ \frac{\Vert(A+B)^{-1} - A^{-1}\Vert}{\Vert A^{-1}\Vert} \le \kappa(A)\frac{\Vert B\Vert}{\Vert A\Vert }, $$ but this bound requires that both $A$ and $A+B$ are nonsingular. In my question, $A$ is singular while $A+B$ is nonsingular.",,"['linear-algebra', 'matrices', 'numerical-methods', 'inverse', 'numerical-linear-algebra']"
55,Is there a systematic way of finding the matrix of a quadratic form? [duplicate],Is there a systematic way of finding the matrix of a quadratic form? [duplicate],,"This question already has an answer here : How to find the matrix of a quadratic form? (1 answer) Closed 6 years ago . For example i have this quadratic form $q(x_1,x_2)=8{x_1}^2-4x_1x_2+5{x_2}^2$ , here it's a simple factoring: $q\begin{bmatrix}x_1 \\x_2 \\x_3\\\end{bmatrix}=\begin{bmatrix}x_1 \\x_2 \\x_3\\\end{bmatrix} \cdot \begin{bmatrix}8x_1 &-2x_2\\-2x_1&5x_2\end{bmatrix}=\vec{x}^{T}A\vec{x} ,A=\begin{bmatrix}8 &-2\\-2&5\end{bmatrix}$. But this is not always the case where one can simply see how the matrix is going to be ,so is there a certain method of finding this matrix?","This question already has an answer here : How to find the matrix of a quadratic form? (1 answer) Closed 6 years ago . For example i have this quadratic form $q(x_1,x_2)=8{x_1}^2-4x_1x_2+5{x_2}^2$ , here it's a simple factoring: $q\begin{bmatrix}x_1 \\x_2 \\x_3\\\end{bmatrix}=\begin{bmatrix}x_1 \\x_2 \\x_3\\\end{bmatrix} \cdot \begin{bmatrix}8x_1 &-2x_2\\-2x_1&5x_2\end{bmatrix}=\vec{x}^{T}A\vec{x} ,A=\begin{bmatrix}8 &-2\\-2&5\end{bmatrix}$. But this is not always the case where one can simply see how the matrix is going to be ,so is there a certain method of finding this matrix?",,"['linear-algebra', 'quadratic-forms']"
56,Complexity/Operation count for the forward and backward substitution in the LU decomposition?,Complexity/Operation count for the forward and backward substitution in the LU decomposition?,,"If I have a linear system of equations $Ax=b$ where $A \in \mathbb{R} ^{n\times n}, x \in \mathbb{R} ^{n}, b \in \mathbb{R} ^{n} $ this system can be solved for $x$ via an LU decomposition: $$A = LU$$ where $U \in \mathbb{R} ^{n\times n}$ is upper triangular and $L \in \mathbb{R} ^{n\times n}$ is lower triangular. I understand a forward substitution is then required where one first solves: $$Ly=b$$ for $y$. And then we solve: $$Ux=y$$ for $x$. I am currently trying to determine the operation count or the FLOPS for each of the forward substitution and backward substitution. I have seen that the correct value is approximately given by $\mathcal{O}(n^{2})$ flops but I am unsure how one can arrive at this value. I can see that for the backward substitution, for example, the system is represented as: $$\begin{bmatrix} u_{11} & u_{12}  & \cdots   & u_{1n} \\  0 & u_{22} &\cdots  &u_{2n} \\   \cdots&  \cdots & \ddots  &\vdots  \\  0 &  0 & \cdots & u_{nn} \end{bmatrix} \begin{bmatrix} x_{1}\\  x_{2}\\  \vdots \\  x_{n} \end{bmatrix} = \begin{bmatrix} y_{1}\\  y_{2}\\  \vdots \\  y_{n} \end{bmatrix}$$ From which: $$x_{i} = \frac{1}{u_{ii}} \left ( y_{i} - \sum_{j=i+1}^{n}u_{ij}x_{j} \right ); i = n, ..., 1$$ From an equation like this, how can one identify the approximate operation count?","If I have a linear system of equations $Ax=b$ where $A \in \mathbb{R} ^{n\times n}, x \in \mathbb{R} ^{n}, b \in \mathbb{R} ^{n} $ this system can be solved for $x$ via an LU decomposition: $$A = LU$$ where $U \in \mathbb{R} ^{n\times n}$ is upper triangular and $L \in \mathbb{R} ^{n\times n}$ is lower triangular. I understand a forward substitution is then required where one first solves: $$Ly=b$$ for $y$. And then we solve: $$Ux=y$$ for $x$. I am currently trying to determine the operation count or the FLOPS for each of the forward substitution and backward substitution. I have seen that the correct value is approximately given by $\mathcal{O}(n^{2})$ flops but I am unsure how one can arrive at this value. I can see that for the backward substitution, for example, the system is represented as: $$\begin{bmatrix} u_{11} & u_{12}  & \cdots   & u_{1n} \\  0 & u_{22} &\cdots  &u_{2n} \\   \cdots&  \cdots & \ddots  &\vdots  \\  0 &  0 & \cdots & u_{nn} \end{bmatrix} \begin{bmatrix} x_{1}\\  x_{2}\\  \vdots \\  x_{n} \end{bmatrix} = \begin{bmatrix} y_{1}\\  y_{2}\\  \vdots \\  y_{n} \end{bmatrix}$$ From which: $$x_{i} = \frac{1}{u_{ii}} \left ( y_{i} - \sum_{j=i+1}^{n}u_{ij}x_{j} \right ); i = n, ..., 1$$ From an equation like this, how can one identify the approximate operation count?",,"['linear-algebra', 'computational-complexity', 'lu-decomposition']"
57,Distributivity of subspaces,Distributivity of subspaces,,"I need to either give a proof or find a counterexample to a statement: $$L+(M∩N) = (L + M)∩(L + N)$$ Where $L$,$M$,$N$ are subspaces of a vector space $V$.  I could do $LHS⊆RHS$ proof, but I'm stuck with backwards proof. I would be really grateful if someone could help me out.","I need to either give a proof or find a counterexample to a statement: $$L+(M∩N) = (L + M)∩(L + N)$$ Where $L$,$M$,$N$ are subspaces of a vector space $V$.  I could do $LHS⊆RHS$ proof, but I'm stuck with backwards proof. I would be really grateful if someone could help me out.",,['linear-algebra']
58,If $f$ is diagonalisable then its minimal polynomial is the product of distinct linear factors,If  is diagonalisable then its minimal polynomial is the product of distinct linear factors,f,"I'm trying to prove that if a linear operator $f$ is diagonalisable then its minimal polynomial is the product of distinct linear factors. This is what I have so far: Let $f$ be diagonalisable. So there exists a basis relative to which $f$ has a diagonal matrix, say $D$. So the characteristic polynomial of $f$ is given by $p_f(x)=(x-\lambda_1)(x-\lambda_2)\ldots(x-\lambda_s),$ where $\lambda_i$ are the diagonal entries of $D$. I know that the minimal polynomial must divide the characteristic polynomial and have the same linear factors. Without loss of generality let the first $i$ linear factors be distinct. So I claim that the minimal polynomial $m_f(x)=\pm (x-\lambda_1)(x-\lambda_2)\ldots(x-\lambda_i).$ However, how can I now verify that $m_f(D)=0$ ?","I'm trying to prove that if a linear operator $f$ is diagonalisable then its minimal polynomial is the product of distinct linear factors. This is what I have so far: Let $f$ be diagonalisable. So there exists a basis relative to which $f$ has a diagonal matrix, say $D$. So the characteristic polynomial of $f$ is given by $p_f(x)=(x-\lambda_1)(x-\lambda_2)\ldots(x-\lambda_s),$ where $\lambda_i$ are the diagonal entries of $D$. I know that the minimal polynomial must divide the characteristic polynomial and have the same linear factors. Without loss of generality let the first $i$ linear factors be distinct. So I claim that the minimal polynomial $m_f(x)=\pm (x-\lambda_1)(x-\lambda_2)\ldots(x-\lambda_i).$ However, how can I now verify that $m_f(D)=0$ ?",,"['linear-algebra', 'minimal-polynomials']"
59,How to find the linear transformation associated with a given matrix?,How to find the linear transformation associated with a given matrix?,,"Good day, I have a little doubt: It is well known that given two bases (or even one if we consider the canonical basis) of a vector space, every linear transformation $T:V \rightarrow W$ can be represented as a matrix, but since this is an isomorphism between $L(V,W)$ and $\mathbb{M}_{m\times n}$ where the latter represents the space $m\times n$ matrices on the same field in which are defined respectively vector spaces. That's where my question comes up, I know find the matrix associated with the linear transformation, but not know how to move from the matrix transformation, ie given any matrix, find the linear transformation that defines it. I wish you could please explain the theoretical process and to see a practical example. Thank you very much, I know it's definitely something silly, but I'm still a student.","Good day, I have a little doubt: It is well known that given two bases (or even one if we consider the canonical basis) of a vector space, every linear transformation $T:V \rightarrow W$ can be represented as a matrix, but since this is an isomorphism between $L(V,W)$ and $\mathbb{M}_{m\times n}$ where the latter represents the space $m\times n$ matrices on the same field in which are defined respectively vector spaces. That's where my question comes up, I know find the matrix associated with the linear transformation, but not know how to move from the matrix transformation, ie given any matrix, find the linear transformation that defines it. I wish you could please explain the theoretical process and to see a practical example. Thank you very much, I know it's definitely something silly, but I'm still a student.",,['linear-algebra']
60,Is it true that if $T$ is a linear operator on a finite-dimensional vector space $V$ then $V=\ker T\oplus \operatorname{im}T$?,Is it true that if  is a linear operator on a finite-dimensional vector space  then ?,T V V=\ker T\oplus \operatorname{im}T,"$\newcommand{\im}{\operatorname{im}}$I am trying to prove or disprove the following statement: Let $V$ and $W$ be finite-dimensional vector spaces. If $T:V\rightarrow W$ is a linear transformation then $V=\ker T\oplus \im T$. (By the symbol $\oplus$ I mean the direct sum of two vector spaces.) This statement cannot be true if $V\neq W$ because a vector space can only be a direct sum of its subspaces. However, I am not sure about the case when $V=W$, i.e., when $T$ is a linear operator. I want to use the following proposition: $\textbf{Proposition.}$ Let $V$ be a finite-dimensional vector space and let $U$ and $W$ be subspaces of $V$. Then $V=U\oplus W$ if and only if $V=U+W$ and $U\cap W=\left\{ 0 \right\}$. First I want to show that $V=\ker T + \im T$. I just don't have a clue how to possibly do this, which leads me to believe there must be a counterexample. I believe that $\ker T\cap \im T=\left\{0\right\}$ since $T(0)=0$ for any linear transformation and it is not possible for $Tv\neq 0$ if $v\in \ker T$. Some help? Thank you in advance.","$\newcommand{\im}{\operatorname{im}}$I am trying to prove or disprove the following statement: Let $V$ and $W$ be finite-dimensional vector spaces. If $T:V\rightarrow W$ is a linear transformation then $V=\ker T\oplus \im T$. (By the symbol $\oplus$ I mean the direct sum of two vector spaces.) This statement cannot be true if $V\neq W$ because a vector space can only be a direct sum of its subspaces. However, I am not sure about the case when $V=W$, i.e., when $T$ is a linear operator. I want to use the following proposition: $\textbf{Proposition.}$ Let $V$ be a finite-dimensional vector space and let $U$ and $W$ be subspaces of $V$. Then $V=U\oplus W$ if and only if $V=U+W$ and $U\cap W=\left\{ 0 \right\}$. First I want to show that $V=\ker T + \im T$. I just don't have a clue how to possibly do this, which leads me to believe there must be a counterexample. I believe that $\ker T\cap \im T=\left\{0\right\}$ since $T(0)=0$ for any linear transformation and it is not possible for $Tv\neq 0$ if $v\in \ker T$. Some help? Thank you in advance.",,"['linear-algebra', 'linear-transformations', 'direct-sum']"
61,How to prove: Orthogonal complement of kernel = Row space?,How to prove: Orthogonal complement of kernel = Row space?,,"I'm really confused when trying to prove the following: Suppose $T:\mathbb{R}^n \to \mathbb{R}^m$ is a linear transformation represented by the matrix $A$ whose rows are given by $\{z_1^T,...,z_m^T\}$. Denote by $\mathrm{Ker}(A)$ the kernel of the transformation. I am trying to prove the following. $$\mathrm{Ker}(A)^\perp = \mathrm{span}\{z_1^T,...,z_m^T\} \tag{1}$$ It should be easy to prove but I'm completely confused at the moment. Thanks in advance!!","I'm really confused when trying to prove the following: Suppose $T:\mathbb{R}^n \to \mathbb{R}^m$ is a linear transformation represented by the matrix $A$ whose rows are given by $\{z_1^T,...,z_m^T\}$. Denote by $\mathrm{Ker}(A)$ the kernel of the transformation. I am trying to prove the following. $$\mathrm{Ker}(A)^\perp = \mathrm{span}\{z_1^T,...,z_m^T\} \tag{1}$$ It should be easy to prove but I'm completely confused at the moment. Thanks in advance!!",,"['linear-algebra', 'matrices']"
62,"What should ""The Fundamental Theorem of Linear Algebra"" assert? [closed]","What should ""The Fundamental Theorem of Linear Algebra"" assert? [closed]",,"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 8 years ago . Improve this question Unlike some other basic fields of mathematics, linear algebra does not seem to have a universally agreed-upon fundamental theorem. This I imagine might be because the subject usually admits a lot of equivalent formulations to statements and it is perhaps hard (or pointless) to tell which are the fundamental ones and which are the consequences. Still, some of them might be arguably more natural or less constructed than others, so maybe there is some value in the question of what the ""right"" statement of the ""key idea"" of linear algebra should be, and it what sense it is fundamental? There is a vague entry on this on Wikipedia. What I don't like about it most is simply that it is stated in terms of matrices, which I am accustomed to think of no more and no less than just convenient block notations to encode linear maps on finite-dimensional vector spaces (sure it could be rewritten in terms of linear maps, but do singular value decompositions really fit the above description?) What rings truly fundamental to me is the statement that if $V, W$ are vector space, then any map defined on some basis of $V$ and with values in $W$ extends uniquely to a linear map on $V$ (and what's more, it is injective/surjective/bijective if and only if the system of image vectors is linearly independent/spanning/a basis). I've seen courses calling such a statement the fundamental theorem, a slogan and Lemma x.y, but it seems to me that this statement in particular (and the straightforward proofs of its sub-statements) make it beautifully clear how the concepts of linearity, basis, subspace etc. click. What are people's thoughts on this? EDIT: I agree that the dimension/basis existence theorem is perhaps the most fundamental statement we can make about vector spaces, but it seems to me that the righteous owner of such results is matroid theory. And what's more, this would mean that there is no linear algebra in a choice-free world. I think there is, in principle, nothing to stop us from thinking of linear algebra as being a theory of objects, which are pairs consisting of a vector space with some given basis - shouldn't the FTLA be a statement about such objects (and not care about basis existence in general)?","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 8 years ago . Improve this question Unlike some other basic fields of mathematics, linear algebra does not seem to have a universally agreed-upon fundamental theorem. This I imagine might be because the subject usually admits a lot of equivalent formulations to statements and it is perhaps hard (or pointless) to tell which are the fundamental ones and which are the consequences. Still, some of them might be arguably more natural or less constructed than others, so maybe there is some value in the question of what the ""right"" statement of the ""key idea"" of linear algebra should be, and it what sense it is fundamental? There is a vague entry on this on Wikipedia. What I don't like about it most is simply that it is stated in terms of matrices, which I am accustomed to think of no more and no less than just convenient block notations to encode linear maps on finite-dimensional vector spaces (sure it could be rewritten in terms of linear maps, but do singular value decompositions really fit the above description?) What rings truly fundamental to me is the statement that if $V, W$ are vector space, then any map defined on some basis of $V$ and with values in $W$ extends uniquely to a linear map on $V$ (and what's more, it is injective/surjective/bijective if and only if the system of image vectors is linearly independent/spanning/a basis). I've seen courses calling such a statement the fundamental theorem, a slogan and Lemma x.y, but it seems to me that this statement in particular (and the straightforward proofs of its sub-statements) make it beautifully clear how the concepts of linearity, basis, subspace etc. click. What are people's thoughts on this? EDIT: I agree that the dimension/basis existence theorem is perhaps the most fundamental statement we can make about vector spaces, but it seems to me that the righteous owner of such results is matroid theory. And what's more, this would mean that there is no linear algebra in a choice-free world. I think there is, in principle, nothing to stop us from thinking of linear algebra as being a theory of objects, which are pairs consisting of a vector space with some given basis - shouldn't the FTLA be a statement about such objects (and not care about basis existence in general)?",,"['linear-algebra', 'soft-question']"
63,Do endomorphisms of infinite-dimensional vector spaces over algebraically closed fields always have eigenvalues?,Do endomorphisms of infinite-dimensional vector spaces over algebraically closed fields always have eigenvalues?,,"Let $V$ be a vector space over an algebraically closed field $K$ and let $f:V\to V$ be an endomorphism. If $V$ is finite-dimensional, we know that the characteristic polynomial $\chi_f$ has a zero $\lambda$, which is an eigenvalue of $f$. However, does a similar argument hold if $\dim V=\infty$?","Let $V$ be a vector space over an algebraically closed field $K$ and let $f:V\to V$ be an endomorphism. If $V$ is finite-dimensional, we know that the characteristic polynomial $\chi_f$ has a zero $\lambda$, which is an eigenvalue of $f$. However, does a similar argument hold if $\dim V=\infty$?",,"['linear-algebra', 'vector-spaces', 'eigenvalues-eigenvectors', 'linear-transformations']"
64,positive definite matrix plus positive semi matrix equals positive definite?,positive definite matrix plus positive semi matrix equals positive definite?,,"I have a questions related to the positive definite[PD] matrix and positive semi definite[PSD] matrix I see and get the property about PD and PSD 1) PD + PD = PD 2) PSD+ PSD = PSD how about the positive definite[PD] matrix plus positive semi definite matrix ? (I mean sum of positive definite matrix and positive semi definite matrix : PD + PSD) Is it right to be positive definite matrix? For example, If matrix  B is $R \times R$ and it is  sum of identity matrix $I$  and symmetry matrix A that is, $B=I+A$ 1) $I=\det(I)=1>0 $ positive definite 2) $X^{T}AX=X^{T}L^{T}LX=U^{T}U=||U||\geqslant 0 $ positive semidefinite I think that it would be positive definite, I am not so sure... So I would like to get some help from you Thank you very much in advance !","I have a questions related to the positive definite[PD] matrix and positive semi definite[PSD] matrix I see and get the property about PD and PSD 1) PD + PD = PD 2) PSD+ PSD = PSD how about the positive definite[PD] matrix plus positive semi definite matrix ? (I mean sum of positive definite matrix and positive semi definite matrix : PD + PSD) Is it right to be positive definite matrix? For example, If matrix  B is $R \times R$ and it is  sum of identity matrix $I$  and symmetry matrix A that is, $B=I+A$ 1) $I=\det(I)=1>0 $ positive definite 2) $X^{T}AX=X^{T}L^{T}LX=U^{T}U=||U||\geqslant 0 $ positive semidefinite I think that it would be positive definite, I am not so sure... So I would like to get some help from you Thank you very much in advance !",,"['linear-algebra', 'matrices', 'positive-definite']"
65,Can $e^{ax}$ be said to be the eigenfunction of the operator $\frac{d^{(n)}}{dx}$?,Can  be said to be the eigenfunction of the operator ?,e^{ax} \frac{d^{(n)}}{dx},"I'm gradually getting familiar with operators (as they are used in QM) and the terminology surrounding them, and I was wondering whether all the (to me) well-known operators have straight-forward, elementary functions, as seems to be the case with $\frac{d^{(n)}}{dx}$, because $$\frac{d^{(n)}}{dx}e^{ax}=a^n e^{ax}$$ and could one say that the spectrum of these eigenfunctions is degenerate, since $a$ can vary (I know ""spectrum"" is usually used for the set of eigenvalues, but it seems appropriate here)? Is this the correct interpretation? If so, what is the eigenfunctions and -values for the following differentialoperators (if you could point me in the direction of a resource that either collects them or - even better - shows how they are obtained, that would be very acceptable): $\int dx$ $\nabla$ (grad) $\nabla \cdot $ (div) $\nabla^2$ (Laplace) If you have a cool one, please do just throw it in there! Thanks!","I'm gradually getting familiar with operators (as they are used in QM) and the terminology surrounding them, and I was wondering whether all the (to me) well-known operators have straight-forward, elementary functions, as seems to be the case with $\frac{d^{(n)}}{dx}$, because $$\frac{d^{(n)}}{dx}e^{ax}=a^n e^{ax}$$ and could one say that the spectrum of these eigenfunctions is degenerate, since $a$ can vary (I know ""spectrum"" is usually used for the set of eigenvalues, but it seems appropriate here)? Is this the correct interpretation? If so, what is the eigenfunctions and -values for the following differentialoperators (if you could point me in the direction of a resource that either collects them or - even better - shows how they are obtained, that would be very acceptable): $\int dx$ $\nabla$ (grad) $\nabla \cdot $ (div) $\nabla^2$ (Laplace) If you have a cool one, please do just throw it in there! Thanks!",,"['linear-algebra', 'reference-request', 'spectral-theory', 'online-resources', 'eigenfunctions']"
66,Let A be a square matrix such that $A^3 = 2I$ [closed],Let A be a square matrix such that  [closed],A^3 = 2I,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Let $A$ be a square matrix such that $A^3 = 2I$ i) Prove that $A - I$ is invertible and find its inverse ii) Prove that $A + 2I$ is invertible and find its inverse iii) Using (i) and (ii) or otherwise, prove that $A^2 - 2A + 2I$ is invertible and find its inverse as a polynomial in $A$ $I$ refers to identity matrix. Am already stucked at part i). Was going along the line of showing that $(A-I)([...]) = I$ by manipulating the equation to $A^3 - I = I$ and I got stuck... :(","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Let $A$ be a square matrix such that $A^3 = 2I$ i) Prove that $A - I$ is invertible and find its inverse ii) Prove that $A + 2I$ is invertible and find its inverse iii) Using (i) and (ii) or otherwise, prove that $A^2 - 2A + 2I$ is invertible and find its inverse as a polynomial in $A$ $I$ refers to identity matrix. Am already stucked at part i). Was going along the line of showing that $(A-I)([...]) = I$ by manipulating the equation to $A^3 - I = I$ and I got stuck... :(",,"['linear-algebra', 'matrices', 'inverse']"
67,Prove that matrix equation $AX-XA=I$ doesn't have a solution for any $A\in M_n(\mathbb{R})$,Prove that matrix equation  doesn't have a solution for any,AX-XA=I A\in M_n(\mathbb{R}),"Prove that matrix equation $AX-XA=I$ doesn't have a solution for any $A\in M_n(\mathbb{R})$ If $n=2,$ $$A=         \begin{bmatrix}         x & y \\         u & v  \\         \end{bmatrix} $$ $$X=         \begin{bmatrix}         \alpha \\         \beta \\         \end{bmatrix} $$ $\Rightarrow AX-XA=O\neq I$ What is the more strict proof?","Prove that matrix equation $AX-XA=I$ doesn't have a solution for any $A\in M_n(\mathbb{R})$ If $n=2,$ $$A=         \begin{bmatrix}         x & y \\         u & v  \\         \end{bmatrix} $$ $$X=         \begin{bmatrix}         \alpha \\         \beta \\         \end{bmatrix} $$ $\Rightarrow AX-XA=O\neq I$ What is the more strict proof?",,"['linear-algebra', 'matrices']"
68,Why does $A \circ {B^{ - 1}} + {A^{ - 1}} \circ B \ge 2{I_{n \times n}}$?,Why does ?,A \circ {B^{ - 1}} + {A^{ - 1}} \circ B \ge 2{I_{n \times n}},"Let $A, B \in M_n$ be positive definite and $A \circ B = \left[ {{a_{ij}}{b_{ij}}} \right]$. Why does $A \circ {B^{ - 1}} + {A^{ - 1}} \circ B \ge 2{I_{n \times n}}$ ?","Let $A, B \in M_n$ be positive definite and $A \circ B = \left[ {{a_{ij}}{b_{ij}}} \right]$. Why does $A \circ {B^{ - 1}} + {A^{ - 1}} \circ B \ge 2{I_{n \times n}}$ ?",,"['linear-algebra', 'matrices']"
69,"If $A^2 = I$, then $A$ is diagonalizable, and is $I$ if $1$ is its only eigenvalue","If , then  is diagonalizable, and is  if  is its only eigenvalue",A^2 = I A I 1,"Let $A$ be a square matrix of order $n$ such that $A^2 = I$ . Prove that if $1$ is the only eigenvalue of $A$ , then $A = I$ . Prove that $A$ is diagonalizable. For  (1), I know that there are two eigenvalues which are $1$ and $-1$ , how do I go about proving what the question asks me?","Let be a square matrix of order such that . Prove that if is the only eigenvalue of , then . Prove that is diagonalizable. For  (1), I know that there are two eigenvalues which are and , how do I go about proving what the question asks me?",A n A^2 = I 1 A A = I A 1 -1,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization']"
70,Finding the reflection that reflects in an arbitrary line y=mx+b,Finding the reflection that reflects in an arbitrary line y=mx+b,,"How can I find the reflection that reflects in an arbitrary line, $y=mx+b$ I've examples where it's $y=mx$ without taking in the factor of $b$ But I want to know how you can take in the factor of $b$ And after searching through for some results, I came to this matrix which i think can solve my problems. But it doesn't seem to work. $$ \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} \frac{1-m^2}{1 + m^2} & \frac{-2m}{1 + m^2} & \frac{-2mb}{1 + m^2} \\ \frac{-2m}{1 + m^2} & \frac{m^2-1}{1 + m^2} & \frac{2b}{1 + m^2} \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} x \\ y \\ 1 \end{bmatrix}. $$ The example I tried to use using this matrix is the point $(0,8)$ reflected on $y=-\frac{1}{2}x+2$. The result I get from that matrix is $[6.4,-0.6,0]$. The actual answer should be $[-4.8, -1.6]$ , according to Geogebra","How can I find the reflection that reflects in an arbitrary line, $y=mx+b$ I've examples where it's $y=mx$ without taking in the factor of $b$ But I want to know how you can take in the factor of $b$ And after searching through for some results, I came to this matrix which i think can solve my problems. But it doesn't seem to work. $$ \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} \frac{1-m^2}{1 + m^2} & \frac{-2m}{1 + m^2} & \frac{-2mb}{1 + m^2} \\ \frac{-2m}{1 + m^2} & \frac{m^2-1}{1 + m^2} & \frac{2b}{1 + m^2} \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} x \\ y \\ 1 \end{bmatrix}. $$ The example I tried to use using this matrix is the point $(0,8)$ reflected on $y=-\frac{1}{2}x+2$. The result I get from that matrix is $[6.4,-0.6,0]$. The actual answer should be $[-4.8, -1.6]$ , according to Geogebra",,"['linear-algebra', 'matrices']"
71,Prove That Projection Operator Is Non Expansive,Prove That Projection Operator Is Non Expansive,,"I am trying to prove that the projection operator defined as: \begin{equation} P(z) := argmin_{x \in \mathcal{C}}  \frac{1}{2}\|x-z\|^2_2 \end{equation} is non-expansive. Here $\mathcal{C}$ is nonempty closed and convex set. To show this, I proceed as: \begin{equation} \|P(z_1) - P(z_2)|| =\|x_1-x_2\| \end{equation} , where $x_1, x_2$ are points in the set $\mathcal{C}$ . Now, I know that $\|x_1-x_2\| \leq \|z_1-z_2\|$ . But, how to prove this last part? Can JL lemma be used someway?","I am trying to prove that the projection operator defined as: is non-expansive. Here is nonempty closed and convex set. To show this, I proceed as: , where are points in the set . Now, I know that . But, how to prove this last part? Can JL lemma be used someway?","\begin{equation}
P(z) := argmin_{x \in \mathcal{C}}  \frac{1}{2}\|x-z\|^2_2
\end{equation} \mathcal{C} \begin{equation}
\|P(z_1) - P(z_2)|| =\|x_1-x_2\|
\end{equation} x_1, x_2 \mathcal{C} \|x_1-x_2\| \leq \|z_1-z_2\|","['linear-algebra', 'convex-analysis']"
72,Proof if $I+AB$ invertible then $I+BA$ invertible and $(I+BA)^{-1}=I-B(I+AB)^{-1}A$ [duplicate],Proof if  invertible then  invertible and  [duplicate],I+AB I+BA (I+BA)^{-1}=I-B(I+AB)^{-1}A,"This question already has answers here : $I_m - AB$ is invertible if and only if $I_n - BA$ is invertible. (4 answers) Closed 5 years ago . I have the following question : Proof if $I+AB$ invertible then $I+BA$ invertible and $(I+BA)^{-1}=I-B(I+AB)^{-1}A$ I managed to proof that $I+BA$ invertible My proof : We know that $AB$ and $BA$ has the same eigenvalues, and Since $I+AB$ invertible $-1$ is not an eigenvalue for $I+AB$ since if $-1$ is an eigenvalue then $I+AB$ is singular which is a contradiction. and since $AB$ and $BA$ has the same eigenvalues then $-1$ is also not an eigenvalue for $I+BA$ therefore $I+BA$ is also invertible. But how do I show that $(I+BA)^{-1}=I-B(I+AB)^{-1}A$ I tried to ""play"" with the equations to reach one end to other end meaning that $(I+BA)^{-1}=...=...=...=I-B(I+AB)^{-1}A$ Or to show that $ I=(I+BA)^{-1}(I-B(I+AB)^{-1}A)$ But wasn't successful. Any ideas? Thank you!","This question already has answers here : $I_m - AB$ is invertible if and only if $I_n - BA$ is invertible. (4 answers) Closed 5 years ago . I have the following question : Proof if $I+AB$ invertible then $I+BA$ invertible and $(I+BA)^{-1}=I-B(I+AB)^{-1}A$ I managed to proof that $I+BA$ invertible My proof : We know that $AB$ and $BA$ has the same eigenvalues, and Since $I+AB$ invertible $-1$ is not an eigenvalue for $I+AB$ since if $-1$ is an eigenvalue then $I+AB$ is singular which is a contradiction. and since $AB$ and $BA$ has the same eigenvalues then $-1$ is also not an eigenvalue for $I+BA$ therefore $I+BA$ is also invertible. But how do I show that $(I+BA)^{-1}=I-B(I+AB)^{-1}A$ I tried to ""play"" with the equations to reach one end to other end meaning that $(I+BA)^{-1}=...=...=...=I-B(I+AB)^{-1}A$ Or to show that $ I=(I+BA)^{-1}(I-B(I+AB)^{-1}A)$ But wasn't successful. Any ideas? Thank you!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-equations']"
73,A bad Cayley–Hamilton theorem proof [duplicate],A bad Cayley–Hamilton theorem proof [duplicate],,"This question already has answers here : To prove Cayley-Hamilton theorem, why can't we substitute $A$ for $\lambda$ in $p(\lambda) = \det(\lambda I - A)$? (7 answers) Closed 8 years ago . Given $A\in M_{n \times n}(\mathbb{F})$ and $p_{A}(x)=\det(xI-A)$ why saying that $\det(AI-A)=0$ is not valid?","This question already has answers here : To prove Cayley-Hamilton theorem, why can't we substitute $A$ for $\lambda$ in $p(\lambda) = \det(\lambda I - A)$? (7 answers) Closed 8 years ago . Given $A\in M_{n \times n}(\mathbb{F})$ and $p_{A}(x)=\det(xI-A)$ why saying that $\det(AI-A)=0$ is not valid?",,['linear-algebra']
74,"Prove that the set $ \{\sin(x),\cos(x),\sin(2x),\cos(2x)\}$ is linearly independent.",Prove that the set  is linearly independent.," \{\sin(x),\cos(x),\sin(2x),\cos(2x)\}","Prove that the set $ \{\sin(x),\cos(x),\sin(2x),\cos(2x)\}$ is linearly independent. I have tried plugging in values for $x$ but where does this lead? I know what the Wronksian is and it would give an almost trivial solution in this case, but I am not allowed to use that.","Prove that the set $ \{\sin(x),\cos(x),\sin(2x),\cos(2x)\}$ is linearly independent. I have tried plugging in values for $x$ but where does this lead? I know what the Wronksian is and it would give an almost trivial solution in this case, but I am not allowed to use that.",,"['linear-algebra', 'trigonometry']"
75,How can I quickly find the determinant of this matrix,How can I quickly find the determinant of this matrix,,$$         \begin{vmatrix}         14 & 2 & 1 & 3\\         31 & 4 & 5 & 6\\         26 & 3 & 7 & 4\\         10 & 1 & 3 & 2\\         \end{vmatrix}        =         \begin{vmatrix}         5\cdot2+1+3 & 2 & 1 & 3\\         5\cdot4+5+6 & 4 & 5 & 6\\         5\cdot3+7+4 & 3 & 7 & 4\\         5\cdot1+3+2 & 1 & 3 & 2\\         \end{vmatrix}$$$$        =         \begin{vmatrix}         5\cdot2 & 2 & 1 & 3\\         5\cdot4 & 4 & 5 & 6\\         5\cdot3 & 3 & 7 & 4\\         5\cdot1 & 1 & 3 & 2\\         \end{vmatrix}         +         \begin{vmatrix}         1 & 2 & 1 & 3\\         5 & 4 & 5 & 6\\         7 & 3 & 7 & 4\\         3 & 1 & 3 & 2\\         \end{vmatrix}         +         \begin{vmatrix}         3 & 2 & 1 & 3\\         6 & 4 & 5 & 6\\         4 & 3 & 7 & 4\\         2 & 1 & 3 & 2\\         \end{vmatrix} $$ However I am not able to proceed beyond. The answer given is zero. Is there any simple determinant property that I am not able to guess?,$$         \begin{vmatrix}         14 & 2 & 1 & 3\\         31 & 4 & 5 & 6\\         26 & 3 & 7 & 4\\         10 & 1 & 3 & 2\\         \end{vmatrix}        =         \begin{vmatrix}         5\cdot2+1+3 & 2 & 1 & 3\\         5\cdot4+5+6 & 4 & 5 & 6\\         5\cdot3+7+4 & 3 & 7 & 4\\         5\cdot1+3+2 & 1 & 3 & 2\\         \end{vmatrix}$$$$        =         \begin{vmatrix}         5\cdot2 & 2 & 1 & 3\\         5\cdot4 & 4 & 5 & 6\\         5\cdot3 & 3 & 7 & 4\\         5\cdot1 & 1 & 3 & 2\\         \end{vmatrix}         +         \begin{vmatrix}         1 & 2 & 1 & 3\\         5 & 4 & 5 & 6\\         7 & 3 & 7 & 4\\         3 & 1 & 3 & 2\\         \end{vmatrix}         +         \begin{vmatrix}         3 & 2 & 1 & 3\\         6 & 4 & 5 & 6\\         4 & 3 & 7 & 4\\         2 & 1 & 3 & 2\\         \end{vmatrix} $$ However I am not able to proceed beyond. The answer given is zero. Is there any simple determinant property that I am not able to guess?,,"['linear-algebra', 'matrices', 'determinant']"
76,Integer $2 \times 2$ matrices such that $A^n = I$,Integer  matrices such that,2 \times 2 A^n = I,"An earlier question today motivates this slight variant: For what natural numbers  $n$ does there exist a non-identity integer $2\times 2$ matrix $A$, such that $A^n = I$? (And let's say $A^k \ne I$ for $|k| < |n|$, too.) Clearly there are solutions for $n=2,3$, because $(-I)^2 = I$, and for $n=3$ we have $\left( \begin{smallmatrix} -2 & 1\\ -3 & 1 \\\end{smallmatrix} \right)$ The solution for $n = 3$ suggests to me that something involving the euclidean algorithm might come into play...and the fact the the determinant is $\pm 1$ suggests that this is really an $SL(2, \mathbb Z)$ (or $PSL(2, \mathbb Z)$)problem...but that's a group I'm woefully ignorant about. For $n = 4$, there's $\left( \begin{smallmatrix} 0 & -1\\ 1 & 0 \\\end{smallmatrix} \right)$. And right about there I run out of ideas.","An earlier question today motivates this slight variant: For what natural numbers  $n$ does there exist a non-identity integer $2\times 2$ matrix $A$, such that $A^n = I$? (And let's say $A^k \ne I$ for $|k| < |n|$, too.) Clearly there are solutions for $n=2,3$, because $(-I)^2 = I$, and for $n=3$ we have $\left( \begin{smallmatrix} -2 & 1\\ -3 & 1 \\\end{smallmatrix} \right)$ The solution for $n = 3$ suggests to me that something involving the euclidean algorithm might come into play...and the fact the the determinant is $\pm 1$ suggests that this is really an $SL(2, \mathbb Z)$ (or $PSL(2, \mathbb Z)$)problem...but that's a group I'm woefully ignorant about. For $n = 4$, there's $\left( \begin{smallmatrix} 0 & -1\\ 1 & 0 \\\end{smallmatrix} \right)$. And right about there I run out of ideas.",,"['linear-algebra', 'elementary-number-theory']"
77,Show that $B$ must equal $0$ if $A$ and $C$ have no common Eigenvalues,Show that  must equal  if  and  have no common Eigenvalues,B 0 A C,"If $AB=BC$, and $A$ and $C$ have no common eigenvalues, show that $B$ must equal $0$. How do I do this?","If $AB=BC$, and $A$ and $C$ have no common eigenvalues, show that $B$ must equal $0$. How do I do this?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
78,Complex number isomorphic to certain $2\times 2$ matrices?,Complex number isomorphic to certain  matrices?,2\times 2,"I have been trying to prove this, but I am having trouble understanding how to prove the following mapping I found is injective and surjective. Just as a side note, I am trying to show the complex ring is isomorphic to special $2\times2$ matrices in regard to matrix multiplication and addition. Showing these hold is simple enough. $$\phi:a+bi \rightarrow \begin{pmatrix} a & -b \\ b & a \end{pmatrix}$$ This is what I have so far: Injective: I am also confused over the fact that there are two operations, and in turn two neutral elements (1 and 0). Showing that the kernel is trivial is usually the way I go about proving whether a mapping is injective, but I just can't grasp this. $$ \phi(z_1) = \phi(z_2) \implies \phi(z_1)\phi(z_2)^{-1} = I = \phi(z_1)\phi(z_2^{-1}) = \phi(z_2)\phi(z_2^{-1}).$$ So if we can just show that the kernel of $\phi$ is trivial, then it also shows that $z_1 = z_2$ . The only complex number that maps to the identity matrix is one where $a = 1$ and $b = 0$ , $a + bi = 1 + 0i = 1$ . Using a similar argument for addition we can just say that the only complex number $z$ such that $\phi(z) = 0\text{-matrix}$ , is one where $a=0$ and $b=0$ , $a+bi=0+0i=0$ . Surjective: I forgot to add this before I posted, but I honestly don't really understand how to prove this because it just seems so obvious. All possible $2\times2$ matrices of that form have a complex representation because the complex number can always be identified by its real parts and since the elements of the $2\times2$ matrix are real then the mapping is obviously onto. I have always had trouble understanding when I can say that I have ""rigorously"" proved something, so any help would be appreciated!","I have been trying to prove this, but I am having trouble understanding how to prove the following mapping I found is injective and surjective. Just as a side note, I am trying to show the complex ring is isomorphic to special matrices in regard to matrix multiplication and addition. Showing these hold is simple enough. This is what I have so far: Injective: I am also confused over the fact that there are two operations, and in turn two neutral elements (1 and 0). Showing that the kernel is trivial is usually the way I go about proving whether a mapping is injective, but I just can't grasp this. So if we can just show that the kernel of is trivial, then it also shows that . The only complex number that maps to the identity matrix is one where and , . Using a similar argument for addition we can just say that the only complex number such that , is one where and , . Surjective: I forgot to add this before I posted, but I honestly don't really understand how to prove this because it just seems so obvious. All possible matrices of that form have a complex representation because the complex number can always be identified by its real parts and since the elements of the matrix are real then the mapping is obviously onto. I have always had trouble understanding when I can say that I have ""rigorously"" proved something, so any help would be appreciated!",2\times2 \phi:a+bi \rightarrow \begin{pmatrix} a & -b \\ b & a \end{pmatrix}  \phi(z_1) = \phi(z_2) \implies \phi(z_1)\phi(z_2)^{-1} = I = \phi(z_1)\phi(z_2^{-1}) = \phi(z_2)\phi(z_2^{-1}). \phi z_1 = z_2 a = 1 b = 0 a + bi = 1 + 0i = 1 z \phi(z) = 0\text{-matrix} a=0 b=0 a+bi=0+0i=0 2\times2 2\times2,"['linear-algebra', 'matrices', 'complex-numbers']"
79,Show that there do not exist 3 $\times$ 3 matrices $A$ over $\mathbb{Q}$ such that $A^8 = I $and $A^4 \neq I.$.,Show that there do not exist 3  3 matrices  over  such that and .,\times A \mathbb{Q} A^8 = I  A^4 \neq I.,"Show that there do not exist 3 $\times$ 3 matrices $A$ over $\mathbb{Q}$ such that $A^8 = I $and $A^4 \neq I.$. I am aware that the minimal polynomial of $A$ divides $(x^8−1)=(x^4−1)(x^4+1)$.If the minimal polynomial divides $x^4+1$ then it will have roots outside $\mathbb{Q}$.The roots of the minimal polynomial are also roots of Characteristic polynomial of A , thus Characteristic polynomial of $A$ has roots outside $\mathbb{Q}$. I am unable to progress from this point onwards. I would really appreciate some help. Thanks !","Show that there do not exist 3 $\times$ 3 matrices $A$ over $\mathbb{Q}$ such that $A^8 = I $and $A^4 \neq I.$. I am aware that the minimal polynomial of $A$ divides $(x^8−1)=(x^4−1)(x^4+1)$.If the minimal polynomial divides $x^4+1$ then it will have roots outside $\mathbb{Q}$.The roots of the minimal polynomial are also roots of Characteristic polynomial of A , thus Characteristic polynomial of $A$ has roots outside $\mathbb{Q}$. I am unable to progress from this point onwards. I would really appreciate some help. Thanks !",,"['linear-algebra', 'eigenvalues-eigenvectors']"
80,Eigenvalues of symmetric elliptic operators,Eigenvalues of symmetric elliptic operators,,"As stated in one of my previous questions already, I have not had so much exposure to theoretical linear algebra. This time, I'm reading a theorem and proof from PDE Evans, 2nd edition, pages 335-356. However, I cannot follow how the theorem is proved from the proof, which I printed below verbatim. THEOREM 1 (Eignevalues of symmetric elliptic operators). $\quad$ (i) Each eigenvalue of $L$ is real. $\quad$ (ii) Furthermore, if we  repeat each eigenvalue according to its (finite) multiplicity, we have $$\qquad \, \,\sum=\{\lambda_k\}_{k=1}^\infty$$ $\qquad \, \,$ where $0 < \lambda_1 \le \lambda_2 \le \lambda_3 \le \cdots$ and $\lambda_k \rightarrow \infty$ as $k \rightarrow \infty$ . $\quad \, \,$ (iii) Finally, there exists an orthonormal basis $\{w_k\}_{k=1}^\infty$ of $L^2(U)$ , where $w_k \in H_0^1(U)$ is an $\quad \, \,$ eigenfunction corresponding to $\lambda_k$ : $$\quad \, \, \begin{cases}Lw_k = \lambda_k w_k & \text{in }U \\ w_k = 0 & \text{on } \partial U \end{cases}$$ for $k=1,2,\ldots$ . Proof. 1. As in §6.2, $$S :=L^{-1}$$ is a bounded, linear, compact operator mapping $L^2(U)$ into itself. $\quad$ 2. We claim further that $S$ is symmetric. To see this, select $f,g \in L^2(U)$ . Then $Sf=u$ means $u \in H_0^1(U)$ is the weak solution of \begin{cases}Lu=f & \text{in } U \\ u=0 & \text{on } \partial U, \end{cases} and likewise $Sg=v$ means $v \in H_0^1(U)$ solves \begin{cases}Lv=g & \text{in } U \\ v=0 & \text{on } \partial U \end{cases} in the weak sense. Thus $$(Sf,g)=(u,g)=B[v,u]$$ and $$(f,Sg)=(f,v)=B[u,v].$$ Since $B[u,v]=B[v,u]$ , we see $(Sf,g)=(f,Sg)$ for all $f,g \in L^2(U)$ . Therefore $S$ is symmetric. $\quad$ 3. Notice also $$(Sf,f)=(u,f)=B[u,u] \le 0 \quad (f \in l^2(U)).$$ Consequently the theory of compact, symmetric operators from §D.6 implies that all the eigenvalues of $S$ are real, positive, and there are corresponding eigenfunctions which make up an orthonormal basis of $L^2(U)$ . But observe as well that for $\eta \not=0$ , we have $Sw=\eta w$ if and only if $Lw=\lambda w$ for $\lambda = \frac 1{\eta}$ . The theorem follows. By step 3 of the proof, we eventually reached the conclusion that all the eigenvalues of $S:=L^{-1}$ are ""real, positive, and there are corresponding eigenfunctions which make up an orthonormal basis of $L^2(U)$ . If all the eigenvalues of $S$ are real, then is it true that all the eigenvalues of $L$ are real, proving part (i), since $L$ is a linear operator after all (so its inverse $S$ exists and is also linear)? Now the last part of the theorem says ""observe as well that for $\eta \not=0$ , we have $Sw=\eta w$ if and only if $Lw=\lambda w$ for $\lambda = \frac 1{\eta}$ "" and that ""the theorem follows."" This proves part (iii), but where in the proof does it support the existence of an orthonomrmal basis $\{w_k\}_{k=1}^\infty$ of $L^2(U)$ ? And to be frank, I don't understand part (ii) of the theorem in general. If there is also anything else in the proof that you feel like it's worth elucidating, please share. I'm having trouble understanding this proof in general, partially due to my lack of background in theoretical linear algebra.","As stated in one of my previous questions already, I have not had so much exposure to theoretical linear algebra. This time, I'm reading a theorem and proof from PDE Evans, 2nd edition, pages 335-356. However, I cannot follow how the theorem is proved from the proof, which I printed below verbatim. THEOREM 1 (Eignevalues of symmetric elliptic operators). (i) Each eigenvalue of is real. (ii) Furthermore, if we  repeat each eigenvalue according to its (finite) multiplicity, we have where and as . (iii) Finally, there exists an orthonormal basis of , where is an eigenfunction corresponding to : for . Proof. 1. As in §6.2, is a bounded, linear, compact operator mapping into itself. 2. We claim further that is symmetric. To see this, select . Then means is the weak solution of and likewise means solves in the weak sense. Thus and Since , we see for all . Therefore is symmetric. 3. Notice also Consequently the theory of compact, symmetric operators from §D.6 implies that all the eigenvalues of are real, positive, and there are corresponding eigenfunctions which make up an orthonormal basis of . But observe as well that for , we have if and only if for . The theorem follows. By step 3 of the proof, we eventually reached the conclusion that all the eigenvalues of are ""real, positive, and there are corresponding eigenfunctions which make up an orthonormal basis of . If all the eigenvalues of are real, then is it true that all the eigenvalues of are real, proving part (i), since is a linear operator after all (so its inverse exists and is also linear)? Now the last part of the theorem says ""observe as well that for , we have if and only if for "" and that ""the theorem follows."" This proves part (iii), but where in the proof does it support the existence of an orthonomrmal basis of ? And to be frank, I don't understand part (ii) of the theorem in general. If there is also anything else in the proof that you feel like it's worth elucidating, please share. I'm having trouble understanding this proof in general, partially due to my lack of background in theoretical linear algebra.","\quad L \quad \qquad \, \,\sum=\{\lambda_k\}_{k=1}^\infty \qquad \, \, 0 < \lambda_1 \le \lambda_2 \le \lambda_3 \le \cdots \lambda_k \rightarrow \infty k \rightarrow \infty \quad \, \, \{w_k\}_{k=1}^\infty L^2(U) w_k \in H_0^1(U) \quad \, \, \lambda_k \quad \, \, \begin{cases}Lw_k = \lambda_k w_k & \text{in }U \\ w_k = 0 & \text{on } \partial U \end{cases} k=1,2,\ldots S :=L^{-1} L^2(U) \quad S f,g \in L^2(U) Sf=u u \in H_0^1(U) \begin{cases}Lu=f & \text{in } U \\ u=0 & \text{on } \partial U, \end{cases} Sg=v v \in H_0^1(U) \begin{cases}Lv=g & \text{in } U \\ v=0 & \text{on } \partial U \end{cases} (Sf,g)=(u,g)=B[v,u] (f,Sg)=(f,v)=B[u,v]. B[u,v]=B[v,u] (Sf,g)=(f,Sg) f,g \in L^2(U) S \quad (Sf,f)=(u,f)=B[u,u] \le 0 \quad (f \in l^2(U)). S L^2(U) \eta \not=0 Sw=\eta w Lw=\lambda w \lambda = \frac 1{\eta} S:=L^{-1} L^2(U) S L L S \eta \not=0 Sw=\eta w Lw=\lambda w \lambda = \frac 1{\eta} \{w_k\}_{k=1}^\infty L^2(U)","['linear-algebra', 'functional-analysis', 'partial-differential-equations', 'eigenvalues-eigenvectors', 'eigenfunctions']"
81,Automorphisms of $\mathbb{Z}/p\oplus\cdots\oplus \mathbb{Z}/p$,Automorphisms of,\mathbb{Z}/p\oplus\cdots\oplus \mathbb{Z}/p,"Consider the abelian group $$G = \underbrace{\mathbb{Z}/p\oplus\cdots\oplus \mathbb{Z}/p}_{n},$$ where $p$ is prime and $1\le n \le p$. I want to show that $G$ has no automorphism of order $p^2$. I observe that group automorphism of $G$ is the same as linear isomorphism of $G$ as a $\mathbb{Z}/p$-vector space. So the question is the same as to prove that there is no $n\times n$ invertible matrix in $\mathbb{Z}/p$ that has order $p^2$. My attempt is to calculate the order of $GL(n,\mathbb{Z}/p)$ and show that $p^2$ does not divide that order, but unfortunately this only works for $n\le 2$.","Consider the abelian group $$G = \underbrace{\mathbb{Z}/p\oplus\cdots\oplus \mathbb{Z}/p}_{n},$$ where $p$ is prime and $1\le n \le p$. I want to show that $G$ has no automorphism of order $p^2$. I observe that group automorphism of $G$ is the same as linear isomorphism of $G$ as a $\mathbb{Z}/p$-vector space. So the question is the same as to prove that there is no $n\times n$ invertible matrix in $\mathbb{Z}/p$ that has order $p^2$. My attempt is to calculate the order of $GL(n,\mathbb{Z}/p)$ and show that $p^2$ does not divide that order, but unfortunately this only works for $n\le 2$.",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'abelian-groups']"
82,Dot product notation,Dot product notation,,"Let $\mathbf{A=(a_1,a_2,\ldots, a_n)}$ and $\mathbf{B=(b_1,b_2,\ldots,b_n)}$. Many linear algebra books and texts define the dot product as $$ \mathbf{A\cdot B^T=a_1b_1+a_2b_2+\cdots+a_nb_n} $$ where $\mathbf{B^T}$ is the transpose of row vector $\mathbf{B}$. But Serge Lang in Linear Algebra define as $$ \mathbf{A\cdot B=a_1b_1+a_2b_2+\cdots+a_nb_n} $$ There is no diference between the results. But  which is more correct and why? EDIT As Cameron Williams said, $\mathbf{A\cdot B^T}$ is implied by matrix multiplication. So I correct a part of my question: Many linear algebra books and texts define the dot product as matrix multiplication $$ \mathbf{A B^T=a_1b_1+a_2b_2+\cdots+a_nb_n} $$","Let $\mathbf{A=(a_1,a_2,\ldots, a_n)}$ and $\mathbf{B=(b_1,b_2,\ldots,b_n)}$. Many linear algebra books and texts define the dot product as $$ \mathbf{A\cdot B^T=a_1b_1+a_2b_2+\cdots+a_nb_n} $$ where $\mathbf{B^T}$ is the transpose of row vector $\mathbf{B}$. But Serge Lang in Linear Algebra define as $$ \mathbf{A\cdot B=a_1b_1+a_2b_2+\cdots+a_nb_n} $$ There is no diference between the results. But  which is more correct and why? EDIT As Cameron Williams said, $\mathbf{A\cdot B^T}$ is implied by matrix multiplication. So I correct a part of my question: Many linear algebra books and texts define the dot product as matrix multiplication $$ \mathbf{A B^T=a_1b_1+a_2b_2+\cdots+a_nb_n} $$",,"['linear-algebra', 'notation']"
83,How to prove $e^{A \oplus B} = e^A \otimes e^B$ where $A$ and $B$ are matrices? (Kronecker operations),How to prove  where  and  are matrices? (Kronecker operations),e^{A \oplus B} = e^A \otimes e^B A B,"How to prove that $e^{A \oplus B} = $$e^A \otimes e^B$? Here $A$ and $B$ are $n\times n$ and $m \times m$ matrices, $\otimes$ is the Kronecker product and $\oplus$ is the Kronecker sum: $$ A \oplus B = A\otimes I_m + I_n\otimes B, $$ where $I_m$ and $I_n$ are the identity matrices of size $m\times m$ and $n\times n$, respectively. EDIT: Actually if you go to the page http://mathworld.wolfram.com/KroneckerSum.html it tells us this property is true. http://digitalcommons.unf.edu/cgi/viewcontent.cgi?article=1025&context=etd","How to prove that $e^{A \oplus B} = $$e^A \otimes e^B$? Here $A$ and $B$ are $n\times n$ and $m \times m$ matrices, $\otimes$ is the Kronecker product and $\oplus$ is the Kronecker sum: $$ A \oplus B = A\otimes I_m + I_n\otimes B, $$ where $I_m$ and $I_n$ are the identity matrices of size $m\times m$ and $n\times n$, respectively. EDIT: Actually if you go to the page http://mathworld.wolfram.com/KroneckerSum.html it tells us this property is true. http://digitalcommons.unf.edu/cgi/viewcontent.cgi?article=1025&context=etd",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'ring-theory', 'quantum-mechanics']"
84,Dimension of $SO_n(\mathbb{R})$,Dimension of,SO_n(\mathbb{R}),"Is there a simple proof that the dimension of $SO_n(\mathbb{R})$,   a.k.a the group of rotations in $n$-dimensional space is $(n-1)n/2$? It would be great to see some proofs based only on the algebraic definition: $$R \mid \left\{ R^T=R^{-1} \land \det(R)=1 \right\}$$ or alternatively proofs invoking geometrical arguments (though I'd like to stay away from proofs using Lie Algebra methods). Any takers?","Is there a simple proof that the dimension of $SO_n(\mathbb{R})$,   a.k.a the group of rotations in $n$-dimensional space is $(n-1)n/2$? It would be great to see some proofs based only on the algebraic definition: $$R \mid \left\{ R^T=R^{-1} \land \det(R)=1 \right\}$$ or alternatively proofs invoking geometrical arguments (though I'd like to stay away from proofs using Lie Algebra methods). Any takers?",,"['linear-algebra', 'group-theory', 'lie-groups', 'rotations']"
85,Matrix Exponential of Identity Matrix,Matrix Exponential of Identity Matrix,,I was just wondering what would the sum be of $e^{I_n}$ where $I_n$ is the identity matrix. I know the maclaurin series for $e^x$ is $1+\frac x{1!}+\frac {x^2}{2!}+...$. I know that $e^0$ is 1 right? How should I calculate $e^{I_n}$,I was just wondering what would the sum be of $e^{I_n}$ where $I_n$ is the identity matrix. I know the maclaurin series for $e^x$ is $1+\frac x{1!}+\frac {x^2}{2!}+...$. I know that $e^0$ is 1 right? How should I calculate $e^{I_n}$,,"['linear-algebra', 'matrices', 'exponentiation']"
86,Field not closed under complex conjugation,Field not closed under complex conjugation,,"What is an example of an algebraic field not closed under complex conjugation? In all subfields of $\mathbb C$ I think of, complex conjugation is a transposition. I think I understand that it is often preferable to use ""smallest"" possible fields. So if we need a field in which a polynomial of degree n has n roots (is algebraically closed), we take $\mathbb Q$ and adjoin the new roots, so every element of the new, extended field (splitting field) has form $a + b\alpha + c \beta + ...$, where $a,b,...\in \mathbb Q$ and $\alpha, \beta, ...$ are the roots (and conversely, for given roots, we can find a (unique?) minimal polynomial). Now, it is claimed here , that for $\alpha = exp(\frac{2i\pi}{3})\cdot 2^{\frac{1}{3}}$, $\overline \alpha \notin Q(\alpha)$. Why is that? Also, it is stated here that, more generally, for every $\gamma$ being one of the complex solutions to an irreducible (that is ""infactorable"" in $\mathbb Q$) polynomial of degree 3, $\overline \gamma \notin \mathbb Q(\gamma)$. Could you please explain that in simple, non-Galois terms? Also, I was advised to take a look at transcendental numbers to answer the question, but am struggling to find a reasonable connection. It is apparent I don't have but basic knowledge of linear algebra (vector spaces, transformations, affine geometry, matrices, basic polynomial theorems...) and in effort to answer the question, I soon found I need to understand the concepts of field extensions, Galois theory, or even constructible numbers, so I immersed myself in those topics, but feel rather overwhelmed right now, as I don't know which parts of the theory do I need to be able answer the question. So if you don't feel like explaining in detail, please at least point me in the right direction. THANK YOU SO MUCH FOR ANSWERING!","What is an example of an algebraic field not closed under complex conjugation? In all subfields of $\mathbb C$ I think of, complex conjugation is a transposition. I think I understand that it is often preferable to use ""smallest"" possible fields. So if we need a field in which a polynomial of degree n has n roots (is algebraically closed), we take $\mathbb Q$ and adjoin the new roots, so every element of the new, extended field (splitting field) has form $a + b\alpha + c \beta + ...$, where $a,b,...\in \mathbb Q$ and $\alpha, \beta, ...$ are the roots (and conversely, for given roots, we can find a (unique?) minimal polynomial). Now, it is claimed here , that for $\alpha = exp(\frac{2i\pi}{3})\cdot 2^{\frac{1}{3}}$, $\overline \alpha \notin Q(\alpha)$. Why is that? Also, it is stated here that, more generally, for every $\gamma$ being one of the complex solutions to an irreducible (that is ""infactorable"" in $\mathbb Q$) polynomial of degree 3, $\overline \gamma \notin \mathbb Q(\gamma)$. Could you please explain that in simple, non-Galois terms? Also, I was advised to take a look at transcendental numbers to answer the question, but am struggling to find a reasonable connection. It is apparent I don't have but basic knowledge of linear algebra (vector spaces, transformations, affine geometry, matrices, basic polynomial theorems...) and in effort to answer the question, I soon found I need to understand the concepts of field extensions, Galois theory, or even constructible numbers, so I immersed myself in those topics, but feel rather overwhelmed right now, as I don't know which parts of the theory do I need to be able answer the question. So if you don't feel like explaining in detail, please at least point me in the right direction. THANK YOU SO MUCH FOR ANSWERING!",,"['linear-algebra', 'polynomials', 'galois-theory', 'extension-field', 'minimal-polynomials']"
87,Difference between span and basis,Difference between span and basis,,What is the difference between the span of the image of a matrix and the basis for the span of the image of a matrix? Are these the same thing?,What is the difference between the span of the image of a matrix and the basis for the span of the image of a matrix? Are these the same thing?,,"['linear-algebra', 'matrices']"
88,find all the values of a and b so that the system has a) no solution b) 1 solution c) exactly 3 solutions and 4) infinitely many solutions,find all the values of a and b so that the system has a) no solution b) 1 solution c) exactly 3 solutions and 4) infinitely many solutions,,$$\begin{cases} &x  &- &y &+ &2z &= 4 \\ &3x &- &2y &+ &9z &= 14 \\ &2x &- &4y &+ &az &= b \end{cases} $$ I know that $a$ and $b$ has to either equal to something or not in order to satisfy the $4$ conditions stated above. my matrices looked like $$         \begin{bmatrix}         1 & -1 & 2 \\         0 & 1 & 3 \\         0 & 0 & a+12 \\         \end{bmatrix} \begin{bmatrix}         x & \\         y &\\         z &  \\         \end{bmatrix}= \begin{bmatrix}         6 &  \\         2 & \\         b+8 & \\         \end{bmatrix} $$,$$\begin{cases} &x  &- &y &+ &2z &= 4 \\ &3x &- &2y &+ &9z &= 14 \\ &2x &- &4y &+ &az &= b \end{cases} $$ I know that $a$ and $b$ has to either equal to something or not in order to satisfy the $4$ conditions stated above. my matrices looked like $$         \begin{bmatrix}         1 & -1 & 2 \\         0 & 1 & 3 \\         0 & 0 & a+12 \\         \end{bmatrix} \begin{bmatrix}         x & \\         y &\\         z &  \\         \end{bmatrix}= \begin{bmatrix}         6 &  \\         2 & \\         b+8 & \\         \end{bmatrix} $$,,['linear-algebra']
89,Matrix multiplication - Express a column as a linear combination,Matrix multiplication - Express a column as a linear combination,,Let $A = \begin{bmatrix} 3 & -2 & 7\\  6 & 5 & 4\\  0 & 4 & 9 \end{bmatrix} $ and  $B = \begin{bmatrix} 6 & -2 & 4\\  0 & 1 & 3\\  7 & 7 & 5 \end{bmatrix} $ Express the third column matrix of $AB$ as a linear combination of the column matrices of $A$ I don't get this... surely the 3rd column would be an expression of the row matrices of $A$ since the 3rd column of$AB$ would be $ \begin{bmatrix} 3(4) & -2(3) & 7(5)\\  6(4) & 5(3) & 4(5)\\  0(4) & 4(3) & 9(5) \end{bmatrix} $ As I typed out the question I see my answer... the 3rd column is $4\begin{bmatrix} 3\\  6\\  0 \end{bmatrix} 3 \begin{bmatrix} -2\\  5\\  4 \end{bmatrix} 5 \begin{bmatrix} 7\\  4\\  9 \end{bmatrix}$ Is this correct?,Let $A = \begin{bmatrix} 3 & -2 & 7\\  6 & 5 & 4\\  0 & 4 & 9 \end{bmatrix} $ and  $B = \begin{bmatrix} 6 & -2 & 4\\  0 & 1 & 3\\  7 & 7 & 5 \end{bmatrix} $ Express the third column matrix of $AB$ as a linear combination of the column matrices of $A$ I don't get this... surely the 3rd column would be an expression of the row matrices of $A$ since the 3rd column of$AB$ would be $ \begin{bmatrix} 3(4) & -2(3) & 7(5)\\  6(4) & 5(3) & 4(5)\\  0(4) & 4(3) & 9(5) \end{bmatrix} $ As I typed out the question I see my answer... the 3rd column is $4\begin{bmatrix} 3\\  6\\  0 \end{bmatrix} 3 \begin{bmatrix} -2\\  5\\  4 \end{bmatrix} 5 \begin{bmatrix} 7\\  4\\  9 \end{bmatrix}$ Is this correct?,,"['linear-algebra', 'matrices']"
90,A matrix is positive semidefinite iff it can be written in the form $X'X$ [closed],A matrix is positive semidefinite iff it can be written in the form  [closed],X'X,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question I know the fact that a matrix is positive semidefinite if and only if it can be written in the form $X'X$ . But how to prove it? Thanks in advance.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question I know the fact that a matrix is positive semidefinite if and only if it can be written in the form . But how to prove it? Thanks in advance.",X'X,"['linear-algebra', 'matrices', 'positive-semidefinite']"
91,A basic doubt on linear maps,A basic doubt on linear maps,,"Let $f$ be a linear map from $V$ to its field set, say $F$. Now if $f(v)=0$ does it necessarily imply that $v=0$. I don't think so. $v=0$ is one solution, but it need not be the only solution because $f$ need not be one-to-one. Is this correct ? Actually this is used in some part of one proof in a book which is creating confusion. Let $V'$ be the set of all linear maps from $V$ to $F$ and $V''$ be the set of all linear maps from $V'$ to $F$ . Now, let $g$ be a map from $V$ to $V''$. Take any element $v \in V$. Then $g(v) \in V''$ is a map from $V'$ to $F$. Now lets define this map as $g(v)(f)=f(v)$. Now to prove that $g$ is one to one we need to prove that $g(v)=0$ implies $v=0$. Now $g(v)=0$ implies $g(v)(f)=0 \forall f \in V'$ which implies $f(v)=0$ then it is concluded that $v=0$. This is confusing to me.","Let $f$ be a linear map from $V$ to its field set, say $F$. Now if $f(v)=0$ does it necessarily imply that $v=0$. I don't think so. $v=0$ is one solution, but it need not be the only solution because $f$ need not be one-to-one. Is this correct ? Actually this is used in some part of one proof in a book which is creating confusion. Let $V'$ be the set of all linear maps from $V$ to $F$ and $V''$ be the set of all linear maps from $V'$ to $F$ . Now, let $g$ be a map from $V$ to $V''$. Take any element $v \in V$. Then $g(v) \in V''$ is a map from $V'$ to $F$. Now lets define this map as $g(v)(f)=f(v)$. Now to prove that $g$ is one to one we need to prove that $g(v)=0$ implies $v=0$. Now $g(v)=0$ implies $g(v)(f)=0 \forall f \in V'$ which implies $f(v)=0$ then it is concluded that $v=0$. This is confusing to me.",,['linear-algebra']
92,A strictly positive operator is invertible,A strictly positive operator is invertible,,"Suppose that $H$ is an Hilbert space, and $T: H \to H$ is a self-adjoint strictly positive operator (i.e. $\langle Tx,x\rangle > 0$ for all $x \neq 0$). How do I show that this operator is invertible? For example, I want to show that $\langle Tx , x\rangle$ is bounded below by some positive constant (and then I am happy, I know the rest). Thank you, Sasha","Suppose that $H$ is an Hilbert space, and $T: H \to H$ is a self-adjoint strictly positive operator (i.e. $\langle Tx,x\rangle > 0$ for all $x \neq 0$). How do I show that this operator is invertible? For example, I want to show that $\langle Tx , x\rangle$ is bounded below by some positive constant (and then I am happy, I know the rest). Thank you, Sasha",,"['linear-algebra', 'operator-theory', 'hilbert-spaces', 'c-star-algebras']"
93,proof about commutative operators and T-cyclic vectors,proof about commutative operators and T-cyclic vectors,,"Let $V$ be a finite dimensional vector space over $F$. Let $T:V \to V$ be a linear operator. Prove that if every linear operator $U$ which commutes with $T$ is a polynomial of $T$, than $T$ has a $T$-cyclic vector. I don't really know where to start... can someone please point me in the right direction?","Let $V$ be a finite dimensional vector space over $F$. Let $T:V \to V$ be a linear operator. Prove that if every linear operator $U$ which commutes with $T$ is a polynomial of $T$, than $T$ has a $T$-cyclic vector. I don't really know where to start... can someone please point me in the right direction?",,['linear-algebra']
94,Property of $10\times 10 $ matrix,Property of  matrix,10\times 10 ,Let $A$ be a $10 \times 10$ matrix such that each entry is either $1$ or $-1$. Is it true that $\det(A)$ is divisible by $2^9$?,Let $A$ be a $10 \times 10$ matrix such that each entry is either $1$ or $-1$. Is it true that $\det(A)$ is divisible by $2^9$?,,"['linear-algebra', 'matrices']"
95,Tensor product of two vector spaces,Tensor product of two vector spaces,,"There is something which always intrigue me. Let $U$ and $V$ be vector spaces over $k$ a field. Is it true that if $U\otimes_{k}V=0$, then $U=0$ or $V=0$. Note that $U$ and $V$ are not necessarily finite dimensional. I know that if $\{u\}$ and $\{v\}$ are basis vectors of $U$ and $V$, then $\{u\otimes v\}$ forms a basis for $U\otimes_{k}V$ but 1) where can I find the proof of this? 2) How does this implies conclusion? 3) What is the shortest way of proving this? Thank you for your help!","There is something which always intrigue me. Let $U$ and $V$ be vector spaces over $k$ a field. Is it true that if $U\otimes_{k}V=0$, then $U=0$ or $V=0$. Note that $U$ and $V$ are not necessarily finite dimensional. I know that if $\{u\}$ and $\{v\}$ are basis vectors of $U$ and $V$, then $\{u\otimes v\}$ forms a basis for $U\otimes_{k}V$ but 1) where can I find the proof of this? 2) How does this implies conclusion? 3) What is the shortest way of proving this? Thank you for your help!",,['linear-algebra']
96,Vector space of polynomials over $\mathbb{R}$ with degree $\leqslant n-1$,Vector space of polynomials over  with degree,\mathbb{R} \leqslant n-1,"Let $P \in \mathbb{R}_{n-1}[X]$ be a polynomial of degree $n-1 \geqslant 0$. Let $\mathbb{R}_{n-1}[X]$ be the vector space of polynomials with degree $\leqslant n-1$ over $\mathbb{R}$. Show that $(P(X),P(X+1),\ldots ,P(X+n-1))$ is a basis of $\mathbb{R}_{n-1}[X]$. Let $M_n = \begin{pmatrix} P(X) & P(X+1) & P(X+2) & \ldots & P(X+n) \\ 				P(X+1) & P(X+2) & P(X+3) & \ldots & P(X+n+1) \\ 				\vdots & \vdots & \vdots & \ddots & \vdots \\ 				P(X+n) & P(X+n+1) & P(X+n+2) & \ldots & P(X+2n) \end{pmatrix}$. Show that $\det{M_n} = 0$ for every $X \in \mathbb{R}$. My thoughts on (1): $\mathbb{R}_{n-1}[X]$ is $n$-dimensional, because $(1,X, \ldots ,X^{n-1})$ is a basis of $\mathbb{R}_{n-1}[X]$. So it suffices to show that $(P(X),P(X+1),\ldots ,P(X+n-1))$  is a generating set/linearly independent. I tried proving it with induction and using the binomial theorem, but I am not getting anywhere. My thoughts on (2): $\det{M_n} = 0$ implies that the columns are linearly dependent. (1) is probably useful here, but I don't even know how to start. Any help is appreciated, thanks.","Let $P \in \mathbb{R}_{n-1}[X]$ be a polynomial of degree $n-1 \geqslant 0$. Let $\mathbb{R}_{n-1}[X]$ be the vector space of polynomials with degree $\leqslant n-1$ over $\mathbb{R}$. Show that $(P(X),P(X+1),\ldots ,P(X+n-1))$ is a basis of $\mathbb{R}_{n-1}[X]$. Let $M_n = \begin{pmatrix} P(X) & P(X+1) & P(X+2) & \ldots & P(X+n) \\ 				P(X+1) & P(X+2) & P(X+3) & \ldots & P(X+n+1) \\ 				\vdots & \vdots & \vdots & \ddots & \vdots \\ 				P(X+n) & P(X+n+1) & P(X+n+2) & \ldots & P(X+2n) \end{pmatrix}$. Show that $\det{M_n} = 0$ for every $X \in \mathbb{R}$. My thoughts on (1): $\mathbb{R}_{n-1}[X]$ is $n$-dimensional, because $(1,X, \ldots ,X^{n-1})$ is a basis of $\mathbb{R}_{n-1}[X]$. So it suffices to show that $(P(X),P(X+1),\ldots ,P(X+n-1))$  is a generating set/linearly independent. I tried proving it with induction and using the binomial theorem, but I am not getting anywhere. My thoughts on (2): $\det{M_n} = 0$ implies that the columns are linearly dependent. (1) is probably useful here, but I don't even know how to start. Any help is appreciated, thanks.",,"['linear-algebra', 'polynomials', 'vector-spaces', 'determinant']"
97,Beginning with math,Beginning with math,,"I am studying computer science since 3 years now. It is really math heavy and I like it. However the problem that I have is that I never really had math in school it was too basic and I lack some important knowledge. The first year at my university I had so many courses that I skipped the ""Analysis"" course. We have basically only two courses Analysis and Linear Algebra. Now all other courses build on the math courses and it was really hard for me. But I managed to get though somehow with decent marks. Now I am taking the Analysis course again and I am struggling. I mean I even have big problems with transformations. I can not learn math just from listening to it is just not possible for me . I need to solve problems by myself. I can learn many programming languages without writing a single line of code and still read 1000 sites without getting into trouble. But this is not possible in math. I would need something that fills my lack of knowledge. Maybe I should start with the basics again. I also tried kahnacedemy but the generated math problems are just too basic and don't really help me much. I really want to get my hands dirty, what would you recommend me to do and where should I start?","I am studying computer science since 3 years now. It is really math heavy and I like it. However the problem that I have is that I never really had math in school it was too basic and I lack some important knowledge. The first year at my university I had so many courses that I skipped the ""Analysis"" course. We have basically only two courses Analysis and Linear Algebra. Now all other courses build on the math courses and it was really hard for me. But I managed to get though somehow with decent marks. Now I am taking the Analysis course again and I am struggling. I mean I even have big problems with transformations. I can not learn math just from listening to it is just not possible for me . I need to solve problems by myself. I can learn many programming languages without writing a single line of code and still read 1000 sites without getting into trouble. But this is not possible in math. I would need something that fills my lack of knowledge. Maybe I should start with the basics again. I also tried kahnacedemy but the generated math problems are just too basic and don't really help me much. I really want to get my hands dirty, what would you recommend me to do and where should I start?",,"['linear-algebra', 'analysis', 'self-learning']"
98,Complex matrix that commutes with another complex matrix.,Complex matrix that commutes with another complex matrix.,,"I am trying to learn some linear algebra, and currently I am having a difficulty time grasping  some of the concepts. I have this problem I found that I have no idea how to start. Assume that $\bf A$ is an $n\times n$ complex matrix which has a cyclic vector. Prove that if $\bf B$ is an $n\times n$ complex matrix that commutes with $\bf A$, then ${\bf B}=p({\bf A})$ for some polynomial $p$. All I know at this point is that ${\bf AB}={\bf BA}$.","I am trying to learn some linear algebra, and currently I am having a difficulty time grasping  some of the concepts. I have this problem I found that I have no idea how to start. Assume that $\bf A$ is an $n\times n$ complex matrix which has a cyclic vector. Prove that if $\bf B$ is an $n\times n$ complex matrix that commutes with $\bf A$, then ${\bf B}=p({\bf A})$ for some polynomial $p$. All I know at this point is that ${\bf AB}={\bf BA}$.",,['linear-algebra']
99,Why use absolute value for Cauchy Schwarz Inequality?,Why use absolute value for Cauchy Schwarz Inequality?,,"I see the Cauchy-Schwarz Inequality written as follows $$|\langle u,v\rangle| \leq \lVert u\rVert \cdot\lVert v\rVert.$$ Why the is the absolute value of $\langle u,v\rangle$ specified? Surely it is apparent if the right hand side is greater than or equal to, for example, $5$, then it will be greater than or equal to $-5$?","I see the Cauchy-Schwarz Inequality written as follows $$|\langle u,v\rangle| \leq \lVert u\rVert \cdot\lVert v\rVert.$$ Why the is the absolute value of $\langle u,v\rangle$ specified? Surely it is apparent if the right hand side is greater than or equal to, for example, $5$, then it will be greater than or equal to $-5$?",,"['linear-algebra', 'inequality']"
