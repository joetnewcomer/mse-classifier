,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Lipschitz continuity and $l^1$ norm,Lipschitz continuity and  norm,l^1,"Let $f: \mathbb{R}^2 \to \mathbb{R}^2$ be a function defind as follows: $$ f(x) =  \begin{cases}    x,        & \|x\| \leq 1 \\    x/ \|x\|, & \|x\| > 1 \end{cases} $$ where $\| \cdot \|$ is the $l^1$ norm. Then it can be shown, that $\|f(x) - f(y)\| \leq 2\|x-y\|$. I need to prove, that $2$ cannot be improved. How can I do that?","Let $f: \mathbb{R}^2 \to \mathbb{R}^2$ be a function defind as follows: $$ f(x) =  \begin{cases}    x,        & \|x\| \leq 1 \\    x/ \|x\|, & \|x\| > 1 \end{cases} $$ where $\| \cdot \|$ is the $l^1$ norm. Then it can be shown, that $\|f(x) - f(y)\| \leq 2\|x-y\|$. I need to prove, that $2$ cannot be improved. How can I do that?",,"['functional-analysis', 'normed-spaces', 'lipschitz-functions']"
1,How does Lebesgue integral handle functions with non-zero infimum?,How does Lebesgue integral handle functions with non-zero infimum?,,"How do Lebesgue integral handle functions with non-zero intercept? For example, take dyadic coefficients. The $blue$ boxes are summed by$$\sum_ka_k1_{A_k}$$ but what about the area coloured $green$? Only $\emptyset$ is mapped to that part of $A_k$'s so the simple function doesn't give any weight on those $A_k$'s. However to get an ""area under curve"", one should include that part to preserve the notion of integration.","How do Lebesgue integral handle functions with non-zero intercept? For example, take dyadic coefficients. The $blue$ boxes are summed by$$\sum_ka_k1_{A_k}$$ but what about the area coloured $green$? Only $\emptyset$ is mapped to that part of $A_k$'s so the simple function doesn't give any weight on those $A_k$'s. However to get an ""area under curve"", one should include that part to preserve the notion of integration.",,"['real-analysis', 'probability', 'integration', 'functional-analysis', 'measure-theory']"
2,Show the continuous embedding $ \ell^2 \subseteq c_0. $,Show the continuous embedding, \ell^2 \subseteq c_0. ,"Show the continuous embedding $$ \ell^2 \subseteq c_0. $$ I couldn't find a question that showed this ""simple"" proof, and I'm having trouble doing it myself. For normed spaces $\mathcal{X},\mathcal{Y}$, let $x \in \mathcal{X}$. Then \begin{equation}    \mathcal{X} \subseteq \mathcal{Y} \iff \exists c > 0 \;:\;\Vert x \Vert_{\mathcal{Y}} \le c \Vert x \Vert_{\mathcal{X}} \quad \forall x\in\mathcal{X}. \end{equation} We say normed space $\mathcal{X}$ continuously embeds into normed space $\mathcal{Y}$. The sequence spaces $\ell^2,c_0$ have norms $$\Vert x \Vert_2 = \left( \sum_{n} |x_n|^2 \right)^{1/2}$$ and $$\Vert x \Vert_{c_0} = \sup_{n} |x_n|,$$ respectively. We say $x \in \ell^2 \iff \Vert x \Vert_2 < \infty$ and $x \in c_0 \iff \lim_{n\to\infty} x_n = 0$. My attempt: Suppose $x \in \ell^2$. Then $\lim_{n \to \infty} |x_n| = 0$. Hence, there exists $k$ such that $x_k = \sup_{n} |x_n|$. Therefore, \begin{align}    \mathrm{LHS}    & = \sup_{n} |x_n| \\    & = |x_k| \\    & \le \sum_{n} |x_n| \text{ since $|x_n| \ge 0$ for $n\ne k$} \\    & \le c \left( \sum_{n} |x_n|^2 \right)^{1/2} \end{align} since it is true that for all $u,v\in\mathbb{R}$, $u \ge 0$, $v \ge 0$, there exists $c \ge 0$ such that $u \le cv$. Hence, $\ell^2 \subseteq c_0$. Is the property of real numbers that I used enough to justify the ending of my ""proof""?","Show the continuous embedding $$ \ell^2 \subseteq c_0. $$ I couldn't find a question that showed this ""simple"" proof, and I'm having trouble doing it myself. For normed spaces $\mathcal{X},\mathcal{Y}$, let $x \in \mathcal{X}$. Then \begin{equation}    \mathcal{X} \subseteq \mathcal{Y} \iff \exists c > 0 \;:\;\Vert x \Vert_{\mathcal{Y}} \le c \Vert x \Vert_{\mathcal{X}} \quad \forall x\in\mathcal{X}. \end{equation} We say normed space $\mathcal{X}$ continuously embeds into normed space $\mathcal{Y}$. The sequence spaces $\ell^2,c_0$ have norms $$\Vert x \Vert_2 = \left( \sum_{n} |x_n|^2 \right)^{1/2}$$ and $$\Vert x \Vert_{c_0} = \sup_{n} |x_n|,$$ respectively. We say $x \in \ell^2 \iff \Vert x \Vert_2 < \infty$ and $x \in c_0 \iff \lim_{n\to\infty} x_n = 0$. My attempt: Suppose $x \in \ell^2$. Then $\lim_{n \to \infty} |x_n| = 0$. Hence, there exists $k$ such that $x_k = \sup_{n} |x_n|$. Therefore, \begin{align}    \mathrm{LHS}    & = \sup_{n} |x_n| \\    & = |x_k| \\    & \le \sum_{n} |x_n| \text{ since $|x_n| \ge 0$ for $n\ne k$} \\    & \le c \left( \sum_{n} |x_n|^2 \right)^{1/2} \end{align} since it is true that for all $u,v\in\mathbb{R}$, $u \ge 0$, $v \ge 0$, there exists $c \ge 0$ such that $u \le cv$. Hence, $\ell^2 \subseteq c_0$. Is the property of real numbers that I used enough to justify the ending of my ""proof""?",,"['sequences-and-series', 'functional-analysis']"
3,Convergence in quotient space $X/Y$,Convergence in quotient space,X/Y,"Let $Y$ is closed subspace of $X$ and $(x_n+Y)\rightarrow (x+Y)$ in $X/Y$ for some $x_n,x\in X$. Question is to see that there exist $y_n\in Y$ such that $x_n+y_n\rightarrow x$. This seems to be true but i could not give a proof. Given $\epsilon>0$ there exists $N\in \mathbb{N}$ such that $||x_n+Y-(x+Y)||<\epsilon$ for all $n\geq N$ Fix $n\geq N$ then $||(x_n-x)+Y||<\epsilon$ i.e., $\inf\{||x_n-x+y||:y\in Y\}<\epsilon$  so, I can get $y_n\in Y$ such that $||x_n-x+y_n||<\epsilon$. I thought this sequence $(y_n)_{n\geq N}$ adding some random $y_1,\cdots,y_{N-1}$ would work. But then i see that these $y_n$ even for $n\geq N$  depends on $\epsilon$. Suggest some hints to get rid of this $\epsilon$.","Let $Y$ is closed subspace of $X$ and $(x_n+Y)\rightarrow (x+Y)$ in $X/Y$ for some $x_n,x\in X$. Question is to see that there exist $y_n\in Y$ such that $x_n+y_n\rightarrow x$. This seems to be true but i could not give a proof. Given $\epsilon>0$ there exists $N\in \mathbb{N}$ such that $||x_n+Y-(x+Y)||<\epsilon$ for all $n\geq N$ Fix $n\geq N$ then $||(x_n-x)+Y||<\epsilon$ i.e., $\inf\{||x_n-x+y||:y\in Y\}<\epsilon$  so, I can get $y_n\in Y$ such that $||x_n-x+y_n||<\epsilon$. I thought this sequence $(y_n)_{n\geq N}$ adding some random $y_1,\cdots,y_{N-1}$ would work. But then i see that these $y_n$ even for $n\geq N$  depends on $\epsilon$. Suggest some hints to get rid of this $\epsilon$.",,[]
4,Linear operator as sum of linear functionals?,Linear operator as sum of linear functionals?,,"Is it possible to get an analouge of the matrix represenation of a linear operator on finite dimensional space for general linear operators on Banach spaces. The matrix represenation look like a ""sum of functionals"" hence maybe some kind of integral operators with certain kernals? I think I did an exam question which looks like this but I cant rememeber the formulation or find the exam. Furthermore, given that I remember correctly what is the widest class of operators for which this is possible? Found possible duplicate ; Can all continuous linear operators on a function space be represented using integrals?","Is it possible to get an analouge of the matrix represenation of a linear operator on finite dimensional space for general linear operators on Banach spaces. The matrix represenation look like a ""sum of functionals"" hence maybe some kind of integral operators with certain kernals? I think I did an exam question which looks like this but I cant rememeber the formulation or find the exam. Furthermore, given that I remember correctly what is the widest class of operators for which this is possible? Found possible duplicate ; Can all continuous linear operators on a function space be represented using integrals?",,"['functional-analysis', 'soft-question']"
5,I - T not isomorphism in non-Banach normed spaces,I - T not isomorphism in non-Banach normed spaces,,"So I know that if $T : X \to X$ is a bounded operator with $X$ a Banach space and that $\|T\| < 1$, then $I - T$ is an isomorphism. Is this true in general normed spaces?","So I know that if $T : X \to X$ is a bounded operator with $X$ a Banach space and that $\|T\| < 1$, then $I - T$ is an isomorphism. Is this true in general normed spaces?",,"['functional-analysis', 'operator-theory']"
6,What is the coproduct in the category of commutative unital C*-Algebras?,What is the coproduct in the category of commutative unital C*-Algebras?,,"Let $A$ be a commutative unital C*-Algebra and let $X= Spec(A) $ be the corresponding compact Hausdorff space of characters. By Gelfand-Naimark duality we know that $$ X \times X = Spec(A \coprod A) $$ or in other words $$ A \coprod A = \mathcal{C}(X \times X).$$ Is there an algebraic way to construct the coproduct, as for example some form of topological tensor product or similar?","Let $A$ be a commutative unital C*-Algebra and let $X= Spec(A) $ be the corresponding compact Hausdorff space of characters. By Gelfand-Naimark duality we know that $$ X \times X = Spec(A \coprod A) $$ or in other words $$ A \coprod A = \mathcal{C}(X \times X).$$ Is there an algebraic way to construct the coproduct, as for example some form of topological tensor product or similar?",,"['abstract-algebra', 'general-topology', 'functional-analysis', 'category-theory']"
7,Adjoint operator of generalized Volterra operator $\int_{0}^{t} f(t-s)g(s) ds$,Adjoint operator of generalized Volterra operator,\int_{0}^{t} f(t-s)g(s) ds,"Let $f \in C(\mathbb{R})$, $g\in L^2([0,a])$ (where $a$ is finite) and we define  $(Vg)(t):=\int_{0}^{t} f(t-s)g(s) ds.$ Then I want to find the operator $V^*,$ i.e. $$\langle Vg,h \rangle_{L^2[0,a]} = \langle g,V^*h \rangle_{L^2[0,a]}.$$ This operator somehow reminded me of the Volterra operator, but now there is  a mean integral kernel there additionally. Does anybody know how to determine this one?","Let $f \in C(\mathbb{R})$, $g\in L^2([0,a])$ (where $a$ is finite) and we define  $(Vg)(t):=\int_{0}^{t} f(t-s)g(s) ds.$ Then I want to find the operator $V^*,$ i.e. $$\langle Vg,h \rangle_{L^2[0,a]} = \langle g,V^*h \rangle_{L^2[0,a]}.$$ This operator somehow reminded me of the Volterra operator, but now there is  a mean integral kernel there additionally. Does anybody know how to determine this one?",,"['calculus', 'real-analysis', 'functional-analysis', 'analysis', 'partial-differential-equations']"
8,$f\in L^1_{\textrm{loc}}(U)$ with $ \int_Ufg\ dx=0 $ for every $g\in C_c^\infty(U)$ implies $f=0$ a.e.,with  for every  implies  a.e.,f\in L^1_{\textrm{loc}}(U)  \int_Ufg\ dx=0  g\in C_c^\infty(U) f=0,"Suppose $U$ is an open subset of $\mathbb{R}^n$, and $f:U\to\mathbb{R}$ is a measurable [Edited: locally integrable] function such that   $$ \int_Ufg\ dx=0,\quad \textrm{for every } g\in C_c^\infty(U). $$    Then $f=0$ a.e. Suppose otherwise $f\neq 0$ on some measurable subset $V$ of $U$ such that $V$ has positive Lebesgue measure. To get a contradiction, how shall I handle the sign of $f$ on $V$? [Added:] Could anyone come up with a handy theorem that gives the above proposition? [Added:] This argument is used in the proof of uniqueness of weak derivatives in Evans's Partial Differential Equations:","Suppose $U$ is an open subset of $\mathbb{R}^n$, and $f:U\to\mathbb{R}$ is a measurable [Edited: locally integrable] function such that   $$ \int_Ufg\ dx=0,\quad \textrm{for every } g\in C_c^\infty(U). $$    Then $f=0$ a.e. Suppose otherwise $f\neq 0$ on some measurable subset $V$ of $U$ such that $V$ has positive Lebesgue measure. To get a contradiction, how shall I handle the sign of $f$ on $V$? [Added:] Could anyone come up with a handy theorem that gives the above proposition? [Added:] This argument is used in the proof of uniqueness of weak derivatives in Evans's Partial Differential Equations:",,"['real-analysis', 'functional-analysis']"
9,"Find the spectrum of compact operator ""min""","Find the spectrum of compact operator ""min""",,"Consider the (compact) operator $T:C([0,1],\mathbb{R})\rightarrow C([0,1],\mathbb{R})$ s.t. \begin{equation} T(f)(x)=\int_0^1\min\{x,y\} f(y)dy \; . \end{equation} How could one find its spectrum? I have tried to impose $Tf-\lambda f \equiv 0$ but then I couldn't solve anything...","Consider the (compact) operator $T:C([0,1],\mathbb{R})\rightarrow C([0,1],\mathbb{R})$ s.t. \begin{equation} T(f)(x)=\int_0^1\min\{x,y\} f(y)dy \; . \end{equation} How could one find its spectrum? I have tried to impose $Tf-\lambda f \equiv 0$ but then I couldn't solve anything...",,"['real-analysis', 'functional-analysis', 'eigenvalues-eigenvectors', 'spectral-theory', 'compact-operators']"
10,$L^2$ operator norm and convergence,operator norm and convergence,L^2,"Question : Consider $T_n : L^2[-\pi,\pi] \to L^2[-\pi,\pi]: u(x) \mapsto  \displaystyle \int_{-\pi}^{\pi} \left( e^{\frac{x}{n} - \frac{y}{n}} \right) u(y) \; dy$, for $n \in \mathbb{Z}^{\ge 1}$. (a) Is $T_n$ compact? Self-adjoint? (b) Calculate $\|T_n\|_{op}$. (c) Does $T_n$ converge? If so, to what and in what (operator) sense? Solution : (a) Compactness - Notice that $T_n (u) (x) = e^{\frac{x}{n}} \displaystyle \int_{-\pi}^\pi e^{-\frac{y}{n} } u(y) dy$, so ran$(T_n)$ is contained in the finite dimensional subspace spanned by $e^{x/n}$, meaning it's compact. I calculated $T_n^*(u)(x) = \displaystyle \int_{-\pi}^{\pi} \left( e^{\frac{y}{n} - \frac{x}{n}} \right) u(y) \; dy$, so it's not self-adjoint. (b) I'm stuck here. I can use Cauchy Schwarz to show that $\|T_n\|_{op} \le \|K\|_{L^2[-\pi,\pi]}$, where $K(x,y) = e^{x/n-y/n}$. But any attempts at the reverse inequality have not been successful . . . (c) It converges to the identity . . . I think strongly. Because of LDCT I can push a limit inside the integral. Uniformly, I'm unsure. This is an old test problem, so part of why I ask this question is because I will have to answer similar questions on the exam in a timely manner. I could maybe mess around with part (c) for a while and come up with some function $u(y)$ that demonstrates non-uniform convergence, but I'm curious if there's a better way to go about it. I can't think of any examples now. Please let me know if my work is correct or you can offer help. Thank you","Question : Consider $T_n : L^2[-\pi,\pi] \to L^2[-\pi,\pi]: u(x) \mapsto  \displaystyle \int_{-\pi}^{\pi} \left( e^{\frac{x}{n} - \frac{y}{n}} \right) u(y) \; dy$, for $n \in \mathbb{Z}^{\ge 1}$. (a) Is $T_n$ compact? Self-adjoint? (b) Calculate $\|T_n\|_{op}$. (c) Does $T_n$ converge? If so, to what and in what (operator) sense? Solution : (a) Compactness - Notice that $T_n (u) (x) = e^{\frac{x}{n}} \displaystyle \int_{-\pi}^\pi e^{-\frac{y}{n} } u(y) dy$, so ran$(T_n)$ is contained in the finite dimensional subspace spanned by $e^{x/n}$, meaning it's compact. I calculated $T_n^*(u)(x) = \displaystyle \int_{-\pi}^{\pi} \left( e^{\frac{y}{n} - \frac{x}{n}} \right) u(y) \; dy$, so it's not self-adjoint. (b) I'm stuck here. I can use Cauchy Schwarz to show that $\|T_n\|_{op} \le \|K\|_{L^2[-\pi,\pi]}$, where $K(x,y) = e^{x/n-y/n}$. But any attempts at the reverse inequality have not been successful . . . (c) It converges to the identity . . . I think strongly. Because of LDCT I can push a limit inside the integral. Uniformly, I'm unsure. This is an old test problem, so part of why I ask this question is because I will have to answer similar questions on the exam in a timely manner. I could maybe mess around with part (c) for a while and come up with some function $u(y)$ that demonstrates non-uniform convergence, but I'm curious if there's a better way to go about it. I can't think of any examples now. Please let me know if my work is correct or you can offer help. Thank you",,"['functional-analysis', 'operator-theory', 'lp-spaces', 'compact-operators']"
11,Adjoint Operator and Subspaces of Hilbert Spaces,Adjoint Operator and Subspaces of Hilbert Spaces,,"Let $H_1$ , $H_2$  Hilbert Spaces with $T:H_1 \to H_2$ adjoint operator  and $M_1 < H_1$ , $M_2 < H_2$ their subspaces. Show that: $T(M_1) \subset M_2 $ if, and only if, $T^*({M_2}^{\perp}) \subset {M_1}^{\perp} $","Let $H_1$ , $H_2$  Hilbert Spaces with $T:H_1 \to H_2$ adjoint operator  and $M_1 < H_1$ , $M_2 < H_2$ their subspaces. Show that: $T(M_1) \subset M_2 $ if, and only if, $T^*({M_2}^{\perp}) \subset {M_1}^{\perp} $",,"['functional-analysis', 'hilbert-spaces', 'adjoint-operators']"
12,$H^1(\mathbb{R}^3)$ vs $H^1_0(\mathbb{R}^3\!\setminus\!\{0\})$,vs,H^1(\mathbb{R}^3) H^1_0(\mathbb{R}^3\!\setminus\!\{0\}),"I would like to understand whether the spaces $H^1(\mathbb{R}^3)$ and $H^1_0(\mathbb{R}^3\!\setminus\!\{0\})$ are the same or not. The first space is the standard Sobolev space, for which one also has $H^1(\mathbb{R}^3)=\overline{C^\infty_0(\mathbb{R}^3)}^{\|\,\|_{H^1}}$, that is, the closure in the $H^1$-norm of the smooth functions compactly supported away from the origin. The second space is, by definition, $H^1_0(\mathbb{R}^3\!\setminus\!\{0\})=\overline{C^\infty_0(\mathbb{R}^3\!\setminus\!\{0\})}^{\|\,\|_{H^1}}$. Clearly, $H^1_0(\mathbb{R}^3\!\setminus\!\{0\})$ is a closed subspace of $H^1(\mathbb{R}^3)$. Thus, an equivalent version of the question is: is the space $C^\infty_0(\mathbb{R}^3\!\setminus\!\{0\})$ dense in $H^1(\mathbb{R}^3)$ ?","I would like to understand whether the spaces $H^1(\mathbb{R}^3)$ and $H^1_0(\mathbb{R}^3\!\setminus\!\{0\})$ are the same or not. The first space is the standard Sobolev space, for which one also has $H^1(\mathbb{R}^3)=\overline{C^\infty_0(\mathbb{R}^3)}^{\|\,\|_{H^1}}$, that is, the closure in the $H^1$-norm of the smooth functions compactly supported away from the origin. The second space is, by definition, $H^1_0(\mathbb{R}^3\!\setminus\!\{0\})=\overline{C^\infty_0(\mathbb{R}^3\!\setminus\!\{0\})}^{\|\,\|_{H^1}}$. Clearly, $H^1_0(\mathbb{R}^3\!\setminus\!\{0\})$ is a closed subspace of $H^1(\mathbb{R}^3)$. Thus, an equivalent version of the question is: is the space $C^\infty_0(\mathbb{R}^3\!\setminus\!\{0\})$ dense in $H^1(\mathbb{R}^3)$ ?",,"['functional-analysis', 'sobolev-spaces']"
13,Extreme points of the unit ball of the space $c_0 = \{ \{x_n\}_{n=1}^\infty \in \ell^\infty : \lim_{n\to\infty} x_n = 0\}$,Extreme points of the unit ball of the space,c_0 = \{ \{x_n\}_{n=1}^\infty \in \ell^\infty : \lim_{n\to\infty} x_n = 0\},"I want to prove that all ""closed unit ball"" of $$ c_0 = \{ \{x_n\}_{n=1}^\infty \in \ell^\infty : \lim_{n\to\infty} x_n = 0\} $$ do not have any extreme point. Would you please help me? (Extreme Point) Let $X$ be a vector space and $A \subset X$ be convex. We say $x\in A$ is an extreme point if for $x = (1-t)y + tz,\; y,z,\in A, \;t\in(0,1)$ then $y = z = x$. What I tried is as follows: Let $B$ be a closed unit ball of $c_0$, that is, $$B = \{\{x_n\}_{n=1}^\infty \in \ell^\infty : \lim_{n\to \infty} x_n = 0 \text{ and } \|x\|_{\ell^\infty}\le 1\}.$$ If there is a extreme point $b = \{b_n\}_{n=1}^\infty\in B$, then we have for $$ b = (1-t)y + tz, \quad y,z\in B,\quad t\in (0,1) $$ implies $$ y = z = b. $$ But I cannot do anymore here. Would you please help me?","I want to prove that all ""closed unit ball"" of $$ c_0 = \{ \{x_n\}_{n=1}^\infty \in \ell^\infty : \lim_{n\to\infty} x_n = 0\} $$ do not have any extreme point. Would you please help me? (Extreme Point) Let $X$ be a vector space and $A \subset X$ be convex. We say $x\in A$ is an extreme point if for $x = (1-t)y + tz,\; y,z,\in A, \;t\in(0,1)$ then $y = z = x$. What I tried is as follows: Let $B$ be a closed unit ball of $c_0$, that is, $$B = \{\{x_n\}_{n=1}^\infty \in \ell^\infty : \lim_{n\to \infty} x_n = 0 \text{ and } \|x\|_{\ell^\infty}\le 1\}.$$ If there is a extreme point $b = \{b_n\}_{n=1}^\infty\in B$, then we have for $$ b = (1-t)y + tz, \quad y,z\in B,\quad t\in (0,1) $$ implies $$ y = z = b. $$ But I cannot do anymore here. Would you please help me?",,"['real-analysis', 'functional-analysis', 'convex-analysis']"
14,$G$ is dense in $X^*$ in weak* sense if and only if $G$ is total set,is dense in  in weak* sense if and only if  is total set,G X^* G,"I have some question on functional analysis. Recently, I'm reading an article of Coifman and Weiss, ""Extensions of hardy spaces and their use in analysis"". They proved some important theorem to me by using the following functional analysis fact without proof. Exercise (Dunford&Schwartz, p.439, #41). Let $X$ be a locally convex linear topological space and let $G$ be a linear subspace of $X^*$. Then $G$ is $X$-dense in $X^*$  if and only if $G$ is a total set of functionals on $X$. In the case of Banach space, I can prove the one direction ($G$ is a total implies $G$ is dense in $X^*$ in the sense of weak* topology). First, I proved that if $N$ is a subspace of $X^*$, then $(^\bot N)^\bot = \overline{N}^*$, where $-^*$ denotes the weak* closure.  Here I follow Rudin's notation. So the result of one direction is proved one takes $^\bot N=\{0\}$. But I fail to prove its reverse direction and I cannot extend to the locally convex linear topological space case. Thank you in advance.","I have some question on functional analysis. Recently, I'm reading an article of Coifman and Weiss, ""Extensions of hardy spaces and their use in analysis"". They proved some important theorem to me by using the following functional analysis fact without proof. Exercise (Dunford&Schwartz, p.439, #41). Let $X$ be a locally convex linear topological space and let $G$ be a linear subspace of $X^*$. Then $G$ is $X$-dense in $X^*$  if and only if $G$ is a total set of functionals on $X$. In the case of Banach space, I can prove the one direction ($G$ is a total implies $G$ is dense in $X^*$ in the sense of weak* topology). First, I proved that if $N$ is a subspace of $X^*$, then $(^\bot N)^\bot = \overline{N}^*$, where $-^*$ denotes the weak* closure.  Here I follow Rudin's notation. So the result of one direction is proved one takes $^\bot N=\{0\}$. But I fail to prove its reverse direction and I cannot extend to the locally convex linear topological space case. Thank you in advance.",,"['functional-analysis', 'harmonic-analysis']"
15,The convergence of unordered sum,The convergence of unordered sum,,"$X$ is an arbitrary normed vector space. $A=\{x_i\in X\mid i\in J\}$ is an indexed set. $J$ contains the indices and is uncountably infinite. Let $\mathcal F=\{F\mid F\subseteq J, F \text{ is finite}\}$. The following definition of the convergence of unordered sum is taken from https://www.math.ucdavis.edu/~hunter/m201b_old/sums.pdf . We claim that the unordered sum $\sum \limits_{i\in J}x_i$ converges if and only if $$\exists x\in X,\ \forall \varepsilon>0,\ \exists F_\varepsilon \in \mathcal F,\ \forall F_\varepsilon \subseteq F\in \mathcal F,\ \left\|\sum_{i\in F} x_i-x\right\|\leq \varepsilon.$$ Suppose the unordered sum $\sum \limits_{i\in J} x_i$ converges. Let $I\subseteq J$. Does $\sum \limits_{i\in I}x_i$ converge? (Note that it may converge to a different point) How to prove it? Thanks!","$X$ is an arbitrary normed vector space. $A=\{x_i\in X\mid i\in J\}$ is an indexed set. $J$ contains the indices and is uncountably infinite. Let $\mathcal F=\{F\mid F\subseteq J, F \text{ is finite}\}$. The following definition of the convergence of unordered sum is taken from https://www.math.ucdavis.edu/~hunter/m201b_old/sums.pdf . We claim that the unordered sum $\sum \limits_{i\in J}x_i$ converges if and only if $$\exists x\in X,\ \forall \varepsilon>0,\ \exists F_\varepsilon \in \mathcal F,\ \forall F_\varepsilon \subseteq F\in \mathcal F,\ \left\|\sum_{i\in F} x_i-x\right\|\leq \varepsilon.$$ Suppose the unordered sum $\sum \limits_{i\in J} x_i$ converges. Let $I\subseteq J$. Does $\sum \limits_{i\in I}x_i$ converge? (Note that it may converge to a different point) How to prove it? Thanks!",,"['real-analysis', 'functional-analysis']"
16,Resolvent of a Self-Adjoint Operator,Resolvent of a Self-Adjoint Operator,,"Let $\mathbb{H}$ be a Hilbert space and $A$ a self-adjoint operator with domain $D(A) \subset \mathbb{H}$.. Suppose that the spectrum $\sigma(A)$ of $A$ is contained in $[0,\infty)$. Let $R_{A}(z)$ be the resolvent of $A$. I am looking for an elementary proof of the fact that for every $\lambda > 0$: \begin{equation} ||R_{A}(-\lambda)|| \leq \lambda . \end{equation} I found this inequality in Hoslip & Sigal, Introduction to Spectral Theory, Sect. 5.2, where it is stated without proof. Thank you very much in advance for your help. PS I tried the following line of proof. We have for every $\phi \in D_{A}$: \begin{equation} ||(A+\lambda) \phi||^2 = ||A\phi||^2 + 2 \lambda (A\phi,\phi) + \lambda^2 ||\phi||^2. \end{equation} So the result would immediately follow if we could prove that $(A\phi,\phi) \geq 0$, that is if we could prove that $A$ is positive. Even though I suspect this is true, I cannot see a simple way to prove this.","Let $\mathbb{H}$ be a Hilbert space and $A$ a self-adjoint operator with domain $D(A) \subset \mathbb{H}$.. Suppose that the spectrum $\sigma(A)$ of $A$ is contained in $[0,\infty)$. Let $R_{A}(z)$ be the resolvent of $A$. I am looking for an elementary proof of the fact that for every $\lambda > 0$: \begin{equation} ||R_{A}(-\lambda)|| \leq \lambda . \end{equation} I found this inequality in Hoslip & Sigal, Introduction to Spectral Theory, Sect. 5.2, where it is stated without proof. Thank you very much in advance for your help. PS I tried the following line of proof. We have for every $\phi \in D_{A}$: \begin{equation} ||(A+\lambda) \phi||^2 = ||A\phi||^2 + 2 \lambda (A\phi,\phi) + \lambda^2 ||\phi||^2. \end{equation} So the result would immediately follow if we could prove that $(A\phi,\phi) \geq 0$, that is if we could prove that $A$ is positive. Even though I suspect this is true, I cannot see a simple way to prove this.",,"['functional-analysis', 'operator-theory']"
17,Weak solution for equation $-\Delta u = f$.,Weak solution for equation .,-\Delta u = f,"Suppose $f \in L^{2}$ . I know that $$\left\{\begin{array}{c} −\Delta u = f(x) & \text{on }\Omega \\ u(x)=0 &  \text{on } \partial\Omega \end{array}\right.,$$ where $\Omega \subset \mathbb{R}^{N}$ is open and bounded, has a weak solution $u \in H^{1}_{0}(\Omega)$ . I can obtain this solution using the Riesz Representation Theorem. But, how obtain a weak soution for this equation with Neumann boundary conditions? And in the case where $\Omega = \mathbb{R}^{N}$ ? In both cases I have been tried use the Lax Milgran Theorem, but I can't show the coercitive condition. Another question: What the regularity of these weak solutions? I know that if $f$ is $C^{0,\alpha}$ and the problem has Dirichlet conditions, then the solution is $C^{2}$ , am I right? Thanks in advance.","Suppose . I know that where is open and bounded, has a weak solution . I can obtain this solution using the Riesz Representation Theorem. But, how obtain a weak soution for this equation with Neumann boundary conditions? And in the case where ? In both cases I have been tried use the Lax Milgran Theorem, but I can't show the coercitive condition. Another question: What the regularity of these weak solutions? I know that if is and the problem has Dirichlet conditions, then the solution is , am I right? Thanks in advance.","f \in L^{2} \left\{\begin{array}{c}
−\Delta u = f(x) & \text{on }\Omega \\
u(x)=0 &  \text{on } \partial\Omega
\end{array}\right., \Omega \subset \mathbb{R}^{N} u \in H^{1}_{0}(\Omega) \Omega = \mathbb{R}^{N} f C^{0,\alpha} C^{2}","['functional-analysis', 'partial-differential-equations', 'sobolev-spaces', 'regularity-theory-of-pdes']"
18,Application Banach-Alaoglu Theorem,Application Banach-Alaoglu Theorem,,"When reading about Banach-Alaoglu Theorem on Wikipedia, I read the following assertion: '' Let $f_n$ be a bounded sequence of functions in $L^p$. Then there exists a subsequence $f_{n_k}$ and an $f\in L^p$ such that  $$\int f_{n_k}g\rightarrow\int fg $$ for all $g\in L^q$ (where $1/p+1/q=1$). The corresponeding result for $p=1$ is not true, as $L^1$ is not reflexive.'' I'm working in $\Omega\subseteq\mathbb{R}^n$. If $f\in L^1(\Omega)$, then $f:L^{\infty}(\Omega)\rightarrow \mathbb{C}$ defined by $f(g)=\int_{\Omega} fg$ is a bounded linear operator. Also, the norm of $f$ as a function coincides with the norm of $f$ as an operator by taking $$g(x)=\begin{cases} |f(x)|/f(x),\,f(x)\neq0 \\ 0,\,f(x)=0 \end{cases}\in L^{\infty}(\Omega)$$ (note that $fg=|f|$). Then, by Banach-Alaoglu Theorem, if $(f_n)\subseteq L^1(\Omega)$ is bounded, there exists a subsequence $f_{n_k}$ and $f\in L^1(\Omega)$ with $\int f_{n_k}g\rightarrow\int fg$ for all $g\in L^{\infty}(\Omega)$. Then I don't understand the assertion from Wikipedia. I mean, it's true that $L^1(\Omega)^{**}\neq L^1(\Omega)$, but I'm not using that. Could you clarify all this for me? EDIT: I think I understand the counterexample, but I'd like to know what's wrong in my reasoning.","When reading about Banach-Alaoglu Theorem on Wikipedia, I read the following assertion: '' Let $f_n$ be a bounded sequence of functions in $L^p$. Then there exists a subsequence $f_{n_k}$ and an $f\in L^p$ such that  $$\int f_{n_k}g\rightarrow\int fg $$ for all $g\in L^q$ (where $1/p+1/q=1$). The corresponeding result for $p=1$ is not true, as $L^1$ is not reflexive.'' I'm working in $\Omega\subseteq\mathbb{R}^n$. If $f\in L^1(\Omega)$, then $f:L^{\infty}(\Omega)\rightarrow \mathbb{C}$ defined by $f(g)=\int_{\Omega} fg$ is a bounded linear operator. Also, the norm of $f$ as a function coincides with the norm of $f$ as an operator by taking $$g(x)=\begin{cases} |f(x)|/f(x),\,f(x)\neq0 \\ 0,\,f(x)=0 \end{cases}\in L^{\infty}(\Omega)$$ (note that $fg=|f|$). Then, by Banach-Alaoglu Theorem, if $(f_n)\subseteq L^1(\Omega)$ is bounded, there exists a subsequence $f_{n_k}$ and $f\in L^1(\Omega)$ with $\int f_{n_k}g\rightarrow\int fg$ for all $g\in L^{\infty}(\Omega)$. Then I don't understand the assertion from Wikipedia. I mean, it's true that $L^1(\Omega)^{**}\neq L^1(\Omega)$, but I'm not using that. Could you clarify all this for me? EDIT: I think I understand the counterexample, but I'd like to know what's wrong in my reasoning.",,"['integration', 'functional-analysis', 'measure-theory']"
19,Question on Inequality from Bartle's Elements of Integration: Riesz Fischer Theorem,Question on Inequality from Bartle's Elements of Integration: Riesz Fischer Theorem,,"I am puzzled how did Bartle get $$|g_k|\leq\sum_{j=k}^\infty |g_{j+1}-g_j|$$ (second last line)? I tried using Triangle Inequality and ended up with one extra term: $$\begin{align*} |g_k|&=|g_k-g_{k+1}+g_{k+1}-g_{k+2}\dots+g_{k+N-1}-g_{k+N}+g_{k+N}|\\ &\leq\sum_{j=k}^{N-1}|g_{j+1}-g_j|+|g_{k+N}| \end{align*}$$ Even after taking limits $N\to\infty$, the extra term $|g_{k+N}|$ does not necessarily go to zero. Thanks for any help!","I am puzzled how did Bartle get $$|g_k|\leq\sum_{j=k}^\infty |g_{j+1}-g_j|$$ (second last line)? I tried using Triangle Inequality and ended up with one extra term: $$\begin{align*} |g_k|&=|g_k-g_{k+1}+g_{k+1}-g_{k+2}\dots+g_{k+N-1}-g_{k+N}+g_{k+N}|\\ &\leq\sum_{j=k}^{N-1}|g_{j+1}-g_j|+|g_{k+N}| \end{align*}$$ Even after taking limits $N\to\infty$, the extra term $|g_{k+N}|$ does not necessarily go to zero. Thanks for any help!",,"['real-analysis', 'functional-analysis', 'analysis', 'lp-spaces']"
20,Why is $C(\beta \mathbb{R})/C_0(\mathbb{R})\cong C(\beta \mathbb{R}\setminus \mathbb{R})$ as $C^*$-algebras?,Why is  as -algebras?,C(\beta \mathbb{R})/C_0(\mathbb{R})\cong C(\beta \mathbb{R}\setminus \mathbb{R}) C^*,"Let $\beta \mathbb{R}$ be the Stone-Čech compactification of $\mathbb{R}$ (with euclidean topology) and $C_0(\mathbb{R})$ the $C^*$-algebra of continuous complex-valued functions vanishing at infinity. Why is $C(\beta \mathbb{R})/C_0(\mathbb{R})\cong C(\beta \mathbb{R}\setminus \mathbb{R})$ as $C^*$-algebras? My problem is that I'm stuck to understand the Stone-Čech compactification of $\mathbb{R}$ and I dont know how to understand $\beta \mathbb{R}\setminus \mathbb{R}$.  I know that one can identify $C(\beta \mathbb{R})$ with $C_b(\mathbb{R})$ as $C^*$-algebras, the continuous bounded complex-valued functions on $\mathbb{R}$.","Let $\beta \mathbb{R}$ be the Stone-Čech compactification of $\mathbb{R}$ (with euclidean topology) and $C_0(\mathbb{R})$ the $C^*$-algebra of continuous complex-valued functions vanishing at infinity. Why is $C(\beta \mathbb{R})/C_0(\mathbb{R})\cong C(\beta \mathbb{R}\setminus \mathbb{R})$ as $C^*$-algebras? My problem is that I'm stuck to understand the Stone-Čech compactification of $\mathbb{R}$ and I dont know how to understand $\beta \mathbb{R}\setminus \mathbb{R}$.  I know that one can identify $C(\beta \mathbb{R})$ with $C_b(\mathbb{R})$ as $C^*$-algebras, the continuous bounded complex-valued functions on $\mathbb{R}$.",,['general-topology']
21,Can you recover a distribution from mollification?,Can you recover a distribution from mollification?,,"Let $f\in \mathcal S'(\mathbb R)$ be a Schwartz distribution. Given $\rho \in C^\infty_c(\mathbb R)$ define the convolution as the function  $$x\mapsto (f\ast\rho)(x):=\langle f, \rho (\cdot -x)\rangle.$$ I believe this function should be smooth. Anyway my question is: suppose further that $\rho\geq 0$ and $\int \rho = 1$ and for $\varepsilon>0$, define  $$\rho_\varepsilon(x):=\frac{1}{\varepsilon}\rho(x\varepsilon^{-1}).$$ Then is it the case that $f\ast\rho_\varepsilon\to f$ in $\mathcal S'(\mathbb R)$ as $\varepsilon\to 0$?","Let $f\in \mathcal S'(\mathbb R)$ be a Schwartz distribution. Given $\rho \in C^\infty_c(\mathbb R)$ define the convolution as the function  $$x\mapsto (f\ast\rho)(x):=\langle f, \rho (\cdot -x)\rangle.$$ I believe this function should be smooth. Anyway my question is: suppose further that $\rho\geq 0$ and $\int \rho = 1$ and for $\varepsilon>0$, define  $$\rho_\varepsilon(x):=\frac{1}{\varepsilon}\rho(x\varepsilon^{-1}).$$ Then is it the case that $f\ast\rho_\varepsilon\to f$ in $\mathcal S'(\mathbb R)$ as $\varepsilon\to 0$?",,"['real-analysis', 'functional-analysis', 'fourier-analysis', 'distribution-theory']"
22,"Are $X'\otimes Y$ and $\mathfrak L(X,Y)$ isomorphic?",Are  and  isomorphic?,"X'\otimes Y \mathfrak L(X,Y)","Let $\mathbb F\in\left\{\mathbb C,\mathbb R\right\}$ $X$ and $Y$ be normed $\mathbb F$-vector spaces $X'$ be the topological dual space of $X$ $\mathfrak L(X,Y)$ be the set of bounded, linear operators from $X$ to $Y$ $\mathfrak B(X\times Y)$ be the set of bilinear forms on $X\times Y$ Let $$(x\otimes y)(A):=A(x,y)\;\;\;\text{for }A\in\mathfrak B(X\times Y)$$ for $(x,y)\in X\times Y$ and $$X\otimes Y:=\operatorname{span}\left\{x\otimes y:(x,y)\in X\times Y\right\}\;.$$ Can we show that there is an isomorphism $\iota$ between $X'\otimes Y$ and $\mathfrak L(X,Y)$? It's clear that any $u\in X'\otimes Y$ with $$u=\sum_{i=1}^n\varphi_i\otimes y_i$$ for some $n\in\mathbb N$ and $(\varphi_i,y_i)\in X'\times Y$ yields some $L\in\mathfrak L(X,Y)$ via $$L:=\varphi_iy_i\;.$$ However, is the mapping $X'\otimes Y\to\mathfrak L(X,Y)$ which maps $u$ to $L$ injective? The other inclusion is not clear to me at all. However, the question arose as I saw people identifying $X'\otimes Y$ with $\mathfrak L(X,Y)$.","Let $\mathbb F\in\left\{\mathbb C,\mathbb R\right\}$ $X$ and $Y$ be normed $\mathbb F$-vector spaces $X'$ be the topological dual space of $X$ $\mathfrak L(X,Y)$ be the set of bounded, linear operators from $X$ to $Y$ $\mathfrak B(X\times Y)$ be the set of bilinear forms on $X\times Y$ Let $$(x\otimes y)(A):=A(x,y)\;\;\;\text{for }A\in\mathfrak B(X\times Y)$$ for $(x,y)\in X\times Y$ and $$X\otimes Y:=\operatorname{span}\left\{x\otimes y:(x,y)\in X\times Y\right\}\;.$$ Can we show that there is an isomorphism $\iota$ between $X'\otimes Y$ and $\mathfrak L(X,Y)$? It's clear that any $u\in X'\otimes Y$ with $$u=\sum_{i=1}^n\varphi_i\otimes y_i$$ for some $n\in\mathbb N$ and $(\varphi_i,y_i)\in X'\times Y$ yields some $L\in\mathfrak L(X,Y)$ via $$L:=\varphi_iy_i\;.$$ However, is the mapping $X'\otimes Y\to\mathfrak L(X,Y)$ which maps $u$ to $L$ injective? The other inclusion is not clear to me at all. However, the question arose as I saw people identifying $X'\otimes Y$ with $\mathfrak L(X,Y)$.",,"['functional-analysis', 'operator-theory', 'tensor-products']"
23,Schauder bases of subspaces of the sequence space $\ell^p(\mathbb{N})$,Schauder bases of subspaces of the sequence space,\ell^p(\mathbb{N}),"Consider the canonical Schauder basis $\{e_i:i\in \mathbb{N}\}$ for $\ell^p(\mathbb{N})$, where $e_i(j)=\delta_{ij}$. Let $M$ be a subspace of $\ell^p(\mathbb{N})$. Is it right that $\{e_i:i\in \mathbb{N}\}\cap M$ is a basis for $M$? For example, if we take the subspaces $$M_1=\{(\cdots,0, 0, x_1,x_2,x_3,0,0,\cdots)\}$$ or $$M_2=\{\{x_i\}:x_i=0 \text{ if } i \text{ is even }  \}$$ Then the answer is clearly yes. However, what about the general cases? If the answer is no, is there any condition to be added such that the question has a positive answer?","Consider the canonical Schauder basis $\{e_i:i\in \mathbb{N}\}$ for $\ell^p(\mathbb{N})$, where $e_i(j)=\delta_{ij}$. Let $M$ be a subspace of $\ell^p(\mathbb{N})$. Is it right that $\{e_i:i\in \mathbb{N}\}\cap M$ is a basis for $M$? For example, if we take the subspaces $$M_1=\{(\cdots,0, 0, x_1,x_2,x_3,0,0,\cdots)\}$$ or $$M_2=\{\{x_i\}:x_i=0 \text{ if } i \text{ is even }  \}$$ Then the answer is clearly yes. However, what about the general cases? If the answer is no, is there any condition to be added such that the question has a positive answer?",,"['functional-analysis', 'banach-spaces', 'lp-spaces', 'schauder-basis']"
24,Positive maps and $*$-homomorphisms,Positive maps and -homomorphisms,*,"If $\varphi:A \to B$ is a linear map between $C^*$-algebras, it is said to be positive if it sends positive elements in $A$ to positive elements in $B$. We know that every $*$-homomorphism is positive. But how about the converse? Does every positive map between $C^*$-algebras necessarily a $*$-homomorphism? Thanks in advance!","If $\varphi:A \to B$ is a linear map between $C^*$-algebras, it is said to be positive if it sends positive elements in $A$ to positive elements in $B$. We know that every $*$-homomorphism is positive. But how about the converse? Does every positive map between $C^*$-algebras necessarily a $*$-homomorphism? Thanks in advance!",,"['functional-analysis', 'c-star-algebras']"
25,Tomita Theory: Involution,Tomita Theory: Involution,,Given a Hilbert space $\mathcal{H}$. Consider a von Neumann algebra: $$M\subseteq\mathcal{B}(\mathcal{H}):\quad M=M''$$ Suppose a cyclic vector: $$\Omega\in\mathcal{H}:\quad\overline{\mathcal{M}\Omega}=\mathcal{H}$$ Regard the involution: $$S_0:\mathcal{M}\Omega\to\mathcal{H}:\quad S_0M\Omega:=M^*\Omega$$ This operator is closable! But I didn't succeed checking this? What if there's no cyclic vector?,Given a Hilbert space $\mathcal{H}$. Consider a von Neumann algebra: $$M\subseteq\mathcal{B}(\mathcal{H}):\quad M=M''$$ Suppose a cyclic vector: $$\Omega\in\mathcal{H}:\quad\overline{\mathcal{M}\Omega}=\mathcal{H}$$ Regard the involution: $$S_0:\mathcal{M}\Omega\to\mathcal{H}:\quad S_0M\Omega:=M^*\Omega$$ This operator is closable! But I didn't succeed checking this? What if there's no cyclic vector?,,"['functional-analysis', 'operator-theory', 'hilbert-spaces', 'operator-algebras', 'von-neumann-algebras']"
26,Problem with a proof on Conway's book,Problem with a proof on Conway's book,,"Now assume that $C_b(X)$ is separable. Thus $({\rm ball}\,C_b(X)^*,{\rm wk}^*)$ is metrizable (5.1). Since $X$ is homeomorphic to a subset of ${\rm ball}\,C_b(X)^*$ (6.1), $X$ is metrizable. It also follows that $\beta X$ is metrizable. It must be shown that $X=\beta X$ . Suppose there is a $\tau$ in $\beta  X\setminus X$ . Let $\{x_n\}$ be a sequence in $X$ such that $x_n\to\tau$ . It can be assumed that $x_n\neq x_m$ for $n\neq m$ . Let $A=\{x_n:n\,{\rm is\, even}\}$ and $B=\{x_n:n\,{\rm is\, odd}\}$ . Then $A$ and $B$ are disjoint closed subsets of $X$ (not closed in $\beta X$ , but in $X$ ) since $A$ and $B$ contain all of their limit points in $X$ . Since $X$ is normal, there is a continuous function $f:X\to[0,1]$ such that $f=0$ on $A$ and $f=1$ on $B$ . But then $f^\beta(\tau)=\lim f(x_{2n})=0$ and $f^\beta(\tau)=\lim f(x_{2n+1})=1$ , a contradiction. Thus $\beta X\setminus X=\varnothing$ . $\hspace{10cm}\blacksquare$ The quote above comes from the end of p. 140 on Conway's A course in functional analysis . The assumption is only that $X$ is completely regular, plus the one in the first sentence of course. My problem is: how does he deduce $\beta X$ is metrizable? He indicates no reference for this fact, and I can't seem to be able to do it myself. I thought of the completion theorem, so if I prove the completion of the metric space $X$ is compact then it is $\beta X$ by uniqueness and $\beta X$ is therefore metrizable. Indeed, if the completion $X'$ is compact, supposing $f:X\to\mathbb{R}$ is continuous, let $y\in X'\smallsetminus X$ , then we have $x_n\to y$ a sequence in $X$ , and by continuity I can prove $f(x_n)$ converges for any such sequence to a unique limit which I call $f(y)$ , and this extends $f$ to $X'$ , right? So $X'$ has the extension property for continuous functions which characterizes $\beta X$ , hence $X'=\beta X$ . But is it true that $X'$ is compact? And how can I prove it? And if otherwise, why is $\beta X$ metrizable? Update OK the above argument works for continuous functions to complete metric spaces, but not any compact Hausdorff space, so I'd have to chuck it away since it doesn't get to $\beta X$ . I was thinking maybe the weak-* closure of $X$ in $C_b(X)^\ast$ is a candidate? But again, how do I extend functions to prove that thing is $\beta X$ ? Clarification The last question in the update seems a bit cryptic, so let me expand. $\beta X$ is characterized by being compact, having $X$ as a dense subspace, and being such that for any $f:X\to K$ , with $K$ ha compact hausdorff space, there exists $\tilde f:\beta X\to K$ an extension of $f$ to a continuous map. So if we take the weak-* closure of $X$ in $C_b(X)^\ast$ , it is compact because the ball by Banach-Alaoglu is and that closure is closed in the ball, it has $X$ as a dense subspace by definition, but what about that extension property?","Now assume that is separable. Thus is metrizable (5.1). Since is homeomorphic to a subset of (6.1), is metrizable. It also follows that is metrizable. It must be shown that . Suppose there is a in . Let be a sequence in such that . It can be assumed that for . Let and . Then and are disjoint closed subsets of (not closed in , but in ) since and contain all of their limit points in . Since is normal, there is a continuous function such that on and on . But then and , a contradiction. Thus . The quote above comes from the end of p. 140 on Conway's A course in functional analysis . The assumption is only that is completely regular, plus the one in the first sentence of course. My problem is: how does he deduce is metrizable? He indicates no reference for this fact, and I can't seem to be able to do it myself. I thought of the completion theorem, so if I prove the completion of the metric space is compact then it is by uniqueness and is therefore metrizable. Indeed, if the completion is compact, supposing is continuous, let , then we have a sequence in , and by continuity I can prove converges for any such sequence to a unique limit which I call , and this extends to , right? So has the extension property for continuous functions which characterizes , hence . But is it true that is compact? And how can I prove it? And if otherwise, why is metrizable? Update OK the above argument works for continuous functions to complete metric spaces, but not any compact Hausdorff space, so I'd have to chuck it away since it doesn't get to . I was thinking maybe the weak-* closure of in is a candidate? But again, how do I extend functions to prove that thing is ? Clarification The last question in the update seems a bit cryptic, so let me expand. is characterized by being compact, having as a dense subspace, and being such that for any , with ha compact hausdorff space, there exists an extension of to a continuous map. So if we take the weak-* closure of in , it is compact because the ball by Banach-Alaoglu is and that closure is closed in the ball, it has as a dense subspace by definition, but what about that extension property?","C_b(X) ({\rm ball}\,C_b(X)^*,{\rm wk}^*) X {\rm ball}\,C_b(X)^* X \beta X X=\beta X \tau \beta  X\setminus X \{x_n\} X x_n\to\tau x_n\neq x_m n\neq m A=\{x_n:n\,{\rm is\, even}\} B=\{x_n:n\,{\rm is\, odd}\} A B X \beta X X A B X X f:X\to[0,1] f=0 A f=1 B f^\beta(\tau)=\lim f(x_{2n})=0 f^\beta(\tau)=\lim f(x_{2n+1})=1 \beta X\setminus X=\varnothing \hspace{10cm}\blacksquare X \beta X X \beta X \beta X X' f:X\to\mathbb{R} y\in X'\smallsetminus X x_n\to y X f(x_n) f(y) f X' X' \beta X X'=\beta X X' \beta X \beta X X C_b(X)^\ast \beta X \beta X X f:X\to K K \tilde f:\beta X\to K f X C_b(X)^\ast X",['functional-analysis']
27,Frechet derivative in a Hilbert space,Frechet derivative in a Hilbert space,,"Let $\mathcal{H}$ be a Hilbert space and $A$ a self-adjoint operator. With $(\, ,\, )$ denoting the inner product and $\psi\in \mathcal{H}$, I want to formally show that the Frechet derivative of the expression $F[\psi]=\left(\psi, \, A\psi \right)$ at $\varphi$ is equal to $d_\varphi F[\psi]=\left(\varphi, \, A\psi \right) + \left(A \psi, \, \varphi \right) $. The way I show it is the following. For some $\epsilon\in \mathbb{R}$, I write \begin{align} F[\psi+\epsilon\varphi]=\left(\psi+\epsilon \varphi, \, A(\psi+\epsilon\varphi) \right)= \left(\psi, \, A\psi \right) +\epsilon \left(A\psi, \, \varphi \right)+\epsilon \left(\varphi, \, A\psi \right)+\epsilon^2 \left(\varphi, \, A\varphi \right). \end{align} Finally, \begin{equation} \lim_{\epsilon\rightarrow 0} \frac{F[\psi+\epsilon\varphi]-F[\psi]}{\epsilon}=\left(A\psi, \, \varphi \right)+ \left(\varphi, \, A\psi \right)= d_\varphi F[\psi]. \end{equation} Is this sufficient for my claim or have I missed anything?  Moreover, assuming this is correct, how would I extend the above argument to something like $f(F[\psi])$, where $f$ is some function from $\mathbb{R}$ to $\mathbb{R}$. Essentially, I would like to know what would be the chain rule in this case, and what are the restrictions(if any) on the function $f$, for the Frechet derivative to exist? Thanks in advance!","Let $\mathcal{H}$ be a Hilbert space and $A$ a self-adjoint operator. With $(\, ,\, )$ denoting the inner product and $\psi\in \mathcal{H}$, I want to formally show that the Frechet derivative of the expression $F[\psi]=\left(\psi, \, A\psi \right)$ at $\varphi$ is equal to $d_\varphi F[\psi]=\left(\varphi, \, A\psi \right) + \left(A \psi, \, \varphi \right) $. The way I show it is the following. For some $\epsilon\in \mathbb{R}$, I write \begin{align} F[\psi+\epsilon\varphi]=\left(\psi+\epsilon \varphi, \, A(\psi+\epsilon\varphi) \right)= \left(\psi, \, A\psi \right) +\epsilon \left(A\psi, \, \varphi \right)+\epsilon \left(\varphi, \, A\psi \right)+\epsilon^2 \left(\varphi, \, A\varphi \right). \end{align} Finally, \begin{equation} \lim_{\epsilon\rightarrow 0} \frac{F[\psi+\epsilon\varphi]-F[\psi]}{\epsilon}=\left(A\psi, \, \varphi \right)+ \left(\varphi, \, A\psi \right)= d_\varphi F[\psi]. \end{equation} Is this sufficient for my claim or have I missed anything?  Moreover, assuming this is correct, how would I extend the above argument to something like $f(F[\psi])$, where $f$ is some function from $\mathbb{R}$ to $\mathbb{R}$. Essentially, I would like to know what would be the chain rule in this case, and what are the restrictions(if any) on the function $f$, for the Frechet derivative to exist? Thanks in advance!",,"['functional-analysis', 'operator-theory', 'inner-products', 'frechet-derivative']"
28,What's the second Fréchet derivative of a function $\mathbb R^d\to\mathbb R$,What's the second Fréchet derivative of a function,\mathbb R^d\to\mathbb R,"Let $u:\mathbb R^d\to\mathbb R$ be twice Fréchet differentiable. What's the second Fréchet derivative ${\rm D}^2u$ of $u$? It's clear that ${\rm D}u$ is a mapping$^1$ $\mathbb R^d\to\mathfrak L(\mathbb R^d,\mathbb R)$ and ${\rm D}^2u$ is a mapping $\mathbb R^d\to\mathfrak L(\mathbb R^d,\mathfrak L(\mathbb R^d,\mathbb R))$. From that, it's easy to see that $${\rm D}u(x)y=\nabla u(x)\cdot y=\sum_{i=1}^dy_i\frac{\partial u}{\partial x_i}(x)=(y\cdot\nabla)u(x)\;\;\;\text{for all }x,y\in\mathbb R^d\;.\tag 1$$ How can we obtain ${\rm D}^2u$? I'm sure ${\rm D}^2u$ can be expressed somehow in terms of the Hessian $\nabla^2u$, maybe we've got $${\rm D}^2u(x)yz=\nabla^2u(x)y\cdot z\;\;\;\text{for all }x,y,z\in\mathbb R^d\;,$$ but I'm unsure. $^1$ Let $\mathfrak L(A,B)$ be the set of bounded, linear operators from $A$ to $B$.","Let $u:\mathbb R^d\to\mathbb R$ be twice Fréchet differentiable. What's the second Fréchet derivative ${\rm D}^2u$ of $u$? It's clear that ${\rm D}u$ is a mapping$^1$ $\mathbb R^d\to\mathfrak L(\mathbb R^d,\mathbb R)$ and ${\rm D}^2u$ is a mapping $\mathbb R^d\to\mathfrak L(\mathbb R^d,\mathfrak L(\mathbb R^d,\mathbb R))$. From that, it's easy to see that $${\rm D}u(x)y=\nabla u(x)\cdot y=\sum_{i=1}^dy_i\frac{\partial u}{\partial x_i}(x)=(y\cdot\nabla)u(x)\;\;\;\text{for all }x,y\in\mathbb R^d\;.\tag 1$$ How can we obtain ${\rm D}^2u$? I'm sure ${\rm D}^2u$ can be expressed somehow in terms of the Hessian $\nabla^2u$, maybe we've got $${\rm D}^2u(x)yz=\nabla^2u(x)y\cdot z\;\;\;\text{for all }x,y,z\in\mathbb R^d\;,$$ but I'm unsure. $^1$ Let $\mathfrak L(A,B)$ be the set of bounded, linear operators from $A$ to $B$.",,"['functional-analysis', 'derivatives', 'operator-theory', 'partial-derivative', 'frechet-derivative']"
29,why is $f\otimes g:A\otimes_{\min}C\to B \otimes_{\min} D$ injective?,why is  injective?,f\otimes g:A\otimes_{\min}C\to B \otimes_{\min} D,"If $f:A\to B$, $g:C\to D$ are injective $\ast$-homomorphisms between $C^*$-algebras $A, B, C, D$, is the induced map on the spatial tensor product $$f\otimes g:A\otimes_{\min}C\to  B \otimes_{\min} D$$ injective as well? I tried to prove it, but I'm stuck. The spartial tensor norm on $A\otimes B$ is $$\|\sum\limits_{i=1}^na_i\otimes b_i\|_{\min}=\sup\{\|\pi_A\otimes \pi_B)(\sum\limits_{i=1}^na_i\otimes b_i)\|: \pi_a,\pi_B\;\ast-\text{respresentations}\}.$$ Maybe the fact $\pi_A$ and $\pi_B$ injective $=>$ $\pi_A\otimes \pi_B$ injective will be helpful. But my problem is that  it shouln't be enough to check injectivity of $f\otimes g$ on sums $z=\sum\limits_{i=1}^na_i\otimes c_i$. Could anyone give some help?","If $f:A\to B$, $g:C\to D$ are injective $\ast$-homomorphisms between $C^*$-algebras $A, B, C, D$, is the induced map on the spatial tensor product $$f\otimes g:A\otimes_{\min}C\to  B \otimes_{\min} D$$ injective as well? I tried to prove it, but I'm stuck. The spartial tensor norm on $A\otimes B$ is $$\|\sum\limits_{i=1}^na_i\otimes b_i\|_{\min}=\sup\{\|\pi_A\otimes \pi_B)(\sum\limits_{i=1}^na_i\otimes b_i)\|: \pi_a,\pi_B\;\ast-\text{respresentations}\}.$$ Maybe the fact $\pi_A$ and $\pi_B$ injective $=>$ $\pi_A\otimes \pi_B$ injective will be helpful. But my problem is that  it shouln't be enough to check injectivity of $f\otimes g$ on sums $z=\sum\limits_{i=1}^na_i\otimes c_i$. Could anyone give some help?",,"['functional-analysis', 'tensor-products', 'operator-algebras', 'c-star-algebras']"
30,Proving function is Schwartz,Proving function is Schwartz,,"I want to prove that $f(t)=e^{-t^2C\pi}$ is Schwartz. I tried computing derivatives and showing that for all $n,k\in \mathbb{N}_0$ $$\lim_{t\to \infty}t^{k}f^{(n)}(t)=0$$ but it gets messy pretty fast. Is there another way?","I want to prove that $f(t)=e^{-t^2C\pi}$ is Schwartz. I tried computing derivatives and showing that for all $n,k\in \mathbb{N}_0$ $$\lim_{t\to \infty}t^{k}f^{(n)}(t)=0$$ but it gets messy pretty fast. Is there another way?",,"['real-analysis', 'functional-analysis', 'schwartz-space']"
31,Linearspan of Gaussians dense in Schwartz space,Linearspan of Gaussians dense in Schwartz space,,"as the title already says I am trying to show that the linear span ""A"" of the gaussians $e^{\frac{-|x|^2}{2}}$ and their translations/ dilations are dense in the Schwartzspace. This is the space of rapidly decreasing smooth functions, i.e. $\mathcal{S}(\mathbb{R}^n) =\{f\in C^\infty(\mathbb{R}^n) | \forall_{\alpha, \beta \in \mathbb{N}_0^n} \sup_{x\in\mathbb{R}^n}|x^\alpha \partial^\beta u(x)| < \infty \}$ which is equipped the metric $$\rho(u,v)= \sum _{\alpha, \beta \in\mathbb{N}_0^n} 2^{-|\alpha|-|\beta|} \frac{\|u-v\|_{\alpha, \beta}}{1+\|u-v\|_{\alpha, \beta}},$$  where $\alpha$ and $\beta$ are multiindices and $\|u\|_{\alpha,\beta}= \sup_{x\in\mathbb{R}^n}|x^\alpha \partial^\beta u(x)|$ Thus I need to show that for every $\varphi \in \mathcal{S}(\mathbb{R}^n)$ there is a sequence $(u_k)_{k\geq 1}$ such that for all multiindices $\alpha$ and $\beta$ as above $\|\varphi-u_k\|_{\alpha, \beta}$ converges to $0$ as  $k\rightarrow \infty$.  Does anyone have a hint/source on how to construct such a sequence? I'd really like to figure this out but as it stands I'm stuck at the beginning. My first guess was to use that $C_0^\infty(\mathbb{R}^n)$ is dense in $\mathcal{S}(\mathbb{R}^n)$, then show that the linear span of the Gaussians is dense in $C_0^\infty(\mathbb{R}^n)$ with respect to $\rho$ and afterwards apply some diagonal sequence argument. But so far I haven't succeeded in finding such an approximation. EDIT: So I have thought about the suggestion and this is what I understand so far:  I know that the convolution for $\varphi \in\mathcal{D}(\mathbb{R}^n)$ and $T \in \mathcal{D}'(\mathbb{R}^n)$ is defined for all  distributions $\psi$ via  $$< \varphi *T,\psi> := <T, \mathcal{R}(\varphi) * \psi> $$ where $\mathcal{R}f(x) = f(-x)$ is the reflexion operator. But this definition should also translate to a tempered distribution $\phi$ convoluted with a gaussian of the type $G=ae^{\frac{|x-x_0|^2}{2b}}$ for $a\in \mathbb{R}$,$b>0$ and $x\in\mathbb{R}^n$.  Therefore if I choose $\phi$ in $\mathcal{S}'(\mathbb{R}^n)$such that $<\phi, G> =0 $ for all G and define $g_\epsilon = e^{\frac{-|x|^2}{2\epsilon^2}}$ as below the convolution $g_\epsilon*\phi$ is well-defined. Furthermore by the choice of $\phi$ I know  $$ <g_\epsilon * \phi, G> = <\phi, \mathcal{R}(g_\epsilon)*G> = 0$$ as $\mathcal{R}(g_\epsilon)*G $ is again of type G.  On the other hand $\lim_{\epsilon \rightarrow 0} \mathcal{R}(g_\epsilon)*G = G$ so also $$\lim_{\epsilon \rightarrow 0} <g_\epsilon * \phi, G> = <\phi,\mathcal{R}(g_\epsilon)*G>  =<\phi, g_\epsilon *G> =  <\phi, G> = 0$$ by the symmetry of $g_\epsilon$ and the definition of $\phi$.  Maybe my thoughts are flawed but so far I don't know how to conclude that $\phi =0$. (This should be equivalent to the support of $\phi$ being the empty set, but I only see that $<\phi, \psi>$ vanishes for $\psi \in G$ and not for all $\psi \in \mathcal{S}(\mathbb{R}^n))$","as the title already says I am trying to show that the linear span ""A"" of the gaussians $e^{\frac{-|x|^2}{2}}$ and their translations/ dilations are dense in the Schwartzspace. This is the space of rapidly decreasing smooth functions, i.e. $\mathcal{S}(\mathbb{R}^n) =\{f\in C^\infty(\mathbb{R}^n) | \forall_{\alpha, \beta \in \mathbb{N}_0^n} \sup_{x\in\mathbb{R}^n}|x^\alpha \partial^\beta u(x)| < \infty \}$ which is equipped the metric $$\rho(u,v)= \sum _{\alpha, \beta \in\mathbb{N}_0^n} 2^{-|\alpha|-|\beta|} \frac{\|u-v\|_{\alpha, \beta}}{1+\|u-v\|_{\alpha, \beta}},$$  where $\alpha$ and $\beta$ are multiindices and $\|u\|_{\alpha,\beta}= \sup_{x\in\mathbb{R}^n}|x^\alpha \partial^\beta u(x)|$ Thus I need to show that for every $\varphi \in \mathcal{S}(\mathbb{R}^n)$ there is a sequence $(u_k)_{k\geq 1}$ such that for all multiindices $\alpha$ and $\beta$ as above $\|\varphi-u_k\|_{\alpha, \beta}$ converges to $0$ as  $k\rightarrow \infty$.  Does anyone have a hint/source on how to construct such a sequence? I'd really like to figure this out but as it stands I'm stuck at the beginning. My first guess was to use that $C_0^\infty(\mathbb{R}^n)$ is dense in $\mathcal{S}(\mathbb{R}^n)$, then show that the linear span of the Gaussians is dense in $C_0^\infty(\mathbb{R}^n)$ with respect to $\rho$ and afterwards apply some diagonal sequence argument. But so far I haven't succeeded in finding such an approximation. EDIT: So I have thought about the suggestion and this is what I understand so far:  I know that the convolution for $\varphi \in\mathcal{D}(\mathbb{R}^n)$ and $T \in \mathcal{D}'(\mathbb{R}^n)$ is defined for all  distributions $\psi$ via  $$< \varphi *T,\psi> := <T, \mathcal{R}(\varphi) * \psi> $$ where $\mathcal{R}f(x) = f(-x)$ is the reflexion operator. But this definition should also translate to a tempered distribution $\phi$ convoluted with a gaussian of the type $G=ae^{\frac{|x-x_0|^2}{2b}}$ for $a\in \mathbb{R}$,$b>0$ and $x\in\mathbb{R}^n$.  Therefore if I choose $\phi$ in $\mathcal{S}'(\mathbb{R}^n)$such that $<\phi, G> =0 $ for all G and define $g_\epsilon = e^{\frac{-|x|^2}{2\epsilon^2}}$ as below the convolution $g_\epsilon*\phi$ is well-defined. Furthermore by the choice of $\phi$ I know  $$ <g_\epsilon * \phi, G> = <\phi, \mathcal{R}(g_\epsilon)*G> = 0$$ as $\mathcal{R}(g_\epsilon)*G $ is again of type G.  On the other hand $\lim_{\epsilon \rightarrow 0} \mathcal{R}(g_\epsilon)*G = G$ so also $$\lim_{\epsilon \rightarrow 0} <g_\epsilon * \phi, G> = <\phi,\mathcal{R}(g_\epsilon)*G>  =<\phi, g_\epsilon *G> =  <\phi, G> = 0$$ by the symmetry of $g_\epsilon$ and the definition of $\phi$.  Maybe my thoughts are flawed but so far I don't know how to conclude that $\phi =0$. (This should be equivalent to the support of $\phi$ being the empty set, but I only see that $<\phi, \psi>$ vanishes for $\psi \in G$ and not for all $\psi \in \mathcal{S}(\mathbb{R}^n))$",,"['functional-analysis', 'fourier-analysis', 'approximation-theory', 'schwartz-space']"
32,Poisson Equation: 'Dirichlet' type problem on all of $\mathbb{R}^N$,Poisson Equation: 'Dirichlet' type problem on all of,\mathbb{R}^N,"I've seen a great deal about solving the Dirichlet problem for Poisson's equation \begin{equation} \Delta u = f \quad \mbox{in} \quad \Omega \subset \mathbb{R}^N\\ u = 0  \quad \mbox{on} \quad \partial\Omega \end{equation} when $\Omega$ is bounded, and quite a bit when $\Omega$ is unbounded but has nontrivial complement (e.g. when it's all of $\mathbb{R}^N$ except for the unit ball). My question concerns when, if at all, this is possible when $\Omega = \mathbb{R}^N$. Obviously, the boundary of $\mathbb{R}^N$ is meaningless, but it seems reasonable to consider an analogous problem in the class of functions with the 'boundary condition' $u \to 0$ as $|x| \to \infty$. However, we quickly run into problems; to understand what the problem might entail I picked $f = \exp(-x^2)$ and  $\Omega = \mathbb{R}$ as friendly looking example and found to my dismay that the general solution takes the form \begin{equation} u(x) = \frac{\sqrt{\pi}}{2}x\mbox{erf}(x)+\frac{1}{2}\exp(-x^2)+Ax+B \end{equation} where erf$(x) = \frac{2}{\sqrt{\pi}}\int_0^x\exp(-t^2)\mathrm{d}t$ is the error function and $A,B$ are constants. The $x$erf$(x)$ term assures that for any choice of $A,B$ that $u\to\infty$ as $|x| \to \infty$, at least in one of the directions: if we pick $A=\frac{\sqrt{\pi}}{2}$ the forward direction decays appropriately but not so in the backward direction, and vice versa. Of course, on a bounded domain we can always make a choice of constants such that the Dirichlet boundary condition is fulfilled, in particular on $[-L,L]$ the choice $A=0$, $B=-\frac{\sqrt{\pi}}{2}L\mbox{erf}(L)-\frac{1}{2}\exp(-L^2)$, but clearly as $L\to\infty$, $B$ also grows without bound. My question then, is how bad is the situation? Do things improve for $N\geq 2$? The greater variety of harmonic functions might mean we can find one such that summing it with the solution derived via convolution with the Newton Kernel will allow us to satisfy the decay conditions but then again, maybe not. How can we understand this? Can we find success by restricting the class of functions? In 1D we can guarantee that the integral of a function decays iff the mass of the function is zero; is there an analogy for integrating twice? Assuming the answer to either of these is positive, can we then say anything about the integrability of the resulting $u$ over $\mathbb{R}^N$? In particular, I'd like to be able to talk about quantities such as $||\Delta^{-1/2} f||_{L^2 (\mathbb{R}^N)}$, but I'm not sure this will be possible without moving into some kind of weighted spaces if at all.","I've seen a great deal about solving the Dirichlet problem for Poisson's equation \begin{equation} \Delta u = f \quad \mbox{in} \quad \Omega \subset \mathbb{R}^N\\ u = 0  \quad \mbox{on} \quad \partial\Omega \end{equation} when $\Omega$ is bounded, and quite a bit when $\Omega$ is unbounded but has nontrivial complement (e.g. when it's all of $\mathbb{R}^N$ except for the unit ball). My question concerns when, if at all, this is possible when $\Omega = \mathbb{R}^N$. Obviously, the boundary of $\mathbb{R}^N$ is meaningless, but it seems reasonable to consider an analogous problem in the class of functions with the 'boundary condition' $u \to 0$ as $|x| \to \infty$. However, we quickly run into problems; to understand what the problem might entail I picked $f = \exp(-x^2)$ and  $\Omega = \mathbb{R}$ as friendly looking example and found to my dismay that the general solution takes the form \begin{equation} u(x) = \frac{\sqrt{\pi}}{2}x\mbox{erf}(x)+\frac{1}{2}\exp(-x^2)+Ax+B \end{equation} where erf$(x) = \frac{2}{\sqrt{\pi}}\int_0^x\exp(-t^2)\mathrm{d}t$ is the error function and $A,B$ are constants. The $x$erf$(x)$ term assures that for any choice of $A,B$ that $u\to\infty$ as $|x| \to \infty$, at least in one of the directions: if we pick $A=\frac{\sqrt{\pi}}{2}$ the forward direction decays appropriately but not so in the backward direction, and vice versa. Of course, on a bounded domain we can always make a choice of constants such that the Dirichlet boundary condition is fulfilled, in particular on $[-L,L]$ the choice $A=0$, $B=-\frac{\sqrt{\pi}}{2}L\mbox{erf}(L)-\frac{1}{2}\exp(-L^2)$, but clearly as $L\to\infty$, $B$ also grows without bound. My question then, is how bad is the situation? Do things improve for $N\geq 2$? The greater variety of harmonic functions might mean we can find one such that summing it with the solution derived via convolution with the Newton Kernel will allow us to satisfy the decay conditions but then again, maybe not. How can we understand this? Can we find success by restricting the class of functions? In 1D we can guarantee that the integral of a function decays iff the mass of the function is zero; is there an analogy for integrating twice? Assuming the answer to either of these is positive, can we then say anything about the integrability of the resulting $u$ over $\mathbb{R}^N$? In particular, I'd like to be able to talk about quantities such as $||\Delta^{-1/2} f||_{L^2 (\mathbb{R}^N)}$, but I'm not sure this will be possible without moving into some kind of weighted spaces if at all.",,"['real-analysis', 'functional-analysis', 'partial-differential-equations', 'elliptic-equations']"
33,Hilbert space .....,Hilbert space .....,,"i got the following operator. Let H be a Hilbert space, $(\lambda_n)_{n \in \mathbb{N}}$ a bounded sequence in $\mathbb{K}$ and  $T^: H \to H, x \to \sum_{n\in\mathbb{N}} \lambda_n \left<x, e_n\right> e_n$.  for a given orthonormal system $(e_n)_{n \in \mathbb{N}}$ I have shown quite a bit about this operator, but i got some open questions. Is there an easy way to show that $T$ is well-defined? Is it sufficient to show that $\Vert Tx \Vert$ is bounded for a $x \in H$? Because that is pretty easy. Im supposed to show that $T$ is compact if and only if $\lambda_n \to 0$ for $n \to \infty$.  For the ""$\Rightarrow$"" direction I have the following: Let $T$ compact, and $(x_n)_n$ a bounded sequence in $H$. Suppose $\lambda_n \not\to 0$ for $n \to \infty$. Since $T$ is compact, $(Tx_n)_n$ should have a convergent subsequence $(Tx_{n_k})_k$. My Idea is to use the Assumption $\lambda_n \not\to 0$ to show, that $\Vert Tx_{n_k} - Tx_{n_l} \Vert > C$ for a constant $C$. But i dont know how to continue the proof.  For the ""$\Leftarrow$"" direction i got no idea how to start. Has this operator any eigenvalues beside the $\lambda_n$'s? And if so, how can i show this?","i got the following operator. Let H be a Hilbert space, $(\lambda_n)_{n \in \mathbb{N}}$ a bounded sequence in $\mathbb{K}$ and  $T^: H \to H, x \to \sum_{n\in\mathbb{N}} \lambda_n \left<x, e_n\right> e_n$.  for a given orthonormal system $(e_n)_{n \in \mathbb{N}}$ I have shown quite a bit about this operator, but i got some open questions. Is there an easy way to show that $T$ is well-defined? Is it sufficient to show that $\Vert Tx \Vert$ is bounded for a $x \in H$? Because that is pretty easy. Im supposed to show that $T$ is compact if and only if $\lambda_n \to 0$ for $n \to \infty$.  For the ""$\Rightarrow$"" direction I have the following: Let $T$ compact, and $(x_n)_n$ a bounded sequence in $H$. Suppose $\lambda_n \not\to 0$ for $n \to \infty$. Since $T$ is compact, $(Tx_n)_n$ should have a convergent subsequence $(Tx_{n_k})_k$. My Idea is to use the Assumption $\lambda_n \not\to 0$ to show, that $\Vert Tx_{n_k} - Tx_{n_l} \Vert > C$ for a constant $C$. But i dont know how to continue the proof.  For the ""$\Leftarrow$"" direction i got no idea how to start. Has this operator any eigenvalues beside the $\lambda_n$'s? And if so, how can i show this?",,"['functional-analysis', 'operator-theory', 'compact-operators']"
34,Problem following proof of Šmulian theorem for separable space,Problem following proof of Šmulian theorem for separable space,,"I tried to solve Problem 10 on p. 464 of Brezis to get a proof of part of the Eberlein-Šmulian theorem, precisely the equivalence between compactness and sequential compactness in the weak topology of a Banach space. I think I have managed everything except the first point, which is to prove that: Let $X$ be a separable Banach space and $A\subseteq X$ be (relatively) weakly compact. Then $A$ is (relatively) weakly sequentially compact. I basically didn't know where to start, so I googled and found this . I have two problems here though. The proof starts by taking a sequence $a_n$ in a weakly compact set $A$ and a countable subset $H\subseteq X^\ast$ , say $H=\{f_i\}_{i\in\mathbb{N}}$ , and lining up $f_i(a_j)$ in a table. Then since the sequences are all bounded ( $A$ is bounded after all) we conclude that every row $\{f_i(x_j)\}_j$ has a convergent subsequence $\{f_i(a_{n(i,j)}\}_j$ . Then it takes the diagonal sequence $f_i(a_{n(i,i)})$ and the corresponding subsequence $a_{n(i,i)}$ and says $f_i(a_{n(j,j)})$ converges for $j\to\infty$ for all $i$ . I really cannot see how this is proved. How do I show that converges? The second problem is in the following paragraph. This can be done for every countable $H$ . Let us now make $H$ ""big"" to get closer to weak convergence of $(y_n)$ . Let $(x_n)$ be dense in $S_X$ and pick $x_n^*\subset S_X$ such that $x_n^*x_n=1$ . Let $H=(x_n^*)$ . Then $H$ is total in $X^*$ . To see this, let $x\in S_X$ be such that $x_n^*x=0$ for every $n$ and let $x^*\in S_X$ . Let $\epsilon>0$ and find some $x_n$ such that $\lVert x-x_n\rVert<\epsilon$ . Then, since $\lVert x^*\rVert=1$ , $|x^*x|<\epsilon$ . Since $\epsilon$ was arbitrary, it follows from the Hahn-Banach separation theorem that $H$ is total. I understand practically everything (though I had to look up ""total"" to find it means that if all functionals in it evalue a point to 0 then the point is 0) except how it deduces $|x^\ast(x)|<\epsilon$ . How do I go about showing that? I thought of adding and subtracting some terms. $x^\ast(x)=x^\ast(x-x_n)+x^\ast(x_n)$ , put absolute values, triangular inequality, $|x^\ast(x-x_n)|\leq\|x^\ast\|\|x-x_n\|=1\cdot\epsilon$ , but then I'd need the other bit to be 0, how do I know that? $x^\ast(x)=(x^\ast-x^\ast_n)(x)+x^\ast_n(x)$ , but I know nothing of the first term, and the second one is zero. If I combine the two, i.e. $x^\ast(x)=x^\ast(x-x_n)+(x^\ast-x^\ast_n)(x_n)+x^\ast_n(x_n)$ , the first is less than $\epsilon$ , the third is 1, and the other one? I don't know. So how do I prove this can be estimated by $\epsilon$ ? Update Over here is a slightly different but totally understandable way of constructing the desired total set, so the second part of the question is no longer a problem. I am, however, still left with the first part. @Qiyu Wen noted that we have metrizability of bounded sets in $X^\ast$ under the weak-* topology by theorem 3.16 of Rudin's Functional Analysis. But I need to prove compactness in the weak topology of $X$ implies sequential compactness in that same topology, nothing to do with $X^\ast$ . Maybe I can try to see things up in $X^{\ast\ast}$ , where the weak-* topology is metrizable, and if a weak closure in $X$ embeds to be weak-* closed in $X^{\ast\ast}$ then I'll be done. But can I prove that $\overline{A}^{\sigma(X^{\ast\ast},X^\ast)}\subseteq\overline{A}^{\sigma(X,X^\ast)}$ , where $A$ and its weak closure are being identified to their images via the canonical embedding $J$ ? And in any case, I'm still curious about why that thing converges, i.e. why the argument given in that pdf works.","I tried to solve Problem 10 on p. 464 of Brezis to get a proof of part of the Eberlein-Šmulian theorem, precisely the equivalence between compactness and sequential compactness in the weak topology of a Banach space. I think I have managed everything except the first point, which is to prove that: Let be a separable Banach space and be (relatively) weakly compact. Then is (relatively) weakly sequentially compact. I basically didn't know where to start, so I googled and found this . I have two problems here though. The proof starts by taking a sequence in a weakly compact set and a countable subset , say , and lining up in a table. Then since the sequences are all bounded ( is bounded after all) we conclude that every row has a convergent subsequence . Then it takes the diagonal sequence and the corresponding subsequence and says converges for for all . I really cannot see how this is proved. How do I show that converges? The second problem is in the following paragraph. This can be done for every countable . Let us now make ""big"" to get closer to weak convergence of . Let be dense in and pick such that . Let . Then is total in . To see this, let be such that for every and let . Let and find some such that . Then, since , . Since was arbitrary, it follows from the Hahn-Banach separation theorem that is total. I understand practically everything (though I had to look up ""total"" to find it means that if all functionals in it evalue a point to 0 then the point is 0) except how it deduces . How do I go about showing that? I thought of adding and subtracting some terms. , put absolute values, triangular inequality, , but then I'd need the other bit to be 0, how do I know that? , but I know nothing of the first term, and the second one is zero. If I combine the two, i.e. , the first is less than , the third is 1, and the other one? I don't know. So how do I prove this can be estimated by ? Update Over here is a slightly different but totally understandable way of constructing the desired total set, so the second part of the question is no longer a problem. I am, however, still left with the first part. @Qiyu Wen noted that we have metrizability of bounded sets in under the weak-* topology by theorem 3.16 of Rudin's Functional Analysis. But I need to prove compactness in the weak topology of implies sequential compactness in that same topology, nothing to do with . Maybe I can try to see things up in , where the weak-* topology is metrizable, and if a weak closure in embeds to be weak-* closed in then I'll be done. But can I prove that , where and its weak closure are being identified to their images via the canonical embedding ? And in any case, I'm still curious about why that thing converges, i.e. why the argument given in that pdf works.","X A\subseteq X A a_n A H\subseteq X^\ast H=\{f_i\}_{i\in\mathbb{N}} f_i(a_j) A \{f_i(x_j)\}_j \{f_i(a_{n(i,j)}\}_j f_i(a_{n(i,i)}) a_{n(i,i)} f_i(a_{n(j,j)}) j\to\infty i H H (y_n) (x_n) S_X x_n^*\subset S_X x_n^*x_n=1 H=(x_n^*) H X^* x\in S_X x_n^*x=0 n x^*\in S_X \epsilon>0 x_n \lVert x-x_n\rVert<\epsilon \lVert x^*\rVert=1 |x^*x|<\epsilon \epsilon H |x^\ast(x)|<\epsilon x^\ast(x)=x^\ast(x-x_n)+x^\ast(x_n) |x^\ast(x-x_n)|\leq\|x^\ast\|\|x-x_n\|=1\cdot\epsilon x^\ast(x)=(x^\ast-x^\ast_n)(x)+x^\ast_n(x) x^\ast(x)=x^\ast(x-x_n)+(x^\ast-x^\ast_n)(x_n)+x^\ast_n(x_n) \epsilon \epsilon X^\ast X X^\ast X^{\ast\ast} X X^{\ast\ast} \overline{A}^{\sigma(X^{\ast\ast},X^\ast)}\subseteq\overline{A}^{\sigma(X,X^\ast)} A J","['functional-analysis', 'banach-spaces', 'compactness', 'weak-convergence']"
35,Proving that a set of functions is a linear subspace of a vector space,Proving that a set of functions is a linear subspace of a vector space,,"I am attempting to solve the following problem: Let $V$ be the vector space of all continuous functions $f : R → R$ with point-wise addition and scalar multiplication defined. (a) Show that $M_1$ = {$f ∈ V :$ There exists $a, b ∈ R$ with $f(x) = a sin(x + b)$ for all $x ∈ R$} is a linear subspace of $V$. (b) Show that $M_2$ = {$f ∈ V :$ There exists $a ∈ R$ with $f(x) = sin(ax)$ for all $x ∈ R$} is not a linear subspace of $V$. As far as I understand, the requirement for being a linear subspace is that the set must be algebraically closed with respect to addition and multiplication by a scalar, that is, (i) If $f_1,f_2 ∈ V$ then $f_1+f_2 ∈ V$ (ii) If $f ∈ V$ then $λf ∈ V$, where $λ∈R$ However, trying to apply the first condition to prove part (a), I need to find a way to express $(f_1+f_2)(x)=f_1(x)+f_2(x)=a_1sin(x+b_1)+a_2sin(x+b_2)$ in the form $asin(x+b)$ so that I can prove that the sum of the two functions belongs to $M_1$ as well. Further, the second condition (multiplication by a scalar) is valid as $(λf)(x)=λasin(x+b)$ which belongs to $M_1$. It is just the first condition at which I am stuck. Also, for part (b), I would appreciate hints for proving how the set is not a linear subspace? What would be a way of showing conclusively that the two conditions do not hold?","I am attempting to solve the following problem: Let $V$ be the vector space of all continuous functions $f : R → R$ with point-wise addition and scalar multiplication defined. (a) Show that $M_1$ = {$f ∈ V :$ There exists $a, b ∈ R$ with $f(x) = a sin(x + b)$ for all $x ∈ R$} is a linear subspace of $V$. (b) Show that $M_2$ = {$f ∈ V :$ There exists $a ∈ R$ with $f(x) = sin(ax)$ for all $x ∈ R$} is not a linear subspace of $V$. As far as I understand, the requirement for being a linear subspace is that the set must be algebraically closed with respect to addition and multiplication by a scalar, that is, (i) If $f_1,f_2 ∈ V$ then $f_1+f_2 ∈ V$ (ii) If $f ∈ V$ then $λf ∈ V$, where $λ∈R$ However, trying to apply the first condition to prove part (a), I need to find a way to express $(f_1+f_2)(x)=f_1(x)+f_2(x)=a_1sin(x+b_1)+a_2sin(x+b_2)$ in the form $asin(x+b)$ so that I can prove that the sum of the two functions belongs to $M_1$ as well. Further, the second condition (multiplication by a scalar) is valid as $(λf)(x)=λasin(x+b)$ which belongs to $M_1$. It is just the first condition at which I am stuck. Also, for part (b), I would appreciate hints for proving how the set is not a linear subspace? What would be a way of showing conclusively that the two conditions do not hold?",,"['linear-algebra', 'functional-analysis', 'vector-spaces']"
36,Why is the Laplacian of $1/r$ a Dirac delta? [duplicate],Why is the Laplacian of  a Dirac delta? [duplicate],1/r,"This question already has answers here : Dirac's delta in 3 dimensions: proof of $\nabla^2(\|\boldsymbol{x}-\boldsymbol{x}_0\|^{-1})=-4\pi\delta(\boldsymbol{x}-\boldsymbol{x}_0)$ (2 answers) Closed 8 years ago . How does one show that $\nabla^2 1/r$ (in spherical coords) is the Dirac delta function ? Intuitively, it would seem that the function undefined at the origin and I'm not able to construct a limiting argument that avoids this problem.","This question already has answers here : Dirac's delta in 3 dimensions: proof of $\nabla^2(\|\boldsymbol{x}-\boldsymbol{x}_0\|^{-1})=-4\pi\delta(\boldsymbol{x}-\boldsymbol{x}_0)$ (2 answers) Closed 8 years ago . How does one show that $\nabla^2 1/r$ (in spherical coords) is the Dirac delta function ? Intuitively, it would seem that the function undefined at the origin and I'm not able to construct a limiting argument that avoids this problem.",,"['functional-analysis', 'partial-differential-equations']"
37,Doubt in definition of symmetric continuous function and norm in Kupka's paper,Doubt in definition of symmetric continuous function and norm in Kupka's paper,,"In this article "" Counterexample to the Morse-Sard theorem in the case of infinite-dimensional manifolds "" of I. Kupka has the following passage: For $H=l^{2}$ ""Let $H^{*}$ be the dual of $H$. A base of $H^{*}$ is formed by the linear functions $e_{1},e_{2},...,e_{n},...$ where $e_{n}(x)=x_{n}$ where $x= (x_{1},x_{2},...,x_{k},...)$ For any $k$, let $\sum_{k}(H^{*})$ be the space of all $k$-linear symmetric continuous functions on $H$.  $\sum_{k}(H^{*})$ is also Hilbert space, with norm $||.||_{k}$."" My doubts: a) What is $k$-linear symmetric continuous functions on $H$? Precisely, what does symmetric mean? Is it $T(x,y,z)= T(z,y,x)$ for $x,y,z \in H$? b) Which norm  $||.||_{k}$ is this? Thank you!","In this article "" Counterexample to the Morse-Sard theorem in the case of infinite-dimensional manifolds "" of I. Kupka has the following passage: For $H=l^{2}$ ""Let $H^{*}$ be the dual of $H$. A base of $H^{*}$ is formed by the linear functions $e_{1},e_{2},...,e_{n},...$ where $e_{n}(x)=x_{n}$ where $x= (x_{1},x_{2},...,x_{k},...)$ For any $k$, let $\sum_{k}(H^{*})$ be the space of all $k$-linear symmetric continuous functions on $H$.  $\sum_{k}(H^{*})$ is also Hilbert space, with norm $||.||_{k}$."" My doubts: a) What is $k$-linear symmetric continuous functions on $H$? Precisely, what does symmetric mean? Is it $T(x,y,z)= T(z,y,x)$ for $x,y,z \in H$? b) Which norm  $||.||_{k}$ is this? Thank you!",,"['functional-analysis', 'differential-topology', 'multilinear-algebra', 'morse-theory']"
38,"Find $y \in W_{2}^{1}[-1,1]$ s.t. $\forall x \in W_{2}^{1}[-1,1]$, $f(x)=\langle x, y \rangle$","Find  s.t. ,","y \in W_{2}^{1}[-1,1] \forall x \in W_{2}^{1}[-1,1] f(x)=\langle x, y \rangle","Consider a Sobolev space $W_{2}^{1}[-1,1]$ with the following inner product: $\langle x, y \rangle = \int_{-1}^{1} [x(t)y(t)+x^{\prime}(t)y^{\prime}(t)]dt$. Let $f(x) = \int_{-1}^{1}e^{2t}x(t)dt$. I need to find $y \in$ the Sobolev space $W_{2}^{1}[-1,1]$ such that $\forall x \in W_{2}^{1}[-1,1]$, $f(x)=\langle x, y \rangle$. However, I don't even know how to begin. I know that I need $\int_{-1}^{1} [x(t)y(t)+x^{\prime}(t)y^{\prime}(t)]dt = \int_{-1}^{1}e^{2t}x(t)dt$, but that's about where what looks familiar to me ends... I've since been informed that what I need to do is find a weak solution of $-y^{\prime\prime} + y = e^{2t}$; however, I have no idea how one goes from the problem I have here to this, much simpler-looking, ODE. A detailed explanation as to where this comes from is needed. Also, I don't know what boundary conditions I'm supposed to use. I am not doing this for a Differential Equations course; I am doing it in a Functional Analysis course, where my exposure to differential equations in the past has been extremely limited (as in, haven't had a class in ODEs since 2003, and even then, it was very basic). Please explain this to me. Then, after I have my solution to the homogeneous form of the 2nd order linear ODE, I should be able to figure out the particular solution (I hope) using either variation of parameters or undetermined coefficients.","Consider a Sobolev space $W_{2}^{1}[-1,1]$ with the following inner product: $\langle x, y \rangle = \int_{-1}^{1} [x(t)y(t)+x^{\prime}(t)y^{\prime}(t)]dt$. Let $f(x) = \int_{-1}^{1}e^{2t}x(t)dt$. I need to find $y \in$ the Sobolev space $W_{2}^{1}[-1,1]$ such that $\forall x \in W_{2}^{1}[-1,1]$, $f(x)=\langle x, y \rangle$. However, I don't even know how to begin. I know that I need $\int_{-1}^{1} [x(t)y(t)+x^{\prime}(t)y^{\prime}(t)]dt = \int_{-1}^{1}e^{2t}x(t)dt$, but that's about where what looks familiar to me ends... I've since been informed that what I need to do is find a weak solution of $-y^{\prime\prime} + y = e^{2t}$; however, I have no idea how one goes from the problem I have here to this, much simpler-looking, ODE. A detailed explanation as to where this comes from is needed. Also, I don't know what boundary conditions I'm supposed to use. I am not doing this for a Differential Equations course; I am doing it in a Functional Analysis course, where my exposure to differential equations in the past has been extremely limited (as in, haven't had a class in ODEs since 2003, and even then, it was very basic). Please explain this to me. Then, after I have my solution to the homogeneous form of the 2nd order linear ODE, I should be able to figure out the particular solution (I hope) using either variation of parameters or undetermined coefficients.",,"['functional-analysis', 'analysis']"
39,Proving that $f = 0$ if $\int fg = 0$ for all $g \in S$,Proving that  if  for all,f = 0 \int fg = 0 g \in S,"Let $f \in L^1$. I want to prove that if $\int f g = 0$ for all $g\in S$, then $f = 0$ a.e. $S$ denotes Schwartz space. My Approach: My idea is to let $h = sgn(f)$ and then smooth it somehow to get a function in $S$. But I don't know how to proceed. Any hint?","Let $f \in L^1$. I want to prove that if $\int f g = 0$ for all $g\in S$, then $f = 0$ a.e. $S$ denotes Schwartz space. My Approach: My idea is to let $h = sgn(f)$ and then smooth it somehow to get a function in $S$. But I don't know how to proceed. Any hint?",,['functional-analysis']
40,Integral of a weak derivative,Integral of a weak derivative,,"While reading chapter 6 of John Hunter's notes ( https://www.math.ucdavis.edu/~hunter/pdes/pde_notes.pdf ) I got stuck on some steps. I think they are all based on a similar idea as the following. Let $u \in L^2(0,T; H_0^1(U))$ and it's weak time derivative $u_t \in L^2(0,T; H^{-1}(U))$ does there hold (for $U$ a bounded set of $\mathbb{R}^N$) $$ u(t) = u(0) + \int_0^t u_t(s) ds \: \: \: ?$$ To write this in a meaningfull manner $u(0)$ has to be defined uniquely, therefore is $u$ continuous? My questions are 1) Does there hold $$ \int_0^t \frac{d}{dt}||u||_{L^{2}(U)}^2(s) ds = ||u||_{L^{2}(U)}^2(t)-||u||_{L^{2}(U)}^2(0) \: \: \: ?$$ again $||u||_{L^{2}(U)}^2(0)$ has to be well-defined. 2) In proposition 6.5 the author gives a prove for the existence of solution of the differential equation $$ \vec{c_n}_t + A(t) \vec{c_n} = \vec{f}(t), \: \: \: \vec{c_n}(0)= \vec{g}  $$ Where $A \in L^{\infty}(0,T; \mathbb{R}^{N \times N}), \: \: \: \vec{f} \in L^2(0,T; \mathbb{R}^N) $ And then he writes the 'equivalent' integral equation $$ \vec{c_n}(t) = \vec{g}- \int_0^t A(s) \vec{c_n}(s) ds + \int_0^t \vec{f}(s)ds $$ Why is this equivalent to the original problem? I know this holds in the classical sence, but here we are considering weak derivatives. I don't see how we get from the weak equation to the integral form and backwards. 3) My last question involves a Gronwall inequality in the context of weak derivatives. If for a.e. $t \in [0,T]: \frac{d}{dt}||u(t)||_{L^2(U)} \leq ||u(t)||_{L^2(U)}$ can we bound $u(t)$? I searched a lot about this but I couldn't find any answer. The most problems will be solved if $u$ was absolutely continous. But since $u_t(t) \in H^{-1}(U)$ and not per se in $H_0^1(U))$ so we can't use the Sobolev theorem directly? The literature I found didn't consider the case for $u_t(t) \in H^{-1}(U)$. EDIT: In the notes of John Hunter and in the book of Evans they use $${u_n}_t(u_n) = \int_{U} {u_n}_t u_n dx $$ Where $u_n$ are approximate solutions. So if the action of ${u_n}_t$ is just the inner product of $L^2(\Omega)$ why not take ${u_n}_t(t)$ to be in $H_0^1(\Omega)$? Can't it be bounded in $H_0^1(\Omega)$-norm?","While reading chapter 6 of John Hunter's notes ( https://www.math.ucdavis.edu/~hunter/pdes/pde_notes.pdf ) I got stuck on some steps. I think they are all based on a similar idea as the following. Let $u \in L^2(0,T; H_0^1(U))$ and it's weak time derivative $u_t \in L^2(0,T; H^{-1}(U))$ does there hold (for $U$ a bounded set of $\mathbb{R}^N$) $$ u(t) = u(0) + \int_0^t u_t(s) ds \: \: \: ?$$ To write this in a meaningfull manner $u(0)$ has to be defined uniquely, therefore is $u$ continuous? My questions are 1) Does there hold $$ \int_0^t \frac{d}{dt}||u||_{L^{2}(U)}^2(s) ds = ||u||_{L^{2}(U)}^2(t)-||u||_{L^{2}(U)}^2(0) \: \: \: ?$$ again $||u||_{L^{2}(U)}^2(0)$ has to be well-defined. 2) In proposition 6.5 the author gives a prove for the existence of solution of the differential equation $$ \vec{c_n}_t + A(t) \vec{c_n} = \vec{f}(t), \: \: \: \vec{c_n}(0)= \vec{g}  $$ Where $A \in L^{\infty}(0,T; \mathbb{R}^{N \times N}), \: \: \: \vec{f} \in L^2(0,T; \mathbb{R}^N) $ And then he writes the 'equivalent' integral equation $$ \vec{c_n}(t) = \vec{g}- \int_0^t A(s) \vec{c_n}(s) ds + \int_0^t \vec{f}(s)ds $$ Why is this equivalent to the original problem? I know this holds in the classical sence, but here we are considering weak derivatives. I don't see how we get from the weak equation to the integral form and backwards. 3) My last question involves a Gronwall inequality in the context of weak derivatives. If for a.e. $t \in [0,T]: \frac{d}{dt}||u(t)||_{L^2(U)} \leq ||u(t)||_{L^2(U)}$ can we bound $u(t)$? I searched a lot about this but I couldn't find any answer. The most problems will be solved if $u$ was absolutely continous. But since $u_t(t) \in H^{-1}(U)$ and not per se in $H_0^1(U))$ so we can't use the Sobolev theorem directly? The literature I found didn't consider the case for $u_t(t) \in H^{-1}(U)$. EDIT: In the notes of John Hunter and in the book of Evans they use $${u_n}_t(u_n) = \int_{U} {u_n}_t u_n dx $$ Where $u_n$ are approximate solutions. So if the action of ${u_n}_t$ is just the inner product of $L^2(\Omega)$ why not take ${u_n}_t(t)$ to be in $H_0^1(\Omega)$? Can't it be bounded in $H_0^1(\Omega)$-norm?",,"['functional-analysis', 'partial-differential-equations', 'sobolev-spaces', 'weak-derivatives', 'bochner-spaces']"
41,Is this basic function space compact?,Is this basic function space compact?,,"Let $A=L^2(X)$ be the space of square integrable functions on a compact Euclidean space $X$. If we equip $A$ with the usual 2-norm, is $A$ compact? Edit: And if we restrict AA by adding the assumption that the functions are totally bounded, i.e. the supremum norms of all the functions are bounded by a constant?","Let $A=L^2(X)$ be the space of square integrable functions on a compact Euclidean space $X$. If we equip $A$ with the usual 2-norm, is $A$ compact? Edit: And if we restrict AA by adding the assumption that the functions are totally bounded, i.e. the supremum norms of all the functions are bounded by a constant?",,"['general-topology', 'functional-analysis', 'compactness']"
42,Show that $||(kI-T)^{-1}|| \le \frac{1}{d}$,Show that,||(kI-T)^{-1}|| \le \frac{1}{d},"Suppose that $T \in BL(H)$ where $H$ is a Hilbert Space. Let $k \in \mathbb{C}$ . Let $d=\text{dist}(k,W(T)) \gt 0$ . Define the numerical range of $T$ as $$W(T)=\{\lambda \in \mathbb{C}: \lambda=\langle Tx,x\rangle, \|x\|=1, x \in H\}.$$ Show that $\|(kI-T)^{-1}\| \le \frac{1}{d}$ . First of all I need to see that $(kI-T)^{-1}$ makes sense. Since $\sigma(T) \subset \overline{W(T)}$ and $d=\text{dist}(k,\overline{W(T)}) \gt 0$ , the inverse makes sense. Now I set out to find the solution. My try: First of all I find relation between $W(T)$ and $W(T-kI)$ . So if $\lambda \in W(T)$ , then $\lambda=\langle Tx,x\rangle$ with $\|x\|=1$ . Then $\langle Tx-kx,x\rangle=\lambda-k$ . Hence $W(T-kI)=\{\lambda-k: \lambda \in W(T)\}$ . The other way around can be proved similarly. Now, \begin{align} \|T-kI\| & = \sup_{\|x\|,\|y\| \le 1 }|\langle Tx-kx,y\rangle | \\ & \ge \sup_{\|x\|=1}|\langle Tx-kIx,x\rangle | \\ & =\sup_{\lambda \in W(T-kI)}|\lambda| \\ & =\sup_{\mu \in W(T)}{|\mu-k|} \ge d \gt 0. \end{align} But after reaching here I am not able to go to its inverse. There might be some other way. I want to know if I can go from here Thanks for the help!!","Suppose that where is a Hilbert Space. Let . Let . Define the numerical range of as Show that . First of all I need to see that makes sense. Since and , the inverse makes sense. Now I set out to find the solution. My try: First of all I find relation between and . So if , then with . Then . Hence . The other way around can be proved similarly. Now, But after reaching here I am not able to go to its inverse. There might be some other way. I want to know if I can go from here Thanks for the help!!","T \in BL(H) H k \in \mathbb{C} d=\text{dist}(k,W(T)) \gt 0 T W(T)=\{\lambda \in \mathbb{C}: \lambda=\langle Tx,x\rangle, \|x\|=1, x \in H\}. \|(kI-T)^{-1}\| \le \frac{1}{d} (kI-T)^{-1} \sigma(T) \subset \overline{W(T)} d=\text{dist}(k,\overline{W(T)}) \gt 0 W(T) W(T-kI) \lambda \in W(T) \lambda=\langle Tx,x\rangle \|x\|=1 \langle Tx-kx,x\rangle=\lambda-k W(T-kI)=\{\lambda-k: \lambda \in W(T)\} \begin{align}
\|T-kI\|
& = \sup_{\|x\|,\|y\| \le 1 }|\langle Tx-kx,y\rangle | \\
& \ge \sup_{\|x\|=1}|\langle Tx-kIx,x\rangle | \\
& =\sup_{\lambda \in W(T-kI)}|\lambda| \\
& =\sup_{\mu \in W(T)}{|\mu-k|}
\ge d
\gt 0.
\end{align}","['analysis', 'functional-analysis', 'spectral-theory']"
43,"If two norms are not equivalent, then we have a sequence $\|x_n\|/\|x_n\|' \to 0$.","If two norms are not equivalent, then we have a sequence .",\|x_n\|/\|x_n\|' \to 0,"Definition 2.22. We say that two norms $\|\cdot\|$ and $\|\cdot\|'$ are equivalent if there exist $C_1,C_2>0$ such that $$C_1\|x_1\|' \le \|x\| \le C_2 \|x\|',$$ for all $x\in V$. If two norms $\|\cdot\|$ and $\|\cdot\|'$ are not equivalent, then how can we construct a sequence such that $\|x_n\|/\|x_n\|' \to 0$? From the definition, we have for all $C_1, C_2 \gt 0$ there is a $x\in V$ such that $C_1\|x\|'\gt \|x\|$ or $C_2\|x\|'\lt \|x\|$. If we have the first strict inequality for all cases then the construction is obvious, but we could get the second strict inequality, in which case I don't see a way to construct the sequence. So how is this possible? This question stems from the following proof in showing that if two norms are not equivalent then they do not generate the same topology. Now suppose that $\|\cdot\|$ and $\|\cdot\|'$ are not equivalent. Then without loss of generality there exists a sequence of points $x_n\in V$, $n\ge 1$, such that $\|x_n\|/\|x_n\|' \to 0$ as $n\to+\infty$. Set $y_n=x_n/\|x_n\|'$. Then $y_n\to0$ in $(V,\|\cdot\|)$ but $I(y_n)$ does not converge to $0$ in $(V,\|\cdot\|'$ (since $\|y_n\|'=1$ for all $n$). Thus $I$ is not continuous, so there exists $U$ which is $\|\cdot\|'$-open such that $U=I^{-1}(U)$ is not $\|\cdot\|$-open, i.e., the topologies are different.","Definition 2.22. We say that two norms $\|\cdot\|$ and $\|\cdot\|'$ are equivalent if there exist $C_1,C_2>0$ such that $$C_1\|x_1\|' \le \|x\| \le C_2 \|x\|',$$ for all $x\in V$. If two norms $\|\cdot\|$ and $\|\cdot\|'$ are not equivalent, then how can we construct a sequence such that $\|x_n\|/\|x_n\|' \to 0$? From the definition, we have for all $C_1, C_2 \gt 0$ there is a $x\in V$ such that $C_1\|x\|'\gt \|x\|$ or $C_2\|x\|'\lt \|x\|$. If we have the first strict inequality for all cases then the construction is obvious, but we could get the second strict inequality, in which case I don't see a way to construct the sequence. So how is this possible? This question stems from the following proof in showing that if two norms are not equivalent then they do not generate the same topology. Now suppose that $\|\cdot\|$ and $\|\cdot\|'$ are not equivalent. Then without loss of generality there exists a sequence of points $x_n\in V$, $n\ge 1$, such that $\|x_n\|/\|x_n\|' \to 0$ as $n\to+\infty$. Set $y_n=x_n/\|x_n\|'$. Then $y_n\to0$ in $(V,\|\cdot\|)$ but $I(y_n)$ does not converge to $0$ in $(V,\|\cdot\|'$ (since $\|y_n\|'=1$ for all $n$). Thus $I$ is not continuous, so there exists $U$ which is $\|\cdot\|'$-open such that $U=I^{-1}(U)$ is not $\|\cdot\|$-open, i.e., the topologies are different.",,"['functional-analysis', 'normed-spaces']"
44,Inequality about disbtribution in Functional Analysis by Rudin,Inequality about disbtribution in Functional Analysis by Rudin,,"I'm reading Functional Analysis by Rudin about distribution theory. I have a problem of derivation of the inequality (10) in theorem 6.25, page 166. It first proves an inequality $$(5)\quad\quad\quad\quad |D^{\alpha}\phi(x)|\le\eta n^{N-|\alpha|}|x|^{N-|\alpha|}\quad\quad\quad(x\in K,|x|\le N)$$ where $\phi\in\mathscr{D}(\Omega)$ ,$\eta\gt 0$,$n$ is that in $R^{n}$ and $N$ is a natural number. I have no problem with this inequality. Then choose an auxiliary function $\psi\in\mathscr{D}(R^{n})$, which is $1$ in some neighborhood of $0$ and whose support is in the unit ball B of $R^{n}$. Define  $$\psi_{r}(x)=\psi(\frac{x}{r})\quad\quad(r\gt 0)$$ I think the precise definition of $\psi$ has nothing to do with the following derivation. I put it here just for completeness. By Leibniz formula,  $$(9)\quad\quad\quad D^{\alpha}(\psi_{r}\phi)=\sum_{\beta\le\alpha}c_{\alpha\beta}(D^{\alpha-\beta}\psi)(\frac{x}{r})(D^{\beta}\phi)(x)r^{|\beta|-|\alpha|}$$ It now follows from (5) that $$(10)\quad\quad\quad\|\psi_{r}\phi\|_{N}\le\eta C\|\psi\|_{N}$$ where C is a constant depending on $n$ and $N$ . The norm $\|\cdot\|_{N}$ is defined as $$\|f\|_{N}=\sup\{D^{\alpha}f(x),x\in X,\alpha\le N\}$$ My question is how to derive (10) from  (5) and what $C$ is. I tried myself but I got a constant depending on $r$ as well.","I'm reading Functional Analysis by Rudin about distribution theory. I have a problem of derivation of the inequality (10) in theorem 6.25, page 166. It first proves an inequality $$(5)\quad\quad\quad\quad |D^{\alpha}\phi(x)|\le\eta n^{N-|\alpha|}|x|^{N-|\alpha|}\quad\quad\quad(x\in K,|x|\le N)$$ where $\phi\in\mathscr{D}(\Omega)$ ,$\eta\gt 0$,$n$ is that in $R^{n}$ and $N$ is a natural number. I have no problem with this inequality. Then choose an auxiliary function $\psi\in\mathscr{D}(R^{n})$, which is $1$ in some neighborhood of $0$ and whose support is in the unit ball B of $R^{n}$. Define  $$\psi_{r}(x)=\psi(\frac{x}{r})\quad\quad(r\gt 0)$$ I think the precise definition of $\psi$ has nothing to do with the following derivation. I put it here just for completeness. By Leibniz formula,  $$(9)\quad\quad\quad D^{\alpha}(\psi_{r}\phi)=\sum_{\beta\le\alpha}c_{\alpha\beta}(D^{\alpha-\beta}\psi)(\frac{x}{r})(D^{\beta}\phi)(x)r^{|\beta|-|\alpha|}$$ It now follows from (5) that $$(10)\quad\quad\quad\|\psi_{r}\phi\|_{N}\le\eta C\|\psi\|_{N}$$ where C is a constant depending on $n$ and $N$ . The norm $\|\cdot\|_{N}$ is defined as $$\|f\|_{N}=\sup\{D^{\alpha}f(x),x\in X,\alpha\le N\}$$ My question is how to derive (10) from  (5) and what $C$ is. I tried myself but I got a constant depending on $r$ as well.",,"['real-analysis', 'functional-analysis', 'distribution-theory']"
45,Uniform lower bound of sequence of linear maps on a Banach space,Uniform lower bound of sequence of linear maps on a Banach space,,"Suppose a sequence of $T_j:X\to\mathbb{R}$, with $X$ Banach, has the following property:  $$\forall j:\|T_j\|\geq c>0$$ Then we have for all $n$,  $$\exists x\in X\setminus \{0\}:\forall j\leq n:|T_jx|\geq\frac12c\|x\|$$ Does anybody know how to prove or disprove this? It seems like a reverse of the uniform boundedness principle somehow, but perhaps this analogy is incorrect.","Suppose a sequence of $T_j:X\to\mathbb{R}$, with $X$ Banach, has the following property:  $$\forall j:\|T_j\|\geq c>0$$ Then we have for all $n$,  $$\exists x\in X\setminus \{0\}:\forall j\leq n:|T_jx|\geq\frac12c\|x\|$$ Does anybody know how to prove or disprove this? It seems like a reverse of the uniform boundedness principle somehow, but perhaps this analogy is incorrect.",,"['sequences-and-series', 'functional-analysis', 'inequality', 'banach-spaces', 'normed-spaces']"
46,Extension of harmonic function with bounded $L^{2}$ norm,Extension of harmonic function with bounded  norm,L^{2},"Let $h:D\setminus \{0\}\rightarrow \mathbb{R}$ be a harmonic function, where $D$ is the unit disc in $\mathbb{R}^{2}$, with bounded $L^{2}$ norm, i.e. $||h||_{L^{2}(D)}^{2}=\int_{D}|h|^{2}(x,y)dxdy < \infty$. Is it possible to extend $h$ harmonically to the whole disc $D$? Roman","Let $h:D\setminus \{0\}\rightarrow \mathbb{R}$ be a harmonic function, where $D$ is the unit disc in $\mathbb{R}^{2}$, with bounded $L^{2}$ norm, i.e. $||h||_{L^{2}(D)}^{2}=\int_{D}|h|^{2}(x,y)dxdy < \infty$. Is it possible to extend $h$ harmonically to the whole disc $D$? Roman",,"['real-analysis', 'complex-analysis', 'functional-analysis', 'partial-differential-equations']"
47,What is the $C^*$-algebra generated by a normal operator?,What is the -algebra generated by a normal operator?,C^*,"The following is the spectral theorem in Banach Algebra Techniques in Operator Theory by Douglas: I don't find the definition for the $C^*$-algebra generated by a normal operator in the book. Would anyone point me to a reference illustrating what it is? Also, in the context of matrices, say a $n\times n$ complex matrix $T$, what is the generated $C^*$-algebra $\mathfrak{C}_T$?","The following is the spectral theorem in Banach Algebra Techniques in Operator Theory by Douglas: I don't find the definition for the $C^*$-algebra generated by a normal operator in the book. Would anyone point me to a reference illustrating what it is? Also, in the context of matrices, say a $n\times n$ complex matrix $T$, what is the generated $C^*$-algebra $\mathfrak{C}_T$?",,"['functional-analysis', 'reference-request']"
48,If $X$ is a normed space over $\mathbb{R}$ and $(x_{n})\rightharpoonup x\in X$ then $(x_{n})$ is bounded?,If  is a normed space over  and  then  is bounded?,X \mathbb{R} (x_{n})\rightharpoonup x\in X (x_{n}),So if $X$ is a normed space over $\mathbb{R}$ and $(x_{n})$ is a weakly convergent sequence then I want to prove that $(x_{n})$ is bounded. Since $(x_{n})$ is weakly convergent we have that $f(x_{n})\to f(x)$ for all $f\in X^{\ast}$. Observe that $|f(x_{n})|$ is bounded for all $f\in X^{\ast}$ and we can write $\|x_{n}\|:=\sup\{|f(x_{n})|:\|f\|_{\ast}\le 1\}$. Then applying the Banach-Steinhaus theorem here would yield that $(\|x_{n}\|)\subset\mathbb{R}$ is bounded. I was wondering if this proof was sufficient?,So if $X$ is a normed space over $\mathbb{R}$ and $(x_{n})$ is a weakly convergent sequence then I want to prove that $(x_{n})$ is bounded. Since $(x_{n})$ is weakly convergent we have that $f(x_{n})\to f(x)$ for all $f\in X^{\ast}$. Observe that $|f(x_{n})|$ is bounded for all $f\in X^{\ast}$ and we can write $\|x_{n}\|:=\sup\{|f(x_{n})|:\|f\|_{\ast}\le 1\}$. Then applying the Banach-Steinhaus theorem here would yield that $(\|x_{n}\|)\subset\mathbb{R}$ is bounded. I was wondering if this proof was sufficient?,,"['sequences-and-series', 'functional-analysis']"
49,The strong topology on $U(\mathcal H)$ is metrisable,The strong topology on  is metrisable,U(\mathcal H),"The strong operator topology on a Banach space $X$ is usually defined via semi-norms: For any $x \in X$, $|\cdot|_x: B(X) \to \mathbb R, A \mapsto \|A(x)\|$ is a semi-norm, the strong topology is the weakest/coarsest topology which makes these maps continuous. Alternatively it is generated by the sub-base $\left\{B_\epsilon(A;x)=\{B\in X \mid |B-A|_x<\epsilon\}\phantom{\sum}\right\}$. If we define this topology on a separable Hilbert space $\mathcal H$ and restrict it to the subset of unitary operators, a statement in a book I am reading is that this is then a metrisable topology. My question is then, how can I prove this (especially, why do we need separability)? What does the metric look like (is it constructable)? When can this result be extended to other bounded subsets of $B(\mathcal H)$?","The strong operator topology on a Banach space $X$ is usually defined via semi-norms: For any $x \in X$, $|\cdot|_x: B(X) \to \mathbb R, A \mapsto \|A(x)\|$ is a semi-norm, the strong topology is the weakest/coarsest topology which makes these maps continuous. Alternatively it is generated by the sub-base $\left\{B_\epsilon(A;x)=\{B\in X \mid |B-A|_x<\epsilon\}\phantom{\sum}\right\}$. If we define this topology on a separable Hilbert space $\mathcal H$ and restrict it to the subset of unitary operators, a statement in a book I am reading is that this is then a metrisable topology. My question is then, how can I prove this (especially, why do we need separability)? What does the metric look like (is it constructable)? When can this result be extended to other bounded subsets of $B(\mathcal H)$?",,"['functional-analysis', 'operator-theory', 'hilbert-spaces', 'c-star-algebras']"
50,"How to distinguish between ""for all $x>0$"", ""on the interval $(0,\infty)$"" and ""on the interval $[a,\infty)$ for all $a>0$""?","How to distinguish between ""for all "", ""on the interval "" and ""on the interval  for all ""?","x>0 (0,\infty) [a,\infty) a>0","Assume in the following that $x,a$ are real numbers. How do I distinguish between the statements ""for all $x>0$"", ""on the interval $(0,\infty)$"" and ""on the interval $[a,\infty)$ for all $a>0$""? As far as I can tell, these three sets are the same set, but in my assignment, there seems to be important differences. For instance in one exercise I want to prove that the series $S(x)$ ""converges uniformly on the interval $[a,\infty)$ for all $a>0$"". Later I am asked to prove that the same series does not converge uniformly ""on $(0,\infty)$"". Then later on, the lecturer claims that by picking an arbitrary closed interval $[a,b] \subset (0,\infty)$, and proving some statement about some series on that closed interval, concludes that the statement is true for all $x>0$ since $[a,b]$ is arbitrary. What is going on? When I am faced with these statements, how should I interpret them? How do I distinguish between these statements and similar statements like them? Which of these statements imply the others? It would be best if you could generalize away from examples involving series.","Assume in the following that $x,a$ are real numbers. How do I distinguish between the statements ""for all $x>0$"", ""on the interval $(0,\infty)$"" and ""on the interval $[a,\infty)$ for all $a>0$""? As far as I can tell, these three sets are the same set, but in my assignment, there seems to be important differences. For instance in one exercise I want to prove that the series $S(x)$ ""converges uniformly on the interval $[a,\infty)$ for all $a>0$"". Later I am asked to prove that the same series does not converge uniformly ""on $(0,\infty)$"". Then later on, the lecturer claims that by picking an arbitrary closed interval $[a,b] \subset (0,\infty)$, and proving some statement about some series on that closed interval, concludes that the statement is true for all $x>0$ since $[a,b]$ is arbitrary. What is going on? When I am faced with these statements, how should I interpret them? How do I distinguish between these statements and similar statements like them? Which of these statements imply the others? It would be best if you could generalize away from examples involving series.",,"['real-analysis', 'general-topology', 'functional-analysis']"
51,lim inf version of sequential criterion,lim inf version of sequential criterion,,"I know that if $f$ is a continuous function, then $\lim f(x_n)=f(\lim x_n)$, provided $\lim x_n$ exists. Can the same thing work for $\lim\inf$? i.e. is it true that $\lim\inf f(x_n)=f(\lim\inf x_n)$, provided $\lim\inf x_n$ exists (which it always does).? Thanks for any help. I am trying to see if I can use this property in another proof.","I know that if $f$ is a continuous function, then $\lim f(x_n)=f(\lim x_n)$, provided $\lim x_n$ exists. Can the same thing work for $\lim\inf$? i.e. is it true that $\lim\inf f(x_n)=f(\lim\inf x_n)$, provided $\lim\inf x_n$ exists (which it always does).? Thanks for any help. I am trying to see if I can use this property in another proof.",,"['real-analysis', 'functional-analysis']"
52,Is every topological space is measurable?,Is every topological space is measurable?,,Actually I am learning about measure theory. But I have confusion between  topological space and measurable . Is there any relationship among them or not?,Actually I am learning about measure theory. But I have confusion between  topological space and measurable . Is there any relationship among them or not?,,"['real-analysis', 'functional-analysis']"
53,"Does a contractible set have contractible preimage, under a linear map?","Does a contractible set have contractible preimage, under a linear map?",,"Let $T:V\to W$ be a linear map of vector spaces, and let $A\subset W$ be contractible. Then is $T^{-1}(A)$ also contractible?","Let $T:V\to W$ be a linear map of vector spaces, and let $A\subset W$ be contractible. Then is $T^{-1}(A)$ also contractible?",,"['linear-algebra', 'functional-analysis']"
54,Is $C_0(\mathbb{R})$ a Banach space?,Is  a Banach space?,C_0(\mathbb{R}),"Let $C(\mathbb{R})$ be a Banach space of continuous real-valued functions defined on $\mathbb{R}$ , with supremum norm, and let $C_0(\mathbb R)$ be the subspace of functions vanishing at infinity. Is $C_0(\mathbb{R})$ a Banach space? I try to see it using: $f\in C_0(\mathbb{R})$ iff for any $\epsilon>0$ there exists $K>0$ such that $|f|<\epsilon$ whenever $|x|>K$ . But I think it is not Banach. Please I need a counter-example or a proof.","Let be a Banach space of continuous real-valued functions defined on , with supremum norm, and let be the subspace of functions vanishing at infinity. Is a Banach space? I try to see it using: iff for any there exists such that whenever . But I think it is not Banach. Please I need a counter-example or a proof.",C(\mathbb{R}) \mathbb{R} C_0(\mathbb R) C_0(\mathbb{R}) f\in C_0(\mathbb{R}) \epsilon>0 K>0 |f|<\epsilon |x|>K,"['functional-analysis', 'banach-spaces', 'uniform-continuity']"
55,Why the set of pure state ‎is ‎weak* ‎compact?,Why the set of pure state ‎is ‎weak* ‎compact?,,"Let ‎$‎‎A$ ‎be a‎ ‎C*-algebra‎. ‎ ‎$‎S(A)‎$ ‎is ‎the ‎set ‎of ‎state ‎on ‎‎$‎‎A$ and $‎‎PS(A)$ ‎is ‎the ‎set ‎of ‎pure ‎state ‎on ‎‎$‎‎A$. ‎ ‎ I ‎know ‎that ‎if ‎‎$‎‎A$ ‎is ‎unital ‎then ‎‎$‎‎S(A)$ ‎is ‎weak* ‎compat.‎ ‎I know that  extreme points of $S(A)$ is $PS(A)$ I ‎want ‎to ‎prove ‎‎$‎‎PS(A)$ ‎is ‎weak* ‎compact.so I‎ ‎should ‎show ‎that ‎‎$‎‎PS(A)$ ‎is ‎weak* ‎closed.(when $A$ is unital) ‎ Q: ‎‎My question is:""  Why $‎PS(A)‎$ ‎is ‎weak* ‎compact?‎""  "" Is the extreme point of compact set compact? ""","Let ‎$‎‎A$ ‎be a‎ ‎C*-algebra‎. ‎ ‎$‎S(A)‎$ ‎is ‎the ‎set ‎of ‎state ‎on ‎‎$‎‎A$ and $‎‎PS(A)$ ‎is ‎the ‎set ‎of ‎pure ‎state ‎on ‎‎$‎‎A$. ‎ ‎ I ‎know ‎that ‎if ‎‎$‎‎A$ ‎is ‎unital ‎then ‎‎$‎‎S(A)$ ‎is ‎weak* ‎compat.‎ ‎I know that  extreme points of $S(A)$ is $PS(A)$ I ‎want ‎to ‎prove ‎‎$‎‎PS(A)$ ‎is ‎weak* ‎compact.so I‎ ‎should ‎show ‎that ‎‎$‎‎PS(A)$ ‎is ‎weak* ‎closed.(when $A$ is unital) ‎ Q: ‎‎My question is:""  Why $‎PS(A)‎$ ‎is ‎weak* ‎compact?‎""  "" Is the extreme point of compact set compact? """,,"['functional-analysis', 'c-star-algebras', 'banach-algebras', 'von-neumann-algebras']"
56,checking whether an operator is bounded or not,checking whether an operator is bounded or not,,"let X be a real normed space with finitely many non zero terms,with supremum norm and let T:X$\to$X be a one-one and onto linear operator defined by $$T(x_1,x_2,x_3,......)=(x_1,\frac{x_2}{4},\frac{x_3}{9},.....)$$ then, which of the following is true: (1) T is bounded but $T^{-1}$ is not bounded (2) T is not bounded but $T^{-1}$ is bounded (3) both T and $T^{-1}$ are bounded (4) neither of the two is bounded my thought: i am trying to apply the result :-a linear operator T:X $\to$ Y is bounded iff it is continuous.now i can say just by looking at the operator that given T is continuous at  point ,say,(1,1,0,0,0,....) and being linear it will be cts on X and hence by above theorem it will be bounded.also,after constructing $T^{-1}$ and following the similar steps i am getting that $T^{-1}$ is bonded.so,(3) must be the correct option is this approach correct??if not,please give other alternatives...","let X be a real normed space with finitely many non zero terms,with supremum norm and let T:X$\to$X be a one-one and onto linear operator defined by $$T(x_1,x_2,x_3,......)=(x_1,\frac{x_2}{4},\frac{x_3}{9},.....)$$ then, which of the following is true: (1) T is bounded but $T^{-1}$ is not bounded (2) T is not bounded but $T^{-1}$ is bounded (3) both T and $T^{-1}$ are bounded (4) neither of the two is bounded my thought: i am trying to apply the result :-a linear operator T:X $\to$ Y is bounded iff it is continuous.now i can say just by looking at the operator that given T is continuous at  point ,say,(1,1,0,0,0,....) and being linear it will be cts on X and hence by above theorem it will be bounded.also,after constructing $T^{-1}$ and following the similar steps i am getting that $T^{-1}$ is bonded.so,(3) must be the correct option is this approach correct??if not,please give other alternatives...",,['functional-analysis']
57,What is the gradient of a distribution?,What is the gradient of a distribution?,,"Let $\Omega\subseteq\mathbb R^d$ be open and $\mathcal D(\Omega)$ be the set of $C^\infty(\Omega)$-functions with compact support equipped with a locally convex topology. Let $\mathcal D(\Omega)'$ be the dual space of $\mathcal D(\Omega)$ and $p\in\mathcal D(\Omega)'$, i.e. $$p:\mathcal D(\Omega)\to\mathbb R$$ is linear and continuous. What is meant by the gradient $\nabla p$ of $p$ as it is been used in the paper A Remark on the Characterization of the Gradient of a Distribution ?","Let $\Omega\subseteq\mathbb R^d$ be open and $\mathcal D(\Omega)$ be the set of $C^\infty(\Omega)$-functions with compact support equipped with a locally convex topology. Let $\mathcal D(\Omega)'$ be the dual space of $\mathcal D(\Omega)$ and $p\in\mathcal D(\Omega)'$, i.e. $$p:\mathcal D(\Omega)\to\mathbb R$$ is linear and continuous. What is meant by the gradient $\nabla p$ of $p$ as it is been used in the paper A Remark on the Characterization of the Gradient of a Distribution ?",,"['functional-analysis', 'distribution-theory']"
58,A bounded linear operator between Banach spaces that cannot be compact,A bounded linear operator between Banach spaces that cannot be compact,,"Let $K: X \to Y$ be a bounded linear operator, where $X$ and $Y$ are two Banach spaces. Further assume that the image $imK$ is a $\infty$-dim closed subspace of Y. In my script they claim that in such a setting $K$ can never be compact because by the closed image theorem we have that $K(B)$ ($B$ closed unit ball in $X$) contains an open ball and hence has no compact closure because the dimension is $\infty$. My questions: I understand that the dimension of the closed unit ball $B$ characterizes the dimension of the underlying Banach space but I don't understand how this argument is used here. Also I don't quite get the thing with the open ball due to the closed image theorem and the thing that follows with the compact closure. How does one mash these things together the right way? EDIT: Ok I might have found another way to proof the above: Define $K_1: X \to im(K)$ by $x \mapsto K(x)$. Then $K_1$ is surjective, hence $K_1(B_{open})$ ($B_{open}$ denotes the open unit ball in x) contains a small $\delta$-ball $B_{\delta}$ centered at the origin of $Y$ by the open mapping theorem. If we assume $cl(K_1(B_{open}))$ to be compact in $im(K)$ we get that $cl(B_{\delta})=\{y \in Y | \Vert y \Vert_Y \leq \delta\} \subset cl(K_1(B_{open}))$ and since Y is Banach it's also Hausdorff therefore it follows that $cl(B_{\delta})$  is compact and hence by scaling the unit ball in $Y$ is also compact which contradicts the $\infty$-dimensional property of $im(K)$. Is this right?","Let $K: X \to Y$ be a bounded linear operator, where $X$ and $Y$ are two Banach spaces. Further assume that the image $imK$ is a $\infty$-dim closed subspace of Y. In my script they claim that in such a setting $K$ can never be compact because by the closed image theorem we have that $K(B)$ ($B$ closed unit ball in $X$) contains an open ball and hence has no compact closure because the dimension is $\infty$. My questions: I understand that the dimension of the closed unit ball $B$ characterizes the dimension of the underlying Banach space but I don't understand how this argument is used here. Also I don't quite get the thing with the open ball due to the closed image theorem and the thing that follows with the compact closure. How does one mash these things together the right way? EDIT: Ok I might have found another way to proof the above: Define $K_1: X \to im(K)$ by $x \mapsto K(x)$. Then $K_1$ is surjective, hence $K_1(B_{open})$ ($B_{open}$ denotes the open unit ball in x) contains a small $\delta$-ball $B_{\delta}$ centered at the origin of $Y$ by the open mapping theorem. If we assume $cl(K_1(B_{open}))$ to be compact in $im(K)$ we get that $cl(B_{\delta})=\{y \in Y | \Vert y \Vert_Y \leq \delta\} \subset cl(K_1(B_{open}))$ and since Y is Banach it's also Hausdorff therefore it follows that $cl(B_{\delta})$  is compact and hence by scaling the unit ball in $Y$ is also compact which contradicts the $\infty$-dimensional property of $im(K)$. Is this right?",,['functional-analysis']
59,Weakly convergence but not strongly - properties of limsup and liminf,Weakly convergence but not strongly - properties of limsup and liminf,,Let $X$ be a Banach space and suppose we have a sequence $\{x_n\}$  which is convergent weakly but not strongly. Define  $y_n:=\sum\limits_{k=1}^{n}x_k$. What we can say about $\limsup\limits_{n\to\infty} \|y_n\|^{\frac{1}{n}}$ and $\liminf\limits_{n\to\infty}\|y_n\|^{\frac{1}{n}}$ ?,Let $X$ be a Banach space and suppose we have a sequence $\{x_n\}$  which is convergent weakly but not strongly. Define  $y_n:=\sum\limits_{k=1}^{n}x_k$. What we can say about $\limsup\limits_{n\to\infty} \|y_n\|^{\frac{1}{n}}$ and $\liminf\limits_{n\to\infty}\|y_n\|^{\frac{1}{n}}$ ?,,"['functional-analysis', 'banach-spaces', 'limsup-and-liminf', 'weak-convergence']"
60,Which functional space does feedforward neural network approximate?,Which functional space does feedforward neural network approximate?,,"I'm pretty sure this question has already studied by at least one paper, but I can't figure out where. The question is the following : Given $l$ layers of $n_l \in L$ neurons, we can build a set of functions $\mathcal{N}(l, \{n_i | 1 \leq i \leq l\}\}$, which are the functions given by any neural feedforward neural network with this amount of layers and neuron (n_1 and n_l are respectively the input layers and output layers). Those functions are continuous $\mathcal{N}(l, n_i) \subset \mathcal{C}(R^n_1, R^n_l)$, and as the number of layers, or the number of neurons in a layer increase, we have bigger set of functions ($\mathcal{N}(l, n_i) \subset \mathcal{N}(l', m_i), \forall l \leq l', n_i \leq m_i$). But what kind of functions those $\mathcal{N}(l, n_i)$ (let's say n_0 and n_l are given and fixed) sets are approximating? I would be surprised if this set is dense in the set of continuous functions. I guess it is not the case, and then ask what is the smallest human-named set containing those functions. Are they dense in that set? If no, which functions are missing? Once we know that, what probability law does a uniform law on the weights ''induce'' on this function set? N.b. : This question is related to computer science, but since I'm looking for functional space, and probabilities, I thought it's better to ask on math exchange. Edit : Just noticed I'm talking about density but didn't said which topology I'm using. Let's say we are interested to this answer for both $L^2$ and infinite norms.","I'm pretty sure this question has already studied by at least one paper, but I can't figure out where. The question is the following : Given $l$ layers of $n_l \in L$ neurons, we can build a set of functions $\mathcal{N}(l, \{n_i | 1 \leq i \leq l\}\}$, which are the functions given by any neural feedforward neural network with this amount of layers and neuron (n_1 and n_l are respectively the input layers and output layers). Those functions are continuous $\mathcal{N}(l, n_i) \subset \mathcal{C}(R^n_1, R^n_l)$, and as the number of layers, or the number of neurons in a layer increase, we have bigger set of functions ($\mathcal{N}(l, n_i) \subset \mathcal{N}(l', m_i), \forall l \leq l', n_i \leq m_i$). But what kind of functions those $\mathcal{N}(l, n_i)$ (let's say n_0 and n_l are given and fixed) sets are approximating? I would be surprised if this set is dense in the set of continuous functions. I guess it is not the case, and then ask what is the smallest human-named set containing those functions. Are they dense in that set? If no, which functions are missing? Once we know that, what probability law does a uniform law on the weights ''induce'' on this function set? N.b. : This question is related to computer science, but since I'm looking for functional space, and probabilities, I thought it's better to ask on math exchange. Edit : Just noticed I'm talking about density but didn't said which topology I'm using. Let's say we are interested to this answer for both $L^2$ and infinite norms.",,"['functional-analysis', 'probability-theory', 'neural-networks']"
61,A density argument in the proof of Hardy's inequality for $H^1(\mathbb R^3)$,A density argument in the proof of Hardy's inequality for,H^1(\mathbb R^3),"I understand $C^{\infty}_0 (\mathbb{R}^3)$ is dense in $H^1 (\mathbb{R}^3)$. But I don't understand the reason it is enough to prove the following Hardy inequality if you prove the case $u \in C^{\infty}_0$. (Hardy inequality) Let$ \ u \in H^1 (\mathbb{R}^3)$. Then, $$\int_{\mathbb{R}^3} \frac{|u(x)|^2}{|x|^2} dx \leq 4\int_{\mathbb{R}^3} |\nabla u(x)|^2 dx$$","I understand $C^{\infty}_0 (\mathbb{R}^3)$ is dense in $H^1 (\mathbb{R}^3)$. But I don't understand the reason it is enough to prove the following Hardy inequality if you prove the case $u \in C^{\infty}_0$. (Hardy inequality) Let$ \ u \in H^1 (\mathbb{R}^3)$. Then, $$\int_{\mathbb{R}^3} \frac{|u(x)|^2}{|x|^2} dx \leq 4\int_{\mathbb{R}^3} |\nabla u(x)|^2 dx$$",,"['functional-analysis', 'inequality', 'sobolev-spaces']"
62,"If a bilinear functional is continuous at $(0,0)$, then it is continuous everywhere","If a bilinear functional is continuous at , then it is continuous everywhere","(0,0)","I need help with this exercise: Let X be a complex Banach space. A bilinear functional on $X\times X$   is a map $B: X \times X\rightarrow \mathbb{C}$ such that for all $x,y  \in X$, the maps $B(x,\cdot),B(\cdot,y): X \rightarrow \mathbb{C}$ are   both linear. Consider the product space $X\times X$ with the norm   $\|(x,y)=\|(x,y)\|_{X\times X}=\|x\|+\|y\|$. By $(x_n,y_n)\rightarrow (x,y)$ we mean convergence in $X\times X$ in this norm. We say that B   is jointly continuous at $(x,y)\in X\times X$ if   $(x_n,y_n)\rightarrow(x,y)$ implies $B(x_n,y_n)\rightarrow B(x,y).$ a) Show that if $B: X\times X \rightarrow \mathbb{C}$ is jointly continous at $(0,0)$ then B is jointly continuous everywhere. (Hint:   note that bilinearity in particular means that   $B(x,y)=B(x/\sqrt{s},\sqrt{s}y)$, for $s >0$.) b) Show that if B is jointly continuous everywhere, then there exists a constant $C>0$ such that $|B(x,y)|\le C\|x\|\|y\|$ for all   $x,y \in X$.(Hint: Banach-Steinhaus). I am struggling with a) but I think I managed b). Do you have any tips for a), and could you check if I solved b correct please? For a), I tried this: Let $(x_n,y_n) \rightarrow (x,y)$. $|B(x_n,y_n)-B(x,y)|=|B(x_n,y_n)-B(x,y_n)+B(x,y_n)-B(x,y)|=|B(x-x_n,y_n)+B(x,y_n-y)|$. But this doesn't seem to lead anywhere. I am also not quite sure how we are supposed to use the hint given. For b) I tried: $|B(x,y)|=|f_x(y)|$, where $f_x(\cdot)=B(x,\cdot)$. Now joint continuity gives continuity in one variable, so we get that: $|f_x(y)|\le\|f_x\|\|y\|$. I will be able to solve the problem if I then can show that $\|f_x\|\le C\|x\|$. If x is the zero vector it is ok. If x is not the zero-vector, we look at the family of operators indexed by x, given by $f_x/\|x\|$. Then if we keep y, then for every operator given by x we have: $|f_x(y)/\|x\||=|B(x,y)|/\|x\|\le \|f_y\|\|x\|/\|x\|=\|f_y\|$. So by the uniform boundedness principle, we have that $\|f_x\|/\|x\|\le C$. (Here we use that X is a Banach-space.) Is b correct? And could you please give some hints for a)?","I need help with this exercise: Let X be a complex Banach space. A bilinear functional on $X\times X$   is a map $B: X \times X\rightarrow \mathbb{C}$ such that for all $x,y  \in X$, the maps $B(x,\cdot),B(\cdot,y): X \rightarrow \mathbb{C}$ are   both linear. Consider the product space $X\times X$ with the norm   $\|(x,y)=\|(x,y)\|_{X\times X}=\|x\|+\|y\|$. By $(x_n,y_n)\rightarrow (x,y)$ we mean convergence in $X\times X$ in this norm. We say that B   is jointly continuous at $(x,y)\in X\times X$ if   $(x_n,y_n)\rightarrow(x,y)$ implies $B(x_n,y_n)\rightarrow B(x,y).$ a) Show that if $B: X\times X \rightarrow \mathbb{C}$ is jointly continous at $(0,0)$ then B is jointly continuous everywhere. (Hint:   note that bilinearity in particular means that   $B(x,y)=B(x/\sqrt{s},\sqrt{s}y)$, for $s >0$.) b) Show that if B is jointly continuous everywhere, then there exists a constant $C>0$ such that $|B(x,y)|\le C\|x\|\|y\|$ for all   $x,y \in X$.(Hint: Banach-Steinhaus). I am struggling with a) but I think I managed b). Do you have any tips for a), and could you check if I solved b correct please? For a), I tried this: Let $(x_n,y_n) \rightarrow (x,y)$. $|B(x_n,y_n)-B(x,y)|=|B(x_n,y_n)-B(x,y_n)+B(x,y_n)-B(x,y)|=|B(x-x_n,y_n)+B(x,y_n-y)|$. But this doesn't seem to lead anywhere. I am also not quite sure how we are supposed to use the hint given. For b) I tried: $|B(x,y)|=|f_x(y)|$, where $f_x(\cdot)=B(x,\cdot)$. Now joint continuity gives continuity in one variable, so we get that: $|f_x(y)|\le\|f_x\|\|y\|$. I will be able to solve the problem if I then can show that $\|f_x\|\le C\|x\|$. If x is the zero vector it is ok. If x is not the zero-vector, we look at the family of operators indexed by x, given by $f_x/\|x\|$. Then if we keep y, then for every operator given by x we have: $|f_x(y)/\|x\||=|B(x,y)|/\|x\|\le \|f_y\|\|x\|/\|x\|=\|f_y\|$. So by the uniform boundedness principle, we have that $\|f_x\|/\|x\|\le C$. (Here we use that X is a Banach-space.) Is b correct? And could you please give some hints for a)?",,"['functional-analysis', 'banach-spaces', 'bilinear-form']"
63,"Mujica's ""Complex analysis in Banach spaces"" exercise 1.2.B","Mujica's ""Complex analysis in Banach spaces"" exercise 1.2.B",,"I'm trying to prove Exercise 1.2.B of Mujica's book ""Complex analysis in Banach spaces"" which states the following: Let $ P: E \to F$ be a mapping such that $P|_M \in P_a(\left.^m M; F \right.)$ for each sub space $M$ of $E$ of dimension $\le m+1$. Show that $P \in P_a(\left.^m E; F \right.)$ Where $E,F$ are Banach spaces and $P_a(\left.^m M; F \right.)$ is the vector space of all the m-homogeneous polynomials from $M$ to $F$. I don't know how to prove this. What I think is that for a given element of the space I should somehow decompose it in parts belonging to different sub spaces and then apply the hypothesis. But the hypothesis only holds for finite dimensional sub spaces and therefore I don't know how to proceed. How should I prove this?","I'm trying to prove Exercise 1.2.B of Mujica's book ""Complex analysis in Banach spaces"" which states the following: Let $ P: E \to F$ be a mapping such that $P|_M \in P_a(\left.^m M; F \right.)$ for each sub space $M$ of $E$ of dimension $\le m+1$. Show that $P \in P_a(\left.^m E; F \right.)$ Where $E,F$ are Banach spaces and $P_a(\left.^m M; F \right.)$ is the vector space of all the m-homogeneous polynomials from $M$ to $F$. I don't know how to prove this. What I think is that for a given element of the space I should somehow decompose it in parts belonging to different sub spaces and then apply the hypothesis. But the hypothesis only holds for finite dimensional sub spaces and therefore I don't know how to proceed. How should I prove this?",,"['functional-analysis', 'polynomials', 'banach-spaces']"
64,Normed Quotient Space,Normed Quotient Space,,"If $X$ is a normed vector space and $M$ is a proper closed subspace, I want to show that for any $\epsilon>0$ there exists an $x\in X$ such that $\|x\|=1$ and $\|x+M\|\geq 1-\epsilon$. Is there anything wrong with arguing as follows: Suppose by contradiction there exists some $\epsilon >0$ such that for all $x\in X$ with $\|x\|=1$, $\|x+M\| <1-\epsilon$. Then $\|x+M\| <1-\epsilon=\|x\|-\epsilon \implies \epsilon <\|x\|-\|x+M\| \leq \|x\|-\|x+y\|$ (for all $y\in M$), $\leq \|x\|-\|x\|-\|y\|$, i.e. $\epsilon \leq -\|y\|$ which is impossible. Can verify if this makes sense? Can someone offer a more intuitive argument? Thanks","If $X$ is a normed vector space and $M$ is a proper closed subspace, I want to show that for any $\epsilon>0$ there exists an $x\in X$ such that $\|x\|=1$ and $\|x+M\|\geq 1-\epsilon$. Is there anything wrong with arguing as follows: Suppose by contradiction there exists some $\epsilon >0$ such that for all $x\in X$ with $\|x\|=1$, $\|x+M\| <1-\epsilon$. Then $\|x+M\| <1-\epsilon=\|x\|-\epsilon \implies \epsilon <\|x\|-\|x+M\| \leq \|x\|-\|x+y\|$ (for all $y\in M$), $\leq \|x\|-\|x\|-\|y\|$, i.e. $\epsilon \leq -\|y\|$ which is impossible. Can verify if this makes sense? Can someone offer a more intuitive argument? Thanks",,"['real-analysis', 'analysis', 'functional-analysis', 'normed-spaces', 'quotient-spaces']"
65,Best approximation theorem. Hilbert space,Best approximation theorem. Hilbert space,,"$Y$ is a closed subspace of a Hilbert space $H$ and $x \in H$. To prove that $x \in $ orthogonal complement of Y if and only if $||x-y|| \geq ||x||$ for all $y \in Y$. Necessary condition is easy to proof, but I am trying to prove the sufficiency. I tried with the Best approximation theorem but can not been able to prove that $x \in $ orthogonal complement of $Y$. Any help will be appreciated.","$Y$ is a closed subspace of a Hilbert space $H$ and $x \in H$. To prove that $x \in $ orthogonal complement of Y if and only if $||x-y|| \geq ||x||$ for all $y \in Y$. Necessary condition is easy to proof, but I am trying to prove the sufficiency. I tried with the Best approximation theorem but can not been able to prove that $x \in $ orthogonal complement of $Y$. Any help will be appreciated.",,"['functional-analysis', 'hilbert-spaces']"
66,How do I find a bounded linear functional under the assumptions of the following theorem?,How do I find a bounded linear functional under the assumptions of the following theorem?,,"Theorem: Let $X$ be a normed space and $0 \not= x_0 \in X$ be a arbitrary. Then there exists a bounded linear functional $\bar f$ on $X$ such that $$\|\bar f \|=1, \quad \bar f(x_0) =\|x_0\|.$$ Problem: Find $\bar f$ when $X$ is the Euclidean plane $\mathbb{R}^2$. My interpretation of the task is that I have to explicitly specify a functional that satisfies the conditions and works for arbitrary non-zero points in the plane. I've been told however that such a functional need not be unique and may depend on each $x_0$. I'm confused now about how to go about solving this problem. I'd also appreciate if someone can comment on if the following solution to finding some $f \in X'$ such that $\|f\|=\|x_0\|^{-1}$ and $f(x)=1$ under the assumptions is correct? I've defined $f(x)=\frac{1}{\|x_0\|} \bar f(x).$ Then $f$ is a bounded linear functional on $X$ because of $\bar f$ of the theorem. $$f(x_0)=\frac{1}{\|x_0\|}\bar f(x_0)=\frac{1}{\|x_0\|} \|x_0\|=1.$$ and since $f(x)=\frac{1}{\|x_0\|} \bar f(x)$, $\|f\|=\frac{1}{\|x_0\|} \|\bar f\|$ (Does this equality require any further justification? Intuitively I can see it has to hold, but I'm unable to find a good reason for why and would like someone to explain this to me.) $=\frac{1}{\|x_0\|}$.","Theorem: Let $X$ be a normed space and $0 \not= x_0 \in X$ be a arbitrary. Then there exists a bounded linear functional $\bar f$ on $X$ such that $$\|\bar f \|=1, \quad \bar f(x_0) =\|x_0\|.$$ Problem: Find $\bar f$ when $X$ is the Euclidean plane $\mathbb{R}^2$. My interpretation of the task is that I have to explicitly specify a functional that satisfies the conditions and works for arbitrary non-zero points in the plane. I've been told however that such a functional need not be unique and may depend on each $x_0$. I'm confused now about how to go about solving this problem. I'd also appreciate if someone can comment on if the following solution to finding some $f \in X'$ such that $\|f\|=\|x_0\|^{-1}$ and $f(x)=1$ under the assumptions is correct? I've defined $f(x)=\frac{1}{\|x_0\|} \bar f(x).$ Then $f$ is a bounded linear functional on $X$ because of $\bar f$ of the theorem. $$f(x_0)=\frac{1}{\|x_0\|}\bar f(x_0)=\frac{1}{\|x_0\|} \|x_0\|=1.$$ and since $f(x)=\frac{1}{\|x_0\|} \bar f(x)$, $\|f\|=\frac{1}{\|x_0\|} \|\bar f\|$ (Does this equality require any further justification? Intuitively I can see it has to hold, but I'm unable to find a good reason for why and would like someone to explain this to me.) $=\frac{1}{\|x_0\|}$.",,['functional-analysis']
67,"$\int_0^\infty t|f'(t)|^2\,dt < \infty$ and $\lim_{T \to \infty} T^{-1} \int_0^T f(t)\,dt = L$, do we have that $f(t) \to L$ as $t \to \infty$?","and , do we have that  as ?","\int_0^\infty t|f'(t)|^2\,dt < \infty \lim_{T \to \infty} T^{-1} \int_0^T f(t)\,dt = L f(t) \to L t \to \infty","Let $f \in C^1([0, \infty))$ and suppose that $\int_0^\infty t|f'(t)|^2\,dt < \infty$ and $\lim_{T \to \infty} T^{-1} \int_0^T f(t)\,dt = L$. Do we have that $f(t) \to L$ as $t \to \infty$?","Let $f \in C^1([0, \infty))$ and suppose that $\int_0^\infty t|f'(t)|^2\,dt < \infty$ and $\lim_{T \to \infty} T^{-1} \int_0^T f(t)\,dt = L$. Do we have that $f(t) \to L$ as $t \to \infty$?",,"['calculus', 'real-analysis']"
68,"Let $X=\ell^\infty$, $f_m \in X^*$ be defined as $f_m(\langle x_n\rangle)=x_m$. Does $(f_n)_{n\in \bf N}$ has a weak* convergent subsequence?","Let ,  be defined as . Does  has a weak* convergent subsequence?",X=\ell^\infty f_m \in X^* f_m(\langle x_n\rangle)=x_m (f_n)_{n\in \bf N},"Let $X=\ell^\infty$, $f_m \in X^*$ be defined as $f_m(\langle x_n\rangle)=x_m$.Does $(f_n)_{n\in \bf N}$ has a weak* convergent subsequence? I think $(f_n)$ does not have convergent subsequence but unable to prove this properly. Please help!","Let $X=\ell^\infty$, $f_m \in X^*$ be defined as $f_m(\langle x_n\rangle)=x_m$.Does $(f_n)_{n\in \bf N}$ has a weak* convergent subsequence? I think $(f_n)$ does not have convergent subsequence but unable to prove this properly. Please help!",,"['functional-analysis', 'weak-convergence']"
69,Applications of the Hahn Banach theorem for normed spaces?,Applications of the Hahn Banach theorem for normed spaces?,,"If $X$ is the Euclidean space $\mathbb{R}^3$ and $f(x)=a_1 x_1 + a_2 x_2$, $x=(x_1, x_2) $  a bounded linear functional on the subspace $\mathbb{R}^2$ of $X$, how do I find a bounded linear functional $f'$ that is an extension of $f$ with the same norm as that of $f$? Does the functional $f'(x) = a_1 x_1 + a_2 x_2 + 0 \cdot x_3$ work?","If $X$ is the Euclidean space $\mathbb{R}^3$ and $f(x)=a_1 x_1 + a_2 x_2$, $x=(x_1, x_2) $  a bounded linear functional on the subspace $\mathbb{R}^2$ of $X$, how do I find a bounded linear functional $f'$ that is an extension of $f$ with the same norm as that of $f$? Does the functional $f'(x) = a_1 x_1 + a_2 x_2 + 0 \cdot x_3$ work?",,['functional-analysis']
70,"In the Krein-Milman theorem, can the weak closure of the convex hull be replaced by norm-closure?","In the Krein-Milman theorem, can the weak closure of the convex hull be replaced by norm-closure?",,"I have a question on the following formulation of the Krein-Milman theorem: Consider a vector space $X$ equipped with the  weak topology induced by a separating space $X^*$ of functionals on $X$. Then for each convex, compact subset $D$ of $X$, the convex hull of the extremal boundary $\partial D$ of $D$ is dense in $D$. Now I want to apply this result to $X^*$, the dual of a Banach space $X$, equipped with the wk* topology. What I'm interested in is whether or not we can say that the convex hull of $\partial D$ is also norm-dense in $D$? Since the wk* topology is weaker then the norm topology on $X^*$ this is not a given, and I suspect it's generally not true, but I'm curious about this, and don't see a way of proving/disproving it. The book I'm reading (by Pedersen) proves density by taking $x\in D\setminus\overline{\operatorname{convhull}(\partial D)}$. Then since $x$ is part of an open set in a LCS there is an open convex nbd $U$ of $x$ disjoint from $\overline{\operatorname{convhull}(\partial D)}$. Then there is a functional separating $U$ from $\overline{\operatorname{convhull}(\partial D)}$, from which a contradiction is derived. However, there are more open nbds of $x$ in the norm topology then in the wk* topology, so although there cannot be a wk*-open nbd of $x$ in $D\setminus\overline{\operatorname{convhull}(\partial D)}$, there might be such norm-open nbds of $x$. Again, I don't see why denisty in the norm topology should hold (it's not an exercise to prove this; it might very well not be true), but I'm wondering if there is something to be said here.","I have a question on the following formulation of the Krein-Milman theorem: Consider a vector space $X$ equipped with the  weak topology induced by a separating space $X^*$ of functionals on $X$. Then for each convex, compact subset $D$ of $X$, the convex hull of the extremal boundary $\partial D$ of $D$ is dense in $D$. Now I want to apply this result to $X^*$, the dual of a Banach space $X$, equipped with the wk* topology. What I'm interested in is whether or not we can say that the convex hull of $\partial D$ is also norm-dense in $D$? Since the wk* topology is weaker then the norm topology on $X^*$ this is not a given, and I suspect it's generally not true, but I'm curious about this, and don't see a way of proving/disproving it. The book I'm reading (by Pedersen) proves density by taking $x\in D\setminus\overline{\operatorname{convhull}(\partial D)}$. Then since $x$ is part of an open set in a LCS there is an open convex nbd $U$ of $x$ disjoint from $\overline{\operatorname{convhull}(\partial D)}$. Then there is a functional separating $U$ from $\overline{\operatorname{convhull}(\partial D)}$, from which a contradiction is derived. However, there are more open nbds of $x$ in the norm topology then in the wk* topology, so although there cannot be a wk*-open nbd of $x$ in $D\setminus\overline{\operatorname{convhull}(\partial D)}$, there might be such norm-open nbds of $x$. Again, I don't see why denisty in the norm topology should hold (it's not an exercise to prove this; it might very well not be true), but I'm wondering if there is something to be said here.",,"['functional-analysis', 'convex-analysis', 'banach-spaces', 'topological-vector-spaces', 'locally-convex-spaces']"
71,Show that $T: X \to Y $ is a bounded linear operator,Show that  is a bounded linear operator,T: X \to Y ,"Let $T: X \to Y$ be a linear operator where $X$ and $Y$ are Banach spaces and $\mathcal F$ be a subset of  $Y^*$ that separates the points of $Y$. Given that $f(T(x_n))\to  0$ whenever $f$ is in $\mathcal F$ and $||x_n|| \to 0$. Show that $T$ is bounded. My work: I was trying to solve this using the definition of a operator,  $$i.e.,\, ||T|| = sup\{ ||Tx|| : ||x||=1 \}$$ but I failed.Need some help.","Let $T: X \to Y$ be a linear operator where $X$ and $Y$ are Banach spaces and $\mathcal F$ be a subset of  $Y^*$ that separates the points of $Y$. Given that $f(T(x_n))\to  0$ whenever $f$ is in $\mathcal F$ and $||x_n|| \to 0$. Show that $T$ is bounded. My work: I was trying to solve this using the definition of a operator,  $$i.e.,\, ||T|| = sup\{ ||Tx|| : ||x||=1 \}$$ but I failed.Need some help.",,"['functional-analysis', 'banach-spaces', 'linear-transformations']"
72,Existence of Unitary Map,Existence of Unitary Map,,"I've been recently introduced to Unitary operators of a Hilbert space and I've been wondering the following. Existence of a unitary operator $T$ on a (possibly infinite) Hilbert space $H$ is simple enough. The identity map works and otherwise any ""partial isometry"" with full initial and final subspace does as well. However, if I'm given subspaces $U, V \subset H$, then is there any assumption (not directly assuming that $U = V$) I can make that guarantees the existence of unitary operator $T:H\to H$ such that the restriction $$T\restriction_U:U\to V$$ is a surjective isometry? Would I somehow be able to construct a unitary operator if I assumed $\dim U = \dim V$?","I've been recently introduced to Unitary operators of a Hilbert space and I've been wondering the following. Existence of a unitary operator $T$ on a (possibly infinite) Hilbert space $H$ is simple enough. The identity map works and otherwise any ""partial isometry"" with full initial and final subspace does as well. However, if I'm given subspaces $U, V \subset H$, then is there any assumption (not directly assuming that $U = V$) I can make that guarantees the existence of unitary operator $T:H\to H$ such that the restriction $$T\restriction_U:U\to V$$ is a surjective isometry? Would I somehow be able to construct a unitary operator if I assumed $\dim U = \dim V$?",,"['real-analysis', 'functional-analysis', 'operator-theory', 'hilbert-spaces', 'banach-spaces']"
73,Minkowski functional of a convex set is a convex function.,Minkowski functional of a convex set is a convex function.,,"Let $X$ be a real vector space, and $K$ be a convex set with $2$ properties: $0\in K$ and $\forall x\in X, \exists t >0$ s.t. $x/t\in K$. Define the Minkowski functional of the set $K$ to be $p_K(x)=\inf \{t>0: x/t\in K \}$. Show that  $p_K(x)$ is convex. I have tried for some while using the definition of a convex function, but failed to prove this fact. Any idea on how to show it? Thanks!","Let $X$ be a real vector space, and $K$ be a convex set with $2$ properties: $0\in K$ and $\forall x\in X, \exists t >0$ s.t. $x/t\in K$. Define the Minkowski functional of the set $K$ to be $p_K(x)=\inf \{t>0: x/t\in K \}$. Show that  $p_K(x)$ is convex. I have tried for some while using the definition of a convex function, but failed to prove this fact. Any idea on how to show it? Thanks!",,"['functional-analysis', 'convex-analysis']"
74,"Let $f$ and $g$ linear operators where $f$ and $g$ commute and $f$ has simple spectrumm, then there is $P$ a polynomial such thah $g=P(f)$.","Let  and  linear operators where  and  commute and  has simple spectrumm, then there is  a polynomial such thah .",f g f g f P g=P(f),"Let $f : \mathbb{C}^{n}\rightarrow \mathbb{C}^{n}$ be a linear operator with  a simple spectrum, furthermore, let $g : \mathbb{C}^{n}\rightarrow \mathbb{C}^n $ be a linear operator such that  $f$ and $g$ commute. Show that there is $P$ a polynomial such that $g=P(f)$. Remark: The spectrum is simple when the characteristic polynomial has not multiple roots.","Let $f : \mathbb{C}^{n}\rightarrow \mathbb{C}^{n}$ be a linear operator with  a simple spectrum, furthermore, let $g : \mathbb{C}^{n}\rightarrow \mathbb{C}^n $ be a linear operator such that  $f$ and $g$ commute. Show that there is $P$ a polynomial such that $g=P(f)$. Remark: The spectrum is simple when the characteristic polynomial has not multiple roots.",,"['linear-algebra', 'functional-analysis', 'spectral-theory']"
75,Example (for some $X$) of a nonclosed ideal in $\mathbb{C}(X)$?,Example (for some ) of a nonclosed ideal in ?,X \mathbb{C}(X),"Let $X$ be a compact metric space and $\mathbb{C}(X)$ the algebra of continuous functions $f: X \to \mathbb{C}$, with pointwise operations. We equip $\mathbb{C}(X)$ with the maximum norm $N(f) := \max_{x \in X}|f(x)|$. An ideal $I \subset \mathbb{C}(X)$ is called a closed ideal if $I$ is a closed subset of $\mathbb{C}(X)$ viewed as a metric space. For any subset $Y \subset X$, the set $I_Y := \{f \in \mathbb{C}(X) : f(y) = 0 \text{ for all }y \in Y\}$ is a closed ideal in $\mathbb{C}(X)$. My question is, what is an example (for some $X$) of a nonclosed ideal in $\mathbb{C}(X)$?","Let $X$ be a compact metric space and $\mathbb{C}(X)$ the algebra of continuous functions $f: X \to \mathbb{C}$, with pointwise operations. We equip $\mathbb{C}(X)$ with the maximum norm $N(f) := \max_{x \in X}|f(x)|$. An ideal $I \subset \mathbb{C}(X)$ is called a closed ideal if $I$ is a closed subset of $\mathbb{C}(X)$ viewed as a metric space. For any subset $Y \subset X$, the set $I_Y := \{f \in \mathbb{C}(X) : f(y) = 0 \text{ for all }y \in Y\}$ is a closed ideal in $\mathbb{C}(X)$. My question is, what is an example (for some $X$) of a nonclosed ideal in $\mathbb{C}(X)$?",,"['abstract-algebra', 'functional-analysis', 'commutative-algebra', 'ideals']"
76,weak convergence in $l^p$ implies bounded and pointwise convergence,weak convergence in  implies bounded and pointwise convergence,l^p,"Let $1<p<\infty$, $(u_n)_{n\in\mathbb{N}}\subseteq l^p(\mathbb{N})$ such that $u_n$ converges weakly to $u\in l^p(\mathbb{N})$. Prove that (1) $(u_n)$ is bounded and (2) for all fixed component $j\in n\in\mathbb{N}$ it is $\lim_{n\to\infty} u_{n,j}=u_j$ (in lecture we called it ""pointwise convergence of sequences""). My questions are: (1) is ""$(u_n)$ bounded"", i.e. $\|u_n\|<\infty$ for all $n\in\mathbb{N}$ already satisfied because $(u_n)_{n\in\mathbb{N}}\subseteq l^p(\mathbb{N})$? For (2) it is to show that $|u_{n,j}-u_j|\to 0,\; n\to\infty$ for fixed $j\in\mathbb{N}$. But I don't know how to do this.","Let $1<p<\infty$, $(u_n)_{n\in\mathbb{N}}\subseteq l^p(\mathbb{N})$ such that $u_n$ converges weakly to $u\in l^p(\mathbb{N})$. Prove that (1) $(u_n)$ is bounded and (2) for all fixed component $j\in n\in\mathbb{N}$ it is $\lim_{n\to\infty} u_{n,j}=u_j$ (in lecture we called it ""pointwise convergence of sequences""). My questions are: (1) is ""$(u_n)$ bounded"", i.e. $\|u_n\|<\infty$ for all $n\in\mathbb{N}$ already satisfied because $(u_n)_{n\in\mathbb{N}}\subseteq l^p(\mathbb{N})$? For (2) it is to show that $|u_{n,j}-u_j|\to 0,\; n\to\infty$ for fixed $j\in\mathbb{N}$. But I don't know how to do this.",,"['sequences-and-series', 'functional-analysis', 'convergence-divergence', 'weak-convergence']"
77,Dual of a Banach space is isometric to a subspace of a Lipschitz dual,Dual of a Banach space is isometric to a subspace of a Lipschitz dual,,"Suppose $E$ is a Banach space. Denote $E^*$ as a dual space of $E$ and $Lip(E)$ as the Lipschitz dual of $E$. In other words, $Lip(E) = \{ f:E \rightarrow \mathbb{R} | f(0)=0 \}$ and all $f$ is Lipschitz. In the book Geometric Nonlinear Functional Analysis, volume $1$, page $173$, there is this sentence before Proposition $7.5$: $E^*$ is naturally isometric to a subspace of $Lip(E)$. Question: How to show the statement above? Can give any hint?","Suppose $E$ is a Banach space. Denote $E^*$ as a dual space of $E$ and $Lip(E)$ as the Lipschitz dual of $E$. In other words, $Lip(E) = \{ f:E \rightarrow \mathbb{R} | f(0)=0 \}$ and all $f$ is Lipschitz. In the book Geometric Nonlinear Functional Analysis, volume $1$, page $173$, there is this sentence before Proposition $7.5$: $E^*$ is naturally isometric to a subspace of $Lip(E)$. Question: How to show the statement above? Can give any hint?",,"['functional-analysis', 'lipschitz-functions']"
78,Holomorphic Functional Calculus for the Square Root,Holomorphic Functional Calculus for the Square Root,,"I'm working on a problem set, so I'm not looking for a solution, but just maybe a pointer on where I'm going wrong. I want to use the holomorphic functional calculus to determine the square root of the matrix $$T=\begin{bmatrix}1&1\\0&1\end{bmatrix}.$$ The spectrum of this matrix is just $\{1\}$, so a convenient curve enclosing the spectrum is $\gamma(t)=2e^{it}$ for $t\in\left[-\pi,\pi\right)$. Now I know that for $f(z)$ holomorphic (at least on a region containing both the spectrum and $\gamma$), the operator $\hspace{20pt}f(T)=\frac{1}{2\pi i}\int_\gamma f(\xi)(\xi I-T)^{-1}d\xi=\frac{1}{2\pi i}\int_{-\pi}^\pi f(2e^{it})(\xi I-T)^{-1}\cdot 2ie^{it}dt$ is well-defined. When I then plug in a polynomial for $f$, I get the right answer. However, when I plug in the square root, which I'm taking to be $\sqrt{\gamma(t)}=\sqrt{2}e^{it/2}$ so that the branch cut is along the negative real axis, I get a nonsensical answer: $\hspace{20pt}\left(\frac{20+\sqrt{10}\pi-2\sqrt{10}\arctan\sqrt{10}}{5\sqrt{2}\pi},\frac{10\sqrt{2}+11\sqrt{5}\pi-22\sqrt{5}\arctan\sqrt{10}}{110\pi};0,\frac{20+\sqrt{10}\pi-2\sqrt{10}\arctan\sqrt{10}}{5\sqrt{2}\pi}\right).$ Of course I expect $\sqrt{T}=\begin{bmatrix}1&1/2\\0&1\end{bmatrix}$. The only thing that seems different is that the square root is a multi-valued function whereas polynomials are single-valued, so I'm assuming my error is failing to take something along these lines into account. (I should also not that I'm doing the integral in Mathematica, so maybe there's some problem with doing that.)","I'm working on a problem set, so I'm not looking for a solution, but just maybe a pointer on where I'm going wrong. I want to use the holomorphic functional calculus to determine the square root of the matrix $$T=\begin{bmatrix}1&1\\0&1\end{bmatrix}.$$ The spectrum of this matrix is just $\{1\}$, so a convenient curve enclosing the spectrum is $\gamma(t)=2e^{it}$ for $t\in\left[-\pi,\pi\right)$. Now I know that for $f(z)$ holomorphic (at least on a region containing both the spectrum and $\gamma$), the operator $\hspace{20pt}f(T)=\frac{1}{2\pi i}\int_\gamma f(\xi)(\xi I-T)^{-1}d\xi=\frac{1}{2\pi i}\int_{-\pi}^\pi f(2e^{it})(\xi I-T)^{-1}\cdot 2ie^{it}dt$ is well-defined. When I then plug in a polynomial for $f$, I get the right answer. However, when I plug in the square root, which I'm taking to be $\sqrt{\gamma(t)}=\sqrt{2}e^{it/2}$ so that the branch cut is along the negative real axis, I get a nonsensical answer: $\hspace{20pt}\left(\frac{20+\sqrt{10}\pi-2\sqrt{10}\arctan\sqrt{10}}{5\sqrt{2}\pi},\frac{10\sqrt{2}+11\sqrt{5}\pi-22\sqrt{5}\arctan\sqrt{10}}{110\pi};0,\frac{20+\sqrt{10}\pi-2\sqrt{10}\arctan\sqrt{10}}{5\sqrt{2}\pi}\right).$ Of course I expect $\sqrt{T}=\begin{bmatrix}1&1/2\\0&1\end{bmatrix}$. The only thing that seems different is that the square root is a multi-valued function whereas polynomials are single-valued, so I'm assuming my error is failing to take something along these lines into account. (I should also not that I'm doing the integral in Mathematica, so maybe there's some problem with doing that.)",,"['functional-analysis', 'operator-theory', 'operator-algebras', 'functional-calculus']"
79,"Existence of a function in $C([0,1], \mathbb{R}), ||.||_{\infty}$ s.t. $2f(x) -f(x)^2 + f(x^2)=g(x)$",Existence of a function in  s.t.,"C([0,1], \mathbb{R}), ||.||_{\infty} 2f(x) -f(x)^2 + f(x^2)=g(x)","Let $g \in E= C([0,1], \mathbb{R}), ||.||_{\infty}$ s.t. $\|g\|_\infty< \frac{1}{4}$. I need to show that there exists $f$ in the same set such that  $$2f(x) - f(x)^2 + f(x^2)=g(x).$$ Can anyone please provide me with a hint? Should I maybe look into $\Gamma :E \to E$, $\Gamma (f)= 2f(x)-f(x)^2+ f(x^2)$?","Let $g \in E= C([0,1], \mathbb{R}), ||.||_{\infty}$ s.t. $\|g\|_\infty< \frac{1}{4}$. I need to show that there exists $f$ in the same set such that  $$2f(x) - f(x)^2 + f(x^2)=g(x).$$ Can anyone please provide me with a hint? Should I maybe look into $\Gamma :E \to E$, $\Gamma (f)= 2f(x)-f(x)^2+ f(x^2)$?",,"['functional-analysis', 'metric-spaces']"
80,Compactness of the unit sphere in finite-dimensional normed vector space,Compactness of the unit sphere in finite-dimensional normed vector space,,"In order to prove that norms defined on any finite-dimensional real (or complex) vector space $E$ are equivalents, I need to proof the compactness of the unit sphere $S_{\infty}=\{x\in E\,\vert\,||x||_{\infty}=1\}$ where the particular norm depends on a basis $\mathcal{B}$. Let $\dim E=n$. In my notes, we define an isometry $\phi:E,||\cdot||_{\infty}\to\mathbb{C}^{n},||\cdot||_{\infty}:x\mapsto(x_{1},\dots,x_{n})$ and we say that, as it is an isometry, $\phi(S_{\infty})$ is compact implies $S_{\infty}$ is compact. I understand that $\phi(S_{\infty})$ is compact because bounded and closed. I don't understand the implication $\phi(A)$ compact $\implies$ $A$ compact. Thanks in advance for your answers","In order to prove that norms defined on any finite-dimensional real (or complex) vector space $E$ are equivalents, I need to proof the compactness of the unit sphere $S_{\infty}=\{x\in E\,\vert\,||x||_{\infty}=1\}$ where the particular norm depends on a basis $\mathcal{B}$. Let $\dim E=n$. In my notes, we define an isometry $\phi:E,||\cdot||_{\infty}\to\mathbb{C}^{n},||\cdot||_{\infty}:x\mapsto(x_{1},\dots,x_{n})$ and we say that, as it is an isometry, $\phi(S_{\infty})$ is compact implies $S_{\infty}$ is compact. I understand that $\phi(S_{\infty})$ is compact because bounded and closed. I don't understand the implication $\phi(A)$ compact $\implies$ $A$ compact. Thanks in advance for your answers",,"['functional-analysis', 'metric-spaces', 'compactness', 'normed-spaces', 'spheres']"
81,$\overline V= \overline {V_1}\oplus \overline {V_2}$?,?,\overline V= \overline {V_1}\oplus \overline {V_2},"Let $V$ be a topological vector space. And $V=V_1\oplus V_2$(Algebraic Sum) Now, consider $\overline V$, the closure of $V$. Can we say that, $$\overline V= \overline {V_1}\oplus \overline {V_2}$$ MY WORK Clearly, $\overline {V_1}\oplus \overline{V_2}\subset \overline{V}$. For the otherway round, letting $x\in \overline{V}$, we have a sequence $(x_n)$ in $V$ such that $x_n\to x$. So, for all $n$ there exist $x_{n1},x_{n2}$ s.t, $x_{n1}+x_{n2}\to x$ But after I couldn't say anything about this.","Let $V$ be a topological vector space. And $V=V_1\oplus V_2$(Algebraic Sum) Now, consider $\overline V$, the closure of $V$. Can we say that, $$\overline V= \overline {V_1}\oplus \overline {V_2}$$ MY WORK Clearly, $\overline {V_1}\oplus \overline{V_2}\subset \overline{V}$. For the otherway round, letting $x\in \overline{V}$, we have a sequence $(x_n)$ in $V$ such that $x_n\to x$. So, for all $n$ there exist $x_{n1},x_{n2}$ s.t, $x_{n1}+x_{n2}\to x$ But after I couldn't say anything about this.",,"['functional-analysis', 'vector-spaces']"
82,Let ${a_n}$ be in $\ell_\infty$ . Prove that $f$ defined by $f({b_n})=\sum_\infty a_nb_n$ is a continuous real valued function on $\ell_1$,Let  be in  . Prove that  defined by  is a continuous real valued function on,{a_n} \ell_\infty f f({b_n})=\sum_\infty a_nb_n \ell_1,"Let $a_n$ be in $l^\infty$ . Prove that $f$ defined by  \begin{equation} f(b_n) = \sum_{n\geq 0} a_nb_n \end{equation} is a continuous real valued function on $l^1$ My thoughts on the problem are that to show continuity, I must show that for any $\epsilon >0$, there exists a $\delta >0$ such that $\vert b_n-x_n\vert< \delta$ implies  $\vert a_nb_n-a_nx_n \vert< \epsilon$. I'm thinking about factoring out the $a_n$, and letting my $\epsilon = \delta/a_n$.","Let $a_n$ be in $l^\infty$ . Prove that $f$ defined by  \begin{equation} f(b_n) = \sum_{n\geq 0} a_nb_n \end{equation} is a continuous real valued function on $l^1$ My thoughts on the problem are that to show continuity, I must show that for any $\epsilon >0$, there exists a $\delta >0$ such that $\vert b_n-x_n\vert< \delta$ implies  $\vert a_nb_n-a_nx_n \vert< \epsilon$. I'm thinking about factoring out the $a_n$, and letting my $\epsilon = \delta/a_n$.",,"['functional-analysis', 'lp-spaces']"
83,Convergence of vector-valued truncation in $H_0^1(\Omega)^m$,Convergence of vector-valued truncation in,H_0^1(\Omega)^m,"Let $\Omega \subset \mathbb{R}^n$ be a bounded domain. For $m \in \mathbb{N}$ and $M > 0$ we denote by $T_M$ the truncation of vectors in $\mathbb{R}^m$ to length $M$, i.e., $$T_M(x) = \begin{cases} x & \text{if } \|x\| \le M, \\ M \, \frac{x}{\|x\|} & \text{if } \|x\| > M.\end{cases}$$ (We use the Euclidean norm in $\mathbb{R}^m$) Then $T_M$ is globally Lipschitz with modulus $1$ and thus, $x \mapsto T_M (u(x))$ belongs to $H_0^1(\Omega)^m$ for all $u \in H_0^1(\Omega)^m$, see, e.g., here . We denote this function by $T_M u$. Do we have $T_M u \to u$ in $H_0^1(\Omega)^m$ as $M \to \infty$? Some thoughts: Since $T_M u$ is bounded in $H_0^1(\Omega)^m$ and converges pointwise to $u$, we get weak convergence in $H_0^1(\Omega)$. The case $m = 1$ is classical. Here, one can use a cain rule and $\|\nabla(T_M u)(x)\| \le \|\nabla u(x)\|$ f.a.a. $x \in \Omega$. Then one can use dominated convergence for the gradient to get strong convergence in $H_0^1(\Omega)$. Such a chain rule seems not exist in the vectorial case above.","Let $\Omega \subset \mathbb{R}^n$ be a bounded domain. For $m \in \mathbb{N}$ and $M > 0$ we denote by $T_M$ the truncation of vectors in $\mathbb{R}^m$ to length $M$, i.e., $$T_M(x) = \begin{cases} x & \text{if } \|x\| \le M, \\ M \, \frac{x}{\|x\|} & \text{if } \|x\| > M.\end{cases}$$ (We use the Euclidean norm in $\mathbb{R}^m$) Then $T_M$ is globally Lipschitz with modulus $1$ and thus, $x \mapsto T_M (u(x))$ belongs to $H_0^1(\Omega)^m$ for all $u \in H_0^1(\Omega)^m$, see, e.g., here . We denote this function by $T_M u$. Do we have $T_M u \to u$ in $H_0^1(\Omega)^m$ as $M \to \infty$? Some thoughts: Since $T_M u$ is bounded in $H_0^1(\Omega)^m$ and converges pointwise to $u$, we get weak convergence in $H_0^1(\Omega)$. The case $m = 1$ is classical. Here, one can use a cain rule and $\|\nabla(T_M u)(x)\| \le \|\nabla u(x)\|$ f.a.a. $x \in \Omega$. Then one can use dominated convergence for the gradient to get strong convergence in $H_0^1(\Omega)$. Such a chain rule seems not exist in the vectorial case above.",,"['functional-analysis', 'sobolev-spaces']"
84,$L(\bigoplus_{i\in I} H_i)\cong \prod_{i\in I}L(H_i)$ as Banach spaces?,as Banach spaces?,L(\bigoplus_{i\in I} H_i)\cong \prod_{i\in I}L(H_i),"Let $\{H_i:i\in I\}$ be a system of complex Hilbert spaces, $L(\bigoplus_{i\in I} H_i)$ the set of bounded linear maps $T:\bigoplus_{i\in I} H_i\to \bigoplus_{i\in I} H_i$, equipped with the operator norm.  Let $\prod_{i\in I}L(H_i)$ the direct product of linear maps endowed with the norm $\|(T_i)_{i\in I}\|=sup_{i\in I}\|T_i\|$. Is $$L(\bigoplus_{i\in I} H_i)\cong \prod_{i\in I}L(H_i)$$ as banach spaces? The map which comes to my mind constructed as follows: Is $$\iota_i: H_i\to \bigoplus_{i\in I} H_i$$ the canonical injection, we define for each $i\in I$ a map $$\varphi_i:L(\bigoplus_{i\in I} H_i)\to L(H_i),\; \alpha\mapsto \alpha\circ \iota_i$$ Then extend it to a map $$L(\bigoplus_{i\in I} H_i)\to \prod_{i\in I}L(H_i).$$ But I'm not sure if it's possible here or if the construction makes sense in this context.","Let $\{H_i:i\in I\}$ be a system of complex Hilbert spaces, $L(\bigoplus_{i\in I} H_i)$ the set of bounded linear maps $T:\bigoplus_{i\in I} H_i\to \bigoplus_{i\in I} H_i$, equipped with the operator norm.  Let $\prod_{i\in I}L(H_i)$ the direct product of linear maps endowed with the norm $\|(T_i)_{i\in I}\|=sup_{i\in I}\|T_i\|$. Is $$L(\bigoplus_{i\in I} H_i)\cong \prod_{i\in I}L(H_i)$$ as banach spaces? The map which comes to my mind constructed as follows: Is $$\iota_i: H_i\to \bigoplus_{i\in I} H_i$$ the canonical injection, we define for each $i\in I$ a map $$\varphi_i:L(\bigoplus_{i\in I} H_i)\to L(H_i),\; \alpha\mapsto \alpha\circ \iota_i$$ Then extend it to a map $$L(\bigoplus_{i\in I} H_i)\to \prod_{i\in I}L(H_i).$$ But I'm not sure if it's possible here or if the construction makes sense in this context.",,['functional-analysis']
85,Derivative with respect to function,Derivative with respect to function,,"I am looking to calculate the derivative of a functional $\phi(\rho)$ with respect to $\rho$, that looks like $$\phi[\rho](x)=\rho(x)\int_0^1\log|x-y|\rho(y)dy.$$ I have read that the Gateaux derivative or Frechet derivative and calculus of variations are the right key words to look for. However I am very new to functional analysis and I am not sure who to proceed. Could someone help? PS: I need this for a computation so numerically I wanted to do $(\phi(\rho+tu)-\phi(\rho))/t$ for a small $t$ to approximate the derivative. Is this correct? I wouldn't mind to have an analytic result though. Thank you!","I am looking to calculate the derivative of a functional $\phi(\rho)$ with respect to $\rho$, that looks like $$\phi[\rho](x)=\rho(x)\int_0^1\log|x-y|\rho(y)dy.$$ I have read that the Gateaux derivative or Frechet derivative and calculus of variations are the right key words to look for. However I am very new to functional analysis and I am not sure who to proceed. Could someone help? PS: I need this for a computation so numerically I wanted to do $(\phi(\rho+tu)-\phi(\rho))/t$ for a small $t$ to approximate the derivative. Is this correct? I wouldn't mind to have an analytic result though. Thank you!",,"['functional-analysis', 'calculus-of-variations']"
86,universal $C^*$-algebra isomorphic to $K(l^2(I))$,universal -algebra isomorphic to,C^* K(l^2(I)),"Let $I\neq \emptyset$ be a set, $X=\{e_{ij}:i,j\in I\}$ with the relations $$R=\{e_{ij}e_{kl}=\begin{cases}e_{il},  & \text{if }j=k,\\ 0, & \text{ else} \end{cases},\;\; e_{ij}^*=e_{ji}:\; i,j,k,l\in I\}.$$ Prove that $C^*(X,R)\cong K(l^2(I))$. Here is $C^*(X,R)$ the universal $C^*$-algebra for $(X,R)$ and $K(l^2(I))$ are the linear, bounded operators $l^2(I)\to l^2(I)$ which are compact. The strategy is to prove first that $C^*(X,R)$ exists. I've already done that. Then I've constructed a representation $\varphi:X\to K(l^2(I)), e_{ij}\mapsto E_{ij}$ (i.e. $\varphi$ preserves the relations) where $\{E_{ij} \}_{i,j\in I}$ are the rank-1 projections in $K(l^2(I))$. Then from lecture we know that $\varphi$ induces a $*$-homomorphism $\hat{\varphi}:C^*(X,R)\to K(l^2(I))$. Now my questions: There is a hint on the worksheet that we have to show that every irreducible $\ast$-representation $\tau:C^*(X,R)\to L(H)$ is equivalent to $\hat{\varphi}$, i.e. there is an unitary operator $V:H\to l^2(I)$ such that $V\tau(a)=\hat{\varphi}(a) V$ for all $a\in A$. I know how to prove it, but why do I have to prove it? And why is $\hat{\varphi}$ bijective?","Let $I\neq \emptyset$ be a set, $X=\{e_{ij}:i,j\in I\}$ with the relations $$R=\{e_{ij}e_{kl}=\begin{cases}e_{il},  & \text{if }j=k,\\ 0, & \text{ else} \end{cases},\;\; e_{ij}^*=e_{ji}:\; i,j,k,l\in I\}.$$ Prove that $C^*(X,R)\cong K(l^2(I))$. Here is $C^*(X,R)$ the universal $C^*$-algebra for $(X,R)$ and $K(l^2(I))$ are the linear, bounded operators $l^2(I)\to l^2(I)$ which are compact. The strategy is to prove first that $C^*(X,R)$ exists. I've already done that. Then I've constructed a representation $\varphi:X\to K(l^2(I)), e_{ij}\mapsto E_{ij}$ (i.e. $\varphi$ preserves the relations) where $\{E_{ij} \}_{i,j\in I}$ are the rank-1 projections in $K(l^2(I))$. Then from lecture we know that $\varphi$ induces a $*$-homomorphism $\hat{\varphi}:C^*(X,R)\to K(l^2(I))$. Now my questions: There is a hint on the worksheet that we have to show that every irreducible $\ast$-representation $\tau:C^*(X,R)\to L(H)$ is equivalent to $\hat{\varphi}$, i.e. there is an unitary operator $V:H\to l^2(I)$ such that $V\tau(a)=\hat{\varphi}(a) V$ for all $a\in A$. I know how to prove it, but why do I have to prove it? And why is $\hat{\varphi}$ bijective?",,"['functional-analysis', 'c-star-algebras']"
87,$L^p$ and $\mathscr{l}^p$ embeddings and norm inequalities,and  embeddings and norm inequalities,L^p \mathscr{l}^p,"I'm trying to summarize what I know about $L^p$ spaces at the moment and have a few questions. I think that for $1\le p\le q\le \infty$ one has $$\mathscr{l}^p(\mathbb{N})\subseteq\mathscr{l}^q(\mathbb{N}) \text{ and for all }x\in\mathscr{l}^p(\mathbb{N}): \| x\|_q\le\| x \|_p$$ while for a finite measure space $(X,\mu)$ $$L^q\subseteq L^p \text{ and for all }f\in L^q:\| f\|_p\le\mu(X)^{\frac{1}{p}-\frac{1}{q}}\| f\|_q$$ My questions:  1. Are the above statements correct? Are the two statements about sequence spaces also true if I consider $\mathbb{Z}$ instead of $\mathbb{N}$? Are the space-inclusions 'proper' for $p<q$? For the non-sequential $L^p$ spaces: What if my measure isn't finite. Is there still something I can say about embeddings and norm-inequalities? I tried Wikipedia but couldn't make much sense of what I read. Is there anything else important I should know about $L^p$ spaces? I'm talking very basic stuff that may be useful for an undergraduate writing a functional analysis exam. Save inequalities, I know most of these already. I know that's quite a lot but I tried to find answers to these questions for quite a while without much success.","I'm trying to summarize what I know about $L^p$ spaces at the moment and have a few questions. I think that for $1\le p\le q\le \infty$ one has $$\mathscr{l}^p(\mathbb{N})\subseteq\mathscr{l}^q(\mathbb{N}) \text{ and for all }x\in\mathscr{l}^p(\mathbb{N}): \| x\|_q\le\| x \|_p$$ while for a finite measure space $(X,\mu)$ $$L^q\subseteq L^p \text{ and for all }f\in L^q:\| f\|_p\le\mu(X)^{\frac{1}{p}-\frac{1}{q}}\| f\|_q$$ My questions:  1. Are the above statements correct? Are the two statements about sequence spaces also true if I consider $\mathbb{Z}$ instead of $\mathbb{N}$? Are the space-inclusions 'proper' for $p<q$? For the non-sequential $L^p$ spaces: What if my measure isn't finite. Is there still something I can say about embeddings and norm-inequalities? I tried Wikipedia but couldn't make much sense of what I read. Is there anything else important I should know about $L^p$ spaces? I'm talking very basic stuff that may be useful for an undergraduate writing a functional analysis exam. Save inequalities, I know most of these already. I know that's quite a lot but I tried to find answers to these questions for quite a while without much success.",,"['functional-analysis', 'measure-theory', 'lp-spaces']"
88,Prove that matrix multiplication and inversion is differentiable.,Prove that matrix multiplication and inversion is differentiable.,,"Let $f:GL(n,\mathbb{R})\rightarrow GL(n,\mathbb{R})$ be defined by $f(x)=x^{-1}$ and let $g:GL(n,\mathbb{R})\times GL(n,\mathbb{R}) \rightarrow GL(n,\mathbb{R})$ be defined by $g(x,y)=xy$ where we take the relative topologies from $\mathbb{R}^{n^2}$ and $\mathbb{R}^{n^2}\times \mathbb{R}^{n^2}$ respectively.  Prove that $f$ and $g$ are differentiable when considered as functions on open subsets of $\mathbb{R}^{n^2}$ and $\mathbb{R}^{n^2}\times \mathbb{R}^{n^2}$ respectively mapping into $\mathbb{R}^{n^2}$. So far I've managed to prove that $GL(n,\mathbb{R})$ is open in $\mathbb{R}^{n^2}$ and disconnected. I've also shown that $f$ and $g$ are continuous, but I'm not sure how to show that differentiable follows. Any help would be greatly appreciated.","Let $f:GL(n,\mathbb{R})\rightarrow GL(n,\mathbb{R})$ be defined by $f(x)=x^{-1}$ and let $g:GL(n,\mathbb{R})\times GL(n,\mathbb{R}) \rightarrow GL(n,\mathbb{R})$ be defined by $g(x,y)=xy$ where we take the relative topologies from $\mathbb{R}^{n^2}$ and $\mathbb{R}^{n^2}\times \mathbb{R}^{n^2}$ respectively.  Prove that $f$ and $g$ are differentiable when considered as functions on open subsets of $\mathbb{R}^{n^2}$ and $\mathbb{R}^{n^2}\times \mathbb{R}^{n^2}$ respectively mapping into $\mathbb{R}^{n^2}$. So far I've managed to prove that $GL(n,\mathbb{R})$ is open in $\mathbb{R}^{n^2}$ and disconnected. I've also shown that $f$ and $g$ are continuous, but I'm not sure how to show that differentiable follows. Any help would be greatly appreciated.",,"['real-analysis', 'general-topology', 'functional-analysis', 'continuity', 'differential-topology']"
89,"Does a sequence bounded by a function of $L^{\infty}$, converges in $L^{\infty}$?","Does a sequence bounded by a function of , converges in ?",L^{\infty} L^{\infty},"I want to know if this statement is right? Let $(g_n)$ be a measurable sequence from $[0,T]$ to $\mathbb{R}^n$ such that $\exists \beta \in L^{\infty}_{\mathbb{R}_+}([0,T])$ where $|g_n(t)| \leq \beta(t) \forall t \in [0,T]$ then $(g_n) \in  L^{\infty}_{\mathbb{R}^n}([0,T])$ and it admits a subsequence converges weakly* to some $g \in L^{\infty}_{\mathbb{R}^n}([0,T])$ satisfying $|g(t)|  \leq \beta(t)  \; \forall t \in [0,T]$ I try to prove it this way : since the functions $\{g_n, \forall n\}$ are measurable also bounded by $\beta$ then $\exists c >0$ such that $|g_n(t)| \leq \beta(t) \leq c$ then $g_n \in L^{\infty}_{\mathbb{R}^n}([0,T])$ 2) $|g_n(t)| \leq \beta(t) \Rightarrow  | \frac{g_n(t)}{\beta(t)} | \leq 1 \Rightarrow (\frac{g_n}{\beta})$ in the unit ball of $L^{\infty}$ which is weakly* compact, then we can extract a subsequence $(\frac{g_k}{\beta})$ converges to $(\frac{g}{\beta})$ in that ball where $|g(t) | \leq \beta(t) $ Please tell me if something is wrong and thank you very much.","I want to know if this statement is right? Let be a measurable sequence from to such that where then and it admits a subsequence converges weakly* to some satisfying I try to prove it this way : since the functions are measurable also bounded by then such that then 2) in the unit ball of which is weakly* compact, then we can extract a subsequence converges to in that ball where Please tell me if something is wrong and thank you very much.","(g_n) [0,T] \mathbb{R}^n \exists \beta \in L^{\infty}_{\mathbb{R}_+}([0,T]) |g_n(t)| \leq \beta(t) \forall t \in [0,T] (g_n) \in
 L^{\infty}_{\mathbb{R}^n}([0,T]) g \in L^{\infty}_{\mathbb{R}^n}([0,T]) |g(t)|
 \leq \beta(t)  \; \forall t \in [0,T] \{g_n, \forall n\} \beta \exists c >0 |g_n(t)| \leq \beta(t) \leq c g_n \in L^{\infty}_{\mathbb{R}^n}([0,T]) |g_n(t)| \leq \beta(t) \Rightarrow  | \frac{g_n(t)}{\beta(t)} | \leq 1 \Rightarrow (\frac{g_n}{\beta}) L^{\infty} (\frac{g_k}{\beta}) (\frac{g}{\beta}) |g(t) | \leq \beta(t) ",['functional-analysis']
90,A simple $C_{0}$-semigroup question.,A simple -semigroup question.,C_{0},"Let $u:[0,t_{e}]\to\mathcal{D}(A)$ satisfy $$\begin{cases} \frac{du}{dt}=Au &  0\le t \le t_{e} \\ u(0)=x \end{cases}$$ I want to prove that necessarily $u(t)=T(t)x$. So it's clear to see that $T(0)x=x=u(0)$. And $\frac{d}{dt}T(t)x=AT(t)x=Au$, if $u(t)=T(t)x$. How can I complete this proof? I assume, at least, by making the supposition that $u(t)\ne T(t)x$ and then looking for a proof by contradiction.","Let $u:[0,t_{e}]\to\mathcal{D}(A)$ satisfy $$\begin{cases} \frac{du}{dt}=Au &  0\le t \le t_{e} \\ u(0)=x \end{cases}$$ I want to prove that necessarily $u(t)=T(t)x$. So it's clear to see that $T(0)x=x=u(0)$. And $\frac{d}{dt}T(t)x=AT(t)x=Au$, if $u(t)=T(t)x$. How can I complete this proof? I assume, at least, by making the supposition that $u(t)\ne T(t)x$ and then looking for a proof by contradiction.",,"['functional-analysis', 'operator-theory', 'semigroup-of-operators']"
91,About closedness and boundedness of $H=\left\{(x_n)\in \ell^2(\mathbb{N})\mid\sum \frac{x_n}{n}=1\right\}$,About closedness and boundedness of,H=\left\{(x_n)\in \ell^2(\mathbb{N})\mid\sum \frac{x_n}{n}=1\right\},"Let $H=\left\{(x_n)\in \ell^2(\mathbb{N})\mid\sum \frac{x_n}{n}=1\right\}$. To check which one is true: (a) $H$ is bounded (b) $H$ is closed (c) $H$ is a subspace (d) $H$  has interior points My try: (c) is not true as $x_n\in H$ does not imply that $cx_n\in H$ for $c\in \mathbb R$. (d) is also not true as $x_n\in H$ does not imply $x_n+t\in H$ for $t$ however small it is. I am not sure about (a),(b). How to proceed?","Let $H=\left\{(x_n)\in \ell^2(\mathbb{N})\mid\sum \frac{x_n}{n}=1\right\}$. To check which one is true: (a) $H$ is bounded (b) $H$ is closed (c) $H$ is a subspace (d) $H$  has interior points My try: (c) is not true as $x_n\in H$ does not imply that $cx_n\in H$ for $c\in \mathbb R$. (d) is also not true as $x_n\in H$ does not imply $x_n+t\in H$ for $t$ however small it is. I am not sure about (a),(b). How to proceed?",,['functional-analysis']
92,Simple Inequality for Proving Equivalent Besov Seminorms,Simple Inequality for Proving Equivalent Besov Seminorms,,"For $f\in L^{p}(\mathbb{R}^{n})$, $1\leq p<\infty$, and $h\in\mathbb{R}^{n}$, define the quantity $$I_{p}(h):=\left(\int_{\mathbb{R}^{n}}\left|f(x+h)-f(x)\right|^{p}dx\right)^{1/p}$$ and define the quantity $$\omega_{p}(f,\delta):=\sup_{\left|h\right|\leq\delta}I_{p}(h), \qquad\forall 0<\delta<\infty$$ Now define $$\overline{\omega}_{p}(f,\delta):=\left(\delta^{-n}\int_{\left|h\right|\leq\delta}I_{p}^{p}(h)dh\right)^{1/p}$$ As part a step towards proving two seminorms for Besov spaces are equivalent, I am trying to prove the following inequality: $$v_{n}^{1/p}2^{-n-2}\omega_{p}(f,\delta)\leq\overline{\omega}_{p}(f,\delta)\leq v_{n}^{1/p}w_{p}(f,\delta), \qquad\forall \delta>0 \tag{*}$$ Can anyone give me a suggestion to get me on the right track? Edit: So, Stephen Montgomery-Smith came up with a nice proof of an inequality with the sharper lower constant $v_{n}^{1/p}2^{-1-n/p}$. I am still curious if anyone sees a ""cruder argument"" that gives the lower estimate in the question.","For $f\in L^{p}(\mathbb{R}^{n})$, $1\leq p<\infty$, and $h\in\mathbb{R}^{n}$, define the quantity $$I_{p}(h):=\left(\int_{\mathbb{R}^{n}}\left|f(x+h)-f(x)\right|^{p}dx\right)^{1/p}$$ and define the quantity $$\omega_{p}(f,\delta):=\sup_{\left|h\right|\leq\delta}I_{p}(h), \qquad\forall 0<\delta<\infty$$ Now define $$\overline{\omega}_{p}(f,\delta):=\left(\delta^{-n}\int_{\left|h\right|\leq\delta}I_{p}^{p}(h)dh\right)^{1/p}$$ As part a step towards proving two seminorms for Besov spaces are equivalent, I am trying to prove the following inequality: $$v_{n}^{1/p}2^{-n-2}\omega_{p}(f,\delta)\leq\overline{\omega}_{p}(f,\delta)\leq v_{n}^{1/p}w_{p}(f,\delta), \qquad\forall \delta>0 \tag{*}$$ Can anyone give me a suggestion to get me on the right track? Edit: So, Stephen Montgomery-Smith came up with a nice proof of an inequality with the sharper lower constant $v_{n}^{1/p}2^{-1-n/p}$. I am still curious if anyone sees a ""cruder argument"" that gives the lower estimate in the question.",,"['functional-analysis', 'inequality', 'sobolev-spaces', 'besov-space']"
93,"Existence of a solution to $f(x) = \int_0^1 k(x,y) f(y) dy$",Existence of a solution to,"f(x) = \int_0^1 k(x,y) f(y) dy","Let $X = (0,1)\times (0,1)$ with the Lebesgue measure, and $k\colon X \to \mathbb{R}$ be a measurable non-negative function such that $$ \int_0^1 k(x,y) dy = 1$$ for every $x \in (0,1)$. My question is: What (more) should we assume on $k$ to guarantee that there exists a unique (up to equivalence class) function $f \in L^1(0,1)$ such that   $$f(x) = \int_0^1 k(y,x) f(y) dy$$   for every $x \in (0,1)$ and $\| f \|_{L^1(0,1)} = 1$. Additionally, if we can prove that such unique $f$ exists, when it is strictly positive a.e.? I would be grateful for any reference. Edit: I realized that it makes no sense to ask for a unique solution, since if there is one solution, then all its scalar multiple are also solutions. Hence, I added the requirement that $f \in L^1(0,1)$ and $\| f \|_{L^1(0,1)} = 1$.","Let $X = (0,1)\times (0,1)$ with the Lebesgue measure, and $k\colon X \to \mathbb{R}$ be a measurable non-negative function such that $$ \int_0^1 k(x,y) dy = 1$$ for every $x \in (0,1)$. My question is: What (more) should we assume on $k$ to guarantee that there exists a unique (up to equivalence class) function $f \in L^1(0,1)$ such that   $$f(x) = \int_0^1 k(y,x) f(y) dy$$   for every $x \in (0,1)$ and $\| f \|_{L^1(0,1)} = 1$. Additionally, if we can prove that such unique $f$ exists, when it is strictly positive a.e.? I would be grateful for any reference. Edit: I realized that it makes no sense to ask for a unique solution, since if there is one solution, then all its scalar multiple are also solutions. Hence, I added the requirement that $f \in L^1(0,1)$ and $\| f \|_{L^1(0,1)} = 1$.",,"['functional-analysis', 'functional-equations']"
94,"If the gradient of $f$ at $x$ has the same direction with $x$ for all $x$, is $f$ radial?","If the gradient of  at  has the same direction with  for all , is  radial?",f x x x f,"I would like to ask the following question: If $f:% %TCIMACRO{\U{211d} }% %BeginExpansion \mathbb{R} %EndExpansion ^{n}\rightarrow% %TCIMACRO{\U{211d} }% %BeginExpansion \mathbb{R} %EndExpansion $ is a smooth function such that for all $x\in% %TCIMACRO{\U{211d} }% %BeginExpansion \mathbb{R} %EndExpansion ^{n}$, its gradient at $x$ has the same direction with $x$, is the function $f$ radial? That is if there exists a function $g:% %TCIMACRO{\U{211d} }% %BeginExpansion \mathbb{R} %EndExpansion ^{n}\rightarrow% %TCIMACRO{\U{211d} }% %BeginExpansion \mathbb{R} %EndExpansion $ such that $\nabla f\left(  x\right)  =g\left(  x\right)  x$, then is $f\left(  x\right)  =f\left(  y\right)  $ when $\left\vert x\right\vert =\left\vert y\right\vert ?$ Of course the radial functions have the above property. So I guess that the answer is yes. But so far I could not find proofs/ counterexamples for it.","I would like to ask the following question: If $f:% %TCIMACRO{\U{211d} }% %BeginExpansion \mathbb{R} %EndExpansion ^{n}\rightarrow% %TCIMACRO{\U{211d} }% %BeginExpansion \mathbb{R} %EndExpansion $ is a smooth function such that for all $x\in% %TCIMACRO{\U{211d} }% %BeginExpansion \mathbb{R} %EndExpansion ^{n}$, its gradient at $x$ has the same direction with $x$, is the function $f$ radial? That is if there exists a function $g:% %TCIMACRO{\U{211d} }% %BeginExpansion \mathbb{R} %EndExpansion ^{n}\rightarrow% %TCIMACRO{\U{211d} }% %BeginExpansion \mathbb{R} %EndExpansion $ such that $\nabla f\left(  x\right)  =g\left(  x\right)  x$, then is $f\left(  x\right)  =f\left(  y\right)  $ when $\left\vert x\right\vert =\left\vert y\right\vert ?$ Of course the radial functions have the above property. So I guess that the answer is yes. But so far I could not find proofs/ counterexamples for it.",,"['real-analysis', 'functional-analysis', 'partial-differential-equations']"
95,Example of a nowhere dense subset of a metric space.,Example of a nowhere dense subset of a metric space.,,"We recall the definition of a nowhere dense subset of a metric space: ""A subset $A$ of a metric space $(X,d)$ is nowhere dense if $Int(\bar A)=\emptyset$"" I don't understand how it is that $\mathbb Z\subset \mathbb R$ is nowhere dense; how can the interior of its closure be the empty set?","We recall the definition of a nowhere dense subset of a metric space: ""A subset $A$ of a metric space $(X,d)$ is nowhere dense if $Int(\bar A)=\emptyset$"" I don't understand how it is that $\mathbb Z\subset \mathbb R$ is nowhere dense; how can the interior of its closure be the empty set?",,"['general-topology', 'analysis', 'functional-analysis', 'metric-spaces']"
96,Continuity of multiplication of operators in the strong operator topology - find an error,Continuity of multiplication of operators in the strong operator topology - find an error,,"I need help in finding the mistake in the following reasoning. I proved that if dimension of Banach space $X$ is infinite, then multiplication of bounded operators is separately continuous but not continuous in strong operator topology. This fact can be found in many sources, e.g. Halmos or Simon & Reed so I know it is true. It is also claimed (e.g. in Halmos) that multiplication is sequentially continuous. Problem is that I think proof I came up with also works for nets, which would contradict statement above. I present it below. I need help with finding error. Suppose $A_i \to A$ and $B_i \to B$ strongly. Then both nets are bounded by uniform boundedness principle. In particular $\| A_i \| \leq M$. Furthermore we have: $AB-A_iB_i=(A-A_i)B+A_i(B-B_i)$ Therefore: $\|(AB-A_iB_i)x\| \leq\|(A-A_i)Bx\|+\|A_i\|\|(B-B_i)x\|$ Which converges to zero, so $A_i B_i \to AB$.","I need help in finding the mistake in the following reasoning. I proved that if dimension of Banach space $X$ is infinite, then multiplication of bounded operators is separately continuous but not continuous in strong operator topology. This fact can be found in many sources, e.g. Halmos or Simon & Reed so I know it is true. It is also claimed (e.g. in Halmos) that multiplication is sequentially continuous. Problem is that I think proof I came up with also works for nets, which would contradict statement above. I present it below. I need help with finding error. Suppose $A_i \to A$ and $B_i \to B$ strongly. Then both nets are bounded by uniform boundedness principle. In particular $\| A_i \| \leq M$. Furthermore we have: $AB-A_iB_i=(A-A_i)B+A_i(B-B_i)$ Therefore: $\|(AB-A_iB_i)x\| \leq\|(A-A_i)Bx\|+\|A_i\|\|(B-B_i)x\|$ Which converges to zero, so $A_i B_i \to AB$.",,"['functional-analysis', 'operator-theory', 'banach-spaces']"
97,"Is $W_{0}^{1,p}\left(\Omega\right)$ compactly embedded in $L^{\infty}\left(\Omega\right)$?",Is  compactly embedded in ?,"W_{0}^{1,p}\left(\Omega\right) L^{\infty}\left(\Omega\right)","Let $\Omega$ be an open bounded smooth domain in $\mathbb{R}^{N}$. Let $p>N$, is $W_{0}^{1,p}\left(\Omega\right)$ compactly embedded in $L^{\infty}\left(\Omega\right)$? In some textbook such as Sobolev Spaces (Adams-1975), it was only said that if $p>N$ , $W_{0}^{1,p}\left(\Omega\right)$ compactly embedded in $C\left(\overline{\Omega}\right)$","Let $\Omega$ be an open bounded smooth domain in $\mathbb{R}^{N}$. Let $p>N$, is $W_{0}^{1,p}\left(\Omega\right)$ compactly embedded in $L^{\infty}\left(\Omega\right)$? In some textbook such as Sobolev Spaces (Adams-1975), it was only said that if $p>N$ , $W_{0}^{1,p}\left(\Omega\right)$ compactly embedded in $C\left(\overline{\Omega}\right)$",,"['real-analysis', 'functional-analysis', 'sobolev-spaces']"
98,Why is functional analysis so obsessed with sequences?,Why is functional analysis so obsessed with sequences?,,"Beginning functional analysis I thought I would learn about generalized properties of functions and operators but yet I am flipping through pages after pages of texts on cauchy sequence, convergence, etc. What is the purpose of learning about sequences in functional analysis? Seems rather pointless to me as I have already learned about sequences in elementary calculus","Beginning functional analysis I thought I would learn about generalized properties of functions and operators but yet I am flipping through pages after pages of texts on cauchy sequence, convergence, etc. What is the purpose of learning about sequences in functional analysis? Seems rather pointless to me as I have already learned about sequences in elementary calculus",,"['functional-analysis', 'soft-question']"
99,How is completeness used in the Eberlein–Šmulian theorem?,How is completeness used in the Eberlein–Šmulian theorem?,,"So for weakly closed subsets of Banach spaces compactness and sequential compactness coincide, but upon studying the proof I can't put my finger on what exactly would go wrong if we dropped the completeness assumption. I figure the result doesn't hold true for general normed spaces as several sources explicitly require a Banach space. What part of the proof does necessarily require completeness? What would be a counterexample if it is dropped?","So for weakly closed subsets of Banach spaces compactness and sequential compactness coincide, but upon studying the proof I can't put my finger on what exactly would go wrong if we dropped the completeness assumption. I figure the result doesn't hold true for general normed spaces as several sources explicitly require a Banach space. What part of the proof does necessarily require completeness? What would be a counterexample if it is dropped?",,"['functional-analysis', 'banach-spaces']"
