,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Heronian triangle Generator,Heronian triangle Generator,,"I'm trouble shooting my code I wrote to generate all Heronian Triangles (triangle with integer sides and integer area).  I'm using the following algorithm $$a=n(m^{2}+k^{2})$$ $$b=m(n^{2}+k^{2})$$ $$c=(m+n)(mn-k^{2})$$ $$\text{Semiperimeter}=mn(m+n)$$ $$\text{Area}=mnk(m+n)(mn-k^{2})$$ for integers $m$, $n$ and $k$ subject to the contraints: $$\gcd{(m,n,k)}=1$$ $$mn > k^2 \ge m^2n/(2m+n)$$ $$m \ge n \ge 1$$ which I found here http://en.wikipedia.org/wiki/Integer_triangle#Heronian_triangles The odd part is that this algorithm doesn't seem to ever generate 7, 24, 25 which is a Hero Triangle (a right triangle, in fact) with integer area 84. I had originally assumed it was a breakdown in my code, but then I realized I can't seem to find any values of $m$, $n$, or $k$ within the constraints or even ignoring the constraints that generate this triangle. I don't know which is which between which of the 7, 24, or 25 equal the $a$, $b$, or $c$, but I've tried manually solving for $m$ and $n$ using wolframalpha.  Since $a$ and $b$ are symmetric (since $m$ and $n$ are symmetric when ignoring constraints), I really only have 3 sets of linear equations to check: $$7=n(m^{2}+k^{2})$$ $$24=m(n^{2}+k^{2})$$ $$25=(m+n)(mn-k^{2})$$ Wolframalpha - Linear equations 1 $$24=n(m^{2}+k^{2})$$ $$25=m(n^{2}+k^{2})$$ $$7=(m+n)(mn-k^{2})$$ Wolframalpha - Linear equations 2 $$25=n(m^{2}+k^{2})$$ $$7=m(n^{2}+k^{2})$$ $$24=(m+n)(mn-k^{2})$$ Wolframalpha - Linear equations 3 None of which have integer solutions. Is my understanding of Hero Triangles wrong? Is the algorithm wrong? Is my implementation wrong?","I'm trouble shooting my code I wrote to generate all Heronian Triangles (triangle with integer sides and integer area).  I'm using the following algorithm $$a=n(m^{2}+k^{2})$$ $$b=m(n^{2}+k^{2})$$ $$c=(m+n)(mn-k^{2})$$ $$\text{Semiperimeter}=mn(m+n)$$ $$\text{Area}=mnk(m+n)(mn-k^{2})$$ for integers $m$, $n$ and $k$ subject to the contraints: $$\gcd{(m,n,k)}=1$$ $$mn > k^2 \ge m^2n/(2m+n)$$ $$m \ge n \ge 1$$ which I found here http://en.wikipedia.org/wiki/Integer_triangle#Heronian_triangles The odd part is that this algorithm doesn't seem to ever generate 7, 24, 25 which is a Hero Triangle (a right triangle, in fact) with integer area 84. I had originally assumed it was a breakdown in my code, but then I realized I can't seem to find any values of $m$, $n$, or $k$ within the constraints or even ignoring the constraints that generate this triangle. I don't know which is which between which of the 7, 24, or 25 equal the $a$, $b$, or $c$, but I've tried manually solving for $m$ and $n$ using wolframalpha.  Since $a$ and $b$ are symmetric (since $m$ and $n$ are symmetric when ignoring constraints), I really only have 3 sets of linear equations to check: $$7=n(m^{2}+k^{2})$$ $$24=m(n^{2}+k^{2})$$ $$25=(m+n)(mn-k^{2})$$ Wolframalpha - Linear equations 1 $$24=n(m^{2}+k^{2})$$ $$25=m(n^{2}+k^{2})$$ $$7=(m+n)(mn-k^{2})$$ Wolframalpha - Linear equations 2 $$25=n(m^{2}+k^{2})$$ $$7=m(n^{2}+k^{2})$$ $$24=(m+n)(mn-k^{2})$$ Wolframalpha - Linear equations 3 None of which have integer solutions. Is my understanding of Hero Triangles wrong? Is the algorithm wrong? Is my implementation wrong?",,"['linear-algebra', 'geometry', 'algorithms', 'diophantine-equations', 'triangles']"
1,number of elements on a finite vector space,number of elements on a finite vector space,,I know that there is no vector space over any field $\mathbb{F}$ having precisely $6$ elements. I would like to know if there is a theorem which characterizes those $n's \in \mathbb{N}$ such that there is a vector space having exactly $n$ elements over a field $\mathbb{F}$. For example: $n=p^{k}$ where $p$ is a prime number and $k\in \mathbb{N}$ ($k\neq 0$).,I know that there is no vector space over any field $\mathbb{F}$ having precisely $6$ elements. I would like to know if there is a theorem which characterizes those $n's \in \mathbb{N}$ such that there is a vector space having exactly $n$ elements over a field $\mathbb{F}$. For example: $n=p^{k}$ where $p$ is a prime number and $k\in \mathbb{N}$ ($k\neq 0$).,,[]
2,Minimize $ \|A-BXC \|_F$ subject to $\mbox{rank} (X) \leq r$,Minimize  subject to, \|A-BXC \|_F \mbox{rank} (X) \leq r,"I have the following problem. Let $A$, $B$ $C$ be real-valued matrices of size $m  \times q$, $m \times n$, $p \times q$ respectively. Find matrix $X$ of size $n \times p$ and maximum rank $r$ such that the Frobenius norm $\|A - B X C \|$ is minimized. This is a generalization of the best approximation property of the truncated Singular Value Decomposition; however it doesn't appear to be trivial. Any insights? Known literature?","I have the following problem. Let $A$, $B$ $C$ be real-valued matrices of size $m  \times q$, $m \times n$, $p \times q$ respectively. Find matrix $X$ of size $n \times p$ and maximum rank $r$ such that the Frobenius norm $\|A - B X C \|$ is minimized. This is a generalization of the best approximation property of the truncated Singular Value Decomposition; however it doesn't appear to be trivial. Any insights? Known literature?",,"['linear-algebra', 'optimization', 'approximation', 'matrix-rank', 'least-squares']"
3,A projection satisfying $\| Px \| \leq \|x\|$ for all $x$ is an orthogonal projection [duplicate],A projection satisfying  for all  is an orthogonal projection [duplicate],\| Px \| \leq \|x\| x,"This question already has answers here : Orthogonal Projection (3 answers) Closed 6 years ago . How to prove that if $V$ is a finite dimensional inner product space and $W$ a subspace of $V$, if $P$ is projection map ($P^2=P$) having $W$ as its range and is such that $\|Px\| \leq \|x\|$ for all $x \in V$, then $P$ is orthogonal projection of $V$ onto $W$.","This question already has answers here : Orthogonal Projection (3 answers) Closed 6 years ago . How to prove that if $V$ is a finite dimensional inner product space and $W$ a subspace of $V$, if $P$ is projection map ($P^2=P$) having $W$ as its range and is such that $\|Px\| \leq \|x\|$ for all $x \in V$, then $P$ is orthogonal projection of $V$ onto $W$.",,['linear-algebra']
4,"Prove that a sum of projections is a projection iff they are orthogonal, if the characteristic of the space is not $2$","Prove that a sum of projections is a projection iff they are orthogonal, if the characteristic of the space is not",2,"Let $E_1$ and $E_2$ be projections on $V$, a vector space over $F$. Why is if $\operatorname{char}F\neq2$ then $E_1+E_2$ is a projection iff $E_1E_2=E_2E_1=0$ ?","Let $E_1$ and $E_2$ be projections on $V$, a vector space over $F$. Why is if $\operatorname{char}F\neq2$ then $E_1+E_2$ is a projection iff $E_1E_2=E_2E_1=0$ ?",,"['linear-algebra', 'vector-spaces', 'transformation']"
5,An eigenvalue problem for positive semidefinite matrix,An eigenvalue problem for positive semidefinite matrix,,"Let  $  \begin{bmatrix} A& B   \\ B^*  &C \end{bmatrix}$ be positive semidefinite, $A,C$ are of size $n\times n$. Is it true that $$\quad \sum\limits_{i=1}^n\lambda_i\begin{bmatrix} A& B   \\ B^*  &C \end{bmatrix}\le \sum\limits_{i=1}^n\left(\lambda_i(A)+\lambda_i(C)\right)\quad? $$ Here, $\lambda_i(\cdot)$ means the $i$th largest eigenvalue of $\cdot\quad$","Let  $  \begin{bmatrix} A& B   \\ B^*  &C \end{bmatrix}$ be positive semidefinite, $A,C$ are of size $n\times n$. Is it true that $$\quad \sum\limits_{i=1}^n\lambda_i\begin{bmatrix} A& B   \\ B^*  &C \end{bmatrix}\le \sum\limits_{i=1}^n\left(\lambda_i(A)+\lambda_i(C)\right)\quad? $$ Here, $\lambda_i(\cdot)$ means the $i$th largest eigenvalue of $\cdot\quad$",,"['linear-algebra', 'matrices']"
6,"Why is the identity representation of $SL(2,k)$ isomorphic to its dual representation?",Why is the identity representation of  isomorphic to its dual representation?,"SL(2,k)","By identity representation I mean the representation sending each element of $SL(2,k)$ to itself. Is there a simple way to see this isomorphism? I feel like I am missing something incredibly basic here.","By identity representation I mean the representation sending each element of $SL(2,k)$ to itself. Is there a simple way to see this isomorphism? I feel like I am missing something incredibly basic here.",,"['linear-algebra', 'representation-theory']"
7,Deriving volume of parallelepiped as a function of edge lengths and angles between the edges,Deriving volume of parallelepiped as a function of edge lengths and angles between the edges,,"In Wikipedia it is stated that the volume of the parallelepiped given its edge  lengths $a,b,c$, and the internal angles between the edges $\alpha ,\beta ,\gamma $ is: \begin{equation*} V=abc\sqrt{1+2\cos \alpha \cos \beta \cos \gamma -\cos ^{2}\alpha - \cos^{2}\beta - \cos ^{2}\gamma }\qquad(*). \end{equation*} I was not able to derive it by using the determinant formula and expressing $\cos \alpha ,\cos \beta ,\cos \gamma $ in terms of $a,b,c$ and $\alpha ,\beta ,\gamma $. For instance \begin{equation*} a_{1}b_{1}+a_{2}b_{2}+a_{3}b_{3}=ab\cos \alpha . \end{equation*} Question: Could you give a hint on how can the formula (*) be proved?","In Wikipedia it is stated that the volume of the parallelepiped given its edge  lengths $a,b,c$, and the internal angles between the edges $\alpha ,\beta ,\gamma $ is: \begin{equation*} V=abc\sqrt{1+2\cos \alpha \cos \beta \cos \gamma -\cos ^{2}\alpha - \cos^{2}\beta - \cos ^{2}\gamma }\qquad(*). \end{equation*} I was not able to derive it by using the determinant formula and expressing $\cos \alpha ,\cos \beta ,\cos \gamma $ in terms of $a,b,c$ and $\alpha ,\beta ,\gamma $. For instance \begin{equation*} a_{1}b_{1}+a_{2}b_{2}+a_{3}b_{3}=ab\cos \alpha . \end{equation*} Question: Could you give a hint on how can the formula (*) be proved?",,"['geometry', 'linear-algebra', 'trigonometry', 'euclidean-geometry']"
8,Prove that $\det(A^{-1}-A)+\det(A^{-1}+A)\geq 6$,Prove that,\det(A^{-1}-A)+\det(A^{-1}+A)\geq 6,"Let $A$ be a $3 \times 3$ matrix with real numbers, with $\det(A)=1$ and $\operatorname{Tr}(A)=-1$ . Prove that $$\det(A^{-1}-A)+\det(A^{-1}+A)\geq 6.$$ This is supposed to be a 11th grade problem. Attempts: I used the formula $\det(A+xB)=\det(A)+ax+bx^{2}+\det(A)x^{3}$ and then I tried expanding the determinant $\det(A^{-1}-A)=\det(A^{-1})-a+b-\det(A)$ , with $\det(A)=\frac{1}{\det A}$ , and the same with $\det(A^{-1}+A)=\det(A^{-1})+c+d+\det(A)$ but then I got stuck. I also used the equation of the matrix $A_{3 \times 3}$ : $A^{3}-\operatorname{Tr}(A)A^{2}+\operatorname{Tr}(A^{*})A-\det(A)I_3=O_3$ , I also know that $\operatorname{Tr}(A^{*})=\frac{\operatorname{Tr}(A)^{2}-\operatorname{Tr}(A^{2})}{2}$ but I couldn't find $\operatorname{Tr}(A^2)$ . I am missing something or is there another way of solving maybe involving matrix ranks and Sylvester’s Theorem? I could't find a way to apply it. Any help would be appreciated.","Let be a matrix with real numbers, with and . Prove that This is supposed to be a 11th grade problem. Attempts: I used the formula and then I tried expanding the determinant , with , and the same with but then I got stuck. I also used the equation of the matrix : , I also know that but I couldn't find . I am missing something or is there another way of solving maybe involving matrix ranks and Sylvester’s Theorem? I could't find a way to apply it. Any help would be appreciated.",A 3 \times 3 \det(A)=1 \operatorname{Tr}(A)=-1 \det(A^{-1}-A)+\det(A^{-1}+A)\geq 6. \det(A+xB)=\det(A)+ax+bx^{2}+\det(A)x^{3} \det(A^{-1}-A)=\det(A^{-1})-a+b-\det(A) \det(A)=\frac{1}{\det A} \det(A^{-1}+A)=\det(A^{-1})+c+d+\det(A) A_{3 \times 3} A^{3}-\operatorname{Tr}(A)A^{2}+\operatorname{Tr}(A^{*})A-\det(A)I_3=O_3 \operatorname{Tr}(A^{*})=\frac{\operatorname{Tr}(A)^{2}-\operatorname{Tr}(A^{2})}{2} \operatorname{Tr}(A^2),"['linear-algebra', 'matrices', 'inequality', 'contest-math', 'determinant']"
9,Create a unitary matrix out of a column vector,Create a unitary matrix out of a column vector,,"I need an algorithmic way, to create a unitary matrix out of a first, given, column vector. i.e. if the column vector was $(0 1 1 1)^T$ , I'd need an algorithm to complete it to a matrix like this. $$ \frac{1}{\sqrt{3}}  \quad \begin{pmatrix} 0 & 1 & 1 & 1 \\ 1 & 0 & 1 & -1 \\ 1 & -1 & 0 & 1 \\ 1 & 1 & -1 & 0 \end{pmatrix}$$ The content of the other column vectors is not relevant, as long as it makes the matrix unitary. The only partial answer I found was in this previous post, but it only covers vectors containing only $1$ . Does anyone know a smart way to accomplish this? EDIT Based on the answer by @ben-grossmann, I've written this Python function to create the matrix, if anyone else needs it. def create_unitary(v):     dim = v.size     # Return identity if v is a multiple of e1     if v[0][0] and not np.any(v[0][1:]):         return np.identity(dim)     e1 = np.zeros(dim)     e1[0] = 1     w = v/np.linalg.norm(v) - e1     return np.identity(dim) - 2*((np.dot(w.T, w))/(np.dot(w, w.T)))","I need an algorithmic way, to create a unitary matrix out of a first, given, column vector. i.e. if the column vector was , I'd need an algorithm to complete it to a matrix like this. The content of the other column vectors is not relevant, as long as it makes the matrix unitary. The only partial answer I found was in this previous post, but it only covers vectors containing only . Does anyone know a smart way to accomplish this? EDIT Based on the answer by @ben-grossmann, I've written this Python function to create the matrix, if anyone else needs it. def create_unitary(v):     dim = v.size     # Return identity if v is a multiple of e1     if v[0][0] and not np.any(v[0][1:]):         return np.identity(dim)     e1 = np.zeros(dim)     e1[0] = 1     w = v/np.linalg.norm(v) - e1     return np.identity(dim) - 2*((np.dot(w.T, w))/(np.dot(w, w.T)))","(0 1 1 1)^T 
\frac{1}{\sqrt{3}}  \quad
\begin{pmatrix}
0 & 1 & 1 & 1 \\
1 & 0 & 1 & -1 \\
1 & -1 & 0 & 1 \\
1 & 1 & -1 & 0
\end{pmatrix} 1","['linear-algebra', 'matrices', 'orthogonal-matrices']"
10,What does LSQR stand for,What does LSQR stand for,,"One of the most popular and efficient iterative methods to solve large sparse systems of equations in the least squares sense is LSQR. It is related to CGLS (Conjugate Gradient Least Squares) in that it has the same iterates (mathematically, not numerically). But what does its name mean? is it L east S quares with QR factorisation? If so, why is it called that, does it provide Q and R? I suppose not because Q would not be sparse. L east SQ ua R es? L east SQ uares R egression? Something else? I have looked at the three references over at the SciPy documentation , but none of them seem to explain the naming. In the MATLAB documentation , the call it ""the Least Squares Method"", even stating it as ""... the least squares (LSQR) algorithm..."", suggesting that my second option is correct, but with no source.","One of the most popular and efficient iterative methods to solve large sparse systems of equations in the least squares sense is LSQR. It is related to CGLS (Conjugate Gradient Least Squares) in that it has the same iterates (mathematically, not numerically). But what does its name mean? is it L east S quares with QR factorisation? If so, why is it called that, does it provide Q and R? I suppose not because Q would not be sparse. L east SQ ua R es? L east SQ uares R egression? Something else? I have looked at the three references over at the SciPy documentation , but none of them seem to explain the naming. In the MATLAB documentation , the call it ""the Least Squares Method"", even stating it as ""... the least squares (LSQR) algorithm..."", suggesting that my second option is correct, but with no source.",,"['linear-algebra', 'terminology', 'sparse-matrices']"
11,Why does putting the eigenvectors as columns in a matrix give us the diagonalizing matrix?,Why does putting the eigenvectors as columns in a matrix give us the diagonalizing matrix?,,"For $A$ of $n \times n$ if we have $n$ eigenvectors,  we can put them as columns in a matrix and get the diagonalizing matrix - why does it work?","For of if we have eigenvectors,  we can put them as columns in a matrix and get the diagonalizing matrix - why does it work?",A n \times n n,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
12,$\text{det}(\text{abs}({\bf d - d}^T))$ being zero implies two or more elements equal in $\bf d$?,being zero implies two or more elements equal in ?,\text{det}(\text{abs}({\bf d - d}^T)) \bf d,"When implementing a clustering algorithm earlier today I investigated (in Matlab syntax): $$\text{det}(\text{abs}({\bf d - d}^T)), {\bf d }\in \mathbb R^{N}$$ edit: In non-Matlab syntax: $$\text{det}(\text{abs}({\bf d1}^T-{\bf 1d}^T))$$ In other words the determinant of a matrix consisting of absolute value of pairwise differences. Now since determinants are usually terrible to calculate, this is mostly of a curiosity of mine, but it seems that if any two elements of $\bf d$ are the same, then the above determinant is 0, and if no two elements of $\bf d$ are the same, then determinant $\neq 0$ . Can we prove this? For example the vector ${\bf d} =  [1,2,3]^T$ : $$\text{det}(\text{abs}({\bf d - d}^T)) = \begin{bmatrix}|1-1|&|1-2|&|1-3|\\|2-1|&|2-2|&|2-3|\\|3-1|&|3-2|&|3-3|\end{bmatrix} = \left\|\begin{bmatrix}0&1&2\\1&0&1\\2&1&0\end{bmatrix}\right\|\neq 0$$ And the vector : ${\bf d} = [3,2,3]^T$ : $$\text{det}(\text{abs}({\bf d - d}^T)) = \begin{bmatrix}|3-3|&|3-2|&|3-3|\\|2-3|&|2-2|&|3-3|\\|3-3|&|3-2|&|3-3|\end{bmatrix} = \left\|\begin{bmatrix}0&1&0\\1&0&1\\0&1&0\end{bmatrix}\right\|= 0$$","When implementing a clustering algorithm earlier today I investigated (in Matlab syntax): edit: In non-Matlab syntax: In other words the determinant of a matrix consisting of absolute value of pairwise differences. Now since determinants are usually terrible to calculate, this is mostly of a curiosity of mine, but it seems that if any two elements of are the same, then the above determinant is 0, and if no two elements of are the same, then determinant . Can we prove this? For example the vector : And the vector : :","\text{det}(\text{abs}({\bf d - d}^T)), {\bf d }\in \mathbb R^{N} \text{det}(\text{abs}({\bf d1}^T-{\bf 1d}^T)) \bf d \bf d \neq 0 {\bf d} =  [1,2,3]^T \text{det}(\text{abs}({\bf d - d}^T)) = \begin{bmatrix}|1-1|&|1-2|&|1-3|\\|2-1|&|2-2|&|2-3|\\|3-1|&|3-2|&|3-3|\end{bmatrix} = \left\|\begin{bmatrix}0&1&2\\1&0&1\\2&1&0\end{bmatrix}\right\|\neq 0 {\bf d} = [3,2,3]^T \text{det}(\text{abs}({\bf d - d}^T)) = \begin{bmatrix}|3-3|&|3-2|&|3-3|\\|2-3|&|2-2|&|3-3|\\|3-3|&|3-2|&|3-3|\end{bmatrix} = \left\|\begin{bmatrix}0&1&0\\1&0&1\\0&1&0\end{bmatrix}\right\|= 0","['linear-algebra', 'soft-question', 'determinant', 'matrix-rank', 'computational-mathematics']"
13,What is the vector $x ∈ \mathbb{R^3}$ that achieves $max||x||_1$ subject to $||x||_2 = 1$?,What is the vector  that achieves  subject to ?,x ∈ \mathbb{R^3} max||x||_1 ||x||_2 = 1,"I'm trying to answer the questions ""What is the vector $x ∈ \mathbb{R^3}$ that achieves $max||x||_1$ subject to $||x||_2 = 1$ ?"" and ""What is the vector x ∈ $R^3$ that achieves $max||x||_∞$ subject to $||x||_2 = 1$ ? I think the first question is asking me to find a vector with three components that will have the maximum $||x||_1$ norm value where $\sqrt{x_1^2 + x_3^2 + x_2^2} = 1$ , so $x_1^2 + x_3^2 + x_2^2 = 1$ . I know the The L1 norm is just the sum of the absolute values of the vector's components. After trial and error I came up with $x = [\sqrt{\frac{1}{3}}, \sqrt{\frac{1}{3}}, \sqrt{\frac{1}{3}}]$ , but also $[-\sqrt{\frac{1}{3}}, -\sqrt{\frac{1}{3}}, -\sqrt{\frac{1}{3}}]$ , and $[-\sqrt{\frac{1}{3}}, \sqrt{\frac{1}{3}}, \sqrt{\frac{1}{3}}]$ , etc. For my the second question, I think I need to find the vector in $\mathbb{R^3}$ that will give me the maximum value of the absolute value of the vector's components given $x_1^2 + x_3^2 + x_2^2 = 1$ . I came up with $[1, 0, 0]$ , $[0, 1, 0]$ , $[0, 0, 1]$ , $[-1, 0, 0]$ , $[0, -1, 0]$ , and $[0, 0, -1]$ . Am I correct? Is there a more formal way to figure this out and write my solution?","I'm trying to answer the questions ""What is the vector that achieves subject to ?"" and ""What is the vector x ∈ that achieves subject to ? I think the first question is asking me to find a vector with three components that will have the maximum norm value where , so . I know the The L1 norm is just the sum of the absolute values of the vector's components. After trial and error I came up with , but also , and , etc. For my the second question, I think I need to find the vector in that will give me the maximum value of the absolute value of the vector's components given . I came up with , , , , , and . Am I correct? Is there a more formal way to figure this out and write my solution?","x ∈ \mathbb{R^3} max||x||_1 ||x||_2 = 1 R^3 max||x||_∞ ||x||_2 = 1 ||x||_1 \sqrt{x_1^2 + x_3^2 + x_2^2} = 1 x_1^2 + x_3^2 + x_2^2 = 1 x = [\sqrt{\frac{1}{3}}, \sqrt{\frac{1}{3}}, \sqrt{\frac{1}{3}}] [-\sqrt{\frac{1}{3}}, -\sqrt{\frac{1}{3}}, -\sqrt{\frac{1}{3}}] [-\sqrt{\frac{1}{3}}, \sqrt{\frac{1}{3}}, \sqrt{\frac{1}{3}}] \mathbb{R^3} x_1^2 + x_3^2 + x_2^2 = 1 [1, 0, 0] [0, 1, 0] [0, 0, 1] [-1, 0, 0] [0, -1, 0] [0, 0, -1]",['linear-algebra']
14,"Let $A$ be a real $n\times n$ such that the diagonal entries are positive, the off diagonal entries are negative, and the row sums are positive.","Let  be a real  such that the diagonal entries are positive, the off diagonal entries are negative, and the row sums are positive.",A n\times n,"Let $A$ be a $n\times n$ matrix over the reals such that the diagonal entries are all positive, the off-diagonal entries are all negative, and the row sums are all positive. Show that $\det A \neq 0$ . To show that $\det A\neq 0$ , it would be sufficient to show that the system $AX = 0$ does not have a nontrivial solution. Then I suppose that to show that, one could assume that it has a nontrivial solution and then obtain a contradiction. But I'm stuck on actually doing that part. Any suggestions? I'm also interested in where I can find questions similar to this one to practice.","Let be a matrix over the reals such that the diagonal entries are all positive, the off-diagonal entries are all negative, and the row sums are all positive. Show that . To show that , it would be sufficient to show that the system does not have a nontrivial solution. Then I suppose that to show that, one could assume that it has a nontrivial solution and then obtain a contradiction. But I'm stuck on actually doing that part. Any suggestions? I'm also interested in where I can find questions similar to this one to practice.",A n\times n \det A \neq 0 \det A\neq 0 AX = 0,['linear-algebra']
15,Why are |vertical lines| used to mark matrix determinants?,Why are |vertical lines| used to mark matrix determinants?,,This notation is sometimes used to denote the determinant: $$ \begin{vmatrix}a & b \\ c & d\end{vmatrix} = ad-bc$$ Why? Where did this notation come from? Was there any relationship between this notation and the absolute value $|x|$ or the norm $\lVert\mathbf{x}\rVert$ ?,This notation is sometimes used to denote the determinant: Why? Where did this notation come from? Was there any relationship between this notation and the absolute value or the norm ?, \begin{vmatrix}a & b \\ c & d\end{vmatrix} = ad-bc |x| \lVert\mathbf{x}\rVert,"['linear-algebra', 'soft-question', 'notation']"
16,Determinant of an n x n matrix,Determinant of an n x n matrix,,"I do not know what this kind of matrix is called, it does not really look Circulant, but I tried to do many row and columns operation in order to make it into an upper triangular matrix so the determinant would be the product of the diagonal elements but I couldn't find a way. Any thoughts? This is the matrix : $$\begin{bmatrix}n&n-1&n-2&\cdots&2&1\\1&n&n-1&\cdots&3&2\\1&1&n&\cdots&4&3\\\vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\1&1&1&\cdots&n&n-1\\1&1&1&\cdots&1&\lambda\end{bmatrix}$$","I do not know what this kind of matrix is called, it does not really look Circulant, but I tried to do many row and columns operation in order to make it into an upper triangular matrix so the determinant would be the product of the diagonal elements but I couldn't find a way. Any thoughts? This is the matrix :",\begin{bmatrix}n&n-1&n-2&\cdots&2&1\\1&n&n-1&\cdots&3&2\\1&1&n&\cdots&4&3\\\vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\1&1&1&\cdots&n&n-1\\1&1&1&\cdots&1&\lambda\end{bmatrix},"['linear-algebra', 'matrices', 'determinant']"
17,"Vector space with x+y=xy, cx=x^c","Vector space with x+y=xy, cx=x^c",,"If $x+y=xy$ and $cx=x^c$ ($c$ is a real number) where $x,y$ posivitive. I want to prove that this can be a vector space for all $x,y>0$. Isn't there a problem with the axiom that states ""For every $v \in V$, there exists an element $−v \in V$, called the additive inverse of $v$, such that $v + (−v) = 0$"" because by the way that addition is defined $x+y=xy$ for every positive number x theatre element will be zero so that $x+0=x0=0$. Except if the ""zero"" is actually $1$ because $x+1=x$ and the additive invent will be $1/x$.  $x+1/x=1$. Can somebody clear things a bit?","If $x+y=xy$ and $cx=x^c$ ($c$ is a real number) where $x,y$ posivitive. I want to prove that this can be a vector space for all $x,y>0$. Isn't there a problem with the axiom that states ""For every $v \in V$, there exists an element $−v \in V$, called the additive inverse of $v$, such that $v + (−v) = 0$"" because by the way that addition is defined $x+y=xy$ for every positive number x theatre element will be zero so that $x+0=x0=0$. Except if the ""zero"" is actually $1$ because $x+1=x$ and the additive invent will be $1/x$.  $x+1/x=1$. Can somebody clear things a bit?",,"['linear-algebra', 'vector-spaces']"
18,$\det(A_1\cdot B_1 +A_2\cdot B_2)=0$,,\det(A_1\cdot B_1 +A_2\cdot B_2)=0,"Let $A_1, A_2\in M_n(\mathbb{R})$  two symmetric matrices s.t. $det(A_1^2+A_2^2)=0$. Show that $det(A_1\cdot B_1 +A_2\cdot B_2)=0$ for every $B_1, B_2\in M_n(\mathbb{R})$. My idea: I consider the matrix C :\begin{bmatrix}     A_1 & A_2 \\     B_1^t & B_2^t  \end{bmatrix} $det(C\cdot C^t)\geq 0\Rightarrow det(\begin{bmatrix}     A_1^2 +A_2^2 & D \\     D^t & E\\ \end{bmatrix})\geq 0  $ where $D=A_1B_1+A_2B_2$. I tried to expand the determinant with Laplace Rule. I am not sure if $ det(\begin{bmatrix}     A_1^2 +A_2^2 & D \\     D^t & E\\ \end{bmatrix}) = -det(D\cdot D^t)$. In this way I would  get $det(D)=0$.","Let $A_1, A_2\in M_n(\mathbb{R})$  two symmetric matrices s.t. $det(A_1^2+A_2^2)=0$. Show that $det(A_1\cdot B_1 +A_2\cdot B_2)=0$ for every $B_1, B_2\in M_n(\mathbb{R})$. My idea: I consider the matrix C :\begin{bmatrix}     A_1 & A_2 \\     B_1^t & B_2^t  \end{bmatrix} $det(C\cdot C^t)\geq 0\Rightarrow det(\begin{bmatrix}     A_1^2 +A_2^2 & D \\     D^t & E\\ \end{bmatrix})\geq 0  $ where $D=A_1B_1+A_2B_2$. I tried to expand the determinant with Laplace Rule. I am not sure if $ det(\begin{bmatrix}     A_1^2 +A_2^2 & D \\     D^t & E\\ \end{bmatrix}) = -det(D\cdot D^t)$. In this way I would  get $det(D)=0$.",,"['linear-algebra', 'matrices', 'determinant']"
19,How to Find Moore Penrose Inverse,How to Find Moore Penrose Inverse,,"I have a matrix:  $$A=     \begin{bmatrix}     -1 & 0 & 1 & 2 \\     -1 & 1 & 0 & -1 \\     0 & -1 & 1 & 3 \\     0 & 1 & -1 & -3 \\     1 & -1 & 0 & 1 \\     1 & 0 & -1 & -2 \\     \end{bmatrix} $$ I want to find a Moore Penrose generalized inverse for it, but I only know how to do this using computer software.  My attempt: I think the inverse is supposed to be of the form  $$C'(CC')^{-1}(B'B)^{-1}B'$$ where $A$ has dimensions $m\times n$, $B$ has dimensions $m\times k$, $C$ has dimensions $k\times n$, and all three have rank $k$. I just don't understand how to actually find this inverse matrix.","I have a matrix:  $$A=     \begin{bmatrix}     -1 & 0 & 1 & 2 \\     -1 & 1 & 0 & -1 \\     0 & -1 & 1 & 3 \\     0 & 1 & -1 & -3 \\     1 & -1 & 0 & 1 \\     1 & 0 & -1 & -2 \\     \end{bmatrix} $$ I want to find a Moore Penrose generalized inverse for it, but I only know how to do this using computer software.  My attempt: I think the inverse is supposed to be of the form  $$C'(CC')^{-1}(B'B)^{-1}B'$$ where $A$ has dimensions $m\times n$, $B$ has dimensions $m\times k$, $C$ has dimensions $k\times n$, and all three have rank $k$. I just don't understand how to actually find this inverse matrix.",,"['linear-algebra', 'matrices', 'inverse', 'pseudoinverse']"
20,Is $XX^T$ invertible?,Is  invertible?,XX^T,"In one of the lectures today, the professor said that if $X \in \mathbb{R}^{m \times n}$ matrix, and the columns of $X$ span $\mathbb{R}^m$, then the matrix $XX^T$ is invertible. I am not sure why this is the case unless $m=n$?","In one of the lectures today, the professor said that if $X \in \mathbb{R}^{m \times n}$ matrix, and the columns of $X$ span $\mathbb{R}^m$, then the matrix $XX^T$ is invertible. I am not sure why this is the case unless $m=n$?",,"['linear-algebra', 'matrices']"
21,How to compute the gradient of the norm for linear least squares?,How to compute the gradient of the norm for linear least squares?,,I am reading about Solution of the Linear Least Squares Problem. Given the function $$f(\theta) = \frac{1}{2} \lVert y - \Phi \theta \rVert _{2}^{2}$$ To find the minimizer we need to compute the gradient of f. The text says that $$\nabla f(\theta^{*}) = 0 \leftrightarrow \Phi^{T}\Phi\theta^{*} - \Phi^{T}y = 0$$ Can someone help me and explain how to compute the gradient of f? I don't understand why $$\nabla f(\theta^{*}) = \Phi^{T}\Phi\theta^{*} - \Phi^{T}y$$,I am reading about Solution of the Linear Least Squares Problem. Given the function $$f(\theta) = \frac{1}{2} \lVert y - \Phi \theta \rVert _{2}^{2}$$ To find the minimizer we need to compute the gradient of f. The text says that $$\nabla f(\theta^{*}) = 0 \leftrightarrow \Phi^{T}\Phi\theta^{*} - \Phi^{T}y = 0$$ Can someone help me and explain how to compute the gradient of f? I don't understand why $$\nabla f(\theta^{*}) = \Phi^{T}\Phi\theta^{*} - \Phi^{T}y$$,,"['linear-algebra', 'matrices', 'matrix-calculus', 'least-squares']"
22,Is the space of periodic real sequences a closed subspace of the bounded real sequences?,Is the space of periodic real sequences a closed subspace of the bounded real sequences?,,"Let $E$ be the set of bounded real sequences equipped with the following norm: $$||(a_n)_{n \in \mathbb{N}}||_{\infty}=\sup_{n \in \mathbb{N}}|a_n|$$ Let $P$ be the set of real periodic sequences, which is a subspace of $E$. Is $P$ is closed in $E$? I suspect the answer is yes. I tried of a good while to construct a counterexample but failed. Edit: what I tried... Suppose $(s_n)_{n \in \mathbb{N}}$ is a sequence of elements of $P$ with limit $a$ in $E$, and let $p_n$ denotes the period of $s_n$. If $(p_n)_{n \in \mathbb{N}}$ is bounded then there is some number $p$ and a subsequence of $(s_n)_{n \in \mathbb{N}}$ in which each term is a sequence of period $p$, which implies $s$ has period $p$. To find a counterexample we would thus need $(p_n)$ unbounded and, assuming without loss of generality that $(p_n)$ is increasing, we would even need $p_{n+1}-p_n$ to be unbounded. Finally the limit sequence $a$ would need to satisfy for all $k$ and $m$: $a_{k+mp_n} \to a_{k}$  as $n$ tends to infinity. Already constructing such a limit sequence $a$ is far from obvious!","Let $E$ be the set of bounded real sequences equipped with the following norm: $$||(a_n)_{n \in \mathbb{N}}||_{\infty}=\sup_{n \in \mathbb{N}}|a_n|$$ Let $P$ be the set of real periodic sequences, which is a subspace of $E$. Is $P$ is closed in $E$? I suspect the answer is yes. I tried of a good while to construct a counterexample but failed. Edit: what I tried... Suppose $(s_n)_{n \in \mathbb{N}}$ is a sequence of elements of $P$ with limit $a$ in $E$, and let $p_n$ denotes the period of $s_n$. If $(p_n)_{n \in \mathbb{N}}$ is bounded then there is some number $p$ and a subsequence of $(s_n)_{n \in \mathbb{N}}$ in which each term is a sequence of period $p$, which implies $s$ has period $p$. To find a counterexample we would thus need $(p_n)$ unbounded and, assuming without loss of generality that $(p_n)$ is increasing, we would even need $p_{n+1}-p_n$ to be unbounded. Finally the limit sequence $a$ would need to satisfy for all $k$ and $m$: $a_{k+mp_n} \to a_{k}$  as $n$ tends to infinity. Already constructing such a limit sequence $a$ is far from obvious!",,"['linear-algebra', 'sequences-and-series', 'general-topology']"
23,Matrix Linear Least Squares Problem with Diagonal Matrix Constraint,Matrix Linear Least Squares Problem with Diagonal Matrix Constraint,,"How could one solve the following least-squares problem with Frobenius Norm and diagonal matrix constraint? $$\hat{S} := \arg \min_{S} \left\| Y - XUSV^T \right\|_{F}^{2}$$ where the $S$ is a diagonal matrix and $U,V$ are column-orthogonal matrix. Is there any fast algorithm?",How could one solve the following least-squares problem with Frobenius Norm and diagonal matrix constraint? where the is a diagonal matrix and are column-orthogonal matrix. Is there any fast algorithm?,"\hat{S} := \arg \min_{S} \left\| Y - XUSV^T \right\|_{F}^{2} S U,V","['linear-algebra', 'optimization', 'convex-optimization', 'least-squares', 'svd']"
24,Does $\operatorname{tr} (A)=0$ imply $\operatorname{tr} (A^3)=0$?,Does  imply ?,\operatorname{tr} (A)=0 \operatorname{tr} (A^3)=0,"Let $A_{(2n+1)\times(2n+1)}$ be a symmetric matrix of Rank $2n$. Then does $\operatorname{tr}A=0$ imply $\operatorname{tr}A^3=0$? If not, Under what condition?","Let $A_{(2n+1)\times(2n+1)}$ be a symmetric matrix of Rank $2n$. Then does $\operatorname{tr}A=0$ imply $\operatorname{tr}A^3=0$? If not, Under what condition?",,"['linear-algebra', 'matrices']"
25,Linear Algebra - basis question,Linear Algebra - basis question,,"I am revising for a Linear Algebra exam by going through some previous quiz questions, that I have True/False answers to, but not the reasoning or counterexamples. I am stuck on the following: If $v_1,v_2,v_3,v_4$ is a basis for $V$, and $U$ is a subspace of $V$ such that $v_1,v_2\in U$ but $v_3,v_4\notin U$, then $v_1,v_2$ is a basis of U. The answer is listed as False, which intuitively seems right, but I can't seem to find a counterexample.","I am revising for a Linear Algebra exam by going through some previous quiz questions, that I have True/False answers to, but not the reasoning or counterexamples. I am stuck on the following: If $v_1,v_2,v_3,v_4$ is a basis for $V$, and $U$ is a subspace of $V$ such that $v_1,v_2\in U$ but $v_3,v_4\notin U$, then $v_1,v_2$ is a basis of U. The answer is listed as False, which intuitively seems right, but I can't seem to find a counterexample.",,['linear-algebra']
26,Integer matrices with determinant equal to $1$,Integer matrices with determinant equal to,1,"Integer matrices with determinant equal to $1$ are quite useful in many situations. Take, for example, this question . For the $2 \times 2$ case it's easy to find many such matrices, e.g., $$\begin{bmatrix} 2 & 3 \\ 3 & 5 \\ \end{bmatrix}$$ $$\begin{bmatrix} 4 & 3 \\ 5 & 4 \\ \end{bmatrix}$$ But how to construct the procedure for generation integer matrix with arbitrarily chosen dimension $n \times n$? Is it a method which is as general as it is possible? I'm also interested in the answer how many degrees of freedom has an integer matrix with determinant equal 1 (or other perhaps number) ? Without determinant constraint $n \times n$ matrix has of course $n^2$ degrees of freedom.. how many is lost when we constrain it with determinant?","Integer matrices with determinant equal to $1$ are quite useful in many situations. Take, for example, this question . For the $2 \times 2$ case it's easy to find many such matrices, e.g., $$\begin{bmatrix} 2 & 3 \\ 3 & 5 \\ \end{bmatrix}$$ $$\begin{bmatrix} 4 & 3 \\ 5 & 4 \\ \end{bmatrix}$$ But how to construct the procedure for generation integer matrix with arbitrarily chosen dimension $n \times n$? Is it a method which is as general as it is possible? I'm also interested in the answer how many degrees of freedom has an integer matrix with determinant equal 1 (or other perhaps number) ? Without determinant constraint $n \times n$ matrix has of course $n^2$ degrees of freedom.. how many is lost when we constrain it with determinant?",,"['linear-algebra', 'matrices', 'determinant']"
27,Why is it that when a determinant = 0 then the homogeneous equations represented by the matrix has a non trivial solution? [duplicate],Why is it that when a determinant = 0 then the homogeneous equations represented by the matrix has a non trivial solution? [duplicate],,This question already has answers here : Why square matrix with zero determinant have non trivial solution (2 answers) Closed 6 years ago . As I was following a lecture the instructor seemed to assume this and when on solve for the equations where the right side was equal to 0 and proceed with the problem but I know if a determinant is non zero than an inverse matrix exists and visa versa but I cannot seem to relate this to this.  In any event I cannot see the intuition as to why when the determinant is non zero then there are or is a non trivial solution.  I assume in this case the equations are homogeneous.  Thank you,This question already has answers here : Why square matrix with zero determinant have non trivial solution (2 answers) Closed 6 years ago . As I was following a lecture the instructor seemed to assume this and when on solve for the equations where the right side was equal to 0 and proceed with the problem but I know if a determinant is non zero than an inverse matrix exists and visa versa but I cannot seem to relate this to this.  In any event I cannot see the intuition as to why when the determinant is non zero then there are or is a non trivial solution.  I assume in this case the equations are homogeneous.  Thank you,,['linear-algebra']
28,"If $T^m$ is diagonalizable for a $m\in\mathbb N$, then $T$ is diagonalizable.","If  is diagonalizable for a , then  is diagonalizable.",T^m m\in\mathbb N T,"Suppose that $V$ is a finite dimensional $\mathbb C$-vector space, and suppose that $T:V\rightarrow V$ is injective. If there is a $m\in\mathbb N$ such $T^m$ is diagonalizable, then $T$ is diagonalizable. I've found the proof for the case that $T^m=Id$, but I can't adapt it to this case.","Suppose that $V$ is a finite dimensional $\mathbb C$-vector space, and suppose that $T:V\rightarrow V$ is injective. If there is a $m\in\mathbb N$ such $T^m$ is diagonalizable, then $T$ is diagonalizable. I've found the proof for the case that $T^m=Id$, but I can't adapt it to this case.",,"['linear-algebra', 'linear-transformations']"
29,Relation between basis vectors and unit vectors,Relation between basis vectors and unit vectors,,"Preamble I know from linear algebra that any vector in a vector space can be written as a linear combination of basis vectors. However, in physics, unit vectors are used as basis vectors, which brings me to my question. Note : I do not have the math tools to formally prove any of this... I just need an explanation The question Which is correct (and why): Are all unit vectors basis vectors? Are all basis vector unit vectors? Are unit vectors a subset of basis vectors? Are basis vectors a subset of unit vectors? In a nutshell if the set of all basis vectors was b and the set of all unit vectors was u , what is the relation between the two sets? (i.e. b ___ u)","Preamble I know from linear algebra that any vector in a vector space can be written as a linear combination of basis vectors. However, in physics, unit vectors are used as basis vectors, which brings me to my question. Note : I do not have the math tools to formally prove any of this... I just need an explanation The question Which is correct (and why): Are all unit vectors basis vectors? Are all basis vector unit vectors? Are unit vectors a subset of basis vectors? Are basis vectors a subset of unit vectors? In a nutshell if the set of all basis vectors was b and the set of all unit vectors was u , what is the relation between the two sets? (i.e. b ___ u)",,"['linear-algebra', 'vectors']"
30,Sufficient conditions for positive semidefinite matrices,Sufficient conditions for positive semidefinite matrices,,"Is a symmetric matrix with positive terms (i.e., $a_{ij} > 0$) and positive determinant positive semidefinite? Is a symmetric matrix with positive terms, positive determinant, and terms that satisfy $a_{ij}^2 \leq a_{ii} a_{jj}$ positive semidefinite?","Is a symmetric matrix with positive terms (i.e., $a_{ij} > 0$) and positive determinant positive semidefinite? Is a symmetric matrix with positive terms, positive determinant, and terms that satisfy $a_{ij}^2 \leq a_{ii} a_{jj}$ positive semidefinite?",,"['linear-algebra', 'matrices', 'symmetric-matrices', 'positive-semidefinite']"
31,Back-projecting Pixel to 3D Rays in World Coordinates using PseudoInverse Method,Back-projecting Pixel to 3D Rays in World Coordinates using PseudoInverse Method,,"For perspective projection with given camera matrices and rotation and translation we can compute the 2D pixel coordinate of a 3D point. using the projection matrix, $$ P = K [R | t] $$ where $K$ is intrinsic camera matrix, $R$ is rotation $t$ is translation. The projection is simple matrix multiplication $x = P X $.  Zisserman's book , pg. 161 suggests using $3 \times 4$ projection matrix and taking pseudoinverse. Then one would compute $X$ which defined up to scale which can then be interpreted as the ray starting from camera center going to infinity. I quickly coded this up, I took $Z$ as depth, so I translated the camera in $Y$ direction (up 1 meter), and after retrieving $X$ flipped $Y,Z$ for plotting (most projective geom. math seems to be built to make $Z$ depth), K = [[ 282.363047,      0.,          166.21515189],      [   0.,          280.10715905,  108.05494375],      [   0.,            0.,            1.        ]] K = np.array(K) R = np.eye(3) t = np.array([[0],[1],[0]]) P = K.dot(np.hstack((R,t)))  import scipy.linalg as lin  x = np.array([300,300,1]) X = np.dot(lin.pinv(P),x) X = X / X[3]  from mpl_toolkits.mplot3d import Axes3D w = 20 f = plt.figure() XX  = X[:]; XX[1] = X[2]; XX[2] = X[1] ax = f.gca(projection='3d') ax.quiver(0, 0, 1., XX[:3][0], XX[:3][1], XX[:3][2],color='red') ax.set_xlim(0,10);ax.set_ylim(0,10);ax.set_zlim(0,10) ax.quiver(0., 0., 1., 0, 5., 0.,color='blue') ax.set_xlabel(""X"") ax.set_ylabel(""Y"") ax.set_zlabel(""Z"") ax.set_title(str(x[0])+"",""+str(x[1])) ax.set_xlim(-w,w);ax.set_ylim(-w,w);ax.set_zlim(-w,w)  ax.view_init(elev=29, azim=-30) fout = 'test_%s_01.png' % (str(x[0])+str(x[1])) plt.savefig(fout) ax.view_init(elev=29, azim=-60) fout = 'test_%s_02.png' % (str(x[0])+str(x[1])) plt.savefig(fout) These images below are the result (blue arrow shows the normal vector perpendicular to the image plane, the images demonstrate all x=10,300 y=10,300 combinations): I give the camera/ray plot for each pixel from two different angles. Do these results look sensible? 10,10 and 200,200 looked odd, I played around with signs a little bit, if I translate up using negative -1, and using -Z after X calc., things improve somewhat? t = np.array([[0],[-1],[0]]) .. XX  = X[:]; XX[1] = X[2]; XX[2] = -X[1] I do not know why that is.","For perspective projection with given camera matrices and rotation and translation we can compute the 2D pixel coordinate of a 3D point. using the projection matrix, $$ P = K [R | t] $$ where $K$ is intrinsic camera matrix, $R$ is rotation $t$ is translation. The projection is simple matrix multiplication $x = P X $.  Zisserman's book , pg. 161 suggests using $3 \times 4$ projection matrix and taking pseudoinverse. Then one would compute $X$ which defined up to scale which can then be interpreted as the ray starting from camera center going to infinity. I quickly coded this up, I took $Z$ as depth, so I translated the camera in $Y$ direction (up 1 meter), and after retrieving $X$ flipped $Y,Z$ for plotting (most projective geom. math seems to be built to make $Z$ depth), K = [[ 282.363047,      0.,          166.21515189],      [   0.,          280.10715905,  108.05494375],      [   0.,            0.,            1.        ]] K = np.array(K) R = np.eye(3) t = np.array([[0],[1],[0]]) P = K.dot(np.hstack((R,t)))  import scipy.linalg as lin  x = np.array([300,300,1]) X = np.dot(lin.pinv(P),x) X = X / X[3]  from mpl_toolkits.mplot3d import Axes3D w = 20 f = plt.figure() XX  = X[:]; XX[1] = X[2]; XX[2] = X[1] ax = f.gca(projection='3d') ax.quiver(0, 0, 1., XX[:3][0], XX[:3][1], XX[:3][2],color='red') ax.set_xlim(0,10);ax.set_ylim(0,10);ax.set_zlim(0,10) ax.quiver(0., 0., 1., 0, 5., 0.,color='blue') ax.set_xlabel(""X"") ax.set_ylabel(""Y"") ax.set_zlabel(""Z"") ax.set_title(str(x[0])+"",""+str(x[1])) ax.set_xlim(-w,w);ax.set_ylim(-w,w);ax.set_zlim(-w,w)  ax.view_init(elev=29, azim=-30) fout = 'test_%s_01.png' % (str(x[0])+str(x[1])) plt.savefig(fout) ax.view_init(elev=29, azim=-60) fout = 'test_%s_02.png' % (str(x[0])+str(x[1])) plt.savefig(fout) These images below are the result (blue arrow shows the normal vector perpendicular to the image plane, the images demonstrate all x=10,300 y=10,300 combinations): I give the camera/ray plot for each pixel from two different angles. Do these results look sensible? 10,10 and 200,200 looked odd, I played around with signs a little bit, if I translate up using negative -1, and using -Z after X calc., things improve somewhat? t = np.array([[0],[-1],[0]]) .. XX  = X[:]; XX[1] = X[2]; XX[2] = -X[1] I do not know why that is.",,"['linear-algebra', 'geometry', 'projective-geometry', 'computational-geometry', 'image-processing']"
32,What I am missing in this simple equation from Nesterov's paper?,What I am missing in this simple equation from Nesterov's paper?,,"In this paper by Prof Nesterov, First-order methods of smooth convex optimization with inexact oracle , proof of Theorem 2, there is the following very simple equation which I think it is wrong, \begin{align} \Vert x_{k+1}-x^*\Vert^2 = \Vert x_{k}-x^*\Vert^2 + 2\langle B(x_{k+1}-x_k),x_{k+1}-x^*\rangle -\Vert x_{k+1}-x_k\Vert^2 \end{align}  (They defined norm as $\Vert x\Vert^2 =\langle Bx,x\rangle$) Obviously, it must be  \begin{align} \Vert x_{k+1}-x^*\Vert^2 = \Vert x_k-x^*+x_{k+1}-x_{k}\Vert^2 = \Vert x_{k}-x^*\Vert^2 + 2\langle B(x_{k+1}-x_k),x_{k}-x^*\rangle \\ +\Vert x_{k+1}-x_k\Vert^2 \end{align} Am I missing something? I also, checked journal version, Mathematical Programming .","In this paper by Prof Nesterov, First-order methods of smooth convex optimization with inexact oracle , proof of Theorem 2, there is the following very simple equation which I think it is wrong, \begin{align} \Vert x_{k+1}-x^*\Vert^2 = \Vert x_{k}-x^*\Vert^2 + 2\langle B(x_{k+1}-x_k),x_{k+1}-x^*\rangle -\Vert x_{k+1}-x_k\Vert^2 \end{align}  (They defined norm as $\Vert x\Vert^2 =\langle Bx,x\rangle$) Obviously, it must be  \begin{align} \Vert x_{k+1}-x^*\Vert^2 = \Vert x_k-x^*+x_{k+1}-x_{k}\Vert^2 = \Vert x_{k}-x^*\Vert^2 + 2\langle B(x_{k+1}-x_k),x_{k}-x^*\rangle \\ +\Vert x_{k+1}-x_k\Vert^2 \end{align} Am I missing something? I also, checked journal version, Mathematical Programming .",,"['linear-algebra', 'optimization', 'convex-optimization']"
33,"Show that $A=B$, provided that $A^2=B^2$ (and some other conditions are satisfied)","Show that , provided that  (and some other conditions are satisfied)",A=B A^2=B^2,"I'm trying to do this question from an old past paper, no answers to look at and because it's from a previous year I'm not entirely sure I've even covered the material; here it is: Let $A,B\in M_n(\mathbb R)$ be such that $A^2 = B^2,\, AB = BA$ and $\det(A + B) \ne 0$. Show that $A = B$. I've been playing around with it for ages but can't get anything, from the determinant part I'm guessing I have to involve the extant inverse of $A+B$ but I've never done that before and from looking it up it seems abit beyond what I should be doing. Any ideas?","I'm trying to do this question from an old past paper, no answers to look at and because it's from a previous year I'm not entirely sure I've even covered the material; here it is: Let $A,B\in M_n(\mathbb R)$ be such that $A^2 = B^2,\, AB = BA$ and $\det(A + B) \ne 0$. Show that $A = B$. I've been playing around with it for ages but can't get anything, from the determinant part I'm guessing I have to involve the extant inverse of $A+B$ but I've never done that before and from looking it up it seems abit beyond what I should be doing. Any ideas?",,"['linear-algebra', 'matrices']"
34,Dimension of space of linear maps between vector spaces,Dimension of space of linear maps between vector spaces,,"Let $F$ be a field, $V$ and $W$ are vector spaces over the field $F$. The dimension of $V$ is $n$ and the dimension of $W$ is $m$, where $m, n$ are natural numbers. Let $\mathcal{L}$ be a vector space of all linear maps from V to W. Determine the dimension of $\mathcal{L}$ depending on values $m,n$. I know there should be a solution using isomorphism between $\mathcal{L}$ an a vector space of $m \times n$ matrices, but I can't prove it.","Let $F$ be a field, $V$ and $W$ are vector spaces over the field $F$. The dimension of $V$ is $n$ and the dimension of $W$ is $m$, where $m, n$ are natural numbers. Let $\mathcal{L}$ be a vector space of all linear maps from V to W. Determine the dimension of $\mathcal{L}$ depending on values $m,n$. I know there should be a solution using isomorphism between $\mathcal{L}$ an a vector space of $m \times n$ matrices, but I can't prove it.",,"['linear-algebra', 'matrices', 'vector-spaces']"
35,"When are $\beta_1, \beta_2, \ldots, \beta_n$ linear independent?",When are  linear independent?,"\beta_1, \beta_2, \ldots, \beta_n","Given $n$ linear independent vectors, $\alpha_1, \alpha_2, \ldots, \alpha_n$. Now, let $$\beta_1 = \alpha_1 + \alpha_2 + \cdots + \alpha_m$$ $$\beta_2 = \alpha_2 + \alpha_3 + \cdots + \alpha_{m+1}$$ $$\ldots$$ $$\beta_{n-m} = \alpha_{n-m} + \alpha_{n-m+1} + \cdots + \alpha_n$$ $$\beta_{n-m+1} = \alpha_{n-m+1} + \alpha_{n-m+2} + \cdots + \alpha_{n+1}$$ $$\ldots$$ $$\beta_{n-1} = \alpha_{n-1} + \alpha_{n} + \cdots + \alpha_{m-2}$$ $$\beta_{n} = \alpha_{n} + \alpha_{1} + \cdots + \alpha_{m-1}$$ where $1 \lt m \lt n$. For example, if $n=3$ and $m=2$, then $$\beta_1 = \alpha_1 + \alpha_2$$ $$\beta_2 = \alpha_2 + \alpha_3$$ $$\beta_3 = \alpha_3 + \alpha_1$$ The question is, to which condition $n$ and $m$ must meet when $\beta_1, \beta_2, \ldots, \beta_n$ are linear independent? A guess is that $n$ and $m$ must be relatively prime, but I can neither prove or disprove it.","Given $n$ linear independent vectors, $\alpha_1, \alpha_2, \ldots, \alpha_n$. Now, let $$\beta_1 = \alpha_1 + \alpha_2 + \cdots + \alpha_m$$ $$\beta_2 = \alpha_2 + \alpha_3 + \cdots + \alpha_{m+1}$$ $$\ldots$$ $$\beta_{n-m} = \alpha_{n-m} + \alpha_{n-m+1} + \cdots + \alpha_n$$ $$\beta_{n-m+1} = \alpha_{n-m+1} + \alpha_{n-m+2} + \cdots + \alpha_{n+1}$$ $$\ldots$$ $$\beta_{n-1} = \alpha_{n-1} + \alpha_{n} + \cdots + \alpha_{m-2}$$ $$\beta_{n} = \alpha_{n} + \alpha_{1} + \cdots + \alpha_{m-1}$$ where $1 \lt m \lt n$. For example, if $n=3$ and $m=2$, then $$\beta_1 = \alpha_1 + \alpha_2$$ $$\beta_2 = \alpha_2 + \alpha_3$$ $$\beta_3 = \alpha_3 + \alpha_1$$ The question is, to which condition $n$ and $m$ must meet when $\beta_1, \beta_2, \ldots, \beta_n$ are linear independent? A guess is that $n$ and $m$ must be relatively prime, but I can neither prove or disprove it.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'vector-spaces', 'vectors']"
36,Proving a given formula for projection matrix,Proving a given formula for projection matrix,,"In an $n$-dimensional inner product space $V$, I have $k$ ($k\le n$) linearly independent vectors $\{b_1,b_2,\cdots,b_k\}$ spanning a subspace $U$. The $k$ vectors need not be orthogonal. Then I was told that the projection of an arbitrary vector $c$ onto $U$ is given by $$P_Uc=A(A^TA)^{-1}A^Tc$$ where $A$ is the $n\times k$ matrix with column vectors $\left((b_1)(b_2)\cdots(b_k)\right)$. My questions are (1) How to prove that it is the projection? (EDIT: which has been answered, thanks!) (2) how can one be sure that the matrix $A^TA$ is invertible?","In an $n$-dimensional inner product space $V$, I have $k$ ($k\le n$) linearly independent vectors $\{b_1,b_2,\cdots,b_k\}$ spanning a subspace $U$. The $k$ vectors need not be orthogonal. Then I was told that the projection of an arbitrary vector $c$ onto $U$ is given by $$P_Uc=A(A^TA)^{-1}A^Tc$$ where $A$ is the $n\times k$ matrix with column vectors $\left((b_1)(b_2)\cdots(b_k)\right)$. My questions are (1) How to prove that it is the projection? (EDIT: which has been answered, thanks!) (2) how can one be sure that the matrix $A^TA$ is invertible?",,"['linear-algebra', 'matrices', 'inner-products', 'projection-matrices']"
37,Uniqueness of QR decomposition [duplicate],Uniqueness of QR decomposition [duplicate],,This question already has an answer here : Is the matrix $R$ in the $QR$ decomposition unique? (1 answer) Closed 7 years ago . Let $M$ be a square $N\times N$ matrix with linearly independent columns. I was wondering why the $QR$ decomposition is unique in this case and how to show it? The linked question below does not explain why $Q$ is also unique and I didn't quite understand its explanation of why $R$ is unique also.,This question already has an answer here : Is the matrix $R$ in the $QR$ decomposition unique? (1 answer) Closed 7 years ago . Let $M$ be a square $N\times N$ matrix with linearly independent columns. I was wondering why the $QR$ decomposition is unique in this case and how to show it? The linked question below does not explain why $Q$ is also unique and I didn't quite understand its explanation of why $R$ is unique also.,,"['linear-algebra', 'matrices']"
38,Let $A$ be a $2 \times 2$ real matrix such that $A^2 - A + (1/2)I = 0$. Prove that $A^n \to 0$ as $n \to \infty$.,Let  be a  real matrix such that . Prove that  as .,A 2 \times 2 A^2 - A + (1/2)I = 0 A^n \to 0 n \to \infty,"Question: Let $A$ be a $2 \times 2$ matrix with real entries such that $A^2 - A + (1/2)I = 0$, where $I$ is the $2 \times 2$ identity matrix and $0$ is the $2 \times 2$ zero matrix. Prove that $A^n \to 0$ as $n \to \infty$. My attempt: Here is my idea so far. Consider $A$ as a $2 \times 2$ matrix over the field of complex numbers. Now, the polynomial $$g(t) = t^2 - t + \frac{1}{2} $$ factors as $$g(t) = \left(\frac{1}{2} - \frac{i}{2}\right)\left(\frac{1}{2}+\frac{i}{2}\right)$$ over $C$. This means that the minimal polynomial of $A$ over $C$ is: $$m(t) = \left(t-(\frac{1}{2} - \frac{i}{2})\right), m(t) = \left(t-(\frac{1}{2} + \frac{i}{2})\right), \ \ \ \text{ or } \ \ \ m(t) = \left(t-(\frac{1}{2} - \frac{i}{2})\right)\left(t-(\frac{1}{2} + \frac{i}{2})\right),$$ and thus we have $A = Q D Q^{-1}$, where $$D =\left[ {\begin{array}{*{20}{c}} {\frac{1}{2}(1 - i)}&0\\ 0&{\frac{1}{2}(1 - i)} \end{array}} \right],$$ $$D = \left[ {\begin{array}{*{20}{c}} {\frac{1}{2}(1 + i)}&0\\ 0&{\frac{1}{2}(1 + i)} \end{array}} \right],\ \ \ \text{ or } $$ $$D = \left[ {\begin{array}{*{20}{c}} {\frac{1}{2}(1 - i)}&0\\ 0&{\frac{1}{2}(1 + i)} \end{array}} \right].$$ Now, $A^n = QD^nQ^{-1}$. Here is where I get stuck. $D^n$ doesn't seem to be converging to $0$ as $n$ approaches infinity. In addition, I am concerned that my strategy is bad, since we are talking about a real matrix and I'm using a minimal polynomial over $C$. Is it still true that $A$ must have one of the three forms above, even if $A$ is supposed to be real? Thanks for any help/suggestions you may be able to provide.","Question: Let $A$ be a $2 \times 2$ matrix with real entries such that $A^2 - A + (1/2)I = 0$, where $I$ is the $2 \times 2$ identity matrix and $0$ is the $2 \times 2$ zero matrix. Prove that $A^n \to 0$ as $n \to \infty$. My attempt: Here is my idea so far. Consider $A$ as a $2 \times 2$ matrix over the field of complex numbers. Now, the polynomial $$g(t) = t^2 - t + \frac{1}{2} $$ factors as $$g(t) = \left(\frac{1}{2} - \frac{i}{2}\right)\left(\frac{1}{2}+\frac{i}{2}\right)$$ over $C$. This means that the minimal polynomial of $A$ over $C$ is: $$m(t) = \left(t-(\frac{1}{2} - \frac{i}{2})\right), m(t) = \left(t-(\frac{1}{2} + \frac{i}{2})\right), \ \ \ \text{ or } \ \ \ m(t) = \left(t-(\frac{1}{2} - \frac{i}{2})\right)\left(t-(\frac{1}{2} + \frac{i}{2})\right),$$ and thus we have $A = Q D Q^{-1}$, where $$D =\left[ {\begin{array}{*{20}{c}} {\frac{1}{2}(1 - i)}&0\\ 0&{\frac{1}{2}(1 - i)} \end{array}} \right],$$ $$D = \left[ {\begin{array}{*{20}{c}} {\frac{1}{2}(1 + i)}&0\\ 0&{\frac{1}{2}(1 + i)} \end{array}} \right],\ \ \ \text{ or } $$ $$D = \left[ {\begin{array}{*{20}{c}} {\frac{1}{2}(1 - i)}&0\\ 0&{\frac{1}{2}(1 + i)} \end{array}} \right].$$ Now, $A^n = QD^nQ^{-1}$. Here is where I get stuck. $D^n$ doesn't seem to be converging to $0$ as $n$ approaches infinity. In addition, I am concerned that my strategy is bad, since we are talking about a real matrix and I'm using a minimal polynomial over $C$. Is it still true that $A$ must have one of the three forms above, even if $A$ is supposed to be real? Thanks for any help/suggestions you may be able to provide.",,"['linear-algebra', 'matrices']"
39,Linearity of the right inverse of a surjective linear map,Linearity of the right inverse of a surjective linear map,,"Suppose we have a surjective linear map $f:V\to V$ on an infinite-dimensional vector space $V$. We know that every surjective map has at least one right inverse. So I was wondering... I know not all right inverses are linear, for example, on the space of real sequences $$(a_0,a_1,a_2,\cdots)\mapsto(1,a_0,a_1,a_2,\cdots)$$ is a right inverse of the surjective linear map $$(a_0,a_1,a_2,\cdots)\mapsto(a_1,a_2,\cdots)$$ but obviously isn't a linear map itself, but on the other hand $$(a_0,a_1,a_2,\cdots)\mapsto(0,a_0,a_1,a_2,\cdots)$$ is a linear right inverse.  So I was wondering... does there always exist a linear right inverse of any linear surjective map on an infinite-dimensional vector space over an arbitrary field $K$?","Suppose we have a surjective linear map $f:V\to V$ on an infinite-dimensional vector space $V$. We know that every surjective map has at least one right inverse. So I was wondering... I know not all right inverses are linear, for example, on the space of real sequences $$(a_0,a_1,a_2,\cdots)\mapsto(1,a_0,a_1,a_2,\cdots)$$ is a right inverse of the surjective linear map $$(a_0,a_1,a_2,\cdots)\mapsto(a_1,a_2,\cdots)$$ but obviously isn't a linear map itself, but on the other hand $$(a_0,a_1,a_2,\cdots)\mapsto(0,a_0,a_1,a_2,\cdots)$$ is a linear right inverse.  So I was wondering... does there always exist a linear right inverse of any linear surjective map on an infinite-dimensional vector space over an arbitrary field $K$?",,"['linear-algebra', 'vector-spaces', 'linear-transformations']"
40,Expected value of $\log(\det(AA^T))$,Expected value of,\log(\det(AA^T)),"Consider uniform random $n$ by $n$ matrix $A$ where $A_{i,j} \in \{-1,1\}$.  We know that with high probability $A$ is non-singular. Are there known estimates or bounds for $$\mathbb{E}(\log(\det(AA^T)))\;?$$","Consider uniform random $n$ by $n$ matrix $A$ where $A_{i,j} \in \{-1,1\}$.  We know that with high probability $A$ is non-singular. Are there known estimates or bounds for $$\mathbb{E}(\log(\det(AA^T)))\;?$$",,['linear-algebra']
41,Prove that $A(A+B)^{-1}B=B(A+B)^{-1}A$,Prove that,A(A+B)^{-1}B=B(A+B)^{-1}A,"Given $A$, $B$ are two square matrices, $A+B$ is non-singular, prove above state. This is one part of my algebra mid term, of course, I failed, I have no idea where I should start. One thing appeared in my mind during the exam is $A(I+A)=(I+A)A$, $(I+A)$ is non-singular, how to represent A+B under that form... That maybe a wrong idea.","Given $A$, $B$ are two square matrices, $A+B$ is non-singular, prove above state. This is one part of my algebra mid term, of course, I failed, I have no idea where I should start. One thing appeared in my mind during the exam is $A(I+A)=(I+A)A$, $(I+A)$ is non-singular, how to represent A+B under that form... That maybe a wrong idea.",,"['linear-algebra', 'matrices']"
42,Can a non-square matrix have a full rank?,Can a non-square matrix have a full rank?,,"Can a non-square matrix have a full rank? I always see cases with square matrix with full rank but seldom with non-square matrix. Can anyone help on this? For example, is the following matrix full rank? $$ A =  \begin{pmatrix} 1 & 3 & 10 \\  2 & 3 & 14 \end{pmatrix} $$ My lecture slide says this does not have a full rank because any multiple of $x'=\begin{bmatrix} 2 & 1 & -1/2 \end{bmatrix}$ will give $Ax=0$ $$ \begin{pmatrix} 1 & 3 & 10 \\  2 & 3 & 14 \end{pmatrix} \begin{pmatrix} 2 \\  1 \\ -1/2 \end{pmatrix} = \begin{pmatrix} 0 \\  0  \end{pmatrix} $$ I don't think this is correct but may I check?","Can a non-square matrix have a full rank? I always see cases with square matrix with full rank but seldom with non-square matrix. Can anyone help on this? For example, is the following matrix full rank? My lecture slide says this does not have a full rank because any multiple of will give I don't think this is correct but may I check?","
A = 
\begin{pmatrix}
1 & 3 & 10 \\ 
2 & 3 & 14
\end{pmatrix}
 x'=\begin{bmatrix} 2 & 1 & -1/2 \end{bmatrix} Ax=0 
\begin{pmatrix}
1 & 3 & 10 \\ 
2 & 3 & 14
\end{pmatrix}
\begin{pmatrix}
2 \\ 
1 \\
-1/2
\end{pmatrix} =
\begin{pmatrix}
0 \\ 
0 
\end{pmatrix}
","['linear-algebra', 'matrices', 'matrix-rank']"
43,Dimension of totally isotropic subspaces for a given quadratic form,Dimension of totally isotropic subspaces for a given quadratic form,,"Let $X$ be a real linear space with the quadratic form given in a basis $(e_1,...,e_r,e_{r+1},...,e_{r+s} )$ by $$Q(x)=\sum_{i=1}^r x_i^2-\sum_{i=r+1}^{r+s} x_i^2$$ for $x=\sum_{i=1}^{r+s}x_i e_i.$ A linear subspace $V\subset X$ is called totally isotropic if $Q|_V=0$ . I know that all maximal (in the sense of inclusion) totally isotropic subspace of $X$ have the same dimension. Assume for example that $r\ge s$ . How to prove the maximal isotropic subspaces for the $Q$ have dimension $r$ ? Thanks.",Let be a real linear space with the quadratic form given in a basis by for A linear subspace is called totally isotropic if . I know that all maximal (in the sense of inclusion) totally isotropic subspace of have the same dimension. Assume for example that . How to prove the maximal isotropic subspaces for the have dimension ? Thanks.,"X (e_1,...,e_r,e_{r+1},...,e_{r+s} ) Q(x)=\sum_{i=1}^r x_i^2-\sum_{i=r+1}^{r+s} x_i^2 x=\sum_{i=1}^{r+s}x_i e_i. V\subset X Q|_V=0 X r\ge s Q r","['linear-algebra', 'quadratic-forms']"
44,$T^2=I$ implies $T$ is diagonalizable,implies  is diagonalizable,T^2=I T,"Suppose $T:V\rightarrow V$ is linear and $T^2=I$. Prove that $T$ is diagonalizable. First, I know that $T$ has only eigenvalues 1 or -1. Also I observed that $(T-I)(T+I)=0$, does this fact help to show that $T$ is diagonalizable?","Suppose $T:V\rightarrow V$ is linear and $T^2=I$. Prove that $T$ is diagonalizable. First, I know that $T$ has only eigenvalues 1 or -1. Also I observed that $(T-I)(T+I)=0$, does this fact help to show that $T$ is diagonalizable?",,['linear-algebra']
45,Geometric meaning of outer product of a vector with itself,Geometric meaning of outer product of a vector with itself,,"This question is related to the question in the link below: Is there a geometric meaning to the outer product of two vectors? The answer is clear, but I am wondering: If we take a outer product of a vector with itself, then is there a specific geometric meaning of the matrix which is not evident from an interpretation of the outer product of two general vectors?","This question is related to the question in the link below: Is there a geometric meaning to the outer product of two vectors? The answer is clear, but I am wondering: If we take a outer product of a vector with itself, then is there a specific geometric meaning of the matrix which is not evident from an interpretation of the outer product of two general vectors?",,['linear-algebra']
46,Nipotent matrix over a ring,Nipotent matrix over a ring,,"This question is linked to this one: nilpotent elements of $M_2(\mathbb{R})$, $M_2(\mathbb{Z}/4\mathbb{Z})$ Let $R$ be a commutative ring with unity and let $A\in M_2(R)$. Show that $A$ is a nilpotent matrix iff $\det(A)$ and $\mathrm{trace}(A)$ are nilpotent elements of $R$.","This question is linked to this one: nilpotent elements of $M_2(\mathbb{R})$, $M_2(\mathbb{Z}/4\mathbb{Z})$ Let $R$ be a commutative ring with unity and let $A\in M_2(R)$. Show that $A$ is a nilpotent matrix iff $\det(A)$ and $\mathrm{trace}(A)$ are nilpotent elements of $R$.",,['linear-algebra']
47,Examples of combinatorial/probabilistic proofs of theorems in linear algebra,Examples of combinatorial/probabilistic proofs of theorems in linear algebra,,"Are there any examples of combinatorial/probabilistic proofs of theorems in linear algebra? Motivation: I see here , the inverse is true.","Are there any examples of combinatorial/probabilistic proofs of theorems in linear algebra? Motivation: I see here , the inverse is true.",,"['linear-algebra', 'probability', 'combinatorics', 'soft-question', 'big-list']"
48,"Let $A_{j,k} = \langle x_j, x_k\rangle$. Show $A$ is invertible if and only if $x_1, \ldots, x_n$ are linearly independent.",Let . Show  is invertible if and only if  are linearly independent.,"A_{j,k} = \langle x_j, x_k\rangle A x_1, \ldots, x_n","Let $V$ be a vector space over $\mathbb C$ with inner product $\langle, \rangle$ and let $x_1, \ldots, x_n$ be vectors in $V$. Consider the $n \times n$-matrix $A$ with entries $A_{j,k} = \langle x_j, x_k\rangle$. I want to show that $A$ is invertible if and only if $x_1, \ldots, x_n$ are linearly independent. I know that orthogonal vectors are linearly independent, so if $x_1, \ldots, x_n$ where orthogonal the result is easily proven. However, the general case I'm stuck at.","Let $V$ be a vector space over $\mathbb C$ with inner product $\langle, \rangle$ and let $x_1, \ldots, x_n$ be vectors in $V$. Consider the $n \times n$-matrix $A$ with entries $A_{j,k} = \langle x_j, x_k\rangle$. I want to show that $A$ is invertible if and only if $x_1, \ldots, x_n$ are linearly independent. I know that orthogonal vectors are linearly independent, so if $x_1, \ldots, x_n$ where orthogonal the result is easily proven. However, the general case I'm stuck at.",,"['linear-algebra', 'matrices', 'vector-spaces', 'inner-products']"
49,Question about determinants,Question about determinants,,"I am working on some practice problems and I'm unsure where to begin this problem. It starts off by giving $\det(X)= 1$ for the following matrix $X$:$$ \begin{matrix} a & 1 & d \\ b & 1 & e \\ c & 1 & f \end{matrix} $$ and the $\det(Y)= 4$ for the following matrix $Y$: $$ \begin{matrix} a & 1 & d \\ b & 2 & e \\ c & 3 & f \end{matrix} $$ Knowing this, the text asked me to solve some other matrices, which I did with relative ease. For some reason, this last one I cannot solve: Find the determinant of: $$ \begin{matrix} a & 5 & d \\ b & 7 & e \\ c & 9 & f \end{matrix} $$ I have tried various rules and identities, but I can't seem to get to this form using scalar multiples or anything like that. Thanks so much for any help/suggestions/solutions.","I am working on some practice problems and I'm unsure where to begin this problem. It starts off by giving $\det(X)= 1$ for the following matrix $X$:$$ \begin{matrix} a & 1 & d \\ b & 1 & e \\ c & 1 & f \end{matrix} $$ and the $\det(Y)= 4$ for the following matrix $Y$: $$ \begin{matrix} a & 1 & d \\ b & 2 & e \\ c & 3 & f \end{matrix} $$ Knowing this, the text asked me to solve some other matrices, which I did with relative ease. For some reason, this last one I cannot solve: Find the determinant of: $$ \begin{matrix} a & 5 & d \\ b & 7 & e \\ c & 9 & f \end{matrix} $$ I have tried various rules and identities, but I can't seem to get to this form using scalar multiples or anything like that. Thanks so much for any help/suggestions/solutions.",,"['linear-algebra', 'matrices', 'determinant']"
50,Dimension of the sum of three subspaces,Dimension of the sum of three subspaces,,"We know that $$\dim(U_1 + U_2) = \dim U_1 + \dim U_2 - \dim(U_1 \cap U_2)$$ if $U_1$ and $U_2$ are finite dimensional subspaces. For three finite dimensional subspaces prove or give a counterexample for the following: $$ \begin{align} \dim(U_1 + U_2 + U_3) &= \dim U_1 + \dim U_2 + \dim U_3 \\ &- \dim(U_1 \cap U_2) - \dim(U_2 \cap U_3) - \dim(U_1 \cap U_2)\\ &+ \dim(U_1 \cap U_2 \cap U_3)  \end{align}$$ Its basically the formula for the union of three sets. I think that this is false but the only reason I can think it would be is because the union of subspaces is rarely a subspace itself, even though it applies to sets, so it seems to me like this formula which works for sets shouldn't work for subsets because it would disrupt overlapping elements of $U_1 + U_2 + U_3$ and run into complications with the sum being closed under addition or something like that. Help much appreciated thank you!","We know that if and are finite dimensional subspaces. For three finite dimensional subspaces prove or give a counterexample for the following: Its basically the formula for the union of three sets. I think that this is false but the only reason I can think it would be is because the union of subspaces is rarely a subspace itself, even though it applies to sets, so it seems to me like this formula which works for sets shouldn't work for subsets because it would disrupt overlapping elements of and run into complications with the sum being closed under addition or something like that. Help much appreciated thank you!","\dim(U_1 + U_2) = \dim U_1 + \dim U_2 - \dim(U_1 \cap U_2) U_1 U_2 
\begin{align}
\dim(U_1 + U_2 + U_3) &= \dim U_1 + \dim U_2 + \dim U_3 \\
&- \dim(U_1 \cap U_2) - \dim(U_2 \cap U_3) - \dim(U_1 \cap U_2)\\
&+ \dim(U_1 \cap U_2 \cap U_3) 
\end{align} U_1 + U_2 + U_3",['linear-algebra']
51,Proving commutativity of addition for vector spaces,Proving commutativity of addition for vector spaces,,"I'm trying to prove commutativity of addition for vector spaces, using the axioms for vector spaces. Apparently commutativity can be proven! Im having trouble getting a good feel for what is allowed and what is not. Here's my work so far: $u+v+u+v = 2(u+v) = 2u + 2v = u+u+v+v = u+(u+v)+v$ Here I just wanna claim that $u+(v+u)+v = u+(u+v)+v$ $\Rightarrow -u+u+(v+u)+v+(-v) = -u+u+(u+v)+v+(-v)$ : here im just adding -u to the right, and -v to the left. Question: is this ""adding to both sides"" really legit in this context? Why? Quick help proof: $-v+v = (-1)v+(1)v = (-1+1)v = 0v = 0 = v-v$ And another: $ 0+v = v+(-v) + v = (1)v + (-1)v + v = (1-1)v + v = v = v+0$ We have $0 + (u+v) + 0 = 0+(v+u)+0 \Rightarrow u+v = v+u$ This feels ugly and not at all elegant, especially the great leap ""add -u to both sides"" feels completely out of place. Do I need more lemmas? Is there a more elegant way? //not homework or anything, just for my own pleasure, feel free to provide theory, as it is more insightful than solutions. :) Thanks! EDIT: corrected notation a little.","I'm trying to prove commutativity of addition for vector spaces, using the axioms for vector spaces. Apparently commutativity can be proven! Im having trouble getting a good feel for what is allowed and what is not. Here's my work so far: $u+v+u+v = 2(u+v) = 2u + 2v = u+u+v+v = u+(u+v)+v$ Here I just wanna claim that $u+(v+u)+v = u+(u+v)+v$ $\Rightarrow -u+u+(v+u)+v+(-v) = -u+u+(u+v)+v+(-v)$ : here im just adding -u to the right, and -v to the left. Question: is this ""adding to both sides"" really legit in this context? Why? Quick help proof: $-v+v = (-1)v+(1)v = (-1+1)v = 0v = 0 = v-v$ And another: $ 0+v = v+(-v) + v = (1)v + (-1)v + v = (1-1)v + v = v = v+0$ We have $0 + (u+v) + 0 = 0+(v+u)+0 \Rightarrow u+v = v+u$ This feels ugly and not at all elegant, especially the great leap ""add -u to both sides"" feels completely out of place. Do I need more lemmas? Is there a more elegant way? //not homework or anything, just for my own pleasure, feel free to provide theory, as it is more insightful than solutions. :) Thanks! EDIT: corrected notation a little.",,"['linear-algebra', 'proof-verification']"
52,Prove that p has m distinct roots if and only if p and p' have no roots in common,Prove that p has m distinct roots if and only if p and p' have no roots in common,,"Problem: Suppose $p \in \mathcal{P}(\mathbf{C})$ has degree $m$. Prove that $p$ has $m$ distinct roots if and only if $p$ and its derivative $p'$ have no roots in common. My proof so far: If $m=0$, then $p(z)=a_0\neq 0$ and $p$ has no roots. Then $p'(z)=0$ and has no roots. If $m=1$, then $p(z)=a_0+a_1z$ with $a_1 \neq 0$ so $p$ has  exactly one root, namely $-a_0/a_1$ and $p'(z)=a_1$ and has no roots. In both cases, $p$ and $p'$ have no roots in common. Now suppose $m > 1$. We use induction on $m$, assuming that for every polynomial $r$ with $m-1$ distinct roots, $r$ and $r'$ have no roots in common. Let  $p$ be a polynomial of degree $m$ with distinct roots. There exists $q$ such that \begin{align*} p(z)=(z-\lambda)q(z) \end{align*} for all $z \in \mathbf{F}$. Since q has $m-1$ distinct roots, $q$ and $q'$ have no roots in common. By the chain rule,  \begin{align*} p'(z)=(z-\lambda)q'(z)+q(z) \end{align*} We know that $\lambda$ is not a root of $p'$ since $p'(\lambda)=(\lambda-\lambda)q'(z)+q(\lambda)=0+q(\lambda)\neq 0$ as lambda is not a root of $q$. All other roots of $p$ are roots of $q$. For these roots $\lambda_q$, $p'(\lambda_q)=(\lambda-\lambda_q)q'(\lambda_q)+q(\lambda_q)=(\lambda-\lambda_q)q'(\lambda_q) + 0 \neq 0$. Now suppose $m > 1$. We use induction on $m$, this time assuming that for every polynomial $r$ of degree $m-1$ such that $r$ and $r'$ have no roots in common, $r$ has $m-1$ distinct roots. I'm stuck here, since I don't know how to how to manipulate the derivatives. I'm not sure if proof by induction is the best approach here as well and I'd appreciate your help!","Problem: Suppose $p \in \mathcal{P}(\mathbf{C})$ has degree $m$. Prove that $p$ has $m$ distinct roots if and only if $p$ and its derivative $p'$ have no roots in common. My proof so far: If $m=0$, then $p(z)=a_0\neq 0$ and $p$ has no roots. Then $p'(z)=0$ and has no roots. If $m=1$, then $p(z)=a_0+a_1z$ with $a_1 \neq 0$ so $p$ has  exactly one root, namely $-a_0/a_1$ and $p'(z)=a_1$ and has no roots. In both cases, $p$ and $p'$ have no roots in common. Now suppose $m > 1$. We use induction on $m$, assuming that for every polynomial $r$ with $m-1$ distinct roots, $r$ and $r'$ have no roots in common. Let  $p$ be a polynomial of degree $m$ with distinct roots. There exists $q$ such that \begin{align*} p(z)=(z-\lambda)q(z) \end{align*} for all $z \in \mathbf{F}$. Since q has $m-1$ distinct roots, $q$ and $q'$ have no roots in common. By the chain rule,  \begin{align*} p'(z)=(z-\lambda)q'(z)+q(z) \end{align*} We know that $\lambda$ is not a root of $p'$ since $p'(\lambda)=(\lambda-\lambda)q'(z)+q(\lambda)=0+q(\lambda)\neq 0$ as lambda is not a root of $q$. All other roots of $p$ are roots of $q$. For these roots $\lambda_q$, $p'(\lambda_q)=(\lambda-\lambda_q)q'(\lambda_q)+q(\lambda_q)=(\lambda-\lambda_q)q'(\lambda_q) + 0 \neq 0$. Now suppose $m > 1$. We use induction on $m$, this time assuming that for every polynomial $r$ of degree $m-1$ such that $r$ and $r'$ have no roots in common, $r$ has $m-1$ distinct roots. I'm stuck here, since I don't know how to how to manipulate the derivatives. I'm not sure if proof by induction is the best approach here as well and I'd appreciate your help!",,"['linear-algebra', 'polynomials']"
53,Gaussian Matrix Integral,Gaussian Matrix Integral,,"I need your help to solve this exercise : Let $S$ be a symmetric Hermitian matrix $N\times N$ : $S=(s_{ij})$ with $s_{ij}=s_{ji}$. When $\langle s_{ij}s_{kl}\rangle\neq 0$ What $$\int Tr(S^{2n})\;d\mu(s)$$ counts? Calculate the moments of the matrix trace given by $$ \int Tr(S^2) dS,\quad \int Tr(S^4) dS $$ where S is a real symmetric matrix satisfying $S_{ij}=S_{ji}$. We are supposed to use The Wick Theorem to solve it. any idea on how to calculate this, thank you.","I need your help to solve this exercise : Let $S$ be a symmetric Hermitian matrix $N\times N$ : $S=(s_{ij})$ with $s_{ij}=s_{ji}$. When $\langle s_{ij}s_{kl}\rangle\neq 0$ What $$\int Tr(S^{2n})\;d\mu(s)$$ counts? Calculate the moments of the matrix trace given by $$ \int Tr(S^2) dS,\quad \int Tr(S^4) dS $$ where S is a real symmetric matrix satisfying $S_{ij}=S_{ji}$. We are supposed to use The Wick Theorem to solve it. any idea on how to calculate this, thank you.",,"['linear-algebra', 'integration', 'matrices', 'measure-theory', 'definite-integrals']"
54,"Calculate the determinant of the $2n \times 2n$ matrix with entries equal to zero on the main diagonal, $1$ below and $-1$ above [duplicate]","Calculate the determinant of the  matrix with entries equal to zero on the main diagonal,  below and  above [duplicate]",2n \times 2n 1 -1,"This question already has answers here : Determinant of a special skew-symmetric matrix (6 answers) Closed 10 years ago . Calculate the determinant of the $2n \times 2n$ matrix with entries equal to zero on the main diagonal, equal to $1$ below and equal to $-1$ above. I'll denote this matrix $A_{2n}$. So for example you have $A_{2} = \begin{bmatrix}         0 & -1  \\         1 & 0  \\               \end{bmatrix}  $ and $A_{4}= \begin{bmatrix}         0 & -1 & -1  &-1 \\         1 & 0  & -1  & -1\\         1 & 1  & 0   & -1 \\         1 & 1  & 1   & 0\\         \end{bmatrix} $ From trying it out by hand I think you have $\det(A_{2n})=1$ for all odd $n$ , or $=-1$ for all even $n$. Can anyone come up with some way to prove this? From doing these two examples my algorithm seems to be something like this: Switch the first and second rows (multiplies det by $-1$) If the matrix is now upper triangular, calculate the  determinant by product of main diagonal. If not, make a new pivot and clear that column below. Swap the third and fourth rows (multiplies det by $-1$). Repeat from 2.","This question already has answers here : Determinant of a special skew-symmetric matrix (6 answers) Closed 10 years ago . Calculate the determinant of the $2n \times 2n$ matrix with entries equal to zero on the main diagonal, equal to $1$ below and equal to $-1$ above. I'll denote this matrix $A_{2n}$. So for example you have $A_{2} = \begin{bmatrix}         0 & -1  \\         1 & 0  \\               \end{bmatrix}  $ and $A_{4}= \begin{bmatrix}         0 & -1 & -1  &-1 \\         1 & 0  & -1  & -1\\         1 & 1  & 0   & -1 \\         1 & 1  & 1   & 0\\         \end{bmatrix} $ From trying it out by hand I think you have $\det(A_{2n})=1$ for all odd $n$ , or $=-1$ for all even $n$. Can anyone come up with some way to prove this? From doing these two examples my algorithm seems to be something like this: Switch the first and second rows (multiplies det by $-1$) If the matrix is now upper triangular, calculate the  determinant by product of main diagonal. If not, make a new pivot and clear that column below. Swap the third and fourth rows (multiplies det by $-1$). Repeat from 2.",,"['linear-algebra', 'matrices', 'determinant']"
55,How to compute the similarity transformation matrix,How to compute the similarity transformation matrix,,"Stuck on this question: Let $$A=\begin{pmatrix} 2&1\\ -1&-1 \end{pmatrix}$$$$B=\begin{pmatrix} -2&5\\ -1&3 \end{pmatrix}$$$$C=\begin{pmatrix} 5&2\\ 4&1 \end{pmatrix}$$   Show that A is similar to B, but that A is not similar to C. I can do the second part of the question as $det(A)\neq\det(C)$, therefore as similar matrices have the same determinant $A\nsim C$. I also understand that I need to find $P$ such that $AP=PB$ for the first part but have no idea how I would go about finding it.Would anyone be able to provide an answer and explanation of the method used? Thanks Edit:Had a look at the solution from the textbook, it gives $$P=\begin{pmatrix} 2&3\\ 1&1 \end{pmatrix}$$","Stuck on this question: Let $$A=\begin{pmatrix} 2&1\\ -1&-1 \end{pmatrix}$$$$B=\begin{pmatrix} -2&5\\ -1&3 \end{pmatrix}$$$$C=\begin{pmatrix} 5&2\\ 4&1 \end{pmatrix}$$   Show that A is similar to B, but that A is not similar to C. I can do the second part of the question as $det(A)\neq\det(C)$, therefore as similar matrices have the same determinant $A\nsim C$. I also understand that I need to find $P$ such that $AP=PB$ for the first part but have no idea how I would go about finding it.Would anyone be able to provide an answer and explanation of the method used? Thanks Edit:Had a look at the solution from the textbook, it gives $$P=\begin{pmatrix} 2&3\\ 1&1 \end{pmatrix}$$",,['linear-algebra']
56,Bijection $f: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ preserves collinearity $\iff \ \ f(x)=Ax+b$,Bijection  preserves collinearity,f: \mathbb{R}^2 \rightarrow \mathbb{R}^2 \iff \ \ f(x)=Ax+b,"I don't know how to prove the following: Bijection $f: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ preserves collinearity $\iff \ \ f(x)=Ax+b$, where $A \in GL_2(\mathbb{R})$, $b$ is a fixed vector in $\mathbb{R}^2$. $\Leftarrow$ is easy, because such $f$ is an affine transformation of the plane and so it preserves collinearity (if $x,y,z \in \mathbb{R}^2$ are collinear, then $f(x), f(y), f(z)$ are also collinear). I have problems proving the reverse direction. Could you help me with that? Thank you.","I don't know how to prove the following: Bijection $f: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ preserves collinearity $\iff \ \ f(x)=Ax+b$, where $A \in GL_2(\mathbb{R})$, $b$ is a fixed vector in $\mathbb{R}^2$. $\Leftarrow$ is easy, because such $f$ is an affine transformation of the plane and so it preserves collinearity (if $x,y,z \in \mathbb{R}^2$ are collinear, then $f(x), f(y), f(z)$ are also collinear). I have problems proving the reverse direction. Could you help me with that? Thank you.",,"['linear-algebra', 'affine-geometry']"
57,Writing real invertible matrices as exponential of real matrices,Writing real invertible matrices as exponential of real matrices,,Every invertible square matrix with complex entries can be written as the exponential of a complex matrix. I wish to ask if it is true that Every invertible real matrix with positive determinant can be written as the exponential of a real matrix. (We need +ve determinant condition because if $A=e^X$ then $\det A=e^{\operatorname{tr}(X)} > 0$.) If not is there a simple characterization of such real matrices (with +ve determinant) which are exponentials of other matrices ?,Every invertible square matrix with complex entries can be written as the exponential of a complex matrix. I wish to ask if it is true that Every invertible real matrix with positive determinant can be written as the exponential of a real matrix. (We need +ve determinant condition because if $A=e^X$ then $\det A=e^{\operatorname{tr}(X)} > 0$.) If not is there a simple characterization of such real matrices (with +ve determinant) which are exponentials of other matrices ?,,"['linear-algebra', 'abstract-algebra', 'matrices']"
58,The second largest eigenvalue for Perron-Frobenius matrix,The second largest eigenvalue for Perron-Frobenius matrix,,"The Perron-Frobenius theorem is about the largest eigenvalue and eigenvector of a non-negative (irreducible) matrix. My question: Is there any estimation of the difference between the first and second largest eigenvalues, say an upper or a lower bound? An general theory may be tough, so please feel free to add other conditions to limit our discussion.","The Perron-Frobenius theorem is about the largest eigenvalue and eigenvector of a non-negative (irreducible) matrix. My question: Is there any estimation of the difference between the first and second largest eigenvalues, say an upper or a lower bound? An general theory may be tough, so please feel free to add other conditions to limit our discussion.",,"['linear-algebra', 'matrices', 'graph-theory', 'markov-chains']"
59,Prove that the only operator on $\mathbb{C}$ for which his inner product is zero is zero,Prove that the only operator on  for which his inner product is zero is zero,\mathbb{C},"How do I prove this statement: Let $V$ be an unitary vector space over $\Bbb C$, $(,)$ be an inner product on $V$ and $\Bbb A$ operator $V\rightarrow V$. Then $(\Bbb Av,v)=0$ if and only if $\Bbb A=0$. I know that on $\Bbb R$ this does not apply, since we are searching for an operator that assigns to my vector a vector that is orthogonal and so every orthogonal projection on a subspace of $\Bbb R$ can be this operator. But how is this possible on $\Bbb C$? Thank you.","How do I prove this statement: Let $V$ be an unitary vector space over $\Bbb C$, $(,)$ be an inner product on $V$ and $\Bbb A$ operator $V\rightarrow V$. Then $(\Bbb Av,v)=0$ if and only if $\Bbb A=0$. I know that on $\Bbb R$ this does not apply, since we are searching for an operator that assigns to my vector a vector that is orthogonal and so every orthogonal projection on a subspace of $\Bbb R$ can be this operator. But how is this possible on $\Bbb C$? Thank you.",,"['linear-algebra', 'operator-theory', 'inner-products']"
60,"Does $Ax=x$ imply $A^* x=x$, if $A^*$ is the conjugate transpose of $A$?","Does  imply , if  is the conjugate transpose of ?",Ax=x A^* x=x A^* A,"I have a fairly simple question. If $A$ is a matrix and $A^*$ denotes its conjugate transpose, is it true that if $Ax = x$ , then $A^*x = x$ ? The matrix $A^*$ will certainly have $1$ as an eigenvalue, but will it be with the same eigenvector? And if not, what is the relation between the eigenvector of $A$ and the one of $A^*$ ?","I have a fairly simple question. If is a matrix and denotes its conjugate transpose, is it true that if , then ? The matrix will certainly have as an eigenvalue, but will it be with the same eigenvector? And if not, what is the relation between the eigenvector of and the one of ?",A A^* Ax = x A^*x = x A^* 1 A A^*,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
61,Solving matrix equation $XA=AY$ with known $X$ and $Y$,Solving matrix equation  with known  and,XA=AY X Y,"I am having problem in solving set of matrix multiplication. There are three matrices $A,X$ and $Y$ , all are non-singular $2\times 2$ matrices. Where matrix $X$ and $Y$ are known and $A$ is unknown. $$ X = \begin{bmatrix} x_{11} & x_{12} \\ x_{21} & x_{22} \end{bmatrix} $$ $$ Y = \begin{bmatrix} y_{11} & y_{12} \\ y_{21} & y_{22} \end{bmatrix} $$ $$ A = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix} $$ and multiplication is as follows $$ X\cdot A=A\cdot Y $$ I expanded it and tried to solve it as such I would be able to get elements of matrix $A$ at the end but it end up in homogenous linear equation having trivial solution zero. That makes all calculation meaningless. $$ X\cdot A - A\cdot Y = 0 $$ What is the better way to compute it as such I can find result of matrix $A$ (in terms of elements of matrics $X$ and $Y$ ) at the end.","I am having problem in solving set of matrix multiplication. There are three matrices and , all are non-singular matrices. Where matrix and are known and is unknown. and multiplication is as follows I expanded it and tried to solve it as such I would be able to get elements of matrix at the end but it end up in homogenous linear equation having trivial solution zero. That makes all calculation meaningless. What is the better way to compute it as such I can find result of matrix (in terms of elements of matrics and ) at the end.","A,X Y 2\times 2 X Y A  X = \begin{bmatrix} x_{11} & x_{12} \\ x_{21} & x_{22} \end{bmatrix}   Y = \begin{bmatrix} y_{11} & y_{12} \\ y_{21} & y_{22} \end{bmatrix}   A = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix}   X\cdot A=A\cdot Y  A  X\cdot A - A\cdot Y = 0  A X Y","['linear-algebra', 'matrices', 'matrix-equations']"
62,What is the rank of the cofactor matrix of a given matrix?,What is the rank of the cofactor matrix of a given matrix?,,"Let $A = (a_{ij}) ∈M_n(\mathbb{R})$; $n≥3$. Let $B = (b_{ij})$ be the matrix of its co- factors, i.e. $b_{ij}$ is the cofactor of the entry $a_{ij}$ in $A$. What is the rank of $B$ when a. the rank of $A$ is $n$? b. the rank of $A$ is less than, or equal to, $n ≥ 2$? I  am completely stuck on it.how can i solve this.","Let $A = (a_{ij}) ∈M_n(\mathbb{R})$; $n≥3$. Let $B = (b_{ij})$ be the matrix of its co- factors, i.e. $b_{ij}$ is the cofactor of the entry $a_{ij}$ in $A$. What is the rank of $B$ when a. the rank of $A$ is $n$? b. the rank of $A$ is less than, or equal to, $n ≥ 2$? I  am completely stuck on it.how can i solve this.",,['linear-algebra']
63,Nilpotent matrix over a division algebra,Nilpotent matrix over a division algebra,,"Suppose I have an $n\times n$ nilpotent matrix $A$. If the entries are from any field, then I can show that all eigenvalues are zero and the trace is zero. Indeed, if we consider the algebraic closure of the field then the Jordan normal form $J$ of $A$ must be nilpotent, so all its eigenvalues are zero, which is of course in the original field itself. Also $\operatorname{tr}(J)$ is zero, and using the fact that $\operatorname{tr}(AB)=\operatorname{tr}(BA)$, it follows that $A$ (which is similar to $J$) must also have zero trace. But now I want to consider the case where the entries of $A$ are from a finite dimensional associative division algebra $D$ over a field $K$. If $K$ is algebraically closed then we are back in the case above since the only finite dimensional division algebra over an algebraically closed field is the field itself. But I'm having some difficulty with the case where $K$ is not necessarily algebraically closed - are the above still true? For simplicity let's assume that $K$ has characteristic $0$ but is not necessarily algebraically closed. The proof above (for a field) does not seem applicable in this case - at least I can't convince myself of it. I can't use the idea of algebraic closure, so I do not know if there exists any eigenvalues in $D$. Also, since commutativity does not in general hold in $D$, I do not know if the trace is invariant under a change of basis. The difficulty seems to be that I don't know what results continue to hold for a division algebra. Any ideas of a good way to think about this?","Suppose I have an $n\times n$ nilpotent matrix $A$. If the entries are from any field, then I can show that all eigenvalues are zero and the trace is zero. Indeed, if we consider the algebraic closure of the field then the Jordan normal form $J$ of $A$ must be nilpotent, so all its eigenvalues are zero, which is of course in the original field itself. Also $\operatorname{tr}(J)$ is zero, and using the fact that $\operatorname{tr}(AB)=\operatorname{tr}(BA)$, it follows that $A$ (which is similar to $J$) must also have zero trace. But now I want to consider the case where the entries of $A$ are from a finite dimensional associative division algebra $D$ over a field $K$. If $K$ is algebraically closed then we are back in the case above since the only finite dimensional division algebra over an algebraically closed field is the field itself. But I'm having some difficulty with the case where $K$ is not necessarily algebraically closed - are the above still true? For simplicity let's assume that $K$ has characteristic $0$ but is not necessarily algebraically closed. The proof above (for a field) does not seem applicable in this case - at least I can't convince myself of it. I can't use the idea of algebraic closure, so I do not know if there exists any eigenvalues in $D$. Also, since commutativity does not in general hold in $D$, I do not know if the trace is invariant under a change of basis. The difficulty seems to be that I don't know what results continue to hold for a division algebra. Any ideas of a good way to think about this?",,"['linear-algebra', 'abstract-algebra', 'division-algebras']"
64,Showing Unit sphere is convex,Showing Unit sphere is convex,,"Good evening guys! I have to show that the unit sphere represented by is convex. A set is said to be convex when $sx + (1 - s)y \in M$, where $x, y \in M$ and $s \in (0,1)$ I've read on wikipedia that this can be proven over the triangle inequality, but I think it can be solved in another way? Would this be enough as proof: For the unit sphere, we have to prove that $0 \leq sx + (1 - s)y \leq 1$ (because $||x||\leq 1$ therefore, $0 \leq x,y \leq 1$). Seeing as the maximal value that x and y can take are 1, the maximum the equation can achieve is 1 (when s=1,x=1 or s=0,y=1). The same can be shown for the minimum 0, therefore it is really between 0 and 1. Finished? Many thanks in advance!","Good evening guys! I have to show that the unit sphere represented by is convex. A set is said to be convex when $sx + (1 - s)y \in M$, where $x, y \in M$ and $s \in (0,1)$ I've read on wikipedia that this can be proven over the triangle inequality, but I think it can be solved in another way? Would this be enough as proof: For the unit sphere, we have to prove that $0 \leq sx + (1 - s)y \leq 1$ (because $||x||\leq 1$ therefore, $0 \leq x,y \leq 1$). Seeing as the maximal value that x and y can take are 1, the maximum the equation can achieve is 1 (when s=1,x=1 or s=0,y=1). The same can be shown for the minimum 0, therefore it is really between 0 and 1. Finished? Many thanks in advance!",,['linear-algebra']
65,$\beta_k$ for Conjugate Gradient Method,for Conjugate Gradient Method,\beta_k,"I followed the derivation for the Conjugate Gradient method from the documents shared below http://en.wikipedia.org/wiki/Conjugate_gradient_method http://www.idi.ntnu.no/~elster/tdt24/tdt24-f09/cg.pdf I understand almost all of this, except how $\beta_k$ is derived. From the Wiki derivation I understand we can get $$ r_{k+1} = r_k - \alpha_k A d_k$$ reworking this $$Ad_k = -\frac{1}{\alpha_k} (r_{k+1} - r_k)$$ And $\alpha_k$ is $$ \alpha_k = \frac{r_k^Tr_k}{d_k^TAd_k} $$ Now, somehow all derivations I see $\beta_k$ declared to be $$ \beta_k = \frac{r_{k+1}^Tr_{k+1}}{r_k^Tr_k} $$ and imply that it follows $$ d_{k+1} = r_{k+1} + \beta_k d_k$$ I was not able to verify this however, my algebra must have gone wrong somewhere. Also, it seems $d_k$ and $r_k$ are assumed to be the same thing, I explain that as first direction $p_0$ is the same as the residual $r_0$ and from there we always calculate the next orthogonal direction, so $d_k$ and $r_k$ remain equal. Any help would be appreciated,","I followed the derivation for the Conjugate Gradient method from the documents shared below http://en.wikipedia.org/wiki/Conjugate_gradient_method http://www.idi.ntnu.no/~elster/tdt24/tdt24-f09/cg.pdf I understand almost all of this, except how $\beta_k$ is derived. From the Wiki derivation I understand we can get $$ r_{k+1} = r_k - \alpha_k A d_k$$ reworking this $$Ad_k = -\frac{1}{\alpha_k} (r_{k+1} - r_k)$$ And $\alpha_k$ is $$ \alpha_k = \frac{r_k^Tr_k}{d_k^TAd_k} $$ Now, somehow all derivations I see $\beta_k$ declared to be $$ \beta_k = \frac{r_{k+1}^Tr_{k+1}}{r_k^Tr_k} $$ and imply that it follows $$ d_{k+1} = r_{k+1} + \beta_k d_k$$ I was not able to verify this however, my algebra must have gone wrong somewhere. Also, it seems $d_k$ and $r_k$ are assumed to be the same thing, I explain that as first direction $p_0$ is the same as the residual $r_0$ and from there we always calculate the next orthogonal direction, so $d_k$ and $r_k$ remain equal. Any help would be appreciated,",,"['linear-algebra', 'optimization', 'numerical-methods', 'numerical-optimization']"
66,Finding Boolean/Logical Expressions for truth tables in algebraic normal form(ANF),Finding Boolean/Logical Expressions for truth tables in algebraic normal form(ANF),,Karnaugh map is a method to simplify Boolean algebra expressions in two different notations sum-of-products( DNF ) and product-of-sums( CNF ). I need to find a minimized boolean expression in algebraic normal form (ANF) for a truth table. Is there a solution? A solution is simplifying the boolean algebra expression in CNF or DNF then converting it to ANF. But is it minimized? Is there a software to convert CNF boolean algebra expressions to ANF?,Karnaugh map is a method to simplify Boolean algebra expressions in two different notations sum-of-products( DNF ) and product-of-sums( CNF ). I need to find a minimized boolean expression in algebraic normal form (ANF) for a truth table. Is there a solution? A solution is simplifying the boolean algebra expression in CNF or DNF then converting it to ANF. But is it minimized? Is there a software to convert CNF boolean algebra expressions to ANF?,,"['linear-algebra', 'logic']"
67,How do I find the matrix M that transforms a vector A into the vector B?,How do I find the matrix M that transforms a vector A into the vector B?,,"How can I find the matrix that transforms a vector A to a vector B ? By that I mean, if I have a vector A(x, y, z), what is the matrix M that transforms it to the vector B(a, b, c) ? What I am trying to do is this; I have three vector A, B, C and I want to find the matrix M that transforms those vector to D, E, F (using only one matrix M).","How can I find the matrix that transforms a vector A to a vector B ? By that I mean, if I have a vector A(x, y, z), what is the matrix M that transforms it to the vector B(a, b, c) ? What I am trying to do is this; I have three vector A, B, C and I want to find the matrix M that transforms those vector to D, E, F (using only one matrix M).",,"['linear-algebra', 'matrices']"
68,Duality with a stochastic matrix,Duality with a stochastic matrix,,"If I have a stochastic matrix $X$- the sum of each row is equal to $1$ and all elements are non-negative. Given this property, how can I show that: $x'X=x'$ ,  $x\geq 0$ Has a non-zero solution? I'm assuming this has something to do with proving a feasible dual, but not may be wrong..","If I have a stochastic matrix $X$- the sum of each row is equal to $1$ and all elements are non-negative. Given this property, how can I show that: $x'X=x'$ ,  $x\geq 0$ Has a non-zero solution? I'm assuming this has something to do with proving a feasible dual, but not may be wrong..",,"['linear-algebra', 'stochastic-matrices']"
69,Eigenvalue of diagonally dominant matrices,Eigenvalue of diagonally dominant matrices,,"I want to ask a question about eigenvalue  of diagonally dominant matrices. The question is : Assume $A=(a_{ij})_{n\times n}\in M_n(\mathbb{R})$ and $\lambda_1,\lambda_2, \cdots,\lambda_n$ are the eigenvalues of $A$ . $A$ satisfies: $(1):$ $a_{ii}>0~~,i=1,2,\cdots,n$ . $(2):$ $a_{ij}<0~~,i\ne j$ . $(3):$ $a_{ii}>-\sum_{i\ne j}a_{ij}~~,i=1,2,\cdots,n$ . I would like to ask if there is $\mathrm{Re}(\lambda_i)\ge\vert \mathrm{Im}(\lambda_i)\vert,~~ i=1,2,\cdots,n$ ? I think it might have something to do with the estimation of the eigenvalues of the diagonal dominance matrix. Any help or suggestions are appreciated. Maybe it is not true and I can't find an counterexample. Thanks!",I want to ask a question about eigenvalue  of diagonally dominant matrices. The question is : Assume and are the eigenvalues of . satisfies: . . . I would like to ask if there is ? I think it might have something to do with the estimation of the eigenvalues of the diagonal dominance matrix. Any help or suggestions are appreciated. Maybe it is not true and I can't find an counterexample. Thanks!,"A=(a_{ij})_{n\times n}\in M_n(\mathbb{R}) \lambda_1,\lambda_2,
\cdots,\lambda_n A A (1): a_{ii}>0~~,i=1,2,\cdots,n (2): a_{ij}<0~~,i\ne j (3): a_{ii}>-\sum_{i\ne j}a_{ij}~~,i=1,2,\cdots,n \mathrm{Re}(\lambda_i)\ge\vert \mathrm{Im}(\lambda_i)\vert,~~ i=1,2,\cdots,n","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'numerical-linear-algebra']"
70,Will this determinant of the matrix of determinants of the transformations of the group applied to a square matrix always be zero?,Will this determinant of the matrix of determinants of the transformations of the group applied to a square matrix always be zero?,,"Background Let $A = \left[ \begin{matrix} a & b \\ c & d  \end{matrix} \right]$ be a matrix in $\mathbb{R}^{2 \times 2}$ . While matrices are often used to represent a variety of linear transformations, including rotations, here I am transforming the matrix itself. A classic way to introduce group transformations is via the following 8 transformations of a square with distinct corners: Rotation of 0 degrees: $R_0(A)=\left[\begin{matrix}a & b\\c & d\end{matrix}\right]$ Rotation of 90 degrees: $R_{90}(A)=\left[\begin{matrix}b & d\\a & c\end{matrix}\right]$ Rotation of 180 degrees: $R_{180}(A)=\left[\begin{matrix}d & c\\b & a\end{matrix}\right]$ Rotation of 270 degrees: $R_{270}(A)=\left[\begin{matrix}c & a\\d & b\end{matrix}\right]$ Flipping about a horizontal axis: $H(A)=\left[\begin{matrix}c & d\\a & b\end{matrix}\right]$ Flipping about a vertical axis: $V(A)=\left[\begin{matrix}b & a\\d & c\end{matrix}\right]$ Flipping about the main diagonal: $D(A)=\left[\begin{matrix}a & c\\b & d\end{matrix}\right]$ Flipping about the other diagonal: $D^\prime(A)= \left[\begin{matrix}d & b\\c & a\end{matrix}\right]$ Considering the compositions of all pairs of operations yields an 8 by 8 Cayley table. Thinking of my matrix $A$ as the square to be rotated, I created a similar table except the entries are the determinants of the resulting matrices after applying a composition of two of the above operations which I have represented with matrix $B$ below. The order of the rows and columns follow the order of the transformations as they are listed above. $$B = \left[\begin{matrix}a d - b c & - a d + b c & a d - b c & - a d + b c & - a d + b c & - a d + b c & a d - b c & a d - b c\\- a d + b c & a d - b c & - a d + b c & a d - b c & a d - b c & a d - b c & - a d + b c & a d - b c\\a d - b c & - a d + b c & a d - b c & - a d + b c & - a d + b c & - a d + b c & a d - b c & a d - b c\\- a d + b c & a d - b c & - a d + b c & a d - b c & a d - b c & a d - b c & - a d + b c & a d - b c\\- a d + b c & a d - b c & - a d + b c & a d - b c & a d - b c & a d - b c & - a d + b c & a d - b c\\- a d + b c & a d - b c & - a d + b c & a d - b c & a d - b c & a d - b c & - a d + b c & a d - b c\\a d - b c & - a d + b c & a d - b c & - a d + b c & - a d + b c & - a d + b c & a d - b c & a d - b c\\a d - b c & - a d + b c & a d - b c & - a d + b c & - a d + b c & - a d + b c & a d - b c & a d - b c\end{matrix}\right]$$ Finally I computed the determinant of $B$ to be $\det (B) = 0$ . Question Just as I started with 2 by 2 matrices and got a final ""determinant of the matrix of determinants of the transformations of the group applied to the original matrix"", will I also such a determinant of zero for any n by n matrix? Edits 1 Starting with $A = \left[\begin{matrix}x_{0} & x_{1} & x_{2}\\x_{3} & x_{4} & x_{5}\\x_{6} & x_{7} & x_{8}\end{matrix}\right]$ , I found that the matrix of determinants was $$B = \left[\begin{matrix}x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6}\\- x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6}\\x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6}\\- x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6}\\- x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6}\\- x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6}\\x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6}\\x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6}\end{matrix}\right]$$ And I similarly found $\det(B) = 0$ as was found in the 2 by 2 case. 2 I found the resulting determinant be zero in the 4 by 4 and 5 by 5 cases. Note that the transformations above are rotating/reflecting the matrix entries themselves, rather than vectors in a vector space.","Background Let be a matrix in . While matrices are often used to represent a variety of linear transformations, including rotations, here I am transforming the matrix itself. A classic way to introduce group transformations is via the following 8 transformations of a square with distinct corners: Rotation of 0 degrees: Rotation of 90 degrees: Rotation of 180 degrees: Rotation of 270 degrees: Flipping about a horizontal axis: Flipping about a vertical axis: Flipping about the main diagonal: Flipping about the other diagonal: Considering the compositions of all pairs of operations yields an 8 by 8 Cayley table. Thinking of my matrix as the square to be rotated, I created a similar table except the entries are the determinants of the resulting matrices after applying a composition of two of the above operations which I have represented with matrix below. The order of the rows and columns follow the order of the transformations as they are listed above. Finally I computed the determinant of to be . Question Just as I started with 2 by 2 matrices and got a final ""determinant of the matrix of determinants of the transformations of the group applied to the original matrix"", will I also such a determinant of zero for any n by n matrix? Edits 1 Starting with , I found that the matrix of determinants was And I similarly found as was found in the 2 by 2 case. 2 I found the resulting determinant be zero in the 4 by 4 and 5 by 5 cases. Note that the transformations above are rotating/reflecting the matrix entries themselves, rather than vectors in a vector space.",A = \left[ \begin{matrix} a & b \\ c & d  \end{matrix} \right] \mathbb{R}^{2 \times 2} R_0(A)=\left[\begin{matrix}a & b\\c & d\end{matrix}\right] R_{90}(A)=\left[\begin{matrix}b & d\\a & c\end{matrix}\right] R_{180}(A)=\left[\begin{matrix}d & c\\b & a\end{matrix}\right] R_{270}(A)=\left[\begin{matrix}c & a\\d & b\end{matrix}\right] H(A)=\left[\begin{matrix}c & d\\a & b\end{matrix}\right] V(A)=\left[\begin{matrix}b & a\\d & c\end{matrix}\right] D(A)=\left[\begin{matrix}a & c\\b & d\end{matrix}\right] D^\prime(A)= \left[\begin{matrix}d & b\\c & a\end{matrix}\right] A B B = \left[\begin{matrix}a d - b c & - a d + b c & a d - b c & - a d + b c & - a d + b c & - a d + b c & a d - b c & a d - b c\\- a d + b c & a d - b c & - a d + b c & a d - b c & a d - b c & a d - b c & - a d + b c & a d - b c\\a d - b c & - a d + b c & a d - b c & - a d + b c & - a d + b c & - a d + b c & a d - b c & a d - b c\\- a d + b c & a d - b c & - a d + b c & a d - b c & a d - b c & a d - b c & - a d + b c & a d - b c\\- a d + b c & a d - b c & - a d + b c & a d - b c & a d - b c & a d - b c & - a d + b c & a d - b c\\- a d + b c & a d - b c & - a d + b c & a d - b c & a d - b c & a d - b c & - a d + b c & a d - b c\\a d - b c & - a d + b c & a d - b c & - a d + b c & - a d + b c & - a d + b c & a d - b c & a d - b c\\a d - b c & - a d + b c & a d - b c & - a d + b c & - a d + b c & - a d + b c & a d - b c & a d - b c\end{matrix}\right] B \det (B) = 0 A = \left[\begin{matrix}x_{0} & x_{1} & x_{2}\\x_{3} & x_{4} & x_{5}\\x_{6} & x_{7} & x_{8}\end{matrix}\right] B = \left[\begin{matrix}x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6}\\- x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6}\\x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6}\\- x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6}\\- x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6}\\- x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6}\\x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6}\\x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & - x_{0} x_{4} x_{8} + x_{0} x_{5} x_{7} + x_{1} x_{3} x_{8} - x_{1} x_{5} x_{6} - x_{2} x_{3} x_{7} + x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6} & x_{0} x_{4} x_{8} - x_{0} x_{5} x_{7} - x_{1} x_{3} x_{8} + x_{1} x_{5} x_{6} + x_{2} x_{3} x_{7} - x_{2} x_{4} x_{6}\end{matrix}\right] \det(B) = 0,"['linear-algebra', 'matrices', 'group-theory', 'determinant', 'cayley-table']"
71,Does Frobenius norm (not operator 2 norm) preserve the positive semidefinite order of matrices?,Does Frobenius norm (not operator 2 norm) preserve the positive semidefinite order of matrices?,,"Let $A, B$ be two real symmetric positive semidefinite $n\times n$ matrices (these conditions might be unnecessary). We say $A\le B$ if $B-A$ is positive semidefinite. If $A\le B$ , do we necessarily have $\|A\|_F \le \|B\|_F$ , where the norm is Frobenius norm? Note that for a general matrix $A$ , $\|A\|_F^2=\text{Trace} (AA^T)$ . So we are really asking whether $\text{Trace} (AA^T) \le \text{Trace} (BB^T)$ . I have tried a couple of examples and believe this is true but can't find a proof. the proof posted below is about the operator 2 norm I believe Symmetric positive semi-definite matrices and norm inequalities","Let be two real symmetric positive semidefinite matrices (these conditions might be unnecessary). We say if is positive semidefinite. If , do we necessarily have , where the norm is Frobenius norm? Note that for a general matrix , . So we are really asking whether . I have tried a couple of examples and believe this is true but can't find a proof. the proof posted below is about the operator 2 norm I believe Symmetric positive semi-definite matrices and norm inequalities","A, B n\times n A\le B B-A A\le B \|A\|_F \le \|B\|_F A \|A\|_F^2=\text{Trace} (AA^T) \text{Trace} (AA^T) \le \text{Trace} (BB^T)","['linear-algebra', 'matrices']"
72,Pattern with $\begin{bmatrix}0&1\\-1&3\end{bmatrix}^k$. How can I prove what corner element will be?,Pattern with . How can I prove what corner element will be?,\begin{bmatrix}0&1\\-1&3\end{bmatrix}^k,"The matrix $$B= \begin{bmatrix}0&1\\-1&3\end{bmatrix}$$ Is inspired by this matrix which generates the Fibonacci sequence . Upon calculating a few powers of $B$ , I noticed that $({B^k})_{2,2}$ seems to be ${F_{2(k+1)}} - {F_{2k}}$ but I lack an idea of how to prove it. What would be a fruitful approach to do it?","The matrix Is inspired by this matrix which generates the Fibonacci sequence . Upon calculating a few powers of , I noticed that seems to be but I lack an idea of how to prove it. What would be a fruitful approach to do it?","B= \begin{bmatrix}0&1\\-1&3\end{bmatrix} B ({B^k})_{2,2} {F_{2(k+1)}} - {F_{2k}}","['linear-algebra', 'matrices', 'soft-question', 'integers', 'fibonacci-numbers']"
73,Vector $x$ Solves Least Squares Problem $\iff$ $b - Ax \in \text{Null}(A^*)$,Vector  Solves Least Squares Problem,x \iff b - Ax \in \text{Null}(A^*),"I am preparing for a graduate exam in Numerical Linear Algebra and I have to solve the following question (without calculus, and without any other theorems about Least Squares): Let $A \in \mathbb{C}^{m \times n}$ with $n < m$ (not necessarily of full rank). For any $x \in \mathbb{C}^n$ let $r(x) = b - Ax$ . Show that $x$ solves the least squares problem $\min_{x \in \mathbb{R}^n}||b - Ax||_2$ if and only if $r(x) \in \text{Null}(A^*).$ Below, I wrote down my attempt. Not only is it incomplete (it purports to prove just one direction of the equivalence), but later I realized that it contains a major flaw. I would immensely appreciate your help  in either fixing this solution, or finding a new one. I have very little time on the exam, so as a bonus, I would be very thankful if you could tell me if you see bad habits in my mathematical writing which make the solution too long. P.S. I have an inkling that a correct solution might involve a QR decomposition, but I'm not sure. Flawed Solution: Lemma: The equation $A^*Ap = A^*b$ has a solution. Proof: We are trying to show that $A^*b \in \text{Col}(A^*A) = \text{Null}(A^*A)^{\perp}$ . Take any $y \in \text{Null}(A^*A).$ We will show that $A^*b \perp y$ . But first, we will show $Ay = 0$ . Indeed, $$y \in \text{Null}(A^*A) \implies A^*Ay = 0 \implies y^*A^*Ay = 0 \implies ||Ay|| = 0 \implies Ay = 0.$$ Now, we show perpendicularity. We have $$(A^*b)^*y = b^*(Ay) = b^*0 = 0.$$ So $A^*b \in \text{Col}(A^*A)$ . Q.E.D. We proceed to the main proof. Let $p$ be a solution of $A^*Ap = A^*b$ . Then $A^*b - A^*Ap = 0$ . We claim $r(p) \perp \text{Col(A)}.$ Indeed, for any $Az \in \text{Col}(A)$ , we have $$(Az)^*(b - Ap) = z^*(A^*b - A^*Ap ) = 0.$$ Now, by the Pythagorean Theorem, $$||b - Ay||^2 = ||b - Ap + Ap - Ay||^2 = ||b - Ap||^2 + ||Ap - Ay||^2 \ge ||b - Ap||^2$$ with equality $\iff Ay = Ap$ . $\color{red}{\text{The flaw is here. I need to bound the function } ||b - Ax||}$ $\color{red}{\text{ by a constant, not by a function of p.}}$ Now suppose $x$ is a vector that solves the L.S. problem. Then $Ax = Ay$ , so $r(x) = b - Ax = b - Ap = r(p)$ and $r(p) \in \text{Null}(A^*)$ because $A^*Ap = A^*b \implies A^*(b - Ap) = 0$ . Now for the converse, suppose that $A^*Ax = A^*b$ . I don't yet know how to show that $Ax = Ap$ .","I am preparing for a graduate exam in Numerical Linear Algebra and I have to solve the following question (without calculus, and without any other theorems about Least Squares): Let with (not necessarily of full rank). For any let . Show that solves the least squares problem if and only if Below, I wrote down my attempt. Not only is it incomplete (it purports to prove just one direction of the equivalence), but later I realized that it contains a major flaw. I would immensely appreciate your help  in either fixing this solution, or finding a new one. I have very little time on the exam, so as a bonus, I would be very thankful if you could tell me if you see bad habits in my mathematical writing which make the solution too long. P.S. I have an inkling that a correct solution might involve a QR decomposition, but I'm not sure. Flawed Solution: Lemma: The equation has a solution. Proof: We are trying to show that . Take any We will show that . But first, we will show . Indeed, Now, we show perpendicularity. We have So . Q.E.D. We proceed to the main proof. Let be a solution of . Then . We claim Indeed, for any , we have Now, by the Pythagorean Theorem, with equality . Now suppose is a vector that solves the L.S. problem. Then , so and because . Now for the converse, suppose that . I don't yet know how to show that .","A \in \mathbb{C}^{m \times n} n < m x \in \mathbb{C}^n r(x) = b - Ax x \min_{x \in \mathbb{R}^n}||b - Ax||_2 r(x) \in \text{Null}(A^*). A^*Ap = A^*b A^*b \in \text{Col}(A^*A) = \text{Null}(A^*A)^{\perp} y \in \text{Null}(A^*A). A^*b \perp y Ay = 0 y \in \text{Null}(A^*A) \implies A^*Ay = 0 \implies y^*A^*Ay = 0 \implies ||Ay|| = 0 \implies Ay = 0. (A^*b)^*y = b^*(Ay) = b^*0 = 0. A^*b \in \text{Col}(A^*A) p A^*Ap = A^*b A^*b - A^*Ap = 0 r(p) \perp \text{Col(A)}. Az \in \text{Col}(A) (Az)^*(b - Ap) = z^*(A^*b - A^*Ap ) = 0. ||b - Ay||^2 = ||b - Ap + Ap - Ay||^2 = ||b - Ap||^2 + ||Ap - Ay||^2 \ge ||b - Ap||^2 \iff Ay = Ap \color{red}{\text{The flaw is here. I need to bound the function } ||b - Ax||} \color{red}{\text{ by a constant, not by a function of p.}} x Ax = Ay r(x) = b - Ax = b - Ap = r(p) r(p) \in \text{Null}(A^*) A^*Ap = A^*b \implies A^*(b - Ap) = 0 A^*Ax = A^*b Ax = Ap","['linear-algebra', 'abstract-algebra', 'matrices', 'numerical-methods', 'numerical-linear-algebra']"
74,On the definition of a tensor in the study of manifolds,On the definition of a tensor in the study of manifolds,,"In the book An introduction to manifolds by Tu, Loring W, it defines the $k$ -tensor as follows Denote by $V^{k}=V \times \cdots \times V$ the Cartesian product of $k$ copies of a real vector space $V $ .  A function $f: V^{k} \rightarrow \mathbb{R}$ is $ k  $ -linear if it is linear in each of its $ k $ arguments: $$ f(\ldots, a v+b w, \ldots)=a f(\ldots, v, \ldots)+b f(\ldots, w, \ldots) $$ for all $ a, b \in \mathbb{R} $ and $ v, w \in V $ .  Instead of $2$ -linear and $3 $ -linear, it is customary to say ""bilinear"" and ""trilinear."" A $k $ -linear function on $V$ is also called a $ k  $ -tensor on $ V $ . We will denote the vector space of all $ k  $ -tensors on $ V $ by $ L_{k}(V) $ .  If $f $ is a $ k $ -tensor on $ V $ , we also call $ k $ the degree of $ f $ . However in Wikipedia , it defines the $(p,q)$ -tensor as follows In this approach, a type $(p,q)$ tensor $T$ is defined as a multilinear map, $$ T: \underbrace{ V^* \times\dots\times V^*}_{p \text{ copies}} \times \underbrace{ V \times\dots\times V}_{q \text{ copies}} \rightarrow \mathbf{R}, $$ where $V^*$ is the corresponding dual space of covectors, which is linear in each of its arguments. It is obvious that the definition of a $k$ -tensor in An introduction to manifolds is just the $(0,q)$ -tensors in the definition of Wikipedia . My question is , why do not we define the tensors to be $(p,q)$ -tensor in Tu, Loring W? EDIT: I have noticed that there are no $(p,q)$ -tensors in the whole book of Loring Tu. Is it because that the elements from dual space will never be used in tensors in manifolds in the following study (so $p$ always equals to $0$ )? If not, where do we use $(p,q)$ -tensors in the study of manifolds ? (maybe it will appear in tensor fields?) I would really appreciate it if anyone could tell me the connections of these things.","In the book An introduction to manifolds by Tu, Loring W, it defines the -tensor as follows Denote by the Cartesian product of copies of a real vector space .  A function is -linear if it is linear in each of its arguments: for all and .  Instead of -linear and -linear, it is customary to say ""bilinear"" and ""trilinear."" A -linear function on is also called a -tensor on . We will denote the vector space of all -tensors on by .  If is a -tensor on , we also call the degree of . However in Wikipedia , it defines the -tensor as follows In this approach, a type tensor is defined as a multilinear map, where is the corresponding dual space of covectors, which is linear in each of its arguments. It is obvious that the definition of a -tensor in An introduction to manifolds is just the -tensors in the definition of Wikipedia . My question is , why do not we define the tensors to be -tensor in Tu, Loring W? EDIT: I have noticed that there are no -tensors in the whole book of Loring Tu. Is it because that the elements from dual space will never be used in tensors in manifolds in the following study (so always equals to )? If not, where do we use -tensors in the study of manifolds ? (maybe it will appear in tensor fields?) I would really appreciate it if anyone could tell me the connections of these things.","k V^{k}=V \times \cdots \times V k V  f: V^{k} \rightarrow \mathbb{R}  k    k  
f(\ldots, a v+b w, \ldots)=a f(\ldots, v, \ldots)+b f(\ldots, w, \ldots)
  a, b \in \mathbb{R}   v, w \in V  2 3  k  V  k    V   k    V   L_{k}(V)  f   k   V   k   f  (p,q) (p,q) T 
T: \underbrace{ V^* \times\dots\times V^*}_{p \text{ copies}} \times \underbrace{ V \times\dots\times V}_{q \text{ copies}} \rightarrow \mathbf{R},
 V^* k (0,q) (p,q) (p,q) p 0 (p,q)","['linear-algebra', 'differential-geometry', 'manifolds', 'tensor-products']"
75,"The matrix $B = \left(\begin{smallmatrix}A& 0\\A & A\end{smallmatrix}\right)$ is diagonalizable, if only if $A$ is diagonalizable.","The matrix  is diagonalizable, if only if  is diagonalizable.",B = \left(\begin{smallmatrix}A& 0\\A & A\end{smallmatrix}\right) A,"Let $A\in\mathcal {M}_{n}(\mathbb {C}), $ I am trying to prove that: The matrix $B = \left(\begin{smallmatrix}A& 0\\A & A\end{smallmatrix}\right)$ is diagonalizable, if only if $A$ is diagonalizable. My effrot : $(\Rightarrow) $ we suppose that $B $ is diagonalizable then $m_B$ is a product of distinct linear polynomials.  Let $m_A(x)$ , $m_B(x)$ ,  be the minimal polynomials of $A$ , and $B$ , respectively. We have $$ 0 = m_B(B) = \pmatrix{m_B(A) & 0\\(*)&m_B(A)} $$ so, $m_B(A) = 0$ .  It follows that $m_A$ divides $m_B$ , thus $m_A $ is a product of distinct linear polynomials. Then $A $ is diagonalisable. For $(\Leftarrow )$ I have no idea An idea please.","Let I am trying to prove that: The matrix is diagonalizable, if only if is diagonalizable. My effrot : we suppose that is diagonalizable then is a product of distinct linear polynomials.  Let , ,  be the minimal polynomials of , and , respectively. We have so, .  It follows that divides , thus is a product of distinct linear polynomials. Then is diagonalisable. For I have no idea An idea please.","A\in\mathcal {M}_{n}(\mathbb {C}),  B = \left(\begin{smallmatrix}A& 0\\A & A\end{smallmatrix}\right) A (\Rightarrow)  B  m_B m_A(x) m_B(x) A B 
0 = m_B(B) = \pmatrix{m_B(A) & 0\\(*)&m_B(A)}
 m_B(A) = 0 m_A m_B m_A  A  (\Leftarrow )","['linear-algebra', 'matrices', 'matrix-calculus']"
76,"Solve $x^5 - 1 = y^2$ for integer values of $(x,y)$ .",Solve  for integer values of  .,"x^5 - 1 = y^2 (x,y)","Solve $x^5 - 1 = y^2$ for integer values of $(x,y)$ . What I Tried :- I see that $x^5 = y^2 + 1$ , from here I can  conclude that $x$ has to be positive , because if $x \leq 0$ , then $x^5 \leq 0$ , but $y^2 + 1 > 0$ . Also I thought that maybe $(x^\frac{5}{2} + 1)(x^\frac{5}{2} - 1) = 1$ would do the trick [which would have implied that both the terms are $(1,1)$ or $(-1,-1)$ ], but that dosen't necessarily mean that $x^\frac{5}{2}$ is an integer . Then I see that :- $x^5 - 1 = y^2$ $\rightarrow (x-1)(x^4 + x^3 + x^2 + x + 1) = y^2$ From here the only idea is that $x \neq 2$ , because if it is then it can't be a perfect square (it's a foolish idea though) . But I tried playing this problem with many ways , but I am stuck at the same place . Can anyone help ? Note :- It's given that answer is only $(1,0)$ , but how is it coming?","Solve for integer values of . What I Tried :- I see that , from here I can  conclude that has to be positive , because if , then , but . Also I thought that maybe would do the trick [which would have implied that both the terms are or ], but that dosen't necessarily mean that is an integer . Then I see that :- From here the only idea is that , because if it is then it can't be a perfect square (it's a foolish idea though) . But I tried playing this problem with many ways , but I am stuck at the same place . Can anyone help ? Note :- It's given that answer is only , but how is it coming?","x^5 - 1 = y^2 (x,y) x^5 = y^2 + 1 x x \leq 0 x^5 \leq 0 y^2 + 1 > 0 (x^\frac{5}{2} + 1)(x^\frac{5}{2} - 1) = 1 (1,1) (-1,-1) x^\frac{5}{2} x^5 - 1 = y^2 \rightarrow (x-1)(x^4 + x^3 + x^2 + x + 1) = y^2 x \neq 2 (1,0)","['linear-algebra', 'complex-numbers', 'problem-solving']"
77,Limit of operators,Limit of operators,,"Let $T:V\to V$ be a bounded linear operator on a finite vector space $V$ . If the sequence $\frac{1}{n}T^n$ converges, can we prove that its limit is the zero operator? I think that the answer is yes, but I am struggling a bit with the proof. One approach could be to prove that $\|T\|\leq 1$ but I don't know how to proceed. One could also play around with the sequence terms by setting $S_n=\frac{1}{n}T^n$ , $S_0:=\lim_{n\to+\infty}S_n$ and observing that $S_{n+1}=\frac{n}{n+1}TS_n$ which gives $S_0=TS_0$ but I don't know if it is helpful.","Let be a bounded linear operator on a finite vector space . If the sequence converges, can we prove that its limit is the zero operator? I think that the answer is yes, but I am struggling a bit with the proof. One approach could be to prove that but I don't know how to proceed. One could also play around with the sequence terms by setting , and observing that which gives but I don't know if it is helpful.",T:V\to V V \frac{1}{n}T^n \|T\|\leq 1 S_n=\frac{1}{n}T^n S_0:=\lim_{n\to+\infty}S_n S_{n+1}=\frac{n}{n+1}TS_n S_0=TS_0,"['linear-algebra', 'operator-theory']"
78,Invertibility of combinatorial matrix,Invertibility of combinatorial matrix,,"Fix integers $n$ and $k$ with $0 \leq k < \dfrac{n}{2}$ . Let $I$ be the set of all $k$ -element subsets of $\{1,\ldots,n\}$ . Consider the matrix $A=(a_{ij})_{i,j\in I}$ where $a_{ij}=1$ if $i\cap j=\emptyset$ and $a_{ij}=0$ otherwise. What is the easiest way to see that $\det(A)\neq 0$ ? For example, for $n = 3$ and $k = 1$ , we have $\det(A) = 2$ . More generally, for $k = 1$ , we have $\det(A) = \pm (n-1)$ (since $A$ is the all-ones matrix minus the identity matrix).","Fix integers and with . Let be the set of all -element subsets of . Consider the matrix where if and otherwise. What is the easiest way to see that ? For example, for and , we have . More generally, for , we have (since is the all-ones matrix minus the identity matrix).","n k 0 \leq k < \dfrac{n}{2} I k \{1,\ldots,n\} A=(a_{ij})_{i,j\in I} a_{ij}=1 i\cap j=\emptyset a_{ij}=0 \det(A)\neq 0 n = 3 k = 1 \det(A) = 2 k = 1 \det(A) = \pm (n-1) A","['linear-algebra', 'combinatorics', 'matrices']"
79,Index of a subgroup in $SL_2(\mathbb{Z})$,Index of a subgroup in,SL_2(\mathbb{Z}),"Let $SL_2(\mathbb{Z})$ denote the group (under usual matrix multiplication) of $2\times2$ matrices with integer entries and determinant $1$ . Let $H$ be the subgroup of $SL_2(\mathbb{Z})$ consisting of those matrices such that the diagonal entries are all equivalent to $1 \pmod 3$ and the off-diagonal entries are all divisible by $3$ . What is the index of $H$ in $SL_2(\mathbb{Z})$ ? There are a total of $3^4=81$ different equivalence classes of matrices in $SL_2(\mathbb{Z})$ modulo $3$ (each of the entries can have $0,1,2$ as remainders). Now, the given condition implies only one of the possible $81$ combinations modulo $3$ . How do we proceed? any hints? Thanks beforehand.","Let denote the group (under usual matrix multiplication) of matrices with integer entries and determinant . Let be the subgroup of consisting of those matrices such that the diagonal entries are all equivalent to and the off-diagonal entries are all divisible by . What is the index of in ? There are a total of different equivalence classes of matrices in modulo (each of the entries can have as remainders). Now, the given condition implies only one of the possible combinations modulo . How do we proceed? any hints? Thanks beforehand.","SL_2(\mathbb{Z}) 2\times2 1 H SL_2(\mathbb{Z}) 1 \pmod 3 3 H SL_2(\mathbb{Z}) 3^4=81 SL_2(\mathbb{Z}) 3 0,1,2 81 3","['linear-algebra', 'matrices', 'group-theory', 'normal-subgroups']"
80,"From the collection of all permutation matrices of size $10\times10$, one such matrix is randomly picked. What is the expected value of its trace?","From the collection of all permutation matrices of size , one such matrix is randomly picked. What is the expected value of its trace?",10\times10,"From the collection of all permutation matrices of size $10\times10$ , one such matrix is randomly picked. What is the expected value of its trace? (A permutation matrix is one that has precisely one non-zero entry in each column and in each row, that non-zero entry being 1.) I know that possible options for traces are $0,1,2,3,4,5,6,7,8,9,10$ . Now from this how to find the expectation of the trace?","From the collection of all permutation matrices of size , one such matrix is randomly picked. What is the expected value of its trace? (A permutation matrix is one that has precisely one non-zero entry in each column and in each row, that non-zero entry being 1.) I know that possible options for traces are . Now from this how to find the expectation of the trace?","10\times10 0,1,2,3,4,5,6,7,8,9,10","['linear-algebra', 'matrices', 'trace', 'permutation-matrices']"
81,Difference between Adjoint of a matrix and its transpose,Difference between Adjoint of a matrix and its transpose,,"For simplicity let $T:R^{n}\rightarrow R^{n},$ be a linear operator and $[A]_T$ be the matrix of  operator $T.$ Then matrix of adjoint $T^{\times}$ operator of $T$ is given as $$[A]_{T^{\times}}=[A]_{T}^t$$ where $t$ denotes transpose of a matrix. My Question is that then from above equality the notions of transpose of a matrix and adjoint of a matrix are same, they why we use separate names?? while in some textbooks adjoint of a matrix is referred to the transpose of coefficient matrix.","For simplicity let be a linear operator and be the matrix of  operator Then matrix of adjoint operator of is given as where denotes transpose of a matrix. My Question is that then from above equality the notions of transpose of a matrix and adjoint of a matrix are same, they why we use separate names?? while in some textbooks adjoint of a matrix is referred to the transpose of coefficient matrix.","T:R^{n}\rightarrow R^{n}, [A]_T T. T^{\times} T [A]_{T^{\times}}=[A]_{T}^t t","['linear-algebra', 'linear-transformations', 'transpose']"
82,Vector in 9 dimensions always has a solution?,Vector in 9 dimensions always has a solution?,,"I was watching Professor Gilbert Strang's lecture on Linear Algebra, and in the video, he talks about when we can or cannot say that linear combinations of vectors span the entire region in that dimension. He said that if we had nine vectors, each of dimension 9,  and if the vectors were all random, then the answer is probably yes. But if it is the case that two of the nine vectors are the same, then they don't add anything new to the equation, and what we will get is probably an 8-dimensional plane. I don't understand how this is the case. Also, could someone explain what this means in terms of the number of solutions we will get?","I was watching Professor Gilbert Strang's lecture on Linear Algebra, and in the video, he talks about when we can or cannot say that linear combinations of vectors span the entire region in that dimension. He said that if we had nine vectors, each of dimension 9,  and if the vectors were all random, then the answer is probably yes. But if it is the case that two of the nine vectors are the same, then they don't add anything new to the equation, and what we will get is probably an 8-dimensional plane. I don't understand how this is the case. Also, could someone explain what this means in terms of the number of solutions we will get?",,[]
83,Why are Hermitian matrices called a generalization of real symmetric matrices?,Why are Hermitian matrices called a generalization of real symmetric matrices?,,"I frequently see people explaining Hermitian matrices as a generalization of real symmetric matrices e.g. Wikipedia , Math StackExchange . I understand that all real symmetric matrices are Hermitian matrices, but it seems like there's really two changes between real symmetric matrices and Hermitian matrices: (1) permit complex numbers, and (2) the transpose must equal the element-wise complex conjugate. Why was this second step included in the definition of Hermitian matrices? My guess would be that it has something to do with physicists wanting to ensure particular sequences of linear transformations produce real-valued outputs; is this correct?","I frequently see people explaining Hermitian matrices as a generalization of real symmetric matrices e.g. Wikipedia , Math StackExchange . I understand that all real symmetric matrices are Hermitian matrices, but it seems like there's really two changes between real symmetric matrices and Hermitian matrices: (1) permit complex numbers, and (2) the transpose must equal the element-wise complex conjugate. Why was this second step included in the definition of Hermitian matrices? My guess would be that it has something to do with physicists wanting to ensure particular sequences of linear transformations produce real-valued outputs; is this correct?",,"['linear-algebra', 'complex-numbers', 'symmetric-matrices']"
84,Eigenvalues of tensor product of matrices : question about general properties,Eigenvalues of tensor product of matrices : question about general properties,,"I would like to refresh my mind on eigenvalues/eigenvector of tensor product of operator (for Quantum Mechanics purposes). Consider two hermitic matrices $A$ and $B$ living in Hilbert spaces of finite dimensions $N_A$ and $N_B$ respectively. I would like to know the most general properties about the eigenvalues of the operator $U=A \otimes B$ . Let's assume I know the spectrum of $A$ and $B$ : $$A |a^i\rangle = a |a^i\rangle$$ $$B |b^j\rangle = b |b^j\rangle$$ Where the indices $i$ and $j$ are here to take in account possible degeneracy in the eigenvalues. Proposition 1 A possible eigenbasis of $U$ is the set of all $|a^i\rangle \otimes |b^j\rangle$ with eigenvalues $ab$ . Proof: $|a^i\rangle \otimes |b^j\rangle$ is an eigenvector of $U$ with eigenvalue $ab$ . And as I have $N_A$ orthogonal vectors $|a^i\rangle$ and $N_B$ orthogonal vectors $|b^j\rangle$ , I have found $N_A*N_B$ orthogonal vectors that are eigenstates of $U$ . However, there might have quantum state of $U$ that are not of the form of a tensor product. Proposition 2 $U=A \otimes B$ admits one degenerated eigenvalue if and only if there exist eigenvectors of $U$ that are not a tensor product. Proof: Necessary condition: As $\{ |a^i\rangle \otimes |b^j\rangle \}$ are basis of eigenstate of $U$ , and there is a degenerated eigenvalue of $U$ , then there exist $(a,b) \neq (a',b')$ such that $|a\rangle \otimes |b\rangle$ and $|a'\rangle \otimes |b'\rangle$ are eigenstates of $U$ with the same eigenvalue. Thus $|a\rangle \otimes |b\rangle + |a'\rangle \otimes |b'\rangle$ also. Then I found an eigenstate of $U$ that is not a product state. Sufficient condition: If there is an eigenvector of $U$ that is not a tensor product, then it must be a linear combination of different $|a^i\rangle \otimes |b^j \rangle$ as they diagonalise $U$ . And if a linear combination of eigenvector is an eigenvector, then the two initial eigenvector must have the same eigenvalue. Then, $U$ has degenerated eigenvalues. Do you agree ? Especially with my second proposition (I am quite confident about the first one).","I would like to refresh my mind on eigenvalues/eigenvector of tensor product of operator (for Quantum Mechanics purposes). Consider two hermitic matrices and living in Hilbert spaces of finite dimensions and respectively. I would like to know the most general properties about the eigenvalues of the operator . Let's assume I know the spectrum of and : Where the indices and are here to take in account possible degeneracy in the eigenvalues. Proposition 1 A possible eigenbasis of is the set of all with eigenvalues . Proof: is an eigenvector of with eigenvalue . And as I have orthogonal vectors and orthogonal vectors , I have found orthogonal vectors that are eigenstates of . However, there might have quantum state of that are not of the form of a tensor product. Proposition 2 admits one degenerated eigenvalue if and only if there exist eigenvectors of that are not a tensor product. Proof: Necessary condition: As are basis of eigenstate of , and there is a degenerated eigenvalue of , then there exist such that and are eigenstates of with the same eigenvalue. Thus also. Then I found an eigenstate of that is not a product state. Sufficient condition: If there is an eigenvector of that is not a tensor product, then it must be a linear combination of different as they diagonalise . And if a linear combination of eigenvector is an eigenvector, then the two initial eigenvector must have the same eigenvalue. Then, has degenerated eigenvalues. Do you agree ? Especially with my second proposition (I am quite confident about the first one).","A B N_A N_B U=A \otimes B A B A |a^i\rangle = a |a^i\rangle B |b^j\rangle = b |b^j\rangle i j U |a^i\rangle \otimes |b^j\rangle ab |a^i\rangle \otimes |b^j\rangle U ab N_A |a^i\rangle N_B |b^j\rangle N_A*N_B U U U=A \otimes B U \{ |a^i\rangle \otimes |b^j\rangle \} U U (a,b) \neq (a',b') |a\rangle \otimes |b\rangle |a'\rangle \otimes |b'\rangle U |a\rangle \otimes |b\rangle + |a'\rangle \otimes |b'\rangle U U |a^i\rangle \otimes |b^j \rangle U U","['linear-algebra', 'hilbert-spaces', 'tensor-products']"
85,Maximal determinant of $3 \times 3$ matrix where sum of squared entries is $\leq 1$,Maximal determinant of  matrix where sum of squared entries is,3 \times 3 \leq 1,"Given a $3 \times 3$ matrix in which the sum of the squares of all the elements is not more than one, what is the maximal determinant of this matrix? I have only one idea that equal elements are located on the main diagonal, and the remaining elements are zero.","Given a matrix in which the sum of the squares of all the elements is not more than one, what is the maximal determinant of this matrix? I have only one idea that equal elements are located on the main diagonal, and the remaining elements are zero.",3 \times 3,"['linear-algebra', 'matrices', 'optimization', 'determinant', 'matrix-calculus']"
86,"The orthogonal complement of the orthogonal complement from ""Linear Algebra Done Right""","The orthogonal complement of the orthogonal complement from ""Linear Algebra Done Right""",,"The following content is from ""Linear Algebra Done Right"" by Sheldon Axler Corollary: Suppose $U$ is a finite-dimensional subspace of $V$ . Then $$U = (U ^\perp)^\perp.$$ We need to prove the following: i) $U \subseteq (U^\perp)^\perp$ and     ii) $(U^\perp)^\perp \subseteq U$ Proof i): Supposer $u \in U$ . Then $\langle u,v \rangle = 0$ for $v \in U^\perp$ . Since all vector $u$ is orthogonal to $v$ . Then $u \in U$ . Hence $u \in (U ^\perp)^\perp$ I do understand the proof for i) as stated in the above. But I'm confused about the proof in ii) ii) Let $u \in (U ^\perp)^\perp$ From Proposition 2, we have that $V = U \bigoplus U^\perp$ . and so $u$ can be written as $u=v+w$ where $v \in U$ and $w \in U^\perp$ . But $u - v = w$ , so $u - v \in U^\perp$ . Now we already have that $u \in (U ^\perp)^\perp$ and $v \in (U ^\perp)^\perp$ and $u - v \in (U ^\perp)^\perp$ . Therfore, $u - v \in U ^\perp \cap (U^\perp)^\perp$ . ***Here is where I don't understand! Why we can let $u = v + w$ , but since $u \in (U ^\perp)^\perp$ ? and Why does $u - v \in U ^\perp \cap (U^\perp)^\perp$ ?","The following content is from ""Linear Algebra Done Right"" by Sheldon Axler Corollary: Suppose is a finite-dimensional subspace of . Then We need to prove the following: i) and     ii) Proof i): Supposer . Then for . Since all vector is orthogonal to . Then . Hence I do understand the proof for i) as stated in the above. But I'm confused about the proof in ii) ii) Let From Proposition 2, we have that . and so can be written as where and . But , so . Now we already have that and and . Therfore, . ***Here is where I don't understand! Why we can let , but since ? and Why does ?","U V U = (U ^\perp)^\perp. U \subseteq (U^\perp)^\perp (U^\perp)^\perp \subseteq U u \in U \langle u,v \rangle = 0 v \in U^\perp u v u \in U u \in (U ^\perp)^\perp u \in (U ^\perp)^\perp V = U \bigoplus U^\perp u u=v+w v \in U w \in U^\perp u - v = w u - v \in U^\perp u \in (U ^\perp)^\perp v \in (U ^\perp)^\perp u - v \in (U ^\perp)^\perp u - v \in U ^\perp \cap (U^\perp)^\perp u = v + w u \in (U ^\perp)^\perp u - v \in U ^\perp \cap (U^\perp)^\perp","['linear-algebra', 'abstract-algebra', 'proof-verification', 'orthogonality']"
87,When are $z^n - 1$ and $(z+1)^n - 1$ relatively prime?,When are  and  relatively prime?,z^n - 1 (z+1)^n - 1,"The question and answer from this post seem to imply that the polynomials $z^n - 1$ and $(z+1)^n - 1$ (over $\Bbb C[x])$ will be relatively prime if and only if $6 \nmid n$ . Is this true?  If so, is there a quick, direct justification that this is the case?","The question and answer from this post seem to imply that the polynomials and (over will be relatively prime if and only if . Is this true?  If so, is there a quick, direct justification that this is the case?",z^n - 1 (z+1)^n - 1 \Bbb C[x]) 6 \nmid n,"['linear-algebra', 'abstract-algebra', 'polynomials']"
88,Constructing a continuous path between two sets of oriented basis for a vector space out of a collection of subspaces,Constructing a continuous path between two sets of oriented basis for a vector space out of a collection of subspaces,,"Let $V_1, V_2, \dots, V_n$ be a collection of vector subspaces in $\mathbb R^n$ . For each $j=1, \dots, n$ , $\dim(V_j) = m$ with $m < n$ . Suppose we can construct a basis $U = \{u_1, \dots, u_n\}$ of $\mathbb R^n$ in the manner: $u_j \in V_j$ for each $j$ . Now suppose we construct another basis $W = \{w_1, \dots, w_n\}$ in the same manner, i.e., $w_j \in V_j$ for each $j$ . I am wondering whether $U$ is connected with $W$ in the sense: there is a path $\gamma = \gamma_1 \times \gamma_2 \times \dots \times \gamma_n$ , where each $\gamma_j: [0,1] \to V_j$ is a continuous path connecting $v_j$ and $w_j$ in $V_j$ and for each $t$ : $\gamma(t)$ forms a basis for $\mathbb R^n$ . Intuitively, it seems the path can be chosen within each subspace. But I failed to formally state this: I was thinking to continuously choose a path $\gamma_j: [0,1] \to V_j$ such that $\gamma_j(0) = v_j$ and $\gamma_j(1) = w_j$ but get lost on whether or not we can guarantee the linearly independence in the process. I am not sure whether orientation would be relevant, but if so let us assume the basis $\{v_j\}$ and $\{w_j\}$ have the same orientation. Edit: As pointed out by Paul Frost, if $m=1$ , this is not possible. But I would love to see a general case for $m \ge 2$ .","Let be a collection of vector subspaces in . For each , with . Suppose we can construct a basis of in the manner: for each . Now suppose we construct another basis in the same manner, i.e., for each . I am wondering whether is connected with in the sense: there is a path , where each is a continuous path connecting and in and for each : forms a basis for . Intuitively, it seems the path can be chosen within each subspace. But I failed to formally state this: I was thinking to continuously choose a path such that and but get lost on whether or not we can guarantee the linearly independence in the process. I am not sure whether orientation would be relevant, but if so let us assume the basis and have the same orientation. Edit: As pointed out by Paul Frost, if , this is not possible. But I would love to see a general case for .","V_1, V_2, \dots, V_n \mathbb R^n j=1, \dots, n \dim(V_j) = m m < n U = \{u_1, \dots, u_n\} \mathbb R^n u_j \in V_j j W = \{w_1, \dots, w_n\} w_j \in V_j j U W \gamma = \gamma_1 \times \gamma_2 \times \dots \times \gamma_n \gamma_j: [0,1] \to V_j v_j w_j V_j t \gamma(t) \mathbb R^n \gamma_j: [0,1] \to V_j \gamma_j(0) = v_j \gamma_j(1) = w_j \{v_j\} \{w_j\} m=1 m \ge 2","['linear-algebra', 'general-topology', 'path-connected']"
89,Showing that given matrix does not have negative eigenvalues without using the knowledge that it is positive definite.,Showing that given matrix does not have negative eigenvalues without using the knowledge that it is positive definite.,,"Let $a,b,c$ be a positive real number such that $b^2+c^2<a<1$. Let  $A=\begin{bmatrix} 1&b&c\\ b&a & 0\\ c & 0 & 1\end{bmatrix}$. Then Consider the above matrix. I want to comment about the nature of eigenvalues of the matrix in the sense that, they are all positive, all negative, mix of positive or negative, non zero, real or non real etc.. My efforts We look at the matrix first and see if it looks like one of the familiar type introduced in standard Linear Algebra Text. We can see, this matrix is symmetric. As soon we hear the term ""symmetric matrix"" and there is already  the term ""eigenvalue"" in the question. We go the next standard result which says that a real symmetric matrix is diagonalizable with all eigenvalues real. Conclusion so far The given matrix has only real eigenvalues. Another standard result is the sum of eigenvalues is equal to the trace of the matrix. Trace is positive here due to the conditions specified. So not all eigenvalues can be negative. So we are left with two choices all eigenvalues are positive Eigenvalues of A are either positive or negative. I know this matrix is positive definite(I have already proved it, by showing that all sub determinant are positive) so all eigenvalues are positive. My aim is to show that there are no negative eigenvalues without going into the theory of positive definite matrices.","Let $a,b,c$ be a positive real number such that $b^2+c^2<a<1$. Let  $A=\begin{bmatrix} 1&b&c\\ b&a & 0\\ c & 0 & 1\end{bmatrix}$. Then Consider the above matrix. I want to comment about the nature of eigenvalues of the matrix in the sense that, they are all positive, all negative, mix of positive or negative, non zero, real or non real etc.. My efforts We look at the matrix first and see if it looks like one of the familiar type introduced in standard Linear Algebra Text. We can see, this matrix is symmetric. As soon we hear the term ""symmetric matrix"" and there is already  the term ""eigenvalue"" in the question. We go the next standard result which says that a real symmetric matrix is diagonalizable with all eigenvalues real. Conclusion so far The given matrix has only real eigenvalues. Another standard result is the sum of eigenvalues is equal to the trace of the matrix. Trace is positive here due to the conditions specified. So not all eigenvalues can be negative. So we are left with two choices all eigenvalues are positive Eigenvalues of A are either positive or negative. I know this matrix is positive definite(I have already proved it, by showing that all sub determinant are positive) so all eigenvalues are positive. My aim is to show that there are no negative eigenvalues without going into the theory of positive definite matrices.",,['linear-algebra']
90,"Why does every symplectic matrix $M\in{\rm Sp}(2n,{\Bbb R})$ admit a symplectic eigenbasis?",Why does every symplectic matrix  admit a symplectic eigenbasis?,"M\in{\rm Sp}(2n,{\Bbb R})","I tripped over the following assertion. Let $M \in \mathrm{Sp}(2n)$ be a symplectic real matrix which is diagonalizable. Then we can write it down as $M = S^{-1}D\ S$ where $S \in \mathrm{Sp}(2n)$ and $D$ is diagonal. I don't see why I can diagonalize $M$ by a symplectic matrix, I haven't found any reference of this statement somewhere else, Does anyone knows a reference or a proof about this statement? I tried to prove and the only thing I could get was that $\lambda$ and $\lambda^{-1}$ are eigenvalues of $M$ and probably this could get to the symplecticity of the eigenvectos of $S$.","I tripped over the following assertion. Let $M \in \mathrm{Sp}(2n)$ be a symplectic real matrix which is diagonalizable. Then we can write it down as $M = S^{-1}D\ S$ where $S \in \mathrm{Sp}(2n)$ and $D$ is diagonal. I don't see why I can diagonalize $M$ by a symplectic matrix, I haven't found any reference of this statement somewhere else, Does anyone knows a reference or a proof about this statement? I tried to prove and the only thing I could get was that $\lambda$ and $\lambda^{-1}$ are eigenvalues of $M$ and probably this could get to the symplecticity of the eigenvectos of $S$.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'symplectic-geometry', 'symplectic-linear-algebra']"
91,How to find complex eigenvectors from complex eigenvalues?,How to find complex eigenvectors from complex eigenvalues?,,I have this matrix that represents a $2 \times 2$ linear system and I am supposed to solve to find what $x(t)$ and $y(t)$ are. $\left[ {\begin{array}{cc}    1 & 5 \\    -1 & -3 \\   \end{array} } \right] $ I got that my eigenvalues were $-1+i$ and $-1-i$. But how do I use these to find my eigenvectors?,I have this matrix that represents a $2 \times 2$ linear system and I am supposed to solve to find what $x(t)$ and $y(t)$ are. $\left[ {\begin{array}{cc}    1 & 5 \\    -1 & -3 \\   \end{array} } \right] $ I got that my eigenvalues were $-1+i$ and $-1-i$. But how do I use these to find my eigenvectors?,,"['linear-algebra', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
92,Understanding the concept of $(X^TX)^{-1}X^Ty$,Understanding the concept of,(X^TX)^{-1}X^Ty,"$w = (X^TX)^{-1}X^Ty$ Equation above produces weight $w$ which solves quadratic minimization problem in linear functions. From my understanding, the goal is to find $w$ such that: $Xw ≈ y$ Thus, I need to minimize this function: $||Xw-y||^2$ According to wikipedia , Euclidean norm is used to minimize this function: $2X^T(Xw - y) = 0$ Then this equation is differentiated with respect of $w$, from my understanding we use multivariable chain rule: $X^TXw-X^Ty=0$ $X^TXw=X^Ty$ $w=(X^TX)^{-1}(X^Ty)$ Somehow, by utilizing Euclidean norm, I minimized the function, but I'm unable to understand how does it exactly work. Why is Euclidean norm used to solve quadratic minimization problems in linear functions?","$w = (X^TX)^{-1}X^Ty$ Equation above produces weight $w$ which solves quadratic minimization problem in linear functions. From my understanding, the goal is to find $w$ such that: $Xw ≈ y$ Thus, I need to minimize this function: $||Xw-y||^2$ According to wikipedia , Euclidean norm is used to minimize this function: $2X^T(Xw - y) = 0$ Then this equation is differentiated with respect of $w$, from my understanding we use multivariable chain rule: $X^TXw-X^Ty=0$ $X^TXw=X^Ty$ $w=(X^TX)^{-1}(X^Ty)$ Somehow, by utilizing Euclidean norm, I minimized the function, but I'm unable to understand how does it exactly work. Why is Euclidean norm used to solve quadratic minimization problems in linear functions?",,"['linear-algebra', 'multivariable-calculus', 'least-squares']"
93,Hilbert space basis which is not a vector space basis,Hilbert space basis which is not a vector space basis,,"Is the set $(e_n)_{n>0}$ a (vector space) basis for the sequence Hilbert space $l^2$? It is a Hilbert space basis anyway. I would say no, because the sequence $\left(\frac{1}{n}\right)_{n>0}$ is in $l^2$ but it can't be written as a finite linear combination of $e_i$'s. Is that right?","Is the set $(e_n)_{n>0}$ a (vector space) basis for the sequence Hilbert space $l^2$? It is a Hilbert space basis anyway. I would say no, because the sequence $\left(\frac{1}{n}\right)_{n>0}$ is in $l^2$ but it can't be written as a finite linear combination of $e_i$'s. Is that right?",,"['linear-algebra', 'functional-analysis']"
94,What is the connection between Jordan Canonical Form and minimal polynomial?,What is the connection between Jordan Canonical Form and minimal polynomial?,,"I saw a bunch of examples of Jordan Canonical Form and how it is related to the minimal polynomial. I have noticed the following patterns: Let $A$ be a matrix and $\lambda_1,\dots,\lambda_k$ be the eigenvalues of $A$. (1). The number of Jordan blocks corresponding to the eigenvalue $\lambda_j$ is the dimension of the eigenspace of $\lambda_j$. (2). The power of $(x-\lambda_j)$ in the minimal polynomial of $A$ is the size of the largest Jordan block corresponding to $\lambda_j$. Are (1) and (2) true in general?","I saw a bunch of examples of Jordan Canonical Form and how it is related to the minimal polynomial. I have noticed the following patterns: Let $A$ be a matrix and $\lambda_1,\dots,\lambda_k$ be the eigenvalues of $A$. (1). The number of Jordan blocks corresponding to the eigenvalue $\lambda_j$ is the dimension of the eigenspace of $\lambda_j$. (2). The power of $(x-\lambda_j)$ in the minimal polynomial of $A$ is the size of the largest Jordan block corresponding to $\lambda_j$. Are (1) and (2) true in general?",,"['linear-algebra', 'jordan-normal-form']"
95,Is every vector space an injective module?,Is every vector space an injective module?,,"Let R be a commutative ring and $Q$ an $R$-module, we say that $Q$ is injective if it has the property that for all $R$-modules $M$ and $N$ and homomorphisms $$f\ :\ M\to Q$$ and $$\phi\ :\ M\to N$$ such that $\phi$ is injective, there is a homomorphism $\overline{f}\ :\ N\to Q$ such that $\overline{f}\circ \phi =f$. Let $F$ be a field and let $Q$ be a finite dimensional vector space over $F$, that is, a finitely generated $F$-module.  Prove that $Q$ is injective. This question is from an old preliminary exam, and I believe I have a solution, which I will provide below, but my proof does not rely on the fact that $Q$ is finite dimensional.  Is it true that all vector spaces are injective?  I didn't think that was true, but I can't see where my proof goes wrong.  I would appreciate either some validation or a reason why my proof fails.  Thanks. Proof: Let $M$ and $N$ be vector spaces over $F$.  Let $\{x_i\}_{i\in I}$ be a basis for $M$ where $I$ is some indexing set.  I claim that $\{\phi(x_i)\}_{i\in I}$ is linearly indpendent and spans $\text{im}\phi$.  Indeed,for $\{s_i\}_{i=1}^n\subset I$ and $\alpha_{s_i}\in F$ for each $1\le i\le n$, we have  $$\sum_{i=1}^n\alpha_{s_i}\phi(x_{s_i})=0$$ $$\implies\phi\left(\sum_{i=1}^n\alpha_{s_i}x_{s_i}\right)=0$$ $$\implies \sum_{i=1}^n\alpha_{s_i}x_{s_i}=0$$ $$\implies \alpha_{s_i}=0\text{ for all }i$$ Where the third line follows from the injectivity of $\phi$.  Hence, $\{\phi(x_i)\}_{i\in I}$ is linearly independent.  To see that this set spans $\phi(M)$, let $y\in \phi(M)$  and write  $$y=\phi(x)=\phi\left(\sum_{j\in J}^n\alpha_jx_j\right)=\sum_{j\in J}\alpha_j\phi(x_j)\text{ for some finite subset }J\text{ of }I\text{ and }\{\alpha_j\}_{j\in J}\subset F.$$ Extend this set to a basis for $N$.  Define $\overline{f}\ :\ N\to Q$ by $\overline{f}(\phi(x_i))=f(x_i)$ and define $\overline{f}$ to be $0$ on all other basis elements.  It appears that $\overline{f}$ has the property required.  Hence, $Q$ is injective. Note that not once did I use the fact that $Q$ had a finite basis.  So, this would imply that all vector spaces are injective.  Note also that I only relied on the fact that $F$ was a field in order to extend the basis.  Could I have also just gotten away with defining $\overline{f}$ to be $0$ on the complement of $\phi(M)$ in order to prove that all free modules are injective?","Let R be a commutative ring and $Q$ an $R$-module, we say that $Q$ is injective if it has the property that for all $R$-modules $M$ and $N$ and homomorphisms $$f\ :\ M\to Q$$ and $$\phi\ :\ M\to N$$ such that $\phi$ is injective, there is a homomorphism $\overline{f}\ :\ N\to Q$ such that $\overline{f}\circ \phi =f$. Let $F$ be a field and let $Q$ be a finite dimensional vector space over $F$, that is, a finitely generated $F$-module.  Prove that $Q$ is injective. This question is from an old preliminary exam, and I believe I have a solution, which I will provide below, but my proof does not rely on the fact that $Q$ is finite dimensional.  Is it true that all vector spaces are injective?  I didn't think that was true, but I can't see where my proof goes wrong.  I would appreciate either some validation or a reason why my proof fails.  Thanks. Proof: Let $M$ and $N$ be vector spaces over $F$.  Let $\{x_i\}_{i\in I}$ be a basis for $M$ where $I$ is some indexing set.  I claim that $\{\phi(x_i)\}_{i\in I}$ is linearly indpendent and spans $\text{im}\phi$.  Indeed,for $\{s_i\}_{i=1}^n\subset I$ and $\alpha_{s_i}\in F$ for each $1\le i\le n$, we have  $$\sum_{i=1}^n\alpha_{s_i}\phi(x_{s_i})=0$$ $$\implies\phi\left(\sum_{i=1}^n\alpha_{s_i}x_{s_i}\right)=0$$ $$\implies \sum_{i=1}^n\alpha_{s_i}x_{s_i}=0$$ $$\implies \alpha_{s_i}=0\text{ for all }i$$ Where the third line follows from the injectivity of $\phi$.  Hence, $\{\phi(x_i)\}_{i\in I}$ is linearly independent.  To see that this set spans $\phi(M)$, let $y\in \phi(M)$  and write  $$y=\phi(x)=\phi\left(\sum_{j\in J}^n\alpha_jx_j\right)=\sum_{j\in J}\alpha_j\phi(x_j)\text{ for some finite subset }J\text{ of }I\text{ and }\{\alpha_j\}_{j\in J}\subset F.$$ Extend this set to a basis for $N$.  Define $\overline{f}\ :\ N\to Q$ by $\overline{f}(\phi(x_i))=f(x_i)$ and define $\overline{f}$ to be $0$ on all other basis elements.  It appears that $\overline{f}$ has the property required.  Hence, $Q$ is injective. Note that not once did I use the fact that $Q$ had a finite basis.  So, this would imply that all vector spaces are injective.  Note also that I only relied on the fact that $F$ was a field in order to extend the basis.  Could I have also just gotten away with defining $\overline{f}$ to be $0$ on the complement of $\phi(M)$ in order to prove that all free modules are injective?",,"['linear-algebra', 'ring-theory', 'modules', 'injective-module']"
96,What can be said if $A^2+B^2+2AB=0$ for some real $2\times2$ matrices $A$ and $B$?,What can be said if  for some real  matrices  and ?,A^2+B^2+2AB=0 2\times2 A B,"Let $A,B\in M_2(\mathbb{R})$ be such that $A^2+B^2+2AB=0$ and $\det A= \det B$ . Our goal is to compute $\det(A^2 - B^2)$ .  According to the chain of comments on Art of Problem Solving , the following statements are true: $\det(A^2+B^2)+\det(A^2-B^2)=2(\det A^2+\det B^2)$ . (Is this well known?) (1) $\implies \det(A^2-B^2)=0$ . If $A,B\in M_2(\mathbb{C})$ satisfy $A^2+B^2+2AB=0$ , then $AB=BA$ . $(A+B)^2=0 \implies \det(A^2-B^2)=0$ . Can someone help me with justifying these statements? Edit. Doug M provided an explanation for (1) in the answers. Here is an explanation for (2): $A^2+B^2+2AB=O_2 \implies A^2+B^2=-2AB$ . So $\det(A^2+B^2)=4\det(AB)$ . Now using (1), $\det(A^2-B^2)= 2\left(\det(A^2)-2\det(AB)+\det(B^2)\right) = 2\left((\det(A)^2-\det(B)^2\right) = 0$ .","Let be such that and . Our goal is to compute .  According to the chain of comments on Art of Problem Solving , the following statements are true: . (Is this well known?) (1) . If satisfy , then . . Can someone help me with justifying these statements? Edit. Doug M provided an explanation for (1) in the answers. Here is an explanation for (2): . So . Now using (1), .","A,B\in M_2(\mathbb{R}) A^2+B^2+2AB=0 \det A= \det B \det(A^2 - B^2) \det(A^2+B^2)+\det(A^2-B^2)=2(\det A^2+\det B^2) \implies \det(A^2-B^2)=0 A,B\in M_2(\mathbb{C}) A^2+B^2+2AB=0 AB=BA (A+B)^2=0 \implies \det(A^2-B^2)=0 A^2+B^2+2AB=O_2 \implies A^2+B^2=-2AB \det(A^2+B^2)=4\det(AB) \det(A^2-B^2)= 2\left(\det(A^2)-2\det(AB)+\det(B^2)\right) = 2\left((\det(A)^2-\det(B)^2\right) = 0","['linear-algebra', 'matrices']"
97,Geometric multiplicity equal to 0,Geometric multiplicity equal to 0,,Is it possible for an eigenvalue of endomorphism to have geometric multiplicity equal to 0? I would be grateful if anyone who has an answer to this question would care to explain.,Is it possible for an eigenvalue of endomorphism to have geometric multiplicity equal to 0? I would be grateful if anyone who has an answer to this question would care to explain.,,"['linear-algebra', 'eigenvalues-eigenvectors', 'linear-transformations']"
98,Characteristic Polynomial Independent From the Choice of Basis,Characteristic Polynomial Independent From the Choice of Basis,,"I want to prove that definition of characteristic polynomial of a linear operator on a finite-dimensional vector space $V$ is independent of the choice of basis for $V$. Proof>> Choose two different basis for V, $\beta, \beta '$. Let $Q = [I_v]_{\beta'}^\beta$. Then, $[T]_{\beta'} = Q^{-1 }[T]_\beta Q  $ ,which means that $[T]_\beta$ and $[T]_{\beta'}$ are similar. Since similar matrices are having same characteristic polynomial, $det([T]_\beta-\lambda I$) = $det([T]_{\beta '}-\lambda I$). How's this proof?","I want to prove that definition of characteristic polynomial of a linear operator on a finite-dimensional vector space $V$ is independent of the choice of basis for $V$. Proof>> Choose two different basis for V, $\beta, \beta '$. Let $Q = [I_v]_{\beta'}^\beta$. Then, $[T]_{\beta'} = Q^{-1 }[T]_\beta Q  $ ,which means that $[T]_\beta$ and $[T]_{\beta'}$ are similar. Since similar matrices are having same characteristic polynomial, $det([T]_\beta-\lambda I$) = $det([T]_{\beta '}-\lambda I$). How's this proof?",,['linear-algebra']
99,"Prove that $A+I_n$ is invertible, where $\left\lVert A\right\rVert<1$","Prove that  is invertible, where",A+I_n \left\lVert A\right\rVert<1,"Let $\left\lVert\cdot\right\rVert:\mathbb R^{n\times n}\to\mathbb R$ be a submultiplicative matrix norm and $A\in\mathbb R^{n\times n}$ such that $\lVert A\rVert<1$. Prove that $A+I_n$ is invertible, where $I_n$ is the identity matrix in $\mathbb R^{n\times n}$. I tried coming up with something like $$\lVert A+I_n\rVert=\lVert A(I_n+A^{-1})\rVert\leq\lVert A\rVert\cdot\lVert(I_n+A^{-1})\rVert<\lVert I_n+A^{-1}\rVert,$$ but that doesn't seem to get me anywhere. In the end, I think I should have some (in)equality with the determinant of $I_n+A^{-1}$ in it (and conclude that it is not $0$), but I don't know how to get there. How could I proceed?","Let $\left\lVert\cdot\right\rVert:\mathbb R^{n\times n}\to\mathbb R$ be a submultiplicative matrix norm and $A\in\mathbb R^{n\times n}$ such that $\lVert A\rVert<1$. Prove that $A+I_n$ is invertible, where $I_n$ is the identity matrix in $\mathbb R^{n\times n}$. I tried coming up with something like $$\lVert A+I_n\rVert=\lVert A(I_n+A^{-1})\rVert\leq\lVert A\rVert\cdot\lVert(I_n+A^{-1})\rVert<\lVert I_n+A^{-1}\rVert,$$ but that doesn't seem to get me anywhere. In the end, I think I should have some (in)equality with the determinant of $I_n+A^{-1}$ in it (and conclude that it is not $0$), but I don't know how to get there. How could I proceed?",,"['linear-algebra', 'matrices', 'normed-spaces', 'inverse']"
