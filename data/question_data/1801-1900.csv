,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Inequality $\frac{a + \sqrt{ab} + \sqrt[3]{abc}}{3} \leq \sqrt[3]{a \cdot \frac{a+b}{2} \cdot \frac{a+b+c}{3}}.$,Inequality,\frac{a + \sqrt{ab} + \sqrt[3]{abc}}{3} \leq \sqrt[3]{a \cdot \frac{a+b}{2} \cdot \frac{a+b+c}{3}}.,"Someone can to help me with a hint in the following problem: Show that for any $a,b,c>0$,    $$\frac{a + \sqrt{ab} + \sqrt[3]{abc}}{3} \leq \sqrt[3]{a \cdot \frac{a+b}{2} \cdot \frac{a+b+c}{3}}.$$ I have tried using the Hölder inequality, but can not apply it efficiently. Thanks!","Someone can to help me with a hint in the following problem: Show that for any $a,b,c>0$,    $$\frac{a + \sqrt{ab} + \sqrt[3]{abc}}{3} \leq \sqrt[3]{a \cdot \frac{a+b}{2} \cdot \frac{a+b+c}{3}}.$$ I have tried using the Hölder inequality, but can not apply it efficiently. Thanks!",,"['real-analysis', 'inequality', 'problem-solving']"
1,Necessary and sufficient conditions for a polynomial $p$ to satisfy $\|x\|\to\infty\implies p(x)\to\infty$?,Necessary and sufficient conditions for a polynomial  to satisfy ?,p \|x\|\to\infty\implies p(x)\to\infty,"I'm looking for a necessary and sufficient conditions (I'm not even sure these exist) for a polynomial $p:\mathbb{R}^n\to\mathbb{R}$ to be ""radially unbounded"", that is $$\|x\|\to\infty\implies p(x)\to\infty,$$ where $\|{\cdot}\|$ denotes any $p$-norm on $\mathbb{R}^n$. Ideally, I'm looking for conditions in terms of the polynomial's coefficients and degree. For example, if $n=1$ it is straightforward to see that $p$ is radially unbounded if and only if its degree is even and the monomial of highest degree has a positive coefficient. However, I'm struggling to generalise this to arbitrary $n$. Any help would great. Motivation: I'm interested in the above because I'm trying to come up with an automatised test that can decide whether or not the all the sublevel sets of a given polynomial are compact (this is so if and only if the polynomial is radially unbounded). Edit: If no necessary and sufficient conditions (or argument that no such conditions exist in general) are posted before the bounty ends, I'd be more than happy to award the bounty to any answer containing insightful remarks or necessary or sufficient conditions.","I'm looking for a necessary and sufficient conditions (I'm not even sure these exist) for a polynomial $p:\mathbb{R}^n\to\mathbb{R}$ to be ""radially unbounded"", that is $$\|x\|\to\infty\implies p(x)\to\infty,$$ where $\|{\cdot}\|$ denotes any $p$-norm on $\mathbb{R}^n$. Ideally, I'm looking for conditions in terms of the polynomial's coefficients and degree. For example, if $n=1$ it is straightforward to see that $p$ is radially unbounded if and only if its degree is even and the monomial of highest degree has a positive coefficient. However, I'm struggling to generalise this to arbitrary $n$. Any help would great. Motivation: I'm interested in the above because I'm trying to come up with an automatised test that can decide whether or not the all the sublevel sets of a given polynomial are compact (this is so if and only if the polynomial is radially unbounded). Edit: If no necessary and sufficient conditions (or argument that no such conditions exist in general) are posted before the bounty ends, I'd be more than happy to award the bounty to any answer containing insightful remarks or necessary or sufficient conditions.",,"['real-analysis', 'analysis', 'polynomials']"
2,Approximation of Semicontinuous Functions,Approximation of Semicontinuous Functions,,"Assume that $k \in \mathbb{N}$ and $f : \mathbb{R}^d \rightarrow [0,\infty)$ is lower semicontinuous, i.e. $f(x) \leq \liminf_{y \rightarrow x} f(y)$ for all $x \in \mathbb{R}^d$. Does there exist an increasing sequence of $k$-times continuously differentiable functions $(g_n)_n \subset C^k(\mathbb{R}^d ; [0,\infty))$ that converges pointwise to $f$, i.e. $g_n(x) \leq g_{n+1}(x)$ for all $x \in \mathbb{R}^d$ and $n \in \mathbb{N}$ as well as $\lim_{n \rightarrow \infty} g_n(x) = f(x)$ for all $x \in \mathbb{R}^d$? My intuition would be yes, since we could tile $\mathbb{R}^d$ into dyadic cubes, take for the center of every cube the minimum of the values of $f$ on neighboring cubes and then interpolate between these center points with nice $C^\infty$ functions. Unfortunately this approach sounds very technical to me and I'm wondering whether there is something more elegant.","Assume that $k \in \mathbb{N}$ and $f : \mathbb{R}^d \rightarrow [0,\infty)$ is lower semicontinuous, i.e. $f(x) \leq \liminf_{y \rightarrow x} f(y)$ for all $x \in \mathbb{R}^d$. Does there exist an increasing sequence of $k$-times continuously differentiable functions $(g_n)_n \subset C^k(\mathbb{R}^d ; [0,\infty))$ that converges pointwise to $f$, i.e. $g_n(x) \leq g_{n+1}(x)$ for all $x \in \mathbb{R}^d$ and $n \in \mathbb{N}$ as well as $\lim_{n \rightarrow \infty} g_n(x) = f(x)$ for all $x \in \mathbb{R}^d$? My intuition would be yes, since we could tile $\mathbb{R}^d$ into dyadic cubes, take for the center of every cube the minimum of the values of $f$ on neighboring cubes and then interpolate between these center points with nice $C^\infty$ functions. Unfortunately this approach sounds very technical to me and I'm wondering whether there is something more elegant.",,"['real-analysis', 'approximation', 'approximation-theory', 'semicontinuous-functions']"
3,How to prove this integral inequality $ \int_0^{2\pi} p(x)[p(x)+p''(x)] dx \int_0^{2\pi}\frac{1}{p(x)+p''(x)} dx\geq 2\pi \int _0^{2\pi} p(x) dx $?,How to prove this integral inequality ?, \int_0^{2\pi} p(x)[p(x)+p''(x)] dx \int_0^{2\pi}\frac{1}{p(x)+p''(x)} dx\geq 2\pi \int _0^{2\pi} p(x) dx ,Let $p\in C^2(\mathbb{R})$ be a $2\pi$-periodic function such that $p(x)>0$ and $p(x)+p''(x)>0$ for all $x\in \mathbb{R}$. Then it holds $$ \int_0^{2\pi} p(x)[p(x)+p''(x)] dx \int_0^{2\pi}\frac{1}{p(x)+p''(x)} dx\geq 2\pi \int _0^{2\pi} p(x) dx  $$ The identity holds when $p+p''$ is a constant.,Let $p\in C^2(\mathbb{R})$ be a $2\pi$-periodic function such that $p(x)>0$ and $p(x)+p''(x)>0$ for all $x\in \mathbb{R}$. Then it holds $$ \int_0^{2\pi} p(x)[p(x)+p''(x)] dx \int_0^{2\pi}\frac{1}{p(x)+p''(x)} dx\geq 2\pi \int _0^{2\pi} p(x) dx  $$ The identity holds when $p+p''$ is a constant.,,"['real-analysis', 'inequality', 'integral-inequality']"
4,Can (linear) differential equations of infinite order be recast into equations of first order?,Can (linear) differential equations of infinite order be recast into equations of first order?,,"In most analysis courses one sees that differential equations of order $n$ are basically a subset of higher dimensional differential equations of order $1$ , for example the equation: $$f^{(n)}(t)=F\left(f(t),f'(t),...,f^{(n-1)}(t),t\right)$$ is the same as: $$\frac{d}{dt}\,\begin{pmatrix}g_0(t)\\g_1(t)\\\vdots\\g_{n-1(t)}\end{pmatrix}=\begin{pmatrix}g_1(t)\\\vdots\\g_{n-1}(t)\\F\left(g_0(t),g_1(t),...,g_{n-1}(t),t\right)\end{pmatrix}.$$ This is especially useful, as it allows one to write down explicitly the solutions to linear differential equations of finite order, if we have: $$f^{(n)}(t)=\sum_{k=0}^{n-1}a_k\, f^{(k)}(t)$$ Then the corresponding $n$ -dimensional equation is of the form: $$\frac{d}{dt} g(t) = A\cdot g(t)$$ for some matrix $A$ and the solution is $g(t)=\exp(A\,t)g(0)$ . It is possible to generalise to time dependent coefficients $a_k$ . Is there a way to implement this trick for differential equations that are essentially of infinite order? For example the equation $$f=\sum_{k=1}^\infty f^{(k)}(t),$$ of which the solution space is $f=\{C\exp(\frac t2)\mid C\in\mathbb R$ (or $\mathbb C$ ) $\}$ . More generally I would like to put something of the form $$\sum_{k=0}^\infty a_k\, f^{(k)}(t)=0$$ (where $a_k$ are as regular as needed (but with infinite non-zero terms)) into the form $$\frac{d}{dt} u = A(u)$$ Where $u$ is a map $C^\infty(\mathbb R,X)$ with $X$ a Banach space and $A\in \mathcal L(X)$ .","In most analysis courses one sees that differential equations of order are basically a subset of higher dimensional differential equations of order , for example the equation: is the same as: This is especially useful, as it allows one to write down explicitly the solutions to linear differential equations of finite order, if we have: Then the corresponding -dimensional equation is of the form: for some matrix and the solution is . It is possible to generalise to time dependent coefficients . Is there a way to implement this trick for differential equations that are essentially of infinite order? For example the equation of which the solution space is (or ) . More generally I would like to put something of the form (where are as regular as needed (but with infinite non-zero terms)) into the form Where is a map with a Banach space and .","n 1 f^{(n)}(t)=F\left(f(t),f'(t),...,f^{(n-1)}(t),t\right) \frac{d}{dt}\,\begin{pmatrix}g_0(t)\\g_1(t)\\\vdots\\g_{n-1(t)}\end{pmatrix}=\begin{pmatrix}g_1(t)\\\vdots\\g_{n-1}(t)\\F\left(g_0(t),g_1(t),...,g_{n-1}(t),t\right)\end{pmatrix}. f^{(n)}(t)=\sum_{k=0}^{n-1}a_k\, f^{(k)}(t) n \frac{d}{dt} g(t) = A\cdot g(t) A g(t)=\exp(A\,t)g(0) a_k f=\sum_{k=1}^\infty f^{(k)}(t), f=\{C\exp(\frac t2)\mid C\in\mathbb R \mathbb C \} \sum_{k=0}^\infty a_k\, f^{(k)}(t)=0 a_k \frac{d}{dt} u = A(u) u C^\infty(\mathbb R,X) X A\in \mathcal L(X)","['real-analysis', 'sequences-and-series', 'functional-analysis', 'ordinary-differential-equations', 'analysis']"
5,Asymptotic Behaviour Of A Bizarre Function 2,Asymptotic Behaviour Of A Bizarre Function 2,,"It is well-known that $$\frac{1}{1}-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\cdots = \ln 2 $$ Hence $$\frac{x}{1}-\frac{x}{2}+\frac{x}{3}-\frac{x}{4}+\cdots= x\ln 2  $$ However, consider $f(x)$, where $$f(x)=\left\lfloor\frac{x}{1}\right\rfloor-\left\lfloor\frac{x}{2}\right\rfloor+\left\lfloor\frac{x}{3}\right\rfloor-\left\lfloor\frac{x}{4}\right\rfloor+\cdots  $$ and $\lfloor t\rfloor$ denotes the floor function. Can anyone determine and prove the asymptotic behaviour of $f(x)$? Maybe even a good error term as well or even the second term in the asymptotic expansion? I'm guessing that $$ f(x) \sim x \ln 2$$ but cannot prove it. Edit: I believe I may have solved my own problem. The infinite sum is equal to the limit of the partial sum as the amount of terms tends to infinity. The first $n$ terms can be approximated by dropping the floor function signs at a cost of an error of $O(n)$, leaving one with $$\frac{x}{1}-\frac{x}{2}+\frac{x}{3}-\frac{x}{4}+\cdots + (-1)^{n+1}\frac{x}{n} + O(n)$$ The alternating sum multiplied by $x$ is almost $ x \ln 2$ and the error in approximating the partial sum as the infinite sum is $O(\frac{1}{n})$, since the series is alternating. Hence, the error introduced by approximating $$ \frac{x}{1}-\frac{x}{2}+\frac{x}{3}-\frac{x}{4}+\cdots + (-1)^{n+1}\frac{x}{n}$$ as $x \ln 2$ is $O(\frac{x}{n})$ and so the nth partial sum is equal to $$ x \ln2 + O\left(\frac{x}{n}\right) + O(n)$$ But $n$ is independent of $x$ and so we minimise the error by letting $n=\sqrt x$ giving the final result $$f(x)= x \ln2 + O(\sqrt x) $$ I believe I have made no mistake in this solution.","It is well-known that $$\frac{1}{1}-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\cdots = \ln 2 $$ Hence $$\frac{x}{1}-\frac{x}{2}+\frac{x}{3}-\frac{x}{4}+\cdots= x\ln 2  $$ However, consider $f(x)$, where $$f(x)=\left\lfloor\frac{x}{1}\right\rfloor-\left\lfloor\frac{x}{2}\right\rfloor+\left\lfloor\frac{x}{3}\right\rfloor-\left\lfloor\frac{x}{4}\right\rfloor+\cdots  $$ and $\lfloor t\rfloor$ denotes the floor function. Can anyone determine and prove the asymptotic behaviour of $f(x)$? Maybe even a good error term as well or even the second term in the asymptotic expansion? I'm guessing that $$ f(x) \sim x \ln 2$$ but cannot prove it. Edit: I believe I may have solved my own problem. The infinite sum is equal to the limit of the partial sum as the amount of terms tends to infinity. The first $n$ terms can be approximated by dropping the floor function signs at a cost of an error of $O(n)$, leaving one with $$\frac{x}{1}-\frac{x}{2}+\frac{x}{3}-\frac{x}{4}+\cdots + (-1)^{n+1}\frac{x}{n} + O(n)$$ The alternating sum multiplied by $x$ is almost $ x \ln 2$ and the error in approximating the partial sum as the infinite sum is $O(\frac{1}{n})$, since the series is alternating. Hence, the error introduced by approximating $$ \frac{x}{1}-\frac{x}{2}+\frac{x}{3}-\frac{x}{4}+\cdots + (-1)^{n+1}\frac{x}{n}$$ as $x \ln 2$ is $O(\frac{x}{n})$ and so the nth partial sum is equal to $$ x \ln2 + O\left(\frac{x}{n}\right) + O(n)$$ But $n$ is independent of $x$ and so we minimise the error by letting $n=\sqrt x$ giving the final result $$f(x)= x \ln2 + O(\sqrt x) $$ I believe I have made no mistake in this solution.",,"['calculus', 'real-analysis', 'asymptotics']"
6,How to prove this lemma related to Rolle's theorem,How to prove this lemma related to Rolle's theorem,,"For any function $f$ denote by $Z(f)$ and $Z_o(f)$ the cardinalities of $f^{-1}(0)\cap[0,1]$ and $f^{-1}(0)\cap(0,1)$, respectively. Let $H=\{f\in C^\infty(\mathbb{R}): \text{supp}(f) = [0,1]\}$ From this question we have Lemma 1 Let $q:x\mapsto(x-r)p(x)$ where $p\in H$ and $r\in\mathbb{R}$. Then $Z(q^{(n)})\geq Z(p^{(n-1)})+1$ for all $n\in\mathbb{N}$. Proof Note that $q^{(n)}(x) = n p^{(n-1)}(x) + (x-r)p^{(n)}(x)$. Hence $r$ is a root of $q^{(n)}$ if and only if it is a root of $p^{(n-1)}$. Moreover we have $$\underbrace{(x-r)^{n-1}q^{(n)}(x)}_{\text{LHS}} = n (x-r)^{n-1} p^{(n-1)}(x) + (x-r)^np^{(n)}(x) = \underbrace{\frac{d}{dx}(x-r)^n p^{(n-1)}(x)}_{\text{RHS}}$$ If $q^{(n)}(r) = 0$ then $Z(q^{(n)}) - 2 = Z_o(\text{LHS}) = Z_o(\text{RHS}) \geq Z(p^{(n-1)}) - 1$. If $q^{(n)}(r) \neq 0$ then $Z(q^{(n)}) - 1 = Z_o(\text{LHS}) = Z_o(\text{RHS}) \geq Z(p^{(n-1)})\quad\quad\quad\quad\text{q.e.d.}$ $\text{ }$ The following modification Lemma 2 Let $q:x\mapsto(x-r)(x-\bar{r})p(x)$ where $p\in H$ and $r\in\mathbb{C}\setminus\mathbb{R}$. Then $Z(q^{(n)})\geq Z(p^{(n-2)})+2$ for all $n\in\mathbb{N}\setminus\{1\}$. I tried to prove the same way by writing $$\frac{j(x)k(x)}{(x-r)(x-\bar{r})}q^{(n)}(x) = \frac{d}{dx}\left(j(x)^2 k(x)\frac{d}{dx}j(x)^{-1}p^{(n-2)}(x)\right)$$ where $\frac{d}{dx}$ is differentiation along the real axis and $j(x)=c_1(x-r)^{1-n}+c_2(x-\bar{r})^{1-n}$ $k(x) = c_3 ((x-r)(x-\bar{r}))^n$ $c_1,c_2,c_3\in\mathbb{C}$ The real and imaginary part of $j(x)^{-1}$ are proportional, and $j(x)k(x)\in \mathbb{R}$ when choosing $c_1 = e^{i d_1}\\c_2=e^{i d_2}\\c_3=e^{-i\cdot   (d_1+d_2)/2}\\d_1,d_2\in\mathbb{R}$ but even then I couldn't get Rolle's theorem to work unless $j(x)$ has no roots on $(0,1)$. I have looked at many functions/different $n$s and even when choosing $r$ such that $j(x)$ has such roots, it seems impossible to find a counterexample to the lemma. How to prove the lemma? Or maybe this simpler version which remains when removing some of the assumptions: If $p$ is smooth with $p^{(n-2)}$ having $m$ roots on $[0,1]$, then how to prove that the $n$'th derivative of $x \mapsto p(x)(x-r)(x-\bar{r})$ has at least $m-2$ roots on $(0,1)$.","For any function $f$ denote by $Z(f)$ and $Z_o(f)$ the cardinalities of $f^{-1}(0)\cap[0,1]$ and $f^{-1}(0)\cap(0,1)$, respectively. Let $H=\{f\in C^\infty(\mathbb{R}): \text{supp}(f) = [0,1]\}$ From this question we have Lemma 1 Let $q:x\mapsto(x-r)p(x)$ where $p\in H$ and $r\in\mathbb{R}$. Then $Z(q^{(n)})\geq Z(p^{(n-1)})+1$ for all $n\in\mathbb{N}$. Proof Note that $q^{(n)}(x) = n p^{(n-1)}(x) + (x-r)p^{(n)}(x)$. Hence $r$ is a root of $q^{(n)}$ if and only if it is a root of $p^{(n-1)}$. Moreover we have $$\underbrace{(x-r)^{n-1}q^{(n)}(x)}_{\text{LHS}} = n (x-r)^{n-1} p^{(n-1)}(x) + (x-r)^np^{(n)}(x) = \underbrace{\frac{d}{dx}(x-r)^n p^{(n-1)}(x)}_{\text{RHS}}$$ If $q^{(n)}(r) = 0$ then $Z(q^{(n)}) - 2 = Z_o(\text{LHS}) = Z_o(\text{RHS}) \geq Z(p^{(n-1)}) - 1$. If $q^{(n)}(r) \neq 0$ then $Z(q^{(n)}) - 1 = Z_o(\text{LHS}) = Z_o(\text{RHS}) \geq Z(p^{(n-1)})\quad\quad\quad\quad\text{q.e.d.}$ $\text{ }$ The following modification Lemma 2 Let $q:x\mapsto(x-r)(x-\bar{r})p(x)$ where $p\in H$ and $r\in\mathbb{C}\setminus\mathbb{R}$. Then $Z(q^{(n)})\geq Z(p^{(n-2)})+2$ for all $n\in\mathbb{N}\setminus\{1\}$. I tried to prove the same way by writing $$\frac{j(x)k(x)}{(x-r)(x-\bar{r})}q^{(n)}(x) = \frac{d}{dx}\left(j(x)^2 k(x)\frac{d}{dx}j(x)^{-1}p^{(n-2)}(x)\right)$$ where $\frac{d}{dx}$ is differentiation along the real axis and $j(x)=c_1(x-r)^{1-n}+c_2(x-\bar{r})^{1-n}$ $k(x) = c_3 ((x-r)(x-\bar{r}))^n$ $c_1,c_2,c_3\in\mathbb{C}$ The real and imaginary part of $j(x)^{-1}$ are proportional, and $j(x)k(x)\in \mathbb{R}$ when choosing $c_1 = e^{i d_1}\\c_2=e^{i d_2}\\c_3=e^{-i\cdot   (d_1+d_2)/2}\\d_1,d_2\in\mathbb{R}$ but even then I couldn't get Rolle's theorem to work unless $j(x)$ has no roots on $(0,1)$. I have looked at many functions/different $n$s and even when choosing $r$ such that $j(x)$ has such roots, it seems impossible to find a counterexample to the lemma. How to prove the lemma? Or maybe this simpler version which remains when removing some of the assumptions: If $p$ is smooth with $p^{(n-2)}$ having $m$ roots on $[0,1]$, then how to prove that the $n$'th derivative of $x \mapsto p(x)(x-r)(x-\bar{r})$ has at least $m-2$ roots on $(0,1)$.",,"['real-analysis', 'derivatives', 'roots']"
7,How to construct examples of functions in the Spaces of type $\mathcal{S}$,How to construct examples of functions in the Spaces of type,\mathcal{S},"There are $3$  $\mathcal{S}$-type Spaces, namely $\mathcal{S}_\alpha\:,\: \mathcal{S}^\beta\:,\:\mathcal{S}_\alpha^\beta$. They are defined by: $\mathcal{S}_\alpha: |x^k\varphi^{(q)}(x)|\le C_qA^kk^{k\alpha}\qquad (k,q=0,1,2,...)$ $\mathcal{S}^\beta: |x^k\varphi^{(q)}(x)|\le C_kB^qq^{q\beta}\qquad (k,q=0,1,2,...)$ $\mathcal{S}_\alpha^\beta: |x^k\varphi^{(q)}(x)|\le CA^kB^qk^{k\alpha}q^{q\beta}\qquad (k,q=0,1,2,...)$ where the constants $A,B,C_k,C_q$ depend on $\varphi \:\&\:\alpha+\beta\ge1$. In the first space we have functions which are rapidly decaying as $|x|\to \infty$. The second space imposes conditions on the growth of derivatives of these functions as $|x|\to \infty$. The third space $$\mathcal{S}_\alpha^\beta\subset\mathcal{S}_\alpha\cap \mathcal{S}^\beta$$In general the converse also holds which is a classical result due to Kashpirovsky. I want to construct examples to get a better understanding of these definitions. From the definition it is clear that every function in these spaces is at least $C_c^\infty$. So intuitively the first thing that comes to my mind is Gaussian-like functions. Now say I consider $\mathcal{S}_1^2(\mathbb{R})$. Which type of functions belong to this space ? How to come up with suitable constants ? Reference: Generalized Functions, Volume 2 by I.M.Gelfand and G.E. Shilov","There are $3$  $\mathcal{S}$-type Spaces, namely $\mathcal{S}_\alpha\:,\: \mathcal{S}^\beta\:,\:\mathcal{S}_\alpha^\beta$. They are defined by: $\mathcal{S}_\alpha: |x^k\varphi^{(q)}(x)|\le C_qA^kk^{k\alpha}\qquad (k,q=0,1,2,...)$ $\mathcal{S}^\beta: |x^k\varphi^{(q)}(x)|\le C_kB^qq^{q\beta}\qquad (k,q=0,1,2,...)$ $\mathcal{S}_\alpha^\beta: |x^k\varphi^{(q)}(x)|\le CA^kB^qk^{k\alpha}q^{q\beta}\qquad (k,q=0,1,2,...)$ where the constants $A,B,C_k,C_q$ depend on $\varphi \:\&\:\alpha+\beta\ge1$. In the first space we have functions which are rapidly decaying as $|x|\to \infty$. The second space imposes conditions on the growth of derivatives of these functions as $|x|\to \infty$. The third space $$\mathcal{S}_\alpha^\beta\subset\mathcal{S}_\alpha\cap \mathcal{S}^\beta$$In general the converse also holds which is a classical result due to Kashpirovsky. I want to construct examples to get a better understanding of these definitions. From the definition it is clear that every function in these spaces is at least $C_c^\infty$. So intuitively the first thing that comes to my mind is Gaussian-like functions. Now say I consider $\mathcal{S}_1^2(\mathbb{R})$. Which type of functions belong to this space ? How to come up with suitable constants ? Reference: Generalized Functions, Volume 2 by I.M.Gelfand and G.E. Shilov",,"['real-analysis', 'functional-analysis', 'gelfand-shilov-spaces']"
8,"Let $f: A \to \mathbb{R^n}$ be of class $C^r$; $Df(x)$ is non-singular for $x\in A$. Show that even if $f$ is not 1-1, the set $B=f(A)$ is open.","Let  be of class ;  is non-singular for . Show that even if  is not 1-1, the set  is open.",f: A \to \mathbb{R^n} C^r Df(x) x\in A f B=f(A),"Let $A$ be open in $\mathbb{R^n}$; let $f: A \to \mathbb{R^n}$ be of class $C^r$; assume $Df(x)$ is non-singular for $x\in A$. Show that even if $f$ is not one-to-one on $A$, the set $B=f(A)$ is open in $\mathbb{R^n}$. I tried to solve this using the inverse function theorem. The theorem from my text states Theorem : Let $A$ be open in $\mathbb{R^n}$; let $f: A \to \mathbb{R^n}$ be of class $C^r$. If $Df(x)$ is non-singular at the point $a$ of $A$, there is a neighborhood $U$ of the point $a$ such that $f$ carries $U$ in a one-to-one fashion onto an open set $V$ of $\mathbb{R^n}$ and the inverse function is of class $C^r$. Solution : Using this theorem, for each $b\in B=f(A)$, choosing any $a\in A$ such that $f(a)=b$, since $Df(a)$ is nonsingular, there is a neighborhood $U_a$ of $a$ such that $f$ carries $U_a$ in a one-to-one fashion onto a neighborhood $V_b=f(U_a)\subset B$ of $b$. Since we can do this for every $b\in B$, $f(A)=\bigcup_{b\in B}V_b$, so $f(A)=B$ being the union of open sets, is also open in $R^n$. I'm not sure if my solution is correct. I would appreciate anyone's verification. Also, is there any other way to approach this problem?","Let $A$ be open in $\mathbb{R^n}$; let $f: A \to \mathbb{R^n}$ be of class $C^r$; assume $Df(x)$ is non-singular for $x\in A$. Show that even if $f$ is not one-to-one on $A$, the set $B=f(A)$ is open in $\mathbb{R^n}$. I tried to solve this using the inverse function theorem. The theorem from my text states Theorem : Let $A$ be open in $\mathbb{R^n}$; let $f: A \to \mathbb{R^n}$ be of class $C^r$. If $Df(x)$ is non-singular at the point $a$ of $A$, there is a neighborhood $U$ of the point $a$ such that $f$ carries $U$ in a one-to-one fashion onto an open set $V$ of $\mathbb{R^n}$ and the inverse function is of class $C^r$. Solution : Using this theorem, for each $b\in B=f(A)$, choosing any $a\in A$ such that $f(a)=b$, since $Df(a)$ is nonsingular, there is a neighborhood $U_a$ of $a$ such that $f$ carries $U_a$ in a one-to-one fashion onto a neighborhood $V_b=f(U_a)\subset B$ of $b$. Since we can do this for every $b\in B$, $f(A)=\bigcup_{b\in B}V_b$, so $f(A)=B$ being the union of open sets, is also open in $R^n$. I'm not sure if my solution is correct. I would appreciate anyone's verification. Also, is there any other way to approach this problem?",,"['real-analysis', 'analysis', 'multivariable-calculus', 'vector-analysis']"
9,Question regarding the Kolmogorov-Riesz theorem on relatively compact subsets of $L^p(\Omega)$.,Question regarding the Kolmogorov-Riesz theorem on relatively compact subsets of .,L^p(\Omega),"Usually, the Kolmogorov-Riesz theorem is quoted for $L^p(\mathbb R^n)$, but I am looking for versions considering spaces over subsets in $\mathbb R^n$. The following is from the book ""Sobolev spaces"" by Adams and Fournier: Theorem. Let $1\leq p < \infty$. A bounded subset $K \subseteq L^p(\Omega)$ with $\Omega \subseteq \mathbb R^n$ a domain is precompact in $L^p(\Omega)$ if and only if for every number $\varepsilon >0$ there exists a number $\delta >0$ and a subset $G \subset \subset \Omega$ such that for every $u\in K$ and $h \in \mathbb R^n$ with $|h| < \delta$ both of the following inequalities hold:   $$\int_\Omega |\tilde u(x+h) -\tilde u(x)|^p\,dx <\varepsilon^p \quad \text{and} \quad \int_{\Omega \setminus \overline G} |u(x)|^p\,dx < \varepsilon^p.$$ Here, $\tilde u$ is defined by $\tilde u = u$ on $\Omega$ and $\tilde u = 0$ on $\mathbb R^n\setminus \Omega$. The theorem is obtained by proving the version for $\mathbb R^n$ and applying this to $\tilde K = \{\tilde u : u \in K\}$. In a book by Brezis (""Analisis functional, Theoria y aplicaciones""), the following theorem is stated: Corollary. Let $\Omega \subseteq \mathbb R^n$ be open and let $\mathscr F$ be a bounded subset of $L^p(\Omega)$ with $1\leq p <\infty$. Suppose that $\forall \varepsilon >0 \; \forall \omega \subset \subset \Omega,\;\exists \delta >0,\;\delta < \mathrm{dist}(\omega,\Omega^c) \text{ such that } \|\tau_h f -f\|_{L^p(\omega)} < \varepsilon \quad \forall |h|<\delta \text{ and } f\in\mathscr{F}.$ $\forall \varepsilon >0 \; \exists \omega \subset \subset \Omega \text{ such that } \|f\|_{L^p(\Omega\setminus\omega)} < \varepsilon \quad \forall f \in \mathscr F$ Then $\mathscr F$ is relatively compact in $L^p(\Omega)$. Here, $\tau_h$ denotes the operator $\tau_h f(x) = f(x+h)$. (Disclaimer: I translated this from the Spanish version of the book and I really don't know any Spanish (the original version is in French; unfortunately there is no English version).) The crucial part here is that the first property need only be tested on $\omega \subset \subset \Omega$. Using the Theorem from Adams and Fournier, the statement of Brezis should also characterize relative compactness, since $\|\tau_h f - f\|_{L^p(\omega)} \leq \|\tau_h \tilde f - \tilde f\|_{L^p(\Omega)}$ for every $\omega \subset \subset \Omega$. My question is this: Does anybody know if there are other references (preferably in English) where relative compactness is characterized by the properties in the Corollary of Brezis? As a follow-up, are there references which deal with relative compactness in the case of weighted $L^p$-spaces? Specifically, I'm interested in the case where the weight takes the form $e^{-\varphi}$ for some $C^2$ function $\varphi$. One probably could take the proofs for non-weighted spaces and adapt them, but I guess one would have to deal with adjusting the necessary tools (e.g. smoothing by convolution) to the weighted case, hence a reference would be nice. Thanks in advance!","Usually, the Kolmogorov-Riesz theorem is quoted for $L^p(\mathbb R^n)$, but I am looking for versions considering spaces over subsets in $\mathbb R^n$. The following is from the book ""Sobolev spaces"" by Adams and Fournier: Theorem. Let $1\leq p < \infty$. A bounded subset $K \subseteq L^p(\Omega)$ with $\Omega \subseteq \mathbb R^n$ a domain is precompact in $L^p(\Omega)$ if and only if for every number $\varepsilon >0$ there exists a number $\delta >0$ and a subset $G \subset \subset \Omega$ such that for every $u\in K$ and $h \in \mathbb R^n$ with $|h| < \delta$ both of the following inequalities hold:   $$\int_\Omega |\tilde u(x+h) -\tilde u(x)|^p\,dx <\varepsilon^p \quad \text{and} \quad \int_{\Omega \setminus \overline G} |u(x)|^p\,dx < \varepsilon^p.$$ Here, $\tilde u$ is defined by $\tilde u = u$ on $\Omega$ and $\tilde u = 0$ on $\mathbb R^n\setminus \Omega$. The theorem is obtained by proving the version for $\mathbb R^n$ and applying this to $\tilde K = \{\tilde u : u \in K\}$. In a book by Brezis (""Analisis functional, Theoria y aplicaciones""), the following theorem is stated: Corollary. Let $\Omega \subseteq \mathbb R^n$ be open and let $\mathscr F$ be a bounded subset of $L^p(\Omega)$ with $1\leq p <\infty$. Suppose that $\forall \varepsilon >0 \; \forall \omega \subset \subset \Omega,\;\exists \delta >0,\;\delta < \mathrm{dist}(\omega,\Omega^c) \text{ such that } \|\tau_h f -f\|_{L^p(\omega)} < \varepsilon \quad \forall |h|<\delta \text{ and } f\in\mathscr{F}.$ $\forall \varepsilon >0 \; \exists \omega \subset \subset \Omega \text{ such that } \|f\|_{L^p(\Omega\setminus\omega)} < \varepsilon \quad \forall f \in \mathscr F$ Then $\mathscr F$ is relatively compact in $L^p(\Omega)$. Here, $\tau_h$ denotes the operator $\tau_h f(x) = f(x+h)$. (Disclaimer: I translated this from the Spanish version of the book and I really don't know any Spanish (the original version is in French; unfortunately there is no English version).) The crucial part here is that the first property need only be tested on $\omega \subset \subset \Omega$. Using the Theorem from Adams and Fournier, the statement of Brezis should also characterize relative compactness, since $\|\tau_h f - f\|_{L^p(\omega)} \leq \|\tau_h \tilde f - \tilde f\|_{L^p(\Omega)}$ for every $\omega \subset \subset \Omega$. My question is this: Does anybody know if there are other references (preferably in English) where relative compactness is characterized by the properties in the Corollary of Brezis? As a follow-up, are there references which deal with relative compactness in the case of weighted $L^p$-spaces? Specifically, I'm interested in the case where the weight takes the form $e^{-\varphi}$ for some $C^2$ function $\varphi$. One probably could take the proofs for non-weighted spaces and adapt them, but I guess one would have to deal with adjusting the necessary tools (e.g. smoothing by convolution) to the weighted case, hence a reference would be nice. Thanks in advance!",,"['real-analysis', 'functional-analysis', 'reference-request', 'lp-spaces']"
10,Combinatorial Proofs of Real Analysis Identity,Combinatorial Proofs of Real Analysis Identity,,"In this question , a proof using real analysis is given of the following identity: $$ \sum_{n=1}^{\infty} \frac{(n-1)!}{n \prod_{i=1}^{n} (a+i)} = \sum_{k=1}^{\infty} \frac{1}{(a+k)^2}$$ Is there a combinatorial proof of this identity? Is so, does the proof require that $a$ be a natural number? Also is there an easy way to verify if combinatorial proofs exist of particular identities?","In this question , a proof using real analysis is given of the following identity: $$ \sum_{n=1}^{\infty} \frac{(n-1)!}{n \prod_{i=1}^{n} (a+i)} = \sum_{k=1}^{\infty} \frac{1}{(a+k)^2}$$ Is there a combinatorial proof of this identity? Is so, does the proof require that $a$ be a natural number? Also is there an easy way to verify if combinatorial proofs exist of particular identities?",,"['real-analysis', 'sequences-and-series', 'combinatorics', 'partial-fractions']"
11,Approximating the area below average of a concave function,Approximating the area below average of a concave function,,"Given a non-decreasing concave function $f:[0,1]\rightarrow \mathbb{R}^+$. Define \begin{align*} F(n)=\displaystyle\sum_{i=1}^n \min\left\{\frac{1}{n},\frac{f\left(\frac{i}{n+1}\right)}{n+1}\right\} \end{align*} We want to find a tight lower bound for $F(n)$ in terms of $n$, knowing that $\int_0^{1} f(x) dx =1 $. For instance, I am able to prove that $F(1)\geq \frac{1}{2}$, $F(2)\geq \frac{2}{3}$, $F(3)\geq \frac{1}{3}+\frac{3}{8}$ and see that they are tight for the function $f(x)=2x$. Also, I can prove that $\displaystyle\lim_{n\rightarrow \infty} F(n) \geq 3/4$, and again observe that it is tight for the function $f(x)=2x$.   Thanks for your time and attention!","Given a non-decreasing concave function $f:[0,1]\rightarrow \mathbb{R}^+$. Define \begin{align*} F(n)=\displaystyle\sum_{i=1}^n \min\left\{\frac{1}{n},\frac{f\left(\frac{i}{n+1}\right)}{n+1}\right\} \end{align*} We want to find a tight lower bound for $F(n)$ in terms of $n$, knowing that $\int_0^{1} f(x) dx =1 $. For instance, I am able to prove that $F(1)\geq \frac{1}{2}$, $F(2)\geq \frac{2}{3}$, $F(3)\geq \frac{1}{3}+\frac{3}{8}$ and see that they are tight for the function $f(x)=2x$. Also, I can prove that $\displaystyle\lim_{n\rightarrow \infty} F(n) \geq 3/4$, and again observe that it is tight for the function $f(x)=2x$.   Thanks for your time and attention!",,"['real-analysis', 'functions', 'convex-analysis']"
12,Show that that $\lim_{n\to\infty}\sqrt[n]{\binom{2n}{n}} = 4$,Show that that,\lim_{n\to\infty}\sqrt[n]{\binom{2n}{n}} = 4,"I know that $$ \lim_{n\to\infty}{{2n}\choose{n}}^\frac{1}{n} = 4 $$ but I have no Idea how to show that; I think it has something to do with reducing ${n}!$ to $n^n$ in the limit, but don't know how to get there. How might I prove that the limit is four?","I know that $$ \lim_{n\to\infty}{{2n}\choose{n}}^\frac{1}{n} = 4 $$ but I have no Idea how to show that; I think it has something to do with reducing ${n}!$ to $n^n$ in the limit, but don't know how to get there. How might I prove that the limit is four?",,"['calculus', 'real-analysis', 'combinatorics', 'limits', 'catalan-numbers']"
13,"For every rational number, does there exist a sequence of irrationals which converges to it?","For every rational number, does there exist a sequence of irrationals which converges to it?",,I can think of of examples where a sequence of irrationals converges to $0$. But if we pick any rational will there always exist a sequence of irrationals which converges to it? I cannot find a straight answer to this question.,I can think of of examples where a sequence of irrationals converges to $0$. But if we pick any rational will there always exist a sequence of irrationals which converges to it? I cannot find a straight answer to this question.,,['real-analysis']
14,Proving that the Union of Two Compact Sets is Compact,Proving that the Union of Two Compact Sets is Compact,,"Prove if $S_1,S_2$ are compact, then their union $S_1\cup S_2$ is compact as well. The attempt at a proof: Since $S_1$ and $S_2$ are compact, every open cover contains a finite subcover. Let the open cover of $S_1$ and $S_2$ be $\mathscr{F}_1$ and $\mathscr{F}_2$, and let the finite subcover of $\mathscr{F}_1$ and $\mathscr{F}_2$ be $\mathscr{G}_1$ and $\mathscr{G}_2$, respectively. If I can show that $S_1\cup S_2$ contains a finite subcover for every open cover, then I will have showed that the union is indeed compact. I note that $\mathscr{G}_1\subset\mathscr{F}_1$ and that $\mathscr{G}_2\subset\mathscr{F}_2$ (by definition of an open subcover). Then, I note that $\mathscr{G}_1\subset\mathscr{F}_1\cup\mathscr{F}_2$ and that $\mathscr{G}_2\subset\mathscr{F}_1\cup\mathscr{F}_2.$ I'm not sure how to proceed from this point. I think I am on the right track though. Any suggestions to proceed would be appreciated. Thanks in advance.","Prove if $S_1,S_2$ are compact, then their union $S_1\cup S_2$ is compact as well. The attempt at a proof: Since $S_1$ and $S_2$ are compact, every open cover contains a finite subcover. Let the open cover of $S_1$ and $S_2$ be $\mathscr{F}_1$ and $\mathscr{F}_2$, and let the finite subcover of $\mathscr{F}_1$ and $\mathscr{F}_2$ be $\mathscr{G}_1$ and $\mathscr{G}_2$, respectively. If I can show that $S_1\cup S_2$ contains a finite subcover for every open cover, then I will have showed that the union is indeed compact. I note that $\mathscr{G}_1\subset\mathscr{F}_1$ and that $\mathscr{G}_2\subset\mathscr{F}_2$ (by definition of an open subcover). Then, I note that $\mathscr{G}_1\subset\mathscr{F}_1\cup\mathscr{F}_2$ and that $\mathscr{G}_2\subset\mathscr{F}_1\cup\mathscr{F}_2.$ I'm not sure how to proceed from this point. I think I am on the right track though. Any suggestions to proceed would be appreciated. Thanks in advance.",,"['real-analysis', 'compactness']"
15,How do I prove that $3<\pi<4$?,How do I prove that ?,3<\pi<4,Let's not invoke the polynomial expansion of $\arctan$ function. I remember I saw somewhere here a very simple proof showing that $3<\pi<4$ but I don't remember where I saw it.. (I remember that this proof is also in Wikipedia) How do I prove this inequality? My definition for $\pi$ is the twice the first positive real number such that $\cos x= 0$ where $\cos x = \frac{e^{ix} + e^{-ix}}{2}$,Let's not invoke the polynomial expansion of $\arctan$ function. I remember I saw somewhere here a very simple proof showing that $3<\pi<4$ but I don't remember where I saw it.. (I remember that this proof is also in Wikipedia) How do I prove this inequality? My definition for $\pi$ is the twice the first positive real number such that $\cos x= 0$ where $\cos x = \frac{e^{ix} + e^{-ix}}{2}$,,"['real-analysis', 'inequality', 'pi']"
16,Polynomial cannot have all roots real?,Polynomial cannot have all roots real?,,"Let $P \in \mathbb R[x]$ be a degree- $n$ polynomial with real coefficients such that $P(a) \neq 0$ , where $a$ is real. If $P'(a) = P ''(a) = 0$ then prove that $P$ cannot have all roots real. Can someone suggest a possible solution using Rolle's Theorem ? All I could gather was that $P'(x) = 0$ has a repeated root by Rolle's Theorem. But I am stuck after this.","Let be a degree- polynomial with real coefficients such that , where is real. If then prove that cannot have all roots real. Can someone suggest a possible solution using Rolle's Theorem ? All I could gather was that has a repeated root by Rolle's Theorem. But I am stuck after this.",P \in \mathbb R[x] n P(a) \neq 0 a P'(a) = P ''(a) = 0 P P'(x) = 0,"['real-analysis', 'calculus', 'polynomials', 'roots']"
17,How to show that $\sum_{n=1}^{\infty} \frac{1}{(2n+1)(2n+2)(2n+3)}=\ln(2)-1/2$?,How to show that ?,\sum_{n=1}^{\infty} \frac{1}{(2n+1)(2n+2)(2n+3)}=\ln(2)-1/2,How i can prove that $$ \sum_{n=1}^{\infty} \frac{1}{(2n+1)(2n+2)(2n+3)}=\ln(2)-1/2 $$ And $$ \sum_{n=1}^{\infty} \frac{1}{(4n+1)(4n+2)(4n+3)(4n+4)}=\frac{1}{4}\left(\ln(2) - \frac{\pi}{6}\right). $$ Thanks in advance.,How i can prove that $$ \sum_{n=1}^{\infty} \frac{1}{(2n+1)(2n+2)(2n+3)}=\ln(2)-1/2 $$ And $$ \sum_{n=1}^{\infty} \frac{1}{(4n+1)(4n+2)(4n+3)(4n+4)}=\frac{1}{4}\left(\ln(2) - \frac{\pi}{6}\right). $$ Thanks in advance.,,"['real-analysis', 'sequences-and-series']"
18,$\sum a_n$ converges $\implies\ \sum a_n^2$ converges? [duplicate],converges  converges? [duplicate],\sum a_n \implies\ \sum a_n^2,"This question already has answers here : Prove that if $\sum{a_n}$ converges absolutely, then $\sum{a_n^2}$ converges absolutely (3 answers) Series proof $\sum_1^\infty|a_n|<\infty$ then show that $\sum_1^\infty{a_n^2}<\infty$ [duplicate] (3 answers) Closed 9 years ago . If $\sum a_n$ with $a_n>0$ is convergent, then is $\sum {a_n}^2$ always convergent? Either prove it or give a counter example. Im trying in this way, Suppose $a_n \in [0,1] \ \forall\  n.\ $ Then ${a_n}^2\leq a_n\ \forall\  n.$ Therefore by comparison test $\sum {a_n}^2$ converges. So If $a_n$ has certain restrictions then the result is true.  what about the general case? How to proceed further? Hints will be greatly appreciated.","This question already has answers here : Prove that if $\sum{a_n}$ converges absolutely, then $\sum{a_n^2}$ converges absolutely (3 answers) Series proof $\sum_1^\infty|a_n|<\infty$ then show that $\sum_1^\infty{a_n^2}<\infty$ [duplicate] (3 answers) Closed 9 years ago . If $\sum a_n$ with $a_n>0$ is convergent, then is $\sum {a_n}^2$ always convergent? Either prove it or give a counter example. Im trying in this way, Suppose $a_n \in [0,1] \ \forall\  n.\ $ Then ${a_n}^2\leq a_n\ \forall\  n.$ Therefore by comparison test $\sum {a_n}^2$ converges. So If $a_n$ has certain restrictions then the result is true.  what about the general case? How to proceed further? Hints will be greatly appreciated.",,"['real-analysis', 'sequences-and-series']"
19,Find $\lim_{n \to +\infty} \frac{1}{n}\sqrt[n]{\frac{(2n)!}{n!}} $ using Riemann integral [duplicate],Find  using Riemann integral [duplicate],\lim_{n \to +\infty} \frac{1}{n}\sqrt[n]{\frac{(2n)!}{n!}} ,"This question already has an answer here : Evaluate the given limit by recognizing it as a Riemann sum. (1 answer) Closed 2 years ago . Wonder how to determine this limit by the use of Riemann integral. The limit is as follows: $$\lim_{n \to +\infty} \frac{1}{n}\sqrt[n]{\frac{(2n)!}{n!}} $$ My instructor told me that the usage of Riemann integral gives spectacular result. Checked Rudin, but did not find any valuable references. I am very interested in seeing how this ""spectacular result"" emanates. Help/advices/solutions very, very appreciated!","This question already has an answer here : Evaluate the given limit by recognizing it as a Riemann sum. (1 answer) Closed 2 years ago . Wonder how to determine this limit by the use of Riemann integral. The limit is as follows: $$\lim_{n \to +\infty} \frac{1}{n}\sqrt[n]{\frac{(2n)!}{n!}} $$ My instructor told me that the usage of Riemann integral gives spectacular result. Checked Rudin, but did not find any valuable references. I am very interested in seeing how this ""spectacular result"" emanates. Help/advices/solutions very, very appreciated!",,"['calculus', 'real-analysis', 'limits', 'factorial', 'radicals']"
20,$\epsilon$ - $\delta$ definition of a limit - smaller $\epsilon$ implies smaller $\delta$?,-  definition of a limit - smaller  implies smaller ?,\epsilon \delta \epsilon \delta,"The definition in my book is as follows: Let $f$ be a function defined on an open interval containing $c$ (except possibly at $c$) and let $L$ be a real number. The statement $$\lim_{x \to c} f(x) = L$$ means that for each $\epsilon>0$ there exists a $\delta>0$ such that if $0<|x-c|<\delta$, then $|f(x)-L|<\epsilon$. With the definition the way it is, I don't see how choosing a smaller and smaller $\epsilon$ implies a smaller and smaller $\delta$. To me, in order to produce that implication, we would need to restrict $\epsilon$ to be small enough to force $f(x)$ to be strictly increasing/decreasing on $(L-\epsilon, L+\epsilon)$, and define increasing/decreasing without the use of derivatives. However, that is not part of the definition. P.S. Please refrain from using too much notation for logic, I am not familiar with most of the symbols such as the upside down A and such.","The definition in my book is as follows: Let $f$ be a function defined on an open interval containing $c$ (except possibly at $c$) and let $L$ be a real number. The statement $$\lim_{x \to c} f(x) = L$$ means that for each $\epsilon>0$ there exists a $\delta>0$ such that if $0<|x-c|<\delta$, then $|f(x)-L|<\epsilon$. With the definition the way it is, I don't see how choosing a smaller and smaller $\epsilon$ implies a smaller and smaller $\delta$. To me, in order to produce that implication, we would need to restrict $\epsilon$ to be small enough to force $f(x)$ to be strictly increasing/decreasing on $(L-\epsilon, L+\epsilon)$, and define increasing/decreasing without the use of derivatives. However, that is not part of the definition. P.S. Please refrain from using too much notation for logic, I am not familiar with most of the symbols such as the upside down A and such.",,"['calculus', 'real-analysis', 'limits', 'definition', 'epsilon-delta']"
21,"Problem from the 2020 Latvian ""Sophomore's Dream"" competition [duplicate]","Problem from the 2020 Latvian ""Sophomore's Dream"" competition [duplicate]",,"This question already has an answer here : Evaluating $\int_{-a}^a \frac{f(x)+1}{b^x+1}\,\mathrm dx$ for a $y$-axis symmetrical function $f$ [duplicate] (1 answer) Closed 4 years ago . Evaluate $$\int_{-a\pi}^{a\pi} \frac{\cos^5(x)+1}{e^x+1}dx, \quad a \in \mathbb{N}$$ In the beginning, I didn't have any ideas of how to solve this. The one that later came to mind was to try using Feynman's technique, but I couldn't think of the proper function to use for the second variable. Any ideas?","This question already has an answer here : Evaluating $\int_{-a}^a \frac{f(x)+1}{b^x+1}\,\mathrm dx$ for a $y$-axis symmetrical function $f$ [duplicate] (1 answer) Closed 4 years ago . Evaluate In the beginning, I didn't have any ideas of how to solve this. The one that later came to mind was to try using Feynman's technique, but I couldn't think of the proper function to use for the second variable. Any ideas?","\int_{-a\pi}^{a\pi} \frac{\cos^5(x)+1}{e^x+1}dx, \quad a \in \mathbb{N}","['real-analysis', 'calculus', 'integration', 'definite-integrals', 'contest-math']"
22,Calculate sum of $\sum_{n=1}^{\infty}(-1)^n\frac{\ln n}{n}$,Calculate sum of,\sum_{n=1}^{\infty}(-1)^n\frac{\ln n}{n},What is the exact sum of $$\sum_{n=1}^{\infty}(-1)^n\frac{\ln n}{n}$$,What is the exact sum of $$\sum_{n=1}^{\infty}(-1)^n\frac{\ln n}{n}$$,,['real-analysis']
23,Where is the flaw in my proof that the union of two continuous functions is continuous?,Where is the flaw in my proof that the union of two continuous functions is continuous?,,"Problem. Let $f:A\to\mathbb{R}$ be a continuous function on $A$ and $g:B\to\mathbb{R}$ be a continuous function on $B$ such that $A\cap B=\emptyset$. Let $h:A\cup B\to\mathbb{R}$ be defined by, $$h(x)=\begin{cases}f(x)& x\in A\\ g(x)& x\in B\end{cases}$$Is $h$ continuous on $A\cup B$? Proof. Let $(x_n)_{n\ge1}$ be any sequence from $A\cup B$ converging to $c\in A\cup B$. Let us now form two subsequences of $(x_n)_{n\ge1}$, namely $(y_n)_{n\ge1}$ and $(z_n)_{n\ge1}$ such that $y_n\in A$ and $z_n\in B$ for all $n\in \mathbb{N}$. Then clearly $(h(y_n))_{n\ge1}\to h(c)$ since $h(y_n)=f(y_n)$ for all $n\in\mathbb{N}$ and $(h(z_n))_{n\ge1}\to h(c)$ since $h(z_n)=g(y_n)$ for all $n\in\mathbb{N}$. Consequently $(h(x_n))_{n\ge1}\to h(c)$ and we are done. But the problem is that when I told our professor about this proof he told me that there should be some other conditions. But the argument seems to work well. Where is the flaw (if any) in my argument?","Problem. Let $f:A\to\mathbb{R}$ be a continuous function on $A$ and $g:B\to\mathbb{R}$ be a continuous function on $B$ such that $A\cap B=\emptyset$. Let $h:A\cup B\to\mathbb{R}$ be defined by, $$h(x)=\begin{cases}f(x)& x\in A\\ g(x)& x\in B\end{cases}$$Is $h$ continuous on $A\cup B$? Proof. Let $(x_n)_{n\ge1}$ be any sequence from $A\cup B$ converging to $c\in A\cup B$. Let us now form two subsequences of $(x_n)_{n\ge1}$, namely $(y_n)_{n\ge1}$ and $(z_n)_{n\ge1}$ such that $y_n\in A$ and $z_n\in B$ for all $n\in \mathbb{N}$. Then clearly $(h(y_n))_{n\ge1}\to h(c)$ since $h(y_n)=f(y_n)$ for all $n\in\mathbb{N}$ and $(h(z_n))_{n\ge1}\to h(c)$ since $h(z_n)=g(y_n)$ for all $n\in\mathbb{N}$. Consequently $(h(x_n))_{n\ge1}\to h(c)$ and we are done. But the problem is that when I told our professor about this proof he told me that there should be some other conditions. But the argument seems to work well. Where is the flaw (if any) in my argument?",,['real-analysis']
24,"Pie Integral $\int_0^1 \log\frac{(x+\sqrt{1-x^2})^2}{(x-\sqrt{1-x^2})^2} \frac{x\, dx}{1-x^2}=\frac{\pi^2}{2}.$",Pie Integral,"\int_0^1 \log\frac{(x+\sqrt{1-x^2})^2}{(x-\sqrt{1-x^2})^2} \frac{x\, dx}{1-x^2}=\frac{\pi^2}{2}.","Hi I am trying to show this astonishing result$$ \int_0^1 \log\frac{\big(x+\sqrt{1-x^2}\big)^2}{\big(x-\sqrt{1-x^2}\big)^2} \frac{x\, dx}{1-x^2}=\frac{\pi^2}{2}. $$ Note we can to use $\ln(a/b)=\ln a-\ln b$ but that didn't help me much .  After this I obtained integrals of the form $$ \int_0^1 \log \big[\big(x\pm\sqrt{1-x^2}\big)^2\big] \frac{x\, dx}{1-x^2} $$ which I am not sure how to handle.  Thanks.","Hi I am trying to show this astonishing result$$ \int_0^1 \log\frac{\big(x+\sqrt{1-x^2}\big)^2}{\big(x-\sqrt{1-x^2}\big)^2} \frac{x\, dx}{1-x^2}=\frac{\pi^2}{2}. $$ Note we can to use $\ln(a/b)=\ln a-\ln b$ but that didn't help me much .  After this I obtained integrals of the form $$ \int_0^1 \log \big[\big(x\pm\sqrt{1-x^2}\big)^2\big] \frac{x\, dx}{1-x^2} $$ which I am not sure how to handle.  Thanks.",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
25,Closed ball is not compact,Closed ball is not compact,,"Show that the closed ball in $C([0,1])$ of center $0$ and radius $1$ is not compact. I thought it will be compact since every closed and bounded set in $\mathbb{R}$ is compact? Why is it not compact and how can I prove it?","Show that the closed ball in $C([0,1])$ of center $0$ and radius $1$ is not compact. I thought it will be compact since every closed and bounded set in $\mathbb{R}$ is compact? Why is it not compact and how can I prove it?",,['real-analysis']
26,For every real number $a$ there exists a sequence $r_n$ of rational numbers such that $r_n$ approaches $a$.,For every real number  there exists a sequence  of rational numbers such that  approaches .,a r_n r_n a,How to prove that for every real number $a$ there exists a sequence $r_n$ of rational numbers such that $r_n \rightarrow a$.,How to prove that for every real number $a$ there exists a sequence $r_n$ of rational numbers such that $r_n \rightarrow a$.,,['real-analysis']
27,Examples of involutions on $\mathbb{R}$,Examples of involutions on,\mathbb{R},"Just recently I read up something about involutions (functions $f: A \rightarrow A$ such that $f(f(x))=x$, for all $x$ in the domain of $f$), and was wondering how many (if there is a small set of general functions) such involutions exist for $A = \mathbb{R}$, or maybe $A = \mathbb{R} - S$, where S is some set of points that would make the involution work if they were left out of $A$. In general I'm interested in real functions that are involutions, continuous or otherwise. There were some examples I found here , but any more would be certainly very interesting!","Just recently I read up something about involutions (functions $f: A \rightarrow A$ such that $f(f(x))=x$, for all $x$ in the domain of $f$), and was wondering how many (if there is a small set of general functions) such involutions exist for $A = \mathbb{R}$, or maybe $A = \mathbb{R} - S$, where S is some set of points that would make the involution work if they were left out of $A$. In general I'm interested in real functions that are involutions, continuous or otherwise. There were some examples I found here , but any more would be certainly very interesting!",,"['real-analysis', 'elementary-set-theory']"
28,How to evaluate the sum $\frac{1}{\sqrt{n}}+\frac{1}{\sqrt{2n}}+\cdots+\frac{1}{\sqrt{n^2}}$ when $n$ grows?,How to evaluate the sum  when  grows?,\frac{1}{\sqrt{n}}+\frac{1}{\sqrt{2n}}+\cdots+\frac{1}{\sqrt{n^2}} n,I need help with the following limit $$\lim_{n\to\infty}\sum_{k=1}^n \frac{1}{\sqrt{kn}}$$ Thanks.,I need help with the following limit $$\lim_{n\to\infty}\sum_{k=1}^n \frac{1}{\sqrt{kn}}$$ Thanks.,,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
29,Characterising Continuous functions,Characterising Continuous functions,,"We know that if $f : \mathbb{R} \to \mathbb{R}$ is a continuous function, then $f$ carries connected sets to connected sets and compact sets to compact sets. That is if $A \subset \mathbb{R}$ is connected then $f(A)$ is connected, and if $A$ is compact then $f(A)$ is compact. Question: Suppose $f: \mathbb{R} \to \mathbb{R}$ is a function such that for every connected, compact subsets $A \subset \mathbb{R}$, $f(A)$ is connected, compact, then is $f$ continuous? If yes, i would like to see a proof. Update : Does this result remain true if $f: \mathbb{R}^{2} \to \mathbb{R}$, or from any $f: \mathbb{R}^{m} \to \mathbb{R}^{n}$.","We know that if $f : \mathbb{R} \to \mathbb{R}$ is a continuous function, then $f$ carries connected sets to connected sets and compact sets to compact sets. That is if $A \subset \mathbb{R}$ is connected then $f(A)$ is connected, and if $A$ is compact then $f(A)$ is compact. Question: Suppose $f: \mathbb{R} \to \mathbb{R}$ is a function such that for every connected, compact subsets $A \subset \mathbb{R}$, $f(A)$ is connected, compact, then is $f$ continuous? If yes, i would like to see a proof. Update : Does this result remain true if $f: \mathbb{R}^{2} \to \mathbb{R}$, or from any $f: \mathbb{R}^{m} \to \mathbb{R}^{n}$.",,['real-analysis']
30,If $f\circ g$ is continuous and $g$ is continuous what about $f$?,If  is continuous and  is continuous what about ?,f\circ g g f,I don't know if $f$ is continuous. I believe that isn't necessarly continuous but I don't know some example. If it is continuous I don't know how to prove.,I don't know if $f$ is continuous. I believe that isn't necessarly continuous but I don't know some example. If it is continuous I don't know how to prove.,,"['real-analysis', 'general-topology']"
31,Proof involving norm of an integral,Proof involving norm of an integral,,"I am totally stuck and have no idea whatsoever on how to prove the following inequality (by the way this is a problem from an undergraduate book in multivariable advanced calculus at Junior/Senior level ): Let $g=\left ( g_{1},g_{2},...,g_{n} \right ): \left [ a,b \right ]\rightarrow \mathbb{R}^{n}$ is a continuous function, then we define: $\int_{a}^{b}g\left ( x \right )dx=\left \langle \int_{a}^{b}g_{1}\left ( x \right )dx,...,\int_{a}^{b}g_{n}\left ( x \right ) \right \rangle$ Prove that: $\left \| \int_{a}^{b}g\left ( x \right )dx \right \|\leq \int_{a}^{b}\left \| g\left ( x \right ) \right \|dx$ In the book, there is a hint saying that I should use the Cauchy Schwarz inequality, but I have no clue how to use it. The only I was able to prove is: Left hand side= $\sqrt{\left (\int_{a}^{b}g_{1}\left ( x \right )dx  \right )^{2}+...+\left ( \int_{a}^{b}g_{2}\left ( x \right )dx \right )^{2}}$ Right hand side is= $\int_{a}^{b}\sqrt{\left (g_{1}\left ( x \right )  \right )^{2}+...+\left ( g_{n}\left ( x \right ) \right )^{2}}dx$ I am looking forward for your suggestions and answers.","I am totally stuck and have no idea whatsoever on how to prove the following inequality (by the way this is a problem from an undergraduate book in multivariable advanced calculus at Junior/Senior level ): Let $g=\left ( g_{1},g_{2},...,g_{n} \right ): \left [ a,b \right ]\rightarrow \mathbb{R}^{n}$ is a continuous function, then we define: $\int_{a}^{b}g\left ( x \right )dx=\left \langle \int_{a}^{b}g_{1}\left ( x \right )dx,...,\int_{a}^{b}g_{n}\left ( x \right ) \right \rangle$ Prove that: $\left \| \int_{a}^{b}g\left ( x \right )dx \right \|\leq \int_{a}^{b}\left \| g\left ( x \right ) \right \|dx$ In the book, there is a hint saying that I should use the Cauchy Schwarz inequality, but I have no clue how to use it. The only I was able to prove is: Left hand side= $\sqrt{\left (\int_{a}^{b}g_{1}\left ( x \right )dx  \right )^{2}+...+\left ( \int_{a}^{b}g_{2}\left ( x \right )dx \right )^{2}}$ Right hand side is= $\int_{a}^{b}\sqrt{\left (g_{1}\left ( x \right )  \right )^{2}+...+\left ( g_{n}\left ( x \right ) \right )^{2}}dx$ I am looking forward for your suggestions and answers.",,"['linear-algebra', 'real-analysis', 'analysis', 'multivariable-calculus']"
32,Can we express $\pi$ in terms of $\sum_{n=1}^\infty\frac1{n^2}$?,Can we express  in terms of ?,\pi \sum_{n=1}^\infty\frac1{n^2},Since $$\sum_{n=1}^{\infty} \frac{1}{n^2}=\frac{\pi^2}{6}$$ can we now express $\pi$ in terms of this series by multiplying by $6$ and taking the square root? If not why is this not true? I was wondering since I had an exam question that required to write $\pi$ in terms of some infinite sum. I did it exactly like this and got 0 points. So I thought maybe I'm doing something wrong by manipulating it this way,Since $$\sum_{n=1}^{\infty} \frac{1}{n^2}=\frac{\pi^2}{6}$$ can we now express $\pi$ in terms of this series by multiplying by $6$ and taking the square root? If not why is this not true? I was wondering since I had an exam question that required to write $\pi$ in terms of some infinite sum. I did it exactly like this and got 0 points. So I thought maybe I'm doing something wrong by manipulating it this way,,"['real-analysis', 'sequences-and-series', 'summation', 'pi']"
33,Integral $\int_0^{\infty} \frac{\ln \cos^2 x}{x^2}dx=-\pi$,Integral,\int_0^{\infty} \frac{\ln \cos^2 x}{x^2}dx=-\pi,"$$ I:=\int_0^{\infty} \frac{\ln \cos^2 x}{x^2}\text{d}x=-\pi. $$ Using $2\cos^2 x=1+\cos 2x$ failed me because I ran into two divergent integrals after using $\ln(ab)=\ln a + \ln b$ since I obtained $\int_0^\infty x^{-2}\text{d}x$ and $\int_0^\infty (1+\cos^2 x)\text{d}x $ which both diverge.  Perhaps we should try a complex analysis approach?  I also tried writing $$ I(\alpha)=\int_0^\infty \frac{\ln \cos^2 \alpha \,x}{x^2}\text{d}x $$ and obtained $$ -\frac{dI(\alpha)}{d\alpha}=2\int_0^\infty \frac{\tan \alpha x}{x}\text{d}x=\int_{-\infty}^\infty\frac{\tan \alpha x}{x}\text{d}x. $$ Taking a second derivative $$ I''(\alpha)=\int_{-\infty}^\infty {\sec^2 (\alpha x)}\, \text{d}x $$ Random Variable pointed out how to continue from the integral after the 1st derivative, but is it possible to work with this integral $\sec^2 \alpha x$ ?  Thanks","Using failed me because I ran into two divergent integrals after using since I obtained and which both diverge.  Perhaps we should try a complex analysis approach?  I also tried writing and obtained Taking a second derivative Random Variable pointed out how to continue from the integral after the 1st derivative, but is it possible to work with this integral ?  Thanks","
I:=\int_0^{\infty} \frac{\ln \cos^2 x}{x^2}\text{d}x=-\pi.
 2\cos^2 x=1+\cos 2x \ln(ab)=\ln a + \ln b \int_0^\infty x^{-2}\text{d}x \int_0^\infty (1+\cos^2 x)\text{d}x  
I(\alpha)=\int_0^\infty \frac{\ln \cos^2 \alpha \,x}{x^2}\text{d}x
 
-\frac{dI(\alpha)}{d\alpha}=2\int_0^\infty \frac{\tan \alpha x}{x}\text{d}x=\int_{-\infty}^\infty\frac{\tan \alpha x}{x}\text{d}x.
 
I''(\alpha)=\int_{-\infty}^\infty {\sec^2 (\alpha x)}\, \text{d}x
 \sec^2 \alpha x","['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
34,How do you prove that differentiability implies continuity with $\epsilon$-$\delta$ definition?,How do you prove that differentiability implies continuity with - definition?,\epsilon \delta,"How do you prove that differentiability implies continuity with $\epsilon$ - $\delta$ definition? I know that's a very common theorem in calculus but when I try to prove it with $\epsilon$ - $\delta$ definition of continuity, I found that it is not so obvious. Attempt: Let $f:\mathbb{R}\to\mathbb{R}$ be a function differentiable at point $$a \implies \forall \epsilon>0, \exists \delta>0 \text{ s.t. } \left|\frac{f(x)-f(a)}{x-a}-f'(a)\right|<\epsilon$$ for any $|x-a|<\delta$ . So what we want to show is that for all $\epsilon>0$ , we can find an $\delta>0$ such that $|f(x)-f(a)|<\epsilon$ for any $|x-a|< \delta$ . First of all, we can apply the triangula inequality $$|f(x)-f(a)|\le |f(x)-f(a)-f'(a)(x-a)|+|f'(a)(x-a)|<\epsilon+|f'(a)(x-a)|,$$ but I found that $|f'(a)(x-a)|$ could be very large even $\epsilon$ can be any real number.","How do you prove that differentiability implies continuity with - definition? I know that's a very common theorem in calculus but when I try to prove it with - definition of continuity, I found that it is not so obvious. Attempt: Let be a function differentiable at point for any . So what we want to show is that for all , we can find an such that for any . First of all, we can apply the triangula inequality but I found that could be very large even can be any real number.","\epsilon \delta \epsilon \delta f:\mathbb{R}\to\mathbb{R} a \implies \forall \epsilon>0, \exists \delta>0 \text{ s.t. } \left|\frac{f(x)-f(a)}{x-a}-f'(a)\right|<\epsilon |x-a|<\delta \epsilon>0 \delta>0 |f(x)-f(a)|<\epsilon |x-a|< \delta |f(x)-f(a)|\le |f(x)-f(a)-f'(a)(x-a)|+|f'(a)(x-a)|<\epsilon+|f'(a)(x-a)|, |f'(a)(x-a)| \epsilon","['calculus', 'real-analysis', 'analysis', 'derivatives', 'continuity']"
35,Period of 3 implies chaos,Period of 3 implies chaos,,"Let $f(x)$ be a continuous function from $\mathbb{R}\rightarrow\mathbb{R}$. Let's denote $k$-times repeated application of the function, $f(f(f(...f(x)...)))$ as $f^{(k)}(x)$. Let's call any $x$ a periodic point with period $n$ if $f^{(n)}(x)=x$. Is it true that if a point with period 3 exists, then points with all possible periods exist? In other words is it true that $$\exists x:f^{(3)}(x)=x\Rightarrow \forall n>0 \exists y:f^{(n)}(y)=y$$ and if so, why?","Let $f(x)$ be a continuous function from $\mathbb{R}\rightarrow\mathbb{R}$. Let's denote $k$-times repeated application of the function, $f(f(f(...f(x)...)))$ as $f^{(k)}(x)$. Let's call any $x$ a periodic point with period $n$ if $f^{(n)}(x)=x$. Is it true that if a point with period 3 exists, then points with all possible periods exist? In other words is it true that $$\exists x:f^{(3)}(x)=x\Rightarrow \forall n>0 \exists y:f^{(n)}(y)=y$$ and if so, why?",,"['real-analysis', 'dynamical-systems']"
36,"If $E$ has Lebesgue measure $0$, must there exist a translate such that $E\cap E+x=\varnothing$?","If  has Lebesgue measure , must there exist a translate such that ?",E 0 E\cap E+x=\varnothing,"If $y\in E\cap E+x$ then $y=h=h'+x \implies x=h-h'$ for $h,h'\in E$. Likewise, if $x=h-h'$, then $h\in E\cap E+x$. So that $E\cap E+x$ is non empty iff $x=h-h'$ where $h,h'\in E$. Thus, if there exists a real $x$ which cannot be expressed as above, the result is proven. The problem is I cannot prove that such an $x$ must exist. I have tried assuming that all real $x$ are of that form, but that gives me nothing except that $E$ must be uncountable and unbounded. Any hints?","If $y\in E\cap E+x$ then $y=h=h'+x \implies x=h-h'$ for $h,h'\in E$. Likewise, if $x=h-h'$, then $h\in E\cap E+x$. So that $E\cap E+x$ is non empty iff $x=h-h'$ where $h,h'\in E$. Thus, if there exists a real $x$ which cannot be expressed as above, the result is proven. The problem is I cannot prove that such an $x$ must exist. I have tried assuming that all real $x$ are of that form, but that gives me nothing except that $E$ must be uncountable and unbounded. Any hints?",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
37,Evaluate $\int_{0}^{+\infty }{\left( \frac{x}{{{\text{e}}^{x}}-{{\text{e}}^{-x}}}-\frac{1}{2} \right)\frac{1}{{{x}^{2}}}\text{d}x}$,Evaluate,\int_{0}^{+\infty }{\left( \frac{x}{{{\text{e}}^{x}}-{{\text{e}}^{-x}}}-\frac{1}{2} \right)\frac{1}{{{x}^{2}}}\text{d}x},Evaluate : $$\int_{0}^{+\infty }{\left( \frac{x}{{{\text{e}}^{x}}-{{\text{e}}^{-x}}}-\frac{1}{2} \right)\frac{1}{{{x}^{2}}}\text{d}x}$$,Evaluate : $$\int_{0}^{+\infty }{\left( \frac{x}{{{\text{e}}^{x}}-{{\text{e}}^{-x}}}-\frac{1}{2} \right)\frac{1}{{{x}^{2}}}\text{d}x}$$,,"['calculus', 'real-analysis', 'sequences-and-series', 'integration']"
38,Is a dense subset of the plane always dense in some line segment?,Is a dense subset of the plane always dense in some line segment?,,"Consider the following question: Given a dense set in the plane, does there always exist a line segment in which this set is dense? I have been puzzling over this for some time. Can someone help or give me some hints?","Consider the following question: Given a dense set in the plane, does there always exist a line segment in which this set is dense? I have been puzzling over this for some time. Can someone help or give me some hints?",,['real-analysis']
39,Irrationality of powers of $\pi$,Irrationality of powers of,\pi,"Everyone knows that $\pi$ is an irrational number, and one can refer to this page for the proof that $\pi^{2}$ is also irrational. What about the highers powers of $\pi$ , meaning is $\pi^{n}$ irrational for all $n \in \mathbb{N}$ or does there exists a $m \in \mathbb{N}$ when $\pi^{m}$ is rational.","Everyone knows that is an irrational number, and one can refer to this page for the proof that is also irrational. What about the highers powers of , meaning is irrational for all or does there exists a when is rational.",\pi \pi^{2} \pi \pi^{n} n \in \mathbb{N} m \in \mathbb{N} \pi^{m},['number-theory']
40,Are there continuous functions who are the same in an interval but differ in at least one other point?,Are there continuous functions who are the same in an interval but differ in at least one other point?,,"You are given a function $f: \mathbb{R}\rightarrow \mathbb{R}$ . Every derivative $\frac{d^n}{dx^n}(f(x)), \,n >0$ of the function is continuous. Is there a function $g: \mathbb{R}\rightarrow \mathbb{R}$ , for which every derivative $\frac{d^n}{dx^n}(g(x)), \,n >0$ is also continuous, such that: $$\forall x\in[a,b]: \, g(x) = f(x)\land \, \exists x \notin [a,b]: f(x) \neq g(x),\, a \neq b$$ Thanks! Edit: I asked the question because I intuitively wondered if there would be functions, which could behave like ""the same"" in a given interval, but then behave differently so that they start diverging or at least stopped being the same the anymore. The answer to this question gave me a bigger understanding of real analysis. If you would like to know which made me think about such a problem: Although this is a vague formulation, generally, this question asks if two (completely) different things can develop (themselves) to be exactly equal in at least one part of there whole existence...","You are given a function . Every derivative of the function is continuous. Is there a function , for which every derivative is also continuous, such that: Thanks! Edit: I asked the question because I intuitively wondered if there would be functions, which could behave like ""the same"" in a given interval, but then behave differently so that they start diverging or at least stopped being the same the anymore. The answer to this question gave me a bigger understanding of real analysis. If you would like to know which made me think about such a problem: Although this is a vague formulation, generally, this question asks if two (completely) different things can develop (themselves) to be exactly equal in at least one part of there whole existence...","f: \mathbb{R}\rightarrow \mathbb{R} \frac{d^n}{dx^n}(f(x)), \,n >0 g: \mathbb{R}\rightarrow \mathbb{R} \frac{d^n}{dx^n}(g(x)), \,n >0 \forall x\in[a,b]: \, g(x) = f(x)\land \, \exists x \notin [a,b]: f(x) \neq g(x),\, a \neq b","['real-analysis', 'calculus']"
41,Why is the topological definition of continuous the way it is?,Why is the topological definition of continuous the way it is?,,"I was learning the definition of continuous as: $f\colon X\to Y$ is continuous if $f^{-1}(U)$ is open for every open $U\subseteq Y$ For me this translates to the following implication: IF $U \subseteq Y$ is open THEN $f^{-1}(U)$ is open however, I would have expected the definition to be the other way round, i.e. with the 1st implication I defined. The reason for that is that just by looking at the metric space definition of continuous: $\exists q = f(p) \in Y, \forall \epsilon>0,\exists \delta >0, \forall x \in X, 0 < d(x,p) < \delta \implies d(f(x),q) < \epsilon$ seems to be talking about Balls (i.e. open sets) in X and then has a forward arrow for open sets in Y, so it seems natural to expect the direction of the implication to go in that way round. However, it does not. Why does it not go that way? Whats is wrong with the implication going from open in X to open in Y? And of course, why is the current direction the correct one? I think conceptually I might be even confused why the topological definition of continuous requires to start from things in the target space Y and then require things in the domain. Can't we just say map things from X to Y and have them be close? Why do we require to posit things about Y first in either definition for the definition of continuous to work properly ? I can't help but point out that this question The definition of continuous function in topology seems to be similar but perhaps lack the detailed discussion on the direction on the implication for me to really understand why the definition is not reversed or what happens if we do reverse it. The second answer there tries to make an attempt at explaining why we require $f^{-1}$ to preserve the property of openness but its not conceptually obvious to me why thats the case or whats going on. Any help? For whoever suggest to close the question, the question is quite clear: why is the reverse implication not the ""correct"" definition of continuous? As an additional important point I noticed is, pointing out the difference between open mapping and continuous function would be very useful . Note: I encountered this in baby Rudin, so thats as far as my background in analysis goes, i.e. metric spaces is my place of understanding. Extra confusion/Appendix: Conceptually, I think I've managed to nail what my main confusion is. In conceptual terms continuous functions are suppose to map ""nearby points to nearby points"" so for me its metric space definition makes sense in that sense. However, that doesn't seem obvious to me unless we equate ""open sets"" to be the definition of ""close by"". Balls are open but there are plenty of sets that are open but are not ""close by"", for example the union of two open balls. I think this is what is confusing me most. How is the topological def respecting that conceptual requirement?","I was learning the definition of continuous as: $f\colon X\to Y$ is continuous if $f^{-1}(U)$ is open for every open $U\subseteq Y$ For me this translates to the following implication: IF $U \subseteq Y$ is open THEN $f^{-1}(U)$ is open however, I would have expected the definition to be the other way round, i.e. with the 1st implication I defined. The reason for that is that just by looking at the metric space definition of continuous: $\exists q = f(p) \in Y, \forall \epsilon>0,\exists \delta >0, \forall x \in X, 0 < d(x,p) < \delta \implies d(f(x),q) < \epsilon$ seems to be talking about Balls (i.e. open sets) in X and then has a forward arrow for open sets in Y, so it seems natural to expect the direction of the implication to go in that way round. However, it does not. Why does it not go that way? Whats is wrong with the implication going from open in X to open in Y? And of course, why is the current direction the correct one? I think conceptually I might be even confused why the topological definition of continuous requires to start from things in the target space Y and then require things in the domain. Can't we just say map things from X to Y and have them be close? Why do we require to posit things about Y first in either definition for the definition of continuous to work properly ? I can't help but point out that this question The definition of continuous function in topology seems to be similar but perhaps lack the detailed discussion on the direction on the implication for me to really understand why the definition is not reversed or what happens if we do reverse it. The second answer there tries to make an attempt at explaining why we require $f^{-1}$ to preserve the property of openness but its not conceptually obvious to me why thats the case or whats going on. Any help? For whoever suggest to close the question, the question is quite clear: why is the reverse implication not the ""correct"" definition of continuous? As an additional important point I noticed is, pointing out the difference between open mapping and continuous function would be very useful . Note: I encountered this in baby Rudin, so thats as far as my background in analysis goes, i.e. metric spaces is my place of understanding. Extra confusion/Appendix: Conceptually, I think I've managed to nail what my main confusion is. In conceptual terms continuous functions are suppose to map ""nearby points to nearby points"" so for me its metric space definition makes sense in that sense. However, that doesn't seem obvious to me unless we equate ""open sets"" to be the definition of ""close by"". Balls are open but there are plenty of sets that are open but are not ""close by"", for example the union of two open balls. I think this is what is confusing me most. How is the topological def respecting that conceptual requirement?",,"['real-analysis', 'general-topology', 'limits', 'metric-spaces', 'continuity']"
42,Prove Minkowski's Inequality for Integrals,Prove Minkowski's Inequality for Integrals,,"I am interested in proving the following claims : Suppose that ( $X$ , $\mathcal{M}$ , $\mu$ ) and ( $Y$ , $\mathcal{N}$ , $\nu$ ) are $\sigma$ -finite measure spaces, and let $f$ be an ( $\mathcal{M} \otimes \mathcal{N}$ )-measurable function on $X \times Y.$ a ) If $f \ge 0$ and $1 \le p < \infty$ , then $$ \left[\int \left(\int f(x,y) d\nu(y) \right)^pd\mu(x)\right]^\frac{1}{p} \le \int \left[\int f(x,y)^p d\mu(x)\right]^\frac{1}{p}d\nu(y)$$ b ) If $1 \le p \le \infty$ , $f(\cdot, y) \in L^p(\mu)$ for a.e. $y$ , and the function $y \to ||f(\cdot, y)||_p$ is in $L^1(\nu)$ , then $f(x, \cdot) \in L^1(\nu)$ for a.e. $x$ , the function $x \to \int f(x,y) d\nu(y)$ is in $L^p(\mu)$ , and $$\left|\left|\int f(\cdot, y)d\nu(y)\right|\right|_p \le \int||f(\cdot, y)||_pd\nu(y).$$","I am interested in proving the following claims : Suppose that ( , , ) and ( , , ) are -finite measure spaces, and let be an ( )-measurable function on a ) If and , then b ) If , for a.e. , and the function is in , then for a.e. , the function is in , and","X \mathcal{M} \mu Y \mathcal{N} \nu \sigma f \mathcal{M} \otimes \mathcal{N} X \times Y. f \ge 0 1 \le p < \infty  \left[\int \left(\int f(x,y) d\nu(y) \right)^pd\mu(x)\right]^\frac{1}{p} \le \int \left[\int f(x,y)^p d\mu(x)\right]^\frac{1}{p}d\nu(y) 1 \le p \le \infty f(\cdot, y) \in L^p(\mu) y y \to ||f(\cdot, y)||_p L^1(\nu) f(x, \cdot) \in L^1(\nu) x x \to \int f(x,y) d\nu(y) L^p(\mu) \left|\left|\int f(\cdot, y)d\nu(y)\right|\right|_p \le \int||f(\cdot, y)||_pd\nu(y).","['real-analysis', 'integration', 'measure-theory', 'inequality']"
43,Using Fatou's Lemmas in proving Scheffe's Lemma Part (ii),Using Fatou's Lemmas in proving Scheffe's Lemma Part (ii),,"Based on Williams' Probability w/ Martingales: Let $(S, \Sigma, \mu)$ be a measure space. Scheffe's Lemma Part (ii) : Suppose $\{f_n\}_{n \in \mathbb{N}}, f \in \mathscr{L}^1 (S, \Sigma, \mu)$ and $\lim_{n \to \infty} f_n(s) = f(s) \forall s \in S$ or a.e. in S. Then $$\lim_{n \to \infty} \int_S |f_n - f| d\mu = 0 \iff \lim_{n \to \infty} \int_S |f_n| d\mu = \int_S |f| d\mu$$ In proving Scheffe's Lemma, we could use Fatou's Lemmas to show that $$\lim_{n \to \infty} \int_S f_n^{+} d\mu = \int_S f^{+} d\mu$$ $$\lim_{n \to \infty} \int_S f_n^{-} d\mu = \int_S f^{-} d\mu$$ What I tried: Fatou's Lemmas for $f_n^{+}$ $$\int_S \limsup f_n^{+} d\mu \ge \limsup \int_S f_n^{+} d\mu \ge \liminf \int_S f_n^{+} d\mu \ge \int_S \liminf f_n^{+} d\mu$$ And that's about it. I have no idea if $$\lim_{n \to \infty} f_n^{+}(s) = f^{+}(s) \forall s \in S$$ or a.e. in S. Is $$\lim_{n \to \infty} \max(f_n, 0) = \max(\lim_{n \to \infty} f_n, 0)$$ ? I seem to recall from basic calculus that $$\lim_{x \to \infty} f(g(x)) = f(\lim_{x \to \infty} g(x))$$ if $f$ is continuous. Even if it was true, I'm not sure what I can use here. I don't think I can use monotone convergence theorem or dominated convergence theorem. Can I? How else can I approach this?","Based on Williams' Probability w/ Martingales: Let be a measure space. Scheffe's Lemma Part (ii) : Suppose and or a.e. in S. Then In proving Scheffe's Lemma, we could use Fatou's Lemmas to show that What I tried: Fatou's Lemmas for And that's about it. I have no idea if or a.e. in S. Is ? I seem to recall from basic calculus that if is continuous. Even if it was true, I'm not sure what I can use here. I don't think I can use monotone convergence theorem or dominated convergence theorem. Can I? How else can I approach this?","(S, \Sigma, \mu) \{f_n\}_{n \in \mathbb{N}}, f \in \mathscr{L}^1 (S, \Sigma, \mu) \lim_{n \to \infty} f_n(s) = f(s) \forall s \in S \lim_{n \to \infty} \int_S |f_n - f| d\mu = 0 \iff \lim_{n \to \infty} \int_S |f_n| d\mu = \int_S |f| d\mu \lim_{n \to \infty} \int_S f_n^{+} d\mu = \int_S f^{+} d\mu \lim_{n \to \infty} \int_S f_n^{-} d\mu = \int_S f^{-} d\mu f_n^{+} \int_S \limsup f_n^{+} d\mu \ge \limsup \int_S f_n^{+} d\mu \ge \liminf \int_S f_n^{+} d\mu \ge \int_S \liminf f_n^{+} d\mu \lim_{n \to \infty} f_n^{+}(s) = f^{+}(s) \forall s \in S \lim_{n \to \infty} \max(f_n, 0) = \max(\lim_{n \to \infty} f_n, 0) \lim_{x \to \infty} f(g(x)) = f(\lim_{x \to \infty} g(x)) f","['real-analysis', 'integration', 'probability-theory', 'measure-theory', 'lebesgue-integral']"
44,What does it mean to show that something is well defined?,What does it mean to show that something is well defined?,,"This is coming from my first course in undergraduate analysis, and it's confusing to me how to show that some operation is ""well-defined"". For example, my professor left as something for us to figure out on our own, not homework, to show ourselves that if $a,b,c$ and $d$ are integers, and ($b,d\not=0$) that $$\left[\left(\frac{a}{b}\right)\right]+\left[\left(\frac{c}{d}\right)\right]=\left[\left(\frac{ad+bc}{bd}\right)\right]$$ is well defined. He then made an example that said: If $\frac{a'}{b'}\sim\frac{a}{b}$ and $\frac{c'}{d'}\sim \frac{c}{d}$, then $\frac{a'c'}{b'd'}\sim \frac{ac}{bd}$. Also above in the brackets are supposed to be $2$ classes of element's.","This is coming from my first course in undergraduate analysis, and it's confusing to me how to show that some operation is ""well-defined"". For example, my professor left as something for us to figure out on our own, not homework, to show ourselves that if $a,b,c$ and $d$ are integers, and ($b,d\not=0$) that $$\left[\left(\frac{a}{b}\right)\right]+\left[\left(\frac{c}{d}\right)\right]=\left[\left(\frac{ad+bc}{bd}\right)\right]$$ is well defined. He then made an example that said: If $\frac{a'}{b'}\sim\frac{a}{b}$ and $\frac{c'}{d'}\sim \frac{c}{d}$, then $\frac{a'c'}{b'd'}\sim \frac{ac}{bd}$. Also above in the brackets are supposed to be $2$ classes of element's.",,['real-analysis']
45,Is the reciprocal of an analytic function analytic?,Is the reciprocal of an analytic function analytic?,,"Let $f(x)=\sum_{n=0}^{\infty}a_nx^n$ be a function on the real line with a positive(or infinite) radius of convergence about $x=0$, and $a_0\neq0$. Then is it true that the function $1/f(x)$ can also be Taylor expanded about the origin? The question is essentially whether the formal inverse series has a positive or infinite radius of convergence.","Let $f(x)=\sum_{n=0}^{\infty}a_nx^n$ be a function on the real line with a positive(or infinite) radius of convergence about $x=0$, and $a_0\neq0$. Then is it true that the function $1/f(x)$ can also be Taylor expanded about the origin? The question is essentially whether the formal inverse series has a positive or infinite radius of convergence.",,"['real-analysis', 'analysis', 'complex-analysis']"
46,What does $\lim\limits_{x \to \infty} f(x) = 1$ say about $\lim\limits_{x \to \infty} f'(x)$?,What does  say about ?,\lim\limits_{x \to \infty} f(x) = 1 \lim\limits_{x \to \infty} f'(x),"Given that $f$ is differentiable, what does $\lim\limits_{x \to \infty} f(x) = 1$ say about $\lim\limits_{x \to \infty} f^\prime(x)$ ? Intuitively I feel that it's $0$. I attempted to solve this by trying to evaluate $$\lim_{x \to \infty} \lim_{h \to 0} \frac{f(x + h) - f(x)}{h} $$ by trying to interchange the limits after showing that $\frac{f(x + h) - f(x)}{h}$ converges uniformly as $x \to \infty$. But I couldn't proceed further. Trying to work backwards, as a specific example, $f(x) = \arctan(x)$ came to my mind. It's derivative certainly goes to $0$ as $x \to \infty$. Doesn't this show that $\frac{f(x + h) - f(x)}{h}$ converges uniformly to $0$ as $x \to \infty$ ? I'm totally confused! I would really appreciate if anyone told me what I am doing wrong.","Given that $f$ is differentiable, what does $\lim\limits_{x \to \infty} f(x) = 1$ say about $\lim\limits_{x \to \infty} f^\prime(x)$ ? Intuitively I feel that it's $0$. I attempted to solve this by trying to evaluate $$\lim_{x \to \infty} \lim_{h \to 0} \frac{f(x + h) - f(x)}{h} $$ by trying to interchange the limits after showing that $\frac{f(x + h) - f(x)}{h}$ converges uniformly as $x \to \infty$. But I couldn't proceed further. Trying to work backwards, as a specific example, $f(x) = \arctan(x)$ came to my mind. It's derivative certainly goes to $0$ as $x \to \infty$. Doesn't this show that $\frac{f(x + h) - f(x)}{h}$ converges uniformly to $0$ as $x \to \infty$ ? I'm totally confused! I would really appreciate if anyone told me what I am doing wrong.",,"['real-analysis', 'limits']"
47,Difficulty in finding a counterexample,Difficulty in finding a counterexample,,"I am finding difficulties in finding a counterexample to the following statement. If $f\colon (0,\infty) \to(0,\infty) $ is uniformly continuous, then $$\lim_{x\to \infty} \frac{f(x+\frac{1}{x})}{f(x)} =1.$$","I am finding difficulties in finding a counterexample to the following statement. If is uniformly continuous, then","f\colon (0,\infty) \to(0,\infty)  \lim_{x\to \infty} \frac{f(x+\frac{1}{x})}{f(x)} =1.","['real-analysis', 'limits', 'continuity', 'examples-counterexamples']"
48,why is each open set an $F_\sigma$?,why is each open set an ?,F_\sigma,"In Royden's Real Analysis: On $\mathbb{R}$, a set which is a countable union of closed sets is called an $F_\sigma$.    Thus every countable set is an $F_\sigma$, as is, of course, every closed set.    A countable union of sets in Fa is again in $F_\sigma$.    Since   $$(a, b)= \cup_{n=1}^\infty [a + 1/n, b - 1/n],$$   each open interval is an $F_\sigma$, and hence each open set is an $F_\sigma$. Each open interval is an $F_\sigma$, but why is each open set an $F_\sigma$? An open set can be uncountable union of open intervals. If we are considering a general Borel sigma algebra instead of $B(\mathbb R)$, will each open set be an  $F_\sigma$? Thanks!","In Royden's Real Analysis: On $\mathbb{R}$, a set which is a countable union of closed sets is called an $F_\sigma$.    Thus every countable set is an $F_\sigma$, as is, of course, every closed set.    A countable union of sets in Fa is again in $F_\sigma$.    Since   $$(a, b)= \cup_{n=1}^\infty [a + 1/n, b - 1/n],$$   each open interval is an $F_\sigma$, and hence each open set is an $F_\sigma$. Each open interval is an $F_\sigma$, but why is each open set an $F_\sigma$? An open set can be uncountable union of open intervals. If we are considering a general Borel sigma algebra instead of $B(\mathbb R)$, will each open set be an  $F_\sigma$? Thanks!",,"['real-analysis', 'measure-theory']"
49,Graph of discontinuous additive function is dense in $ \mathbb R ^ 2 $,Graph of discontinuous additive function is dense in, \mathbb R ^ 2 ,"$ f : \mathbb R \to \mathbb R $ is a function such that for all $ x , y \in \mathbb R $ , $$ f ( x + y ) = f ( x  ) +f ( y ) \text . $$ If $f$ is continuous, then of course it has to be linear. But here $ f $ is not continuous. Then show that the graph of $ f $ , i.e. the set $ \left\{ \big( x , f ( x ) \big) : x \in \mathbb R \right\} $ , is dense in $ \mathbb R ^ 2 $ .","is a function such that for all , If is continuous, then of course it has to be linear. But here is not continuous. Then show that the graph of , i.e. the set , is dense in ."," f : \mathbb R \to \mathbb R   x , y \in \mathbb R   f ( x + y ) = f ( x  ) +f ( y ) \text .  f  f   f   \left\{ \big( x , f ( x ) \big) : x \in \mathbb R \right\}   \mathbb R ^ 2 ","['real-analysis', 'functional-equations']"
50,Polynomial $p(x) = 0$ for all $x$ implies coefficients of polynomial are zero,Polynomial  for all  implies coefficients of polynomial are zero,p(x) = 0 x,"I am curious why the following is true. The text I am reading is ""An Introduction to Numerical Analysis"" by Atkinson, 2nd edition, page 133, line 4. $p(x)$ is a polynomial of the form: $$ p(x) = b_0 + b_1 x + \cdots + b_n x^n$$ If $p(x) = 0$ for all $x$, then $b_i = 0$ for $i=0,1,\ldots,n$. Why is this true? For example, for $n=2$, I can first prove $b_0=0$, then set $x=2$ to get a linear system of two equations. Then I can prove $b_1=b_2 = 0$. Similarly, for $n=3$, I first prove $b_0=0$, then I calculate the rank of the resulting linear system of equations. That shows that $b_1=b_2=b_3=0$. But if $n$ is very large, I cannot keep manually solving systems of equations. Is there some other argument to show all the coefficients must be zero when the polynomial is always zero for all $x$? Thanks.","I am curious why the following is true. The text I am reading is ""An Introduction to Numerical Analysis"" by Atkinson, 2nd edition, page 133, line 4. $p(x)$ is a polynomial of the form: $$ p(x) = b_0 + b_1 x + \cdots + b_n x^n$$ If $p(x) = 0$ for all $x$, then $b_i = 0$ for $i=0,1,\ldots,n$. Why is this true? For example, for $n=2$, I can first prove $b_0=0$, then set $x=2$ to get a linear system of two equations. Then I can prove $b_1=b_2 = 0$. Similarly, for $n=3$, I first prove $b_0=0$, then I calculate the rank of the resulting linear system of equations. That shows that $b_1=b_2=b_3=0$. But if $n$ is very large, I cannot keep manually solving systems of equations. Is there some other argument to show all the coefficients must be zero when the polynomial is always zero for all $x$? Thanks.",,"['linear-algebra', 'real-analysis', 'polynomials', 'analyticity']"
51,How do people pick $\delta$ so fast in $\epsilon$-$\delta$ proofs,How do people pick  so fast in - proofs,\delta \epsilon \delta,"For example, in a proof that shows $f(x) = \sqrt x$ is uniformly continuous on the positive real line, the proof goes like: Let $\epsilon > 0$ be given, and $\delta = \epsilon^2$.... Or to show that every Lipschitz continuous function is uniformly continuous Let $\epsilon > 0$ be given, and $\delta = \epsilon$.... Do these people have a magic ball that let them see what the $\delta$ value is going to work? I often find myself struggling coming up with the $\delta$ value after doing a bunch of inequalities on $|f(x) - f(y)|< \delta$ to make it less than $\epsilon$. How do people know what $\delta$ is going to be in the first line of their proof?","For example, in a proof that shows $f(x) = \sqrt x$ is uniformly continuous on the positive real line, the proof goes like: Let $\epsilon > 0$ be given, and $\delta = \epsilon^2$.... Or to show that every Lipschitz continuous function is uniformly continuous Let $\epsilon > 0$ be given, and $\delta = \epsilon$.... Do these people have a magic ball that let them see what the $\delta$ value is going to work? I often find myself struggling coming up with the $\delta$ value after doing a bunch of inequalities on $|f(x) - f(y)|< \delta$ to make it less than $\epsilon$. How do people know what $\delta$ is going to be in the first line of their proof?",,"['calculus', 'real-analysis', 'proof-writing', 'epsilon-delta', 'proof-explanation']"
52,Show that $e^{x+y}=e^xe^y$ using $e^x=\lim_{n\to\infty }\left(1+\frac{x}{n}\right)^n$.,Show that  using .,e^{x+y}=e^xe^y e^x=\lim_{n\to\infty }\left(1+\frac{x}{n}\right)^n,I was looking for a proof of $e^{x+y}=e^xe^y$ using the fact that $$e^x=\lim_{n\to\infty }\left(1+\frac{x}{n}\right)^n.$$ So I have that $$\left(1+\frac{x+y}{n}\right)^n=\sum_{k=0}^n\binom{n}{k}\frac{(x+y)^k}{n^k}=\sum_{k=0}^n\frac{1}{n^k}\sum_{i=0}^k\binom{k}{i}x^iy^{k-i}=\sum_{k=0}^n\binom{n}{k}\sum_{i=0}^k\binom{k}{i}\left(\frac{x}{n}\right)^i\left(\frac{y}{n}\right)^{k-i}$$ But I can't get $$\left(1+\frac{x+y}{n}\right)^n=\sum_{k=0}^n\binom{n}{k}\left(\frac{x}{n}\right)^k\sum_{i=0}^n\binom{n}{i}\left(\frac{y}{n}\right)^i.$$ Any idea ?,I was looking for a proof of $e^{x+y}=e^xe^y$ using the fact that $$e^x=\lim_{n\to\infty }\left(1+\frac{x}{n}\right)^n.$$ So I have that $$\left(1+\frac{x+y}{n}\right)^n=\sum_{k=0}^n\binom{n}{k}\frac{(x+y)^k}{n^k}=\sum_{k=0}^n\frac{1}{n^k}\sum_{i=0}^k\binom{k}{i}x^iy^{k-i}=\sum_{k=0}^n\binom{n}{k}\sum_{i=0}^k\binom{k}{i}\left(\frac{x}{n}\right)^i\left(\frac{y}{n}\right)^{k-i}$$ But I can't get $$\left(1+\frac{x+y}{n}\right)^n=\sum_{k=0}^n\binom{n}{k}\left(\frac{x}{n}\right)^k\sum_{i=0}^n\binom{n}{i}\left(\frac{y}{n}\right)^i.$$ Any idea ?,,"['real-analysis', 'exponential-function']"
53,Taking limits on each term in inequality invalid?,Taking limits on each term in inequality invalid?,,"So this inequality came up in a proof I was going through. $$c - 1/n < f(x_n) \leq c$$ Where $c$ is a real number, $f(x_n)$ is the image sequence of some arbitrary sequence being passed through a function and $n$ is a natural number. At this point the author simply concludes that this implies the function sequence converges to $c$. It's pretty clear that this is happening, but I'm not exactly sure what the proper justification is. Are we taking limits as $n \to \infty$ on all sides of the inequality? So that we get $$c < \lim_{n \to \infty} f(x_n) \leq c$$ Can we just take limits on all sides of an inequality like that? It seems like that could lead to problems as you could take a situation like $$1 - 1/n < 1$$ but then doing what I suggested would just lead to $1<1$ which is not true. So what is the argument that I seem to be overlooking? Thanks.","So this inequality came up in a proof I was going through. $$c - 1/n < f(x_n) \leq c$$ Where $c$ is a real number, $f(x_n)$ is the image sequence of some arbitrary sequence being passed through a function and $n$ is a natural number. At this point the author simply concludes that this implies the function sequence converges to $c$. It's pretty clear that this is happening, but I'm not exactly sure what the proper justification is. Are we taking limits as $n \to \infty$ on all sides of the inequality? So that we get $$c < \lim_{n \to \infty} f(x_n) \leq c$$ Can we just take limits on all sides of an inequality like that? It seems like that could lead to problems as you could take a situation like $$1 - 1/n < 1$$ but then doing what I suggested would just lead to $1<1$ which is not true. So what is the argument that I seem to be overlooking? Thanks.",,"['real-analysis', 'limits', 'inequality', 'proof-verification']"
54,"Is every Lipschitz continuous function is holder continuous with exponent $\in (0,1)$?",Is every Lipschitz continuous function is holder continuous with exponent ?,"\in (0,1)","Is every Lipschitz continuous function is holder continuous with exponent $\in (0,1)$? This seems to be true,but I haven't found such a conclusion in any textbook.","Is every Lipschitz continuous function is holder continuous with exponent $\in (0,1)$? This seems to be true,but I haven't found such a conclusion in any textbook.",,"['real-analysis', 'analysis']"
55,Show $\int_0^\infty \frac{\cos a x-\cos b x}{\sinh \beta x}\frac{dx}{x}=\log\big( \frac{\cosh \frac{b\pi}{2 \beta}}{\cosh \frac{a\pi}{2\beta}}\big)$,Show,\int_0^\infty \frac{\cos a x-\cos b x}{\sinh \beta x}\frac{dx}{x}=\log\big( \frac{\cosh \frac{b\pi}{2 \beta}}{\cosh \frac{a\pi}{2\beta}}\big),"Hi I am trying to prove this interesting integral $$ \mathcal{I}:=\int_0^\infty \frac{\cos a x-\cos b x}{\sinh \beta x}\frac{dx}{x}=\log\left( \frac{\cosh \frac{b\pi}{2 \beta}}{\cosh \frac{a\pi}{2\beta}}\right), \qquad Re(\beta)>0. $$ I was thinking this was possibly a Frullani integral $$ \int_0^\infty \frac{f(ax)-f(bx)}{x}dx=\big[f(0)-f(\infty)\big]\log \frac{b}{a}, $$ but am unable to get rid of the $\sinh \beta x$ in the denominator.  I have tried partial integration and splitting the integral up but ran into convergent issues.  How can we solve this integral?  Possibly we can go to exponential representation using $2\sinh x=e^x-e^{-x}$, but that didn't help either.  Thanks","Hi I am trying to prove this interesting integral $$ \mathcal{I}:=\int_0^\infty \frac{\cos a x-\cos b x}{\sinh \beta x}\frac{dx}{x}=\log\left( \frac{\cosh \frac{b\pi}{2 \beta}}{\cosh \frac{a\pi}{2\beta}}\right), \qquad Re(\beta)>0. $$ I was thinking this was possibly a Frullani integral $$ \int_0^\infty \frac{f(ax)-f(bx)}{x}dx=\big[f(0)-f(\infty)\big]\log \frac{b}{a}, $$ but am unable to get rid of the $\sinh \beta x$ in the denominator.  I have tried partial integration and splitting the integral up but ran into convergent issues.  How can we solve this integral?  Possibly we can go to exponential representation using $2\sinh x=e^x-e^{-x}$, but that didn't help either.  Thanks",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
56,Intuition for dense sets. (Real analysis),Intuition for dense sets. (Real analysis),,I have been having problems with dense sets as my lecturer didn't really develop an intuition for dense sets in my class. So can any of you please help me with that? And can you please tell me (the general case) how I should go about proving that a set is dense in R.,I have been having problems with dense sets as my lecturer didn't really develop an intuition for dense sets in my class. So can any of you please help me with that? And can you please tell me (the general case) how I should go about proving that a set is dense in R.,,['real-analysis']
57,$f(x)=1/q$ for $x=p/q$ is integrable,for  is integrable,f(x)=1/q x=p/q,"Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be defined by setting $f(x)=1/q$ if $x=p/q$, where $p$ and $q$ are positive integers with no common factor, and $f(x)=0$ otherwise. Show that $f$ is integrable over $[0,1]$. I'm using the Darboux definition of integration, so I want to prove that for any $\epsilon>0$ there exists a partition $P$ of $[0,1]$ such that $U(f,P)-L(f,P)<\epsilon$. Equivalently, there exists a partition $P$ of $[0,1]$ such that $$\sum_Rv(R)(M_R(f)-m_R(f)) < \epsilon$$ where $M_R(f)$ is the supremum of $f$ inside interval $R$, $m_R(f)$ is the infimum of $f$ inside interval $R$, and $R$ ranges over all intervals in the partition. So I tried taking $P=[0,\dfrac1n,\dfrac2n,\ldots,1]$. The sum in question becomes $$\dfrac1n\sum_{i=0}^{n-1}(M_{[\frac{i}{n},\frac{i+1}{n}]}(f)-m_{[\frac{i}{n},\frac{i+1}{n}]}(f))$$ I know that $m_{[\frac{i}{n},\frac{i+1}{n}]}(f)=0$, because in the interval $[\dfrac{i}{n},\dfrac{i+1}{n}]$ there is an irrational number, so the sum reduces to $$\dfrac1n\sum_{i=0}^{n-1}M_{[\frac{i}{n},\frac{i+1}{n}]}(f)$$ I don't really know anything about the fraction with lowest denominator inside $[\dfrac{i}{n},\dfrac{i+1}{n}]$. How can I prove that this sum goes to $0$ as $n\rightarrow\infty$?","Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be defined by setting $f(x)=1/q$ if $x=p/q$, where $p$ and $q$ are positive integers with no common factor, and $f(x)=0$ otherwise. Show that $f$ is integrable over $[0,1]$. I'm using the Darboux definition of integration, so I want to prove that for any $\epsilon>0$ there exists a partition $P$ of $[0,1]$ such that $U(f,P)-L(f,P)<\epsilon$. Equivalently, there exists a partition $P$ of $[0,1]$ such that $$\sum_Rv(R)(M_R(f)-m_R(f)) < \epsilon$$ where $M_R(f)$ is the supremum of $f$ inside interval $R$, $m_R(f)$ is the infimum of $f$ inside interval $R$, and $R$ ranges over all intervals in the partition. So I tried taking $P=[0,\dfrac1n,\dfrac2n,\ldots,1]$. The sum in question becomes $$\dfrac1n\sum_{i=0}^{n-1}(M_{[\frac{i}{n},\frac{i+1}{n}]}(f)-m_{[\frac{i}{n},\frac{i+1}{n}]}(f))$$ I know that $m_{[\frac{i}{n},\frac{i+1}{n}]}(f)=0$, because in the interval $[\dfrac{i}{n},\dfrac{i+1}{n}]$ there is an irrational number, so the sum reduces to $$\dfrac1n\sum_{i=0}^{n-1}M_{[\frac{i}{n},\frac{i+1}{n}]}(f)$$ I don't really know anything about the fraction with lowest denominator inside $[\dfrac{i}{n},\dfrac{i+1}{n}]$. How can I prove that this sum goes to $0$ as $n\rightarrow\infty$?",,['real-analysis']
58,A question on countability of isolated points of a subset of R,A question on countability of isolated points of a subset of R,,"The question is to prove that with respect to the euclidean metric on the Real numbers prove that if A is any subset of R, isoA is countable and hence deduce  that if A is uncountable the A' is uncountable. I am still not that comfortable with proving things like countability so any answers or help is appreciated thanks","The question is to prove that with respect to the euclidean metric on the Real numbers prove that if A is any subset of R, isoA is countable and hence deduce  that if A is uncountable the A' is uncountable. I am still not that comfortable with proving things like countability so any answers or help is appreciated thanks",,"['real-analysis', 'metric-spaces']"
59,"Compute the fourier coefficients, and series for $\log(\sin(x))$","Compute the fourier coefficients, and series for",\log(\sin(x)),"I posted a similar question with a bad response, so I am retrying with hopes of better knowledge. The fourier series is in the form: $$f(x) = \frac{a_0}{2} + \sum_{n=1}^{\infty} a_n\cos(nx) + \sum_{n=1}^{\infty} b_n\sin(nx)$$ Where: $$a_n = \frac{1}{\pi} \int_{-\pi}^{\pi} f(x)\cos(nx) dx$$ $$b_n = \frac{1}{\pi} \int_{-\pi}^{\pi} f(x)\sin(nx) dx$$ The problem is computing the coefficients. The goal of trying to derive the series is to solve: $$\int_{0}^{\pi} \log(\sin(x)) dx$$ $\displaystyle a_n = \frac{1}{\pi} \int_{-\pi}^{\pi} \log(\sin(x))\cos(nx) dx$ Which is very difficult to compute. What can be done? The series representation is: $$\log(\sin(x)) = -\log(2) - \sum_{k=1}^{\infty} \frac{\cos(2kx)}{k}$$","I posted a similar question with a bad response, so I am retrying with hopes of better knowledge. The fourier series is in the form: Where: The problem is computing the coefficients. The goal of trying to derive the series is to solve: Which is very difficult to compute. What can be done? The series representation is:",f(x) = \frac{a_0}{2} + \sum_{n=1}^{\infty} a_n\cos(nx) + \sum_{n=1}^{\infty} b_n\sin(nx) a_n = \frac{1}{\pi} \int_{-\pi}^{\pi} f(x)\cos(nx) dx b_n = \frac{1}{\pi} \int_{-\pi}^{\pi} f(x)\sin(nx) dx \int_{0}^{\pi} \log(\sin(x)) dx \displaystyle a_n = \frac{1}{\pi} \int_{-\pi}^{\pi} \log(\sin(x))\cos(nx) dx \log(\sin(x)) = -\log(2) - \sum_{k=1}^{\infty} \frac{\cos(2kx)}{k},"['calculus', 'real-analysis', 'integration', 'sequences-and-series', 'fourier-series']"
60,"Integral $\int_0^{\pi/4}\frac{dx}{{\sin 2x}\,(\tan^ax+\cot^ax)}=\frac{\pi}{8a}$",Integral,"\int_0^{\pi/4}\frac{dx}{{\sin 2x}\,(\tan^ax+\cot^ax)}=\frac{\pi}{8a}","I am trying to prove this interesting integral$$ \mathcal{I}:=\int_0^{\pi/4}\frac{dx}{{\sin 2x}\,(\tan^ax+\cot^ax)}=\frac{\pi}{8a},\qquad \mathcal{Re}(a)\neq 0. $$ This result is breath taking but I am more stumped than usual.  It truly is magnificent.  I am not sure how to approach this, note $\sin 2x=2\sin x \cos x$.  I am not sure how to approach this because of the term $$ (\tan^ax+\cot^ax) $$ in the denominator.  I was trying to use the identity  $$ \tan \left(\frac{\pi}{2}-x\right)=\cot x $$ since this method solves a similar kind of integral but didn't get anywhere.    A bad idea I tried was to try and differentiate with respect to a $$ \frac{dI(a)}{da}=\int_0^{\pi/4}\partial_a \left(\frac{dx}{{\sin 2x}\,(\tan^ax+\cot^ax)}\right)=\int_0^{\pi/4}  \frac{(\cot^a x \log(\cot x )+\log(\tan x ) \tan^a x)}{\sin 2x \, (\cot^a x+\tan^a x)^2}dx $$ which seems more complicated when I break it up into two integrals.  How can we solve the integral?  Thanks.","I am trying to prove this interesting integral$$ \mathcal{I}:=\int_0^{\pi/4}\frac{dx}{{\sin 2x}\,(\tan^ax+\cot^ax)}=\frac{\pi}{8a},\qquad \mathcal{Re}(a)\neq 0. $$ This result is breath taking but I am more stumped than usual.  It truly is magnificent.  I am not sure how to approach this, note $\sin 2x=2\sin x \cos x$.  I am not sure how to approach this because of the term $$ (\tan^ax+\cot^ax) $$ in the denominator.  I was trying to use the identity  $$ \tan \left(\frac{\pi}{2}-x\right)=\cot x $$ since this method solves a similar kind of integral but didn't get anywhere.    A bad idea I tried was to try and differentiate with respect to a $$ \frac{dI(a)}{da}=\int_0^{\pi/4}\partial_a \left(\frac{dx}{{\sin 2x}\,(\tan^ax+\cot^ax)}\right)=\int_0^{\pi/4}  \frac{(\cot^a x \log(\cot x )+\log(\tan x ) \tan^a x)}{\sin 2x \, (\cot^a x+\tan^a x)^2}dx $$ which seems more complicated when I break it up into two integrals.  How can we solve the integral?  Thanks.",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
61,Is there a function $f\colon\mathbb{R}\to\mathbb{R}$ such that every non-empty open interval is mapped onto $\mathbb{R}$?,Is there a function  such that every non-empty open interval is mapped onto ?,f\colon\mathbb{R}\to\mathbb{R} \mathbb{R},"I wonder  whether there is a function $f\colon\Bbb R\to\Bbb R$ with the folowing characteristic?   for every two real numbers $\alpha,\beta,\alpha\lt\beta$,  $$\{f(x):x\in(\alpha,\beta)\}=\Bbb R$$ I can't  say such a function does not exist, neither can I construct a example Thanks a lot!","I wonder  whether there is a function $f\colon\Bbb R\to\Bbb R$ with the folowing characteristic?   for every two real numbers $\alpha,\beta,\alpha\lt\beta$,  $$\{f(x):x\in(\alpha,\beta)\}=\Bbb R$$ I can't  say such a function does not exist, neither can I construct a example Thanks a lot!",,"['real-analysis', 'analysis', 'functions']"
62,$X_n\leq Y_n$ implies $\liminf X_n \leq \liminf Y_n$ and $\limsup X_n \leq \limsup Y_n$,implies  and,X_n\leq Y_n \liminf X_n \leq \liminf Y_n \limsup X_n \leq \limsup Y_n,"Can anyone prove this question? I tried but I didn't get any I idea, so I hope someone can solve it. Let $X_n\leq Y_n$ for each $n\in \Bbb N$. Show that $\liminf X_n \leq \liminf Y_n$ and $\limsup X_n \leq \limsup Y_n$. Please prove this question - thanks. The definition I have: Let $X_n$ be a sequence in real number and let $$E=\{x\in \Bbb R^\sharp:(X_{n_k}) \rightarrow x \text{ for some subsequence }(X_{n_k})\text{ of }(X_n)\}$$ for all $n \in \Bbb N$ and $k$ from $1$ to $\infty$. Then by definition $\lim\sup X_n = \sup E$ and $\lim\inf X_n = \inf E$.","Can anyone prove this question? I tried but I didn't get any I idea, so I hope someone can solve it. Let $X_n\leq Y_n$ for each $n\in \Bbb N$. Show that $\liminf X_n \leq \liminf Y_n$ and $\limsup X_n \leq \limsup Y_n$. Please prove this question - thanks. The definition I have: Let $X_n$ be a sequence in real number and let $$E=\{x\in \Bbb R^\sharp:(X_{n_k}) \rightarrow x \text{ for some subsequence }(X_{n_k})\text{ of }(X_n)\}$$ for all $n \in \Bbb N$ and $k$ from $1$ to $\infty$. Then by definition $\lim\sup X_n = \sup E$ and $\lim\inf X_n = \inf E$.",,"['real-analysis', 'sequences-and-series', 'inequality', 'limsup-and-liminf']"
63,Compute $ \sum\limits_{m=1}^{\infty} \sum\limits_{n=1}^{\infty} \sum\limits_{p=1}^{\infty}\frac{(-1)^{m+n+p}}{m+n+p}$,Compute, \sum\limits_{m=1}^{\infty} \sum\limits_{n=1}^{\infty} \sum\limits_{p=1}^{\infty}\frac{(-1)^{m+n+p}}{m+n+p},"How would you compute this sum? It's not a problem I need to immediately solve, but a problem that came to my mind today. I think that the generalization to more than three nested sums would be interesting as well. $$ \sum_{m=1}^{\infty} \sum_{n=1}^{\infty} \sum_{p=1}^{\infty}\frac{(-1)^{m+n+p}}{m+n+p}$$","How would you compute this sum? It's not a problem I need to immediately solve, but a problem that came to my mind today. I think that the generalization to more than three nested sums would be interesting as well. $$ \sum_{m=1}^{\infty} \sum_{n=1}^{\infty} \sum_{p=1}^{\infty}\frac{(-1)^{m+n+p}}{m+n+p}$$",,"['calculus', 'real-analysis', 'sequences-and-series']"
64,Proving the set of points at which a function diverges to $\infty$ is countable,Proving the set of points at which a function diverges to  is countable,\infty,"Let $f\colon\mathbb{R}\to\mathbb{R}$. Prove that the set    $$\{x \mid \mbox{if $y$ converges to $x$, then $f(y)$ converges to $\infty$}\}$$ is countable. My book told me to consider  $g(x)=\arctan(f(x))$, then it said ""it is easy to see the set is countable."" But I still can't understand what it mean.","Let $f\colon\mathbb{R}\to\mathbb{R}$. Prove that the set    $$\{x \mid \mbox{if $y$ converges to $x$, then $f(y)$ converges to $\infty$}\}$$ is countable. My book told me to consider  $g(x)=\arctan(f(x))$, then it said ""it is easy to see the set is countable."" But I still can't understand what it mean.",,['real-analysis']
65,What can we say about $f$ if $\int_0^1 f(x)p(x)dx=0$ for all polynomials $p$?,What can we say about  if  for all polynomials ?,f \int_0^1 f(x)p(x)dx=0 p,"This question was motivated by another question in this site. As explained in that problem (and its answers), if $\displaystyle f$ is continuous on $\displaystyle [0,1]$ and $\displaystyle \int_0^1 f(x)p(x)dx=0$ for all polynomials $\displaystyle p$, then $\displaystyle f$ is zero everywhere. Suppose we remove the restriction that $\displaystyle f$ is continuous. Can we conclude from $\displaystyle f\in L^1([0,1])$ that $\displaystyle f$ is zero almost everywhere? (This should be terribly standard. My apologies, I am rusty of late.)","This question was motivated by another question in this site. As explained in that problem (and its answers), if $\displaystyle f$ is continuous on $\displaystyle [0,1]$ and $\displaystyle \int_0^1 f(x)p(x)dx=0$ for all polynomials $\displaystyle p$, then $\displaystyle f$ is zero everywhere. Suppose we remove the restriction that $\displaystyle f$ is continuous. Can we conclude from $\displaystyle f\in L^1([0,1])$ that $\displaystyle f$ is zero almost everywhere? (This should be terribly standard. My apologies, I am rusty of late.)",,['real-analysis']
66,"If $\sum (a_n)^2$ converges and $\sum (b_n)^2$ converges, does $\sum (a_n)(b_n)$ converge?","If  converges and  converges, does  converge?",\sum (a_n)^2 \sum (b_n)^2 \sum (a_n)(b_n),"If $\sum (a_n)^2$ converges and $\sum (b_n)^2$ converges, does $\sum (a_n)(b_n)$ converge? Could someone help me to solve this or at least give me a hint?, I have tried using Cauchy's criterion, the Dirichlet test for convergence, etc, but I can´t prove it.Honestly I don´t know where to start. Any help will be appreciated.","If converges and converges, does converge? Could someone help me to solve this or at least give me a hint?, I have tried using Cauchy's criterion, the Dirichlet test for convergence, etc, but I can´t prove it.Honestly I don´t know where to start. Any help will be appreciated.",\sum (a_n)^2 \sum (b_n)^2 \sum (a_n)(b_n),"['real-analysis', 'sequences-and-series', 'hilbert-spaces']"
67,convex function right left derivatives,convex function right left derivatives,,Hello how to show the followings for a convex function $f(x)$: Let $f(x_0) \in R$ then  $\frac{f(x_0 + \epsilon) - f(x_0)}{\epsilon}$  is nondecreasing in $\epsilon$ Similarly how to show the left and right derivatives of $f(x)$ always exists  and right derivative is larger than the left derivative. Thanks a lot!,Hello how to show the followings for a convex function $f(x)$: Let $f(x_0) \in R$ then  $\frac{f(x_0 + \epsilon) - f(x_0)}{\epsilon}$  is nondecreasing in $\epsilon$ Similarly how to show the left and right derivatives of $f(x)$ always exists  and right derivative is larger than the left derivative. Thanks a lot!,,"['real-analysis', 'convex-analysis']"
68,$f$ measurable with $f=g$ a.e. then $g$ measurable,measurable with  a.e. then  measurable,f f=g g,"How do I prove this proposition from Royden's Real Analysis: If $\mu$ is a complete measure and $f$ is a measurable function, then   $f=g$ almost everywhere implies $g$ is measurable. In proving this proposition, what differs from the proof of a proposition from the first chapters stating: If $f$ is a measurable function $f=g$ almost everywhere then    $g$ is measurable. In particular, what has to be modified in the following proof: Take $E=\lbrace x \in X | f(x) \neq g(x) \rbrace,$ which is measurable and has measure $0$. For a measurable set $A$ in the range of $g$, we show that the set $Y=g^{-1}(A)$ is measurable. Now, $Y \cap E$ has is measurable with measure $0$. Since $Y \setminus E = f^{-1}(A) \setminus E$ is a difference of two measurable sets, we are done.","How do I prove this proposition from Royden's Real Analysis: If $\mu$ is a complete measure and $f$ is a measurable function, then   $f=g$ almost everywhere implies $g$ is measurable. In proving this proposition, what differs from the proof of a proposition from the first chapters stating: If $f$ is a measurable function $f=g$ almost everywhere then    $g$ is measurable. In particular, what has to be modified in the following proof: Take $E=\lbrace x \in X | f(x) \neq g(x) \rbrace,$ which is measurable and has measure $0$. For a measurable set $A$ in the range of $g$, we show that the set $Y=g^{-1}(A)$ is measurable. Now, $Y \cap E$ has is measurable with measure $0$. Since $Y \setminus E = f^{-1}(A) \setminus E$ is a difference of two measurable sets, we are done.",,"['real-analysis', 'measure-theory']"
69,"Given any non-real, complex number with positive real part, must some power of it has negative real part?","Given any non-real, complex number with positive real part, must some power of it has negative real part?",,"Let $z\in \mathbb C$ such that $Re (z) >0 $ and $Im(z)\ne 0$. Then must there exist  integer $n>1$ such that $Re (z^n) <0$ ? Equivalently, given $\theta \in [0, 2\pi)$ such that $\cos \theta >0$ and $\sin \theta \ne 0$ , must there exist $n>1$ integer such that $\cos (n\theta) <0$ ?","Let $z\in \mathbb C$ such that $Re (z) >0 $ and $Im(z)\ne 0$. Then must there exist  integer $n>1$ such that $Re (z^n) <0$ ? Equivalently, given $\theta \in [0, 2\pi)$ such that $\cos \theta >0$ and $\sin \theta \ne 0$ , must there exist $n>1$ integer such that $\cos (n\theta) <0$ ?",,['real-analysis']
70,Evaluating a trigonometric integral.,Evaluating a trigonometric integral.,,"Show That $$\int_{0}^{\pi}\ln (\sin x) \cos(2nx) \, dx = -\dfrac{\pi}{2n}$$ I tried to use substitutions and taylor series of $\ln (\sin x)$ , but to no avail. Any help will be appreciated. Thanks.","Show That I tried to use substitutions and taylor series of , but to no avail. Any help will be appreciated. Thanks.","\int_{0}^{\pi}\ln (\sin x) \cos(2nx) \, dx = -\dfrac{\pi}{2n} \ln (\sin x)","['real-analysis', 'integration', 'trigonometry', 'definite-integrals']"
71,"Can a continuous function on [0,1] be constructed which is differentiable exactly at two points in [0,1]?","Can a continuous function on [0,1] be constructed which is differentiable exactly at two points in [0,1]?",,"We see that we can find a function nowhere differentiable or finitely not differentiable. But I want to understand, can a continuous function on $[0,1]$ be constructed which is differentiable exactly at two points in $[0,1]$? How can I construct such a function which is finitely differentiable?","We see that we can find a function nowhere differentiable or finitely not differentiable. But I want to understand, can a continuous function on $[0,1]$ be constructed which is differentiable exactly at two points in $[0,1]$? How can I construct such a function which is finitely differentiable?",,['real-analysis']
72,Example of a function that has the Luzin $n$-property and is not absolutely continuous.,Example of a function that has the Luzin -property and is not absolutely continuous.,n,"The Banach–Zaretsky theorem ( page 196 ) says that a continuous function $f:[a,b]\to\mathbb{R}$ of bounded variation is absolutely continuous if and only if $$E\subset I \text{ has zero Lebesgue measure }\Rightarrow f(E) \text{ has zero Lebesgue measure }\;\;[\#]$$ I would like see an example of a function that satisfies $[\#]$ but is not absolutely continuous. Thanks.","The Banach–Zaretsky theorem ( page 196 ) says that a continuous function $f:[a,b]\to\mathbb{R}$ of bounded variation is absolutely continuous if and only if $$E\subset I \text{ has zero Lebesgue measure }\Rightarrow f(E) \text{ has zero Lebesgue measure }\;\;[\#]$$ I would like see an example of a function that satisfies $[\#]$ but is not absolutely continuous. Thanks.",,"['real-analysis', 'measure-theory', 'continuity', 'examples-counterexamples', 'absolute-continuity']"
73,Does the series $\;\sum\limits_{n=0}^{\infty}\left(\frac{\pi}{2} - \arctan(n)\right)$ converge or diverge? [closed],Does the series  converge or diverge? [closed],\;\sum\limits_{n=0}^{\infty}\left(\frac{\pi}{2} - \arctan(n)\right),"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question I tried everything that I know but I couldn't solve this series: $$\sum_{n=0}^{\infty} \left(\frac{\pi}{2} - \arctan(n)\right)$$ Does it diverge or converge?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question I tried everything that I know but I couldn't solve this series: $$\sum_{n=0}^{\infty} \left(\frac{\pi}{2} - \arctan(n)\right)$$ Does it diverge or converge?",,"['real-analysis', 'sequences-and-series']"
74,A vector space over $R$ is not a countable union of proper subspaces,A vector space over  is not a countable union of proper subspaces,R,"I was looking for alternate proofs of the theorem that ""a vector space $V$ of dimension greater than $1$ over an infinite field $\mathbf{F}$ is not a union of fewer than $|\mathbf{F}|$ proper subspaces"" and possible generalizations. A simple measure-theoretic proof over $\mathbb{R}$ is as follows: By countable additivity the sum of the measures of any collection of subspaces is zero since the measure of each subspace is zero, which is a contradiction. I would like to look at proofs over arbitrary infinite fields and would like to know if similar statements hold for say modules (finitely generated or otherwise) over infinite rings.","I was looking for alternate proofs of the theorem that ""a vector space $V$ of dimension greater than $1$ over an infinite field $\mathbf{F}$ is not a union of fewer than $|\mathbf{F}|$ proper subspaces"" and possible generalizations. A simple measure-theoretic proof over $\mathbb{R}$ is as follows: By countable additivity the sum of the measures of any collection of subspaces is zero since the measure of each subspace is zero, which is a contradiction. I would like to look at proofs over arbitrary infinite fields and would like to know if similar statements hold for say modules (finitely generated or otherwise) over infinite rings.",,"['linear-algebra', 'real-analysis', 'abstract-algebra']"
75,If $\sum_{n=1}^{\infty} a_{n}^{3}$ converges does $\sum_{n=1}^{\infty} \frac{a_{n}}{n}$ converge?,If  converges does  converge?,\sum_{n=1}^{\infty} a_{n}^{3} \sum_{n=1}^{\infty} \frac{a_{n}}{n},Suppose $a_{n}>0$ and the following series converges $\sum_{n=1}^{\infty} a_{n}^{3}$ Does this imply that $\sum_{n=1}^{\infty} \frac{a_{n}}{n}$ converges? I was able to prove that the second series also converges by using the limit comparision test.   Is there another way to show the second series converges (e.g. root or ratio test)?,Suppose $a_{n}>0$ and the following series converges $\sum_{n=1}^{\infty} a_{n}^{3}$ Does this imply that $\sum_{n=1}^{\infty} \frac{a_{n}}{n}$ converges? I was able to prove that the second series also converges by using the limit comparision test.   Is there another way to show the second series converges (e.g. root or ratio test)?,,['real-analysis']
76,Can the Cauchy product of divergent series with itself be convergent?,Can the Cauchy product of divergent series with itself be convergent?,,For series $\sum a_n$ and $\sum b_n$ their Cauchy product is the series $\sum c_n$ where $$ c_n = a_0b_n+a_1b_{n-1}+\ldots+a_nb_0. $$ Does there exist a sequence $\sum a_n$ such that: $\sum a_n$ is divergent? The Cauchy product of $ \sum a_n$ and $\sum a_n$ is convergent?,For series and their Cauchy product is the series where Does there exist a sequence such that: is divergent? The Cauchy product of and is convergent?,"\sum a_n \sum b_n \sum c_n 
c_n = a_0b_n+a_1b_{n-1}+\ldots+a_nb_0.
 \sum a_n \sum a_n  \sum a_n \sum a_n","['real-analysis', 'sequences-and-series', 'divergent-series']"
77,Proof of the Intermediate Value Theorem,Proof of the Intermediate Value Theorem,,"Theorem: Let $f$ be continuous on $[a,\,b]$ and assume $f(a)<f(b)$. Then for every $k$ such that $f(a)<k<f(b)$, there exists a $c\in[a,\,b]$ such that $f(c)=k$. proof: $f$ continuous at $a\implies$ for $\varepsilon=k-f(a)>0$, $\exists\delta>0$ s.t. $$|f(x)-f(a)|<\varepsilon=k-f(a)\quad\forall x\colon |x-a|<\delta.$$ Consider the set $H=\{x\in[a,\,b]\colon f(x)<k\}\not=\emptyset \implies c=\sup{(H)}$. Show $f(c)=k$, suppose $f(c)<k\iff k-f(c)>0$. We know $f$ is continuous at $c$ so $\forall\varepsilon>k-f(c)>0$ $\exists\delta>0:|f(x)-f(c)|<\varepsilon=k-f(c)$ when $|x-c|<\delta$. $\implies f(x)-f(c)<k-f(c)$. Say $x=c+\delta/2\implies f(x)<k\implies c+\delta/2\in H$ which contradicts the fact $c=\sup{(H)}$, since $\delta>0$. Same proof works if $f(c)>k$, thus $f(c)=k$. This is a proof for the intermediate value theorem given by my lecturer, I was wondering if someone could explain a few things: What is the set $H$, what does it define? Why does contradicting the fact that $c=\sup{(H)}$ prove that $f(c)\not<k$? How would I continue to actually finish this proof, ie. show $f(c)\not>k$?","Theorem: Let $f$ be continuous on $[a,\,b]$ and assume $f(a)<f(b)$. Then for every $k$ such that $f(a)<k<f(b)$, there exists a $c\in[a,\,b]$ such that $f(c)=k$. proof: $f$ continuous at $a\implies$ for $\varepsilon=k-f(a)>0$, $\exists\delta>0$ s.t. $$|f(x)-f(a)|<\varepsilon=k-f(a)\quad\forall x\colon |x-a|<\delta.$$ Consider the set $H=\{x\in[a,\,b]\colon f(x)<k\}\not=\emptyset \implies c=\sup{(H)}$. Show $f(c)=k$, suppose $f(c)<k\iff k-f(c)>0$. We know $f$ is continuous at $c$ so $\forall\varepsilon>k-f(c)>0$ $\exists\delta>0:|f(x)-f(c)|<\varepsilon=k-f(c)$ when $|x-c|<\delta$. $\implies f(x)-f(c)<k-f(c)$. Say $x=c+\delta/2\implies f(x)<k\implies c+\delta/2\in H$ which contradicts the fact $c=\sup{(H)}$, since $\delta>0$. Same proof works if $f(c)>k$, thus $f(c)=k$. This is a proof for the intermediate value theorem given by my lecturer, I was wondering if someone could explain a few things: What is the set $H$, what does it define? Why does contradicting the fact that $c=\sup{(H)}$ prove that $f(c)\not<k$? How would I continue to actually finish this proof, ie. show $f(c)\not>k$?",,"['real-analysis', 'proof-explanation']"
78,"If $\int_0^1 f(x)x^n \ dx=0$ for every $n$, then $f=0$. [duplicate]","If  for every , then . [duplicate]",\int_0^1 f(x)x^n \ dx=0 n f=0,"This question already has answers here : Closed 11 years ago . Possible Duplicates: Nonzero $f \in C([0, 1])$ for which $\int_0^1 f(x)x^n dx = 0$ for all $n$ Slight generalization of an exercise in (blue) Rudin What can we say about $f$ if $\int_0^1 f(x)p(x)dx=0$ for all polynomials $p$ ? I found a nice problem I would like to share. Problem: If $f$ is continuous on $[0,1]$ , and if $$\int_0^1 f(x)x^n \ dx =0$$ for every non-negative integer $n$ , prove that $f(x)=0$ on $[0,1]$ . Source: W. Rudin, Principles of Mathematical Analysis , Chapter 7, Exercise 20. I have posted a proposed solution in the answers.","This question already has answers here : Closed 11 years ago . Possible Duplicates: Nonzero $f \in C([0, 1])$ for which $\int_0^1 f(x)x^n dx = 0$ for all $n$ Slight generalization of an exercise in (blue) Rudin What can we say about if for all polynomials ? I found a nice problem I would like to share. Problem: If is continuous on , and if for every non-negative integer , prove that on . Source: W. Rudin, Principles of Mathematical Analysis , Chapter 7, Exercise 20. I have posted a proposed solution in the answers.","f \int_0^1 f(x)p(x)dx=0 p f [0,1] \int_0^1 f(x)x^n \ dx =0 n f(x)=0 [0,1]",['real-analysis']
79,"Showing that if $\sum a_n,a_n\ge 0$ converges, then $\sum_{n=1}^\infty\frac {1}{n^2a_n}$ diverges.","Showing that if  converges, then  diverges.","\sum a_n,a_n\ge 0 \sum_{n=1}^\infty\frac {1}{n^2a_n}","Showing that $\sum_{n=1}^\infty\frac {1}{n^2a_n}$ diverges if $\sum a_n,a_n\ge 0$ converges. Since $\sum a_n$ converges, there exists $N$ such that for any $m\gt N$ , the following holds: $$a_{m+1}+a_{m+2}+\cdots+a_{2m}<\frac 12$$ By AM $\ge$ HM, $$\frac{\sum_{n=m+1}^{2m}a_n}{m}\ge \frac m{\sum_{n=m+1}^{2m}\frac 1{a_n} }\implies \frac 1{m^2}\sum_{n=m+1}^{2m}\frac 1{a_n}\ge \frac 1{\sum_{n=m+1}^{2m}a_n}\gt2. \tag 1$$ Let $S_n:=\sum_{j=1}^n\frac {1}{j^2a_j}$ $$|S_{2m}-S_m|\ge \frac 1{4m^2}\sum_{n=m+1}^{2m}\frac 1{a_n}\gt \frac 12,$$ (by $(1)$ ). It follows that $\lim S_n$ doesn't exist. Is my proof correct? Thanks.","Showing that diverges if converges. Since converges, there exists such that for any , the following holds: By AM HM, Let (by ). It follows that doesn't exist. Is my proof correct? Thanks.","\sum_{n=1}^\infty\frac {1}{n^2a_n} \sum a_n,a_n\ge 0 \sum a_n N m\gt N a_{m+1}+a_{m+2}+\cdots+a_{2m}<\frac 12 \ge \frac{\sum_{n=m+1}^{2m}a_n}{m}\ge \frac m{\sum_{n=m+1}^{2m}\frac 1{a_n} }\implies \frac 1{m^2}\sum_{n=m+1}^{2m}\frac 1{a_n}\ge \frac 1{\sum_{n=m+1}^{2m}a_n}\gt2. \tag 1 S_n:=\sum_{j=1}^n\frac {1}{j^2a_j} |S_{2m}-S_m|\ge \frac 1{4m^2}\sum_{n=m+1}^{2m}\frac 1{a_n}\gt \frac 12, (1) \lim S_n","['real-analysis', 'sequences-and-series', 'solution-verification']"
80,"Prove that $\int_0^1 \big(1-x^2\big) \big(f'(x)\big)^2\,dx \ge 24 \left(\int_0^1 xf(x)\,dx\right)^{\!2}$",Prove that,"\int_0^1 \big(1-x^2\big) \big(f'(x)\big)^2\,dx \ge 24 \left(\int_0^1 xf(x)\,dx\right)^{\!2}","Prove that if $f:[0,1] \to \mathbb{R}$ is a continuously differentiable function with $\int_0^1 f(x)\,dx=0$ , then $$\int_0^1 \big(1-x^2\big) \big(f'(x)\big)^2\,dx \ge 24 \left(\int_0^1 xf(x)\,dx\right)^{\!2}.$$ I think that I should somehow use the Cauchy-Schwarz inequality, but I wasn't succesful in doing this. I know that $$\left(\int_0^1 xf(x)\,dx\right)^2\le \int_0^1 x^2\,dx \cdot \int _0^1 f^2(x)\,dx=\frac{1}{3}\int _0^1 f^2(x)\,dx,$$ but this is clearly not enough.","Prove that if is a continuously differentiable function with , then I think that I should somehow use the Cauchy-Schwarz inequality, but I wasn't succesful in doing this. I know that but this is clearly not enough.","f:[0,1] \to \mathbb{R} \int_0^1 f(x)\,dx=0 \int_0^1 \big(1-x^2\big) \big(f'(x)\big)^2\,dx \ge 24 \left(\int_0^1 xf(x)\,dx\right)^{\!2}. \left(\int_0^1 xf(x)\,dx\right)^2\le \int_0^1 x^2\,dx \cdot \int _0^1 f^2(x)\,dx=\frac{1}{3}\int _0^1 f^2(x)\,dx,","['real-analysis', 'inequality', 'integral-inequality']"
81,Derivative of continuous function exists if limit of derivative exists,Derivative of continuous function exists if limit of derivative exists,,"I'm stuck on this old qualifier problem.  I suppose one could do it using the basic definitions of continuity and differentiability, but is there a simpler way? (For example, using DCT, FTC, Lebesgue differentiation theorem, etc.) Let $f:\mathbb{R} \mapsto \mathbb{R}$ be continuous.  Suppose $f$ is differentiable away from $0$ and lim$_{x \to 0} f^\prime(x)$ exists.  Show $f^\prime(0)$ exists.","I'm stuck on this old qualifier problem.  I suppose one could do it using the basic definitions of continuity and differentiability, but is there a simpler way? (For example, using DCT, FTC, Lebesgue differentiation theorem, etc.) Let $f:\mathbb{R} \mapsto \mathbb{R}$ be continuous.  Suppose $f$ is differentiable away from $0$ and lim$_{x \to 0} f^\prime(x)$ exists.  Show $f^\prime(0)$ exists.",,"['calculus', 'real-analysis', 'analysis']"
82,Proving that if $|f''(x)| \le A$ then $|f'(x)| \le A/2$,Proving that if  then,|f''(x)| \le A |f'(x)| \le A/2,"Suppose that $f(x)$ is differentiable on $[0,1]$ and $f(0) = f(1) = 0$. It is also   known that $|f''(x)| \le A$ for every $x \in (0,1)$. Prove that   $|f'(x)| \le A/2$ for every $x \in [0,1]$. I'll explain what I did so far. First using Rolle's theorem, there is some point $c \in [0,1]$ so $f'(c) = 0$. EDIT: My first preliminary solution was wrong so I tried something else. EDIT2: Another revision :\ I define a Taylor series of a second order around the point $1$: $$ f(x) = f(1) + f'(1)(x-1) + \frac12 f''(d_1)(x-1)^2 $$ $$ f(0) = f(1) + f'(1)(-1) + \frac12 f''(d_1)(-1)^2 $$ $$ |f'(1)| = \frac12 |f''(d_1)| <= \frac12 A $$ Now I develop a Taylor series of a first order for $f'(x)$ around $1$: $$ f'(x) = f'(1) + f''(d_2)(x-1) $$ $$ |f'(x)| = |f'(1)| + x*|f''(d_2)|-|f''(d_2)| \leq \frac{A}{2} + A - A = \frac{A}{2} $$ It looks correct to me, what do you guys think? Note: I cannot use integrals, because we have not covered them yet.","Suppose that $f(x)$ is differentiable on $[0,1]$ and $f(0) = f(1) = 0$. It is also   known that $|f''(x)| \le A$ for every $x \in (0,1)$. Prove that   $|f'(x)| \le A/2$ for every $x \in [0,1]$. I'll explain what I did so far. First using Rolle's theorem, there is some point $c \in [0,1]$ so $f'(c) = 0$. EDIT: My first preliminary solution was wrong so I tried something else. EDIT2: Another revision :\ I define a Taylor series of a second order around the point $1$: $$ f(x) = f(1) + f'(1)(x-1) + \frac12 f''(d_1)(x-1)^2 $$ $$ f(0) = f(1) + f'(1)(-1) + \frac12 f''(d_1)(-1)^2 $$ $$ |f'(1)| = \frac12 |f''(d_1)| <= \frac12 A $$ Now I develop a Taylor series of a first order for $f'(x)$ around $1$: $$ f'(x) = f'(1) + f''(d_2)(x-1) $$ $$ |f'(x)| = |f'(1)| + x*|f''(d_2)|-|f''(d_2)| \leq \frac{A}{2} + A - A = \frac{A}{2} $$ It looks correct to me, what do you guys think? Note: I cannot use integrals, because we have not covered them yet.",,"['real-analysis', 'functions', 'derivatives', 'uniform-continuity']"
83,How to calculate $\int_{0}^{1}(\arcsin{x})(\sin{\frac{\pi}{2}x})dx$?,How to calculate ?,\int_{0}^{1}(\arcsin{x})(\sin{\frac{\pi}{2}x})dx,"How to find the follwing integral's value ? $$\int_{0}^{1}(\arcsin{x})(\sin{\frac{\pi}{2}x})dx$$ Actually, I don't know it can be represented as closed form.","How to find the follwing integral's value ? $$\int_{0}^{1}(\arcsin{x})(\sin{\frac{\pi}{2}x})dx$$ Actually, I don't know it can be represented as closed form.",,"['calculus', 'real-analysis', 'definite-integrals']"
84,$x^2+1=0$ uncountable many solutions [duplicate],uncountable many solutions [duplicate],x^2+1=0,"This question already has answers here : Closed 11 years ago . Possible Duplicate: Why are the solutions of polynomial equations so unconstrained over the quaternions? Coudl someone explain me the following: Why should $x^2+1=0$ have uncountable infinite many solutions $x\in\mathbb H$? In my opinion it has only 4 solutions, namely $i^2=j^2=k^2=ijk=-1$ ?","This question already has answers here : Closed 11 years ago . Possible Duplicate: Why are the solutions of polynomial equations so unconstrained over the quaternions? Coudl someone explain me the following: Why should $x^2+1=0$ have uncountable infinite many solutions $x\in\mathbb H$? In my opinion it has only 4 solutions, namely $i^2=j^2=k^2=ijk=-1$ ?",,"['real-analysis', 'abstract-algebra', 'complex-numbers', 'quaternions']"
85,Unsure of my work evaluating $\int \frac{dx}{\sqrt{x + \sqrt{x + \sqrt{x + \cdots}}}}$,Unsure of my work evaluating,\int \frac{dx}{\sqrt{x + \sqrt{x + \sqrt{x + \cdots}}}},"This Question is an Extension of this Previously Asked Question: Nested root integral $\int_0^1 \frac{dx}{\sqrt{x+\sqrt{x+\sqrt{x}}}}$ I was looking into answering the question of whether it was possible to integrate the fully nested root integral of the variety described in the previous problem: $$ \int \frac{dx}{\sqrt{x+\sqrt{x+\sqrt{x+\ldots}}}} $$ So I started by defining the nested root in another way $$ u=\sqrt{x+u} \therefore \\ u^2-u=x \\ (2u-1)du = dx $$ Using the results of the substitution I have set up $$ \int \frac{dx}{\sqrt{x+\sqrt{x+\sqrt{x+\ldots}}}} = \int \frac{2u-1}{u} du \\ = \int \left(2-\frac{1}{u}\right)du = 2u-\ln(u) $$ I am unsure of my work, as I have never attempted to integrate any infinitely nested functions. Therefore I have no idea whether my method for u-substitution is valid. Am I just living under a rock or have other people seen this method used previously? For such a seemingly intimidating problem it was surely quite easy. I had difficulty checking my work using wolfram alpha, but I managed to confirm that this works for the definite integral limits from $x = 1$ to $x = 2$ and from $x = 1$ to $x = 3$ . Maybe I am just flat out wrong and got lucky on these two calculations?","This Question is an Extension of this Previously Asked Question: Nested root integral $\int_0^1 \frac{dx}{\sqrt{x+\sqrt{x+\sqrt{x}}}}$ I was looking into answering the question of whether it was possible to integrate the fully nested root integral of the variety described in the previous problem: So I started by defining the nested root in another way Using the results of the substitution I have set up I am unsure of my work, as I have never attempted to integrate any infinitely nested functions. Therefore I have no idea whether my method for u-substitution is valid. Am I just living under a rock or have other people seen this method used previously? For such a seemingly intimidating problem it was surely quite easy. I had difficulty checking my work using wolfram alpha, but I managed to confirm that this works for the definite integral limits from to and from to . Maybe I am just flat out wrong and got lucky on these two calculations?","
\int \frac{dx}{\sqrt{x+\sqrt{x+\sqrt{x+\ldots}}}}
 
u=\sqrt{x+u} \therefore
\\
u^2-u=x
\\
(2u-1)du = dx
 
\int \frac{dx}{\sqrt{x+\sqrt{x+\sqrt{x+\ldots}}}} = \int \frac{2u-1}{u} du
\\
= \int \left(2-\frac{1}{u}\right)du = 2u-\ln(u)
 x = 1 x = 2 x = 1 x = 3","['real-analysis', 'integration', 'definite-integrals', 'nested-radicals']"
86,What are the pre- requisites required to learn Real Analysis? [closed],What are the pre- requisites required to learn Real Analysis? [closed],,"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 6 months ago . The community reviewed whether to reopen this question 6 months ago and left it closed: Original close reason(s) were not resolved Improve this question I already have quite a solid foundation in Single and Multivariable calculus. But how do I know if I'm prepared to tackle real analysis? Before I get into Real Analysis, I want to know everything that I need to know first. Reading a book, but having to look up sources on the basics that I missed, is a complete waste of time. Please advise me on everything that I need to know before studying Real Analysis. Please outline what to expect from Real Analysis, and recommend textbook for beginners.","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 6 months ago . The community reviewed whether to reopen this question 6 months ago and left it closed: Original close reason(s) were not resolved Improve this question I already have quite a solid foundation in Single and Multivariable calculus. But how do I know if I'm prepared to tackle real analysis? Before I get into Real Analysis, I want to know everything that I need to know first. Reading a book, but having to look up sources on the basics that I missed, is a complete waste of time. Please advise me on everything that I need to know before studying Real Analysis. Please outline what to expect from Real Analysis, and recommend textbook for beginners.",,['real-analysis']
87,Rearrange all the real numbers between $0$ and $1$,Rearrange all the real numbers between  and,0 1,"Can we rearrange all the real numbers between $0$ and $1$, denote  $f(x):(0,1) \rightarrow (0,1)$ (a bijection), such that for $$\forall 0<x_1<x_2<\dots<x_n<1,$$ neither $$f(x_1) <f(x_2) <\dots <f(x_n)$$ nor $$f(x_1)>f(x_2)>\dots>f(x_n)$$ can be hold $(n>3)$? I'd like to see an answer in particular for $n=4$ and $n=5$.","Can we rearrange all the real numbers between $0$ and $1$, denote  $f(x):(0,1) \rightarrow (0,1)$ (a bijection), such that for $$\forall 0<x_1<x_2<\dots<x_n<1,$$ neither $$f(x_1) <f(x_2) <\dots <f(x_n)$$ nor $$f(x_1)>f(x_2)>\dots>f(x_n)$$ can be hold $(n>3)$? I'd like to see an answer in particular for $n=4$ and $n=5$.",,['real-analysis']
88,An inequality from Littlewood's Miscellany,An inequality from Littlewood's Miscellany,,Why is the following inequality from Chapter 1 of Littlewood's A Mathematician's Miscellany true? Suppose $a_n>0$ for all $n$. Then $$\limsup_{n\to\infty}\left(\frac{1+a_{n+1}}{a_n}\right)^n\geq e.$$,Why is the following inequality from Chapter 1 of Littlewood's A Mathematician's Miscellany true? Suppose $a_n>0$ for all $n$. Then $$\limsup_{n\to\infty}\left(\frac{1+a_{n+1}}{a_n}\right)^n\geq e.$$,,"['real-analysis', 'limits', 'inequality']"
89,How do I prove the completeness of $\ell^p$?,How do I prove the completeness of ?,\ell^p,"Say $\{x_n\}$ is Cauchy in $\ell^p$ and $x$ is its pointwise limit. To argue that $x \in \ell^p$ would the following be correct: Let $\varepsilon > 0$ and let $N$ be s.t. $n,m > N$ $\Rightarrow$ $|x_n - x_m|_p < \varepsilon$. Then $\lim_{m \to \infty} |x_n - x_m|_p = |x-x_n|_p \le \varepsilon$. I saw the following different argument: $|x_n - x_m|_p < \varepsilon$ implies $\left(\sum_{k=0}^M |(x_n - x_m)_k|^p\right)^{1/p} < \varepsilon$ for all $M$ therefore $\lim_{m \to \infty}\left(\sum_{k=0}^M |(x_n - x_m)_k|^p\right)^{1/p} = \left(\sum_{k=0}^M |(x_n - x)_k|^p\right)^{1/p} \le \varepsilon$ for all $M$ therefore $\lim_{M \to \infty} \left(\sum_{k=0}^M |(x_n - x)_k|^p\right)^{1/p} \le \varepsilon$. The difference is to use a finite sum step in between. Is it correct to drop it?  And if not: why not? Norm seems to be continuous so one should be able to exchange norm and limit. Thanks.","Say $\{x_n\}$ is Cauchy in $\ell^p$ and $x$ is its pointwise limit. To argue that $x \in \ell^p$ would the following be correct: Let $\varepsilon > 0$ and let $N$ be s.t. $n,m > N$ $\Rightarrow$ $|x_n - x_m|_p < \varepsilon$. Then $\lim_{m \to \infty} |x_n - x_m|_p = |x-x_n|_p \le \varepsilon$. I saw the following different argument: $|x_n - x_m|_p < \varepsilon$ implies $\left(\sum_{k=0}^M |(x_n - x_m)_k|^p\right)^{1/p} < \varepsilon$ for all $M$ therefore $\lim_{m \to \infty}\left(\sum_{k=0}^M |(x_n - x_m)_k|^p\right)^{1/p} = \left(\sum_{k=0}^M |(x_n - x)_k|^p\right)^{1/p} \le \varepsilon$ for all $M$ therefore $\lim_{M \to \infty} \left(\sum_{k=0}^M |(x_n - x)_k|^p\right)^{1/p} \le \varepsilon$. The difference is to use a finite sum step in between. Is it correct to drop it?  And if not: why not? Norm seems to be continuous so one should be able to exchange norm and limit. Thanks.",,"['real-analysis', 'sequences-and-series', 'proof-writing', 'banach-spaces', 'lp-spaces']"
90,Neat series convergence result,Neat series convergence result,,"This is a nice problem on series convergence that I recently stumbled upon. Given a non-negative sequence of real numbers $(a_n)$ such that $$\sum_{n=1}^\infty a_n < \infty,$$ show that there exists a non-decreasing sequence of non-negative numbers $b_n$ such that $$b_n \to \infty \quad\text{ and } \quad \sum_{n=1}^\infty a_n b_n < \infty.$$ In other words, for every convergent series with non-negative terms, there is another convergent series with ""substantially larger"" terms. I have a solution (see below), but maybe someone else has a different, simpler, and/or more elegant solution.","This is a nice problem on series convergence that I recently stumbled upon. Given a non-negative sequence of real numbers $(a_n)$ such that $$\sum_{n=1}^\infty a_n < \infty,$$ show that there exists a non-decreasing sequence of non-negative numbers $b_n$ such that $$b_n \to \infty \quad\text{ and } \quad \sum_{n=1}^\infty a_n b_n < \infty.$$ In other words, for every convergent series with non-negative terms, there is another convergent series with ""substantially larger"" terms. I have a solution (see below), but maybe someone else has a different, simpler, and/or more elegant solution.",,"['calculus', 'real-analysis', 'sequences-and-series']"
91,What does a simple function actually mean?,What does a simple function actually mean?,,"I am having a problem in understanding clearly what simple function actually means .  Royden says:  A real-valued function  $\phi$ is called simple if it is measurable and assumes only a finite number of values. If $\phi$  is simple and has the ${\alpha_1,\alpha_2,.....\alpha_n}$ values then $\phi=\sum_{i=1} ^n \alpha_i\chi_{A_i}$, where $A_i=${x:$\phi$(x)=$\alpha_i$}. first question is : does $\phi$ have measure zero ? ( because it has a finite number of elements) Why do we write the simple function in such a linear combination ? Suppose if i have to write a general function in terms of a simple function, what should i take care of , so that a function can be written in terms of simple function? Is there a geometric presentation to understand this concept ? Thanks for the help!","I am having a problem in understanding clearly what simple function actually means .  Royden says:  A real-valued function  $\phi$ is called simple if it is measurable and assumes only a finite number of values. If $\phi$  is simple and has the ${\alpha_1,\alpha_2,.....\alpha_n}$ values then $\phi=\sum_{i=1} ^n \alpha_i\chi_{A_i}$, where $A_i=${x:$\phi$(x)=$\alpha_i$}. first question is : does $\phi$ have measure zero ? ( because it has a finite number of elements) Why do we write the simple function in such a linear combination ? Suppose if i have to write a general function in terms of a simple function, what should i take care of , so that a function can be written in terms of simple function? Is there a geometric presentation to understand this concept ? Thanks for the help!",,"['real-analysis', 'measure-theory']"
92,Visualizing Uniform Continuity,Visualizing Uniform Continuity,,"Given a colloquial definition of uniform continuity as $f(x)$ and $f(y)$ can be made to be arbitrarily close when $x$ and $y$ are sufficiently close, and the distance between $x$ and $y$ is independent of $x$ and $y$. I'm not really sure how to picture a uniformly continuous function in my head. I showed that if the derivative of a function is bounded, then it will be uniform continuous. (I had trouble with the converse though.) Thinking along the lines that I need to bound the change in $f$. How do you visualize uniform continuity?","Given a colloquial definition of uniform continuity as $f(x)$ and $f(y)$ can be made to be arbitrarily close when $x$ and $y$ are sufficiently close, and the distance between $x$ and $y$ is independent of $x$ and $y$. I'm not really sure how to picture a uniformly continuous function in my head. I showed that if the derivative of a function is bounded, then it will be uniform continuous. (I had trouble with the converse though.) Thinking along the lines that I need to bound the change in $f$. How do you visualize uniform continuity?",,['real-analysis']
93,"Prove that $\int_0^\infty\,\frac{\sin(kx)}{x(x^2+1)}\,\text{d}x=\frac{\pi}{2}\,\left(1-\exp(-k)\right)$ for all $k\in\mathbb{R}_{\ge0}$.",Prove that  for all .,"\int_0^\infty\,\frac{\sin(kx)}{x(x^2+1)}\,\text{d}x=\frac{\pi}{2}\,\left(1-\exp(-k)\right) k\in\mathbb{R}_{\ge0}","I found the following result (I apologize if this has been posted before, but I could not find anything here).  I am wondering whether there is an approach without using contour integration. Let $k$ be a nonnegative real number.  Prove that $$\int_0^\infty\,\frac{\sin(kx)}{x\,\left(x^2+1\right)}\,\text{d}x=\frac{\pi}{2}\,\Big(1-\exp(-k)\Big)=\pi\,\exp\left(-\frac{k}{2}\right)\,\sinh\left(\frac{k}{2}\right)\,.$$ I am especially interested in a solution not using complex analysis.  However, a complex-analytic solution that is different from mine is very welcome as well.  I have three approaches, but all of them end up using complex analysis as a major part. We also have this very nice consequence.  This result can be proven on its own without knowing the integral $\displaystyle \int_0^\infty\,\frac{\sin(kx)}{x\,\left(x^2+1\right)}\,\text{d}x$. Let $k$ be a nonnegative real number.  Then,   $$\int_0^\infty\,\frac{1-\cos(kx)}{x^2\,\left(x^2+1\right)}\,\text{d}x=\frac{\pi}{2}\,\big(k-1+\exp(-k)\big)\,.$$   Equivalently,   $$\int_0^\infty\,\frac{\sin^2(kx)}{x^2\,\left(x^2+1\right)}\,\text{d}x=\frac{\pi}{4}\,\big(2k-1+\exp(-2k)\big)\,.$$ Interestingly, during my quest to obtain this integral, I discovered two more integral relations, although I do not know how to obtain the exact values of any of them.  The exact values of these integrals involve the exponential integral $\text{Ei}$, where $\text{Ei}(x)=\displaystyle\text{PV}\int_{-\infty}^x\,\frac{\exp(t)}{t}\,\text{d}t$ for all $x\in\mathbb{R}$.  Both of the results below also came from contour integrations. Let $k$ be a nonnegative real number.  Then,   $$\int_0^\infty\,\frac{1-\cos(kx)}{x\,\left(x^2+1\right)}\,\text{d}x=-\frac{2}{\pi}\,\int_0^\infty\,\frac{\ln(x)\,\sin(kx)}{x\,\left(x^2+1\right)}\,\text{d}x\,.$$   and   $$\int_0^\infty\,\frac{\sin(kx)}{x^2+1}\,\text{d}x=-\frac{2}{\pi}\,\int_0^\infty\,\frac{\ln(x)\,\cos(kx)}{x^2+1}\,\text{d}x\,.$$ Mathematica says that  $$\int_0^\infty\,\frac{\sin(kx)}{x^2+1}\,\text{d}x=\frac{\exp(-k)\,\text{Ei}(+k)-\exp(+k)\,\text{Ei}(-k)}{2}$$ and that $$\int_0^\infty\,\frac{1-\cos(kx)}{x\,\left(x^2+1\right)}\,\text{d}x=\gamma+\ln(k)-\frac{\exp(-k)\,\text{Ei}(+k)+\exp(+k)\,\text{Ei}(-k)}{2}\,.$$ Here, $\gamma\approx 0.57722$ is the Euler-Mascheroni constant . Approach I. Consider the meromorphic function $f(z):=\dfrac{\exp(\text{i}kz)}{z(z^2+1)}$ for all $z\in \mathbb{C}\setminus \{0,-\text{i},+\text{i}\}$.  For $\epsilon\in(0,1)$, let $C_\epsilon$ be the positively oriented contour $$\left[+\epsilon,+\frac{1}{\epsilon}\right]\cup \Biggl\{\frac{\exp(\text{i}\theta)}{\epsilon}\,\Bigg|\,\theta\in[0,\pi]\Biggr\}\cup\left[-\frac{1}{\epsilon},-\epsilon\right]\cup \Big\{\epsilon\,\exp(\text{i}\theta)\,\Big|\,\theta\in[\pi,0]\Big\}\,.$$  Write $\Gamma_r$ for the positively-oriented (with respect to $0$) semicircle $\Big\{r\,\exp(\text{i}\theta)\,\Big|\,\theta\in[0,\pi]\Big\}$ for every $r>0$.  We have $$I(k):=\lim\limits_{\epsilon\to 0^+}\,\oint_{C_\epsilon}\,f(z)\,\text{d}z=2\pi\text{i}\,\text{Res}_{z=\text{i}}\big(f(z)\big)=-\pi\text{i}\,\exp(-k)\,.$$  Now, note that $$\lim_{\epsilon\to0^+}\,\int_{\Gamma_\epsilon}\,f(z)\,\text{d}z=\pi\text{i}\text{ and }\lim_{\epsilon\to0^+}\,\int_{\Gamma_{\frac{1}{\epsilon}}}\,f(z)\,\text{d}z=0\,.$$  Because $$I(k)=\int_0^\infty\,\frac{\exp(+\text{i}kx)-\exp(-\text{i}kx)}{x(x^2+1)}\,\text{d}x-\lim_{\epsilon\to0^+}\,\int_{\Gamma_\epsilon}\,f(z)\,\text{d}z+\lim_{\epsilon\to0^+}\,\int_{\Gamma_{\frac{1}{\epsilon}}}\,f(z)\,\text{d}z\,,$$ we see that $$I(k)=2\text{i}\,\int_0^\infty\,\frac{\sin(kx)}{x(x^2+1)}\,\text{d}x-\pi\text{i}\,.$$ The result follows immediately. Approach II. We apply Richard Feynman's integral trick.  First, define $J(k)$ to be the required integral: $$J(k):=\int_0^\infty\,\frac{\sin(kx)}{x(x^2+1)}\,\text{d}x\,.$$  Thus, by the Leibniz Integral Rule, we have $J'(k)=\displaystyle \int_0^\infty\,\frac{\cos(kx)}{x^2+1}\,\text{d}x$.   Let $g(z):=\dfrac{\exp(\text{i}kz)}{z^2+1}$ for all $z\in\mathbb{C}\setminus\{-\text{i},+\text{i}\}$.  It follows that $$\lim_{\epsilon\to0^+}\,\oint_{C_\epsilon}\,g(z)\,\text{d}z=2\,\int_{0}^\infty\,\frac{\cos(kx)}{x^2+1}\,\text{d}x\,,$$ where $C_\epsilon$ is the positively oriented contour $$\left[+\epsilon,+\frac{1}{\epsilon}\right]\cup \Biggl\{\frac{\exp(\text{i}\theta)}{\epsilon}\,\Bigg|\,\theta\in[0,\pi]\Biggr\}\cup\left[-\frac{1}{\epsilon},-\epsilon\right]\cup \Big\{\epsilon\,\exp(\text{i}\theta)\,\Big|\,\theta\in[\pi,0]\Big\}\text{ for }\epsilon\in(0,1)\,.$$   Furthermore, we have $$\oint_{C_\epsilon}\,g(z)\,\text{d}z=2\pi\text{i}\,\text{Res}_{z=\text{i}}\big(g(z)\big)=\pi\,\exp(-k)\text{ for all }\epsilon\in(0,1)$$   Ergo, $J'(k)=\displaystyle\int_{0}^\infty\,\frac{\cos(kx)}{x^2+1}\,\text{d}x=\dfrac{\pi}{2}\,\exp(-k)$.  Since $J(0)=0$, $$J(k)=\int_0^k\,J'(t)\,\text{d}t=\frac{\pi}{2}\,\int_0^k\,\exp(-t)\,\text{d}t=\frac{\pi}{2}\,\big(1-\exp(-k)\big)\,.$$ Approach III. It is easy to see that $\dfrac{\sin(t)}{t}=\displaystyle\frac{1}{2}\,\int_{-1}^{+1}\,\exp(\text{i}t\tau)\,\text{d}\tau$ for all $t\neq 0$.  That is, the required integral is given by $$\begin{align}J(k):=\int_0^\infty\,\frac{\sin(kx)}{x\,\left(x^2+1\right)}\,\text{d}x&=\frac{1}{2}\,\int_{0}^\infty\,\frac{k}{x^2+1}\,\int_{-1}^{+1}\,\exp(\text{i}kxt)\,\text{d}t\,\text{d}x\\&=\frac{1}{2}\,\int_{-\infty}^{+\infty}\,\frac{k}{x^2+1}\,\int_{0}^{1}\,\exp(\text{i}kxt)\,\text{d}t\,\text{d}x\,.\end{align}$$  Using Fubini's Theorem, we obtain $$J(k)=\frac{1}{2}\,\int_0^1\,k\,\int_{-\infty}^{+\infty}\,\frac{\exp(\text{i}kxt)}{x^2+1}\,\text{d}x\,\text{d}t\,.$$ For a real number $R>1$, let $\gamma_R$ be the positively oriented contour $$[-R,+R]\cup\big\{R\,\exp(\text{i}\theta)\,\big|\,\theta\in[0,2\pi]\big\}\,.$$  Then, for $\omega \geq 0$, we have $$\lim_{R\to\infty}\,\oint_{\gamma_R}\,\frac{\exp(\text{i}\omega z)}{z^2+1}\,\text{d}z=\int_{-\infty}^{+\infty}\,\frac{\exp(\text{i}\omega x)}{x^2+1}\,\text{d}x=:K(\omega)\,.$$  Ergo, $$K(\omega)=2\pi\text{i}\,\text{Res}_{z=\text{i}}\left(\frac{\exp(\text{i}\omega z)}{z^2+1}\right)=2\pi\text{i}\,\left(\frac{\exp(-\omega)}{2\text{i}}\right)=\pi\,\exp(-\omega)\,.$$ As $\displaystyle J(k)=\frac{1}{2}\,\int_0^1\,k\,K(kt)\,\text{d}t$, we conclude that $$J(k)=\frac{\pi}{2}\,\int_0^1\,k\,\exp(-kt)\,\text{d}t=\frac{\pi}{2}\,\big(1-\exp(-k)\big)\,.$$  From this proof, we also obtain $$\int_0^\infty\,\frac{\cos(kx)}{x^2+1}\,\text{d}x=\frac{1}{2}\,K(k)=\frac{\pi}{2}\,\exp(-k)\,.$$ P.S. I put my solutions in spoilers for people who want to try to solve the problem without being led on by my attempts.  If you do not want to waste your time doing what I have already done, then please look at the spoilers.  All of the solutions in the spoilers are complex-analytic proofs anyhow, so if you are giving me a real-analytic proof, then there is no way you are going to replicate my work.","I found the following result (I apologize if this has been posted before, but I could not find anything here).  I am wondering whether there is an approach without using contour integration. Let $k$ be a nonnegative real number.  Prove that $$\int_0^\infty\,\frac{\sin(kx)}{x\,\left(x^2+1\right)}\,\text{d}x=\frac{\pi}{2}\,\Big(1-\exp(-k)\Big)=\pi\,\exp\left(-\frac{k}{2}\right)\,\sinh\left(\frac{k}{2}\right)\,.$$ I am especially interested in a solution not using complex analysis.  However, a complex-analytic solution that is different from mine is very welcome as well.  I have three approaches, but all of them end up using complex analysis as a major part. We also have this very nice consequence.  This result can be proven on its own without knowing the integral $\displaystyle \int_0^\infty\,\frac{\sin(kx)}{x\,\left(x^2+1\right)}\,\text{d}x$. Let $k$ be a nonnegative real number.  Then,   $$\int_0^\infty\,\frac{1-\cos(kx)}{x^2\,\left(x^2+1\right)}\,\text{d}x=\frac{\pi}{2}\,\big(k-1+\exp(-k)\big)\,.$$   Equivalently,   $$\int_0^\infty\,\frac{\sin^2(kx)}{x^2\,\left(x^2+1\right)}\,\text{d}x=\frac{\pi}{4}\,\big(2k-1+\exp(-2k)\big)\,.$$ Interestingly, during my quest to obtain this integral, I discovered two more integral relations, although I do not know how to obtain the exact values of any of them.  The exact values of these integrals involve the exponential integral $\text{Ei}$, where $\text{Ei}(x)=\displaystyle\text{PV}\int_{-\infty}^x\,\frac{\exp(t)}{t}\,\text{d}t$ for all $x\in\mathbb{R}$.  Both of the results below also came from contour integrations. Let $k$ be a nonnegative real number.  Then,   $$\int_0^\infty\,\frac{1-\cos(kx)}{x\,\left(x^2+1\right)}\,\text{d}x=-\frac{2}{\pi}\,\int_0^\infty\,\frac{\ln(x)\,\sin(kx)}{x\,\left(x^2+1\right)}\,\text{d}x\,.$$   and   $$\int_0^\infty\,\frac{\sin(kx)}{x^2+1}\,\text{d}x=-\frac{2}{\pi}\,\int_0^\infty\,\frac{\ln(x)\,\cos(kx)}{x^2+1}\,\text{d}x\,.$$ Mathematica says that  $$\int_0^\infty\,\frac{\sin(kx)}{x^2+1}\,\text{d}x=\frac{\exp(-k)\,\text{Ei}(+k)-\exp(+k)\,\text{Ei}(-k)}{2}$$ and that $$\int_0^\infty\,\frac{1-\cos(kx)}{x\,\left(x^2+1\right)}\,\text{d}x=\gamma+\ln(k)-\frac{\exp(-k)\,\text{Ei}(+k)+\exp(+k)\,\text{Ei}(-k)}{2}\,.$$ Here, $\gamma\approx 0.57722$ is the Euler-Mascheroni constant . Approach I. Consider the meromorphic function $f(z):=\dfrac{\exp(\text{i}kz)}{z(z^2+1)}$ for all $z\in \mathbb{C}\setminus \{0,-\text{i},+\text{i}\}$.  For $\epsilon\in(0,1)$, let $C_\epsilon$ be the positively oriented contour $$\left[+\epsilon,+\frac{1}{\epsilon}\right]\cup \Biggl\{\frac{\exp(\text{i}\theta)}{\epsilon}\,\Bigg|\,\theta\in[0,\pi]\Biggr\}\cup\left[-\frac{1}{\epsilon},-\epsilon\right]\cup \Big\{\epsilon\,\exp(\text{i}\theta)\,\Big|\,\theta\in[\pi,0]\Big\}\,.$$  Write $\Gamma_r$ for the positively-oriented (with respect to $0$) semicircle $\Big\{r\,\exp(\text{i}\theta)\,\Big|\,\theta\in[0,\pi]\Big\}$ for every $r>0$.  We have $$I(k):=\lim\limits_{\epsilon\to 0^+}\,\oint_{C_\epsilon}\,f(z)\,\text{d}z=2\pi\text{i}\,\text{Res}_{z=\text{i}}\big(f(z)\big)=-\pi\text{i}\,\exp(-k)\,.$$  Now, note that $$\lim_{\epsilon\to0^+}\,\int_{\Gamma_\epsilon}\,f(z)\,\text{d}z=\pi\text{i}\text{ and }\lim_{\epsilon\to0^+}\,\int_{\Gamma_{\frac{1}{\epsilon}}}\,f(z)\,\text{d}z=0\,.$$  Because $$I(k)=\int_0^\infty\,\frac{\exp(+\text{i}kx)-\exp(-\text{i}kx)}{x(x^2+1)}\,\text{d}x-\lim_{\epsilon\to0^+}\,\int_{\Gamma_\epsilon}\,f(z)\,\text{d}z+\lim_{\epsilon\to0^+}\,\int_{\Gamma_{\frac{1}{\epsilon}}}\,f(z)\,\text{d}z\,,$$ we see that $$I(k)=2\text{i}\,\int_0^\infty\,\frac{\sin(kx)}{x(x^2+1)}\,\text{d}x-\pi\text{i}\,.$$ The result follows immediately. Approach II. We apply Richard Feynman's integral trick.  First, define $J(k)$ to be the required integral: $$J(k):=\int_0^\infty\,\frac{\sin(kx)}{x(x^2+1)}\,\text{d}x\,.$$  Thus, by the Leibniz Integral Rule, we have $J'(k)=\displaystyle \int_0^\infty\,\frac{\cos(kx)}{x^2+1}\,\text{d}x$.   Let $g(z):=\dfrac{\exp(\text{i}kz)}{z^2+1}$ for all $z\in\mathbb{C}\setminus\{-\text{i},+\text{i}\}$.  It follows that $$\lim_{\epsilon\to0^+}\,\oint_{C_\epsilon}\,g(z)\,\text{d}z=2\,\int_{0}^\infty\,\frac{\cos(kx)}{x^2+1}\,\text{d}x\,,$$ where $C_\epsilon$ is the positively oriented contour $$\left[+\epsilon,+\frac{1}{\epsilon}\right]\cup \Biggl\{\frac{\exp(\text{i}\theta)}{\epsilon}\,\Bigg|\,\theta\in[0,\pi]\Biggr\}\cup\left[-\frac{1}{\epsilon},-\epsilon\right]\cup \Big\{\epsilon\,\exp(\text{i}\theta)\,\Big|\,\theta\in[\pi,0]\Big\}\text{ for }\epsilon\in(0,1)\,.$$   Furthermore, we have $$\oint_{C_\epsilon}\,g(z)\,\text{d}z=2\pi\text{i}\,\text{Res}_{z=\text{i}}\big(g(z)\big)=\pi\,\exp(-k)\text{ for all }\epsilon\in(0,1)$$   Ergo, $J'(k)=\displaystyle\int_{0}^\infty\,\frac{\cos(kx)}{x^2+1}\,\text{d}x=\dfrac{\pi}{2}\,\exp(-k)$.  Since $J(0)=0$, $$J(k)=\int_0^k\,J'(t)\,\text{d}t=\frac{\pi}{2}\,\int_0^k\,\exp(-t)\,\text{d}t=\frac{\pi}{2}\,\big(1-\exp(-k)\big)\,.$$ Approach III. It is easy to see that $\dfrac{\sin(t)}{t}=\displaystyle\frac{1}{2}\,\int_{-1}^{+1}\,\exp(\text{i}t\tau)\,\text{d}\tau$ for all $t\neq 0$.  That is, the required integral is given by $$\begin{align}J(k):=\int_0^\infty\,\frac{\sin(kx)}{x\,\left(x^2+1\right)}\,\text{d}x&=\frac{1}{2}\,\int_{0}^\infty\,\frac{k}{x^2+1}\,\int_{-1}^{+1}\,\exp(\text{i}kxt)\,\text{d}t\,\text{d}x\\&=\frac{1}{2}\,\int_{-\infty}^{+\infty}\,\frac{k}{x^2+1}\,\int_{0}^{1}\,\exp(\text{i}kxt)\,\text{d}t\,\text{d}x\,.\end{align}$$  Using Fubini's Theorem, we obtain $$J(k)=\frac{1}{2}\,\int_0^1\,k\,\int_{-\infty}^{+\infty}\,\frac{\exp(\text{i}kxt)}{x^2+1}\,\text{d}x\,\text{d}t\,.$$ For a real number $R>1$, let $\gamma_R$ be the positively oriented contour $$[-R,+R]\cup\big\{R\,\exp(\text{i}\theta)\,\big|\,\theta\in[0,2\pi]\big\}\,.$$  Then, for $\omega \geq 0$, we have $$\lim_{R\to\infty}\,\oint_{\gamma_R}\,\frac{\exp(\text{i}\omega z)}{z^2+1}\,\text{d}z=\int_{-\infty}^{+\infty}\,\frac{\exp(\text{i}\omega x)}{x^2+1}\,\text{d}x=:K(\omega)\,.$$  Ergo, $$K(\omega)=2\pi\text{i}\,\text{Res}_{z=\text{i}}\left(\frac{\exp(\text{i}\omega z)}{z^2+1}\right)=2\pi\text{i}\,\left(\frac{\exp(-\omega)}{2\text{i}}\right)=\pi\,\exp(-\omega)\,.$$ As $\displaystyle J(k)=\frac{1}{2}\,\int_0^1\,k\,K(kt)\,\text{d}t$, we conclude that $$J(k)=\frac{\pi}{2}\,\int_0^1\,k\,\exp(-kt)\,\text{d}t=\frac{\pi}{2}\,\big(1-\exp(-k)\big)\,.$$  From this proof, we also obtain $$\int_0^\infty\,\frac{\cos(kx)}{x^2+1}\,\text{d}x=\frac{1}{2}\,K(k)=\frac{\pi}{2}\,\exp(-k)\,.$$ P.S. I put my solutions in spoilers for people who want to try to solve the problem without being led on by my attempts.  If you do not want to waste your time doing what I have already done, then please look at the spoilers.  All of the solutions in the spoilers are complex-analytic proofs anyhow, so if you are giving me a real-analytic proof, then there is no way you are going to replicate my work.",,"['real-analysis', 'integration', 'definite-integrals', 'improper-integrals', 'contour-integration']"
94,Proof of archimedean property,Proof of archimedean property,,"I am trying to self-study Baby Rudin (and it's proving quite challenging to me) Could someone clarify where the underlined part comes from? Text: (a) If $x \in R, y \in R,$ and $x > 0$ , then there is a positive integer $n$ such that $nx > y$ . Proof (a) Let $A$ be the set of all $nx$ , where $n$ runs through the positive integers.  If (a) were false, then $y$ would be an upper bound of $A$ .  But then $A$ has a least upper bound in $\mathbb{R}$ .  Put $\alpha = \sup A$ .  Since $x > 0$ , $\alpha - x < \alpha$ , and $\alpha - x$ is not an upper bound of $A$ . $\underline{\text{Hence $\alpha - x < mx$ for some positive integer $m$}}$ .  But then $\alpha < (m+1)x \in A$ , which is impossible, since $\alpha$ is an upper bound of $A$ . Thanks in advance","I am trying to self-study Baby Rudin (and it's proving quite challenging to me) Could someone clarify where the underlined part comes from? Text: (a) If and , then there is a positive integer such that . Proof (a) Let be the set of all , where runs through the positive integers.  If (a) were false, then would be an upper bound of .  But then has a least upper bound in .  Put .  Since , , and is not an upper bound of . .  But then , which is impossible, since is an upper bound of . Thanks in advance","x \in R, y \in R, x > 0 n nx > y A nx n y A A \mathbb{R} \alpha = \sup A x > 0 \alpha - x < \alpha \alpha - x A \underline{\text{Hence \alpha - x < mx for some positive integer m}} \alpha < (m+1)x \in A \alpha A",['real-analysis']
95,If $\lim_{n\to\infty} x_{n+1}-\frac12 x_n = 0$ then $x_n\to 0$,If  then,\lim_{n\to\infty} x_{n+1}-\frac12 x_n = 0 x_n\to 0,"Let $x_n$ be a sequence of real numbers such that $\displaystyle\lim_{n\to\infty} x_{n+1}-\frac12 x_n = 0$ Prove that $\lim_{n\to\infty} x_n = 0$ I have a proof if one assumes that $(x_n)$ is bounded. In that case, $(x_n)$ has as accumulation point, say $\alpha$ , and the hypothesis $\lim_{n\to\infty} x_{n+1}-\frac12 x_n = 0$ implies that $2\alpha$ is also an accumulation point of $(x_n)$ . Hence $\forall p \;, \; 2^p\alpha$ is an accumulation point, which contradicts the boundedness assumption, unless $\alpha =0$ . $0$ is  consequently the only accumulation point of $(x_n)$ and the claim is proved. It remains to show that $(x_n)$ is bounded. I've tried contradiction for that matter, without success.","Let be a sequence of real numbers such that Prove that I have a proof if one assumes that is bounded. In that case, has as accumulation point, say , and the hypothesis implies that is also an accumulation point of . Hence is an accumulation point, which contradicts the boundedness assumption, unless . is  consequently the only accumulation point of and the claim is proved. It remains to show that is bounded. I've tried contradiction for that matter, without success.","x_n \displaystyle\lim_{n\to\infty} x_{n+1}-\frac12 x_n = 0 \lim_{n\to\infty} x_n = 0 (x_n) (x_n) \alpha \lim_{n\to\infty} x_{n+1}-\frac12 x_n = 0 2\alpha (x_n) \forall p \;, \; 2^p\alpha \alpha =0 0 (x_n) (x_n)","['real-analysis', 'sequences-and-series', 'limits']"
96,$\ell^p$ is not isometric to $\ell^q$,is not isometric to,\ell^p \ell^q,The problem is this: if $1\le p<q<\infty$ then $\ell^p$ and $\ell^q$ are not isometric (as Banach spaces). This is an exercise but I'd like to see an elegant proof.,The problem is this: if $1\le p<q<\infty$ then $\ell^p$ and $\ell^q$ are not isometric (as Banach spaces). This is an exercise but I'd like to see an elegant proof.,,"['real-analysis', 'functional-analysis', 'banach-spaces', 'lp-spaces']"
97,How to show these two definitions of the Riemann integral are equivalent?,How to show these two definitions of the Riemann integral are equivalent?,,"Let $f:[a,b]\longrightarrow \mathbb{R}$ Let $P=\{a=t_0<t_1<\cdots<t_n=b\}$ be partition of $[a,b]$ $P^*=(P,\xi)$ , $\xi=(\xi_1,\xi_2,\cdots,\xi_n)$ , $t_{i-1}\le\xi_i \le t_i$ We define $\displaystyle \sum (f,P^*)=\sum_{i=1}^{n} f(\xi_i) \cdot (t_i-t_{i-1}) $ $||P||=\underset {1\le i\le n}{\text{max}} (t_i-t_{i-1})$ Definition $1$ We define $\displaystyle \int_a^b f =\lim_{||P||\to0} \sum(f,P^*)$ if the limit exists $\forall \epsilon >0 , \exists\delta>0: \forall P^* ,||P||<\delta \Longrightarrow \left|\displaystyle \int_a^b f-\sum(f,P^*)\right|<\epsilon$ Definition $2$ Another definition of Riemann integral: If there is a number $L$ such that: $\forall \epsilon >0 , \exists P_{\epsilon} $ partition of $[a,b]: \forall P^*=(P,\xi) ,  P_{\epsilon}\subset P $ finer partition $ \Longrightarrow |L-\sum(f,P^*)|<\epsilon$ then $\displaystyle \int_a^b f=L$ We note that Definition $1$ implies Definition $2$, but what about the converse ? Any hints would be appreciated.","Let $f:[a,b]\longrightarrow \mathbb{R}$ Let $P=\{a=t_0<t_1<\cdots<t_n=b\}$ be partition of $[a,b]$ $P^*=(P,\xi)$ , $\xi=(\xi_1,\xi_2,\cdots,\xi_n)$ , $t_{i-1}\le\xi_i \le t_i$ We define $\displaystyle \sum (f,P^*)=\sum_{i=1}^{n} f(\xi_i) \cdot (t_i-t_{i-1}) $ $||P||=\underset {1\le i\le n}{\text{max}} (t_i-t_{i-1})$ Definition $1$ We define $\displaystyle \int_a^b f =\lim_{||P||\to0} \sum(f,P^*)$ if the limit exists $\forall \epsilon >0 , \exists\delta>0: \forall P^* ,||P||<\delta \Longrightarrow \left|\displaystyle \int_a^b f-\sum(f,P^*)\right|<\epsilon$ Definition $2$ Another definition of Riemann integral: If there is a number $L$ such that: $\forall \epsilon >0 , \exists P_{\epsilon} $ partition of $[a,b]: \forall P^*=(P,\xi) ,  P_{\epsilon}\subset P $ finer partition $ \Longrightarrow |L-\sum(f,P^*)|<\epsilon$ then $\displaystyle \int_a^b f=L$ We note that Definition $1$ implies Definition $2$, but what about the converse ? Any hints would be appreciated.",,"['real-analysis', 'integration']"
98,Geometrical Interpertation of Cauchy's Mean Value Theorem,Geometrical Interpertation of Cauchy's Mean Value Theorem,,"Cauchy MVT: If functions f and g are both continuous on the closed interval [a,b], and differentiable on the open interval (a, b), then there exists some c ∈ (a,b), such that $$\frac{f'(c)}{g'(c)}= \frac{f(b) - f(a)}{g(b)-g(a)}$$ Lately, after I proved the CMVT, I was trying to intuitively understand (using geometry of course) the meaning of CMVT by comparing it to the MVT; for, I know that the CMVT is just an extension of the MVT, such that the only difference is that $$g(x)=x$$ for the MVT. Yet, even though this is self-evident, I could not inhabit an intuition through a geometric representation of it; for, in all demonstrations of the MVT I have viewed, I only see one function $$f(x)$$ in the geometrical representation, which thusly implies there exists no $$g(x)$$ - not to my perspective at least.So, I searched for another demonstration of the CMVT and I found something related to parametric curves as follows: And,  since I have not encountered parametric curves, I could not fully grasp this demonstration. So, can anyone help me with my confusions over a graphical demonstration of the CMVT. (Note: If a explanation of this intuition requires a knowledge of parametric curves etc..., feel free to include it in the answer).","Cauchy MVT: If functions f and g are both continuous on the closed interval [a,b], and differentiable on the open interval (a, b), then there exists some c ∈ (a,b), such that $$\frac{f'(c)}{g'(c)}= \frac{f(b) - f(a)}{g(b)-g(a)}$$ Lately, after I proved the CMVT, I was trying to intuitively understand (using geometry of course) the meaning of CMVT by comparing it to the MVT; for, I know that the CMVT is just an extension of the MVT, such that the only difference is that $$g(x)=x$$ for the MVT. Yet, even though this is self-evident, I could not inhabit an intuition through a geometric representation of it; for, in all demonstrations of the MVT I have viewed, I only see one function $$f(x)$$ in the geometrical representation, which thusly implies there exists no $$g(x)$$ - not to my perspective at least.So, I searched for another demonstration of the CMVT and I found something related to parametric curves as follows: And,  since I have not encountered parametric curves, I could not fully grasp this demonstration. So, can anyone help me with my confusions over a graphical demonstration of the CMVT. (Note: If a explanation of this intuition requires a knowledge of parametric curves etc..., feel free to include it in the answer).",,"['calculus', 'real-analysis']"
99,Examples of Baire class 2 functions,Examples of Baire class 2 functions,,"Do you know of examples of Baire class 2 functions which are not Baire class 1 functions, besides the the indicator function of the rationals and the indicator function of the Cantor set?","Do you know of examples of Baire class 2 functions which are not Baire class 1 functions, besides the the indicator function of the rationals and the indicator function of the Cantor set?",,"['real-analysis', 'examples-counterexamples']"
