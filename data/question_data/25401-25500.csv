,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"If $A$ is a square matrix and $Ax = b$ has a unique solution for some $b$, is $A$ necessarily invertible?","If  is a square matrix and  has a unique solution for some , is  necessarily invertible?",A Ax = b b A,Let $A$ be a square matrix. Suppose that $A x = b$ has a unique solution for some $b$. Is $A$ necessarily invertible? I said no because the invertible matrix theorem states that $A x = b$ has a unique solution for each $b$. Is this correct or does the wording not make a difference?,Let $A$ be a square matrix. Suppose that $A x = b$ has a unique solution for some $b$. Is $A$ necessarily invertible? I said no because the invertible matrix theorem states that $A x = b$ has a unique solution for each $b$. Is this correct or does the wording not make a difference?,,"['linear-algebra', 'matrices']"
1,Reference Text that develops Linear Algebra with Knowledge of Abstract Algebra,Reference Text that develops Linear Algebra with Knowledge of Abstract Algebra,,"Background: Due to some unfortunate sequencing, I have developed my abstract algebra skills before most of my linear algebra skills. I've worked through Topics in Algebra by Herstein and generally liked his approach to vector spaces and modules. Besides a very elementary course in linear algebra (where most of the time went towards matrix multiplication), I have not developed any other linear algebra skills. But it seems that it is now necessary for me to do so. Most of the topics that I am looking at now require some background of linear algebra and I still lack understanding of ideas like: bilinear forms, invariant subspaces, eigenvalues, requirements for diagonalization of a matrix and so forth. This brings me to my question (provided there are any) Question: What are some good texts that develop the theory of linear algebra from a perspective of general algebra? Are there any texts that develop the key (elementary) ideas of linear algebra in an abstract setting? Thank you for the help!","Background: Due to some unfortunate sequencing, I have developed my abstract algebra skills before most of my linear algebra skills. I've worked through Topics in Algebra by Herstein and generally liked his approach to vector spaces and modules. Besides a very elementary course in linear algebra (where most of the time went towards matrix multiplication), I have not developed any other linear algebra skills. But it seems that it is now necessary for me to do so. Most of the topics that I am looking at now require some background of linear algebra and I still lack understanding of ideas like: bilinear forms, invariant subspaces, eigenvalues, requirements for diagonalization of a matrix and so forth. This brings me to my question (provided there are any) Question: What are some good texts that develop the theory of linear algebra from a perspective of general algebra? Are there any texts that develop the key (elementary) ideas of linear algebra in an abstract setting? Thank you for the help!",,"['linear-algebra', 'abstract-algebra', 'reference-request', 'soft-question', 'book-recommendation']"
2,What is the purpose of finding the kernel of a matrix,What is the purpose of finding the kernel of a matrix,,Can someone help me understand why you would ever want to calculate the kernel of a matrix? I kept on trying to find applications when $\ker(A) = 0$ is useful but I could not find any. Can someone please show an example where it is important to calculate $\ker(A)$ and we could obtain some information from calculating the kernel?,Can someone help me understand why you would ever want to calculate the kernel of a matrix? I kept on trying to find applications when $\ker(A) = 0$ is useful but I could not find any. Can someone please show an example where it is important to calculate $\ker(A)$ and we could obtain some information from calculating the kernel?,,"['linear-algebra', 'matrices', 'matrix-rank']"
3,Prove that if $I-A$ is invertible then $I-A^p$ is invertible in $\mathrm{Mat}_n(\mathbb{Z}_p)$.,Prove that if  is invertible then  is invertible in .,I-A I-A^p \mathrm{Mat}_n(\mathbb{Z}_p),"How do you prove that if $I-A$ invertible then $I-A^p$ is invertible for  $A \in \mathrm{Mat}_n(\mathbb{Z}_p)$, where $p$ is prime?","How do you prove that if $I-A$ invertible then $I-A^p$ is invertible for  $A \in \mathrm{Mat}_n(\mathbb{Z}_p)$, where $p$ is prime?",,"['linear-algebra', 'finite-fields']"
4,Rank of $ T_3$ s.t $ T_3 (T_1)=T_2$,Rank of  s.t, T_3  T_3 (T_1)=T_2,"Let $T_1,T_2 : R^5 \to R^3$ be linear transformations s.t rank($T_1$)=3 and nullity ($T_2$)=3 . Let $T_3:R^3 \to R^3 $be linear transformation s.t  $ T_3(T_1)=T_2.$ Then find rank of $T_3$","Let $T_1,T_2 : R^5 \to R^3$ be linear transformations s.t rank($T_1$)=3 and nullity ($T_2$)=3 . Let $T_3:R^3 \to R^3 $be linear transformation s.t  $ T_3(T_1)=T_2.$ Then find rank of $T_3$",,['linear-algebra']
5,$2\times 2$ matrix $C=AB-BA$,matrix,2\times 2 C=AB-BA,"Let $C=\begin{pmatrix}c_{11}& c_{12} \\ c_{21} & c_{22}\end{pmatrix}$ be a $2 \times 2$ matrix. Show that there exist matrices $A$ and $B$ such that $C=AB-BA$ if and only if $c_{11}+c_{22}=0$. I could show that if such matrices exist, then the trace of the matrix $C$ is equal to zero, I did it by hand, just adding up the entries of the diagonal of the matrix $AB-BA$. I could not prove the other implication. Suppose that $c_{11}+c_{22}=0$, then $c_{22}=-c_{11}$ so $C$ is of the form $C=\begin{pmatrix}c_{11}& c_{12} \\ c_{21} & -c_{11}\end{pmatrix}$. I've tried to find matrices $A$ and $B$ using this condition. If $$A=\begin{pmatrix}a& b \\ c & d\end{pmatrix},B=\begin{pmatrix}e& f\\ g & h\end{pmatrix},$$ then $$AB-BA=\begin{pmatrix}bg-cf& b(h-e)+f(a-d) \\ c(e-h)+g(d-a) & cf-bg\end{pmatrix}$$ I got stuck trying to find the values of each entry, any help would be appreciated.","Let $C=\begin{pmatrix}c_{11}& c_{12} \\ c_{21} & c_{22}\end{pmatrix}$ be a $2 \times 2$ matrix. Show that there exist matrices $A$ and $B$ such that $C=AB-BA$ if and only if $c_{11}+c_{22}=0$. I could show that if such matrices exist, then the trace of the matrix $C$ is equal to zero, I did it by hand, just adding up the entries of the diagonal of the matrix $AB-BA$. I could not prove the other implication. Suppose that $c_{11}+c_{22}=0$, then $c_{22}=-c_{11}$ so $C$ is of the form $C=\begin{pmatrix}c_{11}& c_{12} \\ c_{21} & -c_{11}\end{pmatrix}$. I've tried to find matrices $A$ and $B$ using this condition. If $$A=\begin{pmatrix}a& b \\ c & d\end{pmatrix},B=\begin{pmatrix}e& f\\ g & h\end{pmatrix},$$ then $$AB-BA=\begin{pmatrix}bg-cf& b(h-e)+f(a-d) \\ c(e-h)+g(d-a) & cf-bg\end{pmatrix}$$ I got stuck trying to find the values of each entry, any help would be appreciated.",,"['linear-algebra', 'matrices']"
6,Kreyszig's Functional Analysis Section 2.8: How is the canonical embedding map injective?,Kreyszig's Functional Analysis Section 2.8: How is the canonical embedding map injective?,,"Let $X$ be a vector space over the field $K$ of the real or complex numbers. Let $X^*$ denote the vector space of all linear functionals defined on $X$, and let $X^{**}$ denote the vector space of all linear functionals defined on $X^*$. Let $x \in X$ be fixed, and let $g_x \colon X^* \to X^{**}$ be defined as  $$g_x(f) \colon= f(x) \; \; \; \forall f \in X^*.$$  Then $g_x$ is a linear functional on $X^*$ so that $g_x \in X^{**}$. Then the mapping $C \colon X \to X^{**}$ defined by  $$Cx \colon= g_x \; \; \; \forall x \in X$$  is linear. But Kryszeg states that this map $C$ is also injective. How to show this? My work: Suppose that, for some $x$, $y \in X$, we have the equality $$Cx = Cy.$$ Then  $$g_x = g_y.$$ So, for all $f \in X^*$, we have  $$g_x(f) = g_y(f).$$  This implies that $f(x) = f(y)$ and so $x-y \in N(f)$ for all linear functionals $f$ defined on $X$, where $N(f)$ denotes the null space of $f$. What next? How to show that $x = y$, especially in the case when $X$ is not finite-dimensional?","Let $X$ be a vector space over the field $K$ of the real or complex numbers. Let $X^*$ denote the vector space of all linear functionals defined on $X$, and let $X^{**}$ denote the vector space of all linear functionals defined on $X^*$. Let $x \in X$ be fixed, and let $g_x \colon X^* \to X^{**}$ be defined as  $$g_x(f) \colon= f(x) \; \; \; \forall f \in X^*.$$  Then $g_x$ is a linear functional on $X^*$ so that $g_x \in X^{**}$. Then the mapping $C \colon X \to X^{**}$ defined by  $$Cx \colon= g_x \; \; \; \forall x \in X$$  is linear. But Kryszeg states that this map $C$ is also injective. How to show this? My work: Suppose that, for some $x$, $y \in X$, we have the equality $$Cx = Cy.$$ Then  $$g_x = g_y.$$ So, for all $f \in X^*$, we have  $$g_x(f) = g_y(f).$$  This implies that $f(x) = f(y)$ and so $x-y \in N(f)$ for all linear functionals $f$ defined on $X$, where $N(f)$ denotes the null space of $f$. What next? How to show that $x = y$, especially in the case when $X$ is not finite-dimensional?",,"['real-analysis', 'linear-algebra', 'functional-analysis', 'vector-spaces', 'operator-theory']"
7,prove that $|AA^t|=nk^2$ ; $k\in \mathbb Z$,prove that  ;,|AA^t|=nk^2 k\in \mathbb Z,"Let $A_{n-1 \times n}=(a_{ij})$ be with entries in $\mathbb Z$ such that $\sum_{j=1}^{n}a_{ij}=0 , \forall i\in \{1,2,\cdots,n-1\}$. Show that $|AA^t|=nk^2$ ; $k\in \mathbb Z$.","Let $A_{n-1 \times n}=(a_{ij})$ be with entries in $\mathbb Z$ such that $\sum_{j=1}^{n}a_{ij}=0 , \forall i\in \{1,2,\cdots,n-1\}$. Show that $|AA^t|=nk^2$ ; $k\in \mathbb Z$.",,"['linear-algebra', 'matrices']"
8,Midpoint of the shortest distance between 2 rays in 3D,Midpoint of the shortest distance between 2 rays in 3D,,"I would like to come with an algorithm to find the midpoint of the shortest between 2 rays a+t b and c+s d , where t and s are scalars. I have a scenario which I try to depict like this. One of the strategies I was planning to try was finding out the equation of the plane a(s b )c, and project the point t d on to abc, and then solve for intersection to get q. Then do the same for p, and then add and divide them by 2 to get r. But, without knowing t and s, I'm wondering whether this is the right way to go. Any help on this problem would be appreciated.","I would like to come with an algorithm to find the midpoint of the shortest between 2 rays a+t b and c+s d , where t and s are scalars. I have a scenario which I try to depict like this. One of the strategies I was planning to try was finding out the equation of the plane a(s b )c, and project the point t d on to abc, and then solve for intersection to get q. Then do the same for p, and then add and divide them by 2 to get r. But, without knowing t and s, I'm wondering whether this is the right way to go. Any help on this problem would be appreciated.",,"['linear-algebra', 'geometry', '3d']"
9,Why does the set of positive definite matrices define a half-space?,Why does the set of positive definite matrices define a half-space?,,"A half-space is a set of the form $\{x \mid a^Tx \leq b\}$ . Also it is stated that the set $$\{X \in S^n \mid z^T X z \geq 0 \}$$ where $S^n$ denote the set of symmetric $n\times n$ matrices, is a half space $^1$ , Can we expand it algebraically to prove it? Is is possible to make a graphical intuitive example, e.g in matlab, as the one that can easily be made for $\{x \mid a^Tx \leq b\}$ ? Thanks!! $^{1}$ Convex Optimization by Boyd & Vandenberghe, pp.36","A half-space is a set of the form . Also it is stated that the set where denote the set of symmetric matrices, is a half space , Can we expand it algebraically to prove it? Is is possible to make a graphical intuitive example, e.g in matlab, as the one that can easily be made for ? Thanks!! Convex Optimization by Boyd & Vandenberghe, pp.36",\{x \mid a^Tx \leq b\} \{X \in S^n \mid z^T X z \geq 0 \} S^n n\times n ^1 \{x \mid a^Tx \leq b\} ^{1},"['linear-algebra', 'matrices', 'positive-definite', 'symmetric-matrices']"
10,How to tell if a columns of matrix are linear dependent?,How to tell if a columns of matrix are linear dependent?,,How can it be seen if the following matrix is linear dependent? Let $A= \begin{bmatrix} 0 & -3 & 9&   \\ 2&1& 7 \\ -1& 4 &-5 \\ 1&-4&-2 \end{bmatrix} $ First operation I perfomed was switch r1 to r4 and -2r1+r2 $A= \begin{bmatrix} 1 & -4 & -2&   \\ 0&9& -3 \\ -1& 4 &-5 \\ 0&-3&9 \end{bmatrix} $ Next performed was $r1+r3$ to get Let $A= \begin{bmatrix} 1& -4 & -2&   \\ 0&9& -3\\ 0& 0 &-7 \\ 0&-3&9 \end{bmatrix} $ next done was$-1/3r2+r4$ and got Let $A= \begin{bmatrix} 1& -4 & -2&   \\ 0&9& -3\\ 0& 0 &-7 \\ 0&0&-8 \end{bmatrix} $ Thus it is shown because there no free variable it is linear independent. (I think) My question is is it also linear indepedent because the vectors are not multiples of the first vector.,How can it be seen if the following matrix is linear dependent? Let $A= \begin{bmatrix} 0 & -3 & 9&   \\ 2&1& 7 \\ -1& 4 &-5 \\ 1&-4&-2 \end{bmatrix} $ First operation I perfomed was switch r1 to r4 and -2r1+r2 $A= \begin{bmatrix} 1 & -4 & -2&   \\ 0&9& -3 \\ -1& 4 &-5 \\ 0&-3&9 \end{bmatrix} $ Next performed was $r1+r3$ to get Let $A= \begin{bmatrix} 1& -4 & -2&   \\ 0&9& -3\\ 0& 0 &-7 \\ 0&-3&9 \end{bmatrix} $ next done was$-1/3r2+r4$ and got Let $A= \begin{bmatrix} 1& -4 & -2&   \\ 0&9& -3\\ 0& 0 &-7 \\ 0&0&-8 \end{bmatrix} $ Thus it is shown because there no free variable it is linear independent. (I think) My question is is it also linear indepedent because the vectors are not multiples of the first vector.,,['linear-algebra']
11,Dimension of vector space of 2x2 skew symmetric matrices,Dimension of vector space of 2x2 skew symmetric matrices,,"I had a question about the dimension of this subspace. This was related to a problem that had a case of $n\times n$ matrices, but I accidentally read it as the special case of $2\times 2$ , but never the less the answer to this question should help me with the general case. Question Consider the set of all skew-symmetric $M_{2\times 2}$ matrices, call it $W$ . A matrix $A$ is an element of this set provided that $-A = A^{t}$ . Find a basis for $W$ , what is the dimension of $W$ ? Attempted Answer If we consider $$A = \begin{pmatrix} a &b \\   c&d  \end{pmatrix}$$ Then $$A^{t} = \begin{pmatrix} a & c \\ b & d\end{pmatrix}.$$ So, for a matrix to be in our subspace, we would have $$\begin{pmatrix}-a & -b \\ -c & -d\end{pmatrix} = \begin{pmatrix}a & c \\ b & d\end{pmatrix}$$ This implies that $-a = a$ and $-d = d$ , so $a$ and $d$ are $0$ and that $b = -c$ . Therefore the resulting set of matrices will look like $$\begin{pmatrix}0 & -c\\c & 0\end{pmatrix}.$$ This is where I run into trouble. I'm not sure if the dimension of this subspace is $1$ or $2$ . If I factor the $c$ out, then I have the matrix $$ \begin{pmatrix} 0 & -1 \\ 1 & 0\end{pmatrix}, $$ and by taking any scalar multiple of this matrix, I can create any matrix in the subspace of $2\times 2$ skew-symmetric matrices. It's also linearly independent so it should be a basis. My confusion arises in that I could also decompose the matrix further into the two matrices $$\begin{pmatrix}0 & -c \\ 0 & 0\end{pmatrix} \quad \text{and} \quad  \begin{pmatrix} 0 & 0 \\c & 0\end{pmatrix}$$ These are both linearly independent vectors and span the subspace, so it's a basis, and I was wondering if that means the dimension of the subspace should be $2$ . But I wonder if the fact that the choice of $c$ affects both matrices means that this decomposition isn't necessary and that since you can't choose two different weights for these matrices that the dimension of the subspace must in fact be one. Sorry that was a long post, and maybe not the cleanest, still getting used to Latex. Thanks for any help.","I had a question about the dimension of this subspace. This was related to a problem that had a case of matrices, but I accidentally read it as the special case of , but never the less the answer to this question should help me with the general case. Question Consider the set of all skew-symmetric matrices, call it . A matrix is an element of this set provided that . Find a basis for , what is the dimension of ? Attempted Answer If we consider Then So, for a matrix to be in our subspace, we would have This implies that and , so and are and that . Therefore the resulting set of matrices will look like This is where I run into trouble. I'm not sure if the dimension of this subspace is or . If I factor the out, then I have the matrix and by taking any scalar multiple of this matrix, I can create any matrix in the subspace of skew-symmetric matrices. It's also linearly independent so it should be a basis. My confusion arises in that I could also decompose the matrix further into the two matrices These are both linearly independent vectors and span the subspace, so it's a basis, and I was wondering if that means the dimension of the subspace should be . But I wonder if the fact that the choice of affects both matrices means that this decomposition isn't necessary and that since you can't choose two different weights for these matrices that the dimension of the subspace must in fact be one. Sorry that was a long post, and maybe not the cleanest, still getting used to Latex. Thanks for any help.","n\times n 2\times 2 M_{2\times 2} W A -A = A^{t} W W A = \begin{pmatrix}
a &b \\ 
 c&d 
\end{pmatrix} A^{t} = \begin{pmatrix} a & c \\ b & d\end{pmatrix}. \begin{pmatrix}-a & -b \\ -c & -d\end{pmatrix} = \begin{pmatrix}a & c \\ b & d\end{pmatrix} -a = a -d = d a d 0 b = -c \begin{pmatrix}0 & -c\\c & 0\end{pmatrix}. 1 2 c 
\begin{pmatrix} 0 & -1 \\ 1 & 0\end{pmatrix},
 2\times 2 \begin{pmatrix}0 & -c \\ 0 & 0\end{pmatrix}
\quad \text{and} \quad 
\begin{pmatrix} 0 & 0 \\c & 0\end{pmatrix} 2 c",['linear-algebra']
12,How to get determinant of $A$ in terms of tr$(A^k)$?,How to get determinant of  in terms of tr?,A (A^k),"Suppose that $A$ is $n$-square  matrix such that $t_r:=$ tr$(A^r), r=1, 2, \cdots, n$ are given real numbers. How shall we  compute $\det(A)$ in terms of $t_r$s? I am completely unable to do this. Please help me. Thanks in advance","Suppose that $A$ is $n$-square  matrix such that $t_r:=$ tr$(A^r), r=1, 2, \cdots, n$ are given real numbers. How shall we  compute $\det(A)$ in terms of $t_r$s? I am completely unable to do this. Please help me. Thanks in advance",,"['linear-algebra', 'matrices']"
13,"Is calling a linear-equation a linear-function, misnomer or completely wrong?","Is calling a linear-equation a linear-function, misnomer or completely wrong?",,"From my college life, I remember many professors used to call a linear-equation a linear-function, however: A standard definition of linear function (or linear map) is: $$f(x+y)=f(x)+f(y),$$ $$f(\alpha x)=\alpha f(x).$$ Where as linear equation is defined as: $$f(x)=mx+b.$$ So, linear-equation is NOT a linear-function, according to the definitions defined above. Though, for $b=0$ the linear-equation becomes a linear-function, but it is not true in general. Question: Is it misnomer to call a linear-equation a linear-function, or it is completely wrong to say that? And linear-equation must be considered strictly as an affine-mapping.","From my college life, I remember many professors used to call a linear-equation a linear-function, however: A standard definition of linear function (or linear map) is: $$f(x+y)=f(x)+f(y),$$ $$f(\alpha x)=\alpha f(x).$$ Where as linear equation is defined as: $$f(x)=mx+b.$$ So, linear-equation is NOT a linear-function, according to the definitions defined above. Though, for $b=0$ the linear-equation becomes a linear-function, but it is not true in general. Question: Is it misnomer to call a linear-equation a linear-function, or it is completely wrong to say that? And linear-equation must be considered strictly as an affine-mapping.",,"['linear-algebra', 'soft-question', 'terminology']"
14,Are all fields vector spaces?,Are all fields vector spaces?,,"Are $\mathbb{Z_p},\mathbb{Q},\mathbb{R},\mathbb{C}$ above themselves vector space? Is a field above anoother field a vector space? As for 1. we know that $\Bbb R^n$ is a vector space so in particular it is true for $n=1$ For 2. by definition a vector space is $V$ with addition and multiplication over a field, therefore 2 is true.","Are $\mathbb{Z_p},\mathbb{Q},\mathbb{R},\mathbb{C}$ above themselves vector space? Is a field above anoother field a vector space? As for 1. we know that $\Bbb R^n$ is a vector space so in particular it is true for $n=1$ For 2. by definition a vector space is $V$ with addition and multiplication over a field, therefore 2 is true.",,"['linear-algebra', 'vector-spaces', 'field-theory']"
15,General solution of a system of linear differential equations with multiple generalized eigenvectors,General solution of a system of linear differential equations with multiple generalized eigenvectors,,"I am looking for general solutions for the linear sODE's $$\textbf{x}'(t) = A\textbf{x}(t)$$ with $t \geq 0$ and $A \in \mathbb{R}^{n \times n}$ Let focus on just real eigenvalues and eigenvectors. For the case of $n=2$, where we have one eigenvalue, $\lambda \in \mathbb{R}$, such that $am(\lambda)=2, gm(\lambda)=1$ I know that the general solution is $$\textbf{x}(t) = e^{\lambda t}(c_1 \textbf{v} + c_2\textbf{w}) + te^{\lambda t}(c_2 \textbf{w})$$ where $c_1,c_2 \in \mathbb{R}$, $\textbf{v}$ is the eigenvector corresponding to $\lambda$ and $\textbf{w}$ is a generalized eigenvector of $A$. But what would be the general solution be in the case $n = 3$ with one eigenvalue, $\lambda \in \mathbb{R}$, such that $am(\lambda)=3, gm(\lambda)=1$ or in the case of two eigenvalues  $\lambda_1,\lambda_2$ with $am(\lambda_1)=2, gm(\lambda_1) = 1, am(\lambda_2)=1, gm(\lambda_2) = 1$? So, all in all, how would one find the general solution to such systems of linear differential equations? Full answers are appreciated, but I prefer some hints to find the solution myself. Thanks in advance! Bonus: The same question but then with difference equations.","I am looking for general solutions for the linear sODE's $$\textbf{x}'(t) = A\textbf{x}(t)$$ with $t \geq 0$ and $A \in \mathbb{R}^{n \times n}$ Let focus on just real eigenvalues and eigenvectors. For the case of $n=2$, where we have one eigenvalue, $\lambda \in \mathbb{R}$, such that $am(\lambda)=2, gm(\lambda)=1$ I know that the general solution is $$\textbf{x}(t) = e^{\lambda t}(c_1 \textbf{v} + c_2\textbf{w}) + te^{\lambda t}(c_2 \textbf{w})$$ where $c_1,c_2 \in \mathbb{R}$, $\textbf{v}$ is the eigenvector corresponding to $\lambda$ and $\textbf{w}$ is a generalized eigenvector of $A$. But what would be the general solution be in the case $n = 3$ with one eigenvalue, $\lambda \in \mathbb{R}$, such that $am(\lambda)=3, gm(\lambda)=1$ or in the case of two eigenvalues  $\lambda_1,\lambda_2$ with $am(\lambda_1)=2, gm(\lambda_1) = 1, am(\lambda_2)=1, gm(\lambda_2) = 1$? So, all in all, how would one find the general solution to such systems of linear differential equations? Full answers are appreciated, but I prefer some hints to find the solution myself. Thanks in advance! Bonus: The same question but then with difference equations.",,"['linear-algebra', 'ordinary-differential-equations', 'eigenvalues-eigenvectors', 'dynamical-systems', 'recurrence-relations']"
16,Is the image of a closed subspace under a bounded linear operator closed?,Is the image of a closed subspace under a bounded linear operator closed?,,"This seems obvious, but I can't get the proof straight, and I made up the statement myself, so I'm not sure if it's true in the stated generality. Given a bounded linear operator $T$ in Hilbert space, and a closed subspace $A$, I assert that $T(A)$ is a closed subspace. (Note: my definition of closed is in terms of sequences - the set contains all its limits under countable sequences - not the usual topology definition.) Proof strategy: Let $f:\Bbb N\to T(A)$ be given. Using countable choice, select a function $g:\Bbb N\to A$ such that $T\circ g=f$. Assuming $g$ is convergent to some $x$, we have $x\in A$, and since $T$ is continuous, $T\circ g\to T(x)\in T(A)$. The problem is that the preimage function $g$ may not converge even if $f$ does, and there is no limit on $\frac{|g(x)-g(y)|}{|f(x)-f(y)|}$ even though $T$ is bounded, because the inverse may not even exist so that there are whole subspaces to select very distant $g(x),g(y)$ even if $f(x),f(y)$ are close or even identical. Is my search hopeless - have I bitten off more than I can chew with this statement, or can I somehow restrict the base sets from which the $g(x)$ were drawn so that $g$ will converge? What is the ""correct"" statement along these lines?","This seems obvious, but I can't get the proof straight, and I made up the statement myself, so I'm not sure if it's true in the stated generality. Given a bounded linear operator $T$ in Hilbert space, and a closed subspace $A$, I assert that $T(A)$ is a closed subspace. (Note: my definition of closed is in terms of sequences - the set contains all its limits under countable sequences - not the usual topology definition.) Proof strategy: Let $f:\Bbb N\to T(A)$ be given. Using countable choice, select a function $g:\Bbb N\to A$ such that $T\circ g=f$. Assuming $g$ is convergent to some $x$, we have $x\in A$, and since $T$ is continuous, $T\circ g\to T(x)\in T(A)$. The problem is that the preimage function $g$ may not converge even if $f$ does, and there is no limit on $\frac{|g(x)-g(y)|}{|f(x)-f(y)|}$ even though $T$ is bounded, because the inverse may not even exist so that there are whole subspaces to select very distant $g(x),g(y)$ even if $f(x),f(y)$ are close or even identical. Is my search hopeless - have I bitten off more than I can chew with this statement, or can I somehow restrict the base sets from which the $g(x)$ were drawn so that $g$ will converge? What is the ""correct"" statement along these lines?",,"['linear-algebra', 'hilbert-spaces']"
17,Computing quotients of abelian groups,Computing quotients of abelian groups,,"Suppose that $A \cong \oplus_{i = 1}^{n} Z_{p_{i}^{k_{i}}}$ is some finite abelian group, and $(a_1, a_2, \ldots a_n)$ generates a subgroup $N$. If $\langle (a_1, a_2, \ldots a_n) \rangle$ was a direct sum of subgroups in the decomposition, then the quotient $A / N$ would be easy to compute by everyone's favorite isomorphism theorem. But what about the case when it doesn't decompose nicely? Is it possible to redo the decomposition so that it does? For instance, how would one ""see at a glance"" what $Z_8 \times Z_2 / \langle (2,1) \rangle$ is? (These computations are coming up a lot in computing homology, hence the algebraic topology tag.)","Suppose that $A \cong \oplus_{i = 1}^{n} Z_{p_{i}^{k_{i}}}$ is some finite abelian group, and $(a_1, a_2, \ldots a_n)$ generates a subgroup $N$. If $\langle (a_1, a_2, \ldots a_n) \rangle$ was a direct sum of subgroups in the decomposition, then the quotient $A / N$ would be easy to compute by everyone's favorite isomorphism theorem. But what about the case when it doesn't decompose nicely? Is it possible to redo the decomposition so that it does? For instance, how would one ""see at a glance"" what $Z_8 \times Z_2 / \langle (2,1) \rangle$ is? (These computations are coming up a lot in computing homology, hence the algebraic topology tag.)",,"['linear-algebra', 'group-theory', 'algebraic-topology']"
18,Gap between the induced norm of a matrix and largest Eigenvalue?,Gap between the induced norm of a matrix and largest Eigenvalue?,,"Are there any known results on how big the gap between the absolute value of the largest Eigen  value of matrix and the induced norm can be? More formally, let the induced norm of A is given by $\|A\| = max_{\|x\| = 1}\|Ax\|$ and let $\lambda_{max}$ denote the largest Eigenvalue. I am interested in the quantity $\|A\| - |\lambda_{max}|$. Since the norm bounds the absolute value of the Eigenvalues, the quantity $\|A\| - |\lambda_{max}|$ is always positive. I also know that for positive-definite matrices, the quantity $\|A\| - |\lambda_{max}|$ is zero. But are there any results known for a generic matrix $A$? p.s: I am working on a problem where I am trying to compute a bound on the norm of a matrix. I have bounds on the Eigenvalues of my matrix via Gresghorin's circle theorem and I am trying to see whether I can use that in some way to obtain a bound on the matrix norm.... Edit: To clarify, A is a square matrix over the field of reals and I am using the standard 2-norm on $R^n$","Are there any known results on how big the gap between the absolute value of the largest Eigen  value of matrix and the induced norm can be? More formally, let the induced norm of A is given by $\|A\| = max_{\|x\| = 1}\|Ax\|$ and let $\lambda_{max}$ denote the largest Eigenvalue. I am interested in the quantity $\|A\| - |\lambda_{max}|$. Since the norm bounds the absolute value of the Eigenvalues, the quantity $\|A\| - |\lambda_{max}|$ is always positive. I also know that for positive-definite matrices, the quantity $\|A\| - |\lambda_{max}|$ is zero. But are there any results known for a generic matrix $A$? p.s: I am working on a problem where I am trying to compute a bound on the norm of a matrix. I have bounds on the Eigenvalues of my matrix via Gresghorin's circle theorem and I am trying to see whether I can use that in some way to obtain a bound on the matrix norm.... Edit: To clarify, A is a square matrix over the field of reals and I am using the standard 2-norm on $R^n$",,['linear-algebra']
19,"Characteristic polynomial with coefficients c0=, c1=cn=1. Prove: $V = Ker(T) \oplus T(V) $","Characteristic polynomial with coefficients c0=, c1=cn=1. Prove:",V = Ker(T) \oplus T(V) ,"Question from final exam:  $V$ is a vector space , $\dim V = n$, and $T:V\rightarrow V$ is a linear transformation. We assume that the characteristic polynomial of the linear transformation $$p_T(x) = \sum_{i=0}^nc_ix^i $$  has coefficients of $c_0 = 0$, $c_1=c_n= 1$. I need to prove that:  $$V = Ker(T) \oplus T(V)$$ Please any hints, I don't have a clue how to approach this question.","Question from final exam:  $V$ is a vector space , $\dim V = n$, and $T:V\rightarrow V$ is a linear transformation. We assume that the characteristic polynomial of the linear transformation $$p_T(x) = \sum_{i=0}^nc_ix^i $$  has coefficients of $c_0 = 0$, $c_1=c_n= 1$. I need to prove that:  $$V = Ker(T) \oplus T(V)$$ Please any hints, I don't have a clue how to approach this question.",,"['linear-algebra', 'matrices']"
20,non zero linear functional and which of the following statements are true. (NBHM-$2014$),non zero linear functional and which of the following statements are true. (NBHM-),2014,"Let $V$ be finite dimensional real vector space and let $f$ and $g$ be non zero linear functionals on $V$.  Assume that $\ker(f) \subset \ker(g).$ Which of the following are true?? a. $\ker(f)=\ker(g)$ b. $f=\lambda g$ for some real number $\lambda \ne 0$. c. The linear map $A\colon V\to \mathbb{R}^2$ defined by $Ax=(f(x),g(x))$ for all $x \in V$, is onto. Since $\ker(f)\subset \ker(g)$ we get $\ker(f)=\ker(g)$ and hence (a) and (b) are true. Now the linear map will look like $Ax=(\lambda g(x),g(x))$ . I guess (c) will be false. Not sure though.","Let $V$ be finite dimensional real vector space and let $f$ and $g$ be non zero linear functionals on $V$.  Assume that $\ker(f) \subset \ker(g).$ Which of the following are true?? a. $\ker(f)=\ker(g)$ b. $f=\lambda g$ for some real number $\lambda \ne 0$. c. The linear map $A\colon V\to \mathbb{R}^2$ defined by $Ax=(f(x),g(x))$ for all $x \in V$, is onto. Since $\ker(f)\subset \ker(g)$ we get $\ker(f)=\ker(g)$ and hence (a) and (b) are true. Now the linear map will look like $Ax=(\lambda g(x),g(x))$ . I guess (c) will be false. Not sure though.",,"['linear-algebra', 'functional-analysis']"
21,Prove that there is a natural isomorphism between $V$ and $(V^*)^*$,Prove that there is a natural isomorphism between  and,V (V^*)^*,"Let me just start by saying I'm very very new to this material. I have very little idea what's going on. I've read Wikipedia and a few other sources but this is still very hard for me, so I would much appreciate if someone could help me solve this question, slowly and patiently. We are given $V$ a vector space over field $F$ , $\mathrm{dim}(V)$ is a finite number. Show that there is an isomorphism $i: V \longrightarrow (V^{*})^{*}$ , where $V^*$ is the dual space of $V$ . Could someone please help me with this?","Let me just start by saying I'm very very new to this material. I have very little idea what's going on. I've read Wikipedia and a few other sources but this is still very hard for me, so I would much appreciate if someone could help me solve this question, slowly and patiently. We are given a vector space over field , is a finite number. Show that there is an isomorphism , where is the dual space of . Could someone please help me with this?",V F \mathrm{dim}(V) i: V \longrightarrow (V^{*})^{*} V^* V,"['linear-algebra', 'vector-spaces', 'vector-space-isomorphism']"
22,Consequences when the commutator is a scalar multiple of the identity matrix,Consequences when the commutator is a scalar multiple of the identity matrix,,"I just stumbled over the question below. As to the first, I could easily find out the answer (D) by invoking the commutation relation. But I don't figure out how to solve other two. Could anybody give a hint? Here's the question: The commutator $[B,A] \equiv BA-AB = \lambda I$ of two $n\times n$ matrices $A$ and $B$ is proportional to the identity matrix $I$. Choose the only answer for each question. (1) $[B,A^n]=?\;$ (A) $0\;$ (B) $n\;$ (C) $n\lambda A\;$ (D) $n\lambda A^{n-1}\;$ (E) $n\lambda A^n$ (2) $e^{A+B}=?\;$ (A) $e^{A}e^{B}\;$ (B) $e^{B}e^{A}\;$ (C) $e^{A}e^{B}e^{{1\over 2}[B,A]}\;$ (D) $e^{A}e^{B}e^{[B,A]}$ (3) $e^{A}e^{B}=?\;$ (A) $e^{A+B}\;$ (B) $e^{B}e^{A}\;$ (C) $e^{A}e^{B}e^{[B,A]}\;$ (D) $e^{A}e^{B}e^{-[B,A]}$","I just stumbled over the question below. As to the first, I could easily find out the answer (D) by invoking the commutation relation. But I don't figure out how to solve other two. Could anybody give a hint? Here's the question: The commutator $[B,A] \equiv BA-AB = \lambda I$ of two $n\times n$ matrices $A$ and $B$ is proportional to the identity matrix $I$. Choose the only answer for each question. (1) $[B,A^n]=?\;$ (A) $0\;$ (B) $n\;$ (C) $n\lambda A\;$ (D) $n\lambda A^{n-1}\;$ (E) $n\lambda A^n$ (2) $e^{A+B}=?\;$ (A) $e^{A}e^{B}\;$ (B) $e^{B}e^{A}\;$ (C) $e^{A}e^{B}e^{{1\over 2}[B,A]}\;$ (D) $e^{A}e^{B}e^{[B,A]}$ (3) $e^{A}e^{B}=?\;$ (A) $e^{A+B}\;$ (B) $e^{B}e^{A}\;$ (C) $e^{A}e^{B}e^{[B,A]}\;$ (D) $e^{A}e^{B}e^{-[B,A]}$",,"['linear-algebra', 'matrices']"
23,How Find $\cos{(\pi A)}$ if $A$ is Orthogonal matrix,How Find  if  is Orthogonal matrix,\cos{(\pi A)} A,"let $A_{n\times n}$ is Orthogonal matrix, Find the value  $$\cos{(\pi A)}=?$$ and before I guess $$\cos{(\pi A)}=E-2A$$ Now this is wrong,and this problem relsut is what? I know this http://en.wikipedia.org/wiki/Matrix_exponential and we konw if $x\in Z$,then $$\cos{(\pi x)}=(-1)^x$$ and How find  this value? Thank you","let $A_{n\times n}$ is Orthogonal matrix, Find the value  $$\cos{(\pi A)}=?$$ and before I guess $$\cos{(\pi A)}=E-2A$$ Now this is wrong,and this problem relsut is what? I know this http://en.wikipedia.org/wiki/Matrix_exponential and we konw if $x\in Z$,then $$\cos{(\pi x)}=(-1)^x$$ and How find  this value? Thank you",,[]
24,Is a diagonal matrix diagonalizable?,Is a diagonal matrix diagonalizable?,,"A matrix $A$ is a diagonalizable if there exists a diagonal matrix $D$ such that $A$ is similar to $D$. If $A$ is a diagonal matrix, though, is it diagonalizable? If so, it would seem $D$ would just be $A$. I suppose my real question is if it is even proper to ask if a diagonal matrix is diagonalizable. (I am writing a proof, and I want to be as correct as possible.)","A matrix $A$ is a diagonalizable if there exists a diagonal matrix $D$ such that $A$ is similar to $D$. If $A$ is a diagonal matrix, though, is it diagonalizable? If so, it would seem $D$ would just be $A$. I suppose my real question is if it is even proper to ask if a diagonal matrix is diagonalizable. (I am writing a proof, and I want to be as correct as possible.)",,['linear-algebra']
25,Find all 4x4 A matrices so that $A^4=A^6$,Find all 4x4 A matrices so that,A^4=A^6,"Find all 4x4 A matrices so that $A^4=A^6$. I think the method has to do something with eigenvalues, eigenvectors etc'... Thanks in advance for any assistance!","Find all 4x4 A matrices so that $A^4=A^6$. I think the method has to do something with eigenvalues, eigenvectors etc'... Thanks in advance for any assistance!",,['linear-algebra']
26,Repeated eigenvalues: How to check if eigenvectors are linearly independent or not?,Repeated eigenvalues: How to check if eigenvectors are linearly independent or not?,,"I have two related questions. My question is hypothetical, i.e. not from an actual physical problem. If I give you a matrix and tell you that it has a repeated eigenvalue, can you say anything about whether the eigenvectors of that repeated eigenvalue are linearly independent or not? Is there a general procedure to check? I don't know the elements of the matrix itself so I can't work out the eigenvectors in the usual way. On that note, can someone provide me an example of a matrix with a repeated eigenvalue but where the eigenvectors are not linearly independent? Assume the field is complex numbers.","I have two related questions. My question is hypothetical, i.e. not from an actual physical problem. If I give you a matrix and tell you that it has a repeated eigenvalue, can you say anything about whether the eigenvectors of that repeated eigenvalue are linearly independent or not? Is there a general procedure to check? I don't know the elements of the matrix itself so I can't work out the eigenvectors in the usual way. On that note, can someone provide me an example of a matrix with a repeated eigenvalue but where the eigenvectors are not linearly independent? Assume the field is complex numbers.",,"['linear-algebra', 'eigenvalues-eigenvectors']"
27,Eigenvalues of the product of two symmetric matrices,Eigenvalues of the product of two symmetric matrices,,"This question may seem a little dumb, but I really googled around with no success. Let $A$ and $B$ be two square matrices of size $n$ whose (real) eigenvalues are  denoted by $$\lambda_1(A) \leq \lambda_2(A) \leq \cdots \leq \lambda_n(A)$$ $$\lambda_1(B) \leq \lambda_2(B) \leq \cdots \leq \lambda_n(B)$$ What can I say about $\lambda_i (AB)$ ?","This question may seem a little dumb, but I really googled around with no success. Let and be two square matrices of size whose (real) eigenvalues are  denoted by What can I say about ?",A B n \lambda_1(A) \leq \lambda_2(A) \leq \cdots \leq \lambda_n(A) \lambda_1(B) \leq \lambda_2(B) \leq \cdots \leq \lambda_n(B) \lambda_i (AB),"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices']"
28,Defining an inner product from a norm which satisfies parallelogram law,Defining an inner product from a norm which satisfies parallelogram law,,"Suppose we define inner product on complex inner-product space as the following : $$ \langle u,v\rangle =\frac{\|u+v\|^2 - \|u-v\|^2 + \|u+iv\|^2i - \|u-iv\|^2i}{4}$$ Given that the norm satisfies parallelogram inequality, I want to check the linearity condition of this inner product (I have checked the other conditions). How to apply parallelogram law to show that $\langle u+w,v\rangle = \langle v\rangle + \langle w,v\rangle$","Suppose we define inner product on complex inner-product space as the following : $$ \langle u,v\rangle =\frac{\|u+v\|^2 - \|u-v\|^2 + \|u+iv\|^2i - \|u-iv\|^2i}{4}$$ Given that the norm satisfies parallelogram inequality, I want to check the linearity condition of this inner product (I have checked the other conditions). How to apply parallelogram law to show that $\langle u+w,v\rangle = \langle v\rangle + \langle w,v\rangle$",,"['linear-algebra', 'inner-products']"
29,"If the expectation $\langle v,Mv \rangle$ of an operator is $0$ for all $v$ is the operator $0$?",If the expectation  of an operator is  for all  is the operator ?,"\langle v,Mv \rangle 0 v 0",I ran into this a while back and convinced my self that it was true for all finite dimensional vector spaces with complex coefficients. My question is to what extent could I trust this result in the infinite dimensional case. If there is a circumstance where it is not true then I would like the corresponding counter example. The statement is $$\langle v| \textbf{M} v \rangle = 0 \quad \forall \mid v\rangle \in \mathbb{V} \Rightarrow \textbf{M}=0$$ $\textbf{M}$ is a linear operator on $\mathbb{V}$. $\mathbb{V}$ is a vector space over the field $\mathbb{C}$. In the case of finite dimensions we can see that this is true by going to the eigenbasis of $\textbf{M}$. In that case if $\textbf{M} \mid \lambda_i \rangle  = \lambda_i \mid \lambda_i \rangle $ then $\lambda_i \langle \lambda_i \mid \lambda_i \rangle= \langle \lambda_i \mid \textbf{M} \lambda_i \rangle = 0 \quad \forall i$. This means the diagonal form of $\textbf{M}$ is the zero matrix. This is assuming that the eigen vectors of $\textbf{M}$ can form a basis for the space which I believe is always true in finite dimensional vector spaces with complex coefficients because of the spectral theorem. Thanks ahead of time.,I ran into this a while back and convinced my self that it was true for all finite dimensional vector spaces with complex coefficients. My question is to what extent could I trust this result in the infinite dimensional case. If there is a circumstance where it is not true then I would like the corresponding counter example. The statement is $$\langle v| \textbf{M} v \rangle = 0 \quad \forall \mid v\rangle \in \mathbb{V} \Rightarrow \textbf{M}=0$$ $\textbf{M}$ is a linear operator on $\mathbb{V}$. $\mathbb{V}$ is a vector space over the field $\mathbb{C}$. In the case of finite dimensions we can see that this is true by going to the eigenbasis of $\textbf{M}$. In that case if $\textbf{M} \mid \lambda_i \rangle  = \lambda_i \mid \lambda_i \rangle $ then $\lambda_i \langle \lambda_i \mid \lambda_i \rangle= \langle \lambda_i \mid \textbf{M} \lambda_i \rangle = 0 \quad \forall i$. This means the diagonal form of $\textbf{M}$ is the zero matrix. This is assuming that the eigen vectors of $\textbf{M}$ can form a basis for the space which I believe is always true in finite dimensional vector spaces with complex coefficients because of the spectral theorem. Thanks ahead of time.,,"['linear-algebra', 'vector-spaces', 'operator-theory', 'hilbert-spaces', 'quantum-mechanics']"
30,Maximum distance between two unit norm vectors,Maximum distance between two unit norm vectors,,"I have 2 random vectors. I want to limit the euclidean distance between those two vectors to a certain  number (say 2) by normalizing them. I think that if I normalize them such that they have a unit (L2) norm then any two vectors arbitrarily selected of any dimensionality will have the distance between them at most equal to 2.  Is this correct? If not, is there any way to achieve this. Remember, vectors can take any real values and can have any number of dimensions. Also, how would I do it if I want to limit the cosine distance of any two vectors to a certain value?","I have 2 random vectors. I want to limit the euclidean distance between those two vectors to a certain  number (say 2) by normalizing them. I think that if I normalize them such that they have a unit (L2) norm then any two vectors arbitrarily selected of any dimensionality will have the distance between them at most equal to 2.  Is this correct? If not, is there any way to achieve this. Remember, vectors can take any real values and can have any number of dimensions. Also, how would I do it if I want to limit the cosine distance of any two vectors to a certain value?",,"['linear-algebra', 'vector-spaces']"
31,Singular values in SVD,Singular values in SVD,,"I have recently started reading about SVD. If factorization of a matrix $A$ is required, we calculate the eigenvectors of $AA^T$ and $A^TA$ and they become the column vectors of $U$ and $V$ matrices correspondingly. The $\Sigma$ matrix is filled with square roots of the eigenvalues. Now we find that the eigenvalues of $AA^T$ and $A^TA$ are the same. We also find that the obtained eigenvectors are orthogonal to each other. Question 1: Why are the eigenvalues of $AA^T$ and $A^TA$ the same? Question 2: Are eigenvectors for a matrix always orthogonal? If yes, why? If not, why are they orthogonal here? PS - I'm new to Lin-Al, so it would be really helpful if the explanations are intuitive.","I have recently started reading about SVD. If factorization of a matrix $A$ is required, we calculate the eigenvectors of $AA^T$ and $A^TA$ and they become the column vectors of $U$ and $V$ matrices correspondingly. The $\Sigma$ matrix is filled with square roots of the eigenvalues. Now we find that the eigenvalues of $AA^T$ and $A^TA$ are the same. We also find that the obtained eigenvectors are orthogonal to each other. Question 1: Why are the eigenvalues of $AA^T$ and $A^TA$ the same? Question 2: Are eigenvectors for a matrix always orthogonal? If yes, why? If not, why are they orthogonal here? PS - I'm new to Lin-Al, so it would be really helpful if the explanations are intuitive.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'svd']"
32,Invertibility of a square matrix with zero diagonal elements and positive non-diagonal elements,Invertibility of a square matrix with zero diagonal elements and positive non-diagonal elements,,"$M$ is square and $$M(i,j)=0, i=j$$ $$M(i,j)>0, i\ne j$$ Is $M$ full-rank or invertible? Actually the $M$ I am studying has much stronger properties but I guess the simple conditions above might be enough to make $M$ non-singular. The stronger properties of $M$ are: All elements in $M$ are non-negative integers between $0$ and $N$; The sum of each row is equal to $N$. Edit $N$ is not the dimension of $M$. It's just a constant positive integer. I did search for this problem, but it seemed there was no much work on such matrices. It looks easy, but I don't know how to prove it and I couldn't find a counter-example either.","$M$ is square and $$M(i,j)=0, i=j$$ $$M(i,j)>0, i\ne j$$ Is $M$ full-rank or invertible? Actually the $M$ I am studying has much stronger properties but I guess the simple conditions above might be enough to make $M$ non-singular. The stronger properties of $M$ are: All elements in $M$ are non-negative integers between $0$ and $N$; The sum of each row is equal to $N$. Edit $N$ is not the dimension of $M$. It's just a constant positive integer. I did search for this problem, but it seemed there was no much work on such matrices. It looks easy, but I don't know how to prove it and I couldn't find a counter-example either.",,"['linear-algebra', 'matrices', 'singularity-theory']"
33,Confirming an error in a textbook: O'Neill's Semi-Riemannian Geometry,Confirming an error in a textbook: O'Neill's Semi-Riemannian Geometry,,"I'm working on the following exercise in O'Neill's ""Semi-Riemannian Geometry"": (Page 53, 12) Let $b$ be a symmetric bilinear form on $V$. The nullspace of $b$ is $N = \{v : b(v,w) = 0, \, \forall w \in V\}$. The nullcone of $b$ is the set $\Lambda$ of all null vectors in $V$. Let $A = \Lambda \cup 0$, so $A \supset N$. Prove: (a) $N$ is a subspace, but $A$ is not unless $A=0$ or $V$. (b) $b$ is non-degenerate iff $N=0$; $b$ is definite iff $A=0$. (c) $b$ is semidefinite iff $N=A$. I think two of the claims here are false. $A$ is not a subspace unless it is $0$ or $V$. Counterexample:  Take $V = \mathbb{R}^2$, and let $b(e_1,e_1) = b(e_2,e_2) = 1$ but $b(e_1,e_2) = -1$. Then if we write $v = (x,y)$, the condition $b(v,v) = 0$ corresponds to the equation $$0 = x^2 - 2xy + y^2 = (x-y)^2$$ whose solution space is the diagonal subspace spanned by $(1,1)$, which is a subspace equal to neither $0$ nor $V$. $N=A$ implies $b$ semidefinite. Counterexample: Take $V = \mathbb{R}^2$, $b(e_1,e_1) = -1$, $b(e_2,e_2) = b(e_1,e_2) = 0$. Then if $v = (x,y)$, we have $b(v,v) = -x^2$, so $A$ is the set of vectors with zero in the first coordinate. However, if $v$ has zero in its first coordinate, then it is in $N$, so $A=N$. However, $b$ is not semidefinite. Are these indeed counterexamples, or am I simply confused? Thanks for your time.","I'm working on the following exercise in O'Neill's ""Semi-Riemannian Geometry"": (Page 53, 12) Let $b$ be a symmetric bilinear form on $V$. The nullspace of $b$ is $N = \{v : b(v,w) = 0, \, \forall w \in V\}$. The nullcone of $b$ is the set $\Lambda$ of all null vectors in $V$. Let $A = \Lambda \cup 0$, so $A \supset N$. Prove: (a) $N$ is a subspace, but $A$ is not unless $A=0$ or $V$. (b) $b$ is non-degenerate iff $N=0$; $b$ is definite iff $A=0$. (c) $b$ is semidefinite iff $N=A$. I think two of the claims here are false. $A$ is not a subspace unless it is $0$ or $V$. Counterexample:  Take $V = \mathbb{R}^2$, and let $b(e_1,e_1) = b(e_2,e_2) = 1$ but $b(e_1,e_2) = -1$. Then if we write $v = (x,y)$, the condition $b(v,v) = 0$ corresponds to the equation $$0 = x^2 - 2xy + y^2 = (x-y)^2$$ whose solution space is the diagonal subspace spanned by $(1,1)$, which is a subspace equal to neither $0$ nor $V$. $N=A$ implies $b$ semidefinite. Counterexample: Take $V = \mathbb{R}^2$, $b(e_1,e_1) = -1$, $b(e_2,e_2) = b(e_1,e_2) = 0$. Then if $v = (x,y)$, we have $b(v,v) = -x^2$, so $A$ is the set of vectors with zero in the first coordinate. However, if $v$ has zero in its first coordinate, then it is in $N$, so $A=N$. However, $b$ is not semidefinite. Are these indeed counterexamples, or am I simply confused? Thanks for your time.",,"['linear-algebra', 'differential-geometry']"
34,"Prove that $f$ is a linear combination of $f_1,f_2,\dots,f_n$.",Prove that  is a linear combination of .,"f f_1,f_2,\dots,f_n","Let $V$ be a vector space and let $f, f_1,f_2,\dots,f_n$ be linear maps from $V$ to $\mathbb{R}$. Suppose that $f(x)=0$ whenever $f_1(x)=f_2(x)=\cdots=f_n(x)=0$. Prove that $f$ is a linear combination of $f_1,f_2,\ldots,f_n$. The solution can be found here (first problem). . but I disagree that $a_k$ is guaranteed to exist. What if the set containing all vectors $u\in V$ such that $f_1(u)=f_2(u)=\cdots =f_{k-1}(u)$ is empty?","Let $V$ be a vector space and let $f, f_1,f_2,\dots,f_n$ be linear maps from $V$ to $\mathbb{R}$. Suppose that $f(x)=0$ whenever $f_1(x)=f_2(x)=\cdots=f_n(x)=0$. Prove that $f$ is a linear combination of $f_1,f_2,\ldots,f_n$. The solution can be found here (first problem). . but I disagree that $a_k$ is guaranteed to exist. What if the set containing all vectors $u\in V$ such that $f_1(u)=f_2(u)=\cdots =f_{k-1}(u)$ is empty?",,"['linear-algebra', 'contest-math']"
35,Nth roots of square matrices,Nth roots of square matrices,,Is there a general method (which can be implemented by hand) to finding the $n$-th roots of $2 \times 2$ matrices? Is there a similar method for a general $m \times m$ matrix? (for $n > 1$ and $n\in\mathbb{Z}$),Is there a general method (which can be implemented by hand) to finding the $n$-th roots of $2 \times 2$ matrices? Is there a similar method for a general $m \times m$ matrix? (for $n > 1$ and $n\in\mathbb{Z}$),,"['linear-algebra', 'matrices']"
36,Showing that the zero vector has norm zero,Showing that the zero vector has norm zero,,I need to show that this is a property of a norm.  I know this is supposed to be straightforward but I am somehow not seeing it. The property is  $$\lVert 0\rVert = 0$$ I was trying to use the fact that for norms $$ \lVert\lambda \cdot x\rVert = |\lambda|\cdot  \lVert  x\rVert $$ but it seems to be lacking any real reasoning. EDIT I am using the following for the definition of a norm: 1) $||x|| > 0$ if $x\neq 0$ 2) $||\lambda x|| = |\lambda|||x||$ for $\lambda \in \mathbb{R}$ 3) $||x+y|| \leq ||x|| + ||y||$ Let me know if you need any more information!,I need to show that this is a property of a norm.  I know this is supposed to be straightforward but I am somehow not seeing it. The property is  $$\lVert 0\rVert = 0$$ I was trying to use the fact that for norms $$ \lVert\lambda \cdot x\rVert = |\lambda|\cdot  \lVert  x\rVert $$ but it seems to be lacking any real reasoning. EDIT I am using the following for the definition of a norm: 1) $||x|| > 0$ if $x\neq 0$ 2) $||\lambda x|| = |\lambda|||x||$ for $\lambda \in \mathbb{R}$ 3) $||x+y|| \leq ||x|| + ||y||$ Let me know if you need any more information!,,"['linear-algebra', 'vector-spaces', 'normed-spaces']"
37,row echelon vs reduced row echelon form,row echelon vs reduced row echelon form,,"I apologize if this is a very basic question. I understand the difference between the two forms, but i was curious when row echelon from is enough. where is row echelon form used?. Why shouldn't I always go for reduced row echelon form?","I apologize if this is a very basic question. I understand the difference between the two forms, but i was curious when row echelon from is enough. where is row echelon form used?. Why shouldn't I always go for reduced row echelon form?",,['linear-algebra']
38,"Given a matrix $A$ and what it maps two vectors to, is $0$ an eigenvalue of it?","Given a matrix  and what it maps two vectors to, is  an eigenvalue of it?",A 0,"Studying for my Algebra exam, and this question popped out with no solution in a previous exam: Given a matrix $A$ such that $A \begin{pmatrix} 0 \\ -1 \\ 0 \end{pmatrix} = \begin{pmatrix} -2 \\ -4 \\ 6 \end{pmatrix},\ A \begin{pmatrix} 2 \\ 0 \\ 0 \end{pmatrix} = \begin{pmatrix} 2 \\ 4 \\ -6 \end{pmatrix}$ . (I) Is $0$ an eigenvalue of the matrix? (II) Find a matrix like that, where the sum of its' eigenvalues is $0$ . So I (think) solved (I) but have no clue for (II). Here's my solution for (I): $A \begin{pmatrix} 0 \\ -1 \\ 0 \end{pmatrix} + A \begin{pmatrix} 2 \\ 0 \\ 0 \end{pmatrix} = A \begin{pmatrix} 2 \\ -1 \\ 0 \end{pmatrix} = \begin{pmatrix} -2 \\ -4 \\ 6 \end{pmatrix} + \begin{pmatrix} 2 \\ 4 \\ -6 \end{pmatrix} = 0$ , and then, the vector $v = \begin{pmatrix} 2 \\ -1 \\0 \end{pmatrix}$ supplies that $Av = 0v = 0$ meaning that $0$ is an eigenvalue of $A$ with an eigenvector $v$ .","Studying for my Algebra exam, and this question popped out with no solution in a previous exam: Given a matrix such that . (I) Is an eigenvalue of the matrix? (II) Find a matrix like that, where the sum of its' eigenvalues is . So I (think) solved (I) but have no clue for (II). Here's my solution for (I): , and then, the vector supplies that meaning that is an eigenvalue of with an eigenvector .","A A \begin{pmatrix} 0 \\ -1 \\ 0 \end{pmatrix} = \begin{pmatrix} -2 \\ -4 \\ 6 \end{pmatrix},\ A \begin{pmatrix} 2 \\ 0 \\ 0 \end{pmatrix} = \begin{pmatrix} 2 \\ 4 \\ -6 \end{pmatrix} 0 0 A \begin{pmatrix} 0 \\ -1 \\ 0 \end{pmatrix} + A \begin{pmatrix} 2 \\ 0 \\ 0 \end{pmatrix} = A \begin{pmatrix} 2 \\ -1 \\ 0 \end{pmatrix} = \begin{pmatrix} -2 \\ -4 \\ 6 \end{pmatrix} + \begin{pmatrix} 2 \\ 4 \\ -6 \end{pmatrix} = 0 v = \begin{pmatrix} 2 \\ -1 \\0 \end{pmatrix} Av = 0v = 0 0 A v",['linear-algebra']
39,Prove this block matrices are similar,Prove this block matrices are similar,,"Prove that the block matrices $ \left( \begin{array}{cc} AB & 0\\ B & 0\\ \end{array} \right) $ and $ \left( \begin{array}{cc} 0 & 0\\ B & BA\\ \end{array} \right) $ are similar. Where $\mathbf{K}$ is any field, $A\in \mathbf{K}^{m\times n}$, $B\in \mathbf{K}^{n\times m}$ and both matrices in $\mathbf{K}^{(m+n)\times (m+n)}$. I searched the Internet well enough and found no similar problem. Thanks in advance!","Prove that the block matrices $ \left( \begin{array}{cc} AB & 0\\ B & 0\\ \end{array} \right) $ and $ \left( \begin{array}{cc} 0 & 0\\ B & BA\\ \end{array} \right) $ are similar. Where $\mathbf{K}$ is any field, $A\in \mathbf{K}^{m\times n}$, $B\in \mathbf{K}^{n\times m}$ and both matrices in $\mathbf{K}^{(m+n)\times (m+n)}$. I searched the Internet well enough and found no similar problem. Thanks in advance!",,"['linear-algebra', 'matrices', 'block-matrices']"
40,Matrix Inverses,Matrix Inverses,,"So in class we have been discussing matrix inverses and the quickest way that I know of is to get a matrix A, and put it side by side with the identity matrix, like $[A|I_{n}]$ and apply the Gauss-Jordan algorithm until it is of the form $[I_{n}|A^{-1}$], where $A^{-1}$ will show up assuming A is invertible. We also discussed using the formula $A^{-1}=\frac{\operatorname{adj}(A)}{\det(A)}$, however after a few examples, it was clear that this formula would take far too long to find the inverse of A as the matrix size got bigger. Is the first method I described the quickest way to find a inverse of a matrix or is there a more efficient way?","So in class we have been discussing matrix inverses and the quickest way that I know of is to get a matrix A, and put it side by side with the identity matrix, like $[A|I_{n}]$ and apply the Gauss-Jordan algorithm until it is of the form $[I_{n}|A^{-1}$], where $A^{-1}$ will show up assuming A is invertible. We also discussed using the formula $A^{-1}=\frac{\operatorname{adj}(A)}{\det(A)}$, however after a few examples, it was clear that this formula would take far too long to find the inverse of A as the matrix size got bigger. Is the first method I described the quickest way to find a inverse of a matrix or is there a more efficient way?",,"['linear-algebra', 'matrices', 'inverse']"
41,Spanning a Vector space of matrices by symmetric and skew symmetric matrices.,Spanning a Vector space of matrices by symmetric and skew symmetric matrices.,,"How do I span a vector space of $4\times 4$ matrices with real values by symmetric and skew symmetric matrices? The basis of vector space of $4\times 4$ matrices has 16 elements, each containing one 1 and fifteen 0's. All I have to figure out is finding a combination of symmetric and skew symmetric matrices to get each of these elements. Please just provide a hint. Thank you in advance.","How do I span a vector space of $4\times 4$ matrices with real values by symmetric and skew symmetric matrices? The basis of vector space of $4\times 4$ matrices has 16 elements, each containing one 1 and fifteen 0's. All I have to figure out is finding a combination of symmetric and skew symmetric matrices to get each of these elements. Please just provide a hint. Thank you in advance.",,[]
42,"$f\colon \Bbb R^3 \to \Bbb R^3 $ be defined by $f(x_1,x_2,x_3)=....$",be defined by,"f\colon \Bbb R^3 \to \Bbb R^3  f(x_1,x_2,x_3)=....","I am stuck on the following problem: Let $f\colon \Bbb R^3 \to \Bbb R^3 $ be defined by $f(x_1,x_2,x_3)=(x_2+x_3,x_3+x_1,x_1+x_2).$ Then the first derivative of $f$ is : 1.not invertible anywhere 2.invertible only at the origin 3.invertible everywhere except at the origin 4.invertible everywhere. My problem is I do not know how to calculate the derivative of $f$. Can someone point me in the right direction?","I am stuck on the following problem: Let $f\colon \Bbb R^3 \to \Bbb R^3 $ be defined by $f(x_1,x_2,x_3)=(x_2+x_3,x_3+x_1,x_1+x_2).$ Then the first derivative of $f$ is : 1.not invertible anywhere 2.invertible only at the origin 3.invertible everywhere except at the origin 4.invertible everywhere. My problem is I do not know how to calculate the derivative of $f$. Can someone point me in the right direction?",,[]
43,How can I prove $\det(\overline M)=\overline{\det(M)}$?,How can I prove ?,\det(\overline M)=\overline{\det(M)},"Of course $\overline M$ is the complex conjugate of an $n\times n$ matrix $M$. Someone gave me advice to use the definition of determinant, then it means I have to use cofactor expasion here?","Of course $\overline M$ is the complex conjugate of an $n\times n$ matrix $M$. Someone gave me advice to use the definition of determinant, then it means I have to use cofactor expasion here?",,"['linear-algebra', 'matrices', 'determinant']"
44,"If $A$ is an invertible skew-symmetric matrix, then prove $A^{-1}$ is also skew symmetric","If  is an invertible skew-symmetric matrix, then prove  is also skew symmetric",A A^{-1},"Let $A$ be an invertible skew-symmetric $(2n \times 2n)$-matrix. Prove that $A^{-1}$ is also skew-symmetric. (You may assume that $(AB)^T = B^TA^T$). I did this with a $2 \times 2$ matrix and got that it worked, but I don't know how to show it for a general $2n \times 2n$ matrix, as it is a little harder to calculate the inverse of that. Obviously the hint comes into play somehow but I can't see how. I have the definition of a skew symmetric bileanr function to be $B(u,v) = - B(v,u)$, but again, I can't see how to put this into matrix form and use that. Can someone give me some hints please?","Let $A$ be an invertible skew-symmetric $(2n \times 2n)$-matrix. Prove that $A^{-1}$ is also skew-symmetric. (You may assume that $(AB)^T = B^TA^T$). I did this with a $2 \times 2$ matrix and got that it worked, but I don't know how to show it for a general $2n \times 2n$ matrix, as it is a little harder to calculate the inverse of that. Obviously the hint comes into play somehow but I can't see how. I have the definition of a skew symmetric bileanr function to be $B(u,v) = - B(v,u)$, but again, I can't see how to put this into matrix form and use that. Can someone give me some hints please?",,"['linear-algebra', 'matrices', 'proof-writing', 'inverse']"
45,How to solve this linear matrix equation?,How to solve this linear matrix equation?,,How should I solve the following matrix equation? What is the solution for $X$? $$B X C + B^T X C^T = D$$,How should I solve the following matrix equation? What is the solution for $X$? $$B X C + B^T X C^T = D$$,,"['linear-algebra', 'matrices', 'matrix-equations']"
46,Determinants and diagonalizability,Determinants and diagonalizability,,"Does the determinant of a matrix affect if it is diagonalizable or not? Like, if $\det(A) = 0$ does that mean the matrix is NOT diagonalizable?","Does the determinant of a matrix affect if it is diagonalizable or not? Like, if $\det(A) = 0$ does that mean the matrix is NOT diagonalizable?",,"['linear-algebra', 'matrices', 'determinant', 'diagonalization']"
47,Interpolation of surface normals on the face of a triangle and Goroud shading,Interpolation of surface normals on the face of a triangle and Goroud shading,,"I am learning lightning for 3D graphics. What I'm about to ask about is described on this tutorial page here . The color of a certain pixel with lightning is given by $$D*I*(\hat{L}\cdot \hat{N}))$$ Where $D$ is the color of the surface, $I$ is the color of the light source (also called intensity). $L$ is the vector pointing towards the light source, and $N$ is the surface normal. Note that D and I are not vectors, even though colors are actually 4D vectors in computer science. I assume this is because you're suppose to assume that you do this operation component wise for each of the components of the 4D vector (except the last component probably, which is the alpha value. If you can, let me know if my assumption on this is correct). For directional lightning (the direction to the light never changes, like the sun), a technique used is called gouraud shading. The vertices of a face (triangle) have predefined normals and the lightning color is calculated at each of the vertices, and then interpolated along the entire face of the triangle. This means that a color of a point along the interpolation can be given as $$(D*I*(\hat{L}\cdot \hat{N_{a}}))\alpha +(D*I*(\hat{L}\cdot \hat{N_{b}}))(1-\alpha)$$ Alpha is the interpolation value. $N_{a}$ and $N_{b}$ are two different normals at different vertices. The only reason this technique appears realistic is because the direction to the light does not change. Really, this way of calculating it is a shortcut, the proper way to do it is by just interpolating the normals instead of the color result. Like this. $$D*I*(\hat{L}\cdot (\hat{N_{a}}\alpha+\hat{N_{b}}(1-\alpha)))$$ However, this is a more expensive interpolation. At the page I linked in the second sentence of this post, there is a proof that the above equation is equal to the below equation. $$(D*I*(\hat{L}\cdot \hat{N_{a}}))\alpha +(D*I*(\hat{L}\cdot \hat{N_{b}}))(1-\alpha)$$ That is why goroud shading supposedly works. However, I have thought of a case where I think goroud shading does not work, meaning interpolating the resulting colors at the vertices of the face will not achieve the correct effect as interpolating the normals would. Consider this image. Pretend it is a triangular face. Pretend the direction to the light is given by the straight ""up"" vector at the center of that face as seen in the picture. Therefore, the face should be brightest at the center of the face. However, if you use goroud shading, notice it would not achieve this effect because the vertices of the face are supposed to be slightly darker than the middle. This is because the normals at the vertices are slightly pointed away from the direction towards the light. So interpolating dark to dark will obviously not achieve a brighter color in the middle. If I am correct in this, does this mean the proof that showed the interpolation of the normals is equivalent to the interpolation of the results is wrong, or is there something else going on that I am missing?","I am learning lightning for 3D graphics. What I'm about to ask about is described on this tutorial page here . The color of a certain pixel with lightning is given by $$D*I*(\hat{L}\cdot \hat{N}))$$ Where $D$ is the color of the surface, $I$ is the color of the light source (also called intensity). $L$ is the vector pointing towards the light source, and $N$ is the surface normal. Note that D and I are not vectors, even though colors are actually 4D vectors in computer science. I assume this is because you're suppose to assume that you do this operation component wise for each of the components of the 4D vector (except the last component probably, which is the alpha value. If you can, let me know if my assumption on this is correct). For directional lightning (the direction to the light never changes, like the sun), a technique used is called gouraud shading. The vertices of a face (triangle) have predefined normals and the lightning color is calculated at each of the vertices, and then interpolated along the entire face of the triangle. This means that a color of a point along the interpolation can be given as $$(D*I*(\hat{L}\cdot \hat{N_{a}}))\alpha +(D*I*(\hat{L}\cdot \hat{N_{b}}))(1-\alpha)$$ Alpha is the interpolation value. $N_{a}$ and $N_{b}$ are two different normals at different vertices. The only reason this technique appears realistic is because the direction to the light does not change. Really, this way of calculating it is a shortcut, the proper way to do it is by just interpolating the normals instead of the color result. Like this. $$D*I*(\hat{L}\cdot (\hat{N_{a}}\alpha+\hat{N_{b}}(1-\alpha)))$$ However, this is a more expensive interpolation. At the page I linked in the second sentence of this post, there is a proof that the above equation is equal to the below equation. $$(D*I*(\hat{L}\cdot \hat{N_{a}}))\alpha +(D*I*(\hat{L}\cdot \hat{N_{b}}))(1-\alpha)$$ That is why goroud shading supposedly works. However, I have thought of a case where I think goroud shading does not work, meaning interpolating the resulting colors at the vertices of the face will not achieve the correct effect as interpolating the normals would. Consider this image. Pretend it is a triangular face. Pretend the direction to the light is given by the straight ""up"" vector at the center of that face as seen in the picture. Therefore, the face should be brightest at the center of the face. However, if you use goroud shading, notice it would not achieve this effect because the vertices of the face are supposed to be slightly darker than the middle. This is because the normals at the vertices are slightly pointed away from the direction towards the light. So interpolating dark to dark will obviously not achieve a brighter color in the middle. If I am correct in this, does this mean the proof that showed the interpolation of the normals is equivalent to the interpolation of the results is wrong, or is there something else going on that I am missing?",,"['linear-algebra', 'vector-spaces']"
48,Question on determinants of matrices changing between integer matrices [duplicate],Question on determinants of matrices changing between integer matrices [duplicate],,"This question already has an answer here : Index of a sublattice in a lattice and a homomorphism between them (1 answer) Closed 5 years ago . The following problem came up from a though I had while reading: Let's say we have $M=\mathbb{Z}^n$ and we have another free $\mathbb{Z}$-module, $N$, inside of $M$ also with rank $n$. We know we can make a matrix $A$ that changes $N$ to its representation in $M$ (ie has as columns the basis $N$ expressed in coordinates coming from the basis of $M$). If the index of $N$ in $M$ is $m$, I am pretty sure just from tinkering that the determinant of $A$ should also be $m$, however, I have not been able to show this. It is unclear to me how to proceed. I though maybe I could do something related to the points in $\mathbb{Z}^n$ that are within the unit box of $N$, but I cannot really make sense of this. Thank you for any help or direction.","This question already has an answer here : Index of a sublattice in a lattice and a homomorphism between them (1 answer) Closed 5 years ago . The following problem came up from a though I had while reading: Let's say we have $M=\mathbb{Z}^n$ and we have another free $\mathbb{Z}$-module, $N$, inside of $M$ also with rank $n$. We know we can make a matrix $A$ that changes $N$ to its representation in $M$ (ie has as columns the basis $N$ expressed in coordinates coming from the basis of $M$). If the index of $N$ in $M$ is $m$, I am pretty sure just from tinkering that the determinant of $A$ should also be $m$, however, I have not been able to show this. It is unclear to me how to proceed. I though maybe I could do something related to the points in $\mathbb{Z}^n$ that are within the unit box of $N$, but I cannot really make sense of this. Thank you for any help or direction.",,"['linear-algebra', 'abstract-algebra', 'determinant']"
49,Prove that there is no non-zero linear operator on $\mathbb C^{2}$ such that $(\alpha | T\alpha) = 0$,Prove that there is no non-zero linear operator on  such that,\mathbb C^{2} (\alpha | T\alpha) = 0,I'm stuck on this problem for a long time. I can't find a proper solution for this without using complex calculations. I hope some one can help me with this problem: Let $ (\;|\;)$ be the standard inner product on $\mathbb C^{2}$ . Prove that there is no non-zero linear operator $T$ on $\mathbb C^{2}$ such that $(\alpha|T\alpha) = 0\;$ for every $\alpha$ in $\mathbb C^{2}$ . Generalize.,I'm stuck on this problem for a long time. I can't find a proper solution for this without using complex calculations. I hope some one can help me with this problem: Let be the standard inner product on . Prove that there is no non-zero linear operator on such that for every in . Generalize., (\;|\;) \mathbb C^{2} T \mathbb C^{2} (\alpha|T\alpha) = 0\; \alpha \mathbb C^{2},"['linear-algebra', 'linear-transformations', 'inner-products']"
50,About the definition of n-tuple,About the definition of n-tuple,,"I've read from the theory of sets that the definition of ordered pair is foundational. To define formally the term of $n$-tuple I suppose we need to use the concept of ordered pair as well as the definition of recursion. So I was wondering about the difference between doing so and defining an $n$-tuple just like we do with matrices, I mean we can just say that an $n$-tuple is a sequence (a function with domain $\{ 1,2,3,4,...,n\}$). What is the advantage of making a definition by recursion? What is the difference between a sequence and an $n$-tuple? What is the difference between considering a vector as a matrix and considering a vector as $n$-tuple?","I've read from the theory of sets that the definition of ordered pair is foundational. To define formally the term of $n$-tuple I suppose we need to use the concept of ordered pair as well as the definition of recursion. So I was wondering about the difference between doing so and defining an $n$-tuple just like we do with matrices, I mean we can just say that an $n$-tuple is a sequence (a function with domain $\{ 1,2,3,4,...,n\}$). What is the advantage of making a definition by recursion? What is the difference between a sequence and an $n$-tuple? What is the difference between considering a vector as a matrix and considering a vector as $n$-tuple?",,"['linear-algebra', 'elementary-set-theory', 'definition']"
51,Finding a congruent matrix,Finding a congruent matrix,,I have the matrix $$A =\begin{pmatrix}0&1\\1&0\end{pmatrix}$$ How would I diagonalize it using elementary row operations? It's been a while since I've worked with them so I'm doubting myself when doing the same operations to both columns and rows. Steps would be much appreciated.,I have the matrix $$A =\begin{pmatrix}0&1\\1&0\end{pmatrix}$$ How would I diagonalize it using elementary row operations? It's been a while since I've worked with them so I'm doubting myself when doing the same operations to both columns and rows. Steps would be much appreciated.,,"['linear-algebra', 'matrices']"
52,What happens when the basis vectors of an integer lattice are not linearly independent?,What happens when the basis vectors of an integer lattice are not linearly independent?,,"The definition of a lattice requires basis vectors that are linearly independent. Why? For example, the following three vectors are linearly independent and form the basis of a lattice: \begin{array}{ccc} 0 & 0 & 1 \\ 0 & 2 & -2 \\ 1 & -2 & 1 \end{array} But what if we add a fourth vector such that they're not linearly independent anymore. For example: \begin{array}{ccc} 0 & 0 & 1 & 4\\ 0 & 2 & -2 & 2\\ 1 & -2 & 1 & 3\end{array} Are the four vectors the basis of a lattice? Why or why not? And, if it is, why does the definition require linear independence? Is there an equivalent basis that is linearly independent?","The definition of a lattice requires basis vectors that are linearly independent. Why? For example, the following three vectors are linearly independent and form the basis of a lattice: But what if we add a fourth vector such that they're not linearly independent anymore. For example: Are the four vectors the basis of a lattice? Why or why not? And, if it is, why does the definition require linear independence? Is there an equivalent basis that is linearly independent?","\begin{array}{ccc}
0 & 0 & 1 \\
0 & 2 & -2 \\
1 & -2 & 1 \end{array} \begin{array}{ccc}
0 & 0 & 1 & 4\\
0 & 2 & -2 & 2\\
1 & -2 & 1 & 3\end{array}","['linear-algebra', 'integer-lattices']"
53,Geometric interpretation of a vector space and subspace?,Geometric interpretation of a vector space and subspace?,,"I understand how to manipulate vector spaces and subspaces and how to prove various statements about them, but I still don't fully understand what they represent geometrically. I just need an intuitive grasp as to what these are. Is a vector space a geometric area that contains all possible vectors of the field $\Bbb F$? For example, if $V$ is a vector space over $\Bbb R^3$ then does that mean $V$ contains all vectors in three-dimensions that are part of $\Bbb R$? But then what is a subspace of $V$? Would that perhaps be a plane? Would it be a vector space in $\Bbb R^2$?","I understand how to manipulate vector spaces and subspaces and how to prove various statements about them, but I still don't fully understand what they represent geometrically. I just need an intuitive grasp as to what these are. Is a vector space a geometric area that contains all possible vectors of the field $\Bbb F$? For example, if $V$ is a vector space over $\Bbb R^3$ then does that mean $V$ contains all vectors in three-dimensions that are part of $\Bbb R$? But then what is a subspace of $V$? Would that perhaps be a plane? Would it be a vector space in $\Bbb R^2$?",,['linear-algebra']
54,How to find Matrix Inverse over finite field?,How to find Matrix Inverse over finite field?,,"How to find matrix Inverse over finite field? I am using MATLAB, and I know gf() in MATLAB can enable me to do linear algebra operations over finite field $F_{2^m}$ for some m. However, if I want to find the matrix inverse over finite field $F_p$ for some prime $p\neq 2$, how should I do in MATLAB and on the paper? For example, if $p=11$ and if matrix A is: $$\begin{bmatrix}2&1&2\\1&2&9\\1&2&7\end{bmatrix}$$ I need to derive its inverse $A^{-1}$ as: $$\begin{bmatrix}8&6&1\\7&9&10\\0&6&5\end{bmatrix}$$","How to find matrix Inverse over finite field? I am using MATLAB, and I know gf() in MATLAB can enable me to do linear algebra operations over finite field $F_{2^m}$ for some m. However, if I want to find the matrix inverse over finite field $F_p$ for some prime $p\neq 2$, how should I do in MATLAB and on the paper? For example, if $p=11$ and if matrix A is: $$\begin{bmatrix}2&1&2\\1&2&9\\1&2&7\end{bmatrix}$$ I need to derive its inverse $A^{-1}$ as: $$\begin{bmatrix}8&6&1\\7&9&10\\0&6&5\end{bmatrix}$$",,"['linear-algebra', 'finite-fields']"
55,Solution to underdetermined linear equations,Solution to underdetermined linear equations,,"I have a set of numbers $x_i$ and I know sums of certain subsets $y_i=\sum x_{\sigma_k}$. All $x_i>0$ and I'm looking for a simple solution. With some internet research I found that this might be related to problems in signal processing. So basically I have given a vector $\mathbf{y}$ and a matrix $\mathbf{A}$ with $y_i>0$ and $A_{ij}\in\{0,1\}$. I'm looking for a solution to the vector $\mathbf{x}$ ($x_i\geq 0$) with $\mathbf{A}\mathbf{x}=\mathbf{y}$ where this linear equation is underdetermined . Apparently to complete this problem several norms to minimize on $\mathbf{x}$ are possible. For my particular task it's not clear whether I need L0, L1 or L2 norm, so any solution will do - as long as it's simple. Approximate solution like iterative approaches are also fine. Can you suggest a way to solve this problem? I'm looking for a reference to an algorithm which I can understand as a non-mathematician. Even better would be an open source implementation that I can download. And it would be perfect if it were a Python solution.","I have a set of numbers $x_i$ and I know sums of certain subsets $y_i=\sum x_{\sigma_k}$. All $x_i>0$ and I'm looking for a simple solution. With some internet research I found that this might be related to problems in signal processing. So basically I have given a vector $\mathbf{y}$ and a matrix $\mathbf{A}$ with $y_i>0$ and $A_{ij}\in\{0,1\}$. I'm looking for a solution to the vector $\mathbf{x}$ ($x_i\geq 0$) with $\mathbf{A}\mathbf{x}=\mathbf{y}$ where this linear equation is underdetermined . Apparently to complete this problem several norms to minimize on $\mathbf{x}$ are possible. For my particular task it's not clear whether I need L0, L1 or L2 norm, so any solution will do - as long as it's simple. Approximate solution like iterative approaches are also fine. Can you suggest a way to solve this problem? I'm looking for a reference to an algorithm which I can understand as a non-mathematician. Even better would be an open source implementation that I can download. And it would be perfect if it were a Python solution.",,['linear-algebra']
56,"$\mathbb{R}$ isn't group with operation $\circ$ but $\mathbb{R}\setminus \{p\}$ is, find $p$","isn't group with operation  but  is, find",\mathbb{R} \circ \mathbb{R}\setminus \{p\} p,"I have exercise for exam and I can't find solution for it Set of real numbers with operation $\circ$ is not a group but set   $\mathbb{R}\setminus \{p\}$ is. Find $p$ and check all axioms for group. $$ a \circ b = a + b - \frac{5ab}{3} $$ I tried to find solution by myself, and I begin with finding identity element: $$ a \circ e  = a \\ a+e - \frac{5ae}{3} = a \\ e - \frac{5ae}{3} = 0 \\ 3e = 5ae\\ a = \frac{3}{5} $$ But this is value for what $(a = \frac{3}{5} \lor b = \frac{3}{5}) \Rightarrow a \circ b = \frac{3}{5}$, so it isn't identity element (it will be one if and only if $a = b = \frac{3}{5}$).","I have exercise for exam and I can't find solution for it Set of real numbers with operation $\circ$ is not a group but set   $\mathbb{R}\setminus \{p\}$ is. Find $p$ and check all axioms for group. $$ a \circ b = a + b - \frac{5ab}{3} $$ I tried to find solution by myself, and I begin with finding identity element: $$ a \circ e  = a \\ a+e - \frac{5ae}{3} = a \\ e - \frac{5ae}{3} = 0 \\ 3e = 5ae\\ a = \frac{3}{5} $$ But this is value for what $(a = \frac{3}{5} \lor b = \frac{3}{5}) \Rightarrow a \circ b = \frac{3}{5}$, so it isn't identity element (it will be one if and only if $a = b = \frac{3}{5}$).",,"['linear-algebra', 'group-theory']"
57,Why is intersection of kernels of finite linear functional a nontrivial linear subspace?,Why is intersection of kernels of finite linear functional a nontrivial linear subspace?,,"I am trying to prove that the closure of $S=\{x\in X : ||x||=1\}$ in weak topology is the closure of $B_1(0)$ .  I have a doubt about what i am doing is correct and for that i need to know whether the intersection of kernels of finite linear functionals is non-trivial . I think here comes the main point about $X$ being infinite dimensional . And it looks like the closure seems to be the whole space , the neighbourhood of a point $x_0$ contains the lines passing through $x_0$, so i am finding it a bit tricky to get an idea about the closure .","I am trying to prove that the closure of $S=\{x\in X : ||x||=1\}$ in weak topology is the closure of $B_1(0)$ .  I have a doubt about what i am doing is correct and for that i need to know whether the intersection of kernels of finite linear functionals is non-trivial . I think here comes the main point about $X$ being infinite dimensional . And it looks like the closure seems to be the whole space , the neighbourhood of a point $x_0$ contains the lines passing through $x_0$, so i am finding it a bit tricky to get an idea about the closure .",,"['linear-algebra', 'functional-analysis']"
58,How to check that whather a Polygon is completly inside of another Polygon?,How to check that whather a Polygon is completly inside of another Polygon?,,"Let's say I have two polygons. I know the co-ordinates of both polygons. Now, I need to check whether the first Polygon is completely inside of second polygon? IN this figure only 1 polygon is completely inside of red polygon.","Let's say I have two polygons. I know the co-ordinates of both polygons. Now, I need to check whether the first Polygon is completely inside of second polygon? IN this figure only 1 polygon is completely inside of red polygon.",,"['linear-algebra', 'geometry']"
59,Necessary and sufficient condition for the matrix $A = I - 2 x x^t$ to be orthogonal,Necessary and sufficient condition for the matrix  to be orthogonal,A = I - 2 x x^t,Let $x$ be a non zeo (column) vector in $\mathbb{R}^n$ . What is the necessary and sufficient condition for the matrix $A = I-2xx^t$ to be orthogonal?,Let be a non zeo (column) vector in . What is the necessary and sufficient condition for the matrix to be orthogonal?,x \mathbb{R}^n A = I-2xx^t,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'orthogonal-matrices']"
60,$A$ be a $10\times 10$ matrix in which each row has exactly one entry equal to 1. find the possible value of the determinant,be a  matrix in which each row has exactly one entry equal to 1. find the possible value of the determinant,A 10\times 10,"Let $A$ be a $10\times 10$ matrix in which each row has exactly one entry equal to $1$. And remaining nine entries of the row being $0$. Which of the following is not a possible value of the determinant? $0, 1 ,-1, 10$. I am able to identify for $2\times 2$ cross two matrices for which possible value of determinant is $1$ or $-1$. How to identify for such a big size matrix? Can we identify such matrix?","Let $A$ be a $10\times 10$ matrix in which each row has exactly one entry equal to $1$. And remaining nine entries of the row being $0$. Which of the following is not a possible value of the determinant? $0, 1 ,-1, 10$. I am able to identify for $2\times 2$ cross two matrices for which possible value of determinant is $1$ or $-1$. How to identify for such a big size matrix? Can we identify such matrix?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
61,Finding the dimension of real symmetric matrices with trace zero,Finding the dimension of real symmetric matrices with trace zero,,What is the dimension of the vector space of all symmetric matrices of order $n\times n$ $(n\geq 2)$ with real entries and trace equal to zero?,What is the dimension of the vector space of all symmetric matrices of order $n\times n$ $(n\geq 2)$ with real entries and trace equal to zero?,,"['linear-algebra', 'matrices']"
62,Unitary and Upper Triangular Matrix,Unitary and Upper Triangular Matrix,,"I am trying to prove that a matrix that is both ""unitary and upper triangular"" must be a diagonal matrix. I am thinking that the fact that columns of all unitary matrices form an orthonormal basis of F^n will ensure that all columns of this matrix are mutually orthogonal. Also, the fact that the matrix is upper triangular will then give me the required result....but I am trying to derive a formal proof. Please suggest if my approach is OK. Thanks in advance.","I am trying to prove that a matrix that is both ""unitary and upper triangular"" must be a diagonal matrix. I am thinking that the fact that columns of all unitary matrices form an orthonormal basis of F^n will ensure that all columns of this matrix are mutually orthogonal. Also, the fact that the matrix is upper triangular will then give me the required result....but I am trying to derive a formal proof. Please suggest if my approach is OK. Thanks in advance.",,['linear-algebra']
63,Why is a linear equation with three variables a plane?,Why is a linear equation with three variables a plane?,,"In linear algebra, why is the graph of a three variable equation of the form $ax+by+cz+d=0$ a plane? With two variables, it is easy to convince oneself that the graph is a line (using similar triangles, for example). However with three variables, this same technique does not seem to work: setting one of the variables to be a constant yields a line in 3-D space (I think), and one could repeat this process for each constant value of that variable, but in the end there seems not to be an easy way to check that all these lines are coplanar. I don't remember seeing why this is in a book, and Khan Academy's video , for example, simply states that this is the case.","In linear algebra, why is the graph of a three variable equation of the form $ax+by+cz+d=0$ a plane? With two variables, it is easy to convince oneself that the graph is a line (using similar triangles, for example). However with three variables, this same technique does not seem to work: setting one of the variables to be a constant yields a line in 3-D space (I think), and one could repeat this process for each constant value of that variable, but in the end there seems not to be an easy way to check that all these lines are coplanar. I don't remember seeing why this is in a book, and Khan Academy's video , for example, simply states that this is the case.",,['linear-algebra']
64,Best books on A Second Course in Linear Algebra [duplicate],Best books on A Second Course in Linear Algebra [duplicate],,"This question already has answers here : Closed 12 years ago . Possible Duplicate: Prerequisites/Books for Linear Algebra I've studied from David Poole's Linear Algebra: A Modern Introduction However, it's not very complete. I want to study subjects as Bilinear Forms, Transformations. And also, I want to study it more deeply. Do you know of any book on Linear Algebra that studies the subject deeply but also explains it clearly? I prefer the practical approach over the theoretic one. Thank you a lot.","This question already has answers here : Closed 12 years ago . Possible Duplicate: Prerequisites/Books for Linear Algebra I've studied from David Poole's Linear Algebra: A Modern Introduction However, it's not very complete. I want to study subjects as Bilinear Forms, Transformations. And also, I want to study it more deeply. Do you know of any book on Linear Algebra that studies the subject deeply but also explains it clearly? I prefer the practical approach over the theoretic one. Thank you a lot.",,"['linear-algebra', 'reference-request', 'big-list']"
65,Does an $n\times n$ adjacency matrix of a scale-free network graph have $n$ distinct eigenvalues?,Does an  adjacency matrix of a scale-free network graph have  distinct eigenvalues?,n\times n n,"Question updated Suppose that I have an $n\times n$ adjacency matrix $\mathbf{A}$ of a simple graph $G$ where the entry $(i,j)$ represent the number of edges between node $i$ and $j$ in $G$. Note that a simple graph is a unweighted, undirected graph containing no self-loops or multiple edges. All these datasets are scale-free networks whose degree distribution follows a power law , at least asymptotically; that is, the fraction $P(k)$ of nodes in the network having $k$ connections to other nodes goes for large values of $k$ as $$P(k) \sim ck^{-\gamma}$$ where $c$ is a normalization constant and γ is a parameter whose value is typically in the range $2 < \gamma < 3$, although occasionally it may lie outside these bounds. (This definition is from the corresponding article from Wikipedia.) I'm interested in applying spectral techniques to analyze real-life large graph datasets. For example, these datasets are available at Stanford Large Network Dataset Collection . These datasets may be collaboration networks (who collaborated with whom), protein-protein interaction networks, or friend relations on a social network such as Facebook or Twitter. When analyzing a certain property of these scale-free networks, my approach would be much simpler if the adjacency matrix representation of graphs has all distinct eigenvalues. My question is, can I assume that all these matrices have distinct eigenvalues? Very few is known about the spectrum of these matrices, but it is conjectured that the eigenvalue distribution also follows a power law for the highest eigenvalues. Reference Eigenvalue distribution in scale free graphs S. Gago, Aplimat-Journal of Applied Mathematics , 2008","Question updated Suppose that I have an $n\times n$ adjacency matrix $\mathbf{A}$ of a simple graph $G$ where the entry $(i,j)$ represent the number of edges between node $i$ and $j$ in $G$. Note that a simple graph is a unweighted, undirected graph containing no self-loops or multiple edges. All these datasets are scale-free networks whose degree distribution follows a power law , at least asymptotically; that is, the fraction $P(k)$ of nodes in the network having $k$ connections to other nodes goes for large values of $k$ as $$P(k) \sim ck^{-\gamma}$$ where $c$ is a normalization constant and γ is a parameter whose value is typically in the range $2 < \gamma < 3$, although occasionally it may lie outside these bounds. (This definition is from the corresponding article from Wikipedia.) I'm interested in applying spectral techniques to analyze real-life large graph datasets. For example, these datasets are available at Stanford Large Network Dataset Collection . These datasets may be collaboration networks (who collaborated with whom), protein-protein interaction networks, or friend relations on a social network such as Facebook or Twitter. When analyzing a certain property of these scale-free networks, my approach would be much simpler if the adjacency matrix representation of graphs has all distinct eigenvalues. My question is, can I assume that all these matrices have distinct eigenvalues? Very few is known about the spectrum of these matrices, but it is conjectured that the eigenvalue distribution also follows a power law for the highest eigenvalues. Reference Eigenvalue distribution in scale free graphs S. Gago, Aplimat-Journal of Applied Mathematics , 2008",,"['linear-algebra', 'graph-theory', 'random-graphs', 'spectral-graph-theory']"
66,Ax=b problem solving,Ax=b problem solving,,"Can you please tell how to solve this: Ax=b $$A=\begin{pmatrix}  2 & 4   & -8 &  4 & 1\\  4 & 10  &-16 &  8 &-8\\ -4 & -12 & 17 & -8 & 20\\ -2 & -10 & 10 & -3 & 34  \end{pmatrix}$$ $$b=\begin{pmatrix}-33\\-36\\4\\-62\end{pmatrix}$$ I have to give a fundamental and particular solutions. I have tried so: But I do not know whether it is right and what to do next ... Please, help me! Thanks.","Can you please tell how to solve this: Ax=b $$A=\begin{pmatrix}  2 & 4   & -8 &  4 & 1\\  4 & 10  &-16 &  8 &-8\\ -4 & -12 & 17 & -8 & 20\\ -2 & -10 & 10 & -3 & 34  \end{pmatrix}$$ $$b=\begin{pmatrix}-33\\-36\\4\\-62\end{pmatrix}$$ I have to give a fundamental and particular solutions. I have tried so: But I do not know whether it is right and what to do next ... Please, help me! Thanks.",,"['linear-algebra', 'matrices']"
67,"How do I get this matrix in Smith Normal Form? And, is Smith Normal Form unique?","How do I get this matrix in Smith Normal Form? And, is Smith Normal Form unique?",,"As part of a larger problem, I want to compute the Smith Normal Form of $xI-B$ over $\mathbb{Q}[x]$ where  $$ B=\begin{pmatrix} 5 & 2 & -8 & -8 \\ -6 & -3 & 8 & 8 \\ -3 & -1 & 3 & 4 \\ 3 & 1 & -4 & -5\end{pmatrix}. $$ So I do some elementary row and column operations and get to $$\begin{pmatrix} 1+x & -2 & 0 & 0 \\ -3(x+1) & x+3 & 0 & 0 \\ 0 & 1 & x+1 & 0 \\ 0 & 0 & 0 & x+1\end{pmatrix}. $$ Then I work with the upper left 3x3 matrix, and ultimately get: $$\begin{pmatrix} x-3 & 0 & 0 & 0 \\ 0 & x+1 & 0 & 0 \\ 0 & 0 & x+1 & 0 \\ 0 & 0 & 0 & x+1\end{pmatrix}. $$ So now I have a diagonal matrix (and I'm pretty sure I didn't mess anything up in performing row and column operations), except according to http://mathworld.wolfram.com/SmithNormalForm.html , the diagonal entries are supposed to divide each other, but obviously x-3 does not divide x+1. This means that: either I did something wrong, or diagonal matrix is not unique. Any ideas for how to transform my final matrix into a matrix whose diagonal entries divide each other?","As part of a larger problem, I want to compute the Smith Normal Form of $xI-B$ over $\mathbb{Q}[x]$ where  $$ B=\begin{pmatrix} 5 & 2 & -8 & -8 \\ -6 & -3 & 8 & 8 \\ -3 & -1 & 3 & 4 \\ 3 & 1 & -4 & -5\end{pmatrix}. $$ So I do some elementary row and column operations and get to $$\begin{pmatrix} 1+x & -2 & 0 & 0 \\ -3(x+1) & x+3 & 0 & 0 \\ 0 & 1 & x+1 & 0 \\ 0 & 0 & 0 & x+1\end{pmatrix}. $$ Then I work with the upper left 3x3 matrix, and ultimately get: $$\begin{pmatrix} x-3 & 0 & 0 & 0 \\ 0 & x+1 & 0 & 0 \\ 0 & 0 & x+1 & 0 \\ 0 & 0 & 0 & x+1\end{pmatrix}. $$ So now I have a diagonal matrix (and I'm pretty sure I didn't mess anything up in performing row and column operations), except according to http://mathworld.wolfram.com/SmithNormalForm.html , the diagonal entries are supposed to divide each other, but obviously x-3 does not divide x+1. This means that: either I did something wrong, or diagonal matrix is not unique. Any ideas for how to transform my final matrix into a matrix whose diagonal entries divide each other?",,"['linear-algebra', 'principal-ideal-domains', 'smith-normal-form']"
68,Question about normed vector spaces and quotients,Question about normed vector spaces and quotients,,"If you have a norm defined on a vector space $V$ you can define the norm for the quotient by a subspace $W$: $$ || v + W|| := \inf_{w \in W} || v + w || $$ My question is: why does $W$ have to be a closed subspace? In $\mathbb{R}^n$, as it happens, any subspace is closed, i.e. if something is a subspace then it's closed (e.g. lines or planes in $R^3$). Is this true for any vector space? I find it difficult to visualise subspaces of e.g. $L^1$. Many thanks for your help!! (as always ; ) )","If you have a norm defined on a vector space $V$ you can define the norm for the quotient by a subspace $W$: $$ || v + W|| := \inf_{w \in W} || v + w || $$ My question is: why does $W$ have to be a closed subspace? In $\mathbb{R}^n$, as it happens, any subspace is closed, i.e. if something is a subspace then it's closed (e.g. lines or planes in $R^3$). Is this true for any vector space? I find it difficult to visualise subspaces of e.g. $L^1$. Many thanks for your help!! (as always ; ) )",,"['linear-algebra', 'functional-analysis']"
69,notation for real matrices,notation for real matrices,,"Is this a valid notation for real $m \times n$ matrices: $\mathbb{R}^{m,n}$. $m$ and $n$ are known. If it is not, then what would be the right notation for the set of such matrices?","Is this a valid notation for real $m \times n$ matrices: $\mathbb{R}^{m,n}$. $m$ and $n$ are known. If it is not, then what would be the right notation for the set of such matrices?",,"['linear-algebra', 'matrices', 'notation']"
70,Determining the Jordan form of a matrix given the invariant factors,Determining the Jordan form of a matrix given the invariant factors,,"I am trying to recover the Jordan normal form of a matrix given a list of invariant factors and was wondering if I am proceeding correctly in constructing the Jordan blocks. Let $F = \mathbb{C}$ and let $V$ be a finite dimensional vector space over $F$.  Let $T:V\to V$ be a linear operator and give $V$ the structure of a module over the polynomial ring $F[x]$ by defining $x \alpha = T(\alpha) \alpha \in V$ let $$ A = \left( \begin{array}{ccc} x^2(x-1)^2 & 0 & 0 \\ 0 & x(x-1)(x-2)^2 & 0 \\ 0 & 0 &  x(x-2)^3 \end{array} \right)   $$ be a relation matrix for V with respect to $\{v_1, v_2, v_3\}$ generators of $V$. Then $d_1 = x$, $d_2 = x(x-1)(x-2)^2$ and $d_3 = x^2(x-1)^2(x-2)^3$ are the invariant factors of $T$. Then we know $ V = F[x] / (x) \oplus F[x]/ (x(x-1)(x-2)^2) \oplus F[x]/(x^2(x-1)^2(x-3)^3)$.  Further we know that the minimal polynomial of $T$ is the largest of the invariant factors so that $m_T(x) = (x^2(x-1)^2(x-2)^3)$ and the characteristic polynomial will be the product of $d_1 d_2 d_3$. Question: what is the appropriate Jordan normal form of T? Since 0, 1 and are repeated roots and 2 is repeated 3 times. Does that give me Jordan blocks $$ J_1 =  \begin{pmatrix}0 & 1 \\0 & 0 \end{pmatrix}$$ $$ J_2 = \begin{pmatrix}1 & 1 \\0 & 1 \end{pmatrix}$$ and  $$ J_3 = \begin{pmatrix}2 & 1 \\0 & 2 \end{pmatrix}$$","I am trying to recover the Jordan normal form of a matrix given a list of invariant factors and was wondering if I am proceeding correctly in constructing the Jordan blocks. Let $F = \mathbb{C}$ and let $V$ be a finite dimensional vector space over $F$.  Let $T:V\to V$ be a linear operator and give $V$ the structure of a module over the polynomial ring $F[x]$ by defining $x \alpha = T(\alpha) \alpha \in V$ let $$ A = \left( \begin{array}{ccc} x^2(x-1)^2 & 0 & 0 \\ 0 & x(x-1)(x-2)^2 & 0 \\ 0 & 0 &  x(x-2)^3 \end{array} \right)   $$ be a relation matrix for V with respect to $\{v_1, v_2, v_3\}$ generators of $V$. Then $d_1 = x$, $d_2 = x(x-1)(x-2)^2$ and $d_3 = x^2(x-1)^2(x-2)^3$ are the invariant factors of $T$. Then we know $ V = F[x] / (x) \oplus F[x]/ (x(x-1)(x-2)^2) \oplus F[x]/(x^2(x-1)^2(x-3)^3)$.  Further we know that the minimal polynomial of $T$ is the largest of the invariant factors so that $m_T(x) = (x^2(x-1)^2(x-2)^3)$ and the characteristic polynomial will be the product of $d_1 d_2 d_3$. Question: what is the appropriate Jordan normal form of T? Since 0, 1 and are repeated roots and 2 is repeated 3 times. Does that give me Jordan blocks $$ J_1 =  \begin{pmatrix}0 & 1 \\0 & 0 \end{pmatrix}$$ $$ J_2 = \begin{pmatrix}1 & 1 \\0 & 1 \end{pmatrix}$$ and  $$ J_3 = \begin{pmatrix}2 & 1 \\0 & 2 \end{pmatrix}$$",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
71,Probability Calculations on Highway,Probability Calculations on Highway,,"I read this Google Interview Question . Q :If the probability of observing a car in 30 minutes on a highway is 0.95, what is the probability of observing a car in 10 minutes (assuming constant default probability)? A :The trick here is that .95 is the probability for 1 or more cars, not the probability of seeing just one car. The prob. of NO cars in 30 minutes is 0.05, so the prob of no cars in 10 minutes is the cube root of that, so the prob of seeing a car in 10 minutes is one minus that , or ~63% My question is, if The probability of NO cars in 30 minutes is 0.05 , why the probability of no cars in 10 minutes is the cube root of that ?? Which algorithm used in this question?","I read this Google Interview Question . Q :If the probability of observing a car in 30 minutes on a highway is 0.95, what is the probability of observing a car in 10 minutes (assuming constant default probability)? A :The trick here is that .95 is the probability for 1 or more cars, not the probability of seeing just one car. The prob. of NO cars in 30 minutes is 0.05, so the prob of no cars in 10 minutes is the cube root of that, so the prob of seeing a car in 10 minutes is one minus that , or ~63% My question is, if The probability of NO cars in 30 minutes is 0.05 , why the probability of no cars in 10 minutes is the cube root of that ?? Which algorithm used in this question?",,"['linear-algebra', 'probability', 'algorithms']"
72,Proof of Sylvester's Law of Inertia,Proof of Sylvester's Law of Inertia,,"I'm getting confused about the proof in my notes of Sylvester's Law of Inertia, and positive definiteness in general. I'll try to state my problems explicitly: The theorem states that if $ \psi : V\times V \to \mathbb{R} $ is a real symmetric bilinear form, represented by  $$  \left(   \begin{array}{c c c}     I_p  \\      & -I_q &  \\      & & 0    \end{array} \right) \;\;\text{and}\;\;  \left(   \begin{array}{c c c}     I_p'  \\      & -I_q' &  \\      & & 0    \end{array} \right) $$ then $ p = p' $ and $ q = q' $. The proof I have shows that $ p $ is the largest possible dimension of a positive definite subspace, and so $ p = p' $. Why is this implication true? It then goes on to say that ""similarly, $ q = q'$"". What exactly does this mean? Wouldn't it suffice, after having shown uniqueness of $ p$, to say that the rank($ = p + q $) of a matrix is well-defined, and so it follows $ q $ is well-defined? What does it actually mean for a real symmetric bilinear form to be positive definite? The definition of positive-definitness in my notes is in terms of subspaces. Does it mean that the entire space is positive definite (w.r.t that particular form)? If so, how does this relate to matrices representing the form? How do we get from this definition to the one I'm finding online, which states that $A$ is positive definite iff $ x^T A x > 0 $ for all x in the space? Thanks. I know that I'm asking a lot, and will greatly appreciate any help.","I'm getting confused about the proof in my notes of Sylvester's Law of Inertia, and positive definiteness in general. I'll try to state my problems explicitly: The theorem states that if $ \psi : V\times V \to \mathbb{R} $ is a real symmetric bilinear form, represented by  $$  \left(   \begin{array}{c c c}     I_p  \\      & -I_q &  \\      & & 0    \end{array} \right) \;\;\text{and}\;\;  \left(   \begin{array}{c c c}     I_p'  \\      & -I_q' &  \\      & & 0    \end{array} \right) $$ then $ p = p' $ and $ q = q' $. The proof I have shows that $ p $ is the largest possible dimension of a positive definite subspace, and so $ p = p' $. Why is this implication true? It then goes on to say that ""similarly, $ q = q'$"". What exactly does this mean? Wouldn't it suffice, after having shown uniqueness of $ p$, to say that the rank($ = p + q $) of a matrix is well-defined, and so it follows $ q $ is well-defined? What does it actually mean for a real symmetric bilinear form to be positive definite? The definition of positive-definitness in my notes is in terms of subspaces. Does it mean that the entire space is positive definite (w.r.t that particular form)? If so, how does this relate to matrices representing the form? How do we get from this definition to the one I'm finding online, which states that $A$ is positive definite iff $ x^T A x > 0 $ for all x in the space? Thanks. I know that I'm asking a lot, and will greatly appreciate any help.",,['linear-algebra']
73,Sign of det(UV) in SVD,Sign of det(UV) in SVD,,"Let $A\in \mathbb{R}^{n\times n}$. Its Singular Value Decomposition (SVD) is $$A=U\Sigma V^T$$ We know $U$ and $V$ are orthogonal matrices. Sometimes $\det UV=1$ and sometimes $\det UV=-1$. My question is : what kind of matrices give $\det UV=1$? Can we say something about the sign of $\det UV$ based on some properties of $A$ before we do SVD? Thanks. Edit : A trivial example is: if $A$ is a rotation matrix, i.e., $AA^T=I$ and $\det A=1$, then of course any SVD of $A$ gives $\det UV=1$. Now I am focusing on $A=R+ab^T$ with $R$ as a rotation matrix and $a,b\in \mathbb{R}^n$. Can we say $det UV=1$ for $A$ all the time?","Let $A\in \mathbb{R}^{n\times n}$. Its Singular Value Decomposition (SVD) is $$A=U\Sigma V^T$$ We know $U$ and $V$ are orthogonal matrices. Sometimes $\det UV=1$ and sometimes $\det UV=-1$. My question is : what kind of matrices give $\det UV=1$? Can we say something about the sign of $\det UV$ based on some properties of $A$ before we do SVD? Thanks. Edit : A trivial example is: if $A$ is a rotation matrix, i.e., $AA^T=I$ and $\det A=1$, then of course any SVD of $A$ gives $\det UV=1$. Now I am focusing on $A=R+ab^T$ with $R$ as a rotation matrix and $a,b\in \mathbb{R}^n$. Can we say $det UV=1$ for $A$ all the time?",,"['linear-algebra', 'matrices']"
74,Finding a basis for a subspace,Finding a basis for a subspace,,Find a basis for the subspace of $\mathbb{R}^4$ consisting of all vectors that satisfy $x_1 + 2x_2 - x_3 = 0$ My general idea so far is: $x_1 = -2x_2 +x_3 + 0x_4$ $x_2 = $ free $x_3 = $ free $x_4 = $ free Where should I go from here? Am I even going about it correctly?,Find a basis for the subspace of $\mathbb{R}^4$ consisting of all vectors that satisfy $x_1 + 2x_2 - x_3 = 0$ My general idea so far is: $x_1 = -2x_2 +x_3 + 0x_4$ $x_2 = $ free $x_3 = $ free $x_4 = $ free Where should I go from here? Am I even going about it correctly?,,['linear-algebra']
75,Why does the Gram-Schmidt procedure divide by 0 on a linearly dependent lists of vectors?,Why does the Gram-Schmidt procedure divide by 0 on a linearly dependent lists of vectors?,,"Let $v_1, \dots, v_m$ be a linearly dependent list of vectors. If $v_1 \ne 0$, then there is some $v_j$ in the span of $v_1, \dots, v_{j-1}$ If we let j be the smallest integer with this property, and apply the gram-schmidt procedure to produce an orthonormal list $(e_1, \dots, e_{j-1})$ then $v_j$ is in the span of $(e_1, \dots, e_{j-1})$ and $$v_j = \langle v_j, e_1\rangle e_1+ \dots + \langle v_j, e_{j-1}\rangle e_{j-1}$$ Why does this guarantee that length of $v_j$=0? I'm missing something about linear dependence that should probably be obvious sorry :\","Let $v_1, \dots, v_m$ be a linearly dependent list of vectors. If $v_1 \ne 0$, then there is some $v_j$ in the span of $v_1, \dots, v_{j-1}$ If we let j be the smallest integer with this property, and apply the gram-schmidt procedure to produce an orthonormal list $(e_1, \dots, e_{j-1})$ then $v_j$ is in the span of $(e_1, \dots, e_{j-1})$ and $$v_j = \langle v_j, e_1\rangle e_1+ \dots + \langle v_j, e_{j-1}\rangle e_{j-1}$$ Why does this guarantee that length of $v_j$=0? I'm missing something about linear dependence that should probably be obvious sorry :\",,"['linear-algebra', 'inner-products']"
76,Why does $L \cap (M + N) = (L \cap M) + (L \cap N) $ not hold for subspaces,Why does  not hold for subspaces,L \cap (M + N) = (L \cap M) + (L \cap N) ,"Let $L$, $M$, and $N$ are subspaces of a vector space. Prove that following is not necessarily true. $L \cap (M + N) = (L \cap M) + (L \cap N) $ This problem is given in 'Finite dimensional vector spaces' by Halmos. I was using 'if a vector belongs to L.H.S. then it must belong to R.H.S and vice versa' argument. Neither I can disprove it using this argument nor I could find a case where this is wrong!","Let $L$, $M$, and $N$ are subspaces of a vector space. Prove that following is not necessarily true. $L \cap (M + N) = (L \cap M) + (L \cap N) $ This problem is given in 'Finite dimensional vector spaces' by Halmos. I was using 'if a vector belongs to L.H.S. then it must belong to R.H.S and vice versa' argument. Neither I can disprove it using this argument nor I could find a case where this is wrong!",,"['linear-algebra', 'vector-spaces']"
77,Explain why a set of mutually orthogonal non-zero vectors is linearly independent given a clause,Explain why a set of mutually orthogonal non-zero vectors is linearly independent given a clause,,"""Given $\vec{u}_1,\ldots ,\vec{u}_n$ mutually orthogonal non-zero vectors, explain why for $\vec{v}=c_1\vec{u}_1+\ldots +c_n\vec{u}_n$ $c_k=\frac{\vec{v} \cdot \vec{u}_k}{\vec{u}_k \cdot \vec{u}_k}$"" This I explained by dotting both sides with $\vec{u}_k$ and simplifying everything. However, the question I now have is, how using this achieved result, can I show that $\vec{u}_1,\ldots ,\vec{u}_n$ are linearly independent? I was thinking of saying that in accordance to $c_k=\frac{\vec{v} \cdot \vec{u}_k}{\vec{u}_k \cdot \vec{u}_k}$, every coefficient can be of only one fixed value, so there is no room for changing one at the expense of another (as one could with coefficients of linearly dependent vectors), but I am not sure if this is right, and whether I am phrasing this correctly. Thanks for your help!","""Given $\vec{u}_1,\ldots ,\vec{u}_n$ mutually orthogonal non-zero vectors, explain why for $\vec{v}=c_1\vec{u}_1+\ldots +c_n\vec{u}_n$ $c_k=\frac{\vec{v} \cdot \vec{u}_k}{\vec{u}_k \cdot \vec{u}_k}$"" This I explained by dotting both sides with $\vec{u}_k$ and simplifying everything. However, the question I now have is, how using this achieved result, can I show that $\vec{u}_1,\ldots ,\vec{u}_n$ are linearly independent? I was thinking of saying that in accordance to $c_k=\frac{\vec{v} \cdot \vec{u}_k}{\vec{u}_k \cdot \vec{u}_k}$, every coefficient can be of only one fixed value, so there is no room for changing one at the expense of another (as one could with coefficients of linearly dependent vectors), but I am not sure if this is right, and whether I am phrasing this correctly. Thanks for your help!",,['linear-algebra']
78,Is there a matrix with rational entries similar to a given matrix?,Is there a matrix with rational entries similar to a given matrix?,,"I am working on the following problem. Is there a matrix $A$ with rational entries similar to the matrix $\begin{pmatrix} \sqrt{2} & 0\\0 & \sqrt{2}\end{pmatrix}$ ? What about $\begin{pmatrix} \sqrt{2} & 0\\0 & -\sqrt{2}\end{pmatrix}$ ? Now, in the first case, there is no such matrix because its trace would be $2 \sqrt{2}$ . But in the second case, all I can answer is ""maybe"". The trace is zero, so I can't rule it out. I know the eigenvalues will be $\pm \sqrt{2}$ , but this doesn't tell me anything. Is there a systematic way to solve this? Can I ""undiagonalize"" a matrix in such a way that I can tell directly whether this matrix $A$ exists or not? Any help is appreciated.","I am working on the following problem. Is there a matrix with rational entries similar to the matrix ? What about ? Now, in the first case, there is no such matrix because its trace would be . But in the second case, all I can answer is ""maybe"". The trace is zero, so I can't rule it out. I know the eigenvalues will be , but this doesn't tell me anything. Is there a systematic way to solve this? Can I ""undiagonalize"" a matrix in such a way that I can tell directly whether this matrix exists or not? Any help is appreciated.",A \begin{pmatrix} \sqrt{2} & 0\\0 & \sqrt{2}\end{pmatrix} \begin{pmatrix} \sqrt{2} & 0\\0 & -\sqrt{2}\end{pmatrix} 2 \sqrt{2} \pm \sqrt{2} A,"['linear-algebra', 'matrices', 'similar-matrices']"
79,Derivatives of the determinant of a singular matrix w.r.t the matrix,Derivatives of the determinant of a singular matrix w.r.t the matrix,,"I have a $3 \times 3$ matrix $\bf{A}$ , and want to find the second order derivative of its determinant w.r.t the matrix itself $\frac{\partial ^2 \det({\bf{A}})}{\partial {\bf{A}} ^ 2}$ . Everything I can find online has assumed ${\bf{A}}$ to be invertible. In my case, $\bf{A}$ can be singular. I started from the Jacobian formula \begin{equation} {\bf{A}} \cdot \frac{\partial \det({\bf{A}})}{\partial {\bf{A}}} = \det(A) {\bf{I}} \end{equation} Differentiate both sides w.r.t ${\bf{A}}$ , I got \begin{equation} {\bf 1} \cdot \frac{\partial \det({\bf{A}})}{\partial {\bf{A}}} + {\bf A} \cdot \frac{\partial ^2 \det({\bf{A}})}{\partial {\bf{A}} ^ 2} = \frac{\partial \det({\bf{A}})}{\partial {\bf{A}}} \cdot {\bf I} \end{equation} where I use $\bf 1$ to denote the rank 4 identity tensor. However, when $\bf A$ is singular, this equation is also not helpful since I cannot solve $\frac{\partial ^2 \det({\bf{A}})}{\partial {\bf{A}} ^ 2}$ from it. So, for a general $3 \times 3$ matrix $\bf A$ which is not necessarily invertible, do we have a closed form solution for $\frac{\partial ^2 \det({\bf{A}})}{\partial {\bf{A}} ^ 2}$ ?","I have a matrix , and want to find the second order derivative of its determinant w.r.t the matrix itself . Everything I can find online has assumed to be invertible. In my case, can be singular. I started from the Jacobian formula Differentiate both sides w.r.t , I got where I use to denote the rank 4 identity tensor. However, when is singular, this equation is also not helpful since I cannot solve from it. So, for a general matrix which is not necessarily invertible, do we have a closed form solution for ?","3 \times 3 \bf{A} \frac{\partial ^2 \det({\bf{A}})}{\partial {\bf{A}} ^ 2} {\bf{A}} \bf{A} \begin{equation}
{\bf{A}} \cdot \frac{\partial \det({\bf{A}})}{\partial {\bf{A}}} = \det(A) {\bf{I}}
\end{equation} {\bf{A}} \begin{equation}
{\bf 1} \cdot \frac{\partial \det({\bf{A}})}{\partial {\bf{A}}} + {\bf A} \cdot \frac{\partial ^2 \det({\bf{A}})}{\partial {\bf{A}} ^ 2} = \frac{\partial \det({\bf{A}})}{\partial {\bf{A}}} \cdot {\bf I}
\end{equation} \bf 1 \bf A \frac{\partial ^2 \det({\bf{A}})}{\partial {\bf{A}} ^ 2} 3 \times 3 \bf A \frac{\partial ^2 \det({\bf{A}})}{\partial {\bf{A}} ^ 2}","['calculus', 'linear-algebra', 'multivariable-calculus', 'matrix-calculus', 'tensors']"
80,Representation determined by traces,Representation determined by traces,,"Let $A$ be a unital algebra over the complex numbers and let $\pi,\pi':A\to M_n(\mathbb C)$ be two unital algebra homomorphisms. Assume both are surjective and their traces agree, i.e., $tr\ \pi(a)=tr\ \pi'(a)$ holds for every $a\in A$ . Does it follow that they are conjugate, i.e., there exists an invertible matrix $S$ with $\pi'(a)=S\pi(a)S^{-1}$ for every $a\in A$ ? If so, is it a classical result?","Let be a unital algebra over the complex numbers and let be two unital algebra homomorphisms. Assume both are surjective and their traces agree, i.e., holds for every . Does it follow that they are conjugate, i.e., there exists an invertible matrix with for every ? If so, is it a classical result?","A \pi,\pi':A\to M_n(\mathbb C) tr\ \pi(a)=tr\ \pi'(a) a\in A S \pi'(a)=S\pi(a)S^{-1} a\in A","['linear-algebra', 'representation-theory']"
81,What is canonical spectral theorem?,What is canonical spectral theorem?,,"My teacher has given me the following definition of the canonical spectrum theorem: Let a matrix the $A \in M_{n\times n}(\mathbb{R})$ , and set of eigenvalues, $\sigma(A)$ ={ $\lambda_1$ , $\lambda_2$ ........, $\lambda_k$ }. $A$ is diagonalisable $\Leftrightarrow A = \sum{\lambda_ip_i}$ such that $p_i:$ orthogonal projection onto $\mathcal{E}_{\lambda_i(A)},$ the eigen space $p_ip_j  = 0 \space \text{if} \space i\neq j$ $\sum{p_i}=I,$ the identity matrix. My question is: I don't understand above 3 points, what does mean of orthogonal projection here, and how it is related with eigen space? Also point 2 and 3 don't understand. Anybody help me to understand by giving example of above 3 points.","My teacher has given me the following definition of the canonical spectrum theorem: Let a matrix the , and set of eigenvalues, ={ , ........, }. is diagonalisable such that orthogonal projection onto the eigen space the identity matrix. My question is: I don't understand above 3 points, what does mean of orthogonal projection here, and how it is related with eigen space? Also point 2 and 3 don't understand. Anybody help me to understand by giving example of above 3 points.","A \in M_{n\times n}(\mathbb{R}) \sigma(A) \lambda_1 \lambda_2 \lambda_k A \Leftrightarrow A = \sum{\lambda_ip_i} p_i: \mathcal{E}_{\lambda_i(A)}, p_ip_j  = 0 \space \text{if} \space i\neq j \sum{p_i}=I,","['linear-algebra', 'matrices']"
82,Intuition for geometric multiplicity $\leq$ algebraic multiplicity,Intuition for geometric multiplicity  algebraic multiplicity,\leq,"Can someone provide intuition (not a proof) for why geometric multiplicity (i.e. the dimension of the $\lambda$ -eigenspace) $\leq$ algebraic multiplicity (i.e. the number of times $\lambda$ appears as a root of $\det(A - \lambda I) = 0$ ). Here is my bad intuition: Since there are $k$ linearly independent vector solutions to $(A - \lambda I)\vec{v} = 0$ , where $k$ is the geometric multiplicity of $\lambda$ , $\lambda$ must have a ""degree"" of at least $k$ , which translates to algebraic multiplicity. Hopefully, someone can provide some better intuition, perhaps in terms of a visualisation (I struggle to visualise the geometric significance of the algebraic multiplicity of $\lambda$ ).","Can someone provide intuition (not a proof) for why geometric multiplicity (i.e. the dimension of the -eigenspace) algebraic multiplicity (i.e. the number of times appears as a root of ). Here is my bad intuition: Since there are linearly independent vector solutions to , where is the geometric multiplicity of , must have a ""degree"" of at least , which translates to algebraic multiplicity. Hopefully, someone can provide some better intuition, perhaps in terms of a visualisation (I struggle to visualise the geometric significance of the algebraic multiplicity of ).",\lambda \leq \lambda \det(A - \lambda I) = 0 k (A - \lambda I)\vec{v} = 0 k \lambda \lambda k \lambda,"['linear-algebra', 'eigenvalues-eigenvectors']"
83,Prove with induction if $A= \begin{pmatrix}2&1\\-1&0 \end{pmatrix}$ $\forall n \in \mathbb{N}$ $A^n= \begin{pmatrix} n+1&n\\-n&1-n \end{pmatrix}$,Prove with induction if,A= \begin{pmatrix}2&1\\-1&0 \end{pmatrix} \forall n \in \mathbb{N} A^n= \begin{pmatrix} n+1&n\\-n&1-n \end{pmatrix},Prove with induction if $$A = \begin{pmatrix} 2 & 1 \\ -1 & 0  \end{pmatrix}$$ then $\forall n \in \mathbb{N}$ $$A^n = \begin{pmatrix} n + 1 & n \\ -n & 1 - n  \end{pmatrix}$$ For $n = 1$ we have $$A^1 = \begin{pmatrix} 1 + 1 & 1 \\ -1 & 1 - 1  \end{pmatrix} = \begin{pmatrix} 2 & 1 \\ -1 & 0  \end{pmatrix}$$ so the base case is correct. EDIT: we assume it holds true for $n$ (Thank you @gdcvdqpl !) For $n+1$ we have $$A^{(n+1)} = \begin{pmatrix} (n+1) + 1 & (n+1) \\ -(n+1) & 1 - (n+1)  \end{pmatrix} = \begin{pmatrix} n + 2 & n + 1 \\ - n - 1 & - n  \end{pmatrix}$$ And also $$A^nA^1 = \begin{pmatrix} n + 1 & n \\ -n & 1 - n  \end{pmatrix}  \begin{pmatrix} 2 & 1 \\ -1 & 0  \end{pmatrix} =  \begin{pmatrix} 2(n+1) - n & n + 1 \\ -2n - (1 - n) & -n  \end{pmatrix} = \begin{pmatrix} n + 2 & n + 1 \\ - n - 1 & - n  \end{pmatrix} $$ So it also holds for $n+1$ . Is that correct ? We don't have solutions to this exercise. Feel free to point out any inconsistency. Thank you for helping me,Prove with induction if then For we have so the base case is correct. EDIT: we assume it holds true for (Thank you @gdcvdqpl !) For we have And also So it also holds for . Is that correct ? We don't have solutions to this exercise. Feel free to point out any inconsistency. Thank you for helping me,"A = \begin{pmatrix}
2 & 1 \\
-1 & 0 
\end{pmatrix} \forall n \in \mathbb{N} A^n = \begin{pmatrix}
n + 1 & n \\
-n & 1 - n 
\end{pmatrix} n = 1 A^1 = \begin{pmatrix}
1 + 1 & 1 \\
-1 & 1 - 1 
\end{pmatrix} = \begin{pmatrix}
2 & 1 \\
-1 & 0 
\end{pmatrix} n n+1 A^{(n+1)} = \begin{pmatrix}
(n+1) + 1 & (n+1) \\
-(n+1) & 1 - (n+1) 
\end{pmatrix} = \begin{pmatrix}
n + 2 & n + 1 \\
- n - 1 & - n 
\end{pmatrix} A^nA^1 = \begin{pmatrix}
n + 1 & n \\
-n & 1 - n 
\end{pmatrix} 
\begin{pmatrix}
2 & 1 \\
-1 & 0 
\end{pmatrix} = 
\begin{pmatrix}
2(n+1) - n & n + 1 \\
-2n - (1 - n) & -n 
\end{pmatrix} =
\begin{pmatrix}
n + 2 & n + 1 \\
- n - 1 & - n 
\end{pmatrix}
 n+1","['linear-algebra', 'matrices', 'induction']"
84,"$T: V \to V$ is a linear map. If $\dim(\ker T \cap \text{Im}T)\neq0$, prove $\dim\ker T^2\geq2$","is a linear map. If , prove",T: V \to V \dim(\ker T \cap \text{Im}T)\neq0 \dim\ker T^2\geq2,"$T: V \to V$ is a linear map. If $\dim(\ker T \cap \text{Im}T)\neq0$ , prove $\dim\ker T^2\geq2$ so I can deduce $\dim\ker T\geq 1$ Because the intersection of the image and the kernel, is contained in the kernel. And it is known that $\dim\ker T^2\geq \dim\ker T$ So $\dim\ker T^2\geq 1$ . But I have no idea how to prove that it can't be $1$ .","is a linear map. If , prove so I can deduce Because the intersection of the image and the kernel, is contained in the kernel. And it is known that So . But I have no idea how to prove that it can't be .",T: V \to V \dim(\ker T \cap \text{Im}T)\neq0 \dim\ker T^2\geq2 \dim\ker T\geq 1 \dim\ker T^2\geq \dim\ker T \dim\ker T^2\geq 1 1,['linear-algebra']
85,Is it true that all matrices of this type are involutory?,Is it true that all matrices of this type are involutory?,,"I came across a problem that suggested the following for a $3 \times 3$ matrix. $$\text{Let } H = I - \frac{2}{3}A$$ $$\text{Then } H^2 = I$$ For $H, A \in M_{3,3}$ and $A$ being a matrix of all ones. I played around with this and found that for $H, A \in M_{n,n}$ and $A$ being a matrix of all ones $$\text{Let } H = I - \frac{2}{n}A$$ $$\text{Then } H^2 = I$$ Works in the cases for $0 \leq n \leq 4$ , which makes me think it's probably true for any $n$ . Is that the case?","I came across a problem that suggested the following for a matrix. For and being a matrix of all ones. I played around with this and found that for and being a matrix of all ones Works in the cases for , which makes me think it's probably true for any . Is that the case?","3 \times 3 \text{Let } H = I - \frac{2}{3}A \text{Then } H^2 = I H, A \in M_{3,3} A H, A \in M_{n,n} A \text{Let } H = I - \frac{2}{n}A \text{Then } H^2 = I 0 \leq n \leq 4 n","['linear-algebra', 'matrices', 'involutions']"
86,"Solving master equation for binding kinetics $\frac{dp_m(t)}{dt}=a\,(N-m+1)(M-m+1)p_{m-1}(t)+b\,(m+1)p_{m+1}(t)-[b\,m+a(N-m)(M-m)]p_m(t)$",Solving master equation for binding kinetics,"\frac{dp_m(t)}{dt}=a\,(N-m+1)(M-m+1)p_{m-1}(t)+b\,(m+1)p_{m+1}(t)-[b\,m+a(N-m)(M-m)]p_m(t)","Background : This problem appears when you try solving the stochastic binding kinetics . There are a total of $M$ and $N$ molecules of the types $R$ and $L$ . These molecules can be either in a bound ( $RL$ ) or an unbound state. Each pair of unbound $R$ and $L$ molecules can bind with the rate $a$ , and each bound molecule $RL$ can unbind with the rate $b$ . Starting with $m_0$ initially bound pairs, I would like to find $p_m(t)$ , the probability of having $m$ molecules in the bound state $RL$ at time $t$ . Problem : The following set of coupled differential equations (also known as Master equations) describe the dynamics of the probabilities $p_m(t)$ as functions of time $$ \frac{dp_m(t)}{dt}=a\,(N-m+1)(M-m+1)p_{m-1}(t)+b\,(m+1)p_{m+1}(t)-[b\,m+a(N-m)(M-m)]p_m(t), $$ where $M$ and $N$ are integers with $0<M\leq N$ and $a>0$ and $b>0$ are real numbers, for integer $m$ s between $0\leq m\leq M$ , with the initial conditions $p_m(0) = \delta_{m,m_0}$ , $0\leq m_0\leq M$ . Assume $p_{-1}(t)=p_{M+1}(t)=0$ . I would like to solve these equations. What I tried so far : We can write the set of ODEs as a Matrix equation of the form $$ \frac{d}{dt}\left(\begin{array}{c} p_0\\ \vdots\\ p_M \end{array}\right)=\underbrace{\left(\begin{array}{cccc} -aNM&b &0&0&\dots\\ aNM&-b-a(N-1)(M-1) &2b&0&\dots\\ &\vdots \end{array}\right)}_{\mathbf{A}}\left(\begin{array}{c} p_0\\ \vdots\\ p_M \end{array}\right) $$ with the formal solution $\mathbf{p}(t)=e^{\mathbf{A}t}\mathbf{p}(0)$ . But in order to exponential the matrix $t\mathbf{A}$ , I need to diagonalize $\mathbf A$ , but I don't know how to do that. It is a tridiagonal matrix, each column adds up to one, and it has a simple form. I'm sure someone on SE knows how to diagonalize this. Alternatively, I tried solving the problem using a generating function defined as $$ f(z,t) = \sum_{m=0}^M p_m(t) z^m. $$ You can differentiate $f$ with respect to $t$ and find the following PDE satisfied by $f$ : $$ \partial_t f = a(z-1)\left[MNf-\left((M+N-1)z+\frac{b}{a}\right)\partial_zf+z^2\partial^2_zf\right], $$ with the initial condition $f(z,0)=z^{m_0}$ . But I do not know how to solve this equation either. It is a second-order linear PDE with a solution that is a polynomial in $z$ , so it should be solvable. Steady State Results : Binding kinetics has an equilibrium solution that is given by Boltzmann distribution with the partition function $$\mathcal{Z} = \sum_{m=0}^M \left(\frac ba\right)^m {M \choose m}{N \choose m}={}_2F_1(-M,-N,1,b/a).$$ This gives $$p_m(\infty) = \frac{\left(\frac ba\right)^m {M \choose m}{N \choose m}}{{}_2F_1(-M,-N,1,b/a)}.$$ What I need : Ideally looking for a closed-form solution for either $p_m(t)$ or $f(z,t)$ . If there is no closed-form solution, at least an explicit expression for $p_m(t)$ that could involve special functions or maybe even a sum or series, but something that I can explicitly calculate. I'm not looking for a numerical solution.","Background : This problem appears when you try solving the stochastic binding kinetics . There are a total of and molecules of the types and . These molecules can be either in a bound ( ) or an unbound state. Each pair of unbound and molecules can bind with the rate , and each bound molecule can unbind with the rate . Starting with initially bound pairs, I would like to find , the probability of having molecules in the bound state at time . Problem : The following set of coupled differential equations (also known as Master equations) describe the dynamics of the probabilities as functions of time where and are integers with and and are real numbers, for integer s between , with the initial conditions , . Assume . I would like to solve these equations. What I tried so far : We can write the set of ODEs as a Matrix equation of the form with the formal solution . But in order to exponential the matrix , I need to diagonalize , but I don't know how to do that. It is a tridiagonal matrix, each column adds up to one, and it has a simple form. I'm sure someone on SE knows how to diagonalize this. Alternatively, I tried solving the problem using a generating function defined as You can differentiate with respect to and find the following PDE satisfied by : with the initial condition . But I do not know how to solve this equation either. It is a second-order linear PDE with a solution that is a polynomial in , so it should be solvable. Steady State Results : Binding kinetics has an equilibrium solution that is given by Boltzmann distribution with the partition function This gives What I need : Ideally looking for a closed-form solution for either or . If there is no closed-form solution, at least an explicit expression for that could involve special functions or maybe even a sum or series, but something that I can explicitly calculate. I'm not looking for a numerical solution.","M N R L RL R L a RL b m_0 p_m(t) m RL t p_m(t) 
\frac{dp_m(t)}{dt}=a\,(N-m+1)(M-m+1)p_{m-1}(t)+b\,(m+1)p_{m+1}(t)-[b\,m+a(N-m)(M-m)]p_m(t),
 M N 0<M\leq N a>0 b>0 m 0\leq m\leq M p_m(0) = \delta_{m,m_0} 0\leq m_0\leq M p_{-1}(t)=p_{M+1}(t)=0 
\frac{d}{dt}\left(\begin{array}{c}
p_0\\
\vdots\\
p_M
\end{array}\right)=\underbrace{\left(\begin{array}{cccc}
-aNM&b &0&0&\dots\\
aNM&-b-a(N-1)(M-1) &2b&0&\dots\\
&\vdots
\end{array}\right)}_{\mathbf{A}}\left(\begin{array}{c}
p_0\\
\vdots\\
p_M
\end{array}\right)
 \mathbf{p}(t)=e^{\mathbf{A}t}\mathbf{p}(0) t\mathbf{A} \mathbf A 
f(z,t) = \sum_{m=0}^M p_m(t) z^m.
 f t f 
\partial_t f = a(z-1)\left[MNf-\left((M+N-1)z+\frac{b}{a}\right)\partial_zf+z^2\partial^2_zf\right],
 f(z,0)=z^{m_0} z \mathcal{Z} = \sum_{m=0}^M \left(\frac ba\right)^m {M \choose m}{N \choose m}={}_2F_1(-M,-N,1,b/a). p_m(\infty) = \frac{\left(\frac ba\right)^m {M \choose m}{N \choose m}}{{}_2F_1(-M,-N,1,b/a)}. p_m(t) f(z,t) p_m(t)","['linear-algebra', 'probability', 'partial-differential-equations', 'stochastic-processes', 'markov-chains']"
87,"Easy ways to prove that a binary $[9,3,5]$ linear code can't exist",Easy ways to prove that a binary  linear code can't exist,"[9,3,5]","I've tried to solve an exercise that goes like this: Prove that a $[9,3,d]$ linear binary code that corrects 2 mistakes doesn't exist. To prove this, first I've determined that $d=5$ is the most restrictive case. Then, I've tried to use Hamming's, Singleton's and Plotkin's bounds in order to easily see if said code can't exist but I haven't been able to prove anything with the bounds. Another thing I've tried is to use the fact that $d-1$ is the largest number of linearly independent columns in the control matrix. This way, I've proved that said code cannot exist by analizing the different combinations of vectors to put in each column. However, this has taken such a long time and I was wondering wether there was a time efficient way to solve this kind of exercises. Thanks in advance","I've tried to solve an exercise that goes like this: Prove that a linear binary code that corrects 2 mistakes doesn't exist. To prove this, first I've determined that is the most restrictive case. Then, I've tried to use Hamming's, Singleton's and Plotkin's bounds in order to easily see if said code can't exist but I haven't been able to prove anything with the bounds. Another thing I've tried is to use the fact that is the largest number of linearly independent columns in the control matrix. This way, I've proved that said code cannot exist by analizing the different combinations of vectors to put in each column. However, this has taken such a long time and I was wondering wether there was a time efficient way to solve this kind of exercises. Thanks in advance","[9,3,d] d=5 d-1","['linear-algebra', 'combinatorics', 'discrete-mathematics', 'upper-lower-bounds', 'coding-theory']"
88,How to solve this problem with Linear Algebra?,How to solve this problem with Linear Algebra?,,"I have a $N \times N$ grid, each cell initialized with an integer between $0$ and $5$ . If I select one cell, each adjacent cell (8 cells if not on an edge or a corner) - but not the selected one- are incremented by $1$ then taken $\text{mod } 5$ . I need to find the cells to select to get from the initial state to a given targeted state. I assume there is always a solution but I don't know if it is unique. I have done this heuristically, but my implementation is very inefficient, and I wondered if linear algebra could be used here. In case it can be done with LA, any leads to help me formulate the problems would be very appreciated. Thank you in advance for your help","I have a grid, each cell initialized with an integer between and . If I select one cell, each adjacent cell (8 cells if not on an edge or a corner) - but not the selected one- are incremented by then taken . I need to find the cells to select to get from the initial state to a given targeted state. I assume there is always a solution but I don't know if it is unique. I have done this heuristically, but my implementation is very inefficient, and I wondered if linear algebra could be used here. In case it can be done with LA, any leads to help me formulate the problems would be very appreciated. Thank you in advance for your help",N \times N 0 5 1 \text{mod } 5,"['linear-algebra', 'algorithms', 'recreational-mathematics']"
89,The Inverse Equation for a 2x2 matrix,The Inverse Equation for a 2x2 matrix,,"I am relatively new to mathematics, especially matrices. I saw a similar question on here about my question but I could not follow what was being said. I am trying to work out the inverse of a $2\times 2$ matrix $A$ . I know the inverse matrix $A^{-1}$ reads: $$A^{-1} = \dfrac{1}{\text{det}(A)} \begin{pmatrix} d & -b \\ -c & a \end{pmatrix}$$ My question is: how is the inverse matrix obtained? I have tried transposing the cofactor matrix $$\begin{pmatrix}a & -b \\ c & -d\end{pmatrix} \longrightarrow \begin{pmatrix} a & c \\ -b & -d\end{pmatrix}$$ I am using the book Maths for Chemistry Paul Monk Lindsey J Munro as a starting point. Thank you. Alternatively, I have tried using the determinant $(ad-bc)$ therefore, \begin{pmatrix}a & -b \\ -c & d\end{pmatrix} . However, I am not sure how the $a$ and $d$ terms swap as I thought this was not allowed during a transpose, as the principal diagonal remains unchanged.","I am relatively new to mathematics, especially matrices. I saw a similar question on here about my question but I could not follow what was being said. I am trying to work out the inverse of a matrix . I know the inverse matrix reads: My question is: how is the inverse matrix obtained? I have tried transposing the cofactor matrix I am using the book Maths for Chemistry Paul Monk Lindsey J Munro as a starting point. Thank you. Alternatively, I have tried using the determinant therefore, . However, I am not sure how the and terms swap as I thought this was not allowed during a transpose, as the principal diagonal remains unchanged.",2\times 2 A A^{-1} A^{-1} = \dfrac{1}{\text{det}(A)} \begin{pmatrix} d & -b \\ -c & a \end{pmatrix} \begin{pmatrix}a & -b \\ c & -d\end{pmatrix} \longrightarrow \begin{pmatrix} a & c \\ -b & -d\end{pmatrix} (ad-bc) \begin{pmatrix}a & -b \\ -c & d\end{pmatrix} a d,"['linear-algebra', 'matrices', 'inverse', 'matrix-calculus']"
90,The result of a standard form line equation [duplicate],The result of a standard form line equation [duplicate],,"This question already has answers here : What does distance of a point from line being negative signify? (4 answers) Closed 1 year ago . If we substituted a point in the standard form equation, what the resulting number indicates? For example: We have the line: $$3x + 6y + 12 = 0$$ And we have the point $I(1,2)$ Substituting I in the line equation: $$3(1) + 6(2) + 12 = 0$$ $$=> 27 = 0$$ Does this only means that the point does not belong to the line? And what the $27$ indicates in this example? (I know that if the resulted number is positive that means that the point is above the line and if it is negative the point is below the line). But my question is that if the number $27$ indicates something, apart from the sign","This question already has answers here : What does distance of a point from line being negative signify? (4 answers) Closed 1 year ago . If we substituted a point in the standard form equation, what the resulting number indicates? For example: We have the line: And we have the point Substituting I in the line equation: Does this only means that the point does not belong to the line? And what the indicates in this example? (I know that if the resulted number is positive that means that the point is above the line and if it is negative the point is below the line). But my question is that if the number indicates something, apart from the sign","3x + 6y + 12 = 0 I(1,2) 3(1) + 6(2) + 12 = 0 => 27 = 0 27 27","['linear-algebra', 'geometry', 'algebra-precalculus']"
91,Why use elementary matrices?,Why use elementary matrices?,,"I just started learning linear algebra and I'm having a hard time figuring out why creating an elementary matrix to perform row operations on another matrix is necessary if we could just perform the row operations on the matrix itself. Also, which of the two methods would be more efficient?","I just started learning linear algebra and I'm having a hard time figuring out why creating an elementary matrix to perform row operations on another matrix is necessary if we could just perform the row operations on the matrix itself. Also, which of the two methods would be more efficient?",,"['linear-algebra', 'matrices', 'gaussian-elimination']"
92,"What is the difference, geometrically, between row vectors and column vectors?","What is the difference, geometrically, between row vectors and column vectors?",,"I have been told in the textbook I'm following that ""By convention, always assume that vectors are in column orientation, unless stated otherwise."" (Cohen, 2021: 27). Fair enough. But as I progress into the study of vector spaces,this distinction between row and column vectors seems to take on more significance. In particular,I am introduced to fields and vector spaces and being told that ""The space $\mathbb{R}^n$ consists of all column vectors with $n$ components"". So is this different from having your space made of all row vectors with n components? Generally: what is the geometrical interpretation of the distinction between row vectors and column vectors? (As I understand it, algebraically, vectors are just ordered list of numbers. The different between row vectors and column vectors here only shows up in the outcome of basic vectors operations like the dot product.)","I have been told in the textbook I'm following that ""By convention, always assume that vectors are in column orientation, unless stated otherwise."" (Cohen, 2021: 27). Fair enough. But as I progress into the study of vector spaces,this distinction between row and column vectors seems to take on more significance. In particular,I am introduced to fields and vector spaces and being told that ""The space consists of all column vectors with components"". So is this different from having your space made of all row vectors with n components? Generally: what is the geometrical interpretation of the distinction between row vectors and column vectors? (As I understand it, algebraically, vectors are just ordered list of numbers. The different between row vectors and column vectors here only shows up in the outcome of basic vectors operations like the dot product.)",\mathbb{R}^n n,"['linear-algebra', 'vectors']"
93,Finding eigenvalues and eigenvectors geometrically.,Finding eigenvalues and eigenvectors geometrically.,,"Let $A=\left[\begin{matrix} 0 & 1\\1 & 0 \end{matrix}\right]$ represent reflection about the line $y=x$ . I can calculate eigenvalues and eigenvectors mathematically, but I have hard time getting the results geometrically. There are some similar posts available but they are not of any help. For example in this post , I don't know how they found 1 and -1 as eigenvalues?  I must be missing some points. So the question is, how to find eigenvalues and eigenvectors geometrically? Why in this case eigenvalues are 1 and -1? and how to obtain corresponding eigenvectors?","Let represent reflection about the line . I can calculate eigenvalues and eigenvectors mathematically, but I have hard time getting the results geometrically. There are some similar posts available but they are not of any help. For example in this post , I don't know how they found 1 and -1 as eigenvalues?  I must be missing some points. So the question is, how to find eigenvalues and eigenvectors geometrically? Why in this case eigenvalues are 1 and -1? and how to obtain corresponding eigenvectors?",A=\left[\begin{matrix} 0 & 1\\1 & 0 \end{matrix}\right] y=x,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
94,What is the reason that isomorphisms between vector spaces are invertible?,What is the reason that isomorphisms between vector spaces are invertible?,,"I ask this soft-question because I am confused about the intuition of the mechanics of proving something is an isomorphism: My professor gives the following steps to check for isomorphism: The function is one-to-one The function is onto The function preserves addition The function preserves multiplication However, I looked up another video to further clarify the intuition behind isomorphism, and the video states: ""Suppose V and W are vector spaces over the same field. We say that $V$ and $W$ are isomorphic if there exists an invertible linear transformation $T: V\to W$ "" I think I understand why the preservation of addition and multiplication fits into the ""invertibility"" as they are commutative. But I don't know how the ""1-to-1 and onto (i.e., bijection)"" contribute to invertibility as defined by the video? Thanks!","I ask this soft-question because I am confused about the intuition of the mechanics of proving something is an isomorphism: My professor gives the following steps to check for isomorphism: The function is one-to-one The function is onto The function preserves addition The function preserves multiplication However, I looked up another video to further clarify the intuition behind isomorphism, and the video states: ""Suppose V and W are vector spaces over the same field. We say that and are isomorphic if there exists an invertible linear transformation "" I think I understand why the preservation of addition and multiplication fits into the ""invertibility"" as they are commutative. But I don't know how the ""1-to-1 and onto (i.e., bijection)"" contribute to invertibility as defined by the video? Thanks!",V W T: V\to W,"['linear-algebra', 'soft-question', 'inverse', 'vector-space-isomorphism']"
95,Should we always regard a $1\times 1$ matrix as a scalar?,Should we always regard a  matrix as a scalar?,1\times 1,"Should we always regard a $1\times 1$ matrix as a scalar? (I think, ""yes"".) And if so, how should we address this in our elementaty Linear Algebra courses? Let me give an example to illustrate my question. Suppose $A= \left[\begin{array}{rr}1 & 2 \\-2 & 1\end{array}\right],$ $B = \bigl[-2\,,2\bigr],$ and and $C = \left[\begin{array}{r}4 \\5\end{array}\right].$ Then the calculation \begin{equation}  A(BC) = \left[\begin{array}{rr}1 & 2 \\-2 & 1\end{array}\right]   \left(\begin{array}{r} \bigl[-2\,,2\bigr] \\ \rule{1pt}{0pt} \end{array}\left[\begin{array}{r}4 \\5\end{array}\right] \right)     = \left[\begin{array}{rr}1 & 2 \\-2 & 1\end{array}\right]\cdot 2 = \left[\begin{array}{rr}2 & 4 \\-4 & 2\end{array}\right] \end{equation} seems completely reasonable, doesn't it? And yet it's technically incorrect, since $A$ is a $2\times 2$ matrix and $BC$ is a $1\times 1$ matrix. I've ben trying to come up with a technically correct way to conclude that $A(BC)$ can indeed by computed as above. And here's the best I can come up with. There's an obvious bijection, let's call it $J$ , from the the $1\times 1$ matrices to the scalars, with $x = J([x])$ for any scalar $x$ . If we want to be able to carry out `` $A(BC)$ '' as above, what we really mean is that it is equal to $AJ\bigl([BC]).$ But someone could ask how we know when it's appropriate to interpret $A(BC)$ as $2A$ and when it's appropriate to interpret that product as undefined. My own answer is that it depends on context or something, but that seems unsatifying to me. Does anyone know of a good way to address this matter, which is both rigorous at the foundational level and can easily be inserted into an elementary discussion? For example, when we define multiplication of two matrices, should we add a caveat that any $1\times 1$ matrix should be be regard as a scalar? But then, is there ever a situation where we want to regard a $1\times 1$ matrix as just that, and calling it scalar would mess something else up at the level of foundations/definition? Thanks in advance. -JGW","Should we always regard a matrix as a scalar? (I think, ""yes"".) And if so, how should we address this in our elementaty Linear Algebra courses? Let me give an example to illustrate my question. Suppose and and Then the calculation seems completely reasonable, doesn't it? And yet it's technically incorrect, since is a matrix and is a matrix. I've ben trying to come up with a technically correct way to conclude that can indeed by computed as above. And here's the best I can come up with. There's an obvious bijection, let's call it , from the the matrices to the scalars, with for any scalar . If we want to be able to carry out `` '' as above, what we really mean is that it is equal to But someone could ask how we know when it's appropriate to interpret as and when it's appropriate to interpret that product as undefined. My own answer is that it depends on context or something, but that seems unsatifying to me. Does anyone know of a good way to address this matter, which is both rigorous at the foundational level and can easily be inserted into an elementary discussion? For example, when we define multiplication of two matrices, should we add a caveat that any matrix should be be regard as a scalar? But then, is there ever a situation where we want to regard a matrix as just that, and calling it scalar would mess something else up at the level of foundations/definition? Thanks in advance. -JGW","1\times 1 A= \left[\begin{array}{rr}1 & 2 \\-2 & 1\end{array}\right], B = \bigl[-2\,,2\bigr], C = \left[\begin{array}{r}4 \\5\end{array}\right]. \begin{equation}  A(BC) = \left[\begin{array}{rr}1 & 2 \\-2 & 1\end{array}\right]
  \left(\begin{array}{r} \bigl[-2\,,2\bigr] \\ \rule{1pt}{0pt} \end{array}\left[\begin{array}{r}4 \\5\end{array}\right] \right) 
   = \left[\begin{array}{rr}1 & 2 \\-2 & 1\end{array}\right]\cdot 2 = \left[\begin{array}{rr}2 & 4 \\-4 & 2\end{array}\right]
\end{equation} A 2\times 2 BC 1\times 1 A(BC) J 1\times 1 x = J([x]) x A(BC) AJ\bigl([BC]). A(BC) 2A 1\times 1 1\times 1","['linear-algebra', 'matrices']"
96,Blend multiple shapes.,Blend multiple shapes.,,"Using circle and square equations I was able to draw this . Any way to blend these two shapes(equations) like this and this . I want to achieve metaballs like effect. I'm using unity engine to render the above, I want a 2D version of it. I basically colored all pixels which are inside a circle. Eg: using unit circle at origin equation $x^2 + y^2 = 1$ I'm coloring all pixels if $p_x^2 + p_y^2 <= 1$ , where $p_x$ and $p_y$ are position of pixel.So I want a equation(or method) which blends the shapes and determines if a pixel is inside the overall shape.","Using circle and square equations I was able to draw this . Any way to blend these two shapes(equations) like this and this . I want to achieve metaballs like effect. I'm using unity engine to render the above, I want a 2D version of it. I basically colored all pixels which are inside a circle. Eg: using unit circle at origin equation I'm coloring all pixels if , where and are position of pixel.So I want a equation(or method) which blends the shapes and determines if a pixel is inside the overall shape.",x^2 + y^2 = 1 p_x^2 + p_y^2 <= 1 p_x p_y,"['linear-algebra', 'geometry']"
97,Cauchy-Schwarz inequality for nonsymmetric matrices,Cauchy-Schwarz inequality for nonsymmetric matrices,,"I am interested in Cauchy-Schwarz inequalities for inner products of the form $\langle Ax,y\rangle$ where $A$ is some matrix. It is rather easy to see that the Cauchy-Schwarz inequality continues to hold when relaxing the assumption of positive definiteness on $A$ to positive semi-definitess. I asked myself whether one can loosen the assumption on symmetry of $A$ . Suppose $A \in \mathbb{R}^{n \times n}$ is a matrix such that $\langle Ax,x\rangle \ge \lambda |x|^2$ . Is it true that for all $x,y \in \mathbb{R}^n$ we have the following inequality $$\langle Ax,y \rangle^2 \le \langle Ax, x \rangle \langle Ay,y \rangle? $$ If not, is there any other similar inequality, which replaces the Cauchy-Schwarz inequality?","I am interested in Cauchy-Schwarz inequalities for inner products of the form where is some matrix. It is rather easy to see that the Cauchy-Schwarz inequality continues to hold when relaxing the assumption of positive definiteness on to positive semi-definitess. I asked myself whether one can loosen the assumption on symmetry of . Suppose is a matrix such that . Is it true that for all we have the following inequality If not, is there any other similar inequality, which replaces the Cauchy-Schwarz inequality?","\langle Ax,y\rangle A A A A \in \mathbb{R}^{n \times n} \langle Ax,x\rangle \ge \lambda |x|^2 x,y \in \mathbb{R}^n \langle Ax,y \rangle^2 \le \langle Ax, x \rangle \langle Ay,y \rangle? ","['real-analysis', 'linear-algebra', 'cauchy-schwarz-inequality']"
98,"If $\vert A\vert+\vert B\vert =0,$ then What is the value of $\vert A+B\vert$?",If  then What is the value of ?,"\vert A\vert+\vert B\vert =0, \vert A+B\vert","There are two square matrices $A$ and $B$ of same order such that $A^2=I$ and $B^2=I,$ Where $I$ is a unit matrix.If $\vert A\vert+\vert  B\vert =0,$ then find the value of $\vert A+B\vert ,$ here $\vert  A\vert$ denotes the determinant of matrix $A$ Solution: Since $A^2=I$ then Cayley-Hamilton theorem implies that the characteristic polynomial of last equation is $\lambda_A^2-1=0\implies \lambda_A=+1,-1$ . Since the product of Eigenvalues of a matrix is equal to the determinant of the matrix,So $\vert A\vert =(+1)(-1)=-1\tag{1}$ . Similarly,Since $B^2=I$ then Cayley-Hamilton theorem implies that the characteristic polynomial of last equation is $\lambda_B^2-1=0\implies \lambda_B=+1,-1$ . Since the product of Eigenvalues of a matrix is equal to the determinant of the matrix,So $\vert B\vert =(+1)(-1)=-1\tag{2}$ . Equations $(1)$ & $(2)$ implies that $\vert A\vert +\vert B\vert=-2$ But,it is given that $\vert A\vert+\vert B\vert =0$ Please point out my mistake??","There are two square matrices and of same order such that and Where is a unit matrix.If then find the value of here denotes the determinant of matrix Solution: Since then Cayley-Hamilton theorem implies that the characteristic polynomial of last equation is . Since the product of Eigenvalues of a matrix is equal to the determinant of the matrix,So . Similarly,Since then Cayley-Hamilton theorem implies that the characteristic polynomial of last equation is . Since the product of Eigenvalues of a matrix is equal to the determinant of the matrix,So . Equations & implies that But,it is given that Please point out my mistake??","A B A^2=I B^2=I, I \vert A\vert+\vert
 B\vert =0, \vert A+B\vert , \vert
 A\vert A A^2=I \lambda_A^2-1=0\implies \lambda_A=+1,-1 \vert A\vert =(+1)(-1)=-1\tag{1} B^2=I \lambda_B^2-1=0\implies \lambda_B=+1,-1 \vert B\vert =(+1)(-1)=-1\tag{2} (1) (2) \vert A\vert +\vert B\vert=-2 \vert A\vert+\vert B\vert =0","['linear-algebra', 'matrices', 'solution-verification', 'eigenvalues-eigenvectors', 'determinant']"
99,What field do eigenvalues live in?,What field do eigenvalues live in?,,"Given a vector space $V$ over scalar field $F$ , and given a linear transformation $T : V → V$ , the definition of eigenvalues and eigenvectors of $T$ is: A nonzero vector $\mathbf{v} \in V$ is an eigenvector of $T$ iff there exists a scalar $\lambda \in F$ such that $T(\mathbf{v}) = \lambda \mathbf{v}$ . $\lambda$ is then said to be an eigenvalue of $T$ corresponding to $\mathbf{v}$ . But it seems some eigenvalues escape this definition. For example, let $F = \mathbb{R}$ , $V = \mathbb{R}^2$ , and $T = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}$ . Then some computation reveals that the eigenvalues are $i$ and $-i$ , which do not live in $F$ . If $V$ is finite-dimensional, I can at least confidently say that all eigenvalues live in $\overline{F}$ , the algebraic closure of $F$ . But what for infinite-dimensional cases?","Given a vector space over scalar field , and given a linear transformation , the definition of eigenvalues and eigenvectors of is: A nonzero vector is an eigenvector of iff there exists a scalar such that . is then said to be an eigenvalue of corresponding to . But it seems some eigenvalues escape this definition. For example, let , , and . Then some computation reveals that the eigenvalues are and , which do not live in . If is finite-dimensional, I can at least confidently say that all eigenvalues live in , the algebraic closure of . But what for infinite-dimensional cases?",V F T : V → V T \mathbf{v} \in V T \lambda \in F T(\mathbf{v}) = \lambda \mathbf{v} \lambda T \mathbf{v} F = \mathbb{R} V = \mathbb{R}^2 T = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix} i -i F V \overline{F} F,"['linear-algebra', 'eigenvalues-eigenvectors', 'linear-transformations']"
