,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Solving inequality for convex functions.,Solving inequality for convex functions.,,"A function $f : I \rightarrow \mathbb R$ on an interval $I$ is convex   if $f((1-t)x+ty)\le (1-t)f(x)+tf(y), \forall x,y \in I, t \in [0,1]$ Assume now that $I$ is an open interval.  Show that if $f$ is convex   then for each $c \in I$ there exists $m \in \mathbb R$ such that $m(x  − c) + f(c) \le f(x)$ for all $x \in I$ , and if in addition $f$ is   differentiable at $c$ then $f'(c)$ is the unique $m$ that works. In   general, must m be unique? (we have previously shown that convex functions are continuous) I think I'm missing the point of the question. Surely, by the mean value theorem we can find some $y\in (x,c)$ such that $f'(y)=\frac{f(x)-f(c)}{x-c}$ setting $m=f'(y)$ we get $m=\frac{f(x)-f(c)}{x-c}, m(x-c)+f(c)=f(x)$ such an $m$ fits the inequality we're asked to show. I also cannot see why $f'(c)$ would be the unique solution or why we would ever get a unique solution of the inequality at all for that matter. Clearly I'm misunderstanding the question, so if someone could point out where I would appreciate that. Thank you","A function $f : I \rightarrow \mathbb R$ on an interval $I$ is convex   if $f((1-t)x+ty)\le (1-t)f(x)+tf(y), \forall x,y \in I, t \in [0,1]$ Assume now that $I$ is an open interval.  Show that if $f$ is convex   then for each $c \in I$ there exists $m \in \mathbb R$ such that $m(x  − c) + f(c) \le f(x)$ for all $x \in I$ , and if in addition $f$ is   differentiable at $c$ then $f'(c)$ is the unique $m$ that works. In   general, must m be unique? (we have previously shown that convex functions are continuous) I think I'm missing the point of the question. Surely, by the mean value theorem we can find some $y\in (x,c)$ such that $f'(y)=\frac{f(x)-f(c)}{x-c}$ setting $m=f'(y)$ we get $m=\frac{f(x)-f(c)}{x-c}, m(x-c)+f(c)=f(x)$ such an $m$ fits the inequality we're asked to show. I also cannot see why $f'(c)$ would be the unique solution or why we would ever get a unique solution of the inequality at all for that matter. Clearly I'm misunderstanding the question, so if someone could point out where I would appreciate that. Thank you",,"['real-analysis', 'functional-analysis', 'functions']"
1,Proving $\lim_{x\to1}(x^3+5x^2-2)=4$ using the $\epsilon$-$\delta$ definition of a limit,Proving  using the - definition of a limit,\lim_{x\to1}(x^3+5x^2-2)=4 \epsilon \delta,"I want to prove that the limit of $f(x)=x^3+5x^2-2$ when $x\to 1$ is $4$.  So, I want to show that for any $\epsilon >0$ $\exists \delta_{\epsilon}$ such that for all $x$ that satisfies $|x-1|<\delta$ then $|f(x)-4|< \epsilon$.  Thus, $|x^3+5x^2 - 2 - 4|<|x^3+5x^2|<|x^2(x+5)|=x^2|x+5|<x^2|x-1|<\epsilon.$ And we know that $x^2|x-1|<x^2\delta<\epsilon $. As such, for every $\epsilon >0$ the corresponding $\delta$ is $\min\{\epsilon/x^2, \epsilon\}, x\neq0$. Is it right?","I want to prove that the limit of $f(x)=x^3+5x^2-2$ when $x\to 1$ is $4$.  So, I want to show that for any $\epsilon >0$ $\exists \delta_{\epsilon}$ such that for all $x$ that satisfies $|x-1|<\delta$ then $|f(x)-4|< \epsilon$.  Thus, $|x^3+5x^2 - 2 - 4|<|x^3+5x^2|<|x^2(x+5)|=x^2|x+5|<x^2|x-1|<\epsilon.$ And we know that $x^2|x-1|<x^2\delta<\epsilon $. As such, for every $\epsilon >0$ the corresponding $\delta$ is $\min\{\epsilon/x^2, \epsilon\}, x\neq0$. Is it right?",,"['calculus', 'real-analysis', 'proof-verification']"
2,A strong version of the Dominated Convergence Theorem,A strong version of the Dominated Convergence Theorem,,"Let $(X, \Sigma, \mu)$  be a measure space, and let $f, f_n:X\rightarrow \mathbb{C}$ be measurable functions with $f_n\rightarrow f$ pointwise. Assume that there are integrable functions $G, g_n:X\rightarrow[0, \infty]$ with finite integrals such that $|f_n|\leq G+g_n$ for every $n$. Also assume that $\int g_n \rightarrow 0$. We need to show that $\int |f-f_n|\rightarrow 0$. I've tried using the dominated convergence theorem but couldn't find the dominating function. I have 2 main ""problems"" with the statement: $\underset{n}{\sup}  g_n$ doesn't necessarily have a finite intgeral; $g_n$ doesn't have to converge pointwise to anything. That's what makes this statement different from all the others in this site I've checked.","Let $(X, \Sigma, \mu)$  be a measure space, and let $f, f_n:X\rightarrow \mathbb{C}$ be measurable functions with $f_n\rightarrow f$ pointwise. Assume that there are integrable functions $G, g_n:X\rightarrow[0, \infty]$ with finite integrals such that $|f_n|\leq G+g_n$ for every $n$. Also assume that $\int g_n \rightarrow 0$. We need to show that $\int |f-f_n|\rightarrow 0$. I've tried using the dominated convergence theorem but couldn't find the dominating function. I have 2 main ""problems"" with the statement: $\underset{n}{\sup}  g_n$ doesn't necessarily have a finite intgeral; $g_n$ doesn't have to converge pointwise to anything. That's what makes this statement different from all the others in this site I've checked.",,"['real-analysis', 'measure-theory', 'lebesgue-integral']"
3,"Is $\bigcup_{x \in A} [x - 1, x + 1]$ Lebesgue measurable, where $A$ is a Lebesgue measurable subset of $\mathbb{R}$?","Is  Lebesgue measurable, where  is a Lebesgue measurable subset of ?","\bigcup_{x \in A} [x - 1, x + 1] A \mathbb{R}","Suppose $A$ is a Lebesgue measurable subset of $\mathbb{R}$ and $$B = \bigcup_{x \in A} [x - 1, x + 1].$$Is $B$ Lebesgue measurable?","Suppose $A$ is a Lebesgue measurable subset of $\mathbb{R}$ and $$B = \bigcup_{x \in A} [x - 1, x + 1].$$Is $B$ Lebesgue measurable?",,['real-analysis']
4,How to prove this statement? (Real analysis),How to prove this statement? (Real analysis),,"This might be the basic question in real analysis. A function $f$   is $ C^2 $ function on the closed interval$ [0,1]$ Also the function $ f $ is satisfying $ f(0) = f(1) =0 $ Plus, $\vert f''(x) \vert  \le \  A $ on the open interval $(0,1)$ Show $\vert f'(x) \vert  \le \frac  A2 $  on the interval $(0,1]$ I tried many times through the Rolle's thm, Mean value thm etc. But failed. Please give me some hints.","This might be the basic question in real analysis. A function $f$   is $ C^2 $ function on the closed interval$ [0,1]$ Also the function $ f $ is satisfying $ f(0) = f(1) =0 $ Plus, $\vert f''(x) \vert  \le \  A $ on the open interval $(0,1)$ Show $\vert f'(x) \vert  \le \frac  A2 $  on the interval $(0,1]$ I tried many times through the Rolle's thm, Mean value thm etc. But failed. Please give me some hints.",,"['real-analysis', 'derivatives']"
5,Doubt in Baby Rudin Theorem 6.17 (Riemann-Stieltjes integral),Doubt in Baby Rudin Theorem 6.17 (Riemann-Stieltjes integral),,"I have a doubt in one step of the proof of the following theorem in Rudin's Principles of Mathematical Analysis Rudin proceeds to prove this as follows: Given $\epsilon>0, \exists$ a partition P such that $$U(P,\alpha')-L(P,\alpha') < \epsilon$$ Let $M=sup|f(x)|$ He then proves that $$ |U(P,f,\alpha)-U(P,f\alpha')| \leq M\epsilon$$ Also, he has shown that the above is true for any refinement of $P$ as well. It is clear to me till here. But then he 'concludes' that I fail to understand how this follows from the the fact that the inequality for $P$ is also true for any refinement of $P$. Please help!","I have a doubt in one step of the proof of the following theorem in Rudin's Principles of Mathematical Analysis Rudin proceeds to prove this as follows: Given $\epsilon>0, \exists$ a partition P such that $$U(P,\alpha')-L(P,\alpha') < \epsilon$$ Let $M=sup|f(x)|$ He then proves that $$ |U(P,f,\alpha)-U(P,f\alpha')| \leq M\epsilon$$ Also, he has shown that the above is true for any refinement of $P$ as well. It is clear to me till here. But then he 'concludes' that I fail to understand how this follows from the the fact that the inequality for $P$ is also true for any refinement of $P$. Please help!",,"['real-analysis', 'integration']"
6,Showing that $\sigma=\prod_{n=1}^{\infty}(n!)^{\frac{1}{2^{n+1}}}$,Showing that,\sigma=\prod_{n=1}^{\infty}(n!)^{\frac{1}{2^{n+1}}},Somos's quadratic recurrence constant The Somos's Quadratic recurrence constant is defined by the sequence $g_n=ng_{n-1}$ with initial value of $ g_0= 1$ The value of $\sigma=1.661687...$ An infinite product from maths world $\sigma=\prod_{k=1}^{\infty}k^{\frac{1}{2^k}}$ We found another infinite product involving the factorial numbers by experiments on a sum calculator. $$\sigma=\prod_{n=1}^{\infty}(n!)^{\frac{1}{2^{n+1}}}$$ Where n! is valid for non-negative integers and defined by $n!=n(n-1)(n-2)\cdots2\cdot1$ Can somebody help us to prove this,Somos's quadratic recurrence constant The Somos's Quadratic recurrence constant is defined by the sequence $g_n=ng_{n-1}$ with initial value of $ g_0= 1$ The value of $\sigma=1.661687...$ An infinite product from maths world $\sigma=\prod_{k=1}^{\infty}k^{\frac{1}{2^k}}$ We found another infinite product involving the factorial numbers by experiments on a sum calculator. $$\sigma=\prod_{n=1}^{\infty}(n!)^{\frac{1}{2^{n+1}}}$$ Where n! is valid for non-negative integers and defined by $n!=n(n-1)(n-2)\cdots2\cdot1$ Can somebody help us to prove this,,"['calculus', 'real-analysis']"
7,$\epsilon-\delta$ definition for limits involving $\infty$,definition for limits involving,\epsilon-\delta \infty,"For a general limit the $\epsilon-\delta$ definition of a limit (the formal definition of a limit) states that $$\lim_{x \ \to \  a} f(x) = L \Leftrightarrow \forall \epsilon > 0 \ (\exists \  \delta > 0  \ : \  (0<|x-a|<\delta \implies |f(x)-L| < \epsilon))$$ However the $\epsilon -\delta$ definition of a limit changes for limits as $x \to +\infty$ and $x \to  -\infty$, and it changes again for limits that evaluate to $+\infty$ and $-\infty$ 1. Limit as $x \to +\infty$ $$\lim_{x \ \to \  +\infty} f(x) = L \Leftrightarrow    \forall \ \epsilon>0\; (\exists \ \delta : (\;x>\delta\implies |f(x) - L|\leq\epsilon)) $$ 2. Limit as $x \to -\infty$ $$\lim_{x \ \to \  -\infty} f(x) = L \Leftrightarrow    \forall \ \epsilon>0\; (\exists \ \delta : (\;x<\delta\implies |f(x) - L|\leq\epsilon)) $$ 3. Limit evaluating to +$\infty$ $$\lim_{x \ \to \  a} f(x) = +\infty \Leftrightarrow \forall M  > 0 \ (\exists \  \delta > 0  \ : \ (0<|x-a|<\delta \implies f(x) >M)$$ 4. Limit evaluating to -$\infty$ $$\lim_{x \ \to \  a} f(x) = -\infty \Leftrightarrow \forall  N  < 0 \ (\exists \  \delta > 0  \ : \ (0<|x-a|<\delta \implies f(x)  < N)$$ But why is that so? I understand that if you use the normal $\epsilon-\delta$ definition of a limit in these cases, you get contradictions that pop up like $0 < |x-\infty| < \delta \implies \delta > \infty$, which you cannot do anything further with. While some of these differences might be subtle, it just seems counter-intuitive to change a formal and general definition. I know that the fundamental idea behind the $\epsilon - \delta$ definition remains the same throughout all of these examples (that no matter how small we want to make our ""error distance"" ($\epsilon$) we can always find a ""distance to our limit point"" ($\delta$) that satisfies the definition of a limit) , but to get to that fundamental idea, the $\epsilon - \delta$ definition has to be subtly modified (and I'm not referring to modifications in notation) for each of these examples. Or is it just a case that the $\epsilon - \delta$ definition for the general limit I put at the very start of this post is not as general as I thought? Furthermore, how does the $\epsilon - \delta$ definition of a limit change for these cases, (the formal definitions of these don't seem to be covered in any introductory Calculus textbook). 5. Limit as $x \to +\infty$ $= +\infty$ $$ \lim_{x \ \to \ +\infty} f(x) = +\infty \Leftrightarrow \ ???$$ 6. Limit as $x \to -\infty$ $= -\infty$ $$ \lim_{x \ \to \ -\infty} f(x) = -\infty \Leftrightarrow \ ???$$ 7. Limit as $x \to -\infty$ $= +\infty$ $$ \lim_{x \ \to \ -\infty} f(x) = +\infty \Leftrightarrow \ ???$$ 8. Limit as $x \to +\infty$ $= -\infty$ $$ \lim_{x \ \to \ +\infty} f(x) = -\infty \Leftrightarrow \ ???$$","For a general limit the $\epsilon-\delta$ definition of a limit (the formal definition of a limit) states that $$\lim_{x \ \to \  a} f(x) = L \Leftrightarrow \forall \epsilon > 0 \ (\exists \  \delta > 0  \ : \  (0<|x-a|<\delta \implies |f(x)-L| < \epsilon))$$ However the $\epsilon -\delta$ definition of a limit changes for limits as $x \to +\infty$ and $x \to  -\infty$, and it changes again for limits that evaluate to $+\infty$ and $-\infty$ 1. Limit as $x \to +\infty$ $$\lim_{x \ \to \  +\infty} f(x) = L \Leftrightarrow    \forall \ \epsilon>0\; (\exists \ \delta : (\;x>\delta\implies |f(x) - L|\leq\epsilon)) $$ 2. Limit as $x \to -\infty$ $$\lim_{x \ \to \  -\infty} f(x) = L \Leftrightarrow    \forall \ \epsilon>0\; (\exists \ \delta : (\;x<\delta\implies |f(x) - L|\leq\epsilon)) $$ 3. Limit evaluating to +$\infty$ $$\lim_{x \ \to \  a} f(x) = +\infty \Leftrightarrow \forall M  > 0 \ (\exists \  \delta > 0  \ : \ (0<|x-a|<\delta \implies f(x) >M)$$ 4. Limit evaluating to -$\infty$ $$\lim_{x \ \to \  a} f(x) = -\infty \Leftrightarrow \forall  N  < 0 \ (\exists \  \delta > 0  \ : \ (0<|x-a|<\delta \implies f(x)  < N)$$ But why is that so? I understand that if you use the normal $\epsilon-\delta$ definition of a limit in these cases, you get contradictions that pop up like $0 < |x-\infty| < \delta \implies \delta > \infty$, which you cannot do anything further with. While some of these differences might be subtle, it just seems counter-intuitive to change a formal and general definition. I know that the fundamental idea behind the $\epsilon - \delta$ definition remains the same throughout all of these examples (that no matter how small we want to make our ""error distance"" ($\epsilon$) we can always find a ""distance to our limit point"" ($\delta$) that satisfies the definition of a limit) , but to get to that fundamental idea, the $\epsilon - \delta$ definition has to be subtly modified (and I'm not referring to modifications in notation) for each of these examples. Or is it just a case that the $\epsilon - \delta$ definition for the general limit I put at the very start of this post is not as general as I thought? Furthermore, how does the $\epsilon - \delta$ definition of a limit change for these cases, (the formal definitions of these don't seem to be covered in any introductory Calculus textbook). 5. Limit as $x \to +\infty$ $= +\infty$ $$ \lim_{x \ \to \ +\infty} f(x) = +\infty \Leftrightarrow \ ???$$ 6. Limit as $x \to -\infty$ $= -\infty$ $$ \lim_{x \ \to \ -\infty} f(x) = -\infty \Leftrightarrow \ ???$$ 7. Limit as $x \to -\infty$ $= +\infty$ $$ \lim_{x \ \to \ -\infty} f(x) = +\infty \Leftrightarrow \ ???$$ 8. Limit as $x \to +\infty$ $= -\infty$ $$ \lim_{x \ \to \ +\infty} f(x) = -\infty \Leftrightarrow \ ???$$",,"['calculus', 'real-analysis', 'limits', 'infinity', 'epsilon-delta']"
8,What polytopes can be induced by a norm?,What polytopes can be induced by a norm?,,"Let $\|\cdot\|:\mathbb{R}^n\to\mathbb{R}_{\ge0}$ be a norm and define $B_{\|\cdot\|}(0,1):=\{x\in\mathbb{R}^n\mid\|x\|\le1\}$ be the unit circle. For which regular n-dimensional polytopes (relative to the Euclidean norm resp. the Euclidean product) there exists a norm such that $B_{\|\cdot\|}(0,1)$ is equal to the polytope given? For $n=3$ we can easily obtain a cube (wit the max norm) and an oktahedra (with $\|\cdot\|_1$) but can we find a norm withe the unit circle of a tetrahedra? What other forms are possible? I came up with this question purely out of curiosity, but I am quite clueless how to approach it. So any thoughts are appreciated.","Let $\|\cdot\|:\mathbb{R}^n\to\mathbb{R}_{\ge0}$ be a norm and define $B_{\|\cdot\|}(0,1):=\{x\in\mathbb{R}^n\mid\|x\|\le1\}$ be the unit circle. For which regular n-dimensional polytopes (relative to the Euclidean norm resp. the Euclidean product) there exists a norm such that $B_{\|\cdot\|}(0,1)$ is equal to the polytope given? For $n=3$ we can easily obtain a cube (wit the max norm) and an oktahedra (with $\|\cdot\|_1$) but can we find a norm withe the unit circle of a tetrahedra? What other forms are possible? I came up with this question purely out of curiosity, but I am quite clueless how to approach it. So any thoughts are appreciated.",,"['calculus', 'real-analysis', 'geometry']"
9,Approximating Borel Measure with Atomic Measures,Approximating Borel Measure with Atomic Measures,,"I see some posts that are related to this one, e.g. Borel Measures: Atoms (Summary) I have a sort of particular question: I have one professor saying the following is true, while another says it's false.  I think it's true.  Neither have a justification haha Suppose we take a Borel probability measure on a compact Euclidean space.  Then can we approximate it by uniform atomic measures in the weak*-topology of Borel probability measures on X? Specifically, if $\mu$ is a Borel probability measure on the compact Euclidean $X$, then does there exist a sequence of (not necessarily distinct!) points $( x_i )$ in X so that $$\frac{1}{n} \sum_{i = 1}^n \delta_{x_i} \xrightarrow{weak^*} \mu$$ where $\delta_{x_i}$ is the Dirac measure at $x_i$?  The reason I think it's true is that we know this measure has a density by the Lebesgue Density Theorem.","I see some posts that are related to this one, e.g. Borel Measures: Atoms (Summary) I have a sort of particular question: I have one professor saying the following is true, while another says it's false.  I think it's true.  Neither have a justification haha Suppose we take a Borel probability measure on a compact Euclidean space.  Then can we approximate it by uniform atomic measures in the weak*-topology of Borel probability measures on X? Specifically, if $\mu$ is a Borel probability measure on the compact Euclidean $X$, then does there exist a sequence of (not necessarily distinct!) points $( x_i )$ in X so that $$\frac{1}{n} \sum_{i = 1}^n \delta_{x_i} \xrightarrow{weak^*} \mu$$ where $\delta_{x_i}$ is the Dirac measure at $x_i$?  The reason I think it's true is that we know this measure has a density by the Lebesgue Density Theorem.",,"['real-analysis', 'probability-theory', 'measure-theory', 'dynamical-systems', 'ergodic-theory']"
10,Show that the integral of a positive function is positive,Show that the integral of a positive function is positive,,"Suppose $f:[0,1] \to (0,\infty)$ is a Riemann integrable function. Prove that the integral of the function from $0$ to $1$ is strictly positive. I have been trying to do this for awhile but I can't seem to get it. Here is my thought process: If the function is Riemann integrable, then its set of discontinuities has measure zero (Lebesgue). I am not sure how to connect this to the integral being positive","Suppose $f:[0,1] \to (0,\infty)$ is a Riemann integrable function. Prove that the integral of the function from $0$ to $1$ is strictly positive. I have been trying to do this for awhile but I can't seem to get it. Here is my thought process: If the function is Riemann integrable, then its set of discontinuities has measure zero (Lebesgue). I am not sure how to connect this to the integral being positive",,"['real-analysis', 'riemann-integration']"
11,Defining weak* convergence of measures using compactly supported continuous functions,Defining weak* convergence of measures using compactly supported continuous functions,,"I'm reading some lecture notes and the author defines the following: Let $\mu_{n},\mu$   be probability measures on $\left(\mathbb{R}^{k},\mathcal{B}\left(\mathbb{R}^{k}\right)\right)$  , we say $\mu_{n}$   converge weakly to $\mu$   if $\int fd\mu_{n}\longrightarrow\int fd\mu$   for all continuous compactly supported functions $f:\mathbb{R}^{k}\to\mathbb{R}$  . An almost identical definition appears in many text books with the change of requiring the same thing for any continuous bounded function. I couldn't find any reference which showed that it actually does suffice to look only at compactly supported functions. Is this actually true?","I'm reading some lecture notes and the author defines the following: Let $\mu_{n},\mu$   be probability measures on $\left(\mathbb{R}^{k},\mathcal{B}\left(\mathbb{R}^{k}\right)\right)$  , we say $\mu_{n}$   converge weakly to $\mu$   if $\int fd\mu_{n}\longrightarrow\int fd\mu$   for all continuous compactly supported functions $f:\mathbb{R}^{k}\to\mathbb{R}$  . An almost identical definition appears in many text books with the change of requiring the same thing for any continuous bounded function. I couldn't find any reference which showed that it actually does suffice to look only at compactly supported functions. Is this actually true?",,"['real-analysis', 'probability-theory', 'measure-theory']"
12,Product measure and independence,Product measure and independence,,What's the difference between product measure and independence in probability? $$ (\mu_1\times\mu_2)(B_1\times B_2)= \mu_1(B_1)\mu_2(B_2) $$ $$ P(A_m\cap A_k)= P(A_m)P(A_k) $$ Are product measure it self independent?,What's the difference between product measure and independence in probability? Are product measure it self independent?,"
(\mu_1\times\mu_2)(B_1\times B_2)= \mu_1(B_1)\mu_2(B_2)
 
P(A_m\cap A_k)= P(A_m)P(A_k)
","['real-analysis', 'probability']"
13,Does differentiability imply having bounded variation on some subinterval?,Does differentiability imply having bounded variation on some subinterval?,,"Suppose that $f:(a,b)\to\mathbb{R}$ is a differentiable function. Does it follow that $f$ has bounded variation on some subinterval $[c,d]\subset (a,b)$? Details and ideas Being differentiable means only that $f'(x)$ exists for all $x\in (a,b)$. Continuity of $f'$ is not assumed. $f$ need not be of bounded variation on every subinterval $[c,d]\subset (a,b)$. For example, $f(x)=x^2 \sin x^{-2}$ is differentiable on $\mathbb{R}$ but has infinite variation on any interval containing $0$. One can add several copies of $f$ as above to create several points where variation blows up. But trying to add infinitely many of them, e.g., $\sum c_n f(x-q_n)$ with $q_n$ running over a dense set, appears likely to destroy differentiability somewhere. A post by Dave L. Renfro gives a list of bad properties that the derivative of a differentiable function may have, but I didn't find anything inconsistent with being BV on some subinterval. The question is motivated by this problem .","Suppose that $f:(a,b)\to\mathbb{R}$ is a differentiable function. Does it follow that $f$ has bounded variation on some subinterval $[c,d]\subset (a,b)$? Details and ideas Being differentiable means only that $f'(x)$ exists for all $x\in (a,b)$. Continuity of $f'$ is not assumed. $f$ need not be of bounded variation on every subinterval $[c,d]\subset (a,b)$. For example, $f(x)=x^2 \sin x^{-2}$ is differentiable on $\mathbb{R}$ but has infinite variation on any interval containing $0$. One can add several copies of $f$ as above to create several points where variation blows up. But trying to add infinitely many of them, e.g., $\sum c_n f(x-q_n)$ with $q_n$ running over a dense set, appears likely to destroy differentiability somewhere. A post by Dave L. Renfro gives a list of bad properties that the derivative of a differentiable function may have, but I didn't find anything inconsistent with being BV on some subinterval. The question is motivated by this problem .",,['real-analysis']
14,Riemann Integrability of Step Function,Riemann Integrability of Step Function,,"Call $f: [a,b] \to \mathbb{R}$ a step function if there exist a partition $P=\{x_0, \ldots, x_n \}$ of $[a,b]$ such that $f$ is constant on the interval $[x_i, x_{i+1})$. I'm having some issues proving that f is Riemann integrable. If $f$ was defined such that it was constant on the closed interval $[x_i, x_{i+1}]$ then it would be trivial to define a partition such that the supremum and infinum coincide, and the upper and lower sums would cancel. But since we are working with a half-open interval, I'm having issues defining the partitions that will allow me to prove integrability. All proofs I've seen online rely on theorems we haven't proven in class, so I'm assuming I must be missing something simple.","Call $f: [a,b] \to \mathbb{R}$ a step function if there exist a partition $P=\{x_0, \ldots, x_n \}$ of $[a,b]$ such that $f$ is constant on the interval $[x_i, x_{i+1})$. I'm having some issues proving that f is Riemann integrable. If $f$ was defined such that it was constant on the closed interval $[x_i, x_{i+1}]$ then it would be trivial to define a partition such that the supremum and infinum coincide, and the upper and lower sums would cancel. But since we are working with a half-open interval, I'm having issues defining the partitions that will allow me to prove integrability. All proofs I've seen online rely on theorems we haven't proven in class, so I'm assuming I must be missing something simple.",,"['real-analysis', 'integration', 'riemann-sum', 'riemann-integration']"
15,$f \in C^1$ defined on a compact set $K$ is Lipschitz?,defined on a compact set  is Lipschitz?,f \in C^1 K,"Let $f: \Omega \subseteq \mathbb{R}^N \to \mathbb{R}^M$ be $C^1$ , and $K \subseteq \Omega$ . Prove that $f \mid_K$ is Lipschitz. Letting $x,y \in K$ , I know that $f$ is locally Lipschitz, I thought about taking a finite subcover of $K$ by  the open cover of the balls which makes $f$ locally Lipschitz, and then take a sequence of $x_k, k=0,\dots, n$ in such that $x_0 =x$ and $x_n=y$ , $x_k =y$ and $(x_i)$ are aligned, then try to use $|f(x)- f(y)| \le \sum_{i=1}^{n}|f(x_{i=1})- f(x_i)|$ . But the very first problem is to take this sequence as $K$ is not necessary convex. Again, if $K$ was convex I would make use of the Mean Value Inequality and that $K$ is compact to limit the value of $||Df_{p}||$ , but as it is not the case I don't know how to proceed. So how can I avoid that? Does it hold for a general $K$ compact? Finally, I am aware of this question , but his question, at the end, was only concerning the continuity, which I already know how to deal with there was no useful answer, nor comment for me, I apologise if I acted wrong by opening a new one.","Let be , and . Prove that is Lipschitz. Letting , I know that is locally Lipschitz, I thought about taking a finite subcover of by  the open cover of the balls which makes locally Lipschitz, and then take a sequence of in such that and , and are aligned, then try to use . But the very first problem is to take this sequence as is not necessary convex. Again, if was convex I would make use of the Mean Value Inequality and that is compact to limit the value of , but as it is not the case I don't know how to proceed. So how can I avoid that? Does it hold for a general compact? Finally, I am aware of this question , but his question, at the end, was only concerning the continuity, which I already know how to deal with there was no useful answer, nor comment for me, I apologise if I acted wrong by opening a new one.","f: \Omega \subseteq \mathbb{R}^N \to \mathbb{R}^M C^1 K \subseteq \Omega f \mid_K x,y \in K f K f x_k, k=0,\dots, n x_0 =x x_n=y x_k =y (x_i) |f(x)- f(y)| \le \sum_{i=1}^{n}|f(x_{i=1})- f(x_i)| K K K ||Df_{p}|| K","['real-analysis', 'derivatives', 'lipschitz-functions']"
16,calculate the the limit of the sequence $a_n = \lim_{n \to \infty} n^\frac{2}{3}\cdot ( \sqrt{n-1} + \sqrt{n+1} -2\sqrt{n} )$,calculate the the limit of the sequence,a_n = \lim_{n \to \infty} n^\frac{2}{3}\cdot ( \sqrt{n-1} + \sqrt{n+1} -2\sqrt{n} ),"Iv'e been struggling with this one for a bit too long: $$ a_n = \lim_{n \to \infty} n^\frac{2}{3}\cdot ( \sqrt{n-1} + \sqrt{n+1} -2\sqrt{n} )$$ What Iv'e tried so far was using the fact that the inner expression is equivalent to that: $$ a_n = \lim_{n \to \infty} n^\frac{2}{3}\cdot ( \sqrt{n-1}-\sqrt{n} + \sqrt{n+1} -\sqrt{n} ) $$ Then I tried multiplying each of the expression by their conjugate and got: $$ a_n = \lim_{n \to \infty} n^\frac{2}{3}\cdot ( \frac{1}{\sqrt{n+1} +\sqrt{n}} - \frac{1}{\sqrt{n-1} +\sqrt{n}} ) $$ But now I'm in a dead end. Since I have this annyoing $n^\frac{2}{3}$ outside of the brackets, each of my attemps to finalize this, ends up with the undefined expression of $(\infty\cdot0)$ I've thought about using the squeeze theorem some how, but didn't manage to connect the dots right. Thanks.","Iv'e been struggling with this one for a bit too long: $$ a_n = \lim_{n \to \infty} n^\frac{2}{3}\cdot ( \sqrt{n-1} + \sqrt{n+1} -2\sqrt{n} )$$ What Iv'e tried so far was using the fact that the inner expression is equivalent to that: $$ a_n = \lim_{n \to \infty} n^\frac{2}{3}\cdot ( \sqrt{n-1}-\sqrt{n} + \sqrt{n+1} -\sqrt{n} ) $$ Then I tried multiplying each of the expression by their conjugate and got: $$ a_n = \lim_{n \to \infty} n^\frac{2}{3}\cdot ( \frac{1}{\sqrt{n+1} +\sqrt{n}} - \frac{1}{\sqrt{n-1} +\sqrt{n}} ) $$ But now I'm in a dead end. Since I have this annyoing $n^\frac{2}{3}$ outside of the brackets, each of my attemps to finalize this, ends up with the undefined expression of $(\infty\cdot0)$ I've thought about using the squeeze theorem some how, but didn't manage to connect the dots right. Thanks.",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
17,Is my proof of the uniqueness of $0$ non-circular?,Is my proof of the uniqueness of  non-circular?,0,"Please try to avoid jumping directly the proof, the text before it is crucial to my question as well. I had a proof of this here , but I have come to realize that the proof is circular since I implied the result in all $3$ lemmas. I also believe that the one liner given in a comment to that question ($e_1=e_1+e_2=e_2$) also relies on some assumptions which were not given in the text. So allow me to write exactly what is given, exactly as written from the text (Apostol's ""Mathematical Analysis""): Definition of addition and multiplication: Along with the set R of real numbers we assume the existence of two operations, called addition and multiplication , such that for every pair of real numbers $x$ and $y$ the sum $x+y$ and the product $xy$ are real numbers satisfying the following axioms. (In the axioms that appeat below, $x, y, z$ represent arbitrary real numbers unless something is said to the contrary) Axiom 1: Commutative Laws $x+y=y+x$, $xy=yx$ Axiom 2: Associative Laws $x+(y+z)=(x+y)+z$, $x(yz)=(xy)z$ Axiom 3: Distributive Law $x(y+z)=xy+yz$ Axiom 4: Given any two real numbers $x$ and $y$, there exists a real number $z$ such that $x+z=y$. This $z$ is denoted by $y-x$; the number $x-x$ is denoted by $0$ (it can be proved that $0$ is independent of $x$.) We write $-x$ for $0-x$ and call $-x$ the negative of $x$. Please Note: I will denote the number $x-x$ as $0_x$ and $y-y$ as $0_y$.  We are not given $x + 0 = x$ or $x + (-x) = 0$ in the general sense (which would imply a piece of information that is not given in the text about $0$). What I mean by that is that we are given that $x + (x-x)=x$, but we are not given that $x+(y-y)=x$. And we are given $x + ((x-x)-x) = x-x$, but we are not given $x + ((y-y)-x) = x-x$. The only piece of information we have about $0$ is that it is the symbol which denotes $x-x$, the number which when added to $x$ results in $x$. Before I start the actual proof, I want to make another note; the way I understand it, the ""uniqueness of $0$"" can have at least $2$ different meanings: $1)$ The number $0_x$ that satisfies $x + 0_x = x$ is unique $2)$ The number $0_x$ is the same as $0_y$ I believe the first meaning of uniqueness follows from the definition of $z$ in axiom $4$, as the wording in axiom $4$ seems to imply that $z$ in axiom $4$ is unique (please let me know if I am correct/incorrect here). Also, were are not given $y-x=y+(-x)$; $y-x$ is just a symbol for the number $z$ in axiom $4$. So now I am trying to prove the second meaning of uniqueness (this proof is similar to the one in the linked question, only hopefully witouth circular assumptions): Lemma: If $x+z=y+z$, then $x=y$ Let $x=y$ Add $z$ to both sides $x+z=y+z$. Therefore, if $x+z=y+z$, then $x=y$ I think the proof of this lemma is correct only if we assume that addition is an operation which maps two real numbers to one unique real number. Is this a fair assumption to make from the definition of addition given above? Is there a way to prove this lemma without this assumption? Proof that $0_x=0_y$ So just a refresher on the definition, $0_x=x-x$ and $0_y=y-y$ $x + 0_x = x$ $y + 0_y = y$ By axiom $4$, we are guaranteed that there exists a (unique?) $z$ such that $x + z = y$, so we will replace $y$ by $x + z$. $x +z + 0_y = x+z$ By associative and commutative laws, $(x + 0_y)+z = (x)+z$ By the lemma, $(x + 0_y) = (x)$ but $(x + 0_x) = (x)$ And if meaning number $1$ of uniqueness given above is true, then $0_x = 0_y$","Please try to avoid jumping directly the proof, the text before it is crucial to my question as well. I had a proof of this here , but I have come to realize that the proof is circular since I implied the result in all $3$ lemmas. I also believe that the one liner given in a comment to that question ($e_1=e_1+e_2=e_2$) also relies on some assumptions which were not given in the text. So allow me to write exactly what is given, exactly as written from the text (Apostol's ""Mathematical Analysis""): Definition of addition and multiplication: Along with the set R of real numbers we assume the existence of two operations, called addition and multiplication , such that for every pair of real numbers $x$ and $y$ the sum $x+y$ and the product $xy$ are real numbers satisfying the following axioms. (In the axioms that appeat below, $x, y, z$ represent arbitrary real numbers unless something is said to the contrary) Axiom 1: Commutative Laws $x+y=y+x$, $xy=yx$ Axiom 2: Associative Laws $x+(y+z)=(x+y)+z$, $x(yz)=(xy)z$ Axiom 3: Distributive Law $x(y+z)=xy+yz$ Axiom 4: Given any two real numbers $x$ and $y$, there exists a real number $z$ such that $x+z=y$. This $z$ is denoted by $y-x$; the number $x-x$ is denoted by $0$ (it can be proved that $0$ is independent of $x$.) We write $-x$ for $0-x$ and call $-x$ the negative of $x$. Please Note: I will denote the number $x-x$ as $0_x$ and $y-y$ as $0_y$.  We are not given $x + 0 = x$ or $x + (-x) = 0$ in the general sense (which would imply a piece of information that is not given in the text about $0$). What I mean by that is that we are given that $x + (x-x)=x$, but we are not given that $x+(y-y)=x$. And we are given $x + ((x-x)-x) = x-x$, but we are not given $x + ((y-y)-x) = x-x$. The only piece of information we have about $0$ is that it is the symbol which denotes $x-x$, the number which when added to $x$ results in $x$. Before I start the actual proof, I want to make another note; the way I understand it, the ""uniqueness of $0$"" can have at least $2$ different meanings: $1)$ The number $0_x$ that satisfies $x + 0_x = x$ is unique $2)$ The number $0_x$ is the same as $0_y$ I believe the first meaning of uniqueness follows from the definition of $z$ in axiom $4$, as the wording in axiom $4$ seems to imply that $z$ in axiom $4$ is unique (please let me know if I am correct/incorrect here). Also, were are not given $y-x=y+(-x)$; $y-x$ is just a symbol for the number $z$ in axiom $4$. So now I am trying to prove the second meaning of uniqueness (this proof is similar to the one in the linked question, only hopefully witouth circular assumptions): Lemma: If $x+z=y+z$, then $x=y$ Let $x=y$ Add $z$ to both sides $x+z=y+z$. Therefore, if $x+z=y+z$, then $x=y$ I think the proof of this lemma is correct only if we assume that addition is an operation which maps two real numbers to one unique real number. Is this a fair assumption to make from the definition of addition given above? Is there a way to prove this lemma without this assumption? Proof that $0_x=0_y$ So just a refresher on the definition, $0_x=x-x$ and $0_y=y-y$ $x + 0_x = x$ $y + 0_y = y$ By axiom $4$, we are guaranteed that there exists a (unique?) $z$ such that $x + z = y$, so we will replace $y$ by $x + z$. $x +z + 0_y = x+z$ By associative and commutative laws, $(x + 0_y)+z = (x)+z$ By the lemma, $(x + 0_y) = (x)$ but $(x + 0_x) = (x)$ And if meaning number $1$ of uniqueness given above is true, then $0_x = 0_y$",,"['real-analysis', 'analysis', 'proof-verification', 'axioms']"
18,"Find the derived set of $\{\frac{1}{n} + \frac{1}{m}: m,n \in \mathbb{N}\}$ and prove it is such.",Find the derived set of  and prove it is such.,"\{\frac{1}{n} + \frac{1}{m}: m,n \in \mathbb{N}\}",It is easy to see the derived set is $A' = \{\frac{1}{n}: n \in \mathbb{N}\}\bigcup\{0\}$. To prove these are the only elements of the derived set we need to show that the shape of the derived set can only be $\frac{1}{n}$ or $0$. We can see the derived set is bounded above by $1$ and below by $0$. So we look for points between $0$ and $\frac{1}{N}$ where $N$ is the largest number and $\frac{1}{n}$ and $\frac{1}{n+1}$. I know I need to show that only a finite number of points exist between these points but I am having trouble doing so.,It is easy to see the derived set is $A' = \{\frac{1}{n}: n \in \mathbb{N}\}\bigcup\{0\}$. To prove these are the only elements of the derived set we need to show that the shape of the derived set can only be $\frac{1}{n}$ or $0$. We can see the derived set is bounded above by $1$ and below by $0$. So we look for points between $0$ and $\frac{1}{N}$ where $N$ is the largest number and $\frac{1}{n}$ and $\frac{1}{n+1}$. I know I need to show that only a finite number of points exist between these points but I am having trouble doing so.,,"['real-analysis', 'general-topology']"
19,"Is there a sequence of real numbers such that the set of its subsequential limits is $[0,1]$?",Is there a sequence of real numbers such that the set of its subsequential limits is ?,"[0,1]","Is there a sequence of real numbers such that the set of its subsequential limits is $[0,1]$? I've been considering the sequence: $$1,~~\frac12,~~\frac22,~~\frac32,~~\frac13,~~\frac23,~~\frac33,~~\frac43,~\ldots~,~~\frac1n,~~\frac2n,~~\frac3n,~\ldots~,~~\frac{n+1}{n},~\ldots$$ Does it satisfy the condition?","Is there a sequence of real numbers such that the set of its subsequential limits is $[0,1]$? I've been considering the sequence: $$1,~~\frac12,~~\frac22,~~\frac32,~~\frac13,~~\frac23,~~\frac33,~~\frac43,~\ldots~,~~\frac1n,~~\frac2n,~~\frac3n,~\ldots~,~~\frac{n+1}{n},~\ldots$$ Does it satisfy the condition?",,"['real-analysis', 'sequences-and-series']"
20,"Evaluate the limit of $(n+1)\int_0^1x^n\ln(1+x)\,dx$ when $n\to\infty$",Evaluate the limit of  when,"(n+1)\int_0^1x^n\ln(1+x)\,dx n\to\infty","Evaluate the following limit : $$\lim_{n\to \infty}\left[(n+1)\int_0^1x^n\ln(1+x)\,dx\right].$$ We have , $$\lim_{n\to \infty}\left[(n+1)\int_0^1x^n\ln(1+x)\,dx\right]$$ $$=\lim_{n\to \infty}\int_0^1\ln(1+x)\,d(x^{n+1})$$Now put , $x^{n+1}=y$. Then , $$=\lim_{n\to \infty}\int_0^1\ln\left(1+y^{\frac{1}{n+1}}\right)\,dy$$Let , $\displaystyle g_n(y)=\ln\left(1+y^{\frac{1}{n+1}}\right)$. Edit  : Then , $\displaystyle g(y)=\lim_ng_n(y)=\ln 2$ in $(0,1]$. Now , $\displaystyle \sup_{x\in (0,1]}|g_n(y)-g(y)|=\sup_{x\in (0,1]}\ln\left(\frac{1+y^{\frac{1}{n+1}}}{2}\right)=0$. ( As , $y^{\frac{1}{n+1}}$ is monotone increasing function in $(0,1]$ , so $\ln\left(\frac{1+y^{\frac{1}{n+1}}}{2}\right)$ is also monotone increasing in $(0,1]$ and so it attains its maximum value at $y=1$ . ) So , $\{g_n(y)\}$ converges uniformly to $\ln 2$. Then ,we can show that $g_n(y)$ converges uniformly to $\ln 2$ in $(0,1]$ and hence the given limit is $\ln 2$. Is this correct ? Does there any other technique to evaluate the limit ?","Evaluate the following limit : $$\lim_{n\to \infty}\left[(n+1)\int_0^1x^n\ln(1+x)\,dx\right].$$ We have , $$\lim_{n\to \infty}\left[(n+1)\int_0^1x^n\ln(1+x)\,dx\right]$$ $$=\lim_{n\to \infty}\int_0^1\ln(1+x)\,d(x^{n+1})$$Now put , $x^{n+1}=y$. Then , $$=\lim_{n\to \infty}\int_0^1\ln\left(1+y^{\frac{1}{n+1}}\right)\,dy$$Let , $\displaystyle g_n(y)=\ln\left(1+y^{\frac{1}{n+1}}\right)$. Edit  : Then , $\displaystyle g(y)=\lim_ng_n(y)=\ln 2$ in $(0,1]$. Now , $\displaystyle \sup_{x\in (0,1]}|g_n(y)-g(y)|=\sup_{x\in (0,1]}\ln\left(\frac{1+y^{\frac{1}{n+1}}}{2}\right)=0$. ( As , $y^{\frac{1}{n+1}}$ is monotone increasing function in $(0,1]$ , so $\ln\left(\frac{1+y^{\frac{1}{n+1}}}{2}\right)$ is also monotone increasing in $(0,1]$ and so it attains its maximum value at $y=1$ . ) So , $\{g_n(y)\}$ converges uniformly to $\ln 2$. Then ,we can show that $g_n(y)$ converges uniformly to $\ln 2$ in $(0,1]$ and hence the given limit is $\ln 2$. Is this correct ? Does there any other technique to evaluate the limit ?",,"['real-analysis', 'limits', 'definite-integrals']"
21,Product $\sigma$-algebra in countable case (Proposition 1.3 in Folland),Product -algebra in countable case (Proposition 1.3 in Folland),\sigma,"In the real analysis, by Folland , p. 23: I know $\prod_{\alpha\in A}E_{\alpha}=\bigcap_{\alpha\in A}\pi_{\alpha}^{-1}(E_{\alpha})$. But I cannot figure out why the product $\sigma$-algebra in the countable case should be defined in $\bigcap_{\alpha\in A}\pi_{\alpha}^{-1}(E_{\alpha})$. And I have no idea of ""The result therefore follows from Lemma 1.1"". The following is the general definition of product $\sigma$-algebra (on p.22) (It does not say anything about the intersection): The following is the Lemma 1.1: I can understand this Lemma, however, what does this Lemma relate to that proposition?","In the real analysis, by Folland , p. 23: I know $\prod_{\alpha\in A}E_{\alpha}=\bigcap_{\alpha\in A}\pi_{\alpha}^{-1}(E_{\alpha})$. But I cannot figure out why the product $\sigma$-algebra in the countable case should be defined in $\bigcap_{\alpha\in A}\pi_{\alpha}^{-1}(E_{\alpha})$. And I have no idea of ""The result therefore follows from Lemma 1.1"". The following is the general definition of product $\sigma$-algebra (on p.22) (It does not say anything about the intersection): The following is the Lemma 1.1: I can understand this Lemma, however, what does this Lemma relate to that proposition?",,['real-analysis']
22,Proof that the following sum is bounded above.,Proof that the following sum is bounded above.,,"Let  $i_1,  i_2 , ... , i_n, ...$ be a sequence of positive integers such that, a) No $i_n$ is a prime. b) For all pairs of distinct positive integers, $m$ and $n$, the pair of integers $i_m$ and $i_n$ are relatively prime. Show that $\frac{1}{i_1}  + \frac{1}{i_2}+ ... + \frac{1}{i_n} + ...$ is bounded above by some   finite real number. I know that all $i_k$ are $> 1$ for every $k$, and $q_k$ the smallest prime that divides $i_k$(Thanks Andre for this hint). I try several ways (more calculus related, infinite series) but all my attempts end in a failure. Therefore I can't find such a finite real number that its bounded the sum of my problem. For me is a very hard problem, and I would really appreciate if I receive help solving this exercise, and then been able to understand the path of thinking of how to solve such a kind of problem.  Thanks again community.","Let  $i_1,  i_2 , ... , i_n, ...$ be a sequence of positive integers such that, a) No $i_n$ is a prime. b) For all pairs of distinct positive integers, $m$ and $n$, the pair of integers $i_m$ and $i_n$ are relatively prime. Show that $\frac{1}{i_1}  + \frac{1}{i_2}+ ... + \frac{1}{i_n} + ...$ is bounded above by some   finite real number. I know that all $i_k$ are $> 1$ for every $k$, and $q_k$ the smallest prime that divides $i_k$(Thanks Andre for this hint). I try several ways (more calculus related, infinite series) but all my attempts end in a failure. Therefore I can't find such a finite real number that its bounded the sum of my problem. For me is a very hard problem, and I would really appreciate if I receive help solving this exercise, and then been able to understand the path of thinking of how to solve such a kind of problem.  Thanks again community.",,"['calculus', 'real-analysis', 'sequences-and-series', 'number-theory']"
23,Show that sup$AB$=(sup$A$)(sup$B$),Show that sup=(sup)(sup),AB A B,"Where $AB$ is the product of the sets and $A,B \in \mathbb{R^+}$. Since $A,B$ are bounded above sup $A$ and sup $B$ exist. Let $\alpha = $ sup $A$ and $\beta = $ sup $B$. This implies $\forall a \in A$ and $\forall b \in B$ $a \leq \alpha$ and $b \leq \beta$. Then $ab \leq \alpha\beta$ because $a,b > 0$. Thus $ab$ is bounded above and sup $AB$ exists and sup $AB \leq \alpha\beta$. \     We now show sup $AB \geq \alpha\beta$. \     Let $\varepsilon > 0$ then $\exists a \in A$ s.t. $\alpha - \varepsilon < a \leq \alpha$ and $\exists b \in B$ s.t. $\beta - \varepsilon < b \leq \beta$. So:     \begin{equation*}       (\alpha-\varepsilon)(\beta-\varepsilon) < ab \leq \alpha\beta \text{ since } a,b,\varepsilon > 0     \end{equation*}     \begin{equation*}       = \alpha\beta-\varepsilon(\alpha+\beta-\varepsilon) < ab \leq \alpha\beta     \end{equation*} I spoke with my professor today about this and he suggested I show that $\varepsilon$ is sufficiently small to proceed. I'm not sure exactly how to write this detail. EDIT: I was meant to show what $\varepsilon$ was bounded by to proceed. The proof below realizes this idea. Feedback is welcome and appreciated. Since $A,B$ are bounded above sup $A$ and sup $B$ exist. Let $\alpha = $ sup $A$ and $\beta = $ sup $B$. This implies $\forall a \in A$ and $\forall b \in B$ $a \leq \alpha$ and $b \leq \beta$. Then $ab \leq \alpha\beta$ because $a,b > 0$. Thus $ab$ is bounded above, sup $AB$ exists and sup $AB \leq \alpha\beta$.     Let $\varepsilon > 0$ then $\exists a \in A$ s.t. $\alpha - \varepsilon < a \leq \alpha$ and $\exists b \in B$ s.t. $\beta - \varepsilon < b \leq \beta$. So:     \begin{equation*}       (\alpha-\varepsilon)(\beta-\varepsilon) < ab \leq \alpha\beta     \end{equation*}     \begin{equation*}       = \alpha\beta-(\varepsilon\alpha+\varepsilon\beta-\varepsilon^2) < ab \leq \alpha\beta     \end{equation*}     Since $ab$ is bounded above by $\alpha\beta$ we have $ab \leq \text{ sup}(AB)$. We let $\varepsilon' = \varepsilon\alpha+\varepsilon\beta-\varepsilon^2 > 0 $ so $\forall(0 < \varepsilon' < \alpha+\beta)$ we have $\alpha\beta-\varepsilon'< ab < \text{ sup}(AB) \implies \alpha\beta \leq \text{ sup}(AB) + \varepsilon' \implies \alpha\beta \leq \text{ sup}(AB)$ by ``elbow room''.","Where $AB$ is the product of the sets and $A,B \in \mathbb{R^+}$. Since $A,B$ are bounded above sup $A$ and sup $B$ exist. Let $\alpha = $ sup $A$ and $\beta = $ sup $B$. This implies $\forall a \in A$ and $\forall b \in B$ $a \leq \alpha$ and $b \leq \beta$. Then $ab \leq \alpha\beta$ because $a,b > 0$. Thus $ab$ is bounded above and sup $AB$ exists and sup $AB \leq \alpha\beta$. \     We now show sup $AB \geq \alpha\beta$. \     Let $\varepsilon > 0$ then $\exists a \in A$ s.t. $\alpha - \varepsilon < a \leq \alpha$ and $\exists b \in B$ s.t. $\beta - \varepsilon < b \leq \beta$. So:     \begin{equation*}       (\alpha-\varepsilon)(\beta-\varepsilon) < ab \leq \alpha\beta \text{ since } a,b,\varepsilon > 0     \end{equation*}     \begin{equation*}       = \alpha\beta-\varepsilon(\alpha+\beta-\varepsilon) < ab \leq \alpha\beta     \end{equation*} I spoke with my professor today about this and he suggested I show that $\varepsilon$ is sufficiently small to proceed. I'm not sure exactly how to write this detail. EDIT: I was meant to show what $\varepsilon$ was bounded by to proceed. The proof below realizes this idea. Feedback is welcome and appreciated. Since $A,B$ are bounded above sup $A$ and sup $B$ exist. Let $\alpha = $ sup $A$ and $\beta = $ sup $B$. This implies $\forall a \in A$ and $\forall b \in B$ $a \leq \alpha$ and $b \leq \beta$. Then $ab \leq \alpha\beta$ because $a,b > 0$. Thus $ab$ is bounded above, sup $AB$ exists and sup $AB \leq \alpha\beta$.     Let $\varepsilon > 0$ then $\exists a \in A$ s.t. $\alpha - \varepsilon < a \leq \alpha$ and $\exists b \in B$ s.t. $\beta - \varepsilon < b \leq \beta$. So:     \begin{equation*}       (\alpha-\varepsilon)(\beta-\varepsilon) < ab \leq \alpha\beta     \end{equation*}     \begin{equation*}       = \alpha\beta-(\varepsilon\alpha+\varepsilon\beta-\varepsilon^2) < ab \leq \alpha\beta     \end{equation*}     Since $ab$ is bounded above by $\alpha\beta$ we have $ab \leq \text{ sup}(AB)$. We let $\varepsilon' = \varepsilon\alpha+\varepsilon\beta-\varepsilon^2 > 0 $ so $\forall(0 < \varepsilon' < \alpha+\beta)$ we have $\alpha\beta-\varepsilon'< ab < \text{ sup}(AB) \implies \alpha\beta \leq \text{ sup}(AB) + \varepsilon' \implies \alpha\beta \leq \text{ sup}(AB)$ by ``elbow room''.",,"['real-analysis', 'proof-verification', 'proof-writing', 'supremum-and-infimum']"
24,"If $f$ is integrable on $[0,1]$, and $\lim_{x\to 0^+}f(x)$ exists, compute $\lim_{x\to 0^{+}}x\int_x^1 \frac{f(t)}{t^2}dt$.","If  is integrable on , and  exists, compute .","f [0,1] \lim_{x\to 0^+}f(x) \lim_{x\to 0^{+}}x\int_x^1 \frac{f(t)}{t^2}dt","If $f$ is integrable on $[0,1]$, and $\lim_{x\to 0}f(x)$ exists, compute $\lim_{x\to 0^{+}}x\int_x^1 \frac{f(t)}{t^2}dt$. I'm lost about what the value is for this limit in the first place. How can I make a guess for this kind of limit?","If $f$ is integrable on $[0,1]$, and $\lim_{x\to 0}f(x)$ exists, compute $\lim_{x\to 0^{+}}x\int_x^1 \frac{f(t)}{t^2}dt$. I'm lost about what the value is for this limit in the first place. How can I make a guess for this kind of limit?",,"['calculus', 'real-analysis', 'analysis']"
25,Show $\Bbb R ^n$ with a finite set removed is still connected.,Show  with a finite set removed is still connected.,\Bbb R ^n,"The question is: Show that if $A⊆\Bbb R^n$ is finite and $n≥2$, then the set $\Bbb R^n\backslash A$ is connected in $(\Bbb R^n,τ_d)$ where $τ_d$ is the topology induced by the Euclidean distance in $\Bbb R^n$. My attempt: Denote $A^C=\Bbb R^n\backslash A$ and suppose $A=\{a_1,…,a_n\}$. Assume by contradiction that $A^C$ is not connected, then there exists $O_1,O_2∈τ_{A^C }$ st. $A^C=O_1⋃O_2,O_1⋂O_2=∅$. Here $\tau_{A^C}$ is a subspace topology. Intuitively, part of $A$, denoted by $A_1$ is ""enclosed"" in $O_1$ and the rest, denoted by $A_2$ are ""enclosed"" in $O_2$. Then $G_1=O_1\bigcup A_1$ is an open set in $\tau_d$, and $G_2=O_2\bigcup A_2$ is also an open set in $\tau_d$, and $\Bbb R^n=G_1\bigcup G_2$. However, $G_1, G_2$are disjoint, contradicting with the fact that $\Bbb R^n$ is connected. However I got stuck when trying to implement this idea. Can anyone provide some help? Thank you!","The question is: Show that if $A⊆\Bbb R^n$ is finite and $n≥2$, then the set $\Bbb R^n\backslash A$ is connected in $(\Bbb R^n,τ_d)$ where $τ_d$ is the topology induced by the Euclidean distance in $\Bbb R^n$. My attempt: Denote $A^C=\Bbb R^n\backslash A$ and suppose $A=\{a_1,…,a_n\}$. Assume by contradiction that $A^C$ is not connected, then there exists $O_1,O_2∈τ_{A^C }$ st. $A^C=O_1⋃O_2,O_1⋂O_2=∅$. Here $\tau_{A^C}$ is a subspace topology. Intuitively, part of $A$, denoted by $A_1$ is ""enclosed"" in $O_1$ and the rest, denoted by $A_2$ are ""enclosed"" in $O_2$. Then $G_1=O_1\bigcup A_1$ is an open set in $\tau_d$, and $G_2=O_2\bigcup A_2$ is also an open set in $\tau_d$, and $\Bbb R^n=G_1\bigcup G_2$. However, $G_1, G_2$are disjoint, contradicting with the fact that $\Bbb R^n$ is connected. However I got stuck when trying to implement this idea. Can anyone provide some help? Thank you!",,"['real-analysis', 'general-topology', 'connectedness']"
26,Looking for a direct proof of the following exercise,Looking for a direct proof of the following exercise,,"A friend of mine told me about the following problem: Let $\{r_n\}$ be a sequence of rational numbers such that $\lim_{n\to\infty}r_n=x\in\Bbb R,$ $r_n\neq x,$ for every $n\in\Bbb N$ and $r_n=\dfrac{a_n}{b_n},$ for each $n\in\Bbb N,$ where $\{a_n\}$ is a sequence of integers and $\{b_n\}$ is a sequence of positive integers. Prove that $\lim_{n\to\infty}b_n=+\infty.$ I proved such result by contradiction: If $\lim_{n\to\infty}b_n\neq+\infty,$ then $\exists M\in\Bbb R$ such that $\forall N\in\Bbb N,$ there exists some $n\geq N$ such that $b_n<M.$ Therefore, we can construct a subsequence $\{b_{m_k}\}$ of $\{b_n\}$ as follows: First $\exists m_0\geq0$ such that $b_{m_0}<M.$ Having chosen $m_1,\ldots,m_p,$ let $m_{p+1}$ be such that $m_{p+1}>m_p$ and $b_{m_{p+1}}<M.$ Since $1\leq b_{m_k}<M,$ for every $k\in\Bbb N$ and $\{b_n\}$ is a sequence of positive integers, there must exist some constant subsequence $\{b_{n_k}\}$ of $\{b_n\}.$ Let $b$ be the positive integer such that $b_{n_k}=b$ for every $k\in\Bbb N$ and let $\delta>0$ be arbitrary. Since $\{r_n\}$ converges to $x,$ then $\{r_{n_k}\}$ converges to $x$ and hence, there is some natural $N_0$ such that $$0<\left|\dfrac{a_{n_k}}{b}-x\right|<\dfrac{\delta}{b},$$ for each $k\geq N_0.$ Then $$0<\left|a_{n_k}-bx\right|<\delta,$$ for every $k\geq N_0.$ Since $\delta$ is arbitrary, this means that $\lim_{k\to\infty}a_{n_k}=bx.$ Therefore $\exists N_1\in\Bbb N$ and $\exists a\in\Bbb Z$ such that $a_{n_k}=a,$ for every $k\geq N_1.$ Therefore, $r_{n_k}=\dfrac{a}{b},$ for sufficiently large $k,$ which contradicts the fact that $r_n\neq x,$ for every $n\in\Bbb N.$ My questions are , is there a direct proof of the exercise? or is there an elegant solution? and, is the idea of my proof correct?","A friend of mine told me about the following problem: Let $\{r_n\}$ be a sequence of rational numbers such that $\lim_{n\to\infty}r_n=x\in\Bbb R,$ $r_n\neq x,$ for every $n\in\Bbb N$ and $r_n=\dfrac{a_n}{b_n},$ for each $n\in\Bbb N,$ where $\{a_n\}$ is a sequence of integers and $\{b_n\}$ is a sequence of positive integers. Prove that $\lim_{n\to\infty}b_n=+\infty.$ I proved such result by contradiction: If $\lim_{n\to\infty}b_n\neq+\infty,$ then $\exists M\in\Bbb R$ such that $\forall N\in\Bbb N,$ there exists some $n\geq N$ such that $b_n<M.$ Therefore, we can construct a subsequence $\{b_{m_k}\}$ of $\{b_n\}$ as follows: First $\exists m_0\geq0$ such that $b_{m_0}<M.$ Having chosen $m_1,\ldots,m_p,$ let $m_{p+1}$ be such that $m_{p+1}>m_p$ and $b_{m_{p+1}}<M.$ Since $1\leq b_{m_k}<M,$ for every $k\in\Bbb N$ and $\{b_n\}$ is a sequence of positive integers, there must exist some constant subsequence $\{b_{n_k}\}$ of $\{b_n\}.$ Let $b$ be the positive integer such that $b_{n_k}=b$ for every $k\in\Bbb N$ and let $\delta>0$ be arbitrary. Since $\{r_n\}$ converges to $x,$ then $\{r_{n_k}\}$ converges to $x$ and hence, there is some natural $N_0$ such that $$0<\left|\dfrac{a_{n_k}}{b}-x\right|<\dfrac{\delta}{b},$$ for each $k\geq N_0.$ Then $$0<\left|a_{n_k}-bx\right|<\delta,$$ for every $k\geq N_0.$ Since $\delta$ is arbitrary, this means that $\lim_{k\to\infty}a_{n_k}=bx.$ Therefore $\exists N_1\in\Bbb N$ and $\exists a\in\Bbb Z$ such that $a_{n_k}=a,$ for every $k\geq N_1.$ Therefore, $r_{n_k}=\dfrac{a}{b},$ for sufficiently large $k,$ which contradicts the fact that $r_n\neq x,$ for every $n\in\Bbb N.$ My questions are , is there a direct proof of the exercise? or is there an elegant solution? and, is the idea of my proof correct?",,"['real-analysis', 'sequences-and-series', 'proof-verification', 'alternative-proof']"
27,Question about required rigour with intro real analysis text,Question about required rigour with intro real analysis text,,"I have just begun trying to self study introductory analysis and I am just having some questions about being specific on rigour. In the book I am using, titled Introduction to Real Analysis, 4th edition by Robert G. Bartle and Donald R. Sherbert. The second chapter begins with listing the properties of $\mathbb{R}$ and then does a bunch of the basic proofs of the properties of reals using just the given axioms. My question is, it mentions that It is worth noting that no smallest positive real number can exist, in fact if $$a \gt 0$$ then we have $$0 \lt \frac{1}{2}a \lt a$$    (why?) Now, of course I understand that is true but I am just wanting to only use justifications. And I am wondering how we can say that $\frac{1}{2}a \lt a$, I mean, I am not sure that it defines any sort of rule that ordered the numbers like that or some sort of division or order of rationals. How can that result come from Trichototmy? I know that since $a \gt 0$ and since it can be shown that if $a \gt 0$ then $a^{2} \gt 0$ (because if $a \in P$ then $a(a) \in P$) so that settles that, and also that $\frac{1}{2} \gt 0$ since $\pm \frac{\sqrt2}{2} \in \mathbb{R}$ hence $\frac{1}{2} \gt 0$ and so $(1/2)a$ is greater then zero, but how to conclude that $(1/2)a$ is ordered less than $a$? Is there any source where I can see how the rationals are ordered anyways (but I would still be interested in how from these principals it can be concluded). I hope it makes sense, maybe I am missing something or being very naive, but hopefully someone gets what I am trying to say. Thanks a lot","I have just begun trying to self study introductory analysis and I am just having some questions about being specific on rigour. In the book I am using, titled Introduction to Real Analysis, 4th edition by Robert G. Bartle and Donald R. Sherbert. The second chapter begins with listing the properties of $\mathbb{R}$ and then does a bunch of the basic proofs of the properties of reals using just the given axioms. My question is, it mentions that It is worth noting that no smallest positive real number can exist, in fact if $$a \gt 0$$ then we have $$0 \lt \frac{1}{2}a \lt a$$    (why?) Now, of course I understand that is true but I am just wanting to only use justifications. And I am wondering how we can say that $\frac{1}{2}a \lt a$, I mean, I am not sure that it defines any sort of rule that ordered the numbers like that or some sort of division or order of rationals. How can that result come from Trichototmy? I know that since $a \gt 0$ and since it can be shown that if $a \gt 0$ then $a^{2} \gt 0$ (because if $a \in P$ then $a(a) \in P$) so that settles that, and also that $\frac{1}{2} \gt 0$ since $\pm \frac{\sqrt2}{2} \in \mathbb{R}$ hence $\frac{1}{2} \gt 0$ and so $(1/2)a$ is greater then zero, but how to conclude that $(1/2)a$ is ordered less than $a$? Is there any source where I can see how the rationals are ordered anyways (but I would still be interested in how from these principals it can be concluded). I hope it makes sense, maybe I am missing something or being very naive, but hopefully someone gets what I am trying to say. Thanks a lot",,"['real-analysis', 'real-numbers']"
28,"Convergent $x_n,y_n$ and $x_n^{y_n}$ diverges",Convergent  and  diverges,"x_n,y_n x_n^{y_n}","Let $x_n, y_n > 0$. I need an example of convergent sequences $x_n,y_n$ such that $x_n^{y_n}$ diverges. Could you help me?","Let $x_n, y_n > 0$. I need an example of convergent sequences $x_n,y_n$ such that $x_n^{y_n}$ diverges. Could you help me?",,"['real-analysis', 'sequences-and-series']"
29,"Show that if $f$ has compact support, then its Fourier transform cannot have compact support unless $f=0$. [closed]","Show that if  has compact support, then its Fourier transform cannot have compact support unless . [closed]",f f=0,"Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 7 years ago . Improve this question Let $f\in$$L^1(\mathbb{R})$ and $g(y)=\int_{\mathbb{R}}f(x)e^{-iyx}dx$. Show that if $f$ has compact support, then $g$ can not have compact support unless $f=0$. First, I assume that $g$ has compact support. Since $f$ has compact support, there is an interval $I$ and a nonnegative integer $n$ such that I has length $2n\pi$ and $g(y)=\int_{\mathbb{R}}f(x)e^{-iyx}dx=\int_{I}f(x)e^{-iyx}dx$. Also, since $g$ has compact support, there is a positive integer $N$ such that $g(k)=0$ for all integer $k$ with $N\leq|k|$. This shows that the Fourrier coefficients $c_k$[$f$] of $f$ on $I$ is equal to zero if $N\leq|k|$. If I can show that  $c_k$[$f$]$=0$ for $|k|<N$, then this question can be done. But I don't know how to show that $c_k$[$f$]$=0$ for $|k|<N$.","Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 7 years ago . Improve this question Let $f\in$$L^1(\mathbb{R})$ and $g(y)=\int_{\mathbb{R}}f(x)e^{-iyx}dx$. Show that if $f$ has compact support, then $g$ can not have compact support unless $f=0$. First, I assume that $g$ has compact support. Since $f$ has compact support, there is an interval $I$ and a nonnegative integer $n$ such that I has length $2n\pi$ and $g(y)=\int_{\mathbb{R}}f(x)e^{-iyx}dx=\int_{I}f(x)e^{-iyx}dx$. Also, since $g$ has compact support, there is a positive integer $N$ such that $g(k)=0$ for all integer $k$ with $N\leq|k|$. This shows that the Fourrier coefficients $c_k$[$f$] of $f$ on $I$ is equal to zero if $N\leq|k|$. If I can show that  $c_k$[$f$]$=0$ for $|k|<N$, then this question can be done. But I don't know how to show that $c_k$[$f$]$=0$ for $|k|<N$.",,['real-analysis']
30,It is possible to demonstrate the taylor's formula (Peano) with the taylor's formula (Lagrange)?,It is possible to demonstrate the taylor's formula (Peano) with the taylor's formula (Lagrange)?,,I wonder if it is possible demonstrate the taylor's formula in the peano form of the reminder with the taylor's formula in the lagrange formula of the reminder. In symbols: \begin{equation}f(x)-F(x)=\frac{f(\xi)}{n!}(x-x_0)^n\implies f(x)-F(x)=o(x-x_0)^n \end{equation} with F(x) the taylor's series. I've tried different times but I have not been able to demonstrate.,I wonder if it is possible demonstrate the taylor's formula in the peano form of the reminder with the taylor's formula in the lagrange formula of the reminder. In symbols: \begin{equation}f(x)-F(x)=\frac{f(\xi)}{n!}(x-x_0)^n\implies f(x)-F(x)=o(x-x_0)^n \end{equation} with F(x) the taylor's series. I've tried different times but I have not been able to demonstrate.,,"['calculus', 'real-analysis', 'analysis', 'taylor-expansion']"
31,the series: compute $ \sum_{n=1}^{\infty}\frac{1}{(4n^2-1)^4} $,the series: compute, \sum_{n=1}^{\infty}\frac{1}{(4n^2-1)^4} ,"Compute $$ \sum_{n=1}^{\infty}\frac{1}{(4n^2-1)^4} $$ the result is $\frac{\pi^4+30\pi^2-384}{768}$, so I'm sure the sums $\sum\frac{1}{n^2}$ and $\sum\frac{1}{n^4}$ should appear in the solution. The standard method $\frac{1}{4n^2-1}=\frac{1}{2}(\frac{1}{2n-1}-\frac{1}{2n+1})$ allows to compute $\sum_{n=1}^{\infty}\frac{1}{4n^2-1}=\frac{1}{2}$ and $\sum_{n=1}^{\infty}\frac{1}{(4n^2-1)^2}=\frac{\pi^2-8}{16}$, what about higher powers?","Compute $$ \sum_{n=1}^{\infty}\frac{1}{(4n^2-1)^4} $$ the result is $\frac{\pi^4+30\pi^2-384}{768}$, so I'm sure the sums $\sum\frac{1}{n^2}$ and $\sum\frac{1}{n^4}$ should appear in the solution. The standard method $\frac{1}{4n^2-1}=\frac{1}{2}(\frac{1}{2n-1}-\frac{1}{2n+1})$ allows to compute $\sum_{n=1}^{\infty}\frac{1}{4n^2-1}=\frac{1}{2}$ and $\sum_{n=1}^{\infty}\frac{1}{(4n^2-1)^2}=\frac{\pi^2-8}{16}$, what about higher powers?",,"['real-analysis', 'sequences-and-series']"
32,How can I calculate $\lim_{n \to \infty} (1 + \frac{1}{n!})^n$ and $\lim_{n \to \infty} (1 + \frac{1}{n!})^{n^n}$?,How can I calculate  and ?,\lim_{n \to \infty} (1 + \frac{1}{n!})^n \lim_{n \to \infty} (1 + \frac{1}{n!})^{n^n},"How do you calculate the following limits? $$\lim_{n \to \infty} \left(1 + \frac{1}{n!}\right)^n$$ $$\lim_{n \to \infty} \left(1  + \frac{1}{n!}\right)^{n^n}.$$ I really don't have any clue about how to proceed: I know the famous limit that defines $e$ ($\lim_{n \to \infty} \left(1 + \frac{1}{n}\right)^n=e$), but the factorials (and the exponent of the second one) here throw me off. Any ideas?","How do you calculate the following limits? $$\lim_{n \to \infty} \left(1 + \frac{1}{n!}\right)^n$$ $$\lim_{n \to \infty} \left(1  + \frac{1}{n!}\right)^{n^n}.$$ I really don't have any clue about how to proceed: I know the famous limit that defines $e$ ($\lim_{n \to \infty} \left(1 + \frac{1}{n}\right)^n=e$), but the factorials (and the exponent of the second one) here throw me off. Any ideas?",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'exponential-function']"
33,$ |f''(x)+2xf'(x)+(x^2+1)f(x)|\leq1 $ for all $x$. Prove $ \lim_{x\rightarrow\infty }f(x)=0$,for all . Prove, |f''(x)+2xf'(x)+(x^2+1)f(x)|\leq1  x  \lim_{x\rightarrow\infty }f(x)=0,"Let $f(x):(0, \infty)\rightarrow \mathbb{R} $ be a twice continuously  differentiable function such that | $ f''(x)+2xf'(x)+(x^2+1)f(x) |\leq1 $ for all $x$. Prove $  \lim_{x\rightarrow\infty }f(x)=0$ I think this can be solved by applying  L'Hospital  rule twice on $\frac{e^{x^2}f(x)}{e^{x^2}}$. My problem is, Is it possible to apply the rule without knowing about numerator . Also I like to see different types of proofs","Let $f(x):(0, \infty)\rightarrow \mathbb{R} $ be a twice continuously  differentiable function such that | $ f''(x)+2xf'(x)+(x^2+1)f(x) |\leq1 $ for all $x$. Prove $  \lim_{x\rightarrow\infty }f(x)=0$ I think this can be solved by applying  L'Hospital  rule twice on $\frac{e^{x^2}f(x)}{e^{x^2}}$. My problem is, Is it possible to apply the rule without knowing about numerator . Also I like to see different types of proofs",,[]
34,How to show $c_n=\frac11 + \frac12 + \cdots + \frac1n - \ln n$ is a sequence of positive numbers? [duplicate],How to show  is a sequence of positive numbers? [duplicate],c_n=\frac11 + \frac12 + \cdots + \frac1n - \ln n,"This question already has answers here : Showing that $\sum_{i=1}^n \frac{1}{i} \geq \log{n}$ (2 answers) Closed 9 years ago . For  $n \in \mathbb{N}$ let  $c_{n}$ be defined by $$c_{n}=\frac{1}{1}  + \frac{1}{2} + \cdots + \frac{1}{n} - \ln n$$     We have to prove that $c_{n}$ is a decreasing sequence of positive numbers. I've already shown the first part, that it is a decreasing sequence by considering the difference of   $$c_{n+1}-c_{n} = \ln \left(1- \frac{1}{n+1} \right) +\frac{1}{n+1} $$ and then using the expansion of $\ln (1-x)$ for $-1\leq x \leq 1$. But I'm having some trouble in showing the second part that all terms in the sequence are positive. I tried using first form of induction but but stuck in the inductive step, can somebody please suggest explain that to me? or better suggest some other way to prove that part? Any sort of welcome as log as it leads to the solution, thanks in advance.","This question already has answers here : Showing that $\sum_{i=1}^n \frac{1}{i} \geq \log{n}$ (2 answers) Closed 9 years ago . For  $n \in \mathbb{N}$ let  $c_{n}$ be defined by $$c_{n}=\frac{1}{1}  + \frac{1}{2} + \cdots + \frac{1}{n} - \ln n$$     We have to prove that $c_{n}$ is a decreasing sequence of positive numbers. I've already shown the first part, that it is a decreasing sequence by considering the difference of   $$c_{n+1}-c_{n} = \ln \left(1- \frac{1}{n+1} \right) +\frac{1}{n+1} $$ and then using the expansion of $\ln (1-x)$ for $-1\leq x \leq 1$. But I'm having some trouble in showing the second part that all terms in the sequence are positive. I tried using first form of induction but but stuck in the inductive step, can somebody please suggest explain that to me? or better suggest some other way to prove that part? Any sort of welcome as log as it leads to the solution, thanks in advance.",,"['real-analysis', 'sequences-and-series', 'inequality', 'summation', 'harmonic-numbers']"
35,"A continuous bijection from the Cantor Set to [0,1]","A continuous bijection from the Cantor Set to [0,1]",,"If $C$ is the Cantor Set, I am asked to show that there exists a continuous bijection, say $f$, that maps $C \to [0,1]$. My best guess thus far has been the Cantor Function, however (using this construction ) it doesn't appear to me to be a bijection, specifically not injective. If this is the case, is it possible for me to modify the Cantor Function to get a bijection? Thanks in advance for any advice!","If $C$ is the Cantor Set, I am asked to show that there exists a continuous bijection, say $f$, that maps $C \to [0,1]$. My best guess thus far has been the Cantor Function, however (using this construction ) it doesn't appear to me to be a bijection, specifically not injective. If this is the case, is it possible for me to modify the Cantor Function to get a bijection? Thanks in advance for any advice!",,"['real-analysis', 'measure-theory']"
36,Covering the plane by squares!,Covering the plane by squares!,,"$K_n$ is a sequence of squares of area $a_n$. Show that if $\sum_{n=1}^\infty a_n=\infty$ then we can arrange the squares $K_n$ to cover $\mathbb{R}^2$. Comments: -obviously we can suppose that $a_n\to0$ as $n\to\infty$. -WLoG we can moreover suppose that $a_1\gt a_2\gt a_3\gt\ldots$. -That's enough to prove that we can cover unit square in the plane, because unit square is compact.","$K_n$ is a sequence of squares of area $a_n$. Show that if $\sum_{n=1}^\infty a_n=\infty$ then we can arrange the squares $K_n$ to cover $\mathbb{R}^2$. Comments: -obviously we can suppose that $a_n\to0$ as $n\to\infty$. -WLoG we can moreover suppose that $a_1\gt a_2\gt a_3\gt\ldots$. -That's enough to prove that we can cover unit square in the plane, because unit square is compact.",,"['real-analysis', 'sequences-and-series', 'general-topology', 'analysis', 'geometry']"
37,"If $\int f=0$ and $f(x) \ge 0$ for all $x \in \mathbb{R}^d$, then $f=0$ a.e.","If  and  for all , then  a.e.",\int f=0 f(x) \ge 0 x \in \mathbb{R}^d f=0,"If $\int f=0$ and $f(x) \ge 0$ for all $x \in \mathbb{R}^d$, then $f=0$ a.e. I let $E \subset \mathbb{R}^d$ be a finite measurable set. I try to break this into two cases: Case 1: If $f(x)=0$ on $E$, then we are done (as trivially $f=0$ a.e.). Case 2: If $f(x) > 0$ on $E$, then $\lim_{n \to \infty} \sum_{k=1}^n a_k m(E_k)=0$. I think I can prove by contradiction here. Can I assume $f \not= 0$, which means $f > 0$ in this case? Then, do I need to show that $\int_E f > 0$, which would contradict the hypothesis of $\int_E f = 0$. Is this a good approach to this?","If $\int f=0$ and $f(x) \ge 0$ for all $x \in \mathbb{R}^d$, then $f=0$ a.e. I let $E \subset \mathbb{R}^d$ be a finite measurable set. I try to break this into two cases: Case 1: If $f(x)=0$ on $E$, then we are done (as trivially $f=0$ a.e.). Case 2: If $f(x) > 0$ on $E$, then $\lim_{n \to \infty} \sum_{k=1}^n a_k m(E_k)=0$. I think I can prove by contradiction here. Can I assume $f \not= 0$, which means $f > 0$ in this case? Then, do I need to show that $\int_E f > 0$, which would contradict the hypothesis of $\int_E f = 0$. Is this a good approach to this?",,"['real-analysis', 'measure-theory', 'lebesgue-integral']"
38,Lebesgue integral of absolute value of sequence of functions [duplicate],Lebesgue integral of absolute value of sequence of functions [duplicate],,"This question already has answers here : If $f_k \to f$ a.e. and the $L^p$ norms converge, then $f_k \to f$ in $L^p$ (2 answers) Closed 9 years ago . I am working on a problem$^{(*)}$ on Lebesgue integral looks like this: Given that both $f_n$ and $f$ are integrable, $f_n \longrightarrow f$ a.e., and $\int|f_n| \longrightarrow \int |f|$. Show that    $$\int |f_n - f| \longrightarrow 0.$$ To me, the question ""makes sense"" since $\int |f_n - f| \longrightarrow 0$ implies $\lim_{n \to \infty} \int |f_n - f| = 0,$ and $|f_n - f|$ is approaching zero as $n$ is approaching infinity, therefore the integral is approaching zero. But I do not know how to say it mathematically. Any help or hints would be very much appreciated. Thanks for your time. (*) Richard F. Bass' Real Analysis , 2nd. edition, chapter 7: Limit Theorems, exercise 7.5, page 57.","This question already has answers here : If $f_k \to f$ a.e. and the $L^p$ norms converge, then $f_k \to f$ in $L^p$ (2 answers) Closed 9 years ago . I am working on a problem$^{(*)}$ on Lebesgue integral looks like this: Given that both $f_n$ and $f$ are integrable, $f_n \longrightarrow f$ a.e., and $\int|f_n| \longrightarrow \int |f|$. Show that    $$\int |f_n - f| \longrightarrow 0.$$ To me, the question ""makes sense"" since $\int |f_n - f| \longrightarrow 0$ implies $\lim_{n \to \infty} \int |f_n - f| = 0,$ and $|f_n - f|$ is approaching zero as $n$ is approaching infinity, therefore the integral is approaching zero. But I do not know how to say it mathematically. Any help or hints would be very much appreciated. Thanks for your time. (*) Richard F. Bass' Real Analysis , 2nd. edition, chapter 7: Limit Theorems, exercise 7.5, page 57.",,"['real-analysis', 'measure-theory', 'lebesgue-integral']"
39,Find $\lim_\limits{x\to 0}{x\left[1\over x\right]}$. Am I correct?,Find . Am I correct?,\lim_\limits{x\to 0}{x\left[1\over x\right]},"Find $\lim_\limits{x\to 0}{x\left[1\over x\right]}$ . Attempt : $\lim_\limits{x\to 0}{x\left[1\over x\right]}=\lim_\limits{x\to 0}x\left ({1\over x}-\{{1\over x}\}\right)=\lim_\limits{x\to 0}\left (1-{x\{{1\over x}\}}\right).$ Since $\{{1\over x}\}$ is bounded and $x\to 0$ , then $x\cdot\{{1\over x}\}\to 0$ and therefore $\lim_\limits{x\to 0}{x\left[1\over x\right]}=1.$","Find . Attempt : Since is bounded and , then and therefore",\lim_\limits{x\to 0}{x\left[1\over x\right]} \lim_\limits{x\to 0}{x\left[1\over x\right]}=\lim_\limits{x\to 0}x\left ({1\over x}-\{{1\over x}\}\right)=\lim_\limits{x\to 0}\left (1-{x\{{1\over x}\}}\right). \{{1\over x}\} x\to 0 x\cdot\{{1\over x}\}\to 0 \lim_\limits{x\to 0}{x\left[1\over x\right]}=1.,"['real-analysis', 'calculus', 'limits', 'solution-verification', 'ceiling-and-floor-functions']"
40,Prove that the Interior of the Boundary is Empty,Prove that the Interior of the Boundary is Empty,,"Suppose X is a Metric Space Let S $\subset X$ Prove that if S is Closed then, the Interior of the Boundary of S is Empty Totally stuck on how to solve this.","Suppose X is a Metric Space Let S $\subset X$ Prove that if S is Closed then, the Interior of the Boundary of S is Empty Totally stuck on how to solve this.",,[]
41,"Prove $\limsup_{n\to\infty}|\{(p,q)\in T\times T,p!q!=n\}|=6$",Prove,"\limsup_{n\to\infty}|\{(p,q)\in T\times T,p!q!=n\}|=6","Let $T$ be the set of nonnegative integers, I need to prove that $$\limsup_{n\to\infty}|\{(p,q)\in T\times T,p!q!=n\}|=6$$ It's really easy to show that $$\limsup_{n\to\infty}|\{(p,q)\in T\times T,p!q!=n\}|\ge6$$ since for $n>2$ $$(n!)!=0!(n!)!=1!(n!)!=n!(n!-1)!=(n!-1)!n!=(n!)!1!=(n!)!0!$$ So I'm looking for a way to prove the apposite inequality.","Let $T$ be the set of nonnegative integers, I need to prove that $$\limsup_{n\to\infty}|\{(p,q)\in T\times T,p!q!=n\}|=6$$ It's really easy to show that $$\limsup_{n\to\infty}|\{(p,q)\in T\times T,p!q!=n\}|\ge6$$ since for $n>2$ $$(n!)!=0!(n!)!=1!(n!)!=n!(n!-1)!=(n!-1)!n!=(n!)!1!=(n!)!0!$$ So I'm looking for a way to prove the apposite inequality.",,"['real-analysis', 'analysis']"
42,Limit $\lim_{n \rightarrow \infty} \frac {1^{a+1}+2^{a+1}+\cdots+n^{a+1}}{n.(1^{a }+2^{a }+\cdots+n^{a })}$,Limit,\lim_{n \rightarrow \infty} \frac {1^{a+1}+2^{a+1}+\cdots+n^{a+1}}{n.(1^{a }+2^{a }+\cdots+n^{a })},The value of $$\lim_{n=\infty} \dfrac {1^{a+1}+2^{a+1}+\cdots+n^{a+1}}{n.(1^{a }+2^{a }+\cdots+n^{a })}  $$ Attempt: $S = \lim_{n \rightarrow \infty} \sum_{n=0} ^\infty \dfrac {k^{a+1}} {n.( 1^{a }+2^{a }+\cdots+ n^{a} )}$ I am trying to use the integral as a limit of sum but the denominator is preventing me from applying. Could anyone please give me a direction to move on. Thank you for your help.,The value of $$\lim_{n=\infty} \dfrac {1^{a+1}+2^{a+1}+\cdots+n^{a+1}}{n.(1^{a }+2^{a }+\cdots+n^{a })}  $$ Attempt: $S = \lim_{n \rightarrow \infty} \sum_{n=0} ^\infty \dfrac {k^{a+1}} {n.( 1^{a }+2^{a }+\cdots+ n^{a} )}$ I am trying to use the integral as a limit of sum but the denominator is preventing me from applying. Could anyone please give me a direction to move on. Thank you for your help.,,"['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'summation']"
43,Matrix representation of shape operator,Matrix representation of shape operator,,"Let $f$ be a parametrized surface $f: \Omega \subset \mathbb{R}^2 \rightarrow \mathbb{R}^3$ and $N : \Omega \rightarrow Tf$ the Gauß map. Then the shape operator is defined as $L = -DN \circ Df^{-1}.$ Now the thing is that $Df$ is a $3 \times 2$ matrix, so I cannot invert this matrix easily. So how do I get a matrix representation for my shape operator? If anything is unclear, please let me know.","Let $f$ be a parametrized surface $f: \Omega \subset \mathbb{R}^2 \rightarrow \mathbb{R}^3$ and $N : \Omega \rightarrow Tf$ the Gauß map. Then the shape operator is defined as $L = -DN \circ Df^{-1}.$ Now the thing is that $Df$ is a $3 \times 2$ matrix, so I cannot invert this matrix easily. So how do I get a matrix representation for my shape operator? If anything is unclear, please let me know.",,"['real-analysis', 'linear-algebra']"
44,Uniform continuity and bounded variation does imply absolute continuity?,Uniform continuity and bounded variation does imply absolute continuity?,,I'm studying absolute continuity. We see that a absolute continuity implies uniform continuity and also see that a function which is uniform continuous but not bounded variation is not absolute continuous.So Uniform continuity and bounded variation does imply absolute continuity?,I'm studying absolute continuity. We see that a absolute continuity implies uniform continuity and also see that a function which is uniform continuous but not bounded variation is not absolute continuous.So Uniform continuity and bounded variation does imply absolute continuity?,,"['real-analysis', 'continuity']"
45,Convergence in measure implies convergence almost everywhere (on a countable set!),Convergence in measure implies convergence almost everywhere (on a countable set!),,"Here is an interesting problem from ""Real Analysis for Graduate Students"" by Richard Bass (which is an amazing book, by the way). Suppose $(X, \mathcal{A}, \mu)$ is a measure space, and $X$ is a countable set. Prove that if $f_n$ is a sequence of measurable functions converging to $f$ in measure, then $f_n$ also converges to $f$ a.e. (almost everywhere). This is Exercise 10.9 in page 79. I tried applying the theorem that if $f_n\to f$ in measure, then $f_n$ has a subsequence $\{f_{n_k}\}$ that converges to $f$ a.e. But I am having trouble using the fact that $X$ is a countable set. I would appreciate a hint in the right direction.","Here is an interesting problem from ""Real Analysis for Graduate Students"" by Richard Bass (which is an amazing book, by the way). Suppose $(X, \mathcal{A}, \mu)$ is a measure space, and $X$ is a countable set. Prove that if $f_n$ is a sequence of measurable functions converging to $f$ in measure, then $f_n$ also converges to $f$ a.e. (almost everywhere). This is Exercise 10.9 in page 79. I tried applying the theorem that if $f_n\to f$ in measure, then $f_n$ has a subsequence $\{f_{n_k}\}$ that converges to $f$ a.e. But I am having trouble using the fact that $X$ is a countable set. I would appreciate a hint in the right direction.",,"['real-analysis', 'measure-theory', 'convergence-divergence']"
46,Baire's property iff first category has dense complement.,Baire's property iff first category has dense complement.,,"Show that $(S, d)$ has Baire's property iff every set of first category has a dense complement. A set is of first category if it is a countable union of nowhere dense sets. First Category Baire's Lemma: Let $(X, \rho)$ be a complete metric space and $\{U_n\}_{n=1}^{\infty}$ a sequence of open dense sets in $X$. Then the set $\bigcap_{n =1}^{\infty} U_n$ is also dense. Note that A is nowhere dense iff $(\overline{A})^c$ is open and dense. Suppose $\forall A$, $A$ is first category. That is, $A$ is a countable union of nowhere dense sets. i.e., $A_i$ is nowhere dense. $\forall A, \; A = \bigcup_{n \in \mathbb{N}}A_i \implies (A)^c = \bigcap_{n \in \mathbb{N}}(A_{i})^{c}$. But we don't know if this is a countable dense set! We know that $(\overline{A}_i)^c$ would be, but not just $(A)^c$. If $A$ is nowhere dense, what can we say about the compliment of $A$? Can anyone clear this up or provide advice? More scratch work: In my proposed proof, I said let all $A \subset S$ be of first category. i.e., each $A_i$ is nowhere dense, i.e, $(\overline{A}_i)^c$ is open and dense. So, $\forall A, (\overline{A}_i)^c = \bigcup (\overline{A}_i)^c \implies ((\overline{A}_i)^c)^c = \bigcap (((\overline{A}_i)^c)^c) = \bigcap (\overline{A}_i)$. Still though, what can I say about $\overline{A}_i$?","Show that $(S, d)$ has Baire's property iff every set of first category has a dense complement. A set is of first category if it is a countable union of nowhere dense sets. First Category Baire's Lemma: Let $(X, \rho)$ be a complete metric space and $\{U_n\}_{n=1}^{\infty}$ a sequence of open dense sets in $X$. Then the set $\bigcap_{n =1}^{\infty} U_n$ is also dense. Note that A is nowhere dense iff $(\overline{A})^c$ is open and dense. Suppose $\forall A$, $A$ is first category. That is, $A$ is a countable union of nowhere dense sets. i.e., $A_i$ is nowhere dense. $\forall A, \; A = \bigcup_{n \in \mathbb{N}}A_i \implies (A)^c = \bigcap_{n \in \mathbb{N}}(A_{i})^{c}$. But we don't know if this is a countable dense set! We know that $(\overline{A}_i)^c$ would be, but not just $(A)^c$. If $A$ is nowhere dense, what can we say about the compliment of $A$? Can anyone clear this up or provide advice? More scratch work: In my proposed proof, I said let all $A \subset S$ be of first category. i.e., each $A_i$ is nowhere dense, i.e, $(\overline{A}_i)^c$ is open and dense. So, $\forall A, (\overline{A}_i)^c = \bigcup (\overline{A}_i)^c \implies ((\overline{A}_i)^c)^c = \bigcap (((\overline{A}_i)^c)^c) = \bigcap (\overline{A}_i)$. Still though, what can I say about $\overline{A}_i$?",,"['real-analysis', 'analysis', 'baire-category']"
47,Are all the finite dimensional vector spaces with a metric isometric to $\mathbb R^n$,Are all the finite dimensional vector spaces with a metric isometric to,\mathbb R^n,"Are all the finite dimensional vector spaces with a metric isometric to $\mathbb R^n$? My goal is to claim that in any finite dimensional vector space, equipped with a metric, a closed-bounded subset is compact. I know that all finite dimensional inner-product spaces are equivalent, but I never heard it about metric so my hunch is that I'm wrong. Yet I'd love to know for sure. Is it right for normed space though? thanks!","Are all the finite dimensional vector spaces with a metric isometric to $\mathbb R^n$? My goal is to claim that in any finite dimensional vector space, equipped with a metric, a closed-bounded subset is compact. I know that all finite dimensional inner-product spaces are equivalent, but I never heard it about metric so my hunch is that I'm wrong. Yet I'd love to know for sure. Is it right for normed space though? thanks!",,"['real-analysis', 'general-topology']"
48,Motivation for the Definition of Compact Sets,Motivation for the Definition of Compact Sets,,"I'm currently taking my first course in real analysis, and was recently introduced to the following definition of compact sets: A set $S \subseteq \mathbb{R}$ is compact if and only if every open cover of $S$ contains a finite subcover. Shortly after learning the above definition of compact sets, I learned the Heine-Borel Theorem, which states that a subset $S$ of $\mathbb{R}$ is compact if and only if $S$ is closed and bounded. Therefore, the Heine-Borel theorem presents us with an equivalent way to think of compactness. To me, it seems much simpler to define compactness in terms of closed and bounded sets instead of open covers and finite subcovers. My question is, why are compact sets defined using open covers and finite subcovers? Why were compact sets not originally defined to be subsets of $\mathbb{R}$ that are closed and bounded? So far in my study of analysis, introducing open covers and finite subcovers seems to be needlessly complicating things. I've searched around the internet for answers, but most sources that I've come across discuss subjects from topology which are beyond what I've studied in analysis. Any answers that can be presented in the context of a third-year introductory course to real analysis will be much appreciated. Thank you!","I'm currently taking my first course in real analysis, and was recently introduced to the following definition of compact sets: A set $S \subseteq \mathbb{R}$ is compact if and only if every open cover of $S$ contains a finite subcover. Shortly after learning the above definition of compact sets, I learned the Heine-Borel Theorem, which states that a subset $S$ of $\mathbb{R}$ is compact if and only if $S$ is closed and bounded. Therefore, the Heine-Borel theorem presents us with an equivalent way to think of compactness. To me, it seems much simpler to define compactness in terms of closed and bounded sets instead of open covers and finite subcovers. My question is, why are compact sets defined using open covers and finite subcovers? Why were compact sets not originally defined to be subsets of $\mathbb{R}$ that are closed and bounded? So far in my study of analysis, introducing open covers and finite subcovers seems to be needlessly complicating things. I've searched around the internet for answers, but most sources that I've come across discuss subjects from topology which are beyond what I've studied in analysis. Any answers that can be presented in the context of a third-year introductory course to real analysis will be much appreciated. Thank you!",,"['real-analysis', 'analysis', 'compactness']"
49,"Looking for an example of a bijective continuous function $f:\mathbb{Q} \to \mathbb{Q}$ such that $f(-1)=0$, $f(0)=1$ and $f(1)=-1$?","Looking for an example of a bijective continuous function  such that ,  and ?",f:\mathbb{Q} \to \mathbb{Q} f(-1)=0 f(0)=1 f(1)=-1,"Clearly such a function does not exist from $\mathbb{R}$ to itself, but apparently it does in $\mathbb{Q}$ and I don't see how it could... Can you give me an example and explain to me how you thought of this one? Thank you very much! I first simply thaugh of putting the values of $f$ at the three special values of $x$ and making it $f(x)=x$ everywhere else, or constant, but it doesn't work...","Clearly such a function does not exist from $\mathbb{R}$ to itself, but apparently it does in $\mathbb{Q}$ and I don't see how it could... Can you give me an example and explain to me how you thought of this one? Thank you very much! I first simply thaugh of putting the values of $f$ at the three special values of $x$ and making it $f(x)=x$ everywhere else, or constant, but it doesn't work...",,"['real-analysis', 'analysis', 'continuity']"
50,A Fundamental Theorem of Calculus,A Fundamental Theorem of Calculus,,"Here is a problem I have been working on recently: Let $f \colon[a,b] \to \mathbb{R}$ be continuous, differentiable on $[a,b]$ except at most for a countable number of points, and $f^{\prime}$ is Lebesgue integrable, then the fundamental theorem of calculus holds, i.e. $\forall x,y \in [a,b]$ we have $$f(y) = f(x) + \int_x^yf'(t)\,dt.$$ The proof I have at the moment is somewhat indirect: I can show that the FTC holds by proving that $f$ is $AC([a,b])$ ( http://en.wikipedia.org/wiki/Absolute_continuity ). The way I can prove this is showing that the Luzin's N-property ( http://en.wikipedia.org/wiki/Luzin_N_property ) is satisfied. I have spent quite a lot of time looking for a direct proof, but nothing seems to work! Can anyone help me? Here is a summary of useful things (which I'll update if I figure out something else interesting!): Thm: If $u \colon [a,b] \to \mathbb{R}$, is continuous and differentiable everywhere on [a,b], with $u' \in L^1$, then the FTC holds. (This is a well know result for the Riemann integral, maybe a little less know in the context of the Lebesgue integral.. At least I've never heard of this result before looking for it! A proof can be found in Rudin's Real & Complex Analysis) Applying 1. we can prove the result in the case the set of non differentiability is finite. (not really useful!) I can also prove it if the set is countable but with only a finite number of accumulation points. This can be done using 2. and adding and subtracting terms to work with telescopic series. Thank you in advance for your help!","Here is a problem I have been working on recently: Let $f \colon[a,b] \to \mathbb{R}$ be continuous, differentiable on $[a,b]$ except at most for a countable number of points, and $f^{\prime}$ is Lebesgue integrable, then the fundamental theorem of calculus holds, i.e. $\forall x,y \in [a,b]$ we have $$f(y) = f(x) + \int_x^yf'(t)\,dt.$$ The proof I have at the moment is somewhat indirect: I can show that the FTC holds by proving that $f$ is $AC([a,b])$ ( http://en.wikipedia.org/wiki/Absolute_continuity ). The way I can prove this is showing that the Luzin's N-property ( http://en.wikipedia.org/wiki/Luzin_N_property ) is satisfied. I have spent quite a lot of time looking for a direct proof, but nothing seems to work! Can anyone help me? Here is a summary of useful things (which I'll update if I figure out something else interesting!): Thm: If $u \colon [a,b] \to \mathbb{R}$, is continuous and differentiable everywhere on [a,b], with $u' \in L^1$, then the FTC holds. (This is a well know result for the Riemann integral, maybe a little less know in the context of the Lebesgue integral.. At least I've never heard of this result before looking for it! A proof can be found in Rudin's Real & Complex Analysis) Applying 1. we can prove the result in the case the set of non differentiability is finite. (not really useful!) I can also prove it if the set is countable but with only a finite number of accumulation points. This can be done using 2. and adding and subtracting terms to work with telescopic series. Thank you in advance for your help!",,['real-analysis']
51,How to approximate a globally Lipschitz function by differentiable functions with bounded derivatives?,How to approximate a globally Lipschitz function by differentiable functions with bounded derivatives?,,"for some positive integer $d \geq 1$ I have a globally Lipschitz continuous function $f \colon \mathbb{R}^d \to \mathbb{R}$ with Lipschitz constant $1$ and would like to approximate it by a sequence $f_\varepsilon$ with the following properties: For all $\varepsilon > 0$, the function $f_\varepsilon$ is $k$ times partially differentiable and all its partial derivatives up to order $k$ are bounded. Here, $k$ is some positive integer ($k=3$ suffices for my purposes but arbitrary $k$ would be nicer); For all $\varepsilon > 0$, it holds that $\| f_\varepsilon \|_{\text{Lip}} \leq \| f \|_{\text{Lip}}$, where $\|\cdot\|_\text{Lip}$ denotes the usual Lipschitz norm; For $\varepsilon \to 0$, it holds that $\| f_\varepsilon - f \|_{\infty} \to 0$. For $d=1$ and $k=2$ I have found the following example in the literature which is stated without proof (it should continue to work for arbitrary $d$ if one replaces the one dimensional Gaussian distribution by a $d$-dimensional one) but don't understand why the second derivative exists (in the classical sense) and is bounded: $$f_\varepsilon(x) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} g(x + \varepsilon y) e^{- y^2/2} \, \mathrm{d}y.$$  Can anybody give me a hint or provide an example of such a sequence? Thanks in advance!","for some positive integer $d \geq 1$ I have a globally Lipschitz continuous function $f \colon \mathbb{R}^d \to \mathbb{R}$ with Lipschitz constant $1$ and would like to approximate it by a sequence $f_\varepsilon$ with the following properties: For all $\varepsilon > 0$, the function $f_\varepsilon$ is $k$ times partially differentiable and all its partial derivatives up to order $k$ are bounded. Here, $k$ is some positive integer ($k=3$ suffices for my purposes but arbitrary $k$ would be nicer); For all $\varepsilon > 0$, it holds that $\| f_\varepsilon \|_{\text{Lip}} \leq \| f \|_{\text{Lip}}$, where $\|\cdot\|_\text{Lip}$ denotes the usual Lipschitz norm; For $\varepsilon \to 0$, it holds that $\| f_\varepsilon - f \|_{\infty} \to 0$. For $d=1$ and $k=2$ I have found the following example in the literature which is stated without proof (it should continue to work for arbitrary $d$ if one replaces the one dimensional Gaussian distribution by a $d$-dimensional one) but don't understand why the second derivative exists (in the classical sense) and is bounded: $$f_\varepsilon(x) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} g(x + \varepsilon y) e^{- y^2/2} \, \mathrm{d}y.$$  Can anybody give me a hint or provide an example of such a sequence? Thanks in advance!",,"['real-analysis', 'functional-analysis']"
52,Evaluating a limit. What makes the equality right?,Evaluating a limit. What makes the equality right?,,"I'm reading a proof of a limit calculation. The limit is: $$\lim\limits_{x\to 0}\left(\frac{a^x+b^x}{2}\right)^\frac{1}{x}$$ where $a,b>0$. The aother claims that: $$\lim\limits_{x\to 0}\left(\frac{a^x+b^x}{2}\right)^\frac{1}{x} = \exp\left( \lim\limits_{x\to 0}\frac{\frac{a^x+b^x}{2} - 1}{x} \right)$$ How come? Update: Of course,  $$\lim\limits_{x\to 0}\left(\frac{a^x+b^x}{2}\right)^\frac{1}{x} = \exp\left(\lim\limits_{x\to 0} \frac{\ln\left( \frac{a^x+b^x}{2} \right)}{x} \right)$$ But how to proceed to reach the auther's expression?","I'm reading a proof of a limit calculation. The limit is: $$\lim\limits_{x\to 0}\left(\frac{a^x+b^x}{2}\right)^\frac{1}{x}$$ where $a,b>0$. The aother claims that: $$\lim\limits_{x\to 0}\left(\frac{a^x+b^x}{2}\right)^\frac{1}{x} = \exp\left( \lim\limits_{x\to 0}\frac{\frac{a^x+b^x}{2} - 1}{x} \right)$$ How come? Update: Of course,  $$\lim\limits_{x\to 0}\left(\frac{a^x+b^x}{2}\right)^\frac{1}{x} = \exp\left(\lim\limits_{x\to 0} \frac{\ln\left( \frac{a^x+b^x}{2} \right)}{x} \right)$$ But how to proceed to reach the auther's expression?",,"['calculus', 'real-analysis', 'limits', 'exponentiation']"
53,"$\sum a_n$ converges, $a_n \in \mathbb{R}$, then there exists real sequence $b_n$ such that $b_n\rightarrow +\infty$ and $\sum a_n b_n$ converges.","converges, , then there exists real sequence  such that  and  converges.",\sum a_n a_n \in \mathbb{R} b_n b_n\rightarrow +\infty \sum a_n b_n,"Assume that $\sum a_n$ converges, $a_n \in \mathbb{R}$, then there exists real sequence $b_n$  such that $b_n\rightarrow +\infty$ and $\sum a_n b_n$ converges. Same to be easy at first thought, can we find such $b_n$ represented by $a_n$?","Assume that $\sum a_n$ converges, $a_n \in \mathbb{R}$, then there exists real sequence $b_n$  such that $b_n\rightarrow +\infty$ and $\sum a_n b_n$ converges. Same to be easy at first thought, can we find such $b_n$ represented by $a_n$?",,"['real-analysis', 'sequences-and-series']"
54,If $f_n$ is a sequence of differentiable functions converging to $f$ uniformly on a compact set,If  is a sequence of differentiable functions converging to  uniformly on a compact set,f_n f,"Suppose $f_n\rightarrow f$ on a compact set in $\mathbb{R}^n$, with $f_n\in C^1$. $f$ is not necessary differentiable. We can easily find a sequence of functions converging to $|f|$, for example. My question is: does there exist any results which says, for example, the derivative exists at all but finitely many places. What about if $f_n\in C^2$?","Suppose $f_n\rightarrow f$ on a compact set in $\mathbb{R}^n$, with $f_n\in C^1$. $f$ is not necessary differentiable. We can easily find a sequence of functions converging to $|f|$, for example. My question is: does there exist any results which says, for example, the derivative exists at all but finitely many places. What about if $f_n\in C^2$?",,['real-analysis']
55,"Is the set $\{ x\in \mathbb{Q}: 2< x^2 <3\}$ closed, bounded, compact in $\mathbb{Q}$?","Is the set  closed, bounded, compact in ?",\{ x\in \mathbb{Q}: 2< x^2 <3\} \mathbb{Q},"Is the set $\{ x\in \mathbb{Q}: 2< x^2 <3\}$ closed, bounded, compact in $\mathbb{Q}$ ? I think $\{ x\in \mathbb{Q}: 2< x^2 <3\}=\{ x\in \mathbb{Q}: 2\leq x^2 \leq 3\}$, so it is bounded and closed in $Q$, is that right?","Is the set $\{ x\in \mathbb{Q}: 2< x^2 <3\}$ closed, bounded, compact in $\mathbb{Q}$ ? I think $\{ x\in \mathbb{Q}: 2< x^2 <3\}=\{ x\in \mathbb{Q}: 2\leq x^2 \leq 3\}$, so it is bounded and closed in $Q$, is that right?",,['real-analysis']
56,Show that $\lim_{n\rightarrow \infty} \int_0^{\pi/2} 2^n \sqrt{n} \sin^n(x) \cos^{n-2}(x) \; dx = \sqrt{2\pi}$,Show that,\lim_{n\rightarrow \infty} \int_0^{\pi/2} 2^n \sqrt{n} \sin^n(x) \cos^{n-2}(x) \; dx = \sqrt{2\pi},"I wish to show $$\lim_{n\rightarrow \infty} \int_0^{\pi/2} 2^n \sqrt{n} \sin^n(x) \cos^{n-2}(x) \; dx = \sqrt{2\pi}$$ I've tried substitution and integration by parts to get a recursive formula for the integral, but so far it hasn't worked. Any help would be appreciated, thanks.","I wish to show $$\lim_{n\rightarrow \infty} \int_0^{\pi/2} 2^n \sqrt{n} \sin^n(x) \cos^{n-2}(x) \; dx = \sqrt{2\pi}$$ I've tried substitution and integration by parts to get a recursive formula for the integral, but so far it hasn't worked. Any help would be appreciated, thanks.",,"['calculus', 'real-analysis', 'limits', 'definite-integrals']"
57,"Show there exists $g:(0,1)\to\mathbb{R}^2$ such that $h\circ g$ constant, $g$ continuously differentiable and $g$ injective","Show there exists  such that  constant,  continuously differentiable and  injective","g:(0,1)\to\mathbb{R}^2 h\circ g g g","Let $h\in C^1(\mathbb{R}^2,\mathbb{R})$. Show:   There exists $g:(0,1)\to\mathbb{R}^2$ such that $h\circ g$ constant, $g$ continuously differentiable and $g$ injective. Dini's theorem: Let $\Omega \subseteq \mathbb{R}^2$ open, $f \in C^1(\Omega, \mathbb{R})$ and $p \in \Omega$ with $f(p) = 0$. If $\frac{\partial f}{\partial y}(p) \neq 0$, then there exist open intervals $V_1, V_2$ with $p \in V_1 \times V_2 \in \Omega$ and $g \in C^1(V, \mathbb{R})$ with $$\{(x,y) \in V_1 \times V_2 \mid f(x,y) = 0 \} = \{(x,g(x) \mid x \in V_1\}$$ and $g$ furthermore satisfies $$g' = -\left(\frac{\partial f}{\partial y}(x, g(x))\right)^{-1}\frac{\partial f}{\partial x}(x, g(x)).$$ Proof of existance of $g$: Let without loss of generality $\frac{\partial f}{\partial y}(p) \gt 0$. Then there exists $\epsilon \gt 0$ with $\overline{B_\epsilon(p)} \subseteq \Omega$ and $\frac{\partial f}{\partial y} \gt 0$ on $\overline{B_\epsilon(p)}$. Let $y^- = p - \frac{\epsilon}{2}$ and $y^+ = p + \frac{\epsilon}{2}$. Then there exists $\delta \in (0, \epsilon)$ with $f(x, y^-) \lt 0 \lt f(x, y^+)$ for $x \in (p - \frac{\delta}{2}, p + \frac{\delta}{2}) =: V_1$ and $(y^-, y^+) =: V_2$. For $x \in V_1$ there exists exactly one $y \in V_2$ with $f(x,y) = 0$ (mean value theorem). Set $y(x) := g$ and the claim follows.","Let $h\in C^1(\mathbb{R}^2,\mathbb{R})$. Show:   There exists $g:(0,1)\to\mathbb{R}^2$ such that $h\circ g$ constant, $g$ continuously differentiable and $g$ injective. Dini's theorem: Let $\Omega \subseteq \mathbb{R}^2$ open, $f \in C^1(\Omega, \mathbb{R})$ and $p \in \Omega$ with $f(p) = 0$. If $\frac{\partial f}{\partial y}(p) \neq 0$, then there exist open intervals $V_1, V_2$ with $p \in V_1 \times V_2 \in \Omega$ and $g \in C^1(V, \mathbb{R})$ with $$\{(x,y) \in V_1 \times V_2 \mid f(x,y) = 0 \} = \{(x,g(x) \mid x \in V_1\}$$ and $g$ furthermore satisfies $$g' = -\left(\frac{\partial f}{\partial y}(x, g(x))\right)^{-1}\frac{\partial f}{\partial x}(x, g(x)).$$ Proof of existance of $g$: Let without loss of generality $\frac{\partial f}{\partial y}(p) \gt 0$. Then there exists $\epsilon \gt 0$ with $\overline{B_\epsilon(p)} \subseteq \Omega$ and $\frac{\partial f}{\partial y} \gt 0$ on $\overline{B_\epsilon(p)}$. Let $y^- = p - \frac{\epsilon}{2}$ and $y^+ = p + \frac{\epsilon}{2}$. Then there exists $\delta \in (0, \epsilon)$ with $f(x, y^-) \lt 0 \lt f(x, y^+)$ for $x \in (p - \frac{\delta}{2}, p + \frac{\delta}{2}) =: V_1$ and $(y^-, y^+) =: V_2$. For $x \in V_1$ there exists exactly one $y \in V_2$ with $f(x,y) = 0$ (mean value theorem). Set $y(x) := g$ and the claim follows.",,"['real-analysis', 'multivariable-calculus', 'implicit-function-theorem']"
58,Differential inequality implies inequality for points at distance pi.,Differential inequality implies inequality for points at distance pi.,,"Given a function $f$ with $f+f''\ge 0$, show that $f(x)+f(x+\pi) \ge 0$ for all $x$. Note that for sine and cosine both inequalities become equations. It seems reasonable to look at $f+f''=g$, but the resulting expressions seem inconclusive.","Given a function $f$ with $f+f''\ge 0$, show that $f(x)+f(x+\pi) \ge 0$ for all $x$. Note that for sine and cosine both inequalities become equations. It seems reasonable to look at $f+f''=g$, but the resulting expressions seem inconclusive.",,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'inequality']"
59,Proof of continuity - (ε-δ) definition - Can anyone check this?,Proof of continuity - (ε-δ) definition - Can anyone check this?,,"I've been trying to get my head around this problem for quite some time by now.  I want to prove that $$f(x) := \left|\frac{x-1}{x^2+1}\right|$$ is continuous for $$x_0 = -1$$ Now, in order to prove this, I want to use to ε-δ defintion which states that for any given $\varepsilon$, there exists $\delta>0$ such that $$\left|x-x_0\right| < \delta \Rightarrow \left|f(x)-f(x_0)\right|<\varepsilon$$ I'm fairly new to the concepts of analysis; but I understand that in order to conduct these sorts of proofs, you first need to spend a few minutes to find out a reasonable value for $\delta$. Unfortunately, I can't find any particular ""rules"" on how to do that. So, I first tried to fill in some gaps for my particular problem: Given $\varepsilon > 0$, we can find $\delta > 0$ such that, if $\left|x-(-1)\right| = \left|x+1\right| < \delta$, then $$\left|\frac{x-1}{x^2+1}-(-1)\right|=\left|\frac{x-1}{x^2+1}+1\right|=\left|\frac{x²+x}{x^2+1}\right|<\varepsilon$$ My plan is now to take care of the implication first. That is, I want to paraphrase $\left|\frac{x²+x}{x^2+1}\right|<\varepsilon$ in a way that makes sure that the actual $\varepsilon > 0$ chosen does not matter at all. So, in the end, I think manipulating $\left|\frac{x²+x}{x^2+1}\right|<\varepsilon$ to look like $\left|x+1\right| < ε \cdot \varphi$ would be my goal, right? Because then, if we let $\delta := \varepsilon \cdot \varphi$, the implication holds. I hope everything to this point was correct. Now, I find $$\left|\frac{x²+x}{x^2+1}\right| = \left|\frac{x(x^2+1)}{x+1}\right| = \left|x+1\right|\left|\frac{x}{x^2+1}\right|<\varepsilon$$ which is why I would want to let $\delta := \frac{\varepsilon}{\left|\frac{x}{x^2+1}\right|}$ (which looks quite ugly, point taken). Is my choice of $\delta$ okay? I never saw a $\delta$ contain $x$ as a variable before, my professor always somehow managed to set $\delta$ to $\varepsilon$ times some constant $\varphi$, but I can't seem to do it. Can anyone check my current steps and help me find a constant $\varphi$, if this is necessary for the proof I'm building up?","I've been trying to get my head around this problem for quite some time by now.  I want to prove that $$f(x) := \left|\frac{x-1}{x^2+1}\right|$$ is continuous for $$x_0 = -1$$ Now, in order to prove this, I want to use to ε-δ defintion which states that for any given $\varepsilon$, there exists $\delta>0$ such that $$\left|x-x_0\right| < \delta \Rightarrow \left|f(x)-f(x_0)\right|<\varepsilon$$ I'm fairly new to the concepts of analysis; but I understand that in order to conduct these sorts of proofs, you first need to spend a few minutes to find out a reasonable value for $\delta$. Unfortunately, I can't find any particular ""rules"" on how to do that. So, I first tried to fill in some gaps for my particular problem: Given $\varepsilon > 0$, we can find $\delta > 0$ such that, if $\left|x-(-1)\right| = \left|x+1\right| < \delta$, then $$\left|\frac{x-1}{x^2+1}-(-1)\right|=\left|\frac{x-1}{x^2+1}+1\right|=\left|\frac{x²+x}{x^2+1}\right|<\varepsilon$$ My plan is now to take care of the implication first. That is, I want to paraphrase $\left|\frac{x²+x}{x^2+1}\right|<\varepsilon$ in a way that makes sure that the actual $\varepsilon > 0$ chosen does not matter at all. So, in the end, I think manipulating $\left|\frac{x²+x}{x^2+1}\right|<\varepsilon$ to look like $\left|x+1\right| < ε \cdot \varphi$ would be my goal, right? Because then, if we let $\delta := \varepsilon \cdot \varphi$, the implication holds. I hope everything to this point was correct. Now, I find $$\left|\frac{x²+x}{x^2+1}\right| = \left|\frac{x(x^2+1)}{x+1}\right| = \left|x+1\right|\left|\frac{x}{x^2+1}\right|<\varepsilon$$ which is why I would want to let $\delta := \frac{\varepsilon}{\left|\frac{x}{x^2+1}\right|}$ (which looks quite ugly, point taken). Is my choice of $\delta$ okay? I never saw a $\delta$ contain $x$ as a variable before, my professor always somehow managed to set $\delta$ to $\varepsilon$ times some constant $\varphi$, but I can't seem to do it. Can anyone check my current steps and help me find a constant $\varphi$, if this is necessary for the proof I'm building up?",,"['real-analysis', 'analysis', 'convergence-divergence', 'epsilon-delta']"
60,What is the completion of this space?,What is the completion of this space?,,"This question asks us to show that $\Bbb R$ with the following metric is not complete: Fix a strictly positive function $f \in L^1(\Bbb R)$, and let $d(x,y)=\left|\int_x^y f(t)dt\right|$. It's easy to see (in line with the answers to the linked question) that any unbounded increasing sequence of real numbers is Cauchy without limit in this metric. What is the completion of this metric space? Is it homeomorphic to any well-known space? If not, what are some interesting properties it has? If the problem is intractable for arbitrary $f$, feel free to choose a suitably interesting $f$ or subclass of such $f$. (In particular, continuous $f$ or smooth $f$.)","This question asks us to show that $\Bbb R$ with the following metric is not complete: Fix a strictly positive function $f \in L^1(\Bbb R)$, and let $d(x,y)=\left|\int_x^y f(t)dt\right|$. It's easy to see (in line with the answers to the linked question) that any unbounded increasing sequence of real numbers is Cauchy without limit in this metric. What is the completion of this metric space? Is it homeomorphic to any well-known space? If not, what are some interesting properties it has? If the problem is intractable for arbitrary $f$, feel free to choose a suitably interesting $f$ or subclass of such $f$. (In particular, continuous $f$ or smooth $f$.)",,['real-analysis']
61,What's wrong with the classical Cauchy construction of the reals?,What's wrong with the classical Cauchy construction of the reals?,,"I am reading Bishop's ""Constructive Analysis"" and he says that defining a real number to just be an equivalence class of Cauchy sequences of rationals would be wrong.  Why is that?","I am reading Bishop's ""Constructive Analysis"" and he says that defining a real number to just be an equivalence class of Cauchy sequences of rationals would be wrong.  Why is that?",,"['real-analysis', 'analysis', 'cauchy-sequences', 'constructive-mathematics']"
62,"Integral $\int_0^\infty \ln x\,\exp\left(-\frac{1+x^4}{2\alpha x^2}\right) \frac{x^4+3\alpha x^2- 1}{x^6}dx$",Integral,"\int_0^\infty \ln x\,\exp\left(-\frac{1+x^4}{2\alpha x^2}\right) \frac{x^4+3\alpha x^2- 1}{x^6}dx","$$I:=\int_0^\infty \ln x\,\exp\left(-\frac{1+x^4}{2\alpha x^2}\right) \frac{x^4+3\alpha x^2- 1}{x^6}dx=\frac{(1+\alpha)\sqrt{2\alpha^3 \pi}}{2\sqrt[\alpha]e},\qquad \alpha>0.$$ This one looks very nice.  It has stumped me. Differentiation with respect to parameter does not seem to work either if I try $I(\alpha)$ and $I'(\alpha)$. at x=0 there seems to be a problem with the integrand also however I am not sure how to go about using this. Perhaps we could try and use a series expansion for $e^x=\sum_{n=0}^\infty  x^n /n!$, however the function $e^{-1/x^2}$ is well known that its taylor series is zero despite the function not being.","$$I:=\int_0^\infty \ln x\,\exp\left(-\frac{1+x^4}{2\alpha x^2}\right) \frac{x^4+3\alpha x^2- 1}{x^6}dx=\frac{(1+\alpha)\sqrt{2\alpha^3 \pi}}{2\sqrt[\alpha]e},\qquad \alpha>0.$$ This one looks very nice.  It has stumped me. Differentiation with respect to parameter does not seem to work either if I try $I(\alpha)$ and $I'(\alpha)$. at x=0 there seems to be a problem with the integrand also however I am not sure how to go about using this. Perhaps we could try and use a series expansion for $e^x=\sum_{n=0}^\infty  x^n /n!$, however the function $e^{-1/x^2}$ is well known that its taylor series is zero despite the function not being.",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
63,How can a simple closed curve not look locally like the rotated graph of a continuous function?,How can a simple closed curve not look locally like the rotated graph of a continuous function?,,"A simple closed curve is a continuous closed curve without self-intersections.  The question of whether you can inscribe a square in every simple closed curve is currently an open problem, but this page describes an important partial result: Stromquist's Theorem: If the simple closed curve J is ""nice enough"" then it has an inscribed square.... Here, ""nice enough"" means the following: for each point P on the curve there must be a coordinate system for the plane in which some piece of the curve containing P is the graph y = f(x) of a continuous function. My question isn't about Stromquist's theorem itself, but about the condition that's dubbed  ""nice enough"".  It's mind-boggling that there could be a simple closed curve curve that fails to meet this condition. I can understand continuous curves that fail the vertical line test, because two distant points on the curve have the same x-value, and I can understand situations where the vertical line test fails locally in the part of a curve containing P, because the points in that part of the curve are clustered around a vertical line.  But it's completely counterintuitive to me that you could have a point P on a simple closed curve such that no arc of the curve in the vicinity of P, no matter how small, can pass the vertical line test no matter how you rotate the arc. So is there a simple example of such a curve?  Preferably I'd want an example where the interior of the simple closed curve was a convex region, because that's the case that my intuition most strenuously objects to. Any help would be greatly appreciated. Thank You in Advance. EDIT: Wikipedia tells me that the property I've been calling ""nice enough"" is actually known as ""locally monotone"".","A simple closed curve is a continuous closed curve without self-intersections.  The question of whether you can inscribe a square in every simple closed curve is currently an open problem, but this page describes an important partial result: Stromquist's Theorem: If the simple closed curve J is ""nice enough"" then it has an inscribed square.... Here, ""nice enough"" means the following: for each point P on the curve there must be a coordinate system for the plane in which some piece of the curve containing P is the graph y = f(x) of a continuous function. My question isn't about Stromquist's theorem itself, but about the condition that's dubbed  ""nice enough"".  It's mind-boggling that there could be a simple closed curve curve that fails to meet this condition. I can understand continuous curves that fail the vertical line test, because two distant points on the curve have the same x-value, and I can understand situations where the vertical line test fails locally in the part of a curve containing P, because the points in that part of the curve are clustered around a vertical line.  But it's completely counterintuitive to me that you could have a point P on a simple closed curve such that no arc of the curve in the vicinity of P, no matter how small, can pass the vertical line test no matter how you rotate the arc. So is there a simple example of such a curve?  Preferably I'd want an example where the interior of the simple closed curve was a convex region, because that's the case that my intuition most strenuously objects to. Any help would be greatly appreciated. Thank You in Advance. EDIT: Wikipedia tells me that the property I've been calling ""nice enough"" is actually known as ""locally monotone"".",,"['real-analysis', 'general-topology', 'geometry', 'plane-curves']"
64,Is the dual of a complete topological vector space always complete?,Is the dual of a complete topological vector space always complete?,,"Let $X$ be a complete topological vector space (over $\mathbb{C}$ say), and $X'$ its dual with the weak*-topology. Then is $X'$ always complete? You may assume $X$ is locally convex if you like.","Let $X$ be a complete topological vector space (over $\mathbb{C}$ say), and $X'$ its dual with the weak*-topology. Then is $X'$ always complete? You may assume $X$ is locally convex if you like.",,"['real-analysis', 'analysis', 'functional-analysis']"
65,Showing $2^{n_2} + 3^{n_3}+\cdots+9^{n_9}$ is dense in $\mathbb{R}^+$,Showing  is dense in,2^{n_2} + 3^{n_3}+\cdots+9^{n_9} \mathbb{R}^+,"I encountered this problem via a friend. He asked me to prove that  $$ \left\{u: u= \sum_{k=2}^9 k^{n_k} \quad n_k \in \mathbb{Z}  \right\}$$  is dense in $\mathbb{R}^+$. I was able to show that $0$ is approachable(can get arbitrarily close to) by numbers of this type. However to proceed ahead, I need to know what operations preserve the structure of these numbers. For instance, to show $m+n\sqrt{2}$ is dense for all integers $m,n$, I used the fact that product of any two numbers in this form yields a number in the same form. My problem is I don't know what pattern to exploit in the given problem. I appreciate any hints / patterns you can provide. One idea I have been working on is as follows: If we could partition $2^{n_2}, 3^{n_3},\ldots, 9^{n_9}$ into two parts, one that would determine the integer part, the other that would approximate the fractional part, we would be done.","I encountered this problem via a friend. He asked me to prove that  $$ \left\{u: u= \sum_{k=2}^9 k^{n_k} \quad n_k \in \mathbb{Z}  \right\}$$  is dense in $\mathbb{R}^+$. I was able to show that $0$ is approachable(can get arbitrarily close to) by numbers of this type. However to proceed ahead, I need to know what operations preserve the structure of these numbers. For instance, to show $m+n\sqrt{2}$ is dense for all integers $m,n$, I used the fact that product of any two numbers in this form yields a number in the same form. My problem is I don't know what pattern to exploit in the given problem. I appreciate any hints / patterns you can provide. One idea I have been working on is as follows: If we could partition $2^{n_2}, 3^{n_3},\ldots, 9^{n_9}$ into two parts, one that would determine the integer part, the other that would approximate the fractional part, we would be done.",,"['real-analysis', 'elementary-number-theory']"
66,Theorem 6.11 of Rudin's Principles of Mathematical Analysis,Theorem 6.11 of Rudin's Principles of Mathematical Analysis,,"In the proof of Theorem 6.11, $\varphi$ is uniformly continuous and hence for arbitrary $\epsilon > 0$ we can pick $\delta > 0$ s.t. $\left|s-t\right| \leq \delta$ implies $\left|\varphi\left(s\right)-\varphi\left(t\right)\right|<\epsilon$. However, I do not understand why he claims that $\delta < \epsilon$. Help? Statement of the theorem: Suppose $f\colon\left[a,b\right]\rightarrow \mathbb{R}$ is Riemann integrable on $\left[a,b\right]$, $m\leq f \leq M$ (for $m,M\in\mathbb{R}$), $\varphi\colon \left[m,M\right]\rightarrow \mathbb{R}$ is continuous on $\left[m,M\right]$. Let $h\equiv\varphi\circ f$. Then $h$ is Riemann integrable on $\left[a,b\right]$.","In the proof of Theorem 6.11, $\varphi$ is uniformly continuous and hence for arbitrary $\epsilon > 0$ we can pick $\delta > 0$ s.t. $\left|s-t\right| \leq \delta$ implies $\left|\varphi\left(s\right)-\varphi\left(t\right)\right|<\epsilon$. However, I do not understand why he claims that $\delta < \epsilon$. Help? Statement of the theorem: Suppose $f\colon\left[a,b\right]\rightarrow \mathbb{R}$ is Riemann integrable on $\left[a,b\right]$, $m\leq f \leq M$ (for $m,M\in\mathbb{R}$), $\varphi\colon \left[m,M\right]\rightarrow \mathbb{R}$ is continuous on $\left[m,M\right]$. Let $h\equiv\varphi\circ f$. Then $h$ is Riemann integrable on $\left[a,b\right]$.",,['real-analysis']
67,Find all function satisfying a condition with $\min$ and $\max$,Find all function satisfying a condition with  and,\min \max,"Find all functions $f:\mathbb{R}\rightarrow\mathbb{R}$ such that $$ \forall (x,y)\in\mathbb{R}^2,x\ne y,\quad \min (f(x),f(y)) \leq \frac{f(x)-f(y)}{x-y} \leq \max(f(x),f(y)) $$ I have started with $f$ continuous, then precisely here $f$ is differentiable and $f'=f$; Thus,  $$ f(x)=K\exp(x) $$ Now,if $f$ has a discontinuity point with a different limit to the left and right we have a contradiction. How can I continue ? Thank you in advance for your help, NB: If anyone has a better title ..","Find all functions $f:\mathbb{R}\rightarrow\mathbb{R}$ such that $$ \forall (x,y)\in\mathbb{R}^2,x\ne y,\quad \min (f(x),f(y)) \leq \frac{f(x)-f(y)}{x-y} \leq \max(f(x),f(y)) $$ I have started with $f$ continuous, then precisely here $f$ is differentiable and $f'=f$; Thus,  $$ f(x)=K\exp(x) $$ Now,if $f$ has a discontinuity point with a different limit to the left and right we have a contradiction. How can I continue ? Thank you in advance for your help, NB: If anyone has a better title ..",,['real-analysis']
68,Limit of convolution,Limit of convolution,,"I am having trouble calculating limit of a certain integral expression. In some computation, I need to show that $\|f_n * g\|_{L^p} \to \|g\|_{L^1}$ as $n\to\infty$, where $f_n=(2n)^{-1/p}\chi_{(-n,n)}$ and $g$ is a positive integrable function. Please help, thank you.","I am having trouble calculating limit of a certain integral expression. In some computation, I need to show that $\|f_n * g\|_{L^p} \to \|g\|_{L^1}$ as $n\to\infty$, where $f_n=(2n)^{-1/p}\chi_{(-n,n)}$ and $g$ is a positive integrable function. Please help, thank you.",,"['real-analysis', 'integration', 'analysis']"
69,Exercise on Boolean closure of elementary sets,Exercise on Boolean closure of elementary sets,,"I am reading Tao's notes on measure theory and I am stuck with an exercise, so here is the problem: Definition We define $|I|$ the lenght of an interval of endpoints $a<b$ to be $|I|=b-a$. A box in $\mathbb R^d$ is a cartesian product $B=I_1 \times...\times I_d$ of $d$ intervals. An elementary set is any subset of $\mathbb R^d$ which is the union of a fnite number of boxes. Show that if $E,F \subset \mathbb R^d$ are elementary sets, then $E \cup F$, $E \cap F$, $E \setminus F$ and $E \Delta F$ are elementary sets. If $x \in \mathbb R^d$ show that the translate $E+x=\{y+x : y \in E\}$ is an elementary set. Attempt at a solution If $E$ and $F$ are elementary sets then $E=B_1 \cup ... \cup B_k$ and $F=S_1 \cup ... \cup S_n$ union of $k$ and $n$ boxes respectively , so $E \cup F=(B_1 \cup ... \cup B_k) \cup (S_1 \cup ... \cup S_n)$ the union of $k+n$ boxes, i.e., $E \cup F$ is an elementary set by definition. I don't know how to show that the intersection and $E \setminus F$ are elementary. If I could prove these, then $E \Delta F= (E \setminus F) \cup (F \setminus E)$ is union of elementary sets which is an elementary set. For the translation, I know that $E=B_1 \cup ... \cup B_k$, so I can write $E+x=B_1+x \cup ... \cup B_k+x$. For each $B_i, 1\leq i \leq k$, $B_i=I_1 \times ... \times I_n$, so $B_i+x=I_1+x \times ... \times I_n+x$ and if an interval $I$ has endpoints $a<b$, then $I+x$ is an interval of endpoints $a+x<b+x$, so $E+x$ is a finite union of boxes, i.e., $E+x$ is an elementary set. I would appreciate some help with the two remaining proofs.","I am reading Tao's notes on measure theory and I am stuck with an exercise, so here is the problem: Definition We define $|I|$ the lenght of an interval of endpoints $a<b$ to be $|I|=b-a$. A box in $\mathbb R^d$ is a cartesian product $B=I_1 \times...\times I_d$ of $d$ intervals. An elementary set is any subset of $\mathbb R^d$ which is the union of a fnite number of boxes. Show that if $E,F \subset \mathbb R^d$ are elementary sets, then $E \cup F$, $E \cap F$, $E \setminus F$ and $E \Delta F$ are elementary sets. If $x \in \mathbb R^d$ show that the translate $E+x=\{y+x : y \in E\}$ is an elementary set. Attempt at a solution If $E$ and $F$ are elementary sets then $E=B_1 \cup ... \cup B_k$ and $F=S_1 \cup ... \cup S_n$ union of $k$ and $n$ boxes respectively , so $E \cup F=(B_1 \cup ... \cup B_k) \cup (S_1 \cup ... \cup S_n)$ the union of $k+n$ boxes, i.e., $E \cup F$ is an elementary set by definition. I don't know how to show that the intersection and $E \setminus F$ are elementary. If I could prove these, then $E \Delta F= (E \setminus F) \cup (F \setminus E)$ is union of elementary sets which is an elementary set. For the translation, I know that $E=B_1 \cup ... \cup B_k$, so I can write $E+x=B_1+x \cup ... \cup B_k+x$. For each $B_i, 1\leq i \leq k$, $B_i=I_1 \times ... \times I_n$, so $B_i+x=I_1+x \times ... \times I_n+x$ and if an interval $I$ has endpoints $a<b$, then $I+x$ is an interval of endpoints $a+x<b+x$, so $E+x$ is a finite union of boxes, i.e., $E+x$ is an elementary set. I would appreciate some help with the two remaining proofs.",,['real-analysis']
70,Show that function is differentiable but its derivative is discontinuous.,Show that function is differentiable but its derivative is discontinuous.,,"Let $g(x)=x^2\sin(1/x)$, if $x \neq 0$ and $g(0)=0$. If $\{r_i\}$ is the numeration of all rational numbers in $[0,1]$, define   $$ f(x)=\sum_{n=1}^\infty \frac{g(x-r_n)}{n^2} $$   Show that $f:[0,1] \rightarrow R$ is differentiable in each point over [0,1] but $f'(x)$ is discontinuous over each $r_n$. Is possible that the set of all discontinuous points of $f'$ is precisely $\{r_n\}$? I'm not seeing how this function is working. I could not even derive it. I need to fix some $ n $ to work? And to see the discontinuity of $ f '(x) $ after that? Can anyone give me any tips? I am not knowing how to work with this exercise and do not know where to start.","Let $g(x)=x^2\sin(1/x)$, if $x \neq 0$ and $g(0)=0$. If $\{r_i\}$ is the numeration of all rational numbers in $[0,1]$, define   $$ f(x)=\sum_{n=1}^\infty \frac{g(x-r_n)}{n^2} $$   Show that $f:[0,1] \rightarrow R$ is differentiable in each point over [0,1] but $f'(x)$ is discontinuous over each $r_n$. Is possible that the set of all discontinuous points of $f'$ is precisely $\{r_n\}$? I'm not seeing how this function is working. I could not even derive it. I need to fix some $ n $ to work? And to see the discontinuity of $ f '(x) $ after that? Can anyone give me any tips? I am not knowing how to work with this exercise and do not know where to start.",,"['calculus', 'real-analysis', 'sequences-and-series']"
71,Project Euler - 34 / Find a mathematical approach for upper bound,Project Euler - 34 / Find a mathematical approach for upper bound,,"145 is a curious number, as 1! + 4! + 5! = 1 + 24 + 120 = 145. Find   the sum of all numbers which are equal to the sum of the factorial of   their digits. Note: as 1! = 1 and 2! = 2 are not sums they are not   included. So I tried a random enough large upper bound ( 100000 ) in my program and it worked. I deleted this part to not spoil the answer My question is : How to find the closest upper bound using a mathematical approach ?","145 is a curious number, as 1! + 4! + 5! = 1 + 24 + 120 = 145. Find   the sum of all numbers which are equal to the sum of the factorial of   their digits. Note: as 1! = 1 and 2! = 2 are not sums they are not   included. So I tried a random enough large upper bound ( 100000 ) in my program and it worked. I deleted this part to not spoil the answer My question is : How to find the closest upper bound using a mathematical approach ?",,"['calculus', 'real-analysis', 'project-euler']"
72,Subsequence convergence in $L^p$,Subsequence convergence in,L^p,"I recall a fact that for functions $f_1,f_2,\ldots\in L^1$ such that $\|f_n-f\|_1\rightarrow 0$ as $n\rightarrow\infty$, there exists a subsequence $f_{n_i}$ that converges to $f$ almost everywhere. Is this still true if we replace $L^1$ by $L^p$, and $\|f_n-f\|_1$ by $\|f_n-f\|_p$?","I recall a fact that for functions $f_1,f_2,\ldots\in L^1$ such that $\|f_n-f\|_1\rightarrow 0$ as $n\rightarrow\infty$, there exists a subsequence $f_{n_i}$ that converges to $f$ almost everywhere. Is this still true if we replace $L^1$ by $L^p$, and $\|f_n-f\|_1$ by $\|f_n-f\|_p$?",,"['real-analysis', 'convergence-divergence', 'lp-spaces']"
73,"Prove that $\mathbb{R}$ and the interval $(0, \infty)$ have the same cardinality.",Prove that  and the interval  have the same cardinality.,"\mathbb{R} (0, \infty)","Prove that Real Numbers and the interval $(0,\infty)$ have the same cardinality. Attempt: Consider the function $f(x) = e^x$. The domain of this function is all real numbers. The range of this function is from $0$ to infinity. Let $e^a = e^b$. Then $\ln(e^a) = \ln(e^b)$ Then $a\ln(e) = b\ln(e)$ This means that $a = b$ Hence, $f$ is injective. Let $c > 0$ Then $e^{ln(c)} = c$ Since $c > 0, \ln(c)$ is defined, so $f(\ln(c)) = c$ Therefore, f is surjective. Then f is bijective. Hence, $\mathbb{R}$ and $(0, \infty)$ have the same cardinality.","Prove that Real Numbers and the interval $(0,\infty)$ have the same cardinality. Attempt: Consider the function $f(x) = e^x$. The domain of this function is all real numbers. The range of this function is from $0$ to infinity. Let $e^a = e^b$. Then $\ln(e^a) = \ln(e^b)$ Then $a\ln(e) = b\ln(e)$ This means that $a = b$ Hence, $f$ is injective. Let $c > 0$ Then $e^{ln(c)} = c$ Since $c > 0, \ln(c)$ is defined, so $f(\ln(c)) = c$ Therefore, f is surjective. Then f is bijective. Hence, $\mathbb{R}$ and $(0, \infty)$ have the same cardinality.",,"['real-analysis', 'elementary-set-theory']"
74,Does the series $\sum_{n=2}^\infty \frac {\sin(n+\frac 1n)}{(\ln n)^2}$ converge?,Does the series  converge?,\sum_{n=2}^\infty \frac {\sin(n+\frac 1n)}{(\ln n)^2},Could you give me some hint how to deal with this series ? I could not conclude about absolute convergence because  $\frac {\left|\sin\left( n+\frac 1n \right)\right|}{(\ln n)^2}\le \frac 1{(\ln n)^2}$ does not get me anywhere. I tried to use the Dirichlet's test to prove convergence of this series: $\sum_{n=2}^\infty \frac {\sin\left( n+\frac 1n \right)}{(\ln n)^2}$  but I could not prove that $\sum_{n=2}^\infty \sin(n+\frac 1n)$  are bounded. Thanks.,Could you give me some hint how to deal with this series ? I could not conclude about absolute convergence because  $\frac {\left|\sin\left( n+\frac 1n \right)\right|}{(\ln n)^2}\le \frac 1{(\ln n)^2}$ does not get me anywhere. I tried to use the Dirichlet's test to prove convergence of this series: $\sum_{n=2}^\infty \frac {\sin\left( n+\frac 1n \right)}{(\ln n)^2}$  but I could not prove that $\sum_{n=2}^\infty \sin(n+\frac 1n)$  are bounded. Thanks.,,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
75,"Writing $f\in L^2([-\pi,\pi])$ as a power series.",Writing  as a power series.,"f\in L^2([-\pi,\pi])","Consider the space $L^2([-\pi,\pi])$. I want to show that every function $f\in L^2([-\pi,\pi])$ can be written as a power series. I remember a result that polynomials are dense in $L^2([-\pi,\pi])$. I thought about extending it (since power series are polynomials with infinite number of terms), but it is not clear how to do that. Density means that polynomials can get arbitrarily close to a function $f$, but here we want an exact representation. (Note: See also my previous question )","Consider the space $L^2([-\pi,\pi])$. I want to show that every function $f\in L^2([-\pi,\pi])$ can be written as a power series. I remember a result that polynomials are dense in $L^2([-\pi,\pi])$. I thought about extending it (since power series are polynomials with infinite number of terms), but it is not clear how to do that. Density means that polynomials can get arbitrarily close to a function $f$, but here we want an exact representation. (Note: See also my previous question )",,"['real-analysis', 'measure-theory', 'power-series']"
76,Does $\mathcal{L}^2(\mathbb{R})$ form a metric space with this distance/similarity measure?,Does  form a metric space with this distance/similarity measure?,\mathcal{L}^2(\mathbb{R}),"Consider the set $\mathcal{L}^2(\mathbb{R})$, where two functions $f$ and $g$ are said to be equal, if they agree almost everywhere. I would like to define a distance/similarity measure and would like to study its properties such as whether it would along with this set, forms a metric space. Let $f$,$g$ be two functions on this set. We define a normalized cross correlation function as  $$h(x) = \frac{\int \limits_{-\infty}^{\infty}f(t)g(t+x)\mathrm dt}{\int \limits_{-\infty}^{\infty}f(t)^2\mathrm dt \int \limits_{-\infty}^{\infty}g(t)^2\mathrm dt}$$ and our distance measure is given as $$d(f,g) = \frac{1}{\sup_x |h(x)|} - 1.$$ What I am interested to know is whether this set along with this distance measure qualifies as a metric space and also whether it has any interesting properties. Modification (as it doesnt sound interesting in this current form) I hope its ok in this special case. First we define an equivalence class and then modify the metric (infact would like to call it a similarity rather than metric and disacuss its properties and ask whether it would be modified to a metric if necessary) Equivalence class : Let $f$ be a function and we define its equivalence class as $\{kf_{\theta}\}$, $\require{enclose}      \enclose{horizontalstrike}{$\theta \in [0,\pi]}$, $\theta \in [-\pi,\pi], k \in \mathbb{R}\setminus \{0\}$ and $$f_{\theta} = f \cos(\theta) + f_h \sin(\theta)$$ where $f_h$ is the hilbert transform of $f$. Similarity measure (metric) let $$h(x) = \frac{\int \limits_{-\infty}^{\infty}f(t)g(t+x)\mathrm dt}{\sqrt{\int \limits_{-\infty}^{\infty}f(t)^2\mathrm dt \int \limits_{-\infty}^{\infty}g(t)^2\mathrm dt}}.$$ Now define $$H(x) = \sqrt{h(x)^2 + h_h(x)^2}$$ where $h_h(x)$ is the hilbert transform of $h(x)$. and our similarity measure is given as $$ s(f,g) = \sup_x H(x). $$ Define metric as $$d(f,g) =-\log(s(f,g)).$$ Is $d(f,g)$ a metric in the usual sense? Does this form a metric space? PS : We assume sufficient regularity requirements on this space for the Hilbert transform to exist.","Consider the set $\mathcal{L}^2(\mathbb{R})$, where two functions $f$ and $g$ are said to be equal, if they agree almost everywhere. I would like to define a distance/similarity measure and would like to study its properties such as whether it would along with this set, forms a metric space. Let $f$,$g$ be two functions on this set. We define a normalized cross correlation function as  $$h(x) = \frac{\int \limits_{-\infty}^{\infty}f(t)g(t+x)\mathrm dt}{\int \limits_{-\infty}^{\infty}f(t)^2\mathrm dt \int \limits_{-\infty}^{\infty}g(t)^2\mathrm dt}$$ and our distance measure is given as $$d(f,g) = \frac{1}{\sup_x |h(x)|} - 1.$$ What I am interested to know is whether this set along with this distance measure qualifies as a metric space and also whether it has any interesting properties. Modification (as it doesnt sound interesting in this current form) I hope its ok in this special case. First we define an equivalence class and then modify the metric (infact would like to call it a similarity rather than metric and disacuss its properties and ask whether it would be modified to a metric if necessary) Equivalence class : Let $f$ be a function and we define its equivalence class as $\{kf_{\theta}\}$, $\require{enclose}      \enclose{horizontalstrike}{$\theta \in [0,\pi]}$, $\theta \in [-\pi,\pi], k \in \mathbb{R}\setminus \{0\}$ and $$f_{\theta} = f \cos(\theta) + f_h \sin(\theta)$$ where $f_h$ is the hilbert transform of $f$. Similarity measure (metric) let $$h(x) = \frac{\int \limits_{-\infty}^{\infty}f(t)g(t+x)\mathrm dt}{\sqrt{\int \limits_{-\infty}^{\infty}f(t)^2\mathrm dt \int \limits_{-\infty}^{\infty}g(t)^2\mathrm dt}}.$$ Now define $$H(x) = \sqrt{h(x)^2 + h_h(x)^2}$$ where $h_h(x)$ is the hilbert transform of $h(x)$. and our similarity measure is given as $$ s(f,g) = \sup_x H(x). $$ Define metric as $$d(f,g) =-\log(s(f,g)).$$ Is $d(f,g)$ a metric in the usual sense? Does this form a metric space? PS : We assume sufficient regularity requirements on this space for the Hilbert transform to exist.",,"['real-analysis', 'functional-analysis', 'metric-spaces']"
77,Question about proof of Countable subadditivity of Lebesgue outer measure,Question about proof of Countable subadditivity of Lebesgue outer measure,,"I'm studying real analysis from the Terence Tao book. In Exercise 1.2.3. (iii) they ask the reader to prove subadditivity of Lebesgue outer measure. It mentions the proof should use the axiom of choice, Tonelli's theorem for series, and $\epsilon/2^n$ trick. The proof I came up with used only tonelli's theorem, and seemed almost immediate. So there's probably something wrong with it... Given $\{E_n\}_{n=1}^\infty$ sets in $\mathbb{R}^d$ $$m^*(\cup_{n=1}^\infty E_n) = \inf \sum_{n=1}^\infty |B_n| $$ over all covers of $\cup_{n=1}^\infty E_n$ by a countable set of boxes $\{B_n\}_{n=1}^\infty$ . Given any cover of each of individual sets $E_n$ by a sequence of boxes $\{B_{n,m}\}_{m=1}^\infty$ , we have $\cup_{n=1}^\infty E_n \subseteq \cup_{n,m=1}^\infty B_{n,m}$ making it a cover included in the infimum above. Therefore $$  \inf \sum_{n=1}^\infty |B_n| \le \inf \sum_{n,m=1}^\infty |B_{n,m}| = \inf \sum_{n=1}^\infty \sum_{m=1}^\infty |B_{n,m}| = \sum_{n=1}^\infty m^*(E_n) $$ The inequality is due to the right side infimum being over the same function but over a set contained in the one considered by the left side infimum. The first equality is due to Tonelli's theorem for series. The second equality is direct from the definition of $m^*$ .","I'm studying real analysis from the Terence Tao book. In Exercise 1.2.3. (iii) they ask the reader to prove subadditivity of Lebesgue outer measure. It mentions the proof should use the axiom of choice, Tonelli's theorem for series, and trick. The proof I came up with used only tonelli's theorem, and seemed almost immediate. So there's probably something wrong with it... Given sets in over all covers of by a countable set of boxes . Given any cover of each of individual sets by a sequence of boxes , we have making it a cover included in the infimum above. Therefore The inequality is due to the right side infimum being over the same function but over a set contained in the one considered by the left side infimum. The first equality is due to Tonelli's theorem for series. The second equality is direct from the definition of .","\epsilon/2^n \{E_n\}_{n=1}^\infty \mathbb{R}^d m^*(\cup_{n=1}^\infty E_n) = \inf \sum_{n=1}^\infty |B_n|  \cup_{n=1}^\infty E_n \{B_n\}_{n=1}^\infty E_n \{B_{n,m}\}_{m=1}^\infty \cup_{n=1}^\infty E_n \subseteq \cup_{n,m=1}^\infty B_{n,m}  
\inf \sum_{n=1}^\infty |B_n| \le \inf \sum_{n,m=1}^\infty |B_{n,m}| = \inf \sum_{n=1}^\infty \sum_{m=1}^\infty |B_{n,m}| = \sum_{n=1}^\infty m^*(E_n)
 m^*","['real-analysis', 'measure-theory']"
78,Convergence of $\int_0^1 \frac{\sqrt{x-x^2}\ln(1-x)}{\sin{\pi x^2}} \mathrm{d}x.$,Convergence of,\int_0^1 \frac{\sqrt{x-x^2}\ln(1-x)}{\sin{\pi x^2}} \mathrm{d}x.,"I would like to prove the convergence of the Newton integral $$\int_0^1 f(x)\mathrm{d}x =\int_0^1 \frac{\sqrt{x-x^2}\ln(1-x)}{\sin{\pi x^2}} \mathrm{d}x.$$ I split this into two integrals $\displaystyle\int_0^\epsilon f(x)\mathrm{d}x$ and $\displaystyle\int_\epsilon^1 f(x)\mathrm{d}x$. It is easy to show that the integral is convergent on $(0, \epsilon]$ by limit comparison with $\displaystyle\int_0^\epsilon \frac{\sqrt{x}x}{\pi x^2}\mathrm{d}x$. But I cannot find anything to compare with around $x = 1$ on $[\epsilon, 1)$.","I would like to prove the convergence of the Newton integral $$\int_0^1 f(x)\mathrm{d}x =\int_0^1 \frac{\sqrt{x-x^2}\ln(1-x)}{\sin{\pi x^2}} \mathrm{d}x.$$ I split this into two integrals $\displaystyle\int_0^\epsilon f(x)\mathrm{d}x$ and $\displaystyle\int_\epsilon^1 f(x)\mathrm{d}x$. It is easy to show that the integral is convergent on $(0, \epsilon]$ by limit comparison with $\displaystyle\int_0^\epsilon \frac{\sqrt{x}x}{\pi x^2}\mathrm{d}x$. But I cannot find anything to compare with around $x = 1$ on $[\epsilon, 1)$.",,"['real-analysis', 'integration', 'convergence-divergence', 'improper-integrals']"
79,Revisited$^2$: Why is $(-n)^2$ divergent? How can it be shown rigorously?,Revisited: Why is  divergent? How can it be shown rigorously?,^2 (-n)^2,"Why is $(-n)^2$ divergent? How is this proven? I've tried using the $\epsilon$ definition of convergence to come to a contradiction, but I don't know that using the definition is the way to go. I get that $n^2-s\leq \frac{1}{n}$. Not sure where to go from here. A hint would be nice. Attempt1: Say $\lim_{n\rightarrow\infty}=(-n)^2=s$. Then given an $\epsilon>0$ we can find an $N\in\mathbb{N}$ so that $\lvert (-n)^2-s\rvert \leq \epsilon$ for every $n\geq N$. But if $n\geq N$, then we must have that $$\lvert (-n)^2-s\rvert\leq \frac{1}{n}\leq\frac{1}{N},$$ but . . . hmm . . . I don't know that this is a fruitful approach. Attempt2: Assuming that the negation of convergence is $$\text{$\exists\epsilon\leq 0$ s.t. $\forall N\in\mathbb{N}$ $n>N$ so that $\lvert s_n-s\rvert \geq \epsilon$},$$ then if we let $\epsilon=0$, then $\lvert s_n-s\rvert \geq \epsilon\implies \lvert n^2-s\rvert \geq 0$, which means that $n^2\geq s$ and $s\geq n^2$, so $s=n^2$, which means $\lvert s_n-s\rvert \geq \epsilon$ holds as $0\geq 0$, right?","Why is $(-n)^2$ divergent? How is this proven? I've tried using the $\epsilon$ definition of convergence to come to a contradiction, but I don't know that using the definition is the way to go. I get that $n^2-s\leq \frac{1}{n}$. Not sure where to go from here. A hint would be nice. Attempt1: Say $\lim_{n\rightarrow\infty}=(-n)^2=s$. Then given an $\epsilon>0$ we can find an $N\in\mathbb{N}$ so that $\lvert (-n)^2-s\rvert \leq \epsilon$ for every $n\geq N$. But if $n\geq N$, then we must have that $$\lvert (-n)^2-s\rvert\leq \frac{1}{n}\leq\frac{1}{N},$$ but . . . hmm . . . I don't know that this is a fruitful approach. Attempt2: Assuming that the negation of convergence is $$\text{$\exists\epsilon\leq 0$ s.t. $\forall N\in\mathbb{N}$ $n>N$ so that $\lvert s_n-s\rvert \geq \epsilon$},$$ then if we let $\epsilon=0$, then $\lvert s_n-s\rvert \geq \epsilon\implies \lvert n^2-s\rvert \geq 0$, which means that $n^2\geq s$ and $s\geq n^2$, so $s=n^2$, which means $\lvert s_n-s\rvert \geq \epsilon$ holds as $0\geq 0$, right?",,"['real-analysis', 'sequences-and-series', 'limits', 'inequality']"
80,Trigonometric Identities for $\sin nx$ and $\cos nx $,Trigonometric Identities for  and,\sin nx \cos nx ,"These are generalizations of simple trigonometric identities for $\sin 2x$ and $\cos 2x$, but in general how can we prove them?  $$\sin nx =\sum_{k=1}^{\left\lceil\frac{n}{2}\right\rceil}(-1)^{k-1}\binom{n}{2k-1}\sin^{2k-1}x\cos^{n-2k+1}x,$$ $$\cos nx =\sum_{k=0}^{\left\lfloor\frac{n}{2}\right\rfloor}(-1)^{k}\binom{n}{2k}\sin^{2k}x\cos^{n-2k}x,$$ $$\sin nx =\sin x\left(\sum_{k=1}^{\left\lceil\frac n 2\right\rceil}(-1)^{k-1}\binom{n-k}{k-1}(2\cos x)^{n-2k+1}\right),$$ $$\cos nx =\cos x\left((2\cos x)^{n-1}+\sum_{k=1}^{\left\lfloor\frac n 2\right\rfloor}\frac n k(-1)^k\binom{n-k-1}{k-1}(2\cos x)^{n-2k-1}\right).$$","These are generalizations of simple trigonometric identities for $\sin 2x$ and $\cos 2x$, but in general how can we prove them?  $$\sin nx =\sum_{k=1}^{\left\lceil\frac{n}{2}\right\rceil}(-1)^{k-1}\binom{n}{2k-1}\sin^{2k-1}x\cos^{n-2k+1}x,$$ $$\cos nx =\sum_{k=0}^{\left\lfloor\frac{n}{2}\right\rfloor}(-1)^{k}\binom{n}{2k}\sin^{2k}x\cos^{n-2k}x,$$ $$\sin nx =\sin x\left(\sum_{k=1}^{\left\lceil\frac n 2\right\rceil}(-1)^{k-1}\binom{n-k}{k-1}(2\cos x)^{n-2k+1}\right),$$ $$\cos nx =\cos x\left((2\cos x)^{n-1}+\sum_{k=1}^{\left\lfloor\frac n 2\right\rfloor}\frac n k(-1)^k\binom{n-k-1}{k-1}(2\cos x)^{n-2k-1}\right).$$",,"['real-analysis', 'complex-analysis', 'algebra-precalculus', 'trigonometry', 'chebyshev-polynomials']"
81,"How to show that $\{\sqrt m - \sqrt n : m,n \in \mathbb N\}$ is dense in $\mathbb R$? [duplicate]",How to show that  is dense in ? [duplicate],"\{\sqrt m - \sqrt n : m,n \in \mathbb N\} \mathbb R","This question already has answers here : Prove that the following set is dense (3 answers) Closed 10 years ago . Show that $S=\{\sqrt m - \sqrt n : m,n \in  \mathbb N\}$ is dense in $\mathbb R$. I know the definition of dense as: A set S is dense in $\mathbb R$ if there exists $a,b \in \mathbb R$ such that $S \cap(a,b) \neq \emptyset.$ I don't understand how to proceed. Please help.","This question already has answers here : Prove that the following set is dense (3 answers) Closed 10 years ago . Show that $S=\{\sqrt m - \sqrt n : m,n \in  \mathbb N\}$ is dense in $\mathbb R$. I know the definition of dense as: A set S is dense in $\mathbb R$ if there exists $a,b \in \mathbb R$ such that $S \cap(a,b) \neq \emptyset.$ I don't understand how to proceed. Please help.",,"['real-analysis', 'general-topology']"
82,Proof of Fourier Inverse formula for $L^1$ case,Proof of Fourier Inverse formula for  case,L^1,"I know this may be a stupid question, but still hope someone can help me. I was trying to prove the Fourier inversion formula for which $f$ and $\hat{f}=\int_{\mathbb{R}}f(x)e^{-i2\pi xy}dx$ both lie in $L^1$, and then $f=\int_{\mathbb{R}}\hat{f}(y)e^{i2\pi xy}dy$. I assume the knowing of FT of the Gaussian function $g_c(x)=e^{-c\pi x^2}$, which is $\frac{1}{\sqrt{c}}e^{-\frac{1}{c}\pi x^2}$. Then I have $$ \int_{\mathbb{R}}\hat{f}(y)e^{i2\pi xy}dy=\lim_{c\to0}\int_{\mathbb{R}}\hat{f}(y)g_c(y)e^{i2\pi xy}dy\\ $$ Using the definition of $\hat{f}$ and Fubini, this becomes $$ \lim_{c\to0}\int_{\mathbb{R}}\int_{\mathbb{R}}f(t)e^{-i2\pi ty}dtg_c(y)e^{i2\pi xy}dy= \lim_{c\to0}\int_{\mathbb{R}}f(t)\hat{g}_c(t-x)dt $$ Now I came to the problem. Formally one can guess that $\hat{g}_c(t)$ is just the Dirac function as $c\to0$ so that convolution gives $f$ itself, but how can one show that strictly, especially for an $L^1$ function? On the other hand, I also directly calculated $g_c(t-x)$ which gives me $$ \lim_{c\to0}\int_{\mathbb{R}}f(t)\frac{1}{\sqrt{c}}e^{-\frac{1}{c}\pi(t-x)^2}dt. $$Change of variables gives me $$ \lim_{c\to0}\int_{\mathbb{R}}f(x-\sqrt{c}y)e^{-\pi y^2}dy $$ Then why is it okay for me to pass the limit directly to $f$, since $f$ is just absolutely integrable instead of being continuous? I know this seems true formally, but, again, how can one show this strictly? Thank you for help!","I know this may be a stupid question, but still hope someone can help me. I was trying to prove the Fourier inversion formula for which $f$ and $\hat{f}=\int_{\mathbb{R}}f(x)e^{-i2\pi xy}dx$ both lie in $L^1$, and then $f=\int_{\mathbb{R}}\hat{f}(y)e^{i2\pi xy}dy$. I assume the knowing of FT of the Gaussian function $g_c(x)=e^{-c\pi x^2}$, which is $\frac{1}{\sqrt{c}}e^{-\frac{1}{c}\pi x^2}$. Then I have $$ \int_{\mathbb{R}}\hat{f}(y)e^{i2\pi xy}dy=\lim_{c\to0}\int_{\mathbb{R}}\hat{f}(y)g_c(y)e^{i2\pi xy}dy\\ $$ Using the definition of $\hat{f}$ and Fubini, this becomes $$ \lim_{c\to0}\int_{\mathbb{R}}\int_{\mathbb{R}}f(t)e^{-i2\pi ty}dtg_c(y)e^{i2\pi xy}dy= \lim_{c\to0}\int_{\mathbb{R}}f(t)\hat{g}_c(t-x)dt $$ Now I came to the problem. Formally one can guess that $\hat{g}_c(t)$ is just the Dirac function as $c\to0$ so that convolution gives $f$ itself, but how can one show that strictly, especially for an $L^1$ function? On the other hand, I also directly calculated $g_c(t-x)$ which gives me $$ \lim_{c\to0}\int_{\mathbb{R}}f(t)\frac{1}{\sqrt{c}}e^{-\frac{1}{c}\pi(t-x)^2}dt. $$Change of variables gives me $$ \lim_{c\to0}\int_{\mathbb{R}}f(x-\sqrt{c}y)e^{-\pi y^2}dy $$ Then why is it okay for me to pass the limit directly to $f$, since $f$ is just absolutely integrable instead of being continuous? I know this seems true formally, but, again, how can one show this strictly? Thank you for help!",,"['real-analysis', 'fourier-analysis']"
83,"The union of a sequence of infinite, countable sets is countable.","The union of a sequence of infinite, countable sets is countable.",,"While reading Walter Rudin's Principles of Mathematical Analysis , I ran into the following theorem and proof: Theorem 2.12. Let $\left\{E_n\right\}$, $n=1,2,\dots$, be a sequence of countable sets, and put $$ S=\bigcup_{n=1}^\infty E_n. $$ Then $S$ is countable. Proof. Let every set $E_n$ be arranged in a sequence $\left\{X_{nk}\right\}$, $k=1,2,3,\dots$, and consider the infinite array in which the elements of $E_n$ form the $n$th row. The array contains all elements of $S$. As indicated by the arrows, these elements can be arranged in a sequence $$ x_{11};x_{21},x_{12};x_{31},x_{22},x_{13};x_{41},x_{32},x_{23},x_{14};\dots\tag{*} $$ If any two of the sets $E_n$ have elements in common, these will appear more than once in $(*)$. Hence there is a subset $T$ of the set of all positive integers such that $S\sim T$, which shows that $S$ is at most countable. Since $E_1\subset S$, and $E_1$ is infinite, $S$ is infinite, and thus countable. $\blacksquare$ How does the bolded sentence follow from all previous? In fact, I do not know how the matrix and $(*)$ come into play.","While reading Walter Rudin's Principles of Mathematical Analysis , I ran into the following theorem and proof: Theorem 2.12. Let $\left\{E_n\right\}$, $n=1,2,\dots$, be a sequence of countable sets, and put $$ S=\bigcup_{n=1}^\infty E_n. $$ Then $S$ is countable. Proof. Let every set $E_n$ be arranged in a sequence $\left\{X_{nk}\right\}$, $k=1,2,3,\dots$, and consider the infinite array in which the elements of $E_n$ form the $n$th row. The array contains all elements of $S$. As indicated by the arrows, these elements can be arranged in a sequence $$ x_{11};x_{21},x_{12};x_{31},x_{22},x_{13};x_{41},x_{32},x_{23},x_{14};\dots\tag{*} $$ If any two of the sets $E_n$ have elements in common, these will appear more than once in $(*)$. Hence there is a subset $T$ of the set of all positive integers such that $S\sim T$, which shows that $S$ is at most countable. Since $E_1\subset S$, and $E_1$ is infinite, $S$ is infinite, and thus countable. $\blacksquare$ How does the bolded sentence follow from all previous? In fact, I do not know how the matrix and $(*)$ come into play.",,['real-analysis']
84,"Show that $\int_0^\infty e^{-(x-u/x)^2}(1-\frac{u}{x^2})dx$ converges uniformly on $u\in [\delta,L]$",Show that  converges uniformly on,"\int_0^\infty e^{-(x-u/x)^2}(1-\frac{u}{x^2})dx u\in [\delta,L]","I would like to show that $$\int_0^\infty e^{-(x-u/x)^2}\left(1-\frac{u}{x^2}\right)dx$$ converges uniformly on, say, $u\in [\delta,L]$, for arbitrarily small $\delta>0$ and arbitrarily large $L>0$. The important thing is that we can fit any $u>0$ into some interval on which the integral converges uniformly. If we can major the exponential by some appropriate exponential in terms of $x$, then the rest is taken care of. But I am having a hard time doing so. For instance, I may try $e^{-(x-u/x)^2}\leq e^{-x^2}$, but it is not clear that this is true for instance when both $u$ and $x$ are close to zero. Any ideas?","I would like to show that $$\int_0^\infty e^{-(x-u/x)^2}\left(1-\frac{u}{x^2}\right)dx$$ converges uniformly on, say, $u\in [\delta,L]$, for arbitrarily small $\delta>0$ and arbitrarily large $L>0$. The important thing is that we can fit any $u>0$ into some interval on which the integral converges uniformly. If we can major the exponential by some appropriate exponential in terms of $x$, then the rest is taken care of. But I am having a hard time doing so. For instance, I may try $e^{-(x-u/x)^2}\leq e^{-x^2}$, but it is not clear that this is true for instance when both $u$ and $x$ are close to zero. Any ideas?",,"['calculus', 'real-analysis', 'integration', 'uniform-convergence']"
85,Check my answer: Prove that every open set in $\Bbb R^n$ is a countable union of open intervals.,Check my answer: Prove that every open set in  is a countable union of open intervals.,\Bbb R^n,"I have a question. I have solved this but please can you check my solution? Thank you. If there are any mistakes or something is missing and so on, please tell me. This is important to me. Is this proof enough to get a successful grade on an exam? Btw, I underlined the question with pink a pencil.","I have a question. I have solved this but please can you check my solution? Thank you. If there are any mistakes or something is missing and so on, please tell me. This is important to me. Is this proof enough to get a successful grade on an exam? Btw, I underlined the question with pink a pencil.",,"['real-analysis', 'general-topology', 'analysis', 'proof-verification']"
86,"How to show that for any real numbers $x,y,z$ $|x|+|y|+|z|\le|x+y-z|+|y+z-x|+|z+x-y|?$",How to show that for any real numbers,"x,y,z |x|+|y|+|z|\le|x+y-z|+|y+z-x|+|z+x-y|?","How to show that for any real numbers $x,y,z$$$|x|+|y|+|z|\le|x+y-z|+|y+z-x|+|z+x-y|?$$ I'm don't know how to split RHS.","How to show that for any real numbers $x,y,z$$$|x|+|y|+|z|\le|x+y-z|+|y+z-x|+|z+x-y|?$$ I'm don't know how to split RHS.",,['real-analysis']
87,Lebesgue measure zero set of cardinality $\mathfrak c$,Lebesgue measure zero set of cardinality,\mathfrak c,"Suppose $A\subset\mathbb  R$ is a Lebesgue measure zero set. Must $\mathbb R\setminus A$ has cardinality $\mathfrak c$? If so, does there exist another Lebesgue measure zero set $B$ of cardinality $\mathfrak c$ such that $A\cap B=\emptyset$?","Suppose $A\subset\mathbb  R$ is a Lebesgue measure zero set. Must $\mathbb R\setminus A$ has cardinality $\mathfrak c$? If so, does there exist another Lebesgue measure zero set $B$ of cardinality $\mathfrak c$ such that $A\cap B=\emptyset$?",,"['real-analysis', 'measure-theory', 'elementary-set-theory']"
88,"If $K$ is compact, then $C(K,\mathbb{R}^n)$ is a Banach space under the norm $\|f\|=\sup_{x\in K} \|f(x)\|$","If  is compact, then  is a Banach space under the norm","K C(K,\mathbb{R}^n) \|f\|=\sup_{x\in K} \|f(x)\|","Let $K$ be a topological space that is compact. Show that the space $C(K,\mathbb{R}^n)$ of all the continuous functions $f:K\to\mathbb{R}^n$ is a Banach space with the norm $\|f\|=\sup_{x\in K} \|f(x)\|$. Below is my attempt: $\|f\|=\sup_{x\in K} \|f(x)\|$ is a norm: i. Suppose that $\|f\|=\sup_{x\in K} \|f(x)\|= 0$; then $f=0$. Conversely, suppose that $f=0$. Then $0=\|f\|=\sup_{x\in K} \|f(x)\|$. ii. $\|\alpha f\|=\sup_{x\in K} \|\alpha f(x)\|=\sup_{x\in K} |\alpha|\|f(x)\|=|\alpha|\sup_{x\in K} \|f(x)\|=|\alpha|\|f\|$ iii. $\|f+g\|= \sup_{x\in K} \|f(x)+g(x)\|\leq \sup_{x\in K} \|f(x)\| + \|g(x)\|= \sup_{x\in K} \|f(x)\| + \sup_{x\in K} g(x)\|$ $C(K,\mathbb{R}^n)$ is complete (my doubt is here) i. For all $\epsilon>0$, $\exists\, n_{0}$ such that if $m,n>n_{0}$ then $d(f_m,f_n)<\epsilon$. We know that $|f_m-f_n|\leq \|f_m -f_n\|= \sup_{x\in K} \|f_m(x)-f_n(x)\|< \epsilon $ but how can I prove the last inequality? ii. How to prove that $f_n$ converges to an $f$ in $C(K,\mathbb{R}^n)$?","Let $K$ be a topological space that is compact. Show that the space $C(K,\mathbb{R}^n)$ of all the continuous functions $f:K\to\mathbb{R}^n$ is a Banach space with the norm $\|f\|=\sup_{x\in K} \|f(x)\|$. Below is my attempt: $\|f\|=\sup_{x\in K} \|f(x)\|$ is a norm: i. Suppose that $\|f\|=\sup_{x\in K} \|f(x)\|= 0$; then $f=0$. Conversely, suppose that $f=0$. Then $0=\|f\|=\sup_{x\in K} \|f(x)\|$. ii. $\|\alpha f\|=\sup_{x\in K} \|\alpha f(x)\|=\sup_{x\in K} |\alpha|\|f(x)\|=|\alpha|\sup_{x\in K} \|f(x)\|=|\alpha|\|f\|$ iii. $\|f+g\|= \sup_{x\in K} \|f(x)+g(x)\|\leq \sup_{x\in K} \|f(x)\| + \|g(x)\|= \sup_{x\in K} \|f(x)\| + \sup_{x\in K} g(x)\|$ $C(K,\mathbb{R}^n)$ is complete (my doubt is here) i. For all $\epsilon>0$, $\exists\, n_{0}$ such that if $m,n>n_{0}$ then $d(f_m,f_n)<\epsilon$. We know that $|f_m-f_n|\leq \|f_m -f_n\|= \sup_{x\in K} \|f_m(x)-f_n(x)\|< \epsilon $ but how can I prove the last inequality? ii. How to prove that $f_n$ converges to an $f$ in $C(K,\mathbb{R}^n)$?",,"['real-analysis', 'functional-analysis']"
89,Analogy between Integration and Summation,Analogy between Integration and Summation,,"There are many analogies between definite integral and Summation: $$\int_a^b \leftrightarrow \sum_a^b$$, This makes me wonder if there is analogous concept of indefinite integral, derivative and anythings like that? More generally, is there any ""discrete version"" of calculus?? thanks.","There are many analogies between definite integral and Summation: $$\int_a^b \leftrightarrow \sum_a^b$$, This makes me wonder if there is analogous concept of indefinite integral, derivative and anythings like that? More generally, is there any ""discrete version"" of calculus?? thanks.",,"['calculus', 'real-analysis', 'soft-question']"
90,Does an absolutely integrable function tend to $0$ as its argument tends to infinity?,Does an absolutely integrable function tend to  as its argument tends to infinity?,0,"Suppose that $f:[0,\infty)\rightarrow\mathbb{R}$ is continuous. Is it true that $$\int_{0}^\infty|f(t)|dt<\infty\Rightarrow \lim_{t\rightarrow\infty}f(t)=0?$$ If so can you provide a proof, otherwise a counter example. Thank you.","Suppose that $f:[0,\infty)\rightarrow\mathbb{R}$ is continuous. Is it true that $$\int_{0}^\infty|f(t)|dt<\infty\Rightarrow \lim_{t\rightarrow\infty}f(t)=0?$$ If so can you provide a proof, otherwise a counter example. Thank you.",,"['calculus', 'real-analysis', 'measure-theory']"
91,Prove that $f^{(n)}(x) = 0$ for some $x$.,Prove that  for some .,f^{(n)}(x) = 0 x,"Suppose that $f : \mathbb{R} \rightarrow \mathbb{R}$ is $n$-times differentiable and $f$ has $n+1$ distinct zeros. Prove that $f^{(n)}(x) = 0$ for some $x$. My attempt: Prove by induction For $n=1$, we have $f$ is differentiable and has $2$ distinct zeros. Let the two zeros be $a$ and $b$. Hence, we have $f(a)=f(b)$. By Rolle's Theorem, there exists $x \in (a,b)$ such that $f^{\prime}(x)=0$ Suppose that $f^{(n-1)}(x) = 0$ is true. Then I stuck here. Can anyone guide me ?","Suppose that $f : \mathbb{R} \rightarrow \mathbb{R}$ is $n$-times differentiable and $f$ has $n+1$ distinct zeros. Prove that $f^{(n)}(x) = 0$ for some $x$. My attempt: Prove by induction For $n=1$, we have $f$ is differentiable and has $2$ distinct zeros. Let the two zeros be $a$ and $b$. Hence, we have $f(a)=f(b)$. By Rolle's Theorem, there exists $x \in (a,b)$ such that $f^{\prime}(x)=0$ Suppose that $f^{(n-1)}(x) = 0$ is true. Then I stuck here. Can anyone guide me ?",,['real-analysis']
92,Which books can I study (learn) better the topics about convergence in $\Bbb R^{n}$ and euclidean space?,Which books can I study (learn) better the topics about convergence in  and euclidean space?,\Bbb R^{n},I have question. Which books can I study (learn) better the topics about convergence in $\Bbb R^{n}$ and euclidean space ? Please can you give me an advice some book names?  Thank you!,I have question. Which books can I study (learn) better the topics about convergence in $\Bbb R^{n}$ and euclidean space ? Please can you give me an advice some book names?  Thank you!,,"['calculus', 'real-analysis', 'reference-request', 'metric-spaces', 'euclidean-geometry']"
93,"Completeness of $\langle \mathscr{C} [0, 1], \| \cdot \|_1 \rangle$",Completeness of,"\langle \mathscr{C} [0, 1], \| \cdot \|_1 \rangle","That's really embarrassing, however I need to ask it. I could not prove that the normed space    $\langle \mathscr{C} [0, 1], \| \cdot \|_1 \rangle $ is complete (as a metric space), where $\| f\|_1 = \int_0^1 |f(x)|dx$. I don't really know how to explicit a continuous functions as a limit of a Cauchy sequence. I tried to prove that the pointwise limit (of the Cauchy sequence) have just a finite number of discontinuities (I don't know if it's true) and then approximate it by a continuous function. Thanks in advance.","That's really embarrassing, however I need to ask it. I could not prove that the normed space    $\langle \mathscr{C} [0, 1], \| \cdot \|_1 \rangle $ is complete (as a metric space), where $\| f\|_1 = \int_0^1 |f(x)|dx$. I don't really know how to explicit a continuous functions as a limit of a Cauchy sequence. I tried to prove that the pointwise limit (of the Cauchy sequence) have just a finite number of discontinuities (I don't know if it's true) and then approximate it by a continuous function. Thanks in advance.",,"['real-analysis', 'functional-analysis', 'banach-spaces']"
94,Prove that $\liminf x_n = -\limsup (-x_n)$,Prove that,\liminf x_n = -\limsup (-x_n),How can we prove that $\liminf x_n = -\limsup (-x_n)$? The definitions we are using are $\limsup x_n = \lim\limits_{n\to\infty} \sup\{x_k; k\ge n\}$ $\liminf x_n = \lim\limits_{n\to\infty} \inf\{x_k; k\ge n\}$,How can we prove that $\liminf x_n = -\limsup (-x_n)$? The definitions we are using are $\limsup x_n = \lim\limits_{n\to\infty} \sup\{x_k; k\ge n\}$ $\liminf x_n = \lim\limits_{n\to\infty} \inf\{x_k; k\ge n\}$,,"['real-analysis', 'limits', 'limsup-and-liminf']"
95,How to derive a union of sets as a disjoint union?,How to derive a union of sets as a disjoint union?,,"$$\bigcup_{n=1}^\infty A_n = \bigcup_{n=1}^\infty (A_{1}^c \cap\cdots\cap A_{n-1}^c \cap A_n)$$ The results is obvious enough, but how to prove this","$$\bigcup_{n=1}^\infty A_n = \bigcup_{n=1}^\infty (A_{1}^c \cap\cdots\cap A_{n-1}^c \cap A_n)$$ The results is obvious enough, but how to prove this",,"['real-analysis', 'measure-theory', 'elementary-set-theory']"
96,Schwarz Reflection Prinnciple for (Real) Harmonic Functions,Schwarz Reflection Prinnciple for (Real) Harmonic Functions,,"Assume $u$ is harmonic in $U^{+}$, $u\equiv0$ on $\partial U^{+}\cap\mathbb{R}^{n}_{+}$, and $u\in\mathscr{C}^{2}(\bar{U}^{+})$, where $U$ is the open ball $B_{1}(0)$ of radius $1$ about the origin in $\mathbb{R}^{n}$, $U^{+}$ being the upper half-ball: $U^{+}:=U\cap\partial\mathbb{R}^{n}_{+}.$ (This is problem #2.5.-something in Evans PDE text). We want to show (under the assumed regularity of $u$) that the odd extension of $u$ into $U^{-}$ provides us a with a harmonic function on all of $U$.  That is, if $v=u$ in $U^{+}$, $v=0$ on $\partial U\cap\mathbb{R}^{n}_{\pm}$, and $v=-u(-x)$ in $U^{-}$, then $v$ is harmonic and $\mathscr{C}^{2}$ in all of $U$. Okay, it is obvious $v$ is $\mathscr{C}^{2}$ in the separated sets $U^{+}$ and $U^{-}$.  Since $u$ is $\mathscr{C}^{2}$ upto the boundary of $U^{+}$ (in particular upto $\bar{U}\cap\mathbb{R}^{n}_{+}$), then it is also clear that $v$ is $\mathscr{C}^{2}$ in all of $\bar{U}$.  We also see that $v$ satisfies the mean-value-properties in $U^{+}$ and $U^{-}$, and also on $U\cap\mathbb{R}^{n}_{\pm}$ because of the odd symmetry. Here's my problem, and of all the proofs I have seen, this is overlooked.  The mean-value properties of $u$ are satisfied on $U^{+}$, $U^{-}$ and $\partial U\cap\mathbb{R}^{n}_{+}$, yes.  But only when viewed individually.  How do you use the fact that $v\in\mathscr{C}^{2}(\bar{U})$ to then show that the mean-value property is satisfied in all of $U$ (not just the three aforementioned sets when the spherical averages are restricted to the individuals sets).  In other words, how do you justify the extending of a spherical average across the three sets (say at a point $x\in U^{+}$ with radius sufficiently large to intersect all three sets, but sufficiently small to remain in $U$). I will reiterate this: every proof I have seen does not make explicit reference to the $\mathscr{C}^{2}$ regularity of $v$.  If the mean-value property can be demonstrated without $\mathscr{C}^{2}$ regularity, then all one needs is $\mathscr{C}$ regularity (not even differentiability) of $v$ in order to conclude $v$ is harmonic (it is easy to prove that a continuous function which satisfies the mean-value property at every point in an open set is harmonic there).  But if this were the case, then why would Evans (and other texts where the problem is posed) be insistent on requiring $u$ being $\mathscr{C}^{2}$ in $\bar{U}^{+}$, and thus $v$ $\mathscr{C}^{2}$ in $\bar{U}$? NOTE: In part (b) of this problem, Evans drops the hypothesis that $u$ is $\mathscr{C}^{2}$ upto the boundary, only that $u\in\mathscr{C}^{2}(U^{+})\cap\mathscr{C}(\bar{U})$.  But the suggested proof is entirely different: apply the Poisson integral formula for harmonic functions on a disc.  Indeed, one solves the problem $$\left\{\begin{array}{rl} \Delta w=0&\text{in}\;U\\ w=g&\text{on}\;\partial U,\end{array}\right.$$ where $g(x)=u(x)$ on the upper boundary and $g(x)=-u(-x)$ on the lower boundary.  The solution is given by the Poisson integral formula, and computing $w(x^{+})$ where $x^{+}\in\mathbb{R}^{n}_{+}\cap U$, we find $w(x^{+})=0$.  From uniqueness, we conclude that $w(x)=v$ as above (the odd extension of $u$), and the theorem is proved. Anyway, if anyone could help me fill in the details of the mean-value property argument in the first part, I would appreciate it!","Assume $u$ is harmonic in $U^{+}$, $u\equiv0$ on $\partial U^{+}\cap\mathbb{R}^{n}_{+}$, and $u\in\mathscr{C}^{2}(\bar{U}^{+})$, where $U$ is the open ball $B_{1}(0)$ of radius $1$ about the origin in $\mathbb{R}^{n}$, $U^{+}$ being the upper half-ball: $U^{+}:=U\cap\partial\mathbb{R}^{n}_{+}.$ (This is problem #2.5.-something in Evans PDE text). We want to show (under the assumed regularity of $u$) that the odd extension of $u$ into $U^{-}$ provides us a with a harmonic function on all of $U$.  That is, if $v=u$ in $U^{+}$, $v=0$ on $\partial U\cap\mathbb{R}^{n}_{\pm}$, and $v=-u(-x)$ in $U^{-}$, then $v$ is harmonic and $\mathscr{C}^{2}$ in all of $U$. Okay, it is obvious $v$ is $\mathscr{C}^{2}$ in the separated sets $U^{+}$ and $U^{-}$.  Since $u$ is $\mathscr{C}^{2}$ upto the boundary of $U^{+}$ (in particular upto $\bar{U}\cap\mathbb{R}^{n}_{+}$), then it is also clear that $v$ is $\mathscr{C}^{2}$ in all of $\bar{U}$.  We also see that $v$ satisfies the mean-value-properties in $U^{+}$ and $U^{-}$, and also on $U\cap\mathbb{R}^{n}_{\pm}$ because of the odd symmetry. Here's my problem, and of all the proofs I have seen, this is overlooked.  The mean-value properties of $u$ are satisfied on $U^{+}$, $U^{-}$ and $\partial U\cap\mathbb{R}^{n}_{+}$, yes.  But only when viewed individually.  How do you use the fact that $v\in\mathscr{C}^{2}(\bar{U})$ to then show that the mean-value property is satisfied in all of $U$ (not just the three aforementioned sets when the spherical averages are restricted to the individuals sets).  In other words, how do you justify the extending of a spherical average across the three sets (say at a point $x\in U^{+}$ with radius sufficiently large to intersect all three sets, but sufficiently small to remain in $U$). I will reiterate this: every proof I have seen does not make explicit reference to the $\mathscr{C}^{2}$ regularity of $v$.  If the mean-value property can be demonstrated without $\mathscr{C}^{2}$ regularity, then all one needs is $\mathscr{C}$ regularity (not even differentiability) of $v$ in order to conclude $v$ is harmonic (it is easy to prove that a continuous function which satisfies the mean-value property at every point in an open set is harmonic there).  But if this were the case, then why would Evans (and other texts where the problem is posed) be insistent on requiring $u$ being $\mathscr{C}^{2}$ in $\bar{U}^{+}$, and thus $v$ $\mathscr{C}^{2}$ in $\bar{U}$? NOTE: In part (b) of this problem, Evans drops the hypothesis that $u$ is $\mathscr{C}^{2}$ upto the boundary, only that $u\in\mathscr{C}^{2}(U^{+})\cap\mathscr{C}(\bar{U})$.  But the suggested proof is entirely different: apply the Poisson integral formula for harmonic functions on a disc.  Indeed, one solves the problem $$\left\{\begin{array}{rl} \Delta w=0&\text{in}\;U\\ w=g&\text{on}\;\partial U,\end{array}\right.$$ where $g(x)=u(x)$ on the upper boundary and $g(x)=-u(-x)$ on the lower boundary.  The solution is given by the Poisson integral formula, and computing $w(x^{+})$ where $x^{+}\in\mathbb{R}^{n}_{+}\cap U$, we find $w(x^{+})=0$.  From uniqueness, we conclude that $w(x)=v$ as above (the odd extension of $u$), and the theorem is proved. Anyway, if anyone could help me fill in the details of the mean-value property argument in the first part, I would appreciate it!",,"['real-analysis', 'partial-differential-equations']"
97,Composing two discontinuous functions into a continuous one,Composing two discontinuous functions into a continuous one,,Please help me think of an example of two discontinuous functions on $\mathbb R$ whose composition gives a continuous function on $\mathbb R$.,Please help me think of an example of two discontinuous functions on $\mathbb R$ whose composition gives a continuous function on $\mathbb R$.,,"['real-analysis', 'functions', 'continuity']"
98,"Exercises on $\inf$, $\sup$, $\liminf$ and $\limsup$","Exercises on , ,  and",\inf \sup \liminf \limsup,"I am studying analysis and having difficulties with $\inf$, $\sup$, $\liminf$ and $\limsup$ as you can see from the title. As discussed in a number of math.SE questions, definitions for sequences, functions and sets differ from each other. I sense, I have to tackle with some problems which are really useful for the pedagogic purposes. Also, I want to learn definitions in all senses. So, can anyone suggest a reference which have a number of good exercises? Thanks.","I am studying analysis and having difficulties with $\inf$, $\sup$, $\liminf$ and $\limsup$ as you can see from the title. As discussed in a number of math.SE questions, definitions for sequences, functions and sets differ from each other. I sense, I have to tackle with some problems which are really useful for the pedagogic purposes. Also, I want to learn definitions in all senses. So, can anyone suggest a reference which have a number of good exercises? Thanks.",,['real-analysis']
99,About frullani integral [duplicate],About frullani integral [duplicate],,"This question already has answers here : Closed 11 years ago . Possible Duplicate: Frullani proof integrals Let $f:\left[ {0,\infty } \right] \to \mathbb R$ be a a continuous function such that $$ \mathop {\lim }\limits_{x \to0+ } f\left( x \right) = L $$Prove that $$ \int\limits_0^{\infty}  {\frac{{f\left( {ax} \right) - f\left( {bx} \right)}} {x}}dx $$ converges and calculate the value. It is known that $\int_a^\infty (f(x)/x)\,\mathrm{d}x$ converges for all a>0, but nothing of $\lim\limits_{x\to\infty}f(x)$ is told. Also, what if $a>b$ or $a<b$?","This question already has answers here : Closed 11 years ago . Possible Duplicate: Frullani proof integrals Let $f:\left[ {0,\infty } \right] \to \mathbb R$ be a a continuous function such that $$ \mathop {\lim }\limits_{x \to0+ } f\left( x \right) = L $$Prove that $$ \int\limits_0^{\infty}  {\frac{{f\left( {ax} \right) - f\left( {bx} \right)}} {x}}dx $$ converges and calculate the value. It is known that $\int_a^\infty (f(x)/x)\,\mathrm{d}x$ converges for all a>0, but nothing of $\lim\limits_{x\to\infty}f(x)$ is told. Also, what if $a>b$ or $a<b$?",,"['real-analysis', 'integration']"
