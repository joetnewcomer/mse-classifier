,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Are we unable to find the extrema of the function $f(x,y) = \frac{y}{x^2+y^2}$ using the second partial test?",Are we unable to find the extrema of the function  using the second partial test?,"f(x,y) = \frac{y}{x^2+y^2}","$$\begin{align*} &f(x,y) = \frac{y}{x^2+y^2}\\ &f_{xx} = \frac{∂}{∂x}\left(-\frac{2xy}{(x^2+y^2)^2}\right)=-\frac{2y(y^2-3x^2)}{(x^2+y^2)^3}\\ &f_{yy} = \frac{∂}{∂x}\left(\frac{x^2-y^2}{(x^2+y^2)^2}\right)=\frac{2y(y^2-3x^2)}{(x^2+y^2)^3} \end{align*}$$ $a$ and $b$ must be $(0,0)$ or at least one of them has to be $0$ to be critical points of the function if the point is $(0,0)$, $f_{xx}$ or $f_{yy}$ will be undefined and finding the point $(?,0)$ for which $f_x$ or $f_y$ would $= 0$ would be mighty hard. Are we unable to find the extrema of this function using the second partial test?","$$\begin{align*} &f(x,y) = \frac{y}{x^2+y^2}\\ &f_{xx} = \frac{∂}{∂x}\left(-\frac{2xy}{(x^2+y^2)^2}\right)=-\frac{2y(y^2-3x^2)}{(x^2+y^2)^3}\\ &f_{yy} = \frac{∂}{∂x}\left(\frac{x^2-y^2}{(x^2+y^2)^2}\right)=\frac{2y(y^2-3x^2)}{(x^2+y^2)^3} \end{align*}$$ $a$ and $b$ must be $(0,0)$ or at least one of them has to be $0$ to be critical points of the function if the point is $(0,0)$, $f_{xx}$ or $f_{yy}$ will be undefined and finding the point $(?,0)$ for which $f_x$ or $f_y$ would $= 0$ would be mighty hard. Are we unable to find the extrema of this function using the second partial test?",,['multivariable-calculus']
1,How determine or visualize level curves,How determine or visualize level curves,,"Let $f:\mathbb{C}\to\mathbb{C}$ given for $f(z)=\int_0^z \frac{1-e^t}{t} dt-\ln z$ and put $g(x,y)=\text{Re}(f(z))$. While using the computer, how to determine the curve $g(x,y)=0$? Thanks for the help.","Let $f:\mathbb{C}\to\mathbb{C}$ given for $f(z)=\int_0^z \frac{1-e^t}{t} dt-\ln z$ and put $g(x,y)=\text{Re}(f(z))$. While using the computer, how to determine the curve $g(x,y)=0$? Thanks for the help.",,"['complex-analysis', 'multivariable-calculus']"
2,How do I determine the measure for a volume integral?,How do I determine the measure for a volume integral?,,"If $I = \int r^2 dm$, how do I set up an integral over the volume of any object?  I can't use any assumptions about symmetry or shortcuts because the goal is to rotate around an arbitrary axis. $m = \rho v$ so $I = \rho\int r^2 dv$, but for a cube $v = xyz$ so $dv = yz dx + zx dy + xy dz$.  I guess? How do I go from that to $I = \rho\int\int\int r^2 dx dy dz$? What is actually going on? Why don't I replace $dv$ with $yz dx + zx dy + xy dz$ and get $I = \rho\int yzr^2 dz + \rho\int zxr^2 dy + \rho\int xyr^2 dz$? Or for a cylinder, $v = \pi(r_o^2 - r_1^2)h$ and $dr = \pi(r_o^2 - r_1^2)dh + 2\pi hr_o dr_o - 2\pi hr_i dr_i$.  How do I set up the volume integral with this? To clarify: I'm asking for the general principle. When I think of a shape, such as an arbitrarily rotated cylinder, I need to know what to do to set up the volume integral. How does this work?","If $I = \int r^2 dm$, how do I set up an integral over the volume of any object?  I can't use any assumptions about symmetry or shortcuts because the goal is to rotate around an arbitrary axis. $m = \rho v$ so $I = \rho\int r^2 dv$, but for a cube $v = xyz$ so $dv = yz dx + zx dy + xy dz$.  I guess? How do I go from that to $I = \rho\int\int\int r^2 dx dy dz$? What is actually going on? Why don't I replace $dv$ with $yz dx + zx dy + xy dz$ and get $I = \rho\int yzr^2 dz + \rho\int zxr^2 dy + \rho\int xyr^2 dz$? Or for a cylinder, $v = \pi(r_o^2 - r_1^2)h$ and $dr = \pi(r_o^2 - r_1^2)dh + 2\pi hr_o dr_o - 2\pi hr_i dr_i$.  How do I set up the volume integral with this? To clarify: I'm asking for the general principle. When I think of a shape, such as an arbitrarily rotated cylinder, I need to know what to do to set up the volume integral. How does this work?",,"['integration', 'multivariable-calculus']"
3,Solving a problem involving continuity of a piecewise function,Solving a problem involving continuity of a piecewise function,,"I'm trying to solve the following problem: Determine which $n\in\mathbb{N}$ make the following function   $f:\mathbb{R}^3\rightarrow \mathbb{R}$ continuous at the origin: $   f(x,y,z)=\left\{       \begin{array}{cr}          \frac{(\cos^2\left(|x|+|y|\right)-1)\sin(y^2+z^2)}{(x^2+y^2+z^2)^{n/2}}  & \text{if } (x,y,z) \neq 0\\         0 & \text{if } (x,y,z) = 0\\       \end{array} 	\right. $ It's obvious that $f$ is continuous at the origin if $n\leq 2$, since: $\left|\frac{(\cos^2\left(|x|+|y|\right)-1)\sin(y^2+z^2)}{(x^2+y^2+z^2)^{n/2}}\right|\leq\frac{|\cos^2\left(|x|+|y|\right)-1|\,|y^2+z^2|}{|x^2+y^2+z^2|}\leq\frac{|\cos^2\left(|x|+|y|\right)-1|\,|y^2+z^2|}{|y^2+z^2|}=|\cos^2\left(|x|+|y|\right)-1|\rightarrow 0$ I'm almost sure the function isn't continuous if $n> 2$ but I can't seem to find a way to prove it.","I'm trying to solve the following problem: Determine which $n\in\mathbb{N}$ make the following function   $f:\mathbb{R}^3\rightarrow \mathbb{R}$ continuous at the origin: $   f(x,y,z)=\left\{       \begin{array}{cr}          \frac{(\cos^2\left(|x|+|y|\right)-1)\sin(y^2+z^2)}{(x^2+y^2+z^2)^{n/2}}  & \text{if } (x,y,z) \neq 0\\         0 & \text{if } (x,y,z) = 0\\       \end{array} 	\right. $ It's obvious that $f$ is continuous at the origin if $n\leq 2$, since: $\left|\frac{(\cos^2\left(|x|+|y|\right)-1)\sin(y^2+z^2)}{(x^2+y^2+z^2)^{n/2}}\right|\leq\frac{|\cos^2\left(|x|+|y|\right)-1|\,|y^2+z^2|}{|x^2+y^2+z^2|}\leq\frac{|\cos^2\left(|x|+|y|\right)-1|\,|y^2+z^2|}{|y^2+z^2|}=|\cos^2\left(|x|+|y|\right)-1|\rightarrow 0$ I'm almost sure the function isn't continuous if $n> 2$ but I can't seem to find a way to prove it.",,"['calculus', 'multivariable-calculus', 'limits']"
4,"Is a Fréchet differentiable map between complex Banach spaces locally given by a ""power series""?","Is a Fréchet differentiable map between complex Banach spaces locally given by a ""power series""?",,"Let $X,Y$ be Banach spaces over $\mathbb{C}$ and let $U \subset X$ be open. If $f:U \to Y$ is Fréchet differentiable at every point of $U$, can we locally expand $f$ as a ""power series""? To be more precise (and I hope I'm making this precise in the right way), given $x \in U$ must there exist $A_0,A_1,A_2,\ldots$ with $A_n$ a bounded, symmetric $n$-linear map $X^n \to Y$ such that the expansion $$ f(y) = \sum_{n = 0}^\infty A_n (y-x)^n $$ is valid for all $y$ in some neighbourhood of $x$? To explain the above notation: Let $X^0 := \mathbb{C}$ by definition so that $A_0$ is essentially an element of $Y$ (namely $f(x)$). Let $(y-x)^n : = (y-x,y-x,\ldots,y-x) \in X^n$ for $n \geq 1$ and let $(y-x)^0 := 1 \in \mathbb{C}$. Wikipedia's entry on infinite dimensional holomorphy seems to indicate the answer is yes. I found it pretty incredible that a classical result should hold at this level of generality so I checked out the reference they give which is Kadison & Ringrose Fundamentals of the theory of operator algebras, vol. 1, sect. 3.3 but it turns out that only the case $X = \mathbb{C}$ is treated — pretty unsatisfactory! I'd be interested in to know a) whether Wikipedia has this right, even though the reference is faulty, and b) how difficult is this to prove?","Let $X,Y$ be Banach spaces over $\mathbb{C}$ and let $U \subset X$ be open. If $f:U \to Y$ is Fréchet differentiable at every point of $U$, can we locally expand $f$ as a ""power series""? To be more precise (and I hope I'm making this precise in the right way), given $x \in U$ must there exist $A_0,A_1,A_2,\ldots$ with $A_n$ a bounded, symmetric $n$-linear map $X^n \to Y$ such that the expansion $$ f(y) = \sum_{n = 0}^\infty A_n (y-x)^n $$ is valid for all $y$ in some neighbourhood of $x$? To explain the above notation: Let $X^0 := \mathbb{C}$ by definition so that $A_0$ is essentially an element of $Y$ (namely $f(x)$). Let $(y-x)^n : = (y-x,y-x,\ldots,y-x) \in X^n$ for $n \geq 1$ and let $(y-x)^0 := 1 \in \mathbb{C}$. Wikipedia's entry on infinite dimensional holomorphy seems to indicate the answer is yes. I found it pretty incredible that a classical result should hold at this level of generality so I checked out the reference they give which is Kadison & Ringrose Fundamentals of the theory of operator algebras, vol. 1, sect. 3.3 but it turns out that only the case $X = \mathbb{C}$ is treated — pretty unsatisfactory! I'd be interested in to know a) whether Wikipedia has this right, even though the reference is faulty, and b) how difficult is this to prove?",,"['reference-request', 'multivariable-calculus', 'banach-spaces']"
5,Problem sketching the surface after figuring out minima,Problem sketching the surface after figuring out minima,,"I have a firm understanding when it comes to 2-D graphs. However 3-D plots/graphs are confusing to me. I know there exists several software packages which neatly does the job. I need to sketch it by hand in order to understand it. I know 3-d graph is an extension to 2-D with an addition of $z$-axis, I am not sure where lies the $x$-axis, where lies the $y$-axis and the $z$-axis. Here's the situation. Given the surface $f(x,y) = z = x^2 + y^2$. I have been told to determine the nature and sketch the surface after determining the partial derivatives with respect to $x$ and $y$ respectively. I obtained the critical point $(0,0)$. I also could determine that its minimum at that point by evaluating delta. But I'm not sure how to sketch this. Please help. By the way, what exactly is contour map? Please help me sketch that one too. If you are curious to know from where I am solving this problem, it's from John Bird's higher engineering math, page 359.","I have a firm understanding when it comes to 2-D graphs. However 3-D plots/graphs are confusing to me. I know there exists several software packages which neatly does the job. I need to sketch it by hand in order to understand it. I know 3-d graph is an extension to 2-D with an addition of $z$-axis, I am not sure where lies the $x$-axis, where lies the $y$-axis and the $z$-axis. Here's the situation. Given the surface $f(x,y) = z = x^2 + y^2$. I have been told to determine the nature and sketch the surface after determining the partial derivatives with respect to $x$ and $y$ respectively. I obtained the critical point $(0,0)$. I also could determine that its minimum at that point by evaluating delta. But I'm not sure how to sketch this. Please help. By the way, what exactly is contour map? Please help me sketch that one too. If you are curious to know from where I am solving this problem, it's from John Bird's higher engineering math, page 359.",,"['multivariable-calculus', 'graphing-functions']"
6,"Continuity of multidimensional function: $f_1(x,y)=\frac{x^2y}{x^2+y^2}$ and $f_2(x,y)=\frac{2xy^3}{(x^2+y^2)^2}$",Continuity of multidimensional function:  and,"f_1(x,y)=\frac{x^2y}{x^2+y^2} f_2(x,y)=\frac{2xy^3}{(x^2+y^2)^2}","I am wondering how to check whether a multidimensional function is continuous. I.e. I am thinking of functions like $f_1(x,y)=\frac{x^2y}{x^2+y^2}$ with $f_1(0,0)=0$ $f_2(x,y)=\frac{2xy^3}{(x^2+y^2)^2}$ with $f_2(0,0)=0$ In order to check for continuity at point (0,0) I know to approaches: 1) express x,y in a different coordinate system. I.e. expressing x,y in polar coordinates tells me that $f_1$ is continuous and $f_2$ is not. 2) searching for constant $C$ so that $f < xC$ or $f < yC$ However what can I try if these two criteria are inconclusive? How do you check for continuity of a multidimensional function? Thanks in advance","I am wondering how to check whether a multidimensional function is continuous. I.e. I am thinking of functions like $f_1(x,y)=\frac{x^2y}{x^2+y^2}$ with $f_1(0,0)=0$ $f_2(x,y)=\frac{2xy^3}{(x^2+y^2)^2}$ with $f_2(0,0)=0$ In order to check for continuity at point (0,0) I know to approaches: 1) express x,y in a different coordinate system. I.e. expressing x,y in polar coordinates tells me that $f_1$ is continuous and $f_2$ is not. 2) searching for constant $C$ so that $f < xC$ or $f < yC$ However what can I try if these two criteria are inconclusive? How do you check for continuity of a multidimensional function? Thanks in advance",,"['calculus', 'multivariable-calculus']"
7,Chain rule for multi-variable,Chain rule for multi-variable,,"If  $z = x + D(x + y)$ and I let $g(x,y) = x + y$ Would I be right in saying that : $z = x + f(g(x))$ and $\frac{dz}{dx} = 1 + \frac{df}{dg}\cdot\frac{dg}{dx}$?","If  $z = x + D(x + y)$ and I let $g(x,y) = x + y$ Would I be right in saying that : $z = x + f(g(x))$ and $\frac{dz}{dx} = 1 + \frac{df}{dg}\cdot\frac{dg}{dx}$?",,['multivariable-calculus']
8,Find volume of body between surfaces,Find volume of body between surfaces,,"Problem: Find volume of body defined as follows: $z^2=xy, (\frac{x^2}{2}+\frac{y^2}{3})^4=\frac{xy}{\sqrt{6}},   x, y, z \ge 0$ . My solution: So we're working in the all positive octant of the Eucledian space. $0\le z \le \sqrt{xy}$ . Then I did a swap to polar coordinates as follows: $$x=\sqrt{2}r\cos\theta, y=\sqrt{3}r\sin\theta \\ 0\le r\le \sqrt[6]{sin\theta\cos\theta} \\ |J|=\sqrt{6}r \\ 0\le z \le r\sqrt{sin\theta\cos\theta}$$ Now I tried the following 2 integrals: $$1)\sqrt{6}\int_0^\frac{\pi}{2}\int_0^\sqrt[6]{sin\theta\cos\theta} r\sqrt{sin\theta\cos\theta}*r \ dr \,d\theta = \frac{\sqrt{6}}{3}\int_0^\frac{\pi}{2} sin\theta\cos\theta \ d\theta= \frac{\sqrt6}{6}$$ $$2)\sqrt{6}\int_0^\frac{\pi}{2}\int_0^\sqrt[6]{sin\theta\cos\theta}\int_0^{r\sqrt{sin\theta\cos\theta}} r \ dz \,dr \,d\theta = \sqrt{6}\int_0^\frac{\pi}{2}\sqrt{sin\theta\cos\theta}\int_0^\sqrt[6]{sin\theta\cos\theta} r^2 \,dr \,d\theta = \frac{\sqrt{6}}{3}\int_0^\frac{\pi}{2}sin\theta\cos\theta  \ d\theta=\frac{\sqrt{6}}{6}$$ So is my solution correct?",Problem: Find volume of body defined as follows: . My solution: So we're working in the all positive octant of the Eucledian space. . Then I did a swap to polar coordinates as follows: Now I tried the following 2 integrals: So is my solution correct?,"z^2=xy, (\frac{x^2}{2}+\frac{y^2}{3})^4=\frac{xy}{\sqrt{6}},   x, y, z \ge 0 0\le z \le \sqrt{xy} x=\sqrt{2}r\cos\theta, y=\sqrt{3}r\sin\theta \\ 0\le r\le \sqrt[6]{sin\theta\cos\theta} \\ |J|=\sqrt{6}r \\ 0\le z \le r\sqrt{sin\theta\cos\theta} 1)\sqrt{6}\int_0^\frac{\pi}{2}\int_0^\sqrt[6]{sin\theta\cos\theta} r\sqrt{sin\theta\cos\theta}*r \ dr \,d\theta = \frac{\sqrt{6}}{3}\int_0^\frac{\pi}{2} sin\theta\cos\theta \ d\theta= \frac{\sqrt6}{6} 2)\sqrt{6}\int_0^\frac{\pi}{2}\int_0^\sqrt[6]{sin\theta\cos\theta}\int_0^{r\sqrt{sin\theta\cos\theta}} r \ dz \,dr \,d\theta = \sqrt{6}\int_0^\frac{\pi}{2}\sqrt{sin\theta\cos\theta}\int_0^\sqrt[6]{sin\theta\cos\theta} r^2 \,dr \,d\theta = \frac{\sqrt{6}}{3}\int_0^\frac{\pi}{2}sin\theta\cos\theta  \ d\theta=\frac{\sqrt{6}}{6}","['calculus', 'integration', 'multivariable-calculus', 'solution-verification', 'volume']"
9,region above the cone and inside the sphere,region above the cone and inside the sphere,,"I've come across a problem where \begin{array}{c} \mbox{I have to compute the volume of the region} \\ \mbox{above the cone}\ z^2 = \sqrt{x^2 + y^2}\ \mbox{and inside the sphere}\ x^2 + y^2 + z^2 = 1 \end{array} I can't understand one thing, since this equation of the cone depicts an open $\mbox{cone ?}$ , what would be the region above the cone and inside the sphere I can include the image of what I sketched for a better view on how I tried to do the problem I just can't seem to pinpoint what would be the region above the cone and inside the sphere as stated in my problem requirements.","I've come across a problem where I can't understand one thing, since this equation of the cone depicts an open , what would be the region above the cone and inside the sphere I can include the image of what I sketched for a better view on how I tried to do the problem I just can't seem to pinpoint what would be the region above the cone and inside the sphere as stated in my problem requirements.","\begin{array}{c}
\mbox{I have to compute the volume of the region}
\\ \mbox{above the cone}\
z^2 = \sqrt{x^2 + y^2}\ \mbox{and inside the sphere}\ x^2 + y^2 + z^2 = 1
\end{array} \mbox{cone ?}","['calculus', 'integration', 'multivariable-calculus', 'vector-analysis']"
10,Multivariable chain rule for function with one negative component,Multivariable chain rule for function with one negative component,,"I have a (smooth) function $\psi:\mathbb{R}^2 \to \mathbb{R}\: : \: (y_1,y_2) \mapsto \psi(y_1,y_2)$ . Now I want to calculate the partialderivative of $\psi(-y_1,y_2)$ with respect to $y_1$ . I thought of writing $\psi(-y_1,y_2) = (\psi \circ f)(y_1,y_2)$ where $f(y_1,y_2) = (-y_1,y_2)$ and then applying the chain rule. To do this I wrote $u_1 = -y_1, u_2 = y_2$ . $$ \frac{\partial \psi(-y_1,y_2)}{\partial y_1} = \frac{\partial \psi}{\partial u_1}(u_1,u_2) \frac{\partial u_1}{\partial y_1} + \frac{\partial \psi}{\partial u_2}(u_1,u_2) \frac{\partial u_2}{\partial y_1}   $$ and since $\partial_{y_1} u_2 = 0$ and $\partial_{y_1} u_1 = -1$ we get: $$ \frac{\partial \psi(-y_1,y_2)}{\partial y_1} = -\frac{\partial \psi}{\partial u_1}(u_1,u_2)  $$ Now I was wondering if we can rewrite the right hand side to some derivative of $y_1$ instead of a derivative of $u_1$ . Any help would be appreciated.",I have a (smooth) function . Now I want to calculate the partialderivative of with respect to . I thought of writing where and then applying the chain rule. To do this I wrote . and since and we get: Now I was wondering if we can rewrite the right hand side to some derivative of instead of a derivative of . Any help would be appreciated.,"\psi:\mathbb{R}^2 \to \mathbb{R}\: : \: (y_1,y_2) \mapsto \psi(y_1,y_2) \psi(-y_1,y_2) y_1 \psi(-y_1,y_2) = (\psi \circ f)(y_1,y_2) f(y_1,y_2) = (-y_1,y_2) u_1 = -y_1, u_2 = y_2 
\frac{\partial \psi(-y_1,y_2)}{\partial y_1} = \frac{\partial \psi}{\partial u_1}(u_1,u_2) \frac{\partial u_1}{\partial y_1} + \frac{\partial \psi}{\partial u_2}(u_1,u_2) \frac{\partial u_2}{\partial y_1}  
 \partial_{y_1} u_2 = 0 \partial_{y_1} u_1 = -1 
\frac{\partial \psi(-y_1,y_2)}{\partial y_1} = -\frac{\partial \psi}{\partial u_1}(u_1,u_2) 
 y_1 u_1","['calculus', 'analysis', 'multivariable-calculus', 'partial-derivative', 'chain-rule']"
11,Expressing integral based on the change of variables in double integral,Expressing integral based on the change of variables in double integral,,"Let B be the region in the first quadrant bounded by the curves $xy=1$ , $xy=3$ , $x^2−y^2=1$ , and $x^2−y^2=4.$ Evaluate $\iint_{B} (x^2 + y^2 ) dx dy$ using the change of variables $u=x^2−y^2$ , $v=xy.$ The Jacobian is $J = 2(x^2+y^2)$ . But I have a difficulty to express the new transformed integral .It must be $$\iint_{D^*}f (x(u, v), y(u, v)) J dudv$$ $$ \iint_{D^*} ?  2(x^2+y^2) dudv$$ What is placed in the $f(x(u, v), y(u, v))$ in the solutions book is $1/2$ .But I can understand why? How can I  express $x^2+y^2$ based on $u=x^2−y^2,v=xy$ ? is $$ \iint_{D^*} \frac{x^2+y^2}{  2(x^2+y^2)} dudv = \iint_{D^*} \frac{1}{  2} dudv$$ ?","Let B be the region in the first quadrant bounded by the curves , , , and Evaluate using the change of variables , The Jacobian is . But I have a difficulty to express the new transformed integral .It must be What is placed in the in the solutions book is .But I can understand why? How can I  express based on ? is ?","xy=1 xy=3 x^2−y^2=1 x^2−y^2=4. \iint_{B} (x^2 + y^2 ) dx dy u=x^2−y^2 v=xy. J = 2(x^2+y^2) \iint_{D^*}f (x(u, v), y(u, v)) J dudv  \iint_{D^*} ?  2(x^2+y^2) dudv f(x(u, v), y(u, v)) 1/2 x^2+y^2 u=x^2−y^2,v=xy  \iint_{D^*} \frac{x^2+y^2}{  2(x^2+y^2)} dudv = \iint_{D^*} \frac{1}{  2} dudv","['integration', 'multivariable-calculus', 'change-of-variable']"
12,Showing explicitly that every 2-form on a plane minus the origin is exact,Showing explicitly that every 2-form on a plane minus the origin is exact,,"Consider $X=\Bbb R^2-0$ . Since it deformation retracts onto the unit circle $S^1$ , the second de Rham cohomology of the manifold $X$ is zero. This means that every 2-form $hdxdy$ on $X$ is exact. But can we find an explicit 1-form $fdx+gdy$ on $X$ satisfying $d(fdx+gdy)=hdxdy$ , i.e. $g_x-f_y=h$ ? I cannot see how to define $f$ and $g$ from $h$ .","Consider . Since it deformation retracts onto the unit circle , the second de Rham cohomology of the manifold is zero. This means that every 2-form on is exact. But can we find an explicit 1-form on satisfying , i.e. ? I cannot see how to define and from .",X=\Bbb R^2-0 S^1 X hdxdy X fdx+gdy X d(fdx+gdy)=hdxdy g_x-f_y=h f g h,"['multivariable-calculus', 'differential-topology', 'vector-analysis', 'smooth-manifolds', 'differential-forms']"
13,Line integral of a vector field over a triangle [closed],Line integral of a vector field over a triangle [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last month . Improve this question I am trying to solve this line integral, but I get different answers when doing a normal line integral and using the vector form of green's theorem and am wondering why this is the case? I got 4 as my answer solving the line integral and 12 using green's theorem. I am also confused as to why the integral has $d\mathbf{s}$ , and not $d\mathbf{r}$ , so any clarification on that would be awesome. The question states: Evaluate $\int_{C} \mathbf{F}\cdot d\mathbf{s}$ where $\mathbf{F} = \langle 2z,8x-3y,3x+y \rangle $ and $C$ is the triangle with vertices $(1,0,0), (0,1,0), (0,0,2)$ Attempt with line integrals: Parametrizing $C$ into $C_1 = <1-t,t,0>$ , $C_2 = <-,1-t,2t>$ , and $C_3 = <5,0,2-2t>$ where $0\le t \le 1$ $$\int_{C} \mathbf{F}\cdot d\mathbf{s} = \sum_{i=1}^{3}\int_{C_i} \mathbf{F}\cdot d\mathbf{s} = \int_{C_1} \mathbf{F}\cdot d\mathbf{s} + \int_{C_2} \mathbf{F}\cdot d\mathbf{s} + \int_{C_3} \mathbf{F}\cdot d\mathbf{s}$$ $$\int_{C_1} \mathbf{F}\cdot d\mathbf{s} = \int_{0}^{1} \mathbf{F}(\mathbf{r}(t))\cdot \mathbf{r}\prime(t)dt$$ $$\int_{0}^{1} <2(0),8(1-t)-3(t),3(1-t)+(t)>\cdot<-1,1,0> dt$$ $$\int_{0}^{1} 8-8t-3t dt = \frac{5}{2}$$ With $C_2$ and $C_3$ being $$\int_{C_2} \mathbf{F}\cdot d\mathbf{s} = \frac{5}{2}$$ and $$\int_{C_3} \mathbf{F}\cdot d\mathbf{s} = -1$$ So $$\int_{C} \mathbf{F}\cdot d\mathbf{s} = \frac{5}{2} + \frac{5}{2} - 1 = 4$$ With Green's Theorem: $$\int_{C} \mathbf{F}\cdot d\mathbf{s} = \iint_{D}curl\mathbf{F}\cdot k dA$$ $$curl\mathbf{F} = <1,-1,8>$$ $$\iint_{D}curl\mathbf{F}\cdot k dA = \iint_{D}8 dA$$ $$= 8 \iint_{D}dA$$ where $\iint_{D}dA$ would be the area of the triangle area of triangle $$A = \frac{1}{2}|<-1,1,0>\times<-1,0,2>| = \frac{\sqrt{9}}{2} = \frac{3}{2}$$ and so $$8 \iint_{D}dA = 8 * \frac{3}{2} = 12$$ Any help on how to solve this would be greatly appreciated, thanks!","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last month . Improve this question I am trying to solve this line integral, but I get different answers when doing a normal line integral and using the vector form of green's theorem and am wondering why this is the case? I got 4 as my answer solving the line integral and 12 using green's theorem. I am also confused as to why the integral has , and not , so any clarification on that would be awesome. The question states: Evaluate where and is the triangle with vertices Attempt with line integrals: Parametrizing into , , and where With and being and So With Green's Theorem: where would be the area of the triangle area of triangle and so Any help on how to solve this would be greatly appreciated, thanks!","d\mathbf{s} d\mathbf{r} \int_{C} \mathbf{F}\cdot d\mathbf{s} \mathbf{F} = \langle 2z,8x-3y,3x+y \rangle  C (1,0,0), (0,1,0), (0,0,2) C C_1 = <1-t,t,0> C_2 = <-,1-t,2t> C_3 = <5,0,2-2t> 0\le t \le 1 \int_{C} \mathbf{F}\cdot d\mathbf{s} = \sum_{i=1}^{3}\int_{C_i} \mathbf{F}\cdot d\mathbf{s} = \int_{C_1} \mathbf{F}\cdot d\mathbf{s} + \int_{C_2} \mathbf{F}\cdot d\mathbf{s} + \int_{C_3} \mathbf{F}\cdot d\mathbf{s} \int_{C_1} \mathbf{F}\cdot d\mathbf{s} = \int_{0}^{1} \mathbf{F}(\mathbf{r}(t))\cdot \mathbf{r}\prime(t)dt \int_{0}^{1} <2(0),8(1-t)-3(t),3(1-t)+(t)>\cdot<-1,1,0> dt \int_{0}^{1} 8-8t-3t dt = \frac{5}{2} C_2 C_3 \int_{C_2} \mathbf{F}\cdot d\mathbf{s} = \frac{5}{2} \int_{C_3} \mathbf{F}\cdot d\mathbf{s} = -1 \int_{C} \mathbf{F}\cdot d\mathbf{s} = \frac{5}{2} + \frac{5}{2} - 1 = 4 \int_{C} \mathbf{F}\cdot d\mathbf{s} = \iint_{D}curl\mathbf{F}\cdot k dA curl\mathbf{F} = <1,-1,8> \iint_{D}curl\mathbf{F}\cdot k dA = \iint_{D}8 dA = 8 \iint_{D}dA \iint_{D}dA A = \frac{1}{2}|<-1,1,0>\times<-1,0,2>| = \frac{\sqrt{9}}{2} = \frac{3}{2} 8 \iint_{D}dA = 8 * \frac{3}{2} = 12","['multivariable-calculus', 'vector-fields', 'line-integrals', 'greens-theorem']"
14,Strange usage of chain rule. Can anyone explain why this derivation was done this way?,Strange usage of chain rule. Can anyone explain why this derivation was done this way?,,"There are 2 issues I have with the way this was done. The first was how chain rule was used in the (1.35), and the second was how chain rule was used in (1.36). It all seems so counterintuitive. For (1.35), to start I tried plugging for the initial equation, but without known values for $\delta p$ , I can't use it to verify this equation. So, I attempted to obtain it using chain rule, and doesn't make a lot of sense the way I'm assuming it was done. Unless I'm just blatantly abusing chain rule, this result just doesn't seem possible. Trying to do this a bit more intuitively: Transform 1: $p(x,z) = g(x,z) \therefore z = z(x,z)$ (pretty sure we can't do this) $$\frac{\partial p}{\partial x} = \frac{\partial g}{\partial x} = \frac{\partial g}{\partial x}+\frac{\partial g}{\partial z}\frac{\partial z}{\partial x}$$ It's my understanding that we could also abuse chain rule here and say $\frac{\partial a_i}{\partial n_i}\frac{\partial n_i}{\partial k_i} = \frac{\partial a_i}{\partial k_i}$ , otherwise we're saying that z is a function of x when it just plainly isn't. When and Why would we choose to abuse chain rule like this? Drop the functions to get a definition for the differential operator $$\tag{1}\frac{\partial}{\partial x}  = \frac{\partial}{\partial x} = \frac{\partial}{\partial x}+\frac{\partial z}{\partial x}\frac{\partial}{\partial z}$$ Second transformation: $p(x,z) = h(x,s) \therefore s = s(x,z)$ (THIS would be a proper usage of transforming a variable and then applying chain rule, but why do it here rather than at the beginning?) $$\frac{\partial h}{\partial z} = \frac{\partial h}{\partial s}\frac{\partial s}{\partial z}$$ Drop the functions to get a differential operator to substitute into (1) $$\tag{2}\frac{\partial}{\partial z} = \frac{\partial s}{\partial z}\frac{\partial}{\partial s}$$ And then plugging in (2) into (1) and applying the derivative to the original function $p$ would give: $$\tag{5} \frac{\partial p}{\partial x} = \frac{\partial p}{\partial x} + \frac{\partial s}{\partial z}\frac{\partial z}{\partial x}\frac{\partial p}{\partial s}$$ I have no idea why we couldn't have just instead done $p(x,z) = g(x,s) \therefore s = s(x,z)$ at the very beginning and then done $\frac{\partial p}{\partial x} = \frac{\partial p}{\partial x} + \frac{\partial p}{\partial s}\frac{\partial s}{\partial x}$ , nor do I understand why this would be incorrect whereas we would consider (1.36) correct. Does anyone understand why it was done this way?","There are 2 issues I have with the way this was done. The first was how chain rule was used in the (1.35), and the second was how chain rule was used in (1.36). It all seems so counterintuitive. For (1.35), to start I tried plugging for the initial equation, but without known values for , I can't use it to verify this equation. So, I attempted to obtain it using chain rule, and doesn't make a lot of sense the way I'm assuming it was done. Unless I'm just blatantly abusing chain rule, this result just doesn't seem possible. Trying to do this a bit more intuitively: Transform 1: (pretty sure we can't do this) It's my understanding that we could also abuse chain rule here and say , otherwise we're saying that z is a function of x when it just plainly isn't. When and Why would we choose to abuse chain rule like this? Drop the functions to get a definition for the differential operator Second transformation: (THIS would be a proper usage of transforming a variable and then applying chain rule, but why do it here rather than at the beginning?) Drop the functions to get a differential operator to substitute into (1) And then plugging in (2) into (1) and applying the derivative to the original function would give: I have no idea why we couldn't have just instead done at the very beginning and then done , nor do I understand why this would be incorrect whereas we would consider (1.36) correct. Does anyone understand why it was done this way?","\delta p p(x,z) = g(x,z) \therefore z = z(x,z) \frac{\partial p}{\partial x} = \frac{\partial g}{\partial x} = \frac{\partial g}{\partial x}+\frac{\partial g}{\partial z}\frac{\partial z}{\partial x} \frac{\partial a_i}{\partial n_i}\frac{\partial n_i}{\partial k_i} = \frac{\partial a_i}{\partial k_i} \tag{1}\frac{\partial}{\partial x}  = \frac{\partial}{\partial x} = \frac{\partial}{\partial x}+\frac{\partial z}{\partial x}\frac{\partial}{\partial z} p(x,z) = h(x,s) \therefore s = s(x,z) \frac{\partial h}{\partial z} = \frac{\partial h}{\partial s}\frac{\partial s}{\partial z} \tag{2}\frac{\partial}{\partial z} = \frac{\partial s}{\partial z}\frac{\partial}{\partial s} p \tag{5} \frac{\partial p}{\partial x} = \frac{\partial p}{\partial x} + \frac{\partial s}{\partial z}\frac{\partial z}{\partial x}\frac{\partial p}{\partial s} p(x,z) = g(x,s) \therefore s = s(x,z) \frac{\partial p}{\partial x} = \frac{\partial p}{\partial x} + \frac{\partial p}{\partial s}\frac{\partial s}{\partial x}","['multivariable-calculus', 'chain-rule']"
15,How can I find a positively oriented parametrization?,How can I find a positively oriented parametrization?,,"I need to find a parametrization of the surface defined by $x^{2}+y^{2}+z^{2}=9$ , $x+y+z\geq 1$ . The orientation must be compatible with the orientation of its boundary, that is, if the loop lies in the $xy$ -plane then we have to choose the circulation counterclockwise and the flux upward. How can I do that? Any hint?","I need to find a parametrization of the surface defined by , . The orientation must be compatible with the orientation of its boundary, that is, if the loop lies in the -plane then we have to choose the circulation counterclockwise and the flux upward. How can I do that? Any hint?",x^{2}+y^{2}+z^{2}=9 x+y+z\geq 1 xy,"['multivariable-calculus', 'parametrization']"
16,"Can use one-variable integration to tell whether $R=\{(x,y):\;0\leq y\leq\left|\mathrm{sin}\frac{1}{x}|,\;0<x\ <1\right\}$ is rectifiable(has area)?",Can use one-variable integration to tell whether  is rectifiable(has area)?,"R=\{(x,y):\;0\leq y\leq\left|\mathrm{sin}\frac{1}{x}|,\;0<x\ <1\right\}","$R=\left\{(x,y):\;0\leq y\leq\left|\mathrm{sin}\frac{1}{x}\right|,\;0<x\ <1\right\}$ , to determine whether it's rectifiable, can I use the method of one-variable integration to prove? Like this question, if we see it as a one-variable integration $f(x)=\int_0^1 |\sin(\frac{1}{x})|$ , then it has only one discontinuous point at (0,0), so it's integrable. Can I use this to tell that R is rectifiable? If using the def. of rectifiable(or has area), we need to take partition first and check the area of $\partial D$ has zero area, but it's not so simple to handle the discontinuity at $(0,0)$ . So I hope the question can be converted into the integration on the x,y plane. I guess it's OK, but I don't know how to prove it.",", to determine whether it's rectifiable, can I use the method of one-variable integration to prove? Like this question, if we see it as a one-variable integration , then it has only one discontinuous point at (0,0), so it's integrable. Can I use this to tell that R is rectifiable? If using the def. of rectifiable(or has area), we need to take partition first and check the area of has zero area, but it's not so simple to handle the discontinuity at . So I hope the question can be converted into the integration on the x,y plane. I guess it's OK, but I don't know how to prove it.","R=\left\{(x,y):\;0\leq y\leq\left|\mathrm{sin}\frac{1}{x}\right|,\;0<x\ <1\right\} f(x)=\int_0^1 |\sin(\frac{1}{x})| \partial D (0,0)","['integration', 'multivariable-calculus', 'area']"
17,Line integral over two squares,Line integral over two squares,,"I want to calculate the following integral $$\oint\limits_{C_1} \oint\limits_{C_2} \frac{\vec {dl_1} \cdot \vec {dl_2}} r,$$ where The square paths $C_1$ and $C_2$ over which the integration should be carried are shown in this picture : (the length of both sides of the squares is a given parameter $a$ , and the orientations are the ones shown). $r$ represents the distance from each $\vec {dl_1}$ to $\vec {dl_2}$ . The approach I took in order to solve this problem is considering the total integral as the (finite) sum of integrals restricted to a single side from each square and performing the calculation of each of these integrals: obviously, the chosen sides being perpendicular to each other imply the vanishing of the dot product and thus zero contribution from that term to the total value of the integral. My problem arises from the behaviour of $r$ , due to its dependence on both $\vec {dl_1}$ and $\vec {dl_2}$ . For example, the $r$ i get for the top of the left square and the bottom of the right square is: $$  r = \sqrt{a^2 + (3a - x - y)^2} $$ where $x$ is the coordinate of the point $\vec {dl_1}$ on the left square and $y$ is the coordinate of $\vec {dl_2}$ on the right square (I would integrate from 0 to $a$ both times). Can this issue be solved and consequently is my approach fruitful? And if it is not, is there something easier that I am missing?","I want to calculate the following integral where The square paths and over which the integration should be carried are shown in this picture : (the length of both sides of the squares is a given parameter , and the orientations are the ones shown). represents the distance from each to . The approach I took in order to solve this problem is considering the total integral as the (finite) sum of integrals restricted to a single side from each square and performing the calculation of each of these integrals: obviously, the chosen sides being perpendicular to each other imply the vanishing of the dot product and thus zero contribution from that term to the total value of the integral. My problem arises from the behaviour of , due to its dependence on both and . For example, the i get for the top of the left square and the bottom of the right square is: where is the coordinate of the point on the left square and is the coordinate of on the right square (I would integrate from 0 to both times). Can this issue be solved and consequently is my approach fruitful? And if it is not, is there something easier that I am missing?","\oint\limits_{C_1} \oint\limits_{C_2} \frac{\vec {dl_1} \cdot \vec {dl_2}} r, C_1 C_2 a r \vec {dl_1} \vec {dl_2} r \vec {dl_1} \vec {dl_2} r  
r = \sqrt{a^2 + (3a - x - y)^2}
 x \vec {dl_1} y \vec {dl_2} a","['calculus', 'integration', 'multivariable-calculus', 'physics', 'line-integrals']"
18,"Determine a tangent plane on the surface $\psi(u,v)=(3u^2-2v^2,u-v,u+v)$ at the point $\psi(1,2)$.",Determine a tangent plane on the surface  at the point .,"\psi(u,v)=(3u^2-2v^2,u-v,u+v) \psi(1,2)","So in this question basically I'm getting stuck after calculating the Jacobian of the function $\psi$ , somehow I'm trying to find the values of a vector that is on this plane, but I'm not so sure of what is going on. I've already tried to find the normal vector $\psi_u \times \psi_v$ , but still its hard to move with this. Here is what I've been trying: $\psi_u = \frac{\partial\psi}{\partial u} = (6u,1,1)$ $\psi_v = \frac{\partial\psi}{\partial v} = (-4v,-1,1)$ In the point $\psi(1,2)$ we got: $\psi_u=(6,1,1)$ and $\psi_v=(-8,-1,1)$ Finding the normal vector: $\psi_u \times \psi_v=(2,-14,2)$ Then finally in the equation of the tangent plane (in summary): $z+7y-x+3=0$ . If someone could give me a hint what step did I miss , I would appreciate. The solution here says: $x-7y+z-5=0$","So in this question basically I'm getting stuck after calculating the Jacobian of the function , somehow I'm trying to find the values of a vector that is on this plane, but I'm not so sure of what is going on. I've already tried to find the normal vector , but still its hard to move with this. Here is what I've been trying: In the point we got: and Finding the normal vector: Then finally in the equation of the tangent plane (in summary): . If someone could give me a hint what step did I miss , I would appreciate. The solution here says:","\psi \psi_u \times \psi_v \psi_u = \frac{\partial\psi}{\partial u} = (6u,1,1) \psi_v = \frac{\partial\psi}{\partial v} = (-4v,-1,1) \psi(1,2) \psi_u=(6,1,1) \psi_v=(-8,-1,1) \psi_u \times \psi_v=(2,-14,2) z+7y-x+3=0 x-7y+z-5=0","['calculus', 'multivariable-calculus', 'surfaces']"
19,Semantics of the angle between velocity vector and the positive $x$-axis,Semantics of the angle between velocity vector and the positive -axis,x,"Let's say a particle moves in plane with curvature equal to $\kappa(t) = 2t$ , with constant speed of $\|v(t)\| = 5$ , such that $v(0) = 5\textbf{i}$ , and the particle never goes to the left of the $y$ -axis. The angle that the velocity vector makes with the positive $x$ -axis at time $t$ is $\alpha(t)=\frac{\pi}{2} - 5t^2$ . $\alpha(t)=\frac{\pi}{2} - 5t^2 \implies \frac{d\alpha}{dt} = -10t$ , which is always negative because $t \ge 0$ . That would normally imply that the angle is decreasing without bound, but intuitively I don't see how it can go below $-\pi$ , so I'm not sure how to interpret that angle. Note: This is the exercise $14.15.11$ in Tom Apostol's Calculus vol. $1$ . Clarification: We know the curvature is $\kappa(t) = |\frac{d \alpha}{ds}|$ , where $s$ is arc length. To compute $\alpha$ , I would like to know the sign of the $\frac{d \alpha}{ds}$ . If we take $\alpha$ is always decreasing, we know $\kappa(t) = -\frac{d \alpha}{ds}$ , and I can integrate to find $\alpha$ . However, if the angle is constrained to be in the range $[-\pi, \pi]$ , then I'm not sure how to assign the sign to the absolute value (and even the derivative is not continuous).","Let's say a particle moves in plane with curvature equal to , with constant speed of , such that , and the particle never goes to the left of the -axis. The angle that the velocity vector makes with the positive -axis at time is . , which is always negative because . That would normally imply that the angle is decreasing without bound, but intuitively I don't see how it can go below , so I'm not sure how to interpret that angle. Note: This is the exercise in Tom Apostol's Calculus vol. . Clarification: We know the curvature is , where is arc length. To compute , I would like to know the sign of the . If we take is always decreasing, we know , and I can integrate to find . However, if the angle is constrained to be in the range , then I'm not sure how to assign the sign to the absolute value (and even the derivative is not continuous).","\kappa(t) = 2t \|v(t)\| = 5 v(0) = 5\textbf{i} y x t \alpha(t)=\frac{\pi}{2} - 5t^2 \alpha(t)=\frac{\pi}{2} - 5t^2 \implies \frac{d\alpha}{dt} = -10t t \ge 0 -\pi 14.15.11 1 \kappa(t) = |\frac{d \alpha}{ds}| s \alpha \frac{d \alpha}{ds} \alpha \kappa(t) = -\frac{d \alpha}{ds} \alpha [-\pi, \pi]","['multivariable-calculus', 'intuition', 'plane-geometry', 'angle', 'plane-curves']"
20,"If rank($J_f(x))=n$, then there exists a neighborhood such that for each of its points $y$, $rank(J_f(y))\geq n$","If rank(, then there exists a neighborhood such that for each of its points ,",J_f(x))=n y rank(J_f(y))\geq n,"Let $f: U \rightarrow \mathbb{R}^p$ be a $C^1$ function defined on an open subset $U \subseteq \mathbb{R}^q$ . If for some $x \in U$ , the jacobi matrix $J_f(x)$ has $rank$ equal to $n$ , then there exists an open neighborhood $V$ of $x$ for each $x \in V$ , the jacobi matrix $J_f(y)$ has rank at least $n$ . My approach: I do not really have an approach. I know that for a matrix $A$ , $rank(A):= \dim im(A)$ . What also comes to mind is that $rank(A)=n$ , would mean that $A$ has $n$ linearly independent columns. If I would have to guess, I would say it has something to do with the $C^1$ , i.e. $J_f$ is continuous. And somehow using the continuity the result should follow. But I don't know how to approach this. Any hints would be appreciated.","Let be a function defined on an open subset . If for some , the jacobi matrix has equal to , then there exists an open neighborhood of for each , the jacobi matrix has rank at least . My approach: I do not really have an approach. I know that for a matrix , . What also comes to mind is that , would mean that has linearly independent columns. If I would have to guess, I would say it has something to do with the , i.e. is continuous. And somehow using the continuity the result should follow. But I don't know how to approach this. Any hints would be appreciated.",f: U \rightarrow \mathbb{R}^p C^1 U \subseteq \mathbb{R}^q x \in U J_f(x) rank n V x x \in V J_f(y) n A rank(A):= \dim im(A) rank(A)=n A n C^1 J_f,"['real-analysis', 'linear-algebra', 'analysis', 'multivariable-calculus']"
21,Defining a multivariable function to make it continuous at a point,Defining a multivariable function to make it continuous at a point,,"I am working with the function $f(x,y) = \frac{\sin(x)\sin^3(y)}{1-\cos(x^2+y^2)}$ and am asked to make it continuous at $(0,0)$ . My approach was to take the limit along the line $x = 0$ and I got the limit to be $0$ and concluded that the function must be defined as $0$ at $(0,0)$ to make it continuous. The question that I have now is whether it is right to do so or if there is any other curve along which the limit is not zero and in general what is the best way to work out such a problem where I need to find the continuity for a function with several variables.",I am working with the function and am asked to make it continuous at . My approach was to take the limit along the line and I got the limit to be and concluded that the function must be defined as at to make it continuous. The question that I have now is whether it is right to do so or if there is any other curve along which the limit is not zero and in general what is the best way to work out such a problem where I need to find the continuity for a function with several variables.,"f(x,y) = \frac{\sin(x)\sin^3(y)}{1-\cos(x^2+y^2)} (0,0) x = 0 0 0 (0,0)","['real-analysis', 'limits', 'multivariable-calculus']"
22,Integrating general powers of the Mahalanobis norm with respect to spherical measure,Integrating general powers of the Mahalanobis norm with respect to spherical measure,,"Let $S^{d-1}$ denote the $d$ -dimensional sphere, and $\sigma_{d-1}$ denote the corresponding spherical measure. I am wondering how to go about solving the following integral for $c>0$ $$ \int_{S^{d-1}} \|M u\|_2^{2c} d \sigma_{d-1}(u), $$ where $M$ is a positive definite and symmetric matrix. The obvious first step is to substitute $v=Mu$ but I am not quite sure how to work with the spherical measure, even for the simpler setting of $c=1$ . In the setting where $M=I$ then $\|Mu\|= \|u\|=1$ whenever $u \in S^{d-1}$ , so here we have $$ \int_{S^{d-1}} \|M u\|_2^{2c} d \sigma_{d-1}(u) = \int_{S^{d-1}} \|u\|_2^{2c} d \sigma_{d-1}(u) = \int_{S^{d-1}} d \sigma_{d-1}(u) = S(d) $$ where $S(d)$ is the surface area of the $d$ -dimensional sphere. Any hints or references to relevant material for dealing with the more general setting would be appreciated!","Let denote the -dimensional sphere, and denote the corresponding spherical measure. I am wondering how to go about solving the following integral for where is a positive definite and symmetric matrix. The obvious first step is to substitute but I am not quite sure how to work with the spherical measure, even for the simpler setting of . In the setting where then whenever , so here we have where is the surface area of the -dimensional sphere. Any hints or references to relevant material for dealing with the more general setting would be appreciated!","S^{d-1} d \sigma_{d-1} c>0 
\int_{S^{d-1}} \|M u\|_2^{2c} d \sigma_{d-1}(u),
 M v=Mu c=1 M=I \|Mu\|= \|u\|=1 u \in S^{d-1} 
\int_{S^{d-1}} \|M u\|_2^{2c} d \sigma_{d-1}(u)
=
\int_{S^{d-1}} \|u\|_2^{2c} d \sigma_{d-1}(u)
=
\int_{S^{d-1}} d \sigma_{d-1}(u)
= S(d)
 S(d) d","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'spherical-coordinates']"
23,Question about function of two variables.,Question about function of two variables.,,"The function is: $$ f(x,y)=\begin{cases} xy^3 \sin \left(\frac{1}{{x^4+y^4}}\right), & \text{if } (x,y) \neq (0,0) \\ 0, & \text{if } (x,y) = (0,0) \end{cases}.$$ I have to check if it is of class $C^1$ . I've found that $d_xf(0,0) = 0$ and $d_{y} f(0,0) = 0$ I know how to prove by definition it's differentiable at the origin and obviously it's differentiable elsewhere. But I don't know how to prove that partial derivatives are/are not continuous. For example i get $d_xf(x,y) = y^3\sin\left(\frac{1}{x^4+y^4}\right) - \frac{(4x^4y^3)\cos\left(\frac{1}{x^4+y^4}\right)}{(x^4+y^4)^2}$ Any kind of help would be appreciated.",The function is: I have to check if it is of class . I've found that and I know how to prove by definition it's differentiable at the origin and obviously it's differentiable elsewhere. But I don't know how to prove that partial derivatives are/are not continuous. For example i get Any kind of help would be appreciated.,"
f(x,y)=\begin{cases}
xy^3 \sin \left(\frac{1}{{x^4+y^4}}\right), & \text{if } (x,y) \neq (0,0) \\
0, & \text{if } (x,y) = (0,0)
\end{cases}. C^1 d_xf(0,0) = 0 d_{y} f(0,0) = 0 d_xf(x,y) = y^3\sin\left(\frac{1}{x^4+y^4}\right) - \frac{(4x^4y^3)\cos\left(\frac{1}{x^4+y^4}\right)}{(x^4+y^4)^2}","['multivariable-calculus', 'continuity', 'partial-derivative']"
24,An upper bound of a function using its Taylor series.,An upper bound of a function using its Taylor series.,,Let $f \in C^{m}(\mathbb{R}^n)$ of compact support. Then Taylor's theorem give us $$ f(x) = \sum_{|\alpha| \leq m} \frac{D^{\alpha} f(x_0)}{\alpha!} (x - x_0)^{\alpha} + \sum_{|\alpha| = m} h_{\alpha}(x)  (x - x_0)^{\alpha} $$ using the notation in [https://en.wikipedia.org/wiki/Taylor%27s_theorem#Taylor's_theorem_for_multivariate_functions]. Suppose $D^{\alpha} f(x_0) = 0$ for all $|\alpha| \leq m$ . Take some $|\beta| < m$ . I want to prove that $$ |D^{\beta} f(x)| < |x - x_0|^{m - |\beta|} C \sum_{|\alpha| \leq m} | D^{\alpha} f |_{\infty} $$ for some $C > 0$ depending only on $m$ and $n$ . I am sure this is true but I am having difficulty dealing with the $h_\alpha$ function coming from the remainder term of the Taylor expansion.... how can I prove this? Thank you,Let of compact support. Then Taylor's theorem give us using the notation in [https://en.wikipedia.org/wiki/Taylor%27s_theorem#Taylor's_theorem_for_multivariate_functions]. Suppose for all . Take some . I want to prove that for some depending only on and . I am sure this is true but I am having difficulty dealing with the function coming from the remainder term of the Taylor expansion.... how can I prove this? Thank you,"f \in C^{m}(\mathbb{R}^n) 
f(x) = \sum_{|\alpha| \leq m} \frac{D^{\alpha} f(x_0)}{\alpha!} (x - x_0)^{\alpha}
+
\sum_{|\alpha| = m} h_{\alpha}(x)  (x - x_0)^{\alpha}
 D^{\alpha} f(x_0) = 0 |\alpha| \leq m |\beta| < m 
|D^{\beta} f(x)| < |x - x_0|^{m - |\beta|} C \sum_{|\alpha| \leq m} | D^{\alpha} f |_{\infty}
 C > 0 m n h_\alpha","['real-analysis', 'multivariable-calculus', 'taylor-expansion']"
25,Linear approximation VS exact value for this function in $\mathbb{R}^2$,Linear approximation VS exact value for this function in,\mathbb{R}^2,"Consider $f: \mathbb{R}^2\to \mathbb{R}$ to be a differentiable function. We have $f\left(\frac{9}{10}, \frac{1}{10}\right) = 3$ , $f'_x\left(\frac{9}{10}, \frac{1}{10}\right) = 1$ , $f'_y\left(\frac{9}{10}, \frac{1}{10}\right) = -2$ . Using the best linear local approximation of $f$ around the point $\left(\frac{9}{10}, \frac{1}{10}\right)$ , calculate an approximate value for $f(1, 0)$ . Then, assuming that $f$ is strictly concave, say if the exact value of $f(1, 0)$ is greater or lower than the approximate value. Attempts So for the first request, I just applied Taylor series: $$f(x, y) = f(x_0, y_0) + (x-x_0)f'_x(x_0, y_0) + (y-y_0)f'_y(x_0, y_0) + O(||x-y||^2)$$ With $x_0, y_0$ the given point and hence I got $$f(x, y) = x + y + 2$$ hence $$f(1, 0) \approx 3$$ Now for the second part I don't know how to move. I thought about using the definition of concave function in two variables namely $$f(\lambda x_1 + (1-\lambda)x_2, \lambda y_1 + (1-\lambda)y_2) \geq \lambda f(x_1, y_1) + (1-\lambda)f(x_2, y_2)$$ I have no other information. I tried to chose $\lambda = 1/2$ but I got no result since I don't know the values of the function at other points. Any help? Thank you!","Consider to be a differentiable function. We have , , . Using the best linear local approximation of around the point , calculate an approximate value for . Then, assuming that is strictly concave, say if the exact value of is greater or lower than the approximate value. Attempts So for the first request, I just applied Taylor series: With the given point and hence I got hence Now for the second part I don't know how to move. I thought about using the definition of concave function in two variables namely I have no other information. I tried to chose but I got no result since I don't know the values of the function at other points. Any help? Thank you!","f: \mathbb{R}^2\to \mathbb{R} f\left(\frac{9}{10}, \frac{1}{10}\right) = 3 f'_x\left(\frac{9}{10}, \frac{1}{10}\right) = 1 f'_y\left(\frac{9}{10}, \frac{1}{10}\right) = -2 f \left(\frac{9}{10}, \frac{1}{10}\right) f(1, 0) f f(1, 0) f(x, y) = f(x_0, y_0) + (x-x_0)f'_x(x_0, y_0) + (y-y_0)f'_y(x_0, y_0) + O(||x-y||^2) x_0, y_0 f(x, y) = x + y + 2 f(1, 0) \approx 3 f(\lambda x_1 + (1-\lambda)x_2, \lambda y_1 + (1-\lambda)y_2) \geq \lambda f(x_1, y_1) + (1-\lambda)f(x_2, y_2) \lambda = 1/2","['multivariable-calculus', 'convex-analysis', 'convex-optimization']"
26,"About $\operatorname{Alt}(\varphi_{i_1}\otimes\cdots\otimes\varphi_{i_k})$ (""Calculus on Manifolds"" by Michael Spivak)","About  (""Calculus on Manifolds"" by Michael Spivak)",\operatorname{Alt}(\varphi_{i_1}\otimes\cdots\otimes\varphi_{i_k}),"I am reading ""Calculus on Manifolds"" by Michael Spivak. The author wrote as follows: Since each $\operatorname{Alt}(\varphi_{i_1}\otimes\cdots\otimes\varphi_{i_k})$ is a constant times one of the $\varphi_{i_1}\wedge\cdots\wedge\varphi_{i_k}$ , these elements span $\Lambda^k(V)$ . I wonder why this holds. My proof of the above fact: Let $\omega_1,\dots,\omega_n\in\Lambda^1(V)$ . By the definition of the wedge product, $$\omega_1\wedge\omega_2=\frac{(1+1)!}{1!1!}\operatorname{Alt}(\omega_1\otimes\omega_2).$$ So, $\operatorname{Alt}(\omega_1\otimes\omega_2)=\frac{1}{2}(\omega_1\wedge\omega_2)$ . By Theorem 4-4(3) in the book, $$\omega_1\wedge\omega_2\wedge\omega_3=\frac{(1+1+1)!}{1!1!1!}\operatorname{Alt}(\omega_1\otimes\omega_2\otimes\omega_3).$$ So, $\operatorname{Alt}(\omega_1\otimes\omega_2\otimes\omega_3)=\frac{1}{6}(\omega_1\wedge\omega_2\wedge\omega_3)$ . By Theorem 4-4(2), $\operatorname{Alt}(\operatorname{Alt}(\omega\otimes\eta)\otimes\theta)=\operatorname{Alt}(\omega\otimes\eta\otimes\theta)$ . $$\operatorname{Alt}(\omega_1\otimes\omega_2\otimes\omega_3\otimes\omega_4) =\operatorname{Alt}(\omega_1\otimes\omega_2\otimes(\omega_3\otimes\omega_4)) =\operatorname{Alt}(\operatorname{Alt}(\omega_1\otimes\omega_2)\otimes(\omega_3\otimes\omega_4)) \\  =\operatorname{Alt}\left(\frac{1}{2}(\omega_1\wedge\omega_2)\otimes\omega_3\otimes\omega_4\right) =\frac{1}{12}(\omega_1\wedge\omega_2\wedge\omega_3\wedge\omega_4).$$ Let $n\geq 3$ . Similarly, $\operatorname{Alt}(\omega_1\otimes\cdots\otimes\omega_n)=\frac{1}{6}\left(\frac{1}{2}\right)^{n-3}(\omega_1\wedge\cdots\wedge\omega_n)$ . So, $\operatorname{Alt}(\varphi_{i_1}\otimes\cdots\otimes\varphi_{i_k})=\frac{1}{6}\left(\frac{1}{2}\right)^{k-3}(\varphi_{i_1}\wedge\cdots\wedge\varphi_{i_k})$ if $k\geq 3$ . Is my proof ok? If my proof is not ok, please tell me a proof. If my proof is ok, please tell me a better standard proof.","I am reading ""Calculus on Manifolds"" by Michael Spivak. The author wrote as follows: Since each is a constant times one of the , these elements span . I wonder why this holds. My proof of the above fact: Let . By the definition of the wedge product, So, . By Theorem 4-4(3) in the book, So, . By Theorem 4-4(2), . Let . Similarly, . So, if . Is my proof ok? If my proof is not ok, please tell me a proof. If my proof is ok, please tell me a better standard proof.","\operatorname{Alt}(\varphi_{i_1}\otimes\cdots\otimes\varphi_{i_k}) \varphi_{i_1}\wedge\cdots\wedge\varphi_{i_k} \Lambda^k(V) \omega_1,\dots,\omega_n\in\Lambda^1(V) \omega_1\wedge\omega_2=\frac{(1+1)!}{1!1!}\operatorname{Alt}(\omega_1\otimes\omega_2). \operatorname{Alt}(\omega_1\otimes\omega_2)=\frac{1}{2}(\omega_1\wedge\omega_2) \omega_1\wedge\omega_2\wedge\omega_3=\frac{(1+1+1)!}{1!1!1!}\operatorname{Alt}(\omega_1\otimes\omega_2\otimes\omega_3). \operatorname{Alt}(\omega_1\otimes\omega_2\otimes\omega_3)=\frac{1}{6}(\omega_1\wedge\omega_2\wedge\omega_3) \operatorname{Alt}(\operatorname{Alt}(\omega\otimes\eta)\otimes\theta)=\operatorname{Alt}(\omega\otimes\eta\otimes\theta) \operatorname{Alt}(\omega_1\otimes\omega_2\otimes\omega_3\otimes\omega_4)
=\operatorname{Alt}(\omega_1\otimes\omega_2\otimes(\omega_3\otimes\omega_4))
=\operatorname{Alt}(\operatorname{Alt}(\omega_1\otimes\omega_2)\otimes(\omega_3\otimes\omega_4)) \\ 
=\operatorname{Alt}\left(\frac{1}{2}(\omega_1\wedge\omega_2)\otimes\omega_3\otimes\omega_4\right)
=\frac{1}{12}(\omega_1\wedge\omega_2\wedge\omega_3\wedge\omega_4). n\geq 3 \operatorname{Alt}(\omega_1\otimes\cdots\otimes\omega_n)=\frac{1}{6}\left(\frac{1}{2}\right)^{n-3}(\omega_1\wedge\cdots\wedge\omega_n) \operatorname{Alt}(\varphi_{i_1}\otimes\cdots\otimes\varphi_{i_k})=\frac{1}{6}\left(\frac{1}{2}\right)^{k-3}(\varphi_{i_1}\wedge\cdots\wedge\varphi_{i_k}) k\geq 3","['multivariable-calculus', 'tensors', 'multilinear-algebra']"
27,what does it mean by $\| A \|$ for a matrix $A$ here?,what does it mean by  for a matrix  here?,\| A \| A,"I am trying to understand notation which I can not find the explanation for in the book... It says: Since $$ (f_1(\mathbf{x}), \ldots, f_n(\mathbf{x})) = A (\mathbf{x} - \mathbf{x}_0) + O(|\mathbf{x} - \mathbf{x}_0  |^2) $$ we have $$ |\mathbf{x} - \mathbf{x}_0| \leq \| A^{-1} \| (|(f_1(\mathbf{x}), \ldots, f_n(\mathbf{x}))| + C|\mathbf{x} - \mathbf{x}_0  |^2 ).   $$ Here $A$ is an invertible real matrix, $|\cdot |$ denotes $L^2$ norm on $\mathbb{R}^n$ and $\mathbf{x}_0$ is a fixed vector. What exactly does $\| A^{-1}\|$ mean in this context? Thank you for the clarification.","I am trying to understand notation which I can not find the explanation for in the book... It says: Since we have Here is an invertible real matrix, denotes norm on and is a fixed vector. What exactly does mean in this context? Thank you for the clarification.","
(f_1(\mathbf{x}), \ldots, f_n(\mathbf{x})) = A (\mathbf{x} - \mathbf{x}_0) + O(|\mathbf{x} - \mathbf{x}_0  |^2)
 
|\mathbf{x} - \mathbf{x}_0| \leq \| A^{-1} \| (|(f_1(\mathbf{x}), \ldots, f_n(\mathbf{x}))| + C|\mathbf{x} - \mathbf{x}_0  |^2 ).  
 A |\cdot | L^2 \mathbb{R}^n \mathbf{x}_0 \| A^{-1}\|","['real-analysis', 'multivariable-calculus']"
28,"Define $f(0,0)$ so that $f'_x(0,0)$ exists",Define  so that  exists,"f(0,0) f'_x(0,0)","I am stuck with this problem and do not know how to proceed. I have been dealing with Discrete Math for years and forgot many things about Calculus. Consider the function $f:\mathbb{R}\setminus \{(0,0)\}\to \mathbb{R}$ such that $$ f(x,y) = \frac{e^{x+y}-x-y-1}{x^2+y^2}.  $$ Determine $f(0,0)$ such that $f'_x(0,0)$ exists and also determine $f'_x(0,0)$ . What I have tried: We know that $$ f'_x(0,0) = \lim_{h\to 0} \frac{f(0+h,0)-f(0,0)}{h}. $$ Firstly I tried to calculate: $$ \lim_{h\to 0} \frac{f(0+h,0)}{h}  = \lim_{h\to 0} \frac{e^h-h-1}{h^3} $$ but the limit does not exist. So I do not think that it is possible to give any value to $f(0,0)$ such that $f'_x(0,0)$ exists. Am I missing something? Any help is appreciated.",I am stuck with this problem and do not know how to proceed. I have been dealing with Discrete Math for years and forgot many things about Calculus. Consider the function such that Determine such that exists and also determine . What I have tried: We know that Firstly I tried to calculate: but the limit does not exist. So I do not think that it is possible to give any value to such that exists. Am I missing something? Any help is appreciated.,"f:\mathbb{R}\setminus \{(0,0)\}\to \mathbb{R} 
f(x,y) = \frac{e^{x+y}-x-y-1}{x^2+y^2}. 
 f(0,0) f'_x(0,0) f'_x(0,0) 
f'_x(0,0) = \lim_{h\to 0} \frac{f(0+h,0)-f(0,0)}{h}.
 
\lim_{h\to 0} \frac{f(0+h,0)}{h} 
= \lim_{h\to 0} \frac{e^h-h-1}{h^3}
 f(0,0) f'_x(0,0)","['calculus', 'limits', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
29,Convergence of $\int_0^1 \int_0^1 (1-xy)^{-a}dxdy$,Convergence of,\int_0^1 \int_0^1 (1-xy)^{-a}dxdy,"How can I investigate the convergence of $\int_0^1 \int_0^1 (1-xy)^{-a}dxdy$ where $a \in (0, \infty)$ ? I know that one approach is to explicitly find the antiderivative but that seemed too laborious. Also, I believe that the integral converges for $a \in (0,2)$ by numerical solutions provided by WolframAlpha. Any help is greatly appreciated :)","How can I investigate the convergence of where ? I know that one approach is to explicitly find the antiderivative but that seemed too laborious. Also, I believe that the integral converges for by numerical solutions provided by WolframAlpha. Any help is greatly appreciated :)","\int_0^1 \int_0^1 (1-xy)^{-a}dxdy a \in (0, \infty) a \in (0,2)","['multivariable-calculus', 'convergence-divergence', 'improper-integrals']"
30,Limits and multivariable,Limits and multivariable,,"I want to find the limit of the function $f(x,y)$ . Note that $f: \mathbb{R}^2 \to \mathbb{R}$ . $$ f(x,y) = \begin{cases}  \dfrac{e^{-x^2}-e^{-y^2}}{x^2+y^2}  &\text{if } (x,y) \neq (0,0), \\[2pt] k&\text{if } (x,y) = (0,0).  \end{cases} $$ I want to find a function $F(x) = \lim_{y \to 0} f(x,y)$ . The way I have done has been to first substitute $0$ into $y$ , and then I got the first expression, where $x \neq 0$ . In the second I used L'Hôpital's rule to get that $k = -1$ . Note that on $x \neq 0$ the function is undefined. $$ F(x) = \begin{cases}  \dfrac{e^{-x^2}-1}{x^2}  &\text{if } x \neq 0, \\[2pt]  -1 &\text{if } x = 0. \end{cases} $$ However, I am in doubt whether my answer is correct.","I want to find the limit of the function . Note that . I want to find a function . The way I have done has been to first substitute into , and then I got the first expression, where . In the second I used L'Hôpital's rule to get that . Note that on the function is undefined. However, I am in doubt whether my answer is correct.","f(x,y) f: \mathbb{R}^2 \to \mathbb{R} 
f(x,y) = \begin{cases} 
\dfrac{e^{-x^2}-e^{-y^2}}{x^2+y^2} 
&\text{if } (x,y) \neq (0,0), \\[2pt]
k&\text{if } (x,y) = (0,0). 
\end{cases}
 F(x) = \lim_{y \to 0} f(x,y) 0 y x \neq 0 k = -1 x \neq 0 
F(x) = \begin{cases} 
\dfrac{e^{-x^2}-1}{x^2} 
&\text{if } x \neq 0, \\[2pt] 
-1 &\text{if } x = 0.
\end{cases}
","['limits', 'multivariable-calculus']"
31,Volume of a solid bounded by surfaces [closed],Volume of a solid bounded by surfaces [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 8 months ago . Improve this question can someone help me with setting up the required integral? The solid is bounded by $z = 4x^2 + 4y^2 -27$ and by $z = x^2+y^2$ . Should I project this on $x-y$ plane or use cylindrical or spherical coordinates or do something else? I am a bit lost and could use an example to crystalize how to approach these types of problems.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 8 months ago . Improve this question can someone help me with setting up the required integral? The solid is bounded by and by . Should I project this on plane or use cylindrical or spherical coordinates or do something else? I am a bit lost and could use an example to crystalize how to approach these types of problems.",z = 4x^2 + 4y^2 -27 z = x^2+y^2 x-y,"['calculus', 'integration', 'geometry', 'multivariable-calculus', 'volume']"
32,How to do the derivation of the MLE for Linear Discriminant Analysis,How to do the derivation of the MLE for Linear Discriminant Analysis,,"Derivation of the MLE for Linear Discriminant Analysis $$ \ell(\phi, \mu, \Sigma) = \log \prod_{i=1}^{M} p(x^{(i)}, y^{(i)}; \phi, \mu, \Sigma) $$ $$ = \log \prod_{i=1}^{M} p(x^{(i)}|y^{(i)}; \mu, \Sigma) p(y^{(i)}; \phi) $$ $$ = \log \prod_{i=1}^{M} \frac{1}{\sqrt{2\pi}^N |\Sigma|} \exp\left(-\frac{1}{2}(x^{(i)} - \mu_{y^{(i)}})^T \Sigma^{-1} (x^{(i)} - \mu_{y^{(i)}})\right) \prod_{c=1}^{C} \phi^{I[y^{(i)}=c]} $$ $$ = \sum_{i=1}^{M} \left[-\frac{N}{2} \log(2\pi) - \frac{1}{2} \log|\Sigma| - \frac{1}{2}(x^{(i)} - \mu_{y^{(i)}})^T \Sigma^{-1} (x^{(i)} - \mu_{y^{(i)}}) + \sum_{c=1}^{C} I[y^{(i)} = c] \log \phi_c\right]. $$ Now we need to take partial derivatives with respect to each parameter and equate it to zero. For $\mu_c$ , $$ \frac{\partial\ell(\phi, \mu_c, \Sigma)}{\partial \mu_c} = \sum_{i=1}^{M} I[y^{(i)} = c] \Sigma^{-1}(x^{(i)} - \mu_c) = 0. $$ My question So my question is, I don't really know how to do derivation of this kind of vector-equation aka going from this step: $$ = \sum_{i=1}^{M} \left[-\frac{N}{2} \log(2\pi) - \frac{1}{2} \log|\Sigma| - \frac{1}{2}(x^{(i)} - \mu_{y^{(i)}})^T \Sigma^{-1} (x^{(i)} - \mu_{y^{(i)}}) + \sum_{c=1}^{C} I[y^{(i)} = c] \log \phi_c\right]. $$ to this step: $$ \frac{\partial\ell(\phi, \mu_c, \Sigma)}{\partial \mu_c} = \sum_{i=1}^{M} I[y^{(i)} = c] \Sigma^{-1}(x^{(i)} - \mu_c) = 0. $$ I know basic linear algebra and calculus, but I have not encountered this kind of derivation problem before, and I don't know where to learn it, I have been stuck here for a long time, can someone provide me a step-by-step proof of going from that step to this step please?","Derivation of the MLE for Linear Discriminant Analysis Now we need to take partial derivatives with respect to each parameter and equate it to zero. For , My question So my question is, I don't really know how to do derivation of this kind of vector-equation aka going from this step: to this step: I know basic linear algebra and calculus, but I have not encountered this kind of derivation problem before, and I don't know where to learn it, I have been stuck here for a long time, can someone provide me a step-by-step proof of going from that step to this step please?","
\ell(\phi, \mu, \Sigma) = \log \prod_{i=1}^{M} p(x^{(i)}, y^{(i)}; \phi, \mu, \Sigma)
 
= \log \prod_{i=1}^{M} p(x^{(i)}|y^{(i)}; \mu, \Sigma) p(y^{(i)}; \phi)
 
= \log \prod_{i=1}^{M} \frac{1}{\sqrt{2\pi}^N |\Sigma|} \exp\left(-\frac{1}{2}(x^{(i)} - \mu_{y^{(i)}})^T \Sigma^{-1} (x^{(i)} - \mu_{y^{(i)}})\right) \prod_{c=1}^{C} \phi^{I[y^{(i)}=c]}
 
= \sum_{i=1}^{M} \left[-\frac{N}{2} \log(2\pi) - \frac{1}{2} \log|\Sigma| - \frac{1}{2}(x^{(i)} - \mu_{y^{(i)}})^T \Sigma^{-1} (x^{(i)} - \mu_{y^{(i)}}) + \sum_{c=1}^{C} I[y^{(i)} = c] \log \phi_c\right].
 \mu_c 
\frac{\partial\ell(\phi, \mu_c, \Sigma)}{\partial \mu_c} = \sum_{i=1}^{M} I[y^{(i)} = c] \Sigma^{-1}(x^{(i)} - \mu_c) = 0.
 
= \sum_{i=1}^{M} \left[-\frac{N}{2} \log(2\pi) - \frac{1}{2} \log|\Sigma| - \frac{1}{2}(x^{(i)} - \mu_{y^{(i)}})^T \Sigma^{-1} (x^{(i)} - \mu_{y^{(i)}}) + \sum_{c=1}^{C} I[y^{(i)} = c] \log \phi_c\right].
 
\frac{\partial\ell(\phi, \mu_c, \Sigma)}{\partial \mu_c} = \sum_{i=1}^{M} I[y^{(i)} = c] \Sigma^{-1}(x^{(i)} - \mu_c) = 0.
","['calculus', 'linear-algebra', 'probability', 'multivariable-calculus', 'machine-learning']"
33,Extension of functions with bounded derivatives,Extension of functions with bounded derivatives,,"Consider an analytic function $f:\mathbb{R}^n\to\mathbb{R}$ . Let $A\subset\mathbb{R}^n$ be a closed set and let $L$ be a positive constant such that $\|\nabla f(x)\|\le L$ for all $x\in A$ , where $\nabla f$ is the gradient of $f$ and $\|{\cdot}\|$ is a norm. I wonder whether there exists a function $F:\mathbb{R}^n\to\mathbb{R}$ such that $(1)$ $F\in C^1(\mathbb{R}^n)$ ; $(2)$ $F(x)=f(x)$ for all $x\in A$ ; $(3)$ $\|\nabla F(x)\|\le L$ for all $x\in\mathbb{R}^n$ . In other words, $F$ extends condition $(3)$ from $A$ to $\mathbb{R}^n$ with or without analyticity.","Consider an analytic function . Let be a closed set and let be a positive constant such that for all , where is the gradient of and is a norm. I wonder whether there exists a function such that ; for all ; for all . In other words, extends condition from to with or without analyticity.",f:\mathbb{R}^n\to\mathbb{R} A\subset\mathbb{R}^n L \|\nabla f(x)\|\le L x\in A \nabla f f \|{\cdot}\| F:\mathbb{R}^n\to\mathbb{R} (1) F\in C^1(\mathbb{R}^n) (2) F(x)=f(x) x\in A (3) \|\nabla F(x)\|\le L x\in\mathbb{R}^n F (3) A \mathbb{R}^n,"['real-analysis', 'multivariable-calculus', 'derivatives']"
34,Clarification on treating a variable as constant in partial derivatives vs. allowing it to vary in total differentials,Clarification on treating a variable as constant in partial derivatives vs. allowing it to vary in total differentials,,"I am taking a course in mathematics for physicists and in one lesson we discussed total differentials ( $df=\frac{\partial f}{\partial x}dx+\frac{\partial f}{\partial y}dy$ ). There is a point that's puzzling me (I know that it is basic, sorry): the apparent contradiction in the treatment of a variable as both constant and varying in different contexts. Specifically, I'm trying to reconcile how, in the calculation of partial derivatives, we treat a variable (let's say y) as constant, yet in the context of total differentials, we allow the same variable (y) to vary (as indicated by its non-zero differential dy). This seems contradictory to me, as we're simultaneously treating y as both a constant and a variable. How can we justify treating a variable as constant in one scenario while allowing it to vary in another scenario? Could someone provide an explanation that clarifies this situation, other than the ""different perspectives"" or ""two contexts"" explanation? Thank you for any insights or references you can provide!","I am taking a course in mathematics for physicists and in one lesson we discussed total differentials ( ). There is a point that's puzzling me (I know that it is basic, sorry): the apparent contradiction in the treatment of a variable as both constant and varying in different contexts. Specifically, I'm trying to reconcile how, in the calculation of partial derivatives, we treat a variable (let's say y) as constant, yet in the context of total differentials, we allow the same variable (y) to vary (as indicated by its non-zero differential dy). This seems contradictory to me, as we're simultaneously treating y as both a constant and a variable. How can we justify treating a variable as constant in one scenario while allowing it to vary in another scenario? Could someone provide an explanation that clarifies this situation, other than the ""different perspectives"" or ""two contexts"" explanation? Thank you for any insights or references you can provide!",df=\frac{\partial f}{\partial x}dx+\frac{\partial f}{\partial y}dy,['calculus']
35,Partial Differential when a variable is a function of itself,Partial Differential when a variable is a function of itself,,"I am given the following: $$ z = f(\frac{ny-mz}{nx -lz}) $$ And I have to show that $$ (nx-lz) \frac{\partial z}{\partial x} + (ny-mz) \frac{\partial z}{\partial y} = 0$$ The book I am referring to proceeds in the following way: $$ \frac{\partial z}{\partial x} = f'(\frac{ny-mz}{nx -lz})\cdot(ny-mz)\cdot\frac{-n}{(nx-lz)^2}$$ But shouldn't it be $$ \frac{\partial z}{\partial x} = f'(\frac{ny-mz}{nx -lz})\cdot[(ny-mz)\cdot\frac{-(n -l \frac{\partial z}{\partial x})}{(nx-lz)^2} + \frac{(-m)\frac{\partial z}{\partial x}}{(nx-lz)}]$$ According to my understanding if we are computing the differential change in $z$ wrt $x$ , then it cannot be treated as a constant anywhere it appears in the equation. Am I correct?","I am given the following: And I have to show that The book I am referring to proceeds in the following way: But shouldn't it be According to my understanding if we are computing the differential change in wrt , then it cannot be treated as a constant anywhere it appears in the equation. Am I correct?", z = f(\frac{ny-mz}{nx -lz})   (nx-lz) \frac{\partial z}{\partial x} + (ny-mz) \frac{\partial z}{\partial y} = 0  \frac{\partial z}{\partial x} = f'(\frac{ny-mz}{nx -lz})\cdot(ny-mz)\cdot\frac{-n}{(nx-lz)^2}  \frac{\partial z}{\partial x} = f'(\frac{ny-mz}{nx -lz})\cdot[(ny-mz)\cdot\frac{-(n -l \frac{\partial z}{\partial x})}{(nx-lz)^2} + \frac{(-m)\frac{\partial z}{\partial x}}{(nx-lz)}] z x,"['multivariable-calculus', 'partial-differential-equations']"
36,$u(x)=\frac{1}{4\pi} \int_{G} \Bigl[\nabla_y \frac{1}{\lvert x-y\rvert} \Bigr] \times \omega(y) dy+A(x)$ for $\omega=\nabla \times u$ and harmonic $A$,for  and harmonic,u(x)=\frac{1}{4\pi} \int_{G} \Bigl[\nabla_y \frac{1}{\lvert x-y\rvert} \Bigr] \times \omega(y) dy+A(x) \omega=\nabla \times u A,"Let $U$ be an open region in $\mathbb{R}^3$ and $G \subset U$ be another open set whose closure is compact in $U$ . Then, for any smooth divergence-free $u : U \to \mathbb{R}^3$ , I ran into the statement that \begin{equation} u(x)=\frac{1}{4\pi} \int_{G} \Bigl[\nabla_y \frac{1}{\lvert x-y\rvert} \Bigr] \times \omega(y) dy+A(x) \text{ for } x \in G \end{equation} where $\omega=\nabla \times u$ and $A : G \to \mathbb{R}^3$ satisfies $\Delta A=0$ componentwise. There is no detailed proof and just a brief mention of Hodge's Theorem. But, I cannot see how to make use of the theorem. Also, I have difficulty with proving the formula directly with vector calculus.. Could anyone please help me? Edit : I forgot the divergence-free condition for $u$ , so I add it to the main body.","Let be an open region in and be another open set whose closure is compact in . Then, for any smooth divergence-free , I ran into the statement that where and satisfies componentwise. There is no detailed proof and just a brief mention of Hodge's Theorem. But, I cannot see how to make use of the theorem. Also, I have difficulty with proving the formula directly with vector calculus.. Could anyone please help me? Edit : I forgot the divergence-free condition for , so I add it to the main body.","U \mathbb{R}^3 G \subset U U u : U \to \mathbb{R}^3 \begin{equation}
u(x)=\frac{1}{4\pi} \int_{G} \Bigl[\nabla_y \frac{1}{\lvert x-y\rvert} \Bigr] \times \omega(y) dy+A(x) \text{ for } x \in G
\end{equation} \omega=\nabla \times u A : G \to \mathbb{R}^3 \Delta A=0 u","['calculus', 'multivariable-calculus', 'differential-geometry', 'vector-analysis']"
37,Matrix of rotation in $\mathbb R^{3}$,Matrix of rotation in,\mathbb R^{3},"I'm attempting to teach myself basic multivariable calculus and so far finding some of the 3d visualizations a bit difficult. Here's an example: Let $f: \mathbb R^{3} \longrightarrow \mathbb R^{3}$ be the rotation by $\pi$ about the line $x_1 = x_2$ and $x_3 = 0$ in the $x_1x_2$ plane. What is the matrix of $f$ with respect to the standard bases? I suppose the line is the intersection of the plane $x_1 = x_2$ with plane $x_3 = 0$ . However, it's unclear to me just what thing (for lack of a better term) is being rotated when we say ""rotated about such-and-such in the so-and-so (plane, etc)"". Silly question, but what would be the difference from just rotating the axes $x_1$ and $x_2$ themselves by $\pi$ , vs. what is specifically asked to be rotated in this problem? The more images and visual intuitions one can offer, the better. Thank you.","I'm attempting to teach myself basic multivariable calculus and so far finding some of the 3d visualizations a bit difficult. Here's an example: Let be the rotation by about the line and in the plane. What is the matrix of with respect to the standard bases? I suppose the line is the intersection of the plane with plane . However, it's unclear to me just what thing (for lack of a better term) is being rotated when we say ""rotated about such-and-such in the so-and-so (plane, etc)"". Silly question, but what would be the difference from just rotating the axes and themselves by , vs. what is specifically asked to be rotated in this problem? The more images and visual intuitions one can offer, the better. Thank you.",f: \mathbb R^{3} \longrightarrow \mathbb R^{3} \pi x_1 = x_2 x_3 = 0 x_1x_2 f x_1 = x_2 x_3 = 0 x_1 x_2 \pi,"['linear-algebra', 'multivariable-calculus', 'linear-transformations', 'rotations']"
38,Stokes' Theorem and perimeter,Stokes' Theorem and perimeter,,"As a consequence of Stokes' Theorem it seems that the perimeter of a closed curve $C$ can be obtained by choosing $F$ to be the vector field formed by rewriting the unit tangent $T$ of $C$ as a function of position. Let $T$ be rewritten as a vector field on $\Bbb R^3$ : $$ T =  \langle f(x,y,z), g(x,y,z), h(x,y,z) \rangle. $$ This vector field obtained from $T$ is defined on the surface patch bounded by $C$ if this vector field for $T$ is defined on a subset of $\Bbb R^3$ containing the surface patch bounded by $C$ . Integrating the curl of this vector field over the surface patch whose boundary is $C$ , you get the perimeter (length) of the curve. For example, suppose the surface is the paraboloid $z=x^2+y^2$ and the curve $C$ is the circle of radius $R$ in the $z=R^2$ plane. The unit tangent of $C$ in terms of $x, y$ and $z$ would be $\langle  -y/R , x/R , 0 \rangle$ . Integrating the curl of this vector field over the surface patch whose boundary is $C$ , you get $2\pi R$ . I've verified it works on a lot of examples. Analogously, Green's theorem can also be used to find perimeter. Simply find $f$ and $g$ for $T = \langle f(x,y), g(x,y) \rangle $ . Is this a common application of Stokes' Theorem / Green's Theorem? Can anyone attest to ever having heard of it before? The only reasons I have for believing this actually works are that the math loosely seems to make intuitive sense and that it didn't produce any contradictions when I tested it on examples. And when I say that the math loosely seems to make intuitive sense, I mean that the length of a curve is equal to the line integral of the unit tangent over the curve.","As a consequence of Stokes' Theorem it seems that the perimeter of a closed curve can be obtained by choosing to be the vector field formed by rewriting the unit tangent of as a function of position. Let be rewritten as a vector field on : This vector field obtained from is defined on the surface patch bounded by if this vector field for is defined on a subset of containing the surface patch bounded by . Integrating the curl of this vector field over the surface patch whose boundary is , you get the perimeter (length) of the curve. For example, suppose the surface is the paraboloid and the curve is the circle of radius in the plane. The unit tangent of in terms of and would be . Integrating the curl of this vector field over the surface patch whose boundary is , you get . I've verified it works on a lot of examples. Analogously, Green's theorem can also be used to find perimeter. Simply find and for . Is this a common application of Stokes' Theorem / Green's Theorem? Can anyone attest to ever having heard of it before? The only reasons I have for believing this actually works are that the math loosely seems to make intuitive sense and that it didn't produce any contradictions when I tested it on examples. And when I say that the math loosely seems to make intuitive sense, I mean that the length of a curve is equal to the line integral of the unit tangent over the curve.","C F T C T \Bbb R^3 
T =  \langle f(x,y,z), g(x,y,z), h(x,y,z) \rangle.
 T C T \Bbb R^3 C C z=x^2+y^2 C R z=R^2 C x, y z \langle  -y/R , x/R , 0 \rangle C 2\pi R f g T = \langle f(x,y), g(x,y) \rangle ","['multivariable-calculus', 'differential-geometry', 'vector-analysis', 'stokes-theorem', 'greens-theorem']"
39,Does the double integral diverge? $\iint_{\Omega} \sqrt{\frac{x^2 + y^2}{xy(1 - x^2 - y^2)}} ~ dx dy$,Does the double integral diverge?,\iint_{\Omega} \sqrt{\frac{x^2 + y^2}{xy(1 - x^2 - y^2)}} ~ dx dy,"Does the double integral converge $$\iint_{\Omega}  \sqrt{\frac{x^2 + y^2}{xy(1 - x^2 - y^2)}} ~ dx dy$$ I tried using polar coordinate but then half way I realised it was not a good idea so what do I do? I got $$\iint_{\Omega} \frac{r}{\sqrt{\cos(\theta)\sin(\theta) (1-r^2)}}drd\theta = \int \frac1{\sqrt{\cos(\theta)\sin(\theta)}}d\theta \times \int \frac{r}{\sqrt{(1-r^2)}},dr$$ $$\Omega = \left\{ (x, y) \in \mathbb{R}^2 : x^2 + y^2 < 1, x > 0, y > 0 \right\}$$ But I don't know how to continue, I haven't written the bounds yet because I was trying to simplify it.","Does the double integral converge I tried using polar coordinate but then half way I realised it was not a good idea so what do I do? I got But I don't know how to continue, I haven't written the bounds yet because I was trying to simplify it.","\iint_{\Omega}  \sqrt{\frac{x^2 + y^2}{xy(1 - x^2 - y^2)}} ~ dx dy \iint_{\Omega} \frac{r}{\sqrt{\cos(\theta)\sin(\theta) (1-r^2)}}drd\theta = \int \frac1{\sqrt{\cos(\theta)\sin(\theta)}}d\theta \times \int \frac{r}{\sqrt{(1-r^2)}},dr \Omega = \left\{ (x, y) \in \mathbb{R}^2 : x^2 + y^2 < 1, x > 0, y > 0 \right\}","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'improper-integrals']"
40,"Multivariable calculus: strict local minimum of $f(x,y) = y^2 - 3x^2y+2x^4$",Multivariable calculus: strict local minimum of,"f(x,y) = y^2 - 3x^2y+2x^4","Task Let $p:=(0,0)$ and $f: \mathbb{R}^{2} \longrightarrow \mathbb{R}$ be the function $$f(x, y):=y^{2}-3 x^{2} y+2 x^{4}$$ Show: a) For every $a=\left(a_{1}, a_{2}\right) \in \mathbb{R}^{2} \backslash\{(0,0)\}$ , the function $g_{a}: \mathbb{R} \longrightarrow \mathbb{R}$ $$g_{a}(t):=f(p+t a)$$ has a strict local minimum at $t=0$ . b) $f$ has no local minimum at $p$ . Hint: Consider $f$ along the curve $\gamma: t \mapsto\left(t, \sqrt{2} t^{2}\right)$ passing through $p$ . My solution: a) We have $$g_{a}(t)=f(p+ta)=f((0,0)+t\cdot(a_1,a_2))=f(ta_1,ta_2)=t^2a_2^2-3t^3a_1^2a_2+2t^4a_1^4$$ $$g_a'(t)=2ta_2^2-9t^2a_1^2a_2+8t^3a_1^4$$ Necessary condition: $g_a'(t)=0$ , $0=2ta_2^2-9t^2a_1^2a_2+8t^3a_1^4$ , $0=t(2a_2^2-9ta_1^2a_2+8t^2a_1^4)$ Case 1 $t_1=0$ Case 2 $$\begin{aligned} 2a_2^2-9ta_1^2a_2+8t^2a_1^4 &= \frac{2a_2^2}{8a_1^4}-\frac{9a_1^2a_2}{8a_1^4}t+t^2 \\ &= \frac{a_2^2}{4a_1^4}-\frac{9a_2}{8a_1^2}t+t^2, \\ t_{2,3} &= \frac{9a_2}{16a_1^2}\pm\sqrt{\frac{81a_2^2}{256a_1^4}-\frac{64a_2^2}{256a_1^4}} \\ &= \frac{9a_2}{16a_1^2}\pm\frac{\sqrt{17}a_2}{16a_1^2}, \\ t_2 &= \frac{a_2(9+\sqrt{17})}{16a_1^2}, \\ t_3 &= \frac{a_2(9-\sqrt{17})}{16a_1^2} \end{aligned}$$ Sufficient condition $$g_a''(t)=2a_2^2-18a_1^2a_2t+24t^2a_1^4$$ Then, $$g_a''(t_1)=2a_2^2>0$$ and $$\begin{aligned}0<t^2a_2^2-3t^3a_1^2a_2+2t^4a_1^4,\\  0<t^2(ta_1^2-a_2)(2ta_1^2-a_2)\end{aligned}(*)$$ $$\Rightarrow \text{True statement}$$ $$\Rightarrow t_1=0 \text{ is a strict local minimum}$$ Problems: I'm struggeling with my approach of (*). And for b) I don't really have an idea but I substituted $(t,\sqrt{2}t^2)$ into $f$ and get $f(t,\sqrt{2}t^2) = t^4(\sqrt{16}-\sqrt{18})< 0$ . Thanks so much for your help!","Task Let and be the function Show: a) For every , the function has a strict local minimum at . b) has no local minimum at . Hint: Consider along the curve passing through . My solution: a) We have Necessary condition: , , Case 1 Case 2 Sufficient condition Then, and Problems: I'm struggeling with my approach of (*). And for b) I don't really have an idea but I substituted into and get . Thanks so much for your help!","p:=(0,0) f: \mathbb{R}^{2} \longrightarrow \mathbb{R} f(x, y):=y^{2}-3 x^{2} y+2 x^{4} a=\left(a_{1}, a_{2}\right) \in \mathbb{R}^{2} \backslash\{(0,0)\} g_{a}: \mathbb{R} \longrightarrow \mathbb{R} g_{a}(t):=f(p+t a) t=0 f p f \gamma: t \mapsto\left(t, \sqrt{2} t^{2}\right) p g_{a}(t)=f(p+ta)=f((0,0)+t\cdot(a_1,a_2))=f(ta_1,ta_2)=t^2a_2^2-3t^3a_1^2a_2+2t^4a_1^4 g_a'(t)=2ta_2^2-9t^2a_1^2a_2+8t^3a_1^4 g_a'(t)=0 0=2ta_2^2-9t^2a_1^2a_2+8t^3a_1^4 0=t(2a_2^2-9ta_1^2a_2+8t^2a_1^4) t_1=0 \begin{aligned}
2a_2^2-9ta_1^2a_2+8t^2a_1^4 &= \frac{2a_2^2}{8a_1^4}-\frac{9a_1^2a_2}{8a_1^4}t+t^2 \\
&= \frac{a_2^2}{4a_1^4}-\frac{9a_2}{8a_1^2}t+t^2, \\
t_{2,3} &= \frac{9a_2}{16a_1^2}\pm\sqrt{\frac{81a_2^2}{256a_1^4}-\frac{64a_2^2}{256a_1^4}} \\
&= \frac{9a_2}{16a_1^2}\pm\frac{\sqrt{17}a_2}{16a_1^2}, \\
t_2 &= \frac{a_2(9+\sqrt{17})}{16a_1^2}, \\
t_3 &= \frac{a_2(9-\sqrt{17})}{16a_1^2}
\end{aligned} g_a''(t)=2a_2^2-18a_1^2a_2t+24t^2a_1^4 g_a''(t_1)=2a_2^2>0 \begin{aligned}0<t^2a_2^2-3t^3a_1^2a_2+2t^4a_1^4,\\ 
0<t^2(ta_1^2-a_2)(2ta_1^2-a_2)\end{aligned}(*) \Rightarrow \text{True statement} \Rightarrow t_1=0 \text{ is a strict local minimum} (t,\sqrt{2}t^2) f f(t,\sqrt{2}t^2) = t^4(\sqrt{16}-\sqrt{18})< 0","['calculus', 'multivariable-calculus', 'derivatives', 'solution-verification', 'maxima-minima']"
41,Is the curve $t \mapsto t \sin \frac 1 t$ rectifiable?,Is the curve  rectifiable?,t \mapsto t \sin \frac 1 t,"Let $a\le 0\le b $ and let $ \gamma : \left[ a,b \right] \to \mathbb{R}$ be defined by $\gamma(t)= t\text{sin}\frac{1}{t}$ for $  t\neq 0 $ and $\gamma(t)=0$ for $t=0$ . I want to investigate that if this function is rectifiable or not. I know that for paths or smooth curves being rectifiable is guaranteed. So, since $\gamma'(t) = \sin \frac{1}{t} - \frac{1}{t} \cos \frac{1}{t}$ for $  t\neq 0 $ , $\gamma'$ is continuous and bounded for $  t\neq 0 $ , hence a smooth curve and hence rectifiable. However, I highly doubt that this line of thinking is correct because the function is piecewisely defined, which is my main problem when attacking the problem. I know that there are two equivalent definitions for being rectifiable, one with the sup and the other is a convergence of the net of lengths of polygonal paths. However, I could not find a way to proceed from either definition.","Let and let be defined by for and for . I want to investigate that if this function is rectifiable or not. I know that for paths or smooth curves being rectifiable is guaranteed. So, since for , is continuous and bounded for , hence a smooth curve and hence rectifiable. However, I highly doubt that this line of thinking is correct because the function is piecewisely defined, which is my main problem when attacking the problem. I know that there are two equivalent definitions for being rectifiable, one with the sup and the other is a convergence of the net of lengths of polygonal paths. However, I could not find a way to proceed from either definition.","a\le 0\le b   \gamma : \left[ a,b \right] \to \mathbb{R} \gamma(t)= t\text{sin}\frac{1}{t}   t\neq 0  \gamma(t)=0 t=0 \gamma'(t) = \sin \frac{1}{t} - \frac{1}{t} \cos \frac{1}{t}   t\neq 0  \gamma'   t\neq 0 ","['real-analysis', 'analysis', 'multivariable-calculus', 'vector-analysis', 'arc-length']"
42,Prove that if $\textbf{g}(t)\parallel \textbf{g}''(t)$ the area of the triangle $OAB$ does not depend on the variable $t$,Prove that if  the area of the triangle  does not depend on the variable,\textbf{g}(t)\parallel \textbf{g}''(t) OAB t,"Let's assume $\textbf{g}(t)$ is a position vector of point $A$ , where $t$ is a variable. In addition, $\textbf{g}'(t)$ is the position vector of point $B$ . I have to prove that if $\textbf{g}(t)\parallel \textbf{g}''(t)$ , the area of the triangle $OAB$ does not depend on $t$ assuming that $\textbf{g}(t)$ is differentiable $2$ times. This is what I tried to do: Let's say $$\textbf{g}(t)=\begin{bmatrix} a_{1} \\a_{2} \\ a_{3} \end{bmatrix}$$ $$\textbf{g}'(t)=\begin{bmatrix} a'_{1} \\a'_{2} \\ a'_{3} \end{bmatrix}$$ $$\textbf{g}''(t)=\begin{bmatrix} a''_{1} \\a''_{2} \\ a''_{3} \end{bmatrix}$$ When $\textbf{g}(t)\parallel \textbf{g}''(t)$ the cross product of the two vectors becomes $$\begin{vmatrix} \textbf{i} & \textbf{j} & \textbf{k} \\ a_{1} & a_{2} & a_{3} \\  a''_{1}& a''_{2} &a''_{3}  \\ \end{vmatrix}=\textbf{i}(a_{2}a''_{3}-a''_{2} a_{3})+\textbf{j}(a_{3} a''_{1}-a_{1}a''_{3})+\textbf{k}(a_{1}a''_{2}- a''_{1}a_{2})=\textbf{0}$$ The area of $OAB$ is $$S_{OAB}=\frac{1}{2}\left|\textbf{g}(t)\times  \textbf{g}'(t)\right|$$ The derivative of the inner product of $\textbf{g}(t)$ and $\textbf{g}'(t)$ is $$(\textbf{g}(t)\cdot \textbf{g}'(t))'=\textbf{g}'(t)\cdot \textbf{g}'(t)+\textbf{g}(t)\cdot \textbf{g}''(t)=\left\| \textbf{g}'(t)\right\|^2+\textbf{g}(t)\cdot \textbf{g}''(t)$$ That's all of the information I could extract and now I don't know what to do. Any suggestions?","Let's assume is a position vector of point , where is a variable. In addition, is the position vector of point . I have to prove that if , the area of the triangle does not depend on assuming that is differentiable times. This is what I tried to do: Let's say When the cross product of the two vectors becomes The area of is The derivative of the inner product of and is That's all of the information I could extract and now I don't know what to do. Any suggestions?","\textbf{g}(t) A t \textbf{g}'(t) B \textbf{g}(t)\parallel \textbf{g}''(t) OAB t \textbf{g}(t) 2 \textbf{g}(t)=\begin{bmatrix}
a_{1} \\a_{2}
\\ a_{3}
\end{bmatrix} \textbf{g}'(t)=\begin{bmatrix}
a'_{1} \\a'_{2}
\\ a'_{3}
\end{bmatrix} \textbf{g}''(t)=\begin{bmatrix}
a''_{1} \\a''_{2}
\\ a''_{3}
\end{bmatrix} \textbf{g}(t)\parallel \textbf{g}''(t) \begin{vmatrix}
\textbf{i} & \textbf{j} & \textbf{k} \\
a_{1} & a_{2} & a_{3} \\
 a''_{1}& a''_{2} &a''_{3}  \\
\end{vmatrix}=\textbf{i}(a_{2}a''_{3}-a''_{2} a_{3})+\textbf{j}(a_{3} a''_{1}-a_{1}a''_{3})+\textbf{k}(a_{1}a''_{2}- a''_{1}a_{2})=\textbf{0} OAB S_{OAB}=\frac{1}{2}\left|\textbf{g}(t)\times  \textbf{g}'(t)\right| \textbf{g}(t) \textbf{g}'(t) (\textbf{g}(t)\cdot \textbf{g}'(t))'=\textbf{g}'(t)\cdot \textbf{g}'(t)+\textbf{g}(t)\cdot \textbf{g}''(t)=\left\| \textbf{g}'(t)\right\|^2+\textbf{g}(t)\cdot \textbf{g}''(t)","['multivariable-calculus', 'vectors', 'cross-product']"
43,"Second order derivative of $f(x):=\frac{1}{2} ⟨x,Ax⟩$",Second order derivative of,"f(x):=\frac{1}{2} ⟨x,Ax⟩","Let $A=\left(A_{i j}\right)$ be an $n \times n$ symmetric matrix, and define the function $f: \mathbb{R}^n \rightarrow \mathbb{R}$ as $$ f(x):=  \frac{1}{2} ​  ⟨x,Ax⟩ $$ Using the definition, determine the second-order derivative $D^2 f(a) \in \operatorname{Hom}^2\left(\mathbb{R}^n, \mathbb{R}\right)$ . (Here, $\operatorname{Hom}^2\left(\mathbb{R}^n, \mathbb{R}\right)$ refers to the space of bilinear maps $\mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R}$ .). I have found the derivative using the definition, that is $a^TA$ , and I know that the definition of the second-order derivative is $\left(D^2 f\right)(a):=D(D f)(a) \in \operatorname{Hom}^2\left(\mathbb{R}^n, \mathbb{R}^m\right)$ , but I don't know how to use this to find the second-order derivative.","Let be an symmetric matrix, and define the function as Using the definition, determine the second-order derivative . (Here, refers to the space of bilinear maps .). I have found the derivative using the definition, that is , and I know that the definition of the second-order derivative is , but I don't know how to use this to find the second-order derivative.","A=\left(A_{i j}\right) n \times n f: \mathbb{R}^n \rightarrow \mathbb{R} 
f(x):= 
\frac{1}{2}
​
 ⟨x,Ax⟩
 D^2 f(a) \in \operatorname{Hom}^2\left(\mathbb{R}^n, \mathbb{R}\right) \operatorname{Hom}^2\left(\mathbb{R}^n, \mathbb{R}\right) \mathbb{R}^n \times \mathbb{R}^n \rightarrow \mathbb{R} a^TA \left(D^2 f\right)(a):=D(D f)(a) \in \operatorname{Hom}^2\left(\mathbb{R}^n, \mathbb{R}^m\right)","['multivariable-calculus', 'derivatives', 'quadratic-forms', 'hessian-matrix', 'scalar-fields']"
44,Extension of the Multivariate Faa di bruno's formula with more than two composite functions,Extension of the Multivariate Faa di bruno's formula with more than two composite functions,,"The Faa di bruno formula for one variable (Wikipedia) is The combinatorial forms in terms of bell polynomials are also included Similarly, the multivariate formula (Wikipedia) is expressed combinatorially. A more generalized version of this multivariate formula is also found and proven in this article by L. Hernandez Encinas and J Munoz Masque https://reader.elsevier.com/reader/sd/pii/S0893965903900267?token=C596F6F1C8B7EFD6FF6D8B18EDE8A76848F5F401BAE2F7A41A1BDB28EC54645DA66A482C4567EC33FA70B6D1B2B6F064&originRegion=eu-west-1&originCreation=20230512055810 . Few other citations have also proved that same formula, so I will refer to them as well. https://www.cambridge.org/core/services/aop-cambridge-core/content/view/BA960C9CAB40DE52DB84799BEB0C730A/S1446181100003527a.pdf/the-multivariate-faa-di-bruno-formula-and-multivariate-taylor-expansions-with-explicit-integral-remainder-term.pdf https://www.ams.org/journals/tran/1996-348-02/S0002-9947-96-01501-2/S0002-9947-96-01501-2.pdf https://arxiv.org/pdf/1012.6008.pdf As the readers of these articles can see that in all these cases, Faa Di Bruno's formula was restricted to only two nested functions, there was no mentioning of extending the formula for several nested functions, which brings me back to my question that, What methodologies shall be opted in order to extend the multivariate Faa di Bruno's formula for several nested functions ? Any advise or editing in the question format or any correction in the formatting would be acknowledged from my side as a part of learning. Thanks for giving me your very precious time Kabir Munjal","The Faa di bruno formula for one variable (Wikipedia) is The combinatorial forms in terms of bell polynomials are also included Similarly, the multivariate formula (Wikipedia) is expressed combinatorially. A more generalized version of this multivariate formula is also found and proven in this article by L. Hernandez Encinas and J Munoz Masque https://reader.elsevier.com/reader/sd/pii/S0893965903900267?token=C596F6F1C8B7EFD6FF6D8B18EDE8A76848F5F401BAE2F7A41A1BDB28EC54645DA66A482C4567EC33FA70B6D1B2B6F064&originRegion=eu-west-1&originCreation=20230512055810 . Few other citations have also proved that same formula, so I will refer to them as well. https://www.cambridge.org/core/services/aop-cambridge-core/content/view/BA960C9CAB40DE52DB84799BEB0C730A/S1446181100003527a.pdf/the-multivariate-faa-di-bruno-formula-and-multivariate-taylor-expansions-with-explicit-integral-remainder-term.pdf https://www.ams.org/journals/tran/1996-348-02/S0002-9947-96-01501-2/S0002-9947-96-01501-2.pdf https://arxiv.org/pdf/1012.6008.pdf As the readers of these articles can see that in all these cases, Faa Di Bruno's formula was restricted to only two nested functions, there was no mentioning of extending the formula for several nested functions, which brings me back to my question that, What methodologies shall be opted in order to extend the multivariate Faa di Bruno's formula for several nested functions ? Any advise or editing in the question format or any correction in the formatting would be acknowledged from my side as a part of learning. Thanks for giving me your very precious time Kabir Munjal",,"['calculus', 'linear-algebra', 'combinatorics', 'multivariable-calculus', 'bell-numbers']"
45,Proving the existence of the limit of a multivariate function,Proving the existence of the limit of a multivariate function,,"It is possible to show that the limit $$ \lim_{(x,y)\to (0,0)} \frac{\sqrt{x^2+y^2}}{|x|+|y|^{\frac{1}{3}}} $$ doesn't exist by approaching the origin in two different ways: along the $x$ -axis the limit equals 1, but along the $y$ -axis the limit equals 0. However if I suppose that $\lim_{(x,y)\to (0,0)} \frac{\sqrt{x^2+y^2}}{|x|+|y|^{\frac{1}{3}}}=0$ and use the definition of the limit of a function: $$ \forall \varepsilon>0 \; \exists \delta : \forall(x,y) \in \mathbb{R}^2\setminus {(0,0)} \; \; |(x,y)|<\delta \implies \left|\frac{\sqrt{x^2+y^2}}{|x|+|y|^{\frac{1}{3}}} \right|<\varepsilon $$ I can set $\delta = \varepsilon (|x|+|y|^{\frac{1}{3}})$ and then by definition the limit equals $0$ .","It is possible to show that the limit doesn't exist by approaching the origin in two different ways: along the -axis the limit equals 1, but along the -axis the limit equals 0. However if I suppose that and use the definition of the limit of a function: I can set and then by definition the limit equals ."," \lim_{(x,y)\to (0,0)} \frac{\sqrt{x^2+y^2}}{|x|+|y|^{\frac{1}{3}}}  x y \lim_{(x,y)\to (0,0)} \frac{\sqrt{x^2+y^2}}{|x|+|y|^{\frac{1}{3}}}=0  \forall \varepsilon>0 \; \exists \delta : \forall(x,y) \in \mathbb{R}^2\setminus {(0,0)} \; \; |(x,y)|<\delta \implies \left|\frac{\sqrt{x^2+y^2}}{|x|+|y|^{\frac{1}{3}}} \right|<\varepsilon  \delta = \varepsilon (|x|+|y|^{\frac{1}{3}}) 0","['real-analysis', 'calculus', 'multivariable-calculus']"
46,How to find the mean distance between a point inside a ball and the surface of this ball?,How to find the mean distance between a point inside a ball and the surface of this ball?,,"Say I have a sphere of radius $R$ , I would like to know the average distance between a point $M$ inside the ball and the sphere of radius $R$ . Because of the spherical symmetry, we can say without loss of generality that my point $M$ inside the ball is defined by the coordinates: $$ (x=r,y=0,z=0) $$ , and the points on the sphere are defined by the coordinates: $$(x=R\cos(\phi)\cos(\theta),y=R\cos(\phi)\sin(\theta),z=R\cos(\theta))$$ then the average distance is : \begin{equation} d = \frac{1}{4 \pi} \int_0^{\pi} \int_0^{2\pi} \sqrt{ (R \cdot \cos(\phi) \cdot \cos(\theta)-r)^2 + (R \cdot \cos(\phi) \cdot \sin(\theta) )^2 + ( R \cdot \sin(\theta) )^2 } \cdot  \sin(\theta) d\theta d\phi  \end{equation} \begin{equation} d = \frac{1}{4 \pi } \int_0^{\pi} \int_0^{2\pi} \sqrt{ R^2 + r^2 - 2 R r \cos(\theta)\cos(\phi) } \sin(\theta) d\theta d\phi  \end{equation} does someone know how to solve this integral? I computed it numerically (the mean distance between a point inside a ball and the surface) and the result should be: $$ d = \frac{r^2}{3R}+R $$ Thank you","Say I have a sphere of radius , I would like to know the average distance between a point inside the ball and the sphere of radius . Because of the spherical symmetry, we can say without loss of generality that my point inside the ball is defined by the coordinates: , and the points on the sphere are defined by the coordinates: then the average distance is : does someone know how to solve this integral? I computed it numerically (the mean distance between a point inside a ball and the surface) and the result should be: Thank you","R M R M  (x=r,y=0,z=0)  (x=R\cos(\phi)\cos(\theta),y=R\cos(\phi)\sin(\theta),z=R\cos(\theta)) \begin{equation}
d = \frac{1}{4 \pi} \int_0^{\pi} \int_0^{2\pi} \sqrt{ (R \cdot \cos(\phi) \cdot \cos(\theta)-r)^2 + (R \cdot \cos(\phi) \cdot \sin(\theta) )^2 + ( R \cdot \sin(\theta) )^2 } \cdot  \sin(\theta) d\theta d\phi 
\end{equation} \begin{equation}
d = \frac{1}{4 \pi } \int_0^{\pi} \int_0^{2\pi} \sqrt{ R^2 + r^2 - 2 R r \cos(\theta)\cos(\phi) } \sin(\theta) d\theta d\phi 
\end{equation}  d = \frac{r^2}{3R}+R ","['probability', 'multivariable-calculus', 'spherical-coordinates']"
47,How to calculate the hemisphere using triple integrals?,How to calculate the hemisphere using triple integrals?,,"How to solve this $\displaystyle\iiint _K (y + x^2) \mathrm{d}x\mathrm{d}y\mathrm{d}z$ where $K$ is the hemisphere $x^2 + y^2 + z^2 \leq 4$ , $z \geq 0$ . This is what I tried so far: I used variable substitution $$\iiint_K (r \sin(\phi)\sin(\theta) + r^2 \sin^2(\phi)\cos^2(\theta)) r^2 \sin(\phi) \mathrm{d}r\mathrm{d}\phi\mathrm{d}\theta$$ Here is $K$ given by the new variables: $ 0 \leq r \leq 2$ and $ 0 \leq \theta \leq 2\pi$ and $0 \leq \phi \leq \pi$ The problem is I have no idea how to integrate this.","How to solve this where is the hemisphere , . This is what I tried so far: I used variable substitution Here is given by the new variables: and and The problem is I have no idea how to integrate this.",\displaystyle\iiint _K (y + x^2) \mathrm{d}x\mathrm{d}y\mathrm{d}z K x^2 + y^2 + z^2 \leq 4 z \geq 0 \iiint_K (r \sin(\phi)\sin(\theta) + r^2 \sin^2(\phi)\cos^2(\theta)) r^2 \sin(\phi) \mathrm{d}r\mathrm{d}\phi\mathrm{d}\theta K  0 \leq r \leq 2  0 \leq \theta \leq 2\pi 0 \leq \phi \leq \pi,"['multivariable-calculus', 'spherical-coordinates']"
48,Does Hessian of rank one mean curvature in one direction and flatness in the other,Does Hessian of rank one mean curvature in one direction and flatness in the other,,"I'm following a course on convex optimization by Stephen P. Boyd. https://learning.edx.org/course/course-v1:StanfordOnline+SOE-YCVX0001+1T2020/ My question is about Lecture 3 ( https://youtu.be/JrWSn5wW_Z0 ). Boyd gives a function: quadratic-over-linear : $f(x,y) = x^2/y,\: y>0$ The Hessian matrix of the function is: $$\nabla^2f(x,y)=\begin{bmatrix}2/y&&-2x/y^2\\-2x/y^2&&2x^2/y^3\end{bmatrix}=\frac{2}{y^3}\begin{bmatrix}y\\-x\end{bmatrix}\begin{bmatrix}y\\-x\end{bmatrix}^T$$ So  the Hessian matrix has rank one. Boyd says at https://youtu.be/JrWSn5wW_Z0?t=1527 : The fact that it's rank one tells you that at any point on that surface it's sort of curving in one direction, but not the other. Alright, so that's what it says. In one direction, it's flat. In the other direction, it's got positive curvature. I don't see why it's true for any point. For example, $\nabla^2f(1,1)= \begin{bmatrix}2&&-2\\-2&&2\end{bmatrix}$ . So the function has curvature in both $x$ and $y$ direction. Maybe Boyd meant to say that this is true for any point $(x_0, y_0)$ where $\nabla f(x_0,y_0) = 0$ ? In that case the line $x_0=0$ would represent the set of such points, and the Hessian matrix would be $\nabla^2f(0,y_0)= \begin{bmatrix}2/y_0&&0\\0&&0\end{bmatrix}$ . This would mean that the function has curvature only in $y$ direction. So, what am I missing here? Is the claim really true for any point? Is it true only for points where the gradient is zero? Is it true for any function in general?","I'm following a course on convex optimization by Stephen P. Boyd. https://learning.edx.org/course/course-v1:StanfordOnline+SOE-YCVX0001+1T2020/ My question is about Lecture 3 ( https://youtu.be/JrWSn5wW_Z0 ). Boyd gives a function: quadratic-over-linear : The Hessian matrix of the function is: So  the Hessian matrix has rank one. Boyd says at https://youtu.be/JrWSn5wW_Z0?t=1527 : The fact that it's rank one tells you that at any point on that surface it's sort of curving in one direction, but not the other. Alright, so that's what it says. In one direction, it's flat. In the other direction, it's got positive curvature. I don't see why it's true for any point. For example, . So the function has curvature in both and direction. Maybe Boyd meant to say that this is true for any point where ? In that case the line would represent the set of such points, and the Hessian matrix would be . This would mean that the function has curvature only in direction. So, what am I missing here? Is the claim really true for any point? Is it true only for points where the gradient is zero? Is it true for any function in general?","f(x,y) = x^2/y,\: y>0 \nabla^2f(x,y)=\begin{bmatrix}2/y&&-2x/y^2\\-2x/y^2&&2x^2/y^3\end{bmatrix}=\frac{2}{y^3}\begin{bmatrix}y\\-x\end{bmatrix}\begin{bmatrix}y\\-x\end{bmatrix}^T \nabla^2f(1,1)= \begin{bmatrix}2&&-2\\-2&&2\end{bmatrix} x y (x_0, y_0) \nabla f(x_0,y_0) = 0 x_0=0 \nabla^2f(0,y_0)= \begin{bmatrix}2/y_0&&0\\0&&0\end{bmatrix} y","['multivariable-calculus', 'convex-analysis', 'hessian-matrix']"
49,Two definitions of continuous differentiability,Two definitions of continuous differentiability,,"Recently I have seen two different definitions of continuous differentiability for a real-valued function of several variables $f(\mathbf{x}) = f(x_1,\ldots,x_n)$ on a set $E \subseteq \mathrm{int} \operatorname{dom} f \subseteq \mathbb{R}^n$ : Real-valued function of several variables $f(\mathbf{x}) = f(x_1,\ldots,x_n)$ is said to be continuously differentiable on a set $E \subseteq \mathrm{int} \operatorname{dom} f \subseteq \mathbb{R}^n$ if for each $\mathbf{x}_0 \in E$ the following holds: all first-order partial derivatives of $f$ are defined in some neighborhood $U(\mathbf{x}_0)$ and are continuous at $\mathbf{x}_0$ . Real-valued function of several variables $f(\mathbf{x}) = f(x_1,\ldots,x_n)$ is said to be continuously differentiable on a set $E \subseteq \mathrm{int} \operatorname{dom} f \subseteq \mathbb{R}^n$ if it is differentiable at every point of $E$ and its derivative $f'(\mathbf{x})$ is continuous at every point of $E$ (as vector function). I guess that these two definitions are equivalent if the set $E$ is open (I think this basically follows from Theorem 9.21 from Rudin's Principles of Mathematical Analysis). But they  don't seem to be equivalent if the set $E$ is not open. So which of these two definitions is more correct and popular, especially in the context of mathematical optimization problems ? ( as far as I know, in optimization problems we often have a function $f(\mathbf{x})$ which is continuously differentiable on a set $E \subseteq \mathrm{int} \operatorname{dom} f \subseteq \mathbb{R}^n$ , where the set $E$ is called the feasible set of the problem $\min_{\mathbf{x} \in E} f(\mathbf{x})$ ) Edit. I think there exist two more definitions that seem reasonable. However, I have not seen them in textbooks (for the considered case of $E \subseteq \operatorname{int} \operatorname{dom} f \subseteq \mathbb{R}^n$ ). Real-valued function of several variables $f(\mathbf{x}) = f(x_1,\ldots,x_n)$ is said to be continuously differentiable on a set $E \subseteq \mathrm{int} \operatorname{dom} f \subseteq \mathbb{R}^n$ if there is an open set $U$ , such that $E \subseteq U \subseteq \operatorname{int} \mathrm{dom} f$ and $f \in C^1(U)$ Real-valued function of several variables $f(\mathbf{x}) = f(x_1,\ldots,x_n)$ is said to be continuously differentiable on a set $E \subseteq \mathrm{int} \operatorname{dom} f \subseteq \mathbb{R}^n$ if $f|_{\operatorname{int} E} \in C^1(\operatorname{int} E)$ and all first-order partial derivatives of $f|_{\operatorname{int} E}$ extend continuously up to $E$ .","Recently I have seen two different definitions of continuous differentiability for a real-valued function of several variables on a set : Real-valued function of several variables is said to be continuously differentiable on a set if for each the following holds: all first-order partial derivatives of are defined in some neighborhood and are continuous at . Real-valued function of several variables is said to be continuously differentiable on a set if it is differentiable at every point of and its derivative is continuous at every point of (as vector function). I guess that these two definitions are equivalent if the set is open (I think this basically follows from Theorem 9.21 from Rudin's Principles of Mathematical Analysis). But they  don't seem to be equivalent if the set is not open. So which of these two definitions is more correct and popular, especially in the context of mathematical optimization problems ? ( as far as I know, in optimization problems we often have a function which is continuously differentiable on a set , where the set is called the feasible set of the problem ) Edit. I think there exist two more definitions that seem reasonable. However, I have not seen them in textbooks (for the considered case of ). Real-valued function of several variables is said to be continuously differentiable on a set if there is an open set , such that and Real-valued function of several variables is said to be continuously differentiable on a set if and all first-order partial derivatives of extend continuously up to .","f(\mathbf{x}) = f(x_1,\ldots,x_n) E \subseteq \mathrm{int} \operatorname{dom} f \subseteq \mathbb{R}^n f(\mathbf{x}) = f(x_1,\ldots,x_n) E \subseteq \mathrm{int} \operatorname{dom} f \subseteq \mathbb{R}^n \mathbf{x}_0 \in E f U(\mathbf{x}_0) \mathbf{x}_0 f(\mathbf{x}) = f(x_1,\ldots,x_n) E \subseteq \mathrm{int} \operatorname{dom} f \subseteq \mathbb{R}^n E f'(\mathbf{x}) E E E f(\mathbf{x}) E \subseteq \mathrm{int} \operatorname{dom} f \subseteq \mathbb{R}^n E \min_{\mathbf{x} \in E} f(\mathbf{x}) E \subseteq \operatorname{int} \operatorname{dom} f \subseteq \mathbb{R}^n f(\mathbf{x}) = f(x_1,\ldots,x_n) E \subseteq \mathrm{int} \operatorname{dom} f \subseteq \mathbb{R}^n U E \subseteq U \subseteq \operatorname{int} \mathrm{dom} f f \in C^1(U) f(\mathbf{x}) = f(x_1,\ldots,x_n) E \subseteq \mathrm{int} \operatorname{dom} f \subseteq \mathbb{R}^n f|_{\operatorname{int} E} \in C^1(\operatorname{int} E) f|_{\operatorname{int} E} E","['real-analysis', 'multivariable-calculus', 'optimization', 'partial-derivative']"
50,Exercise on multivariable calculus: am I reasoning right?,Exercise on multivariable calculus: am I reasoning right?,,"Consider $$f(x, y) = \begin{cases} x^2+y^2 & x^2+y^2 \geq 1 \\\\ \frac{1}{x^2+y^2} & 0 < x^2+y^2 < 1\end{cases} $$ Now, the domain of $f$ is: $D: \mathbb{R}^2\backslash\{(0, 0)\}$ When it's asked to check for the continuity of the function in all the points of its domain, I guess that it's really asked to only check for what happens when $x^2+y^2 = 1$ . Question: could I turn this into a one dimensional problem where $x^2+y^2 = m$ and hence studying the continuity of $f(m)$ which turns to be trivial? At least using directional limits or sequences... I don't have any idea on how it would be to prove continuity in this case, as if I had to use the same method of $2$ variables function in the usual way (that is, with majorisation and distance). Is it doable?","Consider Now, the domain of is: When it's asked to check for the continuity of the function in all the points of its domain, I guess that it's really asked to only check for what happens when . Question: could I turn this into a one dimensional problem where and hence studying the continuity of which turns to be trivial? At least using directional limits or sequences... I don't have any idea on how it would be to prove continuity in this case, as if I had to use the same method of variables function in the usual way (that is, with majorisation and distance). Is it doable?","f(x, y) = \begin{cases} x^2+y^2 & x^2+y^2 \geq 1 \\\\ \frac{1}{x^2+y^2} & 0 < x^2+y^2 < 1\end{cases}
 f D: \mathbb{R}^2\backslash\{(0, 0)\} x^2+y^2 = 1 x^2+y^2 = m f(m) 2","['real-analysis', 'multivariable-calculus', 'continuity']"
51,Triple integral over the body $Q$ between two spheres,Triple integral over the body  between two spheres,Q,"This is an exercise in using spherical coordinates. Calculate the triple integral $$J = \iiint_{Q}\ \frac{1}{x^2+y^2+z^2} \,dx \,dy \,dz$$ where $Q$ is the body between the two spheres $$x^2+y^2+z^2 = 1 $$ and $$x^2+y^2+z^2 = 9 $$ The calculation is quite easy. I am getting answer $8\pi$ but the answer given is $4\pi$ . Which one is correct?",This is an exercise in using spherical coordinates. Calculate the triple integral where is the body between the two spheres and The calculation is quite easy. I am getting answer but the answer given is . Which one is correct?,"J = \iiint_{Q}\ \frac{1}{x^2+y^2+z^2} \,dx \,dy \,dz Q x^2+y^2+z^2 = 1  x^2+y^2+z^2 = 9  8\pi 4\pi","['real-analysis', 'calculus', 'multivariable-calculus', 'solution-verification', 'multiple-integral']"
52,The graph of a continuous function defined on a general open/closed set has measure zero (without Fubini),The graph of a continuous function defined on a general open/closed set has measure zero (without Fubini),,"I was trying to prove the truth (avoiding Fubini) or falsehood of Q: Let $f:A\to\mathbb{R}^m$ be a continuous function where the domain $A\subseteq\mathbb{R}^k$ is open or closed. Then, is it necessarily the case that the graph $\Gamma(f,A)=\{(x,f(x)):x\in A\}$ has Lebesgue measure $0$ (and, thus, is Lebesgue measurable)? But I failed. The thing is: I know that, on this site, the case where $A$ is a rectangle has been asked many times. But what about this more general case? I know how to prove it when $A$ is a rectangle (i.e. of the form $A=\prod_{j=1}^k [a_j,b_j]$ ) but I am unable to generalize this particular case to the more general case (if it is true). I know that every open/closed set in $\mathbb{R}^n$ is the countable union of compact spaces so it would suffice to prove it on compact spaces which seem helpful since continuity on compact sets implies uniform continuity. However, the rectangle case can't be extended to the compact case since there exist compact sets that aren't the union of countably many rectangles such as the Cantor set . Is there a possible counterexample? I don't think so, but there might be one (?). Also: sorry if this question has already been asked here. I've searched as best as I could and, sadly, found nothing... Note: I don’t think that it’s valid to prove the case where $A=\mathbb{R}^k$ and $f$ is a continuous function (which is pretty trivial) and consider every other case of this problem as a simple restriction that follows directly. That is because not every continuous function $f\in C(A,\mathbb{R}^m)$ is the restriction of a continuous function with $\mathbb{R}^k$ as a domain. For example, $f:\mathbb{R}\setminus\{0\}\to\mathbb{R}$ defined $f(x)=\frac{1}{x}$ is continuous but its domain can’t be extended so that it’s still continuous. Or, even worse, $f:\mathbb{R}\setminus\left(\frac{\pi}{2}+\pi\mathbb{Z}\right)\to\mathbb{R}$ defined as $f(x)=\tan(x)$ can’t be continuously extended either. Any help would be highly appreciated! :)","I was trying to prove the truth (avoiding Fubini) or falsehood of Q: Let be a continuous function where the domain is open or closed. Then, is it necessarily the case that the graph has Lebesgue measure (and, thus, is Lebesgue measurable)? But I failed. The thing is: I know that, on this site, the case where is a rectangle has been asked many times. But what about this more general case? I know how to prove it when is a rectangle (i.e. of the form ) but I am unable to generalize this particular case to the more general case (if it is true). I know that every open/closed set in is the countable union of compact spaces so it would suffice to prove it on compact spaces which seem helpful since continuity on compact sets implies uniform continuity. However, the rectangle case can't be extended to the compact case since there exist compact sets that aren't the union of countably many rectangles such as the Cantor set . Is there a possible counterexample? I don't think so, but there might be one (?). Also: sorry if this question has already been asked here. I've searched as best as I could and, sadly, found nothing... Note: I don’t think that it’s valid to prove the case where and is a continuous function (which is pretty trivial) and consider every other case of this problem as a simple restriction that follows directly. That is because not every continuous function is the restriction of a continuous function with as a domain. For example, defined is continuous but its domain can’t be extended so that it’s still continuous. Or, even worse, defined as can’t be continuously extended either. Any help would be highly appreciated! :)","f:A\to\mathbb{R}^m A\subseteq\mathbb{R}^k \Gamma(f,A)=\{(x,f(x)):x\in A\} 0 A A A=\prod_{j=1}^k [a_j,b_j] \mathbb{R}^n A=\mathbb{R}^k f f\in C(A,\mathbb{R}^m) \mathbb{R}^k f:\mathbb{R}\setminus\{0\}\to\mathbb{R} f(x)=\frac{1}{x} f:\mathbb{R}\setminus\left(\frac{\pi}{2}+\pi\mathbb{Z}\right)\to\mathbb{R} f(x)=\tan(x)","['real-analysis', 'measure-theory', 'multivariable-calculus', 'continuity', 'cantor-set']"
53,Minimizing a function in $\mathbb R^3.$,Minimizing a function in,\mathbb R^3.,"I am currently solving the following task: Exercise. Minimize in $\mathbb R^3$ the function $f(x_1,x_2,x_3) = x_1^2 + x_2^2 + x_3^2 - x_1x_2 + x_2x_3 - x_1x_3.$ My attempt. I was able to show that $(0,0,0)$ is a local  strict minimizer (consequently, $f(0,0,0) = 0$ is a local strict minimum) using the usual arguments, i.e., I found the critical points of $f$ (which is only one, $(0,0,0)$ ); then, I found the hessian of $f$ in the point $(0,0,0)$ and proved it was positive defined. Therefore, from the theorem below I am able to guarantee that $(0,0,0)$ is a local strict minimizer. Theorem. Assume that $f$ is twice differentiable in $\bar x \in X.$ Then, if $\nabla f(\bar x) = 0$ and $H(\bar x) = \nabla ^2 f(\bar x)$ is positive defined, then $\bar x$ is a local strict minimizer. My question. After this, one question comes up to my mind: is $(0,0,0)$ a global minimizer? Looking to answer this further question, I gave some values to some of the coordinates ( $x_1, x_2 $ or $x_3$ ) individually and plotted the resultant graphs (which are $3D$ ). Every one I plotted (and I plotted a nice quantity...) leads me to the same conclusion: $(0,0,0)$ is, in fact, a global minimizer. After this, I tried to prove analitically that $f(x_1,x_2,x_3) \geqslant 0,$ for every $(x_1,x_2,x_3) \in \mathbb R^3$ (to show that, in fact $(0,0,0)$ is a global minimizer). But I am having a real hard time doing this. Any help towards proving this is apreciatted in advance.","I am currently solving the following task: Exercise. Minimize in the function My attempt. I was able to show that is a local  strict minimizer (consequently, is a local strict minimum) using the usual arguments, i.e., I found the critical points of (which is only one, ); then, I found the hessian of in the point and proved it was positive defined. Therefore, from the theorem below I am able to guarantee that is a local strict minimizer. Theorem. Assume that is twice differentiable in Then, if and is positive defined, then is a local strict minimizer. My question. After this, one question comes up to my mind: is a global minimizer? Looking to answer this further question, I gave some values to some of the coordinates ( or ) individually and plotted the resultant graphs (which are ). Every one I plotted (and I plotted a nice quantity...) leads me to the same conclusion: is, in fact, a global minimizer. After this, I tried to prove analitically that for every (to show that, in fact is a global minimizer). But I am having a real hard time doing this. Any help towards proving this is apreciatted in advance.","\mathbb R^3 f(x_1,x_2,x_3) = x_1^2 + x_2^2 + x_3^2 - x_1x_2 + x_2x_3 - x_1x_3. (0,0,0) f(0,0,0) = 0 f (0,0,0) f (0,0,0) (0,0,0) f \bar x \in X. \nabla f(\bar x) = 0 H(\bar x) = \nabla ^2 f(\bar x) \bar x (0,0,0) x_1, x_2  x_3 3D (0,0,0) f(x_1,x_2,x_3) \geqslant 0, (x_1,x_2,x_3) \in \mathbb R^3 (0,0,0)","['real-analysis', 'multivariable-calculus', 'optimization', 'solution-verification', 'hessian-matrix']"
54,Differential of $h(x) = \Vert x \Vert ^4$,Differential of,h(x) = \Vert x \Vert ^4,"Find the differential of $h: \mathbb R^k \to \mathbb R$ defined as $h(x) = \Vert x \Vert ^4$ (with $\Vert \Vert$ the Euclidean norm). I'm not sure if I'm correct. We can write $h(x) = \langle x,x \rangle^2$ with the standard inner product. To make things easier define $f(x) = \langle x,x \rangle$ and $g(x) = x^2$ , with $f : \mathbb R^k \to \mathbb R$ and $g : \mathbb R \to \mathbb R$ . $g'(x) = 2x$ and by the product rule $Df_x = 2\langle I,x \rangle$ . $h = g \circ f$ so by the chain rule: $Dh_x = Dg_{f(x)} \circ Df_x = (2 \langle x,x \rangle) \ \circ \  (2\langle I,x \rangle)$ . Now, $2 \langle x,x \rangle$ is a scalar so I'm not sure if the composition with $Df_x$ should result in the product $4 \langle x,x \rangle \langle I,x \rangle = 4\Vert x \Vert ^2\langle I,x \rangle$ , or it should result in $2 \langle 2\langle I,x \rangle,2\langle I,x \rangle \rangle = 8 \langle \langle I,x \rangle,\langle I,x \rangle \rangle = 8 \langle I,x \rangle^2$ since $\langle I,x \rangle$ should result in a scalr when plugging in some vector $v$ (meaning $\langle I,x \rangle(v) = \langle v,x \rangle$ ). There might be a simpler way to do this but I would appreciate any help.","Find the differential of defined as (with the Euclidean norm). I'm not sure if I'm correct. We can write with the standard inner product. To make things easier define and , with and . and by the product rule . so by the chain rule: . Now, is a scalar so I'm not sure if the composition with should result in the product , or it should result in since should result in a scalr when plugging in some vector (meaning ). There might be a simpler way to do this but I would appreciate any help.","h: \mathbb R^k \to \mathbb R h(x) = \Vert x \Vert ^4 \Vert \Vert h(x) = \langle x,x \rangle^2 f(x) = \langle x,x \rangle g(x) = x^2 f : \mathbb R^k \to \mathbb R g : \mathbb R \to \mathbb R g'(x) = 2x Df_x = 2\langle I,x \rangle h = g \circ f Dh_x = Dg_{f(x)} \circ Df_x = (2 \langle x,x \rangle) \ \circ \  (2\langle I,x \rangle) 2 \langle x,x \rangle Df_x 4 \langle x,x \rangle \langle I,x \rangle = 4\Vert x \Vert ^2\langle I,x \rangle 2 \langle 2\langle I,x \rangle,2\langle I,x \rangle \rangle = 8 \langle \langle I,x \rangle,\langle I,x \rangle \rangle = 8 \langle I,x \rangle^2 \langle I,x \rangle v \langle I,x \rangle(v) = \langle v,x \rangle","['multivariable-calculus', 'normed-spaces']"
55,trouble with proving/disproving $\lvert z-x \rvert$=$\sqrt{x+y^2}$ is manifold,trouble with proving/disproving = is manifold,\lvert z-x \rvert \sqrt{x+y^2},"Let $M$ be set of points $(x,y,z) \in \mathbb{R}^3$ such that $\lvert z-x \rvert = \sqrt{x+y^2}$ . Is $M$ two dimensional manifold? I have two different solutions to this problem and in one of them the result is that $\lvert z-x \rvert = \sqrt{x+y^2}$ is manifold but in the second I got that it is not manifold. Clearly one of solution is wrong, I do not know which and I do not know why. First solution. Raise both sides of equation to second power and consider function $$ F(x,y,z) = z^2 - 2xz + x^2 - x - y^2 $$ Gradient of $F$ is: $$ DF(x,y,z) = (-2z+2x-1,\, -2y,\, 2z-2x),  $$ which is nonzero for all $(x.y,z)$ so $DF(x,y,z)$ is epimorphism in every $(x,y,z)$ and therefore $M$ is two dimensional manifold. (This is by theorem I learned in my analysis class.) Second solution. Fix $x=0$ , we get $\lvert z \rvert = \lvert y \rvert$ so none of $y$ and $z$ are functions of two other variables. Fix $z=0$ , we get $x^2-x = y^2$ and when I graph it on Desmos, I see that for many $x$ 's there are two $y$ 's so $x$ is not a function of $y$ and $z$ . So there are points in $M$ where $M$ is not a graph of a function so $M$ is not manifold.","Let be set of points such that . Is two dimensional manifold? I have two different solutions to this problem and in one of them the result is that is manifold but in the second I got that it is not manifold. Clearly one of solution is wrong, I do not know which and I do not know why. First solution. Raise both sides of equation to second power and consider function Gradient of is: which is nonzero for all so is epimorphism in every and therefore is two dimensional manifold. (This is by theorem I learned in my analysis class.) Second solution. Fix , we get so none of and are functions of two other variables. Fix , we get and when I graph it on Desmos, I see that for many 's there are two 's so is not a function of and . So there are points in where is not a graph of a function so is not manifold.","M (x,y,z) \in \mathbb{R}^3 \lvert z-x \rvert = \sqrt{x+y^2} M \lvert z-x \rvert = \sqrt{x+y^2} 
F(x,y,z) = z^2 - 2xz + x^2 - x - y^2
 F 
DF(x,y,z) = (-2z+2x-1,\, -2y,\, 2z-2x), 
 (x.y,z) DF(x,y,z) (x,y,z) M x=0 \lvert z \rvert = \lvert y \rvert y z z=0 x^2-x = y^2 x y x y z M M M","['real-analysis', 'calculus', 'multivariable-calculus', 'differential-geometry', 'manifolds']"
56,"If a function $f$ is analytic and $f(x,y)=f(x,-y)=f(x+y,y)$ then the function depends on $y$ alone",If a function  is analytic and  then the function depends on  alone,"f f(x,y)=f(x,-y)=f(x+y,y) y","Let $f:\mathbb{R^2}\rightarrow\mathbb{R}$ a function such that $f(x,y)=f(x,-y)=f(x+y,y)$ . I need to show that if $f$ is an analytic function, then $f$ depends only on $y$ What I did was this: Given that $f$ is analytic, the Taylor series at the point $(x+h, y+k)$ is equal to $f(x+h,y+k)$ , so: $f(x+h,y+k)=f(x,y)+hf_x(x,y)+kf_y(x,y)+\frac12(h^2f_{xx}+2hkf_{xy}+k^2f_{yy})+...$ for $k=0$ and $h=y$ we'll get: $f(x+y,y)=f(x,y)+yf_x(x,y)+\frac12(y^2f_{xx}(x+y,y)+...$ I'll define $g:\mathbb{R^2}\rightarrow\mathbb{R}$ by $g(x,y)=f(x+y,y)-f(x,y)$ . Based on the initial data $g(x,y)\equiv0$ , and so: $0\equiv f(x+y,y)-f(x,y)=yf_x(x,y)+\frac12y^2f_{xx}(x+y,y)+...$ It follows then that $f_x \equiv f_{xx} \equiv f_{xxx} \equiv ... \equiv0$ What I want now is to show that every mixed derivative, i.e $f_{xy},f_{xyy}$ , etc is also equivalent to $0$ , but I don't how to do this, or really why this would be true. Edit: Because $f_{xy}=f_{yx}$ , all the mixed derivatives would be taking the derivative of a constant function (where the constant is just $0$ ), And so they'll all be equivalent to the $0$ function. And then I'll get that $f(x+h,y+k)=f(x,y)+kf_y+\frac12k^2f_{yy}+...$ is an expression that doesn't depend on $h$ and then $f$ is supposedly a function that only depends on $y$ . But I don't really understand why this means that $f$ is only dependent on $y$ . I can still have $x$ 's in the partial derivatives of $f$ according to $y$ , no? Presumably because $f(x,y)=f(x,-y)$ it'll then mean that $f$ only depends on $|y|$","Let a function such that . I need to show that if is an analytic function, then depends only on What I did was this: Given that is analytic, the Taylor series at the point is equal to , so: for and we'll get: I'll define by . Based on the initial data , and so: It follows then that What I want now is to show that every mixed derivative, i.e , etc is also equivalent to , but I don't how to do this, or really why this would be true. Edit: Because , all the mixed derivatives would be taking the derivative of a constant function (where the constant is just ), And so they'll all be equivalent to the function. And then I'll get that is an expression that doesn't depend on and then is supposedly a function that only depends on . But I don't really understand why this means that is only dependent on . I can still have 's in the partial derivatives of according to , no? Presumably because it'll then mean that only depends on","f:\mathbb{R^2}\rightarrow\mathbb{R} f(x,y)=f(x,-y)=f(x+y,y) f f y f (x+h, y+k) f(x+h,y+k) f(x+h,y+k)=f(x,y)+hf_x(x,y)+kf_y(x,y)+\frac12(h^2f_{xx}+2hkf_{xy}+k^2f_{yy})+... k=0 h=y f(x+y,y)=f(x,y)+yf_x(x,y)+\frac12(y^2f_{xx}(x+y,y)+... g:\mathbb{R^2}\rightarrow\mathbb{R} g(x,y)=f(x+y,y)-f(x,y) g(x,y)\equiv0 0\equiv f(x+y,y)-f(x,y)=yf_x(x,y)+\frac12y^2f_{xx}(x+y,y)+... f_x \equiv f_{xx} \equiv f_{xxx} \equiv ... \equiv0 f_{xy},f_{xyy} 0 f_{xy}=f_{yx} 0 0 f(x+h,y+k)=f(x,y)+kf_y+\frac12k^2f_{yy}+... h f y f y x f y f(x,y)=f(x,-y) f |y|","['multivariable-calculus', 'taylor-expansion', 'analytic-functions']"
57,Parametric Equations for Line(s) Tangent to a Sphere That Pass Through Point Outside of Sphere,Parametric Equations for Line(s) Tangent to a Sphere That Pass Through Point Outside of Sphere,,"Question asks for a the parametric equations for the set of all lines that pass through the point $(4,0,0)$ and are tangent to the curve: $$x^2+y^2+z^2=2$$ My issue here is that I am not really sure how to approach the problem. The previous question asked for a similar thing but it wasn't as complicated as it was just the equation of a circle and not a sphere. Nevertheless, it was still the same point and it was still outside of the equation of the circle. I am assuming there will be a general solution for this question since the question implies that there will be more than one line that passes through the point and is tangent to the sphere. Am I supposed to utilize gradients to solve this? Or maybe the 3D tangent line equation? I am genuinely not sure how to approach this problem. Any help would be appreciated.","Question asks for a the parametric equations for the set of all lines that pass through the point and are tangent to the curve: My issue here is that I am not really sure how to approach the problem. The previous question asked for a similar thing but it wasn't as complicated as it was just the equation of a circle and not a sphere. Nevertheless, it was still the same point and it was still outside of the equation of the circle. I am assuming there will be a general solution for this question since the question implies that there will be more than one line that passes through the point and is tangent to the sphere. Am I supposed to utilize gradients to solve this? Or maybe the 3D tangent line equation? I am genuinely not sure how to approach this problem. Any help would be appreciated.","(4,0,0) x^2+y^2+z^2=2","['linear-algebra', 'multivariable-calculus']"
58,Transformation that maps a rectangular region $D$ in the $uv$-plane onto the region $ R$,Transformation that maps a rectangular region  in the -plane onto the region,D uv  R,"Let $R= (x, y)\in R^2: x≥ 0, y ≥0, 1\le x+y\le 2$ Find a transformation that maps a rectangular region $D$ in the $uv$ -plane onto the region $R$ , and use it to evaluate $\iint_R \frac{y}{x + y} dA $ . This was asked in one of my exams (the exam is over) but I was unable to find a rectangular region $$D in the $uv$-plane which maps to region $R$. I have tried to use the substitution $u=x+y$ and $v=y$ and got a trapezium. Other transformations which I have tried does not work. Does there exist such a transformation? Any help will be appreciated.","Let Find a transformation that maps a rectangular region in the -plane onto the region , and use it to evaluate . This was asked in one of my exams (the exam is over) but I was unable to find a rectangular region $$D in the $uv$-plane which maps to region $R$. I have tried to use the substitution and and got a trapezium. Other transformations which I have tried does not work. Does there exist such a transformation? Any help will be appreciated.","R= (x, y)\in R^2: x≥ 0, y ≥0, 1\le x+y\le 2 D uv R \iint_R \frac{y}{x + y} dA  u=x+y v=y","['calculus', 'multivariable-calculus', 'transformation', 'jacobian']"
59,Is one-sided differentiable inverse sufficient to conclude equal dimensions?,Is one-sided differentiable inverse sufficient to conclude equal dimensions?,,"Let $\Omega$ be an open subset of $\mathbb R^n$ and $\Upsilon$ be of $\mathbb R^m$ . Let $f\colon \Omega\to \Upsilon$ . Then this answer shows that if there exists a point $c\in\Omega$ where $f$ is differentiable, and if $f$ is invertible with the inverse being differentiable at $f(c)$ , then $m = n$ . Note that differentiability at only one point is sufficient. The answer also comments that if there exists a homeomorphism between $\Omega$ and $\Upsilon$ (with these being nonempty), then too $m = n$ . But this left me wondering about the following questions: If $f$ is not invertible, but has a right-inverse $g$ (that is differentiable at $f(c)$ ), is $m = n$ still? Or can we say nothing more than $m\le n$ ? Same question about homeomorphism. The answer said that the homeomorphism statement can be proved by Brouwer's fixed point theorem. Can you give me a reference for that?","Let be an open subset of and be of . Let . Then this answer shows that if there exists a point where is differentiable, and if is invertible with the inverse being differentiable at , then . Note that differentiability at only one point is sufficient. The answer also comments that if there exists a homeomorphism between and (with these being nonempty), then too . But this left me wondering about the following questions: If is not invertible, but has a right-inverse (that is differentiable at ), is still? Or can we say nothing more than ? Same question about homeomorphism. The answer said that the homeomorphism statement can be proved by Brouwer's fixed point theorem. Can you give me a reference for that?",\Omega \mathbb R^n \Upsilon \mathbb R^m f\colon \Omega\to \Upsilon c\in\Omega f f f(c) m = n \Omega \Upsilon m = n f g f(c) m = n m\le n,"['general-topology', 'multivariable-calculus', 'vector-spaces', 'jacobian', 'diffeomorphism']"
60,What is the largest/least value of the function $f$ in $\mathbb{R}^2$?,What is the largest/least value of the function  in ?,f \mathbb{R}^2,"What is the largest/least value of the function $f=x^2ye^{-x^2-2y^2}$ in $\mathbb{R}^2$ ? We have that $f'_x=y\left(2e^{-x^2-2y^2}x-2e^{-x^2-2y^2}x^3\right)$ and $f'_y=x^2\left(e^{-x^2-2y^2}-4e^{-x^2-2y^2}y^2\right)$ . If $x=0$ then $f'_x=f'_y=0$ so $(0,y)$ , where $y\in \mathbb{R}^2$ are stationary points. Further $1-4y^2=0\iff y=+-1/\sqrt{2}$ and $2x-2x^3=0\iff2x(1-x^2)\iff x=0, x=+-1$ so $+-(1,1/\sqrt{2})$ are also stationary points. The values for the stationary points are $f(0,y)=0$ , $f(+-1,1/\sqrt{2})=\frac{e^{-2}}{\sqrt{2}}$ and $f(+-1,-1/\sqrt{2})=-\frac{e^{-2}}{\sqrt{2}}$ . Now how do I prove that $e^{-2}/\sqrt{2}$ is the largest value and $-e^{-2}/\sqrt{2}$ is the smallest? Would be nice with a blueprint that I can use for any function but I guess it doesn't exist. Edit: Not looking for solutions that splits up the $f$ to $x^2e^{-x^2}ye^{-2y^2}$ . I wrote an answer and hopefully someone can check that it is right.","What is the largest/least value of the function in ? We have that and . If then so , where are stationary points. Further and so are also stationary points. The values for the stationary points are , and . Now how do I prove that is the largest value and is the smallest? Would be nice with a blueprint that I can use for any function but I guess it doesn't exist. Edit: Not looking for solutions that splits up the to . I wrote an answer and hopefully someone can check that it is right.","f=x^2ye^{-x^2-2y^2} \mathbb{R}^2 f'_x=y\left(2e^{-x^2-2y^2}x-2e^{-x^2-2y^2}x^3\right) f'_y=x^2\left(e^{-x^2-2y^2}-4e^{-x^2-2y^2}y^2\right) x=0 f'_x=f'_y=0 (0,y) y\in \mathbb{R}^2 1-4y^2=0\iff y=+-1/\sqrt{2} 2x-2x^3=0\iff2x(1-x^2)\iff x=0, x=+-1 +-(1,1/\sqrt{2}) f(0,y)=0 f(+-1,1/\sqrt{2})=\frac{e^{-2}}{\sqrt{2}} f(+-1,-1/\sqrt{2})=-\frac{e^{-2}}{\sqrt{2}} e^{-2}/\sqrt{2} -e^{-2}/\sqrt{2} f x^2e^{-x^2}ye^{-2y^2}","['calculus', 'limits', 'multivariable-calculus', 'optimization']"
61,"Find the inverse of $(\cos t,\sin t,t)$",Find the inverse of,"(\cos t,\sin t,t)","Let $c:\mathbb{R} \rightarrow \mathbb{R}^3$ with: $$ c(t)=(\cos t,\sin t,t) $$ Check if $c:\mathbb{R} \rightarrow c(\mathbb{R})$ is invertible and if so, is the inverse continuous? My attempt: Obviously the fuction is a bijection, so let's find the inverse. So to find $c^{-1}(x,y,z)=t$ ,  comes down to solving the system: $$ \begin{cases} \cos t=x \\ \sin t=y \\ z=t  \end{cases}$$ So, $$x^2+y^2+z^2=1+t^2 \Rightarrow t= \pm \sqrt{x^2+y^2+z^2-1}.$$ I think this is not right, Any ideas?","Let with: Check if is invertible and if so, is the inverse continuous? My attempt: Obviously the fuction is a bijection, so let's find the inverse. So to find ,  comes down to solving the system: So, I think this is not right, Any ideas?","c:\mathbb{R} \rightarrow \mathbb{R}^3  c(t)=(\cos t,\sin t,t)  c:\mathbb{R} \rightarrow c(\mathbb{R}) c^{-1}(x,y,z)=t  \begin{cases} \cos t=x \\ \sin t=y \\ z=t  \end{cases} x^2+y^2+z^2=1+t^2 \Rightarrow t= \pm \sqrt{x^2+y^2+z^2-1}.","['calculus', 'multivariable-calculus', 'systems-of-equations', 'inverse']"
62,"Let $f:\mathbb{R}^n\rightarrow \mathbb{R}$ be continuous such that $\lim\limits_{\|x\| \to\infty} f(x)=\infty$, then there exist a global minimum.","Let  be continuous such that , then there exist a global minimum.",f:\mathbb{R}^n\rightarrow \mathbb{R} \lim\limits_{\|x\| \to\infty} f(x)=\infty,"Statement*: Let $f:\mathbb{R}^n\rightarrow \mathbb{R}$ be continuous such that $\lim\limits_{\|x\| \to\infty} f(x)=\infty$ , then there exist a global minimum. Note: $\|.\|$ is the euclidean norm. Proof: Because $f(x)$ is diverging to infinity if $\|x\| \to\infty$ there exists $K > 0$ for every $M > 0$ , such that if $x\in \mathbb{R}^n$ with $\|x\|>K$ then $f(x) > M$ and if $x\in \mathbb{R}^n$ with $\|x\|\leq K$ then $f(x) \leq M$ . Define $S^{n-1}=\{x\in \mathbb{R}^n |x_1^2+...+x_n^2\leq K^2\}$ then $S^{n-1}$ is compact because it is bounded and closed. So the restriction of $f$ to the compact set $S^{n-1}$ has a global minimum and maximum because $f|_{S^{n-1}}$ is continuuous. Let $a\in S^{n-1}$ be the point in which $f|_{S^{n-1}}$ has its global minimum $f(a)$ then $\|a\|\leq K$ and $f(a)\leq M$ . For any $x\in \mathbb{R}^n\setminus S^{n-1}$ , $\|x\|> K$ , and thereby $f(x) > M\geq f(a)$ . So $f(a)$ is a global minimum. q.e.d. *The statement is almost the same as in the following question: Limit goes to infinity, show that the f has a finite minimum. I want to adapt the proof given in the answer for the domain $\mathbb{R}^n$ instead of $\mathbb{R}$ and show that there is a global minimum. Did I made any mistakes?","Statement*: Let be continuous such that , then there exist a global minimum. Note: is the euclidean norm. Proof: Because is diverging to infinity if there exists for every , such that if with then and if with then . Define then is compact because it is bounded and closed. So the restriction of to the compact set has a global minimum and maximum because is continuuous. Let be the point in which has its global minimum then and . For any , , and thereby . So is a global minimum. q.e.d. *The statement is almost the same as in the following question: Limit goes to infinity, show that the f has a finite minimum. I want to adapt the proof given in the answer for the domain instead of and show that there is a global minimum. Did I made any mistakes?","f:\mathbb{R}^n\rightarrow \mathbb{R} \lim\limits_{\|x\|
\to\infty} f(x)=\infty \|.\| f(x) \|x\|
\to\infty K > 0 M > 0 x\in \mathbb{R}^n \|x\|>K f(x) > M x\in \mathbb{R}^n \|x\|\leq K f(x) \leq M S^{n-1}=\{x\in \mathbb{R}^n |x_1^2+...+x_n^2\leq K^2\} S^{n-1} f S^{n-1} f|_{S^{n-1}} a\in S^{n-1} f|_{S^{n-1}} f(a) \|a\|\leq K f(a)\leq M x\in \mathbb{R}^n\setminus S^{n-1} \|x\|> K f(x) > M\geq f(a) f(a) \mathbb{R}^n \mathbb{R}","['real-analysis', 'multivariable-calculus', 'solution-verification']"
63,Show positivity of a function of two variables in the unit square.,Show positivity of a function of two variables in the unit square.,,"Let $$ f(x,y) =  x^3 (1 + y + y^2) + y^2 \Big[x^2  (5 + 2 y) + x  (-6 - 4 y + y^2) +  (1 + 3 y + y^2)\Big]  $$ Show that $f(x,y) \ge 0 $ for $0\le x \le 1$ and $0\le y \le 1$ . Numerical evaluations seem to support the claim.  This is a cubic function in $x$ with one negative coefficient in the linear term, all other coefficients are positive. Since $f(x=0) > 0$ , the last two terms guarantee positivity for $0 < x < 1/6$ . The last three terms show a quadratic function which has its minimum always for $x^* \in [0 \quad 1]$ ,  namely at $x^* = \frac{6 + 4 y - y^2}{10 + 4 y }$ ; however, for small $y$ , the value of the sum of the three last terms is negative at $x^*$ . How to continue?","Let Show that for and . Numerical evaluations seem to support the claim.  This is a cubic function in with one negative coefficient in the linear term, all other coefficients are positive. Since , the last two terms guarantee positivity for . The last three terms show a quadratic function which has its minimum always for ,  namely at ; however, for small , the value of the sum of the three last terms is negative at . How to continue?","
f(x,y) =  x^3 (1 + y + y^2) + y^2 \Big[x^2  (5 + 2 y) + x  (-6 - 4 y + y^2) +  (1 + 3 y + y^2)\Big] 
 f(x,y) \ge 0  0\le x \le 1 0\le y \le 1 x f(x=0) > 0 0 < x < 1/6 x^* \in [0 \quad 1] x^* = \frac{6 + 4 y - y^2}{10 + 4 y } y x^*","['multivariable-calculus', 'inequality', 'polynomials']"
64,Uniqueness of high-dimensional derivative [Zorich's book],Uniqueness of high-dimensional derivative [Zorich's book],,"I was reading the definition of the high-dimensional derivative from Zorich's book and I'd like to ask a question about the uniqueness of high-dimensional derivative. Definition 1. A function $f:E\to \mathbb{R}^n$ defined on a set $E\subset \mathbb{R}^m$ is differentiable at the point $x\in E$ , which is a limit of $E$ , if $$f(x+h)-f(x)=L(x)h+\alpha(x;h), \quad \quad \quad(1)$$ where $L(x):\mathbb{R}^{m}\to \mathbb{R}^n$ is a function that is linear in $h$ and $\alpha(x;h)=o(h)$ as $h\to 0, x+h\in E$ . The linear function $L(x):\mathbb{R}^m\to \mathbb{R}^n$ in $(1)$ is called the differential of the function $f:E\to  \mathbb{R}^n$ at the point $x\in E$ . Claim. If $E\subset \mathbb{R}^m$ and $f:E\to\mathbb{R}^n$ is differentiable at the point $x\in E$ , which is a limit point of $E$ , then the differential of $f$ at the point $x\in E$ is unique. Proof. Let's assume there are $A,B:\mathbb{R}^m\to \mathbb{R}^n$ linear mappings such that $$f(x+h)-f(x)=Ah+\alpha(x;h) \quad \text{and} \quad f(x+h)-f(x)=Bh+\beta(x;h),$$ where $\alpha(x;h)=o(h)$ and $\beta(x;h)=o(h)$ as $h\to 0, x+h\in E$ . Remark. Here the base is $h\to 0, x+h\in E$ . The elements of this base are $B_{\delta}:=\{h\in \mathbb{R}^m: 0\leq \lVert h\rVert<\delta, x+h\in E\}$ for $\delta>0$ . Indeed, it satisfies to the definition of base. Let $C:=A-B$ , then $C:\mathbb{R}^m\to \mathbb{R}^n$ is a linear mapping also. Then it is not difficult to show that $$\lim \limits_{\substack{h\to 0 \\ x+h\in E}} \frac{\lVert Ch\rVert}{\lVert h\rVert}=0.$$ Let $y$ be a nonzero vector in $\mathbb{R}^m$ . If we can show that $\lim \limits_{\substack{\mathbb{R}\ni t\to 0 \\ x+ty\in E}} \frac{\lVert C(ty)\rVert}{\lVert ty\rVert}=0,$ then we are done. Indeed, in that case it follows that $\lVert Cy\rVert=0$ for all $y\neq 0$ , which implies that $Cy=0$ for all $y$ and hence $Ay=By$ . Therefore, linear mappings $A$ and $B$ are equal. Question. How to prove that if $\lim \limits_{\substack{h\to 0 \\ x+h\in E}} \frac{\lVert Ch\rVert}{\lVert h\rVert}=0$ , then $\lim \limits_{\substack{\mathbb{R}\ni t\to 0 \\ x+ty\in E}} \frac{\lVert C(ty)\rVert}{\lVert ty\rVert}=0$ ?? It is certainly true if $x$ is an interior point of $E$ but it does not look correct to me because we claim that $x$ is the limit point of $E$ . Let me try to explain why do I think so. Let $\varepsilon>0$ be given. Take $\delta_0=\frac{\delta}{\lVert y\rVert}>0$ , where $\delta>0$ comes from the first limit. Then for any $t\in \mathbb{R}$ such that $0<|t|<\delta_0,\ x+ty\in E$ we'll get that $\frac{\lVert C(ty)\rVert}{\lVert ty\rVert}<\varepsilon$ . It seems that we are done but how do we know that such $t$ exists? I mean how do we know that $\{t\in \mathbb{R}: 0<|t|<\delta_0, x+ty\in E\}\neq \varnothing$ ? Thank you so much for your help!","I was reading the definition of the high-dimensional derivative from Zorich's book and I'd like to ask a question about the uniqueness of high-dimensional derivative. Definition 1. A function defined on a set is differentiable at the point , which is a limit of , if where is a function that is linear in and as . The linear function in is called the differential of the function at the point . Claim. If and is differentiable at the point , which is a limit point of , then the differential of at the point is unique. Proof. Let's assume there are linear mappings such that where and as . Remark. Here the base is . The elements of this base are for . Indeed, it satisfies to the definition of base. Let , then is a linear mapping also. Then it is not difficult to show that Let be a nonzero vector in . If we can show that then we are done. Indeed, in that case it follows that for all , which implies that for all and hence . Therefore, linear mappings and are equal. Question. How to prove that if , then ?? It is certainly true if is an interior point of but it does not look correct to me because we claim that is the limit point of . Let me try to explain why do I think so. Let be given. Take , where comes from the first limit. Then for any such that we'll get that . It seems that we are done but how do we know that such exists? I mean how do we know that ? Thank you so much for your help!","f:E\to \mathbb{R}^n E\subset \mathbb{R}^m x\in E E f(x+h)-f(x)=L(x)h+\alpha(x;h), \quad \quad \quad(1) L(x):\mathbb{R}^{m}\to \mathbb{R}^n h \alpha(x;h)=o(h) h\to 0, x+h\in E L(x):\mathbb{R}^m\to \mathbb{R}^n (1) f:E\to
 \mathbb{R}^n x\in E E\subset \mathbb{R}^m f:E\to\mathbb{R}^n x\in E E f x\in E A,B:\mathbb{R}^m\to \mathbb{R}^n f(x+h)-f(x)=Ah+\alpha(x;h) \quad \text{and} \quad f(x+h)-f(x)=Bh+\beta(x;h), \alpha(x;h)=o(h) \beta(x;h)=o(h) h\to 0, x+h\in E h\to 0, x+h\in E B_{\delta}:=\{h\in \mathbb{R}^m: 0\leq \lVert h\rVert<\delta, x+h\in E\} \delta>0 C:=A-B C:\mathbb{R}^m\to \mathbb{R}^n \lim \limits_{\substack{h\to 0 \\ x+h\in E}} \frac{\lVert Ch\rVert}{\lVert h\rVert}=0. y \mathbb{R}^m \lim \limits_{\substack{\mathbb{R}\ni t\to 0 \\ x+ty\in E}} \frac{\lVert C(ty)\rVert}{\lVert ty\rVert}=0, \lVert Cy\rVert=0 y\neq 0 Cy=0 y Ay=By A B \lim \limits_{\substack{h\to 0 \\ x+h\in E}} \frac{\lVert Ch\rVert}{\lVert h\rVert}=0 \lim \limits_{\substack{\mathbb{R}\ni t\to 0 \\ x+ty\in E}} \frac{\lVert C(ty)\rVert}{\lVert ty\rVert}=0 x E x E \varepsilon>0 \delta_0=\frac{\delta}{\lVert y\rVert}>0 \delta>0 t\in \mathbb{R} 0<|t|<\delta_0,\ x+ty\in E \frac{\lVert C(ty)\rVert}{\lVert ty\rVert}<\varepsilon t \{t\in \mathbb{R}: 0<|t|<\delta_0, x+ty\in E\}\neq \varnothing","['real-analysis', 'linear-algebra', 'multivariable-calculus', 'vector-spaces']"
65,"find local maximum/minimum of function $f(x,y)=x+y+4\sin x \sin y$.",find local maximum/minimum of function .,"f(x,y)=x+y+4\sin x \sin y","I have this task: find local maximum/minimum of function $f(x,y)=x+y+4\sin x \sin y$ . Now I have found partial derivatives: $\frac{\partial f}{\partial x} = 1+4\cos x \sin y$ $\frac{\partial f}{\partial y} = 1+4\cos y \sin x$ $\cos x \sin y=-\frac{1}{4}$ and $\cos y \sin x =-\frac{1}{4}$ But from here I am having trouble finding stationary points, if anyone could help I would be grateful!","I have this task: find local maximum/minimum of function . Now I have found partial derivatives: and But from here I am having trouble finding stationary points, if anyone could help I would be grateful!","f(x,y)=x+y+4\sin x \sin y \frac{\partial f}{\partial x} = 1+4\cos x \sin y \frac{\partial f}{\partial y} = 1+4\cos y \sin x \cos x \sin y=-\frac{1}{4} \cos y \sin x =-\frac{1}{4}","['calculus', 'multivariable-calculus']"
66,"verification of continuity $\frac{\partial f}{\partial x}$ for $f(x,y) = \frac{xy}{\sqrt{x^2+y^2}}$?",verification of continuity  for ?,"\frac{\partial f}{\partial x} f(x,y) = \frac{xy}{\sqrt{x^2+y^2}}","$$  f(x,y) = \begin{cases} \dfrac{xy}{\sqrt{x^2+y^2}}   & \text{if $(x,y)\neq(0,0)$ } \\[2ex] 0 & \text{if $(x,y)=(0,0)$ }  \\ \end{cases} $$ we have to verify whether $\frac{\partial f}{\partial x}$ is continuous at $(0,0)$ or not. My answer : $\frac{\partial f}{\partial x}=\operatorname{sgn}(y)$ hence, $\frac{\partial f}{\partial x}$ should not be continuous at $(0,0)$ . Unfortunately, I did not get answers to verify it myself. Thank You.","we have to verify whether is continuous at or not. My answer : hence, should not be continuous at . Unfortunately, I did not get answers to verify it myself. Thank You.","
 f(x,y) =
\begin{cases}
\dfrac{xy}{\sqrt{x^2+y^2}}   & \text{if (x,y)\neq(0,0) } \\[2ex]
0 & \text{if (x,y)=(0,0) }  \\
\end{cases}
 \frac{\partial f}{\partial x} (0,0) \frac{\partial f}{\partial x}=\operatorname{sgn}(y) \frac{\partial f}{\partial x} (0,0)","['calculus', 'multivariable-calculus', 'solution-verification']"
67,Maximum error of cylinder volume with percentages,Maximum error of cylinder volume with percentages,,"This question about maximum error is giving me a hard time, you can see what I tried to do to solve it just below. Q: The height and radius of a circular cylinder are measured with possible errors of 2% and 4%, respectively. Aproximate the maximum error percetange in calculating the cylinder volume. I've tried using dV here, pluggin in the values 0,02 and 0,04 and then $$ P = \frac{dV}{V} $$ to get to the result but it's wrong. I don't know what else to try as this particular section is new to me. Help would be greatly appreciated. Edit: the correct answer is 10%","This question about maximum error is giving me a hard time, you can see what I tried to do to solve it just below. Q: The height and radius of a circular cylinder are measured with possible errors of 2% and 4%, respectively. Aproximate the maximum error percetange in calculating the cylinder volume. I've tried using dV here, pluggin in the values 0,02 and 0,04 and then to get to the result but it's wrong. I don't know what else to try as this particular section is new to me. Help would be greatly appreciated. Edit: the correct answer is 10%", P = \frac{dV}{V} ,"['calculus', 'multivariable-calculus', 'partial-derivative']"
68,Transform the region and compute the integral,Transform the region and compute the integral,,"The solution for the integral $I=\int_{0}^{1}\int_{0}^{1}\frac{1}{1-x^2 y^2}dxdy$ is given by transformation $$ x=\frac{\sin u}{\cos v}\qquad \text{and}\qquad y=\frac{\sin v}{\cos u} $$ and then integral becomes $I=\int\int_{E}dudv$ , where $E$ is the trinagle with vertices $(0,0)$ , $(\frac{\pi}{2},0)$ and $(0,\frac{\pi}{2})$ . I can not see how the square $\{(x,y):0\le x,y\le 1\}$ transform to the triangle $E$ under this transform? Since $x,y\geq 0$ , both sine and cosine should have same sign. It should also be $\sin u\le \cos v$ and $\sin v\le \cos u$ since $x,y\leq 1$ . For example let $A=\{(0,y):0\le y\le 1\}$ be one side of square. Under this transformation $0=x=\frac{\sin u}{\cos v}\rightarrow \sin u=0\rightarrow \cos u=1 \text{  or } \cos u=-1$ . In first case, $\cos u=1\rightarrow y=\sin v\rightarrow 0\le \sin v\le 1\rightarrow v\in [0,\frac{\pi}{2}]+2k\pi$ . In second case, $\cos u=-1\rightarrow y=-\sin v\rightarrow -1\le \sin v\le 0\rightarrow v\in [-\frac{\pi}{2},0]+2k\pi$ . I can not see the line segment $A$ transform which shape under this transformation?","The solution for the integral is given by transformation and then integral becomes , where is the trinagle with vertices , and . I can not see how the square transform to the triangle under this transform? Since , both sine and cosine should have same sign. It should also be and since . For example let be one side of square. Under this transformation . In first case, . In second case, . I can not see the line segment transform which shape under this transformation?","I=\int_{0}^{1}\int_{0}^{1}\frac{1}{1-x^2 y^2}dxdy 
x=\frac{\sin u}{\cos v}\qquad \text{and}\qquad y=\frac{\sin v}{\cos u}
 I=\int\int_{E}dudv E (0,0) (\frac{\pi}{2},0) (0,\frac{\pi}{2}) \{(x,y):0\le x,y\le 1\} E x,y\geq 0 \sin u\le \cos v \sin v\le \cos u x,y\leq 1 A=\{(0,y):0\le y\le 1\} 0=x=\frac{\sin u}{\cos v}\rightarrow \sin u=0\rightarrow \cos u=1 \text{  or } \cos u=-1 \cos u=1\rightarrow y=\sin v\rightarrow 0\le \sin v\le 1\rightarrow v\in [0,\frac{\pi}{2}]+2k\pi \cos u=-1\rightarrow y=-\sin v\rightarrow -1\le \sin v\le 0\rightarrow v\in [-\frac{\pi}{2},0]+2k\pi A","['real-analysis', 'calculus', 'integration', 'multivariable-calculus']"
69,Derivative of a multivariable composite functions,Derivative of a multivariable composite functions,,"Let $\;f(x,y,z)=xz+xy+yz\;$ and $\;g(t)=(e^t,\cos t,\sin t)$ . The thing is that I want to calculate $(f\circ g)'(1)$ . Previously, I was asked to calculate $g'$ and $\nabla f$ , so I do not know if that is necessary to calculate $(f\circ g)'(1)$ or I can simply composite the functions and apply chain rule for $t$ .","Let and . The thing is that I want to calculate . Previously, I was asked to calculate and , so I do not know if that is necessary to calculate or I can simply composite the functions and apply chain rule for .","\;f(x,y,z)=xz+xy+yz\; \;g(t)=(e^t,\cos t,\sin t) (f\circ g)'(1) g' \nabla f (f\circ g)'(1) t","['multivariable-calculus', 'chain-rule']"
70,Is this a valid example of the tangent vector a linear map from a smooth function to the reals?,Is this a valid example of the tangent vector a linear map from a smooth function to the reals?,,"This is intended to give an example of the tangent vector at a point $p$ along a smooth curve $\gamma(t)$ on a manifold as a linear map (directional derivative) from a smooth function $f$ to the real numbers constructed as $$f \mapsto (f \circ \gamma)'(0)$$ and where $p =\gamma(0).$ The attempt at illustrating this construct imagines the function $f(x,y)= e^{-\left((x+ 2.13)^2+(y+1.16)^{2}\right)}$ (a bivariate Gaussian bell curve) representing the density of historically relevant buildings and monuments at any given point in the city of Paris (from this site ): The chart used that allows Cartesian coordinates imposed on the manifold is centered at the Louvre $(0,0),$ and the function $f$ happens to have its maximum around des Champs-Élysées, so that we expect the density of historical buildings to start decreasing away from that point. This function is evaluated along a curve $\gamma(t)= -0.16(t +2.5)^2 +1$ which when mapped onto the chart is $\gamma(t)=(t, -0.16(t +2.5)^2 +1).$ It roughly follows the Seine river in the center of Paris. The Louvre is the point $p=\gamma(0)=(0,0).$ The velocity vector or tangent at a point is $\begin{bmatrix}1 & -0.32(t+2.5)\end{bmatrix}^\top,$ or in the chart, $\begin{bmatrix}1 & -0.32(x+2.5)\end{bmatrix}^\top.$ For the attempted (are they correct (?)) calculations the composition $f\circ \gamma$ is expressed as $f(a(t), b(t)).$ $$\begin{align} \small X_pf &= (f\circ \gamma)'(t=0) \\[2ex] &=\left[ \frac{\partial f}{\partial x} \left(x(t),y(t)\right) \,  \color{red}{x'(t)}+ \frac{\partial f}{\partial y}\left(x(t),y(t)\right)\,\color{red}{y'(t)}\right]\Bigg|_{t=0} \\[3ex] &= \begin{bmatrix}\frac{\partial f}{\partial x} (p) & \frac{\partial f}{\partial y}(p)\end{bmatrix}^\top \color{red}{\underset{\text{tangent vector }\\\text{(velocity)}}{\begin{bmatrix}x'(0)\\y'(0)\end{bmatrix}}}  \\[3ex] & = \left. \begin{bmatrix}-2 (x + 2.13) e^{(-(x + 2.13)^2 - (y + 1.16)^2)} \\  -2 (y + 1.16) e^{(-(x + 2.13)^2 - (y + 1.16)^2)}\end{bmatrix}^\top\right|_{p=\gamma(0)}\small\color{red}{\begin{bmatrix}x'(0) \\ y'(0)\end{bmatrix}} \\[3ex] &=\small{ 2 (t + 2.13) e^{-(t + 2.13)^2 - ((-0.16 (t + 2.5)^2 + 1) + 1.16)^2} \\ + (-2 ((-0.16 (t + 2.5)^2 + 1) + 1.16) e^{-(t + 2.13)^2 - ((-0.16 (t + 2.5)^2 + 1) + 1.16)^2)}(-0.32 (t + 2.5)) \Big|_{t=0}}\\[2ex] &=-0.00670189 \end{align}$$","This is intended to give an example of the tangent vector at a point along a smooth curve on a manifold as a linear map (directional derivative) from a smooth function to the real numbers constructed as and where The attempt at illustrating this construct imagines the function (a bivariate Gaussian bell curve) representing the density of historically relevant buildings and monuments at any given point in the city of Paris (from this site ): The chart used that allows Cartesian coordinates imposed on the manifold is centered at the Louvre and the function happens to have its maximum around des Champs-Élysées, so that we expect the density of historical buildings to start decreasing away from that point. This function is evaluated along a curve which when mapped onto the chart is It roughly follows the Seine river in the center of Paris. The Louvre is the point The velocity vector or tangent at a point is or in the chart, For the attempted (are they correct (?)) calculations the composition is expressed as","p \gamma(t) f f \mapsto (f \circ \gamma)'(0) p =\gamma(0). f(x,y)= e^{-\left((x+ 2.13)^2+(y+1.16)^{2}\right)} (0,0), f \gamma(t)= -0.16(t +2.5)^2 +1 \gamma(t)=(t, -0.16(t +2.5)^2 +1). p=\gamma(0)=(0,0). \begin{bmatrix}1 & -0.32(t+2.5)\end{bmatrix}^\top, \begin{bmatrix}1 & -0.32(x+2.5)\end{bmatrix}^\top. f\circ \gamma f(a(t), b(t)). \begin{align}
\small X_pf &= (f\circ \gamma)'(t=0) \\[2ex]
&=\left[ \frac{\partial f}{\partial x} \left(x(t),y(t)\right) \, 
\color{red}{x'(t)}+ \frac{\partial f}{\partial y}\left(x(t),y(t)\right)\,\color{red}{y'(t)}\right]\Bigg|_{t=0} \\[3ex]
&= \begin{bmatrix}\frac{\partial f}{\partial x} (p) & \frac{\partial f}{\partial y}(p)\end{bmatrix}^\top \color{red}{\underset{\text{tangent vector }\\\text{(velocity)}}{\begin{bmatrix}x'(0)\\y'(0)\end{bmatrix}}}  \\[3ex]
& = \left. \begin{bmatrix}-2 (x + 2.13) e^{(-(x + 2.13)^2 - (y + 1.16)^2)} \\  -2 (y + 1.16) e^{(-(x + 2.13)^2 - (y + 1.16)^2)}\end{bmatrix}^\top\right|_{p=\gamma(0)}\small\color{red}{\begin{bmatrix}x'(0) \\ y'(0)\end{bmatrix}} \\[3ex]
&=\small{ 2 (t + 2.13) e^{-(t + 2.13)^2 - ((-0.16 (t + 2.5)^2 + 1) + 1.16)^2} \\ + (-2 ((-0.16 (t + 2.5)^2 + 1) + 1.16) e^{-(t + 2.13)^2 - ((-0.16 (t + 2.5)^2 + 1) + 1.16)^2)}(-0.32 (t + 2.5)) \Big|_{t=0}}\\[2ex]
&=-0.00670189
\end{align}","['multivariable-calculus', 'differential-geometry']"
71,I stuck on this step on the chapter about Lagrange multipliers.,I stuck on this step on the chapter about Lagrange multipliers.,,"As I understood, to find critical solutions for some system of equations, we assume if giving a function f() restricted to some level set of other functions g i () with the same domain and range, then its gradient vector will be perpendicular to the tangent vector of the level set at a critical points of f() and can be expressed as a linear combination of gradient vectors of functions g i () at that point. Also to get particular results, we should have enough information for our linear equation. The book suggests to just put our linear combinations to restriction functions and solve for lambdas. That is the part that I'm struggling. I can't describe the meaning of this step and why we can acquire the answer without even using information from the distance function.","As I understood, to find critical solutions for some system of equations, we assume if giving a function f() restricted to some level set of other functions g i () with the same domain and range, then its gradient vector will be perpendicular to the tangent vector of the level set at a critical points of f() and can be expressed as a linear combination of gradient vectors of functions g i () at that point. Also to get particular results, we should have enough information for our linear equation. The book suggests to just put our linear combinations to restriction functions and solve for lambdas. That is the part that I'm struggling. I can't describe the meaning of this step and why we can acquire the answer without even using information from the distance function.",,['multivariable-calculus']
72,Most interesting exercises about the implicit and inverse function theorems,Most interesting exercises about the implicit and inverse function theorems,,"I am a TA in a multivariable calculus course this semester. Right now I am writing the exercise session which deals with the implicit function theorem, inverse function theorem and open function theorem (i.e. submersions are open maps). I am looking for your favorite/most interesting exercises which I can use/adapt. I especially like exercises which combine application of the theorems with some geometrical problem, but I am open to anything interesting. For instance, I give one exercise which deals with computing the tangent to the intersection curve of two surfaces (which requires computing the derivatives implicitly). Thanks in advance!","I am a TA in a multivariable calculus course this semester. Right now I am writing the exercise session which deals with the implicit function theorem, inverse function theorem and open function theorem (i.e. submersions are open maps). I am looking for your favorite/most interesting exercises which I can use/adapt. I especially like exercises which combine application of the theorems with some geometrical problem, but I am open to anything interesting. For instance, I give one exercise which deals with computing the tangent to the intersection curve of two surfaces (which requires computing the derivatives implicitly). Thanks in advance!",,"['multivariable-calculus', 'soft-question', 'education', 'implicit-function-theorem', 'inverse-function-theorem']"
73,Multivariate differentiable function on a compact is a Lipschitz function?,Multivariate differentiable function on a compact is a Lipschitz function?,,"Let $n,m\in\mathbb{N}$ , such that $E=(\mathbb{R}^n,\|\cdot\|_E)$ and $F=(\mathbb{R}^m,\|\cdot\|_F)$ with $\|\cdot\|_E$ and $\|\cdot\|_F$ two norms (we do not care which one since $n,m\in\mathbb{N}$ ). I have a little doubt about the ""lipschitzness"" of a mapping $f$ . Assuming $f:E\rightarrow F$ , differentiable over $E$ (thus continuous over $E$ ). If $C\subset E$ is compact, then $f$ is $k$ -lipschitz for a given $k>0$ over $C$ , right ? If $n=m=1$ , I know that is true, but I didn't find the same result in multivariate fashion (maybe because it is too obvious) and I did not go further on this question since it's been a while I do not have pratice math. Since $\mathbb{R}^n$ has very good topological properties, I think that this result is true. Am I wrong ? Thanks ! Edit : Nice counter examples below ! The aforementioned statement is right only if $f$ is $C^1$ .","Let , such that and with and two norms (we do not care which one since ). I have a little doubt about the ""lipschitzness"" of a mapping . Assuming , differentiable over (thus continuous over ). If is compact, then is -lipschitz for a given over , right ? If , I know that is true, but I didn't find the same result in multivariate fashion (maybe because it is too obvious) and I did not go further on this question since it's been a while I do not have pratice math. Since has very good topological properties, I think that this result is true. Am I wrong ? Thanks ! Edit : Nice counter examples below ! The aforementioned statement is right only if is .","n,m\in\mathbb{N} E=(\mathbb{R}^n,\|\cdot\|_E) F=(\mathbb{R}^m,\|\cdot\|_F) \|\cdot\|_E \|\cdot\|_F n,m\in\mathbb{N} f f:E\rightarrow F E E C\subset E f k k>0 C n=m=1 \mathbb{R}^n f C^1","['multivariable-calculus', 'lipschitz-functions']"
74,"How to show that $X =\{(x,0)\mid x \le 0 \text{ and }x \in\Bbb R\}$ is closed?",How to show that  is closed?,"X =\{(x,0)\mid x \le 0 \text{ and }x \in\Bbb R\}","I am comfortable with the intuition of closed sets, however, I am struggling to formalise these ideas. Consider the following question: Consider the set $$X = \{ (x,0) | x \le 0 \text{ and } x \in \Bbb R \} $$ Show that the set $X$ is closed. Formally, to do this, we must show that if we take some arbitrary convergent sequence of points in $X$ , then the limit must also be an element of $X$ . This set seems like a fairly simplistic set to apply this to, but I am not entirely comfortable with constructing these types of arguments, and I'm unclear how to show this generally for some arbitrary sequence. I would be grateful for any guidance or references.","I am comfortable with the intuition of closed sets, however, I am struggling to formalise these ideas. Consider the following question: Consider the set Show that the set is closed. Formally, to do this, we must show that if we take some arbitrary convergent sequence of points in , then the limit must also be an element of . This set seems like a fairly simplistic set to apply this to, but I am not entirely comfortable with constructing these types of arguments, and I'm unclear how to show this generally for some arbitrary sequence. I would be grateful for any guidance or references.","X = \{ (x,0) | x \le 0 \text{ and } x \in \Bbb R \}  X X X","['real-analysis', 'general-topology', 'analysis', 'multivariable-calculus', 'proof-writing']"
75,joint density problem from P1 probability actuary book - solving density function of the subtraction of random variables,joint density problem from P1 probability actuary book - solving density function of the subtraction of random variables,,"Let X and Y be two random variables with joint density function $$f_{XY} $$ . Compute the pdf of $$ U = Y − X $$ I've looked at this problem multiple times and I keep getting a different answer than the book. For my joint density function I'm getting the following: $$f_{Y −X}(a)= \int_{-\infty}^{\infty} f_{X,Y}(x, a+x) dx = \int_{-\infty}^{\infty} -f_{X,Y}(y-a, y) dy $$ If X and Y are independent I'm getting the following: $$ f_{Y −X}(a)= \int_{-\infty}^{\infty} f_{Y}(a+x) \cdot f_X(x) dx =  \int_{-\infty}^{\infty} -f_{X}(y-a) \cdot f_Y(y) dy $$ The book gives this answer: $$ f_{Y −X}(a) = \int_{-\infty}^{\infty} f_{X,Y} (y − a, y)dy $$ Moreover, the book says If X and Y are independent then $$  f_{Y −X}(a) = \int_{-\infty}^{\infty} f_X(y − a)f_Y (y)dy = \int_{-\infty}^{\infty}f_X(y)f_Y (a + y)dy $$ would anyone kindly explain how the book got this answer and am I wrong? I was simply following the same approach as if we were to calculate $$ U = X+Y $$ but it seems something is really off with my answer...","Let X and Y be two random variables with joint density function . Compute the pdf of I've looked at this problem multiple times and I keep getting a different answer than the book. For my joint density function I'm getting the following: If X and Y are independent I'm getting the following: The book gives this answer: Moreover, the book says If X and Y are independent then would anyone kindly explain how the book got this answer and am I wrong? I was simply following the same approach as if we were to calculate but it seems something is really off with my answer...","f_{XY}   U = Y − X  f_{Y −X}(a)= \int_{-\infty}^{\infty} f_{X,Y}(x, a+x) dx = \int_{-\infty}^{\infty} -f_{X,Y}(y-a, y) dy   f_{Y −X}(a)= \int_{-\infty}^{\infty} f_{Y}(a+x) \cdot f_X(x) dx =  \int_{-\infty}^{\infty} -f_{X}(y-a) \cdot f_Y(y) dy   f_{Y −X}(a) = \int_{-\infty}^{\infty} f_{X,Y} (y − a, y)dy    f_{Y −X}(a) = \int_{-\infty}^{\infty} f_X(y − a)f_Y (y)dy = \int_{-\infty}^{\infty}f_X(y)f_Y (a + y)dy   U = X+Y ","['probability-theory', 'multivariable-calculus', 'probability-distributions']"
76,"Find the maximum and minimum of $f(x,y) = x^2 -xy +y^2$ subject to constraints $|x|+|y|\leq1$.",Find the maximum and minimum of  subject to constraints .,"f(x,y) = x^2 -xy +y^2 |x|+|y|\leq1","Find the maximum and minimum of $f(x,y) = x^2 -xy +y^2$ subject to constraints $|x|+|y|\leq1$ . The constraint of this exercise came out of left field for me, I'm not sure how to handle it. Note, we have not gone through Lagrangian multipliers yet, so an approach without them would be preferable. To get started somewhere, I began by computing the gradient, obtaining the following: $$[2x-y \hspace{3mm} 2y-x]$$ Thus we can see that for a critical point, we require that $2x=y$ and $2y=x$ , which leads to $x=y=0$ and $(0,0)$ is a critical point contained inside our restriction. But then I'm not sure how to proceed. Apologies for the lack of an attempt, but I'm simply not sure how to go about this in a systematic way, so any input would be much apriciated!","Find the maximum and minimum of subject to constraints . The constraint of this exercise came out of left field for me, I'm not sure how to handle it. Note, we have not gone through Lagrangian multipliers yet, so an approach without them would be preferable. To get started somewhere, I began by computing the gradient, obtaining the following: Thus we can see that for a critical point, we require that and , which leads to and is a critical point contained inside our restriction. But then I'm not sure how to proceed. Apologies for the lack of an attempt, but I'm simply not sure how to go about this in a systematic way, so any input would be much apriciated!","f(x,y) = x^2 -xy +y^2 |x|+|y|\leq1 [2x-y \hspace{3mm} 2y-x] 2x=y 2y=x x=y=0 (0,0)","['calculus', 'multivariable-calculus']"
77,Question from P1 exam book - joint continuous gamma distribution. This is a question from the actuary P1 book,Question from P1 exam book - joint continuous gamma distribution. This is a question from the actuary P1 book,,"If X and Y are independent gamma random variables with parameters (α, λ) and (β, λ) respectively, compute the joint density of U = X + Y and $$ V = \frac{X}{X+Y} $$ I was stuck on how to find $$ f_U $$ and $$ f_V $$ for a while but was able to figure it out. I had seen some questions pertaining to parts of this problem or slightly different problems with people struggling and thought I should post it. The answer with my work is below. I hope it helps!","If X and Y are independent gamma random variables with parameters (α, λ) and (β, λ) respectively, compute the joint density of U = X + Y and I was stuck on how to find and for a while but was able to figure it out. I had seen some questions pertaining to parts of this problem or slightly different problems with people struggling and thought I should post it. The answer with my work is below. I hope it helps!"," V =
\frac{X}{X+Y}   f_U   f_V ","['probability', 'probability-theory', 'multivariable-calculus', 'probability-distributions']"
78,Generalizing an incomplete Gamma function to many variables.,Generalizing an incomplete Gamma function to many variables.,,"One of the topics of interest in random matrix theory is the joint distribution of eigenvalues of Gaussian random matrices in the Wishart ensemble, see Harnad, John (ed.) , Random matrices, random processes and integrable systems , CRM Series in Mathematical Physics. Berlin: Springer (ISBN 978-1-4419-9513-1/hbk; 978-1-4419-9514-8/ebook). xviii, 524 p. (2011). ZBL1215.15002 ., for example. Now, in order to obtain the density of eigenvalues there is a need to integrate over simplices  with  the integrand being a product of power functions and an exponential. This leads to   a generalization of the Gamma function in $d $ dimensions  and as such motivates the following definition below. Let $d \ge 2$ be an integer.  Let $\vec{n}:= (n_1,n_2,\cdots,n_{d-1}) \in {\mathbb N}_+^{d-1} $ where $n_j \ge 1 $ for $j=1,\cdots,d-1$ . Finally let $\lambda_d \ge 0 $ be real. Definition 1: The multivariate lower, incomplete Gamma function  is defined as follows: \begin{equation} \gamma(\vec{n},\lambda_d) := \int\limits_{\Delta_{d-1}^{(-)}(\lambda_d)} \left( \prod\limits_{\xi=1}^{d-1} \lambda_\xi^{n_\xi-1} \right) \cdot  e^{-\sum\limits_{\xi=1}^d \lambda_\xi} \cdot \prod\limits_{\xi=1}^{d-1} d\lambda_\xi \end{equation} where $\Delta^{(-)}_{d-1} (\lambda) := \left\{ (\lambda_\xi)_{\xi=1}^{d-1} | 0 \le \lambda_1 \le \cdots \le \lambda_{d-1} \le \lambda\right\}$ . Result 1: By using elementary integration and induction in the dimension $d$ and then differentiation with respect to parameters we have found the following formula below: \begin{equation} \gamma(\vec{n},\lambda_d) = (-1)^{|\vec{n}|-d+1}  \prod\limits_{\xi=1}^{d-1}  \frac{d^{n_\xi-1}}{d a_\xi^{n_\xi-1}} \left.\left( \sum\limits_{j=1}^d (-1)^{d-j} \frac{e^{-(\sum\limits_{\xi=j}^d a_\xi) \cdot \lambda_d}}{\prod\limits_{\xi=1}^{j-1} (a_\xi+\cdots+a_{j-1}) \cdot \prod\limits_{\xi=j}^{d-1}(a_j+\cdots+ a_\xi)} \right)\right|_{(a_\xi)_{\xi=1}^d:>1} \tag{1a} \end{equation} Definition 2: The multivariate upper, incomplete Gamma function  is defined as follows: \begin{equation} \Gamma(\vec{n},\lambda_d) := \int\limits_{\Delta_{d-1}^{(+)}(\lambda_d)} \left( \prod\limits_{\xi=1}^{d-1} \lambda_\xi^{n_\xi-1} \right) \cdot  e^{-\sum\limits_{\xi=1}^d \lambda_\xi} \cdot \prod\limits_{\xi=1}^{d-1} d\lambda_\xi \end{equation} where $\Delta^{(+)}_{d-1} (\lambda) := \left\{ (\lambda_\xi)_{\xi=1}^{d-1} | \lambda \le \lambda_1 \le \cdots \le \lambda_{d-1} \le \infty\right\}$ . Result 2: Using the same technique as above  we have found the following formula below: \begin{equation} \Gamma(\vec{n},\lambda_d) = (-1)^{|\vec{n}|-d+1} \prod\limits_{\xi=1}^{d-1}  \frac{d^{n_\xi-1}}{d a_\xi^{n_\xi-1}} \left. \left( \frac{e^{-(\sum\limits_{\xi=1}^d a_\xi) \cdot \lambda_d}}{\prod\limits_{\xi=1}^{d-1} (a_\xi+\cdots+a_{d-1})} \right) \right|_{(a_\xi)_{\xi=1}^d:>1} \tag{2a} \end{equation} The Mathematica code snippet below  verifies the results above: In[772]:= d = RandomInteger[{2, 6}]; L = RandomReal[{0, 5}]; n = RandomInteger[{1, 5}, d - 1]; (*Lower incomplete*) NIntegrate[  Product[l[xi]^(    n[[xi]] - 1), {xi, 1, d - 1}] Exp[-Sum[l[xi], {xi, 1, d - 1}] - L],   Evaluate[   Sequence @@     Table[{l[j], If[j == 1, 0, l[j - 1]], L}, {j, 1, d - 1}]]] D[Sum[(-1)^(Total[n] - j - 1)/1 Exp[-Sum[a[eta], {eta, j, d}] L]/     Product[Sum[a[eta], {eta, xi, j - 1}], {xi, 1, j - 1}] 1/     Product[Sum[a[eta], {eta, j, xi}], {xi, j + 0, d - 1}], {j, 1,      d}], Evaluate[    Sequence @@ Table[{a[xi], n[[xi]] - 1}, {xi, 1, d - 1}]]] /.   a[j_] :> 1  (*Upper incomplete*) NIntegrate[  Product[l[xi]^(    n[[xi]] - 1), {xi, 1, d - 1}] Exp[-Sum[l[xi], {xi, 1, d - 1}] - L],   Evaluate[   Sequence @@     Table[{l[j], If[j == 1, L, l[j - 1]], Infinity}, {j, 1, d - 1}]]] (-1)^(Total[n] - d + 1)    D[Exp[-Sum[a[eta], {eta, 1, d}] L]/1 1/     Product[Sum[a[eta], {eta, xi, d - 1}], {xi, 1 + 0, d - 1}],     Evaluate[     Sequence @@ Table[{a[xi], n[[xi]] - 1}, {xi, 1, d - 1}]]] /.   a[j_] :> 1        Out[775]= 0.0295689  Out[776]= 0.0295689  Out[777]= 0.00111378  Out[778]= 0.00111378 Now, having said all this the question would be twofold. Firstly, can we evaluate those derivatives in $(1a)$ and $(2a)$ in order to obtain some neat closed form expressions. Secondly, can we generalize those results to the case when the components of the vector $\vec{n}$ are real?","One of the topics of interest in random matrix theory is the joint distribution of eigenvalues of Gaussian random matrices in the Wishart ensemble, see Harnad, John (ed.) , Random matrices, random processes and integrable systems , CRM Series in Mathematical Physics. Berlin: Springer (ISBN 978-1-4419-9513-1/hbk; 978-1-4419-9514-8/ebook). xviii, 524 p. (2011). ZBL1215.15002 ., for example. Now, in order to obtain the density of eigenvalues there is a need to integrate over simplices  with  the integrand being a product of power functions and an exponential. This leads to   a generalization of the Gamma function in dimensions  and as such motivates the following definition below. Let be an integer.  Let where for . Finally let be real. Definition 1: The multivariate lower, incomplete Gamma function  is defined as follows: where . Result 1: By using elementary integration and induction in the dimension and then differentiation with respect to parameters we have found the following formula below: Definition 2: The multivariate upper, incomplete Gamma function  is defined as follows: where . Result 2: Using the same technique as above  we have found the following formula below: The Mathematica code snippet below  verifies the results above: In[772]:= d = RandomInteger[{2, 6}]; L = RandomReal[{0, 5}]; n = RandomInteger[{1, 5}, d - 1]; (*Lower incomplete*) NIntegrate[  Product[l[xi]^(    n[[xi]] - 1), {xi, 1, d - 1}] Exp[-Sum[l[xi], {xi, 1, d - 1}] - L],   Evaluate[   Sequence @@     Table[{l[j], If[j == 1, 0, l[j - 1]], L}, {j, 1, d - 1}]]] D[Sum[(-1)^(Total[n] - j - 1)/1 Exp[-Sum[a[eta], {eta, j, d}] L]/     Product[Sum[a[eta], {eta, xi, j - 1}], {xi, 1, j - 1}] 1/     Product[Sum[a[eta], {eta, j, xi}], {xi, j + 0, d - 1}], {j, 1,      d}], Evaluate[    Sequence @@ Table[{a[xi], n[[xi]] - 1}, {xi, 1, d - 1}]]] /.   a[j_] :> 1  (*Upper incomplete*) NIntegrate[  Product[l[xi]^(    n[[xi]] - 1), {xi, 1, d - 1}] Exp[-Sum[l[xi], {xi, 1, d - 1}] - L],   Evaluate[   Sequence @@     Table[{l[j], If[j == 1, L, l[j - 1]], Infinity}, {j, 1, d - 1}]]] (-1)^(Total[n] - d + 1)    D[Exp[-Sum[a[eta], {eta, 1, d}] L]/1 1/     Product[Sum[a[eta], {eta, xi, d - 1}], {xi, 1 + 0, d - 1}],     Evaluate[     Sequence @@ Table[{a[xi], n[[xi]] - 1}, {xi, 1, d - 1}]]] /.   a[j_] :> 1        Out[775]= 0.0295689  Out[776]= 0.0295689  Out[777]= 0.00111378  Out[778]= 0.00111378 Now, having said all this the question would be twofold. Firstly, can we evaluate those derivatives in and in order to obtain some neat closed form expressions. Secondly, can we generalize those results to the case when the components of the vector are real?","d  d \ge 2 \vec{n}:= (n_1,n_2,\cdots,n_{d-1}) \in {\mathbb N}_+^{d-1}  n_j \ge 1  j=1,\cdots,d-1 \lambda_d \ge 0  \begin{equation}
\gamma(\vec{n},\lambda_d) := \int\limits_{\Delta_{d-1}^{(-)}(\lambda_d)}
\left( \prod\limits_{\xi=1}^{d-1} \lambda_\xi^{n_\xi-1} \right) \cdot 
e^{-\sum\limits_{\xi=1}^d \lambda_\xi} \cdot \prod\limits_{\xi=1}^{d-1} d\lambda_\xi
\end{equation} \Delta^{(-)}_{d-1} (\lambda) := \left\{ (\lambda_\xi)_{\xi=1}^{d-1} | 0 \le \lambda_1 \le \cdots \le \lambda_{d-1} \le \lambda\right\} d \begin{equation}
\gamma(\vec{n},\lambda_d) = (-1)^{|\vec{n}|-d+1}  \prod\limits_{\xi=1}^{d-1}  \frac{d^{n_\xi-1}}{d a_\xi^{n_\xi-1}} \left.\left(
\sum\limits_{j=1}^d (-1)^{d-j} \frac{e^{-(\sum\limits_{\xi=j}^d a_\xi) \cdot \lambda_d}}{\prod\limits_{\xi=1}^{j-1} (a_\xi+\cdots+a_{j-1}) \cdot \prod\limits_{\xi=j}^{d-1}(a_j+\cdots+ a_\xi)}
\right)\right|_{(a_\xi)_{\xi=1}^d:>1} \tag{1a}
\end{equation} \begin{equation}
\Gamma(\vec{n},\lambda_d) := \int\limits_{\Delta_{d-1}^{(+)}(\lambda_d)}
\left( \prod\limits_{\xi=1}^{d-1} \lambda_\xi^{n_\xi-1} \right) \cdot 
e^{-\sum\limits_{\xi=1}^d \lambda_\xi} \cdot \prod\limits_{\xi=1}^{d-1} d\lambda_\xi
\end{equation} \Delta^{(+)}_{d-1} (\lambda) := \left\{ (\lambda_\xi)_{\xi=1}^{d-1} | \lambda \le \lambda_1 \le \cdots \le \lambda_{d-1} \le \infty\right\} \begin{equation}
\Gamma(\vec{n},\lambda_d) = (-1)^{|\vec{n}|-d+1} \prod\limits_{\xi=1}^{d-1}  \frac{d^{n_\xi-1}}{d a_\xi^{n_\xi-1}}
\left.
\left(
\frac{e^{-(\sum\limits_{\xi=1}^d a_\xi) \cdot \lambda_d}}{\prod\limits_{\xi=1}^{d-1} (a_\xi+\cdots+a_{d-1})}
\right)
\right|_{(a_\xi)_{\xi=1}^d:>1} \tag{2a}
\end{equation} (1a) (2a) \vec{n}","['multivariable-calculus', 'eigenvalues-eigenvectors', 'gamma-function', 'random-matrices']"
79,Evaluating Surface Integral with Divergence Theorem,Evaluating Surface Integral with Divergence Theorem,,"Evaluate $\displaystyle\int_S \mathbf{F\cdot n}\ dS$ over the entire surface of the region above the $xy$ plane bounded by the cone $z^2=x^2+y^2$ and the plane $z=4$ if $\mathbf F=x\hat i+y\hat j+z^2\hat k$ . Solution: By the divergence theorem, since $\nabla \cdot\mathbf F=2+2z$ , we have that $$\begin{align} \int_S \mathbf{F\cdot n}\ dS&=\int_A \int_{z=\sqrt{x^2+y^2}=r} ^4 (2+2z)\ dz\ dx\ dy\\ &=\iint_A [2z+z^2]_r ^4 \ dx\ dy\\  &=\int_\theta^{2\pi} \int_{r=0} ^{4} (24-2r-r^2)r\ dr\ d\theta\\ &=\int_{\theta=0}^{2\pi}\left[24\frac{r^2}{2}-2\frac{r^2}{3}-\frac{r^4}{4}\right]_0^4\\ &=\int^{2\pi}_0 \frac {256}{3}\ d\theta\\ &=\frac{512}{3} \pi \end{align}$$ The given answer is $\dfrac{ 128\pi}{3}$ . Where did I mistake, exactly?","Evaluate over the entire surface of the region above the plane bounded by the cone and the plane if . Solution: By the divergence theorem, since , we have that The given answer is . Where did I mistake, exactly?","\displaystyle\int_S \mathbf{F\cdot n}\ dS xy z^2=x^2+y^2 z=4 \mathbf F=x\hat i+y\hat j+z^2\hat k \nabla \cdot\mathbf F=2+2z \begin{align}
\int_S \mathbf{F\cdot n}\ dS&=\int_A \int_{z=\sqrt{x^2+y^2}=r} ^4 (2+2z)\ dz\ dx\ dy\\
&=\iint_A [2z+z^2]_r ^4 \ dx\ dy\\ 
&=\int_\theta^{2\pi} \int_{r=0} ^{4} (24-2r-r^2)r\ dr\ d\theta\\
&=\int_{\theta=0}^{2\pi}\left[24\frac{r^2}{2}-2\frac{r^2}{3}-\frac{r^4}{4}\right]_0^4\\
&=\int^{2\pi}_0 \frac {256}{3}\ d\theta\\
&=\frac{512}{3} \pi
\end{align} \dfrac{ 128\pi}{3}","['multivariable-calculus', 'vector-analysis', 'surface-integrals', 'divergence-theorem']"
80,Is this Matrix Indefinite or Positive Semi Definite?,Is this Matrix Indefinite or Positive Semi Definite?,,"Consider the following matrix: $ A=\begin{pmatrix} 1&1&1\\  1&1&1\\  1&1&0 \end{pmatrix} $ The eigenvalues are $\lambda_1=0$ , $\lambda_2=1-\sqrt{3}≤0$ and $\lambda_3=1+\sqrt{3}≥0$ . But when computing the principal minors we obtain: $A_1=1≥0$ $A_2=\begin{vmatrix} 1&1\\  1&1 \end{vmatrix} = 0$ $A_2= \begin{vmatrix} 1&1&1\\ 1&1&1\\ 1&1&0 \end{vmatrix} = 0$ So all leading principal minors are ≥ 0, but we have two eigenvalues with different sign and the third one is zero... Im really confused, thank you!","Consider the following matrix: The eigenvalues are , and . But when computing the principal minors we obtain: So all leading principal minors are ≥ 0, but we have two eigenvalues with different sign and the third one is zero... Im really confused, thank you!"," A=\begin{pmatrix}
1&1&1\\ 
1&1&1\\ 
1&1&0
\end{pmatrix}  \lambda_1=0 \lambda_2=1-\sqrt{3}≤0 \lambda_3=1+\sqrt{3}≥0 A_1=1≥0 A_2=\begin{vmatrix} 1&1\\  1&1 \end{vmatrix} = 0 A_2= \begin{vmatrix} 1&1&1\\ 1&1&1\\ 1&1&0 \end{vmatrix} = 0","['linear-algebra', 'matrices']"
81,Calculating a surface integral,Calculating a surface integral,,"I'm working on a problem which involves calculating a surface integral. The problem is as follows: Consider S the part of the surface with equation $x^2+y^2=13-z$ which lies above the plane $z=4$ . Calculate the following integral $$ I =\iint_S \vec{r} \cdot\vec{n} \mathrm{d}O$$ where $\vec{r} = x\vec{u_1}+y\vec{u_2}+z\vec{u_3}$ and $\vec{n}$ is the normal to $S$ pointing upwards. In my textbook I found that I can use that $ I =\iint_S \vec{r} \cdot\vec{n} \mathrm{d}O = \iiint_G \mathrm{div} \vec{v}$ where $G$ is the area in $\mathbb{R}^3$ enclosed by $S$ . $\mathrm{div} \vec{v}$ was obvious and easy to calculate and equal to $3$ . I feel the problem will be easiest if I convert to cylindrical coordinates so I put $$ \begin{cases} x = r\cos\theta \\ y=r\sin\theta \\ z = 13-r^2  \end{cases} $$ Now I my problem lies with finding out my boundaries in cylindrical coordinates. I know that $0 \leq \theta \leq 2\pi$ and that $ 4\leq z$ but further I'm not sure what the boundaries would be. Is my working so far correct? If so what would the remaining boundaries be? If not, what would be the correct method to work this problem out? Is there any other method to working this problem out? Thanks in advance for any help.","I'm working on a problem which involves calculating a surface integral. The problem is as follows: Consider S the part of the surface with equation which lies above the plane . Calculate the following integral where and is the normal to pointing upwards. In my textbook I found that I can use that where is the area in enclosed by . was obvious and easy to calculate and equal to . I feel the problem will be easiest if I convert to cylindrical coordinates so I put Now I my problem lies with finding out my boundaries in cylindrical coordinates. I know that and that but further I'm not sure what the boundaries would be. Is my working so far correct? If so what would the remaining boundaries be? If not, what would be the correct method to work this problem out? Is there any other method to working this problem out? Thanks in advance for any help.",x^2+y^2=13-z z=4  I =\iint_S \vec{r} \cdot\vec{n} \mathrm{d}O \vec{r} = x\vec{u_1}+y\vec{u_2}+z\vec{u_3} \vec{n} S  I =\iint_S \vec{r} \cdot\vec{n} \mathrm{d}O = \iiint_G \mathrm{div} \vec{v} G \mathbb{R}^3 S \mathrm{div} \vec{v} 3  \begin{cases} x = r\cos\theta \\ y=r\sin\theta \\ z = 13-r^2  \end{cases}  0 \leq \theta \leq 2\pi  4\leq z,"['calculus', 'integration', 'multivariable-calculus']"
82,Measuring the conformality or equal-areaness of a projection from the sphere to some surface embedded in $R^3$,Measuring the conformality or equal-areaness of a projection from the sphere to some surface embedded in,R^3,"Suppose I want to map from the sphere to some other surface that is embedded into $\Bbb R^3$ , which we will treat as an embedding into $\Bbb R^3$ . In this situation, suppose I already have some formula to map from latitudes and longitudes, denoted $(\lambda, \phi)$ to $(x, y, z)$ coordinates in $\Bbb R^3$ . I would like to see if this map is equal area, conformal, etc, and in general just evaluate what the metric looks like at some arbitrary point. I haven't studied this stuff formally, but I would guess that the method would be something like this. Given that I have some function $f: (\lambda, \phi) \mapsto (x, y, z)$ , I can get the Jacobian matrix of partial derivatives. This will give two vectors which span a plane tangent to the surface at that point. Then I would guess that, for instance, we can: See if these vectors are orthogonal and have the same norm. If this is true everywhere, then the map is conformal. Compute the area of the parallelogram formed by the two vectors, and see if it has the same area everywhere. If so, the map is equal area. However I don't think this is exactly correct, because if we differentiate with respect to the coordinate basis of latitudes and longitudes on the sphere, what we're really doing is mapping from the equirectangular projection to $\Bbb R^3$ , and the equirectangular projection is neither a conformal nor equal-area map from the sphere to begin with. So some kind of adjustment term will surely be necessary, but I am hoping I have the correct basic idea with the Jacobian. How does this work?","Suppose I want to map from the sphere to some other surface that is embedded into , which we will treat as an embedding into . In this situation, suppose I already have some formula to map from latitudes and longitudes, denoted to coordinates in . I would like to see if this map is equal area, conformal, etc, and in general just evaluate what the metric looks like at some arbitrary point. I haven't studied this stuff formally, but I would guess that the method would be something like this. Given that I have some function , I can get the Jacobian matrix of partial derivatives. This will give two vectors which span a plane tangent to the surface at that point. Then I would guess that, for instance, we can: See if these vectors are orthogonal and have the same norm. If this is true everywhere, then the map is conformal. Compute the area of the parallelogram formed by the two vectors, and see if it has the same area everywhere. If so, the map is equal area. However I don't think this is exactly correct, because if we differentiate with respect to the coordinate basis of latitudes and longitudes on the sphere, what we're really doing is mapping from the equirectangular projection to , and the equirectangular projection is neither a conformal nor equal-area map from the sphere to begin with. So some kind of adjustment term will surely be necessary, but I am hoping I have the correct basic idea with the Jacobian. How does this work?","\Bbb R^3 \Bbb R^3 (\lambda, \phi) (x, y, z) \Bbb R^3 f: (\lambda, \phi) \mapsto (x, y, z) \Bbb R^3","['geometry', 'multivariable-calculus', 'differential-geometry', 'jacobian', 'cartography']"
83,"Show by divergence theorem $\int_SF.n\, dS=4\pi$ where $F=(x-z)i + (x^3+yz)j - 3xy^2k$",Show by divergence theorem  where,"\int_SF.n\, dS=4\pi F=(x-z)i + (x^3+yz)j - 3xy^2k","Show by divergence theorem $$\int_SF.n\, dS=4\pi$$ where $F=(x-z)i + (x^3+yz)j - 3xy^2k$ and S is the surface of the cone $z=2-\sqrt{x^2+y^2}$ above the xy-plane. $div\,F=1+z$ . $$\int_V\,div\,F\,dV=\int_V(1+z)\,dV$$ Edit: Using cylindrical co-ordinates, $x= r \cos\theta; y=r \sin\theta; z=z$ , $z \in [0,2-r], \theta\in [0,2\pi], r\in [0,2]$ , we have $$\int_{r=0}^{2}\!\int_{\theta=0}^{2\,\pi}\!\int_{z=0}^{2-r}\!(1+z)\,{\rm d}z \,{\rm d}\theta\,{\rm d}r=\frac{20\pi}{3} $$ Ans would be $4\pi$ .....! Where is the mistake? Please suggest parametrization for the inverted cone and correct the limits of integration.","Show by divergence theorem where and S is the surface of the cone above the xy-plane. . Edit: Using cylindrical co-ordinates, , , we have Ans would be .....! Where is the mistake? Please suggest parametrization for the inverted cone and correct the limits of integration.","\int_SF.n\, dS=4\pi F=(x-z)i + (x^3+yz)j - 3xy^2k z=2-\sqrt{x^2+y^2} div\,F=1+z \int_V\,div\,F\,dV=\int_V(1+z)\,dV x= r \cos\theta;
y=r \sin\theta; z=z z \in [0,2-r], \theta\in [0,2\pi], r\in [0,2] \int_{r=0}^{2}\!\int_{\theta=0}^{2\,\pi}\!\int_{z=0}^{2-r}\!(1+z)\,{\rm d}z
\,{\rm d}\theta\,{\rm d}r=\frac{20\pi}{3}
 4\pi","['multivariable-calculus', 'divergence-theorem']"
84,Difficulty with verifying Stokes' theorem for a given domain and a specific differential $(n-1)$-form,Difficulty with verifying Stokes' theorem for a given domain and a specific differential -form,(n-1),"Let $f:\Bbb R^{n-1}\to\Bbb R$ be an everywhere positive $C^\infty$ function and let $U$ be a bounded open subset of $\Bbb R^{n-1}.$ Verify directly that Stokes' theorem is true if $D$ is the domain defined by $$0<x_n<f(x_1,\ldots,x_{n-1}),\quad (x_1,\ldots,x_{n-1})\in U$$ and $\mu$ an $(n-1)$ -form $$\varphi(x_1,\ldots,x_n)dx_1\wedge\ldots\wedge dx_{n-1},$$ where $\varphi\in C^\infty(\Bbb R^n;\Bbb R).$ Source: Differential Forms, page $132$ My attempt: First, $\begin{aligned}d\mu(x_1,\ldots,x_n)&=d\varphi(x_1,\ldots,x_n) dx_1\wedge\ldots\wedge dx_{n-1}\\&=\partial_n\varphi(x_1,\ldots,x_n)dx_n\wedge dx_1\wedge\ldots\wedge dx_{n-1}.\end{aligned}$ I need to verify that $$\int_Dd\mu=\int_{\partial D}\mu.$$ Since $\partial D$ is of Jordan-measure $0,$ I integrated over the closure $\overline D$ and applied Fubini's theorem: $$\begin{aligned}\int_Dd\mu&=\int_{\overline D}d\mu\\&=\int_{\overline U}\int_0^{f(x_1,\ldots,x_{n-1})}\partial_n\varphi(x_1\ldots,x_n)dx_ndx_1\ldots dx_{n-1}\\&=\int_{\overline U}(\varphi(x_1,\ldots,x_{n-1},f(x_1,\ldots,x_{n-1}))-\varphi(x_1,\ldots,x_{n-1},0))dx_1\ldots dx_{n-1}.\boldsymbol{(1)}\end{aligned}$$ On the other hand, (I believe) $$\begin{aligned}&\color{white}=\partial D\\&=\partial\{(x,y)\in\Bbb R^{n-1}\times\Bbb R\mid x\in U, 0<y<f(x)\}\\&=\overline U\times f(\overline U)\cup\overline U\times\{0\}\cup V,\end{aligned}$$ where $$V=\partial U\times\{y\in\Bbb R\mid 0\le y\le f(x),x\in\partial U\},$$ but, I think we should be able to parametrize $V\subset\Bbb R^n$ in some coordinates with $n-1$ conditions so that one of the differentials $dx_1,\ldots,dx_{n-1}$ turns into $0$ when pulled-back in the integral of the differential $(n-1)$ -form $\mu,$ as in the proof of Stokes' theorem for the unit cube, where one coordinate on each edge is constant. However, I got a bit confused. Question: How should I proceed? If $\Phi_{\overline U\times\{0\}}$ and $\Phi_{\overline U\times f(\overline U)}$ are parametrizations of $\overline U\times\{0\}$ and $\overline U\times f(\overline U)$ respectively s. t. the normal to $\overline U\times\{0\}$ has a negative $x_n$ component, and the normal to $\overline U\times f(\overline U)$ has a positive $x_n$ component, I'm going to get $\boldsymbol{(1)}.$","Let be an everywhere positive function and let be a bounded open subset of Verify directly that Stokes' theorem is true if is the domain defined by and an -form where Source: Differential Forms, page My attempt: First, I need to verify that Since is of Jordan-measure I integrated over the closure and applied Fubini's theorem: On the other hand, (I believe) where but, I think we should be able to parametrize in some coordinates with conditions so that one of the differentials turns into when pulled-back in the integral of the differential -form as in the proof of Stokes' theorem for the unit cube, where one coordinate on each edge is constant. However, I got a bit confused. Question: How should I proceed? If and are parametrizations of and respectively s. t. the normal to has a negative component, and the normal to has a positive component, I'm going to get","f:\Bbb R^{n-1}\to\Bbb R C^\infty U \Bbb R^{n-1}. D 0<x_n<f(x_1,\ldots,x_{n-1}),\quad (x_1,\ldots,x_{n-1})\in U \mu (n-1) \varphi(x_1,\ldots,x_n)dx_1\wedge\ldots\wedge dx_{n-1}, \varphi\in C^\infty(\Bbb R^n;\Bbb R). 132 \begin{aligned}d\mu(x_1,\ldots,x_n)&=d\varphi(x_1,\ldots,x_n) dx_1\wedge\ldots\wedge dx_{n-1}\\&=\partial_n\varphi(x_1,\ldots,x_n)dx_n\wedge dx_1\wedge\ldots\wedge dx_{n-1}.\end{aligned} \int_Dd\mu=\int_{\partial D}\mu. \partial D 0, \overline D \begin{aligned}\int_Dd\mu&=\int_{\overline D}d\mu\\&=\int_{\overline U}\int_0^{f(x_1,\ldots,x_{n-1})}\partial_n\varphi(x_1\ldots,x_n)dx_ndx_1\ldots dx_{n-1}\\&=\int_{\overline U}(\varphi(x_1,\ldots,x_{n-1},f(x_1,\ldots,x_{n-1}))-\varphi(x_1,\ldots,x_{n-1},0))dx_1\ldots dx_{n-1}.\boldsymbol{(1)}\end{aligned} \begin{aligned}&\color{white}=\partial D\\&=\partial\{(x,y)\in\Bbb R^{n-1}\times\Bbb R\mid x\in U, 0<y<f(x)\}\\&=\overline U\times f(\overline U)\cup\overline U\times\{0\}\cup V,\end{aligned} V=\partial U\times\{y\in\Bbb R\mid 0\le y\le f(x),x\in\partial U\}, V\subset\Bbb R^n n-1 dx_1,\ldots,dx_{n-1} 0 (n-1) \mu, \Phi_{\overline U\times\{0\}} \Phi_{\overline U\times f(\overline U)} \overline U\times\{0\} \overline U\times f(\overline U) \overline U\times\{0\} x_n \overline U\times f(\overline U) x_n \boldsymbol{(1)}.","['integration', 'multivariable-calculus', 'differential-forms', 'stokes-theorem']"
85,"$\frac{\partial f(x,y)}{\partial x} = n \cdot \frac{\partial f(x,y)}{\partial y} \implies f(x,y) = g(nx+y)$ for some differentiable function $g$",for some differentiable function,"\frac{\partial f(x,y)}{\partial x} = n \cdot \frac{\partial f(x,y)}{\partial y} \implies f(x,y) = g(nx+y) g","Does $\frac{\partial f(x,y)}{\partial x} = n \cdot \frac{\partial f(x,y)}{\partial y} \implies f(x,y) = g(nx+y)$ for some differentiable function $g$ ? I found the proof here but it uses directional derivatives. Is there a more elementary way to do this? Can we do something like this: Consider some $(a,b)$ and the line $nx+y = na+b$ it belongs to. We will show that every $(x,y)$ on this line maps to the same real number. For brevity, let $na+b = c$ . Let $h(x) = U(x,c-nx)$ . Then \begin{align*}\frac{\partial U(x,y)}{\partial x} = n \cdot \frac{\partial U(x,y)}{\partial y} &\implies \frac{dh(x)}{dx} = n \cdot \frac{dh(x)}{dy} = n \cdot \frac{dh(x)}{d(c-nx)} = \frac{dh(x)}{dx} \cdot (-1)   \\ &\implies h'(x) = 0 \end{align*} This tells us that $h$ is constant as desired. Is this proof correct?","Does for some differentiable function ? I found the proof here but it uses directional derivatives. Is there a more elementary way to do this? Can we do something like this: Consider some and the line it belongs to. We will show that every on this line maps to the same real number. For brevity, let . Let . Then This tells us that is constant as desired. Is this proof correct?","\frac{\partial f(x,y)}{\partial x} = n \cdot \frac{\partial f(x,y)}{\partial y} \implies f(x,y) = g(nx+y) g (a,b) nx+y = na+b (x,y) na+b = c h(x) = U(x,c-nx) \begin{align*}\frac{\partial U(x,y)}{\partial x} = n \cdot \frac{\partial U(x,y)}{\partial y} &\implies \frac{dh(x)}{dx} = n \cdot \frac{dh(x)}{dy} = n \cdot \frac{dh(x)}{d(c-nx)} = \frac{dh(x)}{dx} \cdot (-1)  
\\ &\implies h'(x) = 0
\end{align*} h","['calculus', 'multivariable-calculus', 'solution-verification']"
86,Proof of implicit function theorem for a complex function not necessarily holomorphic,Proof of implicit function theorem for a complex function not necessarily holomorphic,,"Let $U,V\subset\mathbb{C}$ be domains, and $F(z,w):U\times V\to\mathbb{C}$ be continuous, and holomorphic in $z$ for every  fixed $w\in V$ . Let $(z_0,w_0)\in U\times V$ be s.t $F(z_0,w_0)=0$ and $\frac{\partial F}{\partial z}(z_0,w_0)=F_1(z_0,w_0)\neq0$ . Let $r>0$ be s.t for every $z_0\neq z\in\overline{\mathbb{D}_r(z_0)}, F(z,w_0)\neq0$ . Prove that there exists $\delta>0$ s.t for every $w\in\mathbb{D}_\delta(w_0)$ there exists a unique $z=g(w)\in\overline{\mathbb{D}_r(z_0)}$ s.t $F(z,w)=0$ . Edit: I now see that I cannot use the implicit function theorem for real functions as $F$ is not necessarily continuiously differentiable as a real function. I tried using the argument theorem but this didnt get me too far. Any help would be appreciated.","Let be domains, and be continuous, and holomorphic in for every  fixed . Let be s.t and . Let be s.t for every . Prove that there exists s.t for every there exists a unique s.t . Edit: I now see that I cannot use the implicit function theorem for real functions as is not necessarily continuiously differentiable as a real function. I tried using the argument theorem but this didnt get me too far. Any help would be appreciated.","U,V\subset\mathbb{C} F(z,w):U\times V\to\mathbb{C} z w\in V (z_0,w_0)\in U\times V F(z_0,w_0)=0 \frac{\partial F}{\partial z}(z_0,w_0)=F_1(z_0,w_0)\neq0 r>0 z_0\neq z\in\overline{\mathbb{D}_r(z_0)}, F(z,w_0)\neq0 \delta>0 w\in\mathbb{D}_\delta(w_0) z=g(w)\in\overline{\mathbb{D}_r(z_0)} F(z,w)=0 F","['real-analysis', 'complex-analysis', 'multivariable-calculus', 'implicit-function-theorem']"
87,Arbitrary Mixed Partial Derivatives,Arbitrary Mixed Partial Derivatives,,"Background: Many textbooks give an example due to Peano of the function $f(x, y) = xy(x^2 - y^2)(x^2 + y^2)^{-1}$ that has mixed partial derivatives at $0$ that are not symmetric. One might wonder how arbitrary mixed partial derivatives can be at a point.  For what choices of an appropriate number of real numbers is there a real-valued function that takes on those values for its partial derivatives at a point? Notation : Let $d\in \mathbb{N}$ and let $[d] = \{1,\ldots,d\}$ .  Let $\mathcal{I}$ be the set of all $n$ -tuples with elements in $[d]$ (that is, $\mathcal{I} = \{(i_{1},\ldots, i_{n}): i_{1},\ldots, i_{n}\in [d]\}$ ).  Let $h:\mathcal{I}\rightarrow \mathbb{R}$ . For which functions $h$ is there a function $f:\mathbb{R}^{d}\rightarrow \mathbb{R}$ so that $$ \frac{\partial}{\partial x_{i_{n}}}\cdots \frac{\partial}{\partial x_{i_{1}}}f = h((i_{1},\ldots, i_{n})) $$ at $x = 0\in \mathbb{R}^{d}$","Background: Many textbooks give an example due to Peano of the function that has mixed partial derivatives at that are not symmetric. One might wonder how arbitrary mixed partial derivatives can be at a point.  For what choices of an appropriate number of real numbers is there a real-valued function that takes on those values for its partial derivatives at a point? Notation : Let and let .  Let be the set of all -tuples with elements in (that is, ).  Let . For which functions is there a function so that at","f(x, y) = xy(x^2 - y^2)(x^2 + y^2)^{-1} 0 d\in \mathbb{N} [d] = \{1,\ldots,d\} \mathcal{I} n [d] \mathcal{I} = \{(i_{1},\ldots, i_{n}): i_{1},\ldots, i_{n}\in [d]\} h:\mathcal{I}\rightarrow \mathbb{R} h f:\mathbb{R}^{d}\rightarrow \mathbb{R} 
\frac{\partial}{\partial x_{i_{n}}}\cdots \frac{\partial}{\partial x_{i_{1}}}f = h((i_{1},\ldots, i_{n}))
 x = 0\in \mathbb{R}^{d}","['real-analysis', 'calculus', 'multivariable-calculus']"
88,Can you prove that this function is bijective?,Can you prove that this function is bijective?,,"Let $T(x,y)=(5x+\sin(y),5y+\arctan(x))$ . Prove T is a bijective map from $\mathbb{R}^2$ to $\mathbb{R}^2$ . This is a problem from a test for undergraduate students, i don't know where to start. EDIT: What i know is that if $f:A\subseteq\mathbb{R}^n \to \mathbb{R}^n$ is $C^1(A)$ where $A$ is open and if $x_0\in A$ is such that $J_f(x_0)$ is not zero then $f$ is locally invertible near $x_0$ . Notice that with $J_f$ i meant the Jacobian of the function $f$","Let . Prove T is a bijective map from to . This is a problem from a test for undergraduate students, i don't know where to start. EDIT: What i know is that if is where is open and if is such that is not zero then is locally invertible near . Notice that with i meant the Jacobian of the function","T(x,y)=(5x+\sin(y),5y+\arctan(x)) \mathbb{R}^2 \mathbb{R}^2 f:A\subseteq\mathbb{R}^n \to \mathbb{R}^n C^1(A) A x_0\in A J_f(x_0) f x_0 J_f f","['real-analysis', 'analysis', 'multivariable-calculus']"
89,Unsure how to solve this integral,Unsure how to solve this integral,,"Let $$F\left(x,y\right)=xi+e^{y^2}j.$$ Evaluate $$\int _CF\cdot dr,$$ where $C$ is the curve $$r\left(t\right)=\frac{1}{t^2+1}i+\left(3+10t^2\cos\left(\frac{\pi t}{2}\right)\right)j,\quad0\le t\le 1.$$ I am having a bit of difficulty with this as I have already tried setting it up as $$\int _0^1F\left(r\left(t\right)\right)\cdot r'\left(t\right)dt.$$ But I get really large equations for the derivative of $r(t)$ which leads to math that I just can't do on paper. I think I can use greens theorem but I'm not really sure where to start.",Let Evaluate where is the curve I am having a bit of difficulty with this as I have already tried setting it up as But I get really large equations for the derivative of which leads to math that I just can't do on paper. I think I can use greens theorem but I'm not really sure where to start.,"F\left(x,y\right)=xi+e^{y^2}j. \int _CF\cdot dr, C r\left(t\right)=\frac{1}{t^2+1}i+\left(3+10t^2\cos\left(\frac{\pi t}{2}\right)\right)j,\quad0\le t\le 1. \int _0^1F\left(r\left(t\right)\right)\cdot r'\left(t\right)dt. r(t)","['integration', 'multivariable-calculus', 'curves']"
90,Abstract exercise about the Implicit Function Theorem,Abstract exercise about the Implicit Function Theorem,,"I am practicing Calculus problems and the following one caught my attention: Let $F:\mathbb{R}^3\to \mathbb{R}$ a $\cal{C}^1$ function, and let $P\in \mathbb{R}^3$ such that $F(P)=0$ , $\dfrac{\partial F}{\partial x}\ne 0$ , $\dfrac{\partial F}{\partial y}\ne 0$ and $\dfrac{\partial F}{\partial z}\ne 0$ . Justify that you can take locally $x = x(y,z)$ , $y = y(x,z)$ and $z = z(x,y)$ around P in such a way that $F(x,y,z)=0$ . Using the notations above, prove that $$ \frac{\partial y}{\partial x}\frac{\partial x}{\partial z}\frac{\partial z}{\partial y} = -1 $$ The first part is clearly an straightforward application of the Implicit Function Theorem, since each of the non-equalities ensures that the corresponding variable can be write as a function of the remaining two. However, I am not sure about the last part. How could I proceed? Thanks in advance.","I am practicing Calculus problems and the following one caught my attention: Let a function, and let such that , , and . Justify that you can take locally , and around P in such a way that . Using the notations above, prove that The first part is clearly an straightforward application of the Implicit Function Theorem, since each of the non-equalities ensures that the corresponding variable can be write as a function of the remaining two. However, I am not sure about the last part. How could I proceed? Thanks in advance.","F:\mathbb{R}^3\to \mathbb{R} \cal{C}^1 P\in \mathbb{R}^3 F(P)=0 \dfrac{\partial F}{\partial x}\ne 0 \dfrac{\partial F}{\partial y}\ne 0 \dfrac{\partial F}{\partial z}\ne 0 x = x(y,z) y = y(x,z) z = z(x,y) F(x,y,z)=0 
\frac{\partial y}{\partial x}\frac{\partial x}{\partial z}\frac{\partial z}{\partial y} = -1
","['real-analysis', 'calculus', 'multivariable-calculus']"
91,Hypervolume of hypersurface over unit hypersphere,Hypervolume of hypersurface over unit hypersphere,,"Given $f:\mathbb{R}^n \to \mathbb{R}^1$ such that $$f(x_1, x_2, x_3,...,x_n)=\frac{1}{(x_1^2+x_2^2+x_3^2+...+x_n^2)^k},$$ what values of $k$ would make the hypervolume of $f(x_1,x_2,x_3,...x_n)$ within a unit $n$ -sphere finite? What values would make it infinite? My attempt thus far has been to integrate it over all $n$ variables, redefining the bounds in hyperspherical coordinates in order to simplify the demonimator into $\text{radius}^{2k}$ .","Given such that what values of would make the hypervolume of within a unit -sphere finite? What values would make it infinite? My attempt thus far has been to integrate it over all variables, redefining the bounds in hyperspherical coordinates in order to simplify the demonimator into .","f:\mathbb{R}^n \to \mathbb{R}^1 f(x_1, x_2, x_3,...,x_n)=\frac{1}{(x_1^2+x_2^2+x_3^2+...+x_n^2)^k}, k f(x_1,x_2,x_3,...x_n) n n \text{radius}^{2k}","['linear-algebra', 'general-topology', 'multivariable-calculus']"
92,Find volume between $x^2+y^2+z^2 \le 2(y+z)$ and $ x^2+y^2 \le z^2$,Find volume between  and,x^2+y^2+z^2 \le 2(y+z)  x^2+y^2 \le z^2,"Recently I've been proposed the next problem at my calculus course. Find the volume of the following corps: $$\left\{(x,y,z) \in \mathbb{R}^3 : \; x^2+y^2+z^2 \le 2(y+z), \; x^2+y^2 \le z^2\right\}$$ On first place, I attempted to plot the functions via Mathematica. This is a screenshot of the plot I've thought to divide the figure in 4 shards like this and this , so I can find the volume of each one separately. (sorry for only having the images, I don't know how to explain it) For the first part, I tried to find the intersection of the sphere ( $x^2+y^2+z^2≤2(y+z)$ ) and the cone ( $x^2+y^2 \le z^2$ ) and try to find a parabolic cylinder that goes trough that intersection and is normal to the XY plane, so I can have $y$ in function of $x$ so I'm able to do the integral, but I'm struggling here, so I hope you can help me.","Recently I've been proposed the next problem at my calculus course. Find the volume of the following corps: On first place, I attempted to plot the functions via Mathematica. This is a screenshot of the plot I've thought to divide the figure in 4 shards like this and this , so I can find the volume of each one separately. (sorry for only having the images, I don't know how to explain it) For the first part, I tried to find the intersection of the sphere ( ) and the cone ( ) and try to find a parabolic cylinder that goes trough that intersection and is normal to the XY plane, so I can have in function of so I'm able to do the integral, but I'm struggling here, so I hope you can help me.","\left\{(x,y,z) \in \mathbb{R}^3 : \; x^2+y^2+z^2 \le 2(y+z), \; x^2+y^2 \le z^2\right\} x^2+y^2+z^2≤2(y+z) x^2+y^2 \le z^2 y x","['calculus', 'integration', 'multivariable-calculus', 'volume']"
93,Investigating continuity for function at given point,Investigating continuity for function at given point,,"Given the function $$ f(x,y)=\begin{cases}\big|1+xy^2\big|^\dfrac{1}{x^2+y^2} & \quad\hfill (x,y)\neq(0,0)\\\\ 1 &\quad\hfill (x,y)=(0,0) \end{cases} $$ investigate whether the function is continuous at $(0,0)$ . Usually, I claim $p\in\mathbb{R}$ such that $y=px$ , $x\rightarrow 0$ , placing them in $\displaystyle \lim_{x\rightarrow0}f(x,y)$ and seeing how that works out. If $\displaystyle \lim_{x\rightarrow0}f(x,y)=f(0,0) $ then (according to how I was taught) the function is continuous at $(0,0)$ . In this specific exercise, I can't seem to solve it using methods I know, i.e the one explained above, or just choosing $y$ to be any variation of $x$ (e.g. $y=\sqrt x $ ). So now I'm at a standstill in my thoughts. Any hints/tips would be really helpful! Thanks!","Given the function investigate whether the function is continuous at . Usually, I claim such that , , placing them in and seeing how that works out. If then (according to how I was taught) the function is continuous at . In this specific exercise, I can't seem to solve it using methods I know, i.e the one explained above, or just choosing to be any variation of (e.g. ). So now I'm at a standstill in my thoughts. Any hints/tips would be really helpful! Thanks!","
f(x,y)=\begin{cases}\big|1+xy^2\big|^\dfrac{1}{x^2+y^2} & \quad\hfill (x,y)\neq(0,0)\\\\ 1 &\quad\hfill (x,y)=(0,0)
\end{cases}
 (0,0) p\in\mathbb{R} y=px x\rightarrow 0 \displaystyle \lim_{x\rightarrow0}f(x,y) \displaystyle \lim_{x\rightarrow0}f(x,y)=f(0,0)  (0,0) y x y=\sqrt x ","['multivariable-calculus', 'continuity', 'absolute-value']"
94,Computing the area of a cone section bounded by a plane,Computing the area of a cone section bounded by a plane,,"Compute the area of the section of the $x^2=y^2+z^2$ in the first octant and bounded by the plane $y+z=a>0.$ source: Demidovich, task 2215 My thoughts: I tried applying the following formula: Suppose $Q\subset\Bbb R^k$ is a cube and $\boldsymbol F\in C^1(Q,\Bbb R^n)$ is injective. Then the $k-$ dimensional area of $\boldsymbol F$ is defined as $\nu(\boldsymbol F)=\int_Q\sqrt{\det\nabla F(\boldsymbol u)^T\nabla F(\boldsymbol u)}d\boldsymbol u.$ I think this section, call it $S$ can be written as $\{(x,y,z)\in\Bbb R^3\mid x^2=y^2+z^2\}\cap\bigcup_{v\in[0,1]}\{(x,y,z)\in\Bbb R^2 y+z=av\}.$ I tried parametrizing each of the intersection in the union as follows: $$(x,y,z)=\boldsymbol F(u,v)=(f(y,z),y,z)=\left(\frac{av}{\sqrt 2\sin\left(\frac\pi2u+\frac\pi4\right)},\frac{av}{\sqrt 2\sin\left(\frac\pi2u+\frac\pi4\right)},\frac\pi2u\right).$$ $\sin\left(\frac\pi2+\frac\pi4\right)$ arose from the polar equation of the line $y+z=av,$ which yields $r=\frac{av}{\sin\varphi+\cos\varphi}.$ Now, $$\nabla F(u,v)=\begin{bmatrix} -\frac{av\pi\cos\left(\frac\pi2u+\frac\pi4\right)}{\sqrt 2(1+\sin(\pi u)} & \frac{a}{\sqrt 2\sin\left(\frac\pi2u+\frac\pi4\right)}\\-\frac{av\pi\cos\left(\frac\pi2u+\frac\pi4\right)}{\sqrt 2(1+\sin(\pi u))}& \frac{a}{\sqrt 2\sin\left(\frac\pi2u+\frac\pi4\right)}\\ \frac\pi2 &0\end{bmatrix},i=1,2$$ where the partials in the first column are obtained as $\frac{\partial F_i(u,v)}{\partial u}=-\frac{av\cos\left(\frac\pi2u+\frac\pi4\right)}{\sqrt 2\left(1-\cos\left(\pi u+\frac\pi2\right)\right)}=-\frac{av\pi\cos\left(\frac\pi2u+\frac\pi4\right)}{\sqrt 2(1+\sin(\pi u)}.$ Proceeding in a similar manner, using the formulae $$\sin^2\alpha=\frac{1-\cos(2\alpha)}2\\\cos^2\alpha=\frac{1+\cos(2\alpha)}2\\\cot\alpha=\frac{\sin(2\alpha)}{1-\cos(2\alpha)}$$ I got $$\nabla F(u,v)^T\nabla F(u,v)=\begin{bmatrix}\frac\pi4+a^2v^2\pi^2\frac{1-\sin(\pi u)}{(1+\sin(\pi u))^2}&-\frac{a^2v\pi\sin(\pi u)}{(1+\sin(\pi u))^2}\\-\frac{a^2v\pi\sin(\pi u)}{(1+\sin(\pi u))^2}&\frac{2a}{1+\sin(\pi u)}\end{bmatrix}$$ but the determinant doesn't seem quite operable. Is there any elegant way? Should I sum the length of the curves that are intersections instead?","Compute the area of the section of the in the first octant and bounded by the plane source: Demidovich, task 2215 My thoughts: I tried applying the following formula: Suppose is a cube and is injective. Then the dimensional area of is defined as I think this section, call it can be written as I tried parametrizing each of the intersection in the union as follows: arose from the polar equation of the line which yields Now, where the partials in the first column are obtained as Proceeding in a similar manner, using the formulae I got but the determinant doesn't seem quite operable. Is there any elegant way? Should I sum the length of the curves that are intersections instead?","x^2=y^2+z^2 y+z=a>0. Q\subset\Bbb R^k \boldsymbol F\in C^1(Q,\Bbb R^n) k- \boldsymbol F \nu(\boldsymbol F)=\int_Q\sqrt{\det\nabla F(\boldsymbol u)^T\nabla F(\boldsymbol u)}d\boldsymbol u. S \{(x,y,z)\in\Bbb R^3\mid x^2=y^2+z^2\}\cap\bigcup_{v\in[0,1]}\{(x,y,z)\in\Bbb R^2 y+z=av\}. (x,y,z)=\boldsymbol F(u,v)=(f(y,z),y,z)=\left(\frac{av}{\sqrt 2\sin\left(\frac\pi2u+\frac\pi4\right)},\frac{av}{\sqrt 2\sin\left(\frac\pi2u+\frac\pi4\right)},\frac\pi2u\right). \sin\left(\frac\pi2+\frac\pi4\right) y+z=av, r=\frac{av}{\sin\varphi+\cos\varphi}. \nabla F(u,v)=\begin{bmatrix} -\frac{av\pi\cos\left(\frac\pi2u+\frac\pi4\right)}{\sqrt 2(1+\sin(\pi u)} & \frac{a}{\sqrt 2\sin\left(\frac\pi2u+\frac\pi4\right)}\\-\frac{av\pi\cos\left(\frac\pi2u+\frac\pi4\right)}{\sqrt 2(1+\sin(\pi u))}& \frac{a}{\sqrt 2\sin\left(\frac\pi2u+\frac\pi4\right)}\\ \frac\pi2 &0\end{bmatrix},i=1,2 \frac{\partial F_i(u,v)}{\partial u}=-\frac{av\cos\left(\frac\pi2u+\frac\pi4\right)}{\sqrt 2\left(1-\cos\left(\pi u+\frac\pi2\right)\right)}=-\frac{av\pi\cos\left(\frac\pi2u+\frac\pi4\right)}{\sqrt 2(1+\sin(\pi u)}. \sin^2\alpha=\frac{1-\cos(2\alpha)}2\\\cos^2\alpha=\frac{1+\cos(2\alpha)}2\\\cot\alpha=\frac{\sin(2\alpha)}{1-\cos(2\alpha)} \nabla F(u,v)^T\nabla F(u,v)=\begin{bmatrix}\frac\pi4+a^2v^2\pi^2\frac{1-\sin(\pi u)}{(1+\sin(\pi u))^2}&-\frac{a^2v\pi\sin(\pi u)}{(1+\sin(\pi u))^2}\\-\frac{a^2v\pi\sin(\pi u)}{(1+\sin(\pi u))^2}&\frac{2a}{1+\sin(\pi u)}\end{bmatrix}","['integration', 'multivariable-calculus']"
95,"Proving $F(x,y)=\left(x,y,\int_x^yf(z^2)dz\right)$ is differentiable and computing $\nabla F(x,y)$",Proving  is differentiable and computing,"F(x,y)=\left(x,y,\int_x^yf(z^2)dz\right) \nabla F(x,y)","Let $f:\Bbb R\to\Bbb R$ be a function of the class $C^1$ and let $F:\Bbb R^2\to\Bbb R^3$ be given by $$F(x,y)=\left(x,y,\int_x^yf(z^2)dz\right).$$ Prove $F$ is differentiable and compute $\nabla F(x,y).$ My thoughts: First, $F$ is differentiable if and only if all of its components are, so it remains to show $F_3(x,y)=\int_x^yf(z^2)dz$ is differentiable. I think I should use the following results: $\underline{\boldsymbol{\text{theorem } 11.1:}}$ Let $f:A=[a,b]\times[c,d]\to\Bbb R$ be continuous s. t. $\frac{\partial f}{\partial y}$ exists and is continuous on $A.$ Suppose $F:[c,d]\to\Bbb R$ is given by $$F(y)=\int_a^bf(x,y)dx.$$ Then $F$ is differentiable on $[c,d]$ and $F'(y)=\int_a^b\frac{\partial f}{\partial y}(x,y)dx.$ And its corollary: $\underline{\boldsymbol{\text{corollary }11.2:}}$ Let $f:A=[a,b]\times[c,d]\to\Bbb R$ is continuous s.t. $\frac{\partial f}{\partial y}$ exists and is continuous on $A.$ Let $u,v:[c,d]\to[a,b]$ be of class $C^1$ and suppose $F:[c,d]\to\Bbb R$ is given by $$F(y)=\int_{u(y)}^{v(y)}f(x,y)dx.$$ Then, $F$ is differentiable on $[c,d]$ and $$F'(y)=f(v(y),y)v'(y)-f(u(y),y)u'(y)+\int_{u(y)}^{v(y)}\frac{\partial f}{\partial y}(x,y)dx.$$ Both results have been proven and I got some additional insight into the corollary here . Let $G, H:\Bbb R\to\Bbb R, G(t):=F_3(0,t), H(t):=F_3(t,0).$ We could write: $$\begin{aligned}F_3(x,y)&=\int_x^yf(z^2)dz\\&=\int_0^yf(z^2)dz-\int_0^xf(z^2)\\&=F_3(0,y)-F(0,x)\\&=G(y)-G(x)\end{aligned}$$ and $$\begin{aligned}F_3(x,y)&=\int_x^yf(z^2)dz\\&=\int_x^0f(z^2)+\int_0^yf(z^2)dz\\&=\int_x^0f(z^2)dz-\int_y^0f(z^2)dz\\&=F_3(x,0)-F_3(y,0)\\&=H(x)-H(y)\end{aligned}$$ Since $f\in C^1(\Bbb R),$ so is the composition $f\circ\pi^2,$ where $\pi:(a,z)\mapsto z.$ Therefore, $\frac{\partial (f\circ\pi^2)}{\partial x}$ exists and is continuous, so we can apply the above corollary to functions $G$ and $H.$ I think $$\begin{aligned}\frac{\partial F_3}{\partial y}(x,y)&=G'(y)-G'(x)=f(z^2)\cdot 0-f(z^2)\cdot 0+\int_0^y\underbrace{\frac{\partial (f\circ\pi^2)}{\partial x}}_{=0}-f(z^2)\cdot 0+f(z^2)\cdot 0-\int_0^x\underbrace{\frac{\partial(f\circ\pi^2)}{\partial x}}_{=0}\\&=0\end{aligned}$$ and analogously $$\frac{\partial F_3}{\partial x}(x,y)=H'(x)-H'(y)=0,$$ so $\operatorname{grad} F_3(x,y)=(0,0)$ and $\nabla F(x,y)=\begin{bmatrix}1&0\\0&1\\0&0\end{bmatrix}$ but I'm not so sure about my result. Question: Is there anything wrong and how can we formalize the answer?","Let be a function of the class and let be given by Prove is differentiable and compute My thoughts: First, is differentiable if and only if all of its components are, so it remains to show is differentiable. I think I should use the following results: Let be continuous s. t. exists and is continuous on Suppose is given by Then is differentiable on and And its corollary: Let is continuous s.t. exists and is continuous on Let be of class and suppose is given by Then, is differentiable on and Both results have been proven and I got some additional insight into the corollary here . Let We could write: and Since so is the composition where Therefore, exists and is continuous, so we can apply the above corollary to functions and I think and analogously so and but I'm not so sure about my result. Question: Is there anything wrong and how can we formalize the answer?","f:\Bbb R\to\Bbb R C^1 F:\Bbb R^2\to\Bbb R^3 F(x,y)=\left(x,y,\int_x^yf(z^2)dz\right). F \nabla F(x,y). F F_3(x,y)=\int_x^yf(z^2)dz \underline{\boldsymbol{\text{theorem } 11.1:}} f:A=[a,b]\times[c,d]\to\Bbb R \frac{\partial f}{\partial y} A. F:[c,d]\to\Bbb R F(y)=\int_a^bf(x,y)dx. F [c,d] F'(y)=\int_a^b\frac{\partial f}{\partial y}(x,y)dx. \underline{\boldsymbol{\text{corollary }11.2:}} f:A=[a,b]\times[c,d]\to\Bbb R \frac{\partial f}{\partial y} A. u,v:[c,d]\to[a,b] C^1 F:[c,d]\to\Bbb R F(y)=\int_{u(y)}^{v(y)}f(x,y)dx. F [c,d] F'(y)=f(v(y),y)v'(y)-f(u(y),y)u'(y)+\int_{u(y)}^{v(y)}\frac{\partial f}{\partial y}(x,y)dx. G, H:\Bbb R\to\Bbb R, G(t):=F_3(0,t), H(t):=F_3(t,0). \begin{aligned}F_3(x,y)&=\int_x^yf(z^2)dz\\&=\int_0^yf(z^2)dz-\int_0^xf(z^2)\\&=F_3(0,y)-F(0,x)\\&=G(y)-G(x)\end{aligned} \begin{aligned}F_3(x,y)&=\int_x^yf(z^2)dz\\&=\int_x^0f(z^2)+\int_0^yf(z^2)dz\\&=\int_x^0f(z^2)dz-\int_y^0f(z^2)dz\\&=F_3(x,0)-F_3(y,0)\\&=H(x)-H(y)\end{aligned} f\in C^1(\Bbb R), f\circ\pi^2, \pi:(a,z)\mapsto z. \frac{\partial (f\circ\pi^2)}{\partial x} G H. \begin{aligned}\frac{\partial F_3}{\partial y}(x,y)&=G'(y)-G'(x)=f(z^2)\cdot 0-f(z^2)\cdot 0+\int_0^y\underbrace{\frac{\partial (f\circ\pi^2)}{\partial x}}_{=0}-f(z^2)\cdot 0+f(z^2)\cdot 0-\int_0^x\underbrace{\frac{\partial(f\circ\pi^2)}{\partial x}}_{=0}\\&=0\end{aligned} \frac{\partial F_3}{\partial x}(x,y)=H'(x)-H'(y)=0, \operatorname{grad} F_3(x,y)=(0,0) \nabla F(x,y)=\begin{bmatrix}1&0\\0&1\\0&0\end{bmatrix}","['real-analysis', 'integration', 'multivariable-calculus', 'solution-verification']"
96,Integrate $\iint_D \frac1{(x^2+y^2)^2 }dx dy$ over a region bounded by four circles,Integrate  over a region bounded by four circles,\iint_D \frac1{(x^2+y^2)^2 }dx dy,"I'm thinking of doing the substitution $x = \frac{u}{u^2 + v^2}, y = \frac{v}{u^2 + v^2},$ but I'm not sure how to exactly compute the range of values $u$ and $v$ take. Clearly, $u=\frac{x}{x^2+y^2}$ and $v = \frac{y}{x^2 + y^2}.$ The Jacobian of the result is $-\frac{1}{u^2 + v^2}$ , and so the resulting integral equals $\iint_{D'} dudv,$ where $D'$ is the region of possible values for the pairs $(u,v)$ . In the circle $x^2 + y^2 - 2x= 0$ , for instance, we have the point $(1,0)$ , which corresponds to $u=1, v=0$ . Also, we have the point $(2,0)$ , corresponding to $u= \frac{1}2, v = 0$ . I can't seem to generalize what values u and v can take on.","I'm thinking of doing the substitution but I'm not sure how to exactly compute the range of values and take. Clearly, and The Jacobian of the result is , and so the resulting integral equals where is the region of possible values for the pairs . In the circle , for instance, we have the point , which corresponds to . Also, we have the point , corresponding to . I can't seem to generalize what values u and v can take on.","x = \frac{u}{u^2 + v^2}, y = \frac{v}{u^2 + v^2}, u v u=\frac{x}{x^2+y^2} v = \frac{y}{x^2 + y^2}. -\frac{1}{u^2 + v^2} \iint_{D'} dudv, D' (u,v) x^2 + y^2 - 2x= 0 (1,0) u=1, v=0 (2,0) u= \frac{1}2, v = 0","['real-analysis', 'calculus', 'integration', 'geometry', 'multivariable-calculus']"
97,Understanding two steps in the proof of Armijo's Convergence Theorem,Understanding two steps in the proof of Armijo's Convergence Theorem,,"I was reading this paper in which Armijo proves his convergence theorem, and I struggle understanding some of the steps in the proof. The following questions regard the first theorem in the paper, which statement begins at the bottom of page 1. The paper begins by defining four conditions that the functions the author is working with must satisfy. Important for our purposes is the third condition: Let $f$ be a real-valued function defined and continuous everywhere on $\mathbb{R}^n$ and bounded below. For a fixed $x_0$ define $S(x_0)=\{x:f(x)\leq f(x_0)\}$ . The function $f$ satisfies condition III at $x_0$ if $f\in C^1$ on $S(x_0)$ and $\nabla f(x)$ is Lipschitz continuous on $S(x_0)$ , i.e., there exists a Lipschitz constant $K>0$ such that $|\nabla f(y)-\nabla f(x)|\leq K|y-x|$ for every pair $x,y\in S(x_0)$ . I do not think understanding condition IV, which is mentinoned in the theorem, is necessary for my post. In any case all conditions can be found in the first page of the linked paper. Convergence Theorem: assume $f$ is a real valued function defined and continuous everywhere on $\mathbb{R}^n$ , bounded below on $\mathbb{R}^n$ , and that conditions III and IV hold at $x_0$ . If $0<\delta \leq 1/4K$ (here $K$ refers to the Lipschitz constant from condition III), then for any $x\in S(x_0)$ , the set $$(1) \ \ S^*(x,\delta)=\{ x_{\lambda} : x_{\lambda} = x - \lambda \nabla f(x), \lambda > 0, f(x_{\lambda}) - f(x) \leq -\delta |\nabla f(x)|^2\}$$ is a nonempty subset of $S(x_0)$ and any sequence $\{ x_k\}^{\infty}_{k=0}$ such that $x_{k+1}\in S^*(x_k, \delta), k=0,1,2,\ldots$ , converges to the point $x^*$ which minimizes $f$ . Proof. If $x\in S(x_0)$ with $x_{\lambda}=x-\lambda \nabla f(x)$ and $0\leq \lambda \leq 1/K$ , condition III and the mean value theorem imply the inequality $$(2) \ \ \ \ f(x_{\lambda})-f(x)\leq -(\lambda - \lambda^2K)|\nabla f(x)|^2$$ which in turn implies that $x_{\lambda} \in S^*(x,\delta)$ for $$(3) \ \ \ \ \lambda_1\leq\lambda\leq\lambda_2, \ \ \ \lambda_i=\frac{1+(-1)^i\sqrt{1-4\delta K}}{2K}$$ so that $S^*(x,\delta)$ is a nonempty subset of $S(x_0)$ . If $\{ x_k\}^{\infty}_{k=0}$ is any sequence for which $x_{k+1}\in S^*(x_k, \delta)$ , $k=0,1,2,\ldots$ , then $(1)$ implies that sequence $\{ f(x_k)\}^{\infty}_{k=0}$ which is bounded below, is monotone nonincreasing and hence that $|\nabla f(x_k)|\rightarrow 0$ as $k\rightarrow \infty$ . The remainder of the theorem follows from condition IV. My questions: How does Armijo get to the equation $$(2) \ \ \ \ f(x_{\lambda})-f(x)\leq -(\lambda  - \lambda ^2K)|\nabla f(x)|^2 \ \text{?}$$ I suspect the argument goes somewhat like this: by the Mean Value Theorem we have that $$(4) \ \ \ \ |f(x_{\lambda})-f(x)| \leq |\nabla f(x+c(x_{\lambda} - x))||\lambda \nabla f(x)|.$$ Meanwhile condition III gives us $|\nabla f(x_{\lambda})-\nabla f(x)| \leq K|x_{\lambda} - x|$ , or, put differently, $$(5) \ \ \ \ |\nabla f(x_{\lambda})| \leq |\nabla f(x)| + K|\lambda \nabla f(x)|=(1+|\lambda |K)|\nabla f(x)|.$$ Now, if we could assume that $|\nabla f(x+c(x_{\lambda} - x))| \leq |\nabla f(x_{\lambda})|$ , and that $f(x_{\lambda})\leq f(x)$ , then we may combine $(4)$ and $(5)$ to get $$f(x_{\lambda})-f(x) \leq (1+|\lambda |K)|\nabla f(x)||\lambda \nabla f(x)|=(\lambda + \lambda ^2K)|\nabla f(x)|^2$$ which is similar to $(2)$ . Why does the sequence $\{ \nabla f(x_k)\} \rightarrow 0$ as $k \rightarrow \infty$ ?","I was reading this paper in which Armijo proves his convergence theorem, and I struggle understanding some of the steps in the proof. The following questions regard the first theorem in the paper, which statement begins at the bottom of page 1. The paper begins by defining four conditions that the functions the author is working with must satisfy. Important for our purposes is the third condition: Let be a real-valued function defined and continuous everywhere on and bounded below. For a fixed define . The function satisfies condition III at if on and is Lipschitz continuous on , i.e., there exists a Lipschitz constant such that for every pair . I do not think understanding condition IV, which is mentinoned in the theorem, is necessary for my post. In any case all conditions can be found in the first page of the linked paper. Convergence Theorem: assume is a real valued function defined and continuous everywhere on , bounded below on , and that conditions III and IV hold at . If (here refers to the Lipschitz constant from condition III), then for any , the set is a nonempty subset of and any sequence such that , converges to the point which minimizes . Proof. If with and , condition III and the mean value theorem imply the inequality which in turn implies that for so that is a nonempty subset of . If is any sequence for which , , then implies that sequence which is bounded below, is monotone nonincreasing and hence that as . The remainder of the theorem follows from condition IV. My questions: How does Armijo get to the equation I suspect the argument goes somewhat like this: by the Mean Value Theorem we have that Meanwhile condition III gives us , or, put differently, Now, if we could assume that , and that , then we may combine and to get which is similar to . Why does the sequence as ?","f \mathbb{R}^n x_0 S(x_0)=\{x:f(x)\leq f(x_0)\} f x_0 f\in C^1 S(x_0) \nabla f(x) S(x_0) K>0 |\nabla f(y)-\nabla f(x)|\leq K|y-x| x,y\in S(x_0) f \mathbb{R}^n \mathbb{R}^n x_0 0<\delta \leq 1/4K K x\in S(x_0) (1) \ \ S^*(x,\delta)=\{ x_{\lambda} : x_{\lambda} = x - \lambda \nabla f(x), \lambda > 0, f(x_{\lambda}) - f(x) \leq -\delta |\nabla f(x)|^2\} S(x_0) \{ x_k\}^{\infty}_{k=0} x_{k+1}\in S^*(x_k, \delta), k=0,1,2,\ldots x^* f x\in S(x_0) x_{\lambda}=x-\lambda \nabla f(x) 0\leq \lambda \leq 1/K (2) \ \ \ \ f(x_{\lambda})-f(x)\leq -(\lambda - \lambda^2K)|\nabla f(x)|^2 x_{\lambda} \in S^*(x,\delta) (3) \ \ \ \ \lambda_1\leq\lambda\leq\lambda_2, \ \ \ \lambda_i=\frac{1+(-1)^i\sqrt{1-4\delta K}}{2K} S^*(x,\delta) S(x_0) \{ x_k\}^{\infty}_{k=0} x_{k+1}\in S^*(x_k, \delta) k=0,1,2,\ldots (1) \{ f(x_k)\}^{\infty}_{k=0} |\nabla f(x_k)|\rightarrow 0 k\rightarrow \infty (2) \ \ \ \ f(x_{\lambda})-f(x)\leq -(\lambda  - \lambda ^2K)|\nabla f(x)|^2 \ \text{?} (4) \ \ \ \ |f(x_{\lambda})-f(x)| \leq |\nabla f(x+c(x_{\lambda} - x))||\lambda \nabla f(x)|. |\nabla f(x_{\lambda})-\nabla f(x)| \leq K|x_{\lambda} - x| (5) \ \ \ \ |\nabla f(x_{\lambda})| \leq |\nabla f(x)| + K|\lambda \nabla f(x)|=(1+|\lambda |K)|\nabla f(x)|. |\nabla f(x+c(x_{\lambda} - x))| \leq |\nabla f(x_{\lambda})| f(x_{\lambda})\leq f(x) (4) (5) f(x_{\lambda})-f(x) \leq (1+|\lambda |K)|\nabla f(x)||\lambda \nabla f(x)|=(\lambda + \lambda ^2K)|\nabla f(x)|^2 (2) \{ \nabla f(x_k)\} \rightarrow 0 k \rightarrow \infty","['analysis', 'multivariable-calculus', 'proof-explanation', 'gradient-descent']"
98,"How to calculate median, variance and correlation on two-dimensional random variable. [closed]","How to calculate median, variance and correlation on two-dimensional random variable. [closed]",,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Two random variables, X and Y, have the joint density function: $$f(x, y) = \begin{cases} 2 & 0 < x \le y < 1 \\ 0 & ioc\end{cases}$$ Calculate the correlation coefficient between X and Y. I am pretty much stuck because y is an upper limit for x, and x is a bottom limit for y; so calculating medians and such is proving too hard for me. I'd appreciate it if anyone could lend me a hand and teach me how to solve this. If it helps save some time, the marginal equations are $f_x (x) = 2 - 2x$ and $f_y (y) = 2y$ .","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Two random variables, X and Y, have the joint density function: Calculate the correlation coefficient between X and Y. I am pretty much stuck because y is an upper limit for x, and x is a bottom limit for y; so calculating medians and such is proving too hard for me. I'd appreciate it if anyone could lend me a hand and teach me how to solve this. If it helps save some time, the marginal equations are and .","f(x, y) = \begin{cases} 2 & 0 < x \le y < 1 \\ 0 & ioc\end{cases} f_x (x) = 2 - 2x f_y (y) = 2y","['multivariable-calculus', 'variance', 'correlation', 'median', 'multivariate-statistical-analysis']"
99,An exercise on the implicit function theorem,An exercise on the implicit function theorem,,"I am trying to learn the implicit function theorem and this is one exercise about it; I have solved it and would be grateful for any feedback on my solution, thanks. Let $f\begin{pmatrix}x\\ y\\ z\end{pmatrix}=x y^2+\sin(xz)+e^z$ and $\textbf{a}=\begin{bmatrix}1\\ -1\\ 0 \end{bmatrix}$ . (a) Show that the equation $f=2$ defines $z$ as a $\mathcal{C}^1$ function $z=\phi\begin{pmatrix}x\\ y\end{pmatrix}$ near $\textbf{a}.$ (b) Find $\frac{\partial\phi}{\partial x}\begin{pmatrix}1\\-1\end{pmatrix}$ and $\frac{\partial\phi}{\partial y}\begin{pmatrix}1\\-1\end{pmatrix}.$ (c) Find the equation of the tangent plane of the surface $f^{-1}(\{2\})$ at $\mathbf{a}$ in two ways. What I have done: (a) Let $F=f-2$ ; $F$ is a $\mathcal{C}^1$ function, $F(\mathbf{a})=0,\ DF\begin{pmatrix}x\\ y\\ z\end{pmatrix}=\begin{bmatrix}y^2+z\cos(xz) &2xy & x\cos(xz)+e^z\end{bmatrix}$ and $DF(\mathbf{a})=DF\begin{pmatrix}1\\ -1\\ 0\end{pmatrix}=\begin{bmatrix}1 & -2 & 2\end{bmatrix}$ so in particular $\frac{\partial F}{\partial z}(\mathbf{a})=2\neq 0$ thus there exists a neighborhood V of $\begin{pmatrix}1\\-1\end{pmatrix}$ and $W$ of $0$ and a $\mathcal{C}^1$ function $\phi:V\to W$ so that $z\in W\Leftrightarrow z=\phi\begin{pmatrix}x\\ y\end{pmatrix},\ \begin{pmatrix}x\\ y\end{pmatrix}\in V.$ (b) $$\frac{\partial\phi}{\partial x}=-\frac{\frac{\partial F}{\partial x}\begin{pmatrix}1\\ -1\\ 0\end{pmatrix}}{\frac{\partial F}{\partial z}\begin{pmatrix}1\\ -1\\ 0\end{pmatrix}}=-\frac{1}{2}$$ and $$\frac{\partial\phi}{\partial y}=-\frac{\frac{\partial F}{\partial y}\begin{pmatrix}1\\ -1\\ 0\end{pmatrix}}{\frac{\partial F}{\partial z}\begin{pmatrix}1\\ -1\\ 0\end{pmatrix}}=-\frac{-2}{2}=1.$$ (c) Tangent plane: $$\begin{bmatrix}1 & -2 & 2\end{bmatrix} \begin{bmatrix}x-1\\ y+1\\ -z\end{bmatrix}=x-1-2(y+1)-2z=0\Leftrightarrow x-2y-2z=3$$","I am trying to learn the implicit function theorem and this is one exercise about it; I have solved it and would be grateful for any feedback on my solution, thanks. Let and . (a) Show that the equation defines as a function near (b) Find and (c) Find the equation of the tangent plane of the surface at in two ways. What I have done: (a) Let ; is a function, and so in particular thus there exists a neighborhood V of and of and a function so that (b) and (c) Tangent plane:","f\begin{pmatrix}x\\ y\\ z\end{pmatrix}=x y^2+\sin(xz)+e^z \textbf{a}=\begin{bmatrix}1\\ -1\\ 0 \end{bmatrix} f=2 z \mathcal{C}^1 z=\phi\begin{pmatrix}x\\ y\end{pmatrix} \textbf{a}. \frac{\partial\phi}{\partial x}\begin{pmatrix}1\\-1\end{pmatrix} \frac{\partial\phi}{\partial y}\begin{pmatrix}1\\-1\end{pmatrix}. f^{-1}(\{2\}) \mathbf{a} F=f-2 F \mathcal{C}^1 F(\mathbf{a})=0,\ DF\begin{pmatrix}x\\ y\\ z\end{pmatrix}=\begin{bmatrix}y^2+z\cos(xz) &2xy & x\cos(xz)+e^z\end{bmatrix} DF(\mathbf{a})=DF\begin{pmatrix}1\\ -1\\ 0\end{pmatrix}=\begin{bmatrix}1 & -2 & 2\end{bmatrix} \frac{\partial F}{\partial z}(\mathbf{a})=2\neq 0 \begin{pmatrix}1\\-1\end{pmatrix} W 0 \mathcal{C}^1 \phi:V\to W z\in W\Leftrightarrow z=\phi\begin{pmatrix}x\\ y\end{pmatrix},\ \begin{pmatrix}x\\ y\end{pmatrix}\in V. \frac{\partial\phi}{\partial x}=-\frac{\frac{\partial F}{\partial x}\begin{pmatrix}1\\ -1\\ 0\end{pmatrix}}{\frac{\partial F}{\partial z}\begin{pmatrix}1\\ -1\\ 0\end{pmatrix}}=-\frac{1}{2} \frac{\partial\phi}{\partial y}=-\frac{\frac{\partial F}{\partial y}\begin{pmatrix}1\\ -1\\ 0\end{pmatrix}}{\frac{\partial F}{\partial z}\begin{pmatrix}1\\ -1\\ 0\end{pmatrix}}=-\frac{-2}{2}=1. \begin{bmatrix}1 & -2 & 2\end{bmatrix} \begin{bmatrix}x-1\\ y+1\\ -z\end{bmatrix}=x-1-2(y+1)-2z=0\Leftrightarrow x-2y-2z=3","['real-analysis', 'multivariable-calculus', 'implicit-function-theorem']"
