,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"How to work out eigenvalues, eigenfunctions, $M$ and $\delta$ for this problem","How to work out eigenvalues, eigenfunctions,  and  for this problem",M \delta,"Let $$p(x)= \begin{cases} M \in \mathbb{R}, & x \in \left(\frac{1}{2}-\delta,\frac{1}{2}+\delta\right) \\ 1, & x\in \left(0,\frac{1}{2}-\delta)\cup(\frac{1}{2}+\delta,1\right) . \end{cases}$$ An asymptotic expansion for the equation (known as the Elastic Differential Equation); $$y'' + \lambda^{2} p(x) y = 0$$ where $\lambda$ is a variable, and $(a,b)=(0,1)$ is; $$y(x,\lambda)=e^{i\lambda}\int_0^1 \sqrt{p(s)ds}\left[\frac{1}{p(x)^{\frac{1}{4}}}+\frac{1}{\lambda_1}(d(x)+\frac{e_+}{p(x)^{\frac{1}{4}}})\right]+e^{-i\lambda}\int_0^1 \sqrt{p(s)ds}\left[\frac{-1}{p(x)^{\frac{1}{4}}}+\frac{1}{\lambda_1}(d(x)-\frac{e_+}{p(x)^{\frac{1}{4}}})\right]+\mathcal{O}(\frac{1}{\lambda^2})$$ Where $e_+$ is a constant, and; $$d(x)=\frac{i}{2(p(x))^{\frac{1}{4}}}\int_a^x \frac{5(p'(x))^2}{16(p(x))^{\frac{5}{2}}}dx -\frac{i}{2(p(x))^{\frac{1}{4}}}\int_a^x \frac{p''(x)}{4(p(x))^{\frac{3}{2}}}dx$$ Now $$y_1'' + \lambda^2 y_1 = 0, x\in\left(0,\frac{1}{2}-\delta\right)$$ $$y_2'' + \lambda^2 M y_2 = 0, x\in\left(\frac{1}{2}-\delta, \frac{1}{2}+\delta\right)$$ $$y_3'' + \lambda^2 y_3 = 0, x\in\left(\frac{1}{2}+\delta,1\right)$$ $y_1$, $y_2$, $y_3$ have solutions of the form; $$y_1=a_1 e^{i\lambda x} + b_1 e^{-i\lambda x}$$ $$y_2=a_2 e^{i\lambda\sqrt{M}x} + b_2 e^{-i\lambda\sqrt{M}x}$$ $$y_3=a_3 e^{i\lambda x} + b_3 e^{-i\lambda x}$$Initial value and boundary conditions: $$y_1\left(\frac{1}{2}-\delta\right)=y_2\left(\frac{1}{2}-\delta\right)$$ $$y_1'\left(\frac{1}{2}-\delta\right)=y_2'\left(\frac{1}{2}-\delta\right)$$ $$y_2\left(\frac{1}{2}+\delta\right)=y_3\left(\frac{1}{2}+\delta\right)$$ $$y_2'\left(\frac{1}{2}+\delta\right)=y_3'\left(\frac{1}{2}+\delta\right)$$ $$y_1(0)=0$$ $$y_3(1)=0$$ Then using these 6 equations I get the system; $$\begin{bmatrix} g & \frac{1}{g} & -f & -\frac{1}{f} & 0 & 0 \\ i\lambda g & \frac{-i\lambda}{g} & -i\lambda \sqrt{M}f & \frac{i\lambda \sqrt{M}}{f} & 0 & 0 \\ 0 & 0 & h & \frac{1}{h} & -k & \frac{-1}{k} \\ 0 & 0 & i\lambda\sqrt{M}h & -\frac{i\lambda\sqrt{M}}{h} & -i\lambda k & \frac{i\lambda}{k} \\ 1 & 1 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 1 & 1 \end{bmatrix}.\begin{bmatrix} a_1 \\ b_1 \\ a_2 \\ b_2 \\ a_3 \\ b_3 \end{bmatrix}=0$$ Where; $$f= e^{i\lambda \sqrt{M}\left(\frac{1}{2}-\delta\right)}$$ $$g= e^{i\lambda \left(\frac{1}{2}-\delta\right)}$$ $$h= e^{i\lambda \sqrt{M}\left(\frac{1}{2}+\delta\right)}$$ $$k= e^{i\lambda \left(\frac{1}{2}+\delta\right)}$$ What I am trying to do; Deriving formulae for $M$ and $\delta$, then hence obtain eigenvalues and eigenfunctions for $y(x,\lambda)$. I have done work on this, and will provide what I have done in the future via updates to this question; Attempt 1); Simplifying the first matrix using Gaussian elimination, then have 6 equations; $a_1=0$, $b_1=0$,  $a_2=0$, $b_2=0$, $a_3=0$, $b_3=0$ Hence $y_1 = y_2 = y_3 =0$, but no apparent useful results to me for $M$ and $\delta$. Attempt 2); --To be updated-- Using the fact that the determinant of the 1st matrix is equal to 0 for the system to have solutions, I have then expanded this determinant to give the following equation; $$-8\sqrt{M}+(1-\sqrt{M})\frac{fg}{hk}+(1+2\sqrt{M}-M)\frac{hk}{fg}+(1+2\sqrt{M}+M)(\frac{fk}{gh}+\frac{gh}{fk})=0$$ This then simplifies to; --To be updated-- $$a^{2}s^{4a\delta}-a^{2}s^{2\delta(2a-2)}-a^{2}s^{2\delta(a+1)}-8(a+1)s^{2a\delta}+(2-a)=0$$ where $M \ge 0 \in \Bbb R$, $a=\sqrt{M}+1 \in \Bbb R$, $s=e^{i\lambda} \in \Bbb C$, $a \ge 1$, $\delta \in \Bbb R$. (I have another question where I ask for help on this equation.) Attempt 3) Using the fact that the determinant of the characteristic matrix is equal to 0 for eigenvalues; --To be updated-- $$\det\begin{bmatrix} f-X & \frac{1}{f} & -g & -\frac{1}{g} \\ i\lambda \sqrt{M}f & \frac{-i\lambda \sqrt{M}}{f}-X & -i\lambda g & \frac{i\lambda}{g} \\ h & \frac{1}{h} & -k-X & -\frac{1}{k} \\ i\lambda\sqrt{M}h & -\frac{i\lambda\sqrt{M}}{h} & -i\lambda k & \frac{i\lambda}{k}-X \end{bmatrix}=0$$ where $X$ is an eigenvalue. I have then got a cubic equation in $X$ , from simplifying this; $$a'X^3 + a''X^2 + a^{(3)}X + a^{(4)} + a^{(5)} + a^{(6)}=0 $$ The coefficients of this cubic equation, $a^{(n)}$ though are complex functions of $\lambda, M, f, g, h$ and $k$. Questions / help requested; 1) Derivation of the eigenvalues and eigenvectors, what this means for $M$ and $\delta$, and an example illustrating this. 2) Derivation of formulae for $M$ and $\delta$. 3) What happens to $M$ & $\delta$ as $\lambda$ tends to infinity? 4) I am not sure which method - Attempt 1 / Attempt 2 / Attempt 3 is more fruitful. Am I approaching this in a sound way? What would be better method(s) if any?","Let $$p(x)= \begin{cases} M \in \mathbb{R}, & x \in \left(\frac{1}{2}-\delta,\frac{1}{2}+\delta\right) \\ 1, & x\in \left(0,\frac{1}{2}-\delta)\cup(\frac{1}{2}+\delta,1\right) . \end{cases}$$ An asymptotic expansion for the equation (known as the Elastic Differential Equation); $$y'' + \lambda^{2} p(x) y = 0$$ where $\lambda$ is a variable, and $(a,b)=(0,1)$ is; $$y(x,\lambda)=e^{i\lambda}\int_0^1 \sqrt{p(s)ds}\left[\frac{1}{p(x)^{\frac{1}{4}}}+\frac{1}{\lambda_1}(d(x)+\frac{e_+}{p(x)^{\frac{1}{4}}})\right]+e^{-i\lambda}\int_0^1 \sqrt{p(s)ds}\left[\frac{-1}{p(x)^{\frac{1}{4}}}+\frac{1}{\lambda_1}(d(x)-\frac{e_+}{p(x)^{\frac{1}{4}}})\right]+\mathcal{O}(\frac{1}{\lambda^2})$$ Where $e_+$ is a constant, and; $$d(x)=\frac{i}{2(p(x))^{\frac{1}{4}}}\int_a^x \frac{5(p'(x))^2}{16(p(x))^{\frac{5}{2}}}dx -\frac{i}{2(p(x))^{\frac{1}{4}}}\int_a^x \frac{p''(x)}{4(p(x))^{\frac{3}{2}}}dx$$ Now $$y_1'' + \lambda^2 y_1 = 0, x\in\left(0,\frac{1}{2}-\delta\right)$$ $$y_2'' + \lambda^2 M y_2 = 0, x\in\left(\frac{1}{2}-\delta, \frac{1}{2}+\delta\right)$$ $$y_3'' + \lambda^2 y_3 = 0, x\in\left(\frac{1}{2}+\delta,1\right)$$ $y_1$, $y_2$, $y_3$ have solutions of the form; $$y_1=a_1 e^{i\lambda x} + b_1 e^{-i\lambda x}$$ $$y_2=a_2 e^{i\lambda\sqrt{M}x} + b_2 e^{-i\lambda\sqrt{M}x}$$ $$y_3=a_3 e^{i\lambda x} + b_3 e^{-i\lambda x}$$Initial value and boundary conditions: $$y_1\left(\frac{1}{2}-\delta\right)=y_2\left(\frac{1}{2}-\delta\right)$$ $$y_1'\left(\frac{1}{2}-\delta\right)=y_2'\left(\frac{1}{2}-\delta\right)$$ $$y_2\left(\frac{1}{2}+\delta\right)=y_3\left(\frac{1}{2}+\delta\right)$$ $$y_2'\left(\frac{1}{2}+\delta\right)=y_3'\left(\frac{1}{2}+\delta\right)$$ $$y_1(0)=0$$ $$y_3(1)=0$$ Then using these 6 equations I get the system; $$\begin{bmatrix} g & \frac{1}{g} & -f & -\frac{1}{f} & 0 & 0 \\ i\lambda g & \frac{-i\lambda}{g} & -i\lambda \sqrt{M}f & \frac{i\lambda \sqrt{M}}{f} & 0 & 0 \\ 0 & 0 & h & \frac{1}{h} & -k & \frac{-1}{k} \\ 0 & 0 & i\lambda\sqrt{M}h & -\frac{i\lambda\sqrt{M}}{h} & -i\lambda k & \frac{i\lambda}{k} \\ 1 & 1 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 1 & 1 \end{bmatrix}.\begin{bmatrix} a_1 \\ b_1 \\ a_2 \\ b_2 \\ a_3 \\ b_3 \end{bmatrix}=0$$ Where; $$f= e^{i\lambda \sqrt{M}\left(\frac{1}{2}-\delta\right)}$$ $$g= e^{i\lambda \left(\frac{1}{2}-\delta\right)}$$ $$h= e^{i\lambda \sqrt{M}\left(\frac{1}{2}+\delta\right)}$$ $$k= e^{i\lambda \left(\frac{1}{2}+\delta\right)}$$ What I am trying to do; Deriving formulae for $M$ and $\delta$, then hence obtain eigenvalues and eigenfunctions for $y(x,\lambda)$. I have done work on this, and will provide what I have done in the future via updates to this question; Attempt 1); Simplifying the first matrix using Gaussian elimination, then have 6 equations; $a_1=0$, $b_1=0$,  $a_2=0$, $b_2=0$, $a_3=0$, $b_3=0$ Hence $y_1 = y_2 = y_3 =0$, but no apparent useful results to me for $M$ and $\delta$. Attempt 2); --To be updated-- Using the fact that the determinant of the 1st matrix is equal to 0 for the system to have solutions, I have then expanded this determinant to give the following equation; $$-8\sqrt{M}+(1-\sqrt{M})\frac{fg}{hk}+(1+2\sqrt{M}-M)\frac{hk}{fg}+(1+2\sqrt{M}+M)(\frac{fk}{gh}+\frac{gh}{fk})=0$$ This then simplifies to; --To be updated-- $$a^{2}s^{4a\delta}-a^{2}s^{2\delta(2a-2)}-a^{2}s^{2\delta(a+1)}-8(a+1)s^{2a\delta}+(2-a)=0$$ where $M \ge 0 \in \Bbb R$, $a=\sqrt{M}+1 \in \Bbb R$, $s=e^{i\lambda} \in \Bbb C$, $a \ge 1$, $\delta \in \Bbb R$. (I have another question where I ask for help on this equation.) Attempt 3) Using the fact that the determinant of the characteristic matrix is equal to 0 for eigenvalues; --To be updated-- $$\det\begin{bmatrix} f-X & \frac{1}{f} & -g & -\frac{1}{g} \\ i\lambda \sqrt{M}f & \frac{-i\lambda \sqrt{M}}{f}-X & -i\lambda g & \frac{i\lambda}{g} \\ h & \frac{1}{h} & -k-X & -\frac{1}{k} \\ i\lambda\sqrt{M}h & -\frac{i\lambda\sqrt{M}}{h} & -i\lambda k & \frac{i\lambda}{k}-X \end{bmatrix}=0$$ where $X$ is an eigenvalue. I have then got a cubic equation in $X$ , from simplifying this; $$a'X^3 + a''X^2 + a^{(3)}X + a^{(4)} + a^{(5)} + a^{(6)}=0 $$ The coefficients of this cubic equation, $a^{(n)}$ though are complex functions of $\lambda, M, f, g, h$ and $k$. Questions / help requested; 1) Derivation of the eigenvalues and eigenvectors, what this means for $M$ and $\delta$, and an example illustrating this. 2) Derivation of formulae for $M$ and $\delta$. 3) What happens to $M$ & $\delta$ as $\lambda$ tends to infinity? 4) I am not sure which method - Attempt 1 / Attempt 2 / Attempt 3 is more fruitful. Am I approaching this in a sound way? What would be better method(s) if any?",,"['matrices', 'ordinary-differential-equations', 'eigenvalues-eigenvectors', 'asymptotics', 'eigenfunctions']"
1,When do I divide by N in SIR and SEIR models?,When do I divide by N in SIR and SEIR models?,,"My question is kind of a duplicate of this I understand every separate term in the SIR model differential equations: $ \frac{dS}{dt}=-\beta S I$ $\frac{dI}{dt} = \beta S I - \gamma I$ $\frac{dR}{dt}=\gamma I $ And I have come across and not understood these: $\frac{dS}{dt}=-\frac{\beta}{N}{S}{I}{N}$ $\frac{dI}{dt} = \frac{\beta}{N}{S}I - \gamma I$ $\frac{dR}{dt}=\gamma I$ I couldn't understand the answer the the question I linked and decided that it wasn't very important. While researching, I noticed that many pages, even wikipedia use the two interchangeably. When I tried to use the model to graph the 2014 EVD epidemic, I also ended up using the two version interchangeably, but it turned out that they require very different $\beta$ and $R_0$. What does the N mean and what do it do? How does the N affect $\beta$ and $R_0$? Thanks in advance.","My question is kind of a duplicate of this I understand every separate term in the SIR model differential equations: $ \frac{dS}{dt}=-\beta S I$ $\frac{dI}{dt} = \beta S I - \gamma I$ $\frac{dR}{dt}=\gamma I $ And I have come across and not understood these: $\frac{dS}{dt}=-\frac{\beta}{N}{S}{I}{N}$ $\frac{dI}{dt} = \frac{\beta}{N}{S}I - \gamma I$ $\frac{dR}{dt}=\gamma I$ I couldn't understand the answer the the question I linked and decided that it wasn't very important. While researching, I noticed that many pages, even wikipedia use the two interchangeably. When I tried to use the model to graph the 2014 EVD epidemic, I also ended up using the two version interchangeably, but it turned out that they require very different $\beta$ and $R_0$. What does the N mean and what do it do? How does the N affect $\beta$ and $R_0$? Thanks in advance.",,"['ordinary-differential-equations', 'mathematical-modeling']"
2,"Laguerre polynomials, how to compute the integral?","Laguerre polynomials, how to compute the integral?",,"The Laguerre Polynomials $L_n = L_n(x)$, $n=0,1,2$ satisfy the Laguerre differential equation, $$xL''+(1-x)L' +nL=0\quad(3)$$ Show that the Laguerre differential equation $(3)$ can be written in the form $$(x(e^{-x}\cdot L'))' +n(e^{-x}\cdot L)=0$$ I used the product rule to compute $$L'(e^{-x}) + x((-e^x)\cdot L' + L''e^{-x}) + ne^{-x}\cdot L$$ which gives you equations $(3)$ However I cannot compute the next part. For $n\neq m$, show that $$\int_{0}^\infty L_nL_me^{-x}dx=0.$$ I apologise for not formatting as I really struggle with this.","The Laguerre Polynomials $L_n = L_n(x)$, $n=0,1,2$ satisfy the Laguerre differential equation, $$xL''+(1-x)L' +nL=0\quad(3)$$ Show that the Laguerre differential equation $(3)$ can be written in the form $$(x(e^{-x}\cdot L'))' +n(e^{-x}\cdot L)=0$$ I used the product rule to compute $$L'(e^{-x}) + x((-e^x)\cdot L' + L''e^{-x}) + ne^{-x}\cdot L$$ which gives you equations $(3)$ However I cannot compute the next part. For $n\neq m$, show that $$\int_{0}^\infty L_nL_me^{-x}dx=0.$$ I apologise for not formatting as I really struggle with this.",,['ordinary-differential-equations']
3,Second series solution to Differential Equation,Second series solution to Differential Equation,,"Find a series solution about the point $x=0$ for $\gamma\ne0$ of the   differential equation $x^2y''-xy'+(1-\gamma x)y=0$ and write down the   form of a second independent solution. So far I have identified $x=0$ as a regular singular point and so multipled through by $x$ to obtain an equidimenisonal equation. Let $y=\sum^{\infty}_{n=0} a_nx^{n+\sigma}$ and eventually arrived at $a_n=\frac{\gamma}{n^2}a_{n-1}$ and so $y=a_0(1+\frac{\gamma}{1^2}x+\frac{\gamma^2}{1^22^2}x^2+...+\frac{\gamma^n}{(n!)^2}x^n+...)$. Firstly I would ask if there is any way to simplify this sum, I don't recognise it and I can't see how to evaluate the series. Secondly, I don't know how to easily find the second solution, the wording 'write down' suggests that no additional working is required, I had a similar problem on an earlier question with finding a second solution, so this is the more important one to be answered. Thank you.","Find a series solution about the point $x=0$ for $\gamma\ne0$ of the   differential equation $x^2y''-xy'+(1-\gamma x)y=0$ and write down the   form of a second independent solution. So far I have identified $x=0$ as a regular singular point and so multipled through by $x$ to obtain an equidimenisonal equation. Let $y=\sum^{\infty}_{n=0} a_nx^{n+\sigma}$ and eventually arrived at $a_n=\frac{\gamma}{n^2}a_{n-1}$ and so $y=a_0(1+\frac{\gamma}{1^2}x+\frac{\gamma^2}{1^22^2}x^2+...+\frac{\gamma^n}{(n!)^2}x^n+...)$. Firstly I would ask if there is any way to simplify this sum, I don't recognise it and I can't see how to evaluate the series. Secondly, I don't know how to easily find the second solution, the wording 'write down' suggests that no additional working is required, I had a similar problem on an earlier question with finding a second solution, so this is the more important one to be answered. Thank you.",,['ordinary-differential-equations']
4,How to solve $y'''+2y''-y'-2y= e^x+x^2$?,How to solve ?,y'''+2y''-y'-2y= e^x+x^2,The equation that needs to be solved is: $$y'''+2y''-y'-2y= e^x+x^2$$ Steps: homogeneous solution and then the particular part.Bbut how do i handle the particular part? Do i need to take them once at a time?,The equation that needs to be solved is: $$y'''+2y''-y'-2y= e^x+x^2$$ Steps: homogeneous solution and then the particular part.Bbut how do i handle the particular part? Do i need to take them once at a time?,,"['calculus', 'ordinary-differential-equations']"
5,Chemical reaction modeled by a differential equation,Chemical reaction modeled by a differential equation,,"I am badly stuck on the following question. Thus, I am asking for some help :) Consider a chemical reaction in which compounds $A$ and $B$ combine to form a third compound $X$. The reaction can be written as $$A + B \xrightarrow{k} X$$ If $2 \, \rm{g}$ of $A$ and $1 \, \rm{g}$ of $B$ are required to produce $3g$ of compound $X$, then the amount of compound $x$ at time $t$ satisfies the differential equation $$ \frac{dx}{dt} = k \left(a - \frac{2}{3}x\right)\left(b - \frac{1}{3}x\right)  $$ where $a$ and $b$ are the amounts of $A$ and $B$ at time $0$ (respectively), and initially none of compound $X$ is present (so $x(0) = 0$). Time is in units of minutes, and $k$ is the reaction rate, per minute per gram. Use separation of variables (and integration by partial fractions) to show that solution can be expressed in the form $$  \ln \left(\frac{a - \frac{2}{3}x(t)}{b - \frac{1}{3}x(t)} \cdot \frac{b}{a}\right) = ct  $$ where the constant c depends on k,a,b. Now suppose that a = 15g, b = 20g and after 10min, 15g of compound X has been formed, find the amount of X after 20 mins. So, first of all, showing this solution can be expressed as second equation, by using separation of variables ( and integration by partial fractions), i got $$  \ln \left(\frac{a - \frac{2}{3}x}{b - \frac{1}{3}x}\right) * \frac{3}{a-2b} = kt + c $$ how can this can be expressed in the form of the second equation ? am I doing something wrong ?, I dont get how I should get  $$ \frac{b}{a} $$ inside the ln... and on the RHS, how kt + c becomes just ct ? and by the second euqation,  """""" where the constant $c$ depends on $k$, $a$ and $b$. Now suppose that a = 15g, b = 20g and after 10min, 15g of compound X has been formed, find the amount of X after 20 mins. """""" how do we find the amount of $X$ after $20$ minutes? Thank you !","I am badly stuck on the following question. Thus, I am asking for some help :) Consider a chemical reaction in which compounds $A$ and $B$ combine to form a third compound $X$. The reaction can be written as $$A + B \xrightarrow{k} X$$ If $2 \, \rm{g}$ of $A$ and $1 \, \rm{g}$ of $B$ are required to produce $3g$ of compound $X$, then the amount of compound $x$ at time $t$ satisfies the differential equation $$ \frac{dx}{dt} = k \left(a - \frac{2}{3}x\right)\left(b - \frac{1}{3}x\right)  $$ where $a$ and $b$ are the amounts of $A$ and $B$ at time $0$ (respectively), and initially none of compound $X$ is present (so $x(0) = 0$). Time is in units of minutes, and $k$ is the reaction rate, per minute per gram. Use separation of variables (and integration by partial fractions) to show that solution can be expressed in the form $$  \ln \left(\frac{a - \frac{2}{3}x(t)}{b - \frac{1}{3}x(t)} \cdot \frac{b}{a}\right) = ct  $$ where the constant c depends on k,a,b. Now suppose that a = 15g, b = 20g and after 10min, 15g of compound X has been formed, find the amount of X after 20 mins. So, first of all, showing this solution can be expressed as second equation, by using separation of variables ( and integration by partial fractions), i got $$  \ln \left(\frac{a - \frac{2}{3}x}{b - \frac{1}{3}x}\right) * \frac{3}{a-2b} = kt + c $$ how can this can be expressed in the form of the second equation ? am I doing something wrong ?, I dont get how I should get  $$ \frac{b}{a} $$ inside the ln... and on the RHS, how kt + c becomes just ct ? and by the second euqation,  """""" where the constant $c$ depends on $k$, $a$ and $b$. Now suppose that a = 15g, b = 20g and after 10min, 15g of compound X has been formed, find the amount of X after 20 mins. """""" how do we find the amount of $X$ after $20$ minutes? Thank you !",,"['ordinary-differential-equations', 'dynamical-systems', 'chemistry']"
6,Solving $ \frac {dy}{dx} = \sqrt{y} x\cos(x) $ with $y(0) = 1$,Solving  with, \frac {dy}{dx} = \sqrt{y} x\cos(x)  y(0) = 1,"I was helping someone work this problem out for an online course and I thought it'd be pretty easy since it's a first order separable DE. I ended up with $$ y = \frac {(x\sin(x) + \cos(x) + 1)^2}{4}    $$ and when I plug this answer back into the original DE, it works out, yet the online system doesn't accept it! Am I missing something really obvious?","I was helping someone work this problem out for an online course and I thought it'd be pretty easy since it's a first order separable DE. I ended up with $$ y = \frac {(x\sin(x) + \cos(x) + 1)^2}{4}    $$ and when I plug this answer back into the original DE, it works out, yet the online system doesn't accept it! Am I missing something really obvious?",,"['calculus', 'ordinary-differential-equations']"
7,Solution of $f(x)^2\frac{d^2}{dx^2}f(x)=x$,Solution of,f(x)^2\frac{d^2}{dx^2}f(x)=x,"I am stuck in finding the solution of this apparently simple differential equation: $$f(x)^2\dfrac{d^2}{dx^2}f(x)=x$$ with$f(0)=a$ and $f(0)'=b$ Using Maple the solution seems to be a combination of integrals of Airy functions, but not in a closed form. Is it possible to find something better than Maple solution? Thanks in advance.","I am stuck in finding the solution of this apparently simple differential equation: $$f(x)^2\dfrac{d^2}{dx^2}f(x)=x$$ with$f(0)=a$ and $f(0)'=b$ Using Maple the solution seems to be a combination of integrals of Airy functions, but not in a closed form. Is it possible to find something better than Maple solution? Thanks in advance.",,"['ordinary-differential-equations', 'special-functions']"
8,"How to ""abstractly"" differentiate function, expressed in terms of itself","How to ""abstractly"" differentiate function, expressed in terms of itself",,"I am interested in differential equations of a single variable in the the dependent function. For example  $$y = e^x \rightarrow \frac{dy}{dx} = y$$  $$y = \frac{1}{1-e^{-x}} \rightarrow \frac{dy}{dx} = y(1-y)$$ In general if i'm given $y= f(x)$, How do I find an $F$ such that $y$ satisfies the differential equation $$ \frac{dy}{dx} = F(y)$$ One solution is $$ \frac{dy}{dx} = f'(f^{-1}(y)) $$ Of which the two above are a special case, but how do we characterize the entire set of such  $F$?","I am interested in differential equations of a single variable in the the dependent function. For example  $$y = e^x \rightarrow \frac{dy}{dx} = y$$  $$y = \frac{1}{1-e^{-x}} \rightarrow \frac{dy}{dx} = y(1-y)$$ In general if i'm given $y= f(x)$, How do I find an $F$ such that $y$ satisfies the differential equation $$ \frac{dy}{dx} = F(y)$$ One solution is $$ \frac{dy}{dx} = f'(f^{-1}(y)) $$ Of which the two above are a special case, but how do we characterize the entire set of such  $F$?",,"['calculus', 'ordinary-differential-equations']"
9,solve $x(x^2+y^2)^{-1/2}+yy\prime(x^2+2y^2)=0$,solve,x(x^2+y^2)^{-1/2}+yy\prime(x^2+2y^2)=0,"Help with this excercise.. :) $$x(x^2+y^2)^{-1/2}+yy\prime(x^2+2y^2)=0$$ the book says it is an exact differential equation, but how? $$x(x^2+y^2)^{-1/2}+yy\prime(x^2+2y^2)=0$$ $$x(x^2+y^2)^{-1/2}dx+y(x^2+2y^2)dy=0$$ $M=x(x^2+y^2)^{-1/2}$ $N=y(x^2+2y^2)$ $$\frac{\partial M}{\partial y}=-\frac{xy}{(x^2+y^2)^{3/2}}$$ $$\frac{\partial N}{\partial x}=2xy$$ I cant find the integrating factor,, :(","Help with this excercise.. :) the book says it is an exact differential equation, but how? I cant find the integrating factor,, :(",x(x^2+y^2)^{-1/2}+yy\prime(x^2+2y^2)=0 x(x^2+y^2)^{-1/2}+yy\prime(x^2+2y^2)=0 x(x^2+y^2)^{-1/2}dx+y(x^2+2y^2)dy=0 M=x(x^2+y^2)^{-1/2} N=y(x^2+2y^2) \frac{\partial M}{\partial y}=-\frac{xy}{(x^2+y^2)^{3/2}} \frac{\partial N}{\partial x}=2xy,['ordinary-differential-equations']
10,Differential equation with multiple solutions,Differential equation with multiple solutions,,The below was a question on exam and I solved it like the below.. $$y^{'}=3t^{2}+3t^{2}y$$ $$\frac{dy}{dt}=3t^2(1+y)$$ $$\frac{dy}{1+y}=3t^{2}dt$$ $$\int\frac{dy}{1+y}=\int3t^{2}dt$$ $$\ln|1+y|= t^{3}+C$$ $$e^{\ln|1+y|}= e^{t^{3}+C}$$ $$1+y = e^{t^{3}+C}$$ $$y= e^{t^{3}+C}-1$$ But the answer provided by the professor has two solutions to the problem and I don't understand what I did wrong..and there are no initial value like $y(0)=$ something given. $$y(t) = -1$$ and $$y(t)=2e^{t^3}-1$$ Can someone explain this?,The below was a question on exam and I solved it like the below.. $$y^{'}=3t^{2}+3t^{2}y$$ $$\frac{dy}{dt}=3t^2(1+y)$$ $$\frac{dy}{1+y}=3t^{2}dt$$ $$\int\frac{dy}{1+y}=\int3t^{2}dt$$ $$\ln|1+y|= t^{3}+C$$ $$e^{\ln|1+y|}= e^{t^{3}+C}$$ $$1+y = e^{t^{3}+C}$$ $$y= e^{t^{3}+C}-1$$ But the answer provided by the professor has two solutions to the problem and I don't understand what I did wrong..and there are no initial value like $y(0)=$ something given. $$y(t) = -1$$ and $$y(t)=2e^{t^3}-1$$ Can someone explain this?,,"['calculus', 'ordinary-differential-equations']"
11,Differential Equation Initial Value Problem,Differential Equation Initial Value Problem,,"Here is a pretty standard initial value problem that I'm having a little trouble with. $$(\ln(y))^2\frac{\mathrm{d}y}{\mathrm{d}x}=x^2y$$ Given $y(1)=e^2$, find the constant $C$. So I separated and integrated to get $\frac{(\ln(y))^3}{3}=\frac{x^3}{3}+C$. Multiplying $3$ to both sides yields $(ln(y))^3=x^3+C$. Here's where I struggled a little.  So I take the cube root of both sides to get $\ln y=\sqrt[3]{x^3+C}$ then raised $e$ to both sides to get $y=e^\sqrt[3]{x^3+C}$...? Basically, the $C$ and the $e$ are giving me trouble.  I appreciate any pointers.","Here is a pretty standard initial value problem that I'm having a little trouble with. $$(\ln(y))^2\frac{\mathrm{d}y}{\mathrm{d}x}=x^2y$$ Given $y(1)=e^2$, find the constant $C$. So I separated and integrated to get $\frac{(\ln(y))^3}{3}=\frac{x^3}{3}+C$. Multiplying $3$ to both sides yields $(ln(y))^3=x^3+C$. Here's where I struggled a little.  So I take the cube root of both sides to get $\ln y=\sqrt[3]{x^3+C}$ then raised $e$ to both sides to get $y=e^\sqrt[3]{x^3+C}$...? Basically, the $C$ and the $e$ are giving me trouble.  I appreciate any pointers.",,"['ordinary-differential-equations', 'initial-value-problems']"
12,How to model a checking account with continuous-time compounding?,How to model a checking account with continuous-time compounding?,,"Say you have a bank account in which your invested money yields 3% every year, continuously compounded. Also, you have estimated that you spend $1000 every month to pay your bills, that are withdrawn from this account. Create a differential model for that, find its equilibriums and determine its stability. My problem here is that the \$1000 withdrawal is not continuous on time, it's discrete. The best I could achieve is, if $S(t)$ is the current balance: $\dot S (t) = 0,0025S(t) - 1000$ . I'm using $0,0025$ as the interest rate because it yields 3% every year, so it should yield 0,25% every month. But I'm pretty confident that it's wrong. Any help would be highly appreciated! Thanks!","Say you have a bank account in which your invested money yields 3% every year, continuously compounded. Also, you have estimated that you spend $1000 every month to pay your bills, that are withdrawn from this account. Create a differential model for that, find its equilibriums and determine its stability. My problem here is that the \$1000 withdrawal is not continuous on time, it's discrete. The best I could achieve is, if is the current balance: . I'm using as the interest rate because it yields 3% every year, so it should yield 0,25% every month. But I'm pretty confident that it's wrong. Any help would be highly appreciated! Thanks!","S(t) \dot S (t) = 0,0025S(t) - 1000 0,0025","['ordinary-differential-equations', 'finance', 'stability-in-odes']"
13,Linear ODE with non-constant coefficients,Linear ODE with non-constant coefficients,,"I have encountered some problem in computing the explicit solution for the following ODE: $$x^\prime(t) = (2t-1) x(t)-1, \quad x(0) =: x_{0}$$ The formula that I have used to solve it is: $$\Large{\left[k-\int^{t}b(s)e^{\int^{s}a(u)du}ds\right]e^{\int^{t}a(s)ds}}$$ with $a=(2t-1)$ and $b=-1$. Am I using the correct formula? Thanks","I have encountered some problem in computing the explicit solution for the following ODE: $$x^\prime(t) = (2t-1) x(t)-1, \quad x(0) =: x_{0}$$ The formula that I have used to solve it is: $$\Large{\left[k-\int^{t}b(s)e^{\int^{s}a(u)du}ds\right]e^{\int^{t}a(s)ds}}$$ with $a=(2t-1)$ and $b=-1$. Am I using the correct formula? Thanks",,"['calculus', 'ordinary-differential-equations']"
14,Linear Differential Equation but with piece wise function ( Cant Solve ) [closed],Linear Differential Equation but with piece wise function ( Cant Solve ) [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Find a continuous solution satisfying the DE $$y' + y = f(x),$$ where \begin{align} f(x) &= \begin{cases} 1, &  0 \leq x \leq 1 \\ -1, &  x>1\text{.}\end{cases}\\ y(0)&=1\end{align} I am new to the forum so sorry guys for the format mistakes but really looking for the solution of this desperately & the 2nd question is $$y' + 2xy = f(x),$$ where \begin{align} f(x) &= \begin{cases} x, &  0 \leq x \leq 3 \\ 0, &  x >1\text{.}\end{cases}\\ y(0)&=2\end{align} These questions are in my Book ODE by Zillls 3rd Edition Exercise 2.5 . Question number 57,58 Kindly help me out mates , i will be thankfull","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Find a continuous solution satisfying the DE $$y' + y = f(x),$$ where \begin{align} f(x) &= \begin{cases} 1, &  0 \leq x \leq 1 \\ -1, &  x>1\text{.}\end{cases}\\ y(0)&=1\end{align} I am new to the forum so sorry guys for the format mistakes but really looking for the solution of this desperately & the 2nd question is $$y' + 2xy = f(x),$$ where \begin{align} f(x) &= \begin{cases} x, &  0 \leq x \leq 3 \\ 0, &  x >1\text{.}\end{cases}\\ y(0)&=2\end{align} These questions are in my Book ODE by Zillls 3rd Edition Exercise 2.5 . Question number 57,58 Kindly help me out mates , i will be thankfull",,['ordinary-differential-equations']
15,Numerical methods for ODE: Taylor vs. Interpolation approaches,Numerical methods for ODE: Taylor vs. Interpolation approaches,,"Hy everybody! I am new to the subject ""numerical methods for ODE"". I read some basic literature but since most of the concepts and methods are new to me, I wanted to ask you, if you could give me feedback if I understand everything correctly: There are two numeric approaches for solving differential equations: a) Based on Taylor Series Approximation: Euler, Runge Kutta, etc. Goal: to have similar accuracy as with Taylor series but without calculating derivatives. Work-around was developed, where you only evaluate functions at certain points without calculating derivatives. b) Based on Interpolation Polynomials: Multi-Step Methods, Collocation methods: Make use of past information; no intermediate calculations (as in Runge-Kutta) . General idea: fit a polynomial using this past data + extrapolate from tn to tn+1 Are ther caes, where Runge-Kutta methods are better compared to Multi-step methods and vice versa? Thank you very much for your help!","Hy everybody! I am new to the subject ""numerical methods for ODE"". I read some basic literature but since most of the concepts and methods are new to me, I wanted to ask you, if you could give me feedback if I understand everything correctly: There are two numeric approaches for solving differential equations: a) Based on Taylor Series Approximation: Euler, Runge Kutta, etc. Goal: to have similar accuracy as with Taylor series but without calculating derivatives. Work-around was developed, where you only evaluate functions at certain points without calculating derivatives. b) Based on Interpolation Polynomials: Multi-Step Methods, Collocation methods: Make use of past information; no intermediate calculations (as in Runge-Kutta) . General idea: fit a polynomial using this past data + extrapolate from tn to tn+1 Are ther caes, where Runge-Kutta methods are better compared to Multi-step methods and vice versa? Thank you very much for your help!",,"['ordinary-differential-equations', 'numerical-methods']"
16,Rounding error of trapezoidal method,Rounding error of trapezoidal method,,"I'm working with the Modified Euler method sometimes called Heun's method or explicit trapezoidal method. I have a book on ordinary differential equations numerical analysis that claims: The effect of rounding error on the accuracy of the numerical solution    is very similar to the error of the numerical differentiation    formulas: the truncation error decreases with h but the rounding error    increases and there exist an optimal value for which the sum of both    errors is minimum. This optimal value for $h$ is very little (for example for Euler's    method is $\sqrt\mu$ where $\mu$ is the accuracy of the machine) and so    it's computationally expensive. I have an example of what he is talking about with numerical differentiation formulas so for example take the differentiation formula: $$f'(x_0) \simeq \frac{f(x_0+h)-f(x_0)}{h}~\text{  with error }~-h \frac{f''(\theta)}{2}$$ then I write $e(x)$ the rounding error on point $x$ and so the total error made while aproximating $f'(x_0)$ is $$ \frac{e(x_0+h)-e(x_0)}{h} - h \frac{f''(\theta)}{2}. $$ Assuming that rounding errors are bounded by $\epsilon$ and that $f''$ is bounded by M in $[x_0,x_0+h]$ then the total error verifies: $$ \left|f'(x_0)-\frac{f_1(x_0+h)-f_1(x_0)}{h}\right| \leq 2 \frac{\epsilon}{h} + \frac{h}{2}M $$ where $f_1$ is the approximation of $f$ with rounding error. So, when $h$ decreases the error of the formula (truncation error) decreases but rounding error increases. My question is very simple, how can I get a similar situation for the trapezoidal method?","I'm working with the Modified Euler method sometimes called Heun's method or explicit trapezoidal method. I have a book on ordinary differential equations numerical analysis that claims: The effect of rounding error on the accuracy of the numerical solution    is very similar to the error of the numerical differentiation    formulas: the truncation error decreases with h but the rounding error    increases and there exist an optimal value for which the sum of both    errors is minimum. This optimal value for is very little (for example for Euler's    method is where is the accuracy of the machine) and so    it's computationally expensive. I have an example of what he is talking about with numerical differentiation formulas so for example take the differentiation formula: then I write the rounding error on point and so the total error made while aproximating is Assuming that rounding errors are bounded by and that is bounded by M in then the total error verifies: where is the approximation of with rounding error. So, when decreases the error of the formula (truncation error) decreases but rounding error increases. My question is very simple, how can I get a similar situation for the trapezoidal method?","h \sqrt\mu \mu f'(x_0) \simeq \frac{f(x_0+h)-f(x_0)}{h}~\text{  with error }~-h \frac{f''(\theta)}{2} e(x) x f'(x_0) 
\frac{e(x_0+h)-e(x_0)}{h} - h \frac{f''(\theta)}{2}.
 \epsilon f'' [x_0,x_0+h] 
\left|f'(x_0)-\frac{f_1(x_0+h)-f_1(x_0)}{h}\right| \leq 2 \frac{\epsilon}{h} + \frac{h}{2}M
 f_1 f h","['ordinary-differential-equations', 'numerical-methods', 'error-propagation', 'rounding-error']"
17,Does this function define first-order ordinary differential equations?,Does this function define first-order ordinary differential equations?,,"I've read 3 different books but, this condition seems to fit them all: It's homogeneous if:   $y'=f(tx,ty)=f(x,y)$ Is this correct?","I've read 3 different books but, this condition seems to fit them all: It's homogeneous if:   $y'=f(tx,ty)=f(x,y)$ Is this correct?",,"['ordinary-differential-equations', 'homogeneous-equation']"
18,How can the Bessel function of the second kind be in the radial eigenfunction?,How can the Bessel function of the second kind be in the radial eigenfunction?,,"Let $0<a<b<\infty$. Consider the heat equations or wave equations on the annulus or the spherical layers $$\Omega:=\{x\in\mathbb{R}^d\mid a<\|x\|_2<b\},$$ $$\partial\Omega:=\{x\in\mathbb{R}^d\mid \|x\|_2=a \}\cup \{x\in\mathbb{R}^d\mid \|x\|_2=b \},$$ i.e., the annulus ($d=2$) or the spherical layers ($d=3$) with the Dirichlet boundary condition. More compactly, a Bessel function of the second kind in eigenfunctions of a Laplace operator with Dirichlet boundary conditions on a spherical domain. We assume the initial condition is nice enough, such that no subtleties arise and we can use the separation of variables. Question: How can the Bessel function of the second kind be in the radial eigenfunction? Context: When we use the separation of variables, the problem boils down to solving two ODEs in the angular and the radial part. In the radial part, when we consider the disk or the ball ($a=0$), we impose the condition $u(0)<+\infty$ at the origin ($u$ is the solution), which eliminates the Bessel function of the second kind from the consideration for the eigenfunction in the radial part. Suppose $a>0$. I have found, e.g., in http://arxiv.org/pdf/1206.1278.pdf , in the radial part people consider both the Bessel function of the first and second. But if you see the plot of the second kind, http://mathworld.wolfram.com/BesselFunctionoftheSecondKind.html , you see it is unbounded not only in the neighborhood of the origin but the ""unboundedness-point"" shifts rightward. How can one consider non-trivial eigenfunctions with Dirichlet boundary condition, but with possibly unbounded functions? Also, are there books that treats $d=3$ in detail? Feels like it would be something like ""eigenvalues are taken large enough so that we can avoid the unboundedness part"", but I would like to see e.g., how large, in relation to the unboundedness part etc.","Let $0<a<b<\infty$. Consider the heat equations or wave equations on the annulus or the spherical layers $$\Omega:=\{x\in\mathbb{R}^d\mid a<\|x\|_2<b\},$$ $$\partial\Omega:=\{x\in\mathbb{R}^d\mid \|x\|_2=a \}\cup \{x\in\mathbb{R}^d\mid \|x\|_2=b \},$$ i.e., the annulus ($d=2$) or the spherical layers ($d=3$) with the Dirichlet boundary condition. More compactly, a Bessel function of the second kind in eigenfunctions of a Laplace operator with Dirichlet boundary conditions on a spherical domain. We assume the initial condition is nice enough, such that no subtleties arise and we can use the separation of variables. Question: How can the Bessel function of the second kind be in the radial eigenfunction? Context: When we use the separation of variables, the problem boils down to solving two ODEs in the angular and the radial part. In the radial part, when we consider the disk or the ball ($a=0$), we impose the condition $u(0)<+\infty$ at the origin ($u$ is the solution), which eliminates the Bessel function of the second kind from the consideration for the eigenfunction in the radial part. Suppose $a>0$. I have found, e.g., in http://arxiv.org/pdf/1206.1278.pdf , in the radial part people consider both the Bessel function of the first and second. But if you see the plot of the second kind, http://mathworld.wolfram.com/BesselFunctionoftheSecondKind.html , you see it is unbounded not only in the neighborhood of the origin but the ""unboundedness-point"" shifts rightward. How can one consider non-trivial eigenfunctions with Dirichlet boundary condition, but with possibly unbounded functions? Also, are there books that treats $d=3$ in detail? Feels like it would be something like ""eigenvalues are taken large enough so that we can avoid the unboundedness part"", but I would like to see e.g., how large, in relation to the unboundedness part etc.",,"['ordinary-differential-equations', 'reference-request', 'partial-differential-equations', 'special-functions', 'spherical-coordinates']"
19,Differential Equation $\frac{dy}{dx} =\frac{x}{(x^2y + y^3}$,Differential Equation,\frac{dy}{dx} =\frac{x}{(x^2y + y^3},Please help with: $$\frac{dy}{dx} =\frac{x}{x^2y + y^3}$$ The hint says let $u = x^2$ I have tried all the possible substitutions and manipulations on this ODE and I just can't separate variables or use any methods on this! What am I missing? The closest is that I have tried to flip everything so $$\frac{dx}{dy} = xy + \frac{y^3}{x}$$ and tried to do Bernoulli's method on it but failed.. Thank you..,Please help with: $$\frac{dy}{dx} =\frac{x}{x^2y + y^3}$$ The hint says let $u = x^2$ I have tried all the possible substitutions and manipulations on this ODE and I just can't separate variables or use any methods on this! What am I missing? The closest is that I have tried to flip everything so $$\frac{dx}{dy} = xy + \frac{y^3}{x}$$ and tried to do Bernoulli's method on it but failed.. Thank you..,,"['ordinary-differential-equations', 'substitution']"
20,ODE $y(1+2xy)dx+x(1-xy)dy=0$,ODE,y(1+2xy)dx+x(1-xy)dy=0,$$y(1+2xy)dx+x(1-xy)dy=0$$ I have tried to isolate $\frac{dy}{dx}$ and got the following: $$\frac{dy}{dx}=-\frac{y(1+2xy)}{x(1-xy)}$$ but I understand that the terms have to be in the same order and that is not the case. What should I do?,$$y(1+2xy)dx+x(1-xy)dy=0$$ I have tried to isolate $\frac{dy}{dx}$ and got the following: $$\frac{dy}{dx}=-\frac{y(1+2xy)}{x(1-xy)}$$ but I understand that the terms have to be in the same order and that is not the case. What should I do?,,"['calculus', 'ordinary-differential-equations']"
21,On solution to simple ODE,On solution to simple ODE,,Consider the ODE $$\frac{dx}{dt} = ax + b$$ where $a$ and $b$ are two parameters.  The way to solve this is to divide both sides by $ax+b$ and integrate: $$\int \frac{\dot x}{ax+b}dt = t+C \\ \frac{\log|ax+b|}{a} = t+C \\ x(t) = Ke^{at}-\frac ba$$ Easy enough.  But I'm not sure why we're not excluding some possible solutions in the first step of this approach.  Doesn't dividing by $ax+b$ immediately rule out any solution where $x(t)=-\frac ba$ anywhere in the interval over which the function is defined?  That seems like we might be losing a lot of potential solutions.  So why is the above solution the general solution ?,Consider the ODE $$\frac{dx}{dt} = ax + b$$ where $a$ and $b$ are two parameters.  The way to solve this is to divide both sides by $ax+b$ and integrate: $$\int \frac{\dot x}{ax+b}dt = t+C \\ \frac{\log|ax+b|}{a} = t+C \\ x(t) = Ke^{at}-\frac ba$$ Easy enough.  But I'm not sure why we're not excluding some possible solutions in the first step of this approach.  Doesn't dividing by $ax+b$ immediately rule out any solution where $x(t)=-\frac ba$ anywhere in the interval over which the function is defined?  That seems like we might be losing a lot of potential solutions.  So why is the above solution the general solution ?,,['ordinary-differential-equations']
22,Techniques/Heuristics for choosing multipliers in method of characteristics for solving PDE,Techniques/Heuristics for choosing multipliers in method of characteristics for solving PDE,,"I am trying to solve this linear partial differential equation via method of multipliers. $$(2y^2+z)z_x+(y+2x)z_y=4xy-z$$ The auxilary Equations are given by: $$\dfrac{dx}{2y^2+z}=\dfrac{dy}{y+2x}=\dfrac{dz}{4xy-z}=\dfrac{l dx+m dy+n dz}{l(2y^2+z)+m(y+2x)+n (4xy-z)}$$ Letting $(l, m, n)=(1, -2y, 1)$ solves the first part of our problem; giving us $dx-2ydy+dz=0\implies x-y^2+z=c$. This is where I am stuck. I tried first to find another triplet of $l, m, n$ such that the denominator vanishes on R.H.S. On failing to find such a triplet, I expressed $x$ in terms of $ y $ and $z$ by using $x-y^2+z=c$ and tried to use $dy/..=dz/..$ in order to get the second solution. This ODE I got was very messy and I couldn't continue. Question: 1) How can another triplet $l, m, n$ be found s.t. denominator on R.H.S. is zero? 2) If not, is there a way to get a simpler ODE? 3) Are there any clever ways to get the multipliers i) in general case ii) When the coefficients of $z_x$ and $z_y$ are polynomials in $x$ and $y$? 4) Is there a deterministic way to solve Linear First Order Partial Differential Equation ? (We have to guess the multipliers in method of multipliers)","I am trying to solve this linear partial differential equation via method of multipliers. $$(2y^2+z)z_x+(y+2x)z_y=4xy-z$$ The auxilary Equations are given by: $$\dfrac{dx}{2y^2+z}=\dfrac{dy}{y+2x}=\dfrac{dz}{4xy-z}=\dfrac{l dx+m dy+n dz}{l(2y^2+z)+m(y+2x)+n (4xy-z)}$$ Letting $(l, m, n)=(1, -2y, 1)$ solves the first part of our problem; giving us $dx-2ydy+dz=0\implies x-y^2+z=c$. This is where I am stuck. I tried first to find another triplet of $l, m, n$ such that the denominator vanishes on R.H.S. On failing to find such a triplet, I expressed $x$ in terms of $ y $ and $z$ by using $x-y^2+z=c$ and tried to use $dy/..=dz/..$ in order to get the second solution. This ODE I got was very messy and I couldn't continue. Question: 1) How can another triplet $l, m, n$ be found s.t. denominator on R.H.S. is zero? 2) If not, is there a way to get a simpler ODE? 3) Are there any clever ways to get the multipliers i) in general case ii) When the coefficients of $z_x$ and $z_y$ are polynomials in $x$ and $y$? 4) Is there a deterministic way to solve Linear First Order Partial Differential Equation ? (We have to guess the multipliers in method of multipliers)",,"['ordinary-differential-equations', 'partial-differential-equations']"
23,Bizarre failure of integrating factor in elementary differential equation,Bizarre failure of integrating factor in elementary differential equation,,"I have an issue with an extremely elementary problem. Consider the differential equation $y' + \cot(x) y = 1$. Obviously, one can use an integrating factor of $e^{\int \cot(x) dx} = e^{\ln(\sin(x)) } $ (the arbitrary constant would cancel out) $= \sin(x)$ to solve the differential equation, obtaining the correct answer $y = - \cot(x) + C \csc(x)$. However, the assertion $ \int \cot(x) dx = \ln(\sin(x)) +C $ is true only modulo subtle things involving branches of $\ln$ in the complex plane. Restricted to the real line, we use $\int \cot (x) dx = \ln |\sin(x)| +C$. If we do the above method, we get an integrating factor not of $\sin(x)$ but of $|\sin(x)|$: $$ |\sin(x)| y' + \cot(x) | \sin(x)| y = |\sin(x)|$$ Indeed, $\frac{d}{dx} |\sin(x)| = \cot(x) | \sin(x)|$, so this is a valid alternate choice of integrating factor. Proceeding, we have $$ \frac{d}{dx} ( |\sin(x)| y ) = |\sin(x)| $$ $$ y = \frac{\int |\sin(x)| dx}{|\sin(x)|}$$ But this is not equal to the correct answer of $- \cot(x) + C \csc(x)$! What is going on? EDIT: It seems every calculus solution manual ever is wrong. EDIT 2: Alternatively, it seems every introductory calculus textbook, including Stewart, gives the wrong definition of ""general solution"".","I have an issue with an extremely elementary problem. Consider the differential equation $y' + \cot(x) y = 1$. Obviously, one can use an integrating factor of $e^{\int \cot(x) dx} = e^{\ln(\sin(x)) } $ (the arbitrary constant would cancel out) $= \sin(x)$ to solve the differential equation, obtaining the correct answer $y = - \cot(x) + C \csc(x)$. However, the assertion $ \int \cot(x) dx = \ln(\sin(x)) +C $ is true only modulo subtle things involving branches of $\ln$ in the complex plane. Restricted to the real line, we use $\int \cot (x) dx = \ln |\sin(x)| +C$. If we do the above method, we get an integrating factor not of $\sin(x)$ but of $|\sin(x)|$: $$ |\sin(x)| y' + \cot(x) | \sin(x)| y = |\sin(x)|$$ Indeed, $\frac{d}{dx} |\sin(x)| = \cot(x) | \sin(x)|$, so this is a valid alternate choice of integrating factor. Proceeding, we have $$ \frac{d}{dx} ( |\sin(x)| y ) = |\sin(x)| $$ $$ y = \frac{\int |\sin(x)| dx}{|\sin(x)|}$$ But this is not equal to the correct answer of $- \cot(x) + C \csc(x)$! What is going on? EDIT: It seems every calculus solution manual ever is wrong. EDIT 2: Alternatively, it seems every introductory calculus textbook, including Stewart, gives the wrong definition of ""general solution"".",,"['integration', 'ordinary-differential-equations']"
24,Application of Grönwall's inequality,Application of Grönwall's inequality,,"Let $F: \mathbb{R} \rightarrow \mathbb{R}$ be a Lipschitz function and $\varphi, \psi: \mathbb{R} \rightarrow \mathbb{R} $ be continuous differentiable functions. Suppose that $\forall t \in \mathbb{R}$: $$\varphi'(t)\geq F(\varphi(t))$$  $$\psi'(t) = F(\psi(t)) $$ Show that if $\psi(t_0)=\varphi(t_0)$, then $\psi(t) \leq \varphi(t), \forall t \geq t_0$. I should use Grönwall's inequality: Let $u: [a,b] \rightarrow \mathbb{R_+}$  be a continuous function.   If there is a function $v: [a,b] \rightarrow \mathbb{R_+}$ and a constant $C>0$ such that:   $$u(t) \leq C + \int_{a}^{t}{v(s)u(s)ds}  \quad \forall t \in [a,b]$$   Then $u(t) \leq C \exp\bigg({\int_{a}^{t}{v(s) ds}} \bigg)$ My attempt: Let $K>0$  be the Lipschitz constant of $F$. We have: $$\varphi'(t)\geq F(\varphi(t)) \Rightarrow -\varphi'(t)\leq -F(\varphi(t))$$ $$\Rightarrow \psi'(t)-\varphi'(t)\leq F(\psi(t))-F(\varphi(t)) \leq K|\psi(t)-\varphi(t)|$$ $$\Rightarrow \int_{t_0}^{t}{(\psi'(s)-\varphi'(s))ds} \leq \int_{t_0}^{t}{K|\psi(s)-\varphi(s)|ds}$$ $$\Rightarrow \psi(t)-\varphi(t) -(\psi(t_0)-\varphi(t_0) \leq \int_{t_0}^{t}{K|\psi(s)-\varphi(s)|ds}$$ Since $\psi(t_0)=\varphi(t_0)$: $$\Rightarrow \psi(t)-\varphi(t) \leq \int_{t_0}^{t}{K|\psi(s)-\varphi(s)|ds}$$ Let $u(t) = \psi(t)-\varphi(t)$, $v(s) = K$ and $C = 0$. Thus  $$u(t) \leq \int_{a}^{t}{v(s)|u(s)|ds}$$ It looks like the inequality in  Grönwall's inequality hypothesis. However, there's a modulus on the right hand side. How could I fix it?","Let $F: \mathbb{R} \rightarrow \mathbb{R}$ be a Lipschitz function and $\varphi, \psi: \mathbb{R} \rightarrow \mathbb{R} $ be continuous differentiable functions. Suppose that $\forall t \in \mathbb{R}$: $$\varphi'(t)\geq F(\varphi(t))$$  $$\psi'(t) = F(\psi(t)) $$ Show that if $\psi(t_0)=\varphi(t_0)$, then $\psi(t) \leq \varphi(t), \forall t \geq t_0$. I should use Grönwall's inequality: Let $u: [a,b] \rightarrow \mathbb{R_+}$  be a continuous function.   If there is a function $v: [a,b] \rightarrow \mathbb{R_+}$ and a constant $C>0$ such that:   $$u(t) \leq C + \int_{a}^{t}{v(s)u(s)ds}  \quad \forall t \in [a,b]$$   Then $u(t) \leq C \exp\bigg({\int_{a}^{t}{v(s) ds}} \bigg)$ My attempt: Let $K>0$  be the Lipschitz constant of $F$. We have: $$\varphi'(t)\geq F(\varphi(t)) \Rightarrow -\varphi'(t)\leq -F(\varphi(t))$$ $$\Rightarrow \psi'(t)-\varphi'(t)\leq F(\psi(t))-F(\varphi(t)) \leq K|\psi(t)-\varphi(t)|$$ $$\Rightarrow \int_{t_0}^{t}{(\psi'(s)-\varphi'(s))ds} \leq \int_{t_0}^{t}{K|\psi(s)-\varphi(s)|ds}$$ $$\Rightarrow \psi(t)-\varphi(t) -(\psi(t_0)-\varphi(t_0) \leq \int_{t_0}^{t}{K|\psi(s)-\varphi(s)|ds}$$ Since $\psi(t_0)=\varphi(t_0)$: $$\Rightarrow \psi(t)-\varphi(t) \leq \int_{t_0}^{t}{K|\psi(s)-\varphi(s)|ds}$$ Let $u(t) = \psi(t)-\varphi(t)$, $v(s) = K$ and $C = 0$. Thus  $$u(t) \leq \int_{a}^{t}{v(s)|u(s)|ds}$$ It looks like the inequality in  Grönwall's inequality hypothesis. However, there's a modulus on the right hand side. How could I fix it?",,"['real-analysis', 'ordinary-differential-equations', 'dynamical-systems']"
25,Finding fixed points of an equation when the derivative is not defined,Finding fixed points of an equation when the derivative is not defined,,"For a dynamical system governed by the equation $f(x) = \mathrm{d}y/\mathrm{d}x = 2(1-x^2)^{1/2}$. Find stable and unstable fixed points. The fixed points for the above equation are $+1$ and $-1$. I took the derivative of given equation and set it equal to zero. If $f'(x)<0$, it's a stable point and if $f'(x)>0$, it's an unstable point. The derivative is $$\frac{-2x}{(1-x^2)^{1/2}}$$ Now if I plug $x=1$ or $x=-1$, it's not defined. How do I find the stability of fixed points in this case?","For a dynamical system governed by the equation $f(x) = \mathrm{d}y/\mathrm{d}x = 2(1-x^2)^{1/2}$. Find stable and unstable fixed points. The fixed points for the above equation are $+1$ and $-1$. I took the derivative of given equation and set it equal to zero. If $f'(x)<0$, it's a stable point and if $f'(x)>0$, it's an unstable point. The derivative is $$\frac{-2x}{(1-x^2)^{1/2}}$$ Now if I plug $x=1$ or $x=-1$, it's not defined. How do I find the stability of fixed points in this case?",,['ordinary-differential-equations']
26,Series solution to this differential equation,Series solution to this differential equation,,"$$ y' - e^{x^2}y = 0 $$ I've learned how to get the series solution for such differential equations when the multiplicating function is polynomial, but I have no clue what to do with another function.  Here's what I've tried : $$ \sum_1^\infty na_nx^{n-1} - e^{x^2}\sum_0^\infty a_nx^n = 0$$ $$ \sum_0^\infty (n+1)a_{n+1}x^n - e^{x^2}\sum_0^\infty a_nx^n = 0$$ Then I'm stuck because I don't know what to do with the exponential function.  If it was polynomial, I could simply distribute it in the sum and do a variable change, but how can I treat a such case? Thank you. Actually, I only need to find the first few terms, not the general solution to it (which is probably complicated).","$$ y' - e^{x^2}y = 0 $$ I've learned how to get the series solution for such differential equations when the multiplicating function is polynomial, but I have no clue what to do with another function.  Here's what I've tried : $$ \sum_1^\infty na_nx^{n-1} - e^{x^2}\sum_0^\infty a_nx^n = 0$$ $$ \sum_0^\infty (n+1)a_{n+1}x^n - e^{x^2}\sum_0^\infty a_nx^n = 0$$ Then I'm stuck because I don't know what to do with the exponential function.  If it was polynomial, I could simply distribute it in the sum and do a variable change, but how can I treat a such case? Thank you. Actually, I only need to find the first few terms, not the general solution to it (which is probably complicated).",,['ordinary-differential-equations']
27,A mirror focusing beams at one point,A mirror focusing beams at one point,,"How can I find a shape of a mirror which focuses all parallel beams in one point? I tried to do it in this way: The mirror must be symmetric hence I assumed it has a center in the point $(0,0)$. The point which focuses all the beams is $ (0,a), \, a>0$. Now $$L_{1}:\frac{-1}{f^{'}(t)}x  +C $$ is a line perpendicular to tangent to a mirror (a mirror given by a function $f: \mathbb{R} \rightarrow \mathbb{R}$ )at point $(t,f(t))$ $$L_2:t$$ is a beam and $$L_3:\frac{f(t)-a}{t}x +a $$ is a line going through both $(0,a)$ and $(t,f(t))$. Now the requirement that  angle between $L_1 $ and $L_2 $ and angle between $L_1 $ and $L_3 $ are equal should give a proper differential  equation. Unfortunately, it is not. Am I doing any mistakes? Is there an easier way to do it?","How can I find a shape of a mirror which focuses all parallel beams in one point? I tried to do it in this way: The mirror must be symmetric hence I assumed it has a center in the point $(0,0)$. The point which focuses all the beams is $ (0,a), \, a>0$. Now $$L_{1}:\frac{-1}{f^{'}(t)}x  +C $$ is a line perpendicular to tangent to a mirror (a mirror given by a function $f: \mathbb{R} \rightarrow \mathbb{R}$ )at point $(t,f(t))$ $$L_2:t$$ is a beam and $$L_3:\frac{f(t)-a}{t}x +a $$ is a line going through both $(0,a)$ and $(t,f(t))$. Now the requirement that  angle between $L_1 $ and $L_2 $ and angle between $L_1 $ and $L_3 $ are equal should give a proper differential  equation. Unfortunately, it is not. Am I doing any mistakes? Is there an easier way to do it?",,"['calculus', 'ordinary-differential-equations', 'analytic-geometry', 'mathematical-physics']"
28,Hölder space continuously embeds into $L^2$ space?,Hölder space continuously embeds into  space?,L^2,"Suppose $S^n$ is the unit sphere of $R^{n+1}.$ Give $\alpha\in (0,1),$ does there exist a constant $c_{n,\alpha}>0$ such that $$\|f\|_{L^{2}(S^n)}\leq c_{n,\alpha}\|f\|_{C^{0,\alpha}(S^n)}?$$ The definition of the Hölder norm is given here .","Suppose $S^n$ is the unit sphere of $R^{n+1}.$ Give $\alpha\in (0,1),$ does there exist a constant $c_{n,\alpha}>0$ such that $$\|f\|_{L^{2}(S^n)}\leq c_{n,\alpha}\|f\|_{C^{0,\alpha}(S^n)}?$$ The definition of the Hölder norm is given here .",,['functional-analysis']
29,Finding the general solution of a second order PDE,Finding the general solution of a second order PDE,,"I want to find the general solution of $$\frac{\partial^2f(x,y)}{\partial x^2}+\frac{\partial^2f(x,y)}{\partial y^2}+A^2f(x,y)=0,$$   with $f$ a real valued function and $A$ a real constant. I know that $f(x,y)=\sin(\frac{m}{\sqrt{2}}(x+y))$ is a solution of this, but how do I find a general solution?","I want to find the general solution of $$\frac{\partial^2f(x,y)}{\partial x^2}+\frac{\partial^2f(x,y)}{\partial y^2}+A^2f(x,y)=0,$$   with $f$ a real valued function and $A$ a real constant. I know that $f(x,y)=\sin(\frac{m}{\sqrt{2}}(x+y))$ is a solution of this, but how do I find a general solution?",,"['ordinary-differential-equations', 'partial-differential-equations']"
30,Hamiltonian system; breakdown for different level set values,Hamiltonian system; breakdown for different level set values,,"I have a system of differential equations defined by the hamiltonian of the scalar function $H=y^2+e^{-xy}-c$, for some $c>0$. I am asked to describe what happens for $c=1$. I can tell there is a problem using mathematica (the level sets for $c=1$ seem to cross, violating uniqueness of solutions to the ode defined by these partials which travel around level sets). Since finding a function $H(x(t),y(t))$ in the first place for a system like this would seem to rely on the implicit function theorem, I am looking for a vanishing partial at the value c=1 to show that for this point the ift fails. I also tried thinking about the gradient of this function being not surjective for this value of c, since this would imply a singularity (or not regular point), I think. However, I think this would involve plugging the expression for $y(x)$ into my grad function, which seems a bit laborious and i want to make sure the thinking is correct first.","I have a system of differential equations defined by the hamiltonian of the scalar function $H=y^2+e^{-xy}-c$, for some $c>0$. I am asked to describe what happens for $c=1$. I can tell there is a problem using mathematica (the level sets for $c=1$ seem to cross, violating uniqueness of solutions to the ode defined by these partials which travel around level sets). Since finding a function $H(x(t),y(t))$ in the first place for a system like this would seem to rely on the implicit function theorem, I am looking for a vanishing partial at the value c=1 to show that for this point the ift fails. I also tried thinking about the gradient of this function being not surjective for this value of c, since this would imply a singularity (or not regular point), I think. However, I think this would involve plugging the expression for $y(x)$ into my grad function, which seems a bit laborious and i want to make sure the thinking is correct first.",,"['real-analysis', 'ordinary-differential-equations', 'multivariable-calculus', 'dynamical-systems']"
31,"Solving the IVP: $y'(x) = \frac{1}{1-(xy)^2}, y(-1)=1$",Solving the IVP:,"y'(x) = \frac{1}{1-(xy)^2}, y(-1)=1","I am only starting to learn diff.equations and have the following initial value problem: $y'(x) = \frac{1}{1-(xy)^2}, y(-1)=1$. So, since $y'(x)$ is undefined at $(-1,1)$ do we say that the solution of the initial value problem doesn't exist or do we say that it ""jumps"" to infinity (or negative infinity?) as $y'(x)$ at the given point is infinite? Is $y(-1)=1$ still a solution of the problem? Is this point the only solution? I am lost. EDITED: Given the response below, if we solve such a problem with Euler's method, will it not provide any solution? I assume the Existence and Uniqueness theorem will also not be applicable. How can we ensure that the solution exists?","I am only starting to learn diff.equations and have the following initial value problem: $y'(x) = \frac{1}{1-(xy)^2}, y(-1)=1$. So, since $y'(x)$ is undefined at $(-1,1)$ do we say that the solution of the initial value problem doesn't exist or do we say that it ""jumps"" to infinity (or negative infinity?) as $y'(x)$ at the given point is infinite? Is $y(-1)=1$ still a solution of the problem? Is this point the only solution? I am lost. EDITED: Given the response below, if we solve such a problem with Euler's method, will it not provide any solution? I assume the Existence and Uniqueness theorem will also not be applicable. How can we ensure that the solution exists?",,['ordinary-differential-equations']
32,"Solving a system of differential equations, and periodicity","Solving a system of differential equations, and periodicity",,"The question Given the following system of differential equations $\left\{\begin{matrix} \; \dot{x} & = & y-1 \\  \; \dot{y} & = & -xy \end{matrix}\right.$ Give a differential equation for its trajectories and solve it. Find which trajectories are closed, and therefore correspond to periodical solutions. My problem I know that it follows from the chain rule that $\frac{\mathrm{d}y}{\mathrm{d}t} = \frac{\mathrm{d}y}{\mathrm{d}x} \cdot \frac{\mathrm{d}x}{\mathrm{d}t} \; \Rightarrow \; \frac{\mathrm{d}y}{\mathrm{d}x} = \frac{\dot{y}}{\dot{x}}$ so that after substituting in the equations from the above system a trajectory is given by the planar coordinates $x(t), \; y(x(t))$ for $y$ a solution to the differential equation $\frac{\mathrm{d}y}{\mathrm{d}x} = \frac{-xy}{y-1}$ When I try to solve this as a seperable differential equation however, I end up with a nasty solution from which I cannot isolate $y$. Furthermore I have no idea how to determine if a trajectory is closed, or how to show any solution is periodical. What should my approach be?","The question Given the following system of differential equations $\left\{\begin{matrix} \; \dot{x} & = & y-1 \\  \; \dot{y} & = & -xy \end{matrix}\right.$ Give a differential equation for its trajectories and solve it. Find which trajectories are closed, and therefore correspond to periodical solutions. My problem I know that it follows from the chain rule that $\frac{\mathrm{d}y}{\mathrm{d}t} = \frac{\mathrm{d}y}{\mathrm{d}x} \cdot \frac{\mathrm{d}x}{\mathrm{d}t} \; \Rightarrow \; \frac{\mathrm{d}y}{\mathrm{d}x} = \frac{\dot{y}}{\dot{x}}$ so that after substituting in the equations from the above system a trajectory is given by the planar coordinates $x(t), \; y(x(t))$ for $y$ a solution to the differential equation $\frac{\mathrm{d}y}{\mathrm{d}x} = \frac{-xy}{y-1}$ When I try to solve this as a seperable differential equation however, I end up with a nasty solution from which I cannot isolate $y$. Furthermore I have no idea how to determine if a trajectory is closed, or how to show any solution is periodical. What should my approach be?",,['ordinary-differential-equations']
33,How can I solve this ODE $ \frac {d^2x}{dt^2} + 8\frac {dx}{dt} + 25x = 10u(t)$?,How can I solve this ODE ?, \frac {d^2x}{dt^2} + 8\frac {dx}{dt} + 25x = 10u(t),"The problem its that I don't know how to treat the $ 10u(t) $ to obtain the particular solution since I don't know what the $ u(t) $ function represents. I've already have the complementary solution of the homogeneous equation associated which is: $$ X_c(t)=C_1e^{-4t}\cos {3t} + C_2e^{-4t}\sin {3t} . $$ Here again, I leave the equation to solve. $$ \frac {d^2x}{dt^2} + 8\frac {dx}{dt} + 25x = 10u(t)$$ Thank you. EDIT: Thanks SplitInfinity, Yes, the $ u(t) $ was the Step Function and something I forgot to mention that the problem says: consider initial conditions in $ 0 $ . So the step function states: $$ u(t)= \begin{cases}  0 & t<0 \\  1 & t \ge 0  \end{cases} $$ in this case $ t=1 $ , therefore the equation ends up like this $$ \begin{align} x''+8x'+25x&=10(1) \\ x''+8x'+25x&=10 \end{align}$$ because the non-homogeneous part its a linear polynomial the $X_p$ (proposal solution) need to be in this way: $$ X_p=A \\ X'_p=0 \\ X''_p=0 $$ So the equation ends up like: $$ \begin{align} x_p''+8x_p'+25x_p & =10 \\ 0+8(0)+25(A) & = 10 \\ A & = \frac {10}{25} \\ A & = \frac 25 \end{align}$$ Now the General Solution is given by $ X=X_c+X_p $ which is: $$ X=C_1e^{-4t}\cos {3t} + C_2e^{-4t}\sin {3t}+\frac 25 $$ To find out the $C_1$ and $C_2$ values we need to apply the initial values, which are: $$ x(0)=0,~~ x'(0)=0 $$ in order to do so, we differentiate $X$ $$ X'=C_1e^{-4t}(-3\sin{3t}-4\cos{3t})+C_2e^{-4t}(3\cos{3t}-4\sin{3t}) $$ Now, applying initial values to X: $\require{cancel}$ \begin{align}  X(0) & = C_1\cancelto{1}{e^{-4(0)}}\cancelto{1}{\cos{3(0)}}+\cancelto{0}{C_2e^{-4(0)}\sin{3(0)}}+\frac25 \\ 0 &= C_1 + \frac 25 \\ C_1 &= -\frac25 \end{align} Now to $ X' $ : \begin{align} X'(0) &= C_1\cancelto{1}{e^{-4(0)}}(\cancelto{0}{-3\sin{3(0)}})+C_2\cancelto{1}{e^{-4(0)}}(3\cancelto{1}{\cos{3(0)}}-\cancelto{0}{4\sin{3(0)}}) \\ 0 &= C_1(-4)+C_2(3) \\ 0 &= -4C_1 + 3C_2 \\ 3C_2 &= 4C_1 \\ \text{Replacing the $C_1$ value} \\ C_2 &= \frac{4(-\frac25)}{3} \\ C_2 &= -\frac8{15} \end{align} Replacing the constants $ C_1 $ and $ C_2 $ in $X$ we found the solution: \begin{align} X &= -\frac25 e^{-4t}\cos{3t}-\frac8{15}e^{-4t}\sin{3t}+\frac25 \\ X &= \frac1{15} \left[ e^{-4t}(-6\cos{3t}-8\sin{3t})+6 \right] \end{align}","The problem its that I don't know how to treat the to obtain the particular solution since I don't know what the function represents. I've already have the complementary solution of the homogeneous equation associated which is: Here again, I leave the equation to solve. Thank you. EDIT: Thanks SplitInfinity, Yes, the was the Step Function and something I forgot to mention that the problem says: consider initial conditions in . So the step function states: in this case , therefore the equation ends up like this because the non-homogeneous part its a linear polynomial the (proposal solution) need to be in this way: So the equation ends up like: Now the General Solution is given by which is: To find out the and values we need to apply the initial values, which are: in order to do so, we differentiate Now, applying initial values to X: Now to : Replacing the constants and in we found the solution:"," 10u(t)   u(t)   X_c(t)=C_1e^{-4t}\cos {3t} + C_2e^{-4t}\sin {3t} .   \frac {d^2x}{dt^2} + 8\frac {dx}{dt} + 25x = 10u(t)  u(t)   0   u(t)=
\begin{cases} 
0 & t<0 \\ 
1 & t \ge 0 
\end{cases}   t=1   \begin{align}
x''+8x'+25x&=10(1) \\
x''+8x'+25x&=10
\end{align} X_p  X_p=A \\
X'_p=0 \\
X''_p=0   \begin{align}
x_p''+8x_p'+25x_p & =10 \\
0+8(0)+25(A) & = 10 \\
A & = \frac {10}{25} \\
A & = \frac 25
\end{align}  X=X_c+X_p   X=C_1e^{-4t}\cos {3t} + C_2e^{-4t}\sin {3t}+\frac 25  C_1 C_2  x(0)=0,~~
x'(0)=0  X  X'=C_1e^{-4t}(-3\sin{3t}-4\cos{3t})+C_2e^{-4t}(3\cos{3t}-4\sin{3t})  \require{cancel} \begin{align} 
X(0) & = C_1\cancelto{1}{e^{-4(0)}}\cancelto{1}{\cos{3(0)}}+\cancelto{0}{C_2e^{-4(0)}\sin{3(0)}}+\frac25 \\
0 &= C_1 + \frac 25 \\
C_1 &= -\frac25
\end{align}  X'  \begin{align}
X'(0) &= C_1\cancelto{1}{e^{-4(0)}}(\cancelto{0}{-3\sin{3(0)}})+C_2\cancelto{1}{e^{-4(0)}}(3\cancelto{1}{\cos{3(0)}}-\cancelto{0}{4\sin{3(0)}}) \\
0 &= C_1(-4)+C_2(3) \\
0 &= -4C_1 + 3C_2 \\
3C_2 &= 4C_1 \\
\text{Replacing the C_1 value} \\
C_2 &= \frac{4(-\frac25)}{3} \\
C_2 &= -\frac8{15}
\end{align}  C_1   C_2  X \begin{align}
X &= -\frac25 e^{-4t}\cos{3t}-\frac8{15}e^{-4t}\sin{3t}+\frac25 \\
X &= \frac1{15} \left[ e^{-4t}(-6\cos{3t}-8\sin{3t})+6 \right]
\end{align}",['ordinary-differential-equations']
34,Is it possible to turn a weighted adjacency matrix into an ODE compartment model?,Is it possible to turn a weighted adjacency matrix into an ODE compartment model?,,"I have an idea for a project that hinges on this idea. Lets say we have an adjacency matrix of a DiGraph where the i,j entry represents an out-going edge from node i to node j and at this position we have a rate describing how much of something leaves node i to node j in respect to time. Is there a straight forward way using matrix algebra to convert this matrix to a system of ODEs that describes a multi-compartment model?","I have an idea for a project that hinges on this idea. Lets say we have an adjacency matrix of a DiGraph where the i,j entry represents an out-going edge from node i to node j and at this position we have a rate describing how much of something leaves node i to node j in respect to time. Is there a straight forward way using matrix algebra to convert this matrix to a system of ODEs that describes a multi-compartment model?",,"['linear-algebra', 'ordinary-differential-equations', 'network-flow']"
35,Help in differentiable equation $f'(x)=6x^2 - 4x -f(x+1)$,Help in differentiable equation,f'(x)=6x^2 - 4x -f(x+1),"I am struggling with my first question, HERE (concerned  with integral) During fighting this problem, I met differentiable equation $$  f'(x)=6x^2 - 4x -f(x+1)$$ $$$$ I can solve solve the equation $f'(x)=6x^2 - 4x -\color{purple}{f(x)}$ not hardly... BUT how can I solve the eqation $ {f'(x)=6x^2 - 4x -\color{purple}{f(x+1)}}$ ? $$$$ I've already knocked 'wolframalpha.com ' - but not helpful- wolframalpha says... :-( $$$$","I am struggling with my first question, HERE (concerned  with integral) During fighting this problem, I met differentiable equation $$  f'(x)=6x^2 - 4x -f(x+1)$$ $$$$ I can solve solve the equation $f'(x)=6x^2 - 4x -\color{purple}{f(x)}$ not hardly... BUT how can I solve the eqation $ {f'(x)=6x^2 - 4x -\color{purple}{f(x+1)}}$ ? $$$$ I've already knocked 'wolframalpha.com ' - but not helpful- wolframalpha says... :-( $$$$",,"['calculus', 'ordinary-differential-equations']"
36,Implicit 2nd order Runge-Kutta,Implicit 2nd order Runge-Kutta,,"I'm familiar with explicit numerical methods for solving ODE including Euler's method, and even Runge-Kutta methods (2nd and 4th order). But I'm really confused when it comes to implicit methods. I understood the ''simplest'' implicit method, being Euler's backward method. I even did a code with it on Matlab and it worked just fine. But, what about implicit Runge-Kutta 2nd order? How can I get the K's that would make it implicit? I've been searching for examples and can't find anything. It would really help if I saw a solution using an implicit RK-2 for a ODE. Is there any book/paper or anything? Let's say we have this simple ODE: Y' = -2Y How would an implicit RK-2 work here?","I'm familiar with explicit numerical methods for solving ODE including Euler's method, and even Runge-Kutta methods (2nd and 4th order). But I'm really confused when it comes to implicit methods. I understood the ''simplest'' implicit method, being Euler's backward method. I even did a code with it on Matlab and it worked just fine. But, what about implicit Runge-Kutta 2nd order? How can I get the K's that would make it implicit? I've been searching for examples and can't find anything. It would really help if I saw a solution using an implicit RK-2 for a ODE. Is there any book/paper or anything? Let's say we have this simple ODE: Y' = -2Y How would an implicit RK-2 work here?",,"['ordinary-differential-equations', 'numerical-methods', 'newton-raphson', 'runge-kutta-methods']"
37,Maximal interval of solutions existence: $x'(t)=-x(t)+\sin x(t)+t^3$,Maximal interval of solutions existence:,x'(t)=-x(t)+\sin x(t)+t^3,"$x'(t)=-x(t)+ \sin x(t)+t^3$  in $\mathbb{R}$ I consider the function: $$ f(t,x)=-x+\sin x + t^3 $$ $$\frac{\partial f}{\partial x}=\cos x-1$$ I see that: $$\left| \frac{\partial f}{\partial x} \right| < 2$$ So, the function $f$ is a globally Lipschitz-function, because the partial derivative (considering the second variable) is bounded. For the global existence theorem, the maximal interval of  solutions existence  is $(-\infty,\infty)$ Is it correct? Thanks!","$x'(t)=-x(t)+ \sin x(t)+t^3$  in $\mathbb{R}$ I consider the function: $$ f(t,x)=-x+\sin x + t^3 $$ $$\frac{\partial f}{\partial x}=\cos x-1$$ I see that: $$\left| \frac{\partial f}{\partial x} \right| < 2$$ So, the function $f$ is a globally Lipschitz-function, because the partial derivative (considering the second variable) is bounded. For the global existence theorem, the maximal interval of  solutions existence  is $(-\infty,\infty)$ Is it correct? Thanks!",,"['real-analysis', 'analysis', 'ordinary-differential-equations']"
38,Turing criteria for Sel'kvo glycolysis model,Turing criteria for Sel'kvo glycolysis model,,"I have the Sel'kov reaction diffusion model for glycolysis as follows: \begin{eqnarray} u_t=D_uu_{xx}-u+av+u^2v\\ v_t=D_vv_{xx}+b-av-u^2v \end{eqnarray} How can I obtain the values for $D_u$ and $D_v$ for a pattern formation via Turing criteria. The last equations model's glycolysis in 1D. In two dimensions: \begin{eqnarray} u_t=D_u(u_{xx}+u_{yy})-u+av+u^2v\\ v_t=D_v(v_{xx}+v_{yy})+b-av-u^2v \end{eqnarray} Using the same Turing criteria, how can I obtain the exact analitic forms for $D_u$ and $D_v$ diffusive constants. I'm using the Turing criteria as follows: \begin{eqnarray} p(\sigma)=\mid J-\sigma I-\lambda D\mid=0\\ Re(p(\sigma))<0 \end{eqnarray} where $J$ is the Jacobian matrix evaluated in the EP $(b,\frac{b}{a+b^2})$, $Re(p(\sigma))$ es the real part of the equation and $\lambda$ is the eigenvalue for the linealization and vectorization to obtain a lineal PDE wich is $\lambda=\frac{n^2\pi^2}{L^2}$ and $D$ is the diffusive matrix as follows $D=\begin{pmatrix} D_u & 0\\ 0 & D_v  \end{pmatrix}$. I've trying to obtain the relation for $D_u$ and $D_v$ and I can't do it, not in the $1D$ case or the $2D$ case. Please somebody helps me out.","I have the Sel'kov reaction diffusion model for glycolysis as follows: \begin{eqnarray} u_t=D_uu_{xx}-u+av+u^2v\\ v_t=D_vv_{xx}+b-av-u^2v \end{eqnarray} How can I obtain the values for $D_u$ and $D_v$ for a pattern formation via Turing criteria. The last equations model's glycolysis in 1D. In two dimensions: \begin{eqnarray} u_t=D_u(u_{xx}+u_{yy})-u+av+u^2v\\ v_t=D_v(v_{xx}+v_{yy})+b-av-u^2v \end{eqnarray} Using the same Turing criteria, how can I obtain the exact analitic forms for $D_u$ and $D_v$ diffusive constants. I'm using the Turing criteria as follows: \begin{eqnarray} p(\sigma)=\mid J-\sigma I-\lambda D\mid=0\\ Re(p(\sigma))<0 \end{eqnarray} where $J$ is the Jacobian matrix evaluated in the EP $(b,\frac{b}{a+b^2})$, $Re(p(\sigma))$ es the real part of the equation and $\lambda$ is the eigenvalue for the linealization and vectorization to obtain a lineal PDE wich is $\lambda=\frac{n^2\pi^2}{L^2}$ and $D$ is the diffusive matrix as follows $D=\begin{pmatrix} D_u & 0\\ 0 & D_v  \end{pmatrix}$. I've trying to obtain the relation for $D_u$ and $D_v$ and I can't do it, not in the $1D$ case or the $2D$ case. Please somebody helps me out.",,"['real-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'biology']"
39,Solve the differential equation : $0.5 \frac{dy}{dx}=4.9-0.1y^2$,Solve the differential equation :,0.5 \frac{dy}{dx}=4.9-0.1y^2,The question is to solve the differential equation : $$0.5 \frac{dy}{dx}=4.9-0.1y^2$$ What I have attempted: $$0.5 \frac{dy}{dx}=4.9-0.1y^2$$ $$ \frac{dy}{dx} = \frac{4.9-0.1y^2}{0.5} $$ $$ \frac{dy}{dx} = 9.8 - 0.2y^2 $$ $$ \int \frac{1}{9.8 - 0.2y^2} dy = \int 1 dx $$ $$ \int \frac{10}{98 - 2y^2} dy = \int 1 dx $$ $$ \int \frac{5}{49 - y^2} dy = \int 1 dx $$ $$ \int \frac{1}{49 - y^2} dy = \int \frac{1}{5} dx $$ How should I continue? EDIT Thanks @andrenicolas for the hint By using partial fractions $$ \int \frac{1}{(7-y)(7+y)} dy = \int \frac{1}{5} dx $$ $$ \frac{1}{14}\int \frac{1}{7-y} + \frac {1}{7+y} dy = \int \frac{1}{5} dx $$ $$ \frac{1}{14} [ ln |y+7| - ln|y-7| ] =  \frac{x}{5} + c$$ $$ [ ln |y+7| - ln|y-7| ] = \frac{14x}{5} + 14c $$ $$ ln\frac{y+7}{y-7} = \frac{14x}{5} + 14c $$ $$ \frac{y+7}{y-7} = Ae^{\frac{14x}{5}} $$ (Letting $A = 14c$ ) $$ y + 7 = yAe^{\frac{14x}{5}} - 7Ae^{\frac{14x}{5}} $$ $$ 7Ae^{\frac{14x}{5}} + 7 = yAe^{\frac{14x}{5}} - y $$ $$ 7Ae^{\frac{14x}{5}} + 7 = y(Ae^{\frac{14x}{5}} - 1) $$ $$ y = \frac{7Ae^{\frac{14x}{5}}}{(Ae^{\frac{14x}{5}} - 1)} $$,The question is to solve the differential equation : What I have attempted: How should I continue? EDIT Thanks @andrenicolas for the hint By using partial fractions (Letting ),0.5 \frac{dy}{dx}=4.9-0.1y^2 0.5 \frac{dy}{dx}=4.9-0.1y^2  \frac{dy}{dx} = \frac{4.9-0.1y^2}{0.5}   \frac{dy}{dx} = 9.8 - 0.2y^2   \int \frac{1}{9.8 - 0.2y^2} dy = \int 1 dx   \int \frac{10}{98 - 2y^2} dy = \int 1 dx   \int \frac{5}{49 - y^2} dy = \int 1 dx   \int \frac{1}{49 - y^2} dy = \int \frac{1}{5} dx   \int \frac{1}{(7-y)(7+y)} dy = \int \frac{1}{5} dx   \frac{1}{14}\int \frac{1}{7-y} + \frac {1}{7+y} dy = \int \frac{1}{5} dx   \frac{1}{14} [ ln |y+7| - ln|y-7| ] =  \frac{x}{5} + c  [ ln |y+7| - ln|y-7| ] = \frac{14x}{5} + 14c   ln\frac{y+7}{y-7} = \frac{14x}{5} + 14c   \frac{y+7}{y-7} = Ae^{\frac{14x}{5}}  A = 14c  y + 7 = yAe^{\frac{14x}{5}} - 7Ae^{\frac{14x}{5}}   7Ae^{\frac{14x}{5}} + 7 = yAe^{\frac{14x}{5}} - y   7Ae^{\frac{14x}{5}} + 7 = y(Ae^{\frac{14x}{5}} - 1)   y = \frac{7Ae^{\frac{14x}{5}}}{(Ae^{\frac{14x}{5}} - 1)} ,"['integration', 'ordinary-differential-equations']"
40,A simple problem on first order differential equations,A simple problem on first order differential equations,,"An ODE (Ordinary Differential Equation) of order $n$ becomes a relation: $$F(x,y,y^{(1)},...,y^{(n)})=0$$ Then $F(x,y,y^{(1)})=0$ defines an ODE of order one. In ""basic standard texts"", for purposes of simplicity, is assumed that some ODE of first order can take the form: $$y^{(1)}=f(x,y)$$ for certain suitable $f$. Here is my ""silly"" question: What if that assumpion is not possible? For example how I can deal with equations of the form: $${(y^{(1)})}^5+sen(y^{(1)})+e^{y^{(1)}} + x=0$$ I appreciate any reference. Thanks in advance for your comments!","An ODE (Ordinary Differential Equation) of order $n$ becomes a relation: $$F(x,y,y^{(1)},...,y^{(n)})=0$$ Then $F(x,y,y^{(1)})=0$ defines an ODE of order one. In ""basic standard texts"", for purposes of simplicity, is assumed that some ODE of first order can take the form: $$y^{(1)}=f(x,y)$$ for certain suitable $f$. Here is my ""silly"" question: What if that assumpion is not possible? For example how I can deal with equations of the form: $${(y^{(1)})}^5+sen(y^{(1)})+e^{y^{(1)}} + x=0$$ I appreciate any reference. Thanks in advance for your comments!",,"['calculus', 'ordinary-differential-equations']"
41,Solving $(4y+2x-5)dx+(6y+4x-1)dy=0$ using 2 methods produced 2 different answers!,Solving  using 2 methods produced 2 different answers!,(4y+2x-5)dx+(6y+4x-1)dy=0,"$$(4y+2x-5)dx+(6y+4x-1)dy=0,y(-1)=2$$ First method: $$\frac{dy}{dx}=-\frac{4y+2x-5}{6y+4x-1}$$ let $Y=y-\frac{9}{2}$, $dY=dy$  and $X=x+\frac{13}{2}$, $dX=dx$; $$\frac{dY}{dX}=-\frac{4Y+2X}{6Y+4X}$$ let $u=\frac{Y}{X}$ , $Y'=u'X+u$; $$u'X+u=-\frac{4u+2}{6u+4}$$ $$u'X=\frac{-6u^2-8u-2}{6u+4}$$ $$\int\frac{(6u+4)du}{6u^2+8u+2}=\int-\frac{dX}{X}$$ $$\frac{1}{2}\ln|3u^2+4u+1|=-\ln|X|+Constant$$ $$\frac{1}{2}\ln|3(\frac{y-\frac{9}{2}}{x+\frac{13}{2}})^2+4(\frac{y-\frac{9}{2}}{x+\frac{13}{2}})+1|=-\ln|x+\frac{13}{2}|+Constant$$ Solving for $y(-1)=2$, $Constant=0.89587977346$; $$\frac{1}{2}\ln|3(\frac{y-\frac{9}{2}}{x+\frac{13}{2}})^2+4(\frac{y-\frac{9}{2}}{x+\frac{13}{2}})+1|=-\ln|x+\frac{13}{2}|+0.89587977346$$ Second method: let $M=4y+2x-5$ , $N=6y+4x-1$ $$\frac{\partial M}{\partial y}=\frac{\partial N}{\partial x}=4\rightarrow Exact$$ $$\varnothing=4xy+x^2-5x+3y^2-y=Constant$$ Solving for $y(-1)=2$, $Constant=8$; $$\varnothing=4xy+x^2-5x+3y^2-y=8$$ I plotted those two answers on MATHEMATICA and found that result: How could the results be different?","$$(4y+2x-5)dx+(6y+4x-1)dy=0,y(-1)=2$$ First method: $$\frac{dy}{dx}=-\frac{4y+2x-5}{6y+4x-1}$$ let $Y=y-\frac{9}{2}$, $dY=dy$  and $X=x+\frac{13}{2}$, $dX=dx$; $$\frac{dY}{dX}=-\frac{4Y+2X}{6Y+4X}$$ let $u=\frac{Y}{X}$ , $Y'=u'X+u$; $$u'X+u=-\frac{4u+2}{6u+4}$$ $$u'X=\frac{-6u^2-8u-2}{6u+4}$$ $$\int\frac{(6u+4)du}{6u^2+8u+2}=\int-\frac{dX}{X}$$ $$\frac{1}{2}\ln|3u^2+4u+1|=-\ln|X|+Constant$$ $$\frac{1}{2}\ln|3(\frac{y-\frac{9}{2}}{x+\frac{13}{2}})^2+4(\frac{y-\frac{9}{2}}{x+\frac{13}{2}})+1|=-\ln|x+\frac{13}{2}|+Constant$$ Solving for $y(-1)=2$, $Constant=0.89587977346$; $$\frac{1}{2}\ln|3(\frac{y-\frac{9}{2}}{x+\frac{13}{2}})^2+4(\frac{y-\frac{9}{2}}{x+\frac{13}{2}})+1|=-\ln|x+\frac{13}{2}|+0.89587977346$$ Second method: let $M=4y+2x-5$ , $N=6y+4x-1$ $$\frac{\partial M}{\partial y}=\frac{\partial N}{\partial x}=4\rightarrow Exact$$ $$\varnothing=4xy+x^2-5x+3y^2-y=Constant$$ Solving for $y(-1)=2$, $Constant=8$; $$\varnothing=4xy+x^2-5x+3y^2-y=8$$ I plotted those two answers on MATHEMATICA and found that result: How could the results be different?",,['ordinary-differential-equations']
42,How could I calculate $\lim_{t\rightarrow 0}\frac{x(t)}{t^{\sqrt{3}}}$ for the following,How could I calculate  for the following,\lim_{t\rightarrow 0}\frac{x(t)}{t^{\sqrt{3}}},"If $x(t)$ satisfy $t^2x''+tx'+(t^2-3)x=0$ then what is the limit $$ \lim_{t\rightarrow 0}\frac{x(t)}{t^{\sqrt{3}}}$$ It is very important to me, hint or full help, please. I know that the solution to the differential equation is a Bessel function so its solution are determinate by $$\{ J_\sqrt{3}(t)\ ;\ J_{-\sqrt{3}}(t)\}\ \ \ \ (because \ \alpha\not\in\mathbb{Z})$$ but I have big problem when I work with $J$ bessel. I need help please.","If $x(t)$ satisfy $t^2x''+tx'+(t^2-3)x=0$ then what is the limit $$ \lim_{t\rightarrow 0}\frac{x(t)}{t^{\sqrt{3}}}$$ It is very important to me, hint or full help, please. I know that the solution to the differential equation is a Bessel function so its solution are determinate by $$\{ J_\sqrt{3}(t)\ ;\ J_{-\sqrt{3}}(t)\}\ \ \ \ (because \ \alpha\not\in\mathbb{Z})$$ but I have big problem when I work with $J$ bessel. I need help please.",,"['calculus', 'ordinary-differential-equations', 'limits', 'bessel-functions']"
43,A 2nd order nonlinear ODE with one boundary and two algebrac equation constraints,A 2nd order nonlinear ODE with one boundary and two algebrac equation constraints,,How to solve the following nonlinear ODE with two algebraic equations and one  boundary condition? $$y''(x)=\dfrac{2\left((x+15)y'(x)-y(x)\right)\left(y'(x)^2+1\right)}{\left(y(x)^2+x(x+30)+236\right)^2}$$ The boundary condition: $$y(-14)=0$$ The algebraic equation constraint: $$\left\{ \begin{array}{ll}  y(x_0)=\sqrt{1-x_0^2} &\\[15pt]  y'(x_0)=\dfrac{-x_0}{\sqrt{1-x_0^2}}& \text{where: }-1\lt x_0\lt 0 \\ \end{array} \right.$$,How to solve the following nonlinear ODE with two algebraic equations and one  boundary condition? $$y''(x)=\dfrac{2\left((x+15)y'(x)-y(x)\right)\left(y'(x)^2+1\right)}{\left(y(x)^2+x(x+30)+236\right)^2}$$ The boundary condition: $$y(-14)=0$$ The algebraic equation constraint: $$\left\{ \begin{array}{ll}  y(x_0)=\sqrt{1-x_0^2} &\\[15pt]  y'(x_0)=\dfrac{-x_0}{\sqrt{1-x_0^2}}& \text{where: }-1\lt x_0\lt 0 \\ \end{array} \right.$$,,"['ordinary-differential-equations', 'nonlinear-system', 'boundary-value-problem']"
44,"How can I solve this PDE: $u_{tt}(\mathbf{x},t)+ku_t(\mathbf{x},t)-c^2 \Delta u(\mathbf{x},t)=0$",How can I solve this PDE:,"u_{tt}(\mathbf{x},t)+ku_t(\mathbf{x},t)-c^2 \Delta u(\mathbf{x},t)=0","Consider the wave equation: $$u_{tt}(\mathbf{x},t)+ku_t(\mathbf{x},t)-c^2 \Delta u(\mathbf{x},t)=0$$ $\mathbf{x}\in\Bbb R^2, t>0 \\ \space \\ u(\mathbf{x},0)=0 \\ u_t(\mathbf{x},0)=\psi(\mathbf{x})$ a) Determine $\alpha \in \Bbb R$ such that $v(\mathbf{x},t):=e^{\alpha t}u(\mathbf{x},t)$ satisfies a first order PDE without first order derivatives on $\Bbb R^2$ b) Determine a $\beta \in \Bbb R$ such that $w(x_1,x_2,x_3,t)=w(\mathbf{x},x_3,t):=e^{\beta x_3}v(\mathbf{x},t)$ satisfies a PDE on $\Bbb R^2$ with second order derivatives only c) Show that the solution of the PDE is: $$u(\mathbf{x},t)=\frac{e^{-kt/2}}{2 \pi c} \int_{\lvert y \rvert >ct}d^2y \frac{\cosh(\frac{k}{2c}\sqrt{c^2t^2-\lvert y \rvert^2})}{\sqrt{c^2t^2-\lvert y \rvert^2}}\psi(x+y)$$ I don't even know how to start solving this problem. a) and b) doesn't make any sense to me. Can anyone explain why I am trying to find $\alpha$ and $\beta$ and what these constants mean? c) Looks a bit like the 2-D Poisson-Formula but as far as I understand the Poisson-Formula only applies to PDE's without a dissipation term. I also don't understand how a) and b) will lead me to figuring out c). Any ideas?","Consider the wave equation: $$u_{tt}(\mathbf{x},t)+ku_t(\mathbf{x},t)-c^2 \Delta u(\mathbf{x},t)=0$$ $\mathbf{x}\in\Bbb R^2, t>0 \\ \space \\ u(\mathbf{x},0)=0 \\ u_t(\mathbf{x},0)=\psi(\mathbf{x})$ a) Determine $\alpha \in \Bbb R$ such that $v(\mathbf{x},t):=e^{\alpha t}u(\mathbf{x},t)$ satisfies a first order PDE without first order derivatives on $\Bbb R^2$ b) Determine a $\beta \in \Bbb R$ such that $w(x_1,x_2,x_3,t)=w(\mathbf{x},x_3,t):=e^{\beta x_3}v(\mathbf{x},t)$ satisfies a PDE on $\Bbb R^2$ with second order derivatives only c) Show that the solution of the PDE is: $$u(\mathbf{x},t)=\frac{e^{-kt/2}}{2 \pi c} \int_{\lvert y \rvert >ct}d^2y \frac{\cosh(\frac{k}{2c}\sqrt{c^2t^2-\lvert y \rvert^2})}{\sqrt{c^2t^2-\lvert y \rvert^2}}\psi(x+y)$$ I don't even know how to start solving this problem. a) and b) doesn't make any sense to me. Can anyone explain why I am trying to find $\alpha$ and $\beta$ and what these constants mean? c) Looks a bit like the 2-D Poisson-Formula but as far as I understand the Poisson-Formula only applies to PDE's without a dissipation term. I also don't understand how a) and b) will lead me to figuring out c). Any ideas?",,"['ordinary-differential-equations', 'partial-differential-equations']"
45,Literature on the differential operator $-\frac{d^2}{dx^2}-\frac{2\nu+1}{x}\frac{d}{dx} + x^2$,Literature on the differential operator,-\frac{d^2}{dx^2}-\frac{2\nu+1}{x}\frac{d}{dx} + x^2,"I'm trying to find any possible literature on the differential operator $-\frac{d^2}{dx^2}-\frac{2\nu+1}{x}\frac{d}{dx} + x^2$. This is constructed out of the Fourier-Bessel differential operator and a harmonic piece. In the case that $2\nu+1=n-1$ is a natural number, this can be understood as the radial part of a Laplacian in $\mathbb{R}^n$ plus a quadratic potential, i.e. a quantum harmonic oscillator in $\mathbb{R}^n$ that is spherically symmetric. I've searched around for a while, but I've come up short. It may be that there is no literature outside of the case of $n=1,2,3$ (and $4$ here or there).","I'm trying to find any possible literature on the differential operator $-\frac{d^2}{dx^2}-\frac{2\nu+1}{x}\frac{d}{dx} + x^2$. This is constructed out of the Fourier-Bessel differential operator and a harmonic piece. In the case that $2\nu+1=n-1$ is a natural number, this can be understood as the radial part of a Laplacian in $\mathbb{R}^n$ plus a quadratic potential, i.e. a quantum harmonic oscillator in $\mathbb{R}^n$ that is spherically symmetric. I've searched around for a while, but I've come up short. It may be that there is no literature outside of the case of $n=1,2,3$ (and $4$ here or there).",,"['ordinary-differential-equations', 'reference-request']"
46,What is an exact differential equation?,What is an exact differential equation?,,"During A levels, we discussed a type of differential equation which we called exact . By this, we meant a differential equation that had its LHS obtainable by differentiating a product. For example the DE $$x^2\cos y\frac{\text{d} y}{\text{d} x}+2x\sin y=\frac{1}{x^2}$$ is exact because we can look at it this way: $~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$ thus obtaining $$\frac{\text{d}}{\text{d}x}\left(x^2\sin y\right)=\frac{1}{x^2}$$ which can easily be solved by integrating. However now, in university, we discuss exact equations in the form $$M(x,y)dx+N(x,y)dy=0$$ which are said to be exact if and only if $\forall (x,y)\in\mathbb R^2$ : $$\frac{\partial M}{\partial y}(x,y)=\frac{\partial N}{\partial x}(x,y)$$ Is there a correlation between the two? Are they the same thing but I'm not realising it?","During A levels, we discussed a type of differential equation which we called exact . By this, we meant a differential equation that had its LHS obtainable by differentiating a product. For example the DE is exact because we can look at it this way: thus obtaining which can easily be solved by integrating. However now, in university, we discuss exact equations in the form which are said to be exact if and only if : Is there a correlation between the two? Are they the same thing but I'm not realising it?","x^2\cos y\frac{\text{d} y}{\text{d} x}+2x\sin y=\frac{1}{x^2} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \frac{\text{d}}{\text{d}x}\left(x^2\sin y\right)=\frac{1}{x^2} M(x,y)dx+N(x,y)dy=0 \forall (x,y)\in\mathbb R^2 \frac{\partial M}{\partial y}(x,y)=\frac{\partial N}{\partial x}(x,y)",['ordinary-differential-equations']
47,Dirichlet problem to the ball with boundary data $1-2y^2$.,Dirichlet problem to the ball with boundary data .,1-2y^2,"Let $\omega=\{(x,y):x^2+y^2<1\}$ be the open unit disk in $\mathbb R^2$ with the boundary $\delta\omega$.If $u(x,y)$ be the solution of Dirichlet problem $$\begin{cases} u_{xx}+ u_{yy}=0 & \text{in} \ \omega \\ u(x,y)=1-2y^2 & \text{on the boundary.} \end{cases} $$ Then $u(1/2,0)=?$ $-1$ $-1/4$ $1$ $1/4$ I have no idea how to find the solution. Does there exist a simple way to find solution? please someone help. Thanks.","Let $\omega=\{(x,y):x^2+y^2<1\}$ be the open unit disk in $\mathbb R^2$ with the boundary $\delta\omega$.If $u(x,y)$ be the solution of Dirichlet problem $$\begin{cases} u_{xx}+ u_{yy}=0 & \text{in} \ \omega \\ u(x,y)=1-2y^2 & \text{on the boundary.} \end{cases} $$ Then $u(1/2,0)=?$ $-1$ $-1/4$ $1$ $1/4$ I have no idea how to find the solution. Does there exist a simple way to find solution? please someone help. Thanks.",,"['ordinary-differential-equations', 'harmonic-functions']"
48,Problem with differential equation RLC circuit series,Problem with differential equation RLC circuit series,,"I am trying solve the differential equation of RLC's circuit in series, I have: $C=4\ F, L= 1\ H$, $R=5\ \Omega$, and $V_e=20\ V$. $1)$ first I got the equation, it is: $i''+5i'+\frac{1}{4}i=0$, what I have to calculate is $v_c$, and I know that $i(0)=-2\ A$ and $v_c(0)=10\ V$ I have calculated the characteristic polynomial and I got that a fundamental system is $\{ e^{-0.051t}; e^{-4.950t}\}$, so $$i=Ae^{-0.051t}+Be^{-4.950t}$$ And now I could calculate $v_c=\int_{0}^{t}idt$ Ok, first problem !... I got $$v_c=-4.950A(e^{-0.051t}-1)-0.051B(e^{-4.950t}-1),$$ SO... when I said that $v_c(0)=10=0-0 $ what happen here? Ok, now I dont said $\int_0^t$, no. I consider $\int$ only, so I got $$v_c=-4.950A(e^{-0.051t})-0.051B(e^{-4.950t})$$ And when I said that $v_c(0)=0$ I got that $$A=-2.020\ B=0.021,$$ in that, $$ v_c\approx10e^{-0.051t}$$ but in my but said that $v_c=20+0.102e^{-4.950t}-10.102e^{-0.051t}\ [V]$, so what happen, I need help please... please... PD; To solve can not use the formula that we all know , the problem is solved by mathematical methods. I need help...","I am trying solve the differential equation of RLC's circuit in series, I have: $C=4\ F, L= 1\ H$, $R=5\ \Omega$, and $V_e=20\ V$. $1)$ first I got the equation, it is: $i''+5i'+\frac{1}{4}i=0$, what I have to calculate is $v_c$, and I know that $i(0)=-2\ A$ and $v_c(0)=10\ V$ I have calculated the characteristic polynomial and I got that a fundamental system is $\{ e^{-0.051t}; e^{-4.950t}\}$, so $$i=Ae^{-0.051t}+Be^{-4.950t}$$ And now I could calculate $v_c=\int_{0}^{t}idt$ Ok, first problem !... I got $$v_c=-4.950A(e^{-0.051t}-1)-0.051B(e^{-4.950t}-1),$$ SO... when I said that $v_c(0)=10=0-0 $ what happen here? Ok, now I dont said $\int_0^t$, no. I consider $\int$ only, so I got $$v_c=-4.950A(e^{-0.051t})-0.051B(e^{-4.950t})$$ And when I said that $v_c(0)=0$ I got that $$A=-2.020\ B=0.021,$$ in that, $$ v_c\approx10e^{-0.051t}$$ but in my but said that $v_c=20+0.102e^{-4.950t}-10.102e^{-0.051t}\ [V]$, so what happen, I need help please... please... PD; To solve can not use the formula that we all know , the problem is solved by mathematical methods. I need help...",,"['ordinary-differential-equations', 'physics']"
49,"Given triangle ABC, how to move point B to a certain angle given that its new location lies within the direction of its old altitude.","Given triangle ABC, how to move point B to a certain angle given that its new location lies within the direction of its old altitude.",,"I have a 2D coordinate system for 3 known points $A$, $B$, $C$. Given that I can only move point $B$, how can I compute for its new coordinate with a certain angle $\theta$ considering that its new location can be found along the axis of its old altitude? Fig. 1: Point B having 110° angle Fig. 2: New location for B where $\theta = 90°$ In the example above, Fig. 1 shows that the initial angle at point $B$ is $110°$. If input $\theta = 90°$, then point $B'$ will be located somewhere along the x-axis (where the altitude in the current example is located). Input $\theta$ can be any angle from $1°$ to $180°$. Is there a way to compute for the coordinate of B' given that we know the direction of the altitude of B?","I have a 2D coordinate system for 3 known points $A$, $B$, $C$. Given that I can only move point $B$, how can I compute for its new coordinate with a certain angle $\theta$ considering that its new location can be found along the axis of its old altitude? Fig. 1: Point B having 110° angle Fig. 2: New location for B where $\theta = 90°$ In the example above, Fig. 1 shows that the initial angle at point $B$ is $110°$. If input $\theta = 90°$, then point $B'$ will be located somewhere along the x-axis (where the altitude in the current example is located). Input $\theta$ can be any angle from $1°$ to $180°$. Is there a way to compute for the coordinate of B' given that we know the direction of the altitude of B?",,"['calculus', 'linear-algebra', 'geometry', 'ordinary-differential-equations', 'vector-spaces']"
50,Which initial functions can be solved by separation of variables,Which initial functions can be solved by separation of variables,,"consider the wave equation on $(0,\pi)$, i.e. let $f,g\in L^2(0,\pi)$ be two fixed functions and consider the following problem: $$\partial_{tt}u=\partial_{xx}u, \quad x\in(0,\pi), t\in (0,\infty) \\ u(0,t)=u(\pi,t)=0, \\ u(x,0)=f(x);\quad \partial_t u=h.$$ When we separate variables we get the following special solutions of the differential equation $u_n(x,t)=\sin(nx)\left( A_n\cos(nt)+B_n\sin(nt) \right)$ satisfying the boundary conditions $u(0,t)=u(\pi,t)=0$. Obviously any finite linear combination $u(x,t)=\sum_{1}^n u_n$ of the functions $u_n$ will still satisfy the PDE and the boundary conditions, even though one cannot always - depending of course on our choice of f and g- find constants $A_n, B_n$ such that $u$ will satisfy the initial conditions. I am wondering what happens if one ""sums up"" all the functions $u_n$, i.e. if we consider $u_{\infty}(x,t):=\sum_{1}^{\infty}u_n(x,t)$? Then at least we can find coefficients $A_n, B_n$ such that the initial conditions are satisfied in the $L^2$ sense, for instance $u(x,0)$ will be equal to $f$ in the space $L^2.$ On the other hand - when we do consider infinite sequences- the question of differentiability of $u_{\infty}$ is unclear to me. Q:Is this function $u_{\infty}(x,t)$ a differentiable function and a solution of the PDE? Are there any instructive examples where this infinite sum happens to be not differentiable etc? Further, I would like to know how ""bad"" the initial functions can be choosen so that $u_{\infty}$ will satisfies them pointwise, i.e. $u_{\infty}(x,0)=f(x,0), \partial_t u_{\infty}(x,0)=g(x,0)$ (pointwise, for some $A_n,B_n$)? Maybe there are instructive examples for this problem? I would also very appreciate if someone can tell me good books which clarifies questions in the spirit of the above ones. Best wishes","consider the wave equation on $(0,\pi)$, i.e. let $f,g\in L^2(0,\pi)$ be two fixed functions and consider the following problem: $$\partial_{tt}u=\partial_{xx}u, \quad x\in(0,\pi), t\in (0,\infty) \\ u(0,t)=u(\pi,t)=0, \\ u(x,0)=f(x);\quad \partial_t u=h.$$ When we separate variables we get the following special solutions of the differential equation $u_n(x,t)=\sin(nx)\left( A_n\cos(nt)+B_n\sin(nt) \right)$ satisfying the boundary conditions $u(0,t)=u(\pi,t)=0$. Obviously any finite linear combination $u(x,t)=\sum_{1}^n u_n$ of the functions $u_n$ will still satisfy the PDE and the boundary conditions, even though one cannot always - depending of course on our choice of f and g- find constants $A_n, B_n$ such that $u$ will satisfy the initial conditions. I am wondering what happens if one ""sums up"" all the functions $u_n$, i.e. if we consider $u_{\infty}(x,t):=\sum_{1}^{\infty}u_n(x,t)$? Then at least we can find coefficients $A_n, B_n$ such that the initial conditions are satisfied in the $L^2$ sense, for instance $u(x,0)$ will be equal to $f$ in the space $L^2.$ On the other hand - when we do consider infinite sequences- the question of differentiability of $u_{\infty}$ is unclear to me. Q:Is this function $u_{\infty}(x,t)$ a differentiable function and a solution of the PDE? Are there any instructive examples where this infinite sum happens to be not differentiable etc? Further, I would like to know how ""bad"" the initial functions can be choosen so that $u_{\infty}$ will satisfies them pointwise, i.e. $u_{\infty}(x,0)=f(x,0), \partial_t u_{\infty}(x,0)=g(x,0)$ (pointwise, for some $A_n,B_n$)? Maybe there are instructive examples for this problem? I would also very appreciate if someone can tell me good books which clarifies questions in the spirit of the above ones. Best wishes",,"['ordinary-differential-equations', 'partial-differential-equations', 'fourier-series', 'boundary-value-problem', 'wave-equation']"
51,Solve ODE by using Fourier Transform,Solve ODE by using Fourier Transform,,"$$- u''(x)+u(x)=f(x)$$ for every $x \in \mathbb{R}$ with $\lim_{|x| \to \infty}u(x)=0, \lim_{|x| \to \infty}u'(x)=0$ and $f \in  L^1(\mathbb{R}) \cap L^2(\mathbb{R})$. I need to solve this by using Fourier Transform. It becomes more complicated since f(x) it is not defined but u(x) has to fulfill above conditions. Can somebody solve this or give more than a short hint?","$$- u''(x)+u(x)=f(x)$$ for every $x \in \mathbb{R}$ with $\lim_{|x| \to \infty}u(x)=0, \lim_{|x| \to \infty}u'(x)=0$ and $f \in  L^1(\mathbb{R}) \cap L^2(\mathbb{R})$. I need to solve this by using Fourier Transform. It becomes more complicated since f(x) it is not defined but u(x) has to fulfill above conditions. Can somebody solve this or give more than a short hint?",,"['ordinary-differential-equations', 'fourier-analysis']"
52,Reducing a system of non-linear ODEs to a single ODE,Reducing a system of non-linear ODEs to a single ODE,,"Given the system; $$\frac{dX}{dt}=-\alpha XY$$ $$\frac{dY}{dt}=\alpha XY-\beta Y$$ $\alpha$ and $\beta$ are constants I need to reduce this system to a single ODE in  $X$ and $Y$ and solve analytically. The initial conditions being $X(0)=X_0,$ $Y(0)=Y_0$ My work: I believe this system to be autonomous as neither ODE involves time explicitly on the RHS, so my thoughts are we can eliminate time and yield an expression relating $X$ and $Y$. That is, $$\frac{dX}{dY}=\frac{\frac{dX}{dt}}{\frac{dY}{dt}}$$ $$\frac{dX}{dY}=\frac{-\alpha XY}{\alpha XY-\beta Y}=\frac{-\alpha X}{\alpha X-\beta}$$ We can then attempt to solve this through the separation of variables; $$\int-1+\frac{\beta}{\alpha X}\ dX=\int dY$$ This could be the completely wrong direction but I always feel its good to add some of my own work, any prompts appreciated. I know these non-linear systems often don't have analytic solutions and require some form of linearization but given I have been asked to find it I assume one exists.","Given the system; $$\frac{dX}{dt}=-\alpha XY$$ $$\frac{dY}{dt}=\alpha XY-\beta Y$$ $\alpha$ and $\beta$ are constants I need to reduce this system to a single ODE in  $X$ and $Y$ and solve analytically. The initial conditions being $X(0)=X_0,$ $Y(0)=Y_0$ My work: I believe this system to be autonomous as neither ODE involves time explicitly on the RHS, so my thoughts are we can eliminate time and yield an expression relating $X$ and $Y$. That is, $$\frac{dX}{dY}=\frac{\frac{dX}{dt}}{\frac{dY}{dt}}$$ $$\frac{dX}{dY}=\frac{-\alpha XY}{\alpha XY-\beta Y}=\frac{-\alpha X}{\alpha X-\beta}$$ We can then attempt to solve this through the separation of variables; $$\int-1+\frac{\beta}{\alpha X}\ dX=\int dY$$ This could be the completely wrong direction but I always feel its good to add some of my own work, any prompts appreciated. I know these non-linear systems often don't have analytic solutions and require some form of linearization but given I have been asked to find it I assume one exists.",,['ordinary-differential-equations']
53,"solve $-u''(x)+\int_0^{\pi}u(y)dy=\lambda u(x)$,where $u(0)=u(\pi)=0$","solve ,where",-u''(x)+\int_0^{\pi}u(y)dy=\lambda u(x) u(0)=u(\pi)=0,"Consider the eigenvalue problem $$-u''(x)+\int_0^{\pi}u(y)dy=\lambda u(x)$$for all $ 0\le x\le\pi$, $\lambda\in\mathbb{R}$ and with $u(0)=u(\pi)=0$. How to find all the solutions $u$ (in $H^1(0,\pi)$ I think) of this eigenvalue problem? My first try was to consider $u(x)=a \sin(kx)+b \cos(kx)$ and find out conditions on $a,b,k\in \mathbb{R}$. But I'm not sure if I receive all solutions with this method. Do you know how to proceed?","Consider the eigenvalue problem $$-u''(x)+\int_0^{\pi}u(y)dy=\lambda u(x)$$for all $ 0\le x\le\pi$, $\lambda\in\mathbb{R}$ and with $u(0)=u(\pi)=0$. How to find all the solutions $u$ (in $H^1(0,\pi)$ I think) of this eigenvalue problem? My first try was to consider $u(x)=a \sin(kx)+b \cos(kx)$ and find out conditions on $a,b,k\in \mathbb{R}$. But I'm not sure if I receive all solutions with this method. Do you know how to proceed?",,"['functional-analysis', 'ordinary-differential-equations']"
54,"The initial value problem $y'=\sqrt {y}, \:\: y(0)=\alpha$.",The initial value problem .,"y'=\sqrt {y}, \:\: y(0)=\alpha",The initial value problem $$\begin{cases}y'=\sqrt {y}\\y(0)=\alpha\end{cases}$$ has $A.$ At least two solutions if $\alpha =0$ $B.$ No solution if $\alpha > 0$ $C.$ At least one solution if $\alpha > 0$ $D.$ A unique solution if $\alpha = 0$ ATTEMPT I solved the differential equation as $y(x) = \frac{x^2}{4} + \alpha +x\sqrt {\alpha}$ How do i choose correct options?,The initial value problem $$\begin{cases}y'=\sqrt {y}\\y(0)=\alpha\end{cases}$$ has $A.$ At least two solutions if $\alpha =0$ $B.$ No solution if $\alpha > 0$ $C.$ At least one solution if $\alpha > 0$ $D.$ A unique solution if $\alpha = 0$ ATTEMPT I solved the differential equation as $y(x) = \frac{x^2}{4} + \alpha +x\sqrt {\alpha}$ How do i choose correct options?,,['ordinary-differential-equations']
55,"Least positive eigenvalue of the BVP $y''-\lambda y'+\frac{2\lambda-1}{x}y=0$, $y(0) = y(1/2) = 0$","Least positive eigenvalue of the BVP ,",y''-\lambda y'+\frac{2\lambda-1}{x}y=0 y(0) = y(1/2) = 0,"Find the first positive eigenvalue $\lambda$ of the boundary value problem over $x\in [0,\frac{1}{2}]$.   $$y''-\lambda y'+\frac{2\lambda-1}{x}y=0, \quad y(0)=y(\tfrac{1}{2})=0.$$ My approach: I have tried to use Frobenius Theorem because $x=0$ is a regular-singular point and also the indicial equation implies that the eigenfunction (non-trivial solution) will not a similar form of a Bessel function. I have managed to use self-adjoint properties but the differential operator of the left hand side turns out to be non self-adjoint.","Find the first positive eigenvalue $\lambda$ of the boundary value problem over $x\in [0,\frac{1}{2}]$.   $$y''-\lambda y'+\frac{2\lambda-1}{x}y=0, \quad y(0)=y(\tfrac{1}{2})=0.$$ My approach: I have tried to use Frobenius Theorem because $x=0$ is a regular-singular point and also the indicial equation implies that the eigenfunction (non-trivial solution) will not a similar form of a Bessel function. I have managed to use self-adjoint properties but the differential operator of the left hand side turns out to be non self-adjoint.",,"['ordinary-differential-equations', 'asymptotics', 'boundary-value-problem', 'eigenfunctions']"
56,Stability and type of equilibria of 2nd-order DEs,Stability and type of equilibria of 2nd-order DEs,,"Given $\ddot{x} = F(x)$, $x(0) = x_0$ and $\dot{x}(0) = y_0$. Assume $c$ is a simple zero of $F$. Let $V(x) = -\int_{x_0}^{x} F(s)ds$. (a) If $V$ has a local min at $c$, find the stability and type of the equilibrium (i.e, saddle, focus, center, center-focus, other) at the point $(x_0, y_0) = (c, 0)$. Use $E(x) = \frac{\dot{x}^{2}}{2} + V(x)$ to explain your answer. What is the stability and type if $V$ has a local max. Explain? (b) Find the type and stability of the equilibria of $\ \ddot{x} = -4x(1-x^2)$. (c) Find the type and stability of $x = \dot{x} = 0$ for the equation: $\ \ddot{x} = -4x(1-x^2) - x^2\dot{x}$ My attempt: For part (a), let $\dot{x} = y$. It's easy to see that we can rewrite the 2nd-order ODE as the system of 1st-order ODE: $\dot{x} = y, \dot{y} = F(x)$, and $E(x,y) = \frac{y^2}{2} + V(x)$. Now, since $\frac{\partial E}{\partial y} = \dot{x}$, and $-\frac{\partial E}{\partial x} = \dot{y}$, our system is Hamiltonian system. Thus, all the solutions $(x(t), y(t))$ belongs to the level sets $E(x,y)= C$ for any constant $C > 0$. Now, since $V(x)$ has a local min at $c$, by Fundamental Theorem of Calculus, $F(c)=0$, and $F'(c)< 0$ (since $c$ is a simple zero of $F$, so $c$ is a strict local min). So the point $(c,0)$ is an equilibrium of our system. By principle of linearization, since $Df((c,0)) = (0 1, F'(c) 0)^T$, which has 2 real eigenvalues $\pm \sqrt{F'(c)}$, we conclude that $(c,0)$ is a hyperbolic equilibrium, and it is an unstable saddle. For the case when $V$ has local max at $c$, we have $F'(c) < 0$, so $Df((c,0))$ would have 2 imaginary eigenvalues with real parts $= 0$, so $(c,0)$ is not hyperbolic equilibrium, thus Principle of Linearized Stability cannot apply in this case. However, as we know that $(c,0)$ would be on some level sets $E(x,y) = C_1$ for some $C_1$, $(c,0)$ is stable . In addition, due to the 2 eigenvalues are all imaginary, $(c,0)$ can't be saddle, focus, center or center-focus, so it must be OTHER . Is my solution correct for this part? Part (b) is quite simple by using part (a)'s result (only need to check whether $0$, $1$ or $-1$ is a local min or max of F(x), and choose $y_0=0$), so I omitted the proof here. For part (c), it's quite hard, as we have the term $x^2\dot{x}$, so I cannot use the result above as the RHS is not purely in terms of $x$. Principle of Linearization also does not work in this case, as $Df((0,0))$ has $2$ eigenvalues $\pm 2i$.","Given $\ddot{x} = F(x)$, $x(0) = x_0$ and $\dot{x}(0) = y_0$. Assume $c$ is a simple zero of $F$. Let $V(x) = -\int_{x_0}^{x} F(s)ds$. (a) If $V$ has a local min at $c$, find the stability and type of the equilibrium (i.e, saddle, focus, center, center-focus, other) at the point $(x_0, y_0) = (c, 0)$. Use $E(x) = \frac{\dot{x}^{2}}{2} + V(x)$ to explain your answer. What is the stability and type if $V$ has a local max. Explain? (b) Find the type and stability of the equilibria of $\ \ddot{x} = -4x(1-x^2)$. (c) Find the type and stability of $x = \dot{x} = 0$ for the equation: $\ \ddot{x} = -4x(1-x^2) - x^2\dot{x}$ My attempt: For part (a), let $\dot{x} = y$. It's easy to see that we can rewrite the 2nd-order ODE as the system of 1st-order ODE: $\dot{x} = y, \dot{y} = F(x)$, and $E(x,y) = \frac{y^2}{2} + V(x)$. Now, since $\frac{\partial E}{\partial y} = \dot{x}$, and $-\frac{\partial E}{\partial x} = \dot{y}$, our system is Hamiltonian system. Thus, all the solutions $(x(t), y(t))$ belongs to the level sets $E(x,y)= C$ for any constant $C > 0$. Now, since $V(x)$ has a local min at $c$, by Fundamental Theorem of Calculus, $F(c)=0$, and $F'(c)< 0$ (since $c$ is a simple zero of $F$, so $c$ is a strict local min). So the point $(c,0)$ is an equilibrium of our system. By principle of linearization, since $Df((c,0)) = (0 1, F'(c) 0)^T$, which has 2 real eigenvalues $\pm \sqrt{F'(c)}$, we conclude that $(c,0)$ is a hyperbolic equilibrium, and it is an unstable saddle. For the case when $V$ has local max at $c$, we have $F'(c) < 0$, so $Df((c,0))$ would have 2 imaginary eigenvalues with real parts $= 0$, so $(c,0)$ is not hyperbolic equilibrium, thus Principle of Linearized Stability cannot apply in this case. However, as we know that $(c,0)$ would be on some level sets $E(x,y) = C_1$ for some $C_1$, $(c,0)$ is stable . In addition, due to the 2 eigenvalues are all imaginary, $(c,0)$ can't be saddle, focus, center or center-focus, so it must be OTHER . Is my solution correct for this part? Part (b) is quite simple by using part (a)'s result (only need to check whether $0$, $1$ or $-1$ is a local min or max of F(x), and choose $y_0=0$), so I omitted the proof here. For part (c), it's quite hard, as we have the term $x^2\dot{x}$, so I cannot use the result above as the RHS is not purely in terms of $x$. Principle of Linearization also does not work in this case, as $Df((0,0))$ has $2$ eigenvalues $\pm 2i$.",,['ordinary-differential-equations']
57,"Is the Wave function a ""Smooth"" function of the Potential?","Is the Wave function a ""Smooth"" function of the Potential?",,"Consider the Schroedinger equation in a spherically symmetric system. In the unit system under which energy is measured in Hartree and length in Bohr radius $a_0$, the schroedinger equation can be written as $$ -{d^2 R_{l,\epsilon}(r) \over dr^2} + \left( V_d(r) + {l(l+1)\over 2 r^2} \right)R_{l, \epsilon}(r) = \epsilon R_{l, \epsilon}(r) $$ . In the above equation, $V_d(r)$ is an attractive potential that's dependent on $d$. Specifically, we will consider the following potential. $V_{0}(r) = -Z(r)/r$, where $\lim_{r \rightarrow 0}Z(r) = Z$ and $\lim_{r \rightarrow \infty} Z(r) = 1$. $V_d(r) = V_0(r) + {1 \over r} - {e^{-r/d} \over r}$ ($d>0$). Consider the wave function $R_{l, \epsilon}^{reg}(r)$ that is regular at the origin and which is normalised to unit magnitude at infinity. In other words, $$ R_{l, \epsilon}^{reg}(r) \rightarrow \sin(k r + \phi) $$, as $r \rightarrow \infty$. $k = \sqrt{2 \epsilon}$. My question is, for at least $\epsilon>0$, is $R_{l, \epsilon}^{reg}(r)$ a ""smooth"" function of $d$?","Consider the Schroedinger equation in a spherically symmetric system. In the unit system under which energy is measured in Hartree and length in Bohr radius $a_0$, the schroedinger equation can be written as $$ -{d^2 R_{l,\epsilon}(r) \over dr^2} + \left( V_d(r) + {l(l+1)\over 2 r^2} \right)R_{l, \epsilon}(r) = \epsilon R_{l, \epsilon}(r) $$ . In the above equation, $V_d(r)$ is an attractive potential that's dependent on $d$. Specifically, we will consider the following potential. $V_{0}(r) = -Z(r)/r$, where $\lim_{r \rightarrow 0}Z(r) = Z$ and $\lim_{r \rightarrow \infty} Z(r) = 1$. $V_d(r) = V_0(r) + {1 \over r} - {e^{-r/d} \over r}$ ($d>0$). Consider the wave function $R_{l, \epsilon}^{reg}(r)$ that is regular at the origin and which is normalised to unit magnitude at infinity. In other words, $$ R_{l, \epsilon}^{reg}(r) \rightarrow \sin(k r + \phi) $$, as $r \rightarrow \infty$. $k = \sqrt{2 \epsilon}$. My question is, for at least $\epsilon>0$, is $R_{l, \epsilon}^{reg}(r)$ a ""smooth"" function of $d$?",,"['functional-analysis', 'ordinary-differential-equations', 'mathematical-physics']"
58,Differential Geometry: ODE question using parametrization of circle,Differential Geometry: ODE question using parametrization of circle,,"Here is an example from the textbook which I need help with: Consider the equation $F(x, y) = x^2 + y^2 = c$. The gradient is given by $(2x, 2y)$ and only vanishes at the origin. The differential equation describing the level sets is $\left(\frac{dx}{dt}, \frac{dy}{dt}\right) = (−2y, 2x)$. The solutions are given by $q(t) = R(\cos(2 (t + p)) , \sin(2 (t + p)))$ where the constants $R$ and $p$ can be adjusted according to any given initial position. I am not able to get $q(t)$ the way the book gets it.  For example, for $x(t)$, instead of $R(\cos(2t))$ I get $R(2\cos t)$ after solving the differential equation. EDIT: Ok, so I think I have figured it out.  I think it was just a matter of choosing a parametrization.  My original choice was $x = r\cos t$, $y = r\sin t$, but I see now that if I use this then $\frac{dx}{dt} = -2y$ implies that $\frac{dx}{dt} = -2r\sin t$, and then $x$ would be $2r\cos t$.  Likewise, $y$ would be $2r\sin t$.  But using this parametrization would not work, since $x^2 + y^2$ does not equal $r^2$ as needed (by equation of circle), rather it would equal $4r^2$.  So I assume that the author just chose a parametrization to work in the equation, and simply that is where the solution comes from.  Is this correct?","Here is an example from the textbook which I need help with: Consider the equation $F(x, y) = x^2 + y^2 = c$. The gradient is given by $(2x, 2y)$ and only vanishes at the origin. The differential equation describing the level sets is $\left(\frac{dx}{dt}, \frac{dy}{dt}\right) = (−2y, 2x)$. The solutions are given by $q(t) = R(\cos(2 (t + p)) , \sin(2 (t + p)))$ where the constants $R$ and $p$ can be adjusted according to any given initial position. I am not able to get $q(t)$ the way the book gets it.  For example, for $x(t)$, instead of $R(\cos(2t))$ I get $R(2\cos t)$ after solving the differential equation. EDIT: Ok, so I think I have figured it out.  I think it was just a matter of choosing a parametrization.  My original choice was $x = r\cos t$, $y = r\sin t$, but I see now that if I use this then $\frac{dx}{dt} = -2y$ implies that $\frac{dx}{dt} = -2r\sin t$, and then $x$ would be $2r\cos t$.  Likewise, $y$ would be $2r\sin t$.  But using this parametrization would not work, since $x^2 + y^2$ does not equal $r^2$ as needed (by equation of circle), rather it would equal $4r^2$.  So I assume that the author just chose a parametrization to work in the equation, and simply that is where the solution comes from.  Is this correct?",,"['ordinary-differential-equations', 'circles', 'parametrization']"
59,Degree of a Differential Equation.,Degree of a Differential Equation.,,Consider the differential equation $$\sin\left(\frac{dy}{dx}\right)=x$$ what is the degree of the above differential equation? According to me it's degree is not defined as the equation is not polynomial in derivatives. Please help. Thanks.,Consider the differential equation $$\sin\left(\frac{dy}{dx}\right)=x$$ what is the degree of the above differential equation? According to me it's degree is not defined as the equation is not polynomial in derivatives. Please help. Thanks.,,[]
60,A Power Series Solution to a differential equation.,A Power Series Solution to a differential equation.,,"Find a power series solution to the following equation:   $$z''=\frac{4}{t}z'+z=0$$ which is bounded by $t=0$ with $z(0)=1$, $z'(0)=0$. So far this is what I have: $z=\sum_{n=0}{c_nt^n}$ $z'=\sum_{n=1}{c_nnt^{n-1}}$ $z''=\sum_{n=2}{c_nn(n-1)t^{n-2}}$ Plugging these into the initial equation and grouping sums I get $0=\sum_{n=0}{c_nt^n+4c_{n+1}(n+1)\frac{t^n}{t}+c_{n+2}(n+2)(n+1)t^n}$ Factoring out $t_n$ I get an equation involving $c_n$: $c_n+4c_{n+1}(\frac{n+1}{t})+c_{n+2}(n+2)(n+1)=0$ So do I just solve this and I am done? I feel like I am missing something.","Find a power series solution to the following equation:   $$z''=\frac{4}{t}z'+z=0$$ which is bounded by $t=0$ with $z(0)=1$, $z'(0)=0$. So far this is what I have: $z=\sum_{n=0}{c_nt^n}$ $z'=\sum_{n=1}{c_nnt^{n-1}}$ $z''=\sum_{n=2}{c_nn(n-1)t^{n-2}}$ Plugging these into the initial equation and grouping sums I get $0=\sum_{n=0}{c_nt^n+4c_{n+1}(n+1)\frac{t^n}{t}+c_{n+2}(n+2)(n+1)t^n}$ Factoring out $t_n$ I get an equation involving $c_n$: $c_n+4c_{n+1}(\frac{n+1}{t})+c_{n+2}(n+2)(n+1)=0$ So do I just solve this and I am done? I feel like I am missing something.",,"['ordinary-differential-equations', 'power-series']"
61,Wave equation with angular variable,Wave equation with angular variable,,"Suppose that the variable of a mono-dimensional wave equation is an angle: $$\frac{\partial^2 f(\phi)}{\partial \phi^2} + k_{\phi}^2 f(\phi) = 0$$ This equation is derived from a more complex (and separable) equation in cylindrical coordinates and $\phi$ is one of these coordinates. According to the definition , $0 \leq \phi < 2\pi$. If I choose a cosinusoidal solution for $f(\phi)$, $k_{\phi}$ must be an integer. $$\cos (\omega t - k_{\phi}\phi), k_{\phi} \in \mathbb{Z}$$ Why? Let the time $t$ be fixed and $0$. I know that the $f(\phi)$ must have for $\phi \to 2 \pi^-$ the same behaviour as for $\phi \to 0^+$, so that the function appears to be ""continuous"" in that point. This is possible only when the period of the function is a submultiple of $2\pi$, so $k_{\phi} \in \mathbb{Z}$. But which could be a rigorous motivation for this? Should the function be continuous across $\phi = 2\pi$? But this point doesn't even belong to the interval $[0,2\pi)$!","Suppose that the variable of a mono-dimensional wave equation is an angle: $$\frac{\partial^2 f(\phi)}{\partial \phi^2} + k_{\phi}^2 f(\phi) = 0$$ This equation is derived from a more complex (and separable) equation in cylindrical coordinates and $\phi$ is one of these coordinates. According to the definition , $0 \leq \phi < 2\pi$. If I choose a cosinusoidal solution for $f(\phi)$, $k_{\phi}$ must be an integer. $$\cos (\omega t - k_{\phi}\phi), k_{\phi} \in \mathbb{Z}$$ Why? Let the time $t$ be fixed and $0$. I know that the $f(\phi)$ must have for $\phi \to 2 \pi^-$ the same behaviour as for $\phi \to 0^+$, so that the function appears to be ""continuous"" in that point. This is possible only when the period of the function is a submultiple of $2\pi$, so $k_{\phi} \in \mathbb{Z}$. But which could be a rigorous motivation for this? Should the function be continuous across $\phi = 2\pi$? But this point doesn't even belong to the interval $[0,2\pi)$!",,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'coordinate-systems', 'wave-equation']"
62,$C_0$ functions with positive $f'''$,functions with positive,C_0 f''',"I am looking for functions $f \in C_0(\mathbb{R})$ such that $f'''$ is positive everywhere. So what I tried was solving ODEs like $y'''(x)=e^{-x^2}$ and so on, but I noticed that all these $y$ are in general not in $C_0$. Does anybody have an idea whether there is such a function $f$ with this property? If you have any question about this problem, please let me know.","I am looking for functions $f \in C_0(\mathbb{R})$ such that $f'''$ is positive everywhere. So what I tried was solving ODEs like $y'''(x)=e^{-x^2}$ and so on, but I noticed that all these $y$ are in general not in $C_0$. Does anybody have an idea whether there is such a function $f$ with this property? If you have any question about this problem, please let me know.",,"['calculus', 'real-analysis']"
63,What's the relationship between a polynomial $P_{n}\left(x\right)$ and $P_{n}\left(\frac{d}{dx}\right)$?,What's the relationship between a polynomial  and ?,P_{n}\left(x\right) P_{n}\left(\frac{d}{dx}\right),"I'm studying for my final exam in differential equations and I'm rethinking the whole concept of a characteristic equation. For instance, if we have the DE \begin{align}0&=\sum_{\kappa=0}^{2}\frac{d^{\kappa}}{{dx}^{\kappa}}y\left(x\right)\\&=L\left[y\left(x\right)\right],\end{align} by supposing $y\left(x\right)=e^{rx}$, we would arrive at the characteristic polynomial \begin{align}r^{2}+r+1&=0.\end{align} Now, if I just substitute $r\mapsto\displaystyle\frac{d}{dx}$ I get the operator $L$ back. Can I substitute $x\mapsto\displaystyle\frac{d}{dx}$ in $\mathbb{P}_{n}\left[x\right]$ to write $\mathbb{P}_{n}[\,\displaystyle\frac{d}{dx}\,]$? Is this the same as the operator $L$? Or does this notation not make sense? I'm basically thinking of a set that looks like \begin{align}\mathbb{P}_{n}[\,\frac{d}{dx}\,]=\left\{I,\frac{d}{dx},\left(\frac{d}{dx}\right)^{2},\cdots,\left(\frac{d}{dx}\right)^{n}\right\},\end{align} where $Iy\left(x\right)=y\left(x\right)$, instead of \begin{align}\mathbb{P}_{n}[\,x\,]=\left\{1,x,x^2,\cdots,x^{n}\right\}.\end{align}","I'm studying for my final exam in differential equations and I'm rethinking the whole concept of a characteristic equation. For instance, if we have the DE \begin{align}0&=\sum_{\kappa=0}^{2}\frac{d^{\kappa}}{{dx}^{\kappa}}y\left(x\right)\\&=L\left[y\left(x\right)\right],\end{align} by supposing $y\left(x\right)=e^{rx}$, we would arrive at the characteristic polynomial \begin{align}r^{2}+r+1&=0.\end{align} Now, if I just substitute $r\mapsto\displaystyle\frac{d}{dx}$ I get the operator $L$ back. Can I substitute $x\mapsto\displaystyle\frac{d}{dx}$ in $\mathbb{P}_{n}\left[x\right]$ to write $\mathbb{P}_{n}[\,\displaystyle\frac{d}{dx}\,]$? Is this the same as the operator $L$? Or does this notation not make sense? I'm basically thinking of a set that looks like \begin{align}\mathbb{P}_{n}[\,\frac{d}{dx}\,]=\left\{I,\frac{d}{dx},\left(\frac{d}{dx}\right)^{2},\cdots,\left(\frac{d}{dx}\right)^{n}\right\},\end{align} where $Iy\left(x\right)=y\left(x\right)$, instead of \begin{align}\mathbb{P}_{n}[\,x\,]=\left\{1,x,x^2,\cdots,x^{n}\right\}.\end{align}",,['ordinary-differential-equations']
64,Are there any theorem about the linearization of PDE?,Are there any theorem about the linearization of PDE?,,"I am a beginner of PDE, and surprise that some nonlinear equation will become a linear equation after variable substitution,for example . So, I am curious that whether there are general theory making equation become linear. If not, why we don't do so ? I want to know the difficult of this way.","I am a beginner of PDE, and surprise that some nonlinear equation will become a linear equation after variable substitution,for example . So, I am curious that whether there are general theory making equation become linear. If not, why we don't do so ? I want to know the difficult of this way.",,"['analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'soft-question']"
65,How to solve this initial value problem?,How to solve this initial value problem?,,"How do I solve this initial value problem? $y'=2x-2\sqrt{\max(y,0)}$ $y(0)=0$ I've never seen a problem of this type with the ""max"" and I don't know how to begin. It must have a unique solution $y:[0,\infty)\rightarrow\mathbb{R}$ I know the solution must be of the type $y(x)=cx^2$.","How do I solve this initial value problem? $y'=2x-2\sqrt{\max(y,0)}$ $y(0)=0$ I've never seen a problem of this type with the ""max"" and I don't know how to begin. It must have a unique solution $y:[0,\infty)\rightarrow\mathbb{R}$ I know the solution must be of the type $y(x)=cx^2$.",,"['ordinary-differential-equations', 'initial-value-problems']"
66,Show that the system $\ddot x+x\dot x+x=0$ is reversible,Show that the system  is reversible,\ddot x+x\dot x+x=0,"Show that the system $\ddot x+x\dot x+x=0$ is reversible My attempt was to use a property from the book that if the system $\dot x = f(x,y)$ $\dot y = g(x,y)$ is reversible then $f(x,-y)=-f(x,y)$ and $g(x,-y)=g(x,y)$. I tried to convert the system into a system like the one above by letting $\dot x=y$ $\ddot x=\dot y=-x-xy$ However, this system does not satisfy the second criteria, namely that $g(x,y)=-x-xy \ne g(x,-y)=-x+xy$ Is there another way to show that this system is reversible?","Show that the system $\ddot x+x\dot x+x=0$ is reversible My attempt was to use a property from the book that if the system $\dot x = f(x,y)$ $\dot y = g(x,y)$ is reversible then $f(x,-y)=-f(x,y)$ and $g(x,-y)=g(x,y)$. I tried to convert the system into a system like the one above by letting $\dot x=y$ $\ddot x=\dot y=-x-xy$ However, this system does not satisfy the second criteria, namely that $g(x,y)=-x-xy \ne g(x,-y)=-x+xy$ Is there another way to show that this system is reversible?",,"['ordinary-differential-equations', 'dynamical-systems']"
67,Show that the derivative of a function is not continuous,Show that the derivative of a function is not continuous,,"$$g(x)=\begin{cases} x+2x^2\sin\left(\frac{1}{x}\right)&\text{ if }x\neq0\\\ 0&\text{ if }x=0 \end{cases}$$ Show that there is a sequence $\{x_n\}$ with $\{x_n\} \to 0$ as $n$ approaches infinity, such that $g'(x_n)=0\  \forall n$ but $g'(0) \ne 0$. I calculated $$g'(x)=4x\sin\left(\frac{1}{x}\right)-2\cos\left(\frac{1}{x}\right)+1 ,\text{ if }x\neq0$$ I tried to construct a sequence where $\{x_n\}=\dfrac 1 {2\pi n}$ so that $g'(x_n)=0$, but I'm not sure if this is the right way to prove the question. If this is not right, how can I go about showing it?","$$g(x)=\begin{cases} x+2x^2\sin\left(\frac{1}{x}\right)&\text{ if }x\neq0\\\ 0&\text{ if }x=0 \end{cases}$$ Show that there is a sequence $\{x_n\}$ with $\{x_n\} \to 0$ as $n$ approaches infinity, such that $g'(x_n)=0\  \forall n$ but $g'(0) \ne 0$. I calculated $$g'(x)=4x\sin\left(\frac{1}{x}\right)-2\cos\left(\frac{1}{x}\right)+1 ,\text{ if }x\neq0$$ I tried to construct a sequence where $\{x_n\}=\dfrac 1 {2\pi n}$ so that $g'(x_n)=0$, but I'm not sure if this is the right way to prove the question. If this is not right, how can I go about showing it?",,"['real-analysis', 'sequences-and-series', 'ordinary-differential-equations', 'continuity']"
68,Frobenius Method Indicial Equation,Frobenius Method Indicial Equation,,"I need to verify that the indicial equation only has one root. $xy''+(1-x)y'+\frac{1}{2}y=0$ Attempt: $y=\sum\limits_{m=0}^\infty {a_mx}^{m+r}$ $y'=\sum\limits_{m=0}^\infty {(m+r)}{a_mx}^{m+r-1}$ $y'=\sum\limits_{m=0}^\infty {(m+r-1)}{(m+r)}{a_mx}^{m+r-2}$ Substitute: $x\sum\limits_{m=0}^\infty {(m+r-1)}{(m+r)}{a_mx}^{m+r-2}+(1+x)\sum\limits_{m=0}^\infty {(m+r)}{a_mx}^{m+r-1}+\frac{1}{2}\sum\limits_{m=0}^\infty {a_mx}^{m+r}=0$ Reduce: $\sum\limits_{m=0}^\infty {(m+r-1)}{(m+r)}{a_mx}^{m+r-1}+\sum\limits_{m=0}^\infty {(m+r)}{a_mx}^{m+r-1} -\sum\limits_{m=0}^\infty {(m+r)}{a_mx}^{m+r}+\frac{1}{2}\sum\limits_{m=0}^\infty {a_mx}^{m+r}=0$ $\sum\limits_{m=0}^\infty[(m+r-1)(m+r)+(m+r)]a_{m}x^{m+r-1}-\frac{1}{2}\sum\limits_{m=0}^\infty {a_mx}^{m+r}=0$ Shift Index: $\sum\limits_{m=0}^\infty[(m+r-1)(m+r)+(m+r)]a_{m}x^{m+r-1}-\frac{1}{2}\sum\limits_{m=1}^\infty {a_{m-1}x}^{m+r-1}=0$ $[r^2]a_0x^{r-1}+\sum\limits_{m=1}^\infty([(m+r-1)(m+r)+(m+r)]a_m-\frac{1}{2}a_{m-1})x^{m+r-1}=0$ Therefore r is the double root of $0$, but I am not sure if this is correct.How would I go about verifying there is only one root?","I need to verify that the indicial equation only has one root. $xy''+(1-x)y'+\frac{1}{2}y=0$ Attempt: $y=\sum\limits_{m=0}^\infty {a_mx}^{m+r}$ $y'=\sum\limits_{m=0}^\infty {(m+r)}{a_mx}^{m+r-1}$ $y'=\sum\limits_{m=0}^\infty {(m+r-1)}{(m+r)}{a_mx}^{m+r-2}$ Substitute: $x\sum\limits_{m=0}^\infty {(m+r-1)}{(m+r)}{a_mx}^{m+r-2}+(1+x)\sum\limits_{m=0}^\infty {(m+r)}{a_mx}^{m+r-1}+\frac{1}{2}\sum\limits_{m=0}^\infty {a_mx}^{m+r}=0$ Reduce: $\sum\limits_{m=0}^\infty {(m+r-1)}{(m+r)}{a_mx}^{m+r-1}+\sum\limits_{m=0}^\infty {(m+r)}{a_mx}^{m+r-1} -\sum\limits_{m=0}^\infty {(m+r)}{a_mx}^{m+r}+\frac{1}{2}\sum\limits_{m=0}^\infty {a_mx}^{m+r}=0$ $\sum\limits_{m=0}^\infty[(m+r-1)(m+r)+(m+r)]a_{m}x^{m+r-1}-\frac{1}{2}\sum\limits_{m=0}^\infty {a_mx}^{m+r}=0$ Shift Index: $\sum\limits_{m=0}^\infty[(m+r-1)(m+r)+(m+r)]a_{m}x^{m+r-1}-\frac{1}{2}\sum\limits_{m=1}^\infty {a_{m-1}x}^{m+r-1}=0$ $[r^2]a_0x^{r-1}+\sum\limits_{m=1}^\infty([(m+r-1)(m+r)+(m+r)]a_m-\frac{1}{2}a_{m-1})x^{m+r-1}=0$ Therefore r is the double root of $0$, but I am not sure if this is correct.How would I go about verifying there is only one root?",,"['ordinary-differential-equations', 'power-series', 'frobenius-method']"
69,How to use boundary condition for Euler-Bernoulli equation,How to use boundary condition for Euler-Bernoulli equation,,How to use enter boundary condition into a Euler-Bernoulli fourth order equation,How to use enter boundary condition into a Euler-Bernoulli fourth order equation,,"['integration', 'ordinary-differential-equations', 'matlab']"
70,Deriving Iteration Formula for $y''(x)=(3x^2+x^6)y(x)$,Deriving Iteration Formula for,y''(x)=(3x^2+x^6)y(x),"As stated in the question I want to derive a iteration formula for the second order differential equation. All I have been working with so far is the general Picard-iteration formula: $$y_{n+1}(x)=y_0+\int_{x_0}^xf(t,y_n(t))dt \space \space \space \text{} \space \space \space y_0(x)=y_0$$ I thought this was some sort of fixed algorithm that one can use to derive some a taylor-series. How can I derive my ""own"" algorithm for the differential equation mentioned above? Edit 1: I am not sure if this is correct but I thought about just integreating the differential equation twice: $$\begin{aligned}& y''(x)=(3x^2+x^6)y(x) \\ &y'(x)=\int (3x^2+x^6)y(x) dx+v_0 \\ &\boxed{y(x)=\color{red}{\int\int(3x^2+x^6)y(x)dxdx}+v_0x+y_0}\end{aligned}$$ I have no idea how to evaluate or simplify the red integral or if this is even the correct approach.","As stated in the question I want to derive a iteration formula for the second order differential equation. All I have been working with so far is the general Picard-iteration formula: $$y_{n+1}(x)=y_0+\int_{x_0}^xf(t,y_n(t))dt \space \space \space \text{} \space \space \space y_0(x)=y_0$$ I thought this was some sort of fixed algorithm that one can use to derive some a taylor-series. How can I derive my ""own"" algorithm for the differential equation mentioned above? Edit 1: I am not sure if this is correct but I thought about just integreating the differential equation twice: $$\begin{aligned}& y''(x)=(3x^2+x^6)y(x) \\ &y'(x)=\int (3x^2+x^6)y(x) dx+v_0 \\ &\boxed{y(x)=\color{red}{\int\int(3x^2+x^6)y(x)dxdx}+v_0x+y_0}\end{aligned}$$ I have no idea how to evaluate or simplify the red integral or if this is even the correct approach.",,['ordinary-differential-equations']
71,Determine the Type and the General Solution of an ODE,Determine the Type and the General Solution of an ODE,,"Not sure where to start with this question. It's got me very confused. Help would be very appreciated, thanks Question Identify the type of first order ode and hence find the general solution: $$t \dot x = x+2te^{- {x\over t}}$$","Not sure where to start with this question. It's got me very confused. Help would be very appreciated, thanks Question Identify the type of first order ode and hence find the general solution: $$t \dot x = x+2te^{- {x\over t}}$$",,['ordinary-differential-equations']
72,Logistic curve through three points,Logistic curve through three points,,"I need to find a logistic curve that passes through three points exactly. This means I cannot do a best fit but rather must use simultaneous equations. Essentially this is used to model population growth. The equation that I need to fit my data to is: $$ P=\frac{M}{be^{-kt}+1} $$ This was derived from the differential equation: $\frac{dP}{dt}=kP(M-P)$ When I attempt to find constants M (maximum population), b and k I end up getting a negative value for b and a value for M which is less than the original data set. This ends up creating some sort of weird hyperbola that does exactly fit my data though is not the necessary equation. From what I gather b, M and k must be positive to form a logistic equation. Here is the data: t=0 -> P=3766124 t=1 -> P=4000687 t=2 -> P=4500789 This is what I end up getting: M -> 3311478946607640967/945032999867 k -> -Log[(1883446144648/1055718570207) b -> -(247632514983464541/3559111461591105508) Please show how this logistic function can be fitted exactly since these values are wrong. Thanks!","I need to find a logistic curve that passes through three points exactly. This means I cannot do a best fit but rather must use simultaneous equations. Essentially this is used to model population growth. The equation that I need to fit my data to is: This was derived from the differential equation: When I attempt to find constants M (maximum population), b and k I end up getting a negative value for b and a value for M which is less than the original data set. This ends up creating some sort of weird hyperbola that does exactly fit my data though is not the necessary equation. From what I gather b, M and k must be positive to form a logistic equation. Here is the data: t=0 -> P=3766124 t=1 -> P=4000687 t=2 -> P=4500789 This is what I end up getting: M -> 3311478946607640967/945032999867 k -> -Log[(1883446144648/1055718570207) b -> -(247632514983464541/3559111461591105508) Please show how this logistic function can be fitted exactly since these values are wrong. Thanks!", P=\frac{M}{be^{-kt}+1}  \frac{dP}{dt}=kP(M-P),"['ordinary-differential-equations', 'functions', 'algebraic-curves', 'logistic-regression']"
73,"Solving the equation find the value of $u(2,t)$",Solving the equation find the value of,"u(2,t)","Let , $u(x,t)$ be the solution of the one dimensional wave equation $\displaystyle u_{tt}-4u_{xx}=0$ , $-\infty <x<\infty$ , $t>0$ , $$u(x,0)=\begin{cases}16-x^2 &\text{ , for }|x|\le 4\\0 & \text{ , elsewhere }\end{cases}$$ and $$u_t(x,0)=\begin{cases}1 &\text{ , for} |x|\le 2\\0& \text{ , otherwise}\end{cases}$$   For $1<t<3$ , find the expression of $u(2,t)$. Here , D'Alembert's solution of the wave equation is $$u(x,t)=\frac{1}{2}[f(x+2t)+f(x-2t)]+\frac{1}{4}\int_{x-2t}^{x+2t}g(\theta)\,d\theta.$$ where , $f(x)=u(x,0)$ and $g(x)=u_t(x,0)$. As , $1<t<3$ , so $2+2t>4$ and so , $f(2+2t)=0$. Then , $$u(2,t)=\frac{1}{2}[0+16-(2-2t)^2]+\frac{1}{4}\int_{-2}^2\,d\theta=\frac{1}{2}[16-(2-2t)^2]+1.$$ Is it correct? Answer of the book is different , but I couldn't find any mistake. Please help me detecting my mistake. Edit : Options are : $$\frac{1}{2}[16-(2-2t)^2]+\frac{1}{2}[1-\min\{1,t-1\}].$$ $$\frac{1}{2}[32-(2-2t)^2-(2+2t)^2]+t.$$ $$\frac{1}{2}[32-(2-2t)^2-(2+2t)^2]+1.$$ $$\frac{1}{2}[16-(2-2t)^2]+\frac{1}{2}[1-\max\{1-t,-1\}].$$","Let , $u(x,t)$ be the solution of the one dimensional wave equation $\displaystyle u_{tt}-4u_{xx}=0$ , $-\infty <x<\infty$ , $t>0$ , $$u(x,0)=\begin{cases}16-x^2 &\text{ , for }|x|\le 4\\0 & \text{ , elsewhere }\end{cases}$$ and $$u_t(x,0)=\begin{cases}1 &\text{ , for} |x|\le 2\\0& \text{ , otherwise}\end{cases}$$   For $1<t<3$ , find the expression of $u(2,t)$. Here , D'Alembert's solution of the wave equation is $$u(x,t)=\frac{1}{2}[f(x+2t)+f(x-2t)]+\frac{1}{4}\int_{x-2t}^{x+2t}g(\theta)\,d\theta.$$ where , $f(x)=u(x,0)$ and $g(x)=u_t(x,0)$. As , $1<t<3$ , so $2+2t>4$ and so , $f(2+2t)=0$. Then , $$u(2,t)=\frac{1}{2}[0+16-(2-2t)^2]+\frac{1}{4}\int_{-2}^2\,d\theta=\frac{1}{2}[16-(2-2t)^2]+1.$$ Is it correct? Answer of the book is different , but I couldn't find any mistake. Please help me detecting my mistake. Edit : Options are : $$\frac{1}{2}[16-(2-2t)^2]+\frac{1}{2}[1-\min\{1,t-1\}].$$ $$\frac{1}{2}[32-(2-2t)^2-(2+2t)^2]+t.$$ $$\frac{1}{2}[32-(2-2t)^2-(2+2t)^2]+1.$$ $$\frac{1}{2}[16-(2-2t)^2]+\frac{1}{2}[1-\max\{1-t,-1\}].$$",,"['analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'definite-integrals']"
74,Peano existence theorem & Uniqueness of solutions to IVPs,Peano existence theorem & Uniqueness of solutions to IVPs,,"I have learned Peano existence theorem and Uniqueness of solutions to IVPs, but I don't understand what does that mean by $y'=f(x,y)$ , $y(x_0)=y_0$ has a solution in $(x_0-d,x_0+d)$ for some $d>0$ . What is this thm trying to show? How to understand its uniqueness? I have a hw question following: Prove that if $f$ is continuous on $\Bbb R× \Bbb  C$ and locally Lipschitz in the second argument, and if $x_0 \in \Bbb R$ , $y_0 \in \Bbb C$ , then there exists an interval $(a; b)$ containing $x_0$ such that the following holds. A solution to IVP $y' = f(x; y)$ and $ y(x_0) = y_0$ exists on $(a; b)$ , and if a solution $\tilde y$ to the IVP exists on some open interval $I$ containing $x_0$ , then $I\subset(a; b)$ and $\tilde y = y$ on $I$ . That is, (a; b) is the largest interval of existence and the solution is unique on it. (Hint: Show that if $F$ is the family of all couples (open interval containing $x_0$ , solution on it), then any two of these solutions coincide on the intersection of their intervals of definition. Conclude that then a solution can be defined on the union of these intervals (show it is an open interval) such that it coincides with each solution in F where the latter is defined.) I just don't understand how is the hint related to the thm. I know I have learned this part poorly, I really appreciate if you can let me understand the thm.","I have learned Peano existence theorem and Uniqueness of solutions to IVPs, but I don't understand what does that mean by , has a solution in for some . What is this thm trying to show? How to understand its uniqueness? I have a hw question following: Prove that if is continuous on and locally Lipschitz in the second argument, and if , , then there exists an interval containing such that the following holds. A solution to IVP and exists on , and if a solution to the IVP exists on some open interval containing , then and on . That is, (a; b) is the largest interval of existence and the solution is unique on it. (Hint: Show that if is the family of all couples (open interval containing , solution on it), then any two of these solutions coincide on the intersection of their intervals of definition. Conclude that then a solution can be defined on the union of these intervals (show it is an open interval) such that it coincides with each solution in F where the latter is defined.) I just don't understand how is the hint related to the thm. I know I have learned this part poorly, I really appreciate if you can let me understand the thm.","y'=f(x,y) y(x_0)=y_0 (x_0-d,x_0+d) d>0 f \Bbb R× \Bbb  C x_0 \in \Bbb R y_0 \in \Bbb C (a; b) x_0 y' = f(x; y)  y(x_0) = y_0 (a; b) \tilde y I x_0 I\subset(a; b) \tilde y = y I F x_0","['real-analysis', 'ordinary-differential-equations']"
75,Nonlinear ODE $\dot{X}(t)X(t)=B$,Nonlinear ODE,\dot{X}(t)X(t)=B,"I am learning ODE now. we know for ODE: $$\dot{X}(t)=A(t)X(t),X(t_0)=x_0$$ where $X\in \mathbb{R}^{n\times 1},A\in\mathbb{R}^{n\times n}$. The solution has the following form: $$X(t)=e^{\int_{t_0}^{t}A(s)ds}x_0$$ My question is for the following ODE: $$\dot{X}(t)X(t)=A(t),X(t_0)=x_0$$ where $X\in \mathbb{R}^{n\times n},A\in\mathbb{R}^{n\times n}$. Do we have general experison? By the way, can we use this ODE to define $\log{A}$ ?","I am learning ODE now. we know for ODE: $$\dot{X}(t)=A(t)X(t),X(t_0)=x_0$$ where $X\in \mathbb{R}^{n\times 1},A\in\mathbb{R}^{n\times n}$. The solution has the following form: $$X(t)=e^{\int_{t_0}^{t}A(s)ds}x_0$$ My question is for the following ODE: $$\dot{X}(t)X(t)=A(t),X(t_0)=x_0$$ where $X\in \mathbb{R}^{n\times n},A\in\mathbb{R}^{n\times n}$. Do we have general experison? By the way, can we use this ODE to define $\log{A}$ ?",,"['ordinary-differential-equations', 'matrix-equations']"
76,Proving that a differential equation has unique solution.,Proving that a differential equation has unique solution.,,"Let $P,Q,f:[-1,1]\to\mathbb{R}$ continuous and $a,b\in\mathbb{R}$. Then I want to prove that the IVP $$\begin{cases} u''(x)+P(x)u'(x)+Q(x)u(x)=f(x)\\u(0)=a,\qquad u'(0)=b\end{cases}$$ has a unique solution in a neighborhood of x=0. To this end I have to show that the function $F(x,y,t)$ is lipschitz in the first variables and the constant independent of the parameter $t$. (This comes when this problem is converted to solve the integral equation and invoke the condition that the operator $\Phi(x)(t)=x_0 + \int_0^tF(x(s),s)ds$ has only one fixed point and the use the Banach fixed point theorem) So I chose $F(x,y,t)=f(x)-P(x)-Q(x)$ then: $$|F(x,y,t)-F(u,v,t)|=|(f(x)-f(u))+(P(u)-P(x))+(Q(u)-Q(x))|$$ Now, by continuity of $f,P,Q$ in a compact set we have that they are uniformly continuous so we pick $\delta = \{ \delta_1,\delta_2, \delta_3 \}$ so if $|x-y|<\delta$ we get $$|F(x,y,t)-F(u,v,t)|< \epsilon<\epsilon |x-y|$$ but the thing is that I don't know how to extend this property to all $x,y \in [-1,1]$ Can someone help me to fix this problem or provide another way to prove this Lipschitz condition ?  because I don't know other way to proceed. Thanks a lot in advance.","Let $P,Q,f:[-1,1]\to\mathbb{R}$ continuous and $a,b\in\mathbb{R}$. Then I want to prove that the IVP $$\begin{cases} u''(x)+P(x)u'(x)+Q(x)u(x)=f(x)\\u(0)=a,\qquad u'(0)=b\end{cases}$$ has a unique solution in a neighborhood of x=0. To this end I have to show that the function $F(x,y,t)$ is lipschitz in the first variables and the constant independent of the parameter $t$. (This comes when this problem is converted to solve the integral equation and invoke the condition that the operator $\Phi(x)(t)=x_0 + \int_0^tF(x(s),s)ds$ has only one fixed point and the use the Banach fixed point theorem) So I chose $F(x,y,t)=f(x)-P(x)-Q(x)$ then: $$|F(x,y,t)-F(u,v,t)|=|(f(x)-f(u))+(P(u)-P(x))+(Q(u)-Q(x))|$$ Now, by continuity of $f,P,Q$ in a compact set we have that they are uniformly continuous so we pick $\delta = \{ \delta_1,\delta_2, \delta_3 \}$ so if $|x-y|<\delta$ we get $$|F(x,y,t)-F(u,v,t)|< \epsilon<\epsilon |x-y|$$ but the thing is that I don't know how to extend this property to all $x,y \in [-1,1]$ Can someone help me to fix this problem or provide another way to prove this Lipschitz condition ?  because I don't know other way to proceed. Thanks a lot in advance.",,"['real-analysis', 'ordinary-differential-equations']"
77,Underdamped free vibration proof,Underdamped free vibration proof,,"I need to prove the solution form of: $$y''+2cwy'+wy=0$$ My book says, after assuming a solution of the form $Ce^pt$, you can show that: $$y=[A\sin(wt)+B\sin(sw)] \cdot e^pt$$ I tried using the characteristic equation on the original ODE but it didn't get me anywhere. Any ideas? Obviously, just plugging in the suggested solution isn't a sufficient proof...Any ideas?","I need to prove the solution form of: $$y''+2cwy'+wy=0$$ My book says, after assuming a solution of the form $Ce^pt$, you can show that: $$y=[A\sin(wt)+B\sin(sw)] \cdot e^pt$$ I tried using the characteristic equation on the original ODE but it didn't get me anywhere. Any ideas? Obviously, just plugging in the suggested solution isn't a sufficient proof...Any ideas?",,"['ordinary-differential-equations', 'harmonic-analysis', 'harmonic-functions']"
78,A calculation problem about differential equation,A calculation problem about differential equation,,"Let $n>2$ and $n\in \mathbb{N}_+$, $\lambda  \ge  - \frac{{{{\left( {n - 1} \right)}^2}}}{4}$.   For $x>0$, we have $$f\left( x \right) = {\left( {\sinh x} \right)^{2 - n}}\int_0^\pi  {{{\left( {\cosh x + \cos t} \right)}^\eta }{{\left( {\sin t} \right)}^{2\sqrt {\lambda  + \frac{{{{\left( {n - 1} \right)}^2}}}{4}} }}dt} .$$   Solving the value of $\eta$ such that function $f(x)$ satisfies differential equation $$f''\left( x \right) + \left( {n - 1} \right)\frac{{\cosh x}}{{\sinh x}}f'\left( x \right) - \lambda f\left( x \right) = 0.$$ Adding: This is a difficult problem! I have get  \begin{align*}&f'\left( x \right) = \left( {2 - n} \right){\left( {\sinh x} \right)^{1 - n}}\cosh x\int_0^\pi  {{{\left( {\cosh x + \cos t} \right)}^\eta }{{\left( {\sin t} \right)}^{2\sqrt {\lambda  + \frac{{{{\left( {n - 1} \right)}^2}}}{4}} }}dt} \\+ &\eta {\left( {\sinh x} \right)^{3 - n}}\int_0^\pi  {{{\left( {\cosh x + \cos t} \right)}^{\eta  - 1}}{{\left( {\sin t} \right)}^{2\sqrt {\lambda  + \frac{{{{\left( {n - 1} \right)}^2}}}{4}} }}dt} .\end{align*} and  \begin{align*}&f''\left( x \right) = \left( {2 - n} \right)\left[ {\left( {1 - n} \right){{\left( {\sinh x} \right)}^{ - n}}{{\left( {\cosh x} \right)}^2} + {{\left( {\sinh x} \right)}^{2 - n}}} \right]\int_0^\pi  {{{\left( {\cosh x + \cos t} \right)}^\eta }{{\left( {\sin t} \right)}^{2\sqrt {\lambda  + \frac{{{{\left( {n - 1} \right)}^2}}}{4}} }}dt} \\+ &\left( {5 - 2n} \right)\eta {\left( {\sinh x} \right)^{2 - n}}\cosh x\int_0^\pi  {{{\left( {\cosh x + \cos t} \right)}^{\eta  - 1}}{{\left( {\sin t} \right)}^{2\sqrt {\lambda  + \frac{{{{\left( {n - 1} \right)}^2}}}{4}} }}dt} \\+ &\eta \left( {\eta  - 1} \right){\left( {\sinh x} \right)^{4 - n}}\int_0^\pi  {{{\left( {\cosh x + \cos t} \right)}^{\eta  - 2}}{{\left( {\sin t} \right)}^{2\sqrt {\lambda  + \frac{{{{\left( {n - 1} \right)}^2}}}{4}} }}dt} .\end{align*} Note that $$f''\left( x \right) + \left( {n - 1} \right)\frac{{\cosh x}}{{\sinh x}}f'\left( x \right) - \lambda f\left( x \right) = 0.$$ So we get \begin{align*}&\left( {2 - n-\lambda} \right)\int_0^\pi  {{{\left( {\cosh x + \cos t} \right)}^\eta }{{\left( {\sin t} \right)}^{2\sqrt {\lambda  + \frac{{{{\left( {n - 1} \right)}^2}}}{4}} }}dt} \\+& \left( {4 - n} \right)\eta \cosh x\int_0^\pi  {{{\left( {\cosh x + \cos t} \right)}^{\eta  - 1}}{{\left( {\sin t} \right)}^{2\sqrt {\lambda  + \frac{{{{\left( {n - 1} \right)}^2}}}{4}} }}dt} \\+& \eta \left( {\eta  - 1} \right){\left( {\sinh x} \right)^2}\int_0^\pi  {{{\left( {\cosh x + \cos t} \right)}^{\eta  - 2}}{{\left( {\sin t} \right)}^{2\sqrt {\lambda  + \frac{{{{\left( {n - 1} \right)}^2}}}{4}} }}dt}  = 0.\end{align*} But I have no way to continue, thanks for your help!","Let $n>2$ and $n\in \mathbb{N}_+$, $\lambda  \ge  - \frac{{{{\left( {n - 1} \right)}^2}}}{4}$.   For $x>0$, we have $$f\left( x \right) = {\left( {\sinh x} \right)^{2 - n}}\int_0^\pi  {{{\left( {\cosh x + \cos t} \right)}^\eta }{{\left( {\sin t} \right)}^{2\sqrt {\lambda  + \frac{{{{\left( {n - 1} \right)}^2}}}{4}} }}dt} .$$   Solving the value of $\eta$ such that function $f(x)$ satisfies differential equation $$f''\left( x \right) + \left( {n - 1} \right)\frac{{\cosh x}}{{\sinh x}}f'\left( x \right) - \lambda f\left( x \right) = 0.$$ Adding: This is a difficult problem! I have get  \begin{align*}&f'\left( x \right) = \left( {2 - n} \right){\left( {\sinh x} \right)^{1 - n}}\cosh x\int_0^\pi  {{{\left( {\cosh x + \cos t} \right)}^\eta }{{\left( {\sin t} \right)}^{2\sqrt {\lambda  + \frac{{{{\left( {n - 1} \right)}^2}}}{4}} }}dt} \\+ &\eta {\left( {\sinh x} \right)^{3 - n}}\int_0^\pi  {{{\left( {\cosh x + \cos t} \right)}^{\eta  - 1}}{{\left( {\sin t} \right)}^{2\sqrt {\lambda  + \frac{{{{\left( {n - 1} \right)}^2}}}{4}} }}dt} .\end{align*} and  \begin{align*}&f''\left( x \right) = \left( {2 - n} \right)\left[ {\left( {1 - n} \right){{\left( {\sinh x} \right)}^{ - n}}{{\left( {\cosh x} \right)}^2} + {{\left( {\sinh x} \right)}^{2 - n}}} \right]\int_0^\pi  {{{\left( {\cosh x + \cos t} \right)}^\eta }{{\left( {\sin t} \right)}^{2\sqrt {\lambda  + \frac{{{{\left( {n - 1} \right)}^2}}}{4}} }}dt} \\+ &\left( {5 - 2n} \right)\eta {\left( {\sinh x} \right)^{2 - n}}\cosh x\int_0^\pi  {{{\left( {\cosh x + \cos t} \right)}^{\eta  - 1}}{{\left( {\sin t} \right)}^{2\sqrt {\lambda  + \frac{{{{\left( {n - 1} \right)}^2}}}{4}} }}dt} \\+ &\eta \left( {\eta  - 1} \right){\left( {\sinh x} \right)^{4 - n}}\int_0^\pi  {{{\left( {\cosh x + \cos t} \right)}^{\eta  - 2}}{{\left( {\sin t} \right)}^{2\sqrt {\lambda  + \frac{{{{\left( {n - 1} \right)}^2}}}{4}} }}dt} .\end{align*} Note that $$f''\left( x \right) + \left( {n - 1} \right)\frac{{\cosh x}}{{\sinh x}}f'\left( x \right) - \lambda f\left( x \right) = 0.$$ So we get \begin{align*}&\left( {2 - n-\lambda} \right)\int_0^\pi  {{{\left( {\cosh x + \cos t} \right)}^\eta }{{\left( {\sin t} \right)}^{2\sqrt {\lambda  + \frac{{{{\left( {n - 1} \right)}^2}}}{4}} }}dt} \\+& \left( {4 - n} \right)\eta \cosh x\int_0^\pi  {{{\left( {\cosh x + \cos t} \right)}^{\eta  - 1}}{{\left( {\sin t} \right)}^{2\sqrt {\lambda  + \frac{{{{\left( {n - 1} \right)}^2}}}{4}} }}dt} \\+& \eta \left( {\eta  - 1} \right){\left( {\sinh x} \right)^2}\int_0^\pi  {{{\left( {\cosh x + \cos t} \right)}^{\eta  - 2}}{{\left( {\sin t} \right)}^{2\sqrt {\lambda  + \frac{{{{\left( {n - 1} \right)}^2}}}{4}} }}dt}  = 0.\end{align*} But I have no way to continue, thanks for your help!",,"['calculus', 'integration', 'analysis', 'ordinary-differential-equations']"
79,Applications of Second Order Differential Equations,Applications of Second Order Differential Equations,,"Hanging Spring. A 10 kilogram mass suspended from the end of a vertical spring stretches the spring $\frac{49}{90}$ metres. At time t = 0, the mass is started in motion from the equilibrium position with an initial velocity of $1 $ $m/s$ in the upward direction. At the same time, a constant downward force of $360$ Newtons is applied to the system. Assume that air resistance is equal to $60 $ times the instantaneous velocity and that the acceleration due to gravity is $g = 9.8 $ m/s^2. (a) Determine the spring constant. (b) Show that the equation of motion is $$ \ddot{x}  + 6 \dot{x} + 18x = 36 $$ where $x(t)$ is the displacement of the mass below the equilibrium position at time $t$ . In your answer include a diagram of all forces acting on the mass. (c) Find the position of the mass at any time. Would you describe the motion as overdamped, underdamped or critically damped? I'm struggling with part (c) and any help would be appreciated. Solving for $\lambda$ I know that $$ X_p =A\cdot t\cdot e^{-3t}\cdot\cos(3t)+B\cdot t\cdot e^{-3t}\cdot\sin(3t) $$ since without $t$ , using the equation gives a result of zero, using $$ \ddot{x} + 6 \dot{x} + 18x = 36, $$ with $X_p$ I get $$  6=e^{-3t} (b \cos(3t)-a \sin(3t))$$ but the answer say $$ x(t) = −2 e^{−3t} \cos (3t) −\frac73 e^{−3t}\sin (3t) + 2 $$ where does this $+2 $ come from, and how is the equation used without multiplying by t if it equals to zero? Sorry if this question is unclear and if any clarification is needed please ask.","Hanging Spring. A 10 kilogram mass suspended from the end of a vertical spring stretches the spring metres. At time t = 0, the mass is started in motion from the equilibrium position with an initial velocity of in the upward direction. At the same time, a constant downward force of Newtons is applied to the system. Assume that air resistance is equal to times the instantaneous velocity and that the acceleration due to gravity is m/s^2. (a) Determine the spring constant. (b) Show that the equation of motion is where is the displacement of the mass below the equilibrium position at time . In your answer include a diagram of all forces acting on the mass. (c) Find the position of the mass at any time. Would you describe the motion as overdamped, underdamped or critically damped? I'm struggling with part (c) and any help would be appreciated. Solving for I know that since without , using the equation gives a result of zero, using with I get but the answer say where does this come from, and how is the equation used without multiplying by t if it equals to zero? Sorry if this question is unclear and if any clarification is needed please ask.","\frac{49}{90} 1  m/s 360 60  g = 9.8   \ddot{x}  + 6 \dot{x} + 18x = 36  x(t) t \lambda  X_p =A\cdot t\cdot e^{-3t}\cdot\cos(3t)+B\cdot t\cdot e^{-3t}\cdot\sin(3t)  t  \ddot{x} + 6 \dot{x} + 18x = 36,  X_p   6=e^{-3t} (b \cos(3t)-a \sin(3t))  x(t) = −2 e^{−3t} \cos (3t) −\frac73 e^{−3t}\sin (3t) + 2  +2 ","['calculus', 'ordinary-differential-equations']"
80,ODE: to show the solution is periodic,ODE: to show the solution is periodic,,"Suppose $f(t)$, defined on $\Bbb R$, is periodic with period $\omega\ne0$, and that $k\ne0$ is a constant, prove: for the following differential equation $$\frac{dx}{dt}=kx+f(t)$$ there exists one and only one solution $x(t)$, which is also a periodic function with period $\omega$. My approach: First of all, it is easy to write the solution out explicitly, suppose that $x(0)=x_0$, then $$x(t)=e^{kt}\int_{0}^{t}f(s)e^{-ks} \, ds+x_0e^{kt}$$ satisfies the DE. To show the uniqueness, suppose both $x_1,x_2$ are solutions, let $\delta=x_1-x_2$, then subtract the two DEs and the two initial values to get $$\frac{d\delta}{dt}=k\delta\quad\text{with}\quad \delta(0)=0$$ and hence $\delta$ has the unique solution $\delta\equiv 0$. But its' been quite tough for me to show that the solution $x(t)$ is an $\omega$-periodic function. Actually, it suffices to show $x(\omega+t)\equiv x(t)$. By letting $\phi(t)=x(\omega+t)$, it is clear that $\phi(t)$ fits the DE. If $\phi(t)$ also fits the initial value condition, namely, if $\phi(0)=x_0$, then by the subtraction technique it is clear that $\phi(t)-x(t)\equiv 0$ and hence the proof. But $\phi(0)=x_0$ means that $$x(\omega)=e^{k\omega}\int_{0}^{\omega}f(s)e^{-ks}ds+e^{k\omega}x_0=x_0$$ which I cannot prove, because I know virtually nothing about $f(t)$ than its period. So can you help me? Thanks in advance!","Suppose $f(t)$, defined on $\Bbb R$, is periodic with period $\omega\ne0$, and that $k\ne0$ is a constant, prove: for the following differential equation $$\frac{dx}{dt}=kx+f(t)$$ there exists one and only one solution $x(t)$, which is also a periodic function with period $\omega$. My approach: First of all, it is easy to write the solution out explicitly, suppose that $x(0)=x_0$, then $$x(t)=e^{kt}\int_{0}^{t}f(s)e^{-ks} \, ds+x_0e^{kt}$$ satisfies the DE. To show the uniqueness, suppose both $x_1,x_2$ are solutions, let $\delta=x_1-x_2$, then subtract the two DEs and the two initial values to get $$\frac{d\delta}{dt}=k\delta\quad\text{with}\quad \delta(0)=0$$ and hence $\delta$ has the unique solution $\delta\equiv 0$. But its' been quite tough for me to show that the solution $x(t)$ is an $\omega$-periodic function. Actually, it suffices to show $x(\omega+t)\equiv x(t)$. By letting $\phi(t)=x(\omega+t)$, it is clear that $\phi(t)$ fits the DE. If $\phi(t)$ also fits the initial value condition, namely, if $\phi(0)=x_0$, then by the subtraction technique it is clear that $\phi(t)-x(t)\equiv 0$ and hence the proof. But $\phi(0)=x_0$ means that $$x(\omega)=e^{k\omega}\int_{0}^{\omega}f(s)e^{-ks}ds+e^{k\omega}x_0=x_0$$ which I cannot prove, because I know virtually nothing about $f(t)$ than its period. So can you help me? Thanks in advance!",,"['calculus', 'integration', 'ordinary-differential-equations']"
81,how to find the adjoint system associated to a differential equation system?,how to find the adjoint system associated to a differential equation system?,,"While proving an optimal control theorem concerning the controlability criterion they used the following : consider the system: $$ (S): \begin{cases} \frac{dx(t)}{dt}=A(t) x(t)+B(t) u(t)\\ x(t_0)=0 \end{cases} $$ where $A(t)$, $B(t)$ are two matrices and $u(t)$ is the control. The adjoint system associated to the system $(S)$ is given by: $$ (S^*): \begin{cases} \frac{dz(t)}{dt}=-A^*(t) z(t)\\ y(t)=B^*(t)z(t)\\ z(t_0)=z_0 \end{cases} $$ I really don't have any clue how they did derive such adjoint system, even on the net I didn't find any method describing how to do it ! does anyone have an idea or maybe  the answer ? thank you for your time.","While proving an optimal control theorem concerning the controlability criterion they used the following : consider the system: $$ (S): \begin{cases} \frac{dx(t)}{dt}=A(t) x(t)+B(t) u(t)\\ x(t_0)=0 \end{cases} $$ where $A(t)$, $B(t)$ are two matrices and $u(t)$ is the control. The adjoint system associated to the system $(S)$ is given by: $$ (S^*): \begin{cases} \frac{dz(t)}{dt}=-A^*(t) z(t)\\ y(t)=B^*(t)z(t)\\ z(t_0)=z_0 \end{cases} $$ I really don't have any clue how they did derive such adjoint system, even on the net I didn't find any method describing how to do it ! does anyone have an idea or maybe  the answer ? thank you for your time.",,"['ordinary-differential-equations', 'optimal-control']"
82,Lagrange Differential equation,Lagrange Differential equation,,"I have problems proceeding in solving the following differential equation $$xy' + y + (y')^2 = 0.$$ After solving for $\frac{dy}{dx}$ in the quadratic and using the substitution $u^2 = x^2 - 4y$ in the discriminant, I obtain $\frac{du}{dx} = 2 \frac{x}{u} -1$. Please how can I proceed?","I have problems proceeding in solving the following differential equation $$xy' + y + (y')^2 = 0.$$ After solving for $\frac{dy}{dx}$ in the quadratic and using the substitution $u^2 = x^2 - 4y$ in the discriminant, I obtain $\frac{du}{dx} = 2 \frac{x}{u} -1$. Please how can I proceed?",,['ordinary-differential-equations']
83,Find piecewise constant function u for $X'(t)=AX(t) + Bu(t)$ and $X(t)=\begin{pmatrix}10 \\0 \end{pmatrix}$ for some T,Find piecewise constant function u for  and  for some T,X'(t)=AX(t) + Bu(t) X(t)=\begin{pmatrix}10 \\0 \end{pmatrix},"Consider the system $$x''(t)=u(t)$$ such that $x(0)=100, \; x'(0)=50$. Find a function $u$ piecewise constant such that $x(T)=0, \; x'(T)=10$ for a time $T$ Using the control theory language, it is asking for a piecewise constant admissible control such that the point $(0,10)$ is reached by $(100,50)$ in some time $T$. My attempt I transformed this into the system $$\begin{bmatrix} x' \\ y'\end{bmatrix} = \begin{bmatrix} 0 & 1 \\ 0 & 0\end{bmatrix}\begin{bmatrix} x \\ y\end{bmatrix}+\begin{bmatrix} 0 \\ 1\end{bmatrix}u$$ which has the solution $$\begin{bmatrix} x \\ y\end{bmatrix} = \begin{bmatrix}1 & t \\ 0 & 1 \end{bmatrix}\begin{bmatrix}100 \\ 50  \end{bmatrix} + \int_0^t \begin{bmatrix} (t-s)u \\ u\end{bmatrix}ds$$ Solving the integral, considering $u$ a constant, I got that $$\begin{bmatrix} x \\ y\end{bmatrix} = \begin{bmatrix} 100 + 50t + \frac{t^2}{2} u \\ 50 + tu\end{bmatrix}$$ I've tried to make this equal to the given vector that we want, but I couldnt find t and u that matches. What is wrong? Thanks! Edit Now I see that I made a mistake calculating the solution, so I fixed the last matrix, made it equal to the vector that I wanted and solved for t and u. Answer can be found here","Consider the system $$x''(t)=u(t)$$ such that $x(0)=100, \; x'(0)=50$. Find a function $u$ piecewise constant such that $x(T)=0, \; x'(T)=10$ for a time $T$ Using the control theory language, it is asking for a piecewise constant admissible control such that the point $(0,10)$ is reached by $(100,50)$ in some time $T$. My attempt I transformed this into the system $$\begin{bmatrix} x' \\ y'\end{bmatrix} = \begin{bmatrix} 0 & 1 \\ 0 & 0\end{bmatrix}\begin{bmatrix} x \\ y\end{bmatrix}+\begin{bmatrix} 0 \\ 1\end{bmatrix}u$$ which has the solution $$\begin{bmatrix} x \\ y\end{bmatrix} = \begin{bmatrix}1 & t \\ 0 & 1 \end{bmatrix}\begin{bmatrix}100 \\ 50  \end{bmatrix} + \int_0^t \begin{bmatrix} (t-s)u \\ u\end{bmatrix}ds$$ Solving the integral, considering $u$ a constant, I got that $$\begin{bmatrix} x \\ y\end{bmatrix} = \begin{bmatrix} 100 + 50t + \frac{t^2}{2} u \\ 50 + tu\end{bmatrix}$$ I've tried to make this equal to the given vector that we want, but I couldnt find t and u that matches. What is wrong? Thanks! Edit Now I see that I made a mistake calculating the solution, so I fixed the last matrix, made it equal to the vector that I wanted and solved for t and u. Answer can be found here",,"['ordinary-differential-equations', 'control-theory']"
84,Finding an equilibrium solution to a first order system of equations.,Finding an equilibrium solution to a first order system of equations.,,"Given a model: $ y''+\alpha y'+\beta y + \gamma y = -g $ I can see that it can be converted to a system of first order equations as follows: $y_{1}=y$, $y_{2}=y'$ and as such $y_{1}'=y'$ and $y_{2}'=y''$. Using this I see that: $ \begin{bmatrix} y_{1}' \\ y_{2}' \end{bmatrix} = \begin{bmatrix} 0 & 1 \\ -(\beta + \gamma) & -\alpha \end{bmatrix} \begin{bmatrix} y_{1} \\ y_{2} \end{bmatrix} + \begin{bmatrix} 0 \\ -g \end{bmatrix} $ However, I am unsure how to use this to find an equilibrium matrix for this first order system of equations. If it helps we are asked to show that the solution is: $\mathbf{y_{eq}}=\begin{bmatrix}\frac{-g}{\beta + \gamma} \\ 0 \end{bmatrix}$","Given a model: $ y''+\alpha y'+\beta y + \gamma y = -g $ I can see that it can be converted to a system of first order equations as follows: $y_{1}=y$, $y_{2}=y'$ and as such $y_{1}'=y'$ and $y_{2}'=y''$. Using this I see that: $ \begin{bmatrix} y_{1}' \\ y_{2}' \end{bmatrix} = \begin{bmatrix} 0 & 1 \\ -(\beta + \gamma) & -\alpha \end{bmatrix} \begin{bmatrix} y_{1} \\ y_{2} \end{bmatrix} + \begin{bmatrix} 0 \\ -g \end{bmatrix} $ However, I am unsure how to use this to find an equilibrium matrix for this first order system of equations. If it helps we are asked to show that the solution is: $\mathbf{y_{eq}}=\begin{bmatrix}\frac{-g}{\beta + \gamma} \\ 0 \end{bmatrix}$",,"['linear-algebra', 'ordinary-differential-equations']"
85,"This theory proof about instability of a point of equilibrium is not understandable for me, any help?","This theory proof about instability of a point of equilibrium is not understandable for me, any help?",,"-This theory is irritating me, because I don't understand it's logic. Theorem: If in some neighboorhood $\mathbb O (0)$, exists a continuous, differentiable function $V(X), V(0)=0,$ such that the derivative based on the system: $X'=F(X)$ satisfies the inequality: $$\forall X \in \mathbb O(0): V'(X) \geq W(X)$$ where $W$ is some continuous, positive function on $\mathbb O (0),$ then the point of equilibrium $X=0$ is unstable. PROOF: In accordance with the conditions of the thereom, for every trajectory $X=X(t)$ of the system $X'=F(X)$ the function $V(X(t))$ is rising on the parts of the trajectory that are within $\mathbb K_r= \{X : \|X\|<r\}\subset \mathbb O(0).$Since $V$ is continuous on $\mathbb K_r,V(0)=0,\exists M, \mathcal E_0 > 0, \mathcal E \leq r:$ $$\forall X \in \mathbb K_{\mathcal E_0}:V(X)<M$$ Let $\delta \in (0, \mathcal E_0)$ be arbitrarily small. Choosing $X_0 \neq 0, \|X_0\|\leq \delta, V(X_0)>0$ and constructing a trajectory: $X=\varphi(t), \varphi(t_0)=X_0.$ Let's show that there exists $T>t_0$ such that $\|\varphi(T)\|> \mathcal E_0.$ Suppose the condradictory: $$\forall t >t_0 :\|\varphi(T)\ \leq \mathcal E_0.$$ Then, because of the rise of $V(\varphi(t))$**(From here on I  start having troubles)** $$\forall t >t_0: V(\varphi(t))> V(\varphi(t_0))=V(X_0)>0$$ or  $$\exists \beta \in (0, \mathcal E_0) \forall t \geq t_0: \|\varphi(t)\| \geq \beta$$ therefore $$V'(\varphi(t))\geq W(\varphi(t))\geq  \min_{\beta \leq \|X\|\leq \mathcal E_0}W(X)= \gamma > 0.$$ (The following line kills me!) -Therefore function V is unlimited on $\mathbb K_{\varepsilon_0}$ a contradiction has arrised and we conclude that: $$\exists \varepsilon_0 > 0 \forall \delta >0, \exists X_0 \neq 0, \|X_0\| \leq \delta, \exists T > t_0, \|\phi(T)\|>\varepsilon_0$$ which is the definition of instability. I would absolutely appreciate help on this and will give 100 rep to the first right answer, garanteed.","-This theory is irritating me, because I don't understand it's logic. Theorem: If in some neighboorhood $\mathbb O (0)$, exists a continuous, differentiable function $V(X), V(0)=0,$ such that the derivative based on the system: $X'=F(X)$ satisfies the inequality: $$\forall X \in \mathbb O(0): V'(X) \geq W(X)$$ where $W$ is some continuous, positive function on $\mathbb O (0),$ then the point of equilibrium $X=0$ is unstable. PROOF: In accordance with the conditions of the thereom, for every trajectory $X=X(t)$ of the system $X'=F(X)$ the function $V(X(t))$ is rising on the parts of the trajectory that are within $\mathbb K_r= \{X : \|X\|<r\}\subset \mathbb O(0).$Since $V$ is continuous on $\mathbb K_r,V(0)=0,\exists M, \mathcal E_0 > 0, \mathcal E \leq r:$ $$\forall X \in \mathbb K_{\mathcal E_0}:V(X)<M$$ Let $\delta \in (0, \mathcal E_0)$ be arbitrarily small. Choosing $X_0 \neq 0, \|X_0\|\leq \delta, V(X_0)>0$ and constructing a trajectory: $X=\varphi(t), \varphi(t_0)=X_0.$ Let's show that there exists $T>t_0$ such that $\|\varphi(T)\|> \mathcal E_0.$ Suppose the condradictory: $$\forall t >t_0 :\|\varphi(T)\ \leq \mathcal E_0.$$ Then, because of the rise of $V(\varphi(t))$**(From here on I  start having troubles)** $$\forall t >t_0: V(\varphi(t))> V(\varphi(t_0))=V(X_0)>0$$ or  $$\exists \beta \in (0, \mathcal E_0) \forall t \geq t_0: \|\varphi(t)\| \geq \beta$$ therefore $$V'(\varphi(t))\geq W(\varphi(t))\geq  \min_{\beta \leq \|X\|\leq \mathcal E_0}W(X)= \gamma > 0.$$ (The following line kills me!) -Therefore function V is unlimited on $\mathbb K_{\varepsilon_0}$ a contradiction has arrised and we conclude that: $$\exists \varepsilon_0 > 0 \forall \delta >0, \exists X_0 \neq 0, \|X_0\| \leq \delta, \exists T > t_0, \|\phi(T)\|>\varepsilon_0$$ which is the definition of instability. I would absolutely appreciate help on this and will give 100 rep to the first right answer, garanteed.",,"['calculus', 'ordinary-differential-equations', 'differential-geometry', 'derivatives', 'differential-topology']"
86,Solving differential equations with repeating forcing function,Solving differential equations with repeating forcing function,,"I am looking for a way to solve differential equation using the Laplace transformation with discontinuous and periodic forcing functions. I found this example and I would like to understand the general solution for this kind of problem. The function has the form: $$u''(t) + \pi^2u(t) = g(t)\\ g(t) = 1, \quad t < c\\ g(t) = 0, \quad t \ge c+1$$ Further they explain: ""The first three terms of the Laplace transform of the homogeneous solution for $u(t):$"" $$ \frac{e^{-2s}}{{s(s^2+\pi^2)}} - \frac{e^{-s}}{{s(s^2+\pi^2)}} + \frac{1}{s(s^2+\pi^2)}$$ Hence I assume that there are more then these three terms. I understand that each term consists of the Laplace transformed unit step function $$\frac{e^{-is}}{s}$$ at $c = 0,1,...$ and  $$\frac{1}{(s^2+\pi^2)}$$ which looks at first like a sine but it's not (I think). Long story short: how do I calculate the solution of a differential equation such as this with an theoretically infinite amount of forcing terms. Ultimately my goal is to understand how a differential equation evolves given a continuous input.","I am looking for a way to solve differential equation using the Laplace transformation with discontinuous and periodic forcing functions. I found this example and I would like to understand the general solution for this kind of problem. The function has the form: $$u''(t) + \pi^2u(t) = g(t)\\ g(t) = 1, \quad t < c\\ g(t) = 0, \quad t \ge c+1$$ Further they explain: ""The first three terms of the Laplace transform of the homogeneous solution for $u(t):$"" $$ \frac{e^{-2s}}{{s(s^2+\pi^2)}} - \frac{e^{-s}}{{s(s^2+\pi^2)}} + \frac{1}{s(s^2+\pi^2)}$$ Hence I assume that there are more then these three terms. I understand that each term consists of the Laplace transformed unit step function $$\frac{e^{-is}}{s}$$ at $c = 0,1,...$ and  $$\frac{1}{(s^2+\pi^2)}$$ which looks at first like a sine but it's not (I think). Long story short: how do I calculate the solution of a differential equation such as this with an theoretically infinite amount of forcing terms. Ultimately my goal is to understand how a differential equation evolves given a continuous input.",,"['ordinary-differential-equations', 'simulation']"
87,A Hamiltonian vector field on $\mathbb{R}^{4}$ which has closed orbit but does not have critical point,A Hamiltonian vector field on  which has closed orbit but does not have critical point,\mathbb{R}^{4},Is there a polynomial  function $H:\mathbb{R}^{4} \to  \mathbb{R}$  without  critical points but the corresponding hamiltonian vector field possess at least one closed orbit?,Is there a polynomial  function $H:\mathbb{R}^{4} \to  \mathbb{R}$  without  critical points but the corresponding hamiltonian vector field possess at least one closed orbit?,,"['ordinary-differential-equations', 'dynamical-systems', 'classical-mechanics']"
88,A second order linear ordinary differential equation,A second order linear ordinary differential equation,,"I am trying to solve the following differential equation: \begin{equation} (1-x^{2})y''+\{\alpha x+1 \}y'+\{\beta^{2}x^{2}-\beta{x}+\gamma\}y=0 \end{equation} where $y=y(x)$. $y'$ and $y''$ are the first and second derivatives respectively. $\alpha$, $\beta$ and $\gamma$ are constants. There are various second order differential equations that are very similar looking to this equation. For instance Mathieu equation, Legendre equation and generalized Hypergeometric equation, but not quite the same. I am guessing that a change of variable can transform the equation into a known form. However, no success yet!","I am trying to solve the following differential equation: \begin{equation} (1-x^{2})y''+\{\alpha x+1 \}y'+\{\beta^{2}x^{2}-\beta{x}+\gamma\}y=0 \end{equation} where $y=y(x)$. $y'$ and $y''$ are the first and second derivatives respectively. $\alpha$, $\beta$ and $\gamma$ are constants. There are various second order differential equations that are very similar looking to this equation. For instance Mathieu equation, Legendre equation and generalized Hypergeometric equation, but not quite the same. I am guessing that a change of variable can transform the equation into a known form. However, no success yet!",,['ordinary-differential-equations']
89,Find the Laplace inverse of the following.,Find the Laplace inverse of the following.,,$$ \frac{2s+5}{s^2+6s+34} $$ I am stuck on this part: Wolfram has the step by step showing that you simply split up the original fraction into $$ \frac{2s}{s^2+6s+34} + \frac{5}{s^2+6s+34} $$ and then it solves it. But that doesn't help. Could someone please help me understand how to do this problem?,$$ \frac{2s+5}{s^2+6s+34} $$ I am stuck on this part: Wolfram has the step by step showing that you simply split up the original fraction into $$ \frac{2s}{s^2+6s+34} + \frac{5}{s^2+6s+34} $$ and then it solves it. But that doesn't help. Could someone please help me understand how to do this problem?,,"['ordinary-differential-equations', 'laplace-transform']"
90,Estimate for a weak solution to a PDE,Estimate for a weak solution to a PDE,,"Let $f \in L^2(B_R(0))$ and let $u \in W^{1,2}(B_R(0))$ be a weak solution of the equation $$Lu = - \sum_{i,j=1}^{n} D_i(a_{ij}D_ju)+ \sum_{i=1}^{n} b_i D_i u + cu =f.$$ There are constants $0 \le B,C < \infty$ so that $$\sup_{B_R(0)} \sqrt{\sum_{i=1}^{n} |b_i|^2 } \le B  \text{ and }\sup_{B_R(0)}|c| \le C.$$ We also assume the uniform ellipticity condition $$\exists 0 < \lambda \le \Lambda < \infty \ \forall x \in B_R(0) \forall \xi \in \mathbb{R}^n : \lambda |\xi|^2 \le \langle \xi,A\xi \rangle \le \Lambda |\xi|^2.$$ I want to show that then $$||Du||_{L^2(B_{R/2}(0))} \le K (||u||_{L^2(B_R(0))} + ||f||_{L^2(B_R(0))}),$$ where $K $ depends only on $B,C, \lambda , \Lambda $ and $R$. The hint is to use the test function $\phi = u \eta^2$ where $\eta \in C_c^{1} (B_{3R/4}(0))$ with $\eta=1$ on $B_{R/2}(0)$ and $ 0 \le \eta \le 1$ with $|D\eta(x)| \le \frac{4}{R} $ for almost all $x \in B_R(0).$ Does anybody have an idea?","Let $f \in L^2(B_R(0))$ and let $u \in W^{1,2}(B_R(0))$ be a weak solution of the equation $$Lu = - \sum_{i,j=1}^{n} D_i(a_{ij}D_ju)+ \sum_{i=1}^{n} b_i D_i u + cu =f.$$ There are constants $0 \le B,C < \infty$ so that $$\sup_{B_R(0)} \sqrt{\sum_{i=1}^{n} |b_i|^2 } \le B  \text{ and }\sup_{B_R(0)}|c| \le C.$$ We also assume the uniform ellipticity condition $$\exists 0 < \lambda \le \Lambda < \infty \ \forall x \in B_R(0) \forall \xi \in \mathbb{R}^n : \lambda |\xi|^2 \le \langle \xi,A\xi \rangle \le \Lambda |\xi|^2.$$ I want to show that then $$||Du||_{L^2(B_{R/2}(0))} \le K (||u||_{L^2(B_R(0))} + ||f||_{L^2(B_R(0))}),$$ where $K $ depends only on $B,C, \lambda , \Lambda $ and $R$. The hint is to use the test function $\phi = u \eta^2$ where $\eta \in C_c^{1} (B_{3R/4}(0))$ with $\eta=1$ on $B_{R/2}(0)$ and $ 0 \le \eta \le 1$ with $|D\eta(x)| \le \frac{4}{R} $ for almost all $x \in B_R(0).$ Does anybody have an idea?",,"['real-analysis', 'analysis', 'functional-analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
91,"Is it possible, that the fist two weak eigenvalues of $-\Delta$ in a bounded domain are equal?","Is it possible, that the fist two weak eigenvalues of  in a bounded domain are equal?",-\Delta,"Let $\Omega\subseteq\mathbb{R}^n$ be a bounded domain $\lambda_1$ be the first weak eigenvalue of $-\Delta$ in $\Omega$ $\varphi_1$ be the weak eigenfunction associated with $\lambda_1$ $H:=W_0^{1,2}(\Omega)$ be the Sobolev space $|\;\cdot\;|_p$ be the seminorm $$|u|_p^p:=\int_\Omega|\nabla u|^p\;d\lambda^n\;\;\;\text{for }u:\Omega\to\mathbb{R}\;\text{weakly differentiable}$$ $\left\|\;\cdot\;\right\|_p$ be the $L^p(\Omega)$-norm and $\langle\;\cdot\;,\;\cdot\;\rangle_2$ be the scalar product in $L^2(\Omega)$ From the basic theory of the eigenvalue problem of the Laplacian, one knows that $$\lambda_1=\inf_{H\setminus\left\{0\right\}}R\;\;\;\;\;\text{with }R(u):=\frac{|u|_2^2}{\left\|u\right\|_2^2}\;\;\;\text{for }u\in H\setminus\left\{0\right\}$$ and $\varphi_1$ can be constructed such that $\left\|\varphi_1\right\|_2=1$ $R(\varphi_1)=\lambda_1$ Since different eigenvalues correspond to $L^2$-orthogonal eigenfunctions (with $L^2$-orthogonal gradients), it's plausible to seek for the next eigenfunction $\varphi_2$ in $$U_1:=\left\{u\in H:\langle u,\varphi_1\rangle_2=0 \right\}\;.$$ We may observe, that $U_1$ is the null space of the continuous operator $\left.\langle\;\cdot\;,\varphi\rangle_2\right|_H$. Thus, $H$ is a closed subspace of $H$ and thereby a Hilbert space itself. So, we can apply the same process used to find $(\lambda_1,\varphi_2)$ in order to find $$\lambda_2:=\inf_{U_1\setminus\left\{0\right\}}R$$ and $\varphi_2$. Now, using the same arguments as discussed in a similar question , it's easy to see, that $\lambda_2\ge \lambda_1$ (Please don't get confused with the naming of the $\lambda_i$ in the other question). However, is it possible that we have $\lambda_1=\lambda_2$?","Let $\Omega\subseteq\mathbb{R}^n$ be a bounded domain $\lambda_1$ be the first weak eigenvalue of $-\Delta$ in $\Omega$ $\varphi_1$ be the weak eigenfunction associated with $\lambda_1$ $H:=W_0^{1,2}(\Omega)$ be the Sobolev space $|\;\cdot\;|_p$ be the seminorm $$|u|_p^p:=\int_\Omega|\nabla u|^p\;d\lambda^n\;\;\;\text{for }u:\Omega\to\mathbb{R}\;\text{weakly differentiable}$$ $\left\|\;\cdot\;\right\|_p$ be the $L^p(\Omega)$-norm and $\langle\;\cdot\;,\;\cdot\;\rangle_2$ be the scalar product in $L^2(\Omega)$ From the basic theory of the eigenvalue problem of the Laplacian, one knows that $$\lambda_1=\inf_{H\setminus\left\{0\right\}}R\;\;\;\;\;\text{with }R(u):=\frac{|u|_2^2}{\left\|u\right\|_2^2}\;\;\;\text{for }u\in H\setminus\left\{0\right\}$$ and $\varphi_1$ can be constructed such that $\left\|\varphi_1\right\|_2=1$ $R(\varphi_1)=\lambda_1$ Since different eigenvalues correspond to $L^2$-orthogonal eigenfunctions (with $L^2$-orthogonal gradients), it's plausible to seek for the next eigenfunction $\varphi_2$ in $$U_1:=\left\{u\in H:\langle u,\varphi_1\rangle_2=0 \right\}\;.$$ We may observe, that $U_1$ is the null space of the continuous operator $\left.\langle\;\cdot\;,\varphi\rangle_2\right|_H$. Thus, $H$ is a closed subspace of $H$ and thereby a Hilbert space itself. So, we can apply the same process used to find $(\lambda_1,\varphi_2)$ in order to find $$\lambda_2:=\inf_{U_1\setminus\left\{0\right\}}R$$ and $\varphi_2$. Now, using the same arguments as discussed in a similar question , it's easy to see, that $\lambda_2\ge \lambda_1$ (Please don't get confused with the naming of the $\lambda_i$ in the other question). However, is it possible that we have $\lambda_1=\lambda_2$?",,"['real-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'eigenvalues-eigenvectors', 'sobolev-spaces']"
92,"If $l_i$ is the first weak eigenvalue of $-\Delta$ in a domain $G_i$ and $G_1\subseteq G_2$, then $l_1\ge l_2$ and equality is possible","If  is the first weak eigenvalue of  in a domain  and , then  and equality is possible",l_i -\Delta G_i G_1\subseteq G_2 l_1\ge l_2,"Let $\Omega_i\subseteq\mathbb{R}^n$ be a domain $\lambda_i$ be the first weak eigenvalue of $-\Delta$ in $\Omega_i$ It's easy to verify that $\Omega_1\subseteq\Omega_2$ implies $\lambda_1\ge \lambda_2$, by considering the corresponding Rayleigh quotients. However, while I'm quite sure that we don't need to have strict inequality, i.e. $\lambda_1>\lambda_2$, I failed to find an elegant counter example.","Let $\Omega_i\subseteq\mathbb{R}^n$ be a domain $\lambda_i$ be the first weak eigenvalue of $-\Delta$ in $\Omega_i$ It's easy to verify that $\Omega_1\subseteq\Omega_2$ implies $\lambda_1\ge \lambda_2$, by considering the corresponding Rayleigh quotients. However, while I'm quite sure that we don't need to have strict inequality, i.e. $\lambda_1>\lambda_2$, I failed to find an elegant counter example.",,"['real-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'eigenvalues-eigenvectors', 'sobolev-spaces']"
93,Finite Difference for Hamilton-Jacobi-Bellman without boundary conditions,Finite Difference for Hamilton-Jacobi-Bellman without boundary conditions,,"Let $t\in\mathbb{R}_+$ denote time, $x \in X$ is the state and $u \in U$ the control. The objective function is $F:X \times U \to\mathbb{R}$ and $f:X \times U \to\mathbb{R}$ is the law of motion for the state, i.e.  \begin{align} \dot{x}(t) = f(x(t),u(t)) \end{align} Define the value function $v:X\to\mathbb{R}$ by  \begin{align} v(x_0):=\max_{\{u(t)\}_{t \geq 0}}\left[\int^\infty_0{e^{-\rho t}F(x(t),u(t))dt}\right] \end{align} where $x_0:=x(0)$ is a given initial condition and $\rho\in\mathbb{R}_{++}$ is a parameter (time preference rate). The Hamilton-Jacobi-Bellman equation in current time reads \begin{align} \rho v(x) = \max_u[F(x,u) + v'(x)f(x,u)] \end{align} which is an ODE. If one can solve for $v(\cdot)$ we can recover the optimal path \begin{align} \{u^*(t):t\in\mathbb{R}_+\}  = \arg\max_{\{u(t)\}_{t \geq 0}}\left[\int^\infty_0{e^{-\rho t}F(x(t),u(t))dt}\right] \end{align} I try to solve the HJB equation by value function iteration. That is guessing an initial value function and then iterate until it converges to the true value. I update via an implicit method a la \begin{align} \frac{v_{j+1}(x_n)-v_j(x_n)}{\Delta} + \rho v_j(x_n) = F(x_n,u^*_n) + v'_{j+1}(x_n)f(x_n,u^*_n) \end{align} where $\Delta$ is a scaling paramter, $j=0,1,2,\ldots$ the number of iterations and $x_n$ an element of the grid of the state space \begin{align} \mathbf{x} := [x_n]_{n=1}^N\in X^N \end{align} and $u_n$ is the solution of \begin{align} u^*_n = \arg\max_u [F(x_n,u_n) + v'_{j+1}(x_n)f(x_n,u_n)] \end{align} To get an expression for $v'(\cdot)$ we shall define slope coefficients by \begin{align} v'(x_n) :\approx \begin{cases} \displaystyle\frac{v(x_{n+1}) - v(x_n)}{\Delta x}\quad&\text{forward difference}\\[2mm] \displaystyle\frac{v(x_{n+1}) - v(x_{n-1})}{2\Delta x}\quad&\text{central difference}\\[2mm] \displaystyle\frac{v(x_n) - v(x_{n-1})}{\Delta x}\quad&\text{backward difference}\\ \end{cases} \end{align} where $\Delta x$ denotes the equidistance between the elements of $\mathbf{x}$, i.e. \begin{align} \Delta x:=x_{n+1} - x_n = x_n - x_{n-1}~\forall n  \end{align} Problem It's known that for forward and backward difference one derivative is missing and for central difference two. We can deal with this issue by a boundary condition on either side. However, consider that there is no boundary condition given. I can still approx $v'(x)$ by a combination of forward, backward and central difference. For instance (case 1) \begin{align} v'(x_n) \approx \begin{cases} \displaystyle\frac{v(x_{n+1}) - v(x_n)}{\Delta x}\quad&n=1\\[2mm] \displaystyle\frac{v(x_{n+1}) - v(x_{n-1})}{2\Delta x}\quad&1 < n <N\\[2mm] \displaystyle\frac{v(x_n) - v(x_{n-1})}{\Delta x}\quad&n=N \end{cases} \end{align} or (case 2) \begin{align} v'(x_n) \approx \begin{cases} \displaystyle\frac{v(x_{n+1}) - v(x_n)}{\Delta x}\quad&1\leq n <N\\[2mm] \displaystyle\frac{v(x_n) - v(x_{n-1})}{\Delta x}\quad&n=N \end{cases} \end{align} Note that in the second case $v'(x_{N-1})=v'(x_N)$. I was wondering what's common practice in the math profession. I'm asking because I get significant different results for the different cases (irrespective of $N$). PS: Note that I'm neither a mathematician nor a native english speaker. So I apologize for misunderstandigs.","Let $t\in\mathbb{R}_+$ denote time, $x \in X$ is the state and $u \in U$ the control. The objective function is $F:X \times U \to\mathbb{R}$ and $f:X \times U \to\mathbb{R}$ is the law of motion for the state, i.e.  \begin{align} \dot{x}(t) = f(x(t),u(t)) \end{align} Define the value function $v:X\to\mathbb{R}$ by  \begin{align} v(x_0):=\max_{\{u(t)\}_{t \geq 0}}\left[\int^\infty_0{e^{-\rho t}F(x(t),u(t))dt}\right] \end{align} where $x_0:=x(0)$ is a given initial condition and $\rho\in\mathbb{R}_{++}$ is a parameter (time preference rate). The Hamilton-Jacobi-Bellman equation in current time reads \begin{align} \rho v(x) = \max_u[F(x,u) + v'(x)f(x,u)] \end{align} which is an ODE. If one can solve for $v(\cdot)$ we can recover the optimal path \begin{align} \{u^*(t):t\in\mathbb{R}_+\}  = \arg\max_{\{u(t)\}_{t \geq 0}}\left[\int^\infty_0{e^{-\rho t}F(x(t),u(t))dt}\right] \end{align} I try to solve the HJB equation by value function iteration. That is guessing an initial value function and then iterate until it converges to the true value. I update via an implicit method a la \begin{align} \frac{v_{j+1}(x_n)-v_j(x_n)}{\Delta} + \rho v_j(x_n) = F(x_n,u^*_n) + v'_{j+1}(x_n)f(x_n,u^*_n) \end{align} where $\Delta$ is a scaling paramter, $j=0,1,2,\ldots$ the number of iterations and $x_n$ an element of the grid of the state space \begin{align} \mathbf{x} := [x_n]_{n=1}^N\in X^N \end{align} and $u_n$ is the solution of \begin{align} u^*_n = \arg\max_u [F(x_n,u_n) + v'_{j+1}(x_n)f(x_n,u_n)] \end{align} To get an expression for $v'(\cdot)$ we shall define slope coefficients by \begin{align} v'(x_n) :\approx \begin{cases} \displaystyle\frac{v(x_{n+1}) - v(x_n)}{\Delta x}\quad&\text{forward difference}\\[2mm] \displaystyle\frac{v(x_{n+1}) - v(x_{n-1})}{2\Delta x}\quad&\text{central difference}\\[2mm] \displaystyle\frac{v(x_n) - v(x_{n-1})}{\Delta x}\quad&\text{backward difference}\\ \end{cases} \end{align} where $\Delta x$ denotes the equidistance between the elements of $\mathbf{x}$, i.e. \begin{align} \Delta x:=x_{n+1} - x_n = x_n - x_{n-1}~\forall n  \end{align} Problem It's known that for forward and backward difference one derivative is missing and for central difference two. We can deal with this issue by a boundary condition on either side. However, consider that there is no boundary condition given. I can still approx $v'(x)$ by a combination of forward, backward and central difference. For instance (case 1) \begin{align} v'(x_n) \approx \begin{cases} \displaystyle\frac{v(x_{n+1}) - v(x_n)}{\Delta x}\quad&n=1\\[2mm] \displaystyle\frac{v(x_{n+1}) - v(x_{n-1})}{2\Delta x}\quad&1 < n <N\\[2mm] \displaystyle\frac{v(x_n) - v(x_{n-1})}{\Delta x}\quad&n=N \end{cases} \end{align} or (case 2) \begin{align} v'(x_n) \approx \begin{cases} \displaystyle\frac{v(x_{n+1}) - v(x_n)}{\Delta x}\quad&1\leq n <N\\[2mm] \displaystyle\frac{v(x_n) - v(x_{n-1})}{\Delta x}\quad&n=N \end{cases} \end{align} Note that in the second case $v'(x_{N-1})=v'(x_N)$. I was wondering what's common practice in the math profession. I'm asking because I get significant different results for the different cases (irrespective of $N$). PS: Note that I'm neither a mathematician nor a native english speaker. So I apologize for misunderstandigs.",,"['ordinary-differential-equations', 'reference-request', 'numerical-methods', 'finite-differences', 'optimal-control']"
94,solving differential equation?,solving differential equation?,,"I have encountered this set of equation in a book explaining the cycloid motion of particle in orthogonal magnetic and electric field. In usual coordinate system. $y''(t)=pz'(t)$ $z''(t)=p(q-y'(t))$ where p and q are constants and $y(t)$, $z(t)$ are position of particle in space at time t.Their general solution they written is $y(t)=C_1\cos(pt)+C_2\sin(pt)+qt+C_3$ $z(t)=C_2\cos(pt)-C_1\sin(pt)+C_4$ which i am unable to solve. Please help.","I have encountered this set of equation in a book explaining the cycloid motion of particle in orthogonal magnetic and electric field. In usual coordinate system. $y''(t)=pz'(t)$ $z''(t)=p(q-y'(t))$ where p and q are constants and $y(t)$, $z(t)$ are position of particle in space at time t.Their general solution they written is $y(t)=C_1\cos(pt)+C_2\sin(pt)+qt+C_3$ $z(t)=C_2\cos(pt)-C_1\sin(pt)+C_4$ which i am unable to solve. Please help.",,['ordinary-differential-equations']
95,About the standard textbook example for Laplace Equation and Separation of Variables,About the standard textbook example for Laplace Equation and Separation of Variables,,"Here is a standard example typically cited to illustrate the technique of separation of variables. Suppose we have a semi-infinite rectangular strip formed by boundaries $x=0, y=0, y=a$ and we are to solve Laplace equation $\Delta f=0$ with boundary conditions (i) $f=0$ when $y=0$ or $y=a$ (ii) $f \to 0$ as $x \to \infty$ (iii) $f\equiv C$ when $x=0$ ($C$ is a constant). A typical solution by separation of variables will first yield the ""solution pieces"" $f_n=c_n e^{-n\pi x/a} \sin(n\pi y/a)$ (where $n=1,2,...$) which satisfies (i) and (ii), and then use the method of ""handwave"" and claim that by linearity, we can consider the candidate $f_\infty := \sum_{n=1}^\infty c_n e^{-n\pi x/a} \sin(n\pi y/a)$ and try to find $c_n$ such that (iii) holds. My question : Yes, the Laplace operator is linear but that only allows for us to piece together finitely many solutions. How do we know that the infinite sum $f_\infty$ is still ""differentiable"" within the strip? (by ""differentiable"" here, I meant that the second partial derivatives exist). For example, along the line $x=0$, isn't it the case that $f(0,0)=f(0,a)=0$ but $f(0,y)=C$ for $0<y<a$ so the fitted $f_\infty$ could not be differentiable at $(0,0)$ and $(0,a)$. Are these the only two spots where $f_\infty$ is allowed to be (or turns out to be) non-differentiable? In addition, this example confuses me because when I learnt the general formulation of a Laplace equation problem, there is the PDE $\Delta f=0$ and boundary conditions, and I always thought that $\Delta f$ should at least make sense on the boundary but in this example, the points $(0,0)$ and $(0,a)$ are doomed to fail. If we completely allow $f$ to be non-differentiable at boundary, then theorems like uniqueness of solution would not hold (e.g. piecewise define $f$ by choosing any $f$ that satisfies $\Delta f=0$ on the interior and separately take the boundary values). So why is it that we consent the poor behavior of the points $(0,0)$ and $(0,a)$ here?","Here is a standard example typically cited to illustrate the technique of separation of variables. Suppose we have a semi-infinite rectangular strip formed by boundaries $x=0, y=0, y=a$ and we are to solve Laplace equation $\Delta f=0$ with boundary conditions (i) $f=0$ when $y=0$ or $y=a$ (ii) $f \to 0$ as $x \to \infty$ (iii) $f\equiv C$ when $x=0$ ($C$ is a constant). A typical solution by separation of variables will first yield the ""solution pieces"" $f_n=c_n e^{-n\pi x/a} \sin(n\pi y/a)$ (where $n=1,2,...$) which satisfies (i) and (ii), and then use the method of ""handwave"" and claim that by linearity, we can consider the candidate $f_\infty := \sum_{n=1}^\infty c_n e^{-n\pi x/a} \sin(n\pi y/a)$ and try to find $c_n$ such that (iii) holds. My question : Yes, the Laplace operator is linear but that only allows for us to piece together finitely many solutions. How do we know that the infinite sum $f_\infty$ is still ""differentiable"" within the strip? (by ""differentiable"" here, I meant that the second partial derivatives exist). For example, along the line $x=0$, isn't it the case that $f(0,0)=f(0,a)=0$ but $f(0,y)=C$ for $0<y<a$ so the fitted $f_\infty$ could not be differentiable at $(0,0)$ and $(0,a)$. Are these the only two spots where $f_\infty$ is allowed to be (or turns out to be) non-differentiable? In addition, this example confuses me because when I learnt the general formulation of a Laplace equation problem, there is the PDE $\Delta f=0$ and boundary conditions, and I always thought that $\Delta f$ should at least make sense on the boundary but in this example, the points $(0,0)$ and $(0,a)$ are doomed to fail. If we completely allow $f$ to be non-differentiable at boundary, then theorems like uniqueness of solution would not hold (e.g. piecewise define $f$ by choosing any $f$ that satisfies $\Delta f=0$ on the interior and separately take the boundary values). So why is it that we consent the poor behavior of the points $(0,0)$ and $(0,a)$ here?",,"['ordinary-differential-equations', 'partial-differential-equations']"
96,Can I lower the order of differential equation in this example?,Can I lower the order of differential equation in this example?,,$$y'''' + 9y'' = 18 (9x+2)$$ Can I make a substitution $$y''=p$$ and solve $$p''+ 9p = 18 (9x+2)$$ Or not ?,$$y'''' + 9y'' = 18 (9x+2)$$ Can I make a substitution $$y''=p$$ and solve $$p''+ 9p = 18 (9x+2)$$ Or not ?,,['ordinary-differential-equations']
97,Initial Value Problem using Laplace Transforms,Initial Value Problem using Laplace Transforms,,"Solve  using Laplace Transform:   $$y''(t)+2y'(t)+5y(t)=xf(t), \\ y(0)=1,y'(0)=1$$  where $x$ is a constant. Once the solution is found, evaluate the limit as $t \to\infty$. Progress: If I have done it right, I have ended up with $Y(s)=(3+sF(s))/(s^2+s+5)$. I am having trouble with how to take the inverse since $F(s)$ is involved.","Solve  using Laplace Transform:   $$y''(t)+2y'(t)+5y(t)=xf(t), \\ y(0)=1,y'(0)=1$$  where $x$ is a constant. Once the solution is found, evaluate the limit as $t \to\infty$. Progress: If I have done it right, I have ended up with $Y(s)=(3+sF(s))/(s^2+s+5)$. I am having trouble with how to take the inverse since $F(s)$ is involved.",,"['ordinary-differential-equations', 'laplace-transform']"
98,"If $Z=\frac{1}{\partial_{x}^{2}+\partial_{x}\partial_{y}-12\partial_{y}^{2}}.y e^{3x+y}$, what is $Z$?","If , what is ?",Z=\frac{1}{\partial_{x}^{2}+\partial_{x}\partial_{y}-12\partial_{y}^{2}}.y e^{3x+y} Z,"If: $$Z=\frac{1}{\partial_{x}^{2}+\partial_{x}\partial_{y}-12\partial_{y}^{2}}y e^{3x+y}$$ Then what is $Z$? I know that: $$\frac{1}{\partial_{x}^{2}+\partial_{x}\partial_{y}-12\partial_{y}^{2}}y e^{3x+y}=e^{3x+y}\frac{1}{(\partial_{x}+3)^{2}+(\partial_{x}+3)(\partial_{y}+1)-12(\partial_{y}+1)^{2}}y $$ but I don't know the value of: $$\frac{1}{(\partial_{x}+3)^{2}+(\partial_{x}+3)(\partial_{y}+1)-12(\partial_{y}+1)^{2}}y$$ Is there a general way to find the value of equations of the form: $$\frac{1}{\phi(\partial_{x},\partial_{y})}f(x,y)?$$ The answer is actually $Z=e^{3x+y}{\frac{1}{98 }(21 x^3 - 8 x + 14 x y)}$","If: $$Z=\frac{1}{\partial_{x}^{2}+\partial_{x}\partial_{y}-12\partial_{y}^{2}}y e^{3x+y}$$ Then what is $Z$? I know that: $$\frac{1}{\partial_{x}^{2}+\partial_{x}\partial_{y}-12\partial_{y}^{2}}y e^{3x+y}=e^{3x+y}\frac{1}{(\partial_{x}+3)^{2}+(\partial_{x}+3)(\partial_{y}+1)-12(\partial_{y}+1)^{2}}y $$ but I don't know the value of: $$\frac{1}{(\partial_{x}+3)^{2}+(\partial_{x}+3)(\partial_{y}+1)-12(\partial_{y}+1)^{2}}y$$ Is there a general way to find the value of equations of the form: $$\frac{1}{\phi(\partial_{x},\partial_{y})}f(x,y)?$$ The answer is actually $Z=e^{3x+y}{\frac{1}{98 }(21 x^3 - 8 x + 14 x y)}$",,"['ordinary-differential-equations', 'partial-differential-equations', 'partial-derivative']"
99,Help finding the critical values of α where the qualitative nature of the phase portrait for the system changes?,Help finding the critical values of α where the qualitative nature of the phase portrait for the system changes?,,"I was asked to solved for the eigenvalues in terms of α for 2X2 matrix and so i did and my answer was marked as correct. Then I was asked to solve for this: The roots are complex when? There is a saddle point for? The equilibrium point is a stable node where? I tried solving the problem by finding values which would give me 1 and zero for the eigenvalues and got the values of -24/11, -25/11 and -26/11 but the were all wrong. How would i solve for these three questions?  can someone walk me through the steps? The eigenvalues in terms of α are: $r = -1 + \dfrac{\sqrt{100 + 44 \alpha}}{2}$ $r_2 = -1 - \dfrac{\sqrt{100 + 44 \alpha}}{2}$","I was asked to solved for the eigenvalues in terms of α for 2X2 matrix and so i did and my answer was marked as correct. Then I was asked to solve for this: The roots are complex when? There is a saddle point for? The equilibrium point is a stable node where? I tried solving the problem by finding values which would give me 1 and zero for the eigenvalues and got the values of -24/11, -25/11 and -26/11 but the were all wrong. How would i solve for these three questions?  can someone walk me through the steps? The eigenvalues in terms of α are: $r = -1 + \dfrac{\sqrt{100 + 44 \alpha}}{2}$ $r_2 = -1 - \dfrac{\sqrt{100 + 44 \alpha}}{2}$",,"['matrices', 'ordinary-differential-equations', 'eigenvalues-eigenvectors', 'matrix-equations', 'eigenfunctions']"
