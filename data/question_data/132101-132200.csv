,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Solve this First Order Non-Linear Differential Equation,Solve this First Order Non-Linear Differential Equation,,"I need to solve $$y'=x\sqrt{x^2+y^2},\quad y(1)=\alpha \in\mathbb{R}$$ I'm not sure how to proceed . it's nonlinear and The method of Separation of Variables cannot  be   be possible. by Cauchy Lipschitz , the Equation admits a local solution","I need to solve I'm not sure how to proceed . it's nonlinear and The method of Separation of Variables cannot  be   be possible. by Cauchy Lipschitz , the Equation admits a local solution","y'=x\sqrt{x^2+y^2},\quad y(1)=\alpha \in\mathbb{R}",['ordinary-differential-equations']
1,Limits of the wave equation with piecewise constant propagation speed,Limits of the wave equation with piecewise constant propagation speed,,"Consider a wave equation $$\frac{\partial^2 u}{\partial t^2} = c(x)^2 \frac{\partial^2 u}{\partial x^2} \tag{1}$$ In frequency domain this becomes an ODE: $$-\omega^2 u = c(x)^2 \frac{\partial^2 u}{\partial x^2} \tag{2}$$ We can solve (2) analytically when $c(x)$ is constant, then the solution is $\exp\left(\pm\frac{i \omega}{c} x\right)$ . We can also solve it analytically if $c(x)$ is piecewise constant: write in every region-of-constant- $c$ a linear combination of leftward and rightward propagating waves, impose the appropriate continuity conditions on the interfaces between regions with different values of $c$ , solve for the coefficients of the linear combinations. Suppose now that $c(x)$ is some arbitrary function. We can still write it as a limit of piecewise constant functions: $$c(x)=\lim_{\Delta\rightarrow 0^+} \sum_{i=-\infty}^{\infty} \left\{\begin{matrix}c(i\Delta) & x\in [i\Delta,(i+1)\Delta[ \\ 0 & \text{otherwise}\end{matrix}\right.$$ Let us call these piecewise constant approximations to $c$ , $c_{\Delta}$ $$c_{\Delta}(x)=\sum_{i=-\infty}^{\infty} \left\{\begin{matrix}c(i\Delta) & x\in [i\Delta,(i+1)\Delta[ \\ 0 & \text{otherwise}\end{matrix}\right.$$ We can analytically solve, for any strictly positive $\Delta$ , $$-\omega^2 u_{\Delta}= c_{\Delta}(x)^2 \frac{\partial^2 u_{\Delta}}{\partial x^2}$$ Question Is $\lim_{\Delta\rightarrow 0^+} u_{\Delta} = u$ ? In other words, can we approximate general solutions of (2) by substituting in a piecewise constant approximation of $c(x)$ , and then solving analytically? I expect the answer to be ""no"". because $\lim_{\Delta\rightarrow 0^+} c_{\Delta} = c$ , but $\lim_{\Delta\rightarrow 0^+} \frac{\partial}{\partial x}c_{\Delta} \neq \frac{\partial}{\partial x} c$ , in fact the limit doesn't even exist In literature on numerical solutions of PDEs, I have never seen this suggested as a viable way of approximating solutions of wave equations.","Consider a wave equation In frequency domain this becomes an ODE: We can solve (2) analytically when is constant, then the solution is . We can also solve it analytically if is piecewise constant: write in every region-of-constant- a linear combination of leftward and rightward propagating waves, impose the appropriate continuity conditions on the interfaces between regions with different values of , solve for the coefficients of the linear combinations. Suppose now that is some arbitrary function. We can still write it as a limit of piecewise constant functions: Let us call these piecewise constant approximations to , We can analytically solve, for any strictly positive , Question Is ? In other words, can we approximate general solutions of (2) by substituting in a piecewise constant approximation of , and then solving analytically? I expect the answer to be ""no"". because , but , in fact the limit doesn't even exist In literature on numerical solutions of PDEs, I have never seen this suggested as a viable way of approximating solutions of wave equations.","\frac{\partial^2 u}{\partial t^2} = c(x)^2 \frac{\partial^2 u}{\partial x^2} \tag{1} -\omega^2 u = c(x)^2 \frac{\partial^2 u}{\partial x^2} \tag{2} c(x) \exp\left(\pm\frac{i \omega}{c} x\right) c(x) c c c(x) c(x)=\lim_{\Delta\rightarrow 0^+} \sum_{i=-\infty}^{\infty} \left\{\begin{matrix}c(i\Delta) & x\in [i\Delta,(i+1)\Delta[ \\ 0 & \text{otherwise}\end{matrix}\right. c c_{\Delta} c_{\Delta}(x)=\sum_{i=-\infty}^{\infty} \left\{\begin{matrix}c(i\Delta) & x\in [i\Delta,(i+1)\Delta[ \\ 0 & \text{otherwise}\end{matrix}\right. \Delta -\omega^2 u_{\Delta}= c_{\Delta}(x)^2 \frac{\partial^2 u_{\Delta}}{\partial x^2} \lim_{\Delta\rightarrow 0^+} u_{\Delta} = u c(x) \lim_{\Delta\rightarrow 0^+} c_{\Delta} = c \lim_{\Delta\rightarrow 0^+} \frac{\partial}{\partial x}c_{\Delta} \neq \frac{\partial}{\partial x} c","['ordinary-differential-equations', 'reference-request', 'approximation-theory']"
2,How to show the optimization/ODE fixed point iteration steps converge?,How to show the optimization/ODE fixed point iteration steps converge?,,"I have $\vec{C} = G(\vec{\beta})$ by solving a system of ODE numerically.  Thanks for the help of Robert the ODE can be found in this link: Solving a system of ODE Also $\vec{\beta}$ should satisfy $$A{\vec{\beta}}\le f(\vec{\beta}, \vec{C})$$ and $$\max 19\beta_1+0.5\beta_2+16\beta_3.$$ where $A$ is a given matrix and $f$ is some given function. I am thinking of solving this process using iteration. I have a initial approximation $\vec{\beta^0}$ , then for $k=1,2,3...$ solve Part $1$ using $\vec{C^k} = G(\vec{\beta^{k-1}})$ then solving part $2$ optimization using $$A{\vec{\beta^{k+1}}}\le f(\vec{\beta^k}, \vec{C^k})$$ and $$\max 19\beta_1^{k+1}+0.5\beta_2^{k+1}+16\beta_3^{k+1}.$$ But I am worried this step will not converge as $k\to\infty$ . My questions is if this method will converge? if it is not, how to solve the optimization/ODE system to make it converge to the true solution? Any help is appreciated! Many thanks!","I have by solving a system of ODE numerically.  Thanks for the help of Robert the ODE can be found in this link: Solving a system of ODE Also should satisfy and where is a given matrix and is some given function. I am thinking of solving this process using iteration. I have a initial approximation , then for solve Part using then solving part optimization using and But I am worried this step will not converge as . My questions is if this method will converge? if it is not, how to solve the optimization/ODE system to make it converge to the true solution? Any help is appreciated! Many thanks!","\vec{C} = G(\vec{\beta}) \vec{\beta} A{\vec{\beta}}\le f(\vec{\beta}, \vec{C}) \max 19\beta_1+0.5\beta_2+16\beta_3. A f \vec{\beta^0} k=1,2,3... 1 \vec{C^k} = G(\vec{\beta^{k-1}}) 2 A{\vec{\beta^{k+1}}}\le f(\vec{\beta^k}, \vec{C^k}) \max 19\beta_1^{k+1}+0.5\beta_2^{k+1}+16\beta_3^{k+1}. k\to\infty","['ordinary-differential-equations', 'convergence-divergence', 'optimization', 'fixed-point-theorems']"
3,$dy/dx=\sqrt{x^2+y^2}$,,dy/dx=\sqrt{x^2+y^2},"$$\frac{dy}{dx}=\sqrt{x^2+y^2}$$ slope=distance from origin, should be simple and interesting. May have no solution! I have tried several approaches, best : $(\frac{dy}{dx}-y)(\frac{dy}{dx}+y)=x^2$  multiply by $e(-x) * e(+x)$ as integrating factor. Substitute $\frac{1}{2}x^2=t$. Second approach: $y=x\sinh(u)$ and $x=e(t)$ yields $\frac{du}{dt} + \tanh(u)=e(t)$. Sorry, I am not yet using the proper format.","$$\frac{dy}{dx}=\sqrt{x^2+y^2}$$ slope=distance from origin, should be simple and interesting. May have no solution! I have tried several approaches, best : $(\frac{dy}{dx}-y)(\frac{dy}{dx}+y)=x^2$  multiply by $e(-x) * e(+x)$ as integrating factor. Substitute $\frac{1}{2}x^2=t$. Second approach: $y=x\sinh(u)$ and $x=e(t)$ yields $\frac{du}{dt} + \tanh(u)=e(t)$. Sorry, I am not yet using the proper format.",,['ordinary-differential-equations']
4,First Year ODE Exam Question,First Year ODE Exam Question,,"What are the eigenfunctions that satisfy   $$xy''(x) + (1-x)y'(x) + y(x) = -\lambda y(x)$$   on the interval $I = [0, \infty)$ given that the eigenvalues are natural numbers (positive integers) such that $y(0) = 1$? This question was on my first-year university ODE final exam and not a single student was able to solve it. We are trying to have it curved from the final exam grade, do we have a strong case or is this fairly ""ordinary"" material?","What are the eigenfunctions that satisfy   $$xy''(x) + (1-x)y'(x) + y(x) = -\lambda y(x)$$   on the interval $I = [0, \infty)$ given that the eigenvalues are natural numbers (positive integers) such that $y(0) = 1$? This question was on my first-year university ODE final exam and not a single student was able to solve it. We are trying to have it curved from the final exam grade, do we have a strong case or is this fairly ""ordinary"" material?",,['ordinary-differential-equations']
5,Show that every solution $x(t)$ of $x'(t)= A(t)x(t)$ converges to some limit,Show that every solution  of  converges to some limit,x(t) x'(t)= A(t)x(t),"(Long-time asymptotics). Suppose $$\int_0^∞\|A(t)\|\,dt < ∞.$$ Show that every solution $x(t)$ of $x'(t)= A(t)x(t)$ converges to some limit: $\lim_{t→∞} x(t) = x_∞.$ (Hint: First show that all solutions are bounded and then use the corresponding integral equation.) I was able to solve a first part: I used that the solutions are limited by $\|\phi(t,t_0)\| \leq e^ {\int_0^∞\|A(t)\|\,dt}$. As $\int_0^∞\|A(t)\|\,dt < ∞$, then $\|\phi(t,t_0)\| \leq K $. The problem now is to show that the above limit always exists. Someone can help","(Long-time asymptotics). Suppose $$\int_0^∞\|A(t)\|\,dt < ∞.$$ Show that every solution $x(t)$ of $x'(t)= A(t)x(t)$ converges to some limit: $\lim_{t→∞} x(t) = x_∞.$ (Hint: First show that all solutions are bounded and then use the corresponding integral equation.) I was able to solve a first part: I used that the solutions are limited by $\|\phi(t,t_0)\| \leq e^ {\int_0^∞\|A(t)\|\,dt}$. As $\int_0^∞\|A(t)\|\,dt < ∞$, then $\|\phi(t,t_0)\| \leq K $. The problem now is to show that the above limit always exists. Someone can help",,['ordinary-differential-equations']
6,How to handle purely imaginary Hamiltonians,How to handle purely imaginary Hamiltonians,,"Suppose I have a system of complex ODE's of the form $$ i\dot{\mathbf{c}}(t)=\mathbf{f}(\mathbf{c}(t))$$ and I can write down a Hamiltonian such that each ODE can be written as $$\dot{c}_j=\frac{\partial\mathcal{H}}{\partial c_j^*}$$ for each $j$ where * denotes complex conjugate. As a very simple example, the Hamiltonian $$ \mathcal{H}=-i\left(|c_0|^2+|c_1|^2\right) $$ leads to the equations $$i\dot{c}_0=c_0, \qquad i\dot{c}_1=c_1. $$ Questions: What are the conjugate momenta for this system (Are they just the complex conjugates)? Also, is there any way to transform this problem (via action-angle coordinates/madelung transform) to one where the Hamiltonian is purely real or where the system evolves under real dynamics? Is an imaginary Hamiltonian even an issue if I want to analyze a much more complicated non-linear system of this type using canonical perturbation or bifurcation theory?","Suppose I have a system of complex ODE's of the form and I can write down a Hamiltonian such that each ODE can be written as for each where * denotes complex conjugate. As a very simple example, the Hamiltonian leads to the equations Questions: What are the conjugate momenta for this system (Are they just the complex conjugates)? Also, is there any way to transform this problem (via action-angle coordinates/madelung transform) to one where the Hamiltonian is purely real or where the system evolves under real dynamics? Is an imaginary Hamiltonian even an issue if I want to analyze a much more complicated non-linear system of this type using canonical perturbation or bifurcation theory?"," i\dot{\mathbf{c}}(t)=\mathbf{f}(\mathbf{c}(t)) \dot{c}_j=\frac{\partial\mathcal{H}}{\partial c_j^*} j  \mathcal{H}=-i\left(|c_0|^2+|c_1|^2\right)
 i\dot{c}_0=c_0, \qquad i\dot{c}_1=c_1. ","['ordinary-differential-equations', 'dynamical-systems', 'classical-mechanics', 'hamilton-equations']"
7,ODE's: study of the case $x'=f(x/t)$ with $f:\Bbb{R}\to \Bbb{R}$ a $C^1$ function with $f(r)=r$ for some $r\in \Bbb{R}$...,ODE's: study of the case  with  a  function with  for some ...,x'=f(x/t) f:\Bbb{R}\to \Bbb{R} C^1 f(r)=r r\in \Bbb{R},"(Exercise 6, Chap. 1, from Sotomayor's ODE Lessons): Let $f:\Bbb{R}\to \Bbb{R}$ be a $C^1$ function and $r\in \Bbb{R}$ such that $f(r)=r$ . Show that a) if $f'(r)<1$ , then no solution of the equation $$x'=f\left(\frac{x}{t}\right)\,\,\,\,(*)$$ is tangent at $0$ to the solution $\varphi(t)=rt$ . b) if $f'(r)>1$ , then there are infinitely many solutions of $(*)$ tangent to $\varphi(t)=rt$ at the origin. Two functions $\varphi$ and $\psi$ defined for $t>0$ are said to be tangent at $0$ if $\lim\limits_{t\to 0}\dfrac{\psi(t)-\varphi(t)}{t}=0$ . There is a duplicate for this question here: If $f$ continuous differentiable and $f'(r) < 1,$ then $x'=f(x/t)$ has no other solution tangent at zero to $\phi(t)=rt$ but only the answer to item a) is given there and, furthermore, I could not understand it...","(Exercise 6, Chap. 1, from Sotomayor's ODE Lessons): Let be a function and such that . Show that a) if , then no solution of the equation is tangent at to the solution . b) if , then there are infinitely many solutions of tangent to at the origin. Two functions and defined for are said to be tangent at if . There is a duplicate for this question here: If $f$ continuous differentiable and $f'(r) < 1,$ then $x'=f(x/t)$ has no other solution tangent at zero to $\phi(t)=rt$ but only the answer to item a) is given there and, furthermore, I could not understand it...","f:\Bbb{R}\to \Bbb{R} C^1 r\in \Bbb{R} f(r)=r f'(r)<1 x'=f\left(\frac{x}{t}\right)\,\,\,\,(*) 0 \varphi(t)=rt f'(r)>1 (*) \varphi(t)=rt \varphi \psi t>0 0 \lim\limits_{t\to 0}\dfrac{\psi(t)-\varphi(t)}{t}=0",['ordinary-differential-equations']
8,Can we use Power Series Solution for points other than x=0 to escape Frobenius Solution?,Can we use Power Series Solution for points other than x=0 to escape Frobenius Solution?,,"Suppose I have an equation $y"" + P(x)y' + Q(x)y = 0$ Now we apply power series when $P$ and $Q$ are analytic at $x=0$ and apply Frobenius method when $P$ and $Q$ are not analytic at $x=0$ . Now, I want to know why do we apply the Frobenius method? We could equally have taken a power series in terms of $(x-a)$ , where $P$ and $Q$ would analytic at $a$ and $a$ could be anything i.e $(2, 3 ...100)$ . But we don't do that. We always use the Frobenius Method at $x=0$ . So why do we not do that? Is that wrong? Why do we always look for a series centered at $x=0$ ? Is it necessary that we find the series centered at $0$ ? And how does the multiplication of $x^r$ in the Frobenius method with our normal power series correct everything ? Can we use power series at points other than $0$ or Frobenius Method is the only way out?","Suppose I have an equation Now we apply power series when and are analytic at and apply Frobenius method when and are not analytic at . Now, I want to know why do we apply the Frobenius method? We could equally have taken a power series in terms of , where and would analytic at and could be anything i.e . But we don't do that. We always use the Frobenius Method at . So why do we not do that? Is that wrong? Why do we always look for a series centered at ? Is it necessary that we find the series centered at ? And how does the multiplication of in the Frobenius method with our normal power series correct everything ? Can we use power series at points other than or Frobenius Method is the only way out?","y"" + P(x)y' + Q(x)y = 0 P Q x=0 P Q x=0 (x-a) P Q a a (2, 3 ...100) x=0 x=0 0 x^r 0","['ordinary-differential-equations', 'power-series', 'frobenius-method']"
9,ODE with inverse function. Solve $f^{-1}(x)=f'(x)$.,ODE with inverse function. Solve .,f^{-1}(x)=f'(x),"Find all functions $f:\mathbb{R}^+\rightarrow\mathbb{R}^+$ such that $$f^{-1}(x)=f'(x)$$ I think one such function would be of the form $f(x)=ax^b$. But then $b$ would be irrational and when $x\lt0$ this causes problems. So I guess letting $f(x)=-a(-x)^b$ for $x<0$ might work. But that's only one possibble solution, what are all the solutions? Edit: With the comment of Joey Zou, the domain and range has been changed to $(0,\infty)$. Edit: I already know the solution of the form $ax^b$. However, what I'm really asking is whether it is the unique solution.","Find all functions $f:\mathbb{R}^+\rightarrow\mathbb{R}^+$ such that $$f^{-1}(x)=f'(x)$$ I think one such function would be of the form $f(x)=ax^b$. But then $b$ would be irrational and when $x\lt0$ this causes problems. So I guess letting $f(x)=-a(-x)^b$ for $x<0$ might work. But that's only one possibble solution, what are all the solutions? Edit: With the comment of Joey Zou, the domain and range has been changed to $(0,\infty)$. Edit: I already know the solution of the form $ax^b$. However, what I'm really asking is whether it is the unique solution.",,['ordinary-differential-equations']
10,Heat equation to mixed boundary conditions,Heat equation to mixed boundary conditions,,"I'm studying heat equations, I've more or less understood how to solve them when given two boundary conditions $u(0,t)=u(L,t)=0$  and when given $\frac{\partial u(0,t)}{\partial x}=\frac{\partial u(L,t)}{\partial x}=0$, for $0<x<L$. My problem starts when asked to solve for a mix of the two: $u(L,t)=\frac{\partial u(0,t)}{\partial x}=0$, The method described in the book details using separation of variables to define $u=X(x)T(t)$ to arrive at $X''-\lambda X= 0 $ Setting $\lambda=-k^2$ gives: $X(x)=Asin(kx) + Bcos(kx)$ But somehow applying the boundary conditions gives: $X(x)=\cos(\frac{(2n+1)\pi x}{2L})$ Would someone mind explaining how this is done? I get $A=0$ from the insulating boundary condition and $B \cos(kL)=0 => k=\frac {n \pi}{L}$ from the other BC. Not sure where the $(2n+1)$ is coming from...","I'm studying heat equations, I've more or less understood how to solve them when given two boundary conditions $u(0,t)=u(L,t)=0$  and when given $\frac{\partial u(0,t)}{\partial x}=\frac{\partial u(L,t)}{\partial x}=0$, for $0<x<L$. My problem starts when asked to solve for a mix of the two: $u(L,t)=\frac{\partial u(0,t)}{\partial x}=0$, The method described in the book details using separation of variables to define $u=X(x)T(t)$ to arrive at $X''-\lambda X= 0 $ Setting $\lambda=-k^2$ gives: $X(x)=Asin(kx) + Bcos(kx)$ But somehow applying the boundary conditions gives: $X(x)=\cos(\frac{(2n+1)\pi x}{2L})$ Would someone mind explaining how this is done? I get $A=0$ from the insulating boundary condition and $B \cos(kL)=0 => k=\frac {n \pi}{L}$ from the other BC. Not sure where the $(2n+1)$ is coming from...",,"['ordinary-differential-equations', 'partial-differential-equations', 'heat-equation']"
11,Find the extended form of the group generated by an operator?,Find the extended form of the group generated by an operator?,,"I tried to find the extended form of the group generated by the following operators. (I): The first operator $$A=z\frac{\partial }{\partial z}+1$$   To find the extended form of the group generated by $A$, we must solve the following equations    $$\frac{\partial z(a')}{\partial a'}=z(a')$$ and $$\frac{\partial v(a')}{\partial a'}=v(a')$$ where $z(0)=z$ and $v(0)=1$ Now from the first equation we get $$\int \frac{\partial z(a')}{\partial a'} \frac {1}{z(a')}da'= \int d a' $$ $$ \log z(a') =  a' +k $$  we put $a'=0$ to find the constant $k$  $$ \log z =  k $$  Then $$ \log z(a') =  a' + \log z $$  $$ z(a') = z e^{a'}$$ On the other hand from the second equation we get $$\int \frac{\partial v(a')}{\partial a'} \frac {1}{v(a')}da'= \int d a' $$ $$ \log v(a') =  a' +k $$  we put $a'=0$ to find the constant $k$  $$  k=0 $$  Then $$ \log v(a') =  a'  $$  $$ v(a') = e^{a'}$$ Thus $$ e^{a' A} f(x,y,z)=e^{a'}f(x,y,ze^{a'}) $$ (II): The second operator $$B=\frac{(x^2 - ay)}{yz}\frac{\partial }{\partial x}-\frac{x}{y}\frac{\partial }{\partial z}$$ To find the extended form of the group generated by $B$, we must solve the following equations $$\frac{\partial x(b')}{\partial b'}=\frac{(x^2(b')-ay)}{yz(b')}$$ and $$\frac{\partial z(b')}{\partial b'}=- \frac{x(b')}{y}$$ where $z(0)=z$ and $x(0)=x$ This equations need to be solved, but I can't do anything after this step. I have the final answer but i want to understand how it result This is the final answer  $$ e^{b' B} f(x,y,z)=f(\frac{(xz-ab')\sqrt{ay}}{\sqrt{(xz-ab')^2 - z^2 (x^2 -ay) }},y,\frac{1}{\sqrt{ay}}\sqrt{(xz-ab')^2 - z^2 (x^2 -ay) }) $$ Any  input would be helpful for me .","I tried to find the extended form of the group generated by the following operators. (I): The first operator $$A=z\frac{\partial }{\partial z}+1$$   To find the extended form of the group generated by $A$, we must solve the following equations    $$\frac{\partial z(a')}{\partial a'}=z(a')$$ and $$\frac{\partial v(a')}{\partial a'}=v(a')$$ where $z(0)=z$ and $v(0)=1$ Now from the first equation we get $$\int \frac{\partial z(a')}{\partial a'} \frac {1}{z(a')}da'= \int d a' $$ $$ \log z(a') =  a' +k $$  we put $a'=0$ to find the constant $k$  $$ \log z =  k $$  Then $$ \log z(a') =  a' + \log z $$  $$ z(a') = z e^{a'}$$ On the other hand from the second equation we get $$\int \frac{\partial v(a')}{\partial a'} \frac {1}{v(a')}da'= \int d a' $$ $$ \log v(a') =  a' +k $$  we put $a'=0$ to find the constant $k$  $$  k=0 $$  Then $$ \log v(a') =  a'  $$  $$ v(a') = e^{a'}$$ Thus $$ e^{a' A} f(x,y,z)=e^{a'}f(x,y,ze^{a'}) $$ (II): The second operator $$B=\frac{(x^2 - ay)}{yz}\frac{\partial }{\partial x}-\frac{x}{y}\frac{\partial }{\partial z}$$ To find the extended form of the group generated by $B$, we must solve the following equations $$\frac{\partial x(b')}{\partial b'}=\frac{(x^2(b')-ay)}{yz(b')}$$ and $$\frac{\partial z(b')}{\partial b'}=- \frac{x(b')}{y}$$ where $z(0)=z$ and $x(0)=x$ This equations need to be solved, but I can't do anything after this step. I have the final answer but i want to understand how it result This is the final answer  $$ e^{b' B} f(x,y,z)=f(\frac{(xz-ab')\sqrt{ay}}{\sqrt{(xz-ab')^2 - z^2 (x^2 -ay) }},y,\frac{1}{\sqrt{ay}}\sqrt{(xz-ab')^2 - z^2 (x^2 -ay) }) $$ Any  input would be helpful for me .",,"['ordinary-differential-equations', 'lie-groups', 'special-functions', 'lie-algebras', 'differential-operators']"
12,"Is there a name for the ""with-respect-to variable"" of a partial derivative?","Is there a name for the ""with-respect-to variable"" of a partial derivative?",,"In a fraction $\dfrac{a}{b}$, a is called the numerator or dividend and b is called the denominator or divisor. These names are helpful when discussing an equation. In a derivative or partial derivative $\dfrac{\partial y}{\partial x}$, is there a common name for the variable $x$? (i.e., ""the variable that the derivative is computed with-respect-to"")","In a fraction $\dfrac{a}{b}$, a is called the numerator or dividend and b is called the denominator or divisor. These names are helpful when discussing an equation. In a derivative or partial derivative $\dfrac{\partial y}{\partial x}$, is there a common name for the variable $x$? (i.e., ""the variable that the derivative is computed with-respect-to"")",,"['ordinary-differential-equations', 'terminology']"
13,Continuous dependence of maximal interval on right hand side for ode,Continuous dependence of maximal interval on right hand side for ode,,"Let $f,f_n\colon\mathbb{R}\times(0,1)\to\mathbb{R}$ be local Lipschitz continuous functions and assume they form the right-hand-side of some ordinary differential equation's $\dot{x} = f(t,x)$ and $\dot{x_n} = f_n(t,x_n)$ with initial values $(t_n^0,x_n^0)$ and $(t^0,x^0)$ respectively. Moreover, let us assume that $f_n$ converges to $f$ (in some sense) and $(t_n^0,x_n^0)\to(t^0,x^0)$. From standard theory on ordinary differential equations we know that for each ode, we find a maximal time interval $(t_n^\text{min},t_n^\text{max})$ and $(t^\text{min},t^\text{max})$, respectively, where our solutions uniquely exists. My question is, when do we have $t_n^\text{max}\to t^\text{max}$? Concrete, I need the result for $f,f_n\colon\mathbb{R}_+\times(0,1)\to\mathbb{R}_+$ with $f_n\to f$ uniformly on all compact subsets of $\mathbb{R}_+\times(0,1)$. Intuitively, I think this should work as the solutions stay close to each other the whole time. Do you maybe have some literature for such questions?","Let $f,f_n\colon\mathbb{R}\times(0,1)\to\mathbb{R}$ be local Lipschitz continuous functions and assume they form the right-hand-side of some ordinary differential equation's $\dot{x} = f(t,x)$ and $\dot{x_n} = f_n(t,x_n)$ with initial values $(t_n^0,x_n^0)$ and $(t^0,x^0)$ respectively. Moreover, let us assume that $f_n$ converges to $f$ (in some sense) and $(t_n^0,x_n^0)\to(t^0,x^0)$. From standard theory on ordinary differential equations we know that for each ode, we find a maximal time interval $(t_n^\text{min},t_n^\text{max})$ and $(t^\text{min},t^\text{max})$, respectively, where our solutions uniquely exists. My question is, when do we have $t_n^\text{max}\to t^\text{max}$? Concrete, I need the result for $f,f_n\colon\mathbb{R}_+\times(0,1)\to\mathbb{R}_+$ with $f_n\to f$ uniformly on all compact subsets of $\mathbb{R}_+\times(0,1)$. Intuitively, I think this should work as the solutions stay close to each other the whole time. Do you maybe have some literature for such questions?",,"['ordinary-differential-equations', 'stability-in-odes']"
14,"Differential equation Laguerre $xy''+(1-x)y'+ay=0, a \in \mathbb{R}$",Differential equation Laguerre,"xy''+(1-x)y'+ay=0, a \in \mathbb{R}","The differential equation Laguerre $xy''+(1-x)y'+ay=0, a \in \mathbb{R}$ is given. Show that the equation has $0$ as its singular regular point . Find a solution of the differential equation of the form $x^m \sum_{n=0}^{\infty} a_n x^n (x>0) (m \in \mathbb{R})$ Show that if $a=n$, where $n \in \mathbb{N}$ then there is a polynomial solution of degree $n$. Let $L_n$ the polynomial $L_n(x)=e^x \frac{d^n}{dx^n} (x^n \cdot e^{-x})$ (show that it is a polynomial), $n=1,2,3, \dots$. Show that $L_n$ satisfies the equation Laguerre if $a=n(n=1,2, \dots)$. That's what I have tried: For $x \neq 0$ the differential equation can be written as: $y''+ \frac{1-x}{x}y'+ \frac{a}{x}y=0$. $p(x)= \frac{1-x}{x}, q(x)= \frac{a}{x}$ The functions $x \cdot p(x)= 1-x, \ x^2q(x)=ax$ can be written as power series in a region of $0$. Thus, $0$ is a singular regular point. We suppose that there is a solution of the form $y(x)=x^m \sum_{n=0}^{\infty} a_n x^n= \sum_{n=0}^{\infty} a_n x^{n+m}$. Then $y'(x)= \sum_{n=0}^{\infty} a_n(n+m) x^{n+m-1} \Rightarrow -xy'(x)= \sum_{n=0}^{\infty} -a_n(n+m) x^{n+m}$ and $y''(x)= \sum_{n=0}^{\infty} a_n(n+m)(n+m-1) x^{n+m-2} \Rightarrow xy''(x)= \sum_{n=0}^{\infty} a_n(n+m)(n+m-1) x^{n+m-1}$ So we have  $$\sum_{n=0}^{\infty} a_n (n+m)(n+m-1) x^{n+m-1}+ \sum_{n=0}^{\infty} a_n (n+m) x^{n+m-1} + \sum_{n=1}^{\infty} -a_{n-1} (n+m-1) x^{n+m-1}+ \sum_{n=1}^{\infty} a a_{n-1} x^{n+m-1}=0 \\ \Rightarrow a_0 m (m-1) x^{m-1}+ a_0 m x^{m-1}+ \sum_{n=1}^{\infty} \left[ a_n (n+m) (n+m-1)+ a_n(n+m)-a_{n-1}(n+m-1)+ a a_{n-1} \right] x^{n+m-1}=0$$ It has to hold: $$a_0 m^2=0 \overset{a_0 \neq 0}{ \Rightarrow } m=0$$ $$a_n (n+m) (n+m-1)+ a_n (n+m)- a_{n-1} (n+m-1)+ a a_{n-1}=0$$ For $m=0$: $a_{n} n (n-1)+ a_n n-a_{n-1} (n-1)+ a a_{n-1}=0 \Rightarrow a_n n^2+ a_{n-1} (a-n+1)=0 \Rightarrow a_n=- \frac{a_{n-1}(a-n+1)}{n^2}$ For $n=1: \ a_1=-aa_0$ For $n=2: \ a_2= \frac{aa_0(a-1)}{2^2} $ For $n=3: \ a_3=- \frac{aa_0(a-1) (a-2)}{2^2 3^2} $ For $n=4: \ a_4= \frac{aa_0(a-1)(a-2)(a-3)}{2^2 3^2 4^2} $ For $n=5: \ a_5= -\frac{aa_0(a-1)(a-2)(a-3)(a-4)}{2^2 3^2 4^2 5^2} $ We see that $a_n=(-1)^n a_0 \frac{\prod_{i=0}^{n-1} (a-i)}{\prod_{i=2}^n i^2}$ $$\frac{a_{n+1}}{a_n}=(-1) \frac{a-n}{(n+1)^2} \to 0$$ So the series $\sum_{n=0}^{\infty} a_n x^n$ converges and its radius of convergence is equal to $+\infty$. Is it right so far? Could you give me a hint what we could do in order to answer the other two questions? EDIT : Do we differentiate the Leguerre polynomial as follows? $$$$ $$\frac{d}{dx} L_n(x)=e^x \frac{d^n}{dx^n} (x^n e^{-x})+\frac{d^{n+1}}{dx^{n+1}}(x^n e^{-x})$$ $$\frac{d^2}{dx^2} L_n(x)=e^x \frac{d^n}{dx^n} (x^n e^{-x})+ e^{x} \frac{d^{n+1}}{dx^{n+1}}(x^n e^{-x})+\frac{d^{n+2}}{dx^{n+2}}(x^n e^{-x})$$ EDIT : Substituting the above, we cannot show that $L_n$ satisfies the equation Laguerre if $a=n$. Do you maybe know how else we could show this?","The differential equation Laguerre $xy''+(1-x)y'+ay=0, a \in \mathbb{R}$ is given. Show that the equation has $0$ as its singular regular point . Find a solution of the differential equation of the form $x^m \sum_{n=0}^{\infty} a_n x^n (x>0) (m \in \mathbb{R})$ Show that if $a=n$, where $n \in \mathbb{N}$ then there is a polynomial solution of degree $n$. Let $L_n$ the polynomial $L_n(x)=e^x \frac{d^n}{dx^n} (x^n \cdot e^{-x})$ (show that it is a polynomial), $n=1,2,3, \dots$. Show that $L_n$ satisfies the equation Laguerre if $a=n(n=1,2, \dots)$. That's what I have tried: For $x \neq 0$ the differential equation can be written as: $y''+ \frac{1-x}{x}y'+ \frac{a}{x}y=0$. $p(x)= \frac{1-x}{x}, q(x)= \frac{a}{x}$ The functions $x \cdot p(x)= 1-x, \ x^2q(x)=ax$ can be written as power series in a region of $0$. Thus, $0$ is a singular regular point. We suppose that there is a solution of the form $y(x)=x^m \sum_{n=0}^{\infty} a_n x^n= \sum_{n=0}^{\infty} a_n x^{n+m}$. Then $y'(x)= \sum_{n=0}^{\infty} a_n(n+m) x^{n+m-1} \Rightarrow -xy'(x)= \sum_{n=0}^{\infty} -a_n(n+m) x^{n+m}$ and $y''(x)= \sum_{n=0}^{\infty} a_n(n+m)(n+m-1) x^{n+m-2} \Rightarrow xy''(x)= \sum_{n=0}^{\infty} a_n(n+m)(n+m-1) x^{n+m-1}$ So we have  $$\sum_{n=0}^{\infty} a_n (n+m)(n+m-1) x^{n+m-1}+ \sum_{n=0}^{\infty} a_n (n+m) x^{n+m-1} + \sum_{n=1}^{\infty} -a_{n-1} (n+m-1) x^{n+m-1}+ \sum_{n=1}^{\infty} a a_{n-1} x^{n+m-1}=0 \\ \Rightarrow a_0 m (m-1) x^{m-1}+ a_0 m x^{m-1}+ \sum_{n=1}^{\infty} \left[ a_n (n+m) (n+m-1)+ a_n(n+m)-a_{n-1}(n+m-1)+ a a_{n-1} \right] x^{n+m-1}=0$$ It has to hold: $$a_0 m^2=0 \overset{a_0 \neq 0}{ \Rightarrow } m=0$$ $$a_n (n+m) (n+m-1)+ a_n (n+m)- a_{n-1} (n+m-1)+ a a_{n-1}=0$$ For $m=0$: $a_{n} n (n-1)+ a_n n-a_{n-1} (n-1)+ a a_{n-1}=0 \Rightarrow a_n n^2+ a_{n-1} (a-n+1)=0 \Rightarrow a_n=- \frac{a_{n-1}(a-n+1)}{n^2}$ For $n=1: \ a_1=-aa_0$ For $n=2: \ a_2= \frac{aa_0(a-1)}{2^2} $ For $n=3: \ a_3=- \frac{aa_0(a-1) (a-2)}{2^2 3^2} $ For $n=4: \ a_4= \frac{aa_0(a-1)(a-2)(a-3)}{2^2 3^2 4^2} $ For $n=5: \ a_5= -\frac{aa_0(a-1)(a-2)(a-3)(a-4)}{2^2 3^2 4^2 5^2} $ We see that $a_n=(-1)^n a_0 \frac{\prod_{i=0}^{n-1} (a-i)}{\prod_{i=2}^n i^2}$ $$\frac{a_{n+1}}{a_n}=(-1) \frac{a-n}{(n+1)^2} \to 0$$ So the series $\sum_{n=0}^{\infty} a_n x^n$ converges and its radius of convergence is equal to $+\infty$. Is it right so far? Could you give me a hint what we could do in order to answer the other two questions? EDIT : Do we differentiate the Leguerre polynomial as follows? $$$$ $$\frac{d}{dx} L_n(x)=e^x \frac{d^n}{dx^n} (x^n e^{-x})+\frac{d^{n+1}}{dx^{n+1}}(x^n e^{-x})$$ $$\frac{d^2}{dx^2} L_n(x)=e^x \frac{d^n}{dx^n} (x^n e^{-x})+ e^{x} \frac{d^{n+1}}{dx^{n+1}}(x^n e^{-x})+\frac{d^{n+2}}{dx^{n+2}}(x^n e^{-x})$$ EDIT : Substituting the above, we cannot show that $L_n$ satisfies the equation Laguerre if $a=n$. Do you maybe know how else we could show this?",,['ordinary-differential-equations']
15,Notational issues on differential equations,Notational issues on differential equations,,"I am studying dynamical systems and I have some trouble in understanding the notation used for differential equations. For example when I read $$\overset{..}{x}=F(x),$$ how should I interpret it? Is it asking to find a function $x$ (defined on an open set) such that $x''(t)=F(x(t))$ for all $t$? If so, why don't they write $x''=F\circ x$, or $\overset{..}x =F\circ x$? Then I see: Let's call $y$ the velocity, i.e., the function $\overset.x$. Suppose that exists a function $V$ such that $F=\dfrac{-dV}{dx}$. Then the quantity $E(x,y)=\frac12y^2+V(x)$ is constant when $t$ changes. Does it mean that the function $t\mapsto E(x(t),y(t))=\frac12y(t)^2+V(x(t))$ is constant? What does $\frac{-dV}{dx}$ mean?  Simply $V'$?  In the definition of derivative of a function I have never seen something like ""derivative with respect to x"" or ""derivative with respect to t"". My guess is that when they write $\frac{dV}{dx}$ they mean the derivative of the function that maps every point to its potential (in this case simply $V'$); and when they write $\frac{dV}{dt}$ they mean the derivative of the function that maps the time to the potential at that time (which in this case is the derivative of another function, the function $V\circ x$. So it is $(V\circ x)'=(V'\circ x)\cdot x'$).  Is it all right?","I am studying dynamical systems and I have some trouble in understanding the notation used for differential equations. For example when I read $$\overset{..}{x}=F(x),$$ how should I interpret it? Is it asking to find a function $x$ (defined on an open set) such that $x''(t)=F(x(t))$ for all $t$? If so, why don't they write $x''=F\circ x$, or $\overset{..}x =F\circ x$? Then I see: Let's call $y$ the velocity, i.e., the function $\overset.x$. Suppose that exists a function $V$ such that $F=\dfrac{-dV}{dx}$. Then the quantity $E(x,y)=\frac12y^2+V(x)$ is constant when $t$ changes. Does it mean that the function $t\mapsto E(x(t),y(t))=\frac12y(t)^2+V(x(t))$ is constant? What does $\frac{-dV}{dx}$ mean?  Simply $V'$?  In the definition of derivative of a function I have never seen something like ""derivative with respect to x"" or ""derivative with respect to t"". My guess is that when they write $\frac{dV}{dx}$ they mean the derivative of the function that maps every point to its potential (in this case simply $V'$); and when they write $\frac{dV}{dt}$ they mean the derivative of the function that maps the time to the potential at that time (which in this case is the derivative of another function, the function $V\circ x$. So it is $(V\circ x)'=(V'\circ x)\cdot x'$).  Is it all right?",,"['ordinary-differential-equations', 'notation', 'dynamical-systems']"
16,Assistance Solving A Second Order Nonlinear ODE (Converted into a First Order),Assistance Solving A Second Order Nonlinear ODE (Converted into a First Order),,"I am trying to find the solution to $y''=y+y^2$ I noticed that if I multiplied by $y'$ on both sides and integrated, the result would be $\frac{1}{2}(y')^2=\frac{1}{2}y^2+\frac{1}{3}y^3+c$ I have almost no experience with nonlinear ODEs (they have this as a sort of bonus problem in the book I'm reading on linear ODEs) so I am not sure how to progress from here (I'm satisfied with solution where c=0, but would also like to see it where c is left there if possible)","I am trying to find the solution to $y''=y+y^2$ I noticed that if I multiplied by $y'$ on both sides and integrated, the result would be $\frac{1}{2}(y')^2=\frac{1}{2}y^2+\frac{1}{3}y^3+c$ I have almost no experience with nonlinear ODEs (they have this as a sort of bonus problem in the book I'm reading on linear ODEs) so I am not sure how to progress from here (I'm satisfied with solution where c=0, but would also like to see it where c is left there if possible)",,['ordinary-differential-equations']
17,Non-linear first order differential equation,Non-linear first order differential equation,,"I've found this particular equation rather tough, can you give me some hints on how to solve  $$\dot{y}+t\cos\frac{\pi y}{2}+1-t=y$$ Thanks a lot.","I've found this particular equation rather tough, can you give me some hints on how to solve  $$\dot{y}+t\cos\frac{\pi y}{2}+1-t=y$$ Thanks a lot.",,['ordinary-differential-equations']
18,Solving differential equation from Cauchy problem,Solving differential equation from Cauchy problem,,"I am getting acquainted with the Cauchy equations and I am trying to solve an exercise, taking the examples from my class notes. The exercise is: $$\begin{cases} y'=xy+x\\y(1)=2 \end{cases}$$ I have attempted to solve it in this way: $$\frac{\partial x}{\partial y} = xy+x \Rightarrow \frac{\partial y}{y} = 2x\partial x$$ $$\partial y \cdot \frac{1}{y} = 2x\partial x $$ so $$\int{\partial y} \int{\frac{1}{y} \partial y} = 2\int{x\partial x}$$ and that leads to $$y \cdot \ln y +c = 2 \cdot \frac{1}{2}x^2+c \Rightarrow y\ln y +c = x^2+c$$ and now I am a bit stuck because I don't know how actually proceed in order to solve the equation. Have I done averything correctly up to this point? Any help is greatly welcome.","I am getting acquainted with the Cauchy equations and I am trying to solve an exercise, taking the examples from my class notes. The exercise is: $$\begin{cases} y'=xy+x\\y(1)=2 \end{cases}$$ I have attempted to solve it in this way: $$\frac{\partial x}{\partial y} = xy+x \Rightarrow \frac{\partial y}{y} = 2x\partial x$$ $$\partial y \cdot \frac{1}{y} = 2x\partial x $$ so $$\int{\partial y} \int{\frac{1}{y} \partial y} = 2\int{x\partial x}$$ and that leads to $$y \cdot \ln y +c = 2 \cdot \frac{1}{2}x^2+c \Rightarrow y\ln y +c = x^2+c$$ and now I am a bit stuck because I don't know how actually proceed in order to solve the equation. Have I done averything correctly up to this point? Any help is greatly welcome.",,['ordinary-differential-equations']
19,Runge-Kutta 4 - solving system of 6 differential equations (BVP),Runge-Kutta 4 - solving system of 6 differential equations (BVP),,"I'm facing a tricky problem. I need to solve a system of 6 differential equations numerically, but I don't have 6 IVP (initial value problem) conditions, instead I have 6 BVP (boundary valye problem) conditions (3 conditions at $x=0$ and 3 conditions at $x=l$, being $l$ the last point to compute, thus $0<x<l$). So I need to convert the 3 boundary conditions at $x=l$ into 3 at $x=0$ (through the shooting method for example) so I have all 6 IVPs to solve my problem numerically through RK4. So far so good. I have two problems to solve. The first has the following form: \begin{cases}     \frac{dT}{dx}&=f_1(g)  &(1)\\     \frac{dV}{dx}&=f_2(e)   &(2)\\     \frac{dM}{dx}&=f_3(V,g)  &(3)\\     \frac{dK}{dx}&=f_4(x,M,T)  &(4)\\     \frac{dg}{dx}&=f_5(x,T)  &(5)\\     \frac{de}{dx}&=f_6(K)  &(6)\\ \end{cases} With the following BVPs: \begin{cases}     T(0)&=c_1  \\     T(l)&=0  \\     V(0)&=c_2  \\     V(l)&=0  \\     M(0)&=c_3  \\     M(l)&=0  \\ \end{cases} Through the shooting method I need to find $K(0)$, $g(0)$ and $e(0)$.  In this case I can trivially find $g(0)$ because (1) and (5) are linearly dependent. To find $K(0)$ and $e(0)$ I shoot, for example: \begin{cases} K(0)=+1 \text{ and } e(0)=+1 \\ K(0)=+1 \text{ and } e(0)=-1 \\ K(0)=-1 \text{ and } e(0)=-1 \\ K(0)=-1 \text{ and } e(0)=+1 \\ \end{cases} Then I record the final values for $V(l)$ and $M(l)$ for each case, make an approximating polynomial for each ($f_V=f(K,e)$ and $f_M=f(K,e)$ respectively), and solve the resulting system for $K$ and $e$: \begin{cases} 	f_V(K,e)=0 \\ 	f_M(K,e)=0 \end{cases} The resulting $K(0)$ and $e(0)$ give me fairly good values (if I use approximating polynomials of order 1 I have an error of $10^-8$, more than acceptable for what I need). Thus, my first problem is solved. The second problem is the real issue. The system is now: \begin{cases}     \frac{dT}{dx}&=f_1(e,g)\\     \frac{dV}{dx}&=f_2(e,g)  \\     \frac{dM}{dx}&=f_3(V,e,g) \\     \frac{dK}{dx}&=f_4(x,M,T) \\     \frac{dg}{dx}&=f_5(x,T) \\     \frac{de}{dx}&=f_6(K) \\ \end{cases} Also, there are now hyperbolic tangents involved (I have $g(x)$ inside $tanh()$ for example). I can't find any of $K(0)$, $g(0)$ or $e(0)$ trivially because now all equations depend on the others. So, my initial idea was to use the same shooting methodology but creating polynomials with 3 variables ($K$, $g$ and $e$) instead of just $K$ and $e$ like I used before. This isn't producing viable results, even if I use higher orders of polynomials. My question is: what can I do ? I don't mind changing methodologies or solving using other methods, I just need some directions so I can correctly solve this. I wholeheartedly welcome different perspectives on this.  I'm using maxima, for what it's worth. Also, I can determine the correct $K(0)$, $g(0)$ and $e(0)$ (through an external Maple file), and if I input those into my program I get correct results, so I know all the equations are correctly inputted. Thank you ! EDIT2: The system I'm aiming to solve is the following: \begin{cases}     \frac{dT(x)}{dx}&={{3339\,{\it g(x)}\,\tanh \left({{50000\,\sqrt{8427\,{\it g(x)}^2+30036  \,{\it e(x)}^2}}\over{163611}}\right)}\over{\sqrt{8427\,{\it g(x)}^2+  30036\,{\it e(x)}^2}}}\\     \frac{dV(x)}{dx}&={{12600\,{\it e(x)}\,\tanh \left({{50000\,\sqrt{8427\,{\it g(x)}^2+  30036\,{\it e(x)}^2}}\over{163611}}\right)}\over{\sqrt{8427\,{\it g(x)}^  2+30036\,{\it e(x)}^2}}}  \\     \frac{dM(x)}{dx}&={{5\,\sqrt{8427\,{\it g(x)}^2+30036\,{\it e(x)}^2}\,{\it V(x)}-10017\,  {\it g(x)}\,\tanh \left({{50000\,\sqrt{8427\,{\it g(x)}^2+30036\,  {\it e(x)}^2}}\over{163611}}\right)}\over{5\,\sqrt{8427\,{\it g(x)}^2+  30036\,{\it e(x)}^2}}} \\     \frac{dK(x)}{dx}&=-{{60092431565\,x+15000000000\,{\it T(x)}+25000000000\,{\it M(x)}-  2410745228515}\over{48076923076923}} \\     \frac{dg(x)}{dx}&={{3281046763449\,x+1069000000000\,{\it T(x)}-156626689476919}\over{  5250000000000000}} \\     \frac{de(x)}{dx}&={\it K(x)} \\ \end{cases} Boundary conditions are: \begin{cases} 	T(0)=200 \\ 	V(0)=-4.8073945252 \\ 	M(0)=-47.1403817188 \\ 	T(l)=0 \\ 	V(l)=0 \\ 	M(l)=0 \\ \end{cases} Where $l=25mm.$","I'm facing a tricky problem. I need to solve a system of 6 differential equations numerically, but I don't have 6 IVP (initial value problem) conditions, instead I have 6 BVP (boundary valye problem) conditions (3 conditions at $x=0$ and 3 conditions at $x=l$, being $l$ the last point to compute, thus $0<x<l$). So I need to convert the 3 boundary conditions at $x=l$ into 3 at $x=0$ (through the shooting method for example) so I have all 6 IVPs to solve my problem numerically through RK4. So far so good. I have two problems to solve. The first has the following form: \begin{cases}     \frac{dT}{dx}&=f_1(g)  &(1)\\     \frac{dV}{dx}&=f_2(e)   &(2)\\     \frac{dM}{dx}&=f_3(V,g)  &(3)\\     \frac{dK}{dx}&=f_4(x,M,T)  &(4)\\     \frac{dg}{dx}&=f_5(x,T)  &(5)\\     \frac{de}{dx}&=f_6(K)  &(6)\\ \end{cases} With the following BVPs: \begin{cases}     T(0)&=c_1  \\     T(l)&=0  \\     V(0)&=c_2  \\     V(l)&=0  \\     M(0)&=c_3  \\     M(l)&=0  \\ \end{cases} Through the shooting method I need to find $K(0)$, $g(0)$ and $e(0)$.  In this case I can trivially find $g(0)$ because (1) and (5) are linearly dependent. To find $K(0)$ and $e(0)$ I shoot, for example: \begin{cases} K(0)=+1 \text{ and } e(0)=+1 \\ K(0)=+1 \text{ and } e(0)=-1 \\ K(0)=-1 \text{ and } e(0)=-1 \\ K(0)=-1 \text{ and } e(0)=+1 \\ \end{cases} Then I record the final values for $V(l)$ and $M(l)$ for each case, make an approximating polynomial for each ($f_V=f(K,e)$ and $f_M=f(K,e)$ respectively), and solve the resulting system for $K$ and $e$: \begin{cases} 	f_V(K,e)=0 \\ 	f_M(K,e)=0 \end{cases} The resulting $K(0)$ and $e(0)$ give me fairly good values (if I use approximating polynomials of order 1 I have an error of $10^-8$, more than acceptable for what I need). Thus, my first problem is solved. The second problem is the real issue. The system is now: \begin{cases}     \frac{dT}{dx}&=f_1(e,g)\\     \frac{dV}{dx}&=f_2(e,g)  \\     \frac{dM}{dx}&=f_3(V,e,g) \\     \frac{dK}{dx}&=f_4(x,M,T) \\     \frac{dg}{dx}&=f_5(x,T) \\     \frac{de}{dx}&=f_6(K) \\ \end{cases} Also, there are now hyperbolic tangents involved (I have $g(x)$ inside $tanh()$ for example). I can't find any of $K(0)$, $g(0)$ or $e(0)$ trivially because now all equations depend on the others. So, my initial idea was to use the same shooting methodology but creating polynomials with 3 variables ($K$, $g$ and $e$) instead of just $K$ and $e$ like I used before. This isn't producing viable results, even if I use higher orders of polynomials. My question is: what can I do ? I don't mind changing methodologies or solving using other methods, I just need some directions so I can correctly solve this. I wholeheartedly welcome different perspectives on this.  I'm using maxima, for what it's worth. Also, I can determine the correct $K(0)$, $g(0)$ and $e(0)$ (through an external Maple file), and if I input those into my program I get correct results, so I know all the equations are correctly inputted. Thank you ! EDIT2: The system I'm aiming to solve is the following: \begin{cases}     \frac{dT(x)}{dx}&={{3339\,{\it g(x)}\,\tanh \left({{50000\,\sqrt{8427\,{\it g(x)}^2+30036  \,{\it e(x)}^2}}\over{163611}}\right)}\over{\sqrt{8427\,{\it g(x)}^2+  30036\,{\it e(x)}^2}}}\\     \frac{dV(x)}{dx}&={{12600\,{\it e(x)}\,\tanh \left({{50000\,\sqrt{8427\,{\it g(x)}^2+  30036\,{\it e(x)}^2}}\over{163611}}\right)}\over{\sqrt{8427\,{\it g(x)}^  2+30036\,{\it e(x)}^2}}}  \\     \frac{dM(x)}{dx}&={{5\,\sqrt{8427\,{\it g(x)}^2+30036\,{\it e(x)}^2}\,{\it V(x)}-10017\,  {\it g(x)}\,\tanh \left({{50000\,\sqrt{8427\,{\it g(x)}^2+30036\,  {\it e(x)}^2}}\over{163611}}\right)}\over{5\,\sqrt{8427\,{\it g(x)}^2+  30036\,{\it e(x)}^2}}} \\     \frac{dK(x)}{dx}&=-{{60092431565\,x+15000000000\,{\it T(x)}+25000000000\,{\it M(x)}-  2410745228515}\over{48076923076923}} \\     \frac{dg(x)}{dx}&={{3281046763449\,x+1069000000000\,{\it T(x)}-156626689476919}\over{  5250000000000000}} \\     \frac{de(x)}{dx}&={\it K(x)} \\ \end{cases} Boundary conditions are: \begin{cases} 	T(0)=200 \\ 	V(0)=-4.8073945252 \\ 	M(0)=-47.1403817188 \\ 	T(l)=0 \\ 	V(l)=0 \\ 	M(l)=0 \\ \end{cases} Where $l=25mm.$",,"['ordinary-differential-equations', 'polynomials', 'numerical-methods', 'maxima-software']"
20,Calculate Runge-Kutta order 4's order of error experimentally,Calculate Runge-Kutta order 4's order of error experimentally,,"The Problem Use the order 4 Runge-Kutta method to solve the differential equation $ \frac{\partial^2 y}{\partial t^2} = -g + \beta e^{-y/\alpha }*\left | \frac{\partial y}{\partial t} \right |^{2} $ And corroborate that its global error is O (h^4) The Mathematical model I turn the problem into a system of order 1 differential equations: $ \frac{\partial y}{\partial t} = v  $ $ \frac{\partial v}{\partial t} = -g + \beta e^{-y/\alpha }*\left | v \right |^{2} $ Therefore I define the discretization variables u (for position) and v (for speed) as: v = f(v, u, t) u = g(v, t) And use the following increments for the Runge-Kutta method of order 4: For u k1v = h f(vn, un, tn) k2v = h f(vn + 0.5 k1v, un + 0.5 k1u, tn + 0.5 h) k3v = h f(vn + 0.5 k2v, un + 0.5 k2u, tn + 0.5 h) k4v = h f(vn + k3v, un + k3u, tn + h) For v k1u = h f(vn, tn) k2u = h f(vn + 0.5 k1v, tn + 0.5 h) k3u = h f(vn + 0.5 k2v, tn + 0.5 h) k4u = h f(vn + k3v, tn + h) And use them in the RK4 expression for each of them: $ u_{n+1} = u_{n} + \frac{1}{6} (k_{1u} + 2 k_{2u} + 2 k_{3u} + k_{4u}) $ $ v_{n+1} = v_{n} + \frac{1}{6} (k_{1v} + 2 k_{2v} + 2 k_{3v} + k_{4v}) $ NOTE: I first solve for v . To calculate the order of the error, I will solve 120 = h i times with h = 0.1, h = 0.05 and use the result given for h = 0.001 as the ""real"" value, since I don't know the function that solves the ODE. Then I should corroborate that the absolute value of the ""real"" minus the result I got from h = 0.1 must be 16 times bigger than what I get when I substract the value I got from h = 0.05 from the ""real"" value. The program I'm using C++ to solve this. #include <iostream> #include <math.h> #include <cmath> #include <sstream> #include <fstream> #include <vector> #include <cstdlib>  long double rungeKutta(long double h) {     long double alpha = 6629;     long double beta = 0.0047;      long double pos = 39068;     long double speed = 0;      for (int i = 1; h*i < 120; i++)     {         long double k1v = h * (-9.8 + beta * exp(-pos/alpha) * pow(speed, 2));         long double k1y = h * speed;         long double k2v = h * (-9.8 + beta * exp(-(pos + 0.5*k1y)/alpha) * pow(speed + 0.5*k1v, 2));         long double k2y = h * (speed + 0.5*k1v);         long double k3v = h * (-9.8 + beta * exp(-(pos + 0.5*k2y)/alpha) * pow(speed + 0.5*k2v, 2));         long double k3y = h * (speed + 0.5*k2v);         long double k4v = h * (-9.8 + beta * exp(-(pos + k3y)/alpha) * pow(speed  + k3v, 2));         long double k4y = h * (speed + k3v);          speed = speed + (k1v + 2.0*(k2v + k3v) + k4v)/6;         pos = pos + (k1y + 2.0*(k2y + k3y) + k4y)/6;     }      return pos; }  int _tmain(int argc, _TCHAR* argv[]) {         long double errorOne = rungeKutta(0.01);     long double errorTwo = rungeKutta(0.005);     long double real = rungeKutta(0.0001);      cout << fabs(real-errorOne) << endl << fabs(real - errorTwo) << endl;     system(""pause"");     return 0; } The results I find that the error is only reduced by HALF and not to the 1/16th of the first result. What am I doing wrong?? I've run out of ideas. Thanks.","The Problem Use the order 4 Runge-Kutta method to solve the differential equation $ \frac{\partial^2 y}{\partial t^2} = -g + \beta e^{-y/\alpha }*\left | \frac{\partial y}{\partial t} \right |^{2} $ And corroborate that its global error is O (h^4) The Mathematical model I turn the problem into a system of order 1 differential equations: $ \frac{\partial y}{\partial t} = v  $ $ \frac{\partial v}{\partial t} = -g + \beta e^{-y/\alpha }*\left | v \right |^{2} $ Therefore I define the discretization variables u (for position) and v (for speed) as: v = f(v, u, t) u = g(v, t) And use the following increments for the Runge-Kutta method of order 4: For u k1v = h f(vn, un, tn) k2v = h f(vn + 0.5 k1v, un + 0.5 k1u, tn + 0.5 h) k3v = h f(vn + 0.5 k2v, un + 0.5 k2u, tn + 0.5 h) k4v = h f(vn + k3v, un + k3u, tn + h) For v k1u = h f(vn, tn) k2u = h f(vn + 0.5 k1v, tn + 0.5 h) k3u = h f(vn + 0.5 k2v, tn + 0.5 h) k4u = h f(vn + k3v, tn + h) And use them in the RK4 expression for each of them: $ u_{n+1} = u_{n} + \frac{1}{6} (k_{1u} + 2 k_{2u} + 2 k_{3u} + k_{4u}) $ $ v_{n+1} = v_{n} + \frac{1}{6} (k_{1v} + 2 k_{2v} + 2 k_{3v} + k_{4v}) $ NOTE: I first solve for v . To calculate the order of the error, I will solve 120 = h i times with h = 0.1, h = 0.05 and use the result given for h = 0.001 as the ""real"" value, since I don't know the function that solves the ODE. Then I should corroborate that the absolute value of the ""real"" minus the result I got from h = 0.1 must be 16 times bigger than what I get when I substract the value I got from h = 0.05 from the ""real"" value. The program I'm using C++ to solve this. #include <iostream> #include <math.h> #include <cmath> #include <sstream> #include <fstream> #include <vector> #include <cstdlib>  long double rungeKutta(long double h) {     long double alpha = 6629;     long double beta = 0.0047;      long double pos = 39068;     long double speed = 0;      for (int i = 1; h*i < 120; i++)     {         long double k1v = h * (-9.8 + beta * exp(-pos/alpha) * pow(speed, 2));         long double k1y = h * speed;         long double k2v = h * (-9.8 + beta * exp(-(pos + 0.5*k1y)/alpha) * pow(speed + 0.5*k1v, 2));         long double k2y = h * (speed + 0.5*k1v);         long double k3v = h * (-9.8 + beta * exp(-(pos + 0.5*k2y)/alpha) * pow(speed + 0.5*k2v, 2));         long double k3y = h * (speed + 0.5*k2v);         long double k4v = h * (-9.8 + beta * exp(-(pos + k3y)/alpha) * pow(speed  + k3v, 2));         long double k4y = h * (speed + k3v);          speed = speed + (k1v + 2.0*(k2v + k3v) + k4v)/6;         pos = pos + (k1y + 2.0*(k2y + k3y) + k4y)/6;     }      return pos; }  int _tmain(int argc, _TCHAR* argv[]) {         long double errorOne = rungeKutta(0.01);     long double errorTwo = rungeKutta(0.005);     long double real = rungeKutta(0.0001);      cout << fabs(real-errorOne) << endl << fabs(real - errorTwo) << endl;     system(""pause"");     return 0; } The results I find that the error is only reduced by HALF and not to the 1/16th of the first result. What am I doing wrong?? I've run out of ideas. Thanks.",,"['ordinary-differential-equations', 'algorithms', 'numerical-methods']"
21,Will solutions of SDE with different initializations intersect at some point?,Will solutions of SDE with different initializations intersect at some point?,,"Inspired by the result for ODE here , which shows that solutions to the same ODE with different initializations do not intersect, I am wondering if similar results also hold for SDE? Consider the SDE $$dX_t=f(X_t,t)dt+g(X_t,t)dB_t,$$ if both $f,g$ are Lipschitz with linear growth, then there must exists a unique strong solution for any independent initialization. If we consider the equation with $X_0=Z$ and $X_0=Y$ , will there exist some $t$ such that $X_t^Y=X_t^Z$ a.s., (or some other way to define 'intersection of the solutions' more properly)? As mentioned, there does not exist such $t$ in ODE. I tried to extend the ODE results here, but it seems one essential part is that for ODE, the uniqueness of solution is ensured on the interval $[-a,a]$ , meaning that you can can extend the solution uniquely to the left-hand direction, so that the result follows. But for SDE, seems the unique results are only for the right-hand direction, and I am not sure whether a similar proof can be obtained. Any existing results can help?","Inspired by the result for ODE here , which shows that solutions to the same ODE with different initializations do not intersect, I am wondering if similar results also hold for SDE? Consider the SDE if both are Lipschitz with linear growth, then there must exists a unique strong solution for any independent initialization. If we consider the equation with and , will there exist some such that a.s., (or some other way to define 'intersection of the solutions' more properly)? As mentioned, there does not exist such in ODE. I tried to extend the ODE results here, but it seems one essential part is that for ODE, the uniqueness of solution is ensured on the interval , meaning that you can can extend the solution uniquely to the left-hand direction, so that the result follows. But for SDE, seems the unique results are only for the right-hand direction, and I am not sure whether a similar proof can be obtained. Any existing results can help?","dX_t=f(X_t,t)dt+g(X_t,t)dB_t, f,g X_0=Z X_0=Y t X_t^Y=X_t^Z t [-a,a]","['ordinary-differential-equations', 'stochastic-processes', 'stochastic-calculus', 'stochastic-differential-equations']"
22,Method of Dominant Balance with high order system,Method of Dominant Balance with high order system,,"This question comes from Bender and Orszag's Asymptotic Methods and Perturbation Theory. I'm practicing applying the method of dominant balance to study behavior as $x\to \infty$ for systems which have an irregular singular point at infinity. My question is about how to find what Bender and Orszag call the leading behavior, after having found the controlling factor, especially in systems with high order derivatives. For example, the system $x^3 d^5y/dx^5 = y$ has an irregular singular point at $\infty.$ We ansatz $y=e^{S(x)}$ , and make the standard assumption that $S'' \ll (S^\prime)^2$ as $x\to \infty$ . It is easy to compute that $y'' = (S''+(S')^2)e^s \sim (S')^2 e^s$ , which then implies that $y''' \sim ((S')^3+2S'S'')e^S$ . However, by assumption, $S'S''\ll (S')^3$ , so $y''' \sim (S')^3 e^S.$ Continuing in this way, we find that $d^5y/dx^5 \sim (S^\prime)^5 e^S$ as $x\to \infty$ . The asymptotic relation for $y$ then reads $x^3 (S')^5 e^S \sim e^S$ . This is easily solved for $S$ : $$S \sim \frac{5 \omega x^{2/5}}{2},$$ where $\omega$ is a $5$ th root of unity. Now that we have the controlling factor $S\sim\frac{5 \omega x^{2/5}}{2}$ , we seek the leading behavior by searching for a $C(x)$ such that $S(x) = \frac{5 \omega x^{2/5}}{2} + C(x)$ and $C(x)\ll \frac{5 \omega x^{2/5}}{2}$ as $x\to \infty$ . This is where I have a question. In computing the aymptotic relation satisfied by $S$ , we were able to simplify the derivatives of $e^S$ by repeatedly making use of the assumption that $S''\ll (S')^2$ . However, now that I want to find $C$ , it seems that I can't do the same. Indeed, if we do, then we would find ourselves with the relation $$(S')^5 \sim x^{-3}$$ $$((\frac{5 \omega x^{2/5}}{2} + C)')^5 \sim x^{-3}$$ which after some simplification leaves us with $$C' \sim 0,$$ which is obviously useless. It therefore seems like what we need to do is actually fully compute $\frac{d^5}{dx^5}\left ( \exp(\frac{5 \omega x^{2/5}}{2} + C(x)) \right )$ and then do dominant balance on the result. This seems like a huge pain, because the derivative will generate a heap of terms. It feels like this can't be how the method works. My question is: What is the ""right"" way of finding an asymptotic relation for $C(x)$ in situations like this? Thank you for your time.","This question comes from Bender and Orszag's Asymptotic Methods and Perturbation Theory. I'm practicing applying the method of dominant balance to study behavior as for systems which have an irregular singular point at infinity. My question is about how to find what Bender and Orszag call the leading behavior, after having found the controlling factor, especially in systems with high order derivatives. For example, the system has an irregular singular point at We ansatz , and make the standard assumption that as . It is easy to compute that , which then implies that . However, by assumption, , so Continuing in this way, we find that as . The asymptotic relation for then reads . This is easily solved for : where is a th root of unity. Now that we have the controlling factor , we seek the leading behavior by searching for a such that and as . This is where I have a question. In computing the aymptotic relation satisfied by , we were able to simplify the derivatives of by repeatedly making use of the assumption that . However, now that I want to find , it seems that I can't do the same. Indeed, if we do, then we would find ourselves with the relation which after some simplification leaves us with which is obviously useless. It therefore seems like what we need to do is actually fully compute and then do dominant balance on the result. This seems like a huge pain, because the derivative will generate a heap of terms. It feels like this can't be how the method works. My question is: What is the ""right"" way of finding an asymptotic relation for in situations like this? Thank you for your time.","x\to \infty x^3 d^5y/dx^5 = y \infty. y=e^{S(x)} S'' \ll (S^\prime)^2 x\to \infty y'' = (S''+(S')^2)e^s \sim (S')^2 e^s y''' \sim ((S')^3+2S'S'')e^S S'S''\ll (S')^3 y''' \sim (S')^3 e^S. d^5y/dx^5 \sim (S^\prime)^5 e^S x\to \infty y x^3 (S')^5 e^S \sim e^S S S \sim \frac{5 \omega x^{2/5}}{2}, \omega 5 S\sim\frac{5 \omega x^{2/5}}{2} C(x) S(x) = \frac{5 \omega x^{2/5}}{2} + C(x) C(x)\ll \frac{5 \omega x^{2/5}}{2} x\to \infty S e^S S''\ll (S')^2 C (S')^5 \sim x^{-3} ((\frac{5 \omega x^{2/5}}{2} + C)')^5 \sim x^{-3} C' \sim 0, \frac{d^5}{dx^5}\left ( \exp(\frac{5 \omega x^{2/5}}{2} + C(x)) \right ) C(x)","['ordinary-differential-equations', 'asymptotics', 'perturbation-theory']"
23,Intuition for expression of most likely trajectory of an SDE,Intuition for expression of most likely trajectory of an SDE,,"Consider a stochastic differential equation evolving on $\mathbb R$ \begin{equation} dx_t = f(x_t)dt + c dw_t ,\quad x_0 = y \in \mathbb R \end{equation} where $f: \mathbb R \to \mathbb R, c \in \mathbb R$ and $w_t$ is an $1$ -dimensional Wiener process/Brownian motion. In [eq. 8.6, 1] Dürr and Bach showed that when $f\in C^2(\mathbb R)$ the most likely trajectory taken by $x_t$ on the time interval $[0,T]$ is the solution to \begin{equation} \ddot z_t = f(z_t) f'(z_t) + \frac {c^2} 2 f''(z_t), \quad z_0 = y, \quad \dot z_T = f(z_T) \end{equation} Note: In [1] the most likely trajectory is defined as the differentiable path starting at $y$ whose surrounding tube of radius $\epsilon$ in the uniform norm has maximal probability under $x_t$ . They show that this is well-defined provided that $\epsilon$ is below some threshold. I am struggling to get an intuition for this result. Intuitively, one can interpret the SDE as repeatedly adding Gaussian noise to the ODE $dx_t = f(x_t)dt$ . (For instance, this is what happens as one integrates the SDE using the Euler-Maruyama scheme). This leads one to think that the most likely path would be given by $\dot z_t = f(z_t), z_0=y$ . (I know that this mental picture is inaccurate since it ignores the contribution of the noise to the derivative of the process and hence its influence on the motion.) Question: Is there some intuition as to why the expression for the most likely path of the process makes sense? Bonus question: How about if instead of considering Brownian noise, we considered the same SDE driven by smooth noise (e.g. Brownian motion convolved with a mollifier), would the most likely path still be very different from $\dot z_t = f(z_t)$ ? I am guessing that, in virtue of the Wong-Zakai theorems, the situation will be analogous to the Brownian case. [1] Dürr, Bach (1978). The Onsager-Machlup function as Lagrangian for the most probable path of a diffusion process. Communications in Mathematical Physics. Addendum: According to the Stratonovich path integral formalism as presented in [2], the action (ie negative log probability) of a path $x= \left\{x_t, t\in [0,T]\right\}$ is (up to an additive constant): \begin{equation} -\log p(x \mid x_0=y ) =\int_0^T \frac 1 {2c^2}(\dot{x}_t-f(x_t))^2 +f'(x_t) d t \end{equation} If $f'$ is a constant then we can see that the most likely path, ie the path of least action, is simply $\dot z_t = f(z_t), z_0=y$ ; this is also what we find by solving the equation of Dürr and Bach in this case. In the non-linear case, the non-constant derivative term $f'$ leads to the more complex expression for the most likely path. In particular, if we apply the Euler-Lagrange equations to the action, we recover the second-order ODE of Dürr and Bach (this is how they obtain the most likely path in their paper). Interestingly, we can see that in the limit of small $c$ , the contribution of $f'$ to the action will be small wrt $(\dot{x}_t-f(x_t))^2$ and so the most likely path will tend to $\dot z_t = f(z_t), z_0=y$ . [2] Seifert (2012). Stochastic thermodynamics, fluctuation theorems and molecular machines. Reports on Progress in Physics.","Consider a stochastic differential equation evolving on where and is an -dimensional Wiener process/Brownian motion. In [eq. 8.6, 1] Dürr and Bach showed that when the most likely trajectory taken by on the time interval is the solution to Note: In [1] the most likely trajectory is defined as the differentiable path starting at whose surrounding tube of radius in the uniform norm has maximal probability under . They show that this is well-defined provided that is below some threshold. I am struggling to get an intuition for this result. Intuitively, one can interpret the SDE as repeatedly adding Gaussian noise to the ODE . (For instance, this is what happens as one integrates the SDE using the Euler-Maruyama scheme). This leads one to think that the most likely path would be given by . (I know that this mental picture is inaccurate since it ignores the contribution of the noise to the derivative of the process and hence its influence on the motion.) Question: Is there some intuition as to why the expression for the most likely path of the process makes sense? Bonus question: How about if instead of considering Brownian noise, we considered the same SDE driven by smooth noise (e.g. Brownian motion convolved with a mollifier), would the most likely path still be very different from ? I am guessing that, in virtue of the Wong-Zakai theorems, the situation will be analogous to the Brownian case. [1] Dürr, Bach (1978). The Onsager-Machlup function as Lagrangian for the most probable path of a diffusion process. Communications in Mathematical Physics. Addendum: According to the Stratonovich path integral formalism as presented in [2], the action (ie negative log probability) of a path is (up to an additive constant): If is a constant then we can see that the most likely path, ie the path of least action, is simply ; this is also what we find by solving the equation of Dürr and Bach in this case. In the non-linear case, the non-constant derivative term leads to the more complex expression for the most likely path. In particular, if we apply the Euler-Lagrange equations to the action, we recover the second-order ODE of Dürr and Bach (this is how they obtain the most likely path in their paper). Interestingly, we can see that in the limit of small , the contribution of to the action will be small wrt and so the most likely path will tend to . [2] Seifert (2012). Stochastic thermodynamics, fluctuation theorems and molecular machines. Reports on Progress in Physics.","\mathbb R \begin{equation}
dx_t = f(x_t)dt + c dw_t ,\quad x_0 = y \in \mathbb R
\end{equation} f: \mathbb R \to \mathbb R, c \in \mathbb R w_t 1 f\in C^2(\mathbb R) x_t [0,T] \begin{equation}
\ddot z_t = f(z_t) f'(z_t) + \frac {c^2} 2 f''(z_t), \quad
z_0 = y, \quad \dot z_T = f(z_T)
\end{equation} y \epsilon x_t \epsilon dx_t = f(x_t)dt \dot z_t = f(z_t), z_0=y \dot z_t = f(z_t) x= \left\{x_t, t\in [0,T]\right\} \begin{equation}
-\log p(x \mid x_0=y ) =\int_0^T \frac 1 {2c^2}(\dot{x}_t-f(x_t))^2 +f'(x_t) d t
\end{equation} f' \dot z_t = f(z_t), z_0=y f' c f' (\dot{x}_t-f(x_t))^2 \dot z_t = f(z_t), z_0=y","['ordinary-differential-equations', 'stochastic-processes', 'soft-question', 'stochastic-differential-equations', 'euler-lagrange-equation']"
24,"The statistical average of a continuous value: $\overline{O} = \int O(x) \rho(x) dx$, but coordinate invariant","The statistical average of a continuous value: , but coordinate invariant",\overline{O} = \int O(x) \rho(x) dx,"I am trying to solve a Lagrange multiplier problem for the following equation $$ L= - \int_{-\infty}^\infty \rho(x) \ln \frac{\rho(x)}{q(x)} dx + \alpha \left( 1- \int_{-\infty}^\infty \rho(x) dx \right) +\beta \left( \overline{O} - \int_{-\infty}^\infty  O(x) \rho(x)  dx \right) \tag{1} $$ for $\frac{\partial L}{\partial \rho(w)} =0$ . Where the first term is the relative entropy, the second term is the constraint that it sums to one, and the last term is the constraint of an observable involving its average value. The problem solves for the probability measure that maximizes the relative entropy of a continuous parametrization x. Eventually, I wish to apply the probability measure to a curved manifold. This is why coordinate invariance is very interesting to me. An interesting property of the relative entropy is that its equations remains invariant with respect to a change in variable. Indeed, $$ \int_{-\infty}^\infty \rho(x)\ln \frac{\rho(x)}{q(x)} dx \to  \int_{-\infty}^\infty \rho(y(x)) \left|\frac{\partial y}{\partial x}\right|\ln \frac{\rho(y(x))\left|\frac{\partial y}{\partial x}\right|}{q(y(x)) \left|\frac{\partial y}{\partial x}\right|} dx  = \int_{-\infty}^\infty \rho(y)\ln \frac{\rho(y)}{q(y)} dy \tag{2} $$ and $$ \int_{-\infty}^\infty \rho(x)dx \to \int_{-\infty}^\infty \rho(y(x)) \left|\frac{\partial y}{\partial x}\right| dx = \int_{-\infty}^\infty \rho(y) dy \tag{3} $$ However, the last term isn't. Indeed: $$ \int_{-\infty}^\infty O(x) \rho (x)  dx \to \int_{-\infty}^\infty O(y(x)) \left|\frac{\partial y}{\partial x}\right|  \rho (y(x))  \left|\frac{\partial y}{\partial x}\right| dx =  \int_{-\infty}^\infty   O(y) \rho (y(x)) \left|\frac{\partial y}{\partial x}\right| dy  \tag{4} $$ How can I modify the integral that contains O so that it is coordinate invariant? What is the expression for the average of a function O(x) over a continuous parametrization x, such that the average value is coordinate invariant? edit: It appears to me that the solution is to consider the observable to be the ratio between two quantities: $$ \overline{O} = \int_{-\infty}^\infty \frac{A(x)}{B(x)} \rho(x) dx $$ where $O(x) := A(x)/B(x)$ . In this case, it is invariant for the same reason that equation (2) is: $$ \int_{-\infty}^\infty \frac{A(x)}{B(x)} \rho(x) dx \to \int_{-\infty}^\infty \frac{A(y(x))  \left| \frac{dy}{dx} \right| }{B(y(x))  \left| \frac{dy}{dx} \right|} \rho(y(x)) \left| \frac{dy}{dx} \right| dx = \int_{-\infty}^\infty \frac{A(y) }{B(y) } \rho(y)  dy $$ Can anyone weight in?","I am trying to solve a Lagrange multiplier problem for the following equation for . Where the first term is the relative entropy, the second term is the constraint that it sums to one, and the last term is the constraint of an observable involving its average value. The problem solves for the probability measure that maximizes the relative entropy of a continuous parametrization x. Eventually, I wish to apply the probability measure to a curved manifold. This is why coordinate invariance is very interesting to me. An interesting property of the relative entropy is that its equations remains invariant with respect to a change in variable. Indeed, and However, the last term isn't. Indeed: How can I modify the integral that contains O so that it is coordinate invariant? What is the expression for the average of a function O(x) over a continuous parametrization x, such that the average value is coordinate invariant? edit: It appears to me that the solution is to consider the observable to be the ratio between two quantities: where . In this case, it is invariant for the same reason that equation (2) is: Can anyone weight in?","
L= - \int_{-\infty}^\infty \rho(x) \ln \frac{\rho(x)}{q(x)} dx + \alpha \left( 1- \int_{-\infty}^\infty \rho(x) dx \right) +\beta \left( \overline{O} - \int_{-\infty}^\infty  O(x) \rho(x)  dx \right) \tag{1}
 \frac{\partial L}{\partial \rho(w)} =0 
\int_{-\infty}^\infty \rho(x)\ln \frac{\rho(x)}{q(x)} dx \to  \int_{-\infty}^\infty \rho(y(x)) \left|\frac{\partial y}{\partial x}\right|\ln \frac{\rho(y(x))\left|\frac{\partial y}{\partial x}\right|}{q(y(x)) \left|\frac{\partial y}{\partial x}\right|} dx  = \int_{-\infty}^\infty \rho(y)\ln \frac{\rho(y)}{q(y)} dy \tag{2}
 
\int_{-\infty}^\infty \rho(x)dx \to \int_{-\infty}^\infty \rho(y(x)) \left|\frac{\partial y}{\partial x}\right| dx = \int_{-\infty}^\infty \rho(y) dy \tag{3}
 
\int_{-\infty}^\infty O(x) \rho (x)  dx \to \int_{-\infty}^\infty O(y(x)) \left|\frac{\partial y}{\partial x}\right|  \rho (y(x))  \left|\frac{\partial y}{\partial x}\right| dx =  \int_{-\infty}^\infty   O(y) \rho (y(x)) \left|\frac{\partial y}{\partial x}\right| dy  \tag{4}
 
\overline{O} = \int_{-\infty}^\infty \frac{A(x)}{B(x)} \rho(x) dx
 O(x) := A(x)/B(x) 
\int_{-\infty}^\infty \frac{A(x)}{B(x)} \rho(x) dx \to \int_{-\infty}^\infty \frac{A(y(x))  \left| \frac{dy}{dx} \right| }{B(y(x))  \left| \frac{dy}{dx} \right|} \rho(y(x)) \left| \frac{dy}{dx} \right| dx = \int_{-\infty}^\infty \frac{A(y) }{B(y) } \rho(y)  dy
","['ordinary-differential-equations', 'statistics', 'lagrange-multiplier', 'entropy']"
25,Nature of ODE $\dot x=x^2-\frac{t^2}{1+t^2}$,Nature of ODE,\dot x=x^2-\frac{t^2}{1+t^2},"Discuss the equation $\dot{x}=x^2-\frac{t^2}{1+t^2}$ . Make a numerical analysis. Show that there is a unique solution which asymptotically approaches the line $x=1$ . Show that all solutions below this solution approach the line $x=$ $-1 .$ Show that all solutions above go to $\infty$ in finite time. This is a problem of Teschl ODE qn 1.30 For the numerical analysis, we have the nullclines are $x_{+}=\frac{t}{\sqrt{\left(1+t^{2}\right)}}$ and $x_{-}=-\frac{t}{\sqrt{\left(1+t^{2}\right)}}$ which are supersolution and subsolution respectively which has given me 3 regions see the picture attached. We can focus on $t\geq 0$ part only. Now I can also see from xpp that the behaviour of the solution goes in the way I am unable to make the proof rigorous. I know the theorem: Let $x_{+}(t), x_{-}(t)$ be super, sub solutions of the differential equation $\dot{x}=f(t, x)$ on $\left[t_0, T\right)$ , respectively. Then for every solution $x(t)$ on $\left[t_0, T\right)$ we have $$ x(t)<x_{+}(t), \quad t \in\left(t_0, T\right), \quad \text { whenever } \quad x\left(t_0\right) \leq x_{+}\left(t_0\right) $$ respectively $$ x_{-}(t)<x(t), \quad t \in\left(t_0, T\right), \quad \text { whenever } \quad x\left(t_0\right) \geq x_{-}\left(t_0\right) $$ but I am unable to find other good super and subsolutions to make it work. Let me know if I need any other theorems. Also, my guess is that the one unique solution starts at an irrational initial point $\alpha$ which is in between $0.544592<\alpha<0.54453$ . I got that while playing with xpp. see this More Information after trying: So I am guessing that $x=-\sqrt{-c+\frac{t^{2}}{1+t^{2}}}$ is a super solution or not, where $c\in (0,1)$ . If it is so then it will help me with part c of the problem. But the equation is getting cubic. If you are also trying in the same direction let me know.","Discuss the equation . Make a numerical analysis. Show that there is a unique solution which asymptotically approaches the line . Show that all solutions below this solution approach the line Show that all solutions above go to in finite time. This is a problem of Teschl ODE qn 1.30 For the numerical analysis, we have the nullclines are and which are supersolution and subsolution respectively which has given me 3 regions see the picture attached. We can focus on part only. Now I can also see from xpp that the behaviour of the solution goes in the way I am unable to make the proof rigorous. I know the theorem: Let be super, sub solutions of the differential equation on , respectively. Then for every solution on we have respectively but I am unable to find other good super and subsolutions to make it work. Let me know if I need any other theorems. Also, my guess is that the one unique solution starts at an irrational initial point which is in between . I got that while playing with xpp. see this More Information after trying: So I am guessing that is a super solution or not, where . If it is so then it will help me with part c of the problem. But the equation is getting cubic. If you are also trying in the same direction let me know.","\dot{x}=x^2-\frac{t^2}{1+t^2} x=1 x= -1 . \infty x_{+}=\frac{t}{\sqrt{\left(1+t^{2}\right)}} x_{-}=-\frac{t}{\sqrt{\left(1+t^{2}\right)}} t\geq 0 x_{+}(t), x_{-}(t) \dot{x}=f(t, x) \left[t_0, T\right) x(t) \left[t_0, T\right) 
x(t)<x_{+}(t), \quad t \in\left(t_0, T\right), \quad \text { whenever } \quad x\left(t_0\right) \leq x_{+}\left(t_0\right)
 
x_{-}(t)<x(t), \quad t \in\left(t_0, T\right), \quad \text { whenever } \quad x\left(t_0\right) \geq x_{-}\left(t_0\right)
 \alpha 0.544592<\alpha<0.54453 x=-\sqrt{-c+\frac{t^{2}}{1+t^{2}}} c\in (0,1)","['ordinary-differential-equations', 'analysis', 'numerical-methods', 'dynamical-systems', 'nonlinear-dynamics']"
26,Does the second order non-linear ODE $yy''=f(x)f(x)''$ have a general solution?,Does the second order non-linear ODE  have a general solution?,yy''=f(x)f(x)'',"I was working on this problem and came across an equation of the form \begin{align} yy''=f(x)f(x)''. \end{align} The equation $yy''=f(x)$ does not have a general solution (that I or WA can find), but this equation has the particular solutions $y_{0,1}=\pm f(x)$ , which gives me a bit of hope. I also tried a solution of the form $y=\sqrt{f(x)^2+g(x)}$ for some $g(x)$ to be determined, but I believe only $g(x)\equiv0$ works. Non-linear equations don't always have general solutions or transformations to simpler equations given particular solutions, but perhaps someone is aware of one for such an equation? Edit I've channeled my inner Claude Leibovici for this idea, taking $x$ to be the dependent variable and $y$ to be the independent variable: \begin{align} y''=-\frac{x''}{(x')^3},\quad\text{and}\quad f''=-\frac{x''}{(x')^3}f'+\frac{1}{(x')^2}f''. \end{align} So then \begin{align} -y\frac{x''}{(x')^3}=-\frac{x''}{(x')^3}ff'+\frac{1}{(x')^2}ff'\quad\longrightarrow\quad x''+\frac{ff''}{y-ff'}x'=0. \end{align} Using the integrating factor \begin{align} \ln(\mathrm E)&=\int\frac{ff''\mathrm dy}{y-ff'},\\ \left(\mathrm Ex'\right)'=0&\quad\longrightarrow\quad x'=\frac{k_1}{\mathrm E},\\ x(y)&=k_1\int\frac{\mathrm dy}{\mathrm E}+k_2. \end{align} This strikes me as a solution at first, but after thinking a little I'm not so certain. Evaluating the integral for $\mathrm E$ requires knowing the first and second derivative of $f$ with respect to $y$ , which I'm not sure is possible without knowing the solution to the original ODE.","I was working on this problem and came across an equation of the form The equation does not have a general solution (that I or WA can find), but this equation has the particular solutions , which gives me a bit of hope. I also tried a solution of the form for some to be determined, but I believe only works. Non-linear equations don't always have general solutions or transformations to simpler equations given particular solutions, but perhaps someone is aware of one for such an equation? Edit I've channeled my inner Claude Leibovici for this idea, taking to be the dependent variable and to be the independent variable: So then Using the integrating factor This strikes me as a solution at first, but after thinking a little I'm not so certain. Evaluating the integral for requires knowing the first and second derivative of with respect to , which I'm not sure is possible without knowing the solution to the original ODE.","\begin{align}
yy''=f(x)f(x)''.
\end{align} yy''=f(x) y_{0,1}=\pm f(x) y=\sqrt{f(x)^2+g(x)} g(x) g(x)\equiv0 x y \begin{align}
y''=-\frac{x''}{(x')^3},\quad\text{and}\quad
f''=-\frac{x''}{(x')^3}f'+\frac{1}{(x')^2}f''.
\end{align} \begin{align}
-y\frac{x''}{(x')^3}=-\frac{x''}{(x')^3}ff'+\frac{1}{(x')^2}ff'\quad\longrightarrow\quad
x''+\frac{ff''}{y-ff'}x'=0.
\end{align} \begin{align}
\ln(\mathrm E)&=\int\frac{ff''\mathrm dy}{y-ff'},\\
\left(\mathrm Ex'\right)'=0&\quad\longrightarrow\quad
x'=\frac{k_1}{\mathrm E},\\
x(y)&=k_1\int\frac{\mathrm dy}{\mathrm E}+k_2.
\end{align} \mathrm E f y",['ordinary-differential-equations']
27,Why does the equation of the circumference of a circle in spherical and hyperbolic space satisfy $C''=-KC$?,Why does the equation of the circumference of a circle in spherical and hyperbolic space satisfy ?,C''=-KC,"In a space of constant curvature $K$ , the function for $C(r)$ where $C$ is the circumference of a circle of radius $r$ satisfies: $C''=−KC$ , with initial conditions $C(0)=0$ and $C'(0)=2\pi$ . (Units check: Gaussian curvature $K$ is units $1/\rm length^2$ , $C$ is units $\rm length$ , and $C''$ is units $1/\rm length$ .) When $K=0$ , as in flat space, this gives us the familiar $C=2\pi r$ that we all know and love. When $K>0$ this gives us $C=2\pi\frac{\sin(r\sqrt K)}{\sqrt K}$ , and when $K<0$ this gives us $C=2\pi\frac{\sinh (r\sqrt{-K})}{\sqrt{-K}}$ . Is there an intuitive reason why $C''=-KC$ holds? In other words, is there an intuitive reason why $C''/C$ is constant in surfaces of constant curvature? (The initial conditions $C(0)=0$ and $C'(0)=2\pi$ are, I think, pretty intuitive.) I think the explanation lies in Jacobi fields, but I forget the details of how those work. We can frame the question in a different way. Given a point $p$ and any function $C$ , we can ""grow"" a manifold around $p$ such that $C$ is the circumference of circles of radius $r$ around $p$ . This gives us a whole family of manifolds, including for example the paraboloid, but the resulting manifold only looks the same at every point when $C''/C$ is constant. (You can make circles grow like $C$ from a given point, but unless $C''/C$ is constant, you'll have trouble making them grow like that from every point.) Why?","In a space of constant curvature , the function for where is the circumference of a circle of radius satisfies: , with initial conditions and . (Units check: Gaussian curvature is units , is units , and is units .) When , as in flat space, this gives us the familiar that we all know and love. When this gives us , and when this gives us . Is there an intuitive reason why holds? In other words, is there an intuitive reason why is constant in surfaces of constant curvature? (The initial conditions and are, I think, pretty intuitive.) I think the explanation lies in Jacobi fields, but I forget the details of how those work. We can frame the question in a different way. Given a point and any function , we can ""grow"" a manifold around such that is the circumference of circles of radius around . This gives us a whole family of manifolds, including for example the paraboloid, but the resulting manifold only looks the same at every point when is constant. (You can make circles grow like from a given point, but unless is constant, you'll have trouble making them grow like that from every point.) Why?",K C(r) C r C''=−KC C(0)=0 C'(0)=2\pi K 1/\rm length^2 C \rm length C'' 1/\rm length K=0 C=2\pi r K>0 C=2\pi\frac{\sin(r\sqrt K)}{\sqrt K} K<0 C=2\pi\frac{\sinh (r\sqrt{-K})}{\sqrt{-K}} C''=-KC C''/C C(0)=0 C'(0)=2\pi p C p C r p C''/C C C''/C,"['ordinary-differential-equations', 'riemannian-geometry', 'circles', 'curvature', 'pi']"
28,"$\Sigma$-equivalence between $(y,z,1)$ and $(y+\mathcal{O}(2),z+\mathcal{O}(2),1)$",-equivalence between  and,"\Sigma (y,z,1) (y+\mathcal{O}(2),z+\mathcal{O}(2),1)","Consider the sets $\mathfrak{X}(\mathbb{R}^3) = \{X: \mathbb{R}^3 \to \mathbb{R}^3; X \mbox{ is smooth}\}$ and $\Sigma = \{0\}\times\mathbb{R}^2$ . Let $X, Y$ be vector fields in $\mathfrak{X}(\mathbb{\mathbb{R}}^3)$ , such that $$X(x,y,z) = (y,z,1) $$ and $$Y(x,y,z) = (y+ \mathcal{O}_1(2),z+\mathcal{O}_2(2),1), $$ where $\mathcal{O}_i(3):\mathbb{R}^3 \to \mathbb{R}$ is a smooth function satisfying $$\exists \ r_i>0, \exists \ K_i>0; \ |\mathcal{O}_i(2)(x,y,z)|\leq K_i\|(x,y,z)\|^2,\ \forall\  (x,y,z) \in B_{r_i}(0,0,0). $$ Definition: Let $Z_1,Z_2$ $\in$ $\mathfrak{X}{(\mathbb{R}^3)}$ , we say that $Z_1,Z_2$ are $\Sigma$ -equivalent at the origin if there exists a homeomorphism $h:U_0\to V_0$ , where $U_0$ and $V_0$ are neighborhoods of the origin such that $h(U_0\cap \Sigma) = V_0 \cap \Sigma$ $h$ maps the orbits of $Z_1$ into orbits of $Z_2$ preserving the orientation of the orbits. My question: Question: Are $X, Y$ (as defined at the beginning of the text) $\Sigma$ -equivalent at the origin? Just for the records, these orbits are in respect of the ODEs $$(\dot{x},\dot{y},\dot{z})= X(x,y,z) $$ and $$(\dot{x},\dot{y},\dot{z})= Y(x,y,z). $$ Moreover, we can assume that $$Y(x,0,z) = (0,z+ \mathcal{O(2)}, 1)\  \text{and} \   Y(x,y,0) = (y+\mathcal{O}(2),0,1).$$ It seems true, however, I don't have many ideas of how construct such $h$ , can anyone help me?","Consider the sets and . Let be vector fields in , such that and where is a smooth function satisfying Definition: Let , we say that are -equivalent at the origin if there exists a homeomorphism , where and are neighborhoods of the origin such that maps the orbits of into orbits of preserving the orientation of the orbits. My question: Question: Are (as defined at the beginning of the text) -equivalent at the origin? Just for the records, these orbits are in respect of the ODEs and Moreover, we can assume that It seems true, however, I don't have many ideas of how construct such , can anyone help me?","\mathfrak{X}(\mathbb{R}^3) = \{X: \mathbb{R}^3 \to \mathbb{R}^3; X \mbox{ is smooth}\} \Sigma = \{0\}\times\mathbb{R}^2 X, Y \mathfrak{X}(\mathbb{\mathbb{R}}^3) X(x,y,z) = (y,z,1)  Y(x,y,z) = (y+ \mathcal{O}_1(2),z+\mathcal{O}_2(2),1),  \mathcal{O}_i(3):\mathbb{R}^3 \to \mathbb{R} \exists \ r_i>0, \exists \ K_i>0; \ |\mathcal{O}_i(2)(x,y,z)|\leq K_i\|(x,y,z)\|^2,\ \forall\  (x,y,z) \in B_{r_i}(0,0,0).  Z_1,Z_2 \in \mathfrak{X}{(\mathbb{R}^3)} Z_1,Z_2 \Sigma h:U_0\to V_0 U_0 V_0 h(U_0\cap \Sigma) = V_0 \cap \Sigma h Z_1 Z_2 X, Y \Sigma (\dot{x},\dot{y},\dot{z})= X(x,y,z)  (\dot{x},\dot{y},\dot{z})= Y(x,y,z).  Y(x,0,z) = (0,z+ \mathcal{O(2)}, 1)\  \text{and} \   Y(x,y,0) = (y+\mathcal{O}(2),0,1). h","['ordinary-differential-equations', 'dynamical-systems']"
29,Non-unique solution of first order PDE,Non-unique solution of first order PDE,,"Question: $$\frac{\partial u}{\partial x} \frac{\partial u}{\partial y}=1 \qquad \qquad u=0 \; \text{ when } \; x+y=1$$ Find all possible solutions and state where each one exists. Attempt: Using the method of characteristics (Charpit's equations), we end up with $$x=s \pm t \;,\; y=1-s \pm t \;,\; \frac{\partial u}{\partial x} = \pm 1 \;,\; \frac{\partial u}{\partial y}= \pm 1 \;,\; u=2t$$ The set of all solutions is then $$u(x,y) = \pm(x+y-1)$$ However, how do you know where these solution does/doesn't ""exist""? There seems to be no problems as far as the characteristic projections are concerned (they are a set of straight lines perpendicular to the initial data), and the Jacobian is $J= \pm 2$ which is non-zero. So is $u$ just supposed to be a multi-valued function in this case?","Question: Find all possible solutions and state where each one exists. Attempt: Using the method of characteristics (Charpit's equations), we end up with The set of all solutions is then However, how do you know where these solution does/doesn't ""exist""? There seems to be no problems as far as the characteristic projections are concerned (they are a set of straight lines perpendicular to the initial data), and the Jacobian is which is non-zero. So is just supposed to be a multi-valued function in this case?","\frac{\partial u}{\partial x} \frac{\partial u}{\partial y}=1 \qquad \qquad u=0 \; \text{ when } \; x+y=1 x=s \pm t \;,\; y=1-s \pm t \;,\; \frac{\partial u}{\partial x} = \pm 1 \;,\; \frac{\partial u}{\partial y}= \pm 1 \;,\; u=2t u(x,y) = \pm(x+y-1) J= \pm 2 u","['ordinary-differential-equations', 'partial-differential-equations', 'cauchy-problem']"
30,Showing that there exists a solution to $y^\prime(t)=y-e^{-y}+e^{-t}$ with $y(0)=0$ and $0\leq t\leq 1$,Showing that there exists a solution to  with  and,y^\prime(t)=y-e^{-y}+e^{-t} y(0)=0 0\leq t\leq 1,"This is Exercise 10 from section 1.10 of Braun's book on differential equations (3rd edition). Show a solution $y(t)$ of the given initial value problem exists on the specified interval: $$y^\prime(t)=y+e^{-y}+e^{-t}$$ $$y(0)=0$$ $$0\leq t\leq 1$$ The function $f(t,y) = y + e^{-y} + e^{-t}$ is continuous and $\frac{\partial f}{\partial y}$ is continuous, both on all of $\mathbb{R}^2$ . We are supposed to find a rectangle $R$ of the form $0 \leq t \leq a$ , $|y| \leq b$ , then set $M = \max\{|f(t,y)| \colon (t,y) \in R\}$ . Then a solution exists on the interval $0 \leq t \leq \alpha$ , where $\alpha = \min(a,b/M)$ . The problem is that $M = -b + e^b + 1$ , attained at the corner $(t,y)=(0,-b)$ of $R$ . So $b/M = \frac{b}{e^b-b+1}$ . We want this to be $\geq 1$ : $\frac{b}{e^b - b + 1} \geq 1$ , or $2b -1 \geq e^b$ . Well, that never happens: $2b-1 < e^b$ for all $b$ .","This is Exercise 10 from section 1.10 of Braun's book on differential equations (3rd edition). Show a solution of the given initial value problem exists on the specified interval: The function is continuous and is continuous, both on all of . We are supposed to find a rectangle of the form , , then set . Then a solution exists on the interval , where . The problem is that , attained at the corner of . So . We want this to be : , or . Well, that never happens: for all .","y(t) y^\prime(t)=y+e^{-y}+e^{-t} y(0)=0 0\leq t\leq 1 f(t,y) = y + e^{-y} + e^{-t} \frac{\partial f}{\partial y} \mathbb{R}^2 R 0 \leq t \leq a |y| \leq b M = \max\{|f(t,y)| \colon (t,y) \in R\} 0 \leq t \leq \alpha \alpha = \min(a,b/M) M = -b + e^b + 1 (t,y)=(0,-b) R b/M = \frac{b}{e^b-b+1} \geq 1 \frac{b}{e^b - b + 1} \geq 1 2b -1 \geq e^b 2b-1 < e^b b","['ordinary-differential-equations', 'initial-value-problems']"
31,Identifying ODE types for solving by hand and when to use computers instead,Identifying ODE types for solving by hand and when to use computers instead,,"So this questions relates to my specific ODE but also ODEs in general. I am a big fan of solving ODEs by hand, but I also know when to give up and use, say, Mathematica to solve it for me. Having said that, a lot of ODEs including nth-order linear ODEs such as an Euler-Cauchy type equation are often solvable by hand and aren't too lengthy. However, ODEs such as my 2nd-order nonlinear ODE $$xf(x)+af'(x) [f'(x)^2+bf''(x)^2)]^{1/2}=0$$ are not easily identified if DEs aren't your speciality. So my question is this, how does one go about deciding if a DE is solvable by hand, i.e. could I solve the above ODE by hand? If it is solvable by hand, how do we ""know""/""decide"" which method to use if it isn't obvious? Lastly, if we resort to using software and it fails to solve it analytically, does that necessarily imply that only a numerical solution exists? Many thanks for all the help and feedback Ken","So this questions relates to my specific ODE but also ODEs in general. I am a big fan of solving ODEs by hand, but I also know when to give up and use, say, Mathematica to solve it for me. Having said that, a lot of ODEs including nth-order linear ODEs such as an Euler-Cauchy type equation are often solvable by hand and aren't too lengthy. However, ODEs such as my 2nd-order nonlinear ODE are not easily identified if DEs aren't your speciality. So my question is this, how does one go about deciding if a DE is solvable by hand, i.e. could I solve the above ODE by hand? If it is solvable by hand, how do we ""know""/""decide"" which method to use if it isn't obvious? Lastly, if we resort to using software and it fails to solve it analytically, does that necessarily imply that only a numerical solution exists? Many thanks for all the help and feedback Ken",xf(x)+af'(x) [f'(x)^2+bf''(x)^2)]^{1/2}=0,"['ordinary-differential-equations', 'computational-mathematics']"
32,How to solve $\mathrm{d}^n y/\mathrm{d}x^n = (\mathrm{d}y/\mathrm{d}x)^n$ or $(\mathrm{e}^D) y = \mathrm{e}^{(Dy)}$ [closed],How to solve  or  [closed],\mathrm{d}^n y/\mathrm{d}x^n = (\mathrm{d}y/\mathrm{d}x)^n (\mathrm{e}^D) y = \mathrm{e}^{(Dy)},"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Is there a general analytical solution for  $$ \frac{\mathrm{d}^ny}{\mathrm{d}x^n} = \left(\frac{\mathrm{d}y}{\mathrm{d}x}\right)^n,\qquad n>2? $$ or equivalently $(\mathrm{e}^D) y = \mathrm{e}^{(Dy)}$ where $\mathrm{D} = \frac{\mathrm{d}}{\mathrm{dx}}$ and $\mathrm{e}^D$ is the exponentiatin operator. For $n=2$, the solution is  $y(x) = c_2 - \log(c_1 + x)$.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Is there a general analytical solution for  $$ \frac{\mathrm{d}^ny}{\mathrm{d}x^n} = \left(\frac{\mathrm{d}y}{\mathrm{d}x}\right)^n,\qquad n>2? $$ or equivalently $(\mathrm{e}^D) y = \mathrm{e}^{(Dy)}$ where $\mathrm{D} = \frac{\mathrm{d}}{\mathrm{dx}}$ and $\mathrm{e}^D$ is the exponentiatin operator. For $n=2$, the solution is  $y(x) = c_2 - \log(c_1 + x)$.",,"['ordinary-differential-equations', 'lie-algebras']"
33,What has more potential: Differential algebraic geometry or Diffiety theory?,What has more potential: Differential algebraic geometry or Diffiety theory?,,"Algebraic geometry is said to be useful to study not only specific solutions of polynomial equations but to understand the intrinsic properties of the totality of solutions of a system of equations. However, varieties (which are the central object of study in algebraic geometry) are generalisations of solution sets of polynomial equations and thus do not seem suitable for investigating (intrinsic properties of solutions of a system of) differential equations. But precisely the investigation of differential equations in such an abstract manner would probably enable great applications in physics and computer science. While looking for approaches that generalise the notions of algebraic geometry to the study of differential equations I basically came across the following two (if someone knows more, please let me know as well): Differential algebraic geometry Diffiety Theory However, the information provided on the wiki-sides are not very helpful. Furthermore, I did not find anything on math.SE about diffieties. I found this short introduction to the idea of a diffiety on the ncat-lab but it does not say much about the applications of the idea and does not contrast it with differential algebraic geometry. My questions are: Does anyone know both approaches well enough to compare their methods and applications? If so, please share this knowledge. If I want to learn one of these approaches, which one should I choose? And what would be good books to start? PS: I found a link to a so-called diffiety institute which provides access to a lot of writings on diffiety theory. And here are some more information on differential algebra.","Algebraic geometry is said to be useful to study not only specific solutions of polynomial equations but to understand the intrinsic properties of the totality of solutions of a system of equations. However, varieties (which are the central object of study in algebraic geometry) are generalisations of solution sets of polynomial equations and thus do not seem suitable for investigating (intrinsic properties of solutions of a system of) differential equations. But precisely the investigation of differential equations in such an abstract manner would probably enable great applications in physics and computer science. While looking for approaches that generalise the notions of algebraic geometry to the study of differential equations I basically came across the following two (if someone knows more, please let me know as well): Differential algebraic geometry Diffiety Theory However, the information provided on the wiki-sides are not very helpful. Furthermore, I did not find anything on math.SE about diffieties. I found this short introduction to the idea of a diffiety on the ncat-lab but it does not say much about the applications of the idea and does not contrast it with differential algebraic geometry. My questions are: Does anyone know both approaches well enough to compare their methods and applications? If so, please share this knowledge. If I want to learn one of these approaches, which one should I choose? And what would be good books to start? PS: I found a link to a so-called diffiety institute which provides access to a lot of writings on diffiety theory. And here are some more information on differential algebra.",,"['ordinary-differential-equations', 'algebraic-geometry', 'reference-request', 'category-theory', 'topos-theory']"
34,book recommendation: differential equations on networks,book recommendation: differential equations on networks,,"I was hoping someone would recommend a good book as an introduction to differential equations on Graphs or networks. So this would include both ODEs and PDEs. After looking around I have not been able to find an comprehensive treatment. In Fan Chung's book on spectral graph theory, there is a chapter on the heat kernel on graphs--but did not find any mention of the wave equation or Schrodinger equation on graphs. Cvetkovic and the Brouwer&Haemers book don't mention PDEs at all. I have not really found any treatments of ODEs on graphs at all. I understand that this type of topic spans a number of disciplines. So for the spectral graph theory connection you need a stronger grasp of differential topology and Riemann geometry. But I was hoping that someone could point out some good starting points on this topic.","I was hoping someone would recommend a good book as an introduction to differential equations on Graphs or networks. So this would include both ODEs and PDEs. After looking around I have not been able to find an comprehensive treatment. In Fan Chung's book on spectral graph theory, there is a chapter on the heat kernel on graphs--but did not find any mention of the wave equation or Schrodinger equation on graphs. Cvetkovic and the Brouwer&Haemers book don't mention PDEs at all. I have not really found any treatments of ODEs on graphs at all. I understand that this type of topic spans a number of disciplines. So for the spectral graph theory connection you need a stronger grasp of differential topology and Riemann geometry. But I was hoping that someone could point out some good starting points on this topic.",,"['ordinary-differential-equations', 'reference-request', 'partial-differential-equations', 'spectral-graph-theory']"
35,Solution of the differential equation $2x^3dy + (1 - y^2)(x^2y^2 + y^2 - 1)dx = 0$.,Solution of the differential equation .,2x^3dy + (1 - y^2)(x^2y^2 + y^2 - 1)dx = 0,Find the solution of the differential equation    $$2x^3dy + (1 - y^2)(x^2y^2 + y^2 - 1)dx = 0$$ My attempt: After arranging the above equation as $$\frac{dy}{dx}=\frac{(1-y^2)(x^2y^2+y^2-1)}{-2x^3}$$ I am not getting any standard method to get to the next step. Any suggestion will be helpful.,Find the solution of the differential equation    $$2x^3dy + (1 - y^2)(x^2y^2 + y^2 - 1)dx = 0$$ My attempt: After arranging the above equation as $$\frac{dy}{dx}=\frac{(1-y^2)(x^2y^2+y^2-1)}{-2x^3}$$ I am not getting any standard method to get to the next step. Any suggestion will be helpful.,,['ordinary-differential-equations']
36,How to solve differential equations of the form $f'(x) = f(x + a)$,How to solve differential equations of the form,f'(x) = f(x + a),"What could one do to find analytic solutions for $f'(x) = f(x + a)$ for various values of $a$? I know that $c_1\sin(x + c_2)$ is solution when $a = \frac{1}{2}\pi$, and of course $c_1e^x$ when $a = 0$. For instance, is there a function satisfying $f'(x) = f(x + 1)$?  What about negative or imaginary $a$? Is there possibly a generalization?","What could one do to find analytic solutions for $f'(x) = f(x + a)$ for various values of $a$? I know that $c_1\sin(x + c_2)$ is solution when $a = \frac{1}{2}\pi$, and of course $c_1e^x$ when $a = 0$. For instance, is there a function satisfying $f'(x) = f(x + 1)$?  What about negative or imaginary $a$? Is there possibly a generalization?",,"['ordinary-differential-equations', 'delay-differential-equations']"
37,Useful reformulation of Goldbach's conjecture?,Useful reformulation of Goldbach's conjecture?,,"Let us assume there exists some infinite order differential equation whose solution is: $$ y= \sum_{n=1}^\infty A_n \exp(p_n^sx)  $$ Where $p_n$ is the $n$'th prime. Substituting $ y=\exp(\lambda x)$ as a trail solution and factorizing. The differential equation must be simplified and factorized to: $$ \prod_{j=1}^\infty (\lambda-p_n^s) = 0$$ $$ \implies \prod_{j=1}^\infty (1-\frac{p_n^s}{\lambda}) = 0$$ Expanding the above equation: $$\implies 1 - (\sum_{i} p_i^s)\frac{1}{\lambda} + \sum_{i < j} (p_i p_j)^s \frac{1}{\lambda^2} - \sum_{i<j<k} (p_i p_j p_k)^s \frac{1}{\lambda^3} + \dots + \lim_{n \to \infty} \sum_{i<j<\dots<n}(-1)^n (p_i p_j \dots p_n)^s \frac{1}{\lambda^n} = 0 $$ Writing more compactly: $$ 1+ \sum_{n=1}^\infty \sum_{x_i<x_j<\dots<x_n} (p_{x_i}p_{x_j} \dots p_{x_n})^s = 0 $$ We conjecture that $\sum_{x_i<x_j<\dots<x_n} (p_{x_i}p_{x_j}\dots p_{x_n})^s $ can be analytically continued as some function of $ P(s)$ where $P(s)$ is the prime zeta function Hence, for $f_1(P(s)) = P(s)$ And $ f_2(P(s)) = P(s)^2 - P(2s)$ And so on ... Where $f_n(P(s))$ is given by comparing the below equation to the equation before the analytic continuation: $$ 1 + \frac{f_1(P(s))}{\lambda} + \frac{f_2(P(s))}{\lambda^2} + \dots = 0 $$ Writing it compactly: $$ 1 + \sum_{i}^\infty \frac{f_i(P(s))}{\lambda^s} =0  $$ Note this can be thought as of the solution of a integral: $$ y + f_1(P(s))\int_{-\infty}^x y(x) dx +  f_2(P(s))\int_{-\infty}^x (\int_{-\infty}^k y(z) dz) dk + \dots$$ (To verify this subsitute $y=Ae^{2x}$) Writing it compactly: $$ y + \sum_{i=1}^\infty f_i(P(s)) \int_{- \infty}^x (\int_{- \infty}^x (\dots( \int_{- \infty}^x y dx )dx)\dots \text{$i$ times})dx) = 0  $$ Strategy from Here My idea was to take some asymptotic limit as $s \nearrow 1$ and rewrite this as a integral equation for $y^2$. Since, $$ y=\sum_{i=1}^\infty A_i \exp{p_i x}  $$ Setting $A_1=0$ and squaring: $$ y^2 = \sum_{i,j}A_{ij}' \exp{(p_i + p_j)x} = \sum_i^\infty B'_i e^{2ix} $$ By Goldbach's conjecture the rightmost side must contain all the even powers. Questions Is this a viable strategy or useful reformulation of Goldbach's conjecture? (Is it easier to show $y^2 =\sum_i^\infty B'_i e^{2ix} $as a solution)? Is the analytic continuation valid?","Let us assume there exists some infinite order differential equation whose solution is: $$ y= \sum_{n=1}^\infty A_n \exp(p_n^sx)  $$ Where $p_n$ is the $n$'th prime. Substituting $ y=\exp(\lambda x)$ as a trail solution and factorizing. The differential equation must be simplified and factorized to: $$ \prod_{j=1}^\infty (\lambda-p_n^s) = 0$$ $$ \implies \prod_{j=1}^\infty (1-\frac{p_n^s}{\lambda}) = 0$$ Expanding the above equation: $$\implies 1 - (\sum_{i} p_i^s)\frac{1}{\lambda} + \sum_{i < j} (p_i p_j)^s \frac{1}{\lambda^2} - \sum_{i<j<k} (p_i p_j p_k)^s \frac{1}{\lambda^3} + \dots + \lim_{n \to \infty} \sum_{i<j<\dots<n}(-1)^n (p_i p_j \dots p_n)^s \frac{1}{\lambda^n} = 0 $$ Writing more compactly: $$ 1+ \sum_{n=1}^\infty \sum_{x_i<x_j<\dots<x_n} (p_{x_i}p_{x_j} \dots p_{x_n})^s = 0 $$ We conjecture that $\sum_{x_i<x_j<\dots<x_n} (p_{x_i}p_{x_j}\dots p_{x_n})^s $ can be analytically continued as some function of $ P(s)$ where $P(s)$ is the prime zeta function Hence, for $f_1(P(s)) = P(s)$ And $ f_2(P(s)) = P(s)^2 - P(2s)$ And so on ... Where $f_n(P(s))$ is given by comparing the below equation to the equation before the analytic continuation: $$ 1 + \frac{f_1(P(s))}{\lambda} + \frac{f_2(P(s))}{\lambda^2} + \dots = 0 $$ Writing it compactly: $$ 1 + \sum_{i}^\infty \frac{f_i(P(s))}{\lambda^s} =0  $$ Note this can be thought as of the solution of a integral: $$ y + f_1(P(s))\int_{-\infty}^x y(x) dx +  f_2(P(s))\int_{-\infty}^x (\int_{-\infty}^k y(z) dz) dk + \dots$$ (To verify this subsitute $y=Ae^{2x}$) Writing it compactly: $$ y + \sum_{i=1}^\infty f_i(P(s)) \int_{- \infty}^x (\int_{- \infty}^x (\dots( \int_{- \infty}^x y dx )dx)\dots \text{$i$ times})dx) = 0  $$ Strategy from Here My idea was to take some asymptotic limit as $s \nearrow 1$ and rewrite this as a integral equation for $y^2$. Since, $$ y=\sum_{i=1}^\infty A_i \exp{p_i x}  $$ Setting $A_1=0$ and squaring: $$ y^2 = \sum_{i,j}A_{ij}' \exp{(p_i + p_j)x} = \sum_i^\infty B'_i e^{2ix} $$ By Goldbach's conjecture the rightmost side must contain all the even powers. Questions Is this a viable strategy or useful reformulation of Goldbach's conjecture? (Is it easier to show $y^2 =\sum_i^\infty B'_i e^{2ix} $as a solution)? Is the analytic continuation valid?",,"['ordinary-differential-equations', 'asymptotics', 'analytic-number-theory', 'integral-equations', 'goldbachs-conjecture']"
38,Solving linear nonhomogenous system of differential equations,Solving linear nonhomogenous system of differential equations,,"I have a question, concerning one special solution of the following system. First I will sum up my results, to give a context for my question. My question will be seperated by a line, if you want to skip the introduction. $\begin{equation} \begin{aligned} \mathrm{II:} \quad\\ \mathrm{III:} \quad\\ \end{aligned} \begin{aligned} \dot{x}_2 &=  a x_0 - b x_2 + cx_3 + g x_0 x_3\\ \dot{x}_3 &= -d y_2 x_3 + b x_2 - cx_3 \\ \end{aligned} \end{equation}$ with the initial conditions $x_0 = x_1(0) \quad \rightarrow \quad x_2(0) = x_3(0) = x_4(0) = 0$ First I solved the homogenous part of the system: \begin{equation} \left(\begin{matrix}\dot{x}_2\\\dot{x}_3\end{matrix}\right) = \underbrace{\left[\begin{matrix}- b & g x_{0} + c\\b & -\left( d y_{2} +c \right)\end{matrix}\right]}_{\substack{\hat{A}}}\left(\begin{matrix}x_2\\x_3\end{matrix}\right) + \underbrace{\left(\begin{matrix}a x_0\\0\end{matrix}\right)}_{\substack{\textbf{b}}} \end{equation} \begin{equation} \begin{aligned} \\ \lambda_1 &= -\frac{1}{2}\left[ b+ c+d y_{2} + \sqrt{\left(b + c + d y_2 \right)^2 - 4 b \left(d y_2 - g x_0 \right)} \right]\\ \\ \lambda_2 &=  -\frac{1}{2}\left[ b+ c+d y_{2} + \sqrt{\left(b + c + d y_2 \right)^2 - 4 b \left(d y_2 - g x_0 \right)} \right] \\ \\ \end{aligned} \begin{aligned} v_1 &= \left(\begin{matrix}- \frac{2\left(c + g x_{0}\right)}{- b + c + d y_{2} +  \sqrt{\left(b + c + d y_2 \right)^2 - 4 b \left(d y_2 - g x_0 \right)}}\\1\end{matrix}\right) \\ v_2 &= \left(\begin{matrix}- \frac{2\left(c + g x_{0}\right)}{- b + c + d y_{2} - \sqrt{\left(b + c + d y_2 \right)^2 - 4 b \left(d y_2 - g x_0 \right)}}\\1\end{matrix}\right)\\ \end{aligned} \label{eq::modell6_eigenwerte_und_vektoren} \end{equation} which is given by  \begin{equation} \textbf{x}_h = c_1 \textbf{v}_1 e^{\lambda_1t} + c_2 \textbf{v}_2 e^{\lambda_2t}  \label{eq::modell5_homogene_loesung} \end{equation} I found the particular solution using the following approach \begin{equation} x_p = \left(\begin{matrix}A_1\\A_2\end{matrix}\right) \quad \dot{\textbf{x}}_p = \hat{A} \textbf{x}_p + \textbf{b} \end{equation} the solution of this equation is \begin{equation} \textbf{x}_p = \frac{ax_0}{\left(d y_2 - g x_0 \right)}\left(\begin{matrix}\frac{d y_2 + c}{b}\\1\end{matrix}\right) \end{equation} Adding homogenous and particular solution and using the initial conditions gave me the following general solution of the system. \begin{equation} \begin{aligned} x_1 &=  x_0\\ x_2 & =   \frac{ax_0}{\left(d y_2 - g x_0 \right)} \left[-\frac{\left(g x_{0} +c\right) \left(b + c + d y_{2} - \sqrt{\dots}\right)}{\sqrt{\dots}\left(- b + c + d y_{2} +  \sqrt{\dots} \right)}  e^{-\frac{1}{2} \left[b + c + d y_{2}+ \sqrt{\dots}\right]t} + \frac{\left(g x_{0} +c\right) \left(b + c + d y_{2} + \sqrt{\dots}\right)}{\sqrt{\dots}\left(- b + c + d y_{2} -  \sqrt{\dots} \right)}  e^{-\frac{1}{2} \left[b + c + d y_{2}- \sqrt{\dots}\right]t} + \frac{d y_2 + c}{b}\right]\\ x_3 &= \frac{ax_0}{\left(d y_2 - g x_0 \right)} \left[\frac{\left(b + c + d y_{2} - \sqrt{\dots}\right)}{2 \sqrt{\dots}}e^{-\frac{1}{2} \left[b + c + d y_{2} + \sqrt{\dots}\right]t}  - \frac{ \left(b + c + d y_{2} + \sqrt{\dots}\right)}{2  \sqrt{\dots}}e^{-\frac{1}{2} \left[b + c + d y_{2} - \sqrt{\dots}\right]t} + 1   \right]\\ x_4 &=\frac{d y_2 a x_0}{\left(d y_{2} - g x_{0}\right)}\left[-\frac{ \left(b + c + d y_{2} - \sqrt{\dots}\right)}{  \sqrt{\dots}\left[b + c + d y_{2} + \sqrt{\dots}\right]}e^{-\frac{1}{2} \left[b + c + d y_{2} + \sqrt{\dots}\right]t}+ \frac{ \left(b + c + d y_{2} + \sqrt{\dots}\right)}{  \sqrt{\dots}\left[b + c + d y_{2} - \sqrt{\dots}\right]}e^{-\frac{1}{2} \left[b + c + d y_{2} - \sqrt{\dots}\right]t} +  t  +\frac{ \left(b + c+  dy_2 \right)}{b\left(d y_{2} - g x_{0}\right)}\right] \end{aligned} \label{eq::modell6_loesung} \end{equation} with \begin{equation} \sqrt{\dots} = \sqrt{\left( b + c + d y_2 \right)^2 - 4b \left(dy_2 - g x_0 \right)} \end{equation} --------------------------------------------- Now I want to analyze the special case $\left(d y_{2} - g x_{0}\right)$. This solution is very interesting because the exponent of the e function, switches from positive to negative at this transition. As I get a zero-division error in my standard solution, I want to solve the system of equations again, with the new set of parameters. \begin{equation} \left(\begin{matrix}\dot{x}_2\\\dot{x}_3\end{matrix}\right) = \underbrace{\left[\begin{matrix}- b & c + d y_{2}\\b & - c - d y_{2}\end{matrix}\right]}_{\substack{\hat{A}}}\left(\begin{matrix}x_2\\x_3\end{matrix}\right) + \underbrace{\left(\begin{matrix}a x_0\\0\end{matrix}\right)}_{\substack{\textbf{b}}} \end{equation} homogenous solution: \begin{equation} \begin{aligned} \\ \lambda_1 &= 0\\ \\ \lambda_2 &=  - \left(b + c + d y_{2}\right)\\ \\ \end{aligned} \begin{aligned} v_1 &= \left(\begin{matrix}\frac{1}{b} \left(c + d y_{2}\right)\\1\end{matrix}\right)\\ v_2 &=  \left(\begin{matrix}-1\\1\end{matrix}\right)\\ \end{aligned} \label{eq::modell6_eigenwerte_und_vektoren_spezialfall_1} \end{equation}  \begin{equation} \textbf{x}_h = c_1 \textbf{v}_1 e^{\lambda_1t} + c_2 \textbf{v}_2 e^{\lambda_2t}  \end{equation} And now I have a problem finding the particular solution. I use the same approach  \begin{equation} x_p = \left(\begin{matrix}A_1\\A_2\end{matrix}\right) \quad \dot{\textbf{x}}_p = \hat{A} \textbf{x}_p + \textbf{b} \end{equation} which gives me the following set of equation \begin{equation} \begin{aligned} \mathrm{I:} \quad\\ \mathrm{II:} \quad\\ \end{aligned} \begin{aligned} 0 &=  -b A_1 + (d y_2 + c) A_2 + ax_0\\ 0 &= b A_1 - (d y_2 + c) A_2 \\ \end{aligned} \end{equation} which gives the result $A_1 = \frac{d y_2 + c}{b} A_2$ if $ax_0 = 0$. Now I have a problem to interprete this solution because I didn't expected the condition $ax_0 = 0$ for my solution. Is there only a solution of the system for $\left(d y_{2} - g x_{0}\right)$ if $ax_0 = 0$. Or is there a more general solution with $ax_0 \neq 0$? Did I make a mistake during my calculation?","I have a question, concerning one special solution of the following system. First I will sum up my results, to give a context for my question. My question will be seperated by a line, if you want to skip the introduction. $\begin{equation} \begin{aligned} \mathrm{II:} \quad\\ \mathrm{III:} \quad\\ \end{aligned} \begin{aligned} \dot{x}_2 &=  a x_0 - b x_2 + cx_3 + g x_0 x_3\\ \dot{x}_3 &= -d y_2 x_3 + b x_2 - cx_3 \\ \end{aligned} \end{equation}$ with the initial conditions $x_0 = x_1(0) \quad \rightarrow \quad x_2(0) = x_3(0) = x_4(0) = 0$ First I solved the homogenous part of the system: \begin{equation} \left(\begin{matrix}\dot{x}_2\\\dot{x}_3\end{matrix}\right) = \underbrace{\left[\begin{matrix}- b & g x_{0} + c\\b & -\left( d y_{2} +c \right)\end{matrix}\right]}_{\substack{\hat{A}}}\left(\begin{matrix}x_2\\x_3\end{matrix}\right) + \underbrace{\left(\begin{matrix}a x_0\\0\end{matrix}\right)}_{\substack{\textbf{b}}} \end{equation} \begin{equation} \begin{aligned} \\ \lambda_1 &= -\frac{1}{2}\left[ b+ c+d y_{2} + \sqrt{\left(b + c + d y_2 \right)^2 - 4 b \left(d y_2 - g x_0 \right)} \right]\\ \\ \lambda_2 &=  -\frac{1}{2}\left[ b+ c+d y_{2} + \sqrt{\left(b + c + d y_2 \right)^2 - 4 b \left(d y_2 - g x_0 \right)} \right] \\ \\ \end{aligned} \begin{aligned} v_1 &= \left(\begin{matrix}- \frac{2\left(c + g x_{0}\right)}{- b + c + d y_{2} +  \sqrt{\left(b + c + d y_2 \right)^2 - 4 b \left(d y_2 - g x_0 \right)}}\\1\end{matrix}\right) \\ v_2 &= \left(\begin{matrix}- \frac{2\left(c + g x_{0}\right)}{- b + c + d y_{2} - \sqrt{\left(b + c + d y_2 \right)^2 - 4 b \left(d y_2 - g x_0 \right)}}\\1\end{matrix}\right)\\ \end{aligned} \label{eq::modell6_eigenwerte_und_vektoren} \end{equation} which is given by  \begin{equation} \textbf{x}_h = c_1 \textbf{v}_1 e^{\lambda_1t} + c_2 \textbf{v}_2 e^{\lambda_2t}  \label{eq::modell5_homogene_loesung} \end{equation} I found the particular solution using the following approach \begin{equation} x_p = \left(\begin{matrix}A_1\\A_2\end{matrix}\right) \quad \dot{\textbf{x}}_p = \hat{A} \textbf{x}_p + \textbf{b} \end{equation} the solution of this equation is \begin{equation} \textbf{x}_p = \frac{ax_0}{\left(d y_2 - g x_0 \right)}\left(\begin{matrix}\frac{d y_2 + c}{b}\\1\end{matrix}\right) \end{equation} Adding homogenous and particular solution and using the initial conditions gave me the following general solution of the system. \begin{equation} \begin{aligned} x_1 &=  x_0\\ x_2 & =   \frac{ax_0}{\left(d y_2 - g x_0 \right)} \left[-\frac{\left(g x_{0} +c\right) \left(b + c + d y_{2} - \sqrt{\dots}\right)}{\sqrt{\dots}\left(- b + c + d y_{2} +  \sqrt{\dots} \right)}  e^{-\frac{1}{2} \left[b + c + d y_{2}+ \sqrt{\dots}\right]t} + \frac{\left(g x_{0} +c\right) \left(b + c + d y_{2} + \sqrt{\dots}\right)}{\sqrt{\dots}\left(- b + c + d y_{2} -  \sqrt{\dots} \right)}  e^{-\frac{1}{2} \left[b + c + d y_{2}- \sqrt{\dots}\right]t} + \frac{d y_2 + c}{b}\right]\\ x_3 &= \frac{ax_0}{\left(d y_2 - g x_0 \right)} \left[\frac{\left(b + c + d y_{2} - \sqrt{\dots}\right)}{2 \sqrt{\dots}}e^{-\frac{1}{2} \left[b + c + d y_{2} + \sqrt{\dots}\right]t}  - \frac{ \left(b + c + d y_{2} + \sqrt{\dots}\right)}{2  \sqrt{\dots}}e^{-\frac{1}{2} \left[b + c + d y_{2} - \sqrt{\dots}\right]t} + 1   \right]\\ x_4 &=\frac{d y_2 a x_0}{\left(d y_{2} - g x_{0}\right)}\left[-\frac{ \left(b + c + d y_{2} - \sqrt{\dots}\right)}{  \sqrt{\dots}\left[b + c + d y_{2} + \sqrt{\dots}\right]}e^{-\frac{1}{2} \left[b + c + d y_{2} + \sqrt{\dots}\right]t}+ \frac{ \left(b + c + d y_{2} + \sqrt{\dots}\right)}{  \sqrt{\dots}\left[b + c + d y_{2} - \sqrt{\dots}\right]}e^{-\frac{1}{2} \left[b + c + d y_{2} - \sqrt{\dots}\right]t} +  t  +\frac{ \left(b + c+  dy_2 \right)}{b\left(d y_{2} - g x_{0}\right)}\right] \end{aligned} \label{eq::modell6_loesung} \end{equation} with \begin{equation} \sqrt{\dots} = \sqrt{\left( b + c + d y_2 \right)^2 - 4b \left(dy_2 - g x_0 \right)} \end{equation} --------------------------------------------- Now I want to analyze the special case $\left(d y_{2} - g x_{0}\right)$. This solution is very interesting because the exponent of the e function, switches from positive to negative at this transition. As I get a zero-division error in my standard solution, I want to solve the system of equations again, with the new set of parameters. \begin{equation} \left(\begin{matrix}\dot{x}_2\\\dot{x}_3\end{matrix}\right) = \underbrace{\left[\begin{matrix}- b & c + d y_{2}\\b & - c - d y_{2}\end{matrix}\right]}_{\substack{\hat{A}}}\left(\begin{matrix}x_2\\x_3\end{matrix}\right) + \underbrace{\left(\begin{matrix}a x_0\\0\end{matrix}\right)}_{\substack{\textbf{b}}} \end{equation} homogenous solution: \begin{equation} \begin{aligned} \\ \lambda_1 &= 0\\ \\ \lambda_2 &=  - \left(b + c + d y_{2}\right)\\ \\ \end{aligned} \begin{aligned} v_1 &= \left(\begin{matrix}\frac{1}{b} \left(c + d y_{2}\right)\\1\end{matrix}\right)\\ v_2 &=  \left(\begin{matrix}-1\\1\end{matrix}\right)\\ \end{aligned} \label{eq::modell6_eigenwerte_und_vektoren_spezialfall_1} \end{equation}  \begin{equation} \textbf{x}_h = c_1 \textbf{v}_1 e^{\lambda_1t} + c_2 \textbf{v}_2 e^{\lambda_2t}  \end{equation} And now I have a problem finding the particular solution. I use the same approach  \begin{equation} x_p = \left(\begin{matrix}A_1\\A_2\end{matrix}\right) \quad \dot{\textbf{x}}_p = \hat{A} \textbf{x}_p + \textbf{b} \end{equation} which gives me the following set of equation \begin{equation} \begin{aligned} \mathrm{I:} \quad\\ \mathrm{II:} \quad\\ \end{aligned} \begin{aligned} 0 &=  -b A_1 + (d y_2 + c) A_2 + ax_0\\ 0 &= b A_1 - (d y_2 + c) A_2 \\ \end{aligned} \end{equation} which gives the result $A_1 = \frac{d y_2 + c}{b} A_2$ if $ax_0 = 0$. Now I have a problem to interprete this solution because I didn't expected the condition $ax_0 = 0$ for my solution. Is there only a solution of the system for $\left(d y_{2} - g x_{0}\right)$ if $ax_0 = 0$. Or is there a more general solution with $ax_0 \neq 0$? Did I make a mistake during my calculation?",,['ordinary-differential-equations']
39,Closed form solution to an ordinary differential equaiton,Closed form solution to an ordinary differential equaiton,,"How to solve the following ordinary differential equation? $$y'(x)= \frac{C_1}{y(x)} +C_2 C_3 \cos\left(C_3 x\right) +C_4$$ where $C_1, C_2, C_3, C_4\in \mathbb{R}$ are all constants. It looks simple but difficult to solve.","How to solve the following ordinary differential equation? $$y'(x)= \frac{C_1}{y(x)} +C_2 C_3 \cos\left(C_3 x\right) +C_4$$ where $C_1, C_2, C_3, C_4\in \mathbb{R}$ are all constants. It looks simple but difficult to solve.",,"['ordinary-differential-equations', 'closed-form', 'fundamental-solution']"
40,Why does the taylor expansion of a nonlinear system of differential equations exist if it has continuous second order partial derivatives?,Why does the taylor expansion of a nonlinear system of differential equations exist if it has continuous second order partial derivatives?,,"My textbook states that for a nonlinear autonomous system $$x^\prime = F(x,y)\qquad y^\prime = G(x,y)$$ The system is locally linear in the neighborhood of a critical point $(x_0,y_0)$ whenever the functions $F$ and $G$ have continuous partial derivatives up to order two. To show this , we use Taylor expansions about the point $(x_0,y_0)$ to write $F(x,y)$ and $G(x,y)$ in the form   $$F(x,y)=F(x_0,y_0)+F_x(x_0,y_0)(x-x_0)+F_y(x_0,y_0)(y-y_0)+\eta_1(x,y)\\ G(x,y)=G(x_0,y_0)+G_x(x_0,y_0)(x-x_0)+G_y(x_0,y_0)(y-y_0)+\eta_2(x,y)$$   where $\eta_1(x,y)/[(x-x_0)^2+(y-y_0)^2]^{1/2}\to 0$ as $(x,y)\to(x_0,y_0)$, and similarly for $\eta_2$. By locally linear, the book means that the nonlinear system of differential equations around the critical point can be approximated by the linear system $$\mathbf{u}^\prime=\left( \begin{array}{@{}cc@{}} F_x(x_0,y_0)&F_y(x_0,y_0)\\ G_x(x_0,y_0)&G_y(x_0,y_0) \end{array} \right)\mathbf{u}$$ where $\mathbf{u}=\mathbf{x}-\mathbf{x}^0$. The critical point $\mathbf{x}^0=(x_0,y_0)$ is an isolated critical point where $$F(x_0,y_0)=0\quad G(x_0,y_0)=0\\ \det\left(\begin{array}{@{}cc@{}} F_x(x_0,y_0)&F_y(x_0,y_0)\\ G_x(x_0,y_0)&G_y(x_0,y_0) \end{array}\right)\neq0$$ Why does $F$ and $G$ need to have second order partial derivatives if the first order partial derivatives only show up in this expression? And how does that guarantee that $\mathbf{\eta} /||\mathbf{x}-\mathbf{x}^0||$ will go to $0$ as $\mathbf{x}$ approaches the critical point?","My textbook states that for a nonlinear autonomous system $$x^\prime = F(x,y)\qquad y^\prime = G(x,y)$$ The system is locally linear in the neighborhood of a critical point $(x_0,y_0)$ whenever the functions $F$ and $G$ have continuous partial derivatives up to order two. To show this , we use Taylor expansions about the point $(x_0,y_0)$ to write $F(x,y)$ and $G(x,y)$ in the form   $$F(x,y)=F(x_0,y_0)+F_x(x_0,y_0)(x-x_0)+F_y(x_0,y_0)(y-y_0)+\eta_1(x,y)\\ G(x,y)=G(x_0,y_0)+G_x(x_0,y_0)(x-x_0)+G_y(x_0,y_0)(y-y_0)+\eta_2(x,y)$$   where $\eta_1(x,y)/[(x-x_0)^2+(y-y_0)^2]^{1/2}\to 0$ as $(x,y)\to(x_0,y_0)$, and similarly for $\eta_2$. By locally linear, the book means that the nonlinear system of differential equations around the critical point can be approximated by the linear system $$\mathbf{u}^\prime=\left( \begin{array}{@{}cc@{}} F_x(x_0,y_0)&F_y(x_0,y_0)\\ G_x(x_0,y_0)&G_y(x_0,y_0) \end{array} \right)\mathbf{u}$$ where $\mathbf{u}=\mathbf{x}-\mathbf{x}^0$. The critical point $\mathbf{x}^0=(x_0,y_0)$ is an isolated critical point where $$F(x_0,y_0)=0\quad G(x_0,y_0)=0\\ \det\left(\begin{array}{@{}cc@{}} F_x(x_0,y_0)&F_y(x_0,y_0)\\ G_x(x_0,y_0)&G_y(x_0,y_0) \end{array}\right)\neq0$$ Why does $F$ and $G$ need to have second order partial derivatives if the first order partial derivatives only show up in this expression? And how does that guarantee that $\mathbf{\eta} /||\mathbf{x}-\mathbf{x}^0||$ will go to $0$ as $\mathbf{x}$ approaches the critical point?",,"['ordinary-differential-equations', 'taylor-expansion', 'systems-of-equations']"
41,Solution of an ODE,Solution of an ODE,,"I am trying to solve the following ODE: $$y''(x)+\left( k_0^2-\frac{\lambda}{1+\cosh^2(ax)}\right)y(x)=0 \qquad k_0,\lambda,a>0$$ when as $x \rightarrow \infty$ the solution is of the form $y(x)=e^{ik_0x}$. My attempt: I did the folllowing substitution $t=-\cosh^2(ax)$ and the ODE which came up was $$ 4a^2t(t+1)y''(t)+2a^2(2t+1)y'(t)+ \left( k_0- \frac{\lambda}{1-t} \right)y(t)=0$$ And then I can use the frobenius method. The problem is that it gets quite complicated with all the regular singular points and also that I don't use anywhere the form of the solution for large x. Any help is appreciated.","I am trying to solve the following ODE: $$y''(x)+\left( k_0^2-\frac{\lambda}{1+\cosh^2(ax)}\right)y(x)=0 \qquad k_0,\lambda,a>0$$ when as $x \rightarrow \infty$ the solution is of the form $y(x)=e^{ik_0x}$. My attempt: I did the folllowing substitution $t=-\cosh^2(ax)$ and the ODE which came up was $$ 4a^2t(t+1)y''(t)+2a^2(2t+1)y'(t)+ \left( k_0- \frac{\lambda}{1-t} \right)y(t)=0$$ And then I can use the frobenius method. The problem is that it gets quite complicated with all the regular singular points and also that I don't use anywhere the form of the solution for large x. Any help is appreciated.",,['ordinary-differential-equations']
42,Stability of periodic solution,Stability of periodic solution,,"I am stuck on this question: $$x'(t) = x(x-p(t))(x-1),$$ where $p$ is $1$-periodic and $0< p(t) <1$ for all $t$. We want to show the existence of a STABLE periodic solution $\mu$ such that $0< \mu(t) <1$ for all $t$. If we require $0< x_0 <1$, where we impose the initial condition that $x(0) = x_0$, then it is clear that there exists a solution $\mu$ that satisfies the ODE with this initial condition. Standard result also shows the existence of such periodic solutions. My question is how to show the stability of such solutions by the definition of stable periodic solutions.","I am stuck on this question: $$x'(t) = x(x-p(t))(x-1),$$ where $p$ is $1$-periodic and $0< p(t) <1$ for all $t$. We want to show the existence of a STABLE periodic solution $\mu$ such that $0< \mu(t) <1$ for all $t$. If we require $0< x_0 <1$, where we impose the initial condition that $x(0) = x_0$, then it is clear that there exists a solution $\mu$ that satisfies the ODE with this initial condition. Standard result also shows the existence of such periodic solutions. My question is how to show the stability of such solutions by the definition of stable periodic solutions.",,"['ordinary-differential-equations', 'dynamical-systems']"
43,"Transforming the solutions of $\dot x = f(\mathbf{x}, t)$ and $\dot x = B(\mathbf{x}, t)f(\mathbf{x}, t)$ into each other",Transforming the solutions of  and  into each other,"\dot x = f(\mathbf{x}, t) \dot x = B(\mathbf{x}, t)f(\mathbf{x}, t)","How can I prove the following theorem? If the function $B(\mathbf{x}, t)$ is strictly positive, then the solutions of the two differential equations $\dot x = f(\mathbf{x}, t)$ and $\dot x = B(\mathbf{x}, t)f(\mathbf{x}, t)$ can be transformed into each other by a strictly monotonic change in the time scale $\tau = \phi(t)$. Note: This theorem is mentioned in Hofbauer and Sigmund's Evolutionary Games and Population Dynamics , p. 32. Here is what I got so far, but I'm not sure how to proceed: \begin{align*}   \frac{d \mathbf{x}}{d \tau} &= B(\mathbf{x}(\tau),                                     \tau)f(\mathbf{x}(\tau), \tau) \\   \frac{d \mathbf{x}}{d t}\frac{dt}{d\tau} &=                                                        B(\mathbf{x}(\tau), \tau)f(\mathbf{x}(\tau), \tau) \\  f(\mathbf{x},t) \frac{dt}{d \tau} &=   B(\mathbf{x}(\tau), \tau)f(\mathbf{x}(\tau), \tau) \\ \end{align*}","How can I prove the following theorem? If the function $B(\mathbf{x}, t)$ is strictly positive, then the solutions of the two differential equations $\dot x = f(\mathbf{x}, t)$ and $\dot x = B(\mathbf{x}, t)f(\mathbf{x}, t)$ can be transformed into each other by a strictly monotonic change in the time scale $\tau = \phi(t)$. Note: This theorem is mentioned in Hofbauer and Sigmund's Evolutionary Games and Population Dynamics , p. 32. Here is what I got so far, but I'm not sure how to proceed: \begin{align*}   \frac{d \mathbf{x}}{d \tau} &= B(\mathbf{x}(\tau),                                     \tau)f(\mathbf{x}(\tau), \tau) \\   \frac{d \mathbf{x}}{d t}\frac{dt}{d\tau} &=                                                        B(\mathbf{x}(\tau), \tau)f(\mathbf{x}(\tau), \tau) \\  f(\mathbf{x},t) \frac{dt}{d \tau} &=   B(\mathbf{x}(\tau), \tau)f(\mathbf{x}(\tau), \tau) \\ \end{align*}",,"['ordinary-differential-equations', 'dynamical-systems']"
44,Examples on conceptual problems for eigenvalues in differential equations,Examples on conceptual problems for eigenvalues in differential equations,,"I am currently holding a discussion class on diff eqs for engineers and I am looking for an interesting conceptual problem on eigenvalues in diff eqs. Most of the problems in 5 different books that I have searched in go like this: Suppose that $y'' + \lambda y = 0$ where $y(0) = 0, y(0) = \pi$. Find the values of $\lambda$ for which a nontrivial solution exists and find the corresponding eigenvector. While I think that the above routine problem is a good practice, the students have done this a lot in HW. I am looking for a conceptual problem (geometric thinking would be great too) to test whether students really understand the concept above. Or is there really nothing much to add? I'd like to hear from you for any suggestions. Thanks!","I am currently holding a discussion class on diff eqs for engineers and I am looking for an interesting conceptual problem on eigenvalues in diff eqs. Most of the problems in 5 different books that I have searched in go like this: Suppose that $y'' + \lambda y = 0$ where $y(0) = 0, y(0) = \pi$. Find the values of $\lambda$ for which a nontrivial solution exists and find the corresponding eigenvector. While I think that the above routine problem is a good practice, the students have done this a lot in HW. I am looking for a conceptual problem (geometric thinking would be great too) to test whether students really understand the concept above. Or is there really nothing much to add? I'd like to hear from you for any suggestions. Thanks!",,"['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'boundary-value-problem']"
45,What should I know before reading Ordinary Differential Equations of VI Arnold?,What should I know before reading Ordinary Differential Equations of VI Arnold?,,"I have just adquired some math books and VI Arnold is one of them. But it was more sophisticated that I may think, so I want to know what should I know to be able to read it. Actually I have on my belt, linear algebra and calculus of one variable. Can somebody help me?","I have just adquired some math books and VI Arnold is one of them. But it was more sophisticated that I may think, so I want to know what should I know to be able to read it. Actually I have on my belt, linear algebra and calculus of one variable. Can somebody help me?",,"['ordinary-differential-equations', 'soft-question']"
46,Exponential of a power of the differential operator,Exponential of a power of the differential operator,,In relation to this question: Exponential of a polynomial of the differential operator Is there an expression for $\exp(aD^n)f(x)$ similar to $\exp(aD)f(x)=f(x+a)$?,In relation to this question: Exponential of a polynomial of the differential operator Is there an expression for $\exp(aD^n)f(x)$ similar to $\exp(aD)f(x)=f(x+a)$?,,"['ordinary-differential-equations', 'differential-geometry', 'taylor-expansion', 'lie-algebras', 'exponentiation']"
47,"Boundary Layer, leading order, Pertubation Theory, Differential Equations","Boundary Layer, leading order, Pertubation Theory, Differential Equations",,"I have got the following problem, taken from Multiple Scale and singular perturbation methods, Kevorkian & Cole book, page 94, exercise 1.b.: Find the leading order of the problem: $\varepsilon y''+ \frac{1}{2}y^2y'-y=0 $, with  $0<x<1$ and $y(0)=A$,  $y(1)=B$, and A,B independent of $\varepsilon$ . I have tried to use the boundary layer method in order to find an outer and inner solution and then make the respective matching to find some constants which appear. The inner layer is at some point inside the interval if $B^2-A^2\ne4$. When doing the expansion for the inner solution I have got the following equation: $Y_0'+ \frac{1}{6}Y_0^3=K$. This integral is not trivial but it can be solved by partial-fraction decomposition to get an implicit solution with depends in two different constants, namely $K$ and $C$. I have got $\frac{x}{6}+C=\frac{1}{3K^{\frac{2}{3}}}\left[-\ln\left(K^{\frac{1}{3}}-Y_{0}\right)+\frac{1}{2}\ln\left(K^{\frac{2}{3}}+K^{\frac{1}{3}}Y_{0}+Y_{0}^{2}\right)+\sqrt{3}\tan^{-1}\left(\frac{2Y_{0}}{\sqrt{3}K^{\frac{1}{3}}}+\frac{1}{\sqrt{3}}\right)\right]$ The problem here is that in order to do the matching, I should get an explicit solution $Y_0$ in order to find the constants. Can someone give a hint? Is there maybe another approach to solve this problem? Thank you.","I have got the following problem, taken from Multiple Scale and singular perturbation methods, Kevorkian & Cole book, page 94, exercise 1.b.: Find the leading order of the problem: $\varepsilon y''+ \frac{1}{2}y^2y'-y=0 $, with  $0<x<1$ and $y(0)=A$,  $y(1)=B$, and A,B independent of $\varepsilon$ . I have tried to use the boundary layer method in order to find an outer and inner solution and then make the respective matching to find some constants which appear. The inner layer is at some point inside the interval if $B^2-A^2\ne4$. When doing the expansion for the inner solution I have got the following equation: $Y_0'+ \frac{1}{6}Y_0^3=K$. This integral is not trivial but it can be solved by partial-fraction decomposition to get an implicit solution with depends in two different constants, namely $K$ and $C$. I have got $\frac{x}{6}+C=\frac{1}{3K^{\frac{2}{3}}}\left[-\ln\left(K^{\frac{1}{3}}-Y_{0}\right)+\frac{1}{2}\ln\left(K^{\frac{2}{3}}+K^{\frac{1}{3}}Y_{0}+Y_{0}^{2}\right)+\sqrt{3}\tan^{-1}\left(\frac{2Y_{0}}{\sqrt{3}K^{\frac{1}{3}}}+\frac{1}{\sqrt{3}}\right)\right]$ The problem here is that in order to do the matching, I should get an explicit solution $Y_0$ in order to find the constants. Can someone give a hint? Is there maybe another approach to solve this problem? Thank you.",,"['ordinary-differential-equations', 'asymptotics', 'perturbation-theory']"
48,Rolling parabola & catenary,Rolling parabola & catenary,,"By rolling a rigid catenary on a straight line one obtains the locus of its center of curvature as a parabola. This is well known as the natural equation connecting arc length and radius of curvature is   $$ R = s^2/a + a $$ EDIT2: which for dynamic rolling we set: $$ s= x , R = y $$ to make its evolute re-appear as $$ y = x^2/ a + a. $$ EDIT1: In the central position the rigid catenary has the equation:  $\, y/ a = cosh (x/a) $ Likewise by rolling a rigid parabolic curve on a straight line do we get back a catenary locus? Apart from parabola/catenary do such rolling duals exist in this way? The famous architect Antoni Gaudí designed columns of La Sagrada Familia  in Barcelona, Spain extensively using the catenary and parabola. “By rotating and moving the dish along the line, the focus of the conic describes the catenary“ Is this correct, as given in the website? Or do I miss something? (In engineering design of suspension bridges advantage is taken from catenary and parabola shapes as they carry constant cable loading along arc and span respectively). Also it is mentioned “He studied how exactly the branches of a tree support the weight of its crown, then applied the same principles to his columns“  Can we get information about this design anywhere? Did he make  branches atop columns shaped as catenary/ parabola arches?","By rolling a rigid catenary on a straight line one obtains the locus of its center of curvature as a parabola. This is well known as the natural equation connecting arc length and radius of curvature is   $$ R = s^2/a + a $$ EDIT2: which for dynamic rolling we set: $$ s= x , R = y $$ to make its evolute re-appear as $$ y = x^2/ a + a. $$ EDIT1: In the central position the rigid catenary has the equation:  $\, y/ a = cosh (x/a) $ Likewise by rolling a rigid parabolic curve on a straight line do we get back a catenary locus? Apart from parabola/catenary do such rolling duals exist in this way? The famous architect Antoni Gaudí designed columns of La Sagrada Familia  in Barcelona, Spain extensively using the catenary and parabola. “By rotating and moving the dish along the line, the focus of the conic describes the catenary“ Is this correct, as given in the website? Or do I miss something? (In engineering design of suspension bridges advantage is taken from catenary and parabola shapes as they carry constant cable loading along arc and span respectively). Also it is mentioned “He studied how exactly the branches of a tree support the weight of its crown, then applied the same principles to his columns“  Can we get information about this design anywhere? Did he make  branches atop columns shaped as catenary/ parabola arches?",,['ordinary-differential-equations']
49,Nonlinear first order ODE with quadratic in the derivative,Nonlinear first order ODE with quadratic in the derivative,,"This equation shouldn't be so hard, and yet I'm stymied. $$ \left( \frac{dw}{dz} \right )^2 + \alpha \frac{dw}{dz} + w \beta = 0 $$ with $w(0) = w_0>0$ $w(L) = 0$ for some known L and $\alpha(z)>0$ and $\beta(z)>0$ known. $\alpha$ and $\beta$ actually begin life as functions of known $w$, so I dont worry about existence of solution, what I want to prove is that I can invert back from $\alpha$ and $\beta$ to give unique $w$. $d \alpha / dz = \beta + C$ for some constant $C$ if it helps. I can solve for some special cases but I'm interested in a general expression, or at least to prove uniqueness. Interestingly, those special cases are not unique if I just put constraint at $z=0$ but become so with the $z=L$ constraint.","This equation shouldn't be so hard, and yet I'm stymied. $$ \left( \frac{dw}{dz} \right )^2 + \alpha \frac{dw}{dz} + w \beta = 0 $$ with $w(0) = w_0>0$ $w(L) = 0$ for some known L and $\alpha(z)>0$ and $\beta(z)>0$ known. $\alpha$ and $\beta$ actually begin life as functions of known $w$, so I dont worry about existence of solution, what I want to prove is that I can invert back from $\alpha$ and $\beta$ to give unique $w$. $d \alpha / dz = \beta + C$ for some constant $C$ if it helps. I can solve for some special cases but I'm interested in a general expression, or at least to prove uniqueness. Interestingly, those special cases are not unique if I just put constraint at $z=0$ but become so with the $z=L$ constraint.",,"['ordinary-differential-equations', 'quadratics', 'nonlinear-system']"
50,Green's function the way George Green defined it,Green's function the way George Green defined it,,"This is a curious question about the way George Green could have defined his Green's function. All the definitions I see have only Dirac-delta $\delta(x-x')$ function as their source on the RHS. But the Dirac-delta was defined about a century after Green's work. I just want to know, how George Green have defined his function (in case he defined it) ? In case he didn't define his Green's function, how did he give a hint about this method (without Dirac-delta function) ?","This is a curious question about the way George Green could have defined his Green's function. All the definitions I see have only Dirac-delta $\delta(x-x')$ function as their source on the RHS. But the Dirac-delta was defined about a century after Green's work. I just want to know, how George Green have defined his function (in case he defined it) ? In case he didn't define his Green's function, how did he give a hint about this method (without Dirac-delta function) ?",,"['ordinary-differential-equations', 'soft-question', 'mathematical-physics']"
51,WKB and asymptotic behavior of second order differential equation,WKB and asymptotic behavior of second order differential equation,,"I want to study the large $x$ solution to a Riccati equation.  After listening to the lectures on Mathematical Physics by Carl Bender, I have fallen in love with asymptotic analysis.  But, by no means do I have a full understanding of it. I transformed the Riccati equation into the following second order differential equation: $$u''(x)-Q(x) u(x)=0\qquad Q(x)=\frac{n(n+2)}{4x^2}+\frac{a^2 b^2\,e^{-2x}}{x^{2n+1}},\enspace\text{with}\,\, a,b>0, \,\,n=\{0,1,2,\ldots\}$$ defined on $0<x<\infty$, subject to the initial condition: $u(1)=1$, $u'(1)=1$.  The solution to this differential equation gives the solution y(x) to the Riccati equation: $$ y_\text{sol}(x)=-\frac{(n+2)}{2a}x^{n+1} + \frac{x^{n+2}}{a}\frac{d}{dx}\ln u(x) $$ For concreteness take $n=0$ and $a=100$, $b=1$. My Attempt My numerical investigations revealed that the large $x$ behavior of $u$ is very insensitive to the initial conditions.  (The hunch I got from this is that the dependence on the initial condition would appear in the negligible part of the solution).  And $y_\text{sol}$ at large $x$ tends to a non-zero constant which I am interested in. I apply WKB.  But, before doing so I recognized this problem as arising from a 3-dimensional QM problem where $x$ is a radial coordinate.  I recalled some obscure claim by Langer (1937) that in order to get an 'accurate WKB result' we must rescale $x$ by putting $x=e^z$ to make the domain of the problem over the entire real numbers $-\infty<z<\infty$. [I have no understanding of why this must be done from an asymptotic analysis point of view; can someone help me with this?].  Then if I put $u(x)=e^{z/2}w(z)$, I get a new second order differential equation $$w''(z) - P(z)w(z)=0\qquad P(z)=\frac{(n+1)^2}{4}+e^{2z}\frac{a^2b^2\,e^{-2(e^{z})}}{(e^{z})^{2n+1}}$$ for which I can apply WKB.  The end result (after transforming back to $u$ in $x$-space) is: $$ u(x)\sim A \frac{1}{Q_L(x)^{1/4}}\Big(e^{\int_1^x dx' \sqrt{Q_L(x')}}+B e^{-\int_1^x dx' \sqrt{Q_L(x')}}\Big) $$ Here, $Q_L = \displaystyle\frac{(n+1)^2}{4x^2}+\frac{a^2 b^2\,e^{-2x}}{x^{2n+1}}$ is slightly different from $Q$ due to the brief transformation to $z$-space, and $A$ and $B$ are constants fixed by boundary conditions. Problems When I plot this result against the numerically integrated solution for $u(x)$ and also for $y(x)$, I find that WKB does a fantastic job approximating the small $x$ behavior of the exact solution, but doesn't get quite get the large $x$ behavior correctly.  WKB does get leading the asymptotic behavior: $u'(x)/u(x) \sim \frac{n+2}{2}\frac{1}{x}$ as $x\rightarrow\infty$.  But I need the next-to-leading asymptotic form which I expect to be $\sim 1/x^{n+2}$. But WKB approximation tells me that it is $\sim e^{-2x}$, which is very subdominant compared to what I am expecting.  I am puzzled by this because I thought WKB was supposed to uniformly get the right answer for all $x$!  I tried even tried taking the WKB approximation to next to leading order in hopes of getting the expected next-to-leading order asymptotic behavior, but I still keep getting highly subdominant terms...  what's going on?? Perhaps I should follow a different strategy to determine the asymptotic behavior? Plot Here is a plot of $y(x)$ satisfying $y(1)=1$ for $n=0$ and $a=100$, $b=1$: Magenta curve: numerically integrated. Blue curve: WKB result.","I want to study the large $x$ solution to a Riccati equation.  After listening to the lectures on Mathematical Physics by Carl Bender, I have fallen in love with asymptotic analysis.  But, by no means do I have a full understanding of it. I transformed the Riccati equation into the following second order differential equation: $$u''(x)-Q(x) u(x)=0\qquad Q(x)=\frac{n(n+2)}{4x^2}+\frac{a^2 b^2\,e^{-2x}}{x^{2n+1}},\enspace\text{with}\,\, a,b>0, \,\,n=\{0,1,2,\ldots\}$$ defined on $0<x<\infty$, subject to the initial condition: $u(1)=1$, $u'(1)=1$.  The solution to this differential equation gives the solution y(x) to the Riccati equation: $$ y_\text{sol}(x)=-\frac{(n+2)}{2a}x^{n+1} + \frac{x^{n+2}}{a}\frac{d}{dx}\ln u(x) $$ For concreteness take $n=0$ and $a=100$, $b=1$. My Attempt My numerical investigations revealed that the large $x$ behavior of $u$ is very insensitive to the initial conditions.  (The hunch I got from this is that the dependence on the initial condition would appear in the negligible part of the solution).  And $y_\text{sol}$ at large $x$ tends to a non-zero constant which I am interested in. I apply WKB.  But, before doing so I recognized this problem as arising from a 3-dimensional QM problem where $x$ is a radial coordinate.  I recalled some obscure claim by Langer (1937) that in order to get an 'accurate WKB result' we must rescale $x$ by putting $x=e^z$ to make the domain of the problem over the entire real numbers $-\infty<z<\infty$. [I have no understanding of why this must be done from an asymptotic analysis point of view; can someone help me with this?].  Then if I put $u(x)=e^{z/2}w(z)$, I get a new second order differential equation $$w''(z) - P(z)w(z)=0\qquad P(z)=\frac{(n+1)^2}{4}+e^{2z}\frac{a^2b^2\,e^{-2(e^{z})}}{(e^{z})^{2n+1}}$$ for which I can apply WKB.  The end result (after transforming back to $u$ in $x$-space) is: $$ u(x)\sim A \frac{1}{Q_L(x)^{1/4}}\Big(e^{\int_1^x dx' \sqrt{Q_L(x')}}+B e^{-\int_1^x dx' \sqrt{Q_L(x')}}\Big) $$ Here, $Q_L = \displaystyle\frac{(n+1)^2}{4x^2}+\frac{a^2 b^2\,e^{-2x}}{x^{2n+1}}$ is slightly different from $Q$ due to the brief transformation to $z$-space, and $A$ and $B$ are constants fixed by boundary conditions. Problems When I plot this result against the numerically integrated solution for $u(x)$ and also for $y(x)$, I find that WKB does a fantastic job approximating the small $x$ behavior of the exact solution, but doesn't get quite get the large $x$ behavior correctly.  WKB does get leading the asymptotic behavior: $u'(x)/u(x) \sim \frac{n+2}{2}\frac{1}{x}$ as $x\rightarrow\infty$.  But I need the next-to-leading asymptotic form which I expect to be $\sim 1/x^{n+2}$. But WKB approximation tells me that it is $\sim e^{-2x}$, which is very subdominant compared to what I am expecting.  I am puzzled by this because I thought WKB was supposed to uniformly get the right answer for all $x$!  I tried even tried taking the WKB approximation to next to leading order in hopes of getting the expected next-to-leading order asymptotic behavior, but I still keep getting highly subdominant terms...  what's going on?? Perhaps I should follow a different strategy to determine the asymptotic behavior? Plot Here is a plot of $y(x)$ satisfying $y(1)=1$ for $n=0$ and $a=100$, $b=1$: Magenta curve: numerically integrated. Blue curve: WKB result.",,"['ordinary-differential-equations', 'asymptotics', 'self-learning']"
52,How can you find the Wronskian of Bessel Functions?,How can you find the Wronskian of Bessel Functions?,,"This is a homework problem I am trying to solve but I'm not sure if I'm doing it correctly, because it seems deceptively simple. Let $\alpha$ be a non-negative real constant. The differential equation $$ x^2y\prime\prime +xy\prime+(x^2-\alpha ^2)y=0 $$ has solutions called Bessel functions. Without solving the differential equation, compute the Wronskian of two Bessel functions by using Abel's Theorem. Here is what I did $$ y\prime\prime + \frac{1}{x}y\prime + \frac{(x^2-\alpha^2)}{x^2}y = 0 \\ W(y_1,y_2) = c\cdot \exp\left[-\int p(t)dt\right]  \\ = c \cdot \exp\left[-\int \frac{1}{x} dt\right]  \\ =c \cdot \exp[-\ln|x|]  \\ =cx^{-1} = c\cdot\frac{1}{x} $$ Is this correct or am I missing something? Any help would be greatly appreciated.","This is a homework problem I am trying to solve but I'm not sure if I'm doing it correctly, because it seems deceptively simple. Let $\alpha$ be a non-negative real constant. The differential equation $$ x^2y\prime\prime +xy\prime+(x^2-\alpha ^2)y=0 $$ has solutions called Bessel functions. Without solving the differential equation, compute the Wronskian of two Bessel functions by using Abel's Theorem. Here is what I did $$ y\prime\prime + \frac{1}{x}y\prime + \frac{(x^2-\alpha^2)}{x^2}y = 0 \\ W(y_1,y_2) = c\cdot \exp\left[-\int p(t)dt\right]  \\ = c \cdot \exp\left[-\int \frac{1}{x} dt\right]  \\ =c \cdot \exp[-\ln|x|]  \\ =cx^{-1} = c\cdot\frac{1}{x} $$ Is this correct or am I missing something? Any help would be greatly appreciated.",,['ordinary-differential-equations']
53,Solving numerically the equation of motion of D7 brane perturbation,Solving numerically the equation of motion of D7 brane perturbation,,"I want to solve this equation $$ \partial_{\rho}^{2}\phi+\frac{3}{\rho}\partial_{\rho}\phi+\left(\frac{M^{2}}{(1+\rho^{2})^{2}}-\frac{l(l+2)}{\rho^{2}}\right)\phi=0 $$ numerically. I know that this equation can be transformed into the hypergeometric equation through the transformation $$ \phi(\rho) = \rho^l (1+\rho^2)^{-\alpha} P(\rho) $$ (in which $P$ is some function) whose exact solution is the well known function see here $$ _2 F_1(a,b;c;\rho) = \sum_{n=0}^\infty \frac{(a)_n (b)_n}{(c)_n} \frac{\rho^n}{n!}. $$ The crucial characteristic of this function is that if $a$ or $b$ are negative integers, then the series is finite. However, I'm interested in exploring a numerical solution for this equation and I would like to know how to obtain numerically the finite series solutions. Any idea? Thanks.","I want to solve this equation $$ \partial_{\rho}^{2}\phi+\frac{3}{\rho}\partial_{\rho}\phi+\left(\frac{M^{2}}{(1+\rho^{2})^{2}}-\frac{l(l+2)}{\rho^{2}}\right)\phi=0 $$ numerically. I know that this equation can be transformed into the hypergeometric equation through the transformation $$ \phi(\rho) = \rho^l (1+\rho^2)^{-\alpha} P(\rho) $$ (in which $P$ is some function) whose exact solution is the well known function see here $$ _2 F_1(a,b;c;\rho) = \sum_{n=0}^\infty \frac{(a)_n (b)_n}{(c)_n} \frac{\rho^n}{n!}. $$ The crucial characteristic of this function is that if $a$ or $b$ are negative integers, then the series is finite. However, I'm interested in exploring a numerical solution for this equation and I would like to know how to obtain numerically the finite series solutions. Any idea? Thanks.",,"['ordinary-differential-equations', 'numerical-methods', 'physics', 'string-theory']"
54,Hypergeometric Function Differential Equation,Hypergeometric Function Differential Equation,,"Is there some nice obvious way to see that the hypergeometric function $$_2F_1(a,b;c:z) = \sum_{i=0}^\infty \tfrac{(a)_n(b)_n}{(c)_n}\tfrac{z^n}{n!}$$ should satisfy the differential equation $$z(1-z)\tfrac{d^2u}{dz^2} + [c-(a+b+1)]\tfrac{du}{dz}-abu=0?$$ I can't get it to work out by directly differentiating & it's driving me crazy - can it be done directly or does it require some nice identity? Thanks","Is there some nice obvious way to see that the hypergeometric function $$_2F_1(a,b;c:z) = \sum_{i=0}^\infty \tfrac{(a)_n(b)_n}{(c)_n}\tfrac{z^n}{n!}$$ should satisfy the differential equation $$z(1-z)\tfrac{d^2u}{dz^2} + [c-(a+b+1)]\tfrac{du}{dz}-abu=0?$$ I can't get it to work out by directly differentiating & it's driving me crazy - can it be done directly or does it require some nice identity? Thanks",,['ordinary-differential-equations']
55,Showing the 3D Ricci flow ODE preserves the order of the curvature tensor eigenvalues,Showing the 3D Ricci flow ODE preserves the order of the curvature tensor eigenvalues,,"The following system of ODEs arises when studying Ricci flow on 3-manifolds: $$ \frac{dm_1}{dt} = m_1^2+m_2m_3 \\ \frac{dm_2}{dt} = m_2^2+m_1m_3 \\ \frac{dm_3}{dt} = m_3^2+m_1m_2 \\ $$ Going back over Hamilton's 1986 paper I realised I didn't understand the first step of his reasoning: Note that $$\frac{d}{dt}(m_2 - m_1) = (m_2-m_1)(m_2+m_1-m_3)$$ so that if $m_1 \le m_2$ to start it remains so. The only extra context required is that $m_2 \le m_3$ at the initial time. How can we make this conclusion? It's not as simple as $m_2 - m_1$ being non-decreasing; choosing a large value for $m_3$ makes this clear. I initially thought to rewrite as $$\frac{d}{dt}\log(m_2-m_1) = m_2 + m_1 - m_3$$ but at this stage of the analysis I see no reason why $\int (m_2 + m_1 - m_3) dt$ should not fly off to $-\infty$; indeed we have blowup in finite time for the ""similar"" equation $\frac{df}{dt}=f^2$. I have a feeling that I'm missing something very easy and will shortly be quite embarrassed, but it's been niggling at me.","The following system of ODEs arises when studying Ricci flow on 3-manifolds: $$ \frac{dm_1}{dt} = m_1^2+m_2m_3 \\ \frac{dm_2}{dt} = m_2^2+m_1m_3 \\ \frac{dm_3}{dt} = m_3^2+m_1m_2 \\ $$ Going back over Hamilton's 1986 paper I realised I didn't understand the first step of his reasoning: Note that $$\frac{d}{dt}(m_2 - m_1) = (m_2-m_1)(m_2+m_1-m_3)$$ so that if $m_1 \le m_2$ to start it remains so. The only extra context required is that $m_2 \le m_3$ at the initial time. How can we make this conclusion? It's not as simple as $m_2 - m_1$ being non-decreasing; choosing a large value for $m_3$ makes this clear. I initially thought to rewrite as $$\frac{d}{dt}\log(m_2-m_1) = m_2 + m_1 - m_3$$ but at this stage of the analysis I see no reason why $\int (m_2 + m_1 - m_3) dt$ should not fly off to $-\infty$; indeed we have blowup in finite time for the ""similar"" equation $\frac{df}{dt}=f^2$. I have a feeling that I'm missing something very easy and will shortly be quite embarrassed, but it's been niggling at me.",,"['ordinary-differential-equations', 'riemannian-geometry', 'ricci-flow']"
56,Inverse spectral problem: How to recover the function $ q(x) $?,Inverse spectral problem: How to recover the function ?, q(x) ,"The forward problem is a second order Sturm-Liouville operator $$ - \frac{d^{2}}{dx^{2}}y(x)+q(x)y(x)=zy(x) $$ with the boundary conditions $ y(0)=0=y(\infty) $. If I know the spectral measure function $ \sigma (x) =\sum_{\lambda_{n} \le x}1 $, then can I reconstruct the inverse of the potential $ q^{-1}(x) $? My question is, how do I use the Gelfand-Levitan-Marchenko theory to reconstruct the potential $ q(x) $?","The forward problem is a second order Sturm-Liouville operator $$ - \frac{d^{2}}{dx^{2}}y(x)+q(x)y(x)=zy(x) $$ with the boundary conditions $ y(0)=0=y(\infty) $. If I know the spectral measure function $ \sigma (x) =\sum_{\lambda_{n} \le x}1 $, then can I reconstruct the inverse of the potential $ q^{-1}(x) $? My question is, how do I use the Gelfand-Levitan-Marchenko theory to reconstruct the potential $ q(x) $?",,"['ordinary-differential-equations', 'inverse-problems', 'sturm-liouville']"
57,Biharmonic equation in  spherical coordinates,Biharmonic equation in  spherical coordinates,,"What is a general solution of biharmonic equation $\nabla^2\nabla^2f=0$ in spherical coordinates? According to Wikipedia any solution of Laplace equation is also a solution of biharmonic equation, but the vice versa is not always true. Therefore I assume there exists a more general solution. According to Oberbeck, A., Crelle, 62, 1876 , a general solution of biharmonic equation in spherical coordinates is $\label{eq:Fsol}  F=\sum_{n=0}^{\infty}\frac{A_n+B_nr^2}{r^{2n+1}}r^nK_n \;, $  where $K_n$ is a spherical function of order $n$. How to show that this holds (if it does)?","What is a general solution of biharmonic equation $\nabla^2\nabla^2f=0$ in spherical coordinates? According to Wikipedia any solution of Laplace equation is also a solution of biharmonic equation, but the vice versa is not always true. Therefore I assume there exists a more general solution. According to Oberbeck, A., Crelle, 62, 1876 , a general solution of biharmonic equation in spherical coordinates is $\label{eq:Fsol}  F=\sum_{n=0}^{\infty}\frac{A_n+B_nr^2}{r^{2n+1}}r^nK_n \;, $  where $K_n$ is a spherical function of order $n$. How to show that this holds (if it does)?",,"['analysis', 'ordinary-differential-equations']"
58,Differential equations and exponential growth,Differential equations and exponential growth,,"I was reading about differential equations and got stuck in a small detail that I can't make peace with. If a population doubles every unit of time, I would write $$ \frac{dP}{dt}=2P $$ which by separation of variables would yield $$ P=P(0)e^{2t} $$ But I also know intuitively that I should have $$ P = P(0)e^{(ln2)t} = P(0)2^{t}$$ which suggests the factor in the differential equation should be $\ln2$ instead of 2. What's wrong with my logic? Thanks! EDIT: This is the part of the textbook that confused me.  Doesn't it confuse discrete and continuous cases as well?","I was reading about differential equations and got stuck in a small detail that I can't make peace with. If a population doubles every unit of time, I would write $$ \frac{dP}{dt}=2P $$ which by separation of variables would yield $$ P=P(0)e^{2t} $$ But I also know intuitively that I should have $$ P = P(0)e^{(ln2)t} = P(0)2^{t}$$ which suggests the factor in the differential equation should be $\ln2$ instead of 2. What's wrong with my logic? Thanks! EDIT: This is the part of the textbook that confused me.  Doesn't it confuse discrete and continuous cases as well?",,['ordinary-differential-equations']
59,$f ≠ 0$ everywhere if $f : \mathbb{R}\to\mathbb{R}$ and $f = f'$ and $f(0) = 1$?,everywhere if  and  and ?,f ≠ 0 f : \mathbb{R}\to\mathbb{R} f = f' f(0) = 1,"Let $f:\mathbb R\rightarrow \mathbb R$ be a differentiable function, and suppose $f=f'$ and $f(0)=1$ . Then prove $f(x)\neq 0$ for all $x\in \mathbb R$ The way I solve this is kind of strange. I first suppose there is a closed interval $[0, a]$ on the real line. Since $f$ is differentiable, there exists a $x_0\in[0, a] $ such that $\frac{f(a)-f(0)}{a}=f'(x_0)=f(x_0)$ . Thus when $f(a)=1$ , $f(x_0)=0$ . Then I just let $f(a)=1$ and try to find some contradictions. Since $f(a)=f(0)=1$ , by Rolle thorem, there should exist a $x_1\in[0, a]$ such that $f'(x_1)=f(x_1)=0$ and this $f(x_1)=0$ is supposed to be the maximum or minimum on the interval $[0,a]$ . Then apply MVT again on the interval $[0, x_0]\implies\frac{f(x_0)-f(0)}{x_0}=\frac{-1}{x_0}=f'(x_2)=f'(x_2)\implies-1=x_0f(x_2)\implies f(x_2)<0$ for a point $x_2\in[0, x_0]$ . The existence of $x_2$ make sure that $f(x_1)=0$ is not the minimum on $[0, a]$ and since $0<f(0)=f(a)=1\implies f(x_1)$ is not the maximum on the $[0, a]$ . Also if there exists other points $\beta$ , for example, and $f(\beta)$ is the maximum of $[0,a]$ this implies that $f'(\beta)=0=f(\beta)$ the contradiction remains. Thus $f(a)\neq 1\implies f(x_0)\neq 0$ . Since $a$ is an arbitrary real number, this means $f$ has no zero point on $[0,\infty]$ . By the similar idea $f$ doesn't have zero point on $[-\infty, 0]. Is this a correct idea? And any shorter version of proof？ Thanks in advance!","Let be a differentiable function, and suppose and . Then prove for all The way I solve this is kind of strange. I first suppose there is a closed interval on the real line. Since is differentiable, there exists a such that . Thus when , . Then I just let and try to find some contradictions. Since , by Rolle thorem, there should exist a such that and this is supposed to be the maximum or minimum on the interval . Then apply MVT again on the interval for a point . The existence of make sure that is not the minimum on and since is not the maximum on the . Also if there exists other points , for example, and is the maximum of this implies that the contradiction remains. Thus . Since is an arbitrary real number, this means has no zero point on . By the similar idea doesn't have zero point on $[-\infty, 0]. Is this a correct idea? And any shorter version of proof？ Thanks in advance!","f:\mathbb R\rightarrow \mathbb R f=f' f(0)=1 f(x)\neq 0 x\in \mathbb R [0, a] f x_0\in[0, a]  \frac{f(a)-f(0)}{a}=f'(x_0)=f(x_0) f(a)=1 f(x_0)=0 f(a)=1 f(a)=f(0)=1 x_1\in[0, a] f'(x_1)=f(x_1)=0 f(x_1)=0 [0,a] [0, x_0]\implies\frac{f(x_0)-f(0)}{x_0}=\frac{-1}{x_0}=f'(x_2)=f'(x_2)\implies-1=x_0f(x_2)\implies f(x_2)<0 x_2\in[0, x_0] x_2 f(x_1)=0 [0, a] 0<f(0)=f(a)=1\implies f(x_1) [0, a] \beta f(\beta) [0,a] f'(\beta)=0=f(\beta) f(a)\neq 1\implies f(x_0)\neq 0 a f [0,\infty] f","['real-analysis', 'ordinary-differential-equations', 'derivatives', 'solution-verification']"
60,Second order homogeneous differential equation with non-constant coefficients,Second order homogeneous differential equation with non-constant coefficients,,"How can I solve a 2nd order differential equation with non-constant coefficients like the following? $$ty''-(t+1)y'+y=0$$ If I'm not wrong, I have only seen methods (apart from the reduction of order method) for finding a solution when the coefficients are constant. How can I do this?","How can I solve a 2nd order differential equation with non-constant coefficients like the following? $$ty''-(t+1)y'+y=0$$ If I'm not wrong, I have only seen methods (apart from the reduction of order method) for finding a solution when the coefficients are constant. How can I do this?",,['ordinary-differential-equations']
61,How do I solve $y' = \sin(x - y)$?,How do I solve ?,y' = \sin(x - y),"How can I solve the differential equation: $$y'=\sin(x-y)$$ Could I do this? $$\frac{dy}{dx}= \sin x \cos y - \sin y \cos x$$ But, how would I continue?","How can I solve the differential equation: $$y'=\sin(x-y)$$ Could I do this? $$\frac{dy}{dx}= \sin x \cos y - \sin y \cos x$$ But, how would I continue?",,['ordinary-differential-equations']
62,Solve the ODE $yy'' + (y')^2 = 0$,Solve the ODE,yy'' + (y')^2 = 0,"I am asked in a book to solve the following ODE: $$y\dfrac{d^2y}{dt^2} + \left(\dfrac{dy}{dt}\right)^2 = 0$$ One solution for the ODE above is $y = 0$. I will use the following substitution: $$v = \dfrac{dy}{dt}$$ Therefore, $y''$ will become, in terms of $v$: $$\dfrac{d^2y}{dt^2} = \dfrac{dv}{dt} = \left( \dfrac{dv}{dy} \right)\left( \dfrac{dy}{dt} \right) = v\dfrac{dv}{dy}$$ (Explanation for the substitution: in the original equation, that has the form $y''=f(y,y')$, the independent variable $t$ doesn't appear explicitly, only through the dependent variable $y$. Therefore, if we let $v=\dfrac{dy}{dt}$, we can obtain a differential equation in terms of $y$ and $v$ only. Thus, $v$ can be treated as a function of $y$ only, $y$ being treated as the independent variable.) Substituting $v$ for $\dfrac{dy}{dt}$ and $v\dfrac{dv}{dy}$ for $\dfrac{d^2y}{dt^2}$ in the original equation gives the following ODE: $$yv\dfrac{dv}{dy} + v^2 = 0$$ One solution to the ODE above is $v(y) = 0$, which gives $y = k$ (where $k$ is a constant). For $v \neq 0$, I will solve the ODE above for $v$ (by separating variables): $$\dfrac{dv}{v}=-\dfrac{dy}{y}$$ Integrating both sides: $$\ln|v| = -\ln|y| + c$$ $$|v| = e^{-\ln|y| + c}$$ $$v = \pm e^{-\ln|y| + c} = \pm \dfrac{e^{c}}{e^{ln|y|}}$$ $$v = \frac{c_1}{|y|} \ \ \ \ \text{where } c_1 = \pm e^{c}$$ Substituting back $\dfrac{dy}{dt}$ for $v$: $$\dfrac{dy}{dt} = \dfrac{c_1}{|y|}$$ $$|y|dy = c_1dt$$ Integrating both sides (remembering that $\int |y| dy = \dfrac{y|y|}{2}$) gives the solution to the original ODE: $$\dfrac{y|y|}{2} = c_1t + c_2$$ But the answer given by the book in the answer section is slightly different: $y^2 = c_1t + c_2$, which would be equal to my answer if $y$ if I removed the absolute value bars. Is my solution correct? Update : In the step of solving $\dfrac{dv}{v}=-\dfrac{dy}{y}$, if I integrated it like this: $\ln v = -\ln y + c$ (instead of $\ln|v| = -\ln|y| + c$), I would obtain the same answer as the book ($y^2 = c_1t + c_2$, without the absolute value signs). But it seems there is no justification for removing the absolute value signs in $\ln|v| = -\ln|y| + c$ (there would be if $y$ were always non-negative). Is my reasoning wrong?","I am asked in a book to solve the following ODE: $$y\dfrac{d^2y}{dt^2} + \left(\dfrac{dy}{dt}\right)^2 = 0$$ One solution for the ODE above is $y = 0$. I will use the following substitution: $$v = \dfrac{dy}{dt}$$ Therefore, $y''$ will become, in terms of $v$: $$\dfrac{d^2y}{dt^2} = \dfrac{dv}{dt} = \left( \dfrac{dv}{dy} \right)\left( \dfrac{dy}{dt} \right) = v\dfrac{dv}{dy}$$ (Explanation for the substitution: in the original equation, that has the form $y''=f(y,y')$, the independent variable $t$ doesn't appear explicitly, only through the dependent variable $y$. Therefore, if we let $v=\dfrac{dy}{dt}$, we can obtain a differential equation in terms of $y$ and $v$ only. Thus, $v$ can be treated as a function of $y$ only, $y$ being treated as the independent variable.) Substituting $v$ for $\dfrac{dy}{dt}$ and $v\dfrac{dv}{dy}$ for $\dfrac{d^2y}{dt^2}$ in the original equation gives the following ODE: $$yv\dfrac{dv}{dy} + v^2 = 0$$ One solution to the ODE above is $v(y) = 0$, which gives $y = k$ (where $k$ is a constant). For $v \neq 0$, I will solve the ODE above for $v$ (by separating variables): $$\dfrac{dv}{v}=-\dfrac{dy}{y}$$ Integrating both sides: $$\ln|v| = -\ln|y| + c$$ $$|v| = e^{-\ln|y| + c}$$ $$v = \pm e^{-\ln|y| + c} = \pm \dfrac{e^{c}}{e^{ln|y|}}$$ $$v = \frac{c_1}{|y|} \ \ \ \ \text{where } c_1 = \pm e^{c}$$ Substituting back $\dfrac{dy}{dt}$ for $v$: $$\dfrac{dy}{dt} = \dfrac{c_1}{|y|}$$ $$|y|dy = c_1dt$$ Integrating both sides (remembering that $\int |y| dy = \dfrac{y|y|}{2}$) gives the solution to the original ODE: $$\dfrac{y|y|}{2} = c_1t + c_2$$ But the answer given by the book in the answer section is slightly different: $y^2 = c_1t + c_2$, which would be equal to my answer if $y$ if I removed the absolute value bars. Is my solution correct? Update : In the step of solving $\dfrac{dv}{v}=-\dfrac{dy}{y}$, if I integrated it like this: $\ln v = -\ln y + c$ (instead of $\ln|v| = -\ln|y| + c$), I would obtain the same answer as the book ($y^2 = c_1t + c_2$, without the absolute value signs). But it seems there is no justification for removing the absolute value signs in $\ln|v| = -\ln|y| + c$ (there would be if $y$ were always non-negative). Is my reasoning wrong?",,['ordinary-differential-equations']
63,Solution to the differential equation $\frac{f}{f^\prime}=\frac{f^\prime}{f^{\prime\prime}}$,Solution to the differential equation,\frac{f}{f^\prime}=\frac{f^\prime}{f^{\prime\prime}},I am wondering if anyone could please post the solution to the following differential equation for the function $f(x)$: $$\frac{f}{f^\prime}=\frac{f^\prime}{f^{\prime\prime}}$$ Thanks!,I am wondering if anyone could please post the solution to the following differential equation for the function $f(x)$: $$\frac{f}{f^\prime}=\frac{f^\prime}{f^{\prime\prime}}$$ Thanks!,,['ordinary-differential-equations']
64,"Proving $\ddot x = w-2x+x^2$, where $w\ge0$, conserves energy","Proving , where , conserves energy",\ddot x = w-2x+x^2 w\ge0,"Take $\ddot x = w-2x+x^2$ , where $w\ge0$ . Show that the evolution of $x$ conserves a form of the energy and identify the potential function, sketching the phase portrait for $w=0$ . I am not too sure what is meant by the question. I know that to prove energy is conserved, you need to show that the derivative equals $0$ . If we take $V = w-2x+x^2$ , then $\dot V = -2\dot x+2x\dot x = 2\dot x(x-1) = 0$ , but how do we prove this equals $0$ , if this is what we need to show? I also thought it may be helpful to rewrite the equation as the system: $\dot x=y$ , $\dot y = w-2x+x^2$ , which would give $\dot V = 2y(x-1)$ . Any help would be hugely appreciated, thank you!","Take , where . Show that the evolution of conserves a form of the energy and identify the potential function, sketching the phase portrait for . I am not too sure what is meant by the question. I know that to prove energy is conserved, you need to show that the derivative equals . If we take , then , but how do we prove this equals , if this is what we need to show? I also thought it may be helpful to rewrite the equation as the system: , , which would give . Any help would be hugely appreciated, thank you!",\ddot x = w-2x+x^2 w\ge0 x w=0 0 V = w-2x+x^2 \dot V = -2\dot x+2x\dot x = 2\dot x(x-1) = 0 0 \dot x=y \dot y = w-2x+x^2 \dot V = 2y(x-1),"['ordinary-differential-equations', 'functions', 'derivatives', 'partial-differential-equations', 'nonlinear-system']"
65,Am I using the right substitution to solve this differential equation?,Am I using the right substitution to solve this differential equation?,,"I want to find the general solution of the following: $$\frac{dy}{dx} = \frac{y}{x+y+2}$$ I substitute $u = x + y+2$ and got the following $du/dx = 1 + dy/dx$. Then I got an expression as such: $$\frac{du}{dx} = 2 - \frac{x+2}{u}$$ But from this expression,  I cannot seem to separate it further. Using an integrating factor also doesn't seem to work. It also doesn't have the form of a bernoulli's equation, due to the $+2$. How should I further proceed?","I want to find the general solution of the following: $$\frac{dy}{dx} = \frac{y}{x+y+2}$$ I substitute $u = x + y+2$ and got the following $du/dx = 1 + dy/dx$. Then I got an expression as such: $$\frac{du}{dx} = 2 - \frac{x+2}{u}$$ But from this expression,  I cannot seem to separate it further. Using an integrating factor also doesn't seem to work. It also doesn't have the form of a bernoulli's equation, due to the $+2$. How should I further proceed?",,['ordinary-differential-equations']
66,"How does an integrating factor geometrically ""uncurl"" a vector field?","How does an integrating factor geometrically ""uncurl"" a vector field?",,"We know that certain 1-D forms $m(x,y,z)\,dx + n(x,y,z)\,dy + p(x,y,z)\,dz$ admit integrating factors as we teach in basic differential equations. How does the integrating factor geometrically turn this ""unlayered"" situation into the ""layered situation"" of level surfaces with the new vector field being the gradient field of of a suitably differentiable function $f(x,y,z)$ of $3$ variables.  I know that the general question of integrability is very complicated. Just a good intuitive example would suffice. I know how an integrating factor which is essentially a continuous ""reweighting"" of the vector field can make the required the field ""exact"" algebraically . What I want to get a feel for is ""What is happening geometrically ?"". How is a vector field which is wandering around and many times trying to form closed loops and does not ""layer"" transformed geometrically into a vector field which is inwardly or outwardly directed by the ""layered"" set of level surfaces when before there were no natural surfaces to assign to the vector field?","We know that certain 1-D forms $m(x,y,z)\,dx + n(x,y,z)\,dy + p(x,y,z)\,dz$ admit integrating factors as we teach in basic differential equations. How does the integrating factor geometrically turn this ""unlayered"" situation into the ""layered situation"" of level surfaces with the new vector field being the gradient field of of a suitably differentiable function $f(x,y,z)$ of $3$ variables.  I know that the general question of integrability is very complicated. Just a good intuitive example would suffice. I know how an integrating factor which is essentially a continuous ""reweighting"" of the vector field can make the required the field ""exact"" algebraically . What I want to get a feel for is ""What is happening geometrically ?"". How is a vector field which is wandering around and many times trying to form closed loops and does not ""layer"" transformed geometrically into a vector field which is inwardly or outwardly directed by the ""layered"" set of level surfaces when before there were no natural surfaces to assign to the vector field?",,['ordinary-differential-equations']
67,Poincaré-Bendixon show periodic solutions.,Poincaré-Bendixon show periodic solutions.,,"Show that the system $x^{'}=x-y-x^{3}$,$y^{'}=x+y-y^{3}$ has a periodic solution. I converted to polar: $r r^{'}=x^{'}x+y^{'}y.$ Thus $r r^{'}=x^{2}-x^{4}+y^{2}-y^{4}.$ Collecting squares: $rr^{'}=x^{2}+y^{2}-(x^{2}+y^{2})^{2} +2x^{2}y^{2}.$ Substitute: $r^{'}=r-r^{3}(1-(\sin^{2}(2\theta))/2)$ $r^{'}=r-r^{3}(1+\cos^{2}(2\theta))/2.$ Setting $r^{'}=0$ yields $r^{2}=2/(1+\cos^{2}(2\theta).$ Thus $r_{max}= 2^{(1/2)}$ and $r_{min}=1.$ I suck at trig and this doesn't look right. Clearly $(0,0)$ is the only fixed point, so I have a bounded region. I just don't think I have found correct values for $r_{min}$ and $r_{max}$ to find my limit cycle.","Show that the system $x^{'}=x-y-x^{3}$,$y^{'}=x+y-y^{3}$ has a periodic solution. I converted to polar: $r r^{'}=x^{'}x+y^{'}y.$ Thus $r r^{'}=x^{2}-x^{4}+y^{2}-y^{4}.$ Collecting squares: $rr^{'}=x^{2}+y^{2}-(x^{2}+y^{2})^{2} +2x^{2}y^{2}.$ Substitute: $r^{'}=r-r^{3}(1-(\sin^{2}(2\theta))/2)$ $r^{'}=r-r^{3}(1+\cos^{2}(2\theta))/2.$ Setting $r^{'}=0$ yields $r^{2}=2/(1+\cos^{2}(2\theta).$ Thus $r_{max}= 2^{(1/2)}$ and $r_{min}=1.$ I suck at trig and this doesn't look right. Clearly $(0,0)$ is the only fixed point, so I have a bounded region. I just don't think I have found correct values for $r_{min}$ and $r_{max}$ to find my limit cycle.",,"['ordinary-differential-equations', 'dynamical-systems']"
68,Change of variables in partial derivative,Change of variables in partial derivative,,"I am stuck on a simple exercise in quantum mechanics because I can't figure out how to modify a partial derivative under a change in variables. If I have a Hamiltonian in two variables $x_1$ and $x_2$, and I introduce two new variables $u = x_1 - x_2$ and $v = x_1+x_2$, how to I change the partial derivatives $\frac{\partial^2}{\partial x_1^2}$ and $\frac{\partial^2}{\partial x_2^2}$ to be expressed in terms of $u$ and $v$? I have the following Hamiltonian: $$ H = - \frac{\hbar^2}{2m} \frac{\partial^2}{\partial x_1^2} -  \frac{\hbar^2}{2m} \frac{\partial^2}{\partial x_2^2} + \frac{1}{2}m\omega^2 x_1^2 + \frac{1}{2}m\omega^2 x_1^2 + \frac{1}{2}\epsilon(x_1-x_2)^2  $$ I tried a change of variables $u = x_1-x_2$ and $v  =x_1+x_2 $. The potential part of the Hamiltonian becomes $$ \frac{1}{4}m\omega^2 (u^2+v^2) + \frac{1}{2}\epsilon u^2 $$ My question is what happens to the kinetic part, $- \frac{\hbar^2}{2m} \frac{\partial^2}{\partial x_1^2} -  \frac{\hbar^2}{2m} \frac{\partial^2}{\partial x_2^2}$? How do these derivative change under this transformation?","I am stuck on a simple exercise in quantum mechanics because I can't figure out how to modify a partial derivative under a change in variables. If I have a Hamiltonian in two variables $x_1$ and $x_2$, and I introduce two new variables $u = x_1 - x_2$ and $v = x_1+x_2$, how to I change the partial derivatives $\frac{\partial^2}{\partial x_1^2}$ and $\frac{\partial^2}{\partial x_2^2}$ to be expressed in terms of $u$ and $v$? I have the following Hamiltonian: $$ H = - \frac{\hbar^2}{2m} \frac{\partial^2}{\partial x_1^2} -  \frac{\hbar^2}{2m} \frac{\partial^2}{\partial x_2^2} + \frac{1}{2}m\omega^2 x_1^2 + \frac{1}{2}m\omega^2 x_1^2 + \frac{1}{2}\epsilon(x_1-x_2)^2  $$ I tried a change of variables $u = x_1-x_2$ and $v  =x_1+x_2 $. The potential part of the Hamiltonian becomes $$ \frac{1}{4}m\omega^2 (u^2+v^2) + \frac{1}{2}\epsilon u^2 $$ My question is what happens to the kinetic part, $- \frac{\hbar^2}{2m} \frac{\partial^2}{\partial x_1^2} -  \frac{\hbar^2}{2m} \frac{\partial^2}{\partial x_2^2}$? How do these derivative change under this transformation?",,"['ordinary-differential-equations', 'partial-derivative', 'change-of-variable']"
69,How was the Runge-Kutta method derived?,How was the Runge-Kutta method derived?,,"By K values, I mean the values described here: https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods#Explicit_Runge.E2.80.93Kutta_methods I know how the K values in the Runge-Kutta method can be proven to be correct, by comparing their taylor expansion with the taylor expansion of the function to be approximated, but how were they originally figured out? I think I understand the Runge-Kutta method derivation when you have the derivative in terms of one variable f'(t). It seems to be a direct consequence of Simpson's rule and its higher order equivalents. But when it is some form of first order differential equation (i.e. f'(t, y(t))), I am still lost. Is there an equivalent of Simpson's rule for multivariable functions?","By K values, I mean the values described here: https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods#Explicit_Runge.E2.80.93Kutta_methods I know how the K values in the Runge-Kutta method can be proven to be correct, by comparing their taylor expansion with the taylor expansion of the function to be approximated, but how were they originally figured out? I think I understand the Runge-Kutta method derivation when you have the derivative in terms of one variable f'(t). It seems to be a direct consequence of Simpson's rule and its higher order equivalents. But when it is some form of first order differential equation (i.e. f'(t, y(t))), I am still lost. Is there an equivalent of Simpson's rule for multivariable functions?",,"['ordinary-differential-equations', 'numerical-methods', 'runge-kutta-methods']"
70,Why does the sum of two linearly independent solutions of a second order homogeneous ODE give a general solution?,Why does the sum of two linearly independent solutions of a second order homogeneous ODE give a general solution?,,"The following is a short extract from the book I am reading: If given a Homogeneous ODE:    $$\frac{\mathrm{d}^2 y}{\mathrm{d}x^2}+5\frac{\mathrm{d} y}{\mathrm{d}x}+4y=0\tag{1}$$   Letting    $$D=\frac{\mathrm{d}}{\mathrm{d}x}$$ then $(1)$ becomes    $$D^2 y + 5Dy + 4y=(D^2+5D+4)y$$   $$\implies\color{blue}{(D+1)(D+4)y=0}\tag{2}$$   $$\implies (D+1)y=0 \space\space\text{or}\space\space (D+4)y=0$$ which has solutions $$y=Ae^{-x}\space\space\text{or}\space\space y=Be^{-4x}\tag{3}$$ respectively, where $A$ and $B$ are both constants. Now if $(D+4)y=0$, then $$(D+1)(D+4)y=(D+1)\cdot 0=0$$   so any solution of $(D + 4)y = 0$ is a solution of the differential equation $(1)$ or $(2)$. Similarly, any solution of $(D + 1)y = 0$ is a solution of $(1)$ or $(2)$. $\color{red}{\text{Since the two solutions (3) are linearly independent, a linear combination}}$ $\color{red}{\text{of them contains two arbitrary constants and so is the general solution.}}$ Thus $$y=Ae^{-x}+Be^{-4x}$$ is the general solution of $(1)$ or $(2)$. The part I don't understand in this extract is marked in $\color{red}{\mathrm{red}}$. Firstly; How do we know that the two solutions: $y=Ae^{-x}\space\text{and}\space y=Be^{-4x}$ are linearly independent? Secondly; Why does a linear combination of linearly independent solutions give the general solution. Or, put in another way, I know that $y=Ae^{-x}\space\text{or}\space y=Be^{-4x}$ are both solutions. But why is their sum a solution: $y=Ae^{-x}+Be^{-4x}$?","The following is a short extract from the book I am reading: If given a Homogeneous ODE:    $$\frac{\mathrm{d}^2 y}{\mathrm{d}x^2}+5\frac{\mathrm{d} y}{\mathrm{d}x}+4y=0\tag{1}$$   Letting    $$D=\frac{\mathrm{d}}{\mathrm{d}x}$$ then $(1)$ becomes    $$D^2 y + 5Dy + 4y=(D^2+5D+4)y$$   $$\implies\color{blue}{(D+1)(D+4)y=0}\tag{2}$$   $$\implies (D+1)y=0 \space\space\text{or}\space\space (D+4)y=0$$ which has solutions $$y=Ae^{-x}\space\space\text{or}\space\space y=Be^{-4x}\tag{3}$$ respectively, where $A$ and $B$ are both constants. Now if $(D+4)y=0$, then $$(D+1)(D+4)y=(D+1)\cdot 0=0$$   so any solution of $(D + 4)y = 0$ is a solution of the differential equation $(1)$ or $(2)$. Similarly, any solution of $(D + 1)y = 0$ is a solution of $(1)$ or $(2)$. $\color{red}{\text{Since the two solutions (3) are linearly independent, a linear combination}}$ $\color{red}{\text{of them contains two arbitrary constants and so is the general solution.}}$ Thus $$y=Ae^{-x}+Be^{-4x}$$ is the general solution of $(1)$ or $(2)$. The part I don't understand in this extract is marked in $\color{red}{\mathrm{red}}$. Firstly; How do we know that the two solutions: $y=Ae^{-x}\space\text{and}\space y=Be^{-4x}$ are linearly independent? Secondly; Why does a linear combination of linearly independent solutions give the general solution. Or, put in another way, I know that $y=Ae^{-x}\space\text{or}\space y=Be^{-4x}$ are both solutions. But why is their sum a solution: $y=Ae^{-x}+Be^{-4x}$?",,['ordinary-differential-equations']
71,What is a particular and a homogenous solution of a differential equation?,What is a particular and a homogenous solution of a differential equation?,,"When solving linear nonhomogeneous equations, we deal with two types of solutions: particular homogeneous Why do we have these two types of solutions for differential equations? What does each of them represent?","When solving linear nonhomogeneous equations, we deal with two types of solutions: particular homogeneous Why do we have these two types of solutions for differential equations? What does each of them represent?",,"['ordinary-differential-equations', 'motivation']"
72,How to solve the following differential equation: $(2x^3y^2-y)dx+(2x^2y^3-x)dy=0$?,How to solve the following differential equation: ?,(2x^3y^2-y)dx+(2x^2y^3-x)dy=0,"I'd love your help with solving this following differential equation: $$(2x^3y^2-y)dx+(2x^2y^3-x)dy=0.$$ I tried to use check if this is an exact equation and find a integration, but it didn't work. Then I tried to divide the equation by some factor and to use some kind of assignment such as $z=\frac{y}{x}$, but it didn't work for me either. Any suggestions? ($y$ is a function of $x$) Even though Julian answer is great,  I wonder if there's a solution without an integration depends both on x and y. Thanks a lot!","I'd love your help with solving this following differential equation: $$(2x^3y^2-y)dx+(2x^2y^3-x)dy=0.$$ I tried to use check if this is an exact equation and find a integration, but it didn't work. Then I tried to divide the equation by some factor and to use some kind of assignment such as $z=\frac{y}{x}$, but it didn't work for me either. Any suggestions? ($y$ is a function of $x$) Even though Julian answer is great,  I wonder if there's a solution without an integration depends both on x and y. Thanks a lot!",,['ordinary-differential-equations']
73,Solve $(y^2+2x^2y)dx+(2x^3-xy)dy=0$,Solve,(y^2+2x^2y)dx+(2x^3-xy)dy=0,"Solve $(y^2+2x^2y)dx+(2x^3-xy)dy=0$ My attempt $Mdx + Ndy$ form $M = (y^2+2x^2y)$, $N =(2x^3-xy) , M_y=2y+2x^2, N_x=6x^2-y$ I am not getting in of standard forms here... Another attempt: $2x^2 d(xy)+y(ydx-xdy)=0$, $2d(xy)=yd(y/x)$ Any clue pls???","Solve $(y^2+2x^2y)dx+(2x^3-xy)dy=0$ My attempt $Mdx + Ndy$ form $M = (y^2+2x^2y)$, $N =(2x^3-xy) , M_y=2y+2x^2, N_x=6x^2-y$ I am not getting in of standard forms here... Another attempt: $2x^2 d(xy)+y(ydx-xdy)=0$, $2d(xy)=yd(y/x)$ Any clue pls???",,['ordinary-differential-equations']
74,What is $\frac d{dx} (x!)$?,What is ?,\frac d{dx} (x!),"Background: I was investigating the factorial function due to their importance in the binomial expansion. They play an important role in calculating the coefficients of the terms of the binomial expansion. Question: What is the derivative of the function $y = x!$ ? My (failed) attempt: I first plotted some points on a graph paper and tried to join them. I tried smoothing out the graph and then I tried to connect them with straight lines. Both techniques seemed to not work and produced something I understood was wrong as the derivative had gone beserk. Then I tried it on Desmos: https://www.desmos.com/calculator/6opi7buw86 . The graph made me want to publish an official complaint against Desmos for intentionally scamming me but thought better of it. I googled up $x!$ and learnt about the $\Gamma$ function which produced this unexplainable graph. I understand the basics of it (or so I believe) but I am not confident that I can continue this quest. Reason for posting this question: What is this $\Gamma$ function? What is the meaning of $\frac 12!$ ? How does it have a value, as well as all the other non-integer values? Especially the negative values which makes no sense to my incompetent brain. Can I use the $\Gamma$ function to find an equation for the derivative of $x!$ (or $\Gamma (x)$ for this matter)? I would prefer explanations as simple and elaborate as possible. Please excuse me for my apparent lack of knowledge. Thanks in advance! P.S. Was this question already posted? I could not find this question. Could anyone direct me to such a question (if it exists)? And please try to avoid Youtube links as I learned that they are not always true (specially Numberphile).","Background: I was investigating the factorial function due to their importance in the binomial expansion. They play an important role in calculating the coefficients of the terms of the binomial expansion. Question: What is the derivative of the function $y = x!$ ? My (failed) attempt: I first plotted some points on a graph paper and tried to join them. I tried smoothing out the graph and then I tried to connect them with straight lines. Both techniques seemed to not work and produced something I understood was wrong as the derivative had gone beserk. Then I tried it on Desmos: https://www.desmos.com/calculator/6opi7buw86 . The graph made me want to publish an official complaint against Desmos for intentionally scamming me but thought better of it. I googled up $x!$ and learnt about the $\Gamma$ function which produced this unexplainable graph. I understand the basics of it (or so I believe) but I am not confident that I can continue this quest. Reason for posting this question: What is this $\Gamma$ function? What is the meaning of $\frac 12!$ ? How does it have a value, as well as all the other non-integer values? Especially the negative values which makes no sense to my incompetent brain. Can I use the $\Gamma$ function to find an equation for the derivative of $x!$ (or $\Gamma (x)$ for this matter)? I would prefer explanations as simple and elaborate as possible. Please excuse me for my apparent lack of knowledge. Thanks in advance! P.S. Was this question already posted? I could not find this question. Could anyone direct me to such a question (if it exists)? And please try to avoid Youtube links as I learned that they are not always true (specially Numberphile).",,"['ordinary-differential-equations', 'soft-question', 'factorial']"
75,Special Differential Equation,Special Differential Equation,,I ended up with a differential equation that looks like this: $$\frac{d^2y}{dx^2} + \frac 1 x \frac{dy}{dx} - \frac{ay}{x^2} + \left(b  -\frac  c x - e x \right )y = 0.$$ I tried with Mathematica. But could not get the sensible answer. May you help me out how to solve it or give me some references that I can go over please? Thanks.,I ended up with a differential equation that looks like this: $$\frac{d^2y}{dx^2} + \frac 1 x \frac{dy}{dx} - \frac{ay}{x^2} + \left(b  -\frac  c x - e x \right )y = 0.$$ I tried with Mathematica. But could not get the sensible answer. May you help me out how to solve it or give me some references that I can go over please? Thanks.,,['ordinary-differential-equations']
76,Determine the *interval* in which the solution is defined?,Determine the *interval* in which the solution is defined?,,"The ODE: $y' = (1-2x)y^2$ Initial Value: $y(0) = -1/6$ I've solved the particular solution, which is $1/(x^2-x-6)$. I don't understand what they mean about the solution is defined, because when I graph $1/(x^2-x-6)$, it's only discontinuous at $x = -2, 3$. What does ""Determine the interval in which the solution is defined"" mean?","The ODE: $y' = (1-2x)y^2$ Initial Value: $y(0) = -1/6$ I've solved the particular solution, which is $1/(x^2-x-6)$. I don't understand what they mean about the solution is defined, because when I graph $1/(x^2-x-6)$, it's only discontinuous at $x = -2, 3$. What does ""Determine the interval in which the solution is defined"" mean?",,['ordinary-differential-equations']
77,Harmonic functions on $S^2$,Harmonic functions on,S^2,"Consider the sphere $S^2 = \lbrace (x,y,z) :\ x^2 + y^2 + z^2 = 1 \rbrace$. This is a smooth manifold in $\mathbb{R}^3$, and for a given point $s \in S^2$, one can consider its coordinate neighborhood. There are many ways to put a smooth structure on $S^2$, but all require at least two coordinate neighborhoods. One simple way is to make use of the local coordinates $\theta, \phi$ (azimuthal angle and inclination angle). For $s \in U = S^2 \setminus {(0,0,1), (0,0,-1)}$, define a coordinate map by $ g : (0,2\pi) \times (0,\pi) \rightarrow U $ with formula given by $ g(\theta, \phi) = (\sin(\phi) \cos(\theta), \sin(\phi) \sin(\theta), \cos(\phi)) $ This map is one-to-one, onto, and bicontinuous, so it is a homeomorphism. One can construct a similar map onto $V = S^2 \setminus {(1,0,0),(-1,0,0)}$, so the whole sphere is covered. The fact that this defines a differentiable structure is easy to work out. The question I want to ask is, are there any non-constant solutions to the equation $ \frac{\partial^2 f}{\partial \theta^2} + \frac{\partial^2 f}{\partial \phi^2} = 0 $ where $f : S^2 \rightarrow R$ is a smooth function. Furthermore, what sorts of boundary conditions are necessary in order to ensure uniqueness.","Consider the sphere $S^2 = \lbrace (x,y,z) :\ x^2 + y^2 + z^2 = 1 \rbrace$. This is a smooth manifold in $\mathbb{R}^3$, and for a given point $s \in S^2$, one can consider its coordinate neighborhood. There are many ways to put a smooth structure on $S^2$, but all require at least two coordinate neighborhoods. One simple way is to make use of the local coordinates $\theta, \phi$ (azimuthal angle and inclination angle). For $s \in U = S^2 \setminus {(0,0,1), (0,0,-1)}$, define a coordinate map by $ g : (0,2\pi) \times (0,\pi) \rightarrow U $ with formula given by $ g(\theta, \phi) = (\sin(\phi) \cos(\theta), \sin(\phi) \sin(\theta), \cos(\phi)) $ This map is one-to-one, onto, and bicontinuous, so it is a homeomorphism. One can construct a similar map onto $V = S^2 \setminus {(1,0,0),(-1,0,0)}$, so the whole sphere is covered. The fact that this defines a differentiable structure is easy to work out. The question I want to ask is, are there any non-constant solutions to the equation $ \frac{\partial^2 f}{\partial \theta^2} + \frac{\partial^2 f}{\partial \phi^2} = 0 $ where $f : S^2 \rightarrow R$ is a smooth function. Furthermore, what sorts of boundary conditions are necessary in order to ensure uniqueness.",,"['real-analysis', 'ordinary-differential-equations', 'differential-geometry']"
78,solve $y'=ay+b$ [duplicate],solve  [duplicate],y'=ay+b,This question already has answers here : Tricky Separable Differential Equation (3 answers) Closed 9 years ago . I have this differential equation which I want to solve $\displaystyle\frac{dT_i}{dt}=\frac{1}{RC}(T_a-T_i)+\frac{1}{C} \Phi_h$ I know it is in the form $y'=ay+b$ But how can I solve it ?,This question already has answers here : Tricky Separable Differential Equation (3 answers) Closed 9 years ago . I have this differential equation which I want to solve $\displaystyle\frac{dT_i}{dt}=\frac{1}{RC}(T_a-T_i)+\frac{1}{C} \Phi_h$ I know it is in the form $y'=ay+b$ But how can I solve it ?,,['ordinary-differential-equations']
79,"If an IVP does not enjoy uniqueness, then it possesses infinitely many solutions","If an IVP does not enjoy uniqueness, then it possesses infinitely many solutions",,"I am trying to prove that when an IVP in ODEs does not enjoy uniqueness, then it has infinitely many different solutions. I know that when the Lipschitz condition is satisfied, then there is a unique solution. I guess, I am trying to show that if the IVP possesses two solutions, then we can create infinitely many solutions using these two. However, I am slightly lost as where to start the proof.","I am trying to prove that when an IVP in ODEs does not enjoy uniqueness, then it has infinitely many different solutions. I know that when the Lipschitz condition is satisfied, then there is a unique solution. I guess, I am trying to show that if the IVP possesses two solutions, then we can create infinitely many solutions using these two. However, I am slightly lost as where to start the proof.",,"['real-analysis', 'analysis', 'ordinary-differential-equations', 'lipschitz-functions']"
80,Differential Equation Question $y'=\frac{1}{\cos y-x}$,Differential Equation Question,y'=\frac{1}{\cos y-x},"I want to find the solution for the first order differential equation $$y'=\frac{1}{\cos y-x}$$ I have no clue what to do, what I tried is: $$(\cos y-x)dy=dx$$ $$\frac{dy}{dx}=\frac{1}{\cos y-x}$$ hints will be welcomed,thanks.","I want to find the solution for the first order differential equation $$y'=\frac{1}{\cos y-x}$$ I have no clue what to do, what I tried is: $$(\cos y-x)dy=dx$$ $$\frac{dy}{dx}=\frac{1}{\cos y-x}$$ hints will be welcomed,thanks.",,['ordinary-differential-equations']
81,New & interesting uses of Differential equations for undergraduates? [closed],New & interesting uses of Differential equations for undergraduates? [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 8 years ago . Improve this question I'm teaching an elementary DE's module to some engineering students.  Now, every book out there, and every set of online notes, trots out two things: DE's are super-important, vital, can't live without 'em, applications in every possible branch of applied mathematics & the sciences etc etc Applications: population growth (exponential & logistic), cooling, mixing problems, occasionally a circuit problem or a springs problem.  Oh - and orthogonal trajectories, so that you can justify teaching non-linear exact equations. I can't believe that these same applications are still all that educators use for examples.  Surely there must some interesting, new applications, which can be explained at (or simplified to) an elementary level?   Interestingly, most of these ""applications"" are separable.  Where are the linear non-separable equations; the linear systems? I've been searching online for some time now, and remarkably enough there's very little out there.  So either educators are completely stuck for good examples, or all the modern uses are simply too difficult and abstruse to be simplified down to beginners level. However - if there are any interesting new & modern uses of DE's, explainable at an elementary level, I'd love to know about them.","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 8 years ago . Improve this question I'm teaching an elementary DE's module to some engineering students.  Now, every book out there, and every set of online notes, trots out two things: DE's are super-important, vital, can't live without 'em, applications in every possible branch of applied mathematics & the sciences etc etc Applications: population growth (exponential & logistic), cooling, mixing problems, occasionally a circuit problem or a springs problem.  Oh - and orthogonal trajectories, so that you can justify teaching non-linear exact equations. I can't believe that these same applications are still all that educators use for examples.  Surely there must some interesting, new applications, which can be explained at (or simplified to) an elementary level?   Interestingly, most of these ""applications"" are separable.  Where are the linear non-separable equations; the linear systems? I've been searching online for some time now, and remarkably enough there's very little out there.  So either educators are completely stuck for good examples, or all the modern uses are simply too difficult and abstruse to be simplified down to beginners level. However - if there are any interesting new & modern uses of DE's, explainable at an elementary level, I'd love to know about them.",,"['ordinary-differential-equations', 'education']"
82,What is the quickest way to solve this 2nd Order Linear ODE?,What is the quickest way to solve this 2nd Order Linear ODE?,,"This appeared on my professor's test review, and its taken me hours to, surprise surprise, get the wrong answer. Could someone help me with the method I should be using to solve this? $$y^{\prime\prime}+y=\tan x$$","This appeared on my professor's test review, and its taken me hours to, surprise surprise, get the wrong answer. Could someone help me with the method I should be using to solve this? $$y^{\prime\prime}+y=\tan x$$",,[]
83,Conditions on a $1$-form in $\mathbb{R}^3$ for there to exist a function such that the form is closed.,Conditions on a -form in  for there to exist a function such that the form is closed.,1 \mathbb{R}^3,"What are the conditions on a $1$-form in $\mathbb{R}^3$ for there to exist a function such that the form is closed? More precisely, given a point, $p$, what are conditions on the coefficients of a $1$-form $\omega$ so that there is a non-zero function, $\lambda$, such that $d(\lambda \omega)=0$ in some neighborhood of $p$. I can derive equations from the definitions, but is there an equation that doesn't rely on $\lambda$? With work, this is equivalent to solving $f_i \dfrac{\partial \lambda}{\partial x^j}-f_j \dfrac{\partial \lambda}{\partial x^i}=\lambda (\dfrac{\partial f^j}{\partial x^i}-\dfrac{\partial f^i}{\partial x^j})$ for $i,j\in \{1, 2, 3\}$.","What are the conditions on a $1$-form in $\mathbb{R}^3$ for there to exist a function such that the form is closed? More precisely, given a point, $p$, what are conditions on the coefficients of a $1$-form $\omega$ so that there is a non-zero function, $\lambda$, such that $d(\lambda \omega)=0$ in some neighborhood of $p$. I can derive equations from the definitions, but is there an equation that doesn't rely on $\lambda$? With work, this is equivalent to solving $f_i \dfrac{\partial \lambda}{\partial x^j}-f_j \dfrac{\partial \lambda}{\partial x^i}=\lambda (\dfrac{\partial f^j}{\partial x^i}-\dfrac{\partial f^i}{\partial x^j})$ for $i,j\in \{1, 2, 3\}$.",,"['ordinary-differential-equations', 'differential-geometry']"
84,dy/dx when x and y are functions,dy/dx when x and y are functions,,"Let $x$ and $y$ be continuously differentiable real-valued functions defined on $\mathbf{R}$ and suppose that $x'(t) = f\,(x(t),y(t))$ and $y'(t) = g(x(t),y(t))$ where $f$ and $g$ are continuously differentiable on $\mathbf{R}^2$. According to the ODE book I am reading, if $f$ is never 0, then  \[ \frac{dy}{dx} = \frac{g(x,y)}{f(x,y)} \] \[ y(x_0) = y_0  \] where $x_0,y_0$ are constants, is an initial value problem. Now what does $dy/dx$ mean here? Is it just the function $y'(x)$? Or is the author being sloppy and $x$ is actually a dummy variable? If the former is true, how is this an initial value problem? Normally, an initial value problem is given as $y'(t) = h(y(t),t)$, $y(t_0) = y_0$ where $t$ is variable. PS. Why is it that ODE books are sloppy with notation and unshamefully nonrigorous?","Let $x$ and $y$ be continuously differentiable real-valued functions defined on $\mathbf{R}$ and suppose that $x'(t) = f\,(x(t),y(t))$ and $y'(t) = g(x(t),y(t))$ where $f$ and $g$ are continuously differentiable on $\mathbf{R}^2$. According to the ODE book I am reading, if $f$ is never 0, then  \[ \frac{dy}{dx} = \frac{g(x,y)}{f(x,y)} \] \[ y(x_0) = y_0  \] where $x_0,y_0$ are constants, is an initial value problem. Now what does $dy/dx$ mean here? Is it just the function $y'(x)$? Or is the author being sloppy and $x$ is actually a dummy variable? If the former is true, how is this an initial value problem? Normally, an initial value problem is given as $y'(t) = h(y(t),t)$, $y(t_0) = y_0$ where $t$ is variable. PS. Why is it that ODE books are sloppy with notation and unshamefully nonrigorous?",,['ordinary-differential-equations']
85,Nonlinear Ordinary differential equation,Nonlinear Ordinary differential equation,,I would like to solve: $$ \frac{d^2y}{dx^2} - \frac{2}{y^2}=0$$ with $y(0)=a$ and $y'(0)=0$ Where $a$ is a known constant. Thanks in advance.,I would like to solve: $$ \frac{d^2y}{dx^2} - \frac{2}{y^2}=0$$ with $y(0)=a$ and $y'(0)=0$ Where $a$ is a known constant. Thanks in advance.,,['ordinary-differential-equations']
86,Solving $\frac{dy}{dx}=\frac{x^2y^2+xy+1}{x^2}$,Solving,\frac{dy}{dx}=\frac{x^2y^2+xy+1}{x^2},"Assuming a substitution $v=xy$ , isn't a standard prescription however it can help solving some ODEs such as $$\frac{dy}{dx}=\frac{x^2y^2+xy+1}{x^2}.$$ Let us use $xy=v \implies xy'+y=v'$ , we get $$\frac{dv}{dx}=\frac{(v+1)^2}{x} \implies \frac{dv}{(v+1)^2}=\frac{dx}{x}.$$ Integrating both sides we get $$\frac{-1}{v+1}=\log x+C \implies y=-\frac{C+\log x+1}{x(C+\log x)}.$$ The question is: How else it can be solved Edit: Unfortunately there was a typo of sign in the  question, it should have been $+xy$ instead of $-xy$ . It is corrected now. Any other method is most welcome.","Assuming a substitution , isn't a standard prescription however it can help solving some ODEs such as Let us use , we get Integrating both sides we get The question is: How else it can be solved Edit: Unfortunately there was a typo of sign in the  question, it should have been instead of . It is corrected now. Any other method is most welcome.",v=xy \frac{dy}{dx}=\frac{x^2y^2+xy+1}{x^2}. xy=v \implies xy'+y=v' \frac{dv}{dx}=\frac{(v+1)^2}{x} \implies \frac{dv}{(v+1)^2}=\frac{dx}{x}. \frac{-1}{v+1}=\log x+C \implies y=-\frac{C+\log x+1}{x(C+\log x)}. +xy -xy,['ordinary-differential-equations']
87,Existence and unicity ODE,Existence and unicity ODE,,"I've been reading ""A text book on ordinary differential equations"" of Sahir Ahmad & Ambrossi. On page 37 we have the following excercise: ""Explain why $x'+\dfrac{\sin(t)}{e^t+1}x=0$ cannot has solution $x(t)$ such that $x(1)=1$ and $x(2)=-1$ ."" As far I know, $f(t,x)$ is continuos everywhere since $e^t\ne -1$ for every $t$ , and $\frac{\partial f}{\partial x}$ doesn't depends on $x$ , so its constant and therefore continous, then the existence and unicity theorem must apply on neighborhood of those points. Where am I wrong? Can anyone explain  me? Im very stuck af this assignament.","I've been reading ""A text book on ordinary differential equations"" of Sahir Ahmad & Ambrossi. On page 37 we have the following excercise: ""Explain why cannot has solution such that and ."" As far I know, is continuos everywhere since for every , and doesn't depends on , so its constant and therefore continous, then the existence and unicity theorem must apply on neighborhood of those points. Where am I wrong? Can anyone explain  me? Im very stuck af this assignament.","x'+\dfrac{\sin(t)}{e^t+1}x=0 x(t) x(1)=1 x(2)=-1 f(t,x) e^t\ne -1 t \frac{\partial f}{\partial x} x","['ordinary-differential-equations', 'initial-value-problems']"
88,"How to solve the differential equation $\dfrac{\partial f}{\partial t}(x,t) = c_1 f(x,t)+c_2f(x-1,t)-c_2 f(x+1,t)$?",How to solve the differential equation ?,"\dfrac{\partial f}{\partial t}(x,t) = c_1 f(x,t)+c_2f(x-1,t)-c_2 f(x+1,t)","I am trying to solve the differential equation$$\frac{\partial f}{\partial t}(x,t) = -ix^3f(x,t)+Cf(x-1,t)-C f(x+1,t)$$ where $C$ is a purely imaginary constant. I am not sure how to go about this as normally when solving differential equations the first coordinate is the same. I can't seem to think of any strategy to employ.","I am trying to solve the differential equation$$\frac{\partial f}{\partial t}(x,t) = -ix^3f(x,t)+Cf(x-1,t)-C f(x+1,t)$$ where $C$ is a purely imaginary constant. I am not sure how to go about this as normally when solving differential equations the first coordinate is the same. I can't seem to think of any strategy to employ.",,"['ordinary-differential-equations', 'partial-differential-equations']"
89,"Let $f:[0,\alpha]\to \mathbb{R}$ solution of a Cauchy problem , prove $\alpha <3.$","Let  solution of a Cauchy problem , prove","f:[0,\alpha]\to \mathbb{R} \alpha <3.","Let $f:[0,\alpha] \to \mathbb{R}$ be a solution of the Cauchy problem  $$  \begin{cases} f'(t)=f(t)^2+t, \\ f(0)=0. \end{cases} $$ Prove that $\alpha <3.$ I know that I should try to prove that $f$ blows up to $+\infty$ before time $t=3$. I tried to apply the comparison theorem, but I cound't find a suitable function to use. Any help is appreciated.","Let $f:[0,\alpha] \to \mathbb{R}$ be a solution of the Cauchy problem  $$  \begin{cases} f'(t)=f(t)^2+t, \\ f(0)=0. \end{cases} $$ Prove that $\alpha <3.$ I know that I should try to prove that $f$ blows up to $+\infty$ before time $t=3$. I tried to apply the comparison theorem, but I cound't find a suitable function to use. Any help is appreciated.",,"['real-analysis', 'ordinary-differential-equations', 'derivatives']"
90,The existence of the laplace transform,The existence of the laplace transform,,"I don't understand why the laplace transform of some function, say f(t), has to be ""piecewise continuous"" and not ""continuous"". Is ""piecewise continuous"" sort of like the minimum requirement? This troubles me because I don't think f(t)=t is piecewise continuous, it's simply continuous... Also, I don't understand what's so special about a function being of ""exponential order"". Can someone explain to me why this property is so special and apparently makes the Laplace Transform exist? Also, my teacher says that f(t) has to be of exponential order from [0,infinity] and other sources say that it at least has to be of exponential order from [T,infinity] where T>0. Thanks.","I don't understand why the laplace transform of some function, say f(t), has to be ""piecewise continuous"" and not ""continuous"". Is ""piecewise continuous"" sort of like the minimum requirement? This troubles me because I don't think f(t)=t is piecewise continuous, it's simply continuous... Also, I don't understand what's so special about a function being of ""exponential order"". Can someone explain to me why this property is so special and apparently makes the Laplace Transform exist? Also, my teacher says that f(t) has to be of exponential order from [0,infinity] and other sources say that it at least has to be of exponential order from [T,infinity] where T>0. Thanks.",,"['calculus', 'ordinary-differential-equations', 'laplace-transform']"
91,Integrating factor in linear differential equations,Integrating factor in linear differential equations,,"I'm watching various videos on differential equations and they all say that linear differential equations are on the form: $y' + P(x)y = Q(x)$ where $P(x)$ is the integrating factor and equals $e^{\int P(x) dx}$ ...But nobody bothers to explain why or what this integrating factor is. What happens for example when $P(x)$ is a constant? Maybe someone here could explain what is going on, really. Thanks.","I'm watching various videos on differential equations and they all say that linear differential equations are on the form: $y' + P(x)y = Q(x)$ where $P(x)$ is the integrating factor and equals $e^{\int P(x) dx}$ ...But nobody bothers to explain why or what this integrating factor is. What happens for example when $P(x)$ is a constant? Maybe someone here could explain what is going on, really. Thanks.",,"['calculus', 'ordinary-differential-equations']"
92,The characteristic polynomial of a recurrence relation.,The characteristic polynomial of a recurrence relation.,,"If I have a linear homogeneous recurrence relation $$y_n=c_1y_{n-1}+\ldots+c_ky_{n-k},$$ I can get its characteristic equation, which is $$r^k=c_1r^{k-1}+\ldots+c_k.$$ In particular for $y_n=cy_{n-1},$ I get $r=c.$ While I see that this obviously gives the right solution, something seems not right. For a differential equation $y'=cy,$ I get a similar characteristic equation: $r=c$, however, as I understand it, the the analogy between differential equations and recurrence relations is given by $y'\sim y_n-y_{n-1},$ not $y'\sim y_n.$ By this logic, I think the characteristic equation for the recurrence relation $y_n=cy_{n-1}$ should be $r=c-1,$ because the recurrence relation is equivalent to $y_n-y_{n-1}=(c-1)y_{n-1}.$ Could you help me understand why the analogy breaks down here (or why it doesn't)?","If I have a linear homogeneous recurrence relation $$y_n=c_1y_{n-1}+\ldots+c_ky_{n-k},$$ I can get its characteristic equation, which is $$r^k=c_1r^{k-1}+\ldots+c_k.$$ In particular for $y_n=cy_{n-1},$ I get $r=c.$ While I see that this obviously gives the right solution, something seems not right. For a differential equation $y'=cy,$ I get a similar characteristic equation: $r=c$, however, as I understand it, the the analogy between differential equations and recurrence relations is given by $y'\sim y_n-y_{n-1},$ not $y'\sim y_n.$ By this logic, I think the characteristic equation for the recurrence relation $y_n=cy_{n-1}$ should be $r=c-1,$ because the recurrence relation is equivalent to $y_n-y_{n-1}=(c-1)y_{n-1}.$ Could you help me understand why the analogy breaks down here (or why it doesn't)?",,"['ordinary-differential-equations', 'recurrence-relations']"
93,Closed form solutions of $\ddot x(t)-x(t)^n=0$,Closed form solutions of,\ddot x(t)-x(t)^n=0,"Given the ODE: $$\ddot x(t)-x(t)=0$$ the solution is: $$x(t)=C_1\exp(-t)+C_2\exp(t)$$ If we square the $x(t)$ we have: $$\ddot x(t)-x(t)^2=0$$ and the solution is given by: $$x(t)=6\wp(t+C_1;0,C_2)$$ and so for $x(t)^3$ which gives: $$x(t)=\operatorname{sn}\left(\left(\frac{1}{2}i\sqrt t +C_1\right)C_2,i\right)$$ More generally, is it possible to find closed form solutions for the equation: $$\ddot x(t)-x(t)^n=0$$ ? Thanks.","Given the ODE: $$\ddot x(t)-x(t)=0$$ the solution is: $$x(t)=C_1\exp(-t)+C_2\exp(t)$$ If we square the $x(t)$ we have: $$\ddot x(t)-x(t)^2=0$$ and the solution is given by: $$x(t)=6\wp(t+C_1;0,C_2)$$ and so for $x(t)^3$ which gives: $$x(t)=\operatorname{sn}\left(\left(\frac{1}{2}i\sqrt t +C_1\right)C_2,i\right)$$ More generally, is it possible to find closed form solutions for the equation: $$\ddot x(t)-x(t)^n=0$$ ? Thanks.",,"['ordinary-differential-equations', 'elliptic-functions']"
94,Existence of global solution of Riccati equation,Existence of global solution of Riccati equation,,"Consider a Riccati differential equation $$   \dot P + A(t)^{T}P + PA(t) -PB(t)R(t)B(t)^{T}P + Q(t) = 0,\;\;\; P(t_0) = P_0 = P_0^{T} \geqslant 0 $$ where $Q(t) = Q(t)^{T} \geqslant 0$, $R(t) = R(t)^{T} > 0$, all matrices are real and continuous. How to show that for any $t_0$ there exists (and unique) a solution defined on $(-\infty,t_0]$?","Consider a Riccati differential equation $$   \dot P + A(t)^{T}P + PA(t) -PB(t)R(t)B(t)^{T}P + Q(t) = 0,\;\;\; P(t_0) = P_0 = P_0^{T} \geqslant 0 $$ where $Q(t) = Q(t)^{T} \geqslant 0$, $R(t) = R(t)^{T} > 0$, all matrices are real and continuous. How to show that for any $t_0$ there exists (and unique) a solution defined on $(-\infty,t_0]$?",,"['ordinary-differential-equations', 'control-theory']"
95,What is a good differential equations textbook?,What is a good differential equations textbook?,,"I have taken a lot of math in university, but chose to omit differential equations. Unfortunately, now I have to read computer science proofs that use them, mostly ODEs, and this is always a struggle. What textbook(s) should I read to take me from the basics to practical use of the theory?","I have taken a lot of math in university, but chose to omit differential equations. Unfortunately, now I have to read computer science proofs that use them, mostly ODEs, and this is always a struggle. What textbook(s) should I read to take me from the basics to practical use of the theory?",,"['reference-request', 'ordinary-differential-equations']"
96,Is my explanation of why we can make $\frac{dy}{dx} {dx} = {dy}$ in the separable differential equations valid? [duplicate],Is my explanation of why we can make  in the separable differential equations valid? [duplicate],\frac{dy}{dx} {dx} = {dy},"This question already has answers here : What am I doing when I separate the variables of a differential equation? (5 answers) Closed 11 days ago . The community reviewed whether to reopen this question 8 days ago and left it closed: Original close reason(s) were not resolved I'm trying to understand why we can separate $\frac{dy}{dx}$ in separable differential equations when rigorously it's not a fraction but an operator (AFAIK). I have an assumption, but I'm not sure that it's right. I'm not a mathematician, just a math-curious self-taught programmer and factory worker (all together). I eventually realized that I don't understand how the separation of $\frac{dy}{dx}$ actually works in the background and got kind of nerd-sniped (xkcd 356). I never took a course on rigorous Analysis, just lectures on Calculus a few years ago, from MIT OCW and Professor Leonard and such. Well, I came here to the community of mathematicians to ask if my first attempt is correct and what should I do with the second one in order to finish it, because I am stuck there. Fist attempt: $\frac{dy}{dx} = f(x)g(y)$ , $\frac{1}{g(y)}\frac{dy}{dx} = f(x)$ , we can write: $\frac{1}{g(y)}$ as: $\frac{d}{dy} \int\frac{1}{g(y)}dy$ . Then: $\frac{d}{dy} \int\frac{1}{g(y)}dy\frac{dy}{dx} = f(x)$ . I see a similarity to the chain rule, so maybe I can use it. Recall ""The Chain Rule"": $\frac{du}{dx} = \frac{du}{dy} \frac{dy}{dx}$ , and let $u = \int\frac{1}{g(y)}dy$ , so if we substitute it into the chain rule, we get: $\frac{d}{dx} \int\frac{1}{g(y)}dy = \frac{d}{dy} \int\frac{1}{g(y)}dy \frac{dy}{dx}$ , since the first term of the right-hand side is simply a differentiation of an integration with respect to the same variable ${y}$ then according to the fundamental theorem of calculus: $\frac{d}{dx} \int\frac{1}{g(y)}dy = \frac{1}{g(y)} \frac{dy}{dx} = f(x)$ . Now if I integrate all this with respect to $x$ : $\int[\frac{d}{dx} \int\frac{1}{g(y)}dy]dx = \int[\frac{1}{g(y)} \frac{dy}{dx}]dx = \int f(x) dx$ . I finally get $\frac{dy}{dx}$ separated without ""abusing notation"": $\int\frac{1}{g(y)}dy = \int\frac{1}{g(y)} \frac{dy}{dx}dx = \int f(x) dx$ ; $\int\frac{1}{g(y)}dy = \int f(x) dx$ . Is this attempt correct? Second attempt (which I stuck with at the end): $\frac{dy}{dx} = f(x)g(y)$ ; $\frac{1}{g(y)}\frac{dy}{dx} = f(x)$ . Let: $G(u) = \int\frac{1}{g(u)}du$$ where $ {u} = y(x) $ and therefore $ {du} = \frac{d}{dx}y(x) $. Then I think that I can rewrite: $ G(u) = \int\frac{1}{g(u)}du $ as: $ G(y(x)) = \int\frac{1}{g(y(x))} \frac{d}{dx} y(x)$. Then: $\frac{1}{g(y)}\frac{dy}{dx} = f(x)$ can be rewritten as: $\frac{d}{dy}G(y(x)) \frac{dy}{dx} = f(x)$ . Recall the Chain Rule: $\frac{du}{dx} = \frac{du}{dy} \frac{dy}{dx}$ and in this case the $u = G(y(x))$ , so we get: $\frac{d}{dx} G(y(x)) = \frac{d}{dy} G(y(x)) \frac{dy}{dx} = f(x)$ , then I integrate all this with respect to ${x}$ like this: $\int\frac{d}{dx} G(y(x)) dx = \int[\frac{d}{dy} G(y(x)) \frac{dy}{dx}]dx = \int f(x) dx$ , since $G(y(x)) = \int\frac{1}{g(y(x))} \frac{d}{dx} y(x)$ : $\int[\frac{d}{dx} \int\frac{1}{g(y(x))} \frac{d}{dx} y(x)] dx = \int[\frac{d}{dy} \int\frac{1}{g(y(x))} \frac{d}{dx} y(x) \frac{dy}{dx}]dx = \int f(x) dx$ And here I stuck and don't know what (and why) to do with all this mess.","This question already has answers here : What am I doing when I separate the variables of a differential equation? (5 answers) Closed 11 days ago . The community reviewed whether to reopen this question 8 days ago and left it closed: Original close reason(s) were not resolved I'm trying to understand why we can separate in separable differential equations when rigorously it's not a fraction but an operator (AFAIK). I have an assumption, but I'm not sure that it's right. I'm not a mathematician, just a math-curious self-taught programmer and factory worker (all together). I eventually realized that I don't understand how the separation of actually works in the background and got kind of nerd-sniped (xkcd 356). I never took a course on rigorous Analysis, just lectures on Calculus a few years ago, from MIT OCW and Professor Leonard and such. Well, I came here to the community of mathematicians to ask if my first attempt is correct and what should I do with the second one in order to finish it, because I am stuck there. Fist attempt: , , we can write: as: . Then: . I see a similarity to the chain rule, so maybe I can use it. Recall ""The Chain Rule"": , and let , so if we substitute it into the chain rule, we get: , since the first term of the right-hand side is simply a differentiation of an integration with respect to the same variable then according to the fundamental theorem of calculus: . Now if I integrate all this with respect to : . I finally get separated without ""abusing notation"": ; . Is this attempt correct? Second attempt (which I stuck with at the end): ; . Let: {u} = y(x) {du} = \frac{d}{dx}y(x) G(u) = \int\frac{1}{g(u)}du G(y(x)) = \int\frac{1}{g(y(x))} \frac{d}{dx} y(x)$. Then: can be rewritten as: . Recall the Chain Rule: and in this case the , so we get: , then I integrate all this with respect to like this: , since : And here I stuck and don't know what (and why) to do with all this mess.","\frac{dy}{dx} \frac{dy}{dx} \frac{dy}{dx} = f(x)g(y) \frac{1}{g(y)}\frac{dy}{dx} = f(x) \frac{1}{g(y)} \frac{d}{dy} \int\frac{1}{g(y)}dy \frac{d}{dy} \int\frac{1}{g(y)}dy\frac{dy}{dx} = f(x) \frac{du}{dx} = \frac{du}{dy} \frac{dy}{dx} u = \int\frac{1}{g(y)}dy \frac{d}{dx} \int\frac{1}{g(y)}dy = \frac{d}{dy} \int\frac{1}{g(y)}dy \frac{dy}{dx} {y} \frac{d}{dx} \int\frac{1}{g(y)}dy = \frac{1}{g(y)} \frac{dy}{dx} = f(x) x \int[\frac{d}{dx} \int\frac{1}{g(y)}dy]dx = \int[\frac{1}{g(y)} \frac{dy}{dx}]dx = \int f(x) dx \frac{dy}{dx} \int\frac{1}{g(y)}dy = \int\frac{1}{g(y)} \frac{dy}{dx}dx = \int f(x) dx \int\frac{1}{g(y)}dy = \int f(x) dx \frac{dy}{dx} = f(x)g(y) \frac{1}{g(y)}\frac{dy}{dx} = f(x) G(u) = \int\frac{1}{g(u)}du where   and therefore  . Then I think that I can rewrite:
  as:  \frac{1}{g(y)}\frac{dy}{dx} = f(x) \frac{d}{dy}G(y(x)) \frac{dy}{dx} = f(x) \frac{du}{dx} = \frac{du}{dy} \frac{dy}{dx} u = G(y(x)) \frac{d}{dx} G(y(x)) = \frac{d}{dy} G(y(x)) \frac{dy}{dx} = f(x) {x} \int\frac{d}{dx} G(y(x)) dx = \int[\frac{d}{dy} G(y(x)) \frac{dy}{dx}]dx = \int f(x) dx G(y(x)) = \int\frac{1}{g(y(x))} \frac{d}{dx} y(x) \int[\frac{d}{dx} \int\frac{1}{g(y(x))} \frac{d}{dx} y(x)] dx = \int[\frac{d}{dy} \int\frac{1}{g(y(x))} \frac{d}{dx} y(x) \frac{dy}{dx}]dx = \int f(x) dx","['calculus', 'ordinary-differential-equations', 'infinitesimals']"
97,A question about a third-order homogenous linear ODE,A question about a third-order homogenous linear ODE,,"Consider the ODE $$ xy'''-2xy''+(x-1)y'+y=0 $$ wolframalpha returns the following solution: $$ y(x) = c_2 e^x x^2 + c_3 e^x (1 - x^2) + c_1 e^x $$ My Question : It seems that the solutions are spanned by $$ \{y_1(x)=e^xx^2\ ,\ y_2(x)=e^x(x^2-1)\ ,\ y_3(x)=e^x\} $$ But this set is linearly dependent since $y_2=y_1-y_3$ . Shouldn't we suppose to have 3 linearly independent  in the basis?",Consider the ODE wolframalpha returns the following solution: My Question : It seems that the solutions are spanned by But this set is linearly dependent since . Shouldn't we suppose to have 3 linearly independent  in the basis?,"
xy'''-2xy''+(x-1)y'+y=0
 
y(x) = c_2 e^x x^2 + c_3 e^x (1 - x^2) + c_1 e^x
 
\{y_1(x)=e^xx^2\ ,\ y_2(x)=e^x(x^2-1)\ ,\ y_3(x)=e^x\}
 y_2=y_1-y_3",['ordinary-differential-equations']
98,How to solve this differential equation via power series,How to solve this differential equation via power series,,"Studying Quantum Mechanics, I encountered the following differential equation: how to solve $$y''-x^2y=0$$ Griffiths, the author of the book, just said that its solutions are $$y=Ae^{x^2/2}+Be^{-x^2/2}$$ I can immediatelly see that this is true, but I do not know how to get to this solution, and I cannot find it solved anywhere else. I tried to use the power series method: $$y=\sum_{n=0}^{\infty}c_n x^n$$ So the equation becomes: $$\sum_{n=0}^{\infty}c_{n+2}x^n(n+1)(n+2)-\sum_{n=0}^{\infty}c_n x^{n+2}=0$$ Here I need to get both sums to the same power of n, so I removed the first two terms from the first sum, so that I can later change the index: $$2c_2+6c_3x+\sum_{n=2}^{\infty}c_{n+2}x^n(n+1)(n+2)-\sum_{n=0}^{\infty}c_n x^{n+2}=0$$ now I change the index in the fist sum: $$2c_2+6c_3x+\sum_{n=2}^{\infty}c_{n+4}x^{n+2}(n+4)n(n+3)-\sum_{n=0}^{\infty}c_n x^{n+2}=0$$ Now I factor out the sum: $$2c_2+6c_3x+\sum_{n=2}^{\infty}x^{n+2}(c_{n+4}(n+4)(n+3)-c_n)=0$$ Now all three terms must be zero at the same time, so $c_2=c_3=0$ , and: $$c_{n+4}(n+4)(n+3)-c_n=0$$ $$c_{n+4}=\frac{c_n}{(n+4)(n+3)}$$ The first terms of the series are: $c_4=\frac{c_0}{4\times 3}$ , $c_8=\frac{c_4}{8\times 7}=\frac{c_0}{8\times 7\times 4\times 3}$ However, I don't really know how to proceed from here. I tried putting the coefficients into the sum for $y$ , but I can't really come close to the answer. Could you please guide me as how to advance from here?","Studying Quantum Mechanics, I encountered the following differential equation: how to solve Griffiths, the author of the book, just said that its solutions are I can immediatelly see that this is true, but I do not know how to get to this solution, and I cannot find it solved anywhere else. I tried to use the power series method: So the equation becomes: Here I need to get both sums to the same power of n, so I removed the first two terms from the first sum, so that I can later change the index: now I change the index in the fist sum: Now I factor out the sum: Now all three terms must be zero at the same time, so , and: The first terms of the series are: , However, I don't really know how to proceed from here. I tried putting the coefficients into the sum for , but I can't really come close to the answer. Could you please guide me as how to advance from here?",y''-x^2y=0 y=Ae^{x^2/2}+Be^{-x^2/2} y=\sum_{n=0}^{\infty}c_n x^n \sum_{n=0}^{\infty}c_{n+2}x^n(n+1)(n+2)-\sum_{n=0}^{\infty}c_n x^{n+2}=0 2c_2+6c_3x+\sum_{n=2}^{\infty}c_{n+2}x^n(n+1)(n+2)-\sum_{n=0}^{\infty}c_n x^{n+2}=0 2c_2+6c_3x+\sum_{n=2}^{\infty}c_{n+4}x^{n+2}(n+4)n(n+3)-\sum_{n=0}^{\infty}c_n x^{n+2}=0 2c_2+6c_3x+\sum_{n=2}^{\infty}x^{n+2}(c_{n+4}(n+4)(n+3)-c_n)=0 c_2=c_3=0 c_{n+4}(n+4)(n+3)-c_n=0 c_{n+4}=\frac{c_n}{(n+4)(n+3)} c_4=\frac{c_0}{4\times 3} c_8=\frac{c_4}{8\times 7}=\frac{c_0}{8\times 7\times 4\times 3} y,"['ordinary-differential-equations', 'power-series']"
99,Solving for $y'' - 4y' - 5y - 2 = 0$,Solving for,y'' - 4y' - 5y - 2 = 0,"I am looking to solve for the above nonhomogeneous ODE. I know how to find the general solution for the reduced equation of the homogeneous form, that is, $$y'' - 4y' - 5y = 0.$$ The characteristic equation is $r^{2} - 4r - 5 = 0$ , which gives two real and distinct roots $r=-1,5$ . So the complementary solution is $y_{c}=c_{1}e^{5x} + c_{2}e^{-x}$ . Now I am looking to guess the particular on the right-hand side but I am not sure about how to do that in order to find the general solution of the above nonhomogenous ODE.","I am looking to solve for the above nonhomogeneous ODE. I know how to find the general solution for the reduced equation of the homogeneous form, that is, The characteristic equation is , which gives two real and distinct roots . So the complementary solution is . Now I am looking to guess the particular on the right-hand side but I am not sure about how to do that in order to find the general solution of the above nonhomogenous ODE.","y'' - 4y' - 5y = 0. r^{2} - 4r - 5 = 0 r=-1,5 y_{c}=c_{1}e^{5x} + c_{2}e^{-x}",['ordinary-differential-equations']
