,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Intuition behind the essential range,Intuition behind the essential range,,"We define the essential range in the following way. Let $F$ be a real-valued function on a measure space $\langle M , \mu \rangle$. We say $\lambda$ is in the essential range of $F$ if and only if $$\mu \{ m \ \vert \ \lambda - \epsilon < F(m) < \lambda + \epsilon \} > 0$$ for all $\epsilon >0$. Intuitively, should we think of the essential range as the range of $F$ seen by the measure? Does this at all relate to the essential supremum defined on $L^{\infty}?$","We define the essential range in the following way. Let $F$ be a real-valued function on a measure space $\langle M , \mu \rangle$. We say $\lambda$ is in the essential range of $F$ if and only if $$\mu \{ m \ \vert \ \lambda - \epsilon < F(m) < \lambda + \epsilon \} > 0$$ for all $\epsilon >0$. Intuitively, should we think of the essential range as the range of $F$ seen by the measure? Does this at all relate to the essential supremum defined on $L^{\infty}?$",,"['functional-analysis', 'operator-theory']"
1,Positive cone of Banach lattice algebra,Positive cone of Banach lattice algebra,,"From the literature on Banach lattice algebras (and that on ordered Banach algebras) there does not appear to be a consensus on a definition. What is agreed is that one should be a Banach lattice, be an associative algebra with a sub-multiplicative norm and that the product of positive elements should be positive. To standardise terminology we propose that a Banach lattice algebra simply is at the same time a Banach lattice, an algebra with the sub-multiplicative norm, and with the product of positive elements being positive. Question: For $X$ being a Banach lattice algebra endowed with an ordering $\leq$ , let $X_+:=\{x\in X:x>0\}$ be the positive cone. This cone would generally be open or closed? Please provide proof or hints to prove it with references.","From the literature on Banach lattice algebras (and that on ordered Banach algebras) there does not appear to be a consensus on a definition. What is agreed is that one should be a Banach lattice, be an associative algebra with a sub-multiplicative norm and that the product of positive elements should be positive. To standardise terminology we propose that a Banach lattice algebra simply is at the same time a Banach lattice, an algebra with the sub-multiplicative norm, and with the product of positive elements being positive. Question: For being a Banach lattice algebra endowed with an ordering , let be the positive cone. This cone would generally be open or closed? Please provide proof or hints to prove it with references.",X \leq X_+:=\{x\in X:x>0\},"['functional-analysis', 'banach-spaces', 'banach-algebras', 'banach-lattices']"
2,The concept/physical meaning/interpretations behind the Bessel's inequality,The concept/physical meaning/interpretations behind the Bessel's inequality,,"If $\{\boldsymbol{\varphi}_1, \boldsymbol{\varphi}_2, ... \}$ is an orthonormal system in   $\mathcal{H}$, then the Bessel's inequality is: $$\sum_{j=1}^{\infty} |\langle \mathbf{x},\boldsymbol{\varphi}_j \rangle|^2 \leq \lVert  \mathbf{x} \rVert^2 \quad \text{for every } \mathbf{x}\in \mathcal{H}.$$ My question is: What is the meaning/concept of this inequality? Is there any geometric or physical interpretations about that? Any comments or answers are appreciated.","If $\{\boldsymbol{\varphi}_1, \boldsymbol{\varphi}_2, ... \}$ is an orthonormal system in   $\mathcal{H}$, then the Bessel's inequality is: $$\sum_{j=1}^{\infty} |\langle \mathbf{x},\boldsymbol{\varphi}_j \rangle|^2 \leq \lVert  \mathbf{x} \rVert^2 \quad \text{for every } \mathbf{x}\in \mathcal{H}.$$ My question is: What is the meaning/concept of this inequality? Is there any geometric or physical interpretations about that? Any comments or answers are appreciated.",,"['linear-algebra', 'abstract-algebra', 'functional-analysis', 'inequality', 'geometric-inequalities']"
3,What would be a criteria to discover if a state is a tensor product or not?,What would be a criteria to discover if a state is a tensor product or not?,,"In Quantum Mechanics states of composite systems are described by tensor products. As is known, if we have $\mathcal{H}_1$ and $\mathcal{H}_2$ (which of course could be the same space) in the tensor product $\mathcal{H}_1\otimes \mathcal{H}_2$ we have the so called factorizable tensors which are writen as $|\psi_1\rangle\otimes |\psi_2\rangle$ with $|\psi_i\rangle \in \mathcal{H}_i$ but we also have more general elements which cannot be factored this way. Indeed if $|u_n(i)\rangle$ is a basis of $\mathcal{H}_i$ then $|u_n(1)\rangle \otimes |u_m(2)\rangle$ is a basis of $\mathcal{H}_1\otimes \mathcal{H}_2$ so that a general element is $$|\psi\rangle = \sum_{nm} c_{nm}|u_n(1)\rangle \otimes |u_m(2)\rangle.$$ My question is: if we have one $|\psi\rangle$ and we want to determine whether or not it is a factorizable tensor, what would be a useful criteria to determine this?","In Quantum Mechanics states of composite systems are described by tensor products. As is known, if we have $\mathcal{H}_1$ and $\mathcal{H}_2$ (which of course could be the same space) in the tensor product $\mathcal{H}_1\otimes \mathcal{H}_2$ we have the so called factorizable tensors which are writen as $|\psi_1\rangle\otimes |\psi_2\rangle$ with $|\psi_i\rangle \in \mathcal{H}_i$ but we also have more general elements which cannot be factored this way. Indeed if $|u_n(i)\rangle$ is a basis of $\mathcal{H}_i$ then $|u_n(1)\rangle \otimes |u_m(2)\rangle$ is a basis of $\mathcal{H}_1\otimes \mathcal{H}_2$ so that a general element is $$|\psi\rangle = \sum_{nm} c_{nm}|u_n(1)\rangle \otimes |u_m(2)\rangle.$$ My question is: if we have one $|\psi\rangle$ and we want to determine whether or not it is a factorizable tensor, what would be a useful criteria to determine this?",,"['linear-algebra', 'functional-analysis', 'hilbert-spaces', 'tensor-products', 'quantum-mechanics']"
4,"Determining the domain of the adjoint of $T = i\frac{d}{dx}$ on $C^1[0,1] \subseteq L^2[0,1]$",Determining the domain of the adjoint of  on,"T = i\frac{d}{dx} C^1[0,1] \subseteq L^2[0,1]","I have read this post Distinguishing between symmetric, Hermitian and self-adjoint operators But I have some specific questions relating to the operator $$T : \psi \longmapsto i \frac{d}{dx} \psi(x).$$ This operator is symmetric, meaning that $\langle \psi, T\varphi \rangle = \langle T\psi, \varphi \rangle$ for all $\varphi, \psi \in D(T)$. I'm aware that $T$ will be self-adjoint if $D(T^{\ast}) \subset D(T)$. My question is, since we know that $T$ is symmetric, does that immediately give us that the formula for $T^{\ast}$ will be $T$ itself? Moreover, if this is true, how do we determine the domain of $T^{\ast}$. For example, suppose the domain of $T$ is given to be $\psi \in C^1[0,1]$ with vanishing boundary conditions $\psi(0) = \psi(1) =0$. How do we determine the domain $D(T^{\ast}$)?","I have read this post Distinguishing between symmetric, Hermitian and self-adjoint operators But I have some specific questions relating to the operator $$T : \psi \longmapsto i \frac{d}{dx} \psi(x).$$ This operator is symmetric, meaning that $\langle \psi, T\varphi \rangle = \langle T\psi, \varphi \rangle$ for all $\varphi, \psi \in D(T)$. I'm aware that $T$ will be self-adjoint if $D(T^{\ast}) \subset D(T)$. My question is, since we know that $T$ is symmetric, does that immediately give us that the formula for $T^{\ast}$ will be $T$ itself? Moreover, if this is true, how do we determine the domain of $T^{\ast}$. For example, suppose the domain of $T$ is given to be $\psi \in C^1[0,1]$ with vanishing boundary conditions $\psi(0) = \psi(1) =0$. How do we determine the domain $D(T^{\ast}$)?",,"['functional-analysis', 'operator-theory']"
5,A norm making $M_n(A)$ a $C^*$-algebra when $A$ is a $C^*$-algebra,A norm making  a -algebra when  is a -algebra,M_n(A) C^* A C^*,"Show that there is an unique norm on $M_n(A)$ making it a $C^*$-algebra, where $A$ is a $C^*$-algebra itself. It is enough to show this when $A$ is a concrete C^*-algebra. Suppose we are able to find a norm $\|.\|_1$ for such a case. Because for any $C^*$-algebra $B$,  there is an isometric isomorphism $\pi$ from $B$ to a concrete $C^*$-algebra. Then for $X \in M_n(B)$, we define $\|X\|=\|\pi(X)\|_1$, where $$\pi(X)=\begin{bmatrix}                         \pi(b_{11})&\pi(b_{12})&\ldots&\pi(b_{1n})\\ \vdots&\vdots& \vdots &\vdots\\ \pi(b_{n1})&\pi(b_{n2})&\ldots&\pi(b_{nn})\\ \end{bmatrix}$$ when  $$X=\begin{bmatrix} (b_{11})&(b_{12})&\ldots&(b_{1n})\\ \vdots   &\vdots& \vdots &\vdots\\ (b_{n1})&(b_{n2})&\ldots&(b_{nn})\\                                                   \end{bmatrix}$$ Then $$\|X^*X\|=\|\pi(X^*X)\|_1=\|(\pi(X))^*\pi(X)\|_1=\|\pi(X)\|_1^2=\|X\|^2$$ So the problem now boils down to finding a norm on $M_n(A$) when $A$ is a concrete C^*-algebra.  Let $X \in M_n(A)$. Suppose that $X=\begin{bmatrix} (b_{11})&(b_{12})&\ldots&(b_{1n})\\ \vdots   &\vdots& \vdots &\vdots\\ (b_{n1})&(b_{n2})&\ldots&(b_{nn})\\ \end{bmatrix}$ Then for $$a=(a_1,a_2,\ldots,a_n) \in H^n, Xa=(\sum_{j=1}^n b_{1j}a_j,\ldots,\sum_{j=1}^nb_{nj}a_j)$$ Naturally I think  $$\|Xa\|^2=\langle \sum_{j=1}^nb_{1j}a_j, \sum_{j=1}^nb_{1j}a_j \rangle + \ldots + \langle \sum_{j=1}^nb_{nj}a_j, \sum_{j=1}^nb_{nj}a_j \rangle=\sum_{i=1}^n \left\|\sum_{j=1}^nb_{ij}a_j\right\|^2$$ So I should define $$\|X\|=\sup_{\|a\| \le 1}\|Xa\|$$ where $$\|a\|=\left(\sum_{i=1}^n\|a_i\|^2\right)^{1/2}$$ I am not sure. Is there any other norm?  Thanks for the help!!","Show that there is an unique norm on $M_n(A)$ making it a $C^*$-algebra, where $A$ is a $C^*$-algebra itself. It is enough to show this when $A$ is a concrete C^*-algebra. Suppose we are able to find a norm $\|.\|_1$ for such a case. Because for any $C^*$-algebra $B$,  there is an isometric isomorphism $\pi$ from $B$ to a concrete $C^*$-algebra. Then for $X \in M_n(B)$, we define $\|X\|=\|\pi(X)\|_1$, where $$\pi(X)=\begin{bmatrix}                         \pi(b_{11})&\pi(b_{12})&\ldots&\pi(b_{1n})\\ \vdots&\vdots& \vdots &\vdots\\ \pi(b_{n1})&\pi(b_{n2})&\ldots&\pi(b_{nn})\\ \end{bmatrix}$$ when  $$X=\begin{bmatrix} (b_{11})&(b_{12})&\ldots&(b_{1n})\\ \vdots   &\vdots& \vdots &\vdots\\ (b_{n1})&(b_{n2})&\ldots&(b_{nn})\\                                                   \end{bmatrix}$$ Then $$\|X^*X\|=\|\pi(X^*X)\|_1=\|(\pi(X))^*\pi(X)\|_1=\|\pi(X)\|_1^2=\|X\|^2$$ So the problem now boils down to finding a norm on $M_n(A$) when $A$ is a concrete C^*-algebra.  Let $X \in M_n(A)$. Suppose that $X=\begin{bmatrix} (b_{11})&(b_{12})&\ldots&(b_{1n})\\ \vdots   &\vdots& \vdots &\vdots\\ (b_{n1})&(b_{n2})&\ldots&(b_{nn})\\ \end{bmatrix}$ Then for $$a=(a_1,a_2,\ldots,a_n) \in H^n, Xa=(\sum_{j=1}^n b_{1j}a_j,\ldots,\sum_{j=1}^nb_{nj}a_j)$$ Naturally I think  $$\|Xa\|^2=\langle \sum_{j=1}^nb_{1j}a_j, \sum_{j=1}^nb_{1j}a_j \rangle + \ldots + \langle \sum_{j=1}^nb_{nj}a_j, \sum_{j=1}^nb_{nj}a_j \rangle=\sum_{i=1}^n \left\|\sum_{j=1}^nb_{ij}a_j\right\|^2$$ So I should define $$\|X\|=\sup_{\|a\| \le 1}\|Xa\|$$ where $$\|a\|=\left(\sum_{i=1}^n\|a_i\|^2\right)^{1/2}$$ I am not sure. Is there any other norm?  Thanks for the help!!",,"['functional-analysis', 'operator-theory', 'operator-algebras', 'c-star-algebras']"
6,Every Hamel basis has cardinality continuum $(c)$,Every Hamel basis has cardinality continuum,(c),"I found the demonstration of this assertion in a book: ""Let the cardinality of some Hamel basis be $\kappa$. We can easily calculate the cardinality of the generated vector space: it is $\aleph_0(\kappa+\kappa^2+\ldots)=\aleph_0\kappa = \kappa$ and since this must be equal to c , we obtain $\kappa$ = c ."" ( c is the cardinality of the continuum) How did they get $\aleph_0(\kappa+\kappa^2+\ldots)$ ?","I found the demonstration of this assertion in a book: ""Let the cardinality of some Hamel basis be $\kappa$. We can easily calculate the cardinality of the generated vector space: it is $\aleph_0(\kappa+\kappa^2+\ldots)=\aleph_0\kappa = \kappa$ and since this must be equal to c , we obtain $\kappa$ = c ."" ( c is the cardinality of the continuum) How did they get $\aleph_0(\kappa+\kappa^2+\ldots)$ ?",,['functional-analysis']
7,"Find the adjoint operator of $T(x_1,x_2,x_3,\dots)=(\sum_{n=2}^\infty x_n, x_1, x_2, x_3, \dots)$ in $\ell^1$.",Find the adjoint operator of  in .,"T(x_1,x_2,x_3,\dots)=(\sum_{n=2}^\infty x_n, x_1, x_2, x_3, \dots) \ell^1","Find the adjoint operator of $T(x_1,x_2,x_3,\dots)=(\sum_{n=2}^\infty x_n, x_1, x_2, x_3, \dots)$ in $\ell^1$. I know that the dual space of $\ell^1$ is $\ell^\infty$ and thus $T^*$ should map from $\ell^\infty$ to $\ell^\infty$. Then I try to see what $T^*$ does to any $a \in \ell^\infty$. $T^*a=aT$ by the definition of adjoint operators in Banach spaces. However, what I want to know is the element to which $T^*$ maps $a$. I don't know how to play with it to find the adjoint.","Find the adjoint operator of $T(x_1,x_2,x_3,\dots)=(\sum_{n=2}^\infty x_n, x_1, x_2, x_3, \dots)$ in $\ell^1$. I know that the dual space of $\ell^1$ is $\ell^\infty$ and thus $T^*$ should map from $\ell^\infty$ to $\ell^\infty$. Then I try to see what $T^*$ does to any $a \in \ell^\infty$. $T^*a=aT$ by the definition of adjoint operators in Banach spaces. However, what I want to know is the element to which $T^*$ maps $a$. I don't know how to play with it to find the adjoint.",,"['functional-analysis', 'operator-theory']"
8,Spectrum of sum of standard operator,Spectrum of sum of standard operator,,"Define on operator $T$ on the space of double sided sequences    $$\ell_2 \left( \mathbb{Z} \right) =  \{ \{ a_n \}_{n = - \infty}^{\infty} \ ; \sum_{n= -\infty} ^{\infty} \left| a_n \right|^2 < \infty \}$$    as following $Tx = y$ so that $y_n =-x_{n-1}+x_n - x_{n+1}$.    Find $T$'s spectrum. I really don't know how to approach this... What I tried so far, is finding eigenvalues and I didn't succeed. Obviously $T = I - S_l - S_r$ where $S_l, S_r$ are left and right shifts, but what else can I draw from this regarding the spectrum? EDIT Since $T = I - S_l - S_r$, the matrix representation of $T$ is  $$T_k^j = \cases{ 1 \qquad ,k=j \\ -1 \qquad ,k-j \in \mp1 }$$ so it's easy to see that $T$ is symmetric thus $T^*=T$... Is this even related?","Define on operator $T$ on the space of double sided sequences    $$\ell_2 \left( \mathbb{Z} \right) =  \{ \{ a_n \}_{n = - \infty}^{\infty} \ ; \sum_{n= -\infty} ^{\infty} \left| a_n \right|^2 < \infty \}$$    as following $Tx = y$ so that $y_n =-x_{n-1}+x_n - x_{n+1}$.    Find $T$'s spectrum. I really don't know how to approach this... What I tried so far, is finding eigenvalues and I didn't succeed. Obviously $T = I - S_l - S_r$ where $S_l, S_r$ are left and right shifts, but what else can I draw from this regarding the spectrum? EDIT Since $T = I - S_l - S_r$, the matrix representation of $T$ is  $$T_k^j = \cases{ 1 \qquad ,k=j \\ -1 \qquad ,k-j \in \mp1 }$$ so it's easy to see that $T$ is symmetric thus $T^*=T$... Is this even related?",,"['functional-analysis', 'operator-theory', 'hilbert-spaces', 'inner-products']"
9,Verify that $\int_0^{\pi/2} f(\sin 2x)\cos x\mathrm dx=\int_0^{\pi/2}f(\cos^2 x)\cos x\mathrm dx$,Verify that,\int_0^{\pi/2} f(\sin 2x)\cos x\mathrm dx=\int_0^{\pi/2}f(\cos^2 x)\cos x\mathrm dx,"Let $f\in C([0,1],\Bbb R)$ . Verify that $\int_0^{\pi/2} f(\sin 2x)\cos x\mathrm dx=\int_0^{\pi/2}f(\cos^2 x)\cos x\mathrm dx$ Hint: observe $\int_{\pi/4}^{\pi/2}f(\sin 2x)\cos x\mathrm dx=\int_0^{\pi/4}f(\sin 2x)\sin x\mathrm dx$ , and make the change of variable $\sin 2x=\cos^2 t$ . Im stuck in this exercise. Following the hint I did $$\int_0^{\pi/2}f(\sin 2x)\cos x\mathrm dx=\int_0^{\pi/4}f(\sin 2x)(\sin x+\cos x)\mathrm dx$$ but I dont know how to continue from here. I get the identities $$\sin x+\cos x=\sqrt 2\sin(x+\pi/4),\quad \mathrm dx=\sqrt{\frac{\cos^2 t}{1+\cos^2 t}}\mathrm dt$$ but I dont see how to transform $\sin x+\cos x$ to something related to the change of variable $\sin 2x=\cos^2 t$ . Some help will be appreciated, thank you.","Let . Verify that Hint: observe , and make the change of variable . Im stuck in this exercise. Following the hint I did but I dont know how to continue from here. I get the identities but I dont see how to transform to something related to the change of variable . Some help will be appreciated, thank you.","f\in C([0,1],\Bbb R) \int_0^{\pi/2} f(\sin 2x)\cos x\mathrm dx=\int_0^{\pi/2}f(\cos^2 x)\cos x\mathrm dx \int_{\pi/4}^{\pi/2}f(\sin 2x)\cos x\mathrm dx=\int_0^{\pi/4}f(\sin 2x)\sin x\mathrm dx \sin 2x=\cos^2 t \int_0^{\pi/2}f(\sin 2x)\cos x\mathrm dx=\int_0^{\pi/4}f(\sin 2x)(\sin x+\cos x)\mathrm dx \sin x+\cos x=\sqrt 2\sin(x+\pi/4),\quad \mathrm dx=\sqrt{\frac{\cos^2 t}{1+\cos^2 t}}\mathrm dt \sin x+\cos x \sin 2x=\cos^2 t","['functional-analysis', 'trigonometric-integrals']"
10,"If a linear operator commutes with right shift in $\ell^p$, then it is continuous","If a linear operator commutes with right shift in , then it is continuous",\ell^p,"I'm studying for a PHD qualifying exam and I got stuck with this problem: Let $p\ge 1$ and $l^p:= L^p(\mathbb{N},\mu)$, where $\mu$ is the counting measure. Let $S:\ell^p\rightarrow \ell^p$ the right shift operator defined by $$S(x_1,x_2,\cdots)=(0,x_1,x_2,\cdots)$$ and suppose that the linear operator $T:\ell^p\rightarrow\ell^p$ satisfies $TS=ST$. Prove that $T$ is continuous. I tried the following approach: First I defined $\{e_j\}_{j=1}^\infty$ as the subset of $l^p$ given by $e_j(n)=\delta_{jn}$, where $\delta_{jn}=1$ if $j=n$ and $\delta_{jn}=0$ otherwise. Noticing that $S^m e_j=e_{j+m}$ I managed to prove $||T(e_{j+1})||_p\le ||T(e_1)||_p$ using the fact that $ST=TS$. I chose to name $C:=||T(e_1)||_p$. Then I let $||x||=1$ and tried to prove $||Tx||_p \le K$ for some constant $K$ by doing the following: $$||Tx||_p=||T(\sum_{n=1}^\infty x_n e_n)||_p = ||\sum_{n=1}^N T(x_ne_n)+T\sum_{n=N+1}^\infty x_n e_n||_p\le ||\sum_{n=1}^N x_n T(e_n)||_p+||T\sum_{n=N+1}^\infty x_ne_n||_p\le \sum_{n=1}^N  ||x_n T(e_n)||_p+||T\sum_{n=N+1}^\infty x_n e_n||_p \le \sum_{n=1}^n |x_n| ||T(e_n)||_p+||T\sum_{n=N+1}^\infty x_n e_n||_p\le \sum_{n=1}^N C|x_n|+ ||T \sum_{n=N+1}^\infty x_n e_n ||_p =C\sum_{n=1}^N |x_n|+ ||T\sum_{n=N+1}^\infty x_n e_n ||_p =C\sum_{n=1}^N |x_n|+||T\sum_{n=N+1}^\infty x_n e_n||_p. $$ Then, if I let $N\rightarrow \infty$ I get $||Tx||_p\le C||x||_1$. This works if $p=1$, but fails for greater values of $p$ since I have no guarantee that $x\in \ell^1$ if $p>1$. I looked for books and articles about linear operators that commute with the shift operator but I only found papers with several lemmas that I couldn't figure out while taking the test with a time limit. I Also tried to generalize the previous discussion by placing $p$ in different parts of the expresions, but I didn't get anything useful. The last approach I tried was noticing that if $L$ is the left shift operator we get $T=LTS$, also $||T||$ and $||S||=1$. Sadly, I didn't get anything nice with that. I'm looking for a hint that allows me to generalize my proof for $p\ge 1$ or a totally different solution or hint to solve it. The problem isn't clear about if I need to prove it for $\ell^\infty$, but I'll be happy if I understand the finite case. Any help is appreciated.","I'm studying for a PHD qualifying exam and I got stuck with this problem: Let $p\ge 1$ and $l^p:= L^p(\mathbb{N},\mu)$, where $\mu$ is the counting measure. Let $S:\ell^p\rightarrow \ell^p$ the right shift operator defined by $$S(x_1,x_2,\cdots)=(0,x_1,x_2,\cdots)$$ and suppose that the linear operator $T:\ell^p\rightarrow\ell^p$ satisfies $TS=ST$. Prove that $T$ is continuous. I tried the following approach: First I defined $\{e_j\}_{j=1}^\infty$ as the subset of $l^p$ given by $e_j(n)=\delta_{jn}$, where $\delta_{jn}=1$ if $j=n$ and $\delta_{jn}=0$ otherwise. Noticing that $S^m e_j=e_{j+m}$ I managed to prove $||T(e_{j+1})||_p\le ||T(e_1)||_p$ using the fact that $ST=TS$. I chose to name $C:=||T(e_1)||_p$. Then I let $||x||=1$ and tried to prove $||Tx||_p \le K$ for some constant $K$ by doing the following: $$||Tx||_p=||T(\sum_{n=1}^\infty x_n e_n)||_p = ||\sum_{n=1}^N T(x_ne_n)+T\sum_{n=N+1}^\infty x_n e_n||_p\le ||\sum_{n=1}^N x_n T(e_n)||_p+||T\sum_{n=N+1}^\infty x_ne_n||_p\le \sum_{n=1}^N  ||x_n T(e_n)||_p+||T\sum_{n=N+1}^\infty x_n e_n||_p \le \sum_{n=1}^n |x_n| ||T(e_n)||_p+||T\sum_{n=N+1}^\infty x_n e_n||_p\le \sum_{n=1}^N C|x_n|+ ||T \sum_{n=N+1}^\infty x_n e_n ||_p =C\sum_{n=1}^N |x_n|+ ||T\sum_{n=N+1}^\infty x_n e_n ||_p =C\sum_{n=1}^N |x_n|+||T\sum_{n=N+1}^\infty x_n e_n||_p. $$ Then, if I let $N\rightarrow \infty$ I get $||Tx||_p\le C||x||_1$. This works if $p=1$, but fails for greater values of $p$ since I have no guarantee that $x\in \ell^1$ if $p>1$. I looked for books and articles about linear operators that commute with the shift operator but I only found papers with several lemmas that I couldn't figure out while taking the test with a time limit. I Also tried to generalize the previous discussion by placing $p$ in different parts of the expresions, but I didn't get anything useful. The last approach I tried was noticing that if $L$ is the left shift operator we get $T=LTS$, also $||T||$ and $||S||=1$. Sadly, I didn't get anything nice with that. I'm looking for a hint that allows me to generalize my proof for $p\ge 1$ or a totally different solution or hint to solve it. The problem isn't clear about if I need to prove it for $\ell^\infty$, but I'll be happy if I understand the finite case. Any help is appreciated.",,"['functional-analysis', 'lp-spaces']"
11,Unitary operator is identity,Unitary operator is identity,,I want to prove that the only positive unitary operator is the identity operator. My attempt has been to been to consider a negative eigenvalue of this linear operator and yield a contradiction. But the operator may not even have an eigenvalue.,I want to prove that the only positive unitary operator is the identity operator. My attempt has been to been to consider a negative eigenvalue of this linear operator and yield a contradiction. But the operator may not even have an eigenvalue.,,['functional-analysis']
12,"Only multiplicative linear functionals on $C[0,1]$",Only multiplicative linear functionals on,"C[0,1]","Consider the Banach space ($C[0,1], \vert\vert\cdot\vert\vert_{\infty}$) of all continuous complex-valued functions on $[0,1]$ w.r.t  $\vert\vert\cdot\vert\vert_{\infty}$. A linear functional $Z: C([0,1]) \to \mathbb{C}$ is called multiplicative if $Z(fg) = Z(f)Z(g)$ for all $f,g \in C([0,1])$. How do I show now that the functionals $$ d_x: C([0,1]) \to \mathbb{C},\quad d_x(f) = f(x)$$ are the only multiplicative functionals on $C([0,1])$ apart from the zero functional?  I am familiar with Riesz representation theorem. Since the underlying set $[0,1]$ is compact, it holds that $C([0,1]) = C_0([0,1]) = C_c([0,1])$: Therefore, according to the Riesz representation theorem every positive linear functional has the form $$Z(f) = \int_{[0,1]} f d\mu.$$ This doesn't help me though since we are not necessarily talking about positive functionals. Should I assume that there is another linear functional that is multiplicative and then, on grounds of Riesz, show that this would lead to a contradiction. What should I consider here?","Consider the Banach space ($C[0,1], \vert\vert\cdot\vert\vert_{\infty}$) of all continuous complex-valued functions on $[0,1]$ w.r.t  $\vert\vert\cdot\vert\vert_{\infty}$. A linear functional $Z: C([0,1]) \to \mathbb{C}$ is called multiplicative if $Z(fg) = Z(f)Z(g)$ for all $f,g \in C([0,1])$. How do I show now that the functionals $$ d_x: C([0,1]) \to \mathbb{C},\quad d_x(f) = f(x)$$ are the only multiplicative functionals on $C([0,1])$ apart from the zero functional?  I am familiar with Riesz representation theorem. Since the underlying set $[0,1]$ is compact, it holds that $C([0,1]) = C_0([0,1]) = C_c([0,1])$: Therefore, according to the Riesz representation theorem every positive linear functional has the form $$Z(f) = \int_{[0,1]} f d\mu.$$ This doesn't help me though since we are not necessarily talking about positive functionals. Should I assume that there is another linear functional that is multiplicative and then, on grounds of Riesz, show that this would lead to a contradiction. What should I consider here?",,"['functional-analysis', 'banach-spaces']"
13,Every linearly independent set can be extended to a basis,Every linearly independent set can be extended to a basis,,Let $E$ be  linear space (infinite dimensional in general). We know by Zorn's lemma that there exists a basis. Now let $S \subset E$ be any linear independent subset. How to prove that it is contained in some basis of $E$? And moreover if $F \subset E$ is subspace then there are linear functional such that $f(F) = 0$ and linear complement to $F$ in $E$. I know how it can be done for finite dimensional spaces but I am always confused when infinite dimension and Zorn's lemma are involved.,Let $E$ be  linear space (infinite dimensional in general). We know by Zorn's lemma that there exists a basis. Now let $S \subset E$ be any linear independent subset. How to prove that it is contained in some basis of $E$? And moreover if $F \subset E$ is subspace then there are linear functional such that $f(F) = 0$ and linear complement to $F$ in $E$. I know how it can be done for finite dimensional spaces but I am always confused when infinite dimension and Zorn's lemma are involved.,,"['linear-algebra', 'functional-analysis']"
14,Is $d^2 / dx^2$ injective in $L^2$?,Is  injective in ?,d^2 / dx^2 L^2,Let $A: L^2(\mathbb R) \to L^2(\mathbb R)$ be a linear operator with domain $domain(A) = H^2(\mathbb R)$ and $A = \frac{d^2}{dx^2}$. The book says that $A$ is injective but I cannot show this. I tried as follows: $A$ is injective iff $Af = 0$ then $f =0$ holds. But $Af = 0$ means $\frac{d^2}{dx^2} f(x) = 0$. Then $f(x)$ can be constant or something like $f(x) = x$ so I could not show that $f=0$. Would you please give me a comment for this?,Let $A: L^2(\mathbb R) \to L^2(\mathbb R)$ be a linear operator with domain $domain(A) = H^2(\mathbb R)$ and $A = \frac{d^2}{dx^2}$. The book says that $A$ is injective but I cannot show this. I tried as follows: $A$ is injective iff $Af = 0$ then $f =0$ holds. But $Af = 0$ means $\frac{d^2}{dx^2} f(x) = 0$. Then $f(x)$ can be constant or something like $f(x) = x$ so I could not show that $f=0$. Would you please give me a comment for this?,,"['real-analysis', 'functional-analysis', 'partial-differential-equations']"
15,Prove the supporting hyperplane theorem for convex sets in Euclidean spaces,Prove the supporting hyperplane theorem for convex sets in Euclidean spaces,,"Let $A\subset \Bbb R^n$ be convex and $p\in\partial A\ne \varnothing$, prove that $\exists v\in\Bbb R^n$ such that  $$v^T(x-p)\ge 0,\quad \forall x\in A.$$ Note that the separation may not be strict (say, if $A$ is itself a hyperplane etc). Since $\text{int}\, A$ and $p$ are disjoint and convex, there exists $v\in\Bbb R^n$ such that $v^T(x-p)>0,\,\forall x\in\text{int}\, A$. Now the major task is to pass this result to $A$ and $p$. Indeed I only need to show that each point in $A$ can be approximated by points in its interior. However this may not be true, since $A$ may have empty interior. Even if $A$ has nonempty interior, it's not immediately clear why points $A$ can be approximated by its interior points, though geometrically it seems obvious. So now there are two major problems: 1). When $A$ has empty interior, then $A$ must lie in a hyperplane. In other words, $A$ has affiné dimension less than $n$. (This would be clearly false in infinite dimensional spaces, say $A$ is the range of an injective compact operator. So I guess the finite dimensionality has a key role to play here.) 2). If $A$ has nonempty interior, then any point in $A$ can be approximated by points in its interior. Looking forward to any help. Cheers. EDIT Just a little caveat: you might be very tempted to use, perhaps implicitly, some ""advanced"" properties of convex sets like ""any convex set in $\Bbb R^n$ is the intersection of affine sets"". But this may lead to circular reasoning, because, for instance, the proof of the property mentioned just above is, AFAIK, based exactly on the supporting hyperplane theorem which we want to prove here.","Let $A\subset \Bbb R^n$ be convex and $p\in\partial A\ne \varnothing$, prove that $\exists v\in\Bbb R^n$ such that  $$v^T(x-p)\ge 0,\quad \forall x\in A.$$ Note that the separation may not be strict (say, if $A$ is itself a hyperplane etc). Since $\text{int}\, A$ and $p$ are disjoint and convex, there exists $v\in\Bbb R^n$ such that $v^T(x-p)>0,\,\forall x\in\text{int}\, A$. Now the major task is to pass this result to $A$ and $p$. Indeed I only need to show that each point in $A$ can be approximated by points in its interior. However this may not be true, since $A$ may have empty interior. Even if $A$ has nonempty interior, it's not immediately clear why points $A$ can be approximated by its interior points, though geometrically it seems obvious. So now there are two major problems: 1). When $A$ has empty interior, then $A$ must lie in a hyperplane. In other words, $A$ has affiné dimension less than $n$. (This would be clearly false in infinite dimensional spaces, say $A$ is the range of an injective compact operator. So I guess the finite dimensionality has a key role to play here.) 2). If $A$ has nonempty interior, then any point in $A$ can be approximated by points in its interior. Looking forward to any help. Cheers. EDIT Just a little caveat: you might be very tempted to use, perhaps implicitly, some ""advanced"" properties of convex sets like ""any convex set in $\Bbb R^n$ is the intersection of affine sets"". But this may lead to circular reasoning, because, for instance, the proof of the property mentioned just above is, AFAIK, based exactly on the supporting hyperplane theorem which we want to prove here.",,"['functional-analysis', 'convex-analysis', 'convex-optimization', 'convex-geometry']"
16,Variational formulation of a PDE with a Dirac's Delta?,Variational formulation of a PDE with a Dirac's Delta?,,"I have the following boundary value problem  $$	\begin{cases} 		-u'' = \delta_{0}  \quad \text{in }(-1,1)  \subset \mathbb{R}   \\[2ex] 		u(-1) = 0, \quad u(1) = 0 	\end{cases}\tag{A} $$ where $ \delta_{0} \in \mathscr{D}^{''}(-1,1) $ is the Dirac delta in $0$. The goal is to obtain a variational formulation of (A), to demonstrate that there is a unique solution $ u \in H_{0}^{1}(-1,1) $ and to derive the analitic expression of the solution $u$. I note that $ H_{0}^{1}(-1,1) \subset C([-1,1]) $, so that $ v \in H_{0}^{1} $. The following should hold: $ \langle \delta_{0}, v \rangle = v(0) $, and $ \lvert\langle \delta_{0}, v \rangle\rvert = \lvert v(0) \rvert \leq \displaystyle \max_{x \in [-1,1]} \lvert v(x) \rvert = \lVert v \rVert_{C([-1,1])} \leq M\lVert v \rVert_{H_{0}^{1}(-1,1)} $, where M is a constant. This should help, but here it is where I'm really stuck. Although I guess this is not a difficult problem, I'm getting stuck with some basic issues. This is a homework problem, and I have very few skills with Hilbert spaces and the application of functional analysis to PDEs. Any guide will be much appreciatted.","I have the following boundary value problem  $$	\begin{cases} 		-u'' = \delta_{0}  \quad \text{in }(-1,1)  \subset \mathbb{R}   \\[2ex] 		u(-1) = 0, \quad u(1) = 0 	\end{cases}\tag{A} $$ where $ \delta_{0} \in \mathscr{D}^{''}(-1,1) $ is the Dirac delta in $0$. The goal is to obtain a variational formulation of (A), to demonstrate that there is a unique solution $ u \in H_{0}^{1}(-1,1) $ and to derive the analitic expression of the solution $u$. I note that $ H_{0}^{1}(-1,1) \subset C([-1,1]) $, so that $ v \in H_{0}^{1} $. The following should hold: $ \langle \delta_{0}, v \rangle = v(0) $, and $ \lvert\langle \delta_{0}, v \rangle\rvert = \lvert v(0) \rvert \leq \displaystyle \max_{x \in [-1,1]} \lvert v(x) \rvert = \lVert v \rVert_{C([-1,1])} \leq M\lVert v \rVert_{H_{0}^{1}(-1,1)} $, where M is a constant. This should help, but here it is where I'm really stuck. Although I guess this is not a difficult problem, I'm getting stuck with some basic issues. This is a homework problem, and I have very few skills with Hilbert spaces and the application of functional analysis to PDEs. Any guide will be much appreciatted.",,"['functional-analysis', 'partial-differential-equations', 'hilbert-spaces', 'dirac-delta']"
17,Subdifferential - equivalent definitions?,Subdifferential - equivalent definitions?,,"I've been reading an article on Clarke critical values of subanalytic Lipschitz functions. There I've come across the following definition(s) of subdifferential: $$f: U \to \mathbb{R}^n, \ \ \ \ \emptyset \neq U \subset \mathbb{R}^n$$ $f$ is locally Lipshitz continuous Let $$x \in U:$$ The Frechet subdifferential: $$ \hat{\partial} f(x) = \left\{x^* \in \mathbb{R}^n \ : \ \liminf_{y \to x, y \neq x} \frac{f(y) - f(x) - \langle x^*, y-x \rangle}{||y - x||} \ge 0 \right\}$$ $$x^* \in \partial f \iff \exists x_n \in U, \exists x_n^* \in  \hat{\partial} f(x_n) : x_n \to x, x_n ^* \to x^*, \ \ n \to \infty $$ and finally, $$\partial^o f(x) = \overline{\rm conv}\partial f(x)$$ is called the Clarke subdifferential of $f$ at $x$. My question is - is $\partial^o f(x)$ the same set as this one defined in Clarke's paper as subdifferential of $f$ at $x$? $$S_1 = \{ s \in \mathbb{R}^n \ : \ \langle v, s \rangle \le f(x+v) - f(x) \ \forall v \in \mathbb{R}^n \}$$ I know that for convex functions, the above set is equal to: $$ S_2 = \{s\in \mathbb{R}^n \ : \ \langle s, d \rangle \le f'(x, d) \ \forall d \in \mathbb{R}^n \} $$ Here $$ f'(x, d) = \lim_{t \to 0} \frac{f(x + td) - f(x)}{t}$$ I suppose it would also be helpful to note that $S_1,S_2$ are closed, convex. Could you help me find a link between $S_1$ and $ \partial^of(x)?$","I've been reading an article on Clarke critical values of subanalytic Lipschitz functions. There I've come across the following definition(s) of subdifferential: $$f: U \to \mathbb{R}^n, \ \ \ \ \emptyset \neq U \subset \mathbb{R}^n$$ $f$ is locally Lipshitz continuous Let $$x \in U:$$ The Frechet subdifferential: $$ \hat{\partial} f(x) = \left\{x^* \in \mathbb{R}^n \ : \ \liminf_{y \to x, y \neq x} \frac{f(y) - f(x) - \langle x^*, y-x \rangle}{||y - x||} \ge 0 \right\}$$ $$x^* \in \partial f \iff \exists x_n \in U, \exists x_n^* \in  \hat{\partial} f(x_n) : x_n \to x, x_n ^* \to x^*, \ \ n \to \infty $$ and finally, $$\partial^o f(x) = \overline{\rm conv}\partial f(x)$$ is called the Clarke subdifferential of $f$ at $x$. My question is - is $\partial^o f(x)$ the same set as this one defined in Clarke's paper as subdifferential of $f$ at $x$? $$S_1 = \{ s \in \mathbb{R}^n \ : \ \langle v, s \rangle \le f(x+v) - f(x) \ \forall v \in \mathbb{R}^n \}$$ I know that for convex functions, the above set is equal to: $$ S_2 = \{s\in \mathbb{R}^n \ : \ \langle s, d \rangle \le f'(x, d) \ \forall d \in \mathbb{R}^n \} $$ Here $$ f'(x, d) = \lim_{t \to 0} \frac{f(x + td) - f(x)}{t}$$ I suppose it would also be helpful to note that $S_1,S_2$ are closed, convex. Could you help me find a link between $S_1$ and $ \partial^of(x)?$",,"['real-analysis', 'functional-analysis', 'convex-analysis', 'non-smooth-analysis']"
18,Understanding definition of tensor product,Understanding definition of tensor product,,"The definition I have of a tensor product of vector finite dimensional vector spaces $V,W$ over a field $F$ is as follows: Let $v_1, ..., v_m$ be a basis for $V$ and let $w_1,...,w_n$ be a basis for $W$. We define $V \otimes W$ to be the set of formal linear combinations of the mn symbols $v_i \otimes w_j$. That is, a typical element of $V \otimes W$ is $$\sum c_{ij}(v_i \otimes w_j).$$ The space $V \otimes W$ is clearly a finite dimensional vector space of dimension mn. We define bilinear map $$B: V \times W \to V \otimes W$$ here is the formula $$B(\sum a_iv_i, \sum b_jw_j) = \sum_{i,j}a_ib_j(v_i \otimes w_j). $$ Why does $V \otimes W$ have to be a formal linear combinations of symbols $v_{i} \otimes w_j$, what would be wrong in defining $V \otimes W$ simply as a linear combination of symbols $v_i \otimes w_j$? Thanks.","The definition I have of a tensor product of vector finite dimensional vector spaces $V,W$ over a field $F$ is as follows: Let $v_1, ..., v_m$ be a basis for $V$ and let $w_1,...,w_n$ be a basis for $W$. We define $V \otimes W$ to be the set of formal linear combinations of the mn symbols $v_i \otimes w_j$. That is, a typical element of $V \otimes W$ is $$\sum c_{ij}(v_i \otimes w_j).$$ The space $V \otimes W$ is clearly a finite dimensional vector space of dimension mn. We define bilinear map $$B: V \times W \to V \otimes W$$ here is the formula $$B(\sum a_iv_i, \sum b_jw_j) = \sum_{i,j}a_ib_j(v_i \otimes w_j). $$ Why does $V \otimes W$ have to be a formal linear combinations of symbols $v_{i} \otimes w_j$, what would be wrong in defining $V \otimes W$ simply as a linear combination of symbols $v_i \otimes w_j$? Thanks.",,"['functional-analysis', 'vector-spaces', 'definition', 'tensor-products']"
19,Arzela-Ascoli variant for $C^1$ functions,Arzela-Ascoli variant for  functions,C^1,"Let $K$ be a compact subset of a smooth manifold, and for simplicity let $d(\cdot,\cdot)$ be any metric on $K$. Let $C^1(K)$ be the set of $C^1$ real-valued functions on $K$ equipped with the norm $$\|f\|_{C^1}:= \max \{\|f\|_\infty, \|df\|_\infty\},$$ Definition: Let $\mathcal{F}$ be a subset of $C^1(K)$. $\mathcal{F}$ is bounded if $$\exists M > 0: \forall f \in \mathcal{F}: \|f\|_{C^1} < M.$$ $\mathcal{F}$ is equicontinuous if for every $x \in K$ and $\epsilon > 0$ there exists $\delta > 0$ such that $$d(x,y) < \delta \implies \max\{|f(x)-f(y)|, \|df(x)-df(y)\|\} < \epsilon.$$   $\mathcal{F}$ is closed if it is a closed subset of $C^1(K)$. I believe the following variant of the Arzela-Ascoli theorem is true: Theorem: A subset $\mathcal{F}$ of $C^1(K)$ is compact (in the topology induced by the $C^1$ norm) if and only if $\mathcal{F}$ is closed, bounded, and equicontinuous. From reading the proof of the Arzela-Ascoli theorem, it seems to me that one could prove this variant with straightforward modifications. Can anyone please provide a reference containing this generalization (or point out that I am mistaken)?","Let $K$ be a compact subset of a smooth manifold, and for simplicity let $d(\cdot,\cdot)$ be any metric on $K$. Let $C^1(K)$ be the set of $C^1$ real-valued functions on $K$ equipped with the norm $$\|f\|_{C^1}:= \max \{\|f\|_\infty, \|df\|_\infty\},$$ Definition: Let $\mathcal{F}$ be a subset of $C^1(K)$. $\mathcal{F}$ is bounded if $$\exists M > 0: \forall f \in \mathcal{F}: \|f\|_{C^1} < M.$$ $\mathcal{F}$ is equicontinuous if for every $x \in K$ and $\epsilon > 0$ there exists $\delta > 0$ such that $$d(x,y) < \delta \implies \max\{|f(x)-f(y)|, \|df(x)-df(y)\|\} < \epsilon.$$   $\mathcal{F}$ is closed if it is a closed subset of $C^1(K)$. I believe the following variant of the Arzela-Ascoli theorem is true: Theorem: A subset $\mathcal{F}$ of $C^1(K)$ is compact (in the topology induced by the $C^1$ norm) if and only if $\mathcal{F}$ is closed, bounded, and equicontinuous. From reading the proof of the Arzela-Ascoli theorem, it seems to me that one could prove this variant with straightforward modifications. Can anyone please provide a reference containing this generalization (or point out that I am mistaken)?",,"['real-analysis', 'functional-analysis', 'reference-request']"
20,I need a resource for self-studying c*algebras.,I need a resource for self-studying c*algebras.,,"I have just started my phd program this semester, I have not worked before on $C^*-$algebras, my master was on Geometry. In my university only reading courses are offered. I have to learn $C^*-$algebras. I have not taken any courses on functional analysis too. I am studying the book "" $C^*-$algebras and their automorphism groups"" by Pedersen, I usually google each topic and take a look at Book by Murphy too. But I could not find a good resource that give me a good prospective, for example how should I use Gelfand representation in problems. I mean I am learning topics, but I cannot learn with deep depth that I can use stuff like tools in my work. And Also I undestood that Functional analysis is a priority for $C^*-$algebras, but I cannot find a good book for understanding deeply materials like weak and weak$^*$ topology. When I read theorems I understand them, but I could not understand these topics as well as I give ideas for solving theorems on my own, and I cannot enjoy studying. I would appreciate if you recommend me some good books for self studying both Functional analysis and $C^*-$algebras.","I have just started my phd program this semester, I have not worked before on $C^*-$algebras, my master was on Geometry. In my university only reading courses are offered. I have to learn $C^*-$algebras. I have not taken any courses on functional analysis too. I am studying the book "" $C^*-$algebras and their automorphism groups"" by Pedersen, I usually google each topic and take a look at Book by Murphy too. But I could not find a good resource that give me a good prospective, for example how should I use Gelfand representation in problems. I mean I am learning topics, but I cannot learn with deep depth that I can use stuff like tools in my work. And Also I undestood that Functional analysis is a priority for $C^*-$algebras, but I cannot find a good book for understanding deeply materials like weak and weak$^*$ topology. When I read theorems I understand them, but I could not understand these topics as well as I give ideas for solving theorems on my own, and I cannot enjoy studying. I would appreciate if you recommend me some good books for self studying both Functional analysis and $C^*-$algebras.",,"['functional-analysis', 'reference-request', 'book-recommendation', 'c-star-algebras']"
21,"Fourier method heat equation, partial time derivative","Fourier method heat equation, partial time derivative",,"Let's consider the heat equation $u_t-u_{xx}=0$. A simple method to solve this PDE is to do a Fourier transform of the equation and get the ODE $\hat{u_t} + x^2 \hat u_{xx}=0$. We can solve this, do a back transformation and obtain a solution to our original PDE. Most people don't care why $\widehat{u_t} = \frac{d}{dt} \hat u$ is justified. When one writes it out, we get into the situation that we need to interchange limit with integration. This is not trivially justified. I would like to know in which situations we are justified in doing this. Someone gave me a hint to consider the question from another viewpoint, namely to consider the spatial Fourier transform as a Banach space values function and consider generalized derivatives, but this idea wasn't made more precise.","Let's consider the heat equation $u_t-u_{xx}=0$. A simple method to solve this PDE is to do a Fourier transform of the equation and get the ODE $\hat{u_t} + x^2 \hat u_{xx}=0$. We can solve this, do a back transformation and obtain a solution to our original PDE. Most people don't care why $\widehat{u_t} = \frac{d}{dt} \hat u$ is justified. When one writes it out, we get into the situation that we need to interchange limit with integration. This is not trivially justified. I would like to know in which situations we are justified in doing this. Someone gave me a hint to consider the question from another viewpoint, namely to consider the spatial Fourier transform as a Banach space values function and consider generalized derivatives, but this idea wasn't made more precise.",,"['functional-analysis', 'partial-differential-equations', 'heat-equation', 'fourier-transform']"
22,$K_0$ groups of direct sum $A\bigoplus B$ and the $K_0$ group of $C_0(\mathbb{R})$,groups of direct sum  and the  group of,K_0 A\bigoplus B K_0 C_0(\mathbb{R}),"I'm reading Rordam's book ""An Introduction to K-theory for c*-algebras"". Given two C*-algebras $A,B$, the book proves that $K_0(A\bigoplus B)\cong K_0(A)\bigoplus K_0(B)$. I'm assuming that the book means $A\bigoplus B$ to be a direct sum over C*-algebras (as opposed to vector spaces). Now to calculate $K_0(C_0(\mathbb{R}))$.  The book has a ""warm up"" exercise that shows for $X$ Hausdorff with separation $X=X_1\cup X_2$, then $C_0(X)\cong C_0(X_1)\bigoplus C_0(X_2)$ (as both vector spaces and c*-algebras).  I'm then supposed to use that result and the knowledge that $K_0(C(S^1))\cong\mathbb{Z}$ to conclude that $K_0(C_0(\mathbb{R}))=0$. Aside from $S^1$ being connected so the warm up is a bit misleading, my issue is that I can show that $C(S^1)\cong C_0(\mathbb{R})\bigoplus\mathbb{C}$ as vector spaces, but the direct sum does not hold as algebras (in partciular $C_0(\mathbb{R})$ is not unital so its unitization is not isomorphic to itself plus a copy of $\mathbb{C}$). I then am not sure how one is supposed to conclude that $K_0(C_0(\mathbb{R}))\cong 0$ or where I am misunderstanding something.","I'm reading Rordam's book ""An Introduction to K-theory for c*-algebras"". Given two C*-algebras $A,B$, the book proves that $K_0(A\bigoplus B)\cong K_0(A)\bigoplus K_0(B)$. I'm assuming that the book means $A\bigoplus B$ to be a direct sum over C*-algebras (as opposed to vector spaces). Now to calculate $K_0(C_0(\mathbb{R}))$.  The book has a ""warm up"" exercise that shows for $X$ Hausdorff with separation $X=X_1\cup X_2$, then $C_0(X)\cong C_0(X_1)\bigoplus C_0(X_2)$ (as both vector spaces and c*-algebras).  I'm then supposed to use that result and the knowledge that $K_0(C(S^1))\cong\mathbb{Z}$ to conclude that $K_0(C_0(\mathbb{R}))=0$. Aside from $S^1$ being connected so the warm up is a bit misleading, my issue is that I can show that $C(S^1)\cong C_0(\mathbb{R})\bigoplus\mathbb{C}$ as vector spaces, but the direct sum does not hold as algebras (in partciular $C_0(\mathbb{R})$ is not unital so its unitization is not isomorphic to itself plus a copy of $\mathbb{C}$). I then am not sure how one is supposed to conclude that $K_0(C_0(\mathbb{R}))\cong 0$ or where I am misunderstanding something.",,"['functional-analysis', 'k-theory']"
23,Conflicting definitions of continuity (strict or non-strict inequality)?,Conflicting definitions of continuity (strict or non-strict inequality)?,,"On page 97 of Kreyzig's functional analysis book he provides a proof that a linear operator $T$ is continuous if and only if it is bounded. When proving that $T$ is continuous implies that $T$ is bounded he says that if we assume $T$ is continuous at some $x_0$ then for any $\varepsilon >0$ there exists a $\delta > 0$ such that $\Vert Tx - Tx_0 \Vert \le \varepsilon$ for all $x$ satisfying $\Vert x - x_0 \Vert \le \delta$. But on the previous page where he gave the definition of a continuous operator $T$ he used strictly less inequalities for $\varepsilon$ and $\delta$. That is, $T$ is continuous at some $x_0$ means that for any $\varepsilon >0$ there exists a $\delta > 0$ such that $\Vert Tx - Tx_0 \Vert < \varepsilon$ for all $x$ satisfying $\Vert x - x_0 \Vert < \delta$. How can he extend the strict inequality to a less than or equal to inequality?","On page 97 of Kreyzig's functional analysis book he provides a proof that a linear operator $T$ is continuous if and only if it is bounded. When proving that $T$ is continuous implies that $T$ is bounded he says that if we assume $T$ is continuous at some $x_0$ then for any $\varepsilon >0$ there exists a $\delta > 0$ such that $\Vert Tx - Tx_0 \Vert \le \varepsilon$ for all $x$ satisfying $\Vert x - x_0 \Vert \le \delta$. But on the previous page where he gave the definition of a continuous operator $T$ he used strictly less inequalities for $\varepsilon$ and $\delta$. That is, $T$ is continuous at some $x_0$ means that for any $\varepsilon >0$ there exists a $\delta > 0$ such that $\Vert Tx - Tx_0 \Vert < \varepsilon$ for all $x$ satisfying $\Vert x - x_0 \Vert < \delta$. How can he extend the strict inequality to a less than or equal to inequality?",,"['functional-analysis', 'continuity', 'definition']"
24,Norm of self-adjoint member of $C^*$-algebra,Norm of self-adjoint member of -algebra,C^*,"This question arose from the proof of proposition $1.11(e)$ in chapter $8$ of  John B. Conway's A Course in Functional Analysis .  This portion of the proposition can be stated: Let $\mathscr{A}$ be a $C^*$-algebra, and let $a\in\mathscr{A}$ be given. If $a=a^*$, then $\|a\|=r(a)$. (Here, $r(a)$ denotes the spectral radius of $a$.) The proof, as stated in the book, proceeds as follows: Since $a^*=a$, $\|a^2\|=\|a^*a\|=\|a\|^2$; by induction, $\|a^{2n}\|=\|a\|^{2n}$ for $n\geq1$  That is, $\|a^{2n}\|^{1/2n}=\|a\|$ for $n\geq1$.  Hence $r(a)=\lim\|a^{2n}\|^{1/2n}=\|a\|$. Now I was able to show by induction that $$ \|a^{2^n}\|=\|a\|^{2^n} \qquad (n\geq1),$$ from which the result follows, but I could not prove it as it is stated in the book. So my question is:  How can we prove (presumably by induction) that $\|a^{2n}\|^{1/2n}=\|a\|$ for $n\geq1$? Is this simply an error in the book, or can it be done?","This question arose from the proof of proposition $1.11(e)$ in chapter $8$ of  John B. Conway's A Course in Functional Analysis .  This portion of the proposition can be stated: Let $\mathscr{A}$ be a $C^*$-algebra, and let $a\in\mathscr{A}$ be given. If $a=a^*$, then $\|a\|=r(a)$. (Here, $r(a)$ denotes the spectral radius of $a$.) The proof, as stated in the book, proceeds as follows: Since $a^*=a$, $\|a^2\|=\|a^*a\|=\|a\|^2$; by induction, $\|a^{2n}\|=\|a\|^{2n}$ for $n\geq1$  That is, $\|a^{2n}\|^{1/2n}=\|a\|$ for $n\geq1$.  Hence $r(a)=\lim\|a^{2n}\|^{1/2n}=\|a\|$. Now I was able to show by induction that $$ \|a^{2^n}\|=\|a\|^{2^n} \qquad (n\geq1),$$ from which the result follows, but I could not prove it as it is stated in the book. So my question is:  How can we prove (presumably by induction) that $\|a^{2n}\|^{1/2n}=\|a\|$ for $n\geq1$? Is this simply an error in the book, or can it be done?",,"['functional-analysis', 'operator-algebras', 'c-star-algebras']"
25,Why isn't $\ell^p$ locally convex for $0<p<1$?,Why isn't  locally convex for ?,\ell^p 0<p<1,"I believe we have to distinguish the finite-dimensional from the infinite dimensional case. Regardless, if $0<p<1$, $\|x\|_p := (\sum |x_i|^p)^{\frac 1 p}$ is not a norm as it fails to satisfy the triangle inequality. That's why we use instead the metric $d(0,x) = \sum |x_i|^p = \|x\|_p^p$ to define the topology and remark that this is also not a norm since it is homogeneous of degree $p$. I do not know how the balls defined by this metric look like, but they ought to be convex because the metric satisfies the triangle inequality, right? In other words, they can't be the corresponding astroid-shaped superellipses for $0<p<1$. So why is $\ell^p$ said to not be locally convex (at least in the infinite dimensional case)?","I believe we have to distinguish the finite-dimensional from the infinite dimensional case. Regardless, if $0<p<1$, $\|x\|_p := (\sum |x_i|^p)^{\frac 1 p}$ is not a norm as it fails to satisfy the triangle inequality. That's why we use instead the metric $d(0,x) = \sum |x_i|^p = \|x\|_p^p$ to define the topology and remark that this is also not a norm since it is homogeneous of degree $p$. I do not know how the balls defined by this metric look like, but they ought to be convex because the metric satisfies the triangle inequality, right? In other words, they can't be the corresponding astroid-shaped superellipses for $0<p<1$. So why is $\ell^p$ said to not be locally convex (at least in the infinite dimensional case)?",,"['functional-analysis', 'convex-analysis', 'normed-spaces', 'lp-spaces', 'topological-vector-spaces']"
26,Proof that group of invertible elements in a Banach algebra have 1 or infinite connected components?,Proof that group of invertible elements in a Banach algebra have 1 or infinite connected components?,,"I'm trying to reconcile this proof that I've read that a group of invertible elements in a commutative (complex) Banach algebra have 1 or infinite connected components with this example I'm looking at.  The example is the Banach algebra, $\mathcal B$ with basis elements $\{a,e\}$, with $a^2=e$.  The singular values of this Banach algebra are, I think $\{\lambda(a+e):\lambda\in\mathbb C\}$ and $\{\lambda(a-e):\lambda\in\mathbb C\}$, which should create $4$ quadrants. The gist of the proof is that $\sigma(a)=\{1,-1\}$, so define a logarithm branch on $\Omega\supseteq\sigma(a)$.  Extend this to $\mathcal B$, basically by its power series.  Now $a'=\log(a)\in\mathcal B$, so that $\exp(\lambda a'),\lambda\in[0,1]$ is a path connecting $a$ to $e$. I believe this proof, but it's hard to see where it breaks down and crosses the singular boundary on my example, because it's hard to get my hands on the extension of $\log$ and $\exp$.  Any help?  What am I missing?","I'm trying to reconcile this proof that I've read that a group of invertible elements in a commutative (complex) Banach algebra have 1 or infinite connected components with this example I'm looking at.  The example is the Banach algebra, $\mathcal B$ with basis elements $\{a,e\}$, with $a^2=e$.  The singular values of this Banach algebra are, I think $\{\lambda(a+e):\lambda\in\mathbb C\}$ and $\{\lambda(a-e):\lambda\in\mathbb C\}$, which should create $4$ quadrants. The gist of the proof is that $\sigma(a)=\{1,-1\}$, so define a logarithm branch on $\Omega\supseteq\sigma(a)$.  Extend this to $\mathcal B$, basically by its power series.  Now $a'=\log(a)\in\mathcal B$, so that $\exp(\lambda a'),\lambda\in[0,1]$ is a path connecting $a$ to $e$. I believe this proof, but it's hard to see where it breaks down and crosses the singular boundary on my example, because it's hard to get my hands on the extension of $\log$ and $\exp$.  Any help?  What am I missing?",,"['functional-analysis', 'banach-algebras']"
27,spectrum of an operator restricted to an invariant subspace,spectrum of an operator restricted to an invariant subspace,,"Let $X$ be an infinite-dimensional real Banach space and $T\in\mathcal{L}(X)$ a continuous linear operator acting on $X$.  Suppose $W$ is a finite-codimensional $T$-invariant closed subspace of $X$, and write $S=T|_W\in\mathcal{L}(W)$ for the restriction of $T$ to $W$. Denote by $\sigma(T)$ the spectrum of the complexification $T_\mathbb{C}\in\mathcal{L}(X_\mathbb{C})$, and similarly for $S$.  In other words, $\lambda\in\sigma(T)$ if and only if $(\lambda-T_\mathbb{C})$ is not invertible in the complex operator algebra $\mathcal{L}(X_\mathbb{C})$. Conjecture. If $\sigma(T)\subseteq\mathbb{R}$ then $\sigma(S)\subseteq\mathbb{R}$. Note. For my purposes we can assume $X$ is reflexive and that $W=\overline{p(T)X}$ for some polynomial $p(t)\in\mathbb{R}[t]$ (with real coefficients). I don't want to make any additional assumptions about $W$ or $S$.  However, if necessary we could make additional assumptions on $X$ and/or $T$, for instance that $X$ is a Hilbert space and that $T$ is selfadjoint.  Of course, the fewer assumptions the better.","Let $X$ be an infinite-dimensional real Banach space and $T\in\mathcal{L}(X)$ a continuous linear operator acting on $X$.  Suppose $W$ is a finite-codimensional $T$-invariant closed subspace of $X$, and write $S=T|_W\in\mathcal{L}(W)$ for the restriction of $T$ to $W$. Denote by $\sigma(T)$ the spectrum of the complexification $T_\mathbb{C}\in\mathcal{L}(X_\mathbb{C})$, and similarly for $S$.  In other words, $\lambda\in\sigma(T)$ if and only if $(\lambda-T_\mathbb{C})$ is not invertible in the complex operator algebra $\mathcal{L}(X_\mathbb{C})$. Conjecture. If $\sigma(T)\subseteq\mathbb{R}$ then $\sigma(S)\subseteq\mathbb{R}$. Note. For my purposes we can assume $X$ is reflexive and that $W=\overline{p(T)X}$ for some polynomial $p(t)\in\mathbb{R}[t]$ (with real coefficients). I don't want to make any additional assumptions about $W$ or $S$.  However, if necessary we could make additional assumptions on $X$ and/or $T$, for instance that $X$ is a Hilbert space and that $T$ is selfadjoint.  Of course, the fewer assumptions the better.",,"['functional-analysis', 'operator-theory', 'hilbert-spaces', 'banach-spaces', 'spectral-theory']"
28,The point spectrum and residual spectrum of an operator on $l_2$ related to backward shift,The point spectrum and residual spectrum of an operator on  related to backward shift,l_2,"I have a problem with the spectrum of this operator: $(Tx)_1 = x_2$ $(Tx)_2 = x_1$ $(Tx)_n = \frac{1}{n}x_{n+1}$ with $n\ge3$ Find the $||T||$, the point spectrum $\sigma_P(T)$ and $\sigma_P(T^{\dagger})$ and the residual spectrum $\sigma_{\rho}(T)$ and $\sigma_{\rho}(T^{\dagger})$. For the $||T||$ I have found: $||T|| = 1$ Then for the point spectrum I try with: $\lambda x_1 = x_2$ $\lambda x_2 = x_1$ $\lambda x_n = \frac{1}{n}x_{n+1}$ I found easily that some eigenvalues are $\lambda_n =0,\pm1$, but I have a problem, when I study the case $\lambda \neq\lambda_n$, I found the eigenvector: $v_{\lambda} = (x_1,\lambda x_1, x_3, 3\lambda x_3, 3\cdot4\lambda^2x_3,....)$ But I don't understand the condition that the $\lambda$ have to satisfies for $v_{\lambda} \in \ell_2 $ For adjoint $T^{\dagger}$ I have found: $\sigma_P(T^{\dagger}) = \{\lambda = \pm 1 \}$ $\sigma_{\rho}(T^{\dagger}) = \{z = 0\}$ It is correct?","I have a problem with the spectrum of this operator: $(Tx)_1 = x_2$ $(Tx)_2 = x_1$ $(Tx)_n = \frac{1}{n}x_{n+1}$ with $n\ge3$ Find the $||T||$, the point spectrum $\sigma_P(T)$ and $\sigma_P(T^{\dagger})$ and the residual spectrum $\sigma_{\rho}(T)$ and $\sigma_{\rho}(T^{\dagger})$. For the $||T||$ I have found: $||T|| = 1$ Then for the point spectrum I try with: $\lambda x_1 = x_2$ $\lambda x_2 = x_1$ $\lambda x_n = \frac{1}{n}x_{n+1}$ I found easily that some eigenvalues are $\lambda_n =0,\pm1$, but I have a problem, when I study the case $\lambda \neq\lambda_n$, I found the eigenvector: $v_{\lambda} = (x_1,\lambda x_1, x_3, 3\lambda x_3, 3\cdot4\lambda^2x_3,....)$ But I don't understand the condition that the $\lambda$ have to satisfies for $v_{\lambda} \in \ell_2 $ For adjoint $T^{\dagger}$ I have found: $\sigma_P(T^{\dagger}) = \{\lambda = \pm 1 \}$ $\sigma_{\rho}(T^{\dagger}) = \{z = 0\}$ It is correct?",,"['functional-analysis', 'operator-theory', 'hilbert-spaces', 'spectral-theory']"
29,Norms inequality in a sequence space,Norms inequality in a sequence space,,"Let  $1 \leq p<q \leq \infty$ (p an q are not related) Let $\Phi$ be the vector space of all sequences with at most finitely many nonzero elements, meaning $\Phi=\{\{x_n\}_{n=1}^\infty|$ there is $n_o$ such that $x_n=0$ whenever $n\leq n_0\}$ i want to show that $\|x\|_q\leq \|x\|_p$ and there exist no C such that $\|x\|_p \leq C\|x\|_q$ by showing that $\sup_{0\neq x\in \Psi} \frac{\|x\|_p}{\|x\|_q}=\infty$ If $\|x\|_p=1$ then $|x_i|\leq 1 \forall x_i$ and then $|x_i|^q<|x_i|^p$ and $\displaystyle\sum_{i=1}^{n_0} |x_i|^q \leq \displaystyle\sum_{i=1}^{n_0} |x_i|^p$. I am not sure if i can simply apply the roots hereon both sides. is this at all a correct direction? Is there another way to go about it?","Let  $1 \leq p<q \leq \infty$ (p an q are not related) Let $\Phi$ be the vector space of all sequences with at most finitely many nonzero elements, meaning $\Phi=\{\{x_n\}_{n=1}^\infty|$ there is $n_o$ such that $x_n=0$ whenever $n\leq n_0\}$ i want to show that $\|x\|_q\leq \|x\|_p$ and there exist no C such that $\|x\|_p \leq C\|x\|_q$ by showing that $\sup_{0\neq x\in \Psi} \frac{\|x\|_p}{\|x\|_q}=\infty$ If $\|x\|_p=1$ then $|x_i|\leq 1 \forall x_i$ and then $|x_i|^q<|x_i|^p$ and $\displaystyle\sum_{i=1}^{n_0} |x_i|^q \leq \displaystyle\sum_{i=1}^{n_0} |x_i|^p$. I am not sure if i can simply apply the roots hereon both sides. is this at all a correct direction? Is there another way to go about it?",,"['functional-analysis', 'inequality', 'convergence-divergence', 'normed-spaces']"
30,Are probability measures weak-* closed?,Are probability measures weak-* closed?,,"Non-duplicates This is in a different setting, and this only deals with compact spaces which is the easy case. Now for the question. Let $X$ be a locally compact Hausdorff space. $\mathcal{C}_0(X)$ is the set of continuous functions $f:X\to\mathbb{R}$ that go to zero at infinity, i.e. are less than any $\epsilon$ outside an appropriate compact set. One of the many Riesz representation theory gives an isometric isomorphism between $\mathcal{C}_0(X)$ and the space $M(X)$ of all complex regular Borel measures on $X$. So Banach-Alaoglu shows the ball of $M(X)$ is compact in the weak-* topology. Consider $P(X)\subseteq\{\mu:\|\mu\|\leq1\}$ the set of positive probability measures, i.e. $P(X)=\{\mu\in M(X):\mu\geq0,\mu(X)=1\}$. Is $P(X)$ weak-* compact? $P(X)$ is contained in the ball, so if it is weak-* closed, it is closed in a compact set, hence compact. If $X$ is compact, certainly $f\equiv1$ is in $\mathcal{C}_0$, so if $\mu_\alpha$ is a net of probabilities converging to $\mu$, $1=\mu_n(X)=\mu_n(f)\to\mu(f)=\mu(X)$, and since limits preserve inequalities for any $g\geq0$ $\mu(g)\geq0$, so $\mu\in P(X)$. The argument for positivity works in general. The hard part with noncompact spaces is $\mu(X)=1$. It is fairly easy to prove $\mu(X)\leq1$: one merely has to work with $\epsilon$s and $\delta$s in an appropriate way. But I couldn't find any way to conclude $\mu(X)\geq1$. Proceeding by contradiction with $\mu(X)=1-\delta$ doesn't lead me anywhere. I googled for 1-2 hours and found nothing: wherever the weak-* closure was mentioned, it was just assumed. So how do I prove this last bit? Is $\mu(X)\geq1$? Or are there cases where it isn't? And if so, counterexamples?","Non-duplicates This is in a different setting, and this only deals with compact spaces which is the easy case. Now for the question. Let $X$ be a locally compact Hausdorff space. $\mathcal{C}_0(X)$ is the set of continuous functions $f:X\to\mathbb{R}$ that go to zero at infinity, i.e. are less than any $\epsilon$ outside an appropriate compact set. One of the many Riesz representation theory gives an isometric isomorphism between $\mathcal{C}_0(X)$ and the space $M(X)$ of all complex regular Borel measures on $X$. So Banach-Alaoglu shows the ball of $M(X)$ is compact in the weak-* topology. Consider $P(X)\subseteq\{\mu:\|\mu\|\leq1\}$ the set of positive probability measures, i.e. $P(X)=\{\mu\in M(X):\mu\geq0,\mu(X)=1\}$. Is $P(X)$ weak-* compact? $P(X)$ is contained in the ball, so if it is weak-* closed, it is closed in a compact set, hence compact. If $X$ is compact, certainly $f\equiv1$ is in $\mathcal{C}_0$, so if $\mu_\alpha$ is a net of probabilities converging to $\mu$, $1=\mu_n(X)=\mu_n(f)\to\mu(f)=\mu(X)$, and since limits preserve inequalities for any $g\geq0$ $\mu(g)\geq0$, so $\mu\in P(X)$. The argument for positivity works in general. The hard part with noncompact spaces is $\mu(X)=1$. It is fairly easy to prove $\mu(X)\leq1$: one merely has to work with $\epsilon$s and $\delta$s in an appropriate way. But I couldn't find any way to conclude $\mu(X)\geq1$. Proceeding by contradiction with $\mu(X)=1-\delta$ doesn't lead me anywhere. I googled for 1-2 hours and found nothing: wherever the weak-* closure was mentioned, it was just assumed. So how do I prove this last bit? Is $\mu(X)\geq1$? Or are there cases where it isn't? And if so, counterexamples?",,"['functional-analysis', 'measure-theory', 'weak-convergence']"
31,Showing a C* Algebra contains a compact operator,Showing a C* Algebra contains a compact operator,,"In my functional analysis class we are currently dealing with C* Algebras, and I just met this problem: Let $ \mathbb{H} $ be a separable Hilbert space, and suppose we have $ A \subset B(\mathbb{H}) $ a C* Algebra of bounded operators of bounded operators on H. Now we suppose there exists $ a \in A $ and that there exists a compact operator $ K \in K(\mathbb{H}) $ such that $ ||a-K|| < ||a|| $. We are to show A contains a compact operator on H that is not the zero operator, that is $ A \cap K(H) \neq \{0\} $. I am quite new to C* Algebras and I still have not much intuition but I cannot really see how to do this. I cannot seem to find a way of doing this. I certainly appreciate all help.","In my functional analysis class we are currently dealing with C* Algebras, and I just met this problem: Let $ \mathbb{H} $ be a separable Hilbert space, and suppose we have $ A \subset B(\mathbb{H}) $ a C* Algebra of bounded operators of bounded operators on H. Now we suppose there exists $ a \in A $ and that there exists a compact operator $ K \in K(\mathbb{H}) $ such that $ ||a-K|| < ||a|| $. We are to show A contains a compact operator on H that is not the zero operator, that is $ A \cap K(H) \neq \{0\} $. I am quite new to C* Algebras and I still have not much intuition but I cannot really see how to do this. I cannot seem to find a way of doing this. I certainly appreciate all help.",,"['functional-analysis', 'operator-theory', 'hilbert-spaces', 'c-star-algebras', 'compact-operators']"
32,Finite Power of Operator Norm,Finite Power of Operator Norm,,"I know that for any bounded operator A on a normed space, we have $||A^n||$ $\leq$ $||A||^n$. I am wondering when the equal sign would be achieved.","I know that for any bounded operator A on a normed space, we have $||A^n||$ $\leq$ $||A||^n$. I am wondering when the equal sign would be achieved.",,"['real-analysis', 'functional-analysis']"
33,Let $A$ be a Banach algebra. Suppose that the spectrum of $x\in A$ is not connected. Prove that $A$ contains a nontrivial idempotent $z$.,Let  be a Banach algebra. Suppose that the spectrum of  is not connected. Prove that  contains a nontrivial idempotent .,A x\in A A z,"While trying to solve the exercise below, I came up with a wrong conclusion, but I can't see why it's wrong. Also I'm accepting suggestions to get the right solution. This is the problem 17 from chapter 10 of Rudin's Functional Analysis. Let $A$ be a Banach algebra. Suppose that the spectrum of $x\in A$ is not connected. Prove that $A$ contains a nontrivial idempotent $z$. My attempt: Let $F_1, F_2$ be two disjoint closed non-empty sets in $\sigma(x)$ such that $F_1\cup F_2 = \sigma(x)$. There is a function $f$ defined over a neighborhood $\Omega$ of $\sigma(x)$ such that $f=1$ in $F_1$ and $f=0$ in $F_2$. Denote $$\tilde{f}(x) = \frac{1}{2\pi i}\int_\Gamma f(\lambda)(\lambda e-x)^{-1}\ d\lambda,$$ which comes from the functional (or symbolic) calculus. $\Gamma$ is a contour of $\sigma(x)$ in $\Omega$. The idea is to show that $\tilde{f}(x)$ is idempotent. This idea of proof was used in some books and I'm trying to follow it. My problem is this: it's clear that $f(\sigma(x)) = F_1$ from the very definition of $f$. I also know that $\sigma(\tilde{f}(x)) = f(\sigma(x)) = F_1$. But from this post, for instance, we have that the spcetrum of idempotents elements is $\{0,1\}$. If $\tilde{f}(x)$ would be idempotent, then $F_1=\{0,1\}$, but this is not necessarily the case. Thank you for your help.","While trying to solve the exercise below, I came up with a wrong conclusion, but I can't see why it's wrong. Also I'm accepting suggestions to get the right solution. This is the problem 17 from chapter 10 of Rudin's Functional Analysis. Let $A$ be a Banach algebra. Suppose that the spectrum of $x\in A$ is not connected. Prove that $A$ contains a nontrivial idempotent $z$. My attempt: Let $F_1, F_2$ be two disjoint closed non-empty sets in $\sigma(x)$ such that $F_1\cup F_2 = \sigma(x)$. There is a function $f$ defined over a neighborhood $\Omega$ of $\sigma(x)$ such that $f=1$ in $F_1$ and $f=0$ in $F_2$. Denote $$\tilde{f}(x) = \frac{1}{2\pi i}\int_\Gamma f(\lambda)(\lambda e-x)^{-1}\ d\lambda,$$ which comes from the functional (or symbolic) calculus. $\Gamma$ is a contour of $\sigma(x)$ in $\Omega$. The idea is to show that $\tilde{f}(x)$ is idempotent. This idea of proof was used in some books and I'm trying to follow it. My problem is this: it's clear that $f(\sigma(x)) = F_1$ from the very definition of $f$. I also know that $\sigma(\tilde{f}(x)) = f(\sigma(x)) = F_1$. But from this post, for instance, we have that the spcetrum of idempotents elements is $\{0,1\}$. If $\tilde{f}(x)$ would be idempotent, then $F_1=\{0,1\}$, but this is not necessarily the case. Thank you for your help.",,"['functional-analysis', 'operator-algebras', 'banach-algebras']"
34,Index of a derivative operator on a circle,Index of a derivative operator on a circle,,"Let $D: C^{1}(S^{1}) \rightarrow C(S^{1})$ be an operator defined as $D(f)=f'$. I would like to find its index (on the road proving that it's a Fredholm operator). First, if $f \in ker(D)$, then $f$ is constant almost everywhere on a circle. What goes about cokernel, its dimension equal the dimension of a kernel of an adjoint operator, by Riez-Markov-Kakutani theorem, the adjoint is $T(f(x)) = \int_{S^{1}}{f'(x) \mu(dx))}$, $\mu$ is some Radon measure. The claim is that the index equals 0, the both the $ker(D)$ and $ker(D^{*})$ have the same dimension. It's clear that $ker(D) \subset ker(D^{*})$, but also it is true that there is no inverse inclusion -- but still this does not affect the dimension property. Probably, the idea is to use the Fourier expansion on the cirlce, $$f(x) = \sum_{n \in \mathbb{Z}}{c_{n} e^{inz}}$$ then $$f'(x) = \sum_{n \in \mathbb{Z}}{in c_{n} e^{inz}}$$ but this no seems to be very benefitial so far. What are the possible approaches to pose the problem? Any sort of help would be much appreciated.","Let $D: C^{1}(S^{1}) \rightarrow C(S^{1})$ be an operator defined as $D(f)=f'$. I would like to find its index (on the road proving that it's a Fredholm operator). First, if $f \in ker(D)$, then $f$ is constant almost everywhere on a circle. What goes about cokernel, its dimension equal the dimension of a kernel of an adjoint operator, by Riez-Markov-Kakutani theorem, the adjoint is $T(f(x)) = \int_{S^{1}}{f'(x) \mu(dx))}$, $\mu$ is some Radon measure. The claim is that the index equals 0, the both the $ker(D)$ and $ker(D^{*})$ have the same dimension. It's clear that $ker(D) \subset ker(D^{*})$, but also it is true that there is no inverse inclusion -- but still this does not affect the dimension property. Probably, the idea is to use the Fourier expansion on the cirlce, $$f(x) = \sum_{n \in \mathbb{Z}}{c_{n} e^{inz}}$$ then $$f'(x) = \sum_{n \in \mathbb{Z}}{in c_{n} e^{inz}}$$ but this no seems to be very benefitial so far. What are the possible approaches to pose the problem? Any sort of help would be much appreciated.",,"['functional-analysis', 'operator-theory', 'fourier-series']"
35,Index of a differential operator,Index of a differential operator,,"Let's consider an operator $D: C^{m+n}[a, b] \rightarrow C^{m}[a, b]$, defined as $D(y(t)) = y^{(n)}+a_{n-1}y^{(n-1)}+\ldots+a_{1}y'+a_{0}$, $a_{k} \in C^{m}[a, b]$. I would like to prove that it is a Fredholm operator and evaluate its index. First, it's not so sophisticated to find the dimension of its kernel, according to the statement from ODEs theory -- any solution can be uniquely described as $C_{1}e^{\alpha_{1}t}+C_{2}e^{\alpha_{2}t}+ \ldots C_{n} e^{\alpha_{n}t}$, $\alpha_{n}$ are the roots of the characteristic polynomial, so the dimension equal $n$. What goes about dimension of cokernel -- by establishing it's precise dimension we can prove that the operator is Fredholm. Let's fix a basis $\{e^{inx} \}_{n \in \mathbb{N}}$ -- since we know that trigonometric polynomials are dence in $C^{q}[a,b]$, the exponent can be written as a linear combination of the form $P(\cos(bx), \sin(bx))$. How to find the dimension of an image? (the first step on the road to cokernel). Probably, the idea is to consider the Fouirer series of $y(t)$ with respect to the fixed basis and obtain the exact formula for the derivatives but this does not seem to be very benefitial. Are there any hints that might help? Any help would be much appreciated.","Let's consider an operator $D: C^{m+n}[a, b] \rightarrow C^{m}[a, b]$, defined as $D(y(t)) = y^{(n)}+a_{n-1}y^{(n-1)}+\ldots+a_{1}y'+a_{0}$, $a_{k} \in C^{m}[a, b]$. I would like to prove that it is a Fredholm operator and evaluate its index. First, it's not so sophisticated to find the dimension of its kernel, according to the statement from ODEs theory -- any solution can be uniquely described as $C_{1}e^{\alpha_{1}t}+C_{2}e^{\alpha_{2}t}+ \ldots C_{n} e^{\alpha_{n}t}$, $\alpha_{n}$ are the roots of the characteristic polynomial, so the dimension equal $n$. What goes about dimension of cokernel -- by establishing it's precise dimension we can prove that the operator is Fredholm. Let's fix a basis $\{e^{inx} \}_{n \in \mathbb{N}}$ -- since we know that trigonometric polynomials are dence in $C^{q}[a,b]$, the exponent can be written as a linear combination of the form $P(\cos(bx), \sin(bx))$. How to find the dimension of an image? (the first step on the road to cokernel). Probably, the idea is to consider the Fouirer series of $y(t)$ with respect to the fixed basis and obtain the exact formula for the derivatives but this does not seem to be very benefitial. Are there any hints that might help? Any help would be much appreciated.",,"['functional-analysis', 'ordinary-differential-equations', 'operator-theory']"
36,Is there an example of a non von Neumann algebra with this property?,Is there an example of a non von Neumann algebra with this property?,,"What is  an example  of a  $C^{*}$  subalgebra  $A$ of  $B(H)$ such that $A$ contains the  identity $I_{H}$ and satisfies the following properties: 1) For every $T\in A$, The  orthogonal projection $\pi_{T}$ on the  closure of $Range(T)$ belongs to $A$. but 2)  $A$ is  not  a  Von  Neumann  Algebra. The  question is  motivated by the fact that every  Von Neumann  algebra satisfies (1).","What is  an example  of a  $C^{*}$  subalgebra  $A$ of  $B(H)$ such that $A$ contains the  identity $I_{H}$ and satisfies the following properties: 1) For every $T\in A$, The  orthogonal projection $\pi_{T}$ on the  closure of $Range(T)$ belongs to $A$. but 2)  $A$ is  not  a  Von  Neumann  Algebra. The  question is  motivated by the fact that every  Von Neumann  algebra satisfies (1).",,"['functional-analysis', 'operator-theory', 'operator-algebras', 'c-star-algebras', 'von-neumann-algebras']"
37,"Show, that $c$ and $c_0$ is a Banach space","Show, that  and  is a Banach space",c c_0,"Let $c=\lbrace x=\lbrace x_n\rbrace ,n\in \mathbb{N}: \exists  \, \text{lim}_{n\to \infty}x_n\rbrace$ and $c_0=\lbrace x=\lbrace x_n\rbrace ,n\in \mathbb{N}: \text{lim}_{n\to \infty}x_n=0\rbrace$.  I want to show that $c$ anc $c_0$ are Banach (I have to show that both are complete metric spaces). Is it enough to show that $\text{lim}_{n\to \infty}x_n$ belongs to space? These are my thoughts: Let: $$x^{(n)}=\left( x^{(n)}_j\right)_{j=1}^\infty =(x^{(n)}_1,x^{(n)}_2,\dots )$$ be a Cauchy sequence in $c$. Lets define the $\infty$-norm $||\cdot||_\infty$: $$||(x_j)_{j\geqslant 0}||_\infty = \text{sup}_{j\geqslant 0} |x_k|$$ Let $\varepsilon >0$, there exists $n_0\in \mathbb{N}$ such that: $$||x_j^{(n)}-x_k^{(m)}||<\varepsilon,\,\,\,\forall n,m >n_0$$ $$\text{sup}_{j\geqslant 0} |x_j^{(n)}-x_j^{(m)}|<\varepsilon,\,\,\,\forall n,m >n_0$$ $$|x_j^{(n)}-x_j^{(m)}|<\varepsilon,\,\,\,\forall n,m >n_0, \,\,\,\forall j\geqslant 0$$ Now I have to prove that $$\text{lim}_{n\to\infty} x^{(n)}= x$$ and $$x\in c.$$ Unfortunately I don't know how to show that $x\in c$.","Let $c=\lbrace x=\lbrace x_n\rbrace ,n\in \mathbb{N}: \exists  \, \text{lim}_{n\to \infty}x_n\rbrace$ and $c_0=\lbrace x=\lbrace x_n\rbrace ,n\in \mathbb{N}: \text{lim}_{n\to \infty}x_n=0\rbrace$.  I want to show that $c$ anc $c_0$ are Banach (I have to show that both are complete metric spaces). Is it enough to show that $\text{lim}_{n\to \infty}x_n$ belongs to space? These are my thoughts: Let: $$x^{(n)}=\left( x^{(n)}_j\right)_{j=1}^\infty =(x^{(n)}_1,x^{(n)}_2,\dots )$$ be a Cauchy sequence in $c$. Lets define the $\infty$-norm $||\cdot||_\infty$: $$||(x_j)_{j\geqslant 0}||_\infty = \text{sup}_{j\geqslant 0} |x_k|$$ Let $\varepsilon >0$, there exists $n_0\in \mathbb{N}$ such that: $$||x_j^{(n)}-x_k^{(m)}||<\varepsilon,\,\,\,\forall n,m >n_0$$ $$\text{sup}_{j\geqslant 0} |x_j^{(n)}-x_j^{(m)}|<\varepsilon,\,\,\,\forall n,m >n_0$$ $$|x_j^{(n)}-x_j^{(m)}|<\varepsilon,\,\,\,\forall n,m >n_0, \,\,\,\forall j\geqslant 0$$ Now I have to prove that $$\text{lim}_{n\to\infty} x^{(n)}= x$$ and $$x\in c.$$ Unfortunately I don't know how to show that $x\in c$.",,"['functional-analysis', 'banach-spaces']"
38,Completeness of Schwartz space,Completeness of Schwartz space,,"I wanna to prove the completeness of Schwartz space $\mathscr{S}(\mathbb{R}^{n})$ equipped with the induced topology from a set of seminnorms $$\|f(x)\|_{\alpha,\beta}=\sup_{x\in \mathbb{R}^{n}}|x^{\beta}D^{\alpha}f(x)|$$ A Cauchy sequence in Schwartz space is a sequence of $\{f_{i}\}$ such that  $$\lim_{k\to\infty}\|f_{i}(x)-f_{j}(x)\|_{\alpha,\beta}=0 \quad\quad \forall i,j\gt k, \alpha,\beta \in \mathbb{Z}_{+}^{n}$$ Since the seminorms are uniform norm, for each $\alpha,\beta$, there must be a pointwise limit function $f_{\alpha,\beta}$ such that $$x^{\beta}D^{\alpha}f_{i}(x)\to f_{\alpha,\beta}(x) \quad as \quad i\to\infty$$ My question is how I can show that there must be a function $f(x)\in\mathscr{S}(\mathbb{R}^{n})$ such that for each $\alpha,\beta$, $x^{\beta}D^{\alpha}f(x)=f_{\alpha,\beta}(x)$?","I wanna to prove the completeness of Schwartz space $\mathscr{S}(\mathbb{R}^{n})$ equipped with the induced topology from a set of seminnorms $$\|f(x)\|_{\alpha,\beta}=\sup_{x\in \mathbb{R}^{n}}|x^{\beta}D^{\alpha}f(x)|$$ A Cauchy sequence in Schwartz space is a sequence of $\{f_{i}\}$ such that  $$\lim_{k\to\infty}\|f_{i}(x)-f_{j}(x)\|_{\alpha,\beta}=0 \quad\quad \forall i,j\gt k, \alpha,\beta \in \mathbb{Z}_{+}^{n}$$ Since the seminorms are uniform norm, for each $\alpha,\beta$, there must be a pointwise limit function $f_{\alpha,\beta}$ such that $$x^{\beta}D^{\alpha}f_{i}(x)\to f_{\alpha,\beta}(x) \quad as \quad i\to\infty$$ My question is how I can show that there must be a function $f(x)\in\mathscr{S}(\mathbb{R}^{n})$ such that for each $\alpha,\beta$, $x^{\beta}D^{\alpha}f(x)=f_{\alpha,\beta}(x)$?",,"['real-analysis', 'functional-analysis', 'harmonic-analysis', 'topological-vector-spaces']"
39,Bergman space norm in terms of coefficients,Bergman space norm in terms of coefficients,,"I am interested in the Bergman space $A^2$ on the unit disc. According to the Wikipedia article on Bergman spaces , if we have $f(z)= \sum_{n=0}^\infty a_n z^n \in A^2$ then $$\|f\|^2_{A^2} := \frac{1}{\pi} \int_\mathbb{D} |f(z)|^2 \, dz = \sum_{n=0}^\infty \frac{|a_n|^2}{n+1}$$ I can't prove this (last) equality. I'm trying to construct an isometry from this space to the $l^2$ space but I'm running into trouble with double integrals and (infinite) summation. I also want to prove the part about the reproducing kernel written later in the Wikipedia article, but I believe it will be easier once I set up the isometry above. Can anyone help me out please? Thanks!","I am interested in the Bergman space $A^2$ on the unit disc. According to the Wikipedia article on Bergman spaces , if we have $f(z)= \sum_{n=0}^\infty a_n z^n \in A^2$ then $$\|f\|^2_{A^2} := \frac{1}{\pi} \int_\mathbb{D} |f(z)|^2 \, dz = \sum_{n=0}^\infty \frac{|a_n|^2}{n+1}$$ I can't prove this (last) equality. I'm trying to construct an isometry from this space to the $l^2$ space but I'm running into trouble with double integrals and (infinite) summation. I also want to prove the part about the reproducing kernel written later in the Wikipedia article, but I believe it will be easier once I set up the isometry above. Can anyone help me out please? Thanks!",,"['functional-analysis', 'hilbert-spaces', 'bergman-spaces']"
40,What is the difference between $H^1_{loc}$ and $H^1$?,What is the difference between  and ?,H^1_{loc} H^1,I have started studying Sobolev spaces and I came across a space referred to as $H^1_{loc}$. I am not sure what the $loc$ subscript infers? What is it that makes this space different from $H^1$? Why would you every specify this space instead of $H^1$?,I have started studying Sobolev spaces and I came across a space referred to as $H^1_{loc}$. I am not sure what the $loc$ subscript infers? What is it that makes this space different from $H^1$? Why would you every specify this space instead of $H^1$?,,"['functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
41,"Regarding ""stronger"" norms","Regarding ""stronger"" norms",,"Let $X$ be a normed linear space. Show that a norm $\|\cdot\|_{1}$ is stronger than a norm $\|\cdot\|_{2}$ if and only if for any sequence $\{x_{n}\} \subset X$, $\|x_{n}\|_{1} \to 0$ always implies $\|x_{n}\|_{2} \to 0$. My work: $\Longrightarrow$ Suppose $\|\cdot\|_{1}$ is stronger than $\|\cdot\|_{2}$. This means that there exists some $M > 0$ such that $\|x\|_{2} \leqslant M\|x\|_{1}$ for all $x \in {X}$. Let $\{x_{n}\} \subset X$ be any sequence such that $\|x_{n}\|_{1} \to 0$. It follows that $M\|x_{n}\|_{1} \to 0$ which necessarily implies $\|x_{n}\|_{2} \to 0$. (Can someone verify this?) $\Longleftarrow$ Suppose that $\|x_{n}\|_{1} \to 0$ always implies $\|x_{n}\|_{2} \to 0$. This implies that $x_{n}$ is a Cauchy sequence under both norms. Thus, for some $\epsilon_{1}, \epsilon_{2} > 0$, there exists $N_{1}, N_{2} > \mathbb{N}$ such that $\|x_{n} - x_{m}\|_{1} < \epsilon_{1}$ and $\|x_{n} - x_{m}\|_{2} < \epsilon_{2}$ for all $n,m > N_{1},N_{2}$, respectively. .... My question is for the reverse direction, how do I connect this idea of a ""stronger"" norm knowing only that both norms converge to $0$.","Let $X$ be a normed linear space. Show that a norm $\|\cdot\|_{1}$ is stronger than a norm $\|\cdot\|_{2}$ if and only if for any sequence $\{x_{n}\} \subset X$, $\|x_{n}\|_{1} \to 0$ always implies $\|x_{n}\|_{2} \to 0$. My work: $\Longrightarrow$ Suppose $\|\cdot\|_{1}$ is stronger than $\|\cdot\|_{2}$. This means that there exists some $M > 0$ such that $\|x\|_{2} \leqslant M\|x\|_{1}$ for all $x \in {X}$. Let $\{x_{n}\} \subset X$ be any sequence such that $\|x_{n}\|_{1} \to 0$. It follows that $M\|x_{n}\|_{1} \to 0$ which necessarily implies $\|x_{n}\|_{2} \to 0$. (Can someone verify this?) $\Longleftarrow$ Suppose that $\|x_{n}\|_{1} \to 0$ always implies $\|x_{n}\|_{2} \to 0$. This implies that $x_{n}$ is a Cauchy sequence under both norms. Thus, for some $\epsilon_{1}, \epsilon_{2} > 0$, there exists $N_{1}, N_{2} > \mathbb{N}$ such that $\|x_{n} - x_{m}\|_{1} < \epsilon_{1}$ and $\|x_{n} - x_{m}\|_{2} < \epsilon_{2}$ for all $n,m > N_{1},N_{2}$, respectively. .... My question is for the reverse direction, how do I connect this idea of a ""stronger"" norm knowing only that both norms converge to $0$.",,"['functional-analysis', 'metric-spaces', 'normed-spaces']"
42,Equivalence of complete norms,Equivalence of complete norms,,"Let $\|.\|_1$ and $\|.\|_2$ be two complete norms on a linear space $X$ such that if a sequence $(x_n)$ converges to $x$ in $(X,\|.\|_1)$ and to $y$ in $(X,\|.\|_2)$, then $x=y$. We have to prove that $\|.\|_1$ and $\|.\|_2$ are equivalent norms. I know that if there exists $K>0$ such that $\|x\|_1\leq K\|x\|_2$ for all $x\in X$, then by the consequence of open mapping theorem, two norms are equivalent. But how to get this from the available information? Please suggest!","Let $\|.\|_1$ and $\|.\|_2$ be two complete norms on a linear space $X$ such that if a sequence $(x_n)$ converges to $x$ in $(X,\|.\|_1)$ and to $y$ in $(X,\|.\|_2)$, then $x=y$. We have to prove that $\|.\|_1$ and $\|.\|_2$ are equivalent norms. I know that if there exists $K>0$ such that $\|x\|_1\leq K\|x\|_2$ for all $x\in X$, then by the consequence of open mapping theorem, two norms are equivalent. But how to get this from the available information? Please suggest!",,"['functional-analysis', 'normed-spaces']"
43,weak-$*$ topology on $X^{**}$,weak- topology on,* X^{**},"In Folland, Exercise 5.52(c), the question is to show that the relative topology on $X$ induced by the weak-$*$ topology on $X^{**}$ is the weak topology on $X$. It is not clear to me what is meant by weak-$*$ topology on $X^{**}$. If I understand it correctly, weak-$*$ topology is the coarsest topology which makes the evaluation functionals continuous. So the weak-$*$ topology should apply to either $X^{*}$ or $X^{***}$, not $X^{**}$. Thanks!","In Folland, Exercise 5.52(c), the question is to show that the relative topology on $X$ induced by the weak-$*$ topology on $X^{**}$ is the weak topology on $X$. It is not clear to me what is meant by weak-$*$ topology on $X^{**}$. If I understand it correctly, weak-$*$ topology is the coarsest topology which makes the evaluation functionals continuous. So the weak-$*$ topology should apply to either $X^{*}$ or $X^{***}$, not $X^{**}$. Thanks!",,['functional-analysis']
44,Minimizing a functional in the Sobolev space $H_0^1$,Minimizing a functional in the Sobolev space,H_0^1,"I am trying to show that, given $f \in H^{-1}(U)$, there exists a unique $u \in H_0^1(U)$ such that: $$\int_U \nabla u\cdot\nabla v \, \mathrm{d}x= \langle f,v \rangle_{H^{-1}} \, , \quad \forall \, v \in H_0^1(U) \, .$$ To this end, I define the functional $J \colon H_0^1(U) \to \mathbb{R}$ by: $$J(v):=\int_U \left\| \nabla v \right\|^2 \, \mathrm{d}x- \langle f,v \rangle_{H^{-1}}.$$ Then I would like to do the following: Show that $J(v)$ is bounded from below; Show that there is a weakly convergent sequence whose limit in $H_0^1(U)$ minimises $J$ (hence the inf is achieved); Show that this limit satisfies the problem and it is unique. Number 3 is quite easy, but I am stuck on 1 and 2. I am not sure how to proceed. So far I have oly been able to use the definition of $\|\cdot\|_{H^{-1}}$ to get: $$J(v) \geq \int_U \left\| \nabla v \right\|^2 \, \mathrm{d}x- \|f \|_{H^{-1}} \|v\|_{H_0^1} \, .$$ Any pointers would be very helpful. P.S.: I am aware that this could be shown by applying Lax-Milgram but I need to take this direct approach.","I am trying to show that, given $f \in H^{-1}(U)$, there exists a unique $u \in H_0^1(U)$ such that: $$\int_U \nabla u\cdot\nabla v \, \mathrm{d}x= \langle f,v \rangle_{H^{-1}} \, , \quad \forall \, v \in H_0^1(U) \, .$$ To this end, I define the functional $J \colon H_0^1(U) \to \mathbb{R}$ by: $$J(v):=\int_U \left\| \nabla v \right\|^2 \, \mathrm{d}x- \langle f,v \rangle_{H^{-1}}.$$ Then I would like to do the following: Show that $J(v)$ is bounded from below; Show that there is a weakly convergent sequence whose limit in $H_0^1(U)$ minimises $J$ (hence the inf is achieved); Show that this limit satisfies the problem and it is unique. Number 3 is quite easy, but I am stuck on 1 and 2. I am not sure how to proceed. So far I have oly been able to use the definition of $\|\cdot\|_{H^{-1}}$ to get: $$J(v) \geq \int_U \left\| \nabla v \right\|^2 \, \mathrm{d}x- \|f \|_{H^{-1}} \|v\|_{H_0^1} \, .$$ Any pointers would be very helpful. P.S.: I am aware that this could be shown by applying Lax-Milgram but I need to take this direct approach.",,"['functional-analysis', 'optimization', 'sobolev-spaces', 'normed-spaces', 'weak-convergence']"
45,A compact Hausdorff space $X$ is finite if and only if $C(X)$ is finite-dimensional [closed],A compact Hausdorff space  is finite if and only if  is finite-dimensional [closed],X C(X),"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Let $X$ be a compact Hausdorff space. Assume that the vector space of real-valued continuous functions on $X$ is finite-dimensional. I would like to conclude that $X$ is ﬁnite. Certainly, the converse implication is trivial.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Let $X$ be a compact Hausdorff space. Assume that the vector space of real-valued continuous functions on $X$ is finite-dimensional. I would like to conclude that $X$ is ﬁnite. Certainly, the converse implication is trivial.",,"['functional-analysis', 'analysis']"
46,"Does it follow that $u_n \rightharpoonup 0$ weakly in $W^{1, p}(\mathbb{R})$ for all $p \in (1, \infty)$?",Does it follow that  weakly in  for all ?,"u_n \rightharpoonup 0 W^{1, p}(\mathbb{R}) p \in (1, \infty)","Fix a function $\varphi \in C_c^\infty$, $\varphi \not\equiv 0$, and set $u_n(x) = \varphi(x + n)$. Does it follow that $u_n \rightharpoonup 0$ weakly in $W^{1, p}(\mathbb{R})$ for all $p \in (1, \infty)$?","Fix a function $\varphi \in C_c^\infty$, $\varphi \not\equiv 0$, and set $u_n(x) = \varphi(x + n)$. Does it follow that $u_n \rightharpoonup 0$ weakly in $W^{1, p}(\mathbb{R})$ for all $p \in (1, \infty)$?",,"['calculus', 'real-analysis']"
47,Dual map is zero if and only if map is zero,Dual map is zero if and only if map is zero,,"A problem from Linear Algebra Done Right (Third Ed): Suppose $W$ is finite dimensional and $T \in \mathcal{L}(V, W)$. Prove that $T=0$ if and only if its dual $T' \in \mathcal{L}(W', V')=0$. I am confused by the information that $W$ is finite dimensional. Why is that required, while nothing is said about finite dimensionality of $V$ ? There is a solution here: Does that fact that the dual map is zero imply that the map is zero? but I am more interested in the reason for the assumptions. I was able to solve it (or so I thought) without using a contrapositive argument or the assumption on $W$ which is what worries me. EDIT: Here is my argument. Since T' = 0,  we get $\phi_w(Tv) = 0 \forall  \phi_w$, which yields Tv = 0 for all v, hence T = 0. I can ""reverse"" the argument to get the converse. What is my error?","A problem from Linear Algebra Done Right (Third Ed): Suppose $W$ is finite dimensional and $T \in \mathcal{L}(V, W)$. Prove that $T=0$ if and only if its dual $T' \in \mathcal{L}(W', V')=0$. I am confused by the information that $W$ is finite dimensional. Why is that required, while nothing is said about finite dimensionality of $V$ ? There is a solution here: Does that fact that the dual map is zero imply that the map is zero? but I am more interested in the reason for the assumptions. I was able to solve it (or so I thought) without using a contrapositive argument or the assumption on $W$ which is what worries me. EDIT: Here is my argument. Since T' = 0,  we get $\phi_w(Tv) = 0 \forall  \phi_w$, which yields Tv = 0 for all v, hence T = 0. I can ""reverse"" the argument to get the converse. What is my error?",,"['linear-algebra', 'functional-analysis', 'duality-theorems']"
48,‎‎‎$‎‎C^*$-algebra generated by ‎$‎‎a$‎,‎‎‎-algebra generated by ‎‎,‎‎C^* ‎‎a,"Let ‎$‎‎A$ ‎be a unital ‎‎‎$‎‎C^*$-algebra. ‎‎ Assume that ‎$‎‎a\in A$ ‎is a ‎‎normal ‎and ‎invertible element ‎i.e ‎‎$‎‎aa^*=a^*a$ ‎and ‎‎$‎‎aa^{-1}=a^{-1}a=1$‎.‎ ‎let $‎‎C^*({a}) $ be the ‎‎‎$‎‎C^*$-algebra generated by ‎$‎‎a$‎. I know that ‎$‎‎C^*({a}) $ ‎is ‎the ‎closed ‎linear ‎span ‎of ‎‎$‎‎a^{m}a^{*{n}}$‎‎‎ such that $m,n\in N$. ‎ ‎ I want to know ‎$‎1 , a^{-1} \in ‎‎C^*({a}) ‎‎$‎‎ ‎ Q: Is it true?""$‎1 , a^{-1} \in ‎‎C^*({a}) ‎‎$‎‎""‎ How can I prove it? ‎‎","Let ‎$‎‎A$ ‎be a unital ‎‎‎$‎‎C^*$-algebra. ‎‎ Assume that ‎$‎‎a\in A$ ‎is a ‎‎normal ‎and ‎invertible element ‎i.e ‎‎$‎‎aa^*=a^*a$ ‎and ‎‎$‎‎aa^{-1}=a^{-1}a=1$‎.‎ ‎let $‎‎C^*({a}) $ be the ‎‎‎$‎‎C^*$-algebra generated by ‎$‎‎a$‎. I know that ‎$‎‎C^*({a}) $ ‎is ‎the ‎closed ‎linear ‎span ‎of ‎‎$‎‎a^{m}a^{*{n}}$‎‎‎ such that $m,n\in N$. ‎ ‎ I want to know ‎$‎1 , a^{-1} \in ‎‎C^*({a}) ‎‎$‎‎ ‎ Q: Is it true?""$‎1 , a^{-1} \in ‎‎C^*({a}) ‎‎$‎‎""‎ How can I prove it? ‎‎",,"['functional-analysis', 'banach-spaces', 'c-star-algebras', 'banach-algebras']"
49,Product spaces and open sets,Product spaces and open sets,,"I have a proposition I have been pondering that I need help with. Let $(X,d_{X})$ and $(Y,d_{Y})$ be metric spaces. Recall that the product space $(X\times Y, d_{1})$ is also a metric space with the metric $d_{1}: (X\times Y)\times(X\times Y) \to \mathbb{R}$ defined as $$ d_{1}((x_{1},y_{1}),(x_{2},y_{2})) = d_{X}(x_{1},x_{2}) + d_{Y}(y_{1},y_{2}) $$ Show that (a) If $A, B$ are subsets of $X,Y$, respectively, and $A\times B$ is an open subset of $X\times Y$, then $A$ and $B$ are open in $X$ and $Y$, respectively. My start: Let $A \subset X$, $B \subset Y$, and let $A\times B$ be an open subset open in $X\times Y$. Since $A\times B$ is open, then $A\times B = \text{Int}(A\times B)$. I'm stuck in how to proceed. (b) If both $(X,d_{X})$ and $(Y,d_{Y})$ are separable, then $(X\times Y, d_{1})$ is also separable. Suppose that both $(X,d_{X})$ and $(Y,d_{Y})$ are separable. Then, there exists some countable dense subset in each of $X$ and $Y$, say $A$ and $B$, respectively. We claim that $A \times B$ is a countable dense subset of $X\times Y$. To show that $A\times B$ is dense, let $C$ be an open set in $X\times Y$. Again, I am stuck in how to proceed. Any clues, tips, or insights?","I have a proposition I have been pondering that I need help with. Let $(X,d_{X})$ and $(Y,d_{Y})$ be metric spaces. Recall that the product space $(X\times Y, d_{1})$ is also a metric space with the metric $d_{1}: (X\times Y)\times(X\times Y) \to \mathbb{R}$ defined as $$ d_{1}((x_{1},y_{1}),(x_{2},y_{2})) = d_{X}(x_{1},x_{2}) + d_{Y}(y_{1},y_{2}) $$ Show that (a) If $A, B$ are subsets of $X,Y$, respectively, and $A\times B$ is an open subset of $X\times Y$, then $A$ and $B$ are open in $X$ and $Y$, respectively. My start: Let $A \subset X$, $B \subset Y$, and let $A\times B$ be an open subset open in $X\times Y$. Since $A\times B$ is open, then $A\times B = \text{Int}(A\times B)$. I'm stuck in how to proceed. (b) If both $(X,d_{X})$ and $(Y,d_{Y})$ are separable, then $(X\times Y, d_{1})$ is also separable. Suppose that both $(X,d_{X})$ and $(Y,d_{Y})$ are separable. Then, there exists some countable dense subset in each of $X$ and $Y$, say $A$ and $B$, respectively. We claim that $A \times B$ is a countable dense subset of $X\times Y$. To show that $A\times B$ is dense, let $C$ be an open set in $X\times Y$. Again, I am stuck in how to proceed. Any clues, tips, or insights?",,"['real-analysis', 'functional-analysis', 'metric-spaces']"
50,To show $T$ bounded,To show  bounded,T,"Let $X,Y$ be normed linear spaces and let $T:X\to Y$ be a linear map such that for every absolutely convergent series $\sum\limits_{n=1}^{\infty}x_n$, the series $\sum\limits_{n=1}^{\infty}Tx_n$ converges. I want to prove that $T$ is bounded. What I have done is as follows: Let $(x_n)$ be a Cauchy sequence in $X$. Then for every $k\in \mathbb N$, there exists $n_k\in \mathbb N$ such that $\parallel x_n-x_m\parallel<\frac{1}{2^k}$ for all $n,m\geq n_k$. Assumuing $n_k<n_{k+1}$ for all $k\in \mathbb N$ we get $\parallel x_{n_k}-x_{k-1}\parallel<\frac{1}{2^{k-1}}$ for each $k\geq 2$. Let $y_1=x_{n_1}$ and $y_k= x_{n_{k}}-x_{n_{k-1}} $ for $k\geq 2$. Now it is easy to show that $\sum\limits_{k=1}^{\infty}y_k$  is absolutely convergent. Therefore, by hypothesis, $\sum\limits_{k=1}^{\infty}Ty_k$ converges. But this gives $(x_{n_k})$ converges. How to proceed further? please help!","Let $X,Y$ be normed linear spaces and let $T:X\to Y$ be a linear map such that for every absolutely convergent series $\sum\limits_{n=1}^{\infty}x_n$, the series $\sum\limits_{n=1}^{\infty}Tx_n$ converges. I want to prove that $T$ is bounded. What I have done is as follows: Let $(x_n)$ be a Cauchy sequence in $X$. Then for every $k\in \mathbb N$, there exists $n_k\in \mathbb N$ such that $\parallel x_n-x_m\parallel<\frac{1}{2^k}$ for all $n,m\geq n_k$. Assumuing $n_k<n_{k+1}$ for all $k\in \mathbb N$ we get $\parallel x_{n_k}-x_{k-1}\parallel<\frac{1}{2^{k-1}}$ for each $k\geq 2$. Let $y_1=x_{n_1}$ and $y_k= x_{n_{k}}-x_{n_{k-1}} $ for $k\geq 2$. Now it is easy to show that $\sum\limits_{k=1}^{\infty}y_k$  is absolutely convergent. Therefore, by hypothesis, $\sum\limits_{k=1}^{\infty}Ty_k$ converges. But this gives $(x_{n_k})$ converges. How to proceed further? please help!",,"['functional-analysis', 'normed-spaces']"
51,Linear independence and Schauder basis,Linear independence and Schauder basis,,"It follows from the definition that a Schauder basis must be linear independent, i.e. every finite subset of the Schauder basis is linear independent. I wonder if the following ""converse"" of this is also true: Let $X$ be a Banach space and $\{e_n:n \ge 1 \}$ be a linear independent subset of $X$. If for any element $x \in X$, there exists a sequence $(a_n)_{n \ge 1}$ such that $x=\sum_{n=1}^\infty a_n e_n$, then such a sequence must be unique , i.e. $\{e_n:n \ge 1 \}$ is a Schauder basis. Clearly it is true for finite dimensional $X$, but how about the infinite dimensional case? Does $X$ have to be complete? If it is not true, are there any counterexamples? Thanks in advance!","It follows from the definition that a Schauder basis must be linear independent, i.e. every finite subset of the Schauder basis is linear independent. I wonder if the following ""converse"" of this is also true: Let $X$ be a Banach space and $\{e_n:n \ge 1 \}$ be a linear independent subset of $X$. If for any element $x \in X$, there exists a sequence $(a_n)_{n \ge 1}$ such that $x=\sum_{n=1}^\infty a_n e_n$, then such a sequence must be unique , i.e. $\{e_n:n \ge 1 \}$ is a Schauder basis. Clearly it is true for finite dimensional $X$, but how about the infinite dimensional case? Does $X$ have to be complete? If it is not true, are there any counterexamples? Thanks in advance!",,"['functional-analysis', 'banach-spaces']"
52,Separable version of Banach-Alaoglu,Separable version of Banach-Alaoglu,,"My notes say that the theorem of Banach-Alaoglu states the following: If $X$ is a normed separable space, then every bounded sequence in $X'$ has a weak-* convergent subsequence. How is this equivalent to the usual formulation from Wikipedia, etc - i.e. the closed unit ball being weak-*-compact, for a (not necessarily separable) normed space $X$? Or is it a special case?","My notes say that the theorem of Banach-Alaoglu states the following: If $X$ is a normed separable space, then every bounded sequence in $X'$ has a weak-* convergent subsequence. How is this equivalent to the usual formulation from Wikipedia, etc - i.e. the closed unit ball being weak-*-compact, for a (not necessarily separable) normed space $X$? Or is it a special case?",,['functional-analysis']
53,"Using subspaces of $C[0,1]$ to show that Riesz's lemma is not true for $r=1$",Using subspaces of  to show that Riesz's lemma is not true for,"C[0,1] r=1","Let $\mathcal{C}^0([0,1];\mathbb{C})$ be the set of all continuous complex-valued functions on $[0,1]$ . Prove that : $a)$ $E:=\{f\in\mathcal{C}^0([0,1];\mathbb{C}) : f(0)=0\}$ is a Banach space. $b)$ $F=\{f\in E: \int_0^1f(x)\mathrm{d}x=0\}$ is a proper closed subspace of E. $c)$ Riesz's lemma is not true for $r=1$ . I tried to prove a) saying that you can use a Cauchy sequence to prove that the limit belongs to $E$ . Instead for b) I could demonstrate only that $F(1)=F(0)$ (one primitive of $f$ ) but I don't know if it's useful. Can you help me?",Let be the set of all continuous complex-valued functions on . Prove that : is a Banach space. is a proper closed subspace of E. Riesz's lemma is not true for . I tried to prove a) saying that you can use a Cauchy sequence to prove that the limit belongs to . Instead for b) I could demonstrate only that (one primitive of ) but I don't know if it's useful. Can you help me?,"\mathcal{C}^0([0,1];\mathbb{C}) [0,1] a) E:=\{f\in\mathcal{C}^0([0,1];\mathbb{C}) : f(0)=0\} b) F=\{f\in E: \int_0^1f(x)\mathrm{d}x=0\} c) r=1 E F(1)=F(0) f","['functional-analysis', 'banach-spaces']"
54,Operator $T^*$ surjective iff $T$ topologically injective,Operator  surjective iff  topologically injective,T^* T,"Let $T : E \to F$ operator between Banach spaces, and $T^* : F^* \to E^*$ - adjoint operator. I want to proof next proposition: $T$ is topologically injective (or equivalently: injective with closed image) iff $T^*$ surjective. It is simple to proof that if $T$ topologically injective than $T^*$ surjective. In fact, if $T$ is topologically injective then $T$ is some embedding of space $E$ in space $F$. So if $f \in F^*$ than $T^* f$ it is just restriction of functional $f$ on space $T(E)$. So we need to proof that every functional $f$ on $T(E)$ can be continued to functional $\hat{f}$ on $F$. But it is just Han Banach theorem. It is simple to proof that if $T^*$ surjective then $T$ injective. Suppose $T$ is not injective. We can take some two distinct points $x,y \in E$ such that $T(x)=T(y)$, and we can build functional $f \in E^*$ such that $f(x) \neq f(y)$ (it is again Han Banach theorem). So, for any functional $g \in F^*$ we have $T^* g \neq f$ because right part separate points $x,y$ but left part not. But I cant proof that if $T^*$ surjective then $T(E)$ - closed. How I can do it?","Let $T : E \to F$ operator between Banach spaces, and $T^* : F^* \to E^*$ - adjoint operator. I want to proof next proposition: $T$ is topologically injective (or equivalently: injective with closed image) iff $T^*$ surjective. It is simple to proof that if $T$ topologically injective than $T^*$ surjective. In fact, if $T$ is topologically injective then $T$ is some embedding of space $E$ in space $F$. So if $f \in F^*$ than $T^* f$ it is just restriction of functional $f$ on space $T(E)$. So we need to proof that every functional $f$ on $T(E)$ can be continued to functional $\hat{f}$ on $F$. But it is just Han Banach theorem. It is simple to proof that if $T^*$ surjective then $T$ injective. Suppose $T$ is not injective. We can take some two distinct points $x,y \in E$ such that $T(x)=T(y)$, and we can build functional $f \in E^*$ such that $f(x) \neq f(y)$ (it is again Han Banach theorem). So, for any functional $g \in F^*$ we have $T^* g \neq f$ because right part separate points $x,y$ but left part not. But I cant proof that if $T^*$ surjective then $T(E)$ - closed. How I can do it?",,"['functional-analysis', 'banach-spaces', 'adjoint-operators']"
55,Showing $\sin(nx)$ is a complete orthonormal system,Showing  is a complete orthonormal system,\sin(nx),"I want to prove that the system $\sin(nx)$ for $n=1,2,\cdots$ is complete in $L_2[0,\pi]$, so what I do is assume that: $$\int_0^\pi f(x)\sin(kx)dx=0,\quad k=1,2,\cdots$$ An define an odd function: $$\overline{f}(x) =\left\{ \begin{matrix}f(x),&0\leq x\lt \pi\\ -f(-x),& -\pi \leq x \lt 0\end{matrix} \right.$$ And we have that $\int_{-\pi}^\pi \overline{f}(x) dx = 0, \int_{-\pi}^\pi \overline{f}(x)\cos(nx) dx=0 $ Since $\overline{f}$ is an odd function we have: $$\int_{-\pi}^\pi \overline{f}(x)\sin(nx) dx =2 \int_{-\pi}^\pi \overline{f}(x)\sin(nx)dx=2\int_0^\pi f(x)\sin(nx)d=0$$ Where the last bit equals zero by assumption. Now this is the bit I don't understand, I am meant to conclude: "" Thus, since the system $\{1,\sin(nx),\cos(nx)\}_n$ is complete in $L^2[-\pi,\pi]$, we obtain $\overline{f}(x)=0$ in $[-\pi,\pi]$ and hence $f(x) = 0$ in $[0,\pi]$""","I want to prove that the system $\sin(nx)$ for $n=1,2,\cdots$ is complete in $L_2[0,\pi]$, so what I do is assume that: $$\int_0^\pi f(x)\sin(kx)dx=0,\quad k=1,2,\cdots$$ An define an odd function: $$\overline{f}(x) =\left\{ \begin{matrix}f(x),&0\leq x\lt \pi\\ -f(-x),& -\pi \leq x \lt 0\end{matrix} \right.$$ And we have that $\int_{-\pi}^\pi \overline{f}(x) dx = 0, \int_{-\pi}^\pi \overline{f}(x)\cos(nx) dx=0 $ Since $\overline{f}$ is an odd function we have: $$\int_{-\pi}^\pi \overline{f}(x)\sin(nx) dx =2 \int_{-\pi}^\pi \overline{f}(x)\sin(nx)dx=2\int_0^\pi f(x)\sin(nx)d=0$$ Where the last bit equals zero by assumption. Now this is the bit I don't understand, I am meant to conclude: "" Thus, since the system $\{1,\sin(nx),\cos(nx)\}_n$ is complete in $L^2[-\pi,\pi]$, we obtain $\overline{f}(x)=0$ in $[-\pi,\pi]$ and hence $f(x) = 0$ in $[0,\pi]$""",,"['functional-analysis', 'orthonormal']"
56,Obtaining Positive Solutions by the Method of Characteristics for a First Order Linear PDE,Obtaining Positive Solutions by the Method of Characteristics for a First Order Linear PDE,,"Consider the function $u(x,y):{\mathbb{R}^2} \to {\mathbb{R}}$ and $u(x,y) \in {{C}^1}({\mathbb{R}^2})$. The function satisfies the following boundary value problem $$c_1 u_x + c_2 u_y = f(x,y)$$ $$u|_{\partial \Omega} =0$$ Where $\Omega$ is an arbitrary simply connected domain in  $\mathbb{R}^2$ with the boundary $\partial \Omega $ which can be assumed to be smooth as you like although weaker conditions may be needed. $c_1,c_2$ are some real constants. Find a function $f(x,y)$ such that  $u(x,y) >0$ for all $(x,y) \in \Omega$. In the next step, it can be aksed that how we can find all such $f(x,y)$? or What are the common properties of all such $f(x,y)$?","Consider the function $u(x,y):{\mathbb{R}^2} \to {\mathbb{R}}$ and $u(x,y) \in {{C}^1}({\mathbb{R}^2})$. The function satisfies the following boundary value problem $$c_1 u_x + c_2 u_y = f(x,y)$$ $$u|_{\partial \Omega} =0$$ Where $\Omega$ is an arbitrary simply connected domain in  $\mathbb{R}^2$ with the boundary $\partial \Omega $ which can be assumed to be smooth as you like although weaker conditions may be needed. $c_1,c_2$ are some real constants. Find a function $f(x,y)$ such that  $u(x,y) >0$ for all $(x,y) \in \Omega$. In the next step, it can be aksed that how we can find all such $f(x,y)$? or What are the common properties of all such $f(x,y)$?",,"['functional-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'dynamical-systems']"
57,Continuity of $L_p$ norm in $p$ with $\varepsilon$-$\delta$ definition,Continuity of  norm in  with - definition,L_p p \varepsilon \delta,"Assume that $\|f\|_p< \infty$ for $1\le p<\infty$. In this question we showed that $$ g(p)=\|f\|_p $$ is continuous in $p \ge 1$. The technique was to use Dominant Convergence theorem. Using $\varepsilon$-$\delta$ language, what this means is that for any $\varepsilon>0$ there is a $\delta>0$ such that for all $|q-p| < \delta(\varepsilon)$ implies that $$ \left | \|f\|_p-\|f\|_q \right| \le \varepsilon $$ My question the following. Can we characterize $\delta(\varepsilon)$ more explicitly in term of $\varepsilon$ and have an expression for $\delta$? Observer, that $\delta$ should probably be a function of $p$ as well, otherwise I don't think it is possible.","Assume that $\|f\|_p< \infty$ for $1\le p<\infty$. In this question we showed that $$ g(p)=\|f\|_p $$ is continuous in $p \ge 1$. The technique was to use Dominant Convergence theorem. Using $\varepsilon$-$\delta$ language, what this means is that for any $\varepsilon>0$ there is a $\delta>0$ such that for all $|q-p| < \delta(\varepsilon)$ implies that $$ \left | \|f\|_p-\|f\|_q \right| \le \varepsilon $$ My question the following. Can we characterize $\delta(\varepsilon)$ more explicitly in term of $\varepsilon$ and have an expression for $\delta$? Observer, that $\delta$ should probably be a function of $p$ as well, otherwise I don't think it is possible.",,"['functional-analysis', 'normed-spaces', 'lp-spaces']"
58,How to show that the heat trace converges,How to show that the heat trace converges,,"Suppose $M$ is a compact Riemannian manifold of dimension $d$ without boundary, with Laplace Beltrami operator $\triangle$. We know that the spectrum of $\triangle$ (defined appropriately as a self - adjoint operator on $L^2(M)$) is discrete and consists of real numbers $\lambda_n$ that tend to $+ \infty$ as $n \to \infty$.  The heat trace is then defined to be the sum $$ Z(t) = \sum_n e^{-\lambda_n t} \qquad (t > 0) $$  I am trying to understand how one can show directly that the sum converges. One thing that I think might help is Weyl's law which tells us something about the growth behaviour of the eigenvalues. Concretely we have $$ \lambda_n \backsim n^{2/d} \quad (n \to \infty) $$ so I think that once I understand the convergence of the series  $$ \sum a^{n^{2/d}} $$ where $|a| < 1$, I have also understood the convergence of $Z(t)$. Here I am stuck -- basically the geometric series does not work as a model case (I think), and I also wonder whether this approach is the right to take ... Thanks for your feedback and comments! I am destined to think it through myself, just need some help to progress in the right direction.","Suppose $M$ is a compact Riemannian manifold of dimension $d$ without boundary, with Laplace Beltrami operator $\triangle$. We know that the spectrum of $\triangle$ (defined appropriately as a self - adjoint operator on $L^2(M)$) is discrete and consists of real numbers $\lambda_n$ that tend to $+ \infty$ as $n \to \infty$.  The heat trace is then defined to be the sum $$ Z(t) = \sum_n e^{-\lambda_n t} \qquad (t > 0) $$  I am trying to understand how one can show directly that the sum converges. One thing that I think might help is Weyl's law which tells us something about the growth behaviour of the eigenvalues. Concretely we have $$ \lambda_n \backsim n^{2/d} \quad (n \to \infty) $$ so I think that once I understand the convergence of the series  $$ \sum a^{n^{2/d}} $$ where $|a| < 1$, I have also understood the convergence of $Z(t)$. Here I am stuck -- basically the geometric series does not work as a model case (I think), and I also wonder whether this approach is the right to take ... Thanks for your feedback and comments! I am destined to think it through myself, just need some help to progress in the right direction.",,"['functional-analysis', 'differential-geometry', 'partial-differential-equations', 'operator-theory', 'heat-equation']"
59,Bounded sequence in a Banach space,Bounded sequence in a Banach space,,"Let $(X,||\cdot||)$ be a Banach space in $\mathbb{K}$ and $(x_n)$ a sequence in $X$, and let $f$ be any function in $X^*$ (the dual space of $X$) such that the sequence $(f(x_n))$ is bounded. I am trying to prove that in this case, the sequence $(||x_n||_X)$ is bounded. I have managed to prove than for a given integer $n$, $||x_n||_X=||\psi_{x_n} ||$, where $\psi$ is a function defined on $X^*$ with values in $\mathbb{K}$ defined by $\psi_{x_n}(f)=f(x_n)$. So if the sequence $(f(x_n))$ is bounded, then so is  $(\psi_{x_n}(f))$. Can we conclude that this implies that $||\psi_{x_n} ||$ (and therefore $||x_n||_X$) is bounded? My intuition tells me it does, as $||\psi_{x_n} ||=\sup\limits_{f\neq0}\{\frac{||\psi_{x_n}(f)||}{||f||}\}$, but I am not sure since $\psi_{x_n}$ is a function while $(\psi_{x_n})$ is a sequence (of functions..).","Let $(X,||\cdot||)$ be a Banach space in $\mathbb{K}$ and $(x_n)$ a sequence in $X$, and let $f$ be any function in $X^*$ (the dual space of $X$) such that the sequence $(f(x_n))$ is bounded. I am trying to prove that in this case, the sequence $(||x_n||_X)$ is bounded. I have managed to prove than for a given integer $n$, $||x_n||_X=||\psi_{x_n} ||$, where $\psi$ is a function defined on $X^*$ with values in $\mathbb{K}$ defined by $\psi_{x_n}(f)=f(x_n)$. So if the sequence $(f(x_n))$ is bounded, then so is  $(\psi_{x_n}(f))$. Can we conclude that this implies that $||\psi_{x_n} ||$ (and therefore $||x_n||_X$) is bounded? My intuition tells me it does, as $||\psi_{x_n} ||=\sup\limits_{f\neq0}\{\frac{||\psi_{x_n}(f)||}{||f||}\}$, but I am not sure since $\psi_{x_n}$ is a function while $(\psi_{x_n})$ is a sequence (of functions..).",,"['real-analysis', 'complex-analysis', 'functional-analysis', 'banach-spaces']"
60,Domain of double adjoint,Domain of double adjoint,,"For $T$ a densely defined linear, not necessarily bounded operator on a Hilbert space $\mathscr{H}$ , and $T^{**}$ the adjoint of $T$ 's adjoint, I read somewhere that $\text{ran}(T)=\text{ran}(T^{**})$ . Is that true? I know that $\text{ran}(T)\subset \text{ran}(T^{**})$ because of $\text{dom}(T)\subset \text{dom}(T^{**})$ , which also gives $\ker(T)\subset \ker(T^{**})$ , but I wouldn't know how to prove $\text{ran}(T^{**})\subset \text{ran}(T)$ .","For a densely defined linear, not necessarily bounded operator on a Hilbert space , and the adjoint of 's adjoint, I read somewhere that . Is that true? I know that because of , which also gives , but I wouldn't know how to prove .",T \mathscr{H} T^{**} T \text{ran}(T)=\text{ran}(T^{**}) \text{ran}(T)\subset \text{ran}(T^{**}) \text{dom}(T)\subset \text{dom}(T^{**}) \ker(T)\subset \ker(T^{**}) \text{ran}(T^{**})\subset \text{ran}(T),"['functional-analysis', 'operator-theory', 'adjoint-operators']"
61,"How is Riesz Lemma ""a substitute for orthogonality""?","How is Riesz Lemma ""a substitute for orthogonality""?",,I been reading the lemma and some consequences but cant figure out why whats stated in the wikipedia aritcle is true. https://en.wikipedia.org/wiki/Riesz%27s_lemma,I been reading the lemma and some consequences but cant figure out why whats stated in the wikipedia aritcle is true. https://en.wikipedia.org/wiki/Riesz%27s_lemma,,"['functional-analysis', 'soft-question']"
62,F. & M. Riesz theorem,F. & M. Riesz theorem,,Can someone explain me in which sense F. & M. Riesz theorem ( https://en.wikipedia.org/wiki/F._and_M._Riesz_theorem ) is important/interesting?,Can someone explain me in which sense F. & M. Riesz theorem ( https://en.wikipedia.org/wiki/F._and_M._Riesz_theorem ) is important/interesting?,,"['functional-analysis', 'measure-theory', 'fourier-series']"
63,Intuitive functional analysis book,Intuitive functional analysis book,,"I would like a functional analysis book like Terence Tao's Real Analysis and Measure Theory book, full of intuition. I am comfortable with concepts in linear algebra, real analysis, measure theory, and probability theory.","I would like a functional analysis book like Terence Tao's Real Analysis and Measure Theory book, full of intuition. I am comfortable with concepts in linear algebra, real analysis, measure theory, and probability theory.",,"['functional-analysis', 'reference-request', 'soft-question', 'book-recommendation', 'big-list']"
64,"concept of the classification of $C^\ast$-algebras, introduction/overview","concept of the classification of -algebras, introduction/overview",C^\ast,"I don't have a specific mathematical problem at the moment but nevertheless I hope, my question is suitable for math.stackexchange. I'm interested in $C^\ast$-algebras and I would like to begin with the study of classification of $C^\ast$-algebras soon. I think, this is a huge field and I heard that K-Theory for operator algebras is an important tool for classification (but I still don't know something about K-Theory). My question is: What exactly is meant by classification of $C^\ast$-algebras? What are interesting properties which $C^\ast$-algebras do have in common? Could you give me a short overview or do you know good literature for beginners, which gives a good overview or introduction of classification of $C^\ast$-algebras? I still know that every $C^\ast$-algebra could be identified with a sub-$C^\ast$-algebra of one of these three $C^\ast$-algebras: $C_0(X)$ (X localcompact, Hausdorff space), $C(X)$ (X compact, Hausdorff space) or $L(H)$ ($H$ is a Hilbert space). But it seems that classification means something different in this case, maybe something similar as in algebraic topology, if you consider homology of topological spaces for example.  In the field of algebraic topology, you can consider homology or cohomology of topological spaces to distinguish between the spaces. Greetings","I don't have a specific mathematical problem at the moment but nevertheless I hope, my question is suitable for math.stackexchange. I'm interested in $C^\ast$-algebras and I would like to begin with the study of classification of $C^\ast$-algebras soon. I think, this is a huge field and I heard that K-Theory for operator algebras is an important tool for classification (but I still don't know something about K-Theory). My question is: What exactly is meant by classification of $C^\ast$-algebras? What are interesting properties which $C^\ast$-algebras do have in common? Could you give me a short overview or do you know good literature for beginners, which gives a good overview or introduction of classification of $C^\ast$-algebras? I still know that every $C^\ast$-algebra could be identified with a sub-$C^\ast$-algebra of one of these three $C^\ast$-algebras: $C_0(X)$ (X localcompact, Hausdorff space), $C(X)$ (X compact, Hausdorff space) or $L(H)$ ($H$ is a Hilbert space). But it seems that classification means something different in this case, maybe something similar as in algebraic topology, if you consider homology of topological spaces for example.  In the field of algebraic topology, you can consider homology or cohomology of topological spaces to distinguish between the spaces. Greetings",,"['analysis', 'functional-analysis']"
65,Essential supremum via cumulant,Essential supremum via cumulant,,"Let $p(t)=\log \mathbb{E}[\exp (tX)]$ for $X$ real valued random variable. Now it holds (assuming that $p$ is smooth and finite on $\mathbb{R}$) that $p'(\infty)=\text{ess}\sup X$. How can I prove that? For step functions it is easy, but how can I (by simple methods) show that it holds for all random variables?","Let $p(t)=\log \mathbb{E}[\exp (tX)]$ for $X$ real valued random variable. Now it holds (assuming that $p$ is smooth and finite on $\mathbb{R}$) that $p'(\infty)=\text{ess}\sup X$. How can I prove that? For step functions it is easy, but how can I (by simple methods) show that it holds for all random variables?",,"['functional-analysis', 'probability-theory', 'probability-distributions', 'probability-limit-theorems']"
66,"Two ODEs, why is one solution the solution of the other?","Two ODEs, why is one solution the solution of the other?",,"This question is based on Zeidler II/B, Problem 30.2. Consider the ODE: find $u:[0,T] \to \mathbb{R}^n$ s.t.  $$u'(t) = F(t,u(t))$$ $$u(0) = u_0$$ given $F:[0,T]\times \mathbb{R}^n \to \mathbb{R}^n$ Caratheodory. We know that if it has a solution, it satisfies $|u(t)| \leq C$ for all $t$. Let us suppose that there is $f \in L^1(0,T)$ such that $|F(t,a)| \leq f(t)$ for all $t$ and all $a$ such that $|a| \leq 2C$. Consider the ODE $$v'(t) = \hat F(t,v(t))$$  $$v(0) = u_0$$ where (*) $$\hat F(t,a) =  \begin{cases} F(t,a) &: |a| \leq 2C\\ F(t, \frac{2Ca}{|a|}) &: |a| \geq 2C \end{cases}.$$ This ODE has a solution, since $\hat F(t,a)$ is dominated by $f$ on the whole of $\mathbb{R}^n$ by definition of $\hat F$. How do I show that the solution $v$ of the second ODE is also a solution of the first ODE (Zeidler claims this)? Of course it's obvious if the solution $|v(t)| \leq 2C$, but how about if it's greater than $2C$? (*) - in Zeidler, the second case is given as $F(t,2a/C|a|)$, which I think is wrong since it would not make $\hat F(t,\cdot)$ continuous. I posted this on MO https://mathoverflow.net/questions/213007/two-odes-why-is-one-the-solution-of-the-other-caratheodory-ode","This question is based on Zeidler II/B, Problem 30.2. Consider the ODE: find $u:[0,T] \to \mathbb{R}^n$ s.t.  $$u'(t) = F(t,u(t))$$ $$u(0) = u_0$$ given $F:[0,T]\times \mathbb{R}^n \to \mathbb{R}^n$ Caratheodory. We know that if it has a solution, it satisfies $|u(t)| \leq C$ for all $t$. Let us suppose that there is $f \in L^1(0,T)$ such that $|F(t,a)| \leq f(t)$ for all $t$ and all $a$ such that $|a| \leq 2C$. Consider the ODE $$v'(t) = \hat F(t,v(t))$$  $$v(0) = u_0$$ where (*) $$\hat F(t,a) =  \begin{cases} F(t,a) &: |a| \leq 2C\\ F(t, \frac{2Ca}{|a|}) &: |a| \geq 2C \end{cases}.$$ This ODE has a solution, since $\hat F(t,a)$ is dominated by $f$ on the whole of $\mathbb{R}^n$ by definition of $\hat F$. How do I show that the solution $v$ of the second ODE is also a solution of the first ODE (Zeidler claims this)? Of course it's obvious if the solution $|v(t)| \leq 2C$, but how about if it's greater than $2C$? (*) - in Zeidler, the second case is given as $F(t,2a/C|a|)$, which I think is wrong since it would not make $\hat F(t,\cdot)$ continuous. I posted this on MO https://mathoverflow.net/questions/213007/two-odes-why-is-one-the-solution-of-the-other-caratheodory-ode",,"['real-analysis', 'functional-analysis', 'ordinary-differential-equations']"
67,Weierstrass's M-test example for uniform convergence and switching Sum and Integral.,Weierstrass's M-test example for uniform convergence and switching Sum and Integral.,,"How would I go about finding $M_n$ in \begin{equation} \sum_{n=1}^{\infty} \int_{0}^\infty x^{\frac{s}{2}-1}e^{-\pi n^{2}x}dx \end{equation} to show that it is uniformly convergent? UPDATE: Just coming back to this and having trouble understanding this. Without relating this integral  to the zeta and gamma functions, how do I show the sum has uniform convergence ( Weierstrass)? For $Re(s) > 1$. And can therefore swap integral with summation by fubinis theorem?","How would I go about finding $M_n$ in \begin{equation} \sum_{n=1}^{\infty} \int_{0}^\infty x^{\frac{s}{2}-1}e^{-\pi n^{2}x}dx \end{equation} to show that it is uniformly convergent? UPDATE: Just coming back to this and having trouble understanding this. Without relating this integral  to the zeta and gamma functions, how do I show the sum has uniform convergence ( Weierstrass)? For $Re(s) > 1$. And can therefore swap integral with summation by fubinis theorem?",,"['complex-analysis', 'functional-analysis', 'riemann-zeta']"
68,Spanning set is closed.,Spanning set is closed.,,"Suppose $\{e_1,e_2,\ldots,e_n\}$ is an orthonormal set in $\mathscr{H}$ (Hilbert space) and define $$M \equiv \operatorname{span}\{e_1,e_2,\ldots,e_n\}.$$ Show that $M$ is closed. Can I show that $M$ is closed by first making use of the fact that it is a finite dimensional subspace?","Suppose $\{e_1,e_2,\ldots,e_n\}$ is an orthonormal set in $\mathscr{H}$ (Hilbert space) and define $$M \equiv \operatorname{span}\{e_1,e_2,\ldots,e_n\}.$$ Show that $M$ is closed. Can I show that $M$ is closed by first making use of the fact that it is a finite dimensional subspace?",,"['linear-algebra', 'functional-analysis', 'hilbert-spaces']"
69,Being compact is necessary for a continuous bijection to have a continuous inverse,Being compact is necessary for a continuous bijection to have a continuous inverse,,"Theorem: Suppose that $f:X \rightarrow Y$ is one-to-one, surjective and continuous. If $X$ is compact, Then $f^{-1}:Y \rightarrow X$ is also continuous. The proof for this theorem is pretty easy. We can show that the inverse of every close set in $X$ is closed in $Y$ (Am I right?) Now I want to show that being compact is necessary. My example is: $$f(t) = (\cos(t),\sin(t)), \forall t\in [0,2\pi)$$ $f$ is one-to-one, surjective and continuous. But $f^{-1}$ is not continuous. Now I'm looking for more examples. Or maybe a set of infinite examples.","Theorem: Suppose that $f:X \rightarrow Y$ is one-to-one, surjective and continuous. If $X$ is compact, Then $f^{-1}:Y \rightarrow X$ is also continuous. The proof for this theorem is pretty easy. We can show that the inverse of every close set in $X$ is closed in $Y$ (Am I right?) Now I want to show that being compact is necessary. My example is: $$f(t) = (\cos(t),\sin(t)), \forall t\in [0,2\pi)$$ $f$ is one-to-one, surjective and continuous. But $f^{-1}$ is not continuous. Now I'm looking for more examples. Or maybe a set of infinite examples.",,"['real-analysis', 'functional-analysis']"
70,The (un)boundedness of an involutive operator,The (un)boundedness of an involutive operator,,"This is a question I've been thinking about for a while, for which I do not have a satisfactory answer. Suppose that $T$ is a densely-defined operator on a Hilbert space $\mathcal{H}$ such that $R(T)\subseteq D(T)$ and $T^2x = x$ for all $x\in D(T)$. Is it possible for $T$ to be unbounded? Or must all densely-defined involutions be bounded? I would think that it must be bounded since coming up with an explicit counter-example is pretty challenging, but that may just be a testament to my lack of creativity with unbounded operators.","This is a question I've been thinking about for a while, for which I do not have a satisfactory answer. Suppose that $T$ is a densely-defined operator on a Hilbert space $\mathcal{H}$ such that $R(T)\subseteq D(T)$ and $T^2x = x$ for all $x\in D(T)$. Is it possible for $T$ to be unbounded? Or must all densely-defined involutions be bounded? I would think that it must be bounded since coming up with an explicit counter-example is pretty challenging, but that may just be a testament to my lack of creativity with unbounded operators.",,"['functional-analysis', 'operator-theory', 'involutions']"
71,Find an inner product that makes a given set of linearly independent vectors orthogonal,Find an inner product that makes a given set of linearly independent vectors orthogonal,,"I need to find an inner product such that given a set $S$ of linearly independent vectors in a Hilbert space $H$, $S$ will be orthogonal with these product. I thought Gram -Schmidt Process would help but it's not, because for the process you already have the inner product.","I need to find an inner product such that given a set $S$ of linearly independent vectors in a Hilbert space $H$, $S$ will be orthogonal with these product. I thought Gram -Schmidt Process would help but it's not, because for the process you already have the inner product.",,"['linear-algebra', 'functional-analysis', 'hilbert-spaces', 'inner-products']"
72,Bergman space. What is area measure?,Bergman space. What is area measure?,,"I have read that the Bergman space $A^p(\Omega)$ consist of all the analytic functions $f$ in $\Omega$, such that  $$ \left( \int_{\Omega} |f(z)|^p dA \right)^{1/p} < \infty $$ where $dA$ is the area measure. I am very confuse with what area measure means, I haven't took measure theory yet, my intuition says that  $$ \int_{\Omega} |f(z)|^p dA = \iint_{\Omega}|f(x+iy)|^p dxdy  $$ Is this right ?? If not how can I compute an integral with $dA$? How about now if $dA_w$ is the weighted area measure?","I have read that the Bergman space $A^p(\Omega)$ consist of all the analytic functions $f$ in $\Omega$, such that  $$ \left( \int_{\Omega} |f(z)|^p dA \right)^{1/p} < \infty $$ where $dA$ is the area measure. I am very confuse with what area measure means, I haven't took measure theory yet, my intuition says that  $$ \int_{\Omega} |f(z)|^p dA = \iint_{\Omega}|f(x+iy)|^p dxdy  $$ Is this right ?? If not how can I compute an integral with $dA$? How about now if $dA_w$ is the weighted area measure?",,"['functional-analysis', 'measure-theory', 'area', 'bergman-spaces']"
73,Defining a bounded operator on $l^p$,Defining a bounded operator on,l^p,"Let $(c_{jk})_{j,k \in \mathbb{N}} \subset \mathbb{C}$ be such that $a:=\sup_{k \in \mathbb{N}} \sum_{j \in \mathbb{N}}|c_{jk}|<\infty$ and $b:=\sup_{j \in \mathbb{N}} \sum_{k \in \mathbb{N}}|c_{jk}|<\infty$ Prove that $$T:l^p \to l^p,(Tx)_j:=\sum_{k \in \mathbb{N}}c_{jk}x_k$$ defines a bounded linear map with $\|T\|\leq a^{\frac{1}{p}}b^{\frac{1}{q}}$ where $p \in (1,\infty)$ and $q$ is its Hölder conjugate I have spent a couple of hours trying to prove this inequality but nothing seems to be working . I can't even prove that T is a bounded operator. Any hints on how could I go about solving this ? I started by writing the definition of the norm as $\|T\|=\sup_{\|x\|_{p}=1}\big(\|Tx\|\big)=\sup_{\|x\|_{p}=1}\big(\sum_{j \in \mathbb{N}} \big(\sum_{j \in \mathbb{N}}c_{jk}x_k \big)^{p} \big)^{1/p}$ Then I tried to use Holder's inequality  inside bracket but nothing seems to be working . I mean I can try to write down what i tried to do, but none of my attempts seem to go anywhere Any hint would be appreciated.","Let $(c_{jk})_{j,k \in \mathbb{N}} \subset \mathbb{C}$ be such that $a:=\sup_{k \in \mathbb{N}} \sum_{j \in \mathbb{N}}|c_{jk}|<\infty$ and $b:=\sup_{j \in \mathbb{N}} \sum_{k \in \mathbb{N}}|c_{jk}|<\infty$ Prove that $$T:l^p \to l^p,(Tx)_j:=\sum_{k \in \mathbb{N}}c_{jk}x_k$$ defines a bounded linear map with $\|T\|\leq a^{\frac{1}{p}}b^{\frac{1}{q}}$ where $p \in (1,\infty)$ and $q$ is its Hölder conjugate I have spent a couple of hours trying to prove this inequality but nothing seems to be working . I can't even prove that T is a bounded operator. Any hints on how could I go about solving this ? I started by writing the definition of the norm as $\|T\|=\sup_{\|x\|_{p}=1}\big(\|Tx\|\big)=\sup_{\|x\|_{p}=1}\big(\sum_{j \in \mathbb{N}} \big(\sum_{j \in \mathbb{N}}c_{jk}x_k \big)^{p} \big)^{1/p}$ Then I tried to use Holder's inequality  inside bracket but nothing seems to be working . I mean I can try to write down what i tried to do, but none of my attempts seem to go anywhere Any hint would be appreciated.",,"['functional-analysis', 'inequality', 'lp-spaces']"
74,Inverse Function Theorem for Banach Spaces,Inverse Function Theorem for Banach Spaces,,"In the middle of a proof of the Inverse Function Theorem (namely, the proof of Baby Rudin), we use the fact that if $A$ is invertible and: $$ ||B-A||~||A^{-1}|| <1$$ then $B$ is invertible. The proof for this, however, relies on the fact that the vector spaces are finite dimensional (because it concludes that $B$ is bijective by using that it is injective). How do we circumvent this, if we want to prove that IFT in banach spaces?","In the middle of a proof of the Inverse Function Theorem (namely, the proof of Baby Rudin), we use the fact that if $A$ is invertible and: $$ ||B-A||~||A^{-1}|| <1$$ then $B$ is invertible. The proof for this, however, relies on the fact that the vector spaces are finite dimensional (because it concludes that $B$ is bijective by using that it is injective). How do we circumvent this, if we want to prove that IFT in banach spaces?",,"['real-analysis', 'functional-analysis']"
75,Eigenfunctions of $-\Delta_{S^n}$,Eigenfunctions of,-\Delta_{S^n},"I read somewhere that the eigenvalues of the Laplacian $-\Delta_{S^n}$ on the sphere $S^n$ consist of $k^2 + (n - 1)k$, with the corresponding eigenspace $V_k$ consisting of homogeneous harmonic polynomials defined on $\mathbb{R}^{n + 1}$. I was trying to derive this myself, but this is where I got stuck: I can prove that if a function is in $V_k$, then, it is the restriction of a homogeneous harmonic polynomial of degree $k$ on $\mathbb{R}^{n + 1}$, but I cannot prove the converse. That is, I cannot prove that a homogeneous harmonic polynomial of degree $k$ defined on $\mathbb{R}^{n + 1}$ restricted to $S^n$ will be an eigenfunction of $-\Delta_{S^n}$ with eigenvalue $k^2 + (n - 1)k$. Any help is appreciated, thanks. Addendum: I see that if I can prove that a homogeneous harmonic polynomial of degree $k$ on $\mathbb{R}^{n  + 1}$ can be expressed as $r^k \varphi (\omega)$, where $\omega \in S^n$, the rest follows. I am not sure how to claim this.","I read somewhere that the eigenvalues of the Laplacian $-\Delta_{S^n}$ on the sphere $S^n$ consist of $k^2 + (n - 1)k$, with the corresponding eigenspace $V_k$ consisting of homogeneous harmonic polynomials defined on $\mathbb{R}^{n + 1}$. I was trying to derive this myself, but this is where I got stuck: I can prove that if a function is in $V_k$, then, it is the restriction of a homogeneous harmonic polynomial of degree $k$ on $\mathbb{R}^{n + 1}$, but I cannot prove the converse. That is, I cannot prove that a homogeneous harmonic polynomial of degree $k$ defined on $\mathbb{R}^{n + 1}$ restricted to $S^n$ will be an eigenfunction of $-\Delta_{S^n}$ with eigenvalue $k^2 + (n - 1)k$. Any help is appreciated, thanks. Addendum: I see that if I can prove that a homogeneous harmonic polynomial of degree $k$ on $\mathbb{R}^{n  + 1}$ can be expressed as $r^k \varphi (\omega)$, where $\omega \in S^n$, the rest follows. I am not sure how to claim this.",,"['functional-analysis', 'differential-geometry']"
76,Compute Quotient Space,Compute Quotient Space,,"I have been struggling with this computation for a while now. I thought I was almost there, but it now results I still have nothing. So here is the initial problem: Let $c=\left\{ (x_j)_j \subset \mathbb{C}: (x_j)_j \ \text{is a convergent sequence }\right\}$ equipped with the supremum norm $\|\cdot\|_\infty$, and $Y=\left\{ (x_j)_j \in c : (x_j)_j \ \text{is a constant sequence }\right\}$. Compute $c/Y$, that is find the Banach space to whom $c/Y$ is isometrically isomorphic. Three things one must know to tackle this problem: 1) The equivalence relation consider here is $x \sim y$ iff $\ x-y \in Y$ 2) The quotient space is $c/Y = \{ [x]: x \in c \}$ 3) The norm in $c/Y$ is given by $\|[x]\|=\inf_{y \in [x]} \{\|y\|_\infty\}$ My advances: I already have proved that $Y \subset c$ is a closed subspace, so indeed $c/Y$ is a Banach space. Lets now consider $c_0=\left\{ (x_j)_j \in c: x_j \to 0 \text{ as } j \to \infty \right\}$, and define $S \subset c_0$ as follows $$ S= \left\{ (z_j)_j \in c_0 : \sum_{j=1}^{\infty} z_j \ \text{ converges in } \mathbb{C} \right\}. $$ I have fisrt conjectured that $c/Y \cong S$, and by defining $\Phi: c/Y \to S$ as $$ \Phi([(x_j)_j]) : = (x_j-x_{j+1})_j \ \ \ \text{for } \ \ (x_j)_j \in c $$ I successfully showed that $\Phi$ is i)Linear , ii)Injective and iii) Onto , however, I could not prove the iv)Isometry part. EDIT: I now know thanks to @Jochen comment that proving iv) is impossible since $S$ is not it self a Banach space, and I thought it was, so $c/Y$ and $S$ are only isomorphic, but they are not the same Banach space. So my question now changes, since $S$ is not isometrically isomorphic to $c/Y$ which Banach space must be? It is correct to still looking for a $c_0$ subspace ot it might be something totally different.I am clueless since all my bets where on $S$. I would appreciate it very much any help given here.","I have been struggling with this computation for a while now. I thought I was almost there, but it now results I still have nothing. So here is the initial problem: Let $c=\left\{ (x_j)_j \subset \mathbb{C}: (x_j)_j \ \text{is a convergent sequence }\right\}$ equipped with the supremum norm $\|\cdot\|_\infty$, and $Y=\left\{ (x_j)_j \in c : (x_j)_j \ \text{is a constant sequence }\right\}$. Compute $c/Y$, that is find the Banach space to whom $c/Y$ is isometrically isomorphic. Three things one must know to tackle this problem: 1) The equivalence relation consider here is $x \sim y$ iff $\ x-y \in Y$ 2) The quotient space is $c/Y = \{ [x]: x \in c \}$ 3) The norm in $c/Y$ is given by $\|[x]\|=\inf_{y \in [x]} \{\|y\|_\infty\}$ My advances: I already have proved that $Y \subset c$ is a closed subspace, so indeed $c/Y$ is a Banach space. Lets now consider $c_0=\left\{ (x_j)_j \in c: x_j \to 0 \text{ as } j \to \infty \right\}$, and define $S \subset c_0$ as follows $$ S= \left\{ (z_j)_j \in c_0 : \sum_{j=1}^{\infty} z_j \ \text{ converges in } \mathbb{C} \right\}. $$ I have fisrt conjectured that $c/Y \cong S$, and by defining $\Phi: c/Y \to S$ as $$ \Phi([(x_j)_j]) : = (x_j-x_{j+1})_j \ \ \ \text{for } \ \ (x_j)_j \in c $$ I successfully showed that $\Phi$ is i)Linear , ii)Injective and iii) Onto , however, I could not prove the iv)Isometry part. EDIT: I now know thanks to @Jochen comment that proving iv) is impossible since $S$ is not it self a Banach space, and I thought it was, so $c/Y$ and $S$ are only isomorphic, but they are not the same Banach space. So my question now changes, since $S$ is not isometrically isomorphic to $c/Y$ which Banach space must be? It is correct to still looking for a $c_0$ subspace ot it might be something totally different.I am clueless since all my bets where on $S$. I would appreciate it very much any help given here.",,"['functional-analysis', 'banach-spaces', 'quotient-spaces']"
77,"Checking that $(C[0,1], \|\cdot\|_1)$ is not Banach.",Checking that  is not Banach.,"(C[0,1], \|\cdot\|_1)","I want to check that $(C[0, 1], \|\cdot\|_1)$ is not a Banach space, where $$\|f\|_1 = \int_0^1 |f(x)|\,{\rm d}x.$$ I took $(f_n)_{n \geq 1}$ a sequence in $C[0, 1]$ given by: $$f_n(x) = \begin{cases} nx, &\text{ if } x < \frac{1}{n} \\ 1, &\text{ if } x \geq \frac{1}{n} \end{cases} $$ If $m > n$, I computed: $$\|f_m - f_n\|_1 = 1 - \frac{n+1}{m} + \frac{1}{n} + \frac{n}{2}\left( \frac{1}{m^2}-\frac{1}{n^2} \right)$$ It is intuitive that the sequence is Cauchy, but I don't know how to exactly formalize this using the definition of a Cauchy sequence. Sending $m,n$ to $+\infty $ at the same time intuitively gives zero, but it doesn't seem rigorous to me. How can I formalize this $m,n \to \infty$? To proving that the sequence does not converge also seems intuitive, we would get a sort of vertical line - not a function. But I'm also not sure of how to approach this - the only thing that comes to mind is some kind of contradiction - suppose that $f_n \to f$, and find $\epsilon > 0 $ such that $\|f_n - f\| > \epsilon $ for infinite values of $n$. Is the above idea in the right track? Thanks.","I want to check that $(C[0, 1], \|\cdot\|_1)$ is not a Banach space, where $$\|f\|_1 = \int_0^1 |f(x)|\,{\rm d}x.$$ I took $(f_n)_{n \geq 1}$ a sequence in $C[0, 1]$ given by: $$f_n(x) = \begin{cases} nx, &\text{ if } x < \frac{1}{n} \\ 1, &\text{ if } x \geq \frac{1}{n} \end{cases} $$ If $m > n$, I computed: $$\|f_m - f_n\|_1 = 1 - \frac{n+1}{m} + \frac{1}{n} + \frac{n}{2}\left( \frac{1}{m^2}-\frac{1}{n^2} \right)$$ It is intuitive that the sequence is Cauchy, but I don't know how to exactly formalize this using the definition of a Cauchy sequence. Sending $m,n$ to $+\infty $ at the same time intuitively gives zero, but it doesn't seem rigorous to me. How can I formalize this $m,n \to \infty$? To proving that the sequence does not converge also seems intuitive, we would get a sort of vertical line - not a function. But I'm also not sure of how to approach this - the only thing that comes to mind is some kind of contradiction - suppose that $f_n \to f$, and find $\epsilon > 0 $ such that $\|f_n - f\| > \epsilon $ for infinite values of $n$. Is the above idea in the right track? Thanks.",,"['real-analysis', 'functional-analysis', 'banach-spaces']"
78,What can we say about the inner product of two Cauchy sequences?,What can we say about the inner product of two Cauchy sequences?,,"Let $(x_n)$, $(y_n)$ be two Cauchy sequences in an inner a real or complex product space $X$, and let the sequence $(\alpha_n)$ be given by  $$ \alpha_n \colon= \ \langle x_n, y_n \rangle \ \ \ \mbox{ for } \ n = 1, 2, 3, \ldots.  $$ Then is the sequence $(a_n)$ also Cauchy and therefore convergent? If so, how to show this rigorously? If not, what counter-example can be given?","Let $(x_n)$, $(y_n)$ be two Cauchy sequences in an inner a real or complex product space $X$, and let the sequence $(\alpha_n)$ be given by  $$ \alpha_n \colon= \ \langle x_n, y_n \rangle \ \ \ \mbox{ for } \ n = 1, 2, 3, \ldots.  $$ Then is the sequence $(a_n)$ also Cauchy and therefore convergent? If so, how to show this rigorously? If not, what counter-example can be given?",,"['real-analysis', 'analysis', 'functional-analysis', 'inner-products', 'cauchy-sequences']"
79,Identity Operator can be uniformly approximated by orthonormal basis,Identity Operator can be uniformly approximated by orthonormal basis,,"Let $H$ be a separable Hilbert space with orthonormal basis $e_1, e_2, ...$.  I know that for any $x \in H$, we have $$\|x\|^2 = \sum\limits_n \|\langle x, e_n \rangle\|^2$$ and in fact $x = \lim\limits_{N \to \infty} \sum\limits_{n=1}^N \langle x, e_n \rangle e_n$ in the norm topology in $H$.  I was wonderdering whether this approximation is in any sense uniform.  In particular, whether it is true that $$\|1_H - T_N\| \to 0$$ where $T_N = \sum\limits_{n=1}^N \langle -,e_n \rangle e_n$.  I don't expect this to be true, but it would make my life a lot easier right now if it was.","Let $H$ be a separable Hilbert space with orthonormal basis $e_1, e_2, ...$.  I know that for any $x \in H$, we have $$\|x\|^2 = \sum\limits_n \|\langle x, e_n \rangle\|^2$$ and in fact $x = \lim\limits_{N \to \infty} \sum\limits_{n=1}^N \langle x, e_n \rangle e_n$ in the norm topology in $H$.  I was wonderdering whether this approximation is in any sense uniform.  In particular, whether it is true that $$\|1_H - T_N\| \to 0$$ where $T_N = \sum\limits_{n=1}^N \langle -,e_n \rangle e_n$.  I don't expect this to be true, but it would make my life a lot easier right now if it was.",,"['functional-analysis', 'hilbert-spaces']"
80,Hölder continuity of $\frac1x$,Hölder continuity of,\frac1x,"I have a question. Is the function $f(x)=1/x$ Hölder continuous if $x\in (\varepsilon,+\infty),\ \varepsilon>0$?","I have a question. Is the function $f(x)=1/x$ Hölder continuous if $x\in (\varepsilon,+\infty),\ \varepsilon>0$?",,"['real-analysis', 'functional-analysis', 'continuity', 'holder-spaces']"
81,what are differences between metric space and metric linear space?,what are differences between metric space and metric linear space?,,A metric linear space is a linear space equipped with metric but i want to know the point wise differences between metric space and metric linear space.Can any body write it down in points?,A metric linear space is a linear space equipped with metric but i want to know the point wise differences between metric space and metric linear space.Can any body write it down in points?,,['functional-analysis']
82,Good books about differentiation in normed spaces?,Good books about differentiation in normed spaces?,,"Typical functional analysis books don't seem to cover this subject at all, so I'm looking for some good books that deal with differentiation in normed spaces(Gateaux/Frechet derivatives etc.). Preferably moving also into other calculus-like topics in infinite dimensional spaces, like integration on Banach spaces, perhaps Banach manifolds. Any recommendations?","Typical functional analysis books don't seem to cover this subject at all, so I'm looking for some good books that deal with differentiation in normed spaces(Gateaux/Frechet derivatives etc.). Preferably moving also into other calculus-like topics in infinite dimensional spaces, like integration on Banach spaces, perhaps Banach manifolds. Any recommendations?",,"['analysis', 'functional-analysis', 'reference-request', 'normed-spaces']"
83,Every finite-dimensional subspace is one-complemented,Every finite-dimensional subspace is one-complemented,,"Let $X$ be a Banach space. It is known that if every closed subspace of $X$ is one-complemented, then $X$ is isometrically isomorphic to a Hilbert space. Now if every finite-dimensional subspace of $X$ is one-complemented, is it true that is $X$ isometrically isomorphic to a Hilbert space?","Let $X$ be a Banach space. It is known that if every closed subspace of $X$ is one-complemented, then $X$ is isometrically isomorphic to a Hilbert space. Now if every finite-dimensional subspace of $X$ is one-complemented, is it true that is $X$ isometrically isomorphic to a Hilbert space?",,"['functional-analysis', 'banach-spaces']"
84,Integration by parts for weak derivatives,Integration by parts for weak derivatives,,"I'm trying to show that if $g$ is such that $f(b) - f(a) = \int_a^b g(t) dt$ for any $a<b \in \mathbb{R}$ then we have: (for $f, g \in L^2(\mathbb{R})$) $$\int_a^b f(t)g(t) = \frac{1}{2}(f(b)^2 - f(a)^2)$$ I can see this would follow if we could treat $g$ as the derivative of $f$ and use integration by parts but I'm not quite sure how to justify this. Thanks for any help","I'm trying to show that if $g$ is such that $f(b) - f(a) = \int_a^b g(t) dt$ for any $a<b \in \mathbb{R}$ then we have: (for $f, g \in L^2(\mathbb{R})$) $$\int_a^b f(t)g(t) = \frac{1}{2}(f(b)^2 - f(a)^2)$$ I can see this would follow if we could treat $g$ as the derivative of $f$ and use integration by parts but I'm not quite sure how to justify this. Thanks for any help",,['functional-analysis']
85,Compact operator space is the greatest ideal of $B(H)$,Compact operator space is the greatest ideal of,B(H),"Suppose $H$ is a separable infinite dimensional Hilbert space. Show that if $A\in B(H)$ is noncompact, then there exist two operators $B,C$ such that $BAC=1$. Clearly if $A$ is invertible it holds, but if $0\in \sigma(A)$, I do not have any idea. Please help me. Thanks.","Suppose $H$ is a separable infinite dimensional Hilbert space. Show that if $A\in B(H)$ is noncompact, then there exist two operators $B,C$ such that $BAC=1$. Clearly if $A$ is invertible it holds, but if $0\in \sigma(A)$, I do not have any idea. Please help me. Thanks.",,"['functional-analysis', 'operator-theory', 'c-star-algebras', 'compact-operators']"
86,A function $f$ such that $f \in L_1$ but $f \notin L_p$ for $p>1$ [duplicate],A function  such that  but  for  [duplicate],f f \in L_1 f \notin L_p p>1,"This question already has answers here : $f \in L^1$, but $f \not\in L^p$ for all $p > 1$ (2 answers) Closed 9 years ago . I want find a function $f: [0,1] \mapsto \mathbb{R}$ such that $f \in L_1[0,1]$ but $f \notin L_p[0,1]$ for all $p>1$. My attempts:  First I thought in the family of functions $\frac{1}{x^\alpha}$ but this function belongs to $L_q$ iff $\alpha \cdot q \leqslant 1$ so I need find $\alpha$ such that: $\alpha <1 $ and $\alpha \cdot q \geqslant 1$ for all $q>1$ but this its impossible!! After other attempts using variations and combinations of $1/x$, $ln x$ and $e^x$ I researched in the mathstack and found this questions: Prove that for any $1 < p < ∞$ there exists a function $f ∈ L_p(μ)$ such that $f \notin L_q(μ)$ for any $q > p.$ The kingkongdonutguy's question is exactly what I was looking for, but I do not understand very well the Tomas' (and of Davide) hint... My interpretation: Choice two sequences $\{a_n\}_{n \in \mathbb{N}}$ and $\{t_n\}_{n \in \mathbb{N}}$ com $a_n,t_n \to 0$ now make a sequence os disjoint intervals $\{I_n\}_{n \in \mathbb{N}}$ such that, for each $n$,$0 < m(I_n) < t_n$ and $\bigcup I_n = [0,1]$. Define a function: $$f(x)= \sum\limits_{n=1}^{\infty} a_n \cdot \chi_{I_n}(x)$$ Make a simple calculation: $$\int\limits_{0}^{1} f(x)dx = \sum\limits_{n=1}^{\infty} \int_{I_n} a_n dx = \sum\limits_{n=1}^{\infty} a_n\cdot m(I_n) \leqslant \sum a_n \cdot t_n$$ So I need choice $\{a_n\}$ and $\{t_n\}$ such that $\sum a_n \cdot t_n$ converges but $\sum a_n ^{p} \cdot t_n$ not converges for $p>1$. The problem: using limit comparison test we have $$\lim_{n \to \infty} \frac{a_n^p \cdot t_n}{a_n\cdot t_n} = \lim_{n \to \infty} a_n^{p-1}=0 $$(because $p>1$) so don't is possible this choice ... Also found this question Is it possible for a function to be in $L^p$ for only one $p$? but I could not adapt for a finite domain.. Someone can give me a (other) hint to construct this function??","This question already has answers here : $f \in L^1$, but $f \not\in L^p$ for all $p > 1$ (2 answers) Closed 9 years ago . I want find a function $f: [0,1] \mapsto \mathbb{R}$ such that $f \in L_1[0,1]$ but $f \notin L_p[0,1]$ for all $p>1$. My attempts:  First I thought in the family of functions $\frac{1}{x^\alpha}$ but this function belongs to $L_q$ iff $\alpha \cdot q \leqslant 1$ so I need find $\alpha$ such that: $\alpha <1 $ and $\alpha \cdot q \geqslant 1$ for all $q>1$ but this its impossible!! After other attempts using variations and combinations of $1/x$, $ln x$ and $e^x$ I researched in the mathstack and found this questions: Prove that for any $1 < p < ∞$ there exists a function $f ∈ L_p(μ)$ such that $f \notin L_q(μ)$ for any $q > p.$ The kingkongdonutguy's question is exactly what I was looking for, but I do not understand very well the Tomas' (and of Davide) hint... My interpretation: Choice two sequences $\{a_n\}_{n \in \mathbb{N}}$ and $\{t_n\}_{n \in \mathbb{N}}$ com $a_n,t_n \to 0$ now make a sequence os disjoint intervals $\{I_n\}_{n \in \mathbb{N}}$ such that, for each $n$,$0 < m(I_n) < t_n$ and $\bigcup I_n = [0,1]$. Define a function: $$f(x)= \sum\limits_{n=1}^{\infty} a_n \cdot \chi_{I_n}(x)$$ Make a simple calculation: $$\int\limits_{0}^{1} f(x)dx = \sum\limits_{n=1}^{\infty} \int_{I_n} a_n dx = \sum\limits_{n=1}^{\infty} a_n\cdot m(I_n) \leqslant \sum a_n \cdot t_n$$ So I need choice $\{a_n\}$ and $\{t_n\}$ such that $\sum a_n \cdot t_n$ converges but $\sum a_n ^{p} \cdot t_n$ not converges for $p>1$. The problem: using limit comparison test we have $$\lim_{n \to \infty} \frac{a_n^p \cdot t_n}{a_n\cdot t_n} = \lim_{n \to \infty} a_n^{p-1}=0 $$(because $p>1$) so don't is possible this choice ... Also found this question Is it possible for a function to be in $L^p$ for only one $p$? but I could not adapt for a finite domain.. Someone can give me a (other) hint to construct this function??",,"['real-analysis', 'functional-analysis', 'lebesgue-integral', 'lp-spaces', 'lebesgue-measure']"
87,Weak formulation for nonhomogeneous problem $-\Delta u = 0$,Weak formulation for nonhomogeneous problem,-\Delta u = 0,"I am wondering about the definition of weak solution to the nonhomogeneous problem $$-\Delta u = 0 \text{ in }\Omega$$ $$u = g \text{ in }\partial\Omega$$ given $g \in H^{\frac 12}(\partial\Omega)$. It should be something like: $u \in H^1(\Omega)$ satisfies $$\int_\Omega \nabla u \nabla v - \int_{\partial\Omega} v \partial_\nu u = 0\quad\text{for all $v \in H^1(\Omega)$}\tag{1}$$ and $\gamma(u) = g$ where $\gamma$ is the trace operator. Edit : as suggested by Tomas, the normal derivative is not defined for $H^1$ functions. So I am not sure what the natural weak formulation should be for this problem. I could test with $H^1_0$ functions instead, but this seems unnatural and we obtain a mixed bilinear form. Let $G$ extend $g$, and consider the homogeneous problem $$-\Delta w = \Delta G \text{ in }\Omega$$ $$w = 0 \text{ in }\partial\Omega$$ This is well-posed via Lax-Milgram and we have $w \in H^1_0(\Omega)$ such that $$\int_\Omega \nabla w \nabla \varphi = -\int_\Omega \nabla G \nabla \varphi$$ for all $\varphi \in H^1_0(\Omega)$. So then set $u=w+G \in H^1(\Omega)$, then $\gamma(u) = g$ and $u$ satisfies $$\int_\Omega \nabla u\nabla \varphi = 0\quad\text{for all $\varphi \in H^1_0(\Omega)$.}$$ How do I reconcile this with what I think should be the weak form $\text{(1)}$? Notice the different spaces the test functions lie in.","I am wondering about the definition of weak solution to the nonhomogeneous problem $$-\Delta u = 0 \text{ in }\Omega$$ $$u = g \text{ in }\partial\Omega$$ given $g \in H^{\frac 12}(\partial\Omega)$. It should be something like: $u \in H^1(\Omega)$ satisfies $$\int_\Omega \nabla u \nabla v - \int_{\partial\Omega} v \partial_\nu u = 0\quad\text{for all $v \in H^1(\Omega)$}\tag{1}$$ and $\gamma(u) = g$ where $\gamma$ is the trace operator. Edit : as suggested by Tomas, the normal derivative is not defined for $H^1$ functions. So I am not sure what the natural weak formulation should be for this problem. I could test with $H^1_0$ functions instead, but this seems unnatural and we obtain a mixed bilinear form. Let $G$ extend $g$, and consider the homogeneous problem $$-\Delta w = \Delta G \text{ in }\Omega$$ $$w = 0 \text{ in }\partial\Omega$$ This is well-posed via Lax-Milgram and we have $w \in H^1_0(\Omega)$ such that $$\int_\Omega \nabla w \nabla \varphi = -\int_\Omega \nabla G \nabla \varphi$$ for all $\varphi \in H^1_0(\Omega)$. So then set $u=w+G \in H^1(\Omega)$, then $\gamma(u) = g$ and $u$ satisfies $$\int_\Omega \nabla u\nabla \varphi = 0\quad\text{for all $\varphi \in H^1_0(\Omega)$.}$$ How do I reconcile this with what I think should be the weak form $\text{(1)}$? Notice the different spaces the test functions lie in.",,"['functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
88,Stone's One Parameter Unitary Group Theorem and the Fourier transform,Stone's One Parameter Unitary Group Theorem and the Fourier transform,,"Stone's theorem on one parameter unitary groups asserts a one-to-one correspondence between strongly continuous one parameter groups of unitary operators $\mathcal{H}\to\mathcal{H}$ on a Hilbert space $\mathcal{H}$ and self-adjoint operators $\mathcal{H}\to\mathcal{H}$ i.e. for each unitary group $$\left\{U:\mathbb{R}\times\mathcal{H}\to\mathcal{H};\begin{array}{ll}U(t)\,U^\dagger(t)=\mathrm{id}&\forall\,t\in\mathbb{R}\\ U(t)\,U(s)=U(s+t)&\forall\,s,\,t\in\mathbb{R}\\\lim\limits_{t\to t_0}U(t)\,X = U(t_0)\,X&\forall t_0\in \mathbb{R};\;X\in\mathcal{H}\end{array}\right\}$$ there is precisely one self adjoint $P:\mathcal{H}\to\mathcal{H}$ such that $U(t) = e^{i\,P\,t}$. My question is simple: with $\mathcal{H}$ the separable Hilbert space of complex $\mathbf{L}^2$ functions on $\mathbb{R}$, is there any such one parameter group which includes the Fourier transform? More informally: can we deform the identity operator into the Fourier transform through a one parameter family of unitary operators? I suspect the answer is no, but cannot see a reason for it.","Stone's theorem on one parameter unitary groups asserts a one-to-one correspondence between strongly continuous one parameter groups of unitary operators $\mathcal{H}\to\mathcal{H}$ on a Hilbert space $\mathcal{H}$ and self-adjoint operators $\mathcal{H}\to\mathcal{H}$ i.e. for each unitary group $$\left\{U:\mathbb{R}\times\mathcal{H}\to\mathcal{H};\begin{array}{ll}U(t)\,U^\dagger(t)=\mathrm{id}&\forall\,t\in\mathbb{R}\\ U(t)\,U(s)=U(s+t)&\forall\,s,\,t\in\mathbb{R}\\\lim\limits_{t\to t_0}U(t)\,X = U(t_0)\,X&\forall t_0\in \mathbb{R};\;X\in\mathcal{H}\end{array}\right\}$$ there is precisely one self adjoint $P:\mathcal{H}\to\mathcal{H}$ such that $U(t) = e^{i\,P\,t}$. My question is simple: with $\mathcal{H}$ the separable Hilbert space of complex $\mathbf{L}^2$ functions on $\mathbb{R}$, is there any such one parameter group which includes the Fourier transform? More informally: can we deform the identity operator into the Fourier transform through a one parameter family of unitary operators? I suspect the answer is no, but cannot see a reason for it.",,"['functional-analysis', 'fourier-analysis', 'representation-theory']"
89,$L_2$ norm and RHKS norm in Hilbert spaces $\mathcal{H}$,norm and RHKS norm in Hilbert spaces,L_2 \mathcal{H},"According to this paper (just right below the Theorem 3 and above the section 3) Reproducing Kernel Hilbert Space(RKHS) $\mathcal{H}$ on $\mathcal{X}$ is a   Hilbert space of functions from $\mathcal{X}$ to $\mathbb{R}$. $\mathcal{H}$ is an RKHS if and only if there exists a $k(x,x'):\mathcal{X}\times\mathcal{X}\to\mathbb{R}$ such that $\forall x\in\mathcal{X},\,k(x,\cdot)\in\mathcal{H}$, and $\forall f\in\mathcal{H},\, \langle f,k(x,\cdot)\rangle_{\mathcal{H}}=f(x)$. If such a $k$ exist, it is unique and it is a PD kernel. A function $f\in\mathcal{H}$ if and only if $\|f\|^2_{\mathcal{H}}=\langle f,f\rangle_{\mathcal{H}}<\infty$, and its $L_2$ norm is dominated by RKHS norm $\|f\|_{L_2}\le \|f\|_{\mathcal{H}}$. How the last claim can be done ? I thought this would be fundamentally mathematical, and the claim and related definitions are independent of this specific paper, thus it might be understood especially by people who had some background knowledge.  I want to know how to prove this claim, and I am a beginner and even do not know how to describe $\|f\|_{L_2} \leq \|f\|_{\mathcal{H}} $ explicitly . Thanks for all your help.","According to this paper (just right below the Theorem 3 and above the section 3) Reproducing Kernel Hilbert Space(RKHS) $\mathcal{H}$ on $\mathcal{X}$ is a   Hilbert space of functions from $\mathcal{X}$ to $\mathbb{R}$. $\mathcal{H}$ is an RKHS if and only if there exists a $k(x,x'):\mathcal{X}\times\mathcal{X}\to\mathbb{R}$ such that $\forall x\in\mathcal{X},\,k(x,\cdot)\in\mathcal{H}$, and $\forall f\in\mathcal{H},\, \langle f,k(x,\cdot)\rangle_{\mathcal{H}}=f(x)$. If such a $k$ exist, it is unique and it is a PD kernel. A function $f\in\mathcal{H}$ if and only if $\|f\|^2_{\mathcal{H}}=\langle f,f\rangle_{\mathcal{H}}<\infty$, and its $L_2$ norm is dominated by RKHS norm $\|f\|_{L_2}\le \|f\|_{\mathcal{H}}$. How the last claim can be done ? I thought this would be fundamentally mathematical, and the claim and related definitions are independent of this specific paper, thus it might be understood especially by people who had some background knowledge.  I want to know how to prove this claim, and I am a beginner and even do not know how to describe $\|f\|_{L_2} \leq \|f\|_{\mathcal{H}} $ explicitly . Thanks for all your help.",,"['functional-analysis', 'hilbert-spaces']"
90,Help with proof that $H^{1}(\mathbb{R})$ is closed under multiplication.,Help with proof that  is closed under multiplication.,H^{1}(\mathbb{R}),"Edit: Prove that if $u,v \in H^{1}(\mathbb{R})$ then $uv \in H^{1}(\mathbb{R})$. My idea is to approximate with functions in $C^{\infty}(\mathbb{R})$ with compact support. Let $u,v \in H^{1}(\mathbb{R})$. Since $C^{\infty}_0(\mathbb{R})$ is dense in $H^1(\mathbb{R})$ there exists sequences $u_n,v_n \in C^{\infty}(\mathbb{R})$ such that  $u_n \rightarrow u$ and $v_n \rightarrow v$ in $|| \cdot ||_{H^1} $. Now $$ uv - u_n v_n = (u-u_n)v + u_n(v-v_n) \Rightarrow $$ $$ || uv - u_n v_n||_{H^1} = || (u-u_n)v + u_n(v-v_n)||_{H^1} \leq  $$ $$|| (u-u_n)v ||_{H^1} + ||u_n(v-v_n)||_{H^1} \rightarrow 0 \quad,\quad n \rightarrow \infty $$ Thus $u_n v_n \rightarrow uv$ when $n \rightarrow \infty$. Since $u_n v_n \in C^{\infty}_0(\mathbb{R}) $ the product $uv \in H^1(\mathbb{R})$. Feedback and corrections are appreciated!","Edit: Prove that if $u,v \in H^{1}(\mathbb{R})$ then $uv \in H^{1}(\mathbb{R})$. My idea is to approximate with functions in $C^{\infty}(\mathbb{R})$ with compact support. Let $u,v \in H^{1}(\mathbb{R})$. Since $C^{\infty}_0(\mathbb{R})$ is dense in $H^1(\mathbb{R})$ there exists sequences $u_n,v_n \in C^{\infty}(\mathbb{R})$ such that  $u_n \rightarrow u$ and $v_n \rightarrow v$ in $|| \cdot ||_{H^1} $. Now $$ uv - u_n v_n = (u-u_n)v + u_n(v-v_n) \Rightarrow $$ $$ || uv - u_n v_n||_{H^1} = || (u-u_n)v + u_n(v-v_n)||_{H^1} \leq  $$ $$|| (u-u_n)v ||_{H^1} + ||u_n(v-v_n)||_{H^1} \rightarrow 0 \quad,\quad n \rightarrow \infty $$ Thus $u_n v_n \rightarrow uv$ when $n \rightarrow \infty$. Since $u_n v_n \in C^{\infty}_0(\mathbb{R}) $ the product $uv \in H^1(\mathbb{R})$. Feedback and corrections are appreciated!",,"['functional-analysis', 'sobolev-spaces']"
91,$c_0$ is not isometric to $c_0 \oplus c_0$,is not isometric to,c_0 c_0 \oplus c_0,"$c_0$ is the Banach space of sequences converging to zero and $c_0 \oplus c_0$ is its algebraical direct sum with itself equipped with the norm $\|(\xi,\eta)\| := \|\xi\|+\|\eta\|$. How to prove that this spaces is not linearly isometric (though they are isomorhic)? Thanks for any help.","$c_0$ is the Banach space of sequences converging to zero and $c_0 \oplus c_0$ is its algebraical direct sum with itself equipped with the norm $\|(\xi,\eta)\| := \|\xi\|+\|\eta\|$. How to prove that this spaces is not linearly isometric (though they are isomorhic)? Thanks for any help.",,['functional-analysis']
92,Is every regular polygon the unit ball for some norm?,Is every regular polygon the unit ball for some norm?,,"For every regular polygon, is there a norm such that the polygon is it's unit ball centered on 0?","For every regular polygon, is there a norm such that the polygon is it's unit ball centered on 0?",,"['geometry', 'functional-analysis', 'normed-spaces']"
93,Bounded spectrum implies bounded operator?,Bounded spectrum implies bounded operator?,,"Let $T$ be a self-adjoint operator with bounded spectrum $\sigma(T)$. Does this imply that $T$ is bounded? I would say: Yes! My attempt: $$||T|| = sup_{||x||=1} \langle Tx,x \rangle =sup_{||x||=1} \int_{\sigma(T)} \lambda d\mu_{x,x}(\lambda) \le \sup_{t \in \sigma(T)}|t| \quad sup_{||x||=1} ||\mu_{x,x}|| \le \sup_{t \in \sigma(T)}|t| < \infty .$$ I am new to the spectral theorem, so I don't know if my proof is correct.","Let $T$ be a self-adjoint operator with bounded spectrum $\sigma(T)$. Does this imply that $T$ is bounded? I would say: Yes! My attempt: $$||T|| = sup_{||x||=1} \langle Tx,x \rangle =sup_{||x||=1} \int_{\sigma(T)} \lambda d\mu_{x,x}(\lambda) \le \sup_{t \in \sigma(T)}|t| \quad sup_{||x||=1} ||\mu_{x,x}|| \le \sup_{t \in \sigma(T)}|t| < \infty .$$ I am new to the spectral theorem, so I don't know if my proof is correct.",,['real-analysis']
94,Partial Isometries: Introduction,Partial Isometries: Introduction,,Attention This question has been modified drastically. It is done so the answer below is still correct. It is done so to allow more specialized threads. Problem How do I deal with partial isometries? How come that partial isometries give rise to projections? And what about the that weird characterization given on wiki? Reference For more details see wiki's: Partial Isometries,Attention This question has been modified drastically. It is done so the answer below is still correct. It is done so to allow more specialized threads. Problem How do I deal with partial isometries? How come that partial isometries give rise to projections? And what about the that weird characterization given on wiki? Reference For more details see wiki's: Partial Isometries,,"['functional-analysis', 'operator-theory', 'hilbert-spaces', 'operator-algebras']"
95,Isometry between finite-dimensional space and its topological dual,Isometry between finite-dimensional space and its topological dual,,"Let $(X, \|\cdot \|_X)$ be a normed, finite dimensional vector space. Show that its topological dual $X'$ is isometrically isomorphic to $X$. The general form of such a linear map between those spaces should be: $$l : X \to X', x_o \mapsto l_{x_o} :=x_0^T A$$ for an $A \in \mathbb{R}^{n \times n}$ with $\dim X = n$, and $l_{x_0}(x) = x_0^T A x$ (representing $x_0$ and $x$ in $\mathbb{R}^n$, which should be possible by isomorphy). For this to be an isometry, I tried to find $A$ such that $$\|x_0\|_X \stackrel{!}{=} \|l_{x_0}\| = \sup_{x \in X \setminus \{0\}} \frac{|x_0^T A x|}{\|x\|_X},$$ the latter terms being the operator norm. If all my assumptions up to this point are correct, this should eventually lead me to the solution, but I just cannot come up with any information about the matrix $A$ that I need to find. It's probably really easy, but I just started on the subject. Thank you in advance!","Let $(X, \|\cdot \|_X)$ be a normed, finite dimensional vector space. Show that its topological dual $X'$ is isometrically isomorphic to $X$. The general form of such a linear map between those spaces should be: $$l : X \to X', x_o \mapsto l_{x_o} :=x_0^T A$$ for an $A \in \mathbb{R}^{n \times n}$ with $\dim X = n$, and $l_{x_0}(x) = x_0^T A x$ (representing $x_0$ and $x$ in $\mathbb{R}^n$, which should be possible by isomorphy). For this to be an isometry, I tried to find $A$ such that $$\|x_0\|_X \stackrel{!}{=} \|l_{x_0}\| = \sup_{x \in X \setminus \{0\}} \frac{|x_0^T A x|}{\|x\|_X},$$ the latter terms being the operator norm. If all my assumptions up to this point are correct, this should eventually lead me to the solution, but I just cannot come up with any information about the matrix $A$ that I need to find. It's probably really easy, but I just started on the subject. Thank you in advance!",,"['functional-analysis', 'vector-spaces']"
96,Weak convergence in $\mathcal{l}_p$ and coordinatewise convergence,Weak convergence in  and coordinatewise convergence,\mathcal{l}_p,"Let $x^n=(x^n_1, x^n_2,...)$ be  a bounded sequence in $\mathcal{l}_p$ for $1<p<\infty$ and such that $x^n_i$ converges to $x_i$ for all $i\in\mathbb{N}$.  I'm trying to prove that $x=(x_1,x_2,...)$ belongs to $\mathcal{l}_p$ and $x^n$ converges weakly to $x$. Since $x_n$ is bounded in a reflexive space there exists a subsequence $x^{n_k}$ weakly convergent  to some $y\in \mathcal{l}_p$. Testing with $e^n=(e^n_i)_i$, $e^n_i=\delta_{n,i}$ i get that the subsequence converges indeed to $x$, so $x\in \mathcal{l}_p$. I'm stuck for the second part. Since the dual of $\mathcal{l}_p$ is $\mathcal{l}_q$ I'm trying to show that for evry $z\in\mathcal{l}_q$ , $z(x^n)-z(x)=\sum_iz_i(x^n_i-x_i)$ converges to zero. $$ |z(x^n)-z(x)|\leq \sum_i|z_i(x^n_i-x^{n_k}_i)|+\left|\sum_iz_i(x^{n_k}_i-x_i)\right|. $$ The second term goes to zero, but i don't know how to estimate the first sum.I don't know if this is the wrong approach, any help would be greatly appreciated.","Let $x^n=(x^n_1, x^n_2,...)$ be  a bounded sequence in $\mathcal{l}_p$ for $1<p<\infty$ and such that $x^n_i$ converges to $x_i$ for all $i\in\mathbb{N}$.  I'm trying to prove that $x=(x_1,x_2,...)$ belongs to $\mathcal{l}_p$ and $x^n$ converges weakly to $x$. Since $x_n$ is bounded in a reflexive space there exists a subsequence $x^{n_k}$ weakly convergent  to some $y\in \mathcal{l}_p$. Testing with $e^n=(e^n_i)_i$, $e^n_i=\delta_{n,i}$ i get that the subsequence converges indeed to $x$, so $x\in \mathcal{l}_p$. I'm stuck for the second part. Since the dual of $\mathcal{l}_p$ is $\mathcal{l}_q$ I'm trying to show that for evry $z\in\mathcal{l}_q$ , $z(x^n)-z(x)=\sum_iz_i(x^n_i-x_i)$ converges to zero. $$ |z(x^n)-z(x)|\leq \sum_i|z_i(x^n_i-x^{n_k}_i)|+\left|\sum_iz_i(x^{n_k}_i-x_i)\right|. $$ The second term goes to zero, but i don't know how to estimate the first sum.I don't know if this is the wrong approach, any help would be greatly appreciated.",,"['analysis', 'functional-analysis', 'lp-spaces', 'weak-convergence']"
97,Spectrum bilateral shift,Spectrum bilateral shift,,"Let $U \in \mathbb{B}(\ell^2(\mathbb{Z}))$ be the bilateral shift. I want to show that $\sigma(U)=\mathbb{T}$. Using functional Calculus I have shown that $\sigma(U)\subseteq\mathbb{T}$. In order to prove $\mathbb{T} \subseteq \sigma(U)$ I have googled a bit and found this proof , but I do not understand the argument the author is making. By showing that the norm converges to zero, is he not showing that $\lambda$ is an eigenvalue, which it is not. Thanks","Let $U \in \mathbb{B}(\ell^2(\mathbb{Z}))$ be the bilateral shift. I want to show that $\sigma(U)=\mathbb{T}$. Using functional Calculus I have shown that $\sigma(U)\subseteq\mathbb{T}$. In order to prove $\mathbb{T} \subseteq \sigma(U)$ I have googled a bit and found this proof , but I do not understand the argument the author is making. By showing that the norm converges to zero, is he not showing that $\lambda$ is an eigenvalue, which it is not. Thanks",,"['functional-analysis', 'operator-algebras']"
98,"Show that if $X$ is a Hilbert space, then so is $Y$","Show that if  is a Hilbert space, then so is",X Y,"Given that $X$ and $Y$ are normed linear spaces and that $T:X\to Y$ is a linear map, such that $T(\alpha x_1 + \beta x_2) = \alpha T(x_1)+ \beta T(x_2)$ for all vectors $x_1,x_2\in X$ and scalars $\alpha , \beta$. Suppose $T$ maps $X$ onto $Y$ and is isometric, $\|Tx\|=\|x\|$ for all $x\in X$. 1) Show that if $X$ is a Hilbert space then so is $Y$ if we define: $$\langle y_1,y_2\rangle_Y=\langle x_1,x_2\rangle_X$$ Where $x_1,x_2$ are the unique points in $X$ satisfying $Tx_1=y_1$ and $Tx_2=y_2$ What I have done so far I have previously proved that if $X$ is a Banach space then so is $Y$. So I am thinking that since $X$ is a Hilbert space, $$||y_1-y_2|| = ||T(x_1)-T(x_2)|| = ||T(x_1-x_2)||=||x_1-x_2|| \implies$$ $$||y_1-y_2||^2 = ||y_1||^2 - 2Re\langle y_1,y_2 \rangle + ||y_2||^2$$ $$||x_1-x_2||^2 = ||x_1||^2 - 2Re\langle x_1,x_2 \rangle + ||x_2||^2$$ and $$||y_1+y_2|| = ||T(x_1)+T(x_2)|| = ||T(x_1+x_2)||=||x_1+x_2|| \implies$$ $$||y_1+y_2||^2 = ||y_1||^2 + 2Re\langle y_1,y_2 \rangle + ||y_2||^2$$ $$||x_1+x_2||^2 = ||x_1||^2 + 2Re\langle x_1,x_2 \rangle + ||x_2||^2$$ Therefore $$||y_1+y_2||^2+||y_1-y_2||^2=2||y_1||^2+2||y_2||^2$$ Therefore the parallelogram equality holds for $Y$, and therefore there is an inner product that gives the norm, and so $||y_1 - y_2||=\sqrt{\langle y_1-y_2,y_1-y_2 \rangle}$ and therefore $Y$ is a Hilbert space.","Given that $X$ and $Y$ are normed linear spaces and that $T:X\to Y$ is a linear map, such that $T(\alpha x_1 + \beta x_2) = \alpha T(x_1)+ \beta T(x_2)$ for all vectors $x_1,x_2\in X$ and scalars $\alpha , \beta$. Suppose $T$ maps $X$ onto $Y$ and is isometric, $\|Tx\|=\|x\|$ for all $x\in X$. 1) Show that if $X$ is a Hilbert space then so is $Y$ if we define: $$\langle y_1,y_2\rangle_Y=\langle x_1,x_2\rangle_X$$ Where $x_1,x_2$ are the unique points in $X$ satisfying $Tx_1=y_1$ and $Tx_2=y_2$ What I have done so far I have previously proved that if $X$ is a Banach space then so is $Y$. So I am thinking that since $X$ is a Hilbert space, $$||y_1-y_2|| = ||T(x_1)-T(x_2)|| = ||T(x_1-x_2)||=||x_1-x_2|| \implies$$ $$||y_1-y_2||^2 = ||y_1||^2 - 2Re\langle y_1,y_2 \rangle + ||y_2||^2$$ $$||x_1-x_2||^2 = ||x_1||^2 - 2Re\langle x_1,x_2 \rangle + ||x_2||^2$$ and $$||y_1+y_2|| = ||T(x_1)+T(x_2)|| = ||T(x_1+x_2)||=||x_1+x_2|| \implies$$ $$||y_1+y_2||^2 = ||y_1||^2 + 2Re\langle y_1,y_2 \rangle + ||y_2||^2$$ $$||x_1+x_2||^2 = ||x_1||^2 + 2Re\langle x_1,x_2 \rangle + ||x_2||^2$$ Therefore $$||y_1+y_2||^2+||y_1-y_2||^2=2||y_1||^2+2||y_2||^2$$ Therefore the parallelogram equality holds for $Y$, and therefore there is an inner product that gives the norm, and so $||y_1 - y_2||=\sqrt{\langle y_1-y_2,y_1-y_2 \rangle}$ and therefore $Y$ is a Hilbert space.",,['functional-analysis']
99,"$\sin(nx)$ does not contain Cauchy subsequence in $L^p([0,2\pi]) $ for $1\leq p < \infty$",does not contain Cauchy subsequence in  for,"\sin(nx) L^p([0,2\pi])  1\leq p < \infty","$\sin(nx)$ does not contain Cauchy subsequence in $L^p([0,2\pi]) $ for $1\leq p < \infty$ My attempt: Set $f_n(x) = \sin(nx)$. Argue by contradiction, suppose there exists a Cauchy subsequence $f_{n_k}$ in $L^p$ for some fixed $p$, then $f_{n_k}$ converges strongly to some $f \in L^p$, this $f$ must be the zero function since we know that $f_{n_k}$ converges weakly to zero in $L^p$. But $f\equiv 0$ is impossible since $||f_{n_k}||_p$ is bounded below by a positive number. To see this, we look at the preimage  $$\{{|f_{n_k}|}^p \geq \big( \frac{\sqrt 2}{2}\big)^p \} = \{{|f_{n_k}|}\geq \frac{\sqrt 2}{2} \} $$ the measure of this set is bounded below by $\pi$. By Chebyshev's Inequality ${||f_{n_k}||_p}^p$ is bounded below by $\pi\big( \frac{\sqrt 2}{2}\big)^p $. Is this okay? thank you very much!","$\sin(nx)$ does not contain Cauchy subsequence in $L^p([0,2\pi]) $ for $1\leq p < \infty$ My attempt: Set $f_n(x) = \sin(nx)$. Argue by contradiction, suppose there exists a Cauchy subsequence $f_{n_k}$ in $L^p$ for some fixed $p$, then $f_{n_k}$ converges strongly to some $f \in L^p$, this $f$ must be the zero function since we know that $f_{n_k}$ converges weakly to zero in $L^p$. But $f\equiv 0$ is impossible since $||f_{n_k}||_p$ is bounded below by a positive number. To see this, we look at the preimage  $$\{{|f_{n_k}|}^p \geq \big( \frac{\sqrt 2}{2}\big)^p \} = \{{|f_{n_k}|}\geq \frac{\sqrt 2}{2} \} $$ the measure of this set is bounded below by $\pi$. By Chebyshev's Inequality ${||f_{n_k}||_p}^p$ is bounded below by $\pi\big( \frac{\sqrt 2}{2}\big)^p $. Is this okay? thank you very much!",,"['real-analysis', 'functional-analysis', 'proof-verification', 'lp-spaces']"
