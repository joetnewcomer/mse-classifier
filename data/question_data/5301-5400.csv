,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Prove that a function is a constant a.e. [duplicate],Prove that a function is a constant a.e. [duplicate],,"This question already has an answer here : Limit of Integral of Difference Quotients of Measurable/Bounded $f$ Being $0$ Implies $f$ is Constant (1 answer) Closed 2 years ago . Let $f(x) \in L^1 [a, b]$ satisfies $\lim_{h \to 0} 1/h \int_{a}^b |f(x+h)-f(x)|dx = 0$ . I want to prove that there exists $c$ such that $f(x) = c$ a.e. Morally what happens is that $\int_{a}^b |f'(x)|dx = 0$ which gives the desired result but $f(x)$ is not necessarily differentiable. How can I show this rigorously?",This question already has an answer here : Limit of Integral of Difference Quotients of Measurable/Bounded $f$ Being $0$ Implies $f$ is Constant (1 answer) Closed 2 years ago . Let satisfies . I want to prove that there exists such that a.e. Morally what happens is that which gives the desired result but is not necessarily differentiable. How can I show this rigorously?,"f(x) \in L^1 [a, b] \lim_{h \to 0} 1/h \int_{a}^b |f(x+h)-f(x)|dx = 0 c f(x) = c \int_{a}^b |f'(x)|dx = 0 f(x)","['real-analysis', 'analysis', 'derivatives', 'lebesgue-integral']"
1,"Does $r(x) >0$ almost everywhere imply $r(x) > 1/k > 0$ almost everywhere on an open set, some $k \in \mathbb{N}$?","Does  almost everywhere imply  almost everywhere on an open set, some ?",r(x) >0 r(x) > 1/k > 0 k \in \mathbb{N},"Let $(a,b)\subseteq \mathbb{R}$ be an open interval, and suppose $r :(a,b) \to \mathbb{R}$ is Lebesgue measurable and positive almost everywhere on $(a,b)$ . Do there exist $k \in \mathbb{N}$ and $a < c < d < b$ so that $r(x) > 1/k$ for almost all $x \in (c,d)?$ Since $r$ is positive almost everywhere, by continuity of Lebesgue measure from below, we are guaranteed that $A_k = \{x \in (a,b) : r(x) > 1/k \}$ has positive Lebesgue measure for some $k$ , but it's unclear to me whether there must be an open interval $(c,d)$ such that $(c,d) \setminus A_k$ has measure zero. Hints or solutions are appreciated.","Let be an open interval, and suppose is Lebesgue measurable and positive almost everywhere on . Do there exist and so that for almost all Since is positive almost everywhere, by continuity of Lebesgue measure from below, we are guaranteed that has positive Lebesgue measure for some , but it's unclear to me whether there must be an open interval such that has measure zero. Hints or solutions are appreciated.","(a,b)\subseteq \mathbb{R} r :(a,b) \to \mathbb{R} (a,b) k \in \mathbb{N} a < c < d < b r(x) > 1/k x \in (c,d)? r A_k = \{x \in (a,b) : r(x) > 1/k \} k (c,d) (c,d) \setminus A_k","['real-analysis', 'measure-theory', 'lebesgue-measure']"
2,If $B\subset\mathbb{R}$ is a Borel set and $f:B\to\mathbb{R}$ is an increasing function then $f(B)$ is a Borel set,If  is a Borel set and  is an increasing function then  is a Borel set,B\subset\mathbb{R} f:B\to\mathbb{R} f(B),"I am trying to prove the following statement: ""If $B\subset\mathbb{R}$ is a Borel set and $f:B\to\mathbb{R}$ is an increasing function then $f(B)$ is a Borel set"" but I have only managed to prove the easier statement ""If $B\subset\mathbb{R}$ is a Borel set and $f:\mathbb{R}\to\mathbb{R}$ is a strictly increasing function then $f(B)$ is a Borel set"" My proof (of the easier statement): By Inverse function $f^{-1}:f(\mathbb{R})\to\mathbb{R}$ of a strictly increasing function $f:\mathbb{R}\to\mathbb{R}$ is continuous we have that $f^{-1}:f(\mathbb{R})\to\mathbb{R}$ is continuous so $(f^{-1})^{-1}(\mathbb{R})=f(\mathbb{R})$ is open hence Borel thus $f^{-1}$ is a continuous function defined on a Borel set so it is Borel measurable which implies that $(f^{-1})^{-1}(B)=f(B)$ is a Borel set, as desired. $\square$ I would like to prove the initial statement but I have been stuck for a while so I would appreciate an hint about how to tackle its proof, thanks.","I am trying to prove the following statement: ""If is a Borel set and is an increasing function then is a Borel set"" but I have only managed to prove the easier statement ""If is a Borel set and is a strictly increasing function then is a Borel set"" My proof (of the easier statement): By Inverse function of a strictly increasing function is continuous we have that is continuous so is open hence Borel thus is a continuous function defined on a Borel set so it is Borel measurable which implies that is a Borel set, as desired. I would like to prove the initial statement but I have been stuck for a while so I would appreciate an hint about how to tackle its proof, thanks.",B\subset\mathbb{R} f:B\to\mathbb{R} f(B) B\subset\mathbb{R} f:\mathbb{R}\to\mathbb{R} f(B) f^{-1}:f(\mathbb{R})\to\mathbb{R} f:\mathbb{R}\to\mathbb{R} f^{-1}:f(\mathbb{R})\to\mathbb{R} (f^{-1})^{-1}(\mathbb{R})=f(\mathbb{R}) f^{-1} (f^{-1})^{-1}(B)=f(B) \square,"['real-analysis', 'measure-theory', 'monotone-functions', 'borel-sets']"
3,Can every increasing negative function be expressed as a product with these properties?,Can every increasing negative function be expressed as a product with these properties?,,"Let $h:(0,1] \to (-\infty,0)$ be a $C^1$ function, with $h'>0$ . I am looking for sufficient conditions on $h$ that imply the existence of $C^1$ functions $\lambda:(0,1] \to (-\infty,0)$ , $g:(0,1] \to (0,1]$ such that $$ h=\lambda g,  \,\,\,\,\,\lambda'>0, \,\,\,\,\,g'<0 \tag{1} $$ Note that $h'=\lambda' g+\lambda g'>0$ , since both summands are positive. Thus, $h'>0$ is a necessary condition for $h$ to be expressible as in $(1)$ . Is it sufficient? Can every $h$ with $h'>0$ be expressed in this way? The motivation for this convoluted question comes from trying to analyses when the solution to a certain minimization  problem is convex. (a bit too long to describe here). An equivalent reformulation: (thanks to Alex Jones). Take any $g: (0,1] \to (0,1]$ with $g' < 0$ . Then, we must have $\lambda = h/g < 0$ . Since $g > 0$ , the condition $\lambda' > 0$ is equivalent to $h'/h < g'/g$ . So, the question boils down to this: Does there exist a function $g:(0,1] \to (0,1]$ , with $g' < 0$ , such that $h'/h < g'/g$ ? Since the quantity $g'/g$ is invariant under positive scaling of $g$ ( $g \to c g$ for $c >0$ ), it suffices to search for bounded $g$ . Note that if $h$ is bounded, then by taking $g=-h$ we get $h'/h = g'/g$ instead of $h'/h < g'/g$ . So we are ""nearly there"".","Let be a function, with . I am looking for sufficient conditions on that imply the existence of functions , such that Note that , since both summands are positive. Thus, is a necessary condition for to be expressible as in . Is it sufficient? Can every with be expressed in this way? The motivation for this convoluted question comes from trying to analyses when the solution to a certain minimization  problem is convex. (a bit too long to describe here). An equivalent reformulation: (thanks to Alex Jones). Take any with . Then, we must have . Since , the condition is equivalent to . So, the question boils down to this: Does there exist a function , with , such that ? Since the quantity is invariant under positive scaling of ( for ), it suffices to search for bounded . Note that if is bounded, then by taking we get instead of . So we are ""nearly there"".","h:(0,1] \to (-\infty,0) C^1 h'>0 h C^1 \lambda:(0,1] \to (-\infty,0) g:(0,1] \to (0,1] 
h=\lambda g,  \,\,\,\,\,\lambda'>0, \,\,\,\,\,g'<0 \tag{1}
 h'=\lambda' g+\lambda g'>0 h'>0 h (1) h h'>0 g: (0,1] \to (0,1] g' < 0 \lambda = h/g < 0 g > 0 \lambda' > 0 h'/h < g'/g g:(0,1] \to (0,1] g' < 0 h'/h < g'/g g'/g g g \to c g c >0 g h g=-h h'/h = g'/g h'/h < g'/g","['real-analysis', 'calculus', 'examples-counterexamples']"
4,Example of function satisfying the growth condition: $\phi\big(\theta \frac{s}{t}\big) \leq \frac{\phi(s)}{\phi(t)}$,Example of function satisfying the growth condition:,\phi\big(\theta \frac{s}{t}\big) \leq \frac{\phi(s)}{\phi(t)},"I am barely looking for example(s) of invertible convex functions $\phi: [0,\infty)\to [0, \infty)$ such that $\phi(0)=0$ and there exists $\theta>0$ and for all $s\leq t$ we have \begin{align}\label{EqI}\tag{I} \phi\big(\theta \frac{s}{t}\big)	\leq \frac{\phi(s)}{\phi(t)} \qquad\text{or equaly}  \qquad  \theta  \leq \phi^{-1}\big(\frac{s}{t}\big)\frac{\phi^{-1}(t)}{\phi^{-1}(s)} \end{align} The most simple class consists of polynomial functions of the form $\phi(t)= ct^p$ with $c>0$ and $p>0$ . Question: Are there other possible non-polynomial examples satisfying $\eqref{EqI}$ ? I have tried without success with $\phi(t)= e^{t^\alpha}-1$ , $\alpha>0$ .","I am barely looking for example(s) of invertible convex functions such that and there exists and for all we have The most simple class consists of polynomial functions of the form with and . Question: Are there other possible non-polynomial examples satisfying ? I have tried without success with , .","\phi: [0,\infty)\to [0, \infty) \phi(0)=0 \theta>0 s\leq t \begin{align}\label{EqI}\tag{I}
\phi\big(\theta \frac{s}{t}\big)	\leq \frac{\phi(s)}{\phi(t)} \qquad\text{or equaly}  \qquad  \theta  \leq \phi^{-1}\big(\frac{s}{t}\big)\frac{\phi^{-1}(t)}{\phi^{-1}(s)}
\end{align} \phi(t)= ct^p c>0 p>0 \eqref{EqI} \phi(t)= e^{t^\alpha}-1 \alpha>0","['real-analysis', 'calculus', 'functions', 'convex-analysis', 'examples-counterexamples']"
5,Non-linear bijections preserving the relation $fg = f^2$,Non-linear bijections preserving the relation,fg = f^2,"Let $X$ and $Y$ be compact spaces and let $C(X)$ and $C(Y)$ be the corresponding spaces of continuous, real-valued functions. Suppose $T\colon C(X)\to C(Y)$ is a non-linear bijection such that $$fg = f^2 \iff (Tf)(Tg) = (Tf)^2\qquad (f,g\in C(X))$$ Once can deduce (see Lemma 3.1 here ) that (#) if $fg = 0$ , then $(Tf)(Tg) = 0$ and in this case $T(f+g) = Tf + Tg$ . Addendum : One can prove that the condition concerning preservation of the relation $fg = f^2$ in both ways is equivalent to $T$ and $T^{-1}$ satisfying (#), which may make the picture cleaner. Suppose that $(f-h)(g-h) = 0$ constantly. Do we have $(Tf-Th)(Tg-Th) = 0$ ? The motivation comes from the fact that I'd like to simplify/amend considerations in the linked preprint. Maps $T$ with this property need not be continuous, but I am happy to assume continuity, or something else, if that helps.","Let and be compact spaces and let and be the corresponding spaces of continuous, real-valued functions. Suppose is a non-linear bijection such that Once can deduce (see Lemma 3.1 here ) that (#) if , then and in this case . Addendum : One can prove that the condition concerning preservation of the relation in both ways is equivalent to and satisfying (#), which may make the picture cleaner. Suppose that constantly. Do we have ? The motivation comes from the fact that I'd like to simplify/amend considerations in the linked preprint. Maps with this property need not be continuous, but I am happy to assume continuity, or something else, if that helps.","X Y C(X) C(Y) T\colon C(X)\to C(Y) fg = f^2 \iff (Tf)(Tg) = (Tf)^2\qquad (f,g\in C(X)) fg = 0 (Tf)(Tg) = 0 T(f+g) = Tf + Tg fg = f^2 T T^{-1} (f-h)(g-h) = 0 (Tf-Th)(Tg-Th) = 0 T","['real-analysis', 'general-topology', 'continuity', 'functional-equations']"
6,If $X_n \to X $ in probability then $f(X_n) \to f(X)$ in probability for a Borel function $f$,If  in probability then  in probability for a Borel function,X_n \to X  f(X_n) \to f(X) f,"$(X_n)_n$ is a sequence of identically distributed random variables, $f:\mathbb{R} \to \mathbb{R}$ a Borel function . Prove that if $X_n$ converges in probability to $X,$ then $f(X_n)$ converges in probability to $f(X).$","is a sequence of identically distributed random variables, a Borel function . Prove that if converges in probability to then converges in probability to","(X_n)_n f:\mathbb{R} \to \mathbb{R} X_n X, f(X_n) f(X).","['real-analysis', 'probability-theory', 'measure-theory', 'probability-limit-theorems']"
7,Can you expand induction proofs to the real numbers?,Can you expand induction proofs to the real numbers?,,"Everyone knows the principle of induction, where you first prove a base case for some $n_0\in \mathbb{N}$ , and than show that by assuming the case for an $n \in \mathbb{N}$ the $n+1$ case follows. This is true because of the application of some sort of domino principle. However, you could also try to combine this principle with an epsilon proof. For any statement $S_x$ , show that $S_{x_{0}}$ holds and than show that from $S_x$ , $S_{x+\epsilon}$ follows, for $|\epsilon |>0$ , where $\epsilon,x_0,x \in \mathbb{R}$ . If I am not mistaken this should at least work for proving statements for all rational numbers, please correct me if I am wrong(I am still new to this). But does this also work for proofs of real numbers and if not, is there some sort of ""induction for the reals""? I hope that I could make my question clear enough. I am quite sorry for any sort of mistakes and hope nonetheless, that you could help/correct me.","Everyone knows the principle of induction, where you first prove a base case for some , and than show that by assuming the case for an the case follows. This is true because of the application of some sort of domino principle. However, you could also try to combine this principle with an epsilon proof. For any statement , show that holds and than show that from , follows, for , where . If I am not mistaken this should at least work for proving statements for all rational numbers, please correct me if I am wrong(I am still new to this). But does this also work for proofs of real numbers and if not, is there some sort of ""induction for the reals""? I hope that I could make my question clear enough. I am quite sorry for any sort of mistakes and hope nonetheless, that you could help/correct me.","n_0\in \mathbb{N} n \in \mathbb{N} n+1 S_x S_{x_{0}} S_x S_{x+\epsilon} |\epsilon |>0 \epsilon,x_0,x \in \mathbb{R}","['real-analysis', 'induction', 'real-numbers', 'rational-numbers']"
8,A complex Banach space satisfying the parallelogram law is Hilbert,A complex Banach space satisfying the parallelogram law is Hilbert,,"Let $H$ be a Banach space with associated norm $\|-\|.$ Suppose that for any $x,y\in H,$ we have: $$\|x+y\|^2+\|x-y\|^2=2\left(\|x\|^2+\|y\|^2\right),$$ which we call the parallelogram law. Then it is a well-known standard fact that $H$ becomes a Hilbert space. This is true both for real and complex coefficients. I managed to prove this fact for an Hilbert space over $\mathbb{R},$ defining the inner product $$(x,y) \mapsto \langle x,y\rangle=\frac{\|x+y\|^2-\|x-y\|^2}{4}= \frac{\|x+y\|^2-\|x\|^2-\|y\|^2}{2}.$$ Question How to prove this for the complex case? The inner product should be in this case $$(x,y)\mapsto\ \alpha(x,y)= \frac{1}{4} \left(\|x + y\|^2 - \|x-y\|^2 + i\|x + iy\|^2 -i\|x-iy\|^2 \right)=  \langle x,y \rangle + i \langle x,iy \rangle$$ but I am not able to replicate the proof of the real case. It would be awesome if there was a slick proof that used the real case to deduce the complex case, but I would still be happy with any kind of direct proof too. Since this is a standard result, if you can provide a reference where a detailed proof is given, that would be excellent too.","Let be a Banach space with associated norm Suppose that for any we have: which we call the parallelogram law. Then it is a well-known standard fact that becomes a Hilbert space. This is true both for real and complex coefficients. I managed to prove this fact for an Hilbert space over defining the inner product Question How to prove this for the complex case? The inner product should be in this case but I am not able to replicate the proof of the real case. It would be awesome if there was a slick proof that used the real case to deduce the complex case, but I would still be happy with any kind of direct proof too. Since this is a standard result, if you can provide a reference where a detailed proof is given, that would be excellent too.","H \|-\|. x,y\in H, \|x+y\|^2+\|x-y\|^2=2\left(\|x\|^2+\|y\|^2\right), H \mathbb{R}, (x,y) \mapsto \langle x,y\rangle=\frac{\|x+y\|^2-\|x-y\|^2}{4}= \frac{\|x+y\|^2-\|x\|^2-\|y\|^2}{2}. (x,y)\mapsto\ \alpha(x,y)= \frac{1}{4} \left(\|x + y\|^2 - \|x-y\|^2 + i\|x + iy\|^2 -i\|x-iy\|^2 \right)=  \langle x,y \rangle + i \langle x,iy \rangle","['real-analysis', 'functional-analysis']"
9,Derivation of an integral result,Derivation of an integral result,,"In this thread it is mentioned that: $$\int_0^\infty \left ( \mathrm{arccot} x \right )^3\; \mathrm{d}x = \frac{3 \pi^2 \ln 2}{4} - \frac{21\zeta(3)}{8}$$ where $\zeta$ is the Riemann zeta function . The steps to the solution I took are: \begin{align*} \int_{0}^{\infty} \left ( \mathrm{arccot}x \right )^3 \, \mathrm{d}x &= \int_{0}^{\infty} \left ( x \right ) ' \left ( \mathrm{arccot}x \right )^3 \, \mathrm{d}x  \\   &=\left [ x \left ( \mathrm{arccot}x \right )^3 \right ]_0^{\infty} +3 \int_{0}^{\infty} \frac{x\left( \mathrm{arccot }x \right )^2}{ x^2+1} \, \mathrm{d}x \\   &=3  \left [ \frac{\ln \left ( 1+x^2 \right ) \, \left (\mathrm{arccot}(x)  \right )^2}{2} \right ]_0^{\infty} + 3 \int_{0}^{\infty} \frac{\ln \left ( 1+x^2 \right ) \mathrm{arccot} x}{x^2+1} \, \mathrm{d}x  \\   &=3 \left ( \int_{0}^{1} \frac{\ln \left ( 1+x^2 \right ) \mathrm{arccot} x}{x^2+1} \, \mathrm{d}x + \int_1^{\infty} \frac{\ln \left ( 1+x^2 \right ) \mathrm{arccot} x}{x^2+1} \, \mathrm{d}x  \right ) \\  &=3 \left ( \int_{0}^{1} \frac{\ln \left ( 1+x^2 \right ) \mathrm{arccot} x}{x^2+1} \, \mathrm{d}x + \int_{0}^{1} \frac{\ln \left ( 1+\frac{1}{x^2} \right ) \arctan x}{1+\frac{1}{x^2}} \cdot \frac{1}{x^2} \, \mathrm{d}x \right ) \\  &=3 \left ( \int_{0}^{1} \frac{\ln \left ( 1+x^2 \right ) \mathrm{arccot} x}{x^2+1} \, \mathrm{d}x  + \int_{0}^{1} \frac{\left (\ln \left ( 1+x^2 \right ) - 2 \ln x  \right ) \arctan x}{1+x^2} \, \mathrm{d}x \right )\\ &=3\left ( \int_{0}^{1} \frac{\ln \left ( 1+x^2 \right )\left ( \mathrm{arccot} x + \arctan x \right )}{x^2+1} \, \mathrm{d}x - 2 \int_{0}^{1} \frac{\ln x \arctan x}{1+x^2} \, \mathrm{d}x \right ) \\ &=3\left ( \frac{\pi}{2} \int_{0}^{1} \frac{\ln \left ( 1+x^2 \right )}{1+x^2} \, \mathrm{d}x - 2 \int_{0}^{1} \frac{\ln x \arctan x}{1+x^2} \, \mathrm{d}x \right )  \end{align*} Let us now calculate the the first integral: \begin{align*} \int_{0}^{1} \frac{\ln \left ( 1+x^2 \right )}{1+x^2} \, \mathrm{d}x &\overset{x=\tan \theta}{=\! =\! =\! =\! =\!} \int_{0}^{\pi/4} \frac{\ln \left ( 1+\tan^2 \theta \right )}{1+\tan^2 \theta} \cdot \sec^2 \theta \, \mathrm{d}\theta\\   &=\int_{0}^{\pi/4} \ln \left ( 1+ \tan^2 \theta \right ) \, \mathrm{d}\theta \\   &= \int_{0}^{\pi/4} \ln \sec^2 \theta \, \mathrm{d} \theta \\  &=2 \int_{0}^{\pi/4} \ln \sec \theta \, \mathrm{d} \theta \\  &=-2 \int_{0}^{\pi/4} \ln \cos \theta \, \mathrm{d} \theta \\  &=-2 \left ( -\int_{0}^{\pi/4} \left (\sum_{n=1}^{\infty} (-1)^n \frac{\cos 2n \theta}{n} - \ln 2  \right ) \, \mathrm{d}\theta \right )\\  &=2 \sum_{n=1}^{\infty}  \frac{(-1)^n}{n} \int_{0}^{\pi/4} \cos 2n\theta \, \mathrm{d} \theta +2 \int_{0}^{\pi/4} \ln 2 \, \mathrm{d}\theta \\  &= 2 \sum_{n=1}^{\infty} \frac{(-1)^n \sin \frac{n \pi}{2}}{2n^2} +\frac{\pi \ln 2}{2} \\  &= \sum_{n=1}^{\infty} \frac{(-1)^n \sin \frac{n \pi}{2}}{2n^2} + \frac{\pi \ln 2}{2} \\  &=\frac{\pi \ln 2}{2} + \sum_{n=0}^{\infty} \frac{(-1)^n}{(2n+1)^2}\\  &= \frac{\pi \ln 2}{2} - \mathcal{G} \end{align*} where $\mathcal{G}$ is the Catalan's constant . Let's move on to the second integral: \begin{align*} \int_{0}^{1} \frac{\ln x \arctan x}{1+x^2} \, \mathrm{d}x &=\int_{0}^{1} \ln x \sum_{n=1}^{\infty} (-1)^{n-1} \left ( \mathcal{H}_{2n} - \frac{\mathcal{H}_n}{2} \right ) x^{2n-1} \, \mathrm{d}x \\   &=\sum_{n=1}^{\infty} (-1)^{n-1} \left ( \mathcal{H}_{2n} - \frac{\mathcal{H}_n}{2} \right ) \int_{0}^{1}x^{2n-1} \ln x \, \mathrm{d}x \\   &=-\frac{1}{4}\sum_{n=1}^{\infty} (-1)^{n-1} \frac{\mathcal{H}_{2n}- \frac{\mathcal{H}_n}{2}}{n^2}  \\   &= \frac{1}{4} \sum_{n=1}^{\infty} (-1)^n \frac{\mathcal{H}_{2n} - \frac{\mathcal{H}_n}{2}}{n^2} \\  &=\frac{1}{4} \sum_{n=1}^{\infty} (-1)^n \frac{\mathcal{H}_{2n}}{n^2}  -\frac{1}{8} \sum_{n=1}^{\infty} (-1)^{n} \frac{\mathcal{H}_n}{n^2} \end{align*} The second sum is an old chestnut evaluating to $$\sum_{n=1}^{\infty} (-1)^n \frac{\mathcal{H}_n}{n^2} = -\frac{5 \zeta(3)}{8}$$ see for example this link .The first sum should bring the $\mathcal{G}$ in . The question is how?",In this thread it is mentioned that: where is the Riemann zeta function . The steps to the solution I took are: Let us now calculate the the first integral: where is the Catalan's constant . Let's move on to the second integral: The second sum is an old chestnut evaluating to see for example this link .The first sum should bring the in . The question is how?,"\int_0^\infty \left ( \mathrm{arccot} x \right )^3\; \mathrm{d}x = \frac{3 \pi^2 \ln 2}{4} - \frac{21\zeta(3)}{8} \zeta \begin{align*}
\int_{0}^{\infty} \left ( \mathrm{arccot}x \right )^3 \, \mathrm{d}x &= \int_{0}^{\infty} \left ( x \right ) ' \left ( \mathrm{arccot}x \right )^3 \, \mathrm{d}x  \\ 
 &=\left [ x \left ( \mathrm{arccot}x \right )^3 \right ]_0^{\infty} +3 \int_{0}^{\infty} \frac{x\left( \mathrm{arccot }x \right )^2}{ x^2+1} \, \mathrm{d}x \\ 
 &=3  \left [ \frac{\ln \left ( 1+x^2 \right ) \, \left (\mathrm{arccot}(x)  \right )^2}{2} \right ]_0^{\infty} + 3 \int_{0}^{\infty} \frac{\ln \left ( 1+x^2 \right ) \mathrm{arccot} x}{x^2+1} \, \mathrm{d}x  \\ 
 &=3 \left ( \int_{0}^{1} \frac{\ln \left ( 1+x^2 \right ) \mathrm{arccot} x}{x^2+1} \, \mathrm{d}x + \int_1^{\infty} \frac{\ln \left ( 1+x^2 \right ) \mathrm{arccot} x}{x^2+1} \, \mathrm{d}x  \right ) \\
 &=3 \left ( \int_{0}^{1} \frac{\ln \left ( 1+x^2 \right ) \mathrm{arccot} x}{x^2+1} \, \mathrm{d}x + \int_{0}^{1} \frac{\ln \left ( 1+\frac{1}{x^2} \right ) \arctan x}{1+\frac{1}{x^2}} \cdot \frac{1}{x^2} \, \mathrm{d}x \right ) \\
 &=3 \left ( \int_{0}^{1} \frac{\ln \left ( 1+x^2 \right ) \mathrm{arccot} x}{x^2+1} \, \mathrm{d}x  + \int_{0}^{1} \frac{\left (\ln \left ( 1+x^2 \right ) - 2 \ln x  \right ) \arctan x}{1+x^2} \, \mathrm{d}x \right )\\
&=3\left ( \int_{0}^{1} \frac{\ln \left ( 1+x^2 \right )\left ( \mathrm{arccot} x + \arctan x \right )}{x^2+1} \, \mathrm{d}x - 2 \int_{0}^{1} \frac{\ln x \arctan x}{1+x^2} \, \mathrm{d}x \right ) \\
&=3\left ( \frac{\pi}{2} \int_{0}^{1} \frac{\ln \left ( 1+x^2 \right )}{1+x^2} \, \mathrm{d}x - 2 \int_{0}^{1} \frac{\ln x \arctan x}{1+x^2} \, \mathrm{d}x \right ) 
\end{align*} \begin{align*}
\int_{0}^{1} \frac{\ln \left ( 1+x^2 \right )}{1+x^2} \, \mathrm{d}x &\overset{x=\tan \theta}{=\! =\! =\! =\! =\!} \int_{0}^{\pi/4} \frac{\ln \left ( 1+\tan^2 \theta \right )}{1+\tan^2 \theta} \cdot \sec^2 \theta \, \mathrm{d}\theta\\ 
 &=\int_{0}^{\pi/4} \ln \left ( 1+ \tan^2 \theta \right ) \, \mathrm{d}\theta \\ 
 &= \int_{0}^{\pi/4} \ln \sec^2 \theta \, \mathrm{d} \theta \\
 &=2 \int_{0}^{\pi/4} \ln \sec \theta \, \mathrm{d} \theta \\
 &=-2 \int_{0}^{\pi/4} \ln \cos \theta \, \mathrm{d} \theta \\
 &=-2 \left ( -\int_{0}^{\pi/4} \left (\sum_{n=1}^{\infty} (-1)^n \frac{\cos 2n \theta}{n} - \ln 2  \right ) \, \mathrm{d}\theta \right )\\
 &=2 \sum_{n=1}^{\infty}  \frac{(-1)^n}{n} \int_{0}^{\pi/4} \cos 2n\theta \, \mathrm{d} \theta +2 \int_{0}^{\pi/4} \ln 2 \, \mathrm{d}\theta \\
 &= 2 \sum_{n=1}^{\infty} \frac{(-1)^n \sin \frac{n \pi}{2}}{2n^2} +\frac{\pi \ln 2}{2} \\
 &= \sum_{n=1}^{\infty} \frac{(-1)^n \sin \frac{n \pi}{2}}{2n^2} + \frac{\pi \ln 2}{2} \\
 &=\frac{\pi \ln 2}{2} + \sum_{n=0}^{\infty} \frac{(-1)^n}{(2n+1)^2}\\
 &= \frac{\pi \ln 2}{2} - \mathcal{G}
\end{align*} \mathcal{G} \begin{align*}
\int_{0}^{1} \frac{\ln x \arctan x}{1+x^2} \, \mathrm{d}x &=\int_{0}^{1} \ln x \sum_{n=1}^{\infty} (-1)^{n-1} \left ( \mathcal{H}_{2n} - \frac{\mathcal{H}_n}{2} \right ) x^{2n-1} \, \mathrm{d}x \\ 
 &=\sum_{n=1}^{\infty} (-1)^{n-1} \left ( \mathcal{H}_{2n} - \frac{\mathcal{H}_n}{2} \right ) \int_{0}^{1}x^{2n-1} \ln x \, \mathrm{d}x \\ 
 &=-\frac{1}{4}\sum_{n=1}^{\infty} (-1)^{n-1} \frac{\mathcal{H}_{2n}- \frac{\mathcal{H}_n}{2}}{n^2}  \\ 
 &= \frac{1}{4} \sum_{n=1}^{\infty} (-1)^n \frac{\mathcal{H}_{2n} - \frac{\mathcal{H}_n}{2}}{n^2} \\
 &=\frac{1}{4} \sum_{n=1}^{\infty} (-1)^n \frac{\mathcal{H}_{2n}}{n^2}  -\frac{1}{8} \sum_{n=1}^{\infty} (-1)^{n} \frac{\mathcal{H}_n}{n^2}
\end{align*} \sum_{n=1}^{\infty} (-1)^n \frac{\mathcal{H}_n}{n^2} = -\frac{5 \zeta(3)}{8} \mathcal{G}","['real-analysis', 'integration', 'sequences-and-series', 'definite-integrals']"
10,Proof of an integral inequality of polynomials [duplicate],Proof of an integral inequality of polynomials [duplicate],,"This question already has answers here : Show that $\int_{0}^{1}\{P(x)\}^{2}\,dx = (n + 1)^{2}\left(\int_{0}^{1}P(x)\,dx\right)^{2}$ (3 answers) Closed 5 years ago . Let $f$ be a polynomial with degree $n$ , also $$\int_{0}^1 f(x)x^kdx=0,k=1,2, \cdots ,n$$ Prove that $$\int_0^1 f^2(x)dx=(n+1)^2\left(\int_0^1 f(x) dx\right)^2$$ My first step is to subtract zero from the left of the equation. $$\begin{aligned} \int_0^1 f^2(x)dx&=\int_0^1 f^2(x)dx-\sum_{k=1}^n\int_0^1 a_k x^k f(x)dx\\ &=\int_0^1 f(x)\left(f(x)-\sum_{k=1}^n a_k x^k\right)dx\\ &=a_0 \int_0^1f(x)dx \end{aligned}$$ Then $$a_0=(n+1)^2\left(\int_0^1 f(x) dx\right)\quad\text{or}\quad \int_0^1 f(x) dx=0$$ I doubt that this is not true because the integral of an arbitrary polynomial is a constant only related to $a_0$ . But this is possible. Could you show a better idea and give a proof? Thanks.","This question already has answers here : Show that $\int_{0}^{1}\{P(x)\}^{2}\,dx = (n + 1)^{2}\left(\int_{0}^{1}P(x)\,dx\right)^{2}$ (3 answers) Closed 5 years ago . Let be a polynomial with degree , also Prove that My first step is to subtract zero from the left of the equation. Then I doubt that this is not true because the integral of an arbitrary polynomial is a constant only related to . But this is possible. Could you show a better idea and give a proof? Thanks.","f n \int_{0}^1 f(x)x^kdx=0,k=1,2, \cdots ,n \int_0^1 f^2(x)dx=(n+1)^2\left(\int_0^1 f(x) dx\right)^2 \begin{aligned}
\int_0^1 f^2(x)dx&=\int_0^1 f^2(x)dx-\sum_{k=1}^n\int_0^1 a_k x^k f(x)dx\\
&=\int_0^1 f(x)\left(f(x)-\sum_{k=1}^n a_k x^k\right)dx\\
&=a_0 \int_0^1f(x)dx
\end{aligned} a_0=(n+1)^2\left(\int_0^1 f(x) dx\right)\quad\text{or}\quad \int_0^1 f(x) dx=0 a_0","['real-analysis', 'calculus']"
11,Is the composition of a Sobolev function and a smooth function Sobolev?,Is the composition of a Sobolev function and a smooth function Sobolev?,,"Let $\Omega \subseteq \mathbb{R}^n$ be an open bounded domain, and let $1<p<n$.  Suppose that $f \in W^{1,p}(\Omega)$ is continuous*, and $g \in C^{\infty}(\mathbb{R})$. Is it true that $g \circ f \in W^{1,p}_{loc}(\Omega)$? My guess was that the answer is positive, and that $\partial_i (g \circ f)(x)=g'(f(x)) \partial_i f(x)$ but a naive calculation to prove it failed. *Note that the continuity of $f$ does not follow from $f \in W^{1,p}(\Omega)$, since $p<n$; this is an additional assumption I am adding.","Let $\Omega \subseteq \mathbb{R}^n$ be an open bounded domain, and let $1<p<n$.  Suppose that $f \in W^{1,p}(\Omega)$ is continuous*, and $g \in C^{\infty}(\mathbb{R})$. Is it true that $g \circ f \in W^{1,p}_{loc}(\Omega)$? My guess was that the answer is positive, and that $\partial_i (g \circ f)(x)=g'(f(x)) \partial_i f(x)$ but a naive calculation to prove it failed. *Note that the continuity of $f$ does not follow from $f \in W^{1,p}(\Omega)$, since $p<n$; this is an additional assumption I am adding.",,"['real-analysis', 'sobolev-spaces', 'regularity-theory-of-pdes', 'weak-derivatives']"
12,When does the equality hold in the Holder inequality?,When does the equality hold in the Holder inequality?,,"I am considering the series case. In the Holder inequality, we have  $$\sum|x_iy_i|\leq\left(\sum|x_i|^p\right)^{\frac1p} \left(\sum|y_i|^q\right)^{\frac1q},$$ where $\frac1p+\frac1q=1,~p, q>1$. In Cauchy inequality (i.e., $p=q=2$), I know that the equality holds if and only if $x$ and $y$ are linearly dependent. I am wondering when the equality holds in the Holder inequality.","I am considering the series case. In the Holder inequality, we have  $$\sum|x_iy_i|\leq\left(\sum|x_i|^p\right)^{\frac1p} \left(\sum|y_i|^q\right)^{\frac1q},$$ where $\frac1p+\frac1q=1,~p, q>1$. In Cauchy inequality (i.e., $p=q=2$), I know that the equality holds if and only if $x$ and $y$ are linearly dependent. I am wondering when the equality holds in the Holder inequality.",,"['real-analysis', 'functional-analysis', 'inequality', 'holder-inequality']"
13,Compute $\limsup |\sin^n (n)|$,Compute,\limsup |\sin^n (n)|,"In the Scottish Book, there is a question posed by Stanislaw Hartman, which goes as follows: It is easy to see that $\liminf |\cos^n n| = \liminf |\sin^n n| = 0$. A bit harder is to prove $\limsup|\cos n|^n = 1$ and $\limsup|\sin n|^n = 1$. Find $\liminf(\cos n)^n$, $\liminf(\sin n)^n$, and $\limsup(\sin n)^n$. For the time being, I can see easily why the first two equalities hold. But how does one prove $\limsup|\cos n|^n = 1$ and $\limsup|\sin n|^n = 1$? My attempt for $\sin$: Try to find $|\sin n|$ arbitrarily close to $1$. Let $d$ sufficiently small be given and suppose we want to find $n$ so that $|\sin n| > 1-d$. Since $\sin(\frac{\pi}{2} - x) = \cos x \geq 1-\frac{x^2}{2}$, we proceed with finding $$n\in \left(\frac{\pi}{2} - \sqrt{2d}, \frac{\pi}{2} + \sqrt{2d}\right) \mod 2\pi.$$ This reduces to finding $n,k\in\mathbb{Z}$ such that $$\left|n - k(2\pi) - \frac{\pi}{2}\right| \leq \sqrt{2d}.$$ It looks like a Dirichlet approximation theorem at this point, but what I need is a bound on $n$ in terms of $d$. I tried using the Pigeonhole Principle (which would elucidate a bound like the below), but I got stuck. The idea is (naively, I think it's possible) to obtain a bound that looks something like \begin{align*} |n| \leq \frac{M}{\sqrt{d}}, \end{align*} from which the proof is easy to complete. Any ideas on how to bridge this gap to obtain such a bound? Thanks in advance! Edits: Typographical errors.","In the Scottish Book, there is a question posed by Stanislaw Hartman, which goes as follows: It is easy to see that $\liminf |\cos^n n| = \liminf |\sin^n n| = 0$. A bit harder is to prove $\limsup|\cos n|^n = 1$ and $\limsup|\sin n|^n = 1$. Find $\liminf(\cos n)^n$, $\liminf(\sin n)^n$, and $\limsup(\sin n)^n$. For the time being, I can see easily why the first two equalities hold. But how does one prove $\limsup|\cos n|^n = 1$ and $\limsup|\sin n|^n = 1$? My attempt for $\sin$: Try to find $|\sin n|$ arbitrarily close to $1$. Let $d$ sufficiently small be given and suppose we want to find $n$ so that $|\sin n| > 1-d$. Since $\sin(\frac{\pi}{2} - x) = \cos x \geq 1-\frac{x^2}{2}$, we proceed with finding $$n\in \left(\frac{\pi}{2} - \sqrt{2d}, \frac{\pi}{2} + \sqrt{2d}\right) \mod 2\pi.$$ This reduces to finding $n,k\in\mathbb{Z}$ such that $$\left|n - k(2\pi) - \frac{\pi}{2}\right| \leq \sqrt{2d}.$$ It looks like a Dirichlet approximation theorem at this point, but what I need is a bound on $n$ in terms of $d$. I tried using the Pigeonhole Principle (which would elucidate a bound like the below), but I got stuck. The idea is (naively, I think it's possible) to obtain a bound that looks something like \begin{align*} |n| \leq \frac{M}{\sqrt{d}}, \end{align*} from which the proof is easy to complete. Any ideas on how to bridge this gap to obtain such a bound? Thanks in advance! Edits: Typographical errors.",,"['real-analysis', 'limsup-and-liminf']"
14,Comparing largest root of degree three polynomial,Comparing largest root of degree three polynomial,,"Given $n\ge 12$, consider the three polynomials: $$f(x)=x^2-\left(\frac{(n-4)^2}{n-3}+\frac{n-2}{3(n-3)}+\frac{4}{3}\right)x+\frac{4(n-4)^2}{3(n-3)},$$ $$g(x)=x^3-\left(2+\frac{(n-5)^2}{n-4}\right)x^2+\left(\frac{3}{4}+\frac{3(n-5)^2}{2(n-4)}\right)x-\frac{(n-5)^2}{4(n-4)},$$ $$h(x)=x^3-\left(2+\frac{(n-5)(n-4)}{n-3}\right)x^2+\left(\frac{3}{4}+\frac{(n-5)(n-4)}{n-3}\right)x-\frac{(n-5)(n-4)}{4(n-3)},$$ Suppose $f_\lambda, g_\lambda,h_\lambda$ are the largest root of $f(x),g(x),h(x)$ respectively. In order to prove one result I need to show that  $f_\lambda >g_\lambda >h_\lambda.$ For some specific values I have verified it. Is it true for all $n\ge 12$ ? Partial Answer: As mathlove mentioned in the below answer, the result does not hold. But still I feel at least $f_{\lambda}>h_{\lambda}$ is true. Any idea for this one?","Given $n\ge 12$, consider the three polynomials: $$f(x)=x^2-\left(\frac{(n-4)^2}{n-3}+\frac{n-2}{3(n-3)}+\frac{4}{3}\right)x+\frac{4(n-4)^2}{3(n-3)},$$ $$g(x)=x^3-\left(2+\frac{(n-5)^2}{n-4}\right)x^2+\left(\frac{3}{4}+\frac{3(n-5)^2}{2(n-4)}\right)x-\frac{(n-5)^2}{4(n-4)},$$ $$h(x)=x^3-\left(2+\frac{(n-5)(n-4)}{n-3}\right)x^2+\left(\frac{3}{4}+\frac{(n-5)(n-4)}{n-3}\right)x-\frac{(n-5)(n-4)}{4(n-3)},$$ Suppose $f_\lambda, g_\lambda,h_\lambda$ are the largest root of $f(x),g(x),h(x)$ respectively. In order to prove one result I need to show that  $f_\lambda >g_\lambda >h_\lambda.$ For some specific values I have verified it. Is it true for all $n\ge 12$ ? Partial Answer: As mathlove mentioned in the below answer, the result does not hold. But still I feel at least $f_{\lambda}>h_{\lambda}$ is true. Any idea for this one?",,"['real-analysis', 'inequality', 'polynomials', 'roots']"
15,Prove that $f(x) = 0$ for all $x \in \mathbb{R}$ (Analysis),Prove that  for all  (Analysis),f(x) = 0 x \in \mathbb{R},"Let $f: \mathbb{R} \to \mathbb{R}$ be a function such that $f(x) = f^{(4)}(x)$ with $f(0) = f’(0) = f’’(0) = f’’’(0) = 0.$ Prove $f(x) = 0$ for all $x \in \mathbb{R}$ My Attempts: Suppose $x \in \mathbb{R}$. Note that $\displaystyle f'(0) = \lim_{x\to 0} \frac{f(x)-f(0)}{x-0} = \lim_{x\to 0} \frac{f(x)}{x} = \lim_{x\to 0} \frac{f'(x)}{1}=0$. (L'Hôpital's Rule was used in the second to last limit due to the form $\frac{0}{0}$). With this approach, I am not necessarily finding if $x = 0$ on the whole real line. This led me to a different approach: Suppose $x \in [0,b]$. By Mean Value Theorem, there exists  $c \in (0,b)$ so that $\displaystyle \frac{f(b)-f(0)}{b-0} = f'(c)$. This approach doesn't bring me anywhere either, even if I repeatedly use Mean Value Theorem. Any suggestions on how to proceed and conclude? (I am currently reading/finishing the chapter on Differentiation in baby Rudin.)","Let $f: \mathbb{R} \to \mathbb{R}$ be a function such that $f(x) = f^{(4)}(x)$ with $f(0) = f’(0) = f’’(0) = f’’’(0) = 0.$ Prove $f(x) = 0$ for all $x \in \mathbb{R}$ My Attempts: Suppose $x \in \mathbb{R}$. Note that $\displaystyle f'(0) = \lim_{x\to 0} \frac{f(x)-f(0)}{x-0} = \lim_{x\to 0} \frac{f(x)}{x} = \lim_{x\to 0} \frac{f'(x)}{1}=0$. (L'Hôpital's Rule was used in the second to last limit due to the form $\frac{0}{0}$). With this approach, I am not necessarily finding if $x = 0$ on the whole real line. This led me to a different approach: Suppose $x \in [0,b]$. By Mean Value Theorem, there exists  $c \in (0,b)$ so that $\displaystyle \frac{f(b)-f(0)}{b-0} = f'(c)$. This approach doesn't bring me anywhere either, even if I repeatedly use Mean Value Theorem. Any suggestions on how to proceed and conclude? (I am currently reading/finishing the chapter on Differentiation in baby Rudin.)",,['real-analysis']
16,Proving that $\sum_{n=1}^{\infty} \left(\frac{a_1^{1/s}+a_2^{1/s}+\cdots +a_n^{1/s}}{n}\right)^s$ converges when $\sum_{n=1}^{\infty}a_n $ converges,Proving that  converges when  converges,\sum_{n=1}^{\infty} \left(\frac{a_1^{1/s}+a_2^{1/s}+\cdots +a_n^{1/s}}{n}\right)^s \sum_{n=1}^{\infty}a_n ,"Assume that $a_n\ge0$ such that $\sum_{n=1}^{\infty}a_n $ converges, then show that for every $s>1$ the following series converges too:   $$\sum_{n=1}^{\infty} \left(\frac{a_1^{1/s}+a_2^{1/s}+\cdots +a_n^{1/s}}{n}\right)^s.$$ I failed to handle this with Hölder inequality. Any tips or hint will be appreciated. Also it might be helpful to see that there is a Césaro sum of $(a_n^{1/s})_n$ appearing in the last series.","Assume that $a_n\ge0$ such that $\sum_{n=1}^{\infty}a_n $ converges, then show that for every $s>1$ the following series converges too:   $$\sum_{n=1}^{\infty} \left(\frac{a_1^{1/s}+a_2^{1/s}+\cdots +a_n^{1/s}}{n}\right)^s.$$ I failed to handle this with Hölder inequality. Any tips or hint will be appreciated. Also it might be helpful to see that there is a Césaro sum of $(a_n^{1/s})_n$ appearing in the last series.",,"['calculus', 'real-analysis', 'sequences-and-series', 'convergence-divergence', 'summation']"
17,If $u\in H^1(\Omega )$ then $|u|\in H^1(\Omega )$.,If  then .,u\in H^1(\Omega ) |u|\in H^1(\Omega ),"Let $\Omega $ a smooth domain and $u\in H^1(\Omega )$. Prove that $$|u|\in H^1(\Omega ).$$ The proof goes as following : We define $$v_\varepsilon=\sqrt{\varepsilon+u^2}.$$ Then by the chain rule $$\nabla v_\varepsilon=\frac{u\nabla u}{\sqrt{\varepsilon+u^2}},$$  and $$v_\varepsilon\to |u|\quad \text{and}\quad \nabla v_\varepsilon=\frac{u\nabla u}{\sqrt{\varepsilon+v^2}}\to \frac{u}{|u|}\nabla u$$ in $L^2(\Omega )$. The claim follow. Questions 1) $\Omega $ is not supposed bounded. So why $\sqrt{\varepsilon+u^2}\in L^2(\Omega )$ ? Isn't it wrong ? To me $\int_\Omega v_\varepsilon^2=\int_\Omega (\varepsilon+u^2)$ and there is no reason for it to be finite. 2) If $v_\varepsilon$ is indeed in $L^2(\Omega )$ then why $$v_\varepsilon\to |u|\quad \text{and}\quad \nabla v_\varepsilon=\frac{u\nabla u}{\sqrt{\varepsilon+v^2}}\to \frac{u}{|u|}\nabla u$$ in $L^2(\Omega )$ give us $|u|\in H^1(\Omega )$ ?","Let $\Omega $ a smooth domain and $u\in H^1(\Omega )$. Prove that $$|u|\in H^1(\Omega ).$$ The proof goes as following : We define $$v_\varepsilon=\sqrt{\varepsilon+u^2}.$$ Then by the chain rule $$\nabla v_\varepsilon=\frac{u\nabla u}{\sqrt{\varepsilon+u^2}},$$  and $$v_\varepsilon\to |u|\quad \text{and}\quad \nabla v_\varepsilon=\frac{u\nabla u}{\sqrt{\varepsilon+v^2}}\to \frac{u}{|u|}\nabla u$$ in $L^2(\Omega )$. The claim follow. Questions 1) $\Omega $ is not supposed bounded. So why $\sqrt{\varepsilon+u^2}\in L^2(\Omega )$ ? Isn't it wrong ? To me $\int_\Omega v_\varepsilon^2=\int_\Omega (\varepsilon+u^2)$ and there is no reason for it to be finite. 2) If $v_\varepsilon$ is indeed in $L^2(\Omega )$ then why $$v_\varepsilon\to |u|\quad \text{and}\quad \nabla v_\varepsilon=\frac{u\nabla u}{\sqrt{\varepsilon+v^2}}\to \frac{u}{|u|}\nabla u$$ in $L^2(\Omega )$ give us $|u|\in H^1(\Omega )$ ?",,"['real-analysis', 'sobolev-spaces']"
18,measurable functions and existence decreasing function,measurable functions and existence decreasing function,,"Let $f(t)$ be a measurable and almost everywhere finite function, defined on the closed interval  $E = [a, b]$. Prove the existence of a decreasing function $g (t)$, defined on [a, b], which satisfies the relation $m(E \cap \left \{ x: {g > x} \right \}) = m(E \cap \left \{ x: {f > x} \right \}) $ for all real x. I'm not sure I can use the theorem: Let $f(x)$ be defined on the entire real line $\mathbb{R}$ and be measurable and finite  almost everywhere. Then, for every $\varepsilon > 0$ and $\alpha> 0$, there exists a continuous function  $g (x)$ defined on the entire line such that $m(\mathbb{R}\cap x:|f-g| \geq \alpha)<   \varepsilon$. Thanks.","Let $f(t)$ be a measurable and almost everywhere finite function, defined on the closed interval  $E = [a, b]$. Prove the existence of a decreasing function $g (t)$, defined on [a, b], which satisfies the relation $m(E \cap \left \{ x: {g > x} \right \}) = m(E \cap \left \{ x: {f > x} \right \}) $ for all real x. I'm not sure I can use the theorem: Let $f(x)$ be defined on the entire real line $\mathbb{R}$ and be measurable and finite  almost everywhere. Then, for every $\varepsilon > 0$ and $\alpha> 0$, there exists a continuous function  $g (x)$ defined on the entire line such that $m(\mathbb{R}\cap x:|f-g| \geq \alpha)<   \varepsilon$. Thanks.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
19,"Let $f$ be continuous on $[-1,1]$, Show that $g_n \to f$ uniformly on $[-1,1]$","Let  be continuous on , Show that  uniformly on","f [-1,1] g_n \to f [-1,1]","Let $f$ be continuous on $[-1,1]$, Show that $g_n \to f$ uniformly on $[-1,1]$ where $$ g_n(x) = \int_{-1}^{1}f(y)p_n(y - x)dy $$ and $$ p_n(x) = \frac{(1-x^2)^n}{\int_{-1}^{1}(1-x^2)^ndx} $$ I was trying to show that $$ \lim_{n\to\infty}\sup_{x \in [-1,1]}|g_n - f| = 0 $$ I permute the sums and integral and use the binomial theorem. Also, I use the fact that $f$ is bounded (continuous on a compact set). After all of that, I could only get an upper bound for the sup. But I can not show that the bound goes to $0$ as $n$ goes to $\infty$. There is a simpler approach? Thanks.","Let $f$ be continuous on $[-1,1]$, Show that $g_n \to f$ uniformly on $[-1,1]$ where $$ g_n(x) = \int_{-1}^{1}f(y)p_n(y - x)dy $$ and $$ p_n(x) = \frac{(1-x^2)^n}{\int_{-1}^{1}(1-x^2)^ndx} $$ I was trying to show that $$ \lim_{n\to\infty}\sup_{x \in [-1,1]}|g_n - f| = 0 $$ I permute the sums and integral and use the binomial theorem. Also, I use the fact that $f$ is bounded (continuous on a compact set). After all of that, I could only get an upper bound for the sup. But I can not show that the bound goes to $0$ as $n$ goes to $\infty$. There is a simpler approach? Thanks.",,"['real-analysis', 'measure-theory', 'continuity', 'uniform-convergence']"
20,"If a function is not-strictly-increasing, is there an interval on which it is weakly decreasing?","If a function is not-strictly-increasing, is there an interval on which it is weakly decreasing?",,"Suppose a continuous function $f$ is not strictly-increasing, i.e, there are $a,b$ such that $a<b$ and $f(a) \geq f(b)$. Does there exists an interval $a',b'$, with $a\leq a' < b' \leq b$, in which $f$ is weakly-decreasing, i.e: $$ \forall x,y \in[a',b']:  ~ x<y \implies f(x)\geq f(y) $$ ? Initially I thought it was trivial, but now I do not see how to prove it... EDIT: if this is not true, what minimal assumption should be made on $f$ to make it true? (e.g, is it sufficient to assume that $f$ is differentiable? smooth?)","Suppose a continuous function $f$ is not strictly-increasing, i.e, there are $a,b$ such that $a<b$ and $f(a) \geq f(b)$. Does there exists an interval $a',b'$, with $a\leq a' < b' \leq b$, in which $f$ is weakly-decreasing, i.e: $$ \forall x,y \in[a',b']:  ~ x<y \implies f(x)\geq f(y) $$ ? Initially I thought it was trivial, but now I do not see how to prove it... EDIT: if this is not true, what minimal assumption should be made on $f$ to make it true? (e.g, is it sufficient to assume that $f$ is differentiable? smooth?)",,"['calculus', 'real-analysis']"
21,Union of fat Cantor sets?,Union of fat Cantor sets?,,"A question came up asking me to find two disjoint sets $A, B$ such that $[0, 1] = A \cup B$, $A$ is meager and $m(B) = 0$. My thought was the following: Let $\mathcal{C}_k$ denote the fat Cantor set obtained, starting with $[0, 1]$, by removing the middle open interval of length $(1/k)^n$ for each $n^{th}$ iteration, ad infinitum. Each fat Cantor set will be nowhere dense, and so the union $A = \bigcup_{k=4}^{\infty} \mathcal{C}_k$ is clearly meager, but does it have full measure? (in which case, $B = A^c$ would work)","A question came up asking me to find two disjoint sets $A, B$ such that $[0, 1] = A \cup B$, $A$ is meager and $m(B) = 0$. My thought was the following: Let $\mathcal{C}_k$ denote the fat Cantor set obtained, starting with $[0, 1]$, by removing the middle open interval of length $(1/k)^n$ for each $n^{th}$ iteration, ad infinitum. Each fat Cantor set will be nowhere dense, and so the union $A = \bigcup_{k=4}^{\infty} \mathcal{C}_k$ is clearly meager, but does it have full measure? (in which case, $B = A^c$ would work)",,['real-analysis']
22,"Prove that if $f:[0,1] \to \mathbb{R}$ is absolutely continuous, then $|f|^p$ is absolutely continuous for $p>1$","Prove that if  is absolutely continuous, then  is absolutely continuous for","f:[0,1] \to \mathbb{R} |f|^p p>1","I am trying to show that if $f:[0,1]\to \mathbb{R}$ is absolutely continuous then $|f|^p$ is absolutely continuous for all $p>1$. Since the product of absolutely continuous functions is absolutely continuous, the result is obvious if $p$ is an integer. My first thought in trying to prove the result was to try to generalize the proof that the product of absolutely continuous functions is absolutely continuous. Following that strategy and using the fact that $f$ is bounded, it's not difficult to reduce the problem to proving the result for the case that $1<p<2$. Once I get here though I am not sure how to proceed. Any help is appreciated!","I am trying to show that if $f:[0,1]\to \mathbb{R}$ is absolutely continuous then $|f|^p$ is absolutely continuous for all $p>1$. Since the product of absolutely continuous functions is absolutely continuous, the result is obvious if $p$ is an integer. My first thought in trying to prove the result was to try to generalize the proof that the product of absolutely continuous functions is absolutely continuous. Following that strategy and using the fact that $f$ is bounded, it's not difficult to reduce the problem to proving the result for the case that $1<p<2$. Once I get here though I am not sure how to proceed. Any help is appreciated!",,"['real-analysis', 'absolute-continuity']"
23,What can you say about a mapping $f : \Bbb Z\to \Bbb Q$?,What can you say about a mapping ?,f : \Bbb Z\to \Bbb Q,"Written with StackEdit .   Which of the following can be true for a mapping $$ f : \Bbb Z \to \Bbb Q $$ A. It is bijective and increasing B. It is onto and decreasing C. It is bijective and satisfied $f(n) \ge 0$ if $n \le 0$ D. It has uncountable image Here's my attempt at finding a mapping that satisfies C(Following Method to prove countability of $\Bbb Q$ from Stephen Abbot  ) Define $A_n = \{ \frac ab : a,b\ge 0, \gcd(a,b) = 1,a+b=n,b \ne 0 \}$ $B_n = \{ \frac {-a}{b} : a,b > 0, \gcd(a,b) = 1, a+b = n \} $ Mapping the negative integers and 0 to the set $A_n \forall  n \in \Bbb N$ and positive integers to the set $B_n \ \forall  n \ \in \Bbb N$ in the same manner as Stephen Abbot, the mapping will be surjective because the sets $A_n$ and $B_n$ would certainly cover $\Bbb Q$ and would be injective because no two sets have any elements in common. Correct Answer - C Source - Tata Institute of Fundamental Research Graduate Studies 2014 Is my proof correct? In any case, I can't imagine such a method to be required in this problem so is there a more 'easy' mapping? Also, why can't we have a mapping as desired in A and B?","Written with StackEdit .   Which of the following can be true for a mapping $$ f : \Bbb Z \to \Bbb Q $$ A. It is bijective and increasing B. It is onto and decreasing C. It is bijective and satisfied $f(n) \ge 0$ if $n \le 0$ D. It has uncountable image Here's my attempt at finding a mapping that satisfies C(Following Method to prove countability of $\Bbb Q$ from Stephen Abbot  ) Define $A_n = \{ \frac ab : a,b\ge 0, \gcd(a,b) = 1,a+b=n,b \ne 0 \}$ $B_n = \{ \frac {-a}{b} : a,b > 0, \gcd(a,b) = 1, a+b = n \} $ Mapping the negative integers and 0 to the set $A_n \forall  n \in \Bbb N$ and positive integers to the set $B_n \ \forall  n \ \in \Bbb N$ in the same manner as Stephen Abbot, the mapping will be surjective because the sets $A_n$ and $B_n$ would certainly cover $\Bbb Q$ and would be injective because no two sets have any elements in common. Correct Answer - C Source - Tata Institute of Fundamental Research Graduate Studies 2014 Is my proof correct? In any case, I can't imagine such a method to be required in this problem so is there a more 'easy' mapping? Also, why can't we have a mapping as desired in A and B?",,"['real-analysis', 'functions', 'elementary-set-theory', 'proof-verification']"
24,How to prove $\sum_{k=1}^\infty\frac{k^k}{k!}x^k=\frac{1}{2}$ where $x=\frac{1}{3}e^{-1/3}$,How to prove  where,\sum_{k=1}^\infty\frac{k^k}{k!}x^k=\frac{1}{2} x=\frac{1}{3}e^{-1/3},"How to prove that $$ \sum_{k=1}^\infty\frac{k^k}{k!}x^k=\frac{1}{2}, ~\text{where}~~ x=\frac{1}{3}e^{-1/3}~? $$ I found this sum in my notes, but I don't remember where I got it. Any hints or references would be nice.","How to prove that $$ \sum_{k=1}^\infty\frac{k^k}{k!}x^k=\frac{1}{2}, ~\text{where}~~ x=\frac{1}{3}e^{-1/3}~? $$ I found this sum in my notes, but I don't remember where I got it. Any hints or references would be nice.",,"['calculus', 'real-analysis', 'sequences-and-series', 'complex-analysis', 'closed-form']"
25,Taylor expansion of $\sqrt{f(x)}$ at $ x=0.$,Taylor expansion of  at,\sqrt{f(x)}  x=0.,"Let $f$ be a strictly positive function. Then we can compute the Taylor expansion of $\sqrt{f(x)}$ at zero. Wolfram alpha gives the first terms as $$\sqrt{f(x)}= \sqrt{f(0)} + \frac{f'(0)x}{2\sqrt{f(0)}}+...$$ Now, I was wondering if there is a closed representation of this as a power series? At first glance this does not look at hard, as we just apply chain and quotient-rule, but I could not come up with a closed equation.","Let $f$ be a strictly positive function. Then we can compute the Taylor expansion of $\sqrt{f(x)}$ at zero. Wolfram alpha gives the first terms as $$\sqrt{f(x)}= \sqrt{f(0)} + \frac{f'(0)x}{2\sqrt{f(0)}}+...$$ Now, I was wondering if there is a closed representation of this as a power series? At first glance this does not look at hard, as we just apply chain and quotient-rule, but I could not come up with a closed equation.",,"['real-analysis', 'analysis', 'functions', 'derivatives', 'taylor-expansion']"
26,"Is there a ""measure"" $\mu$ on $\mathbb{R}$ which encodes Hausdorff measure of every dimension?","Is there a ""measure""  on  which encodes Hausdorff measure of every dimension?",\mu \mathbb{R},"A crazy idea... Question Let $\mathcal{B}$ be the Borel subsets of $\mathbb{R}$, and $X$ the set of continuous functions $[0,\infty) \to \mathbb{R}$. Does there exist a ""measure"" function $\mu: \mathcal{B} \to X$ satisfying the following conditions? $\mu(\varnothing) = \boldsymbol{0}$. For all $d \in [0,\infty)$, if $A \in \mathcal{B}$ and $\mu_d$ is $d$-dimensional Hausdorff measure , then: $$\lim_{x \to \infty} \frac{\mu(A)(x)}{x^d} = \mu_d(A).$$ (Specifically if $A$ has Hausdorff dimension $d$ and measure $a$ in that dimension, we want it to grow like $ax^d$, if $a \ne 0, \infty$.) $\mu$ is countably additive, in the following sense: let $A_1, A_2, A_3, \ldots$ be disjoint Borel sets with union $A$, $\mu(A_n) = f_n$ and $\mu(A) = f$. If $\sum_{n=1}^\infty f_n$ converges uniformly on compact subsets of $\mathbb{R}$, then $$ \sum_{n=1}^\infty f_n = f. $$ Update: Eric Wofsey has shown below by an elementary argument that if $\mu$ also satisfies: Translation-invariance: $\mu(A + r) = \mu(A)$ for all $A \in \mathcal{B}$ and $r \in \mathbb{R}$. then this is impossible. What about 1-3? Motivation While Lebesgue measure on $\mathbb{R}$ fails to capture any size differences between different measure-zero sets, we can capture such information with the Hausdorff dimension . By identifying a set with its Hausdorff dimension and its Hausdorff measure in that dimension, we get a much richer notion of size. For example, $\varnothing$ (dimension 0 measure 0) is smaller than $\{1,4\}$ (dimension 0 measure $2$), which is smaller than $\mathbb{Q}$ (dimension 0 measure $\infty$), which is smaller than the Cantor set (dimension $\frac{\ln 2}{\ln 3}$ measure $1$) which is smaller than $[0,1]$ (dimension $1$ measure $1$), which is smaller than $\mathbb{R}$ (dimension $1$ measure $\infty$). I have long been curious: can we capture this notion of size in a single ""measure"" on Borel sets -- not real-valued, but function-valued? Above is a specific attempt.","A crazy idea... Question Let $\mathcal{B}$ be the Borel subsets of $\mathbb{R}$, and $X$ the set of continuous functions $[0,\infty) \to \mathbb{R}$. Does there exist a ""measure"" function $\mu: \mathcal{B} \to X$ satisfying the following conditions? $\mu(\varnothing) = \boldsymbol{0}$. For all $d \in [0,\infty)$, if $A \in \mathcal{B}$ and $\mu_d$ is $d$-dimensional Hausdorff measure , then: $$\lim_{x \to \infty} \frac{\mu(A)(x)}{x^d} = \mu_d(A).$$ (Specifically if $A$ has Hausdorff dimension $d$ and measure $a$ in that dimension, we want it to grow like $ax^d$, if $a \ne 0, \infty$.) $\mu$ is countably additive, in the following sense: let $A_1, A_2, A_3, \ldots$ be disjoint Borel sets with union $A$, $\mu(A_n) = f_n$ and $\mu(A) = f$. If $\sum_{n=1}^\infty f_n$ converges uniformly on compact subsets of $\mathbb{R}$, then $$ \sum_{n=1}^\infty f_n = f. $$ Update: Eric Wofsey has shown below by an elementary argument that if $\mu$ also satisfies: Translation-invariance: $\mu(A + r) = \mu(A)$ for all $A \in \mathcal{B}$ and $r \in \mathbb{R}$. then this is impossible. What about 1-3? Motivation While Lebesgue measure on $\mathbb{R}$ fails to capture any size differences between different measure-zero sets, we can capture such information with the Hausdorff dimension . By identifying a set with its Hausdorff dimension and its Hausdorff measure in that dimension, we get a much richer notion of size. For example, $\varnothing$ (dimension 0 measure 0) is smaller than $\{1,4\}$ (dimension 0 measure $2$), which is smaller than $\mathbb{Q}$ (dimension 0 measure $\infty$), which is smaller than the Cantor set (dimension $\frac{\ln 2}{\ln 3}$ measure $1$) which is smaller than $[0,1]$ (dimension $1$ measure $1$), which is smaller than $\mathbb{R}$ (dimension $1$ measure $\infty$). I have long been curious: can we capture this notion of size in a single ""measure"" on Borel sets -- not real-valued, but function-valued? Above is a specific attempt.",,"['real-analysis', 'functional-analysis', 'measure-theory', 'hausdorff-measure']"
27,If $(f_n)\to f$ uniformly and $f_n$ is uniformly continuous for all $n$ then $f$ is uniformly continuous,If  uniformly and  is uniformly continuous for all  then  is uniformly continuous,(f_n)\to f f_n n f,"Show if is true or false: if $(f_n)$ converges uniformly to $f$, and $f_n$ is uniformly continuous for all $n$ then $f$ is uniformly continuous I think is true. My attempt to prove it: if $(f_n)\to f$ uniformly then we can write $$(\forall\varepsilon>0)(\exists N\in\Bbb N)(\forall x\in\mathcal D):|f_n(x)-f(x)|<\varepsilon,\quad\forall n>N\tag{1}$$ and cause all $f_n$ are uniformly continuous $$(\forall\varepsilon>0)(\exists\delta>0)(\forall x,y\in\mathcal D):|x-y|<\delta\implies|f_n(x)-f_n(y)|<\varepsilon,\quad\forall n\in\Bbb N\tag{2}$$ and I want to prove that both conditions implies $$(\forall\varepsilon>0)(\exists\delta>0)(\forall x,y\in\mathcal D):|x-y|<\delta\implies|f(x)-f(y)|<\varepsilon\tag{3}$$ where $\mathcal D$ is the domain of all of them (cause I have the previous knowledge that uniform convergence of continuous functions implies that the limit function is continuous). Then from $(3)$ I can write $$|f(x)-f(y)|=|f(x)-f_m(x)+f_m(x)-f(y)|\le |f(x)-f_m(x)|+|f_m(x)-f(y)|$$ Then I will use some $m$ that holds $(1)$ for some $\frac{\varepsilon}{3}$. And from $(2)$ I will use the $\delta$ that holds for the same $\frac{\varepsilon}{3}$. If $|f(y)-f_m(y)|<\frac{\varepsilon}{3}$ then $f(y)<f_m(y)+\frac{\varepsilon}{3}$. And then finally I can write: $$\begin{align}|f(x)-f(y)|&\le|f(x)-f_m(x)|+|f_m(x)-f(y)|\\&<\frac{\varepsilon}{3}+|f_m(x)-f_m(y)-\frac{\varepsilon}{3}|\\&<\frac{\varepsilon}{3}+|f_m(x)-f_m(y)|+\frac{\varepsilon}{3}\\&<\frac{\varepsilon}{3}+\frac{\varepsilon}{3}+\frac{\varepsilon}{3}=\varepsilon\end{align}$$ then it proves that exists a $\delta$ such that $|f(x)-f(y)|<\varepsilon$ for some $\varepsilon$ in the required conditions. Now, can you check my proof, telling me if it is right or if it lacks something? Thank you in advance.","Show if is true or false: if $(f_n)$ converges uniformly to $f$, and $f_n$ is uniformly continuous for all $n$ then $f$ is uniformly continuous I think is true. My attempt to prove it: if $(f_n)\to f$ uniformly then we can write $$(\forall\varepsilon>0)(\exists N\in\Bbb N)(\forall x\in\mathcal D):|f_n(x)-f(x)|<\varepsilon,\quad\forall n>N\tag{1}$$ and cause all $f_n$ are uniformly continuous $$(\forall\varepsilon>0)(\exists\delta>0)(\forall x,y\in\mathcal D):|x-y|<\delta\implies|f_n(x)-f_n(y)|<\varepsilon,\quad\forall n\in\Bbb N\tag{2}$$ and I want to prove that both conditions implies $$(\forall\varepsilon>0)(\exists\delta>0)(\forall x,y\in\mathcal D):|x-y|<\delta\implies|f(x)-f(y)|<\varepsilon\tag{3}$$ where $\mathcal D$ is the domain of all of them (cause I have the previous knowledge that uniform convergence of continuous functions implies that the limit function is continuous). Then from $(3)$ I can write $$|f(x)-f(y)|=|f(x)-f_m(x)+f_m(x)-f(y)|\le |f(x)-f_m(x)|+|f_m(x)-f(y)|$$ Then I will use some $m$ that holds $(1)$ for some $\frac{\varepsilon}{3}$. And from $(2)$ I will use the $\delta$ that holds for the same $\frac{\varepsilon}{3}$. If $|f(y)-f_m(y)|<\frac{\varepsilon}{3}$ then $f(y)<f_m(y)+\frac{\varepsilon}{3}$. And then finally I can write: $$\begin{align}|f(x)-f(y)|&\le|f(x)-f_m(x)|+|f_m(x)-f(y)|\\&<\frac{\varepsilon}{3}+|f_m(x)-f_m(y)-\frac{\varepsilon}{3}|\\&<\frac{\varepsilon}{3}+|f_m(x)-f_m(y)|+\frac{\varepsilon}{3}\\&<\frac{\varepsilon}{3}+\frac{\varepsilon}{3}+\frac{\varepsilon}{3}=\varepsilon\end{align}$$ then it proves that exists a $\delta$ such that $|f(x)-f(y)|<\varepsilon$ for some $\varepsilon$ in the required conditions. Now, can you check my proof, telling me if it is right or if it lacks something? Thank you in advance.",,"['real-analysis', 'proof-verification']"
28,What are vector norms used for?,What are vector norms used for?,,"I'm currently working with a computer science problem that requires me to build vectors that can return their own norms. Based on Wolfram Alpha's description, I think I have an idea of how this is accomplished for the simple $L^2$ norm ($\sqrt{a^2+b^2+c^2}$, and so on) required by the exercise, but I've no notion of what this is actually useful for or why I would want to find it, outside of it being required by the exercise. Any insight is appreciated!","I'm currently working with a computer science problem that requires me to build vectors that can return their own norms. Based on Wolfram Alpha's description, I think I have an idea of how this is accomplished for the simple $L^2$ norm ($\sqrt{a^2+b^2+c^2}$, and so on) required by the exercise, but I've no notion of what this is actually useful for or why I would want to find it, outside of it being required by the exercise. Any insight is appreciated!",,"['real-analysis', 'linear-algebra', 'general-topology', 'vectors', 'normed-spaces']"
29,Is $\int_{M_{n}(\mathbb{R})} e^{-A^{2}}d\mu$ a convergent integral?(2),Is  a convergent integral?(2),\int_{M_{n}(\mathbb{R})} e^{-A^{2}}d\mu,"We  identify $M_{n}(\mathbb{R})$ with $\mathbb{R}^{n^{2}}$ We put $\int_{M_{n}(\mathbb{R})} e^{-A^{2}}d\mu=\lim_{r\to \infty}  \int_{D_{r}} e^{-A^{2}}$  where the later is  counted as a Riemann integral not Lebesgue integral. Here $D_{r}$  is the disc of radius $r$ with respect to the Euclidean  norm of $\mathbb{R}^{n^{2}}$. Is the above integral a convergent improper integral? What about if we consider $D_{r}$  with respect to the matrix norm? The following post shows that this integral is not convergent in the Lebesgue sense. It also shows that if it is Riemann convergent, then the value of integral is  an scalar matrix. Is $\int_{M_{n}(\mathbb{R})} e^{-A^{2}}d\mu$ a convergent integral?","We  identify $M_{n}(\mathbb{R})$ with $\mathbb{R}^{n^{2}}$ We put $\int_{M_{n}(\mathbb{R})} e^{-A^{2}}d\mu=\lim_{r\to \infty}  \int_{D_{r}} e^{-A^{2}}$  where the later is  counted as a Riemann integral not Lebesgue integral. Here $D_{r}$  is the disc of radius $r$ with respect to the Euclidean  norm of $\mathbb{R}^{n^{2}}$. Is the above integral a convergent improper integral? What about if we consider $D_{r}$  with respect to the matrix norm? The following post shows that this integral is not convergent in the Lebesgue sense. It also shows that if it is Riemann convergent, then the value of integral is  an scalar matrix. Is $\int_{M_{n}(\mathbb{R})} e^{-A^{2}}d\mu$ a convergent integral?",,"['real-analysis', 'integration', 'matrices', 'improper-integrals', 'gaussian-integral']"
30,Why do we take the closure of the support?,Why do we take the closure of the support?,,"In topology and analysis we define the support of a continuous real function $f:X\rightarrow \mathbb R$ to be $ \left\{ x\in X:f(x)\neq 0\right\}$. This is the complement of the fiber $f^{-1} \left\{0 \right\}$. So it looks like the support is always an open set. Why then do we take its closure? In algebraic geometry, if we look at elements of a ring as regular functions, then it's tempting to define their support the same way, which yields $\operatorname{supp}f= \left\{\mathfrak p\in \operatorname{Spec}R:f\notin \mathfrak p \right\}$. But these are exactly the basic open sets of the Zariski topology. I'm just trying to understand whether this is not a healthy way to see things because I've been told ""supports should be closed"".","In topology and analysis we define the support of a continuous real function $f:X\rightarrow \mathbb R$ to be $ \left\{ x\in X:f(x)\neq 0\right\}$. This is the complement of the fiber $f^{-1} \left\{0 \right\}$. So it looks like the support is always an open set. Why then do we take its closure? In algebraic geometry, if we look at elements of a ring as regular functions, then it's tempting to define their support the same way, which yields $\operatorname{supp}f= \left\{\mathfrak p\in \operatorname{Spec}R:f\notin \mathfrak p \right\}$. But these are exactly the basic open sets of the Zariski topology. I'm just trying to understand whether this is not a healthy way to see things because I've been told ""supports should be closed"".",,"['real-analysis', 'general-topology']"
31,An exercise about Lebesgue integrable functions & convergence,An exercise about Lebesgue integrable functions & convergence,,"Let $(\Omega ,F,\mu)$ a be a measure space & $\{f_n\}$ a  sequence of nonnegative Lebesgue integrable functions. If $\{f_n\}$ converges in measure  to function $f $ & the integrals converges to $\int_\Omega f < \infty $, prove that  $\int_\Omega |f_n -f|\,d\mu$ converges to  $0$, when  $n $ goes to  infinity. It seems easy, but I didn't success. Help me please.","Let $(\Omega ,F,\mu)$ a be a measure space & $\{f_n\}$ a  sequence of nonnegative Lebesgue integrable functions. If $\{f_n\}$ converges in measure  to function $f $ & the integrals converges to $\int_\Omega f < \infty $, prove that  $\int_\Omega |f_n -f|\,d\mu$ converges to  $0$, when  $n $ goes to  infinity. It seems easy, but I didn't success. Help me please.",,"['real-analysis', 'integration', 'measure-theory', 'lebesgue-integral']"
32,Non measurable function but measurable pre-image,Non measurable function but measurable pre-image,,"I am having trouble with a problem. It gives a hint to use the Vitali construction, but I honestly do not understand it. The question is: Show that there is a function $f:\mathbb{R} \to \mathbb{R}$ is not Lebesgue measurable, but $\forall$ $ a \in \mathbb{R}$ $f^{-1}(a)$ is measurable. My try:  Let $E$ be a non-measurable subset of $\mathbb{R}$ , and $f(x)= \begin{cases}        x & x\in E \\       -x & x\notin E      \end{cases} $ I am not certain that this is correct. Any help would be greatly appreciated, and all apologies for the simplistic inquiries.","I am having trouble with a problem. It gives a hint to use the Vitali construction, but I honestly do not understand it. The question is: Show that there is a function $f:\mathbb{R} \to \mathbb{R}$ is not Lebesgue measurable, but $\forall$ $ a \in \mathbb{R}$ $f^{-1}(a)$ is measurable. My try:  Let $E$ be a non-measurable subset of $\mathbb{R}$ , and $f(x)= \begin{cases}        x & x\in E \\       -x & x\notin E      \end{cases} $ I am not certain that this is correct. Any help would be greatly appreciated, and all apologies for the simplistic inquiries.",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
33,Limit points of particular sets of real numbers.,Limit points of particular sets of real numbers.,,"How to find  limit points of the following sets of real numbers, for irrational $\alpha$  ? $(1)$ $\{ m+n\alpha:m,n\in\mathbb{Z}\}.$ $(2)$ $\{ m+n\alpha:m\in\mathbb{N},n\in\mathbb{Z}\}.$ $(3)$ $\{ m+n\alpha:m\in\mathbb{Z},n\in\mathbb{N}\}.$ $(4)$ $\{ m+n\alpha:m,n\in\mathbb{N}\}.$ There is a result, which says that every additive subgroup of the group  $(\mathbb{R},+)$ is either discrete or dense in $\mathbb{R}.$ Therefore using this fact we can say that set in  $(1)$ is dense in $\mathbb{R}.$ I think set in $(4)$ has no limit point because of $m,n\in\mathbb{N}.$ But I don't know how to find limit points of there sets. Please explain. Thanks in advance.","How to find  limit points of the following sets of real numbers, for irrational $\alpha$  ? $(1)$ $\{ m+n\alpha:m,n\in\mathbb{Z}\}.$ $(2)$ $\{ m+n\alpha:m\in\mathbb{N},n\in\mathbb{Z}\}.$ $(3)$ $\{ m+n\alpha:m\in\mathbb{Z},n\in\mathbb{N}\}.$ $(4)$ $\{ m+n\alpha:m,n\in\mathbb{N}\}.$ There is a result, which says that every additive subgroup of the group  $(\mathbb{R},+)$ is either discrete or dense in $\mathbb{R}.$ Therefore using this fact we can say that set in  $(1)$ is dense in $\mathbb{R}.$ I think set in $(4)$ has no limit point because of $m,n\in\mathbb{N}.$ But I don't know how to find limit points of there sets. Please explain. Thanks in advance.",,"['real-analysis', 'irrational-numbers']"
34,Two inequalities for $\left(\int_{a}^{b}|f(x)|^rdx\right)^{\frac{1}{r}}$,Two inequalities for,\left(\int_{a}^{b}|f(x)|^rdx\right)^{\frac{1}{r}},"Show that if $f\in \mathcal C^{n+1}([a,b])$ and $f(a)=f^{'}(a)=\cdots=f^\left(n\right)(a)=0,$ then the following statements are ture: $\mathbf a)$ $ \forall r\in[1,\infty),$the inequality $$\left(\int_{a}^{b}|f(x)|^rdx\right)^{\frac{1}{r}} \leq \frac{(b-a)^{n+\frac{1}{r}}}{n!(nr+1)^{\frac{1}{r}}}\int_{a}^{b}|f^{(n+1)}(x)|dx$$holds. $\mathbf b)$ $ \forall r\in[1,\infty),$the inequality $$\left(\int_{a}^{b}|f(x)|^rdx\right)^{\frac{1}{r}} \leq \frac{2^{\frac{1}{r}}(b-a)^{n+\frac{1}{r}+\frac{1}{2}}}{n!\sqrt{2n+1}(2nr+r+1)^{\frac{1}{r}}}\left(\int_{a}^{b}|f^{(n+1)}(x)|^{2}dx\right)^{\frac{1}{2}}$$holds. Using Taylor's Theorem with Integral form  of the Remainder ,I can easily get $$\left(\int_{a}^{b}|f(x)|^rdx\right)^{\frac{1}{r}} \leq \frac{(b-a)^{n+\frac{1}{r}}}{n!}\int_{a}^{b}|f^{(n+1)}(x)|dx.\quad(\star)$$ I tried to apply Holder's inequality for integrals to $(\star) $ in question $\mathbf a)$,but I have yet to prove it holds.I believe this two questions might have the same method.Any help you can provide will be greatly appreciated!","Show that if $f\in \mathcal C^{n+1}([a,b])$ and $f(a)=f^{'}(a)=\cdots=f^\left(n\right)(a)=0,$ then the following statements are ture: $\mathbf a)$ $ \forall r\in[1,\infty),$the inequality $$\left(\int_{a}^{b}|f(x)|^rdx\right)^{\frac{1}{r}} \leq \frac{(b-a)^{n+\frac{1}{r}}}{n!(nr+1)^{\frac{1}{r}}}\int_{a}^{b}|f^{(n+1)}(x)|dx$$holds. $\mathbf b)$ $ \forall r\in[1,\infty),$the inequality $$\left(\int_{a}^{b}|f(x)|^rdx\right)^{\frac{1}{r}} \leq \frac{2^{\frac{1}{r}}(b-a)^{n+\frac{1}{r}+\frac{1}{2}}}{n!\sqrt{2n+1}(2nr+r+1)^{\frac{1}{r}}}\left(\int_{a}^{b}|f^{(n+1)}(x)|^{2}dx\right)^{\frac{1}{2}}$$holds. Using Taylor's Theorem with Integral form  of the Remainder ,I can easily get $$\left(\int_{a}^{b}|f(x)|^rdx\right)^{\frac{1}{r}} \leq \frac{(b-a)^{n+\frac{1}{r}}}{n!}\int_{a}^{b}|f^{(n+1)}(x)|dx.\quad(\star)$$ I tried to apply Holder's inequality for integrals to $(\star) $ in question $\mathbf a)$,but I have yet to prove it holds.I believe this two questions might have the same method.Any help you can provide will be greatly appreciated!",,"['calculus', 'real-analysis']"
35,How can I get the exact value of this infinite series?,How can I get the exact value of this infinite series?,,"I want to compute the exact value of this infinite series $$\sum_{n=2}^\infty\arcsin{\left(\dfrac{2}{\sqrt{n(n+1)}(\sqrt{n}+\sqrt{n-1})}\right)}$$ By comparison test, we can get the series is convengence. I tried to find some hints from the exact value of $\displaystyle\sum_{n=2}^\infty\arcsin{\left(\dfrac{\sqrt{n}-\sqrt{n-1}}{\sqrt{n^2-1}}\right)}$ ,but the split phase method maybe difficult to solve this  question. I am not sure whether it has  a closed form.But if not so,how can I evaluate the sum ?","I want to compute the exact value of this infinite series $$\sum_{n=2}^\infty\arcsin{\left(\dfrac{2}{\sqrt{n(n+1)}(\sqrt{n}+\sqrt{n-1})}\right)}$$ By comparison test, we can get the series is convengence. I tried to find some hints from the exact value of $\displaystyle\sum_{n=2}^\infty\arcsin{\left(\dfrac{\sqrt{n}-\sqrt{n-1}}{\sqrt{n^2-1}}\right)}$ ,but the split phase method maybe difficult to solve this  question. I am not sure whether it has  a closed form.But if not so,how can I evaluate the sum ?",,['real-analysis']
36,"If $g$ is continuous and $g(1)=0$, then $x^ng(x)$ converges on $[0,1)$","If  is continuous and , then  converges on","g g(1)=0 x^ng(x) [0,1)","Suppose $g:[0,1]\to\mathbb R$ is a continuous function satisfying $g(1)=0$. Prove that the functions $f_n(x)=x^ng(x)$ converge uniformly on $[0,1]$. Hence or using Mean Value Theorem, prove that if $\int_0^1x^ng(x)dx=0$ for every $n\in\mathbb N$ then $g$ is identically $0$. First of all, I don't think the convergence is uniform. The pointwise limit is $0$. Now consider $\sup_{x\in[0,1)}x^n|g(x)|=c_n^n|g(c_n)|$ for some $c_n$ which depends on $n$. The supremum is attained as the function $x^n|g(x)|$ is continuous. Now, as I vary $n$, $c_n$ varies and thus, we cannot conclude anything I think. Secondly, I don't understand how the first part can imply the second part of the question. I obtained the second part using Bernstein Polynomials (Weierstrauss Approximation Theorem) using the fact that these polynomials are uniformly convergent to $g$. I tried using Mean Value Theorem but could not proceed further. But this lands in a new question: Suppose $f$ is a continuous non-zero function on a closed bounded interval $[a,b]$. Then is it true that $f$ cannot have infinitely many roots in $[a,b]$? If I know the answer to this question, I can answer the second part of the original question. The reasoning being that for every $n$, using Mean Value Theorem, there exists $c_n\in[0,1]$ such that $f(c_n)\int_0^1x^ndx=0$ giving $f(c_n)=0$ for all $n$. I want to show this is a contradiction if $f$ is non-zero.","Suppose $g:[0,1]\to\mathbb R$ is a continuous function satisfying $g(1)=0$. Prove that the functions $f_n(x)=x^ng(x)$ converge uniformly on $[0,1]$. Hence or using Mean Value Theorem, prove that if $\int_0^1x^ng(x)dx=0$ for every $n\in\mathbb N$ then $g$ is identically $0$. First of all, I don't think the convergence is uniform. The pointwise limit is $0$. Now consider $\sup_{x\in[0,1)}x^n|g(x)|=c_n^n|g(c_n)|$ for some $c_n$ which depends on $n$. The supremum is attained as the function $x^n|g(x)|$ is continuous. Now, as I vary $n$, $c_n$ varies and thus, we cannot conclude anything I think. Secondly, I don't understand how the first part can imply the second part of the question. I obtained the second part using Bernstein Polynomials (Weierstrauss Approximation Theorem) using the fact that these polynomials are uniformly convergent to $g$. I tried using Mean Value Theorem but could not proceed further. But this lands in a new question: Suppose $f$ is a continuous non-zero function on a closed bounded interval $[a,b]$. Then is it true that $f$ cannot have infinitely many roots in $[a,b]$? If I know the answer to this question, I can answer the second part of the original question. The reasoning being that for every $n$, using Mean Value Theorem, there exists $c_n\in[0,1]$ such that $f(c_n)\int_0^1x^ndx=0$ giving $f(c_n)=0$ for all $n$. I want to show this is a contradiction if $f$ is non-zero.",,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence', 'continuity']"
37,Prove the set of transcendental number is dense in $\mathbb{R}$ using Baire category theorem,Prove the set of transcendental number is dense in  using Baire category theorem,\mathbb{R},"I want to show the set of transcendental number $T$ is dense in $\mathbb{R}$ using Baire category theorem. The fact is easier to show directly using definition and Archimedean property. But  I was asked to use Baire category theorem specifically. Since $\mathbb{R}$ is complete metric space, I think I should express $T$ as a countable intersection of open dense set, which is also dense. But I don't know how to construct it. I have found a proof using Liouville number , that is show the set of Liouville number is dense using Baire category theorem and also show Liouville number is transcendental. But since it's an optional exercise from 2nd year undergraduate course, I don't expect the proof will be so complicate. Is there any easier way?","I want to show the set of transcendental number $T$ is dense in $\mathbb{R}$ using Baire category theorem. The fact is easier to show directly using definition and Archimedean property. But  I was asked to use Baire category theorem specifically. Since $\mathbb{R}$ is complete metric space, I think I should express $T$ as a countable intersection of open dense set, which is also dense. But I don't know how to construct it. I have found a proof using Liouville number , that is show the set of Liouville number is dense using Baire category theorem and also show Liouville number is transcendental. But since it's an optional exercise from 2nd year undergraduate course, I don't expect the proof will be so complicate. Is there any easier way?",,"['real-analysis', 'general-topology', 'real-numbers']"
38,References on integration: collections of fully worked problems (and explanations) of (1) advanced and (2) unusual techniques,References on integration: collections of fully worked problems (and explanations) of (1) advanced and (2) unusual techniques,,"I am searching for two kinds of books. (1) Comprehensive books that collect , explain , and provide many examples (that is, fully worked problems) of advanced integration techniques (that is,   something at the level of difficulty of the tables written by   Gradshteyn and Ryzhik, but obviously with explanations, examples and   proofs). (2) Comprehensive books that collect , explain , and provide many examples (that is, fully worked problems) of really unusual and slick integration   techniques (which may however not be so advanced or use special   functions). Can you point out some good references? Related question: "" Really advanced techniques of integration (definite or indefinite) "". Remark: Clearly, an answer should add some references that have not been mentioned yet (either in the comments or in the related thread),.","I am searching for two kinds of books. (1) Comprehensive books that collect , explain , and provide many examples (that is, fully worked problems) of advanced integration techniques (that is,   something at the level of difficulty of the tables written by   Gradshteyn and Ryzhik, but obviously with explanations, examples and   proofs). (2) Comprehensive books that collect , explain , and provide many examples (that is, fully worked problems) of really unusual and slick integration   techniques (which may however not be so advanced or use special   functions). Can you point out some good references? Related question: "" Really advanced techniques of integration (definite or indefinite) "". Remark: Clearly, an answer should add some references that have not been mentioned yet (either in the comments or in the related thread),.",,"['calculus', 'real-analysis', 'integration', 'analysis', 'reference-request']"
39,Prove $\int_0^1 \frac{\ln(1+t^{4+\sqrt{15}})}{1+t}\mathrm dt= -\frac{\pi^2}{12}(\sqrt{15}-2)+\ln (2) \ln(\sqrt{3}+\sqrt{5})+\ln(\phi) \ln(2+\sqrt{3})$,Prove,\int_0^1 \frac{\ln(1+t^{4+\sqrt{15}})}{1+t}\mathrm dt= -\frac{\pi^2}{12}(\sqrt{15}-2)+\ln (2) \ln(\sqrt{3}+\sqrt{5})+\ln(\phi) \ln(2+\sqrt{3}),Prove that: \begin{equation} \int_0^1 \frac{\ln\left(1+t^{4+\sqrt{15}}\right)}{1+t}\mathrm dt= -\frac{\pi^2}{12}(\sqrt{15}-2)+\ln (2) \ln(\sqrt{3}+\sqrt{5})+\ln(\phi) \ln(2+\sqrt{3}) \end{equation} where $\phi$ is the golden ratio. My friend gave me this challenging problem but I cannot prove it into the given expression. She doesn't know either how to approach this problem. I have tried to use a substitution to obtain the improper integral so that I can use the derivative of beta function but it failed. I also tried to use Taylor series for $\frac{1}{1+t}$ and $\ln\left(1+t^{4+\sqrt{15}}\right)$ but I am unable to derive the result from double sums of series. Could anyone here please help me how to prove it? Any method is welcome and also any help would be greatly appreciated. Thank you.,Prove that: \begin{equation} \int_0^1 \frac{\ln\left(1+t^{4+\sqrt{15}}\right)}{1+t}\mathrm dt= -\frac{\pi^2}{12}(\sqrt{15}-2)+\ln (2) \ln(\sqrt{3}+\sqrt{5})+\ln(\phi) \ln(2+\sqrt{3}) \end{equation} where $\phi$ is the golden ratio. My friend gave me this challenging problem but I cannot prove it into the given expression. She doesn't know either how to approach this problem. I have tried to use a substitution to obtain the improper integral so that I can use the derivative of beta function but it failed. I also tried to use Taylor series for $\frac{1}{1+t}$ and $\ln\left(1+t^{4+\sqrt{15}}\right)$ but I am unable to derive the result from double sums of series. Could anyone here please help me how to prove it? Any method is welcome and also any help would be greatly appreciated. Thank you.,,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
40,Prove that no function exists such that...,Prove that no function exists such that...,,"The exercise goes like this: Find a continous function $f:\mathbb{R} \rightarrow \mathbb{R}$ such that $\forall c \in \mathbb{R}$ the equation $f(x)=c$ has exactly 3 solutions; Prove that no continuous function $f: \mathbb{R} \rightarrow \mathbb{R}$ exists such that the equation $f(x)=c$ has exactly two solutions $\forall c \in \mathbb{R}$; For what $n \in \mathbb{N}$ it's true that a continous function $f:\mathbb{R} \rightarrow \mathbb{R}$ such that $\forall c \in \mathbb{R}$ the equation $f(x) = c$ has exactly $n$ solutions exists? For the first point I built kind of a zigzag function which is easily generalizable to every odd $n$. It seems true to me that for even natural numbers such a function doesn't exist, because in some ways it would have to ""jump"", but I failed to formalize the argument.","The exercise goes like this: Find a continous function $f:\mathbb{R} \rightarrow \mathbb{R}$ such that $\forall c \in \mathbb{R}$ the equation $f(x)=c$ has exactly 3 solutions; Prove that no continuous function $f: \mathbb{R} \rightarrow \mathbb{R}$ exists such that the equation $f(x)=c$ has exactly two solutions $\forall c \in \mathbb{R}$; For what $n \in \mathbb{N}$ it's true that a continous function $f:\mathbb{R} \rightarrow \mathbb{R}$ such that $\forall c \in \mathbb{R}$ the equation $f(x) = c$ has exactly $n$ solutions exists? For the first point I built kind of a zigzag function which is easily generalizable to every odd $n$. It seems true to me that for even natural numbers such a function doesn't exist, because in some ways it would have to ""jump"", but I failed to formalize the argument.",,"['calculus', 'real-analysis', 'functions']"
41,Curve Selection Conjecture,Curve Selection Conjecture,,"I have the following conjecture which I cannot seem to settle either way: Let $f:[0,1]\to\mathbb R^2$ be a differentiable function such that $f(0)=(0,0)$. Then there exists a continuous function $g:[0,1]\to\mathbb R^2$ such that: 1) $g(0)=(0,0)$ 2) $g([0,1])\cap f([0,1])=\{(0,0)\}$. 3) $g$ is not a constant function. (Thanks to user68061 for pointing this out.) Basically what I am trying to prove is that if we have a differentiable curve in $\mathbb R^2$ which passes through origin then we can find a continuous curve in $\mathbb R^2$ which intersects the given curve only at origin. Does anybody know if this is an already known result or if there exists a counterexample? Thanks in advance for your help.","I have the following conjecture which I cannot seem to settle either way: Let $f:[0,1]\to\mathbb R^2$ be a differentiable function such that $f(0)=(0,0)$. Then there exists a continuous function $g:[0,1]\to\mathbb R^2$ such that: 1) $g(0)=(0,0)$ 2) $g([0,1])\cap f([0,1])=\{(0,0)\}$. 3) $g$ is not a constant function. (Thanks to user68061 for pointing this out.) Basically what I am trying to prove is that if we have a differentiable curve in $\mathbb R^2$ which passes through origin then we can find a continuous curve in $\mathbb R^2$ which intersects the given curve only at origin. Does anybody know if this is an already known result or if there exists a counterexample? Thanks in advance for your help.",,['real-analysis']
42,A continuous function integral inequality,A continuous function integral inequality,,"Let $m$ be a positive integer.  $f\colon[0,\infty)\to[0,\infty)$ is a continuous function such that $f(f(x))=x^m,\forall x\in[0,\infty)$. Show that $$\int_0^1f^2(x)\,dx\ge\frac{2m-1}{m^2+6m-3}$$","Let $m$ be a positive integer.  $f\colon[0,\infty)\to[0,\infty)$ is a continuous function such that $f(f(x))=x^m,\forall x\in[0,\infty)$. Show that $$\int_0^1f^2(x)\,dx\ge\frac{2m-1}{m^2+6m-3}$$",,"['calculus', 'real-analysis', 'integral-inequality']"
43,A baby version of the Stein-Cotlar almost-orthogonality lemma,A baby version of the Stein-Cotlar almost-orthogonality lemma,,"The following is an exercise from Stein and Shakarchi's Real Analysis . Suppose $\{T_k\}$ is a collection of bounded operators on a Hilbert space $H$, each with norm at most $1$. Suppose also that $$T_kT^*_j = T^*_k T_j =0$$ for $k\neq j$. Let $S_n(f)=\sum_{k=1}^n T_k(f)$. The problem asks to show that $\lim_{N\rightarrow \infty} S_n(f)$ converges for every $f$ and that the the resulting operator $S$ has norm at most $1$. Is the sketch written in the spoiler box below correct? I am mostly worried about the construction in the second and third sentences. Any alternative slick solutions are also welcome. Consider first the case of a finite number of operators. Since the operators have mutually orthogonal ranges, the closures of their ranges are also mutually orthogonal. This allows us to decompose the space into $B\oplus_1^n V_k$, where the $V_k$ are the closures of the ranges and $B$ is what is left over after taking the direct sum of all the $V_k$. Let $f=\sum v_k$ denote the decomposition of a function $f$ into these spaces. Then, recalling the ranges are orthogonal, $\|S_n(f)\|\le \sum_1^n \|T_k(f)\|\le \sum |T_k (v_k)|\le \sum_1^n |v_k|\le |f|.$ This shows the sequence is absolutely convergent, so it is convergent, since we are in a complete space. It also immediately shows the limit is an operator and that this operator has norm at most 1.","The following is an exercise from Stein and Shakarchi's Real Analysis . Suppose $\{T_k\}$ is a collection of bounded operators on a Hilbert space $H$, each with norm at most $1$. Suppose also that $$T_kT^*_j = T^*_k T_j =0$$ for $k\neq j$. Let $S_n(f)=\sum_{k=1}^n T_k(f)$. The problem asks to show that $\lim_{N\rightarrow \infty} S_n(f)$ converges for every $f$ and that the the resulting operator $S$ has norm at most $1$. Is the sketch written in the spoiler box below correct? I am mostly worried about the construction in the second and third sentences. Any alternative slick solutions are also welcome. Consider first the case of a finite number of operators. Since the operators have mutually orthogonal ranges, the closures of their ranges are also mutually orthogonal. This allows us to decompose the space into $B\oplus_1^n V_k$, where the $V_k$ are the closures of the ranges and $B$ is what is left over after taking the direct sum of all the $V_k$. Let $f=\sum v_k$ denote the decomposition of a function $f$ into these spaces. Then, recalling the ranges are orthogonal, $\|S_n(f)\|\le \sum_1^n \|T_k(f)\|\le \sum |T_k (v_k)|\le \sum_1^n |v_k|\le |f|.$ This shows the sequence is absolutely convergent, so it is convergent, since we are in a complete space. It also immediately shows the limit is an operator and that this operator has norm at most 1.",,"['real-analysis', 'functional-analysis', 'hilbert-spaces', 'proof-verification']"
44,Baby Rudin Problem 2.17: Homeomorphism to a Cantor Set?,Baby Rudin Problem 2.17: Homeomorphism to a Cantor Set?,,"Problem 2.17 asks us to consider some properties of numbers in $[0,1]$ in base $10$ with decimal expansions consisting only of $4$ and $7$.  Let's call the set of all these numbers $E$. The question asks us something different, but is this set homeomorphic to the Cantor set?  In particular, if we write each element in the Cantor set in base $3$, and send $0$s to $4$s and $2$s to $7$s, will we get a homeomorphism with $E$? Also, what about the set of base-$10$ numbers with decimal expansions consisting only of $3$ or more separate digits, for example?  Will these be homeomorphic to the Cantor set, or to something else?","Problem 2.17 asks us to consider some properties of numbers in $[0,1]$ in base $10$ with decimal expansions consisting only of $4$ and $7$.  Let's call the set of all these numbers $E$. The question asks us something different, but is this set homeomorphic to the Cantor set?  In particular, if we write each element in the Cantor set in base $3$, and send $0$s to $4$s and $2$s to $7$s, will we get a homeomorphism with $E$? Also, what about the set of base-$10$ numbers with decimal expansions consisting only of $3$ or more separate digits, for example?  Will these be homeomorphic to the Cantor set, or to something else?",,"['real-analysis', 'general-topology']"
45,Properties of Continuous Functions,Properties of Continuous Functions,,"Prove that there is no continuous function $f:\mathbb{R} \rightarrow \mathbb{R}$ such that for $c \in \mathbb{R}$ the equation $f(x)=c$ has exactly two solutions. This is what I have so far. Proof by contradiction, suppose $f:\mathbb{R} \rightarrow \mathbb{R}$ is continuous and the equation $f(x)=c$ has exactly two solutions, $a$ and $b$ with $a<b$.  On the interval $[a,b], f(t)\leq c$ for all $t$, or $f(t) \geq c$ for all $t$.  Assume the latter.  Then $f(t)<c$ for $t<a$ and $f(t)<c$ for $t>b$.  $f$ has a maximum value on $[a,b]$ at exactly two points, $d$ and $g$, assume $d<g$.  Now choose any $t$ such that $d<t<g$.  Then $f(a)=c<f(t)<f(d)$ and $f(b)=c<f(t)<f(g)$.  So the equation $f(x)=f(t)$ has three solutions, $t, c_{1}, c_{2}$ where $a<c_{1}<d$ and $g<c_{2}<b$ by the intermediate value theorem.","Prove that there is no continuous function $f:\mathbb{R} \rightarrow \mathbb{R}$ such that for $c \in \mathbb{R}$ the equation $f(x)=c$ has exactly two solutions. This is what I have so far. Proof by contradiction, suppose $f:\mathbb{R} \rightarrow \mathbb{R}$ is continuous and the equation $f(x)=c$ has exactly two solutions, $a$ and $b$ with $a<b$.  On the interval $[a,b], f(t)\leq c$ for all $t$, or $f(t) \geq c$ for all $t$.  Assume the latter.  Then $f(t)<c$ for $t<a$ and $f(t)<c$ for $t>b$.  $f$ has a maximum value on $[a,b]$ at exactly two points, $d$ and $g$, assume $d<g$.  Now choose any $t$ such that $d<t<g$.  Then $f(a)=c<f(t)<f(d)$ and $f(b)=c<f(t)<f(g)$.  So the equation $f(x)=f(t)$ has three solutions, $t, c_{1}, c_{2}$ where $a<c_{1}<d$ and $g<c_{2}<b$ by the intermediate value theorem.",,"['real-analysis', 'general-topology', 'continuity']"
46,"Baby Rudin, chapter 10, problem 1 - independence of order in multiple integrals.","Baby Rudin, chapter 10, problem 1 - independence of order in multiple integrals.",,"In problem 1, Rudin asks for a generalization of example 10.4, which states that if $f$ is continuous in the standard k-simplex $Q^k$, then the integral $\int_{Q^k} f$ exists, and that the order of the $n$ separate integrations is immaterial. He proves that by approximating $f$ by a sequence of functions which are continuous in a k-cell containing $Q^k$. In problem 1 we need to show that the same hold, if $Q^k$ is replaced by any compact and convex $H \subseteq \mathbb R^k$. There's a hint, suggesting that one should approximate $f$ by continuous functions in $\mathbb R^k$ with support $\subseteq H$. For fixed $\delta>0,$I've introduced the function $\varphi_\delta(t) =\begin{cases} \frac{1}{\delta}t & 0 \le t \le \delta \\ 1 & t>\delta \end{cases}$ and approximating $f$ by $F(x)=f(x) \varphi_\delta(\rho_{H^c}(x))$, where $\rho_E(x)=\inf \{ d(x,y) : y \in E\}$ is the distance between $x$ and $E$. I can't follow his proof, Namely the inequality (7) on page 247 is problematic for me. Thanks in advance EDIT: Lebesgue's theory is not allowed","In problem 1, Rudin asks for a generalization of example 10.4, which states that if $f$ is continuous in the standard k-simplex $Q^k$, then the integral $\int_{Q^k} f$ exists, and that the order of the $n$ separate integrations is immaterial. He proves that by approximating $f$ by a sequence of functions which are continuous in a k-cell containing $Q^k$. In problem 1 we need to show that the same hold, if $Q^k$ is replaced by any compact and convex $H \subseteq \mathbb R^k$. There's a hint, suggesting that one should approximate $f$ by continuous functions in $\mathbb R^k$ with support $\subseteq H$. For fixed $\delta>0,$I've introduced the function $\varphi_\delta(t) =\begin{cases} \frac{1}{\delta}t & 0 \le t \le \delta \\ 1 & t>\delta \end{cases}$ and approximating $f$ by $F(x)=f(x) \varphi_\delta(\rho_{H^c}(x))$, where $\rho_E(x)=\inf \{ d(x,y) : y \in E\}$ is the distance between $x$ and $E$. I can't follow his proof, Namely the inequality (7) on page 247 is problematic for me. Thanks in advance EDIT: Lebesgue's theory is not allowed",,"['real-analysis', 'multivariable-calculus', 'integration']"
47,Accumulation Point at Infinity?,Accumulation Point at Infinity?,,"This is a follow up question to the one I posted here . So my new question involves accumulation points and infinity. Can infinity be considered an accumulation point? For example consider the following set  $$S = \{ 2^n + \frac{1}{k} \space : \space n, k \in N \}$$  Now if we fix $n$ the set tends to $2^n$ but if we fix $k$ the set tends to $\infty$. Does this mean that accumulation points exist at $2^n$ and $\infty$? Any help would be appreciated.","This is a follow up question to the one I posted here . So my new question involves accumulation points and infinity. Can infinity be considered an accumulation point? For example consider the following set  $$S = \{ 2^n + \frac{1}{k} \space : \space n, k \in N \}$$  Now if we fix $n$ the set tends to $2^n$ but if we fix $k$ the set tends to $\infty$. Does this mean that accumulation points exist at $2^n$ and $\infty$? Any help would be appreciated.",,"['real-analysis', 'analysis']"
48,Show the iff statement of lower semicontinuous,Show the iff statement of lower semicontinuous,,"Given $X\subseteq \Bbb R^m, f:X\to\Bbb R$ and $x\in X$, we say $f$ is lower semicontinous (l.s.c for short) at x if $\forall \varepsilon>0\ \exists\ \delta >0\ \forall \in B(\delta,x), \ f(x)\le f(y)+\varepsilon$.  I wish to show: If $X$ is closed, then $f$ is l.s.c if and only if the set $f^{-1}((-\infty,r]):=\{a\in X:f(a)\le r\}$ is closed for each $r\in \Bbb R$. I wonder how I could use the requirement that the set $X$ is closed.","Given $X\subseteq \Bbb R^m, f:X\to\Bbb R$ and $x\in X$, we say $f$ is lower semicontinous (l.s.c for short) at x if $\forall \varepsilon>0\ \exists\ \delta >0\ \forall \in B(\delta,x), \ f(x)\le f(y)+\varepsilon$.  I wish to show: If $X$ is closed, then $f$ is l.s.c if and only if the set $f^{-1}((-\infty,r]):=\{a\in X:f(a)\le r\}$ is closed for each $r\in \Bbb R$. I wonder how I could use the requirement that the set $X$ is closed.",,"['real-analysis', 'semicontinuous-functions']"
49,"If the partial sums of a $a_n$ are bounded, then $\sum{}_{n=1}^\infty a_n e^{-nt}$ converges for all $t > 0$","If the partial sums of a  are bounded, then  converges for all",a_n \sum{}_{n=1}^\infty a_n e^{-nt} t > 0,"If the partial sums of a $a_n$ are bounded, then $$\sum_{n=1}^\infty \frac{a_n }{e^{nt}}$$ converges for all $t > 0$. Proof : since the partial sums of $a_n$ are bounded, then exists $C > 0 $ such that $\left|\sum_{n=1}^M a_n \right| < C$ forall $M \in \mathbb{N}$, so for every $n\in \mathbb N $ we have that $|a_n| < C$. Now, $$ \sum_{n=1}^\infty \frac{a_n}{e^{nt}} = \lim_{M\to\infty} \sum_{n=1}^M \frac{a_n}{e^{nt}} \leq \lim_{M\to\infty} \sum_{n=1}^M \frac{C}{e^{nt}} = C \sum_{n=1}^\infty \left(\frac{1}{e^t}\right)^n < \infty \iff \frac{1}{e^t} < 1 $$ and this is satisfied for every $t > 0$. Is there any error? I'm not convinced with my way of writing the proof. Thanks in advance.","If the partial sums of a $a_n$ are bounded, then $$\sum_{n=1}^\infty \frac{a_n }{e^{nt}}$$ converges for all $t > 0$. Proof : since the partial sums of $a_n$ are bounded, then exists $C > 0 $ such that $\left|\sum_{n=1}^M a_n \right| < C$ forall $M \in \mathbb{N}$, so for every $n\in \mathbb N $ we have that $|a_n| < C$. Now, $$ \sum_{n=1}^\infty \frac{a_n}{e^{nt}} = \lim_{M\to\infty} \sum_{n=1}^M \frac{a_n}{e^{nt}} \leq \lim_{M\to\infty} \sum_{n=1}^M \frac{C}{e^{nt}} = C \sum_{n=1}^\infty \left(\frac{1}{e^t}\right)^n < \infty \iff \frac{1}{e^t} < 1 $$ and this is satisfied for every $t > 0$. Is there any error? I'm not convinced with my way of writing the proof. Thanks in advance.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
50,Integral of $\sin(e^t)dt$,Integral of,\sin(e^t)dt,"Let $f(x) = \int_x^{x+1} \sin(e^t)dt$. Show that $$ e^x|f(x)| < 2 $$ and that $$ e^xf(x) = \cos(e^x) - e^{-1}\cos(e^{x+1}) + r(x) $$ where $|r(x)| < Ce^{-x}$ for some constant $C$. I can do the first part easily. Integration by parts gives us $$ f(x) = \frac{\cos e^x}{e^x} - \frac{\cos e^{x+1}}{e^{x+1}} - \int_{e^x}^{e^{x+1}} \frac{\cos u}{u^2}du, $$ so substituting $\cos u = 1$ and $\cos u = -1$ gives \begin{align*} \cos e^x - 1 + \frac{1}{e}\left(1- \cos e^{x+1}\right) &\leq e^xf(x) \leq \cos e^x + 1 - \frac{1}{e}\left(\cos e^{x+1} + 1\right)\\ -2 &< e^xf(x) < 2 \end{align*} So that $e^x|f(x)| < 2$. I get stuck on the second part, though. From integration by parts, I have $e^xf(x) = \cos e^x - e^{-1}\cos e^{x+1} - e^x\int_{e^x}^{e^{x+1}} \frac{\cos u}{u^2}du$, so it reduces to showing that $$ \left|\int_{e^x}^{e^{x+1}} \frac{\cos u}{u^2}du\right| < Ce^{-2x} $$ for some constant $C$. However, I always end up with a constant for $Ce^{x}$ rather than for $C$.","Let $f(x) = \int_x^{x+1} \sin(e^t)dt$. Show that $$ e^x|f(x)| < 2 $$ and that $$ e^xf(x) = \cos(e^x) - e^{-1}\cos(e^{x+1}) + r(x) $$ where $|r(x)| < Ce^{-x}$ for some constant $C$. I can do the first part easily. Integration by parts gives us $$ f(x) = \frac{\cos e^x}{e^x} - \frac{\cos e^{x+1}}{e^{x+1}} - \int_{e^x}^{e^{x+1}} \frac{\cos u}{u^2}du, $$ so substituting $\cos u = 1$ and $\cos u = -1$ gives \begin{align*} \cos e^x - 1 + \frac{1}{e}\left(1- \cos e^{x+1}\right) &\leq e^xf(x) \leq \cos e^x + 1 - \frac{1}{e}\left(\cos e^{x+1} + 1\right)\\ -2 &< e^xf(x) < 2 \end{align*} So that $e^x|f(x)| < 2$. I get stuck on the second part, though. From integration by parts, I have $e^xf(x) = \cos e^x - e^{-1}\cos e^{x+1} - e^x\int_{e^x}^{e^{x+1}} \frac{\cos u}{u^2}du$, so it reduces to showing that $$ \left|\int_{e^x}^{e^{x+1}} \frac{\cos u}{u^2}du\right| < Ce^{-2x} $$ for some constant $C$. However, I always end up with a constant for $Ce^{x}$ rather than for $C$.",,['real-analysis']
51,Proof for Heine Borel theorem,Proof for Heine Borel theorem,,"I am trying to prove the Heine Borel theorem for compactness of the closed interval $[0,1]$ using Konig's lemma . This is what I have so far: I assume $[0,1]$ can be covered by $\{(a_i,b_i):i=0,1,2\cdots\}$. I construct a graph $G$ as follows: First let a vertex be labelled $[0,1]$ (the root). Then consider $[0,1]-(a_0,b_0)\cup(a_1,b_1)$. This consists of $n_1$ closed intervals where $n_1$ is finite. Adjoin the $[0,1]$ vertex with $n_1$ vertices labelled by these closed intervals (these vertices will be at level 1). Next consider $[0,1]-(a_0,b_0)\cup(a_1,b_1)\cup(a_2,b_2)$. This consists of $n_2$ closed intervals. Each of these closed intervals is a subset of exactly one the closed intervals considered in the previous step. Make $n_2$ vertices labelled by these closed intervals and adjoin them to that vertex created in the previous step of which it is a subset of (these vertices will be at level 2). Continue doing so for higher levels, each time obtaining the labels by considering the closed interval obtained from $[0,1]-(a_0,b_0)\cup(a_1,b_1)\cdots \cup(a_i,b_i)$. This yields a rooted tree $G$ where each level is finite. Suppose the tree contained an infinite path: $[0,1]\supset[\alpha_1,\beta_1]\supset[\alpha_2,\beta_2]\cdots$. Since a sequence of nested closed intervals is nonempty so there is an element $x$ in it. As $x\in [0,1]$ so $x\in(a_i,b_i)$ for some $i$. But then $x$ cannot exist in any interval which is at a level beyond $i$, yielding a contradiction to 4. So by the contrapositive form of Konig's lemma, $G$ cannot be infinite. It follows that for some $i$, $[0,1]-(a_0,b_0)\cup(a_1,b_1)\cdots \cup(a_i,b_i)$ is empty. Hence $[0,1]$ is covered by $(a_0,b_0)\cup(a_1,b_1)\cdots \cup(a_i,b_i)$. My doubts are in the arguments presented in 2. and 6. Are they correct? In particular is this statement: ""Each of these closed intervals is a subset of exactly one the closed intervals considered in the previous step."" correct? What is an upper bound for $n_k$? Thanks","I am trying to prove the Heine Borel theorem for compactness of the closed interval $[0,1]$ using Konig's lemma . This is what I have so far: I assume $[0,1]$ can be covered by $\{(a_i,b_i):i=0,1,2\cdots\}$. I construct a graph $G$ as follows: First let a vertex be labelled $[0,1]$ (the root). Then consider $[0,1]-(a_0,b_0)\cup(a_1,b_1)$. This consists of $n_1$ closed intervals where $n_1$ is finite. Adjoin the $[0,1]$ vertex with $n_1$ vertices labelled by these closed intervals (these vertices will be at level 1). Next consider $[0,1]-(a_0,b_0)\cup(a_1,b_1)\cup(a_2,b_2)$. This consists of $n_2$ closed intervals. Each of these closed intervals is a subset of exactly one the closed intervals considered in the previous step. Make $n_2$ vertices labelled by these closed intervals and adjoin them to that vertex created in the previous step of which it is a subset of (these vertices will be at level 2). Continue doing so for higher levels, each time obtaining the labels by considering the closed interval obtained from $[0,1]-(a_0,b_0)\cup(a_1,b_1)\cdots \cup(a_i,b_i)$. This yields a rooted tree $G$ where each level is finite. Suppose the tree contained an infinite path: $[0,1]\supset[\alpha_1,\beta_1]\supset[\alpha_2,\beta_2]\cdots$. Since a sequence of nested closed intervals is nonempty so there is an element $x$ in it. As $x\in [0,1]$ so $x\in(a_i,b_i)$ for some $i$. But then $x$ cannot exist in any interval which is at a level beyond $i$, yielding a contradiction to 4. So by the contrapositive form of Konig's lemma, $G$ cannot be infinite. It follows that for some $i$, $[0,1]-(a_0,b_0)\cup(a_1,b_1)\cdots \cup(a_i,b_i)$ is empty. Hence $[0,1]$ is covered by $(a_0,b_0)\cup(a_1,b_1)\cdots \cup(a_i,b_i)$. My doubts are in the arguments presented in 2. and 6. Are they correct? In particular is this statement: ""Each of these closed intervals is a subset of exactly one the closed intervals considered in the previous step."" correct? What is an upper bound for $n_k$? Thanks",,['real-analysis']
52,Show that the limit of functions is continuous,Show that the limit of functions is continuous,,"Let $f_n$ be a sequence of not necessarily continuous functions $\mathbb{R} \rightarrow \mathbb{R}$ such that $f_n(x_n) \rightarrow f(x)$ whenever $x_n \rightarrow x$. Show that f is continuous. What I am trying to do is to show that whenever we have $x \in \mathbb{R}$ and $x_n \rightarrow x$, then $f(x_n) \rightarrow f(x)$, when $n \rightarrow \infty$. These types of things are usually showed by using the triangle inequality. I know I can make $|f(x) - f_n(x_n)|$ as small as possible by choosing a big enough n. I can also make $|f(x) - f_n(x)|$ as small as possible. But I am not able to combine these to prove that $|f(x) - f(x_n)|$ can be made as small as possible.","Let $f_n$ be a sequence of not necessarily continuous functions $\mathbb{R} \rightarrow \mathbb{R}$ such that $f_n(x_n) \rightarrow f(x)$ whenever $x_n \rightarrow x$. Show that f is continuous. What I am trying to do is to show that whenever we have $x \in \mathbb{R}$ and $x_n \rightarrow x$, then $f(x_n) \rightarrow f(x)$, when $n \rightarrow \infty$. These types of things are usually showed by using the triangle inequality. I know I can make $|f(x) - f_n(x_n)|$ as small as possible by choosing a big enough n. I can also make $|f(x) - f_n(x)|$ as small as possible. But I am not able to combine these to prove that $|f(x) - f(x_n)|$ can be made as small as possible.",,"['real-analysis', 'limits', 'continuity']"
53,Does the specification of a general sequence require the Axiom of Choice?,Does the specification of a general sequence require the Axiom of Choice?,,"Many results in elementary analysis require some form of the Axiom of Choice (often weaker forms, such as countable or dependent). My question is a bit more specific, regarding sequences. For example, consider a standard proof of the boundedness theorem which states that a function continuous on a closed interval $I$ is bounded on that interval. In the first step of the proof, one specifies a sequence as follows: Suppose for contradiction that $f$ is unbounded. Then for every $n\in\mathbb{N}$ there exists $x_n$ such that $f(x_n) > n$. This specifies a sequence $(x_n)$. I'm not sure if the above example requires choice. To me, it certainly feels like it does. More specifically, I think that we are specifying a sequence of sets $$A(n) = \left\{x\in I\mid f(x) > n\right\}$$ and claiming the existence of a choice function $g$ such that $g(n) \in A(n)$ so that this example specifically requires the axiom of countable choice. Please clarify whether my reasoning is correct. More specifically, does the construction of any general sequence (such as one defined as above, or perhaps recursively) then require some form of choice? Thanks for any help.","Many results in elementary analysis require some form of the Axiom of Choice (often weaker forms, such as countable or dependent). My question is a bit more specific, regarding sequences. For example, consider a standard proof of the boundedness theorem which states that a function continuous on a closed interval $I$ is bounded on that interval. In the first step of the proof, one specifies a sequence as follows: Suppose for contradiction that $f$ is unbounded. Then for every $n\in\mathbb{N}$ there exists $x_n$ such that $f(x_n) > n$. This specifies a sequence $(x_n)$. I'm not sure if the above example requires choice. To me, it certainly feels like it does. More specifically, I think that we are specifying a sequence of sets $$A(n) = \left\{x\in I\mid f(x) > n\right\}$$ and claiming the existence of a choice function $g$ such that $g(n) \in A(n)$ so that this example specifically requires the axiom of countable choice. Please clarify whether my reasoning is correct. More specifically, does the construction of any general sequence (such as one defined as above, or perhaps recursively) then require some form of choice? Thanks for any help.",,"['real-analysis', 'analysis', 'set-theory', 'axiom-of-choice']"
54,Dyadic Expansion-Proof?,Dyadic Expansion-Proof?,,"Working through a measure theory textbook, and would like to understand dyadic expansions before I can understand its connections with the law of large numbers. I want to see this proved in detail, For a mapping $F$ from $\Omega=(0,1]$ into itself given by $$F\omega=\begin{Bmatrix} 2\omega & \mbox{if } 0< \omega\leqslant\frac{1}{2} \\  2\omega-1 & \mbox{if } \frac{1}{2}< \omega\leqslant1  \end{Bmatrix} $$ and $$d_1(\omega)=\begin{Bmatrix} 0 & \mbox{if } 0< \omega\leqslant\frac{1}{2} \\  1 & \mbox{if } \frac{1}{2}< \omega\leqslant1  \end{Bmatrix} $$ and $d_i(\omega)=d_1(F^{i-1}\omega)$ , How do we prove that $\omega=\sum_{i=1}^{\infty }d_i(\omega)/{2^i}$?","Working through a measure theory textbook, and would like to understand dyadic expansions before I can understand its connections with the law of large numbers. I want to see this proved in detail, For a mapping $F$ from $\Omega=(0,1]$ into itself given by $$F\omega=\begin{Bmatrix} 2\omega & \mbox{if } 0< \omega\leqslant\frac{1}{2} \\  2\omega-1 & \mbox{if } \frac{1}{2}< \omega\leqslant1  \end{Bmatrix} $$ and $$d_1(\omega)=\begin{Bmatrix} 0 & \mbox{if } 0< \omega\leqslant\frac{1}{2} \\  1 & \mbox{if } \frac{1}{2}< \omega\leqslant1  \end{Bmatrix} $$ and $d_i(\omega)=d_1(F^{i-1}\omega)$ , How do we prove that $\omega=\sum_{i=1}^{\infty }d_i(\omega)/{2^i}$?",,"['real-analysis', 'sequences-and-series']"
55,Bennett's Inequality to Bernstein's Inequality,Bennett's Inequality to Bernstein's Inequality,,"Bennett's Inequality is stated with a rather unintuitive function, $$ h(u) = (1+u) \log(1+u) - u $$ See here .  I have seen in multiple places that Bernstein's Inequality, while slightly weaker, can be obtained by bounding $h(u)$ from below, $$ h(u) \ge \frac{ u^2 }{ 2 + \frac{2}{3} u} $$ and plugging it back into Bennett's Inequality.  However, I can't see where this expression comes from .  Could someone point me in the right direction?","Bennett's Inequality is stated with a rather unintuitive function, See here .  I have seen in multiple places that Bernstein's Inequality, while slightly weaker, can be obtained by bounding from below, and plugging it back into Bennett's Inequality.  However, I can't see where this expression comes from .  Could someone point me in the right direction?","
h(u) = (1+u) \log(1+u) - u
 h(u) 
h(u) \ge \frac{ u^2 }{ 2 + \frac{2}{3} u}
","['real-analysis', 'probability-theory', 'inequality', 'pade-approximation']"
56,Modern Analogue to Whitney's Geometric Integration Theory?,Modern Analogue to Whitney's Geometric Integration Theory?,,Is there a more modern text that someone could suggest that covers roughly the same material as does Whitney's Geometric Integration Theory? There are a number of topics in this text that I haven't seen treated elsewhere.,Is there a more modern text that someone could suggest that covers roughly the same material as does Whitney's Geometric Integration Theory? There are a number of topics in this text that I haven't seen treated elsewhere.,,"['real-analysis', 'reference-request', 'integration']"
57,Which is the Abel's theorem invoked in the context of convergence of this infinite product?,Which is the Abel's theorem invoked in the context of convergence of this infinite product?,,"Motivation: As I wrote in this answer the following product is evaluated in Proposition 5 of Jean-Paul Allouche and Jeffrey Shallit's paper The ubiquitous Prouhet-Thue-Morse sequence $$P=\displaystyle\prod_{n=0}^{\infty }\left( \frac{2n+1}{2n+2}\right) ^{(-1)^{t_{n}}},$$ where $\left( t_{n}\right) _{n\geq 0}=\left( 0,1,1,0,1,0,0,1,\ldots \right) $ is a binary sequence defined recursively by $t_{0}=0,t_{2n}=t_{n}$ and $% t_{2n+1}=1-t_{n}$ for $n\geq 0$. According to the authors the product is ""convergent by Abel's theorem"", but I have no idea which theorem is this. Due to the properties of $\left( t_{n}\right) $ the terms of $P$ can be broken into two groups of two terms each (this idea came from Ross Millikan). I got: $$\begin{eqnarray*} P &=&\displaystyle\prod_{m=0}^{\infty }\left( \frac{8m+1}{8m+2}\frac{8m+4}{8m+3}\cdot  \frac{8m+6}{8m+5}\frac{8m+7}{8m+8}\right) ^{(-1)^{t_{m}}} \\ &=&\displaystyle\prod_{m=0}^{\infty }\left( 1-\frac{1}{32m^{2}+20m+3}\right) ^{(-1)^{t_{m}}}\displaystyle\prod_{m=0}^{\infty }\left( 1+\frac{1}{32m^{2}+52m+20}% \right) ^{(-1)^{t_{m}}}\text{.} \end{eqnarray*}$$ and wonder if this form helps to prove the convergence of $P$ in a similar way to the proof of the $\displaystyle\prod_{n=0}^{\infty }\left( 1+b_{n}\right) $ (with  $b_{n}\rightarrow 0,b_{n}>0$) is convergent iff $\sum_{n=0}^{\infty }b_{n}$ is convergent. Question : A - Which is the Abel's theorem the authors refer to? B - Suppose we have a product $$\begin{equation*} P^{\prime }=\displaystyle\prod_{n=0}^{\infty }\left( 1+b_{n}\right) ^{e_{n}}\qquad (\ast ) \end{equation*},$$ where [edited] $b_n>0$ is such that $\displaystyle\sum b_n$ converges [end edit], and $e_{n}$ is a sequence that takes $-1$ and $+1$ values. Is $P^{\prime }$ convergent? I think so for if I take the logarithm of  $P$ I get $$\begin{equation*} \sum_{n=0}^{\infty }e_{n}\ln (1+b_{n}) \end{equation*}$$ and this series is absolutely convergent  $$\begin{equation*} \sum_{n=0}^{\infty }\left\vert e_{n}\ln (1+b_{n})\right\vert \leq \sum_{n=0}^{\infty }\left\vert \ln (1+b_{n})\right\vert \end{equation*}$$ by the limit comparison test $$\begin{equation*} \underset{n\rightarrow \infty }{\lim }\frac{\ln (1+b_{n})}{b_{n}}=1. \end{equation*}$$ Is this argument valid? Similar for  $$\begin{equation*} P^{\prime \prime }=\displaystyle\prod_{n=0}^{\infty }\left( 1-a_{n}\right) ^{e_{n}},\qquad (\ast \ast ) \end{equation*},$$ where [edit] $0<a_n<1$ is such that $\displaystyle\sum a_n$ converges [end edit].  The limit comparison test would be $$\begin{equation*} \underset{n\rightarrow \infty }{\lim }\left\vert \frac{\ln (1-a_{n})}{-a_{n}}% \right\vert =1\text{.} \end{equation*}$$","Motivation: As I wrote in this answer the following product is evaluated in Proposition 5 of Jean-Paul Allouche and Jeffrey Shallit's paper The ubiquitous Prouhet-Thue-Morse sequence $$P=\displaystyle\prod_{n=0}^{\infty }\left( \frac{2n+1}{2n+2}\right) ^{(-1)^{t_{n}}},$$ where $\left( t_{n}\right) _{n\geq 0}=\left( 0,1,1,0,1,0,0,1,\ldots \right) $ is a binary sequence defined recursively by $t_{0}=0,t_{2n}=t_{n}$ and $% t_{2n+1}=1-t_{n}$ for $n\geq 0$. According to the authors the product is ""convergent by Abel's theorem"", but I have no idea which theorem is this. Due to the properties of $\left( t_{n}\right) $ the terms of $P$ can be broken into two groups of two terms each (this idea came from Ross Millikan). I got: $$\begin{eqnarray*} P &=&\displaystyle\prod_{m=0}^{\infty }\left( \frac{8m+1}{8m+2}\frac{8m+4}{8m+3}\cdot  \frac{8m+6}{8m+5}\frac{8m+7}{8m+8}\right) ^{(-1)^{t_{m}}} \\ &=&\displaystyle\prod_{m=0}^{\infty }\left( 1-\frac{1}{32m^{2}+20m+3}\right) ^{(-1)^{t_{m}}}\displaystyle\prod_{m=0}^{\infty }\left( 1+\frac{1}{32m^{2}+52m+20}% \right) ^{(-1)^{t_{m}}}\text{.} \end{eqnarray*}$$ and wonder if this form helps to prove the convergence of $P$ in a similar way to the proof of the $\displaystyle\prod_{n=0}^{\infty }\left( 1+b_{n}\right) $ (with  $b_{n}\rightarrow 0,b_{n}>0$) is convergent iff $\sum_{n=0}^{\infty }b_{n}$ is convergent. Question : A - Which is the Abel's theorem the authors refer to? B - Suppose we have a product $$\begin{equation*} P^{\prime }=\displaystyle\prod_{n=0}^{\infty }\left( 1+b_{n}\right) ^{e_{n}}\qquad (\ast ) \end{equation*},$$ where [edited] $b_n>0$ is such that $\displaystyle\sum b_n$ converges [end edit], and $e_{n}$ is a sequence that takes $-1$ and $+1$ values. Is $P^{\prime }$ convergent? I think so for if I take the logarithm of  $P$ I get $$\begin{equation*} \sum_{n=0}^{\infty }e_{n}\ln (1+b_{n}) \end{equation*}$$ and this series is absolutely convergent  $$\begin{equation*} \sum_{n=0}^{\infty }\left\vert e_{n}\ln (1+b_{n})\right\vert \leq \sum_{n=0}^{\infty }\left\vert \ln (1+b_{n})\right\vert \end{equation*}$$ by the limit comparison test $$\begin{equation*} \underset{n\rightarrow \infty }{\lim }\frac{\ln (1+b_{n})}{b_{n}}=1. \end{equation*}$$ Is this argument valid? Similar for  $$\begin{equation*} P^{\prime \prime }=\displaystyle\prod_{n=0}^{\infty }\left( 1-a_{n}\right) ^{e_{n}},\qquad (\ast \ast ) \end{equation*},$$ where [edit] $0<a_n<1$ is such that $\displaystyle\sum a_n$ converges [end edit].  The limit comparison test would be $$\begin{equation*} \underset{n\rightarrow \infty }{\lim }\left\vert \frac{\ln (1-a_{n})}{-a_{n}}% \right\vert =1\text{.} \end{equation*}$$",,"['real-analysis', 'sequences-and-series', 'products']"
58,Lusin Theorem conditions,Lusin Theorem conditions,,"Lusin Theorem (as stated by Rudin): Let $X$ be a locally compact Hausdorff space and let $μ$ be a regular Borel measure on $X$ such that $μ(K)<∞$ for every compact $K⊆X$. Suppose $f$ is a complex measurable function on $X$, $μ(A)<∞$, $f(x)=0$ if $x∈X \setminus A$, and $ϵ>0$. Then there exists a continuous complex function $g$ on $X$ with compact support such that $μ(x:f(x)≠g(x))<ϵ$. But I can't seem to find in the proof anywhere a use of the fact that the measure is finite for compact sets. Is the condition nessecary? Is there a counter-example?","Lusin Theorem (as stated by Rudin): Let $X$ be a locally compact Hausdorff space and let $μ$ be a regular Borel measure on $X$ such that $μ(K)<∞$ for every compact $K⊆X$. Suppose $f$ is a complex measurable function on $X$, $μ(A)<∞$, $f(x)=0$ if $x∈X \setminus A$, and $ϵ>0$. Then there exists a continuous complex function $g$ on $X$ with compact support such that $μ(x:f(x)≠g(x))<ϵ$. But I can't seem to find in the proof anywhere a use of the fact that the measure is finite for compact sets. Is the condition nessecary? Is there a counter-example?",,"['real-analysis', 'measure-theory']"
59,Convergence of functions in $L^p$,Convergence of functions in,L^p,"Let $\{f_k\} \subset L^2(\Omega)$, where $\Omega \subset \mathbb{R}^n$ is a bounded domain  and suppose that $f_k \to f$ in $L^2(\Omega)$. Now if $a \geq 1$ is some constant, is it possible to say that $|f_k|^a \to |f|^a$ in $L^p$ for some $p$ (depending on $a$ and also possibly depending on $n$)? Showing the statement is true would probably require a smart way of bounding $\left| |f_k|^a - |f|^a \right|$ by a term including the factor $|f_k - f|^2$. However, I don't really know what to do with the fact that $a$ doesn't have to be an integer...","Let $\{f_k\} \subset L^2(\Omega)$, where $\Omega \subset \mathbb{R}^n$ is a bounded domain  and suppose that $f_k \to f$ in $L^2(\Omega)$. Now if $a \geq 1$ is some constant, is it possible to say that $|f_k|^a \to |f|^a$ in $L^p$ for some $p$ (depending on $a$ and also possibly depending on $n$)? Showing the statement is true would probably require a smart way of bounding $\left| |f_k|^a - |f|^a \right|$ by a term including the factor $|f_k - f|^2$. However, I don't really know what to do with the fact that $a$ doesn't have to be an integer...",,"['real-analysis', 'convergence-divergence', 'lebesgue-integral', 'lp-spaces']"
60,"How do different notions of ""distribution"" relate to one another?","How do different notions of ""distribution"" relate to one another?",,"In reading ""Real Analysis: Modern Techniques and Their Applications"" (Folland), I've come across a few different notions of ""distribution"" or ""distribution functions."" The distribution function of a finite Borel measure $\mu$ on $\mathbb{R}$ is defined by $F(x) = \mu((-\infty, x])$. The distribution function of a measurable function $f\colon X \to \mathbb{R}$ on a measure space $(X, M, \mu)$ is a function $\lambda_f\colon (0,\infty) \to [0,\infty]$ given by $\lambda_f(\alpha) = \mu(\{x\colon |f(x)| > \alpha\})$. A distribution on an open set $U \subset \mathbb{R}^n$ is a continuous linear functional on $C^\infty_c(U)$. Is there any sort of relationship between these concepts?","In reading ""Real Analysis: Modern Techniques and Their Applications"" (Folland), I've come across a few different notions of ""distribution"" or ""distribution functions."" The distribution function of a finite Borel measure $\mu$ on $\mathbb{R}$ is defined by $F(x) = \mu((-\infty, x])$. The distribution function of a measurable function $f\colon X \to \mathbb{R}$ on a measure space $(X, M, \mu)$ is a function $\lambda_f\colon (0,\infty) \to [0,\infty]$ given by $\lambda_f(\alpha) = \mu(\{x\colon |f(x)| > \alpha\})$. A distribution on an open set $U \subset \mathbb{R}^n$ is a continuous linear functional on $C^\infty_c(U)$. Is there any sort of relationship between these concepts?",,"['real-analysis', 'measure-theory', 'distribution-theory']"
61,weakened $L^p$ interpolation using the Fourier transform?,weakened  interpolation using the Fourier transform?,L^p,"It is easy to see that $$f\in L^\infty(\mathbb R^d), \ f\in L^1(\mathbb R^d) \implies f\in L^p(\mathbb R^d) \ \text{for all}\  p\in[1,\infty] \label{1}\tag{A}$$ Indeed, $ \int|f|^p \le \|f\|_{L^\infty}^{p-1}\|f\|_{L^1}. $ Let $\hat f$ denotes the Fourier transform. Another well-known easy result is $$ \|\hat f\|_{L^\infty} \le \|f\|_{L^1}.$$ It seems natural then to ask if the following generalisation of \eqref{1} could hold? $$f\in L^\infty(\mathbb R^d), \ \hat f\in L^\infty(\mathbb R^d) \implies f\in L^p(\mathbb R^d) \ \text{for some}\  p\in(1,\infty] \label{2}\tag{B}$$ I know that \eqref{2} for $p=1$ is false in general by taking $d=1,f(x)=\frac{\sin x}x\notin L^1$ since $f\in L^p$ for all $p>1$ and $\hat f = C \mathbf 1_{[-1,1]} \in L^\infty$ . Could it be true for any other values of $p$ ? Dare I hope for all $p\in(1,\infty]$ ?","It is easy to see that Indeed, Let denotes the Fourier transform. Another well-known easy result is It seems natural then to ask if the following generalisation of \eqref{1} could hold? I know that \eqref{2} for is false in general by taking since for all and . Could it be true for any other values of ? Dare I hope for all ?","f\in L^\infty(\mathbb R^d), \ f\in L^1(\mathbb R^d) \implies f\in L^p(\mathbb R^d) \ \text{for all}\  p\in[1,\infty] \label{1}\tag{A}  \int|f|^p \le \|f\|_{L^\infty}^{p-1}\|f\|_{L^1}.  \hat f  \|\hat f\|_{L^\infty} \le \|f\|_{L^1}. f\in L^\infty(\mathbb R^d), \ \hat f\in L^\infty(\mathbb R^d) \implies f\in L^p(\mathbb R^d) \ \text{for some}\  p\in(1,\infty] \label{2}\tag{B} p=1 d=1,f(x)=\frac{\sin x}x\notin L^1 f\in L^p p>1 \hat f = C \mathbf 1_{[-1,1]} \in L^\infty p p\in(1,\infty]","['real-analysis', 'fourier-analysis', 'lp-spaces', 'harmonic-analysis', 'integral-inequality']"
62,Prove $\int_{0}^{1} \frac{\ln\left ( 1+\sqrt{1-x^2} \right ) \ln(1+x)}{ x} \text{d}x =-\frac{5}{16}\zeta(3)+\frac{\pi^2}{8}\ln(2)$,Prove,\int_{0}^{1} \frac{\ln\left ( 1+\sqrt{1-x^2} \right ) \ln(1+x)}{ x} \text{d}x =-\frac{5}{16}\zeta(3)+\frac{\pi^2}{8}\ln(2),"I recently got an integral accidently( $\zeta(s)$ is Riemann $\zeta$ function), $$ \int_{0}^{1} \frac{\ln\left ( 1+\sqrt{1-x^2}  \right ) \ln(1+x)}{ x} \text{d}x =-\frac{5}{16}\zeta(3)+\frac{\pi^2}{8}\ln(2), $$ which seems slightly harmonious. So I put it here to ask for an elegant proof for the integral. Besides, let $\operatorname{Li}_n(z)$ be polylogarithms, do similar expressions hold true for $$ \int_{0}^{1} \frac{\ln\left ( 1+\sqrt{1-x^2}  \right )  \operatorname{Li}_n(x)}{ x} \text{d}x $$ and $$ \int_{0}^{1} \frac{\ln\left ( 1+\sqrt{1-x^2}  \right )  \operatorname{Li}_n(-x)}{ x} \text{d}x? $$ Thanks for helping and enlightening me.","I recently got an integral accidently( is Riemann function), which seems slightly harmonious. So I put it here to ask for an elegant proof for the integral. Besides, let be polylogarithms, do similar expressions hold true for and Thanks for helping and enlightening me.","\zeta(s) \zeta 
\int_{0}^{1} \frac{\ln\left ( 1+\sqrt{1-x^2}  \right ) \ln(1+x)}{
x} \text{d}x
=-\frac{5}{16}\zeta(3)+\frac{\pi^2}{8}\ln(2),
 \operatorname{Li}_n(z) 
\int_{0}^{1} \frac{\ln\left ( 1+\sqrt{1-x^2}  \right ) 
\operatorname{Li}_n(x)}{
x} \text{d}x
 
\int_{0}^{1} \frac{\ln\left ( 1+\sqrt{1-x^2}  \right ) 
\operatorname{Li}_n(-x)}{
x} \text{d}x?
","['real-analysis', 'calculus', 'integration', 'definite-integrals']"
63,Ratios of higher order derivatives with exponentially growing bound.,Ratios of higher order derivatives with exponentially growing bound.,,"Let $f: \mathbb R \rightarrow \mathbb R$ be a smooth function. Is there a characterization of the space of functions $f$ whose ratios of higher order derivatives are bounded by at most an exponentially growing constant at infinity? That is, for some $K$ large enough, there is a constant $C > 0$ for which $$ \Bigg\lvert\frac{f^{(n)}{(x)}}{f(x)}\Bigg\rvert \leq C^n, \quad \forall |x| \geq K. $$ (It might be that the correct form to avoid issues with zeroes is $|f^{(n)}(x)| \leq C^n(L + |f(x)|)$ instead, where $L$ is large enough.) I have an intuition the answer is functions which are bounded by an exponential but I do not know how to characterize the space to allow for bad behaviour like bounded oscillations with unbounded derivatives, e.g. $f(x) = e^x + \sin^2(e^x)$ , say. I also expect this space to contain functions of the form $f(x) = P(x)e^{cx}$ where $P$ is a polynomial and $c$ is a constant. For instance, if $f(x) = xe^x$ then $f^{(n)}(x) = (x + n)e^x$ and $|n/x| \leq n/K$ is growing linearly. More generally the $n$ -th derivative of $P(x)e^x$ is $Q_n(x)e^x$ where $Q_n$ is a binomial sum of derivatives of $P$ up to order $n$ , and so there at most $2^n$ terms in $Q_n$ and each of the terms can be bounded uniformly since $|P^{(n)}(x)/P(x)|$ is the ratio of a lower order polynomial to a higher order one for $n \geq 1$ . The motivation for this question is to have a class of functions which are ""essentially exponential"" in growth with respect to all derivatives. I suppose I could always claim this definition above is the characterization but I was wondering if there is a cleaner characterization given the examples seem relatively well behaved and almost commonplace. Edit 1: after some reading, I have a feeling this might be related to Fourier transforms and that it might be possible to prevent bad behaviour by asking for a condition on the Fourier transform (e.g. that it has compact support, say). However I don't really know enough about the Fourier transform to make a tractable characterization. Edit 2: I would be okay if someone can give a characterization of the case $n = 1$ , although I expect this to lead to a characterization for all $n \geq 1$ . There was a deleted answer that showed this is equivalent to boundedness of the log derivative which implies at most exponential growth. But this is not sufficient since one can have exponential growth with a bad derivative coming from a bounded oscillatory term, e.g. $f(x) = e^x + \sin^2(e^{e^x})$ .","Let be a smooth function. Is there a characterization of the space of functions whose ratios of higher order derivatives are bounded by at most an exponentially growing constant at infinity? That is, for some large enough, there is a constant for which (It might be that the correct form to avoid issues with zeroes is instead, where is large enough.) I have an intuition the answer is functions which are bounded by an exponential but I do not know how to characterize the space to allow for bad behaviour like bounded oscillations with unbounded derivatives, e.g. , say. I also expect this space to contain functions of the form where is a polynomial and is a constant. For instance, if then and is growing linearly. More generally the -th derivative of is where is a binomial sum of derivatives of up to order , and so there at most terms in and each of the terms can be bounded uniformly since is the ratio of a lower order polynomial to a higher order one for . The motivation for this question is to have a class of functions which are ""essentially exponential"" in growth with respect to all derivatives. I suppose I could always claim this definition above is the characterization but I was wondering if there is a cleaner characterization given the examples seem relatively well behaved and almost commonplace. Edit 1: after some reading, I have a feeling this might be related to Fourier transforms and that it might be possible to prevent bad behaviour by asking for a condition on the Fourier transform (e.g. that it has compact support, say). However I don't really know enough about the Fourier transform to make a tractable characterization. Edit 2: I would be okay if someone can give a characterization of the case , although I expect this to lead to a characterization for all . There was a deleted answer that showed this is equivalent to boundedness of the log derivative which implies at most exponential growth. But this is not sufficient since one can have exponential growth with a bad derivative coming from a bounded oscillatory term, e.g. .","f: \mathbb R \rightarrow \mathbb R f K C > 0  \Bigg\lvert\frac{f^{(n)}{(x)}}{f(x)}\Bigg\rvert \leq C^n, \quad \forall |x| \geq K.  |f^{(n)}(x)| \leq C^n(L + |f(x)|) L f(x) = e^x + \sin^2(e^x) f(x) = P(x)e^{cx} P c f(x) = xe^x f^{(n)}(x) = (x + n)e^x |n/x| \leq n/K n P(x)e^x Q_n(x)e^x Q_n P n 2^n Q_n |P^{(n)}(x)/P(x)| n \geq 1 n = 1 n \geq 1 f(x) = e^x + \sin^2(e^{e^x})","['real-analysis', 'calculus', 'complex-analysis', 'functional-analysis', 'entire-functions']"
64,Does every connected compact metric space have a unique always attainable average distance?,Does every connected compact metric space have a unique always attainable average distance?,,"Problem statement Let $(X,d)$ be a connected compact metric space. Then does there always exist a unique value $\alpha\geq0$ with the following property? For every $x_1,\ldots,x_k\in X$ there exists $x\in X$ such that the average distance from $x$ to $x_i$ for $i=1,\ldots,k$ is exactly $\alpha$ . Proof of existence The following proves such a value always exists, so the main question is whether it is unique. For $v\in X^n$ , define \begin{align*} f_v(x)&:=\frac{d(x,v_1)+\ldots+d(x,v_n)}n, \\a(v)&:=\min_{x\in X}f_v(x), \\b(v)&:=\max_{x\in X}f_v(x). \end{align*} Because $X$ is compact and $f_v$ is continuous, the image of $f_v$ is also compact and thus indeed has a minimum and a maximum. Let $v\in X^n$ and $w\in X^m$ . We have $$a(w)\leq\frac{f_w(v_1)+\ldots+f_w(v_n)}n=\frac{f_v(w_1)+\ldots+f_v(w_m)}m\leq b(v).$$ It follows that there exists a supremum $\alpha$ of $a$ and an infimum $\beta$ of $b$ , and that we have $\alpha\leq\beta$ . Let $v\in X^n$ , so we have $a(v)\leq\alpha\leq\beta\leq b(v)$ . Since $X$ is connected, the image of $f_v$ is connected, so since it contains $a(v)$ and $b(v)$ , it also contains $[a(v),b(v)]$ . It follows that there exists $x\in X$ such that $f_v(x)=\alpha$ . Note that uniqueness is thus equivalent with stating that $\alpha=\beta$ in the above proof. Looking for counterexamples The most straight forward connected compact metric spaces that come to mind are $[0,1]^n$ . Let $v(t):=(t,\ldots,t)$ . We have $$a(v(0),v(1))=b(v(1/2))=\sqrt{n}/2=\alpha=\beta.$$ Consider a regular $n$ -gon $P_1,\ldots,P_n$ with center $O$ . We have $$a(P_1,\ldots,P_n)=b(O)=|P_1O|=\alpha=\beta.$$ If we have the boundary of a unit square $ABCD$ , let $E,F,G,H$ be the centers of the edges. We have $$a(A,B,C,D)=b(E,F,G,H)=\frac14(1+\sqrt5)=\alpha=\beta.$$ If we have the unit circle, let $v_n$ denote $n$ evenly spaced points. We have $$\lim_{n\to\infty}a(v_n)=\lim_{n\to\infty}b(v_n)=\frac1{2\pi}\int_0^{2\pi}|1-e^{i\theta}|\ \mbox{d}\theta=\frac4\pi=\alpha=\beta,$$ so we can essentially approximate any measure, in this case giving us the average distance between two points on a circle. Consider an arbitrary acute triangle $ABC$ with circumcenter $O$ . Note that $b(O)=|AO|$ . Since we can approximate any measure, we can introduce weights $w_A,w_B,w_C$ . Fixing $w_A$ , we find unique values of $w_B,w_C$ such that $f_{w_AA,w_BB,w_CC}(P)$ has gradient $0$ at $P=O$ , which gives $$a(w_AA,w_BB,w_CC)=b(O)=|AO|=\alpha=\beta.$$ So it seems like $\alpha=\beta$ always holds, but I can not come up with a proof. I even tried some really ugly non-symmetric non-convex sets, and some metric spaces which are not subsets of $\mathbb{R}^n$ , and while it becomes more work each time, I always end up proving $\alpha=\beta$ . All proofs rely on constructing measures $\mu,\nu$ such that $a(\mu)=b(\nu)$ , but I can not figure out the bigger picture of what these measures are in general. I would not even know how to define measures in arbitrary metric spaces to begin with. Edit: A measure theory approach We can phrase the question in terms of probability measures as follows. Let $(X,d)$ be a connected compact metric space with Borel $\sigma$ -algebra $B(X)$ . Then does there always exist a unique value $\alpha\geq0$ with the following property? For every probability measure $P$ on $(X,B(X))$ , there exists $x\in X$ , such that $\mathbb{E}_{y\sim P}(d(x,y))=\alpha$ . The existence proof is completely analogous if we use $f_P(x):=\mathbb{E}_{y\sim P}(d(x,y))$ , for $P$ a probability measure on $(X,B(X))$ , because $f_P$ is still continuous. Indeed, $f_P$ has Lipschitz constant $1$ . Equivalence with the finite average approach follows from discrete probability measures are dense in separable metric spaces . Indeed, since $X$ is compact, it is separable. Optimal probability measures The generalization to arbitrary probability measures allows us to do the following. Since the set of probability measures $P$ on $(X,B(X))$ is sequentially compact, see Theorem 10.2 , there exist probabability measures $P_a$ and $P_b$ on $(X,B(X))$ such that $a(P_a)=\alpha$ and $b(P_b)=\beta$ . Claim: We have $\alpha=\beta$ if, and only if, we have $f_{P_a}(x)=\alpha$ for all $x\in\mbox{supp}(P_b)$ , and vice versa. Proof: For the left implication, since $X$ is strongly Lindelöf, we have $$\alpha=\mathbb{E}_{x\sim P_b}(f_{P_a}(x))=\mathbb{E}_{x\sim P_a}(f_{P_b}(x))=\beta.$$ For the right implication, note that, if $f_{P_a}(x)>\alpha$ for some $x\in\mbox{supp}(P_b)$ , then $$\alpha<\mathbb{E}_{x\sim P_b}(f_{P_a}(x))=\mathbb{E}_{x\sim P_a}(f_{P_b}(x))\leq\beta.$$","Problem statement Let be a connected compact metric space. Then does there always exist a unique value with the following property? For every there exists such that the average distance from to for is exactly . Proof of existence The following proves such a value always exists, so the main question is whether it is unique. For , define Because is compact and is continuous, the image of is also compact and thus indeed has a minimum and a maximum. Let and . We have It follows that there exists a supremum of and an infimum of , and that we have . Let , so we have . Since is connected, the image of is connected, so since it contains and , it also contains . It follows that there exists such that . Note that uniqueness is thus equivalent with stating that in the above proof. Looking for counterexamples The most straight forward connected compact metric spaces that come to mind are . Let . We have Consider a regular -gon with center . We have If we have the boundary of a unit square , let be the centers of the edges. We have If we have the unit circle, let denote evenly spaced points. We have so we can essentially approximate any measure, in this case giving us the average distance between two points on a circle. Consider an arbitrary acute triangle with circumcenter . Note that . Since we can approximate any measure, we can introduce weights . Fixing , we find unique values of such that has gradient at , which gives So it seems like always holds, but I can not come up with a proof. I even tried some really ugly non-symmetric non-convex sets, and some metric spaces which are not subsets of , and while it becomes more work each time, I always end up proving . All proofs rely on constructing measures such that , but I can not figure out the bigger picture of what these measures are in general. I would not even know how to define measures in arbitrary metric spaces to begin with. Edit: A measure theory approach We can phrase the question in terms of probability measures as follows. Let be a connected compact metric space with Borel -algebra . Then does there always exist a unique value with the following property? For every probability measure on , there exists , such that . The existence proof is completely analogous if we use , for a probability measure on , because is still continuous. Indeed, has Lipschitz constant . Equivalence with the finite average approach follows from discrete probability measures are dense in separable metric spaces . Indeed, since is compact, it is separable. Optimal probability measures The generalization to arbitrary probability measures allows us to do the following. Since the set of probability measures on is sequentially compact, see Theorem 10.2 , there exist probabability measures and on such that and . Claim: We have if, and only if, we have for all , and vice versa. Proof: For the left implication, since is strongly Lindelöf, we have For the right implication, note that, if for some , then","(X,d) \alpha\geq0 x_1,\ldots,x_k\in X x\in X x x_i i=1,\ldots,k \alpha v\in X^n \begin{align*}
f_v(x)&:=\frac{d(x,v_1)+\ldots+d(x,v_n)}n,
\\a(v)&:=\min_{x\in X}f_v(x),
\\b(v)&:=\max_{x\in X}f_v(x).
\end{align*} X f_v f_v v\in X^n w\in X^m a(w)\leq\frac{f_w(v_1)+\ldots+f_w(v_n)}n=\frac{f_v(w_1)+\ldots+f_v(w_m)}m\leq b(v). \alpha a \beta b \alpha\leq\beta v\in X^n a(v)\leq\alpha\leq\beta\leq b(v) X f_v a(v) b(v) [a(v),b(v)] x\in X f_v(x)=\alpha \alpha=\beta [0,1]^n v(t):=(t,\ldots,t) a(v(0),v(1))=b(v(1/2))=\sqrt{n}/2=\alpha=\beta. n P_1,\ldots,P_n O a(P_1,\ldots,P_n)=b(O)=|P_1O|=\alpha=\beta. ABCD E,F,G,H a(A,B,C,D)=b(E,F,G,H)=\frac14(1+\sqrt5)=\alpha=\beta. v_n n \lim_{n\to\infty}a(v_n)=\lim_{n\to\infty}b(v_n)=\frac1{2\pi}\int_0^{2\pi}|1-e^{i\theta}|\ \mbox{d}\theta=\frac4\pi=\alpha=\beta, ABC O b(O)=|AO| w_A,w_B,w_C w_A w_B,w_C f_{w_AA,w_BB,w_CC}(P) 0 P=O a(w_AA,w_BB,w_CC)=b(O)=|AO|=\alpha=\beta. \alpha=\beta \mathbb{R}^n \alpha=\beta \mu,\nu a(\mu)=b(\nu) (X,d) \sigma B(X) \alpha\geq0 P (X,B(X)) x\in X \mathbb{E}_{y\sim P}(d(x,y))=\alpha f_P(x):=\mathbb{E}_{y\sim P}(d(x,y)) P (X,B(X)) f_P f_P 1 X P (X,B(X)) P_a P_b (X,B(X)) a(P_a)=\alpha b(P_b)=\beta \alpha=\beta f_{P_a}(x)=\alpha x\in\mbox{supp}(P_b) X \alpha=\mathbb{E}_{x\sim P_b}(f_{P_a}(x))=\mathbb{E}_{x\sim P_a}(f_{P_b}(x))=\beta. f_{P_a}(x)>\alpha x\in\mbox{supp}(P_b) \alpha<\mathbb{E}_{x\sim P_b}(f_{P_a}(x))=\mathbb{E}_{x\sim P_a}(f_{P_b}(x))\leq\beta.","['real-analysis', 'probability-theory', 'measure-theory', 'metric-spaces', 'average']"
65,"Given $f \in L^2(\mathbb{R})$, show that $\operatorname{span}\{f(x-n)\}_{n \in \mathbb{Z}}$ is not dense in $L^2(\mathbb{R})$?","Given , show that  is not dense in ?",f \in L^2(\mathbb{R}) \operatorname{span}\{f(x-n)\}_{n \in \mathbb{Z}} L^2(\mathbb{R}),"Given any $f \in L^2(\mathbb{R})$ , show that the span of $\{f(x-n)\}_{n \in \mathbb{Z}}$ is not dense in $L^2(\mathbb{R})$ ? This problem comes from this presentation with slides here . It is claimed under the ""Remark"" section that this set cannot be dense. They actually deal with a more general case but I choose $p=2$ for simplicity. Since $L^2(\mathbb{R})$ is a Hilbert space, then the sequence above being dense in the space is equivalent to the following condition: If we have $g \in L^2(\mathbb{R})$ with $(g,f(x-n)) = 0$ (inner-product) for every $n \in \mathbb{Z}$ , then $g = 0$ (in the $L^2(\mathbb{R})$ - sense). The idea is to find a nonzero $g \in L^2(\mathbb{R})$ that satisfies the hypothesis above. However, I am unable to show this. Is there a simple function that gets the job done? Thanks :)!","Given any , show that the span of is not dense in ? This problem comes from this presentation with slides here . It is claimed under the ""Remark"" section that this set cannot be dense. They actually deal with a more general case but I choose for simplicity. Since is a Hilbert space, then the sequence above being dense in the space is equivalent to the following condition: If we have with (inner-product) for every , then (in the - sense). The idea is to find a nonzero that satisfies the hypothesis above. However, I am unable to show this. Is there a simple function that gets the job done? Thanks :)!","f \in L^2(\mathbb{R}) \{f(x-n)\}_{n \in \mathbb{Z}} L^2(\mathbb{R}) p=2 L^2(\mathbb{R}) g \in L^2(\mathbb{R}) (g,f(x-n)) = 0 n \in \mathbb{Z} g = 0 L^2(\mathbb{R}) g \in L^2(\mathbb{R})","['real-analysis', 'sequences-and-series', 'fourier-analysis', 'hilbert-spaces']"
66,Show $\lim\limits_{t\to\infty}\Bigg|\sum\limits_{n=0}^\infty\Theta(t-nR)\frac{(\Gamma(t-nR))^n}{n!}e^{-\Gamma(t-nR)}\Bigg|^2=\frac1{(1+\Gamma R)^2}$,Show,\lim\limits_{t\to\infty}\Bigg|\sum\limits_{n=0}^\infty\Theta(t-nR)\frac{(\Gamma(t-nR))^n}{n!}e^{-\Gamma(t-nR)}\Bigg|^2=\frac1{(1+\Gamma R)^2},"I have encountered the following problem while studying non-Markovian effects in real-time dynamics of open quantum systems. In particular, I was studying a system comprised of two qubits (qubit is a standard shorthand for two level quantum system) separated by distance (in configuration space e.g. a laboratory) $R$ from one another and coupled to a one-dimensional Bosonic reservoir hosting a pair of boson species corresponding to right and left moving photon fields with linear dispersion relation and fixed propagation speed (equal to $1$ in my units (Planck's constant is set to be $2\pi$ )). It is really one of the simplest system one can imagine which possesses the so-called delayed coherent quantum feedback, a property that dynamics of the quantum system at any time $T$ depends on the entire history of its dynamics for all times $t\in[0, T]$ in deterministic (i.e. classical, not quantum, and thus very human-being controllable) fashion. You can easily guess that without any knowledge of physics whatsoever, you have quantum information that propagates with finite velocity $v=1$ over given distance $R$ , so there is an intrinsic time delay $\tau=vR$ in the system and due to Lorentz covariance this cannot be altered by any quantum effects. The Hamiltonian operator of such a system can be written as $H=H_{0}+V$ ,  where \begin{align}H_{0}&=\sum_{n=1, 2}\frac{\Delta_{n}}{2}\sigma_{3}^{(n)}+\sum_{\mu=1, 2}\int_{-\infty}^{\infty}dk\omega_{\mu}(k)a^{\dagger}_{\mu}(k)a_{\mu}(k),\\V&=\sum_{n=1, 2}\sum_{\mu=1, 2}\int_{-\infty}^{\infty}dkg_{\mu, n}(k)a^{\dagger}_{\mu}(k)\sigma_{+}^{(n)}+g_{\mu, n}^{*}(k)\sigma_{-}^{(n)}a_{\mu}(k).\end{align} Here $\Delta_{n}$ is the detuning of the qubit number $n$ from some reference energy $k_{0}$ , i.e. $\Delta_{n}=\Omega_{n}-k_{0}$ , where $\Omega_{n}$ is the transition frequency of the qubit number $n$ and $k_{0}$ is the momentum around which the spectrum of the bath of boson fields was linearised (we of course expect photons of energy ( $hvk_{0}/(2\pi)=k_{0}$ ) $k_{0}$ to couple most strongly to some system with transition frequency $\min[\Omega_{1}, \ \Omega_{2}]$ ). Further, $\omega_{\mu}(k)=hvk/(2\pi)=k$ is the energy of the photon of flavour $\mu$ and momentum $k$ . Such a photon is created by operator $a^{\dagger}_{\mu}(k)$ and destroyed by $a_{\mu}(k)$ , these obey non-zero commutation $[a_{\mu}(k), a^{\dagger}_{\mu'}(k')]=\delta(k-k')\delta_{\mu, \mu'}$ . $\sigma_{j}^{(1)}=\sigma_{j}\otimes\sigma_{0}, \ \sigma_{j}^{(2)}=\sigma_{0}\otimes\sigma_{j}$ , where $\sigma_{\pm}=(\sigma_{1}+i\sigma_{2})/2$ , and $\sigma_{1,2, 3}$ are usual Pauli-matrices ( $\sigma_{0}$ is an identity on $\mathbb{C}^{2}$ ). The coupling constants are defined as $$g_{\mu, n}(k)=\sqrt{\frac{\Gamma_{n}}{2\pi}}e^{-ic_{\mu}c_{n}(k_{0}+k)R/2},$$ where $\Gamma_{n}$ is the bare decay rate of a single qubit into the continuum, and $c_{s}=(-)^{s+1}, \ s=\mu, \ n$ distinguishes the coupling to right/left (with index $\mu$ ) photons of an atom number $n=1, 2$ , located at $\pm R/2$ . As a mock up problem I consider the following. Consider the system described above Hamiltonian prepared at time $t_{0}=0$ in the following 2-parameter family of states $$|\psi(0)\rangle=(\cos\vartheta|1\rangle\otimes|0\rangle+e^{i\varphi/2}\sin\vartheta|0\rangle\otimes|1\rangle)\otimes|\Omega\rangle.$$ Here the quantum states are ordered as qubit 1, qubit 2, bath, i.e. $|a\rangle\otimes|b\rangle\otimes|c\rangle$ = qubit one is in state $a$ , qubit $2$ is in $b$ and bosons are in $c$ . Here $|\Omega\rangle$ is the ""vacuum"" state of bosons defined by $a_{\mu}(k)|\Omega\rangle=0,\forall \mu=1,2 , \ k\in\mathbb{R}$ . In physics you'll call such a setup a spontaneous emission problem. I was able to deduce with the help of diagrammatic techniques (see our recent paper https://arxiv.org/pdf/2101.07603.pdf related to the $T\rightarrow\infty$ limit of non-Markovian systems) that the exact survival probability amplitude for an initial state defined above has the form (this result is exact in all parameter regime iff $\Theta(t)$ the Heaviside step function is defined equal to $1$ at $t=0$ , it is indeed ambiguous since we use use both Hille–Yosida theorem and Sokhotski-Plemelj theorem which are good until $T=t_{0}=0$ where divergencies happen, then you have to maintain the order of limits with some care) $$P(t)=|a(t)|^{2},\quad a(t)=\oint_{C_{+\eta}}\langle{\psi(0)|G(z)|\psi(0)\rangle}e^{-izt}\frac{dz}{2\pi i}, \ t>0.$$ Here the integration contour $C_{+\eta}$ is a Bromiwich style contour suspending itself above the real axis with positive imaginary part $\eta\searrow0$ . $G(z)$ is the operator valued function known in physics as the retarded Green's function. One thus sees that all important information about dynamics is contained in the poles and branch cuts of $G(z)$ . The projection of the retarded Green's function onto the single atomic excitation subspace can be determined analytically in the exact form: $$ G^{(1)}(z)=\frac{1}{1+g_{1}(z)g_{2}(z)\Gamma_{1}\Gamma_{2}e^{2i(z+k_{0})R}}(g_{1}(zc|10\rangle\langle{10}|+g_{2}(z)|01\rangle\langle{01}|$$ $$-ig_{1}(z)g_{2}(z)\sqrt{\Gamma_{1}\Gamma_{2}}e^{i(z+k_{0})R}(\sigma_{+}^{(1)}\sigma_{-}^{(2)}+\sigma_{-}^{(2)}\sigma_{+}^{(1)})$$ $$=G_{e1}(z)|10\rangle\langle{10}|+G_{e2}(z)|01\rangle\langle{01}|+G_{o}(\sigma_{+}^{(1)}\sigma_{-}^{(2)}+\sigma_{-}^{(2)}\sigma_{+}^{(1)}), $$ where \begin{align} G_{ej}(z)=\frac{g_{j}(z)}{1+g_{1}(z)g_{2}(z)\Gamma_{1}\Gamma_{2}e^{2i(z+k_{0})R}}, \quad G_{o}(z)=\frac{-ig_{1}(z)g_{2}(z)\sqrt{\Gamma_{1}\Gamma_{2}}e^{i(z+k_{0})R}}{1+g_{1}(z)g_{2}(z)\Gamma_{1}\Gamma_{2}e^{2i(z+k_{0})R}}, \end{align} and $g_{j}(z)=(z-\Delta_{j}+i\Gamma_{j})^{-1}$ are the single-quit green's functions (note that their Laplace transform is trivial $e^{-i\Delta_{j}t}e^{-\Gamma_{j}t}$ due to locality (like you've learned in high school-an exponential decay)). The complete answer is a bit involved but can be clearly expressed in terms of Fourier images of $G_{ej}(z), \ G_{o}(z)$ , these are \begin{align} G_{e1}(t)&=\oint_{C_{+\eta}}\frac{dz}{2\pi i}\frac{g_{1}(z)}{1+g_{1}(z)g_{2}(z)\Gamma_{1}\Gamma_{2}e^{2i(z+k_{0})R}}e^{-izt}\\ &=e^{-i\Delta_{1}t}e^{-\Gamma_{1}t}+\sum_{m=1}^{\infty}(-\Gamma_{1}\Gamma_{2}e^{2ik_{0}R})^{m}I(2m, m+1, m, t),\\ G_{e2}(t)&=\oint_{C_{+\eta}}\frac{dz}{2\pi i}\frac{g_{2}(z)}{1+g_{1}(z)g_{2}(z)\Gamma_{1}\Gamma_{2}e^{2i(z+k_{0})R}}e^{-izt}\\ &=e^{-i\Delta_{2}t}e^{-\Gamma_{2}t}+\sum_{m=1}^{\infty}(-\Gamma_{1}\Gamma_{2}e^{2ik_{0}R})^{m}I(2m, m, m+1, t),\\ G_{o}(t)&=\oint_{C_{+\eta}}\frac{dz}{2\pi i}\frac{-ig_{1}(z)g_{2}(z)\sqrt{\Gamma_{1}\Gamma_{2}}e^{i(z+k_{0})R}}{1+g_{1}(z)g_{2}(z)\Gamma_{1}\Gamma_{2}e^{2i(z+k_{0})R}}e^{-izt}\\ =&-i\sqrt{\Gamma_{1}\Gamma_{2}}e^{ik_{0}R}\sum_{m=0}^{\infty}(-\Gamma_{1}\Gamma_{2}e^{2ik_{0}R})^{m}I(2m+1, m+1, m+1, t) \end{align} where \begin{align} &I(a, b, c, t)=\oint_{C_{+\eta}}\frac{dz}{2\pi i}e^{-iz(t-aR)}g_{1}^{b}(z)g_{2}^{c}(z) =\int_{C_{+}}\frac{dz}{2\pi{i}}\frac{e^{-iz(t-aR)}}{(z-\Delta_{1}+i\Gamma_{1})^{b}(z-\Delta_{2}+i\Gamma_{2})^{c}}\\ &=\Theta(t-aR)\Bigg(\sum_{k=0}^{b-1}\frac{(-1)^{k}(c+k-1)!(-i(t-aR))^{b-k-1}}{k!(b-k-1)!(c-1)!}\frac{e^{-i\Delta_{1}(t-aR)}e^{-\Gamma_{1}(t-aR)}}{(\Delta_{2}-\Delta_{1}+i(\Gamma_{2}-\Gamma_{1}))^{c+k}}\\ &+\sum_{k=0}^{c-1}\frac{(-1)^{k}(b+k-1)!(-i(t-aR))^{c-k-1}}{k!(c-k-1)!(b-1)!}\frac{e^{-i\Delta_{2}(t-aR)}e^{-\Gamma_{2}(t-aR)}}{(\Delta_{1}-\Delta_{2}+i(\Gamma_{1}-\Gamma_{2}))^{b+k}}\Bigg). \end{align} So far so good. These functions are looking physically correct, etc. You can see an infinite number of revival peaks in survival probability which happen on all integer multiples of delay time $vR$ , i.e. when the photon emitted by one atom reaches the other, isn't it cool you can control what atom does by just moving it around?=) Many physicists though are recently discussing the possibility of so called DARK states, the states for which $p(t)=1, \ \forall t>0$ . Their argument is based on Markov approximation though, it is the limit where $\max{\Gamma_{n}}_{n=1, \ 2}\times vR\ll1$ . The first assumption to be done is that the quits are equivalent and ""bright"" (no detuning) $\Delta_{1}=\Delta_{2}=0, \ \Gamma_{n}=\Gamma$ . The second is $\theta=\pi/4, \mod 2\pi, \varphi=4\pi, \mod 2\pi$ for bright and $\theta=\pi/4, \mod 2\pi, \varphi=2\pi, \mod 2\pi$ for dark states. My analysis shows \begin{align} \label{eq: DE58} p_{B}(t)=&\Bigg{|}G_{e}(t)+G_{o}(t)\Bigg{|}^{2}=\Bigg{|}\sum_{n=0}^{\infty}\Theta(t-nR)\frac{(-\Gamma(t-nR))^{n}}{n!}e^{-\Gamma(t-nR)}\Bigg{|}^{2},\\ \label{eq: DE59} p_{D}(t)=&\Bigg{|}G_{e}(t)-G_{o}(t)\Bigg{|}^{2}=\Bigg{|}\sum_{n=0}^{\infty}\Theta(t-nR)\frac{(\Gamma(t-nR))^{n}}{n!}e^{-\Gamma(t-nR)}\Bigg{|}^{2}. \end{align} I.e. for the dark state we obtain $p_{D}(t)=1, \forall t>0$ at $R=0$ . For $R\neq0$ when $t\rightarrow \infty$ the sereies converges to a finite value $$p(t)\rightarrow\text{constant}.$$ By numerical exercise I'd like to claim that the $t$ infinity limit of $p_{D}(t)$ is precisely $1/(1+\Gamma R)^{2}$ . Can you help me to prove or disprove that? And if you happen to know the closed form the above series do not hesitate to share. By the long-time limit I understand you take above series with all $\Theta$ equal to $1$ (do not bother about how rigorous this is please). To be specific: if you happen to know something about series like $x^{n}(c-nx)^n/n!$ give us a clue. Another clue from numerics: at super large times $\Gamma t\gg1$ we deduce that the quantum decay of bright state follows a sub-exponential power law $1/t$ in the long time regime. Numerics show the $R/(2\pi t)$ law, see picture.","I have encountered the following problem while studying non-Markovian effects in real-time dynamics of open quantum systems. In particular, I was studying a system comprised of two qubits (qubit is a standard shorthand for two level quantum system) separated by distance (in configuration space e.g. a laboratory) from one another and coupled to a one-dimensional Bosonic reservoir hosting a pair of boson species corresponding to right and left moving photon fields with linear dispersion relation and fixed propagation speed (equal to in my units (Planck's constant is set to be )). It is really one of the simplest system one can imagine which possesses the so-called delayed coherent quantum feedback, a property that dynamics of the quantum system at any time depends on the entire history of its dynamics for all times in deterministic (i.e. classical, not quantum, and thus very human-being controllable) fashion. You can easily guess that without any knowledge of physics whatsoever, you have quantum information that propagates with finite velocity over given distance , so there is an intrinsic time delay in the system and due to Lorentz covariance this cannot be altered by any quantum effects. The Hamiltonian operator of such a system can be written as ,  where Here is the detuning of the qubit number from some reference energy , i.e. , where is the transition frequency of the qubit number and is the momentum around which the spectrum of the bath of boson fields was linearised (we of course expect photons of energy ( ) to couple most strongly to some system with transition frequency ). Further, is the energy of the photon of flavour and momentum . Such a photon is created by operator and destroyed by , these obey non-zero commutation . , where , and are usual Pauli-matrices ( is an identity on ). The coupling constants are defined as where is the bare decay rate of a single qubit into the continuum, and distinguishes the coupling to right/left (with index ) photons of an atom number , located at . As a mock up problem I consider the following. Consider the system described above Hamiltonian prepared at time in the following 2-parameter family of states Here the quantum states are ordered as qubit 1, qubit 2, bath, i.e. = qubit one is in state , qubit is in and bosons are in . Here is the ""vacuum"" state of bosons defined by . In physics you'll call such a setup a spontaneous emission problem. I was able to deduce with the help of diagrammatic techniques (see our recent paper https://arxiv.org/pdf/2101.07603.pdf related to the limit of non-Markovian systems) that the exact survival probability amplitude for an initial state defined above has the form (this result is exact in all parameter regime iff the Heaviside step function is defined equal to at , it is indeed ambiguous since we use use both Hille–Yosida theorem and Sokhotski-Plemelj theorem which are good until where divergencies happen, then you have to maintain the order of limits with some care) Here the integration contour is a Bromiwich style contour suspending itself above the real axis with positive imaginary part . is the operator valued function known in physics as the retarded Green's function. One thus sees that all important information about dynamics is contained in the poles and branch cuts of . The projection of the retarded Green's function onto the single atomic excitation subspace can be determined analytically in the exact form: where and are the single-quit green's functions (note that their Laplace transform is trivial due to locality (like you've learned in high school-an exponential decay)). The complete answer is a bit involved but can be clearly expressed in terms of Fourier images of , these are where So far so good. These functions are looking physically correct, etc. You can see an infinite number of revival peaks in survival probability which happen on all integer multiples of delay time , i.e. when the photon emitted by one atom reaches the other, isn't it cool you can control what atom does by just moving it around?=) Many physicists though are recently discussing the possibility of so called DARK states, the states for which . Their argument is based on Markov approximation though, it is the limit where . The first assumption to be done is that the quits are equivalent and ""bright"" (no detuning) . The second is for bright and for dark states. My analysis shows I.e. for the dark state we obtain at . For when the sereies converges to a finite value By numerical exercise I'd like to claim that the infinity limit of is precisely . Can you help me to prove or disprove that? And if you happen to know the closed form the above series do not hesitate to share. By the long-time limit I understand you take above series with all equal to (do not bother about how rigorous this is please). To be specific: if you happen to know something about series like give us a clue. Another clue from numerics: at super large times we deduce that the quantum decay of bright state follows a sub-exponential power law in the long time regime. Numerics show the law, see picture.","R 1 2\pi T t\in[0, T] v=1 R \tau=vR H=H_{0}+V \begin{align}H_{0}&=\sum_{n=1, 2}\frac{\Delta_{n}}{2}\sigma_{3}^{(n)}+\sum_{\mu=1, 2}\int_{-\infty}^{\infty}dk\omega_{\mu}(k)a^{\dagger}_{\mu}(k)a_{\mu}(k),\\V&=\sum_{n=1, 2}\sum_{\mu=1, 2}\int_{-\infty}^{\infty}dkg_{\mu, n}(k)a^{\dagger}_{\mu}(k)\sigma_{+}^{(n)}+g_{\mu, n}^{*}(k)\sigma_{-}^{(n)}a_{\mu}(k).\end{align} \Delta_{n} n k_{0} \Delta_{n}=\Omega_{n}-k_{0} \Omega_{n} n k_{0} hvk_{0}/(2\pi)=k_{0} k_{0} \min[\Omega_{1}, \ \Omega_{2}] \omega_{\mu}(k)=hvk/(2\pi)=k \mu k a^{\dagger}_{\mu}(k) a_{\mu}(k) [a_{\mu}(k), a^{\dagger}_{\mu'}(k')]=\delta(k-k')\delta_{\mu, \mu'} \sigma_{j}^{(1)}=\sigma_{j}\otimes\sigma_{0}, \ \sigma_{j}^{(2)}=\sigma_{0}\otimes\sigma_{j} \sigma_{\pm}=(\sigma_{1}+i\sigma_{2})/2 \sigma_{1,2, 3} \sigma_{0} \mathbb{C}^{2} g_{\mu, n}(k)=\sqrt{\frac{\Gamma_{n}}{2\pi}}e^{-ic_{\mu}c_{n}(k_{0}+k)R/2}, \Gamma_{n} c_{s}=(-)^{s+1}, \ s=\mu, \ n \mu n=1, 2 \pm R/2 t_{0}=0 |\psi(0)\rangle=(\cos\vartheta|1\rangle\otimes|0\rangle+e^{i\varphi/2}\sin\vartheta|0\rangle\otimes|1\rangle)\otimes|\Omega\rangle. |a\rangle\otimes|b\rangle\otimes|c\rangle a 2 b c |\Omega\rangle a_{\mu}(k)|\Omega\rangle=0,\forall \mu=1,2 , \ k\in\mathbb{R} T\rightarrow\infty \Theta(t) 1 t=0 T=t_{0}=0 P(t)=|a(t)|^{2},\quad a(t)=\oint_{C_{+\eta}}\langle{\psi(0)|G(z)|\psi(0)\rangle}e^{-izt}\frac{dz}{2\pi i}, \ t>0. C_{+\eta} \eta\searrow0 G(z) G(z) 
G^{(1)}(z)=\frac{1}{1+g_{1}(z)g_{2}(z)\Gamma_{1}\Gamma_{2}e^{2i(z+k_{0})R}}(g_{1}(zc|10\rangle\langle{10}|+g_{2}(z)|01\rangle\langle{01}| -ig_{1}(z)g_{2}(z)\sqrt{\Gamma_{1}\Gamma_{2}}e^{i(z+k_{0})R}(\sigma_{+}^{(1)}\sigma_{-}^{(2)}+\sigma_{-}^{(2)}\sigma_{+}^{(1)}) =G_{e1}(z)|10\rangle\langle{10}|+G_{e2}(z)|01\rangle\langle{01}|+G_{o}(\sigma_{+}^{(1)}\sigma_{-}^{(2)}+\sigma_{-}^{(2)}\sigma_{+}^{(1)}),
 \begin{align}
G_{ej}(z)=\frac{g_{j}(z)}{1+g_{1}(z)g_{2}(z)\Gamma_{1}\Gamma_{2}e^{2i(z+k_{0})R}}, \quad G_{o}(z)=\frac{-ig_{1}(z)g_{2}(z)\sqrt{\Gamma_{1}\Gamma_{2}}e^{i(z+k_{0})R}}{1+g_{1}(z)g_{2}(z)\Gamma_{1}\Gamma_{2}e^{2i(z+k_{0})R}},
\end{align} g_{j}(z)=(z-\Delta_{j}+i\Gamma_{j})^{-1} e^{-i\Delta_{j}t}e^{-\Gamma_{j}t} G_{ej}(z), \ G_{o}(z) \begin{align}
G_{e1}(t)&=\oint_{C_{+\eta}}\frac{dz}{2\pi i}\frac{g_{1}(z)}{1+g_{1}(z)g_{2}(z)\Gamma_{1}\Gamma_{2}e^{2i(z+k_{0})R}}e^{-izt}\\
&=e^{-i\Delta_{1}t}e^{-\Gamma_{1}t}+\sum_{m=1}^{\infty}(-\Gamma_{1}\Gamma_{2}e^{2ik_{0}R})^{m}I(2m, m+1, m, t),\\
G_{e2}(t)&=\oint_{C_{+\eta}}\frac{dz}{2\pi i}\frac{g_{2}(z)}{1+g_{1}(z)g_{2}(z)\Gamma_{1}\Gamma_{2}e^{2i(z+k_{0})R}}e^{-izt}\\
&=e^{-i\Delta_{2}t}e^{-\Gamma_{2}t}+\sum_{m=1}^{\infty}(-\Gamma_{1}\Gamma_{2}e^{2ik_{0}R})^{m}I(2m, m, m+1, t),\\
G_{o}(t)&=\oint_{C_{+\eta}}\frac{dz}{2\pi i}\frac{-ig_{1}(z)g_{2}(z)\sqrt{\Gamma_{1}\Gamma_{2}}e^{i(z+k_{0})R}}{1+g_{1}(z)g_{2}(z)\Gamma_{1}\Gamma_{2}e^{2i(z+k_{0})R}}e^{-izt}\\
=&-i\sqrt{\Gamma_{1}\Gamma_{2}}e^{ik_{0}R}\sum_{m=0}^{\infty}(-\Gamma_{1}\Gamma_{2}e^{2ik_{0}R})^{m}I(2m+1, m+1, m+1, t)
\end{align} \begin{align}
&I(a, b, c, t)=\oint_{C_{+\eta}}\frac{dz}{2\pi i}e^{-iz(t-aR)}g_{1}^{b}(z)g_{2}^{c}(z)
=\int_{C_{+}}\frac{dz}{2\pi{i}}\frac{e^{-iz(t-aR)}}{(z-\Delta_{1}+i\Gamma_{1})^{b}(z-\Delta_{2}+i\Gamma_{2})^{c}}\\
&=\Theta(t-aR)\Bigg(\sum_{k=0}^{b-1}\frac{(-1)^{k}(c+k-1)!(-i(t-aR))^{b-k-1}}{k!(b-k-1)!(c-1)!}\frac{e^{-i\Delta_{1}(t-aR)}e^{-\Gamma_{1}(t-aR)}}{(\Delta_{2}-\Delta_{1}+i(\Gamma_{2}-\Gamma_{1}))^{c+k}}\\
&+\sum_{k=0}^{c-1}\frac{(-1)^{k}(b+k-1)!(-i(t-aR))^{c-k-1}}{k!(c-k-1)!(b-1)!}\frac{e^{-i\Delta_{2}(t-aR)}e^{-\Gamma_{2}(t-aR)}}{(\Delta_{1}-\Delta_{2}+i(\Gamma_{1}-\Gamma_{2}))^{b+k}}\Bigg).
\end{align} vR p(t)=1, \ \forall t>0 \max{\Gamma_{n}}_{n=1, \ 2}\times vR\ll1 \Delta_{1}=\Delta_{2}=0, \ \Gamma_{n}=\Gamma \theta=\pi/4, \mod 2\pi, \varphi=4\pi, \mod 2\pi \theta=\pi/4, \mod 2\pi, \varphi=2\pi, \mod 2\pi \begin{align}
\label{eq: DE58}
p_{B}(t)=&\Bigg{|}G_{e}(t)+G_{o}(t)\Bigg{|}^{2}=\Bigg{|}\sum_{n=0}^{\infty}\Theta(t-nR)\frac{(-\Gamma(t-nR))^{n}}{n!}e^{-\Gamma(t-nR)}\Bigg{|}^{2},\\
\label{eq: DE59}
p_{D}(t)=&\Bigg{|}G_{e}(t)-G_{o}(t)\Bigg{|}^{2}=\Bigg{|}\sum_{n=0}^{\infty}\Theta(t-nR)\frac{(\Gamma(t-nR))^{n}}{n!}e^{-\Gamma(t-nR)}\Bigg{|}^{2}.
\end{align} p_{D}(t)=1, \forall t>0 R=0 R\neq0 t\rightarrow \infty p(t)\rightarrow\text{constant}. t p_{D}(t) 1/(1+\Gamma R)^{2} \Theta 1 x^{n}(c-nx)^n/n! \Gamma t\gg1 1/t R/(2\pi t)","['real-analysis', 'sequences-and-series', 'mathematical-physics', 'quantum-field-theory']"
67,On the 'wrong proof' of the chain rule,On the 'wrong proof' of the chain rule,,"I am looking through an old analysis course that I had and I was pondering a bit about the proof of chain rule (especially the notorious wrong proof that you can give). I'd be happy if someone was willing to verify my reasoning below. I end with an actual question. Let's start with the following nice result. Let $f\colon \mathbb{R}\to \mathbb{R}$ be a continuous function which is differentiable on $\mathbb{R}_0$ . Assume that $\lim_{x\to 0}f'(x)=L\in \mathbb{R}$ . Then $f$ is differentiable in $0$ . Proof: For each $h\neq 0$ , the mean value theorem yields a $c_h\in \mathbb{R}$ strictly between $h$ and $0$ such that $f'(c_h)=\frac{f(h)-f(0)}{h}$ . Letting $h\to 0$ , is it is obvious that $c_h\to 0$ as each $|c_h|<|h|$ . Hence $$\lim_{h\to 0}\frac{f(h)-f(0)}{h}=\lim_{h\to 0}f'(c_h)=L.$$ $\square$ Great, let's apply this to the following function: $$\phi\colon \mathbb{R}\to \mathbb{R}:x\mapsto \begin{cases}x^3\sin(\frac{1}{x}) & \mbox{ if }x\neq 0,\\0 & \mbox{ if } x=0.\end{cases}$$ Clearly $\phi$ is differentiable on $\mathbb{R}_0$ and $$\phi'(x)=3x^2\sin(\frac{1}{x})-x^3\cos(\frac{1}{x})\frac{1}{x^2}=3x^2\sin(\frac{1}{x})-x\cos(\frac{1}{x})$$ for all $x\neq 0$ . It is straightforward to see that $\lim_{x\to 0}\phi'(x)=0$ and thus the above result yields that $\phi'(0)=0$ (in particular $\phi$ is differentiable on the whole of $\mathbb{R}$ ). Now at this point, recall the chain rule. Let $f,g\colon \mathbb{R}\to \mathbb{R}$ be functions. If $a\in\mathbb{R}$ such that $f'(a)$ and $g'(f(a))$ both exist, then $(g\circ f)'(a)=g'(f(a))f'(a)$ . The obvious argument to try is the following 'wrong proof': \begin{eqnarray} \lim_{x\to a}\frac{g\circ f(x)-g\circ f(a)}{x-a} &=& \lim_{x\to a}\frac{g\circ f(x)-g\circ f(a)}{f(x)-f(a)}\cdot \frac{f(x)-f(a)}{x-a}\\ &=& \lim_{x\to a}\frac{g\circ f(x)-g\circ f(a)}{f(x)-f(a)}\cdot \lim_{x\to a}\frac{f(x)-f(a)}{x-a}\\ &=& g'(f(a))f'(a). \end{eqnarray} Here we used that $f$ is continuous in $a$ to see that $f(x)\to f(a)$ as $x\to a$ . $\triangle$ However, there is an obvious error in the above reasoning. If for example $f$ is a constant function $f(x)=f(a)$ for all $x\in \mathbb{R}$ , then $\lim_{x\to a}\frac{g\circ f(x)-g\circ f(a)}{f(x)-f(a)}=\lim_{x\to a}\frac{g\circ f(x)-g\circ f(a)}{0}$ is nonsensical! Having said that, it is also clear that the above proof does work for functions such that $\exists \delta>0:\forall x\in (a-\delta,a+\delta)\setminus \{a\}:f(x)\neq f(a)$ . In that case, $f(x)$ does not equal $f(a)$ for $x$ near $a$ (and $x\neq a$ ). So the above proof only fails for a particular type of function, the easiest of which are constant functions. However, for a constant function $f$ , one can calculate $(g\circ f')(a)$ directly and show that it's $0$ . A natural question at this point is to wonder whether there exists a nonconstant function $f$ such that $f$ is differentiable in $a$ and $f(x)=f(a)$ infinitely often for $x$ near $a$ . The answer is yes and the function $\phi$ given in the example above (with $a=0$ ) satisfies these properties. (Also, the wikipedia page of the chain rule gives the function $f(x)=x^2\sin(\frac{1}{x})$ for $x\neq 0$ and $f(0)=0$ as an example, but this function is not differentiable in $0$ . As far as I can tell, this is a worse example than just a constant function to pinpoint the failure of the 'wrong proof'. Perhaps this should be changed?) In general let $f$ be such a function (thus $\forall \delta>0:\exists x\neq a: |x-a|<\delta$ and $f(x)=f(a)$ ). If $\lim_{x\to a}\frac{g\circ f(x)-g\circ f(a)}{x-a}$ exists , then we can compute this limit by choosing an appropriate sequence $x_n\to a$ . For each $n\geq 1$ , there exists an $x_n\neq a$ such that $|x_n-a|<\frac{1}{n}$ and $f(x_n)=f(a)$ . It follows that \begin{eqnarray} \lim_{x\to a}\frac{g\circ f(x)-g\circ f(a)}{x-a}&=&\lim_{n\to \infty}\frac{g\circ f(x_n)-g\circ f(a)}{x_n-a}\\ &=& \lim_{n\to \infty}\frac{g\circ f(a)-g\circ f(a)}{x-a}\\ &=& 0. \end{eqnarray} This shows that if $f$ is a function for which the 'wrong proof' of the chain rule fails, then $(g\circ f)'(a)=0$ . Off course, I was only able to show this under the assumption that $(g\circ f)'(a)$ actually exists (which off course is true as one can actually prove the chain rule). Nonetheless, this begs the question whether there is a more direct way of showing that $(g\circ f)'(a)$ actually exists (and equals zero) if $f$ is a function for which the 'wrong proof' fails. If so, one can actually fix this 'wrong proof' by considering two cases.","I am looking through an old analysis course that I had and I was pondering a bit about the proof of chain rule (especially the notorious wrong proof that you can give). I'd be happy if someone was willing to verify my reasoning below. I end with an actual question. Let's start with the following nice result. Let be a continuous function which is differentiable on . Assume that . Then is differentiable in . Proof: For each , the mean value theorem yields a strictly between and such that . Letting , is it is obvious that as each . Hence Great, let's apply this to the following function: Clearly is differentiable on and for all . It is straightforward to see that and thus the above result yields that (in particular is differentiable on the whole of ). Now at this point, recall the chain rule. Let be functions. If such that and both exist, then . The obvious argument to try is the following 'wrong proof': Here we used that is continuous in to see that as . However, there is an obvious error in the above reasoning. If for example is a constant function for all , then is nonsensical! Having said that, it is also clear that the above proof does work for functions such that . In that case, does not equal for near (and ). So the above proof only fails for a particular type of function, the easiest of which are constant functions. However, for a constant function , one can calculate directly and show that it's . A natural question at this point is to wonder whether there exists a nonconstant function such that is differentiable in and infinitely often for near . The answer is yes and the function given in the example above (with ) satisfies these properties. (Also, the wikipedia page of the chain rule gives the function for and as an example, but this function is not differentiable in . As far as I can tell, this is a worse example than just a constant function to pinpoint the failure of the 'wrong proof'. Perhaps this should be changed?) In general let be such a function (thus and ). If exists , then we can compute this limit by choosing an appropriate sequence . For each , there exists an such that and . It follows that This shows that if is a function for which the 'wrong proof' of the chain rule fails, then . Off course, I was only able to show this under the assumption that actually exists (which off course is true as one can actually prove the chain rule). Nonetheless, this begs the question whether there is a more direct way of showing that actually exists (and equals zero) if is a function for which the 'wrong proof' fails. If so, one can actually fix this 'wrong proof' by considering two cases.","f\colon \mathbb{R}\to \mathbb{R} \mathbb{R}_0 \lim_{x\to 0}f'(x)=L\in \mathbb{R} f 0 h\neq 0 c_h\in \mathbb{R} h 0 f'(c_h)=\frac{f(h)-f(0)}{h} h\to 0 c_h\to 0 |c_h|<|h| \lim_{h\to 0}\frac{f(h)-f(0)}{h}=\lim_{h\to 0}f'(c_h)=L. \square \phi\colon \mathbb{R}\to \mathbb{R}:x\mapsto \begin{cases}x^3\sin(\frac{1}{x}) & \mbox{ if }x\neq 0,\\0 & \mbox{ if } x=0.\end{cases} \phi \mathbb{R}_0 \phi'(x)=3x^2\sin(\frac{1}{x})-x^3\cos(\frac{1}{x})\frac{1}{x^2}=3x^2\sin(\frac{1}{x})-x\cos(\frac{1}{x}) x\neq 0 \lim_{x\to 0}\phi'(x)=0 \phi'(0)=0 \phi \mathbb{R} f,g\colon \mathbb{R}\to \mathbb{R} a\in\mathbb{R} f'(a) g'(f(a)) (g\circ f)'(a)=g'(f(a))f'(a) \begin{eqnarray}
\lim_{x\to a}\frac{g\circ f(x)-g\circ f(a)}{x-a} &=& \lim_{x\to a}\frac{g\circ f(x)-g\circ f(a)}{f(x)-f(a)}\cdot \frac{f(x)-f(a)}{x-a}\\
&=& \lim_{x\to a}\frac{g\circ f(x)-g\circ f(a)}{f(x)-f(a)}\cdot \lim_{x\to a}\frac{f(x)-f(a)}{x-a}\\
&=& g'(f(a))f'(a).
\end{eqnarray} f a f(x)\to f(a) x\to a \triangle f f(x)=f(a) x\in \mathbb{R} \lim_{x\to a}\frac{g\circ f(x)-g\circ f(a)}{f(x)-f(a)}=\lim_{x\to a}\frac{g\circ f(x)-g\circ f(a)}{0} \exists \delta>0:\forall x\in (a-\delta,a+\delta)\setminus \{a\}:f(x)\neq f(a) f(x) f(a) x a x\neq a f (g\circ f')(a) 0 f f a f(x)=f(a) x a \phi a=0 f(x)=x^2\sin(\frac{1}{x}) x\neq 0 f(0)=0 0 f \forall \delta>0:\exists x\neq a: |x-a|<\delta f(x)=f(a) \lim_{x\to a}\frac{g\circ f(x)-g\circ f(a)}{x-a} x_n\to a n\geq 1 x_n\neq a |x_n-a|<\frac{1}{n} f(x_n)=f(a) \begin{eqnarray}
\lim_{x\to a}\frac{g\circ f(x)-g\circ f(a)}{x-a}&=&\lim_{n\to \infty}\frac{g\circ f(x_n)-g\circ f(a)}{x_n-a}\\
&=& \lim_{n\to \infty}\frac{g\circ f(a)-g\circ f(a)}{x-a}\\
&=& 0.
\end{eqnarray} f (g\circ f)'(a)=0 (g\circ f)'(a) (g\circ f)'(a) f","['real-analysis', 'analysis', 'derivatives', 'solution-verification', 'chain-rule']"
68,"Modern, Clear Mathematics books with a similar style to Sheldon Axler's books [closed]","Modern, Clear Mathematics books with a similar style to Sheldon Axler's books [closed]",,"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 3 years ago . The community reviewed whether to reopen this question 6 months ago and left it closed: Original close reason(s) were not resolved Improve this question This list ""is All You Need"" I graduated in Mathematics a couple of years ago but I know feel that I am forgetting quite a few things so I would like to go over most of the mathematical syllabus on my own. I would like to build a physical library ( meaning I want to buy books, not read them online ). I am looking to fill the areas shown below. Importantly, I would like to have books that are clear, fairly modern (I struggle to read 100-year-old manuscripts but if there is no alternative, I'll go for an old book) and allow me to cover the areas below without many gaps. Can you please suggest books following a similar style to Sheldon Axler 's ""Linear Algebra Done Right"" and ""Measure, Integration & Real Analysis""? By similar style I mean that they take the reader by hand and reinforce topics by using different colors, repetition of concepts and, importantly, building a lot of intuition with diagrams, figures or examples. They also shouldn't be huge 1000-page-long bricks. Here are the areas I am trying to cover. Striken over text means I have already bought them and happy with them. Texts in parenthesis are books I heard are good but possibly not quite similar to Sheldon Axler's style. Area Found Book Calculus No C - Spivak or VCLADF - Hubbard Linear Algebra Yes LADR - Axler Analysis Maybe A1- Tao or UA - Abbot Metric Spaces Maybe A2 - Tao or PMA - Rudin Differential Geometry No Functional Analysis Maybe FAFA - Sasane or IFAA - Kreyszig or FA - Stein Measure Theory Yes MIRA - Axler Probability Maybe PI - Grimmett or PT - Klenke or PM - Billingsley ODEs No DEDSLA - Hirsch Dynamical Systems Maybe NDC - Strogatz Stochastic Differential Equations No Optimization No Differential Calculus Maybe DCNS - Cartan or VCLADF - Hubbard Statistics Maybe NSLT - Vapnik or SLT - Vapnik","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 3 years ago . The community reviewed whether to reopen this question 6 months ago and left it closed: Original close reason(s) were not resolved Improve this question This list ""is All You Need"" I graduated in Mathematics a couple of years ago but I know feel that I am forgetting quite a few things so I would like to go over most of the mathematical syllabus on my own. I would like to build a physical library ( meaning I want to buy books, not read them online ). I am looking to fill the areas shown below. Importantly, I would like to have books that are clear, fairly modern (I struggle to read 100-year-old manuscripts but if there is no alternative, I'll go for an old book) and allow me to cover the areas below without many gaps. Can you please suggest books following a similar style to Sheldon Axler 's ""Linear Algebra Done Right"" and ""Measure, Integration & Real Analysis""? By similar style I mean that they take the reader by hand and reinforce topics by using different colors, repetition of concepts and, importantly, building a lot of intuition with diagrams, figures or examples. They also shouldn't be huge 1000-page-long bricks. Here are the areas I am trying to cover. Striken over text means I have already bought them and happy with them. Texts in parenthesis are books I heard are good but possibly not quite similar to Sheldon Axler's style. Area Found Book Calculus No C - Spivak or VCLADF - Hubbard Linear Algebra Yes LADR - Axler Analysis Maybe A1- Tao or UA - Abbot Metric Spaces Maybe A2 - Tao or PMA - Rudin Differential Geometry No Functional Analysis Maybe FAFA - Sasane or IFAA - Kreyszig or FA - Stein Measure Theory Yes MIRA - Axler Probability Maybe PI - Grimmett or PT - Klenke or PM - Billingsley ODEs No DEDSLA - Hirsch Dynamical Systems Maybe NDC - Strogatz Stochastic Differential Equations No Optimization No Differential Calculus Maybe DCNS - Cartan or VCLADF - Hubbard Statistics Maybe NSLT - Vapnik or SLT - Vapnik",,"['real-analysis', 'calculus', 'reference-request', 'self-learning', 'book-recommendation']"
69,"Is Spivak wrong about this counterexample? $f$ integrable on $[-1,1]$, $F=\int_{-1}^xf$, $f$ differentiable at $0$, but $F'$ not continuous at $0$","Is Spivak wrong about this counterexample?  integrable on , ,  differentiable at , but  not continuous at","f [-1,1] F=\int_{-1}^xf f 0 F' 0","Let $f$ be Riemann integrable on $[a,b]$ , let $c\in(a,b)$ , and let $F(x)=\int_a^xf$ for $x\in[a,b]$ . Exercise 3b in Chapter 14 of Spivak's Calculus (3rd edition) asks to give a proof or provide a counterexample of this statement: If $f$ is differentiable at $c$ , then $F'$ is continuous at $c$ . In his solutions manual, Spivak claims to give a counterexample. His function is $f:[-1,1]\to\mathbb{R}$ defined by $$f(x)=\begin{cases}0&x=0\\1&x\in\left\{-1,1\right\}\\\frac{1}{n^2}&\frac{1}{n}\leq |x|<\frac{1}{n-1}, n\geq2\end{cases}$$ Observe that $f$ has jump discontinuities at points of the form $1/n$ for nonzero integers $n$ and is continuous everywhere else. Now certainly $f$ is Riemann integrable on $[-1,1]$ (because its sets of discontinuities is countable, thus measure zero). Moreover $f$ is indeed differentiable at $0$ . But I don't think $F'$ is discontinuous at $0$ . On the contrary, here is my proof that $F'$ is continuous at $0$ . First, since $f$ is differentiable at $0$ , $f$ is continuous at $0$ ; it follows by the fundamental theorem that $F'$ exists at $0$ and that $F'(0)=f(0)$ . We know $f(0)=0$ , so $F'(0)=0$ . It remains to show that $\lim_{x\to0}F'(x)=0$ . Now this is a little delicate because, as Spivak points out, $F'$ is not even defined at $1/n$ for $n$ a nonzero integer. But that doesn't prevent us from calculating the limit of $F'$ . The limit simply must be taken through the domain of $F'$ . Indeed, given $\epsilon>0$ , take $N$ such that $\frac{1}{N^2}<\epsilon$ , and pick $\delta=\frac{1}{N}$ . Then if $0<|x|<\delta$ and $x$ is in the domain of $F'$ , it follows that $|F'(x)|\leq\frac{1}{(N+1)^2}<\frac{1}{N^2}<\epsilon$ . Therefore it is indeed true that $$\lim_{x\to0}F'(x)=F'(0)$$ so $F'$ is continuous at $0$ . What's going on here? Is this just a matter of one's definition of the limit of a function on a set that isn't an interval? Spivak seems to think ""If $\lim_{x\to c}g(x)$ exists then $g$ is defined on a deleted interval around $c$ .""","Let be Riemann integrable on , let , and let for . Exercise 3b in Chapter 14 of Spivak's Calculus (3rd edition) asks to give a proof or provide a counterexample of this statement: If is differentiable at , then is continuous at . In his solutions manual, Spivak claims to give a counterexample. His function is defined by Observe that has jump discontinuities at points of the form for nonzero integers and is continuous everywhere else. Now certainly is Riemann integrable on (because its sets of discontinuities is countable, thus measure zero). Moreover is indeed differentiable at . But I don't think is discontinuous at . On the contrary, here is my proof that is continuous at . First, since is differentiable at , is continuous at ; it follows by the fundamental theorem that exists at and that . We know , so . It remains to show that . Now this is a little delicate because, as Spivak points out, is not even defined at for a nonzero integer. But that doesn't prevent us from calculating the limit of . The limit simply must be taken through the domain of . Indeed, given , take such that , and pick . Then if and is in the domain of , it follows that . Therefore it is indeed true that so is continuous at . What's going on here? Is this just a matter of one's definition of the limit of a function on a set that isn't an interval? Spivak seems to think ""If exists then is defined on a deleted interval around .""","f [a,b] c\in(a,b) F(x)=\int_a^xf x\in[a,b] f c F' c f:[-1,1]\to\mathbb{R} f(x)=\begin{cases}0&x=0\\1&x\in\left\{-1,1\right\}\\\frac{1}{n^2}&\frac{1}{n}\leq |x|<\frac{1}{n-1}, n\geq2\end{cases} f 1/n n f [-1,1] f 0 F' 0 F' 0 f 0 f 0 F' 0 F'(0)=f(0) f(0)=0 F'(0)=0 \lim_{x\to0}F'(x)=0 F' 1/n n F' F' \epsilon>0 N \frac{1}{N^2}<\epsilon \delta=\frac{1}{N} 0<|x|<\delta x F' |F'(x)|\leq\frac{1}{(N+1)^2}<\frac{1}{N^2}<\epsilon \lim_{x\to0}F'(x)=F'(0) F' 0 \lim_{x\to c}g(x) g c","['real-analysis', 'calculus', 'limits']"
70,Harmonic functions without critical points (global isothermal coordinates),Harmonic functions without critical points (global isothermal coordinates),,"Let $(M,g)$ be a compact, orientable Riemannian surface with non-empty boundary $\partial M$ . Question. Does there always exist a smooth function $f:M\rightarrow \mathbb{R}$ with $\Delta_gf=0$ in the interior of $M$ and $d_pf\neq 0$ for all $p\in M$ ? For $M\subset \mathbb{R}^2$ , equipped with the Euclidean metric, this is clear (just take an affine linear map). Given a finite subset $P\subset M$ , one can always find a smooth function $f:M\rightarrow \mathbb{R}$ such that, for all $p\in P$ we have $\Delta f = 0$ near $p$ and $d_pf\neq 0$ . (This follows from a well known trick used to construct isothermal coordinates, see below.) If $M$ is contractible, this is equivalent to the existence of global isothermal coordinates on $(M,g)$ . Proof of 2) Extend $M$ to a closed surface $(N,g)$ and write $R:H_\perp^3(N)\rightarrow \mathbb{R}^{\vert P \vert}$ for the map that sends $f$ to the list $(\vert d_p f\vert: p\in P)\in \mathbb{R}^{\vert P \vert}$ . Here $H^s_\perp(N)$ consists of Sobolev-functions of regularity $s$ with zero mean (i.e. $\perp\{\mathrm{constants}\}$ ). Further write $F:H_\perp^1(N)\rightarrow H_\perp^3(N)$ for the map that sends $h$ to the solution $f$ of $\Delta f = h$ . Now the point is that the set $D_P=\{h\in H_\perp^1(N)\cap C^\infty(N):h \text{ vanishes near } P\}$ is dense in $H_\perp^1(N)$ , which implies that $RF(D_P)\subset \mathbb{R}^{\vert P\vert}$ is dense (as both $R$ and $F$ are continuous and surjective). In particular there is a vector in $RF(D_P)\subset\mathbb{R}^{\vert P\vert}$ with all coordinates non-zero and an $R$ -preimage $f\in F(D_P)$ satisfies the desired requirements.","Let be a compact, orientable Riemannian surface with non-empty boundary . Question. Does there always exist a smooth function with in the interior of and for all ? For , equipped with the Euclidean metric, this is clear (just take an affine linear map). Given a finite subset , one can always find a smooth function such that, for all we have near and . (This follows from a well known trick used to construct isothermal coordinates, see below.) If is contractible, this is equivalent to the existence of global isothermal coordinates on . Proof of 2) Extend to a closed surface and write for the map that sends to the list . Here consists of Sobolev-functions of regularity with zero mean (i.e. ). Further write for the map that sends to the solution of . Now the point is that the set is dense in , which implies that is dense (as both and are continuous and surjective). In particular there is a vector in with all coordinates non-zero and an -preimage satisfies the desired requirements.","(M,g) \partial M f:M\rightarrow \mathbb{R} \Delta_gf=0 M d_pf\neq 0 p\in M M\subset \mathbb{R}^2 P\subset M f:M\rightarrow \mathbb{R} p\in P \Delta f = 0 p d_pf\neq 0 M (M,g) M (N,g) R:H_\perp^3(N)\rightarrow \mathbb{R}^{\vert P \vert} f (\vert d_p f\vert: p\in P)\in \mathbb{R}^{\vert P \vert} H^s_\perp(N) s \perp\{\mathrm{constants}\} F:H_\perp^1(N)\rightarrow H_\perp^3(N) h f \Delta f = h D_P=\{h\in H_\perp^1(N)\cap C^\infty(N):h \text{ vanishes near } P\} H_\perp^1(N) RF(D_P)\subset \mathbb{R}^{\vert P\vert} R F RF(D_P)\subset\mathbb{R}^{\vert P\vert} R f\in F(D_P)","['real-analysis', 'differential-geometry', 'partial-differential-equations', 'riemannian-geometry']"
71,Derivative of a unit vector,Derivative of a unit vector,,"Consider a vector function $r: \mathbb{R} \to \mathbb{R}^n$ defined by $r(t)$ . We use $\hat{r}$ to denote its normalized vector, and $\dot{r}$ to denote $\frac{d}{dt}r(t)$ . We know that the derivative of a normalized vector is orthogonal to itself. It would be suggestive to write \begin{equation} \label{eq_ddtrt} 	\frac{d}{dt} \hat{r}(t) = a(t) N(\hat{r}(t)), \tag{1} \end{equation} where $a(t)$ is a scalar function and $N(\hat{r}(t))$ is a vector orthogonal to $\hat{r}(t)$ and it is a function of $\hat{r}$ explicitly . Consider the 2D case; that is, $n=2$ . Then we can find out that \begin{equation} \label{eq_ddtrt2} 	\frac{d}{dt} \hat{r}(t) = \underbrace{\left(-\frac{1}{\Vert r\Vert} \hat{r}^T E \dot{r} \right)}_{a(t)} \underbrace{E \hat{r}}_{N(\hat{r}(t))}, \tag{2} \end{equation} where $E=\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}$ is a rotational matrix. So $N(t)=E \hat{r}$ is indeed orthogonal to $\hat{r}(t)$ and it is explicitly a function of $\hat{r}$ . The advantage is that I could take $N(\hat{r}(t))$ out, and combine different coefficients, say, $a_i(t)$ , to simplify some other computations. The detailed calculation can be seen in the appendix. The problem is that this seems to work for $n=2$ , but it is difficult to obtain similar result for $n>2$ . Question : How to obtain a similar equation like \eqref{eq_ddtrt} for the higher-dimensional case where $n>2$ ? By similarity, I mean it can be written as $a(t)N(\hat{r}(t))$ , where $N(\hat{r}(t))$ is explicitly a function of $\hat{r}$ . Equation \eqref{eq19c} is not in this form. Question : Is it possible to write $N(\hat{r}(t))$ as a cross product of $\hat{r}(t)$ and some function? Appendix: Calculation of \eqref{eq_ddtrt2} \begin{equation}  \frac{d}{dt}{\frac{r(t)}{\Vert r(t)\Vert}} = \frac{d}{dt}{r(t)} \cdot \frac{1}{\Vert r(t)\Vert} + r(t) \cdot \frac{d}{dt}{\frac{1}{\Vert r(t)\Vert}} = \left(\frac{I}{\Vert r\Vert} - \frac{r r^T}{\Vert r\Vert^3}\right) \dot{r} \label{eq19c} \tag{3} \end{equation} If we define an operator $\hat{\cdot}: \mathbb{R}^n \rightarrow \mathbb{R}^n$ such that $\hat{r}=\frac{r}{\Vert r\Vert}$ , then \eqref{eq19c} can be re-written as follows: \begin{equation}  \frac{d}{dt}{\hat{r}(t)} = \frac{1}{\Vert r\Vert}(I - \hat{r} \hat{r}^T) \dot{r} = -\frac{1}{\Vert r\Vert} E \hat{r} \hat{r}^T E \dot{r} = \left( -\frac{1}{\Vert r\Vert}  \hat{r}^T E \dot{r} \right) E \hat{r}, \end{equation} where note that $$ 	I - \hat{r} \hat{r}^T = (E \hat{r}) (E \hat{r})^T = - E \hat{r} \hat{r}^T E. $$","Consider a vector function defined by . We use to denote its normalized vector, and to denote . We know that the derivative of a normalized vector is orthogonal to itself. It would be suggestive to write where is a scalar function and is a vector orthogonal to and it is a function of explicitly . Consider the 2D case; that is, . Then we can find out that where is a rotational matrix. So is indeed orthogonal to and it is explicitly a function of . The advantage is that I could take out, and combine different coefficients, say, , to simplify some other computations. The detailed calculation can be seen in the appendix. The problem is that this seems to work for , but it is difficult to obtain similar result for . Question : How to obtain a similar equation like \eqref{eq_ddtrt} for the higher-dimensional case where ? By similarity, I mean it can be written as , where is explicitly a function of . Equation \eqref{eq19c} is not in this form. Question : Is it possible to write as a cross product of and some function? Appendix: Calculation of \eqref{eq_ddtrt2} If we define an operator such that , then \eqref{eq19c} can be re-written as follows: where note that","r: \mathbb{R} \to \mathbb{R}^n r(t) \hat{r} \dot{r} \frac{d}{dt}r(t) \begin{equation} \label{eq_ddtrt}
	\frac{d}{dt} \hat{r}(t) = a(t) N(\hat{r}(t)), \tag{1}
\end{equation} a(t) N(\hat{r}(t)) \hat{r}(t) \hat{r} n=2 \begin{equation} \label{eq_ddtrt2}
	\frac{d}{dt} \hat{r}(t) = \underbrace{\left(-\frac{1}{\Vert r\Vert} \hat{r}^T E \dot{r} \right)}_{a(t)} \underbrace{E \hat{r}}_{N(\hat{r}(t))}, \tag{2}
\end{equation} E=\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix} N(t)=E \hat{r} \hat{r}(t) \hat{r} N(\hat{r}(t)) a_i(t) n=2 n>2 n>2 a(t)N(\hat{r}(t)) N(\hat{r}(t)) \hat{r} N(\hat{r}(t)) \hat{r}(t) \begin{equation} 
\frac{d}{dt}{\frac{r(t)}{\Vert r(t)\Vert}} = \frac{d}{dt}{r(t)} \cdot \frac{1}{\Vert r(t)\Vert} + r(t) \cdot \frac{d}{dt}{\frac{1}{\Vert r(t)\Vert}} = \left(\frac{I}{\Vert r\Vert} - \frac{r r^T}{\Vert r\Vert^3}\right) \dot{r} \label{eq19c} \tag{3}
\end{equation} \hat{\cdot}: \mathbb{R}^n \rightarrow \mathbb{R}^n \hat{r}=\frac{r}{\Vert r\Vert} \begin{equation} 
\frac{d}{dt}{\hat{r}(t)} = \frac{1}{\Vert r\Vert}(I - \hat{r} \hat{r}^T) \dot{r} = -\frac{1}{\Vert r\Vert} E \hat{r} \hat{r}^T E \dot{r} = \left( -\frac{1}{\Vert r\Vert}  \hat{r}^T E \dot{r} \right) E \hat{r},
\end{equation} 
	I - \hat{r} \hat{r}^T = (E \hat{r}) (E \hat{r})^T = - E \hat{r} \hat{r}^T E.
","['real-analysis', 'linear-algebra', 'geometry', 'multivariable-calculus', 'derivatives']"
72,Lower Semicontinuous Function = Supremum of Sequence of Continuous Functions,Lower Semicontinuous Function = Supremum of Sequence of Continuous Functions,,"Background I'm reading Cedric Villani's Optimal Transport: Old and New [1] and came across a result (below) I'm not quite sure how to prove. It is used to prove Lemma 4.3 and through my research, I've found it to be known as ""Baire's Theorem for Lower Semi-continuous Functions"" with topological approaches found in other StackExchange posts like [3] and [4] but never formally worked out. Question If $(X, d)$ is a metric space and $F$ is a nonnegative lower semi-continuous function on $X$ , then it can be written as the supremum of an increasing sequence of (uniformly?) continuous nonnegative functions. To see this, choose $$ F_{n}(x) = \inf\limits_{y~\in~X}\{~ F(y) + n\cdot d(x,y) ~\}  $$ and show it is: (i) increasing; (ii) (uniformly?) continuous; (iii) convergent to $F$ [1, pg. 26; 2, pg. 55]. References: C. Villani, Optimal Transport, vol. 338. Berlin, Heidelberg: Springer Berlin Heidelberg, 2009. Available: https://ljk.imag.fr/membres/Emmanuel.Maitre/lib/exe/fetch.php?media=b07.stflour.pdf C. Villani, Topics in Optimal Transportation, 1st ed. American Mathematical Society, 2003. “Prove by definition that every upper semi-continuous function can be expressed as infimum of a sequence of continuous functions.,” Stack Exchange, 2017. [Online]. Available: https://math.stackexchange.com/questions/2227074/prove-by-definition-that-every-upper-semi-continuous-function-can-be-expressed-a?noredirect=1&lq=1. [Accessed: 28-Dec-2019]. “Show that lower semicontinuous function is the supremum of an increasing sequence of continuous functions,” Stack Exchange, 2015. [Online]. Available: https://math.stackexchange.com/questions/1279763/show-that-lower-semicontinuous-function-is-the-supremum-of-an-increasing-sequenc/1284586. [Accessed: 28-Dec-2019]. “What's behind the function g(x)=inf{f(p)+d(x,p):p∈X}?,” Stack Exchange, 2013. [Online]. Available:  https://math.stackexchange.com/questions/616071/whats-behind-the-function-gx-operatornameinf-fpdx-pp-in-x?rq=1. [Accessed: 28-Dec-2019]","Background I'm reading Cedric Villani's Optimal Transport: Old and New [1] and came across a result (below) I'm not quite sure how to prove. It is used to prove Lemma 4.3 and through my research, I've found it to be known as ""Baire's Theorem for Lower Semi-continuous Functions"" with topological approaches found in other StackExchange posts like [3] and [4] but never formally worked out. Question If is a metric space and is a nonnegative lower semi-continuous function on , then it can be written as the supremum of an increasing sequence of (uniformly?) continuous nonnegative functions. To see this, choose and show it is: (i) increasing; (ii) (uniformly?) continuous; (iii) convergent to [1, pg. 26; 2, pg. 55]. References: C. Villani, Optimal Transport, vol. 338. Berlin, Heidelberg: Springer Berlin Heidelberg, 2009. Available: https://ljk.imag.fr/membres/Emmanuel.Maitre/lib/exe/fetch.php?media=b07.stflour.pdf C. Villani, Topics in Optimal Transportation, 1st ed. American Mathematical Society, 2003. “Prove by definition that every upper semi-continuous function can be expressed as infimum of a sequence of continuous functions.,” Stack Exchange, 2017. [Online]. Available: https://math.stackexchange.com/questions/2227074/prove-by-definition-that-every-upper-semi-continuous-function-can-be-expressed-a?noredirect=1&lq=1. [Accessed: 28-Dec-2019]. “Show that lower semicontinuous function is the supremum of an increasing sequence of continuous functions,” Stack Exchange, 2015. [Online]. Available: https://math.stackexchange.com/questions/1279763/show-that-lower-semicontinuous-function-is-the-supremum-of-an-increasing-sequenc/1284586. [Accessed: 28-Dec-2019]. “What's behind the function g(x)=inf{f(p)+d(x,p):p∈X}?,” Stack Exchange, 2013. [Online]. Available:  https://math.stackexchange.com/questions/616071/whats-behind-the-function-gx-operatornameinf-fpdx-pp-in-x?rq=1. [Accessed: 28-Dec-2019]","(X, d) F X 
F_{n}(x) = \inf\limits_{y~\in~X}\{~ F(y) + n\cdot d(x,y) ~\} 
 F","['real-analysis', 'metric-spaces', 'semicontinuous-functions']"
73,Find the number of integral values of $x$ where $f(x)=x^2|x-1|+(x-1)^2|x-4|+(x-4)^2|x-9|+...+(x-2401)^2|x-2500|$ has local minima.,Find the number of integral values of  where  has local minima.,x f(x)=x^2|x-1|+(x-1)^2|x-4|+(x-4)^2|x-9|+...+(x-2401)^2|x-2500|,Find the number of integral values of $x$ for which $f(x)=x^2|x-1|+(x-1)^2|x-4|+(x-4)^2|x-9|+.....+(x-2401)^2|x-2500|$ has local minima. My Attempt $$f(x)=\sum_{m=0}^{49}\left(x-m^2\right)^2\left|x-(m+1)^2\right|$$ It appears that the extrema would appear between $m^2$ and $(m+1)^2$ . So how to proceed after this.,Find the number of integral values of for which has local minima. My Attempt It appears that the extrema would appear between and . So how to proceed after this.,x f(x)=x^2|x-1|+(x-1)^2|x-4|+(x-4)^2|x-9|+.....+(x-2401)^2|x-2500| f(x)=\sum_{m=0}^{49}\left(x-m^2\right)^2\left|x-(m+1)^2\right| m^2 (m+1)^2,"['real-analysis', 'calculus', 'algebra-precalculus', 'maxima-minima']"
74,"The right derivative, $f'_{+}$, of a convex function $f$ is continuous $\iff$ $f$ is differentiable.","The right derivative, , of a convex function  is continuous   is differentiable.",f'_{+} f \iff f,"Let be $f$ a convex function defined on an open set. We know from theory that $f'_{+},f'_{-}$ both exist not decreasing. I want to prove or in case it is false to refute this: The right derivative, $f'_{+}$ , of a convex function $f$ is continuous $\iff$ $f$ is differentiable. I'm stuck with $[\Rightarrow]$ . But also for $[\Leftarrow]$ I don't have a formal proof. Any help,hint or solution would be appreciated. (on both arrows)","Let be a convex function defined on an open set. We know from theory that both exist not decreasing. I want to prove or in case it is false to refute this: The right derivative, , of a convex function is continuous is differentiable. I'm stuck with . But also for I don't have a formal proof. Any help,hint or solution would be appreciated. (on both arrows)","f f'_{+},f'_{-} f'_{+} f \iff f [\Rightarrow] [\Leftarrow]","['real-analysis', 'calculus', 'analysis', 'derivatives', 'convex-analysis']"
75,"Collection of intervals covers $[0,1]$?",Collection of intervals covers ?,"[0,1]","For each $n=1,2,3,...$ and each $m=0,1,2,...,n-1$ , let $$K^n_m = \left[ \frac{3m-n+1}{n} , \frac{3m-n+2}{n} \right] \subset (-1,2). $$ I am struggling these with two questions for quite some time: (1) $ \ $ The collection $\{ K^n_m \} \setminus \{ K^1_0 \}$ covers the interval $[0,1]$ ? In case the answer for (1) is yes, then (2) $ \ $ For each $ \ x \in [0,1] \ $ is there an infinite number of $ \, K^n_m \, $ such that $ \ x \in K^n_m \ $ ?","For each and each , let I am struggling these with two questions for quite some time: (1) The collection covers the interval ? In case the answer for (1) is yes, then (2) For each is there an infinite number of such that ?","n=1,2,3,... m=0,1,2,...,n-1 K^n_m = \left[ \frac{3m-n+1}{n} , \frac{3m-n+2}{n} \right] \subset (-1,2).   \  \{ K^n_m \} \setminus \{ K^1_0 \} [0,1]  \   \ x \in [0,1] \   \, K^n_m \,   \ x \in K^n_m \ ","['real-analysis', 'real-numbers', 'rational-numbers']"
76,Evaluating $\int_0^\infty \left( \frac{x}{e^x-e^{-x}}-\frac{1}{2} \right) \frac{dx}{x^2}$,Evaluating,\int_0^\infty \left( \frac{x}{e^x-e^{-x}}-\frac{1}{2} \right) \frac{dx}{x^2},"Evaluating $$\int_0^\infty \left( \frac{x}{e^x-e^{-x}}-\frac{1}{2} \right) \frac{dx}{x^2}$$ I tried to calculate it by Mathematica, but it failed to give me an answer. Then I got interested in this problem because it actually can be well evaluated. My attempt Put $$ g(x)=\frac{1}{x}\left(\frac{1}{e^x-1}-\frac{1}{x}+\frac{1}{2}e^{-x}\right) $$ Considering that $$ \left( \frac{x}{e^x-e^{-x}}-\frac{1}{2} \right) \frac{1}{x^2} = -\frac{1}{2x}\left(e^{-x}-e^{-2x}\right) + g(x)-2g(2x) $$ and $$ \int_0^\infty g(x) dx = 2 \int_0^\infty g(2x) dx $$ thus via Frullani's integral we have $$ \int_0^\infty \left( \frac{x}{e^x-e^{-x}}-\frac{1}{2} \right) \frac{dx}{x^2} = -\frac{1}{2} \int_0^\infty \frac{1}{x}\left(e^{-x}-e^{-2x}\right) =-\frac{1}{2} \log 2 $$ But I am looking forward to other approaches, beacuse this method doesn't seem quite natural. And I would highly appreciate it if you could share any thoughts on how to solve this problem. Thanks in advance!","Evaluating I tried to calculate it by Mathematica, but it failed to give me an answer. Then I got interested in this problem because it actually can be well evaluated. My attempt Put Considering that and thus via Frullani's integral we have But I am looking forward to other approaches, beacuse this method doesn't seem quite natural. And I would highly appreciate it if you could share any thoughts on how to solve this problem. Thanks in advance!","\int_0^\infty \left( \frac{x}{e^x-e^{-x}}-\frac{1}{2} \right) \frac{dx}{x^2} 
g(x)=\frac{1}{x}\left(\frac{1}{e^x-1}-\frac{1}{x}+\frac{1}{2}e^{-x}\right)
 
\left( \frac{x}{e^x-e^{-x}}-\frac{1}{2} \right) \frac{1}{x^2}
=
-\frac{1}{2x}\left(e^{-x}-e^{-2x}\right)
+ g(x)-2g(2x)
 
\int_0^\infty g(x) dx = 2 \int_0^\infty g(2x) dx
 
\int_0^\infty \left( \frac{x}{e^x-e^{-x}}-\frac{1}{2} \right) \frac{dx}{x^2}
=
-\frac{1}{2} \int_0^\infty \frac{1}{x}\left(e^{-x}-e^{-2x}\right)
=-\frac{1}{2} \log 2
","['real-analysis', 'calculus', 'complex-analysis']"
77,Let $A \subset \mathbb{R}$ be a Lebesgue measurable set with positive Lebesgue measure.,Let  be a Lebesgue measurable set with positive Lebesgue measure.,A \subset \mathbb{R},"Let $A \subset \mathbb{R}$ be a Lebesgue measurable set with positive Lebesgue measure. Show that for any $k\in\mathbb{N}$, there exists $a,t \in\mathbb{R}$ ($t\ne0$) such that \begin{align*} a,a+t,a+2t,\cdots,a+kt\in A\end{align*} I knew there exists $\delta>0$ s.t $(-\delta,\delta)\subset A-A$ because of $\mu(A)>0$. The problem seems to be similar with that. However, I hard to think how to prove that.. Any help is appreciated.. Thank you!","Let $A \subset \mathbb{R}$ be a Lebesgue measurable set with positive Lebesgue measure. Show that for any $k\in\mathbb{N}$, there exists $a,t \in\mathbb{R}$ ($t\ne0$) such that \begin{align*} a,a+t,a+2t,\cdots,a+kt\in A\end{align*} I knew there exists $\delta>0$ s.t $(-\delta,\delta)\subset A-A$ because of $\mu(A)>0$. The problem seems to be similar with that. However, I hard to think how to prove that.. Any help is appreciated.. Thank you!",,['real-analysis']
78,"Is the arclength of $x\sin(\pi/x)$ in the interval $(0, 1)$ finite?",Is the arclength of  in the interval  finite?,"x\sin(\pi/x) (0, 1)","I'm having trouble determining the arclength of $x\sin(\pi/x)$ in the interval $(0, 1)$ (that is, determining if it converges or not). My first attempt was to write $$ \begin{aligned} \int_0^1\sqrt{1+(f'(x))^2}\text{d}x & = \int_0^1\sqrt{1 + \left(\sin\frac \pi x - \frac \pi x\cos\frac \pi x\right)^2}\text{d}x \\ & = \pi\int_1^\infty\sqrt{1 + (\sin x - x\cos x)^2}\frac{\text{d}x}{x^2} \end{aligned} $$ Then I wanted to consider the asymptotics function $\sqrt{1 + (\sin x - x\cos x)^2}$, but I can't find an easy way in. One thought was to write $$ \int_1^\infty\sqrt{1+(\sin x - x\cos x)^2}\frac{\text{d}x}{x^2} > \int_1^\infty\left|\frac{\sin x}{x^2} - \frac{\cos x}x\right|\text{d}x $$ Next, we notice that $x\cos x + \sin x = 0$ occurs approximately when $x = (k+1/2)\pi$ ($k\in\mathbb{N}$), since these points are the points where $x = \tan x$, and as $x$ increases (so that the function $x$ is higher along the $y$-axis), the function $\tan x$ gets closer and closer to its vertical asymptote, which occurs at $(k+1/2)\pi$, so the intersection of the two functions also occurs close to that asymptote. So we consider  $$ \int_{(k-1/2)\pi}^{(k+1/2)\pi}\frac{|\sin x - x\cos x|}{x^2}\text{d} x \ge \frac1{[(k+1/2)\pi]^2}\int_{(k-1/2)\pi}^{(k+1/2)\pi}|\sin x - x\cos x|\text{d} x $$ This is where I get stuck, as I'm not sure how to estimate that integrand. I assume it will be approximately linear in $k$, but I'm not sure how to prove that. If that's true, then the integral is greater than a term that behaves like $\frac 1k$, meaning the original integral is greater than a harmonic sum, and so diverges. So first question: how can I evaluate the integral on the RHS of the inequality above? Second question: I also feel like there's an easier way of thinking about this problem that I'm missing. Any ideas? Edit: I just realized that using $\int |f| \ge |\int f|$, we get $$ \int_{(k-1/2)\pi}^{(k+1/2)\pi}|\sin x - x\cos x|\text{d} x \ge \left|\int_{(k-1/2)\pi}^{(k+1/2)\pi}(\sin x - x\cos x)\text{d}x\right| = |2\pi k\cos(\pi k)| = 2\pi k $$ However, this doesn't completely answer my question. Yes, it's a solution to the integral, but the argument that we should only look at the intervals $[(k-1/2)\pi, (k+1/2)\pi]$ is kind of handwavy, and it still feels like there should be an easier solution.","I'm having trouble determining the arclength of $x\sin(\pi/x)$ in the interval $(0, 1)$ (that is, determining if it converges or not). My first attempt was to write $$ \begin{aligned} \int_0^1\sqrt{1+(f'(x))^2}\text{d}x & = \int_0^1\sqrt{1 + \left(\sin\frac \pi x - \frac \pi x\cos\frac \pi x\right)^2}\text{d}x \\ & = \pi\int_1^\infty\sqrt{1 + (\sin x - x\cos x)^2}\frac{\text{d}x}{x^2} \end{aligned} $$ Then I wanted to consider the asymptotics function $\sqrt{1 + (\sin x - x\cos x)^2}$, but I can't find an easy way in. One thought was to write $$ \int_1^\infty\sqrt{1+(\sin x - x\cos x)^2}\frac{\text{d}x}{x^2} > \int_1^\infty\left|\frac{\sin x}{x^2} - \frac{\cos x}x\right|\text{d}x $$ Next, we notice that $x\cos x + \sin x = 0$ occurs approximately when $x = (k+1/2)\pi$ ($k\in\mathbb{N}$), since these points are the points where $x = \tan x$, and as $x$ increases (so that the function $x$ is higher along the $y$-axis), the function $\tan x$ gets closer and closer to its vertical asymptote, which occurs at $(k+1/2)\pi$, so the intersection of the two functions also occurs close to that asymptote. So we consider  $$ \int_{(k-1/2)\pi}^{(k+1/2)\pi}\frac{|\sin x - x\cos x|}{x^2}\text{d} x \ge \frac1{[(k+1/2)\pi]^2}\int_{(k-1/2)\pi}^{(k+1/2)\pi}|\sin x - x\cos x|\text{d} x $$ This is where I get stuck, as I'm not sure how to estimate that integrand. I assume it will be approximately linear in $k$, but I'm not sure how to prove that. If that's true, then the integral is greater than a term that behaves like $\frac 1k$, meaning the original integral is greater than a harmonic sum, and so diverges. So first question: how can I evaluate the integral on the RHS of the inequality above? Second question: I also feel like there's an easier way of thinking about this problem that I'm missing. Any ideas? Edit: I just realized that using $\int |f| \ge |\int f|$, we get $$ \int_{(k-1/2)\pi}^{(k+1/2)\pi}|\sin x - x\cos x|\text{d} x \ge \left|\int_{(k-1/2)\pi}^{(k+1/2)\pi}(\sin x - x\cos x)\text{d}x\right| = |2\pi k\cos(\pi k)| = 2\pi k $$ However, this doesn't completely answer my question. Yes, it's a solution to the integral, but the argument that we should only look at the intervals $[(k-1/2)\pi, (k+1/2)\pi]$ is kind of handwavy, and it still feels like there should be an easier solution.",,"['real-analysis', 'improper-integrals', 'arc-length']"
79,"If $|x_{n+1} - x_n| < \epsilon,$ is $(x_n) $ a Cauchy sequence?",If  is  a Cauchy sequence?,"|x_{n+1} - x_n| < \epsilon, (x_n) ","Suppose that $\, \forall \epsilon >0, \, \exists N, \,$ such that $$|x_{n+1} - x_n| < \epsilon\, \quad \forall n \geq N.$$ Is the sequence $(x_n)$ a Cauchy sequence? If so, prove it; if not, give a counterexample. My professor gave this counterexample: Let $$x_n := \sqrt{n} $$ then $\, \forall\epsilon > 0\, $ take $ \, N = \lfloor \frac{1}{\epsilon ^2}\rfloor +1 \, $ then we have $$ |x_n - x_{n+1} | = \sqrt{n+1} - \sqrt{n} = \frac{1}{\sqrt{n} + \sqrt{n+1}} < \frac{1}{\sqrt{n}} \leq \frac{1}{\sqrt{N}} < \epsilon $$ But $(x_n) $ is divergent, so $(x_n)$ is not Cauchy. I disagreed that this proof was valid, because the sequence $x_n := \sqrt{n} \, $ doesn't satisfy the hypothesis, particularly that if $\forall \epsilon > 0$ $$ | x_{n+1} - x_n| < \epsilon\, , \quad \forall n \geq N$$ is satisfied, then the hypothesis requires that $$ | x_{n+2} - x_{n+1}| < \epsilon\, , \quad $$ is also satisfied, and so on. Every single time I've disagreed with my analysis professor I've been wrong and I think this is not an exception. Really I would like someone to point out why the following proof (my attempt) that the sequence is Cauchy is wrong, please: For $m \geq n$ \begin{align} |x_m - x_n| &= |x_m - x_{m-1} + x_{m-1} - \cdots + x_{n+2} - x_{n+1} - x_{n+1} + x_n| \\  &\leq |x_m - x_{m-1}| + |x_{m-1} - x_{m-2}| + \cdots + |x_{n+2} - x_{n+1}| + |x_{n+1} + x_n| \\ &< (m-n)\epsilon \end{align} Therefore the sequence is Cauchy. Where I think I'm wrong: the $(m-n)$ factor. I'm claiming that the error of each term can be made as small as we like so adding up however many terms makes no difference. Actually now, that I've typed this all out it seems obvious that I'm wrong, for example: I could have every term in a sum bounded by a one trillionth, but then add up a number of  sequential terms well past one trillion, and make the total error as large as I would like, regardless of $\epsilon$. Can someone please confirm if that thinking is correct.","Suppose that $\, \forall \epsilon >0, \, \exists N, \,$ such that $$|x_{n+1} - x_n| < \epsilon\, \quad \forall n \geq N.$$ Is the sequence $(x_n)$ a Cauchy sequence? If so, prove it; if not, give a counterexample. My professor gave this counterexample: Let $$x_n := \sqrt{n} $$ then $\, \forall\epsilon > 0\, $ take $ \, N = \lfloor \frac{1}{\epsilon ^2}\rfloor +1 \, $ then we have $$ |x_n - x_{n+1} | = \sqrt{n+1} - \sqrt{n} = \frac{1}{\sqrt{n} + \sqrt{n+1}} < \frac{1}{\sqrt{n}} \leq \frac{1}{\sqrt{N}} < \epsilon $$ But $(x_n) $ is divergent, so $(x_n)$ is not Cauchy. I disagreed that this proof was valid, because the sequence $x_n := \sqrt{n} \, $ doesn't satisfy the hypothesis, particularly that if $\forall \epsilon > 0$ $$ | x_{n+1} - x_n| < \epsilon\, , \quad \forall n \geq N$$ is satisfied, then the hypothesis requires that $$ | x_{n+2} - x_{n+1}| < \epsilon\, , \quad $$ is also satisfied, and so on. Every single time I've disagreed with my analysis professor I've been wrong and I think this is not an exception. Really I would like someone to point out why the following proof (my attempt) that the sequence is Cauchy is wrong, please: For $m \geq n$ \begin{align} |x_m - x_n| &= |x_m - x_{m-1} + x_{m-1} - \cdots + x_{n+2} - x_{n+1} - x_{n+1} + x_n| \\  &\leq |x_m - x_{m-1}| + |x_{m-1} - x_{m-2}| + \cdots + |x_{n+2} - x_{n+1}| + |x_{n+1} + x_n| \\ &< (m-n)\epsilon \end{align} Therefore the sequence is Cauchy. Where I think I'm wrong: the $(m-n)$ factor. I'm claiming that the error of each term can be made as small as we like so adding up however many terms makes no difference. Actually now, that I've typed this all out it seems obvious that I'm wrong, for example: I could have every term in a sum bounded by a one trillionth, but then add up a number of  sequential terms well past one trillion, and make the total error as large as I would like, regardless of $\epsilon$. Can someone please confirm if that thinking is correct.",,"['real-analysis', 'convergence-divergence', 'cauchy-sequences', 'telescopic-series']"
80,"Given sequence of $L-$Lipschitz functions which converges pointwise, prove uniform convergence","Given sequence of Lipschitz functions which converges pointwise, prove uniform convergence",L-,"Let $f_n:[a.b]\rightarrow \mathbb{R}$ be sequence of $L-$Lipschitz functions, that is: $$\forall x,y\in[a,b]: |f_n(x)-f_n(y)|\leq L|x-y|$$   Suppose $f_n \rightarrow f$ pointwise, prove $f_n \rightrightarrows f$ I have all the parts of the puzzle for the proof, and I'm trying to put them all together, I'm using this in my answer. I would appreciate is you could correct my proof, and if you have an alternative proof, I would be more then happy to see it. My proof: Let $\epsilon>0.$ $f_n$ are uniformly continuous on $[a,b]:$ $\tag{1} \exists \delta>0\ \forall x,y\in[a,b]: |f_n(x)-f_n(y)|<\frac{\epsilon}{3}$ $f$ is also $L-$Lipschitz: $\tag{2} \forall x,y\in[a,b]:|f(x)-f(y)|<L|x-y|=\frac{\epsilon}{3}$ Let us set a partition of $[a,b]$ such as Stephen Montgomery-Smith suggests: Pick points $x_1,\dots,x_m \in [a,b]$ which are distance   $\frac{\epsilon}{3}$ from each other. For each $1 \le i \le m$, find a number $N_i$ so that for all $n \ge  N_i$ we have $|f_n(x_i)-f(x_i)| \le \epsilon/3$. Let $N = \max_{1 \le i \le m} N_i$ Now given any $x \in [a,b]$, pick $1 \le i \le m$ such that $|x-x_i| <\frac{\epsilon}{3L}:  |f_n(x)-f(x)|<\frac{\epsilon}{3} \tag{3}$ $$\begin{align}|f_n(y)-f(y)| &=|f_n(y)-f_n(x)+f_n(x)-f(x)+f(x)-f(y)| \\ &\leq |f_n(y)-f_n(x)| + |f_n(x)-f(x)|+|f(x)-f(y)| \\ &< \frac{\epsilon}{3} + \frac{\epsilon}{3} + \frac{\epsilon}{3} = \epsilon \\\end{align}$$","Let $f_n:[a.b]\rightarrow \mathbb{R}$ be sequence of $L-$Lipschitz functions, that is: $$\forall x,y\in[a,b]: |f_n(x)-f_n(y)|\leq L|x-y|$$   Suppose $f_n \rightarrow f$ pointwise, prove $f_n \rightrightarrows f$ I have all the parts of the puzzle for the proof, and I'm trying to put them all together, I'm using this in my answer. I would appreciate is you could correct my proof, and if you have an alternative proof, I would be more then happy to see it. My proof: Let $\epsilon>0.$ $f_n$ are uniformly continuous on $[a,b]:$ $\tag{1} \exists \delta>0\ \forall x,y\in[a,b]: |f_n(x)-f_n(y)|<\frac{\epsilon}{3}$ $f$ is also $L-$Lipschitz: $\tag{2} \forall x,y\in[a,b]:|f(x)-f(y)|<L|x-y|=\frac{\epsilon}{3}$ Let us set a partition of $[a,b]$ such as Stephen Montgomery-Smith suggests: Pick points $x_1,\dots,x_m \in [a,b]$ which are distance   $\frac{\epsilon}{3}$ from each other. For each $1 \le i \le m$, find a number $N_i$ so that for all $n \ge  N_i$ we have $|f_n(x_i)-f(x_i)| \le \epsilon/3$. Let $N = \max_{1 \le i \le m} N_i$ Now given any $x \in [a,b]$, pick $1 \le i \le m$ such that $|x-x_i| <\frac{\epsilon}{3L}:  |f_n(x)-f(x)|<\frac{\epsilon}{3} \tag{3}$ $$\begin{align}|f_n(y)-f(y)| &=|f_n(y)-f_n(x)+f_n(x)-f(x)+f(x)-f(y)| \\ &\leq |f_n(y)-f_n(x)| + |f_n(x)-f(x)|+|f(x)-f(y)| \\ &< \frac{\epsilon}{3} + \frac{\epsilon}{3} + \frac{\epsilon}{3} = \epsilon \\\end{align}$$",,"['real-analysis', 'proof-writing', 'uniform-convergence', 'lipschitz-functions', 'sequence-of-function']"
81,"Prob. 6, Sec. 3.3, in Kreyszig's Functional Analysis Book: What is $Y^\perp$ if $Y = \ldots \subset \ell^2$?","Prob. 6, Sec. 3.3, in Kreyszig's Functional Analysis Book: What is  if ?",Y^\perp Y = \ldots \subset \ell^2,"Let $Y$ be the subset of $\ell^2$ given by $$ Y \colon= \left\{ \ \left( \xi_n \right)_{n \in \mathbb{N}} \in \ell^2 \ \colon \ \xi_{2n} = 0 \ \mbox{ for all } \ n \in \mathbb{N} \ \right\}. $$ Then $Y$ is a closed subspace of the Hilbert space $\ell^2$ , and so $Y$ is also complete. What is $Y^\perp$ ? Here is the relevant definition: Let $X$ be an inner product space, and let $M$ be a non-empty subset of $X$ . Then the orthogonal complement $M^\perp$ of $M$ is defined as follows: $$ M \colon= \left\{ \ x \in X \ \colon \  \langle x, y \rangle = 0 \ \mbox{ for all } \ y \in M \ \right\}. $$ My effort: By definition, \begin{align} & Y^\perp \\ &= \left\{ \ x \in \ell^2 \ \colon \ \langle x, y \rangle = 0 \ \forall y \in Y \ \right\} \\ &= \left\{ \ \left( \xi_n \right)_{n \in \mathbb{N} } \in \ell^2 \ \colon \ \left\langle \left( \xi_n \right)_{n \in \mathbb{N} }, \left( \eta_n \right)_{n \in \mathbb{N} } \right\rangle = 0 \ \forall \left( \eta_n \right)_{n \in \mathbb{N} } \in Y \ \right\} \\ &= \left\{ \ \left( \xi_n \right)_{n \in \mathbb{N} } \in \ell^2 \ \colon \ \sum_{n = 1}^\infty  \xi_n  \overline{  \eta_n} = 0 \ \forall \left( \eta_n \right)_{n \in \mathbb{N} } \in Y \ \right\} \\  &= \left\{ \ \left( \xi_n \right)_{n \in \mathbb{N} } \in \ell^2 \ \colon \ \sum_{n = 1}^\infty  \xi_n  \overline{  \eta_n} = 0 \ \forall \left( \eta_n \right)_{n \in \mathbb{N} } \in \ell^2 \ \mbox{ such that } \ \eta_{2n} = 0 \ \forall n \in \mathbb{N}  \ \right\} \\ &= \left\{ \ \left( \xi_n \right)_{n \in \mathbb{N} } \in \ell^2 \ \colon \ \sum_{n = 1}^\infty  \xi_{2n-1}  \overline{  \eta_{2n-1} } = 0 \ \forall \left( \eta_n \right)_{n \in \mathbb{N} } \in \ell^2 \ \mbox{ such that } \ \eta_{2n} = 0 \ \forall n \in \mathbb{N}  \ \right\}.  \end{align} My intuition is that $$Y^\perp = \left\{ \ \left( \xi_n \right)_{n \in \mathbb{N} } \in \ell^2 \ \colon \ \xi_{2n-1} = 0 \ \forall n \in \mathbb{N}  \ \right\}.$$ Is it so? If yes, then how to prove this rigorously? Another approach: As $\ell^2$ is a Hilbert space and as $Y$ is a closed subspace of $\ell^2$ , so $$ \ell^2 = Y \oplus Y^\perp, $$ by Theorem 3.3-4 in the book Introductory Functional Analysis With Applications by Erwine Kreyszig. This means that every $x \in \ell^2$ has a unique representation $$ x = y+z, \ \mbox{ where } \ y \in Y \ \mbox{ and } \ z \in Y^\perp. $$ Now  if $x \colon= \left( \xi_n \right)_{n \in \mathbb{N}} \in \ell^2$ , then, by definition, the series $\sum \left\lvert \xi_n \right\rvert^2$ of non-negative real numbers converges in $\mathbb{R}$ . Therefore, the sequence $$ \left( \sum_{j=1}^n \left\lvert \xi_j \right\rvert^2 \right)_{n \in \mathbb{N}} $$ of the partial sums of the series $\sum \left\lvert \xi_n \right\rvert^2$ , which is also monotonically increasing, is convergent. Hence this sequence is bounded (from above). That is, for each $n \in \mathbb{N}$ , we have $$ \sum_{j=1}^n \left\lvert \xi_j \right\rvert^2 \leq \sum_{j=1}^{n+1} \left\lvert \xi_j \right\rvert^2, $$ and $\lim_{n \to \infty} \sum_{j=1}^n \left\lvert \xi_j \right\rvert^2$ exists in $\mathbb{R}$ , and $$ \lim_{n \to \infty} \sum_{j=1}^n \left\lvert \xi_j \right\rvert^2 = \sup \left\{ \ \sum_{j=1}^n \left\lvert \xi_j \right\rvert^2 \ \colon \ n \in \mathbb{N} \ \right\} < +\infty. $$ Now let $y \colon= \left( \eta_n \right)_{n \in \mathbb{N} }$ be ths sequence of (real or complex) numbers defined as follows: For each $n \in \mathbb{N}$ , let $$ \eta_{2n-1} \colon= \xi_{2n-1} \ \mbox{ and } \eta_{2n} \colon= 0. $$ Then the sequence $$ \left( \sum_{j=1}^n \left\lvert \eta_j \right\rvert^2 \right)_{n \in \mathbb{N} } $$ of the partial sums of the series $\sum \left\lvert \eta_j \right\rvert^2$ is given by $$ \sum_{j=1}^{2n-1} \left\lvert \eta_j \right\rvert^2 = \sum_{j=1}^n \left\lvert \xi_{2j-1} \right\rvert^2, $$ and $$ \sum_{j=1}^{2n} \left\lvert \eta_j \right\rvert^2 = \sum_{j=1}^n \left\lvert \xi_{2j-1} \right\rvert^2 $$ for all $n \in \mathbb{N}$ . And, this sequence, being a sequence of sums of non-negative real numbers, is also monotonically increasing and  bounded above by the sum of the convergent series $ \sum \left\lvert \xi_n \right\rvert^2$ . So the sequence $$ \left( \sum_{j=1}^n \left\lvert \eta_j \right\rvert^2 \right)_{n \in \mathbb{N} } $$ is convergent, which means that the series $ \sum  \left\lvert \eta_j \right\rvert^2$ converges. Therefore, $y \colon= \left( \eta_n \right)_{n \in \mathbb{N} }$ is in $\ell^2$ .    So from the definition of $Y$ we can conclude that $y \in Y$ . Now let $z \colon= \left( \zeta_n \right)_{n \in \mathbb{N}}$ be the sequence of (real or complex) numbers defined as follows: For each $n \in \mathbb{N}$ , let $$ \zeta_{2n-1} \colon= 0 \ \mbox{ and } \ \zeta_{2n} \colon= \xi_{2n}. $$ Then $x = y+z$ , and so $z = x-y \in \ell^2$ , because $\ell^2$ is closed under addition and scalar multiplication. Another way to show that $z \colon= \left( \zeta_n \right)_{n \in \mathbb{N}}$ is in $\ell^2$ is to show that the series $\sum \left\lvert \zeta_n \right\rvert^2$ of real numbers converges in $\mathbb{R}$ . Now the sequence $$ \left( \sum_{j=1}^n  \left\lvert \zeta_j \right\rvert^2 \right)_{n \in \mathbb{N} }$$ of the partial sums of the series $\sum \left\lvert \zeta_n \right\rvert^2$ is  given by $$ \sum_{j=1}^{2n-1}  \left\lvert \zeta_j \right\rvert^2 = \sum_{j=1}^{2n-2}  \left\lvert \xi_{2j} \right\rvert^2,   $$ and $$ \sum_{j=1}^{2n}  \left\lvert \zeta_j \right\rvert^2 = \sum_{j=1}^{2n}  \left\lvert \xi_{2j} \right\rvert^2   $$ for all $n \in \mathbb{N}$ . Thus this sequence, being a sequence of sums of non-negative real numbers, is monotonically increasing and bounded above by the sum of the series $\sum \left\lvert \xi_n \right\rvert^2$ , and  so this sequence is convergent, which implies that $z \in \ell^2$ . Thus $$ y = \left( \eta_1, \eta_2, \eta_3, \ldots \right) = \left( \xi_1, 0, \xi_3, 0, \xi_5, 0, \ldots \right), $$ and $$ z = \left( \zeta_1, \zeta_2, \zeta_3, \ldots \right) = \left( 0, \xi_2, 0, \xi_4, 0, \xi_6,  \ldots \right),  $$ and so $$ \langle y, z \rangle = \sum_{n=1}^\infty \eta_n \overline{\zeta_n} = 0, $$ and so $\langle y, z \rangle = 0$ also. In fact, we can also show that $\langle z, v \rangle = 0$ for all $v \in Y$ . Thus $z \in Y^\perp$ . But so far I have only been able to show that the subspace $$ Z = \left\{  \ \left( \xi_n \right)_{n \in \mathbb{N} }  \in \ell^2 \ \colon \ \xi_{2n-1} = 0 \ \forall \ n \in \mathbb{N} \ \right\}$$ is contained in $Y^\perp$ . Is what I have done so far correct, logically sound, and rigorous enough? If so, then how to complete the above two arguments? If not, then where do I lack? P.S.: In order to show that $Y$ is indeed closed in $\ell^2$ , let us suppose that $x \colon= \left( \xi_n \right)_{n \in \mathbb{N} }$ is an element in the closure of $Y$ in $\ell^2$ . Then, by Theorem 1.4-6 (a) in Kreyszig, there exists a sequence $\left(y_m \right)_{m \in \mathbb{N} }$ , where $y_m \colon= \left( \eta_{mn} \right)_{n \in \mathbb{N} }$ for each $m \in \mathbb{N}$ , converging to $x$ in $\ell^2$ . For each $m \in \mathbb{N}$ , as $y_m = \left( \eta_{mn} \right)_{n \in \mathbb{N} } \in Y$ , so we must have $\eta_{m, 2n} = 0$ for all $n \in \mathbb{N}$ , that is, we must have $$ \eta_{m2} = \eta_{m4} = \eta_{m6} = \cdots = 0. $$ Now let us choose an arbitrary natural number $N$ . We show that the sequence $\left( \eta_{mN} \right)_{m \in \mathbb{N} }$ converges (in $\mathbb{R}$ or $\mathbb{C}$ as the case may be) to the number $\xi_N$ . As the sequence $\left(y_m \right)_{m \in \mathbb{N} }$ converges in $\ell^2$ to the point $x$ , so given any real number $\varepsilon > 0$ , there exists a natural number $M$ such that $$ d_{\ell^2} \left( y_m, x \right) < \varepsilon $$ for all $m \in \mathbb{N}$ such that $m > M$ .    But we have $x = \left( \xi_n \right)_{n \in \mathbb{N} }$ , and, for each $m \in \mathbb{N}$ , we have $y_m = \left( \eta_{m n} \right)_{n \in \mathbb{N} }$ . So we can conclude that $$ \sqrt{ \sum_{n = 1}^\infty \left\lvert \eta_{m n} - \xi_n \right\rvert^2 } < \varepsilon $$ for all $m \in \mathbb{N}$ such that $m > M$ . And, so we obtain $$ \left\lvert \eta_{mN} - \xi_N \right\rvert = \sqrt{ \left\lvert \eta_{mN} - \xi_N \right\rvert  } \leq \sqrt{ \sum_{n = 1}^\infty \left\lvert \eta_{mn} - \xi_n \right\rvert^2 } < \varepsilon $$ for all $m \in \mathbb{N}$ such that $m > M$ . Thus it follows that the sequence $\left( \eta_{m N} \right)_{m \in \mathbb{N} }$ converges to $\xi_N$ . Now since for each $N \in \mathbb{N}$ , we have $$ \xi_N = \lim_{m \to \infty} \eta_{m N}, $$ and since for each $N \in \mathbb{N}$ , we have $\eta_{m, 2N} = 0$ ,    therefore we must have $$ \xi_{2N} = 0$$ for all $N \in \mathbb{N}$ , which shows that $x = \left( \xi_{n} \right)_{n \in \mathbb{N} }$ is in $Y$ . But this $x$ was an arbitrary element of the closure of $Y$ in $\ell^2$ . Thus it follows that $\mbox{Cl}(Y) \subset Y \subset \mbox{Cl}(Y)$ . Hence $Y$ is closed.","Let be the subset of given by Then is a closed subspace of the Hilbert space , and so is also complete. What is ? Here is the relevant definition: Let be an inner product space, and let be a non-empty subset of . Then the orthogonal complement of is defined as follows: My effort: By definition, My intuition is that Is it so? If yes, then how to prove this rigorously? Another approach: As is a Hilbert space and as is a closed subspace of , so by Theorem 3.3-4 in the book Introductory Functional Analysis With Applications by Erwine Kreyszig. This means that every has a unique representation Now  if , then, by definition, the series of non-negative real numbers converges in . Therefore, the sequence of the partial sums of the series , which is also monotonically increasing, is convergent. Hence this sequence is bounded (from above). That is, for each , we have and exists in , and Now let be ths sequence of (real or complex) numbers defined as follows: For each , let Then the sequence of the partial sums of the series is given by and for all . And, this sequence, being a sequence of sums of non-negative real numbers, is also monotonically increasing and  bounded above by the sum of the convergent series . So the sequence is convergent, which means that the series converges. Therefore, is in .    So from the definition of we can conclude that . Now let be the sequence of (real or complex) numbers defined as follows: For each , let Then , and so , because is closed under addition and scalar multiplication. Another way to show that is in is to show that the series of real numbers converges in . Now the sequence of the partial sums of the series is  given by and for all . Thus this sequence, being a sequence of sums of non-negative real numbers, is monotonically increasing and bounded above by the sum of the series , and  so this sequence is convergent, which implies that . Thus and and so and so also. In fact, we can also show that for all . Thus . But so far I have only been able to show that the subspace is contained in . Is what I have done so far correct, logically sound, and rigorous enough? If so, then how to complete the above two arguments? If not, then where do I lack? P.S.: In order to show that is indeed closed in , let us suppose that is an element in the closure of in . Then, by Theorem 1.4-6 (a) in Kreyszig, there exists a sequence , where for each , converging to in . For each , as , so we must have for all , that is, we must have Now let us choose an arbitrary natural number . We show that the sequence converges (in or as the case may be) to the number . As the sequence converges in to the point , so given any real number , there exists a natural number such that for all such that .    But we have , and, for each , we have . So we can conclude that for all such that . And, so we obtain for all such that . Thus it follows that the sequence converges to . Now since for each , we have and since for each , we have ,    therefore we must have for all , which shows that is in . But this was an arbitrary element of the closure of in . Thus it follows that . Hence is closed.","Y \ell^2  Y \colon= \left\{ \ \left( \xi_n \right)_{n \in \mathbb{N}} \in \ell^2 \ \colon \ \xi_{2n} = 0 \ \mbox{ for all } \ n \in \mathbb{N} \ \right\}.  Y \ell^2 Y Y^\perp X M X M^\perp M  M \colon= \left\{ \ x \in X \ \colon \  \langle x, y \rangle = 0 \ \mbox{ for all } \ y \in M \ \right\}.  \begin{align}
& Y^\perp \\
&= \left\{ \ x \in \ell^2 \ \colon \ \langle x, y \rangle = 0 \ \forall y \in Y \ \right\} \\
&= \left\{ \ \left( \xi_n \right)_{n \in \mathbb{N} } \in \ell^2 \ \colon \ \left\langle \left( \xi_n \right)_{n \in \mathbb{N} }, \left( \eta_n \right)_{n \in \mathbb{N} } \right\rangle = 0 \ \forall \left( \eta_n \right)_{n \in \mathbb{N} } \in Y \ \right\} \\
&= \left\{ \ \left( \xi_n \right)_{n \in \mathbb{N} } \in \ell^2 \ \colon \ \sum_{n = 1}^\infty  \xi_n  \overline{  \eta_n} = 0 \ \forall \left( \eta_n \right)_{n \in \mathbb{N} } \in Y \ \right\} \\ 
&= \left\{ \ \left( \xi_n \right)_{n \in \mathbb{N} } \in \ell^2 \ \colon \ \sum_{n = 1}^\infty  \xi_n  \overline{  \eta_n} = 0 \ \forall \left( \eta_n \right)_{n \in \mathbb{N} } \in \ell^2 \ \mbox{ such that } \ \eta_{2n} = 0 \ \forall n \in \mathbb{N}  \ \right\} \\
&= \left\{ \ \left( \xi_n \right)_{n \in \mathbb{N} } \in \ell^2 \ \colon \ \sum_{n = 1}^\infty  \xi_{2n-1}  \overline{  \eta_{2n-1} } = 0 \ \forall \left( \eta_n \right)_{n \in \mathbb{N} } \in \ell^2 \ \mbox{ such that } \ \eta_{2n} = 0 \ \forall n \in \mathbb{N}  \ \right\}. 
\end{align} Y^\perp = \left\{ \ \left( \xi_n \right)_{n \in \mathbb{N} } \in \ell^2 \ \colon \ \xi_{2n-1} = 0 \ \forall n \in \mathbb{N}  \ \right\}. \ell^2 Y \ell^2  \ell^2 = Y \oplus Y^\perp,  x \in \ell^2  x = y+z, \ \mbox{ where } \ y \in Y \ \mbox{ and } \ z \in Y^\perp.  x \colon= \left( \xi_n \right)_{n \in \mathbb{N}} \in \ell^2 \sum \left\lvert \xi_n \right\rvert^2 \mathbb{R}  \left( \sum_{j=1}^n \left\lvert \xi_j \right\rvert^2 \right)_{n \in \mathbb{N}}  \sum \left\lvert \xi_n \right\rvert^2 n \in \mathbb{N}  \sum_{j=1}^n \left\lvert \xi_j \right\rvert^2 \leq \sum_{j=1}^{n+1} \left\lvert \xi_j \right\rvert^2,  \lim_{n \to \infty} \sum_{j=1}^n \left\lvert \xi_j \right\rvert^2 \mathbb{R}  \lim_{n \to \infty} \sum_{j=1}^n \left\lvert \xi_j \right\rvert^2 = \sup \left\{ \ \sum_{j=1}^n \left\lvert \xi_j \right\rvert^2 \ \colon \ n \in \mathbb{N} \ \right\} < +\infty.  y \colon= \left( \eta_n \right)_{n \in \mathbb{N} } n \in \mathbb{N}  \eta_{2n-1} \colon= \xi_{2n-1} \ \mbox{ and } \eta_{2n} \colon= 0.   \left( \sum_{j=1}^n \left\lvert \eta_j \right\rvert^2 \right)_{n \in \mathbb{N} }  \sum \left\lvert \eta_j \right\rvert^2  \sum_{j=1}^{2n-1} \left\lvert \eta_j \right\rvert^2 = \sum_{j=1}^n \left\lvert \xi_{2j-1} \right\rvert^2,   \sum_{j=1}^{2n} \left\lvert \eta_j \right\rvert^2 = \sum_{j=1}^n \left\lvert \xi_{2j-1} \right\rvert^2  n \in \mathbb{N}  \sum \left\lvert \xi_n \right\rvert^2  \left( \sum_{j=1}^n \left\lvert \eta_j \right\rvert^2 \right)_{n \in \mathbb{N} }   \sum  \left\lvert \eta_j \right\rvert^2 y \colon= \left( \eta_n \right)_{n \in \mathbb{N} } \ell^2 Y y \in Y z \colon= \left( \zeta_n \right)_{n \in \mathbb{N}} n \in \mathbb{N}  \zeta_{2n-1} \colon= 0 \ \mbox{ and } \ \zeta_{2n} \colon= \xi_{2n}.  x = y+z z = x-y \in \ell^2 \ell^2 z \colon= \left( \zeta_n \right)_{n \in \mathbb{N}} \ell^2 \sum \left\lvert \zeta_n \right\rvert^2 \mathbb{R}  \left( \sum_{j=1}^n  \left\lvert \zeta_j \right\rvert^2 \right)_{n \in \mathbb{N} } \sum \left\lvert \zeta_n \right\rvert^2  \sum_{j=1}^{2n-1}  \left\lvert \zeta_j \right\rvert^2 = \sum_{j=1}^{2n-2}  \left\lvert \xi_{2j} \right\rvert^2,     \sum_{j=1}^{2n}  \left\lvert \zeta_j \right\rvert^2 = \sum_{j=1}^{2n}  \left\lvert \xi_{2j} \right\rvert^2    n \in \mathbb{N} \sum \left\lvert \xi_n \right\rvert^2 z \in \ell^2  y = \left( \eta_1, \eta_2, \eta_3, \ldots \right) = \left( \xi_1, 0, \xi_3, 0, \xi_5, 0, \ldots \right),   z = \left( \zeta_1, \zeta_2, \zeta_3, \ldots \right) = \left( 0, \xi_2, 0, \xi_4, 0, \xi_6,  \ldots \right),    \langle y, z \rangle = \sum_{n=1}^\infty \eta_n \overline{\zeta_n} = 0,  \langle y, z \rangle = 0 \langle z, v \rangle = 0 v \in Y z \in Y^\perp  Z = \left\{  \ \left( \xi_n \right)_{n \in \mathbb{N} }  \in \ell^2 \ \colon \ \xi_{2n-1} = 0 \ \forall \ n \in \mathbb{N} \ \right\} Y^\perp Y \ell^2 x \colon= \left( \xi_n \right)_{n \in \mathbb{N} } Y \ell^2 \left(y_m \right)_{m \in \mathbb{N} } y_m \colon= \left( \eta_{mn} \right)_{n \in \mathbb{N} } m \in \mathbb{N} x \ell^2 m \in \mathbb{N} y_m = \left( \eta_{mn} \right)_{n \in \mathbb{N} } \in Y \eta_{m, 2n} = 0 n \in \mathbb{N}  \eta_{m2} = \eta_{m4} = \eta_{m6} = \cdots = 0.  N \left( \eta_{mN} \right)_{m \in \mathbb{N} } \mathbb{R} \mathbb{C} \xi_N \left(y_m \right)_{m \in \mathbb{N} } \ell^2 x \varepsilon > 0 M  d_{\ell^2} \left( y_m, x \right) < \varepsilon  m \in \mathbb{N} m > M x = \left( \xi_n \right)_{n \in \mathbb{N} } m \in \mathbb{N} y_m = \left( \eta_{m n} \right)_{n \in \mathbb{N} }  \sqrt{ \sum_{n = 1}^\infty \left\lvert \eta_{m n} - \xi_n \right\rvert^2 } < \varepsilon  m \in \mathbb{N} m > M  \left\lvert \eta_{mN} - \xi_N \right\rvert = \sqrt{ \left\lvert \eta_{mN} - \xi_N \right\rvert  } \leq \sqrt{ \sum_{n = 1}^\infty \left\lvert \eta_{mn} - \xi_n \right\rvert^2 } < \varepsilon  m \in \mathbb{N} m > M \left( \eta_{m N} \right)_{m \in \mathbb{N} } \xi_N N \in \mathbb{N}  \xi_N = \lim_{m \to \infty} \eta_{m N},  N \in \mathbb{N} \eta_{m, 2N} = 0  \xi_{2N} = 0 N \in \mathbb{N} x = \left( \xi_{n} \right)_{n \in \mathbb{N} } Y x Y \ell^2 \mbox{Cl}(Y) \subset Y \subset \mbox{Cl}(Y) Y","['real-analysis', 'functional-analysis', 'analysis', 'hilbert-spaces', 'inner-products']"
82,"Let $f:[0,\infty)\to \mathbb R$ be Continuous and strictly decreasing function such that : $\lim_{x\to \infty}f(x)=0$ [duplicate]",Let  be Continuous and strictly decreasing function such that :  [duplicate],"f:[0,\infty)\to \mathbb R \lim_{x\to \infty}f(x)=0","This question already has an answer here : Divergence of an improper integral (1 answer) Closed 7 years ago . Let $f:[0,\infty)\to \mathbb R$  be Continuous and strictly decreasing function such that : $\lim_{x\to \infty}f(x)=0$ prove  $$\int_0^\infty \frac{f(x)-f(x+1)}{f(x)}$$ Diverges. I can not prove. please help .","This question already has an answer here : Divergence of an improper integral (1 answer) Closed 7 years ago . Let $f:[0,\infty)\to \mathbb R$  be Continuous and strictly decreasing function such that : $\lim_{x\to \infty}f(x)=0$ prove  $$\int_0^\infty \frac{f(x)-f(x+1)}{f(x)}$$ Diverges. I can not prove. please help .",,['real-analysis']
83,Conditions for integrals to be equal,Conditions for integrals to be equal,,"Suppose that $f, g,h:[0,1]\rightarrow \mathbb{R}$ are functions that satisfy $f,g\geq 0$ and  $$\int_0^1 f(t)h(t)dt = \int_{0}^1 g(t)h(t)dt.$$ What are necessary and sufficient conditions on $h$ to ensure that $$\int_0^1f(t)dt = \int_0^1g(t)dt?$$ EDIT: ( Some thoughts: ) I thought maybe if $h>0$, this would be enough. However, I was shown that, for example, if $ f = 1/h $ and $g = 1/\left(\int h\right)$, then $$\int fh = \int gh.$$ In this case, for $\int f = \int g$ to be satisfied,  $h$ must satisfy $$\int \frac{1}{h} = \frac{1}{\int h}$$ and of course many $h>0$ do not satisfy that condition (e.g. $h(t) = 1+t$). Since, as user251257 points out, $h(t) \equiv1$ is sufficient, I was curious what the answer to the above might be. ( Where this question came from ) I originally came across this question when looking at what can be said when you have solutions $y_i$ and $y_2$ to the Ricatti equations $y_i' + y^2_i + r_i =0$ on the interval $[a,b]$ that satisfy $y_1(a) = y_2(a)$ and $y_1(b) = y_2(b)$. In this case, you can write $$0 = g(y_2(t)-y_1(t))|_a^b = \int_a^b g(r_1(t)-r_2(t))dt$$ where $g$ is a function that satisfies $g' = (y_1 + y_2)g$. So, I wanted to claim that this means $\int_a^b r_1(t)dt = \int_a^b r_2(t)dt$, and in the process, I realized I didn't know the answer to the question above.","Suppose that $f, g,h:[0,1]\rightarrow \mathbb{R}$ are functions that satisfy $f,g\geq 0$ and  $$\int_0^1 f(t)h(t)dt = \int_{0}^1 g(t)h(t)dt.$$ What are necessary and sufficient conditions on $h$ to ensure that $$\int_0^1f(t)dt = \int_0^1g(t)dt?$$ EDIT: ( Some thoughts: ) I thought maybe if $h>0$, this would be enough. However, I was shown that, for example, if $ f = 1/h $ and $g = 1/\left(\int h\right)$, then $$\int fh = \int gh.$$ In this case, for $\int f = \int g$ to be satisfied,  $h$ must satisfy $$\int \frac{1}{h} = \frac{1}{\int h}$$ and of course many $h>0$ do not satisfy that condition (e.g. $h(t) = 1+t$). Since, as user251257 points out, $h(t) \equiv1$ is sufficient, I was curious what the answer to the above might be. ( Where this question came from ) I originally came across this question when looking at what can be said when you have solutions $y_i$ and $y_2$ to the Ricatti equations $y_i' + y^2_i + r_i =0$ on the interval $[a,b]$ that satisfy $y_1(a) = y_2(a)$ and $y_1(b) = y_2(b)$. In this case, you can write $$0 = g(y_2(t)-y_1(t))|_a^b = \int_a^b g(r_1(t)-r_2(t))dt$$ where $g$ is a function that satisfies $g' = (y_1 + y_2)g$. So, I wanted to claim that this means $\int_a^b r_1(t)dt = \int_a^b r_2(t)dt$, and in the process, I realized I didn't know the answer to the question above.",,"['calculus', 'real-analysis', 'integration', 'measure-theory', 'lebesgue-integral']"
84,"Irrationality measure of $\alpha+\beta$, $\alpha\beta$","Irrationality measure of ,",\alpha+\beta \alpha\beta,"Let $\alpha$ and $\beta$ be real numbers with finite irrationality measures . My question is: Are the irrationality measures of $\alpha+\beta$ and $\alpha\beta$ also finite? I tried using triangle inequality  $$ \left| \alpha - \frac {p_1}{q_1} + \beta - \frac{p_2}{q_2}\right| \geq \left| \left| \alpha - \frac{p_1}{q_1}\right|-\left|\beta-\frac{p_2}{q_2}\right|\right|$$for the sum, but this doesn't lead me anywhere. For the product, I am not even sure where to start. I wonder if this is well-known. If so, I am looking for a reference. If this is true, we would have results as $e+\pi$ has a finite irrationality measure. But, I could not find any reference for such results. My progress so far is: If one of $\alpha$, $\beta$ is  a rational number, then the result is true. From Hata's result , we have  $\pi/\sqrt{k}$ has finite irrationality measure for any integer $k\geq 1$. So, $\pi/\sqrt{k} +1$ has a finite irrationality measure. But, that would not necessarily imply that $\pi+\sqrt{k}$ has finite irrationality measure. Added on 2017 May 22 In fact $\pi + \sqrt{k}$ has a finite irrationality measure. This can be done by Baker's theorem. [Baker's Theorem] If $\alpha_1, \ldots, \alpha_n$ are algebraic numbers, not $0$ or $1$. If $\log\alpha_1, \ldots, \log\alpha_n$ are linearly independent over $\mathbb{Q}$, then $1, \log\alpha_1, \ldots, \log\alpha_n$ are linearly independent over $\overline{\mathbb{Q}}$.    Moreover, there is an effectively computable constant $C>0$ such that for any algebraic $\beta_0, \ldots, \beta_n$ not all zero,    $$ |\beta_0+\beta_1\log \alpha_1 + \cdots + \beta_n \log \alpha_n |\geq H^{-C} $$   where $H=\max(h(\beta_i))$. For positive integers $p, q$, we have $$ |q(\pi+\sqrt k)-p|\geq H^{-C} $$ where $H=\max(h(q\sqrt k-p) , q^2)\ll q^{A}$ for some effectively computable  constant $A>0$. This shows that $\pi+\sqrt k$ has a finite irrationality measure. This can be generalized that $\pi+\alpha$, $\pi/\alpha$ has a  finite irrationality measure for any nonzero algebraic $\alpha$.","Let $\alpha$ and $\beta$ be real numbers with finite irrationality measures . My question is: Are the irrationality measures of $\alpha+\beta$ and $\alpha\beta$ also finite? I tried using triangle inequality  $$ \left| \alpha - \frac {p_1}{q_1} + \beta - \frac{p_2}{q_2}\right| \geq \left| \left| \alpha - \frac{p_1}{q_1}\right|-\left|\beta-\frac{p_2}{q_2}\right|\right|$$for the sum, but this doesn't lead me anywhere. For the product, I am not even sure where to start. I wonder if this is well-known. If so, I am looking for a reference. If this is true, we would have results as $e+\pi$ has a finite irrationality measure. But, I could not find any reference for such results. My progress so far is: If one of $\alpha$, $\beta$ is  a rational number, then the result is true. From Hata's result , we have  $\pi/\sqrt{k}$ has finite irrationality measure for any integer $k\geq 1$. So, $\pi/\sqrt{k} +1$ has a finite irrationality measure. But, that would not necessarily imply that $\pi+\sqrt{k}$ has finite irrationality measure. Added on 2017 May 22 In fact $\pi + \sqrt{k}$ has a finite irrationality measure. This can be done by Baker's theorem. [Baker's Theorem] If $\alpha_1, \ldots, \alpha_n$ are algebraic numbers, not $0$ or $1$. If $\log\alpha_1, \ldots, \log\alpha_n$ are linearly independent over $\mathbb{Q}$, then $1, \log\alpha_1, \ldots, \log\alpha_n$ are linearly independent over $\overline{\mathbb{Q}}$.    Moreover, there is an effectively computable constant $C>0$ such that for any algebraic $\beta_0, \ldots, \beta_n$ not all zero,    $$ |\beta_0+\beta_1\log \alpha_1 + \cdots + \beta_n \log \alpha_n |\geq H^{-C} $$   where $H=\max(h(\beta_i))$. For positive integers $p, q$, we have $$ |q(\pi+\sqrt k)-p|\geq H^{-C} $$ where $H=\max(h(q\sqrt k-p) , q^2)\ll q^{A}$ for some effectively computable  constant $A>0$. This shows that $\pi+\sqrt k$ has a finite irrationality measure. This can be generalized that $\pi+\alpha$, $\pi/\alpha$ has a  finite irrationality measure for any nonzero algebraic $\alpha$.",,"['real-analysis', 'real-numbers', 'irrational-numbers', 'diophantine-approximation']"
85,If $x_n \geq 0$ for all n $\in N$ and $\lim((-1)^nx_n)$ exists. Show that $x_n$ converges.,If  for all n  and  exists. Show that  converges.,x_n \geq 0 \in N \lim((-1)^nx_n) x_n,"If $x_n \geq 0$ for all n $\in N$ and $lim((-1)^nx_n)$ exists. Show that $x_n$ converges. Let $\lim((-1)^nx_n)=l$ therefore, for $\epsilon>0$ $\exists k\in N $ such that $| (-1)^nx_n - l|<\epsilon/2$ $\forall n\geq k$ $\implies |x_n + l| < \epsilon/2 $ $\forall n\geq k$ & n is odd $-\epsilon/2 - l<x_n<\epsilon/2-l$ $\forall n\geq k$ & n is odd $\hspace{5mm} (1) $ also, $|x_n - l| < \epsilon/2 $ $\forall n\geq k$ & n is even $\implies-\epsilon/2 +l<x_n<\epsilon/2 + l$ $\forall n\geq k$ & n is even $\hspace{5mm} (2)$ from (1) and (2), $-\epsilon  < x_n < \epsilon$ $\forall n\geq k$ $\implies |x_n - 0| < \epsilon $ $\forall n\geq k$ Hence $\lim(x_n) = 0 $ Is this argument correct?","If for all n and exists. Show that converges. Let therefore, for such that & n is odd & n is odd also, & n is even & n is even from (1) and (2), Hence Is this argument correct?",x_n \geq 0 \in N lim((-1)^nx_n) x_n \lim((-1)^nx_n)=l \epsilon>0 \exists k\in N  | (-1)^nx_n - l|<\epsilon/2 \forall n\geq k \implies |x_n + l| < \epsilon/2  \forall n\geq k -\epsilon/2 - l<x_n<\epsilon/2-l \forall n\geq k \hspace{5mm} (1)  |x_n - l| < \epsilon/2  \forall n\geq k \implies-\epsilon/2 +l<x_n<\epsilon/2 + l \forall n\geq k \hspace{5mm} (2) -\epsilon  < x_n < \epsilon \forall n\geq k \implies |x_n - 0| < \epsilon  \forall n\geq k \lim(x_n) = 0 ,['real-analysis']
86,An arctan series with roots $\sum_{n=1}^{\infty} \frac{1}{\sqrt{n}} \arctan \frac{1}{n \sqrt{n}}$,An arctan series with roots,\sum_{n=1}^{\infty} \frac{1}{\sqrt{n}} \arctan \frac{1}{n \sqrt{n}},Solving an exercise today I came across this series and I'm curious to know if we can evaluate it. Here it is: $$\sum_{n=1}^{\infty} \frac{1}{\sqrt{n}} \arctan \frac{1}{n \sqrt{n}}$$ It rings me bells about some other series with arctan's I have come across but I could not see any similarity on how to begin. Wolfram gives an approximation of $1.41379$. Note that $\sqrt{2} \approx 1.4142$. Too sad !!,Solving an exercise today I came across this series and I'm curious to know if we can evaluate it. Here it is: $$\sum_{n=1}^{\infty} \frac{1}{\sqrt{n}} \arctan \frac{1}{n \sqrt{n}}$$ It rings me bells about some other series with arctan's I have come across but I could not see any similarity on how to begin. Wolfram gives an approximation of $1.41379$. Note that $\sqrt{2} \approx 1.4142$. Too sad !!,,"['real-analysis', 'sequences-and-series']"
87,The sum of fractional powers $\sum\limits_{k=1}^x k^t$.,The sum of fractional powers .,\sum\limits_{k=1}^x k^t,"This post is a continuation of Generalization of the Bernoulli polynomials ( in relation to the Index ) , the definition of the Bernoulli polynomial $B_t(x)$ with $|x|<1$ has an extension through $B_t(x+1)=B_t(x)+t x^{t-1}$ . Two equivalent definitions for $B_t(x)$ with $|x|<1$ : $$B_t(x):=-t\zeta(1-t,x)$$ or \begin{align*}   B_t(x+1):=&-\frac{2\Gamma(1+t)}{(2\pi)^t}\cos \left( \frac{\pi t}{2} \right)  \sum_{k=0}^\infty (-1)^k \frac{(2\pi x)^{2k}}{(2k)!}\zeta(t-2k) \\ &-\frac{2\Gamma(1+t)}{(2\pi)^t}\sin \left( \frac{\pi t}{2} \right)  \sum_{k=0}^\infty (-1)^k \frac{(2\pi x)^{2k+1}}{(2k+1)!}\zeta(t-1-2k)  \end{align*} with $-t\in\mathbb{R}\setminus\mathbb{N}$ . With https://www.researchgate.net/publication/238803313_Bernoulli_numbers_and_polynomials_of_arbitrary_complex_indices , page 86, Theorem 5, using equation (11) with the lower limit of $1$ instead of $0$ ( $k=1$ instead of $k=0$ )  the formula for the sum of fractional powers is $$S_x(t):=\sum\limits_{k=1}^x k^t =\frac{B_{t+1}(x+1)-B_{t+1}(1)}{t+1}$$ with $x\in\mathbb{N}_0$ and $t\in\mathbb{R}_0^+$ (general: $t$ can be complex but I don’t need this possibility here). The right side may be differentiated by $x$ and therefore one can write $$\frac{\partial}{\partial x} S_x(t)=B_t(x+1)$$ On the other hand differentiated by $t$ and the definition with $M_x(t):=\prod\limits_{k=1}^x k^{k^t} $ it's $$\ln M_x(t)=\frac{\partial}{\partial t}S_x(t)=\frac{\partial}{\partial t}\frac{B_{t+1}(x+1)-B_{t+1}(1)}{t+1}$$ Together one gets (by exchanging the derivatives, which is possible here) $$\frac{\partial}{\partial t}B_t(x+1)=\frac{\partial}{\partial x}\ln M_x(t)$$ Note : Perhaps this equation becomes a bit clearer if one looks at $$\frac{\partial}{\partial t}\Delta B_t(x)=\frac{\partial}{\partial x}\Delta \ln M_{x-1}(t)$$ with $\Delta B_t(x):=B_t(x+1)-B_t(x)=tx^{t-1}$ and $\Delta \ln M_x(t):=\ln M_{x+1}(t)-\ln M_x(t)=(x+1)^t\ln(x+1)$ . The problem now is : I need a formula for $\ln M_x(t)$ or $M_x(t)$ , independend of $B_t(x)$ (otherwise it's a trivial identity), where $x$ and $t$ are variable. It could be a series of (more or less known) functions of $x$ (or perhaps $x$ and $t$ ) which becomes a sum/term for $t\in\mathbb{N}$ - similar to $B_t(x)$ . Alternative: To proof that the two definitions above for $B_t(x)$ are indeed equivalent (a link to the literatur is enough). Note : The Euler-MacLaurin-formula can perhaps give a formula for $\ln M_x(t)$ . Does someone know a link, where this is computed ? Addition : Maybe http://ac.els-cdn.com/S0377042798001927/1-s2.0-S0377042798001927-main.pdf?_tid=36ead884-7132-11e6-ac53-00000aab0f6b&acdnat=1472837296_60501a990f4d37792d48c76ad38c7e4b , page 198, equation (21), can help. (I will see.) An application example with $\ln M_x(1)$ : The fourier series of $B_t(x)$ is $$ \Re \left( \sum\limits_{k=1}^{\infty}{\frac{e^{i2\pi kx}}{\left( ik \right) ^t}} \right) =\frac{\left( 2\pi \right) ^t}{2\Gamma \left( 1+t \right)}B_t\left( x \right)  $$ for $|x|<1$ and $t>0$ . It is known, that $\frac{d}{dx}\ln M_x(1)=-\ln\sqrt{2\pi}+\frac{1}{2}+x+\ln\Gamma(1+x)$ . Using $$\frac{\partial}{\partial t}B_t(x)|_{t=1}=\frac{d}{dx}\ln M_{x-1}(1)$$ and derivating the fourier series of $B_t(x)$ (above) by $t$ and having regard to $(\ln\Gamma(1+t))'|_{t=1}=1-\gamma$ one gets $$\sum_{k=1}^{\infty}{\frac{\ln k}{k}}\sin \left( 2\pi kx \right) =\frac{\pi}{2}\left( \ln \frac{\Gamma \left( x \right)}{\Gamma \left( 1-x \right)}-\left( 1-2x \right) \left( \gamma +\ln \left( 2\pi \right) \right) \right)  $$ which can be seen in http://reader.digitale-sammlungen.de/en/fs1/object/display/bsb10525489_00011.html?zoom=1.0 (on the top of page 4) and in http://arxiv.org/pdf/1309.3824.pdf (page 30, formula 65.) A  second application example where I use $\frac{d}{dx}\ln M_x(m+1)|_{x=0}$ with $m\in\mathbb{N}_0$ : Adamchik had computed $$\zeta’(-m)=\frac{B_{m+1}H_m}{m+1}-A_m$$ where $B_n$ are the Bernoulli-numbers, $H_n$ are the harmonic numbers and $A_n$ are the generalized Glaisher-Kinkelin constants. See e.g. http://www.sciencedirect.com/science/article/pii/S0377042798001927 (Article; last page, equation (24)) . Dissolving the equation (5.4) on page 36 of https://www.fernuni-hagen.de/analysis/docs/bachelorarbeit_aschauer.pdf for $\ln M_x(k)$ , using $\frac{B_{k+1}(x+1+w_2)- B_{k+1}(1+w_2)}{k+1}$ instead of $\sum\limits_{j=1}^x (w_2+j)^k$ and setting $(w_1;w_2):=(1;0)$ results in \begin{align*}   \ln M_x(m)&=H_m\frac{B_{m+1}(x+1)- B_{m+1}(1)}{m+1}+\ln Q_m(x)+ \\ &+\sum_{k=0}^{m-1}\binom{m}{k}(-x)^{m-k}\sum_{v=0}^k \binom{k}{v}x^{k-v}(\ln A_v -\ln Q_v(x)) \end{align*} The definition of $Q_m(x)$ is (4.2) on page 13, it’s something like a modified Multiple-Gamma-Function. $\frac{d}{dx}\ln M_x(m)$ can be computed by using the differentiation rule (4.4) for the equation above. Now one gets with $B_t(1)=-t\zeta(1-t)$ and $\frac{d}{dt}B_t(1)|_{t=m}=\frac{d}{dx}\ln M_x(m)|_{x=0}$ the equation chain $$\frac{B_{m+1}(1)}{m+1}+(m+1)\zeta’(-m)= \zeta(-m)+(m+1)\zeta’(-m)=(-t\zeta(1-t))’$$ $$=\frac{d}{dt}B_t(1)|_{t=m+1}=\frac{d}{dx}\ln M_x(m+1)|_{x=0}=H_{m+1}B_{m+1}(1)-(m+1)\ln A_m$$ and this result dissolved for $\zeta’(-m)$ and took into account that $H_{m+1}-\frac{1}{m+1}=H_m$ and $H_m B_{m+1}(1)=H_m B_{m+1}$ for $m\in\mathbb{N}_0$ one gets Adamchik’s result. Most simple solution for proofing $\displaystyle \frac{\partial}{\partial t}B_t(x+1)=\frac{\partial}{\partial x}\ln M_x(t)$ by using the 2nd development  of G Cab with the  Hurwitz Zeta function: $\zeta(a,b):= \sum\limits_{k=0}^\infty (b+k)^{-a}$ $\displaystyle \frac{B_{t+1}(x+1)-B_{t+1}(1)}{t+1}=S_x(t)=\zeta(-t,1)-\zeta(-t,x+1)$ and therefore $\displaystyle \frac{\partial}{\partial t}S_x(t)=\ln M_x(t)=\sum\limits_{k=0}^\infty (k+1)^t\ln(k+1) - \sum\limits_{k=0}^\infty (k+x+1)^t\ln (k+x+1)$ $\displaystyle \frac{\partial}{\partial x}S_x(t)= B_t(x+1)=-t\zeta(1-t,x+1)\,$ (as mentioned by gammatester , first link above) \begin{align*}   \frac{\partial}{\partial t}B_t(x+1)&= \frac{\partial}{\partial t}\frac{\partial}{\partial x}(\zeta(-t,1)-\zeta(-t,x+1)) \\ &=\frac{\partial}{\partial x}\frac{\partial}{\partial t}(\zeta(-t,1)-\zeta(-t,x+1))=\frac{\partial}{\partial x}\ln M_x(t)  \end{align*} Note : Substituting $B_t(x)$ and $\ln M_x(t)$ by other formulas are leading to non-trivial equations (as shown in the application examples above).","This post is a continuation of Generalization of the Bernoulli polynomials ( in relation to the Index ) , the definition of the Bernoulli polynomial with has an extension through . Two equivalent definitions for with : or with . With https://www.researchgate.net/publication/238803313_Bernoulli_numbers_and_polynomials_of_arbitrary_complex_indices , page 86, Theorem 5, using equation (11) with the lower limit of instead of ( instead of )  the formula for the sum of fractional powers is with and (general: can be complex but I don’t need this possibility here). The right side may be differentiated by and therefore one can write On the other hand differentiated by and the definition with it's Together one gets (by exchanging the derivatives, which is possible here) Note : Perhaps this equation becomes a bit clearer if one looks at with and . The problem now is : I need a formula for or , independend of (otherwise it's a trivial identity), where and are variable. It could be a series of (more or less known) functions of (or perhaps and ) which becomes a sum/term for - similar to . Alternative: To proof that the two definitions above for are indeed equivalent (a link to the literatur is enough). Note : The Euler-MacLaurin-formula can perhaps give a formula for . Does someone know a link, where this is computed ? Addition : Maybe http://ac.els-cdn.com/S0377042798001927/1-s2.0-S0377042798001927-main.pdf?_tid=36ead884-7132-11e6-ac53-00000aab0f6b&acdnat=1472837296_60501a990f4d37792d48c76ad38c7e4b , page 198, equation (21), can help. (I will see.) An application example with : The fourier series of is for and . It is known, that . Using and derivating the fourier series of (above) by and having regard to one gets which can be seen in http://reader.digitale-sammlungen.de/en/fs1/object/display/bsb10525489_00011.html?zoom=1.0 (on the top of page 4) and in http://arxiv.org/pdf/1309.3824.pdf (page 30, formula 65.) A  second application example where I use with : Adamchik had computed where are the Bernoulli-numbers, are the harmonic numbers and are the generalized Glaisher-Kinkelin constants. See e.g. http://www.sciencedirect.com/science/article/pii/S0377042798001927 (Article; last page, equation (24)) . Dissolving the equation (5.4) on page 36 of https://www.fernuni-hagen.de/analysis/docs/bachelorarbeit_aschauer.pdf for , using instead of and setting results in The definition of is (4.2) on page 13, it’s something like a modified Multiple-Gamma-Function. can be computed by using the differentiation rule (4.4) for the equation above. Now one gets with and the equation chain and this result dissolved for and took into account that and for one gets Adamchik’s result. Most simple solution for proofing by using the 2nd development  of G Cab with the  Hurwitz Zeta function: and therefore (as mentioned by gammatester , first link above) Note : Substituting and by other formulas are leading to non-trivial equations (as shown in the application examples above).","B_t(x) |x|<1 B_t(x+1)=B_t(x)+t x^{t-1} B_t(x) |x|<1 B_t(x):=-t\zeta(1-t,x) \begin{align*}  
B_t(x+1):=&-\frac{2\Gamma(1+t)}{(2\pi)^t}\cos \left( \frac{\pi t}{2} \right)  \sum_{k=0}^\infty (-1)^k \frac{(2\pi x)^{2k}}{(2k)!}\zeta(t-2k) \\
&-\frac{2\Gamma(1+t)}{(2\pi)^t}\sin \left( \frac{\pi t}{2} \right)  \sum_{k=0}^\infty (-1)^k \frac{(2\pi x)^{2k+1}}{(2k+1)!}\zeta(t-1-2k) 
\end{align*} -t\in\mathbb{R}\setminus\mathbb{N} 1 0 k=1 k=0 S_x(t):=\sum\limits_{k=1}^x k^t =\frac{B_{t+1}(x+1)-B_{t+1}(1)}{t+1} x\in\mathbb{N}_0 t\in\mathbb{R}_0^+ t x \frac{\partial}{\partial x} S_x(t)=B_t(x+1) t M_x(t):=\prod\limits_{k=1}^x k^{k^t}  \ln M_x(t)=\frac{\partial}{\partial t}S_x(t)=\frac{\partial}{\partial t}\frac{B_{t+1}(x+1)-B_{t+1}(1)}{t+1} \frac{\partial}{\partial t}B_t(x+1)=\frac{\partial}{\partial x}\ln M_x(t) \frac{\partial}{\partial t}\Delta B_t(x)=\frac{\partial}{\partial x}\Delta \ln M_{x-1}(t) \Delta B_t(x):=B_t(x+1)-B_t(x)=tx^{t-1} \Delta \ln M_x(t):=\ln M_{x+1}(t)-\ln M_x(t)=(x+1)^t\ln(x+1) \ln M_x(t) M_x(t) B_t(x) x t x x t t\in\mathbb{N} B_t(x) B_t(x) \ln M_x(t) \ln M_x(1) B_t(x) 
\Re \left( \sum\limits_{k=1}^{\infty}{\frac{e^{i2\pi kx}}{\left( ik \right) ^t}} \right) =\frac{\left( 2\pi \right) ^t}{2\Gamma \left( 1+t \right)}B_t\left( x \right) 
 |x|<1 t>0 \frac{d}{dx}\ln M_x(1)=-\ln\sqrt{2\pi}+\frac{1}{2}+x+\ln\Gamma(1+x) \frac{\partial}{\partial t}B_t(x)|_{t=1}=\frac{d}{dx}\ln M_{x-1}(1) B_t(x) t (\ln\Gamma(1+t))'|_{t=1}=1-\gamma \sum_{k=1}^{\infty}{\frac{\ln k}{k}}\sin \left( 2\pi kx \right) =\frac{\pi}{2}\left( \ln \frac{\Gamma \left( x \right)}{\Gamma \left( 1-x \right)}-\left( 1-2x \right) \left( \gamma +\ln \left( 2\pi \right) \right) \right) 
 \frac{d}{dx}\ln M_x(m+1)|_{x=0} m\in\mathbb{N}_0 \zeta’(-m)=\frac{B_{m+1}H_m}{m+1}-A_m B_n H_n A_n \ln M_x(k) \frac{B_{k+1}(x+1+w_2)- B_{k+1}(1+w_2)}{k+1} \sum\limits_{j=1}^x (w_2+j)^k (w_1;w_2):=(1;0) \begin{align*}  
\ln M_x(m)&=H_m\frac{B_{m+1}(x+1)- B_{m+1}(1)}{m+1}+\ln Q_m(x)+ \\
&+\sum_{k=0}^{m-1}\binom{m}{k}(-x)^{m-k}\sum_{v=0}^k \binom{k}{v}x^{k-v}(\ln A_v -\ln Q_v(x))
\end{align*} Q_m(x) \frac{d}{dx}\ln M_x(m) B_t(1)=-t\zeta(1-t) \frac{d}{dt}B_t(1)|_{t=m}=\frac{d}{dx}\ln M_x(m)|_{x=0} \frac{B_{m+1}(1)}{m+1}+(m+1)\zeta’(-m)= \zeta(-m)+(m+1)\zeta’(-m)=(-t\zeta(1-t))’ =\frac{d}{dt}B_t(1)|_{t=m+1}=\frac{d}{dx}\ln M_x(m+1)|_{x=0}=H_{m+1}B_{m+1}(1)-(m+1)\ln A_m \zeta’(-m) H_{m+1}-\frac{1}{m+1}=H_m H_m B_{m+1}(1)=H_m B_{m+1} m\in\mathbb{N}_0 \displaystyle \frac{\partial}{\partial t}B_t(x+1)=\frac{\partial}{\partial x}\ln M_x(t) \zeta(a,b):= \sum\limits_{k=0}^\infty (b+k)^{-a} \displaystyle \frac{B_{t+1}(x+1)-B_{t+1}(1)}{t+1}=S_x(t)=\zeta(-t,1)-\zeta(-t,x+1) \displaystyle \frac{\partial}{\partial t}S_x(t)=\ln M_x(t)=\sum\limits_{k=0}^\infty (k+1)^t\ln(k+1) - \sum\limits_{k=0}^\infty (k+x+1)^t\ln (k+x+1) \displaystyle \frac{\partial}{\partial x}S_x(t)= B_t(x+1)=-t\zeta(1-t,x+1)\, \begin{align*}  
\frac{\partial}{\partial t}B_t(x+1)&= \frac{\partial}{\partial t}\frac{\partial}{\partial x}(\zeta(-t,1)-\zeta(-t,x+1)) \\
&=\frac{\partial}{\partial x}\frac{\partial}{\partial t}(\zeta(-t,1)-\zeta(-t,x+1))=\frac{\partial}{\partial x}\ln M_x(t) 
\end{align*} B_t(x) \ln M_x(t)","['real-analysis', 'bernoulli-polynomials']"
88,"Diffeomorphism preserves interior, exterior and boundary","Diffeomorphism preserves interior, exterior and boundary",,"Let $g : A \to B$ be a diffeomorphism of open sets $A, B \subseteq \mathbb{R}^n$. That is, $g$ is a bijection and both $g,g^{-1}$ are of class $C^r$ for some $r \geq 1$. Let $S$ be a subset of $A$, and let $g(S) = T$. Is it true that $g(\text{Int }S) = \text{Int }T$, $g(\text{Ext }S \cap A) = \text{Ext }T \cap B$ and $g(\text{Bd }S) = \text{Bd }T$? [Here we define $\text{Int }S$ as the union of all open sets contained in $S$, $\text{Ext }S$ as the union of all open sets disjoint from $S$, $\text{Bd }S$ as the set of points whose every neighborhood contain points from $S$ and $\mathbb{R}^n-S$. These 3 sets are disjoint and their union is $\mathbb{R}^n$.] I think it's true, and believe I have a proof: Indeed $g^{-1}$ is continuous, and since $\text{Int }S$ is open,   $g(\text{Int }S)$ is open, so $g(\text{Int }S) \subseteq \text{Int }T$.   Similarly $g$ is continuous, so $g^{-1}(\text{Int }T) \subseteq \text{Int }S$. It follows that $g(\text{Int }S) = \text{Int }T$. Similarly $g(\text{Ext }S \cap A)$ is open, and $\text{Ext }S \cap A \subseteq A - S$, so that $g(\text{Ext }S\cap A) \subseteq \text{Ext }T \cap B$. Then $g(\text{Ext }S\cap A) = \text{Ext }T \cap B$. Finally $g(A) = B$ so that $g(\text{Int }S \cup \text{Bd }S \cup (\text{Ext }S\cap A)) = \text{Int }T \cup \text{Bd }T \cup (\text{Ext }T\cap B)$. Since $g$ is a bijection, it follows that $g(\text{Bd }S) = \text{Bd }T$. But I'm having doubts because Munkres' Analysis on Manifolds requires $S$ to be compact (Theorem 18.2, page 154). Is the statement true, or is there a flaw with the above proof?","Let $g : A \to B$ be a diffeomorphism of open sets $A, B \subseteq \mathbb{R}^n$. That is, $g$ is a bijection and both $g,g^{-1}$ are of class $C^r$ for some $r \geq 1$. Let $S$ be a subset of $A$, and let $g(S) = T$. Is it true that $g(\text{Int }S) = \text{Int }T$, $g(\text{Ext }S \cap A) = \text{Ext }T \cap B$ and $g(\text{Bd }S) = \text{Bd }T$? [Here we define $\text{Int }S$ as the union of all open sets contained in $S$, $\text{Ext }S$ as the union of all open sets disjoint from $S$, $\text{Bd }S$ as the set of points whose every neighborhood contain points from $S$ and $\mathbb{R}^n-S$. These 3 sets are disjoint and their union is $\mathbb{R}^n$.] I think it's true, and believe I have a proof: Indeed $g^{-1}$ is continuous, and since $\text{Int }S$ is open,   $g(\text{Int }S)$ is open, so $g(\text{Int }S) \subseteq \text{Int }T$.   Similarly $g$ is continuous, so $g^{-1}(\text{Int }T) \subseteq \text{Int }S$. It follows that $g(\text{Int }S) = \text{Int }T$. Similarly $g(\text{Ext }S \cap A)$ is open, and $\text{Ext }S \cap A \subseteq A - S$, so that $g(\text{Ext }S\cap A) \subseteq \text{Ext }T \cap B$. Then $g(\text{Ext }S\cap A) = \text{Ext }T \cap B$. Finally $g(A) = B$ so that $g(\text{Int }S \cup \text{Bd }S \cup (\text{Ext }S\cap A)) = \text{Int }T \cup \text{Bd }T \cup (\text{Ext }T\cap B)$. Since $g$ is a bijection, it follows that $g(\text{Bd }S) = \text{Bd }T$. But I'm having doubts because Munkres' Analysis on Manifolds requires $S$ to be compact (Theorem 18.2, page 154). Is the statement true, or is there a flaw with the above proof?",,"['real-analysis', 'general-topology', 'analysis', 'differential-geometry', 'manifolds']"
89,Question on proof that closed subset of compact metric space is compact,Question on proof that closed subset of compact metric space is compact,,"Proposition. If $K$ is a compact metric space, $F \subset K$ , and $F$ is closed, then $F$ is compact. Proof. Suppose $F$ is a closed subset of the compact set $K$ . If $\mathcal{G} = \{G_\alpha\}$ is an open cover of $F$ , then $\mathcal{G}' = \mathcal{G} \cup \{F^c\}$ will be an open cover of $K$ . We are given that $K$ is compact, so let $\mathcal{H}$ be a finite subcover of $\mathcal{G}'$ for $K$ . If $F^c$ is one of the open sets in $\mathcal{H}$ , omit it. The resulting finite subcover of $\mathcal{G}$ will be an open cover of $F$ . I have a question. So I understand that we can throw out $F^c$ , as it's disjoint from $F$ . But how do we know that a finite subcover of $\mathcal{G}' = \mathcal{G} \cup \{F^c\}$ (which for all purposes and intents, just consider $\mathcal{G}$ ) for $K$ is actually an open cover of $F$ ? How do we know we aren't missing any part of $F$ , i.e. there's a part of $F$ that hasn't been covered?","Proposition. If is a compact metric space, , and is closed, then is compact. Proof. Suppose is a closed subset of the compact set . If is an open cover of , then will be an open cover of . We are given that is compact, so let be a finite subcover of for . If is one of the open sets in , omit it. The resulting finite subcover of will be an open cover of . I have a question. So I understand that we can throw out , as it's disjoint from . But how do we know that a finite subcover of (which for all purposes and intents, just consider ) for is actually an open cover of ? How do we know we aren't missing any part of , i.e. there's a part of that hasn't been covered?",K F \subset K F F F K \mathcal{G} = \{G_\alpha\} F \mathcal{G}' = \mathcal{G} \cup \{F^c\} K K \mathcal{H} \mathcal{G}' K F^c \mathcal{H} \mathcal{G} F F^c F \mathcal{G}' = \mathcal{G} \cup \{F^c\} \mathcal{G} K F F F,"['calculus', 'real-analysis']"
90,Can we extend Young's convolution inequality with $BMO$ instead of $L^\infty$,Can we extend Young's convolution inequality with  instead of,BMO L^\infty,"Obviously $\|f*g\|_{L^\infty}\leq\|f\|_{L^1}\|g\|_{L^\infty}$. Do we have the stronger bound $\|f*g\|_{L^\infty}\leq C\|f\|_{L^1}\|g\|_{BMO}$? Or almost as good, $\|f*g\|_{L^\infty}\leq C\|f\|_{H^1}\|g\|_{BMO}$? I think this might follow from the fact that interpolation still works when you replace $L^\infty$ with $BMO$. Edit: It seems to me that the first statement is false for example if you take $f=1_{[0,1]}\in L^1$ and $g(x)=\log|x|\in BMO$. Then for $x>1$, \begin{align*} f*g(x)=x\log x-(x-1)\log(x-1)-1 \end{align*} is not $L^\infty$. For the second claim, I'm tempted to use the duality inequality for $H^1$ and $BMO$ to say something like $$|\int f(t)g(x-t)dt|\leq\|f\|_{H^1}\|g\|_{BMO}$$ but I know that this is only really supposed to hold for $f\in H_0^1$.","Obviously $\|f*g\|_{L^\infty}\leq\|f\|_{L^1}\|g\|_{L^\infty}$. Do we have the stronger bound $\|f*g\|_{L^\infty}\leq C\|f\|_{L^1}\|g\|_{BMO}$? Or almost as good, $\|f*g\|_{L^\infty}\leq C\|f\|_{H^1}\|g\|_{BMO}$? I think this might follow from the fact that interpolation still works when you replace $L^\infty$ with $BMO$. Edit: It seems to me that the first statement is false for example if you take $f=1_{[0,1]}\in L^1$ and $g(x)=\log|x|\in BMO$. Then for $x>1$, \begin{align*} f*g(x)=x\log x-(x-1)\log(x-1)-1 \end{align*} is not $L^\infty$. For the second claim, I'm tempted to use the duality inequality for $H^1$ and $BMO$ to say something like $$|\int f(t)g(x-t)dt|\leq\|f\|_{H^1}\|g\|_{BMO}$$ but I know that this is only really supposed to hold for $f\in H_0^1$.",,"['real-analysis', 'harmonic-analysis']"
91,"Is there a meaning to the notation ""\arg \sup""?","Is there a meaning to the notation ""\arg \sup""?",,"When $f$ is a function on a set $A$, the notation: $\arg\max_{x\in A} f(x)$ denotes the set of elements of $A$ for which $f$ attains its maximum value. This set may be empty, for example, if $f(x)=x$ and $A=(0,1)$, then $f$ has no maximum on $A$, so: $$\arg\max_{x\in (0,1)} f(x) = \emptyset$$ However, $f$ always has a supremum (that can be $\infty$ if $f$ is unbounded), so apparently we can define an ""argument-supremum"". In this case, this would be: $$\arg\sup_{x\in (0,1)} f(x) = \{1\}$$ Can this ""arg sup"" operator be defined formally? I thought to define it in the following way: $$\arg\sup_{x\in A} f(x) = \arg \max_{x\in \text{closure}(A)}f(x)$$ Is this definition meaningful? Is it used anywhere in mathematics?","When $f$ is a function on a set $A$, the notation: $\arg\max_{x\in A} f(x)$ denotes the set of elements of $A$ for which $f$ attains its maximum value. This set may be empty, for example, if $f(x)=x$ and $A=(0,1)$, then $f$ has no maximum on $A$, so: $$\arg\max_{x\in (0,1)} f(x) = \emptyset$$ However, $f$ always has a supremum (that can be $\infty$ if $f$ is unbounded), so apparently we can define an ""argument-supremum"". In this case, this would be: $$\arg\sup_{x\in (0,1)} f(x) = \{1\}$$ Can this ""arg sup"" operator be defined formally? I thought to define it in the following way: $$\arg\sup_{x\in A} f(x) = \arg \max_{x\in \text{closure}(A)}f(x)$$ Is this definition meaningful? Is it used anywhere in mathematics?",,"['real-analysis', 'notation', 'definition', 'supremum-and-infimum']"
92,How do I prove that $f_n\to f$ in $L^p$?,How do I prove that  in ?,f_n\to f L^p,"Let $\{f_n\}$ be a sequence in $L^p([0,1])$ for $p\geq 1$. Suppose there exists $f\in L^p([0,1])$ satisfying $\lim_{n\to\infty} \int_0^1 f_n(x)g(x)dx = \int_0^1 f(x)g(x)dx$ for any $g\in L^2([0,1])$. Moreover, assume that $\lim_{n\to\infty} ||f_n||_p=||f||_p$. In this case, how do I prove that $f_n\rightarrow f$ in $L^p$? Even when $p=2$, the result is non-trivial since $\lim_{n\to\infty}<f_n-f,g>=0$ (weak convergence) does not simply imply the convergence in $L^2$. More seriously, if $p\neq 2$, I'm not sure what's going on here. Why for given $f\in L^p$ and $g\in L^2$, $fg\in L^1$?. Holder inequality cannot be applied here. Moreover, I'm not sure how to use the condition $\lim_{n\to\infty} ||f_n||_p =||f||_p$. If $f_n \to f$ pointwise a.e., this condition seems useful, but this is not the case. How do I prove this? Thank you in advance.","Let $\{f_n\}$ be a sequence in $L^p([0,1])$ for $p\geq 1$. Suppose there exists $f\in L^p([0,1])$ satisfying $\lim_{n\to\infty} \int_0^1 f_n(x)g(x)dx = \int_0^1 f(x)g(x)dx$ for any $g\in L^2([0,1])$. Moreover, assume that $\lim_{n\to\infty} ||f_n||_p=||f||_p$. In this case, how do I prove that $f_n\rightarrow f$ in $L^p$? Even when $p=2$, the result is non-trivial since $\lim_{n\to\infty}<f_n-f,g>=0$ (weak convergence) does not simply imply the convergence in $L^2$. More seriously, if $p\neq 2$, I'm not sure what's going on here. Why for given $f\in L^p$ and $g\in L^2$, $fg\in L^1$?. Holder inequality cannot be applied here. Moreover, I'm not sure how to use the condition $\lim_{n\to\infty} ||f_n||_p =||f||_p$. If $f_n \to f$ pointwise a.e., this condition seems useful, but this is not the case. How do I prove this? Thank you in advance.",,['real-analysis']
93,Points in the boundary of a compact set $K\subset\mathbb{R}^2$ reachable by a path in $K^c$,Points in the boundary of a compact set  reachable by a path in,K\subset\mathbb{R}^2 K^c,"Let $K\subset\mathbb{R}^2$ be compact.  Let the path boundary of $K$ denote the set of points in $z\in K$ such that for some point $w\in K^c$, there is a continuous path $\gamma:[0,1]\to\mathbb{R}^2$ such that $\gamma(0)=w$. $\gamma((0,1))\subset K^c$. $\gamma(1)=z$. Of course the path boundary of $K$ is contained in the boundary of $K$, and it is not hard to find a set whose path boundary is a strict subset of its boundary.  Take for example the block $[-1,1]\times[-1,1]$, and remove the sets $\left\{(x,y):y>0,\dfrac{1}{n^2}<x<\dfrac{2}{n^2}\right\}$, for $n\geq2$.  The resulting set $K$ is compact (countably many open sets have been removed), and the set $\{(0,y):0\leq y<1\}$ is contained in the boundary of $K$ but not in the path boundary of $K$. My question is : Is there a nice characterization of the sets for which the path boundary is equal to the boundary?  Even a characterization for the case in which both $K$ and $K^c$ are connected would be welcome. As a cautionary tale, I will say that the answer cannot depend just on smoothness of $\partial K$, since if $g:[0,1]\to[0,\infty)$ is any continuous function, then the path boundary of the set $K=\{(x,y):0\leq x\leq1,0\leq y\leq g(x)\}$ is equal to the boundary of $K$, and of course this $K$ can have quite jagged boundary. PS: If someone has a much better name than ""path boundary"", feel free to change the question.","Let $K\subset\mathbb{R}^2$ be compact.  Let the path boundary of $K$ denote the set of points in $z\in K$ such that for some point $w\in K^c$, there is a continuous path $\gamma:[0,1]\to\mathbb{R}^2$ such that $\gamma(0)=w$. $\gamma((0,1))\subset K^c$. $\gamma(1)=z$. Of course the path boundary of $K$ is contained in the boundary of $K$, and it is not hard to find a set whose path boundary is a strict subset of its boundary.  Take for example the block $[-1,1]\times[-1,1]$, and remove the sets $\left\{(x,y):y>0,\dfrac{1}{n^2}<x<\dfrac{2}{n^2}\right\}$, for $n\geq2$.  The resulting set $K$ is compact (countably many open sets have been removed), and the set $\{(0,y):0\leq y<1\}$ is contained in the boundary of $K$ but not in the path boundary of $K$. My question is : Is there a nice characterization of the sets for which the path boundary is equal to the boundary?  Even a characterization for the case in which both $K$ and $K^c$ are connected would be welcome. As a cautionary tale, I will say that the answer cannot depend just on smoothness of $\partial K$, since if $g:[0,1]\to[0,\infty)$ is any continuous function, then the path boundary of the set $K=\{(x,y):0\leq x\leq1,0\leq y\leq g(x)\}$ is equal to the boundary of $K$, and of course this $K$ can have quite jagged boundary. PS: If someone has a much better name than ""path boundary"", feel free to change the question.",,"['real-analysis', 'general-topology', 'compactness', 'connectedness']"
94,A sequence of continuous functions which is pointwise convergent to zero and not uniformly convergent on any interval.,A sequence of continuous functions which is pointwise convergent to zero and not uniformly convergent on any interval.,,"The exercise is to construct a sequence of continuous functions $f_n:\mathbb{R}\rightarrow \mathbb{R}, n\in \mathbb{N}$ , which is pointwise convergent to $f(x)=0 , x\in \mathbb{R}$ and not uniformly convergent on any interval $(a,b)$. I posted this question once and I got the answer, but I had forgotten to mention that $f_n$ must be continuous. The example of sequence of discontinuous functions looks like this: Let $\{r_n\}_{n\geq 1}$ be an enumeration of the rationals, and define $f_n(x)$ as follows:   $$ f_n(x)=\begin{cases} 1,& x\in \{r_k\}_{k\geq n}\\ 0,& else \end{cases} $$ The person, who posted this, made the remark: If you want to make the $\{f_n\}$ continuous, you can modify this example by using bump functions supported on $(r_n-2^{-n},r_n+2^{-n})$ in place of the spikes to $1$ at $r_n$. I'm not realy sure what he meant. I was thinking of defining $g_k$ as a broken line or maybe more smooth function going through $(r_k-2^{-k},0), (r_k,1), (r_k+2^{-k},0)$ and zero on $\mathbb{R}\setminus [r_k-2^{-k},r_k+2^{-k}]$ and then defining $f_n(x)=\sup\{g_k(x):k\ge n\}$.  But I don't know whether it converges to zero and whether $f_n$ are even continuous. On the other hand I have found this topic A sequence of continuous functions on $[0,1]$ which converge pointwise a.e. but does not converge uniformly on any interval and there is only an example of a sequence converging to zero almost everywhere. That's why I begin to doubt that there exists a sequence converging to zero everywhere (that would mean there is a mistake in my book because the exercise says ""construct such a sequence"").","The exercise is to construct a sequence of continuous functions $f_n:\mathbb{R}\rightarrow \mathbb{R}, n\in \mathbb{N}$ , which is pointwise convergent to $f(x)=0 , x\in \mathbb{R}$ and not uniformly convergent on any interval $(a,b)$. I posted this question once and I got the answer, but I had forgotten to mention that $f_n$ must be continuous. The example of sequence of discontinuous functions looks like this: Let $\{r_n\}_{n\geq 1}$ be an enumeration of the rationals, and define $f_n(x)$ as follows:   $$ f_n(x)=\begin{cases} 1,& x\in \{r_k\}_{k\geq n}\\ 0,& else \end{cases} $$ The person, who posted this, made the remark: If you want to make the $\{f_n\}$ continuous, you can modify this example by using bump functions supported on $(r_n-2^{-n},r_n+2^{-n})$ in place of the spikes to $1$ at $r_n$. I'm not realy sure what he meant. I was thinking of defining $g_k$ as a broken line or maybe more smooth function going through $(r_k-2^{-k},0), (r_k,1), (r_k+2^{-k},0)$ and zero on $\mathbb{R}\setminus [r_k-2^{-k},r_k+2^{-k}]$ and then defining $f_n(x)=\sup\{g_k(x):k\ge n\}$.  But I don't know whether it converges to zero and whether $f_n$ are even continuous. On the other hand I have found this topic A sequence of continuous functions on $[0,1]$ which converge pointwise a.e. but does not converge uniformly on any interval and there is only an example of a sequence converging to zero almost everywhere. That's why I begin to doubt that there exists a sequence converging to zero everywhere (that would mean there is a mistake in my book because the exercise says ""construct such a sequence"").",,"['real-analysis', 'analysis']"
95,"Never seen this notation before: $\int (y-f(x))^2 Pr(dx,dy) $",Never seen this notation before:,"\int (y-f(x))^2 Pr(dx,dy) ","I have never seen an integral like this: $$\int (y-f(x))^2 Pr(dx,dy) $$ What is that? More precisely what is $Pr(dx,dy)$? And how is that integral defined? I found it in Elements of Statistical Learning, By T. Hastie, et. al.","I have never seen an integral like this: $$\int (y-f(x))^2 Pr(dx,dy) $$ What is that? More precisely what is $Pr(dx,dy)$? And how is that integral defined? I found it in Elements of Statistical Learning, By T. Hastie, et. al.",,"['real-analysis', 'integration', 'measure-theory', 'statistics', 'notation']"
96,"Prob. 6, Chap. 1, in Rudin's PMA [duplicate]","Prob. 6, Chap. 1, in Rudin's PMA [duplicate]",,"This question already has answers here : Prob. 6 (d), Chap. 1, in Baby Rudin, 3rd ed: How to complete this proof? (5 answers) Closed 8 years ago . Here's problem $6$ in Chapter $1$ in the book Principles of Mathematical Analysis by Walter Rudin, $3$rd edition: Fix a real number $b$, such that $b > 1$. $(a)$ If $m, n, p, q$ are integers, $n > 0$, $q > 0$, and $r = m/n = p/q$, then (I've managed to show that) $$\left( b^m \right)^{1/n} = \left( b^p \right)^{1/q}.$$ Hence it makes sense to define $$b^r \colon= \left(b^m\right)^{1/n}.$$ $(b)$ (I've also managed to show that) for any rational numbers $r, s$, we have  $$b^{r+s} = b^r b^s.$$ $(c)$ For any real number $x$, define the set $B(x)$ as follows: $$B(x) \colon= \left\{ \ b^t \ \colon \ t \ \mbox{ is rational}, \ t \leq x \ \right\}.$$ We can then prove that $$b^r = \sup B(r)$$ when $r$ is a rational number. Hence it makes sense to define $$b^x \colon= \sup B(x) = \sup \left\{ \ b^t \ \colon \ t \ \mbox{ is rational}, \ t \leq x \ \right\}$$ for every real number $x$. $(d)$ How to prove (USING THE MACHINERY DEVELOPED HERE) that $$b^{x+y} = b^x b^y$$ for all real numbers $x$ and $y$? I've already posted this question. Here's the link . However, none of the answers there seems to work for me. My Work Let  $X, Y$ be any two non-empty subsets of $\mathbb{R}$. Then we have the following facts: $(1)$ If $X \subset Y$ and if $Y$ is bounded above in $\mathbb{R}$, then so is $X$ and we have the inequality $$\sup X \leq \sup Y.$$ $(2)$ If $X, Y$ are non-empty subsets of the set of non-negative real numbers and if both $X$ and $Y$ are bounded above, then so is the set $$\{ \ xy \ \colon \ x \in X, \ y \in Y \ \};$$ moreover, we have the identity $$\sup \{ \ xy \ \colon \ x \in X, \ y \in Y \ \} = \sup X \cdot \sup Y.$$ Thus, $$\begin{align} b^x b^y &= \sup B(x) \sup B(y) \\ &= \sup \{ \ b^r \ \colon \ r \in \mathbb{Q}, r \leq x \ \} \sup \{ \ b^s \ \colon \ s \in \mathbb{Q}, s \leq y \ \} \\ &= \sup \{ \ b^{r+s} \ \colon \ r \in \mathbb{Q}, \ s \in \mathbb{Q}, \  r \leq x, \ s \leq y \ \}. \end{align}$$ But if $r \in \mathbb{Q}$, $s \in \mathbb{Q}$, $r \leq x$, and $s \leq y$, then the sum $r+s \in \mathbb{Q}$ and $r + s \leq x + y$ also. So $$ \{ \ b^{r+s} \ \colon \ r \in \mathbb{Q}, \ s \in \mathbb{Q}, \  r \leq x, \ s \leq y \ \} \subseteq \{ \ b^t \ \colon \ t \in \mathbb{Q}, \ t \leq x+y \ \}.$$ Therefore, $$\begin{align} b^x  b^y &= \sup \{ \ b^{r+s} \ \colon \ r \in \mathbb{Q}, \ s \in \mathbb{Q}, \  r \leq x, \ s \leq y \ \} \\ &\leq \sup \{ \ b^t \ \colon \ t \in \mathbb{Q}, \ t \leq x+y \ \} \\ &= \sup B(x+y) = b^{x+y}. \end{align}$$ Now we have to show that $$b^x  b^y \geq b^{x+y}.$$ Case I. When $x+y$ is irrational: If $t$ is a rational number such that $t \leq x+y$, then since $x+y$ is irrational, we can also conclude that $t < x+y$. So we can find a rational number $r$ such that $$ t-y < r < x.$$ Then $$t-r < y < x+y -r.$$ Let's take $s \colon = t-r$. Then $s \in \mathbb{Q}$. And $s < y$. Thus, we have written $t$ as $t = r + s$, where $r, s \in \mathbb{Q}$, with $r < x$, $s < y$. So, when $x + y \in \mathbb{R} - \mathbb{Q}$, then we have $$\begin{align} b^{x+y} &= \sup B(x+y) = \sup \{ \ b^t \ \colon \ t \in \mathbb{Q}, \ t \leq x+y \ \} \\ &= \sup \{ \ b^t \ \colon \ t \in \mathbb{Q}, \ t < x+y  \} \\ &= \sup \{ \ b^{r+s} \ \colon \ r \in \mathbb{Q}, \ s \in \mathbb{Q}, \ r< x, \ s < y  \}  \\ &= \sup \{ \ b^r b^s  \ \colon \ r \in \mathbb{Q}, \ s \in \mathbb{Q}, \ r< x, \ s < y  \}  \\ &= \sup \{ \ b^r \ \colon \ r \in \mathbb{Q}, \ r < x \ \} \sup  \{ \ b^s \ \colon \ s \in \mathbb{Q}, \ s < y  \} \\  &\leq \sup \{ \ b^r \ \colon \ r \in \mathbb{Q}, \ r \leq x \ \} \sup  \{ \ b^s \ \colon \ s \in \mathbb{Q}, \ s \leq y  \} \\  &= \sup B(x) \sup B(y) \\  &= b^x b^y. \end{align}$$ Case II. What if $x + y \in \mathbb{Q}$? In this case, we can conclude that both $x$ and $y$ are irrational. How do we proceed with showing the remaining reverse inequality in this particular case?","This question already has answers here : Prob. 6 (d), Chap. 1, in Baby Rudin, 3rd ed: How to complete this proof? (5 answers) Closed 8 years ago . Here's problem $6$ in Chapter $1$ in the book Principles of Mathematical Analysis by Walter Rudin, $3$rd edition: Fix a real number $b$, such that $b > 1$. $(a)$ If $m, n, p, q$ are integers, $n > 0$, $q > 0$, and $r = m/n = p/q$, then (I've managed to show that) $$\left( b^m \right)^{1/n} = \left( b^p \right)^{1/q}.$$ Hence it makes sense to define $$b^r \colon= \left(b^m\right)^{1/n}.$$ $(b)$ (I've also managed to show that) for any rational numbers $r, s$, we have  $$b^{r+s} = b^r b^s.$$ $(c)$ For any real number $x$, define the set $B(x)$ as follows: $$B(x) \colon= \left\{ \ b^t \ \colon \ t \ \mbox{ is rational}, \ t \leq x \ \right\}.$$ We can then prove that $$b^r = \sup B(r)$$ when $r$ is a rational number. Hence it makes sense to define $$b^x \colon= \sup B(x) = \sup \left\{ \ b^t \ \colon \ t \ \mbox{ is rational}, \ t \leq x \ \right\}$$ for every real number $x$. $(d)$ How to prove (USING THE MACHINERY DEVELOPED HERE) that $$b^{x+y} = b^x b^y$$ for all real numbers $x$ and $y$? I've already posted this question. Here's the link . However, none of the answers there seems to work for me. My Work Let  $X, Y$ be any two non-empty subsets of $\mathbb{R}$. Then we have the following facts: $(1)$ If $X \subset Y$ and if $Y$ is bounded above in $\mathbb{R}$, then so is $X$ and we have the inequality $$\sup X \leq \sup Y.$$ $(2)$ If $X, Y$ are non-empty subsets of the set of non-negative real numbers and if both $X$ and $Y$ are bounded above, then so is the set $$\{ \ xy \ \colon \ x \in X, \ y \in Y \ \};$$ moreover, we have the identity $$\sup \{ \ xy \ \colon \ x \in X, \ y \in Y \ \} = \sup X \cdot \sup Y.$$ Thus, $$\begin{align} b^x b^y &= \sup B(x) \sup B(y) \\ &= \sup \{ \ b^r \ \colon \ r \in \mathbb{Q}, r \leq x \ \} \sup \{ \ b^s \ \colon \ s \in \mathbb{Q}, s \leq y \ \} \\ &= \sup \{ \ b^{r+s} \ \colon \ r \in \mathbb{Q}, \ s \in \mathbb{Q}, \  r \leq x, \ s \leq y \ \}. \end{align}$$ But if $r \in \mathbb{Q}$, $s \in \mathbb{Q}$, $r \leq x$, and $s \leq y$, then the sum $r+s \in \mathbb{Q}$ and $r + s \leq x + y$ also. So $$ \{ \ b^{r+s} \ \colon \ r \in \mathbb{Q}, \ s \in \mathbb{Q}, \  r \leq x, \ s \leq y \ \} \subseteq \{ \ b^t \ \colon \ t \in \mathbb{Q}, \ t \leq x+y \ \}.$$ Therefore, $$\begin{align} b^x  b^y &= \sup \{ \ b^{r+s} \ \colon \ r \in \mathbb{Q}, \ s \in \mathbb{Q}, \  r \leq x, \ s \leq y \ \} \\ &\leq \sup \{ \ b^t \ \colon \ t \in \mathbb{Q}, \ t \leq x+y \ \} \\ &= \sup B(x+y) = b^{x+y}. \end{align}$$ Now we have to show that $$b^x  b^y \geq b^{x+y}.$$ Case I. When $x+y$ is irrational: If $t$ is a rational number such that $t \leq x+y$, then since $x+y$ is irrational, we can also conclude that $t < x+y$. So we can find a rational number $r$ such that $$ t-y < r < x.$$ Then $$t-r < y < x+y -r.$$ Let's take $s \colon = t-r$. Then $s \in \mathbb{Q}$. And $s < y$. Thus, we have written $t$ as $t = r + s$, where $r, s \in \mathbb{Q}$, with $r < x$, $s < y$. So, when $x + y \in \mathbb{R} - \mathbb{Q}$, then we have $$\begin{align} b^{x+y} &= \sup B(x+y) = \sup \{ \ b^t \ \colon \ t \in \mathbb{Q}, \ t \leq x+y \ \} \\ &= \sup \{ \ b^t \ \colon \ t \in \mathbb{Q}, \ t < x+y  \} \\ &= \sup \{ \ b^{r+s} \ \colon \ r \in \mathbb{Q}, \ s \in \mathbb{Q}, \ r< x, \ s < y  \}  \\ &= \sup \{ \ b^r b^s  \ \colon \ r \in \mathbb{Q}, \ s \in \mathbb{Q}, \ r< x, \ s < y  \}  \\ &= \sup \{ \ b^r \ \colon \ r \in \mathbb{Q}, \ r < x \ \} \sup  \{ \ b^s \ \colon \ s \in \mathbb{Q}, \ s < y  \} \\  &\leq \sup \{ \ b^r \ \colon \ r \in \mathbb{Q}, \ r \leq x \ \} \sup  \{ \ b^s \ \colon \ s \in \mathbb{Q}, \ s \leq y  \} \\  &= \sup B(x) \sup B(y) \\  &= b^x b^y. \end{align}$$ Case II. What if $x + y \in \mathbb{Q}$? In this case, we can conclude that both $x$ and $y$ are irrational. How do we proceed with showing the remaining reverse inequality in this particular case?",,"['real-analysis', 'analysis', 'exponentiation', 'foundations']"
97,Existence of function satisfying a certain limit,Existence of function satisfying a certain limit,,"Does there exist a real function $f$ such that $\lim_{x\to+\infty}f(x)=+\infty$ and such that $$\lim_{x\to+\infty}\frac{f(x+1)f(x)}{x(f(x+1)-f(x))}=0?$$ I tried powers, logarithms, exponentials in all sort of forms, but it seems if $f$ grows 'faster' than $x$ the top of the limit makes it go to infinity, while if it grow 'slower' then the bottom part makes it go to infinity. On the other hand I've not been able to prove that for every such $f$ the limit is not zero.","Does there exist a real function $f$ such that $\lim_{x\to+\infty}f(x)=+\infty$ and such that $$\lim_{x\to+\infty}\frac{f(x+1)f(x)}{x(f(x+1)-f(x))}=0?$$ I tried powers, logarithms, exponentials in all sort of forms, but it seems if $f$ grows 'faster' than $x$ the top of the limit makes it go to infinity, while if it grow 'slower' then the bottom part makes it go to infinity. On the other hand I've not been able to prove that for every such $f$ the limit is not zero.",,"['calculus', 'real-analysis', 'functions']"
98,"Proving that:$\int_X f_n g \, d\mu \to \int_X fg \, d\mu$ for all $g$ in $\mathscr{L}^q (X)$",Proving that: for all  in,"\int_X f_n g \, d\mu \to \int_X fg \, d\mu g \mathscr{L}^q (X)","I found  the following exercise and I'd like to know if my answer is correct. Let $(X, \mathscr A, \mu)$ a finite measure space. Let $\{f_n\}$ a sequence of measurable functions such that $\|f_n\|_p\le M$ for a real constant $M$ $(1<p<\infty)$ and suppose that $f_n \xrightarrow{\text{a.e.}} f$. Then $$\int_X f_n g \,d\mu \to \int_X fg \,d\mu$$ for all $g$ in $\mathscr{L}^q (X)$, where $q^{-1}=1- p^{-1}$. Since $\int |f_n|^p \,d\mu\le M^p $ by Fatou's lemma it follows that $\int |f|^p \,d\mu\le M^p<\infty$, so $f$ belongs to $\mathscr{L}^p (X)$. Let $g$ in $\mathscr{L}^q$ arbitrary but fixed. Then by Hölder inequality it follows that $$\|f_n g\|_1 \le \|f_n\|_p\|g\|_q\le M \|g\|_q<\infty$$ and $$\|f g\|_1 \le \|f\|_p\|g\|_q \le M \|g\|_q<\infty$$ thus $f_n g$ and $fg$ belongs to $\mathscr L ^1(X)$. Now since $g$ is in $\mathscr L ^q (X)$, then $g$ is finite a.e.,without loss of generality we may assume that $g$ is real-valued so $f_n g \xrightarrow{ a.e} fg$. Let $\nu (A) =  \int_A |g|^q \,d\mu $, so $\nu $ is absolutely continuous with respect to $\mu$, also $\nu$ is a finite measure on $(X, \mathscr A )$. We can use the $\epsilon-\delta$ definition of absolutely continuous, so given $\epsilon>0$ there is a $\delta>0$ such that for $\mu(A)<\delta$ then $\nu (A) <\epsilon^q$ for any $A$ in $\mathscr A$. Now by Egoroff's thm exists a $B$ in $\mathscr A$ such that $(f_ng) (x)\xrightarrow{\text{uniformly}} (fg) (x)$ for $x$ in $B$ and $\mu(X\setminus B) <\delta$. Let $N$ such that for all $n\ge N$, $|(f_ng)(x)-(fg)(x)|<\epsilon$ for all $x\in B$. Thus \begin{align*}\left |\int_X (f_n-f) g \, d\mu \right|&\le \int_B |f_ng-fg| \, d\mu +\int_{X-B}|f_n g|\, d\mu +\int_{X-B}|fg|\,d\mu \\[6pt] &\le \int_B |f_ng-fg| d\mu + 2M \left( \int_{X-B}|g|^qd\mu \right)^{1/q}\\[6pt] &\le\epsilon \mu(X)+2M  (\nu(X\setminus B))^{1/q}\\[6pt] &\le \epsilon (\mu(X)+2M)\end{align*} Since $\mu(x)<\infty$ and $M<\infty$ the result follows.","I found  the following exercise and I'd like to know if my answer is correct. Let $(X, \mathscr A, \mu)$ a finite measure space. Let $\{f_n\}$ a sequence of measurable functions such that $\|f_n\|_p\le M$ for a real constant $M$ $(1<p<\infty)$ and suppose that $f_n \xrightarrow{\text{a.e.}} f$. Then $$\int_X f_n g \,d\mu \to \int_X fg \,d\mu$$ for all $g$ in $\mathscr{L}^q (X)$, where $q^{-1}=1- p^{-1}$. Since $\int |f_n|^p \,d\mu\le M^p $ by Fatou's lemma it follows that $\int |f|^p \,d\mu\le M^p<\infty$, so $f$ belongs to $\mathscr{L}^p (X)$. Let $g$ in $\mathscr{L}^q$ arbitrary but fixed. Then by Hölder inequality it follows that $$\|f_n g\|_1 \le \|f_n\|_p\|g\|_q\le M \|g\|_q<\infty$$ and $$\|f g\|_1 \le \|f\|_p\|g\|_q \le M \|g\|_q<\infty$$ thus $f_n g$ and $fg$ belongs to $\mathscr L ^1(X)$. Now since $g$ is in $\mathscr L ^q (X)$, then $g$ is finite a.e.,without loss of generality we may assume that $g$ is real-valued so $f_n g \xrightarrow{ a.e} fg$. Let $\nu (A) =  \int_A |g|^q \,d\mu $, so $\nu $ is absolutely continuous with respect to $\mu$, also $\nu$ is a finite measure on $(X, \mathscr A )$. We can use the $\epsilon-\delta$ definition of absolutely continuous, so given $\epsilon>0$ there is a $\delta>0$ such that for $\mu(A)<\delta$ then $\nu (A) <\epsilon^q$ for any $A$ in $\mathscr A$. Now by Egoroff's thm exists a $B$ in $\mathscr A$ such that $(f_ng) (x)\xrightarrow{\text{uniformly}} (fg) (x)$ for $x$ in $B$ and $\mu(X\setminus B) <\delta$. Let $N$ such that for all $n\ge N$, $|(f_ng)(x)-(fg)(x)|<\epsilon$ for all $x\in B$. Thus \begin{align*}\left |\int_X (f_n-f) g \, d\mu \right|&\le \int_B |f_ng-fg| \, d\mu +\int_{X-B}|f_n g|\, d\mu +\int_{X-B}|fg|\,d\mu \\[6pt] &\le \int_B |f_ng-fg| d\mu + 2M \left( \int_{X-B}|g|^qd\mu \right)^{1/q}\\[6pt] &\le\epsilon \mu(X)+2M  (\nu(X\setminus B))^{1/q}\\[6pt] &\le \epsilon (\mu(X)+2M)\end{align*} Since $\mu(x)<\infty$ and $M<\infty$ the result follows.",,"['real-analysis', 'integration', 'measure-theory', 'proof-verification', 'lp-spaces']"
99,Calculating $\sum_{n=1}^{\infty}\ln^2 \!\left(1+\frac1{2n}\right) \!\ln^2\!\left(1+\frac1{2n+1}\right)$,Calculating,\sum_{n=1}^{\infty}\ln^2 \!\left(1+\frac1{2n}\right) \!\ln^2\!\left(1+\frac1{2n+1}\right),Based upon Oloa's question here Evaluating $\sum_{n \geq 1}\ln \!\left(1+\frac1{2n}\right) \!\ln\!\left(1+\frac1{2n+1}\right)$ I was thinking if we possibly can get a nice way to evaluate the series $$\sum_{n=1}^{\infty}\ln^2 \!\left(1+\frac1{2n}\right) \!\ln^2\!\left(1+\frac1{2n+1}\right).$$ Maybe using the same telescoping idea or it simply doesn't work? What then?,Based upon Oloa's question here Evaluating $\sum_{n \geq 1}\ln \!\left(1+\frac1{2n}\right) \!\ln\!\left(1+\frac1{2n+1}\right)$ I was thinking if we possibly can get a nice way to evaluate the series $$\sum_{n=1}^{\infty}\ln^2 \!\left(1+\frac1{2n}\right) \!\ln^2\!\left(1+\frac1{2n+1}\right).$$ Maybe using the same telescoping idea or it simply doesn't work? What then?,,"['calculus', 'real-analysis', 'integration', 'sequences-and-series', 'definite-integrals']"
