,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Find $f(x) \ $: $f'(x) = f(x)^2 + f^{-1}(x) + \int_{-\infty}^{x} \frac{e^{t}}{t} dt$,Find :,f(x) \  f'(x) = f(x)^2 + f^{-1}(x) + \int_{-\infty}^{x} \frac{e^{t}}{t} dt,$$f'(x) = f(x)^2 + f^{-1}(x) + \int_{-\infty}^{x} \frac{e^{t}}{t} dt$$ I suspect this question is a typo because differential equations typically require initial or boundary conditions or more information about the properties of the function $f(x)$ . Without any other information it's not possible to find $f(x)$ . Am I right?,I suspect this question is a typo because differential equations typically require initial or boundary conditions or more information about the properties of the function . Without any other information it's not possible to find . Am I right?,f'(x) = f(x)^2 + f^{-1}(x) + \int_{-\infty}^{x} \frac{e^{t}}{t} dt f(x) f(x),"['calculus', 'ordinary-differential-equations', 'functions', 'derivatives']"
1,Proof that a first integral is not a constant function,Proof that a first integral is not a constant function,,"Let $M^n$ be a (compact) $n$ susbet of $\mathbb{R}^n$ . And we are given a set of $m$ basis functions $$B=\{\psi_i(x): M^n \rightarrow \mathbb{R}| i=1,...,m \}$$ such that all of them are differentiable and non-constant. We define a DS by $$\dot{x}(t)=F(x(t))$$ and set $\Phi: M \times \mathbb{R} \rightarrow M, \quad (x_0, t) \mapsto \Phi(x_0,t)$ as the flow of the DS. Now, we assume to be given  an open subset $U \subset M$ such that $$ \forall x_0 \in U \ \exists \theta(x_0) \in \mathbb{R}^m, \ \theta^{x_0}   \neq 0 : \quad \Lambda(x(t))=\sum_{i=1}^m \theta_i(x_0) \psi_i(\Phi(x_0,t))=-\theta_0(x_0) \quad \forall t  $$ I want to show that $\Lambda$ is a (local) first integral. I know that the Lie derivative $L_F(\Lambda)=0$ is zero, which is the first thing I need to prove. But I am struggling to prove that the function is not locally constant. My question is the following: Can we prove that $\Lambda $ is not locally constant? More formally, sis it true that $$ \exists x_0^1,x_0^2 \in U: \quad \Lambda(x^1(t))=-\theta_0(x_0^1) \neq -\theta_0(x_0^2)=\Lambda(x^2(t))\;? $$ I think this has to be true, since for two different inital conditions $\Phi(x_0^1,t) \neq \Phi(x_0^2,t) \ \forall t$ . And since all functions are non-constant the parameters can't be the same but I am lacking a formal proof. Especially how I can conclude from the fact that the parameters are not the same that the value of the function is not the same. (If it helps , the vector field $F$ itself is a linear combination of the basis functions $\psi_i$ in each dimension.)","Let be a (compact) susbet of . And we are given a set of basis functions such that all of them are differentiable and non-constant. We define a DS by and set as the flow of the DS. Now, we assume to be given  an open subset such that I want to show that is a (local) first integral. I know that the Lie derivative is zero, which is the first thing I need to prove. But I am struggling to prove that the function is not locally constant. My question is the following: Can we prove that is not locally constant? More formally, sis it true that I think this has to be true, since for two different inital conditions . And since all functions are non-constant the parameters can't be the same but I am lacking a formal proof. Especially how I can conclude from the fact that the parameters are not the same that the value of the function is not the same. (If it helps , the vector field itself is a linear combination of the basis functions in each dimension.)","M^n n \mathbb{R}^n m B=\{\psi_i(x): M^n \rightarrow \mathbb{R}| i=1,...,m \} \dot{x}(t)=F(x(t)) \Phi: M \times \mathbb{R} \rightarrow M, \quad (x_0, t) \mapsto \Phi(x_0,t) U \subset M  \forall x_0 \in U \ \exists \theta(x_0) \in \mathbb{R}^m, \ \theta^{x_0}   \neq 0 : \quad \Lambda(x(t))=\sum_{i=1}^m \theta_i(x_0) \psi_i(\Phi(x_0,t))=-\theta_0(x_0) \quad \forall t
  \Lambda L_F(\Lambda)=0 \Lambda  
\exists x_0^1,x_0^2 \in U: \quad \Lambda(x^1(t))=-\theta_0(x_0^1) \neq -\theta_0(x_0^2)=\Lambda(x^2(t))\;?
 \Phi(x_0^1,t) \neq \Phi(x_0^2,t) \ \forall t F \psi_i","['calculus', 'ordinary-differential-equations', 'differential-geometry', 'dynamical-systems']"
2,"If $f(x)=0,\,g(x)=\sin(x)\,$ and $h(x)=e^x$ is setting $c_1=k\neq0, c_2=0$ and $c_3=0$ a non trivial linear combination?",If  and  is setting  and  a non trivial linear combination?,"f(x)=0,\,g(x)=\sin(x)\, h(x)=e^x c_1=k\neq0, c_2=0 c_3=0","Given these $3$ functions, $f(x)=0,\;g(x)=\sin(x)$ and $h(x)=e^x\,,$ I have to find a non-trivial linear combination, that is, three constants that makes the combination of this functions equal to $0$ . $$ c_1\cdot{0}+c_2\cdot{\sin(x)+c_3\cdot{e^x}}=0 $$ The doubt here is with the definition of non-trivial linear combination, which as I understand, means that not all the constants are $0$ . That means that in this example I can put $c_1=1, c_2=0$ and $c_3=0$ but is this a non-trivial linear combination? Thanks.","Given these functions, and I have to find a non-trivial linear combination, that is, three constants that makes the combination of this functions equal to . The doubt here is with the definition of non-trivial linear combination, which as I understand, means that not all the constants are . That means that in this example I can put and but is this a non-trivial linear combination? Thanks.","3 f(x)=0,\;g(x)=\sin(x) h(x)=e^x\,, 0 
c_1\cdot{0}+c_2\cdot{\sin(x)+c_3\cdot{e^x}}=0
 0 c_1=1, c_2=0 c_3=0","['linear-algebra', 'ordinary-differential-equations']"
3,Adjoint differential equations - Relation,Adjoint differential equations - Relation,,"In my ODE book (ODE by Wolfgang Walter), I have this exercise. Let $G(x)$ be a fundamental matrix of $$ y^{\prime}=A(x) y . $$ Show that $\left(G(x)^{-1}\right)^{\top}$ is a fundamental matrix of $$ z^{\prime}=-A(x)^{\top} z. $$ I solved this Problem, but I can still leave it here open as a puzzle.","In my ODE book (ODE by Wolfgang Walter), I have this exercise. Let be a fundamental matrix of Show that is a fundamental matrix of I solved this Problem, but I can still leave it here open as a puzzle.","G(x) 
y^{\prime}=A(x) y .
 \left(G(x)^{-1}\right)^{\top} 
z^{\prime}=-A(x)^{\top} z.
","['ordinary-differential-equations', 'matrix-calculus']"
4,"Showing that if $f \in C^k$ and $\dot{x}(t) = f(x(t),t)$, then $x \in C^{k+1}$ (Proof verification)","Showing that if  and , then  (Proof verification)","f \in C^k \dot{x}(t) = f(x(t),t) x \in C^{k+1}","I came across the following easy fact (Lemma 3.17) in Meiss' Differential Dynamical Systems . Here is my paraphrased version: Lemma. Let $E \subseteq \mathbb{R}^d$ , let $I \subset \mathbb{R}$ be a compact interval, let $f \in C^k(E,\mathbb{R}^d)$ for some $k \geq 0$ , and suppose that $x: I \to \mathbb{R}^d$ solves the IVP $$\dot{x}(t) = f(x(t)), \; x(0) = x_0.$$ Then $x \in C^{k+1}(I,\mathbb{R}^d)$ . I am trying to use this lemma to prove the following analogous claim for nonautonomous IVPs: Claim. Let $E$ and $I$ be as in the lemma, let $f \in C^k(E \times I, \mathbb{R}^d)$ , and suppose that $x: I \to \mathbb{R}^d$ solves the IVP \begin{equation}     \dot{x}(t) = f(x(t),t), \quad x(0) = x_0.  \end{equation} Then $x \in C^{k+1}(I,\mathbb{R}^d).$ Here is my proof attempt. Proof. Firstly, we can write $x(t) = (x_1(t),\ldots,x_d(t))$ and $f(x,t) = (f_1(x,t),\ldots,f_d(x,t))$ , where $x_i: I \to \mathbb{R}$ and $f_i: \mathbb{R}^d \times I \to \mathbb{R}$ for each $i=1,\ldots,d$ . Now define $\tilde{x}: I \to \mathbb{R}^{d+1}$ by \begin{equation}    \tilde{x}(t) := (x_1(t),\ldots,x_d(t),t) \qquad \forall t \in I.  \end{equation} Then for each $i = 1,\ldots,d$ , define $\tilde{f}_i: \mathbb{R}^{d+1} \to \mathbb{R}$ by \begin{equation}     \tilde{f}_i(s_1,\ldots,s_d,s_{d+1}) := f_i((s_1,\ldots,s_d),s_{d+1})  \end{equation} for all $s = (s_1,\ldots,s_{d+1}) \in \mathbb{R}^{d+1}$ .  Now note that \begin{align*}    \tilde{f}_i(\tilde{x}(t)) &= \tilde{f}_i(x_1(t),\ldots,x_d(t),t) \\[3pt]                              &= f_i((x_1(t),\ldots,x_d(t)),t)  \\[3pt]                              &= f_i(x(t),t).  \end{align*} Next, define $\tilde{f}: \mathbb{R}^{d+1} \to \mathbb{R}^{d+1}$ by \begin{equation}    \tilde{f}(s) := (\tilde{f}_1(s),\ldots,\tilde{f}_d(s),1) \qquad \forall s \in \mathbb{R}^{d+1}.  \end{equation} Clearly, $\tilde{f} \in C^k(\tilde{E},\mathbb{R})$ , where $\tilde{E} := E \times I$ . Let $x_0 := (x_0^{(1)},\ldots,x_0^{(d)})$ and define $\tilde{x}_0 := (x_0^{(1)},\ldots,x_0^{(d)},0)$ . Now observe that $\tilde{x}$ solves the autonomous IVP \begin{equation}     \dot{\tilde{x}}(t) = \tilde{f}(\tilde{x}(t)), \quad \tilde{x}(0) = \tilde{x}_0,  \end{equation} since \begin{align*}    \dot{\tilde{x}}(t) &= (\dot{x}_1(t),\ldots,\dot{x}_d(t),1)  \\[3pt]                       &= (f_1(x(t),t),\ldots,f_d(x(t),t),1)    \\[3pt]                       &= (\tilde{f}_1(\tilde{x}(t)),\ldots,\tilde{f}_d(\tilde{x}(t)),1) \\[3pt]                       &= \tilde{f}(\tilde{x}(t)).  \end{align*} It then follows by the lemma that $\tilde{x} \in C^{k+1}(\tilde{E},\mathbb{R})$ , which then implies that $x \in C^{k+1}(E,\mathbb{R}). \qquad \square$ My main question is: Is this proof correct? I just want to make sure my reasoning is sound. Update 7/16/23: Thanks to @Filippo for pointing out a much simpler proof: Proof : Fix $k \geq 0$ and let $f \in C^k(E \times I,\mathbb{R}^d)$ . We prove that $x \in C^j(I,\mathbb{R}^d)$ for all $j \in \{0,1,\ldots,k+1\}$ by induction on $j$ . Base case $(j = 0)$ : By assumption $x$ is differentiable and hence and hence continuous, and so $x \in C^0(I,\mathbb{R}^d)$ . Inductive step: Fix an arbitrary $j \in \{0,1,\ldots,k\}$ and assume $x \in C^j(I,\mathbb{R}^d)$ . Then the map $t \mapsto f(x(t))$ is in $C^j(I,\mathbb{R}^d)$ being that it's the composition of the maps $f \in C^j(E \times I,\mathbb{R}^d)$ and $t \mapsto (x(t),t) \in C^j(I,\mathbb{R}^d \times I)$ . Hence, $\dot{x} \in C^j(I,\mathbb{R}^d)$ and so $x \in C^{j+1}(I,\mathbb{R}^d)$ . This completes the proof.","I came across the following easy fact (Lemma 3.17) in Meiss' Differential Dynamical Systems . Here is my paraphrased version: Lemma. Let , let be a compact interval, let for some , and suppose that solves the IVP Then . I am trying to use this lemma to prove the following analogous claim for nonautonomous IVPs: Claim. Let and be as in the lemma, let , and suppose that solves the IVP Then Here is my proof attempt. Proof. Firstly, we can write and , where and for each . Now define by Then for each , define by for all .  Now note that Next, define by Clearly, , where . Let and define . Now observe that solves the autonomous IVP since It then follows by the lemma that , which then implies that My main question is: Is this proof correct? I just want to make sure my reasoning is sound. Update 7/16/23: Thanks to @Filippo for pointing out a much simpler proof: Proof : Fix and let . We prove that for all by induction on . Base case : By assumption is differentiable and hence and hence continuous, and so . Inductive step: Fix an arbitrary and assume . Then the map is in being that it's the composition of the maps and . Hence, and so . This completes the proof.","E \subseteq \mathbb{R}^d I \subset \mathbb{R} f \in C^k(E,\mathbb{R}^d) k \geq 0 x: I \to \mathbb{R}^d \dot{x}(t) = f(x(t)), \; x(0) = x_0. x \in C^{k+1}(I,\mathbb{R}^d) E I f \in C^k(E \times I, \mathbb{R}^d) x: I \to \mathbb{R}^d \begin{equation}
    \dot{x}(t) = f(x(t),t), \quad x(0) = x_0. 
\end{equation} x \in C^{k+1}(I,\mathbb{R}^d). x(t) = (x_1(t),\ldots,x_d(t)) f(x,t) = (f_1(x,t),\ldots,f_d(x,t)) x_i: I \to \mathbb{R} f_i: \mathbb{R}^d \times I \to \mathbb{R} i=1,\ldots,d \tilde{x}: I \to \mathbb{R}^{d+1} \begin{equation}
   \tilde{x}(t) := (x_1(t),\ldots,x_d(t),t) \qquad \forall t \in I. 
\end{equation} i = 1,\ldots,d \tilde{f}_i: \mathbb{R}^{d+1} \to \mathbb{R} \begin{equation}
    \tilde{f}_i(s_1,\ldots,s_d,s_{d+1}) := f_i((s_1,\ldots,s_d),s_{d+1}) 
\end{equation} s = (s_1,\ldots,s_{d+1}) \in \mathbb{R}^{d+1} \begin{align*}
   \tilde{f}_i(\tilde{x}(t)) &= \tilde{f}_i(x_1(t),\ldots,x_d(t),t) \\[3pt]
                             &= f_i((x_1(t),\ldots,x_d(t)),t)  \\[3pt]
                             &= f_i(x(t),t). 
\end{align*} \tilde{f}: \mathbb{R}^{d+1} \to \mathbb{R}^{d+1} \begin{equation}
   \tilde{f}(s) := (\tilde{f}_1(s),\ldots,\tilde{f}_d(s),1) \qquad \forall s \in \mathbb{R}^{d+1}. 
\end{equation} \tilde{f} \in C^k(\tilde{E},\mathbb{R}) \tilde{E} := E \times I x_0 := (x_0^{(1)},\ldots,x_0^{(d)}) \tilde{x}_0 := (x_0^{(1)},\ldots,x_0^{(d)},0) \tilde{x} \begin{equation}
    \dot{\tilde{x}}(t) = \tilde{f}(\tilde{x}(t)), \quad \tilde{x}(0) = \tilde{x}_0, 
\end{equation} \begin{align*}
   \dot{\tilde{x}}(t) &= (\dot{x}_1(t),\ldots,\dot{x}_d(t),1)  \\[3pt]
                      &= (f_1(x(t),t),\ldots,f_d(x(t),t),1)    \\[3pt]
                      &= (\tilde{f}_1(\tilde{x}(t)),\ldots,\tilde{f}_d(\tilde{x}(t)),1) \\[3pt]
                      &= \tilde{f}(\tilde{x}(t)). 
\end{align*} \tilde{x} \in C^{k+1}(\tilde{E},\mathbb{R}) x \in C^{k+1}(E,\mathbb{R}). \qquad \square k \geq 0 f \in C^k(E \times I,\mathbb{R}^d) x \in C^j(I,\mathbb{R}^d) j \in \{0,1,\ldots,k+1\} j (j = 0) x x \in C^0(I,\mathbb{R}^d) j \in \{0,1,\ldots,k\} x \in C^j(I,\mathbb{R}^d) t \mapsto f(x(t)) C^j(I,\mathbb{R}^d) f \in C^j(E \times I,\mathbb{R}^d) t \mapsto (x(t),t) \in C^j(I,\mathbb{R}^d \times I) \dot{x} \in C^j(I,\mathbb{R}^d) x \in C^{j+1}(I,\mathbb{R}^d)","['ordinary-differential-equations', 'dynamical-systems', 'initial-value-problems']"
5,What differential equation has a Laplace transform that contains essential singularities?,What differential equation has a Laplace transform that contains essential singularities?,,"I would like to find a simple differential equation that, on taking the Laplace transform, results in essential singularities. I am looking, when teaching, to find examples of differential equations that results in each of the possible singularities on the complex plane. An example with poles is easy; any linear differential equation with constant coefficients will do. The Laplace transform of the homogeneous solution will give poles. The second order differential equation is a good simple candidate. Along these lines I have started with the equation in the Laplace domain of $L_t=-e^{-\frac{1}{s+\epsilon -i}}-e^{-\frac{1}{s+\epsilon +i}}$ where s is the Laplace variable and $\epsilon$ a parameter. I want real results from the inverse Laplace transform so have included the complex conjugate of the singularity located at $s=-\epsilon +i$ . A plot of the absolute value is Which shows the singularities clearly. Using Mathematica to get the inverse Laplace transform I get $f(t)=-e^{t (-\epsilon -i)} \left(\delta (t)-\frac{J_1\left(2 \sqrt{t}\right)}{\sqrt{t}}\right)-e^{t (-\epsilon +i)} \left(\delta    (t)-\frac{J_1\left(2 \sqrt{t}\right)}{\sqrt{t}}\right)$ Where $J_1$ is the Bessel function of the first kind.  Plotting this result for $\epsilon =0.001$ gives Which looks complicated. What differential equation would have this as the homogeneous solution? Perhaps this question can be made simpler by alternative forms in the Laplace plane. Any suggestions?","I would like to find a simple differential equation that, on taking the Laplace transform, results in essential singularities. I am looking, when teaching, to find examples of differential equations that results in each of the possible singularities on the complex plane. An example with poles is easy; any linear differential equation with constant coefficients will do. The Laplace transform of the homogeneous solution will give poles. The second order differential equation is a good simple candidate. Along these lines I have started with the equation in the Laplace domain of where s is the Laplace variable and a parameter. I want real results from the inverse Laplace transform so have included the complex conjugate of the singularity located at . A plot of the absolute value is Which shows the singularities clearly. Using Mathematica to get the inverse Laplace transform I get Where is the Bessel function of the first kind.  Plotting this result for gives Which looks complicated. What differential equation would have this as the homogeneous solution? Perhaps this question can be made simpler by alternative forms in the Laplace plane. Any suggestions?","L_t=-e^{-\frac{1}{s+\epsilon -i}}-e^{-\frac{1}{s+\epsilon +i}} \epsilon s=-\epsilon +i f(t)=-e^{t (-\epsilon -i)} \left(\delta (t)-\frac{J_1\left(2 \sqrt{t}\right)}{\sqrt{t}}\right)-e^{t (-\epsilon +i)} \left(\delta
   (t)-\frac{J_1\left(2 \sqrt{t}\right)}{\sqrt{t}}\right) J_1 \epsilon =0.001","['ordinary-differential-equations', 'laplace-transform']"
6,Solving a Lamé differential equation with a parameter out of boundaries.,Solving a Lamé differential equation with a parameter out of boundaries.,,"I am trying to get rid of the following homogeneous ode. \begin{equation} 	\begin{split} u''(z)+\frac{1}{2} \left(\frac{1}{z}+\frac{1}{z-1}+\frac{1}{z-1}\right) u'(z)+\frac{2\left(A+B\right)- \left(2 B\right)z }{4 z    (z-1) (z-1)}u(z)=0 	\end{split} \end{equation} The form of this equation is very particular and reminds me the algebraic representation of the Lamé equation, as shown in http://dlmf.nist.gov/29.2.E2 , which is : \begin{equation} 	\begin{split} 		\frac{{\mathrm{d}}^{2}w}{{\mathrm{d}\xi}^{2}}+\frac{1}{2}\left(\frac{1}{\xi}+% \frac{1}{\xi-1}+\frac{1}{\xi-k^{-2}}\right)\frac{\mathrm{d}w}{\mathrm{d}\xi}+% \frac{hk^{-2}-\nu_1(\nu_1+1)\xi}{4\xi(\xi-1)(\xi-k^{-2})}w=0 	\end{split} \end{equation} For $0<k<1$ and $\xi={\operatorname{sn}}^{2}\left(z,k\right)$ then the Lamé equation can be expressed in this form: \begin{equation} 	\begin{split} { \frac{{\mathrm{d}}^{2}w}{{\mathrm{d}\zeta}^{2}}+\left[h-\nu(\nu+1) k^2 sn^{2}(\zeta,k)\right] w(\zeta)=0 } 	\end{split} \end{equation} Thus,when comparing my equation and Lame's equation the only way to match is the case in which $k=1$ , but that leaves $k$ out of the parameters. If I ignored the fact that $k$ can't have such value, I could proceed and see how it looks like: \begin{equation} 	\begin{split} { \frac{{\mathrm{d}}^{2}u}{{\mathrm{d}z}^{2}}+\left[2(A+B)-2B   sn^{2}(z,1)\right] u(z)=0 } 	\end{split} \end{equation} Actually, the Jacobi elliptic function $sn(z,1)$ is reducible into $tanh(z)$ And finally I end up with this equation. \begin{equation} 	\begin{split} { \frac{{\mathrm{d}}^{2}u}{{\mathrm{d}z}^{2}}+\left[2(A+B)-2B   \tanh^{2}(z)\right] u(z)=0 } 	\end{split} \end{equation} This equation seems to be solvable in terms of Associated Legendre Polynomials. And some questions have arisen to me: My original equation can be considered a legitimate Lamé differential equation? If so, it will be correct to suggest that Lamé differential equation for the parameter value k=1 degenerates into some form of Associated Legendre differential equation? I would really appreaciate any comments on this, and I apologize for any mistake I could have done while typing this thread.","I am trying to get rid of the following homogeneous ode. The form of this equation is very particular and reminds me the algebraic representation of the Lamé equation, as shown in http://dlmf.nist.gov/29.2.E2 , which is : For and then the Lamé equation can be expressed in this form: Thus,when comparing my equation and Lame's equation the only way to match is the case in which , but that leaves out of the parameters. If I ignored the fact that can't have such value, I could proceed and see how it looks like: Actually, the Jacobi elliptic function is reducible into And finally I end up with this equation. This equation seems to be solvable in terms of Associated Legendre Polynomials. And some questions have arisen to me: My original equation can be considered a legitimate Lamé differential equation? If so, it will be correct to suggest that Lamé differential equation for the parameter value k=1 degenerates into some form of Associated Legendre differential equation? I would really appreaciate any comments on this, and I apologize for any mistake I could have done while typing this thread.","\begin{equation}
	\begin{split}
u''(z)+\frac{1}{2} \left(\frac{1}{z}+\frac{1}{z-1}+\frac{1}{z-1}\right) u'(z)+\frac{2\left(A+B\right)- \left(2 B\right)z }{4 z
   (z-1) (z-1)}u(z)=0
	\end{split}
\end{equation} \begin{equation}
	\begin{split}
		\frac{{\mathrm{d}}^{2}w}{{\mathrm{d}\xi}^{2}}+\frac{1}{2}\left(\frac{1}{\xi}+%
\frac{1}{\xi-1}+\frac{1}{\xi-k^{-2}}\right)\frac{\mathrm{d}w}{\mathrm{d}\xi}+%
\frac{hk^{-2}-\nu_1(\nu_1+1)\xi}{4\xi(\xi-1)(\xi-k^{-2})}w=0
	\end{split}
\end{equation} 0<k<1 \xi={\operatorname{sn}}^{2}\left(z,k\right) \begin{equation}
	\begin{split}
{
\frac{{\mathrm{d}}^{2}w}{{\mathrm{d}\zeta}^{2}}+\left[h-\nu(\nu+1) k^2 sn^{2}(\zeta,k)\right] w(\zeta)=0
}
	\end{split}
\end{equation} k=1 k k \begin{equation}
	\begin{split}
{
\frac{{\mathrm{d}}^{2}u}{{\mathrm{d}z}^{2}}+\left[2(A+B)-2B   sn^{2}(z,1)\right] u(z)=0
}
	\end{split}
\end{equation} sn(z,1) tanh(z) \begin{equation}
	\begin{split}
{
\frac{{\mathrm{d}}^{2}u}{{\mathrm{d}z}^{2}}+\left[2(A+B)-2B   \tanh^{2}(z)\right] u(z)=0
}
	\end{split}
\end{equation}","['complex-analysis', 'ordinary-differential-equations', 'hypergeometric-function', 'homogeneous-equation']"
7,A power series solution to Hermite's differential equation,A power series solution to Hermite's differential equation,,"I'm trying to solve the Hermite's differential equation \begin{equation*} y''(x)-2xy'(x)+2ny(x) = 0 \end{equation*} I look for power series solutions of the form \begin{equation} y(x) = \displaystyle\sum_{\nu=0}^\infty a_\nu x^\nu \end{equation} Then I found the recurrence, \begin{equation} a_{\nu+2} = \frac{2(\nu-n)}{(\nu+2)(\nu+1)} a_\nu \label{eq:anHer} \end{equation} for $\nu = 0,...,n$ . So i am looking for even and odd powers of x solutions. For the even solutions I have \begin{equation} a_{2k} = \frac{4\left(k-1-\frac{n}{2}\right)}{2k(2k-1)}a_{2(k-1)} \end{equation} for $k = 1,2,...$ In terms of $a_0$ I have, \begin{align*} &a_2=\frac{4(-\frac{n}{2})}{2\cdot1}a_0\\ &a_4 =\frac{4\left(1-\frac{n}{2}\right)}{4\cdot3}a_2 = \frac{4^2\left(1-\frac{n}{2}\right)\left(-\frac{n}{2}\right)}{4\cdot3\cdot2\cdot1}a_0   \\ & a_6 = \frac{4\left(2-\frac{n}{2}\right)}{6\cdot5}a_4 =\frac{4^3\left(2-\frac{n}{2}\right)\left(1-\frac{n}{2}\right)\left(-\frac{n}{2}\right)}{6\cdot5\cdot4\cdot3\cdot2\cdot1}a_0 \\ \vdots\\ &a_{2k} = \frac{2^{2k}(-\frac{n}{2})_k}{(2k)!}a_0 \end{align*} where $(x)_\mu$ is the Pochhammer symbol. Then the even solutions are \begin{equation} y_{even}(x) = \displaystyle\sum_{k = 0}^\infty a_{2k}x^{2k} = a_0\left(1+ \displaystyle\sum_{k=1}^\infty \frac{2^{2k}(-\frac{n}{2})_k}{(2k)!}x^{2k}\right) \end{equation} in https://mathworld.wolfram.com/HermitePolynomial.html the even solutions are given by \begin{equation} y_{2n}(x) = c_0 \left(1+\displaystyle\sum_{k=1}^n\frac{2^{2k}(-n)_k}{(2k)!}x^{2k}\right) \end{equation} I don't see why I have $(-\frac{n}{2})_k$ instead of $(-n)_k$ . Since the series I've obtained terminates for $k = 1+\frac{n}{2}$ maybe I can truncate the series and write my series in this form but I'm not sure. Or maybe is just because the normalization of $c_0$ given in the previous link, which is $c_0= (-1)^n2^n(2n-1)!!$ , while my $a_0$ is a general normalization constant without a particular expression so is a ''more general'' solution.","I'm trying to solve the Hermite's differential equation I look for power series solutions of the form Then I found the recurrence, for . So i am looking for even and odd powers of x solutions. For the even solutions I have for In terms of I have, where is the Pochhammer symbol. Then the even solutions are in https://mathworld.wolfram.com/HermitePolynomial.html the even solutions are given by I don't see why I have instead of . Since the series I've obtained terminates for maybe I can truncate the series and write my series in this form but I'm not sure. Or maybe is just because the normalization of given in the previous link, which is , while my is a general normalization constant without a particular expression so is a ''more general'' solution.","\begin{equation*}
y''(x)-2xy'(x)+2ny(x) = 0
\end{equation*} \begin{equation}
y(x) = \displaystyle\sum_{\nu=0}^\infty a_\nu x^\nu
\end{equation} \begin{equation}
a_{\nu+2} = \frac{2(\nu-n)}{(\nu+2)(\nu+1)} a_\nu
\label{eq:anHer}
\end{equation} \nu = 0,...,n \begin{equation}
a_{2k} = \frac{4\left(k-1-\frac{n}{2}\right)}{2k(2k-1)}a_{2(k-1)}
\end{equation} k = 1,2,... a_0 \begin{align*}
&a_2=\frac{4(-\frac{n}{2})}{2\cdot1}a_0\\
&a_4 =\frac{4\left(1-\frac{n}{2}\right)}{4\cdot3}a_2 = \frac{4^2\left(1-\frac{n}{2}\right)\left(-\frac{n}{2}\right)}{4\cdot3\cdot2\cdot1}a_0   \\
& a_6 = \frac{4\left(2-\frac{n}{2}\right)}{6\cdot5}a_4 =\frac{4^3\left(2-\frac{n}{2}\right)\left(1-\frac{n}{2}\right)\left(-\frac{n}{2}\right)}{6\cdot5\cdot4\cdot3\cdot2\cdot1}a_0 \\
\vdots\\
&a_{2k} = \frac{2^{2k}(-\frac{n}{2})_k}{(2k)!}a_0
\end{align*} (x)_\mu \begin{equation}
y_{even}(x) = \displaystyle\sum_{k = 0}^\infty a_{2k}x^{2k} = a_0\left(1+ \displaystyle\sum_{k=1}^\infty \frac{2^{2k}(-\frac{n}{2})_k}{(2k)!}x^{2k}\right)
\end{equation} \begin{equation}
y_{2n}(x) = c_0 \left(1+\displaystyle\sum_{k=1}^n\frac{2^{2k}(-n)_k}{(2k)!}x^{2k}\right)
\end{equation} (-\frac{n}{2})_k (-n)_k k = 1+\frac{n}{2} c_0 c_0= (-1)^n2^n(2n-1)!! a_0","['ordinary-differential-equations', 'power-series', 'hermite-polynomials']"
8,Find all functions whose convolution is the same as their square.,Find all functions whose convolution is the same as their square.,,"Find all functions who's self-convolution is the same as their square. To make it explicit, find all $f:\mathbb{R} \rightarrow \mathbb{R}$ such that: $$ \int_{-\infty}^{\infty} f(t - \tau)f(\tau) d\tau = (f(t))^2 $$ Or the same problem on a limited domain $f:(-a,a) \rightarrow \mathbb{R}$ $$ \int_{-a}^{a} f(t - \tau)f(\tau) d\tau = (f(t))^2 $$ Edit: Anne Bauval has asked for a motivation for this problem.  If you consider any non-linear ODE in 1D with derivatives beyond the first, there are no simple methods of solution.  For example: $$x''' + (x'')^2 + x^2 x' = 0$$ The fourier transform of these types of problems become polynomials of multiplication and convolution for example: $$(i\omega)^3\hat{x}(\omega) + [(i\omega)^2\hat{x}(\omega)] * [(i\omega)^2\hat{x}(\omega)] + [(i\omega)\hat{x}(\omega)] * \hat{x}(\omega) * \hat{x}(\omega) = 0$$ The above example is the simplest polynomial using multiplication and convolution. So, solving this problem would be a first step towards solving more complex convolution and multiplication polynomials. At some point, this method could allow you to solve previously unsolvable non-linear ODEs. Bruno B.: Pointed out a related, but not identical question math.stackexchange.com/q/3336615/1104384","Find all functions who's self-convolution is the same as their square. To make it explicit, find all such that: Or the same problem on a limited domain Edit: Anne Bauval has asked for a motivation for this problem.  If you consider any non-linear ODE in 1D with derivatives beyond the first, there are no simple methods of solution.  For example: The fourier transform of these types of problems become polynomials of multiplication and convolution for example: The above example is the simplest polynomial using multiplication and convolution. So, solving this problem would be a first step towards solving more complex convolution and multiplication polynomials. At some point, this method could allow you to solve previously unsolvable non-linear ODEs. Bruno B.: Pointed out a related, but not identical question math.stackexchange.com/q/3336615/1104384","f:\mathbb{R} \rightarrow \mathbb{R} 
\int_{-\infty}^{\infty} f(t - \tau)f(\tau) d\tau = (f(t))^2
 f:(-a,a) \rightarrow \mathbb{R} 
\int_{-a}^{a} f(t - \tau)f(\tau) d\tau = (f(t))^2
 x''' + (x'')^2 + x^2 x' = 0 (i\omega)^3\hat{x}(\omega) + [(i\omega)^2\hat{x}(\omega)] * [(i\omega)^2\hat{x}(\omega)] + [(i\omega)\hat{x}(\omega)] * \hat{x}(\omega) * \hat{x}(\omega) = 0","['functional-analysis', 'complex-analysis', 'ordinary-differential-equations', 'convolution']"
9,When is a solution $P(f'(x)) = Q(f(x))$ periodic or double periodic?,When is a solution  periodic or double periodic?,P(f'(x)) = Q(f(x)),"Consider the differential equation $$P(f '(x)) = Q(f(x))$$ Where $P(x),Q(x)$ are polynomials. Examples are $f'(x) = 1 + f(x)^2$ where we get a tan solution and $f'(x)^2 = 4 f(x)^3 - g_2 f(x) - g_3$ where we get a Weierstrass elliptic function solution. One of them is periodic , the other double periodic. In general, When is a solution $P(f'(x)) = Q(f(x))$ periodic or double periodic ? I looked at some famous elliptic functions and most of them are defined with 2 or 3 functions like the Dixon elliptic functions with $cm'(x) = - sm^2(x),sm'(x) = cm^2(x)$ what is related to the Fermat curve $x^3 + y^3 = 1$ and the Eisenstein integers. Or the lemniscate elliptic functions with $sl'(x) = (1+ sl^2(x)) cl(x) , cl'(x) = -(1 + cl^2(x)) sl(x)$ . However the lemniscate elliptic function $sl$ also satisfies a selfreference one : $(sl'(x))^2 = 1 - sl(x)^4$ thereby satifying the type of differential equation I was looking for. It is basically just that solving $$P(x) + Q(y) = 1$$ for $x$ or $y$ results in at least one function that satisfies : $$P(f '(x)) = Q(f(x))$$ with the right initial conditions. Usually what I find is that the degrees of $P$ and $Q$ are between $2$ and $4$ . So, what is going on ? Does every pair of polynomials $P,Q$ with degrees between $2$ and $4$ give double periodic functions ? Are degrees above $4$ possible to get double periodic functions ? And when do we get periodic functions that are not double periodic ? Does the Fermat curve $$x^5 + y^5 = 1 $$ or $$x^7 + y^7 = 1$$ and their related differential equations give us double periodic functions ? I want to point out that a function of a periodic function is also periodic and the same applies to the double periodic case. Also we get the trivial case for a polynomial $M(x)$ : $$ M(P(f'(x))) = M(Q(f(x)))$$ which has as its solutions the same function $f$ as if $M$ was the identity function. What basicly is an answer to my question of bounded degree, but I am looking for more insightful and general results ofcourse. Some ideas I had were plugging in a fourier series with variable coefficients. But I was dealing with infinitely many variables and not sure if my fourier series was even valid ; was it still analytic and did it still agree with the function it was describing ?? Another ideas was an analogue for fourier series, a series expansion for double periodic functions. But I got stuck there too. Not sure if that was going in the right direction or not. Even if that works, I want to prove it does. I tried some (complex analysis and geometric function theory ) theorems but they had problems with the poles and analytic continuation around those. The taylor radius was too small and fourier requires $L^2$ spaces anyways. I might be able to solve a specific case but I want the general idea. I am ofcourse slightly aware of some basic results such as relating the period(if it exists) with some coefficients ( such as $g_2,g_3$ in the Weierstrass case ) and rewriting the equations as an integral. Or writing functions in terms of eachother. Or some infinite sums. But that does not give me the insight I seek. I am not an expert at the addition formula's but I also understand that an addition formula implies periodic or double periodic. But again that does not give me what I seek. How to look at this ?","Consider the differential equation Where are polynomials. Examples are where we get a tan solution and where we get a Weierstrass elliptic function solution. One of them is periodic , the other double periodic. In general, When is a solution periodic or double periodic ? I looked at some famous elliptic functions and most of them are defined with 2 or 3 functions like the Dixon elliptic functions with what is related to the Fermat curve and the Eisenstein integers. Or the lemniscate elliptic functions with . However the lemniscate elliptic function also satisfies a selfreference one : thereby satifying the type of differential equation I was looking for. It is basically just that solving for or results in at least one function that satisfies : with the right initial conditions. Usually what I find is that the degrees of and are between and . So, what is going on ? Does every pair of polynomials with degrees between and give double periodic functions ? Are degrees above possible to get double periodic functions ? And when do we get periodic functions that are not double periodic ? Does the Fermat curve or and their related differential equations give us double periodic functions ? I want to point out that a function of a periodic function is also periodic and the same applies to the double periodic case. Also we get the trivial case for a polynomial : which has as its solutions the same function as if was the identity function. What basicly is an answer to my question of bounded degree, but I am looking for more insightful and general results ofcourse. Some ideas I had were plugging in a fourier series with variable coefficients. But I was dealing with infinitely many variables and not sure if my fourier series was even valid ; was it still analytic and did it still agree with the function it was describing ?? Another ideas was an analogue for fourier series, a series expansion for double periodic functions. But I got stuck there too. Not sure if that was going in the right direction or not. Even if that works, I want to prove it does. I tried some (complex analysis and geometric function theory ) theorems but they had problems with the poles and analytic continuation around those. The taylor radius was too small and fourier requires spaces anyways. I might be able to solve a specific case but I want the general idea. I am ofcourse slightly aware of some basic results such as relating the period(if it exists) with some coefficients ( such as in the Weierstrass case ) and rewriting the equations as an integral. Or writing functions in terms of eachother. Or some infinite sums. But that does not give me the insight I seek. I am not an expert at the addition formula's but I also understand that an addition formula implies periodic or double periodic. But again that does not give me what I seek. How to look at this ?","P(f '(x)) = Q(f(x)) P(x),Q(x) f'(x) = 1 + f(x)^2 f'(x)^2 = 4 f(x)^3 - g_2 f(x) - g_3 P(f'(x)) = Q(f(x)) cm'(x) = - sm^2(x),sm'(x) = cm^2(x) x^3 + y^3 = 1 sl'(x) = (1+ sl^2(x)) cl(x) , cl'(x) = -(1 + cl^2(x)) sl(x) sl (sl'(x))^2 = 1 - sl(x)^4 P(x) + Q(y) = 1 x y P(f '(x)) = Q(f(x)) P Q 2 4 P,Q 2 4 4 x^5 + y^5 = 1  x^7 + y^7 = 1 M(x)  M(P(f'(x))) = M(Q(f(x))) f M L^2 g_2,g_3","['ordinary-differential-equations', 'polynomials', 'fourier-analysis', 'periodic-functions', 'elliptic-functions']"
10,Prove that every solution to $z''+e^{z^2}=1$ is periodic.,Prove that every solution to  is periodic.,z''+e^{z^2}=1,"This is a problem from a homework assignment that is causing me some trouble. The technique provided in the section is as follows. Set $x=z,y=z'$ and convert the second order ODE to a linear system. In this case, we have $$(x,y)'=(y,1-e^{x^2})$$ Find the orbits of the linear system by solving $\frac{dy}{dx}=\frac{1-e^{x^2}}{y}$ . The orbits turn out to be level sets of $(x,y)\rightarrow \frac{y^2}{2}+\int_0^x e^{t^2}\mathrm{d}t-x$ If an aforementioned orbit forms a closed curve that doesn't contain an equilibrium point (the only equilibrium point of this system is the origin, FYI) then any solution to this system passing through a point on said orbit is periodic. Unfortunately, these level sets are not closed. Is there any other way to attack this problem? I assumed the problem set required the student to solve the problem using this theory since this theory was presented and proved in the corresponding section.","This is a problem from a homework assignment that is causing me some trouble. The technique provided in the section is as follows. Set and convert the second order ODE to a linear system. In this case, we have Find the orbits of the linear system by solving . The orbits turn out to be level sets of If an aforementioned orbit forms a closed curve that doesn't contain an equilibrium point (the only equilibrium point of this system is the origin, FYI) then any solution to this system passing through a point on said orbit is periodic. Unfortunately, these level sets are not closed. Is there any other way to attack this problem? I assumed the problem set required the student to solve the problem using this theory since this theory was presented and proved in the corresponding section.","x=z,y=z' (x,y)'=(y,1-e^{x^2}) \frac{dy}{dx}=\frac{1-e^{x^2}}{y} (x,y)\rightarrow \frac{y^2}{2}+\int_0^x e^{t^2}\mathrm{d}t-x",[]
11,Verify the solution to a Bessel's equation has finite length or not,Verify the solution to a Bessel's equation has finite length or not,,"Consider the following Bessel's equation $$\ddot{x}(t) + \frac{3}{t}\dot{x}(t) + x(t) = 0$$ with initial condition $x(0)=1$ and $\dot{x}(0)=0$ . From WolframAlpha, we know the solution is given by $$x(t) = \frac{2J_1(t)}{t}$$ where $J_1(t)$ is the Bessel's function of the first kind with degree $1$ . It is easy to see that $x(t)\to 0$ as $t\to\infty$ . My question is that can we prove the trajectory of the solution $x(t)$ has finite length, i.e., $$\int_0^\infty |\dot{x}(t)| dt <\infty$$ I find that the trajectory is oscillating, so it seems that the trajectory length is pretty long, but since we have extra $1/t$ factor, maybe it makes the solution converge fast enough so that the length is indeed finite. Any ideas to prove or disprove would be helpful. My trial: \begin{align} \int_0^\infty |\dot{x}(t)| dt &= \int_0^\infty \left|\frac{2J_1'(t)}{t}-\frac{2J_1(t)}{t^2}\right|dt \\ &= \int_0^\infty \left|\frac{J_0(t)-J_2(t)}{t}-\frac{2J_1(t)}{t^2}\right|dt \end{align} Thus, a sufficient condition for the above quantity to be finite is $$\frac{J_0(t)}{t},\; \frac{J_2(t)}{t} \text{ and } \frac{J_1(t)}{t^2} \text{ are integrable on } \mathbb{R}_+.$$ From WolframAlpha I know $|J_1(t)|\leq 1$ for all $t\in\mathbb{R}_+$ , so the last term above is obviously integrable. It suffices to check the first two are integrable. I guess $J_0(t)$ and $J_2(t)$ will have similar behavior at infinity, but I don't know how that implies integrability of $J_0(t)/t$ and $J_2(t)/t$ .","Consider the following Bessel's equation with initial condition and . From WolframAlpha, we know the solution is given by where is the Bessel's function of the first kind with degree . It is easy to see that as . My question is that can we prove the trajectory of the solution has finite length, i.e., I find that the trajectory is oscillating, so it seems that the trajectory length is pretty long, but since we have extra factor, maybe it makes the solution converge fast enough so that the length is indeed finite. Any ideas to prove or disprove would be helpful. My trial: Thus, a sufficient condition for the above quantity to be finite is From WolframAlpha I know for all , so the last term above is obviously integrable. It suffices to check the first two are integrable. I guess and will have similar behavior at infinity, but I don't know how that implies integrability of and .","\ddot{x}(t) + \frac{3}{t}\dot{x}(t) + x(t) = 0 x(0)=1 \dot{x}(0)=0 x(t) = \frac{2J_1(t)}{t} J_1(t) 1 x(t)\to 0 t\to\infty x(t) \int_0^\infty |\dot{x}(t)| dt <\infty 1/t \begin{align}
\int_0^\infty |\dot{x}(t)| dt &= \int_0^\infty \left|\frac{2J_1'(t)}{t}-\frac{2J_1(t)}{t^2}\right|dt \\
&= \int_0^\infty \left|\frac{J_0(t)-J_2(t)}{t}-\frac{2J_1(t)}{t^2}\right|dt
\end{align} \frac{J_0(t)}{t},\; \frac{J_2(t)}{t} \text{ and } \frac{J_1(t)}{t^2} \text{ are integrable on } \mathbb{R}_+. |J_1(t)|\leq 1 t\in\mathbb{R}_+ J_0(t) J_2(t) J_0(t)/t J_2(t)/t","['ordinary-differential-equations', 'bessel-functions', 'stability-in-odes']"
12,Differential equation in $\mathcal D'$ (generalized function/distributions),Differential equation in  (generalized function/distributions),\mathcal D',"I'm struggling with understending of how should I solve differential equations in distributions. For example,if I'm asked to solve in $\mathcal D'$ the following equation: $$ F'-2F=0$$ Okay, so $F'=2F$ in terms of distributions, which means $F'$ and $2F$ act the same on every test function. Recalling the definition: $$\langle F', \varphi \rangle = - \langle F, \varphi' \rangle \ \ \ \ \ \ \ \langle 2F, \varphi \rangle = \langle F, 2\varphi \rangle$$ So should I write it in the way $$ -\int\limits_{\mathbf R} F\varphi'\ dx =2\int\limits_{\mathbf R} F\varphi\ dx $$ But it feels like I go back from definition and losing solutions which do not have this form. Reading about distributions I found out that sometimes problems are solved with ""well-known"" little facts, perhaps I miss something like that. I'm sure I have to reduce this equation to something known, like, $F'=\delta(x)$ , but I don't see how I can do that. So basically I'm looking for hints in techniques or facts/tricks that could help me to solve this and other linear DE in distributions. Thanks!","I'm struggling with understending of how should I solve differential equations in distributions. For example,if I'm asked to solve in the following equation: Okay, so in terms of distributions, which means and act the same on every test function. Recalling the definition: So should I write it in the way But it feels like I go back from definition and losing solutions which do not have this form. Reading about distributions I found out that sometimes problems are solved with ""well-known"" little facts, perhaps I miss something like that. I'm sure I have to reduce this equation to something known, like, , but I don't see how I can do that. So basically I'm looking for hints in techniques or facts/tricks that could help me to solve this and other linear DE in distributions. Thanks!","\mathcal D'  F'-2F=0 F'=2F F' 2F \langle F', \varphi \rangle = - \langle F, \varphi' \rangle \ \ \ \ \ \ \ \langle 2F, \varphi \rangle = \langle F, 2\varphi \rangle  -\int\limits_{\mathbf R} F\varphi'\ dx =2\int\limits_{\mathbf R} F\varphi\ dx  F'=\delta(x)","['ordinary-differential-equations', 'distribution-theory']"
13,Proving that a curve is closed,Proving that a curve is closed,,"Consider the $2$ -nd order dynamic system defined as: $\dot{x}_1=x_2, \qquad\dot{x}_2=-g(x_1)$ where $g$ is a continuously differentiable function and $\xi\cdot g(\xi)>0, \forall \xi \in (-a,a),$ with $a$ being a known positive scalar. Defining the following energy function: $V(x)=\frac{1}{2}x_2^2 + \int_{0}^{x_1}g(\xi) d\xi$ Is it true that, for sufficiently small $||x(0)||$ , every solution is periodic? (We can easily show that $\dot{V}(x)=0$ over any phase trajectory, namely $V(x)=constant$ . However, before analysing whether a solution is periodic, I do not know how to show that the curve $V(x)=constant$ is closed). Thanks in advance!","Consider the -nd order dynamic system defined as: where is a continuously differentiable function and with being a known positive scalar. Defining the following energy function: Is it true that, for sufficiently small , every solution is periodic? (We can easily show that over any phase trajectory, namely . However, before analysing whether a solution is periodic, I do not know how to show that the curve is closed). Thanks in advance!","2 \dot{x}_1=x_2, \qquad\dot{x}_2=-g(x_1) g \xi\cdot g(\xi)>0, \forall \xi \in (-a,a), a V(x)=\frac{1}{2}x_2^2 + \int_{0}^{x_1}g(\xi) d\xi ||x(0)|| \dot{V}(x)=0 V(x)=constant V(x)=constant","['ordinary-differential-equations', 'dynamical-systems', 'classical-mechanics']"
14,Global existence of solution to first order complex-valued differential equation,Global existence of solution to first order complex-valued differential equation,,"Consider a trivial fiber bundle $\mathbb{R} \times M \rightarrow M$ with a fundamental vector field $\xi$ running up the fibers. I would like to study the pair of differential equations $$\mathcal{L}_{\xi} f_1 = F(x,f_1,f_2) $$ $$\mathcal{L}_{\xi} f_2 = G(x,f_1,f_2) $$ Here, $x$ are coordinates on the total space $\mathbb{R} \times M$ , and assume that everything here is real. Note that $F,G$ are not in general homogeneous functions of $f_1$ or $f_2$ . Under what conditions do global solutions $f_1,f_2$ exist? My first instinct was Picard's theorem, but as far as I know, that doesn't necessarily apply unless I can decouple $f_1$ and $f_2$ . After some googling I came upon the Cauchy–Kowalevski theorem, however that does not seem to imply global existence, which is necessary for my application. Is this problem too general to say anything about, or are there known results for such differential equations?","Consider a trivial fiber bundle with a fundamental vector field running up the fibers. I would like to study the pair of differential equations Here, are coordinates on the total space , and assume that everything here is real. Note that are not in general homogeneous functions of or . Under what conditions do global solutions exist? My first instinct was Picard's theorem, but as far as I know, that doesn't necessarily apply unless I can decouple and . After some googling I came upon the Cauchy–Kowalevski theorem, however that does not seem to imply global existence, which is necessary for my application. Is this problem too general to say anything about, or are there known results for such differential equations?","\mathbb{R} \times M \rightarrow M \xi \mathcal{L}_{\xi} f_1 = F(x,f_1,f_2)  \mathcal{L}_{\xi} f_2 = G(x,f_1,f_2)  x \mathbb{R} \times M F,G f_1 f_2 f_1,f_2 f_1 f_2","['ordinary-differential-equations', 'differential-geometry']"
15,Must solutions to the time-independent Schrodinger equation that have discrete spectra or negative eigenvalues be square-integrable?,Must solutions to the time-independent Schrodinger equation that have discrete spectra or negative eigenvalues be square-integrable?,,"Consider the following version of the time-independent Schrodinger equation: $$ \left( -\frac{d^2}{dx^2} + V(x) \right) \psi(x) = \lambda\ \psi(x) $$ (where we have absorbed some unimportant physical constants into the function $\psi(x)$ ). The function $V(x)$ is a given real smooth function $\mathbb{R} \to \mathbb{R}$ , which we assume to be continuous and to approach 0 at large arguments: $$ \lim_{|x| \to \infty} V(x) = 0. $$ The smooth complex-valued function $\psi:\mathbb{R} \to \mathbb{C}$ and the real constant $\lambda \in \mathbb{R}$ are to be determined. This equation is simply the eigenvalue equation for the linear second-order differential operator in parentheses. (We can loosen the smoothness requirements on $V(x)$ and $\psi(x)$ , if doing so makes the problem more tractable.) Non-rigorous physical heuristics suggest that these three statements are equivalent: $\psi(x)$ is square-integrable, i.e. $$\int \limits_{-\infty}^\infty dx\ |\psi(x)|^2 < \infty,$$ $\lambda < 0$ , and $\lambda$ lies in a discrete part of the eigenvalue spectrum of the differential operator in parentheses, i.e. there exists a proper real interval such that $\lambda$ is the only eigenvalue in the differential operator's spectrum that lies within that interval. A related piece of ""folk wisdom"" considers the same eigenvalue equation in the case where $$ \lim_{|x| \to \infty} V(x) = +\infty, $$ and claims that in this case, (a) all eigenfunctions $\psi(x)$ must be square-integrable, and (b) the eigenvalue spectrum of the differential operator must be discrete. But this ""folk wisdom"" is incorrect. This answer gives an explicit example of a function $V(x)$ and a square-integrable eigenfunction $\psi(x)$ with positive eigenvalue $\lambda$ . Therefore, statement #1 above does not imply statement #2. (I do not know whether the spectrum for the particular differential operator given in that example is discrete or continuous around the relevant eigenvalue $\lambda = 1$ , so I don't know the status of claim #3 for this example.) What are the exact implications between the three statements above? Of the six possible implications, which have been proven to be true, which (other than $1 \implies 2$ ) have explicit known counterexamples, and which are still open problems? I'd also like to know about the case of multiple spatial dimensions, although I assume that the answers are probably the same as for the 1D case.","Consider the following version of the time-independent Schrodinger equation: (where we have absorbed some unimportant physical constants into the function ). The function is a given real smooth function , which we assume to be continuous and to approach 0 at large arguments: The smooth complex-valued function and the real constant are to be determined. This equation is simply the eigenvalue equation for the linear second-order differential operator in parentheses. (We can loosen the smoothness requirements on and , if doing so makes the problem more tractable.) Non-rigorous physical heuristics suggest that these three statements are equivalent: is square-integrable, i.e. , and lies in a discrete part of the eigenvalue spectrum of the differential operator in parentheses, i.e. there exists a proper real interval such that is the only eigenvalue in the differential operator's spectrum that lies within that interval. A related piece of ""folk wisdom"" considers the same eigenvalue equation in the case where and claims that in this case, (a) all eigenfunctions must be square-integrable, and (b) the eigenvalue spectrum of the differential operator must be discrete. But this ""folk wisdom"" is incorrect. This answer gives an explicit example of a function and a square-integrable eigenfunction with positive eigenvalue . Therefore, statement #1 above does not imply statement #2. (I do not know whether the spectrum for the particular differential operator given in that example is discrete or continuous around the relevant eigenvalue , so I don't know the status of claim #3 for this example.) What are the exact implications between the three statements above? Of the six possible implications, which have been proven to be true, which (other than ) have explicit known counterexamples, and which are still open problems? I'd also like to know about the case of multiple spatial dimensions, although I assume that the answers are probably the same as for the 1D case.","
\left( -\frac{d^2}{dx^2} + V(x) \right) \psi(x) = \lambda\ \psi(x)
 \psi(x) V(x) \mathbb{R} \to \mathbb{R} 
\lim_{|x| \to \infty} V(x) = 0.
 \psi:\mathbb{R} \to \mathbb{C} \lambda \in \mathbb{R} V(x) \psi(x) \psi(x) \int \limits_{-\infty}^\infty dx\ |\psi(x)|^2 < \infty, \lambda < 0 \lambda \lambda 
\lim_{|x| \to \infty} V(x) = +\infty,
 \psi(x) V(x) \psi(x) \lambda \lambda = 1 1 \implies 2","['functional-analysis', 'ordinary-differential-equations', 'lp-spaces', 'eigenfunctions', 'sturm-liouville']"
16,An enquiry regarding asymptotic expansion of Jacobi function.,An enquiry regarding asymptotic expansion of Jacobi function.,,"For $\alpha,\beta \in \mathbb{C},\, \alpha$ a non-negative integer, we define $$A_{\alpha,\beta}(t)=(\sinh t)^{2\alpha+1}(\cosh t)^{2\beta+1} $$ and $$ \mathcal{L}_{\alpha,\beta}=\frac{d^2}{dt^2}+\frac{A'_{\alpha,\beta}(t)}{A_{\alpha,\beta}(t)}\frac{d}{dt}.$$ Then the Jacobi function $\phi_\lambda^{\alpha,\beta}(r)$ is the unique solution on $(0,\infty)$ of the initial value problem $$\mathcal{L}_{\alpha,\beta}f=-(\lambda^2+(\alpha+\beta+1)^2)f,\, f(0)=1,f'(0)=0. $$ We have the following asymptotic expansion $\phi_\lambda^{\alpha,\beta}$ (see [1, p. 219]) for $\alpha,\beta $ half odd integers with $\alpha> -\frac{1}{2},$ $$ \sqrt{A_{\alpha,\beta}(t)}\phi_\lambda^{\alpha,\beta}(t)= \sum_{m=0}^M a_m(t)\frac{\mathcal{J}_{\alpha+m}(\lambda t)}{\lambda^{m+\alpha+1/2}}+R_M(λ, t),$$ where $a_m$ are holomorphic and $M\geq 0.$ $\mathcal{J}_{\alpha}(x)=\sqrt{x}J_\alpha(x),$ where $J_\alpha$ is the Bessel function of first kind. My question: Is the above asymptotic expansion also valid when $\alpha> -1$ and $\beta$ is any integer such that $\alpha \pm\beta>-1$ ? It will be immensely helpful if someone suggests some papers or books regarding this. Thanks in advance. [1] Brandolini, Luca, Gigante, Giacomo: Equiconvergence theorems for chebli-trimeche hypergroups. Ann. Sc. Norm. Super. Pisa Cl. Sci. (5) 8(2), 211–265 (2009)","For a non-negative integer, we define and Then the Jacobi function is the unique solution on of the initial value problem We have the following asymptotic expansion (see [1, p. 219]) for half odd integers with where are holomorphic and where is the Bessel function of first kind. My question: Is the above asymptotic expansion also valid when and is any integer such that ? It will be immensely helpful if someone suggests some papers or books regarding this. Thanks in advance. [1] Brandolini, Luca, Gigante, Giacomo: Equiconvergence theorems for chebli-trimeche hypergroups. Ann. Sc. Norm. Super. Pisa Cl. Sci. (5) 8(2), 211–265 (2009)","\alpha,\beta \in \mathbb{C},\, \alpha A_{\alpha,\beta}(t)=(\sinh t)^{2\alpha+1}(\cosh t)^{2\beta+1}   \mathcal{L}_{\alpha,\beta}=\frac{d^2}{dt^2}+\frac{A'_{\alpha,\beta}(t)}{A_{\alpha,\beta}(t)}\frac{d}{dt}. \phi_\lambda^{\alpha,\beta}(r) (0,\infty) \mathcal{L}_{\alpha,\beta}f=-(\lambda^2+(\alpha+\beta+1)^2)f,\, f(0)=1,f'(0)=0.  \phi_\lambda^{\alpha,\beta} \alpha,\beta  \alpha> -\frac{1}{2},  \sqrt{A_{\alpha,\beta}(t)}\phi_\lambda^{\alpha,\beta}(t)= \sum_{m=0}^M a_m(t)\frac{\mathcal{J}_{\alpha+m}(\lambda t)}{\lambda^{m+\alpha+1/2}}+R_M(λ, t), a_m M\geq 0. \mathcal{J}_{\alpha}(x)=\sqrt{x}J_\alpha(x), J_\alpha \alpha> -1 \beta \alpha \pm\beta>-1","['real-analysis', 'ordinary-differential-equations', 'asymptotics', 'special-functions']"
17,Prove that differential equations are right continuous with respect to initial values.,Prove that differential equations are right continuous with respect to initial values.,,"Here the question. Consider the initial value problem: \begin{equation} \begin{cases} \frac{dy}{dx}=f(x,y) \\y(x_0)=y_0, \end{cases} \end{equation} $f(x,y)$ is continuous. Suppose that $y=\phi(x;x_0,y_0)$ is the maximum solution of the initial value problem. Prove: $\phi(x;x_0,y_0)$ is right continuous for $y$ , that is \begin{equation} \lim_{y_1\to y_0^+}\phi(x;x_0,y_1)=\phi(x;x_0,y_0) \end{equation} establishs on $|x-x_0|\leq \alpha$ , $\alpha$ is constant. Below is my idea. Consider the equations \begin{equation} (*)_n=\begin{cases}\frac{dy}{dx}=f(x,y)+\frac{1}{n}, \\y(x_0)=y_0, \end{cases} \end{equation} write the sequence of solutions as $\{\phi_n(x;x_0,y_0)\}$ .It has a uniformly convergent subsequence,let itself be uniformly convergent. Assume that $\phi(x;x_0,y_0)$ is not right continuous for $y_0$ , then $\exists \epsilon>0,\forall \delta>0,\exists y_{\delta}-y_0<\delta$ , such that $$|\phi(x;x_0,y_{\delta})-\phi(x;x_0,y_0)|\geq  \epsilon.$$ Let $\{\delta_n\}$ decreases to $0$ ,The corresponding $y_{\delta}$ is denoted by $y_n$ . According to the uniform convergence, $\exists N>0,\forall n>N$ , we have \begin{equation} |\phi_n(x;x_0,y_0)-\phi(x;x_0,y_0)|<\epsilon. \end{equation} Then we can get \begin{equation} |\phi(x;x_0,y_{n})-\phi(x;x_0,y_0)|\leq|\phi(x;x_0,y_{n})-\phi_n(x;x_0,y_{0})|+|\phi_n(x;x_0,y_{0})-\phi(x;x_0,y_0)|. \end{equation} The latter item can be controlled by $\epsilon$ ,but I don't know how to handle the previous item.Do you have a good idea? I really appreciate it!","Here the question. Consider the initial value problem: is continuous. Suppose that is the maximum solution of the initial value problem. Prove: is right continuous for , that is establishs on , is constant. Below is my idea. Consider the equations write the sequence of solutions as .It has a uniformly convergent subsequence,let itself be uniformly convergent. Assume that is not right continuous for , then , such that Let decreases to ,The corresponding is denoted by . According to the uniform convergence, , we have Then we can get The latter item can be controlled by ,but I don't know how to handle the previous item.Do you have a good idea? I really appreciate it!","\begin{equation}
\begin{cases} \frac{dy}{dx}=f(x,y) \\y(x_0)=y_0,
\end{cases}
\end{equation} f(x,y) y=\phi(x;x_0,y_0) \phi(x;x_0,y_0) y \begin{equation}
\lim_{y_1\to y_0^+}\phi(x;x_0,y_1)=\phi(x;x_0,y_0)
\end{equation} |x-x_0|\leq \alpha \alpha \begin{equation}
(*)_n=\begin{cases}\frac{dy}{dx}=f(x,y)+\frac{1}{n}, \\y(x_0)=y_0,
\end{cases}
\end{equation} \{\phi_n(x;x_0,y_0)\} \phi(x;x_0,y_0) y_0 \exists \epsilon>0,\forall \delta>0,\exists y_{\delta}-y_0<\delta |\phi(x;x_0,y_{\delta})-\phi(x;x_0,y_0)|\geq  \epsilon. \{\delta_n\} 0 y_{\delta} y_n \exists N>0,\forall n>N \begin{equation}
|\phi_n(x;x_0,y_0)-\phi(x;x_0,y_0)|<\epsilon.
\end{equation} \begin{equation}
|\phi(x;x_0,y_{n})-\phi(x;x_0,y_0)|\leq|\phi(x;x_0,y_{n})-\phi_n(x;x_0,y_{0})|+|\phi_n(x;x_0,y_{0})-\phi(x;x_0,y_0)|.
\end{equation} \epsilon","['ordinary-differential-equations', 'initial-value-problems']"
18,Some questions on the backwards heat equation,Some questions on the backwards heat equation,,"Let $\Delta f = f''$ on $\mathbb R$ ; we know the semigroup $(P_t)_{t \geq 0} = (e^{t\Delta})_{t \geq 0}$ acts on functions $f \in L^2(\mathbb R)$ via the heat kernel $$ u(x, t) = P_tf(x) = \int_{\mathbb R} \frac{1}{(4\pi t)^{1/2}}e^{-|x-y|^2/(4t)}f(y)dy. $$ This makes sense though for any function $f$ growing slower than any Gaussian, e.g. if $f(x) = e^x$ then $P_tf(x) = e^te^x$ , and if $f(x) = x^2$ then $P_tf(x) = x^2 + 2t$ . If I prescribe data at, say, $t = 1$ , then I can solve backwards in time: if $u(x, 1) = P_1f(x) = e^x$ or $u(x, 1) = x^2$ then respectively the solution is $u(x, 0) = f(x) = e^{-t}e^x$ or $u(x, 0) = x^2 - 2$ . Even if $u$ has very rapid growth at infinity, e.g. $u(x, 1) = e^{\alpha x^2}$ with $\alpha > 0$ , we can solve to find $f(x) = \sqrt{1+4\alpha}e^{\alpha x^2/(1 + 4\alpha)}$ . Formally, I know the solution should be $$e^{-\Delta}u(x, 1) = \sum_{k \geq 0} \frac{(-1)^k}{k!}\Delta^k u(x, 1)$$ which, e.g. for polynomials, is a finite sum and, e.g. for exponentials (and probably the Gaussian too), is a convergent infinite sum. My question is: is there a class of functions (to which these non $L^2(\mathbb R)$ examples belong) for which this inversion/backwards heat equation makes sense and has nice estimates? For instance, if $u \in L^2(\gamma)$ where $\gamma$ is the standard Gaussian measure, we can write $u$ as an infinite sum of $u_n = \sum_{i=0}^\infty a_iH_i$ of Hermite polynomials (being an orthonormal base for $L^2(\gamma)$ ), and with enough decay on the $(a_i)$ the formal sum $f_n = e^{-\Delta}u_n$ converges to some $f \in L^2(\gamma)$ and it has the expected behaviour (that is, $P_1f = u$ ). But in doing so I lose (or do not know how to recover) statements about smoothness (which I expect is important since the heat kernel is smoothing) or even continuity. I have an inkling this question is also related to the nature of the error term (based off a paper [""A well posed problem for the backward heat equation"" by Miranker) in the expansion $$e^{-\Delta}u = u - \Delta u + \frac{1}{2}\Delta^2 u - \cdots$$ and I suspect the answer is that the question is ""well-posed"" if $u(x, 1) \leq Ce^{\alpha x}$ has at most exponential growth based on some simple examples for which the error $|e^{-\Delta u} - u| \leq C(1 + |\Delta u|)$ is roughly controlled by the first term. But I do not know if this problem is no longer well-posed if I introduce some pathological behaviour, e.g. by adding a fast oscillation like $u(x, 1) = e^x + x^2\sin^2(x^2)$ and if I need some extra conditions to prohibit this.","Let on ; we know the semigroup acts on functions via the heat kernel This makes sense though for any function growing slower than any Gaussian, e.g. if then , and if then . If I prescribe data at, say, , then I can solve backwards in time: if or then respectively the solution is or . Even if has very rapid growth at infinity, e.g. with , we can solve to find . Formally, I know the solution should be which, e.g. for polynomials, is a finite sum and, e.g. for exponentials (and probably the Gaussian too), is a convergent infinite sum. My question is: is there a class of functions (to which these non examples belong) for which this inversion/backwards heat equation makes sense and has nice estimates? For instance, if where is the standard Gaussian measure, we can write as an infinite sum of of Hermite polynomials (being an orthonormal base for ), and with enough decay on the the formal sum converges to some and it has the expected behaviour (that is, ). But in doing so I lose (or do not know how to recover) statements about smoothness (which I expect is important since the heat kernel is smoothing) or even continuity. I have an inkling this question is also related to the nature of the error term (based off a paper [""A well posed problem for the backward heat equation"" by Miranker) in the expansion and I suspect the answer is that the question is ""well-posed"" if has at most exponential growth based on some simple examples for which the error is roughly controlled by the first term. But I do not know if this problem is no longer well-posed if I introduce some pathological behaviour, e.g. by adding a fast oscillation like and if I need some extra conditions to prohibit this.","\Delta f = f'' \mathbb R (P_t)_{t \geq 0} = (e^{t\Delta})_{t \geq 0} f \in L^2(\mathbb R)  u(x, t) = P_tf(x) = \int_{\mathbb R} \frac{1}{(4\pi t)^{1/2}}e^{-|x-y|^2/(4t)}f(y)dy.  f f(x) = e^x P_tf(x) = e^te^x f(x) = x^2 P_tf(x) = x^2 + 2t t = 1 u(x, 1) = P_1f(x) = e^x u(x, 1) = x^2 u(x, 0) = f(x) = e^{-t}e^x u(x, 0) = x^2 - 2 u u(x, 1) = e^{\alpha x^2} \alpha > 0 f(x) = \sqrt{1+4\alpha}e^{\alpha x^2/(1 + 4\alpha)} e^{-\Delta}u(x, 1) = \sum_{k \geq 0} \frac{(-1)^k}{k!}\Delta^k u(x, 1) L^2(\mathbb R) u \in L^2(\gamma) \gamma u u_n = \sum_{i=0}^\infty a_iH_i L^2(\gamma) (a_i) f_n = e^{-\Delta}u_n f \in L^2(\gamma) P_1f = u e^{-\Delta}u = u - \Delta u + \frac{1}{2}\Delta^2 u - \cdots u(x, 1) \leq Ce^{\alpha x} |e^{-\Delta u} - u| \leq C(1 + |\Delta u|) u(x, 1) = e^x + x^2\sin^2(x^2)","['real-analysis', 'functional-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'heat-equation']"
19,Problem with the differential equation $2u_{xx}-3u_{xy}+u_{yy}+u_x-u_y=1$,Problem with the differential equation,2u_{xx}-3u_{xy}+u_{yy}+u_x-u_y=1,"Specify the largest domain in which the given Cauchy problem has a single solution, and find this solution $$2u_{xx}-3u_{xy}+u_{yy}+u_x-u_y=1, \; u\Bigg|_{x=0,y>0}=-2y, \; u_x\Bigg|_{x=0,y>0}=-1$$ Note that a linear hyperbolic equation of second order is given. To find the domain in which the Cauchy problem has a single solution, we first find the characteristic equations $$2y'^2-3y+1=0\Rightarrow y'=\left \{ \frac{1}{2},1 \right \}$$ If $y' = 1/2$ we get $y = x/2 + C_1$ , then at $x = 0$ , $y = C_1$ , and $u =-2y = -2C_1$ . Hence our characteristic passes through the point $(0, C_1)$ , and $u = -2C_1$ . Given $y' = 1$ we obtain $y = x + C_2$ , then at $x = 0$ , $y = C_2$ , and $u_x = -1$ . Hence, our characteristic passes through point $(0, C_2)$ , and $u_x = -1$ . Substitute the variables $ξ = x - y$ and $η = x/2 + y$ , then $$ \begin{aligned} \frac{\partial}{\partial x} & =\frac{\partial \xi}{\partial x} \frac{\partial}{\partial \xi}+\frac{\partial \eta}{\partial x} \frac{\partial}{\partial \eta}=\frac{\partial}{\partial \xi}+\frac{1}{2} \frac{\partial}{\partial \eta} \\ \frac{\partial}{\partial y} & =\frac{\partial \xi}{\partial y} \frac{\partial}{\partial \xi}+\frac{\partial \eta}{\partial y} \frac{\partial}{\partial \eta}=-\frac{\partial}{\partial \xi}+\frac{\partial}{\partial \eta} \end{aligned} $$ $$ \begin{aligned} 2 u_{x x}-3 u_{x y}+u_{y y}=2 & \left(\frac{\partial^2}{\partial \xi^2}+\frac{\partial^2}{\partial \eta^2}\right) u-3\left(\frac{\partial^2}{\partial \xi^2}-\frac{\partial^2}{\partial \eta^2}\right) u+\left(\frac{\partial^2}{\partial \xi^2}+\frac{\partial^2}{\partial \eta^2}\right) u \Rightarrow \\ & \Rightarrow u_{\xi \eta}=\frac{1}{2} \Rightarrow u_{\xi}=\frac{1}{2} \eta+f(\xi) \Rightarrow u_{\xi \eta}=\frac{1}{4} \xi \eta+F(\xi)+G(\eta) \end{aligned} $$ \begin{gathered} \frac{1}{4} y-2 y-G(y)=-1 \Rightarrow G(y)=-\frac{7}{4} y+1 \Rightarrow F(-y)=-2 y-\left(-\frac{7}{4} y+1\right)=\frac{1}{4} y-1 \Rightarrow \\ \Rightarrow u_{\xi \eta}=\frac{1}{4} \xi \eta+\frac{1}{4} \xi-1-\frac{7}{4} \eta+1=\frac{1}{4} \xi \eta+\frac{1}{4} \xi-\frac{7}{4} \eta \Rightarrow \\ \Rightarrow u(x, y)=\frac{1}{4}(x-y)\left(\frac{1}{2} x+y\right)+\frac{1}{4}(x-y)-\frac{7}{4}\left(\frac{1}{2} x+y\right)=\frac{x^2}{8}+\frac{x y}{8}-\frac{5 x}{8}-\frac{y^2}{4}-2 y \end{gathered} The problem is that I substitute $x=0$ and don't get $-2y$ . I have solved the problem incorrectly. Can you tell me how to solve it correctly?","Specify the largest domain in which the given Cauchy problem has a single solution, and find this solution Note that a linear hyperbolic equation of second order is given. To find the domain in which the Cauchy problem has a single solution, we first find the characteristic equations If we get , then at , , and . Hence our characteristic passes through the point , and . Given we obtain , then at , , and . Hence, our characteristic passes through point , and . Substitute the variables and , then The problem is that I substitute and don't get . I have solved the problem incorrectly. Can you tell me how to solve it correctly?","2u_{xx}-3u_{xy}+u_{yy}+u_x-u_y=1, \; u\Bigg|_{x=0,y>0}=-2y, \; u_x\Bigg|_{x=0,y>0}=-1 2y'^2-3y+1=0\Rightarrow y'=\left \{ \frac{1}{2},1 \right \} y' = 1/2 y = x/2 + C_1 x = 0 y = C_1 u =-2y = -2C_1 (0, C_1) u = -2C_1 y' = 1 y = x + C_2 x = 0 y = C_2 u_x = -1 (0, C_2) u_x = -1 ξ = x - y η = x/2 + y 
\begin{aligned}
\frac{\partial}{\partial x} & =\frac{\partial \xi}{\partial x} \frac{\partial}{\partial \xi}+\frac{\partial \eta}{\partial x} \frac{\partial}{\partial \eta}=\frac{\partial}{\partial \xi}+\frac{1}{2} \frac{\partial}{\partial \eta} \\
\frac{\partial}{\partial y} & =\frac{\partial \xi}{\partial y} \frac{\partial}{\partial \xi}+\frac{\partial \eta}{\partial y} \frac{\partial}{\partial \eta}=-\frac{\partial}{\partial \xi}+\frac{\partial}{\partial \eta}
\end{aligned}
 
\begin{aligned}
2 u_{x x}-3 u_{x y}+u_{y y}=2 & \left(\frac{\partial^2}{\partial \xi^2}+\frac{\partial^2}{\partial \eta^2}\right) u-3\left(\frac{\partial^2}{\partial \xi^2}-\frac{\partial^2}{\partial \eta^2}\right) u+\left(\frac{\partial^2}{\partial \xi^2}+\frac{\partial^2}{\partial \eta^2}\right) u \Rightarrow \\
& \Rightarrow u_{\xi \eta}=\frac{1}{2} \Rightarrow u_{\xi}=\frac{1}{2} \eta+f(\xi) \Rightarrow u_{\xi \eta}=\frac{1}{4} \xi \eta+F(\xi)+G(\eta)
\end{aligned}
 \begin{gathered}
\frac{1}{4} y-2 y-G(y)=-1 \Rightarrow G(y)=-\frac{7}{4} y+1 \Rightarrow F(-y)=-2 y-\left(-\frac{7}{4} y+1\right)=\frac{1}{4} y-1 \Rightarrow \\
\Rightarrow u_{\xi \eta}=\frac{1}{4} \xi \eta+\frac{1}{4} \xi-1-\frac{7}{4} \eta+1=\frac{1}{4} \xi \eta+\frac{1}{4} \xi-\frac{7}{4} \eta \Rightarrow \\
\Rightarrow u(x, y)=\frac{1}{4}(x-y)\left(\frac{1}{2} x+y\right)+\frac{1}{4}(x-y)-\frac{7}{4}\left(\frac{1}{2} x+y\right)=\frac{x^2}{8}+\frac{x y}{8}-\frac{5 x}{8}-\frac{y^2}{4}-2 y
\end{gathered} x=0 -2y","['ordinary-differential-equations', 'partial-differential-equations', 'cauchy-problem']"
20,Stability of numerical solution of ODE,Stability of numerical solution of ODE,,"I want to solve the ODE \begin{array}{ll} -u''(x)=f(x) & x\in (0,1) \\ u(0)=g(0) \\ u(1)=g(1)\,  \\ \end{array} with finite differences using $u''(x)\approx \frac{u(x-h)-2u(x)+u(x+h)}{h^2}$ . To approximate $[0,1]$ I use a grid $T=\{0=x_0<\dots , x_n=1\}$ . This leads to the equations $\frac{1}{h^2}(-u_{i-1}+2u_i-u_{i+1})=f(x_i)$ for $1\leq i\leq n-1$ , $u_0=g(x_0), u_n=g(x_n)$ and the linear system \begin{align}  A=\frac{1}{h^2}\begin{pmatrix} 1 & 0 & 0 &  &\cdots & 0\\ -1 & 2 & -1 & 0 & \cdots & 0 \\ 0 & -1 & 2 & -1 & \cdots & 0 \\ 0 & 0 & \ddots & \ddots & \ddots & 0 \\ 0 & \cdots & 0 & -1 & 2 & -1 \\ 0 & \cdots & 0 & 0 & 0 & 1 \end{pmatrix} \begin{pmatrix} u_0\\ \vdots \\ u_n \end{pmatrix}= \begin{pmatrix} \frac{g(0)}{h^2} \\ f(x_1)\\ \vdots\\ f(x_{n-1})\\ \frac{g(1)}{h^2} \end{pmatrix} \end{align} I already showed that $(Av)_i<0 \Rightarrow v_i<0$ for all $i=0,\dots ,n$ . How can I show that there exists $C>0$ so that $\|u\|_\infty\leq C\big (\|f\|_\infty+\|g\|_\infty\big )$ ?( $u$ is a solution of the linear system)","I want to solve the ODE with finite differences using . To approximate I use a grid . This leads to the equations for , and the linear system I already showed that for all . How can I show that there exists so that ?( is a solution of the linear system)","\begin{array}{ll}
-u''(x)=f(x) & x\in (0,1) \\
u(0)=g(0) \\
u(1)=g(1)\,  \\
\end{array} u''(x)\approx \frac{u(x-h)-2u(x)+u(x+h)}{h^2} [0,1] T=\{0=x_0<\dots , x_n=1\} \frac{1}{h^2}(-u_{i-1}+2u_i-u_{i+1})=f(x_i) 1\leq i\leq n-1 u_0=g(x_0), u_n=g(x_n) \begin{align}
 A=\frac{1}{h^2}\begin{pmatrix}
1 & 0 & 0 &  &\cdots & 0\\
-1 & 2 & -1 & 0 & \cdots & 0 \\
0 & -1 & 2 & -1 & \cdots & 0 \\
0 & 0 & \ddots & \ddots & \ddots & 0 \\
0 & \cdots & 0 & -1 & 2 & -1 \\
0 & \cdots & 0 & 0 & 0 & 1
\end{pmatrix}
\begin{pmatrix}
u_0\\
\vdots \\
u_n
\end{pmatrix}=
\begin{pmatrix}
\frac{g(0)}{h^2} \\
f(x_1)\\
\vdots\\
f(x_{n-1})\\
\frac{g(1)}{h^2}
\end{pmatrix}
\end{align} (Av)_i<0 \Rightarrow v_i<0 i=0,\dots ,n C>0 \|u\|_\infty\leq C\big (\|f\|_\infty+\|g\|_\infty\big ) u","['ordinary-differential-equations', 'numerical-methods', 'stability-in-odes']"
21,Prove that the solution to this differential equation must become constant once it hits 1.,Prove that the solution to this differential equation must become constant once it hits 1.,,"I have the following conjecture which I hope to prove. Conjecture. Let $f :[a,b]\to \mathbb R$ be differentiable on $[a,b]$ that satisfies $$ f'(x) = {\left(1-f(x)^3\right)}^{\frac 1 3} $$ for all $x\in [a,b]$ . Then, for all $x_0 \in [a,b]$ , if $f(x_0) = 1$ , then $f(x) = 1$ for all $x\in[a,b]$ and $x > x_0$ . I tried to do the following: Proof. Let $x_0 \in [a,b]$ such that $f(x_0) = 1$ . Let $x>x_0$ . Then, either $f(x)>1$ or $f(x) = 1$ or $f(x) < 1$ . We use proof by contradiction to eliminate the other $2$ cases. First, assume towards contradiction that $f(x) > 1$ . We construct a sequence $\{x_n\}_{n\in\mathbb N}$ as follows. By Mean Value Theorem, there exists $x_1 \in (x_0,x)$ such that $f'(x_1) = \frac{f(x) - f(x_0)}{x-x_0} > 0$ . Hence, $f(x_1) < 1$ . For all $n> 1$ , let $x_n \in (x_{n-1}, x)$ be such that $f'(x_n) = \frac {f(x)-f(x_{n-1})}{x-x_{n-1}} > 0 $ . Then it is clear that $\{x_n\}_{n\in\mathbb N}$ is increasing and bounded above, so it converges. Let $L = \lim_{n\to \infty}x_n$ . Then, since $f$ is continuous, $f(L) = \lim_{n\to\infty} f(x_n) \leq 1$ .(because $f'(x_n) > 0$ for all $n\in \mathbb N$ , which means $f(x_n)< 1$ for all $n\in\mathbb N$ ). And I stopped there, not knowing how should I proceed.","I have the following conjecture which I hope to prove. Conjecture. Let be differentiable on that satisfies for all . Then, for all , if , then for all and . I tried to do the following: Proof. Let such that . Let . Then, either or or . We use proof by contradiction to eliminate the other cases. First, assume towards contradiction that . We construct a sequence as follows. By Mean Value Theorem, there exists such that . Hence, . For all , let be such that . Then it is clear that is increasing and bounded above, so it converges. Let . Then, since is continuous, .(because for all , which means for all ). And I stopped there, not knowing how should I proceed.","f :[a,b]\to \mathbb R [a,b] 
f'(x) = {\left(1-f(x)^3\right)}^{\frac 1 3}
 x\in [a,b] x_0 \in [a,b] f(x_0) = 1 f(x) = 1 x\in[a,b] x > x_0 x_0 \in [a,b] f(x_0) = 1 x>x_0 f(x)>1 f(x) = 1 f(x) < 1 2 f(x) > 1 \{x_n\}_{n\in\mathbb N} x_1 \in (x_0,x) f'(x_1) = \frac{f(x) - f(x_0)}{x-x_0} > 0 f(x_1) < 1 n> 1 x_n \in (x_{n-1}, x) f'(x_n) = \frac {f(x)-f(x_{n-1})}{x-x_{n-1}} > 0  \{x_n\}_{n\in\mathbb N} L = \lim_{n\to \infty}x_n f f(L) = \lim_{n\to\infty} f(x_n) \leq 1 f'(x_n) > 0 n\in \mathbb N f(x_n)< 1 n\in\mathbb N","['real-analysis', 'ordinary-differential-equations', 'derivatives']"
22,numerical approaches to functional equations,numerical approaches to functional equations,,"I'm interested in finding numerical approaches to solving functional equations such as $$f(xy) = f(x)+f(y),$$ where the equations had no derivatives or integrals, and contains arguments involving $x$ and $y$ . Reason I ask is that when searching the web/literature, I can find information on some methods for functional differential/integral equations (e.g. Laplace transform and method of steps for time-variant problems, etc.).  But I have not been able to find numerical methods (if there are any) for the type of functional equations like shown above. My idea for general approach (which has been around a long time): Search for smooth solutions by repeatedly differentiating functional equation wrt $x$ or $y$ , solving resulting system to eliminate one variable, obtaining ODE, solving ODE numerically given value of function at single point.  This method can be used to solve d'Alembert's equation, for example.  Of course, requiring differentiability greatly limits possible numerical solutions that could be searched for. I'm looking for areas of research and curious what information is out there on this topic.  Any guidance is appreciated. Intuitively, ideal method would involve being able to evaluate $f$ at points which can be iteratively generated, do not form a cycle, and are closely spaced in desired interval on the real line.","I'm interested in finding numerical approaches to solving functional equations such as where the equations had no derivatives or integrals, and contains arguments involving and . Reason I ask is that when searching the web/literature, I can find information on some methods for functional differential/integral equations (e.g. Laplace transform and method of steps for time-variant problems, etc.).  But I have not been able to find numerical methods (if there are any) for the type of functional equations like shown above. My idea for general approach (which has been around a long time): Search for smooth solutions by repeatedly differentiating functional equation wrt or , solving resulting system to eliminate one variable, obtaining ODE, solving ODE numerically given value of function at single point.  This method can be used to solve d'Alembert's equation, for example.  Of course, requiring differentiability greatly limits possible numerical solutions that could be searched for. I'm looking for areas of research and curious what information is out there on this topic.  Any guidance is appreciated. Intuitively, ideal method would involve being able to evaluate at points which can be iteratively generated, do not form a cycle, and are closely spaced in desired interval on the real line.","f(xy) = f(x)+f(y), x y x y f","['abstract-algebra', 'ordinary-differential-equations', 'recurrence-relations', 'functional-equations']"
23,Logarithm and absolute value,Logarithm and absolute value,,"$$y' - y \tan x = 2x \sec x,\quad y(0)=0\tag1$$ integrating factor $= e^{-\int \tan x\ dx} = e^{\ln|\cos x|} = \cos x$ Can we write $|\cos x|$ as $\cos x$ above? $$I.F. y = \int I.F.\ 2x\ \sec x\ dx\\(\cos x) y = \int \cos x \ 2x\ \sec x\ dx = \int 2x\ dx$$ If we had taken the integrating factor to be $ |\cos x|$ , then in the above line $|\cos x|$ and $\sec x$ wouldn't have cancelled.","integrating factor Can we write as above? If we had taken the integrating factor to be , then in the above line and wouldn't have cancelled.","y' - y \tan x = 2x \sec x,\quad y(0)=0\tag1 = e^{-\int \tan x\ dx} = e^{\ln|\cos x|} = \cos x |\cos x| \cos x I.F. y = \int I.F.\ 2x\ \sec x\ dx\\(\cos x) y = \int \cos x \ 2x\ \sec x\ dx = \int 2x\ dx  |\cos x| |\cos x| \sec x","['calculus', 'ordinary-differential-equations', 'absolute-value']"
24,Center is origin for linearized system,Center is origin for linearized system,,"I have the following system \begin{equation}   \begin{aligned} \dot{x} & {}={} 1 + x^2y - 3x = f(x,y) \\ \dot{y} & {}={}2x - yx^2 = g(x,y)   \end{aligned} \tag{1} \end{equation} with one equilibrium point $A(1,2)$ . After linearizing around $A$ , I obtain the system \begin{equation}   \begin{aligned} \dot{x} & = x + y \\ \dot{y} & = -2x - y   \end{aligned} \tag{2} \end{equation} The eigenvalues of the matrix $J$ of system (2) are $λ_{1,2} = \pm \mathrm{i}$ . Because the origin is center for system (2) we can't apply Hartman-Grobamn Theorem. But can I do this instead? Let us consider $F=(f,g)$ . Then $\mathrm{div}F = 2xy - 3 - x^2 < 0$ around $A(1,2)$ if we consider a region $S = \{ (x,y) \in \mathbb{R}^2: \frac{1}{2} \leq x \leq \frac{3}{2} \ \mathrm{and} \ \frac{3}{2} \leq y \leq \frac{5}{2} \}$ . Then, the equilibrium point $A$ will be an attractor. Thus, $A$ is asympotically stable focus. Is this right or wrong? Do we know that if the origin is center for the linearized system then the equilibrium point for the nonlinear system will be either a center or a focus? Thanks for any help / guidance.","I have the following system with one equilibrium point . After linearizing around , I obtain the system The eigenvalues of the matrix of system (2) are . Because the origin is center for system (2) we can't apply Hartman-Grobamn Theorem. But can I do this instead? Let us consider . Then around if we consider a region . Then, the equilibrium point will be an attractor. Thus, is asympotically stable focus. Is this right or wrong? Do we know that if the origin is center for the linearized system then the equilibrium point for the nonlinear system will be either a center or a focus? Thanks for any help / guidance.","\begin{equation}
  \begin{aligned}
\dot{x} & {}={} 1 + x^2y - 3x = f(x,y) \\
\dot{y} & {}={}2x - yx^2 = g(x,y)
  \end{aligned} \tag{1}
\end{equation} A(1,2) A \begin{equation}
  \begin{aligned}
\dot{x} & = x + y \\
\dot{y} & = -2x - y
  \end{aligned} \tag{2}
\end{equation} J λ_{1,2} = \pm \mathrm{i} F=(f,g) \mathrm{div}F = 2xy - 3 - x^2 < 0 A(1,2) S = \{ (x,y) \in \mathbb{R}^2: \frac{1}{2} \leq x \leq \frac{3}{2} \ \mathrm{and} \ \frac{3}{2} \leq y \leq \frac{5}{2} \} A A","['ordinary-differential-equations', 'dynamical-systems', 'nonlinear-system', 'linearization']"
25,Some good books for ODE and Dynamical system.,Some good books for ODE and Dynamical system.,,"I am trying to shift my research area from Pure math to math bio for various reasons. So whatever time I invested in my algebra is not of much use plus I have to make the basics of ODE and the Dynamical system strong. I was reading Teschl's book on ODE and the Dynamical system and I felt it very hard to have a good hold on the base concept while reading the book. It is not the case that I don't understand it. Rather I can't grasp the inner concept well. I didn't read Strogatz Nonlinear Dynamics before but now I am enjoying it. Is there any other ODE-Dynamical system(if it is focused on oscillation theory that's a plus) undergraduate to graduate level bridging book and some books that give a deeper insight in an easier way and make the concept and problem-solving skills clearer? In pure math there are some books that state learning through problems and they try to build the conception with a series of easy problems. You can also refer to those books. This will be of great help because I believe in making my base strong. Please don't suggest me a hard book. When I am finding Teschl hard and Strogatz good. You can get an idea. E.g. I liked one undergraduate thesis on Floquet theory very helpful. It will be good if I get some easy to moderate to hard problems to solve with hints. What I really care for is learning about Existence,uniqueness, regularity Sturm Liouville, Sturm comparison theorems Linear stability, Liapunov stability, Hamiltonian, gradient systems Poincare bendixson theorem Some basic dynamical systems like omega limit sets Floquet Theory Linear ODEs etc etc.","I am trying to shift my research area from Pure math to math bio for various reasons. So whatever time I invested in my algebra is not of much use plus I have to make the basics of ODE and the Dynamical system strong. I was reading Teschl's book on ODE and the Dynamical system and I felt it very hard to have a good hold on the base concept while reading the book. It is not the case that I don't understand it. Rather I can't grasp the inner concept well. I didn't read Strogatz Nonlinear Dynamics before but now I am enjoying it. Is there any other ODE-Dynamical system(if it is focused on oscillation theory that's a plus) undergraduate to graduate level bridging book and some books that give a deeper insight in an easier way and make the concept and problem-solving skills clearer? In pure math there are some books that state learning through problems and they try to build the conception with a series of easy problems. You can also refer to those books. This will be of great help because I believe in making my base strong. Please don't suggest me a hard book. When I am finding Teschl hard and Strogatz good. You can get an idea. E.g. I liked one undergraduate thesis on Floquet theory very helpful. It will be good if I get some easy to moderate to hard problems to solve with hints. What I really care for is learning about Existence,uniqueness, regularity Sturm Liouville, Sturm comparison theorems Linear stability, Liapunov stability, Hamiltonian, gradient systems Poincare bendixson theorem Some basic dynamical systems like omega limit sets Floquet Theory Linear ODEs etc etc.",,"['ordinary-differential-equations', 'reference-request', 'dynamical-systems', 'book-recommendation', 'mathematical-biology']"
26,Behavior of the solutions to a system of linear differential equations,Behavior of the solutions to a system of linear differential equations,,"Let $x(t)=(x_1(t),\cdots,x_n(t)):\mathbb{R}\rightarrow\mathbb{R}^n$ be differentiable and satisfy: $$x'(t)=Ax(t)$$ where $A=(a_{ij})_{n\times n}$ is a real matrix. I have known that if the real part of all eigenvalues of $A$ are strictly negative, then $\lim\limits_{t\rightarrow+\infty}x(t)=0$ . However, if we assume $\lim\limits_{t\rightarrow+\infty}x(t)=0$ and $x_1(t),\cdots,x_n(t)$ are linearly independent, what can we say about the matrix $A$ ? I guess the real part of all eigenvalues of $A$ should be strictly negative, but I cannot deal with the case that the eigenvalue $\lambda$ is purely imaginary number. In this case, $x_i(t)$ may be like this: $$c_1\sin \mu_1t+c_2\sin\mu_2t+\cdots+c_2\sin\mu_kt$$ where $\mu_k>0$ I guess this is impossible since $\lim\limits_{t\rightarrow+\infty}x(t)=0$ , but I don't know how to prove it. By the way, the original problem is that if all $a_{ij}\geqslant0$ and $\lim\limits_{t\rightarrow+\infty}x(t)=0$ , prove that $x_1(t),\cdots ,x_n(t)$ cannot be linearly independent. The solution is that use Perron's theorem to find a non-negative eigenvalue $\lambda$ and its eigenvector $(\alpha_1,\cdots ,\alpha_n)$ . Let $y(t)=\alpha_1x_1(t)+\cdots+\alpha_nx_n(t)$ and we will find that $y'=\lambda y$ and $\lim\limits_{t\rightarrow +\infty}y(t)=0$ , so $y(t)\equiv0$ , which means $x_1(t),\cdots,x_n(t)$ are linearly dependent.","Let be differentiable and satisfy: where is a real matrix. I have known that if the real part of all eigenvalues of are strictly negative, then . However, if we assume and are linearly independent, what can we say about the matrix ? I guess the real part of all eigenvalues of should be strictly negative, but I cannot deal with the case that the eigenvalue is purely imaginary number. In this case, may be like this: where I guess this is impossible since , but I don't know how to prove it. By the way, the original problem is that if all and , prove that cannot be linearly independent. The solution is that use Perron's theorem to find a non-negative eigenvalue and its eigenvector . Let and we will find that and , so , which means are linearly dependent.","x(t)=(x_1(t),\cdots,x_n(t)):\mathbb{R}\rightarrow\mathbb{R}^n x'(t)=Ax(t) A=(a_{ij})_{n\times n} A \lim\limits_{t\rightarrow+\infty}x(t)=0 \lim\limits_{t\rightarrow+\infty}x(t)=0 x_1(t),\cdots,x_n(t) A A \lambda x_i(t) c_1\sin \mu_1t+c_2\sin\mu_2t+\cdots+c_2\sin\mu_kt \mu_k>0 \lim\limits_{t\rightarrow+\infty}x(t)=0 a_{ij}\geqslant0 \lim\limits_{t\rightarrow+\infty}x(t)=0 x_1(t),\cdots ,x_n(t) \lambda (\alpha_1,\cdots ,\alpha_n) y(t)=\alpha_1x_1(t)+\cdots+\alpha_nx_n(t) y'=\lambda y \lim\limits_{t\rightarrow +\infty}y(t)=0 y(t)\equiv0 x_1(t),\cdots,x_n(t)","['real-analysis', 'linear-algebra', 'ordinary-differential-equations']"
27,Regarding common zeros of a pair of solutions to an ODE,Regarding common zeros of a pair of solutions to an ODE,,"$\mathbf{Question}:$ Let $y_1$ and $y_2$ be the solutions to $y''x^2+y'+\sin(x)y=0$ which satisfy the boundary conditions $y_1(0)=0$ and $y_1'(1)=1$ and $y_2(0)=1$ and $y_2'(1)=0$ respectively. Then, $y_1$ and $y_2$ do not have common zeros $y_1$ and $y_2$ do have common zeros either $y_1$ or $y_2$ has a zero of order $2$ both $y_1$ and $y_2$ have zeros of order $2$ $\mathbf{Attempt}:$ Each of the particular solutions can be written in terms of linear combinations that abide by the specified conditions. Let $A\phi_1(x)+B\phi_2(x)=y_1(x)$ and $C\phi_1(x)+D\phi_2(x)=y_2(x)$ be the solutions, with $\phi_1$ and $\phi_2$ being the fundamental ones. Supposing that they indeed have at least one common zero, say $x_0$ , then: $A\phi_1(x_0)+B\phi_2(x_0)=0$ and $C\phi_1(x_0)+D\phi_2(x_0)=0$ . Now, we know from the Sturm Separation Theorem that the roots of the fundamental solutions alternate, hence, they cannot vanish together; implying that $\begin{vmatrix} A & B\\ C & D \end{vmatrix}=0$ . This implies that $y_1(x)$ and $y_2(x)$ are linearly dependent, thus violating the boundary condition $y_1(0)=0 \neq y_2(0)$ . Is this the correct approach? Kindly Verify.","Let and be the solutions to which satisfy the boundary conditions and and and respectively. Then, and do not have common zeros and do have common zeros either or has a zero of order both and have zeros of order Each of the particular solutions can be written in terms of linear combinations that abide by the specified conditions. Let and be the solutions, with and being the fundamental ones. Supposing that they indeed have at least one common zero, say , then: and . Now, we know from the Sturm Separation Theorem that the roots of the fundamental solutions alternate, hence, they cannot vanish together; implying that . This implies that and are linearly dependent, thus violating the boundary condition . Is this the correct approach? Kindly Verify.",\mathbf{Question}: y_1 y_2 y''x^2+y'+\sin(x)y=0 y_1(0)=0 y_1'(1)=1 y_2(0)=1 y_2'(1)=0 y_1 y_2 y_1 y_2 y_1 y_2 2 y_1 y_2 2 \mathbf{Attempt}: A\phi_1(x)+B\phi_2(x)=y_1(x) C\phi_1(x)+D\phi_2(x)=y_2(x) \phi_1 \phi_2 x_0 A\phi_1(x_0)+B\phi_2(x_0)=0 C\phi_1(x_0)+D\phi_2(x_0)=0 \begin{vmatrix} A & B\\ C & D \end{vmatrix}=0 y_1(x) y_2(x) y_1(0)=0 \neq y_2(0),"['ordinary-differential-equations', 'solution-verification', 'boundary-value-problem', 'sturm-liouville', 'fundamental-solution']"
28,How to solve the following polynomial recursion $p_n(x)= p_{n-1}(x)(x^2-a)+\left( \frac{b}{x}-2x \right) p_{n-1}'(x)+p_{n-1}''(x)$,How to solve the following polynomial recursion,p_n(x)= p_{n-1}(x)(x^2-a)+\left( \frac{b}{x}-2x \right) p_{n-1}'(x)+p_{n-1}''(x),How to find the solution to the following polynomial recursion \begin{align} p_n(x)= p_{n-1}(x)(x^2-a)+\left( \frac{b}{x}-2x \right) p_{n-1}'(x)+p_{n-1}''(x) \end{align} with $p_0(x)=1$ for some fixed constant $a$ and $b$ . The first two terms of this are given by \begin{align} p_1(x)&=x^2-a\\ p_2(x)&= x^4 - (2 a+4) x^2 + 2 b+a^2   + 2\\ p_3(x)&= x^6 +(2a+ 6b+ 3a^2+ 30)x^2  - (3a+12)x^4    - 8- 8b - 6ab - 6a- a^3 \end{align} This question arose as part of another question here .,How to find the solution to the following polynomial recursion with for some fixed constant and . The first two terms of this are given by This question arose as part of another question here .,"\begin{align}
p_n(x)= p_{n-1}(x)(x^2-a)+\left( \frac{b}{x}-2x \right) p_{n-1}'(x)+p_{n-1}''(x)
\end{align} p_0(x)=1 a b \begin{align}
p_1(x)&=x^2-a\\
p_2(x)&= x^4 - (2 a+4) x^2 + 2 b+a^2   + 2\\
p_3(x)&= x^6 +(2a+ 6b+ 3a^2+ 30)x^2  - (3a+12)x^4    - 8- 8b - 6ab - 6a- a^3
\end{align}","['ordinary-differential-equations', 'recurrence-relations']"
29,Existence of solution for an ODE,Existence of solution for an ODE,,"This is a homework question but no hint is given. I think it is related to comparison principle of the ordinary differential equation. Let f be a $C^{1}$ -function in $\mathbf{R}^{2}$ such that $|f(x,y)|\leq 1+|y|$ for all $(x,y)\in\mathbf{R}^{2}$ Show that the initial value problem $$y'=f(x,y), x(0)=0$$ has a solution y(x) for all $t\in(-\infty,\infty)$ My try is Let $E(x)=\frac{1}{2}|y(x)|^{2}$ ,then $|E'(x)|=|y(x)\dot y'(x)|\leq C |y|(1+|y|)\leq C_{0}(1+|E(x)|)$ Then $E(x)\leq (E(0)+1)exp(C_{0} x)-1$ and E(x) never blow up thus y is globally defined. I am not sure whether this is the standard answer because comparison principle or Gronwall's inequality is not taught in my course but this is the only way I can figure it out. Are there any other methods?  Any help is appreciated!","This is a homework question but no hint is given. I think it is related to comparison principle of the ordinary differential equation. Let f be a -function in such that for all Show that the initial value problem has a solution y(x) for all My try is Let ,then Then and E(x) never blow up thus y is globally defined. I am not sure whether this is the standard answer because comparison principle or Gronwall's inequality is not taught in my course but this is the only way I can figure it out. Are there any other methods?  Any help is appreciated!","C^{1} \mathbf{R}^{2} |f(x,y)|\leq 1+|y| (x,y)\in\mathbf{R}^{2} y'=f(x,y), x(0)=0 t\in(-\infty,\infty) E(x)=\frac{1}{2}|y(x)|^{2} |E'(x)|=|y(x)\dot y'(x)|\leq C |y|(1+|y|)\leq C_{0}(1+|E(x)|) E(x)\leq (E(0)+1)exp(C_{0} x)-1","['ordinary-differential-equations', 'analysis']"
30,How do we solve this rather simple ODE (Loewner equation with driving function $\sqrt t$)?,How do we solve this rather simple ODE (Loewner equation with driving function )?,\sqrt t,"Remember the following result for the Loewner equation : If $\lambda:[0,\infty)\to\mathbb R$ is continuous, then for all $z\in\mathbb C\setminus\{\lambda(0)\}$ there is a uniqe $\zeta(z)\in(0,\infty]$ and a unique continuous $g(\;\cdot\;,z):[0,\zeta(t))\to\mathbb C$ with $$g(t,z)\ne\lambda(t)\tag1$$ and $$g(t,z)=z+\int_0^t\frac2{g(s,z)-\lambda(s)}\:{\rm d}s\tag2$$ for all $t\in[0,\zeta(z))$ . Now assume $$\lambda(t)=2\sqrt{\kappa t}\;\;\;\text{for all }t\ge0$$ for some $\kappa\ge0$ . How can we determine $g$ here and how can we determine $\zeta(z)$ ? Let $y_\pm:=\sqrt\kappa\pm\sqrt{\kappa+4}$ and $$H(w):=\frac{2y_+\ln(w-y_-)-2y_-\ln(w-y_+)}{y_+-y_-}.$$ Using this, I was able to find the relation $$H\left(\frac{g(t,z)}{\sqrt t}\right)=2\ln\frac z{\sqrt t}\tag3,$$ but how do we solve this for $g(t,z)$ ? EDIT : From $(3)$ , I was only able to obtain $$\gamma(t):=g_t^{-1}(\lambda(t))=c\sqrt t,$$ where $$c:=\exp\left(\frac12H(2\sqrt\kappa)\right).$$ If we cannot find an explicit expression for $g$ , can we at least determien $\zeta(z)$ ? My guess is that $\zeta(z)=\infty$ for all $z\in\mathbb C\setminus\{\lambda(0)\}$ , but how do we prove this?","Remember the following result for the Loewner equation : If is continuous, then for all there is a uniqe and a unique continuous with and for all . Now assume for some . How can we determine here and how can we determine ? Let and Using this, I was able to find the relation but how do we solve this for ? EDIT : From , I was only able to obtain where If we cannot find an explicit expression for , can we at least determien ? My guess is that for all , but how do we prove this?","\lambda:[0,\infty)\to\mathbb R z\in\mathbb C\setminus\{\lambda(0)\} \zeta(z)\in(0,\infty] g(\;\cdot\;,z):[0,\zeta(t))\to\mathbb C g(t,z)\ne\lambda(t)\tag1 g(t,z)=z+\int_0^t\frac2{g(s,z)-\lambda(s)}\:{\rm d}s\tag2 t\in[0,\zeta(z)) \lambda(t)=2\sqrt{\kappa t}\;\;\;\text{for all }t\ge0 \kappa\ge0 g \zeta(z) y_\pm:=\sqrt\kappa\pm\sqrt{\kappa+4} H(w):=\frac{2y_+\ln(w-y_-)-2y_-\ln(w-y_+)}{y_+-y_-}. H\left(\frac{g(t,z)}{\sqrt t}\right)=2\ln\frac z{\sqrt t}\tag3, g(t,z) (3) \gamma(t):=g_t^{-1}(\lambda(t))=c\sqrt t, c:=\exp\left(\frac12H(2\sqrt\kappa)\right). g \zeta(z) \zeta(z)=\infty z\in\mathbb C\setminus\{\lambda(0)\}","['ordinary-differential-equations', 'partial-differential-equations', 'mathematical-physics', 'conformal-geometry']"
31,Resources to study climate models and the stability of solutions to differential equations,Resources to study climate models and the stability of solutions to differential equations,,"I am currently taking a course on the mathematical models of climate change, and we are studying from a book called ""Mathematics & Climate"" by Kaper & Engler, and these two papers: UMAPclimate.pdf WalshStommel.pdf While these two papers are not super hard to read, they are a bit advanced for my level. Same goes for the book as well. I took an introductory course on differential equations, but it was all about how to solve them. We didn't really talk about the solutions themselves. And a big chunk of this paper is about stability of the solutions -- or at least that's what my professor is focusing on. We covered stuff like Lipschitz Condition, asymptotic stability, Lyapunov stability, and the Adomian decomposition method -- all of which seem completely foreign to me and a bit advanced. Is there a textbook or a course somewhere that explains at least some of these topics in a more beginner-friendly way and/or in a bit more detail? Any help would be greatly appreciated!","I am currently taking a course on the mathematical models of climate change, and we are studying from a book called ""Mathematics & Climate"" by Kaper & Engler, and these two papers: UMAPclimate.pdf WalshStommel.pdf While these two papers are not super hard to read, they are a bit advanced for my level. Same goes for the book as well. I took an introductory course on differential equations, but it was all about how to solve them. We didn't really talk about the solutions themselves. And a big chunk of this paper is about stability of the solutions -- or at least that's what my professor is focusing on. We covered stuff like Lipschitz Condition, asymptotic stability, Lyapunov stability, and the Adomian decomposition method -- all of which seem completely foreign to me and a bit advanced. Is there a textbook or a course somewhere that explains at least some of these topics in a more beginner-friendly way and/or in a bit more detail? Any help would be greatly appreciated!",,"['ordinary-differential-equations', 'reference-request', 'book-recommendation']"
32,Are there any ODEs like these?,Are there any ODEs like these?,,"I have been working on using automatic differentiation to compute the sensitivities of systems of ordinary differential equations of the form \begin{equation} \frac{d\mathbf{u}(t)}{dt} = F(\mathbf{u}(t),t,\alpha),\qquad \mathbf{u}(0) = \mathbf{u}^0. \end{equation} where $\alpha\in\mathbb{R}^M$ is a vetor of real valued parameters and $\mathbf{u}(t)\in\mathbb{R}^N$ . To showcase the advantages of the algorithm, I was looking for a real scenario where the vector valued function $F$ depends on a number of parameters $M$ much bigger than the number of equations in the system $N$ . By real, I mean some equation  with applicability in any scientific field. In the article ""Neural Ordinary Differential Equations"" by Ricky T. Q. Chen, they consider $F$ to be a neural network, where indeed $M \gg N$ , but I was looking for an alternative. What I have considered so far are PDEs discretized in space using finite differences, for example, the 1D heat equation with spatial dependent thermal diffusivity $\alpha(x)$ . However, if I consider the set of parameters $\alpha_i = \alpha(x_i)$ , where $x_i$ are the grid points, that would result in $M=N$ . I could artificially consider more parameters, like $\alpha_{i+\frac{1}{2}} = \alpha(x_{i+\frac{1}{2}})$ and force these on the function $F$ , but still I would get $M=2N$ , and I don't think it's usual for numerical methods to consider a finer grid for the parameters, rather the other way around  (thinking of FEM for example, where $\alpha(x)$ could possibly be represented by its mean value in the element). Another thing I have considered is if $F$ depends on some function which needs to be numerically evaluated, and thus would be approximated by a finite set of $M$ parameters ( for example, coefficientes for a given set of basis functions). Can you think of any real example like this where $M \gg N$ ? What I really really want is something like $M \approx N^2$ or at least $M \approx kN$ with $k \gg 1$ .","I have been working on using automatic differentiation to compute the sensitivities of systems of ordinary differential equations of the form where is a vetor of real valued parameters and . To showcase the advantages of the algorithm, I was looking for a real scenario where the vector valued function depends on a number of parameters much bigger than the number of equations in the system . By real, I mean some equation  with applicability in any scientific field. In the article ""Neural Ordinary Differential Equations"" by Ricky T. Q. Chen, they consider to be a neural network, where indeed , but I was looking for an alternative. What I have considered so far are PDEs discretized in space using finite differences, for example, the 1D heat equation with spatial dependent thermal diffusivity . However, if I consider the set of parameters , where are the grid points, that would result in . I could artificially consider more parameters, like and force these on the function , but still I would get , and I don't think it's usual for numerical methods to consider a finer grid for the parameters, rather the other way around  (thinking of FEM for example, where could possibly be represented by its mean value in the element). Another thing I have considered is if depends on some function which needs to be numerically evaluated, and thus would be approximated by a finite set of parameters ( for example, coefficientes for a given set of basis functions). Can you think of any real example like this where ? What I really really want is something like or at least with .","\begin{equation}
\frac{d\mathbf{u}(t)}{dt} = F(\mathbf{u}(t),t,\alpha),\qquad \mathbf{u}(0) = \mathbf{u}^0.
\end{equation} \alpha\in\mathbb{R}^M \mathbf{u}(t)\in\mathbb{R}^N F M N F M \gg N \alpha(x) \alpha_i = \alpha(x_i) x_i M=N \alpha_{i+\frac{1}{2}} = \alpha(x_{i+\frac{1}{2}}) F M=2N \alpha(x) F M M \gg N M \approx N^2 M \approx kN k \gg 1","['ordinary-differential-equations', 'partial-differential-equations', 'numerical-methods', 'numerical-optimization']"
33,Solve: $t\ddot{x}+\dot{x}=0$,Solve:,t\ddot{x}+\dot{x}=0,"To solve the above mentioned equation I used a substitution $u=\dot{x}$ . Thus I now have $t\dot{u}+u=0 \rightarrow \dot{u} = -\frac{1}{t}u \rightarrow \int\frac{1}{u}du = -\int\frac{1}{t}dt \rightarrow ln(u) = -ln(t) + C$ . This is where I got stuck, can someone point out if something went wrong or help me continue.","To solve the above mentioned equation I used a substitution . Thus I now have . This is where I got stuck, can someone point out if something went wrong or help me continue.",u=\dot{x} t\dot{u}+u=0 \rightarrow \dot{u} = -\frac{1}{t}u \rightarrow \int\frac{1}{u}du = -\int\frac{1}{t}dt \rightarrow ln(u) = -ln(t) + C,['ordinary-differential-equations']
34,How to solve this first order non-linear (quadratic) inhomogenous ODE?,How to solve this first order non-linear (quadratic) inhomogenous ODE?,,"How can I find a solution to the following first order nonlinear (quadratic) ODE? $$ h(t)=a\dot h(t)\sqrt{b-c \dot h(t)^2} $$ I am solving a physics problem, namely, finding the time it takes for a cylinder to sink in water, and need to find the function $h$ . The water that the cylinder is in can be considered as nonviscous. Therefore I used conservation of energy and the continuity equation to obtain this equation. I think that my physical reasoning is correct although some approximations I made might not be reasonable. I want to verify this by solving this equation and evaluating the time. In case this raises concerns, I am not interested in obtaining a solution by the work of others without own contribution, I want to know if this ODE is solvable analytically and if so, which methods I should use / learn to solve it. Regarding my knowledge on DEs, aside from special cases like using characteristic polynomials or the harmonic oscillator, I only know separation of variables and variation of constants for inhomogeneous first order linear ODEs.","How can I find a solution to the following first order nonlinear (quadratic) ODE? I am solving a physics problem, namely, finding the time it takes for a cylinder to sink in water, and need to find the function . The water that the cylinder is in can be considered as nonviscous. Therefore I used conservation of energy and the continuity equation to obtain this equation. I think that my physical reasoning is correct although some approximations I made might not be reasonable. I want to verify this by solving this equation and evaluating the time. In case this raises concerns, I am not interested in obtaining a solution by the work of others without own contribution, I want to know if this ODE is solvable analytically and if so, which methods I should use / learn to solve it. Regarding my knowledge on DEs, aside from special cases like using characteristic polynomials or the harmonic oscillator, I only know separation of variables and variation of constants for inhomogeneous first order linear ODEs.", h(t)=a\dot h(t)\sqrt{b-c \dot h(t)^2}  h,"['ordinary-differential-equations', 'physics']"
35,Solving functional equations with antiderivatives as input : $f(F^{-1}(x)) = 1 - \frac{1}{f(x)}$,Solving functional equations with antiderivatives as input :,f(F^{-1}(x)) = 1 - \frac{1}{f(x)},"I want to solve below equation by relation $F(x) =\int{f(x)}$ , where $F^{-1}(x)$ is $F(x)$ 's inverse function. $$f(F^{-1}(x)) = 1 - \frac{1}{f(x)}$$ $f(x)$ is probably not an elementary function. Is there a general approach to determining the features of these function? Maybe it is a fairly naive relation to get the function itself, so it's okay to get only the constraints on the function. Input of antiderivative is also same as input of original function, because of it is $\mathbf{R}$ to $\mathbf{R}$ function. Also this equation can be reduced into ordinary differential equation form, $$f'(x) = f(x)(1-f(x))^2 f'(f^{-1}(\frac{1}{1-f(x)}))$$","I want to solve below equation by relation , where is 's inverse function. is probably not an elementary function. Is there a general approach to determining the features of these function? Maybe it is a fairly naive relation to get the function itself, so it's okay to get only the constraints on the function. Input of antiderivative is also same as input of original function, because of it is to function. Also this equation can be reduced into ordinary differential equation form,",F(x) =\int{f(x)} F^{-1}(x) F(x) f(F^{-1}(x)) = 1 - \frac{1}{f(x)} f(x) \mathbf{R} \mathbf{R} f'(x) = f(x)(1-f(x))^2 f'(f^{-1}(\frac{1}{1-f(x)})),"['ordinary-differential-equations', 'functional-equations']"
36,Deriving the differential equations with solutions corresponding to classical orthogonal polynomials using the orthogonality condition,Deriving the differential equations with solutions corresponding to classical orthogonal polynomials using the orthogonality condition,,"I'm trying to understand a bit more about the classical orthogonal polynomials that appear throughout quantum mechanics. My question is whether it is possible to derive the differential equations with solutions corresponding to the classical orthogonal polynomials, using only the orthogonality condition. For example, if I require that a set of functions, $\{f_{n}(x)\}$ are orthogonal on the interval $I=[0,\infty]$ under weighting function $w(x)=e^{-x}$ , $$\int_{I}w(x)f_{n}(x)f_{m}(x)dx=h_{n}\delta_{nm}$$ for some constant $h_{n}$ , is it possible to derive Laguerre's equation $$xf_{n}''+(1-x)f_{n}'+nf_{n}=0$$ If so, is this the traditional historic approach, or were these functions discovered seperately, and then were coincidentally found to be orthogonal? Also, if anyone has any resources where they go through this process it would be massively appreciated, thanks.","I'm trying to understand a bit more about the classical orthogonal polynomials that appear throughout quantum mechanics. My question is whether it is possible to derive the differential equations with solutions corresponding to the classical orthogonal polynomials, using only the orthogonality condition. For example, if I require that a set of functions, are orthogonal on the interval under weighting function , for some constant , is it possible to derive Laguerre's equation If so, is this the traditional historic approach, or were these functions discovered seperately, and then were coincidentally found to be orthogonal? Also, if anyone has any resources where they go through this process it would be massively appreciated, thanks.","\{f_{n}(x)\} I=[0,\infty] w(x)=e^{-x} \int_{I}w(x)f_{n}(x)f_{m}(x)dx=h_{n}\delta_{nm} h_{n} xf_{n}''+(1-x)f_{n}'+nf_{n}=0","['ordinary-differential-equations', 'polynomials', 'orthogonality']"
37,Real eigenspaces of a matrix $A\in\mathcal{M}_{2\times 2}(\mathbb{R})$ are the union of straight lines trajectories and the fixed point $y\equiv 0$,Real eigenspaces of a matrix  are the union of straight lines trajectories and the fixed point,A\in\mathcal{M}_{2\times 2}(\mathbb{R}) y\equiv 0,"I'm trying to show that real eigenspaces of a matrix $A\in\mathcal{M}_{2\times 2}(\mathbb{R})$ are the union of straight lines trajectories and the fixed point $y\equiv 0$ . I'm studying this in the field of systems of linear differential equations. If we have the equation \begin{equation} y'(t)=Ay(t)+g(t) \end{equation} and consider \begin{equation} y'(t)=Ay(t) \end{equation} the solution to the homogeneous system involves scalar multiples of the eigenvectors of the matrix $A$ . Moreover, the general solutions to this homogeneous system involves linearly independent eigenvectors (which can be generalized eigenvectors). Of course, here we have two straight lines in the phase plane corresponding to this solution, which isn't the same line since those eigenvectors are linearly independent. If we think about an eigenspace as $E(A,\lambda)=\{\alpha v_\lambda:\alpha\in\mathbb{R}\}$ since if $\lambda$ is an eigenvalue, then there exist $v_\lambda\neq\vec{0}$ such that $Av_\lambda=\lambda v_\lambda$ and if we multiply by $\alpha\in\mathbb{R}$ both sides, $\alpha (Av_\lambda)=A(\alpha v_\lambda)=\alpha\lambda v_\lambda\in E(A,\lambda)$ . Is is trivial that $y\equiv 0$ is in any eigenspace. However, i'm not sure if I'm done. Do I have to take any consideration with the non-homogeneous part of the system? I think I don't because eigenspaces of the matrix are not related to $g(t)$ , but I'm not completely sure I have to prove something else. Any hint with this pseudo-proof?","I'm trying to show that real eigenspaces of a matrix are the union of straight lines trajectories and the fixed point . I'm studying this in the field of systems of linear differential equations. If we have the equation and consider the solution to the homogeneous system involves scalar multiples of the eigenvectors of the matrix . Moreover, the general solutions to this homogeneous system involves linearly independent eigenvectors (which can be generalized eigenvectors). Of course, here we have two straight lines in the phase plane corresponding to this solution, which isn't the same line since those eigenvectors are linearly independent. If we think about an eigenspace as since if is an eigenvalue, then there exist such that and if we multiply by both sides, . Is is trivial that is in any eigenspace. However, i'm not sure if I'm done. Do I have to take any consideration with the non-homogeneous part of the system? I think I don't because eigenspaces of the matrix are not related to , but I'm not completely sure I have to prove something else. Any hint with this pseudo-proof?","A\in\mathcal{M}_{2\times 2}(\mathbb{R}) y\equiv 0 \begin{equation}
y'(t)=Ay(t)+g(t)
\end{equation} \begin{equation}
y'(t)=Ay(t)
\end{equation} A E(A,\lambda)=\{\alpha v_\lambda:\alpha\in\mathbb{R}\} \lambda v_\lambda\neq\vec{0} Av_\lambda=\lambda v_\lambda \alpha\in\mathbb{R} \alpha (Av_\lambda)=A(\alpha v_\lambda)=\alpha\lambda v_\lambda\in E(A,\lambda) y\equiv 0 g(t)","['ordinary-differential-equations', 'vector-spaces', 'systems-of-equations']"
38,Given any ODE is there a way to know if it has any explicit solution?,Given any ODE is there a way to know if it has any explicit solution?,,"Given any ODE of the form $$F(x,y,y',...,y^n)=0$$ is there a way to know if there are any explicit solutions? For example for the differential equation $$(1+y^2)(e^{2x}-e^yy')-(1+y)y'=0$$ the solution I can come up with is $$\dfrac{e^{2x}}{2}+C=\arctan(y)+\dfrac{ln|1+y^2|}{2}+e^y+K$$ And I cannot find any explicit form of $y$ for that equation. (Don't know if there are) So I am trying to generalize the case to any ODE, is there a way I can know if an equation has an explicit solution before even doing it?","Given any ODE of the form is there a way to know if there are any explicit solutions? For example for the differential equation the solution I can come up with is And I cannot find any explicit form of for that equation. (Don't know if there are) So I am trying to generalize the case to any ODE, is there a way I can know if an equation has an explicit solution before even doing it?","F(x,y,y',...,y^n)=0 (1+y^2)(e^{2x}-e^yy')-(1+y)y'=0 \dfrac{e^{2x}}{2}+C=\arctan(y)+\dfrac{ln|1+y^2|}{2}+e^y+K y",['real-analysis']
39,Solve $x^2y' + xy + 1 = 0$ first order differential equation. Where is my mistake?,Solve  first order differential equation. Where is my mistake?,x^2y' + xy + 1 = 0,"Solve $x^2y' + xy + 1 = 0$ first order differential equation. The answer from the textbook is $xy = C - \ln |x|$ . Here is my long (but wrong) solution and I can't see the mistake. First I rewrite it as $y' + \dfrac{y}{x} = -\dfrac{1}{x^2}$ , this is a first order linear differential equation. Let's consider $y' = -\dfrac{y}{x}$ . From here $\dfrac{dy}{y} = -\dfrac{dx}{x}$ and $\ln y = - \ln x + C$ and $y = Ce^{\frac{1}{x}}$ . Now for the general solution assume $y = C(x)e^{\frac{1}{x}}$ . Then $C'(x)e^{\frac{1}{x}} - \dfrac{C(x)e^{\frac{1}{x}}}{x^2} + \dfrac{C(x)e{\frac{1}{x}}}{x} = -\dfrac{1}{x^2}$ and $C'(x) - \dfrac{C(x)}{x^2} + \dfrac{C(x)}{x} = -\dfrac{1}{x^2e^{\frac{1}{x}}}$ . Let $u \equiv C(x)$ . We have $u' - u(\frac{1}{x} - \frac{1}{x^2}) = -\dfrac{1}{x^2e^{\frac{1}{x}}}$ This is another linear differential equation. Let's consider $\dfrac{du}{u} = dx(\dfrac{1}{x} - \dfrac{1}{x^2})$ . Integrating we have $\ln u = \ln x + \dfrac{1}{x} + C_1$ and $u = C_1xe^{\frac{1}{x}}$ . Now suppose $u = C_1(x)xe^{\frac{1}{x}}$ . Then $C_1'(x)xe^{\frac{1}{x}} + C_1(x)e^{\frac{1}{x}} - \dfrac{C_1(x)e^{\frac{1}{x}}}{x} - C_1(x)e^{\frac{1}{x}}(1 - \dfrac{1}{x}) = -\dfrac{1}{x^2e^{\frac{1}{x}}}$ and $C_1'(x)x + C_1(x) - \dfrac{C_1(x)}{x} - C_1(x)(1 - \dfrac{1}{x}) = -\dfrac{1}{x^2e^{\frac{2}{x}}}$ and finally $C_1'(x) = -\dfrac{1}{x^2e^{\frac{2}{x}}}$ . Also before passing to integration suppose $x = \dfrac{1}{\ln z}$ . Then $dx = \dfrac{dz}{z \ln^2 z}$ . So $$C_1(x) = - \int {\dfrac{dx}{x^2e^{\frac{2}{x}}}} = - \int{zdz} = -z^2/2 + C = -e^{\frac{2}{x}} + C$$ Then finally $y = C(x)e^{\frac{1}{x}} \equiv ue^{\frac{1}{x}} = C_1(x)xe^{\frac{2}{x}} =-e^{\frac{4}{x}}x + Cxe^{\frac{2}{x}}$ .","Solve first order differential equation. The answer from the textbook is . Here is my long (but wrong) solution and I can't see the mistake. First I rewrite it as , this is a first order linear differential equation. Let's consider . From here and and . Now for the general solution assume . Then and . Let . We have This is another linear differential equation. Let's consider . Integrating we have and . Now suppose . Then and and finally . Also before passing to integration suppose . Then . So Then finally .",x^2y' + xy + 1 = 0 xy = C - \ln |x| y' + \dfrac{y}{x} = -\dfrac{1}{x^2} y' = -\dfrac{y}{x} \dfrac{dy}{y} = -\dfrac{dx}{x} \ln y = - \ln x + C y = Ce^{\frac{1}{x}} y = C(x)e^{\frac{1}{x}} C'(x)e^{\frac{1}{x}} - \dfrac{C(x)e^{\frac{1}{x}}}{x^2} + \dfrac{C(x)e{\frac{1}{x}}}{x} = -\dfrac{1}{x^2} C'(x) - \dfrac{C(x)}{x^2} + \dfrac{C(x)}{x} = -\dfrac{1}{x^2e^{\frac{1}{x}}} u \equiv C(x) u' - u(\frac{1}{x} - \frac{1}{x^2}) = -\dfrac{1}{x^2e^{\frac{1}{x}}} \dfrac{du}{u} = dx(\dfrac{1}{x} - \dfrac{1}{x^2}) \ln u = \ln x + \dfrac{1}{x} + C_1 u = C_1xe^{\frac{1}{x}} u = C_1(x)xe^{\frac{1}{x}} C_1'(x)xe^{\frac{1}{x}} + C_1(x)e^{\frac{1}{x}} - \dfrac{C_1(x)e^{\frac{1}{x}}}{x} - C_1(x)e^{\frac{1}{x}}(1 - \dfrac{1}{x}) = -\dfrac{1}{x^2e^{\frac{1}{x}}} C_1'(x)x + C_1(x) - \dfrac{C_1(x)}{x} - C_1(x)(1 - \dfrac{1}{x}) = -\dfrac{1}{x^2e^{\frac{2}{x}}} C_1'(x) = -\dfrac{1}{x^2e^{\frac{2}{x}}} x = \dfrac{1}{\ln z} dx = \dfrac{dz}{z \ln^2 z} C_1(x) = - \int {\dfrac{dx}{x^2e^{\frac{2}{x}}}} = - \int{zdz} = -z^2/2 + C = -e^{\frac{2}{x}} + C y = C(x)e^{\frac{1}{x}} \equiv ue^{\frac{1}{x}} = C_1(x)xe^{\frac{2}{x}} =-e^{\frac{4}{x}}x + Cxe^{\frac{2}{x}},"['integration', 'ordinary-differential-equations', 'solution-verification']"
40,How can I solve this second order ODE with Dirac delta,How can I solve this second order ODE with Dirac delta,,"I need to solve the following ODE (a steady state Fokker-Planck/Kolmogorov Forward equation) \begin{equation}    \dfrac{\partial^2}{\partial x^2}\left[ax^2f(x)\right] - \dfrac{\partial}{\partial x}\left[bxf(x)\right] - c f(x) + c\delta(x-\hat{x}) = 0 \end{equation} where $x\in(0,\infty)$ and where $f(x)\rightarrow 0$ as $x \rightarrow \infty$ . The constants $a,b,c$ are all positive and $\hat{x}$ is positive and far away from zero (far enough such that the solution at $x=0$ should be $f(0)=0$ ). The solution ignoring the Dirac impulse $\delta(x-\hat{x})$ (the solutions where $x \neq \hat{x}$ ) and imposing that $\lim_{x \rightarrow \infty}f(x) = \lim_{x \rightarrow 0}f(x) = 0$ is given by $$ f(x) = \begin{cases} c_1 x^{\lambda_1} \quad \text{if}  \ x<\hat{x}\\ c_2 x^{\lambda_2} \quad \text{if}  \ x>\hat{x}. \end{cases} $$ This is found by guessing the solution has the shape $Ax^\lambda$ and plugging such guess to the ODE shown above. Doing this we can verify that the solution satisfies the ODE and gives us the expression for $\lambda$ . $$ \lambda = \dfrac{b-3a \pm \sqrt{(b-a)^2+4ac}}{2a} $$ Since I have a Dirac impulse at $x=\hat{x}$ , I should be solving for two ODEs, one below $x=\hat{x}$ and another above $x=\hat{x}$ . I then have to glue together both solutions at $x=\hat{x}$ . This tells us that $$c_1 = c_2 \hat{x}^{\lambda_2-\lambda_1}.$$ Finally, as we have a Dirac delta at $x=\hat{x}$ , we get yet another boundary condition by integrating the ODE around $\hat{x}$ . $$ \lim_{\epsilon \rightarrow 0}\int_{\hat{x}-\epsilon}^{\hat{x}+\epsilon} \left(\dfrac{\partial^2}{\partial x^2}\left[ax^2f(x)\right] - \dfrac{\partial}{\partial x}\left[bxf(x)\right] - c f(x) + c\delta(x-\hat{x}) \right) \text{d}x = 0$$ When we do this, the second and third terms of the ODE drop out (by continuity these two terms must be equal to zero when we integrate them around $\hat{x}$ ). Thus we end up with the next condition. $\dfrac{c}{a} + c_2\lambda_2\hat{x}^{\lambda_2+1} = c_1\lambda_1\hat{x}^{\lambda_1+1}$ Putting everything together we get the coefficients $c_1,c_2$ $$ c_1 = \dfrac{c}{a}\dfrac{\hat{x}^{-(1+\lambda_1)}}{(\lambda_1-\lambda_2)}, \ c_2 = \dfrac{c}{a}\dfrac{\hat{x}^{-(1+\lambda_2)}}{(\lambda_1-\lambda_2)},$$ and we use the $\lambda$ that is a negative root for $x > \hat{x}$ , (and viceversa for $x<\hat{x}$ ). The steps I followed to tackle this question are based on two previous questions I asked here. One case did not have diffusion but had convection (just a first order ODE - link here ), whereas the other case had diffusion but no convection (second order ODE without the first derivative term - link here ). This case now has diffusion and convection. The problem I have is that 1) The solution does not match comparisons with numerical results which I know are correct. 2) The solution does not integrate to 1, that is $\int f(x)\text{d}x \neq 1$ . I tried to impose this as a boundary condition but once again my results do not make much sense. Where did I make a mistake, or how would you solve this problem? A bit more on the intuition behind the problem. The ODE in question is a steady state Fokker-Planck (or Kolmogorov Forward) Equation. Mass is injected at $x=\hat{x}$ and dissipates both to the left and right of $x=\hat{x}$ because of diffusion. Diffusion is controled by $a$ . The result should display a density with mass skewed to the right of $\hat{x}$ since $b>0$ . $b$ controls convection. Then, mass anywhere in $x\in(0,\infty)$ is taken out at a rate $c$ and reinjected back to $x=\hat{x}$ .","I need to solve the following ODE (a steady state Fokker-Planck/Kolmogorov Forward equation) where and where as . The constants are all positive and is positive and far away from zero (far enough such that the solution at should be ). The solution ignoring the Dirac impulse (the solutions where ) and imposing that is given by This is found by guessing the solution has the shape and plugging such guess to the ODE shown above. Doing this we can verify that the solution satisfies the ODE and gives us the expression for . Since I have a Dirac impulse at , I should be solving for two ODEs, one below and another above . I then have to glue together both solutions at . This tells us that Finally, as we have a Dirac delta at , we get yet another boundary condition by integrating the ODE around . When we do this, the second and third terms of the ODE drop out (by continuity these two terms must be equal to zero when we integrate them around ). Thus we end up with the next condition. Putting everything together we get the coefficients and we use the that is a negative root for , (and viceversa for ). The steps I followed to tackle this question are based on two previous questions I asked here. One case did not have diffusion but had convection (just a first order ODE - link here ), whereas the other case had diffusion but no convection (second order ODE without the first derivative term - link here ). This case now has diffusion and convection. The problem I have is that 1) The solution does not match comparisons with numerical results which I know are correct. 2) The solution does not integrate to 1, that is . I tried to impose this as a boundary condition but once again my results do not make much sense. Where did I make a mistake, or how would you solve this problem? A bit more on the intuition behind the problem. The ODE in question is a steady state Fokker-Planck (or Kolmogorov Forward) Equation. Mass is injected at and dissipates both to the left and right of because of diffusion. Diffusion is controled by . The result should display a density with mass skewed to the right of since . controls convection. Then, mass anywhere in is taken out at a rate and reinjected back to .","\begin{equation}
   \dfrac{\partial^2}{\partial x^2}\left[ax^2f(x)\right] - \dfrac{\partial}{\partial x}\left[bxf(x)\right] - c f(x) + c\delta(x-\hat{x}) = 0
\end{equation} x\in(0,\infty) f(x)\rightarrow 0 x \rightarrow \infty a,b,c \hat{x} x=0 f(0)=0 \delta(x-\hat{x}) x \neq \hat{x} \lim_{x \rightarrow \infty}f(x) = \lim_{x \rightarrow 0}f(x) = 0 
f(x) =
\begin{cases}
c_1 x^{\lambda_1} \quad \text{if}  \ x<\hat{x}\\
c_2 x^{\lambda_2} \quad \text{if}  \ x>\hat{x}.
\end{cases}
 Ax^\lambda \lambda  \lambda = \dfrac{b-3a \pm \sqrt{(b-a)^2+4ac}}{2a}  x=\hat{x} x=\hat{x} x=\hat{x} x=\hat{x} c_1 = c_2 \hat{x}^{\lambda_2-\lambda_1}. x=\hat{x} \hat{x}  \lim_{\epsilon \rightarrow 0}\int_{\hat{x}-\epsilon}^{\hat{x}+\epsilon} \left(\dfrac{\partial^2}{\partial x^2}\left[ax^2f(x)\right] - \dfrac{\partial}{\partial x}\left[bxf(x)\right] - c f(x) + c\delta(x-\hat{x}) \right) \text{d}x = 0 \hat{x} \dfrac{c}{a} + c_2\lambda_2\hat{x}^{\lambda_2+1} = c_1\lambda_1\hat{x}^{\lambda_1+1} c_1,c_2  c_1 = \dfrac{c}{a}\dfrac{\hat{x}^{-(1+\lambda_1)}}{(\lambda_1-\lambda_2)}, \ c_2 = \dfrac{c}{a}\dfrac{\hat{x}^{-(1+\lambda_2)}}{(\lambda_1-\lambda_2)}, \lambda x > \hat{x} x<\hat{x} \int f(x)\text{d}x \neq 1 x=\hat{x} x=\hat{x} a \hat{x} b>0 b x\in(0,\infty) c x=\hat{x}","['ordinary-differential-equations', 'probability-distributions', 'partial-differential-equations', 'stochastic-processes', 'dirac-delta']"
41,"What are the advantages of ""modern"" differential geometry to solve ODEs","What are the advantages of ""modern"" differential geometry to solve ODEs",,"I am currently learning ""modern"" differential geometry (i.e. in terms of vector fields, differential forms, etc, in a coordinate-free way). I know that the study of ODEs can be rewritten in those terms, see for instance this math.se question . However, I don't really understand yet what is gained in using this language compared to what one learns in the first few years of undergrad. I am looking for a good reference that would describe in detail how this modern view of differential geometry can be used to solve flows induced by vector fields (in particular, the existence of fixed points, perturbations of those, etc), and/or one that explains the advantages of that approach. Skimming through Lee's introduction to differential geometry (p. 333), it seems that I am looking for a reference for ""smooth dynamical systems"" in this language, but I can't find something relevant online.","I am currently learning ""modern"" differential geometry (i.e. in terms of vector fields, differential forms, etc, in a coordinate-free way). I know that the study of ODEs can be rewritten in those terms, see for instance this math.se question . However, I don't really understand yet what is gained in using this language compared to what one learns in the first few years of undergrad. I am looking for a good reference that would describe in detail how this modern view of differential geometry can be used to solve flows induced by vector fields (in particular, the existence of fixed points, perturbations of those, etc), and/or one that explains the advantages of that approach. Skimming through Lee's introduction to differential geometry (p. 333), it seems that I am looking for a reference for ""smooth dynamical systems"" in this language, but I can't find something relevant online.",,"['ordinary-differential-equations', 'differential-geometry', 'dynamical-systems']"
42,Concept of weak solutions for ODEs,Concept of weak solutions for ODEs,,"Consider the inital problem for ODE \begin{eqnarray} \frac{du}{dt}&=&f(t,u),\\ u(0)&=&u_0. \end{eqnarray} If $f$ is locally bounded function i.e., $f(t,u) \leq M_K$ for all $(t,u)$ in some compactset $K,$ then $u(t)=u_0+\int\limits_{0}^t f(s,u(s))ds$ is a locally Lipschitz continuous functions. Thus, for a.e. $t>0,$ $t\mapsto \int\limits_{0}^t f(s,u(s))ds$ is differentiable and hence by Fundamental theorem of calculus, $u$ satisfies the following weak formulation. \begin{eqnarray} \int\limits_0^{\infty} u\phi_t dt=\int\limits_0^{\infty}f(t,u(t))\phi(t)dt +u_0\phi(0) \quad \quad \text{for all } \phi \in C_c^1([0,\infty). \end{eqnarray} In conclusion, the above IVP admits a Lipschitz continuous solution (globally for all $t\geq 0$ ) even for $f$ which can be discontinuous in $t$ as well as $u$ variable! Is it correct or I am making some mistakes? Is this solution unique? In other words can this IVP have multiple weak solutions? Though the concept of weak solution is quite popular for PDEs, I have not seen this being discussed for initial value problems of ODEs in any of the textbooks. Whys is this so?","Consider the inital problem for ODE If is locally bounded function i.e., for all in some compactset then is a locally Lipschitz continuous functions. Thus, for a.e. is differentiable and hence by Fundamental theorem of calculus, satisfies the following weak formulation. In conclusion, the above IVP admits a Lipschitz continuous solution (globally for all ) even for which can be discontinuous in as well as variable! Is it correct or I am making some mistakes? Is this solution unique? In other words can this IVP have multiple weak solutions? Though the concept of weak solution is quite popular for PDEs, I have not seen this being discussed for initial value problems of ODEs in any of the textbooks. Whys is this so?","\begin{eqnarray}
\frac{du}{dt}&=&f(t,u),\\
u(0)&=&u_0.
\end{eqnarray} f f(t,u) \leq M_K (t,u) K, u(t)=u_0+\int\limits_{0}^t f(s,u(s))ds t>0, t\mapsto \int\limits_{0}^t f(s,u(s))ds u \begin{eqnarray}
\int\limits_0^{\infty} u\phi_t dt=\int\limits_0^{\infty}f(t,u(t))\phi(t)dt +u_0\phi(0) \quad \quad \text{for all } \phi \in C_c^1([0,\infty).
\end{eqnarray} t\geq 0 f t u","['real-analysis', 'ordinary-differential-equations', 'analysis', 'partial-differential-equations']"
43,Is this a legitimate substitution to solve the Mathieu equation?,Is this a legitimate substitution to solve the Mathieu equation?,,"My apologies if question is too applied, but it seemed to be a little too mathematically technical for the physics stack exchange. This paper claims to have found an analytic solution to the Mathieu differntial equation ( https://academic.oup.com/ptep/article/2020/4/043A01/5823816 ), which would be quite spectacular. To keep the question focused, I wanted to ask about an early part of the paper, where the author simplifies \begin{equation}   \frac{d^2y}{dx^2} + (a-2q\cos(2x)) y(x) = 0 \end{equation} by using the substitution $z=e^{ix}$ . This substitution strikes me as odd, as even if we allow $x \in \mathbb{C}$ , the domain of $z$ would only be the the punctured plane; this would seem problematic if we are to allow for initial conditions at $x=0$ . My question is the following: Does my concern make this substitution illegitimate? If the substitution is legitimate, does any sort of special handling of the $x=0$ case need to happen? Also if anyone wants to go through the paper and point out anything else that seems odd, they are more than welcome.","My apologies if question is too applied, but it seemed to be a little too mathematically technical for the physics stack exchange. This paper claims to have found an analytic solution to the Mathieu differntial equation ( https://academic.oup.com/ptep/article/2020/4/043A01/5823816 ), which would be quite spectacular. To keep the question focused, I wanted to ask about an early part of the paper, where the author simplifies by using the substitution . This substitution strikes me as odd, as even if we allow , the domain of would only be the the punctured plane; this would seem problematic if we are to allow for initial conditions at . My question is the following: Does my concern make this substitution illegitimate? If the substitution is legitimate, does any sort of special handling of the case need to happen? Also if anyone wants to go through the paper and point out anything else that seems odd, they are more than welcome.","\begin{equation}
  \frac{d^2y}{dx^2} + (a-2q\cos(2x)) y(x) = 0
\end{equation} z=e^{ix} x \in \mathbb{C} z x=0 x=0",['ordinary-differential-equations']
44,The infinite series $\sum_{k=-\infty}^\infty I_k(a)\operatorname{sinc}(k\phi)$,The infinite series,\sum_{k=-\infty}^\infty I_k(a)\operatorname{sinc}(k\phi),"While trying to solve the differential equation $$ a \frac{\partial^2y}{\partial a^2} + \frac{\partial y}{\partial a} -a y = e^{a\cos\phi}\sin\phi\;\;\;,\;\;\; y(0,\phi) = \phi $$ during a calculation I was doing, I found an intriguing infinite series form of the solution: $$ y(a,\phi) = \phi\sum_{k=-\infty}^\infty I_k(a)\operatorname{sinc}(k\phi). $$ This looks like the kind of thing that would have a ""known"" form, but no amount of massaging seemed to get Mathematica to give an answer (the above is in fact the result of said massaging--the initial form was much more complicated). Anyone have any ideas?","While trying to solve the differential equation during a calculation I was doing, I found an intriguing infinite series form of the solution: This looks like the kind of thing that would have a ""known"" form, but no amount of massaging seemed to get Mathematica to give an answer (the above is in fact the result of said massaging--the initial form was much more complicated). Anyone have any ideas?","
a \frac{\partial^2y}{\partial a^2} + \frac{\partial y}{\partial a} -a y = e^{a\cos\phi}\sin\phi\;\;\;,\;\;\; y(0,\phi) = \phi
 
y(a,\phi) = \phi\sum_{k=-\infty}^\infty I_k(a)\operatorname{sinc}(k\phi).
","['sequences-and-series', 'ordinary-differential-equations', 'bessel-functions']"
45,Existence and uniqueness for IVP with a square root,Existence and uniqueness for IVP with a square root,,"This question is from an old exam that a classmate shared with me. Question: Determine if the following IVP have a solution and if it is unique or not. i) $$\left \{\begin{matrix}y'(s) & = & \sqrt{y(s)+s}, \\ y(0) & = & x,\end{matrix}\right .$$ where $x$ is a given real number. ii) $$\left \{\begin{matrix}y_1'(s) & = & y_1(s)y_2(s), \\ y_2'(s) & = & y_1(s)+\sqrt{s}, \\ y_1(0) & = & x_1, \\ y_2(0) & = & x_2,\end{matrix}\right .$$ where $x_1,x_2$ are given real numbers. Ideas/Attempt: For i) Wolfram gives a solution using Lambert's $W$ -function, and this isn't something I've been taught (or know) about yet. For ii) I know how to solve for $y_1$ as a function of $s$ and $y_2$ (integrating factor, linear diff eq, etc.), but I don't know how to prove/disprove the existence of a solution. I'll like to solve this using Peano or Picard-Lindelöf to prove existence and uniquness if possible, but I'm confused because both theorems have as hypothesis that the function $f(s,y)$ is defined in an open neighbourhodd of the point determined by the initial conditions, in this case, $(0,\text{something})$ . What should I do in this case? Is there some kind of (relatively common) trick that works for differential equations involving square roots (or derivatives squared) that I should be aware of? Thank you in advance.","This question is from an old exam that a classmate shared with me. Question: Determine if the following IVP have a solution and if it is unique or not. i) where is a given real number. ii) where are given real numbers. Ideas/Attempt: For i) Wolfram gives a solution using Lambert's -function, and this isn't something I've been taught (or know) about yet. For ii) I know how to solve for as a function of and (integrating factor, linear diff eq, etc.), but I don't know how to prove/disprove the existence of a solution. I'll like to solve this using Peano or Picard-Lindelöf to prove existence and uniquness if possible, but I'm confused because both theorems have as hypothesis that the function is defined in an open neighbourhodd of the point determined by the initial conditions, in this case, . What should I do in this case? Is there some kind of (relatively common) trick that works for differential equations involving square roots (or derivatives squared) that I should be aware of? Thank you in advance.","\left \{\begin{matrix}y'(s) & = & \sqrt{y(s)+s}, \\ y(0) & = & x,\end{matrix}\right . x \left \{\begin{matrix}y_1'(s) & = & y_1(s)y_2(s), \\ y_2'(s) & = & y_1(s)+\sqrt{s}, \\ y_1(0) & = & x_1, \\ y_2(0) & = & x_2,\end{matrix}\right . x_1,x_2 W y_1 s y_2 f(s,y) (0,\text{something})",['ordinary-differential-equations']
46,Doubt in a differential equation (solution given),Doubt in a differential equation (solution given),,"Let $y=y(x)$ be the solution of the differential equation $\frac{\mathrm{d} y}{\mathrm{~d} x}+\frac{\sqrt{2} y}{2 \cos ^{4} x-\cos 2 x}=x \mathrm{e}^{\tan ^{-1}(\sqrt{2} \cot 2 x)}, 0<x<\pi / 2$ with $y\left(\frac{\pi}{4}\right)=\frac{\pi^{2}}{32} .$ If $y\left(\frac{\pi}{3}\right)=\frac{\pi^{2}}{18} \mathrm{e}^{-\tan ^{-1}(\alpha)}$ , then the value of $3 \alpha^{2}$ is equal to Source : Jee Main 2022 29th June shift 1 Now I  started off by simplifying the trigonometric expression(s) and then searching for the DE. Simplfying the DE I simplified the trignometric term in the denominator by the following sequence of equalities: $$ \begin{align}2(\cos x)^4 -\cos2x &=2(\cos x)^4 -2(\cos x)^2+1 \\  &= 1-2(\sin x)^2(\cos x)^2\\ &= 1-\frac{(\sin2x)^2}{2} \end{align}$$ My differential equation simplifies as: $$\frac{dy}{dx} + \frac{\sqrt{2}y}{1-\frac{(\sin2x)^2}{2}} =xe^{\tan^{-1}{\sqrt{2}\cot2x}}              $$ on further simplification I reached this $$ \frac{dy}{dx} + \frac{4\sqrt{2}y}{3+\cos4x} =xe\large^{\tan^{-1}{\sqrt{2}\cot2x}} $$ Finding IF We have by the standard formulas, $$IF=  e^{\int\frac{4\sqrt{2}}{3+cos4x}dx } $$ here I use substitution ( I have put $\tan2x=t$ ) so that $ \frac{dt}{2(1+t^2)}$ =dx and I wrote $\cos4x$ as $ \large \frac{1-t^2}{(1+t^2)}$ Doing everything gives me IF as $$IF= \huge e^{\tan^{-1}(\frac{t}{\sqrt{2}})} $$ now t was tan2x so  my equation becomes (when we multiply IF on both sides of the equation) $$\large d(y e^{\tan^{-1}(\frac{tan2x}{\sqrt{2}})})=xe^{\tan^{-1}{\sqrt{2}cot2x}+\tan^{-1}(\frac{tan2x}{\sqrt{2}})} dx $$ now the solution given to me was that I should convert $\tan^{-1}{\sqrt{2}cot2x}+\tan^{-1}(\frac{tan2x}{\sqrt{2}})$ into $\frac{\pi}{2}$ I think that this step is wrong as this is only valid if x was in $0<x<\frac{\pi}{4}$ as then $2x$ would be less than $\frac{\pi}{2}$ We know that $\cot^{-1}{x}-\pi =\tan^{-1}{\frac{1}{x}}$ when $x<0$ so we can't say that its pi/2 and integrate it. BTW if you assume it to be pi/2 the answer matches to what was given to me. Edit: I think it should be like this: $for0<x<\frac{\pi}{4}$ $$\large y e^{\tan^{-1}(\frac{tan2x}{\sqrt{2}})}=\frac{x^2}{2} e^{\frac{\pi}{2}} +c$$ and $for\frac{\pi}{4}<x<\frac{\pi}{2}$ $$\large y e^{\tan^{-1}(\frac{tan2x}{\sqrt{2}})}=\frac{x^2}{2} e^{\frac{-\pi}{2}} +c'$$ Final conclusion : Thanks to Mr IvanKaznacheyeu for pointing out that I only needed the second one of my 2 solutions for solving the problem. Here for completely solving the problem: We consider 2nd part of my answer. Here taking limits on both sides to solve for c'. $$\large \lim\limits_{0\to \frac{\pi}{4}+} y e^{\tan^{-1}(\frac{tan2x}{\sqrt{2}})}=\lim\limits_{0\to \frac{\pi}{4}+}\frac{x^2}{2} e^{\frac{-\pi}{2}} +c'$$ solving the above will give c=0 and finally when we put x=pi/3 after some rearrangement this yields $\alpha$ =( $\sqrt\frac{2}{3}$ ) hence answer is 2","Let be the solution of the differential equation with If , then the value of is equal to Source : Jee Main 2022 29th June shift 1 Now I  started off by simplifying the trigonometric expression(s) and then searching for the DE. Simplfying the DE I simplified the trignometric term in the denominator by the following sequence of equalities: My differential equation simplifies as: on further simplification I reached this Finding IF We have by the standard formulas, here I use substitution ( I have put ) so that =dx and I wrote as Doing everything gives me IF as now t was tan2x so  my equation becomes (when we multiply IF on both sides of the equation) now the solution given to me was that I should convert into I think that this step is wrong as this is only valid if x was in as then would be less than We know that when so we can't say that its pi/2 and integrate it. BTW if you assume it to be pi/2 the answer matches to what was given to me. Edit: I think it should be like this: and Final conclusion : Thanks to Mr IvanKaznacheyeu for pointing out that I only needed the second one of my 2 solutions for solving the problem. Here for completely solving the problem: We consider 2nd part of my answer. Here taking limits on both sides to solve for c'. solving the above will give c=0 and finally when we put x=pi/3 after some rearrangement this yields =( ) hence answer is 2","y=y(x) \frac{\mathrm{d} y}{\mathrm{~d} x}+\frac{\sqrt{2} y}{2 \cos ^{4} x-\cos 2 x}=x \mathrm{e}^{\tan ^{-1}(\sqrt{2} \cot 2 x)}, 0<x<\pi / 2 y\left(\frac{\pi}{4}\right)=\frac{\pi^{2}}{32} . y\left(\frac{\pi}{3}\right)=\frac{\pi^{2}}{18} \mathrm{e}^{-\tan ^{-1}(\alpha)} 3 \alpha^{2}  \begin{align}2(\cos x)^4 -\cos2x &=2(\cos x)^4 -2(\cos x)^2+1 \\  &= 1-2(\sin x)^2(\cos x)^2\\ &= 1-\frac{(\sin2x)^2}{2} \end{align} \frac{dy}{dx} + \frac{\sqrt{2}y}{1-\frac{(\sin2x)^2}{2}} =xe^{\tan^{-1}{\sqrt{2}\cot2x}}                \frac{dy}{dx} + \frac{4\sqrt{2}y}{3+\cos4x} =xe\large^{\tan^{-1}{\sqrt{2}\cot2x}}  IF=  e^{\int\frac{4\sqrt{2}}{3+cos4x}dx }  \tan2x=t  \frac{dt}{2(1+t^2)} \cos4x  \large \frac{1-t^2}{(1+t^2)} IF= \huge e^{\tan^{-1}(\frac{t}{\sqrt{2}})}  \large d(y e^{\tan^{-1}(\frac{tan2x}{\sqrt{2}})})=xe^{\tan^{-1}{\sqrt{2}cot2x}+\tan^{-1}(\frac{tan2x}{\sqrt{2}})} dx  \tan^{-1}{\sqrt{2}cot2x}+\tan^{-1}(\frac{tan2x}{\sqrt{2}}) \frac{\pi}{2} 0<x<\frac{\pi}{4} 2x \frac{\pi}{2} \cot^{-1}{x}-\pi =\tan^{-1}{\frac{1}{x}} x<0 for0<x<\frac{\pi}{4} \large y e^{\tan^{-1}(\frac{tan2x}{\sqrt{2}})}=\frac{x^2}{2} e^{\frac{\pi}{2}} +c for\frac{\pi}{4}<x<\frac{\pi}{2} \large y e^{\tan^{-1}(\frac{tan2x}{\sqrt{2}})}=\frac{x^2}{2} e^{\frac{-\pi}{2}} +c' \large \lim\limits_{0\to \frac{\pi}{4}+} y e^{\tan^{-1}(\frac{tan2x}{\sqrt{2}})}=\lim\limits_{0\to \frac{\pi}{4}+}\frac{x^2}{2} e^{\frac{-\pi}{2}} +c' \alpha \sqrt\frac{2}{3}","['ordinary-differential-equations', 'solution-verification']"
47,Estimate from a differential inequality,Estimate from a differential inequality,,"Let $[0,T]$ be some finite interval. Consider the differential inequality $$f'(t)\leq C t^{\alpha}f(t)^2+g(t),$$ where $g$ is positive and bounded and $\alpha >0$ , $C<0$ . Can we get a bound for $f(t)$ , $0<t\leq T$ in terms of $f(0)$ and $g$ ? Edit: Let me just clarify what I am aiming or. If there is no $g$ in the inequality we can divide by $f(t)^2$ and integrate on both sides such that $$\frac{1}{f(t)}\geq \frac{D}{\alpha + 1}t^{\alpha+1}+\frac{1}{f(0)},$$ $D=-C$ . Hence $$f(t)\leq \frac{D f(0)\frac{1}{t^{\alpha +1 }}+\alpha + 1}{(\alpha + 1)f(0)}.$$ Can we reach something similar when $g$ is present? Separating the ""variables"" is not so easy as we don't know a potential for $\frac{g}{f}$ . I also tried variation of the parameters but I failed. Is there maybe some other way do this?","Let be some finite interval. Consider the differential inequality where is positive and bounded and , . Can we get a bound for , in terms of and ? Edit: Let me just clarify what I am aiming or. If there is no in the inequality we can divide by and integrate on both sides such that . Hence Can we reach something similar when is present? Separating the ""variables"" is not so easy as we don't know a potential for . I also tried variation of the parameters but I failed. Is there maybe some other way do this?","[0,T] f'(t)\leq C t^{\alpha}f(t)^2+g(t), g \alpha >0 C<0 f(t) 0<t\leq T f(0) g g f(t)^2 \frac{1}{f(t)}\geq \frac{D}{\alpha + 1}t^{\alpha+1}+\frac{1}{f(0)}, D=-C f(t)\leq \frac{D f(0)\frac{1}{t^{\alpha +1 }}+\alpha + 1}{(\alpha + 1)f(0)}. g \frac{g}{f}","['real-analysis', 'ordinary-differential-equations', 'inequality']"
48,Mathematical formulation of a gravitational law for galaxies beyond newtonian,Mathematical formulation of a gravitational law for galaxies beyond newtonian,,"The author is not familiared with the current theories and the specific model for dark matter, nor the PDE aspect of mathematical theories related to the dark matter problem, so I apologize in advance if the opinions are out of date or the question is mathematically ill-posed currently, and welcome any advise and improvement! Some backgrouds: Originally dark matter was posed to explain the flattening of spiral rotational curve observed in galaxies, which does not match the density distribution measured from luminosity, modelled by Boltzmann equation (and its deviations). See for example [Chandrasekhar S.;Principles of Stellar Dynamics]. There is also an interesting stability theory which explained the formation of spiral arms [C.LIN,F.SHU;On the spiral structure of disk galaxies]. To quote from Poincare's celebrated ""Les Méthodes Nouvelles de la Mécanique Céleste"": 'Le but final de la Mécanique céleste est de résoudre cette grande question de savoir si la loi de Newton explique à elle seule tous les phénomènes astronomiques.' Motivated by this, if one considers a simplified  2-dimensional system, with axially-symmetric velocity field $u_i(x,t)$ given by rotation curves observed, and $\rho_i(x,t)$ the correponding density distribution measured. My question may be formulated: Question: How to find the functional $F(u_i,\rho_i,\frac{\partial u_i}{\partial x},\frac{\partial \rho_i}{\partial x},\dots,x,t)=0$ which hold for all $i$ , such that  the functional $F$ coincide with newtonian gravitational law locally in $x$ , and is generic in the functional space? A first impression is to do as Kepler did: trial and error. But from a mathematical point of view, there are still many unexplored area in differential\difference equations which may lead to the well-posedness (or ill-posedness) of the above question. Inverse problems in PDEs seem to deal with the case where the differential equations underlining the problem are given, and to find the coefficients. But is there any progress on finding the underlying differential equation itself?","The author is not familiared with the current theories and the specific model for dark matter, nor the PDE aspect of mathematical theories related to the dark matter problem, so I apologize in advance if the opinions are out of date or the question is mathematically ill-posed currently, and welcome any advise and improvement! Some backgrouds: Originally dark matter was posed to explain the flattening of spiral rotational curve observed in galaxies, which does not match the density distribution measured from luminosity, modelled by Boltzmann equation (and its deviations). See for example [Chandrasekhar S.;Principles of Stellar Dynamics]. There is also an interesting stability theory which explained the formation of spiral arms [C.LIN,F.SHU;On the spiral structure of disk galaxies]. To quote from Poincare's celebrated ""Les Méthodes Nouvelles de la Mécanique Céleste"": 'Le but final de la Mécanique céleste est de résoudre cette grande question de savoir si la loi de Newton explique à elle seule tous les phénomènes astronomiques.' Motivated by this, if one considers a simplified  2-dimensional system, with axially-symmetric velocity field given by rotation curves observed, and the correponding density distribution measured. My question may be formulated: Question: How to find the functional which hold for all , such that  the functional coincide with newtonian gravitational law locally in , and is generic in the functional space? A first impression is to do as Kepler did: trial and error. But from a mathematical point of view, there are still many unexplored area in differential\difference equations which may lead to the well-posedness (or ill-posedness) of the above question. Inverse problems in PDEs seem to deal with the case where the differential equations underlining the problem are given, and to find the coefficients. But is there any progress on finding the underlying differential equation itself?","u_i(x,t) \rho_i(x,t) F(u_i,\rho_i,\frac{\partial u_i}{\partial x},\frac{\partial \rho_i}{\partial x},\dots,x,t)=0 i F x","['functional-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'dynamical-systems', 'mathematical-astronomy']"
49,"Find the extinction time $T$ for the solutions $\ddot{y}+a\,\text{sgn}(\dot{y})\sqrt{|\dot{y}|}(1+|\dot{y}|^{3/2})+b\,\sin(y)=0,\,y(0)>0,\,y'(0)=0$?",Find the extinction time  for the solutions ?,"T \ddot{y}+a\,\text{sgn}(\dot{y})\sqrt{|\dot{y}|}(1+|\dot{y}|^{3/2})+b\,\sin(y)=0,\,y(0)>0,\,y'(0)=0","For the differential equation with two real-valued positive parameters $\{a,\,b\}>0$ : $$\ddot{y}+a\cdot\text{sgn}(\dot{y})\sqrt{|\dot{y}|}\left(1+|\dot{y}|^{\frac{3}{2}}\right)+b\cdot\sin(y)=0,\,y(0)>0,\,\dot{y}(0)=0 \tag{Eq. 1}\label{Eq. 1}$$ I would like to know if is possible to find its extinction time $0 < T < \infty$ such their solutions become exactly $y(t)=0,\,t>T$ , as a function of their parameters and initial conditions $\{a,\,b,\,y(0),\,\dot{y}(0)\}$ , being in principle, $y(t)$ a real-valued scalar function. Motivation - not needed for finding the solution Recently I learned that a ordinary differential equation (ODE) require to have a point in time where is locally non-Lipschitz (LNL) in order to been able of having solution with a finite extinction time (details here ), which have interesting consequences in using ODEs as a tool for model reality: An ODE that stands solutions of finite duration cannot hold uniqueness of solutions at the same time (or at least, violates Picard-Lindelöf Theorem's conditions due the LNL singular point). If a solution is of finite duration, it don't going to hold time-reversal symmetry (property is commonly only attributed to entropy). Since traditional physics models holds uniqueness of solutions, their solutions, accurately speaking, are always never-ending in time: this means that everything have ever happen before in time could be, in principle, affecting any experiment in the present time, which is in conflict with locality and causality. With this in mind, I am exploring how to modify the classic nonlinear pendulum with friction in order of having a finite extinction time, and I am using a LNL drag force: $$F_d =  a\cdot\text{sgn}(\dot{y})\sqrt{|\dot{y}|}\left(1+|\dot{y}|^{\frac{3}{2}}\right) = a\cdot\text{sgn}(\dot{y})\sqrt{|\dot{y}|} + a\cdot\dot{y}|\dot{y}|  \tag{Eq. 2}\label{Eq. 2}$$ which behaves similar to the classic quadratic drag force for high speed $F_d \approx c\cdot (\dot{y})^2$ , also behaves similar the classic Stokes' Law for non-zero-low-speeds $F_d \approx 2c\cdot \dot{y},\,0.2 <\dot{y}<1.2$ , and makes a singular LNL point for $\dot{y}\to 0$ so finite duration solutions could arise (see details here ). Issues with numerical examples: If the classic nonlinear pendulum with friction equation is reviewed as example, in Wolfram-Alpha it can be seen it have decaying solutions as expected: $$\ddot{x}+2\cdot0.021\,\dot{x}+0.2\sin(x)=0, x(0)=\frac{\pi}{2}, \dot{x}(0)=0 \tag{Eq. 3}\label{Eq. 3}$$ But following the analysis for T-Symmetry , under the transformation $\hat{t} \to - t$ the position and acceleration remains the same, but the velocity profile change in sign: this applied to \eqref{Eq. 3} shows it change under this transformation, so it not fulfill T-Symmetry. If instead the standard drag force $F_{\text{drag}}\propto (\dot{x})^2$ is used as is shown here for the equation: $$\ddot{x}+0.021(\dot{x})^2+0.2\sin(x)=0, x(0)=\frac{\pi}{2}, \dot{x}(0)=0 \tag{Eq. 4}\label{Eq. 4}$$ their solution aren't showing the expected decay one can see on the experimental pendulums. This is commonly solve using an ansatz for the drag force $F_{\text{drag}}\propto \dot{x}|\dot{x}|$ , which as can be seen here for the equation: $$\ddot{x}+0.021\dot{x}|\dot{x}|+0.2\sin(x)=0, x(0)=\frac{\pi}{2}, \dot{x}(0)=0 \tag{Eq. 5}\label{Eq. 5}$$ their solution are indeed having the expected decaying behavior for a pendulum with friction. Since \eqref{Eq. 4} fulfills T-Symmetry but fails to proper model the pendulum with friction, but both \eqref{Eq. 3} and \eqref{Eq. 5} don't fulfill T-Symmetry but modeled properly the amplitudes, its look like for having a decaying behavior it was required to choose a differential equation that is not fulfilling the time-reversal-symmetry . But, since the function $f(x)=x|x|$ is locally Lipschitz (see here ), it cannot be used for having a singular point on a finite time where uniqueness could be broken, so this ansatz wouldn't have solutions of finite duration, this is why I chose to work instead with $F_d$ which also shows to have the decaying solutions (but also without T-Symmetry): $$\ddot{x}+0.021\,\text{sgn}(\dot{x})\sqrt{|\dot{x}|}(1+|\dot{x}|^{\frac{3}{2}})+0.2\sin(x)=0, x(0)=\frac{\pi}{2}, \dot{x}(0)=0 \tag{Eq. 6}\label{Eq. 6}$$ Analysis of the decay Since for some parameters $\{a,\,b\}$ the equation \eqref{Eq. 1} is having decaying solutions as can be seen for the example of \eqref{Eq. 6}, when the time is reaching the extinction time, the behavior of \eqref{Eq. 1} will be driving by: For the position the small-angle approximation could be used so $\sin(y)\approx y$ For the drag force $F_d$ , the quadratic term $\dot{y}|\dot{y}| \to 0$ for small speeds, so the remaining term $\text{sgn}(\dot{y})\sqrt{|\dot{y}|}$ will be only present. With this, in the small-angles low-speeds regimen the equation will be behaving as: $$\ddot{y}+a\cdot\text{sgn}(\dot{y})\sqrt{|\dot{y}|}+b\cdot y=0 \tag{Eq. 7}\label{Eq. 7}$$ Which, as for the previous examples without T-Symmetry, the equation show its having decaying solutions: $$\ddot{x}+0.021\,\text{sgn}(\dot{x})\sqrt{|\dot{x}|}+0.2\,x=0, x(0)=\frac{\pi}{14}, \dot{x}(0)=0 \tag{Eq. 8}\label{Eq. 8}$$ I have found the following papers under the term sublinear damping that shows example in physics of equations really similar to \eqref{Eq. 7} and \eqref{Eq. 8}: ""A note on the dynamics of an oscillator in the presence of strong friction"" - H.Amann & J.I.Diaz ""Behavior of Solutions of Second-Order Differential Equations with Sublinear Damping"" - J. Karsai & J. R. Graef So I believe they will be achieving a finite extinction time, but I have not being able to formally prove it. Added later I found this paper: ""The pendulum—Rich physics from a simple system"" - Robert A. Nelson & M. G. Olsson where the autors, in equation $(19)$ use a similar drag force of the form $F_d = b\ |\dot{y}| + c\ (\dot{y})^2$ , so at least a description through two components doesn't look like a complete insane approach. Also on equations $(59)$ and $(60)$ they introduce a Drag Force $F_d = b\ \dot{y} + c\ \dot{y}|\dot{y}|$ for a more accurate description of the air effects on the pendulum.","For the differential equation with two real-valued positive parameters : I would like to know if is possible to find its extinction time such their solutions become exactly , as a function of their parameters and initial conditions , being in principle, a real-valued scalar function. Motivation - not needed for finding the solution Recently I learned that a ordinary differential equation (ODE) require to have a point in time where is locally non-Lipschitz (LNL) in order to been able of having solution with a finite extinction time (details here ), which have interesting consequences in using ODEs as a tool for model reality: An ODE that stands solutions of finite duration cannot hold uniqueness of solutions at the same time (or at least, violates Picard-Lindelöf Theorem's conditions due the LNL singular point). If a solution is of finite duration, it don't going to hold time-reversal symmetry (property is commonly only attributed to entropy). Since traditional physics models holds uniqueness of solutions, their solutions, accurately speaking, are always never-ending in time: this means that everything have ever happen before in time could be, in principle, affecting any experiment in the present time, which is in conflict with locality and causality. With this in mind, I am exploring how to modify the classic nonlinear pendulum with friction in order of having a finite extinction time, and I am using a LNL drag force: which behaves similar to the classic quadratic drag force for high speed , also behaves similar the classic Stokes' Law for non-zero-low-speeds , and makes a singular LNL point for so finite duration solutions could arise (see details here ). Issues with numerical examples: If the classic nonlinear pendulum with friction equation is reviewed as example, in Wolfram-Alpha it can be seen it have decaying solutions as expected: But following the analysis for T-Symmetry , under the transformation the position and acceleration remains the same, but the velocity profile change in sign: this applied to \eqref{Eq. 3} shows it change under this transformation, so it not fulfill T-Symmetry. If instead the standard drag force is used as is shown here for the equation: their solution aren't showing the expected decay one can see on the experimental pendulums. This is commonly solve using an ansatz for the drag force , which as can be seen here for the equation: their solution are indeed having the expected decaying behavior for a pendulum with friction. Since \eqref{Eq. 4} fulfills T-Symmetry but fails to proper model the pendulum with friction, but both \eqref{Eq. 3} and \eqref{Eq. 5} don't fulfill T-Symmetry but modeled properly the amplitudes, its look like for having a decaying behavior it was required to choose a differential equation that is not fulfilling the time-reversal-symmetry . But, since the function is locally Lipschitz (see here ), it cannot be used for having a singular point on a finite time where uniqueness could be broken, so this ansatz wouldn't have solutions of finite duration, this is why I chose to work instead with which also shows to have the decaying solutions (but also without T-Symmetry): Analysis of the decay Since for some parameters the equation \eqref{Eq. 1} is having decaying solutions as can be seen for the example of \eqref{Eq. 6}, when the time is reaching the extinction time, the behavior of \eqref{Eq. 1} will be driving by: For the position the small-angle approximation could be used so For the drag force , the quadratic term for small speeds, so the remaining term will be only present. With this, in the small-angles low-speeds regimen the equation will be behaving as: Which, as for the previous examples without T-Symmetry, the equation show its having decaying solutions: I have found the following papers under the term sublinear damping that shows example in physics of equations really similar to \eqref{Eq. 7} and \eqref{Eq. 8}: ""A note on the dynamics of an oscillator in the presence of strong friction"" - H.Amann & J.I.Diaz ""Behavior of Solutions of Second-Order Differential Equations with Sublinear Damping"" - J. Karsai & J. R. Graef So I believe they will be achieving a finite extinction time, but I have not being able to formally prove it. Added later I found this paper: ""The pendulum—Rich physics from a simple system"" - Robert A. Nelson & M. G. Olsson where the autors, in equation use a similar drag force of the form , so at least a description through two components doesn't look like a complete insane approach. Also on equations and they introduce a Drag Force for a more accurate description of the air effects on the pendulum.","\{a,\,b\}>0 \ddot{y}+a\cdot\text{sgn}(\dot{y})\sqrt{|\dot{y}|}\left(1+|\dot{y}|^{\frac{3}{2}}\right)+b\cdot\sin(y)=0,\,y(0)>0,\,\dot{y}(0)=0 \tag{Eq. 1}\label{Eq. 1} 0 < T < \infty y(t)=0,\,t>T \{a,\,b,\,y(0),\,\dot{y}(0)\} y(t) F_d =  a\cdot\text{sgn}(\dot{y})\sqrt{|\dot{y}|}\left(1+|\dot{y}|^{\frac{3}{2}}\right) = a\cdot\text{sgn}(\dot{y})\sqrt{|\dot{y}|} + a\cdot\dot{y}|\dot{y}|  \tag{Eq. 2}\label{Eq. 2} F_d \approx c\cdot (\dot{y})^2 F_d \approx 2c\cdot \dot{y},\,0.2 <\dot{y}<1.2 \dot{y}\to 0 \ddot{x}+2\cdot0.021\,\dot{x}+0.2\sin(x)=0, x(0)=\frac{\pi}{2}, \dot{x}(0)=0 \tag{Eq. 3}\label{Eq. 3} \hat{t} \to - t F_{\text{drag}}\propto (\dot{x})^2 \ddot{x}+0.021(\dot{x})^2+0.2\sin(x)=0, x(0)=\frac{\pi}{2}, \dot{x}(0)=0 \tag{Eq. 4}\label{Eq. 4} F_{\text{drag}}\propto \dot{x}|\dot{x}| \ddot{x}+0.021\dot{x}|\dot{x}|+0.2\sin(x)=0, x(0)=\frac{\pi}{2}, \dot{x}(0)=0 \tag{Eq. 5}\label{Eq. 5} f(x)=x|x| F_d \ddot{x}+0.021\,\text{sgn}(\dot{x})\sqrt{|\dot{x}|}(1+|\dot{x}|^{\frac{3}{2}})+0.2\sin(x)=0, x(0)=\frac{\pi}{2}, \dot{x}(0)=0 \tag{Eq. 6}\label{Eq. 6} \{a,\,b\} \sin(y)\approx y F_d \dot{y}|\dot{y}| \to 0 \text{sgn}(\dot{y})\sqrt{|\dot{y}|} \ddot{y}+a\cdot\text{sgn}(\dot{y})\sqrt{|\dot{y}|}+b\cdot y=0 \tag{Eq. 7}\label{Eq. 7} \ddot{x}+0.021\,\text{sgn}(\dot{x})\sqrt{|\dot{x}|}+0.2\,x=0, x(0)=\frac{\pi}{14}, \dot{x}(0)=0 \tag{Eq. 8}\label{Eq. 8} (19) F_d = b\ |\dot{y}| + c\ (\dot{y})^2 (59) (60) F_d = b\ \dot{y} + c\ \dot{y}|\dot{y}|","['real-analysis', 'ordinary-differential-equations', 'dynamical-systems', 'mathematical-physics', 'finite-duration']"
50,Convergence to locally stable equilibria,Convergence to locally stable equilibria,,"I am working with a system whose trajectories converge to the set of equilibria. I can characterize all the equilibria in a nice way and easily compute their stability and whether they are isolated or not. The dynamical system is described by a Lipschitz continuous vector field. I conjecture that the system is ""well behaved"" so that the trajectories converge (almost always, depending on their initial condition) to the set of locally stable equilibria. I know that this is not always the case as there are plenty of counterexamples, e.g. this classic system with a homoclinic trajectory ( example ). I am just wondering if there is any approach/result available in the literature to show that this is true, given certain conditions on the vector field.","I am working with a system whose trajectories converge to the set of equilibria. I can characterize all the equilibria in a nice way and easily compute their stability and whether they are isolated or not. The dynamical system is described by a Lipschitz continuous vector field. I conjecture that the system is ""well behaved"" so that the trajectories converge (almost always, depending on their initial condition) to the set of locally stable equilibria. I know that this is not always the case as there are plenty of counterexamples, e.g. this classic system with a homoclinic trajectory ( example ). I am just wondering if there is any approach/result available in the literature to show that this is true, given certain conditions on the vector field.",,"['ordinary-differential-equations', 'dynamical-systems', 'vector-fields', 'nonlinear-dynamics']"
51,A Sturm–Liouville problem on the whole space,A Sturm–Liouville problem on the whole space,,"Let $q \in C^\infty(\mathbb{R}) \cap L^\infty(\mathbb{R})$ be an even function, with $q(0)<0$ , and assume $q$ converges exponentially fast to $\bar{q}>0$ for $x \to \pm\infty$ . Let $f \in L^2(\mathbb{R})$ be an odd function. Then, I am looking for a solution of $$ -u'' + q(x)u = f(x) $$ such that $u \in L^2(\mathbb{R})$ . The problem looks very innocent, reminiscent of a Sturm–Liouville problem (but on the whole of $\mathbb{R}$ ). However, the non-positivity of $q$ around $0$ jeopardizes many of my attempts to approach it. I also thought of going to the Fourier domain, but then I get stuck on an integral equation, as I am stuck on the above. Am I missing a reference where this problem is already extensively treated? Or otherwise, any suggestion on how to approach the existence of an $L^2$ solution?","Let be an even function, with , and assume converges exponentially fast to for . Let be an odd function. Then, I am looking for a solution of such that . The problem looks very innocent, reminiscent of a Sturm–Liouville problem (but on the whole of ). However, the non-positivity of around jeopardizes many of my attempts to approach it. I also thought of going to the Fourier domain, but then I get stuck on an integral equation, as I am stuck on the above. Am I missing a reference where this problem is already extensively treated? Or otherwise, any suggestion on how to approach the existence of an solution?","q \in C^\infty(\mathbb{R}) \cap L^\infty(\mathbb{R}) q(0)<0 q \bar{q}>0 x \to \pm\infty f \in L^2(\mathbb{R}) 
-u'' + q(x)u = f(x)
 u \in L^2(\mathbb{R}) \mathbb{R} q 0 L^2","['ordinary-differential-equations', 'sturm-liouville']"
52,"In the system $\dot x =(|x|^2 -1)(\bar x -x)$, characterize the type of equilibrium point $\bar x$ is","In the system , characterize the type of equilibrium point  is",\dot x =(|x|^2 -1)(\bar x -x) \bar x,"Consider the following system of ODEs : $\dot x =(|x|^2 -1)(\bar x -x)$ with $x \in \mathbb R^n, n\in \mathbb N, n\ge 1$ and $\bar x \in \mathbb R^n$ ,  assigned. Which of the following statements is surely true? (a) $\bar x$ is a locally attractive equilibrium point if it is close enough to the origin (b) $\bar x$ is a stable equilibrium point if it is far enough away from the origin (c) $\bar x$ is the only point of equilibrium in the system (d) $\bar x$ < is a globally asymptotically stable equilibrium point if it is close enough to the origin The given correct answer was (a) (but seems to be wrong) Can someone help me out to prove it formaly and also what is the reasoning to rule out the other ones? If it were just in n=1, I could just solve $(|x|^2 -1)(\bar x -x)=0 $ to find the equilibrium points and $(|x|^2 -1)(\bar x -x)>0 $ to find the intervals where the solution trajectory increases and $(|x|^2 -1)(\bar x -x)<0 $ the intervals where it decreases, but in n dimensions how do I deal with that ?** So so far I only have that the equilibrium points are $x=\bar x$ and all the points in the hypersphere $|x|=1$ Edit : Even in the one-dimensional case my analysis is not making any sense with the  answer that is supposed to be correct: For n=1, the equilibrium points are $\bar x, 1,-1$ Now analyzing the sign of the right-hand side of the ODE: Suppose $\bar x$ is close enough to the origin,as option (a) states, that is $ -1< \bar x < 1 $ , then $(x^2-1)(\bar x- x)<0 \iff x \in (-1,\bar x)\cup(1,+\infty)$ , so I get the following diagram for the trajectories over the real line: ---->>>-1<<<< $\bar x$ >>>>1<<<<---- , meaning that inside $\bar x$ is unstable. What am doing wrong? Edit 2 So with the help of MatthewH, it seems like (a) is certanly wrong at least in the n=1 case I will continue the analysis at least in the n=1 case Now option (b) seems to be the correct one because if $\bar x$ is far enough away from the origin, that is $\bar x>1$ or $\bar x<-1$ . To fix the idea I consider $\bar x >1$ : $(x^2-1)(\bar x- x)<0 \iff x \in (-1,1)\cup(\bar x,+\infty)$ so I get the following diagram for the trajectories over the real line: ---->>>-1<<<<1>>>> $\bar x$ <<<<---- , meaning that $\bar x$ is a sink, that  is asymptotically stable: attractive and stable. Therefore option b is the correct one To rule out the other ones: Option c is not correct because there are infinite equilibrium points Option d is not correct : $\bar x$ can't be globally asymptotically stable equilibrium point because when it is inside $(-1,1)$ , solution curves with initial value outside of that interval can't reach it Does this seems ok? Can someone provide a solution in n dimensions?","Consider the following system of ODEs : with and ,  assigned. Which of the following statements is surely true? (a) is a locally attractive equilibrium point if it is close enough to the origin (b) is a stable equilibrium point if it is far enough away from the origin (c) is the only point of equilibrium in the system (d) < is a globally asymptotically stable equilibrium point if it is close enough to the origin The given correct answer was (a) (but seems to be wrong) Can someone help me out to prove it formaly and also what is the reasoning to rule out the other ones? If it were just in n=1, I could just solve to find the equilibrium points and to find the intervals where the solution trajectory increases and the intervals where it decreases, but in n dimensions how do I deal with that ?** So so far I only have that the equilibrium points are and all the points in the hypersphere Edit : Even in the one-dimensional case my analysis is not making any sense with the  answer that is supposed to be correct: For n=1, the equilibrium points are Now analyzing the sign of the right-hand side of the ODE: Suppose is close enough to the origin,as option (a) states, that is , then , so I get the following diagram for the trajectories over the real line: ---->>>-1<<<< >>>>1<<<<---- , meaning that inside is unstable. What am doing wrong? Edit 2 So with the help of MatthewH, it seems like (a) is certanly wrong at least in the n=1 case I will continue the analysis at least in the n=1 case Now option (b) seems to be the correct one because if is far enough away from the origin, that is or . To fix the idea I consider : so I get the following diagram for the trajectories over the real line: ---->>>-1<<<<1>>>> <<<<---- , meaning that is a sink, that  is asymptotically stable: attractive and stable. Therefore option b is the correct one To rule out the other ones: Option c is not correct because there are infinite equilibrium points Option d is not correct : can't be globally asymptotically stable equilibrium point because when it is inside , solution curves with initial value outside of that interval can't reach it Does this seems ok? Can someone provide a solution in n dimensions?","\dot x =(|x|^2 -1)(\bar x -x) x \in \mathbb R^n, n\in \mathbb N, n\ge 1 \bar x \in \mathbb R^n \bar x \bar x \bar x \bar x (|x|^2 -1)(\bar x -x)=0  (|x|^2 -1)(\bar x -x)>0  (|x|^2 -1)(\bar x -x)<0  x=\bar x |x|=1 \bar x, 1,-1 \bar x  -1< \bar x < 1  (x^2-1)(\bar x- x)<0 \iff x \in (-1,\bar x)\cup(1,+\infty) \bar x \bar x \bar x \bar x>1 \bar x<-1 \bar x >1 (x^2-1)(\bar x- x)<0 \iff x \in (-1,1)\cup(\bar x,+\infty) \bar x \bar x \bar x (-1,1)","['ordinary-differential-equations', 'multivariable-calculus', 'dynamical-systems', 'stability-in-odes']"
53,Proof of Lyapunov instability theorem,Proof of Lyapunov instability theorem,,"I'm reading through my university notes and I've noticed we only proved the Lyapunov stability and asymptotical stability theorems, and haven't touched upon instability. I'm familiar with the concept of the Lyapunov function, I just find it a bit strange that even on the internet I haven't been able to find a detailed proof, or even the formal statement of the instability theorem. I'm wondering if anyone knows where can I find a detailed proof of the Lyapunov instability theorem on the internet. Thanks!","I'm reading through my university notes and I've noticed we only proved the Lyapunov stability and asymptotical stability theorems, and haven't touched upon instability. I'm familiar with the concept of the Lyapunov function, I just find it a bit strange that even on the internet I haven't been able to find a detailed proof, or even the formal statement of the instability theorem. I'm wondering if anyone knows where can I find a detailed proof of the Lyapunov instability theorem on the internet. Thanks!",,"['ordinary-differential-equations', 'lyapunov-functions']"
54,How to quickly generate an equilibrium point of a strange attractor numerically?,How to quickly generate an equilibrium point of a strange attractor numerically?,,"In many strange attractors (for example the Lorentz system , given appropriate parameter values), the point that is described by the system's equations of motion seems to approach a manifold with a dimensionality lower than that of the point as $t\to\infty$ . (For example, in the Lorentz system, the point has three dimensions as it is described by $x$ , $y$ and $z$ , but the manifold to which it eventually ends up arbitrarily close only seems to have two dimensions.) If the point is in this manifold, the system can be considered to be in a kind of equilibrium, as the point will never stray from this manifold as time progresses (similarly to how the entropy of a physical system in equilibrium will remain constant). My question is, is there some algorithm that can be used to quickly generate an equilibrium point of the system? I don't consider a stationary point an equilibrium point as it has a very low entropy (nor would I consider any other point with significantly lower entropy than the entropy of the manifold an equilibrium point). Obviously, the stationary point is also part of the manifold, but if our strategy is to just find a stationary point, we limit ourselves to a subset of the manifold, and since this subset contains much fewer points than what the entire manifold does, the entropy of the solutions that we find using this strategy becomes much smaller. One way to generate an equilibrium point is to simply simulate the trajectory of the point, using a numerical ODE solver, sufficiently far into the future. (In fact, a more rigorous way to define the set of equilibrium points may be as the limit set $\lim_{T\to\infty} \{\vec{u}(\vec{u}_0,t)\,|\,t\geq T,\, \vec{u}_0\in U \}$ , where $U$ is the set of all starting points that are well-behaved according to some appropriate criterion, or maybe it would even be enough if $U$ just was a set containing a single well-behaved starting point.) However, this feels very inefficient, and I'm looking for a much quicker way to get to equilibrium. I then asked myself: Can we device an equation that describes the manifold? That is, an equation that is true if and only if a given point lies within the manifold? If so, we could perhaps generate a point at random, outside of the manifold, and then either use use gradient decent or the nonlinear conjugate gradient method in order to force the value $(\text{LHS} - \text{RHS})^2$ to become zero by pushing the point more directly towards the manifold, and in that case this method could probably be used to reach an equilibrium point much faster. It would therefore also be nice to know whether we can describe the manifold by using an equation (rather than by using the much less practical limit set).","In many strange attractors (for example the Lorentz system , given appropriate parameter values), the point that is described by the system's equations of motion seems to approach a manifold with a dimensionality lower than that of the point as . (For example, in the Lorentz system, the point has three dimensions as it is described by , and , but the manifold to which it eventually ends up arbitrarily close only seems to have two dimensions.) If the point is in this manifold, the system can be considered to be in a kind of equilibrium, as the point will never stray from this manifold as time progresses (similarly to how the entropy of a physical system in equilibrium will remain constant). My question is, is there some algorithm that can be used to quickly generate an equilibrium point of the system? I don't consider a stationary point an equilibrium point as it has a very low entropy (nor would I consider any other point with significantly lower entropy than the entropy of the manifold an equilibrium point). Obviously, the stationary point is also part of the manifold, but if our strategy is to just find a stationary point, we limit ourselves to a subset of the manifold, and since this subset contains much fewer points than what the entire manifold does, the entropy of the solutions that we find using this strategy becomes much smaller. One way to generate an equilibrium point is to simply simulate the trajectory of the point, using a numerical ODE solver, sufficiently far into the future. (In fact, a more rigorous way to define the set of equilibrium points may be as the limit set , where is the set of all starting points that are well-behaved according to some appropriate criterion, or maybe it would even be enough if just was a set containing a single well-behaved starting point.) However, this feels very inefficient, and I'm looking for a much quicker way to get to equilibrium. I then asked myself: Can we device an equation that describes the manifold? That is, an equation that is true if and only if a given point lies within the manifold? If so, we could perhaps generate a point at random, outside of the manifold, and then either use use gradient decent or the nonlinear conjugate gradient method in order to force the value to become zero by pushing the point more directly towards the manifold, and in that case this method could probably be used to reach an equilibrium point much faster. It would therefore also be nice to know whether we can describe the manifold by using an equation (rather than by using the much less practical limit set).","t\to\infty x y z \lim_{T\to\infty} \{\vec{u}(\vec{u}_0,t)\,|\,t\geq T,\, \vec{u}_0\in U \} U U (\text{LHS} - \text{RHS})^2","['ordinary-differential-equations', 'limits', 'entropy', 'chaos-theory']"
55,Finite time explosion on the boundary of basin of attraction of ODE,Finite time explosion on the boundary of basin of attraction of ODE,,"Let $f: \mathbb{R}^n \to \mathbb{R}^n$ be locally Lipschitz and $f(0) = 0$ (so that $x = 0$ is an equilibrium). Consider ODE \begin{equation}    \frac{dx(t)}{dt} = f(x(t)), \quad x(0) = x_0 \in \mathbb{R}^n, \quad t \geq 0 \end{equation} and its unique solution $t \mapsto \varphi(t; x_0) \in \mathbb{R}^n$ w.r.t. initial condition $x_0$ . I am concerning about the global existence of the solution on the boundary $\partial B$ of the basin of attraction \begin{equation}    B := \big \{ x_0 \in \mathbb{R}^n : \varphi(t; x_0) \text{ exists } \forall t \geq 0 \text{ and } \lim_{t \to \infty} \varphi(t; x_0) = 0 \big \} \end{equation} Since it is well-known that $B$ is open, $\partial B \cap B = \varnothing$ . Therefore, it is possible that solution $t \mapsto \varphi(t ; x_0)$ starting at $x_0 \in \partial B$ explodes in finite time and does not exist thereafter (when $\partial B$ is unbounded). For unbounded $\partial B$ , can anyone prove that every solution $t \mapsto \varphi(t ; x_0)$ starting at $x_0 \in \partial B$ does not explode in finite time, or show a counter-example? Many thanks in advance.","Let be locally Lipschitz and (so that is an equilibrium). Consider ODE and its unique solution w.r.t. initial condition . I am concerning about the global existence of the solution on the boundary of the basin of attraction Since it is well-known that is open, . Therefore, it is possible that solution starting at explodes in finite time and does not exist thereafter (when is unbounded). For unbounded , can anyone prove that every solution starting at does not explode in finite time, or show a counter-example? Many thanks in advance.","f: \mathbb{R}^n \to \mathbb{R}^n f(0) = 0 x = 0 \begin{equation}
   \frac{dx(t)}{dt} = f(x(t)), \quad x(0) = x_0 \in \mathbb{R}^n, \quad t \geq 0
\end{equation} t \mapsto \varphi(t; x_0) \in \mathbb{R}^n x_0 \partial B \begin{equation}
   B := \big \{ x_0 \in \mathbb{R}^n : \varphi(t; x_0) \text{ exists } \forall t \geq 0 \text{ and } \lim_{t \to \infty} \varphi(t; x_0) = 0 \big \}
\end{equation} B \partial B \cap B = \varnothing t \mapsto \varphi(t ; x_0) x_0 \in \partial B \partial B \partial B t \mapsto \varphi(t ; x_0) x_0 \in \partial B","['real-analysis', 'ordinary-differential-equations', 'multivariable-calculus', 'stability-in-odes', 'stability-theory']"
56,Kneser theorem about the Klein bottle,Kneser theorem about the Klein bottle,,"I know that in $1923$ H. Kneser showed that a continuous flow in a Klein bottle without singular points has a periodic trajectory. The original article is this , but does anyone know another old or new proof of this result? I would really like to read this result, I tried to do it from your original article but the language is too complicated for me. I searched on the internet but found almost nothing about the proof.","I know that in H. Kneser showed that a continuous flow in a Klein bottle without singular points has a periodic trajectory. The original article is this , but does anyone know another old or new proof of this result? I would really like to read this result, I tried to do it from your original article but the language is too complicated for me. I searched on the internet but found almost nothing about the proof.",1923,"['ordinary-differential-equations', 'manifolds', 'smooth-manifolds', 'vector-fields', 'klein-bottle']"
57,Steady state density of Langevin SDE,Steady state density of Langevin SDE,,"Suppose we have the SDE $$dX_t = -U’(X_t) dt+\sqrt{2} D dW_t$$ where $D>0$ . This has Fokker-Planck equation $$\partial_t p =-\partial_x \left[-U’(x) p(t,x)\right]+\partial_{xx} \left[D^2 p(t,x)\right]$$ and to find the steady state density $$f(x)=\lim_{t\to \infty} p(t,x)$$ we take the limit as $t\to \infty$ and assume $\partial_t p(t,x)\to 0$ . This yields the ODE $$\frac{d}{dx} \left[ U’(x) f(x)\right] +D^2 f’’(x)=0.$$ My question How do we solve this ODE to show the steady state is given by $$f(x) \propto e^{-U(x)/D^2} ?$$ My attempt Note we can “factor” out a derivative to write $$\frac{d}{dx} \left[ U’(x)f(x)+D^2 f’(x) \right]=0$$ which implies the expression inside the parentheses must be constant, i.e. $$D^2 f’(x) +U’(x) f(x) =C$$ and we can solve this using the integrating factor method, so that $$f(x)= C_1 e^{-U(x)/D^2}+\frac{C}{D^2} e^{-U(x)/D^2} \int e^{-U(x)/D^2} dx$$ How do I conclude that $C=0$ from here? For we must have $\int_{\mathbb{R}} f(x) dx <\infty$ so we can normalize, a la Boltzmann statistics, but I can’t see the justification. Or perhaps I made a mistake. An alternative derivation is okay but I would prefer to see how this is corrected/finished. Thank you.","Suppose we have the SDE where . This has Fokker-Planck equation and to find the steady state density we take the limit as and assume . This yields the ODE My question How do we solve this ODE to show the steady state is given by My attempt Note we can “factor” out a derivative to write which implies the expression inside the parentheses must be constant, i.e. and we can solve this using the integrating factor method, so that How do I conclude that from here? For we must have so we can normalize, a la Boltzmann statistics, but I can’t see the justification. Or perhaps I made a mistake. An alternative derivation is okay but I would prefer to see how this is corrected/finished. Thank you.","dX_t = -U’(X_t) dt+\sqrt{2} D dW_t D>0 \partial_t p =-\partial_x \left[-U’(x) p(t,x)\right]+\partial_{xx} \left[D^2 p(t,x)\right] f(x)=\lim_{t\to \infty} p(t,x) t\to \infty \partial_t p(t,x)\to 0 \frac{d}{dx} \left[ U’(x) f(x)\right] +D^2 f’’(x)=0. f(x) \propto e^{-U(x)/D^2} ? \frac{d}{dx} \left[ U’(x)f(x)+D^2 f’(x) \right]=0 D^2 f’(x) +U’(x) f(x) =C f(x)= C_1 e^{-U(x)/D^2}+\frac{C}{D^2} e^{-U(x)/D^2} \int e^{-U(x)/D^2} dx C=0 \int_{\mathbb{R}} f(x) dx <\infty","['ordinary-differential-equations', 'partial-differential-equations']"
58,Time evolution and matrix ODE,Time evolution and matrix ODE,,"Let $M$ be a linear operator on a finite-dimensional complex Hilbert space (thus, $M$ is just a matrix). Assume that $\text{Tr}M = 1$ , $M$ is self-adjoint and $M \ge 0$ (positive semi-definite). In quantum mechanics, one postulates that every such $M$ might depend on $t$ , in which case it satisfies the following ODE: $$\frac{dM(t)}{dt} = -i[H, M] \tag{1} \label{1}$$ where $H$ is a fixed self-adjoint operator (the Hamiltonian operator) and $[\cdot, \cdot]$ is the commutator $[A,B] := AB-BA$ . I have two questions about this equation. First Question: I am considering $M$ as a bounded linear operator on an underlying Hilbert space with norm operator. A natural way to define $dM(t)/dt$ is as follows. Given $\varepsilon > 0$ , there exists $\delta > 0$ and an operator $L = L(t)$ such that if $0 < |h| < \delta$ then $$\bigg{\|} \frac{M(t+h)-M(t)}{h}-L(t)\bigg{\|} = \bigg{\|} \frac{M(h)}{h}-L(t)\bigg{\|} < \varepsilon$$ in which case $L(t) := dM(t)/dt$ . On the other hand, we could interpret (\ref{1}) as a collection of ODEs given by the entries of $M$ , that is, if $M_{ij}$ is the $ij$ -th entry of $M$ then (\ref{1}) means: $$\frac{dM_{ij}(t)}{dt} = -i(H_{ij}M_{ij}-M_{ij}H_{ij}) \tag{2}\label{2}$$ Are these two interpretations equivalent? Second Question: If $M$ is fixed and we define $M(t) := e^{-iHt}Me^{iHt}$ , we see that $M(t)$ satisfies (\ref{1}) with initial condition $M(0) = M$ . Is it a unique solution? If we interpret (\ref{1}) as being (\ref{2}), then the answer seems to be a yes. However, I don't know any uniqueness results if (\ref{1}) is treated as an ODE for operators (or elements of a Banach space).","Let be a linear operator on a finite-dimensional complex Hilbert space (thus, is just a matrix). Assume that , is self-adjoint and (positive semi-definite). In quantum mechanics, one postulates that every such might depend on , in which case it satisfies the following ODE: where is a fixed self-adjoint operator (the Hamiltonian operator) and is the commutator . I have two questions about this equation. First Question: I am considering as a bounded linear operator on an underlying Hilbert space with norm operator. A natural way to define is as follows. Given , there exists and an operator such that if then in which case . On the other hand, we could interpret (\ref{1}) as a collection of ODEs given by the entries of , that is, if is the -th entry of then (\ref{1}) means: Are these two interpretations equivalent? Second Question: If is fixed and we define , we see that satisfies (\ref{1}) with initial condition . Is it a unique solution? If we interpret (\ref{1}) as being (\ref{2}), then the answer seems to be a yes. However, I don't know any uniqueness results if (\ref{1}) is treated as an ODE for operators (or elements of a Banach space).","M M \text{Tr}M = 1 M M \ge 0 M t \frac{dM(t)}{dt} = -i[H, M] \tag{1} \label{1} H [\cdot, \cdot] [A,B] := AB-BA M dM(t)/dt \varepsilon > 0 \delta > 0 L = L(t) 0 < |h| < \delta \bigg{\|} \frac{M(t+h)-M(t)}{h}-L(t)\bigg{\|} = \bigg{\|} \frac{M(h)}{h}-L(t)\bigg{\|} < \varepsilon L(t) := dM(t)/dt M M_{ij} ij M \frac{dM_{ij}(t)}{dt} = -i(H_{ij}M_{ij}-M_{ij}H_{ij}) \tag{2}\label{2} M M(t) := e^{-iHt}Me^{iHt} M(t) M(0) = M","['linear-algebra', 'functional-analysis', 'ordinary-differential-equations', 'mathematical-physics', 'quantum-mechanics']"
59,Modified Gronwall lemma for $f' \le a + b f^\alpha$,Modified Gronwall lemma for,f' \le a + b f^\alpha,"Let $f\colon [0,\infty)\to [0,\infty)$ be a smooth function such that $f(0)=0$ and \begin{equation} f'(t) \le a(t)+b(t) \bigl( f(t) \bigr)^\alpha \label{eq:1} \tag{1} \end{equation} for all $t\ge 0$ , some $\alpha\in(0,1)$ and some functions $a,b\colon[0,\infty)\to(0,\infty)$ . Question: What (possibly sharp) bound on $f$ can be derived in terms of $a,b$ and $\alpha$ ? For my problem, we can wlog. assume that $a$ and $b$ are non-decreasing, but the answer itself might be interesting in generality. My attempt If $a\equiv 0$ , then we can reduce the problem to the linear setting and apply the standard Gronwall lemma. Namely, \eqref{eq:1} is then equivalent to $(f^{1-\alpha}(t))'\le (1-\alpha)b(t)$ , whence \begin{equation}f(t)\le \bigl( (1-\alpha)\int_0^t b(s)\,ds \bigr)^{1/(1-\alpha)}.\label{eq:2}\tag{2}\end{equation} Also, we could estimate brutally $x^\alpha\le x$ for $x\ge 1$ and $f(t)\le f(t)+1$ to get that \eqref{eq:1} implies $f'(t) \le a(t)+b(t)+b(t)f(t)$ , whence by the Gronwall lemma $$ f(t) \le \int_0^t (a(s)+b(s))e^{\int_s^t b(u)\,du}\,ds. $$ This estimate however does not take $\alpha$ into considerations and seems way off (compare e.g. with \eqref{eq:2} in case $a\equiv 0$ ).","Let be a smooth function such that and for all , some and some functions . Question: What (possibly sharp) bound on can be derived in terms of and ? For my problem, we can wlog. assume that and are non-decreasing, but the answer itself might be interesting in generality. My attempt If , then we can reduce the problem to the linear setting and apply the standard Gronwall lemma. Namely, \eqref{eq:1} is then equivalent to , whence Also, we could estimate brutally for and to get that \eqref{eq:1} implies , whence by the Gronwall lemma This estimate however does not take into considerations and seems way off (compare e.g. with \eqref{eq:2} in case ).","f\colon [0,\infty)\to [0,\infty) f(0)=0 \begin{equation}
f'(t) \le a(t)+b(t) \bigl( f(t) \bigr)^\alpha
\label{eq:1}
\tag{1}
\end{equation} t\ge 0 \alpha\in(0,1) a,b\colon[0,\infty)\to(0,\infty) f a,b \alpha a b a\equiv 0 (f^{1-\alpha}(t))'\le (1-\alpha)b(t) \begin{equation}f(t)\le \bigl( (1-\alpha)\int_0^t b(s)\,ds \bigr)^{1/(1-\alpha)}.\label{eq:2}\tag{2}\end{equation} x^\alpha\le x x\ge 1 f(t)\le f(t)+1 f'(t) \le a(t)+b(t)+b(t)f(t)  f(t) \le \int_0^t (a(s)+b(s))e^{\int_s^t b(u)\,du}\,ds.  \alpha a\equiv 0","['real-analysis', 'ordinary-differential-equations', 'functional-inequalities']"
60,Attractive problems in Differential Geometry for a talk [closed],Attractive problems in Differential Geometry for a talk [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 2 years ago . Improve this question I am preparing a presentation of Differential Geometry aimed to people with moderate knowledge of Mathematics (think about highschool students). I would like to find some concepts or applications of Differential Geometry that are easy to explain and attractive to an audience that may not have heard of the field. My ideas: Speak about the brachistocrone curve. This is a problem both easy to explain and with an interesting historical context, so I think it is ideal for a presentation like this. Egregious Gaussian theorem. Also easy to explain and surprising (I won't be defining formally the concepts of course) I would prefer examples about curves if possible, although every idea is welcome. Any reference is great also, I have been consulting Differential equations with historical notes by Simmons, which provides a very beautiful solution to the brachistocrone problem, based in Snell law. Thanks in advance.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 2 years ago . Improve this question I am preparing a presentation of Differential Geometry aimed to people with moderate knowledge of Mathematics (think about highschool students). I would like to find some concepts or applications of Differential Geometry that are easy to explain and attractive to an audience that may not have heard of the field. My ideas: Speak about the brachistocrone curve. This is a problem both easy to explain and with an interesting historical context, so I think it is ideal for a presentation like this. Egregious Gaussian theorem. Also easy to explain and surprising (I won't be defining formally the concepts of course) I would prefer examples about curves if possible, although every idea is welcome. Any reference is great also, I have been consulting Differential equations with historical notes by Simmons, which provides a very beautiful solution to the brachistocrone problem, based in Snell law. Thanks in advance.",,"['ordinary-differential-equations', 'differential-geometry', 'soft-question']"
61,Using generating functions to construct or solve differential equations,Using generating functions to construct or solve differential equations,,"I know that $T_n(x)$ is the solution of the differential equation $(1-x^2)y''-xy'+n^2y=0$ , where $$ T_n(x)=\begin{cases}  T_n(x)=1 & \text{if $n=0$}\\ T_n(x)=x & \text{if $n=1$}\\ T_{n}(x)=2xT_{n-1}(x)- T_{n-2}(x) & \text{if $n\geq 2$}\\ \end{cases} $$ this can be proved using power series ( https://en.wikipedia.org/wiki/Chebyshev_equation ). I was wondering if there is a way to go ""backwards"", given any recurrence (as a generating function) $f_n$ , can I construct a differential equation such that $f_n$ is a solution of the constructed equation?. For example, we know that $$T_n(x)=\frac{(x+\sqrt{x^2-1})^n+(x-\sqrt{x^2-1})^n} {2},$$ How can I construct $(1-x^2)y''-xy'+n^2y=0$ given that $y(x)=\frac{(x+\sqrt{x^2-1})^n+(x-\sqrt{x^2-1})^n} {2}$ is a solution of that equation?. Thanks in advance.","I know that is the solution of the differential equation , where this can be proved using power series ( https://en.wikipedia.org/wiki/Chebyshev_equation ). I was wondering if there is a way to go ""backwards"", given any recurrence (as a generating function) , can I construct a differential equation such that is a solution of the constructed equation?. For example, we know that How can I construct given that is a solution of that equation?. Thanks in advance.","T_n(x) (1-x^2)y''-xy'+n^2y=0 
T_n(x)=\begin{cases} 
T_n(x)=1 & \text{if n=0}\\
T_n(x)=x & \text{if n=1}\\
T_{n}(x)=2xT_{n-1}(x)- T_{n-2}(x) & \text{if n\geq 2}\\
\end{cases}
 f_n f_n T_n(x)=\frac{(x+\sqrt{x^2-1})^n+(x-\sqrt{x^2-1})^n} {2}, (1-x^2)y''-xy'+n^2y=0 y(x)=\frac{(x+\sqrt{x^2-1})^n+(x-\sqrt{x^2-1})^n} {2}","['ordinary-differential-equations', 'recurrence-relations', 'intuition', 'constructive-mathematics', 'chebyshev-polynomials']"
62,Differential equations with a derivative in the exponent,Differential equations with a derivative in the exponent,,I would like to know if there is a method to find solutions to ODEs where the derivative is in the exponent. For example: $y^{y'}=y''$ Thanks!,I would like to know if there is a method to find solutions to ODEs where the derivative is in the exponent. For example: Thanks!,y^{y'}=y'',['ordinary-differential-equations']
63,Solving an ODE and determining limiting value,Solving an ODE and determining limiting value,,"Context: This is from Chapter 3 of Christodoulou and Klainerman's stability of Minkowski space. We have a family of metrics $m_u$ on $S^2$ for each $u \in (u_0, \infty)$ (for some fixed $u_0 < 0$ ). We also define, separately, $m_\infty$ to be the standard unit round metric on $S^2$ . The goal is to prove, in an appropriate sense, that $(S^2, m_u)$ converges to $(S^2, m_\infty)$ (under some additional assumptions, the most geometrically illuminating of which is that $K(u) \to 1$ as $u \to \infty$ , where $K(u)$ is the Gaussian curvature of $m_u$ . Let $(e_A)_{A = 1, 2}$ be an $m_\infty$ -ONB such that the matrix $m_u(e_A, e_B)$ is diagonal, with smallest eigenvalue $\lambda(u)$ and largest eigenvalue $\Lambda(u)$ . The goal is to show that $\lambda(u), \Lambda(u) \to 1$ as $u \to \infty$ . Notation: We let $r(u)$ be the ""radius"" of $(S^2, m_u)$ , defined by the formula $$ \text{Area}(S^2, m_u) = 4\pi r(u)^2. $$ Define $\mu(u) = \sqrt{\det_{m_\infty} m_u(e_A, e_B)} = \sqrt{\lambda(u)\Lambda(u)}$ . Assumptions: 1) The following integral is finite, and uniformly bounded for all values of $u$ : $$ \int_u^\infty r(u')^{-1} \kappa(u')\, du'. $$ Here $\kappa$ is some nonnegative scalar function. One can derive the following ODE for $\mu$ : $$ \partial_u \mu(u) = r^{-1}\kappa \mu. $$ C-K then say that $$ \mu(u) = \exp(-\int_u^\infty r(u')^{-1}\kappa(u')\, du'). $$ My question: There should be a constant out front corresponding to the ""initial"" value $\lim_{u \to \infty}\mu(u)$ . Hence it appears they are saying it is 1. I don't see how this is possible, as this seems to be what we are trying to prove?","Context: This is from Chapter 3 of Christodoulou and Klainerman's stability of Minkowski space. We have a family of metrics on for each (for some fixed ). We also define, separately, to be the standard unit round metric on . The goal is to prove, in an appropriate sense, that converges to (under some additional assumptions, the most geometrically illuminating of which is that as , where is the Gaussian curvature of . Let be an -ONB such that the matrix is diagonal, with smallest eigenvalue and largest eigenvalue . The goal is to show that as . Notation: We let be the ""radius"" of , defined by the formula Define . Assumptions: 1) The following integral is finite, and uniformly bounded for all values of : Here is some nonnegative scalar function. One can derive the following ODE for : C-K then say that My question: There should be a constant out front corresponding to the ""initial"" value . Hence it appears they are saying it is 1. I don't see how this is possible, as this seems to be what we are trying to prove?","m_u S^2 u \in (u_0, \infty) u_0 < 0 m_\infty S^2 (S^2, m_u) (S^2, m_\infty) K(u) \to 1 u \to \infty K(u) m_u (e_A)_{A = 1, 2} m_\infty m_u(e_A, e_B) \lambda(u) \Lambda(u) \lambda(u), \Lambda(u) \to 1 u \to \infty r(u) (S^2, m_u) 
\text{Area}(S^2, m_u) = 4\pi r(u)^2.
 \mu(u) = \sqrt{\det_{m_\infty} m_u(e_A, e_B)} = \sqrt{\lambda(u)\Lambda(u)} u 
\int_u^\infty r(u')^{-1} \kappa(u')\, du'.
 \kappa \mu 
\partial_u \mu(u) = r^{-1}\kappa \mu.
 
\mu(u) = \exp(-\int_u^\infty r(u')^{-1}\kappa(u')\, du').
 \lim_{u \to \infty}\mu(u)","['ordinary-differential-equations', 'differential-geometry', 'partial-differential-equations', 'general-relativity']"
64,Solving a second order differential equation to obtain trajectories,Solving a second order differential equation to obtain trajectories,,"I don't know if this is appropriate to ask so please let me know if I should keep this post or not. While calculating some accelerated trajectories, i encountered a rather tedious differential equation of the form, $$  \frac{da^0}{d\tau} + \frac{\Big(\mu - \frac{GQ^2}{r}  \Big) }{GQ^2 + r^2 - 2r\mu} (u^0 a^1 + u^1 a^0) = \vert a \vert^2 u^0$$ where, $u^i$ is a 4-vector of velocity (dependent on $\tau$ ), $G$ , $Q$ can be treated as constants along with $\mu$ and $r$ is a variable. Here, $a^i$ is the acceleration 4-vector and the expression for which is, $$a^0 = \frac{du^0}{d\tau} + \frac{2\Big(\mu - \frac{GQ^2}{r}  \Big) }{GQ^2 + r^2 - 2r\mu} (u^1 u^0)\\ a^1 = \frac{du^1}{d\tau} - \frac {(Mr-Q^{2})}{r^{2}-2Mr+Q^{2}}(u^0)^2 + \frac {Q^{2}-Mr}{Q^{2}r-2Mr^{2}+r^{3}} (u^1)^2$$ So, after combining these equations (where $\vert a \vert^2$ is also a constant ) and substituting the later expressions into the former equation,we get a long differential equation like, $$\frac{d}{d\tau} \Big(\frac{du^0}{d\tau} + \frac{2\Big(\mu - \frac{GQ^2}{r}  \Big) }{GQ^2 + r^2 - 2r\mu} (u^1 u^0)\Big) +  \frac{\Big(\mu - \frac{GQ^2}{r}  \Big) }{GQ^2 + r^2 - 2r\mu} \Bigg(u^0 \Big( \frac{du^1}{d\tau} - \frac {(Mr-Q^{2})}{r^{2}-2Mr+Q^{2}}(u^0)^2 + \frac {Q^{2}-Mr}{Q^{2}r-2Mr^{2}+r^{3}} (u^1)^2\Big) + u^1 \Big(\frac{du^0}{d\tau} + \frac{2\Big(\mu - \frac{GQ^2}{r}  \Big) }{GQ^2 + r^2 - 2r\mu} (u^1 u^0)\Big)\Bigg) = \vert a \vert^2 u^0 $$ which I am not getting anywhere with. Any help/clue would be greatly appreciated. Thanks! EDIT: I think the additional information about the $a^1$ equation could be needed. $$  \frac{da^1}{d\tau} =  \vert a \vert^2 u^1$$","I don't know if this is appropriate to ask so please let me know if I should keep this post or not. While calculating some accelerated trajectories, i encountered a rather tedious differential equation of the form, where, is a 4-vector of velocity (dependent on ), , can be treated as constants along with and is a variable. Here, is the acceleration 4-vector and the expression for which is, So, after combining these equations (where is also a constant ) and substituting the later expressions into the former equation,we get a long differential equation like, which I am not getting anywhere with. Any help/clue would be greatly appreciated. Thanks! EDIT: I think the additional information about the equation could be needed.","  \frac{da^0}{d\tau} + \frac{\Big(\mu - \frac{GQ^2}{r}  \Big) }{GQ^2 + r^2 - 2r\mu} (u^0 a^1 + u^1 a^0) = \vert a \vert^2 u^0 u^i \tau G Q \mu r a^i a^0 = \frac{du^0}{d\tau} + \frac{2\Big(\mu - \frac{GQ^2}{r}  \Big) }{GQ^2 + r^2 - 2r\mu} (u^1 u^0)\\
a^1 = \frac{du^1}{d\tau} - \frac {(Mr-Q^{2})}{r^{2}-2Mr+Q^{2}}(u^0)^2 + \frac {Q^{2}-Mr}{Q^{2}r-2Mr^{2}+r^{3}} (u^1)^2 \vert a \vert^2 \frac{d}{d\tau} \Big(\frac{du^0}{d\tau} + \frac{2\Big(\mu - \frac{GQ^2}{r}  \Big) }{GQ^2 + r^2 - 2r\mu} (u^1 u^0)\Big) +  \frac{\Big(\mu - \frac{GQ^2}{r}  \Big) }{GQ^2 + r^2 - 2r\mu} \Bigg(u^0 \Big( \frac{du^1}{d\tau} - \frac {(Mr-Q^{2})}{r^{2}-2Mr+Q^{2}}(u^0)^2 + \frac {Q^{2}-Mr}{Q^{2}r-2Mr^{2}+r^{3}} (u^1)^2\Big) + u^1 \Big(\frac{du^0}{d\tau} + \frac{2\Big(\mu - \frac{GQ^2}{r}  \Big) }{GQ^2 + r^2 - 2r\mu} (u^1 u^0)\Big)\Bigg) = \vert a \vert^2 u^0  a^1   \frac{da^1}{d\tau} =  \vert a \vert^2 u^1","['ordinary-differential-equations', 'physics']"
65,"General solution of a second order, non-homogeneous ODE [closed]","General solution of a second order, non-homogeneous ODE [closed]",,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question How might I go about computing the general solution (i.e. with an arbitrary initial condition) of $$tu''−(t+1)u'+u=t^2$$ where $u'= \frac{du}{dt}$ and $u''= \frac{d^2u}{dt^2}$ I am a bit confused. Can anyone give me the exact solution?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question How might I go about computing the general solution (i.e. with an arbitrary initial condition) of where and I am a bit confused. Can anyone give me the exact solution?",tu''−(t+1)u'+u=t^2 u'= \frac{du}{dt} u''= \frac{d^2u}{dt^2},['ordinary-differential-equations']
66,Why are there two possible values for the degree of the ODE $\frac{\mathrm{d}^3}{\mathrm{dx^3}}(\frac{\mathrm{d}^2y}{\mathrm{dx^2}})^{-3/2}=0$?,Why are there two possible values for the degree of the ODE ?,\frac{\mathrm{d}^3}{\mathrm{dx^3}}(\frac{\mathrm{d}^2y}{\mathrm{dx^2}})^{-3/2}=0,"I want to find the order and degree of the ODE $\frac{\mathrm{d}^3}{\mathrm{dx^3}}(\frac{\mathrm{d}^2y}{\mathrm{dx^2}})^{-3/2}=0$ ? I'm getting two different values of the degree of the ODE. Simplifying gives: $-\frac 52 (y'')^{-\frac 72}(y''')^2+y'''' (y'')^{-5/2}=0$ We have two possible continuations from here. Continuation $1$ : Multiplying both sides by $(y'')^{\frac 72}$ yields: $-\frac 52 (y''')^2+\color{blue}{y''''}y''=0$ . From here, the order is $4$ and the degree is $1$ . Continuation $2$ : $-\frac 52 (y'')^{-\frac 72}(y''')^2+y'''' (y'')^{-5/2}=0\implies\frac {25}4(y''')^4(y'')^{-7}=(\color{blue}{y''''})^2 (y'')^{-5}$ and multiplying both sides by $(y'')^{-7}$ suggests that the ode has order 4 and the degree equal to $2$ . So which one of the apparently two possible degrees is correct? Please help. Thanks.","I want to find the order and degree of the ODE ? I'm getting two different values of the degree of the ODE. Simplifying gives: We have two possible continuations from here. Continuation : Multiplying both sides by yields: . From here, the order is and the degree is . Continuation : and multiplying both sides by suggests that the ode has order 4 and the degree equal to . So which one of the apparently two possible degrees is correct? Please help. Thanks.",\frac{\mathrm{d}^3}{\mathrm{dx^3}}(\frac{\mathrm{d}^2y}{\mathrm{dx^2}})^{-3/2}=0 -\frac 52 (y'')^{-\frac 72}(y''')^2+y'''' (y'')^{-5/2}=0 1 (y'')^{\frac 72} -\frac 52 (y''')^2+\color{blue}{y''''}y''=0 4 1 2 -\frac 52 (y'')^{-\frac 72}(y''')^2+y'''' (y'')^{-5/2}=0\implies\frac {25}4(y''')^4(y'')^{-7}=(\color{blue}{y''''})^2 (y'')^{-5} (y'')^{-7} 2,"['real-analysis', 'calculus', 'ordinary-differential-equations']"
67,Uniqueness of non-autonomous ODE,Uniqueness of non-autonomous ODE,,"From Picard-Lindelöf Theorem it is well known that, roughly speaking, an ODE $\dot x= f(x)$ , with continously differentiable or Lipschitz continous right-hand side (RHS) has a unique solution (in some interval). What I struggle with is the following: For a given non-autonomous RHS $f(x,d)$ , e.g. $f=d(t)x^2+1$ with continous function $d(\cdot)$ , I can locally obtain the following: $|f(x,d)-f(y,d)|\leq L |d(t)||x-y|$ .Is the assertion about uniqueness of the solution still valid for a non-uniform Lipschitz constant $L d$ ?","From Picard-Lindelöf Theorem it is well known that, roughly speaking, an ODE , with continously differentiable or Lipschitz continous right-hand side (RHS) has a unique solution (in some interval). What I struggle with is the following: For a given non-autonomous RHS , e.g. with continous function , I can locally obtain the following: .Is the assertion about uniqueness of the solution still valid for a non-uniform Lipschitz constant ?","\dot x= f(x) f(x,d) f=d(t)x^2+1 d(\cdot) |f(x,d)-f(y,d)|\leq L |d(t)||x-y| L d","['functional-analysis', 'ordinary-differential-equations']"
68,Proving single solution of initial value problem is increasing,Proving single solution of initial value problem is increasing,,"Given the initial value problem $$ y'(x)=y(x)-\sin{y(x)}, y(0)=1 $$ I need to prove that there is a solution defined on $\mathbb{R}$ and that the solution, $u(x)$ , is an increasing function where $\lim_{x\to -\infty}{u(x)=0}$ The first part is quite easy, let $f(x, y) = y - \sin{y}$ $$f_y(x, y)=1-\cos{y}\Longrightarrow \left|f_y(x,y)\right| \le 2$$ hence $f(x, y)$ is Lipschitz continuous on $y$ for every $(x, y)\in (-\infty, \infty)\times (-\infty, \infty)$ therefore the initial value problem has a single solution on $\mathbb{R}$ I know that the solution, $u(x)$ has the form $$u(x)=y_0+\int_{x_0}^x{f(t, u(t))dt}=1+\int_0^x{(u(t)-\sin{u(t))}dt}$$ and also $$u(x)=\lim_{n\to\infty}u_n(x)$$ where $$u_n(x)=y_0+\int_{x_0}^x{f(t, u_{n-1}(t))}dt=1+\int_0^x{(u_{n-1}-\sin{(u_{n-1})})}dt,\space\space\space u_0(x)=y_0=1$$ But I'm wasn't able to find a way to prove the solution is increasing and has the desired limit. EDIT: I got a hint to look at $y\equiv 0$ EDIT2: Let assume that the solution, $u(x)$ is decreasing at $(x_1, u(x_1))$ because $u(x)$ is continuous, there must be $(x_2, u(x_2)$ where $u'(x_2)=0=u(x_2)-\sin{u(x_2)}\Longrightarrow u(x_2)=0$ now if I look at the initial value problem $$y'(x)=y(x)-\sin{y(x)}, y(x_2)=0$$ I can prove that it has a single solution in $\mathbb{R}$ like I already did, and $u(x)$ is my solution, but $u_1(x)\equiv 0$ is also a solution to this problem, contradiction, hence $u'(x)>0$ for $x\in\mathbb{R}$ , i.e $u(x)$ is increasing. But I still don't know how I can show the limit at $-\infty$","Given the initial value problem I need to prove that there is a solution defined on and that the solution, , is an increasing function where The first part is quite easy, let hence is Lipschitz continuous on for every therefore the initial value problem has a single solution on I know that the solution, has the form and also where But I'm wasn't able to find a way to prove the solution is increasing and has the desired limit. EDIT: I got a hint to look at EDIT2: Let assume that the solution, is decreasing at because is continuous, there must be where now if I look at the initial value problem I can prove that it has a single solution in like I already did, and is my solution, but is also a solution to this problem, contradiction, hence for , i.e is increasing. But I still don't know how I can show the limit at","
y'(x)=y(x)-\sin{y(x)}, y(0)=1
 \mathbb{R} u(x) \lim_{x\to -\infty}{u(x)=0} f(x, y) = y - \sin{y} f_y(x, y)=1-\cos{y}\Longrightarrow \left|f_y(x,y)\right| \le 2 f(x, y) y (x, y)\in (-\infty, \infty)\times (-\infty, \infty) \mathbb{R} u(x) u(x)=y_0+\int_{x_0}^x{f(t, u(t))dt}=1+\int_0^x{(u(t)-\sin{u(t))}dt} u(x)=\lim_{n\to\infty}u_n(x) u_n(x)=y_0+\int_{x_0}^x{f(t, u_{n-1}(t))}dt=1+\int_0^x{(u_{n-1}-\sin{(u_{n-1})})}dt,\space\space\space u_0(x)=y_0=1 y\equiv 0 u(x) (x_1, u(x_1)) u(x) (x_2, u(x_2) u'(x_2)=0=u(x_2)-\sin{u(x_2)}\Longrightarrow u(x_2)=0 y'(x)=y(x)-\sin{y(x)}, y(x_2)=0 \mathbb{R} u(x) u_1(x)\equiv 0 u'(x)>0 x\in\mathbb{R} u(x) -\infty","['ordinary-differential-equations', 'initial-value-problems']"
69,Solve $X''(t) + f(g(t)) X(t) = 0$ using the solutions of $X''(t) + f(t)X(t) = 0$,Solve  using the solutions of,X''(t) + f(g(t)) X(t) = 0 X''(t) + f(t)X(t) = 0,"I want to solve the ODE $$X''(t) + f(g(t)) X(t) = 0$$ And I know the solutions of $$X''(t) + f(t) X(t) =0$$ Is there a way I can find the solutions of the first ODE using the second? I think I can't in general, they seem to be completely different problems even though the equations are similar. But for $g(t) = t$ , the solutions are the same, obviously. Or if $g(t) = -t$ , then $\varphi(-t)$ is a solution for the first if $\varphi(t)$ is a solution for the second. So its possible in some cases. I would like to know if it is possible to do the same for other $g$ . I notice if $F(r,s) = A(r)B(s)$ , where $A$ is a solution to the first ODE and $B$ is a solution to the second, then $F_{rr}(t,g(t)) = F_{ss}(t,g(t))$ . I don't know if this helps. Any ideas/hints/references are welcome (I am a newbie in differential equations). Thanks","I want to solve the ODE And I know the solutions of Is there a way I can find the solutions of the first ODE using the second? I think I can't in general, they seem to be completely different problems even though the equations are similar. But for , the solutions are the same, obviously. Or if , then is a solution for the first if is a solution for the second. So its possible in some cases. I would like to know if it is possible to do the same for other . I notice if , where is a solution to the first ODE and is a solution to the second, then . I don't know if this helps. Any ideas/hints/references are welcome (I am a newbie in differential equations). Thanks","X''(t) + f(g(t)) X(t) = 0 X''(t) + f(t) X(t) =0 g(t) = t g(t) = -t \varphi(-t) \varphi(t) g F(r,s) = A(r)B(s) A B F_{rr}(t,g(t)) = F_{ss}(t,g(t))","['ordinary-differential-equations', 'partial-differential-equations']"
70,"What is the formal, rigorous definition of a differential equation?","What is the formal, rigorous definition of a differential equation?",,"In college classes, we solve differential equations. However, I have never seen a book give a rigorous definition of what a differential equation actually is. Can someone give me a rigorous definition of a differential equation? I want a definition in terms of set theory.","In college classes, we solve differential equations. However, I have never seen a book give a rigorous definition of what a differential equation actually is. Can someone give me a rigorous definition of a differential equation? I want a definition in terms of set theory.",,"['ordinary-differential-equations', 'definition']"
71,Reference Request: Explicit formula for the exponential of a triangular matrix in terms of convolutions,Reference Request: Explicit formula for the exponential of a triangular matrix in terms of convolutions,,"In this previous question it came up that there's a pretty explicit formula for the exponential of a triangular matrix. In the below, let $*$ denote convolution and define $$ G_{\lambda_i}(t) = \begin{cases} \exp(\lambda_i t) & t \geq 0 \\ 0 & \text{otherwise.} \end{cases} $$ Then, given a lower triangular matrix $$ L = \begin{pmatrix} \lambda_1 &  &  &  \\ l_{21} & \lambda_2 &  & \\ \vdots& & \ddots & \\ l_{n1}&l_{n2} &\dots &\lambda_n \end{pmatrix} $$ for $t \geq 0$ , the $(a,b)$ entry of the matrix exponential is $$ \left(\exp(tL)\right)_{ab} = \sum_{a = i_1 > i_2 > \dots > i_k = b} l_{i_1i_2}\cdots l_{i_{k-1}i_k} (G_{\lambda_{i_1}} * \dots * G_{\lambda_{i_k}})(t). $$ For example, $$ \left(\exp(tL)\right)_{31} = l_{31} (G_{\lambda_3} * G_{\lambda_1})(t) + l_{32} l_{21} (G_{\lambda_3} * G_{\lambda_2} * G_{\lambda_1})(t). $$ This is fairly straightforward to see by induction. We're building up a solution to a differential equation $d/dt \exp(tA) = A \exp(tA)$ . At each induction step there is an inhomogenous linear equation for the new entries, whose homogenous part has Green's function $G_{\lambda_n}$ , so the inhomogenous solution is a combination of convolutions of the previous bits with that. I suspect there's also some combinatorial interpretation and argument. Probably something in terms of Laplace transforms too. This is a bit different from the way explicit formulas for exponentials of matrices are usually discussed: Usually, we reduce things all the way to Jordan blocks rather than merely triangular matrices. Usually, we decompose $G_{\lambda} * G_{\mu}$ into a linear combination $(\lambda - \mu)^{-1} G_\lambda - (\lambda - \mu)^{-1} G_\mu$ , and handle the $\lambda = \mu$ case separately: $(G_\lambda * G_\lambda)(t) = t G_\lambda(t).$ (Avoiding both of these was useful for the previous question linked above; the goal was to find uniform bounds, and both the JNF and the decomposed-into-linear-combinations expressions are sensitive to small changes in the matrix entries.) This direct formulation in terms of convolutions of Green's functions feels like a ""textbook"" result from some textbook somewhere, but I'm not familiar enough with the topic to know where. Does anyone have a reference in the literature that states more or less this?","In this previous question it came up that there's a pretty explicit formula for the exponential of a triangular matrix. In the below, let denote convolution and define Then, given a lower triangular matrix for , the entry of the matrix exponential is For example, This is fairly straightforward to see by induction. We're building up a solution to a differential equation . At each induction step there is an inhomogenous linear equation for the new entries, whose homogenous part has Green's function , so the inhomogenous solution is a combination of convolutions of the previous bits with that. I suspect there's also some combinatorial interpretation and argument. Probably something in terms of Laplace transforms too. This is a bit different from the way explicit formulas for exponentials of matrices are usually discussed: Usually, we reduce things all the way to Jordan blocks rather than merely triangular matrices. Usually, we decompose into a linear combination , and handle the case separately: (Avoiding both of these was useful for the previous question linked above; the goal was to find uniform bounds, and both the JNF and the decomposed-into-linear-combinations expressions are sensitive to small changes in the matrix entries.) This direct formulation in terms of convolutions of Green's functions feels like a ""textbook"" result from some textbook somewhere, but I'm not familiar enough with the topic to know where. Does anyone have a reference in the literature that states more or less this?","* 
G_{\lambda_i}(t) =
\begin{cases}
\exp(\lambda_i t) & t \geq 0 \\
0 & \text{otherwise.}
\end{cases}
 
L = \begin{pmatrix}
\lambda_1 &  &  &  \\
l_{21} & \lambda_2 &  & \\
\vdots& & \ddots & \\
l_{n1}&l_{n2} &\dots &\lambda_n
\end{pmatrix}
 t \geq 0 (a,b) 
\left(\exp(tL)\right)_{ab}
=
\sum_{a = i_1 > i_2 > \dots > i_k = b} l_{i_1i_2}\cdots l_{i_{k-1}i_k} (G_{\lambda_{i_1}} * \dots * G_{\lambda_{i_k}})(t).
 
\left(\exp(tL)\right)_{31}
=
l_{31} (G_{\lambda_3} * G_{\lambda_1})(t) + l_{32} l_{21} (G_{\lambda_3} * G_{\lambda_2} * G_{\lambda_1})(t).
 d/dt \exp(tA) = A \exp(tA) G_{\lambda_n} G_{\lambda} * G_{\mu} (\lambda - \mu)^{-1} G_\lambda - (\lambda - \mu)^{-1} G_\mu \lambda = \mu (G_\lambda * G_\lambda)(t) = t G_\lambda(t).","['linear-algebra', 'combinatorics', 'ordinary-differential-equations', 'reference-request']"
72,Differntial inclusions that are single valued near hyperbolic rest point: Hartman Grobman still fine?,Differntial inclusions that are single valued near hyperbolic rest point: Hartman Grobman still fine?,,"Suppose for $x\in \mathbb{R}^m$ we have a well behaved differential inclusion $\dot x \in F(x)$ , where $F(x)$ is a Marchaud map, i.e. u.s.c. with compact convex values and linear growth condition:  there exists $c>0$ s.t. $$\sup\{\|y\|:\, y\in F(x) \}\le c(1+\|x\|)$$ for all $x$ . So absolutely continuous solutions exist, but may not be unique. We also assume $F$ is $C^1$ whenever it is single-valued. Now suppose there is a rest point $x^*$ in a neighborhood $U$ , s.t. $F(x)$ is single valued on that neighborhood, $F(x^*)=0$ , and $x^*$ is hyperbolic. Can we still apply Hartman-Grobman in a neighborhood around that rest point? I would think yes, since the construction is local, but I'm not sure if somehow the non-uniqueness of solutions may give us an issue. Could someone give me a hint please?","Suppose for we have a well behaved differential inclusion , where is a Marchaud map, i.e. u.s.c. with compact convex values and linear growth condition:  there exists s.t. for all . So absolutely continuous solutions exist, but may not be unique. We also assume is whenever it is single-valued. Now suppose there is a rest point in a neighborhood , s.t. is single valued on that neighborhood, , and is hyperbolic. Can we still apply Hartman-Grobman in a neighborhood around that rest point? I would think yes, since the construction is local, but I'm not sure if somehow the non-uniqueness of solutions may give us an issue. Could someone give me a hint please?","x\in \mathbb{R}^m \dot x \in F(x) F(x) c>0 \sup\{\|y\|:\, y\in F(x) \}\le c(1+\|x\|) x F C^1 x^* U F(x) F(x^*)=0 x^*","['ordinary-differential-equations', 'set-valued-analysis']"
73,Find the root of $x(t;A)=0$ for ordinary differential equation $\ddot{x}(t;A)=A(p(t)\dot{x}(t;A)+q(t)x(t;A))$,Find the root of  for ordinary differential equation,x(t;A)=0 \ddot{x}(t;A)=A(p(t)\dot{x}(t;A)+q(t)x(t;A)),"It is given an ordinary differential equation $\ddot{x}(t;A)=A(p(t)\dot{x}(t;A)+q(t)x(t;A))$ with real parameter $A$ , initial conditions $x(t=0;A)=1,\dot{x}(t=0;A)=0$ for all real $A$ . If $r(A)$ is the root of $x(t;A)=0$ , i.e. $x(r(A);A)=0$ , is it possible to determine $r(A)$ (or at least determine some properties of it) in terms of the forms of $p(t),q(t)$ ? Especially important is the correlation between $r(A)$ and $A$ , e.g. whether they are correlated linearly, exponentially, etc, given the forms of $p(t)$ and $q(t)$ . The $x(t;A)$ that I am dealing in my actual problem only has one root, so whenever necessary, this can be taken as an assumption. If the general case is unsolvable, then it is also good if the problem can be solved for special  forms of $p(t),q(t)$ , e.g. $p(ct)=c^\gamma p(t), q(ct)=c^\gamma q(t)$ for some real/complex $\gamma$ .","It is given an ordinary differential equation with real parameter , initial conditions for all real . If is the root of , i.e. , is it possible to determine (or at least determine some properties of it) in terms of the forms of ? Especially important is the correlation between and , e.g. whether they are correlated linearly, exponentially, etc, given the forms of and . The that I am dealing in my actual problem only has one root, so whenever necessary, this can be taken as an assumption. If the general case is unsolvable, then it is also good if the problem can be solved for special  forms of , e.g. for some real/complex .","\ddot{x}(t;A)=A(p(t)\dot{x}(t;A)+q(t)x(t;A)) A x(t=0;A)=1,\dot{x}(t=0;A)=0 A r(A) x(t;A)=0 x(r(A);A)=0 r(A) p(t),q(t) r(A) A p(t) q(t) x(t;A) p(t),q(t) p(ct)=c^\gamma p(t), q(ct)=c^\gamma q(t) \gamma","['ordinary-differential-equations', 'nonlinear-dynamics']"
74,Estimate the number of zeros for the solution for Schrödinger equation.,Estimate the number of zeros for the solution for Schrödinger equation.,,"I have moved heaven and earth to solving the problem for three weeks, but there is no progress... The problem comes from ""Inverse scattering on the line"". Here I want to estimate the number of zeros for the solution for Schrödinger equation $$m''(x,k)+2ikm'(x,k)=q(x)m(x,k),\quad m(x,k)\rightarrow1\ as\ x\rightarrow\infty$$ in $Im(k)>0$ . Here $x$ is a real number (it's a 1-dim. Schrodinger equation), and $q(x)$ is a real potential which belongs to $\{q(x):\int^\infty_{-\infty}|q(x)|(1+|t|)dt<\infty\}$ I want to give an explanation for the upper bound of the number of zeros is $\int^{\infty}_x(t-x)|q(t)|dt$ . Previously, I finished that $m(x,k)=1+\sum^\infty_{k=1}g_n(x,k)$ , where $$g_n(x,k)=\int^\infty_x\int^{x_n}_x\int^{x_{n-1}}_x...\int^{x_2}_x\prod^{n}_{i=1} q(x_i)\prod^n_{i=1}\frac{e^{2ik(x_i-x_{i-1})}-1}{2ik}dx_1dx_2...dx_n$$ and $x_{-1}$ is $x$ . In this form, when $Im(k)>0$ , $m(x,k)$ is analytic. About $\int^{\infty}_x(t-x)|q(t)|dt$ , I have a estimate that $$|g_n(x,k)|\leq\frac{(\int^{\infty}_x(t-x)|q(t)|dt)^n}{n!}$$ By the way, my advisor told me use Sturm Comparison theorem, so I turn it into $$(e^{2ikx}m'(x,k))'-q(x)e^{2ikx}m(x,k)=0.$$ But I don't have any way to estimate the number of zeros for a differential equation. Please give me some ideas.","I have moved heaven and earth to solving the problem for three weeks, but there is no progress... The problem comes from ""Inverse scattering on the line"". Here I want to estimate the number of zeros for the solution for Schrödinger equation in . Here is a real number (it's a 1-dim. Schrodinger equation), and is a real potential which belongs to I want to give an explanation for the upper bound of the number of zeros is . Previously, I finished that , where and is . In this form, when , is analytic. About , I have a estimate that By the way, my advisor told me use Sturm Comparison theorem, so I turn it into But I don't have any way to estimate the number of zeros for a differential equation. Please give me some ideas.","m''(x,k)+2ikm'(x,k)=q(x)m(x,k),\quad m(x,k)\rightarrow1\ as\ x\rightarrow\infty Im(k)>0 x q(x) \{q(x):\int^\infty_{-\infty}|q(x)|(1+|t|)dt<\infty\} \int^{\infty}_x(t-x)|q(t)|dt m(x,k)=1+\sum^\infty_{k=1}g_n(x,k) g_n(x,k)=\int^\infty_x\int^{x_n}_x\int^{x_{n-1}}_x...\int^{x_2}_x\prod^{n}_{i=1} q(x_i)\prod^n_{i=1}\frac{e^{2ik(x_i-x_{i-1})}-1}{2ik}dx_1dx_2...dx_n x_{-1} x Im(k)>0 m(x,k) \int^{\infty}_x(t-x)|q(t)|dt |g_n(x,k)|\leq\frac{(\int^{\infty}_x(t-x)|q(t)|dt)^n}{n!} (e^{2ikx}m'(x,k))'-q(x)e^{2ikx}m(x,k)=0.","['ordinary-differential-equations', 'partial-differential-equations']"
75,Why this function is analytic at $x=0$?,Why this function is analytic at ?,x=0,"In example 1 of Differential Equations 9th edition by Nagle page 436, we are asked to find the singular points for the second-order linear differential equation $$xy''+\frac{x}{(x+1)}y'+(\sin x)y=0$$ Dividing the equation by $x$ , we have the coefficient of the middle term is $p(x) = \frac{x}{x(x+1)}$ .  Clearly $p(x)$ is not defined at x=0, however, he states that $p(x)$ is indeed analytic at $x=0$ since we can cancel the $x$ in the numerator and denominator and this new function $ \frac{1}{x+1}$ does have a power series expansion around $x=0$ .  He references the following footnote: †Such points are called removable singularities. In this chapter we assume in such cases that the function has been defined (or redefined) so that it is analytic at the point. I'd like to have more explanation on why we can make this assumption. Does it have something to do with... changing one point will not change the solution to a differential equation? Why not? I'm also not sure why he uses the phrase ""In this chapter..."" and ""...in such cases.."" in the footnote. Does the rule not apply in other cases? Maybe not for nonlinear equations?  Why not?  When is the assumption not valid? It's possible the answer to these questions is found further along in the textbook, but I'm looking for some explanation now if possible as I'm having trouble wrapping my mind around what is meant by a function being analytic at a point. Thank you in advance for any help. *If I can learn the answers to the questions above, I should be able to understand why the coefficient of the last term $q(x) = \frac{\sin(x)}{x}$ is analytic at $x=0$ as well.  It's the same idea I believe.","In example 1 of Differential Equations 9th edition by Nagle page 436, we are asked to find the singular points for the second-order linear differential equation Dividing the equation by , we have the coefficient of the middle term is .  Clearly is not defined at x=0, however, he states that is indeed analytic at since we can cancel the in the numerator and denominator and this new function does have a power series expansion around .  He references the following footnote: †Such points are called removable singularities. In this chapter we assume in such cases that the function has been defined (or redefined) so that it is analytic at the point. I'd like to have more explanation on why we can make this assumption. Does it have something to do with... changing one point will not change the solution to a differential equation? Why not? I'm also not sure why he uses the phrase ""In this chapter..."" and ""...in such cases.."" in the footnote. Does the rule not apply in other cases? Maybe not for nonlinear equations?  Why not?  When is the assumption not valid? It's possible the answer to these questions is found further along in the textbook, but I'm looking for some explanation now if possible as I'm having trouble wrapping my mind around what is meant by a function being analytic at a point. Thank you in advance for any help. *If I can learn the answers to the questions above, I should be able to understand why the coefficient of the last term is analytic at as well.  It's the same idea I believe.",xy''+\frac{x}{(x+1)}y'+(\sin x)y=0 x p(x) = \frac{x}{x(x+1)} p(x) p(x) x=0 x  \frac{1}{x+1} x=0 q(x) = \frac{\sin(x)}{x} x=0,"['ordinary-differential-equations', 'singularity']"
76,General solution of $\dot{x} = (t+x) / (t-x)$,General solution of,\dot{x} = (t+x) / (t-x),"I have the following question: I need to find the general solution of $\dot{x} = (t+x) / (t-x)$ . I tried to use the substitution $x = y \cdot t$ to get: \begin{align*} \dot{(y \cdot t)} &= \dot{y}t + y = \frac{t + ty}{t - ty} = \frac{1 + y}{1 - y} \\ \dot{y}t &= \frac{1 + y}{1 - y} - y \\ \dot{y}t &= \frac{1 + y - y + y^2}{1 - y} \\ \dot{y} &= \frac{1 + y^2}{(1 - y)t} \\ \int_{y_0}^y \frac{1-\xi}{1 + \xi^2}d\xi &= \int_{t_0}^t 1/\eta \; d \eta \\\end{align*} $$ \arctan(y) - \arctan(y_0) - \frac{\ln(1+ y^2)}{2} + \frac{\ln(1 + y_0^2)}{2} = \ln |t| - \ln |t_0| $$ However, I have no idea how to solve this equation for $y$ . There is probably a much better way to do it than what I did, could someone please help me? Thanks!","I have the following question: I need to find the general solution of . I tried to use the substitution to get: However, I have no idea how to solve this equation for . There is probably a much better way to do it than what I did, could someone please help me? Thanks!","\dot{x} = (t+x) / (t-x) x = y \cdot t \begin{align*}
\dot{(y \cdot t)} &= \dot{y}t + y = \frac{t + ty}{t - ty} = \frac{1 + y}{1 - y} \\
\dot{y}t &= \frac{1 + y}{1 - y} - y \\
\dot{y}t &= \frac{1 + y - y + y^2}{1 - y} \\
\dot{y} &= \frac{1 + y^2}{(1 - y)t} \\
\int_{y_0}^y \frac{1-\xi}{1 + \xi^2}d\xi &= \int_{t_0}^t 1/\eta \; d \eta \\\end{align*} 
\arctan(y) - \arctan(y_0) - \frac{\ln(1+ y^2)}{2} + \frac{\ln(1 + y_0^2)}{2} = \ln |t| - \ln |t_0|
 y","['real-analysis', 'calculus', 'ordinary-differential-equations']"
77,Determining solution of Riccati equations,Determining solution of Riccati equations,,"Given are a $\mathbb{C}$ -valued function $\phi$ , $\mathbb{C}^d$ -valued function $\psi$ , $\mathcal{X}\subseteq \mathbb{R}^d$ and the following differential equation, which holds for all $x \in \mathcal{X},\ T \geq 0,\ u \in \mathrm{i}\mathbb{R}^{d}$ : $$ \begin{aligned} \partial_{T} \phi(T, u)+\partial_{T} \psi(T, u)^{\top} x&=\psi(T, u)^{\top} b(x)+\frac{1}{2} \psi(T, u)^{\top} a(x) \psi(T, u)\\ \psi(0,u)&=u\\ \phi(0,u)&=0 \end{aligned} $$ Now, the author states the following: Since $\psi(0, u)=u$ , this implies that $a$ and $b$ are of the form $$ \begin{aligned} &a(x)=A+\sum_{i=1}^{d} x_{i} \alpha_{i} \\ &b(x)=B+\sum_{i=1}^{d} x_{i} \beta_{i} \end{aligned} $$ for some $d \times d$ -matrices $A$ and $\alpha_{i}$ , and $d$ -vectors $B$ and $\beta_{i}$ . Plugging this back into the above equation yields the Riccati equations \begin{aligned} \partial_{t} \phi(t, u) &=\frac{1}{2} \psi(t, u)^{\top} A\  \psi(t, u)+B^{\top} \psi(t, u) \\ \phi(0, u) &=0 \\ \partial_{t} \psi_{i}(t, u) &=\frac{1}{2} \psi(t, u)^{\top} \alpha_{i} \psi(t, u)+\beta_{i}^{\top} \psi(t, u), \quad 1 \leq i \leq d \\ \psi(0, u) &=u \end{aligned} For $T=0$ , I find $$ \begin{aligned} \partial_{T} \phi(0, u)+\partial_{T} \psi(0, u)^{\top} x&=u^{\top} b(x)+\frac{1}{2} u^{\top} a(x) u\\ \end{aligned} $$ But how do I find the form of $a(x)$ and $b(x)$ from that? How do I find the Riccati equations? The equation $$ \begin{aligned} \partial_{T} \phi(T, u)+\partial_{T} \psi(T, u)^{\top} x&=\psi(T, u)^{\top} b(x)+\frac{1}{2} \psi(T, u)^{\top} a(x) \psi(T, u)\\ \end{aligned} $$ reminds me on some sort of Taylor expansion. Maybe this could be used? Thank you in advance!","Given are a -valued function , -valued function , and the following differential equation, which holds for all : Now, the author states the following: Since , this implies that and are of the form for some -matrices and , and -vectors and . Plugging this back into the above equation yields the Riccati equations For , I find But how do I find the form of and from that? How do I find the Riccati equations? The equation reminds me on some sort of Taylor expansion. Maybe this could be used? Thank you in advance!","\mathbb{C} \phi \mathbb{C}^d \psi \mathcal{X}\subseteq \mathbb{R}^d x \in \mathcal{X},\ T \geq 0,\ u \in \mathrm{i}\mathbb{R}^{d} 
\begin{aligned}
\partial_{T} \phi(T, u)+\partial_{T} \psi(T, u)^{\top} x&=\psi(T, u)^{\top} b(x)+\frac{1}{2} \psi(T, u)^{\top} a(x) \psi(T, u)\\
\psi(0,u)&=u\\
\phi(0,u)&=0
\end{aligned}
 \psi(0, u)=u a b 
\begin{aligned}
&a(x)=A+\sum_{i=1}^{d} x_{i} \alpha_{i} \\
&b(x)=B+\sum_{i=1}^{d} x_{i} \beta_{i}
\end{aligned}
 d \times d A \alpha_{i} d B \beta_{i} \begin{aligned}
\partial_{t} \phi(t, u) &=\frac{1}{2} \psi(t, u)^{\top} A\  \psi(t, u)+B^{\top} \psi(t, u) \\
\phi(0, u) &=0 \\
\partial_{t} \psi_{i}(t, u) &=\frac{1}{2} \psi(t, u)^{\top} \alpha_{i} \psi(t, u)+\beta_{i}^{\top} \psi(t, u), \quad 1 \leq i \leq d \\
\psi(0, u) &=u
\end{aligned} T=0 
\begin{aligned}
\partial_{T} \phi(0, u)+\partial_{T} \psi(0, u)^{\top} x&=u^{\top} b(x)+\frac{1}{2} u^{\top} a(x) u\\
\end{aligned}
 a(x) b(x) 
\begin{aligned}
\partial_{T} \phi(T, u)+\partial_{T} \psi(T, u)^{\top} x&=\psi(T, u)^{\top} b(x)+\frac{1}{2} \psi(T, u)^{\top} a(x) \psi(T, u)\\
\end{aligned}
","['real-analysis', 'linear-algebra', 'ordinary-differential-equations', 'partial-differential-equations', 'partial-derivative']"
78,Solving an equation involving a matrix exponential,Solving an equation involving a matrix exponential,,"Suppose we have unknown scalars $x_1, x_2, ...,x_m \in \mathbb{R}$ ,  known matrices $A_1, A_2, ...,A_m \in \mathbb{R}^{n\times n}$ , and two known vectors $s_0, s_1\in\mathbb{R}^n$ . I want to find $x_1, x_2, ...,x_m$ that satisfy the following equation: $$s_1 = \exp\!\left(\sum_{i=1}^{m}x_iA_i\right)\!s_0$$ I know that there can be no solutions, a finite number of solutions, or an infinite number of solutions. I would like to find all of the solutions that exist. The matrices $A_i$ don't have a special form or property. If you have recommended books on the topic that would be extremely helpful. I am also interested in ways to numerically find all of the solutions.","Suppose we have unknown scalars ,  known matrices , and two known vectors . I want to find that satisfy the following equation: I know that there can be no solutions, a finite number of solutions, or an infinite number of solutions. I would like to find all of the solutions that exist. The matrices don't have a special form or property. If you have recommended books on the topic that would be extremely helpful. I am also interested in ways to numerically find all of the solutions.","x_1, x_2, ...,x_m \in \mathbb{R} A_1, A_2, ...,A_m \in \mathbb{R}^{n\times n} s_0, s_1\in\mathbb{R}^n x_1, x_2, ...,x_m s_1 = \exp\!\left(\sum_{i=1}^{m}x_iA_i\right)\!s_0 A_i","['matrices', 'ordinary-differential-equations', 'vectors', 'lambert-w', 'matrix-exponential']"
79,Is there a geometry behind the singularity formation in solutions to nonlinear ODE's?,Is there a geometry behind the singularity formation in solutions to nonlinear ODE's?,,"Disclaimer: I have posted this question on mathoverflow.net following the instructions of this topic . If we take two apparently simple first order ODE's like $y'=y$ and $y'=y^2$ we find that: For the first one the general solution is $y=C\exp(t)$ and is defined for all $t$ . For the second one, the general solution is $y=\frac{1}{C-t}$ and it's defined only for $t<C$ . There is a way to 'see' geometrically the solutions of these equations as leaves of the ""Characteristic Foliation"" of the standard contact structure of $R^3$ . Look at this site and this article for some nice pictured examples. Looking for the characteristic foliation of the two equations, the shape of the surface seens to play a big role. Apparently is the curvature of the surface that, somehow, 'controls' the growth of solutions. My question is: Are there some notion of curvature that explains why nonlinear EDO's can have solutions with movable singularities? I know that Contact geometry doesn't have local invariants as Riemannian geometry, so, I believe, the answer to this question lies beyond the domain of Contact/Symplectic geometry. ps: taking other linear and nonlinear equations, with or without time varying coeficients, I had the same impression. Here are some pictures that I made: Characteristic foliation for $y'=y$ Characteristic foliation for $y'=y^2$","Disclaimer: I have posted this question on mathoverflow.net following the instructions of this topic . If we take two apparently simple first order ODE's like and we find that: For the first one the general solution is and is defined for all . For the second one, the general solution is and it's defined only for . There is a way to 'see' geometrically the solutions of these equations as leaves of the ""Characteristic Foliation"" of the standard contact structure of . Look at this site and this article for some nice pictured examples. Looking for the characteristic foliation of the two equations, the shape of the surface seens to play a big role. Apparently is the curvature of the surface that, somehow, 'controls' the growth of solutions. My question is: Are there some notion of curvature that explains why nonlinear EDO's can have solutions with movable singularities? I know that Contact geometry doesn't have local invariants as Riemannian geometry, so, I believe, the answer to this question lies beyond the domain of Contact/Symplectic geometry. ps: taking other linear and nonlinear equations, with or without time varying coeficients, I had the same impression. Here are some pictures that I made: Characteristic foliation for Characteristic foliation for",y'=y y'=y^2 y=C\exp(t) t y=\frac{1}{C-t} t<C R^3 y'=y y'=y^2,"['ordinary-differential-equations', 'differential-geometry', 'curvature', 'symplectic-geometry', 'contact-geometry']"
80,What functions do we need to solve linear second order differential equations with polynomial coeficients?,What functions do we need to solve linear second order differential equations with polynomial coeficients?,,"Disclaimer: I have posted this question on mathoverflow.net following the instructions of this topic . I'm now trying to understand how can a ordinary differential equation be tested to decide if it's integrable or not. Recently I become aware of the Painlevé property and start to read the following paper by Robert Conte: The Painlevé Approach to Nonlinear Ordinary Differential Equations In section 2.1, he states: ""A very deep result of L. Fuchs, Poincaré and Painlevé is that the class of first order ODEs... ...defines one and only one function... ...the elliptic function introduced earlier by Weierstrass..."" My initial question is: Does this means that the solutions of any integrable first order ODE can by expressed by elementary functions and the Weierstrass elliptic function? I know that many functions that are solutions to second order ODE (Exponentials, Bessel functions , hypergeometric functions, Airy functions ...) can be expressed by generalized hypergeometric series. My main question is: Are there some set of functions such that all solutions to linear second order ODE, with polynomial coeficients, can be expressed with? (may be: rational functions, exponentials and generalized hypergeometric series) If yes, where can I find a comprehensive list?","Disclaimer: I have posted this question on mathoverflow.net following the instructions of this topic . I'm now trying to understand how can a ordinary differential equation be tested to decide if it's integrable or not. Recently I become aware of the Painlevé property and start to read the following paper by Robert Conte: The Painlevé Approach to Nonlinear Ordinary Differential Equations In section 2.1, he states: ""A very deep result of L. Fuchs, Poincaré and Painlevé is that the class of first order ODEs... ...defines one and only one function... ...the elliptic function introduced earlier by Weierstrass..."" My initial question is: Does this means that the solutions of any integrable first order ODE can by expressed by elementary functions and the Weierstrass elliptic function? I know that many functions that are solutions to second order ODE (Exponentials, Bessel functions , hypergeometric functions, Airy functions ...) can be expressed by generalized hypergeometric series. My main question is: Are there some set of functions such that all solutions to linear second order ODE, with polynomial coeficients, can be expressed with? (may be: rational functions, exponentials and generalized hypergeometric series) If yes, where can I find a comprehensive list?",,"['ordinary-differential-equations', 'special-functions', 'integrable-systems']"
81,"How to solve $x=\lim\limits_{t\to0} Q^{-1}(t,t)\implies \text{Ei}(-x)=-1\implies Γ(0,x)=1$?",How to solve ?,"x=\lim\limits_{t\to0} Q^{-1}(t,t)\implies \text{Ei}(-x)=-1\implies Γ(0,x)=1","Based on: Conjecture: $$\lim\limits_{x\to\infty}\operatorname{Re}\text W_x(x)\mathop=\limits^?-\ln(2\pi)$$ and On completing the solution for $$\int_0^1 Q^{-1}(x,x) dx$$ and other constants. Here is the goal limit again using the central Inverse Regularized Gamma function $Q^{-1}(x,x)$ : $$α\mathop=^\text{def} \lim_{x\to0} Q^{-1}(x,x)=Q^{-1}(0,0)\ne 0$$ Note that Wolfram Alpha says that $Q^{-1}(0,0)=0$ , but this is just due to the definition of the function. If you take a look at the graph, the “true” limit clearly is not $0$ : However, we can use the main Taylor series definition centered at $x=1$ with the new $C_{n,x}$ coefficient notation. Note that there are other series in the bolded link. $$Q^{-1}(x,x)=((1-x)x!)^\frac1x+\frac {((1-x)x!)^\frac2x}{x+1}+ \frac {(3x+5)((1-x)x!)^\frac3x}{2(x+1)^2(x+2)}+O\left((x-1)^\frac4x \right)\mathop =^\text{def}\sum_{n=0}^\infty C_{n,x}\cdot ((1-x)x!)^\frac nx$$ Now let’s apply the limit noting that we can apply the limit term by term. The limit is a long process, but ends up being in terms of $e$ and the Euler-Mascheroni constant γ : $$α= \lim_{x\to0} Q^{-1}(x,x)=Q^{-1}(0,0)= \lim_{x\to 0}\sum_{n=0}^\infty C_{n,x}\cdot ((1-x)x!)^\frac nx=\sum_{n=1}^\infty \frac{C_{n,0}}{e^{(γ+1)n}} = e^{-(γ+1)}+ e^{-2(γ+1)}+ \frac 54 e^{-3(γ+1)}+\frac{31}{18} e^{-4(γ+1)}+ \frac{361}{144}e^{-5(γ+1)}+\frac{4537}{1200} e^{-6(γ+1)} +…=.26473… $$ A few observations on the constants: The nth numerator and denominator of $C_{n,0}$ is on the order of $10^n$ for most of the first few terms. Here is a graph of the first few coefficients in the expansion. An equivalent problem is by solving the following differential equation for the function : $$y(x)y’’(x)-y’(x)^2(y(x)+1-a)=0,y(x)=Q^{-1}(a,x)\implies y(x)y’’(x)-y’(x)^2(y(x)+1)=0,y(x)=\lim_{a\to 0} Q^{-1}(a,x),y(x)=y(a,x),y(0,0)=α$$ It turns out that the special case can be solved in terms of an Exponential Integral function . Let me “make up” an inverse Exponential Integral function: $$y(x)y’’(x)-y’(x)^2(y(x)+1)=0\implies c+x=\frac{\text{Ei}(-y(x))}{c}\implies c^2+cx=\text{Ei}(-y(x))\implies y(x)=-\text{Ei}^{-1}(c^2+cx)$$ So the constant is maybe related to Soldner’s Constant Another idea is letting $x=a$ for: $$y(x)y’’(x)-y’(x)^2(y(x)+1-a)=0, y(0)$$ but I am not sure if this would produce $y=Q^{-1}(x,x)$ . Amazingly, our differential equation works: $\text{Ei}(-α)=-1$ Therefore: $$\text{Ei}(-Q^{-1}(0,0))=\text{Ei}\left(-\sum_\Bbb N C_{n,0} e^{-n(γ+1)}\right)=-1$$ So a “closed form” using a made up Inverse Exponential Integral function would be: $$α=\lim_{x\to0}Q^{-1}(x,x)=-\text{Ei}^{-1}(-1)$$ You can also find the following assuming an inverse function: $$y’=ye^y\implies c_1+x=\text{Ei}(-y(x))\implies y(x)=-\text{Ei}^{-1}(x+c_1), x+c_1=-1$$ A better way would be to solve $$\text{Ei}(-x)=-1,x=α$$ for $x$ . Let’s use this method to find many more digits of the constant $$α=0.264737010451543159461927…$$ while the other real root, for fun, of $\text{Ei}(-x)+1$ is: $$x= -0.1724867417161…$$ Here are $3$ other identities of this constant using this Incomplete Gamma function identity and this Generalized Exponential Integral function . The third one uses the Hyperbolic Cosine and Hyperbolic Sine Integral functions $$Γ(0,α)=\text E_1(α)=1\implies \int_α^\infty \frac{dx}{xe^x}=1$$ $$\text{Shi}(α)-\text{Chi}(α)=1$$ Note that other constants may also fit the formulas for the constant. Many other identities can be similarly derived, so please go ahead; I have written enough for now. How can I find an alternate form of the constant? Please correct me and give me feedback!","Based on: Conjecture: and On completing the solution for and other constants. Here is the goal limit again using the central Inverse Regularized Gamma function : Note that Wolfram Alpha says that , but this is just due to the definition of the function. If you take a look at the graph, the “true” limit clearly is not : However, we can use the main Taylor series definition centered at with the new coefficient notation. Note that there are other series in the bolded link. Now let’s apply the limit noting that we can apply the limit term by term. The limit is a long process, but ends up being in terms of and the Euler-Mascheroni constant γ : A few observations on the constants: The nth numerator and denominator of is on the order of for most of the first few terms. Here is a graph of the first few coefficients in the expansion. An equivalent problem is by solving the following differential equation for the function : It turns out that the special case can be solved in terms of an Exponential Integral function . Let me “make up” an inverse Exponential Integral function: So the constant is maybe related to Soldner’s Constant Another idea is letting for: but I am not sure if this would produce . Amazingly, our differential equation works: Therefore: So a “closed form” using a made up Inverse Exponential Integral function would be: You can also find the following assuming an inverse function: A better way would be to solve for . Let’s use this method to find many more digits of the constant while the other real root, for fun, of is: Here are other identities of this constant using this Incomplete Gamma function identity and this Generalized Exponential Integral function . The third one uses the Hyperbolic Cosine and Hyperbolic Sine Integral functions Note that other constants may also fit the formulas for the constant. Many other identities can be similarly derived, so please go ahead; I have written enough for now. How can I find an alternate form of the constant? Please correct me and give me feedback!","\lim\limits_{x\to\infty}\operatorname{Re}\text W_x(x)\mathop=\limits^?-\ln(2\pi) \int_0^1 Q^{-1}(x,x) dx Q^{-1}(x,x) α\mathop=^\text{def} \lim_{x\to0} Q^{-1}(x,x)=Q^{-1}(0,0)\ne 0 Q^{-1}(0,0)=0 0 x=1 C_{n,x} Q^{-1}(x,x)=((1-x)x!)^\frac1x+\frac {((1-x)x!)^\frac2x}{x+1}+ \frac {(3x+5)((1-x)x!)^\frac3x}{2(x+1)^2(x+2)}+O\left((x-1)^\frac4x \right)\mathop =^\text{def}\sum_{n=0}^\infty C_{n,x}\cdot ((1-x)x!)^\frac nx e α= \lim_{x\to0} Q^{-1}(x,x)=Q^{-1}(0,0)= \lim_{x\to 0}\sum_{n=0}^\infty C_{n,x}\cdot ((1-x)x!)^\frac nx=\sum_{n=1}^\infty \frac{C_{n,0}}{e^{(γ+1)n}} = e^{-(γ+1)}+ e^{-2(γ+1)}+ \frac 54 e^{-3(γ+1)}+\frac{31}{18} e^{-4(γ+1)}+ \frac{361}{144}e^{-5(γ+1)}+\frac{4537}{1200} e^{-6(γ+1)} +…=.26473…  C_{n,0} 10^n y(x)y’’(x)-y’(x)^2(y(x)+1-a)=0,y(x)=Q^{-1}(a,x)\implies y(x)y’’(x)-y’(x)^2(y(x)+1)=0,y(x)=\lim_{a\to 0} Q^{-1}(a,x),y(x)=y(a,x),y(0,0)=α y(x)y’’(x)-y’(x)^2(y(x)+1)=0\implies c+x=\frac{\text{Ei}(-y(x))}{c}\implies c^2+cx=\text{Ei}(-y(x))\implies y(x)=-\text{Ei}^{-1}(c^2+cx) x=a y(x)y’’(x)-y’(x)^2(y(x)+1-a)=0, y(0) y=Q^{-1}(x,x) \text{Ei}(-α)=-1 \text{Ei}(-Q^{-1}(0,0))=\text{Ei}\left(-\sum_\Bbb N C_{n,0} e^{-n(γ+1)}\right)=-1 α=\lim_{x\to0}Q^{-1}(x,x)=-\text{Ei}^{-1}(-1) y’=ye^y\implies c_1+x=\text{Ei}(-y(x))\implies y(x)=-\text{Ei}^{-1}(x+c_1), x+c_1=-1 \text{Ei}(-x)=-1,x=α x α=0.264737010451543159461927… \text{Ei}(-x)+1 x= -0.1724867417161… 3 Γ(0,α)=\text E_1(α)=1\implies \int_α^\infty \frac{dx}{xe^x}=1 \text{Shi}(α)-\text{Chi}(α)=1","['sequences-and-series', 'ordinary-differential-equations', 'limits', 'special-functions', 'constants']"
82,Telling First Order Linear vs Non Linear ODE from its Slope Field,Telling First Order Linear vs Non Linear ODE from its Slope Field,,"Is it possible to distinguish between a first order linear and non linear ODE just from its slope field? In autonomous vs non-autonomous, you can easily tell from whether the slope varies with $x$ . I can not think of patterns that would distinguish $y' +p(t)y = q(t)$ from $y\ = f(t,y)$ ... is this generally the case?","Is it possible to distinguish between a first order linear and non linear ODE just from its slope field? In autonomous vs non-autonomous, you can easily tell from whether the slope varies with . I can not think of patterns that would distinguish from ... is this generally the case?","x y' +p(t)y = q(t) y\ = f(t,y)",['ordinary-differential-equations']
83,Any non-zero solution of some second-order differential equation is not $2\pi$-period,Any non-zero solution of some second-order differential equation is not -period,2\pi,"Suppose $$\frac {{d^2}y}{dx^2}+P(x)y = 0$$ where $P(x)$ is continuous and satisfying $n^2<P(x)<(n+1)^2$ where $n$ is a non-negative integer. Prove that any non-zero solution of the above second-order differential equation is not $2\pi$ -periodic. There are two facts I know that may be useful to the proof: $y''+Q(x)y=0$ where $Q(x)$ is continuous on $[a,+\infty]$ and satisfying $Q(x)\geq m>0$ , then the distance between any two immediate zeros of non-zero solution of it is less than $\frac {\pi}{\sqrt{m}}$ . $y''+Q(x)y=0$ where $Q(x)$ is continuous on $[a,+\infty]$ and satisfying $Q(x)\leq M \ (M>0)$ , then the distance between any two immediate zeros of non-zero solution of it is larger than $\frac {\pi}{\sqrt{M}}$ . From this, I can't get a contradiction since if there is such a solution, with $x_0$ and $x_0+2\pi$ two zeros, then there can be $2n$ zeros between them with each of adjacent distance $d_j$ satisfying $\frac {\pi}{n+1}< d_j < \frac {\pi}{n}$ . Does anyone know how to prove the result? Thank you","Suppose where is continuous and satisfying where is a non-negative integer. Prove that any non-zero solution of the above second-order differential equation is not -periodic. There are two facts I know that may be useful to the proof: where is continuous on and satisfying , then the distance between any two immediate zeros of non-zero solution of it is less than . where is continuous on and satisfying , then the distance between any two immediate zeros of non-zero solution of it is larger than . From this, I can't get a contradiction since if there is such a solution, with and two zeros, then there can be zeros between them with each of adjacent distance satisfying . Does anyone know how to prove the result? Thank you","\frac {{d^2}y}{dx^2}+P(x)y = 0 P(x) n^2<P(x)<(n+1)^2 n 2\pi y''+Q(x)y=0 Q(x) [a,+\infty] Q(x)\geq m>0 \frac {\pi}{\sqrt{m}} y''+Q(x)y=0 Q(x) [a,+\infty] Q(x)\leq M \ (M>0) \frac {\pi}{\sqrt{M}} x_0 x_0+2\pi 2n d_j \frac {\pi}{n+1}< d_j < \frac {\pi}{n}",['ordinary-differential-equations']
84,When can remove absolute value symbol in ODE,When can remove absolute value symbol in ODE,,"There are two examples $$ \frac{dy}{dx} = \frac{y}{x} $$ and $$ 2\frac{dy}{dx} = \frac{y}{x} $$ The first one , we have $$ \ln |y| = \ln |x| + C_1 \Rightarrow |y| = e^{C_1}|x| \Rightarrow  y = Cx. $$ Its absolute value sign can be removed and use a constants $C$ . The second one, we have $$ 2\ln |y| = \ln |x| + C_1 \Rightarrow y^2 = e^{C_1}|x| \Rightarrow y^2 = C|x|. $$ Its absolute value sign cant be removed. I use the above way to judge whether the absolute value can be removed. For the first one when $x > 0, y > 0$ , we have $$ 	\ln y = \ln x + C_1 \Rightarrow y = C_2 x. 	$$ when $x < 0, y > 0$ , we have $$ 	\ln y = \ln -x + C_1 \Rightarrow y = C_3 x. 	$$ when $x > 0 , y < 0$ , we have $$ 	\ln -y = \ln x + C_1 \Rightarrow y= C_4 x. 	$$ when $x < 0, y < 0$ , we have $$ 	\ln -y = \ln -x + C_1 \Rightarrow y = C_5 x. 	$$ when $y = 0$ , we have $$ 	y = 0 \Rightarrow y = 0 \cdot x. 	$$ Thus, finally the solution can be $y = Cx$ . For the second, when $x > 0, y > 0$ , we have $$ y = C\sqrt{x}. $$ when $x < 0, y > 0$ , we have $$ y = C\sqrt{-x}. $$ Thus, its cant be removed absolute value. My doubt is there exists cheap way to judge the absolute value can be removed? Thanks in advance!","There are two examples and The first one , we have Its absolute value sign can be removed and use a constants . The second one, we have Its absolute value sign cant be removed. I use the above way to judge whether the absolute value can be removed. For the first one when , we have when , we have when , we have when , we have when , we have Thus, finally the solution can be . For the second, when , we have when , we have Thus, its cant be removed absolute value. My doubt is there exists cheap way to judge the absolute value can be removed? Thanks in advance!","
\frac{dy}{dx} = \frac{y}{x}
 
2\frac{dy}{dx} = \frac{y}{x}
 
\ln |y| = \ln |x| + C_1 \Rightarrow |y| = e^{C_1}|x| \Rightarrow  y = Cx.
 C 
2\ln |y| = \ln |x| + C_1 \Rightarrow y^2 = e^{C_1}|x| \Rightarrow y^2 = C|x|.
 x > 0, y > 0 
	\ln y = \ln x + C_1 \Rightarrow y = C_2 x.
	 x < 0, y > 0 
	\ln y = \ln -x + C_1 \Rightarrow y = C_3 x.
	 x > 0 , y < 0 
	\ln -y = \ln x + C_1 \Rightarrow y= C_4 x.
	 x < 0, y < 0 
	\ln -y = \ln -x + C_1 \Rightarrow y = C_5 x.
	 y = 0 
	y = 0 \Rightarrow y = 0 \cdot x.
	 y = Cx x > 0, y > 0 
y = C\sqrt{x}.
 x < 0, y > 0 
y = C\sqrt{-x}.
","['real-analysis', 'ordinary-differential-equations']"
85,Find the function which satisfies the following functional equation,Find the function which satisfies the following functional equation,,This is the equation $$e^{ia}f\left[\int_{0}^a e^{ix}f(x)dx\right]=\int_{0}^a e^{ix}f(x)dx$$ I tried but wasn't able to figure out. Any suggestion towards how should I approach it are appreciated. My attempt : I tried to differentiate with respect to $a$ . I got this : $$f(a) = e^{ia}f’\left(\int_{0}^a e^{ix}f(x)dx\right)f(a) + if\left[\int_{0}^a e^{ix}f(x)dx\right]$$ then I tried to equate real and imaginary part but nothing happened.,This is the equation I tried but wasn't able to figure out. Any suggestion towards how should I approach it are appreciated. My attempt : I tried to differentiate with respect to . I got this : then I tried to equate real and imaginary part but nothing happened.,e^{ia}f\left[\int_{0}^a e^{ix}f(x)dx\right]=\int_{0}^a e^{ix}f(x)dx a f(a) = e^{ia}f’\left(\int_{0}^a e^{ix}f(x)dx\right)f(a) + if\left[\int_{0}^a e^{ix}f(x)dx\right],"['calculus', 'ordinary-differential-equations']"
86,Bifurcations where eigenvalues become purely imaginary,Bifurcations where eigenvalues become purely imaginary,,"Say a system of 2 ODE's has the following Jacobian at a fixed point $(0,0)$ , $$ Df_{\mu}(0,0) =  \begin{pmatrix} -(1/2 - \mu) & 1/2+\mu \\ -(1/2 + \mu) & 1/2-\mu  \end{pmatrix} $$ with eigenvalues $\lambda = \pm \sqrt{-2\mu}$ . Clearly the eigenvalues are real and distinct for $\mu<0$ , are both $0$ for $\mu=0$ , and are complex conjugate for $\mu>0$ . I've been searching for some references on such a bifurcation, which doesn't seem to resemble a Hopf bifurcation (where the real component of $\lambda$ is dependent on $\mu$ , but not the imaginary component). Any recommendations or references would be greatly appreciated.","Say a system of 2 ODE's has the following Jacobian at a fixed point , with eigenvalues . Clearly the eigenvalues are real and distinct for , are both for , and are complex conjugate for . I've been searching for some references on such a bifurcation, which doesn't seem to resemble a Hopf bifurcation (where the real component of is dependent on , but not the imaginary component). Any recommendations or references would be greatly appreciated.","(0,0) 
Df_{\mu}(0,0) = 
\begin{pmatrix}
-(1/2 - \mu) & 1/2+\mu \\
-(1/2 + \mu) & 1/2-\mu 
\end{pmatrix}
 \lambda = \pm \sqrt{-2\mu} \mu<0 0 \mu=0 \mu>0 \lambda \mu","['ordinary-differential-equations', 'reference-request', 'dynamical-systems', 'stability-theory', 'bifurcation']"
87,differential inequality with $f$ and $f'$,differential inequality with  and,f f',"Let $f$ be a continuously differentiable function such that for all real numbers $x$ : $$(f(x))^2+(1+f'(x))^2 \leq  1 .$$ How do you prove that $f = 0$ ? I managed to prove that $f$ was bounded between $-1$ and $1$ , $f'$ between $-2$ and $0$ and therefore that $f$ was decreasing and I was trying to integrate the inequality but i'm stuck now .","Let be a continuously differentiable function such that for all real numbers : How do you prove that ? I managed to prove that was bounded between and , between and and therefore that was decreasing and I was trying to integrate the inequality but i'm stuck now .",f x (f(x))^2+(1+f'(x))^2 \leq  1 . f = 0 f -1 1 f' -2 0 f,"['real-analysis', 'ordinary-differential-equations', 'derivatives', 'inequality']"
88,Differential equation book with lots of exercises,Differential equation book with lots of exercises,,"I am a post-doc with a PhD in theoretical physics. I have a background in the theory of differential equations, but I feel i lack of technique, so I am looking for a differential equations (ODE and PDE) book with lots of exercises. Can anyone suggest any?","I am a post-doc with a PhD in theoretical physics. I have a background in the theory of differential equations, but I feel i lack of technique, so I am looking for a differential equations (ODE and PDE) book with lots of exercises. Can anyone suggest any?",,"['ordinary-differential-equations', 'partial-differential-equations', 'reference-request']"
89,Selecting a suitable Lyapunov function for the following systems to analyse global stability?,Selecting a suitable Lyapunov function for the following systems to analyse global stability?,,"SECOND BOUNTY! i) SI MODEL Consider \begin{align} \frac{dS}{dt} &= \mu N -\frac{\beta S I}{N} - \nu S\\[2ex] \frac{dI}{dt} &= \frac{\beta S I}{N} -\nu I \end{align} Where $N=S+I$ is the total population. If we assume $\mu =\nu$ , the above reduces to: \begin{align} \frac{dS}{dt} &= -\beta S I + \nu I\\[2ex] \frac{dI}{dt} &= \beta S I -\nu I \end{align} The equilibrium points: \begin{align*} e_1 : \left( S_1^*, I_1^*\right)&= \left(1, 0\right), \\[2ex] e_2 : \left( S_2^*, I_2^*\right)&= \left(\frac{\nu }{\beta}, \frac{\nu}{\beta}\left(\frac{\beta}{\nu}-1 \right)\right) \end{align*} where $\mathcal{R}_0 = \beta/\nu$ . The set $\Omega = \left\lbrace \left(S,I\right)\in \mathbb{R}_+^2 : S\geq 0, I \geq 0, S+I \leq 1  \right\rbrace$ is our domain of definition. This set is a positively invariant set for our system. Now to prove the global (asymptotically) stability of $e_1$ is straightforward, we use the function $V = I$ and the result follows. Now my question is, how would I construct a Lyapunov function for $e_2$ ? I know it should be in the form of a Volterra function but, how can I choose a parricular one? any ideas? ii) SIS MODEL Consider \begin{align} \frac{dS}{dt} &= \mu N -\frac{\beta S I}{N}+ \gamma I - \nu S\\[2ex] \frac{dI}{dt} &= \frac{\beta S I}{N} -(\gamma+\nu) I \end{align} Where $N=S+I$ is the total population. If we assume $\mu =\nu$ , the above reduces to: \begin{align} \frac{dS}{dt} &= -\beta S I + (\gamma +\nu) I\\[2ex] \frac{dI}{dt} &= \beta S I -(\gamma+\nu) I \end{align} The equilibrium points: \begin{align*} e_1 : \left( S_1^*, I_1^*\right)&= \left(1, 0\right), \\[2ex] e_2 : \left( S_2^*, I_2^*\right)&= \left(\frac{\left(\gamma+\nu\right)}{\beta}, \frac{\left(\gamma+\nu\right)}{\beta}\left(\frac{\beta}{\gamma+\nu} -1\right)\right), \end{align*} where $\mathcal{R}_0 = \beta/(\gamma+\nu)$ . Again, as in case (i) we have the set $\Omega = \left\lbrace \left(S,I\right)\in \mathbb{R}_+^2 : S\geq 0, I \geq 0, S+I \leq 1  \right\rbrace$ as our domain of definition. This set is a positively invariant set for our system. Analogously as earlier, to prove the global (asymptotically) stability of $e_1$ is straightforward, we use the function $V = I$ and the result follows. How would I chose a suitable Lyapunov function for this model to analyse the endemic equilibrium $e_2$ ? Well, if we find a suitable Lyapunov function for case (i) then we can just replace $\nu$ with $\gamma+\nu$ for this system. iii) SIR MODEL Consider \begin{align} \frac{dS}{dt} &= \mu N -\frac{\beta S I}{N} - \nu S\\[2ex] \frac{dI}{dt} &= \frac{\beta S I}{N} -(\gamma+\nu) I\\[2ex] \frac{dR}{dt} &= \gamma I -\nu R \end{align} Where $N=S+I+R$ is the total population. If we assume $\mu =\nu$ , the above reduces to: \begin{align} \frac{dS}{dt} &= -\beta S I + \nu I +\nu R \\[2ex] \frac{dI}{dt} &= \beta S I -(\gamma +\nu) I\\[2ex] \frac{dR}{dt} &= \gamma I -\nu R -\xi R \end{align} Using $R=N-S-I$ to kill degeneracy, this further reduces to \begin{align} \frac{dS}{dt} &= -\beta S I + \nu  -\nu S \\[2ex] \frac{dI}{dt} &= \beta S I -(\gamma +\nu) I\\[2ex] \end{align} The equilibrium points read \begin{align*} e_1 : \left( S_1^*, I_1^*, R_1^*\right)&= \left(1, 0, 0\right), \\[2ex] e_2 : \left( S_2^*, I_2^*, R_2^*\right)&= \left(\frac{\left(\gamma+\nu\right)}{\beta}, \frac{\nu}{\beta}\left(\frac{\beta}{\gamma +\nu}-1\right), \frac{\gamma}{\beta}\left(\frac{\beta}{\gamma +\nu}-1\right)\right)\\[1ex] \end{align*} where $\mathcal{R}_0 = \beta/(\gamma+\nu)$ . Again, as in case (i) and (ii) we have the set $\Omega = \left\lbrace \left(S,I\right)\in \mathbb{R}_+^2 : S\geq 0, I \geq 0, S+I \leq 1  \right\rbrace$ as our domain of definition. This set is a positively invariant set for our system. Theorem If $\mathcal{R}_0 \leq 1$ , then the disease-free equilibrium $e_1$ is globally asymptotically stable in $\Omega$ . Proof We use the function $V = I$ and the result follows. Theorem If $\mathcal{R}_0 > 1$ , then the endemic equilibrium $e_2$ is globally asymptotically stable in the interior of $\Omega$ . Proof To prove the global asymptotically stability for $e_2$ , consider the Lyapunov function (in the form of a Volterra function): $$V(S,I) = \left(S-S_2^*\right)+ \left( I-I_2^*\right) -S_2^* \ln \frac{S}{S_2^*} - I_2^* \ln \frac{I}{I_2^*}. $$ working out the time derivatives along our reduced system, we arrive to: $$\dot V = -\beta (S_2^* -S)^2 [\frac{1}{S}] \leq 0 $$ We see $\dot V$ is negative except when $\dot V$ takes on the equilibrium values so that $\dot V =0$ , so we have semi-definiteness. The largest compact invariant set in $\Omega$ so that $\dot V$ is $0$ is the singleton $\lbrace{ e_2\rbrace}$ . Hence, concluding from LaSalle's invariance principle, $e_2$ is globally asymptotically stable in $\Omega$ . iv) SIRS MODEL Consider \begin{align} \frac{dS}{dt} &= \mu N -\frac{\beta S I}{N} +\xi R - \nu S\\[2ex] \frac{dI}{dt} &= \frac{\beta S I}{N} -(\gamma+\nu) I\\[2ex] \frac{dR}{dt} &= \gamma I-\xi R -\nu R \end{align} Where $N=S+I+R$ is the total population. If we assume $\mu =\nu$ , the above reduces to: \begin{align} \frac{dS}{dt} &= -\beta S I + \nu I +(\xi+\nu) R \\[2ex] \frac{dI}{dt} &= \beta S I -(\gamma +\nu) I \\[2ex] \frac{dR}{dt} &= \gamma I -(\xi +\nu) R \end{align} with equilibrium points: \begin{align*} e_1 : \left( S_1^*, I_1^*, R_1^*\right)&= \left(1, 0, 0\right), \\[2ex] e_2 : \left( S_2^*, I_2^*, R_2^*\right)&= \left(\frac{\left(\gamma + \nu\right)}{\beta}, \frac{\left(\gamma+\nu \right) \left( \xi + \nu \right) \left( \frac{\beta}{\gamma+\nu} -1 \right) }{\beta\left(\gamma + \xi+\nu \right)}, \frac{\gamma \left(\gamma+\nu \right)\left( \frac{\beta}{\gamma+\nu} -1 \right) }{\beta\left(\gamma + \xi+\nu \right)}\right) \end{align*} So my question(s) are; how do I find suitable Lyapunov functions for systems (i), (ii) and (iv) similar to my example in (iii)? Can you also show the full solution to systems (i), (ii) and (iv) like how I presented in (iii)? This will be appreciated! EDIT As per Hans comments for (i): By substituting $S=N-I$ into (2.4) we have \begin{align} \frac{dI}{dt} &= (\beta - \nu)I - \beta I^2 \end{align} Solving (2.5) with initial condition $I(0)= I_0$ analytically, we have the solution to the system: \begin{align} I(t) &= \frac{I_0 (\beta - \nu)}{\beta I_0 - e^{-\left(\beta - \nu\right)t} \left[ \beta I_0 - \left(\beta - \nu\right)\right]}.\\[2ex] S(t) &= 1-I(t). \end{align} We can make inferences of the long term behaviour of this model by examining the possible values of $\left(\beta -\nu\right)$ , that is, of course when (2.6) is feasible. We have two cases \begin{align*} \beta - \nu & < 0 \\[2ex] \beta - \nu & > 0 \end{align*} If $\beta - \nu < 0$ , then $e^{-\left(\beta - \nu\right)t} \rightarrow \infty \text{ as } t \rightarrow \infty$ hence \begin{align} \lim_{t \to \infty} I(t) = 0. \end{align} If $\beta - \nu > 0$ , then $e^{-\left(\beta - \nu\right)t} \rightarrow 0 \text{ as } t \rightarrow \infty$ hence we have the limit \begin{align} \lim_{t \to \infty} I(t) = \frac{I_0\left(\beta-\nu\right)}{\beta I_0} = 1-\frac{\nu}{\beta}. \end{align} $R_0 \leq 1$ : $R_0 >1$ : For (ii): Analogously as in section 2.1, we obtain the complete solution to our system: \begin{align} I(t) &= \frac{I_0 (\beta - \gamma-\nu)}{\beta I_0 - e^{-\left(\beta - \gamma -\nu\right)t} \left[ \beta I_0 - \left(\beta -\gamma - \nu\right)\right]}.\\[2ex] S(t) &= 1-I(t). \end{align} As before, we make inferences of the long term behaviour of this model by examining the possible values of $\left(\beta -\gamma -\nu\right)$ , that is, of course when (2.14) is feasible. We have two cases \begin{align*} \beta - \gamma-\nu & < 0 \\[2ex] \beta - \gamma -\nu & > 0 \end{align*} If $\beta - \gamma-\nu < 0$ , then $e^{-\left(\beta - \gamma -\nu\right)t} \rightarrow \infty \text{ as } t \rightarrow \infty$ hence \begin{align} \lim_{t \to \infty} I(t) = 0. \end{align} If $\beta - \gamma -\nu > 0$ , then $e^{-\left(\beta - \gamma -\nu\right)t} \rightarrow 0 \text{ as } t \rightarrow \infty$ hence we have the limit \begin{align} \lim_{t \to \infty} I(t) = \frac{I_0\left(\beta-\gamma -\nu\right)}{\beta I_0} = 1-\frac{\gamma+\nu}{\beta}. \end{align} $R_0 \leq 1$ : $R_0 >1$ : Ignore the wrong equation tags.. But I'm not sure whether this edit is proving global stability for models (i) and (ii)..","SECOND BOUNTY! i) SI MODEL Consider Where is the total population. If we assume , the above reduces to: The equilibrium points: where . The set is our domain of definition. This set is a positively invariant set for our system. Now to prove the global (asymptotically) stability of is straightforward, we use the function and the result follows. Now my question is, how would I construct a Lyapunov function for ? I know it should be in the form of a Volterra function but, how can I choose a parricular one? any ideas? ii) SIS MODEL Consider Where is the total population. If we assume , the above reduces to: The equilibrium points: where . Again, as in case (i) we have the set as our domain of definition. This set is a positively invariant set for our system. Analogously as earlier, to prove the global (asymptotically) stability of is straightforward, we use the function and the result follows. How would I chose a suitable Lyapunov function for this model to analyse the endemic equilibrium ? Well, if we find a suitable Lyapunov function for case (i) then we can just replace with for this system. iii) SIR MODEL Consider Where is the total population. If we assume , the above reduces to: Using to kill degeneracy, this further reduces to The equilibrium points read where . Again, as in case (i) and (ii) we have the set as our domain of definition. This set is a positively invariant set for our system. Theorem If , then the disease-free equilibrium is globally asymptotically stable in . Proof We use the function and the result follows. Theorem If , then the endemic equilibrium is globally asymptotically stable in the interior of . Proof To prove the global asymptotically stability for , consider the Lyapunov function (in the form of a Volterra function): working out the time derivatives along our reduced system, we arrive to: We see is negative except when takes on the equilibrium values so that , so we have semi-definiteness. The largest compact invariant set in so that is is the singleton . Hence, concluding from LaSalle's invariance principle, is globally asymptotically stable in . iv) SIRS MODEL Consider Where is the total population. If we assume , the above reduces to: with equilibrium points: So my question(s) are; how do I find suitable Lyapunov functions for systems (i), (ii) and (iv) similar to my example in (iii)? Can you also show the full solution to systems (i), (ii) and (iv) like how I presented in (iii)? This will be appreciated! EDIT As per Hans comments for (i): By substituting into (2.4) we have Solving (2.5) with initial condition analytically, we have the solution to the system: We can make inferences of the long term behaviour of this model by examining the possible values of , that is, of course when (2.6) is feasible. We have two cases If , then hence If , then hence we have the limit : : For (ii): Analogously as in section 2.1, we obtain the complete solution to our system: As before, we make inferences of the long term behaviour of this model by examining the possible values of , that is, of course when (2.14) is feasible. We have two cases If , then hence If , then hence we have the limit : : Ignore the wrong equation tags.. But I'm not sure whether this edit is proving global stability for models (i) and (ii)..","\begin{align}
\frac{dS}{dt} &= \mu N -\frac{\beta S I}{N} - \nu S\\[2ex]
\frac{dI}{dt} &= \frac{\beta S I}{N} -\nu I
\end{align} N=S+I \mu =\nu \begin{align}
\frac{dS}{dt} &= -\beta S I + \nu I\\[2ex]
\frac{dI}{dt} &= \beta S I -\nu I
\end{align} \begin{align*}
e_1 : \left( S_1^*, I_1^*\right)&= \left(1, 0\right), \\[2ex]
e_2 : \left( S_2^*, I_2^*\right)&= \left(\frac{\nu }{\beta}, \frac{\nu}{\beta}\left(\frac{\beta}{\nu}-1 \right)\right)
\end{align*} \mathcal{R}_0 = \beta/\nu \Omega = \left\lbrace \left(S,I\right)\in \mathbb{R}_+^2 : S\geq 0, I \geq 0, S+I \leq 1  \right\rbrace e_1 V = I e_2 \begin{align}
\frac{dS}{dt} &= \mu N -\frac{\beta S I}{N}+ \gamma I - \nu S\\[2ex]
\frac{dI}{dt} &= \frac{\beta S I}{N} -(\gamma+\nu) I
\end{align} N=S+I \mu =\nu \begin{align}
\frac{dS}{dt} &= -\beta S I + (\gamma +\nu) I\\[2ex]
\frac{dI}{dt} &= \beta S I -(\gamma+\nu) I
\end{align} \begin{align*}
e_1 : \left( S_1^*, I_1^*\right)&= \left(1, 0\right), \\[2ex]
e_2 : \left( S_2^*, I_2^*\right)&= \left(\frac{\left(\gamma+\nu\right)}{\beta}, \frac{\left(\gamma+\nu\right)}{\beta}\left(\frac{\beta}{\gamma+\nu} -1\right)\right),
\end{align*} \mathcal{R}_0 = \beta/(\gamma+\nu) \Omega = \left\lbrace \left(S,I\right)\in \mathbb{R}_+^2 : S\geq 0, I \geq 0, S+I \leq 1  \right\rbrace e_1 V = I e_2 \nu \gamma+\nu \begin{align}
\frac{dS}{dt} &= \mu N -\frac{\beta S I}{N} - \nu S\\[2ex]
\frac{dI}{dt} &= \frac{\beta S I}{N} -(\gamma+\nu) I\\[2ex]
\frac{dR}{dt} &= \gamma I -\nu R
\end{align} N=S+I+R \mu =\nu \begin{align}
\frac{dS}{dt} &= -\beta S I + \nu I +\nu R \\[2ex]
\frac{dI}{dt} &= \beta S I -(\gamma +\nu) I\\[2ex]
\frac{dR}{dt} &= \gamma I -\nu R -\xi R
\end{align} R=N-S-I \begin{align}
\frac{dS}{dt} &= -\beta S I + \nu  -\nu S \\[2ex]
\frac{dI}{dt} &= \beta S I -(\gamma +\nu) I\\[2ex]
\end{align} \begin{align*}
e_1 : \left( S_1^*, I_1^*, R_1^*\right)&= \left(1, 0, 0\right), \\[2ex]
e_2 : \left( S_2^*, I_2^*, R_2^*\right)&= \left(\frac{\left(\gamma+\nu\right)}{\beta}, \frac{\nu}{\beta}\left(\frac{\beta}{\gamma +\nu}-1\right), \frac{\gamma}{\beta}\left(\frac{\beta}{\gamma +\nu}-1\right)\right)\\[1ex]
\end{align*} \mathcal{R}_0 = \beta/(\gamma+\nu) \Omega = \left\lbrace \left(S,I\right)\in \mathbb{R}_+^2 : S\geq 0, I \geq 0, S+I \leq 1  \right\rbrace \mathcal{R}_0 \leq 1 e_1 \Omega V = I \mathcal{R}_0 > 1 e_2 \Omega e_2 V(S,I) = \left(S-S_2^*\right)+ \left( I-I_2^*\right) -S_2^* \ln \frac{S}{S_2^*} - I_2^* \ln \frac{I}{I_2^*}.  \dot V = -\beta (S_2^* -S)^2 [\frac{1}{S}] \leq 0  \dot V \dot V \dot V =0 \Omega \dot V 0 \lbrace{ e_2\rbrace} e_2 \Omega \begin{align}
\frac{dS}{dt} &= \mu N -\frac{\beta S I}{N} +\xi R - \nu S\\[2ex]
\frac{dI}{dt} &= \frac{\beta S I}{N} -(\gamma+\nu) I\\[2ex]
\frac{dR}{dt} &= \gamma I-\xi R -\nu R
\end{align} N=S+I+R \mu =\nu \begin{align}
\frac{dS}{dt} &= -\beta S I + \nu I +(\xi+\nu) R \\[2ex]
\frac{dI}{dt} &= \beta S I -(\gamma +\nu) I \\[2ex]
\frac{dR}{dt} &= \gamma I -(\xi +\nu) R
\end{align} \begin{align*}
e_1 : \left( S_1^*, I_1^*, R_1^*\right)&= \left(1, 0, 0\right), \\[2ex]
e_2 : \left( S_2^*, I_2^*, R_2^*\right)&= \left(\frac{\left(\gamma + \nu\right)}{\beta}, \frac{\left(\gamma+\nu \right) \left( \xi + \nu \right) \left( \frac{\beta}{\gamma+\nu} -1 \right) }{\beta\left(\gamma + \xi+\nu \right)}, \frac{\gamma \left(\gamma+\nu \right)\left( \frac{\beta}{\gamma+\nu} -1 \right) }{\beta\left(\gamma + \xi+\nu \right)}\right)
\end{align*} S=N-I \begin{align}
\frac{dI}{dt} &= (\beta - \nu)I - \beta I^2
\end{align} I(0)= I_0 \begin{align}
I(t) &= \frac{I_0 (\beta - \nu)}{\beta I_0 - e^{-\left(\beta - \nu\right)t} \left[ \beta I_0 - \left(\beta - \nu\right)\right]}.\\[2ex]
S(t) &= 1-I(t).
\end{align} \left(\beta -\nu\right) \begin{align*}
\beta - \nu & < 0 \\[2ex]
\beta - \nu & > 0
\end{align*} \beta - \nu < 0 e^{-\left(\beta - \nu\right)t} \rightarrow \infty \text{ as } t \rightarrow \infty \begin{align}
\lim_{t \to \infty} I(t) = 0.
\end{align} \beta - \nu > 0 e^{-\left(\beta - \nu\right)t} \rightarrow 0 \text{ as } t \rightarrow \infty \begin{align}
\lim_{t \to \infty} I(t) = \frac{I_0\left(\beta-\nu\right)}{\beta I_0} = 1-\frac{\nu}{\beta}.
\end{align} R_0 \leq 1 R_0 >1 \begin{align}
I(t) &= \frac{I_0 (\beta - \gamma-\nu)}{\beta I_0 - e^{-\left(\beta - \gamma -\nu\right)t} \left[ \beta I_0 - \left(\beta -\gamma - \nu\right)\right]}.\\[2ex]
S(t) &= 1-I(t).
\end{align} \left(\beta -\gamma -\nu\right) \begin{align*}
\beta - \gamma-\nu & < 0 \\[2ex]
\beta - \gamma -\nu & > 0
\end{align*} \beta - \gamma-\nu < 0 e^{-\left(\beta - \gamma -\nu\right)t} \rightarrow \infty \text{ as } t \rightarrow \infty \begin{align}
\lim_{t \to \infty} I(t) = 0.
\end{align} \beta - \gamma -\nu > 0 e^{-\left(\beta - \gamma -\nu\right)t} \rightarrow 0 \text{ as } t \rightarrow \infty \begin{align}
\lim_{t \to \infty} I(t) = \frac{I_0\left(\beta-\gamma -\nu\right)}{\beta I_0} = 1-\frac{\gamma+\nu}{\beta}.
\end{align} R_0 \leq 1 R_0 >1","['ordinary-differential-equations', 'dynamical-systems']"
90,Inverse derivation of differential equation $f(x)=\frac{dy}{dx} +\frac{d^2y}{dx^2}+\frac{d^3y}{dx^3}$,Inverse derivation of differential equation,f(x)=\frac{dy}{dx} +\frac{d^2y}{dx^2}+\frac{d^3y}{dx^3},The following function is given: $$ f(x)=\frac{dy}{dx} +\frac{d^2y}{dx^2}+\frac{d^3y}{dx^3}$$ I'am looking for the inverse function in Leibniz writing style. Here is my attempt: $$ f^{-1}(x)=\frac{dx}{dy} -\frac{d^2x}{dy^2} {\Biggl(\frac{dy}{dx}}\Biggl)^3  -\frac{d^3x}{dy^3} {\Biggl(\frac{dy}{dx}}\Biggl)^4+3{\Biggl(\frac{d^2x}{dy^2}}\Biggl)^2  {\Biggl(\frac{dy}{dx}}\Biggl)^5 $$ Is this correct?,The following function is given: I'am looking for the inverse function in Leibniz writing style. Here is my attempt: Is this correct?, f(x)=\frac{dy}{dx} +\frac{d^2y}{dx^2}+\frac{d^3y}{dx^3}  f^{-1}(x)=\frac{dx}{dy} -\frac{d^2x}{dy^2} {\Biggl(\frac{dy}{dx}}\Biggl)^3  -\frac{d^3x}{dy^3} {\Biggl(\frac{dy}{dx}}\Biggl)^4+3{\Biggl(\frac{d^2x}{dy^2}}\Biggl)^2  {\Biggl(\frac{dy}{dx}}\Biggl)^5 ,"['ordinary-differential-equations', 'partial-differential-equations', 'inverse-function']"
91,"Prove that for all $p\in M$ there's an $M$-open set $p\in U\subset M$ and $\varepsilon >0$ such that $(-\varepsilon,\varepsilon)\times U\subset D$",Prove that for all  there's an -open set  and  such that,"p\in M M p\in U\subset M \varepsilon >0 (-\varepsilon,\varepsilon)\times U\subset D","I'm reading the book ""Introduction to Differential Geometry"" written by J.W. Robbin and D.A. Salamon. In the page 41 (Lemma 2.4.10) there's the following assertion: Suppose that $M\subset\mathbb{R}^n$ is a smooth submanifold and $X:M\to \mathbb{R}^n$ is a smooth vector field. Define for all $p\in M$ : $$\color{red}{I_X(p)}:=\bigcup \big\{I\subset \mathbb{R} \,  (\text{open interval}):\text{there's a integral curve }\gamma \in  M^I\text{ of }X\text{ such that }\gamma (0)=p\big\}.$$ Then for all $p\in M$ there exists an $M$ -open neighborhood $U_p\subseteq M$ of $p$ and a constant $\varepsilon _p>0$ such that $(-\varepsilon _p,\varepsilon _p)\times U_p\subseteq D:=\bigcup _{p\in  M}I_X(p)\times \{p\}$ . I tried to show that $U_p:=\big\{q\in M:I_X(q)=I_X(p)\big\}$ is open in $M$ because if this is true then that assertion is also true. However I failed  and I don't know how to prove that assertion. Probably this is a consequence of a known theorem about ordinary differential equations, however I didn't study about ODE yet. I say this because, given a parametrization $\psi :V\to \psi [V]\subseteq M$ of $M$ , we can define $f:V\to \mathbb{R}^n$ by $f(x):=(d\psi )_x^{-1}\circ X\circ \psi (x)$ (which is smooth) and guarantee the existence of integral curves of $X$ with the system $\begin{cases}\dot{x}=f(x)\\x(0)=p\in \psi [V]\end{cases}$ and using the Picard-Lindelöf Theorem. Thank you for your attention!","I'm reading the book ""Introduction to Differential Geometry"" written by J.W. Robbin and D.A. Salamon. In the page 41 (Lemma 2.4.10) there's the following assertion: Suppose that is a smooth submanifold and is a smooth vector field. Define for all : Then for all there exists an -open neighborhood of and a constant such that . I tried to show that is open in because if this is true then that assertion is also true. However I failed  and I don't know how to prove that assertion. Probably this is a consequence of a known theorem about ordinary differential equations, however I didn't study about ODE yet. I say this because, given a parametrization of , we can define by (which is smooth) and guarantee the existence of integral curves of with the system and using the Picard-Lindelöf Theorem. Thank you for your attention!","M\subset\mathbb{R}^n X:M\to \mathbb{R}^n p\in M \color{red}{I_X(p)}:=\bigcup \big\{I\subset \mathbb{R} \,
 (\text{open interval}):\text{there's a integral curve }\gamma \in
 M^I\text{ of }X\text{ such that }\gamma (0)=p\big\}. p\in M M U_p\subseteq M p \varepsilon _p>0 (-\varepsilon _p,\varepsilon _p)\times U_p\subseteq D:=\bigcup _{p\in
 M}I_X(p)\times \{p\} U_p:=\big\{q\in M:I_X(q)=I_X(p)\big\} M \psi :V\to \psi [V]\subseteq M M f:V\to \mathbb{R}^n f(x):=(d\psi )_x^{-1}\circ X\circ \psi (x) X \begin{cases}\dot{x}=f(x)\\x(0)=p\in \psi [V]\end{cases}","['ordinary-differential-equations', 'differential-geometry', 'manifolds', 'smooth-manifolds', 'submanifold']"
92,ODE-separation of variables proof,ODE-separation of variables proof,,"Let's assume an ODE of the type $$y'=g(x)h(y(x)),$$ where $g:I\to\mathbb{R}$ , $h:J\to\mathbb{R}$ are continuous functions and ${\xi\choose\eta}\in I\times J$ . In the case of $h(\eta)=0$ the solution is clear. So let's assume $h(\eta)\neq 0$ . Then, we know that by solving the equation: $$ \int_{\eta}^y\frac{1}{h(s)}ds=\int_{\xi}^xg(t)dt $$ with respect to $y$ delivers a solution of the ODE. In our lecture the professor has proven this statement by constructing another function, namely $$H(u):=\int_{\eta}^u\frac{1}{h(s)}ds$$ and then showing its invertibility. After some additional steps he finally comes to the conclusion that $$y:=H^{-1}\left(\int_{\xi}^xg(t)dt\right)$$ is a (unique) solution of the ODE. In the literature this seems a standard way of proving the theorem. However, I was wondering why we cannot simply say that in the case of $h(\eta)\neq 0$ it directly follows from the implicit function theorem? EDIT: Some more details regarding the implicit function theorem: Let be $y(\xi)=\eta$ and $F:\mathbb{R}^2\to\mathbb{R}$ , where $$ F(x,y)=\int_{\eta}^y\frac{1}{h(s)}ds-\int_{\xi}^xg(t)dt. $$ Now we look at $F(x,y)\overset{!}{=}0$ and see that $F(\xi,y(\xi))=0$ . All assumptions of the implicit function theorem are satisfied (I have skipped the details), so we know there exists a function $f$ such that $F(x,f(x))=0$ . Further, we know how the derivative of $f$ looks like, namely $f'(x)= (-1) \left(\left(\int_{\eta}^{f(x)}\frac{1}{h(s)}ds\right)'\right)^{-1}\left(-\int_{\xi}^xg(t)dt\right)'=f(x) h(f(x)).$ Hence, the implicit function $f$ is a solution of the ODE. (I know this is a bit sloppy but I hope it explains the main idea)","Let's assume an ODE of the type where , are continuous functions and . In the case of the solution is clear. So let's assume . Then, we know that by solving the equation: with respect to delivers a solution of the ODE. In our lecture the professor has proven this statement by constructing another function, namely and then showing its invertibility. After some additional steps he finally comes to the conclusion that is a (unique) solution of the ODE. In the literature this seems a standard way of proving the theorem. However, I was wondering why we cannot simply say that in the case of it directly follows from the implicit function theorem? EDIT: Some more details regarding the implicit function theorem: Let be and , where Now we look at and see that . All assumptions of the implicit function theorem are satisfied (I have skipped the details), so we know there exists a function such that . Further, we know how the derivative of looks like, namely Hence, the implicit function is a solution of the ODE. (I know this is a bit sloppy but I hope it explains the main idea)","y'=g(x)h(y(x)), g:I\to\mathbb{R} h:J\to\mathbb{R} {\xi\choose\eta}\in I\times J h(\eta)=0 h(\eta)\neq 0 
\int_{\eta}^y\frac{1}{h(s)}ds=\int_{\xi}^xg(t)dt
 y H(u):=\int_{\eta}^u\frac{1}{h(s)}ds y:=H^{-1}\left(\int_{\xi}^xg(t)dt\right) h(\eta)\neq 0 y(\xi)=\eta F:\mathbb{R}^2\to\mathbb{R} 
F(x,y)=\int_{\eta}^y\frac{1}{h(s)}ds-\int_{\xi}^xg(t)dt.
 F(x,y)\overset{!}{=}0 F(\xi,y(\xi))=0 f F(x,f(x))=0 f f'(x)= (-1) \left(\left(\int_{\eta}^{f(x)}\frac{1}{h(s)}ds\right)'\right)^{-1}\left(-\int_{\xi}^xg(t)dt\right)'=f(x) h(f(x)). f","['real-analysis', 'ordinary-differential-equations', 'proof-explanation', 'alternative-proof']"
93,How can I make the following expression to be a martingale?,How can I make the following expression to be a martingale?,,"Let $B$ denotes a Brownian motion in a filtration $\mathcal{F}$ and we define $X$ as the following process: $$X_{t}=\int_{0}^{t}h\left(B_{s}\right)ds,$$ where $h:\mathbb{R}\mapsto\left[1,\infty\right)$ is a smooth function. $M$ is definied as the following: $$M_{t}=\int_{0}^{t}e^{-X_{s}}ds+v\left(B_{t}\right)e^{-X_{t}},$$ where $v:\mathbb{R}\mapsto\mathbb{R}$ . What does $v$ have to satisfy if we want $M$ to be a martingale in $\mathcal{F}$ ? Here is my solution so far which may contains a lot of mistakes, misunderstandings and/or inaccuracy. If we want $M$ to be a martingale, then $M$ shouldn't contain any $\int_{0}^{t}\ldots ds$ member in its decomposition. Unfortunatelly there is $\int_{0}^{t}e^{-X_{s}}ds,$ which is a nonnegative process with finite total variation. We know it has finite total variation because we Lebesgue integrate a positive expression. Even $X$ has finite total variation because of the same reason. $X$ is nonnegative, because $h:\mathbb{R}\mapsto\left[1,\infty\right)$ , so $X$ is an integration of positive values. We can make $\int_{0}^{t}e^{-X_{s}}ds$ disappear in $M$ , if $M$ is the constant $0$ process, therefore $$\int_{0}^{t}e^{-X_{s}}ds=-v\left(B_{t}\right)e^{-X_{t}}.$$ I do hope this is the only way to make $M$ to be a martingale, but at this point I am not so sure. We want to find the Itô decomposition of $-v\left(B_{t}\right)e^{-X_{t}}$ in the form of $g\left(X_{t},B_{t}\right)$ . I got the following result for this: $$v\left(B_{t}\right)e^{-X_{t}}=\int_{0}^{t}v\left(B_{s}\right)e^{-X_{s}}h\left(B_{s}\right)-\frac{1}{2}\frac{\partial^{2}v}{\partial y^{2}}\left(B_{s}\right)e^{-X_{s}}ds+\int_{0}^{t}-\frac{\partial v}{\partial y}\left(B_{s}\right)e^{-X_{s}}dB_{s}, $$ which should be equal with $\int_{0}^{t}e^{-X_{s}}ds$ for all $t$ . As far as I know there is a statement, (I think we called it Doob-lemma, but I think I am mistaken here, because I don't really remember this crystal clean...) that the semimartingale decomposition is unique, so $$\begin{cases} e^{-x}=v\left(y\right)e^{-x}h\left(y\right)-\frac{1}{2}\frac{\partial^{2}v}{\partial y^{2}}\left(y\right)e^{-x}\\ 0=-\frac{\partial v}{\partial y}\left(y\right)e^{-x} \end{cases}\Longrightarrow\begin{cases} 1=v\left(y\right)h\left(y\right)-\frac{1}{2}\frac{\partial^{2}v}{\partial y^{2}}\left(y\right)\\ 0=\frac{\partial v}{\partial y}\left(y\right) \end{cases}.$$ We have to solve this system of differential equations. From the last equation we know $v$ is a constant function. Furthermore, $h$ has to be the reciproke of $v$ , because if $v$ is constant, than $\frac{\partial^{2}v}{\partial y^{2}}\left(y\right)=0$ , and $1=const\cdot h\left(y\right)$ , so $h$ is a constant as well. $h$ can't be the constant $0$ , because $h:\mathbb{R}\mapsto\left[1,\infty\right)$ . First I thought we made this clause, because we didn't want $X$ to cross the $0$ , therefore in the case of division we can use Itô's formula appropriately. This is my solution, but I don't think it is right. I feel like a made a lot of mistakes but I want to get a clear solution...I don't want this exercise to be unsolved. Some corrections: You have written in the comments quite well, if $v\left(B_{t}\right)e^{-X_{t}}$ has the form $-\int_{0}^{t}e^{-X_{s}}ds+M_{t}$ , then I get back my own $M$ martingale which means $M$ doesn't necessary have to be the constant zero process. As I did before, the dynamic of $v\left(B_{t}\right)e^{-X_{t}}$ can be written as $$\int_{0}^{t}-v\left(B_{s}\right)e^{-X_{s}}h\left(B_{s}\right)+\frac{1}{2}\frac{d^{2}v}{dy^{2}}\left(B_{s}\right)e^{-X_{s}}ds+\int_{0}^{t}\frac{dv}{dy}\left(B_{s}\right)e^{-X_{s}}dB_{s},$$ which leads back to the differential equation: $$v\left(y\right)h\left(y\right)-\frac{1}{2}\frac{d^{2}v}{dy^{2}}\left(y\right)=1,$$ thanks to the uniqueness of Doob-Meyer decomposition and the fact, that the expressions above are measurable functions of a normal distributed random variable, which is absolute continuous to the Lebesgue measure (moreover they are equivalent). We made the $\int_{0}^{t}\ldots ds$ member disappear, but it is not enough for $M$ to be a martingale. However, I know a condition, which makes $M$ to be a martingale. We know $M_{t}=\int_{0}^{t}\frac{dv}{dy}\left(B_{s}\right)e^{-X_{s}}dB_{s}$ from the dynamic mentioned above. If $\frac{dv}{dy}\left(B_{s}\right)e^{-X_{s}}$ is progressive measurable, and $$\mathbb{E}\left[\int_{0}^{t}\left(\frac{dv}{dy}\left(B_{s}\right)e^{-X_{s}}\right)^{2}ds\right]<\infty,$$ then $M$ is a martingale. If a process is adapted and continuous, then the process is progressive measurable, so it is enough to $\frac{dv}{dy}\left(y\right)$ be continuous, because we would get compositions of continuous processes which is also continuous. Finally, if I did everything alright, and $v$ satisfies the conditions below, than $M$ is martingale. 1. $v$ is a solution of the differential equation: $$v\left(y\right)\cdot h\left(y\right)-\frac{1}{2}\frac{d^{2}v}{dy^{2}}\left(y\right)=1.$$ 2. $\frac{dv}{dy}\left(y\right)$ is continuous. 3. $\mathbb{E}\left[\left(\frac{dv}{dy}\left(B_{s}\right)\right)^{2}e^{-2\int_{0}^{s}h\left(B_{u}\right)du}\right]<\infty$ for all $s$ , thanks to the Fubini theorem. The conditions above perhaps can be generalized, is it a better solution?","Let denotes a Brownian motion in a filtration and we define as the following process: where is a smooth function. is definied as the following: where . What does have to satisfy if we want to be a martingale in ? Here is my solution so far which may contains a lot of mistakes, misunderstandings and/or inaccuracy. If we want to be a martingale, then shouldn't contain any member in its decomposition. Unfortunatelly there is which is a nonnegative process with finite total variation. We know it has finite total variation because we Lebesgue integrate a positive expression. Even has finite total variation because of the same reason. is nonnegative, because , so is an integration of positive values. We can make disappear in , if is the constant process, therefore I do hope this is the only way to make to be a martingale, but at this point I am not so sure. We want to find the Itô decomposition of in the form of . I got the following result for this: which should be equal with for all . As far as I know there is a statement, (I think we called it Doob-lemma, but I think I am mistaken here, because I don't really remember this crystal clean...) that the semimartingale decomposition is unique, so We have to solve this system of differential equations. From the last equation we know is a constant function. Furthermore, has to be the reciproke of , because if is constant, than , and , so is a constant as well. can't be the constant , because . First I thought we made this clause, because we didn't want to cross the , therefore in the case of division we can use Itô's formula appropriately. This is my solution, but I don't think it is right. I feel like a made a lot of mistakes but I want to get a clear solution...I don't want this exercise to be unsolved. Some corrections: You have written in the comments quite well, if has the form , then I get back my own martingale which means doesn't necessary have to be the constant zero process. As I did before, the dynamic of can be written as which leads back to the differential equation: thanks to the uniqueness of Doob-Meyer decomposition and the fact, that the expressions above are measurable functions of a normal distributed random variable, which is absolute continuous to the Lebesgue measure (moreover they are equivalent). We made the member disappear, but it is not enough for to be a martingale. However, I know a condition, which makes to be a martingale. We know from the dynamic mentioned above. If is progressive measurable, and then is a martingale. If a process is adapted and continuous, then the process is progressive measurable, so it is enough to be continuous, because we would get compositions of continuous processes which is also continuous. Finally, if I did everything alright, and satisfies the conditions below, than is martingale. 1. is a solution of the differential equation: 2. is continuous. 3. for all , thanks to the Fubini theorem. The conditions above perhaps can be generalized, is it a better solution?","B \mathcal{F} X X_{t}=\int_{0}^{t}h\left(B_{s}\right)ds, h:\mathbb{R}\mapsto\left[1,\infty\right) M M_{t}=\int_{0}^{t}e^{-X_{s}}ds+v\left(B_{t}\right)e^{-X_{t}}, v:\mathbb{R}\mapsto\mathbb{R} v M \mathcal{F} M M \int_{0}^{t}\ldots ds \int_{0}^{t}e^{-X_{s}}ds, X X h:\mathbb{R}\mapsto\left[1,\infty\right) X \int_{0}^{t}e^{-X_{s}}ds M M 0 \int_{0}^{t}e^{-X_{s}}ds=-v\left(B_{t}\right)e^{-X_{t}}. M -v\left(B_{t}\right)e^{-X_{t}} g\left(X_{t},B_{t}\right) v\left(B_{t}\right)e^{-X_{t}}=\int_{0}^{t}v\left(B_{s}\right)e^{-X_{s}}h\left(B_{s}\right)-\frac{1}{2}\frac{\partial^{2}v}{\partial y^{2}}\left(B_{s}\right)e^{-X_{s}}ds+\int_{0}^{t}-\frac{\partial v}{\partial y}\left(B_{s}\right)e^{-X_{s}}dB_{s},
 \int_{0}^{t}e^{-X_{s}}ds t \begin{cases}
e^{-x}=v\left(y\right)e^{-x}h\left(y\right)-\frac{1}{2}\frac{\partial^{2}v}{\partial y^{2}}\left(y\right)e^{-x}\\
0=-\frac{\partial v}{\partial y}\left(y\right)e^{-x}
\end{cases}\Longrightarrow\begin{cases}
1=v\left(y\right)h\left(y\right)-\frac{1}{2}\frac{\partial^{2}v}{\partial y^{2}}\left(y\right)\\
0=\frac{\partial v}{\partial y}\left(y\right)
\end{cases}. v h v v \frac{\partial^{2}v}{\partial y^{2}}\left(y\right)=0 1=const\cdot h\left(y\right) h h 0 h:\mathbb{R}\mapsto\left[1,\infty\right) X 0 v\left(B_{t}\right)e^{-X_{t}} -\int_{0}^{t}e^{-X_{s}}ds+M_{t} M M v\left(B_{t}\right)e^{-X_{t}} \int_{0}^{t}-v\left(B_{s}\right)e^{-X_{s}}h\left(B_{s}\right)+\frac{1}{2}\frac{d^{2}v}{dy^{2}}\left(B_{s}\right)e^{-X_{s}}ds+\int_{0}^{t}\frac{dv}{dy}\left(B_{s}\right)e^{-X_{s}}dB_{s}, v\left(y\right)h\left(y\right)-\frac{1}{2}\frac{d^{2}v}{dy^{2}}\left(y\right)=1, \int_{0}^{t}\ldots ds M M M_{t}=\int_{0}^{t}\frac{dv}{dy}\left(B_{s}\right)e^{-X_{s}}dB_{s} \frac{dv}{dy}\left(B_{s}\right)e^{-X_{s}} \mathbb{E}\left[\int_{0}^{t}\left(\frac{dv}{dy}\left(B_{s}\right)e^{-X_{s}}\right)^{2}ds\right]<\infty, M \frac{dv}{dy}\left(y\right) v M v v\left(y\right)\cdot h\left(y\right)-\frac{1}{2}\frac{d^{2}v}{dy^{2}}\left(y\right)=1. \frac{dv}{dy}\left(y\right) \mathbb{E}\left[\left(\frac{dv}{dy}\left(B_{s}\right)\right)^{2}e^{-2\int_{0}^{s}h\left(B_{u}\right)du}\right]<\infty s","['ordinary-differential-equations', 'stochastic-processes', 'martingales', 'local-martingales']"
94,Are there numerical methods to solve a differential equation which are actually faster than numerically computing its analytical solution?,Are there numerical methods to solve a differential equation which are actually faster than numerically computing its analytical solution?,,"In the topic of numerical solutions of ODE and PDE, usually it's said that many times it's impractical to try to find an analytical solution due to the complexity of the boundary conditions, or even outright impossible due to the nature of the equation, for example: \begin{equation}     y'=e^{-x^{2}} \end{equation} doesn't have an analytical solution we can write using other known functions. It is implied, though, that an analytical solution it's always preferable whenever possible to speed up calculations. Given that known solutions of differential equations can turn out to be a really complicated mess of nested trigonometric and exponential functions (as an example, this is the solution to the heat equation in 2 spatial coordinates: $$\theta(x,y)=\frac{2}{\pi}\sum_{n=1}^\infty\frac{(-1)^{n+1}+1}{n}\sin\frac{n\pi x}{L}\frac{\sinh(n\pi y/L)}{\sinh(n\pi W/L)}$$ ) and given that, for practical purposes, to output a value, the known functions like sines, cosine and exponentials which make up the solution eventually have to be computed numerically, can there be an equation for which the analytical solution is known, but so complex that solving the DE with a common numerical method like Euler or Runge-Kutta turns out to be actually faster than computing the analytical solution using Taylor series? [Edit: It was pointed out that $ y'=e^{-x^{2}} $ does indeed have an analytical solution, just not a closed form in terms of elementary functions]","In the topic of numerical solutions of ODE and PDE, usually it's said that many times it's impractical to try to find an analytical solution due to the complexity of the boundary conditions, or even outright impossible due to the nature of the equation, for example: doesn't have an analytical solution we can write using other known functions. It is implied, though, that an analytical solution it's always preferable whenever possible to speed up calculations. Given that known solutions of differential equations can turn out to be a really complicated mess of nested trigonometric and exponential functions (as an example, this is the solution to the heat equation in 2 spatial coordinates: ) and given that, for practical purposes, to output a value, the known functions like sines, cosine and exponentials which make up the solution eventually have to be computed numerically, can there be an equation for which the analytical solution is known, but so complex that solving the DE with a common numerical method like Euler or Runge-Kutta turns out to be actually faster than computing the analytical solution using Taylor series? [Edit: It was pointed out that does indeed have an analytical solution, just not a closed form in terms of elementary functions]","\begin{equation}
    y'=e^{-x^{2}}
\end{equation} \theta(x,y)=\frac{2}{\pi}\sum_{n=1}^\infty\frac{(-1)^{n+1}+1}{n}\sin\frac{n\pi x}{L}\frac{\sinh(n\pi y/L)}{\sinh(n\pi W/L)}  y'=e^{-x^{2}} ","['ordinary-differential-equations', 'partial-differential-equations', 'numerical-methods', 'numerical-calculus']"
95,Showing there is exactly one limit cycle for system in unit cirlce,Showing there is exactly one limit cycle for system in unit cirlce,,"Consider the system defined by below equations: $$\dot{x}=x+y-a^2x(x^2+y^2)$$ $$\dot{y}=-x+y-a^2y(x^2+y^2), \quad a>1$$ I'm asked to show there is only $1$ limit cycle in unit circle. I think using Poincare-Bendixon implies there is a limit cycle but I have difficulty in showing that there is no equilibrium point in unit circle. I have no idea for showing there is only $1$ such a limit cycle. I think using polar coordinates may help but I don't know how to use it. Any help is appreciated.",Consider the system defined by below equations: I'm asked to show there is only limit cycle in unit circle. I think using Poincare-Bendixon implies there is a limit cycle but I have difficulty in showing that there is no equilibrium point in unit circle. I have no idea for showing there is only such a limit cycle. I think using polar coordinates may help but I don't know how to use it. Any help is appreciated.,"\dot{x}=x+y-a^2x(x^2+y^2) \dot{y}=-x+y-a^2y(x^2+y^2), \quad a>1 1 1","['ordinary-differential-equations', 'dynamical-systems']"
96,Choosing functions so $p'(r) = g(r) \cdot (1- \frac{p(r)}{k(r)})$ is solvable,Choosing functions so  is solvable,p'(r) = g(r) \cdot (1- \frac{p(r)}{k(r)}),"I am trying to modify the verhulst equation to give it certain desirable qualities, however I keep hitting dead ends. Here is the general form of the equation I am starting with: $$ p'(r) = g(r) \cdot (1- \frac{p(r)}{k(r)}) $$ Obviously both functions $g(r)$ and $k(r)$ will be defined before solving the equation. My question is essentially how I can find an equation for each of those functions such that the following properties are true. Scope I only care about the behavior of the relevant functions between 0 and 1 for $r$ . All values/variables will be Real values, no Complex or anything like that. Desired Properties All integrals should be solvable symbolically, no taylor series or any sort of numerical approximations. Likewise $p(r)$ should be symbolically differentiable and integratable. As well as be computationally feasible and contain only common math functions like Log, e^x, factorial, modulus, +, -, but nothing more obscure or hard to program like Gamma or Hypergeometric functions. Basically something I can translate to source code easily. The resulting function $p(r)$ is $p(0) = 0$ and monotonically increases approaching infinity as r approaches 1. $g(r)$ is monotonically increasing as well with $g(0) = 0$ and with it approaching infinity as r approaches 1. Likewise $k(r)$ is monotonically increasing approaching infinity as $r$ approaches 1 however in this case $k(0) > 0$ . More specifically the value of k when r is 0 should be defined as a constant defined in the function k. For example $k(0) = K$ Both $k(r)$ and $g(r) should have constants defined such that they act as shaping variables to adjust Attempts While I have not been able to solve the problem with all the desirable properties above I have gotten close. but I am always missing one or more of the above properties. Thankfully I can show some graphs of examples of the above functions to give you an idea of what I'm looking to do. Attempting $g(x)$ I have mostly been using the following equation for $g(x)$ . $$ g(r)=A*\frac{r}{(1-r)^B} $$ In this case $A$ and $B$ are my shaping variables. Here is a plot of the function showing different shaping variables. I will plot it below. In the above I am showing the function with shaping variables A=1,B=1 and A=4,B=4 and A=1,B=4. If I use this equation for g I can solve the problem just fine and get a result that meets my desired property when k(r) is a constant. But not when $k(r)$ is a more complex function with my desired properties. Incase it is helpful I will show the solution I get for $p(r)$ to the above equation when k is a constant K. $$ g(r) = K-K e^{-\frac{A(1-r)^{-B}\left((1-r)^B-B(r-1)r+r^2-1\right)}{(B-2)(B-1)K}} $$ Note : This solution seems to have the odd property that B cant be 1 or 2 exactly. Which is kinda unexpected. Which would produce the following plot for A=5, B=3, K= 10. Attempting $k(r)$ For $k(r)$ I have tried several different functions. None of them produce something I can integrate by hand and neither can mathematica. There are some combinations with a different function for g (for example if I drop the B exponent) where I can integrate, but they have different problems. One simple attempt I made, which as I said fails, but gives an idea as to what I am going for is where $k(r)$ is defined the same as $g(r)$ but simply adding some constant $K$ . $$ k(r) = g(r) + K $$ That would produce the following plot when the following shaping constants are used A = 0.1, B = 2, and K = 10 Combining It All Now I couldn't figure out how to combine the above equations exactly, as I said the integrals don't seem solvable. But I have gotten some similar equations with different shaping variables to integrate and work. For example if I dropped the $B$ shaping variable then solve the differential equation I can get a solution that sort of works. Here is one set of functions that almost works: $$ g(r) = \frac{A \cdot r}{1-r} $$ $$ k(r) = B(-r-\log(1-r)) + K $$ Giving the final solution of... $$ p(r) = \frac{A\left((-K)^{\frac{A}{B}+1}(Br+B\log(1-r)-K)^{-\frac{A}{B}}-Br-B\log(1-r)+K\right)}{A+B} $$ Plotting that I get a graph that looks almost exactly like what I wanted. In this case the blue line is $g(r)$ , the red dashed horizontal line is $k(r)$ and the red dashed vertical line is the vertical asymptote at $r=1$ . There are a few problems with this, however, that, despite being close, isnt working. Specifically my solution has two problems, the first one being more dire than the second (if it were just the second I could live with it though it isn't ideal). The property I mentioned above, #2 isnt satisfied. As far as I can tell $p(r)$ can not be symbolically integrated. Since both $g(r)$ and $k(r)$ only have a single shaping variable each it isnt ideal for the application I am trying to use it for (this is why in the above failed attempts I added a second shaping variable). The graph is exactly what I am looking for, a sigmoid that tappers off to infinity on the tail end. Conclusion So what I am asking is either someone maybe help me find a way to combine and fix my above attempts to work as expected, or, suggest entirely new functions I can use that will be easier to work with and is solvable with the desired properties.","I am trying to modify the verhulst equation to give it certain desirable qualities, however I keep hitting dead ends. Here is the general form of the equation I am starting with: Obviously both functions and will be defined before solving the equation. My question is essentially how I can find an equation for each of those functions such that the following properties are true. Scope I only care about the behavior of the relevant functions between 0 and 1 for . All values/variables will be Real values, no Complex or anything like that. Desired Properties All integrals should be solvable symbolically, no taylor series or any sort of numerical approximations. Likewise should be symbolically differentiable and integratable. As well as be computationally feasible and contain only common math functions like Log, e^x, factorial, modulus, +, -, but nothing more obscure or hard to program like Gamma or Hypergeometric functions. Basically something I can translate to source code easily. The resulting function is and monotonically increases approaching infinity as r approaches 1. is monotonically increasing as well with and with it approaching infinity as r approaches 1. Likewise is monotonically increasing approaching infinity as approaches 1 however in this case . More specifically the value of k when r is 0 should be defined as a constant defined in the function k. For example Both and $g(r) should have constants defined such that they act as shaping variables to adjust Attempts While I have not been able to solve the problem with all the desirable properties above I have gotten close. but I am always missing one or more of the above properties. Thankfully I can show some graphs of examples of the above functions to give you an idea of what I'm looking to do. Attempting I have mostly been using the following equation for . In this case and are my shaping variables. Here is a plot of the function showing different shaping variables. I will plot it below. In the above I am showing the function with shaping variables A=1,B=1 and A=4,B=4 and A=1,B=4. If I use this equation for g I can solve the problem just fine and get a result that meets my desired property when k(r) is a constant. But not when is a more complex function with my desired properties. Incase it is helpful I will show the solution I get for to the above equation when k is a constant K. Note : This solution seems to have the odd property that B cant be 1 or 2 exactly. Which is kinda unexpected. Which would produce the following plot for A=5, B=3, K= 10. Attempting For I have tried several different functions. None of them produce something I can integrate by hand and neither can mathematica. There are some combinations with a different function for g (for example if I drop the B exponent) where I can integrate, but they have different problems. One simple attempt I made, which as I said fails, but gives an idea as to what I am going for is where is defined the same as but simply adding some constant . That would produce the following plot when the following shaping constants are used A = 0.1, B = 2, and K = 10 Combining It All Now I couldn't figure out how to combine the above equations exactly, as I said the integrals don't seem solvable. But I have gotten some similar equations with different shaping variables to integrate and work. For example if I dropped the shaping variable then solve the differential equation I can get a solution that sort of works. Here is one set of functions that almost works: Giving the final solution of... Plotting that I get a graph that looks almost exactly like what I wanted. In this case the blue line is , the red dashed horizontal line is and the red dashed vertical line is the vertical asymptote at . There are a few problems with this, however, that, despite being close, isnt working. Specifically my solution has two problems, the first one being more dire than the second (if it were just the second I could live with it though it isn't ideal). The property I mentioned above, #2 isnt satisfied. As far as I can tell can not be symbolically integrated. Since both and only have a single shaping variable each it isnt ideal for the application I am trying to use it for (this is why in the above failed attempts I added a second shaping variable). The graph is exactly what I am looking for, a sigmoid that tappers off to infinity on the tail end. Conclusion So what I am asking is either someone maybe help me find a way to combine and fix my above attempts to work as expected, or, suggest entirely new functions I can use that will be easier to work with and is solvable with the desired properties.", p'(r) = g(r) \cdot (1- \frac{p(r)}{k(r)})  g(r) k(r) r p(r) p(r) p(0) = 0 g(r) g(0) = 0 k(r) r k(0) > 0 k(0) = K k(r) g(x) g(x)  g(r)=A*\frac{r}{(1-r)^B}  A B k(r) p(r)  g(r) = K-K e^{-\frac{A(1-r)^{-B}\left((1-r)^B-B(r-1)r+r^2-1\right)}{(B-2)(B-1)K}}  k(r) k(r) k(r) g(r) K  k(r) = g(r) + K  B  g(r) = \frac{A \cdot r}{1-r}   k(r) = B(-r-\log(1-r)) + K   p(r) = \frac{A\left((-K)^{\frac{A}{B}+1}(Br+B\log(1-r)-K)^{-\frac{A}{B}}-Br-B\log(1-r)+K\right)}{A+B}  g(r) k(r) r=1 p(r) g(r) k(r),"['calculus', 'integration', 'ordinary-differential-equations', 'derivatives', 'partial-derivative']"
97,Differentiability of flow of non-autonomous ODE,Differentiability of flow of non-autonomous ODE,,"I am looking for any reference on the regularity of flows of non-autonomous ODEs. Any suggestions on books or articles are very much welcome. I have had little luck finding any usable results. Let us assume that $$ f: [0,1]\times \mathbb{R}^N \to \mathbb{R}^N $$ is continuous, satisfies the Lipschitz condition (i.e., it is Lipschitz continuous in the second argument), and is continuously differentiable in the second argument. Furthermore, assume that its derivative in the second argument, $\frac{\partial f}{\partial x}$ , also satisfies the Lipschitz condition. Consider the flow $\varphi : [0, 1] \times \mathbb{R}^N \to \mathbb{R}^N$ of the ODE $$ \dot x = f(t, x) $$ Meaning that $\varphi$ is defined via: \begin{equation} \begin{cases} \frac{d}{dt}\varphi(t, x_0) = f(t, \varphi(t, x_0)) \\ \varphi(0, x_0) = x_0 \end{cases} \end{equation} For fixed $t\in[0,1]$ , I expect that $\varphi(t, \cdot):\mathbb{R}^N \to \mathbb{R}^N$ is differentiable. In fact, I would expect the derivative $\frac{\partial \varphi}{\partial x}(\cdot, x_0)$ to satisfy the usual variational equation \begin{equation} \begin{cases} \frac{d}{dt}X(t, x_0) = \frac{\partial f}{\partial x}(t, \varphi(t, x_0))X(t, x_0) \\ X(0, x_0) = I \end{cases} \end{equation} I have been searching through dozens of books for any result like this. Most books, e.g., ""Differential Equations, Dynamical Systems & An Introduction to Chaos"", only consider autonomous ODEs with a right-hand side which is $C^1$ . Oftentimes, non-autonomous systems are dismissed at the very beginning with the note that it is possible to reduce to the autonomous setting. In my case, this will not be possible since the autonomized right-hand side will not be in $C^1$ . I would be grateful if somebody could point me towards helpful literature. Thanks!","I am looking for any reference on the regularity of flows of non-autonomous ODEs. Any suggestions on books or articles are very much welcome. I have had little luck finding any usable results. Let us assume that is continuous, satisfies the Lipschitz condition (i.e., it is Lipschitz continuous in the second argument), and is continuously differentiable in the second argument. Furthermore, assume that its derivative in the second argument, , also satisfies the Lipschitz condition. Consider the flow of the ODE Meaning that is defined via: For fixed , I expect that is differentiable. In fact, I would expect the derivative to satisfy the usual variational equation I have been searching through dozens of books for any result like this. Most books, e.g., ""Differential Equations, Dynamical Systems & An Introduction to Chaos"", only consider autonomous ODEs with a right-hand side which is . Oftentimes, non-autonomous systems are dismissed at the very beginning with the note that it is possible to reduce to the autonomous setting. In my case, this will not be possible since the autonomized right-hand side will not be in . I would be grateful if somebody could point me towards helpful literature. Thanks!","
f: [0,1]\times \mathbb{R}^N \to \mathbb{R}^N
 \frac{\partial f}{\partial x} \varphi : [0, 1] \times \mathbb{R}^N \to \mathbb{R}^N 
\dot x = f(t, x)
 \varphi \begin{equation}
\begin{cases}
\frac{d}{dt}\varphi(t, x_0) = f(t, \varphi(t, x_0)) \\
\varphi(0, x_0) = x_0
\end{cases}
\end{equation} t\in[0,1] \varphi(t, \cdot):\mathbb{R}^N \to \mathbb{R}^N \frac{\partial \varphi}{\partial x}(\cdot, x_0) \begin{equation}
\begin{cases}
\frac{d}{dt}X(t, x_0) = \frac{\partial f}{\partial x}(t, \varphi(t, x_0))X(t, x_0) \\
X(0, x_0) = I
\end{cases}
\end{equation} C^1 C^1",['ordinary-differential-equations']
98,Splitting a set of complex ODE in real ODEs,Splitting a set of complex ODE in real ODEs,,"In an answer to this question it is stated that ""Any n dimensional ODE system in $\mathbb{C}$ can be described as an 2 $n$ dimensional ODE system in $\mathbb{R}$ using $z(t)=\mathrm{Re}(z) + i \ \mathrm{Im}(z)$ and $t=\mathrm{Re}(t) + i\ \mathrm{Im}(t)$ ."" This shall be done by $$ \frac{\mathrm{d}[\mathrm{Re}(z)]}{\mathrm{d}[\mathrm{Re}(t)]} = \mathrm{Re}[f(z,t)] $$ $$ \frac{\mathrm{d}[\mathrm{Im}(z)]}{\mathrm{d}[\mathrm{Im}(t)]} = \mathrm{Im}[f(z,t)] $$ Is this procedure really general and without any additional requirements on $z \in \mathbb{C}$ and $t \in \mathbb{C}$ ? Can any use be made of a fact that the interval of integration in complex plane is a straight line?","In an answer to this question it is stated that ""Any n dimensional ODE system in can be described as an 2 dimensional ODE system in using and ."" This shall be done by Is this procedure really general and without any additional requirements on and ? Can any use be made of a fact that the interval of integration in complex plane is a straight line?","\mathbb{C} n \mathbb{R} z(t)=\mathrm{Re}(z) + i \ \mathrm{Im}(z) t=\mathrm{Re}(t) + i\ \mathrm{Im}(t) 
\frac{\mathrm{d}[\mathrm{Re}(z)]}{\mathrm{d}[\mathrm{Re}(t)]} = \mathrm{Re}[f(z,t)]
 
\frac{\mathrm{d}[\mathrm{Im}(z)]}{\mathrm{d}[\mathrm{Im}(t)]} = \mathrm{Im}[f(z,t)]
 z \in \mathbb{C} t \in \mathbb{C}","['complex-analysis', 'ordinary-differential-equations']"
99,"The integral in ""The Number 23"" movie; What is it?","The integral in ""The Number 23"" movie; What is it?",,"I was watching a movie called The Number 23 and saw this mathematical expression, I was wondering if it has any meaning and if it is, What does it mean? Here you can -Or Can't- see the expression It's not very clear, but I Think it's written like this: $$\triangledown x G=\int_{0}^{1} t\triangledown x(px\frac{\mathrm{d} r}{\mathrm{d} t})dt $$ Any ideas?","I was watching a movie called The Number 23 and saw this mathematical expression, I was wondering if it has any meaning and if it is, What does it mean? Here you can -Or Can't- see the expression It's not very clear, but I Think it's written like this: Any ideas?",\triangledown x G=\int_{0}^{1} t\triangledown x(px\frac{\mathrm{d} r}{\mathrm{d} t})dt ,"['calculus', 'probability', 'ordinary-differential-equations']"
