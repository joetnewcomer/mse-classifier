,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,$f$ convex $\iff$ $f(y) \ge f(x) + \nabla^Tf(x)(y-x)$ and $f$ is convex $\iff$ $\nabla^2f(x)\ge 0$,convex   and  is convex,f \iff f(y) \ge f(x) + \nabla^Tf(x)(y-x) f \iff \nabla^2f(x)\ge 0,"I need to prove $2$ things: 1) Let $f\in C^1$. Then $f$ is convex em a convex $S$ $\iff$ for all    $x,y\in S$ we have $$f(y) \ge f(x) + \nabla^Tf(x)(y-x)$$ 2) Let $f\in C^2$. Let $S\subset\mathbb{R}^n$ be a convex such that    $S^0$ (actually an S with a $0$ on top of it, what is it?). Then $f$   is convex $\iff$ $\nabla^2f(x)\ge 0$ for all $x\in S$ (I've seen Prove this basic inequality $f(y)\geq f(x)+\nabla f(x)^T(y-x)$for differentiable convex functions but since these $2$ propositions are connected I think it has more sense to do a proof a little different) For (1) , I noticed that if we take the Taylor Expansion: $$f(x+p) = f(x) + p^T\nabla^Tf(x) + \frac{1}{2}p^t\nabla^2f(x+tp)p$$ for some $t\in (0,1)$ and do $p = y-x$ we end up with $$f(y) = f(x) + (y-x)^T\nabla^Tf(x) + \frac{1}{2}(y-x)^t\nabla^2f(x+t(y-x))(y-x)$$ If we want to prove that $f(y) \ge f(x) + \nabla^Tf(x)(y-x)$, then using the taylor expansion above, it's just a matter of proving $\frac{1}{2}(y-x)^t\nabla^2f(x+t(y-x))\ge 0$. But there are no assumptions about the second derivative of $f$. For (2) , if we use the convex condition of $1$, then using the second order taylor expansion again: $$f(y) = f(x) + (y-x)^T\nabla^Tf(x) + \frac{1}{2}(y-x)^t\nabla^2f(x+t(y-x))(y-x)$$ Now if we suppose $\nabla^2 f(x)\ge 0$, we get that $f$ is convex because we get the inequality of the first exercise. But how do we know for sure that  $\nabla^2f(x)\ge 0$ for all $x\in S$ $\implies \frac{1}{2}(y-x)^t\nabla^2f(x+t(y-x))(y-x)$? For the converse , if $f$ is convex then $$f(y) \ge f(x) + \nabla^Tf(x)(y-x)$$ If $\nabla^2f(x)$ wereto be $<0$, we'd have the second order term $\frac{1}{2}(y-x)^t\nabla^2f(x+t(y-x))(y-x)$ in a taylor expansion to be negative (how to prove this?), then by the taylor expansion we'd have $f(y) < f(x) + \nabla^Tf(x)(y-x)$ which is a contradiction.","I need to prove $2$ things: 1) Let $f\in C^1$. Then $f$ is convex em a convex $S$ $\iff$ for all    $x,y\in S$ we have $$f(y) \ge f(x) + \nabla^Tf(x)(y-x)$$ 2) Let $f\in C^2$. Let $S\subset\mathbb{R}^n$ be a convex such that    $S^0$ (actually an S with a $0$ on top of it, what is it?). Then $f$   is convex $\iff$ $\nabla^2f(x)\ge 0$ for all $x\in S$ (I've seen Prove this basic inequality $f(y)\geq f(x)+\nabla f(x)^T(y-x)$for differentiable convex functions but since these $2$ propositions are connected I think it has more sense to do a proof a little different) For (1) , I noticed that if we take the Taylor Expansion: $$f(x+p) = f(x) + p^T\nabla^Tf(x) + \frac{1}{2}p^t\nabla^2f(x+tp)p$$ for some $t\in (0,1)$ and do $p = y-x$ we end up with $$f(y) = f(x) + (y-x)^T\nabla^Tf(x) + \frac{1}{2}(y-x)^t\nabla^2f(x+t(y-x))(y-x)$$ If we want to prove that $f(y) \ge f(x) + \nabla^Tf(x)(y-x)$, then using the taylor expansion above, it's just a matter of proving $\frac{1}{2}(y-x)^t\nabla^2f(x+t(y-x))\ge 0$. But there are no assumptions about the second derivative of $f$. For (2) , if we use the convex condition of $1$, then using the second order taylor expansion again: $$f(y) = f(x) + (y-x)^T\nabla^Tf(x) + \frac{1}{2}(y-x)^t\nabla^2f(x+t(y-x))(y-x)$$ Now if we suppose $\nabla^2 f(x)\ge 0$, we get that $f$ is convex because we get the inequality of the first exercise. But how do we know for sure that  $\nabla^2f(x)\ge 0$ for all $x\in S$ $\implies \frac{1}{2}(y-x)^t\nabla^2f(x+t(y-x))(y-x)$? For the converse , if $f$ is convex then $$f(y) \ge f(x) + \nabla^Tf(x)(y-x)$$ If $\nabla^2f(x)$ wereto be $<0$, we'd have the second order term $\frac{1}{2}(y-x)^t\nabla^2f(x+t(y-x))(y-x)$ in a taylor expansion to be negative (how to prove this?), then by the taylor expansion we'd have $f(y) < f(x) + \nabla^Tf(x)(y-x)$ which is a contradiction.",,"['calculus', 'multivariable-calculus', 'optimization', 'convex-analysis']"
1,Example of an incompressible vector field on $\mathbb{R}^3 \setminus 0$ without a vector potential,Example of an incompressible vector field on  without a vector potential,\mathbb{R}^3 \setminus 0,"As is well-known, the vector field $F = (-y,x)/(x^2+y^2)$ on the punctured plane is irrotational yet not conservative, i.e., there is no scalar potential $f$ such that $F = \nabla f$, because for instance $\oint_{S^1} F = 2\pi$. From a more abstract point of view, vector fields on the punctured plane correspond to 1-forms, irrotational fields correspond to closed forms, conservative fields correspond to exact forms, and $H^1_{dR}(\mathbb R^2 \setminus 0) \cong H^1_{dR}(S^1)$ is 1-dimensional and spanned by $d\theta$, so that any irrotational and non-conservative field on the punctured plane is a scalar multiple of $F$ plus a conservative field. I am looking for the 3D analogue of the above vector field. Since $H^2_{dR}(\mathbb R^3 \setminus 0) \cong H^2_{dR}(S^2)$ is 1-dimensional, there must exist an incompressible vector field $F$ (meaning that $\nabla \cdot F = 0$, or that the corresponding 2-form is closed) on $\mathbb R^3 \setminus 0$ that does not admit a vector potential (meaning that there is no field $G$ such that $F = \nabla \times G$, or that the corresponding 2-form is not exact). How could one come up with a formula for such a field? I tried to understand how one could get the formula for $F$ in the 2D case from the 1-form $d\theta$ on the circle by pulling it back through the deformation retract $\mathbb R^2 \setminus 0 \to S^1$ given by $(x,y) \mapsto (x,y)/\sqrt{x^2 + y^2}$, but I was not able to fill in the details.","As is well-known, the vector field $F = (-y,x)/(x^2+y^2)$ on the punctured plane is irrotational yet not conservative, i.e., there is no scalar potential $f$ such that $F = \nabla f$, because for instance $\oint_{S^1} F = 2\pi$. From a more abstract point of view, vector fields on the punctured plane correspond to 1-forms, irrotational fields correspond to closed forms, conservative fields correspond to exact forms, and $H^1_{dR}(\mathbb R^2 \setminus 0) \cong H^1_{dR}(S^1)$ is 1-dimensional and spanned by $d\theta$, so that any irrotational and non-conservative field on the punctured plane is a scalar multiple of $F$ plus a conservative field. I am looking for the 3D analogue of the above vector field. Since $H^2_{dR}(\mathbb R^3 \setminus 0) \cong H^2_{dR}(S^2)$ is 1-dimensional, there must exist an incompressible vector field $F$ (meaning that $\nabla \cdot F = 0$, or that the corresponding 2-form is closed) on $\mathbb R^3 \setminus 0$ that does not admit a vector potential (meaning that there is no field $G$ such that $F = \nabla \times G$, or that the corresponding 2-form is not exact). How could one come up with a formula for such a field? I tried to understand how one could get the formula for $F$ in the 2D case from the 1-form $d\theta$ on the circle by pulling it back through the deformation retract $\mathbb R^2 \setminus 0 \to S^1$ given by $(x,y) \mapsto (x,y)/\sqrt{x^2 + y^2}$, but I was not able to fill in the details.",,"['multivariable-calculus', 'differential-geometry', 'differential-topology', 'vector-analysis']"
2,"Sanity check on example 6.5 from ""Counterexamples in probability and real analysis"" by Wise and Hall","Sanity check on example 6.5 from ""Counterexamples in probability and real analysis"" by Wise and Hall",,"Citing "" Example 6.5. There exists a real-valued function defined on the plane that is discontinuous and yet such that both partial derivatives exist and are continuous. Proof It is well know that if $f:\mathbb{R} \to \mathbb{R}$ is differentiable, then $f$ is continuous. What if $g:\mathbb{R}^2 \to \mathbb{R}$ is such that both partial derivatives exist and are continuous? Must $g$ be continuous? Consider the function defined via $$g(x,y)= \begin{cases} \frac{xy}{x^2+y^2} & \text{ if } (x,y) \neq (0,0) \\ 0 &\text{ if } (x,y)=(0,0)\end{cases}$$ Then it trivially follows that each partial derivative of g exists and is continuous. However, note that $g$ is not continuous. Note also that this function g is continuous in each variable but is not continuous. "" However, computing the $x$ partial derivative at $(x,y) \neq (0,0)$ we get, for $x=0,y \neq 0, 1/y$. So the partial derivatives cannot be continuous at $(x,y)=(0,0)$ even though they exist there. Is it, as I think, some elementary mistake by the authors, or is it me who don't understand something obvious? If it is indeed such an elementary mistake, is this book considered in general to be otherwise trustable?","Citing "" Example 6.5. There exists a real-valued function defined on the plane that is discontinuous and yet such that both partial derivatives exist and are continuous. Proof It is well know that if $f:\mathbb{R} \to \mathbb{R}$ is differentiable, then $f$ is continuous. What if $g:\mathbb{R}^2 \to \mathbb{R}$ is such that both partial derivatives exist and are continuous? Must $g$ be continuous? Consider the function defined via $$g(x,y)= \begin{cases} \frac{xy}{x^2+y^2} & \text{ if } (x,y) \neq (0,0) \\ 0 &\text{ if } (x,y)=(0,0)\end{cases}$$ Then it trivially follows that each partial derivative of g exists and is continuous. However, note that $g$ is not continuous. Note also that this function g is continuous in each variable but is not continuous. "" However, computing the $x$ partial derivative at $(x,y) \neq (0,0)$ we get, for $x=0,y \neq 0, 1/y$. So the partial derivatives cannot be continuous at $(x,y)=(0,0)$ even though they exist there. Is it, as I think, some elementary mistake by the authors, or is it me who don't understand something obvious? If it is indeed such an elementary mistake, is this book considered in general to be otherwise trustable?",,"['real-analysis', 'multivariable-calculus']"
3,"How to proof $\lim\limits_{(x,y)\to(0,0)}\frac{x^2y^2}{x^2+y^4}$ using the $\epsilon-\delta$ definition?",How to proof  using the  definition?,"\lim\limits_{(x,y)\to(0,0)}\frac{x^2y^2}{x^2+y^4} \epsilon-\delta","I am stuck on proving that the limit $L = \lim\limits_{(x,y)\to(0,0)}\frac{x^2y^2}{x^2+y^4}$ does or does not exist (using the definition $\forall\epsilon\in\mathbb R_0^+:\exists\delta\in\mathbb R_0^+:\forall(x, y)\in\mathbb R^2-(0,0):||(x,y)||<\delta\Rightarrow|\frac{x^2y^2}{x^2+y^4}-L|<\epsilon$. I've found that if the limit exists, then $L$ must be equal to $0$ (by approaching the limit on several paths like $y=x$). I haven't found a path were the limit is not equal to 0, so I assume the limit does exist, but I can't find a way to proof the limit using the $\epsilon-\delta$ definition.","I am stuck on proving that the limit $L = \lim\limits_{(x,y)\to(0,0)}\frac{x^2y^2}{x^2+y^4}$ does or does not exist (using the definition $\forall\epsilon\in\mathbb R_0^+:\exists\delta\in\mathbb R_0^+:\forall(x, y)\in\mathbb R^2-(0,0):||(x,y)||<\delta\Rightarrow|\frac{x^2y^2}{x^2+y^4}-L|<\epsilon$. I've found that if the limit exists, then $L$ must be equal to $0$ (by approaching the limit on several paths like $y=x$). I haven't found a path were the limit is not equal to 0, so I assume the limit does exist, but I can't find a way to proof the limit using the $\epsilon-\delta$ definition.",,['multivariable-calculus']
4,Why in a directional derivative it has to be a unit vector [duplicate],Why in a directional derivative it has to be a unit vector [duplicate],,"This question already has an answer here : why normalize and the definition of directional derivative (1 answer) Closed 3 years ago . Why when we compute the directional derivative we have to use the unit vector $\vec{\bf{u}}$ ? I know that using $2\vec{\bf{u}}$ would change the directional derivative as the second point is further apart. But why not use $\frac{\vec{\bf{u}}}{2}$ then? Wouldn't that gradient be ""more precise""?","This question already has an answer here : why normalize and the definition of directional derivative (1 answer) Closed 3 years ago . Why when we compute the directional derivative we have to use the unit vector ? I know that using would change the directional derivative as the second point is further apart. But why not use then? Wouldn't that gradient be ""more precise""?",\vec{\bf{u}} 2\vec{\bf{u}} \frac{\vec{\bf{u}}}{2},"['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
5,"$x_t := a_t -b_t c_t $ , with $dx_t = \theta (\mu-x_t) dt+ \sigma dW_t$",", with",x_t := a_t -b_t c_t  dx_t = \theta (\mu-x_t) dt+ \sigma dW_t,"I would like to solve the following equation explicitly using Ito's lemma: $$ x_t := a_t -b_t c_t  , $$ where $x_t$ is an Ornstein-Uhlenbeck process (see here ) $$ dx_t = \theta (\mu-x_t) dt+ \sigma dW_t $$ Can I simply use the solution provided here ? So: $$ f(x_t, t) = x_t e^{\theta t}$$ and then: \begin{align} df(x_t,t) & =  \theta x_t e^{\theta t}\, dt + e^{\theta t}\, dx_t \\[6pt] & = e^{\theta t}\theta \mu \, dt + \sigma e^{\theta t}\, dW_t. \end{align} $$ x_t  = x_0 e^{-\theta t} + \mu(1-e^{-\theta t}) + e^{-\theta t}\int_0^t \sigma e^{\theta s}\, dW_s. $$","I would like to solve the following equation explicitly using Ito's lemma: $$ x_t := a_t -b_t c_t  , $$ where $x_t$ is an Ornstein-Uhlenbeck process (see here ) $$ dx_t = \theta (\mu-x_t) dt+ \sigma dW_t $$ Can I simply use the solution provided here ? So: $$ f(x_t, t) = x_t e^{\theta t}$$ and then: \begin{align} df(x_t,t) & =  \theta x_t e^{\theta t}\, dt + e^{\theta t}\, dx_t \\[6pt] & = e^{\theta t}\theta \mu \, dt + \sigma e^{\theta t}\, dW_t. \end{align} $$ x_t  = x_0 e^{-\theta t} + \mu(1-e^{-\theta t}) + e^{-\theta t}\int_0^t \sigma e^{\theta s}\, dW_s. $$",,"['multivariable-calculus', 'stochastic-processes', 'stochastic-calculus', 'brownian-motion']"
6,Problems on orthogonality and tangency in 3-space.,Problems on orthogonality and tangency in 3-space.,,"Find the set of all points $(a,b,c)$ in 3-space for which the two spheres $(x-a)^2+(y-b)^2+(z-c)^2=1$ and $x^2+y^2+z^2=1$ intersect orthogonally. A cylinder whose equation is $y=f(x)$ is tangent to the surface $z^2+2xz+y=0$. For the first one, at the set of points, $\nabla f \cdot \nabla g$ should be $0$, where $f$ and $g$ each correspond to the first two spheres. Doing this, I get $4x(x-a)+4y(y-b)+4z(z-c)=0$, which gives the function of a sphere centered at $(a/2,b/2,c/2)$. However, the answer of this problem is a sphere with center at the origin and radius $\sqrt{2}$. For the second one, I tried solving it by setting the first equation as $F(x,y,z)=f(x)-y$ and the second one as $G(x,y,z)=z^2+2xz+y$. But I do not know how to progress further from here. I guess I'm not good at solving these kind of geometric problems yet. I would greatly appreciate any help.","Find the set of all points $(a,b,c)$ in 3-space for which the two spheres $(x-a)^2+(y-b)^2+(z-c)^2=1$ and $x^2+y^2+z^2=1$ intersect orthogonally. A cylinder whose equation is $y=f(x)$ is tangent to the surface $z^2+2xz+y=0$. For the first one, at the set of points, $\nabla f \cdot \nabla g$ should be $0$, where $f$ and $g$ each correspond to the first two spheres. Doing this, I get $4x(x-a)+4y(y-b)+4z(z-c)=0$, which gives the function of a sphere centered at $(a/2,b/2,c/2)$. However, the answer of this problem is a sphere with center at the origin and radius $\sqrt{2}$. For the second one, I tried solving it by setting the first equation as $F(x,y,z)=f(x)-y$ and the second one as $G(x,y,z)=z^2+2xz+y$. But I do not know how to progress further from here. I guess I'm not good at solving these kind of geometric problems yet. I would greatly appreciate any help.",,"['multivariable-calculus', 'vector-analysis']"
7,Question about the Definition of Differential Forms,Question about the Definition of Differential Forms,,"Question: Why does the definition of a differential form guarantee that when we do integration using differential forms, it is the same as the usual Riemann integral (before we introduce the concept of differential form)? Example: If I want to prove Stoke’s Theorem, and I define the “differential form”, then the “differential form” may have certain rules of calculation. Then I use the “differential form” to prove the Stoke’s Theorem. But why is Stoke’s Theorem in the form of “differential forms” is equivalent to the usual Stoke’s Theorem (which we learn in multivariable calculus) ?","Question: Why does the definition of a differential form guarantee that when we do integration using differential forms, it is the same as the usual Riemann integral (before we introduce the concept of differential form)? Example: If I want to prove Stoke’s Theorem, and I define the “differential form”, then the “differential form” may have certain rules of calculation. Then I use the “differential form” to prove the Stoke’s Theorem. But why is Stoke’s Theorem in the form of “differential forms” is equivalent to the usual Stoke’s Theorem (which we learn in multivariable calculus) ?",,"['multivariable-calculus', 'differential-forms']"
8,Cylindrical coordinates on elliptic paraboloids.,Cylindrical coordinates on elliptic paraboloids.,,"I want to compute the volume bounded by: the cylinder $x^2+4y^2=4$. the $z=0$ plane. the elliptic paraboloid $z = x^2 + 6y^2$. I would like to use cylindrical coordinates. However I have never dealt with a problem in which $r$ bounds depend on $\theta$. Here there's a horrible sketch of the solid: The base is an ellipse. So I take the transformation to cylindrical coordinates: \begin{align} x&=r\cos\theta,\\ y&=r\sin\theta, \\ z&=z. \end{align} Which implies $0\leq z\leq x^2+6y^2 = r^2\cos^2\theta + 6r^2\sin^2\theta$ and $0\leq\theta\leq2\pi$. However I'm having difficulties in determining the $r$ bounds. Also I would like to know the equation of the curve of intersection between the cylinder and the paraboloid, but don't know how to do it. For the $r$ bounds I tried considering the equation for the base: $$4y^2+x^2=4\Rightarrow 4r^2\sin^2\theta+r^2\cos^2\theta = 4 \Rightarrow r=\frac{2}{\sqrt{4\sin^2\theta+\cos^2\theta}}.$$ I'm not sure if doing that is correct, also the integral looks absolutely grim. I really appreciate your help.","I want to compute the volume bounded by: the cylinder $x^2+4y^2=4$. the $z=0$ plane. the elliptic paraboloid $z = x^2 + 6y^2$. I would like to use cylindrical coordinates. However I have never dealt with a problem in which $r$ bounds depend on $\theta$. Here there's a horrible sketch of the solid: The base is an ellipse. So I take the transformation to cylindrical coordinates: \begin{align} x&=r\cos\theta,\\ y&=r\sin\theta, \\ z&=z. \end{align} Which implies $0\leq z\leq x^2+6y^2 = r^2\cos^2\theta + 6r^2\sin^2\theta$ and $0\leq\theta\leq2\pi$. However I'm having difficulties in determining the $r$ bounds. Also I would like to know the equation of the curve of intersection between the cylinder and the paraboloid, but don't know how to do it. For the $r$ bounds I tried considering the equation for the base: $$4y^2+x^2=4\Rightarrow 4r^2\sin^2\theta+r^2\cos^2\theta = 4 \Rightarrow r=\frac{2}{\sqrt{4\sin^2\theta+\cos^2\theta}}.$$ I'm not sure if doing that is correct, also the integral looks absolutely grim. I really appreciate your help.",,"['multivariable-calculus', 'coordinate-systems']"
9,"When is $\min_{x\in X,y\in Y}f(x,y)=\min_{x\in X}(\min_{y\in Y}f(x,y))$?",When is ?,"\min_{x\in X,y\in Y}f(x,y)=\min_{x\in X}(\min_{y\in Y}f(x,y))","When is $$ \min_{x\in X,y\in Y}f(x,y)=\min_{x\in X}(\min_{y\in Y}f(x,y))? $$","When is $$ \min_{x\in X,y\in Y}f(x,y)=\min_{x\in X}(\min_{y\in Y}f(x,y))? $$",,"['multivariable-calculus', 'optimization']"
10,Conceptual question: Critical Points,Conceptual question: Critical Points,,"This is my first question posted here, I hope to make it as easy-to-answer as possible. I'm currently studying Vector Calculus it is taught that to find critical points (over the entire surface, not over some domain), we do the following: Let $f_x=0$ and $f_y=0$. Solve the two resulting equations simultaneously if need be. We are taking the partials along the coordinate axes, but what is the guarantee that these are the only critical points? ie: If I take the partial derivatives along any two perpendicular vectors, could they yield critical points that would not be found by taking the partials along the coordinate axes? Below is my 'explanation' based on my current understanding (which may be completely incorrect!) Since we generally study 'friendly' surfaces, the partials (in any direction) will always tend to zero, so we take partials along the coordinate axes for the sake of convenience.","This is my first question posted here, I hope to make it as easy-to-answer as possible. I'm currently studying Vector Calculus it is taught that to find critical points (over the entire surface, not over some domain), we do the following: Let $f_x=0$ and $f_y=0$. Solve the two resulting equations simultaneously if need be. We are taking the partials along the coordinate axes, but what is the guarantee that these are the only critical points? ie: If I take the partial derivatives along any two perpendicular vectors, could they yield critical points that would not be found by taking the partials along the coordinate axes? Below is my 'explanation' based on my current understanding (which may be completely incorrect!) Since we generally study 'friendly' surfaces, the partials (in any direction) will always tend to zero, so we take partials along the coordinate axes for the sake of convenience.",,['multivariable-calculus']
11,How do we make this integration rigorous?,How do we make this integration rigorous?,,"This is from Jaynes, Probability Theory: The Logic of Science , pp 27-28. We have a function $F$ which is $\mathbb{R}^2 \rightarrow \mathbb{R}$, and we set $v = F(y,z)$. We discover that $$ F_1(y,z) = { \partial F \over \partial y }  = { H(v) \over H(y) } \\    F_2(y,z) = { \partial F \over \partial z }  = r{ H(v) \over H(z) } $$ where $H$ is arbitrary, but can't change sign in the region of interest. (Specifically, $H$ is such that $F_2(y,z) / F_1(y,z)$ takes the form $r H(y) / H(z) $.) We later discover $r = 1$, so I'm going to ignore that for clarity. Then, since $\mathrm d v = F_1 \mathrm d y + F_2 \mathrm d z $, we get $$ { \mathrm d v \over H(v) } = { \mathrm d y \over H(y) } + { \mathrm d z \over H(z) } $$ So far, so good. Now we define $$ w(x) = \exp\left( \int^x { \mathrm d x \over H(x) } \right) $$ and it follows that $w(v) = w(y) w(z)$. I can kinda sorta see how this happens, but not really. Apparently, $$ \int { \mathrm d v \over H(v) } = \int \left( { \mathrm d y \over H(y) } + { \mathrm d z \over H(z) } \right)  = \int { \mathrm d y \over H(y) } + \int{ \mathrm d z \over H(z) } $$ And then we just apply $e^{a+b} = e^a e^b$. This makes some sense notationally, but I'm not familiar with the rigor behind it. (I've removed the ${}^x$ from the integrals because I don't really know what I'd do with it. Jaynes credits this proof to Cox (1961), which I looked up - it didn't include any intermediate steps to help me, but it also omitted the ${}^x$, so I feel somewhat comfortable doing so myself.) I think there are a few specific things confusing me, where the notation just doesn't mean what I expect it to: I expect expressions like $\int^x f(s) \mathrm d s$ to be a function of $x$, where $s$ is a bound variable, and we can rewrite $s$ as $p$ or $\alpha$ or anything, and rewriting $s$ as $x$ is just about the most confusing choice we can possibly make. In this case, it seems that the symbol used inside the integration is relevant? $w$ looks like a function $\mathbb R \rightarrow \mathbb R $, but $w(v)$ depends upon $\mathrm d v$ as well as $v$? Can anyone clear this up for me?","This is from Jaynes, Probability Theory: The Logic of Science , pp 27-28. We have a function $F$ which is $\mathbb{R}^2 \rightarrow \mathbb{R}$, and we set $v = F(y,z)$. We discover that $$ F_1(y,z) = { \partial F \over \partial y }  = { H(v) \over H(y) } \\    F_2(y,z) = { \partial F \over \partial z }  = r{ H(v) \over H(z) } $$ where $H$ is arbitrary, but can't change sign in the region of interest. (Specifically, $H$ is such that $F_2(y,z) / F_1(y,z)$ takes the form $r H(y) / H(z) $.) We later discover $r = 1$, so I'm going to ignore that for clarity. Then, since $\mathrm d v = F_1 \mathrm d y + F_2 \mathrm d z $, we get $$ { \mathrm d v \over H(v) } = { \mathrm d y \over H(y) } + { \mathrm d z \over H(z) } $$ So far, so good. Now we define $$ w(x) = \exp\left( \int^x { \mathrm d x \over H(x) } \right) $$ and it follows that $w(v) = w(y) w(z)$. I can kinda sorta see how this happens, but not really. Apparently, $$ \int { \mathrm d v \over H(v) } = \int \left( { \mathrm d y \over H(y) } + { \mathrm d z \over H(z) } \right)  = \int { \mathrm d y \over H(y) } + \int{ \mathrm d z \over H(z) } $$ And then we just apply $e^{a+b} = e^a e^b$. This makes some sense notationally, but I'm not familiar with the rigor behind it. (I've removed the ${}^x$ from the integrals because I don't really know what I'd do with it. Jaynes credits this proof to Cox (1961), which I looked up - it didn't include any intermediate steps to help me, but it also omitted the ${}^x$, so I feel somewhat comfortable doing so myself.) I think there are a few specific things confusing me, where the notation just doesn't mean what I expect it to: I expect expressions like $\int^x f(s) \mathrm d s$ to be a function of $x$, where $s$ is a bound variable, and we can rewrite $s$ as $p$ or $\alpha$ or anything, and rewriting $s$ as $x$ is just about the most confusing choice we can possibly make. In this case, it seems that the symbol used inside the integration is relevant? $w$ looks like a function $\mathbb R \rightarrow \mathbb R $, but $w(v)$ depends upon $\mathrm d v$ as well as $v$? Can anyone clear this up for me?",,['multivariable-calculus']
12,Differentiability of $g:=f(\sqrt{x^2+y^2})$ for a $C^1$ function with $f'(0)=0$ NBHM $2008$,Differentiability of  for a  function with  NBHM,g:=f(\sqrt{x^2+y^2}) C^1 f'(0)=0 2008,"Let $f:\mathbb{R}\rightarrow \mathbb{R}$ be a continuously differentiable function such that $f'(0)=0$. Define for all $x,y\in \mathbb{R}$, $$g(x,y)=f(\sqrt{x^2+y^2})$$ Pick out the true statements : $g$ is differentiable on $\mathbb{R}^2$ $g$ is differentiable on $\mathbb{R}^2$ if and only if $f(0)=0$ $g$ is differentiable on $\mathbb{R}^2- \{ (0,0)\}$ I do not have much idea of differentiation of multivariable calculus. I just tried out some examples to get some idea.. I have to take some function whose derivative at $0$ has to be zero. for simplicity I would take $f(x)=x^2$ Then I would get : $g(x,y)=x^2+y^2$ which is differentiable on $\mathbb{R}^2$ This would suggest me that second option has more chances as $f(0)=0$ in this case. I would now take $f(x)=x^2+1$ then : $g(x,y)=x^2+y^2+1$ which is differentiable on $\mathbb{R}^2$ though $f(0)\neq 0$ So, I would now eliminate second option and third option also... So, It is now got confirmed that i have to work out to ""Prove"" that first option is correct. Now, I have to prove that $g(x,y)=f(\sqrt{x^2+y^2})$ is differentiable. I have to look for partial derivatives first : $$\frac{\partial g}{\partial x}= f'(\sqrt{x^2+y^2}).\frac{2x}{2\sqrt{x^2+y^2}}=f'(\sqrt{x^2+y^2}).\frac{x}{\sqrt{x^2+y^2}}$$ $$\frac{\partial g}{\partial y}= f'(\sqrt{x^2+y^2}).\frac{2y}{2\sqrt{x^2+y^2}}=f'(\sqrt{x^2+y^2}).\frac{y}{\sqrt{x^2+y^2}}$$ I have used here that $f$ is differentiable on $\mathbb{R}$ As $f$ is continuously differentiable, we have $f'$ is continuous and I know that : $\frac{x}{\sqrt{x^2+y^2}}$ and $\frac{y}{\sqrt{x^2+y^2}}$ are also continuous. As $f'$ and $\frac{x}{\sqrt{x^2+y^2}}$ are continuous So is the product $\frac{\partial g}{\partial x}=f'(\sqrt{x^2+y^2}).\frac{x}{\sqrt{x^2+y^2}}$ As $f'$ and $\frac{y}{\sqrt{x^2+y^2}}$ are continuous So is the product $\frac{\partial g}{\partial y}=f'(\sqrt{x^2+y^2}).\frac{y}{\sqrt{x^2+y^2}}$ So we have continuous partial derivatives for $g(x,y)$.. So, I am willing to conclude that: $g(x,y)$ is  differentiable on $\mathbb{R}^2$ I would be thankful if some one can confirm if the answer correct. I would be much more thankful if some one can let me know if there are any gaps in my argument. This is just a proof verification Question.. Thank you.","Let $f:\mathbb{R}\rightarrow \mathbb{R}$ be a continuously differentiable function such that $f'(0)=0$. Define for all $x,y\in \mathbb{R}$, $$g(x,y)=f(\sqrt{x^2+y^2})$$ Pick out the true statements : $g$ is differentiable on $\mathbb{R}^2$ $g$ is differentiable on $\mathbb{R}^2$ if and only if $f(0)=0$ $g$ is differentiable on $\mathbb{R}^2- \{ (0,0)\}$ I do not have much idea of differentiation of multivariable calculus. I just tried out some examples to get some idea.. I have to take some function whose derivative at $0$ has to be zero. for simplicity I would take $f(x)=x^2$ Then I would get : $g(x,y)=x^2+y^2$ which is differentiable on $\mathbb{R}^2$ This would suggest me that second option has more chances as $f(0)=0$ in this case. I would now take $f(x)=x^2+1$ then : $g(x,y)=x^2+y^2+1$ which is differentiable on $\mathbb{R}^2$ though $f(0)\neq 0$ So, I would now eliminate second option and third option also... So, It is now got confirmed that i have to work out to ""Prove"" that first option is correct. Now, I have to prove that $g(x,y)=f(\sqrt{x^2+y^2})$ is differentiable. I have to look for partial derivatives first : $$\frac{\partial g}{\partial x}= f'(\sqrt{x^2+y^2}).\frac{2x}{2\sqrt{x^2+y^2}}=f'(\sqrt{x^2+y^2}).\frac{x}{\sqrt{x^2+y^2}}$$ $$\frac{\partial g}{\partial y}= f'(\sqrt{x^2+y^2}).\frac{2y}{2\sqrt{x^2+y^2}}=f'(\sqrt{x^2+y^2}).\frac{y}{\sqrt{x^2+y^2}}$$ I have used here that $f$ is differentiable on $\mathbb{R}$ As $f$ is continuously differentiable, we have $f'$ is continuous and I know that : $\frac{x}{\sqrt{x^2+y^2}}$ and $\frac{y}{\sqrt{x^2+y^2}}$ are also continuous. As $f'$ and $\frac{x}{\sqrt{x^2+y^2}}$ are continuous So is the product $\frac{\partial g}{\partial x}=f'(\sqrt{x^2+y^2}).\frac{x}{\sqrt{x^2+y^2}}$ As $f'$ and $\frac{y}{\sqrt{x^2+y^2}}$ are continuous So is the product $\frac{\partial g}{\partial y}=f'(\sqrt{x^2+y^2}).\frac{y}{\sqrt{x^2+y^2}}$ So we have continuous partial derivatives for $g(x,y)$.. So, I am willing to conclude that: $g(x,y)$ is  differentiable on $\mathbb{R}^2$ I would be thankful if some one can confirm if the answer correct. I would be much more thankful if some one can let me know if there are any gaps in my argument. This is just a proof verification Question.. Thank you.",,['real-analysis']
13,Geometric Meaning of the Partial Derivative,Geometric Meaning of the Partial Derivative,,"Could anyone explain the geometric meaning behind the partial derivative? I know that for a ""normal"" derivative, its geometric meaning is the slope of the tangent of a curve. From what I understand about the partial derivative, it is the slope of the tangent of a cross section of a function with two or more variables. If anyone could expand upon the definition and clarify, I would be grateful. Thank you.","Could anyone explain the geometric meaning behind the partial derivative? I know that for a ""normal"" derivative, its geometric meaning is the slope of the tangent of a curve. From what I understand about the partial derivative, it is the slope of the tangent of a cross section of a function with two or more variables. If anyone could expand upon the definition and clarify, I would be grateful. Thank you.",,"['multivariable-calculus', 'partial-derivative']"
14,Curl(curl(A)) with Einstein Summation Notation,Curl(curl(A)) with Einstein Summation Notation,,"I have two questions on the computation of $\nabla \times (\nabla \times \mathbf{A}) $ with Einstein summation notation based on http://www.physics.ohio-state.edu/~ntg/263/handouts/tensor_intro.pdf . It considers the $i$th component. All colours have been added by me.  $$ (\color{green}{\nabla} \color{red}{\times} \color{purple}{(\nabla \times \mathbf{A})})_i = \color{red}{\epsilon_{ijk}}\color{green}{\partial_j} \color{purple}{(\nabla \times \mathbf{A})}_k \tag{*}$$ $$ = \color{red}{\epsilon_{ijk}}\color{green}{\partial_j} \color{purple}{\epsilon_{klm}\partial_lA_m}$$  $$ = \epsilon_{ijk}\epsilon_{klm}\partial_j \partial_lA_m $$ $$ = \epsilon_{ijk}\epsilon_{lmk}\partial_j \partial_lA_m $$ $$ = (\delta_{il}\delta_{jm} - \delta_{im}\delta_{jl}) \partial_j \partial_lA_m $$  Now, choose $j \rightarrow m$ and $ l \rightarrow i $ in the first term and let $j \rightarrow l$ and $ m \rightarrow i $ in the second: $$ = \partial_m \partial_iA_m - \color{blue}{\partial_l \partial_lA_i}$$  $$ = \partial_i(\partial_m A_m) - \color{blue}{(\partial_l \partial_l\mathbf{A})_i}\tag{**}$$ $\Large{\text{Question 1}}$. In (*), where does the subscript $k$ on the RHS come from? Aren't we looking at the $i$th component?  I know that $\mathbf{a} \times \mathbf{b} = a_gb_h\epsilon_{ghi}\mathbf{\hat{e_i}} = (..., \underbrace{a_gb_h\epsilon_{ghi}}_{i\text{th component}}, ...)\Longrightarrow (\mathbf{a} \times \mathbf{b})_i  =  a_gb_h\epsilon_{ghi} $. $\Large{\text{Question 2}}$. In (**), how does $ \color{blue}{\partial_l \partial_lA_i = (\partial_l \partial_l\mathbf{A})_i} $ ?","I have two questions on the computation of $\nabla \times (\nabla \times \mathbf{A}) $ with Einstein summation notation based on http://www.physics.ohio-state.edu/~ntg/263/handouts/tensor_intro.pdf . It considers the $i$th component. All colours have been added by me.  $$ (\color{green}{\nabla} \color{red}{\times} \color{purple}{(\nabla \times \mathbf{A})})_i = \color{red}{\epsilon_{ijk}}\color{green}{\partial_j} \color{purple}{(\nabla \times \mathbf{A})}_k \tag{*}$$ $$ = \color{red}{\epsilon_{ijk}}\color{green}{\partial_j} \color{purple}{\epsilon_{klm}\partial_lA_m}$$  $$ = \epsilon_{ijk}\epsilon_{klm}\partial_j \partial_lA_m $$ $$ = \epsilon_{ijk}\epsilon_{lmk}\partial_j \partial_lA_m $$ $$ = (\delta_{il}\delta_{jm} - \delta_{im}\delta_{jl}) \partial_j \partial_lA_m $$  Now, choose $j \rightarrow m$ and $ l \rightarrow i $ in the first term and let $j \rightarrow l$ and $ m \rightarrow i $ in the second: $$ = \partial_m \partial_iA_m - \color{blue}{\partial_l \partial_lA_i}$$  $$ = \partial_i(\partial_m A_m) - \color{blue}{(\partial_l \partial_l\mathbf{A})_i}\tag{**}$$ $\Large{\text{Question 1}}$. In (*), where does the subscript $k$ on the RHS come from? Aren't we looking at the $i$th component?  I know that $\mathbf{a} \times \mathbf{b} = a_gb_h\epsilon_{ghi}\mathbf{\hat{e_i}} = (..., \underbrace{a_gb_h\epsilon_{ghi}}_{i\text{th component}}, ...)\Longrightarrow (\mathbf{a} \times \mathbf{b})_i  =  a_gb_h\epsilon_{ghi} $. $\Large{\text{Question 2}}$. In (**), how does $ \color{blue}{\partial_l \partial_lA_i = (\partial_l \partial_l\mathbf{A})_i} $ ?",,[]
15,Implicit function theorem and implicit differentiation,Implicit function theorem and implicit differentiation,,"This is perhaps something simple; but I am not quite getting why the implication is true; I seem to be missing something. Supposedly, the implicit function theorem: Let $f: \mathbb{R}^{n + m} \rightarrow \mathbb{R}^m$ be a continuously differentiable function, and let $\mathbb{R}^{n+m} $ have coordinates $( x, y)$, where $x \in \mathbb{R}^n$ and $y\in \mathbb{R}^m$. Fix a point $( a , b) = (a_1 , \ldots , a_n , b_1 , \ldots, b_m )$ with $ f( a, b) = c$, where $c \in \mathbb{R}^m$. If the matrix $( \partial f_i/\partial y_j)(a,b)$ is invertible,  then there exists an open set $U$ containing $a$, and an open set $V$ contntaining $b$, and a unique continuously differentiable function $g: U \rightarrow V$ such that    $$ \{ (\mathbf{x}, g(\mathbf{x}))|\mathbf x \in U  \} = \{ (\mathbf{x}, \mathbf{y}) \in U \times V| f(\mathbf{x}, \mathbf{y}) = \mathbf{c} \}.$$ implies that implicit differentiation is okay: $$ \frac{dy}{dx} = -\frac{\partial F / \partial x}{\partial F / \partial y}.$$ What am I missing here? I again apologize if this is something very trivial.","This is perhaps something simple; but I am not quite getting why the implication is true; I seem to be missing something. Supposedly, the implicit function theorem: Let $f: \mathbb{R}^{n + m} \rightarrow \mathbb{R}^m$ be a continuously differentiable function, and let $\mathbb{R}^{n+m} $ have coordinates $( x, y)$, where $x \in \mathbb{R}^n$ and $y\in \mathbb{R}^m$. Fix a point $( a , b) = (a_1 , \ldots , a_n , b_1 , \ldots, b_m )$ with $ f( a, b) = c$, where $c \in \mathbb{R}^m$. If the matrix $( \partial f_i/\partial y_j)(a,b)$ is invertible,  then there exists an open set $U$ containing $a$, and an open set $V$ contntaining $b$, and a unique continuously differentiable function $g: U \rightarrow V$ such that    $$ \{ (\mathbf{x}, g(\mathbf{x}))|\mathbf x \in U  \} = \{ (\mathbf{x}, \mathbf{y}) \in U \times V| f(\mathbf{x}, \mathbf{y}) = \mathbf{c} \}.$$ implies that implicit differentiation is okay: $$ \frac{dy}{dx} = -\frac{\partial F / \partial x}{\partial F / \partial y}.$$ What am I missing here? I again apologize if this is something very trivial.",,['multivariable-calculus']
16,Normal vector to a surface,Normal vector to a surface,,"I've been reading in my calc book that the gradient vector is always orthogonal to the surface. So for a surface in space described by the level surface $f(x,y,z) = k$ where $k$ is a constant, $\nabla f$ is orthogonal to the surface at every point because the gradient is the normal vector of the surface at every point. Then later I read about parametric surfaces where a surface is described by vector valued function $r(u,v) = <x(u,v), y(u,v), z(u,v)>$ and a normal vector $r_u \times r_v$ or $r_v \times r_u$ How are $r_u \times r_v$ and $\nabla f$ related here? I am referring to James Stewart's Text. Also a last comment I want to make is, what about a normal vector to a surface that doesn't need to be described by a level surface? For example $f(x,y) = z = x^2 + y^2$? How would I go finding the normal vector at any point without rewriting it as $z - x^2 - y^2 = 0$ or parametrizing it? A final Remark: I've been confusing the notion of the gradient vector being tangent to a surface instead of normal to it. There is this rather confusing picture I have which seems to suggests that the gradient vector really is tangent to a surface rather than normal because the gradient is formed by the vector sum of $\partial/\partial x$ and $\partial/\partial y$ and according to the picture I have, both $\partial/\partial x$ and $\partial/\partial x$ are ""flat"" and their sum should also be ""flat"" and not ""pointing up"" Added PIcture","I've been reading in my calc book that the gradient vector is always orthogonal to the surface. So for a surface in space described by the level surface $f(x,y,z) = k$ where $k$ is a constant, $\nabla f$ is orthogonal to the surface at every point because the gradient is the normal vector of the surface at every point. Then later I read about parametric surfaces where a surface is described by vector valued function $r(u,v) = <x(u,v), y(u,v), z(u,v)>$ and a normal vector $r_u \times r_v$ or $r_v \times r_u$ How are $r_u \times r_v$ and $\nabla f$ related here? I am referring to James Stewart's Text. Also a last comment I want to make is, what about a normal vector to a surface that doesn't need to be described by a level surface? For example $f(x,y) = z = x^2 + y^2$? How would I go finding the normal vector at any point without rewriting it as $z - x^2 - y^2 = 0$ or parametrizing it? A final Remark: I've been confusing the notion of the gradient vector being tangent to a surface instead of normal to it. There is this rather confusing picture I have which seems to suggests that the gradient vector really is tangent to a surface rather than normal because the gradient is formed by the vector sum of $\partial/\partial x$ and $\partial/\partial y$ and according to the picture I have, both $\partial/\partial x$ and $\partial/\partial x$ are ""flat"" and their sum should also be ""flat"" and not ""pointing up"" Added PIcture",,"['calculus', 'multivariable-calculus']"
17,What do you call a function differentiated with respect to all of its arguments?,What do you call a function differentiated with respect to all of its arguments?,,"Just a simple question. Let $f(x_1, x_2, \ldots, x_n)$ be a smooth function. Is there a particular name for the function $$\frac{\partial^n f}{\partial x_1 \, \partial x_2 \cdots \partial x_n}$$","Just a simple question. Let $f(x_1, x_2, \ldots, x_n)$ be a smooth function. Is there a particular name for the function $$\frac{\partial^n f}{\partial x_1 \, \partial x_2 \cdots \partial x_n}$$",,"['calculus', 'multivariable-calculus', 'terminology']"
18,find a point on ellipse closest to origin,find a point on ellipse closest to origin,,"Find the points on the ellipse $2x^2 + 4xy + 5y^2 = 30$ closest and farthest from origin. How to do this problem? I know how to find a closest point if $z = f(x,y)$ is given, however, this is 2 dimensional.","Find the points on the ellipse $2x^2 + 4xy + 5y^2 = 30$ closest and farthest from origin. How to do this problem? I know how to find a closest point if $z = f(x,y)$ is given, however, this is 2 dimensional.",,['multivariable-calculus']
19,Differentiability on the boundary,Differentiability on the boundary,,"Suppose that we have a non negative real valued function $f$ defined only on $[0,\infty)^n$. Can one talk about the differentiability of such a function on the boundary? In the classical books on multivariable calculus, when they define differentiability of a multivariable function at a point, they always start with an assumption that the function is defined on an open neighborhood of the point. Can someone clear this for me?","Suppose that we have a non negative real valued function $f$ defined only on $[0,\infty)^n$. Can one talk about the differentiability of such a function on the boundary? In the classical books on multivariable calculus, when they define differentiability of a multivariable function at a point, they always start with an assumption that the function is defined on an open neighborhood of the point. Can someone clear this for me?",,"['real-analysis', 'multivariable-calculus']"
20,Derivative of a multivariable function,Derivative of a multivariable function,,"Let us define a function $f$ from $M(n,\mathbb{R})$ to $M(n,\mathbb{R})$ by treating $M(n,\mathbb{R})\approx\mathbb{R}^{n^2}$, by $$f(X)=e^X+X$$ where $$e^X=1+X/{1!}+X^2/{2!}+\dots$$ I want to find the (Frechet) derivative of $f$. We know, if derivative exists at $X$, then $$f(X+H)-f(X)=f'(X)H+r(H)$$ where $r(H)/\|H\|\to \bf{0}$ as $H\to \bf{0}$. So I went on to find the difference, but couldn't figure out the linear part ($f'(X)H$) and the remainder part ($r(H)$). Any help is appreciated.","Let us define a function $f$ from $M(n,\mathbb{R})$ to $M(n,\mathbb{R})$ by treating $M(n,\mathbb{R})\approx\mathbb{R}^{n^2}$, by $$f(X)=e^X+X$$ where $$e^X=1+X/{1!}+X^2/{2!}+\dots$$ I want to find the (Frechet) derivative of $f$. We know, if derivative exists at $X$, then $$f(X+H)-f(X)=f'(X)H+r(H)$$ where $r(H)/\|H\|\to \bf{0}$ as $H\to \bf{0}$. So I went on to find the difference, but couldn't figure out the linear part ($f'(X)H$) and the remainder part ($r(H)$). Any help is appreciated.",,['multivariable-calculus']
21,how do I find the equation of the tangent plane to the parametric surface?,how do I find the equation of the tangent plane to the parametric surface?,,"my professor didn't cover this material that well so I'm not sure how to do this one. the question is: Find an equation of the tangent plane to the parametric surface $x=2r\cos\theta$, $y=5r\sin\theta$, $z=r$, at the point $(2\sqrt2$, $5\sqrt2$, 2) where $r = 2$ and $\theta = \pi/4$","my professor didn't cover this material that well so I'm not sure how to do this one. the question is: Find an equation of the tangent plane to the parametric surface $x=2r\cos\theta$, $y=5r\sin\theta$, $z=r$, at the point $(2\sqrt2$, $5\sqrt2$, 2) where $r = 2$ and $\theta = \pi/4$",,"['calculus', 'multivariable-calculus']"
22,Finding Nash equilibrium aka finding where lines intersect,Finding Nash equilibrium aka finding where lines intersect,,"I am tagging this as multivariable calculus because it potentially involves taking partial derivatives. I am working on some mathematical treatment for Cournot duopoly models (not homework, just preparing for a test tomorrow). A simple two player Cournot model is of the form $p=1000-q_1-q_2$ where $p$ stands for price, and $q_i$ is the quantity produced by firm $i$. The payoffs of each firm are given by $pq$ minus some marginal cost times $q$, say $-10q$. In this case, to find best response for firm $i$, I would take its payoff function $\pi_i=(1000-q_i-q_j)q_i-10q_i$ and take its derivative with respect to $q_i$, getting $1000-2q_i-q_j-10$. I would then set it equal to $0$, solving for $q_i$, and getting $q_i=\frac{990-q_j}{2}$, thus getting an expression maximizing $\pi_i$ in terms of $q_j$. Doing the same thing with $q_j$, I get $q_j=\frac{990-q_i}{2}$ since the function is symmetric. Then I set equations equal to each other/substitute one into the other, and get the Nash equilibrium (profit maximization for both), easy. The issue I am having is dealing with $n$ firms. Assume a>c>0, b>0. Suppose we have total quantity of output $Q=\sum_{i=1}^{n} q_i$ and price be given as $p=a-bQ$. Then let profit of firm $i$ be defined as $\pi_i=p(Q)q_i-cq_i=(a-bQ)q_i-cq_i$. Defining $Q_{-i}$ to denote the sum of quantities of all firms but of firm $i$, we can rewrite the profit function of $i$ as $\pi_i=(a-bq_i-bQ_{-i})q_i-cq_i$ (observe that $q_i+Q_{-i}=Q$). From here on, I can again take a partial derivative with respect to $q_i$, and find the payoff maximization function to be $\pi_i'=a-2bq_i-bQ_{-i}-c$. Rewriting for $q_i$, I get $q_i=\frac{a-bQ_{-i}-c}{2b}$. So, for an arbitrary firm $j$, the maximization function in terms of $Q_{-j}$ will be given by $q_j=\frac{a-bQ_{-j}-c}{2b}$. The problem I have here is that I can't anymore substitute as easily as I did in the first example with only two firms. The book gives me a hint: Summing the best-response functions over the different players will help. I do not know what to make of it. Could you explain how to find an intersection given $n$ such functions in the form presented above? EDIT: I might be onto something here, feedback appreciated! So I decided to follow the books advice and see what comes of it. Summing both sides of $q_i=\frac{a-bQ_{-i}-c}{2b}$ over all the players I get $q_1+\dots +q_n=Q=\frac{na}{2b}-\frac{nc}{2b}-\frac{(n-1)(q_1+\dots + q_n)}{2} \to 2Q=\frac{na}{b}-\frac{nc}{b}-(n-1)(q_1+\dots + q_n)$=$\frac{na}{b}-\frac{nc}{b}-(n-1)(Q) \to (n+1)Q=n\frac{(a-c)}{b} \to Q=\frac{n}{n+1} \frac{(a-c)}{b}$. Thus $q_i=\frac{n}{n+1} \frac{(a-c)}{b} - Q_{-i}$. Hopefully I am at least on the right track, though I can't see where this leads me yet.","I am tagging this as multivariable calculus because it potentially involves taking partial derivatives. I am working on some mathematical treatment for Cournot duopoly models (not homework, just preparing for a test tomorrow). A simple two player Cournot model is of the form $p=1000-q_1-q_2$ where $p$ stands for price, and $q_i$ is the quantity produced by firm $i$. The payoffs of each firm are given by $pq$ minus some marginal cost times $q$, say $-10q$. In this case, to find best response for firm $i$, I would take its payoff function $\pi_i=(1000-q_i-q_j)q_i-10q_i$ and take its derivative with respect to $q_i$, getting $1000-2q_i-q_j-10$. I would then set it equal to $0$, solving for $q_i$, and getting $q_i=\frac{990-q_j}{2}$, thus getting an expression maximizing $\pi_i$ in terms of $q_j$. Doing the same thing with $q_j$, I get $q_j=\frac{990-q_i}{2}$ since the function is symmetric. Then I set equations equal to each other/substitute one into the other, and get the Nash equilibrium (profit maximization for both), easy. The issue I am having is dealing with $n$ firms. Assume a>c>0, b>0. Suppose we have total quantity of output $Q=\sum_{i=1}^{n} q_i$ and price be given as $p=a-bQ$. Then let profit of firm $i$ be defined as $\pi_i=p(Q)q_i-cq_i=(a-bQ)q_i-cq_i$. Defining $Q_{-i}$ to denote the sum of quantities of all firms but of firm $i$, we can rewrite the profit function of $i$ as $\pi_i=(a-bq_i-bQ_{-i})q_i-cq_i$ (observe that $q_i+Q_{-i}=Q$). From here on, I can again take a partial derivative with respect to $q_i$, and find the payoff maximization function to be $\pi_i'=a-2bq_i-bQ_{-i}-c$. Rewriting for $q_i$, I get $q_i=\frac{a-bQ_{-i}-c}{2b}$. So, for an arbitrary firm $j$, the maximization function in terms of $Q_{-j}$ will be given by $q_j=\frac{a-bQ_{-j}-c}{2b}$. The problem I have here is that I can't anymore substitute as easily as I did in the first example with only two firms. The book gives me a hint: Summing the best-response functions over the different players will help. I do not know what to make of it. Could you explain how to find an intersection given $n$ such functions in the form presented above? EDIT: I might be onto something here, feedback appreciated! So I decided to follow the books advice and see what comes of it. Summing both sides of $q_i=\frac{a-bQ_{-i}-c}{2b}$ over all the players I get $q_1+\dots +q_n=Q=\frac{na}{2b}-\frac{nc}{2b}-\frac{(n-1)(q_1+\dots + q_n)}{2} \to 2Q=\frac{na}{b}-\frac{nc}{b}-(n-1)(q_1+\dots + q_n)$=$\frac{na}{b}-\frac{nc}{b}-(n-1)(Q) \to (n+1)Q=n\frac{(a-c)}{b} \to Q=\frac{n}{n+1} \frac{(a-c)}{b}$. Thus $q_i=\frac{n}{n+1} \frac{(a-c)}{b} - Q_{-i}$. Hopefully I am at least on the right track, though I can't see where this leads me yet.",,"['multivariable-calculus', 'game-theory', 'nash-equilibrium']"
23,Formalizing some items in an electrostatics computation,Formalizing some items in an electrostatics computation,,"Consider the question attached (Example 3.4 from Zangwill's Modern Electrodynamics , Ch 3). I can follow the solution quite easily using ""physics math"" but, having just recently finished Spivak's Calculus , I want to formalize the operations performed and am hoping someone can help me with two items in particular. Note that I will refer to the equations in the attached picture as (1) through (6), in the order they appear. The only niceties which I intend to ignore in this ""formalization"" are the rigorous developments of the improper integrals and also the $\epsilon_0$ factor. All integrands etc. are continuous so all of the theorems I invoke apply. For equation (3), I noted that Zangwill is choosing to evaluate the line integral along the path $ \boldsymbol\ell $ which is parametrized as $ \boldsymbol\ell  =  \boldsymbol\ell(r') = r' \mathbf{\hat{r}} $ in the conventional spherical coordinate system, and we are evaluating from $r'=\infty$ to $r' = r$ . But I wanted to challenge myself to compute this line integral via the ""opposite"" path and to show that the same result obtains (as it must, since the line integral from a point to a point is independent of the parametrization of the path which we choose). Thus I tried to consider the parametrization $ \boldsymbol\ell  =  \boldsymbol\ell(u) = r'(u) \mathbf{\hat{r}} $ where $r'(u) = -u$ (perhaps this is my first error -- this was what I thought was appropriate for a path which ""starts"" at infinity and goes radially towards 0) where we go from $u = -\infty$ to $u = -r$ . Thus $ d\boldsymbol\ell  =  \frac{d\boldsymbol\ell}{du}du = -\mathbf{\hat{r}}du $ . Would this process be correct if I plugged it into the LHS equality of equation (3)? More importantly, I want to follow the u-substitution which is performed using the physicist shorthand of ""changing the integration variable"". More rigorously (I think), I could observe that the middle expression in equation (3) is written as (I change the integration variable to $x$ so there is no confusion with derivatives) $$\int_\infty^r dx \ g'(x) (f\circ g)(x)$$ where $$f (x) = \int_0^{1/x} ds \ s^2 \rho(s)$$ $$g(x) = 1/x$$ $$g'(x) = -1/x^2$$ so that by the u-sub theorem (Spivak Theorem 19-2) I have $$\int_\infty^r dx \ g'(x) (f\circ g)(x) = \int_{g(\infty)}^{g(r)} dx f(x)=\int_{0}^{1/r} dx \int_0^{1/x} ds \ s^2 \rho(s)$$ Integrating by parts (Spivak Theorem 19-1), I find then that the integral is given by $$\int_{g(\infty)}^{g(r)} dx f(x)=\int_{0}^{1/r} dx \int_0^{1/x} ds \ s^2 \rho(s) = \left(x\int_0^{1/x} ds \ s^2 \rho(s) \right)\Big|_0^{1/r} - \\ \int_{0}^{1/r} dx \ x (-1/x^2) (1/x)^2 \rho(1/x) = \frac{\int_0^{r} ds \ s^2 \rho(s)}{r} + \int_{0}^{1/r} dx \  \frac{\rho(1/x)}{x^3}$$ where in the last step I've tried to use the fundamental theorem of calculus (Spivak Theorem 14-1) and the chain rule to evaluate the derivative with respect to $x$ of the inner integral. I can then observe that the second term integral in the above has an integrand of the form necessary for u-substitution, with $f(x) = x \rho (x), g(x) = 1/x, g'(x) = -1/x^2$ so that I obtain $$\frac{\int_0^{r} ds \ s^2 \rho(s)}{r} + \int_{0}^{1/r} dx \  \frac{\rho(1/x)}{x^3} = \frac{\int_0^{r} ds \ s^2 \rho(s)}{r} - \int_{\infty}^{r} dx \ x \rho(x) = \frac{\int_0^{r} ds \ s^2 \rho(s)}{r} + \\ \int_{r}^{\infty} dx \ x \rho(x) $$ which is the correct final answer. I'm hoping someone can check and/or set me straight with both steps.","Consider the question attached (Example 3.4 from Zangwill's Modern Electrodynamics , Ch 3). I can follow the solution quite easily using ""physics math"" but, having just recently finished Spivak's Calculus , I want to formalize the operations performed and am hoping someone can help me with two items in particular. Note that I will refer to the equations in the attached picture as (1) through (6), in the order they appear. The only niceties which I intend to ignore in this ""formalization"" are the rigorous developments of the improper integrals and also the factor. All integrands etc. are continuous so all of the theorems I invoke apply. For equation (3), I noted that Zangwill is choosing to evaluate the line integral along the path which is parametrized as in the conventional spherical coordinate system, and we are evaluating from to . But I wanted to challenge myself to compute this line integral via the ""opposite"" path and to show that the same result obtains (as it must, since the line integral from a point to a point is independent of the parametrization of the path which we choose). Thus I tried to consider the parametrization where (perhaps this is my first error -- this was what I thought was appropriate for a path which ""starts"" at infinity and goes radially towards 0) where we go from to . Thus . Would this process be correct if I plugged it into the LHS equality of equation (3)? More importantly, I want to follow the u-substitution which is performed using the physicist shorthand of ""changing the integration variable"". More rigorously (I think), I could observe that the middle expression in equation (3) is written as (I change the integration variable to so there is no confusion with derivatives) where so that by the u-sub theorem (Spivak Theorem 19-2) I have Integrating by parts (Spivak Theorem 19-1), I find then that the integral is given by where in the last step I've tried to use the fundamental theorem of calculus (Spivak Theorem 14-1) and the chain rule to evaluate the derivative with respect to of the inner integral. I can then observe that the second term integral in the above has an integrand of the form necessary for u-substitution, with so that I obtain which is the correct final answer. I'm hoping someone can check and/or set me straight with both steps.","\epsilon_0  \boldsymbol\ell   \boldsymbol\ell  =  \boldsymbol\ell(r') = r' \mathbf{\hat{r}}  r'=\infty r' = r  \boldsymbol\ell  =  \boldsymbol\ell(u) = r'(u) \mathbf{\hat{r}}  r'(u) = -u u = -\infty u = -r  d\boldsymbol\ell  =  \frac{d\boldsymbol\ell}{du}du = -\mathbf{\hat{r}}du  x \int_\infty^r dx \ g'(x) (f\circ g)(x) f (x) = \int_0^{1/x} ds \ s^2 \rho(s) g(x) = 1/x g'(x) = -1/x^2 \int_\infty^r dx \ g'(x) (f\circ g)(x) = \int_{g(\infty)}^{g(r)} dx f(x)=\int_{0}^{1/r} dx \int_0^{1/x} ds \ s^2 \rho(s) \int_{g(\infty)}^{g(r)} dx f(x)=\int_{0}^{1/r} dx \int_0^{1/x} ds \ s^2 \rho(s) = \left(x\int_0^{1/x} ds \ s^2 \rho(s) \right)\Big|_0^{1/r} - \\ \int_{0}^{1/r} dx \ x (-1/x^2) (1/x)^2 \rho(1/x) = \frac{\int_0^{r} ds \ s^2 \rho(s)}{r} + \int_{0}^{1/r} dx \  \frac{\rho(1/x)}{x^3} x f(x) = x \rho (x), g(x) = 1/x, g'(x) = -1/x^2 \frac{\int_0^{r} ds \ s^2 \rho(s)}{r} + \int_{0}^{1/r} dx \  \frac{\rho(1/x)}{x^3} = \frac{\int_0^{r} ds \ s^2 \rho(s)}{r} - \int_{\infty}^{r} dx \ x \rho(x) = \frac{\int_0^{r} ds \ s^2 \rho(s)}{r} + \\ \int_{r}^{\infty} dx \ x \rho(x) ","['multivariable-calculus', 'electromagnetism']"
24,L'hopital's rule mistake for multivariable calculus,L'hopital's rule mistake for multivariable calculus,,"I've been helping some students in multivariable calculus and they're currently working with limits and it's been expressed by the lecturer that L'hopital's is exclusively for single-variable functions. I know this to be true as well, but the students have found that using L'hopital's in the same way that they take partial derivatives, leads them to solutions to their limit problems. Take for example $$\lim_{(x,y)\to(0,0)} \frac{e^{xy}-1}{y}$$ If we plug in our limit, we get an indeterminate form. Students have found that if they ""fix $x$ "" and use L'hopital's in $y$ , they get $$\lim_{(x,y)\to(0,0)} \frac{xe^{xy}}{1} = 0\cdot 1 = 0$$ which is in fact the limit when you do it through less dubious means. It's worked in other problems as well, even those where the limit doesn't exist, students have just used L'hopital's with respect to different variables and found that they ""gave different limits in the end, so the overall limit must not exist"", which for that problem, it definitely did not exist by checking a few simple paths. Here are my questions. Why does this seem to work sometimes? How would you convince students that this is not the way to go? I've tried with counterexamples, but they seem to still prefer it because it works on all their actual homework problems (Marsden).","I've been helping some students in multivariable calculus and they're currently working with limits and it's been expressed by the lecturer that L'hopital's is exclusively for single-variable functions. I know this to be true as well, but the students have found that using L'hopital's in the same way that they take partial derivatives, leads them to solutions to their limit problems. Take for example If we plug in our limit, we get an indeterminate form. Students have found that if they ""fix "" and use L'hopital's in , they get which is in fact the limit when you do it through less dubious means. It's worked in other problems as well, even those where the limit doesn't exist, students have just used L'hopital's with respect to different variables and found that they ""gave different limits in the end, so the overall limit must not exist"", which for that problem, it definitely did not exist by checking a few simple paths. Here are my questions. Why does this seem to work sometimes? How would you convince students that this is not the way to go? I've tried with counterexamples, but they seem to still prefer it because it works on all their actual homework problems (Marsden).","\lim_{(x,y)\to(0,0)} \frac{e^{xy}-1}{y} x y \lim_{(x,y)\to(0,0)} \frac{xe^{xy}}{1} = 0\cdot 1 = 0",['multivariable-calculus']
25,Tips or steps on how to visualise a multivariable function,Tips or steps on how to visualise a multivariable function,,"Question . My doubt is about what steps do you take when given a multivariable function like $z^2 = x^2 - y^2$ , to know how it looks like in general. For example if it looks like a bunch of waves, or a mountain, or a ellipsoid, or an sphere, etc. I want to improve in being able to visualize most functions, of course not knowing exactly how they are but a general shape or knowledge about it. I know it's needed a lot of practice but I'm asking for tips/steps not another thing.","Question . My doubt is about what steps do you take when given a multivariable function like , to know how it looks like in general. For example if it looks like a bunch of waves, or a mountain, or a ellipsoid, or an sphere, etc. I want to improve in being able to visualize most functions, of course not knowing exactly how they are but a general shape or knowledge about it. I know it's needed a lot of practice but I'm asking for tips/steps not another thing.",z^2 = x^2 - y^2,"['calculus', 'multivariable-calculus', 'functions']"
26,Problem 9 M.L Krasnov variational calculus,Problem 9 M.L Krasnov variational calculus,,"Warning: Finding extreme value of a multivariable function My question differs from this since I try to use the Hessian criterion so it is not a repeated question. My question: Problem 9 M.L Krasnov. Find extrema of $f:U \to \mathbb{R}$ where $U=\{(x_1,\ldots,x_n)\in \mathbb{R}^n:x_{i}>0 \text{ for all }i\in \mathbb{N}\}$ and $$f(x_1,x_2,\ldots,x_n)=x_1x_2^2\ldots x_n^n(1-x_1-2x_2-\ldots-nx_n). $$ Well this is my attempt: Critical points are: $$\frac{\partial f}{\partial x_n}=nx_1x_2^2x_3^3(1-x_1-2x_2-\cdots-nx_n)-nx_1x_2^2\cdots x_n^n=0$$ so $$x_1=x_2=\cdots =x_n=\frac{2}{n^2+n+2}.$$ These are critical points of $f$ . Computing second partial derivatives for use Hessian in critical point $x_0=\left(\frac{2}{n^2+n+2},\ldots, \frac{2}{n^2+n+2}\right)$ and suppose no loss of generality $i<j<n$ I get: $$\frac{\partial^2 f}{\partial x_i^2}=i(i-1)\left(\frac{2}{n^2+n+2}\right)^{\frac{n(n+1)}{2}}-2i^2\left(\frac{2}{n^2+n+2}\right)^{\frac{n(n+1)}{2}-1}.$$ So computing the cross derivatives and substituting into $x_1=x_2=\cdots=x_n=(\frac{2}{n^2+n+2})$ $$\frac{\partial^2 f}{\partial x_j \partial x_i}=-ij\left(\frac{2}{n^2+n+2}\right)^{\frac{n(n+1)}{2}-1}.$$ I have tried this exercise for many days without any success, I plan to use Sylvester's criterion to show that this critical point is a local maximum. Sylvester's criterion: The real-symmetric matrix $A$ is positive definite if and only if all the leading principal minors of $A$ are positive. Help me please.","Warning: Finding extreme value of a multivariable function My question differs from this since I try to use the Hessian criterion so it is not a repeated question. My question: Problem 9 M.L Krasnov. Find extrema of where and Well this is my attempt: Critical points are: so These are critical points of . Computing second partial derivatives for use Hessian in critical point and suppose no loss of generality I get: So computing the cross derivatives and substituting into I have tried this exercise for many days without any success, I plan to use Sylvester's criterion to show that this critical point is a local maximum. Sylvester's criterion: The real-symmetric matrix is positive definite if and only if all the leading principal minors of are positive. Help me please.","f:U \to \mathbb{R} U=\{(x_1,\ldots,x_n)\in \mathbb{R}^n:x_{i}>0 \text{ for all }i\in \mathbb{N}\} f(x_1,x_2,\ldots,x_n)=x_1x_2^2\ldots x_n^n(1-x_1-2x_2-\ldots-nx_n).  \frac{\partial f}{\partial x_n}=nx_1x_2^2x_3^3(1-x_1-2x_2-\cdots-nx_n)-nx_1x_2^2\cdots x_n^n=0 x_1=x_2=\cdots =x_n=\frac{2}{n^2+n+2}. f x_0=\left(\frac{2}{n^2+n+2},\ldots, \frac{2}{n^2+n+2}\right) i<j<n \frac{\partial^2 f}{\partial x_i^2}=i(i-1)\left(\frac{2}{n^2+n+2}\right)^{\frac{n(n+1)}{2}}-2i^2\left(\frac{2}{n^2+n+2}\right)^{\frac{n(n+1)}{2}-1}. x_1=x_2=\cdots=x_n=(\frac{2}{n^2+n+2}) \frac{\partial^2 f}{\partial x_j \partial x_i}=-ij\left(\frac{2}{n^2+n+2}\right)^{\frac{n(n+1)}{2}-1}. A A","['multivariable-calculus', 'optimization', 'calculus-of-variations', 'hessian-matrix']"
27,Example of a non-Lipschitz $f \in \mathrm{C}^1(U)$ where $U \subseteq \mathbb{R}^n$ is a non-convex compact connected set,Example of a non-Lipschitz  where  is a non-convex compact connected set,f \in \mathrm{C}^1(U) U \subseteq \mathbb{R}^n,"Consider a smooth function of several variables $f: U \to \mathbb{R} \in \mathrm{C}^1(U)$ where $U \subseteq \mathbb{R}^n$ is a connected set. It can be proven using the mean value theorem that if $U$ is compact and convex (any two points of $U$ can be connected by a segment in $U$ ), then $f$ is Lipschitz, i.e. $$ \exists L \in \mathbb{R} \quad \forall x, y \in U \quad |f(x) - f(y)| \le L \, ||x - y||  .$$ Sketch of a proof goes like this: consider a segment connecting $x$ and $y$ , parametrize it, and consider $f$ along the segment as a function of one variable. Apply mean value theorem to it. Since the differential $\mathrm{d}f$ of $f$ is a continuous mapping on a compact $U$ , it is bounded by Weierstrass theorem, and the inequality follows. See this stackexchange question for a full proof. My textbook (on ODEs) says that if $U$ is compact, but not convex, then $f$ can be non-Lipschitz. It also provides an example. In polar coordinates on a plane, consider $$ (r, \phi) \mapsto (r-1)\phi \quad\text{ with } U = \{(r,\phi) \mid 1 \le r \le 2 \;\land\; \phi \in [(r-1)^2, 2\pi - (r-1)^2] \} $$ Question: Please help me see that this function is indeed non-Lipschitz. What is the motivation for such a strange set of values of $\phi$ ? If you know a simpler example, please share. UPD : My confusion came from the definition of ""f in polar coordinates"". It must be understood that $f$ is still a function of usual cartesian coordinates $x,y$ , but is expressed in terms of the transition-to-polar-coordinates map $(x, y) \mapsto (r(x,y), \phi(x,y))$ as $f(x,y) = (r(x,y)-1) \phi(x,y)$ . The definition of $U$ must be changed accordingly to $$ U = \{(x,y) \mid 1 \le r(x,y) \le 2 \;\land\; \phi(x,y) \in [(r(x,y)-1)^2, 2\pi - (r(x,y)-1)^2]\}.$$","Consider a smooth function of several variables where is a connected set. It can be proven using the mean value theorem that if is compact and convex (any two points of can be connected by a segment in ), then is Lipschitz, i.e. Sketch of a proof goes like this: consider a segment connecting and , parametrize it, and consider along the segment as a function of one variable. Apply mean value theorem to it. Since the differential of is a continuous mapping on a compact , it is bounded by Weierstrass theorem, and the inequality follows. See this stackexchange question for a full proof. My textbook (on ODEs) says that if is compact, but not convex, then can be non-Lipschitz. It also provides an example. In polar coordinates on a plane, consider Question: Please help me see that this function is indeed non-Lipschitz. What is the motivation for such a strange set of values of ? If you know a simpler example, please share. UPD : My confusion came from the definition of ""f in polar coordinates"". It must be understood that is still a function of usual cartesian coordinates , but is expressed in terms of the transition-to-polar-coordinates map as . The definition of must be changed accordingly to","f: U \to \mathbb{R} \in \mathrm{C}^1(U) U \subseteq \mathbb{R}^n U U U f 
\exists L \in \mathbb{R} \quad \forall x, y \in U \quad |f(x) - f(y)| \le L \, ||x - y|| 
. x y f \mathrm{d}f f U U f 
(r, \phi) \mapsto (r-1)\phi \quad\text{ with } U = \{(r,\phi) \mid 1 \le r \le 2 \;\land\; \phi \in [(r-1)^2, 2\pi - (r-1)^2] \}
 \phi f x,y (x, y) \mapsto (r(x,y), \phi(x,y)) f(x,y) = (r(x,y)-1) \phi(x,y) U 
U = \{(x,y) \mid 1
\le r(x,y) \le 2 \;\land\; \phi(x,y) \in [(r(x,y)-1)^2, 2\pi - (r(x,y)-1)^2]\}.","['multivariable-calculus', 'examples-counterexamples', 'lipschitz-functions']"
28,How to prove $\frac{ab^2}{1+2b^2+c^2}+\frac{bc^2}{1+2c^2+a^2}+\frac{ca^2}{1+2a^2+b^2} \le \frac{3}{4}$ if $a+b+c=3$,How to prove  if,\frac{ab^2}{1+2b^2+c^2}+\frac{bc^2}{1+2c^2+a^2}+\frac{ca^2}{1+2a^2+b^2} \le \frac{3}{4} a+b+c=3,"$a,b,c\ge 0,a+b+c=3.$ Prove: $$\frac{ab^2}{1+2b^2+c^2}+\frac{bc^2}{1+2c^2+a^2}+\frac{ca^2}{1+2a^2+b^2} \le \frac{3}{4}$$ This  problem was found in this post . As you can see, no one in that post gave a correct proof but someone pointed out that this inequality might be a problem from Mathematical Reflections . However, the archive of the journal can't be downloaded. Therefore, we must find our own solution. I tried it myself surely. And because all variables show up in a single fraction, we are hard to use technique like tangent line. I tried to homogenise it, and it turned out to be (in case you didn't understand, this triangle denotes the coefficients of ever term of the polynomial. From the left-top is coefficient of $a^7$ , and right-top is that of $b^7$ , and the bottom is that of $c^7$ . And for instance, the "" $762$ "" on the second term of the second line denotes the coefficient of $a^5bc$ ) which is almost impossilbe to create a proof directly from it by hand.(Sure, since expanding is not always a perfect way) Can you come up with a solution with wit?","Prove: This  problem was found in this post . As you can see, no one in that post gave a correct proof but someone pointed out that this inequality might be a problem from Mathematical Reflections . However, the archive of the journal can't be downloaded. Therefore, we must find our own solution. I tried it myself surely. And because all variables show up in a single fraction, we are hard to use technique like tangent line. I tried to homogenise it, and it turned out to be (in case you didn't understand, this triangle denotes the coefficients of ever term of the polynomial. From the left-top is coefficient of , and right-top is that of , and the bottom is that of . And for instance, the "" "" on the second term of the second line denotes the coefficient of ) which is almost impossilbe to create a proof directly from it by hand.(Sure, since expanding is not always a perfect way) Can you come up with a solution with wit?","a,b,c\ge 0,a+b+c=3. \frac{ab^2}{1+2b^2+c^2}+\frac{bc^2}{1+2c^2+a^2}+\frac{ca^2}{1+2a^2+b^2} \le \frac{3}{4} a^7 b^7 c^7 762 a^5bc","['multivariable-calculus', 'inequality', 'summation', 'alternative-proof', 'buffalo-way']"
29,Exchanging the order of integration in improper double integrals,Exchanging the order of integration in improper double integrals,,"My question arise from Improper Riemann Integrals by Ioannis M. Roussos Why the improper double integral is absolutely convergent,then we can exchange order of integration in improper Riemann sense? I really don't understand that it said $\textbf{Condition II}\ \Longleftrightarrow \textbf{Condition III} \Longleftrightarrow \textbf{Condition IV}$ and each one of them can ensure the equalities $$ \iint_{\mathbf{R}^2} f(x,y) \, \mathrm{d}x \, \mathrm{d}y = \int_{-\infty}^\infty \left[ \int_{-\infty}^\infty f(x,y) \, \mathrm{d}x \right] \, \mathrm{d}y = \int_{-\infty}^\infty \left[ \int_{-\infty}^\infty f(x,y) \, \mathrm{d}y \right] \, \mathrm{d}x $$ hold. This has been bothering me for a long time！Any help you can provide would be greatly appreciated! I got a counterexample function $f(x,y)$ as following which is discontinuous on $[0,1]\times[0,1].$ $f(x,y)$ is defined on $\left [0,1  \right ] \times\left [ 0,1 \right ] $ $$f(x,y)=\begin{cases}   2^n,\quad x=\frac{2m-1}{2^n}\text{ and }0<y\le\frac{1}{2^n}\\ \quad \left ( n=1,2,\ldots;m=1,2,\ldots,2^{n-1} \right ) ,\\   0,  \text{ otherwise}. \end{cases}$$ $\textbf(1).$ For any $\varepsilon>0,$ $$\iint_{[0,1]\times[\varepsilon ,1]} f(x,y)\,dx\,dy=0 \Rightarrow $$ $$\iint_{[0,1]\times[0 ,1]} f(x,y) \, dx\, dy = \lim_{\varepsilon \to 0} \iint_{[0,1]\times[\varepsilon ,1]} f(x,y) \, dx \, dy = 0.$$ $\textbf(2).$ For any fixed $y\in\left [ 0,1 \right ],$ $$ \int_0^1 f(x,y) \, dx=0\Rightarrow \int_0^1 dy \int_0^1 f(x,y) \, dx = 0.$$ $\textbf(3).$ When $x\in[0,1]$ but $x\ne\frac{2m-1}{2^n}\left ( n=1,2,\ldots;m=1,2,\ldots,2^{n-1} \right ) ,$ we get $\int_0^1 f(x,y) \, dy = 0;$ When $x\in[0,1]$ and $x=\frac{2m-1}{2^n}\left ( n=1,2,\ldots;m=1,2,\ldots,2^{n-1} \right ) ,$ we get $$\int_0^1 f(x,y) \, dy = \int_0^{\frac{1}{2^n}}f(x,y) \, dy=1.$$ So $$ \int_0^1 dx\int_0^1 f(x,y) \, dy$$ does not exist in Riemann sense! I am definitely not disagreeing with Fubini's Theorem. Actually,I want find a counterexample/example $f(x,y)$ is bounded or unbounded,continuous  on some region $\mathcal{R}$ which is bounde or unbounded and not necessarily closed or open and the function $f(x,y)$ satisfies the $\textbf{Condition IV}$ , but $$ \text{ the improper double integral } \iint_{\mathbf{R}^2}  \lvert f(x,y) \rvert \, \mathrm{d}x \, \mathrm{d}y < \infty \nRightarrow \int_{-\infty}^\infty \left[ \int_{-\infty}^\infty \lvert f(x,y) \rvert \, \mathrm{d}x \right] \mathrm{d}y < \infty ;$$ and $$\text{ the improper double integral } \iint_{\mathbf{R}^2}  \lvert f(x,y) \rvert \, \mathrm{d}x \, \mathrm{d}y < \infty \nRightarrow \int_{-\infty}^\infty \left[ \int_{-\infty}^\infty \lvert f(x,y) \rvert \, \mathrm{d}y \right] \, \mathrm{d}x < \infty.$$ further,the the equalities $$ \iint_{\mathbf{R}^2} f(x,y) \, \mathrm{d}x \, \mathrm{d}y = \int_{-\infty}^\infty \left[ \int_{-\infty}^\infty f(x,y) \, \mathrm{d}x \right] \, \mathrm{d}y = \int_{-\infty}^\infty \left[ \int_{-\infty}^\infty f(x,y) \, \mathrm{d}y \right] \, \mathrm{d}x $$ can not hold.","My question arise from Improper Riemann Integrals by Ioannis M. Roussos Why the improper double integral is absolutely convergent,then we can exchange order of integration in improper Riemann sense? I really don't understand that it said and each one of them can ensure the equalities hold. This has been bothering me for a long time！Any help you can provide would be greatly appreciated! I got a counterexample function as following which is discontinuous on is defined on For any For any fixed When but we get When and we get So does not exist in Riemann sense! I am definitely not disagreeing with Fubini's Theorem. Actually,I want find a counterexample/example is bounded or unbounded,continuous  on some region which is bounde or unbounded and not necessarily closed or open and the function satisfies the , but and further,the the equalities can not hold.","\textbf{Condition II}\ \Longleftrightarrow \textbf{Condition III} \Longleftrightarrow \textbf{Condition IV}  \iint_{\mathbf{R}^2} f(x,y) \, \mathrm{d}x \, \mathrm{d}y
= \int_{-\infty}^\infty \left[ \int_{-\infty}^\infty f(x,y) \, \mathrm{d}x \right] \, \mathrm{d}y
= \int_{-\infty}^\infty \left[ \int_{-\infty}^\infty f(x,y) \, \mathrm{d}y \right] \, \mathrm{d}x  f(x,y) [0,1]\times[0,1]. f(x,y) \left [0,1  \right ] \times\left [ 0,1 \right ]  f(x,y)=\begin{cases}
  2^n,\quad x=\frac{2m-1}{2^n}\text{ and }0<y\le\frac{1}{2^n}\\
\quad \left ( n=1,2,\ldots;m=1,2,\ldots,2^{n-1} \right ) ,\\
  0,  \text{ otherwise}.
\end{cases} \textbf(1). \varepsilon>0, \iint_{[0,1]\times[\varepsilon ,1]} f(x,y)\,dx\,dy=0 \Rightarrow  \iint_{[0,1]\times[0 ,1]} f(x,y) \, dx\, dy = \lim_{\varepsilon \to 0} \iint_{[0,1]\times[\varepsilon ,1]} f(x,y) \, dx \, dy = 0. \textbf(2). y\in\left [ 0,1 \right ],  \int_0^1 f(x,y) \, dx=0\Rightarrow \int_0^1 dy \int_0^1 f(x,y) \, dx = 0. \textbf(3). x\in[0,1] x\ne\frac{2m-1}{2^n}\left ( n=1,2,\ldots;m=1,2,\ldots,2^{n-1} \right ) , \int_0^1 f(x,y) \, dy = 0; x\in[0,1] x=\frac{2m-1}{2^n}\left ( n=1,2,\ldots;m=1,2,\ldots,2^{n-1} \right ) , \int_0^1 f(x,y) \, dy = \int_0^{\frac{1}{2^n}}f(x,y) \, dy=1.  \int_0^1 dx\int_0^1 f(x,y) \, dy f(x,y) \mathcal{R} f(x,y) \textbf{Condition IV}  \text{ the improper double integral } \iint_{\mathbf{R}^2}  \lvert f(x,y) \rvert \, \mathrm{d}x \, \mathrm{d}y < \infty \nRightarrow \int_{-\infty}^\infty \left[ \int_{-\infty}^\infty \lvert f(x,y) \rvert \, \mathrm{d}x \right] \mathrm{d}y < \infty ; \text{ the improper double integral } \iint_{\mathbf{R}^2}  \lvert f(x,y) \rvert \, \mathrm{d}x \, \mathrm{d}y < \infty \nRightarrow \int_{-\infty}^\infty \left[ \int_{-\infty}^\infty \lvert f(x,y) \rvert \, \mathrm{d}y \right] \, \mathrm{d}x < \infty.  \iint_{\mathbf{R}^2} f(x,y) \, \mathrm{d}x \, \mathrm{d}y
= \int_{-\infty}^\infty \left[ \int_{-\infty}^\infty f(x,y) \, \mathrm{d}x \right] \, \mathrm{d}y
= \int_{-\infty}^\infty \left[ \int_{-\infty}^\infty f(x,y) \, \mathrm{d}y \right] \, \mathrm{d}x ","['real-analysis', 'multivariable-calculus', 'improper-integrals', 'multiple-integral']"
30,existence of certain Second-order partial derivatives,existence of certain Second-order partial derivatives,,"The function $f(x,y)=|y|$ is an example for the following conditions: $f$ is defined on the whole $\mathbb{R}^2$ $\dfrac{\partial^2 f}{\partial x\,\partial x}$ and $\dfrac{\partial^2 f}{\partial x\,\partial y}$ do exist at any point $(x_0,y_0)\in\mathbb{R}^2$ $\dfrac{\partial^2 f}{\partial y\,\partial x}$ and $\dfrac{\partial^2 f}{\partial y\,\partial y}$ don't exist at some point $(x_0,y_0)$ Now I would like to find an example for slightly changed conditions: $f$ is defined on the whole $\mathbb{R}^2$ $\dfrac{\partial^2 f}{\partial x\,\partial x}$ and $\dfrac{\partial^2 f}{\partial y\,\partial x}$ do exist at any point $(x_0,y_0)\in\mathbb{R}^2$ $\dfrac{\partial^2 f}{\partial y\,\partial y}$ and $\dfrac{\partial^2 f}{\partial x\,\partial y}$ don't exist at some point $(x_0,y_0)$ and I'm not able to do this. Is this possible?",The function is an example for the following conditions: is defined on the whole and do exist at any point and don't exist at some point Now I would like to find an example for slightly changed conditions: is defined on the whole and do exist at any point and don't exist at some point and I'm not able to do this. Is this possible?,"f(x,y)=|y| f \mathbb{R}^2 \dfrac{\partial^2 f}{\partial x\,\partial x} \dfrac{\partial^2 f}{\partial x\,\partial y} (x_0,y_0)\in\mathbb{R}^2 \dfrac{\partial^2 f}{\partial y\,\partial x} \dfrac{\partial^2 f}{\partial y\,\partial y} (x_0,y_0) f \mathbb{R}^2 \dfrac{\partial^2 f}{\partial x\,\partial x} \dfrac{\partial^2 f}{\partial y\,\partial x} (x_0,y_0)\in\mathbb{R}^2 \dfrac{\partial^2 f}{\partial y\,\partial y} \dfrac{\partial^2 f}{\partial x\,\partial y} (x_0,y_0)","['multivariable-calculus', 'derivatives', 'partial-derivative']"
31,Taylor Expansions with sum of vectors,Taylor Expansions with sum of vectors,,"I have a function $f:\mathbb{R}^n\to\mathbb{R}$ with gradient $\nabla_x f$ and Hessian $H_f$ . Suppose $x, y, z, w\in\mathbb{R}^n$ and $\delta, \epsilon\in\mathbb{R}$ . If I have $x = y + \delta z$ then I could Taylor Expand $f$ around $y$ as $$ f(x) = f(y) + \delta \nabla_xf(x)\bigg|_y z + \frac{\delta^2}{2}z^\top H_f\bigg|_y z + \mathcal{O}(\delta^3) $$ Now I have multiple vectors summed up to $y$ . For instance $x = y + \delta z + \epsilon w$ . How can I Taylor Expand $f$ around $y$ now?",I have a function with gradient and Hessian . Suppose and . If I have then I could Taylor Expand around as Now I have multiple vectors summed up to . For instance . How can I Taylor Expand around now?,"f:\mathbb{R}^n\to\mathbb{R} \nabla_x f H_f x, y, z, w\in\mathbb{R}^n \delta, \epsilon\in\mathbb{R} x = y + \delta z f y 
f(x) = f(y) + \delta \nabla_xf(x)\bigg|_y z + \frac{\delta^2}{2}z^\top H_f\bigg|_y z + \mathcal{O}(\delta^3)
 y x = y + \delta z + \epsilon w f y","['real-analysis', 'calculus']"
32,Can the gradient operator $\nabla_\mathbf{x}$ be treated as a standalone vector?,Can the gradient operator  be treated as a standalone vector?,\nabla_\mathbf{x},"Let $f:\mathbb{R}^n\rightarrow\mathbb{R}$ be a scalar function. Then, the gradient of $f(\mathbf{x})$ is defined as: $$ \nabla_\mathbf{x} f(\mathbf{x}) = \begin{bmatrix} \frac{\partial f(\mathbf{x})}{\partial x_1} \\ \frac{\partial f(\mathbf{x})}{\partial x_2} \\ \vdots \\ \frac{\partial f(\mathbf{x})}{\partial x_n} \end{bmatrix} $$ However, if $f(\mathbf{x})$ is a scalar, then wouldn't the following also be valid? $$ (\nabla_\mathbf{x}) (f(\mathbf{x})) = \begin{bmatrix} \frac{\partial}{\partial x_1} \\ \frac{\partial}{\partial x_2} \\ \vdots \\ \frac{\partial}{\partial x_n} \end{bmatrix} f(\mathbf{x}) =  \begin{bmatrix} \frac{\partial f(\mathbf{x})}{\partial x_1} \\ \frac{\partial f(\mathbf{x})}{\partial x_2} \\ \vdots \\ \frac{\partial f(\mathbf{x})}{\partial x_n} \end{bmatrix} $$ In other words, the gradient $\nabla_\mathbf{x}$ is treated as a standalone vector, and then it is multiplied by the scalar $f(\mathbf{x})$ . Interestingly, if this is indeed true, then I can build the Hessian matrix of $f(\mathbf{x})$ using the following outer product: $$ (\nabla_x \nabla_x^T) (f(\mathbf{x})) $$ In other words, I left-multiply the row vector $\nabla_x^T$ by the column vector $\nabla_x$ to get an $n \times n$ matrix of second-order partial derivative operators. I then multiply this matrix by the scalar $f(\mathbf{x})$ to get the Hessian matrix. However, I am wondering if this is just a coincidence.","Let be a scalar function. Then, the gradient of is defined as: However, if is a scalar, then wouldn't the following also be valid? In other words, the gradient is treated as a standalone vector, and then it is multiplied by the scalar . Interestingly, if this is indeed true, then I can build the Hessian matrix of using the following outer product: In other words, I left-multiply the row vector by the column vector to get an matrix of second-order partial derivative operators. I then multiply this matrix by the scalar to get the Hessian matrix. However, I am wondering if this is just a coincidence.","f:\mathbb{R}^n\rightarrow\mathbb{R} f(\mathbf{x}) 
\nabla_\mathbf{x} f(\mathbf{x}) =
\begin{bmatrix} \frac{\partial f(\mathbf{x})}{\partial x_1} \\
\frac{\partial f(\mathbf{x})}{\partial x_2} \\
\vdots \\
\frac{\partial f(\mathbf{x})}{\partial x_n}
\end{bmatrix}
 f(\mathbf{x}) 
(\nabla_\mathbf{x}) (f(\mathbf{x})) =
\begin{bmatrix} \frac{\partial}{\partial x_1} \\
\frac{\partial}{\partial x_2} \\
\vdots \\
\frac{\partial}{\partial x_n}
\end{bmatrix} f(\mathbf{x}) = 
\begin{bmatrix} \frac{\partial f(\mathbf{x})}{\partial x_1} \\
\frac{\partial f(\mathbf{x})}{\partial x_2} \\
\vdots \\
\frac{\partial f(\mathbf{x})}{\partial x_n}
\end{bmatrix}
 \nabla_\mathbf{x} f(\mathbf{x}) f(\mathbf{x}) 
(\nabla_x \nabla_x^T) (f(\mathbf{x}))
 \nabla_x^T \nabla_x n \times n f(\mathbf{x})",['multivariable-calculus']
33,Proof of the Fundamental Theorem of Space curves using Rigid Transformation by Peter Baxandall ( Vector Calculus ),Proof of the Fundamental Theorem of Space curves using Rigid Transformation by Peter Baxandall ( Vector Calculus ),,"I am reading Vector calculus by Peter Baxandall which proves the Fundamental theorem of Space curves ( Curves with equal torsion and curvature are identical except probably their position) in the following manner: In the proof, the author says : Pick any $p \in E$ . Hold $C_g$ fixed and move $C_h$ rigidly in $\Bbb R^3$ until $T_h(p) = T_g(p) , \cdots$ . I do not see very clearly the motive and mechanism by which the author is able to do so. I understand rigid transformation as something which preserves the length of the curve. However, we may even have to employ rotation to make the unit tangent vector $T_g$ and $T_h$ the same. But, in the last line, he ultimately says that $C_h$ is a translation of $C_g$ . Also, I couldn't find where has the author used the fact that the torsion and curvatures of the two curves are equal . $$\phi = T_g \cdot T_h + N_g \cdot N_h + B_g \cdot B_h \\ \implies \phi' = T_g' \cdot T_h + T_g \cdot T_h' + N_g' \cdot N_h + N_g \cdot N_h' +  B_g' \cdot B_h + B_g \cdot B_h'$$ . But since, we have already have : $T_g=T_h,N_g=N_h,B_g=B_h$ , thus : $T_g⋅T_h'=0=T_g'⋅T_h$ . Similarily, for others, each dot product turns out to be $0$ . We haven't seemed to use the fact that the torsions and curvatures of the two curves are equal ? Could someone please explain what's actually going on. Thanks a lot! NOTE : $T,N,B$ represent the tangent, normal and bi-normal unit - vector respectively","I am reading Vector calculus by Peter Baxandall which proves the Fundamental theorem of Space curves ( Curves with equal torsion and curvature are identical except probably their position) in the following manner: In the proof, the author says : Pick any . Hold fixed and move rigidly in until . I do not see very clearly the motive and mechanism by which the author is able to do so. I understand rigid transformation as something which preserves the length of the curve. However, we may even have to employ rotation to make the unit tangent vector and the same. But, in the last line, he ultimately says that is a translation of . Also, I couldn't find where has the author used the fact that the torsion and curvatures of the two curves are equal . . But since, we have already have : , thus : . Similarily, for others, each dot product turns out to be . We haven't seemed to use the fact that the torsions and curvatures of the two curves are equal ? Could someone please explain what's actually going on. Thanks a lot! NOTE : represent the tangent, normal and bi-normal unit - vector respectively","p \in E C_g C_h \Bbb R^3 T_h(p) = T_g(p) , \cdots T_g T_h C_h C_g \phi = T_g \cdot T_h + N_g \cdot N_h + B_g \cdot B_h \\ \implies \phi' = T_g' \cdot T_h + T_g \cdot T_h' + N_g' \cdot N_h + N_g \cdot N_h' +  B_g' \cdot B_h + B_g \cdot B_h' T_g=T_h,N_g=N_h,B_g=B_h T_g⋅T_h'=0=T_g'⋅T_h 0 T,N,B","['multivariable-calculus', 'curves']"
34,"Continuity of $x\sin\frac{1}{y}$ at $(x, 0)$",Continuity of  at,"x\sin\frac{1}{y} (x, 0)","I need to check if function is continue in $(x,0)$ $$f(x,y) = \left\{ 	\begin{array}{ll} 		x\sin\frac{1}{y}  & \mbox{if } y \ne 0 \\ 		0 & \mbox{if } y = 0 	\end{array} \right.$$ Can someone help me understand if I approached this correctly? First, I check the following limit $$\lim_{(x,y) \to (x,0)} x\sin\frac{1}{y}$$ Which does not exist. I can write it as $$\lim_{(x,y) \to (x,0)} x \lim_{(x,y) \to (x,0)} \sin\frac{1}{y}$$ The first limit tends to $x$ itself, the second one diverges. For $x=0$ the limit $$\lim_{(x,y) \to (0,0)} x\sin\frac{1}{y} = 0$$ Indeed, by the squeeze theorem I can write $$-1\leq\sin\frac{1}{y} \leq 1 $$ $$-x \leq x\sin\frac{1}{y} \leq x$$ Thus reducing the limit to $$ \lim_{(x,y) \to (0,0)} x = 0$$ Thus $f(x,y)$ is continuous $\{ \forall (x, y) \in \Re^2 \mid x \ne 0 \}$ I'm a beginner, thank you in advance!","I need to check if function is continue in Can someone help me understand if I approached this correctly? First, I check the following limit Which does not exist. I can write it as The first limit tends to itself, the second one diverges. For the limit Indeed, by the squeeze theorem I can write Thus reducing the limit to Thus is continuous I'm a beginner, thank you in advance!","(x,0) f(x,y) =
\left\{
	\begin{array}{ll}
		x\sin\frac{1}{y}  & \mbox{if } y \ne 0 \\
		0 & \mbox{if } y = 0
	\end{array}
\right. \lim_{(x,y) \to (x,0)} x\sin\frac{1}{y} \lim_{(x,y) \to (x,0)} x \lim_{(x,y) \to (x,0)} \sin\frac{1}{y} x x=0 \lim_{(x,y) \to (0,0)} x\sin\frac{1}{y} = 0 -1\leq\sin\frac{1}{y} \leq 1  -x \leq x\sin\frac{1}{y} \leq x  \lim_{(x,y) \to (0,0)} x = 0 f(x,y) \{ \forall (x, y) \in \Re^2 \mid x \ne 0 \}","['calculus', 'multivariable-calculus', 'continuity']"
35,Calculus/real analysis problem from a Differential geometry exercice,Calculus/real analysis problem from a Differential geometry exercice,,"I found a calculus/real analysis problem from a differential geometry exercice of surfaces in $\mathbb{R}^3$ . Consider $U=\mathbb{R}\times(0,1/2)$ and let be $f:U\subset\mathbb{R}^2\to \mathbb{R}$ given by $$f(x,y)=(2x-\sqrt{4x^2+1})y\sqrt{1-4y^2}+\sqrt{x^2+y^2}.$$ I ploted the graph of $(x,y,f(x,y))$ in $\mathbb{R}^3$ and I saw that $f\geq 0$ , but I don't realized how prove that $f\geq 0$ on $U$ by hand. First, I worked with the inequality $(2x-\sqrt{4x^2+1})y\sqrt{1-4y^2}+\sqrt{x^2+y^2}\geq 0$ and the assumption that $y\in(0,1/2)$ but nothing. Secondly, I tried a change of variables to polar coordinates, but nothing too. Some help for this? I will appreciate, thanks.","I found a calculus/real analysis problem from a differential geometry exercice of surfaces in . Consider and let be given by I ploted the graph of in and I saw that , but I don't realized how prove that on by hand. First, I worked with the inequality and the assumption that but nothing. Secondly, I tried a change of variables to polar coordinates, but nothing too. Some help for this? I will appreciate, thanks.","\mathbb{R}^3 U=\mathbb{R}\times(0,1/2) f:U\subset\mathbb{R}^2\to \mathbb{R} f(x,y)=(2x-\sqrt{4x^2+1})y\sqrt{1-4y^2}+\sqrt{x^2+y^2}. (x,y,f(x,y)) \mathbb{R}^3 f\geq 0 f\geq 0 U (2x-\sqrt{4x^2+1})y\sqrt{1-4y^2}+\sqrt{x^2+y^2}\geq 0 y\in(0,1/2)","['real-analysis', 'calculus', 'multivariable-calculus', 'differential-geometry']"
36,"$\frac{1}{3}\sum_{cyc}\frac{1}{\sqrt{1+a}}\ge\frac{1}{\sqrt{1+\sqrt[3]{abc}}}$ if $\;a, b, c\;$ are positive reals s.t. $abc\ge2^9$",if  are positive reals s.t.,"\frac{1}{3}\sum_{cyc}\frac{1}{\sqrt{1+a}}\ge\frac{1}{\sqrt{1+\sqrt[3]{abc}}} \;a, b, c\; abc\ge2^9","$$\frac{1}{3}\sum_{cyc}\frac{1}{\sqrt{1+a}}\ge\frac{1}{\sqrt{1+\sqrt[3]{abc}}}$$ if it is given that $\;a, b, c\;$ are positive reals s.t. $abc\ge2^9$ . I have tried (many) dead-end solutions. Initially setting $\;abc\;$ to be greater than $512$ on the left-hand-side to make it $\frac{1}{3}$ lead to an untrue inequality. Letting $f(x)=\frac{1}{\sqrt{1+x}}$ we find that $LHS=\frac{1}{3}\big(f(a)+f(b)+f(c)\big)$ and, since $f''\ge0$ , $f$ is convex and, by Jensen's inequality, $$\frac{1}{3}\big(f(a)+f(b)+f(c)\big)\ge f\left(\frac{a+b+c}{3}\right)= \frac{1}{\sqrt{1+\frac{a+b+c}{3}}},$$ but the result is no longer greater than the original RHS; it is in fact less than or equal to the original RHS, as can be proven with a simple application of AM-GM. Do you have any hints on what to try next? I feel that I have exhausted all of my ideas at this point. Thanks beforehand!","if it is given that are positive reals s.t. . I have tried (many) dead-end solutions. Initially setting to be greater than on the left-hand-side to make it lead to an untrue inequality. Letting we find that and, since , is convex and, by Jensen's inequality, but the result is no longer greater than the original RHS; it is in fact less than or equal to the original RHS, as can be proven with a simple application of AM-GM. Do you have any hints on what to try next? I feel that I have exhausted all of my ideas at this point. Thanks beforehand!","\frac{1}{3}\sum_{cyc}\frac{1}{\sqrt{1+a}}\ge\frac{1}{\sqrt{1+\sqrt[3]{abc}}} \;a, b, c\; abc\ge2^9 \;abc\; 512 \frac{1}{3} f(x)=\frac{1}{\sqrt{1+x}} LHS=\frac{1}{3}\big(f(a)+f(b)+f(c)\big) f''\ge0 f \frac{1}{3}\big(f(a)+f(b)+f(c)\big)\ge f\left(\frac{a+b+c}{3}\right)= \frac{1}{\sqrt{1+\frac{a+b+c}{3}}},","['multivariable-calculus', 'inequality', 'contest-math', 'substitution', 'holder-inequality']"
37,Smooth map with null differential at each point are constant on the connected component of the domain,Smooth map with null differential at each point are constant on the connected component of the domain,,"Let $F:M\to N$ be a smooth map between smooth manifolds $M$ and $N$ (with or without boundary). I want to show that $dF_p:T_pM\to T_{F(p)}N$ is the zero map for each $p\in M$ if and only if $F$ is constant on each component of $M$ . Here is my argument: Suppose $F$ is constant on each component of $M$ , and let's show that $dF_p:T_pM\to T_{F(p)}N$ is the zero map for each $p\in M$ . Let $p\in M$ and let $U$ be the component of $M$ containing $p$ . Since $M$ is locally path connected, I know that $U$ is open in $M$ . By hypothesis we have $F_{|U}:U\to N$ is consant. Then $d(F_{|U})_p:T_pU\to T_{F(p)}N$ is the zero map: let be $v\in T_pU$ and $f\in C^{\infty}(N)$ , then $d(F_{|U})_p(v)(f)=v(f\circ F_{|U})=0 $ since $f\circ F_{|U}$ is costant from $U$ to $\mathbb{R}$ . Since $d(\iota)_p:T_pU \to T_pM$ is isomprhism, and since we have that $dF_p\circ d(\iota)_p=d(F_{|U})_p$ , we have that also $dF_p$ is the zero map. We have to prove the converse (But here my problems begin) The best I have thought is this: For simplicity suppose $M$ itself is connected. We know that $dF_p:T_pM\to T_{F(p)}N$ is the zero map for each $p∈M$ and we have to prove that $F:M→N $ is constant. I want to show that $F$ is locally constant, i.e. each point $p$ in $M$ has an open neighborhood in $M$ such that $F$ is constant on this neighborhood. Let $p\in M$ and let $(U,\phi=(x^1,\dots,x^m))$ be a chart on $M$ in $p$ . Then $\{{\frac{\partial}{\partial x^i}}|_p\}$ is a basis for $T_pM$ . By hypothesis we know that $dF_p(\frac{\partial}{\partial x^i}|_p)(f)=0$ for each $f∈C^∞(N)$ , i.e. $\partial_i|_{\phi(p)}(f\circ F \circ \phi^{-1})=0$ . We can suppose that $U$ and thus $\phi(U)$ are connceted, so by Ordinary Analysis we have thaht $f\circ F \circ \phi^{-1}$ is constant on $\phi(U)$ . But $\phi$ is a diffeomorphism, so we have thaht $f \circ F:U\to N$ is constant, for each $f∈C^∞(N)$ . Now suppose there are $p\ne q \in M$ such that $F(p) \ne F(q)$ . I want to construct a function $f∈C^∞(N)$ such that $f(F(p))\ne f(F(q))$ . The best I come out is: suppose there is a smooth chart $(V,\psi)$ on $N$ containing $F(p)$ and $F(q)$ and such that there is $K$ closed subset of $N$ such that $K\subseteq V$ . Since $\psi $ is injective, then $\psi (F(p))\ne \psi( F(q))$ , so they differ by at least a component, say the $j$ component. Let $\pi_j:\mathbb{R}^n\to \mathbb{R}$ the $j$ projection, and consider $\psi \circ \pi_j:\psi(V)\to \mathbb{R}$ . Extend this function to a function $f∈C^∞(N)$ such that $f$ and $\psi \circ \pi_j$ agree on $K$ . Then we have $f(F(p))\ne f(F(q))$ which is a contraddiction. I know that this is quite completely wrong (I did many not-necessarily-true assumptions). And maybe this argument does not work in the case of manifolds with boundary. So can anyone help me with observations/ sugestions/ hints, or even a full solution? Thank you. I mention that this is Problem 3.1 in John Lee's Book ""Introduction to smooth manifolds, 2 edition"" EDIT Thanks to the hint of @Ted Shifrin I come with another argument. Let's start from a fact that I konw: if $A$ is an open subset of $\mathbb{R}^n$ and $A$ is connected, then each smooth function $f:A\to \mathbb{R}$ whose partial derivatives are zero in $A$ , is constant. Now we can generalize this as: if $A$ is an open subset of $\mathbb{R}^n$ and $A$ is connected, then each smooth function $f:A\to \mathbb{R}^m$ such that all component functions have partial derivatives that are zero in $A$ , is constant. (We can deduce this from the former, simply noting that each component function is constant, right?) Now, let $p\in M$ and $(U,\phi)$ smooth chart on $M$ in $p$ and $(V,\psi)$ smooth chart on $N$ in $F(p)$ with $F(U)\subseteq V$ . Then $\psi \circ F \circ \phi^{-1}:\phi(U)\to \psi (V)$ is smooth, and we can suppose $U$ is connceted, and so is $\phi(U)$ . We have that $d(\psi \circ F \circ \phi^{-1})_{\phi(q)}=d\psi_{F(q)} \circ dF_q \circ d(\phi^{-1})_{\phi(q)}$ and since $dF_q$ is zero for all $q \in U$ , then also $d(\psi \circ F \circ \phi^{-1})_{x}$ is zero for all $x \in \phi(U)$ . But this is the jacobian matrix of $\psi \circ F \circ \phi^{-1}:\phi(U)\to \psi (V)$ . So by the above discussion we have that $\psi \circ F \circ \phi^{-1}:\phi(U)\to \psi (V)$ is constant, and then $F$ is constant on $U$ , right? Then, since $F$ is locally constant, we have that $F$ is constant on each component of $M$ , right? Is my new argument correct? Have I used well the Ted's hint?","Let be a smooth map between smooth manifolds and (with or without boundary). I want to show that is the zero map for each if and only if is constant on each component of . Here is my argument: Suppose is constant on each component of , and let's show that is the zero map for each . Let and let be the component of containing . Since is locally path connected, I know that is open in . By hypothesis we have is consant. Then is the zero map: let be and , then since is costant from to . Since is isomprhism, and since we have that , we have that also is the zero map. We have to prove the converse (But here my problems begin) The best I have thought is this: For simplicity suppose itself is connected. We know that is the zero map for each and we have to prove that is constant. I want to show that is locally constant, i.e. each point in has an open neighborhood in such that is constant on this neighborhood. Let and let be a chart on in . Then is a basis for . By hypothesis we know that for each , i.e. . We can suppose that and thus are connceted, so by Ordinary Analysis we have thaht is constant on . But is a diffeomorphism, so we have thaht is constant, for each . Now suppose there are such that . I want to construct a function such that . The best I come out is: suppose there is a smooth chart on containing and and such that there is closed subset of such that . Since is injective, then , so they differ by at least a component, say the component. Let the projection, and consider . Extend this function to a function such that and agree on . Then we have which is a contraddiction. I know that this is quite completely wrong (I did many not-necessarily-true assumptions). And maybe this argument does not work in the case of manifolds with boundary. So can anyone help me with observations/ sugestions/ hints, or even a full solution? Thank you. I mention that this is Problem 3.1 in John Lee's Book ""Introduction to smooth manifolds, 2 edition"" EDIT Thanks to the hint of @Ted Shifrin I come with another argument. Let's start from a fact that I konw: if is an open subset of and is connected, then each smooth function whose partial derivatives are zero in , is constant. Now we can generalize this as: if is an open subset of and is connected, then each smooth function such that all component functions have partial derivatives that are zero in , is constant. (We can deduce this from the former, simply noting that each component function is constant, right?) Now, let and smooth chart on in and smooth chart on in with . Then is smooth, and we can suppose is connceted, and so is . We have that and since is zero for all , then also is zero for all . But this is the jacobian matrix of . So by the above discussion we have that is constant, and then is constant on , right? Then, since is locally constant, we have that is constant on each component of , right? Is my new argument correct? Have I used well the Ted's hint?","F:M\to N M N dF_p:T_pM\to T_{F(p)}N p\in M F M F M dF_p:T_pM\to T_{F(p)}N p\in M p\in M U M p M U M F_{|U}:U\to N d(F_{|U})_p:T_pU\to T_{F(p)}N v\in T_pU f\in C^{\infty}(N) d(F_{|U})_p(v)(f)=v(f\circ F_{|U})=0  f\circ F_{|U} U \mathbb{R} d(\iota)_p:T_pU \to T_pM dF_p\circ d(\iota)_p=d(F_{|U})_p dF_p M dF_p:T_pM\to T_{F(p)}N p∈M F:M→N  F p M M F p\in M (U,\phi=(x^1,\dots,x^m)) M p \{{\frac{\partial}{\partial x^i}}|_p\} T_pM dF_p(\frac{\partial}{\partial x^i}|_p)(f)=0 f∈C^∞(N) \partial_i|_{\phi(p)}(f\circ F \circ \phi^{-1})=0 U \phi(U) f\circ F \circ \phi^{-1} \phi(U) \phi f \circ F:U\to N f∈C^∞(N) p\ne q \in M F(p) \ne F(q) f∈C^∞(N) f(F(p))\ne f(F(q)) (V,\psi) N F(p) F(q) K N K\subseteq V \psi  \psi (F(p))\ne \psi( F(q)) j \pi_j:\mathbb{R}^n\to \mathbb{R} j \psi \circ \pi_j:\psi(V)\to \mathbb{R} f∈C^∞(N) f \psi \circ \pi_j K f(F(p))\ne f(F(q)) A \mathbb{R}^n A f:A\to \mathbb{R} A A \mathbb{R}^n A f:A\to \mathbb{R}^m A p\in M (U,\phi) M p (V,\psi) N F(p) F(U)\subseteq V \psi \circ F \circ \phi^{-1}:\phi(U)\to \psi (V) U \phi(U) d(\psi \circ F \circ \phi^{-1})_{\phi(q)}=d\psi_{F(q)} \circ dF_q \circ d(\phi^{-1})_{\phi(q)} dF_q q \in U d(\psi \circ F \circ \phi^{-1})_{x} x \in \phi(U) \psi \circ F \circ \phi^{-1}:\phi(U)\to \psi (V) \psi \circ F \circ \phi^{-1}:\phi(U)\to \psi (V) F U F F M","['multivariable-calculus', 'differential-geometry', 'smooth-manifolds', 'smooth-functions']"
38,Tangent space from a level set,Tangent space from a level set,,"I'm beginning my studies of manifolds and am still trying to grasp the basic concepts. The question I want to ask is simple: how do I get the basis of the tangent space from a manifold defined as a level set of a function? I'll give an example: Imagine the surface  $M=\{(x,y) \in \mathbb{R}^2 | x+x^2+y^2=2\}$. This is clearly a circunference centered at the point $(-\frac{1}{2},0)$. We can describe this in two ways: 1)A parametrization. Let $\Psi:\mathbb{R}\rightarrow \mathbb{R}^2;u\mapsto(cos(u)-\frac{1}{2}, sin(u))$. Then the basis of the manifold at a point $p\in M$ is the vector that comes out of the Jacobian matrix, in this case:$$\frac{d}{du}|_p = \begin{pmatrix}     -sin(u) \\     cos(u) \\     \end{pmatrix}$$ 2) A level set (I believe this is called a submersion but I'm not sure). If we define $F:\mathbb{R}^2\rightarrow \mathbb{R};(x,y)\mapsto x+x^2+y^2-2$ then $F^{-1}(0)=M$. We know that:$$T_p M=Ker[J(F,p)]$$ where $T_p M$ is the tangent space at $p$ and $J(F,p)$ is the Jacobian Matrix at $p$. How do I proceed from here to determine the basis of the Tangent Space? How does this generalize for any surface?","I'm beginning my studies of manifolds and am still trying to grasp the basic concepts. The question I want to ask is simple: how do I get the basis of the tangent space from a manifold defined as a level set of a function? I'll give an example: Imagine the surface  $M=\{(x,y) \in \mathbb{R}^2 | x+x^2+y^2=2\}$. This is clearly a circunference centered at the point $(-\frac{1}{2},0)$. We can describe this in two ways: 1)A parametrization. Let $\Psi:\mathbb{R}\rightarrow \mathbb{R}^2;u\mapsto(cos(u)-\frac{1}{2}, sin(u))$. Then the basis of the manifold at a point $p\in M$ is the vector that comes out of the Jacobian matrix, in this case:$$\frac{d}{du}|_p = \begin{pmatrix}     -sin(u) \\     cos(u) \\     \end{pmatrix}$$ 2) A level set (I believe this is called a submersion but I'm not sure). If we define $F:\mathbb{R}^2\rightarrow \mathbb{R};(x,y)\mapsto x+x^2+y^2-2$ then $F^{-1}(0)=M$. We know that:$$T_p M=Ker[J(F,p)]$$ where $T_p M$ is the tangent space at $p$ and $J(F,p)$ is the Jacobian Matrix at $p$. How do I proceed from here to determine the basis of the Tangent Space? How does this generalize for any surface?",,"['multivariable-calculus', 'differential-geometry']"
39,"Restrict the domain and codomain of $f$ to make it one-to-one on new domain, onto on new codomain","Restrict the domain and codomain of  to make it one-to-one on new domain, onto on new codomain",f,"Restrict the domain and codomain of $f$ to make it one-to-one on new domain, onto on new codomain The function $f(x,y)=3x^2+2y^2-5$ $z=3x^2+2y^2-5$ The domain = $\{(x,y)\in\mathbb{R}^2|x,y\in(-\infty,\infty)\}$ The range = $\{z\in\mathbb{R}|z\in[2,\infty)\}$ One to one means that if $f(x_1,y_1)=f(x_2,y_2)$, then $x_1=x_2,y_1=y_2$ Clearly that is not the case here, take $x=-1,y=-1$, then $f(x,y)=3+2-5=0$, take $x=1,y=1$, then $f(x,y)=3+2-5=0$. Therefore, I would restrict the domain to not include negative values of $x,y$ New domain = $\{(x,y)\in\mathbb{R}^2|x,y\in[0,\infty]\}$ Onto means that every $y\in Y$, where $Y$ is the codomain , has a $x\in X$ that maps to it, where $X$ is the domain . The range = $\{z\in\mathbb{R}|z\in[2,\infty)\}$ The codomain is just $\{z\in\mathbb{R}\}$ I am having trouble restricting this. I could just set the new codomain to be the range, but is this correct?","Restrict the domain and codomain of $f$ to make it one-to-one on new domain, onto on new codomain The function $f(x,y)=3x^2+2y^2-5$ $z=3x^2+2y^2-5$ The domain = $\{(x,y)\in\mathbb{R}^2|x,y\in(-\infty,\infty)\}$ The range = $\{z\in\mathbb{R}|z\in[2,\infty)\}$ One to one means that if $f(x_1,y_1)=f(x_2,y_2)$, then $x_1=x_2,y_1=y_2$ Clearly that is not the case here, take $x=-1,y=-1$, then $f(x,y)=3+2-5=0$, take $x=1,y=1$, then $f(x,y)=3+2-5=0$. Therefore, I would restrict the domain to not include negative values of $x,y$ New domain = $\{(x,y)\in\mathbb{R}^2|x,y\in[0,\infty]\}$ Onto means that every $y\in Y$, where $Y$ is the codomain , has a $x\in X$ that maps to it, where $X$ is the domain . The range = $\{z\in\mathbb{R}|z\in[2,\infty)\}$ The codomain is just $\{z\in\mathbb{R}\}$ I am having trouble restricting this. I could just set the new codomain to be the range, but is this correct?",,"['calculus', 'real-analysis', 'functions', 'multivariable-calculus']"
40,How to calculate this surface area? (portion of a cylinder inside a sphere ),How to calculate this surface area? (portion of a cylinder inside a sphere ),,"The surface area of ​​the portion of the cylinder $x^2+y^2=8y$ located inside of the sphere $x^2+y^2+z^2=64$ I'm stuck, so any tip will be helpful Thanks in advance!","The surface area of ​​the portion of the cylinder $x^2+y^2=8y$ located inside of the sphere $x^2+y^2+z^2=64$ I'm stuck, so any tip will be helpful Thanks in advance!",,['multivariable-calculus']
41,Differentiability of the remainder in Taylor's theorem (1D vs 2D),Differentiability of the remainder in Taylor's theorem (1D vs 2D),,"I am interested in the Lagrange's form of the remainder for $C^1$ function $f$ defined in a convex neighborhood of the origin. One dimension case. Let $f:\Omega\to\mathbb{R},$ where $0\in\Omega\subset\mathbb{R}.$ Then $$f(x)=f(0)+\overbrace{x\int_0^1\frac{\partial f}{\partial x}(tx)dt}^{R(x)}.$$ Two dimensions case. Let $f:\Omega\to\mathbb{R},$ where $(0,0)\in\Omega\subset\mathbb{R}^2.$ Then $$f(x,y)=f(0,0)+\overbrace{x\int_0^1\frac{\partial f}{\partial x}(tx,ty)dt}^{R_x(x,y)}+\overbrace{y\int_0^1\frac{\partial f}{\partial y}(tx,ty)dt}^{R_y(x,y)}.$$ Some people may call above formulas Hadamard's lemma, but I will stick to Taylor's formula. Clearly $R$ is $C^1$ cause $R(x)=f(x)-f(0).$ However we cannot use the same argument for multivariable case. Question. Are $R_x,R_y$ also $C^1$?","I am interested in the Lagrange's form of the remainder for $C^1$ function $f$ defined in a convex neighborhood of the origin. One dimension case. Let $f:\Omega\to\mathbb{R},$ where $0\in\Omega\subset\mathbb{R}.$ Then $$f(x)=f(0)+\overbrace{x\int_0^1\frac{\partial f}{\partial x}(tx)dt}^{R(x)}.$$ Two dimensions case. Let $f:\Omega\to\mathbb{R},$ where $(0,0)\in\Omega\subset\mathbb{R}^2.$ Then $$f(x,y)=f(0,0)+\overbrace{x\int_0^1\frac{\partial f}{\partial x}(tx,ty)dt}^{R_x(x,y)}+\overbrace{y\int_0^1\frac{\partial f}{\partial y}(tx,ty)dt}^{R_y(x,y)}.$$ Some people may call above formulas Hadamard's lemma, but I will stick to Taylor's formula. Clearly $R$ is $C^1$ cause $R(x)=f(x)-f(0).$ However we cannot use the same argument for multivariable case. Question. Are $R_x,R_y$ also $C^1$?",,"['calculus', 'multivariable-calculus', 'derivatives', 'taylor-expansion']"
42,A question regarding formulating a PDE for a minimization problem.,A question regarding formulating a PDE for a minimization problem.,,"Question : I'd like to formulate a pde for the following minimization problem. Let $\Omega$ be a convex, closed, compact set in $\mathbb{R}^d$ with a smooth boundary. Given a data $(x_i,d_i)$, $x_i \in \Omega^{\mathrm{o}} $ ,$d_i \in \mathbb{R}$, $i = 1,2,3...N$, $N>d$ and $\sum\limits_{i=1}^N d_i = 0$. Also given that, there are always $d$ vectors in $\{x_i\}$ which are linearly independent. Let $A = \int_{\Omega}dx$ I want to find a continuous function $f:\Omega \to \mathbb{R}$, such that, $\int_{\Omega}f(x)dx = 0$ and $C(f)$ is minimum, where $$C(f) = \frac{A^{\frac{1}{d+1}}}{N}\left\{\sum\limits_{i=1}^N |f(x_i)-d_i|^{d+1}\right\}^{\frac{1}{d+1}} +\|f\|_{L^{d+1}}+ A^{\frac{1}{d+1}} \||\nabla f|\|_{L^{d+1}}$$ The minimum exists, and is unique and is atleast Holder continuous with $\alpha = \frac{1}{d+1}$, due to Sobolev embedding theorem and Morrey's inequality Reference :  This Q&A from math.stackexchange. Motivation : I am interested in a stronger result, that is the minimum $f_{min}$ is atleast Lipschitz. We havent leveraged the fact that it is a minimum, while arriving at the weaker result that it is Holder continuous.","Question : I'd like to formulate a pde for the following minimization problem. Let $\Omega$ be a convex, closed, compact set in $\mathbb{R}^d$ with a smooth boundary. Given a data $(x_i,d_i)$, $x_i \in \Omega^{\mathrm{o}} $ ,$d_i \in \mathbb{R}$, $i = 1,2,3...N$, $N>d$ and $\sum\limits_{i=1}^N d_i = 0$. Also given that, there are always $d$ vectors in $\{x_i\}$ which are linearly independent. Let $A = \int_{\Omega}dx$ I want to find a continuous function $f:\Omega \to \mathbb{R}$, such that, $\int_{\Omega}f(x)dx = 0$ and $C(f)$ is minimum, where $$C(f) = \frac{A^{\frac{1}{d+1}}}{N}\left\{\sum\limits_{i=1}^N |f(x_i)-d_i|^{d+1}\right\}^{\frac{1}{d+1}} +\|f\|_{L^{d+1}}+ A^{\frac{1}{d+1}} \||\nabla f|\|_{L^{d+1}}$$ The minimum exists, and is unique and is atleast Holder continuous with $\alpha = \frac{1}{d+1}$, due to Sobolev embedding theorem and Morrey's inequality Reference :  This Q&A from math.stackexchange. Motivation : I am interested in a stronger result, that is the minimum $f_{min}$ is atleast Lipschitz. We havent leveraged the fact that it is a minimum, while arriving at the weaker result that it is Holder continuous.",,"['real-analysis', 'multivariable-calculus', 'partial-differential-equations', 'calculus-of-variations']"
43,"How to differentiate $z=f\left(xy,\, \frac y x \right)$?",How to differentiate ?,"z=f\left(xy,\, \frac y x \right)","I have a problem solving this problem, since I find it difficult to find the derivatives of $z$ with respect to $u$ and $v$, I would appreciate any help you can give me. $$ z= f\left(xy,\ \frac y x \right) $$ Where x and y belong to $R$ And $$xy=u$$ $$\frac{y}{x}=v$$ Prove that $$x^2\frac{\partial^2 z}{\partial x^2} -y^2 \frac{\partial^2 z}{\partial y^2} = - 4uv\frac{\partial^2 z}{\partial u \, \partial v} +  2v\frac{\partial^2 z}{\partial v ^2}$$","I have a problem solving this problem, since I find it difficult to find the derivatives of $z$ with respect to $u$ and $v$, I would appreciate any help you can give me. $$ z= f\left(xy,\ \frac y x \right) $$ Where x and y belong to $R$ And $$xy=u$$ $$\frac{y}{x}=v$$ Prove that $$x^2\frac{\partial^2 z}{\partial x^2} -y^2 \frac{\partial^2 z}{\partial y^2} = - 4uv\frac{\partial^2 z}{\partial u \, \partial v} +  2v\frac{\partial^2 z}{\partial v ^2}$$",,"['calculus', 'multivariable-calculus']"
44,Find the flux out of a tetrahedron.,Find the flux out of a tetrahedron.,,"The problem: Find the flux $\textbf{F} = 3x\hat{i} + z\hat{j}$ out of the tetrahedron closed in by the plane $5x + 3y + 3z = 4$ and the xy, xz and yz planes. My (wrong) solution: I calculated the divergence of $\textbf{F} = 4$. Then i find the volume of the tetrahedron $$\frac{\frac{4}{5} * \frac{4}{3} * \frac{4}{3}}{3} = \frac{64}{135}$$ Then i multiply the volume by the divergence and get $\frac{256}{135}$ Any help would me much appreciated. Update: The divergence is actually 3, and when calculating the volume I forgot to divide the base by two.","The problem: Find the flux $\textbf{F} = 3x\hat{i} + z\hat{j}$ out of the tetrahedron closed in by the plane $5x + 3y + 3z = 4$ and the xy, xz and yz planes. My (wrong) solution: I calculated the divergence of $\textbf{F} = 4$. Then i find the volume of the tetrahedron $$\frac{\frac{4}{5} * \frac{4}{3} * \frac{4}{3}}{3} = \frac{64}{135}$$ Then i multiply the volume by the divergence and get $\frac{256}{135}$ Any help would me much appreciated. Update: The divergence is actually 3, and when calculating the volume I forgot to divide the base by two.",,"['multivariable-calculus', 'surface-integrals']"
45,Finding points within a multi-variable calculus function,Finding points within a multi-variable calculus function,,"Consider the following function. $f (x, y)  =  [(y + 6) ln x] − xe^2y − x(y − 5)5$ (a) Find  $f_x(1, 0)$ . (b) Find  $f_y(1, 0)$ . I know that I should separate them into two different equations but I do not know how to separate it. I believe that after that I should plug in the numbers to point into the equations to get the final answer but I do not know how to make them into 2 equation. I could really use them help.","Consider the following function. $f (x, y)  =  [(y + 6) ln x] − xe^2y − x(y − 5)5$ (a) Find  $f_x(1, 0)$ . (b) Find  $f_y(1, 0)$ . I know that I should separate them into two different equations but I do not know how to separate it. I believe that after that I should plug in the numbers to point into the equations to get the final answer but I do not know how to make them into 2 equation. I could really use them help.",,"['calculus', 'multivariable-calculus']"
46,"Prove $ dy_1 \, dy_2=|J|\,dx_1 \, dx_2$, where $|J|$ is the determinant of the Jacobian.","Prove , where  is the determinant of the Jacobian."," dy_1 \, dy_2=|J|\,dx_1 \, dx_2 |J|","Suppose,$y_1=y_1(x_1,x_2)$ and $y_2=y_2(x_1,x_2)$, such that, $$dy_1=\frac{\partial y_1}{\partial x_1}dx_1+\frac{\partial y_1}{\partial x_2} \, dx_2$$ $$dy_2=\frac{\partial y_2}{\partial x_1}dx_1+\frac{\partial y_2}{\partial x_1} \, dx_2$$ Then,I've taken the product of the above two,but unable to reach to the result.","Suppose,$y_1=y_1(x_1,x_2)$ and $y_2=y_2(x_1,x_2)$, such that, $$dy_1=\frac{\partial y_1}{\partial x_1}dx_1+\frac{\partial y_1}{\partial x_2} \, dx_2$$ $$dy_2=\frac{\partial y_2}{\partial x_1}dx_1+\frac{\partial y_2}{\partial x_1} \, dx_2$$ Then,I've taken the product of the above two,but unable to reach to the result.",,"['calculus', 'multivariable-calculus', 'differential-geometry']"
47,"Finding an inverse of $f(x,y)=(e^x \cos y,e^x \sin y)$ on a neighborhood of a given point",Finding an inverse of  on a neighborhood of a given point,"f(x,y)=(e^x \cos y,e^x \sin y)","Show that $f\left(x,y\right)=\left(e^{x}\cos y,e^{x}\sin y\right)$ is one-to-one around any point of $\mathbb{R}^{2}$. For the points $\left(0,\pi\right)$ and $\left(-1,\frac{\pi}{2}\right)$ find such neighborhoods and the suitable inverse function of $f$. This is actually the exact same question as: Inverse function theorem question - multivariable calculus But after being able to do all the steps given there I am a bit stuck on the last part. I have done pretty much the same process and calculations there to find that if $u=e^{x}\cos(y)$ and $v=e^{x}\sin(y)$ then $$x=\frac{1}{2}\ln\left(u^{2}+v^{2}\right)$$ and for any integer $k$ (and $\arccos$ defined from $[-1,1]$ to $[0,\pi]$ $$y=\pm\arccos\left(\frac{u}{\sqrt{u^{2}+v^{2}}}\right)+2\pi k$$ but I'm having troubles explicitly finding an inverse function and a neighborhood for the given points. e.g. for $(0,\pi)$ if I want to define the inverse function as $$g\left(u,v\right)=\left(\frac{1}{2}\ln\left(u^{2}+v^{2}\right),\arccos\left(\frac{u}{\sqrt{u^{2}+v^{2}}}\right)\right)$$ so that $g(f(0,\pi))=(0,\pi)$ I'm not sure how to find the neighborhood for which this actually holds (and prove it is true in that neighborhood). Edit: Actually still stuck on this.. Looking at the point given above $(0,\pi)$, any neighborhood of it will include numbers larger than $\pi$ for the second variable, which I won't be able to ""translate back"" with $g$, even if I add $2\pi$. i.e. if I check $(0,\frac{3\pi}{2})$ , then $$ g(f(0,\frac{3\pi}{2}))=g(\cos \frac{3\pi}{2},\sin \frac{3\pi}{2})=(0, \arccos (\cos \frac{3\pi}{2})) = (0, \frac{\pi}{2}) $$ Am I thinking about this wrong?","Show that $f\left(x,y\right)=\left(e^{x}\cos y,e^{x}\sin y\right)$ is one-to-one around any point of $\mathbb{R}^{2}$. For the points $\left(0,\pi\right)$ and $\left(-1,\frac{\pi}{2}\right)$ find such neighborhoods and the suitable inverse function of $f$. This is actually the exact same question as: Inverse function theorem question - multivariable calculus But after being able to do all the steps given there I am a bit stuck on the last part. I have done pretty much the same process and calculations there to find that if $u=e^{x}\cos(y)$ and $v=e^{x}\sin(y)$ then $$x=\frac{1}{2}\ln\left(u^{2}+v^{2}\right)$$ and for any integer $k$ (and $\arccos$ defined from $[-1,1]$ to $[0,\pi]$ $$y=\pm\arccos\left(\frac{u}{\sqrt{u^{2}+v^{2}}}\right)+2\pi k$$ but I'm having troubles explicitly finding an inverse function and a neighborhood for the given points. e.g. for $(0,\pi)$ if I want to define the inverse function as $$g\left(u,v\right)=\left(\frac{1}{2}\ln\left(u^{2}+v^{2}\right),\arccos\left(\frac{u}{\sqrt{u^{2}+v^{2}}}\right)\right)$$ so that $g(f(0,\pi))=(0,\pi)$ I'm not sure how to find the neighborhood for which this actually holds (and prove it is true in that neighborhood). Edit: Actually still stuck on this.. Looking at the point given above $(0,\pi)$, any neighborhood of it will include numbers larger than $\pi$ for the second variable, which I won't be able to ""translate back"" with $g$, even if I add $2\pi$. i.e. if I check $(0,\frac{3\pi}{2})$ , then $$ g(f(0,\frac{3\pi}{2}))=g(\cos \frac{3\pi}{2},\sin \frac{3\pi}{2})=(0, \arccos (\cos \frac{3\pi}{2})) = (0, \frac{\pi}{2}) $$ Am I thinking about this wrong?",,"['multivariable-calculus', 'exponential-function', 'inverse-function', 'inverse-function-theorem']"
48,A converse to Stokes' Theorem in $\mathbb{R}^n$,A converse to Stokes' Theorem in,\mathbb{R}^n,"In a lecture of advanced calculus, my teacher made a very interesting remark about the generalized Stokes' Theorem (actually, he left it as an exercise!), such that, if I understood it right, is something like: Let $\Omega \subseteq \mathbb{R}^n$ be an open set and $\tau:\mathrm{F}_{n-1}(\Omega) \to \mathrm{F}_n(\Omega)$ a linear operator satisfying   $$ \int_{\Theta}\tau\omega = \int_{\partial\Theta} \omega,\quad \forall \omega\in\mathrm{F}_{n-1}(\Omega),\forall\Theta\in\mathrm{C}_{n}(\Omega),$$   where $\mathrm{C}_n(\Omega)$ is the set of singular $n$-chains in $\Omega$. Then $\tau = \mathrm{d}$, the exterior derivative operator. My attempt: From Stokes' Theorem, we have that $\int_{\Theta} (\tau-\mathrm{d})\omega = 0,~ (\forall\omega,\forall \Theta)$, and we have to prove that this implies $(\tau-\mathrm{d})\omega = 0,~ (\forall\omega,\forall \Theta)$. Suppose WLOG that $\Theta=\xi\in S_n(\Omega)$ (set of singular $n$-simplex in $\Omega$). Thus $$ \int_{\xi}(\tau-\mathrm{d})\omega = \int_{\sigma=[v_0,\cdots,v_n]}\left((\tau-\mathrm{d})\omega\right)_\xi,$$ but here there's a problem, as it's not clear whether I can commutate the operator $(\tau-\mathrm{d})$ with the pullback or not. Is this the wrong way?","In a lecture of advanced calculus, my teacher made a very interesting remark about the generalized Stokes' Theorem (actually, he left it as an exercise!), such that, if I understood it right, is something like: Let $\Omega \subseteq \mathbb{R}^n$ be an open set and $\tau:\mathrm{F}_{n-1}(\Omega) \to \mathrm{F}_n(\Omega)$ a linear operator satisfying   $$ \int_{\Theta}\tau\omega = \int_{\partial\Theta} \omega,\quad \forall \omega\in\mathrm{F}_{n-1}(\Omega),\forall\Theta\in\mathrm{C}_{n}(\Omega),$$   where $\mathrm{C}_n(\Omega)$ is the set of singular $n$-chains in $\Omega$. Then $\tau = \mathrm{d}$, the exterior derivative operator. My attempt: From Stokes' Theorem, we have that $\int_{\Theta} (\tau-\mathrm{d})\omega = 0,~ (\forall\omega,\forall \Theta)$, and we have to prove that this implies $(\tau-\mathrm{d})\omega = 0,~ (\forall\omega,\forall \Theta)$. Suppose WLOG that $\Theta=\xi\in S_n(\Omega)$ (set of singular $n$-simplex in $\Omega$). Thus $$ \int_{\xi}(\tau-\mathrm{d})\omega = \int_{\sigma=[v_0,\cdots,v_n]}\left((\tau-\mathrm{d})\omega\right)_\xi,$$ but here there's a problem, as it's not clear whether I can commutate the operator $(\tau-\mathrm{d})$ with the pullback or not. Is this the wrong way?",,"['multivariable-calculus', 'differential-forms', 'stokes-theorem']"
49,Line Integrals FT usage on this strange vector field: so what are the exact conditions?,Line Integrals FT usage on this strange vector field: so what are the exact conditions?,,"I really tried thousands of things before deciding to ask here. Searched all over the internet for an answer, but failed to find it. Let's get started with the Fundamental Theorem of Line Integrals. Take the following statement (found in many places, for example, in this Math.SE question ): Suppose $C$ is a smooth curve given by $r(t)$, $a \leq t \leq b$ and suppose $\nabla f$ is continuous on $C$. Then $$\int_{C}\nabla f\cdot dr = f(r(b)) - f(r(a)).$$ Unfortunately, there must be some mistake here. This theorem clearly implies that if C is a closed curve, the integral must be zero. I will show a counterexample soon. But first, I would like to clarify that I already understand ""part"" of the mistake: I know the previous statement must have forgot some condition, like some sort of differentiability and such. So this is my question, what are EXACTLY the requirements? At first I expected this to be an easy google-search question, but it turned out that no one seemed to care enough with that kind of details. I am interested in those details, though. Very well, take a look at this MIT video , where it's shown that the following (famous) vector field is NOT conservative: $$\vec{F}(x,y) = \dfrac{-y\hat{i} + x\hat{j}}{x^2 + y^2}$$ Very well. I understand that. In fact, if you take $C$ to be the counter-clockwise unit circle around the origin, the integral will be equal to $2\pi$, not zero . The problem is, $\vec{F}$ can be written as a gradient field!! Look: $$ f = -arctan(x/y) \implies \vec{F} = \nabla f$$ It is also true that $C$ is a smooth curve (circle) and $\nabla f$ is continuous on $C$. This shows that the quoted statement for the Fundamental Theorem of Line Integrals must be wrong. As I said before, I suspect the quoted statement forgot some conditions. I want to know what are those conditions, and more importantly, why those conditions are needed! I even looked at a proof for the theorem, but couldn't detect any steps that needed extra conditions. All this also raised another question: is it really true that Gradient Fields and Conservative Fields are the same thing (as stated by this Math.SE question )? If they are indeed the same thing, do you agree that Gradient Field was a poor name choice (given that I showed a field, written as a gradient, that is not conservative)? If they are not the same thing (which would disagree with the linked question ), then what's the difference? ( Meta-parentheses: I hope this part is not considered a duplicate, given that I believe the other question to be lacking - if I'm wrong and it is indeed a duplicate, please help me explaining what exactly should I have done instead ). SUMMARY 1. What is wrong with the quoted Fundamental Theorem of Line Integrals? (what conditions are missing?) 2. Why are those extra conditions needed? 3. With all this in mind, is it really true that ""gradient field"" is the same thing as ""conservative field""? (Please note I am aware of this question but I believe it to be incomplete, therefore I believe this part is not a duplicate. Please read the whole question for details on this. I just don't want a duplicate tag here.) Thank you all for your time. EDIT: After searching even more I finally found out a bit of information that helped - this Wikipedia page , but sadly all my questions still stand. It seems that the missing condition is the need of being simply connected , but how exactly does that go in the Fundamental Theorem of Line Integrals? Actually, my textbook (by James Stewart) doesn't mention anything regarding simply-connectedness on the Fundamental Theorem of Line Integrals. There is no region, just a curve! EDIT 2: I just found this question and this question , answered by Shuhao Cao. Although the major portion of the explanations use deeper knowledge that I am not familiar with, I was able to find out that the equivalence between a vector field being conservative, its rotation being zero, it being the gradient of a scalar potential and its path integral being path-independent only holds in simply connected domains (as said by joriki in a comment). I believe that but didn't understand the reason yet - probably the reason is the lack of knowledge to understand Shuhao Cao's explanation. Nevertheless, my questions 1 and 2 still stand. Also, if someone can provide a simpler explanation than Shuhao Cao's ones, that would be great.","I really tried thousands of things before deciding to ask here. Searched all over the internet for an answer, but failed to find it. Let's get started with the Fundamental Theorem of Line Integrals. Take the following statement (found in many places, for example, in this Math.SE question ): Suppose $C$ is a smooth curve given by $r(t)$, $a \leq t \leq b$ and suppose $\nabla f$ is continuous on $C$. Then $$\int_{C}\nabla f\cdot dr = f(r(b)) - f(r(a)).$$ Unfortunately, there must be some mistake here. This theorem clearly implies that if C is a closed curve, the integral must be zero. I will show a counterexample soon. But first, I would like to clarify that I already understand ""part"" of the mistake: I know the previous statement must have forgot some condition, like some sort of differentiability and such. So this is my question, what are EXACTLY the requirements? At first I expected this to be an easy google-search question, but it turned out that no one seemed to care enough with that kind of details. I am interested in those details, though. Very well, take a look at this MIT video , where it's shown that the following (famous) vector field is NOT conservative: $$\vec{F}(x,y) = \dfrac{-y\hat{i} + x\hat{j}}{x^2 + y^2}$$ Very well. I understand that. In fact, if you take $C$ to be the counter-clockwise unit circle around the origin, the integral will be equal to $2\pi$, not zero . The problem is, $\vec{F}$ can be written as a gradient field!! Look: $$ f = -arctan(x/y) \implies \vec{F} = \nabla f$$ It is also true that $C$ is a smooth curve (circle) and $\nabla f$ is continuous on $C$. This shows that the quoted statement for the Fundamental Theorem of Line Integrals must be wrong. As I said before, I suspect the quoted statement forgot some conditions. I want to know what are those conditions, and more importantly, why those conditions are needed! I even looked at a proof for the theorem, but couldn't detect any steps that needed extra conditions. All this also raised another question: is it really true that Gradient Fields and Conservative Fields are the same thing (as stated by this Math.SE question )? If they are indeed the same thing, do you agree that Gradient Field was a poor name choice (given that I showed a field, written as a gradient, that is not conservative)? If they are not the same thing (which would disagree with the linked question ), then what's the difference? ( Meta-parentheses: I hope this part is not considered a duplicate, given that I believe the other question to be lacking - if I'm wrong and it is indeed a duplicate, please help me explaining what exactly should I have done instead ). SUMMARY 1. What is wrong with the quoted Fundamental Theorem of Line Integrals? (what conditions are missing?) 2. Why are those extra conditions needed? 3. With all this in mind, is it really true that ""gradient field"" is the same thing as ""conservative field""? (Please note I am aware of this question but I believe it to be incomplete, therefore I believe this part is not a duplicate. Please read the whole question for details on this. I just don't want a duplicate tag here.) Thank you all for your time. EDIT: After searching even more I finally found out a bit of information that helped - this Wikipedia page , but sadly all my questions still stand. It seems that the missing condition is the need of being simply connected , but how exactly does that go in the Fundamental Theorem of Line Integrals? Actually, my textbook (by James Stewart) doesn't mention anything regarding simply-connectedness on the Fundamental Theorem of Line Integrals. There is no region, just a curve! EDIT 2: I just found this question and this question , answered by Shuhao Cao. Although the major portion of the explanations use deeper knowledge that I am not familiar with, I was able to find out that the equivalence between a vector field being conservative, its rotation being zero, it being the gradient of a scalar potential and its path integral being path-independent only holds in simply connected domains (as said by joriki in a comment). I believe that but didn't understand the reason yet - probably the reason is the lack of knowledge to understand Shuhao Cao's explanation. Nevertheless, my questions 1 and 2 still stand. Also, if someone can provide a simpler explanation than Shuhao Cao's ones, that would be great.",,"['multivariable-calculus', 'vector-fields', 'line-integrals']"
50,Curl of a vector field cross itself?,Curl of a vector field cross itself?,,Is there a neat expression for $(\nabla \times f ) \times f$ for some vector field $f$? Here is my attempt at a solution: $$((\nabla \times f ) \times f)_i = \epsilon_{ijk}(\nabla \times f )_jf_k$$ $$ = \epsilon_{ijk}\epsilon_{jlm} \frac{d}{dx_l}f_mf_k$$ $$ = (\delta_{im}\delta_{kl} - \delta_{il}\delta_{km})\frac{d}{dx_l}f_mf_k$$ $$ = \frac{d}{dx_k}f_if_k - \frac{d}{dx_i}f_k^2$$ I had interpreted this as being $f  (\nabla \cdot f) - \nabla (f \cdot f)$ but I don't believe this is correct. Can anyone tell me where i went wrong?,Is there a neat expression for $(\nabla \times f ) \times f$ for some vector field $f$? Here is my attempt at a solution: $$((\nabla \times f ) \times f)_i = \epsilon_{ijk}(\nabla \times f )_jf_k$$ $$ = \epsilon_{ijk}\epsilon_{jlm} \frac{d}{dx_l}f_mf_k$$ $$ = (\delta_{im}\delta_{kl} - \delta_{il}\delta_{km})\frac{d}{dx_l}f_mf_k$$ $$ = \frac{d}{dx_k}f_if_k - \frac{d}{dx_i}f_k^2$$ I had interpreted this as being $f  (\nabla \cdot f) - \nabla (f \cdot f)$ but I don't believe this is correct. Can anyone tell me where i went wrong?,,"['multivariable-calculus', 'vectors']"
51,Square of the distance function,Square of the distance function,,"I am confused about a certain type of problem. I was taught that when solving for a point on a plane (must use partial derivatives) say, $x+y+z=1$ that is closest to the origin, we are to minimize the square of the distance function, i.e. minimize $f(x,y)=x^2+y^2+z^2$. But I don't understand the intuition of why we do this. How could we know that we should minimize the square of the distance formula, and not just the distance formula itself? Thanks","I am confused about a certain type of problem. I was taught that when solving for a point on a plane (must use partial derivatives) say, $x+y+z=1$ that is closest to the origin, we are to minimize the square of the distance function, i.e. minimize $f(x,y)=x^2+y^2+z^2$. But I don't understand the intuition of why we do this. How could we know that we should minimize the square of the distance formula, and not just the distance formula itself? Thanks",,['multivariable-calculus']
52,"Green Theorem in 3 dimensions, calculating the volume with a vector integral identity","Green Theorem in 3 dimensions, calculating the volume with a vector integral identity",,"Let $E$ be a region in $\mathbb{R}^2$ with a smooth and non self-intersecting boundary $\partial E$ oriented in the counterclockwise direction, then from green theorem, we know that $$Area(E)=\frac{1}{2}\int_{\partial E} x\ dy-y\ dx$$ What is the analogue for Green theorem for volume of $E$ in 3 dimension? Also, a proof will be  nice?","Let $E$ be a region in $\mathbb{R}^2$ with a smooth and non self-intersecting boundary $\partial E$ oriented in the counterclockwise direction, then from green theorem, we know that $$Area(E)=\frac{1}{2}\int_{\partial E} x\ dy-y\ dx$$ What is the analogue for Green theorem for volume of $E$ in 3 dimension? Also, a proof will be  nice?",,"['real-analysis', 'multivariable-calculus', 'vector-analysis']"
53,Normal derivative expressed in polar coordinates,Normal derivative expressed in polar coordinates,,"A normal derivative on a given contour $\Gamma$ of a function $u$ expressed in Cartesian coordinates $u=u(x,y)$, can be calculated as: $$\frac{\partial u}{\partial n} = \nabla u \cdot n =  \frac{\partial u}{\partial x} n_x +  \frac{\partial u}{\partial y} n_y $$  where symbols are depicted in the figure. When a segment of a curve is parametrized by normalized variable $\xi \in [-1,1]$ we can describe its geometry as: $x=x(\xi)= \sum_i x_i N_i(\xi)$        and  $y=y(\xi)= \sum_i y_i N_i(\xi)$ where $(x_i, y_i)$ are nodal points, and $N_i$ - interpolation polynomials, e.g. Lagrange polynomials.  This leads to the derivatives: $\frac{dx}{d\xi}= \sum_i x_i \frac{dN_i}{d\xi}$ and  $\frac{dy}{d\xi}= \sum_i y_i \frac{dN_i}{d\xi}$ The Jacobian of the transformation between local and global coordinates can be calculated as $$d\Gamma = \sqrt{dx^2+dy^2} $$ and the components of the normal vector are equal $n_x = \cos \alpha = \frac{dy}{d\Gamma}$ $n_y = - \sin \alpha = -\frac{dx}{d\Gamma}$ In this way one can describe components of the normal vector in terms of Cartesian coordinates and consequently in terms of parameter $\xi$. Now what if the function $u$ and a curve $\Gamma$ are expressed in polar coordinates $r = r(\theta)$, $\theta = [0,2\pi]$, $u=u(r,\theta)$? Recently, some paper that I found provided the following formula: $$\frac{\partial u}{\partial n} = \frac{\partial u}{\partial r} - \frac{r'}{r^2} \frac{\partial u}{\partial \theta}.$$ According to the theory the normal derivative in polar coordinates is given by: $$\frac{\partial u}{\partial n} = \nabla u \cdot n =  \frac{\partial u}{\partial r} n_r +  \frac{1}{r} \frac{\partial u}{\partial \theta} n_\theta $$  and what I need is to express the radial and angular components of the normal vector in terms of polar coordinates, or in fact in terms of parameter $\theta$ because $r=r(\theta)$.  Taking into account that $x=r(\theta) \cos \theta$, $y=r(\theta) \sin \theta$ and $d\Gamma = \sqrt{dx^2+dy^2} $ it can be obtained that $$d\Gamma = \sqrt{r'^2 + r^2} $$ but I can't find any trigonometric relations between $dr$, $d\theta$ and  $n_r$, $n_\theta$ as it can be found for $dx$, $dy$, $n_x$, $n_y$. So my question is: Could anyone provide formulae for $n_r$ and $n_\theta$ in terms of polar coordinates? EDIT :  I found another paper which contains formula that I was looking for. It is different from the formula from formerly cited paper by the same author. What is more, it is a bit different from what Ted Schifrin derived. $$\frac{\partial u}{\partial n} =  \frac{r}{\sqrt{r'^2 + r^2}} \left[ \frac{\partial u}{\partial r} - \frac{r'}{r^2}  \frac{\partial u}{\partial \theta} \right] $$ The appendix of the paper gives the derivation of the formula. Now everything seems OK to me.","A normal derivative on a given contour $\Gamma$ of a function $u$ expressed in Cartesian coordinates $u=u(x,y)$, can be calculated as: $$\frac{\partial u}{\partial n} = \nabla u \cdot n =  \frac{\partial u}{\partial x} n_x +  \frac{\partial u}{\partial y} n_y $$  where symbols are depicted in the figure. When a segment of a curve is parametrized by normalized variable $\xi \in [-1,1]$ we can describe its geometry as: $x=x(\xi)= \sum_i x_i N_i(\xi)$        and  $y=y(\xi)= \sum_i y_i N_i(\xi)$ where $(x_i, y_i)$ are nodal points, and $N_i$ - interpolation polynomials, e.g. Lagrange polynomials.  This leads to the derivatives: $\frac{dx}{d\xi}= \sum_i x_i \frac{dN_i}{d\xi}$ and  $\frac{dy}{d\xi}= \sum_i y_i \frac{dN_i}{d\xi}$ The Jacobian of the transformation between local and global coordinates can be calculated as $$d\Gamma = \sqrt{dx^2+dy^2} $$ and the components of the normal vector are equal $n_x = \cos \alpha = \frac{dy}{d\Gamma}$ $n_y = - \sin \alpha = -\frac{dx}{d\Gamma}$ In this way one can describe components of the normal vector in terms of Cartesian coordinates and consequently in terms of parameter $\xi$. Now what if the function $u$ and a curve $\Gamma$ are expressed in polar coordinates $r = r(\theta)$, $\theta = [0,2\pi]$, $u=u(r,\theta)$? Recently, some paper that I found provided the following formula: $$\frac{\partial u}{\partial n} = \frac{\partial u}{\partial r} - \frac{r'}{r^2} \frac{\partial u}{\partial \theta}.$$ According to the theory the normal derivative in polar coordinates is given by: $$\frac{\partial u}{\partial n} = \nabla u \cdot n =  \frac{\partial u}{\partial r} n_r +  \frac{1}{r} \frac{\partial u}{\partial \theta} n_\theta $$  and what I need is to express the radial and angular components of the normal vector in terms of polar coordinates, or in fact in terms of parameter $\theta$ because $r=r(\theta)$.  Taking into account that $x=r(\theta) \cos \theta$, $y=r(\theta) \sin \theta$ and $d\Gamma = \sqrt{dx^2+dy^2} $ it can be obtained that $$d\Gamma = \sqrt{r'^2 + r^2} $$ but I can't find any trigonometric relations between $dr$, $d\theta$ and  $n_r$, $n_\theta$ as it can be found for $dx$, $dy$, $n_x$, $n_y$. So my question is: Could anyone provide formulae for $n_r$ and $n_\theta$ in terms of polar coordinates? EDIT :  I found another paper which contains formula that I was looking for. It is different from the formula from formerly cited paper by the same author. What is more, it is a bit different from what Ted Schifrin derived. $$\frac{\partial u}{\partial n} =  \frac{r}{\sqrt{r'^2 + r^2}} \left[ \frac{\partial u}{\partial r} - \frac{r'}{r^2}  \frac{\partial u}{\partial \theta} \right] $$ The appendix of the paper gives the derivation of the formula. Now everything seems OK to me.",,"['multivariable-calculus', 'vector-analysis']"
54,Chain rule notation for composite functions,Chain rule notation for composite functions,,"Suppose I have a function  $ f(x, y, g(x, y)) $ How would I express $ \frac{\partial f}{\partial x} $? Using the chain rule, you'd naturally come up with $ \frac{\partial f}{\partial x} + \frac{\partial f}{\partial g} \frac{\partial g}{\partial x} $, except in this expression, $ \frac{\partial f}{\partial x} $ is really only the partial derivative of $f$ with respect to that one parameter, and not $x$. So, my question is, what notation would I use to show this differentiation that is less ambiguous and meaningless than $ \frac{\partial f}{\partial x} = \frac{\partial f}{\partial x} + \frac{\partial f}{\partial g} \frac{\partial g}{\partial x} $?","Suppose I have a function  $ f(x, y, g(x, y)) $ How would I express $ \frac{\partial f}{\partial x} $? Using the chain rule, you'd naturally come up with $ \frac{\partial f}{\partial x} + \frac{\partial f}{\partial g} \frac{\partial g}{\partial x} $, except in this expression, $ \frac{\partial f}{\partial x} $ is really only the partial derivative of $f$ with respect to that one parameter, and not $x$. So, my question is, what notation would I use to show this differentiation that is less ambiguous and meaningless than $ \frac{\partial f}{\partial x} = \frac{\partial f}{\partial x} + \frac{\partial f}{\partial g} \frac{\partial g}{\partial x} $?",,"['multivariable-calculus', 'derivatives', 'notation']"
55,Intuitive understanding of path integral formula,Intuitive understanding of path integral formula,,"I have learned a general formula for a path/line integral $$ \int_a^b f\left(\mathbf{r}(t)\right) \|\mathbf{r}'(t)\|\  dt \tag{1} $$ and I'm trying to better understand it.  Specifically, I'm wondering what's not right about this: $$ \int_a^b f\left(\mathbf{r}(t)\right)\ dt \tag{2} $$ If I try to work out what's happening in $(2)$, I reason that $f(\mathbf{r}(t))$ is the value of the field $f$ at the point $\mathbf{r}(t)$ for some $t$.  As you vary $t$ from $a$ to $b$, the sum of those values $f(\mathbf{r}(t))$ seems to me like the value of the whole path, i.e. the path integral.  Clearly, there's a gap in my understanding. I do feel that I understand the similar formula for the arc length $s$ for the same path $$ s = \int_a^b \|\mathbf{r}'(t)\|\ dt \tag{3} $$ from geometry, so I understand extending $(3)$ to $(1)$ by simply including the scalar field $f$.  But, I think knowing what $(2)$ does mean might help me really take things to the next level .","I have learned a general formula for a path/line integral $$ \int_a^b f\left(\mathbf{r}(t)\right) \|\mathbf{r}'(t)\|\  dt \tag{1} $$ and I'm trying to better understand it.  Specifically, I'm wondering what's not right about this: $$ \int_a^b f\left(\mathbf{r}(t)\right)\ dt \tag{2} $$ If I try to work out what's happening in $(2)$, I reason that $f(\mathbf{r}(t))$ is the value of the field $f$ at the point $\mathbf{r}(t)$ for some $t$.  As you vary $t$ from $a$ to $b$, the sum of those values $f(\mathbf{r}(t))$ seems to me like the value of the whole path, i.e. the path integral.  Clearly, there's a gap in my understanding. I do feel that I understand the similar formula for the arc length $s$ for the same path $$ s = \int_a^b \|\mathbf{r}'(t)\|\ dt \tag{3} $$ from geometry, so I understand extending $(3)$ to $(1)$ by simply including the scalar field $f$.  But, I think knowing what $(2)$ does mean might help me really take things to the next level .",,"['multivariable-calculus', 'intuition', 'line-integrals']"
56,Notation Question with Line Integrals over Vector Field,Notation Question with Line Integrals over Vector Field,,"Previous Question: What is the Convention in Arc Length Parametrization? This is a follow-up question to my previous post on line integrals going in opposite directions.  At first I thought I understood it but later had another question.  My text denotes the smooth parametrization of $C$ by $\mathbf r:  [a, b] \rightarrow C$, and uses $u \in [0, L]$ as the arc-length parameter.  The line integral of a vector field is: $$\int_{C}\ F \cdot \mathbf T(u)\ du = \int_a^b\ F \cdot \mathbf r'(t)\ dt = \int_{C}\ F \cdot d \mathbf r$$ My best understanding of the last notation is to take the integral along $C$ as $\mathbf r$ varies.  Then, the text says that if you take the line integral of the vector field in the opposite direction, you will get the negative: $$-\int_{C}\ F \cdot d \mathbf r = \int_{-C}\ F \cdot d \mathbf r$$ I am not able to reconcile the various notations.  To my previous question, Mr. John D. wrote: $$\int_C \mathbf{F}\cdot d\mathbf{s}=\int_a^b \mathbf{F}(\mathbf{c}(t))\cdot\mathbf{c}'(t)\,dt=-\int_a^b \mathbf{F}(\mathbf{d}(t))\cdot\mathbf{d}'(t)\,dt=-\int_{-C}\mathbf{F}\cdot d\mathbf{s}$$ where $\mathbf c, \mathbf d$ are parametrizations of $C$ running in opposite directions.  Thus, the equality in the middle makes sense; it follows the change of variable.  What about the first and third equality?  If $\mathbf s\ \text{(or}\ \mathbf r \text{)}$ parametrizes $C$ in one direction, how can you use the same thing to parametrize $C$ in the opposite direction, therefore yielding $-\int_{C}\ F \cdot d \mathbf s = \int_{-C}\ F \cdot d \mathbf s$?  In my opinion, it makes better sense to write $-\int_{C}\ F \cdot d \mathbf s = \int_{-C}\ F \cdot d (\mathbf -s)$.  What is your verdict on this?","Previous Question: What is the Convention in Arc Length Parametrization? This is a follow-up question to my previous post on line integrals going in opposite directions.  At first I thought I understood it but later had another question.  My text denotes the smooth parametrization of $C$ by $\mathbf r:  [a, b] \rightarrow C$, and uses $u \in [0, L]$ as the arc-length parameter.  The line integral of a vector field is: $$\int_{C}\ F \cdot \mathbf T(u)\ du = \int_a^b\ F \cdot \mathbf r'(t)\ dt = \int_{C}\ F \cdot d \mathbf r$$ My best understanding of the last notation is to take the integral along $C$ as $\mathbf r$ varies.  Then, the text says that if you take the line integral of the vector field in the opposite direction, you will get the negative: $$-\int_{C}\ F \cdot d \mathbf r = \int_{-C}\ F \cdot d \mathbf r$$ I am not able to reconcile the various notations.  To my previous question, Mr. John D. wrote: $$\int_C \mathbf{F}\cdot d\mathbf{s}=\int_a^b \mathbf{F}(\mathbf{c}(t))\cdot\mathbf{c}'(t)\,dt=-\int_a^b \mathbf{F}(\mathbf{d}(t))\cdot\mathbf{d}'(t)\,dt=-\int_{-C}\mathbf{F}\cdot d\mathbf{s}$$ where $\mathbf c, \mathbf d$ are parametrizations of $C$ running in opposite directions.  Thus, the equality in the middle makes sense; it follows the change of variable.  What about the first and third equality?  If $\mathbf s\ \text{(or}\ \mathbf r \text{)}$ parametrizes $C$ in one direction, how can you use the same thing to parametrize $C$ in the opposite direction, therefore yielding $-\int_{C}\ F \cdot d \mathbf s = \int_{-C}\ F \cdot d \mathbf s$?  In my opinion, it makes better sense to write $-\int_{C}\ F \cdot d \mathbf s = \int_{-C}\ F \cdot d (\mathbf -s)$.  What is your verdict on this?",,"['multivariable-calculus', 'line-integrals']"
57,Understanding Spherical coordinates on ellipses.,Understanding Spherical coordinates on ellipses.,,"I was given the following problem: $$\iiint\limits_D (4x^2+9y^2+36z^2)\,dV,$$ where $V$ is the interior of the ellipsoid $$\frac{x^2}{9}+\frac{y^2}{4}+z^2=1.$$ The problem gives what the new coordinate system will be: \begin{align} x&=3\rho\sin\theta\cos\phi,\\ y&=2\rho\sin\theta\sin\phi,\\ z&=\rho\cos\theta. \end{align} I don't really know why that would work. Let's take the ellipse on the $xy$ plane and polar coordinates: $$\frac{x^2}{9}+\frac{y^2}{4}=1;~~~~~~~~~~~~x=3r\cos\theta,~~y=2r\sin\theta.$$ How do I know that for every $\theta$ I will end up with a point on the ellipse? Moreover, how do I know that with the change of variables given by the problem I will end up with a point on the ellipsoid? I appreciate your thoughts.","I was given the following problem: $$\iiint\limits_D (4x^2+9y^2+36z^2)\,dV,$$ where $V$ is the interior of the ellipsoid $$\frac{x^2}{9}+\frac{y^2}{4}+z^2=1.$$ The problem gives what the new coordinate system will be: \begin{align} x&=3\rho\sin\theta\cos\phi,\\ y&=2\rho\sin\theta\sin\phi,\\ z&=\rho\cos\theta. \end{align} I don't really know why that would work. Let's take the ellipse on the $xy$ plane and polar coordinates: $$\frac{x^2}{9}+\frac{y^2}{4}=1;~~~~~~~~~~~~x=3r\cos\theta,~~y=2r\sin\theta.$$ How do I know that for every $\theta$ I will end up with a point on the ellipse? Moreover, how do I know that with the change of variables given by the problem I will end up with a point on the ellipsoid? I appreciate your thoughts.",,"['multivariable-calculus', 'coordinate-systems', 'spherical-coordinates']"
58,"How to prove set $S=\{(x,y)\in \mathbb{R}^2~\vert~y>x^2\}$ is open (I need some hints)",How to prove set  is open (I need some hints),"S=\{(x,y)\in \mathbb{R}^2~\vert~y>x^2\}","Q: Prove $S=\{(x,y)\in \mathbb{R}^2~\vert~y>x^2\}$ is in open in $\mathbb{R}^2$. One of my intuitions: $S=\{(x,y)\in \mathbb{R}^2~\vert~y>x^2\}=\{(x,y)\in \mathbb{R}^2~\vert~y-x^2>0\}$ Define $f:\mathbb{R^2}\rightarrow\mathbb{R}$ by $f(x,y)=y-x^2$. Then $f$ is continuous and $S=f^{-1}((0,+\infty))$. Since $(0,+\infty)$ is open, $S=\{(x,y)\in \mathbb{R}^2~\vert~y>x^2\}$ is open in $\mathbb{R^2}$. Can anyone tell me whether this proof is valid or not? I'm not sure about the set $(0,+\infty)$ being open. If someone know how to prove this by open ball $B((a,b),r)$ for $(a,b)\in S$, pray tell.","Q: Prove $S=\{(x,y)\in \mathbb{R}^2~\vert~y>x^2\}$ is in open in $\mathbb{R}^2$. One of my intuitions: $S=\{(x,y)\in \mathbb{R}^2~\vert~y>x^2\}=\{(x,y)\in \mathbb{R}^2~\vert~y-x^2>0\}$ Define $f:\mathbb{R^2}\rightarrow\mathbb{R}$ by $f(x,y)=y-x^2$. Then $f$ is continuous and $S=f^{-1}((0,+\infty))$. Since $(0,+\infty)$ is open, $S=\{(x,y)\in \mathbb{R}^2~\vert~y>x^2\}$ is open in $\mathbb{R^2}$. Can anyone tell me whether this proof is valid or not? I'm not sure about the set $(0,+\infty)$ being open. If someone know how to prove this by open ball $B((a,b),r)$ for $(a,b)\in S$, pray tell.",,"['calculus', 'real-analysis', 'multivariable-calculus']"
59,Find the parametric equation to the curve,Find the parametric equation to the curve,,"Find the parametric equation for the curve. $$x^{2}+y^{2}=10$$ I haven't learned parametric equations fully yet, so I wanted to check with you guys and see if you can confirm if I'm doing this correctly and possibly go more in depth on the problem if you can? Because it's centered at (0,0) the origin, and it has a radius of sqrt(10) then the answer is this, right? $$(x(t),y(t)) = (\sqrt{10}\cos t\,,\, \sqrt{10}\sin t)$$","Find the parametric equation for the curve. $$x^{2}+y^{2}=10$$ I haven't learned parametric equations fully yet, so I wanted to check with you guys and see if you can confirm if I'm doing this correctly and possibly go more in depth on the problem if you can? Because it's centered at (0,0) the origin, and it has a radius of sqrt(10) then the answer is this, right? $$(x(t),y(t)) = (\sqrt{10}\cos t\,,\, \sqrt{10}\sin t)$$",,"['calculus', 'multivariable-calculus', 'parametric']"
60,Equivalence of definitions of multivariable differentiability,Equivalence of definitions of multivariable differentiability,,"The usual definition of differentiability for a multivariable function says that $f:\mathbb{R}^2 \to \mathbb{R}$ is differentiable at $(x,y)$ if there is a linear map $d_{(x,y)} f : \mathbb{R}^2\to \mathbb{R}$ such that $$ \lim_{(h,k)\to 0} \frac{f(x+h,y+k) - f(x,y) - d_{(x,y)}f(h,k)}{\Vert(h,k)\Vert} = 0. $$ This is equivalent to asking that $$ f(x+h,y+k) = f(x,y) + d_{(x,y)}f(h,k) + \epsilon(h,k) \Vert(h,k)\Vert $$ where $\epsilon(h,k)\to 0$ as $(h,k)\to 0$. However, the calculus book that I'm teaching out of this semester defines differentiability differently, asking that $$ f(x+h,y+k) = f(x,y) + d_{(x,y)}f(h,k) + \epsilon_1(h,k) h + \epsilon_2(h,k) k $$ where $\epsilon_1(h,k)\to 0$ and $\epsilon_2(h,k)\to 0$ as $(h,k)\to 0$.  Why is this an equivalent definition?  I see why the second implies the first: the obvious definition $\epsilon = \frac{\epsilon_1 h + \epsilon_2 k}{\Vert(h,k)\Vert}$ works because $\frac{|h|}{\Vert(h,k)\Vert} \le 1$ and $\frac{|k|}{\Vert(h,k)\Vert} \le 1$.  But I don't see the converse: given $\epsilon(h,k)$, how do I define $\epsilon_1$ and $\epsilon_2$?","The usual definition of differentiability for a multivariable function says that $f:\mathbb{R}^2 \to \mathbb{R}$ is differentiable at $(x,y)$ if there is a linear map $d_{(x,y)} f : \mathbb{R}^2\to \mathbb{R}$ such that $$ \lim_{(h,k)\to 0} \frac{f(x+h,y+k) - f(x,y) - d_{(x,y)}f(h,k)}{\Vert(h,k)\Vert} = 0. $$ This is equivalent to asking that $$ f(x+h,y+k) = f(x,y) + d_{(x,y)}f(h,k) + \epsilon(h,k) \Vert(h,k)\Vert $$ where $\epsilon(h,k)\to 0$ as $(h,k)\to 0$. However, the calculus book that I'm teaching out of this semester defines differentiability differently, asking that $$ f(x+h,y+k) = f(x,y) + d_{(x,y)}f(h,k) + \epsilon_1(h,k) h + \epsilon_2(h,k) k $$ where $\epsilon_1(h,k)\to 0$ and $\epsilon_2(h,k)\to 0$ as $(h,k)\to 0$.  Why is this an equivalent definition?  I see why the second implies the first: the obvious definition $\epsilon = \frac{\epsilon_1 h + \epsilon_2 k}{\Vert(h,k)\Vert}$ works because $\frac{|h|}{\Vert(h,k)\Vert} \le 1$ and $\frac{|k|}{\Vert(h,k)\Vert} \le 1$.  But I don't see the converse: given $\epsilon(h,k)$, how do I define $\epsilon_1$ and $\epsilon_2$?",,['multivariable-calculus']
61,Find $\alpha$ such that the given point is critical for a implicitly defined funtion.,Find  such that the given point is critical for a implicitly defined funtion.,\alpha,"Can anyone check my solution for this exercise? Let $F:\mathbb{R}^3\rightarrow\mathbb{R}$ be given by $F(x,y,z) = \alpha xz + x\arctan(z) + z\sin(2x+y) -1.$ Prove that a function $z=f(x,y)$ can be defined around $(0, \pi/2,1)$ and find $\alpha$ such that $(0, \pi/2)$ is a critical point of $f$ . To show that $z=f(x,y)$ can be defined around $(0,\pi/2,1)$ I should apply the implicit function theorem, this is, I should check that $F(0,\pi/2,1)=0$ and $\displaystyle\frac{\partial F}{\partial z}(0,\pi/2,1) \neq .0$ I have $F(0,\pi/2,1) = \alpha(0)(1) + (0)\arctan(1) + (1)\sin(\pi/2) -1 = 0$ ; and also $\displaystyle\frac{\partial F}{\partial z} = \alpha x + \displaystyle\frac{x}{1+z^2}+\sin(2x+y)$ which means that $\displaystyle\frac{\partial F}{\partial z}(0,\pi/2,1) = \sin(\pi/2) = 1 \neq 0$ . Then by the implicit function theorem a function $z=f(x,y)$ can be defined. Now to find $\alpha$ such that $(0,\pi/2)$ is a critical point, this means that must be $\displaystyle\frac{\partial f}{\partial x} = \displaystyle\frac{\partial f}{\partial y} = 0$ . Considering that $\displaystyle\frac{\partial F}{\partial x} = \alpha z + \arctan z + 2z\cos(2x+y), \displaystyle\frac{\partial F}{\partial y} = z\cos(2x+y)$ and $\displaystyle\frac{\partial f}{\partial x} = \displaystyle\frac{\frac{\partial F}{\partial x} (x,y,f(x,y))}{\frac{\partial F}{\partial z}(x,y,f(x,y))}$ , $\displaystyle\frac{\partial f}{\partial y} = \displaystyle\frac{\frac{\partial F}{\partial y} (x,y,f(x,y))}{\frac{\partial F}{\partial z}(x,y,f(x,y))}$ . If $(x,y) = (0,\pi/2)$ then $F(x,y,z) = 0$ implies that $z=1$ . Also $\displaystyle\frac{\partial F}{\partial x}(0,\pi/2,1) = \alpha + \arctan(1), \displaystyle\frac{\partial F}{\partial y}(0,\pi/2,1) = 0$ and $\displaystyle\frac{\partial F}{\partial z}(0,\pi/2,1) = 1$ . Thus, $\displaystyle\frac{\partial f}{\partial y} = 0$ and $\displaystyle\frac{\partial f}{\partial x} = \alpha + \arctan(1) \implies \alpha = -1$ Am I missing something?.","Can anyone check my solution for this exercise? Let be given by Prove that a function can be defined around and find such that is a critical point of . To show that can be defined around I should apply the implicit function theorem, this is, I should check that and I have ; and also which means that . Then by the implicit function theorem a function can be defined. Now to find such that is a critical point, this means that must be . Considering that and , . If then implies that . Also and . Thus, and Am I missing something?.","F:\mathbb{R}^3\rightarrow\mathbb{R} F(x,y,z) = \alpha xz + x\arctan(z) + z\sin(2x+y) -1. z=f(x,y) (0, \pi/2,1) \alpha (0, \pi/2) f z=f(x,y) (0,\pi/2,1) F(0,\pi/2,1)=0 \displaystyle\frac{\partial F}{\partial z}(0,\pi/2,1) \neq .0 F(0,\pi/2,1) = \alpha(0)(1) + (0)\arctan(1) + (1)\sin(\pi/2) -1 = 0 \displaystyle\frac{\partial F}{\partial z} = \alpha x + \displaystyle\frac{x}{1+z^2}+\sin(2x+y) \displaystyle\frac{\partial F}{\partial z}(0,\pi/2,1) = \sin(\pi/2) = 1 \neq 0 z=f(x,y) \alpha (0,\pi/2) \displaystyle\frac{\partial f}{\partial x} = \displaystyle\frac{\partial f}{\partial y} = 0 \displaystyle\frac{\partial F}{\partial x} = \alpha z + \arctan z + 2z\cos(2x+y), \displaystyle\frac{\partial F}{\partial y} = z\cos(2x+y) \displaystyle\frac{\partial f}{\partial x} = \displaystyle\frac{\frac{\partial F}{\partial x} (x,y,f(x,y))}{\frac{\partial F}{\partial z}(x,y,f(x,y))} \displaystyle\frac{\partial f}{\partial y} = \displaystyle\frac{\frac{\partial F}{\partial y} (x,y,f(x,y))}{\frac{\partial F}{\partial z}(x,y,f(x,y))} (x,y) = (0,\pi/2) F(x,y,z) = 0 z=1 \displaystyle\frac{\partial F}{\partial x}(0,\pi/2,1) = \alpha + \arctan(1), \displaystyle\frac{\partial F}{\partial y}(0,\pi/2,1) = 0 \displaystyle\frac{\partial F}{\partial z}(0,\pi/2,1) = 1 \displaystyle\frac{\partial f}{\partial y} = 0 \displaystyle\frac{\partial f}{\partial x} = \alpha + \arctan(1) \implies \alpha = -1","['multivariable-calculus', 'optimization', 'solution-verification', 'implicit-function-theorem']"
62,Inverse Laplace,Inverse Laplace,,Hi how to verify the following  I tried substitution and integration by parts but can bot figure it out.. $$\int_0^{\infty} \exp(- \lambda t )   \frac{x}{\sqrt{2\pi t^3}}\exp(-\frac{x^2}{2t}) dt = \exp(-\sqrt{2\lambda}  x)$$ Thank you...,Hi how to verify the following  I tried substitution and integration by parts but can bot figure it out.. $$\int_0^{\infty} \exp(- \lambda t )   \frac{x}{\sqrt{2\pi t^3}}\exp(-\frac{x^2}{2t}) dt = \exp(-\sqrt{2\lambda}  x)$$ Thank you...,,"['calculus', 'multivariable-calculus', 'improper-integrals', 'laplace-transform']"
63,Basic theorem of multivariable calculus,Basic theorem of multivariable calculus,,"Consider the following theorem: It appears in many different places but always with both conditions stated: One: $g_i(0,...,0) = {\partial f\over \partial x_i}(0,...,0)$ Two: $f(x_1,...,x_n) = \sum_i x_i g_i$ But as far as I can tell the first one follows from the second one. Consider the case in two dimensions: Assume $f(x,y) = xg(x,y) + yh(x,y)$ where $f,g,h$ are smooth. Then  $$ {\partial f \over \partial x } (x,y) = g(x,y) + x {\partial g \over \partial x } (x,y) + y {\partial h \over \partial x } (x,y)$$ and then $${\partial f \over \partial x } (0,0) = g(0,0)$$ The same argument works for $n$ dimensions. What am I missing here? (Surely if one really did follow from two then only two would be stated?) If ""two"" then of course the $g_i$ are already shown to exist.","Consider the following theorem: It appears in many different places but always with both conditions stated: One: $g_i(0,...,0) = {\partial f\over \partial x_i}(0,...,0)$ Two: $f(x_1,...,x_n) = \sum_i x_i g_i$ But as far as I can tell the first one follows from the second one. Consider the case in two dimensions: Assume $f(x,y) = xg(x,y) + yh(x,y)$ where $f,g,h$ are smooth. Then  $$ {\partial f \over \partial x } (x,y) = g(x,y) + x {\partial g \over \partial x } (x,y) + y {\partial h \over \partial x } (x,y)$$ and then $${\partial f \over \partial x } (0,0) = g(0,0)$$ The same argument works for $n$ dimensions. What am I missing here? (Surely if one really did follow from two then only two would be stated?) If ""two"" then of course the $g_i$ are already shown to exist.",,['multivariable-calculus']
64,Surface infinitesimals and its intuitive manipulation?,Surface infinitesimals and its intuitive manipulation?,,"The excess pressure in the concave side of any liquid bubble or drop with surface tension of the liquid being $T$ is $\frac {4T}r$ and $\frac {2T}r$ respectively . I wanted to derive it using a parametrized sphere and then considering the equilibrium of an infinitesimal area element , but this heavily depended on the intuition behind surface integrals and was unconventional. I was hoping for some support here:- Alternative Method Consider a spherical drop (the bubble case is almost similar) centred at $(0,0,0)$ and characterised by $$\vec r=r \cos \theta \hat z+r \sin \theta \cos \phi \hat x+ r \sin \theta \sin \phi \hat y$$  $$0<\theta<\pi\text{ };\text{ }0<\phi<2\pi$$  Where $\theta$ is the angle of the position vector with $z$ axis and $\phi$ the angle of the projection of the position vector in x-y plane with x axis. The infinitesimal area element will be $$|\frac {d\vec r}{d\theta}\times \frac{d\vec r}{d\phi}|d\theta d\phi\hat r=r^2\sin\theta d\theta d\phi \hat r=\vec {da}$$  This is, intuitively( or so i believe), actually an area bound by the parallelogram having its two sides as $\frac {d\vec r}{d\theta}d\theta(=r_\theta d\theta)$ and $\frac{d\vec r}{d\phi}d\phi(=r_\phi d\phi)$[Fig(a)]. Thus, to calculate the downward force by the surface tension, we consider the net force in the downward ($-\hat r$) direction and equate it to $\Delta P\vec {da}$ to get the value of $\Delta P$. To evaluate the force of surface tension, consider the side $r_\theta$ of the infinitesimal area parallelogram. Considering $T$ to be per unit length , the force on this (sidewards) is $2T \sin\frac{d\theta}{2}$ which is $Td\theta$ per unit length[Fig(b)]. This is valid for the length of $|r_\phi|d\phi$ [Fig(c)]which evaluates to net downward force as $T|r_\phi|d\theta d\phi$. Considering the side of $r_\phi$, the same arguments apply and the net downward force on either side of this will be $T|r_\theta|d\theta d\phi$. hence:-$$\Delta P\vec {da}=Td\theta d\phi (|r_\theta|+|r_\phi|)(\hat {-r})$$ But this does not yield the answer. It comes close but not quite the answer. In fact it predicts non-uniform pressure for different spherical co-ordinates, which is obviously not correct. Is there something wrong with my intuition or my calculations? Sorry, if this has a trivial mistake .","The excess pressure in the concave side of any liquid bubble or drop with surface tension of the liquid being $T$ is $\frac {4T}r$ and $\frac {2T}r$ respectively . I wanted to derive it using a parametrized sphere and then considering the equilibrium of an infinitesimal area element , but this heavily depended on the intuition behind surface integrals and was unconventional. I was hoping for some support here:- Alternative Method Consider a spherical drop (the bubble case is almost similar) centred at $(0,0,0)$ and characterised by $$\vec r=r \cos \theta \hat z+r \sin \theta \cos \phi \hat x+ r \sin \theta \sin \phi \hat y$$  $$0<\theta<\pi\text{ };\text{ }0<\phi<2\pi$$  Where $\theta$ is the angle of the position vector with $z$ axis and $\phi$ the angle of the projection of the position vector in x-y plane with x axis. The infinitesimal area element will be $$|\frac {d\vec r}{d\theta}\times \frac{d\vec r}{d\phi}|d\theta d\phi\hat r=r^2\sin\theta d\theta d\phi \hat r=\vec {da}$$  This is, intuitively( or so i believe), actually an area bound by the parallelogram having its two sides as $\frac {d\vec r}{d\theta}d\theta(=r_\theta d\theta)$ and $\frac{d\vec r}{d\phi}d\phi(=r_\phi d\phi)$[Fig(a)]. Thus, to calculate the downward force by the surface tension, we consider the net force in the downward ($-\hat r$) direction and equate it to $\Delta P\vec {da}$ to get the value of $\Delta P$. To evaluate the force of surface tension, consider the side $r_\theta$ of the infinitesimal area parallelogram. Considering $T$ to be per unit length , the force on this (sidewards) is $2T \sin\frac{d\theta}{2}$ which is $Td\theta$ per unit length[Fig(b)]. This is valid for the length of $|r_\phi|d\phi$ [Fig(c)]which evaluates to net downward force as $T|r_\phi|d\theta d\phi$. Considering the side of $r_\phi$, the same arguments apply and the net downward force on either side of this will be $T|r_\theta|d\theta d\phi$. hence:-$$\Delta P\vec {da}=Td\theta d\phi (|r_\theta|+|r_\phi|)(\hat {-r})$$ But this does not yield the answer. It comes close but not quite the answer. In fact it predicts non-uniform pressure for different spherical co-ordinates, which is obviously not correct. Is there something wrong with my intuition or my calculations? Sorry, if this has a trivial mistake .",,"['multivariable-calculus', 'physics', 'surfaces', 'infinitesimals']"
65,Did Brook Taylor develop his formula also in many variables by himself?,Did Brook Taylor develop his formula also in many variables by himself?,,I was wondering whether Brook Taylor was also familiar with analysis in many variables at that time. I found no information about it online. Greetings Eu2718,I was wondering whether Brook Taylor was also familiar with analysis in many variables at that time. I found no information about it online. Greetings Eu2718,,"['multivariable-calculus', 'math-history']"
66,How to construct a vector field under these conditions?,How to construct a vector field under these conditions?,,"Task: Construct a vector field $v:\mathbb{R}^2\rightarrow \mathbb{R}^2$ such that all the circles that ""touch"" y-axis in the origin (i.e. derivative in $(0,0)$ is in form $(0,a)$ for some $a$) are field lines of this field, and this vector field has to be continuously differentiable. My thoughts: It could look like this (up to direction): At first I consider the case $x>0$ (the right half of the plane). My idea was to parametrize each circle that ""touches"" y-axis as follows: $$\begin{align}x-r&=r\cdot \cos(t)\\y&=r\cdot \sin(t)\end{align}$$ and it implies $$\begin{align}x&=r\cdot( \cos(t)+1)\\y&=r\cdot \sin(t)\end{align}$$ Also for each point $(x,y)$ in $\mathbb{R}^2$ ($x>0$) there is only one $r$ such that $(x-r)^2+y^2=r^2$: $$\begin{align} (x-r)^2+y^2&=r^2\\ x^2-2xr+r^2+y^2&=r^2\\ x^2-2xr+y^2&=0 \end{align} $$ and thus $r=\frac{x^2+y^2}{2x}$. If I want all those circles to be the field lines, I have to set the values of $v$ to be the derivatives of the curve: $$\frac{d}{dt}c(t)=\frac{d}{dt}(r\cdot(\cos(t)+1),r\cdot\sin(t))=(-r\sin(t),r\cos(t))$$ So the vector field $v$ could be $$v(x,y)=v(r\cdot(\cos(t)+1),r\cdot\sin(t))=(-r\sin(t),r\cos(t))=(-y,x-r)=(-y,x-\frac{x^2+y^2}{2x})=(-y,\frac{x^2-y^2}{2x})$$ This $v$ is actually the function plotted above. So the question is now whether this function is continuously differentiable or not. The critical point is when $x$ is close to $0$. If we take a look at partial derivative, then $$\partial_xv=(0,\frac{x^2+y^2}{2x^2})$$ and it diverges when $x\rightarrow 0$, so it seems that this $v$ is not suitable, but I think the real $v$ should be somehow similar to the given $v$. But these are only my considerations that for sure can be wrong. I'd appreciate your help.","Task: Construct a vector field $v:\mathbb{R}^2\rightarrow \mathbb{R}^2$ such that all the circles that ""touch"" y-axis in the origin (i.e. derivative in $(0,0)$ is in form $(0,a)$ for some $a$) are field lines of this field, and this vector field has to be continuously differentiable. My thoughts: It could look like this (up to direction): At first I consider the case $x>0$ (the right half of the plane). My idea was to parametrize each circle that ""touches"" y-axis as follows: $$\begin{align}x-r&=r\cdot \cos(t)\\y&=r\cdot \sin(t)\end{align}$$ and it implies $$\begin{align}x&=r\cdot( \cos(t)+1)\\y&=r\cdot \sin(t)\end{align}$$ Also for each point $(x,y)$ in $\mathbb{R}^2$ ($x>0$) there is only one $r$ such that $(x-r)^2+y^2=r^2$: $$\begin{align} (x-r)^2+y^2&=r^2\\ x^2-2xr+r^2+y^2&=r^2\\ x^2-2xr+y^2&=0 \end{align} $$ and thus $r=\frac{x^2+y^2}{2x}$. If I want all those circles to be the field lines, I have to set the values of $v$ to be the derivatives of the curve: $$\frac{d}{dt}c(t)=\frac{d}{dt}(r\cdot(\cos(t)+1),r\cdot\sin(t))=(-r\sin(t),r\cos(t))$$ So the vector field $v$ could be $$v(x,y)=v(r\cdot(\cos(t)+1),r\cdot\sin(t))=(-r\sin(t),r\cos(t))=(-y,x-r)=(-y,x-\frac{x^2+y^2}{2x})=(-y,\frac{x^2-y^2}{2x})$$ This $v$ is actually the function plotted above. So the question is now whether this function is continuously differentiable or not. The critical point is when $x$ is close to $0$. If we take a look at partial derivative, then $$\partial_xv=(0,\frac{x^2+y^2}{2x^2})$$ and it diverges when $x\rightarrow 0$, so it seems that this $v$ is not suitable, but I think the real $v$ should be somehow similar to the given $v$. But these are only my considerations that for sure can be wrong. I'd appreciate your help.",,['multivariable-calculus']
67,"In a Frenet-Serret frame, what are $\Delta\vec T$ and $(\vec a\vec\nabla)\vec T$","In a Frenet-Serret frame, what are  and",\Delta\vec T (\vec a\vec\nabla)\vec T,"Given a Frenet-Serret frame $(\vec T(t), \vec N(t), \vec B(t))$ defined by a curve $\vec \gamma(t)$ with $$\begin{array}{rcl}    |\tfrac{d}{dt}\vec\gamma(t)| &\equiv& 1, \\ \vec T(t) &:=& \tfrac{d}{dt}\vec\gamma(t), \\ \kappa(t) &:=& |\tfrac{d}{dt}\vec T(t)|, \\ \kappa(t)\vec N(t) &:=& \tfrac{d}{dt}\vec T(t), \\ \vec B(t) &:=& \vec T(t)\times\vec N(t), \\ \tfrac{d}{dt}\vec B(t) &=:& -\tau\vec N(t) \end{array}$$ one can define a local coordinate system $$\vec x(t,n,b) := \vec\gamma(t) + n\vec N(t) + b\vec B(t)$$ (where the range of $n,b$ is defined such that this yields an injective function, assuming $\kappa,\tau\neq0$). $(\vec T,\vec N ,\vec B)$ are assumed to be independent of $n,b$, i.e. for a fixed $t$ the $\vec x(t,n,b)$ span a 2D plane perpendicular to $\vec T(t)$ with Cartesian coordinates $(n,b)$. Then what are the derivatives of these unit vectors, especially what are $(\vec a\vec\nabla)\vec T$ (for a vector $\vec a$, i.e. directional derivative of $\vec T$) and $\Delta\vec T$ (i.e. Laplacian of $\vec T$) expressed in these coordinates?","Given a Frenet-Serret frame $(\vec T(t), \vec N(t), \vec B(t))$ defined by a curve $\vec \gamma(t)$ with $$\begin{array}{rcl}    |\tfrac{d}{dt}\vec\gamma(t)| &\equiv& 1, \\ \vec T(t) &:=& \tfrac{d}{dt}\vec\gamma(t), \\ \kappa(t) &:=& |\tfrac{d}{dt}\vec T(t)|, \\ \kappa(t)\vec N(t) &:=& \tfrac{d}{dt}\vec T(t), \\ \vec B(t) &:=& \vec T(t)\times\vec N(t), \\ \tfrac{d}{dt}\vec B(t) &=:& -\tau\vec N(t) \end{array}$$ one can define a local coordinate system $$\vec x(t,n,b) := \vec\gamma(t) + n\vec N(t) + b\vec B(t)$$ (where the range of $n,b$ is defined such that this yields an injective function, assuming $\kappa,\tau\neq0$). $(\vec T,\vec N ,\vec B)$ are assumed to be independent of $n,b$, i.e. for a fixed $t$ the $\vec x(t,n,b)$ span a 2D plane perpendicular to $\vec T(t)$ with Cartesian coordinates $(n,b)$. Then what are the derivatives of these unit vectors, especially what are $(\vec a\vec\nabla)\vec T$ (for a vector $\vec a$, i.e. directional derivative of $\vec T$) and $\Delta\vec T$ (i.e. Laplacian of $\vec T$) expressed in these coordinates?",,"['differential-geometry', 'multivariable-calculus']"
68,Proof of coarea formula,Proof of coarea formula,,I want to prove the coarea formula $$ \operatorname{Vol}(M) = \int_M d\operatorname{Vol}_M = \int_{-\infty}^\infty \frac{1}{|\nabla f|} \operatorname{area }(f^{-1}(t)) dt $$ where $f\colon M \rightarrow {\Bbb R}$ is a smooth function. Here I have a question. The above formula says that $|\nabla f|$ is constant on $f^{-1}(t)$. So how can we prove this ? Thank you in advance. Correction : I checked that the following equality is right (See 85 page in the book Eigenvalues in Riemannian geometry - Chavel) $$\operatorname{Vol}(M) = \int_M d\operatorname{Vol}_M = \int_{-\infty}^\infty \int_{f^{-1} (t) } \frac{1}{|\nabla f|} d\operatorname{area}(f^{-1}(t)) dt$$ I believe that this formula is the generalization of arc-length parametrization. But (1) I cannot explain clearly and (2) I cannot prove the above inequality. So if you have an interests in this please help me in (1) and (2).,I want to prove the coarea formula $$ \operatorname{Vol}(M) = \int_M d\operatorname{Vol}_M = \int_{-\infty}^\infty \frac{1}{|\nabla f|} \operatorname{area }(f^{-1}(t)) dt $$ where $f\colon M \rightarrow {\Bbb R}$ is a smooth function. Here I have a question. The above formula says that $|\nabla f|$ is constant on $f^{-1}(t)$. So how can we prove this ? Thank you in advance. Correction : I checked that the following equality is right (See 85 page in the book Eigenvalues in Riemannian geometry - Chavel) $$\operatorname{Vol}(M) = \int_M d\operatorname{Vol}_M = \int_{-\infty}^\infty \int_{f^{-1} (t) } \frac{1}{|\nabla f|} d\operatorname{area}(f^{-1}(t)) dt$$ I believe that this formula is the generalization of arc-length parametrization. But (1) I cannot explain clearly and (2) I cannot prove the above inequality. So if you have an interests in this please help me in (1) and (2).,,"['multivariable-calculus', 'analytic-geometry']"
69,"$-\iint_{A}(y+x)\,dA$ Evaluating",Evaluating,"-\iint_{A}(y+x)\,dA","I am bit unsure about the following problem: Evaluate the double integral: $$-\iint_{A}(y+x)\,dA$$ over the triangle with vertices $(0,0), (1,1), (2,0)$ OK, so I figured here that I would do this by first evaluating the integral over the region bounded by the vertices $(0,0), (1,1), (1,0)$ and then evaluate the integral over the region bounded by the vertices $(1,0), (1,1), (2,0)$ before adding the two answers together, and then reversing the sign of this answer (since there is a minus sign in front of the original double integral).  Thus, I begin by finding: $$\int_{0}^{1}dx \int_{0}^{x}(y+x)\,dy$$ When solved this gives me the answer $\frac{1}{2}$. Next I solve: $$\int_{1}^{2}dx \int_{1}^{2-x}(y+x)\,dy$$ When solved this gives me the answer $-\frac{7}{6}$. I have verified both the integrals in Wolframalpha, and they give me the same answer.  I would therefore believe that the final answer should be: $$-(\frac{1}{2} - \frac{7}{6}) = \frac{2}{3}$$ However, the final answer should, according to the book, be $-\frac{4}{3}$. Thus, obviously I do something wrong here.  If anyone can help me out, I would greatly appreciate it.  Is it perhaps that it is not allowed to ""split up"" this into two separate integrals?  I couldn't find a way to solve this without doing this.","I am bit unsure about the following problem: Evaluate the double integral: $$-\iint_{A}(y+x)\,dA$$ over the triangle with vertices $(0,0), (1,1), (2,0)$ OK, so I figured here that I would do this by first evaluating the integral over the region bounded by the vertices $(0,0), (1,1), (1,0)$ and then evaluate the integral over the region bounded by the vertices $(1,0), (1,1), (2,0)$ before adding the two answers together, and then reversing the sign of this answer (since there is a minus sign in front of the original double integral).  Thus, I begin by finding: $$\int_{0}^{1}dx \int_{0}^{x}(y+x)\,dy$$ When solved this gives me the answer $\frac{1}{2}$. Next I solve: $$\int_{1}^{2}dx \int_{1}^{2-x}(y+x)\,dy$$ When solved this gives me the answer $-\frac{7}{6}$. I have verified both the integrals in Wolframalpha, and they give me the same answer.  I would therefore believe that the final answer should be: $$-(\frac{1}{2} - \frac{7}{6}) = \frac{2}{3}$$ However, the final answer should, according to the book, be $-\frac{4}{3}$. Thus, obviously I do something wrong here.  If anyone can help me out, I would greatly appreciate it.  Is it perhaps that it is not allowed to ""split up"" this into two separate integrals?  I couldn't find a way to solve this without doing this.",,"['multivariable-calculus', 'definite-integrals']"
70,Continuously differentiable with constraint on gradient implies the function is convex,Continuously differentiable with constraint on gradient implies the function is convex,,"I am not sure what the relevant theorems for this problem are. I have been searching through Rudin for some hints, but I have come up short. This is an example question for an exam, so not homework. Can anyone point me in the right direction? Thanks. A function $f: \mathbb{R}^n \to \mathbb{R}$ is called convex if $f$ satifies $$ f(\alpha x + (1 - \alpha)y) \le \alpha f(x) + (1 - \alpha)f(y) \quad \forall x, y \in \mathbb{R}^n,\ 0 \le \alpha \le 1.$$ Assume that $f$ is continuously differentiable and that for some constant $c > 0$, the gradient  $$(\nabla f(x) - \nabla f(y)) \cdot (x - y) \ge c(x - y) \cdot (x - y), \quad \forall x, y \in \mathbb{R}^n,$$ where $\cdot$ denotes the dot product. Show that $f$ is convex.","I am not sure what the relevant theorems for this problem are. I have been searching through Rudin for some hints, but I have come up short. This is an example question for an exam, so not homework. Can anyone point me in the right direction? Thanks. A function $f: \mathbb{R}^n \to \mathbb{R}$ is called convex if $f$ satifies $$ f(\alpha x + (1 - \alpha)y) \le \alpha f(x) + (1 - \alpha)f(y) \quad \forall x, y \in \mathbb{R}^n,\ 0 \le \alpha \le 1.$$ Assume that $f$ is continuously differentiable and that for some constant $c > 0$, the gradient  $$(\nabla f(x) - \nabla f(y)) \cdot (x - y) \ge c(x - y) \cdot (x - y), \quad \forall x, y \in \mathbb{R}^n,$$ where $\cdot$ denotes the dot product. Show that $f$ is convex.",,"['real-analysis', 'multivariable-calculus']"
71,"$\frac{\partial f_i}{x_j}=\frac{\partial f_j}{x_i}\implies(f_1,\ldots,f_n)$ is a gradient",is a gradient,"\frac{\partial f_i}{x_j}=\frac{\partial f_j}{x_i}\implies(f_1,\ldots,f_n)",I was reading a solution when I came across this statement. So $$\frac{\partial f_i}{x_j}=\frac{\partial f_j}{x_i}.$$ Then there exists a differentiable function $g$ on $\mathbb{R}^n$ such that $\frac{\partial g}{\partial x_i}=f_i$. Why is this true?,I was reading a solution when I came across this statement. So $$\frac{\partial f_i}{x_j}=\frac{\partial f_j}{x_i}.$$ Then there exists a differentiable function $g$ on $\mathbb{R}^n$ such that $\frac{\partial g}{\partial x_i}=f_i$. Why is this true?,,['multivariable-calculus']
72,Is there a quadruple product rule?,Is there a quadruple product rule?,,"The triple product rule in multivariable calculus is widely used.  Can a quadruple product rule equation be written for an equation f(x,y,z,z2)=0?","The triple product rule in multivariable calculus is widely used.  Can a quadruple product rule equation be written for an equation f(x,y,z,z2)=0?",,['multivariable-calculus']
73,Implicit function theorem for $f:\mathbb{R}^{3} \rightarrow \mathbb{R}$,Implicit function theorem for,f:\mathbb{R}^{3} \rightarrow \mathbb{R},"I am stuck at this problem and I would be glad if somebody helped me out: By the implicit function theorem, it should be shown that: $$f(x,y,z) := z^{3}+2xy-4xz+2y-1 .$$ The zero level set $f^{-1}(0)$ in a neighborhood $U$ of $(x_0, y_0) = (1,1)$ can be rewritten through a differentiable function $z=g(x,y)$ where $g(1,1)=1$. Then also the partial derivatives: $g_x(1,1), g_y(1,1)$ should be calculated. The partial derivative in respect to $z$ is: $f_z = 3z^2- 4x$ and it holds that $3z^2 - 4x \ne 0$, because otherwise the invertibility would not be given anymore. So if $(x_0, y_0)=(1,1)$ then $z^3- 4z+4=0$ gives $3$ solutions, of which none are such that $3z^2-4 = 0$. So the zero level set in $U$ can be rewritten through $g(x,y)$. Now the remaining task is to solve $z^3 +2xy-4xz+2y-1=0$ for $z$. I sit around at this end: $$z^3- 4xz=-2xy-2y+1 = z(z^2- 4x)=-2xy-2y+1 \Rightarrow z = \frac{-2xy-2y+1}{z^2-4x} .$$ According to Wolfram Alpha, there are 3 exact solutions .","I am stuck at this problem and I would be glad if somebody helped me out: By the implicit function theorem, it should be shown that: $$f(x,y,z) := z^{3}+2xy-4xz+2y-1 .$$ The zero level set $f^{-1}(0)$ in a neighborhood $U$ of $(x_0, y_0) = (1,1)$ can be rewritten through a differentiable function $z=g(x,y)$ where $g(1,1)=1$. Then also the partial derivatives: $g_x(1,1), g_y(1,1)$ should be calculated. The partial derivative in respect to $z$ is: $f_z = 3z^2- 4x$ and it holds that $3z^2 - 4x \ne 0$, because otherwise the invertibility would not be given anymore. So if $(x_0, y_0)=(1,1)$ then $z^3- 4z+4=0$ gives $3$ solutions, of which none are such that $3z^2-4 = 0$. So the zero level set in $U$ can be rewritten through $g(x,y)$. Now the remaining task is to solve $z^3 +2xy-4xz+2y-1=0$ for $z$. I sit around at this end: $$z^3- 4xz=-2xy-2y+1 = z(z^2- 4x)=-2xy-2y+1 \Rightarrow z = \frac{-2xy-2y+1}{z^2-4x} .$$ According to Wolfram Alpha, there are 3 exact solutions .",,['multivariable-calculus']
74,Different ways of treating vector calculus?,Different ways of treating vector calculus?,,"I learned that there are different ways of treating vector calculus: vector fields and differential forms, if I understand correctly. The former is used in calculus, and the latter is in differential geometry. My memory of calculus is vague on the vector calculus part and I have limit knowledge about differential geometry, so I was wondering how these two ways are different and related in a brief? There is no need to refrain from using some terminology in reply, and I can look them up if I am not familiar. A side question: is multivariate calculus synonym of vector calculus? Thanks for your input! PS: The book description of Differential Forms: Integration on Manifolds and Stokes's Theorem by Steven H. Weintraub is from which I learned the above: Book Description: This text is one of the first to treat vector calculus using   differential forms in place of vector fields and other outdated   techniques. Geared towards students taking courses in multivariable   calculus , this innovative book aims to make the subject more readily   understandable. Differential forms unify and simplify the subject of   multivariable calculus, and students who learn the subject as it is   presented in this book should come away with a better conceptual   understanding of it than those who learn using conventional methods. Treats vector calculus using differential forms Presents a very concrete introduction to differential forms Develops Stokess theorem in an easily understandable way Gives well-supported, carefully stated, and thoroughly explained definitions and theorems. Provides glimpses of further topics to entice the interested student","I learned that there are different ways of treating vector calculus: vector fields and differential forms, if I understand correctly. The former is used in calculus, and the latter is in differential geometry. My memory of calculus is vague on the vector calculus part and I have limit knowledge about differential geometry, so I was wondering how these two ways are different and related in a brief? There is no need to refrain from using some terminology in reply, and I can look them up if I am not familiar. A side question: is multivariate calculus synonym of vector calculus? Thanks for your input! PS: The book description of Differential Forms: Integration on Manifolds and Stokes's Theorem by Steven H. Weintraub is from which I learned the above: Book Description: This text is one of the first to treat vector calculus using   differential forms in place of vector fields and other outdated   techniques. Geared towards students taking courses in multivariable   calculus , this innovative book aims to make the subject more readily   understandable. Differential forms unify and simplify the subject of   multivariable calculus, and students who learn the subject as it is   presented in this book should come away with a better conceptual   understanding of it than those who learn using conventional methods. Treats vector calculus using differential forms Presents a very concrete introduction to differential forms Develops Stokess theorem in an easily understandable way Gives well-supported, carefully stated, and thoroughly explained definitions and theorems. Provides glimpses of further topics to entice the interested student",,"['differential-geometry', 'multivariable-calculus', 'vector-analysis']"
75,Find surface which generated by revolving a line in $\mathbb{R}^3$,Find surface which generated by revolving a line in,\mathbb{R}^3,"Problem : Let $l$ be a line which passes two points : $(1,0,0), (1,1,1)$ . And $S $ be a surface which generated by revolving line $l$ around $z$ -axis. Find a volume enclosed by surface $S$ and two planes : $z=0, z=1$ . My Attempt Parametric equation of $l$ can be obtained easily : $$l(t) = (1,t,t)$$ And, distance $d(t)$ between $l$ and $z$ -axis is same with distance between $(0,0, t), (1,t,t)$ : $$d(t) = \sqrt{1+t^2}$$ Since $l(0)=(1,0,0), l(1)=(1,1,1)$ , volume $V$ what we want is : $$\begin{align} V &= \pi\int_0^1 d(t)^2 dt \\ &= \pi\int_0^1 (1+t^2)dt \\ &= \frac{4}{3}\pi\end{align}$$ Here are my main questions : Is this method legit? If not, I'd like to know what the problem is. If yes, is this method can be used anytime? (For problems which request volume enclosed by surface which generated revolving line or curve and planes.) Can I find equation of $S$ explicitly to evaluate $V$ with triple integral?","Problem : Let be a line which passes two points : . And be a surface which generated by revolving line around -axis. Find a volume enclosed by surface and two planes : . My Attempt Parametric equation of can be obtained easily : And, distance between and -axis is same with distance between : Since , volume what we want is : Here are my main questions : Is this method legit? If not, I'd like to know what the problem is. If yes, is this method can be used anytime? (For problems which request volume enclosed by surface which generated revolving line or curve and planes.) Can I find equation of explicitly to evaluate with triple integral?","l (1,0,0), (1,1,1) S  l z S z=0, z=1 l l(t) = (1,t,t) d(t) l z (0,0, t), (1,t,t) d(t) = \sqrt{1+t^2} l(0)=(1,0,0), l(1)=(1,1,1) V \begin{align} V &= \pi\int_0^1 d(t)^2 dt \\ &= \pi\int_0^1 (1+t^2)dt \\ &= \frac{4}{3}\pi\end{align} S V","['calculus', 'multivariable-calculus', 'volume', 'solid-of-revolution']"
76,Convexity of $f(\boldsymbol{p}) = \sum_i p_i \log \sum_j p_j \frac{4 a_i a_j}{(a_i + a_j)^2}$,Convexity of,f(\boldsymbol{p}) = \sum_i p_i \log \sum_j p_j \frac{4 a_i a_j}{(a_i + a_j)^2},"Question Given that $\boldsymbol{a} > 0$ , how to prove $$f(\boldsymbol{p}) = \sum_i p_i \log \sum_j p_j \frac{4 a_i a_j}{(a_i + a_j)^2}$$ is a convex function of probability distribution $\boldsymbol{p}$ ? Is it possible to generalize from the convexity of the negative entropy $p \log p$ ? Attempts Midpoint convex + continuous => convex: to prove $$f\left(\frac{\boldsymbol{p} + \boldsymbol{q}}{2}\right) \le \frac{f(\boldsymbol{p}) + f(\boldsymbol{q})}{2}$$ It reduces to $$\prod_i \left(\sum_j \frac{p_j+q_j}{2} \frac{4 a_i a_j}{(a_i + a_j)^2}\right)^{p_i+q_i} \le \prod_i \left(\sum_j p_j \frac{4 a_i a_j}{(a_i + a_j)^2}\right)^{p_i} \left(\sum_j q_j \frac{4 a_i a_j}{(a_i + a_j)^2}\right)^{q_i}$$ and the original GM-HM trick for $p \log p$ does not work. It looks very similar to Holder's inequality but I failed to gain more insights... Hessian: the $(m,n)$ entry of Hessian is $$\frac{b_{m,n}}{\sum_j p_j b_{m,j}} + \frac{b_{n,m}}{\sum_j p_j b_{n,j}} - \sum_i p_i \frac{b_{i,m} b_{i,n}}{(\sum_j p_j b_{i,j})^2}$$ where $b_{i,j} = \frac{4 a_i a_j}{(a_i + a_j)^2}$ . Simulation suggests the Hessian is generally positive semidefinite, but finding a proof seems nontrivial. Thank you very much for the attention.","Question Given that , how to prove is a convex function of probability distribution ? Is it possible to generalize from the convexity of the negative entropy ? Attempts Midpoint convex + continuous => convex: to prove It reduces to and the original GM-HM trick for does not work. It looks very similar to Holder's inequality but I failed to gain more insights... Hessian: the entry of Hessian is where . Simulation suggests the Hessian is generally positive semidefinite, but finding a proof seems nontrivial. Thank you very much for the attention.","\boldsymbol{a} > 0 f(\boldsymbol{p}) = \sum_i p_i \log \sum_j p_j \frac{4 a_i a_j}{(a_i + a_j)^2} \boldsymbol{p} p \log p f\left(\frac{\boldsymbol{p} + \boldsymbol{q}}{2}\right) \le \frac{f(\boldsymbol{p}) + f(\boldsymbol{q})}{2} \prod_i \left(\sum_j \frac{p_j+q_j}{2} \frac{4 a_i a_j}{(a_i + a_j)^2}\right)^{p_i+q_i} \le \prod_i \left(\sum_j p_j \frac{4 a_i a_j}{(a_i + a_j)^2}\right)^{p_i} \left(\sum_j q_j \frac{4 a_i a_j}{(a_i + a_j)^2}\right)^{q_i} p \log p (m,n) \frac{b_{m,n}}{\sum_j p_j b_{m,j}} + \frac{b_{n,m}}{\sum_j p_j b_{n,j}} - \sum_i p_i \frac{b_{i,m} b_{i,n}}{(\sum_j p_j b_{i,j})^2} b_{i,j} = \frac{4 a_i a_j}{(a_i + a_j)^2}","['real-analysis', 'multivariable-calculus', 'convex-analysis']"
77,"$f(x, y, z)$ with both $\leq$ and $=$ constraints. General questions.",with both  and  constraints. General questions.,"f(x, y, z) \leq =","I need to ask you for this question, which is a rather general one, in order to understand how to behave when studying maxima and minima with constraints, in many variables. The specific question is the following: suppose I have some $f(x, y, z)$ (in this case I'm specifically asking for three variables) subject to two constraint: $$ \begin{cases} g(x, y, z) \leq 0 \\\\ h(x, y, z) = 0\end{cases}$$ Question: I use Langrange multipliers in the usual way, by building the Lagrangian $$L(x, y, z, \lambda, \mu) = f(x, y, z) - \lambda g(x, y, z) - \mu h(x, y, z)$$ (then I know how to proceed). Say I will be able to reduce the problem to a two dimensiona one, and I cannot go further than this. Then I study $\nabla f(x, y) = (0, 0)$ and I hopefully find some points. What now? Should I use them to find the third point $z$ by using the constraints, or should I study $f(x, y)$ with the Hessian matrix? Thank you! AN EXAMPLE Just to make things more concrete, I thought to write here an example for what I meant. Say $$f(x, y, z) = xyz$$ Subject to $$\begin{cases} x^2+y^2+z^2 \leq 1 \\\\ x+y+z = 1 \end{cases}$$ So a sphere and a plane, which intersect to form a cicumference. $$L(x, y, z, \lambda, \mu) = xyz - \lambda(x^2+y^2+z^2-1) - \mu(x+y+z-1)$$ And the derived equations read $$ \begin{cases} yz = 2\lambda x + \mu \\ xz = 2\lambda y + \mu \\ xy = 2\lambda z + \mu \\ \text{the two constraints} \end{cases} $$ I was able to write down: $$\mu = zy - 2\lambda x$$ hence from arranging the second with this one: $$\lambda = \frac{-z}{2}$$ And then subtituting all in the third: $$xy = -z^2 + zy + zx$$ Using the second constraint for $z \to z = 1-x-y$ I conclude with $$f(x, y) = 2x^2+ 2y^2-3x-3y+xy +1$$ So at this point: $\nabla f(x, y) = (0, 0)$ gives the points $$x = 0 \qquad \qquad y = 0$$ $$x = \frac{3}{5} \qquad \qquad y = \frac{3}{5}$$ and fron this, question two: should I find $z$ and then simply evaluate $f$ on this points, or should I go on with $f(x, y)$ with eh Hessian? Bonus question: am I reasoning right or wrong? Am I missing something? EDIT I understood that with mixed constraints I have to set up Kuhn Tucker conditions. So I did it and the result reads: $$\begin{cases} x^2+y^2+z^2 -1 \leq 0 \\ x+y+z-1 = 0 \\ \lambda(x^2+y^2+z^2-1) = 0 \\ \lambda \geq 0 \\ \nabla L = 0 \end{cases} $$ And the last one, creating $$L(x, y, z, \lambda, \mu) = xyz - \lambda(x^2+y^2+z^2-1) -\mu(x+y+z-1) $$ then reads $$\begin{cases} yz - 2\lambda x-\mu = 0 \\ xz - 2\lambda y - \mu =0 \\ xy - 2\lambda z - \mu = 0 \end{cases} $$ Now, for the case $\lambda = 0$ , all beautiful, I solve and obtain the point $\left(\frac{1}{3}, \frac{1}{3}, \frac{1}{3}\right)$ Which shoudl be a max (?) When $\lambda \neq 0$ the mess starts. Arranging, I obtain $\lambda = -\frac{z}{2}$ , which results in $$z^2 - z(x+y) + xy = 0$$ And using th constraints I can reduce it to $$3xy - x - y+1 = 0$$ And I am stuck. If I try the gradient, I get the same points I got above. W. Mathematica says $(\text{ at } x,y,z) \left(\min  \left\{x y z\left|x^2+y^2+z^2\leq 1\land x+y+z=1\right.\right\}=-\frac{4}{27}\right)=\frac{2}{3},\frac{2}{3},-\frac{1}{3}$ And at two other very similar points, but $z$ is not negative in those ones...","I need to ask you for this question, which is a rather general one, in order to understand how to behave when studying maxima and minima with constraints, in many variables. The specific question is the following: suppose I have some (in this case I'm specifically asking for three variables) subject to two constraint: Question: I use Langrange multipliers in the usual way, by building the Lagrangian (then I know how to proceed). Say I will be able to reduce the problem to a two dimensiona one, and I cannot go further than this. Then I study and I hopefully find some points. What now? Should I use them to find the third point by using the constraints, or should I study with the Hessian matrix? Thank you! AN EXAMPLE Just to make things more concrete, I thought to write here an example for what I meant. Say Subject to So a sphere and a plane, which intersect to form a cicumference. And the derived equations read I was able to write down: hence from arranging the second with this one: And then subtituting all in the third: Using the second constraint for I conclude with So at this point: gives the points and fron this, question two: should I find and then simply evaluate on this points, or should I go on with with eh Hessian? Bonus question: am I reasoning right or wrong? Am I missing something? EDIT I understood that with mixed constraints I have to set up Kuhn Tucker conditions. So I did it and the result reads: And the last one, creating then reads Now, for the case , all beautiful, I solve and obtain the point Which shoudl be a max (?) When the mess starts. Arranging, I obtain , which results in And using th constraints I can reduce it to And I am stuck. If I try the gradient, I get the same points I got above. W. Mathematica says And at two other very similar points, but is not negative in those ones...","f(x, y, z)  \begin{cases} g(x, y, z) \leq 0 \\\\ h(x, y, z) = 0\end{cases} L(x, y, z, \lambda, \mu) = f(x, y, z) - \lambda g(x, y, z) - \mu h(x, y, z) \nabla f(x, y) = (0, 0) z f(x, y) f(x, y, z) = xyz \begin{cases} x^2+y^2+z^2 \leq 1 \\\\ x+y+z = 1 \end{cases} L(x, y, z, \lambda, \mu) = xyz - \lambda(x^2+y^2+z^2-1) - \mu(x+y+z-1) 
\begin{cases}
yz = 2\lambda x + \mu \\
xz = 2\lambda y + \mu \\
xy = 2\lambda z + \mu \\
\text{the two constraints}
\end{cases}
 \mu = zy - 2\lambda x \lambda = \frac{-z}{2} xy = -z^2 + zy + zx z \to z = 1-x-y f(x, y) = 2x^2+ 2y^2-3x-3y+xy +1 \nabla f(x, y) = (0, 0) x = 0 \qquad \qquad y = 0 x = \frac{3}{5} \qquad \qquad y = \frac{3}{5} z f f(x, y) \begin{cases}
x^2+y^2+z^2 -1 \leq 0 \\
x+y+z-1 = 0 \\
\lambda(x^2+y^2+z^2-1) = 0 \\
\lambda \geq 0 \\
\nabla L = 0
\end{cases}
 L(x, y, z, \lambda, \mu) = xyz - \lambda(x^2+y^2+z^2-1) -\mu(x+y+z-1)  \begin{cases}
yz - 2\lambda x-\mu = 0 \\
xz - 2\lambda y - \mu =0 \\
xy - 2\lambda z - \mu = 0
\end{cases}
 \lambda = 0 \left(\frac{1}{3}, \frac{1}{3}, \frac{1}{3}\right) \lambda \neq 0 \lambda = -\frac{z}{2} z^2 - z(x+y) + xy = 0 3xy - x - y+1 = 0 (\text{ at } x,y,z) \left(\min  \left\{x y z\left|x^2+y^2+z^2\leq 1\land x+y+z=1\right.\right\}=-\frac{4}{27}\right)=\frac{2}{3},\frac{2}{3},-\frac{1}{3} z","['multivariable-calculus', 'optimization', 'maxima-minima', 'lagrange-multiplier', 'constraints']"
78,Intuition regarding Lagrange multipliers with many constraints,Intuition regarding Lagrange multipliers with many constraints,,"Studying for my finals in calculus 3, returning to the proof of Lagrange multipliers with multiple constraints, I'm having a hard time getting any form of intuition about why this is the case - why does this give us the extrema points. While looking at one constraint, I understand the geometric interpretation of the gradient vectors being linearly dependent. And I'm able to understand that each constant defines a surface one dimension lower since we are looking at the group defined by intersecting all: $\forall i\in [n]: g_i(x)=0$ If my function $f$ is linearly dependent on all other constraining functions, what does this mean visually(what does it mean visually that all other constraints are independent and $f$ is dependent on all of them)? Why is it necessary for all gradients of $g_i$ to be linearly independent? (or is this just the version of the proof I was shown, and a stronger argument exists? I'll add that we proved the Theorem using the Open Mapping Theorem). Additions I can see why (2) is required since we want to think of the simplified version with one constraint, which is only possible when the intersection defines some curve. I still am unsure if this is required since we know we can find local extrema on open groups and are not limited to a single curve. Or is the whole concept trying to avoid this method and use the dependency of gradients to optimize?","Studying for my finals in calculus 3, returning to the proof of Lagrange multipliers with multiple constraints, I'm having a hard time getting any form of intuition about why this is the case - why does this give us the extrema points. While looking at one constraint, I understand the geometric interpretation of the gradient vectors being linearly dependent. And I'm able to understand that each constant defines a surface one dimension lower since we are looking at the group defined by intersecting all: If my function is linearly dependent on all other constraining functions, what does this mean visually(what does it mean visually that all other constraints are independent and is dependent on all of them)? Why is it necessary for all gradients of to be linearly independent? (or is this just the version of the proof I was shown, and a stronger argument exists? I'll add that we proved the Theorem using the Open Mapping Theorem). Additions I can see why (2) is required since we want to think of the simplified version with one constraint, which is only possible when the intersection defines some curve. I still am unsure if this is required since we know we can find local extrema on open groups and are not limited to a single curve. Or is the whole concept trying to avoid this method and use the dependency of gradients to optimize?",\forall i\in [n]: g_i(x)=0 f f g_i,"['multivariable-calculus', 'derivatives', 'intuition', 'lagrange-multiplier']"
79,"Finding the $z$ coordinate of center of mass for $C:= \{ (x,y,z) \in \mathbb{R^3}: \sqrt{x^2+y^2}\leq z \leq 1$",Finding the  coordinate of center of mass for,"z C:= \{ (x,y,z) \in \mathbb{R^3}: \sqrt{x^2+y^2}\leq z \leq 1","The Problem Let $C$ be the cone $$C:= \{ (x,y,z) \in \mathbb{R^3}: \sqrt{x^2+y^2}\leq z \leq 1$$ Assume that $C$ has a constant mass density and find the z coordinate of the center of mass. The work I have done: \begin{align*} M_{xy}&=\iint_{R}\int_{\sqrt{x^2+y^2}}^{1} z \,\delta \,dx \,dy \,dx\\ &=\iint_{R} \delta \,\frac{z^2}{2} \Bigg|_{\sqrt{x^2+y^2}}^{1} \,dy \,dx\\ &=\iint_{R} \delta \,\Big(\frac{1}{2}-\frac{\sqrt{x^2+y^2}}{2} \Big) \,dy \,dx \end{align*} Switching to polar coordinates now and pulling out the constants: \begin{align*} &=\frac{\delta}{2}\int_{0}^{2\pi}\int_{0}^{1} \big(r-r^3 \big) \,dr \,d\theta\\ &=\frac{\delta}{2} (\frac{\pi}{2})=\frac{\pi}{4}\,\delta\,.\\ \\ M&=\iint_{R}\int_{\sqrt{x^2+y^2}}^{1} \,\delta \,dx \,dy \,dx\\ &=\iint_{R} \delta (1-\sqrt{x^2+y^2}) \,dy \,dx \end{align*} Switching again to polar: \begin{align*} &=\delta \int_{0}^{2\pi} \int_{0}^{1} (1-r) \,r \,dr \,d\theta\\ &=\delta \int_{0}^{2\pi} \frac{1}{6} d\theta = \frac{\pi}{3}\,\delta \end{align*} Therefore: $$\bar{z}=\frac{M_{xy}}{M} = \frac{\pi\delta}{4}\cdot\frac{3}{\pi\delta}=\frac{3}{4}$$ Is my answer correct? Edit: I also know that I could have switched to cylindrical from the beginning. If I did, would the integral I need to evaluate be $$M=\int_{0}^{2\pi}\int_{0}^{1}\int_{r}^{1} \,r \,dz \,dr\,d\theta$$ and $$M_{xy}=\int_{0}^{2\pi}\int_{0}^{1}\int_{r}^{1} \,rz \,dz \,dr\,d\theta\,?$$","The Problem Let be the cone Assume that has a constant mass density and find the z coordinate of the center of mass. The work I have done: Switching to polar coordinates now and pulling out the constants: Switching again to polar: Therefore: Is my answer correct? Edit: I also know that I could have switched to cylindrical from the beginning. If I did, would the integral I need to evaluate be and","C C:= \{ (x,y,z) \in \mathbb{R^3}: \sqrt{x^2+y^2}\leq z \leq 1 C \begin{align*}
M_{xy}&=\iint_{R}\int_{\sqrt{x^2+y^2}}^{1} z \,\delta \,dx \,dy \,dx\\
&=\iint_{R} \delta \,\frac{z^2}{2} \Bigg|_{\sqrt{x^2+y^2}}^{1} \,dy \,dx\\
&=\iint_{R} \delta \,\Big(\frac{1}{2}-\frac{\sqrt{x^2+y^2}}{2} \Big) \,dy \,dx
\end{align*} \begin{align*}
&=\frac{\delta}{2}\int_{0}^{2\pi}\int_{0}^{1} \big(r-r^3 \big) \,dr \,d\theta\\
&=\frac{\delta}{2} (\frac{\pi}{2})=\frac{\pi}{4}\,\delta\,.\\
\\
M&=\iint_{R}\int_{\sqrt{x^2+y^2}}^{1} \,\delta \,dx \,dy \,dx\\
&=\iint_{R} \delta (1-\sqrt{x^2+y^2}) \,dy \,dx
\end{align*} \begin{align*}
&=\delta \int_{0}^{2\pi} \int_{0}^{1} (1-r) \,r \,dr \,d\theta\\
&=\delta \int_{0}^{2\pi} \frac{1}{6} d\theta = \frac{\pi}{3}\,\delta
\end{align*} \bar{z}=\frac{M_{xy}}{M} = \frac{\pi\delta}{4}\cdot\frac{3}{\pi\delta}=\frac{3}{4} M=\int_{0}^{2\pi}\int_{0}^{1}\int_{r}^{1} \,r \,dz \,dr\,d\theta M_{xy}=\int_{0}^{2\pi}\int_{0}^{1}\int_{r}^{1} \,rz \,dz \,dr\,d\theta\,?",['multivariable-calculus']
80,Why is Green's theorem a special case of Stokes theorem?,Why is Green's theorem a special case of Stokes theorem?,,"I have already seen related questions and don't understand. Please help me. $\oint_C \mathbf{A} \cdot d\mathbf{r} = \iint_S (\nabla x \mathbf{A})\cdot \mathbf{n}$ dS Let $A \leq P,Q,0>$ Then $\nabla x \mathbf{A}= <0,0,(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} )>$ . So $\iint_S (\nabla x \mathbf{A})\cdot \mathbf{n}$ dS $=\iint_S <0,0,(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} )>\cdot \mathbf{n}$ dS $=\iint_S <0,0,(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} )>\cdot \mathbf{k}$ dS $=\iint_S (\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} )$ dS Now, here people replace dS with dA and then say its equal to green's theorem but why would be able to do that? Isn't dS = $|r_u x r_v|$ dA Please see screenshot or this second screenshot. Why do we take unit normal vector. We don't do that when going from dS to dA as seen in this screenshot. in this third screenshot, Wikipedia does it but doesn't explain the step I struggle with.","I have already seen related questions and don't understand. Please help me. dS Let Then . So dS dS dS dS Now, here people replace dS with dA and then say its equal to green's theorem but why would be able to do that? Isn't dS = dA Please see screenshot or this second screenshot. Why do we take unit normal vector. We don't do that when going from dS to dA as seen in this screenshot. in this third screenshot, Wikipedia does it but doesn't explain the step I struggle with.","\oint_C \mathbf{A} \cdot d\mathbf{r} = \iint_S (\nabla x \mathbf{A})\cdot \mathbf{n} A \leq P,Q,0> \nabla x \mathbf{A}= <0,0,(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} )> \iint_S (\nabla x \mathbf{A})\cdot \mathbf{n} =\iint_S <0,0,(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} )>\cdot \mathbf{n} =\iint_S <0,0,(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} )>\cdot \mathbf{k} =\iint_S (\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} ) |r_u x r_v|","['multivariable-calculus', 'stokes-theorem', 'greens-theorem']"
81,Rudin's PMA: theorem 10.43,Rudin's PMA: theorem 10.43,,"This is the definition which we need for the proof of the theorem : This is the definition of the $\mathscr C''$ - equivalent : There is the theorem: Suppose E is an open set in $R^3$ , $u$ $\in$ $\mathscr C''(E)$ , and $G$ is a vector field in $E$ , of class $C''$ . $(a)$ if $F$ $=$ $\nabla$$u$ , then $\nabla$ $\times$ $F$ $=$ $0$ . $(b)$ if $F$ $=$ $\nabla$ $\times$ $G$ , then $\nabla$ $\cdot$ $F$ $=$ $0$ . Furthermore, if $E$ is $\mathscr C''$ -equivalent to a convex set, then $(a)$ and $(b)$ have converses, in which we assume that $F$ is a vector field in $E$ , of class $\mathscr C'$ : $(a')$ if $\nabla$ $\times$ $F$ $=$ $0$ , then $F$ = $\nabla$$u$ for some $u$ $\in$ $\mathscr C''(E)$ . $(b')$ if $\nabla$ $\cdot$ $F$ $=$ $0$ , then $F$ $=$ $\nabla$ $\times$ $G$ for some vector field $G$ in $E$ , of class $\mathscr C''$ There is the proof  : If we compare the definitions of $\nabla$$u$ , $\nabla$ $\times$ $F$ , and $\nabla$ $\cdot$ $F$ with the differential forms $\lambda_F$ and $\omega_F$ given by ( $124$ ) and ( $125$ ), we obtain the following four statements: $F$ $=$ $\nabla$$u$ if and only if $\lambda_F$ $=$ $du$ .                    ( $\star$ ). $\nabla$ $\times$ $F$ $=$ $0$ if and only if $d\lambda_F$ $=$ $0$ .             ( $\ast$ ) $F$ $=$ $\nabla$ $\times$ $G$ if and only if $\omega_F$ $=$ $d\lambda_G$ .  ( $\oplus$ ) $\nabla$ $\cdot$ $F$ $=$ $0$ if and  only if $d\omega_F$ $=$ $0$ .            ( $\circ$ ) I could n't understand these four statements ( $\star$ ),( $\ast$ ),( $\oplus$ ),( $\circ$ ). Any help would be appreciated.","This is the definition which we need for the proof of the theorem : This is the definition of the - equivalent : There is the theorem: Suppose E is an open set in , , and is a vector field in , of class . if , then . if , then . Furthermore, if is -equivalent to a convex set, then and have converses, in which we assume that is a vector field in , of class : if , then = for some . if , then for some vector field in , of class There is the proof  : If we compare the definitions of , , and with the differential forms and given by ( ) and ( ), we obtain the following four statements: if and only if .                    ( ). if and only if .             ( ) if and only if .  ( ) if and  only if .            ( ) I could n't understand these four statements ( ),( ),( ),( ). Any help would be appreciated.",\mathscr C'' R^3 u \in \mathscr C''(E) G E C'' (a) F = \nablau \nabla \times F = 0 (b) F = \nabla \times G \nabla \cdot F = 0 E \mathscr C'' (a) (b) F E \mathscr C' (a') \nabla \times F = 0 F \nablau u \in \mathscr C''(E) (b') \nabla \cdot F = 0 F = \nabla \times G G E \mathscr C'' \nablau \nabla \times F \nabla \cdot F \lambda_F \omega_F 124 125 F = \nablau \lambda_F = du \star \nabla \times F = 0 d\lambda_F = 0 \ast F = \nabla \times G \omega_F = d\lambda_G \oplus \nabla \cdot F = 0 d\omega_F = 0 \circ \star \ast \oplus \circ,"['real-analysis', 'calculus', 'multivariable-calculus', 'convex-analysis', 'vector-fields']"
82,What is the precise definition about $k$ times continously differentiable at a point $\mathbf{c}\in\mathbb {R}^{m}?$,What is the precise definition about  times continously differentiable at a point,k \mathbf{c}\in\mathbb {R}^{m}?,"$k$ times continously differentiable A funcation $f$ ,with domain $S$ in $\mathbb {R}^{m}$ is $k$ times continously differentiable at an interior point $\mathbf {c}$ of $S$ if it and all of its first-through $(k-1)$ th-order partial derivatives are continuously differentiable at $\mathbf {c}$ or, ${\color{red}{\text{equivalently}}}$ ,if all of the first-through $k$ th-order partial derivatives of $f$ exist and are continuous at every point in some neighborhood of $\mathbf{c}$ --a vector or matrix of functions is $k$ times continuously differentiable at $\mathbf{c}$ if all of its elements are $k$ times continuously differentiable at $\mathbf{c}.$ I'm really puzzled about the above definition.Conventionally,we say a funcation $f:S\rightarrow \mathbb {R}$ ,with domain $S$ in $\mathbb {R}^{m}$ is continuously differentiable at the point $\mathbf {c}\in \text {int} S$ if $f$ is differentiable in a neighborhood of $\mathbf {c}$ ,and the partial derivatives of the $f$ are continuous at $\mathbf {c}$ . From this statement and the above definition "" if it and all of its first-through $(k-1)$ th-order partial derivatives are continuously differentiable at $\mathbf {c}$ "", I only conclude that all of the first-through $(k-1)$ th-order partial derivatives of $f$ are continuous on a neighborhood of $\mathbf{c}$ , all $k$ th-order partial derivatives of $f$ exist on this neighborhood and are continuous at $\mathbf{c}$ .Why the above defintion said ""equivalently,if all of the first-through $k$ th-order partial derivatives of $f$ exist and are continuous at every point in some neighborhood of $\mathbf{c}$ ""? Why they are equivalent ?  Further ,what is the precise definition about $k$ times continously differentiable at a point $\mathbf{c}\in\mathbb {R}^{m}?$","times continously differentiable A funcation ,with domain in is times continously differentiable at an interior point of if it and all of its first-through th-order partial derivatives are continuously differentiable at or, ,if all of the first-through th-order partial derivatives of exist and are continuous at every point in some neighborhood of --a vector or matrix of functions is times continuously differentiable at if all of its elements are times continuously differentiable at I'm really puzzled about the above definition.Conventionally,we say a funcation ,with domain in is continuously differentiable at the point if is differentiable in a neighborhood of ,and the partial derivatives of the are continuous at . From this statement and the above definition "" if it and all of its first-through th-order partial derivatives are continuously differentiable at "", I only conclude that all of the first-through th-order partial derivatives of are continuous on a neighborhood of , all th-order partial derivatives of exist on this neighborhood and are continuous at .Why the above defintion said ""equivalently,if all of the first-through th-order partial derivatives of exist and are continuous at every point in some neighborhood of ""? Why they are equivalent ?  Further ,what is the precise definition about times continously differentiable at a point",k f S \mathbb {R}^{m} k \mathbf {c} S (k-1) \mathbf {c} {\color{red}{\text{equivalently}}} k f \mathbf{c} k \mathbf{c} k \mathbf{c}. f:S\rightarrow \mathbb {R} S \mathbb {R}^{m} \mathbf {c}\in \text {int} S f \mathbf {c} f \mathbf {c} (k-1) \mathbf {c} (k-1) f \mathbf{c} k f \mathbf{c} k f \mathbf{c} k \mathbf{c}\in\mathbb {R}^{m}?,"['real-analysis', 'multivariable-calculus', 'definition']"
83,the curvature as a function of $t$,the curvature as a function of,t,"let $r,c>$ some real numbers, and $X(t)=\langle r\cos t, r\sin t, ct \rangle$ is a curve. Find the curvature as a function of $t$ I think that the curvature of a curve is $$\left\lVert\frac{dT}{ds}\right\rVert=\frac{\left\lVert T'(t)\right\rVert}{\left\lVert X'(t)\right\rVert}\text{ Where }T=\frac{X'(t)}{\left\lVert X'(t) \right\rVert}$$ Since $$X'(t)=\langle -r\sin t, r\cos t, c\rangle $$ and $$T(t)=\frac{1} {\sqrt{r^2+c^2}}\langle -r\sin t, r\cos t,c \rangle$$ It follows that $$T'(t)=\frac{1} {\sqrt{r^2+c^2}}\langle -r\cos t, -r\sin t, 0\rangle$$ Hence $$\left\lVert\frac{dT}{ds}\right\rVert=\frac{r}{\sqrt{r^2+c^2}}\frac{1}{\sqrt{r^2+c^2}}=\frac{r}{r^2+c^2}$$ But apparently, this is not a function of $t$ , Does this mean that the curvature is constant?","let some real numbers, and is a curve. Find the curvature as a function of I think that the curvature of a curve is Since and It follows that Hence But apparently, this is not a function of , Does this mean that the curvature is constant?","r,c> X(t)=\langle r\cos t, r\sin t, ct \rangle t \left\lVert\frac{dT}{ds}\right\rVert=\frac{\left\lVert T'(t)\right\rVert}{\left\lVert X'(t)\right\rVert}\text{ Where }T=\frac{X'(t)}{\left\lVert X'(t) \right\rVert} X'(t)=\langle -r\sin t, r\cos t, c\rangle  T(t)=\frac{1} {\sqrt{r^2+c^2}}\langle -r\sin t, r\cos t,c \rangle T'(t)=\frac{1} {\sqrt{r^2+c^2}}\langle -r\cos t, -r\sin t, 0\rangle \left\lVert\frac{dT}{ds}\right\rVert=\frac{r}{\sqrt{r^2+c^2}}\frac{1}{\sqrt{r^2+c^2}}=\frac{r}{r^2+c^2} t","['multivariable-calculus', 'vectors', 'curvature']"
84,Question regarding solving multivariable function continuity problems,Question regarding solving multivariable function continuity problems,,"$$f(x,y)=\begin{Bmatrix} \frac{x+y}{x^2+y^2},(x,y)\neq (0,0)) \\0   , (x,y)=(0,0))  \end{Bmatrix}$$ Let's say I have this problem. I use $y = mx$ , solving this I get $\frac{1+m}{x(1+m^2)}$ . Calculating the limit I get $\frac{1+m}{0}$ . Could this prove that the limit doesn't exist? Some follow-up questions: When can I use th $y=mx$ method, if I can? What is the rule of paths? I guess I can't just plug in $(1,2)$ or $(2,3)$ , but can I plug in let's say $(\frac{1}{n}, \frac{1}{n})$ ? If yes, what should I look for when plugging in values? The 'n' has to go, but how? If I need to prove the continuity of a function, what is the simplest way to prove it? I heard about the Squeeze Theorem, but does that work outside of sin, cos functions? At seminars we used $a^2 + b^2 >= 2ab$ , but I don't find that at all clear. EDIT: $$f(x,y)=\begin{Bmatrix} \frac{x^2y3}{x^2+y^2},(x,y)\neq (0,0)) \\0   , (x,y)=(0,0))  \end{Bmatrix}$$ I have this function. Using $a^2 + b^2 >= 2ab$ : $$x^2 + y^2 >= |x^2| + |y^2| >= 2|x||y|$$ $|f(x,y)| = \frac{x^2 |y^3|}{x^2 + y^2} = \frac{x^2 |y^3|}{2|x||y|} = \frac{x y^2}{2}$ ---> this tending to 0 would prove continuity? if yes, does this method work all the time? EDIT 2: $$f(x,y)=\begin{Bmatrix} \frac{3x^2y}{2x^2+5y^2},(x,y)\neq (0,0)) \\0   , (x,y)=(0,0))  \end{Bmatrix}$$ $$|f(x,y)| = \frac{3x^2|y|}{2x^2+5y^2} <= \frac{3x^2|y|}{2\sqrt{10}|x||y|} = \frac{3}{2\sqrt{10}}*x$$ $$(a^2 + b^2 >= 2ab)  ==> 2x^2 + 5y^2 >= (|x|\sqrt{2})^2 + (|y|\sqrt{5})^2 >= 2\sqrt{10}*x$$ $f(x,y) <= \frac{3}{2\sqrt{10}}*x$ (1) $x --> 0, y --> 0$ (2) (1)(2) --> f(x,y) --> 0 = f(0, 0).","Let's say I have this problem. I use , solving this I get . Calculating the limit I get . Could this prove that the limit doesn't exist? Some follow-up questions: When can I use th method, if I can? What is the rule of paths? I guess I can't just plug in or , but can I plug in let's say ? If yes, what should I look for when plugging in values? The 'n' has to go, but how? If I need to prove the continuity of a function, what is the simplest way to prove it? I heard about the Squeeze Theorem, but does that work outside of sin, cos functions? At seminars we used , but I don't find that at all clear. EDIT: I have this function. Using : ---> this tending to 0 would prove continuity? if yes, does this method work all the time? EDIT 2: (1) (2) (1)(2) --> f(x,y) --> 0 = f(0, 0).","f(x,y)=\begin{Bmatrix}
\frac{x+y}{x^2+y^2},(x,y)\neq (0,0)) \\0 
 , (x,y)=(0,0)) 
\end{Bmatrix} y = mx \frac{1+m}{x(1+m^2)} \frac{1+m}{0} y=mx (1,2) (2,3) (\frac{1}{n}, \frac{1}{n}) a^2 + b^2 >= 2ab f(x,y)=\begin{Bmatrix}
\frac{x^2y3}{x^2+y^2},(x,y)\neq (0,0)) \\0 
 , (x,y)=(0,0)) 
\end{Bmatrix} a^2 + b^2 >= 2ab x^2 + y^2 >= |x^2| + |y^2| >= 2|x||y| |f(x,y)| = \frac{x^2 |y^3|}{x^2 + y^2} = \frac{x^2 |y^3|}{2|x||y|} = \frac{x y^2}{2} f(x,y)=\begin{Bmatrix}
\frac{3x^2y}{2x^2+5y^2},(x,y)\neq (0,0)) \\0 
 , (x,y)=(0,0)) 
\end{Bmatrix} |f(x,y)| = \frac{3x^2|y|}{2x^2+5y^2} <= \frac{3x^2|y|}{2\sqrt{10}|x||y|} = \frac{3}{2\sqrt{10}}*x (a^2 + b^2 >= 2ab)  ==> 2x^2 + 5y^2 >= (|x|\sqrt{2})^2 + (|y|\sqrt{5})^2 >= 2\sqrt{10}*x f(x,y) <= \frac{3}{2\sqrt{10}}*x x --> 0, y --> 0","['calculus', 'multivariable-calculus', 'continuity']"
85,Does a singular Jacobian matrix imply functional dependence?,Does a singular Jacobian matrix imply functional dependence?,,"I often find the following ""fact"" mentioned in engineering mathematics texts and videos: If $f_1,...,f_n$ are $C^1$ functions from $\mathbb{R}^n$ to $\mathbb{R}$ such that the Jacobian matrix of the map $(f_1,...,f_n)$ vanishes everywhere, then the $f_i$ 's must be functionally related. They seem to use the following definition of functionally related : There is some $C^1$ function $\phi:\mathbb{R}^n \rightarrow\mathbb{R}$ such that $\nabla\phi$ is nonzero everywhere, and $\phi(f_1,...,f_n)=0$ everywhere. It is easy to see (using chain rule and some elementary linear algebra) why this definition of functionally related would lead to the Jacobian being singular everywhere. But what intrigues me is the converse claim. I am unable to come up with a rigorous proof or a counterexample.  All the multivariable calculus books I have seen so far are silent on this point. Any helpful pointer would be greatly appreciated.","I often find the following ""fact"" mentioned in engineering mathematics texts and videos: If are functions from to such that the Jacobian matrix of the map vanishes everywhere, then the 's must be functionally related. They seem to use the following definition of functionally related : There is some function such that is nonzero everywhere, and everywhere. It is easy to see (using chain rule and some elementary linear algebra) why this definition of functionally related would lead to the Jacobian being singular everywhere. But what intrigues me is the converse claim. I am unable to come up with a rigorous proof or a counterexample.  All the multivariable calculus books I have seen so far are silent on this point. Any helpful pointer would be greatly appreciated.","f_1,...,f_n C^1 \mathbb{R}^n \mathbb{R} (f_1,...,f_n) f_i C^1 \phi:\mathbb{R}^n \rightarrow\mathbb{R} \nabla\phi \phi(f_1,...,f_n)=0","['multivariable-calculus', 'jacobian']"
86,An inequality involving real numbers,An inequality involving real numbers,,"Let $x,y,z$ be real numbers such that $xyz=-1$ . Prove that $$\sqrt[3/2]{\frac{3}{2}}\geq E:=\frac{4(x^3+y^3+z^3)}{(x^2+y^2+z^2)^2}$$ I tried to imitate an idea by River Li but it does not work. The idea is to find a function $f$ such that for all $x,y>0$ , $$E\leq f(x+y)$$ and then use calculus to show $\displaystyle f_{\max}=\sqrt[3/2]{\frac{3}{2}}$ . For instance, $$x^2+y^2\geq\frac{(x+y)^2}{2},\qquad z^2=\frac{1}{x^2y^2}\geq\frac{16}{(x+y)^4},\qquad z^3=-\frac{1}{x^3y^3}\leq-\frac{64}{(x+y)^6}$$ Unfortunately, there does not exist any function $g$ such that $x^3+y^3\leq g(x+y)$ .","Let be real numbers such that . Prove that I tried to imitate an idea by River Li but it does not work. The idea is to find a function such that for all , and then use calculus to show . For instance, Unfortunately, there does not exist any function such that .","x,y,z xyz=-1 \sqrt[3/2]{\frac{3}{2}}\geq E:=\frac{4(x^3+y^3+z^3)}{(x^2+y^2+z^2)^2} f x,y>0 E\leq f(x+y) \displaystyle f_{\max}=\sqrt[3/2]{\frac{3}{2}} x^2+y^2\geq\frac{(x+y)^2}{2},\qquad z^2=\frac{1}{x^2y^2}\geq\frac{16}{(x+y)^4},\qquad z^3=-\frac{1}{x^3y^3}\leq-\frac{64}{(x+y)^6} g x^3+y^3\leq g(x+y)","['multivariable-calculus', 'inequality', 'quadratics', 'symmetric-polynomials', 'uvw']"
87,$\Delta F' / \Delta x'$ is approximately twice as large as $\Delta F / \Delta x$?,is approximately twice as large as ?,\Delta F' / \Delta x' \Delta F / \Delta x,"I am currently studying the textbook Introduction to Tensor Analysis and the Calculus of Moving Surfaces , by Pavel Grinfeld. On page 3, the author begins with the following motivating example: What is the gradient of a function $F$ and a point $P$ ? You are familiar with two definitions, one geometric and one analytical. According to the geometric definition, the gradient $\nabla F$ of $F$ is the vector that points in the direction of the greatest increase of the function $F$ , and its magnitude equals the greatest rate of increase. According to the analytical definition that requires the presence of a coordinate system, the gradient of $F$ is the triplet of numbers $$\nabla F = \left( \dfrac{\partial{F}}{\partial{x}}, \dfrac{\partial{F}}{\partial{y}} \right) \tag{1.1}$$ Are the two definitions equivalent in some sense? If you believe that the connection is $$\nabla F = \dfrac{\partial{F}}{\partial{x}}\mathbf{i}  + \dfrac{\partial{F}}{\partial{y}} \mathbf{j}\tag{1.2}$$ where $\mathbf{i}$ and $\mathbf{j}$ are the coordinate basis, you are in for a surprise! Equation (1.2) can only be considered valid if it produces the same vector in all coordinate systems. You may not be familiar with the definition of a coordinate basis in general curvilinear coordinates, such as spherical coordinates. The appropriate definition will be given in Chap. 5. However, equation (1.2) yields different answers even for the two coordinate systems in Fig. 1.1. For a more specific example, consider a temperature distribution $T$ in a two-dimensional rectangular room. Refer the interior of the room to a rectangular coordinate system $x$ , $y$ where the coordinate lines are one meter apart. This coordinate system is illustrated on the left of Fig. 1.1. Express the temperature field in terms of these coordinates and construct the vector gradient $\nabla T$ according to equation (1.2). Alternatively, refer the interior of the room to another rectangular system $x'$ , $y'$ , illustrated on the right of Fig. 1.1), whose coordinate lines are two meters apart. For example, at a point where $x = 2$ , the new coordinate $x'$ equals $1$ . Therefore, the new coordinates and the old coordinates are related by the identities $$x = 2x' \ \ \ \ \ y = 2y' \tag{1.3}$$ Now repeat the construction of the gradient according to equation (1.2) in the new coordinate system: refer the temperature field to the new coordinates, resulting in the function $F'(x', y')$ , calculate the partial derivatives and evaluate the expression in equation (1.2), except with “primed” elements: $$(\nabla T)' = \dfrac{\partial{F'}}{\partial{x'}} \mathbf{i}' + \dfrac{\partial{F'}}{\partial{y'}} \mathbf{j}' \tag{1.4}$$ How does $\nabla T$ compare to $(\nabla T)'$ ? The magnitudes of the new coordinate vectors $\mathbf{i}'$ and $\mathbf{j}'$ are double those of the old coordinate vectors $\mathbf{i}$ and $\mathbf{j}$ . What happens to the partial derivatives? Do they halve (this would be good) or do they double (this would be trouble)? They double! This is because in the new coordinates, quantities change twice as fast. In evaluating the rate of change with respect to, say, $x$ , one increments $x$ by a small amount $\Delta x$ , such as $\Delta x = 10^{-3}$ , and determines how much the function $F(x, y)$ changes in response to that small change in $x$ . When one evaluates the partial derivative with respect to $x'$ in the new coordinate system, the same increment in the new variable $x'$ is, in physical terms, twice as large. It results in twice as large a change $\Delta F'$ in the function $F'(x', y')$ . Therefore, $\Delta F' / \Delta x'$ is approximately twice as large as $\Delta F / \Delta x$ and we conclude that partial derivatives double: $$\dfrac{\partial{F'(x', y')}}{\partial{x'}} = 2\dfrac{\partial{F(x, y)}}{\partial{x}} \tag{1.5}$$ The first point I'm seeking clarification on is a minor problem: (1.1) is actually a doublet, right? The author refers to it as a triplet. The point that I'm uncomfortable with is the idea that $\Delta F' / \Delta x'$ is approximately twice as large as $\Delta F / \Delta x$ . If we're increasing the denominator by $10^{-3}$ , then why doesn't the numerator also increase by approximately the same amount, so that the result approximately cancels out to result in approximately the same value as before? I'm not satisfied that the author explains this clearly enough to make it evident. I would greatly appreciate it if people would please take the time to clarify this. EDIT The first +50 bounty has expired, so I might as well post my thoughts so far, so that it might help others formulate an answer. In trying to understand this, I had two streams of thought. The first stream of thought was focused on trying to understand (1.5), whilst the second stream of though was focused on trying to understand what I alluded to in my main post (which was the ideas behind the $\Delta F / \Delta x$ and $\Delta F' / \Delta x'$ ). It seems to me that an answer to the latter type of problem would naturally be suited to a real-analytic approach, so that's what I've been trying to do. I tried to understand the former by just using straightforward manipulations of the functions and their various implications. The main problem is that I don't have a lot of experience in real analysis, so I'm very unsure of my reasoning. I am not claiming that this post is an answer to the questions I have; rather, it is an expansion of my main post. In this expansion, I present my thoughts in trying to understand this section of the textbook, as well as some new questions that arose in the process of my reasoning. I am now going to post a second, larger bounty for this post. In addition to my original questions, I would appreciate it if people would review my reasoning here, and perhaps build upon it in their answer, and answer the questions that arose in this reasoning. The first problem I see here is that we don't know the form of the function $F(x, y)$ . It seems to me that this would have made things much easier. So we don't know whether, for instance, the function takes the form $F(x, y) = x + y$ , and, therefore, $\nabla F = (1, 1)$ , or whether the function takes some other form. I also wondered if we could use the antiderivative to recover the form of the function somehow, as is done to solve certain forms of differential equations, but I'm not sure whether that is possible/helpful in this situation. On the other hand, based on what I can infer from reasoning about this situation, it may not matter what form the function takes , or, rather, we may not need to know the precise form of the function , since we are told that the function form is such that $x = 2x', y = 2y'$ and $\dfrac{\partial{F'(x', y')}}{\partial{x'}} = 2\dfrac{\partial{F(x, y)}}{\partial{x}}$ , which might already give us enough information to understand what's going on here without needing the precise form of the function. The first part of my thoughts: So, we have that the change of coordinates $$x = 2x', \ \ \ y = 2y'$$ leads to the new function $$F'(x', y') = F' \left( \dfrac{x}{2}, \dfrac{y}{2} \right) = \dfrac{1}{2} F' \left( x, y \right).$$ Side-note: This assumes that $F' \left( \dfrac{x}{2}, \dfrac{y}{2} \right) = \dfrac{1}{2} F' \left( x, y \right)$ . This begs the question: If we have a function of the form $F'(x', y') = F'\left( \dfrac{x}{2}, \dfrac{y}{2} \right)$ , where $x = 2x'$ and $y = 2y'$ are a change of variables, then what conditions must be satisfied for us to be able to say that $F'(x', y') = F'\left( \dfrac{x}{2}, \dfrac{y}{2} \right) = \dfrac{1}{2} F'\left( x, y \right)$ ? I ask that question here , but have, as of yet, received no answers. I proceed assuming that it is valid in this case. EDIT: This property of functions seems to be referred to as "" homogeneity "" (see the aforementioned question for further details). So I proceed assuming that $F'$ is a homogeneous function . Taking the partial derivatives with respect to $x$ , we have $$\dfrac{\partial{F'(x', y')}}{\partial{x'}} = \dfrac{1}{2} \dfrac{\partial{F'\left( x, y \right)}}{\partial{x}'} \Rightarrow 2 \dfrac{\partial{F'(x', y')}}{\partial{x'}} = \dfrac{\partial{F'\left( x, y \right)}}{\partial{x}'}.$$ I wonder if $\dfrac{\partial{F'\left( x, y \right)}}{\partial{x}'} = \dfrac{\partial{F\left( x, y \right)}}{\partial{x}}$ ? Because then we would have that $$\dfrac{\partial{F'(x', y')}}{\partial{x'}} = \dfrac{1}{2} \dfrac{\partial{F'\left( x, y \right)}}{\partial{x}'} \Rightarrow 2 \dfrac{\partial{F'(x', y')}}{\partial{x'}} = \dfrac{\partial{F'\left( x, y \right)}}{\partial{x}'} = \dfrac{\partial{F\left( x, y \right)}}{\partial{x}} \\ \therefore 2 \dfrac{\partial{F'(x', y')}}{\partial{x'}} = \dfrac{\partial{F\left( x, y \right)}}{\partial{x}},$$ as required. The second part of my thoughts: Using the definition of partial differentiation , I think we can proceed as follows: $$\begin{align} \dfrac{\Delta F}{\Delta x} = \dfrac{\partial{F(x, y)}}{\partial{x}} &= \lim_{\Delta x \to 10^{-3}} \dfrac{F(x + \Delta x, y) - F(x, y)}{\Delta x} \\ &= \lim_{\Delta (2x') \to 10^{-3}} \dfrac{F(2x' + \Delta (2x'), 2y') - F(2x', 2y')}{\Delta (2x')} \\ &= \lim_{\Delta (x') \to \frac{10^{-3}}{2}} \dfrac{2[F(x' + \Delta (x'), y') - F(x', y')]}{2\Delta (x')} \ \ \text{(Assuming that $F$ is a **homogeneous function**.)} \\ &= \lim_{\Delta (x') \to \frac{10^{-3}}{2}} \dfrac{F(x' + \Delta (x'), y') - F(x', y')}{\Delta (x')} \end{align}$$ I'm not sure whether I made an error here or whether it's just a dead-end, but it seems to that proceeding in this way, using the definition of partial differentiation, is a good way to gain an understanding of what the author meant. EDIT 2: My new reasoning is as follows: The denominator is $\Delta x$ . And since $x = 2x' \Rightarrow x' = \dfrac{x}{2}$ , we have that $\dfrac{\Delta F'}{\Delta x'} = \dfrac{\Delta F'}{\Delta \frac{x}{2}} = \dfrac{2\Delta F'}{ \Delta x}$ . If $\Delta x = 10^{-3}$ , then $\dfrac{2\Delta F'}{10^{-3}}$ . So, by this, we can only say that $\dfrac{\Delta F'}{\Delta x'}$ is approximately double $\dfrac{\Delta F}{\Delta x}$ if $F' = F$ , so that $\dfrac{\Delta F'}{\Delta x'} = \dfrac{2\Delta F'}{10^{-3}} = \dfrac{2\Delta F}{10^{-3}}$ .","I am currently studying the textbook Introduction to Tensor Analysis and the Calculus of Moving Surfaces , by Pavel Grinfeld. On page 3, the author begins with the following motivating example: What is the gradient of a function and a point ? You are familiar with two definitions, one geometric and one analytical. According to the geometric definition, the gradient of is the vector that points in the direction of the greatest increase of the function , and its magnitude equals the greatest rate of increase. According to the analytical definition that requires the presence of a coordinate system, the gradient of is the triplet of numbers Are the two definitions equivalent in some sense? If you believe that the connection is where and are the coordinate basis, you are in for a surprise! Equation (1.2) can only be considered valid if it produces the same vector in all coordinate systems. You may not be familiar with the definition of a coordinate basis in general curvilinear coordinates, such as spherical coordinates. The appropriate definition will be given in Chap. 5. However, equation (1.2) yields different answers even for the two coordinate systems in Fig. 1.1. For a more specific example, consider a temperature distribution in a two-dimensional rectangular room. Refer the interior of the room to a rectangular coordinate system , where the coordinate lines are one meter apart. This coordinate system is illustrated on the left of Fig. 1.1. Express the temperature field in terms of these coordinates and construct the vector gradient according to equation (1.2). Alternatively, refer the interior of the room to another rectangular system , , illustrated on the right of Fig. 1.1), whose coordinate lines are two meters apart. For example, at a point where , the new coordinate equals . Therefore, the new coordinates and the old coordinates are related by the identities Now repeat the construction of the gradient according to equation (1.2) in the new coordinate system: refer the temperature field to the new coordinates, resulting in the function , calculate the partial derivatives and evaluate the expression in equation (1.2), except with “primed” elements: How does compare to ? The magnitudes of the new coordinate vectors and are double those of the old coordinate vectors and . What happens to the partial derivatives? Do they halve (this would be good) or do they double (this would be trouble)? They double! This is because in the new coordinates, quantities change twice as fast. In evaluating the rate of change with respect to, say, , one increments by a small amount , such as , and determines how much the function changes in response to that small change in . When one evaluates the partial derivative with respect to in the new coordinate system, the same increment in the new variable is, in physical terms, twice as large. It results in twice as large a change in the function . Therefore, is approximately twice as large as and we conclude that partial derivatives double: The first point I'm seeking clarification on is a minor problem: (1.1) is actually a doublet, right? The author refers to it as a triplet. The point that I'm uncomfortable with is the idea that is approximately twice as large as . If we're increasing the denominator by , then why doesn't the numerator also increase by approximately the same amount, so that the result approximately cancels out to result in approximately the same value as before? I'm not satisfied that the author explains this clearly enough to make it evident. I would greatly appreciate it if people would please take the time to clarify this. EDIT The first +50 bounty has expired, so I might as well post my thoughts so far, so that it might help others formulate an answer. In trying to understand this, I had two streams of thought. The first stream of thought was focused on trying to understand (1.5), whilst the second stream of though was focused on trying to understand what I alluded to in my main post (which was the ideas behind the and ). It seems to me that an answer to the latter type of problem would naturally be suited to a real-analytic approach, so that's what I've been trying to do. I tried to understand the former by just using straightforward manipulations of the functions and their various implications. The main problem is that I don't have a lot of experience in real analysis, so I'm very unsure of my reasoning. I am not claiming that this post is an answer to the questions I have; rather, it is an expansion of my main post. In this expansion, I present my thoughts in trying to understand this section of the textbook, as well as some new questions that arose in the process of my reasoning. I am now going to post a second, larger bounty for this post. In addition to my original questions, I would appreciate it if people would review my reasoning here, and perhaps build upon it in their answer, and answer the questions that arose in this reasoning. The first problem I see here is that we don't know the form of the function . It seems to me that this would have made things much easier. So we don't know whether, for instance, the function takes the form , and, therefore, , or whether the function takes some other form. I also wondered if we could use the antiderivative to recover the form of the function somehow, as is done to solve certain forms of differential equations, but I'm not sure whether that is possible/helpful in this situation. On the other hand, based on what I can infer from reasoning about this situation, it may not matter what form the function takes , or, rather, we may not need to know the precise form of the function , since we are told that the function form is such that and , which might already give us enough information to understand what's going on here without needing the precise form of the function. The first part of my thoughts: So, we have that the change of coordinates leads to the new function Side-note: This assumes that . This begs the question: If we have a function of the form , where and are a change of variables, then what conditions must be satisfied for us to be able to say that ? I ask that question here , but have, as of yet, received no answers. I proceed assuming that it is valid in this case. EDIT: This property of functions seems to be referred to as "" homogeneity "" (see the aforementioned question for further details). So I proceed assuming that is a homogeneous function . Taking the partial derivatives with respect to , we have I wonder if ? Because then we would have that as required. The second part of my thoughts: Using the definition of partial differentiation , I think we can proceed as follows: I'm not sure whether I made an error here or whether it's just a dead-end, but it seems to that proceeding in this way, using the definition of partial differentiation, is a good way to gain an understanding of what the author meant. EDIT 2: My new reasoning is as follows: The denominator is . And since , we have that . If , then . So, by this, we can only say that is approximately double if , so that .","F P \nabla F F F F \nabla F = \left( \dfrac{\partial{F}}{\partial{x}}, \dfrac{\partial{F}}{\partial{y}} \right) \tag{1.1} \nabla F = \dfrac{\partial{F}}{\partial{x}}\mathbf{i}  + \dfrac{\partial{F}}{\partial{y}} \mathbf{j}\tag{1.2} \mathbf{i} \mathbf{j} T x y \nabla T x' y' x = 2 x' 1 x = 2x' \ \ \ \ \ y = 2y' \tag{1.3} F'(x', y') (\nabla T)' = \dfrac{\partial{F'}}{\partial{x'}} \mathbf{i}' + \dfrac{\partial{F'}}{\partial{y'}} \mathbf{j}' \tag{1.4} \nabla T (\nabla T)' \mathbf{i}' \mathbf{j}' \mathbf{i} \mathbf{j} x x \Delta x \Delta x = 10^{-3} F(x, y) x x' x' \Delta F' F'(x', y') \Delta F' / \Delta x' \Delta F / \Delta x \dfrac{\partial{F'(x', y')}}{\partial{x'}} = 2\dfrac{\partial{F(x, y)}}{\partial{x}} \tag{1.5} \Delta F' / \Delta x' \Delta F / \Delta x 10^{-3} \Delta F / \Delta x \Delta F' / \Delta x' F(x, y) F(x, y) = x + y \nabla F = (1, 1) x = 2x', y = 2y' \dfrac{\partial{F'(x', y')}}{\partial{x'}} = 2\dfrac{\partial{F(x, y)}}{\partial{x}} x = 2x', \ \ \ y = 2y' F'(x', y') = F' \left( \dfrac{x}{2}, \dfrac{y}{2} \right) = \dfrac{1}{2} F' \left( x, y \right). F' \left( \dfrac{x}{2}, \dfrac{y}{2} \right) = \dfrac{1}{2} F' \left( x, y \right) F'(x', y') = F'\left( \dfrac{x}{2}, \dfrac{y}{2} \right) x = 2x' y = 2y' F'(x', y') = F'\left( \dfrac{x}{2}, \dfrac{y}{2} \right) = \dfrac{1}{2} F'\left( x, y \right) F' x \dfrac{\partial{F'(x', y')}}{\partial{x'}} = \dfrac{1}{2} \dfrac{\partial{F'\left( x, y \right)}}{\partial{x}'} \Rightarrow 2 \dfrac{\partial{F'(x', y')}}{\partial{x'}} = \dfrac{\partial{F'\left( x, y \right)}}{\partial{x}'}. \dfrac{\partial{F'\left( x, y \right)}}{\partial{x}'} = \dfrac{\partial{F\left( x, y \right)}}{\partial{x}} \dfrac{\partial{F'(x', y')}}{\partial{x'}} = \dfrac{1}{2} \dfrac{\partial{F'\left( x, y \right)}}{\partial{x}'} \Rightarrow 2 \dfrac{\partial{F'(x', y')}}{\partial{x'}} = \dfrac{\partial{F'\left( x, y \right)}}{\partial{x}'} = \dfrac{\partial{F\left( x, y \right)}}{\partial{x}} \\ \therefore 2 \dfrac{\partial{F'(x', y')}}{\partial{x'}} = \dfrac{\partial{F\left( x, y \right)}}{\partial{x}}, \begin{align} \dfrac{\Delta F}{\Delta x} = \dfrac{\partial{F(x, y)}}{\partial{x}} &= \lim_{\Delta x \to 10^{-3}} \dfrac{F(x + \Delta x, y) - F(x, y)}{\Delta x} \\ &= \lim_{\Delta (2x') \to 10^{-3}} \dfrac{F(2x' + \Delta (2x'), 2y') - F(2x', 2y')}{\Delta (2x')} \\ &= \lim_{\Delta (x') \to \frac{10^{-3}}{2}} \dfrac{2[F(x' + \Delta (x'), y') - F(x', y')]}{2\Delta (x')} \ \ \text{(Assuming that F is a **homogeneous function**.)} \\ &= \lim_{\Delta (x') \to \frac{10^{-3}}{2}} \dfrac{F(x' + \Delta (x'), y') - F(x', y')}{\Delta (x')} \end{align} \Delta x x = 2x' \Rightarrow x' = \dfrac{x}{2} \dfrac{\Delta F'}{\Delta x'} = \dfrac{\Delta F'}{\Delta \frac{x}{2}} = \dfrac{2\Delta F'}{ \Delta x} \Delta x = 10^{-3} \dfrac{2\Delta F'}{10^{-3}} \dfrac{\Delta F'}{\Delta x'} \dfrac{\Delta F}{\Delta x} F' = F \dfrac{\Delta F'}{\Delta x'} = \dfrac{2\Delta F'}{10^{-3}} = \dfrac{2\Delta F}{10^{-3}}","['real-analysis', 'multivariable-calculus', 'partial-derivative', 'vector-analysis', 'coordinate-systems']"
88,Intuition of divergence and curl,Intuition of divergence and curl,,"There is the well known expression for the divergence of a vector field $V$ as the limit of smaller and smaller surfaces of the flux of a surface. However it occurred to me that there is another way to describe the divergence which I could not figure out how to search to read more about or to even find out if it is correct. If $V = \sum V^i e_i$ is a vector field described in the standard basis then the derivative is $$\sum_{i,j}\partial_jV^i e^j\otimes e_i$$ The trace of this is the divergence. Now from what I understand of the trace $\sum\partial_iV^i$ it is proportional to averaging the tensor over all all unit vectors $v$ and their corresponding forms $w$ in the following sense. $$\text{div}(V) = \text{tr}(\sum_{i,j}\partial_jV^i e^j\otimes e_i) \propto \int_C (\sum_{i,j}\partial_jV^i e^j\otimes e_i)(v,w)$$ where $C$ is the unit circle and $w$ is $v$ as a form. Concretely with $v=(\cos\theta,\sin\theta)$ then this becomes $$\int_0^{2\pi} \partial_1V^1\cos^2\theta + \partial_1V^2\cos\theta\sin\theta +\partial_2V^1\cos\theta\sin\theta +\partial_2V^2\sin^2\theta d\theta$$ The terms with $\sin\theta\cos\theta$ evaluate to 0. So this gives $$c(\partial_1V^1 +\partial_2V^2)$$ for a constant $c$ . The initial integral can be interpreted as saying for each direction we look at the derivative of $V$ in that direction $v$ and then project that onto $v$ (this is the role $w$ plays). Said another way this is how much the vector field changes in direction $v$ but only the part along $v$ and only the magnitude change. And then we kind of average over all directions. This is different from the usual flux over surface description in that first off it deals with the derivative of the vector field whereas the the integral of the flux does not deal with the derivative of the vector field. Is there a description like this for the curl?",There is the well known expression for the divergence of a vector field as the limit of smaller and smaller surfaces of the flux of a surface. However it occurred to me that there is another way to describe the divergence which I could not figure out how to search to read more about or to even find out if it is correct. If is a vector field described in the standard basis then the derivative is The trace of this is the divergence. Now from what I understand of the trace it is proportional to averaging the tensor over all all unit vectors and their corresponding forms in the following sense. where is the unit circle and is as a form. Concretely with then this becomes The terms with evaluate to 0. So this gives for a constant . The initial integral can be interpreted as saying for each direction we look at the derivative of in that direction and then project that onto (this is the role plays). Said another way this is how much the vector field changes in direction but only the part along and only the magnitude change. And then we kind of average over all directions. This is different from the usual flux over surface description in that first off it deals with the derivative of the vector field whereas the the integral of the flux does not deal with the derivative of the vector field. Is there a description like this for the curl?,"V V = \sum V^i e_i \sum_{i,j}\partial_jV^i e^j\otimes e_i \sum\partial_iV^i v w \text{div}(V) = \text{tr}(\sum_{i,j}\partial_jV^i e^j\otimes e_i) \propto \int_C (\sum_{i,j}\partial_jV^i e^j\otimes e_i)(v,w) C w v v=(\cos\theta,\sin\theta) \int_0^{2\pi} \partial_1V^1\cos^2\theta + \partial_1V^2\cos\theta\sin\theta +\partial_2V^1\cos\theta\sin\theta +\partial_2V^2\sin^2\theta d\theta \sin\theta\cos\theta c(\partial_1V^1 +\partial_2V^2) c V v v w v v","['multivariable-calculus', 'tensors']"
89,Finding the probability distribution using transformations,Finding the probability distribution using transformations,,"The question : Suppose $X_1,X_2$ are iid with a common standard normal distribution. Find the joint pdf of $Y_1=X_1^{2}+X_2^{2}$ and $Y_2=X_2$ and the marginal pdf of $Y_1$ . Hint:Note that the space of $Y_1$ and $Y_2$ is given by $-\sqrt{y_1}< y_2< \sqrt{y_1}$ , $0< y_1<\infty $ . My attempt : $X_1^{2}\sim \chi ^{2}(1)$ and $X_2^{2}\sim \chi ^{2}(1)$ , so $Y_1\sim \chi ^{2}(2)$ . The pdf of $Y_1$ should be $\frac{1}{2}e^{\frac{-y}{2}}$ . I tried to get the result by using the transformations of random variables. If $0< y_2<\sqrt{y_1}$ , then $x_1=\sqrt{y_1-y_2^{2}}$ . If $-\sqrt{y_1}< y_2<0$ , then $x_1=-\sqrt{y_1-y_2^{2}}$ In both cases, the absolute value of the jacobian is $\frac{1}{2\sqrt{y_1-y_2^{2}}}$ . So, $f_{Y_1 ,Y_2}(y_1,y_2)=f_{X_1,X_2}(x_1,x_2)\left | J \right |$ , where $\left | J \right |$ is $\frac{1}{2\sqrt{y_1-y_2^{2}}}$ . $f_{X_1,X_2}(x_1,x_2)=\frac{1}{2\pi }e^{\frac{-y_1}{2}}$ (multiplication of two standard normal pdf's) $f_{Y_1 ,Y_2}(y_1,y_2)=f_{X_1,X_2}(x_1,x_2)\left | J \right |=\frac{1}{2\pi }e^{\frac{-y_1}{2}}\frac{1}{2\sqrt{y_1-y_2^{2}}}$ $f_{Y_1}(y_1)=\frac{e^{-y_1/2}}{4\pi }\int_{-\sqrt{y_1}}^{\sqrt{y_1}}\frac{1}{\sqrt{y_1-y_2^{2}}}dy_2$ . I used this integral: $\int_{-a}^{a}\frac{1}{\sqrt{a^2-x^2}}=\pi $ for $a>0$ . Then $f_{Y_1}(y_1)=\frac{e^{-y_1/2}}{4 }$ , which is not same as $\frac{e^{-y_1/2}}{2 }$ . I cannot find out where it went wrong. I need some help.","The question : Suppose are iid with a common standard normal distribution. Find the joint pdf of and and the marginal pdf of . Hint:Note that the space of and is given by , . My attempt : and , so . The pdf of should be . I tried to get the result by using the transformations of random variables. If , then . If , then In both cases, the absolute value of the jacobian is . So, , where is . (multiplication of two standard normal pdf's) . I used this integral: for . Then , which is not same as . I cannot find out where it went wrong. I need some help.","X_1,X_2 Y_1=X_1^{2}+X_2^{2} Y_2=X_2 Y_1 Y_1 Y_2 -\sqrt{y_1}< y_2< \sqrt{y_1} 0< y_1<\infty  X_1^{2}\sim \chi ^{2}(1) X_2^{2}\sim \chi ^{2}(1) Y_1\sim \chi ^{2}(2) Y_1 \frac{1}{2}e^{\frac{-y}{2}} 0< y_2<\sqrt{y_1} x_1=\sqrt{y_1-y_2^{2}} -\sqrt{y_1}< y_2<0 x_1=-\sqrt{y_1-y_2^{2}} \frac{1}{2\sqrt{y_1-y_2^{2}}} f_{Y_1 ,Y_2}(y_1,y_2)=f_{X_1,X_2}(x_1,x_2)\left | J \right | \left | J \right | \frac{1}{2\sqrt{y_1-y_2^{2}}} f_{X_1,X_2}(x_1,x_2)=\frac{1}{2\pi }e^{\frac{-y_1}{2}} f_{Y_1 ,Y_2}(y_1,y_2)=f_{X_1,X_2}(x_1,x_2)\left | J \right |=\frac{1}{2\pi }e^{\frac{-y_1}{2}}\frac{1}{2\sqrt{y_1-y_2^{2}}} f_{Y_1}(y_1)=\frac{e^{-y_1/2}}{4\pi }\int_{-\sqrt{y_1}}^{\sqrt{y_1}}\frac{1}{\sqrt{y_1-y_2^{2}}}dy_2 \int_{-a}^{a}\frac{1}{\sqrt{a^2-x^2}}=\pi  a>0 f_{Y_1}(y_1)=\frac{e^{-y_1/2}}{4 } \frac{e^{-y_1/2}}{2 }","['multivariable-calculus', 'probability-distributions']"
90,"""Unrolling"" a 3d wedge","""Unrolling"" a 3d wedge",,"I have a parametric equation that describes a particular intersection in 3d space.  I'd like to flatten this by deforming the object without stretching its length, as if this were the outline of a sticker, and I was making it flat again.  Clearly this is not possible for a general curve, and this is not possible for even the curve in question except along one of two axes.  (It is possible for this curve because it is the intersection of two right cylinders, and it's possible to ""unroll"" the length of a right cylinder into a rectangle.) I had considered using the arc length formula dx' = $\sqrt{dx^2 + dz^2}$.  However, this will only give a positive value for dx' and dx has clearly positive and negative parts. What is the proper way to perform this operation, and what is this called? If you're interested or it's relevant, the equation of this curve is: $$\begin{split}   x(t) &= b \cos(t) \\   y(t) &= b \sin(t) \\   z(t) &= \sqrt{a^2 - b^2 \sin^2(t)} \end{split}$$ Which is the intersection of the cylinders: $$\begin{split}   x^2 + y^2 &= b^2 \\   x^2 + z^2 &= a^2 \\ \end{split}$$ Where $b < a$.  And the plane I'm attempting to unroll is the one defined by the second of those two equations.","I have a parametric equation that describes a particular intersection in 3d space.  I'd like to flatten this by deforming the object without stretching its length, as if this were the outline of a sticker, and I was making it flat again.  Clearly this is not possible for a general curve, and this is not possible for even the curve in question except along one of two axes.  (It is possible for this curve because it is the intersection of two right cylinders, and it's possible to ""unroll"" the length of a right cylinder into a rectangle.) I had considered using the arc length formula dx' = $\sqrt{dx^2 + dz^2}$.  However, this will only give a positive value for dx' and dx has clearly positive and negative parts. What is the proper way to perform this operation, and what is this called? If you're interested or it's relevant, the equation of this curve is: $$\begin{split}   x(t) &= b \cos(t) \\   y(t) &= b \sin(t) \\   z(t) &= \sqrt{a^2 - b^2 \sin^2(t)} \end{split}$$ Which is the intersection of the cylinders: $$\begin{split}   x^2 + y^2 &= b^2 \\   x^2 + z^2 &= a^2 \\ \end{split}$$ Where $b < a$.  And the plane I'm attempting to unroll is the one defined by the second of those two equations.",,"['calculus', 'multivariable-calculus', 'parametric', 'curves', 'arc-length']"
91,Product of Two Functions is Riemann Integrable (Multivariable Calculus),Product of Two Functions is Riemann Integrable (Multivariable Calculus),,"If $S$ is a non-empty bounded set in $\mathbb{R}^N$, and $f,g:S\longrightarrow\mathbb{R}$ are Riemann integrable, show $fg$ is Riemann integrable. I am not too sure with the $N$ dimensional proof of this, and have only worked it out in $1$-dimension. My work so far is: Suppose $f$ is bounded and Riemann integrable on $[a,b]$, then clearly $|f(x)|<a$ for some bound $a\geq 0$ So, $$|f^2(x)-f^2(y)|=|f(x)+f(y)||f(x)-f(y)|\leq2a|f(x)-f(y)|$$ By letting $M(f):=\sup\{f:x_{i-1}\leq x\leq x_i\}$ and $m(f):=\inf\{f:x_{i-1}\leq x \leq x_i\}$, We see, $M(f^2)-m(f^2)\leq 2a(M(f)-m(f))$ By Cauchy Criterion, because $f$ is integrable on $[a,b]$, $\forall \epsilon > 0$, $\exists$ a partition $P$, such that $$U(P,f)-L(P,f)< \frac{\epsilon}{2a}$$ $$\therefore U(P,f^2)-L(P,f^2)< \epsilon$$ Thus $f^2$ is Riemann integrable. The claim then follows by noting $fg=\frac 12((f+g)^2-f^2-g^2)$ Is this the right way to proceed for a multivariable proof of the statement in n-dimensions? Or am I not approaching this correctly? How would my proof change for a multivariable version? Any help would be appreciated. Thanks in advance!","If $S$ is a non-empty bounded set in $\mathbb{R}^N$, and $f,g:S\longrightarrow\mathbb{R}$ are Riemann integrable, show $fg$ is Riemann integrable. I am not too sure with the $N$ dimensional proof of this, and have only worked it out in $1$-dimension. My work so far is: Suppose $f$ is bounded and Riemann integrable on $[a,b]$, then clearly $|f(x)|<a$ for some bound $a\geq 0$ So, $$|f^2(x)-f^2(y)|=|f(x)+f(y)||f(x)-f(y)|\leq2a|f(x)-f(y)|$$ By letting $M(f):=\sup\{f:x_{i-1}\leq x\leq x_i\}$ and $m(f):=\inf\{f:x_{i-1}\leq x \leq x_i\}$, We see, $M(f^2)-m(f^2)\leq 2a(M(f)-m(f))$ By Cauchy Criterion, because $f$ is integrable on $[a,b]$, $\forall \epsilon > 0$, $\exists$ a partition $P$, such that $$U(P,f)-L(P,f)< \frac{\epsilon}{2a}$$ $$\therefore U(P,f^2)-L(P,f^2)< \epsilon$$ Thus $f^2$ is Riemann integrable. The claim then follows by noting $fg=\frac 12((f+g)^2-f^2-g^2)$ Is this the right way to proceed for a multivariable proof of the statement in n-dimensions? Or am I not approaching this correctly? How would my proof change for a multivariable version? Any help would be appreciated. Thanks in advance!",,"['multivariable-calculus', 'riemann-integration']"
92,What is the derivative of $f(tx)$ with respect to $t$?,What is the derivative of  with respect to ?,f(tx) t,"I think is a super basic question but I just can't seem to wrap my head around how think about this. Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be differentiable. We fix an $x \in \mathbb{R}^n$ and define the function $g: \mathbb{R} \rightarrow \mathbb{R}$ by $g(t) = f(tx), \forall t \in \mathbb{R}$. Now, my question is what is $g'(t)$? If I were deriving $f(tx)$ with respect to $x$, this is easy, I have $f'(tx)t$, but it's not really clear how to derive with respect to $t$. Is it $f'(tx)x = \nabla f(tx)x$? If yes, why? Wouldn't that imply that $g'(t) \in \mathbb{R}^n$, since $f'(tx) =\nabla f(tx) \in \mathbb{R}$ and $x \in \mathbb{R}^n$? I apologize if this a super basic question and I'm just missing something crucial! EDIT: Ok, I think I have an understanding now. Here's what I have: Let $h: \mathbb{R} \rightarrow \mathbb{R}^n, h(t) = tx$. Now, we have $g(t) = f(h(t))$, and so, by the chain rule: \begin{align} Dg(t) & = Df(h(t))\cdot Dh(t) = \begin{pmatrix}\frac{\partial f}{\partial h_1(t)} & \dots & \frac{\partial f}{\partial h_n(t)}\end{pmatrix} \begin{pmatrix}\frac{\partial h_1}{\partial t} \\ \vdots \\ \frac{\partial h_n}{\partial t}\end{pmatrix} \\[10pt] & = \frac{\partial f}{\partial h_1(t)}\frac{\partial h_1}{\partial t} + \dots + \frac{\partial f}{\partial h_n(t)}\frac{\partial h_n}{\partial t} \\[10pt] & = \frac{\partial f}{\partial h_1(t)}x_1 + \dots + \frac{\partial f}{\partial h_n(t)}x_n = \frac{\partial f}{\partial tx_1}x_1 + \dots + \frac{\partial f}{\partial tx_n}x_n = Df(tx)x. \end{align} Is this understanding correct? Moreover, is $\nabla f(x) = \begin{pmatrix}\frac{\partial f}{\partial x_1} & \dots & \frac{\partial f}{\partial x_n}\end{pmatrix}$ or $\nabla f(x) = \frac{\partial f}{\partial x_1} + \dots + \frac{\partial f}{\partial x_n}$? I thought it was the latter, which led to my confusion, but dealing with everything in jacobian form, it seems that it is the former?","I think is a super basic question but I just can't seem to wrap my head around how think about this. Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ be differentiable. We fix an $x \in \mathbb{R}^n$ and define the function $g: \mathbb{R} \rightarrow \mathbb{R}$ by $g(t) = f(tx), \forall t \in \mathbb{R}$. Now, my question is what is $g'(t)$? If I were deriving $f(tx)$ with respect to $x$, this is easy, I have $f'(tx)t$, but it's not really clear how to derive with respect to $t$. Is it $f'(tx)x = \nabla f(tx)x$? If yes, why? Wouldn't that imply that $g'(t) \in \mathbb{R}^n$, since $f'(tx) =\nabla f(tx) \in \mathbb{R}$ and $x \in \mathbb{R}^n$? I apologize if this a super basic question and I'm just missing something crucial! EDIT: Ok, I think I have an understanding now. Here's what I have: Let $h: \mathbb{R} \rightarrow \mathbb{R}^n, h(t) = tx$. Now, we have $g(t) = f(h(t))$, and so, by the chain rule: \begin{align} Dg(t) & = Df(h(t))\cdot Dh(t) = \begin{pmatrix}\frac{\partial f}{\partial h_1(t)} & \dots & \frac{\partial f}{\partial h_n(t)}\end{pmatrix} \begin{pmatrix}\frac{\partial h_1}{\partial t} \\ \vdots \\ \frac{\partial h_n}{\partial t}\end{pmatrix} \\[10pt] & = \frac{\partial f}{\partial h_1(t)}\frac{\partial h_1}{\partial t} + \dots + \frac{\partial f}{\partial h_n(t)}\frac{\partial h_n}{\partial t} \\[10pt] & = \frac{\partial f}{\partial h_1(t)}x_1 + \dots + \frac{\partial f}{\partial h_n(t)}x_n = \frac{\partial f}{\partial tx_1}x_1 + \dots + \frac{\partial f}{\partial tx_n}x_n = Df(tx)x. \end{align} Is this understanding correct? Moreover, is $\nabla f(x) = \begin{pmatrix}\frac{\partial f}{\partial x_1} & \dots & \frac{\partial f}{\partial x_n}\end{pmatrix}$ or $\nabla f(x) = \frac{\partial f}{\partial x_1} + \dots + \frac{\partial f}{\partial x_n}$? I thought it was the latter, which led to my confusion, but dealing with everything in jacobian form, it seems that it is the former?",,"['calculus', 'real-analysis', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
93,Line integral with vector field in polar coordinates,Line integral with vector field in polar coordinates,,"I have the following problem: Given a vector field in polar coordinates $$ \mathbf{F}(r,\theta) = -4 \sin \theta\ \mathbf{i}\ +\ 4 \sin \theta\ \mathbf{j},$$ calculate the work done when a particle is moved from point $(1,0)$ to the origin, following the spiral whose polar equation is $r = e^{-\theta}.$ My attempt was to write the equation of the spiral like so $$\mathbf{\alpha}(t) = e^{-\theta} \cos \theta\ \mathbf{i} + e^{-\theta} \sin \theta\ \mathbf{j} \\ \mathbf{\alpha}'(t) = -e^{-\theta}(\cos \theta + \sin \theta\ \mathbf{i}\ + \sin \theta - \cos \theta\ \mathbf{j}),$$ so the line integral would become $$\int_C \mathbf{F}\cdot\mathbf{\alpha'}(t) = \int_C 8 e^{-\theta}\sin\theta\cos\theta\ d\theta.$$ But this doesn't give me the right answer, what am I doing wrong? NOTE: I know this question was asked before, but it doesn't have an accepted answer, and what I read from there wasn't very helpful.","I have the following problem: Given a vector field in polar coordinates calculate the work done when a particle is moved from point to the origin, following the spiral whose polar equation is My attempt was to write the equation of the spiral like so so the line integral would become But this doesn't give me the right answer, what am I doing wrong? NOTE: I know this question was asked before, but it doesn't have an accepted answer, and what I read from there wasn't very helpful."," \mathbf{F}(r,\theta) = -4 \sin \theta\ \mathbf{i}\ +\ 4 \sin \theta\ \mathbf{j}, (1,0) r = e^{-\theta}. \mathbf{\alpha}(t) = e^{-\theta} \cos \theta\ \mathbf{i} + e^{-\theta} \sin \theta\ \mathbf{j} \\ \mathbf{\alpha}'(t) = -e^{-\theta}(\cos \theta + \sin \theta\ \mathbf{i}\ + \sin \theta - \cos \theta\ \mathbf{j}), \int_C \mathbf{F}\cdot\mathbf{\alpha'}(t) = \int_C 8 e^{-\theta}\sin\theta\cos\theta\ d\theta.",['multivariable-calculus']
94,An inequality concerning Lagrange's identity,An inequality concerning Lagrange's identity,,"Does the following inequality still hold   $$(a^2_{1}+b^2_{2}+b^2_{3})(a^2_{2}+b^2_{3}+b^2_{1})(a^2_{3}+b^2_{1}+b^2_{2})\ge (b^2_{1}+b^2_{2}+b^2_{3})(a_{1}b_{1}+a_{2}b_{2}+a_{3}b_{3})^2 $$   $$+\dfrac{1}{2}(b_{1}a_{2}b_{3}-b_{1}b_{2}a_{3})^2+\dfrac{1}{2}(b_{1}b_{2}a_{3}-a_{1}b_{2}b_{3})^2+\dfrac{1}{2}(a_{1}b_{2}b_{3}-b_{1}a_{2}b_{3})^2\tag{*}$$   for $a_{i},b_{i}\in \mathbb R,i=1,2,3$? we  know Lagrange's identity $$(a^2_{1}+a^2_{2}+a^2_{3})(b^2_{1}+b^2_{2}+b^2_{3})=(a_{1}b_{1}+a_{2}b_{2}+a_{3}b_{3})^2+\sum_{i=1}^{2}\sum_{j=i+1}^{3}(a_{i}b_{j}-a_{j}b_{i})^2$$ then we have Cauchy-Schwarz inequality $$(a^2_{1}+a^2_{2}+a^2_{3})(b^2_{1}+b^2_{2}+b^2_{3})\ge (a_{1}b_{1}+a_{2}b_{2}+a_{3}b_{3})^2$$","Does the following inequality still hold   $$(a^2_{1}+b^2_{2}+b^2_{3})(a^2_{2}+b^2_{3}+b^2_{1})(a^2_{3}+b^2_{1}+b^2_{2})\ge (b^2_{1}+b^2_{2}+b^2_{3})(a_{1}b_{1}+a_{2}b_{2}+a_{3}b_{3})^2 $$   $$+\dfrac{1}{2}(b_{1}a_{2}b_{3}-b_{1}b_{2}a_{3})^2+\dfrac{1}{2}(b_{1}b_{2}a_{3}-a_{1}b_{2}b_{3})^2+\dfrac{1}{2}(a_{1}b_{2}b_{3}-b_{1}a_{2}b_{3})^2\tag{*}$$   for $a_{i},b_{i}\in \mathbb R,i=1,2,3$? we  know Lagrange's identity $$(a^2_{1}+a^2_{2}+a^2_{3})(b^2_{1}+b^2_{2}+b^2_{3})=(a_{1}b_{1}+a_{2}b_{2}+a_{3}b_{3})^2+\sum_{i=1}^{2}\sum_{j=i+1}^{3}(a_{i}b_{j}-a_{j}b_{i})^2$$ then we have Cauchy-Schwarz inequality $$(a^2_{1}+a^2_{2}+a^2_{3})(b^2_{1}+b^2_{2}+b^2_{3})\ge (a_{1}b_{1}+a_{2}b_{2}+a_{3}b_{3})^2$$",,"['multivariable-calculus', 'inequality', 'polynomials', 'cauchy-schwarz-inequality']"
95,"if $abc=1$,show that $a^3+b^3+c^3+\frac{256}{(a+1)(b+1)(c+1)}\ge 35$","if ,show that",abc=1 a^3+b^3+c^3+\frac{256}{(a+1)(b+1)(c+1)}\ge 35,"Let $a,b,c>0,abc=1$ show that $$a^3+b^3+c^3+\dfrac{256}{(a+1)(b+1)(c+1)}\ge 35\tag{1}$$ iff $a=b=c=1$ I know use AM-GM inequality $$a^3+b^3+c^3\ge 3abc=3$$ and $$(a+1)(b+1)(c+1)\ge 2\sqrt{a}\cdot 2\sqrt{b}\cdot 2\sqrt{c}=8$$ In this way, will lead to inequality reverse, but $(1)$ seem is right,so How to prove this inequality","Let show that iff I know use AM-GM inequality and In this way, will lead to inequality reverse, but seem is right,so How to prove this inequality","a,b,c>0,abc=1 a^3+b^3+c^3+\dfrac{256}{(a+1)(b+1)(c+1)}\ge 35\tag{1} a=b=c=1 a^3+b^3+c^3\ge 3abc=3 (a+1)(b+1)(c+1)\ge 2\sqrt{a}\cdot 2\sqrt{b}\cdot 2\sqrt{c}=8 (1)","['multivariable-calculus', 'inequality', 'symmetric-polynomials', 'uvw']"
96,How prove this inequality: $\frac1{1-a}+\frac1{1-b}+\frac1{1-c}\ge \frac1{ab+bc+ac}+\frac1{2(a^2+b^2+c^2)}$ for $a+b+c=1$?,How prove this inequality:  for ?,\frac1{1-a}+\frac1{1-b}+\frac1{1-c}\ge \frac1{ab+bc+ac}+\frac1{2(a^2+b^2+c^2)} a+b+c=1,"Let $a,b,c>0$ and such $a+b+c=1$ show that  $$\dfrac{1}{1-a}+\dfrac{1}{1-b}+\dfrac{1}{1-c}\ge \dfrac{1}{ab+bc+ac}+\dfrac{1}{2(a^2+b^2+c^2)}$$ Let $p=a+b+c=1,ab+bc+ac=q,abc=r$ $$\Longleftrightarrow -4q^3+q^2-3qr+2r\ge 0$$ it seem hard to prove. why I say it hard prove: use Schur inequality $$p^3-4pq+9r\ge 0\Longrightarrow r\ge\dfrac{4q-1}{9}$$ it  remains to prove that $$\dfrac{4q-1}{9}(2-3q)+q^2-4q^3\ge 0$$ In fact, this inequality can't hold (4q-1)","Let $a,b,c>0$ and such $a+b+c=1$ show that  $$\dfrac{1}{1-a}+\dfrac{1}{1-b}+\dfrac{1}{1-c}\ge \dfrac{1}{ab+bc+ac}+\dfrac{1}{2(a^2+b^2+c^2)}$$ Let $p=a+b+c=1,ab+bc+ac=q,abc=r$ $$\Longleftrightarrow -4q^3+q^2-3qr+2r\ge 0$$ it seem hard to prove. why I say it hard prove: use Schur inequality $$p^3-4pq+9r\ge 0\Longrightarrow r\ge\dfrac{4q-1}{9}$$ it  remains to prove that $$\dfrac{4q-1}{9}(2-3q)+q^2-4q^3\ge 0$$ In fact, this inequality can't hold (4q-1)",,"['multivariable-calculus', 'inequality', 'proof-writing', 'uvw']"
97,Finding extreme value,Finding extreme value,,"It is given: $f\left(x,y\right)=-7x^{2}-5xy+4y^{2}   $ and I should find x coordinate of the extreme value with condition $x-4y=4$. I think I know how to do this, but my solution is not the correct. I hope someone can tell me where is my mistake. What I did: From condition I got $y=\frac{x}{4}-1 $. I put that in my function and got this: $-8x^{2}+3x+4=0 $. I derivative it and got: -16x+3=0. So solution should be $\frac{3}{16} $. But it is not. What is my mistake?","It is given: $f\left(x,y\right)=-7x^{2}-5xy+4y^{2}   $ and I should find x coordinate of the extreme value with condition $x-4y=4$. I think I know how to do this, but my solution is not the correct. I hope someone can tell me where is my mistake. What I did: From condition I got $y=\frac{x}{4}-1 $. I put that in my function and got this: $-8x^{2}+3x+4=0 $. I derivative it and got: -16x+3=0. So solution should be $\frac{3}{16} $. But it is not. What is my mistake?",,"['calculus', 'multivariable-calculus']"
98,Can we detect smoothness of a norm by its behavior along paths?,Can we detect smoothness of a norm by its behavior along paths?,,"We say a norm $\| \cdot \|$ on $\mathbb{R}^n$ is smooth if it is smooth as a function $\mathbb{R}^n\setminus \{0\} \to \mathbb{R}$. (i.e, after restricting the domain). We say a norm is smooth along paths , if for any smooth path $\alpha:I \to \mathbb{R}^n \setminus \{0\}$, the composition$\|\cdot\| \circ \alpha$ is smooth. Does smoothness along paths of a norm implies that it's smooth? (Note that in general differentiability along paths is not enough to detect smoothness , but in this case we are talking about a norm, not a general function). Remarks: 1) Since any norm is Lipschitz (w.r.t the metric induced by it), it follows that the value of the derivative $\frac{d}{dt}\big|_{t=0} \|\alpha(t)\| $ depends only on $\dot \alpha(0),\alpha(0)$ and not on other properties of the specific path chosen. (See a proof at the end). Thus, for each $a \in \mathbb{R}^n\setminus\{0\}$, there is a function: $T_a:\mathbb{R}^n \to \mathbb{R}$ defined by: $T_a(v)=\frac{d}{dt}\big|_{t=0} \|\alpha(t)\|$ where $\alpha(t)$ is any path satisfying $\alpha(0)=a,\dot\alpha(0)=v$. It is proved here that if $T_a$ is linear, then $\|\cdot  \|$ is differentiable at the point $a$. So, proving linearity of $T_a$ (for every $a$) will show at least that $\|\cdot\|$ is one-time differentiable. (Showing harder degrees of regularity, i.e twice differentiability etc sounds more challenging, but let's start with that). So far, using homogeneity and triangle inequality, I managed to prove: $$T_a(\lambda v)=|\lambda|T_{\frac{a}{\lambda}}(v) \, \, \forall \lambda \in \mathbb{R},$$ $$T_a(v+w)\le T_a(v)+\|w\|\, \, , \, \,T_a(v+w) \le T_{\frac{a}{2}}(v)+T_{\frac{a}{2}}(w)$$ Proof of claim (1): (The derivative depends only on the velocity of the path) $$\big| \|\alpha(t)\|-\|\tilde \alpha(t)\| \big| \le \|\alpha(t)-\tilde \alpha(t) \|,$$ so assuming $\tilde \alpha(0)=\alpha(0),\dot{\tilde \alpha}(0)=\dot \alpha(0)=v \in \mathbb{R}^n$ we get: $$(*)\, \, \bigg| \frac{\|\alpha(t)\| - \|\alpha(0)\|}{t}-\frac{\|\tilde \alpha(t)\| - \|\tilde \alpha(0)\|}{t}\bigg| \le \|\frac{\alpha(t)-\tilde \alpha(t)}{t} \| $$ Now since $\lim_{t \to 0}\frac{\alpha(t)-\tilde \alpha(t)}{t} =\lim_{t \to 0}\big( \frac{\alpha(t)- \alpha(0)}{t}-\frac{\tilde \alpha(t)-\tilde \alpha(0)}{t}\big)=v-v=0$ we get by the continuity of the norm, together with $(*)$ we obtain the required equality $$ \frac{d}{dt}\big|_{t=0} \|\alpha(t)\|  = \lim_{t \to 0} \frac{\|\alpha(t)\| - \|\alpha(0)\|}{t}=\lim_{t \to 0} \frac{\|\tilde \alpha(t)\| - \|\tilde \alpha(0)\|}{t}=\frac{d}{dt}\big|_{t=0} \|\tilde \alpha(t)\|$$","We say a norm $\| \cdot \|$ on $\mathbb{R}^n$ is smooth if it is smooth as a function $\mathbb{R}^n\setminus \{0\} \to \mathbb{R}$. (i.e, after restricting the domain). We say a norm is smooth along paths , if for any smooth path $\alpha:I \to \mathbb{R}^n \setminus \{0\}$, the composition$\|\cdot\| \circ \alpha$ is smooth. Does smoothness along paths of a norm implies that it's smooth? (Note that in general differentiability along paths is not enough to detect smoothness , but in this case we are talking about a norm, not a general function). Remarks: 1) Since any norm is Lipschitz (w.r.t the metric induced by it), it follows that the value of the derivative $\frac{d}{dt}\big|_{t=0} \|\alpha(t)\| $ depends only on $\dot \alpha(0),\alpha(0)$ and not on other properties of the specific path chosen. (See a proof at the end). Thus, for each $a \in \mathbb{R}^n\setminus\{0\}$, there is a function: $T_a:\mathbb{R}^n \to \mathbb{R}$ defined by: $T_a(v)=\frac{d}{dt}\big|_{t=0} \|\alpha(t)\|$ where $\alpha(t)$ is any path satisfying $\alpha(0)=a,\dot\alpha(0)=v$. It is proved here that if $T_a$ is linear, then $\|\cdot  \|$ is differentiable at the point $a$. So, proving linearity of $T_a$ (for every $a$) will show at least that $\|\cdot\|$ is one-time differentiable. (Showing harder degrees of regularity, i.e twice differentiability etc sounds more challenging, but let's start with that). So far, using homogeneity and triangle inequality, I managed to prove: $$T_a(\lambda v)=|\lambda|T_{\frac{a}{\lambda}}(v) \, \, \forall \lambda \in \mathbb{R},$$ $$T_a(v+w)\le T_a(v)+\|w\|\, \, , \, \,T_a(v+w) \le T_{\frac{a}{2}}(v)+T_{\frac{a}{2}}(w)$$ Proof of claim (1): (The derivative depends only on the velocity of the path) $$\big| \|\alpha(t)\|-\|\tilde \alpha(t)\| \big| \le \|\alpha(t)-\tilde \alpha(t) \|,$$ so assuming $\tilde \alpha(0)=\alpha(0),\dot{\tilde \alpha}(0)=\dot \alpha(0)=v \in \mathbb{R}^n$ we get: $$(*)\, \, \bigg| \frac{\|\alpha(t)\| - \|\alpha(0)\|}{t}-\frac{\|\tilde \alpha(t)\| - \|\tilde \alpha(0)\|}{t}\bigg| \le \|\frac{\alpha(t)-\tilde \alpha(t)}{t} \| $$ Now since $\lim_{t \to 0}\frac{\alpha(t)-\tilde \alpha(t)}{t} =\lim_{t \to 0}\big( \frac{\alpha(t)- \alpha(0)}{t}-\frac{\tilde \alpha(t)-\tilde \alpha(0)}{t}\big)=v-v=0$ we get by the continuity of the norm, together with $(*)$ we obtain the required equality $$ \frac{d}{dt}\big|_{t=0} \|\alpha(t)\|  = \lim_{t \to 0} \frac{\|\alpha(t)\| - \|\alpha(0)\|}{t}=\lim_{t \to 0} \frac{\|\tilde \alpha(t)\| - \|\tilde \alpha(0)\|}{t}=\frac{d}{dt}\big|_{t=0} \|\tilde \alpha(t)\|$$",,"['multivariable-calculus', 'normed-spaces', 'smooth-manifolds']"
99,Why curl of a vector field measures its tendency of rotation,Why curl of a vector field measures its tendency of rotation,,I was trying to understand why curl measures a vector field's tendency of rotation. Two examples from physics seem to answer my question: Curl of the velocity field is twice the angular velocity Curl of the force field is the torque. But I can only prove the first one when the velocity field describes a uniform circular motion. How can I show that the two examples are true in general to show that curl is really the measure of rotation.,I was trying to understand why curl measures a vector field's tendency of rotation. Two examples from physics seem to answer my question: Curl of the velocity field is twice the angular velocity Curl of the force field is the torque. But I can only prove the first one when the velocity field describes a uniform circular motion. How can I show that the two examples are true in general to show that curl is really the measure of rotation.,,"['multivariable-calculus', 'vector-fields']"
