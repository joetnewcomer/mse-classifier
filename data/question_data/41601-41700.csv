,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Artinian rings are perfect,Artinian rings are perfect,,Definition. A ring is called perfect if every flat module is projective. Is there a simple way to prove that an Artinian ring is perfect (in the commutative case)?,Definition. A ring is called perfect if every flat module is projective. Is there a simple way to prove that an Artinian ring is perfect (in the commutative case)?,,"['abstract-algebra', 'ring-theory', 'commutative-algebra', 'modules', 'projective-module']"
1,find a special element in group algebra,find a special element in group algebra,,"Let $G=\langle x, y, z| xyx^{-1}=zy, xzx^{-1}=z, yz=zy\rangle$, denote $l^1(G)^{\times}$ to be the set of units in $l^1(G)$, which we have considered as a ring with multiplication defined by the usual convolution, i.e., $(\sum_{g\in G}\lambda_gg)(\sum_{h\in G}\mu_hh)=\sum_{g, h\in G}\lambda_g\mu_hgh$. Can we find $l=p_1(y, z)x^{n_1}+\cdots p_k(y,z)x^{n_k}\in l^1(G)^{\times}$ such that $\sum_{i=1}^k2^{n_i}p_i(y,z)(1-z^{n_i}y)=0$? Here, $\forall~ 1\leq i\leq k, ~p_i(y,z)\in \mathbb{Z}G$ and $n_1<\cdots <n_k\in \mathbb{Z}$ to be determined. Note that the group element $x$ does not appear in $p_i(y, z)$. Remarks: This problem is related to the Ore condition. I want to show that $l$ does not exist, suppose it exists, then I have considered the natural quotient map $\phi: G\to H=G/<z^2>$. Note that it would induce a map $\phi: l^1(G)^{\times}\to l^1(H)^{\times}$, then $\phi(l)\in l^1(H)^{\times}$, but I still could not handle this..","Let $G=\langle x, y, z| xyx^{-1}=zy, xzx^{-1}=z, yz=zy\rangle$, denote $l^1(G)^{\times}$ to be the set of units in $l^1(G)$, which we have considered as a ring with multiplication defined by the usual convolution, i.e., $(\sum_{g\in G}\lambda_gg)(\sum_{h\in G}\mu_hh)=\sum_{g, h\in G}\lambda_g\mu_hgh$. Can we find $l=p_1(y, z)x^{n_1}+\cdots p_k(y,z)x^{n_k}\in l^1(G)^{\times}$ such that $\sum_{i=1}^k2^{n_i}p_i(y,z)(1-z^{n_i}y)=0$? Here, $\forall~ 1\leq i\leq k, ~p_i(y,z)\in \mathbb{Z}G$ and $n_1<\cdots <n_k\in \mathbb{Z}$ to be determined. Note that the group element $x$ does not appear in $p_i(y, z)$. Remarks: This problem is related to the Ore condition. I want to show that $l$ does not exist, suppose it exists, then I have considered the natural quotient map $\phi: G\to H=G/<z^2>$. Note that it would induce a map $\phi: l^1(G)^{\times}\to l^1(H)^{\times}$, then $\phi(l)\in l^1(H)^{\times}$, but I still could not handle this..",,"['abstract-algebra', 'analysis', 'ring-theory', 'group-rings']"
2,A doubt in the proof of Frucht's theorem,A doubt in the proof of Frucht's theorem,,"I am trying to understand the proof of Frucht's theorem which is: Every finite group is isomorphic to the automorphism group of some   simple graph. The proof (which I am reading from this book) begins as follows: Let $\Gamma=\{g_1,\cdots, g_n\}$ be a group. Consider a directed graph $\hat{G_0}$ with $V(\hat{G_0})=\Gamma$ and a directed edge between $g_i$ and $g_j$ colored $k$ where $g_ig_j^{-1}=g_k$. Next it goes on to show that $Aut(\hat{G_0})\approx\Gamma$. Next, a graph $G$ is constructed: Whenever $g_i$ has a directed edge leading into $g_j$ of color $k$, then that edge is replaced by a (non-directed) path of length $k+2$. In this $k+2$-path there are paths of length $1$ attached to each inner point except for the inner point next to $g_j$ where we attach a path of length $2$. Then the following is established: Each automorphism of $\hat{G_0}$ induces a unique automorphism of $G$. If $\alpha$ is an automorphism of $G$, then $\alpha$ is induced by some automorphism of $\hat{G_0}$. This finishes the proof. What I don't understand is as the last two points presumably only establish that $Aut(\hat{G_0})$ and $Aut(G)$ have the same cardinality. It doesn't establish that they are isomorphic as groups which is essential for the final conclusion that $Aut(G)\approx \Gamma$. Update: There is no proof of (1) provided in the book. Instead it just says that the proof is clear! I assume what is meant is this: For any automorphism $f$ of $\hat{G_0}$, we construct an automorphism of $G$ by first permuting $V(G_0)$ as per $f$, then rearranging the paths appropriately (the path joining $g_i$ and $g_j$ is send to the path joining $f(g_i)$ and $f(g_j)$. The structure of the paths is such that there is no permutation possible within a particular path.).","I am trying to understand the proof of Frucht's theorem which is: Every finite group is isomorphic to the automorphism group of some   simple graph. The proof (which I am reading from this book) begins as follows: Let $\Gamma=\{g_1,\cdots, g_n\}$ be a group. Consider a directed graph $\hat{G_0}$ with $V(\hat{G_0})=\Gamma$ and a directed edge between $g_i$ and $g_j$ colored $k$ where $g_ig_j^{-1}=g_k$. Next it goes on to show that $Aut(\hat{G_0})\approx\Gamma$. Next, a graph $G$ is constructed: Whenever $g_i$ has a directed edge leading into $g_j$ of color $k$, then that edge is replaced by a (non-directed) path of length $k+2$. In this $k+2$-path there are paths of length $1$ attached to each inner point except for the inner point next to $g_j$ where we attach a path of length $2$. Then the following is established: Each automorphism of $\hat{G_0}$ induces a unique automorphism of $G$. If $\alpha$ is an automorphism of $G$, then $\alpha$ is induced by some automorphism of $\hat{G_0}$. This finishes the proof. What I don't understand is as the last two points presumably only establish that $Aut(\hat{G_0})$ and $Aut(G)$ have the same cardinality. It doesn't establish that they are isomorphic as groups which is essential for the final conclusion that $Aut(G)\approx \Gamma$. Update: There is no proof of (1) provided in the book. Instead it just says that the proof is clear! I assume what is meant is this: For any automorphism $f$ of $\hat{G_0}$, we construct an automorphism of $G$ by first permuting $V(G_0)$ as per $f$, then rearranging the paths appropriately (the path joining $g_i$ and $g_j$ is send to the path joining $f(g_i)$ and $f(g_j)$. The structure of the paths is such that there is no permutation possible within a particular path.).",,['abstract-algebra']
3,Basis of $SL_q(2)$,Basis of,SL_q(2),"While trying to show that $SL_q(2)$ is noncocommutative, I needed to prove the following fact: Show that the set $\{a^ib^jc^k\}_{i,j,k\geq 0}\cup\{b^ic^jd^k\}_{i,j\geq 0,k>0}$ is a basis of $SL_q(2)$. I was stuck with proving the above fact. (Incidentally, the above is exercise IV.9.7 in Kassel's book on Quantum Groups.) I have tried to search for a similar proof in the book, and found that Lemma IV.4.5 is rather similar ($\{a^ib^jc^kd^l\}_{i,j,k,l\geq 0}$ is a basis of $M_q(2)$). However, the proof involves Ore extensions, which I do not really understand. Sincere thanks for any help or hint to all. Edit: For those who have Kassel's book, $a,b,c,d$ are four variables described in Chapter IV.3 (pg. 77). For those without Kassel's book, I have attached the following paraphrase of Kassel's book from my report. The Algebra $M_q(2)$ For any algebra $A$ we denote by $M_2(A)$ the algebra of $2\times 2$ matrices with entries in $A$. As a set, $M_2(A)$ is in bijection with the set $A^4$ of 4-tuples of $A$. We have a natural bijection $$Hom_{Alg}(M(2),A)\cong M_2(A)$$ for any commutative algebra $A$, where $M(2)$ is defined as the polynomial algebra $k[a,b,c,d]$. This bijection maps an algebra morphism $f:M(2)\to A$ to the matrix $$\begin{pmatrix} f(a) & f(b) \\ f(c) & f(d) \end{pmatrix}.$$ From now on, we assume that $q^2 \not= -1$. We define a $q$-analogue of the algebra $M(2)$. In addition to variables $x,y$ subject to the quantum plane relation $yx=qxy$, consider four variables $a,b,c,d$ commuting with $x$ and $y$. Define $x',y',x''$, and $y''$ using the following matrix relations $\begin{pmatrix} x' \\ y'\end{pmatrix}=\begin{pmatrix} a & b \\ c & d \end{pmatrix}\begin{pmatrix}x \\ y\end{pmatrix} \text{and} \begin{pmatrix}x'' \\ y'' \end{pmatrix}=\begin{pmatrix}a & c \\ b& d \end{pmatrix}\begin{pmatrix}x \\ y\end{pmatrix}$.","While trying to show that $SL_q(2)$ is noncocommutative, I needed to prove the following fact: Show that the set $\{a^ib^jc^k\}_{i,j,k\geq 0}\cup\{b^ic^jd^k\}_{i,j\geq 0,k>0}$ is a basis of $SL_q(2)$. I was stuck with proving the above fact. (Incidentally, the above is exercise IV.9.7 in Kassel's book on Quantum Groups.) I have tried to search for a similar proof in the book, and found that Lemma IV.4.5 is rather similar ($\{a^ib^jc^kd^l\}_{i,j,k,l\geq 0}$ is a basis of $M_q(2)$). However, the proof involves Ore extensions, which I do not really understand. Sincere thanks for any help or hint to all. Edit: For those who have Kassel's book, $a,b,c,d$ are four variables described in Chapter IV.3 (pg. 77). For those without Kassel's book, I have attached the following paraphrase of Kassel's book from my report. The Algebra $M_q(2)$ For any algebra $A$ we denote by $M_2(A)$ the algebra of $2\times 2$ matrices with entries in $A$. As a set, $M_2(A)$ is in bijection with the set $A^4$ of 4-tuples of $A$. We have a natural bijection $$Hom_{Alg}(M(2),A)\cong M_2(A)$$ for any commutative algebra $A$, where $M(2)$ is defined as the polynomial algebra $k[a,b,c,d]$. This bijection maps an algebra morphism $f:M(2)\to A$ to the matrix $$\begin{pmatrix} f(a) & f(b) \\ f(c) & f(d) \end{pmatrix}.$$ From now on, we assume that $q^2 \not= -1$. We define a $q$-analogue of the algebra $M(2)$. In addition to variables $x,y$ subject to the quantum plane relation $yx=qxy$, consider four variables $a,b,c,d$ commuting with $x$ and $y$. Define $x',y',x''$, and $y''$ using the following matrix relations $\begin{pmatrix} x' \\ y'\end{pmatrix}=\begin{pmatrix} a & b \\ c & d \end{pmatrix}\begin{pmatrix}x \\ y\end{pmatrix} \text{and} \begin{pmatrix}x'' \\ y'' \end{pmatrix}=\begin{pmatrix}a & c \\ b& d \end{pmatrix}\begin{pmatrix}x \\ y\end{pmatrix}$.",,"['abstract-algebra', 'quantum-groups', 'hopf-algebras']"
4,Groupification and perfection of a commutative monoid,Groupification and perfection of a commutative monoid,,"The following definitions and proposition are taken from the paper The geometry of Frobenioids I by S. Mochizuki. Let $\mathbb{N}_{\ge 1}$ be the set of positive integers. $\mathbb{N}_{\ge 1}$ is a directed set by the order relation $a|b$. Let $M$, $N$ be commutative monoids(the binary operations are written additively). A map $f\colon M \rightarrow N$ is called a morphism if $f(x + y) = f(x) + f(y)$ for any $x, y$ and $f(0) = 0$. For every $n \in \mathbb{N}_{\ge 1}$, the multiplication map $x \rightarrow nx$ on $M$ is a morphism. $M$ is said to be torsion-free if $nx = 0$ implies $x = 0$ for every $n \in \mathbb{N}_{\ge 1}$. Let $M^{gp}$ be the groupification of $M$, i.e. the grothendieck group of $M$. Let $\psi\colon M \rightarrow M^{gp}$ be the canonical map. If $\psi$ is injective, $M$ is said to be integral . $M$ is said to be saturated if $na \in \psi(M)$ for some $n \in \mathbb{N}_{\ge 1}$ and $a \in M^{gp}$, then $a \in \psi(M)$. $M$ is said to be perfect if the multiplication map $x \rightarrow nx$ is bijective for every $n \in \mathbb{N}_{\ge 1}$. The perfection $M^{pf}$ of $M$ is defined as follows. Let $M_a = M$ for every $a \in \mathbb{N}_{\ge 1}$. if $a|b$, we define a morphism $f_{ba}\colon M_a \rightarrow M_b$ by $f_{ba}(x) = (b/a)x$. Then $(M_a)_{a\in \mathbb{N}_{\ge 1}}$ and $(f_{ba})$ consitute a direct(or inductive) system. We define $M^{pf} = colim_{a\in \mathbb{N}_{\ge 1}} M_a$. There exists the canonical morphism $\phi\colon M = M_1 \rightarrow M^{pf}$. My question: How do we prove the following proposition? Proposition (1) $\phi\colon M \rightarrow M^{pf}$ is injective if $M$ is torsion-free, integral, and saturated. (2) $M$ is perfect if and only if $\phi$ is an isomorhism. Motivation Recently(August, 2012), S. Mochizuki submitted a series of papers( Inter-universal Teichmuller Theory I,II,III,IV ) which develops his new theory. As an application of his theory, he wrote a proof of ABC conjecture. I think the validity of the proof has not yet been confirmed by other mathematicians. However, considering his track record, I think it's worthwhile to read the papers. He referred to The geometry of Frobenioids I for the notation and terminolgy concerning monoids and categories.","The following definitions and proposition are taken from the paper The geometry of Frobenioids I by S. Mochizuki. Let $\mathbb{N}_{\ge 1}$ be the set of positive integers. $\mathbb{N}_{\ge 1}$ is a directed set by the order relation $a|b$. Let $M$, $N$ be commutative monoids(the binary operations are written additively). A map $f\colon M \rightarrow N$ is called a morphism if $f(x + y) = f(x) + f(y)$ for any $x, y$ and $f(0) = 0$. For every $n \in \mathbb{N}_{\ge 1}$, the multiplication map $x \rightarrow nx$ on $M$ is a morphism. $M$ is said to be torsion-free if $nx = 0$ implies $x = 0$ for every $n \in \mathbb{N}_{\ge 1}$. Let $M^{gp}$ be the groupification of $M$, i.e. the grothendieck group of $M$. Let $\psi\colon M \rightarrow M^{gp}$ be the canonical map. If $\psi$ is injective, $M$ is said to be integral . $M$ is said to be saturated if $na \in \psi(M)$ for some $n \in \mathbb{N}_{\ge 1}$ and $a \in M^{gp}$, then $a \in \psi(M)$. $M$ is said to be perfect if the multiplication map $x \rightarrow nx$ is bijective for every $n \in \mathbb{N}_{\ge 1}$. The perfection $M^{pf}$ of $M$ is defined as follows. Let $M_a = M$ for every $a \in \mathbb{N}_{\ge 1}$. if $a|b$, we define a morphism $f_{ba}\colon M_a \rightarrow M_b$ by $f_{ba}(x) = (b/a)x$. Then $(M_a)_{a\in \mathbb{N}_{\ge 1}}$ and $(f_{ba})$ consitute a direct(or inductive) system. We define $M^{pf} = colim_{a\in \mathbb{N}_{\ge 1}} M_a$. There exists the canonical morphism $\phi\colon M = M_1 \rightarrow M^{pf}$. My question: How do we prove the following proposition? Proposition (1) $\phi\colon M \rightarrow M^{pf}$ is injective if $M$ is torsion-free, integral, and saturated. (2) $M$ is perfect if and only if $\phi$ is an isomorhism. Motivation Recently(August, 2012), S. Mochizuki submitted a series of papers( Inter-universal Teichmuller Theory I,II,III,IV ) which develops his new theory. As an application of his theory, he wrote a proof of ABC conjecture. I think the validity of the proof has not yet been confirmed by other mathematicians. However, considering his track record, I think it's worthwhile to read the papers. He referred to The geometry of Frobenioids I for the notation and terminolgy concerning monoids and categories.",,"['abstract-algebra', 'monoid']"
5,"Extension of Homomorphisms (Lang, Atiyah and McDonald)","Extension of Homomorphisms (Lang, Atiyah and McDonald)",,"Let $A$ be a subring of a field $K$, and suppose that $A$ is a local ring with maximal ideal $\mathfrak{m}$. Let $x \in K, \, x \neq 0$. Let $\phi: A \rightarrow L$ be a homomorphism of $A$ into the algebraically closed field $L$ with kernel $\mathfrak{m}$. Suppose that $\mathfrak{m} A[x] \neq A[x]$. Then $\mathfrak{m} A[x]$ is contained into a maximal ideal $\mathfrak{P}$ of $A[x]$ and $\mathfrak{P} \cap A = \mathfrak{m}$. Now, there exists an embedding $\psi : A/ \mathfrak{m} \rightarrow L$ such that $A \rightarrow A/ \mathfrak{m} \stackrel{\psi}{\rightarrow} L$ is equal to $\phi$. Suppose that the . According to Lang's Algebra p. 348, we can extend $\psi$ to $A[x]/ \mathfrak{P}$ whether the image of $x$ in $A[x]/ \mathfrak{P}$ is transcendental over $A/ \mathfrak{m}$ or not, but i can not see why this is possible. Any insights? Thanks Edited: Related to the question is the observation that, according to the proof of Theorem 5.21 in Atiyah's and McDonald's ""Introduction to Commutative Algebra"", where they follow a similar construction, it is mentioned that the image of $x$ in $A[x]/ \mathfrak{P}$ is algebraic over $A/ \mathfrak{m}$ (notation is different). But i can't see why that's the case.","Let $A$ be a subring of a field $K$, and suppose that $A$ is a local ring with maximal ideal $\mathfrak{m}$. Let $x \in K, \, x \neq 0$. Let $\phi: A \rightarrow L$ be a homomorphism of $A$ into the algebraically closed field $L$ with kernel $\mathfrak{m}$. Suppose that $\mathfrak{m} A[x] \neq A[x]$. Then $\mathfrak{m} A[x]$ is contained into a maximal ideal $\mathfrak{P}$ of $A[x]$ and $\mathfrak{P} \cap A = \mathfrak{m}$. Now, there exists an embedding $\psi : A/ \mathfrak{m} \rightarrow L$ such that $A \rightarrow A/ \mathfrak{m} \stackrel{\psi}{\rightarrow} L$ is equal to $\phi$. Suppose that the . According to Lang's Algebra p. 348, we can extend $\psi$ to $A[x]/ \mathfrak{P}$ whether the image of $x$ in $A[x]/ \mathfrak{P}$ is transcendental over $A/ \mathfrak{m}$ or not, but i can not see why this is possible. Any insights? Thanks Edited: Related to the question is the observation that, according to the proof of Theorem 5.21 in Atiyah's and McDonald's ""Introduction to Commutative Algebra"", where they follow a similar construction, it is mentioned that the image of $x$ in $A[x]/ \mathfrak{P}$ is algebraic over $A/ \mathfrak{m}$ (notation is different). But i can't see why that's the case.",,"['abstract-algebra', 'commutative-algebra', 'field-theory']"
6,"Integer extensions, rings $\mathbb{Z}[\sqrt{s}]$","Integer extensions, rings",\mathbb{Z}[\sqrt{s}],"I'm not sure if this type of question is acceptable here, but I'd really appreciate someone's help. I'm about to start writing a semestral work that we need to achieve the Bachelor's degree in our country. But my teacher is not an expert in this particular field so he told me I'll have to prepare it all mainly by myself. :-( During the course of algebra, in part about UFD, PID and Euclidean domains, I found really interesting the structures $\mathbb{Z}[\sqrt{s}]$ where $s$ is a non-square integer. The properties of norm, irreducible and invertible elements etc. Also their classification as UFD, PID etc. is pretty interesting. My questions are: What should be the topic of my work? I barely know the topic so I don't know if it can be somehow made in one whole topic - not just some chunk of some bigger topic with the unexpected end. I also can't decide whether is it enough / too big / too small topic - there are also other extensions of integers, there are also something called fields of rational numbers which my rings seem to be a part of... What would you recommend to be the content of my work? One chapter should be that basic introduction to $\mathbb{Z}[\sqrt{s}]$ - properties of the norm, invertible elements, algorithm for finding factorization to irreducible elements etc. Another one can be about classificaton of $\mathbb{Z}[\sqrt{s}]$ as UFD, PID, etc. Some facts seem to be easy - proof that $\mathbb{Z}[\sqrt{5}]$ is not a PID, $\mathbb{Z}[i]$ is Euclidean domain... But there are also some unsolved problems so I just want to be sure that I'm not digging into something way above my level. I can also focus on some particular ring, like $\mathbb{Z}[i]$, if there is something specially interesting on it. Any other chapters come into your mind? Literature? There is almost nothing in my language about it. In english I googled some classic books like G. H. Hardy, E. M. Wright: An Introduction to the Theory of Numbers or D. S. Dummit, R. M. Foote: Abstract Algebra , both books slightly touching the topic but I would appreciate something (basic / advanced) more focused to study. I guess here are many people with much better overview of the topic so I appreciate any comment / recommendation from you.","I'm not sure if this type of question is acceptable here, but I'd really appreciate someone's help. I'm about to start writing a semestral work that we need to achieve the Bachelor's degree in our country. But my teacher is not an expert in this particular field so he told me I'll have to prepare it all mainly by myself. :-( During the course of algebra, in part about UFD, PID and Euclidean domains, I found really interesting the structures $\mathbb{Z}[\sqrt{s}]$ where $s$ is a non-square integer. The properties of norm, irreducible and invertible elements etc. Also their classification as UFD, PID etc. is pretty interesting. My questions are: What should be the topic of my work? I barely know the topic so I don't know if it can be somehow made in one whole topic - not just some chunk of some bigger topic with the unexpected end. I also can't decide whether is it enough / too big / too small topic - there are also other extensions of integers, there are also something called fields of rational numbers which my rings seem to be a part of... What would you recommend to be the content of my work? One chapter should be that basic introduction to $\mathbb{Z}[\sqrt{s}]$ - properties of the norm, invertible elements, algorithm for finding factorization to irreducible elements etc. Another one can be about classificaton of $\mathbb{Z}[\sqrt{s}]$ as UFD, PID, etc. Some facts seem to be easy - proof that $\mathbb{Z}[\sqrt{5}]$ is not a PID, $\mathbb{Z}[i]$ is Euclidean domain... But there are also some unsolved problems so I just want to be sure that I'm not digging into something way above my level. I can also focus on some particular ring, like $\mathbb{Z}[i]$, if there is something specially interesting on it. Any other chapters come into your mind? Literature? There is almost nothing in my language about it. In english I googled some classic books like G. H. Hardy, E. M. Wright: An Introduction to the Theory of Numbers or D. S. Dummit, R. M. Foote: Abstract Algebra , both books slightly touching the topic but I would appreciate something (basic / advanced) more focused to study. I guess here are many people with much better overview of the topic so I appreciate any comment / recommendation from you.",,"['abstract-algebra', 'principal-ideal-domains']"
7,Generalisation of abelianisation using representation theory?,Generalisation of abelianisation using representation theory?,,"Let $G$ be a finite group and let $k$ be an algebraically closed field of characteristic zero. Every $1$ -dimensional representation of $G$ over $k$ factors through $G^{\mathrm{ab}} = G/[G, G]$ , and every finite abelian group has a faithful representation over $k$ . Taken together, these simple facts show that one can characterise the abelianisation $G^{\mathrm{ab}} = G/[G, G]$ of $G$ as the smallest quotient of $G$ such that every $1$ -dimensional representation of $G$ factors through it. Let $G_n$ be the smallest quotient of $G$ such that every $n$ -dimensional representation of $G$ over $k$ factors through $G_n$ . Clearly $G_1 = G^{\mathrm{ab}}$ and I've read that if $G$ has a generating set of size $m$ then $G_m = G$ , i.e. $G$ has an $m$ -dimensional faithful representation. What does $G_2$ , or what do the $G_n$ look like in general? Can they be characterised in a different way? What do they tell us about $G$ ?","Let be a finite group and let be an algebraically closed field of characteristic zero. Every -dimensional representation of over factors through , and every finite abelian group has a faithful representation over . Taken together, these simple facts show that one can characterise the abelianisation of as the smallest quotient of such that every -dimensional representation of factors through it. Let be the smallest quotient of such that every -dimensional representation of over factors through . Clearly and I've read that if has a generating set of size then , i.e. has an -dimensional faithful representation. What does , or what do the look like in general? Can they be characterised in a different way? What do they tell us about ?","G k 1 G k G^{\mathrm{ab}} = G/[G, G] k G^{\mathrm{ab}} = G/[G, G] G G 1 G G_n G n G k G_n G_1 = G^{\mathrm{ab}} G m G_m = G G m G_2 G_n G","['abstract-algebra', 'group-theory', 'representation-theory']"
8,Prove that $\Phi_{420}(69) > \Phi_{69}(420)$,Prove that,\Phi_{420}(69) > \Phi_{69}(420),"Let $\Phi_n(x)$ denote the nth cyclotomic polynomial. Prove that $\Phi_{420}(69) > \Phi_{69}(420)$ . Observe that if $\phi$ denotes the Euler-phi function, \begin{align} \phi(420) &= (2^2-2) \cdot (5-1) \cdot (3-1)\cdot (7-1) = 96, \\  \phi(69) &= (3-1) \cdot (23-1) = 44.  \end{align} We want to show that $$ (69^{420} - 1) \hspace{-1em} \prod_{\substack{0\leq k < 69 \\ \gcd(k,69) > 1}}  \hspace{-1em} (420-e^{2 i \pi k/69})  \; > \; (420^{69} - 1) \hspace{-1em} \prod_{\substack{0\leq k<420, \\ \gcd(k,420) > 1}}  \hspace{-1em} (69-e^{2 i \pi k/420}). $$ But both sides of the inequality we need to show look very complicated, and I'm not sure how to simplify them. The product $$ \prod_{\substack{0\leq k<420, \\ \gcd(k,420) > 1}}  \hspace{-1em} (e^{2 i \pi k/420}) $$ includes $210$ th, $84$ th, $140$ th, and $60$ th roots of unity. Note that an $a$ th root of unity that is also a $b$ th root of unity is a $\gcd(a,b)$ th root of unity, since $\gcd(a,b)$ is a linear combination of a and b.","Let denote the nth cyclotomic polynomial. Prove that . Observe that if denotes the Euler-phi function, We want to show that But both sides of the inequality we need to show look very complicated, and I'm not sure how to simplify them. The product includes th, th, th, and th roots of unity. Note that an th root of unity that is also a th root of unity is a th root of unity, since is a linear combination of a and b.","\Phi_n(x) \Phi_{420}(69) > \Phi_{69}(420) \phi \begin{align}
\phi(420) &= (2^2-2) \cdot (5-1) \cdot (3-1)\cdot (7-1) = 96, \\ 
\phi(69) &= (3-1) \cdot (23-1) = 44. 
\end{align} 
(69^{420} - 1) \hspace{-1em}
\prod_{\substack{0\leq k < 69 \\ \gcd(k,69) > 1}} 
\hspace{-1em} (420-e^{2 i \pi k/69}) 
\; > \; (420^{69} - 1) \hspace{-1em}
\prod_{\substack{0\leq k<420, \\ \gcd(k,420) > 1}} 
\hspace{-1em} (69-e^{2 i \pi k/420}).
 
\prod_{\substack{0\leq k<420, \\ \gcd(k,420) > 1}} 
\hspace{-1em} (e^{2 i \pi k/420})
 210 84 140 60 a b \gcd(a,b) \gcd(a,b)","['abstract-algebra', 'elementary-number-theory', 'inequality', 'polynomials', 'contest-math']"
9,Understanding the construction of the split extension of group,Understanding the construction of the split extension of group,,"This is the 2nd part of another question, mainly general extension . Please have a look to understand the notation. A brief description was copied from that thread, Let $\phi$ be an isomorphism of $G/H$ onto $K$ . Let $X$ be a left transversal of $H$ in $G$ . If $g\in G, g=xh$ for some $x\in X,h\in H$ Then $$gx\in \text{some coset }yH\implies gx=yh=ym_{g,x}\tag1$$ $(gH)\phi=k\in K$ , let $x_k\in X$ be the representative of $gH$ . $X=\{x_k:k\in K\}$ and $x_1=1$ . Now, $(x_kx_{k'}H)\phi=kk'$ and using $(1)$ , $$x_kx_{k'}=x_{kk'}m_{x_k x_{k'}}=x_{kk'}m_{kk'}\quad[\text{shorthand notation}]$$ Consider the split extension where $m_{x,x'}=1$ for all $x,x'\in X$ in $(1)\implies xx'=x''$ . Question: Can we always guarantee the existence of $m_{x,x'}=1$ ? Then $X<G,H\triangleleft G$ and $G=XH$ . $G$ in an extension of $H$ by $X$ . Therefore we may suspect that if we are given, a group $H$ , a group $K$ , a homomorphism $\alpha$ of $K$ into the automorphism group of $H$ then we can create a splitting extension of $H$ by $K$ , $$ \begin{align} G=K\times H=&\{(k,h):k\in K,h\in H\}\\ (k,h)(k',h')=&(kk',h(k'\alpha)h')\\ (k,h)^{-1}=&(k^{-1},h^{-1}(k^{-1}\alpha)) \end{align} $$ Question: How they come up with this multiplication and the inverse? Lastly, I couldn't catch the whole story, it seems like the construction isn't intuitive enough. And I couldn't get the similarity with the Wikipedia definition , A split extension is an extension $1\to K\to G\to H\to 1$ with a homomorphism $s\colon H\to G$ such that going from $H$ to $G$ by $s$ and then back to $H$ by the quotient map of the short exact sequence induces the identity map on $H$ i.e., $\pi \circ s={\mathrm  {id}}_{H}$ . In this situation, it is usually said that $s$ splits the above exact sequence.","This is the 2nd part of another question, mainly general extension . Please have a look to understand the notation. A brief description was copied from that thread, Let be an isomorphism of onto . Let be a left transversal of in . If for some Then , let be the representative of . and . Now, and using , Consider the split extension where for all in . Question: Can we always guarantee the existence of ? Then and . in an extension of by . Therefore we may suspect that if we are given, a group , a group , a homomorphism of into the automorphism group of then we can create a splitting extension of by , Question: How they come up with this multiplication and the inverse? Lastly, I couldn't catch the whole story, it seems like the construction isn't intuitive enough. And I couldn't get the similarity with the Wikipedia definition , A split extension is an extension with a homomorphism such that going from to by and then back to by the quotient map of the short exact sequence induces the identity map on i.e., . In this situation, it is usually said that splits the above exact sequence.","\phi G/H K X H G g\in G, g=xh x\in X,h\in H gx\in \text{some coset }yH\implies gx=yh=ym_{g,x}\tag1 (gH)\phi=k\in K x_k\in X gH X=\{x_k:k\in K\} x_1=1 (x_kx_{k'}H)\phi=kk' (1) x_kx_{k'}=x_{kk'}m_{x_k x_{k'}}=x_{kk'}m_{kk'}\quad[\text{shorthand notation}] m_{x,x'}=1 x,x'\in X (1)\implies xx'=x'' m_{x,x'}=1 X<G,H\triangleleft G G=XH G H X H K \alpha K H H K 
\begin{align}
G=K\times H=&\{(k,h):k\in K,h\in H\}\\
(k,h)(k',h')=&(kk',h(k'\alpha)h')\\
(k,h)^{-1}=&(k^{-1},h^{-1}(k^{-1}\alpha))
\end{align}
 1\to K\to G\to H\to 1 s\colon H\to G H G s H H \pi \circ s={\mathrm  {id}}_{H} s","['abstract-algebra', 'group-theory', 'reference-request', 'soft-question', 'group-extensions']"
10,Extensions of Height-Preserving Isomorphisms in Ulm's Theorem,Extensions of Height-Preserving Isomorphisms in Ulm's Theorem,,"I am working through the proof of Ulm's Theorem in Kaplansky's book ``Infinite Abelian Groups."" Ulm's theorem states that two countable reduced abelian $p$ -groups $G,H$ with the same Ulm invariants are isomorphic. Kaplansky's particular proof of Ulm's theorem is important because it is often used, word-for-word, to prove similar classification theorems for other kinds of structures. The key step in this proof is to extend a given height-preserving isomorphism to another height-preserving isomorphism. The part I'm confused about has nothing to do with the Ulm invariants or the fact that these groups are reduced, so please excuse me if I don't define these notions. However, central to the proof is the notion of height. We can define, inductively, $G_0 = G$ , $G_{\alpha + 1} = pG_{\alpha}$ , and $G_{\alpha}$ for limit $\alpha$ is defined in the obvious way by taking intersections. For $x \in G$ , we define the height of $x$ , denoted $h(x)$ , to be $\infty$ if $x=0$ and $\alpha$ if $x \in G_{\alpha}$ but not $G_{\alpha + 1}$ . It can be easily shown that if $h(x) \neq h(y)$ then $h(x+y) =$ min $(h(x),h(y))$ and  if $h(x) = h(y)$ then $h(x+y) \geq h(x)$ . The proof proceeds in stages. At stage $n$ , we start with a height-preserving isomorphism $V:S \rightarrow T$ between finite subgroups $S,T$ of $G,H$ respectively. We then take an arbitrary element $x$ of $G$ and extend $V$ to include $x$ in its domain. We are allowed to assume that i) $x \not \in S$ but $px \in S$ , ii) $x$ is proper with respect to $S$ , meaning that $h(x) \geq h(x+s)$ for all $s \in S$ , and iii) $h(px) \geq h(p(x+s))$ for all $s \in S$ . We then find an element $w \in H$ such that $w \not \in T$ , $pw = V(px)$ , $h(w) = h(x)$ , and $w$ is proper with respect to $T$ . Extending $V$ in the obvious way we get a map $V': \langle S,x \rangle \rightarrow \langle T,w \rangle$ , which should still be an isomorphism and height-preserving. My question is: Why is $V'$ height-preserving? Note that all heights are supposed to be calculated with respect to the larger groups $G,H$ . Now, an arbitrary element of $\langle S,x \rangle$ is of the form $s \pm nx$ . Since for any $x$ , $h(x) = h(-x)$ , it suffices to show that $V'$ preserves the height of an element of the form $s+nx$ . Moreover since $px \in S$ we may assume $n < p$ . That is, we want to show that $$ h(nx+s) = h(nw + V(s)).$$ So far, I have only been able to show that $h(x+s) = h(w+V(s))$ , which follows since $h(x+s) =$ min $(h(x), h(s))$ by the fact that $x$ is proper with respect to $S$ . There is also a proof of Ulm's theorem in Fuchs' ``Infinite Abelian Groups"", which seems to claim that this fact is enough to conclude that $V'$ is height-preserving. I don't understand this at all; there doesn't seem to be any clear connection between $h(nx+s)$ and $h(x+s)$ .  Even ignoring this advice, I am pretty stuck since height does not seem to be well-behaved with respect to addition.","I am working through the proof of Ulm's Theorem in Kaplansky's book ``Infinite Abelian Groups."" Ulm's theorem states that two countable reduced abelian -groups with the same Ulm invariants are isomorphic. Kaplansky's particular proof of Ulm's theorem is important because it is often used, word-for-word, to prove similar classification theorems for other kinds of structures. The key step in this proof is to extend a given height-preserving isomorphism to another height-preserving isomorphism. The part I'm confused about has nothing to do with the Ulm invariants or the fact that these groups are reduced, so please excuse me if I don't define these notions. However, central to the proof is the notion of height. We can define, inductively, , , and for limit is defined in the obvious way by taking intersections. For , we define the height of , denoted , to be if and if but not . It can be easily shown that if then min and  if then . The proof proceeds in stages. At stage , we start with a height-preserving isomorphism between finite subgroups of respectively. We then take an arbitrary element of and extend to include in its domain. We are allowed to assume that i) but , ii) is proper with respect to , meaning that for all , and iii) for all . We then find an element such that , , , and is proper with respect to . Extending in the obvious way we get a map , which should still be an isomorphism and height-preserving. My question is: Why is height-preserving? Note that all heights are supposed to be calculated with respect to the larger groups . Now, an arbitrary element of is of the form . Since for any , , it suffices to show that preserves the height of an element of the form . Moreover since we may assume . That is, we want to show that So far, I have only been able to show that , which follows since min by the fact that is proper with respect to . There is also a proof of Ulm's theorem in Fuchs' ``Infinite Abelian Groups"", which seems to claim that this fact is enough to conclude that is height-preserving. I don't understand this at all; there doesn't seem to be any clear connection between and .  Even ignoring this advice, I am pretty stuck since height does not seem to be well-behaved with respect to addition.","p G,H G_0 = G G_{\alpha + 1} = pG_{\alpha} G_{\alpha} \alpha x \in G x h(x) \infty x=0 \alpha x \in G_{\alpha} G_{\alpha + 1} h(x) \neq h(y) h(x+y) = (h(x),h(y)) h(x) = h(y) h(x+y) \geq h(x) n V:S \rightarrow T S,T G,H x G V x x \not \in S px \in S x S h(x) \geq h(x+s) s \in S h(px) \geq h(p(x+s)) s \in S w \in H w \not \in T pw = V(px) h(w) = h(x) w T V V': \langle S,x \rangle \rightarrow \langle T,w \rangle V' G,H \langle S,x \rangle s \pm nx x h(x) = h(-x) V' s+nx px \in S n < p  h(nx+s) = h(nw + V(s)). h(x+s) = h(w+V(s)) h(x+s) = (h(x), h(s)) x S V' h(nx+s) h(x+s)","['abstract-algebra', 'group-theory', 'abelian-groups']"
11,Square roots of finite abelian groups,Square roots of finite abelian groups,,"For a finite cyclic group $\mathbb{Z}/n\mathbb{Z}$ we can define an epimorphism $$\pi: \mathbb{Z}/\text{gcd}(2,n)n\mathbb{Z} \to \mathbb{Z}/n\mathbb{Z}, \quad [x] \mapsto \frac{2}{\text{gcd}(2,n)} [x].$$ I would think of $\sqrt{\mathbb{Z}/n\mathbb{Z}}=\mathbb{Z}/\text{gcd}(2,n)n\mathbb{Z}$ as the group of all squareroots of $\mathbb{Z}/n\mathbb{Z}$ (similar to a branch covering). My questions are: For a general finite abelian group $G = \bigoplus_{i =1}^r\mathbb{Z}/n_i\mathbb{Z}$ we could define $\sqrt{G}$ factorwise. I don't think that this depends on the factor decomposition, but I am not sure. Does it? If the above is true, is there a more abstract characterization of $\sqrt{G}$ (maybe in terms of a universal property) If all of the above is true, this group probably has a name. What is it? EDIT: I think that $G \xrightarrow{\iota} \sqrt{G}\xrightarrow{\pi} G$ is the unique (up to isomorphism) mono-epi factorization of the square map $G \xrightarrow{2} G$ such that $G = \operatorname{ker} p\circ\pi$ . Here $p:G \to G/2G$ is the canonical quotient map. In particular $2\sqrt{G}=G$ .","For a finite cyclic group we can define an epimorphism I would think of as the group of all squareroots of (similar to a branch covering). My questions are: For a general finite abelian group we could define factorwise. I don't think that this depends on the factor decomposition, but I am not sure. Does it? If the above is true, is there a more abstract characterization of (maybe in terms of a universal property) If all of the above is true, this group probably has a name. What is it? EDIT: I think that is the unique (up to isomorphism) mono-epi factorization of the square map such that . Here is the canonical quotient map. In particular .","\mathbb{Z}/n\mathbb{Z} \pi: \mathbb{Z}/\text{gcd}(2,n)n\mathbb{Z} \to \mathbb{Z}/n\mathbb{Z}, \quad [x] \mapsto \frac{2}{\text{gcd}(2,n)} [x]. \sqrt{\mathbb{Z}/n\mathbb{Z}}=\mathbb{Z}/\text{gcd}(2,n)n\mathbb{Z} \mathbb{Z}/n\mathbb{Z} G = \bigoplus_{i =1}^r\mathbb{Z}/n_i\mathbb{Z} \sqrt{G} \sqrt{G} G \xrightarrow{\iota} \sqrt{G}\xrightarrow{\pi} G G \xrightarrow{2} G G = \operatorname{ker} p\circ\pi p:G \to G/2G 2\sqrt{G}=G","['abstract-algebra', 'group-theory', 'abelian-groups']"
12,Why does Galois theory most naturally take place in the context of fields?,Why does Galois theory most naturally take place in the context of fields?,,"At least as far as I can tell, historically Galois theory was a more computational tool than it appears now, and https://hsm.stackexchange.com/questions/8099/how-did-the-modern-understanding-of-galois-theory-come-about , How Did Galois Understand the Galois Group? and Intuition behind looking at permutations of the roots in Galois theory seem to say that the ""fundamental idea of Galois theory"" is to study permutations of the roots, which was first done via symmetric polynomials. The often presented motivation of Galois theory, the insolubility of the quintic, can also be presented without ""modern Galois theory"", as in this wonderful video on Arnold's proof: https://www.youtube.com/watch?v=BSHv9Elk1MU&ab_channel=notallwrong . This video also uses the ""fundamental idea of Galois theory"", i.e. studying permutations of the roots, but this time in the context of basic Riemann surfaces. My question is this: going back to the late 1800's and early 1900's, how would I realize that the ""correct"" setting of Galois theory is to study field extensions containing the roots, and automorphisms of fields? One idea I had was that from one of the links above : Let $A=\{a_1,...,a_n\}$ be the (distinct) roots of a polynomial $f$ with coefficients in a base field $k$ . Then a permutation $\pi$ of the set $A$ is in the Galois group of $f$ (over $k$ ) if (and only if):  for every polynomial $g$ in $R=k[x_1,...x_n]$ , $g(a_1,....,a_n)=0 \iff g(\pi(a_1),...,\pi(a_n))=0$ . and hence we are really looking for permutations ""that algebra can't see"", i.e. the heart of Galois theory is really the observation that there are some permutations of the symbols denoting roots of polynomials that algebra/polynomials are completely unable to distinguish (e.g. algebra can't distinguish between $i$ and $-i$ , only our geometric embedding into the plane can). Thus we are not able to permute all the roots willy-nilly (""they are not free"")... however, if we linearize and look at the vector space (over the base field $k$ ) containing all these roots (actually we should look at it as a $k$ -algebra because only then can we talk about multiplicative correlations between the roots), then we can try to find a basis for this vector space (""a free set"") which allows us to permute willy-nilly. In this perspective, the fact that this $k$ -algebra turns out to be a field is sort of irrelevant, I think?","At least as far as I can tell, historically Galois theory was a more computational tool than it appears now, and https://hsm.stackexchange.com/questions/8099/how-did-the-modern-understanding-of-galois-theory-come-about , How Did Galois Understand the Galois Group? and Intuition behind looking at permutations of the roots in Galois theory seem to say that the ""fundamental idea of Galois theory"" is to study permutations of the roots, which was first done via symmetric polynomials. The often presented motivation of Galois theory, the insolubility of the quintic, can also be presented without ""modern Galois theory"", as in this wonderful video on Arnold's proof: https://www.youtube.com/watch?v=BSHv9Elk1MU&ab_channel=notallwrong . This video also uses the ""fundamental idea of Galois theory"", i.e. studying permutations of the roots, but this time in the context of basic Riemann surfaces. My question is this: going back to the late 1800's and early 1900's, how would I realize that the ""correct"" setting of Galois theory is to study field extensions containing the roots, and automorphisms of fields? One idea I had was that from one of the links above : Let be the (distinct) roots of a polynomial with coefficients in a base field . Then a permutation of the set is in the Galois group of (over ) if (and only if):  for every polynomial in , . and hence we are really looking for permutations ""that algebra can't see"", i.e. the heart of Galois theory is really the observation that there are some permutations of the symbols denoting roots of polynomials that algebra/polynomials are completely unable to distinguish (e.g. algebra can't distinguish between and , only our geometric embedding into the plane can). Thus we are not able to permute all the roots willy-nilly (""they are not free"")... however, if we linearize and look at the vector space (over the base field ) containing all these roots (actually we should look at it as a -algebra because only then can we talk about multiplicative correlations between the roots), then we can try to find a basis for this vector space (""a free set"") which allows us to permute willy-nilly. In this perspective, the fact that this -algebra turns out to be a field is sort of irrelevant, I think?","A=\{a_1,...,a_n\} f k \pi A f k g R=k[x_1,...x_n] g(a_1,....,a_n)=0 \iff g(\pi(a_1),...,\pi(a_n))=0 i -i k k k","['abstract-algebra', 'field-theory', 'galois-theory', 'big-picture']"
13,Prove that $\mathbb{Z} \times\Bbb Z_2$ and $\mathbb{Z}$ are not isomorphic.,Prove that  and  are not isomorphic.,\mathbb{Z} \times\Bbb Z_2 \mathbb{Z},"Prove that $\mathbb{Z} \times\Bbb Z_2$ and $\mathbb{Z}$ are not isomorphic. Here by $\mathbb{Z}$ I mean the group $(\mathbb{Z},+)$ and by $\Bbb Z_2$ I mean the cyclic group of order 2. This is exercise 2.3.13 part (a) from Dummit/Foote, Abstract Algebra - I am aware the solution may be posted somewhere online, but I try to look at full solutions as little as possible for my own understanding. Here is my general argument. Is this a correct approach, and is it missing any important details? Let $\Bbb Z_2=\{0,1\}$ . The element $(0,1)$ in $\mathbb{Z} \times\Bbb Z_2$ has order 2, and since isomorphism preserves order of elements and there exists no integer with order 2, there cannot be an isomorphism between the groups.","Prove that and are not isomorphic. Here by I mean the group and by I mean the cyclic group of order 2. This is exercise 2.3.13 part (a) from Dummit/Foote, Abstract Algebra - I am aware the solution may be posted somewhere online, but I try to look at full solutions as little as possible for my own understanding. Here is my general argument. Is this a correct approach, and is it missing any important details? Let . The element in has order 2, and since isomorphism preserves order of elements and there exists no integer with order 2, there cannot be an isomorphism between the groups.","\mathbb{Z} \times\Bbb Z_2 \mathbb{Z} \mathbb{Z} (\mathbb{Z},+) \Bbb Z_2 \Bbb Z_2=\{0,1\} (0,1) \mathbb{Z} \times\Bbb Z_2","['abstract-algebra', 'group-theory', 'solution-verification', 'cyclic-groups', 'group-isomorphism']"
14,Is theory of equations a dead field today?,Is theory of equations a dead field today?,,"Is theory of equations a dead field today? By theory of equations I mean, specially, the study of polynomials and solving algebraic equations through radicals. There seem to be very few journals on such subjects. Also most of the great results are at least 100 years old, there are not many new results out there. Any journals on that topics would be appreciated as well. Thanks","Is theory of equations a dead field today? By theory of equations I mean, specially, the study of polynomials and solving algebraic equations through radicals. There seem to be very few journals on such subjects. Also most of the great results are at least 100 years old, there are not many new results out there. Any journals on that topics would be appreciated as well. Thanks",,"['abstract-algebra', 'polynomials', 'soft-question', 'galois-theory']"
15,"One of the Sylow $p$-subgroups of $G$ must be normal, for some prime $p $ dividing $|G|=870$.","One of the Sylow -subgroups of  must be normal, for some prime  dividing .",p G p  |G|=870,"Let $G$ be a group of order $870 = 2 \cdot 3 \cdot 5 \cdot 29$ . Show that at least one of the Sylow $p$ -subgroups of $G$ must be normal, for some prime $p $ dividing $|G|$ . The number of Sylow $29$ -subgroups is of the form $1 +29k$ , with $k \in \mathbb{Z}$ , and divides $30$ . Thus the number of Sylow $29$ -subgroups is  either $1$ or $30$ . If there is only one Sylow $29$ -subgroup, then we are done. Thus, let us assume that there are $30$ of them. Now, these Sylow $29$ -subgroups have prime order, so they intersect trivially, and provide us with $30\cdot 28=840$ elements of order $29$ . Symilarly, there are at least $6$ Sylow $5$ -subgroups (If we assume that it is not unique) and thus there are $6\cdot 4=24$ elements of order $5$ in $G$ . Repeating the same procedure, there are at least $10$ Sylow $3$ -subgroups (If we assume that it is not unique) and thus there are $10\cdot 2=20$ elements of order $3$ in $G$ . Finally,  there are at least $3$ Sylow $2$ -subgroups (If we assume that it is not unique) and thus there are $3\cdot 1=3$ elements of order $2$ in $G$ . So We have counted (adding the identity element) $840+ 24+20+3+1>870$ elements, wich is impossible. Thus at least one of the Sylow $p$ -subgroups of $G$ must be unique and hence normal, for some prime $p $ dividing $|G|$ . Is what I have done correct?","Let be a group of order . Show that at least one of the Sylow -subgroups of must be normal, for some prime dividing . The number of Sylow -subgroups is of the form , with , and divides . Thus the number of Sylow -subgroups is  either or . If there is only one Sylow -subgroup, then we are done. Thus, let us assume that there are of them. Now, these Sylow -subgroups have prime order, so they intersect trivially, and provide us with elements of order . Symilarly, there are at least Sylow -subgroups (If we assume that it is not unique) and thus there are elements of order in . Repeating the same procedure, there are at least Sylow -subgroups (If we assume that it is not unique) and thus there are elements of order in . Finally,  there are at least Sylow -subgroups (If we assume that it is not unique) and thus there are elements of order in . So We have counted (adding the identity element) elements, wich is impossible. Thus at least one of the Sylow -subgroups of must be unique and hence normal, for some prime dividing . Is what I have done correct?",G 870 = 2 \cdot 3 \cdot 5 \cdot 29 p G p  |G| 29 1 +29k k \in \mathbb{Z} 30 29 1 30 29 30 29 30\cdot 28=840 29 6 5 6\cdot 4=24 5 G 10 3 10\cdot 2=20 3 G 3 2 3\cdot 1=3 2 G 840+ 24+20+3+1>870 p G p  |G|,"['abstract-algebra', 'group-theory', 'solution-verification', 'normal-subgroups', 'sylow-theory']"
16,Are pointwise finitely divisible subgroups of $\mathbb Q^n$ finitely generated?,Are pointwise finitely divisible subgroups of  finitely generated?,\mathbb Q^n,"I have a question regarding additive subgroups of $\mathbb Q^n$ . Suppose we have such a guy $M$ , and suppose we know it is pointwise finitely divisible. That is, for every $m \in M$ , there are only finitely many integers $n$ such that $m/n \in M$ . Does it follow that $M$ is finitely generated? Some thoughts: It is well-known that a subgroup of $\mathbb Q^n$ is finitely generated if and only if it is discrete, i.e. the denominators are ""globally bounded"", whereas the given property only says each element has bounded denominators. Thus it seems weaker, but I haven't been able to find an example of an $M$ with this property and which is not finitely generated. If $n = 1$ , the statement is true: we may assume that $1 \in M$ , and one can then show that $1/N = \text{min}\{m \in M \ | \ nm = 1 \text{ for some }n \geq 1\}$ generates $M$ . By the above point, an equivalent way to state the question is this: suppose $M \subset \mathbb Q^n$ is a subgroup whose intersection with any line through the origin is finitely generated. Does it follow that $M$ is finitely generated?","I have a question regarding additive subgroups of . Suppose we have such a guy , and suppose we know it is pointwise finitely divisible. That is, for every , there are only finitely many integers such that . Does it follow that is finitely generated? Some thoughts: It is well-known that a subgroup of is finitely generated if and only if it is discrete, i.e. the denominators are ""globally bounded"", whereas the given property only says each element has bounded denominators. Thus it seems weaker, but I haven't been able to find an example of an with this property and which is not finitely generated. If , the statement is true: we may assume that , and one can then show that generates . By the above point, an equivalent way to state the question is this: suppose is a subgroup whose intersection with any line through the origin is finitely generated. Does it follow that is finitely generated?",\mathbb Q^n M m \in M n m/n \in M M \mathbb Q^n M n = 1 1 \in M 1/N = \text{min}\{m \in M \ | \ nm = 1 \text{ for some }n \geq 1\} M M \subset \mathbb Q^n M,"['abstract-algebra', 'abelian-groups', 'integer-lattices', 'finitely-generated']"
17,Existence of primitive permutation group one of whose arc stabilisers is normal in the point stabiliser,Existence of primitive permutation group one of whose arc stabilisers is normal in the point stabiliser,,"Let $G$ be a primitive permutation group with point stabiliser $G_\alpha$ for some $\alpha$ . For $\beta\ne\alpha$ , by an arc stabiliser we mean $G_{\alpha\beta}=G_\alpha\cap G_\beta$ and an edge stabiliser we mean $G_{\{\alpha,\beta\}}$ , the stabiliser of the set $\{\alpha,\beta\}$ . Note that $G_{\{\alpha,\beta\}}\ge G_{\alpha\beta}$ . My question is: can an arc stabiliser $G_{\alpha\beta}\ne 1$ be normal in $G_\alpha$ ? This is impossible if $G_{\{\alpha,\beta\}}> G_{\alpha\beta}$ . In this case, there exists $g\in G$ such that $\alpha^g=\beta$ and $\beta^g=\alpha$ . It follows that $g\in N_G(G_{\alpha\beta})$ . Note that $G_\alpha$ is maximal in $G$ and $G_{\alpha\beta}$ cannot be normal in $G$ (otherwise $G_{\alpha\beta}$ will be in the kernel of this action), the normaliser $N_G(G_{\alpha\beta})=G_\alpha$ . This gives a contradiction: $g\in G_\alpha$ but $\alpha^g=\beta\ne \alpha$ . I think there might exist an example in the case when $G_{\{\alpha,\beta\}}=G_{\alpha\beta}$ . However, by a quick check with Magma there is no such primitive permutation group $G$ with $|G|\le 300$ . Added: I asked the same question here in MO one day after this was asked.","Let be a primitive permutation group with point stabiliser for some . For , by an arc stabiliser we mean and an edge stabiliser we mean , the stabiliser of the set . Note that . My question is: can an arc stabiliser be normal in ? This is impossible if . In this case, there exists such that and . It follows that . Note that is maximal in and cannot be normal in (otherwise will be in the kernel of this action), the normaliser . This gives a contradiction: but . I think there might exist an example in the case when . However, by a quick check with Magma there is no such primitive permutation group with . Added: I asked the same question here in MO one day after this was asked.","G G_\alpha \alpha \beta\ne\alpha G_{\alpha\beta}=G_\alpha\cap G_\beta G_{\{\alpha,\beta\}} \{\alpha,\beta\} G_{\{\alpha,\beta\}}\ge G_{\alpha\beta} G_{\alpha\beta}\ne 1 G_\alpha G_{\{\alpha,\beta\}}> G_{\alpha\beta} g\in G \alpha^g=\beta \beta^g=\alpha g\in N_G(G_{\alpha\beta}) G_\alpha G G_{\alpha\beta} G G_{\alpha\beta} N_G(G_{\alpha\beta})=G_\alpha g\in G_\alpha \alpha^g=\beta\ne \alpha G_{\{\alpha,\beta\}}=G_{\alpha\beta} G |G|\le 300","['abstract-algebra', 'group-theory', 'permutations', 'finite-groups']"
18,A generalizaton of nilpotent groups,A generalizaton of nilpotent groups,,"Let  $G$ be a group and  $\alpha\in Aut(G)$ be a fixed automorphism of  $G$ . An  $\alpha$ -commutator of elements  $x, y\in G$ is  $[x, y]_{\alpha}= x^{-1}y^{-1}xy^{\alpha}$ . The  $\alpha$ -center subgroup of  $G$ , denoted by  $Z^{\alpha}(G)$ is defined as  $Z^{\alpha}(G)= \{x\in G : [y, x]_{\alpha}= 1, \forall y\in G \}$ .  If  $N$ is a normal subgroup of  $G$ which is invariant under  $\alpha$ and  $\bar{\alpha}$ is an automorphism of quotient group  $G/N$ such that  send an element  $gN$ to  $g^{\alpha}N$ , then the following normal series $$ \{ 1\}= G_{0}\unlhd G_{1}\unlhd \dots \unlhd G_{n}= G, $$ is called a central  $\alpha$ -series whenever  $G_{i}^{\alpha}= G_{i}$ and  $G_{i+1}/G_{i}\leq Z^{\bar{\alpha}}(G/G_{i})$ , for  $0\leq i\leq n-1$ . An  $\alpha$ -nilpotent group is a group which possesses  at least a central  $\alpha$ -series. It is to see that f $\alpha$ is an inner automorphism of a nilpotent group $G$ , then $G$ is an $\alpha$ -nilpotent group. My Question is: Is there any non-inner automorphim $\alpha$ of a finite non-abelian $p$ -group $P$ , such that $P$ is $\alpha$ -nilpotent?","Let  be a group and  be a fixed automorphism of  . An  -commutator of elements  is  . The  -center subgroup of  , denoted by  is defined as  .  If  is a normal subgroup of  which is invariant under  and  is an automorphism of quotient group  such that  send an element  to  , then the following normal series is called a central  -series whenever  and  , for  . An  -nilpotent group is a group which possesses  at least a central  -series. It is to see that f is an inner automorphism of a nilpotent group , then is an -nilpotent group. My Question is: Is there any non-inner automorphim of a finite non-abelian -group , such that is -nilpotent?","G \alpha\in Aut(G) G \alpha x, y\in G [x, y]_{\alpha}= x^{-1}y^{-1}xy^{\alpha} \alpha G Z^{\alpha}(G) Z^{\alpha}(G)= \{x\in G : [y, x]_{\alpha}= 1, \forall y\in G \} N G \alpha \bar{\alpha} G/N gN g^{\alpha}N 
\{ 1\}= G_{0}\unlhd G_{1}\unlhd \dots \unlhd G_{n}= G,
 \alpha G_{i}^{\alpha}= G_{i} G_{i+1}/G_{i}\leq Z^{\bar{\alpha}}(G/G_{i}) 0\leq i\leq n-1 \alpha \alpha \alpha G G \alpha \alpha p P P \alpha","['abstract-algebra', 'group-theory', 'automorphism-group', 'p-groups', 'nilpotent-groups']"
19,10 equivalent definitions of normal subgroup,10 equivalent definitions of normal subgroup,,"I've found various equivalent definitions of normal subgroup from this Wikipedia page . I've just finished proving their equivalence. Could you please verify if it is fine or contains logical mistakes? Let $N$ be a subgroup of $G$ . Then the following statements are equivalent. a. For all $g,h \in G$ : $gh \in N \iff hg \in N$ . b. For all $g \in G$ : $gNg^{-1} \subseteq N$ . c. For all $g \in G$ : $gNg^{-1} = N$ . d. The sets of left and right cosets of $N$ in $G$ coincide. e. For all $x,y,g,h \in G$ : $x \in gN$ and $y \in hN \implies xy \in (gh)N$ . f. For all $n \in N, g \in G$ : $n^{-1} g^{-1} n g \in N$ . g. For all $n \in N, g \in G$ : $g n g^{-1} \in N$ . h. $N = \bigcup_{n \in N} \operatorname{Cl}(n)$ where $\operatorname{Cl}(n) := \{gng^{-1} \mid g \in G\}$ . i. For all $g \in G$ : $gN = Ng$ . j. There exists a group homomorphism whose domain is $G$ and kernel is $N$ . My attempt: To make it easier to follow, I put each part of the proof between two consecutive definitions. For all $g,h \in G$ : $gh \in N \iff hg \in N$ . Assume the contrary that there exist $n\in N$ and $g \in G$ such that $gng^{-1} \notin N$ . Then $g^{-1} (gn) = n\notin N$ by (a). This is a contradiction. For all $g \in G$ : $gNg^{-1} \subseteq N$ . Substituting $g$ for $g^{-1}$ in $gNg^{-1} \subseteq N$ , we get $g^{-1}  N g\subseteq N$ . Substituting $N$ for $g^{-1} N g$ in $gNg^{-1} \subseteq N$ , we get $N \subseteq g^{-1} N g$ . As a result, $g^{-1} N g = N$ . Substituting $g$ for $g^{-1}$ in $g^{-1} N g = N$ , we get the desired result. For all $g \in G$ : $gNg^{-1} = N$ . It follows from (c) that $gN= Ng$ . The result then follows. The sets of left and right cosets of $N$ in $G$ coincide. Because of (d) and the fact that $h \in hN \cap Nh$ , we have $hN=Nh$ . It follows from $x \in gN$ and $y \in hN$ that $x = gn_1$ and $y = hn_2$ for some $n_1,n_2 \in N$ . Then $xy = gn_1 hn_2$ . Because $Nh = hN$ , $n_1 h = h n_3$ for some $n_3 \in N$ . Then $xy = g h n_3 n_2 = (gh) (n_3 n_2) \in (gh) N$ . For all $x,y,g,h \in G$ : $x \in gN$ and $y \in hN \implies xy \in (gh)N$ . We have $n^{-1} g^{-1} n \in n^{-1} g^{-1} N$ and $g \in gN$ . Then by (e), we have $n^{-1} g^{-1} n g \in (n^{-1} g^{-1} g)N = n^{-1} N = N$ . For all $n \in N, g \in G$ : $n^{-1} g^{-1} n g \in N$ . Substituting $g$ for $g^{-1}$ in $n^{-1} g^{-1} n g \in N$ , we get $n^{-1} g n g^{-1}  \in N$ . Because $n^{-1} \in N$ , we have $g n g^{-1}  \in N$ . For all $n \in N, g \in G$ : $g n g^{-1} \in N$ . It follows from (g) that $\operatorname{Cl}(n) \subseteq N$ for all $n \in N$ . Then $\bigcup_{n \in N} \operatorname{Cl}(n) \subseteq N$ . On the other hand, $n \in \operatorname{Cl}(n)$ and thus $N \subseteq \bigcup_{n \in N} \operatorname{Cl}(n)$ . The result the follows. $N = \bigcup_{n \in N} \operatorname{Cl}(n)$ where $\operatorname{Cl}(n) := \{gng^{-1} \mid g \in G\}$ . It follows from (h) that $\operatorname{Cl}(n) \subseteq N$ for all $n \in N$ .  As a result, for all $g \in G, n \in N$ , we have $g n g^{-1} = n'$ for some $n' \in N$ . Hence $gn=n'g$ for some $n' \in N$ . Thus $gN \subseteq Ng$ . By symmetry, we also have $Ng \subseteq gN$ . The result then follows. For all $g \in G$ : $gN = Ng$ . Let $G/N := \{gN \mid g \in G\}$ . We define a binary operation $G/N \times G/N \to G/N$ by $(gN) (hN) \mapsto (gh)N$ . Let's prove that it's well-defined, i.e. $gN = aN$ and $hN = bN$ implies $(gh) N = (ab) N$ . We have $gN= aN$ implies $g=an_1$ for some $n_1 \in N$ . Similarly, $h=bn_2$ for some $n_2 \in N$ . Then $gh = an_1bn_2$ . It follows from (i) that $Nb=bN$ and thus $n_1b=bn_3$ for some $n_3 \in N$ . Hence $gh =abn_3n_2$ . Because $n_2,n_3 \in N$ , we have $n_3n_2 \in N$ and thus $(n_3n_2)N =N$ . As a result, $(gh)N = (abn_3n_2)N = (ab)(n_3n_2)N = (ab)N$ . It's then straightforward to verify that $G/N$ together with above operation is group. Now we define a map $\phi: G \to G/N, g \mapsto gN$ . It's easy to verify that $\phi$ is in fact a group homomorphism such that $\operatorname{ker} \phi = \{g \in G \mid gN = 1N =N\} =N$ . There exists a group homomorphism whose domain is $G$ and kernel is $N$ . Let $\phi: G \to K$ be such a group homomorphism that $\operatorname{ker} \phi = N$ . If $gh \in N$ then $\phi (gh) = \phi(g) \phi (h) = 1$ . This means $\phi(g) = (\phi(h))^{-1}$ . As a result, $\phi (hg) = \phi(h) \phi (g) = \phi(h) (\phi(h))^{-1} = 1$ and thus $hg \in N$ . By symmetry, we have $hg \in N \implies gh \in N$ . This completes the proof. For all $g,h \in G$ : $gh \in N \iff hg \in N$ .","I've found various equivalent definitions of normal subgroup from this Wikipedia page . I've just finished proving their equivalence. Could you please verify if it is fine or contains logical mistakes? Let be a subgroup of . Then the following statements are equivalent. a. For all : . b. For all : . c. For all : . d. The sets of left and right cosets of in coincide. e. For all : and . f. For all : . g. For all : . h. where . i. For all : . j. There exists a group homomorphism whose domain is and kernel is . My attempt: To make it easier to follow, I put each part of the proof between two consecutive definitions. For all : . Assume the contrary that there exist and such that . Then by (a). This is a contradiction. For all : . Substituting for in , we get . Substituting for in , we get . As a result, . Substituting for in , we get the desired result. For all : . It follows from (c) that . The result then follows. The sets of left and right cosets of in coincide. Because of (d) and the fact that , we have . It follows from and that and for some . Then . Because , for some . Then . For all : and . We have and . Then by (e), we have . For all : . Substituting for in , we get . Because , we have . For all : . It follows from (g) that for all . Then . On the other hand, and thus . The result the follows. where . It follows from (h) that for all .  As a result, for all , we have for some . Hence for some . Thus . By symmetry, we also have . The result then follows. For all : . Let . We define a binary operation by . Let's prove that it's well-defined, i.e. and implies . We have implies for some . Similarly, for some . Then . It follows from (i) that and thus for some . Hence . Because , we have and thus . As a result, . It's then straightforward to verify that together with above operation is group. Now we define a map . It's easy to verify that is in fact a group homomorphism such that . There exists a group homomorphism whose domain is and kernel is . Let be such a group homomorphism that . If then . This means . As a result, and thus . By symmetry, we have . This completes the proof. For all : .","N G g,h \in G gh \in N \iff hg \in N g \in G gNg^{-1} \subseteq N g \in G gNg^{-1} = N N G x,y,g,h \in G x \in gN y \in hN \implies xy \in (gh)N n \in N, g \in G n^{-1} g^{-1} n g \in N n \in N, g \in G g n g^{-1} \in N N = \bigcup_{n \in N} \operatorname{Cl}(n) \operatorname{Cl}(n) := \{gng^{-1} \mid g \in G\} g \in G gN = Ng G N g,h \in G gh \in N \iff hg \in N n\in N g \in G gng^{-1} \notin N g^{-1} (gn) = n\notin N g \in G gNg^{-1} \subseteq N g g^{-1} gNg^{-1} \subseteq N g^{-1}  N g\subseteq N N g^{-1} N g gNg^{-1} \subseteq N N \subseteq g^{-1} N g g^{-1} N g = N g g^{-1} g^{-1} N g = N g \in G gNg^{-1} = N gN= Ng N G h \in hN \cap Nh hN=Nh x \in gN y \in hN x = gn_1 y = hn_2 n_1,n_2 \in N xy = gn_1 hn_2 Nh = hN n_1 h = h n_3 n_3 \in N xy = g h n_3 n_2 = (gh) (n_3 n_2) \in (gh) N x,y,g,h \in G x \in gN y \in hN \implies xy \in (gh)N n^{-1} g^{-1} n \in n^{-1} g^{-1} N g \in gN n^{-1} g^{-1} n g \in (n^{-1} g^{-1} g)N = n^{-1} N = N n \in N, g \in G n^{-1} g^{-1} n g \in N g g^{-1} n^{-1} g^{-1} n g \in N n^{-1} g n g^{-1}  \in N n^{-1} \in N g n g^{-1}  \in N n \in N, g \in G g n g^{-1} \in N \operatorname{Cl}(n) \subseteq N n \in N \bigcup_{n \in N} \operatorname{Cl}(n) \subseteq N n \in \operatorname{Cl}(n) N \subseteq \bigcup_{n \in N} \operatorname{Cl}(n) N = \bigcup_{n \in N} \operatorname{Cl}(n) \operatorname{Cl}(n) := \{gng^{-1} \mid g \in G\} \operatorname{Cl}(n) \subseteq N n \in N g \in G, n \in N g n g^{-1} = n' n' \in N gn=n'g n' \in N gN \subseteq Ng Ng \subseteq gN g \in G gN = Ng G/N := \{gN \mid g \in G\} G/N \times G/N \to G/N (gN) (hN) \mapsto (gh)N gN = aN hN = bN (gh) N = (ab) N gN= aN g=an_1 n_1 \in N h=bn_2 n_2 \in N gh = an_1bn_2 Nb=bN n_1b=bn_3 n_3 \in N gh =abn_3n_2 n_2,n_3 \in N n_3n_2 \in N (n_3n_2)N =N (gh)N = (abn_3n_2)N = (ab)(n_3n_2)N = (ab)N G/N \phi: G \to G/N, g \mapsto gN \phi \operatorname{ker} \phi = \{g \in G \mid gN = 1N =N\} =N G N \phi: G \to K \operatorname{ker} \phi = N gh \in N \phi (gh) = \phi(g) \phi (h) = 1 \phi(g) = (\phi(h))^{-1} \phi (hg) = \phi(h) \phi (g) = \phi(h) (\phi(h))^{-1} = 1 hg \in N hg \in N \implies gh \in N g,h \in G gh \in N \iff hg \in N","['abstract-algebra', 'group-theory', 'solution-verification', 'normal-subgroups']"
20,Is the cap product the same on cochain complexes with isomorphic cohomology rings?,Is the cap product the same on cochain complexes with isomorphic cohomology rings?,,"Let $X$ be a topological space. Let $\mathcal{D}$ be a cochain algebra, with cohomology ring $H^* (\mathcal{D})$ . Suppose that we have that the cohomology ring $H^* (X)$ (endowed with the cup product) is isomorphic, as a ring, to $H^* (\mathcal{D})$ , equipped with its respective operation (and the homology groups of $X$ are isomorphic to the homology groups of $\mathcal{D}$ ). There is an action of $H^* (X)$ on $H_* (X)$ given by the cap product: $\frown:H_* (X) \otimes H^* (X) \rightarrow H_* (X)$ . There is also an action of $H^* (\mathcal{D})$ on $H_* (\mathcal{D})$ given by dualizing the algebra structure of $\mathcal{D}$ (described below). I am interested in whether this ""cap product"" on $\mathcal{D}$ induces the same action of $H^* (\mathcal{D})$ on $H_* (\mathcal{D})$ as the cap product does for the cohomology and homology of $X$ . Denote by $*$ the multiplication in $\mathcal{D}$ . Let $\widehat{\mathcal{D}}$ denote the chain complex obtained by dualizing the cochain complex $\mathcal{D}$ . We obtain an operation $$\frown:\mathcal{D} \times \widehat{\mathcal{D}} \rightarrow \widehat{\mathcal{D}}$$ by defining $\langle u * v, w \rangle = \langle u, v \frown w \rangle$ , the dual of $*$ . This is the exact way in which, if we were given the cup product on the level of cochains of $X$ , we would dualize to get the cap product on the cochains/chains of $X$ . There is an induced operation on homology: $$\frown:H_* (\mathcal{D}) \otimes H^* (\mathcal{D}) \rightarrow H_* (\mathcal{D}).$$ Is this operation the same (isomorphic to) the cap product on the isomorphic cohomology and homology groups of $X$ ? I think that the answer is yes, but I cannot find a proof. EDIT: Additionally, I would at least like to know if I am correct in thinking that this is always true in the case of zero torsion.","Let be a topological space. Let be a cochain algebra, with cohomology ring . Suppose that we have that the cohomology ring (endowed with the cup product) is isomorphic, as a ring, to , equipped with its respective operation (and the homology groups of are isomorphic to the homology groups of ). There is an action of on given by the cap product: . There is also an action of on given by dualizing the algebra structure of (described below). I am interested in whether this ""cap product"" on induces the same action of on as the cap product does for the cohomology and homology of . Denote by the multiplication in . Let denote the chain complex obtained by dualizing the cochain complex . We obtain an operation by defining , the dual of . This is the exact way in which, if we were given the cup product on the level of cochains of , we would dualize to get the cap product on the cochains/chains of . There is an induced operation on homology: Is this operation the same (isomorphic to) the cap product on the isomorphic cohomology and homology groups of ? I think that the answer is yes, but I cannot find a proof. EDIT: Additionally, I would at least like to know if I am correct in thinking that this is always true in the case of zero torsion.","X \mathcal{D} H^* (\mathcal{D}) H^* (X) H^* (\mathcal{D}) X \mathcal{D} H^* (X) H_* (X) \frown:H_* (X) \otimes H^* (X) \rightarrow H_* (X) H^* (\mathcal{D}) H_* (\mathcal{D}) \mathcal{D} \mathcal{D} H^* (\mathcal{D}) H_* (\mathcal{D}) X * \mathcal{D} \widehat{\mathcal{D}} \mathcal{D} \frown:\mathcal{D} \times \widehat{\mathcal{D}} \rightarrow \widehat{\mathcal{D}} \langle u * v, w \rangle = \langle u, v \frown w \rangle * X X \frown:H_* (\mathcal{D}) \otimes H^* (\mathcal{D}) \rightarrow H_* (\mathcal{D}). X","['abstract-algebra', 'ring-theory', 'algebraic-topology', 'homology-cohomology']"
21,Fields involving NaN,Fields involving NaN,,"Motivation NaN as a value exists in IEEE floating-point numbers. Since every operation involving NaN has NaN as the outcome, IEEE floating-point numbers are not fields. I want to define a new algebraic structure so NaN could be encapsulated. Definitions A field $(F,+,\times)$ is NaNable if there exists a binary operation $\sim$ such that $(F \cup \{\text{NaN}\}, \sim, +)$ is a field, where $\text{NaN}$ is the identity element of $\sim$ . $(F \cup \{\text{NaN}\}, \sim, +,\times)$ is a NaNification of $(F,+,\times)$ , and is a NaN-field . Examples Examples of NaNable finite fields include $\mathbb{Z}_2$ , $\mathbb{Z}_3$ , $\mathbb{Z}_7$ , and $\mathbb{Z}_{31}$ . The operation tables of NaNification of $\mathbb{Z}_3$ is: $$ \begin{array}{c|cccc} \sim & \text{NaN} & 0 & 1 & 2 \\ \hline \text{NaN} & \text{NaN} & 0 & 1 & 2 \\ 0 & 0 & \text{NaN} & 2 & 1 \\ 1 & 1 & 2 & \text{NaN} & 0 \\ 2 & 2 & 1 & 0 & \text{NaN} \\ \end{array} $$ $$ \begin{array}{c|cccc} + & \text{NaN} & 0 & 1 & 2 \\ \hline \text{NaN} & \text{NaN} & \text{NaN} & \text{NaN} & \text{NaN} \\ 0 & \text{NaN} & 0 & 1 & 2 \\ 1 & \text{NaN} & 1 & 2 & 0 \\ 2 & \text{NaN} & 2 & 0 & 1 \\ \end{array} $$ $$ \begin{array}{c|ccc} \times & 0 & 1 & 2 \\ \hline 0 & 0 & 0 & 0 \\ 1 & 0 & 1 & 2 \\ 2 & 0 & 2 & 1 \\ \end{array} $$ Note that multiplication involving $\text{NaN}$ is not defined yet. Questions Is there a notable example of NaNable infinite field? Is there a dedicated name for $\sim$ ? Not as a symbol, but as an operation? It turns out that if every multiplication involving $\text{NaN}$ is defined as $\text{NaN}$ , distributivity is satisfied in the NaN-field. Does this mean I should define it accordingly?","Motivation NaN as a value exists in IEEE floating-point numbers. Since every operation involving NaN has NaN as the outcome, IEEE floating-point numbers are not fields. I want to define a new algebraic structure so NaN could be encapsulated. Definitions A field is NaNable if there exists a binary operation such that is a field, where is the identity element of . is a NaNification of , and is a NaN-field . Examples Examples of NaNable finite fields include , , , and . The operation tables of NaNification of is: Note that multiplication involving is not defined yet. Questions Is there a notable example of NaNable infinite field? Is there a dedicated name for ? Not as a symbol, but as an operation? It turns out that if every multiplication involving is defined as , distributivity is satisfied in the NaN-field. Does this mean I should define it accordingly?","(F,+,\times) \sim (F \cup \{\text{NaN}\}, \sim, +) \text{NaN} \sim (F \cup \{\text{NaN}\}, \sim, +,\times) (F,+,\times) \mathbb{Z}_2 \mathbb{Z}_3 \mathbb{Z}_7 \mathbb{Z}_{31} \mathbb{Z}_3 
\begin{array}{c|cccc}
\sim & \text{NaN} & 0 & 1 & 2 \\ \hline
\text{NaN} & \text{NaN} & 0 & 1 & 2 \\
0 & 0 & \text{NaN} & 2 & 1 \\
1 & 1 & 2 & \text{NaN} & 0 \\
2 & 2 & 1 & 0 & \text{NaN} \\
\end{array}
 
\begin{array}{c|cccc}
+ & \text{NaN} & 0 & 1 & 2 \\ \hline
\text{NaN} & \text{NaN} & \text{NaN} & \text{NaN} & \text{NaN} \\
0 & \text{NaN} & 0 & 1 & 2 \\
1 & \text{NaN} & 1 & 2 & 0 \\
2 & \text{NaN} & 2 & 0 & 1 \\
\end{array}
 
\begin{array}{c|ccc}
\times & 0 & 1 & 2 \\ \hline
0 & 0 & 0 & 0 \\
1 & 0 & 1 & 2 \\
2 & 0 & 2 & 1 \\
\end{array}
 \text{NaN} \sim \text{NaN} \text{NaN}",['abstract-algebra']
22,A simple group with $|\operatorname{Syl}_p G| \le 6$ is cyclic,A simple group with  is cyclic,|\operatorname{Syl}_p G| \le 6,"Let $G$ be a simple, finite group, s.t. for every prime $p$ , it satisfies $k_p=\left|\operatorname{Syl}_p G\right| \le 6$ . Show that $G$ is cyclic. My attempt: Let $n=p_1^{e_1}p_2^{e_2}\ldots p_r^{e_r}$ be the (distinct) prime factorization of $n = \left|G\right|$ . If $n$ is prime, $G$ is cyclic. So we assume $e_1\ge 1$ and $e_2\ge1$ . From Sylow's theorems, we have $k_{p_1}\mid\prod_{i\ne1} p_i^{e_i}$ and $k_{p_1}\equiv1\ (\operatorname{mod} p_1)$ , and the same applies for $p_2$ . Sicne $G$ is simple, $k_{p_{1,2}}\ne1$ , and it is given that $k_{p_{1,2}}\le6$ . The options for $p_1$ are No prime $p_1$ satisfies $k_{p_1}=2$ . The other options are that $p_1=2,3,5$ and $k_{p_1}=3,4,6$ respectively. How do I continue from here?","Let be a simple, finite group, s.t. for every prime , it satisfies . Show that is cyclic. My attempt: Let be the (distinct) prime factorization of . If is prime, is cyclic. So we assume and . From Sylow's theorems, we have and , and the same applies for . Sicne is simple, , and it is given that . The options for are No prime satisfies . The other options are that and respectively. How do I continue from here?","G p k_p=\left|\operatorname{Syl}_p G\right| \le 6 G n=p_1^{e_1}p_2^{e_2}\ldots p_r^{e_r} n = \left|G\right| n G e_1\ge 1 e_2\ge1 k_{p_1}\mid\prod_{i\ne1} p_i^{e_i} k_{p_1}\equiv1\ (\operatorname{mod} p_1) p_2 G k_{p_{1,2}}\ne1 k_{p_{1,2}}\le6 p_1 p_1 k_{p_1}=2 p_1=2,3,5 k_{p_1}=3,4,6","['abstract-algebra', 'group-theory', 'finite-groups', 'sylow-theory', 'p-groups']"
23,Extensions of $A_5$ by $C_2$,Extensions of  by,A_5 C_2,"I'm attempting the following exercise: Prove that there are only three finite groups whose composition factors are $A_5$ and $C_2$ . The direct product $A_5 \times C_2$ . The symmetric group $S_5$ . A central extension of $A_5$ by $C_2$ known as the binary icosahedral group. I've completed parts 1 and 2. For part 3, I'm trying to look at extensions of the form: $$1 \to C_2 \to G \to A_5 \to 1$$ Is there any straightforward argument to show that there cannot exist a non-central split extension of this form? Furthermore, I would be interested in knowing if there is any neat way to show that the unique non-split central extension of $A_5$ by $C_2$ is isomorphic to $I_{120}$ . (@DerekHolt gave some hints in the comments.)","I'm attempting the following exercise: Prove that there are only three finite groups whose composition factors are and . The direct product . The symmetric group . A central extension of by known as the binary icosahedral group. I've completed parts 1 and 2. For part 3, I'm trying to look at extensions of the form: Is there any straightforward argument to show that there cannot exist a non-central split extension of this form? Furthermore, I would be interested in knowing if there is any neat way to show that the unique non-split central extension of by is isomorphic to . (@DerekHolt gave some hints in the comments.)",A_5 C_2 A_5 \times C_2 S_5 A_5 C_2 1 \to C_2 \to G \to A_5 \to 1 A_5 C_2 I_{120},"['abstract-algebra', 'group-theory']"
24,"Suppose $\exists a\in (G, \cdot), a\neq e$ with $G\setminus \{a\}\le G$. Prove that $(G,\cdot) \cong (\mathbb{Z}/2\mathbb Z,+)$.",Suppose  with . Prove that .,"\exists a\in (G, \cdot), a\neq e G\setminus \{a\}\le G (G,\cdot) \cong (\mathbb{Z}/2\mathbb Z,+)","Consider a group $(G,\cdot)$ with the property that $\exists a\in G, a\neq e$ , such that $G\setminus \{a\}$ is a subgroup of $G$ . Prove that $(G,\cdot) \cong (\mathbb{Z}/2\mathbb Z,+)$ . We know that if $H$ is a subgroup of $G$ then $\forall x \in H, y\in G\setminus H$ we have that $xy \in G \setminus H$ . In our case, $\forall x\neq a$ , $xa \in G \setminus (G\setminus \{a\})=\{a\}$ and this implies that $\forall x\neq a$ , $x=e$ . As a result, $G=\{e,a\}$ and it is well-known and easy to prove that any group of order $2$ is isomorphic to $(\mathbb{Z}/2\mathbb Z,+)$ . I would like to know if my proof is correct.","Consider a group with the property that , such that is a subgroup of . Prove that . We know that if is a subgroup of then we have that . In our case, , and this implies that , . As a result, and it is well-known and easy to prove that any group of order is isomorphic to . I would like to know if my proof is correct.","(G,\cdot) \exists a\in G, a\neq e G\setminus \{a\} G (G,\cdot) \cong (\mathbb{Z}/2\mathbb Z,+) H G \forall x \in H, y\in G\setminus H xy \in G \setminus H \forall x\neq a xa \in G \setminus (G\setminus \{a\})=\{a\} \forall x\neq a x=e G=\{e,a\} 2 (\mathbb{Z}/2\mathbb Z,+)","['abstract-algebra', 'group-theory', 'proof-verification']"
25,Intuition of Liouville's Theorem (differential algebra) Proof,Intuition of Liouville's Theorem (differential algebra) Proof,,"At the end of my abstract algebra class this spring, we were given an overview of differential algebra and some differential Galois theory. We went too fast to prove anything nontrivial, but I found Liouville's theorem on elementary antiderivatives really interesting and have been trying to get a handle on the proof recently. The theorem is as follows: Theorem (Liouville) Let $\mathbb{F}$ be a differential field. For $a\in \mathbb{F}$ , there exists an elementary differential extension $\mathbb{E}$ containing an antiderivative of $a$ if and only if there exist $u_1,...,u_n,v\in\mathbb{F}$ , and constants $c_1,...,c_n\in\mathbb{F}$ such that $$ a = v' + \sum_{i=1}^nc_i\frac{u_i'}{u_i}.$$ The only proof I've found online is that due to Maxwell Rosenlicht (see theorem 4.3 here ). Its logic is reasonably easy to follow but it's long enough that I can't piece together an intuition from it. Does anyone have / know of an intuitive account for why Liouville's differential algebra theorem is true?","At the end of my abstract algebra class this spring, we were given an overview of differential algebra and some differential Galois theory. We went too fast to prove anything nontrivial, but I found Liouville's theorem on elementary antiderivatives really interesting and have been trying to get a handle on the proof recently. The theorem is as follows: Theorem (Liouville) Let be a differential field. For , there exists an elementary differential extension containing an antiderivative of if and only if there exist , and constants such that The only proof I've found online is that due to Maxwell Rosenlicht (see theorem 4.3 here ). Its logic is reasonably easy to follow but it's long enough that I can't piece together an intuition from it. Does anyone have / know of an intuitive account for why Liouville's differential algebra theorem is true?","\mathbb{F} a\in \mathbb{F} \mathbb{E} a u_1,...,u_n,v\in\mathbb{F} c_1,...,c_n\in\mathbb{F}  a = v' + \sum_{i=1}^nc_i\frac{u_i'}{u_i}.","['abstract-algebra', 'proof-explanation', 'intuition', 'differential-algebra']"
26,Let $G$ be a nilpotent group generated by a finite set of torsion (i.e. finite order) elements. Show that $G$ is finite.,Let  be a nilpotent group generated by a finite set of torsion (i.e. finite order) elements. Show that  is finite.,G G,Given: Let $G$ be a nilpotent group generated by a finite set of torsion (i.e. finite order) elements. Show that $G$ is finite. Also would love to know if it's possible to show that an infinite finitely generated nilpotent group has an infinite center.,Given: Let be a nilpotent group generated by a finite set of torsion (i.e. finite order) elements. Show that is finite. Also would love to know if it's possible to show that an infinite finitely generated nilpotent group has an infinite center.,G G,"['abstract-algebra', 'group-theory', 'finite-groups', 'nilpotent-groups']"
27,"""Factoring Behavior"" of a cubic polynomial mod $p$","""Factoring Behavior"" of a cubic polynomial mod",p,"Given an irreducible polynomial $f(x)$ with integer coefficients, I want to characterize all the primes $p$ based on how $f(x)$ factors $\mod p$ . For quadratic polynomials, it follows directly from the Law of Quadratic Reciprocity. Specifically, if $f(x)$ has discriminant $D$ , then $f(x)$ splits completely $\mod p$ ( $p>2$ ) precisely when $D$ is a square $\mod p$ and remains irreducible otherwise. I want to generalize this as much as I can for the case where $f(x)$ is a cubic polynomial. Specifically, I want to determine for which primes, $f$ splits, remains irreducible, or factors as a product of a linear and quadratic polynomial. Any hints, full or partial solutions, or references to papers or textbooks with ideas that could help me would be greatly appreciated! Here's what I've done so far: Let $f(x)$ be an irreducible cubic polynomial with integer coefficients. Let $\alpha, \beta, \gamma$ be the roots and $D = (\alpha-\beta)^2(\beta-\gamma)^2(\alpha-\gamma)^2$ its discriminant. Let $p$ be a prime. For simplicity, I'm assuming $p>3$ and that $p \nmid D$ . Let $K/\mathbb{Q}$ be its splitting field and $G$ its Galois group. Let $K_p/\mathbb{F}_p$ be the splitting field of $f$ over the finite field with $p$ elements. Case 1: $D = d^2$ for some integer $d$ . In this case, $G$ is cyclic of order 3. This implies that $\beta$ and $\gamma$ can be written as polynomials in $\alpha$ , so we have either $f$ splits completely $\mod p$ , or $f$ remains irreducible. I'm conjecturing that $K \subset \mathbb{Q}(\zeta_d)$ ( $\zeta_d$ is a primitive $d$ -th root of unity) and from there I can obtain congruence conditions for which the polynomial splits. I imagine that this can be proven easily using Class Field Theory, but I'm still in the process of learning it. My main idea so far has been to use Gaussian periods corresponding to index 3 subgroups of $(\mathbb{Z}/d\mathbb{Z})^\times$ . It worked for all the explicit examples I've checked, but I have no idea how to prove it works in general. Case 2: $D$ is not a square. If $\left(\frac{D}{p}\right) = -1$ , then I'm able to show that $f$ has exactly one root in $\mathbb{F}_p$ as then $\mathbb{F}_p(\sqrt{D})$ is a quadratic subextension of $K_p$ , which implies $K_p$ is itself a quadratic extension. I have no idea how to show the converse statement or if it is even true. If $\left(\frac{D}{p}\right) = 1$ , things get a lot more complicated and this where I'm having the most trouble. In the case $f(x) = x^3 - 2$ , we have that $D = -3\cdot6^2$ . Thus, $$\left(\frac{D}{p}\right) = \left(\frac{-3\cdot6^2}{p}\right) = \left(\frac{-3}{p}\right) = 1 \iff p \equiv 1\mod 3.$$ In this case, I know that there exists integers $A,B$ such that $p = A^2 + 3B^2$ and that $f$ splits $\mod p$ iff $3 | B$ . These congruence conditions were derived from the Law of Cubic Reciprocity, which heavily relies on the Eisenstein integers and $p = A^2 + 3B^2$ being possible because the Eisenstein integers are a UFD. Back to the general case, my first guess has been to write $$kp = A^2 - DB^2$$ for some appropriate $k$ , and then hopefully be able to derive some congruence conditions for $A$ and $B$ . I imagine that this might involve deriving some kind of analogue for Cubic Reciprocity, but the ring of integers of $\mathbb{Q}(\sqrt{D})$ would replace the Eisenstein integers. My next idea (which might be more fruitful), is to use modular (or automorphic) forms. Admittedly, I don't know much about them, just a few basic facts like how they form a finite dimensional $\mathbb{C}$ -vector space etc. If I try this, my approach would be to first try to use modular forms (after learning more about them) to try solving the case $f(x) = x^3 - 2$ , replacing $x^2 + 3y^2$ with $$\theta = \sum_{i,j \in \mathbb{Z}} q^{x^2+3y^2}$$ and see if this sheds any light on the general problem. I've heard tidbits from people much smarter than me that makes me think this could maybe work out, but I'm mostly in the dark here. If you have any input on what would be the best approach, or if you're thinking, ""From what you've written, it doesn't seem like you know enough math. You should probably know X,Y, and Z before you can even hope to solve this,"" let me know! I'd be glad to hear any solution, even if it goes way over my head! I've been trying to solve this problem for over a year now, and I feel as if I've made no significant progress. It's VERY disheartening.","Given an irreducible polynomial with integer coefficients, I want to characterize all the primes based on how factors . For quadratic polynomials, it follows directly from the Law of Quadratic Reciprocity. Specifically, if has discriminant , then splits completely ( ) precisely when is a square and remains irreducible otherwise. I want to generalize this as much as I can for the case where is a cubic polynomial. Specifically, I want to determine for which primes, splits, remains irreducible, or factors as a product of a linear and quadratic polynomial. Any hints, full or partial solutions, or references to papers or textbooks with ideas that could help me would be greatly appreciated! Here's what I've done so far: Let be an irreducible cubic polynomial with integer coefficients. Let be the roots and its discriminant. Let be a prime. For simplicity, I'm assuming and that . Let be its splitting field and its Galois group. Let be the splitting field of over the finite field with elements. Case 1: for some integer . In this case, is cyclic of order 3. This implies that and can be written as polynomials in , so we have either splits completely , or remains irreducible. I'm conjecturing that ( is a primitive -th root of unity) and from there I can obtain congruence conditions for which the polynomial splits. I imagine that this can be proven easily using Class Field Theory, but I'm still in the process of learning it. My main idea so far has been to use Gaussian periods corresponding to index 3 subgroups of . It worked for all the explicit examples I've checked, but I have no idea how to prove it works in general. Case 2: is not a square. If , then I'm able to show that has exactly one root in as then is a quadratic subextension of , which implies is itself a quadratic extension. I have no idea how to show the converse statement or if it is even true. If , things get a lot more complicated and this where I'm having the most trouble. In the case , we have that . Thus, In this case, I know that there exists integers such that and that splits iff . These congruence conditions were derived from the Law of Cubic Reciprocity, which heavily relies on the Eisenstein integers and being possible because the Eisenstein integers are a UFD. Back to the general case, my first guess has been to write for some appropriate , and then hopefully be able to derive some congruence conditions for and . I imagine that this might involve deriving some kind of analogue for Cubic Reciprocity, but the ring of integers of would replace the Eisenstein integers. My next idea (which might be more fruitful), is to use modular (or automorphic) forms. Admittedly, I don't know much about them, just a few basic facts like how they form a finite dimensional -vector space etc. If I try this, my approach would be to first try to use modular forms (after learning more about them) to try solving the case , replacing with and see if this sheds any light on the general problem. I've heard tidbits from people much smarter than me that makes me think this could maybe work out, but I'm mostly in the dark here. If you have any input on what would be the best approach, or if you're thinking, ""From what you've written, it doesn't seem like you know enough math. You should probably know X,Y, and Z before you can even hope to solve this,"" let me know! I'd be glad to hear any solution, even if it goes way over my head! I've been trying to solve this problem for over a year now, and I feel as if I've made no significant progress. It's VERY disheartening.","f(x) p f(x) \mod p f(x) D f(x) \mod p p>2 D \mod p f(x) f f(x) \alpha, \beta, \gamma D = (\alpha-\beta)^2(\beta-\gamma)^2(\alpha-\gamma)^2 p p>3 p \nmid D K/\mathbb{Q} G K_p/\mathbb{F}_p f p D = d^2 d G \beta \gamma \alpha f \mod p f K \subset \mathbb{Q}(\zeta_d) \zeta_d d (\mathbb{Z}/d\mathbb{Z})^\times D \left(\frac{D}{p}\right) = -1 f \mathbb{F}_p \mathbb{F}_p(\sqrt{D}) K_p K_p \left(\frac{D}{p}\right) = 1 f(x) = x^3 - 2 D = -3\cdot6^2 \left(\frac{D}{p}\right) = \left(\frac{-3\cdot6^2}{p}\right) = \left(\frac{-3}{p}\right) = 1 \iff p \equiv 1\mod 3. A,B p = A^2 + 3B^2 f \mod p 3 | B p = A^2 + 3B^2 kp = A^2 - DB^2 k A B \mathbb{Q}(\sqrt{D}) \mathbb{C} f(x) = x^3 - 2 x^2 + 3y^2 \theta = \sum_{i,j \in \mathbb{Z}} q^{x^2+3y^2}","['abstract-algebra', 'number-theory']"
28,"MacLane-Birkhoff's ""Algebra"" vs Jacobson's ""Basic Algebra I,II"" vs Lang's ""Algebra""","MacLane-Birkhoff's ""Algebra"" vs Jacobson's ""Basic Algebra I,II"" vs Lang's ""Algebra""",,"I'm searching for an apt textbook(s) on Abstract Algebra for a very advanced undergraduate/graduate level course in Algebra, and would be grateful for any help. I've thought of the aforementioned texts, but additional suggestions would be welcome. Some points about my background: Ive taken a course in Linear Algebra where I read Romans Advanced Linear Algebra in addition to Halmos Finite Dimensional Vector Spaces and Axlers Linear Algebra Done Right as primary texts. Ive already had a course of Abstract Algebra from Artins Algebra . Additionally, I have finished the first 7 chapters of Baby Rudin, and plan to try Loomis and Sternbergs Advanced Calculus next. These upcoming semesters I have courses in Algebra (the syllabus for which is attached at the end). The prescribed texts are Jacobsons Basic Algebra I, II , and Langs Algebra . Some of the material is familiar, so Im looking to study beyond the syllabus. I've looked through Lang's Algebra , MacLane and Birkhoffs Algebra , and Jacobsons Basic Algebra I,II . So far, Basic Algebra I seems much easier and more leisurely than the other two. Understanding the exposition was not an issue for any of the books (I used G. Bergman's Companion to Lang for some assistance). Unfortunately Basic Algebra I usually gives explicit constructions as opposed to using categories or universal properties. I would prefer to learn Abstract Algebra using Category Theory and Universal Properties openly; to do this from Jacobsons book, I would have to use both volumes together. Im not sure how to do this. I referred to the Chicago Undergraduate Mathematics Bibliography , which suggested that few portions from Basic Algebra II (such as Group Representation Theory) were best done from elsewhere. I would be grateful if somebody could compare using Basic Algebra I, II , MacLane and Birkhoffs Algebra , and Langs Algebra ; In particular, their relative merits/demerits, levels of difficulty, modernness of the treatment, and quality of the exercises. I enjoy struggling through the texts that are terse, and leave significant gaps (such as in proofs) for the reader to fill in, something like Rudin or Lang's books. Lastly, is it better to do any one of these books from cover to cover? Or is it better to do individual sections from each book, or perhaps one book followed by another? If it is the latter two, then could the relevant chapters/order please be pointed out to me? All comments and answers are greatly appreciated. Thank you so much for your time! (Syllabus below) Rings, ideals, homomorphisms, quotient rings, fraction fields, maximal ideals, factorization, UFD, PID, Gauss Lemma, fields, field extensions, finite fields, function fields, algebraically closed fields. Galois theory: separable and normal extensions, purely inseparable extensions, fundamental theorem of Galois theory. Module theory, structure theorem for modules over PIDs. multilinear algebra: tensor, symmetric and exterior products, tensor product of algebras. Categories and functors, some notions of homological algebra. Non-commutative rings, semisimplicity, Jacobson theory, Artin-Wedderburn theorem, group-rings, matrix groups, introduction to representations.","I'm searching for an apt textbook(s) on Abstract Algebra for a very advanced undergraduate/graduate level course in Algebra, and would be grateful for any help. I've thought of the aforementioned texts, but additional suggestions would be welcome. Some points about my background: Ive taken a course in Linear Algebra where I read Romans Advanced Linear Algebra in addition to Halmos Finite Dimensional Vector Spaces and Axlers Linear Algebra Done Right as primary texts. Ive already had a course of Abstract Algebra from Artins Algebra . Additionally, I have finished the first 7 chapters of Baby Rudin, and plan to try Loomis and Sternbergs Advanced Calculus next. These upcoming semesters I have courses in Algebra (the syllabus for which is attached at the end). The prescribed texts are Jacobsons Basic Algebra I, II , and Langs Algebra . Some of the material is familiar, so Im looking to study beyond the syllabus. I've looked through Lang's Algebra , MacLane and Birkhoffs Algebra , and Jacobsons Basic Algebra I,II . So far, Basic Algebra I seems much easier and more leisurely than the other two. Understanding the exposition was not an issue for any of the books (I used G. Bergman's Companion to Lang for some assistance). Unfortunately Basic Algebra I usually gives explicit constructions as opposed to using categories or universal properties. I would prefer to learn Abstract Algebra using Category Theory and Universal Properties openly; to do this from Jacobsons book, I would have to use both volumes together. Im not sure how to do this. I referred to the Chicago Undergraduate Mathematics Bibliography , which suggested that few portions from Basic Algebra II (such as Group Representation Theory) were best done from elsewhere. I would be grateful if somebody could compare using Basic Algebra I, II , MacLane and Birkhoffs Algebra , and Langs Algebra ; In particular, their relative merits/demerits, levels of difficulty, modernness of the treatment, and quality of the exercises. I enjoy struggling through the texts that are terse, and leave significant gaps (such as in proofs) for the reader to fill in, something like Rudin or Lang's books. Lastly, is it better to do any one of these books from cover to cover? Or is it better to do individual sections from each book, or perhaps one book followed by another? If it is the latter two, then could the relevant chapters/order please be pointed out to me? All comments and answers are greatly appreciated. Thank you so much for your time! (Syllabus below) Rings, ideals, homomorphisms, quotient rings, fraction fields, maximal ideals, factorization, UFD, PID, Gauss Lemma, fields, field extensions, finite fields, function fields, algebraically closed fields. Galois theory: separable and normal extensions, purely inseparable extensions, fundamental theorem of Galois theory. Module theory, structure theorem for modules over PIDs. multilinear algebra: tensor, symmetric and exterior products, tensor product of algebras. Categories and functors, some notions of homological algebra. Non-commutative rings, semisimplicity, Jacobson theory, Artin-Wedderburn theorem, group-rings, matrix groups, introduction to representations.",,"['abstract-algebra', 'group-theory', 'reference-request', 'ring-theory', 'soft-question']"
29,"$m = [K(\alpha):K], n=[K(\beta):K]$ and $\gcd(m,n)=1$, prove that $K(\alpha + \beta) = K(\alpha,\beta)$","and , prove that","m = [K(\alpha):K], n=[K(\beta):K] \gcd(m,n)=1 K(\alpha + \beta) = K(\alpha,\beta)","I am looking for an elementary demonstration of this: Suppose $K$ a field, $\mathrm{char}(K)=0$ and $K(\alpha) \supseteq K$ and $K(\beta) \supseteq K$ field extensions. Denote $n=[K(\alpha):K]$ and $m=[K(\beta):K]$ and suppose $\gcd(m,n)=1$ . Then $K(\alpha,\beta)=K(\alpha + \beta)$ . To let everyone know what my level is: I am an undergraduate math student and my interest sparked when my professor showed us the demonstration of what I have written above in the specific case $K = \mathbb{Q}$ , which only works thanks the specific structure of $\mathbb{C}$ . I also found a paper that demonstrates it in fields with $\text{char}(K) \ne 0$ , but involves subjects that are not in my course. link I've tried to find mn distinct homomorphisms from $K(\alpha+\beta)\to E$ where $E$ is an algebraic closure of $K(\alpha + \beta)$ . I've tried using the $mn$ homomorphisms from $K(\alpha,\beta)$ , which has degree $mn$ over $K$ , and I can say that my problem is equivalent of proof that: call $\alpha_i$ and $\beta_j$ for $i\in 1,\ldots,n$ and $j\in 1,\ldots,m$ the conjugates of $\alpha$ and $\beta$ , the other roots of minimum polynomial. Thus $\alpha+\beta$ has degree $mn$ over $K$ $\iff$ "" $\alpha+\beta=\alpha_i+\beta_j\iff \alpha=\alpha_i\wedge \beta=\beta_j$ "".  My problem is that I don't know how prove that. Thank you in advance Edit : The specific structure of $\mathbb{C}$ is that, if you consider only $(\mathbb{C},+)$ you can orderer it in the following way: $z = a + bi; w = c +di$ ( $a,b,c,d \in \mathbb{R}$ ) $z < w \iff  a < c  \vee a = c \land b < d$ In that way we have a maximum $\alpha_{i} + \beta_{j} = \max(\alpha_{i}) + \max(\beta_{j})$ . Now we have that: $$\sigma(\max(\alpha_{i}) + \max(\beta_{j}))= \sigma(\max(\alpha_{i})) + \sigma(\max(\beta_{j})) = \max(\alpha_{i}) + \max(\beta_{j})$$ If and only if $\sigma(\max(\alpha_{i})) = \max(\alpha_{i}) \land \sigma(\max(\beta_{j})) = \max(\beta_{j}).$ This lets us prove that there exist $mn$ distinct homomorphisms. Since $\alpha + \beta$ and $\alpha_{i} + \beta_{j}$ are conjugate, we can conclude.","I am looking for an elementary demonstration of this: Suppose a field, and and field extensions. Denote and and suppose . Then . To let everyone know what my level is: I am an undergraduate math student and my interest sparked when my professor showed us the demonstration of what I have written above in the specific case , which only works thanks the specific structure of . I also found a paper that demonstrates it in fields with , but involves subjects that are not in my course. link I've tried to find mn distinct homomorphisms from where is an algebraic closure of . I've tried using the homomorphisms from , which has degree over , and I can say that my problem is equivalent of proof that: call and for and the conjugates of and , the other roots of minimum polynomial. Thus has degree over "" "".  My problem is that I don't know how prove that. Thank you in advance Edit : The specific structure of is that, if you consider only you can orderer it in the following way: ( ) In that way we have a maximum . Now we have that: If and only if This lets us prove that there exist distinct homomorphisms. Since and are conjugate, we can conclude.","K \mathrm{char}(K)=0 K(\alpha) \supseteq K K(\beta) \supseteq K n=[K(\alpha):K] m=[K(\beta):K] \gcd(m,n)=1 K(\alpha,\beta)=K(\alpha + \beta) K = \mathbb{Q} \mathbb{C} \text{char}(K) \ne 0 K(\alpha+\beta)\to E E K(\alpha + \beta) mn K(\alpha,\beta) mn K \alpha_i \beta_j i\in 1,\ldots,n j\in 1,\ldots,m \alpha \beta \alpha+\beta mn K \iff \alpha+\beta=\alpha_i+\beta_j\iff \alpha=\alpha_i\wedge \beta=\beta_j \mathbb{C} (\mathbb{C},+) z = a + bi; w = c +di a,b,c,d \in \mathbb{R} z < w \iff  a < c  \vee a = c \land b < d \alpha_{i} + \beta_{j} = \max(\alpha_{i}) + \max(\beta_{j}) \sigma(\max(\alpha_{i}) + \max(\beta_{j}))= \sigma(\max(\alpha_{i})) + \sigma(\max(\beta_{j})) = \max(\alpha_{i}) + \max(\beta_{j}) \sigma(\max(\alpha_{i})) = \max(\alpha_{i}) \land \sigma(\max(\beta_{j})) = \max(\beta_{j}). mn \alpha + \beta \alpha_{i} + \beta_{j}","['abstract-algebra', 'field-theory', 'galois-theory', 'extension-field']"
30,What is an example of a non-simple finite extension $K/F$ such that the purely inseparable closure of $F$ in $K$ is simple?,What is an example of a non-simple finite extension  such that the purely inseparable closure of  in  is simple?,K/F F K,"The standard example of a finite extension that is not simple is to take $k$ to be a field of characteristic $p > 0$ and consider $k(x,y)$ over $k(x^p,y^p)$ . In this case, the extension is purely inseparable too. I was wondering the following: Is it possible to construct a finite extension of fields $K/F$ which is not simple and such that the purely inseparable closure of $F$ in $K$ is simple? The purely inseparable closure of $F$ in $K$ , denoted $I$ , is defined to be the set of all $\alpha \in K$ that are purely inseparable over $F$ . Similarly, the separable closure of $F$ in $K$ is denoted $S$ and is defined to be the set of all $\alpha \in K$ that are separable over $F$ . One can show that $I$ and $S$ are subfields of $K$ containing $F$ . Can anyone give me a hint on how to construct, or even search for, such an extension? I know that we cannot have $SI = K$ , because we can show from the hypotheses that $SI$ is a simple extension of $F$ . In particular, $K$ cannot be normal over $F$ (see Proposition V.6.11 in Lang's Algebra , page 251, third edition). Moreover, $F$ cannot be perfect because every algebraic extension of a perfect field is separable (see Corollary V.6.12 in Lang's Algebra , page 252, third edition), so in particular $F$ cannot be a finite field or have characteristic $0$ . Moreover, this answer by @KConrad on MathOverflow shows that every finite extension of $\Bbb{F}_p(x)$ is simple, so these are also ruled out as candidates for $F$ . I haven't been able to make any progress beyond this, though. I've tried fiddling around with the above standard example of $k(x,y)/k(x^p,y^p)$ , but no matter how I make $K/F$ non-simple I end up with a non-simple purely inseparable closure. Maybe it's because the only example I know of a non-simple extension is the one mentioned at the beginning that I don't know how to create new examples. I spent a bit of time trying to prove that no such extension could exist, but could not make any progress there either. Any help is appreciated. This question is inspired by the problem asked here: A finite extension is simple iff the purely inseparable closure is simple? . Any such example of extension fields $K/F$ would provide a counter-example to the claim in the title of the linked question.","The standard example of a finite extension that is not simple is to take to be a field of characteristic and consider over . In this case, the extension is purely inseparable too. I was wondering the following: Is it possible to construct a finite extension of fields which is not simple and such that the purely inseparable closure of in is simple? The purely inseparable closure of in , denoted , is defined to be the set of all that are purely inseparable over . Similarly, the separable closure of in is denoted and is defined to be the set of all that are separable over . One can show that and are subfields of containing . Can anyone give me a hint on how to construct, or even search for, such an extension? I know that we cannot have , because we can show from the hypotheses that is a simple extension of . In particular, cannot be normal over (see Proposition V.6.11 in Lang's Algebra , page 251, third edition). Moreover, cannot be perfect because every algebraic extension of a perfect field is separable (see Corollary V.6.12 in Lang's Algebra , page 252, third edition), so in particular cannot be a finite field or have characteristic . Moreover, this answer by @KConrad on MathOverflow shows that every finite extension of is simple, so these are also ruled out as candidates for . I haven't been able to make any progress beyond this, though. I've tried fiddling around with the above standard example of , but no matter how I make non-simple I end up with a non-simple purely inseparable closure. Maybe it's because the only example I know of a non-simple extension is the one mentioned at the beginning that I don't know how to create new examples. I spent a bit of time trying to prove that no such extension could exist, but could not make any progress there either. Any help is appreciated. This question is inspired by the problem asked here: A finite extension is simple iff the purely inseparable closure is simple? . Any such example of extension fields would provide a counter-example to the claim in the title of the linked question.","k p > 0 k(x,y) k(x^p,y^p) K/F F K F K I \alpha \in K F F K S \alpha \in K F I S K F SI = K SI F K F F F 0 \Bbb{F}_p(x) F k(x,y)/k(x^p,y^p) K/F K/F","['abstract-algebra', 'field-theory']"
31,"Generalization of ""Characteristic subgroup of a normal subgroup is normal""","Generalization of ""Characteristic subgroup of a normal subgroup is normal""",,"Let $K\leq H \leq G$. Prove that if $A\subseteq Aut(G)$, $H$ is $A$-invariant subgroup of $G$ and $K$ is a characteristic subgroup of $H$, then $K$ is an $A$-invariant subgroup of $G$. Let $\alpha\in A$. We want to show that $\alpha(K)\subseteq K$. Since $H$ is $A$-invariant subgroup of $G$, we have $\alpha(H)\subseteq H$. So the usual way is to restrict $\alpha$ to form an automorphism of $H$. But I only know that $\alpha$ is 1-1 and I cannot make sure that $\alpha(H)=H$. Also, since $A$ is not a subgroup of $Aut(G)$, I do not know that whether $\alpha^{-1}\in A$. Remark: $H$ is an $A$-invariant subgroup of $G$ if $\alpha(h)\in H$ for every $\alpha\in A$ and $h\in H$.","Let $K\leq H \leq G$. Prove that if $A\subseteq Aut(G)$, $H$ is $A$-invariant subgroup of $G$ and $K$ is a characteristic subgroup of $H$, then $K$ is an $A$-invariant subgroup of $G$. Let $\alpha\in A$. We want to show that $\alpha(K)\subseteq K$. Since $H$ is $A$-invariant subgroup of $G$, we have $\alpha(H)\subseteq H$. So the usual way is to restrict $\alpha$ to form an automorphism of $H$. But I only know that $\alpha$ is 1-1 and I cannot make sure that $\alpha(H)=H$. Also, since $A$ is not a subgroup of $Aut(G)$, I do not know that whether $\alpha^{-1}\in A$. Remark: $H$ is an $A$-invariant subgroup of $G$ if $\alpha(h)\in H$ for every $\alpha\in A$ and $h\in H$.",,"['abstract-algebra', 'group-theory']"
32,$(x)$ prime ideal in $R[x]$ iff $R$ integral domain by contrapositive,prime ideal in  iff  integral domain by contrapositive,(x) R[x] R,"I've done this proof a few ways and I like this one but since it wasn't the ""official"" one, I wanted to ask if anyone sees a reason it's invalid. It just makes more sense to me on a concrete level. Thanks! Statement: $R$ is a commutative ring with unity. Show that (1) $(x)$ is a prime ideal in $R[x]$ if and only if (2) $R$ is an integral domain. Proof: By contrapositives. NOT (2) $\implies$ NOT (1). Assume $R$ is not an integral domain; i.e. there exist some zero divisors, nonzero $a, b \in R$ such that $ab = 0$. Then consider a counterexample using $p(x) = x+a$ and $q(x) = x + b$ which are both not in the ideal $(x)$, but  $$ p(x)q(x) = (x+a)(x+b) = x^2 + (a+b)x + ab  = x(x + a+b) \in (x). $$  Thus $(x)$ is not a prime ideal. NOT (1) $\implies$ NOT (2). Assume $(x)$ is not a prime ideal in $R[x]$; i.e. there exist some $f(x), g(x) \in R[x]$ such that neither are in $(x)$ but $f(x)g(x) \in (x)$. To be specific,  $$ f(x) = a_n x^n + ... + a_1 x + a_0, \quad a_0 \neq 0,$$ $$ g(x) = b_n x^n + ... + b_1 x + b_0, \quad b_0 \neq 0,$$ but the term $a_0b_0$ in $f(x)g(x)$ is $0$. Then since $a_0, b_0 \in R$, they are zero divisors in $R$ and $R$ cannot be an integral domain. Basically I was looking for a way around using the intermediate $R[x]/(x) \cong R$ theorem because although I know it and can prove it, I'm paranoid I won't have time on my qual, and I don't want to just use it without justification so counterexamples seemed faster. If I've broken logic somewhere please let me know!","I've done this proof a few ways and I like this one but since it wasn't the ""official"" one, I wanted to ask if anyone sees a reason it's invalid. It just makes more sense to me on a concrete level. Thanks! Statement: $R$ is a commutative ring with unity. Show that (1) $(x)$ is a prime ideal in $R[x]$ if and only if (2) $R$ is an integral domain. Proof: By contrapositives. NOT (2) $\implies$ NOT (1). Assume $R$ is not an integral domain; i.e. there exist some zero divisors, nonzero $a, b \in R$ such that $ab = 0$. Then consider a counterexample using $p(x) = x+a$ and $q(x) = x + b$ which are both not in the ideal $(x)$, but  $$ p(x)q(x) = (x+a)(x+b) = x^2 + (a+b)x + ab  = x(x + a+b) \in (x). $$  Thus $(x)$ is not a prime ideal. NOT (1) $\implies$ NOT (2). Assume $(x)$ is not a prime ideal in $R[x]$; i.e. there exist some $f(x), g(x) \in R[x]$ such that neither are in $(x)$ but $f(x)g(x) \in (x)$. To be specific,  $$ f(x) = a_n x^n + ... + a_1 x + a_0, \quad a_0 \neq 0,$$ $$ g(x) = b_n x^n + ... + b_1 x + b_0, \quad b_0 \neq 0,$$ but the term $a_0b_0$ in $f(x)g(x)$ is $0$. Then since $a_0, b_0 \in R$, they are zero divisors in $R$ and $R$ cannot be an integral domain. Basically I was looking for a way around using the intermediate $R[x]/(x) \cong R$ theorem because although I know it and can prove it, I'm paranoid I won't have time on my qual, and I don't want to just use it without justification so counterexamples seemed faster. If I've broken logic somewhere please let me know!",,"['abstract-algebra', 'ring-theory', 'ideals', 'maximal-and-prime-ideals', 'integral-domain']"
33,Is the set of upper(and separately lower)-triangular matrices a ring?,Is the set of upper(and separately lower)-triangular matrices a ring?,,"I was reading lecture notes which mentioned the set of upper (and separately lower)-triangular matrices of a certain dimensionality is a group under matrix multiplication. That made me wonder if they also form a ring under addition and multiplication. So first, they are an abelian group under matrix addition: The sum of any number of triangular matrices is itself a triangular matrix. The 0 matrix is the 0 element. There is an additive inverse. (Element-wise negation) Matrix addition is commutative. Then, they are a monoid under multiplication. The product of any number of triangular matrices is itself a triangular matrix. The identity matrix is the multiplicative identity. And finally, multiplication distributes over addition. Is that correct?","I was reading lecture notes which mentioned the set of upper (and separately lower)-triangular matrices of a certain dimensionality is a group under matrix multiplication. That made me wonder if they also form a ring under addition and multiplication. So first, they are an abelian group under matrix addition: The sum of any number of triangular matrices is itself a triangular matrix. The 0 matrix is the 0 element. There is an additive inverse. (Element-wise negation) Matrix addition is commutative. Then, they are a monoid under multiplication. The product of any number of triangular matrices is itself a triangular matrix. The identity matrix is the multiplicative identity. And finally, multiplication distributes over addition. Is that correct?",,"['abstract-algebra', 'matrices', 'ring-theory']"
34,Field having exactly two extensions of each degree,Field having exactly two extensions of each degree,,"It is well-known that a finite field has a unique extension of degree $n$, in a given algebraic closure, for every $n \geq 1$. Is there a field $F$ such that, in some given algebraic closure of $F$, there are exactly two extensions of $F$ of degree $n$, for every $n \geq 3$ (or $n$ big enough)? (Here, ""exactly two"" means that $F \subset K, L \subset \overline F$ simply satisfy $K \neq L$. In particular, I'm not identifying isomorphic fields or $F$-algebras.) A more general question would be to replace ""exactly two"" by  ""exactly $N$"" (for some fixed integer $N > 1$). Actually, the most general setting is the study of the maps $d_F$ (resp. $d_{F, \text{iso}}$, resp. $d_{F, F\text{-iso}}$) which assign, to every integer $n \geq 2$, the cardinal of the set of (resp. field isomorphism classes, resp. $F$-algebras isomorphism classes of) extensions of $F$ of degree $n$. For instance: Can $d_F$ be a constant map $n \mapsto c$ for some $1<c<\aleph_0$ ? Can $d_F$ be a bounded function (different from the constant function $1$) ? Can $d_F$ take both finite and infinite values? It can be shown that a local field of characteristic $0$ has finitely many extensions of some given degree, but the number might grow with the degree (instead of being bounded by $N$ as I want). Thank you!","It is well-known that a finite field has a unique extension of degree $n$, in a given algebraic closure, for every $n \geq 1$. Is there a field $F$ such that, in some given algebraic closure of $F$, there are exactly two extensions of $F$ of degree $n$, for every $n \geq 3$ (or $n$ big enough)? (Here, ""exactly two"" means that $F \subset K, L \subset \overline F$ simply satisfy $K \neq L$. In particular, I'm not identifying isomorphic fields or $F$-algebras.) A more general question would be to replace ""exactly two"" by  ""exactly $N$"" (for some fixed integer $N > 1$). Actually, the most general setting is the study of the maps $d_F$ (resp. $d_{F, \text{iso}}$, resp. $d_{F, F\text{-iso}}$) which assign, to every integer $n \geq 2$, the cardinal of the set of (resp. field isomorphism classes, resp. $F$-algebras isomorphism classes of) extensions of $F$ of degree $n$. For instance: Can $d_F$ be a constant map $n \mapsto c$ for some $1<c<\aleph_0$ ? Can $d_F$ be a bounded function (different from the constant function $1$) ? Can $d_F$ take both finite and infinite values? It can be shown that a local field of characteristic $0$ has finitely many extensions of some given degree, but the number might grow with the degree (instead of being bounded by $N$ as I want). Thank you!",,"['abstract-algebra', 'field-theory', 'extension-field']"
35,"Is there a non-constant $h \in \mathbb{C}[x_1 , \dots , x_n ]$ that divides every element of this given ideal?",Is there a non-constant  that divides every element of this given ideal?,"h \in \mathbb{C}[x_1 , \dots , x_n ]","I'm trying to prove the following: Let $I  \mathbb{C}[x_1 , \dots , x_n ]$ be an ideal with the property that any two   elements $f_1$ , $f_2 \in I$ have a non-trivial common divisor. Then there is a non-constant $h$ which divides every element of $I$. I tried to use the fact that $\mathbb{C}[x_1 , \dots , x_n ]$ is a Noetherian ring to do induction on the number of generators of $I$. But I did not succeed. Can anyone help me?","I'm trying to prove the following: Let $I  \mathbb{C}[x_1 , \dots , x_n ]$ be an ideal with the property that any two   elements $f_1$ , $f_2 \in I$ have a non-trivial common divisor. Then there is a non-constant $h$ which divides every element of $I$. I tried to use the fact that $\mathbb{C}[x_1 , \dots , x_n ]$ is a Noetherian ring to do induction on the number of generators of $I$. But I did not succeed. Can anyone help me?",,"['abstract-algebra', 'ring-theory', 'ideals']"
36,Let $G$ be a group and $p$ divide the order of G. Show that if $p^2$ divides the order of $G$ then $p$ divides the order of the automorphism group,Let  be a group and  divide the order of G. Show that if  divides the order of  then  divides the order of the automorphism group,G p p^2 G p,"Let $G$ be a finite group and $p$ a prime divisor for the order of $G$. Show that if $p^{2}$ divides the order of G, then $p$ divides $|Aut(G)|$ . I know that $G$ has a subgroup of order $p$ and  I think that it would be a good idea to consider the following homomorphism to prove that $Aut(G)$ has a subgroup of order $p$ : $$ \phi :G \to Aut(G)$$ $$\phi(g) = C_{g}$$ where $C_{g}$ is the conjugation given by $g$. Also I know that $Ker(\phi)=Z(G)$ then $G/Z(G) \cong Im(\phi)=Inn(G)$, but I can't figure out how to use the fact that $p^{2}$ divides the order of $G$.","Let $G$ be a finite group and $p$ a prime divisor for the order of $G$. Show that if $p^{2}$ divides the order of G, then $p$ divides $|Aut(G)|$ . I know that $G$ has a subgroup of order $p$ and  I think that it would be a good idea to consider the following homomorphism to prove that $Aut(G)$ has a subgroup of order $p$ : $$ \phi :G \to Aut(G)$$ $$\phi(g) = C_{g}$$ where $C_{g}$ is the conjugation given by $g$. Also I know that $Ker(\phi)=Z(G)$ then $G/Z(G) \cong Im(\phi)=Inn(G)$, but I can't figure out how to use the fact that $p^{2}$ divides the order of $G$.",,['abstract-algebra']
37,Intuition on Krull Principal Ideal Theorem Proof's Major Ingredients,Intuition on Krull Principal Ideal Theorem Proof's Major Ingredients,,"The krull principal ideal theorem I am interested in is the one involving that over Noetherian ring, minimal primes over the principal ideal must be height at most 1. In the proof, it involves symbolic power of prime ideals localization at the same prime ideal such that symbolic power of prime ideals is identified as extension of powers of the prime ideal Quotienting out the principal ideal results in artinian ring Nakayama Lemma shows any $Q'$ such that $Q'\subset Q\subset P$ where $P$ minimal over $(x)$, $Q$ and $Q'$ are primes, $Q'=0$. I could reproduce the proof of PIT. I think 4 is necessary for noetherian ring. $2$ is consequence of $1$. $2$ combining $3$ deduces $4$. Q1. Was there alternative versions of proof which does not invoke symbolic power of prime ideals? I have seen Eisenbud, Atiyah's proof on PIT and they all use symbolic power of prime ideals. The usage of symbolic power of prime ideals is too much ad hoc. Q2. Symbolic power of prime ideal is more or less generalization of primary ideal. What is the motivation for adopting it and adapting it to the proof? Q3. What are usage of symbolic power of prime ideals? I have not encountered a problem that needs to use it unless the problem requires PIT application which will quote symbolic power.","The krull principal ideal theorem I am interested in is the one involving that over Noetherian ring, minimal primes over the principal ideal must be height at most 1. In the proof, it involves symbolic power of prime ideals localization at the same prime ideal such that symbolic power of prime ideals is identified as extension of powers of the prime ideal Quotienting out the principal ideal results in artinian ring Nakayama Lemma shows any $Q'$ such that $Q'\subset Q\subset P$ where $P$ minimal over $(x)$, $Q$ and $Q'$ are primes, $Q'=0$. I could reproduce the proof of PIT. I think 4 is necessary for noetherian ring. $2$ is consequence of $1$. $2$ combining $3$ deduces $4$. Q1. Was there alternative versions of proof which does not invoke symbolic power of prime ideals? I have seen Eisenbud, Atiyah's proof on PIT and they all use symbolic power of prime ideals. The usage of symbolic power of prime ideals is too much ad hoc. Q2. Symbolic power of prime ideal is more or less generalization of primary ideal. What is the motivation for adopting it and adapting it to the proof? Q3. What are usage of symbolic power of prime ideals? I have not encountered a problem that needs to use it unless the problem requires PIT application which will quote symbolic power.",,"['abstract-algebra', 'commutative-algebra', 'maximal-and-prime-ideals', 'krull-dimension']"
38,Actions of groups on categories,Actions of groups on categories,,"Let $\Gamma$ be a group and denote by $\underline{\Gamma}$ the tensor category whose objects are simply the group elements, hom-sets only contain identities and the tensor product is given by the group product. The following definitions are standard: Let $\mathcal{C}$ be a category. An action of $\Gamma$ on $\mathcal{C}$ is a monoidal functor $\underline{\Gamma}\rightarrow \text{Aut}(\mathcal{C})$. In a similar fashion one has the following definition. Let $\mathcal{C}$ be a monoidal category. An action of $\Gamma$ on $\mathcal{C}$ is a monoidal functor $\underline{\Gamma}\rightarrow \text{Aut}_{\otimes}(\mathcal{C})$. Here $\text{Aut}_{\otimes}(\mathcal{C})$ is the category of tensor auto-equivalences. In the book tensor categories by Etingof, Gelaki, Nikshych and Ostrik, there is the following exercise: Show that any action of $\Gamma$ on $\text{Vec}$ corresponds to an element in $H^2(\Gamma, k^*)$. Show that any action of $\Gamma$ on $\text{Vec}$ viewed as a monoidal category is trivial. I was wondering why any action of $\Gamma$ on $\text{Vec}$ is trivial when we view the latter as a monoidal category. The only difference with the first part of the question is that we only allow tensor-auto-equivalences. How many tensor-auto-equivalences are there on $\text{Vec}$? EDIT For those who are wondering how to prove the first part of the exercise, I'll include the details in this post. Suppose $\mathcal{C}=\ _k\text{Vec}$ which we view as an abelian category. That means that $\text{Aut}(\mathcal{C})$ are those additive functors that are auto-equivalences. Let $\Gamma$ act on $\mathcal{C}$. Then for any $\gamma\in \Gamma$ we have an auto-equivalence $T_{\gamma}$. Since $T_{\gamma}$ is an auto-equivalence, it takes any one dimensional space to a one dimensional space. Moreover, there are natural isomorphisms $\theta_{\gamma_1,\gamma_2}:T_{\gamma_1}\circ T_{\gamma_2}\rightarrow T_{\gamma_1\gamma_2}$. This natural isomorphism is completely determined by it's action on the one-dimensional space $k$, the corresponding isomorphism is given by an invertible number which I will also denote by $\theta_{\gamma_1,\gamma_2}$. Since $T$ is a monoidal functor and since $_k\text{Vec}$ is strict, we have that $$\theta_{\gamma_1\gamma_2,\gamma_3}\theta_{\gamma_1,\gamma_2}=\theta_{\gamma_1,\gamma_2\gamma_3}\theta_{\gamma_2,\gamma_3}.$$ This says that $\theta_{-,-}\in Z^2(\Gamma,k^*)$. I guess one says two actions $T_1,T_2:\underline{\Gamma}\rightarrow \text{Aut}(\mathcal{C})$ are the same it the functors $T_1,T_2$ are monoidally equivalent. One should (probably) find the following statement. The functors $T_1,T_2$ are monoidally equivalent if and only if the corresponding $2$-cocycles are cohomologous. Carefully writing down the definitions should give the proof (I didn't check this though).","Let $\Gamma$ be a group and denote by $\underline{\Gamma}$ the tensor category whose objects are simply the group elements, hom-sets only contain identities and the tensor product is given by the group product. The following definitions are standard: Let $\mathcal{C}$ be a category. An action of $\Gamma$ on $\mathcal{C}$ is a monoidal functor $\underline{\Gamma}\rightarrow \text{Aut}(\mathcal{C})$. In a similar fashion one has the following definition. Let $\mathcal{C}$ be a monoidal category. An action of $\Gamma$ on $\mathcal{C}$ is a monoidal functor $\underline{\Gamma}\rightarrow \text{Aut}_{\otimes}(\mathcal{C})$. Here $\text{Aut}_{\otimes}(\mathcal{C})$ is the category of tensor auto-equivalences. In the book tensor categories by Etingof, Gelaki, Nikshych and Ostrik, there is the following exercise: Show that any action of $\Gamma$ on $\text{Vec}$ corresponds to an element in $H^2(\Gamma, k^*)$. Show that any action of $\Gamma$ on $\text{Vec}$ viewed as a monoidal category is trivial. I was wondering why any action of $\Gamma$ on $\text{Vec}$ is trivial when we view the latter as a monoidal category. The only difference with the first part of the question is that we only allow tensor-auto-equivalences. How many tensor-auto-equivalences are there on $\text{Vec}$? EDIT For those who are wondering how to prove the first part of the exercise, I'll include the details in this post. Suppose $\mathcal{C}=\ _k\text{Vec}$ which we view as an abelian category. That means that $\text{Aut}(\mathcal{C})$ are those additive functors that are auto-equivalences. Let $\Gamma$ act on $\mathcal{C}$. Then for any $\gamma\in \Gamma$ we have an auto-equivalence $T_{\gamma}$. Since $T_{\gamma}$ is an auto-equivalence, it takes any one dimensional space to a one dimensional space. Moreover, there are natural isomorphisms $\theta_{\gamma_1,\gamma_2}:T_{\gamma_1}\circ T_{\gamma_2}\rightarrow T_{\gamma_1\gamma_2}$. This natural isomorphism is completely determined by it's action on the one-dimensional space $k$, the corresponding isomorphism is given by an invertible number which I will also denote by $\theta_{\gamma_1,\gamma_2}$. Since $T$ is a monoidal functor and since $_k\text{Vec}$ is strict, we have that $$\theta_{\gamma_1\gamma_2,\gamma_3}\theta_{\gamma_1,\gamma_2}=\theta_{\gamma_1,\gamma_2\gamma_3}\theta_{\gamma_2,\gamma_3}.$$ This says that $\theta_{-,-}\in Z^2(\Gamma,k^*)$. I guess one says two actions $T_1,T_2:\underline{\Gamma}\rightarrow \text{Aut}(\mathcal{C})$ are the same it the functors $T_1,T_2$ are monoidally equivalent. One should (probably) find the following statement. The functors $T_1,T_2$ are monoidally equivalent if and only if the corresponding $2$-cocycles are cohomologous. Carefully writing down the definitions should give the proof (I didn't check this though).",,"['abstract-algebra', 'group-theory', 'category-theory', 'monoidal-categories']"
39,Determine all local rings containing $\mathbb{C}$ and having dimension 5 as $\mathbb{C}$-vector space,Determine all local rings containing  and having dimension 5 as -vector space,\mathbb{C} \mathbb{C},"Let $R$ be a local ring containing $\mathbb{C}$ and $\dim_{\mathbb{C}}R = 5$ (as $\mathbb{C}$-vector space).   Let $\mathfrak{m}$ be the maximal ideal of $R$ and $d=\dim_{\mathbb{C}}\mathfrak{m}/\mathfrak{m}^2$. (1) Prove that $1\leq d  \leq4$. (2) For each $d=1,2,3,4$ determine all $R$ up to isomorphism. (1) is easy, but I'm stuck in (2). I know that $R$ is finitely generated $\mathbb{C}$-algebra and $R$ is Artinian. In the case $d=1$, I think $R\cong \mathbb{C}[x]/(x^5)$, but I can't prove it. In the case $d=2,3,4$, I have no idea. How to solve (2)?","Let $R$ be a local ring containing $\mathbb{C}$ and $\dim_{\mathbb{C}}R = 5$ (as $\mathbb{C}$-vector space).   Let $\mathfrak{m}$ be the maximal ideal of $R$ and $d=\dim_{\mathbb{C}}\mathfrak{m}/\mathfrak{m}^2$. (1) Prove that $1\leq d  \leq4$. (2) For each $d=1,2,3,4$ determine all $R$ up to isomorphism. (1) is easy, but I'm stuck in (2). I know that $R$ is finitely generated $\mathbb{C}$-algebra and $R$ is Artinian. In the case $d=1$, I think $R\cong \mathbb{C}[x]/(x^5)$, but I can't prove it. In the case $d=2,3,4$, I have no idea. How to solve (2)?",,"['abstract-algebra', 'ring-theory', 'commutative-algebra']"
40,Subgroups of $GL_n$ and group actions,Subgroups of  and group actions,GL_n,"For my abstract algebra class I have to do some exercises concerning group actions of $GL_n(K)$ on the set of flags $$F_n = \{0 \subseteq V_1 \subseteq V_2 \subseteq \ldots \subseteq V_r = K^n \,\vert\, \text{dim}_K V_i = n_1 + \ldots + n_i\}$$ where $n_1 + \ldots + n_r = n$. Although I got to know the basic things such as orbit and stabilizer, I don't have any practice on how to apply these concepts and, concretely, for these exercise I don't know how to proceed. The question is threefold and as follows. $\textit{First}$, show that the parabolic subgroup  $$P_n = \Bigg\{ \begin{pmatrix} A_1 & B_{12} \ldots & B_{1r} \\                                  0 & \ddots & \ddots & \vdots \\                                 \vdots & \ddots & \ddots & B_{r-1r} \\                                   0     & \ldots & 0    & A_r \end{pmatrix}\Bigg\}$$ where $$A_i \in GL_{n_i}, \, 1\leq i \leq r, B_{ij}\in \text{Mat}(n_{i} \times n_{j}, K) \, 1\leq i < j \leq r $$ is a subgroup of $GL_{n}$ by using the group action of $GL_n(K)$ on $F_n$. $\textit{Secondly}$, with the help of this result, show that the corresponding Levi subgroup $$M_n = \Bigg\{ \begin{pmatrix} A_1 & 0 \ldots & 0 \\                                  0 & \ddots & \ddots & \vdots \\                                 \vdots & \ddots & \ddots & 0 \\                                   0     & \ldots & 0    & A_r \end{pmatrix}\Bigg\}$$ where $$A_i \in GL_{n_i}$$ is in turn a subgroup of $P_n$. And $\textit{finally}$, show that the unipotent radical $$U_n = \Bigg\{ \begin{pmatrix} I_1 & B_{12} \ldots & B_{1r} \\                                  0 & \ddots & \ddots & \vdots \\                                 \vdots & \ddots & \ddots & B_{r-1r} \\                                   0     & \ldots & 0    & I_r \end{pmatrix}\Bigg\}$$ where $$I_i = Id_{GL_{n}}, \, 1\leq i \leq r, B_{ij}\in \text{Mat}(n_{i} \times n_{j}, K) \, 1\leq i < j \leq r $$ is a normal subgroup in $P_n$. With that, also prove that $P_n/U_n \cong M_n$ What I know is that the stabilizer for a group G and a set M is the set $G_x = \{g \in G\vert g\cdot x = x\}$ with $x \in M$ and a subgroup of $G$. I would merely guess that in the first or second part I would have to show that along these lines. In short, I do not have a clear conceptual picture and, most of all, I wouldn't know how to write it down properly even in case I had it. EDIT : Due to Derek Holt's comment, I tried to figure out how to show that $P_n$ is the stabilizer of the flags in which $V_i$ is the subspace spanned by the first $n_1 + \ldots + n_i$ basis vectors. Although I can somehow imagine it with simple block matrices of the kind of $$\begin{pmatrix} A & B \\ C & D \end{pmatrix} $$ I do not know how to put it on paper. What I tried is to rewrite each vector $x \in K^n$ for all $V_i$ where $i \in \{1, \ldots, n\}$ to $$ x = \begin{pmatrix} x_1 \\ \vdots \\ \vdots \\ x_n\end{pmatrix} = \begin{pmatrix} x_{V_i} \\ x'_{V_i} \end{pmatrix}$$ where $$ x \in V_i \Leftrightarrow x'_{V_i} = 0$$ Put informally, I think I 'see' that if I left-multiply some vector $x \in V_i \subset F_n$ with some matrix $p \in P_n$, then I should still get some vector $x^* \in V_i \subset F_n$. But how to write that down in a correct way? Also, for illustration purposes I tried to left-multiply the vector $$ x = \begin{pmatrix} x_1 \\ x_2 \\ x_3 \\ 0 \\ \vdots \\ 0 \end{pmatrix}$$ where $x \in \mathbb{R}^n$ with the given matrix $$ \begin{pmatrix} A_1 & B_{12} \ldots & B_{1r} \\                                  0 & \ddots & \ddots & \vdots \\                                 \vdots & \ddots & \ddots & B_{r-1r} \\                                   0     & \ldots & 0    & A_r \end{pmatrix}$$ in order to show that this really affects the resulting vector $x^*$ only in the first 3 coordinates, i.e. that it still lies in the subspace $V_i \in F_n$ it was before. But I got in some trouble regarding how to actually multiply it with the given block matrices. For example, $A_1$ would be in $GL_{n_1}(K)$, but how would I know what $n_1$ is? With regard to the third part, I know what a normal subgroup is and how to check it, i.e. $xVx^{-1} \subset V \, \forall x \in P_n$. I already did that with triangular matrices and the triangular matrices where $1$s are on the diagonal. However, here are entire matrices within other matrices, so I assume this complicates things. Hence, what's the idea?","For my abstract algebra class I have to do some exercises concerning group actions of $GL_n(K)$ on the set of flags $$F_n = \{0 \subseteq V_1 \subseteq V_2 \subseteq \ldots \subseteq V_r = K^n \,\vert\, \text{dim}_K V_i = n_1 + \ldots + n_i\}$$ where $n_1 + \ldots + n_r = n$. Although I got to know the basic things such as orbit and stabilizer, I don't have any practice on how to apply these concepts and, concretely, for these exercise I don't know how to proceed. The question is threefold and as follows. $\textit{First}$, show that the parabolic subgroup  $$P_n = \Bigg\{ \begin{pmatrix} A_1 & B_{12} \ldots & B_{1r} \\                                  0 & \ddots & \ddots & \vdots \\                                 \vdots & \ddots & \ddots & B_{r-1r} \\                                   0     & \ldots & 0    & A_r \end{pmatrix}\Bigg\}$$ where $$A_i \in GL_{n_i}, \, 1\leq i \leq r, B_{ij}\in \text{Mat}(n_{i} \times n_{j}, K) \, 1\leq i < j \leq r $$ is a subgroup of $GL_{n}$ by using the group action of $GL_n(K)$ on $F_n$. $\textit{Secondly}$, with the help of this result, show that the corresponding Levi subgroup $$M_n = \Bigg\{ \begin{pmatrix} A_1 & 0 \ldots & 0 \\                                  0 & \ddots & \ddots & \vdots \\                                 \vdots & \ddots & \ddots & 0 \\                                   0     & \ldots & 0    & A_r \end{pmatrix}\Bigg\}$$ where $$A_i \in GL_{n_i}$$ is in turn a subgroup of $P_n$. And $\textit{finally}$, show that the unipotent radical $$U_n = \Bigg\{ \begin{pmatrix} I_1 & B_{12} \ldots & B_{1r} \\                                  0 & \ddots & \ddots & \vdots \\                                 \vdots & \ddots & \ddots & B_{r-1r} \\                                   0     & \ldots & 0    & I_r \end{pmatrix}\Bigg\}$$ where $$I_i = Id_{GL_{n}}, \, 1\leq i \leq r, B_{ij}\in \text{Mat}(n_{i} \times n_{j}, K) \, 1\leq i < j \leq r $$ is a normal subgroup in $P_n$. With that, also prove that $P_n/U_n \cong M_n$ What I know is that the stabilizer for a group G and a set M is the set $G_x = \{g \in G\vert g\cdot x = x\}$ with $x \in M$ and a subgroup of $G$. I would merely guess that in the first or second part I would have to show that along these lines. In short, I do not have a clear conceptual picture and, most of all, I wouldn't know how to write it down properly even in case I had it. EDIT : Due to Derek Holt's comment, I tried to figure out how to show that $P_n$ is the stabilizer of the flags in which $V_i$ is the subspace spanned by the first $n_1 + \ldots + n_i$ basis vectors. Although I can somehow imagine it with simple block matrices of the kind of $$\begin{pmatrix} A & B \\ C & D \end{pmatrix} $$ I do not know how to put it on paper. What I tried is to rewrite each vector $x \in K^n$ for all $V_i$ where $i \in \{1, \ldots, n\}$ to $$ x = \begin{pmatrix} x_1 \\ \vdots \\ \vdots \\ x_n\end{pmatrix} = \begin{pmatrix} x_{V_i} \\ x'_{V_i} \end{pmatrix}$$ where $$ x \in V_i \Leftrightarrow x'_{V_i} = 0$$ Put informally, I think I 'see' that if I left-multiply some vector $x \in V_i \subset F_n$ with some matrix $p \in P_n$, then I should still get some vector $x^* \in V_i \subset F_n$. But how to write that down in a correct way? Also, for illustration purposes I tried to left-multiply the vector $$ x = \begin{pmatrix} x_1 \\ x_2 \\ x_3 \\ 0 \\ \vdots \\ 0 \end{pmatrix}$$ where $x \in \mathbb{R}^n$ with the given matrix $$ \begin{pmatrix} A_1 & B_{12} \ldots & B_{1r} \\                                  0 & \ddots & \ddots & \vdots \\                                 \vdots & \ddots & \ddots & B_{r-1r} \\                                   0     & \ldots & 0    & A_r \end{pmatrix}$$ in order to show that this really affects the resulting vector $x^*$ only in the first 3 coordinates, i.e. that it still lies in the subspace $V_i \in F_n$ it was before. But I got in some trouble regarding how to actually multiply it with the given block matrices. For example, $A_1$ would be in $GL_{n_1}(K)$, but how would I know what $n_1$ is? With regard to the third part, I know what a normal subgroup is and how to check it, i.e. $xVx^{-1} \subset V \, \forall x \in P_n$. I already did that with triangular matrices and the triangular matrices where $1$s are on the diagonal. However, here are entire matrices within other matrices, so I assume this complicates things. Hence, what's the idea?",,"['abstract-algebra', 'group-theory', 'group-actions']"
41,If prime $p=a_n10^n+a_{n-1}10^{n-1}+\ldots+a_110+a_0$ then $f(x)=a_nx^n+\ldots+a_0$ is irreducible in $\mathbb{Z}[x]$,If prime  then  is irreducible in,p=a_n10^n+a_{n-1}10^{n-1}+\ldots+a_110+a_0 f(x)=a_nx^n+\ldots+a_0 \mathbb{Z}[x],"I have been trying to solve this problem on my own for four days now, and I cannot figure out how to prove it: If we express a prime $p$ in base $10$ as $$p= a_m10^m+a_{m-1}10^{m-1}+\ldots +a_110+a_0,$$ with $0\leq a_i \leq 9$ for all $i \in [m]$, then the polynomial  $$f(x)= a_mx^m+a_{m-1}x^{m-1}+\ldots +a_1x+a_0$$ is irreducible in $\mathbb{Z}[x]$. First, note that $\gcd(a_n, \ldots, a_0)=1$, otherwise $p$ would not be prime. I want to prove this by contradiction. Suppose, to the contrary, that $f(x)$ is reducible in $\mathbb{Z}[x]$. Then there exist $a(x), b(x) \in \mathbb{Z}[x]$, neither of which are units, such that $f(x)=a(x)b(x)$. Now $p=f(10)=a(10)b(10)$, which implies that $a(10)\in \{1, -1\}$ or $b(10)\in \{1, -1\}$ (whichever is the case will imply that the other is $\pm p$). Without loss of generality, suppose that $a(10)=1$. Now, I was given the following hint: Show that $f(x)$ takes on infinitely many prime values, that is, show that show that there exists a sequence $\{x_n\}$ of integers such that $f(x_n)=q_n$, where $q_n$ is prime for each $n$. If I can show this, I am done because by the argument made above, without loss of generality, this would imply that $a(x)$ takes on the value 1 infinitely many times. Then $a(x)-1=0$ for infinitely many $x$-values, but since a polynomial has infinitely many zeros if and only if it is the zero function itself, this would imply that $a(x)=1$ for all integers $x$, a contradiction. Can someone help me figure out why the hint is true? Thank you!! This problem is very interesting, don't you agree?","I have been trying to solve this problem on my own for four days now, and I cannot figure out how to prove it: If we express a prime $p$ in base $10$ as $$p= a_m10^m+a_{m-1}10^{m-1}+\ldots +a_110+a_0,$$ with $0\leq a_i \leq 9$ for all $i \in [m]$, then the polynomial  $$f(x)= a_mx^m+a_{m-1}x^{m-1}+\ldots +a_1x+a_0$$ is irreducible in $\mathbb{Z}[x]$. First, note that $\gcd(a_n, \ldots, a_0)=1$, otherwise $p$ would not be prime. I want to prove this by contradiction. Suppose, to the contrary, that $f(x)$ is reducible in $\mathbb{Z}[x]$. Then there exist $a(x), b(x) \in \mathbb{Z}[x]$, neither of which are units, such that $f(x)=a(x)b(x)$. Now $p=f(10)=a(10)b(10)$, which implies that $a(10)\in \{1, -1\}$ or $b(10)\in \{1, -1\}$ (whichever is the case will imply that the other is $\pm p$). Without loss of generality, suppose that $a(10)=1$. Now, I was given the following hint: Show that $f(x)$ takes on infinitely many prime values, that is, show that show that there exists a sequence $\{x_n\}$ of integers such that $f(x_n)=q_n$, where $q_n$ is prime for each $n$. If I can show this, I am done because by the argument made above, without loss of generality, this would imply that $a(x)$ takes on the value 1 infinitely many times. Then $a(x)-1=0$ for infinitely many $x$-values, but since a polynomial has infinitely many zeros if and only if it is the zero function itself, this would imply that $a(x)=1$ for all integers $x$, a contradiction. Can someone help me figure out why the hint is true? Thank you!! This problem is very interesting, don't you agree?",,"['abstract-algebra', 'polynomials', 'ring-theory', 'prime-numbers', 'irreducible-polynomials']"
42,Automorphisms of the group $\operatorname{GL}_n \mathbb C$,Automorphisms of the group,\operatorname{GL}_n \mathbb C,"I'm interested in the determination of a group $\operatorname{Aut}( \operatorname{GL}_n \mathbb C)$. A class of examples of automorphisms of $\operatorname{GL}_n \mathbb C$ is given by conjugations $c_g$, for $g \in \operatorname{GL}_n \mathbb C$: $$ c_g(A) = gAg^{-1}.$$ These automorphisms form a normal subgroup of $\operatorname{Aut}( \operatorname{GL}_n \mathbb C)$, a group of inner automorphisms. But for $n > 2$, we have an example of an outer automorphism: define $$ \iota(A) = (A^T)^{-1} = (A^{-1})^T.$$ Why we can't have $\iota = c_g$, for some $g \in G$? Take $\lambda \in \mathbb C$ such that $\lambda^n = 1, \lambda^2 \neq 1$. Then: $$\iota(\lambda I) = \lambda^{-1} I,$$ (which isn't equal to $\lambda I$ because of $\lambda^2 \neq 1$) but $$ c_g(\lambda I) = \lambda I,$$ for all $g$. My questions are, in an order of increasing difficulty: 1) Is $\operatorname{Aut}( \operatorname{GL}_2 \mathbb C) = \operatorname{Inn}( \operatorname{GL}_2 \mathbb C)$? 2) Is $ \operatorname{Aut}( \operatorname{GL}_n \mathbb C)/ \operatorname{Inn}( \operatorname{GL}_n \mathbb C) =  \{\mathrm{id}, \iota \} \simeq \mathbb Z / 2\mathbb Z $? 3) What is $\operatorname{Aut}( \operatorname{GL}_n \mathbb C)$? I would be really grateful if someone could give me a good reference in which all of these questions are discussed (especially if it's done for general fields).","I'm interested in the determination of a group $\operatorname{Aut}( \operatorname{GL}_n \mathbb C)$. A class of examples of automorphisms of $\operatorname{GL}_n \mathbb C$ is given by conjugations $c_g$, for $g \in \operatorname{GL}_n \mathbb C$: $$ c_g(A) = gAg^{-1}.$$ These automorphisms form a normal subgroup of $\operatorname{Aut}( \operatorname{GL}_n \mathbb C)$, a group of inner automorphisms. But for $n > 2$, we have an example of an outer automorphism: define $$ \iota(A) = (A^T)^{-1} = (A^{-1})^T.$$ Why we can't have $\iota = c_g$, for some $g \in G$? Take $\lambda \in \mathbb C$ such that $\lambda^n = 1, \lambda^2 \neq 1$. Then: $$\iota(\lambda I) = \lambda^{-1} I,$$ (which isn't equal to $\lambda I$ because of $\lambda^2 \neq 1$) but $$ c_g(\lambda I) = \lambda I,$$ for all $g$. My questions are, in an order of increasing difficulty: 1) Is $\operatorname{Aut}( \operatorname{GL}_2 \mathbb C) = \operatorname{Inn}( \operatorname{GL}_2 \mathbb C)$? 2) Is $ \operatorname{Aut}( \operatorname{GL}_n \mathbb C)/ \operatorname{Inn}( \operatorname{GL}_n \mathbb C) =  \{\mathrm{id}, \iota \} \simeq \mathbb Z / 2\mathbb Z $? 3) What is $\operatorname{Aut}( \operatorname{GL}_n \mathbb C)$? I would be really grateful if someone could give me a good reference in which all of these questions are discussed (especially if it's done for general fields).",,"['abstract-algebra', 'matrices', 'group-theory']"
43,Domains such that $R[X] \cong S[X]$ but $\mathrm{Frac}(R)[X] \not \cong \mathrm{Frac}(S)[X]$,Domains such that  but,R[X] \cong S[X] \mathrm{Frac}(R)[X] \not \cong \mathrm{Frac}(S)[X],"Is it possible to find two integral domains $R,S$ such that $R[X] \cong S[X]$ but $\mathrm{Frac}(R)[X] \not \cong \mathrm{Frac}(S)[X]$ ?   $\renewcommand{\Frac}{\mathrm{Frac}}$ Here $\Frac(R)$ denotes the fraction field of $R$. Obviously, $R[X] \cong S[X] \implies \Frac(R[X]) = \Frac(R)(X) \cong \Frac(S)(X)$, but it doesn't follow that $\Frac(R) \cong \Frac(S)$ (see (1) or (2) ). So we can't conclude $\mathrm{Frac}(R)[X] \cong \mathrm{Frac}(S)[X]$. However, the restriction of an isomorphism $f : \mathrm{Frac}(R)[X] \to \mathrm{Frac}(S)[X]$ to $(\mathrm{Frac}(R))^*$ has range lying in $(\Frac(S))^*$, and since $f(0)=0$ it follows that $f\vert_{\Frac(R)} : \Frac(R) \to \Frac(S)$ is an isomorphism. It doesn't follow that $R \cong S$. That's why I think that a counter-example may come from such examples . See also here . Let $$R=\frac{\mathbb{C}[x,y,z]}{(xy - (1 - z^2))}, \quad S=\frac{\mathbb{C}[x,y,z]}{(x^2y - (1 - z^2))}$$ Apparently, $R \not \cong S$ (see the previous link). It looks like $R,S$ are domains. But do we have $\Frac(R) \not \cong \Frac(S)$ ? It would solve my problem. Or is there an easier way to solve it? Notice that the ""reverse"" question is easy to solve $\Bbb Z \not \cong \Bbb Q$, but $\Frac(\Bbb Z)[X] \cong \Bbb Q[X]$. Thank you for your help.","Is it possible to find two integral domains $R,S$ such that $R[X] \cong S[X]$ but $\mathrm{Frac}(R)[X] \not \cong \mathrm{Frac}(S)[X]$ ?   $\renewcommand{\Frac}{\mathrm{Frac}}$ Here $\Frac(R)$ denotes the fraction field of $R$. Obviously, $R[X] \cong S[X] \implies \Frac(R[X]) = \Frac(R)(X) \cong \Frac(S)(X)$, but it doesn't follow that $\Frac(R) \cong \Frac(S)$ (see (1) or (2) ). So we can't conclude $\mathrm{Frac}(R)[X] \cong \mathrm{Frac}(S)[X]$. However, the restriction of an isomorphism $f : \mathrm{Frac}(R)[X] \to \mathrm{Frac}(S)[X]$ to $(\mathrm{Frac}(R))^*$ has range lying in $(\Frac(S))^*$, and since $f(0)=0$ it follows that $f\vert_{\Frac(R)} : \Frac(R) \to \Frac(S)$ is an isomorphism. It doesn't follow that $R \cong S$. That's why I think that a counter-example may come from such examples . See also here . Let $$R=\frac{\mathbb{C}[x,y,z]}{(xy - (1 - z^2))}, \quad S=\frac{\mathbb{C}[x,y,z]}{(x^2y - (1 - z^2))}$$ Apparently, $R \not \cong S$ (see the previous link). It looks like $R,S$ are domains. But do we have $\Frac(R) \not \cong \Frac(S)$ ? It would solve my problem. Or is there an easier way to solve it? Notice that the ""reverse"" question is easy to solve $\Bbb Z \not \cong \Bbb Q$, but $\Frac(\Bbb Z)[X] \cong \Bbb Q[X]$. Thank you for your help.",,"['abstract-algebra', 'polynomials', 'ring-theory']"
44,characteristic polynomial of an element of a finite flat $R$-algebra,characteristic polynomial of an element of a finite flat -algebra,R,"A book I'm reading says this: Let $R$ be a noetherian ring, and $B$ a finite locally free $R$-algebra. Since $B$ is locally free, for every $b\in B$, multiplication by $b$ gives an $R$-linear endomorphism of $B$ as a locally free $R$-module, and consider its characteristic polynomial $$P_b(T) := \text{det}(T-b)\in R[T]$$ Ie, we cover Spec $R$ with open neighborhoods $U_i$ where $B$ is free, and on each $U_i$ we pick a basis, compute the characteristic polynomial there, and using basis-invariance of the char poly, we glue them together to obtain the char poly ""$P_b(T) := \text{det}(T-b)$"" in $R[T]$. Is there a good reference/book that discusses the properties of this characteristic polynomial? (I'm of course familiar with the situation where $R$ is a field. I'm just wondering what carries over from the field case to our case here). For example, are the roots of $P_b(T)$ precisely the values of $R$ for which there exists a nonzero $x\in B$ with $bx = rx$? Is $P_b(b) = 0$? Intuitively in terms of algebraic geometry, what does this characteristic polynomial represent/tell you?","A book I'm reading says this: Let $R$ be a noetherian ring, and $B$ a finite locally free $R$-algebra. Since $B$ is locally free, for every $b\in B$, multiplication by $b$ gives an $R$-linear endomorphism of $B$ as a locally free $R$-module, and consider its characteristic polynomial $$P_b(T) := \text{det}(T-b)\in R[T]$$ Ie, we cover Spec $R$ with open neighborhoods $U_i$ where $B$ is free, and on each $U_i$ we pick a basis, compute the characteristic polynomial there, and using basis-invariance of the char poly, we glue them together to obtain the char poly ""$P_b(T) := \text{det}(T-b)$"" in $R[T]$. Is there a good reference/book that discusses the properties of this characteristic polynomial? (I'm of course familiar with the situation where $R$ is a field. I'm just wondering what carries over from the field case to our case here). For example, are the roots of $P_b(T)$ precisely the values of $R$ for which there exists a nonzero $x\in B$ with $bx = rx$? Is $P_b(b) = 0$? Intuitively in terms of algebraic geometry, what does this characteristic polynomial represent/tell you?",,['abstract-algebra']
45,Invertible matrices over ring of formal Laurent series,Invertible matrices over ring of formal Laurent series,,"Let $A$ be a commutative ring, let $A[[t]]$ be the ring of formal power series and consider the ring of formal Laurent series $A((t)) = A[[t]][t^{-1}]$. I would like to know: What is $Gl_n(A((t)))$, the group of invertible matrices with entries in the formal Laurent series over A? Let me give some background to clarify what kind of answer I am looking for: One can verify that for any $n\geq 1$, the ring of matrices $M_n$ with entries in $A[[t]]$ is isomorphic to the formal power series with coefficients in $M_n(A)$, via \begin{align} M_n(A[[t]]) \cong & \; M_n(A)[[t]] \\ \left (\sum_k a^{(k)}_{i,j}t^k \right)_{i,j} \mapsto & \sum_k (a_{i,j}^{(k)})_{i,j} \,t^k.\end{align} Using the well known result that a power series over any ring is invertible if and only if the first coefficient is invertible, we see from this that the invertible matrices $Gl_n(A[[t]])$ are precisely those for which the first coefficient of the corresponding series is invertible: $$Gl_n(A[[t]]) = Gl_n(A) + t\cdot M_n(A)[[t]]$$ One could see this as a generalization of the mentioned one-dimensional case $A[[t]]^\times=A^\times + tA[[t]]$. I am interested in finding a similar description for the situation where $A[[t]]$ is replaced by the ring of formal Laurent series $A((t)) = A[[t]][t^{-1}]$. In particular, I would like to get a feeling for: How ""large"" is $Gl_n(A((t)))$ compared to $Gl_n(A[[t]])$? Here are some thoughts: While I think we still get an isomorphism $$M_n(A((t))) \cong M_n(A)((t)),$$ it seems much harder to determine what the units of the right hand side are, because $M_n(A)$ has many zero divisors. As an example, consider the matrix  $$\pmatrix{1&0\\ 0&t} \mapsto  \pmatrix{1&0\\ 0&0} + \pmatrix{0&0\\ 0&1} t.$$  This element of $M_n(A)((t))$ clearly has an inverse, although all coefficients are zero-divisors. As for the second question, in case that $A$ is an integral domain, for $n=1$ we know that $A((t))^\times/A[[t]]^\times=\mathbb{Z}$. In general, $Gl_n(A[[t]])$ is not a normal subgroup anymore, but I was hoping that one could still say something about coset representatives of $Gl_n(A((t)))/Gl_n(A[[t]])$. I came to this question when I was trying to find out which finite locally free $A((t))$-modules come from locally free $A[[t]]$-modules.","Let $A$ be a commutative ring, let $A[[t]]$ be the ring of formal power series and consider the ring of formal Laurent series $A((t)) = A[[t]][t^{-1}]$. I would like to know: What is $Gl_n(A((t)))$, the group of invertible matrices with entries in the formal Laurent series over A? Let me give some background to clarify what kind of answer I am looking for: One can verify that for any $n\geq 1$, the ring of matrices $M_n$ with entries in $A[[t]]$ is isomorphic to the formal power series with coefficients in $M_n(A)$, via \begin{align} M_n(A[[t]]) \cong & \; M_n(A)[[t]] \\ \left (\sum_k a^{(k)}_{i,j}t^k \right)_{i,j} \mapsto & \sum_k (a_{i,j}^{(k)})_{i,j} \,t^k.\end{align} Using the well known result that a power series over any ring is invertible if and only if the first coefficient is invertible, we see from this that the invertible matrices $Gl_n(A[[t]])$ are precisely those for which the first coefficient of the corresponding series is invertible: $$Gl_n(A[[t]]) = Gl_n(A) + t\cdot M_n(A)[[t]]$$ One could see this as a generalization of the mentioned one-dimensional case $A[[t]]^\times=A^\times + tA[[t]]$. I am interested in finding a similar description for the situation where $A[[t]]$ is replaced by the ring of formal Laurent series $A((t)) = A[[t]][t^{-1}]$. In particular, I would like to get a feeling for: How ""large"" is $Gl_n(A((t)))$ compared to $Gl_n(A[[t]])$? Here are some thoughts: While I think we still get an isomorphism $$M_n(A((t))) \cong M_n(A)((t)),$$ it seems much harder to determine what the units of the right hand side are, because $M_n(A)$ has many zero divisors. As an example, consider the matrix  $$\pmatrix{1&0\\ 0&t} \mapsto  \pmatrix{1&0\\ 0&0} + \pmatrix{0&0\\ 0&1} t.$$  This element of $M_n(A)((t))$ clearly has an inverse, although all coefficients are zero-divisors. As for the second question, in case that $A$ is an integral domain, for $n=1$ we know that $A((t))^\times/A[[t]]^\times=\mathbb{Z}$. In general, $Gl_n(A[[t]])$ is not a normal subgroup anymore, but I was hoping that one could still say something about coset representatives of $Gl_n(A((t)))/Gl_n(A[[t]])$. I came to this question when I was trying to find out which finite locally free $A((t))$-modules come from locally free $A[[t]]$-modules.",,"['abstract-algebra', 'laurent-series', 'formal-power-series']"
46,$|G:H|=p^n$ means $O_p(H)\leq O_p(G)$?,means ?,|G:H|=p^n O_p(H)\leq O_p(G),"Let $H\leq G$ (finite group) and $|G:H|=p^n$, ($p$ is a prime number) prove that: $$O_p(H)\leq O_p(G)$$ note: $O_p(G)$ defined as the intersection of all Sylow-$p$ groups in $G$ I try to prove $G_p\cap H\in Sylow_p H$, however it's not obvious and maybe wrong. And the proposition is definitely true if $H$ is normal( if $H$ is normal, we don't need the condition: $|G:H|=p^n$).","Let $H\leq G$ (finite group) and $|G:H|=p^n$, ($p$ is a prime number) prove that: $$O_p(H)\leq O_p(G)$$ note: $O_p(G)$ defined as the intersection of all Sylow-$p$ groups in $G$ I try to prove $G_p\cap H\in Sylow_p H$, however it's not obvious and maybe wrong. And the proposition is definitely true if $H$ is normal( if $H$ is normal, we don't need the condition: $|G:H|=p^n$).",,"['abstract-algebra', 'group-theory', 'finite-groups']"
47,index 2 subgroups of the infinite product of Z/2Z,index 2 subgroups of the infinite product of Z/2Z,,"Is it possible to describe all the index 2 subgroups of the group $G = \prod_{i\in \mathbb{N}}\; \mathbb{Z}/2\mathbb{Z}$? For example, one can take the kernel of the $i$-th projection map $\pi_i\colon G\to \mathbb{Z}/2\mathbb{Z}$. More generally, by forming linear combinations of these projection maps we get other surjective homomorphisms $G\to \mathbb{Z}/2\mathbb{Z}$. But still, there are many more index 2 subgroups which do not appear as kernels of the maps described above. One easy argument to see this is to equip $G$ with the product ring structure so that the group structure is the underlying additive group of this ring. Then the direct sum $I = \bigoplus_{i\in \mathbb{N}} \mathbb{Z}/2\mathbb{Z}$ is a proper ideal of $G$, so by Krull's theorem we can find a prime ideal $\mathfrak p$ of $G$ containing $I$. Since $G$ is Boolean every prime ideal is maximal and of index 2. But $\mathfrak p$ cannot be obtained as the kernel of one of the projection maps given before. I am aware of the fact that $\text{Spec}(G)$ can be described using ultrafilters. So now we also have these subgroups of index 2. But are there still more of them? Any information related to the above is appreciated.  Thanks in advance, AYK.","Is it possible to describe all the index 2 subgroups of the group $G = \prod_{i\in \mathbb{N}}\; \mathbb{Z}/2\mathbb{Z}$? For example, one can take the kernel of the $i$-th projection map $\pi_i\colon G\to \mathbb{Z}/2\mathbb{Z}$. More generally, by forming linear combinations of these projection maps we get other surjective homomorphisms $G\to \mathbb{Z}/2\mathbb{Z}$. But still, there are many more index 2 subgroups which do not appear as kernels of the maps described above. One easy argument to see this is to equip $G$ with the product ring structure so that the group structure is the underlying additive group of this ring. Then the direct sum $I = \bigoplus_{i\in \mathbb{N}} \mathbb{Z}/2\mathbb{Z}$ is a proper ideal of $G$, so by Krull's theorem we can find a prime ideal $\mathfrak p$ of $G$ containing $I$. Since $G$ is Boolean every prime ideal is maximal and of index 2. But $\mathfrak p$ cannot be obtained as the kernel of one of the projection maps given before. I am aware of the fact that $\text{Spec}(G)$ can be described using ultrafilters. So now we also have these subgroups of index 2. But are there still more of them? Any information related to the above is appreciated.  Thanks in advance, AYK.",,"['abstract-algebra', 'group-theory']"
48,Classify groups of order 171,Classify groups of order 171,,"This is a problem from Stanford Algebra Qualifying Exam, Fall 1998. I know the standard way is to use Sylow theorems and semidirect product. $171 = 9\cdot 19$. By Sylow theorems, $n_3|19$ and $n_3\equiv 1\text{ mod }3$, hence $n_3 = 1\text{ or }19$. $n_{19}|9$ and $n_{19}\equiv 1\text{ mod }19$, hence $n_{19} = 1$. Denote one Sylow $3$-subgroup by $K$ and one Sylow $19$-subgroup by $N$. $N$ is normal because $n_{19} = 1$. $NK = G, N\cap K = \{e\}$ and $N$ is normal. Hence, $G$ is the semiproduct of $G = N\rtimes K$. Case 1: $G$ is abelian. Since a finite abelian group is isomorphic to direct product of cyclic groups of prime power orders, $G\cong C_9\times C_{19}$ or $C\cong C_3\times C_3\times C_{19}$. Case 2: $G$ is not abelian. The structure of $G$ is determined by $N, K$ and homomorphism $\varphi: K\to\text{Aut}(N)$, which stands for conjugation. What's more, $N, K, \varphi$ and $N, K,\varphi'$ define two isomorphic groups if there exists automorphism $\alpha:K\to K$ such that $\varphi' = \varphi\circ\alpha$. Since $N\cong C_{19}$, $\text{Aut}(N)\cong C_{18}$. Since $G$ is not abelian, the image $\varphi(K)$ is not the trivial group.  Denote the generating element in $\text{Aut}(N)\cong C_{18}$ by $z$. When $K\cong C_9$, there are two possiblities for $\varphi(K)$: $\varphi(K) = \{1, z^6, z^{12}\}$ or $\varphi(K) = \{1, z^2, z^4,\ldots,z^{16}\}$. When $C\cong C_3\times C_3$, $\varphi(K)$ can only be $\{1,z^6,z^{12}\}$. What's more, given $\varphi,\varphi':K\to C_{18}$ with $\varphi(K) = \varphi'(K)$, there exists automorphism $\alpha:K\to K$ such that $\varphi' = \varphi\circ\alpha$. Therefore, when $G$ is nonabelain, it has two isomorphism classes when $K\cong C_9$ and one isomorphism class when $K\cong C_3\times C_3$. I think I've solved this problem. Thanks for your help. Welcome for any comment.","This is a problem from Stanford Algebra Qualifying Exam, Fall 1998. I know the standard way is to use Sylow theorems and semidirect product. $171 = 9\cdot 19$. By Sylow theorems, $n_3|19$ and $n_3\equiv 1\text{ mod }3$, hence $n_3 = 1\text{ or }19$. $n_{19}|9$ and $n_{19}\equiv 1\text{ mod }19$, hence $n_{19} = 1$. Denote one Sylow $3$-subgroup by $K$ and one Sylow $19$-subgroup by $N$. $N$ is normal because $n_{19} = 1$. $NK = G, N\cap K = \{e\}$ and $N$ is normal. Hence, $G$ is the semiproduct of $G = N\rtimes K$. Case 1: $G$ is abelian. Since a finite abelian group is isomorphic to direct product of cyclic groups of prime power orders, $G\cong C_9\times C_{19}$ or $C\cong C_3\times C_3\times C_{19}$. Case 2: $G$ is not abelian. The structure of $G$ is determined by $N, K$ and homomorphism $\varphi: K\to\text{Aut}(N)$, which stands for conjugation. What's more, $N, K, \varphi$ and $N, K,\varphi'$ define two isomorphic groups if there exists automorphism $\alpha:K\to K$ such that $\varphi' = \varphi\circ\alpha$. Since $N\cong C_{19}$, $\text{Aut}(N)\cong C_{18}$. Since $G$ is not abelian, the image $\varphi(K)$ is not the trivial group.  Denote the generating element in $\text{Aut}(N)\cong C_{18}$ by $z$. When $K\cong C_9$, there are two possiblities for $\varphi(K)$: $\varphi(K) = \{1, z^6, z^{12}\}$ or $\varphi(K) = \{1, z^2, z^4,\ldots,z^{16}\}$. When $C\cong C_3\times C_3$, $\varphi(K)$ can only be $\{1,z^6,z^{12}\}$. What's more, given $\varphi,\varphi':K\to C_{18}$ with $\varphi(K) = \varphi'(K)$, there exists automorphism $\alpha:K\to K$ such that $\varphi' = \varphi\circ\alpha$. Therefore, when $G$ is nonabelain, it has two isomorphism classes when $K\cong C_9$ and one isomorphism class when $K\cong C_3\times C_3$. I think I've solved this problem. Thanks for your help. Welcome for any comment.",,"['abstract-algebra', 'group-theory', 'proof-verification', 'finite-groups', 'groups-enumeration']"
49,When is a group the product of its subgroup and their factor group?,When is a group the product of its subgroup and their factor group?,,"If G is a group and H a normal subgroup of G, under what conditions is it true that  $$H \times (G/H) =G ?$$ To be more specific: Characterise groups G which allow proper non-trivial normal subgroups H such that $H \times G/H =G.$ Given a group G which allow such a decomposition, characterise (normal) subgroups H of G such that $H \times G/H =G.$ Clearly, simple groups allow no such factorisation except the trivial ones. And the smallest non-Abelian group $S_3$ cannot be the product of its (normal) subgroup $A_3$ and the quotient $\mathbb{Z}_2$. Also, it follows from the structure theorem for finitely generated Abelian groups that cyclic groups with order a power of a prime do not allow any non-trivial decomposition. It is also not difficult to see that $\mathbb Z$ is not decomposable, i.e., expressible as the product of two proper non-trivial subgroups. These two facts together with the full force of the structure theorem tells us precisely which finitely generated Abelian groups are decomposable, and also enables us to list all the decompositions. To summarise, in suggestive language, to what extent is the group-quotient operation an inverse for the group-product operation?","If G is a group and H a normal subgroup of G, under what conditions is it true that  $$H \times (G/H) =G ?$$ To be more specific: Characterise groups G which allow proper non-trivial normal subgroups H such that $H \times G/H =G.$ Given a group G which allow such a decomposition, characterise (normal) subgroups H of G such that $H \times G/H =G.$ Clearly, simple groups allow no such factorisation except the trivial ones. And the smallest non-Abelian group $S_3$ cannot be the product of its (normal) subgroup $A_3$ and the quotient $\mathbb{Z}_2$. Also, it follows from the structure theorem for finitely generated Abelian groups that cyclic groups with order a power of a prime do not allow any non-trivial decomposition. It is also not difficult to see that $\mathbb Z$ is not decomposable, i.e., expressible as the product of two proper non-trivial subgroups. These two facts together with the full force of the structure theorem tells us precisely which finitely generated Abelian groups are decomposable, and also enables us to list all the decompositions. To summarise, in suggestive language, to what extent is the group-quotient operation an inverse for the group-product operation?",,"['abstract-algebra', 'group-theory']"
50,How do I prove that $S_A\cong S_B\implies |A|=|B|$?,How do I prove that ?,S_A\cong S_B\implies |A|=|B|,"Let $A,B$ be infinite sets such that $S_A\cong S_B$. (Symmetric groups are group isomorphic) How do I prove that $|A|=|B|$? The only proof I know uses Axiom of choice. (That is, using AC to give orders to infinite family of finite sets) Is there a way to prove this not invoking AC?","Let $A,B$ be infinite sets such that $S_A\cong S_B$. (Symmetric groups are group isomorphic) How do I prove that $|A|=|B|$? The only proof I know uses Axiom of choice. (That is, using AC to give orders to infinite family of finite sets) Is there a way to prove this not invoking AC?",,"['abstract-algebra', 'axiom-of-choice']"
51,Axioms as recreational mathematics,Axioms as recreational mathematics,,"Before modern group theory, mathematicians studied concrete permutation groups: algebraically closed subsets of the set of all bijections on a set $X$ in which all inverses was included. This was the kind of groups studied by Galois and others. Then the axioms were abstracted from the concrete example and it was proved that any abstract group was isomorphic to a concrete group. I'm trying to repeat this process with different concrete structures on sets, as  $\mathcal P(X)$ and $\text{End}(X)$, just for fun . Both of these examples seems more complicated, however. $\text{Aut}(X)$ rather obvious boils down to an associative composition $\circ$, an identity $e$ and an inverse $f^{-1}$ for all functions $f\in\text{Aut}(X)$. But how to deal with $\text{End}(X)$? Again there is an associative composition $\circ$ and an identity $e$, which point towards monoids. But here it seems to exist hidden structures: unlike for bijections (and surjections), there is a non-trivial function $\text{Im}:\text{End}(X)\to \mathcal P(X)$, that makes algebraically closed subsets (including $e$) of $\text{End}(X)$ to somewhat special monoids. And the question is, how to formulate the axioms for this concrete structure? One idea would be to study a monoid action on sets $$\text{End}(X)\times\mathcal P(X)\to\mathcal P(X),\;(f,A)\mapsto f(A)$$ but that ends up in finding axioms for the $\mathcal P(X)$-structure with it's own more or less hidden structures. Is there a way to abstract axioms for $\text{End}(X)$ without dealing with $\mathcal P(X)$? I'm interested in canonically pre-structured sets. The simplest one is $X\times X$ with the structures  $\displaystyle X\times X\overset {p_1,p_2}{\longrightarrow} X,\;p_i(x_1,x_2)=x_i$.  Obviously there are no more structures. For the sets of all bijections $Bij(X)$ or surjections $Sur(X)$ on a set $X$ it is rather obvious that there is one canonical structure: $Bij(X)\times Bij(X)\to Bij(X)$ etc. Now suppose the example $N(X)$ has a binary $N(X)\times N(X)\to N(X)$ structure and a topological structure, canonically. If you're only interested in one of those structures you might describe the axioms for that structure and try to show that all the abstract objects are isomorphic with a concrete subset of $N(X)$. But it might also be of interest to find the axioms for both structures and their interaction. An abstract monoid doesn't have an extra structure corresponding to $\text{Im}:\text{End}(X)\to \mathcal P(X)$, but the functions in End$(X)$ do have it.","Before modern group theory, mathematicians studied concrete permutation groups: algebraically closed subsets of the set of all bijections on a set $X$ in which all inverses was included. This was the kind of groups studied by Galois and others. Then the axioms were abstracted from the concrete example and it was proved that any abstract group was isomorphic to a concrete group. I'm trying to repeat this process with different concrete structures on sets, as  $\mathcal P(X)$ and $\text{End}(X)$, just for fun . Both of these examples seems more complicated, however. $\text{Aut}(X)$ rather obvious boils down to an associative composition $\circ$, an identity $e$ and an inverse $f^{-1}$ for all functions $f\in\text{Aut}(X)$. But how to deal with $\text{End}(X)$? Again there is an associative composition $\circ$ and an identity $e$, which point towards monoids. But here it seems to exist hidden structures: unlike for bijections (and surjections), there is a non-trivial function $\text{Im}:\text{End}(X)\to \mathcal P(X)$, that makes algebraically closed subsets (including $e$) of $\text{End}(X)$ to somewhat special monoids. And the question is, how to formulate the axioms for this concrete structure? One idea would be to study a monoid action on sets $$\text{End}(X)\times\mathcal P(X)\to\mathcal P(X),\;(f,A)\mapsto f(A)$$ but that ends up in finding axioms for the $\mathcal P(X)$-structure with it's own more or less hidden structures. Is there a way to abstract axioms for $\text{End}(X)$ without dealing with $\mathcal P(X)$? I'm interested in canonically pre-structured sets. The simplest one is $X\times X$ with the structures  $\displaystyle X\times X\overset {p_1,p_2}{\longrightarrow} X,\;p_i(x_1,x_2)=x_i$.  Obviously there are no more structures. For the sets of all bijections $Bij(X)$ or surjections $Sur(X)$ on a set $X$ it is rather obvious that there is one canonical structure: $Bij(X)\times Bij(X)\to Bij(X)$ etc. Now suppose the example $N(X)$ has a binary $N(X)\times N(X)\to N(X)$ structure and a topological structure, canonically. If you're only interested in one of those structures you might describe the axioms for that structure and try to show that all the abstract objects are isomorphic with a concrete subset of $N(X)$. But it might also be of interest to find the axioms for both structures and their interaction. An abstract monoid doesn't have an extra structure corresponding to $\text{Im}:\text{End}(X)\to \mathcal P(X)$, but the functions in End$(X)$ do have it.",,"['abstract-algebra', 'elementary-set-theory', 'recreational-mathematics', 'boolean-algebra', 'axioms']"
52,Can we ascertain that there exists an epimorphism $G\rightarrow H$?,Can we ascertain that there exists an epimorphism ?,G\rightarrow H,"Let $G,H$ be finite groups. Suppose we have an epimorphism $$G\times G\rightarrow H\times H$$  Can we find an epimorphism $G\rightarrow H$?","Let $G,H$ be finite groups. Suppose we have an epimorphism $$G\times G\rightarrow H\times H$$  Can we find an epimorphism $G\rightarrow H$?",,"['group-theory', 'finite-groups', 'abstract-algebra']"
53,Simple Modules over the Weyl Algebra,Simple Modules over the Weyl Algebra,,"Let $k$ be a field of characteristic zero and let $A_1=k\langle x,y| \, xy-yx=1 \rangle$ be the Weyl algebra. Is there a (more or less explicit) possibility of writing down all simple modules over $A_1$ (by which I mean all left $A_1$-modules that have no nontrivial left $A_1$-submodule)?","Let $k$ be a field of characteristic zero and let $A_1=k\langle x,y| \, xy-yx=1 \rangle$ be the Weyl algebra. Is there a (more or less explicit) possibility of writing down all simple modules over $A_1$ (by which I mean all left $A_1$-modules that have no nontrivial left $A_1$-submodule)?",,"['abstract-algebra', 'operator-theory', 'noncommutative-algebra']"
54,The order of $H$ is relatively prime to its index $[G:H]$ [closed],The order of  is relatively prime to its index  [closed],H [G:H],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Suppose that a subgroup $H$ of a finite group $G$ satisfies one of the following two conditions: (i) For any nonidentity element $x$ of $H$ we have $C_{G}(x) \subset H$ (ii) If $K$ is a subgroup of $H$ and $K \neq 1$, then $N_{G}(K) \subset H$ Prove that the order of $H$ is relatively prime to its index $[G:H]$ Thanks in advance","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Suppose that a subgroup $H$ of a finite group $G$ satisfies one of the following two conditions: (i) For any nonidentity element $x$ of $H$ we have $C_{G}(x) \subset H$ (ii) If $K$ is a subgroup of $H$ and $K \neq 1$, then $N_{G}(K) \subset H$ Prove that the order of $H$ is relatively prime to its index $[G:H]$ Thanks in advance",,"['abstract-algebra', 'group-theory', 'finite-groups']"
55,Showing that a p-group has a normal subgroup for each divisor of its order,Showing that a p-group has a normal subgroup for each divisor of its order,,"I'd appreciate input on this proof.  Does it successfully prove the result? ""Let $P$ be a finite $p$-group for some prime $p$, and let $q$ be a divisor of $|P|$.  Show that $P$ has a normal subgroup of order $q$."" We will use induction on $n$, where $|P|=p^n$.  Clearly the result is true if $|P|=p^1$.  Now assume the statement is true for all groups of order $p^k$ where $k<n$. Since $P$ is a $p$-group it has nontrivial center, and $Z(P)$ is also a $p$-group.  It follows that $P$ has a normal subgroup of order $p$, say $N$.  Form the quotient group $P/N$, which has order $p^{n-1}$, and therefore has a normal subgroup of order $q$ for each divisor $q$ of $p^{n-1}$ by the induction hypothesis. We can show that $P$ has a normal subgroup of index $i$ for $1 \le i \le p^n$.  Clearly this is true for $i=1,p^n$, and $|P:N|=p^{n-1}$.  Consider the canonical homomorphism $\pi:P \to P/N$, which is surjective.  Then by the Correspondence Theorem, $\pi$ and $\pi^{-1}$ are inverse bijections between subgroups of $P/N$ and subgroups of $P$ containing $N$, that respect normality and index.  Then since $P/N$ (by the induction hypothesis) has a normal subgroup of index $p \le i \le p^{n-2}$, so does $P$, and the result follows. Thanks.","I'd appreciate input on this proof.  Does it successfully prove the result? ""Let $P$ be a finite $p$-group for some prime $p$, and let $q$ be a divisor of $|P|$.  Show that $P$ has a normal subgroup of order $q$."" We will use induction on $n$, where $|P|=p^n$.  Clearly the result is true if $|P|=p^1$.  Now assume the statement is true for all groups of order $p^k$ where $k<n$. Since $P$ is a $p$-group it has nontrivial center, and $Z(P)$ is also a $p$-group.  It follows that $P$ has a normal subgroup of order $p$, say $N$.  Form the quotient group $P/N$, which has order $p^{n-1}$, and therefore has a normal subgroup of order $q$ for each divisor $q$ of $p^{n-1}$ by the induction hypothesis. We can show that $P$ has a normal subgroup of index $i$ for $1 \le i \le p^n$.  Clearly this is true for $i=1,p^n$, and $|P:N|=p^{n-1}$.  Consider the canonical homomorphism $\pi:P \to P/N$, which is surjective.  Then by the Correspondence Theorem, $\pi$ and $\pi^{-1}$ are inverse bijections between subgroups of $P/N$ and subgroups of $P$ containing $N$, that respect normality and index.  Then since $P/N$ (by the induction hypothesis) has a normal subgroup of index $p \le i \le p^{n-2}$, so does $P$, and the result follows. Thanks.",,"['abstract-algebra', 'group-theory', 'proof-verification']"
56,Free Graded Commutative Algebra on a Graded Vector Space,Free Graded Commutative Algebra on a Graded Vector Space,,"Let $V$ be a graded vector space, thought of as a collection $\{ V^n \}_{n \ge 0}$ of vector spaces. Let $V_{odd} = \bigoplus_{n \text{ odd}} V^n$ and $V_{even} = \bigoplus_{n \text{ even}} V^n$. I am trying to show that there is a functor $$ \Lambda \colon \mathsf{GVec} \longrightarrow \mathsf{CGAlg} $$ between graded vector spaces, and commutative graded algebras which is left adjoint to the forgetful functor - i.e. $\Lambda V$ should be the free commutative graded algebra on $V$. One way to define this is $\Lambda V = E( V_{odd} ) \otimes S(V_{even} )$ where $E$ denotes the exterior algebra and $S$ the symmetric algebra. However I am having trouble showing that it is commutative. We can think of the exterior and symmetric algebras as polynomial algebras on basis elements of each of $V_{odd}$, $V_{even}$ respectively, subject to some relations. Let $\underline{x} \otimes \underline{y}, \underline{z} \otimes \underline{w}$ be elements in $\Lambda V \otimes \Lambda V$, where the underline means a monic monomial. Multiplying these gives $$ (\underline{x} \otimes \underline{y} ) \cdot (\underline{z} \otimes \underline{w}) = (-1)^{|\underline{y}| | \underline{z}|} \underline{x} \underline{z} \otimes \underline{y} \underline{w} \ \ \ \ \ \ \ \ \ \ \ \ \ (1) $$ by definition of the tensor product of graded algebras. However if we apply the braid to the element $\underline{x} \otimes \underline{y}\otimes \underline{z} \otimes \underline{w}$ this gives $$ (-1)^{| \underline{x} \otimes \underline{y}||\underline{z}  \otimes \underline{w}| } \underline{z} \otimes \underline{w} \otimes \underline{x} \otimes \underline{y} = (-1)^{(| \underline{x}| +  |\underline{y}|)(|\underline{z}\ + |\underline{w}| )} \underline{z} \otimes \underline{w} \otimes \underline{x} \otimes \underline{y} $$ then multiply we get $$ (-1)^{(| \underline{x}| +  |\underline{y}|)(|\underline{z}| + |\underline{w}| )} (-1)^{|\underline{w}| |\underline{x}|}\underline{z} \underline{x}\otimes \underline{w} \underline{y} $$ We can swap $\underline w$ and $\underline y$ since they live in the symmetric algebra, and can swap $\underline z$ and $\underline x$ after multiplying by $(-1)^{| \underline x| |\underline z | }$ so the above expression simplifies to $$ (-1)^{|\underline y| |\underline z| + |\underline y| |\underline w |} \underline{x} \underline{z}\otimes \underline{y} \underline{w}  \ \ \ \ \ \ \ \ \ \ \ \ \ (2) $$ (1) and (2) should be the same. What have I done wrong? EDIT: Just to be clear I am trying to show that $\mu \sigma = \mu$ where $\sigma$ in $\mathsf{GVec}$ is the braiding $a \otimes b \mapsto (-1)^{|a||b|}b \otimes a$ and $\mu$ is the multiplication.","Let $V$ be a graded vector space, thought of as a collection $\{ V^n \}_{n \ge 0}$ of vector spaces. Let $V_{odd} = \bigoplus_{n \text{ odd}} V^n$ and $V_{even} = \bigoplus_{n \text{ even}} V^n$. I am trying to show that there is a functor $$ \Lambda \colon \mathsf{GVec} \longrightarrow \mathsf{CGAlg} $$ between graded vector spaces, and commutative graded algebras which is left adjoint to the forgetful functor - i.e. $\Lambda V$ should be the free commutative graded algebra on $V$. One way to define this is $\Lambda V = E( V_{odd} ) \otimes S(V_{even} )$ where $E$ denotes the exterior algebra and $S$ the symmetric algebra. However I am having trouble showing that it is commutative. We can think of the exterior and symmetric algebras as polynomial algebras on basis elements of each of $V_{odd}$, $V_{even}$ respectively, subject to some relations. Let $\underline{x} \otimes \underline{y}, \underline{z} \otimes \underline{w}$ be elements in $\Lambda V \otimes \Lambda V$, where the underline means a monic monomial. Multiplying these gives $$ (\underline{x} \otimes \underline{y} ) \cdot (\underline{z} \otimes \underline{w}) = (-1)^{|\underline{y}| | \underline{z}|} \underline{x} \underline{z} \otimes \underline{y} \underline{w} \ \ \ \ \ \ \ \ \ \ \ \ \ (1) $$ by definition of the tensor product of graded algebras. However if we apply the braid to the element $\underline{x} \otimes \underline{y}\otimes \underline{z} \otimes \underline{w}$ this gives $$ (-1)^{| \underline{x} \otimes \underline{y}||\underline{z}  \otimes \underline{w}| } \underline{z} \otimes \underline{w} \otimes \underline{x} \otimes \underline{y} = (-1)^{(| \underline{x}| +  |\underline{y}|)(|\underline{z}\ + |\underline{w}| )} \underline{z} \otimes \underline{w} \otimes \underline{x} \otimes \underline{y} $$ then multiply we get $$ (-1)^{(| \underline{x}| +  |\underline{y}|)(|\underline{z}| + |\underline{w}| )} (-1)^{|\underline{w}| |\underline{x}|}\underline{z} \underline{x}\otimes \underline{w} \underline{y} $$ We can swap $\underline w$ and $\underline y$ since they live in the symmetric algebra, and can swap $\underline z$ and $\underline x$ after multiplying by $(-1)^{| \underline x| |\underline z | }$ so the above expression simplifies to $$ (-1)^{|\underline y| |\underline z| + |\underline y| |\underline w |} \underline{x} \underline{z}\otimes \underline{y} \underline{w}  \ \ \ \ \ \ \ \ \ \ \ \ \ (2) $$ (1) and (2) should be the same. What have I done wrong? EDIT: Just to be clear I am trying to show that $\mu \sigma = \mu$ where $\sigma$ in $\mathsf{GVec}$ is the braiding $a \otimes b \mapsto (-1)^{|a||b|}b \otimes a$ and $\mu$ is the multiplication.",,"['abstract-algebra', 'category-theory', 'exterior-algebra']"
57,"$A\subseteq B\subseteq C$ ring extensions, $A\subseteq C$ finite/finitely-generated $\Rightarrow$ $A\subseteq B$ finite/finitely-generated?","ring extensions,  finite/finitely-generated   finite/finitely-generated?",A\subseteq B\subseteq C A\subseteq C \Rightarrow A\subseteq B,"Let $A\subseteq B\subseteq C$ be commutative unital rings. Recall that the extension $A \subseteq B$ is finite / of finite type / integral , when $B$ is a finitely generated $R$-module / when $B$ is a finitely generated $A$-algebra / when $\forall b \in B$ $\exists$ monic polynomial $f \in A[x]$ with $f(b)=0$. Notation $_AB$ means ""$A$-module $B$"". We know that $A\subseteq C$ is integral iff $A\subseteq B$ and $B\subseteq C$ are integral (Grillet, Abstract Algebra , 7.3.3). Do we also have the following: $A\subseteq C$ is finite $\Leftrightarrow$ $A\subseteq B$ and $B\subseteq C$ are finite. $A\subseteq C$ is of finite type $\Leftrightarrow$ $A\subseteq B$ and $B\subseteq C$ are of finite type. I'm having problems with ($A\subseteq C$ finite $\Rightarrow$ $A\subseteq B$ finite) and with ($A\subseteq C$ of finite type $\Rightarrow$ $A\subseteq B$ of finite type). If $_AB$ is a direct summand of $_AC$ (for example when $A$ is a field), i.e. $_AC= _A B\oplus _A B'$ for some submodule $B'$ of $C$, then $C=Ac_1+\cdots+Ac_n$ implies $c_i=b_i+b'_i$ for some $b_i\in B$ and $b'_i \in B$, hence $B = Ab_1 + \cdots + Ab_n$. But what if $_AB$ is not a direct summand of $_AC$? And what about the 'finite type' case?","Let $A\subseteq B\subseteq C$ be commutative unital rings. Recall that the extension $A \subseteq B$ is finite / of finite type / integral , when $B$ is a finitely generated $R$-module / when $B$ is a finitely generated $A$-algebra / when $\forall b \in B$ $\exists$ monic polynomial $f \in A[x]$ with $f(b)=0$. Notation $_AB$ means ""$A$-module $B$"". We know that $A\subseteq C$ is integral iff $A\subseteq B$ and $B\subseteq C$ are integral (Grillet, Abstract Algebra , 7.3.3). Do we also have the following: $A\subseteq C$ is finite $\Leftrightarrow$ $A\subseteq B$ and $B\subseteq C$ are finite. $A\subseteq C$ is of finite type $\Leftrightarrow$ $A\subseteq B$ and $B\subseteq C$ are of finite type. I'm having problems with ($A\subseteq C$ finite $\Rightarrow$ $A\subseteq B$ finite) and with ($A\subseteq C$ of finite type $\Rightarrow$ $A\subseteq B$ of finite type). If $_AB$ is a direct summand of $_AC$ (for example when $A$ is a field), i.e. $_AC= _A B\oplus _A B'$ for some submodule $B'$ of $C$, then $C=Ac_1+\cdots+Ac_n$ implies $c_i=b_i+b'_i$ for some $b_i\in B$ and $b'_i \in B$, hence $B = Ab_1 + \cdots + Ab_n$. But what if $_AB$ is not a direct summand of $_AC$? And what about the 'finite type' case?",,"['abstract-algebra', 'commutative-algebra']"
58,Orders of elements and homomorphisms.,Orders of elements and homomorphisms.,,"Corollary 4.6.8 There is a group $G$ of order $n^3$ given by $G= \{b^ic^ja^k \mid 0  i, j, k < n\}$, where $a$, $b$, and $c$ all have order $n$, and $b$ commutes with $c$, $a$ commutes with $c$, and $aba^{1} = bc$. Thus,   $(b^ic^ja^k) (b^{i'}c^{j'}a^{k'}) = b^{i+i'}c^{j+j'+ki'}a^{k+k'}$ The order of $(b^ic^ja^k)$ is n if n is odd and 2n if n is even. Now suppose that $s: K \rightarrow H \rtimes_{\alpha} K$ is given by s(k)=(e,k) and $i : H \rightarrow H \rtimes_{\alpha} K$ is given by $i(h)=(h,e)$. Proposition 4.6.9 Let  : K  Aut(H) be a homomorphism. Suppose given homomorphisms $f_1$ : H  G and $f_2$ : K  G for a group G. Then there is a homomorphism f : $H \rtimes_{\alpha}$ K  G with $f_1 = f$  i and $f_2$ = f  s if and only if $f_2(k)f_1(h)(f_2(k))^{1} = f_1(\alpha(k)(h))$ for all h  H and k  K. Such an f, if it exists, is unique, and is given in the HK notation by $f(hk) = f_1(h)f_2(k)$. a) In this problem we assume a familiarity with the group of invertible 33 matrices   over the ring $Z_n$. Let $$G = \left\{ \begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix} \mid \bar{x},\bar{y},\bar{z} \in Z_n \right\}$$ Show that $G$ forms a group under matrix multiplication. Show, using Proposition   4.6.9, that $G$ is isomorphic to the group constructed in Corollary 4.6.8. I already showed that $G$ is a group under matrix multiplication, but I'm having trouble with the second part of this problem. Let $G = \left\{ \begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix} \mid \bar{x}, \bar{y}, \bar{z} \in Z_n \right\}$ Let $G' = \{ b^ic^ja^k \mid 0 \leq i, j, k < n \}$ where a, b, c all have order n, and b commutes with c, a commutes with c, and $aba^{-1} = bc$. Thus, $$(b^ic^ja^k)(b^{i'}c^{j'}a^{k'}) = b^{i+i'}c^{j + j' + ki'}a^{k+k'}.$$ I want to show that G is isomorphic to G' by using proposition 4.6.9 However, in order to use that proposition, I need to come up with an H and a K, right? So... Let $H = \left\{ \begin{bmatrix} 1 & 0 & 0 \\0 & 1 & 0 \\0 & 0 & 1 \end{bmatrix} \right\}$ Let K=G Obviously $G \cong H \rtimes_{\alpha} K$ Let $f_1 : H \rightarrow G$ be the trivial homomorphism. Let $f_2: K \rightarrow G'$ be given by $f_2 (\begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix}) = b^ic^ja^k$ Now we need to show that $f_2(\begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix})f_1( \begin{bmatrix} 1 & 0 & 0 \\0 & 1 & 0 \\0 & 0 & 1 \end{bmatrix})f_2(\begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix}^{-1})$ = $f_1(\begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 \\0 & 1 & 0 \\0 & 0 & 1 \end{bmatrix}\begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix}^{-1})$ But, $$f_2(\begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix})f_1( \begin{bmatrix} 1 & 0 & 0 \\0 & 1 & 0 \\0 & 0 & 1 \end{bmatrix})f_2(\begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix}^{-1}) = (b^ic^ja^k)(e)(b^ic^ja^k)^{-1} = e$$ and $$f_1(\begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 \\0 & 1 & 0 \\0 & 0 & 1 \end{bmatrix}\begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix}^{-1}) = e$$ So they must be equal. So the homomorphism $f: H \rtimes_{\alpha} K \rightarrow G$ exists. So now we just need to show that this homomorphism is an isomorphism. I tried to show that by proving that $\begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix}$ has order n when n is odd and order 2n when n is even. I first tried to show the case when n is odd: Let n=2k+1 where $k \geq 0$ Base Case:  n=0 is trivial. We can also try n=3, $( \begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix})^3 = \begin{bmatrix} 1 & 3(\bar{x}) & 3(\bar{y}) + 3(\bar{xy}) \\0 & 1 & 3(\bar{z}) \\0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 \\0 & 1 & 0 \\0 & 0 & 1 \end{bmatrix} $ For the induction step, suppose for some 2w+1, an exponent of $\begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix}$ is 2w+1. We need to show that, for 2(w+1)+1 = 2w+3, an exponent of $ \begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix}$ must be 2w+3. By induction, we know that $$\begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix}^{2w+1} = \begin{bmatrix} 1 & (2w+1)\bar{x} & (2w+1)\bar{y}+(2w+1)\bar{xz} \\0 & 1 & (2w+1)\bar{z} \\0 & 0 & 1 \end{bmatrix}$$ However, $$\begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix}^{2w+3} = \begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix}^{2w+1} \begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix}^2 = \begin{bmatrix} 1 & (2w+1)\bar{x} & (2w+1)\bar{y}+(2w+1)\bar{xz} \\0 & 1 & (2w+1)\bar{z} \\0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 1 & 2\bar{x} & 2(\bar{y})+\bar{xz} \\0 & 1 & 2\bar{z} \\0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 2(\bar{x})+(2w+1)(\bar{x}) & 2(\bar{y})+\bar{xz}+(2w+1)(\bar{x})(2)(\bar{z})+ (2w+1)(\bar{xz})+(2w+1)\bar{y} \\0 & 1 & (2w+1)\bar{z}+2(\bar{z}) \\0 & 0 & 1 \end{bmatrix}$$ But that wasnt the result we were supposed to get, right? Because $2(\bar{y})+\bar{xz}+(2w+1)(\bar{x})(2)(\bar{z})+ (2w+1)(\bar{xz})+(2w+1)\bar{y} = (2w+3)\bar{y} +(6w+4)\bar{xz}$ and 6w+4 is not a multiple of 2w+3. So I was a bit confusedand I was wondering if anybody could help me with this. Thank you in advance","Corollary 4.6.8 There is a group $G$ of order $n^3$ given by $G= \{b^ic^ja^k \mid 0  i, j, k < n\}$, where $a$, $b$, and $c$ all have order $n$, and $b$ commutes with $c$, $a$ commutes with $c$, and $aba^{1} = bc$. Thus,   $(b^ic^ja^k) (b^{i'}c^{j'}a^{k'}) = b^{i+i'}c^{j+j'+ki'}a^{k+k'}$ The order of $(b^ic^ja^k)$ is n if n is odd and 2n if n is even. Now suppose that $s: K \rightarrow H \rtimes_{\alpha} K$ is given by s(k)=(e,k) and $i : H \rightarrow H \rtimes_{\alpha} K$ is given by $i(h)=(h,e)$. Proposition 4.6.9 Let  : K  Aut(H) be a homomorphism. Suppose given homomorphisms $f_1$ : H  G and $f_2$ : K  G for a group G. Then there is a homomorphism f : $H \rtimes_{\alpha}$ K  G with $f_1 = f$  i and $f_2$ = f  s if and only if $f_2(k)f_1(h)(f_2(k))^{1} = f_1(\alpha(k)(h))$ for all h  H and k  K. Such an f, if it exists, is unique, and is given in the HK notation by $f(hk) = f_1(h)f_2(k)$. a) In this problem we assume a familiarity with the group of invertible 33 matrices   over the ring $Z_n$. Let $$G = \left\{ \begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix} \mid \bar{x},\bar{y},\bar{z} \in Z_n \right\}$$ Show that $G$ forms a group under matrix multiplication. Show, using Proposition   4.6.9, that $G$ is isomorphic to the group constructed in Corollary 4.6.8. I already showed that $G$ is a group under matrix multiplication, but I'm having trouble with the second part of this problem. Let $G = \left\{ \begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix} \mid \bar{x}, \bar{y}, \bar{z} \in Z_n \right\}$ Let $G' = \{ b^ic^ja^k \mid 0 \leq i, j, k < n \}$ where a, b, c all have order n, and b commutes with c, a commutes with c, and $aba^{-1} = bc$. Thus, $$(b^ic^ja^k)(b^{i'}c^{j'}a^{k'}) = b^{i+i'}c^{j + j' + ki'}a^{k+k'}.$$ I want to show that G is isomorphic to G' by using proposition 4.6.9 However, in order to use that proposition, I need to come up with an H and a K, right? So... Let $H = \left\{ \begin{bmatrix} 1 & 0 & 0 \\0 & 1 & 0 \\0 & 0 & 1 \end{bmatrix} \right\}$ Let K=G Obviously $G \cong H \rtimes_{\alpha} K$ Let $f_1 : H \rightarrow G$ be the trivial homomorphism. Let $f_2: K \rightarrow G'$ be given by $f_2 (\begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix}) = b^ic^ja^k$ Now we need to show that $f_2(\begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix})f_1( \begin{bmatrix} 1 & 0 & 0 \\0 & 1 & 0 \\0 & 0 & 1 \end{bmatrix})f_2(\begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix}^{-1})$ = $f_1(\begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 \\0 & 1 & 0 \\0 & 0 & 1 \end{bmatrix}\begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix}^{-1})$ But, $$f_2(\begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix})f_1( \begin{bmatrix} 1 & 0 & 0 \\0 & 1 & 0 \\0 & 0 & 1 \end{bmatrix})f_2(\begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix}^{-1}) = (b^ic^ja^k)(e)(b^ic^ja^k)^{-1} = e$$ and $$f_1(\begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 \\0 & 1 & 0 \\0 & 0 & 1 \end{bmatrix}\begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix}^{-1}) = e$$ So they must be equal. So the homomorphism $f: H \rtimes_{\alpha} K \rightarrow G$ exists. So now we just need to show that this homomorphism is an isomorphism. I tried to show that by proving that $\begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix}$ has order n when n is odd and order 2n when n is even. I first tried to show the case when n is odd: Let n=2k+1 where $k \geq 0$ Base Case:  n=0 is trivial. We can also try n=3, $( \begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix})^3 = \begin{bmatrix} 1 & 3(\bar{x}) & 3(\bar{y}) + 3(\bar{xy}) \\0 & 1 & 3(\bar{z}) \\0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 \\0 & 1 & 0 \\0 & 0 & 1 \end{bmatrix} $ For the induction step, suppose for some 2w+1, an exponent of $\begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix}$ is 2w+1. We need to show that, for 2(w+1)+1 = 2w+3, an exponent of $ \begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix}$ must be 2w+3. By induction, we know that $$\begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix}^{2w+1} = \begin{bmatrix} 1 & (2w+1)\bar{x} & (2w+1)\bar{y}+(2w+1)\bar{xz} \\0 & 1 & (2w+1)\bar{z} \\0 & 0 & 1 \end{bmatrix}$$ However, $$\begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix}^{2w+3} = \begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix}^{2w+1} \begin{bmatrix} 1 & \bar{x} & \bar{y} \\0 & 1 & \bar{z} \\0 & 0 & 1 \end{bmatrix}^2 = \begin{bmatrix} 1 & (2w+1)\bar{x} & (2w+1)\bar{y}+(2w+1)\bar{xz} \\0 & 1 & (2w+1)\bar{z} \\0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 1 & 2\bar{x} & 2(\bar{y})+\bar{xz} \\0 & 1 & 2\bar{z} \\0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 2(\bar{x})+(2w+1)(\bar{x}) & 2(\bar{y})+\bar{xz}+(2w+1)(\bar{x})(2)(\bar{z})+ (2w+1)(\bar{xz})+(2w+1)\bar{y} \\0 & 1 & (2w+1)\bar{z}+2(\bar{z}) \\0 & 0 & 1 \end{bmatrix}$$ But that wasnt the result we were supposed to get, right? Because $2(\bar{y})+\bar{xz}+(2w+1)(\bar{x})(2)(\bar{z})+ (2w+1)(\bar{xz})+(2w+1)\bar{y} = (2w+3)\bar{y} +(6w+4)\bar{xz}$ and 6w+4 is not a multiple of 2w+3. So I was a bit confusedand I was wondering if anybody could help me with this. Thank you in advance",,['abstract-algebra']
59,Is there a general algorithm to find a primitive element of a given finite extension (with a finite number of intermediate fields)?),Is there a general algorithm to find a primitive element of a given finite extension (with a finite number of intermediate fields)?),,"I just want to know if there is an algorithm to find the primitive element of a given finite extension $F/k$ if the intermediate fields are given. I know how to approach it in particular examples picking linear combinations (the case $\mathbb{Q}(\sqrt{2}, \sqrt{3})/\mathbb{Q}$ for instance). If it does not exists, is there, then, an algorithm for the finite separable extension case or for extensions of $\mathbb{Q}$? Thanks in advance.","I just want to know if there is an algorithm to find the primitive element of a given finite extension $F/k$ if the intermediate fields are given. I know how to approach it in particular examples picking linear combinations (the case $\mathbb{Q}(\sqrt{2}, \sqrt{3})/\mathbb{Q}$ for instance). If it does not exists, is there, then, an algorithm for the finite separable extension case or for extensions of $\mathbb{Q}$? Thanks in advance.",,"['abstract-algebra', 'field-theory']"
60,A Regular Map Has Finitely Many Ramification Points,A Regular Map Has Finitely Many Ramification Points,,"Let $C,D$ be nonsingular projective curves, $f \colon C \to D$ nonconstant, $K = k(C), L = k(D)$, $d = \text{deg }f = [K:L]$, and of course $k$ algebraically closed. Furthermore let's suppose that $K:L$ is separable. I'm trying to understand why it is that $|f^{-1}(p)| = d$ for all but finitely many $p \in D$. Basically, I want to show that if $f$ is unramified at $p$ then it's unramified on an open neighborhood of $p$, then show that if $K:L$ is separable then $f$ is unramified at at least one such point. I have an argument in progress but there's something I don't quite understand. If $p \in D$ then we can let $q_1,\ldots,q_k$ be the preimages under $f$, find some $g$ that has distinct values at all $q_i$, then let $m$ be the minimal polynomial of $g$ over $L$ (which has coefficients in $B$, for algebraic reasons). Because $m$ has coefficients in $B$ we can form $m_0$ where we just evaluate all of its coefficients at $p$. Then $m_0$ has at least $k$ distinct roots (namely, $g(q_i)$). Obviously $\partial m = \partial m_0 \leq d$ as well. So if $p$ is ramified, then the roots of $m$ - and $m_0$ - are distinct hence $\Delta m_0 = \Delta m(p) \neq 0$. This is an algebraic condition so there is a neighborhood on which $\Delta m$ is nonzero, etc. However, there's something I'm missing. I don't know that it makes sense to say that $\Delta m(p') \neq 0$ means that $f$ is unramified at $p'$. After all, $m$ doesn't appear to have a lot to do with $p'$. But $m$ can't change from point to point because then it's not an algebraic condition anymore - it's a logical condition. There should be something that says that if some fixed $m_0$ has $d$ distinct roots then $p$ has $d$ distinct pullbacks, but I can't find it. Can someone point me in the right direction?","Let $C,D$ be nonsingular projective curves, $f \colon C \to D$ nonconstant, $K = k(C), L = k(D)$, $d = \text{deg }f = [K:L]$, and of course $k$ algebraically closed. Furthermore let's suppose that $K:L$ is separable. I'm trying to understand why it is that $|f^{-1}(p)| = d$ for all but finitely many $p \in D$. Basically, I want to show that if $f$ is unramified at $p$ then it's unramified on an open neighborhood of $p$, then show that if $K:L$ is separable then $f$ is unramified at at least one such point. I have an argument in progress but there's something I don't quite understand. If $p \in D$ then we can let $q_1,\ldots,q_k$ be the preimages under $f$, find some $g$ that has distinct values at all $q_i$, then let $m$ be the minimal polynomial of $g$ over $L$ (which has coefficients in $B$, for algebraic reasons). Because $m$ has coefficients in $B$ we can form $m_0$ where we just evaluate all of its coefficients at $p$. Then $m_0$ has at least $k$ distinct roots (namely, $g(q_i)$). Obviously $\partial m = \partial m_0 \leq d$ as well. So if $p$ is ramified, then the roots of $m$ - and $m_0$ - are distinct hence $\Delta m_0 = \Delta m(p) \neq 0$. This is an algebraic condition so there is a neighborhood on which $\Delta m$ is nonzero, etc. However, there's something I'm missing. I don't know that it makes sense to say that $\Delta m(p') \neq 0$ means that $f$ is unramified at $p'$. After all, $m$ doesn't appear to have a lot to do with $p'$. But $m$ can't change from point to point because then it's not an algebraic condition anymore - it's a logical condition. There should be something that says that if some fixed $m_0$ has $d$ distinct roots then $p$ has $d$ distinct pullbacks, but I can't find it. Can someone point me in the right direction?",,"['abstract-algebra', 'algebraic-geometry', 'algebraic-curves']"
61,Monoids with positive and negative elements,Monoids with positive and negative elements,,"By a pointed monoid I mean a monoid $M$ together with an absorbing element $0 \in M$ (i.e. $0x=x0=0$). Equivalently, this is a monoid in the monoidal category $(\mathsf{Set}_*,\wedge)$. By a $\pm$-monoid I mean a pointed monoid $M$ together with a decomposition $M = M^+ \cup M^-$ such that $M^+ \cap M^- = \{0\}$, $1 \in M^+$ and $M^{\pm} \cdot M^{\pm} \subseteq M^+$, $M^{\pm} \cdot M^{\mp} \subseteq M^-$. You can imagine $M^+$ as the set of elements $\geq 0$ and $M^-$ as the set of elements $\leq 0$, even though $M$ doesn't have to be ordered in the usual sense. As a class of examples, take the multiplicative structure of an ordered (semi)ring, for example $\mathbb{N}$ and $\mathbb{Z}$. These have the $\pm$-submonoids $\{0,1\},\{0,-1\},\{0,1,-1\}$. The ""generic"" example in terms of representation theory is the following: Take any monoid $G$ and consider $E = E^+ \vee_{0} E^-$, where $EM^+$ is the set of endomorphisms of $G$ and $E^-$ is the set of anti-endomorphisms of $G$. Let $0$ be the trivial endomorphism, and $1$ be the identity. The multiplication in $M$ is the composition of functions. Then $E$ is a $\pm$-monoid, which may be called $\mathrm{End}^{\pm}(G)$. A module over a $\pm$-monoid $M$ is a monoid $G$ together with a morphism $M \to \mathrm{End}^{\pm}(G)$ of $\pm$-monoids. Question. Have these $\pm$-monoids and their modules already been studied in the literature? Or do they have another common name? What is known about them (classification, properties, etc)?","By a pointed monoid I mean a monoid $M$ together with an absorbing element $0 \in M$ (i.e. $0x=x0=0$). Equivalently, this is a monoid in the monoidal category $(\mathsf{Set}_*,\wedge)$. By a $\pm$-monoid I mean a pointed monoid $M$ together with a decomposition $M = M^+ \cup M^-$ such that $M^+ \cap M^- = \{0\}$, $1 \in M^+$ and $M^{\pm} \cdot M^{\pm} \subseteq M^+$, $M^{\pm} \cdot M^{\mp} \subseteq M^-$. You can imagine $M^+$ as the set of elements $\geq 0$ and $M^-$ as the set of elements $\leq 0$, even though $M$ doesn't have to be ordered in the usual sense. As a class of examples, take the multiplicative structure of an ordered (semi)ring, for example $\mathbb{N}$ and $\mathbb{Z}$. These have the $\pm$-submonoids $\{0,1\},\{0,-1\},\{0,1,-1\}$. The ""generic"" example in terms of representation theory is the following: Take any monoid $G$ and consider $E = E^+ \vee_{0} E^-$, where $EM^+$ is the set of endomorphisms of $G$ and $E^-$ is the set of anti-endomorphisms of $G$. Let $0$ be the trivial endomorphism, and $1$ be the identity. The multiplication in $M$ is the composition of functions. Then $E$ is a $\pm$-monoid, which may be called $\mathrm{End}^{\pm}(G)$. A module over a $\pm$-monoid $M$ is a monoid $G$ together with a morphism $M \to \mathrm{End}^{\pm}(G)$ of $\pm$-monoids. Question. Have these $\pm$-monoids and their modules already been studied in the literature? Or do they have another common name? What is known about them (classification, properties, etc)?",,"['abstract-algebra', 'reference-request', 'order-theory', 'monoid']"
62,What are Connes and co. talking about 'field with one element'?,What are Connes and co. talking about 'field with one element'?,,"What are Connes and coworkers talking about: Fun with a field with one element ? Also, http://arxiv.org/pdf/0806.2401v1.pdf I don't have much algebra background but thought a field with 1=0 is degenerate?","What are Connes and coworkers talking about: Fun with a field with one element ? Also, http://arxiv.org/pdf/0806.2401v1.pdf I don't have much algebra background but thought a field with 1=0 is degenerate?",,['abstract-algebra']
63,Natural Euclidean Function Not Satisfying the $d$-inequality,Natural Euclidean Function Not Satisfying the -inequality,d,"Let me provide some background before I begin (although I feel as though it's hardly needed): Let $R$ be an integral domain. I call a function $d:R\setminus \{0\}\to\mathbb{N}\cup\{0\}$ a Euclidean function if for every $a,b\in R$ , $a\ne0$ , there exists $q,r\in R$ with $b=aq+r$ and    either $r=0$ or $d(r)<d(a)$ . I say that a Euclidean function satisfies the $d$ -inequality if $x\mid y$ implies $d(x)\leqslant d(y)$ . It is often assumed that Euclidean functions for example, in the context of Euclidean domains, always satisfy the $d$ -inequality since given a Euclidean function $d$ the function $\displaystyle \widetilde{d}(x)=\min_{y\ne 0}d(xy)$ is a Euclidean function satisfying the $d$ -inequality. Thus, Euclidean domains (i.e. rings for which there exists a Euclidean function on) are precisely the same as the rings that admit Euclidean functions satisfying the $d$ -inequality. Now, while, as I said above, the study of such rings (where the only key is the existence of such functions) is no different, practically it's much nicer to have Euclidean functions satisfying the $d$ -inequality since they enjoy such benefits as $a\in R^\times$ if and only if $d(a)=d(1)$ . The strange thing is, most naturally occurring Euclidean functions satisfy the $d$ -inequality (e.g. the degree function on $F[x]$ , field norms, etc.) And, for the life of me, I can't think of a non-contrived example of a Euclidean function that does not satisfy the $d$ -inequality. So, what are some? Moreover, there will undoubtedly be some trivial, common one that I've overlooked, then I would still love to hear more obscure ones that arise naturally in more advanced contexts. Thanks for your time!","Let me provide some background before I begin (although I feel as though it's hardly needed): Let be an integral domain. I call a function a Euclidean function if for every , , there exists with and    either or . I say that a Euclidean function satisfies the -inequality if implies . It is often assumed that Euclidean functions for example, in the context of Euclidean domains, always satisfy the -inequality since given a Euclidean function the function is a Euclidean function satisfying the -inequality. Thus, Euclidean domains (i.e. rings for which there exists a Euclidean function on) are precisely the same as the rings that admit Euclidean functions satisfying the -inequality. Now, while, as I said above, the study of such rings (where the only key is the existence of such functions) is no different, practically it's much nicer to have Euclidean functions satisfying the -inequality since they enjoy such benefits as if and only if . The strange thing is, most naturally occurring Euclidean functions satisfy the -inequality (e.g. the degree function on , field norms, etc.) And, for the life of me, I can't think of a non-contrived example of a Euclidean function that does not satisfy the -inequality. So, what are some? Moreover, there will undoubtedly be some trivial, common one that I've overlooked, then I would still love to hear more obscure ones that arise naturally in more advanced contexts. Thanks for your time!","R d:R\setminus \{0\}\to\mathbb{N}\cup\{0\} a,b\in R a\ne0 q,r\in R b=aq+r r=0 d(r)<d(a) d x\mid y d(x)\leqslant d(y) d d \displaystyle \widetilde{d}(x)=\min_{y\ne 0}d(xy) d d d a\in R^\times d(a)=d(1) d F[x] d","['abstract-algebra', 'euclidean-domain']"
64,Example of a group where $o(a)$ and $o(b)$ are finite but $o(ab)$ is infinite [duplicate],Example of a group where  and  are finite but  is infinite [duplicate],o(a) o(b) o(ab),"This question already has answers here : Examples and further results about the order of the product of two elements in a group (8 answers) Closed 11 years ago . Let G be a group and $a,b \in G$. It is given that $o(a)$ and $o(b)$ are finite. Can you give an example of a group where $o(ab)$ is infinite?","This question already has answers here : Examples and further results about the order of the product of two elements in a group (8 answers) Closed 11 years ago . Let G be a group and $a,b \in G$. It is given that $o(a)$ and $o(b)$ are finite. Can you give an example of a group where $o(ab)$ is infinite?",,"['abstract-algebra', 'group-theory', 'examples-counterexamples']"
65,Does $z ^k+ z^{-k}$ belong to $\Bbb Z[z + z^{-1}]$?,Does  belong to ?,z ^k+ z^{-k} \Bbb Z[z + z^{-1}],Let $z$ be a non-zero element of $\mathbb{C}$. Does $z^k + z^{-k}$ belong to $\mathbb{Z}[z + z^{-1}]$ for every positive integer $k$? Motivation: I came up with this problem from the following question. Maximal real subfield of $\mathbb{Q}(\zeta )$,Let $z$ be a non-zero element of $\mathbb{C}$. Does $z^k + z^{-k}$ belong to $\mathbb{Z}[z + z^{-1}]$ for every positive integer $k$? Motivation: I came up with this problem from the following question. Maximal real subfield of $\mathbb{Q}(\zeta )$,,"['abstract-algebra', 'ring-theory']"
66,Why do the Localization of a Ring,Why do the Localization of a Ring,,"This question may be a bit vague, but neverthless, i would like to see an answer. Wikipedia tells me that: In abstract algebra, localization is a systematic method of adding multiplicative inverses to a ring. It is clear that for integral domains , we have the Field of Fractions , and we work on it. It's obvious that a Field has multiplicative inverses. Now if we consider any arbitrary ring $R$, by the definition of localization, it means that we are adding multiplicative inverses to $R$ thereby wanting $R$ to be a division ring or a field . My question, why is it so important to look at this concept of Localization. Or how would the theory look like if we never had the concept of Localization.","This question may be a bit vague, but neverthless, i would like to see an answer. Wikipedia tells me that: In abstract algebra, localization is a systematic method of adding multiplicative inverses to a ring. It is clear that for integral domains , we have the Field of Fractions , and we work on it. It's obvious that a Field has multiplicative inverses. Now if we consider any arbitrary ring $R$, by the definition of localization, it means that we are adding multiplicative inverses to $R$ thereby wanting $R$ to be a division ring or a field . My question, why is it so important to look at this concept of Localization. Or how would the theory look like if we never had the concept of Localization.",,['abstract-algebra']
67,Showing the polynomial $x^4 + x^3 + 4x + 1$ is irreducible in $\mathbb{Q}[x]$.,Showing the polynomial  is irreducible in .,x^4 + x^3 + 4x + 1 \mathbb{Q}[x],"Showing the polynomial $f(x) = x^4 + x^3 + 4x + 1$ is irreducible in $\mathbb{Q}[x]$ . I have two question relating to this which I've bolded below. Attempt to answer (*) $f$ has degree $4 \ge 2$ so if $f$ has a root then $f$ is reducible. Well the rational root test says that if $\exists$ a rational root $\frac{p}{q}$ with $p, q$ coprime then $p|a_0$ and $q|a_n$ . Hence the potential rational roots are $\pm 1$ . Neither of these are roots so $f$ does not have a root. But (*) only says that if $f$ has a root then $f$ is reducible, it does not imply that if $f$ does not have a root then $f$ is irreducible. Is this correct? Now we can't apply Eisenstein's irreducibility criterion theorem as there is no prime $p$ such that $p\mid a_0, a_1,..., a_{n-1}$ $p \nmid a_n$ $p^2 \nmid a_0$ So where do we go from here? One other fact I am aware of is that if $f$ is primitive, which it is, $f$ irreducible in $Q[x] \iff f$ is irreducible in $Z[x]$ . But I don't see how that can help me here. So how do I proceed from here?","Showing the polynomial is irreducible in . I have two question relating to this which I've bolded below. Attempt to answer (*) has degree so if has a root then is reducible. Well the rational root test says that if a rational root with coprime then and . Hence the potential rational roots are . Neither of these are roots so does not have a root. But (*) only says that if has a root then is reducible, it does not imply that if does not have a root then is irreducible. Is this correct? Now we can't apply Eisenstein's irreducibility criterion theorem as there is no prime such that So where do we go from here? One other fact I am aware of is that if is primitive, which it is, irreducible in is irreducible in . But I don't see how that can help me here. So how do I proceed from here?","f(x) = x^4 + x^3 + 4x + 1 \mathbb{Q}[x] f 4 \ge 2 f f \exists \frac{p}{q} p, q p|a_0 q|a_n \pm 1 f f f f f p p\mid a_0, a_1,..., a_{n-1} p \nmid a_n p^2 \nmid a_0 f f Q[x] \iff f Z[x]","['abstract-algebra', 'ring-theory']"
68,"If in a group G an element $x$ has the property such that $x^2 = e$, does that entail $x = e$?","If in a group G an element  has the property such that , does that entail ?",x x^2 = e x = e,I can't seem to prove that $x = e$ from $x = x^{-1}$.,I can't seem to prove that $x = e$ from $x = x^{-1}$.,,"['abstract-algebra', 'group-theory']"
69,Noetherian module implies Noetherian ring?,Noetherian module implies Noetherian ring?,,"I know that a finitely generated $R$-module $M$ over a Noetherian ring $R$ is Noetherian. I wonder about the converse. I believe it has to be false and I am looking for counterexamples. Also I wonder if $M$ Noetherian imply that $R$ is Noetherian is true? And if $M$ Noetherian implies $M$ finitely generated is true? That is, do both implications fail or only one of them?","I know that a finitely generated $R$-module $M$ over a Noetherian ring $R$ is Noetherian. I wonder about the converse. I believe it has to be false and I am looking for counterexamples. Also I wonder if $M$ Noetherian imply that $R$ is Noetherian is true? And if $M$ Noetherian implies $M$ finitely generated is true? That is, do both implications fail or only one of them?",,"['abstract-algebra', 'commutative-algebra', 'modules']"
70,"In a monoid, does $x \cdot y=e$ imply $y \cdot x=e$?","In a monoid, does  imply ?",x \cdot y=e y \cdot x=e,"A monoid is a set $S$ together with a binary operation $\cdot:S \times S \rightarrow S$ such that: The binary operation $\cdot$ is associative, that is, $(a\cdot b) \cdot c=a\cdot (b \cdot c)$ for all $a,b,c \in S$. There is an identity element $e \in S$, that is, there exists $e \in S$ such that $e \cdot a=a$ and $a \cdot e=a$ for all $a \in S$. Question :  Suppose, $x,y \in S$ such that $x \cdot y=e$.  Does $y \cdot x=e$? This question was motivated by the question here , where the author attempts to prove a special case of the above in the context of matrix multiplication.  It was subsequently proved, but the proofs require the properties of the matrix. I attempted to use Prover9 to prove the statement.  Here's the input: formulas(assumptions).  % associativity (x * y) * z = x * (y * z).  % identity element a x * a = x. a * x = x.  end_of_list.  formulas(goals).  x * y = a -> y * x = a.  end_of_list. and it returned sos_empty , which, I guess, implies that no proof of the above statement is possible from the axioms of monoids alone.  I ran Mace4 on the same input, and found no counter-examples for monoids of sizes $1,2,\ldots,82$. A comment by Martin Brandenburg here regarding K-algebras might also apply here.  For example, the property might be true for finite monoids, but not all infinite monoids.  A counter-example would (obviously) need to be non-commutative.","A monoid is a set $S$ together with a binary operation $\cdot:S \times S \rightarrow S$ such that: The binary operation $\cdot$ is associative, that is, $(a\cdot b) \cdot c=a\cdot (b \cdot c)$ for all $a,b,c \in S$. There is an identity element $e \in S$, that is, there exists $e \in S$ such that $e \cdot a=a$ and $a \cdot e=a$ for all $a \in S$. Question :  Suppose, $x,y \in S$ such that $x \cdot y=e$.  Does $y \cdot x=e$? This question was motivated by the question here , where the author attempts to prove a special case of the above in the context of matrix multiplication.  It was subsequently proved, but the proofs require the properties of the matrix. I attempted to use Prover9 to prove the statement.  Here's the input: formulas(assumptions).  % associativity (x * y) * z = x * (y * z).  % identity element a x * a = x. a * x = x.  end_of_list.  formulas(goals).  x * y = a -> y * x = a.  end_of_list. and it returned sos_empty , which, I guess, implies that no proof of the above statement is possible from the axioms of monoids alone.  I ran Mace4 on the same input, and found no counter-examples for monoids of sizes $1,2,\ldots,82$. A comment by Martin Brandenburg here regarding K-algebras might also apply here.  For example, the property might be true for finite monoids, but not all infinite monoids.  A counter-example would (obviously) need to be non-commutative.",,"['abstract-algebra', 'automated-theorem-proving']"
71,"No isomorphism between $k[x,y]/(x^{2}-y^{5})$ and ring of polynomials in one indeterminate",No isomorphism between  and ring of polynomials in one indeterminate,"k[x,y]/(x^{2}-y^{5})","Let $k$ be an algebraically closed field. Why there is no isomorphism (as finitely generated $k$-algebras) between the ring $k[x,y]/(x^{2}-y^{5})$ and $k[t]$?","Let $k$ be an algebraically closed field. Why there is no isomorphism (as finitely generated $k$-algebras) between the ring $k[x,y]/(x^{2}-y^{5})$ and $k[t]$?",,['abstract-algebra']
72,Name of algebraic structure (group-like),Name of algebraic structure (group-like),,"What is the name of an algebraic structure (group, quasi group, monoid) defined like the following: It is a set of three elements: $a,b,c$ ; Has an operation such that when applied to two elements of the group it returns the other element of the group (example: $ab=c$ ; $cb=a$ ); Has an ""identity"" structure such as $aa=a$ , $bb=b$ , $cc=c$ ; It is commutative: $ab=ba$ ; also $a(ba)=ac=b$ . This is no homework. I'm just trying to define a structure like this in order to learn, but as I'm reading about group-theory and semigroups it doesn't seem to really define this (for example, the ""identity"" here is ambiguous - so how should I call it?) Thank you!","What is the name of an algebraic structure (group, quasi group, monoid) defined like the following: It is a set of three elements: ; Has an operation such that when applied to two elements of the group it returns the other element of the group (example: ; ); Has an ""identity"" structure such as , , ; It is commutative: ; also . This is no homework. I'm just trying to define a structure like this in order to learn, but as I'm reading about group-theory and semigroups it doesn't seem to really define this (for example, the ""identity"" here is ambiguous - so how should I call it?) Thank you!","a,b,c ab=c cb=a aa=a bb=b cc=c ab=ba a(ba)=ac=b","['abstract-algebra', 'group-theory', 'category-theory', 'abelian-groups']"
73,Given $x^9 = e$ and $x^{11} = e$ prove $x = e$.,Given  and  prove .,x^9 = e x^{11} = e x = e,"Full Problem: Prove that for any element $x$ in a group $G$ that satisfies  $$x^9 = e \\ x^{11} = e,$$ where $e$ is the identity element, that $x$ itself must be $e$. Is this as simple as showing that $x^{11} = x^{9} \cdot x^{2} = e \cdot x^2 \Rightarrow x^2 = e$ $x^{9} = x^{2} \cdot x^{7} = e \cdot x^7 \Rightarrow x^7 = e$ $x^{7} = x^{2} \cdot x^{5} = e \cdot x^5 \Rightarrow x^5 = e$ $x^{5} = x^{2} \cdot x^{3} = e \cdot x^3 \Rightarrow x^3 = e$ $x^{3} = x^{2} \cdot x = e \cdot x \Rightarrow x = e$ Therefore, $x = e$.","Full Problem: Prove that for any element $x$ in a group $G$ that satisfies  $$x^9 = e \\ x^{11} = e,$$ where $e$ is the identity element, that $x$ itself must be $e$. Is this as simple as showing that $x^{11} = x^{9} \cdot x^{2} = e \cdot x^2 \Rightarrow x^2 = e$ $x^{9} = x^{2} \cdot x^{7} = e \cdot x^7 \Rightarrow x^7 = e$ $x^{7} = x^{2} \cdot x^{5} = e \cdot x^5 \Rightarrow x^5 = e$ $x^{5} = x^{2} \cdot x^{3} = e \cdot x^3 \Rightarrow x^3 = e$ $x^{3} = x^{2} \cdot x = e \cdot x \Rightarrow x = e$ Therefore, $x = e$.",,"['abstract-algebra', 'group-theory']"
74,Why is the set of natural numbers not considered a non-Abelian group? [closed],Why is the set of natural numbers not considered a non-Abelian group? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I don't understand why the set of natural numbers constitutes a commutative monoid with addition, but is not considered an Abelian group.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I don't understand why the set of natural numbers constitutes a commutative monoid with addition, but is not considered an Abelian group.",,"['abstract-algebra', 'group-theory', 'abelian-groups']"
75,Why does the characteristic of a finite field always have to be prime? [duplicate],Why does the characteristic of a finite field always have to be prime? [duplicate],,"This question already has answers here : Characteristic of a field is $0$ or prime [closed] (2 answers) Closed 6 years ago . I know little about group theory, and the concept of a field characteristic is new to me. Although there are some posts on this website that explain it very well. ""In layman's terms, it's the amount of times we can keep adding 1 to itself without looping back to 0. The characteristic being $p$ means that $\underbrace{1+1+\dots+1}_p=0$."" I think I can get my head around the idea, but there is still something I don't understand. ""We can see quickly using a zero divisor argument that the characteristic of any field with positive characteristic must be prime."" Why is it that it has to be prime? Why can't it be something else?","This question already has answers here : Characteristic of a field is $0$ or prime [closed] (2 answers) Closed 6 years ago . I know little about group theory, and the concept of a field characteristic is new to me. Although there are some posts on this website that explain it very well. ""In layman's terms, it's the amount of times we can keep adding 1 to itself without looping back to 0. The characteristic being $p$ means that $\underbrace{1+1+\dots+1}_p=0$."" I think I can get my head around the idea, but there is still something I don't understand. ""We can see quickly using a zero divisor argument that the characteristic of any field with positive characteristic must be prime."" Why is it that it has to be prime? Why can't it be something else?",,"['abstract-algebra', 'ring-theory']"
76,Examples of non-abelian groups with no abelian subgroups,Examples of non-abelian groups with no abelian subgroups,,I have been trying to find a counterexample that all groups (finite and infinite) have an abelian subgroup (besides the trivial subgroup). Any ideas?,I have been trying to find a counterexample that all groups (finite and infinite) have an abelian subgroup (besides the trivial subgroup). Any ideas?,,"['abstract-algebra', 'group-theory']"
77,Why isn't $\mathbb Z_9 \cong \mathbb Z_3 \times \mathbb Z_3$ by the fundamental theorem of finite abelian groups?,Why isn't  by the fundamental theorem of finite abelian groups?,\mathbb Z_9 \cong \mathbb Z_3 \times \mathbb Z_3,I was reading the answer to this question: Explicit descriptions of groups of order 45 and the accepted answer says the Sylow $3$-subgroup is either isomorphic to $\mathbb Z_9$ or $\mathbb Z_3 \times \mathbb Z_3$. But now my question is: Why isn't $\mathbb Z_9 \cong \mathbb Z_3 \times \mathbb Z_3$ by the fundamental theorem of finite abelian groups? The fundamental theorem of finite abelian groups says: Every finite abelian group is the direct product of cyclic groups . (Herstein) So isn't $\mathbb Z_9 \cong \mathbb Z_3 \times \mathbb Z_3$ by this theorem?,I was reading the answer to this question: Explicit descriptions of groups of order 45 and the accepted answer says the Sylow $3$-subgroup is either isomorphic to $\mathbb Z_9$ or $\mathbb Z_3 \times \mathbb Z_3$. But now my question is: Why isn't $\mathbb Z_9 \cong \mathbb Z_3 \times \mathbb Z_3$ by the fundamental theorem of finite abelian groups? The fundamental theorem of finite abelian groups says: Every finite abelian group is the direct product of cyclic groups . (Herstein) So isn't $\mathbb Z_9 \cong \mathbb Z_3 \times \mathbb Z_3$ by this theorem?,,"['abstract-algebra', 'group-theory', 'finite-groups']"
78,Why is enveloping algebra called enveloping algebra?,Why is enveloping algebra called enveloping algebra?,,"What does the enveloping algebra of $\mathfrak{g}$ have to do with envelopes? If $\mathfrak{g}$ is a Lie algebra, we take tensor algebra on $\mathfrak{g}$ and make quotient through ideal of T, so we put elements of $\mathfrak{g}$ into some equivalence classes determined by the ideal. How is this ""enveloping"" the $\mathfrak{g}$ ? What is the intuition? (Maybe there is no interesting interpretation of this and I am just being too curious, but had to try it). Remark 1 : I am refering to the enveloping algebra of $\mathfrak{g}$ , which is sometimes called universal enveloping algebra . It is the same. Remark 2 : I am using the definition from Dixmier:","What does the enveloping algebra of have to do with envelopes? If is a Lie algebra, we take tensor algebra on and make quotient through ideal of T, so we put elements of into some equivalence classes determined by the ideal. How is this ""enveloping"" the ? What is the intuition? (Maybe there is no interesting interpretation of this and I am just being too curious, but had to try it). Remark 1 : I am refering to the enveloping algebra of , which is sometimes called universal enveloping algebra . It is the same. Remark 2 : I am using the definition from Dixmier:",\mathfrak{g} \mathfrak{g} \mathfrak{g} \mathfrak{g} \mathfrak{g} \mathfrak{g},"['abstract-algebra', 'lie-groups', 'lie-algebras', 'tensors', 'envelope']"
79,Are two finite groups of the same order always isomorphic?,Are two finite groups of the same order always isomorphic?,,Are two finite groups of the same order always isomorphic? Some simple example would be great!,Are two finite groups of the same order always isomorphic? Some simple example would be great!,,"['abstract-algebra', 'group-theory', 'finite-groups']"
80,Definition of Simple Group,Definition of Simple Group,,Herstein defined the definition of a simple group as follows: A group is said to be simple if it has no non-trivial homomorphic image. Please help me to understand what is meant by non-trivial homomorphic image. Thanks.,Herstein defined the definition of a simple group as follows: A group is said to be simple if it has no non-trivial homomorphic image. Please help me to understand what is meant by non-trivial homomorphic image. Thanks.,,"['abstract-algebra', 'group-theory', 'simple-groups']"
81,Representing a countable field by $\Bbb N$ [transport of algebraic structure],Representing a countable field by  [transport of algebraic structure],\Bbb N,"To make sure that we are talking about the same, I would like to post the relevant definitions I know first. Definitions: A pair $(G, +)$ where $G$ is a set and $+: G \times G \rightarrow G$ is called a commutative group if it has all of the following three characteristics: (associativity): $\forall x,y,z \in G: (x+y)+z=x+(y+z)$ (identity element): $\exists e \in G \forall x \in G: e + x = x = x +e$ (inverse elements): $\forall x \in G \exists x^{-1} \in G: x^{-1} + x = e = x + x^{-1}$ (commutativity): $\forall x,y \in G: x+y=y+x$. A triple $(\mathbb{K}, +, \cdot)$, where $\mathbb{K}$ is a set and $+: \mathbb{K} \times \mathbb{K} \rightarrow \mathbb{K}$ $\cdot: \mathbb{K} \times \mathbb{K} \rightarrow \mathbb{K}$ is called a field if it has all of the following three characteristics: $(\mathbb{K}, +)$ is a commutative group $(\mathbb{K} \setminus \{0\}, \cdot)$ is a commutative group distributive properties : $\forall x,y,z \in \mathbb{K}: x \cdot (y+z) = (x \cdot y) + (x \cdot z)$ and $\forall x,y,z \in \mathbb{K}: (y+z) \cdot x = (y \cdot x) + (z \cdot x)$ $\mathbb{N^+} := 1, 2, 3, 4, 5 ...$ (all positive integers) $\mathbb{N}_0 := \{0\} \cup \mathbb{N}$ (the natural numbers with zero) $\mathbb{N}$ can be both: $\mathbb{N^+}$ or $\mathbb{N}_0$. My question: Does a field exist that has $\mathbb{N}$ as its set? I know that $(\mathbb{Q}, +, \cdot)$ is a field and I know that $(\mathbb{N}, + )$ is only a commutative semigroup. But maybe it's possible to define two mappings $\circ, *$ for $\mathbb{N}$ in such a way that $(\mathbb{N}, \circ, *)$ is a field. Related : The smallest field containing the integers is the field of rational   numbers. Source: en-Wikipedia: Integer I don't know if this is true. If somebody can prove (or at least scribble the proof of) the quote it would be a good answer, I guess.","To make sure that we are talking about the same, I would like to post the relevant definitions I know first. Definitions: A pair $(G, +)$ where $G$ is a set and $+: G \times G \rightarrow G$ is called a commutative group if it has all of the following three characteristics: (associativity): $\forall x,y,z \in G: (x+y)+z=x+(y+z)$ (identity element): $\exists e \in G \forall x \in G: e + x = x = x +e$ (inverse elements): $\forall x \in G \exists x^{-1} \in G: x^{-1} + x = e = x + x^{-1}$ (commutativity): $\forall x,y \in G: x+y=y+x$. A triple $(\mathbb{K}, +, \cdot)$, where $\mathbb{K}$ is a set and $+: \mathbb{K} \times \mathbb{K} \rightarrow \mathbb{K}$ $\cdot: \mathbb{K} \times \mathbb{K} \rightarrow \mathbb{K}$ is called a field if it has all of the following three characteristics: $(\mathbb{K}, +)$ is a commutative group $(\mathbb{K} \setminus \{0\}, \cdot)$ is a commutative group distributive properties : $\forall x,y,z \in \mathbb{K}: x \cdot (y+z) = (x \cdot y) + (x \cdot z)$ and $\forall x,y,z \in \mathbb{K}: (y+z) \cdot x = (y \cdot x) + (z \cdot x)$ $\mathbb{N^+} := 1, 2, 3, 4, 5 ...$ (all positive integers) $\mathbb{N}_0 := \{0\} \cup \mathbb{N}$ (the natural numbers with zero) $\mathbb{N}$ can be both: $\mathbb{N^+}$ or $\mathbb{N}_0$. My question: Does a field exist that has $\mathbb{N}$ as its set? I know that $(\mathbb{Q}, +, \cdot)$ is a field and I know that $(\mathbb{N}, + )$ is only a commutative semigroup. But maybe it's possible to define two mappings $\circ, *$ for $\mathbb{N}$ in such a way that $(\mathbb{N}, \circ, *)$ is a field. Related : The smallest field containing the integers is the field of rational   numbers. Source: en-Wikipedia: Integer I don't know if this is true. If somebody can prove (or at least scribble the proof of) the quote it would be a good answer, I guess.",,"['abstract-algebra', 'ring-theory', 'field-theory']"
82,Does the cartesian product have a neutral element?,Does the cartesian product have a neutral element?,,"Let $A$ be any set. Is there a set $E$ such that $A \times E = E \times A = A$? I thought of the empty set, but Wikipedia says otherwise. This operation changes dimension, so an isomorphism might be needed for such element to exist.","Let $A$ be any set. Is there a set $E$ such that $A \times E = E \times A = A$? I thought of the empty set, but Wikipedia says otherwise. This operation changes dimension, so an isomorphism might be needed for such element to exist.",,"['abstract-algebra', 'elementary-set-theory']"
83,Does every prime ideal in a ring arise as kernel of a homomorphism into $\mathbb{Z}$?,Does every prime ideal in a ring arise as kernel of a homomorphism into ?,\mathbb{Z},Let $R$ be a commutative ring. Clearly the kernel of $h$ is a prime ideal whenever $h : R \rightarrow                                                                                                                 \mathbb{Z}$ is a ring homomorphism.  But is the converse true: does every prime ideal arise as kernel of a homomorphism into $\mathbb{Z}$?,Let $R$ be a commutative ring. Clearly the kernel of $h$ is a prime ideal whenever $h : R \rightarrow                                                                                                                 \mathbb{Z}$ is a ring homomorphism.  But is the converse true: does every prime ideal arise as kernel of a homomorphism into $\mathbb{Z}$?,,"['abstract-algebra', 'ring-theory', 'ideals']"
84,In a ring $aba=0$ implies $ab=0$ or $ba=0$?,In a ring  implies  or ?,aba=0 ab=0 ba=0,"In a ring, $a\neq0$ and $b\neq0$. $aba=0$. Prove $ab=0$ or $ba=0$. This is one question in my abstract algebra homework-- it seems pretty easy at first glance, yet I have spent hours thinking about possible solutions...still I haven't find a way out. Could you give me some hint? Thank you~~","In a ring, $a\neq0$ and $b\neq0$. $aba=0$. Prove $ab=0$ or $ba=0$. This is one question in my abstract algebra homework-- it seems pretty easy at first glance, yet I have spent hours thinking about possible solutions...still I haven't find a way out. Could you give me some hint? Thank you~~",,"['abstract-algebra', 'ring-theory']"
85,Examples of rings with idempotent elements,Examples of rings with idempotent elements,,"As a part of my studies in ring theory, I've encountered the concept of an idempotent element, i.e., an element $e$ such that $e^2=e$. Are there some interesting examples of rings with idempotent elements?","As a part of my studies in ring theory, I've encountered the concept of an idempotent element, i.e., an element $e$ such that $e^2=e$. Are there some interesting examples of rings with idempotent elements?",,"['abstract-algebra', 'ring-theory', 'examples-counterexamples']"
86,"How to approach proofs similar to ""Show a group, $G$, is infinite if $G = \langle r, s, t\mid rst = 1\rangle $""","How to approach proofs similar to ""Show a group, , is infinite if ""","G G = \langle r, s, t\mid rst = 1\rangle ","How to approach proofs similar to ""Show a group, $G$ , is infinite if $G = \langle r, s, t\mid rst = 1\rangle $ "" I have not worked much with relations and tend to get lost in notation. I am practicing solving problems like the one in the title but am having a hard time as I am not sure the tricks to try or areas to investigate first in trying to make a proof. What are some hints for starting a proof about some quality of a group defined by a relation? So far the only relations I know about are the dihedral groups of order $2n$ , the quaternions, and cyclically generated groups so comparisons to how we show properties of those might be illuminating.","How to approach proofs similar to ""Show a group, , is infinite if "" I have not worked much with relations and tend to get lost in notation. I am practicing solving problems like the one in the title but am having a hard time as I am not sure the tricks to try or areas to investigate first in trying to make a proof. What are some hints for starting a proof about some quality of a group defined by a relation? So far the only relations I know about are the dihedral groups of order , the quaternions, and cyclically generated groups so comparisons to how we show properties of those might be illuminating.","G G = \langle r, s, t\mid rst = 1\rangle  2n","['abstract-algebra', 'group-theory', 'relations', 'group-presentation', 'combinatorial-group-theory']"
87,"Basic proof that $GL(2, \mathbb{Z})$ is not nilpotent",Basic proof that  is not nilpotent,"GL(2, \mathbb{Z})","I need to show that $GL(2,\mathbb{Z})=\left\{ \begin{pmatrix} a&b \\ c&d \end{pmatrix} \mid a,b,c,d \in \mathbb{Z}, ad-bc = \pm 1\right\}$ is not nilpotent. I have seen this question but the answer given there is too advanced for where I am currently in my studies. In order to show something is not nilpotent, all I have at my disposal is to show that the upper central series does not terminate or that it is not solvable (using a derived series that does not terminate or a subnormal series that either does not terminate or whose factors are not all abelian.) This seems a little messy, and I've never used a direct approach to show that something was not nilpotent. What is the best approach to this proof without appealing to $p$-groups, Sylow Groups, Galois Theory, or free abelian groups? Thank you.","I need to show that $GL(2,\mathbb{Z})=\left\{ \begin{pmatrix} a&b \\ c&d \end{pmatrix} \mid a,b,c,d \in \mathbb{Z}, ad-bc = \pm 1\right\}$ is not nilpotent. I have seen this question but the answer given there is too advanced for where I am currently in my studies. In order to show something is not nilpotent, all I have at my disposal is to show that the upper central series does not terminate or that it is not solvable (using a derived series that does not terminate or a subnormal series that either does not terminate or whose factors are not all abelian.) This seems a little messy, and I've never used a direct approach to show that something was not nilpotent. What is the best approach to this proof without appealing to $p$-groups, Sylow Groups, Galois Theory, or free abelian groups? Thank you.",,['abstract-algebra']
88,"What does it mean for a group to ""act algebraically""?","What does it mean for a group to ""act algebraically""?",,"I'm reading the paper How to use finite fields for problems concerning infinite fields , by Serre. In Theorem 1.2 on page 1, he says Let $G$ be a finite $p$-group acting algebraically... In the context of actions, what is the meaning of the word ""algebraically""?","I'm reading the paper How to use finite fields for problems concerning infinite fields , by Serre. In Theorem 1.2 on page 1, he says Let $G$ be a finite $p$-group acting algebraically... In the context of actions, what is the meaning of the word ""algebraically""?",,"['abstract-algebra', 'algebraic-geometry', 'group-actions']"
89,Is it possible that $\mathbb{Q}(\alpha)=\mathbb{Q}(\alpha^{n})$ for all $n>1$?,Is it possible that  for all ?,\mathbb{Q}(\alpha)=\mathbb{Q}(\alpha^{n}) n>1,"Is it possible that $\mathbb{Q}(\alpha)=\mathbb{Q}(\alpha^{n})$ for all $n>1$ when $\mathbb{Q}(\alpha)$ is a $p$th degree Galois extension of $\mathbb{Q}$? ($p$ is prime) I got stuck with this problem while trying to construct polynomials whose Galois group is cyclic group of order $p$. Edit: Okay, I got two nice answers for this question but to fulfill my original purpose(constructing polynomials with cyclic Galois group) I realized that I should ask for all primes $p$ if there is any such $\alpha$(depending on $p$) such that the above condition is satisfied. If the answer is no (i.e. there are  primes $p$ for which there is no $\alpha$ such that $\mathbb{Q}(\alpha)=\mathbb{Q}(\alpha^{n})$ for all $n>1$) then I succeed up to certain stage.","Is it possible that $\mathbb{Q}(\alpha)=\mathbb{Q}(\alpha^{n})$ for all $n>1$ when $\mathbb{Q}(\alpha)$ is a $p$th degree Galois extension of $\mathbb{Q}$? ($p$ is prime) I got stuck with this problem while trying to construct polynomials whose Galois group is cyclic group of order $p$. Edit: Okay, I got two nice answers for this question but to fulfill my original purpose(constructing polynomials with cyclic Galois group) I realized that I should ask for all primes $p$ if there is any such $\alpha$(depending on $p$) such that the above condition is satisfied. If the answer is no (i.e. there are  primes $p$ for which there is no $\alpha$ such that $\mathbb{Q}(\alpha)=\mathbb{Q}(\alpha^{n})$ for all $n>1$) then I succeed up to certain stage.",,"['abstract-algebra', 'field-theory']"
90,"Suppose $a$ and $b$ belong to a ring. If $ab$ is a zero divisor, then is $ba$ also a zero divisor?","Suppose  and  belong to a ring. If  is a zero divisor, then is  also a zero divisor?",a b ab ba,"Suppose $(R,+,\cdot)$ be a ring and there is two elements, $a$ and $b$ are in the ring such that $ab$ is a zero divisor. My attempt has two parts. 1 - if $a$ or $b$ equal to $0$ ( $0$ is the zero element of the ring), then it is obvious that $a.b = 0$ and $b.a = 0$ . then for every $x  R, a.b.x = 0$ so $a.b$ is a right zero divisor. Then $xba = 0$ so $b.a$ is right zero divisor. Similarly we can see that if ab is left zero divisor then $ba$ is right zero divisor. So for this case , if $ab$ is zero divisor, then $ba$ is also a zero divisor.(I think ring R should have at least one non-zero element so that $x0$ , otherwise zero divisor won't have meaning in such a ring ?) 2 - if $a$ and $b$ does not equal $0$ . so from the assumptions, we know that ab is zero divisor. It means that it is both right and left divisor. So there exists some $z  R, z 0, s.t, abz = 0$ .then if we consider the element $bz$ , it results that $ba.bz = b.(abz) = 0$ . Similarly , there exist some $z'  R, z'  0, s.t, z'ab = 0$ . Then $z'a.ba = (z'ab).a = 0$ . But it now suffices to show that $z'a  0$ and $bz  0$ so that $ba$ can be a zero divisor. How can I show that? or there is some counterexamples that shows $ba$ is not necessarily zero divisor even hen ab is a zero divisor?","Suppose be a ring and there is two elements, and are in the ring such that is a zero divisor. My attempt has two parts. 1 - if or equal to ( is the zero element of the ring), then it is obvious that and . then for every so is a right zero divisor. Then so is right zero divisor. Similarly we can see that if ab is left zero divisor then is right zero divisor. So for this case , if is zero divisor, then is also a zero divisor.(I think ring R should have at least one non-zero element so that , otherwise zero divisor won't have meaning in such a ring ?) 2 - if and does not equal . so from the assumptions, we know that ab is zero divisor. It means that it is both right and left divisor. So there exists some .then if we consider the element , it results that . Similarly , there exist some . Then . But it now suffices to show that and so that can be a zero divisor. How can I show that? or there is some counterexamples that shows is not necessarily zero divisor even hen ab is a zero divisor?","(R,+,\cdot) a b ab a b 0 0 a.b = 0 b.a = 0 x  R, a.b.x = 0 a.b xba = 0 b.a ba ab ba x0 a b 0 z  R, z 0, s.t, abz = 0 bz ba.bz = b.(abz) = 0 z'  R, z'  0, s.t, z'ab = 0 z'a.ba = (z'ab).a = 0 z'a  0 bz  0 ba ba","['abstract-algebra', 'ring-theory']"
91,"Prove that the symmetric group $S_n$, $n \geq 3$, has trivial center. [duplicate]","Prove that the symmetric group , , has trivial center. [duplicate]",S_n n \geq 3,"This question already has answers here : Find the center of the symmetry group $S_n$. (4 answers) Closed 6 years ago . I am trying to prove this: Let $\sigma$ be a non-identity element of $S_{n}$ . If $n \geq 3$ show that $\exists \gamma \in  S_{n}$ such that $\sigma\gamma \neq \gamma\sigma$ . Hint: Let $\sigma*k=1$ where $k \neq 1$ . Let $m$ not be an element of $\{k,1\}$ . Let $\gamma=(k,m)$ Here's the answer from the textbook: Let $\sigma*k=1$ for some $k \neq 1$ . Then as $m\geq 3$ , choose an $m$ not an element of $\{k,1\}$ . Now let $\gamma=(k,m)$ . This gives $\gamma*\sigma*k=\gamma*(1)=1$ but $\sigma*\gamma*k=\sigma*m \neq 1$ , since if $\sigma*m=1=\sigma*k$ , then $m=k$ , as $\sigma$ is one-to-one contrary to the assumption. I guess I'm just not understanding the steps to their proof. Can anybody make sense of this to someone new to proofs? I mostly don't understand how $\sigma*\gamma*k=\sigma*m=1$","This question already has answers here : Find the center of the symmetry group $S_n$. (4 answers) Closed 6 years ago . I am trying to prove this: Let be a non-identity element of . If show that such that . Hint: Let where . Let not be an element of . Let Here's the answer from the textbook: Let for some . Then as , choose an not an element of . Now let . This gives but , since if , then , as is one-to-one contrary to the assumption. I guess I'm just not understanding the steps to their proof. Can anybody make sense of this to someone new to proofs? I mostly don't understand how","\sigma S_{n} n \geq 3 \exists \gamma \in  S_{n} \sigma\gamma \neq \gamma\sigma \sigma*k=1 k \neq 1 m \{k,1\} \gamma=(k,m) \sigma*k=1 k \neq 1 m\geq 3 m \{k,1\} \gamma=(k,m) \gamma*\sigma*k=\gamma*(1)=1 \sigma*\gamma*k=\sigma*m \neq 1 \sigma*m=1=\sigma*k m=k \sigma \sigma*\gamma*k=\sigma*m=1","['abstract-algebra', 'group-theory', 'proof-verification', 'symmetric-groups', 'alternative-proof']"
92,Finding the degree of $1+\sqrt[3]{2}+\sqrt[3]{4}$ over $\mathbb{Q}$,Finding the degree of  over,1+\sqrt[3]{2}+\sqrt[3]{4} \mathbb{Q},"This is an exercise for the book Abstract Algebra by Dummit and Foote (pg. 530): Find the degree of $\alpha:=1+\sqrt[3]{2}+\sqrt[3]{4}$ over $\mathbb{Q}$ My efforts : I first try to find the minimal polynomial by writing $\alpha=1+\sqrt[3]{2}+\sqrt[3]{4}\implies\alpha-1=\sqrt[3]{2}(1+\sqrt[3]{2})\implies(\alpha-1)^{3}=2(1+\sqrt[3]{2})^{3}$ but I didn't manage to get the minimal polynomial from this (which is, according to Wolfram, of degree $3$ ). I also tried another method that failed: I noted that $\mathbb{Q}(\alpha)\subset\mathbb{Q}(\sqrt[3]{2})$ hence is of degree $\leq3$ , moreover, since it is a subfield and $3$ is prime it only remains to show that $\alpha$ is not rational (which I can't prove). Can someone pleases help me show that the degree is $3$ (preferably is one of the two methods I tried) ?","This is an exercise for the book Abstract Algebra by Dummit and Foote (pg. 530): Find the degree of over My efforts : I first try to find the minimal polynomial by writing but I didn't manage to get the minimal polynomial from this (which is, according to Wolfram, of degree ). I also tried another method that failed: I noted that hence is of degree , moreover, since it is a subfield and is prime it only remains to show that is not rational (which I can't prove). Can someone pleases help me show that the degree is (preferably is one of the two methods I tried) ?",\alpha:=1+\sqrt[3]{2}+\sqrt[3]{4} \mathbb{Q} \alpha=1+\sqrt[3]{2}+\sqrt[3]{4}\implies\alpha-1=\sqrt[3]{2}(1+\sqrt[3]{2})\implies(\alpha-1)^{3}=2(1+\sqrt[3]{2})^{3} 3 \mathbb{Q}(\alpha)\subset\mathbb{Q}(\sqrt[3]{2}) \leq3 3 \alpha 3,"['abstract-algebra', 'field-theory']"
93,Find every ring homomorphism between $\mathbb Z$ and $\mathbb Q$,Find every ring homomorphism between  and,\mathbb Z \mathbb Q,"One of the problems I got is nothing but this sentence. No hints, no context. In one direction, a function $\varphi:\mathbb Z\to\mathbb Q$ is a morphism iff $$\forall\star\in\{+,\times\}\quad \forall a,b\in\mathbb Z\quad\varphi(a\star_{\mathbb Z}b)=\varphi(a)\star_{\mathbb Q}\varphi(b)\quad,$$ and after some manipulation, I found out that if $\varphi$ is polynomial, it has to be either $\varphi(x)\equiv0$ or $\varphi(x)=x$. Analogously, a function $\phi:\mathbb Q\to \mathbb Z$ is a ring morphism iff $$\forall\star\in\{+,\times\}\quad \forall a,b\in\mathbb Q\quad\phi(a\star_{\mathbb Q} b)=\phi(a)\star_{\mathbb Z}\phi(b)\quad,$$ and $\phi$ polynomial implies $\phi(x)\equiv0$, because even the identity function would go wrong this time. This is way too particular. It doesn't seem smart to obtain morphisms by starting with ""if $\varphi$ is this kind of function"". Is there a clever way to construct general homomorphism between two given sets? (If that makes any difference, I can't use the Homomorphism Theorems, except for the First.)","One of the problems I got is nothing but this sentence. No hints, no context. In one direction, a function $\varphi:\mathbb Z\to\mathbb Q$ is a morphism iff $$\forall\star\in\{+,\times\}\quad \forall a,b\in\mathbb Z\quad\varphi(a\star_{\mathbb Z}b)=\varphi(a)\star_{\mathbb Q}\varphi(b)\quad,$$ and after some manipulation, I found out that if $\varphi$ is polynomial, it has to be either $\varphi(x)\equiv0$ or $\varphi(x)=x$. Analogously, a function $\phi:\mathbb Q\to \mathbb Z$ is a ring morphism iff $$\forall\star\in\{+,\times\}\quad \forall a,b\in\mathbb Q\quad\phi(a\star_{\mathbb Q} b)=\phi(a)\star_{\mathbb Z}\phi(b)\quad,$$ and $\phi$ polynomial implies $\phi(x)\equiv0$, because even the identity function would go wrong this time. This is way too particular. It doesn't seem smart to obtain morphisms by starting with ""if $\varphi$ is this kind of function"". Is there a clever way to construct general homomorphism between two given sets? (If that makes any difference, I can't use the Homomorphism Theorems, except for the First.)",,['abstract-algebra']
94,Why is it that $x^4+2x^2+1$ is reducible in $\mathbb{Z}[x]$ but has no roots in $\mathbb{Q}$?,Why is it that  is reducible in  but has no roots in ?,x^4+2x^2+1 \mathbb{Z}[x] \mathbb{Q},$\textbf{ Lemma:}$  A non constant primitive polynomial $f(x) \in \mathbb{Z}[x]$ is irreducible in $\mathbb{Q}[x]$ if and only if $f(x)$ is irreducible in $\mathbb{Z}[x]$. I am reading a book in which it is given that $f(x)=x^4+2x^2+1$ is primitive in $\mathbb{Z}[x]$ and it has no roots in $\mathbb{Q}$ but it is reducible over $\mathbb{Z} $ as $f(x)=(x^2+1)(x^2+1)$. My question is doen't it conradict this lemma?  since irreducible over $\mathbb{Z}$ and irreducible over $\mathbb{Q}$ is the same thing for primitive polynomial.,$\textbf{ Lemma:}$  A non constant primitive polynomial $f(x) \in \mathbb{Z}[x]$ is irreducible in $\mathbb{Q}[x]$ if and only if $f(x)$ is irreducible in $\mathbb{Z}[x]$. I am reading a book in which it is given that $f(x)=x^4+2x^2+1$ is primitive in $\mathbb{Z}[x]$ and it has no roots in $\mathbb{Q}$ but it is reducible over $\mathbb{Z} $ as $f(x)=(x^2+1)(x^2+1)$. My question is doen't it conradict this lemma?  since irreducible over $\mathbb{Z}$ and irreducible over $\mathbb{Q}$ is the same thing for primitive polynomial.,,"['abstract-algebra', 'ring-theory', 'roots', 'irreducible-polynomials']"
95,How to find all irreducible polynomials in Z2 with degree 5? [duplicate],How to find all irreducible polynomials in Z2 with degree 5? [duplicate],,"This question already has answers here : Find all irreducible monic polynomials in $\mathbb{Z}/(2)[x]$ with degree equal or less than 5 (3 answers) Closed 5 years ago . I am totally lost on how to do this one. I am supposed to accomplish the following: Find all irreducible polynomials in $\mathbb{Z}_2[x]$ with degree $5$.  I may use the fact that x, $x+1$ and $x^2+x+1$ are the irreducibles of degree less than or equal to 2 If someone could provide a step by step explanation of how to do this, that would be amazing! Thanks in advance! P.S.: This is not a homework, but is a question on a past exam that I couldn't answer and I want to know how to do it.","This question already has answers here : Find all irreducible monic polynomials in $\mathbb{Z}/(2)[x]$ with degree equal or less than 5 (3 answers) Closed 5 years ago . I am totally lost on how to do this one. I am supposed to accomplish the following: Find all irreducible polynomials in $\mathbb{Z}_2[x]$ with degree $5$.  I may use the fact that x, $x+1$ and $x^2+x+1$ are the irreducibles of degree less than or equal to 2 If someone could provide a step by step explanation of how to do this, that would be amazing! Thanks in advance! P.S.: This is not a homework, but is a question on a past exam that I couldn't answer and I want to know how to do it.",,['abstract-algebra']
96,Localisation isomorphic to a quotient of polynomial ring [duplicate],Localisation isomorphic to a quotient of polynomial ring [duplicate],,"This question already has answers here : Localisation is isomorphic to a quotient of polynomial ring (5 answers) Closed 9 years ago . Let $R$ be a commutative ring and $A=\{1,a,a^2,\dots\}$ for some $a\in R$. Prove that $A^{-1}R$ is isomorphic to $R[T]/(aT-1)$. I guess I'm meant to find a surjective homomorphism between $A^{-1}R$ and $R[T]$ and then use first isomorphism theorem. What homomorphism should I use?","This question already has answers here : Localisation is isomorphic to a quotient of polynomial ring (5 answers) Closed 9 years ago . Let $R$ be a commutative ring and $A=\{1,a,a^2,\dots\}$ for some $a\in R$. Prove that $A^{-1}R$ is isomorphic to $R[T]/(aT-1)$. I guess I'm meant to find a surjective homomorphism between $A^{-1}R$ and $R[T]$ and then use first isomorphism theorem. What homomorphism should I use?",,"['abstract-algebra', 'ring-theory', 'commutative-algebra']"
97,Do there exist groups whose elements of finite order do not form a subgroup? [duplicate],Do there exist groups whose elements of finite order do not form a subgroup? [duplicate],,"This question already has answers here : Closed 12 years ago . Possible Duplicate: Examples and further results about the order of the product of two elements in a group I was browsing around, and came across the little exercise that elements of finite order in an abelian group form a subgroup. This was easy enough, but now I'm wondering, are there cases of groups where this is not true? I'm thinking that if there are, these groups are necessarily nonabelian and infinite, since if the group is finite the elements of finite order would be the whole group anyway. I also noticed that if an element $a$ has order $n$, then so does $a^{-1}$, so to find such a group I would need to two elements of finite order whose composition does not have finite order. The two infinite nonabelian groups I could think of off the top of my head were $\textbf{GL}_n(R)$ and $\textbf{SL}_n(R)$, but I had trouble finding a pair of elements satisfying the above property. Does anyone have an example of such a group and a pair of elements which prove it? Thank you.","This question already has answers here : Closed 12 years ago . Possible Duplicate: Examples and further results about the order of the product of two elements in a group I was browsing around, and came across the little exercise that elements of finite order in an abelian group form a subgroup. This was easy enough, but now I'm wondering, are there cases of groups where this is not true? I'm thinking that if there are, these groups are necessarily nonabelian and infinite, since if the group is finite the elements of finite order would be the whole group anyway. I also noticed that if an element $a$ has order $n$, then so does $a^{-1}$, so to find such a group I would need to two elements of finite order whose composition does not have finite order. The two infinite nonabelian groups I could think of off the top of my head were $\textbf{GL}_n(R)$ and $\textbf{SL}_n(R)$, but I had trouble finding a pair of elements satisfying the above property. Does anyone have an example of such a group and a pair of elements which prove it? Thank you.",,"['abstract-algebra', 'group-theory', 'big-list']"
98,What is an extension field? Covered differently in math & in cryptography.,What is an extension field? Covered differently in math & in cryptography.,,"In his book on Cryptography, Paar has this theorem Theorem 4.3.1 A field with order m only exists if m is a prime power, i.e., m = p^n, for some positive integer n and prime integer p. p is called the characteristic of the finite field. So here he says that the order has to be a prime power - He also has this as examples This theorem implies that there are, for instance, finite fields with 11 elements, or with 81 elements (since 81 = 3^4) or with 256 elements (since 256 = 2^8, and 2 is a prime). So he explicitly says that you can have a field with 256 elements - the order of a finite field needs to be a prime power & not necessarily a prime itself. He then goes on to talk about extension fields - he says that if the order of the field is not a prime then it's called as an extension field. In AES the finite field contains 256 elements and is denoted as GF(2^8). This field was chosen because each of the field elements can be represented by one byte. For the S-Box and MixColumn transforms, AES treats every byte of the internal data path as an element of the field GF(2^8) and manipulates the data by performing arithmetic in this finite field. However, if the order of a finite field is not prime, and 2^8 is clearly not a prime, the addition and multiplication operation cannot be represented by addition and multiplication of integers modulo 2^8. Such fields with m > 1 are called extension fields. So as per this, I get the definition of an extension field as this - an extension field is any finite field where the order of the field is a prime power but not a prime itself. However, when I look at books on abstract algebra, I see a totally different definition of extension fields which seem to be unconnected with what Paar says. For e.g. from ""Topics in Algebra"" by Hernstein: Let F be a field; a field K is said to be an extension of F if K contains F. Equivalently, K is an extension of F if F is a subfield of K. So are Extension fields described in Cryptography different from those described in Algebra? But is Paar's description wrong? Or are the 2 definitions equivalent in some way?","In his book on Cryptography, Paar has this theorem Theorem 4.3.1 A field with order m only exists if m is a prime power, i.e., m = p^n, for some positive integer n and prime integer p. p is called the characteristic of the finite field. So here he says that the order has to be a prime power - He also has this as examples This theorem implies that there are, for instance, finite fields with 11 elements, or with 81 elements (since 81 = 3^4) or with 256 elements (since 256 = 2^8, and 2 is a prime). So he explicitly says that you can have a field with 256 elements - the order of a finite field needs to be a prime power & not necessarily a prime itself. He then goes on to talk about extension fields - he says that if the order of the field is not a prime then it's called as an extension field. In AES the finite field contains 256 elements and is denoted as GF(2^8). This field was chosen because each of the field elements can be represented by one byte. For the S-Box and MixColumn transforms, AES treats every byte of the internal data path as an element of the field GF(2^8) and manipulates the data by performing arithmetic in this finite field. However, if the order of a finite field is not prime, and 2^8 is clearly not a prime, the addition and multiplication operation cannot be represented by addition and multiplication of integers modulo 2^8. Such fields with m > 1 are called extension fields. So as per this, I get the definition of an extension field as this - an extension field is any finite field where the order of the field is a prime power but not a prime itself. However, when I look at books on abstract algebra, I see a totally different definition of extension fields which seem to be unconnected with what Paar says. For e.g. from ""Topics in Algebra"" by Hernstein: Let F be a field; a field K is said to be an extension of F if K contains F. Equivalently, K is an extension of F if F is a subfield of K. So are Extension fields described in Cryptography different from those described in Algebra? But is Paar's description wrong? Or are the 2 definitions equivalent in some way?",,"['abstract-algebra', 'finite-fields', 'extension-field', 'cryptography']"
99,"Show that $S_4/V$ is isomorphic to $S_3 $, where $V$ is the Klein Four Group.","Show that  is isomorphic to , where  is the Klein Four Group.",S_4/V S_3  V,"(i) Show that $S_4/V$ is isomorphic to $S_3 $, where $V$ is the Klein Four Group. I understand isomorphism to be a bijective homomorphism but I'm unsure how one would go about proving this. $S_4/V$ has order 6 and I think that will be of use but that's as far as I can go. (ii) Let $G = D_7$ be the dihedral group of order 14 and let $H = C_7$ be the cyclic group of order 7. Find all the homomorphisms from $G$ to $H$. Are there any homomorpisms at all?","(i) Show that $S_4/V$ is isomorphic to $S_3 $, where $V$ is the Klein Four Group. I understand isomorphism to be a bijective homomorphism but I'm unsure how one would go about proving this. $S_4/V$ has order 6 and I think that will be of use but that's as far as I can go. (ii) Let $G = D_7$ be the dihedral group of order 14 and let $H = C_7$ be the cyclic group of order 7. Find all the homomorphisms from $G$ to $H$. Are there any homomorpisms at all?",,"['abstract-algebra', 'group-theory', 'finite-groups']"
