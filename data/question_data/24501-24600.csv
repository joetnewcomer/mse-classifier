,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Computing the distance between vector and its projection on a random subspace,Computing the distance between vector and its projection on a random subspace,,"Let $V\in\mathbb{R}^{n\times m}$ , where $n>m$ , be a random matrix of standard normal gaussians. Given an arbitrary vector $y\in\mathbb{R}^n$ , I need to understand the distance $||y-p_V(y)||^2$ , where $p_V$ is the projection onto the range of $V$ , meaning $p_V = V(V^TV)^{-1}V^T$ as a matrix. Through a slightly different lens, I would like to understand the distance of $y$ from $R(V)$ . Given that $V$ is random, this makes the above distance a random variable whose distribution (or at least expected value) is important to my research. This feels like a not-so-novel problem but I couldn't find a solution on my own. How can I approach this? Thanks in advance!","Let , where , be a random matrix of standard normal gaussians. Given an arbitrary vector , I need to understand the distance , where is the projection onto the range of , meaning as a matrix. Through a slightly different lens, I would like to understand the distance of from . Given that is random, this makes the above distance a random variable whose distribution (or at least expected value) is important to my research. This feels like a not-so-novel problem but I couldn't find a solution on my own. How can I approach this? Thanks in advance!",V\in\mathbb{R}^{n\times m} n>m y\in\mathbb{R}^n ||y-p_V(y)||^2 p_V V p_V = V(V^TV)^{-1}V^T y R(V) V,"['linear-algebra', 'probability', 'projection', 'random-matrices', 'projection-matrices']"
1,"Linear relation between three infinite matrices, the first two are symmetric and the third is a matrix of ones plus a diagonal matrix","Linear relation between three infinite matrices, the first two are symmetric and the third is a matrix of ones plus a diagonal matrix",,"For $n\geq 1$ , let $G_n$ be the matrix $G_n=(g(i,j))_{1\leq i,j \leq n}$ where $g(i,j)=\max(i,j)$ if $i\neq j$ and $0$ otherwise. If we define $$A_2=\begin{pmatrix}1 & 0 \\ 0 & 1\end{pmatrix}, B_2=\begin{pmatrix}0 & 1 \\ 1 & 0\end{pmatrix}, H_2=\begin{pmatrix}0 & 1 \\ 1 & 0\end{pmatrix} \tag{1}\label{1}$$ Then we have the identities $G_2A_2=2H_2$ and $G_2B_2=2I_2$ . Next, if we consider $$ A_3=\begin{pmatrix}1 & 0 & \frac{1}{3} \\ 0 & 1 & \frac{1}{3} \\ \frac{1}{3} & \frac{1}{3} & \frac{10}{9} \end{pmatrix}, B_3=\begin{pmatrix}0 & 1 & 1 \\ 1 & 0 & 1 \\ 1 & 1 & \frac{2}{3}\end{pmatrix}, H_3=\begin{pmatrix}0 & 1 & \frac{4}{3} \\ 1 & 0 & \frac{4}{3} \\ 1 & 1 & \frac{1}{3} \end{pmatrix}\tag{2} $$ Then we have $G_3A_3=I_3+3H_3$ and $G_3B_3=5I_3+3H_3$ . Continuing in this vein, if we put $$ A_4=\begin{pmatrix}1 & 0 & \frac{1}{3} & \frac{7}{12} \\ 0 & 1 & \frac{1}{3} & \frac{7}{12} \\ \frac{1}{3} & \frac{1}{3} & \frac{10}{9} & \frac{7}{9} \\ \frac{7}{12} & \frac{7}{12} & \frac{7}{9} & \frac{113}{72} \end{pmatrix}, B_4=\begin{pmatrix}0 & 1 & 1 & \frac{5}{4} \\ 1 & 0 & 1 & \frac{5}{4} \\ 1 & 1 & \frac{2}{3} & \frac{5}{3} \\  \frac{5}{4} & \frac{5}{4} & \frac{5}{3} & \frac{43}{24}\end{pmatrix}, H_4=\begin{pmatrix}0 & 1 & \frac{4}{3} & \frac{11}{6} \\ 1 & 0 & \frac{4}{3} & \frac{11}{6} \\ 1 & 1 & \frac{1}{3} & \frac{11}{6} \\  1 & 1 & \frac{4}{3} & \frac{5}{6} \end{pmatrix}\tag{3} $$ then $G_4A_4=\frac{10}{3}I_4+\frac{16}{3}H_4$ and $G_4B_4=10I_4+8H_4$ . We can now formulate the following conjecture : there are three infinite matrices $A_{\infty}=(a(i,j))_{i,j\in {\mathbb N}}$ , $B_{\infty}=(b(i,j))_{i,j\in {\mathbb N}}$ and $H_{\infty}=(h(i,j))_{i,j\in {\mathbb N}}$ , such that if we denote by $A_n,B_n,H_n$ respectively the upper-left corner submatrices of those matrices (so that $A_n=(a(i,j))_{1\leq i,j \leq n}$ , and so on), then $A_{\infty}$ and $B_{\infty}$ are symmetric (i.e. $a(i,j)=a(j,i),\  b(i,j)=b(j,i)$ ) $h(i,j)=h(j,j)+1$ when $i\neq j$ $A_2,B_2$ and $H_2$ are given by \eqref{1} above, For every $n$ , one has constants $a_n,b_n$ such that $G_nA_n=a_nI_n+(a_n+2)H_n$ , $G_nB_n=b_nI_n+(b_n-2)H_n$ . Any ideas on how to prove that conjecture (or find a counterexample) ? Here are a few possibly useful data for small values : $$ \begin{array}{|c|c|c|c|c|c|c|c|c|c|} \hline n & 2 & 3 & 4 & 5 & 6  & 7 & 8 & 9 & 10 \\ \hline a_n & 0 & 1 & \frac{10}{3} & \frac{91}{12} & \frac{73}{5}  & \frac{9199}{360}   & \frac{13232}{315} &  \frac{147879}{2240}  & \frac{129929}{1296} \\ \hline b_n & 2 & 5 & 10 & \frac{73}{4} & \frac{175}{5}  & \frac{6197}{120}   & \frac{1228}{15} &  \frac{40273}{320}  & \frac{81427}{432} \\ \hline h(n,n) & 0 & 0 & \frac{1}{3} & \frac{5}{6} & \frac{3}{2} & \frac{85}{36}  & \frac{871}{252}   & \frac{1083}{224} &  \frac{118939}{18144}  \\ \hline \end{array} \tag{4} $$ Other things I noticed : All the entries of $A_{\infty},B_{\infty}$ and $H_{\infty}$ seem to be positive rationals, except for the zeroes from $A_2,B_2,H_2$ . If we exclude the upper-left $2\times 2$ square, the first two rows of $A_n$ are identical ; same property for $B_n$ .","For , let be the matrix where if and otherwise. If we define Then we have the identities and . Next, if we consider Then we have and . Continuing in this vein, if we put then and . We can now formulate the following conjecture : there are three infinite matrices , and , such that if we denote by respectively the upper-left corner submatrices of those matrices (so that , and so on), then and are symmetric (i.e. ) when and are given by \eqref{1} above, For every , one has constants such that , . Any ideas on how to prove that conjecture (or find a counterexample) ? Here are a few possibly useful data for small values : Other things I noticed : All the entries of and seem to be positive rationals, except for the zeroes from . If we exclude the upper-left square, the first two rows of are identical ; same property for .","n\geq 1 G_n G_n=(g(i,j))_{1\leq i,j \leq n} g(i,j)=\max(i,j) i\neq j 0 A_2=\begin{pmatrix}1 & 0 \\ 0 & 1\end{pmatrix}, B_2=\begin{pmatrix}0 & 1 \\ 1 & 0\end{pmatrix}, H_2=\begin{pmatrix}0 & 1 \\ 1 & 0\end{pmatrix} \tag{1}\label{1} G_2A_2=2H_2 G_2B_2=2I_2 
A_3=\begin{pmatrix}1 & 0 & \frac{1}{3} \\ 0 & 1 & \frac{1}{3} \\ \frac{1}{3} & \frac{1}{3} & \frac{10}{9} \end{pmatrix}, B_3=\begin{pmatrix}0 & 1 & 1 \\ 1 & 0 & 1 \\ 1 & 1 & \frac{2}{3}\end{pmatrix}, H_3=\begin{pmatrix}0 & 1 & \frac{4}{3} \\ 1 & 0 & \frac{4}{3} \\ 1 & 1 & \frac{1}{3} \end{pmatrix}\tag{2}
 G_3A_3=I_3+3H_3 G_3B_3=5I_3+3H_3 
A_4=\begin{pmatrix}1 & 0 & \frac{1}{3} & \frac{7}{12} \\
0 & 1 & \frac{1}{3} & \frac{7}{12} \\
\frac{1}{3} & \frac{1}{3} & \frac{10}{9} & \frac{7}{9} \\ \frac{7}{12} & \frac{7}{12} & \frac{7}{9} & \frac{113}{72} \end{pmatrix}, B_4=\begin{pmatrix}0 & 1 & 1 & \frac{5}{4} \\
1 & 0 & 1 & \frac{5}{4} \\ 1 & 1 & \frac{2}{3} & \frac{5}{3} \\
 \frac{5}{4} & \frac{5}{4} & \frac{5}{3} & \frac{43}{24}\end{pmatrix}, H_4=\begin{pmatrix}0 & 1 & \frac{4}{3} & \frac{11}{6} \\ 1 & 0 & \frac{4}{3} & \frac{11}{6} \\ 1 & 1 & \frac{1}{3} & \frac{11}{6} \\  1 & 1 & \frac{4}{3} & \frac{5}{6} \end{pmatrix}\tag{3}
 G_4A_4=\frac{10}{3}I_4+\frac{16}{3}H_4 G_4B_4=10I_4+8H_4 A_{\infty}=(a(i,j))_{i,j\in {\mathbb N}} B_{\infty}=(b(i,j))_{i,j\in {\mathbb N}} H_{\infty}=(h(i,j))_{i,j\in {\mathbb N}} A_n,B_n,H_n A_n=(a(i,j))_{1\leq i,j \leq n} A_{\infty} B_{\infty} a(i,j)=a(j,i),\  b(i,j)=b(j,i) h(i,j)=h(j,j)+1 i\neq j A_2,B_2 H_2 n a_n,b_n G_nA_n=a_nI_n+(a_n+2)H_n G_nB_n=b_nI_n+(b_n-2)H_n 
\begin{array}{|c|c|c|c|c|c|c|c|c|c|}
\hline
n & 2 & 3 & 4 & 5 & 6  & 7 & 8 & 9 & 10 \\
\hline
a_n & 0 & 1 & \frac{10}{3} & \frac{91}{12} & \frac{73}{5}  & \frac{9199}{360}   & \frac{13232}{315} &  \frac{147879}{2240}  & \frac{129929}{1296} \\
\hline
b_n & 2 & 5 & 10 & \frac{73}{4} & \frac{175}{5}  & \frac{6197}{120}   & \frac{1228}{15} &  \frac{40273}{320}  & \frac{81427}{432} \\
\hline
h(n,n) & 0 & 0 & \frac{1}{3} & \frac{5}{6} & \frac{3}{2} & \frac{85}{36}  & \frac{871}{252}   & \frac{1083}{224} &  \frac{118939}{18144}  \\
\hline
\end{array}
\tag{4}
 A_{\infty},B_{\infty} H_{\infty} A_2,B_2,H_2 2\times 2 A_n B_n","['linear-algebra', 'sequences-and-series', 'symmetric-matrices']"
2,Rewriting $\max \log \det (I+ X + Y + Y^T)$ as max-det problem,Rewriting  as max-det problem,\max \log \det (I+ X + Y + Y^T),"The following optimization is convex: \begin{align} \max_{X\succ0,Y} &\ \ \log \det (I + X +Y + Y^T)\\ & \text{s.t.} \begin{pmatrix} X& Y \\ Y^T & Z \end{pmatrix}\succeq 0, \ \ \ \mathbf{Tr}(X)\le P \end{align} where $Z\succ0$ and $P>0$ are given. My question is whether the decision variable $Y$ can be transformed or replaced with a be positive semi-definite decision variable. My motivation is to write the optimization as a standard max-det problem or in a nicer form in order to show that the maximizer is unique.",The following optimization is convex: where and are given. My question is whether the decision variable can be transformed or replaced with a be positive semi-definite decision variable. My motivation is to write the optimization as a standard max-det problem or in a nicer form in order to show that the maximizer is unique.,"\begin{align}
\max_{X\succ0,Y} &\ \ \log \det (I + X +Y + Y^T)\\
& \text{s.t.} \begin{pmatrix} X& Y \\ Y^T & Z \end{pmatrix}\succeq 0, \ \ \ \mathbf{Tr}(X)\le P
\end{align} Z\succ0 P>0 Y","['linear-algebra', 'optimization', 'convex-optimization', 'control-theory']"
3,How many $n \times n$ integer matrices with bounded entries are diagonalizable?,How many  integer matrices with bounded entries are diagonalizable?,n \times n,"I am trying to do some calculations involving the total number of diagonalizable $n \times n$ integer matrices whose coefficients are within $[-k,k]$ . Denote this number $g_n(k)$ . Using Mathematica, I have been able to compute $$g_2(5)=11690,g_2(6)=23744, g_2(7)=43290, g_2(8)=72864.$$ I computed these by brute force, ie. checking every $2 \times 2$ matrix with integer entries in $[-k,k]$ . This is quite inefficient! Is there an explicit formula for $g_2(k)$ ? What about for $g_n(k)$ ? I hoped this sequence would be on the OEIS, but it was not.","I am trying to do some calculations involving the total number of diagonalizable integer matrices whose coefficients are within . Denote this number . Using Mathematica, I have been able to compute I computed these by brute force, ie. checking every matrix with integer entries in . This is quite inefficient! Is there an explicit formula for ? What about for ? I hoped this sequence would be on the OEIS, but it was not.","n \times n [-k,k] g_n(k) g_2(5)=11690,g_2(6)=23744, g_2(7)=43290, g_2(8)=72864. 2 \times 2 [-k,k] g_2(k) g_n(k)","['linear-algebra', 'combinatorics', 'matrices']"
4,any method to estimate determinant of a matrix?,any method to estimate determinant of a matrix?,,"I'm writing a kernel on GPU to compute determinant for my image processing app. The matrices are binary (only have value $[0,1]$ ), sparse, and have size $32\times32$ . Is there any method for estimating determinants quickly? The error can be high, $10$ %, even $20$ %. Some relevant discussions & papers: https://mathoverflow.net/questions/180604/are-bounds-known-for-the-maximum-determinant-of-a-0-1-matrix-of-specified-size?rq=1 https://stackoverflow.com/questions/47135703/determinant-of-sparse-matrix https://www.sciencedirect.com/science/article/abs/pii/S0024379518302593","I'm writing a kernel on GPU to compute determinant for my image processing app. The matrices are binary (only have value ), sparse, and have size . Is there any method for estimating determinants quickly? The error can be high, %, even %. Some relevant discussions & papers: https://mathoverflow.net/questions/180604/are-bounds-known-for-the-maximum-determinant-of-a-0-1-matrix-of-specified-size?rq=1 https://stackoverflow.com/questions/47135703/determinant-of-sparse-matrix https://www.sciencedirect.com/science/article/abs/pii/S0024379518302593","[0,1] 32\times32 10 20","['linear-algebra', 'matrices', 'determinant']"
5,"Example of a field $F$ and a structure $(V,+,s)$ satisfying all but one of the vector space axioms.",Example of a field  and a structure  satisfying all but one of the vector space axioms.,"F (V,+,s)","I have to find an example (if there exists one) of a field $F$ and a structure $(V,+,s)$ (where $s$ stands for scalar multiplication)   satisfying all but one of the axioms of a vector space, namely the distributivity  of vector addition: For all $c ∈ F$ and $v,w ∈ V$ , $c(v + w) = cv + cw$ I believe such example exists, but I'm not sure where to start. An exercise I did before asked to show how the axiom follows from the other axioms if $F=\mathbb{Q}$ . So I think perhaps using a different field like $\mathbb{R}$ (or anything similar) could be of use. Could anyone give me some advice or hint to solve it? Thanks!","I have to find an example (if there exists one) of a field and a structure (where stands for scalar multiplication)   satisfying all but one of the axioms of a vector space, namely the distributivity  of vector addition: For all and , I believe such example exists, but I'm not sure where to start. An exercise I did before asked to show how the axiom follows from the other axioms if . So I think perhaps using a different field like (or anything similar) could be of use. Could anyone give me some advice or hint to solve it? Thanks!","F (V,+,s) s c ∈ F v,w ∈ V c(v + w) = cv + cw F=\mathbb{Q} \mathbb{R}","['linear-algebra', 'abstract-algebra', 'vector-spaces']"
6,Proving or disproving this matrix $V$ is invertible.,Proving or disproving this matrix  is invertible.,V,"Here is a question on the invertibility of a special structured matrix: Notations: Let us take $n\in \mathbb{N}^*$ bins and $d\in \mathbb{N}^*$ balls. Denote the set $B = \{\alpha^1, \ldots, \alpha^m\}$ to be all possible choices for putting $d$ balls into $n$ bins, such as $$\alpha^1 = (d,0,\ldots, 0), ~ \alpha^2 = (0,d,\ldots, 0), \ldots$$ Let us define the matrix $V$ as: $$V = \begin{bmatrix} (\alpha^1)^{\alpha^1} & \cdots & (\alpha^1)^{\alpha^m}\\ (\alpha^2)^{\alpha^1} & \cdots & (\alpha^2)^{\alpha^m}\\ \vdots & \vdots & \vdots\\ (\alpha^m)^{\alpha^1} & \cdots & (\alpha^m)^{\alpha^m} \end{bmatrix}$$ where the notation $(\alpha^i)^{\alpha^j} = \displaystyle\prod_{k=1}^{n}(\alpha^i_k)^{\alpha^j_k}$ (under assumption that $0^0=1$ , and $\alpha^i_k$ indicates the $k$ th element of $\alpha^i$ ). Question: "" is the matrix $V$ invertible? "" I have tested several examples, and it seems that $V$ is always invertible, but I have no idea how to prove it or find a counterexample. Here are two facts I understood: all diagonal elements are strictly positives, so the trace of $V$ is strictly positive. the matrix $V$ is not symmetric. Can anyone help prove or disprove the invertibility of $V$ ? Thanks a lot in advance for sharing any idea, any useful discussion and remark. For a better understanding of the problem, I add here an example: Example: Let $n=3$ and $d=2$ , then we have all possible choices for putting $2$ balls into $3$ bins as: $$B = \{(2,0,0),(0,2,0),(0,0,2),(1,1,0),(1,0,1),(0,1,1)\}.$$ The elements in the first row of the matrix $V$ are computed as: $$(\alpha^1)^{\alpha^1} = (2,0,0)^{(2,0,0)} = 2^2\times 0^0\times 0^0 = 4,$$ $$(\alpha^1)^{\alpha^2} = (2,0,0)^{(0,2,0)} = 2^0\times 0^2\times 0^0 = 0,$$ and so on. Thus, we have the matrix $V$ as: $$V = \begin{bmatrix} 4&0&0&0&0&0\\     0&4&0&0&0&0\\     0&0&4&0&0&0\\     1&1&0&1&0&0\\     1&0&1&0&1&0\\     0&1&1&0&0&1\\ \end{bmatrix} $$ which is clearly invertible. Remarks: Note that, the matrix $V$ is not always triangular. as an example, for n=3, d=3, the element $$(2,1,0)^{(1,2,0)} = 2^1\times 1^2 \times 0^0 = 2,$$ and $$(1,2,0)^{(2,1,0)} = 1^2\times 2^1 \times 0^0 = 2$$ which are both non-zeros, thus $V$ will be never invertible in this case. When the elements $(\alpha^i)^{\alpha^j}$ and $(\alpha^j)^{\alpha^i}$ (with $i\neq j$ ) are non-zeros, they may not be equal. E.g., n=4, d=8, we have $$(1,1,2,4)^{(2,2,2,2)} = 64$$ but $$(2,2,2,2)^{(1,1,2,4)} = 256.$$ As a conclusion: ""the matrix $V$ is neither triangular nor symmetric in general.""","Here is a question on the invertibility of a special structured matrix: Notations: Let us take bins and balls. Denote the set to be all possible choices for putting balls into bins, such as Let us define the matrix as: where the notation (under assumption that , and indicates the th element of ). Question: "" is the matrix invertible? "" I have tested several examples, and it seems that is always invertible, but I have no idea how to prove it or find a counterexample. Here are two facts I understood: all diagonal elements are strictly positives, so the trace of is strictly positive. the matrix is not symmetric. Can anyone help prove or disprove the invertibility of ? Thanks a lot in advance for sharing any idea, any useful discussion and remark. For a better understanding of the problem, I add here an example: Example: Let and , then we have all possible choices for putting balls into bins as: The elements in the first row of the matrix are computed as: and so on. Thus, we have the matrix as: which is clearly invertible. Remarks: Note that, the matrix is not always triangular. as an example, for n=3, d=3, the element and which are both non-zeros, thus will be never invertible in this case. When the elements and (with ) are non-zeros, they may not be equal. E.g., n=4, d=8, we have but As a conclusion: ""the matrix is neither triangular nor symmetric in general.""","n\in \mathbb{N}^* d\in \mathbb{N}^* B = \{\alpha^1, \ldots, \alpha^m\} d n \alpha^1 = (d,0,\ldots, 0), ~ \alpha^2 = (0,d,\ldots, 0), \ldots V V = \begin{bmatrix}
(\alpha^1)^{\alpha^1} & \cdots & (\alpha^1)^{\alpha^m}\\
(\alpha^2)^{\alpha^1} & \cdots & (\alpha^2)^{\alpha^m}\\
\vdots & \vdots & \vdots\\
(\alpha^m)^{\alpha^1} & \cdots & (\alpha^m)^{\alpha^m}
\end{bmatrix} (\alpha^i)^{\alpha^j} = \displaystyle\prod_{k=1}^{n}(\alpha^i_k)^{\alpha^j_k} 0^0=1 \alpha^i_k k \alpha^i V V V V V n=3 d=2 2 3 B = \{(2,0,0),(0,2,0),(0,0,2),(1,1,0),(1,0,1),(0,1,1)\}. V (\alpha^1)^{\alpha^1} = (2,0,0)^{(2,0,0)} = 2^2\times 0^0\times 0^0 = 4, (\alpha^1)^{\alpha^2} = (2,0,0)^{(0,2,0)} = 2^0\times 0^2\times 0^0 = 0, V V = \begin{bmatrix}
4&0&0&0&0&0\\
    0&4&0&0&0&0\\
    0&0&4&0&0&0\\
    1&1&0&1&0&0\\
    1&0&1&0&1&0\\
    0&1&1&0&0&1\\
\end{bmatrix}
 V (2,1,0)^{(1,2,0)} = 2^1\times 1^2 \times 0^0 = 2, (1,2,0)^{(2,1,0)} = 1^2\times 2^1 \times 0^0 = 2 V (\alpha^i)^{\alpha^j} (\alpha^j)^{\alpha^i} i\neq j (1,1,2,4)^{(2,2,2,2)} = 64 (2,2,2,2)^{(1,1,2,4)} = 256. V","['linear-algebra', 'matrices', 'inverse']"
7,Phase angles of a complex eigenvector,Phase angles of a complex eigenvector,,"I have the following system for $\lambda \in \Bbb C, \lambda \neq 0$ and $\pmb{p},\pmb{q} \in \Bbb C^n$ , $(\pmb{p}^T, \pmb{q}^T)\neq0$ : $$\begin{cases} F(\lambda) \pmb{p} - g(\lambda) \pmb{q} - \lambda \alpha\beta M^{-1}C \pmb{q}   = 0 \ , \\  g(\lambda) \pmb{p} + F(\lambda) \pmb{q} + k \alpha\beta M^{-1}C \pmb{p}   = 0 \ . \end{cases} $$ Where $F(\lambda) = \lambda^3 I + \lambda^2(\alpha I + \beta D) + \lambda \alpha \beta D$ , $g(\lambda) =  \lambda^2\beta + \lambda \alpha \beta$ , $\alpha, \beta, k$ are positive scalars, $D,M$ are $n\times n$ diagonal matrices with positive elements on its diagonals, $C$ is $n\times n$ real positive semi-definite matrix (non-diagonal). For which matrix $D$ phase angles of $\pmb{p}$ are opposite to $\pmb{q}$ (namely, $p_k = |p_k|e^{i\phi_k}$ and $q_k = |q_k|e^{-i\phi_k}, \ k=1,\cdots,n$ )? I suspect it is valid only if $D = dI$ . Because in this case, $F(\lambda) = [\lambda^3 + \lambda^2(\alpha + \beta d) + \lambda \alpha \beta d]I = f(\lambda)I$ and $\pmb{p}$ will be an eigenvector of $M^{-1}C$ . Further, $\pmb{q}= -\frac{g(\lambda) + k\alpha\beta\mu}{f(\lambda)} \pmb{p}$ , where $\mu$ is the correspoding eigenvalue of $M^{-1}C$ such that difference between phase angles of $\pmb{p}$ and $\pmb{q}$ is $\phi_0 = \angle  \frac{g(\lambda) + k\alpha\beta\mu}{f(\lambda)}$ , and the pair $(\pmb{p} = e^{-i\frac{\phi_0}{2}}\pmb{\psi}, \pmb{q}= - e^{-i\frac{\phi_0}{2}}\frac{g(\lambda) + k\alpha\beta\mu}{f(\lambda)}\pmb{\psi})$ with $M^{-1}C$ eigenvector $\pmb{\psi} \in \Bbb R^n$ will have the opposite phase angles. I do not know how to show $D=dI$ is necessary or find conterexample with $D\neq d I$ .","I have the following system for and , : Where , , are positive scalars, are diagonal matrices with positive elements on its diagonals, is real positive semi-definite matrix (non-diagonal). For which matrix phase angles of are opposite to (namely, and )? I suspect it is valid only if . Because in this case, and will be an eigenvector of . Further, , where is the correspoding eigenvalue of such that difference between phase angles of and is , and the pair with eigenvector will have the opposite phase angles. I do not know how to show is necessary or find conterexample with .","\lambda \in \Bbb C, \lambda \neq 0 \pmb{p},\pmb{q} \in \Bbb C^n (\pmb{p}^T, \pmb{q}^T)\neq0 \begin{cases} F(\lambda) \pmb{p} - g(\lambda) \pmb{q} - \lambda \alpha\beta M^{-1}C \pmb{q} 
 = 0 \ , \\  g(\lambda) \pmb{p} + F(\lambda) \pmb{q} + k \alpha\beta M^{-1}C \pmb{p} 
 = 0 \ . \end{cases}  F(\lambda) = \lambda^3 I + \lambda^2(\alpha I + \beta D) + \lambda \alpha \beta D g(\lambda) =  \lambda^2\beta + \lambda \alpha \beta \alpha, \beta, k D,M n\times n C n\times n D \pmb{p} \pmb{q} p_k = |p_k|e^{i\phi_k} q_k = |q_k|e^{-i\phi_k}, \ k=1,\cdots,n D = dI F(\lambda) = [\lambda^3 + \lambda^2(\alpha + \beta d) + \lambda \alpha \beta d]I = f(\lambda)I \pmb{p} M^{-1}C \pmb{q}= -\frac{g(\lambda) + k\alpha\beta\mu}{f(\lambda)} \pmb{p} \mu M^{-1}C \pmb{p} \pmb{q} \phi_0 = \angle  \frac{g(\lambda) + k\alpha\beta\mu}{f(\lambda)} (\pmb{p} = e^{-i\frac{\phi_0}{2}}\pmb{\psi}, \pmb{q}= - e^{-i\frac{\phi_0}{2}}\frac{g(\lambda) + k\alpha\beta\mu}{f(\lambda)}\pmb{\psi}) M^{-1}C \pmb{\psi} \in \Bbb R^n D=dI D\neq d I","['linear-algebra', 'complex-numbers', 'eigenvalues-eigenvectors']"
8,Fast computation of $\det(A^\top A)$ with $A$ lower triangular,Fast computation of  with  lower triangular,\det(A^\top A) A,"Let $A \in \mathbb{R}^{m \times n}$ , where $m \geq n$ , be a lower triangular matrix, i.e. $A_{ij} = 0$ whenever $j > i$ . I am interested in computing $\det(A^\top A)$ as efficiently as possible, and would rather avoid having to compute the product $A^\top A$ . In the case where $m=n$ , the problem simplifies as: $$\det(A^\top A) = \det(A^\top) \det(A) = \det(A)^2 = \prod_{i=1}^n A_{ii}^2$$ Is there a similar simplification (or any other) for the case where $m > n$ ? Thanks!","Let , where , be a lower triangular matrix, i.e. whenever . I am interested in computing as efficiently as possible, and would rather avoid having to compute the product . In the case where , the problem simplifies as: Is there a similar simplification (or any other) for the case where ? Thanks!",A \in \mathbb{R}^{m \times n} m \geq n A_{ij} = 0 j > i \det(A^\top A) A^\top A m=n \det(A^\top A) = \det(A^\top) \det(A) = \det(A)^2 = \prod_{i=1}^n A_{ii}^2 m > n,"['linear-algebra', 'numerical-linear-algebra']"
9,Is there a classification of bivector-valued cross products?,Is there a classification of bivector-valued cross products?,,"Background Vector-valued cross products Let $\mathbb{F}$ be an field of characteristic $0$ . A $k$ -ary cross product in the vector space $V=\mathbb{F}^n$ equipped with an inner product $\langle \cdot, \cdot \rangle$ is a $k$ -linear map $\times: V^k \to V$ which outputs a vector with the following two properties: It is orthogonal to all inputs. Its norm is the volume of the $k$ -dimensional paralellotope formed by these inputs, that is, $$|\!\times\!\!(\mathbf{v}_1,\mathbf{v}_2,\ldots,\mathbf{v}_k)|^2 = \det \langle \mathbf{v}_i, \mathbf{v}_j \rangle,$$ where the right hand side is a Gram determinant . These two conditions turn out to imply that composition with the inner product $\langle \times, \cdot \rangle$ ("" lowering an index "") defines a totally antisymmetric $(k+1)$ -linear map $\times : V^{k+1} \to \mathbb{F}$ , i.e. an alternating $(k+1)$ -form. Equivalently, given a $(k+1)$ -form satisfying the appropriate volume condition one can define a cross product by ""raising an index"". In the following we restrict to $\mathbb{F}=\mathbb{R}$ with the standard inner product. The dimensions where a cross product exists have been completely classified (see for example here ): There is a trivial $k$ -ary cross product for any $k\ge n+1$ , which is identically zero. A nullary cross product (equivalently a unit vector) exists for any $n\ge 1$ . A unary cross product (corresponding to a symplectic form) exists in any even dimension $n=2m$ . A $(n-1)$ -ary cross product (corresponding to a volume form) exists in any dimension $n$ . For example, the ordinary cross product in 3D is of this kind. There are only two exceptional cross products not covered by the above cases, binary in dimension $7$ (corresponding to an associative $3$ -form ) and ternary in dimension $8$ (corresponding to a Cayley $4$ -form ). They can both be expressed in terms of the octonions, see for example this question for details. So we have the following table: $$\begin{array} {|c|c|c|c|c|}  \hline k\setminus n & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & \cdots\\ \hline 0 & & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \ldots\\ \hline 1 & \checkmark & & \checkmark & & \checkmark & & \checkmark & & \checkmark & & \checkmark & & \ldots\\ \hline 2 & \checkmark & \checkmark & & \checkmark & & & & \checkmark & & & & &\\ \hline 3 & \checkmark & \checkmark & \checkmark & & \checkmark & & & & \checkmark & & & &\\ \hline 4 & \checkmark & \checkmark & \checkmark & \checkmark & & \checkmark & & & & & & &\\ \hline 5 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & &\checkmark & & & & & &\\ \hline 6 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & &\checkmark & & & & &\\ \hline \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & & \ddots & & & &\\ \hline\end{array}$$ Note that if there is a $k$ -ary product in dimension $n$ , there is automatically a $(k-1)$ -ary product in dimension $n-1$ obtained by contracting with an unit vector. This explains the diagonal patterns. Bivector-valued cross products Similarly, we can define a $k$ -ary bivector-valued cross product (BV cross product or BVCP for short), which is a $k$ -linear map $\times : V^k \to \Lambda^2 V$ outputting a bivector with the following two properties: It is orthogonal to all inputs (i.e. the left contraction $\mathbf{v}_j \: \lrcorner \times\!\!(\mathbf{v}_1,\mathbf{v}_2,\ldots,\mathbf{v}_k)$ vanishes for all $1\le j \le k$ ). Its norm is the volume of the paralellotope formed by these inputs. As before, given any BV cross product we may define a corresponding $(k+2)$ -form and viceversa, by lowering or raising two indices. Note that the wedge/outer product of two vectors is not a BV cross product, since it doesn't satisfy the first condition (in fact it does the complete opposite, since the plane spanned by $a \wedge b$ contains $a$ and $b$ ). So far I haven't been able to find a classification of BV cross products anywhere (this might be because I've been searching for the wrong terms). However, we can give a partial classification by trying to find bivector-valued analogues of each class of vector-valued cross product: There are trivial $k$ -ary BV cross products for any $k\ge n+1$ . A nullary BV cross product, or equivalently a bivector of norm $1$ , exists for any $n\ge 2$ . A unary BV cross product exists in any dimension $n=3a+7b$ , since then we can decompose $V$ into the direct sum of vector spaces of the form $\mathbb{R}^3$ and $\mathbb{R}^7$ , and take a corresponding $3$ -form which restricts to a suitable multiple of the volume form in each $\mathbb{R}^3$ and a multiple of the associative $3$ -form in each $\mathbb{R}^7$ . On the other hand, no unary product exists for $n = 1, 2$ for dimensional reasons. Since every number greater than $11$ can be expressed as $3a+7b$ for some $a, b$ , this leaves in doubt only the cases $n=4, 5, 8$ and $11$ . (The cases $n=4, 5$ are now ruled out and $n=8, 11$ are confirmed, see the updates). A $(n-2)$ -ary cross product (corresponding to a properly normalized volume form) exists in any dimension $n$ . The Hodge dual of the associative $3$ -form, properly normalized, induces a binary BV cross product in dimension $7$ , and similarly the Cayley form itself induces another in dimension $8$ . I'm sure there is a better way to prove it, but I just used the well-known correspondence between bivectors and skew-symmetric matrices and used a CAS to check symbolically that the relations $M_{\mathbf{x}\times\mathbf{y}}\mathbf{x} = M_{\mathbf{x}\times\mathbf{y}}\mathbf{y} = 0$ and $||M_{\mathbf{x}\times\mathbf{y}}|| = |\mathbf{x}\wedge\mathbf{y}|$ hold, where $||\cdot||$ is the Frobenius norm. Here is a tentative table summarizing the above: $$\begin{array} {|c|c|c|c|c|}  \hline k\setminus n & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & \cdots\\ \hline 0 & \color{darkred}\bullet & \color{darkred}\bullet & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \ldots\\ \hline 1 & \checkmark & & & \checkmark & \color{darkred}\bullet & \color{darkred}\bullet & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \ldots\\ \hline 2 & \checkmark & \checkmark & & & \checkmark & & & \checkmark & \checkmark & & & &\\ \hline 3 & \checkmark & \checkmark & \checkmark & & & \checkmark & & & \color{darkred}\bullet & \color{darkred}\bullet & & &\\ \hline 4 & \checkmark & \checkmark & \checkmark & \checkmark & & & \checkmark & & & & & &\\ \hline 5 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & & &\checkmark & & & & &\\ \hline 6 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & & &\checkmark & & & &\\ \hline \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & & & \ddots & & &\\ \hline\end{array}$$ Question My question is: Is the list above correct? Are there any other possible BV cross products, e.g. binary in dimension $10$ ? UPDATE: We can generalize the Hodge dual argument we used to get the $7$ -dimensional binary BVCP to rule out the existence of BVCPs for some $n$ and $k$ . First, note that if we know a cross product doesn't exist for some $n$ and $k$ , automatically we know that none exists for $n+a$ and $k+a$ with any $a>0$ , since otherwise we could contract the latter with $a$ unit vectors and obtain a contradiction. A $p$ -vector-valued cross product is the obvious generalization of vector-valued ( $p=1$ ) and bivector-valued ( $p=2$ ) cross products to multivectors of arbitrary degree. With some effort, I have proven that a $(n,k,p)$ -cross product induces a $(n,k,n-2k-p)$ -cross product by lowering $k$ indices, taking the Hodge dual, raising $k$ indices again and renormalizing. Conversely, if there are no $(n,k,p)$ -cross products, we can deduce the nonexistence of $(n,k,n-2k-p)$ -cross products. Note that for scalar-valued cross products ( $p=0$ ) the orthogonality condition is trivially satisfied. Their classification is easy to state: they exist only for $k\ge n$ or for $k=0$ , $n$ arbitrary. Using the classifications of scalar and vector-valued products and applying Hodge duality to $(n,k,p) = (4,1,0)$ , $(5,1,1)$ , $(8,3,0)$ and $(9,3,1)$ , we prove the nonexistence of BVCPs at $(4,1,2)$ , $(5,1,2)$ , $(8,3,2)$ and $(9,3,2)$ , and of any others as we move diagonally in the bottom right direction. I marked some cells in the table with dark-red dots where we know BVCPs don't exist. Note that the diagonal below any dot is also ruled out by the discussion above. In light of this new information, the question can now be divided in two main subquestions: Are there unary BVCPs in dimensions $8$ and $11$ ? UPDATE 2: I have found an $8$ -dimensional unary BVCP satisfying the conditions, whose corresponding $3$ -form is given by a rescaling of the structure constants of the Lie algebra $\mathfrak{su}(3)$ . It follows that an unary product exists in dimensions $n=3a+7b+8c$ , and in particular also for $n=11$ . I updated the table with these two new cases. Are there binary BVCPs in dimension greater than $8$ ?","Background Vector-valued cross products Let be an field of characteristic . A -ary cross product in the vector space equipped with an inner product is a -linear map which outputs a vector with the following two properties: It is orthogonal to all inputs. Its norm is the volume of the -dimensional paralellotope formed by these inputs, that is, where the right hand side is a Gram determinant . These two conditions turn out to imply that composition with the inner product ("" lowering an index "") defines a totally antisymmetric -linear map , i.e. an alternating -form. Equivalently, given a -form satisfying the appropriate volume condition one can define a cross product by ""raising an index"". In the following we restrict to with the standard inner product. The dimensions where a cross product exists have been completely classified (see for example here ): There is a trivial -ary cross product for any , which is identically zero. A nullary cross product (equivalently a unit vector) exists for any . A unary cross product (corresponding to a symplectic form) exists in any even dimension . A -ary cross product (corresponding to a volume form) exists in any dimension . For example, the ordinary cross product in 3D is of this kind. There are only two exceptional cross products not covered by the above cases, binary in dimension (corresponding to an associative -form ) and ternary in dimension (corresponding to a Cayley -form ). They can both be expressed in terms of the octonions, see for example this question for details. So we have the following table: Note that if there is a -ary product in dimension , there is automatically a -ary product in dimension obtained by contracting with an unit vector. This explains the diagonal patterns. Bivector-valued cross products Similarly, we can define a -ary bivector-valued cross product (BV cross product or BVCP for short), which is a -linear map outputting a bivector with the following two properties: It is orthogonal to all inputs (i.e. the left contraction vanishes for all ). Its norm is the volume of the paralellotope formed by these inputs. As before, given any BV cross product we may define a corresponding -form and viceversa, by lowering or raising two indices. Note that the wedge/outer product of two vectors is not a BV cross product, since it doesn't satisfy the first condition (in fact it does the complete opposite, since the plane spanned by contains and ). So far I haven't been able to find a classification of BV cross products anywhere (this might be because I've been searching for the wrong terms). However, we can give a partial classification by trying to find bivector-valued analogues of each class of vector-valued cross product: There are trivial -ary BV cross products for any . A nullary BV cross product, or equivalently a bivector of norm , exists for any . A unary BV cross product exists in any dimension , since then we can decompose into the direct sum of vector spaces of the form and , and take a corresponding -form which restricts to a suitable multiple of the volume form in each and a multiple of the associative -form in each . On the other hand, no unary product exists for for dimensional reasons. Since every number greater than can be expressed as for some , this leaves in doubt only the cases and . (The cases are now ruled out and are confirmed, see the updates). A -ary cross product (corresponding to a properly normalized volume form) exists in any dimension . The Hodge dual of the associative -form, properly normalized, induces a binary BV cross product in dimension , and similarly the Cayley form itself induces another in dimension . I'm sure there is a better way to prove it, but I just used the well-known correspondence between bivectors and skew-symmetric matrices and used a CAS to check symbolically that the relations and hold, where is the Frobenius norm. Here is a tentative table summarizing the above: Question My question is: Is the list above correct? Are there any other possible BV cross products, e.g. binary in dimension ? UPDATE: We can generalize the Hodge dual argument we used to get the -dimensional binary BVCP to rule out the existence of BVCPs for some and . First, note that if we know a cross product doesn't exist for some and , automatically we know that none exists for and with any , since otherwise we could contract the latter with unit vectors and obtain a contradiction. A -vector-valued cross product is the obvious generalization of vector-valued ( ) and bivector-valued ( ) cross products to multivectors of arbitrary degree. With some effort, I have proven that a -cross product induces a -cross product by lowering indices, taking the Hodge dual, raising indices again and renormalizing. Conversely, if there are no -cross products, we can deduce the nonexistence of -cross products. Note that for scalar-valued cross products ( ) the orthogonality condition is trivially satisfied. Their classification is easy to state: they exist only for or for , arbitrary. Using the classifications of scalar and vector-valued products and applying Hodge duality to , , and , we prove the nonexistence of BVCPs at , , and , and of any others as we move diagonally in the bottom right direction. I marked some cells in the table with dark-red dots where we know BVCPs don't exist. Note that the diagonal below any dot is also ruled out by the discussion above. In light of this new information, the question can now be divided in two main subquestions: Are there unary BVCPs in dimensions and ? UPDATE 2: I have found an -dimensional unary BVCP satisfying the conditions, whose corresponding -form is given by a rescaling of the structure constants of the Lie algebra . It follows that an unary product exists in dimensions , and in particular also for . I updated the table with these two new cases. Are there binary BVCPs in dimension greater than ?","\mathbb{F} 0 k V=\mathbb{F}^n \langle \cdot, \cdot \rangle k \times: V^k \to V k |\!\times\!\!(\mathbf{v}_1,\mathbf{v}_2,\ldots,\mathbf{v}_k)|^2 = \det \langle \mathbf{v}_i, \mathbf{v}_j \rangle, \langle \times, \cdot \rangle (k+1) \times : V^{k+1} \to \mathbb{F} (k+1) (k+1) \mathbb{F}=\mathbb{R} k k\ge n+1 n\ge 1 n=2m (n-1) n 7 3 8 4 \begin{array}
{|c|c|c|c|c|} 
\hline
k\setminus n & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & \cdots\\
\hline
0 & & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \ldots\\
\hline
1 & \checkmark & & \checkmark & & \checkmark & & \checkmark & & \checkmark & & \checkmark & & \ldots\\
\hline
2 & \checkmark & \checkmark & & \checkmark & & & & \checkmark & & & & &\\
\hline
3 & \checkmark & \checkmark & \checkmark & & \checkmark & & & & \checkmark & & & &\\
\hline
4 & \checkmark & \checkmark & \checkmark & \checkmark & & \checkmark & & & & & & &\\
\hline
5 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & &\checkmark & & & & & &\\
\hline
6 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & &\checkmark & & & & &\\
\hline
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & & \ddots & & & &\\
\hline\end{array} k n (k-1) n-1 k k \times : V^k \to \Lambda^2 V \mathbf{v}_j \: \lrcorner \times\!\!(\mathbf{v}_1,\mathbf{v}_2,\ldots,\mathbf{v}_k) 1\le j \le k (k+2) a \wedge b a b k k\ge n+1 1 n\ge 2 n=3a+7b V \mathbb{R}^3 \mathbb{R}^7 3 \mathbb{R}^3 3 \mathbb{R}^7 n = 1, 2 11 3a+7b a, b n=4, 5, 8 11 n=4, 5 n=8, 11 (n-2) n 3 7 8 M_{\mathbf{x}\times\mathbf{y}}\mathbf{x} = M_{\mathbf{x}\times\mathbf{y}}\mathbf{y} = 0 ||M_{\mathbf{x}\times\mathbf{y}}|| = |\mathbf{x}\wedge\mathbf{y}| ||\cdot|| \begin{array}
{|c|c|c|c|c|} 
\hline
k\setminus n & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & \cdots\\
\hline
0 & \color{darkred}\bullet & \color{darkred}\bullet & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \ldots\\
\hline
1 & \checkmark & & & \checkmark & \color{darkred}\bullet & \color{darkred}\bullet & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \ldots\\
\hline
2 & \checkmark & \checkmark & & & \checkmark & & & \checkmark & \checkmark & & & &\\
\hline
3 & \checkmark & \checkmark & \checkmark & & & \checkmark & & & \color{darkred}\bullet & \color{darkred}\bullet & & &\\
\hline
4 & \checkmark & \checkmark & \checkmark & \checkmark & & & \checkmark & & & & & &\\
\hline
5 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & & &\checkmark & & & & &\\
\hline
6 & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & & &\checkmark & & & &\\
\hline
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots & & & \ddots & & &\\
\hline\end{array} 10 7 n k n k n+a k+a a>0 a p p=1 p=2 (n,k,p) (n,k,n-2k-p) k k (n,k,p) (n,k,n-2k-p) p=0 k\ge n k=0 n (n,k,p) = (4,1,0) (5,1,1) (8,3,0) (9,3,1) (4,1,2) (5,1,2) (8,3,2) (9,3,2) 8 11 8 3 \mathfrak{su}(3) n=3a+7b+8c n=11 8","['linear-algebra', 'recreational-mathematics', 'exterior-algebra', 'cross-product', 'octonions']"
10,How to find a Cartan subalgebra of $\mathfrak{so}(n)$?,How to find a Cartan subalgebra of ?,\mathfrak{so}(n),"To find a Cartan subalgebra for $\mathfrak{su}(n)$ we take advantage that $\mathfrak{su}(n)$ consists of anti-hermitian matrices. If a set of elements of $\mathfrak{su}(n)$ commute then they can be simultaneously diagonalized. This means that if we take a linearly independent set $\{X_a\}$ of commuting elements (which we shall take as a basis for the Cartan subalgebra), we can always find a unitary $U$ such that the set $\{U^\dagger X_a U\}$ is a set of linearly independent diagonal matrices. Thus a Cartan subalgebra can be generated by a maximal set of diagonal matrices of zero trace. This yields a subalgebra of dimension $n-1$ with the generators of the form $$\begin{pmatrix}i & 0 & \cdots & 0\\ 0 & 0 & \cdots & \vdots\\\vdots & \vdots & \ddots & \vdots\\ 0 & 0 & \cdots & -i\end{pmatrix},\quad \begin{pmatrix}0 & 0 & \cdots & 0\\ 0 & i & \cdots & \vdots\\\vdots & \vdots & \ddots & \vdots\\ 0 & 0 & \cdots & -i\end{pmatrix},\quad \dots$$ Now for $\mathfrak{so}(n)$ this seems harder. The elements have zero diagonal so we cannot even expect to have some clever argument that allows us to look for diagonal matrices. My only idea was to take the usual basis $\{E_a\}$ of $\mathfrak{so}(n)$ fix $X_1 = E_1$ and then look for matrices commuting with $X_1$ . Then inside that subspace pick some $X_2$ and look for matrices commuting with $X_1,X_2$ , and so on, until not being able to find any other matrix. This seems to work for specific values of $n$ , but there ought to be a better approach for general $n$ like $\mathfrak{su}(n)$ . So: how do we find a Cartan subalgebra for $\mathfrak{so}(n)$ ?","To find a Cartan subalgebra for we take advantage that consists of anti-hermitian matrices. If a set of elements of commute then they can be simultaneously diagonalized. This means that if we take a linearly independent set of commuting elements (which we shall take as a basis for the Cartan subalgebra), we can always find a unitary such that the set is a set of linearly independent diagonal matrices. Thus a Cartan subalgebra can be generated by a maximal set of diagonal matrices of zero trace. This yields a subalgebra of dimension with the generators of the form Now for this seems harder. The elements have zero diagonal so we cannot even expect to have some clever argument that allows us to look for diagonal matrices. My only idea was to take the usual basis of fix and then look for matrices commuting with . Then inside that subspace pick some and look for matrices commuting with , and so on, until not being able to find any other matrix. This seems to work for specific values of , but there ought to be a better approach for general like . So: how do we find a Cartan subalgebra for ?","\mathfrak{su}(n) \mathfrak{su}(n) \mathfrak{su}(n) \{X_a\} U \{U^\dagger X_a U\} n-1 \begin{pmatrix}i & 0 & \cdots & 0\\ 0 & 0 & \cdots & \vdots\\\vdots & \vdots & \ddots & \vdots\\ 0 & 0 & \cdots & -i\end{pmatrix},\quad \begin{pmatrix}0 & 0 & \cdots & 0\\ 0 & i & \cdots & \vdots\\\vdots & \vdots & \ddots & \vdots\\ 0 & 0 & \cdots & -i\end{pmatrix},\quad \dots \mathfrak{so}(n) \{E_a\} \mathfrak{so}(n) X_1 = E_1 X_1 X_2 X_1,X_2 n n \mathfrak{su}(n) \mathfrak{so}(n)","['linear-algebra', 'abstract-algebra', 'representation-theory', 'lie-algebras', 'problem-solving']"
11,Kernel and image of linear operator $\alpha(f) = f(t+1) - f(t)$,Kernel and image of linear operator,\alpha(f) = f(t+1) - f(t),"Let $V = K[t]_n = \{f \in K[t], \deg f \le n\}$ . Find kernel and image of the operator $\alpha(f) = f(t+1) - f(t)$ . My attempt: I can take a standard basis $1, t, \cdots, t^n$ and find matrix of the linear operator: $\begin{bmatrix}  0&  \binom{1}{0}&  \binom{2}{0}&  .& \binom{n}{0} \\   0&  0&  \binom{2}{1}&  .& \binom{n}{1}\\   .&  .&  0&  .&. \\   .&  .&  .&  .&\binom{n}{n - 1} \\   0&  0&  0&  0&0  \end{bmatrix}$ So, if $\text{char}K = 0$ , then $\text{rank } A = \dim\text{Im } \alpha = n$ (it is $K[t]_{n - 1}$ ), and $\ker \alpha = K[t]_0 = K$ But I don't know how to solve it if $\text{char} K\ne 0$ ( $K$ is a field)","Let . Find kernel and image of the operator . My attempt: I can take a standard basis and find matrix of the linear operator: So, if , then (it is ), and But I don't know how to solve it if ( is a field)","V = K[t]_n = \{f \in K[t], \deg f \le n\} \alpha(f) = f(t+1) - f(t) 1, t, \cdots, t^n \begin{bmatrix}
 0&  \binom{1}{0}&  \binom{2}{0}&  .& \binom{n}{0} \\ 
 0&  0&  \binom{2}{1}&  .& \binom{n}{1}\\ 
 .&  .&  0&  .&. \\ 
 .&  .&  .&  .&\binom{n}{n - 1} \\ 
 0&  0&  0&  0&0 
\end{bmatrix} \text{char}K = 0 \text{rank } A = \dim\text{Im } \alpha = n K[t]_{n - 1} \ker \alpha = K[t]_0 = K \text{char} K\ne 0 K","['linear-algebra', 'abstract-algebra', 'matrices', 'elementary-number-theory']"
12,When a linear map on exterior power is induced,When a linear map on exterior power is induced,,"Let $k$ be a field, $V$ is a finite dimensional vector space. If $A: V \to V$ is a linear map, it induces a linear map $\wedge^k(A): \wedge^k(V) \to \wedge^k(V)$ . Suppose $B: \wedge^k(V) \to \wedge^k(V)$ is any linear map. What are conditions on $B$ that are equivalent to the existence of $A$ such that $B=\wedge^k(A)$ ? Notice that the map $A \to \wedge^k(A)$ is polynomial, so we have a polynomial map of affine spaces $\phi: \mathbb{A}^{n^2} \to \mathbb{A}^N$ , where $N ={n \choose k}^2$ . From the point of view of algebraic geometry I'm asking to find a set of generators of the ideal $I_X$ of the subvariety $X=\phi(\mathbb{A}^{n^2}) \subset \mathbb{A}^N$ .","Let be a field, is a finite dimensional vector space. If is a linear map, it induces a linear map . Suppose is any linear map. What are conditions on that are equivalent to the existence of such that ? Notice that the map is polynomial, so we have a polynomial map of affine spaces , where . From the point of view of algebraic geometry I'm asking to find a set of generators of the ideal of the subvariety .",k V A: V \to V \wedge^k(A): \wedge^k(V) \to \wedge^k(V) B: \wedge^k(V) \to \wedge^k(V) B A B=\wedge^k(A) A \to \wedge^k(A) \phi: \mathbb{A}^{n^2} \to \mathbb{A}^N N ={n \choose k}^2 I_X X=\phi(\mathbb{A}^{n^2}) \subset \mathbb{A}^N,"['linear-algebra', 'algebraic-geometry', 'reference-request', 'exterior-algebra']"
13,Invertibility (and eigen-decomposition) of complex symmetric matrix?,Invertibility (and eigen-decomposition) of complex symmetric matrix?,,"Consider $Z = K - i \omega S - \omega^2 M$ , where: $\omega$ is a positive real number; $K$ is a real, symmetric, and positive semi-definite matrix; $M$ is a real, symmetric, and positive definite matrix. $S$ is a non-zero matrix. Which property (or properties) $S$ should satisfy so that $Z$ is invertible? Or so that $(Z, M)$ has an eigendecomposition $Z = M \Phi \Lambda \Phi^T M$ (with $\Phi^T M \Phi = I$ )? Or which reference book discusses this class of problems? Remark 1: Matrices like $Z$ arise in the discretization of the Helmholtz equation with absorbing boundary conditions or of the frequency response analysis with damping. Remark 2: Many posts on the forum discuss the matrix $\left[ \begin{array}{cc} 1 & i \\ i & -1 \end{array} \right]$ as an example of non-diagonalizable complex symmetric matrix. Here I want to know which non-zero matrix $S$ would work. EDIT: Generalization of proportional damping gives us a family of $S$ matrices that would allow the eigendecomposition: $S = M \sum_{j = J_1}^{J_2} s_j \left( M^{-1} K \right)^j$","Consider , where: is a positive real number; is a real, symmetric, and positive semi-definite matrix; is a real, symmetric, and positive definite matrix. is a non-zero matrix. Which property (or properties) should satisfy so that is invertible? Or so that has an eigendecomposition (with )? Or which reference book discusses this class of problems? Remark 1: Matrices like arise in the discretization of the Helmholtz equation with absorbing boundary conditions or of the frequency response analysis with damping. Remark 2: Many posts on the forum discuss the matrix as an example of non-diagonalizable complex symmetric matrix. Here I want to know which non-zero matrix would work. EDIT: Generalization of proportional damping gives us a family of matrices that would allow the eigendecomposition:","Z = K - i \omega S - \omega^2 M \omega K M S S Z (Z, M) Z = M \Phi \Lambda \Phi^T M \Phi^T M \Phi = I Z \left[ \begin{array}{cc} 1 & i \\ i & -1 \end{array} \right] S S S = M \sum_{j = J_1}^{J_2} s_j \left( M^{-1} K \right)^j","['linear-algebra', 'eigenvalues-eigenvectors', 'inverse']"
14,Euclidean distance (cosine) between two random positive unit vectors in high dimensional space,Euclidean distance (cosine) between two random positive unit vectors in high dimensional space,,"I found out that the largest possible euclidean distance (which is the cosine) between two random positive unit vectors decreases as the dimension of vector increases and approximates 0.71. This was done by a simulation where I randomly sample unit vectors and compute the maximal pair-wise Euclidean distance. I found the value 0.71 intriguing. It's as if the maximal angle between all vectors is 45 degrees (cosine(45) ≈ 0.71), which is half of 90 degrees related to the constraint of having only positive elements. Why is that? I sense there is an intuitive explanation to this which can be generalized to other cases, e.g. without the positive vector constraints. You can use the following code to reproduce the simulation. import numpy as np from sklearn.preprocessing import Normalizer from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances matrices = [Normalizer(norm = 'l2').fit_transform(np.random.rand(1000, N)) for N in range(2,10000, 100)] res = [euclidean_distances(matrix, matrix).max() for matrix in matrices] plt.plot(range(len(res)),res) plot of the max euclidean distance with different dimension","I found out that the largest possible euclidean distance (which is the cosine) between two random positive unit vectors decreases as the dimension of vector increases and approximates 0.71. This was done by a simulation where I randomly sample unit vectors and compute the maximal pair-wise Euclidean distance. I found the value 0.71 intriguing. It's as if the maximal angle between all vectors is 45 degrees (cosine(45) ≈ 0.71), which is half of 90 degrees related to the constraint of having only positive elements. Why is that? I sense there is an intuitive explanation to this which can be generalized to other cases, e.g. without the positive vector constraints. You can use the following code to reproduce the simulation. import numpy as np from sklearn.preprocessing import Normalizer from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances matrices = [Normalizer(norm = 'l2').fit_transform(np.random.rand(1000, N)) for N in range(2,10000, 100)] res = [euclidean_distances(matrix, matrix).max() for matrix in matrices] plt.plot(range(len(res)),res) plot of the max euclidean distance with different dimension",,"['linear-algebra', 'probability', 'geometry', 'machine-learning']"
15,"Prove that the spectral radius $\rho(A)$ is a continuous function, where $A$ is a square matrix.","Prove that the spectral radius  is a continuous function, where  is a square matrix.",\rho(A) A,"Let $||. ||$ be some norm on $\mathbb{R^n}.$ Interpret the real-valued matrices as a Euclidean space, $\mathbb{R^{n^2}}=Mat_{n\times n},$ and prove that the following are continuous functions of the $n^2$ matrix entries $A_{ij}$ (a) The spectral radius of $A$ , $\rho(A)=\max\{|\lambda| ~~| \text{such that}~\lambda  ~\text{is an eigenvalue of}~A  \}.$ (b) The spectral radius of the inverse, $f(A)=\rho (A^{-1}),$ on the domain of invertible matrices. For part (b), I think the goal is $\displaystyle\lim_{\Delta A \to 0} \rho(A+\Delta A)=\rho(A).$ I tried to use Gelfand formula. Then I have $\rho(A)=\displaystyle\lim_{\Delta A \to 0} \rho(A+\Delta A)=\displaystyle\lim_{\Delta A \to 0} \lim_{k\to \infty}||(A+\Delta A)^k||^{1/k},$ which becomes more complicated. In addition, I tried to use the operator norm to bound the spectral radius, but I felt that I was still far away from the proof. Why does the part (b) need to prove if the part (a) is true? $f(A)=\rho(A) $ restrict on the domain of invertable matrices. I was confuled about part(b). I know how to prove this statement using a theorem in complex analysis: The roots of a complex-valued polynomial are continuous wrt the coefficients of the polynomial. By using this theorem, (b) is very easy to prove. I am looking for another proof of (b) without the theorem in complex analysis.","Let be some norm on Interpret the real-valued matrices as a Euclidean space, and prove that the following are continuous functions of the matrix entries (a) The spectral radius of , (b) The spectral radius of the inverse, on the domain of invertible matrices. For part (b), I think the goal is I tried to use Gelfand formula. Then I have which becomes more complicated. In addition, I tried to use the operator norm to bound the spectral radius, but I felt that I was still far away from the proof. Why does the part (b) need to prove if the part (a) is true? restrict on the domain of invertable matrices. I was confuled about part(b). I know how to prove this statement using a theorem in complex analysis: The roots of a complex-valued polynomial are continuous wrt the coefficients of the polynomial. By using this theorem, (b) is very easy to prove. I am looking for another proof of (b) without the theorem in complex analysis.","||. || \mathbb{R^n}. \mathbb{R^{n^2}}=Mat_{n\times n}, n^2 A_{ij} A \rho(A)=\max\{|\lambda| ~~| \text{such that}~\lambda 
~\text{is an eigenvalue of}~A  \}. f(A)=\rho (A^{-1}), \displaystyle\lim_{\Delta A \to 0} \rho(A+\Delta A)=\rho(A). \rho(A)=\displaystyle\lim_{\Delta A \to 0} \rho(A+\Delta A)=\displaystyle\lim_{\Delta A \to 0} \lim_{k\to \infty}||(A+\Delta A)^k||^{1/k}, f(A)=\rho(A) ","['linear-algebra', 'matrices']"
16,Is there a generalization of Pfaffians?,Is there a generalization of Pfaffians?,,"For an skew-symmetric matrix $A$ (meaning $A^T=-A$ ), the Pfaffian is defined by the equation $(\text{Pf}\,A)^2=\det A$ . It is my understanding that this is defined for anti-symmetric matrices because it is known that the determinant of an anti-symmetric matrix is always a square of a polynomial in the entries of the matrix. Now, skew-symmetry is sufficient to prove that the determinant is a square of a polynomial, but it is not necessary. The simplest example is the $2n\times 2n$ matrix $A=a I_{2n}$ with $a\in\mathbb{C}$ and $I_k$ the $k\times k$ identity matrix. The determinant is $\det A = a^{2n} = (a^n)^2$ . Of course, for $a\neq 0$ , $A$ is not skew-symmetric. I have a few questions about this. Is there a generalization of a Pfaffian for any matrix whose determinant is a square of a polynomial? Is there a characterization (or some known set of properties) of matrices  whose determinants are squares of polynomials? (Edit) Are there any known necessary and sufficient conditions for a matrix to have its determinant be the square of a polynomial (aside from skew-symmetry being sufficient)? (Edit 2) For those who are curious, these questions arise from a problem from physics I am working on. I have a certain class of matrices  whose characteristic polynomials (which arise as the determinant of a non-skew-symmetric matrix) appear to be the squares of Chebyshev polynomials. If I could prove that these characteristic polynomials must be squares of polynomials (using properties of the matrix) then I may be able to use some of the properties attributed to Pfaffians (or the proper generalization to non-skew-symmetric matrices) to confirm that they are indeed squared Chebyshev polynomials. (Edit 3) To be as concrete as possible, I am looking for any information (e.g., answers to questions 1-3) on the set $$\{A\in\mathcal{M}_n(\mathbb{C}): \det A = p(\{a_{ij}\})^2\text{ with }p\text{ a polynomial} \}$$ where $\mathcal{M}_n(\mathbb{C})$ is the set of $n\times n$ complex matrices and $a_{ij}$ is the $i,j$ 'th entry of $A$ .","For an skew-symmetric matrix (meaning ), the Pfaffian is defined by the equation . It is my understanding that this is defined for anti-symmetric matrices because it is known that the determinant of an anti-symmetric matrix is always a square of a polynomial in the entries of the matrix. Now, skew-symmetry is sufficient to prove that the determinant is a square of a polynomial, but it is not necessary. The simplest example is the matrix with and the identity matrix. The determinant is . Of course, for , is not skew-symmetric. I have a few questions about this. Is there a generalization of a Pfaffian for any matrix whose determinant is a square of a polynomial? Is there a characterization (or some known set of properties) of matrices  whose determinants are squares of polynomials? (Edit) Are there any known necessary and sufficient conditions for a matrix to have its determinant be the square of a polynomial (aside from skew-symmetry being sufficient)? (Edit 2) For those who are curious, these questions arise from a problem from physics I am working on. I have a certain class of matrices  whose characteristic polynomials (which arise as the determinant of a non-skew-symmetric matrix) appear to be the squares of Chebyshev polynomials. If I could prove that these characteristic polynomials must be squares of polynomials (using properties of the matrix) then I may be able to use some of the properties attributed to Pfaffians (or the proper generalization to non-skew-symmetric matrices) to confirm that they are indeed squared Chebyshev polynomials. (Edit 3) To be as concrete as possible, I am looking for any information (e.g., answers to questions 1-3) on the set where is the set of complex matrices and is the 'th entry of .","A A^T=-A (\text{Pf}\,A)^2=\det A 2n\times 2n A=a I_{2n} a\in\mathbb{C} I_k k\times k \det A = a^{2n} = (a^n)^2 a\neq 0 A \{A\in\mathcal{M}_n(\mathbb{C}): \det A = p(\{a_{ij}\})^2\text{ with }p\text{ a polynomial} \} \mathcal{M}_n(\mathbb{C}) n\times n a_{ij} i,j A","['linear-algebra', 'determinant', 'pfaffian']"
17,"Testing if two finite sets of points differ only by rotation (unordered, in polynomial time in size and dimension)?","Testing if two finite sets of points differ only by rotation (unordered, in polynomial time in size and dimension)?",,"Imagine we have two size $m$ sets (without order) of points $X=\{x^i\}_{i=1..m}, Y=\{y^i\}_{i=1..m} \subset \mathbb{R}^n$ and we want to answer the question if they differ only by rotation: if there exists othogonal $O^TO=OO^T=I$ such that $X=\{Oy^i:i=1..m\}$ . It simple to test in exponential time, the question is if it be answered in polynomial time (in both $m$ and $n$ )? While it might seem simple, it isn't - the most difficult cases of graph isomorphism problem: for strongly regular graphs (SRG) can be translated to above problem. We can assume that all points are on a sphere , for SRGs they form very regular high dimensional polyhedron (diagram below) having $m\approx 2n$ points. Testing differing only by rotation is simple for symmetric matrices - they differ only by rotation if traces of all $n$ powers are equal ( $\exists_O P=O^TQO \Leftrightarrow \forall_k \textrm{Tr}(P^k)=\textrm{Tr}(Q^k)$ ). Maybe we could use it for sets of points - by translating them into matrix e.g. $n\times n$ : $$P_{ab} = \sum_{i=1..m} x^i_a x^i_b\qquad Q_{ab} = \sum_{i=1..m} y^i_a y^i_b \qquad \forall_{k=1..n}\textrm{Tr}(P^k)=^?\textrm{Tr}(Q^k).$$ Above formula looks like eigendecomposition $(P=\sum_i \lambda_i v^i v^{iT})$ if vectors are orthogonal and $m\leq n$ , but generally it does not determine the used sets of points: for SRGs we have $m\approx 2n$ and above matrices turn out proportional to identity matrix like for POVM s. One question is: for how many points above test works? (assume all have the same norm) It trivially works for $m=1$ point, for two they can be opposite - zeroing the matrix, but it works otherwise (determining single angle), hence we need some independence assumption (?). Generally, for $m$ vectors spanning $n'$ dimensional subspace $(m\leq n'\leq n)$ , they contain $m(n'-1)-n'(n'-1)/2=(m-n'/2)(n'-1)$ degrees of freedom modulo rotation, while above test uses $n$ degrees of freedom, from which all but $n'$ are trivial (zero eigenvalue). Hence we need $(m-n'/2)(n'-1)\leq n'$ condition. Is it sufficient? We can also analogously use tests with higher order matrix (on tensor product) to increase the number of independent invariants, e.g. (corresponding to ladder-like graphs) $n^2 \times n^2$ : $$P_{ab,cd}=\sum_{ij} x^i_a x^i_c (x^i\cdot x^j) x^j_b x^j_d \qquad Q_{ab,cd}=\sum_{ij} y^i_a y^i_c (y^i\cdot y^j) y^j_b y^j_d$$ and testing if $\forall_{k=1..n^2 }\textrm{Tr}(P^k)=\textrm{Tr}(Q^k)$ . I have recently checked it for SRGs and it worked: distinguished all I have tested (up to 29 vertices). The question is how to prove it: what maximal number of points (assuming some independence) it always distinguishes? How this number changes with order of such method? Some example of third order: $P_{abc,def}=\sum_{ijk} x^i_a x^i_d x^j_b x^j_e x^k_c x^k_f (x^i\cdot x^j) (x^i\cdot x^k)(x^j\cdot x^k)$ . Any other way to test it in polynomial time? Example of two non-isomorphic $m=16$ vertex SRGs of the same parameters we would like to distinguish (for any vertex permutation). Eigenspectrums of their adjacency matrices are highly degenerated - taking orthonomal basis of $n=6$ dimensional eigenspace, treating its columns(!) as vectors, they form very regular polyhedron: scalar products recreate neighborhood relation: has one value for all neighbors, second for all non-neighbors, like in schematic diagram on the right: Update (2018-09-21): interesting comment from https://redd.it/9hdwnl : any graph isomorphism problem can be translated to above by putting points in basis $\{e_i\}$ and $\{e_i + e_j \textrm{ for each edge }(i,j)\}$ . This construction restricts rotations to permutations. Hence, solving the above problem for $m\sim n^2$ would also solve graph isomorphism problem. For strongly regular graphs it suffice to do it for $m\sim 2n$ . Update (2018-09-22): there was pointed similarity to orthogonal Procrustes problem , which finds optimal rotation knowing which point should rotate to which. Here we don't know it - just have sets without order: there are $m!$ possibilities.","Imagine we have two size sets (without order) of points and we want to answer the question if they differ only by rotation: if there exists othogonal such that . It simple to test in exponential time, the question is if it be answered in polynomial time (in both and )? While it might seem simple, it isn't - the most difficult cases of graph isomorphism problem: for strongly regular graphs (SRG) can be translated to above problem. We can assume that all points are on a sphere , for SRGs they form very regular high dimensional polyhedron (diagram below) having points. Testing differing only by rotation is simple for symmetric matrices - they differ only by rotation if traces of all powers are equal ( ). Maybe we could use it for sets of points - by translating them into matrix e.g. : Above formula looks like eigendecomposition if vectors are orthogonal and , but generally it does not determine the used sets of points: for SRGs we have and above matrices turn out proportional to identity matrix like for POVM s. One question is: for how many points above test works? (assume all have the same norm) It trivially works for point, for two they can be opposite - zeroing the matrix, but it works otherwise (determining single angle), hence we need some independence assumption (?). Generally, for vectors spanning dimensional subspace , they contain degrees of freedom modulo rotation, while above test uses degrees of freedom, from which all but are trivial (zero eigenvalue). Hence we need condition. Is it sufficient? We can also analogously use tests with higher order matrix (on tensor product) to increase the number of independent invariants, e.g. (corresponding to ladder-like graphs) : and testing if . I have recently checked it for SRGs and it worked: distinguished all I have tested (up to 29 vertices). The question is how to prove it: what maximal number of points (assuming some independence) it always distinguishes? How this number changes with order of such method? Some example of third order: . Any other way to test it in polynomial time? Example of two non-isomorphic vertex SRGs of the same parameters we would like to distinguish (for any vertex permutation). Eigenspectrums of their adjacency matrices are highly degenerated - taking orthonomal basis of dimensional eigenspace, treating its columns(!) as vectors, they form very regular polyhedron: scalar products recreate neighborhood relation: has one value for all neighbors, second for all non-neighbors, like in schematic diagram on the right: Update (2018-09-21): interesting comment from https://redd.it/9hdwnl : any graph isomorphism problem can be translated to above by putting points in basis and . This construction restricts rotations to permutations. Hence, solving the above problem for would also solve graph isomorphism problem. For strongly regular graphs it suffice to do it for . Update (2018-09-22): there was pointed similarity to orthogonal Procrustes problem , which finds optimal rotation knowing which point should rotate to which. Here we don't know it - just have sets without order: there are possibilities.","m X=\{x^i\}_{i=1..m}, Y=\{y^i\}_{i=1..m} \subset \mathbb{R}^n O^TO=OO^T=I X=\{Oy^i:i=1..m\} m n m\approx 2n n \exists_O P=O^TQO \Leftrightarrow \forall_k \textrm{Tr}(P^k)=\textrm{Tr}(Q^k) n\times n P_{ab} = \sum_{i=1..m} x^i_a x^i_b\qquad Q_{ab} = \sum_{i=1..m} y^i_a y^i_b \qquad \forall_{k=1..n}\textrm{Tr}(P^k)=^?\textrm{Tr}(Q^k). (P=\sum_i \lambda_i v^i v^{iT}) m\leq n m\approx 2n m=1 m n' (m\leq n'\leq n) m(n'-1)-n'(n'-1)/2=(m-n'/2)(n'-1) n n' (m-n'/2)(n'-1)\leq n' n^2 \times n^2 P_{ab,cd}=\sum_{ij} x^i_a x^i_c (x^i\cdot x^j) x^j_b x^j_d \qquad Q_{ab,cd}=\sum_{ij} y^i_a y^i_c (y^i\cdot y^j) y^j_b y^j_d \forall_{k=1..n^2 }\textrm{Tr}(P^k)=\textrm{Tr}(Q^k) P_{abc,def}=\sum_{ijk} x^i_a x^i_d x^j_b x^j_e x^k_c x^k_f (x^i\cdot x^j) (x^i\cdot x^k)(x^j\cdot x^k) m=16 n=6 \{e_i\} \{e_i + e_j \textrm{ for each edge }(i,j)\} m\sim n^2 m\sim 2n m!","['linear-algebra', 'geometry', 'algebraic-geometry', 'graph-isomorphism', 'invariance']"
18,Squaring matrices and positive semidefiniteness,Squaring matrices and positive semidefiniteness,,"Suppose that $A$ and $B$ are positive definite, symmetric, $n\times n$ real matrices such that $A-B$ is positive semidefinite. Write $A\succeq B$. When is it true that $A^2\succeq B^2$? I know given the conditions, $A^2\succeq B^2$ is not true in general. On the other hand, let $u_1\geq\ldots\geq u_n\geq 0$ and $v_1\geq\ \ldots \geq v_n\geq 0$ be the eigenvalues of $A$ and $B$. By the Min-Max Principle , we have $$ u_i\geq v_i,\quad i=1,\ldots,n. $$ Then, $u_1^2\geq\ldots\geq u_n^2$ and $v_1^2\geq\ldots\geq v_n^2$ are the eigenvalues of $A^2$ and $B^2$ and satisfy $$ u_i^2\geq v_i^2,\quad i=1,\ldots,n.\tag{$*$} $$ I used to think that ($*$) would be enough to give $A^2\succeq B^2$ but as the counter-example linked above shows, it clearly doesn't. So I'm wondering how we can strengthen ($*$) to give $A^2\succeq B^2$.","Suppose that $A$ and $B$ are positive definite, symmetric, $n\times n$ real matrices such that $A-B$ is positive semidefinite. Write $A\succeq B$. When is it true that $A^2\succeq B^2$? I know given the conditions, $A^2\succeq B^2$ is not true in general. On the other hand, let $u_1\geq\ldots\geq u_n\geq 0$ and $v_1\geq\ \ldots \geq v_n\geq 0$ be the eigenvalues of $A$ and $B$. By the Min-Max Principle , we have $$ u_i\geq v_i,\quad i=1,\ldots,n. $$ Then, $u_1^2\geq\ldots\geq u_n^2$ and $v_1^2\geq\ldots\geq v_n^2$ are the eigenvalues of $A^2$ and $B^2$ and satisfy $$ u_i^2\geq v_i^2,\quad i=1,\ldots,n.\tag{$*$} $$ I used to think that ($*$) would be enough to give $A^2\succeq B^2$ but as the counter-example linked above shows, it clearly doesn't. So I'm wondering how we can strengthen ($*$) to give $A^2\succeq B^2$.",,"['linear-algebra', 'matrices', 'inequality', 'soft-question', 'examples-counterexamples']"
19,Representation of differentiation operator,Representation of differentiation operator,,"So I have to find a basis for a polynomial space such that differentiation operator is in Jordan form. I noticed that if I chose a basis to be ${1,x^2,x^3,x^4,...,x^n}$ then the differentiation operator is  $$\begin{matrix} 0&1&0&0&\dots&0 \\0&0&2&0&\dots&0 \\0&0&0&3&\dots&0 \\\vdots&&&&\ddots \\0&0&0&0&\dots &n \\0&0&0&0&\dots&0 \end{matrix}$$ This is Jordanish but not Jordan... I wanted to try with a basis where instead of vector $x^n$ I have $\frac{1}{n} x^n$  but of course it does not work. What is the trick here? Update: That was quick but I think I found it $\{n!,n!x,\frac{n!}{2!}x^2,\dots,\frac{n!}{(n-1)!}x^{n-1},n!x^n\}$ does the job.","So I have to find a basis for a polynomial space such that differentiation operator is in Jordan form. I noticed that if I chose a basis to be ${1,x^2,x^3,x^4,...,x^n}$ then the differentiation operator is  $$\begin{matrix} 0&1&0&0&\dots&0 \\0&0&2&0&\dots&0 \\0&0&0&3&\dots&0 \\\vdots&&&&\ddots \\0&0&0&0&\dots &n \\0&0&0&0&\dots&0 \end{matrix}$$ This is Jordanish but not Jordan... I wanted to try with a basis where instead of vector $x^n$ I have $\frac{1}{n} x^n$  but of course it does not work. What is the trick here? Update: That was quick but I think I found it $\{n!,n!x,\frac{n!}{2!}x^2,\dots,\frac{n!}{(n-1)!}x^{n-1},n!x^n\}$ does the job.",,"['linear-algebra', 'linear-transformations']"
20,Prescribing the dimension of intersections of sub-vector spaces,Prescribing the dimension of intersections of sub-vector spaces,,"Let $n$ be a positive integer. For each subset $S$ of $\{1,\dots,n\}$ let $d_S$ be a nonnegative integer. Assume that the $(d_S)$ satisfy: $$ S\subset T\implies d_S\ge d_T, $$  $$ d_{S\cap T}\ge d_S+d_T-d_{S\cup T} $$  for all $S,T\subset\{1,\dots,n\}$. [In this post $X\subset Y$ denotes what some people designate by $X\subseteq Y$.] Is there a tuple $(V,V_1,\dots,V_n)$, such that $V$ is a finite dimensional $\mathbb Q$-vector space $V$ and the $V_i$ are sub-vector spaces of $V$, satisfying  $$ \dim\left(\bigcap_{s\in S}V_s\right)=d_S $$  for all $S\subset\{1,\dots,n\}\ ?$ EDIT 1. The conditions  $$ S\subset T\implies d_S\ge d_T, $$  $$ d_{S\cap T}\ge d_S+d_T-d_{S\cup T} $$  for all $S,T\subset\{1,\dots,n\}$ stated above are necessary. The first one follows from the fact that the dimension of a subspace can't exceed the dimension of the ambient space, the second one follows from the first one coupled to the equality $(\star)\quad\dim(U+W)=\dim U+\dim W-\dim(U\cap W)$ for finite dimensional subspaces $U$ and $W$. There is an similar question for finite sets, with cardinalities instead of dimensions; that is, one can ask which families of nonnegative integers indexed by subsets of $\{1,\dots,n\}$ come (in the obvious sense) from a tuple $(V,V_1,\dots,V_n)$ of finite sets . In this setting, we still have the trivial obstruction that the cardinality of a subset can't exceed the cardinality of the ambient set, but there are other obstructions, given by the inclusion-exclusion principle for $k$ finite subsets, for all $k$ varying from $2$ to $n$. In the case of finite dimensional vector spaces, however, the inclusion-exclusion principle holds for $k=2$ (see $(\star)$), but not for $k\ge3$: see Sebastien Palcoux's posts in this thread (see also the references therein). It seems that (for dimensional vector spaces) no obstructions other that the trivial one and the one coming from $(\star)$ have been found so far. This is perhaps because they don't exist, but, for the time being, it seems that the existence of subtler obstructions can't be ruled out. The ground field has been chosen to be $\mathbb Q$, but this was only for the sake of concreteness. I'd be very interested in any result involving another field. EDIT 2. In fact, if the ground field is finite, there are obvious obstructions coming from the fact that the cardinality of a union of subspaces can't exceed the cardinality the ambient space. To avoid this kind of complications, let's assume that ground field is infinite . EDIT 3. Here is a proof of the existence of $V$ and the $V_i$ in the case $n=3$ (assuming that the ground field is infinite). Let $K$ be the ground field, which is assumed to be infinite . For each set $Z$ let $|Z|$ be the cardinality of $Z$, and let $KZ$ be the free vector space over $Z$. If there are sets $Z,Z_1,Z_2,Z_3$ such that  $$ \left|\bigcap_{i\in S}Z_i\right|=d_S $$  for all $S\subset\{1,2,3\}$, we solve our problem by applying the functor $X\mapsto KX$. So we can assume from now on that no such sets exist. There are still sets $Z_1,Z_2,Z_3$ such that  $$ \left|\bigcap_{i\in S}Z_i\right|=d_S $$  for all nonempty subset $S$ of $\{1,2,3\}$. We define $Z$ by $Z:=Z_1\cup Z_2\cup Z_3$. Setting $r:=|Z|-d_\varnothing$, we get  $$ 1\le r\le|Z\setminus(Z_i\cup Z_j)| $$  for all $i,j$. As $K$ is infinite, there is an $r$ dimensional subspace $W$ in $KZ$ such that $W\cap(K\,(Z_i\cup Z_j))=0$ for all $i,j$. Let $\pi:KZ\to(KZ)/W$ be the canonical projection. Then it suffices to set  $$ V:=(KZ)/W,\quad V_i:=\pi(KZ_i). $$","Let $n$ be a positive integer. For each subset $S$ of $\{1,\dots,n\}$ let $d_S$ be a nonnegative integer. Assume that the $(d_S)$ satisfy: $$ S\subset T\implies d_S\ge d_T, $$  $$ d_{S\cap T}\ge d_S+d_T-d_{S\cup T} $$  for all $S,T\subset\{1,\dots,n\}$. [In this post $X\subset Y$ denotes what some people designate by $X\subseteq Y$.] Is there a tuple $(V,V_1,\dots,V_n)$, such that $V$ is a finite dimensional $\mathbb Q$-vector space $V$ and the $V_i$ are sub-vector spaces of $V$, satisfying  $$ \dim\left(\bigcap_{s\in S}V_s\right)=d_S $$  for all $S\subset\{1,\dots,n\}\ ?$ EDIT 1. The conditions  $$ S\subset T\implies d_S\ge d_T, $$  $$ d_{S\cap T}\ge d_S+d_T-d_{S\cup T} $$  for all $S,T\subset\{1,\dots,n\}$ stated above are necessary. The first one follows from the fact that the dimension of a subspace can't exceed the dimension of the ambient space, the second one follows from the first one coupled to the equality $(\star)\quad\dim(U+W)=\dim U+\dim W-\dim(U\cap W)$ for finite dimensional subspaces $U$ and $W$. There is an similar question for finite sets, with cardinalities instead of dimensions; that is, one can ask which families of nonnegative integers indexed by subsets of $\{1,\dots,n\}$ come (in the obvious sense) from a tuple $(V,V_1,\dots,V_n)$ of finite sets . In this setting, we still have the trivial obstruction that the cardinality of a subset can't exceed the cardinality of the ambient set, but there are other obstructions, given by the inclusion-exclusion principle for $k$ finite subsets, for all $k$ varying from $2$ to $n$. In the case of finite dimensional vector spaces, however, the inclusion-exclusion principle holds for $k=2$ (see $(\star)$), but not for $k\ge3$: see Sebastien Palcoux's posts in this thread (see also the references therein). It seems that (for dimensional vector spaces) no obstructions other that the trivial one and the one coming from $(\star)$ have been found so far. This is perhaps because they don't exist, but, for the time being, it seems that the existence of subtler obstructions can't be ruled out. The ground field has been chosen to be $\mathbb Q$, but this was only for the sake of concreteness. I'd be very interested in any result involving another field. EDIT 2. In fact, if the ground field is finite, there are obvious obstructions coming from the fact that the cardinality of a union of subspaces can't exceed the cardinality the ambient space. To avoid this kind of complications, let's assume that ground field is infinite . EDIT 3. Here is a proof of the existence of $V$ and the $V_i$ in the case $n=3$ (assuming that the ground field is infinite). Let $K$ be the ground field, which is assumed to be infinite . For each set $Z$ let $|Z|$ be the cardinality of $Z$, and let $KZ$ be the free vector space over $Z$. If there are sets $Z,Z_1,Z_2,Z_3$ such that  $$ \left|\bigcap_{i\in S}Z_i\right|=d_S $$  for all $S\subset\{1,2,3\}$, we solve our problem by applying the functor $X\mapsto KX$. So we can assume from now on that no such sets exist. There are still sets $Z_1,Z_2,Z_3$ such that  $$ \left|\bigcap_{i\in S}Z_i\right|=d_S $$  for all nonempty subset $S$ of $\{1,2,3\}$. We define $Z$ by $Z:=Z_1\cup Z_2\cup Z_3$. Setting $r:=|Z|-d_\varnothing$, we get  $$ 1\le r\le|Z\setminus(Z_i\cup Z_j)| $$  for all $i,j$. As $K$ is infinite, there is an $r$ dimensional subspace $W$ in $KZ$ such that $W\cap(K\,(Z_i\cup Z_j))=0$ for all $i,j$. Let $\pi:KZ\to(KZ)/W$ be the canonical projection. Then it suffices to set  $$ V:=(KZ)/W,\quad V_i:=\pi(KZ_i). $$",,"['linear-algebra', 'abstract-algebra', 'matroids']"
21,computing the inversion of a matrix which is the sum of a Kronecker product and an identity matrix,computing the inversion of a matrix which is the sum of a Kronecker product and an identity matrix,,"I am using Gibbs sampling to compute the posterior of $\mathbf{S}_{N\times K}$ $(N>>K)$ while I should calculate a Gaussian likelihood which its covariance matrix is given as $$\mathbf{P}_{K^2\times K^2}=\Big[\Big(\mathbf{S}\otimes\mathbf{S}\Big)^T\Big(\mathbf{S}\otimes\mathbf{S}\Big)+\gamma\mathbf{I}_{K^2}\Big]^{-1}$$ where $\otimes$ is a Kronecker product. I need to compute $\mathbf{P}$ for each entry update of $s_{nk}$ which is computationally very costly. $\mathbf{S}$ is a binary matrix and has this property $\sum_{i=1}^N \mathbf{s}_i^T\mathbf{s}_i$ where $\mathbf{s}$ is the rank one matrix.  I am wondering whether there is a rank one updates of $\mathbf{S}$ exist in order to compute efficiently the inverse of $\mathbf{P}$ when only one entry of $\mathbf{s}_i$ (the $i$ -th row of $\mathbf{S}$ ) is modified? A working example when there is no Kronecker product , if we have $$\mathbf{M}^{-1}=\mathbf{S}^T\mathbf{S}+\lambda\mathbf{I}_{k},$$ since the first term can be expressed as $\sum_{i}\mathbf{s}_i\mathbf{s}_i$ then calculating $\mathbf{S}_{-i,k}$ can be done by removing the influence of a single vector $\mathbf{s}_i$ is a rank-one update to $\mathbf{M}$ , so we have $$\mathbf{M}_{-i}=(\sum_{j\neq i} \mathbf{s}_j^T\mathbf{s}_j+\lambda\mathbf{I})^{-1},$$ then we can use Sherman–Morrison formula and we get $$\mathbf{M}_{-i}=( \mathbf{M}^{-1}-\mathbf{s}_i^T\mathbf{s}_i)^{-1}=\mathbf{M}-\frac{\mathbf{M}\mathbf{s}_i^T\mathbf{s}_i\mathbf{M}}{\mathbf{s}_i\mathbf{M}\mathbf{s}_i^T-1}$$ and we also have $$\mathbf{M}=\mathbf{M}_{-i}-\frac{\mathbf{M}_{-i}\mathbf{s}_{i}^T\mathbf{s}_{i}\mathbf{M}_{-i}}{\mathbf{s}_{i}^T\mathbf{M}_{-i}\mathbf{s}_{i}+1}$$ which leads to having $M^{-1}=\mathbf{M}_{-i}^{-1}+\mathbf{s}_i^T\mathbf{s}_i$ and $M_{-i}^{-1}=\mathbf{S}_{-i}^T\mathbf{S}_{-i}+\gamma\mathbf{I}$ . By computing the product of two vectors $\mathbf{s}_i^T\mathbf{s}_i$ at the $i$ -th row, one can basically update this matrix inversion. Is there any solution already out there for this problem? Any suggestion for computing inverse of $\mathbf{P}$ ( $\mathbf{P}_{-i}$ ) for just rank one update of $\mathbf{s}$ (deleting the $i$ -th row)?? Update : Is there any recursive method, some idea similar to this article that reduce the computation of matrix inversion here considerably?","I am using Gibbs sampling to compute the posterior of while I should calculate a Gaussian likelihood which its covariance matrix is given as where is a Kronecker product. I need to compute for each entry update of which is computationally very costly. is a binary matrix and has this property where is the rank one matrix.  I am wondering whether there is a rank one updates of exist in order to compute efficiently the inverse of when only one entry of (the -th row of ) is modified? A working example when there is no Kronecker product , if we have since the first term can be expressed as then calculating can be done by removing the influence of a single vector is a rank-one update to , so we have then we can use Sherman–Morrison formula and we get and we also have which leads to having and . By computing the product of two vectors at the -th row, one can basically update this matrix inversion. Is there any solution already out there for this problem? Any suggestion for computing inverse of ( ) for just rank one update of (deleting the -th row)?? Update : Is there any recursive method, some idea similar to this article that reduce the computation of matrix inversion here considerably?","\mathbf{S}_{N\times K} (N>>K) \mathbf{P}_{K^2\times K^2}=\Big[\Big(\mathbf{S}\otimes\mathbf{S}\Big)^T\Big(\mathbf{S}\otimes\mathbf{S}\Big)+\gamma\mathbf{I}_{K^2}\Big]^{-1} \otimes \mathbf{P} s_{nk} \mathbf{S} \sum_{i=1}^N \mathbf{s}_i^T\mathbf{s}_i \mathbf{s} \mathbf{S} \mathbf{P} \mathbf{s}_i i \mathbf{S} \mathbf{M}^{-1}=\mathbf{S}^T\mathbf{S}+\lambda\mathbf{I}_{k}, \sum_{i}\mathbf{s}_i\mathbf{s}_i \mathbf{S}_{-i,k} \mathbf{s}_i \mathbf{M} \mathbf{M}_{-i}=(\sum_{j\neq i} \mathbf{s}_j^T\mathbf{s}_j+\lambda\mathbf{I})^{-1}, \mathbf{M}_{-i}=( \mathbf{M}^{-1}-\mathbf{s}_i^T\mathbf{s}_i)^{-1}=\mathbf{M}-\frac{\mathbf{M}\mathbf{s}_i^T\mathbf{s}_i\mathbf{M}}{\mathbf{s}_i\mathbf{M}\mathbf{s}_i^T-1} \mathbf{M}=\mathbf{M}_{-i}-\frac{\mathbf{M}_{-i}\mathbf{s}_{i}^T\mathbf{s}_{i}\mathbf{M}_{-i}}{\mathbf{s}_{i}^T\mathbf{M}_{-i}\mathbf{s}_{i}+1} M^{-1}=\mathbf{M}_{-i}^{-1}+\mathbf{s}_i^T\mathbf{s}_i M_{-i}^{-1}=\mathbf{S}_{-i}^T\mathbf{S}_{-i}+\gamma\mathbf{I} \mathbf{s}_i^T\mathbf{s}_i i \mathbf{P} \mathbf{P}_{-i} \mathbf{s} i","['linear-algebra', 'inverse', 'covariance', 'sampling', 'kronecker-product']"
22,"Different ""eigenspaces"" of a module automorphism with non-trivial intersection","Different ""eigenspaces"" of a module automorphism with non-trivial intersection",,"I'm messing around in the following setting: let $$\Bbb C' = \{a+bh \mid a,b \in \Bbb R, h \not\in \Bbb R, h^2=1\} \cong \frac{\Bbb R[x]}{(x^2-1)}$$be the ring of split-complex numbers. Define a linear operator $J: \Bbb R^2 \to \Bbb R^2$ by $J(e_1) = e_2$ and $J(e_2) =e_1$, where $(e_1,e_2)$ is the standard basis of $\Bbb R^2$. The split-complexification $\Bbb C' \otimes_{\Bbb R} \Bbb R^2 \cong (\Bbb C')^2$ has a $\Bbb C'$-module structure, and $(e_1,e_2)$ is a $\Bbb C'$-basis, great. We consider the unique $\Bbb C'$-linear extension of $J$ to $(\Bbb C')^2$, also denoted by $J$. The characteristic polynomial of $J$ is $x^2-1$, which has four roots in $\Bbb C'$: $1$, $-1$, $h$ and $-h$. Since $\Bbb C'$ is commutative, each ""eigenspace"" $E_\lambda$ is s submodule of $(\Bbb C')^2$. I have computed $$\begin{array}{|c|c|} \hline {\boldsymbol \lambda} & \textbf{basis} \\  \hline  1 & (1,1) \\ \hline  -1 & (1,-1) \\ \hline h & (1,h) \\ \hline -h & (1,-h) \\ \hline \end{array}$$ Every basis of $(\Bbb C')^2$ over $\Bbb C'$ must have the same cardinality, so clearly $(\Bbb C')^2$ cannot be the direct sum of these $E_\lambda$. Also: $$(1+h)(1,1) + (-1-h)(1,h) = (0,0).$$This is very disturbing to me. I do realize that $1+h$ and $1-h$ are the only zero divisors (up to real scaling) in $\Bbb C'$. If $$E_1 = \{(a+bh,a+bh) \mid a,b \in \Bbb R \}\quad\mbox{and}\quad E_h = \{(a+bh, b+ah) \mid a,b \in \Bbb R\},$$the above says that these ""eigenspaces"" intersect along a ""line"" $a=b$ (corresponding to the zero divisor). What is really going on here? Is there a ""better"" choice of decomposition for $(\Bbb C')^2$ here? Better yet, is there a way to develop ""spectral"" theory in this context of modules? I feel like I'm walking on eggs here. Thanks for any help!","I'm messing around in the following setting: let $$\Bbb C' = \{a+bh \mid a,b \in \Bbb R, h \not\in \Bbb R, h^2=1\} \cong \frac{\Bbb R[x]}{(x^2-1)}$$be the ring of split-complex numbers. Define a linear operator $J: \Bbb R^2 \to \Bbb R^2$ by $J(e_1) = e_2$ and $J(e_2) =e_1$, where $(e_1,e_2)$ is the standard basis of $\Bbb R^2$. The split-complexification $\Bbb C' \otimes_{\Bbb R} \Bbb R^2 \cong (\Bbb C')^2$ has a $\Bbb C'$-module structure, and $(e_1,e_2)$ is a $\Bbb C'$-basis, great. We consider the unique $\Bbb C'$-linear extension of $J$ to $(\Bbb C')^2$, also denoted by $J$. The characteristic polynomial of $J$ is $x^2-1$, which has four roots in $\Bbb C'$: $1$, $-1$, $h$ and $-h$. Since $\Bbb C'$ is commutative, each ""eigenspace"" $E_\lambda$ is s submodule of $(\Bbb C')^2$. I have computed $$\begin{array}{|c|c|} \hline {\boldsymbol \lambda} & \textbf{basis} \\  \hline  1 & (1,1) \\ \hline  -1 & (1,-1) \\ \hline h & (1,h) \\ \hline -h & (1,-h) \\ \hline \end{array}$$ Every basis of $(\Bbb C')^2$ over $\Bbb C'$ must have the same cardinality, so clearly $(\Bbb C')^2$ cannot be the direct sum of these $E_\lambda$. Also: $$(1+h)(1,1) + (-1-h)(1,h) = (0,0).$$This is very disturbing to me. I do realize that $1+h$ and $1-h$ are the only zero divisors (up to real scaling) in $\Bbb C'$. If $$E_1 = \{(a+bh,a+bh) \mid a,b \in \Bbb R \}\quad\mbox{and}\quad E_h = \{(a+bh, b+ah) \mid a,b \in \Bbb R\},$$the above says that these ""eigenspaces"" intersect along a ""line"" $a=b$ (corresponding to the zero divisor). What is really going on here? Is there a ""better"" choice of decomposition for $(\Bbb C')^2$ here? Better yet, is there a way to develop ""spectral"" theory in this context of modules? I feel like I'm walking on eggs here. Thanks for any help!",,"['linear-algebra', 'abstract-algebra', 'eigenvalues-eigenvectors', 'modules', 'free-modules']"
23,The `square root of a tensor'; conditions of existence.,The `square root of a tensor'; conditions of existence.,,"Suppose I have some tensor, for concreteness I will consider a rank 4 tensor with components $R_{abcd}$ where the indices run over 0,1,2,3 (my question arises in a physics context). Under what conditions is the existence of another tensor, call it $G_{abcd}$ , which satisfies $G_{ab}^{~~~mn}G_{cdmn}=R_{abcd}$ guaranteed? Edit : Indices are raised and lowered using the metric tensor : $G_{ab}^{~~~mn}=g^{cm}g^{dn}G_{abcd}$ . Also, for my problem $R_{abcd}$ is the Riemann tensor, and I suppose this would place extra conditions on the existence of a suitable $G_{abcd}$ since it is defined in terms of the derivatives of the metric (in the torsion free case). This probably complicates things quite some, so for now I will leave $R_{abcd}$ to be any rank 4 tensor.","Suppose I have some tensor, for concreteness I will consider a rank 4 tensor with components where the indices run over 0,1,2,3 (my question arises in a physics context). Under what conditions is the existence of another tensor, call it , which satisfies guaranteed? Edit : Indices are raised and lowered using the metric tensor : . Also, for my problem is the Riemann tensor, and I suppose this would place extra conditions on the existence of a suitable since it is defined in terms of the derivatives of the metric (in the torsion free case). This probably complicates things quite some, so for now I will leave to be any rank 4 tensor.",R_{abcd} G_{abcd} G_{ab}^{~~~mn}G_{cdmn}=R_{abcd} G_{ab}^{~~~mn}=g^{cm}g^{dn}G_{abcd} R_{abcd} G_{abcd} R_{abcd},['linear-algebra']
24,$\exists\alpha\in\mathbf{F}(|\alpha-\lambda|<\frac{1}{1000}\ \text{and}\ T-\alpha I \ \text{is}\ \text{invertible})$,,\exists\alpha\in\mathbf{F}(|\alpha-\lambda|<\frac{1}{1000}\ \text{and}\ T-\alpha I \ \text{is}\ \text{invertible}),"I was asked to prove the following Theorem. Theorem. Given that $V$ is finite dimensional and $T\in\mathcal{L}(V)$ and $\lambda\in\mathbf{F}$. There exists a   $\alpha\in\mathbf{F}$ such that $|\alpha-\lambda|<\frac{1}{1000}$ and   $T-\alpha I$ is invertible. The Following is my attempt any extra results that i make use of have been quoted below the proof. Is my proof correct? Proof. Assume on the contrary that given any $\alpha\in\mathbf{F}$, $|\alpha-\lambda|<\frac{1}{1000}$ implies that $T-\alpha I$ is not invertible or equivalently $\alpha$ is an eigenvalue of $T$. Let $n = \dim V$ and $I_{n+1} = \{1,2,3,...,n,n+1\}$. Consider now the function $\mathcal{V}:I_{n+1}\to \mathbf{F}$ defined by $$\mathcal{V}(x) = \lambda+\frac{1}{10^{3+x}}$$ with the above definition it is not difficult to see that $\forall x\in I_{n+1}\left(|\mathcal{V}(x)-\lambda|<\frac{1}{1000}\right)$. Consequently we have a list of $n+1$ distinct eigenvalues namely $\mathcal{V}(1),\mathcal{V}(2),...,\mathcal{V}(n),\mathcal{V}(n+1)$ taking this together with theorem $5.10$ implies that the corresponding list of eigenvectors namely $v_1,v_2,...,v_n,v_{n+1}$ is linearly independent. However we know that any list of linearly independent vectors in $V$ must have length $m\leq n = \dim V$ resulting in a contradiction. $\blacksquare$ $5.10$ If $T\in\mathcal{L}(V)$ and $\lambda_1,\lambda_2,...,\lambda_n$ is a list of distinct eigenvalues   of $T$ then the corresponding list of vectors $v_1,v_2,...,v_n$ is   linearly independent.","I was asked to prove the following Theorem. Theorem. Given that $V$ is finite dimensional and $T\in\mathcal{L}(V)$ and $\lambda\in\mathbf{F}$. There exists a   $\alpha\in\mathbf{F}$ such that $|\alpha-\lambda|<\frac{1}{1000}$ and   $T-\alpha I$ is invertible. The Following is my attempt any extra results that i make use of have been quoted below the proof. Is my proof correct? Proof. Assume on the contrary that given any $\alpha\in\mathbf{F}$, $|\alpha-\lambda|<\frac{1}{1000}$ implies that $T-\alpha I$ is not invertible or equivalently $\alpha$ is an eigenvalue of $T$. Let $n = \dim V$ and $I_{n+1} = \{1,2,3,...,n,n+1\}$. Consider now the function $\mathcal{V}:I_{n+1}\to \mathbf{F}$ defined by $$\mathcal{V}(x) = \lambda+\frac{1}{10^{3+x}}$$ with the above definition it is not difficult to see that $\forall x\in I_{n+1}\left(|\mathcal{V}(x)-\lambda|<\frac{1}{1000}\right)$. Consequently we have a list of $n+1$ distinct eigenvalues namely $\mathcal{V}(1),\mathcal{V}(2),...,\mathcal{V}(n),\mathcal{V}(n+1)$ taking this together with theorem $5.10$ implies that the corresponding list of eigenvectors namely $v_1,v_2,...,v_n,v_{n+1}$ is linearly independent. However we know that any list of linearly independent vectors in $V$ must have length $m\leq n = \dim V$ resulting in a contradiction. $\blacksquare$ $5.10$ If $T\in\mathcal{L}(V)$ and $\lambda_1,\lambda_2,...,\lambda_n$ is a list of distinct eigenvalues   of $T$ then the corresponding list of vectors $v_1,v_2,...,v_n$ is   linearly independent.",,"['linear-algebra', 'proof-verification', 'eigenvalues-eigenvectors', 'linear-transformations']"
25,"Given a set of polynomials that correspond to weak compositions of an integer, are all possible products of d such polynomials linearly independent?","Given a set of polynomials that correspond to weak compositions of an integer, are all possible products of d such polynomials linearly independent?",,"Fix two positive integers $k$ and $N$. Let $a = (a_{1},a_{2},\dots,a_{N})$ be a weak composition of $k$ into $N$ parts. In other words, $a$ is an $N$ dimensional vector of nonnegative integers such that $\sum_{\ell=1}^N a_{\ell} = k$. For any weak composition $a$, define $$P_{a}(\{X_{i,\ell}\}) = \prod_{i=1}^k\sum_{\ell=1}^N a_{\ell}X_{i,\ell},$$ which is a polynomial with integer coefficients over the set of formal variables $\{X_{i,\ell}\}_{i \in [k],\ell \in [N]}$. There are $\binom{N+k-1}{k}$ different polynomials $P_a(\{X_{i,\ell}\})$, one for every possible weak composition $a$. Let $d \leq k$ be some fixed positive integer. Consider all distinct products of exactly $d$ of these polynomials, allowing for repetition. More precisely, for any size $d$ multiset $\{a^{(1)},a^{(2)},\dots,a^{(d)}\}$ where each $a^{(j)}$ is a weak composition, the corresponding product is  $$Q_{a^{(1)},a^{(2)},\dots,a^{(d)}}(\{X_{i,\ell}\}) = P_{a^{(1)}}(\{X_{i,\ell}\})P_{a^{(2)}}(\{X_{i,\ell}\})\cdots P_{a^{(d)}}(\{X_{i,\ell}\}).$$ Note that there are $$\binom{\binom{N+k-1}{k}+d-1}{d}$$ possible polynomials $Q_{a^{(1)},a^{(2)},\dots,a^{(d)}}(\{X_{i,\ell}\})$, corresponding to all possible size $d$ multisets over the $\binom{N+k-1}{k}$ weak compositions. My question is: When $d \leq k$, are all the $Q_{a^{(1)},a^{(2)},\dots,a^{(d)}}(\{X_{i,\ell}\})$ polynomials linearly independent? There are no other restrictions on $N,d,k$ except that they are all positive integers. Any help would be greatly appreciated! Some notes: The motivation is to justify Assumption 1 in https://eprint.iacr.org/2017/946.pdf , a paper which attempts to build secure cryptographic multilinear maps. In that paper, the $X_{i,\ell}$'s actually correspond to matrices (with dimensions chosen so that the above products still evaluate to a scalar). A proof of the above would show that there are no degree $d\leq k$ polynomials that ""annihilate"" the $P_a$ polynomials over the $\{X_{i,\ell}\}$ formal variables. If we attempt to count the number of possible monomials that can have distinct coefficients, I believe we get $\binom{\binom{N+d-1}{d}+k-1}{k}$. This should give an upper bound on the number of linearly independent $Q$ polynomials. When $d > k$, the number of polynomials exceeds this value, which is why I think $d \leq k$ is required. I believe the $d=1$ case is implied by this result: http://epubs.siam.org/doi/pdf/10.1137/S0895479800369141 (they show that when $X_{i,\ell}$ is replaced by $X_{\ell}$, that the resulting $Q$ polynomials are linearly independent). Edit: I've checked all small cases in Mathematica that my computer can handle and this claim seems to hold true. For example, when $k=3,N=3,d=3$ this produces 220 linearly independent polynomials, and when $k=4,N=3,d=3$ it produces 680 linearly independent polynomials.","Fix two positive integers $k$ and $N$. Let $a = (a_{1},a_{2},\dots,a_{N})$ be a weak composition of $k$ into $N$ parts. In other words, $a$ is an $N$ dimensional vector of nonnegative integers such that $\sum_{\ell=1}^N a_{\ell} = k$. For any weak composition $a$, define $$P_{a}(\{X_{i,\ell}\}) = \prod_{i=1}^k\sum_{\ell=1}^N a_{\ell}X_{i,\ell},$$ which is a polynomial with integer coefficients over the set of formal variables $\{X_{i,\ell}\}_{i \in [k],\ell \in [N]}$. There are $\binom{N+k-1}{k}$ different polynomials $P_a(\{X_{i,\ell}\})$, one for every possible weak composition $a$. Let $d \leq k$ be some fixed positive integer. Consider all distinct products of exactly $d$ of these polynomials, allowing for repetition. More precisely, for any size $d$ multiset $\{a^{(1)},a^{(2)},\dots,a^{(d)}\}$ where each $a^{(j)}$ is a weak composition, the corresponding product is  $$Q_{a^{(1)},a^{(2)},\dots,a^{(d)}}(\{X_{i,\ell}\}) = P_{a^{(1)}}(\{X_{i,\ell}\})P_{a^{(2)}}(\{X_{i,\ell}\})\cdots P_{a^{(d)}}(\{X_{i,\ell}\}).$$ Note that there are $$\binom{\binom{N+k-1}{k}+d-1}{d}$$ possible polynomials $Q_{a^{(1)},a^{(2)},\dots,a^{(d)}}(\{X_{i,\ell}\})$, corresponding to all possible size $d$ multisets over the $\binom{N+k-1}{k}$ weak compositions. My question is: When $d \leq k$, are all the $Q_{a^{(1)},a^{(2)},\dots,a^{(d)}}(\{X_{i,\ell}\})$ polynomials linearly independent? There are no other restrictions on $N,d,k$ except that they are all positive integers. Any help would be greatly appreciated! Some notes: The motivation is to justify Assumption 1 in https://eprint.iacr.org/2017/946.pdf , a paper which attempts to build secure cryptographic multilinear maps. In that paper, the $X_{i,\ell}$'s actually correspond to matrices (with dimensions chosen so that the above products still evaluate to a scalar). A proof of the above would show that there are no degree $d\leq k$ polynomials that ""annihilate"" the $P_a$ polynomials over the $\{X_{i,\ell}\}$ formal variables. If we attempt to count the number of possible monomials that can have distinct coefficients, I believe we get $\binom{\binom{N+d-1}{d}+k-1}{k}$. This should give an upper bound on the number of linearly independent $Q$ polynomials. When $d > k$, the number of polynomials exceeds this value, which is why I think $d \leq k$ is required. I believe the $d=1$ case is implied by this result: http://epubs.siam.org/doi/pdf/10.1137/S0895479800369141 (they show that when $X_{i,\ell}$ is replaced by $X_{\ell}$, that the resulting $Q$ polynomials are linearly independent). Edit: I've checked all small cases in Mathematica that my computer can handle and this claim seems to hold true. For example, when $k=3,N=3,d=3$ this produces 220 linearly independent polynomials, and when $k=4,N=3,d=3$ it produces 680 linearly independent polynomials.",,"['linear-algebra', 'combinatorics']"
26,Group cohomology of $\mathrm{GL}(V)$,Group cohomology of,\mathrm{GL}(V),"Let $K$ be a field not isomorphic to $\mathbb F_2$, $V = K^n$ - vector space over $K$ on which $\mathrm{GL}(V)$ acts. How to compute cohomology groups $H^i(\mathrm{GL}(V),V)$? It is easy to see that $H^0(\mathrm{GL}(V),V) = 0$ but what about other cohomology groups?","Let $K$ be a field not isomorphic to $\mathbb F_2$, $V = K^n$ - vector space over $K$ on which $\mathrm{GL}(V)$ acts. How to compute cohomology groups $H^i(\mathrm{GL}(V),V)$? It is easy to see that $H^0(\mathrm{GL}(V),V) = 0$ but what about other cohomology groups?",,"['linear-algebra', 'homological-algebra', 'group-cohomology']"
27,$M$'s entries are known up to $(1\pm \varepsilon)$. How well do we know $M$'s determinant?,'s entries are known up to . How well do we know 's determinant?,M (1\pm \varepsilon) M,"$M$ is a $n\times n$ positive definite matrix and we would like to compute its determinant. We observe another positive definite matrix $\hat{M}$ whose components are within some factor of those of $M$. In particular $\hat{M}_{ij}=c_{ij}\cdot M_{ij}$ and each $c_{ij}\in (1-\varepsilon, 1+\varepsilon).$ How is $|\hat{M}|$ related to $|M|$? In particular how small does $\varepsilon$ have to be in terms of $M$'s entries for strong bounds to exist? When each $c_{ij}=c_{i}$ for each $i$, it's immediate that $|\hat{M}|=\prod_ic_i\cdot |M|.$ There must be some correspondence, since the determinant is just a polynomial of a matrix's entries so it's continuous.","$M$ is a $n\times n$ positive definite matrix and we would like to compute its determinant. We observe another positive definite matrix $\hat{M}$ whose components are within some factor of those of $M$. In particular $\hat{M}_{ij}=c_{ij}\cdot M_{ij}$ and each $c_{ij}\in (1-\varepsilon, 1+\varepsilon).$ How is $|\hat{M}|$ related to $|M|$? In particular how small does $\varepsilon$ have to be in terms of $M$'s entries for strong bounds to exist? When each $c_{ij}=c_{i}$ for each $i$, it's immediate that $|\hat{M}|=\prod_ic_i\cdot |M|.$ There must be some correspondence, since the determinant is just a polynomial of a matrix's entries so it's continuous.",,"['linear-algebra', 'numerical-methods', 'determinant']"
28,Why does the inverse of the Hilbert matrix have integer entries?,Why does the inverse of the Hilbert matrix have integer entries?,,Let $A$ be the $n\times n$ matrix given by $$A_{ij}=\frac{1}{i + j - 1}$$ Show that $A$ is invertible and that the inverse has integer entries. I was able to show that $A$ is invertible. How do I show that $A^{-1}$ has integer entries? This matrix is called the Hilbert matrix. The problem appears as exercise 12 in section 1.6 of Hoffman and Kunze's Linear Algebra (2nd edition).,Let be the matrix given by Show that is invertible and that the inverse has integer entries. I was able to show that is invertible. How do I show that has integer entries? This matrix is called the Hilbert matrix. The problem appears as exercise 12 in section 1.6 of Hoffman and Kunze's Linear Algebra (2nd edition).,A n\times n A_{ij}=\frac{1}{i + j - 1} A A A^{-1},"['linear-algebra', 'matrices', 'inverse', 'hilbert-matrices']"
29,"Nielsen & Chuang, Problem 2.2 — Properties of the Schmidt number","Nielsen & Chuang, Problem 2.2 — Properties of the Schmidt number",,"I started reading the Nielsen & Chuang's Quantum Computation and Quantum Information . I got stuck by the last question of Problem 2.2. I got the other problems, but I can't see this one. I guess it's not really difficult, but as I am new in this field, some help will be nice. :) Problem statement: Suppose $|\psi\rangle $ is a pure state of a composite system with components $A$ and $B$ , such that: $$|\psi\rangle = \alpha \rvert \phi \rangle + \beta \rvert \gamma \rangle$$ Prove that: $$ \operatorname{Sch}(\psi) \geq | \operatorname{Sch}(\phi) - \operatorname{Sch}(\gamma)|$$ where $\operatorname{Sch}(x)$ is the Schmidt number of the pure state labeled $x$ . My attempt: Here is what I tried. Let's assume that $ \operatorname{Sch}(\phi) > \operatorname{Sch}(\gamma)$ . If we write the Schmidt decomposition of $\phi$ and $\gamma$ : \begin{align*} | \phi \rangle &= \sum_i \phi_i | a_i^{\phi} \rangle | b_i^{\phi} \rangle\\ | \gamma \rangle &= \sum_i \gamma_i | a_i^{\gamma} \rangle | b_i^{\gamma} \rangle, \end{align*} we can then calculate the partial trace of $\psi$ regarding component $A$ : $$ \rho  \equiv \operatorname{tr}_B(| \psi \rangle \langle \psi |) = |\alpha|^2 \rho_{\phi \phi} + |\beta|^2 \rho_{\gamma \gamma} + \alpha \bar{\beta} \rho_{\phi \gamma} + \bar{\alpha} \beta \rho_{\gamma \phi}, $$ where: $$ \rho_{\phi \phi} \equiv \operatorname{tr}_B(| \phi \rangle \langle \phi |) = \sum_i \phi_i^2 | a_i^{\phi} \rangle  \langle a_i^{\phi} | \\ \rho_{\gamma \gamma} \equiv \operatorname{tr}_B(| \gamma \rangle \langle \gamma |) = \sum_i \gamma_i^2 | a_i^{\gamma} \rangle  \langle a_i^{\gamma} |  \\ \rho_{\phi \gamma}\equiv \operatorname{tr}_B(| \phi \rangle \langle \gamma |) = \sum_i \phi_i | a_i^{\phi} \rangle  \sum_j \gamma_j \langle b_j^{\gamma} | b_i^{\phi} \rangle \langle a_j^{\gamma} | \\ \rho_{\gamma \phi}\equiv \operatorname{tr}_B(| \gamma \rangle \langle \phi |) = \sum_i \gamma_i | a_i^{\gamma} \rangle  \sum_j \phi_j \langle b_j^{\phi} | b_i^{\gamma} \rangle \langle a_j^{\phi} |. $$ It can be seen that $\operatorname{Im}(\rho_{\phi \gamma})$ is a subspace of $\operatorname{Im}(\rho_{\phi})$ , and similarly that $\operatorname{Im}(\rho_{\gamma \phi})$ is a subspace of $\operatorname{Im}(\rho_{\gamma})$ . We define $ P_{\gamma} $ the projector onto $\operatorname{Im}(\rho_{\gamma})$ and $$ P_{\gamma}^{\perp} = I - P_{\gamma} $$ the projection onto the orthogonal complement of $\operatorname{Im}(\rho_{\gamma})$ . As $\operatorname{Sch}(\gamma)  < \operatorname{Sch}(\phi)$ , the subspace corresponding to $P_{\gamma}^{\perp}$ is not reduced to the zero vector. We end up with: $$ \rho = P_{\gamma}( |\alpha|^2 \rho_{\phi \phi} + |\beta|^2 \rho_{\gamma \gamma} + \alpha \bar{\beta} \rho_{\phi \gamma} + \bar{\alpha} \beta \rho_{\gamma \phi} ) + P_{\gamma}^{\perp} ( |\alpha|^2 \rho_{\phi \phi} + \alpha \bar{\beta} \rho_{\phi \gamma} ). $$ As the subspaces corresponding to the two defined projectors are orthogonal: \begin{align*} \operatorname{rank}(\rho) &= \operatorname{rank}(P_{\gamma}( |\alpha|^2 \rho_{\phi \phi} + |\beta|^2 \rho_{\gamma \gamma} + \alpha \bar{\beta} \rho_{\phi \gamma} + \bar{\alpha} \beta \rho_{\gamma \phi} ))\\ &+ \operatorname{rank}(P_{\gamma}^{\perp} ( |\alpha|^2 \rho_{\phi \phi} + \alpha \bar{\beta} \rho_{\phi \gamma} )) \\ &\geq \operatorname{rank}(P_{\gamma}^{\perp} ( |\alpha|^2 \rho_{\phi \phi} + \alpha \bar{\beta} \rho_{\phi \gamma} )), \end{align*} I was hoping to conclude by claiming that: $$ \operatorname{rank}(P_{\gamma}^{\perp} ( |\alpha|^2 \rho_{\phi \phi} + \alpha \bar{\beta} \rho_{\phi \gamma} )) \geq \operatorname{Sch}(\phi) - \operatorname{Sch}(\gamma), $$ but it is not correct, as I find counterexamples. Therefore I don't think that I am following the right track.","I started reading the Nielsen & Chuang's Quantum Computation and Quantum Information . I got stuck by the last question of Problem 2.2. I got the other problems, but I can't see this one. I guess it's not really difficult, but as I am new in this field, some help will be nice. :) Problem statement: Suppose is a pure state of a composite system with components and , such that: Prove that: where is the Schmidt number of the pure state labeled . My attempt: Here is what I tried. Let's assume that . If we write the Schmidt decomposition of and : we can then calculate the partial trace of regarding component : where: It can be seen that is a subspace of , and similarly that is a subspace of . We define the projector onto and the projection onto the orthogonal complement of . As , the subspace corresponding to is not reduced to the zero vector. We end up with: As the subspaces corresponding to the two defined projectors are orthogonal: I was hoping to conclude by claiming that: but it is not correct, as I find counterexamples. Therefore I don't think that I am following the right track.","|\psi\rangle  A B |\psi\rangle = \alpha \rvert \phi \rangle + \beta \rvert \gamma \rangle  \operatorname{Sch}(\psi) \geq | \operatorname{Sch}(\phi) - \operatorname{Sch}(\gamma)| \operatorname{Sch}(x) x  \operatorname{Sch}(\phi) > \operatorname{Sch}(\gamma) \phi \gamma \begin{align*}
| \phi \rangle &= \sum_i \phi_i | a_i^{\phi} \rangle | b_i^{\phi} \rangle\\
| \gamma \rangle &= \sum_i \gamma_i | a_i^{\gamma} \rangle | b_i^{\gamma} \rangle,
\end{align*} \psi A 
\rho  \equiv \operatorname{tr}_B(| \psi \rangle \langle \psi |) = |\alpha|^2 \rho_{\phi \phi} + |\beta|^2 \rho_{\gamma \gamma} + \alpha \bar{\beta} \rho_{\phi \gamma} + \bar{\alpha} \beta \rho_{\gamma \phi},
 
\rho_{\phi \phi} \equiv \operatorname{tr}_B(| \phi \rangle \langle \phi |) = \sum_i \phi_i^2 | a_i^{\phi} \rangle  \langle a_i^{\phi} | \\
\rho_{\gamma \gamma} \equiv \operatorname{tr}_B(| \gamma \rangle \langle \gamma |) = \sum_i \gamma_i^2 | a_i^{\gamma} \rangle  \langle a_i^{\gamma} | 
\\
\rho_{\phi \gamma}\equiv \operatorname{tr}_B(| \phi \rangle \langle \gamma |) = \sum_i \phi_i | a_i^{\phi} \rangle  \sum_j \gamma_j \langle b_j^{\gamma} | b_i^{\phi} \rangle \langle a_j^{\gamma} | \\
\rho_{\gamma \phi}\equiv \operatorname{tr}_B(| \gamma \rangle \langle \phi |) = \sum_i \gamma_i | a_i^{\gamma} \rangle  \sum_j \phi_j \langle b_j^{\phi} | b_i^{\gamma} \rangle \langle a_j^{\phi} |.
 \operatorname{Im}(\rho_{\phi \gamma}) \operatorname{Im}(\rho_{\phi}) \operatorname{Im}(\rho_{\gamma \phi}) \operatorname{Im}(\rho_{\gamma})  P_{\gamma}  \operatorname{Im}(\rho_{\gamma}) 
P_{\gamma}^{\perp} = I - P_{\gamma}
 \operatorname{Im}(\rho_{\gamma}) \operatorname{Sch}(\gamma)  < \operatorname{Sch}(\phi) P_{\gamma}^{\perp} 
\rho = P_{\gamma}( |\alpha|^2 \rho_{\phi \phi} + |\beta|^2 \rho_{\gamma \gamma} + \alpha \bar{\beta} \rho_{\phi \gamma} + \bar{\alpha} \beta \rho_{\gamma \phi} ) + P_{\gamma}^{\perp} ( |\alpha|^2 \rho_{\phi \phi} + \alpha \bar{\beta} \rho_{\phi \gamma} ).
 \begin{align*}
\operatorname{rank}(\rho) &= \operatorname{rank}(P_{\gamma}( |\alpha|^2 \rho_{\phi \phi} + |\beta|^2 \rho_{\gamma \gamma} + \alpha \bar{\beta} \rho_{\phi \gamma} + \bar{\alpha} \beta \rho_{\gamma \phi} ))\\
&+ \operatorname{rank}(P_{\gamma}^{\perp} ( |\alpha|^2 \rho_{\phi \phi} + \alpha \bar{\beta} \rho_{\phi \gamma} )) \\
&\geq \operatorname{rank}(P_{\gamma}^{\perp} ( |\alpha|^2 \rho_{\phi \phi} + \alpha \bar{\beta} \rho_{\phi \gamma} )),
\end{align*} 
\operatorname{rank}(P_{\gamma}^{\perp} ( |\alpha|^2 \rho_{\phi \phi} + \alpha \bar{\beta} \rho_{\phi \gamma} )) \geq \operatorname{Sch}(\phi) - \operatorname{Sch}(\gamma),
",['linear-algebra']
30,Matrix of absolute values and largest eigenvector,Matrix of absolute values and largest eigenvector,,"Let $A$ be a complex Hermitian $n\times n$ matrix and define the matrix $B$ to be the entry-wise absolute value of $A$, i.e., $B_{ab}=\lvert A_{ab}\rvert$. Furthermore suppose that $B$ has a unique normalised eigenvector $x$ of maximal eigenvalue $\lambda>0$, $Bx=\lambda x$, in particular $\lVert B\lVert=\lambda$ (here $\lVert \cdot\rVert$ denotes the induced matrix norm from the Euclidean norm on $\mathbb C^n$). It is obvious that $\lVert A\rVert\le \lVert B\rVert$. I now first want to consider the extreme case when $\lVert A\rVert=\lVert B\lVert$. It follows that $A$ has an normalized eigenvector $y$ of eigenvalue $\lambda$, $Ay=\lambda y$ and we can compute  \begin{align*} \lambda =\langle y, Ay\rangle= \sum_{ab}\overline{y_a} A_{ab}y_b\le \sum_{ab} \lvert y_a\rvert B_{ab} \lvert y_b\rvert =\langle\lvert y\rvert,B \lvert y\rvert\rangle\le \lambda =\langle x,Bx\rangle.\end{align*} In particular, it follows that $\lvert y\rvert =x$. Now I am interested in how this can be made quantitative. For example, if $\lVert A\rVert \ge (1-\epsilon)\lVert B\rVert$, is it true that $\lVert \lvert y\rvert -x\rVert\le C \epsilon$ for some universal constant $C$? I think the above argument can be made quantitative to give $\lVert \lvert y\rvert -x\rVert\le C \sqrt\epsilon$, where $C$ depends on the spectral gap of $B$ below the eigenvalue $\lambda$.","Let $A$ be a complex Hermitian $n\times n$ matrix and define the matrix $B$ to be the entry-wise absolute value of $A$, i.e., $B_{ab}=\lvert A_{ab}\rvert$. Furthermore suppose that $B$ has a unique normalised eigenvector $x$ of maximal eigenvalue $\lambda>0$, $Bx=\lambda x$, in particular $\lVert B\lVert=\lambda$ (here $\lVert \cdot\rVert$ denotes the induced matrix norm from the Euclidean norm on $\mathbb C^n$). It is obvious that $\lVert A\rVert\le \lVert B\rVert$. I now first want to consider the extreme case when $\lVert A\rVert=\lVert B\lVert$. It follows that $A$ has an normalized eigenvector $y$ of eigenvalue $\lambda$, $Ay=\lambda y$ and we can compute  \begin{align*} \lambda =\langle y, Ay\rangle= \sum_{ab}\overline{y_a} A_{ab}y_b\le \sum_{ab} \lvert y_a\rvert B_{ab} \lvert y_b\rvert =\langle\lvert y\rvert,B \lvert y\rvert\rangle\le \lambda =\langle x,Bx\rangle.\end{align*} In particular, it follows that $\lvert y\rvert =x$. Now I am interested in how this can be made quantitative. For example, if $\lVert A\rVert \ge (1-\epsilon)\lVert B\rVert$, is it true that $\lVert \lvert y\rvert -x\rVert\le C \epsilon$ for some universal constant $C$? I think the above argument can be made quantitative to give $\lVert \lvert y\rvert -x\rVert\le C \sqrt\epsilon$, where $C$ depends on the spectral gap of $B$ below the eigenvalue $\lambda$.",,"['real-analysis', 'linear-algebra', 'eigenvalues-eigenvectors', 'spectral-theory']"
31,"Does this ""apparently simple"" linear system have a solution?","Does this ""apparently simple"" linear system have a solution?",,"Does there exist $x \in (0,1)^\mathbb{N}$ such that for any decreasing $v \in (0,1)^\mathbb{N}$ that converges to $0$ (meaning, $v_{n+1} < v_n$ and $v_n \rightarrow 0$), there is $a \in (0,1)^\mathbb{N}$ with:   $$ \begin{array}{lll} \sum a_n & =& v_0\\ \sum a_n x_n& = &v_1\\ \sum a_n x^2_n & = &v_2\\ \mbox{etc?}&& \end{array} $$ [NB: I asked a related question before, which I then rewrote as the above. But I was recommended to post it anew to avoid invalidating old answers.] Note that my question is equivalent to this: think of an infinite Vandermonde matrix with coefficients $x \in (0,1)^\mathbb{N}$ $$ V(x) = \left( \begin{array}{llllll} 1      & 1    & 1      & \cdots & 1     & \cdots\\ x_1    & x_2  & x_3    & \cdots & x_n   & \cdots\\ x^2_1  & x^2_2& x^2_3  & \cdots & x^2_n & \cdots\\ \vdots &\vdots&\vdots  & \ddots &\vdots & \vdots\\ x_1^m & x^m_2 &x^m_3 & \cdots & x^m_n  & \cdots\\ \vdots &\vdots&\vdots  & \ddots &\vdots & \vdots  \end{array} \right) $$ (where superscrits represent exponents and not just row position) and ask whether there is $x \in (0,1)^\mathbb{N}$ such that for any decreasing and convergent (to $0$) $v \in (0, 1)^\mathbb{N}$, there is $a \in (0, 1)^\mathbb{N}$ satisfying $V(x) \cdot a= v$. I've found the following relevant paper: http://www.sciencedirect.com/science/article/pii/S0019357712000419 but it requires that $x\,$ satisfy $\sum_n \frac{1}{|x_n|} < \infty$, which cannot be true in the above (because $1 / |x_n| > 1$ for all $n$). The paper makes this assumption because it assumes nothing about $v$, so there's hope. As always, thank you all for your help.","Does there exist $x \in (0,1)^\mathbb{N}$ such that for any decreasing $v \in (0,1)^\mathbb{N}$ that converges to $0$ (meaning, $v_{n+1} < v_n$ and $v_n \rightarrow 0$), there is $a \in (0,1)^\mathbb{N}$ with:   $$ \begin{array}{lll} \sum a_n & =& v_0\\ \sum a_n x_n& = &v_1\\ \sum a_n x^2_n & = &v_2\\ \mbox{etc?}&& \end{array} $$ [NB: I asked a related question before, which I then rewrote as the above. But I was recommended to post it anew to avoid invalidating old answers.] Note that my question is equivalent to this: think of an infinite Vandermonde matrix with coefficients $x \in (0,1)^\mathbb{N}$ $$ V(x) = \left( \begin{array}{llllll} 1      & 1    & 1      & \cdots & 1     & \cdots\\ x_1    & x_2  & x_3    & \cdots & x_n   & \cdots\\ x^2_1  & x^2_2& x^2_3  & \cdots & x^2_n & \cdots\\ \vdots &\vdots&\vdots  & \ddots &\vdots & \vdots\\ x_1^m & x^m_2 &x^m_3 & \cdots & x^m_n  & \cdots\\ \vdots &\vdots&\vdots  & \ddots &\vdots & \vdots  \end{array} \right) $$ (where superscrits represent exponents and not just row position) and ask whether there is $x \in (0,1)^\mathbb{N}$ such that for any decreasing and convergent (to $0$) $v \in (0, 1)^\mathbb{N}$, there is $a \in (0, 1)^\mathbb{N}$ satisfying $V(x) \cdot a= v$. I've found the following relevant paper: http://www.sciencedirect.com/science/article/pii/S0019357712000419 but it requires that $x\,$ satisfy $\sum_n \frac{1}{|x_n|} < \infty$, which cannot be true in the above (because $1 / |x_n| > 1$ for all $n$). The paper makes this assumption because it assumes nothing about $v$, so there's hope. As always, thank you all for your help.",,"['real-analysis', 'linear-algebra', 'functional-analysis']"
32,Minimal Polynomial of $AB$ and $BA$,Minimal Polynomial of  and,AB BA,"Let $A,B$ be linear operators on a complex vector space $V$ . Prove the following: (a) For any polynomial $p$ such that $p(AB)=0$ , if $q(x)=x\cdot p(x)$ , then $q(BA)=0$ (b) Use part (a) to show that if $m_{AB}$ is the minimal polynomial of $AB$ , then $m_{AB}=m_{BA}$ or $m_{AB}=x\cdot m_{BA}$ . I've proven part (a). For part (b), I think there should be a third possibility, $m_{BA}=x\cdot m_{AB}$ . Is this correct? Here's what I have: $m_{AB}(AB)=0$ , so by part (a), if $q(x)=x\cdot m_{AB}$ , then $q(BA)=0$ . So $m_{BA}|q\implies\exists \ g\in\mathbb{C}[x]$ s.t. $m_{BA}\cdot g=q=x\cdot m_{AB}$ . Reversing the roles of $AB$ and $BA$ $\exists\tilde{g}\in\mathbb{C}[x]$ s.t. $m_{AB}\cdot\tilde{g}=x\cdot m_{BA}$ . Hence, \begin{cases}m_{BA}\cdot g=x\cdot m_{AB}\\ m_{AB}\cdot\tilde{g}=x\cdot m_{BA}\end{cases} . This implies $m_{AB}\cdot\tilde{g}g=x\cdot m_{BA}\cdot g=x^2\cdot m_{AB}$ . So $\tilde{g}g=x^2$ . Which means we have three cases: (1) $g=\tilde{g}=x$ , (2) $g=x^2,\tilde{g}=1$ , or (3) $g=1,\tilde{g}=x^2$ . For case (1), $m_{BA}\cdot x=x\cdot m_{AB}\implies m_{AB}=m_{BA}$ . For case (2) $m_{AB}=x\cdot m_{BA}$ . For case (3), $m_{BA}=x\cdot m_{AB}$ .","Let be linear operators on a complex vector space . Prove the following: (a) For any polynomial such that , if , then (b) Use part (a) to show that if is the minimal polynomial of , then or . I've proven part (a). For part (b), I think there should be a third possibility, . Is this correct? Here's what I have: , so by part (a), if , then . So s.t. . Reversing the roles of and s.t. . Hence, . This implies . So . Which means we have three cases: (1) , (2) , or (3) . For case (1), . For case (2) . For case (3), .","A,B V p p(AB)=0 q(x)=x\cdot p(x) q(BA)=0 m_{AB} AB m_{AB}=m_{BA} m_{AB}=x\cdot m_{BA} m_{BA}=x\cdot m_{AB} m_{AB}(AB)=0 q(x)=x\cdot m_{AB} q(BA)=0 m_{BA}|q\implies\exists \ g\in\mathbb{C}[x] m_{BA}\cdot g=q=x\cdot m_{AB} AB BA \exists\tilde{g}\in\mathbb{C}[x] m_{AB}\cdot\tilde{g}=x\cdot m_{BA} \begin{cases}m_{BA}\cdot g=x\cdot m_{AB}\\ m_{AB}\cdot\tilde{g}=x\cdot m_{BA}\end{cases} m_{AB}\cdot\tilde{g}g=x\cdot m_{BA}\cdot g=x^2\cdot m_{AB} \tilde{g}g=x^2 g=\tilde{g}=x g=x^2,\tilde{g}=1 g=1,\tilde{g}=x^2 m_{BA}\cdot x=x\cdot m_{AB}\implies m_{AB}=m_{BA} m_{AB}=x\cdot m_{BA} m_{BA}=x\cdot m_{AB}","['linear-algebra', 'proof-verification', 'minimal-polynomials']"
33,Factorization of certain polynomials over a finite field,Factorization of certain polynomials over a finite field,,"Suppose we have the polynomial $$F_s(x)=cx^{q^s+1}+dx^{q^s}-ax-b \in \mathbb{F}_{q^n}[x]$$ where $ad-bc \neq 0$. The fact that $ad-bc \neq 0$ means that we can take the coefficients of $F_s(x)$ as entries of a matrix $A \in GL(2,q^n)$. This problem was considered by Stichtenoth and Garefalakis for $A=\left( \begin{array}{cc} a&b\\ c&d \end{array}\right) $ and $A=\left( \begin{array}{cc} a&b\\ 0&1 \end{array}\right) $ respectively. In both cases $A$ was taken to be in $GL(2,q)$. In Garefalakis, the exact number of irreducible polynomials of degree say $r$ in the factorization of $F_s(x)$ is obtained. Now, how I can I find the number of irreducible polynomials(factors) of degree $r$ say in the factorization of $F_s(x)=cx^{q^s+1}+dx^{q^s}-ax-b \in \mathbb{F}_{q^n}[x]$ where the minimal polynomial of $A$ is an irreducible quadratic polynomial over $\mathbb{F}_{q^n}[x]$ where $gcd(n,r)\neq 1$ and $s\mid nr$?","Suppose we have the polynomial $$F_s(x)=cx^{q^s+1}+dx^{q^s}-ax-b \in \mathbb{F}_{q^n}[x]$$ where $ad-bc \neq 0$. The fact that $ad-bc \neq 0$ means that we can take the coefficients of $F_s(x)$ as entries of a matrix $A \in GL(2,q^n)$. This problem was considered by Stichtenoth and Garefalakis for $A=\left( \begin{array}{cc} a&b\\ c&d \end{array}\right) $ and $A=\left( \begin{array}{cc} a&b\\ 0&1 \end{array}\right) $ respectively. In both cases $A$ was taken to be in $GL(2,q)$. In Garefalakis, the exact number of irreducible polynomials of degree say $r$ in the factorization of $F_s(x)$ is obtained. Now, how I can I find the number of irreducible polynomials(factors) of degree $r$ say in the factorization of $F_s(x)=cx^{q^s+1}+dx^{q^s}-ax-b \in \mathbb{F}_{q^n}[x]$ where the minimal polynomial of $A$ is an irreducible quadratic polynomial over $\mathbb{F}_{q^n}[x]$ where $gcd(n,r)\neq 1$ and $s\mid nr$?",,"['linear-algebra', 'group-theory', 'polynomials', 'finite-fields']"
34,How to find such an isotropic space?,How to find such an isotropic space?,,"Suppose $(V,f)$ is a nonsingular orthogonal geometry with dimension $n$, and $W$ is a totally isotropic space with dimension $r$. How to find such a totally isotropic space $N$   with dimension $r$ such that 1)$V=W^\bot  \oplus N$ 2)$V=W \oplus H\oplus N$ where $H=W^\bot \cap N^\bot$ In addition, if the first condition holds, then the second one is true. Suppose there's a base $\{a_1,\dots,a_s,a_{-1},\dots,a_{-(n-s)}\}$ that $f$ under this base is $\begin{pmatrix} I_s& 0\\0&-I_{n-s}\end{pmatrix}$. And suppose $W=\langle a_1+a_{-1},...,a_r+a_{-r}\rangle$. Then how to construct such a $N$? It seems that $N=\langle a_1+a_{-(n-2r+1)}, \dots ,a_r+a_{-(n-r)}\rangle $ satisfies the conditions, but I not pretty sure. Am I wrong or right? If I'm right, then how to prove it? Is there an clearer way to see this? Thanks in advance. $\textbf{EDIT}$:After reading some suggestions on meta, I realized that I was not explicit with my post and efforts. As I've mentioned above, I $\textbf{seemed}$ to find an $N$ that satisfied the conditions. But when I examined this $N$ in the general case $n$, I found that the calculation was too difficult. So I worked with some simple cases when $n=5$, $r=s=2$ and $n=9$, $r=s=3$. In these cases, the first condition is always satisfied. But as for the second one, it's not clear because the basis of $H$ is really complicated. So I'm thinking that my answer might be wrong or I chose a ""complicated"" basis. Sorry that this might be a silly question. Thanks again.","Suppose $(V,f)$ is a nonsingular orthogonal geometry with dimension $n$, and $W$ is a totally isotropic space with dimension $r$. How to find such a totally isotropic space $N$   with dimension $r$ such that 1)$V=W^\bot  \oplus N$ 2)$V=W \oplus H\oplus N$ where $H=W^\bot \cap N^\bot$ In addition, if the first condition holds, then the second one is true. Suppose there's a base $\{a_1,\dots,a_s,a_{-1},\dots,a_{-(n-s)}\}$ that $f$ under this base is $\begin{pmatrix} I_s& 0\\0&-I_{n-s}\end{pmatrix}$. And suppose $W=\langle a_1+a_{-1},...,a_r+a_{-r}\rangle$. Then how to construct such a $N$? It seems that $N=\langle a_1+a_{-(n-2r+1)}, \dots ,a_r+a_{-(n-r)}\rangle $ satisfies the conditions, but I not pretty sure. Am I wrong or right? If I'm right, then how to prove it? Is there an clearer way to see this? Thanks in advance. $\textbf{EDIT}$:After reading some suggestions on meta, I realized that I was not explicit with my post and efforts. As I've mentioned above, I $\textbf{seemed}$ to find an $N$ that satisfied the conditions. But when I examined this $N$ in the general case $n$, I found that the calculation was too difficult. So I worked with some simple cases when $n=5$, $r=s=2$ and $n=9$, $r=s=3$. In these cases, the first condition is always satisfied. But as for the second one, it's not clear because the basis of $H$ is really complicated. So I'm thinking that my answer might be wrong or I chose a ""complicated"" basis. Sorry that this might be a silly question. Thanks again.",,"['linear-algebra', 'abstract-algebra', 'vector-spaces', 'quadratic-forms']"
35,Number of solutions of the linear equation,Number of solutions of the linear equation,,"There are two positive integers $a$ and $b$ such that $a \mod b$ is not zero. We find a value $n=\lfloor a/b \rfloor+1$ where $\lfloor .\rfloor$ is the block function. We are supposed to find the number of solutions of the following equation: $$x_1+x_2+x_3+....x_n=a;\text{ where }1\le x_i \le b \text{ and } 1 \le i \le n.$$ Example: Given: $a=7;b=3$. We get $n =\lfloor a/b \rfloor + 1 = \lfloor 7/3 \rfloor +1 =2+1=3.$ Thus, we need to find the number of solutions of the following equation: $$x_1+x_2+x_3=7; \text{ where } 1 \le x_i \le 3 \text{ and } 1\le i\le 3.$$ This equation has 6 solutions: $$x_1=1;x_2=3;x_3=3.$$ $$x_1=3;x_2=1;x_3=3.$$ $$x_1=3;x_2=3;x_3=1.$$ $$x_1=2;x_2=2;x_3=3.$$ $$x_1=2;x_2=3;x_3=2.$$ $$x_1=3;x_2=2;x_3=2.$$ Hence for $a=7;b=3$, the answer is $6$. I could find this manually because the values of $a$ and $b$ are small. But for large values of $a$ and $b$ in the order of 100s or 1000s, I need to generalise this.","There are two positive integers $a$ and $b$ such that $a \mod b$ is not zero. We find a value $n=\lfloor a/b \rfloor+1$ where $\lfloor .\rfloor$ is the block function. We are supposed to find the number of solutions of the following equation: $$x_1+x_2+x_3+....x_n=a;\text{ where }1\le x_i \le b \text{ and } 1 \le i \le n.$$ Example: Given: $a=7;b=3$. We get $n =\lfloor a/b \rfloor + 1 = \lfloor 7/3 \rfloor +1 =2+1=3.$ Thus, we need to find the number of solutions of the following equation: $$x_1+x_2+x_3=7; \text{ where } 1 \le x_i \le 3 \text{ and } 1\le i\le 3.$$ This equation has 6 solutions: $$x_1=1;x_2=3;x_3=3.$$ $$x_1=3;x_2=1;x_3=3.$$ $$x_1=3;x_2=3;x_3=1.$$ $$x_1=2;x_2=2;x_3=3.$$ $$x_1=2;x_2=3;x_3=2.$$ $$x_1=3;x_2=2;x_3=2.$$ Hence for $a=7;b=3$, the answer is $6$. I could find this manually because the values of $a$ and $b$ are small. But for large values of $a$ and $b$ in the order of 100s or 1000s, I need to generalise this.",,"['linear-algebra', 'systems-of-equations']"
36,"Can commuting matrices $X,Y$ always be written as polynomials of some matrix $A$?",Can commuting matrices  always be written as polynomials of some matrix ?,"X,Y A","Consider square matrices over a field $K$. I don't think additional assumptions about $K$ like algebraically closed or characteristic $0$ are pertinent, but feel free to make them for comfort. For any such matrix $A$, the set $K[A]$ of polynomials in $A$ is a commutative subalgebra of $M_n(K)$; the question is whether for any pair of commuting matrices $X,Y$ at least one such commutative subalgebra can be found that contains both $X$ and $Y$. I was asking myself this in connection with frequently recurring requests to completely characterise commuting pairs of matrices, like this one . While providing a useful characterisation seems impossible, a positive anwer to the current question would at least provide some answer. Note that in many rather likely situations one can in fact take $A$ to be one of the matrices $X,Y$, for instance when one of the matrices has distinct eigenvalues , or more generally if its minimal polynomial has degree $n$ (so coincides with the characteristic polynomial). However this is not always possible, as can be easily seen for instance for diagonal matrices $X=\operatorname{diag}(0,0,1)$ and $Y=\operatorname{diag}(0,1,1)$. However in that case both will be polynomials in $A=\operatorname{diag}(x,y,z)$ for any distinct values $x,y,z$ (then $K[A]$ consists of all diagonal matrices); although in the example in this answer the matrices are not both diagonalisable, an appropriate $A$ can be found there as well. I thought for some time that any maximal commutative subalgebra of $M_n(K)$ was of the form $K[A]$ (which would imply a positive answer) for some $A$ with minimal polynomial of degree$~n$, and that a positive answer to my question was in fact instrumental in proving this. However I was wrong on both counts: there exist (for $n\geq 4$) commutative subalgebras of dimension${}>n$ (whereas $\dim_KK[A]\leq n$ for all $A\in M_n(K)$) as shown in this MathOverflow answer , and I was forced to correct an anwer I gave here in the light of this; however it seems (at least in the cases I looked at) that many (all?) pairs of matrices $X,Y$ in such a subalgebra still admit a matrix $A$ (which in general is not in the subalgebra ) such that $X,Y\in K[A]$. This indicates that a positive answer to my question would not contradict the existence of such large commutative subalgebras: it would just mean that to obtain a maximal dimensional subalgebra containing $X,Y$ one should in general avoid throwing in an $A$ with $X,Y\in K[A]$. I do think these large subalgebras easily show that my question but for three commuting matrices has a negative answer. Finally I note that this other answer to the cited MO question mentions a result by Gerstenhaber that the dimension of the subalgebra generated two commuting matrices in $M_n(K)$ cannot exceed$~n$. This unfortunately does not settle my question (if $X,Y$ would generate a subalgebra of dimension${}>n$, it would have proved a negative answer); it just might be that the mentioned result is true because of the existence of $A$ (I don't have access to a proof right now, but given the formulation it seems unlikely that it was done this way). OK, I've tried to build up the suspense. Honesty demands that I say that I do know the answer to my question, since a colleague of mine provided a convincing one. I will however not give this answer right away, but post it once there has been some time for gathering answers here; who knows somebody will prove a different answer than the one I have (heaven forbid), or at least give the same answer with a different justification.","Consider square matrices over a field $K$. I don't think additional assumptions about $K$ like algebraically closed or characteristic $0$ are pertinent, but feel free to make them for comfort. For any such matrix $A$, the set $K[A]$ of polynomials in $A$ is a commutative subalgebra of $M_n(K)$; the question is whether for any pair of commuting matrices $X,Y$ at least one such commutative subalgebra can be found that contains both $X$ and $Y$. I was asking myself this in connection with frequently recurring requests to completely characterise commuting pairs of matrices, like this one . While providing a useful characterisation seems impossible, a positive anwer to the current question would at least provide some answer. Note that in many rather likely situations one can in fact take $A$ to be one of the matrices $X,Y$, for instance when one of the matrices has distinct eigenvalues , or more generally if its minimal polynomial has degree $n$ (so coincides with the characteristic polynomial). However this is not always possible, as can be easily seen for instance for diagonal matrices $X=\operatorname{diag}(0,0,1)$ and $Y=\operatorname{diag}(0,1,1)$. However in that case both will be polynomials in $A=\operatorname{diag}(x,y,z)$ for any distinct values $x,y,z$ (then $K[A]$ consists of all diagonal matrices); although in the example in this answer the matrices are not both diagonalisable, an appropriate $A$ can be found there as well. I thought for some time that any maximal commutative subalgebra of $M_n(K)$ was of the form $K[A]$ (which would imply a positive answer) for some $A$ with minimal polynomial of degree$~n$, and that a positive answer to my question was in fact instrumental in proving this. However I was wrong on both counts: there exist (for $n\geq 4$) commutative subalgebras of dimension${}>n$ (whereas $\dim_KK[A]\leq n$ for all $A\in M_n(K)$) as shown in this MathOverflow answer , and I was forced to correct an anwer I gave here in the light of this; however it seems (at least in the cases I looked at) that many (all?) pairs of matrices $X,Y$ in such a subalgebra still admit a matrix $A$ (which in general is not in the subalgebra ) such that $X,Y\in K[A]$. This indicates that a positive answer to my question would not contradict the existence of such large commutative subalgebras: it would just mean that to obtain a maximal dimensional subalgebra containing $X,Y$ one should in general avoid throwing in an $A$ with $X,Y\in K[A]$. I do think these large subalgebras easily show that my question but for three commuting matrices has a negative answer. Finally I note that this other answer to the cited MO question mentions a result by Gerstenhaber that the dimension of the subalgebra generated two commuting matrices in $M_n(K)$ cannot exceed$~n$. This unfortunately does not settle my question (if $X,Y$ would generate a subalgebra of dimension${}>n$, it would have proved a negative answer); it just might be that the mentioned result is true because of the existence of $A$ (I don't have access to a proof right now, but given the formulation it seems unlikely that it was done this way). OK, I've tried to build up the suspense. Honesty demands that I say that I do know the answer to my question, since a colleague of mine provided a convincing one. I will however not give this answer right away, but post it once there has been some time for gathering answers here; who knows somebody will prove a different answer than the one I have (heaven forbid), or at least give the same answer with a different justification.",,"['linear-algebra', 'abstract-algebra', 'matrices']"
37,Is there a relationship between the trace and the Clifford/geometric product?,Is there a relationship between the trace and the Clifford/geometric product?,,"In what follows, let $V=\mathbb{R}^n$ (although the following probably applies also to a larger number of finite-dimensional spaces). We assume throughout that we have made a choice for an inner product $\langle \cdot, \cdot \rangle$ on $V$. Geometric/Clifford Product: The geometric/Clifford product of any two vectors $v,w \in V$ decomposes as: $$v w = \langle v, w\rangle + v \wedge w\,, $$ where the first term on the right-hand side is the inner product of $v,w$, and the second term corresponds to the bivector spanned by $v$ and $w$ in that orientation. Trace: Our choice of inner product $\langle \cdot, \cdot \rangle$ on $V$ induces a canonical (w.r.t. $\langle \cdot, \cdot \rangle$) isomorphism between $V$ and $V^*$. This allows us to identify $V \otimes V$ and $V^* \otimes V$, and $V^* \otimes V$ can always be identified with $\operatorname{End}(V)=\operatorname{Hom}(V,V)=\mathscr{L}(V)$ the space of linear operators on $V$ ( I think ). Then the bilinear function $\langle \cdot, \cdot \rangle : V \times V \to \mathbb{R}$ induces under the identification $V^* \cong V$ a bilinear function $V^* \times V \to \mathbb{R}$ which via the universal property of tensor products induces a linear function $V^* \otimes V \cong \operatorname{End}(V) \to \mathbb{R}$ which turns out to be the trace. (See my two previous questions (1) (2) of which this question is a follow-up.) Now note that the kernel of the trace $\mathfrak{sl}(n,\mathbb{R})$, and also note that an important subspace of $\mathfrak{sl}(n,\mathbb{R})$ is $\mathfrak{o}(n,\mathbb{R})=\mathfrak{so}(n,\mathbb{R})$, which can be identified with the space of all skew-symmetric matrices. The latter space, $\mathfrak{so}(n,\mathbb{R})$, can be identified with the space of all bivectors (see here or here ). Thus, by factoring through $V^* \otimes V$, the Clifford/geometric product decomposition above assumes the form $$(v,w) \mapsto (v^*,w) \mapsto v^* \otimes w \mapsto \operatorname{trace}(v^* \otimes w) + v \wedge w = \operatorname{trace}(v^* \otimes w) + (v^* \otimes w - w^* \otimes v)\,,  $$ with the first term $\operatorname{trace}(v^* \otimes w)$ being in the image of the trace and the second term $v \wedge w$ being in the kernel. Thus the decomposition of the Clifford product $vw$ seems to (implicitly) have the form of an image-kernel decomposition with respect to the linear map trace. Question: Is this correct? Can the Clifford product really be interpreted as being related to or derived from the trace? Specifically, it seems like in the above we are disregarding the elements $\mathfrak{sl}(n,\mathbb{R}) \setminus \mathfrak{so}(n,\mathbb{R})$. Thus (assuming that there is a natural splitting of $\mathfrak{sl}(n,\mathbb{R})= S \oplus \mathfrak{so}(n,\mathbb{R}) $, and I don't know whether there is or isn't), can the quotienting process which gives us the Clifford algebra from the tensor algebra be considered as equivalent to ""quotienting out"" those elements of the kernel of the trace which are not also skew-symmetric operators? Or am I over-simplifying? Also for some reason I thought there might be some relationship between this and semidirect products of Lie groups, but I don't know that topic anywhere near well enough for that to be considered anything besides wild speculation based on visual similarities of the formulas.","In what follows, let $V=\mathbb{R}^n$ (although the following probably applies also to a larger number of finite-dimensional spaces). We assume throughout that we have made a choice for an inner product $\langle \cdot, \cdot \rangle$ on $V$. Geometric/Clifford Product: The geometric/Clifford product of any two vectors $v,w \in V$ decomposes as: $$v w = \langle v, w\rangle + v \wedge w\,, $$ where the first term on the right-hand side is the inner product of $v,w$, and the second term corresponds to the bivector spanned by $v$ and $w$ in that orientation. Trace: Our choice of inner product $\langle \cdot, \cdot \rangle$ on $V$ induces a canonical (w.r.t. $\langle \cdot, \cdot \rangle$) isomorphism between $V$ and $V^*$. This allows us to identify $V \otimes V$ and $V^* \otimes V$, and $V^* \otimes V$ can always be identified with $\operatorname{End}(V)=\operatorname{Hom}(V,V)=\mathscr{L}(V)$ the space of linear operators on $V$ ( I think ). Then the bilinear function $\langle \cdot, \cdot \rangle : V \times V \to \mathbb{R}$ induces under the identification $V^* \cong V$ a bilinear function $V^* \times V \to \mathbb{R}$ which via the universal property of tensor products induces a linear function $V^* \otimes V \cong \operatorname{End}(V) \to \mathbb{R}$ which turns out to be the trace. (See my two previous questions (1) (2) of which this question is a follow-up.) Now note that the kernel of the trace $\mathfrak{sl}(n,\mathbb{R})$, and also note that an important subspace of $\mathfrak{sl}(n,\mathbb{R})$ is $\mathfrak{o}(n,\mathbb{R})=\mathfrak{so}(n,\mathbb{R})$, which can be identified with the space of all skew-symmetric matrices. The latter space, $\mathfrak{so}(n,\mathbb{R})$, can be identified with the space of all bivectors (see here or here ). Thus, by factoring through $V^* \otimes V$, the Clifford/geometric product decomposition above assumes the form $$(v,w) \mapsto (v^*,w) \mapsto v^* \otimes w \mapsto \operatorname{trace}(v^* \otimes w) + v \wedge w = \operatorname{trace}(v^* \otimes w) + (v^* \otimes w - w^* \otimes v)\,,  $$ with the first term $\operatorname{trace}(v^* \otimes w)$ being in the image of the trace and the second term $v \wedge w$ being in the kernel. Thus the decomposition of the Clifford product $vw$ seems to (implicitly) have the form of an image-kernel decomposition with respect to the linear map trace. Question: Is this correct? Can the Clifford product really be interpreted as being related to or derived from the trace? Specifically, it seems like in the above we are disregarding the elements $\mathfrak{sl}(n,\mathbb{R}) \setminus \mathfrak{so}(n,\mathbb{R})$. Thus (assuming that there is a natural splitting of $\mathfrak{sl}(n,\mathbb{R})= S \oplus \mathfrak{so}(n,\mathbb{R}) $, and I don't know whether there is or isn't), can the quotienting process which gives us the Clifford algebra from the tensor algebra be considered as equivalent to ""quotienting out"" those elements of the kernel of the trace which are not also skew-symmetric operators? Or am I over-simplifying? Also for some reason I thought there might be some relationship between this and semidirect products of Lie groups, but I don't know that topic anywhere near well enough for that to be considered anything besides wild speculation based on visual similarities of the formulas.",,"['linear-algebra', 'lie-algebras', 'tensor-products', 'clifford-algebras', 'geometric-algebras']"
38,Finding the Equations of Motion in a Classical Mechanics Problem,Finding the Equations of Motion in a Classical Mechanics Problem,,"Question Consider an axle which connects two wheels of radius $R$ each at distance $L$ from its center; each wheel may spin frictionlessly about the axle but they are constrained by strong static friction such that they roll with respect to the ground (See the figure below). The following equations are the Newton-Euler equations for each part of the system shown in the figure \begin{align*} {\bf{X}}_{P_1}+{\bf{X}}_{C_1}+\,m{\bf{g}}  &=  m \ddot{{\bf{r}}}_{C_1}  \\ {\bf{X}}_{P_2}+{\bf{X}}_{C_2}+\,m{\bf{g}}  &=  m \ddot{{\bf{r}}}_{C_2}  \\ -{\bf{X}}_{C_1}-{\bf{X}}_{C_2}+M{\bf{g}}  &=  M \ddot{{\bf{r}}}_{C_3} \\ {\bf{T}}_{1}+{\bf{Y}}_{C_1}+{}_{C_1}{\bf{r}}_{P_1}\times{\bf{X}}_{P_1}  &=  {}_{C_1}\dot{\bf{H}}_{1}  \\ {\bf{T}}_{2}+{\bf{Y}}_{C_2}+{}_{C_2}{\bf{r}}_{P_2}\times{\bf{X}}_{P_2}  &=  {}_{C_2}\dot{\bf{H}}_{2}  \\ -{\bf{Y}}_{C_1}-{\bf{Y}}_{C_2}-{}_{C_3}{\bf{r}}_{C_1}\times{\bf{X}}_{C_1}-{}_{C_3}{\bf{r}}_{C_2}\times{\bf{X}}_{C_2}  &=  {}_{C_3}\dot{\bf{H}}_{3} \tag{1} \end{align*} where we also know that \begin{align*} {\bf{Y}}_{C_1}  \boldsymbol{\cdot} {\bf{b}}_1  &=  0  \\ {\bf{Y}}_{C_2}  \boldsymbol{\cdot} {\bf{b}}_1  &=  0 \tag{2} \end{align*} which means that the connection of the wheels and the shaft is such that no constraint torque is applied in ${\bf{b}}_1$ direction. Also, by assuming that both wheels roll with respect to the ground one can obtain the following constraint equations \begin{align*} \dot{x}  &=  \frac{R}{2}(\dot{\varphi_1}+\dot{\varphi_2})\sin\theta  \\ -\dot{y}  &=  \frac{R}{2}(\dot{\varphi_1}+\dot{\varphi_2})\cos\theta  \\ \theta-\theta_0   &=  \frac{R}{2L}(\varphi_1-\varphi_2) \tag{3} \end{align*} My final aim is to find the generalized coordinates $\varphi_1,\varphi_2,\varphi_3,\theta,x,y$ as a function of time. So I need another three scalar equations in terms of generalized coordinates and their derivatives other than the three constraint equations in $(3)$. I think that we should obtain them by manipulating equations in $(1)$; however, I am  unable to obtain those. Any hint or help for finding them is appreciated. :) Notice All of the quantities $\ddot{{\bf{r}}}_{C_1},\ddot{{\bf{r}}}_{C_2},\ddot{{\bf{r}}}_{C_3},\dot{\bf{H}}_{1},\dot{\bf{H}}_{2},\dot{\bf{H}}_{3},{}_{C_3}{\bf{r}}_{C_1},{}_{C_1}{\bf{r}}_{P_1}$ depend on the generalized coordinates $\varphi_1,\varphi_2,\varphi_3,\theta,x,y$ and their first and second time derivatives. The system is linear with respect to the constraint forces ${\bf{X}}_{P_1},{\bf{X}}_{P_2},{\bf{X}}_{C_1},{\bf{X}}_{C_2},{\bf{Y}}_{C_1},{\bf{Y}}_{C_2}$. ${}_{P_1}{\bf{r}}_{C_1}={}_{P_2}{\bf{r}}_{C_2}=R{\bf{a}}_3$ and $-{}_{C_3}{\bf{r}}_{C_1}={}_{C_3}{\bf{r}}_{C_2}=L{\bf{b}}_1$. That would be nice if we could obtain a vectorial equation without choosing a basis. So we may think of elimiating constraint forces from equations in $(1)$. As an attemp I summed the first three and second three equations in $(1)$ to obtain \begin{align*} {\bf{X}}_{P_1}+{\bf{X}}_{P_2}+(M+2m){\bf{g}}  &=  (M+2m)\ddot{{\bf{r}}}_{C_3} \\ {\bf{T}}_{1}+{\bf{T}}_{2}+{}_{C_1}{\bf{r}}_{P_1}\times({\bf{X}}_{P_1}+{\bf{X}}_{P_2})  &=  {}_{C_1}\dot{\bf{H}}_{1}+{}_{C_2}\dot{\bf{H}}_{2}+{}_{C_3}\dot{\bf{H}}_{3} \\ -{}_{C_3}{\bf{r}}_{C_1}\times({\bf{X}}_{C_1}-{\bf{X}}_{C_2}) & \tag{4} \end{align*} $\quad\,\,$ however, I don't know how to proceed further and make ${\bf{X}}_{C_1}-{\bf{X}}_{C_2}$ disappear. Notation $m$ is the mass of each wheel and $M$ is the mass of the shaft. $R$ is the the radius of wheels and $2L$ is the length of the shaft. ${\bf{g}}$: accelaration due to gravity. ${\bf{X}}_{P_i}$: The constraint force applied at point $P_i$ which is exerted by surface to the wheel $i$. ${\bf{X}}_{C_i}$: The constraint force applied at point $C_i$ which is exerted by shaft to the wheel $i$. ${\bf{Y}}_{C_i}$: The constraint torque which is exerted by shaft to the wheel $i$. ${\bf{T}}_{i}$: The external torque applied by the actuator to the wheel $i$. ${\bf{r}}_{P}$: The position vector of point $P$ with respect to origin of frame $a$. ${}_{Q}{\bf{r}}_{P}$: The position vector of point $P$ with respect to piont $Q$. ${}_{P}{\bf{H}}_i$: The angular momentum of part $i$ with respect to point $P$. The overdot notation represents the time derivative with respect to the inertial frame $a$. Figure","Question Consider an axle which connects two wheels of radius $R$ each at distance $L$ from its center; each wheel may spin frictionlessly about the axle but they are constrained by strong static friction such that they roll with respect to the ground (See the figure below). The following equations are the Newton-Euler equations for each part of the system shown in the figure \begin{align*} {\bf{X}}_{P_1}+{\bf{X}}_{C_1}+\,m{\bf{g}}  &=  m \ddot{{\bf{r}}}_{C_1}  \\ {\bf{X}}_{P_2}+{\bf{X}}_{C_2}+\,m{\bf{g}}  &=  m \ddot{{\bf{r}}}_{C_2}  \\ -{\bf{X}}_{C_1}-{\bf{X}}_{C_2}+M{\bf{g}}  &=  M \ddot{{\bf{r}}}_{C_3} \\ {\bf{T}}_{1}+{\bf{Y}}_{C_1}+{}_{C_1}{\bf{r}}_{P_1}\times{\bf{X}}_{P_1}  &=  {}_{C_1}\dot{\bf{H}}_{1}  \\ {\bf{T}}_{2}+{\bf{Y}}_{C_2}+{}_{C_2}{\bf{r}}_{P_2}\times{\bf{X}}_{P_2}  &=  {}_{C_2}\dot{\bf{H}}_{2}  \\ -{\bf{Y}}_{C_1}-{\bf{Y}}_{C_2}-{}_{C_3}{\bf{r}}_{C_1}\times{\bf{X}}_{C_1}-{}_{C_3}{\bf{r}}_{C_2}\times{\bf{X}}_{C_2}  &=  {}_{C_3}\dot{\bf{H}}_{3} \tag{1} \end{align*} where we also know that \begin{align*} {\bf{Y}}_{C_1}  \boldsymbol{\cdot} {\bf{b}}_1  &=  0  \\ {\bf{Y}}_{C_2}  \boldsymbol{\cdot} {\bf{b}}_1  &=  0 \tag{2} \end{align*} which means that the connection of the wheels and the shaft is such that no constraint torque is applied in ${\bf{b}}_1$ direction. Also, by assuming that both wheels roll with respect to the ground one can obtain the following constraint equations \begin{align*} \dot{x}  &=  \frac{R}{2}(\dot{\varphi_1}+\dot{\varphi_2})\sin\theta  \\ -\dot{y}  &=  \frac{R}{2}(\dot{\varphi_1}+\dot{\varphi_2})\cos\theta  \\ \theta-\theta_0   &=  \frac{R}{2L}(\varphi_1-\varphi_2) \tag{3} \end{align*} My final aim is to find the generalized coordinates $\varphi_1,\varphi_2,\varphi_3,\theta,x,y$ as a function of time. So I need another three scalar equations in terms of generalized coordinates and their derivatives other than the three constraint equations in $(3)$. I think that we should obtain them by manipulating equations in $(1)$; however, I am  unable to obtain those. Any hint or help for finding them is appreciated. :) Notice All of the quantities $\ddot{{\bf{r}}}_{C_1},\ddot{{\bf{r}}}_{C_2},\ddot{{\bf{r}}}_{C_3},\dot{\bf{H}}_{1},\dot{\bf{H}}_{2},\dot{\bf{H}}_{3},{}_{C_3}{\bf{r}}_{C_1},{}_{C_1}{\bf{r}}_{P_1}$ depend on the generalized coordinates $\varphi_1,\varphi_2,\varphi_3,\theta,x,y$ and their first and second time derivatives. The system is linear with respect to the constraint forces ${\bf{X}}_{P_1},{\bf{X}}_{P_2},{\bf{X}}_{C_1},{\bf{X}}_{C_2},{\bf{Y}}_{C_1},{\bf{Y}}_{C_2}$. ${}_{P_1}{\bf{r}}_{C_1}={}_{P_2}{\bf{r}}_{C_2}=R{\bf{a}}_3$ and $-{}_{C_3}{\bf{r}}_{C_1}={}_{C_3}{\bf{r}}_{C_2}=L{\bf{b}}_1$. That would be nice if we could obtain a vectorial equation without choosing a basis. So we may think of elimiating constraint forces from equations in $(1)$. As an attemp I summed the first three and second three equations in $(1)$ to obtain \begin{align*} {\bf{X}}_{P_1}+{\bf{X}}_{P_2}+(M+2m){\bf{g}}  &=  (M+2m)\ddot{{\bf{r}}}_{C_3} \\ {\bf{T}}_{1}+{\bf{T}}_{2}+{}_{C_1}{\bf{r}}_{P_1}\times({\bf{X}}_{P_1}+{\bf{X}}_{P_2})  &=  {}_{C_1}\dot{\bf{H}}_{1}+{}_{C_2}\dot{\bf{H}}_{2}+{}_{C_3}\dot{\bf{H}}_{3} \\ -{}_{C_3}{\bf{r}}_{C_1}\times({\bf{X}}_{C_1}-{\bf{X}}_{C_2}) & \tag{4} \end{align*} $\quad\,\,$ however, I don't know how to proceed further and make ${\bf{X}}_{C_1}-{\bf{X}}_{C_2}$ disappear. Notation $m$ is the mass of each wheel and $M$ is the mass of the shaft. $R$ is the the radius of wheels and $2L$ is the length of the shaft. ${\bf{g}}$: accelaration due to gravity. ${\bf{X}}_{P_i}$: The constraint force applied at point $P_i$ which is exerted by surface to the wheel $i$. ${\bf{X}}_{C_i}$: The constraint force applied at point $C_i$ which is exerted by shaft to the wheel $i$. ${\bf{Y}}_{C_i}$: The constraint torque which is exerted by shaft to the wheel $i$. ${\bf{T}}_{i}$: The external torque applied by the actuator to the wheel $i$. ${\bf{r}}_{P}$: The position vector of point $P$ with respect to origin of frame $a$. ${}_{Q}{\bf{r}}_{P}$: The position vector of point $P$ with respect to piont $Q$. ${}_{P}{\bf{H}}_i$: The angular momentum of part $i$ with respect to point $P$. The overdot notation represents the time derivative with respect to the inertial frame $a$. Figure",,"['linear-algebra', 'vectors', 'systems-of-equations', 'classical-mechanics']"
39,Inverse of almost diagonal matrixes,Inverse of almost diagonal matrixes,,"Consider an $n\times n$ matrix $A$, and it's perturbation matrix $dA$. Let for simplicity $A=I$ be a matrix with ones on the diagonal and zeros off-diagonal. Let $dA$ have zeros on the diagonal and off-diagonal elements in the range $(0,\frac{1}{n})$. Question : Can I argue that the inverse matrix has the off-diagonal elements less than  $\frac{1}{n}$ as well?  Normalize them by the diagonal ones that get perturbed up a little bit. I ran a bunch of simulations on my computer and it seems like this is indeed true. For n=30, the off-diagonal elements hardly reach 30% of my target bound. For n=100, it hardly exceeds 25% of it. The best theory I could find tells the following. Pick a 1- or $\infty$ norm of the matrix, then $$\frac{||(A+dA)^{-1}-A^{-1}||}{||A^{-1}||} \leq K(A) \cdot  \frac{||dA||}{||A||}, \quad K(A)=||A||\cdot ||A^{-1}||$$ What this means is that if the original matrix $A+dA$ was diagonally dominant, then the inverse $(A+dA)^{-1}$ is diagonally dominant as well, because the conditioning number $K(A)$ is equal to 1. The inequality holds for any sub-multiplicative norm and the problem is that the max-norm is not sub-multiplicative, or I would have got my result immediately. I would really appreciate if somebody knows a way to show this or at least some clue. Thanks!","Consider an $n\times n$ matrix $A$, and it's perturbation matrix $dA$. Let for simplicity $A=I$ be a matrix with ones on the diagonal and zeros off-diagonal. Let $dA$ have zeros on the diagonal and off-diagonal elements in the range $(0,\frac{1}{n})$. Question : Can I argue that the inverse matrix has the off-diagonal elements less than  $\frac{1}{n}$ as well?  Normalize them by the diagonal ones that get perturbed up a little bit. I ran a bunch of simulations on my computer and it seems like this is indeed true. For n=30, the off-diagonal elements hardly reach 30% of my target bound. For n=100, it hardly exceeds 25% of it. The best theory I could find tells the following. Pick a 1- or $\infty$ norm of the matrix, then $$\frac{||(A+dA)^{-1}-A^{-1}||}{||A^{-1}||} \leq K(A) \cdot  \frac{||dA||}{||A||}, \quad K(A)=||A||\cdot ||A^{-1}||$$ What this means is that if the original matrix $A+dA$ was diagonally dominant, then the inverse $(A+dA)^{-1}$ is diagonally dominant as well, because the conditioning number $K(A)$ is equal to 1. The inequality holds for any sub-multiplicative norm and the problem is that the max-norm is not sub-multiplicative, or I would have got my result immediately. I would really appreciate if somebody knows a way to show this or at least some clue. Thanks!",,"['linear-algebra', 'matrices', 'perturbation-theory']"
40,Generalization of Liouville's formula to other coefficients of the characteristic polynomial,Generalization of Liouville's formula to other coefficients of the characteristic polynomial,,"If $X(t)$ is an $n \times n$ matrix solving linear homogeneous ODE $$ \frac{d}{dt} X(t) = A(t)X(t), $$ then for $\det X(t)$ we have Liouville's formula: $$ \frac{d}{dt} \det X(t) = \text{tr} A(t) \det X(t). $$ The determinant $\det X(t)$ is the constant term of the characteristic polynomial $\det(X(t)-\lambda I_n)$. I want to know differential equations for other coefficients of this polynomial $a_i(t) = \text{tr} \wedge^i X(t)$. Is something like this known? Update: Even in the simplest case $n=2$ it is not clear hot to find ODE for $\text{tr} X(t)$. It is clear that $\frac{d}{dt} \text{tr} X(t)=\text{tr}(A(t)X(t))$, but it is not clear how to proceed from this.","If $X(t)$ is an $n \times n$ matrix solving linear homogeneous ODE $$ \frac{d}{dt} X(t) = A(t)X(t), $$ then for $\det X(t)$ we have Liouville's formula: $$ \frac{d}{dt} \det X(t) = \text{tr} A(t) \det X(t). $$ The determinant $\det X(t)$ is the constant term of the characteristic polynomial $\det(X(t)-\lambda I_n)$. I want to know differential equations for other coefficients of this polynomial $a_i(t) = \text{tr} \wedge^i X(t)$. Is something like this known? Update: Even in the simplest case $n=2$ it is not clear hot to find ODE for $\text{tr} X(t)$. It is clear that $\frac{d}{dt} \text{tr} X(t)=\text{tr}(A(t)X(t))$, but it is not clear how to proceed from this.",,"['linear-algebra', 'ordinary-differential-equations', 'reference-request', 'exterior-algebra']"
41,"Does there exist a $G$-invariant polynomial $f \in \mathbb{C}[x_1, \ldots, x_n]^G$ such that $f(u_1, \ldots, u_n) = 0$ and $f(v_1, \ldots, v_n) = 1$?",Does there exist a -invariant polynomial  such that  and ?,"G f \in \mathbb{C}[x_1, \ldots, x_n]^G f(u_1, \ldots, u_n) = 0 f(v_1, \ldots, v_n) = 1","Let $G$ be a finite subgroup of $\text{GL}(\mathbb{C}^n)$ and let $u = (u_1, \ldots, u_n)$, $v = (v_1, \ldots, v_n) \in \mathbb{C}^n$ be two points which do not belong to the same $G$-orbit in $\mathbb{C}^n$. Question. Does there exist a $G$-invariant polynomial $f \in \mathbb{C}[x_1, \ldots, x_n]^G$ such that $f(u_1, \ldots, u_n) = 0$ and $f(v_1, \ldots, v_n) = 1$?","Let $G$ be a finite subgroup of $\text{GL}(\mathbb{C}^n)$ and let $u = (u_1, \ldots, u_n)$, $v = (v_1, \ldots, v_n) \in \mathbb{C}^n$ be two points which do not belong to the same $G$-orbit in $\mathbb{C}^n$. Question. Does there exist a $G$-invariant polynomial $f \in \mathbb{C}[x_1, \ldots, x_n]^G$ such that $f(u_1, \ldots, u_n) = 0$ and $f(v_1, \ldots, v_n) = 1$?",,"['linear-algebra', 'abstract-algebra']"
42,Solving $Ax = b$ for non-negative $x$ given boolean matrix $A$ and non-negative $b$,Solving  for non-negative  given boolean matrix  and non-negative,Ax = b x A b,"I am trying to solve $Ax = b$ with the following properties: $A$ is a boolean (aka. logical, binary) matrix, i.e., each entry in $A$ is either $0$ or $1$ $A$ is of size $m \times n$ where $m \ll n$ Each entry in $b$ is a non-negative integer Each entry in $x$ should be a non-negative integer It is known that such a $x$ exists I am able to solve it as an integer linear program using standard LP solvers but how to solve it with a matrix based approach? Given the special properties of the problem, I believe there definitely would be some nice matrix based approach. We may also relax $x$ to have non-negative real numbers and/or settle for an approximate solution if getting an exact solution in not easy.","I am trying to solve $Ax = b$ with the following properties: $A$ is a boolean (aka. logical, binary) matrix, i.e., each entry in $A$ is either $0$ or $1$ $A$ is of size $m \times n$ where $m \ll n$ Each entry in $b$ is a non-negative integer Each entry in $x$ should be a non-negative integer It is known that such a $x$ exists I am able to solve it as an integer linear program using standard LP solvers but how to solve it with a matrix based approach? Given the special properties of the problem, I believe there definitely would be some nice matrix based approach. We may also relax $x$ to have non-negative real numbers and/or settle for an approximate solution if getting an exact solution in not easy.",,"['linear-algebra', 'systems-of-equations', 'linear-programming', 'computational-complexity', 'integer-programming']"
43,Properties of $f^{\star}$,Properties of,f^{\star},"Doing some linear algebra exercises I found that: Given $f \in \mathcal{End}(V)$ we define $f^\star$ an endomorphism such that, given $\phi$ a positive scalar product: $\phi(f(x),y)=\phi(x,f^\star(y))$. Let $V$ a finite dimensional vector space, $\dim V=n$. Let $\phi$ a positive scalar product. Let $f \in \mathcal{End}(V)$ such that $$\sum\limits_{\lambda \in Sp(f)} m_\lambda=n,$$ where $m_\lambda$ is the algebraic multiplicity of the eigenvalue $\lambda$. Prove that $$f=f^\star\iff\operatorname{trace}(ff^{\star})=\sum\limits_{\lambda \in Sp(f)} m_{\lambda}|\lambda|^2.$$ I did the $(\Rightarrow)$ using the spectral theorem and the diagonal form of $f$, but I can't handle the $(\Leftarrow)$, any suggestions?","Doing some linear algebra exercises I found that: Given $f \in \mathcal{End}(V)$ we define $f^\star$ an endomorphism such that, given $\phi$ a positive scalar product: $\phi(f(x),y)=\phi(x,f^\star(y))$. Let $V$ a finite dimensional vector space, $\dim V=n$. Let $\phi$ a positive scalar product. Let $f \in \mathcal{End}(V)$ such that $$\sum\limits_{\lambda \in Sp(f)} m_\lambda=n,$$ where $m_\lambda$ is the algebraic multiplicity of the eigenvalue $\lambda$. Prove that $$f=f^\star\iff\operatorname{trace}(ff^{\star})=\sum\limits_{\lambda \in Sp(f)} m_{\lambda}|\lambda|^2.$$ I did the $(\Rightarrow)$ using the spectral theorem and the diagonal form of $f$, but I can't handle the $(\Leftarrow)$, any suggestions?",,['linear-algebra']
44,Geometric interpretation of linear programming dual,Geometric interpretation of linear programming dual,,"Is there a geometric interpretation of the linear programming dual in terms of the primal? I feel like without some sort of intuition of it, I don't truly understand it.","Is there a geometric interpretation of the linear programming dual in terms of the primal? I feel like without some sort of intuition of it, I don't truly understand it.",,"['linear-algebra', 'linear-programming', 'duality-theorems']"
45,Reference needed for Determinant of convex combination of two matrices as a function [closed],Reference needed for Determinant of convex combination of two matrices as a function [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 7 years ago . Improve this question What can one say about the function $(t,A,B) \mapsto \det(tA + (1-t)B)$, with $t \in [0,1]$, $A$, $B$ square matrices, in my case, say, permutational matrices? Where such a function shows up? Hoping for some references.","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 7 years ago . Improve this question What can one say about the function $(t,A,B) \mapsto \det(tA + (1-t)B)$, with $t \in [0,1]$, $A$, $B$ square matrices, in my case, say, permutational matrices? Where such a function shows up? Hoping for some references.",,"['linear-algebra', 'reference-request', 'representation-theory']"
46,How exactly do natural transformations relate to independence of choice of basis?,How exactly do natural transformations relate to independence of choice of basis?,,"Let $V$ be a finite dimensional vector space over some field $K$. When I did not know anything about category theory, I learned about an isomorphism $(V^*)^* \cong V$ that 'does not depend on choice of basis', namely the map that sends a vector $v \in V$ to the element in $(V^*)^*$ that sends a functional $\omega$ to $\omega(v)$. When I learned about category theory, I was told that what this means in a precise sense is that the identity functor is naturally isomorphic to the double dual functor. Now I know exactly what this means, I can intuitively see that a natural isomorphism between two functors means that they 'do the same' in a very strict sense. However, I do not understand intuitively why this natural isomorphism makes precise the notion of the isomorphism $V \cong V^*$ not being dependent of a choice of basis of $V$. In short: what do natural isomorphisms have to do with independence of choice of basis? I am sorry for the vague question, I hope it is legitimate and understandable.","Let $V$ be a finite dimensional vector space over some field $K$. When I did not know anything about category theory, I learned about an isomorphism $(V^*)^* \cong V$ that 'does not depend on choice of basis', namely the map that sends a vector $v \in V$ to the element in $(V^*)^*$ that sends a functional $\omega$ to $\omega(v)$. When I learned about category theory, I was told that what this means in a precise sense is that the identity functor is naturally isomorphic to the double dual functor. Now I know exactly what this means, I can intuitively see that a natural isomorphism between two functors means that they 'do the same' in a very strict sense. However, I do not understand intuitively why this natural isomorphism makes precise the notion of the isomorphism $V \cong V^*$ not being dependent of a choice of basis of $V$. In short: what do natural isomorphisms have to do with independence of choice of basis? I am sorry for the vague question, I hope it is legitimate and understandable.",,"['linear-algebra', 'abstract-algebra', 'category-theory']"
47,Is it Possible to Show that the Determinant of a Symplectic Matrix is 1 Using Induction?,Is it Possible to Show that the Determinant of a Symplectic Matrix is 1 Using Induction?,,"We have for a $2 \times 2$ matrix $A$ that $A$ is symplectic if and only if $\det A =1$. Is there any way to use this fact as the base for an inductive proof of the fact that the determinant of any symplectic matrix is $1$? I tried writing it out in block matrix form but realized quickly that it was not as simple as I had thought, since: $a_{1,2} \dots a_{1,2n}$, and $a_{2,1}, \dots , a_{2n,1}$ don't have to be zero (although maybe this is possible to achieve via some similarity transformation that preserves the symplectic property?) The block matrix formed by deleting the first row and column doesn't have the correct dimensions to even be a symplectic matrix. It is unclear what the $2n-2 \times 2n-2$ matrix would be to use the inductive hypothesis for $n-1$ would be. Even just showing that it is possible to extend the result from $2 \times 2$ symplectic matrices to $4 \times 4$ symplectic matrices seems like it would be an accomplishment (to me).","We have for a $2 \times 2$ matrix $A$ that $A$ is symplectic if and only if $\det A =1$. Is there any way to use this fact as the base for an inductive proof of the fact that the determinant of any symplectic matrix is $1$? I tried writing it out in block matrix form but realized quickly that it was not as simple as I had thought, since: $a_{1,2} \dots a_{1,2n}$, and $a_{2,1}, \dots , a_{2n,1}$ don't have to be zero (although maybe this is possible to achieve via some similarity transformation that preserves the symplectic property?) The block matrix formed by deleting the first row and column doesn't have the correct dimensions to even be a symplectic matrix. It is unclear what the $2n-2 \times 2n-2$ matrix would be to use the inductive hypothesis for $n-1$ would be. Even just showing that it is possible to extend the result from $2 \times 2$ symplectic matrices to $4 \times 4$ symplectic matrices seems like it would be an accomplishment (to me).",,"['linear-algebra', 'symplectic-linear-algebra']"
48,Construct a matrix of polynomials to optimize condition-like score,Construct a matrix of polynomials to optimize condition-like score,,"I'm a physicist currently working on my PhD. Within my studies, my colleagues & I encountered a (strictly mathematical) problem that baffles us (and anyone else we've talked to so far) and is also sort of a showstopper for our studies. We'd like to use this as a forum to ask people with more expertise in linear algebra & numerics than our immediate social circle for suggestions and opinions on this problem. The Problem Consider an expression  \begin{eqnarray*} \mathcal{L} & = & \left(\sum_{g\in G_{1}}g\right)^{2}\cdot\left(\sum_{g\in G_{2}}g\right)^{2} \end{eqnarray*} where $G_{1}$and $G_{2}$ are possibly overlapping sets of parameters $g\in\vec{g}$, where $\vec{g}\in\mathbb{R}^{N}$. After expanding, this expression takes the form \begin{eqnarray*} \mathcal{L} & = & \sum_{i}c_{i}P_{i}\left(\vec{g}\right) \end{eqnarray*} where the $c_{i}$ absorb all the numerical constants arising from the expansion and the $P_{i}$ are polynomials of the type $P_{i}=g_{\alpha}g_{\beta}g_{\gamma}g_{\delta}$ where $\alpha,\beta,\gamma,\delta\in1,\dots,N$. With these polynominals a matrix is constructed  \begin{eqnarray*} M=\left(\begin{matrix}P_{1}\left(\vec{g}_{1}\right) & \cdots & P_{1}\left(\vec{g}_{n}\right)\\ \vdots & \ddots & \vdots\\ P_{n}\left(\vec{g}_{1}\right) & \cdots & P_{n}\left(\vec{g}_{n}\right) \end{matrix}\right) \end{eqnarray*} For a given region $X$, we have to find a set $S$ of vectors $\vec{g}_{i}\in S$ ($i=1,\dots,n$) with $g_{i,j},\ j=1,\dots,N$ such that \begin{eqnarray*}  & \min_{S\in\mathbb{R}^{n\times N}}\left(\left\Vert M^{-1}\right\Vert _{X}\cdot\left\Vert M\right\Vert _{X}\right) \end{eqnarray*} where $\left\Vert \cdot\right\Vert _{X}$ is defined as \begin{eqnarray*} \left\Vert A\right\Vert _{X} & = & \inf\left\{ c>0:\left\Vert Av\right\Vert \leq c\left\Vert v\right\Vert \forall v\in I\left(X\right)\subset\mathbb{R}^{n}\right\}  \end{eqnarray*} where $I\left(X\right)$ is the image of the map $x:\vec{g}\to\left(P_{i}\left(\vec{g}\right)\right)_{i\le n}$. Remark: We have borrowed the definition of the minimized quantity from the condition number. The reason why we don't want to use the condition number directly is that we already know the region $X$ in which we will want to evaluate our method, thus optimizing with respect to this particular region is favorable. If the solution to this problem would become significantly simpler when dropping this additional constraint and using the real condition number instead, that would also be an option. Simple Example For illustration, we have constructed a simple show-case example. For realistic applications, the matrices will be much larger (up to 1600x1600). But even for this simple example, finding a solution is not straight-forward. Consider an expression \begin{eqnarray*} \mathcal{L} & = & g_{1}^{2}\left(g_{2}+g_{3}\right)^{2} \end{eqnarray*} such that $P_{1}\left(\vec{g}\right)=g_{1}^{2}g_{2}^{2}$ and $P_{2}\left(\vec{g}\right)=g_{1}^{2}g_{3}^{2}$ and $P_{3}\left(\vec{g}\right)=g_{1}^{2}g_{2}g_{3}$. Hence,  \begin{eqnarray*} M=\left(\begin{matrix}P_{1}\left(\vec{g}_{1}\right) & P_{1}\left(\vec{g}_{2}\right) & P_{1}\left(\vec{g}_{3}\right)\\ P_{2}\left(\vec{g}_{1}\right) & P_{2}\left(\vec{g}_{2}\right) & P_{2}\left(\vec{g}_{3}\right)\\ P_{3}\left(\vec{g}_{1}\right) & P_{3}\left(\vec{g}_{2}\right) & P_{3}\left(\vec{g}_{3}\right) \end{matrix}\right)=\left(\begin{matrix}g_{1,1}^{2}g_{1,2}^{2} & g_{2,1}^{2}g_{2,2}^{2} & g_{3,1}^{2}g_{3,2}^{2}\\ g_{1,1}^{2}g_{1,3}^{2} & g_{2,1}^{2}g_{2,3}^{2} & g_{3,1}^{2}g_{3,3}^{2}\\ g_{1,1}^{2}g_{1,2}g_{1,3} & g_{2,1}^{2}g_{2,2}g_{2,3} & g_{3,1}^{2}g_{3,2}g_{3,3} \end{matrix}\right) \end{eqnarray*} Now, $S=\left\{ \vec{g}_{1}=\left(g_{1,1},g_{1,2},g_{1,3}\right),\vec{g}_{2}=\left(g_{2,1},g_{2,2},g_{2,3}\right),\vec{g}_{3}=\left(g_{3,1},g_{3,2},g_{3,3}\right)\right\} $. Let us now consider a region $X=\left\{ \left(g_{1},g_{2},g_{3}\right)\in\mathbb{R}^{3}\vert ag_{1}^{2}+bg_{2}^{2}+cg_{3}^{2}\leq1\right\} $ for some fixed values $a,b,c\in\mathbb{R}$. Find $S$ such that \begin{eqnarray*}  & \min_{S\in\mathbb{R}^{n\times N}}\left(\left\Vert M^{-1}\right\Vert _{X}\cdot\left\Vert M\right\Vert _{X}\right) \end{eqnarray*} For the simplest case, consider $a=b=c=1$. Naive Solution Our first attempted solution was a brute-force numeric one, using the gsl minimization library. However, this naive solution has several shortcomings, as it does not converge in acceptable time and is highly inefficient. #include <iostream> #include <fstream> #include <limits> #include <random> #include <Eigen/Dense> #include <Eigen/SVD> #include <gsl/gsl_vector.h> #include <gsl/gsl_multimin.h>  using Eigen::Matrix3d; using Eigen::Vector3d; using Eigen::JacobiSVD;  #define PI 3.1415926  const int nvars = 9; const double shift = 1e-5;  void getMatrix3d(Matrix3d& target, const Vector3d& g1,const Vector3d& g2,const Vector3d& g3){   target(0,0) = g1[0]*g1[0]*g1[1]*g1[1];   target(0,1) = g2[0]*g2[0]*g2[1]*g2[1];   target(0,2) = g3[0]*g3[0]*g3[1]*g3[1];   target(1,0) = g1[0]*g1[0]*g1[2]*g1[2];   target(1,1) = g2[0]*g2[0]*g2[2]*g2[2];   target(1,2) = g3[0]*g3[0]*g3[2]*g3[2];   target(2,0) = g1[0]*g1[0]*g1[1]*g1[2];   target(2,1) = g2[0]*g2[0]*g2[1]*g2[2];   target(2,2) = g3[0]*g3[0]*g3[1]*g3[2]; }   void setVector(Vector3d& v, double r1, double r2, double r3, double phi, double theta){   v[0] = r1*cos(phi)*cos(theta);   v[1] = r2*sin(phi)*cos(theta);   v[2] = r3*sin(theta); }  void print_gsl(const gsl_vector* vec){   for(size_t i=0; i<vec->size; ++i){     std::cout << vec->data[i] << "" , "";   }   std::cout << std::endl; } double abs_gsl(const gsl_vector* vec){   double result;   for(size_t i=0; i<vec->size; ++i){     result += vec->data[i]*vec->data[i];   }   return sqrt(result); }    double condition(const Matrix3d& A){   JacobiSVD<Matrix3d> svd(A);   return svd.singularValues()(0) / svd.singularValues()(svd.singularValues().size()-1); }  double norm(Matrix3d& m){   double x = 0;   static Vector3d myvec;   for(double phi=0; phi<2*PI; phi+=0.02*PI){     for(double theta=0; theta<PI; theta+=0.01*PI){       setVector(myvec,1,1,1,phi,theta);       x = std::max((m * myvec).norm(),x);     }    }   return x; }  double f(const gsl_vector *v, void* /*params*/){   Vector3d x1(gsl_vector_get(v, 0),gsl_vector_get(v, 1),gsl_vector_get(v, 2));   Vector3d x2(gsl_vector_get(v, 3),gsl_vector_get(v, 4),gsl_vector_get(v, 5));   Vector3d x3(gsl_vector_get(v, 6),gsl_vector_get(v, 7),gsl_vector_get(v, 8));   static Matrix3d m;   getMatrix3d(m,x1,x2,x3);   double val = condition(m);   if(val != val) return std::numeric_limits<double>::infinity();   return val; }  void df(const gsl_vector *v, void* /*params*/, gsl_vector *df){   double val = f(v, NULL);   static gsl_vector* v_shift = gsl_vector_alloc(nvars);   for(size_t i=0;i<v->size;++i){     for(size_t j=0;j<v->size;++j){       if(i!=j) gsl_vector_set(v_shift, j, gsl_vector_get(v,j));       else     gsl_vector_set(v_shift, i, gsl_vector_get(v,i)+shift);     }     double thisdf = (val-f(v_shift, NULL))/shift;     gsl_vector_set(df, i, thisdf);   } }  void fdf(const gsl_vector *x, void* /*params*/, double *thisf, gsl_vector *thisdf){   *thisf = f(x, NULL);    df(x, NULL, thisdf); }  int main(){   size_t iter = 0;   int status;    gsl_multimin_function_fdf func;   func.n = nvars;   func.f = &f;   func.df = &df;   func.fdf = &fdf;   func.params = NULL;    /* Starting point */   gsl_vector *x;   x = gsl_vector_alloc(nvars);    gsl_vector_set (x, 0,  1.); // sample1 gHgg   gsl_vector_set (x, 1,  1.); // sample1 gSM   gsl_vector_set (x, 2,  0); // sample1 gBSM    gsl_vector_set (x, 3,  1.); // sample2 gHgg   gsl_vector_set (x, 4,  1.); // sample2 gSM   gsl_vector_set (x, 5,  1.); // sample2 gBSM    gsl_vector_set (x, 6,  1.); // sample3 gHgg   gsl_vector_set (x, 7,  1.); // sample3 gSM   gsl_vector_set (x, 8, -1.); // sample3 gBSM    const gsl_multimin_fdfminimizer_type *T;   T = gsl_multimin_fdfminimizer_steepest_descent;   gsl_multimin_fdfminimizer *s;   s = gsl_multimin_fdfminimizer_alloc(T, nvars);    gsl_multimin_fdfminimizer_set(s, &func, x, 1e20/* step size */, 0.1/* tolerance */);    do{     iter++;     status = gsl_multimin_fdfminimizer_iterate(s);     if(status){       std::cout<<""status ""<<status<<std::endl;       break;     }      status = gsl_multimin_test_gradient(s->gradient, 0.1);    }while (status == GSL_CONTINUE && iter < 1e8);    if (status == GSL_SUCCESS)     printf (""Minimum found at:\n"");   else      printf(""finished with status %d after %d iterations\n"",status,iter);    gsl_multimin_fdfminimizer_free(s);   gsl_vector_free(x);    return 0; } Goal As the ultimate goal is implementing this method in a C++ code environment, a numeric (or even heuristic) solution would be perfectly acceptable. However, given that the number of parameters is extremely large for realistic problems, we do not expect a brute-force approach to be feasible. A closed-form solution would of course be great, but we do not really expect this problem to be sufficiently well-behaved. With this post, we are looking for ideas on how to tackle this problem (or parts of it) using mathematical identities or advanced numerical algorithms we are unaware of.  Any help, including pointers to literature concerned with similar problems, is highly appreciated.","I'm a physicist currently working on my PhD. Within my studies, my colleagues & I encountered a (strictly mathematical) problem that baffles us (and anyone else we've talked to so far) and is also sort of a showstopper for our studies. We'd like to use this as a forum to ask people with more expertise in linear algebra & numerics than our immediate social circle for suggestions and opinions on this problem. The Problem Consider an expression  \begin{eqnarray*} \mathcal{L} & = & \left(\sum_{g\in G_{1}}g\right)^{2}\cdot\left(\sum_{g\in G_{2}}g\right)^{2} \end{eqnarray*} where $G_{1}$and $G_{2}$ are possibly overlapping sets of parameters $g\in\vec{g}$, where $\vec{g}\in\mathbb{R}^{N}$. After expanding, this expression takes the form \begin{eqnarray*} \mathcal{L} & = & \sum_{i}c_{i}P_{i}\left(\vec{g}\right) \end{eqnarray*} where the $c_{i}$ absorb all the numerical constants arising from the expansion and the $P_{i}$ are polynomials of the type $P_{i}=g_{\alpha}g_{\beta}g_{\gamma}g_{\delta}$ where $\alpha,\beta,\gamma,\delta\in1,\dots,N$. With these polynominals a matrix is constructed  \begin{eqnarray*} M=\left(\begin{matrix}P_{1}\left(\vec{g}_{1}\right) & \cdots & P_{1}\left(\vec{g}_{n}\right)\\ \vdots & \ddots & \vdots\\ P_{n}\left(\vec{g}_{1}\right) & \cdots & P_{n}\left(\vec{g}_{n}\right) \end{matrix}\right) \end{eqnarray*} For a given region $X$, we have to find a set $S$ of vectors $\vec{g}_{i}\in S$ ($i=1,\dots,n$) with $g_{i,j},\ j=1,\dots,N$ such that \begin{eqnarray*}  & \min_{S\in\mathbb{R}^{n\times N}}\left(\left\Vert M^{-1}\right\Vert _{X}\cdot\left\Vert M\right\Vert _{X}\right) \end{eqnarray*} where $\left\Vert \cdot\right\Vert _{X}$ is defined as \begin{eqnarray*} \left\Vert A\right\Vert _{X} & = & \inf\left\{ c>0:\left\Vert Av\right\Vert \leq c\left\Vert v\right\Vert \forall v\in I\left(X\right)\subset\mathbb{R}^{n}\right\}  \end{eqnarray*} where $I\left(X\right)$ is the image of the map $x:\vec{g}\to\left(P_{i}\left(\vec{g}\right)\right)_{i\le n}$. Remark: We have borrowed the definition of the minimized quantity from the condition number. The reason why we don't want to use the condition number directly is that we already know the region $X$ in which we will want to evaluate our method, thus optimizing with respect to this particular region is favorable. If the solution to this problem would become significantly simpler when dropping this additional constraint and using the real condition number instead, that would also be an option. Simple Example For illustration, we have constructed a simple show-case example. For realistic applications, the matrices will be much larger (up to 1600x1600). But even for this simple example, finding a solution is not straight-forward. Consider an expression \begin{eqnarray*} \mathcal{L} & = & g_{1}^{2}\left(g_{2}+g_{3}\right)^{2} \end{eqnarray*} such that $P_{1}\left(\vec{g}\right)=g_{1}^{2}g_{2}^{2}$ and $P_{2}\left(\vec{g}\right)=g_{1}^{2}g_{3}^{2}$ and $P_{3}\left(\vec{g}\right)=g_{1}^{2}g_{2}g_{3}$. Hence,  \begin{eqnarray*} M=\left(\begin{matrix}P_{1}\left(\vec{g}_{1}\right) & P_{1}\left(\vec{g}_{2}\right) & P_{1}\left(\vec{g}_{3}\right)\\ P_{2}\left(\vec{g}_{1}\right) & P_{2}\left(\vec{g}_{2}\right) & P_{2}\left(\vec{g}_{3}\right)\\ P_{3}\left(\vec{g}_{1}\right) & P_{3}\left(\vec{g}_{2}\right) & P_{3}\left(\vec{g}_{3}\right) \end{matrix}\right)=\left(\begin{matrix}g_{1,1}^{2}g_{1,2}^{2} & g_{2,1}^{2}g_{2,2}^{2} & g_{3,1}^{2}g_{3,2}^{2}\\ g_{1,1}^{2}g_{1,3}^{2} & g_{2,1}^{2}g_{2,3}^{2} & g_{3,1}^{2}g_{3,3}^{2}\\ g_{1,1}^{2}g_{1,2}g_{1,3} & g_{2,1}^{2}g_{2,2}g_{2,3} & g_{3,1}^{2}g_{3,2}g_{3,3} \end{matrix}\right) \end{eqnarray*} Now, $S=\left\{ \vec{g}_{1}=\left(g_{1,1},g_{1,2},g_{1,3}\right),\vec{g}_{2}=\left(g_{2,1},g_{2,2},g_{2,3}\right),\vec{g}_{3}=\left(g_{3,1},g_{3,2},g_{3,3}\right)\right\} $. Let us now consider a region $X=\left\{ \left(g_{1},g_{2},g_{3}\right)\in\mathbb{R}^{3}\vert ag_{1}^{2}+bg_{2}^{2}+cg_{3}^{2}\leq1\right\} $ for some fixed values $a,b,c\in\mathbb{R}$. Find $S$ such that \begin{eqnarray*}  & \min_{S\in\mathbb{R}^{n\times N}}\left(\left\Vert M^{-1}\right\Vert _{X}\cdot\left\Vert M\right\Vert _{X}\right) \end{eqnarray*} For the simplest case, consider $a=b=c=1$. Naive Solution Our first attempted solution was a brute-force numeric one, using the gsl minimization library. However, this naive solution has several shortcomings, as it does not converge in acceptable time and is highly inefficient. #include <iostream> #include <fstream> #include <limits> #include <random> #include <Eigen/Dense> #include <Eigen/SVD> #include <gsl/gsl_vector.h> #include <gsl/gsl_multimin.h>  using Eigen::Matrix3d; using Eigen::Vector3d; using Eigen::JacobiSVD;  #define PI 3.1415926  const int nvars = 9; const double shift = 1e-5;  void getMatrix3d(Matrix3d& target, const Vector3d& g1,const Vector3d& g2,const Vector3d& g3){   target(0,0) = g1[0]*g1[0]*g1[1]*g1[1];   target(0,1) = g2[0]*g2[0]*g2[1]*g2[1];   target(0,2) = g3[0]*g3[0]*g3[1]*g3[1];   target(1,0) = g1[0]*g1[0]*g1[2]*g1[2];   target(1,1) = g2[0]*g2[0]*g2[2]*g2[2];   target(1,2) = g3[0]*g3[0]*g3[2]*g3[2];   target(2,0) = g1[0]*g1[0]*g1[1]*g1[2];   target(2,1) = g2[0]*g2[0]*g2[1]*g2[2];   target(2,2) = g3[0]*g3[0]*g3[1]*g3[2]; }   void setVector(Vector3d& v, double r1, double r2, double r3, double phi, double theta){   v[0] = r1*cos(phi)*cos(theta);   v[1] = r2*sin(phi)*cos(theta);   v[2] = r3*sin(theta); }  void print_gsl(const gsl_vector* vec){   for(size_t i=0; i<vec->size; ++i){     std::cout << vec->data[i] << "" , "";   }   std::cout << std::endl; } double abs_gsl(const gsl_vector* vec){   double result;   for(size_t i=0; i<vec->size; ++i){     result += vec->data[i]*vec->data[i];   }   return sqrt(result); }    double condition(const Matrix3d& A){   JacobiSVD<Matrix3d> svd(A);   return svd.singularValues()(0) / svd.singularValues()(svd.singularValues().size()-1); }  double norm(Matrix3d& m){   double x = 0;   static Vector3d myvec;   for(double phi=0; phi<2*PI; phi+=0.02*PI){     for(double theta=0; theta<PI; theta+=0.01*PI){       setVector(myvec,1,1,1,phi,theta);       x = std::max((m * myvec).norm(),x);     }    }   return x; }  double f(const gsl_vector *v, void* /*params*/){   Vector3d x1(gsl_vector_get(v, 0),gsl_vector_get(v, 1),gsl_vector_get(v, 2));   Vector3d x2(gsl_vector_get(v, 3),gsl_vector_get(v, 4),gsl_vector_get(v, 5));   Vector3d x3(gsl_vector_get(v, 6),gsl_vector_get(v, 7),gsl_vector_get(v, 8));   static Matrix3d m;   getMatrix3d(m,x1,x2,x3);   double val = condition(m);   if(val != val) return std::numeric_limits<double>::infinity();   return val; }  void df(const gsl_vector *v, void* /*params*/, gsl_vector *df){   double val = f(v, NULL);   static gsl_vector* v_shift = gsl_vector_alloc(nvars);   for(size_t i=0;i<v->size;++i){     for(size_t j=0;j<v->size;++j){       if(i!=j) gsl_vector_set(v_shift, j, gsl_vector_get(v,j));       else     gsl_vector_set(v_shift, i, gsl_vector_get(v,i)+shift);     }     double thisdf = (val-f(v_shift, NULL))/shift;     gsl_vector_set(df, i, thisdf);   } }  void fdf(const gsl_vector *x, void* /*params*/, double *thisf, gsl_vector *thisdf){   *thisf = f(x, NULL);    df(x, NULL, thisdf); }  int main(){   size_t iter = 0;   int status;    gsl_multimin_function_fdf func;   func.n = nvars;   func.f = &f;   func.df = &df;   func.fdf = &fdf;   func.params = NULL;    /* Starting point */   gsl_vector *x;   x = gsl_vector_alloc(nvars);    gsl_vector_set (x, 0,  1.); // sample1 gHgg   gsl_vector_set (x, 1,  1.); // sample1 gSM   gsl_vector_set (x, 2,  0); // sample1 gBSM    gsl_vector_set (x, 3,  1.); // sample2 gHgg   gsl_vector_set (x, 4,  1.); // sample2 gSM   gsl_vector_set (x, 5,  1.); // sample2 gBSM    gsl_vector_set (x, 6,  1.); // sample3 gHgg   gsl_vector_set (x, 7,  1.); // sample3 gSM   gsl_vector_set (x, 8, -1.); // sample3 gBSM    const gsl_multimin_fdfminimizer_type *T;   T = gsl_multimin_fdfminimizer_steepest_descent;   gsl_multimin_fdfminimizer *s;   s = gsl_multimin_fdfminimizer_alloc(T, nvars);    gsl_multimin_fdfminimizer_set(s, &func, x, 1e20/* step size */, 0.1/* tolerance */);    do{     iter++;     status = gsl_multimin_fdfminimizer_iterate(s);     if(status){       std::cout<<""status ""<<status<<std::endl;       break;     }      status = gsl_multimin_test_gradient(s->gradient, 0.1);    }while (status == GSL_CONTINUE && iter < 1e8);    if (status == GSL_SUCCESS)     printf (""Minimum found at:\n"");   else      printf(""finished with status %d after %d iterations\n"",status,iter);    gsl_multimin_fdfminimizer_free(s);   gsl_vector_free(x);    return 0; } Goal As the ultimate goal is implementing this method in a C++ code environment, a numeric (or even heuristic) solution would be perfectly acceptable. However, given that the number of parameters is extremely large for realistic problems, we do not expect a brute-force approach to be feasible. A closed-form solution would of course be great, but we do not really expect this problem to be sufficiently well-behaved. With this post, we are looking for ideas on how to tackle this problem (or parts of it) using mathematical identities or advanced numerical algorithms we are unaware of.  Any help, including pointers to literature concerned with similar problems, is highly appreciated.",,"['linear-algebra', 'matrices', 'optimization']"
49,Find distance to a set (subspace) without computing closest point,Find distance to a set (subspace) without computing closest point,,"General setup: we have a finite-dimensional normed linear space $(V, \| \cdot \|)$, a subspace $U \subset V$, and a fixed vector $v_0 \in V$. We want to find the distance between $v_0$ and $U$. (No cheating: $\| \cdot \|$ is not induced by an inner product!) We don't care about finding the closest $u^* \in U$ to $v$ ; this would of course solve the problem, but let's assume it's computationally infeasible. All we care about is finding the number $d_0 = \min_{u \in U} \|u - v_0\|$ (or a reasonable substitute). It's entirely possible that finding $d_0$ is equivalent to finding $u^*$, and this would be an acceptable answer to my question, but maybe there's some clever way to find $d_0$ without $u^*$. Examples: (This is really a non-example.) If $V = \mathbb R^n$ (or some other Hilbert space), then we may use orthogonal projections, and everything's bunnies and butterflies. Great. If $V = \mathbb F_2^n$ with the Hamming norm $\|v\| = \#\{i \in [n] : v_i = 1\}$, then $U$ is a binary linear code. Finding $u^*$ is the hard-decision decoding problem, which is in general NP-complete. (There may be some special cases of $U$ that make things easier, but we cannot assume we're in such a case.) Then, I suppose what we're after is some way of ""fingerprinting"" elements of $V$ to see if they're near $U$. Fingerprinting $v_0$ will lead to something like an answer. 2 is the case that I really have in mind, but I'd be happy to hear anything relevant: a particular $(V, \| \cdot \|)$ where this does work, for example, or a relaxation of some conditions (maybe $V$ is just a metric space and $U$ is just a subset) which admits an affirmative example.","General setup: we have a finite-dimensional normed linear space $(V, \| \cdot \|)$, a subspace $U \subset V$, and a fixed vector $v_0 \in V$. We want to find the distance between $v_0$ and $U$. (No cheating: $\| \cdot \|$ is not induced by an inner product!) We don't care about finding the closest $u^* \in U$ to $v$ ; this would of course solve the problem, but let's assume it's computationally infeasible. All we care about is finding the number $d_0 = \min_{u \in U} \|u - v_0\|$ (or a reasonable substitute). It's entirely possible that finding $d_0$ is equivalent to finding $u^*$, and this would be an acceptable answer to my question, but maybe there's some clever way to find $d_0$ without $u^*$. Examples: (This is really a non-example.) If $V = \mathbb R^n$ (or some other Hilbert space), then we may use orthogonal projections, and everything's bunnies and butterflies. Great. If $V = \mathbb F_2^n$ with the Hamming norm $\|v\| = \#\{i \in [n] : v_i = 1\}$, then $U$ is a binary linear code. Finding $u^*$ is the hard-decision decoding problem, which is in general NP-complete. (There may be some special cases of $U$ that make things easier, but we cannot assume we're in such a case.) Then, I suppose what we're after is some way of ""fingerprinting"" elements of $V$ to see if they're near $U$. Fingerprinting $v_0$ will lead to something like an answer. 2 is the case that I really have in mind, but I'd be happy to hear anything relevant: a particular $(V, \| \cdot \|)$ where this does work, for example, or a relaxation of some conditions (maybe $V$ is just a metric space and $U$ is just a subset) which admits an affirmative example.",,"['linear-algebra', 'optimization', 'metric-spaces', 'normed-spaces', 'coding-theory']"
50,What is Complex Analysis? Why is it accompanied by Linear Algebra?,What is Complex Analysis? Why is it accompanied by Linear Algebra?,,"I hope this doesn't extend to a lengthy question. I studied Linear Algebra recently in my first term at university. I came to the realization however that some institutions would teach that course during the second year while including what's known as ""Complex Analysis"". I've tried looking it up and even ask about it -- and I am yet to understand how useful is that, specifically for Engineers. Why would someone teach complex analysis alongside linear algebra, and how does it ""fulfill"" the Linear Algebra course. While I did take a brief introduction to complex numbers and whatnot, I can't seem to understand how crucial complex analysis would have been -- in fact, the only thing that stood out to me is the Cauchy-Integrals and some Methods of Contour Integrals. Keep in mind that I do expect I will encounter some applications of complex analysis further down the road, but all I need to know is whether it's worth dedicating portion of the course to. Thanks","I hope this doesn't extend to a lengthy question. I studied Linear Algebra recently in my first term at university. I came to the realization however that some institutions would teach that course during the second year while including what's known as ""Complex Analysis"". I've tried looking it up and even ask about it -- and I am yet to understand how useful is that, specifically for Engineers. Why would someone teach complex analysis alongside linear algebra, and how does it ""fulfill"" the Linear Algebra course. While I did take a brief introduction to complex numbers and whatnot, I can't seem to understand how crucial complex analysis would have been -- in fact, the only thing that stood out to me is the Cauchy-Integrals and some Methods of Contour Integrals. Keep in mind that I do expect I will encounter some applications of complex analysis further down the road, but all I need to know is whether it's worth dedicating portion of the course to. Thanks",,"['linear-algebra', 'complex-analysis', 'contour-integration', 'cauchy-integral-formula']"
51,"Known Results on eigenvalues of $A$, $B$, $A+B$?","Known Results on eigenvalues of , , ?",A B A+B,"Consider the matrices $A$, $B$ and $A+B$. What are the best known results on relations between eigenvalues of these matrices? Please provide a reference.","Consider the matrices $A$, $B$ and $A+B$. What are the best known results on relations between eigenvalues of these matrices? Please provide a reference.",,"['linear-algebra', 'matrices']"
52,Does the sum of weights in Kirchhoff’s construction equal the Gram determinant?,Does the sum of weights in Kirchhoff’s construction equal the Gram determinant?,,"Background: An electrical network is modeled by a complex. Branch current distributions $\mathbf I\in C_1$ are represented by $1$-chains; branch voltage drop distributions $\mathbf V\in C^1$ are $1$-cochains; $\dim C_1=\dim C^1=b$, the number of branches. We define $Z_1=\ker\operatorname{\partial}$ (where $\operatorname{\partial}:C_1\to C_0$ is the boundary operator) as the subspace of cycles; $\dim Z_1=m$, the number of independent meshes in the network. Branch current distributions that satisfy Kirchhoff’s current law, $\operatorname{\partial}\mathbf I=0$ are thus elements of $Z_1$. We establish a basis for $Z_1$ that consists of linearly independent meshes by selecting a maximal tree $T$ for the network. For each branch $\beta\notin T$, there is a unique mesh $\mathbf M_\beta$ that is obtained by adjoining $\beta$ to the maximal tree $T$. Remember that a mesh has coefficients in $\{-1, 0, +1\}$. The orientation of $\mathbf M_\beta$ is chosen so that the coefficient of $\beta$ is $+1$. Then, we define the space $H_1$ as $Z_1$ considered as a vector space in its own right instead of as a subspace of $C_1$. The map $\sigma:H_1\to C_1$ identifies this space with $Z_1$, e.g., if the mesh $\mathbf M\in H_1$ consists of the branch currents $\alpha+\beta-\gamma\in C_1$, then $\sigma(\mathbf M)=\alpha+\beta-\gamma$. The matrix of $\sigma$ relative to this basis has these meshes as its columns. Note that there are $m$ such meshes for a given tree $T$, hence also $m$ non-tree branches. We are given the vector of branch current sources $\mathbf K\in C_1$, voltage sources $\mathbf W\in C^1$ and the map $Z:C_1\to C^1$ that relates branch currents to branch voltages. The matrix of $Z$ relative to the standard bases is the diagonal matrix of branch impedances, which for a resistive network are resistances—positive reals. We must then find $\mathbf I\in C_1$ and $\mathbf V\in C^1$ such that $$ \mathbf V-\mathbf W = Z(\mathbf I-\mathbf K) $$ and Kirchhoff’s laws $$ \operatorname{\partial}\mathbf I=0 \\ \mathbf V=-\operatorname{d}\mathbf\Phi $$ are satisfied. (I haven’t defined the zero-cochain $\mathbf\Phi$ or the coboundary operator $\operatorname{d}$ here, but I don’t think they’re directly relevant to my problem.) One way to solve a resistive network is due to Weyl: We can use $Z$ to define a scalar product $\langle\mathbf I,\mathbf I'\rangle_Z = \sum_\alpha z_\alpha I_\alpha I'_\alpha$. If $\pi:C_1\to Z_1$ is the orthogonal projection operator relative to this scalar product, then $\mathbf I=\pi(\mathbf K-Z^{-1}\mathbf W)$ and $\mathbf V=Z(1-\pi)(\mathbf K-Z^{-1}\mathbf W)$ are the solution. Comparing this to the solution obtained via the mesh-current method provides an explicit expression for this operator in terms of $Z$ and $\sigma$:$$ \pi = \sigma(\sigma^*Z\sigma)^{-1}\sigma^*Z. \tag{*} $$ Kirchhoff’s construction of this orthogonal projection uses a weighted average of a set of (possibly) non-orthogonal projections as follows. Each maximal tree $T$ has an associated projection $\rho_T: C_1\to Z_1$ defined by: $$ \rho_T(\beta) = \begin{cases}  0, &\text{if }\beta\in T \\   \mathbf M_\beta, &\text{if }\beta\notin T,  \end{cases} $$ where $\mathbf M_\beta$ is the mesh associated with the non-tree branch $\beta$. $\sum _T\lambda_T\rho_T$, with $0\le\lambda_T\le1$ and $\sum_T\lambda_T=1$ and summed over all maximal trees, is also a projection of $C_1$ onto $Z_1$. By taking $$ \lambda_T = \frac{Q_T}R \\ Q_T = \prod_{\beta\notin T}z_\beta \\ R = \sum_TQ_T, $$ the resulting projection is orthogonal relative to $\langle\cdot,\cdot\rangle_Z$. The Problem: From solving various networks as exercises, it looks to me like $R=\det(\sigma^*Z\sigma)$, the Gram determinant for the chosen basis of $Z_1$ relative to $\langle\cdot,\cdot\rangle_Z$. Since the entries of the matrices for $\sigma$ and $\sigma^*$ are all integers, the only source of non-integer factors in $(*)$ is $Z$. Similarly, $Z$, and hence $\frac1R$ is the only source of non-integer factors in Kirchhoff’s sum. Setting all resistances to integer values makes it obvious that $R\; | \det(\sigma^*Z\sigma)$ or $\det(\sigma^*Z\sigma)\;|\;R$, but I haven’t been able to come up with a proof of equality. I tried expanding the determinant by cofactors and comparing to the the coefficients in Kirchhoff’s sum, but couldn’t get anywhere with that. Perhaps if I had a better idea of how those coefficients were determined, that might do the trick. Hints and suggestions are welcome. Update: The main diagonal elements of $\sigma^*Z\sigma$ are $\sum_\beta z_\beta[\beta\in\mathbf M_\alpha]$ ($[\cdot]$ are Iverson brackets), i.e., the total resistances in each basis mesh, while the off-diagonal elements are $\pm\sum_\gamma z_\gamma[\gamma\in\mathbf M_\alpha][\gamma\in\mathbf M_\beta]$, i.e., the total common resistances of each pair of basis meshes, negative if the meshes have opposite orientation. Using linear combinations of rows/columns of this matrix to isolate the non-tree resistances looks promising.","Background: An electrical network is modeled by a complex. Branch current distributions $\mathbf I\in C_1$ are represented by $1$-chains; branch voltage drop distributions $\mathbf V\in C^1$ are $1$-cochains; $\dim C_1=\dim C^1=b$, the number of branches. We define $Z_1=\ker\operatorname{\partial}$ (where $\operatorname{\partial}:C_1\to C_0$ is the boundary operator) as the subspace of cycles; $\dim Z_1=m$, the number of independent meshes in the network. Branch current distributions that satisfy Kirchhoff’s current law, $\operatorname{\partial}\mathbf I=0$ are thus elements of $Z_1$. We establish a basis for $Z_1$ that consists of linearly independent meshes by selecting a maximal tree $T$ for the network. For each branch $\beta\notin T$, there is a unique mesh $\mathbf M_\beta$ that is obtained by adjoining $\beta$ to the maximal tree $T$. Remember that a mesh has coefficients in $\{-1, 0, +1\}$. The orientation of $\mathbf M_\beta$ is chosen so that the coefficient of $\beta$ is $+1$. Then, we define the space $H_1$ as $Z_1$ considered as a vector space in its own right instead of as a subspace of $C_1$. The map $\sigma:H_1\to C_1$ identifies this space with $Z_1$, e.g., if the mesh $\mathbf M\in H_1$ consists of the branch currents $\alpha+\beta-\gamma\in C_1$, then $\sigma(\mathbf M)=\alpha+\beta-\gamma$. The matrix of $\sigma$ relative to this basis has these meshes as its columns. Note that there are $m$ such meshes for a given tree $T$, hence also $m$ non-tree branches. We are given the vector of branch current sources $\mathbf K\in C_1$, voltage sources $\mathbf W\in C^1$ and the map $Z:C_1\to C^1$ that relates branch currents to branch voltages. The matrix of $Z$ relative to the standard bases is the diagonal matrix of branch impedances, which for a resistive network are resistances—positive reals. We must then find $\mathbf I\in C_1$ and $\mathbf V\in C^1$ such that $$ \mathbf V-\mathbf W = Z(\mathbf I-\mathbf K) $$ and Kirchhoff’s laws $$ \operatorname{\partial}\mathbf I=0 \\ \mathbf V=-\operatorname{d}\mathbf\Phi $$ are satisfied. (I haven’t defined the zero-cochain $\mathbf\Phi$ or the coboundary operator $\operatorname{d}$ here, but I don’t think they’re directly relevant to my problem.) One way to solve a resistive network is due to Weyl: We can use $Z$ to define a scalar product $\langle\mathbf I,\mathbf I'\rangle_Z = \sum_\alpha z_\alpha I_\alpha I'_\alpha$. If $\pi:C_1\to Z_1$ is the orthogonal projection operator relative to this scalar product, then $\mathbf I=\pi(\mathbf K-Z^{-1}\mathbf W)$ and $\mathbf V=Z(1-\pi)(\mathbf K-Z^{-1}\mathbf W)$ are the solution. Comparing this to the solution obtained via the mesh-current method provides an explicit expression for this operator in terms of $Z$ and $\sigma$:$$ \pi = \sigma(\sigma^*Z\sigma)^{-1}\sigma^*Z. \tag{*} $$ Kirchhoff’s construction of this orthogonal projection uses a weighted average of a set of (possibly) non-orthogonal projections as follows. Each maximal tree $T$ has an associated projection $\rho_T: C_1\to Z_1$ defined by: $$ \rho_T(\beta) = \begin{cases}  0, &\text{if }\beta\in T \\   \mathbf M_\beta, &\text{if }\beta\notin T,  \end{cases} $$ where $\mathbf M_\beta$ is the mesh associated with the non-tree branch $\beta$. $\sum _T\lambda_T\rho_T$, with $0\le\lambda_T\le1$ and $\sum_T\lambda_T=1$ and summed over all maximal trees, is also a projection of $C_1$ onto $Z_1$. By taking $$ \lambda_T = \frac{Q_T}R \\ Q_T = \prod_{\beta\notin T}z_\beta \\ R = \sum_TQ_T, $$ the resulting projection is orthogonal relative to $\langle\cdot,\cdot\rangle_Z$. The Problem: From solving various networks as exercises, it looks to me like $R=\det(\sigma^*Z\sigma)$, the Gram determinant for the chosen basis of $Z_1$ relative to $\langle\cdot,\cdot\rangle_Z$. Since the entries of the matrices for $\sigma$ and $\sigma^*$ are all integers, the only source of non-integer factors in $(*)$ is $Z$. Similarly, $Z$, and hence $\frac1R$ is the only source of non-integer factors in Kirchhoff’s sum. Setting all resistances to integer values makes it obvious that $R\; | \det(\sigma^*Z\sigma)$ or $\det(\sigma^*Z\sigma)\;|\;R$, but I haven’t been able to come up with a proof of equality. I tried expanding the determinant by cofactors and comparing to the the coefficients in Kirchhoff’s sum, but couldn’t get anywhere with that. Perhaps if I had a better idea of how those coefficients were determined, that might do the trick. Hints and suggestions are welcome. Update: The main diagonal elements of $\sigma^*Z\sigma$ are $\sum_\beta z_\beta[\beta\in\mathbf M_\alpha]$ ($[\cdot]$ are Iverson brackets), i.e., the total resistances in each basis mesh, while the off-diagonal elements are $\pm\sum_\gamma z_\gamma[\gamma\in\mathbf M_\alpha][\gamma\in\mathbf M_\beta]$, i.e., the total common resistances of each pair of basis meshes, negative if the meshes have opposite orientation. Using linear combinations of rows/columns of this matrix to isolate the non-tree resistances looks promising.",,"['linear-algebra', 'graph-theory', 'algebraic-topology', 'determinant', 'linear-transformations']"
53,$A \in M_3(\mathbb Z)$ be such that $\det(A)=1$ ; then what is the maximum possible number of entries of $A$ that are even ?,be such that  ; then what is the maximum possible number of entries of  that are even ?,A \in M_3(\mathbb Z) \det(A)=1 A,Let $A \in M_3(\mathbb Z)$ be such that $\det(A)=1$ ; then what is the maximum possible number of entries of $A$ that are even ?,Let $A \in M_3(\mathbb Z)$ be such that $\det(A)=1$ ; then what is the maximum possible number of entries of $A$ that are even ?,,"['linear-algebra', 'matrices']"
54,Maximizing the pairwise Frobenuis distance between M othrogonal matrices,Maximizing the pairwise Frobenuis distance between M othrogonal matrices,,"I want to maximize the pairwise Frobenius distance between $M$ orthogonal matrices. That is, I'm looking for $Q_{i}, i = 1, 2, ... M$ such that \begin{equation*} \begin{aligned} & \underset{ 1 \leq i, j \leq M}{\text{min}} & & || Q_{i} - Q_{j} ||^2_F \end{aligned} \end{equation*} is maximized. I know there are numerical algorithms that can provide the answer to that. I'm just wondering if the solution has a simple structure. For example, if the solution can be parameterized by a single variable $t$ (and possibly some orthogonal matrix $A$). Edit: It would also be nice if I someone could lead me to directions on how to bound that.","I want to maximize the pairwise Frobenius distance between $M$ orthogonal matrices. That is, I'm looking for $Q_{i}, i = 1, 2, ... M$ such that \begin{equation*} \begin{aligned} & \underset{ 1 \leq i, j \leq M}{\text{min}} & & || Q_{i} - Q_{j} ||^2_F \end{aligned} \end{equation*} is maximized. I know there are numerical algorithms that can provide the answer to that. I'm just wondering if the solution has a simple structure. For example, if the solution can be parameterized by a single variable $t$ (and possibly some orthogonal matrix $A$). Edit: It would also be nice if I someone could lead me to directions on how to bound that.",,"['linear-algebra', 'differential-geometry', 'manifolds', 'lie-groups', 'numerical-linear-algebra']"
55,Is it possible to define an inner product to an arbitrary field?,Is it possible to define an inner product to an arbitrary field?,,"I've been trying to find the most general definition of an inner product space. Every definition I've found is either to $\mathbb{R}$ or to $\mathbb{C}$. Is it possible to define an inner product to an arbitrary field? In other words, is there a more general way of formulating the conjugate symmetry property? EDIT: It would have to be an ordered field to have positive definiteness make sense, I guess.","I've been trying to find the most general definition of an inner product space. Every definition I've found is either to $\mathbb{R}$ or to $\mathbb{C}$. Is it possible to define an inner product to an arbitrary field? In other words, is there a more general way of formulating the conjugate symmetry property? EDIT: It would have to be an ordered field to have positive definiteness make sense, I guess.",,"['linear-algebra', 'inner-products']"
56,"Understanding of the proof of ""Cayley-Hamilton thm""","Understanding of the proof of ""Cayley-Hamilton thm""",,"I'm reading the proof of Cayley-Hamilton theorem and I got stuck on the proof. Let $T$ be a linear operator on a finite dimensional vector space $V$. If $f$ is the characteristic polynomial for $T$, then $f(T)=0$; in other words, the minimal polynomial divides the characteristic polynomial for $T$. (proof) Let $K$ be the commutative ring with identity consisting of all polynomials in $T$. Choose an ordered basis $\{\alpha_1, \ldots, \alpha_n\}$ for $V$, and let $A$ be the matrix which represents $T$ in the given basis. Then     $$T\alpha_i = \sum_{j=1}^{n} A_{ji} \alpha_j, \quad 1\leq i \leq n.$$     These equations may be written in the equivalent form     $$\sum_{j=1}^{n} (\delta_{ij} T - A_{ji}I) \alpha_j = 0, \quad 1\leq i\leq n$$     Let $B$ denote the element of $K^{n \times n}$ with entries     $$B_{ij} = \delta_{ij}T - A_{ji}I.$$     When $n=2$, $\det B =f(T)$ makes sense. For the case $n>2$, it is also clear that $$\det B = f(T)$$     since $f$ is the determinant of matrix $xI - A$ whose entries are the polynomials     $$(xI - A)_{ij}= \delta_{ij}x - A_{ji}$$ I got stuck on the last part of the above proof. I think it should be $(xI - A)_{ij}= \delta_{ij}x - A_{ij}$ not $(xI - A)_{ij}= \delta_{ij}x - A_{ji}$, and also I don't understand why this verifies $\det B = f(T)$.","I'm reading the proof of Cayley-Hamilton theorem and I got stuck on the proof. Let $T$ be a linear operator on a finite dimensional vector space $V$. If $f$ is the characteristic polynomial for $T$, then $f(T)=0$; in other words, the minimal polynomial divides the characteristic polynomial for $T$. (proof) Let $K$ be the commutative ring with identity consisting of all polynomials in $T$. Choose an ordered basis $\{\alpha_1, \ldots, \alpha_n\}$ for $V$, and let $A$ be the matrix which represents $T$ in the given basis. Then     $$T\alpha_i = \sum_{j=1}^{n} A_{ji} \alpha_j, \quad 1\leq i \leq n.$$     These equations may be written in the equivalent form     $$\sum_{j=1}^{n} (\delta_{ij} T - A_{ji}I) \alpha_j = 0, \quad 1\leq i\leq n$$     Let $B$ denote the element of $K^{n \times n}$ with entries     $$B_{ij} = \delta_{ij}T - A_{ji}I.$$     When $n=2$, $\det B =f(T)$ makes sense. For the case $n>2$, it is also clear that $$\det B = f(T)$$     since $f$ is the determinant of matrix $xI - A$ whose entries are the polynomials     $$(xI - A)_{ij}= \delta_{ij}x - A_{ji}$$ I got stuck on the last part of the above proof. I think it should be $(xI - A)_{ij}= \delta_{ij}x - A_{ij}$ not $(xI - A)_{ij}= \delta_{ij}x - A_{ji}$, and also I don't understand why this verifies $\det B = f(T)$.",,['linear-algebra']
57,"""Hard"" exercises on Linear Algebra and Analytic Geometry","""Hard"" exercises on Linear Algebra and Analytic Geometry",,"I started lecturing this subject called ""Linear Algebra and Analytic Geometry"" and in the second day of class I was approached by an undergrad student, asking for referenced that would contain ""hard"" exercises. I understand that the ones on the book I'm mostly following might seem pretty basic, since they aim for more ""average"" students (by the way, this is in Brazil). I checked on this student's profile and noticed that she has actually received some medals on Brazil's Olympiads of Mathematics for teenagers. I do not want to lose this student to get bored with her first experience with University-level Mathematics (her graduation subject is Computer Science), and since she said she'd have no problem with an english or spanish textbook, I would like to ask my fellow Stackers for textbooks that contain above-average exercises on Linear Algebra and Analytic Geometry. ""Theoretical"" exercises, as in ""show this identity"" or ""proof this result"" are more into what I'm expecting. Thanks in advance.","I started lecturing this subject called ""Linear Algebra and Analytic Geometry"" and in the second day of class I was approached by an undergrad student, asking for referenced that would contain ""hard"" exercises. I understand that the ones on the book I'm mostly following might seem pretty basic, since they aim for more ""average"" students (by the way, this is in Brazil). I checked on this student's profile and noticed that she has actually received some medals on Brazil's Olympiads of Mathematics for teenagers. I do not want to lose this student to get bored with her first experience with University-level Mathematics (her graduation subject is Computer Science), and since she said she'd have no problem with an english or spanish textbook, I would like to ask my fellow Stackers for textbooks that contain above-average exercises on Linear Algebra and Analytic Geometry. ""Theoretical"" exercises, as in ""show this identity"" or ""proof this result"" are more into what I'm expecting. Thanks in advance.",,"['linear-algebra', 'analytic-geometry']"
58,Number of Arithmetic Operations in Gaussian-elimination/Gauss-Jordan Hybrid Method for Solving Linear Systems,Number of Arithmetic Operations in Gaussian-elimination/Gauss-Jordan Hybrid Method for Solving Linear Systems,,"I am stucked at this problem from the book Numerical Analysis 8-th Edition (Burden) (Exercise 6.1.16) : Consider the following Gaussian-elimination/Gauss-Jordan hybrid method for solving linear systems: First apply the Gaussian-elimination technique to reduce the system to triangular form. Then use the $n$-th equation to eliminate the coefficients of $x_{n}$ in each of the first $n-1$ rows. After this is completed use the $(n-1)$-st equation to eliminate the coefficients of $x_{n-1}$ in the first $n-2$ rows, ans so on. The system will eventually appear as the reduced system we get by applying Gauss-Jordan method to the original system. Show that this method requires $\frac{n^3}{3}+\frac{3}{2}n^2-\frac{5}{6}n$ multiplication/divisions and $\frac{n^3}{3}+\frac{n^2}{2}-\frac{5}{6}n$ additions/subtractions. But when I tried to calculate the total number of multiplication/divisions I got $\frac{n^3}{3}+\frac{3}{2}n^2-\frac{11}{6}n$, Here's what I've done (Sorry for being a bit long): The augmented matrix for the system has the form $$ \left[     \begin{array}{cccc|c}         a_{1,1} & a_{1,2} & \cdots & a_{1,n} & a_{1,n+1} \\         a_{2,1} & a_{2,2} & \cdots & a_{2,n} & a_{2,n+1} \\         \vdots  & \vdots  & \ddots & \vdots & \vdots  \\         a_{n,1} & a_{n,2} & \cdots & a_{n,n} & a_{n,n+1}     \end{array} \right] $$ after applying Gaussian-elimination (without backward substitution) to this system we get $$ \left[     \begin{array}{cccc|c}         \hat{a}_{1,1} & \hat{a}_{1,2} & \cdots & \hat{a}_{1,n} & \hat{a}_{1,n+1} \\         0 & \hat{a}_{2,2} & \cdots & \hat{a}_{2,n} & \hat{a}_{2,n+1} \\         \vdots  & \ddots  & \ddots & \vdots & \vdots  \\         0 & \cdots & 0 & \hat{a}_{n,n} & \hat{a}_{n,n+1}     \end{array} \right] $$ We know that the total number of multiplications/divisions and additions/subtractions of the Gaussian-elimination technique is $\frac{n^3}{3}+\frac{1}{2}n^2-\frac{5}{6}n$ and $\frac{1}{3}n^3-\frac{1}{3}n$ respectively. Now we will use the $i$-th row to eliminate the coefficients of $x_i$ in each of the first $i-1$ rows for each $i=n,n-1,...,2$ ($i$ starts at $n$ and goes down to $2$), and we will get: $$ \left[     \begin{array}{cccc|c}         \hat{\hat{a}}_{1,1} & 0 & \cdots & 0 & \hat{\hat{a}}_{1,n+1} \\         0 & \hat{\hat{a}}_{2,2} & \cdots & 0 & \hat{\hat{a}}_{2,n+1} \\         \vdots  & \ddots  & \ddots & \vdots & \vdots  \\         0 & \cdots & 0 & \hat{\hat{a}}_{n,n} & \hat{\hat{a}}_{n,n+1}     \end{array} \right] $$ Since for each $i$ we do $i - 1$ divisions (inorder to calculate $\frac{\hat{a}_1,i}{\hat{a}_{i,i}}$ , $\frac{\hat{a}_2,i}{\hat{a}_{i,i}}$ , ... , $\frac{\hat{a}_{i-1},i}{\hat{a}_{i,i}}$), After that we subtract each of the elements in the row $j=1,2,...,i-1$, $\frac{\hat{a}_j,i}{\hat{a}_{i,i}}$ times the $i$-th row, and so additional $i-1$ multiplications are carried out, and we get that the total number of multiplications/divisions in this step is $\Sigma_{i=2}^n2(i-1)=n^2-n$. And so we get that the total number of multiplications/divisions required to apply the hybrid method is $\frac{n^3}{3}+\frac{1}{2}n^2-\frac{5}{6}n + \frac{1}{2}n^2-\frac{1}{2}n = \frac{1}{3}n^3+\frac{3}{2}n^2-\frac{11}{6}n$ which is different from $\frac{n^3}{3}+\frac{3}{2}n^2-\frac{5}{6}n$. Thanks for any help.","I am stucked at this problem from the book Numerical Analysis 8-th Edition (Burden) (Exercise 6.1.16) : Consider the following Gaussian-elimination/Gauss-Jordan hybrid method for solving linear systems: First apply the Gaussian-elimination technique to reduce the system to triangular form. Then use the $n$-th equation to eliminate the coefficients of $x_{n}$ in each of the first $n-1$ rows. After this is completed use the $(n-1)$-st equation to eliminate the coefficients of $x_{n-1}$ in the first $n-2$ rows, ans so on. The system will eventually appear as the reduced system we get by applying Gauss-Jordan method to the original system. Show that this method requires $\frac{n^3}{3}+\frac{3}{2}n^2-\frac{5}{6}n$ multiplication/divisions and $\frac{n^3}{3}+\frac{n^2}{2}-\frac{5}{6}n$ additions/subtractions. But when I tried to calculate the total number of multiplication/divisions I got $\frac{n^3}{3}+\frac{3}{2}n^2-\frac{11}{6}n$, Here's what I've done (Sorry for being a bit long): The augmented matrix for the system has the form $$ \left[     \begin{array}{cccc|c}         a_{1,1} & a_{1,2} & \cdots & a_{1,n} & a_{1,n+1} \\         a_{2,1} & a_{2,2} & \cdots & a_{2,n} & a_{2,n+1} \\         \vdots  & \vdots  & \ddots & \vdots & \vdots  \\         a_{n,1} & a_{n,2} & \cdots & a_{n,n} & a_{n,n+1}     \end{array} \right] $$ after applying Gaussian-elimination (without backward substitution) to this system we get $$ \left[     \begin{array}{cccc|c}         \hat{a}_{1,1} & \hat{a}_{1,2} & \cdots & \hat{a}_{1,n} & \hat{a}_{1,n+1} \\         0 & \hat{a}_{2,2} & \cdots & \hat{a}_{2,n} & \hat{a}_{2,n+1} \\         \vdots  & \ddots  & \ddots & \vdots & \vdots  \\         0 & \cdots & 0 & \hat{a}_{n,n} & \hat{a}_{n,n+1}     \end{array} \right] $$ We know that the total number of multiplications/divisions and additions/subtractions of the Gaussian-elimination technique is $\frac{n^3}{3}+\frac{1}{2}n^2-\frac{5}{6}n$ and $\frac{1}{3}n^3-\frac{1}{3}n$ respectively. Now we will use the $i$-th row to eliminate the coefficients of $x_i$ in each of the first $i-1$ rows for each $i=n,n-1,...,2$ ($i$ starts at $n$ and goes down to $2$), and we will get: $$ \left[     \begin{array}{cccc|c}         \hat{\hat{a}}_{1,1} & 0 & \cdots & 0 & \hat{\hat{a}}_{1,n+1} \\         0 & \hat{\hat{a}}_{2,2} & \cdots & 0 & \hat{\hat{a}}_{2,n+1} \\         \vdots  & \ddots  & \ddots & \vdots & \vdots  \\         0 & \cdots & 0 & \hat{\hat{a}}_{n,n} & \hat{\hat{a}}_{n,n+1}     \end{array} \right] $$ Since for each $i$ we do $i - 1$ divisions (inorder to calculate $\frac{\hat{a}_1,i}{\hat{a}_{i,i}}$ , $\frac{\hat{a}_2,i}{\hat{a}_{i,i}}$ , ... , $\frac{\hat{a}_{i-1},i}{\hat{a}_{i,i}}$), After that we subtract each of the elements in the row $j=1,2,...,i-1$, $\frac{\hat{a}_j,i}{\hat{a}_{i,i}}$ times the $i$-th row, and so additional $i-1$ multiplications are carried out, and we get that the total number of multiplications/divisions in this step is $\Sigma_{i=2}^n2(i-1)=n^2-n$. And so we get that the total number of multiplications/divisions required to apply the hybrid method is $\frac{n^3}{3}+\frac{1}{2}n^2-\frac{5}{6}n + \frac{1}{2}n^2-\frac{1}{2}n = \frac{1}{3}n^3+\frac{3}{2}n^2-\frac{11}{6}n$ which is different from $\frac{n^3}{3}+\frac{3}{2}n^2-\frac{5}{6}n$. Thanks for any help.",,"['linear-algebra', 'matrices', 'numerical-methods', 'numerical-linear-algebra', 'gaussian-elimination']"
59,A question about the odd sized minors of a certain matrix,A question about the odd sized minors of a certain matrix,,"In Dimers and Amoebae by Kenyon, Okounkov, and Sheffield (2003), they say that it is easy to see that for matrices of the form $$ \left( \begin{array}{ccccc} a_1 & 0 & 0 & 0 & b_n \\ b_1 & a_2 & 0 & 0 & 0 \\  0 & b_2 & a_3 & 0 & 0 \\  0 & 0 & b_3 & \ddots & 0 \\  0 & 0 & 0 & \ddots & a_n \end{array} \right), $$ where $\{a_1,\dots, a_n\}$ and $\{b_1,\dots, b_n\}$ are nonnegative numbers, all odd sized minors are nonnegative. Does anybody have a way to nicely show this?","In Dimers and Amoebae by Kenyon, Okounkov, and Sheffield (2003), they say that it is easy to see that for matrices of the form $$ \left( \begin{array}{ccccc} a_1 & 0 & 0 & 0 & b_n \\ b_1 & a_2 & 0 & 0 & 0 \\  0 & b_2 & a_3 & 0 & 0 \\  0 & 0 & b_3 & \ddots & 0 \\  0 & 0 & 0 & \ddots & a_n \end{array} \right), $$ where $\{a_1,\dots, a_n\}$ and $\{b_1,\dots, b_n\}$ are nonnegative numbers, all odd sized minors are nonnegative. Does anybody have a way to nicely show this?",,['linear-algebra']
60,(Nonunique) Solvability of Sylvester Equation,(Nonunique) Solvability of Sylvester Equation,,"I am interested in stating existence of solution of a Sylvester equation $$ AX - XB = C, $$ where $A$, $B$, $C$, and $X$ are $(n,n)$ matrices. Existence of a unique solution $X$ is given, if $A$ and $B$ do not have an eigenvalue in common. But what about the nonregular case. Are there results out there, that consider the case where $A$ and $B$ possibly share an eigenvalue. I am only aware of the equivalence, that there exists a solution $X$, if and only if  $$ \begin{bmatrix} A & C \\ 0 & B \end{bmatrix} \backsim \begin{bmatrix} A & 0 \\ 0 & B \end{bmatrix},  $$ where $\backsim$ is the equivalence relation. However, this didn't give me much insight into my particular setup.","I am interested in stating existence of solution of a Sylvester equation $$ AX - XB = C, $$ where $A$, $B$, $C$, and $X$ are $(n,n)$ matrices. Existence of a unique solution $X$ is given, if $A$ and $B$ do not have an eigenvalue in common. But what about the nonregular case. Are there results out there, that consider the case where $A$ and $B$ possibly share an eigenvalue. I am only aware of the equivalence, that there exists a solution $X$, if and only if  $$ \begin{bmatrix} A & C \\ 0 & B \end{bmatrix} \backsim \begin{bmatrix} A & 0 \\ 0 & B \end{bmatrix},  $$ where $\backsim$ is the equivalence relation. However, this didn't give me much insight into my particular setup.",,"['linear-algebra', 'reference-request', 'matrix-equations']"
61,Computing simplicial homology via Smith Normal Form over Rings,Computing simplicial homology via Smith Normal Form over Rings,,"I am not sure whether this is the right forum to ask such a question, if not please let me know. In the context of my masters thesis, I am working on writing a program to compute simplicial homology of certain spaces $X$. The idea is to give the program the differentials $\partial_k:C_k \rightarrow C_{k-1}$ as (quite large) matrices, and compute the homology $\frac{kern\partial_k}{im \partial_{k+1}}$ by bringing both matrices in Smith Normal Form. For integral coefficients one can then use  $$H(X; \mathbb{Z}) = \mathbb{Z}^{r-t} \oplus_{i=1}^{t} \mathbb{Z}/{s_i}\mathbb{Z},$$  with $s_i$ the diagonal entries in the SNF of $\partial_{i+1}$ and $r = rankC_i - rank(\partial_i)$ . As the computation with integral coefficients is too computationally expensive, the idea is to compute $H(X,\mathbb{Z}/ d\mathbb{Z})$ for some $d$ to get the torsion parts of the integral homology using the Universal Coefficient Theorem. As $\mathbb{Z}/ d\mathbb{Z}$ contains zero divisors, the kernel of $\partial_i$ may be a torsion module, if the $s_i$ in the SNF of $\partial_i$ are not units (e.g. not 1). Thus one should keep track of the basis change while computing the SNF to exactly know how the image of $\partial_{i+1}$ lies in the kernel of $\partial_{i}$ to get the homology in the form torsion part + free part as above. How do I do that? I somehow have to compute the SNFs simultaneously, and I do not see how that can work. Maybe I am missing something trivial, but I seem to be stuck here. Thank you!","I am not sure whether this is the right forum to ask such a question, if not please let me know. In the context of my masters thesis, I am working on writing a program to compute simplicial homology of certain spaces $X$. The idea is to give the program the differentials $\partial_k:C_k \rightarrow C_{k-1}$ as (quite large) matrices, and compute the homology $\frac{kern\partial_k}{im \partial_{k+1}}$ by bringing both matrices in Smith Normal Form. For integral coefficients one can then use  $$H(X; \mathbb{Z}) = \mathbb{Z}^{r-t} \oplus_{i=1}^{t} \mathbb{Z}/{s_i}\mathbb{Z},$$  with $s_i$ the diagonal entries in the SNF of $\partial_{i+1}$ and $r = rankC_i - rank(\partial_i)$ . As the computation with integral coefficients is too computationally expensive, the idea is to compute $H(X,\mathbb{Z}/ d\mathbb{Z})$ for some $d$ to get the torsion parts of the integral homology using the Universal Coefficient Theorem. As $\mathbb{Z}/ d\mathbb{Z}$ contains zero divisors, the kernel of $\partial_i$ may be a torsion module, if the $s_i$ in the SNF of $\partial_i$ are not units (e.g. not 1). Thus one should keep track of the basis change while computing the SNF to exactly know how the image of $\partial_{i+1}$ lies in the kernel of $\partial_{i}$ to get the homology in the form torsion part + free part as above. How do I do that? I somehow have to compute the SNFs simultaneously, and I do not see how that can work. Maybe I am missing something trivial, but I seem to be stuck here. Thank you!",,"['linear-algebra', 'modules', 'homology-cohomology', 'smith-normal-form']"
62,An inner product on the dual space of a non-complete inner product space?,An inner product on the dual space of a non-complete inner product space?,,"As is well known, for any Hilbert space $V$, there is a natural inner product on the continuous dual. (the space of all continuous linear functionals). Is there a way to endow an inner product on the algebraic dual of an infinite dimensional inner product space? (which should be connected somehow to the inner product of the original space) what about the continuous dual (of non-complete spaces)?","As is well known, for any Hilbert space $V$, there is a natural inner product on the continuous dual. (the space of all continuous linear functionals). Is there a way to endow an inner product on the algebraic dual of an infinite dimensional inner product space? (which should be connected somehow to the inner product of the original space) what about the continuous dual (of non-complete spaces)?",,"['linear-algebra', 'functional-analysis', 'hilbert-spaces', 'inner-products']"
63,Basis of $\mathbb{F}[[x]]$ over $\mathbb{F}$ without AC,Basis of  over  without AC,\mathbb{F}[[x]] \mathbb{F},"Does the ring of formal power series $\mathbb{F}[[x]]$ as a vector space over $\mathbb{F}$  admit a basis without assuming the Axiom of choice, at least in some special cases of $\mathbb{F}$? I'm trying to find a basis explicitly. In the case $\mathbb{F}[x]$ we can simply take $B=\{ x^0,x^1,... \}$. But even $\mathbb{F}_2[[x]]$ does not seem to have any obvious basis. For every infinite-dimensional vector space $\; |V| = \max(|F|, \dim V)$, hence we know that the basis is of size $|V|=\max(2^{|\mathbb{F}|},{\aleph})$, and thus intuitively it seems 'too large to be explicit'.","Does the ring of formal power series $\mathbb{F}[[x]]$ as a vector space over $\mathbb{F}$  admit a basis without assuming the Axiom of choice, at least in some special cases of $\mathbb{F}$? I'm trying to find a basis explicitly. In the case $\mathbb{F}[x]$ we can simply take $B=\{ x^0,x^1,... \}$. But even $\mathbb{F}_2[[x]]$ does not seem to have any obvious basis. For every infinite-dimensional vector space $\; |V| = \max(|F|, \dim V)$, hence we know that the basis is of size $|V|=\max(2^{|\mathbb{F}|},{\aleph})$, and thus intuitively it seems 'too large to be explicit'.",,"['linear-algebra', 'vector-spaces', 'axiom-of-choice']"
64,Expectation of inverse of a symmetric matrix with gaussian elements,Expectation of inverse of a symmetric matrix with gaussian elements,,"Is there any way to calculate: \begin{equation} \mathbb{E} \; ( H^{T}H )^{-1} \end{equation} assuming that the entries of the matrix $H$ are gaussian random variables with unknown means but same known variances, and $\mathbb{E} $ is the expectation operator. Thanks in advance","Is there any way to calculate: \begin{equation} \mathbb{E} \; ( H^{T}H )^{-1} \end{equation} assuming that the entries of the matrix $H$ are gaussian random variables with unknown means but same known variances, and $\mathbb{E} $ is the expectation operator. Thanks in advance",,"['linear-algebra', 'matrices', 'parameter-estimation']"
65,An inverse of Jordan matrix - basis,An inverse of Jordan matrix - basis,,"Let $A\in M_{n\times n}$ be and invertible matrix over complex field and we assume it's already at Jordan form where $B=\{v_1,…,v_n \}$ is Jordan basis for A. Find Jordan form and Jordan basis for $A^{-1}$ I think I can show that Jordan form will be almost the same with difference that eigenvalue $\lambda$ will be replaced by $\frac{1}{\lambda}$ at diagonal but I don't have idea how basis for this will looks like.","Let $A\in M_{n\times n}$ be and invertible matrix over complex field and we assume it's already at Jordan form where $B=\{v_1,…,v_n \}$ is Jordan basis for A. Find Jordan form and Jordan basis for $A^{-1}$ I think I can show that Jordan form will be almost the same with difference that eigenvalue $\lambda$ will be replaced by $\frac{1}{\lambda}$ at diagonal but I don't have idea how basis for this will looks like.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'jordan-normal-form']"
66,Invariant Subspace of Two Operators [duplicate],Invariant Subspace of Two Operators [duplicate],,"This question already has answers here : Invariant Subspace of Two Linear Involutions (4 answers) Closed 9 years ago . Let $S$ , $T$ be linear operators on a finite-dimensional vector space $V$ over $\mathbb{C}$ . Suppose $$S^2 = T^2 = I.$$ Show that there exists either a $1$ -dimensional or $2$ -dimensional subspace of $V$ which is  invariant under $S$ and $T$ . Ok so, since $S^2 = T^2 = I$ ,  either $1$ or $-1$ are Eigenvalues of S and T. i,e  The Minimal Polynomial $M_T$ and $M_S$ satisfy $$M_T \; | \;  (x+1)(x-1)$$ $$M_S \; | \;  (x+1)(x-1)$$ Edit : Found the same question elsewhere. For those of you who are interested in the answers. Invariant Subspace of Two Linear Involutions","This question already has answers here : Invariant Subspace of Two Linear Involutions (4 answers) Closed 9 years ago . Let , be linear operators on a finite-dimensional vector space over . Suppose Show that there exists either a -dimensional or -dimensional subspace of which is  invariant under and . Ok so, since ,  either or are Eigenvalues of S and T. i,e  The Minimal Polynomial and satisfy Edit : Found the same question elsewhere. For those of you who are interested in the answers. Invariant Subspace of Two Linear Involutions",S T V \mathbb{C} S^2 = T^2 = I. 1 2 V S T S^2 = T^2 = I 1 -1 M_T M_S M_T \; | \;  (x+1)(x-1) M_S \; | \;  (x+1)(x-1),['linear-algebra']
67,calculation of the determinant of a block matrix little help,calculation of the determinant of a block matrix little help,,"I need to prove    $$\operatorname{det}\begin{pmatrix}A & B \\ C & D\\ \end{pmatrix}= \operatorname{det}(DA-CB),$$   where $A,B,C,D \in M_{n\times n}(R)$ with the property that $A$ and $B$ commute and moreover $\operatorname{det}(B) \neq 0,$ using the following hint: $$\begin{pmatrix}A & B \\ C & D\\ \end{pmatrix}\begin{pmatrix}B & 0 \\ -A & I_n\\ \end{pmatrix}.$$ Using the hint, I have: $$\begin{pmatrix}A & B \\ C & D\\ \end{pmatrix}\begin{pmatrix}B & 0 \\ -A & I_n\\ \end{pmatrix} = \begin{pmatrix}0 & B \\ CB-DA & D\\ \end{pmatrix}.$$Then: $$\operatorname{det}\begin{pmatrix}A & B \\ C & D\\ \end{pmatrix}\operatorname{det}\begin{pmatrix}B & 0 \\ -A & I_n\\ \end{pmatrix} = \operatorname{det}\begin{pmatrix}0 & B \\ CB-DA & D\\ \end{pmatrix},$$ but  $$\operatorname{det}\begin{pmatrix}B & 0 \\ -A & I_n\\ \end{pmatrix} = \operatorname{det}(B)$$ $$\operatorname{det}\begin{pmatrix}0 & B \\ CB-DA & D\\ \end{pmatrix} = (-1)^n\operatorname{det}(CB-DA)\operatorname{det}(B).$$ Then: $$\operatorname{det}\begin{pmatrix}A & B \\ C & D\\ \end{pmatrix}\operatorname{det}(B) = (-1)^n\operatorname{det}(CB-DA)\operatorname{det}(B)$$ which can be rewritten as follows: $$[\operatorname{det}\begin{pmatrix}A & B \\ C & D\\ \end{pmatrix}+(-1)^{n+1}\operatorname{det}(CB-DA)]\operatorname{det}(B) = 0$$ But $\operatorname{det}(B) \neq 0$, hence:$$\operatorname{det}\begin{pmatrix}A & B \\ C & D\\ \end{pmatrix}+(-1)^{n+1}\operatorname{det}(CB-DA) = 0,$$ So $$\operatorname{det}\begin{pmatrix}A & B \\ C & D\\ \end{pmatrix} = (-1)^n\operatorname{det}(CB-DA).$$ Im stuck on this part, can anyone help me on this step","I need to prove    $$\operatorname{det}\begin{pmatrix}A & B \\ C & D\\ \end{pmatrix}= \operatorname{det}(DA-CB),$$   where $A,B,C,D \in M_{n\times n}(R)$ with the property that $A$ and $B$ commute and moreover $\operatorname{det}(B) \neq 0,$ using the following hint: $$\begin{pmatrix}A & B \\ C & D\\ \end{pmatrix}\begin{pmatrix}B & 0 \\ -A & I_n\\ \end{pmatrix}.$$ Using the hint, I have: $$\begin{pmatrix}A & B \\ C & D\\ \end{pmatrix}\begin{pmatrix}B & 0 \\ -A & I_n\\ \end{pmatrix} = \begin{pmatrix}0 & B \\ CB-DA & D\\ \end{pmatrix}.$$Then: $$\operatorname{det}\begin{pmatrix}A & B \\ C & D\\ \end{pmatrix}\operatorname{det}\begin{pmatrix}B & 0 \\ -A & I_n\\ \end{pmatrix} = \operatorname{det}\begin{pmatrix}0 & B \\ CB-DA & D\\ \end{pmatrix},$$ but  $$\operatorname{det}\begin{pmatrix}B & 0 \\ -A & I_n\\ \end{pmatrix} = \operatorname{det}(B)$$ $$\operatorname{det}\begin{pmatrix}0 & B \\ CB-DA & D\\ \end{pmatrix} = (-1)^n\operatorname{det}(CB-DA)\operatorname{det}(B).$$ Then: $$\operatorname{det}\begin{pmatrix}A & B \\ C & D\\ \end{pmatrix}\operatorname{det}(B) = (-1)^n\operatorname{det}(CB-DA)\operatorname{det}(B)$$ which can be rewritten as follows: $$[\operatorname{det}\begin{pmatrix}A & B \\ C & D\\ \end{pmatrix}+(-1)^{n+1}\operatorname{det}(CB-DA)]\operatorname{det}(B) = 0$$ But $\operatorname{det}(B) \neq 0$, hence:$$\operatorname{det}\begin{pmatrix}A & B \\ C & D\\ \end{pmatrix}+(-1)^{n+1}\operatorname{det}(CB-DA) = 0,$$ So $$\operatorname{det}\begin{pmatrix}A & B \\ C & D\\ \end{pmatrix} = (-1)^n\operatorname{det}(CB-DA).$$ Im stuck on this part, can anyone help me on this step",,"['linear-algebra', 'abstract-algebra', 'matrices', 'determinant']"
68,Why is it useful to know when a linear operator on a vector space is diagonalizable?,Why is it useful to know when a linear operator on a vector space is diagonalizable?,,"I'm currently taking a conceptual course in linear algebra, and I'm trying to understand why it would be theoretically useful or illuminating to know when a linear operator (or its matrix representation) is diagonalizable. Why is this? How does it help us to more deeply understand vector spaces, linear operators, their applications, etc.?","I'm currently taking a conceptual course in linear algebra, and I'm trying to understand why it would be theoretically useful or illuminating to know when a linear operator (or its matrix representation) is diagonalizable. Why is this? How does it help us to more deeply understand vector spaces, linear operators, their applications, etc.?",,"['linear-algebra', 'matrices', 'diagonalization', 'linear-transformations']"
69,Can we find the GCD of two polynomials in $\mathbb Q[x]$ by representing the coefficients as vectors?,Can we find the GCD of two polynomials in  by representing the coefficients as vectors?,\mathbb Q[x],"Can we find the GCD of two polynomials in $\mathbb Q[x]$ by representing the coefficients as vectors? For example: $f=x^5+3x^4+x^3+4x^2+1$, and $g=x^5+3x^4+4x^3+3x+1$ Can we represent these polynomials as f $=(1,3,1,4,0,1)$ and g $=(1,3,4,0,3,1)$ and somehow perform matrix operations to find the GCD?","Can we find the GCD of two polynomials in $\mathbb Q[x]$ by representing the coefficients as vectors? For example: $f=x^5+3x^4+x^3+4x^2+1$, and $g=x^5+3x^4+4x^3+3x+1$ Can we represent these polynomials as f $=(1,3,1,4,0,1)$ and g $=(1,3,4,0,3,1)$ and somehow perform matrix operations to find the GCD?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'number-theory', 'polynomials']"
70,Prove that: if $T$ is an irreducible linear operator then $T$ is cyclic,Prove that: if  is an irreducible linear operator then  is cyclic,T T,"Let $T:V\to V$ be a linear operator on a finite dimensional vector space $V$. I need to prove that: If $T$ is irreducible then $T$ is cyclic My definitions are: $T$ is an irreducible linear operator iff $V$ and {$0$} are the only complementary invariant subspaces T is a cyclic linear operator iff $V$ is a cyclic subspace (i.e. there is a vector $v\in V$ such that $V$ is generated by the set of vectors {$v, T(v),T^2(v),...$} I don´t know where to start. Any comment, suggestion or hint would be highly appreciated","Let $T:V\to V$ be a linear operator on a finite dimensional vector space $V$. I need to prove that: If $T$ is irreducible then $T$ is cyclic My definitions are: $T$ is an irreducible linear operator iff $V$ and {$0$} are the only complementary invariant subspaces T is a cyclic linear operator iff $V$ is a cyclic subspace (i.e. there is a vector $v\in V$ such that $V$ is generated by the set of vectors {$v, T(v),T^2(v),...$} I don´t know where to start. Any comment, suggestion or hint would be highly appreciated",,['linear-algebra']
71,Optimized way to compute L1 distance matrix,Optimized way to compute L1 distance matrix,,"I'm computing distances between two groups of multi-dimensional points giving a matrix of distances pairwise between points. For the L2 (euclidean) distance I can use optimized matrix multiplication routines (blas etc.). X and Y which are matrices $ n\times k $, and $ m\times k $  comprised of the points $x_1..x_n $, and $y_1..y_m$ stacked as row vectors ($k$ is the dimension of the points), the final output D a matrix $ n\times m $ of pairwise distances, with elements: $ d_{ij} = ||x_i - y_j||^2  = ||x_i||^2 + ||y_j||^2 - 2x_i.y_j $ Where the last term $ 2x_i.y_i $ can be computed by matrix multiplication $2XY^T$ and the first two terms can be computed for each $ ||x_i||^2 = x_i . x_i $ and $ ||y_j||^2 = y_j . y_j  $ Better described in this paper: http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0092409&representation=PDF Is there any way to vectorize the L1 (Manhattan) distance similarly?","I'm computing distances between two groups of multi-dimensional points giving a matrix of distances pairwise between points. For the L2 (euclidean) distance I can use optimized matrix multiplication routines (blas etc.). X and Y which are matrices $ n\times k $, and $ m\times k $  comprised of the points $x_1..x_n $, and $y_1..y_m$ stacked as row vectors ($k$ is the dimension of the points), the final output D a matrix $ n\times m $ of pairwise distances, with elements: $ d_{ij} = ||x_i - y_j||^2  = ||x_i||^2 + ||y_j||^2 - 2x_i.y_j $ Where the last term $ 2x_i.y_i $ can be computed by matrix multiplication $2XY^T$ and the first two terms can be computed for each $ ||x_i||^2 = x_i . x_i $ and $ ||y_j||^2 = y_j . y_j  $ Better described in this paper: http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0092409&representation=PDF Is there any way to vectorize the L1 (Manhattan) distance similarly?",,"['linear-algebra', 'matrices', 'algorithms', 'numerical-methods', 'numerical-linear-algebra']"
72,How to show that two quadratic forms are equivalent?,How to show that two quadratic forms are equivalent?,,"To show to quadratic forms are not equivalent, we can find rank, or discriminant or some element which is represented by either one only etc. But Is there a general criterion to show that two binary(right now I am only concerned for binary) quadratic forms are equivalent. Like here is an example which uses variable transformation, but every time find what transformation will apply is tough. $\textbf{Example-}$ $X^2-Y^2 $ and $4XY$. replacing $X,Y$ by $X+Y,X-Y$ do the job here. Nnow there are criterion which gives an equivalent condition to check whether two binary forms are equivalent or not , like $\textbf{Criterion 1-}$ On algebraically closed fields, same dimension and same rank suffice. $\textbf{Criterion 2-}$ On Reals, same rank and same signature suffices. $\textbf{Criterion 3-}$ On finite fields,same discriminant is enough. But in general if we are given field as some arbitrary $K$, then what to do? I am very new at quadratic forms. Any help will be appreciated. I know that we can check similarity of their corresponding matrices, but is there any other way, beside checking matrices are similar or not. Like how would you proceed on these couple of question below. If there is no other method, can somebody please explain me the method to check whether two matrices are similar or not, I am really weak with matrices. $\textbf{Question 1-}$ Prove $X^2-4XY+3Y^2, X^2-Y^2,aX^2-aY^2$ are equivalent over $K$. $\textbf{Question 2-}$ Show i) $X^2-Y^2 \sim XY$ over $K$,   ii) $3X^2-2Y^2 \sim X^2-6Y^2$ (over $Q$)","To show to quadratic forms are not equivalent, we can find rank, or discriminant or some element which is represented by either one only etc. But Is there a general criterion to show that two binary(right now I am only concerned for binary) quadratic forms are equivalent. Like here is an example which uses variable transformation, but every time find what transformation will apply is tough. $\textbf{Example-}$ $X^2-Y^2 $ and $4XY$. replacing $X,Y$ by $X+Y,X-Y$ do the job here. Nnow there are criterion which gives an equivalent condition to check whether two binary forms are equivalent or not , like $\textbf{Criterion 1-}$ On algebraically closed fields, same dimension and same rank suffice. $\textbf{Criterion 2-}$ On Reals, same rank and same signature suffices. $\textbf{Criterion 3-}$ On finite fields,same discriminant is enough. But in general if we are given field as some arbitrary $K$, then what to do? I am very new at quadratic forms. Any help will be appreciated. I know that we can check similarity of their corresponding matrices, but is there any other way, beside checking matrices are similar or not. Like how would you proceed on these couple of question below. If there is no other method, can somebody please explain me the method to check whether two matrices are similar or not, I am really weak with matrices. $\textbf{Question 1-}$ Prove $X^2-4XY+3Y^2, X^2-Y^2,aX^2-aY^2$ are equivalent over $K$. $\textbf{Question 2-}$ Show i) $X^2-Y^2 \sim XY$ over $K$,   ii) $3X^2-2Y^2 \sim X^2-6Y^2$ (over $Q$)",,"['linear-algebra', 'quadratic-forms']"
73,Decoding of Gabidulin code,Decoding of Gabidulin code,,"Consider the space of matrices in $\mathbb{F}_q^{n \times m}$ where $\mathbb{F}_q$ is the finite field with $q$ elements. We can define a metric on this space, given by $d(A,B) := rank(A-B)$, called the rank-metric. This allows us to consider error-correcting codes in this space. One example of codes in this metric are Gabidulin codes, which are analogues of Reed-Solomon codes. Essentially, we consider the rows of an $n \times m$ matrix as elements of the extension field $\mathbb{F}_{q^m}$. So now we consider the space of linearized polynomials over $\mathbb{F}_{q^m}$ of q-degree k-1 as the set of messages, and to get a codeword, we evaluate such a polynomial at $n$ evaluation points $g_1, g_2, \dots, g_n \in \mathbb{F}_{q^m}$ which are linearly independent as vectors over the base field $\mathbb{F}_q$, giving a vector in $\mathbb{F}_{q^m}^n$ which we can then interpret again as a matrix in $\mathbb{F}_q^{n \times m}$. This code, just like Reed-Solomon codes, has distance $n-k+1$. Now consider the decoding of such codes. We shall consider the analogue of the Berlekamp-Welch decoder for Reed-Solomon codes.  We are given a word $(y_1,y_2,\dots,y_n)^T$ within a distance (according to the rank-metric) of $t$ from a codeword corresponding to the linearized polynomial $f$ say, and our aim is to algorithmically find $f$ when $t$ is less than half of the distance.  This is worked out in the following paper: http://perso.univ-rennes1.fr/pierre.loidreau/articles/wcc_2005/Welch_Berlekamp.pdf If you are familiar with the Berlekamp-Welch decoder, this paper should be easy enough to read through quickly.  In the paper, consider proof of proposition 2 in section 4. There, it claims that ""Now let us consider a non-zero solution $(V_0,P_0)$ of 1), then any solution $(V,N)$ of 2) satisfies the following system of equations: $$V_0[N(g_i)-V(P_0(g_i))]=0 $$ $\forall i=1,\dots,n$"" How is this equation obtained? The paper states it as if its obvious, but I am unable to prove this. Any help would be appreciated. Thanks.","Consider the space of matrices in $\mathbb{F}_q^{n \times m}$ where $\mathbb{F}_q$ is the finite field with $q$ elements. We can define a metric on this space, given by $d(A,B) := rank(A-B)$, called the rank-metric. This allows us to consider error-correcting codes in this space. One example of codes in this metric are Gabidulin codes, which are analogues of Reed-Solomon codes. Essentially, we consider the rows of an $n \times m$ matrix as elements of the extension field $\mathbb{F}_{q^m}$. So now we consider the space of linearized polynomials over $\mathbb{F}_{q^m}$ of q-degree k-1 as the set of messages, and to get a codeword, we evaluate such a polynomial at $n$ evaluation points $g_1, g_2, \dots, g_n \in \mathbb{F}_{q^m}$ which are linearly independent as vectors over the base field $\mathbb{F}_q$, giving a vector in $\mathbb{F}_{q^m}^n$ which we can then interpret again as a matrix in $\mathbb{F}_q^{n \times m}$. This code, just like Reed-Solomon codes, has distance $n-k+1$. Now consider the decoding of such codes. We shall consider the analogue of the Berlekamp-Welch decoder for Reed-Solomon codes.  We are given a word $(y_1,y_2,\dots,y_n)^T$ within a distance (according to the rank-metric) of $t$ from a codeword corresponding to the linearized polynomial $f$ say, and our aim is to algorithmically find $f$ when $t$ is less than half of the distance.  This is worked out in the following paper: http://perso.univ-rennes1.fr/pierre.loidreau/articles/wcc_2005/Welch_Berlekamp.pdf If you are familiar with the Berlekamp-Welch decoder, this paper should be easy enough to read through quickly.  In the paper, consider proof of proposition 2 in section 4. There, it claims that ""Now let us consider a non-zero solution $(V_0,P_0)$ of 1), then any solution $(V,N)$ of 2) satisfies the following system of equations: $$V_0[N(g_i)-V(P_0(g_i))]=0 $$ $\forall i=1,\dots,n$"" How is this equation obtained? The paper states it as if its obvious, but I am unable to prove this. Any help would be appreciated. Thanks.",,"['linear-algebra', 'abstract-algebra', 'algorithms', 'coding-theory']"
74,Is there a name for the group of complex matrices with unimodular determinant?,Is there a name for the group of complex matrices with unimodular determinant?,,"Does the group $$ G = \left\{ A \in \mathbb{C}^{n \times n} : |\det(A)| = 1 \right\} $$ have a name?  It obviously contains the unitary group $U(n)$ and the special linear group $SL(n,\mathbb C)$. After googling a few variantions of ""matrices with unimodular determinant"" and coming up empty, I thought that I would see what people here think. Edit: Notice that $G$ is strictly larger than both $U(n)$ and $SL(n)$.  Consider $$ A=\begin{pmatrix} 1 & 1 \\ 0 & -1 \end{pmatrix}. $$ We have $A \in G$, but $A$ is not unitary and $A \notin SL(2,\mathbb C)$. Even though $G \neq SL(n,\mathbb C)$, they are related in the sense that $S^1 \times SL(n,\mathbb C) $ is an $n$-fold cover of $G$ -- the covering is defined by $(z,A) \mapsto z \cdot A$.  (This is an $n$-to-1 map because $\det(zA) = z^n \det(A)$ for $n \times n$ matrices $A$).","Does the group $$ G = \left\{ A \in \mathbb{C}^{n \times n} : |\det(A)| = 1 \right\} $$ have a name?  It obviously contains the unitary group $U(n)$ and the special linear group $SL(n,\mathbb C)$. After googling a few variantions of ""matrices with unimodular determinant"" and coming up empty, I thought that I would see what people here think. Edit: Notice that $G$ is strictly larger than both $U(n)$ and $SL(n)$.  Consider $$ A=\begin{pmatrix} 1 & 1 \\ 0 & -1 \end{pmatrix}. $$ We have $A \in G$, but $A$ is not unitary and $A \notin SL(2,\mathbb C)$. Even though $G \neq SL(n,\mathbb C)$, they are related in the sense that $S^1 \times SL(n,\mathbb C) $ is an $n$-fold cover of $G$ -- the covering is defined by $(z,A) \mapsto z \cdot A$.  (This is an $n$-to-1 map because $\det(zA) = z^n \det(A)$ for $n \times n$ matrices $A$).",,"['linear-algebra', 'lie-groups']"
75,mathematics of chemical stoichiometry,mathematics of chemical stoichiometry,,"I would like to better understand the mathematical description of chemical stoichiometry and thermodynamic chemical equilibrium. This problem has many features and I know my description might be too vague. There are generally two approaches to the problem, the constrained optimization problem (e.g. minimization of gibb's energy with mass balance constraints) or using the $\Delta G_{rx}=-RTln(K_{eq})$ method based upon the equilibrium constant for a set of linearly independent stoichiometric equations. The latter method can only be applied assuming the initial species assumed present are non-negative because having a negative or zero value for a species concentration would violate the functional form of the Keq method. $N = [\nu_{ij}]$, where $\nu_{ij}$ are the stoichiometric coefficents of the jth species in the ith equation. $K_{eq,i}=\prod activity_j^{\nu_{ij}}$ for each stoichiometric equation $i$ To address the non-negativity constraint, a sort of model reduction can be applied by generating the power set of the set of possible chemical species under consideration. The equilibrium constant method can be applied to each element of the power set assuming non-negativity. This would require that the element of the power set would correctly represent the actual equilbrium by checking some auxiliary condition such as the activity of the chemical species according to thermodynamic equilibrium of ideal substance. Suppose the (very simple) chemical equations that can represent chemical reactions (not chemical mechanisms) indicated by the following construct: General: $(\{Chemical Species\},\{Chemical Elements\})$ Example: $(\{a(c),ab_2(g),b_2(g)\},\{a,b\})$ There now needs to be a choice of ordering for the set of Chemical Species and for the set of Chemical Elements. One choice is to just use the sequence suggested by the listing of elements in the each set (not sure how to mathematically describe this, but i have the feeling it is something related to an indexed set, but dont understand how the ordering arises from a set if a set isn't supposed to have ordering). In most literature, the prior construct is a ordered pair of 'ordered sets' and is called a chemical system. Chemical Species: $a(c) = 1*a$ $ab_2(g) = 1*a + 2*b$ $b_2(g) = 2*b$ One can construct each vector representations of the chemical species from the chemical elements once the ordering is chosen. Chemical Elements: $a=(1,0),b=(0,1)$ And with a chosen ordering of the chemical species the following element abundance matrix can be generated (i.e. a mapping from chemical species quantities to elemental quantities): $ A = \bordermatrix{~ & a(c) & ab_2(g) & b_2(g) \cr               a & 1 & 1 & 0\cr               b & 0 & 2 & 2\cr}  $ $ A = \begin{pmatrix} 1 & 1 & 0\\ 0 & 2 & 2\end{pmatrix} $ The coefficents to the stoichiometric equation can be generated by taking finding the null space (matrix $N$ of the matrix $A$. Once the stoichiometric matrix $N$ is found, it can be put in a 'canonical form' by the operations: $(RREF(N^T))^T$ $ This produces $N=\begin{bmatrix}1 \\ -1 \\ 1\end{bmatrix}$ Where, $AN=0$. (This is essentially the condition of mass balance due to reaction, but the relation doesn't constrain the quantities being balanced to be positive) The process of finding the stoichiometric equations for a given set of chemical species and  set of chemical elements can be constructed in this manner. But the process involved making certain decisions, such as choosing an ordering of the chemical species from the set of allowed species. If a different ordering is chosen for the chemical species, it is possible to represent the $A' = \bordermatrix{~ & ab_2(g) & a(c) & b_2(g) \cr               a & 1 & 1 & 0\cr               b & 2 & 0 & 2\cr} $ $ A' = \begin{pmatrix} 1 & 1 & 0\\ 2 & 0 & 2\end{pmatrix} $ This produces $N'=\begin{bmatrix}1 \\ -1 \\ -1\end{bmatrix}$ for the 'canonical form' of the stoichiometric matrix. From a thermodynamic viewpoint, this essentially changes what one would consider the 'products' compared to the 'reactants' in a chemical reaction. But for equilibrium this doesn't really matter, because the equilibrium is based upon the quantity of the chemical elements and not the starting quantity of chemical species (the starting quantity of chemical species is only indicative of the starting elemental species quantities). The form of the $\Delta(G_{rx})$ and $K_{eq}$ will change according to whatever ordering is chosen. I often chose specific orderings to force the exponents in the $K_{eq}$ expressions to be $1$ for chemical species listed first in the elemental abundance matrix. Additionally, there is the problem of assuming that one of the assumed chemical species present before calculating equilibrium might be a limiting reagant (i.e. a solid reacting to form gas and is no longer present). This wouldn't be properly accounted for in the $K_{eq}$ method due to the condition of non-negative. Therefore, one can consider the original construct to be: $(\{ab_2(g),b_2(g)\},\{a,b\})$ $ A = \begin{pmatrix} 1 & 0\\  2 & 2\end{pmatrix} $ The stoichiometric matrix $N$ is the zero (trivial) subspace, $A$ is of rank 2. Admittedly, The mass balance would be sufficient constraints to this system without needing to address the functional form of the thermodynamic constraints; most problems are much harder! There is also a thermodynamic condition that would need to be checked (such as activity of a condensed species) to confirm that the assumed chemical species present are actually present or not. To Conclude, I have already identified in literature how to generate the stoichiometric equations for a given collection of chemical species and chemical elements. The stoichiometric coefficents are used in thermodynamic theory to calculate equilibrium using certain procedures. However the change of ordering of the matrix columns of $A$ changes the coefficents represented by $N$ and the changes the functional form of the $K_{eq}$ problem. What is the proper mathematical description needed to talk about how changing the ordering of $A$ influences the other constructs. The non-negativity constraint can be realized by taking subsets of the chemical species assumed to be in some parent set with an additional check that the exclusion was valid (thermodynamic check on species activity). In the most extreme case it would be necessary to have $(# of species)!-1$ different cases, that is all elements of the power set of chemical species other than the null element. (That is the maximal number of cases, almost never realized because if the chemical elements are present then there must be some chemical species that are comprised of them.) How can I discuss the 'model reduction' of choosing an element of the power set of initial chemical species to calculate thermodynamic equilibrium. Just like the re-ordering of the elemental abundance matrix, the functional form of the thermodynamic constraints is affected by the choice of starting species. I have already solved my problem and can calculate thermodynamic equilbrium, I just dont know how to talk about it and identify when two of the mathematical constructs generated are actually the same or how to relate the sub-cases to the initial problem. I know this relates to convex optimization type problems, but it has a feel of representation of groups. Feel free to give me any suggestions, places where this might already be explained, or if you would like me to further clarify any concepts I only briefly discussed.","I would like to better understand the mathematical description of chemical stoichiometry and thermodynamic chemical equilibrium. This problem has many features and I know my description might be too vague. There are generally two approaches to the problem, the constrained optimization problem (e.g. minimization of gibb's energy with mass balance constraints) or using the $\Delta G_{rx}=-RTln(K_{eq})$ method based upon the equilibrium constant for a set of linearly independent stoichiometric equations. The latter method can only be applied assuming the initial species assumed present are non-negative because having a negative or zero value for a species concentration would violate the functional form of the Keq method. $N = [\nu_{ij}]$, where $\nu_{ij}$ are the stoichiometric coefficents of the jth species in the ith equation. $K_{eq,i}=\prod activity_j^{\nu_{ij}}$ for each stoichiometric equation $i$ To address the non-negativity constraint, a sort of model reduction can be applied by generating the power set of the set of possible chemical species under consideration. The equilibrium constant method can be applied to each element of the power set assuming non-negativity. This would require that the element of the power set would correctly represent the actual equilbrium by checking some auxiliary condition such as the activity of the chemical species according to thermodynamic equilibrium of ideal substance. Suppose the (very simple) chemical equations that can represent chemical reactions (not chemical mechanisms) indicated by the following construct: General: $(\{Chemical Species\},\{Chemical Elements\})$ Example: $(\{a(c),ab_2(g),b_2(g)\},\{a,b\})$ There now needs to be a choice of ordering for the set of Chemical Species and for the set of Chemical Elements. One choice is to just use the sequence suggested by the listing of elements in the each set (not sure how to mathematically describe this, but i have the feeling it is something related to an indexed set, but dont understand how the ordering arises from a set if a set isn't supposed to have ordering). In most literature, the prior construct is a ordered pair of 'ordered sets' and is called a chemical system. Chemical Species: $a(c) = 1*a$ $ab_2(g) = 1*a + 2*b$ $b_2(g) = 2*b$ One can construct each vector representations of the chemical species from the chemical elements once the ordering is chosen. Chemical Elements: $a=(1,0),b=(0,1)$ And with a chosen ordering of the chemical species the following element abundance matrix can be generated (i.e. a mapping from chemical species quantities to elemental quantities): $ A = \bordermatrix{~ & a(c) & ab_2(g) & b_2(g) \cr               a & 1 & 1 & 0\cr               b & 0 & 2 & 2\cr}  $ $ A = \begin{pmatrix} 1 & 1 & 0\\ 0 & 2 & 2\end{pmatrix} $ The coefficents to the stoichiometric equation can be generated by taking finding the null space (matrix $N$ of the matrix $A$. Once the stoichiometric matrix $N$ is found, it can be put in a 'canonical form' by the operations: $(RREF(N^T))^T$ $ This produces $N=\begin{bmatrix}1 \\ -1 \\ 1\end{bmatrix}$ Where, $AN=0$. (This is essentially the condition of mass balance due to reaction, but the relation doesn't constrain the quantities being balanced to be positive) The process of finding the stoichiometric equations for a given set of chemical species and  set of chemical elements can be constructed in this manner. But the process involved making certain decisions, such as choosing an ordering of the chemical species from the set of allowed species. If a different ordering is chosen for the chemical species, it is possible to represent the $A' = \bordermatrix{~ & ab_2(g) & a(c) & b_2(g) \cr               a & 1 & 1 & 0\cr               b & 2 & 0 & 2\cr} $ $ A' = \begin{pmatrix} 1 & 1 & 0\\ 2 & 0 & 2\end{pmatrix} $ This produces $N'=\begin{bmatrix}1 \\ -1 \\ -1\end{bmatrix}$ for the 'canonical form' of the stoichiometric matrix. From a thermodynamic viewpoint, this essentially changes what one would consider the 'products' compared to the 'reactants' in a chemical reaction. But for equilibrium this doesn't really matter, because the equilibrium is based upon the quantity of the chemical elements and not the starting quantity of chemical species (the starting quantity of chemical species is only indicative of the starting elemental species quantities). The form of the $\Delta(G_{rx})$ and $K_{eq}$ will change according to whatever ordering is chosen. I often chose specific orderings to force the exponents in the $K_{eq}$ expressions to be $1$ for chemical species listed first in the elemental abundance matrix. Additionally, there is the problem of assuming that one of the assumed chemical species present before calculating equilibrium might be a limiting reagant (i.e. a solid reacting to form gas and is no longer present). This wouldn't be properly accounted for in the $K_{eq}$ method due to the condition of non-negative. Therefore, one can consider the original construct to be: $(\{ab_2(g),b_2(g)\},\{a,b\})$ $ A = \begin{pmatrix} 1 & 0\\  2 & 2\end{pmatrix} $ The stoichiometric matrix $N$ is the zero (trivial) subspace, $A$ is of rank 2. Admittedly, The mass balance would be sufficient constraints to this system without needing to address the functional form of the thermodynamic constraints; most problems are much harder! There is also a thermodynamic condition that would need to be checked (such as activity of a condensed species) to confirm that the assumed chemical species present are actually present or not. To Conclude, I have already identified in literature how to generate the stoichiometric equations for a given collection of chemical species and chemical elements. The stoichiometric coefficents are used in thermodynamic theory to calculate equilibrium using certain procedures. However the change of ordering of the matrix columns of $A$ changes the coefficents represented by $N$ and the changes the functional form of the $K_{eq}$ problem. What is the proper mathematical description needed to talk about how changing the ordering of $A$ influences the other constructs. The non-negativity constraint can be realized by taking subsets of the chemical species assumed to be in some parent set with an additional check that the exclusion was valid (thermodynamic check on species activity). In the most extreme case it would be necessary to have $(# of species)!-1$ different cases, that is all elements of the power set of chemical species other than the null element. (That is the maximal number of cases, almost never realized because if the chemical elements are present then there must be some chemical species that are comprised of them.) How can I discuss the 'model reduction' of choosing an element of the power set of initial chemical species to calculate thermodynamic equilibrium. Just like the re-ordering of the elemental abundance matrix, the functional form of the thermodynamic constraints is affected by the choice of starting species. I have already solved my problem and can calculate thermodynamic equilbrium, I just dont know how to talk about it and identify when two of the mathematical constructs generated are actually the same or how to relate the sub-cases to the initial problem. I know this relates to convex optimization type problems, but it has a feel of representation of groups. Feel free to give me any suggestions, places where this might already be explained, or if you would like me to further clarify any concepts I only briefly discussed.",,"['linear-algebra', 'optimization', 'chemistry', 'constraint-programming']"
76,There exist an infinite subset $S\subseteq\mathbb{R}^3$ such that any three vectors in $S$ are linearly independent.,There exist an infinite subset  such that any three vectors in  are linearly independent.,S\subseteq\mathbb{R}^3 S,Could anyone just give me hint for this one? There exist an infinite subset $S\subseteq\mathbb{R}^3$ such that any three vectors in $S$ are linearly independent. True or false?,Could anyone just give me hint for this one? There exist an infinite subset $S\subseteq\mathbb{R}^3$ such that any three vectors in $S$ are linearly independent. True or false?,,['linear-algebra']
77,The close form expression of a Pfaffian,The close form expression of a Pfaffian,,"Recall Schur's Pfaffian identity: $$ \mathrm{Pf}\left(\frac{x_j-x_i}{x_j+x_i}\right)_{1\le i,j\le 2n} = \prod_{1\le i<j \le 2n}\frac{x_j-x_i}{x_j+x_i}. $$ Here $x_1,x_2\cdots x_{2n}$ are $2n$ variables, and $\mathrm{Pf}(A_{ij})$ stands for the Pfaffian of $2n$ by $2n$ skew-symmetric matrix $A$, whose $(i,j)$-th entry is $A_{ij}$. Now, I would like to know whether there is similar closed form for the following Pfaffian(s): $$ \mathrm{Pf}\left(\frac{x_j+x_i}{x_j-x_i}\right)_{1\le i,j\le 2n}, $$ and $$ \mathrm{Pf}\left(\frac{1}{x_j-x_i}\right)_{1\le i,j\le 2n}. $$ By closed form, I mean the expression resembling the RHS of Schur's Pfaffian identity, i.e. only products are involved. This question is motivated by (my) research in physics. The second Pfaffian is related to Majorana fermions and fractional quantum Hall states (the Moore-Read state). The first Pfaffian is a slightly modified form of the second one. Thank you, Isidore","Recall Schur's Pfaffian identity: $$ \mathrm{Pf}\left(\frac{x_j-x_i}{x_j+x_i}\right)_{1\le i,j\le 2n} = \prod_{1\le i<j \le 2n}\frac{x_j-x_i}{x_j+x_i}. $$ Here $x_1,x_2\cdots x_{2n}$ are $2n$ variables, and $\mathrm{Pf}(A_{ij})$ stands for the Pfaffian of $2n$ by $2n$ skew-symmetric matrix $A$, whose $(i,j)$-th entry is $A_{ij}$. Now, I would like to know whether there is similar closed form for the following Pfaffian(s): $$ \mathrm{Pf}\left(\frac{x_j+x_i}{x_j-x_i}\right)_{1\le i,j\le 2n}, $$ and $$ \mathrm{Pf}\left(\frac{1}{x_j-x_i}\right)_{1\le i,j\le 2n}. $$ By closed form, I mean the expression resembling the RHS of Schur's Pfaffian identity, i.e. only products are involved. This question is motivated by (my) research in physics. The second Pfaffian is related to Majorana fermions and fractional quantum Hall states (the Moore-Read state). The first Pfaffian is a slightly modified form of the second one. Thank you, Isidore",,"['linear-algebra', 'combinatorics', 'symmetric-polynomials', 'pfaffian']"
78,How can I solve system of linear equations over finite fields in WolframAlpha?,How can I solve system of linear equations over finite fields in WolframAlpha?,,"Is it possible to solve system of linear equations over finite fields using Wolfram Alpha? If yes, how can I do that? Let us take a system $x+y+z=0$, $2x+y+2z=0$, $x+3y+z=0$. If I want to solve this linear system over $\mathbb Z_7=\mathbb Z/(7)$, one thing I can do is to type (x+y+z) mod 7=0, (2x+y+2z) mod 7=0, (x+3y+z) mod 7=0 into WA, here is a link . (This approach was suggested in an answer to this question .) WA can solve this, but a solution is given by enumerating the residue classes. I would prefer some more compact notation, for example something similar like what WA does for the same system over real numbers , i.e., when I type x+y+z=0, 2x+y+2z=0, x+3y+z=0 into WA. (Getting a basis for a solution space would be nice, but even expression in the form given in this case seems somehow better than listing all residue classes.) Would I be able to solve system linear equations over finite fields $G(p^n)$? (For example if I chose the representation $GF(4)=\mathbb Z_2/(x^2+x+1)$ and tried to solve the above system in this field?) Is it possible to obtain some kind of more compact notation at least in the case of fields of the form $\mathbb Z_p=\mathbb Z/(p)$ (where $p$ is some prime number)?","Is it possible to solve system of linear equations over finite fields using Wolfram Alpha? If yes, how can I do that? Let us take a system $x+y+z=0$, $2x+y+2z=0$, $x+3y+z=0$. If I want to solve this linear system over $\mathbb Z_7=\mathbb Z/(7)$, one thing I can do is to type (x+y+z) mod 7=0, (2x+y+2z) mod 7=0, (x+3y+z) mod 7=0 into WA, here is a link . (This approach was suggested in an answer to this question .) WA can solve this, but a solution is given by enumerating the residue classes. I would prefer some more compact notation, for example something similar like what WA does for the same system over real numbers , i.e., when I type x+y+z=0, 2x+y+2z=0, x+3y+z=0 into WA. (Getting a basis for a solution space would be nice, but even expression in the form given in this case seems somehow better than listing all residue classes.) Would I be able to solve system linear equations over finite fields $G(p^n)$? (For example if I chose the representation $GF(4)=\mathbb Z_2/(x^2+x+1)$ and tried to solve the above system in this field?) Is it possible to obtain some kind of more compact notation at least in the case of fields of the form $\mathbb Z_p=\mathbb Z/(p)$ (where $p$ is some prime number)?",,"['linear-algebra', 'modular-arithmetic', 'finite-fields', 'wolfram-alpha']"
79,The determinant of a special matrix,The determinant of a special matrix,,"Recently, I encounter the problem of calculating the determinant of the following matrix $$\left(\begin{array}{cccc} \sin(\theta_1) & \sin(\theta_1 + \delta_1) & \cdots & \sin(\theta_1 + (n-1) \delta_1)\\ \sin(\theta_2) & \sin(\theta_2 + \delta_2) & \cdots & \sin(\theta_2 + (n-1) \delta_2) \\ \vdots & \vdots & & \vdots \\\sin(\theta_n) & \sin(\theta_n + \delta_n) & \cdots & \sin(\theta_n + (n-1) \delta_n) \end{array}\right).$$ Is there an easy way to get its determinant? Thanks!","Recently, I encounter the problem of calculating the determinant of the following matrix $$\left(\begin{array}{cccc} \sin(\theta_1) & \sin(\theta_1 + \delta_1) & \cdots & \sin(\theta_1 + (n-1) \delta_1)\\ \sin(\theta_2) & \sin(\theta_2 + \delta_2) & \cdots & \sin(\theta_2 + (n-1) \delta_2) \\ \vdots & \vdots & & \vdots \\\sin(\theta_n) & \sin(\theta_n + \delta_n) & \cdots & \sin(\theta_n + (n-1) \delta_n) \end{array}\right).$$ Is there an easy way to get its determinant? Thanks!",,"['linear-algebra', 'matrices', 'determinant']"
80,Determinant vanishing over polynomial ring,Determinant vanishing over polynomial ring,,"Let $R=\mathbb C[t_1,\ldots,t_N]$ be a polynomial ring in some number of variables. Assume that $f_{ij}\in R$ are homogeneous linear polynomials for $1\le i,j\le n$. If $\det(f_{ij})=0$, I can consider this equation over $K:=\mathrm{Frac}(R)$ and get $\lambda_1,\ldots,\lambda_n\in K$, not all zero, with the property that  $$ \forall i:\quad \sum_{j=1}^n \lambda_j\cdot f_{ij} = 0 $$ Now, I can clear denominators and assume $\lambda_j\in R$. Since the $f_{ij}$ are homogeneous, I can also assume that the $\lambda_j$ are homogeneous. In a similar way, I can find homogeneous $\mu_1,\ldots,\mu_n\in R$ which are not all zero with $$ \forall j:\quad \sum_{i=1}^n \mu_i\cdot f_{ij} = 0$$ Question: Under what conditions can I choose either all the $\lambda_j$ or all the $\mu_i$ to be constant? Is this always the case?","Let $R=\mathbb C[t_1,\ldots,t_N]$ be a polynomial ring in some number of variables. Assume that $f_{ij}\in R$ are homogeneous linear polynomials for $1\le i,j\le n$. If $\det(f_{ij})=0$, I can consider this equation over $K:=\mathrm{Frac}(R)$ and get $\lambda_1,\ldots,\lambda_n\in K$, not all zero, with the property that  $$ \forall i:\quad \sum_{j=1}^n \lambda_j\cdot f_{ij} = 0 $$ Now, I can clear denominators and assume $\lambda_j\in R$. Since the $f_{ij}$ are homogeneous, I can also assume that the $\lambda_j$ are homogeneous. In a similar way, I can find homogeneous $\mu_1,\ldots,\mu_n\in R$ which are not all zero with $$ \forall j:\quad \sum_{i=1}^n \mu_i\cdot f_{ij} = 0$$ Question: Under what conditions can I choose either all the $\lambda_j$ or all the $\mu_i$ to be constant? Is this always the case?",,"['linear-algebra', 'abstract-algebra', 'polynomials', 'commutative-algebra', 'determinant']"
81,Is this a spectral decomposition/embedding/isometry?,Is this a spectral decomposition/embedding/isometry?,,"Given a symmetric p.s.d matrix G, we know that a gram matrix/inner-product representation, X exists where $G=X^TX$ and $X=U\lambda^{1/2}$ via the eigen decomposition of $G$. Now if I take the same matrix $G$ and find a matrix $V$ that minimizes $\sum_{i,j}G_{i,j}\langle v_i.,v_j.\rangle$ w.r.t $V$ where $V^T$ is constrained to be have $V^TDV=I$, where $D$ is a diagonal matrix to avoid the trivial zero matrix solution while minimizing this. Also, lets assume that $V$ is non-singular. Given that all entries are real, then: Basic questions: q1) Is $V$ obtained via an isometric(inner-product) preserving transformation? q2) Is $V$ a spectral decomposition? q3) How is $V$ different from $X$? q4) Any examples of where such $V$ can be useful? q5) Is this some sort of an 'Embedding' from a metric spaces/topology point of view where some relations in the p.s.d matrix $G$ are preserved, via inner-products? i.e, if $G_{ij}$ is large, the minimization makes sure that the inner product $\langle v_i,v_j \rangle$ is smaller in comparison to smaller $G_{ij}'s$ where the inner-product can be larger. You may assume positive semi-definiteness of $G$, if these questions make more sense in that case. Advanced question: Now if I said $v_i$ is instead the result of a vector-valued function $f_i(.)$ in a Reproducing Kernel Hilbert Space (RKHS) where $\sum_{i,j}G_{i,j}\langle v_i.,v_j.\rangle$ is minimized again under orthonormal constraints on $V$ and when the inner-product is the RKHS Norm, what is special or useful about this transformation? You may also assume that we use a regularizer under the RKHS norm on $f's$ as well, if you were worried about some sort of lack of smoothening or trivial solutions. Any where that this is applied? What are its properties from  a functional analysis or linear algebraic or spectral analysis or isometric transformations or Hilbert space point of view? What is happening here !? is what am looking for and the mentioned subjects are the only areas that i reasonably can attempt to understand or have at least studied till now.","Given a symmetric p.s.d matrix G, we know that a gram matrix/inner-product representation, X exists where $G=X^TX$ and $X=U\lambda^{1/2}$ via the eigen decomposition of $G$. Now if I take the same matrix $G$ and find a matrix $V$ that minimizes $\sum_{i,j}G_{i,j}\langle v_i.,v_j.\rangle$ w.r.t $V$ where $V^T$ is constrained to be have $V^TDV=I$, where $D$ is a diagonal matrix to avoid the trivial zero matrix solution while minimizing this. Also, lets assume that $V$ is non-singular. Given that all entries are real, then: Basic questions: q1) Is $V$ obtained via an isometric(inner-product) preserving transformation? q2) Is $V$ a spectral decomposition? q3) How is $V$ different from $X$? q4) Any examples of where such $V$ can be useful? q5) Is this some sort of an 'Embedding' from a metric spaces/topology point of view where some relations in the p.s.d matrix $G$ are preserved, via inner-products? i.e, if $G_{ij}$ is large, the minimization makes sure that the inner product $\langle v_i,v_j \rangle$ is smaller in comparison to smaller $G_{ij}'s$ where the inner-product can be larger. You may assume positive semi-definiteness of $G$, if these questions make more sense in that case. Advanced question: Now if I said $v_i$ is instead the result of a vector-valued function $f_i(.)$ in a Reproducing Kernel Hilbert Space (RKHS) where $\sum_{i,j}G_{i,j}\langle v_i.,v_j.\rangle$ is minimized again under orthonormal constraints on $V$ and when the inner-product is the RKHS Norm, what is special or useful about this transformation? You may also assume that we use a regularizer under the RKHS norm on $f's$ as well, if you were worried about some sort of lack of smoothening or trivial solutions. Any where that this is applied? What are its properties from  a functional analysis or linear algebraic or spectral analysis or isometric transformations or Hilbert space point of view? What is happening here !? is what am looking for and the mentioned subjects are the only areas that i reasonably can attempt to understand or have at least studied till now.",,"['linear-algebra', 'general-topology', 'functional-analysis', 'fourier-analysis', 'hilbert-spaces']"
82,Invariant of matrix under elementary transformations,Invariant of matrix under elementary transformations,,"$\DeclareMathOperator{\rank}{rank}$ Let $A \in \mathbb R^{n \times n}$, $b \in \mathbb R^n$, $c \in \mathbb R$. Consider the following matrix $$    B = \begin{bmatrix} A & b \\ b^T & c \end{bmatrix} \in \mathbb R^{(n+1) \times (n+1)}. $$ Let $a \in \mathbb R^n$ be an arbitrary vector. It generates the elementary transformation matrix $T_a$: $$    T_a = \begin{bmatrix} I & a \\ 0 & 1 \end{bmatrix}, \;\;\; I = \left.\begin{bmatrix} 1 & 0 & \cdots & 0 \\ 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1  \end{bmatrix}\right\} n $$ If we denote $\widetilde B = T_a^T B T_a$ then  $$    \widetilde B = \begin{bmatrix} A & b+Aa \\ (b+Aa)^T & a^T A a+ 2b^T a + c \end{bmatrix}. $$ Now let $\chi_B(\lambda) = (-\lambda)^{n+1} + I_0(B)(-\lambda)^{n} + \ldots + I_n(B)$ be characteristical polynomial of matrix $B$. Since $\det T_a = 1$ we have $I_n(B)=I_n(\widetilde B)$. Question. If $\det A = 0$, $\rank B \leqslant \rank A + 1$ is it possible to say that for general $a$ for some $0<k<n$ the equality $I_k(B) = I_k(\widetilde B)$ holds? Geometrical meaning. Matrix $B$ corresponds to the equation of the second order algebraic hypersurface in $\mathbb R^n$: $$    \begin{bmatrix} x & 1 \end{bmatrix} \begin{bmatrix} A & b \\ b^T & c \end{bmatrix} \begin{bmatrix} x \\ 1 \end{bmatrix} = 0, $$ that can also be written in the form $x^T Ax + 2b^T x + c = 0$. If we make a change of coordinates $x = y+a$ then we will obtain an equation  $$    y^T A y + 2(b+Aa)^T x + a^T A a + 2b^T a + c = 0 $$ that can be written in the form $$ \begin{bmatrix} y & 1 \end{bmatrix} \begin{bmatrix} A & b+Aa \\ (b+Aa)^T & a^T A a+ 2b^T a + c \end{bmatrix}\begin{bmatrix} y \\ 1 \end{bmatrix} = 0. $$ My question then is if some $k$, $0<k<n$, $I_k$ is an invariant of second-order algebraic hypersurface under translation transformations $x \mapsto x+a$. In general it isn't true, but under assumptions $\det A = 0$, $\rank B \leqslant \rank A + 1$ I don't know the answer.","$\DeclareMathOperator{\rank}{rank}$ Let $A \in \mathbb R^{n \times n}$, $b \in \mathbb R^n$, $c \in \mathbb R$. Consider the following matrix $$    B = \begin{bmatrix} A & b \\ b^T & c \end{bmatrix} \in \mathbb R^{(n+1) \times (n+1)}. $$ Let $a \in \mathbb R^n$ be an arbitrary vector. It generates the elementary transformation matrix $T_a$: $$    T_a = \begin{bmatrix} I & a \\ 0 & 1 \end{bmatrix}, \;\;\; I = \left.\begin{bmatrix} 1 & 0 & \cdots & 0 \\ 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1  \end{bmatrix}\right\} n $$ If we denote $\widetilde B = T_a^T B T_a$ then  $$    \widetilde B = \begin{bmatrix} A & b+Aa \\ (b+Aa)^T & a^T A a+ 2b^T a + c \end{bmatrix}. $$ Now let $\chi_B(\lambda) = (-\lambda)^{n+1} + I_0(B)(-\lambda)^{n} + \ldots + I_n(B)$ be characteristical polynomial of matrix $B$. Since $\det T_a = 1$ we have $I_n(B)=I_n(\widetilde B)$. Question. If $\det A = 0$, $\rank B \leqslant \rank A + 1$ is it possible to say that for general $a$ for some $0<k<n$ the equality $I_k(B) = I_k(\widetilde B)$ holds? Geometrical meaning. Matrix $B$ corresponds to the equation of the second order algebraic hypersurface in $\mathbb R^n$: $$    \begin{bmatrix} x & 1 \end{bmatrix} \begin{bmatrix} A & b \\ b^T & c \end{bmatrix} \begin{bmatrix} x \\ 1 \end{bmatrix} = 0, $$ that can also be written in the form $x^T Ax + 2b^T x + c = 0$. If we make a change of coordinates $x = y+a$ then we will obtain an equation  $$    y^T A y + 2(b+Aa)^T x + a^T A a + 2b^T a + c = 0 $$ that can be written in the form $$ \begin{bmatrix} y & 1 \end{bmatrix} \begin{bmatrix} A & b+Aa \\ (b+Aa)^T & a^T A a+ 2b^T a + c \end{bmatrix}\begin{bmatrix} y \\ 1 \end{bmatrix} = 0. $$ My question then is if some $k$, $0<k<n$, $I_k$ is an invariant of second-order algebraic hypersurface under translation transformations $x \mapsto x+a$. In general it isn't true, but under assumptions $\det A = 0$, $\rank B \leqslant \rank A + 1$ I don't know the answer.",,"['linear-algebra', 'geometry', 'matrices']"
83,Linear Independence Game,Linear Independence Game,,"Suppose you have a set $X$ of vectors in $\mathbb{F}_2^n$, with $|X| \ge n+1$, and consider the following game. On their turn, each player (2 player game) chooses from $X$ one vector and sets it aside in their pile $P_i$ (we also remove this vector from $X$). If at the end of their turn, there exists $m$ vectors in $P_i$ ($m \le n$) that are linearly dependent, then player $i$ loses. I was wondering if there has been any work on matrix/nim games such as these, and any papers or suggestions would helpful. Edit: I am only interested in the case where $X$ results in no draws. A game I am particularly interested is the following. Let $X$ contain only the vectors with everything $0$ except for $2$ positions that have $1$'s ($n \choose 2$ of them). Now play the following game above, and set $m=3$. Note I am also interested if there is a way to convert, or embed such a game into an impartial game (So Sprague-Grundy applies).","Suppose you have a set $X$ of vectors in $\mathbb{F}_2^n$, with $|X| \ge n+1$, and consider the following game. On their turn, each player (2 player game) chooses from $X$ one vector and sets it aside in their pile $P_i$ (we also remove this vector from $X$). If at the end of their turn, there exists $m$ vectors in $P_i$ ($m \le n$) that are linearly dependent, then player $i$ loses. I was wondering if there has been any work on matrix/nim games such as these, and any papers or suggestions would helpful. Edit: I am only interested in the case where $X$ results in no draws. A game I am particularly interested is the following. Let $X$ contain only the vectors with everything $0$ except for $2$ positions that have $1$'s ($n \choose 2$ of them). Now play the following game above, and set $m=3$. Note I am also interested if there is a way to convert, or embed such a game into an impartial game (So Sprague-Grundy applies).",,"['linear-algebra', 'combinatorics', 'game-theory', 'combinatorial-game-theory', 'combinatorial-geometry']"
84,Low-rank approximation to the Graph Laplacian matrix of a regular grid.,Low-rank approximation to the Graph Laplacian matrix of a regular grid.,,"As mentioned in the title, does anybody know any methods of efficient low-rank approximation $LL^T$ to the Graph Laplacian matrix $A$ corresponding to a square lattice? (except PCA)","As mentioned in the title, does anybody know any methods of efficient low-rank approximation $LL^T$ to the Graph Laplacian matrix $A$ corresponding to a square lattice? (except PCA)",,"['linear-algebra', 'matrices', 'graph-theory', 'approximation', 'graph-laplacian']"
85,Solving a system of linear inequalities,Solving a system of linear inequalities,,"I have a personal problem I want to solve. I have a system of linear inequalities with 97 unknowns and 150000 ineqaulities, I think formal notation of the problem should be something like this \begin{equation}     \begin{bmatrix}         x_{00000101}  & \cdots & x_{00000197}\\         \vdots  & \ddots &\vdots\\         x_{15000001} &  \cdots & x_{15000097}\\     \end{bmatrix}     \times \left[ \begin{array}{c} y_1 \\ \vdots \\ y_{97} \end{array} \right] =     \left[ \begin{array}{c} z_1 \\ \vdots \\ z_{150000} \end{array} \right] \end{equation} \begin{equation} \forall z \in \left\{ z_i | i \in [1,150000]\right\}, z_i > 0 \end{equation} We also know that $\forall x \in $ leftmost matrix $x \in \left\{-1,0,1 \right\}$ (i.e coefficients can only take -1,0,1 as values) and there is exacly $5$ $1$s and $5$ $-1$'s in every row in leftmost matrix. So, each row sums up to $0$. What I want to do is to determine if I can order $y_i$'s and if so, I want to find an algorithm to order them. I am considering using a genetic algorithm and by trying arbitrary values for every $y_i$. But I guess that could be computationaly expensive. Is there a way I can deterministically solve this problem? Some background information: every $y_i$ represents a player in a game, I have a data of 150000 matches of 5 player vs 5 player. I am assuming team's ability to win is sum of the abilities of its players. So I am wondering if I can order the players to best to worse.","I have a personal problem I want to solve. I have a system of linear inequalities with 97 unknowns and 150000 ineqaulities, I think formal notation of the problem should be something like this \begin{equation}     \begin{bmatrix}         x_{00000101}  & \cdots & x_{00000197}\\         \vdots  & \ddots &\vdots\\         x_{15000001} &  \cdots & x_{15000097}\\     \end{bmatrix}     \times \left[ \begin{array}{c} y_1 \\ \vdots \\ y_{97} \end{array} \right] =     \left[ \begin{array}{c} z_1 \\ \vdots \\ z_{150000} \end{array} \right] \end{equation} \begin{equation} \forall z \in \left\{ z_i | i \in [1,150000]\right\}, z_i > 0 \end{equation} We also know that $\forall x \in $ leftmost matrix $x \in \left\{-1,0,1 \right\}$ (i.e coefficients can only take -1,0,1 as values) and there is exacly $5$ $1$s and $5$ $-1$'s in every row in leftmost matrix. So, each row sums up to $0$. What I want to do is to determine if I can order $y_i$'s and if so, I want to find an algorithm to order them. I am considering using a genetic algorithm and by trying arbitrary values for every $y_i$. But I guess that could be computationaly expensive. Is there a way I can deterministically solve this problem? Some background information: every $y_i$ represents a player in a game, I have a data of 150000 matches of 5 player vs 5 player. I am assuming team's ability to win is sum of the abilities of its players. So I am wondering if I can order the players to best to worse.",,"['linear-algebra', 'inequality']"
86,"What is rank of $f(A)$, where $f$ is the minimal polynomial of $A$?","What is rank of , where  is the minimal polynomial of ?",f(A) f A,If $f(x)$ is minimal polynomial of the $4\times 4$ matrix $$A=\begin{pmatrix} 0 &0 &0& 1\\ 1 &0 &0 &0\\	 0 &1 &0 &0\\ 0 &0 &1 &0	 \end{pmatrix}$$ Then what is rank of $f(A)$? I think $f(A)$ will be a zero matrix so its rank is 0. Am I right?,If $f(x)$ is minimal polynomial of the $4\times 4$ matrix $$A=\begin{pmatrix} 0 &0 &0& 1\\ 1 &0 &0 &0\\	 0 &1 &0 &0\\ 0 &0 &1 &0	 \end{pmatrix}$$ Then what is rank of $f(A)$? I think $f(A)$ will be a zero matrix so its rank is 0. Am I right?,,['linear-algebra']
87,Curvature of particular Riemannian metric,Curvature of particular Riemannian metric,,"Let $U = \{ (x_1, \dots, x_n) \mid x_j > 0 \text{ for all } j\}$ and let $\|x\|^2 = \sum_j x_j^2$. The function $x \mapsto -\log \|x\|^2$ is strictly convex on $U$ and thus defines a Riemannian metric $g$ on that space. Up to a factor of $\tfrac 12$ that we don't care about, this metric is $$ g = \frac 1{\|x\|^4} \tau - \frac 1{\|x\|^2} g_{st}, $$ where $\tau$ is the tensor described by the matrix $\tau = (x_jx_k)_{1 \leq j \leq n, 1 \leq k \leq n}$ in these coordinates, and $g_{st}$ is the standard Euclidean metric. I want to know the sectional curvature of $g$ (and eventually of metrics that are perturbations of $g$). Does anyone know a nice trick or method to calculate this?","Let $U = \{ (x_1, \dots, x_n) \mid x_j > 0 \text{ for all } j\}$ and let $\|x\|^2 = \sum_j x_j^2$. The function $x \mapsto -\log \|x\|^2$ is strictly convex on $U$ and thus defines a Riemannian metric $g$ on that space. Up to a factor of $\tfrac 12$ that we don't care about, this metric is $$ g = \frac 1{\|x\|^4} \tau - \frac 1{\|x\|^2} g_{st}, $$ where $\tau$ is the tensor described by the matrix $\tau = (x_jx_k)_{1 \leq j \leq n, 1 \leq k \leq n}$ in these coordinates, and $g_{st}$ is the standard Euclidean metric. I want to know the sectional curvature of $g$ (and eventually of metrics that are perturbations of $g$). Does anyone know a nice trick or method to calculate this?",,"['linear-algebra', 'differential-geometry', 'riemannian-geometry']"
88,quadratic and bilinear forms,quadratic and bilinear forms,,"Does a quadratic form always come from symmetric bilinear form ? We know when $q(x)=b(x,x)$ where $q$ is a quadratic form and $b$ is a symmetric bilinear form. But when we just take a bilinear form and $b(x,y)$ and write $x$ instead of $y$,does it give us a quadratic form ?","Does a quadratic form always come from symmetric bilinear form ? We know when $q(x)=b(x,x)$ where $q$ is a quadratic form and $b$ is a symmetric bilinear form. But when we just take a bilinear form and $b(x,y)$ and write $x$ instead of $y$,does it give us a quadratic form ?",,"['linear-algebra', 'bilinear-form']"
89,Finding vector subspaces,Finding vector subspaces,,"I've got this problem: Let $H = \left \{ x \in \mathbb{R}^{4} \, \left| \, x_2 - x_3 + 2x_4 = 0 \right. \right \}$ Find, if possible, $a \in \mathbb{R}$ and $S, T$ vector subspaces so that $\dim(S) = \dim(T)$, $S + T^\perp = H$, $S \cap T = \left \langle (1, a, 0, -1) \right \rangle$ What I have is: Using the dimension theorem for vector spaces: $\dim(S+T^\perp) = \dim(S) + \dim(T^\perp) - \dim(S \cap T^\perp) = \dim(H)$. Since $H$ is a $\mathbb{R}^{4}$ vector subspace with one equation, $\dim(H) = 3$. So $\dim(S) = 2$, $\dim(T^\perp) = 2$ and $\dim(S \cap T^\perp)=1$. If $\dim(T^\perp) = 2$, then $\dim(T)$ must be 2 as well. So I've got $S=\left \langle s_1, s_2 \right \rangle$ and $T=\left \langle t_1, t_2 \right \rangle$ Let $s_1, s_2$ two linearly independent vectors from subspace $H$. Suppose $s_1 = (0,1,1,0), s_2 = (0,0,2,1)$. Then $S=\left \langle (0,1,1,0),(0,0,2,1) \right \rangle$. Let $t_1, t_2$ two linearly independent vectors from subspace $H$. Suppose $t_1 = (0,-2,0,1), t_2=(1,-1,1,1)$. Then $T^\perp=\left \langle (0,-2,0,1),(1,-1,1,1) \right \rangle$ Because $(T^\perp)^\perp = T \rightarrow T=\left \{ x \in \mathbb{R}^{4} / -2x_2 + x_4 = x_1 - x_2 + x_3 + x_4 = 0 \right \}$ S and T satisfies all the conditions the problem asks. I know how to find $S \cap T$, but I'm a bit disappointed finding $a$. Any suggestion would be appreciated! Thanks in advance!","I've got this problem: Let $H = \left \{ x \in \mathbb{R}^{4} \, \left| \, x_2 - x_3 + 2x_4 = 0 \right. \right \}$ Find, if possible, $a \in \mathbb{R}$ and $S, T$ vector subspaces so that $\dim(S) = \dim(T)$, $S + T^\perp = H$, $S \cap T = \left \langle (1, a, 0, -1) \right \rangle$ What I have is: Using the dimension theorem for vector spaces: $\dim(S+T^\perp) = \dim(S) + \dim(T^\perp) - \dim(S \cap T^\perp) = \dim(H)$. Since $H$ is a $\mathbb{R}^{4}$ vector subspace with one equation, $\dim(H) = 3$. So $\dim(S) = 2$, $\dim(T^\perp) = 2$ and $\dim(S \cap T^\perp)=1$. If $\dim(T^\perp) = 2$, then $\dim(T)$ must be 2 as well. So I've got $S=\left \langle s_1, s_2 \right \rangle$ and $T=\left \langle t_1, t_2 \right \rangle$ Let $s_1, s_2$ two linearly independent vectors from subspace $H$. Suppose $s_1 = (0,1,1,0), s_2 = (0,0,2,1)$. Then $S=\left \langle (0,1,1,0),(0,0,2,1) \right \rangle$. Let $t_1, t_2$ two linearly independent vectors from subspace $H$. Suppose $t_1 = (0,-2,0,1), t_2=(1,-1,1,1)$. Then $T^\perp=\left \langle (0,-2,0,1),(1,-1,1,1) \right \rangle$ Because $(T^\perp)^\perp = T \rightarrow T=\left \{ x \in \mathbb{R}^{4} / -2x_2 + x_4 = x_1 - x_2 + x_3 + x_4 = 0 \right \}$ S and T satisfies all the conditions the problem asks. I know how to find $S \cap T$, but I'm a bit disappointed finding $a$. Any suggestion would be appreciated! Thanks in advance!",,['linear-algebra']
90,Positive definite completion of a matrix,Positive definite completion of a matrix,,"Suppose we have a real, symmetric matrix $A(x_1,x_2,x_3)$ given by \begin{pmatrix} a_{1,1} & a_{1,2} & x_1 & x_2 \\ a_{2,1} & a_{2,2} & a_{2,3} & x_3 \\ x_1 & a_{3,2} & a_{3,3} & a_{3,4} \\ x_2 & x_3 & a_{3,4} & a_{4,4} \end{pmatrix} We would like to complete this matrix into a positive definite matrix by choosing appropriate values for $x_1,x_2, x_3$. (Assume that the $a$ values permit this by themselves). Many completions are possible. However, only one unique completion is possible such that its inverse is of the form  \begin{pmatrix} b_{1,1} & b_{1,2} & 0 & 0 \\ b_{2,1} & b_{2,2} & b_{2,3} & 0 \\ 0 & b_{3,2} & b_{3,3} & b_{3,4} \\ 0 & 0 & b_{3,4} & b_{4,4} \end{pmatrix} i.e it has zeroes in the positions corresponding to $x_1, x_2, x_3$. Furthermore, apparently these values of $x_1,x_2,x_3$ end up maximizing the determinant of $A$. How do we prove this, and what are the corresponding values of $x_1, x_2, x_3$? The relationship between the completion and maximizing determinant seemed clear since the conditions on the inverse are equivalent to the partial derivatives of the determinant function being zero. But I am unable to establish uniqueness.","Suppose we have a real, symmetric matrix $A(x_1,x_2,x_3)$ given by \begin{pmatrix} a_{1,1} & a_{1,2} & x_1 & x_2 \\ a_{2,1} & a_{2,2} & a_{2,3} & x_3 \\ x_1 & a_{3,2} & a_{3,3} & a_{3,4} \\ x_2 & x_3 & a_{3,4} & a_{4,4} \end{pmatrix} We would like to complete this matrix into a positive definite matrix by choosing appropriate values for $x_1,x_2, x_3$. (Assume that the $a$ values permit this by themselves). Many completions are possible. However, only one unique completion is possible such that its inverse is of the form  \begin{pmatrix} b_{1,1} & b_{1,2} & 0 & 0 \\ b_{2,1} & b_{2,2} & b_{2,3} & 0 \\ 0 & b_{3,2} & b_{3,3} & b_{3,4} \\ 0 & 0 & b_{3,4} & b_{4,4} \end{pmatrix} i.e it has zeroes in the positions corresponding to $x_1, x_2, x_3$. Furthermore, apparently these values of $x_1,x_2,x_3$ end up maximizing the determinant of $A$. How do we prove this, and what are the corresponding values of $x_1, x_2, x_3$? The relationship between the completion and maximizing determinant seemed clear since the conditions on the inverse are equivalent to the partial derivatives of the determinant function being zero. But I am unable to establish uniqueness.",,"['linear-algebra', 'matrices', 'polynomials', 'matrix-completion']"
91,Smith normal form of powers of a matrix,Smith normal form of powers of a matrix,,I have been trying to compute the cokernels of powers of integer matrices using Smith normal form. When is $\text{Smith}(A^2) = (\text{Smith}(A))^2$ and more generally when is $\text{Smith}(A^n) = (\text{Smith}(A))^n$ ? Some numerical experiments showed that this is not always true. Is there at least a good sufficient condition for determining when such a formula holds? Thanks!,I have been trying to compute the cokernels of powers of integer matrices using Smith normal form. When is and more generally when is ? Some numerical experiments showed that this is not always true. Is there at least a good sufficient condition for determining when such a formula holds? Thanks!,\text{Smith}(A^2) = (\text{Smith}(A))^2 \text{Smith}(A^n) = (\text{Smith}(A))^n,"['linear-algebra', 'abstract-algebra', 'matrices', 'smith-normal-form']"
92,Identities for other coefficients of the characteristic polynomial,Identities for other coefficients of the characteristic polynomial,,We have $\operatorname{tr}(A+B) = \operatorname{tr}(A) + \operatorname{tr}(B)$ and $\det(AB) = \det(A) \det(B)$.  Are there any analogous identities for the other coefficients of the characteristic polynomial?,We have $\operatorname{tr}(A+B) = \operatorname{tr}(A) + \operatorname{tr}(B)$ and $\det(AB) = \det(A) \det(B)$.  Are there any analogous identities for the other coefficients of the characteristic polynomial?,,"['linear-algebra', 'matrices']"
93,Piecewise Affine Bijections of $\mathbb{R}^n$,Piecewise Affine Bijections of,\mathbb{R}^n,"I have a min-max function $f:\mathbb{R}^n\to\mathbb{R}^n$ of the form $$f(x) = \min_{i=1,\dots,n}\max_{j=1,\dots,n}(\alpha_{ij}^Tx + \beta_{ij})\quad\text{where each } \alpha_{ij}\in \mathbb{R}^{n\times n}\text{ and }\beta_{ij}\in\mathbb{R}^n.$$ I'm looking for a sufficient condition(s) under which $f$ is a bijective function of $x$. To that end, I've found a paper on Piecewise Affine Bijections of $\mathbb{R}^n$ by Dieter Kuhn and Rainer Löwen 1987, in which there are a few theorems on necessary and sufficient conditions for a piecewise affine function $f$ on $\mathbb{R}^n$ to be bijective. However, these theorems require certain properties of $f$ which are not so easy to verify. One of the theorems states that: if $f$ is proper, then it is bijective if and only if $f$ is injective. So I don't know how we can show that $f$ is injective! I appreciate any hint or suggestion.","I have a min-max function $f:\mathbb{R}^n\to\mathbb{R}^n$ of the form $$f(x) = \min_{i=1,\dots,n}\max_{j=1,\dots,n}(\alpha_{ij}^Tx + \beta_{ij})\quad\text{where each } \alpha_{ij}\in \mathbb{R}^{n\times n}\text{ and }\beta_{ij}\in\mathbb{R}^n.$$ I'm looking for a sufficient condition(s) under which $f$ is a bijective function of $x$. To that end, I've found a paper on Piecewise Affine Bijections of $\mathbb{R}^n$ by Dieter Kuhn and Rainer Löwen 1987, in which there are a few theorems on necessary and sufficient conditions for a piecewise affine function $f$ on $\mathbb{R}^n$ to be bijective. However, these theorems require certain properties of $f$ which are not so easy to verify. One of the theorems states that: if $f$ is proper, then it is bijective if and only if $f$ is injective. So I don't know how we can show that $f$ is injective! I appreciate any hint or suggestion.",,"['linear-algebra', 'general-topology']"
94,Isomorphism of representations of the symmetric group,Isomorphism of representations of the symmetric group,,"This might be a silly question, but I don't understand why the solution to the following problem implies the result: Let $A = \mathbb{C}S_d$ and let $c_{\lambda}$ denote the Young symmetrizer (with $c_{\lambda} = a_{\lambda}b_{\lambda}$). The problem asks to show that $Aa_{\lambda}b_{\lambda} \cong Ab_{\lambda}a_{\lambda}$. The solution says to consider the maps from $Aa_{\lambda}b_{\lambda} \to Ab_{\lambda}a_{\lambda} \to Aa_{\lambda}b_{\lambda}$ given by multiplication by $a_\lambda$ and $b_\lambda$ respectively. The composite is a scalar multiplication on a vector space (since $c_\lambda$ is idempotent) and hence an isomorphism. I understand all this argument, but how does it follow from this that implies the needed isomorphism? A composite map on an vector space that is an isomorphism certainly does not imply that the component maps are isomorphisms... edit: Also, it seems that every element $Aa_\lambda b_\lambda$ can be written as $e_g e_p e_q$ where $e_g \in A$, $e_p$ and $e_q$ are components of the sums $a_\lambda$ and $b_\lambda$. So can I just write an explicit map $e_g e_p e_q \mapsto e_g e_q e_p$? Is it right to sa that this doesn't work because this is a bijection but not an isomorphism?","This might be a silly question, but I don't understand why the solution to the following problem implies the result: Let $A = \mathbb{C}S_d$ and let $c_{\lambda}$ denote the Young symmetrizer (with $c_{\lambda} = a_{\lambda}b_{\lambda}$). The problem asks to show that $Aa_{\lambda}b_{\lambda} \cong Ab_{\lambda}a_{\lambda}$. The solution says to consider the maps from $Aa_{\lambda}b_{\lambda} \to Ab_{\lambda}a_{\lambda} \to Aa_{\lambda}b_{\lambda}$ given by multiplication by $a_\lambda$ and $b_\lambda$ respectively. The composite is a scalar multiplication on a vector space (since $c_\lambda$ is idempotent) and hence an isomorphism. I understand all this argument, but how does it follow from this that implies the needed isomorphism? A composite map on an vector space that is an isomorphism certainly does not imply that the component maps are isomorphisms... edit: Also, it seems that every element $Aa_\lambda b_\lambda$ can be written as $e_g e_p e_q$ where $e_g \in A$, $e_p$ and $e_q$ are components of the sums $a_\lambda$ and $b_\lambda$. So can I just write an explicit map $e_g e_p e_q \mapsto e_g e_q e_p$? Is it right to sa that this doesn't work because this is a bijection but not an isomorphism?",,"['linear-algebra', 'representation-theory']"
95,The Space of Bilinear Forms,The Space of Bilinear Forms,,"Background Consider the set of all bilinear forms $\text{Bil}_F(X \times Y)$ over finite dimensional vector spaces $X$ and $Y$ with common ground field F. If we choose bases for $(u_1, \dots, u_m)$ for $X$ and $(v_1, \dots , v_m)$ for Y, a basis for the tensor product of $X$ and $Y$ is given by $\{u^i \otimes v^j | i = 1, \dots, m; j=1, \dots n \}$ where $u^i$ and $v^i$ denote the respective dual bases. Now, it is a theorem that the tensor product  $X \bigotimes Y$ is isomorphic to $\text{Bil}_F(X \times Y)$ and  counting the $m \cdot n$ basis elements of  $X \bigotimes Y$ we know that the tensor product is isomorphic to the set of all $m \times n$ matrices over $F$, $\mathcal{M}^m_n(F)$ which also has $m \cdot n$ basis elements and therefore, $\text{Bil}_F(X \times Y) \approx \mathcal{M}^m_n(F)$ Question Now, my question is: Assuming the above argument makes sense, this seems like a lot of technology to prove this isomorphism. Is there a better/more direct way to do this that doesn't involve, for example, constructions such as the tensor product? Attempt I know that there is a unique matrix associated with given bilinear form that has entries determined by $B_{ij} = B(u_i, v_j)$ and this insures that one can, in principle, construct at least an injection between $\text{Bil}_F(X \times Y)$ and $\mathcal{M}^m_n(F)$. Also, given any matrix $A$, the mapping $B(x,y) = x^T A y$ is a bilinear form. It seems that a good candidate for an isomorphism might be $$ \phi(A)(x,y) = x^T A y $$ but it's not clear to me how this is necessarily a bijection.","Background Consider the set of all bilinear forms $\text{Bil}_F(X \times Y)$ over finite dimensional vector spaces $X$ and $Y$ with common ground field F. If we choose bases for $(u_1, \dots, u_m)$ for $X$ and $(v_1, \dots , v_m)$ for Y, a basis for the tensor product of $X$ and $Y$ is given by $\{u^i \otimes v^j | i = 1, \dots, m; j=1, \dots n \}$ where $u^i$ and $v^i$ denote the respective dual bases. Now, it is a theorem that the tensor product  $X \bigotimes Y$ is isomorphic to $\text{Bil}_F(X \times Y)$ and  counting the $m \cdot n$ basis elements of  $X \bigotimes Y$ we know that the tensor product is isomorphic to the set of all $m \times n$ matrices over $F$, $\mathcal{M}^m_n(F)$ which also has $m \cdot n$ basis elements and therefore, $\text{Bil}_F(X \times Y) \approx \mathcal{M}^m_n(F)$ Question Now, my question is: Assuming the above argument makes sense, this seems like a lot of technology to prove this isomorphism. Is there a better/more direct way to do this that doesn't involve, for example, constructions such as the tensor product? Attempt I know that there is a unique matrix associated with given bilinear form that has entries determined by $B_{ij} = B(u_i, v_j)$ and this insures that one can, in principle, construct at least an injection between $\text{Bil}_F(X \times Y)$ and $\mathcal{M}^m_n(F)$. Also, given any matrix $A$, the mapping $B(x,y) = x^T A y$ is a bilinear form. It seems that a good candidate for an isomorphism might be $$ \phi(A)(x,y) = x^T A y $$ but it's not clear to me how this is necessarily a bijection.",,"['linear-algebra', 'bilinear-form']"
96,Is there a Mazur–Ulam theorem equivalent for vector spaces over finite fields?,Is there a Mazur–Ulam theorem equivalent for vector spaces over finite fields?,,"I know that Mazur–Ulam theorem holds for normed linear spaces over $\mathbb{R}$. I wanted to know whether under some ""weak"" conditions on the map $f$, can we have Mazur-Ulam  theorem for a vector space over ${\mathbb{F}_2}$?. I apologize for being vague about the ""weak"" condition. More generally, I am interested in characterizing the isometries of $\mathbb{F}_2 ^n$ with Hamming distance as the metric. Clearly permutation matrices and translations are isometries. But I wanted to know if there are isometries other than these? Thank you, Iso","I know that Mazur–Ulam theorem holds for normed linear spaces over $\mathbb{R}$. I wanted to know whether under some ""weak"" conditions on the map $f$, can we have Mazur-Ulam  theorem for a vector space over ${\mathbb{F}_2}$?. I apologize for being vague about the ""weak"" condition. More generally, I am interested in characterizing the isometries of $\mathbb{F}_2 ^n$ with Hamming distance as the metric. Clearly permutation matrices and translations are isometries. But I wanted to know if there are isometries other than these? Thank you, Iso",,"['linear-algebra', 'abstract-algebra', 'coding-theory']"
97,linear algebra basic proof,linear algebra basic proof,,"F is a linear functional in $V'$ a linear vector space which operates on $\phi\in V$. Show that there is a one-to one correspondence between F and $f\in V $ such that $F(\phi)=(f,\phi)$ where $V$ is also a linear vector space.  $(,)$ represents inner product. Any hint will be helpful. Attempt: I don't even know if this is right but... If $F(\phi)=c$ , $c\in \mathbb{C}$ then let $(f_1, \phi)=c$ and $(f_2, \phi)=c$ $\rightarrow$ $(f_1-f_2, \phi)=0$ But this only means orthogonality. EDIT: so, from hint let $\{e_i\}$ be a linearly independent basis for vectorspace $V$ $F(\Phi) = F (\sum_n \phi_n e_n) =\sum_n \phi_n F(e_n) $ $(f,\Phi) = \sum_n f_n^* \phi_n$ Therefore if $F(\phi)=c$ , $c\in \mathbb{C}$ and $(f,\Phi)=c$ then $\sum_n (f_n^* -F(e_n)) \phi_n =0$ implies (?) $f_n^* =F(e_n)$ $\forall n$? EDIT Ok. So the above implies that if some $(e_i,\Phi)$ vanishes then we cannot have a unique $f$. So is this solved by specifying at the beginning that $\{e_i\}$ is a basis where $\phi_i=(e_i,\Phi)$ is non vanishing for all i?","F is a linear functional in $V'$ a linear vector space which operates on $\phi\in V$. Show that there is a one-to one correspondence between F and $f\in V $ such that $F(\phi)=(f,\phi)$ where $V$ is also a linear vector space.  $(,)$ represents inner product. Any hint will be helpful. Attempt: I don't even know if this is right but... If $F(\phi)=c$ , $c\in \mathbb{C}$ then let $(f_1, \phi)=c$ and $(f_2, \phi)=c$ $\rightarrow$ $(f_1-f_2, \phi)=0$ But this only means orthogonality. EDIT: so, from hint let $\{e_i\}$ be a linearly independent basis for vectorspace $V$ $F(\Phi) = F (\sum_n \phi_n e_n) =\sum_n \phi_n F(e_n) $ $(f,\Phi) = \sum_n f_n^* \phi_n$ Therefore if $F(\phi)=c$ , $c\in \mathbb{C}$ and $(f,\Phi)=c$ then $\sum_n (f_n^* -F(e_n)) \phi_n =0$ implies (?) $f_n^* =F(e_n)$ $\forall n$? EDIT Ok. So the above implies that if some $(e_i,\Phi)$ vanishes then we cannot have a unique $f$. So is this solved by specifying at the beginning that $\{e_i\}$ is a basis where $\phi_i=(e_i,\Phi)$ is non vanishing for all i?",,[]
98,Find all $\mathbb{Q}$-linear maps $\phi : \mathbb{Q}[x]\to\mathbb{Q}[x]$ that send irreducible polynomials to irreducible polynomials,Find all -linear maps  that send irreducible polynomials to irreducible polynomials,\mathbb{Q} \phi : \mathbb{Q}[x]\to\mathbb{Q}[x],"Let $\mathbb{Q}[x]$ denote the vector space over $\mathbb{Q}$ of polynomials with rational coefficients in $x$ . Find all $\mathbb{Q}$ -linear maps $\phi : \mathbb{Q}[x]\to\mathbb{Q}[x]$ that send irreducible polynomials to irreducible polynomials. If $\mathbb{K}$ is a field over which a vector space $V$ is defined and $W$ is a vector space, a $\mathbb{K}$ -linear map $T :V \to W$ satisfies $T(cx+y) = cT(x)+T(y).$ In particular, for any such linear map $T, T(0) = T(0)+T(0)\Rightarrow T(0)=0$ . Also, linear maps are uniquely determined by their values on a countable or finite basis, since if $\{a_1,a_2,\cdots\}$ is a basis for $V$ and $T:V\to W$ is a linear map, for any $x\in V, T(x) = \sum_{i=1}^k b_i T(a_{j_i})$ where $x = \sum_{i=1}^k b_i a_{j_i}$ is the unique representation of $x$ as a linear combination of basis vectors. In particular, $\{1,x,\cdots\}$ is a countable basis for $\mathbb{Q}[x]$ so it suffices to find the values of $\phi(x^i)$ for all $i\ge 0$ . Note that in $\mathbb{C}[x]$ , the only irreducible polynomials are the polynomials of degree $1$ since it is known that every polynomial of degree $n\ge 1$ has a complex root. In $\mathbb{R}[x]$ , every odd degree polynomial has a real root since complex roots come in pairs. Hence no odd degree polynomial with degree at least 3 is irreducible. Note that the set of units in $\mathbb{K}[x]$ for a field $\mathbb{K}$ is precisely $\mathbb{K}\backslash \{0\}$ , since for any two elements $f,g \in R[x]$ where R is a ring and $f$ and $g$ have leading coefficients that are units, $\deg(fg) = \deg(f) + \deg(g)$ . In general for any ring $R$ and $f,g\in R[x], \deg(fg)\leq \deg(f)+\deg(g)\tag{1}$ . In $\mathbb{Q}[x]$ , any polynomial of degree $1$ is irreducible and the polynomials of degree $2$ that are irreducible are precisely those without rational roots. Also, by Gauss' lemma, in a unique factorization domain (UFD) $R$ , if $\mathbb{K}$ is the field of fractions of $R$ then if $f \in R[x]$ equals $gh$ for some $g,h \in \mathbb{K}[x],$ there exists a unit $u \in \mathbb{K}$ so that $ug, u^{-1} h \in R[x].$ In particular for the UFD $\mathbb{Z}$ , its field of fractions is $\mathbb{Q}$ so if a polynomial with integer coefficients can be written as a product of two nonconstant polynomials with rational coefficients, then it can also be written as a product of two nonconstant polynomials with integer coefficients. Also, if $f\in R$ is irreducible, then $f$ is also irreducible in $R[x]$ whenever $R$ is an integral domain as then (1) always holds with equality. For the given problem, it might be the case that if $f + cg$ is irreducible (in $\mathbb{Q}[x]$ ) for all $c\in\mathbb{Q}$ , then either $g=0$ and $f$ is irreducible or $f$ is of degree 1 and g is a constant or g is linear. If this claim holds, if $f,g$ are polynomials so that $f+cg$ is irreducible for all $c\in\mathbb{Q},$ then since $\phi(f+cg) = \phi(f)+c\phi(g),\phi(f) + c\phi(g)$ are irreducible for all $c\in\mathbb{Q}$ . Hence $\phi(g) = 0$ or $\phi(f)$ has degree $1$ and $\phi(g)$ is a constant or linear. As an attempt to prove the claim, I think one can proceed using a proof by contradiction. Suppose $f(x)$ has degree $1$ and $g$ is nonzero. Write $f(x)=dx+e, d,e\in\mathbb{Q}$ . If $g$ has degree 2, say $g = ax^2+bx+c$ for some $a,b,c\in\mathbb{Q}$ , then $f(x)+rg(x) = rax^2 + (rb+d)x+(rc+e)$ . We need to choose $r$ so that this quadratic has a rational root, which will be the case provided its discriminant is the square of a rational number. But I'm not sure how to find such an $r$ or if it (always) exists. The product of two irreducible polynomials is obviously reducible but the sum may or may not be irreducible. For instance, $x+x^2+x+1$ is reducible even though both $x$ and $x^2+x+1$ are irreducible over $\mathbb{Q}[x]$ . Every polynomial in $\mathbb{Q}[x]$ or $\mathbb{R}[x]$ can be written (uniquely) as a product of irreducibles (note that every Euclidean domain is a PID and every PID is a UFD).","Let denote the vector space over of polynomials with rational coefficients in . Find all -linear maps that send irreducible polynomials to irreducible polynomials. If is a field over which a vector space is defined and is a vector space, a -linear map satisfies In particular, for any such linear map . Also, linear maps are uniquely determined by their values on a countable or finite basis, since if is a basis for and is a linear map, for any where is the unique representation of as a linear combination of basis vectors. In particular, is a countable basis for so it suffices to find the values of for all . Note that in , the only irreducible polynomials are the polynomials of degree since it is known that every polynomial of degree has a complex root. In , every odd degree polynomial has a real root since complex roots come in pairs. Hence no odd degree polynomial with degree at least 3 is irreducible. Note that the set of units in for a field is precisely , since for any two elements where R is a ring and and have leading coefficients that are units, . In general for any ring and . In , any polynomial of degree is irreducible and the polynomials of degree that are irreducible are precisely those without rational roots. Also, by Gauss' lemma, in a unique factorization domain (UFD) , if is the field of fractions of then if equals for some there exists a unit so that In particular for the UFD , its field of fractions is so if a polynomial with integer coefficients can be written as a product of two nonconstant polynomials with rational coefficients, then it can also be written as a product of two nonconstant polynomials with integer coefficients. Also, if is irreducible, then is also irreducible in whenever is an integral domain as then (1) always holds with equality. For the given problem, it might be the case that if is irreducible (in ) for all , then either and is irreducible or is of degree 1 and g is a constant or g is linear. If this claim holds, if are polynomials so that is irreducible for all then since are irreducible for all . Hence or has degree and is a constant or linear. As an attempt to prove the claim, I think one can proceed using a proof by contradiction. Suppose has degree and is nonzero. Write . If has degree 2, say for some , then . We need to choose so that this quadratic has a rational root, which will be the case provided its discriminant is the square of a rational number. But I'm not sure how to find such an or if it (always) exists. The product of two irreducible polynomials is obviously reducible but the sum may or may not be irreducible. For instance, is reducible even though both and are irreducible over . Every polynomial in or can be written (uniquely) as a product of irreducibles (note that every Euclidean domain is a PID and every PID is a UFD).","\mathbb{Q}[x] \mathbb{Q} x \mathbb{Q} \phi : \mathbb{Q}[x]\to\mathbb{Q}[x] \mathbb{K} V W \mathbb{K} T :V \to W T(cx+y) = cT(x)+T(y). T, T(0) = T(0)+T(0)\Rightarrow T(0)=0 \{a_1,a_2,\cdots\} V T:V\to W x\in V, T(x) = \sum_{i=1}^k b_i T(a_{j_i}) x = \sum_{i=1}^k b_i a_{j_i} x \{1,x,\cdots\} \mathbb{Q}[x] \phi(x^i) i\ge 0 \mathbb{C}[x] 1 n\ge 1 \mathbb{R}[x] \mathbb{K}[x] \mathbb{K} \mathbb{K}\backslash \{0\} f,g \in R[x] f g \deg(fg) = \deg(f) + \deg(g) R f,g\in R[x], \deg(fg)\leq \deg(f)+\deg(g)\tag{1} \mathbb{Q}[x] 1 2 R \mathbb{K} R f \in R[x] gh g,h \in \mathbb{K}[x], u \in \mathbb{K} ug, u^{-1} h \in R[x]. \mathbb{Z} \mathbb{Q} f\in R f R[x] R f + cg \mathbb{Q}[x] c\in\mathbb{Q} g=0 f f f,g f+cg c\in\mathbb{Q}, \phi(f+cg) = \phi(f)+c\phi(g),\phi(f) + c\phi(g) c\in\mathbb{Q} \phi(g) = 0 \phi(f) 1 \phi(g) f(x) 1 g f(x)=dx+e, d,e\in\mathbb{Q} g g = ax^2+bx+c a,b,c\in\mathbb{Q} f(x)+rg(x) = rax^2 + (rb+d)x+(rc+e) r r x+x^2+x+1 x x^2+x+1 \mathbb{Q}[x] \mathbb{Q}[x] \mathbb{R}[x]","['linear-algebra', 'polynomials', 'ring-theory', 'vector-spaces', 'linear-transformations']"
99,About finding the inverse of a matrix,About finding the inverse of a matrix,,I am solving a linear algebra problem and this matrix came up from a system of linear equations. $A = \begin{pmatrix} 1 & 2 & \cdots & n \\ 1 & 2^2 & \cdots & n^2 \\ \vdots &  & &\vdots &\\ 1 & 2^n & \cdots & n^n \end{pmatrix} $ I do not know how to check if my system has an unique solution or not (or the matrix is invertible) in this case.,I am solving a linear algebra problem and this matrix came up from a system of linear equations. $A = \begin{pmatrix} 1 & 2 & \cdots & n \\ 1 & 2^2 & \cdots & n^2 \\ \vdots &  & &\vdots &\\ 1 & 2^n & \cdots & n^n \end{pmatrix} $ I do not know how to check if my system has an unique solution or not (or the matrix is invertible) in this case.,,"['linear-algebra', 'matrices', 'systems-of-equations']"
