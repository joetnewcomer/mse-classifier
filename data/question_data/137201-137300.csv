,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Orthogonality relations for Legendre functions second kind,Orthogonality relations for Legendre functions second kind,,Does someone know the orthogonality relations for the associated Legendre functions of the second kind $Q_{n}^{m}(z)$? Are they the same as the orthogonality relations $\int{P_{n}^{m}(z) P_{j}^{k}(z) dz}=\frac{2(n+m)!}{(2n+1)(n-m)!} \delta_{jn} \delta_{km}$ for the associated Legendre functions of the first kind?,Does someone know the orthogonality relations for the associated Legendre functions of the second kind $Q_{n}^{m}(z)$? Are they the same as the orthogonality relations $\int{P_{n}^{m}(z) P_{j}^{k}(z) dz}=\frac{2(n+m)!}{(2n+1)(n-m)!} \delta_{jn} \delta_{km}$ for the associated Legendre functions of the first kind?,,"['calculus', 'ordinary-differential-equations']"
1,Is there an error in my proof of existence of a solution to this integral equation?,Is there an error in my proof of existence of a solution to this integral equation?,,"I have this proof to show that $$x(t) = \log(1+t) + \frac{1}{5} \int^1_0 e^{-t} \cos ^2(ts)(x(s))^2 ds  $$ has a unique integral solution in $C[0,1]$. I was looking at someone who had posted an identical question to this here: Show that the integral equation has a solution on a suitable subset of $C[0,1]$ where someone answered that you need to be careful about which subsets of $C[0,1]$ you choose. I didn't take any care about subsets  at all in my own proof so I was wondering if someone could tell me precisely where I went wrong in the following sketch of my proof. Help is truly appreciated. Sketch of proof at first is that I show that for all $t\in [0,1]:$ $$\frac{1}{5} \int^1_0 e^{-t} \cos ^2(ts)(x(s))^2 ds $$ is continuous with respect to $t$. (and thus locally Lipschitz given the closed bounded interval). Then I define $$T_x(t) = \log(1+t) + \frac{1}{5} \int^1_0 e^{-t} \cos ^2(ts)(x(s))^2 ds  $$ which has a fixed point at: $$x(t) = \log(1+t) + \frac{1}{5} \int^1_0 e^{-t} \cos ^2(ts)(x(s))^2 ds  $$ Then from the lemma above that the integral is continuous, $T$ is also continuous and thus we have that $T: C[0,1] \rightarrow C[0,1]$. Then I make the following argument that $T$ is a contraction map. $$|T_{x_1}(t) - T_{x_2}(t)| = \frac{1}{5} \Big|\int ^1_0 e^{-t} \cos^2 (ts) (x_1(s)-x_2(s))ds\Big| \leq \frac{1}{5} \text{max}_{s\in[0,1]} \Big|x_1(s)-x_2(s)\Big|$$ Thus $\Big\| T_{x_1} - T_{x_2} \Big\| \leq \frac{1}{5} \Big| x_1 - x_2 \Big|$, so $T$ is a contraction map, and since $C[0,1]$ is complete then $T$ has a unique fixed point. Thus there is a unique solution. If this is correct or if I am missing something fundamental? Thanks for the time.","I have this proof to show that $$x(t) = \log(1+t) + \frac{1}{5} \int^1_0 e^{-t} \cos ^2(ts)(x(s))^2 ds  $$ has a unique integral solution in $C[0,1]$. I was looking at someone who had posted an identical question to this here: Show that the integral equation has a solution on a suitable subset of $C[0,1]$ where someone answered that you need to be careful about which subsets of $C[0,1]$ you choose. I didn't take any care about subsets  at all in my own proof so I was wondering if someone could tell me precisely where I went wrong in the following sketch of my proof. Help is truly appreciated. Sketch of proof at first is that I show that for all $t\in [0,1]:$ $$\frac{1}{5} \int^1_0 e^{-t} \cos ^2(ts)(x(s))^2 ds $$ is continuous with respect to $t$. (and thus locally Lipschitz given the closed bounded interval). Then I define $$T_x(t) = \log(1+t) + \frac{1}{5} \int^1_0 e^{-t} \cos ^2(ts)(x(s))^2 ds  $$ which has a fixed point at: $$x(t) = \log(1+t) + \frac{1}{5} \int^1_0 e^{-t} \cos ^2(ts)(x(s))^2 ds  $$ Then from the lemma above that the integral is continuous, $T$ is also continuous and thus we have that $T: C[0,1] \rightarrow C[0,1]$. Then I make the following argument that $T$ is a contraction map. $$|T_{x_1}(t) - T_{x_2}(t)| = \frac{1}{5} \Big|\int ^1_0 e^{-t} \cos^2 (ts) (x_1(s)-x_2(s))ds\Big| \leq \frac{1}{5} \text{max}_{s\in[0,1]} \Big|x_1(s)-x_2(s)\Big|$$ Thus $\Big\| T_{x_1} - T_{x_2} \Big\| \leq \frac{1}{5} \Big| x_1 - x_2 \Big|$, so $T$ is a contraction map, and since $C[0,1]$ is complete then $T$ has a unique fixed point. Thus there is a unique solution. If this is correct or if I am missing something fundamental? Thanks for the time.",,"['functional-analysis', 'analysis', 'ordinary-differential-equations']"
2,How to use Green's theorem?,How to use Green's theorem?,,"$\def\d{\mathrm{d}}$I'm thinking about this differential equation $$\frac{3}{2} x\,\d x + \frac{x}{y}\,\d y = 0.$$ If functions $P(x,y), Q(x,y)$ are difined as$$P = \frac{3}{2} x,\ Q = \frac{x}{y},$$ this differential equation can be rewritten as $$P\,\d x + Q\,\d y = 0.$$ By differentiating $P$ and $Q$, one can easily get $$\frac{\partial P}{\partial y} = 0,\ \frac{\partial Q}{\partial x} = \frac{1}{y} \Longrightarrow \frac{\partial P}{\partial y} \neq \frac{\partial Q}{\partial x}.$$ According to Green's theorem, $$\oint _C (P\,\d x + Q\,\d y) = \iint _R \left( \frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} \right) \,\d x \d y.$$ Now $P\,\d x + Q\,\d y = 0$, so that $\oint _C (P\,\d x + Q\,\d y)$ is always zero, and thus$$\iint _R \left( \frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} \right) \,\d x \d y$$ is always zero. This results in $$\frac{\partial Q}{\partial x} = \frac{\partial P}{\partial y}.$$ Why is my way of using Green's theorem incorrect?","$\def\d{\mathrm{d}}$I'm thinking about this differential equation $$\frac{3}{2} x\,\d x + \frac{x}{y}\,\d y = 0.$$ If functions $P(x,y), Q(x,y)$ are difined as$$P = \frac{3}{2} x,\ Q = \frac{x}{y},$$ this differential equation can be rewritten as $$P\,\d x + Q\,\d y = 0.$$ By differentiating $P$ and $Q$, one can easily get $$\frac{\partial P}{\partial y} = 0,\ \frac{\partial Q}{\partial x} = \frac{1}{y} \Longrightarrow \frac{\partial P}{\partial y} \neq \frac{\partial Q}{\partial x}.$$ According to Green's theorem, $$\oint _C (P\,\d x + Q\,\d y) = \iint _R \left( \frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} \right) \,\d x \d y.$$ Now $P\,\d x + Q\,\d y = 0$, so that $\oint _C (P\,\d x + Q\,\d y)$ is always zero, and thus$$\iint _R \left( \frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y} \right) \,\d x \d y$$ is always zero. This results in $$\frac{\partial Q}{\partial x} = \frac{\partial P}{\partial y}.$$ Why is my way of using Green's theorem incorrect?",,"['ordinary-differential-equations', 'greens-theorem']"
3,Stabily from autonomous to nonautonomous,Stabily from autonomous to nonautonomous,,"Suppose we have some ODE of the form $$\dot{x}=f(\alpha,x),$$ where $\alpha \in \mathbb{R}^k$ is some vector of parameters. And for $\alpha \in A$ we have $x=e_{\alpha}$ stable hyperbolic equilibriums for the corresponding equations.  Now, lets change $\alpha$ for $\alpha (t)$ with $\alpha (t) \in A,\forall t$. Can we ensure (under some conditions on $f$ and $\alpha (t)$) that the equilibrium ""will turn"" into a trajectory $e(t)$ which attracts solutions (locally) of the non-autonomous equation $\dot{x}=f(\alpha(t),x)$ (forward or in a pull-back sense)? In particular, I'm working with a system for which I know the equilibriums for $\alpha \in A$. And if I change $\alpha$ for a periodic function, I can prove (using degree theory) that a periodic solution exists, provided $\alpha (t) \in A$ and the amplitude of oscillation is small. Is this solution a ""perturbation"" of the equilibrium? does it preserve the attractiveness? I hope the question is well posed, Thanks! Edit: Thank you RPA for your answer. My problem is actually a little bit more complex because my system involves time delays. I could write the equations here if you are interested. But basically I have 2 equations which depend on the parameters $\gamma$, $K$, and $\delta$. If they satisfy certain inequalities its not hard to solve for the non trivial equilibrium. And for $\gamma(t)$, $K(t)$, and $\delta(t)$ T-periodic, if the same inequalities hold $\forall t$ then I can probe existence of a non trivial periodic solution. So I believe that if the amplitude of this functions gets small, then this periodic solution should also have small amplitude until it becomes a point. But I don't know how to see this.","Suppose we have some ODE of the form $$\dot{x}=f(\alpha,x),$$ where $\alpha \in \mathbb{R}^k$ is some vector of parameters. And for $\alpha \in A$ we have $x=e_{\alpha}$ stable hyperbolic equilibriums for the corresponding equations.  Now, lets change $\alpha$ for $\alpha (t)$ with $\alpha (t) \in A,\forall t$. Can we ensure (under some conditions on $f$ and $\alpha (t)$) that the equilibrium ""will turn"" into a trajectory $e(t)$ which attracts solutions (locally) of the non-autonomous equation $\dot{x}=f(\alpha(t),x)$ (forward or in a pull-back sense)? In particular, I'm working with a system for which I know the equilibriums for $\alpha \in A$. And if I change $\alpha$ for a periodic function, I can prove (using degree theory) that a periodic solution exists, provided $\alpha (t) \in A$ and the amplitude of oscillation is small. Is this solution a ""perturbation"" of the equilibrium? does it preserve the attractiveness? I hope the question is well posed, Thanks! Edit: Thank you RPA for your answer. My problem is actually a little bit more complex because my system involves time delays. I could write the equations here if you are interested. But basically I have 2 equations which depend on the parameters $\gamma$, $K$, and $\delta$. If they satisfy certain inequalities its not hard to solve for the non trivial equilibrium. And for $\gamma(t)$, $K(t)$, and $\delta(t)$ T-periodic, if the same inequalities hold $\forall t$ then I can probe existence of a non trivial periodic solution. So I believe that if the amplitude of this functions gets small, then this periodic solution should also have small amplitude until it becomes a point. But I don't know how to see this.",,"['calculus', 'ordinary-differential-equations', 'dynamical-systems', 'stability-theory']"
4,Stabilty of leapfrog scheme applied to wave equation,Stabilty of leapfrog scheme applied to wave equation,,The leapfrog scheme applied to the equation: $$\frac{dz}{dt} =  \lambda z$$ gives $$\frac{z^{n+1} - z^{n-1}}{2\Delta t} = \lambda z^n.$$ The region of absolute stability is calculated and it is the pure imaginary interval $-i\leq \lambda \Delta t\leq i$. From that how can I conclude that what are the stability restrictions of the leapfrog scheme applied to the diffusion equation with periodic boundary conditions?,The leapfrog scheme applied to the equation: $$\frac{dz}{dt} =  \lambda z$$ gives $$\frac{z^{n+1} - z^{n-1}}{2\Delta t} = \lambda z^n.$$ The region of absolute stability is calculated and it is the pure imaginary interval $-i\leq \lambda \Delta t\leq i$. From that how can I conclude that what are the stability restrictions of the leapfrog scheme applied to the diffusion equation with periodic boundary conditions?,,"['ordinary-differential-equations', 'numerical-methods', 'stability-in-odes']"
5,Differentiating a derivative with respect to another function of $x$,Differentiating a derivative with respect to another function of,x,"I've been studying second order differentials, during which we were taught how to reduce second order differential equations to a simpler form using substitution. One trick we used was  $$\frac{d}{dx}\bigg(\frac{dy}{dz}\bigg)=\frac{d}{dx}\frac{dy}{dz}\frac{dz}{dz}=\frac{d^2y}{dz^2}\frac{dz}{dx}$$ Where $y$ and $z$ are both functions of $x$ This was introduced by 'grouping' terms together to form $\frac{d^2y}{dz^2}$, which is not what is happening, just a useful trick that happens to work. My question is, what is really happening here? Why does this work?","I've been studying second order differentials, during which we were taught how to reduce second order differential equations to a simpler form using substitution. One trick we used was  $$\frac{d}{dx}\bigg(\frac{dy}{dz}\bigg)=\frac{d}{dx}\frac{dy}{dz}\frac{dz}{dz}=\frac{d^2y}{dz^2}\frac{dz}{dx}$$ Where $y$ and $z$ are both functions of $x$ This was introduced by 'grouping' terms together to form $\frac{d^2y}{dz^2}$, which is not what is happening, just a useful trick that happens to work. My question is, what is really happening here? Why does this work?",,"['calculus', 'ordinary-differential-equations', 'derivatives']"
6,How second-order differential equations do not violate causality? [closed],How second-order differential equations do not violate causality? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question I copy this question from physics stack exchang e The second order differential equations are time reversible. That means: they don't distinguish the time arrow direction. There is no reason for the time to flow forward. My professor told me that there are two solutions to such equations, one of which describes processes going forward and one backward in time. The ""backward"" solution violates causality, so we say that only the ""forward"" solution is physical and the other simply don't exist. Is this explanation correct? Has this anything to do with the fact we use imaginary numbers? And one more question: How is causality not violated?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question I copy this question from physics stack exchang e The second order differential equations are time reversible. That means: they don't distinguish the time arrow direction. There is no reason for the time to flow forward. My professor told me that there are two solutions to such equations, one of which describes processes going forward and one backward in time. The ""backward"" solution violates causality, so we say that only the ""forward"" solution is physical and the other simply don't exist. Is this explanation correct? Has this anything to do with the fact we use imaginary numbers? And one more question: How is causality not violated?",,['ordinary-differential-equations']
7,Finding the equation of electric field lines.,Finding the equation of electric field lines.,,"Say we have a stationary particle located at $(0,0)$ with a positive unit charge, and a stationary particle located at $(a,0)$ with charge a negative unit charge. I want to find the field lines associated with the force field generated by the two charges. Suppose we have a charge with positive unit charge located at $(x_0,y_0)$ . The force acting on this charge can be described with the following (disgusting) vector: $$\left(\frac{x_0}{\left(x_0^2+y_0^2\right)^{3/2}}+\frac{a-x_0}{\left((x_0-a)^2+y_0^2\right)^{3/2}},\frac{y_0}{\left(x_0^2+y_0^2\right)^{3/2}}-\frac{y_0}{\left((x_0-a)^2+y_0^2\right)^{3/2}}\right)$$ This yields the following (disgusting) differetial-equation for the field lines associated with the force field: $$y'\left(\frac{x}{\left(x^2+y^2\right)^{3/2}}+\frac{a-x}{\left((x-a)^2+y^2\right)^{3/2}}\right)=\frac{y}{\left(x^2+y^2\right)^{3/2}}-\frac{y}{\left((x-a)^2+y^2\right)^{3/2}}$$ I can sort of simplify (but not really): $$y'\left(x\left((x-a)^2+y^2\right)^{3/2}+(a-x)\left(x^2+y^2\right)^{3/2}\right)=y\left(\left((x-a)^2+y^2\right)^{3/2}-\left(x^2+y^2\right)^{3/2}\right)$$ Unfortunately I have no idea how to solve this thing. Any help would be very much appreciated. I have posted the exact same question on physics.stackexchange Update: doing the same thing in polar coordinates yields the following differential equation (I think). $$\frac{\frac{dr}{d\theta}\sin\theta+r\cos\theta}{\frac{dr}{d\theta}\cos\theta-r\sin\theta}\left(\frac{\cos\theta}{r^2}+\frac{1}{\left(r^2\sin^2\theta+\left(a-r\cos\theta\right)^2\right)\sqrt{\left(\left(\frac{r\sin\theta}{a-r\cos\theta}\right)^2+1\right)}}\right)=\frac{\sin\theta}{r^2}-\frac{r\sin\theta}{\left(a-r\cos\theta\right)\left(r^2\sin^2\theta+\left(a-r\cos\theta\right)^2\right)\sqrt{\left(\left(\frac{r\sin\theta}{a-r\cos\theta}\right)^2+1\right)}}$$ I think the above only works for points whose $x$ -coordinate is less than $a$ .","Say we have a stationary particle located at with a positive unit charge, and a stationary particle located at with charge a negative unit charge. I want to find the field lines associated with the force field generated by the two charges. Suppose we have a charge with positive unit charge located at . The force acting on this charge can be described with the following (disgusting) vector: This yields the following (disgusting) differetial-equation for the field lines associated with the force field: I can sort of simplify (but not really): Unfortunately I have no idea how to solve this thing. Any help would be very much appreciated. I have posted the exact same question on physics.stackexchange Update: doing the same thing in polar coordinates yields the following differential equation (I think). I think the above only works for points whose -coordinate is less than .","(0,0) (a,0) (x_0,y_0) \left(\frac{x_0}{\left(x_0^2+y_0^2\right)^{3/2}}+\frac{a-x_0}{\left((x_0-a)^2+y_0^2\right)^{3/2}},\frac{y_0}{\left(x_0^2+y_0^2\right)^{3/2}}-\frac{y_0}{\left((x_0-a)^2+y_0^2\right)^{3/2}}\right) y'\left(\frac{x}{\left(x^2+y^2\right)^{3/2}}+\frac{a-x}{\left((x-a)^2+y^2\right)^{3/2}}\right)=\frac{y}{\left(x^2+y^2\right)^{3/2}}-\frac{y}{\left((x-a)^2+y^2\right)^{3/2}} y'\left(x\left((x-a)^2+y^2\right)^{3/2}+(a-x)\left(x^2+y^2\right)^{3/2}\right)=y\left(\left((x-a)^2+y^2\right)^{3/2}-\left(x^2+y^2\right)^{3/2}\right) \frac{\frac{dr}{d\theta}\sin\theta+r\cos\theta}{\frac{dr}{d\theta}\cos\theta-r\sin\theta}\left(\frac{\cos\theta}{r^2}+\frac{1}{\left(r^2\sin^2\theta+\left(a-r\cos\theta\right)^2\right)\sqrt{\left(\left(\frac{r\sin\theta}{a-r\cos\theta}\right)^2+1\right)}}\right)=\frac{\sin\theta}{r^2}-\frac{r\sin\theta}{\left(a-r\cos\theta\right)\left(r^2\sin^2\theta+\left(a-r\cos\theta\right)^2\right)\sqrt{\left(\left(\frac{r\sin\theta}{a-r\cos\theta}\right)^2+1\right)}} x a","['ordinary-differential-equations', 'physics']"
8,non-linear ODE $tdy + \tan^2(t+y)dt = 0$,non-linear ODE,tdy + \tan^2(t+y)dt = 0,I really have tried to solve this equation: That my shot: $\tan(y+t) = u\\ \sec^2(y+t).y = du/dt\\ 1+\tan^2(y+t).y = du/dt\\ (1+u^2)y = du/dt$ This is a final equation: $t.dy/du + (1+u^2).y + u^2 = 0$ My problem is a independent variable $t$.,I really have tried to solve this equation: That my shot: $\tan(y+t) = u\\ \sec^2(y+t).y = du/dt\\ 1+\tan^2(y+t).y = du/dt\\ (1+u^2)y = du/dt$ This is a final equation: $t.dy/du + (1+u^2).y + u^2 = 0$ My problem is a independent variable $t$.,,['ordinary-differential-equations']
9,Deriving Linear Differential Equation,Deriving Linear Differential Equation,,"Derive a linear differential equation for $v(t) = z(t) \overline{z(t)} = x(t)^2 + y(t)^2$ where $z(t)=x(t)+iy(t).$ I start by taking the derivative of this equation, $$v'(t)= \frac{d}{dt} x(t)^2+\frac d {dt} y(t)^2 =2x(t)x'(t)+y(t)y'(t)$$ I am not sure where to go from here. Also, the beginning of the problem gave a real linear system $x'=a(t)x-b(t)y$ and $y'=b(t)x+a(t)y$ and it reduces to the complex linear differential equation $z'=c(t)z$ where $z(t)=x(t)+iy(t)$ and $c(t)=a(t)+ib(t)$","Derive a linear differential equation for $v(t) = z(t) \overline{z(t)} = x(t)^2 + y(t)^2$ where $z(t)=x(t)+iy(t).$ I start by taking the derivative of this equation, $$v'(t)= \frac{d}{dt} x(t)^2+\frac d {dt} y(t)^2 =2x(t)x'(t)+y(t)y'(t)$$ I am not sure where to go from here. Also, the beginning of the problem gave a real linear system $x'=a(t)x-b(t)y$ and $y'=b(t)x+a(t)y$ and it reduces to the complex linear differential equation $z'=c(t)z$ where $z(t)=x(t)+iy(t)$ and $c(t)=a(t)+ib(t)$",,['ordinary-differential-equations']
10,Step on Hopf-Rinow via regularity.,Step on Hopf-Rinow via regularity.,,"On a step of proving (geodesics defined for all time)=>(for any $p,q$ there exists a minimizing geodesic connecting both), besides the traditional approach of taking smooth curves $c_n$ such that $L(c_n) \to d(p,q)$, Sakai mentions that one can use Arzela-Ascoli (changing the assumption of geodesics converging for all time to bounded closed balls being compact) to show that there exists a subsequence of those curves converging to a continuous path $c$. By continuity, we have that this path is length-minimizing. Therefore, it is a geodesic . However, I feel that the result that a continuous length-minimizing path must be a geodesic via the series of exercises and lemmata which are mentioned is very technical and kind of goes against the purpose of the argument of being cleaner. My question is: is there a way to make a shortcut here via some regularity theorem? For example, maybe arrive somehow at the fact that the limit is a weak solution to the geodesic equation and therefore must be smooth? (I don't know any references which approch the subject in this way*, so this would be nice too, if possible). *Except Klingenberg's , but his intentions are others.","On a step of proving (geodesics defined for all time)=>(for any $p,q$ there exists a minimizing geodesic connecting both), besides the traditional approach of taking smooth curves $c_n$ such that $L(c_n) \to d(p,q)$, Sakai mentions that one can use Arzela-Ascoli (changing the assumption of geodesics converging for all time to bounded closed balls being compact) to show that there exists a subsequence of those curves converging to a continuous path $c$. By continuity, we have that this path is length-minimizing. Therefore, it is a geodesic . However, I feel that the result that a continuous length-minimizing path must be a geodesic via the series of exercises and lemmata which are mentioned is very technical and kind of goes against the purpose of the argument of being cleaner. My question is: is there a way to make a shortcut here via some regularity theorem? For example, maybe arrive somehow at the fact that the limit is a weak solution to the geodesic equation and therefore must be smooth? (I don't know any references which approch the subject in this way*, so this would be nice too, if possible). *Except Klingenberg's , but his intentions are others.",,"['ordinary-differential-equations', 'differential-geometry', 'differential-topology', 'riemannian-geometry']"
11,Order and Degree of Differential Equation,Order and Degree of Differential Equation,,"I am unable to find the order and degree of the following differential equation: $x^2(dx)^2 + 2xy dx dy + y^2(dy)^2 - z^2(dz)^2 = 0$ My approach: Take the term $z^2(dz)^2$ to right hand side and then divide throughout by $(dz)^2$ we get  $$\mathrm{\dfrac{x^2(dx)^2}{(dz)^2} + \dfrac{2xy\ dx \ dy}{(dz)^2} +\dfrac{y^2(dy)^2}{(dz)^2} = z^2}$$ By seeing this, we can deduce that order is 1 and degree 2. But when I am looking at the middle term it contains dx dy/(dz)^2 term which is a term of order 2 and hence degree 1. Well am I right here? Is there any other way of finding the degree and order of this total differential equation? Do suggest, if any.","I am unable to find the order and degree of the following differential equation: $x^2(dx)^2 + 2xy dx dy + y^2(dy)^2 - z^2(dz)^2 = 0$ My approach: Take the term $z^2(dz)^2$ to right hand side and then divide throughout by $(dz)^2$ we get  $$\mathrm{\dfrac{x^2(dx)^2}{(dz)^2} + \dfrac{2xy\ dx \ dy}{(dz)^2} +\dfrac{y^2(dy)^2}{(dz)^2} = z^2}$$ By seeing this, we can deduce that order is 1 and degree 2. But when I am looking at the middle term it contains dx dy/(dz)^2 term which is a term of order 2 and hence degree 1. Well am I right here? Is there any other way of finding the degree and order of this total differential equation? Do suggest, if any.",,['ordinary-differential-equations']
12,What are the methods for solving analytically an coupled system of second order linear PDEs?,What are the methods for solving analytically an coupled system of second order linear PDEs?,,"What are the methods for solving analytically an coupled system of second order linear PDE? (As I know we can solve by transforming to linear ODEs system. What else?) If you suggest some books or notes in order to learn to solving coupled system of second-order linear PDEs, I'm very pleased, your suggestions are very important for me.","What are the methods for solving analytically an coupled system of second order linear PDE? (As I know we can solve by transforming to linear ODEs system. What else?) If you suggest some books or notes in order to learn to solving coupled system of second-order linear PDEs, I'm very pleased, your suggestions are very important for me.",,"['ordinary-differential-equations', 'partial-differential-equations', 'systems-of-equations', 'partial-derivative']"
13,When do solutions exist to variational problems in classical mechanics?,When do solutions exist to variational problems in classical mechanics?,,"Suppose I have a configuration space $ M $ and a lagrangian $ L: TM \rightarrow \mathbb{R} $. The action functional is defined as $$ \mathcal{S}[q(t)] = \int_0^T L(\dot q(t) ) \ dt , $$ The principle of least action states that the trajectory starting from $ q_i $ at time $ t_i $ and arriving at $ q_f $ at time $ t_f $ is the path minimizing the action subject to the boundary conditions $ q(t_i) = q_i $ and $ q(t_f) = q_f $. My question is: when do solutions to the principle of least action exist?  What conditions on $ M $ and $ L$ ensure that solutions exist for all boundary conditions $ (q_i, t_i) , (q_f, t_f) $?","Suppose I have a configuration space $ M $ and a lagrangian $ L: TM \rightarrow \mathbb{R} $. The action functional is defined as $$ \mathcal{S}[q(t)] = \int_0^T L(\dot q(t) ) \ dt , $$ The principle of least action states that the trajectory starting from $ q_i $ at time $ t_i $ and arriving at $ q_f $ at time $ t_f $ is the path minimizing the action subject to the boundary conditions $ q(t_i) = q_i $ and $ q(t_f) = q_f $. My question is: when do solutions to the principle of least action exist?  What conditions on $ M $ and $ L$ ensure that solutions exist for all boundary conditions $ (q_i, t_i) , (q_f, t_f) $?",,"['ordinary-differential-equations', 'mathematical-physics', 'classical-mechanics']"
14,How to solve given differential equation?,How to solve given differential equation?,,"$$ 4x^{3}y''(x)+6.y'(x^{2})-2x^{2}y'(x)-3y(x^{2})+2.xy(x)=\dfrac{1}{2(1-x^{2})} $$ I have tried to express $y$ as a power series and then expanding the right side to binomial coefficients but I'm getting stuck there, I cant invert it to standard functions.","$$ 4x^{3}y''(x)+6.y'(x^{2})-2x^{2}y'(x)-3y(x^{2})+2.xy(x)=\dfrac{1}{2(1-x^{2})} $$ I have tried to express $y$ as a power series and then expanding the right side to binomial coefficients but I'm getting stuck there, I cant invert it to standard functions.",,"['calculus', 'ordinary-differential-equations']"
15,Numerically approximating a dynamical system on a graph,Numerically approximating a dynamical system on a graph,,"I am struggling with a project that originated in complex networks and graph theory and has since incorporated some dynamical system elements (I think?). Unfortunately I am not very well versed in dynamical systems, and hope to find some help on how to numerically approximate this system. Consider an undirected connected graph $G = (V, E)$, accompanied by a distance function $d: E \to \mathbb{R}$. In particular, every edge $e \in E$ has a weight $d(e)$ contained in the interval $(0, \infty)$. For each point in time $t$ (starting at zero) we find a value $\kappa_t(e)$, for every edge $e \in E$. This value is found as the result of a linear program and can in general only be calculated analytically for very simple graphs. The value $\kappa_t(e)$ ranges from $(-\infty, 1]$, and is then used to modify the distance $d_t(e)$ in the following manner: we say that \begin{align*} d_{t + \Delta t}(e) = d_{t}(e) - \kappa_t(e) \cdot \Delta t. \end{align*} This implies that the distance on edge $e$ can either increase, decrease, or remain constant. If I remember correctly, it is correct to say that with this definition, we can state \begin{align*} \frac{\partial d_t(e)}{\partial t} = - \kappa_t(e), \end{align*}  right? We do this for every edge in the graph, and the result is that some distances decrease to zero, and some distances increase to infinity (it might be possible that some distances remain constant, but this is unlikely, and might even be impossible). I am having trouble numerically approximating this system, in particular I struggle with choosing the time increments $\Delta t$. I started with using a simple Euler forward approximation, but this is not satisfactory in two ways: first, for most timesteps nothing 'interesting' happens. Second, by chosing the increments independently from the situation in the graph, we might 'jump over' interesting events, changing the geometry of the graph in the process! For my second attempt, I thought I might choose the time increments $\Delta t$ as a function of the current situation in the graph. I did this by first selecting those edges that have a positive value $\kappa_t$, which implies that the distances on those edges would decrease. Next, I took the maximum of those values, and calculated $\Delta t$ as the timeperiod it took for the edge with maximum value $\kappa_t$ to reach exactly distance zero. In that way, we are assured that after every jump, at least one edge has attained distance zero. This massively reduces the number of iterations in my approximation algorithm, but it gave a number of problems as well. For instance, it might be very possible that all values $\kappa_t$ are non-positive at time $t$, but more importantly, the geometry changes at every point in time, so that those edges with maximal positive value $\kappa_t$ are generally not the ones with distance zero on their edges at time $t + \Delta t$. Does anyone have a suggestion on how to simulate a dynamical system such as this where at any point in time the geometry can change (e.g. change from $\kappa(e) > 0$ to $\kappa(e) < 0$)? Thank you in advance for any help you can offer! EDIT: We calculate the value $\kappa_t(x, y)$ as follows: \begin{align*} \kappa_t(x, y) = 1 - \frac{\mathcal{W}_1(P_x, P_y)}{d_t(x, y)}, \qquad (x, y) \in E, \end{align*} where $P_x$ and $P_y$ are the probability measures of the standard random walk associated with the graph (i.e. $P_x(y)$ is the probability of jumping to vertex $y$, when you are at $x$. This probability is proportional to the inverse of the distance on $(x, y)$ and the neighbouring vertices) Next, \begin{align*} \mathcal{W}_1(P_x, P_y) = \inf_{\gamma \in \pi(P_x, P_y)} \left\{ \sum_{(a, b) \in V \times V} d_t(a, b) \gamma(a, b) \right\}, \end{align*} where $\pi(P_x, P_y)$ is the collection of all couplings of $P_x$ and $P_y$. I hope this helps!","I am struggling with a project that originated in complex networks and graph theory and has since incorporated some dynamical system elements (I think?). Unfortunately I am not very well versed in dynamical systems, and hope to find some help on how to numerically approximate this system. Consider an undirected connected graph $G = (V, E)$, accompanied by a distance function $d: E \to \mathbb{R}$. In particular, every edge $e \in E$ has a weight $d(e)$ contained in the interval $(0, \infty)$. For each point in time $t$ (starting at zero) we find a value $\kappa_t(e)$, for every edge $e \in E$. This value is found as the result of a linear program and can in general only be calculated analytically for very simple graphs. The value $\kappa_t(e)$ ranges from $(-\infty, 1]$, and is then used to modify the distance $d_t(e)$ in the following manner: we say that \begin{align*} d_{t + \Delta t}(e) = d_{t}(e) - \kappa_t(e) \cdot \Delta t. \end{align*} This implies that the distance on edge $e$ can either increase, decrease, or remain constant. If I remember correctly, it is correct to say that with this definition, we can state \begin{align*} \frac{\partial d_t(e)}{\partial t} = - \kappa_t(e), \end{align*}  right? We do this for every edge in the graph, and the result is that some distances decrease to zero, and some distances increase to infinity (it might be possible that some distances remain constant, but this is unlikely, and might even be impossible). I am having trouble numerically approximating this system, in particular I struggle with choosing the time increments $\Delta t$. I started with using a simple Euler forward approximation, but this is not satisfactory in two ways: first, for most timesteps nothing 'interesting' happens. Second, by chosing the increments independently from the situation in the graph, we might 'jump over' interesting events, changing the geometry of the graph in the process! For my second attempt, I thought I might choose the time increments $\Delta t$ as a function of the current situation in the graph. I did this by first selecting those edges that have a positive value $\kappa_t$, which implies that the distances on those edges would decrease. Next, I took the maximum of those values, and calculated $\Delta t$ as the timeperiod it took for the edge with maximum value $\kappa_t$ to reach exactly distance zero. In that way, we are assured that after every jump, at least one edge has attained distance zero. This massively reduces the number of iterations in my approximation algorithm, but it gave a number of problems as well. For instance, it might be very possible that all values $\kappa_t$ are non-positive at time $t$, but more importantly, the geometry changes at every point in time, so that those edges with maximal positive value $\kappa_t$ are generally not the ones with distance zero on their edges at time $t + \Delta t$. Does anyone have a suggestion on how to simulate a dynamical system such as this where at any point in time the geometry can change (e.g. change from $\kappa(e) > 0$ to $\kappa(e) < 0$)? Thank you in advance for any help you can offer! EDIT: We calculate the value $\kappa_t(x, y)$ as follows: \begin{align*} \kappa_t(x, y) = 1 - \frac{\mathcal{W}_1(P_x, P_y)}{d_t(x, y)}, \qquad (x, y) \in E, \end{align*} where $P_x$ and $P_y$ are the probability measures of the standard random walk associated with the graph (i.e. $P_x(y)$ is the probability of jumping to vertex $y$, when you are at $x$. This probability is proportional to the inverse of the distance on $(x, y)$ and the neighbouring vertices) Next, \begin{align*} \mathcal{W}_1(P_x, P_y) = \inf_{\gamma \in \pi(P_x, P_y)} \left\{ \sum_{(a, b) \in V \times V} d_t(a, b) \gamma(a, b) \right\}, \end{align*} where $\pi(P_x, P_y)$ is the collection of all couplings of $P_x$ and $P_y$. I hope this helps!",,"['ordinary-differential-equations', 'numerical-methods', 'dynamical-systems']"
16,Sturm–Liouville problem: approximating potential.,Sturm–Liouville problem: approximating potential.,,"Let one-dimentional Sturm–Liouville problem: $$ \frac{d^2f(x)}{dx^2}+U(x)f(x)=\lambda f(x) $$ with some appropriate boundary conditions, have a set of solutions $\{\lambda_i,\phi_i\}$. Let further the sequence of functions $u^{(n)}(x)$ uniformly converge to $U(x)$ as $n$ tends to infinity. Is it true, that assuming the same boundary conditions the set of solutions of the approximate problem $$ \frac{d^2f(x)}{dx^2}+u^{(n)}(x)f(x)=\lambda f(x), $$ $\{\lambda^{(n)}_i,\phi^{(n)}_i\}$ converges to $\{\lambda_i,\phi_i\}$?","Let one-dimentional Sturm–Liouville problem: $$ \frac{d^2f(x)}{dx^2}+U(x)f(x)=\lambda f(x) $$ with some appropriate boundary conditions, have a set of solutions $\{\lambda_i,\phi_i\}$. Let further the sequence of functions $u^{(n)}(x)$ uniformly converge to $U(x)$ as $n$ tends to infinity. Is it true, that assuming the same boundary conditions the set of solutions of the approximate problem $$ \frac{d^2f(x)}{dx^2}+u^{(n)}(x)f(x)=\lambda f(x), $$ $\{\lambda^{(n)}_i,\phi^{(n)}_i\}$ converges to $\{\lambda_i,\phi_i\}$?",,"['ordinary-differential-equations', 'partial-differential-equations', 'eigenvalues-eigenvectors', 'eigenfunctions', 'sturm-liouville']"
17,First order ODE Substitution,First order ODE Substitution,,"When we have a O.D.E that is neither linear or separable, we use substitution to find out what $y(x)$ is. Does the choice of $v$ in substitution matter in outcome? The textbook primarily uses $v=\frac{y}{x}$, but never $v=\frac{x}{y}$, which leads me to think this is some type of convention. My knee-jerk response was that when we obtain $\frac{dy}{dx}$, it shouldn't matter whether $v = \frac{y}{x}$ or $\frac{x}{y}$, because the original O.D.E would reflect changes in $v$ accordingly. However, when I was given a simple differential equation to solve: $2xy\frac{dy}{dx} = 4x^2 + 3y^2$ I was able to compute the correct answer much faster and efficiently when $v=\frac{y}{x}$ but struggled to do so with $v=\frac{x}{y}$. Sorry if this is a silly question.","When we have a O.D.E that is neither linear or separable, we use substitution to find out what $y(x)$ is. Does the choice of $v$ in substitution matter in outcome? The textbook primarily uses $v=\frac{y}{x}$, but never $v=\frac{x}{y}$, which leads me to think this is some type of convention. My knee-jerk response was that when we obtain $\frac{dy}{dx}$, it shouldn't matter whether $v = \frac{y}{x}$ or $\frac{x}{y}$, because the original O.D.E would reflect changes in $v$ accordingly. However, when I was given a simple differential equation to solve: $2xy\frac{dy}{dx} = 4x^2 + 3y^2$ I was able to compute the correct answer much faster and efficiently when $v=\frac{y}{x}$ but struggled to do so with $v=\frac{x}{y}$. Sorry if this is a silly question.",,"['calculus', 'ordinary-differential-equations']"
18,Linear difference equation with matrices,Linear difference equation with matrices,,"Given scalars $a_0,...,a_n$ with $a_0$ nonzero, and given a signal   $\{z_k\}$, the equation $a_0y_{k+n} +$   $a_1y_{k+n-1}+...+a_{n-1}y_{k+1}+a_ny_k=z_k$ is called a linear   difference equation (or linear recurrence relation) of order $n$. For   simplicity, $a_0$ is often taken equal to $1$. If $\{z_k\}$ is the   zero sequence, the equation is homogeneous; otherwise, the equation is   nonhomogeneous. I'm confused by this notation. Why are all the a's subscript being incremented then something happens to them in the ellipses and then it is decremented by $n-1$ but even though it's decreasing from the subscript $n$, it reaches $a_n$ on the last part of the equation? Where are the subscript values of $k$ coming from? I was trying to look at the practical example (image attached below) and trying to piece out which value would map to which but I'm still very lost.","Given scalars $a_0,...,a_n$ with $a_0$ nonzero, and given a signal   $\{z_k\}$, the equation $a_0y_{k+n} +$   $a_1y_{k+n-1}+...+a_{n-1}y_{k+1}+a_ny_k=z_k$ is called a linear   difference equation (or linear recurrence relation) of order $n$. For   simplicity, $a_0$ is often taken equal to $1$. If $\{z_k\}$ is the   zero sequence, the equation is homogeneous; otherwise, the equation is   nonhomogeneous. I'm confused by this notation. Why are all the a's subscript being incremented then something happens to them in the ellipses and then it is decremented by $n-1$ but even though it's decreasing from the subscript $n$, it reaches $a_n$ on the last part of the equation? Where are the subscript values of $k$ coming from? I was trying to look at the practical example (image attached below) and trying to piece out which value would map to which but I'm still very lost.",,"['linear-algebra', 'matrices', 'ordinary-differential-equations']"
19,Embed 1st order linear differential matrix equation in SDP,Embed 1st order linear differential matrix equation in SDP,,"Suppose I have the following 1st order linear differential matrix equation: $$\dot{X}(t) = \frac{1}{2}D(t)X(t) + \frac{1}{2}X(t)D(t)^T $$ $D(t),X(t)\in \mathbb{R}^{n\times n}$ $X$ is positive semidefinite (i.e., $X\succeq 0$). We know the following standard semidefinite programming form: My question is how to embed that differential equation into the SDP? You might consider the following scenario: 1. The variable is evolving over time and $D(t)$ is the given input data changing over time. 2. $C, A_i$ are constant matrices. I do not want to use discretization , i.e.,  $$\dot{X} = \frac{X(k+1)-X(k))}{\Delta t}$$ This is because such differential equation is rank preserving (see the following lemma) and I want the rank of $X$ nor to change over time. ( Optimization and dynamical systems, Helmke and Moore ) Is there any possible way or related papers (or article with similar flavors) about this topic?","Suppose I have the following 1st order linear differential matrix equation: $$\dot{X}(t) = \frac{1}{2}D(t)X(t) + \frac{1}{2}X(t)D(t)^T $$ $D(t),X(t)\in \mathbb{R}^{n\times n}$ $X$ is positive semidefinite (i.e., $X\succeq 0$). We know the following standard semidefinite programming form: My question is how to embed that differential equation into the SDP? You might consider the following scenario: 1. The variable is evolving over time and $D(t)$ is the given input data changing over time. 2. $C, A_i$ are constant matrices. I do not want to use discretization , i.e.,  $$\dot{X} = \frac{X(k+1)-X(k))}{\Delta t}$$ This is because such differential equation is rank preserving (see the following lemma) and I want the rank of $X$ nor to change over time. ( Optimization and dynamical systems, Helmke and Moore ) Is there any possible way or related papers (or article with similar flavors) about this topic?",,"['matrices', 'ordinary-differential-equations', 'convex-optimization', 'semidefinite-programming']"
20,What am I doing wrong with this differential equation?,What am I doing wrong with this differential equation?,,"Take the differential equation $\dot x=\frac{dx}{dt}=x\sqrt x$. Then $\int \frac{1}{x^{1.5}}dx=t=-2x^{-.5}+C$ Hence $x(t)=\frac{4}{(C+t)^2}$ Where if $x_0:=x(0)$,  $C = \frac{2}{\sqrt x_o}$ Final Result : $$x(t)=\frac{4}{(\frac{2}{\sqrt x_o}+t)^2}$$ However, if I set $x_0$ to a positive number, such as $10$, then $x(t)$ is a decreasing function of time. This contradicts the differential equation given, because there the derivative of $x$ w.r.t $t$ is positive if $x$ is positive. What am I doing wrong? Edit : I've further pinpointed down where I'm making the mistake, but I still don't quite understand why it's a mistake: Starting from the equation: $-2x^{-.5}+C=t$, I moved $C$ to the other side, intentionally leaving out the minus sign: $-2x^{-.5}=C+t$. I have always been taught that the $C$ can be any constant, so it doesn't matter whether we add a minus sign or not (because, we could always have added the constant $\tilde C:=-C $ instead). However, here it seems to matter, because not adding the minus sign gives $x(t)=\frac{4}{(C+t)^2}$ instead of $x(t)=\frac{4}{(C-t)^2}$. I would have quessed that this doesn't matter, because of the reason regarding $\tilde C = -C$. However, because of the square $^2$ in the equation $x(t)=\frac{4}{(C+t)^2}$, both $\tilde C$ and $C$ reduce to $\frac {2}{\sqrt x_0}$, which would imply that $\frac {2}{\sqrt x_0}=- \frac {2}{\sqrt x_0}$. What's going on here? what am I doing wrong?","Take the differential equation $\dot x=\frac{dx}{dt}=x\sqrt x$. Then $\int \frac{1}{x^{1.5}}dx=t=-2x^{-.5}+C$ Hence $x(t)=\frac{4}{(C+t)^2}$ Where if $x_0:=x(0)$,  $C = \frac{2}{\sqrt x_o}$ Final Result : $$x(t)=\frac{4}{(\frac{2}{\sqrt x_o}+t)^2}$$ However, if I set $x_0$ to a positive number, such as $10$, then $x(t)$ is a decreasing function of time. This contradicts the differential equation given, because there the derivative of $x$ w.r.t $t$ is positive if $x$ is positive. What am I doing wrong? Edit : I've further pinpointed down where I'm making the mistake, but I still don't quite understand why it's a mistake: Starting from the equation: $-2x^{-.5}+C=t$, I moved $C$ to the other side, intentionally leaving out the minus sign: $-2x^{-.5}=C+t$. I have always been taught that the $C$ can be any constant, so it doesn't matter whether we add a minus sign or not (because, we could always have added the constant $\tilde C:=-C $ instead). However, here it seems to matter, because not adding the minus sign gives $x(t)=\frac{4}{(C+t)^2}$ instead of $x(t)=\frac{4}{(C-t)^2}$. I would have quessed that this doesn't matter, because of the reason regarding $\tilde C = -C$. However, because of the square $^2$ in the equation $x(t)=\frac{4}{(C+t)^2}$, both $\tilde C$ and $C$ reduce to $\frac {2}{\sqrt x_0}$, which would imply that $\frac {2}{\sqrt x_0}=- \frac {2}{\sqrt x_0}$. What's going on here? what am I doing wrong?",,['ordinary-differential-equations']
21,Ordinary Differential Equation of Degree two,Ordinary Differential Equation of Degree two,,"Solve $xy\left(\frac{dy}{dx}\right)=\sqrt{x^2-y^2-x^2y^2-1}$ I have tried solvable by $x, y, p,$ Clairaut's. Nothing seems to work.","Solve $xy\left(\frac{dy}{dx}\right)=\sqrt{x^2-y^2-x^2y^2-1}$ I have tried solvable by $x, y, p,$ Clairaut's. Nothing seems to work.",,['ordinary-differential-equations']
22,Writing a sampled differential equation as a difference equation?,Writing a sampled differential equation as a difference equation?,,"Some background/context A typical Linear Constant Coefficient Difference Equation (a.k.a. LCCDE, or recursive relation) might look like $$y[n] = \frac 34y[n-1]-\frac 18y[n-2]$$ Which is sometimes written more suggestively: $$y[n] - \frac 34y[n-1] + \frac 18y[n-2]=0$$ I've been learning how to solve these, and to my great surprise, there seems to be some kind of profound connection to differential equations. In the above example, the solution is as follows: Assume $y[n]=\lambda^n$ and plug in: (for brevity, some steps are skipped) $$\lambda^{n-2}(\lambda^2-\frac 34\lambda+\frac 18) = 0 \\ \lambda=\frac 12, \frac 14$$ So now we say that $y[n]=C_1(\frac 12)^n+C_2(\frac 14)^n$ What I'm trying to do I mean this is just screaming differential equations, so I'm trying to interpret an LCCDE as sampling an ODE's solution. I can get about this far: if we allow $p_1$ and $p_2$ (and in general, $p_k$) to be the roots of an ODE whose solution (sampled every $T_s$ seconds) is exactly equal to the above $y[n]$ , then we can say $$e^{p_kt}|_{t=nT_s}=e^{p_kT_sn}=\lambda_k^n\ \\p_k=\frac{\ln(\lambda_k)}{T_s}$$ And in our specific example (if we just want a sampling interval of 1 second), $$ p_1=\ln(\frac 12), p_2=\ln(\frac 14)$$ Working backwards even further, we can even write down the ""time-domain"" differential equation which has these roots in its characteristic (homogenous) equation: $$y''(t)-\ln(\frac 18)y'(t)+\ln(\frac 14)\cdot \ln(\frac 12)\cdot y(t)=0$$ What my question is Unfortunately, the whole point of doing all this was because I'm trying to understand how LCCDEs can be so closely connected to ODEs. Given an LCCDE, I can convert to an ODE, but not without first solving all the roots and working backwards, which isn't helping me to see what's going on. Is there another way to think about this, that might explain why difference equations are so similar to differential equations?","Some background/context A typical Linear Constant Coefficient Difference Equation (a.k.a. LCCDE, or recursive relation) might look like $$y[n] = \frac 34y[n-1]-\frac 18y[n-2]$$ Which is sometimes written more suggestively: $$y[n] - \frac 34y[n-1] + \frac 18y[n-2]=0$$ I've been learning how to solve these, and to my great surprise, there seems to be some kind of profound connection to differential equations. In the above example, the solution is as follows: Assume $y[n]=\lambda^n$ and plug in: (for brevity, some steps are skipped) $$\lambda^{n-2}(\lambda^2-\frac 34\lambda+\frac 18) = 0 \\ \lambda=\frac 12, \frac 14$$ So now we say that $y[n]=C_1(\frac 12)^n+C_2(\frac 14)^n$ What I'm trying to do I mean this is just screaming differential equations, so I'm trying to interpret an LCCDE as sampling an ODE's solution. I can get about this far: if we allow $p_1$ and $p_2$ (and in general, $p_k$) to be the roots of an ODE whose solution (sampled every $T_s$ seconds) is exactly equal to the above $y[n]$ , then we can say $$e^{p_kt}|_{t=nT_s}=e^{p_kT_sn}=\lambda_k^n\ \\p_k=\frac{\ln(\lambda_k)}{T_s}$$ And in our specific example (if we just want a sampling interval of 1 second), $$ p_1=\ln(\frac 12), p_2=\ln(\frac 14)$$ Working backwards even further, we can even write down the ""time-domain"" differential equation which has these roots in its characteristic (homogenous) equation: $$y''(t)-\ln(\frac 18)y'(t)+\ln(\frac 14)\cdot \ln(\frac 12)\cdot y(t)=0$$ What my question is Unfortunately, the whole point of doing all this was because I'm trying to understand how LCCDEs can be so closely connected to ODEs. Given an LCCDE, I can convert to an ODE, but not without first solving all the roots and working backwards, which isn't helping me to see what's going on. Is there another way to think about this, that might explain why difference equations are so similar to differential equations?",,"['ordinary-differential-equations', 'recurrence-relations']"
23,Diagonalizing differential operator,Diagonalizing differential operator,,"I would like to diagonalize the differential operator $D=-\partial^2_t+a^2$ with Dirichlet boundary conditions $x(0)=x(T)=0.$ So far I have tried to find the eigenfunctions of $D$, $$Df = \lambda f$$ by considering cases when $\lambda>0, \lambda=0,$ and $\lambda<0.$ However, it seems like for each case, the only function that satisfies this is the zero vector, which isn't an eigenfunction. Is this differential operator diagonalizable?","I would like to diagonalize the differential operator $D=-\partial^2_t+a^2$ with Dirichlet boundary conditions $x(0)=x(T)=0.$ So far I have tried to find the eigenfunctions of $D$, $$Df = \lambda f$$ by considering cases when $\lambda>0, \lambda=0,$ and $\lambda<0.$ However, it seems like for each case, the only function that satisfies this is the zero vector, which isn't an eigenfunction. Is this differential operator diagonalizable?",,['ordinary-differential-equations']
24,Solution for $\int \frac{1}{1-we^w}dw$,Solution for,\int \frac{1}{1-we^w}dw,I am looking for a solution or a method of approximation for :  $$\int \frac{1}{1-we^w}dw$$  that came up while working on an ODE problem. Got any suggestions? Note: $w$ is also a one variable function Thanks to anyone who can lend a hand Update : The original ODE is: $$xdw=(e^{-w}-w)dx$$,I am looking for a solution or a method of approximation for :  $$\int \frac{1}{1-we^w}dw$$  that came up while working on an ODE problem. Got any suggestions? Note: $w$ is also a one variable function Thanks to anyone who can lend a hand Update : The original ODE is: $$xdw=(e^{-w}-w)dx$$,,"['calculus', 'integration', 'ordinary-differential-equations', 'exponential-function', 'approximation']"
25,Distribute water over uneven ground,Distribute water over uneven ground,,"We are in a (closed and connected) uneven area that lives in $X \subset\mathcal R$. Height of the ground is indicated by $f : X \to \mathcal R$, which is a twice continuously differentiable function. It rains a fixed measure of water $e$. The area is nonabsorbant, such that the water does stay on the ground. This water is particularly persistent, in that it does not go for local minima, but intelligently finds the global minima. This implies that there is a unique water stand over $X$. Denote the distribution of water over the space as $w : X \to \mathcal R$. I would like to compute $w(x)$, and the stand of the water (how high it has risen). Define as the effective height of the ground $$ \tilde f(x) = f(x) + w(x) $$ One of the requirements onto $w$ is that $$ w(x) + f(x) = h \, \forall x : w(x) > 0 \tag 1$$ That is, effective surface is flat if there is water on it (and there is a unique effective surface heigh $h$ for any area with water on it). Secondly, $$ \int_X w(x) dx = e \tag 2$$ Easier problem Assume that $f$ was non-decreasing. Then, there exists $\bar x$ such that $w(x) > 0$ for $x < \bar x$ and $w(x) = 0$ for $x > \bar x$. We can find $\bar x$ using that $$ \int_0^{\bar x} w(x) dx = e = f(\bar x) - \int_0^{\bar x} f(x)dx$$ Then, we can simply compute $w(x) = f(\bar x) - f(x)$. I suppose there is some sort of transformation of $X$ into a new domain that is sorted by $f(x)$, and then I could apply this solution approach? Or how would I go on with solving this (original, not the easy) setup? Is there a better characterization/reformulation that allows me to find it quickly numerically?","We are in a (closed and connected) uneven area that lives in $X \subset\mathcal R$. Height of the ground is indicated by $f : X \to \mathcal R$, which is a twice continuously differentiable function. It rains a fixed measure of water $e$. The area is nonabsorbant, such that the water does stay on the ground. This water is particularly persistent, in that it does not go for local minima, but intelligently finds the global minima. This implies that there is a unique water stand over $X$. Denote the distribution of water over the space as $w : X \to \mathcal R$. I would like to compute $w(x)$, and the stand of the water (how high it has risen). Define as the effective height of the ground $$ \tilde f(x) = f(x) + w(x) $$ One of the requirements onto $w$ is that $$ w(x) + f(x) = h \, \forall x : w(x) > 0 \tag 1$$ That is, effective surface is flat if there is water on it (and there is a unique effective surface heigh $h$ for any area with water on it). Secondly, $$ \int_X w(x) dx = e \tag 2$$ Easier problem Assume that $f$ was non-decreasing. Then, there exists $\bar x$ such that $w(x) > 0$ for $x < \bar x$ and $w(x) = 0$ for $x > \bar x$. We can find $\bar x$ using that $$ \int_0^{\bar x} w(x) dx = e = f(\bar x) - \int_0^{\bar x} f(x)dx$$ Then, we can simply compute $w(x) = f(\bar x) - f(x)$. I suppose there is some sort of transformation of $X$ into a new domain that is sorted by $f(x)$, and then I could apply this solution approach? Or how would I go on with solving this (original, not the easy) setup? Is there a better characterization/reformulation that allows me to find it quickly numerically?",,"['algebra-precalculus', 'ordinary-differential-equations']"
26,Is there a link between the two definitions of homogeneity for differential equations (for a first order equation and diff equations of higher order)?,Is there a link between the two definitions of homogeneity for differential equations (for a first order equation and diff equations of higher order)?,,"According to Wikipedia, there are two definitions for homogeneity of a differential equation. The first is for first order differential equations: A first-order ordinary differential equation in the form: ${\displaystyle M(x,y)\,dx+N(x,y)\,dy=0}$ is a homogeneous type if both functions $M(x, y)$ and $N(x, y)$ are homogeneous functions of the same degree $n$.[1] That is, multiplying each variable by a parameter   ${\displaystyle \lambda }$, we find ${\displaystyle M(\lambda x,\lambda y)=\lambda ^{n}M(x,y)\,}$,     and     ${\displaystyle N(\lambda x,\lambda y)=\lambda ^{n}N(x,y)\,.} $ Thus,   ${\displaystyle {\frac {M(\lambda x,\lambda y)}{N(\lambda x,\lambda y)}}={\frac {M(x,y)}{N(x,y)}}\,.}$ Which to me suggests something about how is we were to rescale each of the axes, the differential equation will remain the same. Then there is the definition for higher-order equations: A linear differential equation is called homogeneous if the following condition is satisfied: If   ${\displaystyle \phi (x)}$  is a solution, so is   ${\displaystyle c\phi (x)}$, where ${\displaystyle c}$ is an arbitrary (non-zero) constant. Note that in order for this condition to hold, each term in a linear differential equation of the dependent variable y must contain y or any derivative of y. A linear differential equation that fails this condition is called inhomogeneous.   A linear differential equation can be represented as a linear operator acting on $y(x)$ where $x$ is usually the independent variable and y is the dependent variable. Therefore, the general form of a linear homogeneous differential equation is ${\displaystyle L(y)=0\,}$ ... It should be noted that the existence of a constant term is a sufficient condition for an equation to be inhomogeneous, as in the above example. So I get that this definition also has something to do with scaling, although it seems to be only scaling the y. Is there a connection between these two definitions? Are they referring to the same but just with different order equations?","According to Wikipedia, there are two definitions for homogeneity of a differential equation. The first is for first order differential equations: A first-order ordinary differential equation in the form: ${\displaystyle M(x,y)\,dx+N(x,y)\,dy=0}$ is a homogeneous type if both functions $M(x, y)$ and $N(x, y)$ are homogeneous functions of the same degree $n$.[1] That is, multiplying each variable by a parameter   ${\displaystyle \lambda }$, we find ${\displaystyle M(\lambda x,\lambda y)=\lambda ^{n}M(x,y)\,}$,     and     ${\displaystyle N(\lambda x,\lambda y)=\lambda ^{n}N(x,y)\,.} $ Thus,   ${\displaystyle {\frac {M(\lambda x,\lambda y)}{N(\lambda x,\lambda y)}}={\frac {M(x,y)}{N(x,y)}}\,.}$ Which to me suggests something about how is we were to rescale each of the axes, the differential equation will remain the same. Then there is the definition for higher-order equations: A linear differential equation is called homogeneous if the following condition is satisfied: If   ${\displaystyle \phi (x)}$  is a solution, so is   ${\displaystyle c\phi (x)}$, where ${\displaystyle c}$ is an arbitrary (non-zero) constant. Note that in order for this condition to hold, each term in a linear differential equation of the dependent variable y must contain y or any derivative of y. A linear differential equation that fails this condition is called inhomogeneous.   A linear differential equation can be represented as a linear operator acting on $y(x)$ where $x$ is usually the independent variable and y is the dependent variable. Therefore, the general form of a linear homogeneous differential equation is ${\displaystyle L(y)=0\,}$ ... It should be noted that the existence of a constant term is a sufficient condition for an equation to be inhomogeneous, as in the above example. So I get that this definition also has something to do with scaling, although it seems to be only scaling the y. Is there a connection between these two definitions? Are they referring to the same but just with different order equations?",,"['ordinary-differential-equations', 'linear-transformations', 'transformation']"
27,A solution of a given ODE is infinitely differentiable,A solution of a given ODE is infinitely differentiable,,"I got this problem in a class test: Prove that any solution of the DE $$y''e^x+y'\cos{x}+e^{2x^2}y=0$$ is infinitely differentiable. I solved this as follows but I'm not entirely sure if I'm correct: Let $y_0$ be a solution of the given DE. I need to prove that $y_0^{(n)}$ exists $\forall$ $n\in \mathbb{N}$. I prove this by induction. The statement is true for $n=1,2$ because the order of the DE is $2$. Induction hypothesis: Let the statement be true $\forall$ $n<k$. Now, $y_0''e^x= -y_0'\cos{x} -e^{2x^2}y_0 \Rightarrow y_0'' = F(y_0',y_0,x)$. Differentiating this $k-3$ times, I have $$y_0^{(k-1)}=G(y_0^{(k-2)}, \cdots, y_0', y_0, x)$$ By the induction hypothesis, the right hand side is differentiable, implying that $y_0^{(k)}$ exists. Is my solution correct? If not, how do I solve this problem?","I got this problem in a class test: Prove that any solution of the DE $$y''e^x+y'\cos{x}+e^{2x^2}y=0$$ is infinitely differentiable. I solved this as follows but I'm not entirely sure if I'm correct: Let $y_0$ be a solution of the given DE. I need to prove that $y_0^{(n)}$ exists $\forall$ $n\in \mathbb{N}$. I prove this by induction. The statement is true for $n=1,2$ because the order of the DE is $2$. Induction hypothesis: Let the statement be true $\forall$ $n<k$. Now, $y_0''e^x= -y_0'\cos{x} -e^{2x^2}y_0 \Rightarrow y_0'' = F(y_0',y_0,x)$. Differentiating this $k-3$ times, I have $$y_0^{(k-1)}=G(y_0^{(k-2)}, \cdots, y_0', y_0, x)$$ By the induction hypothesis, the right hand side is differentiable, implying that $y_0^{(k)}$ exists. Is my solution correct? If not, how do I solve this problem?",,['ordinary-differential-equations']
28,Evaluating the integral $\int_0^\infty dke^{-\gamma k}k\cos\left(\sqrt{\alpha^2k^2-\beta^2}t\right)\sin(kr)$,Evaluating the integral,\int_0^\infty dke^{-\gamma k}k\cos\left(\sqrt{\alpha^2k^2-\beta^2}t\right)\sin(kr),"Consider the telegraph equation within a linear medium: $$\left(\frac{\varepsilon\mu}{c^2}\frac{\partial^2}{\partial t^2}+\mu\sigma\frac{\partial}{\partial t}-\nabla^2\right)\Psi(\vec{r},t)=0$$ with initial conditions $\Psi(\vec{r},0)=f(\vec{r}), \Psi_t(\vec{r},0)=0$. Taking a Fourier Transform with respect to spatial coordinates gives $$\left(\frac{\varepsilon\mu}{c^2}\frac{\partial^2}{\partial t^2}+\mu\sigma\frac{\partial}{\partial t}+k^2\right)\hat{\Psi}(\vec{k},t)=0$$ This is a linear ODE in $t$ with characteristic equation $$\lambda^2+\frac{\sigma c^2}{\varepsilon}\lambda+\frac{c^2k^2}{\varepsilon\mu}=0$$ Thus, assuming that $\frac{c^2k^2}{\varepsilon\mu}>\frac{\sigma^2c^4}{4\varepsilon^2}$ its solutions are given by $$\hat{\Psi}(\vec{k},t)=e^{-\beta t}\left[A(\vec{k})\cos\left(\sqrt{\alpha^2k^2-\beta^2}t\right)+B(\vec{k})\sin\left(\sqrt{\alpha^2k^2-\beta^2}t\right)\right]$$ for arbitrary functions $A(\vec{k}), B(\vec{k})$ and $\alpha:=\frac{c}{\sqrt{\varepsilon\mu}}, \beta:=\frac{\sigma c^2}{2\varepsilon}$. Plugging in the initial conditions, we have $A(\vec{k})=\hat{f}(\vec{k}), B(\vec{k}) = 0$, thus $$\hat{\Psi}(\vec{k},t)=\hat{f}(\vec{k})e^{-\beta t}\cos\left(\sqrt{\alpha^2k^2-\beta^2}t\right)$$ To calculate the Inverse Fourier Transform, we calculate the Inverse Fourier Transform of $\cos\left(\sqrt{\alpha^2k^2-\beta^2}t\right)$ and then apply the Convolution Theorem. \begin{align} \mathcal{F}^{-1}\left[\cos\left(\sqrt{\alpha^2k^2-\beta^2}t\right)\right]&=\frac{1}{8\pi^3}\int_0^\infty dkk^2\cos\left(\sqrt{\alpha^2k^2-\beta^2}t\right)\int_0^\pi d\theta\sin\theta e^{ikr\cos\theta}\int_0^{2\pi}d\phi\\ &=\frac{1}{4\pi^2}\int_0^\infty dkk^2\cos\left(\sqrt{\alpha^2k^2-\beta^2}t\right)\int_{-1}^1due^{ikru}\\ &=\frac{1}{4\pi^2r}\int_0^\infty dkk^2\cos\left(\sqrt{\alpha^2k^2-\beta^2}t\right)\left[\frac{e^{ikr}-e^{-ikr}}{ik}\right]_{u=-1}^{u=1}\\ &=\frac{1}{2\pi^2r}\int_0^\infty dkk\cos\left(\sqrt{\alpha^2k^2-\beta^2}t\right)\sin(kr) \end{align} Since this integral does not converge, we insert a regularisation parameter and rewrite it: $$\mathcal{F}^{-1}\left[\cos\left(\sqrt{\alpha^2k^2-\beta^2}t\right)\right]=\frac{1}{2\pi^2r}\lim\limits_{\gamma\rightarrow 0^+}\int_0^\infty dke^{-\gamma k}k\cos\left(\sqrt{\alpha^2k^2-\beta^2}t\right)\sin(kr)$$ This integral converges for all values of $\gamma>0$. It is at this point that I am stuck. I've tried a couple of things - writing out the product using the fact that $\cos(x)\sin(y) = \frac{1}{2}\left[\sin(x+y)-\sin(x-y)\right]$  and then rewriting this as the imaginary part of complex exponentials, using the Residue Theorem, integration by parts - in each case I get stuck because of the square root within the cosine function. Could anybody give me a hint on how to calculate this final integral? Or does there exist no closed form?","Consider the telegraph equation within a linear medium: $$\left(\frac{\varepsilon\mu}{c^2}\frac{\partial^2}{\partial t^2}+\mu\sigma\frac{\partial}{\partial t}-\nabla^2\right)\Psi(\vec{r},t)=0$$ with initial conditions $\Psi(\vec{r},0)=f(\vec{r}), \Psi_t(\vec{r},0)=0$. Taking a Fourier Transform with respect to spatial coordinates gives $$\left(\frac{\varepsilon\mu}{c^2}\frac{\partial^2}{\partial t^2}+\mu\sigma\frac{\partial}{\partial t}+k^2\right)\hat{\Psi}(\vec{k},t)=0$$ This is a linear ODE in $t$ with characteristic equation $$\lambda^2+\frac{\sigma c^2}{\varepsilon}\lambda+\frac{c^2k^2}{\varepsilon\mu}=0$$ Thus, assuming that $\frac{c^2k^2}{\varepsilon\mu}>\frac{\sigma^2c^4}{4\varepsilon^2}$ its solutions are given by $$\hat{\Psi}(\vec{k},t)=e^{-\beta t}\left[A(\vec{k})\cos\left(\sqrt{\alpha^2k^2-\beta^2}t\right)+B(\vec{k})\sin\left(\sqrt{\alpha^2k^2-\beta^2}t\right)\right]$$ for arbitrary functions $A(\vec{k}), B(\vec{k})$ and $\alpha:=\frac{c}{\sqrt{\varepsilon\mu}}, \beta:=\frac{\sigma c^2}{2\varepsilon}$. Plugging in the initial conditions, we have $A(\vec{k})=\hat{f}(\vec{k}), B(\vec{k}) = 0$, thus $$\hat{\Psi}(\vec{k},t)=\hat{f}(\vec{k})e^{-\beta t}\cos\left(\sqrt{\alpha^2k^2-\beta^2}t\right)$$ To calculate the Inverse Fourier Transform, we calculate the Inverse Fourier Transform of $\cos\left(\sqrt{\alpha^2k^2-\beta^2}t\right)$ and then apply the Convolution Theorem. \begin{align} \mathcal{F}^{-1}\left[\cos\left(\sqrt{\alpha^2k^2-\beta^2}t\right)\right]&=\frac{1}{8\pi^3}\int_0^\infty dkk^2\cos\left(\sqrt{\alpha^2k^2-\beta^2}t\right)\int_0^\pi d\theta\sin\theta e^{ikr\cos\theta}\int_0^{2\pi}d\phi\\ &=\frac{1}{4\pi^2}\int_0^\infty dkk^2\cos\left(\sqrt{\alpha^2k^2-\beta^2}t\right)\int_{-1}^1due^{ikru}\\ &=\frac{1}{4\pi^2r}\int_0^\infty dkk^2\cos\left(\sqrt{\alpha^2k^2-\beta^2}t\right)\left[\frac{e^{ikr}-e^{-ikr}}{ik}\right]_{u=-1}^{u=1}\\ &=\frac{1}{2\pi^2r}\int_0^\infty dkk\cos\left(\sqrt{\alpha^2k^2-\beta^2}t\right)\sin(kr) \end{align} Since this integral does not converge, we insert a regularisation parameter and rewrite it: $$\mathcal{F}^{-1}\left[\cos\left(\sqrt{\alpha^2k^2-\beta^2}t\right)\right]=\frac{1}{2\pi^2r}\lim\limits_{\gamma\rightarrow 0^+}\int_0^\infty dke^{-\gamma k}k\cos\left(\sqrt{\alpha^2k^2-\beta^2}t\right)\sin(kr)$$ This integral converges for all values of $\gamma>0$. It is at this point that I am stuck. I've tried a couple of things - writing out the product using the fact that $\cos(x)\sin(y) = \frac{1}{2}\left[\sin(x+y)-\sin(x-y)\right]$  and then rewriting this as the imaginary part of complex exponentials, using the Residue Theorem, integration by parts - in each case I get stuck because of the square root within the cosine function. Could anybody give me a hint on how to calculate this final integral? Or does there exist no closed form?",,"['integration', 'ordinary-differential-equations', 'partial-differential-equations', 'definite-integrals', 'fourier-transform']"
29,Electrical circuit analyses: coil and capacitor in series,Electrical circuit analyses: coil and capacitor in series,,"I've a series circuit of a coil and a capacitor, in between those components we've a switch that will close when $t=0$. We can write: $$ \begin{cases} \text{U}_\text{C}\left(t\right)=-\text{U}_\text{L}\left(t\right)\\ \\ \text{I}_\text{C}\left(t\right)=\text{U}'_\text{C}\left(t\right)\cdot\text{C}\\ \\ \text{U}_\text{L}\left(t\right)=\text{I}'_\text{L}\left(t\right)\cdot\text{L}\\ \\ \text{I}\left(t\right)=\text{I}_\text{C}\left(t\right)=\text{I}_\text{L}\left(t\right)\\ \end{cases}\space\space\space\space\space\therefore\space\space\space\space\space\frac{1}{\text{C}}\cdot\text{I}\left(t\right)=-\text{L}\cdot\text{I}''\left(t\right)\tag1 $$ Using Laplace transform: $$\text{I}\left(\text{s}\right)=\frac{\text{s}\cdot\text{I}\left(0\right)+\text{I}'\left(0\right)}{\frac{1}{\text{C}}+\text{L}\cdot\text{s}}\tag2$$ $$\text{U}_\text{C}\left(\text{s}\right)=\frac{1}{\text{C}\cdot\text{s}}\cdot\left\{\frac{\text{s}\cdot\text{I}\left(0\right)+\text{I}'\left(0\right)}{\frac{1}{\text{C}}+\text{L}\cdot\text{s}}+\text{C}\cdot\text{U}_\text{C}\left(0\right)\right\}\tag3$$ $$\text{U}_\text{L}\left(\text{s}\right)=\text{s}\cdot\text{L}\cdot\frac{\text{s}\cdot\text{I}\left(0\right)+\text{I}'\left(0\right)}{\frac{1}{\text{C}}+\text{L}\cdot\text{s}}-\text{L}\cdot\text{I}\left(0\right)\tag4$$ Well, I know that: $$\text{U}_\text{C}\left(0\right)=200\tag5$$ $$\pi\sqrt{\text{C}\cdot\text{L}}<10\cdot10^{-6}=10^{-5}\space\Longleftrightarrow\space\text{C}\cdot\text{L}<\frac{10^{-10}}{\pi^2}\tag6$$ Question: How can I find the value of $\text{C}$ and $\text{L}$ using the things I know?","I've a series circuit of a coil and a capacitor, in between those components we've a switch that will close when $t=0$. We can write: $$ \begin{cases} \text{U}_\text{C}\left(t\right)=-\text{U}_\text{L}\left(t\right)\\ \\ \text{I}_\text{C}\left(t\right)=\text{U}'_\text{C}\left(t\right)\cdot\text{C}\\ \\ \text{U}_\text{L}\left(t\right)=\text{I}'_\text{L}\left(t\right)\cdot\text{L}\\ \\ \text{I}\left(t\right)=\text{I}_\text{C}\left(t\right)=\text{I}_\text{L}\left(t\right)\\ \end{cases}\space\space\space\space\space\therefore\space\space\space\space\space\frac{1}{\text{C}}\cdot\text{I}\left(t\right)=-\text{L}\cdot\text{I}''\left(t\right)\tag1 $$ Using Laplace transform: $$\text{I}\left(\text{s}\right)=\frac{\text{s}\cdot\text{I}\left(0\right)+\text{I}'\left(0\right)}{\frac{1}{\text{C}}+\text{L}\cdot\text{s}}\tag2$$ $$\text{U}_\text{C}\left(\text{s}\right)=\frac{1}{\text{C}\cdot\text{s}}\cdot\left\{\frac{\text{s}\cdot\text{I}\left(0\right)+\text{I}'\left(0\right)}{\frac{1}{\text{C}}+\text{L}\cdot\text{s}}+\text{C}\cdot\text{U}_\text{C}\left(0\right)\right\}\tag3$$ $$\text{U}_\text{L}\left(\text{s}\right)=\text{s}\cdot\text{L}\cdot\frac{\text{s}\cdot\text{I}\left(0\right)+\text{I}'\left(0\right)}{\frac{1}{\text{C}}+\text{L}\cdot\text{s}}-\text{L}\cdot\text{I}\left(0\right)\tag4$$ Well, I know that: $$\text{U}_\text{C}\left(0\right)=200\tag5$$ $$\pi\sqrt{\text{C}\cdot\text{L}}<10\cdot10^{-6}=10^{-5}\space\Longleftrightarrow\space\text{C}\cdot\text{L}<\frac{10^{-10}}{\pi^2}\tag6$$ Question: How can I find the value of $\text{C}$ and $\text{L}$ using the things I know?",,"['real-analysis', 'integration', 'ordinary-differential-equations', 'physics', 'laplace-transform']"
30,Taylor series from discrete series - Approximation / Lagrange remainder?,Taylor series from discrete series - Approximation / Lagrange remainder?,,"Apologies if this question is already known - I have three population ($x(t),y(t)$ and $z(t)$ evalutated at discrete time steps. If we define the sum of these as $T(t) = x(t) + y(t) + z(t)$, you can write $x(t+1) = ax(t)T(t)$ $y(t+1) = by(t)T(t)$ $z(t+1) = cz(t)T(t)$ where $a$, $b$ and $c$ are constants. For now, $T(t)$ is a constant. If so, one can solve this as a recursive series. However, for a variety of reasons we'd prefer to make this a series of ODEs if possible. Naively, from Taylor's theorem, we know that a function can be written as a sum of the form $x(t + \delta t) = x(t) + \delta t x'(t) + .....$ If we let $\delta t = 1$, and truncate the series after the first derivative, then we might write these as a system of ODEs after re-arrangement to get.. $x'(t) \approx x(t)(aT(t) - 1)$ $y'(t) \approx y(t)(bT(t) - 1)$ $z'(t) \approx z(t)(cT(t) - 1)$ Solving this seems to give answers close to the exact solution (see image attached, these are acceptably close for our purposes) but I'd like to quantify the error if I can - I considered using a version of Lagrange Remainder formula, but since the form here is cobbled together from a discrete series I don't think there's any way of doing this? The other question I have involves our motivation for switching to ODEs - it would be great if we could consider situations where $T(t)$ might not be constant, and there may be growth or decay with time, for example if a growth term $g$ was added to $x$ so that $x'(t) \approx x(t)(aT(t) - 1) + g$ . There might also be situations when $a,b,$ and $c$ could be themselves functions of time. Is it acceptable to make the simplification here, and what is the best way to quantify the error that arises from such approximation?","Apologies if this question is already known - I have three population ($x(t),y(t)$ and $z(t)$ evalutated at discrete time steps. If we define the sum of these as $T(t) = x(t) + y(t) + z(t)$, you can write $x(t+1) = ax(t)T(t)$ $y(t+1) = by(t)T(t)$ $z(t+1) = cz(t)T(t)$ where $a$, $b$ and $c$ are constants. For now, $T(t)$ is a constant. If so, one can solve this as a recursive series. However, for a variety of reasons we'd prefer to make this a series of ODEs if possible. Naively, from Taylor's theorem, we know that a function can be written as a sum of the form $x(t + \delta t) = x(t) + \delta t x'(t) + .....$ If we let $\delta t = 1$, and truncate the series after the first derivative, then we might write these as a system of ODEs after re-arrangement to get.. $x'(t) \approx x(t)(aT(t) - 1)$ $y'(t) \approx y(t)(bT(t) - 1)$ $z'(t) \approx z(t)(cT(t) - 1)$ Solving this seems to give answers close to the exact solution (see image attached, these are acceptably close for our purposes) but I'd like to quantify the error if I can - I considered using a version of Lagrange Remainder formula, but since the form here is cobbled together from a discrete series I don't think there's any way of doing this? The other question I have involves our motivation for switching to ODEs - it would be great if we could consider situations where $T(t)$ might not be constant, and there may be growth or decay with time, for example if a growth term $g$ was added to $x$ so that $x'(t) \approx x(t)(aT(t) - 1) + g$ . There might also be situations when $a,b,$ and $c$ could be themselves functions of time. Is it acceptable to make the simplification here, and what is the best way to quantify the error that arises from such approximation?",,"['ordinary-differential-equations', 'recurrence-relations', 'systems-of-equations', 'approximation', 'discrete-calculus']"
31,Solution to a given matrix initial value problem,Solution to a given matrix initial value problem,,"Consider the IVP : $y''(x)+A \cdot y(x)=0,$ where $A$ is an $n \times n$ positive definite matrix. Also $y(0)=c_0$ and $y'(0)=c_1,$ where $c_0 , c_1 \in \mathbb{R}^n$ are constant vectors. Since $A$ is positive definite, it possesses a square toot. The solution is given by  $$y(x)=C \cos \sqrt{A}x+ D  \sin \sqrt{A}x.$$ Also,  $$y'(x)=-C \sqrt{A} \sin \sqrt{A}x+ D \sqrt{A}  \cos \sqrt{A}x.$$ Using $y(0)=c_0$ and $y'(0)=c_1,$ I got $C=c_0$ and $D \sqrt{A}=c_1.$ Is my approach correct ? Can I just write $D=c_1 \cdot  (\sqrt{A})^{-1}$ ?  Any help is much appreiciated.","Consider the IVP : $y''(x)+A \cdot y(x)=0,$ where $A$ is an $n \times n$ positive definite matrix. Also $y(0)=c_0$ and $y'(0)=c_1,$ where $c_0 , c_1 \in \mathbb{R}^n$ are constant vectors. Since $A$ is positive definite, it possesses a square toot. The solution is given by  $$y(x)=C \cos \sqrt{A}x+ D  \sin \sqrt{A}x.$$ Also,  $$y'(x)=-C \sqrt{A} \sin \sqrt{A}x+ D \sqrt{A}  \cos \sqrt{A}x.$$ Using $y(0)=c_0$ and $y'(0)=c_1,$ I got $C=c_0$ and $D \sqrt{A}=c_1.$ Is my approach correct ? Can I just write $D=c_1 \cdot  (\sqrt{A})^{-1}$ ?  Any help is much appreiciated.",,['ordinary-differential-equations']
32,What will be the number of solutions for the givem differential equation?,What will be the number of solutions for the givem differential equation?,,"If $y''+(\sin(x^2))y' + ( \cos( x^3))y=0$ be a second order linear differential equation. If $y(1)=0 ;y'(1)=0$, then what is the number Of solutions.   What is the fundamental theorem of existence and uniqueness theorem . using that how do we solve this.?","If $y''+(\sin(x^2))y' + ( \cos( x^3))y=0$ be a second order linear differential equation. If $y(1)=0 ;y'(1)=0$, then what is the number Of solutions.   What is the fundamental theorem of existence and uniqueness theorem . using that how do we solve this.?",,['ordinary-differential-equations']
33,Unique solution for implicit multistep method for ODE,Unique solution for implicit multistep method for ODE,,"There is an exercise I came across that I struggle with. Let $f: \mathbb{R}^2 \to \mathbb{R}^2$ be lipschitz continuous in the second argument. How to choose the step size $h$ in the $m$-step method $$ \sum_{k=0}^{m} \alpha_k u_{j+k} = h \sum_{k=0}^{m} \beta_k f(t_{j+k}, u_{j+k}) \quad j= 0, ..., n-m \qquad (1)$$ under the assumption $\beta_m \neq 0$ for the solution of the IVP $$ u'(t) = f(t,y), t > t_0$$ $$ u(t_0) = u_0$$ such that the system of equations in (1) can be uniquely solved for $u_{j+m}$? I know that implicit methods can only be solved iteratively. Also, I was told to consider Banach's fixed-point theorem.  Nevertheless, I do not seem to get a grip on the general idea behind the exercise.","There is an exercise I came across that I struggle with. Let $f: \mathbb{R}^2 \to \mathbb{R}^2$ be lipschitz continuous in the second argument. How to choose the step size $h$ in the $m$-step method $$ \sum_{k=0}^{m} \alpha_k u_{j+k} = h \sum_{k=0}^{m} \beta_k f(t_{j+k}, u_{j+k}) \quad j= 0, ..., n-m \qquad (1)$$ under the assumption $\beta_m \neq 0$ for the solution of the IVP $$ u'(t) = f(t,y), t > t_0$$ $$ u(t_0) = u_0$$ such that the system of equations in (1) can be uniquely solved for $u_{j+m}$? I know that implicit methods can only be solved iteratively. Also, I was told to consider Banach's fixed-point theorem.  Nevertheless, I do not seem to get a grip on the general idea behind the exercise.",,"['ordinary-differential-equations', 'numerical-methods']"
34,Can anyone explain the differential equation for the sunflower seed pattern>,Can anyone explain the differential equation for the sunflower seed pattern>,,"It seems that there is an algorithm to generate the sunflower seed pattern based on the golden ratio. From this article in the Irish Times : A simple mathematical description of the geometry of sunflower seed   patterns was devised by Helmut Vogel (1979). He defined the positions   of the seeds, using polar coordinates (r, θ), by r(n) = √n         and         θ(n) = n φ where φ ≈ 137.5º is the golden angle. Thus, as n increases by one, the   position rotates through the golden angle and the radius increases as   the square root of n. All points are on a curve called the generative   spiral (r = √θ), a form of Fermat spiral which winds ever-tighter as   it curls outwards. But that doesn't explain how the sunflower actually does it. Now someone has come up with a differential equation that claims to generate the pattern. From the same article: The equation comes from a paper by Pennybacker and Newell which I don't have access to (and probably wouldn't understand if I did). Is this for real? Does this differential equation really work? And if so, can anyone give a physical explanation of what the individual terms are doing? I don't mean ""explain to me what the del operator does"", I mean like ""why do you subtract the cube of the hormone concentration..."" etc. Any insights?","It seems that there is an algorithm to generate the sunflower seed pattern based on the golden ratio. From this article in the Irish Times : A simple mathematical description of the geometry of sunflower seed   patterns was devised by Helmut Vogel (1979). He defined the positions   of the seeds, using polar coordinates (r, θ), by r(n) = √n         and         θ(n) = n φ where φ ≈ 137.5º is the golden angle. Thus, as n increases by one, the   position rotates through the golden angle and the radius increases as   the square root of n. All points are on a curve called the generative   spiral (r = √θ), a form of Fermat spiral which winds ever-tighter as   it curls outwards. But that doesn't explain how the sunflower actually does it. Now someone has come up with a differential equation that claims to generate the pattern. From the same article: The equation comes from a paper by Pennybacker and Newell which I don't have access to (and probably wouldn't understand if I did). Is this for real? Does this differential equation really work? And if so, can anyone give a physical explanation of what the individual terms are doing? I don't mean ""explain to me what the del operator does"", I mean like ""why do you subtract the cube of the hormone concentration..."" etc. Any insights?",,['ordinary-differential-equations']
35,Constants of integration for a linear system of differential equations,Constants of integration for a linear system of differential equations,,"Consider the system of linear differential equations \begin{align} x'(t) = \frac{x(t)}{t} - y(t) \tag1\\ y'(t) = \frac{y(t)}{t} + x(t) \tag2 \end{align} for $t>0$. An elegant method to solve this system is to multiply $(2)$ by the imaginary unit $i$ and adding that to $(1)$ to obtain \begin{align} x'(t) + iy'(t) = \frac{x(t) + iy(t)}{t}+ix(t)-y(t) = \frac{x(t) + iy(t)}{t}+i(x(t)+iy(t)) \tag3 \end{align} Letting $z(t):=x(t)+iy(t)$, $(3)$ becomes \begin{align} z'(t) = \left(\frac{1}{t}+i\right)z(t) \tag4 \end{align} which is a seperable equation that can easily be solved. This gives us \begin{align} z(t)=Cte^{it} \tag5 \end{align} where $C$ is an arbitrary constant. From there, $x(t)$ and $y(t)$ arise as the real and imaginary part of $z(t)$. What I'm having trouble with are the constants of integration. As this is a system of two first-order differential equations, shouldn't there be two constants of integration, one for each equation? What happened to the second one?","Consider the system of linear differential equations \begin{align} x'(t) = \frac{x(t)}{t} - y(t) \tag1\\ y'(t) = \frac{y(t)}{t} + x(t) \tag2 \end{align} for $t>0$. An elegant method to solve this system is to multiply $(2)$ by the imaginary unit $i$ and adding that to $(1)$ to obtain \begin{align} x'(t) + iy'(t) = \frac{x(t) + iy(t)}{t}+ix(t)-y(t) = \frac{x(t) + iy(t)}{t}+i(x(t)+iy(t)) \tag3 \end{align} Letting $z(t):=x(t)+iy(t)$, $(3)$ becomes \begin{align} z'(t) = \left(\frac{1}{t}+i\right)z(t) \tag4 \end{align} which is a seperable equation that can easily be solved. This gives us \begin{align} z(t)=Cte^{it} \tag5 \end{align} where $C$ is an arbitrary constant. From there, $x(t)$ and $y(t)$ arise as the real and imaginary part of $z(t)$. What I'm having trouble with are the constants of integration. As this is a system of two first-order differential equations, shouldn't there be two constants of integration, one for each equation? What happened to the second one?",,['ordinary-differential-equations']
36,Mathieu functions calculation,Mathieu functions calculation,,"I am currently working in MATLAB (this is not relevant to the question, though), and MATLAB doesn't have built-in Mathieu functions (such as the Mathieu cosine and sine). In principle this is just fine, because I can generate them with a RK or predictor corrector method. However, this calculates all the points in the function up to a certain $x$, and that is not very efficient if I'm only interested in one specific $f(x_i)$, for a given $x_i$. Could you provide references with explicit algorithms to calculate the Mathieu sine and cosine functions? Any other comments or suggestions are of course also welcome. Edit : to clarify, I'm looking to generate the functions as defined here or here (where they are referred as basic solutions). I'm not looking for the periodic solutions, but the (normalized) odd and even fundamental solutions of the equation $$ \ddot{x} + (a - 2q\cos 2 \tau)x = 0 $$ and also the characteristic exponent $\nu(a,q)$","I am currently working in MATLAB (this is not relevant to the question, though), and MATLAB doesn't have built-in Mathieu functions (such as the Mathieu cosine and sine). In principle this is just fine, because I can generate them with a RK or predictor corrector method. However, this calculates all the points in the function up to a certain $x$, and that is not very efficient if I'm only interested in one specific $f(x_i)$, for a given $x_i$. Could you provide references with explicit algorithms to calculate the Mathieu sine and cosine functions? Any other comments or suggestions are of course also welcome. Edit : to clarify, I'm looking to generate the functions as defined here or here (where they are referred as basic solutions). I'm not looking for the periodic solutions, but the (normalized) odd and even fundamental solutions of the equation $$ \ddot{x} + (a - 2q\cos 2 \tau)x = 0 $$ and also the characteristic exponent $\nu(a,q)$",,"['ordinary-differential-equations', 'reference-request', 'special-functions']"
37,Laplace transform of an integral?,Laplace transform of an integral?,,"I have the following integral: $$ I = \int_{0}^{t} e^{-\tau}\theta(t-\tau )d\tau $$ where $\theta(t) \left\{\begin{matrix}  0&  0 \leq t < 1\\   5 & t\geq 1 \end{matrix}\right.$ is the Heaviside step function. $$$$ I need to get the Laplace transform of it. I know there is a rule for it: $ \mathcal{L} \left \{ I \right \} = F(s)G(s), $ but I don't know if its correct the do the transformation as follows: $$ \mathcal{L} \left \{  e^{-\tau} \right \}= e^{-\tau}\mathcal{L}\left \{  1\right \}$$ Because the transformation should be in respect of $ t $ Thanks in advance","I have the following integral: $$ I = \int_{0}^{t} e^{-\tau}\theta(t-\tau )d\tau $$ where $\theta(t) \left\{\begin{matrix}  0&  0 \leq t < 1\\   5 & t\geq 1 \end{matrix}\right.$ is the Heaviside step function. $$$$ I need to get the Laplace transform of it. I know there is a rule for it: $ \mathcal{L} \left \{ I \right \} = F(s)G(s), $ but I don't know if its correct the do the transformation as follows: $$ \mathcal{L} \left \{  e^{-\tau} \right \}= e^{-\tau}\mathcal{L}\left \{  1\right \}$$ Because the transformation should be in respect of $ t $ Thanks in advance",,"['ordinary-differential-equations', 'laplace-transform']"
38,Math question - specifying the range of ln,Math question - specifying the range of ln,,"The original problem is this: $$\lim_{x\to 0}\frac{1}{2x}\ln \frac{1}{n}\sum_1^n {e^{kx}} = 20$$ Find out the value of $n$, which is a natural number. But I'm having a bit of trouble solving this problem. My problem is that I think that this question can be interpreted in two ways. Since there is no brackets limiting the range of the $\ln$, I believe it can be interpreted as both : 1) $$ \lim_{x\to 0}\frac{1}{2x}\ln \left(\frac{1}{n}\right)\sum_1^n {e^{kx}} = 20 $$ 2) $$ \lim_{x\to 0}\frac{1}{2x}\ln \left(\frac{1}{n}\sum_1^n {e^{kx}}\right) = 20 $$ If I interpret the problem as 2), the value of $n$ becomes $79$. If I interpret the problem as 1), I can't get the value of $n$. Am I right in assuming that all two of my interpretations are correct? What I mean is, just like $\cos x \cos x$ is different from $\cos(x \cos x)$, shouldn't the range of the ln be specified using brackets? Without these brackets, can this equation be interpreted in two ways, just like I did? And if my first interpretation is correct, could anyone please explain in detail why ln and sigma can be diverged? I certainly learned it at some point at school, but I don't exactly remember the specific details.","The original problem is this: $$\lim_{x\to 0}\frac{1}{2x}\ln \frac{1}{n}\sum_1^n {e^{kx}} = 20$$ Find out the value of $n$, which is a natural number. But I'm having a bit of trouble solving this problem. My problem is that I think that this question can be interpreted in two ways. Since there is no brackets limiting the range of the $\ln$, I believe it can be interpreted as both : 1) $$ \lim_{x\to 0}\frac{1}{2x}\ln \left(\frac{1}{n}\right)\sum_1^n {e^{kx}} = 20 $$ 2) $$ \lim_{x\to 0}\frac{1}{2x}\ln \left(\frac{1}{n}\sum_1^n {e^{kx}}\right) = 20 $$ If I interpret the problem as 2), the value of $n$ becomes $79$. If I interpret the problem as 1), I can't get the value of $n$. Am I right in assuming that all two of my interpretations are correct? What I mean is, just like $\cos x \cos x$ is different from $\cos(x \cos x)$, shouldn't the range of the ln be specified using brackets? Without these brackets, can this equation be interpreted in two ways, just like I did? And if my first interpretation is correct, could anyone please explain in detail why ln and sigma can be diverged? I certainly learned it at some point at school, but I don't exactly remember the specific details.",,"['ordinary-differential-equations', 'exponential-function']"
39,Solving a system of equations involving smooth functions,Solving a system of equations involving smooth functions,,"This is a follow-up question of my previous question . Suppose $h_{i\overline{j}}$, where $1\leq i, j\leq n$, are functions defined on $\mathbb{C}^n$ such that $h_{i\overline{j}}$ are smooth when viewed as functions defined on $\mathbb{R}^{2n}$ and $H=(h_{i\overline{j}})$ is a Hermitian positive definite matrix. I wonder if it is always possible to find smooth functions $p_1,..., p_n$ defined on $\mathbb{R}^{2n}$ such that  $$\frac{\partial p_i}{\partial z_j}+\overline{\frac{\partial p_j}{\partial z_i}}=h_{i\overline{j}}\mbox{ and } \frac{\partial p_i}{\partial \overline{z}_j} -\frac{\partial p_j}{\partial \overline{z}_i}=0.$$ In the previous question, I require $p_i$ to be holomorphic, which seems to be too strong.","This is a follow-up question of my previous question . Suppose $h_{i\overline{j}}$, where $1\leq i, j\leq n$, are functions defined on $\mathbb{C}^n$ such that $h_{i\overline{j}}$ are smooth when viewed as functions defined on $\mathbb{R}^{2n}$ and $H=(h_{i\overline{j}})$ is a Hermitian positive definite matrix. I wonder if it is always possible to find smooth functions $p_1,..., p_n$ defined on $\mathbb{R}^{2n}$ such that  $$\frac{\partial p_i}{\partial z_j}+\overline{\frac{\partial p_j}{\partial z_i}}=h_{i\overline{j}}\mbox{ and } \frac{\partial p_i}{\partial \overline{z}_j} -\frac{\partial p_j}{\partial \overline{z}_i}=0.$$ In the previous question, I require $p_i$ to be holomorphic, which seems to be too strong.",,"['ordinary-differential-equations', 'partial-differential-equations', 'systems-of-equations', 'partial-derivative', 'holomorphic-functions']"
40,"Is there a known transformation between $_2F_1\big(\tfrac12,\tfrac12;1;z\big)$ and $_2F_1\big(\tfrac12,\tfrac12;1;z^2\big)$?",Is there a known transformation between  and ?,"_2F_1\big(\tfrac12,\tfrac12;1;z\big) _2F_1\big(\tfrac12,\tfrac12;1;z^2\big)","In this post , the OP seeks a closed-form for, $$A=\,_2F_1\big(\tfrac12,\tfrac12;1;\tfrac19\big)=1.02966\dots$$ Using the transformation, $$\,_2F_1\big(\tfrac12,\tfrac12;1;z\big) = \tfrac2{1+\sqrt{1-z}}\,_2F_1\Big(\tfrac12,\tfrac12;1;\big(\tfrac{1-\sqrt{1-z}}{1+\sqrt{1-z}}\big)^2\Big)$$ or more generally ( DLMF 15.8.21 ), $$\,_2F_1\big(a,b;a-b+1;z\big) = (1+\sqrt{z})^{-2a}\,_2F_1\Big(a,a-b+\tfrac12;2a-2b+1;\tfrac{4\sqrt{z}}{(1+\sqrt{z})^2}\Big)$$ we can transform $A$ to, $$A =6\times\frac{\,_2F_1\big(\tfrac12,\tfrac12;1;\color{blue}{(1-\sqrt2)^8}\big)}{(1+\sqrt2)^2}$$ However, it turns out that the following do have a closed-form, $$\begin{aligned} B&=\,_2F_1\big(\tfrac12,\tfrac12;1;\color{blue}{(1-\sqrt2)^4}\big)=1.00748\dots\\[2.0mm] &=\frac{\big(1+\sqrt2\big)\Gamma^2\big(\tfrac14\big)}{2^{5/2}\,\pi^{3/2}}\end{aligned}$$ $$\begin{aligned} C&=\,_2F_1\big(\tfrac12,\tfrac12;1;\color{blue}{(1-\sqrt2)^2}\big)=1.04760\dots\\[2.0mm] &=\frac{\sqrt{1+\sqrt2}\,\Gamma\big(\tfrac18\big)\Gamma\big(\tfrac38\big)}{2^{9/4}\,\pi^{3/2}}\end{aligned}$$ Q: So, is there a known transformation between $_2F_1\big(\tfrac12,\tfrac12;1;z\big)$ and $_2F_1\big(\tfrac12,\tfrac12;1;z^2\big)$?","In this post , the OP seeks a closed-form for, $$A=\,_2F_1\big(\tfrac12,\tfrac12;1;\tfrac19\big)=1.02966\dots$$ Using the transformation, $$\,_2F_1\big(\tfrac12,\tfrac12;1;z\big) = \tfrac2{1+\sqrt{1-z}}\,_2F_1\Big(\tfrac12,\tfrac12;1;\big(\tfrac{1-\sqrt{1-z}}{1+\sqrt{1-z}}\big)^2\Big)$$ or more generally ( DLMF 15.8.21 ), $$\,_2F_1\big(a,b;a-b+1;z\big) = (1+\sqrt{z})^{-2a}\,_2F_1\Big(a,a-b+\tfrac12;2a-2b+1;\tfrac{4\sqrt{z}}{(1+\sqrt{z})^2}\Big)$$ we can transform $A$ to, $$A =6\times\frac{\,_2F_1\big(\tfrac12,\tfrac12;1;\color{blue}{(1-\sqrt2)^8}\big)}{(1+\sqrt2)^2}$$ However, it turns out that the following do have a closed-form, $$\begin{aligned} B&=\,_2F_1\big(\tfrac12,\tfrac12;1;\color{blue}{(1-\sqrt2)^4}\big)=1.00748\dots\\[2.0mm] &=\frac{\big(1+\sqrt2\big)\Gamma^2\big(\tfrac14\big)}{2^{5/2}\,\pi^{3/2}}\end{aligned}$$ $$\begin{aligned} C&=\,_2F_1\big(\tfrac12,\tfrac12;1;\color{blue}{(1-\sqrt2)^2}\big)=1.04760\dots\\[2.0mm] &=\frac{\sqrt{1+\sqrt2}\,\Gamma\big(\tfrac18\big)\Gamma\big(\tfrac38\big)}{2^{9/4}\,\pi^{3/2}}\end{aligned}$$ Q: So, is there a known transformation between $_2F_1\big(\tfrac12,\tfrac12;1;z\big)$ and $_2F_1\big(\tfrac12,\tfrac12;1;z^2\big)$?",,"['ordinary-differential-equations', 'closed-form', 'gamma-function', 'hypergeometric-function']"
41,Solve equation applying the Laplace transform,Solve equation applying the Laplace transform,,"I'm stuck solving this equation:$$t^2y''+2ty'+t^2y=0 \qquad \text{with} \quad y(0)=0 , \; y'(0)=2 $$ I applied the Laplace transform and get this: $$s^2Y''(s)+Y''(s) + 2sY'(s)=0$$ I tried to solve this by Series, is it correct, or is there another way to solve it? (the answer is $y=-C\frac{\sin t}{t}$)","I'm stuck solving this equation:$$t^2y''+2ty'+t^2y=0 \qquad \text{with} \quad y(0)=0 , \; y'(0)=2 $$ I applied the Laplace transform and get this: $$s^2Y''(s)+Y''(s) + 2sY'(s)=0$$ I tried to solve this by Series, is it correct, or is there another way to solve it? (the answer is $y=-C\frac{\sin t}{t}$)",,"['ordinary-differential-equations', 'taylor-expansion', 'laplace-transform', 'harmonic-functions', 'laplace-method']"
42,SIR infection induced mortality,SIR infection induced mortality,,"I am reading the book Modelling Infectious Diseases in Humans and Animals by Matt J Keeling and Pejman Rohani. In it the SIR model is given as, $\frac{dS}{dt}  =\mu-\beta{S}I-\mu S$ $\frac{dI}{dt}= \beta SI - \gamma I-\mu I$ $\frac{dR}{dt}= \gamma I-\mu R$ where the rate at which individuals in any epidemiological class suffer natural mortality is given by $\mu$ and S,I,R are respectively the proportion of susceptible, infected and recovered individuals in the population. For this model when the infection induced mortality is introduced the following change is made. (page 34) $\frac{dI}{dt}= \beta SI - (\gamma+\mu) I-\frac {\rho}{1-\rho} (\gamma+\mu)  I$  where $\rho $ is the probability that an individual in I class dying from the infection before either recovering or dying from natural causes. In this I don't understand 1) How does $\frac {\rho}{1-\rho} (\gamma+\mu)  I$ capture the infection induced mortality. 2) Also it says rather than having the per capita disease-induced mortality rate for infected individuals it is preferable to think of $\rho$, probability that an individual in I class dying from the infection before either recovering or dying from natural causes. Why is this probability easy than mortality rate?","I am reading the book Modelling Infectious Diseases in Humans and Animals by Matt J Keeling and Pejman Rohani. In it the SIR model is given as, $\frac{dS}{dt}  =\mu-\beta{S}I-\mu S$ $\frac{dI}{dt}= \beta SI - \gamma I-\mu I$ $\frac{dR}{dt}= \gamma I-\mu R$ where the rate at which individuals in any epidemiological class suffer natural mortality is given by $\mu$ and S,I,R are respectively the proportion of susceptible, infected and recovered individuals in the population. For this model when the infection induced mortality is introduced the following change is made. (page 34) $\frac{dI}{dt}= \beta SI - (\gamma+\mu) I-\frac {\rho}{1-\rho} (\gamma+\mu)  I$  where $\rho $ is the probability that an individual in I class dying from the infection before either recovering or dying from natural causes. In this I don't understand 1) How does $\frac {\rho}{1-\rho} (\gamma+\mu)  I$ capture the infection induced mortality. 2) Also it says rather than having the per capita disease-induced mortality rate for infected individuals it is preferable to think of $\rho$, probability that an individual in I class dying from the infection before either recovering or dying from natural causes. Why is this probability easy than mortality rate?",,"['ordinary-differential-equations', 'mathematical-modeling', 'biology']"
43,How to integrate the following elliptic curve period,How to integrate the following elliptic curve period,,"Let $E$ be an elliptic curve given by  $$ y^2=x^3-fx^2+\frac{1}{b}x $$ and let one the cycles of the torus be $A$. Then the period integral is \begin{equation}a= \oint_A \lambda  \end{equation} and this is given by the solution of $$ \frac{d \lambda}{df}=\frac{dx}{y}.$$ So, how exactly do we find $a$? I am trying to directly put $y = \sqrt{x^3-fx^2+\frac{1}{b}x}$ in the differential equation but I do not get somewhere. I am sure there is some kind of identity helping with this. It seems a standard thing to do in the theory of elliptic curves but nowhere is done explicitly.","Let $E$ be an elliptic curve given by  $$ y^2=x^3-fx^2+\frac{1}{b}x $$ and let one the cycles of the torus be $A$. Then the period integral is \begin{equation}a= \oint_A \lambda  \end{equation} and this is given by the solution of $$ \frac{d \lambda}{df}=\frac{dx}{y}.$$ So, how exactly do we find $a$? I am trying to directly put $y = \sqrt{x^3-fx^2+\frac{1}{b}x}$ in the differential equation but I do not get somewhere. I am sure there is some kind of identity helping with this. It seems a standard thing to do in the theory of elliptic curves but nowhere is done explicitly.",,"['integration', 'ordinary-differential-equations', 'elliptic-curves']"
44,Numerical solution of a BVP with a boundary value at infinity,Numerical solution of a BVP with a boundary value at infinity,,Lets say we want to solve $y''=x y$ with boundary conditions $y(0)=1$ and $y(\infty)=0$ I know that this is the airy equation but how would one solve such an equation numerically?,Lets say we want to solve $y''=x y$ with boundary conditions $y(0)=1$ and $y(\infty)=0$ I know that this is the airy equation but how would one solve such an equation numerically?,,"['ordinary-differential-equations', 'numerical-methods', 'boundary-value-problem']"
45,Does the Laplace Transform have any practical use or provide any mathematical insight?,Does the Laplace Transform have any practical use or provide any mathematical insight?,,"Mathematics essentially is the study of the transformation of symbols according to specific rules (inference rules/axioms).  However we don't just study any arbitrary system of mathematics; we carefully pick ones that make it easier to understand and create solutions for the real world. What exactly has the Laplace Transform helped simplify or understand better (in addition to the various simpler tools we already have, Fourier Transforms and Linear Algebra)? In control systems analysis, I've seen the Laplace transform being used to create $s$-domain transfer functions that represent linear time-invariant systems.  However, when various graphs for analysis are created (Nyquist, Nichols, Bode) we substitute $\mathbb{j}\omega$ into the $s$-domain functions.  This would make it equivalent to a Fourier transform. The poles of the $s$-domain function are used to help understand the stability of a system.  It can be shown that these poles are the same as the eigenvalues of the linear operator; however the eigenvalues of the linear operator can be clearly associated with their time-domain modes and gives a direct understanding of why an LTI-system is stable if the eigenvalues are all less than 0. Is there any particular application of the Laplace transform that clearly makes something easier to understand or do?  Why is the Laplace transform so heavily taught in schools?","Mathematics essentially is the study of the transformation of symbols according to specific rules (inference rules/axioms).  However we don't just study any arbitrary system of mathematics; we carefully pick ones that make it easier to understand and create solutions for the real world. What exactly has the Laplace Transform helped simplify or understand better (in addition to the various simpler tools we already have, Fourier Transforms and Linear Algebra)? In control systems analysis, I've seen the Laplace transform being used to create $s$-domain transfer functions that represent linear time-invariant systems.  However, when various graphs for analysis are created (Nyquist, Nichols, Bode) we substitute $\mathbb{j}\omega$ into the $s$-domain functions.  This would make it equivalent to a Fourier transform. The poles of the $s$-domain function are used to help understand the stability of a system.  It can be shown that these poles are the same as the eigenvalues of the linear operator; however the eigenvalues of the linear operator can be clearly associated with their time-domain modes and gives a direct understanding of why an LTI-system is stable if the eigenvalues are all less than 0. Is there any particular application of the Laplace transform that clearly makes something easier to understand or do?  Why is the Laplace transform so heavily taught in schools?",,"['ordinary-differential-equations', 'laplace-transform', 'control-theory']"
46,Exercises where multivariable calculus and differential equations are mixed,Exercises where multivariable calculus and differential equations are mixed,,"I have a course where multivariable calculus and differential equations are mixed, as an application of what you did in class, Hadamard's theorem: Let $f\in \mathcal{C}^2(\Bbb{R}^n,\Bbb{R}^n)$ If $f(0)=0$ and that for all $x\in \Bbb{R}^n$ the derivative is invertible and satisfies $$\Vert (df_x)^{-1}\Vert\le A\Vert x\Vert+B$$ where $A,B$ are two fixed constant. Then $f$ is a diffeomorphism. It was very interesting, we use Cauchy-Lipschitz's Theorem, explosion in finite time and local inversion theorem. Unfortunately , I didn't find a suitable reference in my library for these kind of exercises. I am not sure where I am suppose to search, I am looking for exercises, any ideas?","I have a course where multivariable calculus and differential equations are mixed, as an application of what you did in class, Hadamard's theorem: Let If and that for all the derivative is invertible and satisfies where are two fixed constant. Then is a diffeomorphism. It was very interesting, we use Cauchy-Lipschitz's Theorem, explosion in finite time and local inversion theorem. Unfortunately , I didn't find a suitable reference in my library for these kind of exercises. I am not sure where I am suppose to search, I am looking for exercises, any ideas?","f\in \mathcal{C}^2(\Bbb{R}^n,\Bbb{R}^n) f(0)=0 x\in \Bbb{R}^n \Vert (df_x)^{-1}\Vert\le A\Vert x\Vert+B A,B f","['ordinary-differential-equations', 'multivariable-calculus', 'reference-request']"
47,Is this proof of a version of Lyapunov stability correct?,Is this proof of a version of Lyapunov stability correct?,,"In a textbook I have seen a stability result known as Dirichlet's theorem: let $$\dot{x} = f(x)$$ be  $C^{r}$  a vector field on $\mathbb{R}^{n}$ where $r \geq 1$, with a fixed point at $x = \bar{x}$. Let $I(x)$ be first integral of the vector field defined in a neighbourhood of $x = \bar{x}$ such that $x = \bar{x}$ is a nondegenerate minimum of $I(x)$. Then $x = \bar{x}$ is Lyapunov stable. My proof: since $I(x)$ is the first integral, then $\dot{I}(x) = 0$. Since $\bar{x}$ is the nondegenerate minimum of $I(x)$, we have $I(x) > I(\bar{x})$ for all $x \neq \bar{x}$, in a neighbourhood of $\bar{x}$. Define $\tilde{I}(x) = I(x) - I(\bar{x})$. Then $\tilde{I}(\bar{x}) = 0$ and $\tilde{I}(x) >0$ for $x \neq \bar{x}$, in an neighbourhood of $\bar{x}$. Since $\dot{\tilde{I}} = \dot{I}=0$, we may define $\tilde{I}$ to be the Lyapunov function and by the above criteria $\bar{x}$ is indeed stable. Is this correct?","In a textbook I have seen a stability result known as Dirichlet's theorem: let $$\dot{x} = f(x)$$ be  $C^{r}$  a vector field on $\mathbb{R}^{n}$ where $r \geq 1$, with a fixed point at $x = \bar{x}$. Let $I(x)$ be first integral of the vector field defined in a neighbourhood of $x = \bar{x}$ such that $x = \bar{x}$ is a nondegenerate minimum of $I(x)$. Then $x = \bar{x}$ is Lyapunov stable. My proof: since $I(x)$ is the first integral, then $\dot{I}(x) = 0$. Since $\bar{x}$ is the nondegenerate minimum of $I(x)$, we have $I(x) > I(\bar{x})$ for all $x \neq \bar{x}$, in a neighbourhood of $\bar{x}$. Define $\tilde{I}(x) = I(x) - I(\bar{x})$. Then $\tilde{I}(\bar{x}) = 0$ and $\tilde{I}(x) >0$ for $x \neq \bar{x}$, in an neighbourhood of $\bar{x}$. Since $\dot{\tilde{I}} = \dot{I}=0$, we may define $\tilde{I}$ to be the Lyapunov function and by the above criteria $\bar{x}$ is indeed stable. Is this correct?",,"['ordinary-differential-equations', 'dynamical-systems', 'classical-mechanics', 'stability-in-odes']"
48,Second order evolution ODE,Second order evolution ODE,,"I'm studying an article about the decay of solutions to some evolution equations of second order and i'm trying to understand the simple ODE case : $y'' +  y' + f(y) = 0 $ with $f(y)=y^2$ or $y^3$ or more generally $f(y)=c|y|^{a-1}y$, $c \in \mathbb{R}, a>1$ I would love to understand just the case $y'' +  y' + y^2 = 0$. Any help please ? I thought about using Cauchy-Lipschitz for the existence and uniqueness of the solution.","I'm studying an article about the decay of solutions to some evolution equations of second order and i'm trying to understand the simple ODE case : $y'' +  y' + f(y) = 0 $ with $f(y)=y^2$ or $y^3$ or more generally $f(y)=c|y|^{a-1}y$, $c \in \mathbb{R}, a>1$ I would love to understand just the case $y'' +  y' + y^2 = 0$. Any help please ? I thought about using Cauchy-Lipschitz for the existence and uniqueness of the solution.",,['ordinary-differential-equations']
49,Why the geodesic line is the projections of the integral curves of the geodesic flow?,Why the geodesic line is the projections of the integral curves of the geodesic flow?,,"Picture below is from 54 page of  Jost's Riemannian Geometry and Geometric Analysis , Why the geodesic line is the projections of the integral curves of the geodesic flow ?","Picture below is from 54 page of  Jost's Riemannian Geometry and Geometric Analysis , Why the geodesic line is the projections of the integral curves of the geodesic flow ?",,"['ordinary-differential-equations', 'differential-geometry', 'riemannian-geometry', 'geodesic']"
50,Find the solution of the differential equations $p^2-y^2=0$ and $(p+1)^3=27(x+y)^2$,Find the solution of the differential equations  and,p^2-y^2=0 (p+1)^3=27(x+y)^2,Find the solution of the differential equations : i) $p^2-y^2=0$ ii) $(p+1)^3=27(x+y)^2$ These questions in (non linear ordinary differential equations - can be solve in y) My answer: For (i) $p^2=y^2$ $p=y$ ${dp\over dx}={dy\over dx}$ And we know [$p={dy\over dx}$] So ${dp\over dx}=p$ $\ln p=x+\ln c$ $p=ce^x$ By delet $p$ from : $y=p$ and $p=ce^x$ We get : $y=ce^x$ True ? And for (ii) $(p+1)^3=27(x+y)^2$ $3(p+1)^2 {dp\over dx}=54(x+y+x{dy\over dx}+y{dy\over dx})$ $3(p+1)^2 {dp\over dx}=54(x+y+xp+yp)$ $3(p+1)^2 {dp\over dx}=54(x(p+1)+y(p+1))$ ${dp\over dx}={18(x+y)(p+1)\over (p+1)^2}$ ${dp\over dx}={18(x+y)\over (p+1)}$ Is it true? And how can I complete ? Thanks.,Find the solution of the differential equations : i) $p^2-y^2=0$ ii) $(p+1)^3=27(x+y)^2$ These questions in (non linear ordinary differential equations - can be solve in y) My answer: For (i) $p^2=y^2$ $p=y$ ${dp\over dx}={dy\over dx}$ And we know [$p={dy\over dx}$] So ${dp\over dx}=p$ $\ln p=x+\ln c$ $p=ce^x$ By delet $p$ from : $y=p$ and $p=ce^x$ We get : $y=ce^x$ True ? And for (ii) $(p+1)^3=27(x+y)^2$ $3(p+1)^2 {dp\over dx}=54(x+y+x{dy\over dx}+y{dy\over dx})$ $3(p+1)^2 {dp\over dx}=54(x+y+xp+yp)$ $3(p+1)^2 {dp\over dx}=54(x(p+1)+y(p+1))$ ${dp\over dx}={18(x+y)(p+1)\over (p+1)^2}$ ${dp\over dx}={18(x+y)\over (p+1)}$ Is it true? And how can I complete ? Thanks.,,['ordinary-differential-equations']
51,Matrix sets that are open and/or dense,Matrix sets that are open and/or dense,,"Each of the following properties define a subset of real n x n matrices. Which of these sets are open and/or dense in the $L(R^n)$? Give a brief reason in each case. a) $det A \ne 0$ b) Trace A is rational c) Entries of A are not integers d) $3 \le det A \lt 4$ e) $-1\lt |\lambda|\lt 1$ for every eigenvalue $\lambda$ f) A has no real eigenvalues g) Each real eigenvalue of A has multiplicity one My textbook lists the following definitions for open and dense: a set U is open if whenever $x\in U$ there is an open ball about x contained in U, or that any point sufficiently near x is in U. A set is dense if there are points in U arbitrarily close to each point in $R^n$. From these definitions, I have a few guesses about some of the a)-g) above, but I'm not sure all the guesses are right, and some of the parts I still don't have guesses for.","Each of the following properties define a subset of real n x n matrices. Which of these sets are open and/or dense in the $L(R^n)$? Give a brief reason in each case. a) $det A \ne 0$ b) Trace A is rational c) Entries of A are not integers d) $3 \le det A \lt 4$ e) $-1\lt |\lambda|\lt 1$ for every eigenvalue $\lambda$ f) A has no real eigenvalues g) Each real eigenvalue of A has multiplicity one My textbook lists the following definitions for open and dense: a set U is open if whenever $x\in U$ there is an open ball about x contained in U, or that any point sufficiently near x is in U. A set is dense if there are points in U arbitrarily close to each point in $R^n$. From these definitions, I have a few guesses about some of the a)-g) above, but I'm not sure all the guesses are right, and some of the parts I still don't have guesses for.",,"['matrices', 'ordinary-differential-equations']"
52,Solving the following recurrence equation involving polynomials.,Solving the following recurrence equation involving polynomials.,,"From a problem I have I derived the following equation $$ \left\{ \begin{array}{l} \left(2k + 1\right)xP_k(x) + (1-x^2)\frac{d}{dx}P_k(x) = P_{k+1}(x) \\ P_1(x)=1 \end{array} \right. . $$ The equation is derived by my guess that for all $k \geq 1$ $$ \frac{d^k}{dx^k} \arcsin(x) = \frac{1}{\left(1-x^2\right)^{k+\frac{1}{2}}} P_k(x) $$ Where $P_k(x)$ is a polynomial of degree $k-1$. (I suppose that you can prove it by induction). Is there a way you can suggest me to solve that equation? The purpose would be basically find all the coefficients of $P_k$ for given $k$. Update : Attempt I made I think I've derived a recurrence relationship for $P_k(x)$ Let's consider the relationship $$ \left(2k - 1\right)xP_{k-1}(x) + (1-x^2)\frac{d}{dx}P_{k-1}(x) = P_{k}(x) $$ Let's express $$ P_k(x)= \sum_{j=-\infty}^{+\infty}a_{j,k}x^j, \;\; a_{j,k}=0 \;\; \forall j < 0, j > k - 1 $$ Substituing in recurrence we get $$ \sum_{j=-\infty}^{+\infty} \left[ \left(2k -j\right)a_{j-1,k-1} + \left(j+1 \right)a_{j+1,k-1}\right]x^j = \sum_{j=-\infty}^{+\infty}a_{j,k}x^j $$ To be equal the two polynomial it must be verified that $$ \left(2k -j\right)a_{j-1,k-1} + \left(j+1 \right)a_{j+1,k-1} = a_{j,k} $$ What puzzles me in the relantionship above is that the $j$-th coefficient of $P_k$ depends from the coefficient $j+1$-th of $P_{k-1}$ do you think there's some mistakes in my derivation? I used series instead of summation because it makes easier to handle boundary conditions.","From a problem I have I derived the following equation $$ \left\{ \begin{array}{l} \left(2k + 1\right)xP_k(x) + (1-x^2)\frac{d}{dx}P_k(x) = P_{k+1}(x) \\ P_1(x)=1 \end{array} \right. . $$ The equation is derived by my guess that for all $k \geq 1$ $$ \frac{d^k}{dx^k} \arcsin(x) = \frac{1}{\left(1-x^2\right)^{k+\frac{1}{2}}} P_k(x) $$ Where $P_k(x)$ is a polynomial of degree $k-1$. (I suppose that you can prove it by induction). Is there a way you can suggest me to solve that equation? The purpose would be basically find all the coefficients of $P_k$ for given $k$. Update : Attempt I made I think I've derived a recurrence relationship for $P_k(x)$ Let's consider the relationship $$ \left(2k - 1\right)xP_{k-1}(x) + (1-x^2)\frac{d}{dx}P_{k-1}(x) = P_{k}(x) $$ Let's express $$ P_k(x)= \sum_{j=-\infty}^{+\infty}a_{j,k}x^j, \;\; a_{j,k}=0 \;\; \forall j < 0, j > k - 1 $$ Substituing in recurrence we get $$ \sum_{j=-\infty}^{+\infty} \left[ \left(2k -j\right)a_{j-1,k-1} + \left(j+1 \right)a_{j+1,k-1}\right]x^j = \sum_{j=-\infty}^{+\infty}a_{j,k}x^j $$ To be equal the two polynomial it must be verified that $$ \left(2k -j\right)a_{j-1,k-1} + \left(j+1 \right)a_{j+1,k-1} = a_{j,k} $$ What puzzles me in the relantionship above is that the $j$-th coefficient of $P_k$ depends from the coefficient $j+1$-th of $P_{k-1}$ do you think there's some mistakes in my derivation? I used series instead of summation because it makes easier to handle boundary conditions.",,"['calculus', 'analysis', 'ordinary-differential-equations', 'polynomials', 'recurrence-relations']"
53,Finding mixed strategy Nash equilibria for a game with infinite strategies,Finding mixed strategy Nash equilibria for a game with infinite strategies,,"I run into this problem in my attempt to answer another question on math.stackexchange (In the original problem the strategies can be made discrete and finite, but I am interested in the infinite version here) We have a 2-player game with an infinite set of strategies. The strategies are essentially the same basic strategy with a parameter that tweaks it. The parameter takes values between $0$ and $1$, hence infinite strategies are born. Let's name these strategies $H_a$ where $a$ is the parameter that can change. We also know the payoff function that provides the gain of one strategy over another. Let's denote the gain of strategy $H_b$ over strategy $H_a$ as $G(H_b, H_a)$. The payoff is symmetrical, so one's gain is the other one's loss. For our particular game the payoff function looks like this $$G(H_b, H_a) = \begin{cases} G^-(H_b, H_a), & \text{if $a \le b$} \\ G^+(H_b, H_a), & \text{if $a>b$} \end{cases}$$ with $$G^-(H_b, H_a) = b\left[\frac{a(b-a)}{b} - (1-a)\frac{a-b+2}{2}\right] + (1-b)\left[a + (1-a)\frac{2(b-a)}{1-a}\right]$$ $$G^+(H_b, H_a) = b\left[\frac{a(b-a)}{a} - (1-a)\right] + (1-b)\left[a\cdot\frac{b-a+2}{2} + (1-a)\frac{2(b-a)}{1-b}\right]$$ Simplifying them so that their quadratic form becomes apparent: $$G^-(H_b, H_a) = (a^2b - ab^2 + 5ab - 2a^2 - 3b^2 +2b -2a)/2$$ $$G^+(H_b, H_a) = (a^2b - ab^2 - 5ab + 3a^2 + 2b^2 +2b -2a)/2$$ From either of these formulas we can also calculate that $G(H_a, H_a)=0$ There is no dominant strategy. For any strategy $H_a$ you can find strategies that are better, and strategies that are worse. To get a better a sense have a look at the following two graphs. They show the gain of two strategies $H_{0.5}$ and  $H_{0.8}$ against any possible strategy. The $x$ axis on both graphs is just the parameter $a$ going from $0$ to $1$ while the $y$ axis is $G(H_{0.5}, H_a)$ for the graph on the left and $G(H_{0.8}, H_a)$ for the graph on the right. You can notice for example that $H_{0.5}$ is better than $H_{0.2}$, $H_{0.8}$ is better than $H_{0.5}$, but $H_{0.2}$ is better than $H_{0.8}$. How do we find the mixed strategy Nash equilibrium for our problem? In other words, what is the p.d.f. $f_M$ of random variable M that we use to define our Nash Equilibrium mixed strategy $H_M$? When I was first faced with this problem I did not have any formal knowledge of mixed strategies or Nash equilibria, but I did try intuitively the simplest mixed strategy: $M$ being uniformly distributed in the interval $[0,1]$. I realised that this does not lead to an equilibrium and then I stopped because I was not sure whether this meant that no equilibria exist, or that I needed to find a different distribution for $M$. I read a bit on game theory and mixed strategy equilibria before posting this question and now I believe that a mixed strategy should exist . The infinite number of strategies might complicate things but this is how I am seeing the problem: Since this is a symmetrical zero-sum game I should find the distribution of $M$ such that whatever strategy $H_a$ my opponent chooses, my expected gain will be zero. More formally, we need to find the p.d.f. for $M$ such that $\underset{M}{\mathbb E}[G(H_M, H_a)]=0, \forall a \in [0,1]$. (where $\underset{X}{\mathbb E}[F(X)]$ is the expected value of function $F()$ over random variable $X$ ) Is this right? If so, then I believe we can proceed like so: $$\underset{M}{\mathbb E}[G(H_M, H_a)]=0, \forall a \in [0,1] \Leftrightarrow$$ if $f_M$ is the probability density function of random variable M we get  $$\int_0^1 G(H_x, H_a)f_M(x)dx =0, \forall a \in [0,1] \Leftrightarrow$$ $$\int_0^a G^+(H_x, H_a)f_M(x)dx + \int_a^1 G^-(H_x, H_a)f_M(x)dx=0, \forall a \in [0,1] \Leftrightarrow$$ I am stuck at this point, I am not sure how to solve this parametric differential equation. If an analytic solution is not possible, can I apply numerical methods? Finally, a small side note because I got it wrong initially: In the equation above notice how $G^+$ goes with the integral that spans $[0,a]$, and how $G^-$ goes with the integral that spans $[a,1]$. This is because gain is seen from the viewpoint of the first argument of $G()$. So if the first argument is $H_x$, with $x \le a$, while the second argument is $H_a$, then we should use the $G^+$ branch/case.","I run into this problem in my attempt to answer another question on math.stackexchange (In the original problem the strategies can be made discrete and finite, but I am interested in the infinite version here) We have a 2-player game with an infinite set of strategies. The strategies are essentially the same basic strategy with a parameter that tweaks it. The parameter takes values between $0$ and $1$, hence infinite strategies are born. Let's name these strategies $H_a$ where $a$ is the parameter that can change. We also know the payoff function that provides the gain of one strategy over another. Let's denote the gain of strategy $H_b$ over strategy $H_a$ as $G(H_b, H_a)$. The payoff is symmetrical, so one's gain is the other one's loss. For our particular game the payoff function looks like this $$G(H_b, H_a) = \begin{cases} G^-(H_b, H_a), & \text{if $a \le b$} \\ G^+(H_b, H_a), & \text{if $a>b$} \end{cases}$$ with $$G^-(H_b, H_a) = b\left[\frac{a(b-a)}{b} - (1-a)\frac{a-b+2}{2}\right] + (1-b)\left[a + (1-a)\frac{2(b-a)}{1-a}\right]$$ $$G^+(H_b, H_a) = b\left[\frac{a(b-a)}{a} - (1-a)\right] + (1-b)\left[a\cdot\frac{b-a+2}{2} + (1-a)\frac{2(b-a)}{1-b}\right]$$ Simplifying them so that their quadratic form becomes apparent: $$G^-(H_b, H_a) = (a^2b - ab^2 + 5ab - 2a^2 - 3b^2 +2b -2a)/2$$ $$G^+(H_b, H_a) = (a^2b - ab^2 - 5ab + 3a^2 + 2b^2 +2b -2a)/2$$ From either of these formulas we can also calculate that $G(H_a, H_a)=0$ There is no dominant strategy. For any strategy $H_a$ you can find strategies that are better, and strategies that are worse. To get a better a sense have a look at the following two graphs. They show the gain of two strategies $H_{0.5}$ and  $H_{0.8}$ against any possible strategy. The $x$ axis on both graphs is just the parameter $a$ going from $0$ to $1$ while the $y$ axis is $G(H_{0.5}, H_a)$ for the graph on the left and $G(H_{0.8}, H_a)$ for the graph on the right. You can notice for example that $H_{0.5}$ is better than $H_{0.2}$, $H_{0.8}$ is better than $H_{0.5}$, but $H_{0.2}$ is better than $H_{0.8}$. How do we find the mixed strategy Nash equilibrium for our problem? In other words, what is the p.d.f. $f_M$ of random variable M that we use to define our Nash Equilibrium mixed strategy $H_M$? When I was first faced with this problem I did not have any formal knowledge of mixed strategies or Nash equilibria, but I did try intuitively the simplest mixed strategy: $M$ being uniformly distributed in the interval $[0,1]$. I realised that this does not lead to an equilibrium and then I stopped because I was not sure whether this meant that no equilibria exist, or that I needed to find a different distribution for $M$. I read a bit on game theory and mixed strategy equilibria before posting this question and now I believe that a mixed strategy should exist . The infinite number of strategies might complicate things but this is how I am seeing the problem: Since this is a symmetrical zero-sum game I should find the distribution of $M$ such that whatever strategy $H_a$ my opponent chooses, my expected gain will be zero. More formally, we need to find the p.d.f. for $M$ such that $\underset{M}{\mathbb E}[G(H_M, H_a)]=0, \forall a \in [0,1]$. (where $\underset{X}{\mathbb E}[F(X)]$ is the expected value of function $F()$ over random variable $X$ ) Is this right? If so, then I believe we can proceed like so: $$\underset{M}{\mathbb E}[G(H_M, H_a)]=0, \forall a \in [0,1] \Leftrightarrow$$ if $f_M$ is the probability density function of random variable M we get  $$\int_0^1 G(H_x, H_a)f_M(x)dx =0, \forall a \in [0,1] \Leftrightarrow$$ $$\int_0^a G^+(H_x, H_a)f_M(x)dx + \int_a^1 G^-(H_x, H_a)f_M(x)dx=0, \forall a \in [0,1] \Leftrightarrow$$ I am stuck at this point, I am not sure how to solve this parametric differential equation. If an analytic solution is not possible, can I apply numerical methods? Finally, a small side note because I got it wrong initially: In the equation above notice how $G^+$ goes with the integral that spans $[0,a]$, and how $G^-$ goes with the integral that spans $[a,1]$. This is because gain is seen from the viewpoint of the first argument of $G()$. So if the first argument is $H_x$, with $x \le a$, while the second argument is $H_a$, then we should use the $G^+$ branch/case.",,"['ordinary-differential-equations', 'game-theory', 'nash-equilibrium']"
54,Why a family of curve $y = c · f(x) + g(x)$ can only be a solution to first-order linear equations?,Why a family of curve  can only be a solution to first-order linear equations?,y = c · f(x) + g(x),"This question is from George F. Simmons' Differential equations : theory, technique, and practice. I just began this book and I am learning how to solve some basic first-order linear equation. This problem states that the solution of a first-order linear equation must be a family of curves of the form $$ y = c · f(x) + g(x) $$ Then it requires me to show that the differential equation of any such family is linear and first-order. It seems to me a very strange question. I can attempt to construct an equation, by writing $$ \frac{y-g(x)}{f(x)}=c $$ then differentiate both sides, producing a differential equation without arbitrary constant c: $$ f(x)\frac{dy}{dx}-y=f(x)g'(x)-g(x)f'(x) $$ But I do not feel like it means that the family of curves cannot be the solution of some higher-order or non-linear differential equations. Please help me with this.","This question is from George F. Simmons' Differential equations : theory, technique, and practice. I just began this book and I am learning how to solve some basic first-order linear equation. This problem states that the solution of a first-order linear equation must be a family of curves of the form $$ y = c · f(x) + g(x) $$ Then it requires me to show that the differential equation of any such family is linear and first-order. It seems to me a very strange question. I can attempt to construct an equation, by writing $$ \frac{y-g(x)}{f(x)}=c $$ then differentiate both sides, producing a differential equation without arbitrary constant c: $$ f(x)\frac{dy}{dx}-y=f(x)g'(x)-g(x)f'(x) $$ But I do not feel like it means that the family of curves cannot be the solution of some higher-order or non-linear differential equations. Please help me with this.",,['ordinary-differential-equations']
55,Separable Differential Equation $9x - 4y\sqrt{x^2 + 1} \frac{dy}{dx} = 0$,Separable Differential Equation,9x - 4y\sqrt{x^2 + 1} \frac{dy}{dx} = 0,"Solve the separable differential equation:   $$9x - 4y\sqrt{x^2 + 1} \frac{dy}{dx} = 0$$   Subject to the initial condition:  $y(0) = −3$. This particular situation looks like it can be solves by moving the $x$ to the other side of the equation: $$4y\sqrt{x^2 + 1} \frac{dy}{dx} = 9x$$ $$4y \frac{dy}{dx} = \frac{9x}{\sqrt{x^2 + 1}}$$ Then we can integrate with respect to $x$: $$4\int y\frac{dy}{dx} \, dx = \int \frac{9x}{\sqrt{x^2 + 1}} \, dx$$ which, by canceling $dx$ and integrating the left hand side by $y$ and the right hand side by $x$, we have that: $$4 \frac{y^2}{2} + C_1 = 9\sqrt{x^2 + 1} + C_2$$ $$2y^2 = 9\sqrt{x^2 + 1} + C$$ which, given the initial condition $y(0) = −3$: $$C = 2(-3)^2 - 9\sqrt{1} = 18 - 9 = 9$$ So finally: $$y = \sqrt{\frac{9\sqrt{x^2 + 1} + 9}{2}}$$ But sadly this is not the correct answer. Where am I going wrong with this process? Thanks so much for your help!","Solve the separable differential equation:   $$9x - 4y\sqrt{x^2 + 1} \frac{dy}{dx} = 0$$   Subject to the initial condition:  $y(0) = −3$. This particular situation looks like it can be solves by moving the $x$ to the other side of the equation: $$4y\sqrt{x^2 + 1} \frac{dy}{dx} = 9x$$ $$4y \frac{dy}{dx} = \frac{9x}{\sqrt{x^2 + 1}}$$ Then we can integrate with respect to $x$: $$4\int y\frac{dy}{dx} \, dx = \int \frac{9x}{\sqrt{x^2 + 1}} \, dx$$ which, by canceling $dx$ and integrating the left hand side by $y$ and the right hand side by $x$, we have that: $$4 \frac{y^2}{2} + C_1 = 9\sqrt{x^2 + 1} + C_2$$ $$2y^2 = 9\sqrt{x^2 + 1} + C$$ which, given the initial condition $y(0) = −3$: $$C = 2(-3)^2 - 9\sqrt{1} = 18 - 9 = 9$$ So finally: $$y = \sqrt{\frac{9\sqrt{x^2 + 1} + 9}{2}}$$ But sadly this is not the correct answer. Where am I going wrong with this process? Thanks so much for your help!",,['ordinary-differential-equations']
56,Show that a Lissajous curve has incommesurate frequencies iff it isdense in a rectangle,Show that a Lissajous curve has incommesurate frequencies iff it isdense in a rectangle,,"If we have a Lissajous curve given by the parametric equations $$x(t)=A\sin(\omega_x t)$$ $$y(t)=B\sin(\omega_y t + \delta),$$ how can we show that the curve is dense in the rectangle of sides $A,B$ if and only if $\omega_x$ and $\omega_y$ are incommensurate (i.e. their ratio is irrational)? What I tried: Suppose it is not dense, so there exists a neighborhood of radius $r$ around some $x_0$, $y_0$ through which the curve doesn't pass. Since the parametric functions are continuous in time, a neighborhood of size $r$ in distance translates to some neighborhood of size $\tau$ in time. The curve is at $x=x_0$ at the times $$t_x=\{\frac{1}{\omega_x}\left[\sin^{-1}(x_0/A)+2n\pi\right]|n\in \mathbb{N}\}$$ and at $y=y_0$ at the times $$t_x=\{\frac{1}{\omega_y}\left[\sin{-1}(y_0/B)-\delta+2m\pi\right]|m\in \mathbb N\}.$$ If any two points in in the sets $t_x, t_y$ are less than $\tau$ away, then the trajectory goes through the $r$-neighborhood around $x_0,y_0$, giving a contradiction. For this we need to find integers $m,n$ such that $$\frac{\sin^{-1}(x_0/A)}{2\pi\omega_x} - \frac{\sin^{-1}(y_0/B)-\delta}{2\pi\omega_y}+\frac{n}{\omega_x}-\frac{m}{\omega_y}<\tau$$ At this point it seems that I need something tat states that the difference $n/\omega_x - m/\omega_y$ can be arbitrarily close to any real if $\omega_x$ and $\omega_y$ are incommensurate, but I have not been able to prove anything like that.","If we have a Lissajous curve given by the parametric equations $$x(t)=A\sin(\omega_x t)$$ $$y(t)=B\sin(\omega_y t + \delta),$$ how can we show that the curve is dense in the rectangle of sides $A,B$ if and only if $\omega_x$ and $\omega_y$ are incommensurate (i.e. their ratio is irrational)? What I tried: Suppose it is not dense, so there exists a neighborhood of radius $r$ around some $x_0$, $y_0$ through which the curve doesn't pass. Since the parametric functions are continuous in time, a neighborhood of size $r$ in distance translates to some neighborhood of size $\tau$ in time. The curve is at $x=x_0$ at the times $$t_x=\{\frac{1}{\omega_x}\left[\sin^{-1}(x_0/A)+2n\pi\right]|n\in \mathbb{N}\}$$ and at $y=y_0$ at the times $$t_x=\{\frac{1}{\omega_y}\left[\sin{-1}(y_0/B)-\delta+2m\pi\right]|m\in \mathbb N\}.$$ If any two points in in the sets $t_x, t_y$ are less than $\tau$ away, then the trajectory goes through the $r$-neighborhood around $x_0,y_0$, giving a contradiction. For this we need to find integers $m,n$ such that $$\frac{\sin^{-1}(x_0/A)}{2\pi\omega_x} - \frac{\sin^{-1}(y_0/B)-\delta}{2\pi\omega_y}+\frac{n}{\omega_x}-\frac{m}{\omega_y}<\tau$$ At this point it seems that I need something tat states that the difference $n/\omega_x - m/\omega_y$ can be arbitrarily close to any real if $\omega_x$ and $\omega_y$ are incommensurate, but I have not been able to prove anything like that.",,"['real-analysis', 'complex-analysis', 'functional-analysis', 'analysis', 'ordinary-differential-equations']"
57,Do derivations over a commutative ring form a ring?,Do derivations over a commutative ring form a ring?,,"Given a commutative ring $R$ and a derivation $D : R \rightarrow R$, we might consider a ring built over ""differential operators"" built over this operator, with the elements of the ring being spans of the operators $D,D^2,D^3,\dots$ over the ring $R$. For example, if $R$ is the ring of smooth functions $\mathbb{R} \rightarrow \mathbb{R}$, an example of such an element of this ring would be $sin(x) \frac{d^2}{dx^2} + 5\frac{d}{dx}$. Multiplication in this ring is just composition of operators, and addition is just operator addition. Does this construction actually give a ring in general? If this is a ring, what are its properties? (Specifically, I'd like to know if $R$ is reduced, and $D$ is a derivation on $R$, is the ring of differential operators over $R$ also reduced? But that might be better as another question) What is such a ring typically called in the literature?","Given a commutative ring $R$ and a derivation $D : R \rightarrow R$, we might consider a ring built over ""differential operators"" built over this operator, with the elements of the ring being spans of the operators $D,D^2,D^3,\dots$ over the ring $R$. For example, if $R$ is the ring of smooth functions $\mathbb{R} \rightarrow \mathbb{R}$, an example of such an element of this ring would be $sin(x) \frac{d^2}{dx^2} + 5\frac{d}{dx}$. Multiplication in this ring is just composition of operators, and addition is just operator addition. Does this construction actually give a ring in general? If this is a ring, what are its properties? (Specifically, I'd like to know if $R$ is reduced, and $D$ is a derivation on $R$, is the ring of differential operators over $R$ also reduced? But that might be better as another question) What is such a ring typically called in the literature?",,"['abstract-algebra', 'ordinary-differential-equations', 'reference-request', 'ring-theory', 'differential-algebra']"
58,application of the Euler-Lagrange / Beltrami equation,application of the Euler-Lagrange / Beltrami equation,,Let $$S(\theta)=\int_0^{2\pi}\sqrt{\dot \theta^2(t)+\cos^2(\theta(t))}dt$$ a) How does the Euler-Lagrange equation for S look? b) Show that the solutions of the Euler-Lagrange equation are given by $\theta (t)=\arctan(a\sin(t)+b\cos(t))$. Use the substitution $u=\tan(\theta)$. What I did: a) I've got: $$-\frac{\sin\theta\cos\theta}{\sqrt{\dot\theta^2+\cos^2\theta}}-\frac{d}{dt}\frac{\dot\theta}{\sqrt{\dot\theta^2+\cos^2\theta}}=0$$ Is this correct? b) I've got for the Beltrami equation $\frac{\dot\theta^2}{\sqrt{\dot\theta^2+\cos^2\theta}}-\sqrt{\dot\theta^2+\cos^2\theta}=c$. So from this I get $-\cos^2\theta=c\sqrt{\dot\theta^2+\cos^2\theta}$. Now with $u=\tan\theta$ I get $1/c^2-1=u^2+\dot u^2$ with the chain rule. I think I did something wrong in the substitution. Is it ment $u(t)=\tan(\theta(t))$? Thats what I tried. Does someone know a link where I could read through some examples similar to this porblem? Maybe some with a boundary condition too?,Let $$S(\theta)=\int_0^{2\pi}\sqrt{\dot \theta^2(t)+\cos^2(\theta(t))}dt$$ a) How does the Euler-Lagrange equation for S look? b) Show that the solutions of the Euler-Lagrange equation are given by $\theta (t)=\arctan(a\sin(t)+b\cos(t))$. Use the substitution $u=\tan(\theta)$. What I did: a) I've got: $$-\frac{\sin\theta\cos\theta}{\sqrt{\dot\theta^2+\cos^2\theta}}-\frac{d}{dt}\frac{\dot\theta}{\sqrt{\dot\theta^2+\cos^2\theta}}=0$$ Is this correct? b) I've got for the Beltrami equation $\frac{\dot\theta^2}{\sqrt{\dot\theta^2+\cos^2\theta}}-\sqrt{\dot\theta^2+\cos^2\theta}=c$. So from this I get $-\cos^2\theta=c\sqrt{\dot\theta^2+\cos^2\theta}$. Now with $u=\tan\theta$ I get $1/c^2-1=u^2+\dot u^2$ with the chain rule. I think I did something wrong in the substitution. Is it ment $u(t)=\tan(\theta(t))$? Thats what I tried. Does someone know a link where I could read through some examples similar to this porblem? Maybe some with a boundary condition too?,,"['real-analysis', 'ordinary-differential-equations', 'reference-request', 'calculus-of-variations']"
59,A hyperbolic set which is a unique center manifold,A hyperbolic set which is a unique center manifold,,"Consider the first order ODE \begin{equation} \dot{x} = f(x), \end{equation} with $x \in \mathbb{R}^3$ and $f: \mathbb{R}^3 \rightarrow \mathbb{R}^3$ a smooth function. We assume that the ODE only has equilibria for $x=(0,0,x_3)$ for all $x_3 \in \mathbb{R}$. Furthermore, assume that the Jacobian of $f$ at $x=(0,0,x_3)$ has one positive, one negative and one zero eigenvalue. Does it then follow that the center manifold $W^{c}(0,0,x_3)$ is unique? It is not hard to see that $\{ (0,0,x_3) \; : \; x_3 \in \mathbb{R} \}$ is a center manifold. In the case that you have 2 positive or 2 negative eigenvalues it is easy to see that the center manifold is unique by making use that the unstable or stable manifold which foliates the whole space. This argument does not extend to the `saddle' case so I am not sure how to proceed. It also might be that it is not true so a counter example would also be highly appreciated.","Consider the first order ODE \begin{equation} \dot{x} = f(x), \end{equation} with $x \in \mathbb{R}^3$ and $f: \mathbb{R}^3 \rightarrow \mathbb{R}^3$ a smooth function. We assume that the ODE only has equilibria for $x=(0,0,x_3)$ for all $x_3 \in \mathbb{R}$. Furthermore, assume that the Jacobian of $f$ at $x=(0,0,x_3)$ has one positive, one negative and one zero eigenvalue. Does it then follow that the center manifold $W^{c}(0,0,x_3)$ is unique? It is not hard to see that $\{ (0,0,x_3) \; : \; x_3 \in \mathbb{R} \}$ is a center manifold. In the case that you have 2 positive or 2 negative eigenvalues it is easy to see that the center manifold is unique by making use that the unstable or stable manifold which foliates the whole space. This argument does not extend to the `saddle' case so I am not sure how to proceed. It also might be that it is not true so a counter example would also be highly appreciated.",,"['ordinary-differential-equations', 'dynamical-systems']"
60,'2nd order' Picard Iteration,'2nd order' Picard Iteration,,"I'm self-studying differential equations using MIT's publicly available materials.  One of the problem set exercises deals with what I'm calling a second order Picard Iteration.  To be explicit, we are given the initial value problem \begin{align} 	x'' = f(t,y), \qquad x(t_0) = x_0, \qquad x'(t_0) = x_1 \end{align} where $f$ is continuous and Lipschitz on a rectangle $|t -t_0| \leq T$ and $|x - x_0| \leq K$ and a sequence of functions given by \begin{align} x_n(t) & = \begin{cases} x_0 & n = 0\\ x_0 + x_1(t - t_0) + \int_{t_0}^t(t-s)f(x_{n-1}(s),x)ds & n \geq 1 \end{cases} \end{align} We are to show that $\lbrace x_n \rbrace$ converges to a solution to the IVP on the interval $|t - t_0| \leq \min \left(T, \frac{K}{B}\right)$, where $M$ is the maximum value on the rectangle in question and $B = |x_1| + \frac{MT}{2}$.  It seems to me that we are to proceed in a manner analogous to the proof of the local existence theorem; I am, however, getting stuck on the inductive step. Here's what I have.  In essence, what we want to do is show that $\lbrace x_n \rbrace$ converges uniformly and then use the fact that uniform convergence implies that we can move the limit from the outside to the inside of an integral sign.  So \begin{align} |x_1(t) - x_0(t)| & = \left|(t - t_0)x_1 + \int_{t_0}^t(t-s)f(x_0,t)ds\right|\\ & \leq |(t - t_0)x_1| + \int_{t_0}^t|t-s||f(x_0,x)ds\\ & \leq T|x_1| + M\int_{t_0}^t|t-s|ds\\ & \leq T|x_1| + \frac{MT^2}{2}\\ & \leq TB \end{align} and therefore \begin{align} |x_2(t) - x_1(t)| & \leq \int_{t_0}^t|t-s||f(x_1(s),t) - f(x_2(s),t)|ds\\ & \leq L\int_{t_0}^t|t-s||x_1(s) - x_0(s)|ds\\ & \leq TBL\int_{t_0}^t|t-s|ds\\ & \leq TBL\left(\frac{T^2}{2}\right)\\ \end{align} And here we reach the inductive step, where my troubles begin.  Assume we have \begin{align} |x_n(t) - x_{n-1}(t)| \leq TBL^{n-1}\left(\frac{T}{2}\right)^{2(n-1)} \end{align} Then we obtain \begin{align} |x_{n+1}(t) - x_{n-1}(t)| \leq TBL^n\left(\frac{T}{2}\right)^{2n} \end{align} We want the right side of this inequality to go to $0$, but of course it doesn't, unless $T < 2$.  Now, in the proof of the local existence theorem, the analogous denominator is $n!$, which suggests that instead of $2^{2n}$ we should have $(2n)!$, but I can't see how we'd obtain that, given that each iteration of $\int_{t_0}^t(t-s)$ is going to yield $\frac{(t - t_0)^2}{2}$, so that we're doubling the denominator each time. Any thoughts would be appreciated.","I'm self-studying differential equations using MIT's publicly available materials.  One of the problem set exercises deals with what I'm calling a second order Picard Iteration.  To be explicit, we are given the initial value problem \begin{align} 	x'' = f(t,y), \qquad x(t_0) = x_0, \qquad x'(t_0) = x_1 \end{align} where $f$ is continuous and Lipschitz on a rectangle $|t -t_0| \leq T$ and $|x - x_0| \leq K$ and a sequence of functions given by \begin{align} x_n(t) & = \begin{cases} x_0 & n = 0\\ x_0 + x_1(t - t_0) + \int_{t_0}^t(t-s)f(x_{n-1}(s),x)ds & n \geq 1 \end{cases} \end{align} We are to show that $\lbrace x_n \rbrace$ converges to a solution to the IVP on the interval $|t - t_0| \leq \min \left(T, \frac{K}{B}\right)$, where $M$ is the maximum value on the rectangle in question and $B = |x_1| + \frac{MT}{2}$.  It seems to me that we are to proceed in a manner analogous to the proof of the local existence theorem; I am, however, getting stuck on the inductive step. Here's what I have.  In essence, what we want to do is show that $\lbrace x_n \rbrace$ converges uniformly and then use the fact that uniform convergence implies that we can move the limit from the outside to the inside of an integral sign.  So \begin{align} |x_1(t) - x_0(t)| & = \left|(t - t_0)x_1 + \int_{t_0}^t(t-s)f(x_0,t)ds\right|\\ & \leq |(t - t_0)x_1| + \int_{t_0}^t|t-s||f(x_0,x)ds\\ & \leq T|x_1| + M\int_{t_0}^t|t-s|ds\\ & \leq T|x_1| + \frac{MT^2}{2}\\ & \leq TB \end{align} and therefore \begin{align} |x_2(t) - x_1(t)| & \leq \int_{t_0}^t|t-s||f(x_1(s),t) - f(x_2(s),t)|ds\\ & \leq L\int_{t_0}^t|t-s||x_1(s) - x_0(s)|ds\\ & \leq TBL\int_{t_0}^t|t-s|ds\\ & \leq TBL\left(\frac{T^2}{2}\right)\\ \end{align} And here we reach the inductive step, where my troubles begin.  Assume we have \begin{align} |x_n(t) - x_{n-1}(t)| \leq TBL^{n-1}\left(\frac{T}{2}\right)^{2(n-1)} \end{align} Then we obtain \begin{align} |x_{n+1}(t) - x_{n-1}(t)| \leq TBL^n\left(\frac{T}{2}\right)^{2n} \end{align} We want the right side of this inequality to go to $0$, but of course it doesn't, unless $T < 2$.  Now, in the proof of the local existence theorem, the analogous denominator is $n!$, which suggests that instead of $2^{2n}$ we should have $(2n)!$, but I can't see how we'd obtain that, given that each iteration of $\int_{t_0}^t(t-s)$ is going to yield $\frac{(t - t_0)^2}{2}$, so that we're doubling the denominator each time. Any thoughts would be appreciated.",,['ordinary-differential-equations']
61,Polar coordinates for vector field to find sticking flow,Polar coordinates for vector field to find sticking flow,,"I am currently working on an impacting system which is basically just a spring damper and a circular enclosure. Because of the rotational symmetry of the problem I need the vector field in polar coordinates to derive the sticking flow along the enclosure. It should be an easy problem, but contrary to my expectation there are no scores of textbooks which contain the solution and I am not used to mechanical problems. The system in Cartesian coordinates is the following: $$ \left( \begin{array}{c}  \dot{x} \\  \ddot{x} \\  \dot{y} \\  \ddot{y} \\ \end{array} \right)= \left( \begin{array}{cccc}  0 & 1 & 0 & 0 \\  -\nu^2 & -\gamma & 0 & 0 \\  0 & 0 & 0 & 1 \\  0 & 0 & -\nu^2 & -\gamma \\ \end{array} \right) \left( \begin{array}{c}  x \\  \dot{x} \\  y \\  \dot{y} \\ \end{array} \right) $$ I have replaced $x$, $y$ and their derivatives by $x=r \cos(\theta)$, $y=r \sin(\theta)$ and their derivatives as was also done in this question . Solving for $\ddot{r}$ and $\ddot{\theta}$ I obtained $$ \ddot{r}= -r \nu ^2-\gamma  \dot{r}+r \dot{\theta} ^2 $$ and  $$ \ddot{\theta}=-\frac{\left(r \gamma +2 \dot{r}\right) \dot{\theta}}{r} $$ which I verified with mathematica. But when I test using the following Matlab-code nu2=1.96; gam=0.28; ODE1=@(t,z) [0 1 0 0;-nu2 -gam 0 0;0 0 0 1;0 0 -nu2 -gam]*z; ODE2=@(t,z) [z(2);              -nu2*z(1)-gam*z(2)+z(1)*z(4)^2;               z(4);              -z(4)*(gam*z(1)+2*z(2))/z(1)]; z0=[1;0;1;0.5]; tspan=[0, 1]; z0p=[1.4142 0.3536 0.7854 0.3536]; [T1,Z1]=ode45(ODE1,tspan,z0); [T2,Z2]=ode45(ODE2,tspan,z0p); Z2c=[Z2(:,1).*cos(Z2(:,3)),Z2(:,1).*sin(Z2(:,3))]; hold on plot(Z1(:,1),Z1(:,3)); plot(Z2c(:,1),Z2c(:,2)); hold off legend('Cartesian','Polar') I find that they are not equivalent. Now I am wondering whether there is a conceptual error as I am fairly confident in my derivation. So I am starting to think that my approach must be wrong and I would happy if someone could tell me where my error lies. Update Found the problem, answer below.","I am currently working on an impacting system which is basically just a spring damper and a circular enclosure. Because of the rotational symmetry of the problem I need the vector field in polar coordinates to derive the sticking flow along the enclosure. It should be an easy problem, but contrary to my expectation there are no scores of textbooks which contain the solution and I am not used to mechanical problems. The system in Cartesian coordinates is the following: $$ \left( \begin{array}{c}  \dot{x} \\  \ddot{x} \\  \dot{y} \\  \ddot{y} \\ \end{array} \right)= \left( \begin{array}{cccc}  0 & 1 & 0 & 0 \\  -\nu^2 & -\gamma & 0 & 0 \\  0 & 0 & 0 & 1 \\  0 & 0 & -\nu^2 & -\gamma \\ \end{array} \right) \left( \begin{array}{c}  x \\  \dot{x} \\  y \\  \dot{y} \\ \end{array} \right) $$ I have replaced $x$, $y$ and their derivatives by $x=r \cos(\theta)$, $y=r \sin(\theta)$ and their derivatives as was also done in this question . Solving for $\ddot{r}$ and $\ddot{\theta}$ I obtained $$ \ddot{r}= -r \nu ^2-\gamma  \dot{r}+r \dot{\theta} ^2 $$ and  $$ \ddot{\theta}=-\frac{\left(r \gamma +2 \dot{r}\right) \dot{\theta}}{r} $$ which I verified with mathematica. But when I test using the following Matlab-code nu2=1.96; gam=0.28; ODE1=@(t,z) [0 1 0 0;-nu2 -gam 0 0;0 0 0 1;0 0 -nu2 -gam]*z; ODE2=@(t,z) [z(2);              -nu2*z(1)-gam*z(2)+z(1)*z(4)^2;               z(4);              -z(4)*(gam*z(1)+2*z(2))/z(1)]; z0=[1;0;1;0.5]; tspan=[0, 1]; z0p=[1.4142 0.3536 0.7854 0.3536]; [T1,Z1]=ode45(ODE1,tspan,z0); [T2,Z2]=ode45(ODE2,tspan,z0p); Z2c=[Z2(:,1).*cos(Z2(:,3)),Z2(:,1).*sin(Z2(:,3))]; hold on plot(Z1(:,1),Z1(:,3)); plot(Z2c(:,1),Z2c(:,2)); hold off legend('Cartesian','Polar') I find that they are not equivalent. Now I am wondering whether there is a conceptual error as I am fairly confident in my derivation. So I am starting to think that my approach must be wrong and I would happy if someone could tell me where my error lies. Update Found the problem, answer below.",,"['ordinary-differential-equations', 'dynamical-systems', 'polar-coordinates']"
62,Picard's Iteration -- direct proof of convergence of sequence of integrals to $\tan x$.,Picard's Iteration -- direct proof of convergence of sequence of integrals to .,\tan x,"The extra credit question on one of the midterm exams for the online version of MIT's Honor's Differential Equations asks us to directly solve the IVP $y' = 1 + y^2$, $y(0) = 0$ and then show that the limit of the functions $y_{n+1} = \int y_n(s)ds$ as $n \to \infty$ is the same.  The first part is trivial:  $y = \tan x$.  However, the second part seems tougher.  It's not hard to calculate the first few iterations: \begin{align} y_0 &= 0\\ y_1 &= t\\ y_2 &= t + \frac{t^3}{3}\\ y_3 &= t + \frac{t^3}{3} + \frac{2t^5}{15} + \frac{t^7}{63}\\ \vdots \end{align} And we can write out (the beginning of) a Maclaurin expansion for $\tan t$: \begin{align} \tan t & = t + \frac{t^3}{3} + \frac{2t^5}{15} + \frac{17t^7}{315} + \cdots \end{align} This is, of course, suggestive.  But I see no obvious way to demonstrate that the $y_n$ will approach the latter polynomial as $n \to \infty$. Edit: Overnight reflection brought me to a possible solution, but I'm not particularly confident in it.  I would appreciate confirmation that what I do below makes sense--in particular, I'd appreciate confirmation that my reasoning is not circular. We note that the Maclaurin expansion for $\tan x$ is $x + R_1$, where $R$ is a polynomial of degree $> 1$.  Since $y = \tan x$ is a solution of our original ODE, we have     \begin{align} 		\tan ' x & = 1 + \tan^2x\Rightarrow\\ 		\tan x & = \int (1 + \tan^2x)dx\\ 		& = \int [1 + (x + R_1)^2]dx\\ 		& = \int (1 + x^2 + 2xR_1 + R_1^2)dx\\ 		& = x + \frac{x^3}{3} + \int (2xR_1 + R_1^2)dx 	\end{align}         Note that $2xR_1 + R_1^2$ is a polynomial of degree of at least $3$, so the last term of the sum above is a polynomial of degree at least $4$.  Call this polynomial $R_2$.  So we have     \begin{align} 		\tan x = x + \frac{x^3}{3} + R_2 	\end{align}         We now repeat the process:     \begin{align} 		\tan x & = \int \left[1 + \left(x + \frac{x^3}{3} + R_2\right)^2\right]dx\\ 		& = \int \left[1 + x^2 + \frac{2x^4}{3} + \frac{x^6}{9} + R_2p(x)\right]dx 	\end{align}          where $p(x)$ is some polynomial in $x$.  Then we have         \begin{align} 		\tan x & = x + \frac{x^3}{3} + \frac{2x^5}{15} + \frac{x^7}{63} + \int R_2p(x)dx 		\end{align}         $\int R_3p(x)$ is a polynomial of degree of degree at least $7$; call it $R_4$.  Continuing to iterate in this way, we see that we are, in fact, repeating precisely the calculations we would make for our original Picard iteration.  So $\tan x = \lim_{n \to \infty} y_n$.","The extra credit question on one of the midterm exams for the online version of MIT's Honor's Differential Equations asks us to directly solve the IVP $y' = 1 + y^2$, $y(0) = 0$ and then show that the limit of the functions $y_{n+1} = \int y_n(s)ds$ as $n \to \infty$ is the same.  The first part is trivial:  $y = \tan x$.  However, the second part seems tougher.  It's not hard to calculate the first few iterations: \begin{align} y_0 &= 0\\ y_1 &= t\\ y_2 &= t + \frac{t^3}{3}\\ y_3 &= t + \frac{t^3}{3} + \frac{2t^5}{15} + \frac{t^7}{63}\\ \vdots \end{align} And we can write out (the beginning of) a Maclaurin expansion for $\tan t$: \begin{align} \tan t & = t + \frac{t^3}{3} + \frac{2t^5}{15} + \frac{17t^7}{315} + \cdots \end{align} This is, of course, suggestive.  But I see no obvious way to demonstrate that the $y_n$ will approach the latter polynomial as $n \to \infty$. Edit: Overnight reflection brought me to a possible solution, but I'm not particularly confident in it.  I would appreciate confirmation that what I do below makes sense--in particular, I'd appreciate confirmation that my reasoning is not circular. We note that the Maclaurin expansion for $\tan x$ is $x + R_1$, where $R$ is a polynomial of degree $> 1$.  Since $y = \tan x$ is a solution of our original ODE, we have     \begin{align} 		\tan ' x & = 1 + \tan^2x\Rightarrow\\ 		\tan x & = \int (1 + \tan^2x)dx\\ 		& = \int [1 + (x + R_1)^2]dx\\ 		& = \int (1 + x^2 + 2xR_1 + R_1^2)dx\\ 		& = x + \frac{x^3}{3} + \int (2xR_1 + R_1^2)dx 	\end{align}         Note that $2xR_1 + R_1^2$ is a polynomial of degree of at least $3$, so the last term of the sum above is a polynomial of degree at least $4$.  Call this polynomial $R_2$.  So we have     \begin{align} 		\tan x = x + \frac{x^3}{3} + R_2 	\end{align}         We now repeat the process:     \begin{align} 		\tan x & = \int \left[1 + \left(x + \frac{x^3}{3} + R_2\right)^2\right]dx\\ 		& = \int \left[1 + x^2 + \frac{2x^4}{3} + \frac{x^6}{9} + R_2p(x)\right]dx 	\end{align}          where $p(x)$ is some polynomial in $x$.  Then we have         \begin{align} 		\tan x & = x + \frac{x^3}{3} + \frac{2x^5}{15} + \frac{x^7}{63} + \int R_2p(x)dx 		\end{align}         $\int R_3p(x)$ is a polynomial of degree of degree at least $7$; call it $R_4$.  Continuing to iterate in this way, we see that we are, in fact, repeating precisely the calculations we would make for our original Picard iteration.  So $\tan x = \lim_{n \to \infty} y_n$.",,['ordinary-differential-equations']
63,Radius of convergence from recurrence with variable coefficients,Radius of convergence from recurrence with variable coefficients,,"I am solving via power series the ivp $$y'-2xy=0,\quad y(1)=2.$$ The ""solution"" is $$y(x)=2\left(1+2(x-1)+3(x-1)^2+\frac{10}{3}(x-1)^3+\frac{19}{6}(x-1)^4+\frac{26}{10}(x-1)^5+\cdots\right)$$ with coefficients generated by the recurrence: $$(n+1)a_{n+1}-2a_{n-1}-2a_n=0,\quad n>0$$ or, $$a_{n+1}=\frac{2(a_n+a_{n-1})}{n+1}.$$ Due to the variable coefficient I'm not sure how to determine the radius of convergence from the recurrence.  My naive attempt was to use $$\frac{a_{n+1}}{a_n}=\frac{2\left(1+\frac{a_{n-1}}{a_n}\right)}{n+1}$$ and assume that $$\lim_{n\to\infty}\left|\frac{a_{n+1}}{a_n}\right|=R,\quad 0<R<\infty$$ to obtain a contradiction indicating that the radius is either 0 or infinite, but that is not very helpful.  The generating function methods don't seem particularly helpful, but I'm quite likely wrong about that. I know that the solution (valid for all $x$) is $$y(x)=2\mathrm{e}^{x^2-1}.$$ Question: How would one obtain the radius of convergence of this series from the recurrence.  As an aside are there any general methods that might indicate that the radius is greater than zero?","I am solving via power series the ivp $$y'-2xy=0,\quad y(1)=2.$$ The ""solution"" is $$y(x)=2\left(1+2(x-1)+3(x-1)^2+\frac{10}{3}(x-1)^3+\frac{19}{6}(x-1)^4+\frac{26}{10}(x-1)^5+\cdots\right)$$ with coefficients generated by the recurrence: $$(n+1)a_{n+1}-2a_{n-1}-2a_n=0,\quad n>0$$ or, $$a_{n+1}=\frac{2(a_n+a_{n-1})}{n+1}.$$ Due to the variable coefficient I'm not sure how to determine the radius of convergence from the recurrence.  My naive attempt was to use $$\frac{a_{n+1}}{a_n}=\frac{2\left(1+\frac{a_{n-1}}{a_n}\right)}{n+1}$$ and assume that $$\lim_{n\to\infty}\left|\frac{a_{n+1}}{a_n}\right|=R,\quad 0<R<\infty$$ to obtain a contradiction indicating that the radius is either 0 or infinite, but that is not very helpful.  The generating function methods don't seem particularly helpful, but I'm quite likely wrong about that. I know that the solution (valid for all $x$) is $$y(x)=2\mathrm{e}^{x^2-1}.$$ Question: How would one obtain the radius of convergence of this series from the recurrence.  As an aside are there any general methods that might indicate that the radius is greater than zero?",,"['ordinary-differential-equations', 'recurrence-relations', 'power-series']"
64,(Differential Galois Theory) Where is the proof that the three-body-problem is unsolvable?,(Differential Galois Theory) Where is the proof that the three-body-problem is unsolvable?,,"I'm looking for a proof, which shows that ""the 3-body-problem"" in physics  is mathematically unsolvable. Does anyone know some URLs that contain a proof in mathematical detail? You know, in Astrodynamics,  this problem is famous as an extension of Kepler problem  or two-body-problem. It says that we can solve equations of 2 bodies, such as the motions of earth and moon. But when it comes to 3 objects,  we can NOT solve equations about these three. Analytically, we can't get the solution. Why is that impossible? Related keywords are here: (1) In 3-body-problem, there's a lack of ""first-integral"" (conservatives). (2) This matter is related to system of ""integrable systems"". (3) This matter is also related to ""differential Galois Theory"". because motions are described by differential equations. (When we want to prove that some equations are unsolvable in algebra, Galois Theory is a useful tool. And differential Galois Theory is higher concept.) (4) In some special conditions, 3-body-problem is solvable. For example, they have solutions such that 3 objects are on one line, or 3 objects draws a shape like ""8"". But it's just in a special conditions. Generally, 3-body-problem is unsolvable. I am talking about the general case. I can not find any specific proof about this problem,  though it's very important well-known result of science... Where on the www can I see the mathematical proof? Thanks in advance.","I'm looking for a proof, which shows that ""the 3-body-problem"" in physics  is mathematically unsolvable. Does anyone know some URLs that contain a proof in mathematical detail? You know, in Astrodynamics,  this problem is famous as an extension of Kepler problem  or two-body-problem. It says that we can solve equations of 2 bodies, such as the motions of earth and moon. But when it comes to 3 objects,  we can NOT solve equations about these three. Analytically, we can't get the solution. Why is that impossible? Related keywords are here: (1) In 3-body-problem, there's a lack of ""first-integral"" (conservatives). (2) This matter is related to system of ""integrable systems"". (3) This matter is also related to ""differential Galois Theory"". because motions are described by differential equations. (When we want to prove that some equations are unsolvable in algebra, Galois Theory is a useful tool. And differential Galois Theory is higher concept.) (4) In some special conditions, 3-body-problem is solvable. For example, they have solutions such that 3 objects are on one line, or 3 objects draws a shape like ""8"". But it's just in a special conditions. Generally, 3-body-problem is unsolvable. I am talking about the general case. I can not find any specific proof about this problem,  though it's very important well-known result of science... Where on the www can I see the mathematical proof? Thanks in advance.",,"['ordinary-differential-equations', 'galois-theory', 'physics']"
65,Example of a Symbol: Connection.,Example of a Symbol: Connection.,,"I'm trying to get more intuition for the symbol of a differential operator. In particular, I've tried the example of a connection. What is the most efficient way to compute the symbol for, say, a linear connection $\nabla$ on a vector bundle $E\to X$, given locally by $$\nabla s^{\alpha} =\sum_\beta(\partial_\beta-A_\beta) s^\alpha?$$","I'm trying to get more intuition for the symbol of a differential operator. In particular, I've tried the example of a connection. What is the most efficient way to compute the symbol for, say, a linear connection $\nabla$ on a vector bundle $E\to X$, given locally by $$\nabla s^{\alpha} =\sum_\beta(\partial_\beta-A_\beta) s^\alpha?$$",,"['ordinary-differential-equations', 'differential-geometry']"
66,How to Solve the following Cauchy Problem?,How to Solve the following Cauchy Problem?,,"I want to find $x,y: \mathbb{R} \rightarrow \mathbb{R}$ such that: $x'(t)= -3x(t)+4y(t)$ $y'(t) = -x(t) + y(t)$ with the initial conditions $x(0) = y(0) = 1$ $\begin{bmatrix}          x' \\          y'       \end{bmatrix} =  \begin{bmatrix}          -3 & 4  \\          -1 & 1       \end{bmatrix} \times  \begin{bmatrix}          x \\          y       \end{bmatrix}  $ Let the $A=   \begin{bmatrix}          -3 & 4  \\          -1 & 1       \end{bmatrix}$ $det(A-\lambda I) = (1+\lambda)^2 = 0 \implies \lambda_1 = \lambda_2 = -1$ The eigenvectors are $V_{\lambda_1} = V_{\lambda_2} =  \begin{bmatrix}          1  \\          1       \end{bmatrix}$ From there I am stuck and I don't know what to do","I want to find $x,y: \mathbb{R} \rightarrow \mathbb{R}$ such that: $x'(t)= -3x(t)+4y(t)$ $y'(t) = -x(t) + y(t)$ with the initial conditions $x(0) = y(0) = 1$ $\begin{bmatrix}          x' \\          y'       \end{bmatrix} =  \begin{bmatrix}          -3 & 4  \\          -1 & 1       \end{bmatrix} \times  \begin{bmatrix}          x \\          y       \end{bmatrix}  $ Let the $A=   \begin{bmatrix}          -3 & 4  \\          -1 & 1       \end{bmatrix}$ $det(A-\lambda I) = (1+\lambda)^2 = 0 \implies \lambda_1 = \lambda_2 = -1$ The eigenvectors are $V_{\lambda_1} = V_{\lambda_2} =  \begin{bmatrix}          1  \\          1       \end{bmatrix}$ From there I am stuck and I don't know what to do",,"['real-analysis', 'linear-algebra', 'ordinary-differential-equations']"
67,Dissipation term in wave equation,Dissipation term in wave equation,,"If we're given a string with mass density $\rho$ in units $\frac{M}{L^3}$ with constant cross-section $A$, tension $T$ in units $\frac{F}{L^2}$, and whose length is $L$; and then we assume that the vertical displacement is small, the slope of the string during displacement is also small (which probably means that we can approximate tension to be constant); then I think it would be correct to say that the governing equation in this case is $$u_{tt}=c^2u_{xx}$$ where $c^2 = \frac{T}{\rho}$. (Please correct me if I'm wrong). Now a dissipation term must be added, which should be directly proportional to the mass and velocity of the string. Would it be correct to specify the dissipation term as follows: $-b\underbrace{\rho A L}_\text{mass} (u_t)^2\frac{1}{u}$ (where $b$ is some dimensionless proportionality constant)? If this is correct, then the equation would become $$u_{tt}=c^2u_{xx}-bm(u_t)^2\frac{1}{u}$$ where $m$ is mass.","If we're given a string with mass density $\rho$ in units $\frac{M}{L^3}$ with constant cross-section $A$, tension $T$ in units $\frac{F}{L^2}$, and whose length is $L$; and then we assume that the vertical displacement is small, the slope of the string during displacement is also small (which probably means that we can approximate tension to be constant); then I think it would be correct to say that the governing equation in this case is $$u_{tt}=c^2u_{xx}$$ where $c^2 = \frac{T}{\rho}$. (Please correct me if I'm wrong). Now a dissipation term must be added, which should be directly proportional to the mass and velocity of the string. Would it be correct to specify the dissipation term as follows: $-b\underbrace{\rho A L}_\text{mass} (u_t)^2\frac{1}{u}$ (where $b$ is some dimensionless proportionality constant)? If this is correct, then the equation would become $$u_{tt}=c^2u_{xx}-bm(u_t)^2\frac{1}{u}$$ where $m$ is mass.",,"['ordinary-differential-equations', 'partial-differential-equations', 'physics', 'mathematical-physics']"
68,How to find a general solution for $\frac{dy}{dx}$ + $e^{2x−3y}$ = −$e^x$,How to find a general solution for  +  = −,\frac{dy}{dx} e^{2x−3y} e^x,$\frac{dy}{dx}$ + $e^{2x−3y}$ = −$e^x$ Do I use integrating factors?,$\frac{dy}{dx}$ + $e^{2x−3y}$ = −$e^x$ Do I use integrating factors?,,"['calculus', 'ordinary-differential-equations']"
69,Applied Mathematics Book on Integro-Differential Equations,Applied Mathematics Book on Integro-Differential Equations,,"I'm interested in teaching a course on integro-differential equations and their applications. I was wondering if anyone could suggest a decent book on the subject. I'm currently looking at ""Nonlocal Diffusion Problems"" by Andreu-Vaillo, Mazón, Rossi, and Toledo-Melero. Is there a better book or would this be sufficient for a course? Any advice is greatly appreciated.","I'm interested in teaching a course on integro-differential equations and their applications. I was wondering if anyone could suggest a decent book on the subject. I'm currently looking at ""Nonlocal Diffusion Problems"" by Andreu-Vaillo, Mazón, Rossi, and Toledo-Melero. Is there a better book or would this be sufficient for a course? Any advice is greatly appreciated.",,"['ordinary-differential-equations', 'integral-equations', 'integro-differential-equations']"
70,Hartman-Grobman Theorem - Necessary?,Hartman-Grobman Theorem - Necessary?,,"The Hartman-Grobman theorem states, in layman's terms, that a nonlinear system and the corresponding liniarized system behave similarly around a hyperbolic equilibrium point (in terms of vector fields that dictate the evolution of states). But is this theorem necessary for this to be true? Can a linearized system with a single eigenvalue on the imaginary axis not behave like its nonlinear counterpart? I would request any specific references to the given answers so that I can read it and verify by myself!","The Hartman-Grobman theorem states, in layman's terms, that a nonlinear system and the corresponding liniarized system behave similarly around a hyperbolic equilibrium point (in terms of vector fields that dictate the evolution of states). But is this theorem necessary for this to be true? Can a linearized system with a single eigenvalue on the imaginary axis not behave like its nonlinear counterpart? I would request any specific references to the given answers so that I can read it and verify by myself!",,"['ordinary-differential-equations', 'differential-topology', 'dynamical-systems', 'control-theory', 'nonlinear-system']"
71,WKB problem with 4 turning points?,WKB problem with 4 turning points?,,"I was recently given a problem that asked to find the solvability conditions for $$\epsilon^2y''=(W(x)-E)y;\quad y\rightarrow0\text{ as }|x|\rightarrow0$$ where $W$ was some piecewise linear, $``W""$-shaped potential function (for simplicity, let the middle peak of the W be $(0,0))$. To summarize the setup, since each region is linear, an appropriate variable substitution $z_j$ as a linear function of $x$ for each linear region of $W$ recovers the Airy ODE in each region: $$y''=z_jy$$ with solutions $$y(x)=A_j\cdot \text{Ai}(z_j(x))+B_j\cdot\text{Bi}(z_j(x)),\quad j=1,2,3,4$$ in each of the four intervals where $W(x)$ is linear(say we label them from left to right, where $I_4=(-\infty,a]$, $I_4=(a,0]$, $I_4=(0,b]$, and $I_1=(b,\infty)$. We require continuity of both $y$, and $y'$, so we end up with a matrix for the undetermined coefficients: \begin{bmatrix} \text{Ai}(z_4(b)) & -\text{Ai}(z_3(a)) & -\text{Bi}(z_3(b)) & 0 & 0 & 0\\ z_4'(b)\cdot\text{Ai}'(z_4(b)) & -z_3'(b)\cdot\text{Ai}'(z_3(b)) & -z_3'(b)\cdot\text{Bi}'(z_3(b)) & 0 & 0 & 0\\ 0 & \text{Ai}(z_3(0)) & \text{Bi}(z_3(0)) & -\text{Ai}(z_2(0)) & -\text{Bi}(z_2(0)) & 0\\ 0 & z_3'(0)\cdot\text{Ai}'(z_3(0)) & z_3'(0)\cdot\text{Bi}'(z_3(0)) & -z_2'(0)\cdot\text{Ai}'(z_2(0)) & -z_2'(0)\cdot\text{Bi}'(z_2(0)) & 0\\ 0 & 0 & 0 & \text{Ai}(z_2(a)) & \text{Bi}(z_2(a)) & -\text{Ai}(z_1(a))\\ 0 & 0 & 0 & z_2'(a)\cdot\text{Ai}'(z_2(a)) & z_2'(a)\cdot\text{Bi}'(z_2(a)) & -z_1'(a)\cdot\text{Ai}'(z_1(a)) \end{bmatrix} Note that $B_1$ and $B_4$ must be $0$ to satisfy the $y$ bounded for large $|x|$. If the determinant of this matrix is $0$, then there are nontrivial solutions to the problem. Now, supposedly we can determine conditions on $E$ from the determinant, and we should be able to do it explicitly if we 1.) assume the case of a symmetric $W$ potential, and 2.) then use the first term asymptotic expansions of $\text{Ai}(z)$, $\text{Bi}(z)$, but I couldn't get it. Specifically for the this symmetric case with asymptotic approximations, does anyone know how to find the determinant?","I was recently given a problem that asked to find the solvability conditions for $$\epsilon^2y''=(W(x)-E)y;\quad y\rightarrow0\text{ as }|x|\rightarrow0$$ where $W$ was some piecewise linear, $``W""$-shaped potential function (for simplicity, let the middle peak of the W be $(0,0))$. To summarize the setup, since each region is linear, an appropriate variable substitution $z_j$ as a linear function of $x$ for each linear region of $W$ recovers the Airy ODE in each region: $$y''=z_jy$$ with solutions $$y(x)=A_j\cdot \text{Ai}(z_j(x))+B_j\cdot\text{Bi}(z_j(x)),\quad j=1,2,3,4$$ in each of the four intervals where $W(x)$ is linear(say we label them from left to right, where $I_4=(-\infty,a]$, $I_4=(a,0]$, $I_4=(0,b]$, and $I_1=(b,\infty)$. We require continuity of both $y$, and $y'$, so we end up with a matrix for the undetermined coefficients: \begin{bmatrix} \text{Ai}(z_4(b)) & -\text{Ai}(z_3(a)) & -\text{Bi}(z_3(b)) & 0 & 0 & 0\\ z_4'(b)\cdot\text{Ai}'(z_4(b)) & -z_3'(b)\cdot\text{Ai}'(z_3(b)) & -z_3'(b)\cdot\text{Bi}'(z_3(b)) & 0 & 0 & 0\\ 0 & \text{Ai}(z_3(0)) & \text{Bi}(z_3(0)) & -\text{Ai}(z_2(0)) & -\text{Bi}(z_2(0)) & 0\\ 0 & z_3'(0)\cdot\text{Ai}'(z_3(0)) & z_3'(0)\cdot\text{Bi}'(z_3(0)) & -z_2'(0)\cdot\text{Ai}'(z_2(0)) & -z_2'(0)\cdot\text{Bi}'(z_2(0)) & 0\\ 0 & 0 & 0 & \text{Ai}(z_2(a)) & \text{Bi}(z_2(a)) & -\text{Ai}(z_1(a))\\ 0 & 0 & 0 & z_2'(a)\cdot\text{Ai}'(z_2(a)) & z_2'(a)\cdot\text{Bi}'(z_2(a)) & -z_1'(a)\cdot\text{Ai}'(z_1(a)) \end{bmatrix} Note that $B_1$ and $B_4$ must be $0$ to satisfy the $y$ bounded for large $|x|$. If the determinant of this matrix is $0$, then there are nontrivial solutions to the problem. Now, supposedly we can determine conditions on $E$ from the determinant, and we should be able to do it explicitly if we 1.) assume the case of a symmetric $W$ potential, and 2.) then use the first term asymptotic expansions of $\text{Ai}(z)$, $\text{Bi}(z)$, but I couldn't get it. Specifically for the this symmetric case with asymptotic approximations, does anyone know how to find the determinant?",,"['linear-algebra', 'ordinary-differential-equations', 'asymptotics', 'perturbation-theory']"
72,Closed form of planetary radial motion as time function,Closed form of planetary radial motion as time function,,"What function/ functions express radial motion of planet by means of non-linear ODE $$ \ddot r - \frac{A}{r^3} +\frac{B}{r ^2}=0 $$ (The Kepler/Newton constants are: $\,B= a^3 \omega^2\, ; A=B p \,; $ There is no need to remind.. this differential equation is the, at any rate among the.. oldest of differential equations. If solutions are elliptic first or second type or both, is the form simplest with Jacobi inverse functions? EDIT 1: What is time period in terms of $A,B ?$ EDIT 2: Also is the Kepler's Equation useful in this closed form derivation? Kepler's Equation","What function/ functions express radial motion of planet by means of non-linear ODE $$ \ddot r - \frac{A}{r^3} +\frac{B}{r ^2}=0 $$ (The Kepler/Newton constants are: $\,B= a^3 \omega^2\, ; A=B p \,; $ There is no need to remind.. this differential equation is the, at any rate among the.. oldest of differential equations. If solutions are elliptic first or second type or both, is the form simplest with Jacobi inverse functions? EDIT 1: What is time period in terms of $A,B ?$ EDIT 2: Also is the Kepler's Equation useful in this closed form derivation? Kepler's Equation",,"['ordinary-differential-equations', 'classical-mechanics', 'celestial-mechanics']"
73,Is differentiating on both sides of an equation allowed? [duplicate],Is differentiating on both sides of an equation allowed? [duplicate],,"This question already has answers here : When is differentiating an equation valid? (2 answers) Closed 8 years ago . Let's say we have $x^2=25$ So we have two real roots ie $+5$ and $-5$. But if we were to differentiate on both sides with respect to $x$ we'll have the equation $2x=0$ which gives us the only root as $x=0$. So does differentiating on both sides of an equation alter it? If it does, then how do we conveniently do it in Integration by substitutions? If not then what exactly is going on here ?","This question already has answers here : When is differentiating an equation valid? (2 answers) Closed 8 years ago . Let's say we have $x^2=25$ So we have two real roots ie $+5$ and $-5$. But if we were to differentiate on both sides with respect to $x$ we'll have the equation $2x=0$ which gives us the only root as $x=0$. So does differentiating on both sides of an equation alter it? If it does, then how do we conveniently do it in Integration by substitutions? If not then what exactly is going on here ?",,['calculus']
74,adjoint method for computing derivatives,adjoint method for computing derivatives,,"I am curious if anyone has heard of this problem before: Suppose that $u(x,p)$ is a function of $x$ and $p$. These arguments need not be scalars. Let $u(x,p)$ satisfy some differential equation, say: $\frac{d^2}{dx^2}u(x,p) + f(x,p)\frac{d}{dx}u(x,p) + c(x,p)=0$. $f$ and $c$ are any functions just to make this equation sensible. Now if one is interested in $\frac{d}{dp}u(x,p)$, one way is to differentiate the above differential equation with respect to $p$, reorder the derivatives, and solve the differential equation for $\frac{d}{dp}u(x,p)$. Someone told me that there's another way to do this using the so-called adjoint method. I've been looking around but I am not sure if I have the correct references. Anyone seen this before?","I am curious if anyone has heard of this problem before: Suppose that $u(x,p)$ is a function of $x$ and $p$. These arguments need not be scalars. Let $u(x,p)$ satisfy some differential equation, say: $\frac{d^2}{dx^2}u(x,p) + f(x,p)\frac{d}{dx}u(x,p) + c(x,p)=0$. $f$ and $c$ are any functions just to make this equation sensible. Now if one is interested in $\frac{d}{dp}u(x,p)$, one way is to differentiate the above differential equation with respect to $p$, reorder the derivatives, and solve the differential equation for $\frac{d}{dp}u(x,p)$. Someone told me that there's another way to do this using the so-called adjoint method. I've been looking around but I am not sure if I have the correct references. Anyone seen this before?",,"['ordinary-differential-equations', 'derivatives', 'numerical-methods', 'adjoint-operators']"
75,differential equation y''=1/y^m,differential equation y''=1/y^m,,"I'm interested in knowing the asymptotics of solutions to the nonlinear ordinary differential equation \begin{equation*}     \begin{array}{ll}         y''=1/y^m\tag{*}\\         y(0)=a>0, \text{ and }y'(0)=0.     \end{array} \end{equation*} When $m=3$ $(*)$ has a closed form solution $y=\frac{1}{a}\sqrt{a^4+x^2}$, and $y$ is asymptotic to $\frac{1}{a}|x|$.  I have been trying to deduce for which $m$ $(*)$ has a solution which is asymptotically linear (as for $m=3$), and also the constants $c,b$ depending on $m,a$ such that $y$ is asymptotic to $c|x|+b$. So far I've begun by turning the second order ODE into a first order equation.  This follows from letting $z=\frac{dy}{dx}$ and noticing that $(*)$ is equivalent to $z \frac{dz}{dy}=y'' = 1/y^m$, and solving for $z=\frac{dy}{dx}$ to get \begin{equation} (y')^2=\frac{2}{1-m} y^{1-m}+C. \end{equation} Then we can evaluate $C$ using the initial condition and get for $x\geq 0$ and $y\geq a$ \begin{equation} y' = \left( \frac{2}{m-1} \left(\frac{1}{a^{m-1}} - \frac{1}{y^{m-1}}\right)\right)^{1/2}. \end{equation} $y$ is a convex function because $y''=1/y^m \geq 1/a^m$, so in particular it is unbounded, so letting $y\to \infty$ in the above expression gives  \begin{equation} \lim_{x\to \infty} y'(x) = \left(\frac{2}{(m-1)a^{m-1}}\right)^{1/2} :=c. \end{equation} This step required that $m>1$, so that's the first constraint on $m$.  If $y$ is asymptotic to anything, it will be of the form $y=c|x|+b$ for some constant $b$.  Since $0\leq y' \leq c$ it follows that $y$ is bounded above by $c|x|+a$, but it's not clear to me that $y$ is necessarily bounded below by a linear function $c|x|+b$ for some $b$.  I've tried get asymptotic bounds on the integral \begin{equation} \int_a^y \left( \frac{2}{m-1} \left(\frac{1}{a^{m-1}} - \frac{1}{t^{m-1}}\right)\right)^{1/2} \,dt \end{equation} as $y\to \infty$, but I haven't had much success.  If anyone has any suggestions for how to prove the solution $y$ is bounded below by $c|x|+b$ for some $b$ I'd be very grateful. Update: I haven't proved this yet, but I've made progress by showing that $y$ cannot be asymptotic to $cx-ln(x)$.  This would mean that there exists $\alpha>\beta>0$ such that as $x\to \infty$ \begin{equation} cx-\alpha\, ln(x) \leq y(x) \leq cx-\beta \,ln(x). \end{equation} Because $f$ is convex this implies a bound on the derivative. \begin{equation} c-\frac{\alpha}{x} \leq y'(x) \leq c-\frac{\beta}{x}. \end{equation} And then you can show from $(*)$ that $y'''<0$ so $y'$ is concave and this implies a bound on the second derivative. \begin{equation} \frac{\alpha}{x^2} \leq y''(x) \leq \frac{\beta}{x^2}. \end{equation} Then if $m>2$ we get the following inequality for large enough $x$: \begin{equation} \frac{1}{y^m} \leq \frac{1}{(cx-ln(x))^m} < \frac{\alpha}{x^2} \leq y''. \end{equation} This provides us with a contradiction so if $m>2$ then $y$ cannot be asymptotic to $cx-ln(x)$.  But this is only one example, and doesn't stop $y$ from being some other sublinear function.  One could probably prove a similar thing for $y$ asymptotic to $cx-ln(ln(x))$, but this method will never completely prove that $y$ is asymptotically linear.","I'm interested in knowing the asymptotics of solutions to the nonlinear ordinary differential equation \begin{equation*}     \begin{array}{ll}         y''=1/y^m\tag{*}\\         y(0)=a>0, \text{ and }y'(0)=0.     \end{array} \end{equation*} When $m=3$ $(*)$ has a closed form solution $y=\frac{1}{a}\sqrt{a^4+x^2}$, and $y$ is asymptotic to $\frac{1}{a}|x|$.  I have been trying to deduce for which $m$ $(*)$ has a solution which is asymptotically linear (as for $m=3$), and also the constants $c,b$ depending on $m,a$ such that $y$ is asymptotic to $c|x|+b$. So far I've begun by turning the second order ODE into a first order equation.  This follows from letting $z=\frac{dy}{dx}$ and noticing that $(*)$ is equivalent to $z \frac{dz}{dy}=y'' = 1/y^m$, and solving for $z=\frac{dy}{dx}$ to get \begin{equation} (y')^2=\frac{2}{1-m} y^{1-m}+C. \end{equation} Then we can evaluate $C$ using the initial condition and get for $x\geq 0$ and $y\geq a$ \begin{equation} y' = \left( \frac{2}{m-1} \left(\frac{1}{a^{m-1}} - \frac{1}{y^{m-1}}\right)\right)^{1/2}. \end{equation} $y$ is a convex function because $y''=1/y^m \geq 1/a^m$, so in particular it is unbounded, so letting $y\to \infty$ in the above expression gives  \begin{equation} \lim_{x\to \infty} y'(x) = \left(\frac{2}{(m-1)a^{m-1}}\right)^{1/2} :=c. \end{equation} This step required that $m>1$, so that's the first constraint on $m$.  If $y$ is asymptotic to anything, it will be of the form $y=c|x|+b$ for some constant $b$.  Since $0\leq y' \leq c$ it follows that $y$ is bounded above by $c|x|+a$, but it's not clear to me that $y$ is necessarily bounded below by a linear function $c|x|+b$ for some $b$.  I've tried get asymptotic bounds on the integral \begin{equation} \int_a^y \left( \frac{2}{m-1} \left(\frac{1}{a^{m-1}} - \frac{1}{t^{m-1}}\right)\right)^{1/2} \,dt \end{equation} as $y\to \infty$, but I haven't had much success.  If anyone has any suggestions for how to prove the solution $y$ is bounded below by $c|x|+b$ for some $b$ I'd be very grateful. Update: I haven't proved this yet, but I've made progress by showing that $y$ cannot be asymptotic to $cx-ln(x)$.  This would mean that there exists $\alpha>\beta>0$ such that as $x\to \infty$ \begin{equation} cx-\alpha\, ln(x) \leq y(x) \leq cx-\beta \,ln(x). \end{equation} Because $f$ is convex this implies a bound on the derivative. \begin{equation} c-\frac{\alpha}{x} \leq y'(x) \leq c-\frac{\beta}{x}. \end{equation} And then you can show from $(*)$ that $y'''<0$ so $y'$ is concave and this implies a bound on the second derivative. \begin{equation} \frac{\alpha}{x^2} \leq y''(x) \leq \frac{\beta}{x^2}. \end{equation} Then if $m>2$ we get the following inequality for large enough $x$: \begin{equation} \frac{1}{y^m} \leq \frac{1}{(cx-ln(x))^m} < \frac{\alpha}{x^2} \leq y''. \end{equation} This provides us with a contradiction so if $m>2$ then $y$ cannot be asymptotic to $cx-ln(x)$.  But this is only one example, and doesn't stop $y$ from being some other sublinear function.  One could probably prove a similar thing for $y$ asymptotic to $cx-ln(ln(x))$, but this method will never completely prove that $y$ is asymptotically linear.",,['ordinary-differential-equations']
76,Derivative of solution to IVP,Derivative of solution to IVP,,"I have two IVPs as followings: $$ y'=f(y, t)+g(y, t) \tag{1} $$ $$ y'=f(y, t)\tag{2} $$ with the same initial condition $y(t_0)=y_0$. Both $y$ and $t$ are positive and bounded. Furthermore, $f(y, t)$ and $g(y, t)$ satisfy that (i) both $f(y, t)$ and $g(y, t)$ are positive for all $t$ in the domain. (ii) both $f(y, t)$ and $g(y, t)$ are uniformly Lipschitz continuous w.r.t. y and t. (iii) $f_y(y, t)<0$, $f_t(y, t)>0$, $f_{yt}(y, t)<0$ and $g_t(y, t)>0$. The existence and uniqueness of solution to the IVPs are given by uniformly Lipschitz continuity. Denote $y_1(t)$ and $y_2(t)$ the solution to the first and second IVP respectively. It is not difficult to see that $y_1(t)\geq y_2(t)$. However since $f_y(y, t)<0$, it is difficult for me to see if  $$ y'_1(t)\geq y'_2(t) $$ for all $t$ or not. If the above assumptions do not suffice to prove that $y'_1(t)\geq y'_2(t)$ for all $t$. What kind of assumptions (hopefully mild) do I need in addition? Thanks!","I have two IVPs as followings: $$ y'=f(y, t)+g(y, t) \tag{1} $$ $$ y'=f(y, t)\tag{2} $$ with the same initial condition $y(t_0)=y_0$. Both $y$ and $t$ are positive and bounded. Furthermore, $f(y, t)$ and $g(y, t)$ satisfy that (i) both $f(y, t)$ and $g(y, t)$ are positive for all $t$ in the domain. (ii) both $f(y, t)$ and $g(y, t)$ are uniformly Lipschitz continuous w.r.t. y and t. (iii) $f_y(y, t)<0$, $f_t(y, t)>0$, $f_{yt}(y, t)<0$ and $g_t(y, t)>0$. The existence and uniqueness of solution to the IVPs are given by uniformly Lipschitz continuity. Denote $y_1(t)$ and $y_2(t)$ the solution to the first and second IVP respectively. It is not difficult to see that $y_1(t)\geq y_2(t)$. However since $f_y(y, t)<0$, it is difficult for me to see if  $$ y'_1(t)\geq y'_2(t) $$ for all $t$ or not. If the above assumptions do not suffice to prove that $y'_1(t)\geq y'_2(t)$ for all $t$. What kind of assumptions (hopefully mild) do I need in addition? Thanks!",,['ordinary-differential-equations']
77,System of Linear Differential Equations,System of Linear Differential Equations,,"I'm working on a mass-spring-dashpot system and I have a system of coupled differential equations I'm not sure how to solve. Below I've given a picture of the problem setup. There is a mass $m$ attached to two elements in parallel: a spring $k$ and a dashpot $d$. These are connected to another dashpot $c$. A force $F(t)$ is applied to the junction between $c$ and $k, d$. I've defined some distances such as $x(t)$ and $y(t)$. I'll write my first equation at the yellow point. There are four forces applied here (from $F(t), c, k, d$). Since there is no mass at the yellow dot, we can write: $$0 = F + k(x - y) + d(\dot{x} - \dot{y}) - c\dot{y}$$ I'll write my second equation at mass $m$. There are two forces applied here (from $k, d$). $$m\ddot{x} = -k(x - y) - d(\dot{x} - \dot{y})$$ I now have two equations for two unknowns, $x$ and $y$. What I would like is an equation independent of $y$: it may contain $x, F(t)$, any derivatives of $x$, and any constants. I'm not sure how to solve this system. Both of my equations contain derivatives of both variables, so I don't see a substitution I can make to simplify one to just having $x$. What I attempted to do (from a physics, not a math perspective) was to redraw the diagram using capacitors, inductors, and current sources - this way I could sum up the components using complex impedance. I didn't get very far with this method, though. Any ideas on what I can do? Both equations are linear, so I conjecture a nice solution should exist, but I can't seem to find it.","I'm working on a mass-spring-dashpot system and I have a system of coupled differential equations I'm not sure how to solve. Below I've given a picture of the problem setup. There is a mass $m$ attached to two elements in parallel: a spring $k$ and a dashpot $d$. These are connected to another dashpot $c$. A force $F(t)$ is applied to the junction between $c$ and $k, d$. I've defined some distances such as $x(t)$ and $y(t)$. I'll write my first equation at the yellow point. There are four forces applied here (from $F(t), c, k, d$). Since there is no mass at the yellow dot, we can write: $$0 = F + k(x - y) + d(\dot{x} - \dot{y}) - c\dot{y}$$ I'll write my second equation at mass $m$. There are two forces applied here (from $k, d$). $$m\ddot{x} = -k(x - y) - d(\dot{x} - \dot{y})$$ I now have two equations for two unknowns, $x$ and $y$. What I would like is an equation independent of $y$: it may contain $x, F(t)$, any derivatives of $x$, and any constants. I'm not sure how to solve this system. Both of my equations contain derivatives of both variables, so I don't see a substitution I can make to simplify one to just having $x$. What I attempted to do (from a physics, not a math perspective) was to redraw the diagram using capacitors, inductors, and current sources - this way I could sum up the components using complex impedance. I didn't get very far with this method, though. Any ideas on what I can do? Both equations are linear, so I conjecture a nice solution should exist, but I can't seem to find it.",,['ordinary-differential-equations']
78,Problem in integral on the phase plane,Problem in integral on the phase plane,,"I have this formula to calculate the period of a motion in the phase space (plan, in this case) along a phase curve. \begin{equation} T(E)=\int_{x_1}^{x_2}\frac{dx}{\sqrt{2(E-U(x))}} \end{equation} where $E$ is the total energy of the system, $U(x)$ is the potential energy and $x_1$ and $x_2$ are the points of my phase curve that intercept the $x$-axis of the phase plan (points of inversion). Now, I'm asked to calculate the period of the small oscillations in a neighborhood of the point $x_0$, which is the point of potential energy minimum. So, basically, I have to find: \begin{equation} T_0=\lim_{E\to E_0}{T(E)} \end{equation} where $E_0=U(x_0)$. My attempt: I have used Taylor to evaluate U(x) in a neighborhood of $x_0$. So, considering that E tends to $U(x_0)$ I got: \begin{equation} T(E)=\int_{x_1}^{x_2}\frac{dx}{\sqrt{2(U(x_0)-U(x_0)-U'(x_0)(x-x_0)-\frac{U''(x_0)(x-x_0)^2}{2}  }} \end{equation} and, since $U'(x_0)=0$ (because $x_0$ is a minimum for $U(x)$) \begin{equation} T(E)=\int_{x_{1}}^{x_{2}}\frac{dx}{\sqrt{-U''(x_{0})(x-x_{0})^2}  } \end{equation} So at this point I'm puzzled because $U''(x_{0})$ is positive for sure, and that leaves me with a negative number under a square root. I'm sure that somewhere i have applied some theorem where I shouldn't have. So I'm asking here and not on physics stackexchange because I feel like it's a mathematical issue. My book (Arnold) says that the solution is $2\pi$/$\sqrt{U''(x_{0})}$ but I got totally stuck here. Can you help me? Where's my mistake? P.S. Sorry for my lack in formalism, like omitting little-o notation, etc. but I think you got my point anyways :)","I have this formula to calculate the period of a motion in the phase space (plan, in this case) along a phase curve. \begin{equation} T(E)=\int_{x_1}^{x_2}\frac{dx}{\sqrt{2(E-U(x))}} \end{equation} where $E$ is the total energy of the system, $U(x)$ is the potential energy and $x_1$ and $x_2$ are the points of my phase curve that intercept the $x$-axis of the phase plan (points of inversion). Now, I'm asked to calculate the period of the small oscillations in a neighborhood of the point $x_0$, which is the point of potential energy minimum. So, basically, I have to find: \begin{equation} T_0=\lim_{E\to E_0}{T(E)} \end{equation} where $E_0=U(x_0)$. My attempt: I have used Taylor to evaluate U(x) in a neighborhood of $x_0$. So, considering that E tends to $U(x_0)$ I got: \begin{equation} T(E)=\int_{x_1}^{x_2}\frac{dx}{\sqrt{2(U(x_0)-U(x_0)-U'(x_0)(x-x_0)-\frac{U''(x_0)(x-x_0)^2}{2}  }} \end{equation} and, since $U'(x_0)=0$ (because $x_0$ is a minimum for $U(x)$) \begin{equation} T(E)=\int_{x_{1}}^{x_{2}}\frac{dx}{\sqrt{-U''(x_{0})(x-x_{0})^2}  } \end{equation} So at this point I'm puzzled because $U''(x_{0})$ is positive for sure, and that leaves me with a negative number under a square root. I'm sure that somewhere i have applied some theorem where I shouldn't have. So I'm asking here and not on physics stackexchange because I feel like it's a mathematical issue. My book (Arnold) says that the solution is $2\pi$/$\sqrt{U''(x_{0})}$ but I got totally stuck here. Can you help me? Where's my mistake? P.S. Sorry for my lack in formalism, like omitting little-o notation, etc. but I think you got my point anyways :)",,"['integration', 'ordinary-differential-equations']"
79,Solve $y(1+2xy)dx+x(1-2xy)dy=0$,Solve,y(1+2xy)dx+x(1-2xy)dy=0,"$y(1+2xy)dx+x(1-2xy)dy=0$ I tried to solve this for many methods, but is an inexact differential equation and I couldn't find an integrant factor, can you help me? Thanks.","$y(1+2xy)dx+x(1-2xy)dy=0$ I tried to solve this for many methods, but is an inexact differential equation and I couldn't find an integrant factor, can you help me? Thanks.",,['ordinary-differential-equations']
80,How to solve this DDE: $f'(x-1)-f''(x)=g(x)$.,How to solve this DDE: .,f'(x-1)-f''(x)=g(x),"How can I solve $$f'(x-1)-f''(x)=g(x),$$ or similar delay differential equations (DDEs) where there's some function on the r.h.s.? Is there a general method. I tried the following: Suppose $f(x)=Ce^{ax}$. Then $f'(x)=Cae^{ax}$ and $f''(x)=Ca^2e^{ax}$, and $$Cae^{ax-a}-Ca^2e^{ax}=g(x).$$ Then I'm stuck. Ordinarily we could continue if $g(x)\equiv 0$, but this is not necessarily the case. I'm wondering if I can use a Laplace transform, e.g. $$C\int_0^\infty ae^{-a}-Ca^2dx=\int_0^\infty g(x)e^{-ax}dx,$$ but not sure if this is the right way. I believe this approach is to do with the characteristic polynomial method, but I have also heard of the ""method of steps"" too. I think I'm on the wrong footing. Maybe I need to assume $f(x)=Ce^{ah(g(x))}$... Any help, references, hints, or guidance welcomed.","How can I solve $$f'(x-1)-f''(x)=g(x),$$ or similar delay differential equations (DDEs) where there's some function on the r.h.s.? Is there a general method. I tried the following: Suppose $f(x)=Ce^{ax}$. Then $f'(x)=Cae^{ax}$ and $f''(x)=Ca^2e^{ax}$, and $$Cae^{ax-a}-Ca^2e^{ax}=g(x).$$ Then I'm stuck. Ordinarily we could continue if $g(x)\equiv 0$, but this is not necessarily the case. I'm wondering if I can use a Laplace transform, e.g. $$C\int_0^\infty ae^{-a}-Ca^2dx=\int_0^\infty g(x)e^{-ax}dx,$$ but not sure if this is the right way. I believe this approach is to do with the characteristic polynomial method, but I have also heard of the ""method of steps"" too. I think I'm on the wrong footing. Maybe I need to assume $f(x)=Ce^{ah(g(x))}$... Any help, references, hints, or guidance welcomed.",,"['ordinary-differential-equations', 'delay-differential-equations']"
81,About Second-order Linear Homogenous ODE,About Second-order Linear Homogenous ODE,,"One way to solve second-order linear homogeneous ode with constant coefficients is to do the following things: $$a\left(\frac{\mathrm d^2}{\mathrm dx^2}\right)f+b\left(\frac{\mathrm d}{\mathrm dx}\right)f+cf=0$$ $$aD^2f+bDf+cf=0$$ $$(D-\lambda_1I)(D-\lambda_2I)f=0$$ $$\Longrightarrow(D-\lambda_1I)f=0\textrm{ or }(D-\lambda_2I)f=0$$ What's the theoretical basis of the last step? This is equivalent to prove that $$\ker(T+\lambda I)\oplus\ker(T-\lambda I)=\ker(T^2-\lambda^2I).$$ However that's not generally true, at least when $\lambda=0$, since it becomes $\ker T=\ker T^2$. Is it due to the particularity of $D$ or does this formula hold unless $\lambda=0$?","One way to solve second-order linear homogeneous ode with constant coefficients is to do the following things: $$a\left(\frac{\mathrm d^2}{\mathrm dx^2}\right)f+b\left(\frac{\mathrm d}{\mathrm dx}\right)f+cf=0$$ $$aD^2f+bDf+cf=0$$ $$(D-\lambda_1I)(D-\lambda_2I)f=0$$ $$\Longrightarrow(D-\lambda_1I)f=0\textrm{ or }(D-\lambda_2I)f=0$$ What's the theoretical basis of the last step? This is equivalent to prove that $$\ker(T+\lambda I)\oplus\ker(T-\lambda I)=\ker(T^2-\lambda^2I).$$ However that's not generally true, at least when $\lambda=0$, since it becomes $\ker T=\ker T^2$. Is it due to the particularity of $D$ or does this formula hold unless $\lambda=0$?",,"['linear-algebra', 'ordinary-differential-equations']"
82,Proving that $\lim\limits_{t\to\infty} e^{At}x_0 + \int\limits_0^\infty e^{A(t-s)}b(s)ds=\vec{0}$,Proving that,\lim\limits_{t\to\infty} e^{At}x_0 + \int\limits_0^\infty e^{A(t-s)}b(s)ds=\vec{0},"Consider $x'=Ax+b(t)$, a system of differential equations. Given that $A$ has negative real parts in all its eigenvalues, and that $\lim\limits_{t\to\infty} b(t) = \vec{0}$, I need to prove that $\lim\limits_{t\to\infty} e^{At}x_0 + \int\limits_0^t e^{A(t-s)}b(s)ds=\vec{0}$. Well, it's clear with $\lim\limits_{t\to\infty} e^{At}x_0 = \vec{0}$. But I'm struggling with $\lim\limits_{t\to\infty} \int\limits_0^t e^{A(t-s)}b(s)ds$. Can someone please give me a hint? I feel like I don't know some property which should be used here, such as a property of integrals.","Consider $x'=Ax+b(t)$, a system of differential equations. Given that $A$ has negative real parts in all its eigenvalues, and that $\lim\limits_{t\to\infty} b(t) = \vec{0}$, I need to prove that $\lim\limits_{t\to\infty} e^{At}x_0 + \int\limits_0^t e^{A(t-s)}b(s)ds=\vec{0}$. Well, it's clear with $\lim\limits_{t\to\infty} e^{At}x_0 = \vec{0}$. But I'm struggling with $\lim\limits_{t\to\infty} \int\limits_0^t e^{A(t-s)}b(s)ds$. Can someone please give me a hint? I feel like I don't know some property which should be used here, such as a property of integrals.",,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'dynamical-systems']"
83,"Resolving ODE-1 $(x^2 + y^2 +x)\,dx + xy\,dy=0$ am I wrong or my teacher is?",Resolving ODE-1  am I wrong or my teacher is?,"(x^2 + y^2 +x)\,dx + xy\,dy=0","This is how I've resolve this ODE-1 : $$(x^2 +y^2 +x) \, dx + xy \, dy=0$$ Check if the eq is exact: $${\partial M \over \partial y}={\partial \over \partial y}(x^2 +y^2 x)=2y$$ $${\partial N \over \partial x}={\partial \over \partial x}(xy)=y$$ $$A=2y \neq y=B$$ The equation is not exact. Trying to find the integrating factor : $$f=(A-B){1 \over N}$$ $$f=(2y-y){1 \over xy}={1 \over x}$$ Integrating factor: $$µ=e^{\int f(x) \, dx}=e^{\int {1 \over x} \, dx}=x$$ Now I can find the solution of the ODE by integrate the ODE with the integrating factor: $$\int x(x^2 +y^2 + x)\,dx + \int x(xy)\,dy=0$$ $$\int (x^3 +xy^2 +x^2)\,dx + \int x^2 y\,dy=0$$ $$\int x^3\,dx + \int xy^2 \,dx + \int x^2 \,dx + \int x^2 y\,dy=0$$ $${x^4 \over 4} + y^2 {x^2 \over 2} + {x^3 \over 3} + x^2 {y^2 \over 2} =0$$ $${x^4 \over 4} + {1 \over 2} x^2 y^2 + {x^3 \over 3} + {1 \over 2} x^2 y^2 =0$$ $$x^2 y^2 + {x^4 \over 4} + {x^3 \over 3} =0$$ The solution is : $12x^2 y^2 +3x^4 +4x^3 =C$ My teacher solution : By integrating we get : ${x^4 \over 4}+{x^3 \over 3}+{1 \over 2}x^2 y^2=C$ The solution is : $3x^4 +4x^3 +6x^2 y^2 = C$ That's all she wrote... Did I do something wrong or am I right? :s Thank you","This is how I've resolve this ODE-1 : $$(x^2 +y^2 +x) \, dx + xy \, dy=0$$ Check if the eq is exact: $${\partial M \over \partial y}={\partial \over \partial y}(x^2 +y^2 x)=2y$$ $${\partial N \over \partial x}={\partial \over \partial x}(xy)=y$$ $$A=2y \neq y=B$$ The equation is not exact. Trying to find the integrating factor : $$f=(A-B){1 \over N}$$ $$f=(2y-y){1 \over xy}={1 \over x}$$ Integrating factor: $$µ=e^{\int f(x) \, dx}=e^{\int {1 \over x} \, dx}=x$$ Now I can find the solution of the ODE by integrate the ODE with the integrating factor: $$\int x(x^2 +y^2 + x)\,dx + \int x(xy)\,dy=0$$ $$\int (x^3 +xy^2 +x^2)\,dx + \int x^2 y\,dy=0$$ $$\int x^3\,dx + \int xy^2 \,dx + \int x^2 \,dx + \int x^2 y\,dy=0$$ $${x^4 \over 4} + y^2 {x^2 \over 2} + {x^3 \over 3} + x^2 {y^2 \over 2} =0$$ $${x^4 \over 4} + {1 \over 2} x^2 y^2 + {x^3 \over 3} + {1 \over 2} x^2 y^2 =0$$ $$x^2 y^2 + {x^4 \over 4} + {x^3 \over 3} =0$$ The solution is : $12x^2 y^2 +3x^4 +4x^3 =C$ My teacher solution : By integrating we get : ${x^4 \over 4}+{x^3 \over 3}+{1 \over 2}x^2 y^2=C$ The solution is : $3x^4 +4x^3 +6x^2 y^2 = C$ That's all she wrote... Did I do something wrong or am I right? :s Thank you",,['ordinary-differential-equations']
84,External ballistics: Prove that the range is a concave function of the elevation,External ballistics: Prove that the range is a concave function of the elevation,,"Consider a projectile moving in a plane. One of many different models for this problem is the following ordinary differential equation \begin{align}  x''(t) &= -Ex'(t), \\ y''(t) &= -Ey'(t)- g, \end{align} together with the initial condition \begin{alignat}{2} x(0) &= 0, \qquad x'(0) &= v_0 \cos(\theta), \\ y(0) &= 0, \qquad y'(0) &= v_0 \sin(\theta). \end{alignat} Here $E \ge 0$ is a non-negative function which specifies the friction, $g > 0$ is the acceleration due to gravity, $v_0$ is the muzzle velocity of the gun and $\theta$ is the elevation of the gun. The case of $\theta=\frac{\pi}{2}$ corresponds to shooting straight into the air. Cases of special interest include  \begin{equation} E = f(v)v, \end{equation}  where $f$ reflects the geometry of the projectile or the more general  \begin{equation} E = E(v,y), \end{equation} which also allows the atmosphere to vary with the height of the projectile above the ground level $y=0$. The time of flight $\tau = \tau(\theta)$ from the muzzle of the gun to the point of impact is the unique positive solution of the equation \begin{equation} y(\theta,\tau(\theta)) = 0. \end{equation} The range $r$ of the gun is defined as \begin{equation} r(\theta) = x(\theta,\tau(\theta)), \end{equation} i.e. the $x$ coordinate of the shell at the point of impact. I have enough numerical evidence to suggest that the range is a concave function of the elevation, but no proof. I imagine that this is a result which is at about 90 years old, but I have not found a proof in the literature and I have not had any luck myself. The result is significant in the context of computing a firing solution for a target with coordinates $(d,0)$, i.e. solving the non-linear equation \begin{equation} r(\theta) = d \end{equation} with respect to elevation $\theta$. While a precomputed firing table should always be used to generate a good initial guess $\theta \approx \theta_0$, I have observed that the secant method as well as Newton's method for this equation are both globally convergent. These observations would be explained if the range function could be shown to be concave. I welcome ideas and suggestions about how to attack this problem, but since I suspect that this is an old result, I am primarily looking for a reference to a specific paper.","Consider a projectile moving in a plane. One of many different models for this problem is the following ordinary differential equation \begin{align}  x''(t) &= -Ex'(t), \\ y''(t) &= -Ey'(t)- g, \end{align} together with the initial condition \begin{alignat}{2} x(0) &= 0, \qquad x'(0) &= v_0 \cos(\theta), \\ y(0) &= 0, \qquad y'(0) &= v_0 \sin(\theta). \end{alignat} Here $E \ge 0$ is a non-negative function which specifies the friction, $g > 0$ is the acceleration due to gravity, $v_0$ is the muzzle velocity of the gun and $\theta$ is the elevation of the gun. The case of $\theta=\frac{\pi}{2}$ corresponds to shooting straight into the air. Cases of special interest include  \begin{equation} E = f(v)v, \end{equation}  where $f$ reflects the geometry of the projectile or the more general  \begin{equation} E = E(v,y), \end{equation} which also allows the atmosphere to vary with the height of the projectile above the ground level $y=0$. The time of flight $\tau = \tau(\theta)$ from the muzzle of the gun to the point of impact is the unique positive solution of the equation \begin{equation} y(\theta,\tau(\theta)) = 0. \end{equation} The range $r$ of the gun is defined as \begin{equation} r(\theta) = x(\theta,\tau(\theta)), \end{equation} i.e. the $x$ coordinate of the shell at the point of impact. I have enough numerical evidence to suggest that the range is a concave function of the elevation, but no proof. I imagine that this is a result which is at about 90 years old, but I have not found a proof in the literature and I have not had any luck myself. The result is significant in the context of computing a firing solution for a target with coordinates $(d,0)$, i.e. solving the non-linear equation \begin{equation} r(\theta) = d \end{equation} with respect to elevation $\theta$. While a precomputed firing table should always be used to generate a good initial guess $\theta \approx \theta_0$, I have observed that the secant method as well as Newton's method for this equation are both globally convergent. These observations would be explained if the range function could be shown to be concave. I welcome ideas and suggestions about how to attack this problem, but since I suspect that this is an old result, I am primarily looking for a reference to a specific paper.",,"['ordinary-differential-equations', 'dynamical-systems']"
85,How do we know that an integral is unsolvable?,How do we know that an integral is unsolvable?,,"I am currently learning intro differential equations. I am confused how one knows that an ODE will not be solvable. It seems that for the most part, the equations becomes ""unsolvable"" about halfway through the work, once you hit an impossible integral. For example, in my textbook, they are showing how using the integrating factor technique can go wrong, and end up with this: $e^{-t^3/3}y = \int{e^{-t^3/3}(t-1)dt}$ They then say that we are stuck here, because the integral on the right hand side of the equation is no expressible in terms of familiar functions (sin, cos, ln, etc...) My question is, how does one know that this won't be expressible in familiar functions? Do you just have to ""try it"", until you learn from experience of ""trying it"" what will and won't work (i.e. you simply can visually recognize from experience that you won't be able to evaluate the integral)? Or is there something inherent to the right hand side in this example that can make you certain there won't be a solution, without trying it? That is my main question, but I also am wondering; if this particular solution, which doesn't seem like it should be so overly complicated, cannot be expressed in terms of familiar functions, why haven't more ""standard"" functions been invented over the past 300 years? Are there actually more ""exotic"" functions that can describe the solution?","I am currently learning intro differential equations. I am confused how one knows that an ODE will not be solvable. It seems that for the most part, the equations becomes ""unsolvable"" about halfway through the work, once you hit an impossible integral. For example, in my textbook, they are showing how using the integrating factor technique can go wrong, and end up with this: $e^{-t^3/3}y = \int{e^{-t^3/3}(t-1)dt}$ They then say that we are stuck here, because the integral on the right hand side of the equation is no expressible in terms of familiar functions (sin, cos, ln, etc...) My question is, how does one know that this won't be expressible in familiar functions? Do you just have to ""try it"", until you learn from experience of ""trying it"" what will and won't work (i.e. you simply can visually recognize from experience that you won't be able to evaluate the integral)? Or is there something inherent to the right hand side in this example that can make you certain there won't be a solution, without trying it? That is my main question, but I also am wondering; if this particular solution, which doesn't seem like it should be so overly complicated, cannot be expressed in terms of familiar functions, why haven't more ""standard"" functions been invented over the past 300 years? Are there actually more ""exotic"" functions that can describe the solution?",,['calculus']
86,Global Lyapunov function. Definition,Global Lyapunov function. Definition,,"Our definition of a global Lyapunov function for a system $\dot{x}=f(x)$ is the following: Let a smooth scalar function $V\colon R^n\to R^1$ have the two following properties: $V(x)\to +\infty$ as $\lVert x\rVert\to+ +\infty$ and for all $x$ which are not equilibria of the system (i.e. $f(x)\neq 0$) there exists an $m$ such that $$ \frac{d^m}{dt^m}V(x) <0\text{ and }\frac{d^j}{dt^j}V(x)=0\text{ if }1\leq j <m. $$ Then we call $V$ a global Lyapunov function of the system. For example, we had that an Hamiltonian can be a global Lyapunov function if $H(x,y)\to +\infty$ as $\lVert y\rVert +\lVert x\rVert \to\infty$ and, except for the equilibria of the associated system, for each x there is some $m$  with the property above. My questions are: (1) Why $V(x,y)\to\infty$ as $\lVert x\rVert +\lVert y\rVert\to +\infty$ and not as $\lVert (x,y)\rVert\to +\infty$? Or is this just the same? (2) What if we have $V(x)\to -\infty$ as $\lVert x\rVert\to +\infty$? Is this still okay? For example consider the Hamiltonian $H(x,y)=\frac{y^2}{2}-\frac{x^2}{2}-\frac{x^3}{3}$. Then we have $H(x,y)\to -\infty$ as $\lVert x\rVert +\lVert y\rVert\to +\infty$. So the first condition is not fulfilled. But the second condition is fulfilled. Nonetheless we said $H$ is a global Lyapunov function. So does it play no role whether $H(x,y)\to +\infty$ or $H(x,y)\to -\infty$ as $\lVert x\rVert +\lVert y\rVert\to +\infty$?","Our definition of a global Lyapunov function for a system $\dot{x}=f(x)$ is the following: Let a smooth scalar function $V\colon R^n\to R^1$ have the two following properties: $V(x)\to +\infty$ as $\lVert x\rVert\to+ +\infty$ and for all $x$ which are not equilibria of the system (i.e. $f(x)\neq 0$) there exists an $m$ such that $$ \frac{d^m}{dt^m}V(x) <0\text{ and }\frac{d^j}{dt^j}V(x)=0\text{ if }1\leq j <m. $$ Then we call $V$ a global Lyapunov function of the system. For example, we had that an Hamiltonian can be a global Lyapunov function if $H(x,y)\to +\infty$ as $\lVert y\rVert +\lVert x\rVert \to\infty$ and, except for the equilibria of the associated system, for each x there is some $m$  with the property above. My questions are: (1) Why $V(x,y)\to\infty$ as $\lVert x\rVert +\lVert y\rVert\to +\infty$ and not as $\lVert (x,y)\rVert\to +\infty$? Or is this just the same? (2) What if we have $V(x)\to -\infty$ as $\lVert x\rVert\to +\infty$? Is this still okay? For example consider the Hamiltonian $H(x,y)=\frac{y^2}{2}-\frac{x^2}{2}-\frac{x^3}{3}$. Then we have $H(x,y)\to -\infty$ as $\lVert x\rVert +\lVert y\rVert\to +\infty$. So the first condition is not fulfilled. But the second condition is fulfilled. Nonetheless we said $H$ is a global Lyapunov function. So does it play no role whether $H(x,y)\to +\infty$ or $H(x,y)\to -\infty$ as $\lVert x\rVert +\lVert y\rVert\to +\infty$?",,['ordinary-differential-equations']
87,Periodical solutions of this system of differential equations,Periodical solutions of this system of differential equations,,"We have the system of differential equations: $$x'=(1+m)y+x(1-(x^2+y^2))(4-(x^2+y^2)),$$ $$y'=-x+y(1-(x^2+y^2))(4-(x^2+y^2)),$$ with $m>0$. How do I show that $(0,0)$ is the only (instable) critical point? How do I show that there is an $m_0$ such that for $0<m<m_0$ this system has TWO periodic (not-constant) solutions? What I have done so far: I wrote the sytem above in polar coordinates: $$r'=mr\sin\theta\cos\theta+r(1-r^2)(4-r^2),$$ $$\theta'=-1-m\sin^2\theta.$$ But I don't know how this can help me. For question 2. I must use Poincare-Bendixson, but I don't see how exactly.","We have the system of differential equations: $$x'=(1+m)y+x(1-(x^2+y^2))(4-(x^2+y^2)),$$ $$y'=-x+y(1-(x^2+y^2))(4-(x^2+y^2)),$$ with $m>0$. How do I show that $(0,0)$ is the only (instable) critical point? How do I show that there is an $m_0$ such that for $0<m<m_0$ this system has TWO periodic (not-constant) solutions? What I have done so far: I wrote the sytem above in polar coordinates: $$r'=mr\sin\theta\cos\theta+r(1-r^2)(4-r^2),$$ $$\theta'=-1-m\sin^2\theta.$$ But I don't know how this can help me. For question 2. I must use Poincare-Bendixson, but I don't see how exactly.",,"['ordinary-differential-equations', 'systems-of-equations']"
88,A differential equation that needs some algebra,A differential equation that needs some algebra,,"I have a differential equation: $$\dfrac{\mathrm{d}}{\mathrm{d}t}(x \cos\theta +y \sin\theta)+k(x \cos\theta +y \sin\theta)=\theta'(y \cos\theta-x \sin\theta)$$ where $x, y, \theta$ are all unknown functions of time, while $k$ is constant. So this equation is of the form $$y'+ky=f(t)$$ if I could express the right hand side of this equation in terms of $y$ and $y'$ then I will be able to solve it analytically for $y(t)$, I think it might be possible by a little algebraic manipulation, but I don't know how, I would appreciate any help. Edit: $\theta'$ can be considered constant of known value.","I have a differential equation: $$\dfrac{\mathrm{d}}{\mathrm{d}t}(x \cos\theta +y \sin\theta)+k(x \cos\theta +y \sin\theta)=\theta'(y \cos\theta-x \sin\theta)$$ where $x, y, \theta$ are all unknown functions of time, while $k$ is constant. So this equation is of the form $$y'+ky=f(t)$$ if I could express the right hand side of this equation in terms of $y$ and $y'$ then I will be able to solve it analytically for $y(t)$, I think it might be possible by a little algebraic manipulation, but I don't know how, I would appreciate any help. Edit: $\theta'$ can be considered constant of known value.",,"['ordinary-differential-equations', 'trigonometry']"
89,How does one in general analyze the convergence of the following series?,How does one in general analyze the convergence of the following series?,,"The following question is inspired in the following videos: https://www.youtube.com/playlist?list=PL43B1963F261E6E47 Say one has a general second order linear differential equation $y''+Qy=0$ for some functions $y,Q:U\subseteq\mathbb{R}\rightarrow\mathbb{R}$ with the initial condition $y(0)=a$ and $y'(0)=b$. Replace the equation by $y''+\epsilon Qy=0$ for $\epsilon\in\mathbb{R}$. Assume the solution is of the form $y(x)=\sum_{n=0}^{\infty}y_n(x)\epsilon^n$ for some functions $y_1,\dots,y_n:U\subseteq\mathbb{R}\rightarrow\mathbb{R}$. Then plugging in one finds: $$\sum_{n=0}^{\infty}y''_n\epsilon^n+\sum_{n=0}^{\infty}Qy_n\epsilon^{n+1}=y_0''+\sum_{n=1}^{\infty}\left(y_n''+Qy_{n-1}\right)\epsilon^n=0$$ Therefore, if we let $y_0(x)=a+bx$ and $y_1(0)=\dots=y_n(0)=y_1'(0)=\dots=y_n'(0)=0$ the initial conditions are fulfilled. Finally, one needs to solve $y_n''+Qy_{n-1}=0$ which has the obvious thanks to the initial condition of $y_n(x)=-\int_0^x\int_0^{s_{2n-1}}Q(s_{2n-2})y_{n-1}(s_{2n-2})ds_{2n-2}ds_{2n-1}$. The close form for $y_n$ is $$y_n(x)=(-1)^n\int_0^x\int_0^{s_{2n-1}}Q(s_{2n-2})\int_0^{s_{2n-2}}\int_0^{s_{2n-3}}Q(s_{2n-4})\dots\int_0^{s_{2}}\int_0^{s_1}Q(s_1)(a+s_0b)ds_0\dots ds_{2n-1} \\ =(-1)^n\int_0^x\int_0^{s_{2n-1}}\dots\int_0^{s_1}Q(s_{2n-2})Q(s_{2n-4})\dots Q(s_{0})(a+s_0b)ds_0\dots ds_{2n-1} \\ =(-1)^n\int\limits_{[0,s_1]\times\dots\times[0,s_{2n-1}]\times[0,x]}(a+bs_0)\prod_{\{m\in\mathbb{N}^{\leq2n-2}|\text{m is even}\}}Q(s_m)ds_0\dots ds_{2n-1}$$ and the general solution would be $$y(x)=a+bx+\sum_{n=1}^{\infty}(-1)^n\int\limits_{[0,s_1]\times\dots\times[0,s_{2n-1}]\times[0,x]}(a+bs_0)\prod_{\{m\in\mathbb{N}^{\leq2n-2}|\text{m is even}\}}Q(s_m)ds_0\dots ds_{2n-1}$$ Now, this solution does in fact work for $Q=0$ and $Q=1$ (in the latter the solution yields the taylor expansion of $\sin$ and $\cos$). Non the less, I am guessing that in most cases the solution diverges since otherwise it would be taught in any differential equations class. How does in general one examines the convergence of such a series? In which cases does the solution work? Is there any general rule to assure the convergence of the sequence?","The following question is inspired in the following videos: https://www.youtube.com/playlist?list=PL43B1963F261E6E47 Say one has a general second order linear differential equation $y''+Qy=0$ for some functions $y,Q:U\subseteq\mathbb{R}\rightarrow\mathbb{R}$ with the initial condition $y(0)=a$ and $y'(0)=b$. Replace the equation by $y''+\epsilon Qy=0$ for $\epsilon\in\mathbb{R}$. Assume the solution is of the form $y(x)=\sum_{n=0}^{\infty}y_n(x)\epsilon^n$ for some functions $y_1,\dots,y_n:U\subseteq\mathbb{R}\rightarrow\mathbb{R}$. Then plugging in one finds: $$\sum_{n=0}^{\infty}y''_n\epsilon^n+\sum_{n=0}^{\infty}Qy_n\epsilon^{n+1}=y_0''+\sum_{n=1}^{\infty}\left(y_n''+Qy_{n-1}\right)\epsilon^n=0$$ Therefore, if we let $y_0(x)=a+bx$ and $y_1(0)=\dots=y_n(0)=y_1'(0)=\dots=y_n'(0)=0$ the initial conditions are fulfilled. Finally, one needs to solve $y_n''+Qy_{n-1}=0$ which has the obvious thanks to the initial condition of $y_n(x)=-\int_0^x\int_0^{s_{2n-1}}Q(s_{2n-2})y_{n-1}(s_{2n-2})ds_{2n-2}ds_{2n-1}$. The close form for $y_n$ is $$y_n(x)=(-1)^n\int_0^x\int_0^{s_{2n-1}}Q(s_{2n-2})\int_0^{s_{2n-2}}\int_0^{s_{2n-3}}Q(s_{2n-4})\dots\int_0^{s_{2}}\int_0^{s_1}Q(s_1)(a+s_0b)ds_0\dots ds_{2n-1} \\ =(-1)^n\int_0^x\int_0^{s_{2n-1}}\dots\int_0^{s_1}Q(s_{2n-2})Q(s_{2n-4})\dots Q(s_{0})(a+s_0b)ds_0\dots ds_{2n-1} \\ =(-1)^n\int\limits_{[0,s_1]\times\dots\times[0,s_{2n-1}]\times[0,x]}(a+bs_0)\prod_{\{m\in\mathbb{N}^{\leq2n-2}|\text{m is even}\}}Q(s_m)ds_0\dots ds_{2n-1}$$ and the general solution would be $$y(x)=a+bx+\sum_{n=1}^{\infty}(-1)^n\int\limits_{[0,s_1]\times\dots\times[0,s_{2n-1}]\times[0,x]}(a+bs_0)\prod_{\{m\in\mathbb{N}^{\leq2n-2}|\text{m is even}\}}Q(s_m)ds_0\dots ds_{2n-1}$$ Now, this solution does in fact work for $Q=0$ and $Q=1$ (in the latter the solution yields the taylor expansion of $\sin$ and $\cos$). Non the less, I am guessing that in most cases the solution diverges since otherwise it would be taught in any differential equations class. How does in general one examines the convergence of such a series? In which cases does the solution work? Is there any general rule to assure the convergence of the sequence?",,"['sequences-and-series', 'ordinary-differential-equations', 'perturbation-theory']"
90,Asymptotic analysis of non-linear ODE,Asymptotic analysis of non-linear ODE,,"I'm looking for references on the topic of asymptotic analysis of non linear ODE's of the sort $$ x'' + x'x = 0 $$ This specific case has an analytic solution (with some $\tanh(\cdots)$ involved) and quickly converges to a real value; in my case, I'm dealing with ""similar"" equations, with some form of $x'x$ involved.","I'm looking for references on the topic of asymptotic analysis of non linear ODE's of the sort $$ x'' + x'x = 0 $$ This specific case has an analytic solution (with some $\tanh(\cdots)$ involved) and quickly converges to a real value; in my case, I'm dealing with ""similar"" equations, with some form of $x'x$ involved.",,"['ordinary-differential-equations', 'reference-request', 'asymptotics', 'dynamical-systems']"
91,Why is this a senseful notion of stability? Whyt is the intuition behind this idea of stability?,Why is this a senseful notion of stability? Whyt is the intuition behind this idea of stability?,,"Let $\xi=x-ct$. Moreover, let $U(\xi)$ be a travelling wave solution of a PDE. Suppose that $U(\xi,t)$ is a solution of a PDE. The travelling wave $U(\xi)$ is called stable (with respect to the PDE) if there is a neighborhood $N$ of it, such that for a solution $U(\xi,t)$ whose initial value $U(\xi,0)$ is within $N$,  there exists some $k\in\mathbb{R}$ such that $$ \lVert U(\xi,t)-U(\xi+k)\rVert\to 0~\text{ as }t\to+\infty. $$ In other words, a travelling wave $U(\xi)$ is stable, if each solution whose initial values are sufficiently close to it (in some norm), converges to a translate travelling wave $U(\cdot +k)$ as $t\to +\infty$. I am wondering a bit why this reflects a stable behaviour of $U(\xi)$ since the translate travelling wave $U(\cdot +k)$ can be very far away from $U(\xi)$, or not? Maybe this is not possible because of the wave structure, that is, after some time there is the next ""maximum"" (when the wave is starting again) and so maybe the definition tells us that, indeed, this means that the solution will be close to $U(\xi)$ as $t\to\infty$ if it is close to some translate of it. Is this the reason why it is a stable behaviour? In other words: If I think of a typical wave form, then, when a solution starts near a wave and then, as $t\to\infty$, it is near some translate of the wave, this implies that it is somewhere between to ""maxima"" of the wave, so it is quite near to the original wave again? But what if two maxima of the wave are far away from each other? Then being close to some translate of the wave does not need to mean being near the original wave? Maybe my intuition is wrong.","Let $\xi=x-ct$. Moreover, let $U(\xi)$ be a travelling wave solution of a PDE. Suppose that $U(\xi,t)$ is a solution of a PDE. The travelling wave $U(\xi)$ is called stable (with respect to the PDE) if there is a neighborhood $N$ of it, such that for a solution $U(\xi,t)$ whose initial value $U(\xi,0)$ is within $N$,  there exists some $k\in\mathbb{R}$ such that $$ \lVert U(\xi,t)-U(\xi+k)\rVert\to 0~\text{ as }t\to+\infty. $$ In other words, a travelling wave $U(\xi)$ is stable, if each solution whose initial values are sufficiently close to it (in some norm), converges to a translate travelling wave $U(\cdot +k)$ as $t\to +\infty$. I am wondering a bit why this reflects a stable behaviour of $U(\xi)$ since the translate travelling wave $U(\cdot +k)$ can be very far away from $U(\xi)$, or not? Maybe this is not possible because of the wave structure, that is, after some time there is the next ""maximum"" (when the wave is starting again) and so maybe the definition tells us that, indeed, this means that the solution will be close to $U(\xi)$ as $t\to\infty$ if it is close to some translate of it. Is this the reason why it is a stable behaviour? In other words: If I think of a typical wave form, then, when a solution starts near a wave and then, as $t\to\infty$, it is near some translate of the wave, this implies that it is somewhere between to ""maxima"" of the wave, so it is quite near to the original wave again? But what if two maxima of the wave are far away from each other? Then being close to some translate of the wave does not need to mean being near the original wave? Maybe my intuition is wrong.",,"['ordinary-differential-equations', 'partial-differential-equations', 'stability-theory']"
92,About the second-order linear ODEs with polynomial coefficients,About the second-order linear ODEs with polynomial coefficients,,"I know that for $p(x)y''+q(x)y'+r(x)y=0$ : When $p(x)$ is an incomplete-square quadratic polynomial, $q(x)$ is a quadratic polynomial, $r(x)$ is a linear polynomial, then the ODE relates to Heun's Confluent Equation . When $p(x)$ is a complete-square quadratic polynomial, $q(x)$ is a quadratic polynomial, $r(x)$ is a linear polynomial, then the ODE relates to Heun's Doubly-Confluent Equation . When $p(x)$ is a linear polynomial, $q(x)$ is a quadratic polynomial, $r(x)$ is a linear polynomial, then the ODE relates to Heun's Biconfluent Equation . When $p(x)$ is a constant, $q(x)$ is a quadratic polynomial, $r(x)$ is a linear polynomial, then the ODE relates to Heun's Triconfluent Equation . How about the types of the ODEs belonging when $p(x)$ is either an incomplete-square quadratic polynomial or a complete-square quadratic polynomial, $q(x)$ is a linear polynomial, or a constant, or even the $y'$ term vanishes instead?","I know that for $p(x)y''+q(x)y'+r(x)y=0$ : When $p(x)$ is an incomplete-square quadratic polynomial, $q(x)$ is a quadratic polynomial, $r(x)$ is a linear polynomial, then the ODE relates to Heun's Confluent Equation . When $p(x)$ is a complete-square quadratic polynomial, $q(x)$ is a quadratic polynomial, $r(x)$ is a linear polynomial, then the ODE relates to Heun's Doubly-Confluent Equation . When $p(x)$ is a linear polynomial, $q(x)$ is a quadratic polynomial, $r(x)$ is a linear polynomial, then the ODE relates to Heun's Biconfluent Equation . When $p(x)$ is a constant, $q(x)$ is a quadratic polynomial, $r(x)$ is a linear polynomial, then the ODE relates to Heun's Triconfluent Equation . How about the types of the ODEs belonging when $p(x)$ is either an incomplete-square quadratic polynomial or a complete-square quadratic polynomial, $q(x)$ is a linear polynomial, or a constant, or even the $y'$ term vanishes instead?",,['ordinary-differential-equations']
93,Limits of complicated sum.,Limits of complicated sum.,,"I'm faced with something like: $$ \lim_{r \to \infty} \left[ \sum_{n=0}^r \frac{\hbar^n (-1)^n}{n!} \sum_{k=0}^n \binom{n}{k} \frac{d^k}{dx^k} \left(\sum_{i=0}^{r-n} \frac{x^i}{i!} \right)  \frac{d^{n-k}}{dx^{n-k}} \right] \psi = 0 $$ A huge mess, I know.  I'm not sure exactly how to simplify, and get something in terms of exponentials. Now, obviously it'd be easiest to write this as something like: $$ \lim_{r \to \infty} \left[ \sum_{n=0}^r \frac{\hbar^n (-1)^n}{n!} \sum_{k=0}^{min\{n,r-n\}} \binom{n}{k} \sum_{i=k}^{r-n} \frac{x^{i-k}}{(i-k)!} \frac{d^{n-k}}{dx^{n-k}} \right] \psi = 0 $$ If you apply the limit then obviously $min\{n,r-n\}=n$ and so you would simply get: $$ [e^{-\hbar} e^x e^{-\hbar \partial_x}] \psi = 0 $$ But, I feel like there's something missing with this.  If we do this, we're neglecting the part inside the limit where $r-n<n$. For example, if you break up the expression as: $$ \lim_{r \to \infty} \left[ \sum_{n=0}^{r/2} \frac{\hbar^n (-1)^n}{n!} \sum_{k=0}^n \binom{n}{k} \sum_{i=k}^{r-n} \frac{x^{i-k}}{(i-k)!} \frac{d^{n-k}}{dx^{n-k}} + \sum_{n=r/2+1}^{r} \frac{\hbar^n (-1)^n}{n!} \sum_{k=0}^{r-n} \binom{n}{k} \sum_{i=k}^{r-n} \frac{x^{i-k}}{(i-k)!} \frac{d^{n-k}}{dx^{n-k}} \right] \psi = 0 $$ But, how do you evaluate the limit of this?","I'm faced with something like: $$ \lim_{r \to \infty} \left[ \sum_{n=0}^r \frac{\hbar^n (-1)^n}{n!} \sum_{k=0}^n \binom{n}{k} \frac{d^k}{dx^k} \left(\sum_{i=0}^{r-n} \frac{x^i}{i!} \right)  \frac{d^{n-k}}{dx^{n-k}} \right] \psi = 0 $$ A huge mess, I know.  I'm not sure exactly how to simplify, and get something in terms of exponentials. Now, obviously it'd be easiest to write this as something like: $$ \lim_{r \to \infty} \left[ \sum_{n=0}^r \frac{\hbar^n (-1)^n}{n!} \sum_{k=0}^{min\{n,r-n\}} \binom{n}{k} \sum_{i=k}^{r-n} \frac{x^{i-k}}{(i-k)!} \frac{d^{n-k}}{dx^{n-k}} \right] \psi = 0 $$ If you apply the limit then obviously $min\{n,r-n\}=n$ and so you would simply get: $$ [e^{-\hbar} e^x e^{-\hbar \partial_x}] \psi = 0 $$ But, I feel like there's something missing with this.  If we do this, we're neglecting the part inside the limit where $r-n<n$. For example, if you break up the expression as: $$ \lim_{r \to \infty} \left[ \sum_{n=0}^{r/2} \frac{\hbar^n (-1)^n}{n!} \sum_{k=0}^n \binom{n}{k} \sum_{i=k}^{r-n} \frac{x^{i-k}}{(i-k)!} \frac{d^{n-k}}{dx^{n-k}} + \sum_{n=r/2+1}^{r} \frac{\hbar^n (-1)^n}{n!} \sum_{k=0}^{r-n} \binom{n}{k} \sum_{i=k}^{r-n} \frac{x^{i-k}}{(i-k)!} \frac{d^{n-k}}{dx^{n-k}} \right] \psi = 0 $$ But, how do you evaluate the limit of this?",,"['ordinary-differential-equations', 'limits', 'summation']"
94,Different solutions to the Hermite equation,Different solutions to the Hermite equation,,"The Hermite differential equation is given as such $$ y'' - 2xy'+2\lambda y=0 $$ writing this in strum-liouville form you get $$-(\exp(-x^2)y')'= 2\exp(-x^2)\lambda y $$ However, in order for it to have real eigenvalues, according to strum-liouville theory, the differential operator, $$ Ly = \lambda y$$ must be self adjoint which implies that the solution must have compact support ( i.e. $ y\rightarrow 0 $ as $x \rightarrow \pm\infty $). However, we know that the solutions to the Hermite equation are the Hermite polynomials, which obviously are not compactly supported. So I'm confused. I was wondering if anyone could tell where I'm going wrong in my logic. Also I've seen solutions to the Hermite differential equation such as $ \exp(-x^2/2)H_n $ and $H_n$, where $H_n$ is the Hermite polynomial of order $n$. Basically, what's up with that?","The Hermite differential equation is given as such $$ y'' - 2xy'+2\lambda y=0 $$ writing this in strum-liouville form you get $$-(\exp(-x^2)y')'= 2\exp(-x^2)\lambda y $$ However, in order for it to have real eigenvalues, according to strum-liouville theory, the differential operator, $$ Ly = \lambda y$$ must be self adjoint which implies that the solution must have compact support ( i.e. $ y\rightarrow 0 $ as $x \rightarrow \pm\infty $). However, we know that the solutions to the Hermite equation are the Hermite polynomials, which obviously are not compactly supported. So I'm confused. I was wondering if anyone could tell where I'm going wrong in my logic. Also I've seen solutions to the Hermite differential equation such as $ \exp(-x^2/2)H_n $ and $H_n$, where $H_n$ is the Hermite polynomial of order $n$. Basically, what's up with that?",,"['ordinary-differential-equations', 'differential-operators']"
95,Solving a Nonlinear ODE,Solving a Nonlinear ODE,,"Does this problem have a unique solution in $[t_0,t_1]$? $$ \ddot x(t)=\alpha(1-t)\cos(x(t))$$  $$x(t_0)=0$$ $$ \dot x(t_1)=0$$ Thanks!","Does this problem have a unique solution in $[t_0,t_1]$? $$ \ddot x(t)=\alpha(1-t)\cos(x(t))$$  $$x(t_0)=0$$ $$ \dot x(t_1)=0$$ Thanks!",,"['ordinary-differential-equations', 'boundary-value-problem']"
96,How to linearize this second order differential equation?,How to linearize this second order differential equation?,,"I want to linearize this second order non linear differential equation arround P( $\pi/2 $ ,0) $\frac{d^2y}{dt}+\frac{dy}{dt}-9.8\sin(y)=5\sin(3t)$ I undestand how to linearize single variable functions like $f(x)=Cos(x)$ , but I'm clueless with the differential equation case.","I want to linearize this second order non linear differential equation arround P( ,0) I undestand how to linearize single variable functions like , but I'm clueless with the differential equation case.",\pi/2  \frac{d^2y}{dt}+\frac{dy}{dt}-9.8\sin(y)=5\sin(3t) f(x)=Cos(x),"['ordinary-differential-equations', 'numerical-methods']"
97,Are all solutions of $ f_n (x)^{2n} + f_n ' (x)^{2n} = 1$ periodic?,Are all solutions of  periodic?, f_n (x)^{2n} + f_n ' (x)^{2n} = 1,"Let $n$ be a strict positive integer and $x$ is a complex number. Define $f_n(x)$ as one of the solutions to $$ f_n (x)^{2n} + f_n ' (x)^{2n} = 1$$ Where the derivative is with respect to $x$. Why is every entire solution $f_n(x) $ periodic on the complex plane ? For instance the solutions for $f_1(x)$ are ${-1, 1, -\sin(x),\sin(x),-\cos(x),\cos(x)}$. All of them are periodic with period $2 \pi$. I assume implicit differentiation helps.","Let $n$ be a strict positive integer and $x$ is a complex number. Define $f_n(x)$ as one of the solutions to $$ f_n (x)^{2n} + f_n ' (x)^{2n} = 1$$ Where the derivative is with respect to $x$. Why is every entire solution $f_n(x) $ periodic on the complex plane ? For instance the solutions for $f_1(x)$ are ${-1, 1, -\sin(x),\sin(x),-\cos(x),\cos(x)}$. All of them are periodic with period $2 \pi$. I assume implicit differentiation helps.",,"['ordinary-differential-equations', 'implicit-differentiation', 'periodic-functions', 'square-numbers']"
98,$h$-principle for Legendrian immersions,-principle for Legendrian immersions,h,"It is ""folklore"" that continuous curves in contact 3 manifolds can be approximated by Legendrian curves and it seems that this follows from Gromov's $h$-principle for Legendrian immersions (in arbitrary dimensions). Since reading Gromov is a challenge (at least for me) I wonder if anybody has readable reference for this statement (preferably in any dimension) or at least for the case of approximating curves in 3 manifolds.","It is ""folklore"" that continuous curves in contact 3 manifolds can be approximated by Legendrian curves and it seems that this follows from Gromov's $h$-principle for Legendrian immersions (in arbitrary dimensions). Since reading Gromov is a challenge (at least for me) I wonder if anybody has readable reference for this statement (preferably in any dimension) or at least for the case of approximating curves in 3 manifolds.",,"['ordinary-differential-equations', 'differential-geometry', 'contact-topology', 'h-principle']"
99,How to integrate this ODE?,How to integrate this ODE?,,"How to solve the following ODE: $$ y'(x)=\sqrt{\dfrac{1-x+y}{2x+y}}. $$ I tried to transform it to a homogeneous differential equation, I found $$ s'(x)=\sqrt{\dfrac{s-1}{s+2}}-s$$ where $$ s(x) = \frac{2y(x)+3}{2x-1}.$$ Next? I don't know.","How to solve the following ODE: $$ y'(x)=\sqrt{\dfrac{1-x+y}{2x+y}}. $$ I tried to transform it to a homogeneous differential equation, I found $$ s'(x)=\sqrt{\dfrac{s-1}{s+2}}-s$$ where $$ s(x) = \frac{2y(x)+3}{2x-1}.$$ Next? I don't know.",,"['real-analysis', 'ordinary-differential-equations']"
