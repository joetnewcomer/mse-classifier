,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Rademacher complexity of regularized linear function class: does it depend on dimension or not?,Rademacher complexity of regularized linear function class: does it depend on dimension or not?,,"I am going through some lecture notes on Learning Theory here: http://ttic.uchicago.edu/~tewari/LT_SP2008.html trying to learn about Rademacher complexities.  I'm getting confused about the Rademacher complexity of linear function classes. Consider $F=\{f_w(x)=w^Tx|w\in R^d,\|w\|_2\le 1\}; \|x\|_2\le 1$. Denote the empirical Rademacher complexity of $F$ by $R_N(F)$. (N is the number of points from which this is estimated.) Using the Dudley theorem, as in Lecture 15, Sec. 2.1, we get that $R_N(F)\le \sqrt{\frac{d}{N}}$. However, using some elementary calculations as in Lecture 17, sec. 2.1, we get $R_N(F)\le \frac{1}{\sqrt{N}}$. My question is, should $R_N(F)$ depend on the dimension $d$ or not? Is $\sqrt{d}$ just an artefact of the proof technique in the first case? (First I thought that the application of Dudley theorem as above was not assuming that $\|x\|_2\le 1$, but it must do because otherwise we would not have $|w^Tx|\le 1$ and so the upper limit of the integral would not be 1.)","I am going through some lecture notes on Learning Theory here: http://ttic.uchicago.edu/~tewari/LT_SP2008.html trying to learn about Rademacher complexities.  I'm getting confused about the Rademacher complexity of linear function classes. Consider $F=\{f_w(x)=w^Tx|w\in R^d,\|w\|_2\le 1\}; \|x\|_2\le 1$. Denote the empirical Rademacher complexity of $F$ by $R_N(F)$. (N is the number of points from which this is estimated.) Using the Dudley theorem, as in Lecture 15, Sec. 2.1, we get that $R_N(F)\le \sqrt{\frac{d}{N}}$. However, using some elementary calculations as in Lecture 17, sec. 2.1, we get $R_N(F)\le \frac{1}{\sqrt{N}}$. My question is, should $R_N(F)$ depend on the dimension $d$ or not? Is $\sqrt{d}$ just an artefact of the proof technique in the first case? (First I thought that the application of Dudley theorem as above was not assuming that $\|x\|_2\le 1$, but it must do because otherwise we would not have $|w^Tx|\le 1$ and so the upper limit of the integral would not be 1.)",,"['probability', 'analysis', 'statistics', 'probability-theory', 'machine-learning']"
1,Standard Error of Sample Variance,Standard Error of Sample Variance,,"I have a time-series of values $X_1, X_2, \ldots, X_t$, for which I compute sample variance: $$\hat{\sigma}^2  = \operatorname{var}(X_1, \ldots, X_t)$$ (unabiased estimator using $\frac{1}{t-1})$. In a subsequent calculation, I would like to use to shrink this variance estimate in proportion to its precision. How can I empirically estimate the standard error of this variance estimate? In theory, the variance of sample variance (for normal distribution) is: $$\operatorname{var}(\hat{\sigma}^2) = \frac{2}{t - 1} (\sigma^2)^2 $$ where $\sigma^2$ is the true variance. For the purporse of this calculation, can I safely assume that $\hat{\sigma}^2 = \sigma^2$, and thus let: $$\operatorname{var}(\hat{\sigma}^2) = \frac{2}{t - 1} (\hat{\sigma}^2)^2 $$ be an estimate of the variance of the sample variance? Any help would be appreciated! Thanks!","I have a time-series of values $X_1, X_2, \ldots, X_t$, for which I compute sample variance: $$\hat{\sigma}^2  = \operatorname{var}(X_1, \ldots, X_t)$$ (unabiased estimator using $\frac{1}{t-1})$. In a subsequent calculation, I would like to use to shrink this variance estimate in proportion to its precision. How can I empirically estimate the standard error of this variance estimate? In theory, the variance of sample variance (for normal distribution) is: $$\operatorname{var}(\hat{\sigma}^2) = \frac{2}{t - 1} (\sigma^2)^2 $$ where $\sigma^2$ is the true variance. For the purporse of this calculation, can I safely assume that $\hat{\sigma}^2 = \sigma^2$, and thus let: $$\operatorname{var}(\hat{\sigma}^2) = \frac{2}{t - 1} (\hat{\sigma}^2)^2 $$ be an estimate of the variance of the sample variance? Any help would be appreciated! Thanks!",,"['statistics', 'standard-deviation', 'covariance']"
2,Rotational invariance and distributions,Rotational invariance and distributions,,"Let $k\leqslant n$ denote two positive integers, $A$ an $n \times k$ matrix with $A'A = I_k$, and $X$ and $Y$ two independent random variables on $\mathbb R^n$, each rotationally invariant (that is, their distributions do not change under the orthogonal transformations).    Write $X' = (U',W')$ and $Y' = (V',Z')$ with $U$ and $V$ on $\mathbb R^k$ and $W$ and $Z$ on $\mathbb R^{n-k}$. Prove that $(A'X, A'Y)$ has the same distribution as $(U,V)$. I know that by the Cramer-Wold theorem, for rotationally invariant random variables if we have a linear functional $t$ then $t'X$ has the same distribution as $|t|U$. Also $U$ and $W$ are rotationally invariant each in its own right in this case. I think these two assumptions should be used somewhere in the proof I just don't know how to start, because to me this is basically saying that the joint distribution of $(X, Y)$ is completely determined by the distribution of their first coordinate which is similar to Cramer-Wold except that the factor $|t|$ here is one so somehow ""length"" of $A'$ should be $1$ or it should appear as $A'A$ which is the identity.  Any help would be appreciated.","Let $k\leqslant n$ denote two positive integers, $A$ an $n \times k$ matrix with $A'A = I_k$, and $X$ and $Y$ two independent random variables on $\mathbb R^n$, each rotationally invariant (that is, their distributions do not change under the orthogonal transformations).    Write $X' = (U',W')$ and $Y' = (V',Z')$ with $U$ and $V$ on $\mathbb R^k$ and $W$ and $Z$ on $\mathbb R^{n-k}$. Prove that $(A'X, A'Y)$ has the same distribution as $(U,V)$. I know that by the Cramer-Wold theorem, for rotationally invariant random variables if we have a linear functional $t$ then $t'X$ has the same distribution as $|t|U$. Also $U$ and $W$ are rotationally invariant each in its own right in this case. I think these two assumptions should be used somewhere in the proof I just don't know how to start, because to me this is basically saying that the joint distribution of $(X, Y)$ is completely determined by the distribution of their first coordinate which is similar to Cramer-Wold except that the factor $|t|$ here is one so somehow ""length"" of $A'$ should be $1$ or it should appear as $A'A$ which is the identity.  Any help would be appreciated.",,"['probability', 'statistics', 'probability-theory']"
3,Show that $T$ is a sufficient statistic.,Show that  is a sufficient statistic.,T,"Suppose $X_1,\ldots,X_n$ is a sample from a population with parameter $\theta$.  Prove that if $T$ is a sufficient statistic for $\theta$, and $\theta=h(\eta)$ where $h$ is differentiable, then $T$ is a sufficient statistic for $\eta$.  Use the chain rule to construct your proof.  You may assume that $f_{X_1,\ldots,X_n\mid T}(x_1,\ldots,x_n\mid t;\theta)$ is a differentiable function of $\theta$. I was wondering if it would be better to approach this problem using the definition of sufficiency, or if I should try to implement the Neyman Factorization Theorem somewhere.  A hint or idea about how to start this proof would be appreciated.","Suppose $X_1,\ldots,X_n$ is a sample from a population with parameter $\theta$.  Prove that if $T$ is a sufficient statistic for $\theta$, and $\theta=h(\eta)$ where $h$ is differentiable, then $T$ is a sufficient statistic for $\eta$.  Use the chain rule to construct your proof.  You may assume that $f_{X_1,\ldots,X_n\mid T}(x_1,\ldots,x_n\mid t;\theta)$ is a differentiable function of $\theta$. I was wondering if it would be better to approach this problem using the definition of sufficiency, or if I should try to implement the Neyman Factorization Theorem somewhere.  A hint or idea about how to start this proof would be appreciated.",,"['probability', 'statistics']"
4,Transformation of a Random Variable,Transformation of a Random Variable,,"We have a  random variable $x$ with p.d.f. $\sqrt{\dfrac{\theta}{\pi x}}\exp(-x\theta)$, $x>0$ and $\theta$ a positive parameter. We are required to show that $2\theta x$  has a $\chi^2$ distribution with $1$ degree of freedom and deduce that, if $x_1,\dots,x_n$ are independent r.v. with this p.d.f., then $2\theta\sum_{i=1}^n x_i$ has a $\chi^2$ distribution with $n$ degrees of freedom. Using  transformation $y=2\theta x$ I found  the pdf of $$y=\frac{1}{\sqrt{2\pi}}y^{-1/2}e^{-y/2}.$$ How do I find the distribution of $2\theta\sum_{i=1}^n x_i$? Do I need to find the likelihood function  (which contains $\sum_{i=1}^n x_i$) first? How do I recognise the degrees of freedom of  this distribution (Is it $n$ because it involves $x_1,\dots,x_n$, i.e. $n$ random variables?","We have a  random variable $x$ with p.d.f. $\sqrt{\dfrac{\theta}{\pi x}}\exp(-x\theta)$, $x>0$ and $\theta$ a positive parameter. We are required to show that $2\theta x$  has a $\chi^2$ distribution with $1$ degree of freedom and deduce that, if $x_1,\dots,x_n$ are independent r.v. with this p.d.f., then $2\theta\sum_{i=1}^n x_i$ has a $\chi^2$ distribution with $n$ degrees of freedom. Using  transformation $y=2\theta x$ I found  the pdf of $$y=\frac{1}{\sqrt{2\pi}}y^{-1/2}e^{-y/2}.$$ How do I find the distribution of $2\theta\sum_{i=1}^n x_i$? Do I need to find the likelihood function  (which contains $\sum_{i=1}^n x_i$) first? How do I recognise the degrees of freedom of  this distribution (Is it $n$ because it involves $x_1,\dots,x_n$, i.e. $n$ random variables?",,"['statistics', 'transformation', 'statistical-inference', 'hypothesis-testing']"
5,Outlier Contained in Prediction Interval (Tme series Forecasting Problem),Outlier Contained in Prediction Interval (Tme series Forecasting Problem),,"In my stats class today, the professor was showing us some output from MINITAB on a prediction interval that was calculated (from time series data using standard linear regression). For one of the prediction intervals (which I believe was a forecast in the future), MINITAB had an X marked with the statement that there was an outlier in the prediction interval. I searched the MINITAB documentation and can't find a satisfactory explanation for how a computed prediction interval can contain an outlier. Does anyone know what this means? I know the SE of the PI increases with distance from xbar, but what criterion would you judge to determine the PI is ""too wide"" (which is what I assume the program is doing) TIA, Matt","In my stats class today, the professor was showing us some output from MINITAB on a prediction interval that was calculated (from time series data using standard linear regression). For one of the prediction intervals (which I believe was a forecast in the future), MINITAB had an X marked with the statement that there was an outlier in the prediction interval. I searched the MINITAB documentation and can't find a satisfactory explanation for how a computed prediction interval can contain an outlier. Does anyone know what this means? I know the SE of the PI increases with distance from xbar, but what criterion would you judge to determine the PI is ""too wide"" (which is what I assume the program is doing) TIA, Matt",,"['statistics', 'regression', 'statistical-inference', 'time-series']"
6,Why is the sample set denoted with $\chi$ in statistics?,Why is the sample set denoted with  in statistics?,\chi,"My professor of statistics based his lesson notations on H. Georgii's work 'Stochastics'. The sample set is thereby notated as $\chi$, instead of the usual $\Omega$. I don't really understand the reason for this. At page 190, Georggi writes: The notation $\chi$ instead of $\Omega$ is based on the idea that the observation is given by a random variable $X\colon\Omega\to\chi$, where $\Omega$ yields a detailed description of the randomness, whereas $\chi$ contains only the actually observable outcomes. However, since only the distribution of $X$ (rather than $X$ itself) plays a role, $\Omega$ does not appear explicitly here. Although this fragment is obviously ment as a clarification, I don't fully get it. Can you explain it a little bit better for me, please?","My professor of statistics based his lesson notations on H. Georgii's work 'Stochastics'. The sample set is thereby notated as $\chi$, instead of the usual $\Omega$. I don't really understand the reason for this. At page 190, Georggi writes: The notation $\chi$ instead of $\Omega$ is based on the idea that the observation is given by a random variable $X\colon\Omega\to\chi$, where $\Omega$ yields a detailed description of the randomness, whereas $\chi$ contains only the actually observable outcomes. However, since only the distribution of $X$ (rather than $X$ itself) plays a role, $\Omega$ does not appear explicitly here. Although this fragment is obviously ment as a clarification, I don't fully get it. Can you explain it a little bit better for me, please?",,"['statistics', 'notation']"
7,Maximum Likelihood Estimation with Laplace Distribution,Maximum Likelihood Estimation with Laplace Distribution,,"I want to estimate the parameters $a$ and $b$ of the model $y_i = ax_i + b + \varepsilon_i, i=1,...,n $ via Maximum Likelihood. The $\varepsilon_i$ are assumed to be Laplace-distributed with density  $f(x) = \frac{2}{\beta}\exp\left(\frac{\vert x\vert}{\beta}\right)$, and therefore $y_i \sim \text{Laplace}(\beta, \mu=ax_i + b)$. Maximizing the log-likelihood over a and b is then equivalent to minimizing $\sum_{i=1}^n \vert y_i - ax_i -b \vert$ I've come up with the following partial derivatives of the log-Likelihood l: $$ \frac{\partial l}{\partial a} = c*\sum_{i=1}^n -\text{sgn}(y_i-ax_i-b)x_i \\ \frac{\partial l}{\partial b} = c*\sum_{i=1}^n -\text{sgn}(y_i-ax_i-b) \\ $$ Edited: It seems to me, that $$\hat{b} = min \{b \in \mathbb{R}: \sum_{i=1}^{n}\frac{\mathbb{1}\{y_i-ax_i\leq b \}}{n}\geq \frac{1}{2}\} $$ Whereas $a$ has to be a solution of: $$ \sum_{i=1}^{n} \mathbb{1}\{y_i- \hat{b} \geq ax_i\}x_i = \sum_{i=1}^{n}\mathbb{1}\{y_i- \hat{b} < ax_i\}x_i $$ Any ideas?","I want to estimate the parameters $a$ and $b$ of the model $y_i = ax_i + b + \varepsilon_i, i=1,...,n $ via Maximum Likelihood. The $\varepsilon_i$ are assumed to be Laplace-distributed with density  $f(x) = \frac{2}{\beta}\exp\left(\frac{\vert x\vert}{\beta}\right)$, and therefore $y_i \sim \text{Laplace}(\beta, \mu=ax_i + b)$. Maximizing the log-likelihood over a and b is then equivalent to minimizing $\sum_{i=1}^n \vert y_i - ax_i -b \vert$ I've come up with the following partial derivatives of the log-Likelihood l: $$ \frac{\partial l}{\partial a} = c*\sum_{i=1}^n -\text{sgn}(y_i-ax_i-b)x_i \\ \frac{\partial l}{\partial b} = c*\sum_{i=1}^n -\text{sgn}(y_i-ax_i-b) \\ $$ Edited: It seems to me, that $$\hat{b} = min \{b \in \mathbb{R}: \sum_{i=1}^{n}\frac{\mathbb{1}\{y_i-ax_i\leq b \}}{n}\geq \frac{1}{2}\} $$ Whereas $a$ has to be a solution of: $$ \sum_{i=1}^{n} \mathbb{1}\{y_i- \hat{b} \geq ax_i\}x_i = \sum_{i=1}^{n}\mathbb{1}\{y_i- \hat{b} < ax_i\}x_i $$ Any ideas?",,"['statistics', 'probability-distributions', 'estimation']"
8,Binomial Coefficient Combinations,Binomial Coefficient Combinations,,"I have tried to figure this out and I cannot. The professor gave us an answer of 13,536 but I do not see any way in which he got to his answer. Any help would be greatly appreciated. A certain classroom has two rows of seats. The front row contains 8 seats and the back row contains 10 seats. How many ways are there to seat 15 students if a certain group of 4 or them refuses to sit in the front row?","I have tried to figure this out and I cannot. The professor gave us an answer of 13,536 but I do not see any way in which he got to his answer. Any help would be greatly appreciated. A certain classroom has two rows of seats. The front row contains 8 seats and the back row contains 10 seats. How many ways are there to seat 15 students if a certain group of 4 or them refuses to sit in the front row?",,"['combinatorics', 'statistics', 'permutations']"
9,Optimal solution for the Iowa Gambling Task?,Optimal solution for the Iowa Gambling Task?,,"The Iowa Gambling Task is used to demonstrate our intuition with statistics and probability. Essentially the subject sits in front of four decks. Each round, the participant can draw from any deck of her choosing. Each deck produces ""wins"" or ""losses"" along different probabilities. (Maybe the wins/losses vary in magnitudes, but let's simplify to the binary win/loss here). The goal is to maximize your wins (presumably by sampling from each deck to identify the deck with the best odds, then draw exclusively from that deck). It's described more fully here: http://en.wikipedia.org/wiki/Iowa_gambling_task One of the interesting things about this task is that most people fall into a pretty good intuitive strategy, even if they couldn't formalize what they're doing. Just how hard is it to formalize an optimal approach here? Has anyone rigorously tackled this? (The research on the problem I can find so far is about childhood development and psychology, not mathematics, maybe it's been covered, I'm not sure.) It's an interesting problem because seems to involve a few areas of mathematics, optimal stopping theory (for how long to sample): http://en.wikipedia.org/wiki/Optimal_stopping And maybe something about statistical confidence. If you've drawn from one deck three times and it pays out 2/3, that should impact your decisions differently than if you had drawn from a deck 300 times and seeing it pay out 2/3. Any ideas on ways to tackle this more rigorously?","The Iowa Gambling Task is used to demonstrate our intuition with statistics and probability. Essentially the subject sits in front of four decks. Each round, the participant can draw from any deck of her choosing. Each deck produces ""wins"" or ""losses"" along different probabilities. (Maybe the wins/losses vary in magnitudes, but let's simplify to the binary win/loss here). The goal is to maximize your wins (presumably by sampling from each deck to identify the deck with the best odds, then draw exclusively from that deck). It's described more fully here: http://en.wikipedia.org/wiki/Iowa_gambling_task One of the interesting things about this task is that most people fall into a pretty good intuitive strategy, even if they couldn't formalize what they're doing. Just how hard is it to formalize an optimal approach here? Has anyone rigorously tackled this? (The research on the problem I can find so far is about childhood development and psychology, not mathematics, maybe it's been covered, I'm not sure.) It's an interesting problem because seems to involve a few areas of mathematics, optimal stopping theory (for how long to sample): http://en.wikipedia.org/wiki/Optimal_stopping And maybe something about statistical confidence. If you've drawn from one deck three times and it pays out 2/3, that should impact your decisions differently than if you had drawn from a deck 300 times and seeing it pay out 2/3. Any ideas on ways to tackle this more rigorously?",,"['probability', 'statistics', 'gambling']"
10,Risk in density estimation: grasping the definition,Risk in density estimation: grasping the definition,,"When generalizing estimators to an entire function what is the space in which we perform the integral to obtain the expected value (with respect to this function)? For example, when estimating parameters, the risk definition is $$ R(\theta, \delta) = \mathbb{E}_\theta L\big( \theta, \delta(X) \big) = \int_X L\big( \theta, \delta(x) \big) \, \operatorname{d} P_\theta (x) $$ What happens with it when there are no parameters, but an entire function? What exactly would be an infinite-dimensional integral?","When generalizing estimators to an entire function what is the space in which we perform the integral to obtain the expected value (with respect to this function)? For example, when estimating parameters, the risk definition is $$ R(\theta, \delta) = \mathbb{E}_\theta L\big( \theta, \delta(X) \big) = \int_X L\big( \theta, \delta(x) \big) \, \operatorname{d} P_\theta (x) $$ What happens with it when there are no parameters, but an entire function? What exactly would be an infinite-dimensional integral?",,"['statistics', 'lebesgue-integral', 'expectation', 'estimation', 'parameter-estimation']"
11,How can I minimize the error from fitting two functions to different parts of data?,How can I minimize the error from fitting two functions to different parts of data?,,"I have some data, and I expect that part of the data will fit to a power law, and part of the data to a line. Here's a made up example of some similar data: Where I've made the red part a quadratic with some error, and the blue part a line with some error, just for illustration. Now supposed I don't know where this clear break between the functions is because it was data I just took in. I want to essentially find this point by choosing points throughout the data range, and for each point, suppose that's the ""break point"", and try fitting a power law to the left of it and a line to the right, and getting the R Squared value for each section, and minimizing the total error. My question is, if I have the R Squared for each section, what's the best way to minimize the ""total error""? Do I want to minimize simply $R_{power}^2 + R_{linear}^2$? Edit: To clarify, I don't know the actual exponent of the power law, or the slope of the line. I'll be fitting a general power law of the form $ax^b$ and a line $mx+c$ to the two regions (so it's not a matter of just finding the divider between the region for two known functions).","I have some data, and I expect that part of the data will fit to a power law, and part of the data to a line. Here's a made up example of some similar data: Where I've made the red part a quadratic with some error, and the blue part a line with some error, just for illustration. Now supposed I don't know where this clear break between the functions is because it was data I just took in. I want to essentially find this point by choosing points throughout the data range, and for each point, suppose that's the ""break point"", and try fitting a power law to the left of it and a line to the right, and getting the R Squared value for each section, and minimizing the total error. My question is, if I have the R Squared for each section, what's the best way to minimize the ""total error""? Do I want to minimize simply $R_{power}^2 + R_{linear}^2$? Edit: To clarify, I don't know the actual exponent of the power law, or the slope of the line. I'll be fitting a general power law of the form $ax^b$ and a line $mx+c$ to the two regions (so it's not a matter of just finding the divider between the region for two known functions).",,"['statistics', 'data-analysis']"
12,Distribution of sample means for large number of samples,Distribution of sample means for large number of samples,,"We are given normally distributed body weights with a mean of $195$ lbs. and a standard deviation of $24$ lbs. If an elevator in town has a maximum capacity of $20$ persons and a rated maximum weight of $4100$ lbs., what is the probability that the elevator, when filled to capacity, will not exceed its rated weight? (in other words, what is the probability that a sample of $20$ citizens will have a mean weight of less than $205$ lbs.?) This is how I thought the problem was to be worked $$205-195=24/\sqrt{20}=1.862$$  which of course is not correct.","We are given normally distributed body weights with a mean of $195$ lbs. and a standard deviation of $24$ lbs. If an elevator in town has a maximum capacity of $20$ persons and a rated maximum weight of $4100$ lbs., what is the probability that the elevator, when filled to capacity, will not exceed its rated weight? (in other words, what is the probability that a sample of $20$ citizens will have a mean weight of less than $205$ lbs.?) This is how I thought the problem was to be worked $$205-195=24/\sqrt{20}=1.862$$  which of course is not correct.",,['statistics']
13,Likelyhood of Poisson Distribution,Likelyhood of Poisson Distribution,,"The number of accidents in a week follows a poisson distribution with mean $\lambda$. Likelyhood is given as $$L(\lambda)=\frac{ \lambda^{\sum_1^n x_i } e^{-n\lambda}} { \prod x_i!}$$ However only a proportion p, are reported and each accident is reported with probability p, independent of all others. How would you modify the likelyhood function to take account of this?","The number of accidents in a week follows a poisson distribution with mean $\lambda$. Likelyhood is given as $$L(\lambda)=\frac{ \lambda^{\sum_1^n x_i } e^{-n\lambda}} { \prod x_i!}$$ However only a proportion p, are reported and each accident is reported with probability p, independent of all others. How would you modify the likelyhood function to take account of this?",,"['probability', 'statistics', 'poissons-equation']"
14,Predicting profit with price variation,Predicting profit with price variation,,"I am currently working on a high school project that aims to predict profit from X amount of items to Y amount of profit based off a deviated sale price. For instance: I sale 10 cookies for 10 dollars, but on another sale I am able to sell for a higher price per cookie and achieve to sell 10 cookies for 20 dollars. Now say I sell a bulk order for 200 cookies and sale them for 400 dollars, but someone else makes the same order and I sell it to them for only 300 dollars. Essentially the price per cookie deviates in a range of a couple dollars or so, but when trying to use linear regression, the predictions are not even close to accurate. What is the best way to make an averaged prediction when using this type of business model? ` What is the best regression model in order to achieve this? Thanks in advance for the help.","I am currently working on a high school project that aims to predict profit from X amount of items to Y amount of profit based off a deviated sale price. For instance: I sale 10 cookies for 10 dollars, but on another sale I am able to sell for a higher price per cookie and achieve to sell 10 cookies for 20 dollars. Now say I sell a bulk order for 200 cookies and sale them for 400 dollars, but someone else makes the same order and I sell it to them for only 300 dollars. Essentially the price per cookie deviates in a range of a couple dollars or so, but when trying to use linear regression, the predictions are not even close to accurate. What is the best way to make an averaged prediction when using this type of business model? ` What is the best regression model in order to achieve this? Thanks in advance for the help.",,"['statistics', 'regression', 'mathematical-modeling', 'data-analysis']"
15,Multiplying/dividing mean and std dev of two different values,Multiplying/dividing mean and std dev of two different values,,"I'm working on predicting outcomes of games given statistics from the two teams involved. Two of the statistics I have are below: AGA (average goals against): The arithmetic mean of number of goals a team's opponent scored in each game. AOP (average offensive production): The geometric mean of (the ratio between the number of goals a team scored and their opponent's AGA in each game). Now I have two teams, A and B, that have not played each other, but may have played zero or more of the same teams. A simple way to predict the number of goals scored by A against B is to multiply A's AOP by B's AGA. Is there a way to get a standard deviation of this predicted outcome from the numbers given? What if I had the values averaged to give the AGA and AOP in question? Could the same be done for division between two statistics instead of multiplication?","I'm working on predicting outcomes of games given statistics from the two teams involved. Two of the statistics I have are below: AGA (average goals against): The arithmetic mean of number of goals a team's opponent scored in each game. AOP (average offensive production): The geometric mean of (the ratio between the number of goals a team scored and their opponent's AGA in each game). Now I have two teams, A and B, that have not played each other, but may have played zero or more of the same teams. A simple way to predict the number of goals scored by A against B is to multiply A's AOP by B's AGA. Is there a way to get a standard deviation of this predicted outcome from the numbers given? What if I had the values averaged to give the AGA and AOP in question? Could the same be done for division between two statistics instead of multiplication?",,['statistics']
16,Why does β=α give a symmetric standard beta pdf?,Why does β=α give a symmetric standard beta pdf?,,"I know that β=α is what will give a symmetric standard beta pdf, but why is this so?","I know that β=α is what will give a symmetric standard beta pdf, but why is this so?",,"['statistics', 'beta-function']"
17,orderstatistics of uniform distributions on different ranges,orderstatistics of uniform distributions on different ranges,,"During a simulation I discovered an interesting phenomenon: Given you have 3 agents. 2 are uniformly distributed between [0,1] and one between [0,2]. The question is how often do the smaller agents win and what is there average value when winning. As expected the winning probability is 50%. However it turns out that the smaller agents have an average combined value of approx. 1.167 when winning and the large agent has an average combined value of approx. 1.418 (close to root 2). At least for me this is counterintuitive. Can someone give me an explanation of why this is. Source code: double totalSmall=0;     double totalLarge=0;     int smallCount = 0;     int largeCount = 0;     int count = 0;      for (int i = 0; i < 1e6; ++i) {          double large = Math.random() * 2;         double small = Math.random() + Math.random();         if(large>small){             largeCount++;             totalLarge+=large;         }else{             smallCount++;             totalSmall+=small;         }     }     System.out.println(count);     System.out.println(smallCount);     System.out.println(totalSmall);     System.out.println(totalSmall/smallCount);      System.out.println(largeCount);     System.out.println(totalLarge);     System.out.println(totalLarge/largeCount);","During a simulation I discovered an interesting phenomenon: Given you have 3 agents. 2 are uniformly distributed between [0,1] and one between [0,2]. The question is how often do the smaller agents win and what is there average value when winning. As expected the winning probability is 50%. However it turns out that the smaller agents have an average combined value of approx. 1.167 when winning and the large agent has an average combined value of approx. 1.418 (close to root 2). At least for me this is counterintuitive. Can someone give me an explanation of why this is. Source code: double totalSmall=0;     double totalLarge=0;     int smallCount = 0;     int largeCount = 0;     int count = 0;      for (int i = 0; i < 1e6; ++i) {          double large = Math.random() * 2;         double small = Math.random() + Math.random();         if(large>small){             largeCount++;             totalLarge+=large;         }else{             smallCount++;             totalSmall+=small;         }     }     System.out.println(count);     System.out.println(smallCount);     System.out.println(totalSmall);     System.out.println(totalSmall/smallCount);      System.out.println(largeCount);     System.out.println(totalLarge);     System.out.println(totalLarge/largeCount);",,"['statistics', 'uniform-distribution', 'order-statistics', 'simulation']"
18,Joint PDF of Chi-Square & Normal Distribution,Joint PDF of Chi-Square & Normal Distribution,,"Let the independent random variables X1 and X2 be N(0,1) and $\chi^2(r)$, respectively. Let $Y_1$ = $X_1/sqrt(X_2/r)$ and $Y_2$ = $X_2$ a) Find the joint pdf of $Y_1$ and $Y_2$. b) Determine the marginal pdf of $Y_1$ and show that $Y_1$ has a t distribution. Thoughts: So beyond confused I don't even know where to start. I assume there must be a trick because creating a joint pdf with a chi-square distribution and a normal distribution using the traditional method seems really strenuous.","Let the independent random variables X1 and X2 be N(0,1) and $\chi^2(r)$, respectively. Let $Y_1$ = $X_1/sqrt(X_2/r)$ and $Y_2$ = $X_2$ a) Find the joint pdf of $Y_1$ and $Y_2$. b) Determine the marginal pdf of $Y_1$ and show that $Y_1$ has a t distribution. Thoughts: So beyond confused I don't even know where to start. I assume there must be a trick because creating a joint pdf with a chi-square distribution and a normal distribution using the traditional method seems really strenuous.",,"['statistics', 'random-variables', 'descriptive-statistics']"
19,Expected number of sides of a dice,Expected number of sides of a dice,,"I have two dice, one with m sides (labeled $1,2,...,m$) and one with $n$ sides (labeled $1,2,...,n$). I roll both three times. The $m$-sided one comes up $1, 2, 9$ and the $n$-sided one comes up $7, 7, 8$. Which is higher: the expected value of $m$ or the expected value of $n$? Now compute both expected values and give their approximate value with a $95\%$ confidence interval. Part (a) of the question seems pretty straightforward and I tried approaching part (b) using MLEs but that didn't turn out too well because the likelihood function of this involves a $n!$ term, namely $$P(x_1,x_2,...x_n|n)=\frac{n!}{x_1!x_2!...x_n!}\Big(\frac{1}{n}\Big)^{x_1+x_2+...x_n}$$ Any suggestions? Thanks!","I have two dice, one with m sides (labeled $1,2,...,m$) and one with $n$ sides (labeled $1,2,...,n$). I roll both three times. The $m$-sided one comes up $1, 2, 9$ and the $n$-sided one comes up $7, 7, 8$. Which is higher: the expected value of $m$ or the expected value of $n$? Now compute both expected values and give their approximate value with a $95\%$ confidence interval. Part (a) of the question seems pretty straightforward and I tried approaching part (b) using MLEs but that didn't turn out too well because the likelihood function of this involves a $n!$ term, namely $$P(x_1,x_2,...x_n|n)=\frac{n!}{x_1!x_2!...x_n!}\Big(\frac{1}{n}\Big)^{x_1+x_2+...x_n}$$ Any suggestions? Thanks!",,"['statistics', 'statistical-inference']"
20,Welford's algorithm for standard deviation: combine multiple sets of results,Welford's algorithm for standard deviation: combine multiple sets of results,,"Suppose I use Welford's algorithm to compute the standard deviation of multiple sets of values. I only store all the n, mean and M2 results that the algorithm calculates, thus I have these three results per set. Is it possible to calculate the standard deviation over all samples in all sets, by using only the stored results? Thus I'm looking for a way to combine the n, mean and M2 results. For n and mean this is trivial, but I don't know how to combine the M2 values. Note: this would be easy when I would use the naive sum of squares algorithm , but wikipedia recommends against using this algorithm in practice. Note2: I cannot store all values in all sets, I'm writing software that only gets one chance to do something with each value, thus the algorithm needs to be 'incremental' or 'online'.","Suppose I use Welford's algorithm to compute the standard deviation of multiple sets of values. I only store all the n, mean and M2 results that the algorithm calculates, thus I have these three results per set. Is it possible to calculate the standard deviation over all samples in all sets, by using only the stored results? Thus I'm looking for a way to combine the n, mean and M2 results. For n and mean this is trivial, but I don't know how to combine the M2 values. Note: this would be easy when I would use the naive sum of squares algorithm , but wikipedia recommends against using this algorithm in practice. Note2: I cannot store all values in all sets, I'm writing software that only gets one chance to do something with each value, thus the algorithm needs to be 'incremental' or 'online'.",,"['statistics', 'standard-deviation']"
21,Lower bound for expectation of squared log?,Lower bound for expectation of squared log?,,Is there a (tight) lower bound for $\mathbb{E}[(\log x)^2]$ where $x$ is a non-negative random variable? Jensen's inequality doesn't seem to apply here since the squared of a log isn't convex. Thanks!,Is there a (tight) lower bound for $\mathbb{E}[(\log x)^2]$ where $x$ is a non-negative random variable? Jensen's inequality doesn't seem to apply here since the squared of a log isn't convex. Thanks!,,"['probability', 'statistics', 'expectation']"
22,Expectation of proportion random variable,Expectation of proportion random variable,,Suppose that $X$ and $Y$ are two random variable. $X$ and $Y$ are independent. We also have $f$ and $g$ are two continuous functions on $\mathbb{R}$. Is the following equation true? $$ \mathbb{E}\left[\frac{f(X)}{g(Y)} \right] = \frac{\mathbb{E}[f(X)]}{\mathbb{E}[g(Y)]}$$ Thank you for any answer.,Suppose that $X$ and $Y$ are two random variable. $X$ and $Y$ are independent. We also have $f$ and $g$ are two continuous functions on $\mathbb{R}$. Is the following equation true? $$ \mathbb{E}\left[\frac{f(X)}{g(Y)} \right] = \frac{\mathbb{E}[f(X)]}{\mathbb{E}[g(Y)]}$$ Thank you for any answer.,,"['probability', 'statistics', 'probability-theory']"
23,Question on the correlation between two dependant variables,Question on the correlation between two dependant variables,,"I'm working on this question and it's stumping me. Let Sn = X1 + ... + Xn (with n>=1) be a random walk with X1,...,Xn be iid RV's. E(Xk)=mu Var(Xk)=sigma^2. Find the covariance of Sn and Sm Can anyone help out? I am trying to use the equation Cov[Sn, Sm] = E[SnSm] - E[Sn]E[Sm]","I'm working on this question and it's stumping me. Let Sn = X1 + ... + Xn (with n>=1) be a random walk with X1,...,Xn be iid RV's. E(Xk)=mu Var(Xk)=sigma^2. Find the covariance of Sn and Sm Can anyone help out? I am trying to use the equation Cov[Sn, Sm] = E[SnSm] - E[Sn]E[Sm]",,"['statistics', 'correlation']"
24,Identifying the k points in 2D geographic space which are 'most distant' from each other,Identifying the k points in 2D geographic space which are 'most distant' from each other,,"I have a set of DNA samples from Y plants in a given geographic area.  I'm going to be doing DNA sequencing on individuals in this population (and a number of other, separate populations), however due to financial constraints I'm unable to perform sequencing on all Y individuals.  I've decided that I can afford to sequence k (out of Y) in each separate area. I'd like to select the k samples which are farthest apart/most geographically distributed within the Y samples I collected.  Samples that are taken from plants in close proximity to each other are more likely to be closely related, and I'd like to sequence what are ultimately the most genetically diverse samples for my later analysis. So, the question is: given a set of Y points/samples in 2d geographic space (lat/long coordinates), how do I select the k points that are most geographically distributed/distant from each other? As I've explored this a bit, I think the problem I'm really having is how to define 'distance' or 'most distributed'.  Some of the metrics I've though of (e.g. maximum average distance between the k points) result in really unintuitive point selection in certain cases (for example, if two points are right next to each other but far away from another cluster of all the other points, the two points will be included even if they're almost on top of each other). I have a feeling that this is not an uncommon problem, and there must be good answers out there.  I'll add that I have access to a large cluster and I can brute force the problem to some extent. I'd appreciate any and all advice or discussion; this is an interesting theoretical topic to me.","I have a set of DNA samples from Y plants in a given geographic area.  I'm going to be doing DNA sequencing on individuals in this population (and a number of other, separate populations), however due to financial constraints I'm unable to perform sequencing on all Y individuals.  I've decided that I can afford to sequence k (out of Y) in each separate area. I'd like to select the k samples which are farthest apart/most geographically distributed within the Y samples I collected.  Samples that are taken from plants in close proximity to each other are more likely to be closely related, and I'd like to sequence what are ultimately the most genetically diverse samples for my later analysis. So, the question is: given a set of Y points/samples in 2d geographic space (lat/long coordinates), how do I select the k points that are most geographically distributed/distant from each other? As I've explored this a bit, I think the problem I'm really having is how to define 'distance' or 'most distributed'.  Some of the metrics I've though of (e.g. maximum average distance between the k points) result in really unintuitive point selection in certain cases (for example, if two points are right next to each other but far away from another cluster of all the other points, the two points will be included even if they're almost on top of each other). I have a feeling that this is not an uncommon problem, and there must be good answers out there.  I'll add that I have access to a large cluster and I can brute force the problem to some extent. I'd appreciate any and all advice or discussion; this is an interesting theoretical topic to me.",,"['statistics', 'graph-theory', 'clustering']"
25,find out how 'peaky' histogram is,find out how 'peaky' histogram is,,"I have a histogram made of number of detections made by an application: 9 instances of detecting 1, 1 instances of detecting 2 and so on. Say I have these histograms: first histogram: 9 1 1 second histogram: 9 2 0 I'd like to know the 'confidence' of the histogram so I can find out which one is better. The first or the second. In other words, if the histogram has one big peak and the rest of values are very low, then this histogram is the best one I have. another example: 70% 30% 70% 15% 15% And so on.. I am probably missing the scientific term here, so any direction or help would be great. Thanks.","I have a histogram made of number of detections made by an application: 9 instances of detecting 1, 1 instances of detecting 2 and so on. Say I have these histograms: first histogram: 9 1 1 second histogram: 9 2 0 I'd like to know the 'confidence' of the histogram so I can find out which one is better. The first or the second. In other words, if the histogram has one big peak and the rest of values are very low, then this histogram is the best one I have. another example: 70% 30% 70% 15% 15% And so on.. I am probably missing the scientific term here, so any direction or help would be great. Thanks.",,['statistics']
26,Stratified Sampling: Total and mean estimate of population,Stratified Sampling: Total and mean estimate of population,,"Question: In a sample survey designed to estimate the total number of cattle, the universe of 2072 farms was stratified into 5 strata on the basis of the total acreage of farms. In the hth stratum (h = 1,2,...,5), a simple random sample (with replacement) of farms (of size $n_h$) was taen in proportion to the total number of farms in the stratum ($N_h$), the total sample size being $ n = \Sigma n_h = 500$; for each stratum, the total number of cattle in the sample farms ($\large \Sigma_{i=1}^{n_h}y_{hi})$ are given in the table below. Estimate the total and average number of cattle per farm, along with their standard error. \begin{array}{rcl}  \mbox{Stratum (acres)} &\mbox{# farms in stratum (Nh)}  & \mbox{# farms sampled} & \mbox{number of cows in sample}\\  \ 0-15 & 635 & 153 & 619\\ 16-30 & 570 & 138 & 1423 \\ 31-50 & 475 & 115 & 1578 \\ 51-75  & 303 & 73 & 1691 \\  76-100 & 89 & 21 & 603 \\ \mbox{All Strata} & 2072 \mbox{(N)} & 500 & 6094 \\ \end{array} Attempt: Average cattle per stratum: 0-15 acres: $\large \frac{619}{153} = 4.046$ 16-30 acres: $\large \frac{1423}{138} = 10.312$ 31-50 acres: $\large \frac{1578}{115} = 13.722$ 51-75 acres: $\large \frac{1691}{73} = 23.164$ 76-100 acres: $\large \frac{603}{21} = 28.714$ Then the average number of cows is: $\frac{1}{2072}[(4.046)(635)+(10.312)(570)+(13.722)(475)+(23.164)(303)+(28.714)(89)]$ $\bar{y}=11.843 \approx 12$ Total number of cattle is $2072 * 11.843 = 24538.696 \approx 24539$.","Question: In a sample survey designed to estimate the total number of cattle, the universe of 2072 farms was stratified into 5 strata on the basis of the total acreage of farms. In the hth stratum (h = 1,2,...,5), a simple random sample (with replacement) of farms (of size $n_h$) was taen in proportion to the total number of farms in the stratum ($N_h$), the total sample size being $ n = \Sigma n_h = 500$; for each stratum, the total number of cattle in the sample farms ($\large \Sigma_{i=1}^{n_h}y_{hi})$ are given in the table below. Estimate the total and average number of cattle per farm, along with their standard error. \begin{array}{rcl}  \mbox{Stratum (acres)} &\mbox{# farms in stratum (Nh)}  & \mbox{# farms sampled} & \mbox{number of cows in sample}\\  \ 0-15 & 635 & 153 & 619\\ 16-30 & 570 & 138 & 1423 \\ 31-50 & 475 & 115 & 1578 \\ 51-75  & 303 & 73 & 1691 \\  76-100 & 89 & 21 & 603 \\ \mbox{All Strata} & 2072 \mbox{(N)} & 500 & 6094 \\ \end{array} Attempt: Average cattle per stratum: 0-15 acres: $\large \frac{619}{153} = 4.046$ 16-30 acres: $\large \frac{1423}{138} = 10.312$ 31-50 acres: $\large \frac{1578}{115} = 13.722$ 51-75 acres: $\large \frac{1691}{73} = 23.164$ 76-100 acres: $\large \frac{603}{21} = 28.714$ Then the average number of cows is: $\frac{1}{2072}[(4.046)(635)+(10.312)(570)+(13.722)(475)+(23.164)(303)+(28.714)(89)]$ $\bar{y}=11.843 \approx 12$ Total number of cattle is $2072 * 11.843 = 24538.696 \approx 24539$.",,"['probability', 'statistics', 'probability-theory', 'statistical-inference', 'sampling']"
27,About Bayesian formula and rating system,About Bayesian formula and rating system,,"I'm building a scoring system with score from 0 to 5) and I would like to sort products according to the number of reviews and their scores. After some research on the Internet I have found two formulas to get a good ranking system. Which one is the Bayesian formula ? What is the other formula ? Which one is valid ? I have tried to explain the problem using a simple reviews table and SQL queries, here's the reviews table. Reviews table Product   | Score Product A | 3      \ Product A | 3      | Product A | 3      |-> 5 reviews Product A | 3      | Product A | 3      / Product B | 2      \ Product B | 2      | Product B | 2      | Product B | 2      | Product B | 2      |-> 10 reviews Product B | 2      | Product B | 2      | Product B | 2      | Product B | 2      | Product B | 2      / Product C | 1      \ Product C | 1      | Product C | 1      | Product C | 1      | Product C | 1      | Product C | 1      | Product C | 1      | Product C | 1      |-> 15 reviews Product C | 1      | Product C | 1      | Product C | 1      | Product C | 1      | Product C | 1      | Product C | 1      | Product C | 1      / Method A, general formula: (AVG_NUM_VOTE * AVG_RATING) + (PRODUCT_NUM_VOTES * PRODUCT_AVG_SCORE) ---------------------------------------------------------------------                  (AVG_NUM_VOTE + PRODUCT_NUM_VOTES) AVG_NUM_VOTE : SELECT COUNT(product)/COUNT(DISTINCT product) FROM reviews => 10 AVG_RATING : SELECT AVG(score) FROM reviews => 1.666666... Now, to get the value for each product: Product A: PRODUCT_NUM_VOTE : SELECT COUNT(product) FROM reviews WHERE product = 'Product A' = 5 PRODUCT_AVG_SCORE : SELECT AVG(score) FROM reviews WHERE product = 'Product A' = 3 (10 * 1.66666) + (5 * 3) ------------------------ = 2.111111...           10 + 5 Product B : PRODUCT_NUM_VOTE : SELECT COUNT(product) FROM reviews WHERE product = 'Product B' = 10 PRODUCT_AVG_SCORE : SELECT AVG(score) FROM reviews WHERE product = 'Product B' = 2 (10 * 1.66666) + (10 * 2) ------------------------- = 1.833333...          10 + 10 Product C : PRODUCT_NUM_VOTE : SELECT COUNT(product) FROM reviews WHERE product = 'Product C' = 15 PRODUCT_AVG_SCORE : SELECT AVG(score) FROM reviews WHERE product = 'Product C' = 1 (10 * 1.66666) + (15 * 1) ------------------------- = 1.266666666           10 + 15 Method B, general formula: TOTAL_SUM + (PRODUCT_NUM_VOTE * PRODUCT_AVG_SCORE) --------------------------------------------------         (TOTAL_VOTE + PRODUCT_NUM_VOTE) TOTAL_SUM : SELECT SUM(count) FROM ( SELECT COUNT(DISTINCT product) AS count FROM reviews ) AS SUM = 5*3 + 10*2 + 15*1 = 50 TOTAL_VOTE : SELECT COUNT(product) FROM reviews = 30 Product A : PRODUCT_NUM_VOTE : SELECT COUNT(product) FROM reviews WHERE product = 'Product A' = 5 PRODUCT_AVG_SCORE : SELECT AVG(score) FROM reviews WHERE product = 'Product A' = 3 50 + (5 * 3) ------------ = 1.857142    30 + 5 Product B : PRODUCT_NUM_VOTE : SELECT COUNT(product) FROM reviews WHERE product = 'Product B' = 10 PRODUCT_AVG_SCORE : SELECT AVG(score) FROM reviews WHERE product = 'Product B' = 2 50 + (10 * 2) ------------- = 1.75    30 + 10 Product C : PRODUCT_NUM_VOTE : SELECT COUNT(product) FROM reviews WHERE product = 'Product C' = 15 PRODUCT_AVG_SCORE : SELECT AVG(score) FROM reviews WHERE product = 'Product C' = 1 50 + (15 * 1) ------------- = 1.444444    30 + 15 Now if I compare the two methods: Product   | Method A | Method B Product A | 2.111111 | 1.857142 Product B | 1.833333 | 1.75 Product C | 1.266666 | 1.444444 Which method is the Bayesian average ? And what is the other method ? What is the best formula to use ? Thanks for reading... Edit: During my search, I've noticed that in method B: TOTAL_SUM = (AVG_RATING * AVG_NUM_VOTE * (#products) ) TOTAL_VOTE = (AVG_NUM_VOTE * (#products) ) #products is the number of different products taken in account. My guess is that the first formula is incomplete and we need to add something in it, like this: (AVG_NUM_VOTE * AVG_RATING * #products) + (PRODUCT_NUM_VOTES * PRODUCT_AVG_SCORE) ---------------------------------------------------------------------------------                ( (AVG_NUM_VOTE * #products) + PRODUCT_NUM_VOTES) What do you think ?","I'm building a scoring system with score from 0 to 5) and I would like to sort products according to the number of reviews and their scores. After some research on the Internet I have found two formulas to get a good ranking system. Which one is the Bayesian formula ? What is the other formula ? Which one is valid ? I have tried to explain the problem using a simple reviews table and SQL queries, here's the reviews table. Reviews table Product   | Score Product A | 3      \ Product A | 3      | Product A | 3      |-> 5 reviews Product A | 3      | Product A | 3      / Product B | 2      \ Product B | 2      | Product B | 2      | Product B | 2      | Product B | 2      |-> 10 reviews Product B | 2      | Product B | 2      | Product B | 2      | Product B | 2      | Product B | 2      / Product C | 1      \ Product C | 1      | Product C | 1      | Product C | 1      | Product C | 1      | Product C | 1      | Product C | 1      | Product C | 1      |-> 15 reviews Product C | 1      | Product C | 1      | Product C | 1      | Product C | 1      | Product C | 1      | Product C | 1      | Product C | 1      / Method A, general formula: (AVG_NUM_VOTE * AVG_RATING) + (PRODUCT_NUM_VOTES * PRODUCT_AVG_SCORE) ---------------------------------------------------------------------                  (AVG_NUM_VOTE + PRODUCT_NUM_VOTES) AVG_NUM_VOTE : SELECT COUNT(product)/COUNT(DISTINCT product) FROM reviews => 10 AVG_RATING : SELECT AVG(score) FROM reviews => 1.666666... Now, to get the value for each product: Product A: PRODUCT_NUM_VOTE : SELECT COUNT(product) FROM reviews WHERE product = 'Product A' = 5 PRODUCT_AVG_SCORE : SELECT AVG(score) FROM reviews WHERE product = 'Product A' = 3 (10 * 1.66666) + (5 * 3) ------------------------ = 2.111111...           10 + 5 Product B : PRODUCT_NUM_VOTE : SELECT COUNT(product) FROM reviews WHERE product = 'Product B' = 10 PRODUCT_AVG_SCORE : SELECT AVG(score) FROM reviews WHERE product = 'Product B' = 2 (10 * 1.66666) + (10 * 2) ------------------------- = 1.833333...          10 + 10 Product C : PRODUCT_NUM_VOTE : SELECT COUNT(product) FROM reviews WHERE product = 'Product C' = 15 PRODUCT_AVG_SCORE : SELECT AVG(score) FROM reviews WHERE product = 'Product C' = 1 (10 * 1.66666) + (15 * 1) ------------------------- = 1.266666666           10 + 15 Method B, general formula: TOTAL_SUM + (PRODUCT_NUM_VOTE * PRODUCT_AVG_SCORE) --------------------------------------------------         (TOTAL_VOTE + PRODUCT_NUM_VOTE) TOTAL_SUM : SELECT SUM(count) FROM ( SELECT COUNT(DISTINCT product) AS count FROM reviews ) AS SUM = 5*3 + 10*2 + 15*1 = 50 TOTAL_VOTE : SELECT COUNT(product) FROM reviews = 30 Product A : PRODUCT_NUM_VOTE : SELECT COUNT(product) FROM reviews WHERE product = 'Product A' = 5 PRODUCT_AVG_SCORE : SELECT AVG(score) FROM reviews WHERE product = 'Product A' = 3 50 + (5 * 3) ------------ = 1.857142    30 + 5 Product B : PRODUCT_NUM_VOTE : SELECT COUNT(product) FROM reviews WHERE product = 'Product B' = 10 PRODUCT_AVG_SCORE : SELECT AVG(score) FROM reviews WHERE product = 'Product B' = 2 50 + (10 * 2) ------------- = 1.75    30 + 10 Product C : PRODUCT_NUM_VOTE : SELECT COUNT(product) FROM reviews WHERE product = 'Product C' = 15 PRODUCT_AVG_SCORE : SELECT AVG(score) FROM reviews WHERE product = 'Product C' = 1 50 + (15 * 1) ------------- = 1.444444    30 + 15 Now if I compare the two methods: Product   | Method A | Method B Product A | 2.111111 | 1.857142 Product B | 1.833333 | 1.75 Product C | 1.266666 | 1.444444 Which method is the Bayesian average ? And what is the other method ? What is the best formula to use ? Thanks for reading... Edit: During my search, I've noticed that in method B: TOTAL_SUM = (AVG_RATING * AVG_NUM_VOTE * (#products) ) TOTAL_VOTE = (AVG_NUM_VOTE * (#products) ) #products is the number of different products taken in account. My guess is that the first formula is incomplete and we need to add something in it, like this: (AVG_NUM_VOTE * AVG_RATING * #products) + (PRODUCT_NUM_VOTES * PRODUCT_AVG_SCORE) ---------------------------------------------------------------------------------                ( (AVG_NUM_VOTE * #products) + PRODUCT_NUM_VOTES) What do you think ?",,"['statistics', 'average', 'bayesian', 'bayes-theorem', 'sorting']"
28,Statistica Significane of the Slope Coefficient,Statistica Significane of the Slope Coefficient,,"Can someone please help me with this? Consider that you are examining the relationship between the height of children and their parents. You decide to collect data from 110 college students, and estimate the following relationship : (hat) Studenth = 19.6 + 0.73 × Midparh with R2 = 0.45, the Standard Error of the Regression (SER) = 2.0, standard error for the intercept is (7.2) and for the slope is (0.10), where StudentHeight is the height of students in inches, and AveragePaentHeight is the average of the parental heights. Both variables were adjusted so that the average female height was equal to the average male height. How do you set for the statistical significance of the slope coefficient?","Can someone please help me with this? Consider that you are examining the relationship between the height of children and their parents. You decide to collect data from 110 college students, and estimate the following relationship : (hat) Studenth = 19.6 + 0.73 × Midparh with R2 = 0.45, the Standard Error of the Regression (SER) = 2.0, standard error for the intercept is (7.2) and for the slope is (0.10), where StudentHeight is the height of students in inches, and AveragePaentHeight is the average of the parental heights. Both variables were adjusted so that the average female height was equal to the average male height. How do you set for the statistical significance of the slope coefficient?",,"['statistics', 'economics']"
29,How to show that if $X_n \downarrow 0$ then $P(X_n >\epsilon) \downarrow 0$,How to show that if  then,X_n \downarrow 0 P(X_n >\epsilon) \downarrow 0,"My solution is as follows: If $X_n \downarrow 0$ then $\forall \epsilon > 0~ \exists N\ge n $ such that $X_n < \epsilon$. This means that $$I_{X_n \ge \epsilon} = I_{X_n = \epsilon} + I_{X_n > \epsilon} =0$$ Now if I take expectations, then $$P(X_n=e) +P(X_n > \epsilon) = 0$$ Since probabilities are non-negative, this means that $P(X_n > \epsilon) = 0$ and the proof is complete. I can't help but have the feeling it's somehow wrong. First it seems like I proved a much stronger condition and two, if $X_n$ is a continuous function that limits to a singularity, the probability of $X_n$ being greater than a certain number will never be actually 0. So where did I go wrong? Thanks. UPDATE Thanks to Mike's comments, I am trying the following: Define $E_n = \cup_{i \ge n}\{X_i > \epsilon\}$, the set of points $\omega \in \Omega$ for which the sequence $X_i(ω)$, for $i \ge n$, is above $\epsilon$ at least once. To show that $P(E_n) \downarrow 0$, I proceed as follows: $$P(E_n) = P(\omega \in \cup_{i \ge n}\{X_i > \epsilon\})$$ By the definition of pointwise convergence, I can claim that for each $\omega$, $\exists N \ge n  $ such that $X_i(\omega) < \epsilon$. Essentially I can always take an $N$ big enough to knock out any $\omega$ from $E_n$. Then given an $\epsilon$, I take such $N$, which means that for $i > N$, $$I_{X_i(\omega) > \epsilon} = 0 $$ Taking its expectation, I can say that $$P(E_n) = 0 $$ From here I'm not sure how to show that $P(E_n)$ is strictly greater than $P(X_n > \epsilon)$.","My solution is as follows: If $X_n \downarrow 0$ then $\forall \epsilon > 0~ \exists N\ge n $ such that $X_n < \epsilon$. This means that $$I_{X_n \ge \epsilon} = I_{X_n = \epsilon} + I_{X_n > \epsilon} =0$$ Now if I take expectations, then $$P(X_n=e) +P(X_n > \epsilon) = 0$$ Since probabilities are non-negative, this means that $P(X_n > \epsilon) = 0$ and the proof is complete. I can't help but have the feeling it's somehow wrong. First it seems like I proved a much stronger condition and two, if $X_n$ is a continuous function that limits to a singularity, the probability of $X_n$ being greater than a certain number will never be actually 0. So where did I go wrong? Thanks. UPDATE Thanks to Mike's comments, I am trying the following: Define $E_n = \cup_{i \ge n}\{X_i > \epsilon\}$, the set of points $\omega \in \Omega$ for which the sequence $X_i(ω)$, for $i \ge n$, is above $\epsilon$ at least once. To show that $P(E_n) \downarrow 0$, I proceed as follows: $$P(E_n) = P(\omega \in \cup_{i \ge n}\{X_i > \epsilon\})$$ By the definition of pointwise convergence, I can claim that for each $\omega$, $\exists N \ge n  $ such that $X_i(\omega) < \epsilon$. Essentially I can always take an $N$ big enough to knock out any $\omega$ from $E_n$. Then given an $\epsilon$, I take such $N$, which means that for $i > N$, $$I_{X_i(\omega) > \epsilon} = 0 $$ Taking its expectation, I can say that $$P(E_n) = 0 $$ From here I'm not sure how to show that $P(E_n)$ is strictly greater than $P(X_n > \epsilon)$.",,"['probability', 'statistics', 'probability-theory']"
30,Correlation between sleep hours and brain activity,Correlation between sleep hours and brain activity,,"Say I am tracking my sleep for 1 week and these are the number of hours I sleep each night: (5, 6, 9, 4, 8, 9, 6) everyday that I track my sleep I am also taking a test that measures how well my brain is working these were my scores: (45, 23, 33, 48, 68, 19, 26) The higher the score the better. Given these numbers, I would like to find out the optimal number of hours I would have to sleep to score the highest on this test. For example, the answer could be X amount of hours would give me the best chance of scoring highest on this test. I would like to know how to find X. Any help that points me in the right direction would be incredibly appreciated!","Say I am tracking my sleep for 1 week and these are the number of hours I sleep each night: (5, 6, 9, 4, 8, 9, 6) everyday that I track my sleep I am also taking a test that measures how well my brain is working these were my scores: (45, 23, 33, 48, 68, 19, 26) The higher the score the better. Given these numbers, I would like to find out the optimal number of hours I would have to sleep to score the highest on this test. For example, the answer could be X amount of hours would give me the best chance of scoring highest on this test. I would like to know how to find X. Any help that points me in the right direction would be incredibly appreciated!",,['statistics']
31,Percentages of percentages,Percentages of percentages,,"I do not even know how to title this question, or the name of the thing i'm looking for, but here's an example: there's N participants we'll take just three into account, but i need a formula for N participants A has x% to win B, B has y% of winning C .. x% and y% are known since A hasn't competed with C .. i want to guess what % of A winning against C would be, based on their percentages against B. there could be N participants between A and C, not just 1 as in the example how would i calculate this? Thanks for your time!","I do not even know how to title this question, or the name of the thing i'm looking for, but here's an example: there's N participants we'll take just three into account, but i need a formula for N participants A has x% to win B, B has y% of winning C .. x% and y% are known since A hasn't competed with C .. i want to guess what % of A winning against C would be, based on their percentages against B. there could be N participants between A and C, not just 1 as in the example how would i calculate this? Thanks for your time!",,"['probability', 'statistics', 'percentages']"
32,OLS standard error that corrects for autocorrelation but not heteroskedasticity,OLS standard error that corrects for autocorrelation but not heteroskedasticity,,"Question: By mapping the OLS regression into the GMM framework, write the formula for the standard error of the OLS regression coefficients that corrects for autocorrelation but not heteroskedasticity. Furthermore, show that in this case, the conventional standard errors are OK if the $x$'s are uncorrelated over time, even if the errors $\varepsilon$ are correlated over time. Attempt: So the general model is $y_t = \beta' x_t + \varepsilon_t$. OLS picks parameters $\beta$ to minimize the variance of the residual: $$\min_{\beta} E_T[(y_t-\beta' x_t)^2] $$ where the notation $E_t(\cdot) = \frac{1}{T} \sum_{t=1}^T( \cdot )$ denotes the sample mean. We find $\widehat{\beta}$ from the first-order condition, which states that: $$g_T(\beta) = E_T[x_t(y_t - x_t' \beta)] =0$$ In the GMM context, here, the number of moments equals the number of parameters. Thus, we set the sample moments exactly to zero and solve for the estimate analytically: $$\widehat{\beta} = [E_T(x_tx_t')]^{-1} E_T(x_t y_t)$$ Using the known result from GMM theory that  $$Var(\widehat{b}) = \frac{1}{T} (ad)^{-1} aSa^{\prime} (ad)^{-1 \prime}$$ where in this case $a = I$ (the identity matrix), $d = -E[x_t x_t']$, and $S = \sum_{j=-\infty}^{\infty} E[f(x_t, b), f(x_{t-j}, b)']$ with $f(x_t, \beta) = x_t(y_t - x_t'\beta) = x_t \varepsilon_t$. So the general formula for the standard error of OLS is $$Var(\widehat{\beta}) = \frac{1}{T}E(x_t x_t')^{-1} \left[\sum_{j=-\infty}^{\infty} E(\varepsilon_t x_t x_{t-j}' \varepsilon_{t-j})\right]E(x_t x_t')^{-1}$$ Now I know from the OLS assumptions: (i) No autocorrelation: $E(\varepsilon_t \mid x_t, x_{t-1}, \cdots, \varepsilon_{t-1}, \varepsilon_{t-2}, \cdots) =0$ (ii) No heteroskedasticity: $E(\varepsilon_t^2 \mid x_t, x_{t-1}, \cdots, \varepsilon_{t-1}, \cdots) = constant = \sigma_{\varepsilon}^2$ What would the OLS standard error become if I correct for autocorrelation but not heteroskedasticity? Also how do I show that the conventional standard errors are OK if the $x$'s are uncorrelated over time, even if the errors $\varepsilon$ are correlated over time?","Question: By mapping the OLS regression into the GMM framework, write the formula for the standard error of the OLS regression coefficients that corrects for autocorrelation but not heteroskedasticity. Furthermore, show that in this case, the conventional standard errors are OK if the $x$'s are uncorrelated over time, even if the errors $\varepsilon$ are correlated over time. Attempt: So the general model is $y_t = \beta' x_t + \varepsilon_t$. OLS picks parameters $\beta$ to minimize the variance of the residual: $$\min_{\beta} E_T[(y_t-\beta' x_t)^2] $$ where the notation $E_t(\cdot) = \frac{1}{T} \sum_{t=1}^T( \cdot )$ denotes the sample mean. We find $\widehat{\beta}$ from the first-order condition, which states that: $$g_T(\beta) = E_T[x_t(y_t - x_t' \beta)] =0$$ In the GMM context, here, the number of moments equals the number of parameters. Thus, we set the sample moments exactly to zero and solve for the estimate analytically: $$\widehat{\beta} = [E_T(x_tx_t')]^{-1} E_T(x_t y_t)$$ Using the known result from GMM theory that  $$Var(\widehat{b}) = \frac{1}{T} (ad)^{-1} aSa^{\prime} (ad)^{-1 \prime}$$ where in this case $a = I$ (the identity matrix), $d = -E[x_t x_t']$, and $S = \sum_{j=-\infty}^{\infty} E[f(x_t, b), f(x_{t-j}, b)']$ with $f(x_t, \beta) = x_t(y_t - x_t'\beta) = x_t \varepsilon_t$. So the general formula for the standard error of OLS is $$Var(\widehat{\beta}) = \frac{1}{T}E(x_t x_t')^{-1} \left[\sum_{j=-\infty}^{\infty} E(\varepsilon_t x_t x_{t-j}' \varepsilon_{t-j})\right]E(x_t x_t')^{-1}$$ Now I know from the OLS assumptions: (i) No autocorrelation: $E(\varepsilon_t \mid x_t, x_{t-1}, \cdots, \varepsilon_{t-1}, \varepsilon_{t-2}, \cdots) =0$ (ii) No heteroskedasticity: $E(\varepsilon_t^2 \mid x_t, x_{t-1}, \cdots, \varepsilon_{t-1}, \cdots) = constant = \sigma_{\varepsilon}^2$ What would the OLS standard error become if I correct for autocorrelation but not heteroskedasticity? Also how do I show that the conventional standard errors are OK if the $x$'s are uncorrelated over time, even if the errors $\varepsilon$ are correlated over time?",,"['statistics', 'regression', 'correlation', 'least-squares']"
33,Relationship between chi-squared and standard normal distributions.,Relationship between chi-squared and standard normal distributions.,,"It is well known that if $Z \sim N(0,1)$ then $Z^{2} \sim \chi^{2}(1)$. However, if we know that $X^{2} \sim \chi^{2}(1)$, under what conditions is it true that $X \sim N(0,1)$? As far as I know, this isn't an if and only if. I'm thinking it might be the case provided $X$ is a symmetric distribution.","It is well known that if $Z \sim N(0,1)$ then $Z^{2} \sim \chi^{2}(1)$. However, if we know that $X^{2} \sim \chi^{2}(1)$, under what conditions is it true that $X \sim N(0,1)$? As far as I know, this isn't an if and only if. I'm thinking it might be the case provided $X$ is a symmetric distribution.",,"['statistics', 'probability-distributions']"
34,A criterion for independence based on Characteristic function,A criterion for independence based on Characteristic function,,Let $X$ and $Y$ be real-valued random variables defined on the same space.  Let's use $\phi_X$ to denote the characteristic function of $X$.  If $\phi_{X+Y}=\phi_X\phi_Y$ then must $X$ and $Y$ be independent?,Let $X$ and $Y$ be real-valued random variables defined on the same space.  Let's use $\phi_X$ to denote the characteristic function of $X$.  If $\phi_{X+Y}=\phi_X\phi_Y$ then must $X$ and $Y$ be independent?,,"['probability-theory', 'characteristic-functions', 'independence']"
35,Can you somehow combine the probabilities of two events?,Can you somehow combine the probabilities of two events?,,"I know P(A|X) and P(A|Y), and I'm interested to compute/estimate P(A|X,Y). I suspect it cannot be done directly: if I know that 30% of women have blue eyes and 20% of english people have blue eyes, I can't directly estimate how many english women have blue eyes because I'd find out the percentage of women in England; OTOH, if I know the percentage of women in England then I don't care how many english people have blue eyes. I guess my question is - can I combine these two informations somehow? What is the minimum (additional) information that I need to find out (or estimate) in order to compute P(A|X,Y) ?","I know P(A|X) and P(A|Y), and I'm interested to compute/estimate P(A|X,Y). I suspect it cannot be done directly: if I know that 30% of women have blue eyes and 20% of english people have blue eyes, I can't directly estimate how many english women have blue eyes because I'd find out the percentage of women in England; OTOH, if I know the percentage of women in England then I don't care how many english people have blue eyes. I guess my question is - can I combine these two informations somehow? What is the minimum (additional) information that I need to find out (or estimate) in order to compute P(A|X,Y) ?",,"['probability', 'statistics']"
36,Confidence interval for the conversion on site,Confidence interval for the conversion on site,,"I am the developer of web service and I'm trying to to build some plots for the inner dashboard. I raised two questions that I can not solve on their own. Suppose n visitors went to the site for some period of time. During this time, m visitors were successfully registered and became users. Conversion to registration is the ratio of registered users to all visitors or the probability that a visitor registers on the site - m / n . The questions are: What is the (minimum) range of the true value of the conversion to registration with defined probability p ? What is the probability that the true value of the conversion are greater than a predefined value c ? If I understand correctly, it is Bernoulli process, but I do not understand what to do with this knowledge. Just I have gaps in terminology, so I will be grateful to you if you tell me how to mathematically call those things I'm looking for.","I am the developer of web service and I'm trying to to build some plots for the inner dashboard. I raised two questions that I can not solve on their own. Suppose n visitors went to the site for some period of time. During this time, m visitors were successfully registered and became users. Conversion to registration is the ratio of registered users to all visitors or the probability that a visitor registers on the site - m / n . The questions are: What is the (minimum) range of the true value of the conversion to registration with defined probability p ? What is the probability that the true value of the conversion are greater than a predefined value c ? If I understand correctly, it is Bernoulli process, but I do not understand what to do with this knowledge. Just I have gaps in terminology, so I will be grateful to you if you tell me how to mathematically call those things I'm looking for.",,"['probability', 'statistics', 'probability-distributions']"
37,Moment generating function of a piecewise function,Moment generating function of a piecewise function,,"I am struggling like crazy with question b. Here is what I have come up with: But if I then test $M_X'(0)$, I don't get the expected value I calculated in part a: Where have I gone wrong?","I am struggling like crazy with question b. Here is what I have come up with: But if I then test $M_X'(0)$, I don't get the expected value I calculated in part a: Where have I gone wrong?",,"['statistics', 'moment-generating-functions']"
38,Method for deriving an Exponential Moving Average?,Method for deriving an Exponential Moving Average?,,"I have a formula for an exponentially weighted moving average function defined recursively as: $S_t = a*Y_t+(1-a)*S_{t-1}$ Where: $a\in (0,1)\cap \mathbb{Q}$ $t$ represents time $Y_t$ is the value at a time period $t$ $S_t$ is the value of the EMA at any time period t. I am trying to find a generally applicable solution for the derivative of $S_t$ in regards to $dt$. Any assistance would be most appreciated!","I have a formula for an exponentially weighted moving average function defined recursively as: $S_t = a*Y_t+(1-a)*S_{t-1}$ Where: $a\in (0,1)\cap \mathbb{Q}$ $t$ represents time $Y_t$ is the value at a time period $t$ $S_t$ is the value of the EMA at any time period t. I am trying to find a generally applicable solution for the derivative of $S_t$ in regards to $dt$. Any assistance would be most appreciated!",,"['calculus', 'statistics', 'derivatives', 'recurrence-relations', 'time-series']"
39,Showing the multivariate normal is log-concave?,Showing the multivariate normal is log-concave?,,"I'm trying to show that $\log p(x) = -\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)$ is concave. How would I go about this in $\mathbb{R}^n$? I've tried taking derivatives but I'm getting stuck once I get to $$\nabla\log p(x) = - \det[ (x-\mu)^{T}\Sigma^{-1}(x-\mu) ] * \Sigma^{-1}(x-\mu) * [(x-\mu)^{T}\Sigma^{-1}(x-\mu)]^{-1}.$$ I still would need to take another gradient with respect to $x\in \mathbb{R}^n$ so this takes me nowhere. Can someone help? Btw, $\mu$ is the mean and $\Sigma$ is the covariance matrix.","I'm trying to show that $\log p(x) = -\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)$ is concave. How would I go about this in $\mathbb{R}^n$? I've tried taking derivatives but I'm getting stuck once I get to $$\nabla\log p(x) = - \det[ (x-\mu)^{T}\Sigma^{-1}(x-\mu) ] * \Sigma^{-1}(x-\mu) * [(x-\mu)^{T}\Sigma^{-1}(x-\mu)]^{-1}.$$ I still would need to take another gradient with respect to $x\in \mathbb{R}^n$ so this takes me nowhere. Can someone help? Btw, $\mu$ is the mean and $\Sigma$ is the covariance matrix.",,"['statistics', 'convex-analysis', 'convex-optimization']"
40,Expected Value on code,Expected Value on code,,"I'm trying to figure out the expected number of times this algorithm will print. I'm stuck on how to go about doing so. I used an indicator variable to keep track of the number of print statements being printed, but I'm stumped on the meat part of it. random(X,Y,Z): print(X)   if Y != X:       print(Y)   if Z != Y and Z != X:       print(Z) You will suppose the inputs X,Y,Z are chosen at random, uniformly and independently from the set {1,...,k}, (for some k being >=1).","I'm trying to figure out the expected number of times this algorithm will print. I'm stuck on how to go about doing so. I used an indicator variable to keep track of the number of print statements being printed, but I'm stumped on the meat part of it. random(X,Y,Z): print(X)   if Y != X:       print(Y)   if Z != Y and Z != X:       print(Z) You will suppose the inputs X,Y,Z are chosen at random, uniformly and independently from the set {1,...,k}, (for some k being >=1).",,"['probability', 'statistics', 'probability-distributions', 'computer-science', 'computational-complexity']"
41,How to compute N th Values for the series,How to compute N th Values for the series,,"I have 3 variables X, Y, Z $X_0=3$ . $Y_0=1$ . $Z_0=0$ . $X_n=X_{n-1} + 3 * Z_{n_-1}$ . $Y_n=X_{n-1} + 2 * Z_{n_-1}$ . $Z_n= 5* Y_{n-1} $ . I have tried alot to get a series or pattern for the nth X, Y or Z. But not able to. I tried getting numbers till 10th elements but i cannot find a pattern here. 1- 3,3,5 2- 18,13,15 3- 63,48,65 4- 258,193,240 5- 978,738,965 6- 3873,2908,3690 7- 14943,11253,14540 8- 58563,44023,56265 9- 227358,171093,220115 Could someone Please help. Thanks","I have 3 variables X, Y, Z . . . . . . I have tried alot to get a series or pattern for the nth X, Y or Z. But not able to. I tried getting numbers till 10th elements but i cannot find a pattern here. 1- 3,3,5 2- 18,13,15 3- 63,48,65 4- 258,193,240 5- 978,738,965 6- 3873,2908,3690 7- 14943,11253,14540 8- 58563,44023,56265 9- 227358,171093,220115 Could someone Please help. Thanks",X_0=3 Y_0=1 Z_0=0 X_n=X_{n-1} + 3 * Z_{n_-1} Y_n=X_{n-1} + 2 * Z_{n_-1} Z_n= 5* Y_{n-1} ,"['sequences-and-series', 'statistics']"
42,Real life math to explore/solve [closed],Real life math to explore/solve [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 7 years ago . Improve this question What are some examples of mathematics application in the real life that is interesting to explore about? And not too complicated but not too easy, something that exist around us. I'm interested in doing something related to integration, trigonometry and statistics. I currently taking an IB diploma (Maths HL course) so my knowledge on those topics is not really wide. Thus, I'm searching for real - life problem that up to my level to explore/solve? Thanks in advance","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 7 years ago . Improve this question What are some examples of mathematics application in the real life that is interesting to explore about? And not too complicated but not too easy, something that exist around us. I'm interested in doing something related to integration, trigonometry and statistics. I currently taking an IB diploma (Maths HL course) so my knowledge on those topics is not really wide. Thus, I'm searching for real - life problem that up to my level to explore/solve? Thanks in advance",,"['integration', 'statistics', 'trigonometry']"
43,Differential Equation for brownian bridge?,Differential Equation for brownian bridge?,,"For the brownian motion, we know that probability density of the particle's position at time $ t $, $ \rho(x,t) $ satisfies the diffusion equation pde: $ \partial_t \rho = d \; \partial_x^2 \rho $. Is there some similar statement for the brownian bridge, something of the form: the probability density of the particle's position $\rho $ satisfies $ L \rho = 0 $ for some differential operator $L $ with the boundary conditions $ \rho(x,0) = \rho(x,1) = \delta(x) $?","For the brownian motion, we know that probability density of the particle's position at time $ t $, $ \rho(x,t) $ satisfies the diffusion equation pde: $ \partial_t \rho = d \; \partial_x^2 \rho $. Is there some similar statement for the brownian bridge, something of the form: the probability density of the particle's position $\rho $ satisfies $ L \rho = 0 $ for some differential operator $L $ with the boundary conditions $ \rho(x,0) = \rho(x,1) = \delta(x) $?",,"['statistics', 'probability-distributions', 'partial-differential-equations', 'soft-question']"
44,Algebra problem involving Q functions,Algebra problem involving Q functions,,"I have the following algebra problem, which is actually the end-part of my bigger research problem. Let $a$, $b$ and $m$ be reals with $a<b$. Also, let $Q(\cdot)$ denote the Gaussian Q-function, i.e., $Q(x) = \frac{1}{\sqrt{2\pi}}\int_{x}^{\infty} e^{-t^2/2} dt$. I have the quantities $b_{\infty} = e^{-\frac{m^2}{2}} \left[ Q(a) - Q(b) \right]$ $b_0 = \sqrt{[Q(a+m) - Q(b+m)][Q(a-m) - Q(b-m)]}$ $b_1 = \sqrt{[Q(a+m) - Q(t+m)][Q(a-m) - Q(t-m)]} + \sqrt{[Q(t+m) - Q(b+m)][Q(t-m) - Q(b-m)]}$ and I am trying to prove that, for any $a<b$, and for any $m$, there is a $a<t<b$ such that $b_1^2 \leq b_0 b_{\infty}$. I am certain that such $t$ exists, having done thourough simulations on the matter, but I cant show it anallytically. If you have a good eye in algebra, I would appreciate your input.","I have the following algebra problem, which is actually the end-part of my bigger research problem. Let $a$, $b$ and $m$ be reals with $a<b$. Also, let $Q(\cdot)$ denote the Gaussian Q-function, i.e., $Q(x) = \frac{1}{\sqrt{2\pi}}\int_{x}^{\infty} e^{-t^2/2} dt$. I have the quantities $b_{\infty} = e^{-\frac{m^2}{2}} \left[ Q(a) - Q(b) \right]$ $b_0 = \sqrt{[Q(a+m) - Q(b+m)][Q(a-m) - Q(b-m)]}$ $b_1 = \sqrt{[Q(a+m) - Q(t+m)][Q(a-m) - Q(t-m)]} + \sqrt{[Q(t+m) - Q(b+m)][Q(t-m) - Q(b-m)]}$ and I am trying to prove that, for any $a<b$, and for any $m$, there is a $a<t<b$ such that $b_1^2 \leq b_0 b_{\infty}$. I am certain that such $t$ exists, having done thourough simulations on the matter, but I cant show it anallytically. If you have a good eye in algebra, I would appreciate your input.",,"['calculus', 'analysis', 'statistics']"
45,Binomial distribution (working included),Binomial distribution (working included),,"I've been working on this question but I'm not too sure if I am correct... Any feedback or advice would be much appreciated. Question:  Suppose an aircraft has a capacity of 500 passengers. The airline that operates it 'overbooks' the plane by accepting reservations for up to 575 passengers, because it knows from experience that 5% of passengers do not check in for the flight. a) Assuming that X, the number of passengers who check in for the flight, can be modelled by a binomial distribution, calculate the probability that one or more passengers will be denied the chance to board. My working: We can safely use the normal approximation to the binomial with such large numbers. mean = np = 575*0.95 = 546.25 sd = √(npq) = √(575*0.95*0.05) = 5.226 with continuity correction, > 500 becomes > 500.5 z-score = (500.5-546.25)/5.226 = -8.75 P(z> -8.75) ≈ 1 This means that for all practical persons, it is certain that one or more persons will be denied the chance to board.","I've been working on this question but I'm not too sure if I am correct... Any feedback or advice would be much appreciated. Question:  Suppose an aircraft has a capacity of 500 passengers. The airline that operates it 'overbooks' the plane by accepting reservations for up to 575 passengers, because it knows from experience that 5% of passengers do not check in for the flight. a) Assuming that X, the number of passengers who check in for the flight, can be modelled by a binomial distribution, calculate the probability that one or more passengers will be denied the chance to board. My working: We can safely use the normal approximation to the binomial with such large numbers. mean = np = 575*0.95 = 546.25 sd = √(npq) = √(575*0.95*0.05) = 5.226 with continuity correction, > 500 becomes > 500.5 z-score = (500.5-546.25)/5.226 = -8.75 P(z> -8.75) ≈ 1 This means that for all practical persons, it is certain that one or more persons will be denied the chance to board.",,"['probability', 'statistics', 'binomial-theorem']"
46,Distribution of the sample variance of n iid exponential variables,Distribution of the sample variance of n iid exponential variables,,"I have to check some properties of an estimator, but I can't find its distribution. Let $X_1,...,X_n $ be independent identically distributed exponential variables with parameter $ \theta $, i.e. with pdf $ f(x) = \frac{1}{\theta} \exp(-\frac{x}{\theta})$. What is the distribution of the sample variance $ \overline{X^2} - (\overline X)^2$? What I found so far: $\frac{2n}{\theta}\overline X$ ~ $ \Gamma(n,2)$ But how can I find the distribution of the sample variance? Any help is greatly appreciated!","I have to check some properties of an estimator, but I can't find its distribution. Let $X_1,...,X_n $ be independent identically distributed exponential variables with parameter $ \theta $, i.e. with pdf $ f(x) = \frac{1}{\theta} \exp(-\frac{x}{\theta})$. What is the distribution of the sample variance $ \overline{X^2} - (\overline X)^2$? What I found so far: $\frac{2n}{\theta}\overline X$ ~ $ \Gamma(n,2)$ But how can I find the distribution of the sample variance? Any help is greatly appreciated!",,"['statistics', 'probability-distributions', 'estimation']"
47,Hypergeometric RV - what is the sample/population?,Hypergeometric RV - what is the sample/population?,,"An instructor who taught two sections of engineering statistics last term, the first with 20 students and the second with 30, decided to assign a term project. After all projects had been turned in, the instructor randomly ordered them before grading. Consider the first 15 graded projects. 1) What is the probability that exactly 10 of these are from the second section? My assumptions for a hypergeometric rv X are as follows: You have a population of N objects.  Each object can be characterized as S or F. There are M successes in the population.  A sample of n objects is selected without replacement in such a way that each subset of size n is equally likely to be chosen. If x is the number of S's in a completely random sample of size n drawn from a population consisting of M S's and (N-M) F's, then the probability distribution of X, called the hypergeometric distribution, is given by $$ P(X = x) = h(x; n, M, N) = \frac{{M \choose x} {N-M \choose n-x}}{N \choose n}   $$ Analyzing this problem, it seemed to me that the assumptions led to this correspondence: Population of 50 students (N = 50).  Class 1 represented as F and Class 2 represented as S. M would be 30, since M is the number of S in the population. A sample of n objects is selected (15 papers chosen), and the question asks what the probability is that 10 of these is a S (10 are in class 2). So my understanding of the problem was that the answer would be h(10; 15, 30, 50), which is 0.2069. However, when I was checking my work against the solutions, I found that the correct answer was h(x; 10, 15, 50) and I have difficulty understanding why this is the case... The sample is 15, and then you want to find the probability that 10 of them are S, so how is it that the correct sample is 10? I would deeply appreciate someone explaining this to me. Thank you.","An instructor who taught two sections of engineering statistics last term, the first with 20 students and the second with 30, decided to assign a term project. After all projects had been turned in, the instructor randomly ordered them before grading. Consider the first 15 graded projects. 1) What is the probability that exactly 10 of these are from the second section? My assumptions for a hypergeometric rv X are as follows: You have a population of N objects.  Each object can be characterized as S or F. There are M successes in the population.  A sample of n objects is selected without replacement in such a way that each subset of size n is equally likely to be chosen. If x is the number of S's in a completely random sample of size n drawn from a population consisting of M S's and (N-M) F's, then the probability distribution of X, called the hypergeometric distribution, is given by $$ P(X = x) = h(x; n, M, N) = \frac{{M \choose x} {N-M \choose n-x}}{N \choose n}   $$ Analyzing this problem, it seemed to me that the assumptions led to this correspondence: Population of 50 students (N = 50).  Class 1 represented as F and Class 2 represented as S. M would be 30, since M is the number of S in the population. A sample of n objects is selected (15 papers chosen), and the question asks what the probability is that 10 of these is a S (10 are in class 2). So my understanding of the problem was that the answer would be h(10; 15, 30, 50), which is 0.2069. However, when I was checking my work against the solutions, I found that the correct answer was h(x; 10, 15, 50) and I have difficulty understanding why this is the case... The sample is 15, and then you want to find the probability that 10 of them are S, so how is it that the correct sample is 10? I would deeply appreciate someone explaining this to me. Thank you.",,"['probability', 'statistics', 'probability-distributions']"
48,Prediction error in least squares with a linear model,Prediction error in least squares with a linear model,,"In the classical linear model with  $$Y=X\beta +\epsilon,$$ where $Y \in \mathbb{R}^n$ is the observation, $X\in \mathbb{R}^{n\times p}$ is the known covariates, $\beta \in \mathbb{R}^p$ is the unknown parameter with, $p < n$, and $\epsilon \in \mathbb{R}^{n}$, $\epsilon \sim \mathcal{N}(0,\sigma^{2}I)$. The classical least squares estimator here would be $$\hat{\beta}= (X^TX)^{-1}X^TY.$$ If $\beta^{0}$ is the true parameter, then we have that the prediction error is given by $E=||X(\hat{\beta}- \beta^{0})||_2^2$. We have that, $$E=||X(\hat{\beta}- \beta^{0})||_2^2/\sigma^2=||X(X^TX)^{-1}X^T \epsilon||_2^2/\sigma^2=||\gamma||_2^2/\sigma^2.$$ It is easy to see that $\gamma \sim \mathcal{N}(0,\sigma^2 X(X^TX)^{-1}X^T)$. However it is claimed in High Dimensional Statistics by Bühlmann and van de Geer (on page 101) that $E$ is distributed according to a chi-square distribution with $p$ degrees of freedom. I can not see how this is true (It would be true if $\gamma \sim \mathcal{N}(0,D)$ for some diagonal matrix $D$ with non-negative diagonal entries, but the fact that the co-ordinates of $\gamma$ are correlated make makes this false.) Am I missing something here?","In the classical linear model with  $$Y=X\beta +\epsilon,$$ where $Y \in \mathbb{R}^n$ is the observation, $X\in \mathbb{R}^{n\times p}$ is the known covariates, $\beta \in \mathbb{R}^p$ is the unknown parameter with, $p < n$, and $\epsilon \in \mathbb{R}^{n}$, $\epsilon \sim \mathcal{N}(0,\sigma^{2}I)$. The classical least squares estimator here would be $$\hat{\beta}= (X^TX)^{-1}X^TY.$$ If $\beta^{0}$ is the true parameter, then we have that the prediction error is given by $E=||X(\hat{\beta}- \beta^{0})||_2^2$. We have that, $$E=||X(\hat{\beta}- \beta^{0})||_2^2/\sigma^2=||X(X^TX)^{-1}X^T \epsilon||_2^2/\sigma^2=||\gamma||_2^2/\sigma^2.$$ It is easy to see that $\gamma \sim \mathcal{N}(0,\sigma^2 X(X^TX)^{-1}X^T)$. However it is claimed in High Dimensional Statistics by Bühlmann and van de Geer (on page 101) that $E$ is distributed according to a chi-square distribution with $p$ degrees of freedom. I can not see how this is true (It would be true if $\gamma \sim \mathcal{N}(0,D)$ for some diagonal matrix $D$ with non-negative diagonal entries, but the fact that the co-ordinates of $\gamma$ are correlated make makes this false.) Am I missing something here?",,"['statistics', 'least-squares']"
49,Kernel density estimation in the limit of infinity many samples,Kernel density estimation in the limit of infinity many samples,,"Let ($x_1, ..., x_n$) be i.i.d. samples drawn from some distribution $P$ with an unknown probability density function $f$. Its kernel density estimator is \begin{align}     \hat{f}_h(x) = \frac{1}{n}\sum_{i=1}^n K_h (x - x_i) \quad = \frac{1}{nh} \sum_{i=1}^n K\Big(\frac{x-x_i}{h}\Big), \end{align} where $K$ a symmetric non-negative function that integrates to one. I am interested what happens in the limit $n \to \infty$. Are there any publications that prove  \begin{align}     \hat{f}_h(x) &= \int K_h (x - x_i) \text{d}x \quad \text{or} \\ &= \int K_h (x - x_i) \text{d}P ? \end{align} Is then in the limit $\hat{f}_h(x) = f$?","Let ($x_1, ..., x_n$) be i.i.d. samples drawn from some distribution $P$ with an unknown probability density function $f$. Its kernel density estimator is \begin{align}     \hat{f}_h(x) = \frac{1}{n}\sum_{i=1}^n K_h (x - x_i) \quad = \frac{1}{nh} \sum_{i=1}^n K\Big(\frac{x-x_i}{h}\Big), \end{align} where $K$ a symmetric non-negative function that integrates to one. I am interested what happens in the limit $n \to \infty$. Are there any publications that prove  \begin{align}     \hat{f}_h(x) &= \int K_h (x - x_i) \text{d}x \quad \text{or} \\ &= \int K_h (x - x_i) \text{d}P ? \end{align} Is then in the limit $\hat{f}_h(x) = f$?",,"['probability', 'statistics', 'measure-theory']"
50,Units of vaiance when variable is in %.,Units of vaiance when variable is in %.,,"I have some confusion here. If some random variable is measured in some units, say $kg$ then clearly it's variance is measured in $kg^2$. But if the variable is dimensionless  and measured say in  $\%$ or base points in what unit the variance is measured? $\%^2$? Does it make sense?. It seems for me a bit weird.","I have some confusion here. If some random variable is measured in some units, say $kg$ then clearly it's variance is measured in $kg^2$. But if the variable is dimensionless  and measured say in  $\%$ or base points in what unit the variance is measured? $\%^2$? Does it make sense?. It seems for me a bit weird.",,['statistics']
51,Number of channels required to provide access to subscribers 80% of the time,Number of channels required to provide access to subscribers 80% of the time,,"A multichannel microwave link is to provide telephone communicationto a remote community having 12 subscribers, each of whom uses the link 20% of the time during peak hours. How many channels are needed to make the link available during peak hours to: a. Eighty percent of the subscribers all of the time? b. All of the subscribers 80% of the time? For the a) part I came up with answer 10 channels which is correct according to my TA but I am not sure about the procedure I followed (I just found out what is eighty percent of 12) I have no clue what b part means","A multichannel microwave link is to provide telephone communicationto a remote community having 12 subscribers, each of whom uses the link 20% of the time during peak hours. How many channels are needed to make the link available during peak hours to: a. Eighty percent of the subscribers all of the time? b. All of the subscribers 80% of the time? For the a) part I came up with answer 10 channels which is correct according to my TA but I am not sure about the procedure I followed (I just found out what is eighty percent of 12) I have no clue what b part means",,['statistics']
52,Statistics; looking for a practical example-based book,Statistics; looking for a practical example-based book,,"I'm looking for an example-based 2nd year or thereabouts undergraduate statistics textbook in the style of Engineering Maths by Ken Stroud ( http://www.amazon.co.uk/Stroud-Engineering-Mathematics/dp/0387912185 easily my favourite maths text book). At the moment I feel I've been exposed to lots of statistical tests, but I'm not confident of using any of them (or knowing the correct one to use) in anger on real-world problems. If it could cover Bayesian probability as well, that would be excellent. Thanks","I'm looking for an example-based 2nd year or thereabouts undergraduate statistics textbook in the style of Engineering Maths by Ken Stroud ( http://www.amazon.co.uk/Stroud-Engineering-Mathematics/dp/0387912185 easily my favourite maths text book). At the moment I feel I've been exposed to lots of statistical tests, but I'm not confident of using any of them (or knowing the correct one to use) in anger on real-world problems. If it could cover Bayesian probability as well, that would be excellent. Thanks",,['statistics']
53,"2-dimensional random walk, covariance","2-dimensional random walk, covariance",,"Let $X_1, \ldots X_n \sim N \left( \left[ \begin{array}{c} 0  \\ 0  \end{array} \right], \left[ \begin{array}{cc} 1 & \rho\\ \rho & 1\end{array} \right] \right), S_n = \sum \limits_{i = 1}^n X_i$. If we had one dimensional random walk, the variance is $\sigma^2n$. But, how to compute covariance matrix of $S_n$ in 2-dimensional case?","Let $X_1, \ldots X_n \sim N \left( \left[ \begin{array}{c} 0  \\ 0  \end{array} \right], \left[ \begin{array}{cc} 1 & \rho\\ \rho & 1\end{array} \right] \right), S_n = \sum \limits_{i = 1}^n X_i$. If we had one dimensional random walk, the variance is $\sigma^2n$. But, how to compute covariance matrix of $S_n$ in 2-dimensional case?",,"['probability', 'statistics']"
54,Estimators and confidence interval,Estimators and confidence interval,,"Can someone explain me how to solve the following exercise?  I don't like to post this kind of question, but in this case I have a really bad theory material and I would greatly appreciate a concrete example. Given the following sample: S = {40, 80, 40, 60, 0, 40, 20, 40, 60, 100} Calculate an estimator of parameter μ of the underlying Poisson variable Calculate an estimator of parameter λ of the underlying negative exponential variable Give a confidence interval with confidence level 0.9 of both parameters My attempts The underlying Poisson distribution must be coherent with the sample (i.e. the sample average must be equal to the Poisson distribution expected value). $μ=\frac{1}{|S|}\sum_{x\in S}x=\frac{40+80+40+60+0+40+20+40+60+100}{10}=48$ Given a random variable $X \sim Exponential(λ)$, we have $μ=E[X]=\frac{1}{λ}$, from which we get $λ=\frac{1}{μ}=\frac{1}{48}$ ???","Can someone explain me how to solve the following exercise?  I don't like to post this kind of question, but in this case I have a really bad theory material and I would greatly appreciate a concrete example. Given the following sample: S = {40, 80, 40, 60, 0, 40, 20, 40, 60, 100} Calculate an estimator of parameter μ of the underlying Poisson variable Calculate an estimator of parameter λ of the underlying negative exponential variable Give a confidence interval with confidence level 0.9 of both parameters My attempts The underlying Poisson distribution must be coherent with the sample (i.e. the sample average must be equal to the Poisson distribution expected value). $μ=\frac{1}{|S|}\sum_{x\in S}x=\frac{40+80+40+60+0+40+20+40+60+100}{10}=48$ Given a random variable $X \sim Exponential(λ)$, we have $μ=E[X]=\frac{1}{λ}$, from which we get $λ=\frac{1}{μ}=\frac{1}{48}$ ???",,"['probability', 'statistics', 'parameter-estimation']"
55,Showing Hat matrix equal specific values,Showing Hat matrix equal specific values,,"Consider a one way layout model $y_{ij}$ = $\mu_i + e_{ij}$ (1 $\leq$ i $\leq$ a, 1 $\leq$ j $\leq$ $n_i$) where a = 3 and $n_1$ = 2, $n_2$ = 3, $n_3$ = 4. Show that the hat matrix for this design equals: $ H = \left| \begin{array}{ccc} 1/2 & 1/2 & 0 & 0 & 0 & 0 & 0 &  0 & 0 \\ 1/2 & 1/2 & 0 & 0 & 0 & 0 & 0 &  0 & 0 \\ 0 & 0 & 1/3 & 1/3 & 1/3 & 0 & 0 &  0 & 0 \\ 0 & 0 & 1/3 & 1/3 & 1/3 & 0 & 0 &  0 & 0 \\ 0 & 0 & 1/3 & 1/3 & 1/3 & 0 & 0 &  0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 1/4 & 1/4 &  1/4 & 1/4 \\ 0 & 0 & 0 & 0 & 0 & 1/4 & 1/4 &  1/4 & 1/4 \\ 0 & 0 & 0 & 0 & 0 & 1/4 & 1/4 &  1/4 & 1/4 \\ 0 & 0 & 0 & 0 & 0 & 1/4 & 1/4 &  1/4 & 1/4 \end{array} \right|. $ I am stuck as I am unable to find the inverse of X'X: $ X'X = \left| \begin{array}{ccc} 9 & 2 & 3 & 4  \\ 2 & 2 & 0 & 0  \\ 3 & 0 & 3 & 0\\ 4 & 0 & 0 & 4  \\ \end{array} \right|. $ I believe that I need to put some sort of constraint on the X'X matrix, but I'm not sure how to do this.","Consider a one way layout model $y_{ij}$ = $\mu_i + e_{ij}$ (1 $\leq$ i $\leq$ a, 1 $\leq$ j $\leq$ $n_i$) where a = 3 and $n_1$ = 2, $n_2$ = 3, $n_3$ = 4. Show that the hat matrix for this design equals: $ H = \left| \begin{array}{ccc} 1/2 & 1/2 & 0 & 0 & 0 & 0 & 0 &  0 & 0 \\ 1/2 & 1/2 & 0 & 0 & 0 & 0 & 0 &  0 & 0 \\ 0 & 0 & 1/3 & 1/3 & 1/3 & 0 & 0 &  0 & 0 \\ 0 & 0 & 1/3 & 1/3 & 1/3 & 0 & 0 &  0 & 0 \\ 0 & 0 & 1/3 & 1/3 & 1/3 & 0 & 0 &  0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 1/4 & 1/4 &  1/4 & 1/4 \\ 0 & 0 & 0 & 0 & 0 & 1/4 & 1/4 &  1/4 & 1/4 \\ 0 & 0 & 0 & 0 & 0 & 1/4 & 1/4 &  1/4 & 1/4 \\ 0 & 0 & 0 & 0 & 0 & 1/4 & 1/4 &  1/4 & 1/4 \end{array} \right|. $ I am stuck as I am unable to find the inverse of X'X: $ X'X = \left| \begin{array}{ccc} 9 & 2 & 3 & 4  \\ 2 & 2 & 0 & 0  \\ 3 & 0 & 3 & 0\\ 4 & 0 & 0 & 4  \\ \end{array} \right|. $ I believe that I need to put some sort of constraint on the X'X matrix, but I'm not sure how to do this.",,"['probability', 'matrices', 'statistics', 'regression']"
56,Why use regularization to reduce over-fitting,Why use regularization to reduce over-fitting,,"I'm having trouble understanding why should we use regularization for over-fitting when we can simply reduce the number of order to our polynomial function? Is it because it saves us time from having to come up with a polynomial function of lower order? For linear regression most of the work in figuring out a fit comes from figuring out our coefficients b0, b1, etc which we can simply find with a closed form equation(sometimes known as the normal equations). If we use regularization we have to come up with a lambda that makes sense. Please give me some example or insight on the benefits of using regularization.","I'm having trouble understanding why should we use regularization for over-fitting when we can simply reduce the number of order to our polynomial function? Is it because it saves us time from having to come up with a polynomial function of lower order? For linear regression most of the work in figuring out a fit comes from figuring out our coefficients b0, b1, etc which we can simply find with a closed form equation(sometimes known as the normal equations). If we use regularization we have to come up with a lambda that makes sense. Please give me some example or insight on the benefits of using regularization.",,"['statistics', 'polynomials', 'regression', 'machine-learning', 'regularization']"
57,Is it possible to determine the average and median if we only have the upper part of a bell curve?,Is it possible to determine the average and median if we only have the upper part of a bell curve?,,"Practical example: http://dota2.com/leaderboards shows the ~800 best solo ranked match-making scores in an Elo rating system . The lowest possible score is 1 . There were 9318362 players last month, but not all of them participate in ranked match-making. From http://dota2toplist.com/statistics we can infer normal distribution; the site shows a self-reported small subset of players who are generally more knowledgeable about the game, so that curve is biased towards the larger scores. Taking all of this into account, and assuming a participation quota of 5, 10, 15, … 100 percent of players in solo ranked match-making, can we find out what the unbiased global distribution looks like? How accurate would that model be?","Practical example: http://dota2.com/leaderboards shows the ~800 best solo ranked match-making scores in an Elo rating system . The lowest possible score is 1 . There were 9318362 players last month, but not all of them participate in ranked match-making. From http://dota2toplist.com/statistics we can infer normal distribution; the site shows a self-reported small subset of players who are generally more knowledgeable about the game, so that curve is biased towards the larger scores. Taking all of this into account, and assuming a participation quota of 5, 10, 15, … 100 percent of players in solo ranked match-making, can we find out what the unbiased global distribution looks like? How accurate would that model be?",,['statistics']
58,Sufficient statistic,Sufficient statistic,,"Let $\mathbf{X}=(X_1,\ldots,X_n)$ with joint frequency function $f(\mathbf{x};\theta_1,\theta_2)$ where $\theta_1,\theta_2$ vary independently. The set $S=\{\mathbf{x}:f(\mathbf{x};\theta_1,\theta_2)>0\}$ doesn't depend on $\theta_1,\theta_2$. Suppose $T_1$ is sufficient for $\theta_1$ when $\theta_2$ is known, and $T_2$ is sufficient for $\theta_2$ when $\theta_1$ is known. I need to show that $(T_1,T_2)$ is sufficient for $(\theta_1,\theta_2)$, if $T_1$ doesn't depend on $\theta_2$ and $T_2$ doesn't depend on $\theta_1$. So, I'm having some difficulty in interpreting the problem and expressing it with mathematical expressions. I was thinking of using the Factorization Criterion. «Suppose $T_1$ is sufficient for $\theta_1$ when $\theta_2$ is known.» This sentence I'm writing it as : With $\theta_2$ known, $f(\mathbf{x};\theta_1,\theta_2)=g_1(T_1(x),\theta_1)h_1(x)$ Similarly, for the other sentence: $f(\mathbf{x};\theta_1,\theta_2)=g_2(T_2(x),\theta_2)h_2(x)$ and so, $f(\mathbf{x};\theta_1,\theta_2)=\left(g_2(T_2(x),\theta_2)\cdot g_1(T_1(x),\theta_1)\right)^{1/2}(h_1(x)\cdot h_2(x))^{1/2 }$ However, this doesn't seem to be a correct resolution, since I don't seem to be using the information given about $S$, at least explicitly... Any help would be appreciated.","Let $\mathbf{X}=(X_1,\ldots,X_n)$ with joint frequency function $f(\mathbf{x};\theta_1,\theta_2)$ where $\theta_1,\theta_2$ vary independently. The set $S=\{\mathbf{x}:f(\mathbf{x};\theta_1,\theta_2)>0\}$ doesn't depend on $\theta_1,\theta_2$. Suppose $T_1$ is sufficient for $\theta_1$ when $\theta_2$ is known, and $T_2$ is sufficient for $\theta_2$ when $\theta_1$ is known. I need to show that $(T_1,T_2)$ is sufficient for $(\theta_1,\theta_2)$, if $T_1$ doesn't depend on $\theta_2$ and $T_2$ doesn't depend on $\theta_1$. So, I'm having some difficulty in interpreting the problem and expressing it with mathematical expressions. I was thinking of using the Factorization Criterion. «Suppose $T_1$ is sufficient for $\theta_1$ when $\theta_2$ is known.» This sentence I'm writing it as : With $\theta_2$ known, $f(\mathbf{x};\theta_1,\theta_2)=g_1(T_1(x),\theta_1)h_1(x)$ Similarly, for the other sentence: $f(\mathbf{x};\theta_1,\theta_2)=g_2(T_2(x),\theta_2)h_2(x)$ and so, $f(\mathbf{x};\theta_1,\theta_2)=\left(g_2(T_2(x),\theta_2)\cdot g_1(T_1(x),\theta_1)\right)^{1/2}(h_1(x)\cdot h_2(x))^{1/2 }$ However, this doesn't seem to be a correct resolution, since I don't seem to be using the information given about $S$, at least explicitly... Any help would be appreciated.",,"['probability', 'statistics', 'estimation']"
59,"How to scale ""probabilities"" to a given mean?","How to scale ""probabilities"" to a given mean?",,"I have a set of scores $x_i$, $i=1,\ldots,N$ (mimicking probabilities, $0\le x_i\le 1$) and I want to transform them so that the result has a given mean $m$, while remaining in the interval $[0;1]$. IOW, I am looking for a reasonable (in particular, smooth and strictly increasing) function $f(x): [0;1]\mapsto [0;1]$ such that $f(0)=0$, $f(1)=1$. One transformation which makes sense to me is this: convert probabilities to odds: $o_i=\frac{1-x_i}{x_i}$ (new range is $[0;\infty]$) multiply the odds by a positive factor $\lambda$ convert the scaled odds back to probabilities: $y_i=\frac{1}{1+\lambda o_i}$ The question is : how do I find $\lambda$? Here is a recursive R function, which scales the odds by the ratio of the desired mean over the actual mean: set.mean <- function (p, m, eps=sqrt(.Machine$double.eps)) { # make mean of p equal to m   m1 <- mean(p)   if (abs(m-m1) < eps)     return(p)   set.mean(1/(1 + (m1/m) * (1/p - 1)), m, eps) } It converges too slowly. I guess I can use the Newton's method for faster conversion, but I wonder if there is some brilliant trick which would solve my problem. PS. reasonable in this case means just that: I should be able to reason that it makes sense to transform probabilities this way :-)","I have a set of scores $x_i$, $i=1,\ldots,N$ (mimicking probabilities, $0\le x_i\le 1$) and I want to transform them so that the result has a given mean $m$, while remaining in the interval $[0;1]$. IOW, I am looking for a reasonable (in particular, smooth and strictly increasing) function $f(x): [0;1]\mapsto [0;1]$ such that $f(0)=0$, $f(1)=1$. One transformation which makes sense to me is this: convert probabilities to odds: $o_i=\frac{1-x_i}{x_i}$ (new range is $[0;\infty]$) multiply the odds by a positive factor $\lambda$ convert the scaled odds back to probabilities: $y_i=\frac{1}{1+\lambda o_i}$ The question is : how do I find $\lambda$? Here is a recursive R function, which scales the odds by the ratio of the desired mean over the actual mean: set.mean <- function (p, m, eps=sqrt(.Machine$double.eps)) { # make mean of p equal to m   m1 <- mean(p)   if (abs(m-m1) < eps)     return(p)   set.mean(1/(1 + (m1/m) * (1/p - 1)), m, eps) } It converges too slowly. I guess I can use the Newton's method for faster conversion, but I wonder if there is some brilliant trick which would solve my problem. PS. reasonable in this case means just that: I should be able to reason that it makes sense to transform probabilities this way :-)",,"['probability', 'statistics', 'numerical-methods']"
60,"What is the ""Logistic Regression""? I cannot have a unified concept.","What is the ""Logistic Regression""? I cannot have a unified concept.",,"I have a question about logistic regression. Recently, I interested in logistic regression for modeling some classification problem. So I tried to study logistic regression with two books, ""Discrete Choice Methods with Simulation"" (Train) and ""Applied Logistic Regression"" (Hosmer and Lmeshow). To clarify my question, I picked two logit model examples from those books. Household's choice between a gas and an electric heating system (binary logit) Let the subscripts $g$ and $e$ denote gas and electric, $PP$ and $OC$ are the purchase price and operating cost, $\beta_1$ and $\beta_2$ are scalar parameters. Then the probability that the household choose gas heating is $$P_g = \frac{e^{\beta_1PP_g+\beta_2OC_g}}{e^{\beta_1PP_g+\beta_2OC_g}+e^{\beta_1PP_e+\beta_2OC_e}}$$ Relationship between age and the presence or absence of CHD (binary logit) Let $x$ be the age, the subscripts $0$ and $1$ denote absence and presence of the CHD and $\beta_1^0$, $\beta_2^0$, $\beta_1^1$ and $\beta_2^1$ are scalar parameters. Then the probability that a $x$ years old person has a CHD is $$P_1 = \frac{e^{\beta_1^1+\beta_2^1x}}{e^{\beta_1^1+\beta_2^1x}+e^{\beta_1^0+\beta_2^0x}}$$ (Although the model needs two scalar parameters to be estimated, I wrote the formula in this form to clarify the question) I am familiar with the logistic regression showed in the second example. The indepedent variable (age in this example) included in both choices (or classes), and of course, the value of independent variable is same for different classes. And there are coefficients for each class and each independent variable which has to be estimated, and those coefficients are different for different classes. However, the model of the first example has variables which are different for each choice ($PP_e$, $PP_g$, $OC_e$ and $OC_g$), and the coefficients are same between choices. Roughly, I can understand two models separately, but I cannot build the unified concept of the logistic regression. They seems like different for me. For example, many of textbook and materials provide the (multinomial) logistic regression formula, which contain independent variables and the coefficients (which are different for each class). With this formula, I can understand the second example. But can this formula incorporate the situation showed in the first example? In other words, does this formula have the variable which are different for each class? Please help me to understand logistic regression. Thank you for reading my question and I am sorry about my bad English.","I have a question about logistic regression. Recently, I interested in logistic regression for modeling some classification problem. So I tried to study logistic regression with two books, ""Discrete Choice Methods with Simulation"" (Train) and ""Applied Logistic Regression"" (Hosmer and Lmeshow). To clarify my question, I picked two logit model examples from those books. Household's choice between a gas and an electric heating system (binary logit) Let the subscripts $g$ and $e$ denote gas and electric, $PP$ and $OC$ are the purchase price and operating cost, $\beta_1$ and $\beta_2$ are scalar parameters. Then the probability that the household choose gas heating is $$P_g = \frac{e^{\beta_1PP_g+\beta_2OC_g}}{e^{\beta_1PP_g+\beta_2OC_g}+e^{\beta_1PP_e+\beta_2OC_e}}$$ Relationship between age and the presence or absence of CHD (binary logit) Let $x$ be the age, the subscripts $0$ and $1$ denote absence and presence of the CHD and $\beta_1^0$, $\beta_2^0$, $\beta_1^1$ and $\beta_2^1$ are scalar parameters. Then the probability that a $x$ years old person has a CHD is $$P_1 = \frac{e^{\beta_1^1+\beta_2^1x}}{e^{\beta_1^1+\beta_2^1x}+e^{\beta_1^0+\beta_2^0x}}$$ (Although the model needs two scalar parameters to be estimated, I wrote the formula in this form to clarify the question) I am familiar with the logistic regression showed in the second example. The indepedent variable (age in this example) included in both choices (or classes), and of course, the value of independent variable is same for different classes. And there are coefficients for each class and each independent variable which has to be estimated, and those coefficients are different for different classes. However, the model of the first example has variables which are different for each choice ($PP_e$, $PP_g$, $OC_e$ and $OC_g$), and the coefficients are same between choices. Roughly, I can understand two models separately, but I cannot build the unified concept of the logistic regression. They seems like different for me. For example, many of textbook and materials provide the (multinomial) logistic regression formula, which contain independent variables and the coefficients (which are different for each class). With this formula, I can understand the second example. But can this formula incorporate the situation showed in the first example? In other words, does this formula have the variable which are different for each class? Please help me to understand logistic regression. Thank you for reading my question and I am sorry about my bad English.",,['statistics']
61,"Method of moments for Beta $(\alpha_1,\alpha_2)$ distribution",Method of moments for Beta  distribution,"(\alpha_1,\alpha_2)","I am trying to solve for the first two moments of a Beta$(\alpha_1,\alpha_2)$ distribution. We know that the first moment is equal to: $\mu_1 = \frac{\alpha_1}{\alpha_1+\alpha_2}$ and the second moment is equal to: $\mu_2 = \frac{\alpha_1(\alpha_1 +1)}{(\alpha_1+\alpha_2)(\alpha_1+\alpha_2+1)}$ The solutions for $\alpha_1$ and $\alpha_2$ are: $\alpha_1 = \frac{\mu_1-\mu_2}{\mu_2-1}$ , $\hspace{10mm}\mu_2 = \frac{\mu_1-\mu_2}{\mu_2-1}\frac{1-\mu_1}{\mu_1}$ I have attempted the algebra several times and I am quite close to the solution, but unfortunately I'm still off.","I am trying to solve for the first two moments of a Beta$(\alpha_1,\alpha_2)$ distribution. We know that the first moment is equal to: $\mu_1 = \frac{\alpha_1}{\alpha_1+\alpha_2}$ and the second moment is equal to: $\mu_2 = \frac{\alpha_1(\alpha_1 +1)}{(\alpha_1+\alpha_2)(\alpha_1+\alpha_2+1)}$ The solutions for $\alpha_1$ and $\alpha_2$ are: $\alpha_1 = \frac{\mu_1-\mu_2}{\mu_2-1}$ , $\hspace{10mm}\mu_2 = \frac{\mu_1-\mu_2}{\mu_2-1}\frac{1-\mu_1}{\mu_1}$ I have attempted the algebra several times and I am quite close to the solution, but unfortunately I'm still off.",,"['probability', 'statistics', 'probability-theory', 'probability-distributions', 'statistical-inference']"
62,measure of dependence for copula,measure of dependence for copula,,"I have some question about the paper of Schweizer and Wolff (1981). The question concerns about the following bound $$\int_0^1\int_0^1|C(u,v)-uv|\,du\,dv\leq\frac{1}{12}$$ where $C$ is any copula. I'm not quite sure whether it's exactly 1/12 since my attempt got 7/12. Hence, my question is how can it be 1/12. Thanks in advanced for any response.","I have some question about the paper of Schweizer and Wolff (1981). The question concerns about the following bound $$\int_0^1\int_0^1|C(u,v)-uv|\,du\,dv\leq\frac{1}{12}$$ where $C$ is any copula. I'm not quite sure whether it's exactly 1/12 since my attempt got 7/12. Hence, my question is how can it be 1/12. Thanks in advanced for any response.",,"['probability', 'statistics', 'probability-theory']"
63,Statistically determine whether a set of variables are diverging,Statistically determine whether a set of variables are diverging,,"I'm collecting data on a battery which is being discharged and charged again repeatedly. This battery consists of 4 cells in series and I am recording the fully-charged voltage of each cell at the end of every 10 cycles. Firstly, we know they are not initially at the exact same voltage (though close). Secondly, there is some uncertainty in what the fully-charged voltage will be due to equipment accuracy. Example: +-----------------------------------+ | Cycle Cell0  Cell1  Cell2  Cell3  | +-----------------------------------+ | 0     4.149  4.1745 4.1715 4.1475 | | 10    4.1205 4.1565 4.158  4.1325 | | 20    4.155  4.179  4.191  4.1505 | +-----------------------------------+ I'm thinking about how to prove or disprove (statistically) this hypothesis: The voltage difference between cells is increasing with respect to   the original difference (ie at cycle 0). I know that standard deviation will give me an idea of ""how spread out"" the cells are for a given cycle, and I can compute that for every cycle, and then fit a line to the list of standard deviations. But does that even make sense? I'm just thinking, if (for instance) the resulting slope is tiny, how can I be confident that the result is significant (ie not just a product of noise)?","I'm collecting data on a battery which is being discharged and charged again repeatedly. This battery consists of 4 cells in series and I am recording the fully-charged voltage of each cell at the end of every 10 cycles. Firstly, we know they are not initially at the exact same voltage (though close). Secondly, there is some uncertainty in what the fully-charged voltage will be due to equipment accuracy. Example: +-----------------------------------+ | Cycle Cell0  Cell1  Cell2  Cell3  | +-----------------------------------+ | 0     4.149  4.1745 4.1715 4.1475 | | 10    4.1205 4.1565 4.158  4.1325 | | 20    4.155  4.179  4.191  4.1505 | +-----------------------------------+ I'm thinking about how to prove or disprove (statistically) this hypothesis: The voltage difference between cells is increasing with respect to   the original difference (ie at cycle 0). I know that standard deviation will give me an idea of ""how spread out"" the cells are for a given cycle, and I can compute that for every cycle, and then fit a line to the list of standard deviations. But does that even make sense? I'm just thinking, if (for instance) the resulting slope is tiny, how can I be confident that the result is significant (ie not just a product of noise)?",,"['statistics', 'hypothesis-testing']"
64,UMVUE using complete and sufficient statistic [closed],UMVUE using complete and sufficient statistic [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 5 years ago . Improve this question Let $X_1,X_2,...,X_n$ be a random sample from a normal distribution with mean $\mu$ and variance $\sigma^2$. I showed that $(\bar X,S^2)$ is jointly sufficient for estimating ($\mu$,$\sigma^2$) where $\bar X$ is the sample mean and $S^2$ is the sample variance. Then assuming that$(\bar X,S^2)$ is also complete I have to show that $$\sqrt{ n-1\over 2}{\Gamma ({ n-1\over 2})\over\Gamma (\frac n2)} S$$ is a Uniformly Minimum Variance Unbiased Estimator for $\sigma$. I think I have to use Lehman Scheffe theorem as $(\bar X,S^2)$ is jointly sufficient and complete for $\sigma$. But how can I find a function which is unbiased for  $\sigma$ that contains both $(\bar X,S^2)$. I don't understand how to work when there's  a joint sufficiency and completeness.","Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 5 years ago . Improve this question Let $X_1,X_2,...,X_n$ be a random sample from a normal distribution with mean $\mu$ and variance $\sigma^2$. I showed that $(\bar X,S^2)$ is jointly sufficient for estimating ($\mu$,$\sigma^2$) where $\bar X$ is the sample mean and $S^2$ is the sample variance. Then assuming that$(\bar X,S^2)$ is also complete I have to show that $$\sqrt{ n-1\over 2}{\Gamma ({ n-1\over 2})\over\Gamma (\frac n2)} S$$ is a Uniformly Minimum Variance Unbiased Estimator for $\sigma$. I think I have to use Lehman Scheffe theorem as $(\bar X,S^2)$ is jointly sufficient and complete for $\sigma$. But how can I find a function which is unbiased for  $\sigma$ that contains both $(\bar X,S^2)$. I don't understand how to work when there's  a joint sufficiency and completeness.",,"['statistics', 'statistical-inference', 'estimation']"
65,Does the median tend to the lower limit of the median class?,Does the median tend to the lower limit of the median class?,,"The median for a continous distribution is given by $M= L +\dfrac{(N/2-C)\cdot I}{f}$ where $M$ is median, $L$ is lower limit of median class, $N$ is the total frequency, $C$  is the cumulative frequency of class BELOW the median class, $F$ is frequency of median class, $I$ is class width. The way I see it $N/2$ is close to the medial value. $(N/2-C)$ is hence the frequency of the values in the medial class lesser than the median. You then express that as a fraction of the total class size and add it to the lower limit. So, if there is a higher percentage of values in the $N/2$ to $C$ range, the median should tend to the lower limit of the medial, class, right? But then, in this case, $(N/2-C)$ is high, so the median will tend to the upper limit, which is not right. Where is the mistake? Thanks in advance","The median for a continous distribution is given by $M= L +\dfrac{(N/2-C)\cdot I}{f}$ where $M$ is median, $L$ is lower limit of median class, $N$ is the total frequency, $C$  is the cumulative frequency of class BELOW the median class, $F$ is frequency of median class, $I$ is class width. The way I see it $N/2$ is close to the medial value. $(N/2-C)$ is hence the frequency of the values in the medial class lesser than the median. You then express that as a fraction of the total class size and add it to the lower limit. So, if there is a higher percentage of values in the $N/2$ to $C$ range, the median should tend to the lower limit of the medial, class, right? But then, in this case, $(N/2-C)$ is high, so the median will tend to the upper limit, which is not right. Where is the mistake? Thanks in advance",,"['statistics', 'median']"
66,feature selection for continuous variables,feature selection for continuous variables,,"I wonder how exactly ""feature selection"" should be performed in case of continuous feature values. When feature values are discrete it is very straitforward to apply feature selection, but what to do when you have term-document matrix with (tf*idf|tf|idf) as feature values for text classification task. I don't think that it's correct to take ~20% highest tf values, because it's biased towards features that appear in long documents. In short, what's simple way to evaluate feature selection for continuous feature values.","I wonder how exactly ""feature selection"" should be performed in case of continuous feature values. When feature values are discrete it is very straitforward to apply feature selection, but what to do when you have term-document matrix with (tf*idf|tf|idf) as feature values for text classification task. I don't think that it's correct to take ~20% highest tf values, because it's biased towards features that appear in long documents. In short, what's simple way to evaluate feature selection for continuous feature values.",,"['statistics', 'machine-learning', 'artificial-intelligence']"
67,Probability Statistics Question,Probability Statistics Question,,"I have this formula for determining $x$ and $y$'s effect on $$a\mapsto\frac{(xy/z)}{(xy/z)+ (1-x)(1-y)/(1-z)}$$ If this formula assumes x and y have equal affect on a (say 50% each), how would i modify this formula to reflect if x had 60% effect and y had 40% effect on a? I 'm not sure how to handle the denominator bc there is a (1-x)*(1-y)/(1-z) term. I had initially assumed if x and y are equally weighted then that would mean 50% for each of x and y. So, if x was 60% and y was 40% weight, than I'd have 60%/50% and 40%/50% as weights for x and y. In other words, I thought the formula might be a = (1.2x*.8y)/{(1.2x*.8y/z)+1.2(1-x).8(1-y)/(1-z)} but I wasn't sure if that was right. Or, should I rewrite the formula as such: a = (1.2x*.8y)/{(1.2x*.8y/z)+(1-1.2x)(1-.8y)/(1-z)}","I have this formula for determining $x$ and $y$'s effect on $$a\mapsto\frac{(xy/z)}{(xy/z)+ (1-x)(1-y)/(1-z)}$$ If this formula assumes x and y have equal affect on a (say 50% each), how would i modify this formula to reflect if x had 60% effect and y had 40% effect on a? I 'm not sure how to handle the denominator bc there is a (1-x)*(1-y)/(1-z) term. I had initially assumed if x and y are equally weighted then that would mean 50% for each of x and y. So, if x was 60% and y was 40% weight, than I'd have 60%/50% and 40%/50% as weights for x and y. In other words, I thought the formula might be a = (1.2x*.8y)/{(1.2x*.8y/z)+1.2(1-x).8(1-y)/(1-z)} but I wasn't sure if that was right. Or, should I rewrite the formula as such: a = (1.2x*.8y)/{(1.2x*.8y/z)+(1-1.2x)(1-.8y)/(1-z)}",,"['probability', 'statistics']"
68,Let X be an exponential random variable with P(X < 1/3) = 0.75. What is E(X)?,Let X be an exponential random variable with P(X < 1/3) = 0.75. What is E(X)?,,Let X be an exponential random variable with P(X < 1/3) = 0.75. What is E(X)? I don't get this. Please help.,Let X be an exponential random variable with P(X < 1/3) = 0.75. What is E(X)? I don't get this. Please help.,,"['probability', 'statistics', 'random-variables']"
69,Probabilities of this blackjack hybrid,Probabilities of this blackjack hybrid,,"I am currently trying to study a hybrid / simplified version of Black Jack but as i am not as good with probabilities i am hoping that i could receive some advice about the probabilities behind the game. The rules of the game: The three face cards are all worth 10 points, just like the real BlackJack. The Ace card has a behavior where it gets the value of 11/1. To understand the logic in this hybrid, simply assume that Ace has a value of 11 however if the total goes over 21, minus 10 from the total. All cards are chosen randomly by a computer and hence the same cards can be repeated and the probability of selection of a particular card is independent of the probability of the cards that came before it. All other cards have values 1-9, same as their real number. However the gameplay is modified and simplified The player starts the game and gets two random cards. If the total of these cards adds up to 21 (I.e one ace and one face card) the player immediately wins. If the total is not 21, the player is free to draw as many cards as possible but if the total goes over 21, he loses. Once he's satisfied with the total, he 'stands'. Then the dealer tries to draw cards and beat the player's total without going over 21. What i am wondering is what is the probability that the player wins the game? Also what would be the best number to stop drawing further cards as the player?","I am currently trying to study a hybrid / simplified version of Black Jack but as i am not as good with probabilities i am hoping that i could receive some advice about the probabilities behind the game. The rules of the game: The three face cards are all worth 10 points, just like the real BlackJack. The Ace card has a behavior where it gets the value of 11/1. To understand the logic in this hybrid, simply assume that Ace has a value of 11 however if the total goes over 21, minus 10 from the total. All cards are chosen randomly by a computer and hence the same cards can be repeated and the probability of selection of a particular card is independent of the probability of the cards that came before it. All other cards have values 1-9, same as their real number. However the gameplay is modified and simplified The player starts the game and gets two random cards. If the total of these cards adds up to 21 (I.e one ace and one face card) the player immediately wins. If the total is not 21, the player is free to draw as many cards as possible but if the total goes over 21, he loses. Once he's satisfied with the total, he 'stands'. Then the dealer tries to draw cards and beat the player's total without going over 21. What i am wondering is what is the probability that the player wins the game? Also what would be the best number to stop drawing further cards as the player?",,"['probability', 'statistics']"
70,Concentration inequality of weighted sum of random variables given a tail inequality,Concentration inequality of weighted sum of random variables given a tail inequality,,"I tried to solve the exercise below which can be seen as a generalization of the Bernstein's concentration inequality. However, I have difficulty bounding the moment generating function of $Z$ (see $(2)$ below) with appropriate terms that yield the desired tail bound following the Chernoff method. Let $X_1,X_2,\ldots ,X_n$ be iid copies of a real random variable $X$   that for some $p\geq 1$ obeys \begin{align} \mathbb{P}\left(\left\vert X\right\vert > u\right)&  \leq \exp\left(-u^p\right),\tag{1} \end{align} for all $u>0$. For any $s\in \mathbb{R}^n$ and with   $q$ denoting the conjugate of $p$ (i.e., $1/p+1/q=1$) prove that \begin{align} \mathbb{P}\left(Z:=\sum _{i=1}^n s_i X_i > t \right)&\le L  \exp\left(-\frac{1}{L}\min \left(\frac{t^2}{\left\Vert  s\right\Vert_2^2},\frac{t^p}{\left\Vert  s\right\Vert_q^p}\right)\right),\tag{2} \end{align}    where $L>0$ is a   constant that only depends on $p$, but not $n$. I have shown that it suffices to establish \begin{align*} \log\mathbb{E}\left[e^{\lambda Z}\right] & \leq \log L + \max\left(\frac{L\left\Vert s\right\Vert_2 ^2\lambda ^2}{4} ,\frac{(p-1)L^{q-1}\left\Vert s\right\Vert_q^q\lambda^q}{p^q}\right). \end{align*} I also know that \begin{align*} \mathbb{E}\left[e^{\lambda X}\right]& \leq \mathbb{E}\left[e^{\lambda \left\vert X\right\vert}\right]\\ & = 1 + \lambda \int_0^\infty \mathbb{P}\left(\left\vert X\right\vert > t\right)e^{\lambda t}\mathrm{d}t\\ &\leq 1 + \lambda \int_0^\infty e^{\lambda t - t^p}\mathrm{d}t  \end{align*} However, I have trouble (i) simplifying this bound and (ii) use it obtain the desired bound on the cumulant generating function of $Z$ mentioned above which should be independent of $n$. Update: Here's another incomplete attempt. Clearly we have $$\begin{align*} \mathbb{E}\left[e^{\lambda X}\right]&\leq\sum_{k\geq 0}\frac{\lambda^k}{k!}\mathbb{E}\left[|X|^k\right]. \end{align*}$$ Furthermore, using $(1)$ we can bound the moments as $$\begin{align*} \mathbb{E}\left[|X|^k\right]&=\int_{0}^\infty\mathbb{P}\left(|X|^k\geq u^k\right)du^k\\ &\leq \int_0^\infty e^{-u^p}du^k\\ &=\int_{0}^\infty  e^{-u}du^{k/p}=\Gamma\left(\frac{k}{p}+1\right),\tag{3} \end{align*}$$ where $\Gamma\left(\cdot\right)$ is the Gamma function. Using log-convexity of the Gamma function we have $\Gamma\left(\frac{k}{p}+1\right)\leq \left(\Gamma\left(k+1\right)\right)^{1/p}=\left(k!\right)^{1/p}$, and thus  $$\begin{align*} \mathbb{E}\left[|X|^k\right]&\leq\left(k!\right)^{1/p}. \end{align*}$$ Putting this bound back in $(3)$ we obtain  $$\begin{align*} \mathbb{E}\left[e^{\lambda X}\right]&\leq\sum_{k\geq 0} \frac{\lambda^k}{\left(k!\right)^{1/q}}. \end{align*}$$ If we can simplify the series in the latter inequality or find a good approximation for it then we might be able to follow the standard Chernoff bound and obtain the desired result. However, so far I haven't been able to simplify the series.","I tried to solve the exercise below which can be seen as a generalization of the Bernstein's concentration inequality. However, I have difficulty bounding the moment generating function of $Z$ (see $(2)$ below) with appropriate terms that yield the desired tail bound following the Chernoff method. Let $X_1,X_2,\ldots ,X_n$ be iid copies of a real random variable $X$   that for some $p\geq 1$ obeys \begin{align} \mathbb{P}\left(\left\vert X\right\vert > u\right)&  \leq \exp\left(-u^p\right),\tag{1} \end{align} for all $u>0$. For any $s\in \mathbb{R}^n$ and with   $q$ denoting the conjugate of $p$ (i.e., $1/p+1/q=1$) prove that \begin{align} \mathbb{P}\left(Z:=\sum _{i=1}^n s_i X_i > t \right)&\le L  \exp\left(-\frac{1}{L}\min \left(\frac{t^2}{\left\Vert  s\right\Vert_2^2},\frac{t^p}{\left\Vert  s\right\Vert_q^p}\right)\right),\tag{2} \end{align}    where $L>0$ is a   constant that only depends on $p$, but not $n$. I have shown that it suffices to establish \begin{align*} \log\mathbb{E}\left[e^{\lambda Z}\right] & \leq \log L + \max\left(\frac{L\left\Vert s\right\Vert_2 ^2\lambda ^2}{4} ,\frac{(p-1)L^{q-1}\left\Vert s\right\Vert_q^q\lambda^q}{p^q}\right). \end{align*} I also know that \begin{align*} \mathbb{E}\left[e^{\lambda X}\right]& \leq \mathbb{E}\left[e^{\lambda \left\vert X\right\vert}\right]\\ & = 1 + \lambda \int_0^\infty \mathbb{P}\left(\left\vert X\right\vert > t\right)e^{\lambda t}\mathrm{d}t\\ &\leq 1 + \lambda \int_0^\infty e^{\lambda t - t^p}\mathrm{d}t  \end{align*} However, I have trouble (i) simplifying this bound and (ii) use it obtain the desired bound on the cumulant generating function of $Z$ mentioned above which should be independent of $n$. Update: Here's another incomplete attempt. Clearly we have $$\begin{align*} \mathbb{E}\left[e^{\lambda X}\right]&\leq\sum_{k\geq 0}\frac{\lambda^k}{k!}\mathbb{E}\left[|X|^k\right]. \end{align*}$$ Furthermore, using $(1)$ we can bound the moments as $$\begin{align*} \mathbb{E}\left[|X|^k\right]&=\int_{0}^\infty\mathbb{P}\left(|X|^k\geq u^k\right)du^k\\ &\leq \int_0^\infty e^{-u^p}du^k\\ &=\int_{0}^\infty  e^{-u}du^{k/p}=\Gamma\left(\frac{k}{p}+1\right),\tag{3} \end{align*}$$ where $\Gamma\left(\cdot\right)$ is the Gamma function. Using log-convexity of the Gamma function we have $\Gamma\left(\frac{k}{p}+1\right)\leq \left(\Gamma\left(k+1\right)\right)^{1/p}=\left(k!\right)^{1/p}$, and thus  $$\begin{align*} \mathbb{E}\left[|X|^k\right]&\leq\left(k!\right)^{1/p}. \end{align*}$$ Putting this bound back in $(3)$ we obtain  $$\begin{align*} \mathbb{E}\left[e^{\lambda X}\right]&\leq\sum_{k\geq 0} \frac{\lambda^k}{\left(k!\right)^{1/q}}. \end{align*}$$ If we can simplify the series in the latter inequality or find a good approximation for it then we might be able to follow the standard Chernoff bound and obtain the desired result. However, so far I haven't been able to simplify the series.",,"['probability', 'statistics', 'probability-theory', 'distribution-tails', 'concentration-of-measure']"
71,Literature on Sabermetrics in baseball,Literature on Sabermetrics in baseball,,"For my bachelor's thesis, I would like to study the use of Sabermetrics in baseball. I was fascinated by the book 'Moneyball: The Art of Winning an Unfair Game' by Michael Lewis, and to me, it seemed a great opportunity to show the impact of the use of mathematics in modern culture. The use of statistics to scout players based on their statistical performances, rather than 'subjective' attributes, like hitting percentage and sprint speed, really struck me. Now, I have a problem: there is not a lot of literature available about this phenomenon, and from the available literature, most of it is fairly easy. It makes for a fun read, but it will not suffice as literature for my thesis. I have about two articles that might work, but I need more scientific literature to complete my thesis. I was looking for the use of stochastics in sabermetrics, but that did not yield a lot of results. Hope anyone can help me out.","For my bachelor's thesis, I would like to study the use of Sabermetrics in baseball. I was fascinated by the book 'Moneyball: The Art of Winning an Unfair Game' by Michael Lewis, and to me, it seemed a great opportunity to show the impact of the use of mathematics in modern culture. The use of statistics to scout players based on their statistical performances, rather than 'subjective' attributes, like hitting percentage and sprint speed, really struck me. Now, I have a problem: there is not a lot of literature available about this phenomenon, and from the available literature, most of it is fairly easy. It makes for a fun read, but it will not suffice as literature for my thesis. I have about two articles that might work, but I need more scientific literature to complete my thesis. I was looking for the use of stochastics in sabermetrics, but that did not yield a lot of results. Hope anyone can help me out.",,"['statistics', 'reference-request', 'soft-question', 'stochastic-processes']"
72,Is it compulsory to make transformation to the econometric model in order to have only diagonal elements on variance-covariance matrix of errors?,Is it compulsory to make transformation to the econometric model in order to have only diagonal elements on variance-covariance matrix of errors?,,"I need some sharped and advanced advices for the following issue ... Model and its assumptions I'm working on the methodology of a two-way error component model. Here is the model: $y_{jis} = x_{jis} \beta + \upsilon_{jis}$. $j$ refers to school, $i$ refers to individual and $s$ to tested area/topic (mathematics or english, ...). $J$ is the number of schools and $S$ is the number of tested fields. $N$ is the number of students among all schools. $\upsilon_{jis}$ is the error term and can be decomposed as follows: $\upsilon_{jis} = \theta_{j} + \phi_{js} + \epsilon_{jis}$ $\theta_{j}$ is the random effect for students attenting a school $j$, $\phi_{js}$ is the random effect for students attenting a school $j$ for a topic $s$ and $\epsilon_{jis}$ is the traditionnal idiosyncratic error. In multivariate form it gives: $$Y = X \beta + \upsilon$$  where $\upsilon = R \theta + F \phi + \epsilon $ $R$ and $F$ are matrices that enable to correclty distribute their respective random effect. R: size $= NS \times J$.   F: size $= NS \times JS$ $\theta$ follows a multivariate normal with mean $0$ and a variance covariance matrix $= \tau I_{J}$. $\tau$ is a scalar and $I_{J}$ is the identity matrix (size: $J \times J$). $\phi$ follows a multivariate normal with mean $0$ and a variance covariance matrix $= \gamma I_{JS}$. $\gamma$ is a scalar and $I_{JS}$ is the identity matrix (size: $J S \times J S$). $\epsilon$ follows a multivariate normal with mean $0$ and a variance covariance matrix $= \sigma I_{NS}$. $\sigma$ is a scalar and $I_{JS}$ is the identity matrix (size: $N S \times N S$). Estimation of unknown parameters I have to make estimation of the different variances: $\tau$, $\gamma$ and $\sigma$ before running FGLS method.  I work step by step to delete the different random effect. First I made a within regression to get rid of $\phi$ effect. Since $\phi$ is an effect for a given school, that within operator ($W$) will automatically make the $\theta$ effect vanishing. $\epsilon$ is the only survivor and I can use the methodology in Hayashi to calculate the expectation of $( (We)' We | X) = . ..$ in order to correct with the right degree of freedom. It is easy since the variance of that element is only diagonal, it will then lead to calculate the trace .... Now, I only want to make the term theta disappear with another within operator ($W_{2}$). By construction, it deletes $\theta$, so there are $\phi$ and $\epsilon$ left. Once again I try to perform the expectation of $ ( (W_{2}e)' W_{2}e | X)$ where $e$ is composed now of $\phi$ and $\epsilon$. The variance of that vector is not diagonal, thus we don't have a beautiful formula for the estimated variance of $\phi$ ... I carefully follow the good methodology but I don't have a good-looking formula. Is it compulsory to make transformation to the model in order to have only diagonal elements on variance-covariance matrix of errors? I can also provide more details about the development of Hayashi that I reuse ... Thanks in advance.","I need some sharped and advanced advices for the following issue ... Model and its assumptions I'm working on the methodology of a two-way error component model. Here is the model: $y_{jis} = x_{jis} \beta + \upsilon_{jis}$. $j$ refers to school, $i$ refers to individual and $s$ to tested area/topic (mathematics or english, ...). $J$ is the number of schools and $S$ is the number of tested fields. $N$ is the number of students among all schools. $\upsilon_{jis}$ is the error term and can be decomposed as follows: $\upsilon_{jis} = \theta_{j} + \phi_{js} + \epsilon_{jis}$ $\theta_{j}$ is the random effect for students attenting a school $j$, $\phi_{js}$ is the random effect for students attenting a school $j$ for a topic $s$ and $\epsilon_{jis}$ is the traditionnal idiosyncratic error. In multivariate form it gives: $$Y = X \beta + \upsilon$$  where $\upsilon = R \theta + F \phi + \epsilon $ $R$ and $F$ are matrices that enable to correclty distribute their respective random effect. R: size $= NS \times J$.   F: size $= NS \times JS$ $\theta$ follows a multivariate normal with mean $0$ and a variance covariance matrix $= \tau I_{J}$. $\tau$ is a scalar and $I_{J}$ is the identity matrix (size: $J \times J$). $\phi$ follows a multivariate normal with mean $0$ and a variance covariance matrix $= \gamma I_{JS}$. $\gamma$ is a scalar and $I_{JS}$ is the identity matrix (size: $J S \times J S$). $\epsilon$ follows a multivariate normal with mean $0$ and a variance covariance matrix $= \sigma I_{NS}$. $\sigma$ is a scalar and $I_{JS}$ is the identity matrix (size: $N S \times N S$). Estimation of unknown parameters I have to make estimation of the different variances: $\tau$, $\gamma$ and $\sigma$ before running FGLS method.  I work step by step to delete the different random effect. First I made a within regression to get rid of $\phi$ effect. Since $\phi$ is an effect for a given school, that within operator ($W$) will automatically make the $\theta$ effect vanishing. $\epsilon$ is the only survivor and I can use the methodology in Hayashi to calculate the expectation of $( (We)' We | X) = . ..$ in order to correct with the right degree of freedom. It is easy since the variance of that element is only diagonal, it will then lead to calculate the trace .... Now, I only want to make the term theta disappear with another within operator ($W_{2}$). By construction, it deletes $\theta$, so there are $\phi$ and $\epsilon$ left. Once again I try to perform the expectation of $ ( (W_{2}e)' W_{2}e | X)$ where $e$ is composed now of $\phi$ and $\epsilon$. The variance of that vector is not diagonal, thus we don't have a beautiful formula for the estimated variance of $\phi$ ... I carefully follow the good methodology but I don't have a good-looking formula. Is it compulsory to make transformation to the model in order to have only diagonal elements on variance-covariance matrix of errors? I can also provide more details about the development of Hayashi that I reuse ... Thanks in advance.",,"['statistics', 'regression', 'mathematical-modeling', 'estimation']"
73,A variant of Hoeffding's Inequallity,A variant of Hoeffding's Inequallity,,"I'm new to concentration inequalities and I have a question related to Hoeffding's inequality. Let $X_1 ~ \dots X_n$ be a set of i.i.d random variables, s.t.  $E[X_i] = \mu$, $Var[X_i] = \sigma^2$, and  $0 \leq X_i \leq c$. Let $\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$ be the sample mean. By Hoeffding's inequality we have that: \begin{eqnarray} \Pr \left[ (\bar{X}_n - E[\bar{X}_n]) \geq t \right]& \leq & \exp\left( - \frac{2nt^2}{c^2} \right), \end{eqnarray} for $t > 0$. What I'm looking for is not $\Pr[ (\bar{X}_n - E[\bar{X}_n]) \geq t ]$, but the $\Pr [ \bar{X}_n \geq \beta + t]$, where $\beta > 0$. Is there a different inequality for this probability? Can I say that if $\beta \geq E[\bar{X}_n]$, then let $\beta = \alpha + E[\bar{X}_n]$, for constant $\alpha > 0$, and then proceed with Hoeffding's inequality. Then proceed in a similar way if $\beta \leq E[\bar{X}_n]$. Thanks.","I'm new to concentration inequalities and I have a question related to Hoeffding's inequality. Let $X_1 ~ \dots X_n$ be a set of i.i.d random variables, s.t.  $E[X_i] = \mu$, $Var[X_i] = \sigma^2$, and  $0 \leq X_i \leq c$. Let $\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$ be the sample mean. By Hoeffding's inequality we have that: \begin{eqnarray} \Pr \left[ (\bar{X}_n - E[\bar{X}_n]) \geq t \right]& \leq & \exp\left( - \frac{2nt^2}{c^2} \right), \end{eqnarray} for $t > 0$. What I'm looking for is not $\Pr[ (\bar{X}_n - E[\bar{X}_n]) \geq t ]$, but the $\Pr [ \bar{X}_n \geq \beta + t]$, where $\beta > 0$. Is there a different inequality for this probability? Can I say that if $\beta \geq E[\bar{X}_n]$, then let $\beta = \alpha + E[\bar{X}_n]$, for constant $\alpha > 0$, and then proceed with Hoeffding's inequality. Then proceed in a similar way if $\beta \leq E[\bar{X}_n]$. Thanks.",,"['probability', 'statistics', 'statistical-inference', 'concentration-of-measure']"
74,paramter estimation (maximum likelihood) of a mixture density,paramter estimation (maximum likelihood) of a mixture density,,"I have this mixture distribution $f(x) =w \cdot \mathcal{LN}(\mu_1,\sigma) + (1-w)\cdot \mathcal{LN}(\mu_2,\sigma) $ where $\mathcal{LN}(\mu,\sigma)$ is a lognormal distribution. I now have random variables $ Y = \sum_{i=0}^n X_i $ where all $X_i$ are distributed according to $f(x)$. How can I get the maximum likelihood estimation for the unknown paramters $\mu_1, \mu_2$ and $\sigma$ (w is given)? We can also assume that $n$ is a small number, for example $n=10$. Are there other methods of finding the parameters?","I have this mixture distribution $f(x) =w \cdot \mathcal{LN}(\mu_1,\sigma) + (1-w)\cdot \mathcal{LN}(\mu_2,\sigma) $ where $\mathcal{LN}(\mu,\sigma)$ is a lognormal distribution. I now have random variables $ Y = \sum_{i=0}^n X_i $ where all $X_i$ are distributed according to $f(x)$. How can I get the maximum likelihood estimation for the unknown paramters $\mu_1, \mu_2$ and $\sigma$ (w is given)? We can also assume that $n$ is a small number, for example $n=10$. Are there other methods of finding the parameters?",,"['statistics', 'probability-theory', 'probability-distributions', 'parameter-estimation']"
75,Plot histogram and density function,Plot histogram and density function,,I need to plot a histogram for the data: 10.478 7.268 2.775 0.381 0.979 1.226 1.392 4.706 0.003 5.004 2.835 2.441 1.838 10.780 0.010 4.732 3.042 8.817 0.670 1.693 11.576 2.469 9.905 2.265 1.184 0.093 5.187 1.828 7.140 0.136 1.408 6.349 3.717 7.170 0.609 3.084 0.122 3.154 3.574 0.420 The data seem to be exponentially distributed. With maximum lokelihood I estimate $\lambda$ to $\lambda = \frac{1}{\bar x} = 0.2808$. Now I have to plot the density function in the same figure as histogram. But i have to scale the density function to let it look well. Here is the histogram with density function multiplied by $\sum_{i=1}^{40} x_i$ and without scaling: How do I scale it in the right way? Thanks in advance!,I need to plot a histogram for the data: 10.478 7.268 2.775 0.381 0.979 1.226 1.392 4.706 0.003 5.004 2.835 2.441 1.838 10.780 0.010 4.732 3.042 8.817 0.670 1.693 11.576 2.469 9.905 2.265 1.184 0.093 5.187 1.828 7.140 0.136 1.408 6.349 3.717 7.170 0.609 3.084 0.122 3.154 3.574 0.420 The data seem to be exponentially distributed. With maximum lokelihood I estimate $\lambda$ to $\lambda = \frac{1}{\bar x} = 0.2808$. Now I have to plot the density function in the same figure as histogram. But i have to scale the density function to let it look well. Here is the histogram with density function multiplied by $\sum_{i=1}^{40} x_i$ and without scaling: How do I scale it in the right way? Thanks in advance!,,"['statistics', 'graphing-functions', 'stochastic-calculus']"
76,Determining how well a curve fits data,Determining how well a curve fits data,,"What are some commonly used ways to examine how well a curve fits a given set of data?  I am aware of the R Squared test but I was wondering if there are other tests that take into account the appearance of the curves as well. To be more precise, I am trying to model the spread of diseases using various methods such as the classic differential equation SI model as well as others that take into account a bit of network structure. The latter models have a distinctly different appearance than the S shape of the SI model. Thanks.","What are some commonly used ways to examine how well a curve fits a given set of data?  I am aware of the R Squared test but I was wondering if there are other tests that take into account the appearance of the curves as well. To be more precise, I am trying to model the spread of diseases using various methods such as the classic differential equation SI model as well as others that take into account a bit of network structure. The latter models have a distinctly different appearance than the S shape of the SI model. Thanks.",,['statistics']
77,"Compute variance of ""tree"" random variable","Compute variance of ""tree"" random variable",,"Let A be a random variable defined as: With probability $p[i]$, the random variable $B[i]$ is drawn $B[i] ~ N[mu[i],sigma[i]]$ probabilities $p[i]$ sum up to one I know how to compute the mean, which is given by: $$E[A] = p[1]*mu[1] + .. +  p[N]*mu[N]$$ I would like to know how to compute the variance","Let A be a random variable defined as: With probability $p[i]$, the random variable $B[i]$ is drawn $B[i] ~ N[mu[i],sigma[i]]$ probabilities $p[i]$ sum up to one I know how to compute the mean, which is given by: $$E[A] = p[1]*mu[1] + .. +  p[N]*mu[N]$$ I would like to know how to compute the variance",,['statistics']
78,Dhar's Burning Test - Confusion about Abelian Sandpile Model,Dhar's Burning Test - Confusion about Abelian Sandpile Model,,"Dhar's Burning test is a bijection between the spanning trees of a certain graph and the recurrent states Abelian sandpile model .  I would like some help working out this bijection in different cases: Square graph One we pick a root there are 4 spanning trees: o-o  o-o  o-o  o o  o-o | |  |      |  | |    | x-o  x-o  x-o  x-p  x-o Also the recurrent states should be $\mathbb{Z}^3/V\mathbb{Z}^3$  where $V$ is the lattice $\mathbb{Z}(-2,1,0) + \mathbb{Z}(1,-2,1) + \mathbb{Z}(0,1,-2)$.  There should be 4 such states since: $$ \left| \begin{array}{ccc} -2 & 1 & 0 \\ 1 & -2 & 1 \\ 0 & 1 & -2 \end{array}\right| = 4$$ How do I write down explicit  representatives of $\mathbb{Z}^3/V\mathbb{Z}^3$ ?  And how do we exhibit the Dhar bijection between the sandpile states and spanning trees in this case? I got a list of stable configurations by drawing all marking the root with x and writing down all the possible configurations 1-0  1-1  0-1  1-1 | |  | |  | |  | | x-1  x-0  x-1  x-1 My question is how do we match these configuations with the spanning trees above?","Dhar's Burning test is a bijection between the spanning trees of a certain graph and the recurrent states Abelian sandpile model .  I would like some help working out this bijection in different cases: Square graph One we pick a root there are 4 spanning trees: o-o  o-o  o-o  o o  o-o | |  |      |  | |    | x-o  x-o  x-o  x-p  x-o Also the recurrent states should be $\mathbb{Z}^3/V\mathbb{Z}^3$  where $V$ is the lattice $\mathbb{Z}(-2,1,0) + \mathbb{Z}(1,-2,1) + \mathbb{Z}(0,1,-2)$.  There should be 4 such states since: $$ \left| \begin{array}{ccc} -2 & 1 & 0 \\ 1 & -2 & 1 \\ 0 & 1 & -2 \end{array}\right| = 4$$ How do I write down explicit  representatives of $\mathbb{Z}^3/V\mathbb{Z}^3$ ?  And how do we exhibit the Dhar bijection between the sandpile states and spanning trees in this case? I got a list of stable configurations by drawing all marking the root with x and writing down all the possible configurations 1-0  1-1  0-1  1-1 | |  | |  | |  | | x-1  x-0  x-1  x-1 My question is how do we match these configuations with the spanning trees above?",,"['abstract-algebra', 'statistics', 'graph-theory', 'physics']"
79,correlation estimator,correlation estimator,,Suppose I have independent variables $X$ and $Y$ which follows exponential distribution with parameter $\lambda$. I want to find the variance of correlation estimator $\hat{\rho}$ which is defined as: $$ \hat{\rho} = \frac{\sum_{i=1}^{n} x_i y_i}{\sqrt{\sum_{i=1}^{n}x_i^2 \sum_{i=1}^{n}y_i^2}} $$ Since it involves a division I couldn't even start calculating the variance. How can I do it?,Suppose I have independent variables $X$ and $Y$ which follows exponential distribution with parameter $\lambda$. I want to find the variance of correlation estimator $\hat{\rho}$ which is defined as: $$ \hat{\rho} = \frac{\sum_{i=1}^{n} x_i y_i}{\sqrt{\sum_{i=1}^{n}x_i^2 \sum_{i=1}^{n}y_i^2}} $$ Since it involves a division I couldn't even start calculating the variance. How can I do it?,,"['probability', 'statistics', 'correlation']"
80,Error propagation of the median absolute deviation.,Error propagation of the median absolute deviation.,,"I wonder if there is a way to estimate error propagation of the mad (=median absolute deviation) for the difference between two median distributions, similar to how you estimate error propagation for the standard deviation for the difference between two average distributions? I want to compare the difference between two average size distributions and the difference between two median size distributions for the same data set, and I would like to compare the uncertainties by putting the corresponding form of propagated errors as error bars on each plot. Is there some way to do this or some other way to represent the errors for the median difference distribution?","I wonder if there is a way to estimate error propagation of the mad (=median absolute deviation) for the difference between two median distributions, similar to how you estimate error propagation for the standard deviation for the difference between two average distributions? I want to compare the difference between two average size distributions and the difference between two median size distributions for the same data set, and I would like to compare the uncertainties by putting the corresponding form of propagated errors as error bars on each plot. Is there some way to do this or some other way to represent the errors for the median difference distribution?",,"['statistics', 'median']"
81,How to test a hypothesis which compares set of pairs of statements?,How to test a hypothesis which compares set of pairs of statements?,,"I've conducted an experiment but I'm not sure how to proceed with statistical analysis of it. I have pairs of sentences created by two groups of people A and B, semantically the sentences in each pair are the same but the grammar is little bit different. The participants of the experiment had to decide which sentence from each pair sounds better. They could say that A is much better than B, A is slightly better than B, they are indistinguishable, B is slightly better than A, B is much better than A- 5 options to choose from. I have 20 pairs of sentences graded by 20 participants. I would like to check if it is possible to say that the sentences from group A sounds not worse than the sentences from group B (so  the sentences from A were graded as much better, slightly better or indistinguishable from B). I guess I should start with the normality test, like QQ plots or Shapiro-Wilk, right? But I'm not sure how to proceed with the data I have. Any which tests later I should use?","I've conducted an experiment but I'm not sure how to proceed with statistical analysis of it. I have pairs of sentences created by two groups of people A and B, semantically the sentences in each pair are the same but the grammar is little bit different. The participants of the experiment had to decide which sentence from each pair sounds better. They could say that A is much better than B, A is slightly better than B, they are indistinguishable, B is slightly better than A, B is much better than A- 5 options to choose from. I have 20 pairs of sentences graded by 20 participants. I would like to check if it is possible to say that the sentences from group A sounds not worse than the sentences from group B (so  the sentences from A were graded as much better, slightly better or indistinguishable from B). I guess I should start with the normality test, like QQ plots or Shapiro-Wilk, right? But I'm not sure how to proceed with the data I have. Any which tests later I should use?",,"['statistics', 'normal-distribution', 'hypothesis-testing']"
82,Number of elements and number of different basis of $\mathbb F_5^3$,Number of elements and number of different basis of,\mathbb F_5^3,"Let $\mathbb F:=\mathbb F_5$ the field with five elements. (i) How many elements has $\mathbb F^3$? (ii) How many different basis has $\mathbb F^3$? My idea: (i) $\mathbb F^3$ has $5^3$ elements. (ii) The 1. basis vector must be different from the zero vector, so i've got $5^3-1$ possible choices for $v_1$.The 2. basis vector $v_2$ may not be linear combination $v_2=a\cdot v_1$ with $a \in \mathbb F$, so $5^3-5$ possible choices, and for $v_3$ i've got $5^3-5^2$ possible choices, so altogether $(5^3-1)(5^3-5)(5^3-5^2)$. But i'm unsure, whether i've counted to many? Do i have to divide my result through $n!$?","Let $\mathbb F:=\mathbb F_5$ the field with five elements. (i) How many elements has $\mathbb F^3$? (ii) How many different basis has $\mathbb F^3$? My idea: (i) $\mathbb F^3$ has $5^3$ elements. (ii) The 1. basis vector must be different from the zero vector, so i've got $5^3-1$ possible choices for $v_1$.The 2. basis vector $v_2$ may not be linear combination $v_2=a\cdot v_1$ with $a \in \mathbb F$, so $5^3-5$ possible choices, and for $v_3$ i've got $5^3-5^2$ possible choices, so altogether $(5^3-1)(5^3-5)(5^3-5^2)$. But i'm unsure, whether i've counted to many? Do i have to divide my result through $n!$?",,"['linear-algebra', 'combinatorics', 'statistics', 'field-theory']"
83,Probability a Random Variable will assume a value,Probability a Random Variable will assume a value,,"In a fictional town of ABC, the weather patterns on different days are all independent of each other. Assume that each given day in ABC is sunny with probability 30% and rainy with probability 70%. Let T denote the random variable representing the number of days you need to wait to see two sunny days in a row in ABC. For example, if it is sunny both tomorrow and the day after, the value of T will be 2. If it is sunny tomorrow, rains the day after, and is then sunny the two days after that, the value of T will be 4. What is the probability that T=4? So if the question was asking the probability of T=3, the first day would have to rain and the last two would be sunny, so the probability would be given by 0.7*0.3*0.3. For T = 4, the last two days are sunny but the first two can either be both rainy or have one rainy and one sunny. I'm not sure how to assign probability to that. Any help would be appreciated!","In a fictional town of ABC, the weather patterns on different days are all independent of each other. Assume that each given day in ABC is sunny with probability 30% and rainy with probability 70%. Let T denote the random variable representing the number of days you need to wait to see two sunny days in a row in ABC. For example, if it is sunny both tomorrow and the day after, the value of T will be 2. If it is sunny tomorrow, rains the day after, and is then sunny the two days after that, the value of T will be 4. What is the probability that T=4? So if the question was asking the probability of T=3, the first day would have to rain and the last two would be sunny, so the probability would be given by 0.7*0.3*0.3. For T = 4, the last two days are sunny but the first two can either be both rainy or have one rainy and one sunny. I'm not sure how to assign probability to that. Any help would be appreciated!",,['statistics']
84,Lognormal Distribution,Lognormal Distribution,,"I have a question about MLEs and their regarding to a certain distribution: the lognormal distribution. $$ f_{X}(x ; \mu, \sigma)=\frac{1}{x \sigma \sqrt{2 \pi}} e^{-\frac{(\ln x-\mu)^{2}}{2 \sigma^{2}}}, \quad x>0 $$ And, I have that the expected value for the distribution is: $$ e^{\mu+\sigma^{2} / 2} $$ The MLE for the expected value and the variance are, respectively: $$ \widehat{\mu}=\frac{\sum_{k} \ln x_{k}}{n}, \quad \hat{\sigma}^{2}=\frac{\sum_{k}\left(\ln x_{k}-\widehat{\mu}\right)^{2}}{n} $$ Now, I'm confused since I need to show if the following are true for my mle for the expected value: consistent I have that the mle is unbiased, but now, I am not so sure...and since I am dealing with an mle, how am I to show consistency? Any help or references would be great!","I have a question about MLEs and their regarding to a certain distribution: the lognormal distribution. And, I have that the expected value for the distribution is: The MLE for the expected value and the variance are, respectively: Now, I'm confused since I need to show if the following are true for my mle for the expected value: consistent I have that the mle is unbiased, but now, I am not so sure...and since I am dealing with an mle, how am I to show consistency? Any help or references would be great!","
f_{X}(x ; \mu, \sigma)=\frac{1}{x \sigma \sqrt{2 \pi}} e^{-\frac{(\ln x-\mu)^{2}}{2 \sigma^{2}}}, \quad x>0
 
e^{\mu+\sigma^{2} / 2}
 
\widehat{\mu}=\frac{\sum_{k} \ln x_{k}}{n}, \quad \hat{\sigma}^{2}=\frac{\sum_{k}\left(\ln x_{k}-\widehat{\mu}\right)^{2}}{n}
",['statistics']
85,Moment Generating function proof,Moment Generating function proof,,"Let X have moment generating function M(t), and let v(t)=lnM(t).  show that v'(0)=E(X) and v''(0)=Var(X). I know the formula's for E(X) and Var(X), but don't I need an original pdf of X to compute the expected value and variance?  How would I show these are equal without an original equation or without M(t)?","Let X have moment generating function M(t), and let v(t)=lnM(t).  show that v'(0)=E(X) and v''(0)=Var(X). I know the formula's for E(X) and Var(X), but don't I need an original pdf of X to compute the expected value and variance?  How would I show these are equal without an original equation or without M(t)?",,['statistics']
86,The Null Hypothesis,The Null Hypothesis,,Experience in investigating insurance claims shows that the average cost to process a claim is approximately normally distributed with a mean of 80 dollars. New cost-cutting measures were started and a sample of 25 claims was tested. The sample mean of the costs to process these claims was 76% and the sample standard deviation of the costs was $10. We would like to test whether the cost-cutting measures seem to be working at the 5% significance level. Find the critical value for this test. I need help please. I have calculated that $H_0: \mu = 80$ is the null hypothesis $H_1: \mu < 80$ is the alternative hypothesis Also the test statistics i have calculated is like $z = (76 - 80)/(10/\sqrt{25})=-2$ Can some one help me now in finding the critical value so I can conclude which hypothesis is true here. Thanks,Experience in investigating insurance claims shows that the average cost to process a claim is approximately normally distributed with a mean of 80 dollars. New cost-cutting measures were started and a sample of 25 claims was tested. The sample mean of the costs to process these claims was 76% and the sample standard deviation of the costs was $10. We would like to test whether the cost-cutting measures seem to be working at the 5% significance level. Find the critical value for this test. I need help please. I have calculated that $H_0: \mu = 80$ is the null hypothesis $H_1: \mu < 80$ is the alternative hypothesis Also the test statistics i have calculated is like $z = (76 - 80)/(10/\sqrt{25})=-2$ Can some one help me now in finding the critical value so I can conclude which hypothesis is true here. Thanks,,"['probability', 'statistics']"
87,Power Function for the uniform distribution,Power Function for the uniform distribution,,"Completely stuck on this homework question, I think my knowledge of the power function is nowhere near good enough coming up to finals! Consider the following alternative testing problem: the two hypothesis are $H_0 : θ = θ_0$ versus  $H_1 : θ > θ_0$ (note that the alternative hypothesis is composite). Since we know already from the notes that: $m_n =$max${X_1, . . . , X_n}$ is the MLE, we use it as our test statistic. We reject $H_0$ in favour of $H_1$ if $m_n > t$ for some threshold t. Compute the power function of the test for arbitrary threshold t. Can anyone help?","Completely stuck on this homework question, I think my knowledge of the power function is nowhere near good enough coming up to finals! Consider the following alternative testing problem: the two hypothesis are $H_0 : θ = θ_0$ versus  $H_1 : θ > θ_0$ (note that the alternative hypothesis is composite). Since we know already from the notes that: $m_n =$max${X_1, . . . , X_n}$ is the MLE, we use it as our test statistic. We reject $H_0$ in favour of $H_1$ if $m_n > t$ for some threshold t. Compute the power function of the test for arbitrary threshold t. Can anyone help?",,"['statistics', 'uniform-distribution', 'hypothesis-testing']"
88,How to compute the unique MLE from an Exponential Family of Distributions?,How to compute the unique MLE from an Exponential Family of Distributions?,,"Let  $$ f(x;\theta)=\frac{1}{\pi} \frac{e^{\theta x}\cos(\theta \pi/2)}{\cosh(x)}, x\in{\mathbb{R}} $$ be a family of densities and which is clearly exponential family. Then what is the Maximum Likelihood Estimator $\hat\theta_{n}$ of $\theta$ based on an independent sample of size $n$? My try: When I solved the loglikelihood equation, I got  $$ \tan(\theta \pi/2)=\frac{2}{\pi}\bar{x} \hspace 4cm (*) $$ Now, my problem is, if we solve $(*)$ for $\hat\theta_{n}$ then $\hat\theta_{n}$ is not unique. But for the exponential family it should be unique, right? So, I don't understand what is going wrong here. Your help will be greatly appreciated.","Let  $$ f(x;\theta)=\frac{1}{\pi} \frac{e^{\theta x}\cos(\theta \pi/2)}{\cosh(x)}, x\in{\mathbb{R}} $$ be a family of densities and which is clearly exponential family. Then what is the Maximum Likelihood Estimator $\hat\theta_{n}$ of $\theta$ based on an independent sample of size $n$? My try: When I solved the loglikelihood equation, I got  $$ \tan(\theta \pi/2)=\frac{2}{\pi}\bar{x} \hspace 4cm (*) $$ Now, my problem is, if we solve $(*)$ for $\hat\theta_{n}$ then $\hat\theta_{n}$ is not unique. But for the exponential family it should be unique, right? So, I don't understand what is going wrong here. Your help will be greatly appreciated.",,['statistics']
89,Estimate standard deviation of sample,Estimate standard deviation of sample,,"You have a random sample of 25 objects with mean weight of 24 grams, estimate the standard deviation of the sample. In addition, you know it's supposed to be 25 grams with a deviation of 1 gram, but this has no relevance to the above question, right? How is this done? Looking in my formula reference this is not enough information to give an estimate.","You have a random sample of 25 objects with mean weight of 24 grams, estimate the standard deviation of the sample. In addition, you know it's supposed to be 25 grams with a deviation of 1 gram, but this has no relevance to the above question, right? How is this done? Looking in my formula reference this is not enough information to give an estimate.",,"['statistics', 'standard-deviation']"
90,Joint distribution probabilities,Joint distribution probabilities,,"I have a question that is similar to the following(made up here): The construction of a tower of cards is done is two stages, procrastination and the actual building. The time in minutes needed to complete each stage are independent discrete random variables, X and Y, with probability functions; $f_X(x) = \frac{7}{10}$ if $ x = 2, \frac{3}{10}$ if $x = 3$, and $0$ otherwise. $f_Y(x) = \frac{2}{5}$ if $x = 3, \frac{2}{5}$ if $x = 4, \frac{1}{5}$ if $x = 5$ $0$ otherwise What is the probability the task took more than six minutes to complete? Now I haven't dealt with joint distribution problems before. But I can see 3 scenarios that yield more than 6 minutes of time elapsed. $f_X(2) $ then $f_Y(5)$  or $f_X(3)$ then $f_Y(4)$ or $f_X(3)$ then $f_Y(5)$ Can I simply then take $(\frac{7}{10}*\frac{1}{5} + \frac{3}{10}*\frac{2}{5} + \frac{3}{10}*\frac{1}{5})$? This seems right at $.32$. Furthermore if I do the other three scenarios I get a total probability of one, which increases my confidence with it once again. Any hints or confirmation? Thank you for your time","I have a question that is similar to the following(made up here): The construction of a tower of cards is done is two stages, procrastination and the actual building. The time in minutes needed to complete each stage are independent discrete random variables, X and Y, with probability functions; $f_X(x) = \frac{7}{10}$ if $ x = 2, \frac{3}{10}$ if $x = 3$, and $0$ otherwise. $f_Y(x) = \frac{2}{5}$ if $x = 3, \frac{2}{5}$ if $x = 4, \frac{1}{5}$ if $x = 5$ $0$ otherwise What is the probability the task took more than six minutes to complete? Now I haven't dealt with joint distribution problems before. But I can see 3 scenarios that yield more than 6 minutes of time elapsed. $f_X(2) $ then $f_Y(5)$  or $f_X(3)$ then $f_Y(4)$ or $f_X(3)$ then $f_Y(5)$ Can I simply then take $(\frac{7}{10}*\frac{1}{5} + \frac{3}{10}*\frac{2}{5} + \frac{3}{10}*\frac{1}{5})$? This seems right at $.32$. Furthermore if I do the other three scenarios I get a total probability of one, which increases my confidence with it once again. Any hints or confirmation? Thank you for your time",,"['probability', 'statistics', 'probability-distributions']"
91,Alternatives to Fisher information,Alternatives to Fisher information,,"The Fisher information matrix is defined as the following: $$\mathcal{I}(\theta)=E[(\frac{\partial \log f(x;\theta)}{\partial \theta})^2]=-E[\frac{\partial^2 \log f(x;\theta)}{\partial \theta \partial \theta^T}]$$ Where $f(x;\theta)$ is the probablity distribution function (pdf) of some random (vector) variable $x$ parameterized by (vector) parameter $\theta$.  For an unbiased estimator $\mathcal{I}(\theta)^{-1}$ is an lower bound for the MSE of estimating the parameter $\theta$. The reason that Fisher chose this definition for the Information measure was very intuitive: We are interested in relative changes in $f(x;\theta)$, i.e. $\frac{\partial f(x;\theta)/\partial \theta}{f(x;\theta)}=\frac{\partial \log f(x;\theta)}{\partial \theta}$. In particular, we are interested in average of this quantity regardless of its sign. Therefore, $E[(\frac{\partial \log f(x;\theta)}{\partial \theta})^2]$, is a natural and usually tractable choice (with closed-form solution in many cases). Here is my specific question: Are there alternatives to this definition? For example, $\mathcal{I}_a(\theta)=E[|\frac{\partial \log f(x;\theta)}{\partial \theta}|]$, is the first that comes to my mind. Are there other established formulations, or even totally different approaches that address the problem of lower-bounding the error of parameter estimation? Thanks!","The Fisher information matrix is defined as the following: $$\mathcal{I}(\theta)=E[(\frac{\partial \log f(x;\theta)}{\partial \theta})^2]=-E[\frac{\partial^2 \log f(x;\theta)}{\partial \theta \partial \theta^T}]$$ Where $f(x;\theta)$ is the probablity distribution function (pdf) of some random (vector) variable $x$ parameterized by (vector) parameter $\theta$.  For an unbiased estimator $\mathcal{I}(\theta)^{-1}$ is an lower bound for the MSE of estimating the parameter $\theta$. The reason that Fisher chose this definition for the Information measure was very intuitive: We are interested in relative changes in $f(x;\theta)$, i.e. $\frac{\partial f(x;\theta)/\partial \theta}{f(x;\theta)}=\frac{\partial \log f(x;\theta)}{\partial \theta}$. In particular, we are interested in average of this quantity regardless of its sign. Therefore, $E[(\frac{\partial \log f(x;\theta)}{\partial \theta})^2]$, is a natural and usually tractable choice (with closed-form solution in many cases). Here is my specific question: Are there alternatives to this definition? For example, $\mathcal{I}_a(\theta)=E[|\frac{\partial \log f(x;\theta)}{\partial \theta}|]$, is the first that comes to my mind. Are there other established formulations, or even totally different approaches that address the problem of lower-bounding the error of parameter estimation? Thanks!",,"['statistics', 'measure-theory', 'parameter-estimation']"
92,Expected Value of a function of the standard normal distribution,Expected Value of a function of the standard normal distribution,,Any help is appreciated. Thanks,Any help is appreciated. Thanks,,"['statistics', 'normal-distribution', 'expectation']"
93,Do I use the Standard Deviation of my sample or the population to find the standard error.,Do I use the Standard Deviation of my sample or the population to find the standard error.,,"A professor is interested in determining if attending college influences the level at which an  individual cooperates with the police. The professor is unsure if attending college will teach  respect for authority and thus increase level of cooperation or if college will teach  independent thinking and thus lead to deceased level of cooperation. To address this  question, the professor gathers information from the students in an undergraduate course  and calculates their propensity for cooperating with the police (higher number means higher  level of cooperation) and compares it to the known mean and standard deviation of the  general population. a) Would the professor conduct a 1-tailed or 2-tailed hypothesis test? Explain why. b) Use the information below to conduct a z-test using p=.05 as your alpha level. Make  sure you complete all 5 steps and show your work/answers for each step. Population: Mean = 3.02 SD = 0.54 Sample: Mean = 3.13 SD = 0.53 n = 86 c) Calculate a 95% confidence interval for the above sample. This is the answer I came up with for the standard error:  ( σ μ ) = σ / √ n = .53 / √ 86 = .53 / 9.27 = .057","A professor is interested in determining if attending college influences the level at which an  individual cooperates with the police. The professor is unsure if attending college will teach  respect for authority and thus increase level of cooperation or if college will teach  independent thinking and thus lead to deceased level of cooperation. To address this  question, the professor gathers information from the students in an undergraduate course  and calculates their propensity for cooperating with the police (higher number means higher  level of cooperation) and compares it to the known mean and standard deviation of the  general population. a) Would the professor conduct a 1-tailed or 2-tailed hypothesis test? Explain why. b) Use the information below to conduct a z-test using p=.05 as your alpha level. Make  sure you complete all 5 steps and show your work/answers for each step. Population: Mean = 3.02 SD = 0.54 Sample: Mean = 3.13 SD = 0.53 n = 86 c) Calculate a 95% confidence interval for the above sample. This is the answer I came up with for the standard error:  ( σ μ ) = σ / √ n = .53 / √ 86 = .53 / 9.27 = .057",,"['statistics', 'statistical-inference']"
94,Quantitatively comparing event trains of different lengths for Poissonness,Quantitatively comparing event trains of different lengths for Poissonness,,"I have a parameterized, effectively black box process that generates a series of events (simulated action potentials). Different parameter values often lead to different numbers of events. How can I compare the Poissonness of the event timings across my parameter space? Is there a standard approach? I am not looking for a single ""best"" parameter; I want to make a figure that shows how the Poissonness depends on the parameter choice. By ""Poissonness"" I mean the degree to which the output resembles that of a Poisson process, i.e. the degree to which the inter-event times are distributed according to an exponential distribution. My approaches and their weaknesses Originally, I just looked at the coefficient of variation of the inter-event interval. For a perfect Poisson process, this is near 1, but being near 1 does not mean we have a Poisson process. I then thought I would try fitting an exponential curve to the top midpoints on a bar graph, but that seems unjustified and in any case loses information by aggregating data. To avoid losing information, I could use the maximum likelihood estimate and quantify the Poissonness according to the likelihood. The problem here is that the likelihood is linked to the number of events, so I cannot directly compare likelihoods from different event trains. I can take the $n$th root of the likelihood (where $n$ is the number of events); intuitively this feels fair, but I'm not sure if it is statistically justified. Is there a better way?","I have a parameterized, effectively black box process that generates a series of events (simulated action potentials). Different parameter values often lead to different numbers of events. How can I compare the Poissonness of the event timings across my parameter space? Is there a standard approach? I am not looking for a single ""best"" parameter; I want to make a figure that shows how the Poissonness depends on the parameter choice. By ""Poissonness"" I mean the degree to which the output resembles that of a Poisson process, i.e. the degree to which the inter-event times are distributed according to an exponential distribution. My approaches and their weaknesses Originally, I just looked at the coefficient of variation of the inter-event interval. For a perfect Poisson process, this is near 1, but being near 1 does not mean we have a Poisson process. I then thought I would try fitting an exponential curve to the top midpoints on a bar graph, but that seems unjustified and in any case loses information by aggregating data. To avoid losing information, I could use the maximum likelihood estimate and quantify the Poissonness according to the likelihood. The problem here is that the likelihood is linked to the number of events, so I cannot directly compare likelihoods from different event trains. I can take the $n$th root of the likelihood (where $n$ is the number of events); intuitively this feels fair, but I'm not sure if it is statistically justified. Is there a better way?",,"['statistics', 'probability-distributions', 'computational-mathematics', 'numerical-optimization', 'biology']"
95,How do I measure the goodness of cosine similarity scores across different vector spaces?,How do I measure the goodness of cosine similarity scores across different vector spaces?,,"I am a computer scientist working on a problem that requires some statistical measures, though  (not being very well versed in statistics) I am not quite sure what statistics to use. Overview: I have a set of questions (from StackExchange sites, of course) and with this data, I am exploring algorithms that will find similar questions to one I provide. Yes, StackExchange itself already performs this function, as do many other Q&A sites. What I am trying to do is analyze the methods and algorithms that people employ to accomplish this task to see which methods perform best. My problem is finding appropriate statistical measures to quantitatively determine ""which methods perform best."" The Data: I have a set of StackExchange questions, each of which is saved like this: {'questionID':""..."", 'questionText':""...""} . For each question, I have a set of other questions either linked to it or from it. It is common practice for question answer-ers on StackExchange sites to add links to other similar posts in their answers, i.e. ""Have you read this post [insert link to post here] by so-and-so? They're solving a similar problem..."" I am considering these linked questions to be 'similar' to one another. More concretely, let's say we have question A . Question A has a collection of linked questions {B, C, D} . So A_linked = {B, C, D} . My intuition tells me that the transitive property does not apply here. That is, just because A is similar to B , and A is similar to C , I cannot confirm that B is similar to C . (Or can I?) However, I can confidently say that if A is similar to B , then B is similar to A . So, to simplify these relationships, I will create a set of similar pairs: {A, B}, {A, C}, {A, D} These pairs will serve as a ground truth of sorts. These are questions we know are similar to one another, so their similarity confidence values equals 1. So similarityConfidence({A,B}) = 1 Something to note about this set-up is that we know only a few similar questions for each question in our dataset. What we don't know is whether some other question E is also similar to A . It might be similar, it might not be similar, we don't know. So our 'ground truth' is really only some of the truth. The algorithm: A simplified pseudocode version of the algorithm is this: for q in questions: #remember q = {'questionID':""..."", 'questionText':""...""}      similarities = {} # will hold a mapping from questionID to similarity to q      q_Vector = vectorize(q) # create a vector from question text (each word is a dimension, value is unimportant)      for o in questions: #such that q!=o           o_Vector = vectorize(o)           similarities[o['questionID']] = cosineSimilarity(q_Vector,o_Vector) # values will be in the range of 1.0=identical to 0.0=not similar at all      #now what??? So now I have a complete mapping of cosine similarity scores between q and every other question in my dataset. My ultimate goal is to run this code for many variations of the vectorize() function (each of which will return a slightly different vector) and determine which variation performs best in terms of cosine scores. The Problem: So here lies my question. Now what? How do I quantitatively measure how good these cosine scores are? These are some ideas of measurements I've brainstormed (though I feel like they're unrefined, incomplete): Some sort of error function similar to Root Mean Square Error (RMSE). So for each document in the ground-truth similarities list, accumulate the squared error (with error roughly defined as 1-similarities[questionID] ). We would then divide that accumulation by the total number of similar pairs *2 (since we will consider a->b as well as b->a ). Finally, we'd take the square root of this error. This requires some thought, since these values may need to be normalized. Though all variations of vectorize() will produce cosine scores in the range of 0 to 1, the cosine scores from two vectorize() functions may not compare to one another. vectorize_1() might have generally high cosine scores for each question, so a score of .5 might be a very low score. Alternatively, vectorize_2() might have generally low cosine scores for each question, so a .5 might be a very high score. I need to account for this variation somehow. Also, I proposed an error function of 1-similarities[questionID] . I chose 1 because we know that the two questions are similar, therefore our similarity confidence is 1. However, a cosine similarity score of 1 means the two questions are identical. We are not claiming that our 'linked' questions are identical, merely that they are similar. Is this an issue? We can perform recall (number of similar documents returned/number of similar documents), so long as we set a threshold for which questions we return as 'similar' and which we do not. Although, for the reasons mentioned above, this shouldn't be a predefined threshold like similarity[documentID]>75 because each vectorize() function may return different values. We could do a recall @ k , where we only analyze the top k posts. This could be problematic though, because we don't have the full ground truth. If we set k=5 , and only 1 document ( B ) of the 3 documents we knew to be relevant ( {B,C,D} ) were in the top 5, we do not know whether the other 4 top documents are actually equally or more similar, but no one linked them. Do you have any other ideas? How can I quantitatively measure which vectorize() function performs best?","I am a computer scientist working on a problem that requires some statistical measures, though  (not being very well versed in statistics) I am not quite sure what statistics to use. Overview: I have a set of questions (from StackExchange sites, of course) and with this data, I am exploring algorithms that will find similar questions to one I provide. Yes, StackExchange itself already performs this function, as do many other Q&A sites. What I am trying to do is analyze the methods and algorithms that people employ to accomplish this task to see which methods perform best. My problem is finding appropriate statistical measures to quantitatively determine ""which methods perform best."" The Data: I have a set of StackExchange questions, each of which is saved like this: {'questionID':""..."", 'questionText':""...""} . For each question, I have a set of other questions either linked to it or from it. It is common practice for question answer-ers on StackExchange sites to add links to other similar posts in their answers, i.e. ""Have you read this post [insert link to post here] by so-and-so? They're solving a similar problem..."" I am considering these linked questions to be 'similar' to one another. More concretely, let's say we have question A . Question A has a collection of linked questions {B, C, D} . So A_linked = {B, C, D} . My intuition tells me that the transitive property does not apply here. That is, just because A is similar to B , and A is similar to C , I cannot confirm that B is similar to C . (Or can I?) However, I can confidently say that if A is similar to B , then B is similar to A . So, to simplify these relationships, I will create a set of similar pairs: {A, B}, {A, C}, {A, D} These pairs will serve as a ground truth of sorts. These are questions we know are similar to one another, so their similarity confidence values equals 1. So similarityConfidence({A,B}) = 1 Something to note about this set-up is that we know only a few similar questions for each question in our dataset. What we don't know is whether some other question E is also similar to A . It might be similar, it might not be similar, we don't know. So our 'ground truth' is really only some of the truth. The algorithm: A simplified pseudocode version of the algorithm is this: for q in questions: #remember q = {'questionID':""..."", 'questionText':""...""}      similarities = {} # will hold a mapping from questionID to similarity to q      q_Vector = vectorize(q) # create a vector from question text (each word is a dimension, value is unimportant)      for o in questions: #such that q!=o           o_Vector = vectorize(o)           similarities[o['questionID']] = cosineSimilarity(q_Vector,o_Vector) # values will be in the range of 1.0=identical to 0.0=not similar at all      #now what??? So now I have a complete mapping of cosine similarity scores between q and every other question in my dataset. My ultimate goal is to run this code for many variations of the vectorize() function (each of which will return a slightly different vector) and determine which variation performs best in terms of cosine scores. The Problem: So here lies my question. Now what? How do I quantitatively measure how good these cosine scores are? These are some ideas of measurements I've brainstormed (though I feel like they're unrefined, incomplete): Some sort of error function similar to Root Mean Square Error (RMSE). So for each document in the ground-truth similarities list, accumulate the squared error (with error roughly defined as 1-similarities[questionID] ). We would then divide that accumulation by the total number of similar pairs *2 (since we will consider a->b as well as b->a ). Finally, we'd take the square root of this error. This requires some thought, since these values may need to be normalized. Though all variations of vectorize() will produce cosine scores in the range of 0 to 1, the cosine scores from two vectorize() functions may not compare to one another. vectorize_1() might have generally high cosine scores for each question, so a score of .5 might be a very low score. Alternatively, vectorize_2() might have generally low cosine scores for each question, so a .5 might be a very high score. I need to account for this variation somehow. Also, I proposed an error function of 1-similarities[questionID] . I chose 1 because we know that the two questions are similar, therefore our similarity confidence is 1. However, a cosine similarity score of 1 means the two questions are identical. We are not claiming that our 'linked' questions are identical, merely that they are similar. Is this an issue? We can perform recall (number of similar documents returned/number of similar documents), so long as we set a threshold for which questions we return as 'similar' and which we do not. Although, for the reasons mentioned above, this shouldn't be a predefined threshold like similarity[documentID]>75 because each vectorize() function may return different values. We could do a recall @ k , where we only analyze the top k posts. This could be problematic though, because we don't have the full ground truth. If we set k=5 , and only 1 document ( B ) of the 3 documents we knew to be relevant ( {B,C,D} ) were in the top 5, we do not know whether the other 4 top documents are actually equally or more similar, but no one linked them. Do you have any other ideas? How can I quantitatively measure which vectorize() function performs best?",,"['statistics', 'algorithms', 'data-mining']"
96,Generate correlated random numbers precisely,Generate correlated random numbers precisely,,"Let's assume I want to generate k samples of n random numbers, that are correlated according to a given correlation matrix C (e.g. $n = 3$): 1    0.3  0.3 0.3    1  0.3 0.3  0.3    1 Using Cholesky Decomposition (Python implementation from NumPy), I can calculate L so $C = LL^T$: L = [[ 1.          0.          0.        ]      [ 0.3         0.9539392   0.        ]      [ 0.3         0.22013982  0.92819096]] Generating n (uncorrelated) random numbers (using numpy.random.normal($\mu$, $\sigma$) ) and multiplying each the vector with L should result in one sample with n correlated random variables. – So far my understanding of the algorithm. When I check the random numbers with SPSS, the ""observed"" correlations differ from the ones given in C . Example: I choose $n = 3$, $k = 10 000$ and $r = 0.99$. The observed correlations are: V1        V2        V3 V1     1          .774      .578 V2      .774     1          .443 V3      .578      .443     1 For my use-case I will need random numbers, that represent the given correlation matrix precisely. Did I make a mistake in this process or did I misunderstand the algorithm? Some insight is much appreciated.","Let's assume I want to generate k samples of n random numbers, that are correlated according to a given correlation matrix C (e.g. $n = 3$): 1    0.3  0.3 0.3    1  0.3 0.3  0.3    1 Using Cholesky Decomposition (Python implementation from NumPy), I can calculate L so $C = LL^T$: L = [[ 1.          0.          0.        ]      [ 0.3         0.9539392   0.        ]      [ 0.3         0.22013982  0.92819096]] Generating n (uncorrelated) random numbers (using numpy.random.normal($\mu$, $\sigma$) ) and multiplying each the vector with L should result in one sample with n correlated random variables. – So far my understanding of the algorithm. When I check the random numbers with SPSS, the ""observed"" correlations differ from the ones given in C . Example: I choose $n = 3$, $k = 10 000$ and $r = 0.99$. The observed correlations are: V1        V2        V3 V1     1          .774      .578 V2      .774     1          .443 V3      .578      .443     1 For my use-case I will need random numbers, that represent the given correlation matrix precisely. Did I make a mistake in this process or did I misunderstand the algorithm? Some insight is much appreciated.",,"['statistics', 'random', 'correlation']"
97,standard deviation of sums of numbers of different sets,standard deviation of sums of numbers of different sets,,"Suppose that I have $n$ different sets of numbers , each containing $m$ different numbers and I can only form sums of $n$ numbers by choosing only one element of each set. Is there an easy way to find the standard deviation of the sums or even an approximation especially if the set of sums increases exponentially and reaches huge numbers (in my case $300 \times 10^6$) .","Suppose that I have $n$ different sets of numbers , each containing $m$ different numbers and I can only form sums of $n$ numbers by choosing only one element of each set. Is there an easy way to find the standard deviation of the sums or even an approximation especially if the set of sums increases exponentially and reaches huge numbers (in my case $300 \times 10^6$) .",,['statistics']
98,Switching between two or multiple Poisson processes,Switching between two or multiple Poisson processes,,"Here is the question: Assume that we have $N$ Poisson processes, with arrival rates $\lambda_n, n=1...N$. At the start, we randomly choose, e.g. with equal probability, one Poisson process. Then, when there is an (the first) arrival in this process, we switched to another link, also with equal probability. This switching operation is then repeated. So, what is the resulted process? I'm thinking if it is still a Poisson process? What is the arrival rate then? Could some one give me the answer? Thanks a lot.","Here is the question: Assume that we have $N$ Poisson processes, with arrival rates $\lambda_n, n=1...N$. At the start, we randomly choose, e.g. with equal probability, one Poisson process. Then, when there is an (the first) arrival in this process, we switched to another link, also with equal probability. This switching operation is then repeated. So, what is the resulted process? I'm thinking if it is still a Poisson process? What is the arrival rate then? Could some one give me the answer? Thanks a lot.",,['statistics']
99,Proof about lognormal distribution,Proof about lognormal distribution,,"I'm trying to prove a result about the lognormal distribution that seems to me to be fairly intuitive, but I can't get the proof to work. Basically, I'd like to prove that as the mean increases, the expected value below a certain threshold (say $x^*$) increases more by raising the variance than by lowering the mean. Formally, let $V = \int_0^{x^*} x f(x) dx$, where $f(x)$ is the lognormal pdf with mean $y-a$ and variance $\sigma^2$. I'd like to show $\frac{\partial^2V}{\partial y \partial a} < \frac{\partial^2V}{\partial y \partial \sigma}$ I'd really appreciate any help.","I'm trying to prove a result about the lognormal distribution that seems to me to be fairly intuitive, but I can't get the proof to work. Basically, I'd like to prove that as the mean increases, the expected value below a certain threshold (say $x^*$) increases more by raising the variance than by lowering the mean. Formally, let $V = \int_0^{x^*} x f(x) dx$, where $f(x)$ is the lognormal pdf with mean $y-a$ and variance $\sigma^2$. I'd like to show $\frac{\partial^2V}{\partial y \partial a} < \frac{\partial^2V}{\partial y \partial \sigma}$ I'd really appreciate any help.",,"['calculus', 'statistics']"
