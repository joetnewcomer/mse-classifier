,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Let $F ( x ) = \frac { x } { \| x \| ^ { 3 } }$, if $0 \in \Omega$ show $\iint _ { \partial \Omega } ( F \cdot \nu ) d s = 4 \pi$","Let , if  show",F ( x ) = \frac { x } { \| x \| ^ { 3 } } 0 \in \Omega \iint _ { \partial \Omega } ( F \cdot \nu ) d s = 4 \pi,"Problem : Let $\Omega \subset \mathbb { R } ^ { 3 }$ a regular domain and $\nu$ the unit external vector of $\partial \Omega$ . Let $F ( x ) = \frac { x } { \| x \| ^ { 3 } }$ . Prove that if $0 \in \Omega$ , then $\iint _ { \partial \Omega } ( F \cdot \nu ) d s = 4 \pi$ . I tried to apply the divergence theorem, $$\iiint _ { \Omega } \operatorname { div } F \mathrm { d } x _ { 1 } \mathrm { d } x _ { 2 } \mathrm { d } x _ { 3 } = \iint _ { \partial \Omega } ( F \cdot \nu ) \mathrm { d } s$$ But without getting interesting results since I have so few informations about $\Omega$ .","Problem : Let a regular domain and the unit external vector of . Let . Prove that if , then . I tried to apply the divergence theorem, But without getting interesting results since I have so few informations about .",\Omega \subset \mathbb { R } ^ { 3 } \nu \partial \Omega F ( x ) = \frac { x } { \| x \| ^ { 3 } } 0 \in \Omega \iint _ { \partial \Omega } ( F \cdot \nu ) d s = 4 \pi \iiint _ { \Omega } \operatorname { div } F \mathrm { d } x _ { 1 } \mathrm { d } x _ { 2 } \mathrm { d } x _ { 3 } = \iint _ { \partial \Omega } ( F \cdot \nu ) \mathrm { d } s \Omega,"['calculus', 'multivariable-calculus', 'vector-analysis']"
1,Line Integral Shift,Line Integral Shift,,"How does the integration identity $\int_{a + c}^{b + c} f(x - c)\ dx = \int_a^b f(x)\ dx$ extend to line integrals?  Would a line integral of $f(x(t),\ y(t))$ shifted to $a + c \leq t \leq b + c$ become $f(x(t) - c,\ y(t) - c)$ , $f(x(t - c),\ y(t - c))$ , or something different?  Does this depend on whether the line integral is with respect to $x$ or $y$ (scalar field), arclength (scalar field), or $\vec r$ (vector field), and in each case, which aspects of the problem's geometry would be altered by the shift, and which would be invariant?","How does the integration identity extend to line integrals?  Would a line integral of shifted to become , , or something different?  Does this depend on whether the line integral is with respect to or (scalar field), arclength (scalar field), or (vector field), and in each case, which aspects of the problem's geometry would be altered by the shift, and which would be invariant?","\int_{a + c}^{b + c} f(x - c)\ dx = \int_a^b f(x)\ dx f(x(t),\ y(t)) a + c \leq t \leq b + c f(x(t) - c,\ y(t) - c) f(x(t - c),\ y(t - c)) x y \vec r","['calculus', 'integration', 'geometry', 'multivariable-calculus']"
2,Differential of scalar product,Differential of scalar product,,"Task from homework: Let $f:\Bbb R^n\to\Bbb R$ , $f(x,y)=\langle x, y\rangle$ , where $\langle\cdot,\cdot\rangle$ means the scalar product in $\Bbb R^n$ .   Find the differential $Df(x,y)(h,k)$ . First, the domain of $f$ is surely wrong, so with correcting it to $\Bbb R^n\times \Bbb R^n$ , I'm struggling to even start because every theorem we ever mentioned in class was about functions with the domain in $\Bbb R^n$ . How can $f$ be partially differentiated if the components $x$ and $y$ are again vectors? Partial derivatives were my first idea, but any help would be appreciated.","Task from homework: Let , , where means the scalar product in .   Find the differential . First, the domain of is surely wrong, so with correcting it to , I'm struggling to even start because every theorem we ever mentioned in class was about functions with the domain in . How can be partially differentiated if the components and are again vectors? Partial derivatives were my first idea, but any help would be appreciated.","f:\Bbb R^n\to\Bbb R f(x,y)=\langle x, y\rangle \langle\cdot,\cdot\rangle \Bbb R^n Df(x,y)(h,k) f \Bbb R^n\times \Bbb R^n \Bbb R^n f x y","['multivariable-calculus', 'scalar-fields']"
3,Maximising function of $n$ variables,Maximising function of  variables,n,"I am considering the following function $$f(x_1,\dots,x_n,y)=-\alpha \left(y-k_1\right)^2-\beta \sum_{i=1}^n\left(k_2-x_i\right)^2-\gamma \sum_{i=1}^n\left(y-x_i\right)^2 - \frac{\delta}{y-d} \sum_{i=1}^n (x_i-d)\, ,$$ where $(x_1,\dots,x_n,y)\in[d,1]^{n+1}$ , $d>0$ and $\alpha$ , $\beta$ , $\gamma$ , $\delta$ , $k_1$ , $k_2>0$ . Moreover, $x_i\leq y$ $\forall$ $i=1,\dots,$ $n$ . I'm trying to calculate the maximum of this function on that domain. Befor using ""brute force"" approach (i.e. by calculating derivatives, Hessian and so on), I wonder whether it's possible to obtain the absolute maximum in a more simple way. For example, I notice that $f\leq0$ ...","I am considering the following function where , and , , , , , . Moreover, . I'm trying to calculate the maximum of this function on that domain. Befor using ""brute force"" approach (i.e. by calculating derivatives, Hessian and so on), I wonder whether it's possible to obtain the absolute maximum in a more simple way. For example, I notice that ...","f(x_1,\dots,x_n,y)=-\alpha \left(y-k_1\right)^2-\beta \sum_{i=1}^n\left(k_2-x_i\right)^2-\gamma \sum_{i=1}^n\left(y-x_i\right)^2 - \frac{\delta}{y-d} \sum_{i=1}^n (x_i-d)\, , (x_1,\dots,x_n,y)\in[d,1]^{n+1} d>0 \alpha \beta \gamma \delta k_1 k_2>0 x_i\leq y \forall i=1,\dots, n f\leq0","['multivariable-calculus', 'optimization', 'maxima-minima']"
4,"If a function is uniformly continuous restricted to each line, is it globally uniformly continuous?","If a function is uniformly continuous restricted to each line, is it globally uniformly continuous?",,"Question: Suppose $f: \mathbb{R}^2\to \mathbb{R}$ is a continuous function such that for every line $L$ passing through the origin $(0, 0)$ , the restriction of the function $f|_{L} : L\to\mathbb{R}$ is uniformly continuous (we can view $L\cong\mathbb{R}$ if we wish). Does it follow that $f$ is uniformly continuous? Motivation. A friend and I were discussing the following fact: If $f: \mathbb{R}^2\to\mathbb{R}$ has bounded partial derivatives, then $f$ is uniformly continuous. One way to prove this fact is to show that $f$ must be Lipschitz, i.e. $|f(x)-f(y)|\leq C |x-y|$ for some constant $C$ . My proof for the latter fact uses the following idea: take two points $x$ and $y$ in $\mathbb{R}^2$ , look at the line $L$ joining $x$ and $y$ , then restrict $f$ to $L\cong\mathbb{R}$ , and use the Mean Value Theorem for the function $f|_{L}$ . The derivative of $f|_{L}$ will be bounded, because it is a directional derivative of $f$ , which is a linear combination of the partial derivatives of $f$ , which are themselves bounded. While formulating this proof, I was naturally lead to ask the question above.","Question: Suppose is a continuous function such that for every line passing through the origin , the restriction of the function is uniformly continuous (we can view if we wish). Does it follow that is uniformly continuous? Motivation. A friend and I were discussing the following fact: If has bounded partial derivatives, then is uniformly continuous. One way to prove this fact is to show that must be Lipschitz, i.e. for some constant . My proof for the latter fact uses the following idea: take two points and in , look at the line joining and , then restrict to , and use the Mean Value Theorem for the function . The derivative of will be bounded, because it is a directional derivative of , which is a linear combination of the partial derivatives of , which are themselves bounded. While formulating this proof, I was naturally lead to ask the question above.","f: \mathbb{R}^2\to \mathbb{R} L (0, 0) f|_{L} : L\to\mathbb{R} L\cong\mathbb{R} f f: \mathbb{R}^2\to\mathbb{R} f f |f(x)-f(y)|\leq C |x-y| C x y \mathbb{R}^2 L x y f L\cong\mathbb{R} f|_{L} f|_{L} f f","['real-analysis', 'multivariable-calculus', 'examples-counterexamples', 'uniform-continuity']"
5,Finding limits of integration using spherical coordinates,Finding limits of integration using spherical coordinates,,"I would like to integrate some function $f:\mathbb{R}^3\to\mathbb{R}$ over $C_1\cap C_2$ where $$C_1=\{(x,y,z)\in\mathbb{R}^3:x^2+4y^2+9z^2\leq1\}$$ $$C_2=\{(x,y,z)\in\mathbb{R}^3:x^2+4y^2+9z^2\leq 6z\}$$ My method was to find parametric equations for $C_1,C_2$ using spherical coordinates. However, I am having trouble finding the limits of integration. I attempted to use some geometric intuition but I can't seem to get the right result. Any help would be appreciated. I used the following parametrization for $C_1$ $$x=r\sin(a)\cos(b),\quad y= r \sin(a) \sin(b)/2,\quad z= r \cos(a)/3$$ For $C_2$ I used $$x=r\sin(a)\cos(b),\quad y= r \sin(a) \sin(b)/2,\quad z= r \cos(a)/3+1/3$$ In both cases, $0\leq r\leq 1, 0\leq a \leq \pi, 0\leq b\leq 2\pi$ .","I would like to integrate some function over where My method was to find parametric equations for using spherical coordinates. However, I am having trouble finding the limits of integration. I attempted to use some geometric intuition but I can't seem to get the right result. Any help would be appreciated. I used the following parametrization for For I used In both cases, .","f:\mathbb{R}^3\to\mathbb{R} C_1\cap C_2 C_1=\{(x,y,z)\in\mathbb{R}^3:x^2+4y^2+9z^2\leq1\} C_2=\{(x,y,z)\in\mathbb{R}^3:x^2+4y^2+9z^2\leq 6z\} C_1,C_2 C_1 x=r\sin(a)\cos(b),\quad y= r \sin(a) \sin(b)/2,\quad z= r \cos(a)/3 C_2 x=r\sin(a)\cos(b),\quad y= r \sin(a) \sin(b)/2,\quad z= r \cos(a)/3+1/3 0\leq r\leq 1, 0\leq a \leq \pi, 0\leq b\leq 2\pi","['multivariable-calculus', 'definite-integrals', 'spherical-coordinates', 'parametrization']"
6,Formally proving that the image of $f(E)$ is of measure zero when $E$ is a null set and f is $C^1$,Formally proving that the image of  is of measure zero when  is a null set and f is,f(E) E C^1,"So $f:U \rightarrow R^n$ is a continuously differential function, and $U \subset R^n$ is an open set. When $E$ a set of measure zero, I need to prove that $f(E)$ is also of measure zero. I know that this question was asked here before, but I still can't formalize my proof because I am missing something. I know that since $E$ is of measure zero then $E \subset \bigcup_{i}Q_i$ where $Q_i$ are intervals and $\sum_{i}|Q_i| < \epsilon$ for $\epsilon > 0$ . I also know that since $U$ is an open set there exist intervals $R_i$ that satisfy $U = \bigcup_{j}R_j$ . Also I know that if I look at $E  \bigcap ([-k,k] $ x ... x $[-k,k])$ then $f$ is lipshitz on this domain, let's call it $f_k$ , and $f(E) = \bigcup_{k}E(f_k)$ . But still I am missing here something that will prove that $f(E)$ is covered by intervals with measure zero each.","So is a continuously differential function, and is an open set. When a set of measure zero, I need to prove that is also of measure zero. I know that this question was asked here before, but I still can't formalize my proof because I am missing something. I know that since is of measure zero then where are intervals and for . I also know that since is an open set there exist intervals that satisfy . Also I know that if I look at x ... x then is lipshitz on this domain, let's call it , and . But still I am missing here something that will prove that is covered by intervals with measure zero each.","f:U \rightarrow R^n U \subset R^n E f(E) E E \subset \bigcup_{i}Q_i Q_i \sum_{i}|Q_i| < \epsilon \epsilon > 0 U R_i U = \bigcup_{j}R_j E  \bigcap ([-k,k]  [-k,k]) f f_k f(E) = \bigcup_{k}E(f_k) f(E)","['calculus', 'measure-theory', 'multivariable-calculus']"
7,Triple integral of portion of cone (cylindrical polar coordinates)?,Triple integral of portion of cone (cylindrical polar coordinates)?,,$V$ is the portion of the cone $z=\sqrt{x^2+y^2}\;$ for $\; x\geq 0.$ Find $$\iiint\limits_{V} xe^{-z} dV.$$ I am trying to solve this question.  The answer is supposed to be just $4.$ I have worked out the limits as $0\leq z \leq \infty\;$ and $\;-\pi/2\leq  \theta \leq \pi/2\;$ and $\;0\leq R \leq z.$ What am I doing wrong?,is the portion of the cone for Find I am trying to solve this question.  The answer is supposed to be just I have worked out the limits as and and What am I doing wrong?,V z=\sqrt{x^2+y^2}\; \; x\geq 0. \iiint\limits_{V} xe^{-z} dV. 4. 0\leq z \leq \infty\; \;-\pi/2\leq  \theta \leq \pi/2\; \;0\leq R \leq z.,"['calculus', 'multivariable-calculus']"
8,Find the integral of the vector field,Find the integral of the vector field,,"Let the vector field $$\vec{F}\left(\vec{x}\right)=\begin{pmatrix}{x_1^2+2x_3}\\ x_1x_2\\  x_3^2-2x_1\end{pmatrix}$$ Compute the integral $\int _C\vec{F}\left(\vec{x}\right)d\vec{x}$ from the origin to the point $P(1/2/3)$ if $C$ is a straight line from the origin to $P$ . So in the book they only give us answers, but not how to get the answers. I calculated the integral and got $$\int _0^112t^2-8t$$ which equals to zero, however in the book the answers is $9\frac{2}{3}$ . I think the book is wrong because I just don't see how we can get that answer. Or am I wrong?","Let the vector field Compute the integral from the origin to the point if is a straight line from the origin to . So in the book they only give us answers, but not how to get the answers. I calculated the integral and got which equals to zero, however in the book the answers is . I think the book is wrong because I just don't see how we can get that answer. Or am I wrong?","\vec{F}\left(\vec{x}\right)=\begin{pmatrix}{x_1^2+2x_3}\\ x_1x_2\\
 x_3^2-2x_1\end{pmatrix} \int _C\vec{F}\left(\vec{x}\right)d\vec{x} P(1/2/3) C P \int _0^112t^2-8t 9\frac{2}{3}","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals', 'vectors']"
9,Show there can be no co-ordinate patch at this point,Show there can be no co-ordinate patch at this point,,"I am attempting to prove that the subset of $\mathbf{R}^3$ satisfying $x^2 + z^2 = y^2$ is not a surface, where a surface is a subset of $\mathbf{R}^3$ for every point in which there is a co-ordinate patch whose image contains that point and is contained in the subset and a co-ordinate patch is a smooth, injective, regular function from an open region of $\mathbf{R}^2$ to $\mathbf{R}^3$ . Having seen this visualized, I know this space is composed of two infinite cones joined at their tips (that is, at the origin). Intuitively, I then know that this is the problematic point that stops the entire set being a surface. However, I am having trouble constructing a reason why there cannot be a co-ordinate patch containing the origin; that is, showing which of the conditions on a co-ordinate patch cannot be satisfied, and why. My attempt : If $\bar{x} : D \to \mathbf{R}^3$ were such a co-ordinate patch, I feel that because $D$ is open and $\bar{x}$ is continuous that there would necessarily be a ""jump"" from one of the cones composing the space to the other. As in, there would exist some point on the cone with positive $y$ and some point on the other cone with negative $y$ whose inputs in $D$ are close but which are obviously not close on the space. Any help is greatly appreciated. Thank you in advance.","I am attempting to prove that the subset of satisfying is not a surface, where a surface is a subset of for every point in which there is a co-ordinate patch whose image contains that point and is contained in the subset and a co-ordinate patch is a smooth, injective, regular function from an open region of to . Having seen this visualized, I know this space is composed of two infinite cones joined at their tips (that is, at the origin). Intuitively, I then know that this is the problematic point that stops the entire set being a surface. However, I am having trouble constructing a reason why there cannot be a co-ordinate patch containing the origin; that is, showing which of the conditions on a co-ordinate patch cannot be satisfied, and why. My attempt : If were such a co-ordinate patch, I feel that because is open and is continuous that there would necessarily be a ""jump"" from one of the cones composing the space to the other. As in, there would exist some point on the cone with positive and some point on the other cone with negative whose inputs in are close but which are obviously not close on the space. Any help is greatly appreciated. Thank you in advance.",\mathbf{R}^3 x^2 + z^2 = y^2 \mathbf{R}^3 \mathbf{R}^2 \mathbf{R}^3 \bar{x} : D \to \mathbf{R}^3 D \bar{x} y y D,"['multivariable-calculus', 'differential-geometry', 'manifolds', 'surfaces']"
10,Difficult Region of Integration Involving Gauss's Theorem,Difficult Region of Integration Involving Gauss's Theorem,,"I'm told to use Gauss's Theorem to compute the flux of a field $\vec F = <x,y^2,y+z>$ along the boundary of the cylindrical solid $x^2+y^2 \le 4$ below $z=8$ and above $z=x$ . I know by Gauss's Theorem that: Net Flux = $\iint_{\partial D} \vec F \cdot \vec ndS = \iiint_D \nabla \cdot \vec FdV$ This computation is pretty straight forward. $\nabla \cdot \vec F = 2+2y$ . But the region of integration is particularly difficult to map out. I thought to use cylindrical coordinates and setting the bounds to $0 \le \theta \le 2 \pi$ , $0 \le z \le 8$ , and $0 \le r \le 4$ , but this seems like it would just give me the area of the cylinder of height 8--and wouldn't include the part where z=x slices through the cylinder. What would be the right way to go in terms of the bounds of integration?","I'm told to use Gauss's Theorem to compute the flux of a field along the boundary of the cylindrical solid below and above . I know by Gauss's Theorem that: Net Flux = This computation is pretty straight forward. . But the region of integration is particularly difficult to map out. I thought to use cylindrical coordinates and setting the bounds to , , and , but this seems like it would just give me the area of the cylinder of height 8--and wouldn't include the part where z=x slices through the cylinder. What would be the right way to go in terms of the bounds of integration?","\vec F = <x,y^2,y+z> x^2+y^2 \le 4 z=8 z=x \iint_{\partial D} \vec F \cdot \vec ndS = \iiint_D \nabla \cdot \vec FdV \nabla \cdot \vec F = 2+2y 0 \le \theta \le 2 \pi 0 \le z \le 8 0 \le r \le 4","['calculus', 'integration', 'multivariable-calculus', 'physics', 'divergence-operator']"
11,Writing an iterated double integral in two forms,Writing an iterated double integral in two forms,,"Let $f\left(x,\:y\right)\:=\:x^2e^{x^2}$ and let $R$ be the triangle bounded by the lines $x=5$ , $x=y/2$ , and $y=x$ in the $xy$ -plane. Express $\int _RfdA$ in two different ways. After sketching the region, I got that the first way to write the integral would simply be: $\int _0^5\int _x^{2x}\:x^2e^{x^2}dydx$ However, I was stuck on how to write it in the other way. With the region, there is no obvious way to write the integral as one in terms of $y$ . I feel that this may mean that I need to have the sum of two different double integrals but I am not entirely sure how that would work in this case. Would I have to subtract the higher $x=(1/2)y$ line from the $x=y$ line or is there some other way? Any help would be highly appreciated!","Let and let be the triangle bounded by the lines , , and in the -plane. Express in two different ways. After sketching the region, I got that the first way to write the integral would simply be: However, I was stuck on how to write it in the other way. With the region, there is no obvious way to write the integral as one in terms of . I feel that this may mean that I need to have the sum of two different double integrals but I am not entirely sure how that would work in this case. Would I have to subtract the higher line from the line or is there some other way? Any help would be highly appreciated!","f\left(x,\:y\right)\:=\:x^2e^{x^2} R x=5 x=y/2 y=x xy \int _RfdA \int _0^5\int _x^{2x}\:x^2e^{x^2}dydx y x=(1/2)y x=y","['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
12,Surface area of $x^2+z^2=a^2$ inside of $x^2+y^2 = 2ay$ and in first octant,Surface area of  inside of  and in first octant,x^2+z^2=a^2 x^2+y^2 = 2ay,"The questions is What is the surface area of $x^2+z^2=a^2$ inside of $x^2+y^2 = 2ay$ and in first octant? My attempt The second equation can be rewritten as $x^2 + (y-a)^2=a^2$ to make it easier to work with. After this I tried parametrizing the first cylinder with $x= \operatorname{acos}\theta$ , $z=\operatorname{asin}\theta$ and $y=y$ . This is where I get stuck. The area integral should be $A= \iint||T_{\theta} \times T_y||dS$ = $\iint a dyd\theta$ . I'm not sure how to place the bounds on $\theta$ and $y$ . My first guess was to let $0 \leq\theta \leq \pi/2$ , but I'm still not sure what to do with $y$ . Any help would be greatly appreciated.","The questions is What is the surface area of inside of and in first octant? My attempt The second equation can be rewritten as to make it easier to work with. After this I tried parametrizing the first cylinder with , and . This is where I get stuck. The area integral should be = . I'm not sure how to place the bounds on and . My first guess was to let , but I'm still not sure what to do with . Any help would be greatly appreciated.",x^2+z^2=a^2 x^2+y^2 = 2ay x^2 + (y-a)^2=a^2 x= \operatorname{acos}\theta z=\operatorname{asin}\theta y=y A= \iint||T_{\theta} \times T_y||dS \iint a dyd\theta \theta y 0 \leq\theta \leq \pi/2 y,"['multivariable-calculus', 'surface-integrals']"
13,Notation: limit in two variables,Notation: limit in two variables,,"I want to show that the function $$ f: \mathbb{R}^2 \to \mathbb{R}, \ (x,y) \mapsto \begin{cases} x^2(x-1)(y-1)\sin(xy), & (x,y) \in [0,1]^2 \\ 0, & \text{elsewhere.} \end{cases} $$ is continuous. Obviously, both pieces are continuous because they are composed of elementary continuous functions. Now I only need to show that the transition between both pieces is continuous and don't know how to notate it properly. My Idea was \begin{align*}             \lim_{\|x,y\|_{\infty} \nearrow 1} f|_{[0,1]}             & = \lim_{\max(x,y) \nearrow 1}  x^2(x - 1)(y - 1)\sin(xy) \\             & = \begin{cases}             \lim_{x \nearrow 1}  x^2(x - 1)(y - 1)\sin(xy) \\             \lim_{y \nearrow 1}  x^2(x - 1)(y - 1)\sin(xy)             \end{cases}             = \begin{cases} 1^2(1 - 1)(y - 1)\sin(y) \\ x^2(x - 1)(1 - 1)\sin(x) \end{cases}             = 0         \end{align*} I've never seen it done that way but didn't have a better idea. Second Attempt For all $a \in [0,1]$ we have \begin{equation*}        \begin{cases}         \lim\limits_{(x,y)\to(0,a)} f(x,y)         = 0^2(0 - 1)(a - 1)\sin(0)         = 0         = f(0,a), \\         \lim\limits_{(x,y)\to(1,a)} f(x,y)         = 1^2(1 - 1)(a - 1)\sin(y)         = 0         = f(1,a). \\         \lim\limits_{(x,y)\to(a,0)} f(x,y)         = a^2(a - 1)(0 - 1)\sin(0)         = 0         = f(a,0), \\         \lim\limits_{(x,y)\to(a,1)} f(x,1)         = a^2(a - 1)(1 - 1)\sin(x)         = 0         = f(a,1)      \end{cases} \end{equation*}","I want to show that the function is continuous. Obviously, both pieces are continuous because they are composed of elementary continuous functions. Now I only need to show that the transition between both pieces is continuous and don't know how to notate it properly. My Idea was I've never seen it done that way but didn't have a better idea. Second Attempt For all we have","
f: \mathbb{R}^2 \to \mathbb{R}, \
(x,y) \mapsto \begin{cases}
x^2(x-1)(y-1)\sin(xy), & (x,y) \in [0,1]^2 \\ 0, & \text{elsewhere.}
\end{cases}
 \begin{align*}
            \lim_{\|x,y\|_{\infty} \nearrow 1} f|_{[0,1]}
            & = \lim_{\max(x,y) \nearrow 1}  x^2(x - 1)(y - 1)\sin(xy) \\
            & = \begin{cases}
            \lim_{x \nearrow 1}  x^2(x - 1)(y - 1)\sin(xy) \\
            \lim_{y \nearrow 1}  x^2(x - 1)(y - 1)\sin(xy)
            \end{cases}
            = \begin{cases} 1^2(1 - 1)(y - 1)\sin(y) \\ x^2(x - 1)(1 - 1)\sin(x) \end{cases}
            = 0
        \end{align*} a \in [0,1] \begin{equation*}   
    \begin{cases}
        \lim\limits_{(x,y)\to(0,a)} f(x,y)
        = 0^2(0 - 1)(a - 1)\sin(0)
        = 0
        = f(0,a), \\
        \lim\limits_{(x,y)\to(1,a)} f(x,y)
        = 1^2(1 - 1)(a - 1)\sin(y)
        = 0
        = f(1,a). \\
        \lim\limits_{(x,y)\to(a,0)} f(x,y)
        = a^2(a - 1)(0 - 1)\sin(0)
        = 0
        = f(a,0), \\
        \lim\limits_{(x,y)\to(a,1)} f(x,1)
        = a^2(a - 1)(1 - 1)\sin(x)
        = 0
        = f(a,1) 
    \end{cases}
\end{equation*}","['real-analysis', 'limits', 'multivariable-calculus', 'proof-verification', 'continuity']"
14,Gaussian multi-variate integral,Gaussian multi-variate integral,,I would like to compute the following integral $$ I_n = \frac{1}{\sqrt{det(2\pi A)}} \int_{\mathbb{R}^n} ||x||^2_2 \exp\left(-\frac{1}{2} x^TAx\right) \mathrm{d} x $$ where $A$ is symmetric and positive definite. Any suggestions or hints? Thanks in advance!,I would like to compute the following integral where is symmetric and positive definite. Any suggestions or hints? Thanks in advance!,"
I_n = \frac{1}{\sqrt{det(2\pi A)}} \int_{\mathbb{R}^n} ||x||^2_2 \exp\left(-\frac{1}{2} x^TAx\right) \mathrm{d} x
 A","['multivariable-calculus', 'gaussian-integral']"
15,Unit vector differentiation,Unit vector differentiation,,"I came across an identity for differentiating an unit vector, however I can't prove it so I would appreciate if someone would explain how to derive the identity.  Let $f(t)$ be a vector valued function, then its magnitude is given by $||f(t)||$ , and $f(t)$ is a differentiable curve such that $f(t) ≠ 0$ for all $t$ .  Then the derivative of the unit vector is given by $\frac{d}{dt}\frac{f(t)}{||f(t)||}=\frac{f(t)✕f'(t)✕f(t)}{||f(t)||^3}$ Also the unit tangent vector $T(t)$ is defined as: $T(t)=\frac{f'(t)}{||f'(t)||}$ and in the same way $T'(t)=\frac{f'(t)✕f''(t)✕f'(t)}{||f'(t)||}$ . I appreciate any help you can provide.","I came across an identity for differentiating an unit vector, however I can't prove it so I would appreciate if someone would explain how to derive the identity.  Let be a vector valued function, then its magnitude is given by , and is a differentiable curve such that for all .  Then the derivative of the unit vector is given by Also the unit tangent vector is defined as: and in the same way . I appreciate any help you can provide.",f(t) ||f(t)|| f(t) f(t) ≠ 0 t \frac{d}{dt}\frac{f(t)}{||f(t)||}=\frac{f(t)✕f'(t)✕f(t)}{||f(t)||^3} T(t) T(t)=\frac{f'(t)}{||f'(t)||} T'(t)=\frac{f'(t)✕f''(t)✕f'(t)}{||f'(t)||},"['multivariable-calculus', 'vectors']"
16,"Multivariate Chain rule, why addition?","Multivariate Chain rule, why addition?",,"Lets consider the above graph. Now $\left[ \frac{\delta h}{\delta f},\,\frac{\delta h}{\delta g} \right]$ is the Jacobian. $\frac{\delta h}{\delta f}$ is the magnitude of vector along $f(x)$ and $\frac{\delta h}{\delta g}$ is the magnitude of vector along $g(x)$ . So, $$\sqrt{{{\left( \frac{\delta h}{\delta f} \right)}^{2}}+\,\,{{\left( \frac{\delta h}{\delta g} \right)}^{2}}}$$ gives the magnitude of vector of steepest change of $h( f(x), g(x))$ , which is the estimate of the change of $h$ w.r.t. $f$ and $g$ . Now if I nudge $x$ the jacobian $\left[ \frac{\delta h}{\delta f},\,\frac{\delta h}{\delta g} \right]$ will change i.e. $\frac{\delta h}{\delta f}\frac{\delta f}{\delta x}$ and $\frac{\delta h}{\delta g}\frac{\delta g}{\delta x}$ . These show how the contents of the Jacobian will change w.r.t. $x$ . Now I consider the diagram same as this one So, $$\left[ \frac{\delta h}{\delta f}\frac{\delta f}{\delta x},\,\,\frac{\delta h}{\delta g}\frac{\delta g}{\delta x} \right]$$ can be considered as new Jacobian and the estimate of change in $h$ w.r.t $x$ will be calculated just by finding root of sum of squares of the elements in the Jacobian. But the formula says $$\frac{dh}{dx}=\frac{\delta h}{\delta f}\frac{\delta f}{\delta x}+\frac{\delta h}{\delta g}\frac{\delta g}{\delta x}$$ I am trying to understand the chain rule w.r.t. estimating the change in $h$ w.r.t. $x$ . I am sure I have lots of misconception but will be glad if you point that out.","Lets consider the above graph. Now is the Jacobian. is the magnitude of vector along and is the magnitude of vector along . So, gives the magnitude of vector of steepest change of , which is the estimate of the change of w.r.t. and . Now if I nudge the jacobian will change i.e. and . These show how the contents of the Jacobian will change w.r.t. . Now I consider the diagram same as this one So, can be considered as new Jacobian and the estimate of change in w.r.t will be calculated just by finding root of sum of squares of the elements in the Jacobian. But the formula says I am trying to understand the chain rule w.r.t. estimating the change in w.r.t. . I am sure I have lots of misconception but will be glad if you point that out.","\left[ \frac{\delta h}{\delta f},\,\frac{\delta h}{\delta g} \right] \frac{\delta h}{\delta f} f(x) \frac{\delta h}{\delta g} g(x) \sqrt{{{\left( \frac{\delta h}{\delta f} \right)}^{2}}+\,\,{{\left( \frac{\delta h}{\delta g} \right)}^{2}}} h( f(x), g(x)) h f g x \left[ \frac{\delta h}{\delta f},\,\frac{\delta h}{\delta g} \right] \frac{\delta h}{\delta f}\frac{\delta f}{\delta x} \frac{\delta h}{\delta g}\frac{\delta g}{\delta x} x \left[ \frac{\delta h}{\delta f}\frac{\delta f}{\delta x},\,\,\frac{\delta h}{\delta g}\frac{\delta g}{\delta x} \right] h x \frac{dh}{dx}=\frac{\delta h}{\delta f}\frac{\delta f}{\delta x}+\frac{\delta h}{\delta g}\frac{\delta g}{\delta x} h x","['calculus', 'multivariable-calculus', 'vectors']"
17,Show that the limit exists or does not exist [duplicate],Show that the limit exists or does not exist [duplicate],,"This question already has an answer here : Show that the limit does not exist $\lim_{(x, y) \to (0,0)}\frac{5x^2}{x^2 + y^2}$ [duplicate] (1 answer) Closed 5 years ago . $\lim_{(x, y) \to (0,0)} \frac{5x^2}{x^2 + y^2}$ let $y = 0$ $\lim_{x \to 0} \frac{5x^2}{x^2} = 5$ let $y = x$ $\lim_{x \to 0} \frac{5x^2}{2x^2} = \frac{5}{2}$ Since different values the limit does not exist. Would this be right?","This question already has an answer here : Show that the limit does not exist $\lim_{(x, y) \to (0,0)}\frac{5x^2}{x^2 + y^2}$ [duplicate] (1 answer) Closed 5 years ago . let let Since different values the limit does not exist. Would this be right?","\lim_{(x, y) \to (0,0)} \frac{5x^2}{x^2 + y^2} y = 0 \lim_{x \to 0} \frac{5x^2}{x^2} = 5 y = x \lim_{x \to 0} \frac{5x^2}{2x^2} = \frac{5}{2}",['multivariable-calculus']
18,Explanation of Proof of Second-Derivative Test for Local Extrema,Explanation of Proof of Second-Derivative Test for Local Extrema,,"My textbook introduces the following theorem: Theorem 5 Second-Derivative Test for Local Extrema If $f : U \subset \mathbb{R}^n \to \mathbb{R}$ is of class $C^3$ , $\mathbf{x}_0 \in U$ is a critical point of $f$ , and the Hessian $Hf(\mathbf{x}_0)$ is positive-definite, then $\mathbf{x}_0$ is a relative minimum of $f$ . Similarly, if $Hf(\mathbf{x}_0)$ is negative-definite, then $\mathbf{x}_0$ is a relative maximum. It then goes on to say the following: Actually, we shall prove that the extrema given by this criterion are strict. A relative maximum $\mathbf{x}_0$ is said to be strict if $f(\mathbf{x}) < f(\mathbf{x}_0)$ for nearby $\mathbf{x} \not= \mathbf{x}_0$ . A strict relative minimum is defined similarly. Also, the theorem is valid even if $f$ is only $C^2$ , but we have assumed $C^3$ for simplicity. The proof of theorem $5$ requires Taylor's theorem and the following result from linear algebra. Lemma 1 If $B = [b_{ij}]$ is an $n \times n$ real matrix, and if the associated quadratic function $$H: \mathbb{R}^n \to \mathbb{R}, (h_1, \dots, h_n) \mapsto \dfrac{1}{2} \sum_{i, j = 1}^n b_{ij} h_i h_j$$ is positive-definite, then there is a constant $M > 0$ such that for all $\mathbf{h} \in \mathbb{R}^n$ ; $$H(\mathbf{h}) \ge M || \mathbf{h} ||^2.$$ The proof of theorem 5 is as follows: proof of theorem 5 Recall that if $f: U \subset \mathbb{R}^n \to \mathbb{R}$ is of class $C^3$ and $\mathbf{x}_0 \in U$ is a critical point, Taylor's theorem may be expressed in the form $$f(\mathbf{x}_0 + \mathbf{h}) - f(\mathbf{x}_0) = Hf(\mathbf{x}_0)(\mathbf{h}) + R_2(\mathbf{x}_0, \mathbf{h}),$$ where $\dfrac{R_2(\mathbf{x}_0, \mathbf{h})}{|| \mathbf{h} ||^2} \to 0$ as $\mathbf{h} \to \mathbf{0}$ . Because $Hf(\mathbf{x}_0)$ is positive-definite, Lemma 1 assures us of a constant $M > 0$ such that for all $\mathbf{h} \in \mathbb{R}^n$ $$Hf(\mathbf{x}_0)(\mathbf{h}) \ge M || \mathbf{h} ||^2.$$ Because $\dfrac{R_2(\mathbf{x}_0, \mathbf{h})}{|| \mathbf{h} ||^2} \to 0$ as $\mathbf{h} \to \mathbf{0}$ , there is $\delta > 0$ such that for $0 < || \mathbf{h} || < \delta$ $$| R_2(\mathbf{x}_0, \mathbf{h}) | < M || \mathbf{h} ||^2.$$ Thus, $0 < Hf(\mathbf{x}_0)(\mathbf{h}) + \mathbf{R}_2 ( \mathbf{x}_0, \mathbf{h}) = f(\mathbf{x}_0 + \mathbf{h}) - f(\mathbf{x}_0)$ for $0 < || \mathbf{h} || < \delta$ , so that $\mathbf{x}_0$ is a relative minimum; in fact, a strict relative minimum. The proof in the negative-definite case is similar, or else follows by applying the preceding to $-f$ , and is left as an exercise. The problem I'm having with this proof is that, although I managed to follow it, I don't see how it specifically says/demonstrates anything about relative minimums or strict relative minimums. I would greatly appreciate it if people could please take the time to explain/clarify this. EDIT: For the sake of clarity, I will also include the following information: Theorem 3 Second-Order Taylor Formula Let $f: U \subset \mathbb{R}^n \to \mathbb{R}$ have continuous partial derivatives of third order. Then we may write $$f(\mathbf{x}_0 + \mathbf{h}) = f(\mathbf{x}_0) + \sum_{i = 1}^n h_i \dfrac{\partial{f}}{\partial{x_i}}(\mathbf{x}_0) + \dfrac{1}{2} \sum_{i, j = 1}^n h_i h_j \dfrac{\partial^2{f}}{\partial{x_i}\partial{x_j}}(\mathbf{x}_0) + R_2(\mathbf{x}_0, \mathbf{h}),$$ where $\dfrac{R_2(\mathbf{x}_0, \mathbf{h})}{|| \mathbf{h} ||^2} \to 0$ as $\mathbf{h} \to \mathbf{0}$ and the second sum is over all $i$ 's and $j$ 's between $1$ and $n$ (so there are $n^2$ terms). Suppose that $f: U \subset \mathbb{R}^n \to \mathbb{R}$ has second-order continuous derivatives $\dfrac{\partial^2{f}}{\partial{x_i}\partial{x_j}}(\mathbf{x}_0)$ , for $i, j = 1, \dots, n$ , at a points $\mathbf{x}_0 \in U$ . The Hessian of $f$ at $\mathbf{x}_0$ is the quadratic function defined by \begin{align} Hf(\mathbf{x}_0)(\mathbf{h}) &= \dfrac{1}{2} \sum_{i, j = 1}^n \dfrac{\partial^2{f}}{\partial{x_i}\partial{x_j}}(\mathbf{x}_0) h_i h_j \\ &= \dfrac{1}{2} [h_1, \dots, h_n] \left[\begin{matrix}\frac{\partial^2 f}{\partial x_1^2} & \ldots & \frac{\partial^2 f}{\partial x_1\partial x_n}\\ \vdots & \ddots & \vdots \\ \frac{\partial^2 f}{\partial x_n\partial x_1}& \ldots & \frac{\partial^2 f}{\partial x_n^2}\end{matrix}\right] \left[\begin{matrix} h_1 \\ \vdots \\ h_n \end{matrix}\right] \end{align} A quadratic function $g: \mathbb{R}^n \to \mathbb{R}$ is called positive-definite if $g(\mathbf{h}) \ge 0$ for all $\mathbf{h} \in \mathbb{R}^n$ and $g(\mathbf{h}) = 0$ only for $\mathbf{h} = \mathbf{0}$ . Similarly, $g$ is negative-definite if $g(\mathbf{h}) \le 0$ and $g(\mathbf{h}) = 0$ for $\mathbf{h} = \mathbf{0}$ only.","My textbook introduces the following theorem: Theorem 5 Second-Derivative Test for Local Extrema If is of class , is a critical point of , and the Hessian is positive-definite, then is a relative minimum of . Similarly, if is negative-definite, then is a relative maximum. It then goes on to say the following: Actually, we shall prove that the extrema given by this criterion are strict. A relative maximum is said to be strict if for nearby . A strict relative minimum is defined similarly. Also, the theorem is valid even if is only , but we have assumed for simplicity. The proof of theorem requires Taylor's theorem and the following result from linear algebra. Lemma 1 If is an real matrix, and if the associated quadratic function is positive-definite, then there is a constant such that for all ; The proof of theorem 5 is as follows: proof of theorem 5 Recall that if is of class and is a critical point, Taylor's theorem may be expressed in the form where as . Because is positive-definite, Lemma 1 assures us of a constant such that for all Because as , there is such that for Thus, for , so that is a relative minimum; in fact, a strict relative minimum. The proof in the negative-definite case is similar, or else follows by applying the preceding to , and is left as an exercise. The problem I'm having with this proof is that, although I managed to follow it, I don't see how it specifically says/demonstrates anything about relative minimums or strict relative minimums. I would greatly appreciate it if people could please take the time to explain/clarify this. EDIT: For the sake of clarity, I will also include the following information: Theorem 3 Second-Order Taylor Formula Let have continuous partial derivatives of third order. Then we may write where as and the second sum is over all 's and 's between and (so there are terms). Suppose that has second-order continuous derivatives , for , at a points . The Hessian of at is the quadratic function defined by A quadratic function is called positive-definite if for all and only for . Similarly, is negative-definite if and for only.","f : U \subset \mathbb{R}^n \to \mathbb{R} C^3 \mathbf{x}_0 \in U f Hf(\mathbf{x}_0) \mathbf{x}_0 f Hf(\mathbf{x}_0) \mathbf{x}_0 \mathbf{x}_0 f(\mathbf{x}) < f(\mathbf{x}_0) \mathbf{x} \not= \mathbf{x}_0 f C^2 C^3 5 B = [b_{ij}] n \times n H: \mathbb{R}^n \to \mathbb{R}, (h_1, \dots, h_n) \mapsto \dfrac{1}{2} \sum_{i, j = 1}^n b_{ij} h_i h_j M > 0 \mathbf{h} \in \mathbb{R}^n H(\mathbf{h}) \ge M || \mathbf{h} ||^2. f: U \subset \mathbb{R}^n \to \mathbb{R} C^3 \mathbf{x}_0 \in U f(\mathbf{x}_0 + \mathbf{h}) - f(\mathbf{x}_0) = Hf(\mathbf{x}_0)(\mathbf{h}) + R_2(\mathbf{x}_0, \mathbf{h}), \dfrac{R_2(\mathbf{x}_0, \mathbf{h})}{|| \mathbf{h} ||^2} \to 0 \mathbf{h} \to \mathbf{0} Hf(\mathbf{x}_0) M > 0 \mathbf{h} \in \mathbb{R}^n Hf(\mathbf{x}_0)(\mathbf{h}) \ge M || \mathbf{h} ||^2. \dfrac{R_2(\mathbf{x}_0, \mathbf{h})}{|| \mathbf{h} ||^2} \to 0 \mathbf{h} \to \mathbf{0} \delta > 0 0 < || \mathbf{h} || < \delta | R_2(\mathbf{x}_0, \mathbf{h}) | < M || \mathbf{h} ||^2. 0 < Hf(\mathbf{x}_0)(\mathbf{h}) + \mathbf{R}_2 ( \mathbf{x}_0, \mathbf{h}) = f(\mathbf{x}_0 + \mathbf{h}) - f(\mathbf{x}_0) 0 < || \mathbf{h} || < \delta \mathbf{x}_0 -f f: U \subset \mathbb{R}^n \to \mathbb{R} f(\mathbf{x}_0 + \mathbf{h}) = f(\mathbf{x}_0) + \sum_{i = 1}^n h_i \dfrac{\partial{f}}{\partial{x_i}}(\mathbf{x}_0) + \dfrac{1}{2} \sum_{i, j = 1}^n h_i h_j \dfrac{\partial^2{f}}{\partial{x_i}\partial{x_j}}(\mathbf{x}_0) + R_2(\mathbf{x}_0, \mathbf{h}), \dfrac{R_2(\mathbf{x}_0, \mathbf{h})}{|| \mathbf{h} ||^2} \to 0 \mathbf{h} \to \mathbf{0} i j 1 n n^2 f: U \subset \mathbb{R}^n \to \mathbb{R} \dfrac{\partial^2{f}}{\partial{x_i}\partial{x_j}}(\mathbf{x}_0) i, j = 1, \dots, n \mathbf{x}_0 \in U f \mathbf{x}_0 \begin{align} Hf(\mathbf{x}_0)(\mathbf{h}) &= \dfrac{1}{2} \sum_{i, j = 1}^n \dfrac{\partial^2{f}}{\partial{x_i}\partial{x_j}}(\mathbf{x}_0) h_i h_j \\ &= \dfrac{1}{2} [h_1, \dots, h_n] \left[\begin{matrix}\frac{\partial^2 f}{\partial x_1^2} & \ldots & \frac{\partial^2 f}{\partial x_1\partial x_n}\\
\vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n\partial x_1}& \ldots & \frac{\partial^2 f}{\partial x_n^2}\end{matrix}\right] \left[\begin{matrix} h_1 \\ \vdots \\ h_n
\end{matrix}\right] \end{align} g: \mathbb{R}^n \to \mathbb{R} g(\mathbf{h}) \ge 0 \mathbf{h} \in \mathbb{R}^n g(\mathbf{h}) = 0 \mathbf{h} = \mathbf{0} g g(\mathbf{h}) \le 0 g(\mathbf{h}) = 0 \mathbf{h} = \mathbf{0}","['real-analysis', 'multivariable-calculus', 'optimization', 'taylor-expansion', 'proof-explanation']"
19,Find the function given partial derivatives and points,Find the function given partial derivatives and points,,"Find $f$$(x,y)$ given $f_x$$= $$3x^2y-4y^2$ and $f_y$$=$$x^3-8xy+6y$ and $f$$(1,1)$$=$$5$ Can anyone steer me in the right direction here? I assume I have to integrate but we haven't done integration with multivariable functions yet. Any help at all is appreciated.",Find given and and Can anyone steer me in the right direction here? I assume I have to integrate but we haven't done integration with multivariable functions yet. Any help at all is appreciated.,"f(x,y) f_x= 3x^2y-4y^2 f_y=x^3-8xy+6y f(1,1)=5","['multivariable-calculus', 'partial-derivative']"
20,What's the area enclosed by $x^4-x^2y^2+y^4=1$,What's the area enclosed by,x^4-x^2y^2+y^4=1,What's the area enclosed in $x^4-x^2y^2+y^4=n$ ? The image above has $n=1$ . We can convert to polar coordinates of course. $r^4[\cos^4(\theta)-\cos^2(\theta)\sin^2(\theta)+\sin^4(\theta)]=n$ Not exactly sure how that might help us. I can approximate the area by creating some random points between $0$ and $2$ and asking how often they fall in the space enclosed by $x^4-x^2y^2+y^4=1$ . This leads me to an area of $\approx 4.3$ . Can I get an exact value for this?,What's the area enclosed in ? The image above has . We can convert to polar coordinates of course. Not exactly sure how that might help us. I can approximate the area by creating some random points between and and asking how often they fall in the space enclosed by . This leads me to an area of . Can I get an exact value for this?,x^4-x^2y^2+y^4=n n=1 r^4[\cos^4(\theta)-\cos^2(\theta)\sin^2(\theta)+\sin^4(\theta)]=n 0 2 x^4-x^2y^2+y^4=1 \approx 4.3,"['calculus', 'multivariable-calculus']"
21,Evaluate the integral with and without Green's theorem,Evaluate the integral with and without Green's theorem,,"Evaluate the line integral $\oint_C y^2dx + xdy$ when $C$ has the vector equation $\alpha(t)=(2\cos^3t)i+(2\sin^3t)j$ , $0 \leq t \leq 2\pi$ . My attempt BY USING GREEN's THEOREM, i.e., $P=y^2$ , $Q=x$ we get $\iint_c 1-2y dxdy$ Now putting $x=r\cos^3t$ and $y=r\sin^3t$ we get $dxdy=Jdrdt$ , where $J=3r \sin^2t \cos^2t$ hence $dxdy=3r \sin^2t \cos^2t drdt$ Hence the required integral is, $\int_0^{2\pi} \int_0^2 (1-2r \sin^3t)(3r \sin^2t \cos^2t)drdt$ which is equal to $\pi /4$ But the answer given in APOSTOL's is $3 \pi /2$ NOW BY NORMAL METHOD put $x=2 \cos^3t$ and $y=2 \sin^3t$ $dx=-6 \cos^2t \sin t dt$ and $dy=6 \sin^2t cost dt$ , Hence the integral becomes $-24 \int_0^{2 \pi} \sin^7t \cos^2t dt + 12 \int_0^{2 \pi} \cos^4t \sin^2t dt$ which is equal to ZERO. WHAT MISTAKE AM I DOING AND THE CORRECT ANSWER IS $3 \pi /2$ ?","Evaluate the line integral when has the vector equation , . My attempt BY USING GREEN's THEOREM, i.e., , we get Now putting and we get , where hence Hence the required integral is, which is equal to But the answer given in APOSTOL's is NOW BY NORMAL METHOD put and and , Hence the integral becomes which is equal to ZERO. WHAT MISTAKE AM I DOING AND THE CORRECT ANSWER IS ?",\oint_C y^2dx + xdy C \alpha(t)=(2\cos^3t)i+(2\sin^3t)j 0 \leq t \leq 2\pi P=y^2 Q=x \iint_c 1-2y dxdy x=r\cos^3t y=r\sin^3t dxdy=Jdrdt J=3r \sin^2t \cos^2t dxdy=3r \sin^2t \cos^2t drdt \int_0^{2\pi} \int_0^2 (1-2r \sin^3t)(3r \sin^2t \cos^2t)drdt \pi /4 3 \pi /2 x=2 \cos^3t y=2 \sin^3t dx=-6 \cos^2t \sin t dt dy=6 \sin^2t cost dt -24 \int_0^{2 \pi} \sin^7t \cos^2t dt + 12 \int_0^{2 \pi} \cos^4t \sin^2t dt 3 \pi /2,['multivariable-calculus']
22,Parametrizing the surface $x^2 = 1-z$ and $y^2 = z$,Parametrizing the surface  and,x^2 = 1-z y^2 = z,"I am given the following exercise: Find the parametrization of the surface $C: x^2 = 1 - z$ and $y^2 = z$ I got to the following answer: \begin{cases} x &= \sin (t)\\ y &= \cos (t)\\ z &= \cos^2 (t) \end{cases} Unfortunately, there's no way to evaluate if my answer is correct on the textbook. Could someone please verify if that's the case? Thank you.","I am given the following exercise: Find the parametrization of the surface $C: x^2 = 1 - z$ and $y^2 = z$ I got to the following answer: \begin{cases} x &= \sin (t)\\ y &= \cos (t)\\ z &= \cos^2 (t) \end{cases} Unfortunately, there's no way to evaluate if my answer is correct on the textbook. Could someone please verify if that's the case? Thank you.",,"['multivariable-calculus', 'parametric', 'parametrization']"
23,"Intuition of Laplacian of $f(x,y)$",Intuition of Laplacian of,"f(x,y)","I have some confusion in the physical/geometrical interpretation of the Laplacian of a function of two variables, say, $f(x,y)$ . Recently I have found a video of Khan Academy on Youtube. In this video they gave an intuition of $\nabla^2 f$ . The main theme of this video (as I've understood) is: Since the gradient gives you the slope of steepest ascent so each one of the vectors in gradient field points in the direction that you should walk, such that the graph of $f(x,y)$ is kind of a hill on top of you, it tells you the direction you should go to increase the functional value most rapidly. Now the divergence of a vector field (treating as a fluid flow) represents how much a point in the field acts like a source . So now let's think about what it might mean when we take the divergence of the gradient field of $~f$ . We can see the divergence of the gradient is very high at points that are kind of like minima, at points where everyone around them tends to be higher.But the divergence of the gradient is low at points that look more like maximum points. So this Laplacian operator is kind of like a measure of how much of a minimum point is this $(x,y)$ .You will be very positive when $f$ evaluated at that point tends to give a smaller value than $f$ evaluated at neighbors of that point. But it'll be very negative if when you evaluate $f$ at that point it tends to be bigger than its neighbors. Upto now everything is pretty much clear to me and I am so convinced with this intuition of Laplacian . But the confusion pops up in my mind when I read the second derivative test in the Multivaiable Calculus book Vol 2 of Tom M Apostol. In the second derivative test what I understand is that only $f_{xx}$ and $f_{yy}$ is not sufficient for the conclusion about a local minima or maxima. We have to take care about the mixed partials also. But in the Khan academy's video I think $\text{div}~ \text{grad} f=f_{xx}+f_{yy}$ should determine a point as local minima or maxima at least . Now my question is: if $\nabla^2f(x,y)>0$ , can we say that $f$ has a local minima at $(x,y)$ ? Which I know from second derivative test is not enough for the conclusion whereas according to the first intuitive idea of Khan academy's video it seems pretty reasonable to me. Can anyone here please help me to figure out what's going on here or at least where am I missunderstood? Thank you.","I have some confusion in the physical/geometrical interpretation of the Laplacian of a function of two variables, say, . Recently I have found a video of Khan Academy on Youtube. In this video they gave an intuition of . The main theme of this video (as I've understood) is: Since the gradient gives you the slope of steepest ascent so each one of the vectors in gradient field points in the direction that you should walk, such that the graph of is kind of a hill on top of you, it tells you the direction you should go to increase the functional value most rapidly. Now the divergence of a vector field (treating as a fluid flow) represents how much a point in the field acts like a source . So now let's think about what it might mean when we take the divergence of the gradient field of . We can see the divergence of the gradient is very high at points that are kind of like minima, at points where everyone around them tends to be higher.But the divergence of the gradient is low at points that look more like maximum points. So this Laplacian operator is kind of like a measure of how much of a minimum point is this .You will be very positive when evaluated at that point tends to give a smaller value than evaluated at neighbors of that point. But it'll be very negative if when you evaluate at that point it tends to be bigger than its neighbors. Upto now everything is pretty much clear to me and I am so convinced with this intuition of Laplacian . But the confusion pops up in my mind when I read the second derivative test in the Multivaiable Calculus book Vol 2 of Tom M Apostol. In the second derivative test what I understand is that only and is not sufficient for the conclusion about a local minima or maxima. We have to take care about the mixed partials also. But in the Khan academy's video I think should determine a point as local minima or maxima at least . Now my question is: if , can we say that has a local minima at ? Which I know from second derivative test is not enough for the conclusion whereas according to the first intuitive idea of Khan academy's video it seems pretty reasonable to me. Can anyone here please help me to figure out what's going on here or at least where am I missunderstood? Thank you.","f(x,y) \nabla^2 f f(x,y) ~f (x,y) f f f f_{xx} f_{yy} \text{div}~ \text{grad} f=f_{xx}+f_{yy} \nabla^2f(x,y)>0 f (x,y)","['multivariable-calculus', 'laplacian']"
24,Directional derivative of function with two variables containing a function and its derivative.,Directional derivative of function with two variables containing a function and its derivative.,,"Given $f(x,y)=g(3x+6y)$ and suppose $g'(15)=2$, then I would like to take the directional derivative at the point $(1,2)$ and in the direction of $u=\left(\frac12,\frac{\sqrt{3}}{2}\right)$. Normally this would not be a problem but I can not see through what is going on in the function $f$. How am I supposed to interpret it?","Given $f(x,y)=g(3x+6y)$ and suppose $g'(15)=2$, then I would like to take the directional derivative at the point $(1,2)$ and in the direction of $u=\left(\frac12,\frac{\sqrt{3}}{2}\right)$. Normally this would not be a problem but I can not see through what is going on in the function $f$. How am I supposed to interpret it?",,"['calculus', 'multivariable-calculus']"
25,"Critical points and extremum of $f(x,y)=y^2-x^2+x^3+x^2y+\frac{y^3}{3}\;\;\forall\;(x,y)\in\Bbb{R}^2$",Critical points and extremum of,"f(x,y)=y^2-x^2+x^3+x^2y+\frac{y^3}{3}\;\;\forall\;(x,y)\in\Bbb{R}^2","Let $f:\Bbb{R}^2\to \Bbb{R}$ be a function defined by \begin{align}f(x,y)=y^2-x^2+x^3+x^2y+\frac{y^3}{3}\;\;\forall\;(x,y)\in\Bbb{R}^2\end{align} $i.$ Compute the critical points of $f$ $ii.$ Does $f$ have an extremum? My work: \begin{align}\frac{\partial f}{\partial x}=-2x+3x^2+2xy\qquad (1)\end{align} \begin{align}\frac{\partial f}{\partial y}=2y+x^2+y^2\qquad (2)\end{align} At \begin{align}\frac{\partial f}{\partial x}=\frac{\partial f}{\partial y}=0\end{align} we have  \begin{align}x=\frac{1}{2}(3x^2+2xy)\end{align} Substituting into $2$, we get  \begin{align}0=y(8+12x^3)+y^2(4x^2+4)+9x^4\end{align} \begin{align}y=\frac{-2-3 x^3\pm\sqrt{4+12 x^3-9 x^4}}{2 \left(1+x^2\right)}\end{align} I don't know where to go from here. Can someone please, help?","Let $f:\Bbb{R}^2\to \Bbb{R}$ be a function defined by \begin{align}f(x,y)=y^2-x^2+x^3+x^2y+\frac{y^3}{3}\;\;\forall\;(x,y)\in\Bbb{R}^2\end{align} $i.$ Compute the critical points of $f$ $ii.$ Does $f$ have an extremum? My work: \begin{align}\frac{\partial f}{\partial x}=-2x+3x^2+2xy\qquad (1)\end{align} \begin{align}\frac{\partial f}{\partial y}=2y+x^2+y^2\qquad (2)\end{align} At \begin{align}\frac{\partial f}{\partial x}=\frac{\partial f}{\partial y}=0\end{align} we have  \begin{align}x=\frac{1}{2}(3x^2+2xy)\end{align} Substituting into $2$, we get  \begin{align}0=y(8+12x^3)+y^2(4x^2+4)+9x^4\end{align} \begin{align}y=\frac{-2-3 x^3\pm\sqrt{4+12 x^3-9 x^4}}{2 \left(1+x^2\right)}\end{align} I don't know where to go from here. Can someone please, help?",,"['calculus', 'multivariable-calculus', 'optimization', 'linear-programming', 'nonlinear-optimization']"
26,Show the optimal step to minimize $ax^tQx + b^tx + c$ has $\lambda = -\frac{d^t\nabla q(x)}{d^t\nabla^2q(x)d}$,Show the optimal step to minimize  has,ax^tQx + b^tx + c \lambda = -\frac{d^t\nabla q(x)}{d^t\nabla^2q(x)d},"If a method of descent direction with exact linear search is used to   minimize a quadratic function $q:\mathbb{R}^n\to\mathbb{R}$, show that   the optimal step is given by $$\lambda = -\frac{d^t\nabla  q(x)}{d^t\nabla^2q(x)d}$$ where $d$ is the direction used on point $x$ So I need to minimize a function of the form $$ax^tQx + b^tx + c$$ So I need to minimize $$f(x -\lambda d) = a(x -\lambda d)^tQ(x -\lambda d) + b^t(x -\lambda d) + c$$ for that I take the derivative with respect to $\lambda$ and set it equal to $0$ to get the $\lambda$ that minimizes the expression. First let's open the expression so we can take the derivative $$f(x -\lambda d) = a(x -\lambda d)^t(Qx -Q\lambda d) + b^tx -b^t\lambda d + c = \\ ax^tQx-a\lambda xQd-a\lambda d^tQx+a\lambda^2d^tQd + b^tx -b^t\lambda d + c $$ so $$f' = -axQd-ad^tQx + 2a\lambda d^tQd-b^td = 0\implies\\ 2a\lambda d^tQd = b^td+axQd + aQx$$ but this expression doesn't even have the gradient, neither the hessian. UPDATE: I found another book asking a similar exercise: This one does not have $\nabla^2$ so I guess the first is wrong? This one can be more trusted. Anyways, I still can't arrive at the answer.","If a method of descent direction with exact linear search is used to   minimize a quadratic function $q:\mathbb{R}^n\to\mathbb{R}$, show that   the optimal step is given by $$\lambda = -\frac{d^t\nabla  q(x)}{d^t\nabla^2q(x)d}$$ where $d$ is the direction used on point $x$ So I need to minimize a function of the form $$ax^tQx + b^tx + c$$ So I need to minimize $$f(x -\lambda d) = a(x -\lambda d)^tQ(x -\lambda d) + b^t(x -\lambda d) + c$$ for that I take the derivative with respect to $\lambda$ and set it equal to $0$ to get the $\lambda$ that minimizes the expression. First let's open the expression so we can take the derivative $$f(x -\lambda d) = a(x -\lambda d)^t(Qx -Q\lambda d) + b^tx -b^t\lambda d + c = \\ ax^tQx-a\lambda xQd-a\lambda d^tQx+a\lambda^2d^tQd + b^tx -b^t\lambda d + c $$ so $$f' = -axQd-ad^tQx + 2a\lambda d^tQd-b^td = 0\implies\\ 2a\lambda d^tQd = b^td+axQd + aQx$$ but this expression doesn't even have the gradient, neither the hessian. UPDATE: I found another book asking a similar exercise: This one does not have $\nabla^2$ so I guess the first is wrong? This one can be more trusted. Anyways, I still can't arrive at the answer.",,"['calculus', 'linear-algebra', 'multivariable-calculus', 'optimization', 'maxima-minima']"
27,"Multidimensional chain rule, online calculator","Multidimensional chain rule, online calculator",,"I would like to check my solutions for the derivative with the multidimensional chain rule, and I would like to do that online with a calculator. But I can not find one and I do not know how to use wolframalpha for it, which I am sure that it is capable of doing those. I am talking about tasks like this for example: $f:\mathbb{R}^2\to\mathbb{R}$, $f(u,v)=u^2+v^2$, $g:\mathbb{R}\to\mathbb{R}^2$, $g(t)=(e^t, t^2)$ And then derive $D(f\circ g)(t)$. Do you know an online program, which can I use to check my solution, or how I can use wolframalpha for it? Thanks in advance.","I would like to check my solutions for the derivative with the multidimensional chain rule, and I would like to do that online with a calculator. But I can not find one and I do not know how to use wolframalpha for it, which I am sure that it is capable of doing those. I am talking about tasks like this for example: $f:\mathbb{R}^2\to\mathbb{R}$, $f(u,v)=u^2+v^2$, $g:\mathbb{R}\to\mathbb{R}^2$, $g(t)=(e^t, t^2)$ And then derive $D(f\circ g)(t)$. Do you know an online program, which can I use to check my solution, or how I can use wolframalpha for it? Thanks in advance.",,"['multivariable-calculus', 'wolfram-alpha', 'online-resources', 'calculator']"
28,Application of Implicit Function Theorem: Problem 2-40 from Spivak's Calculus on Manifolds,Application of Implicit Function Theorem: Problem 2-40 from Spivak's Calculus on Manifolds,,"At the end of the section on Implicit Functions in Spivak's Calculus on Manifolds , we have Problem 2-40: Problem 2-40. Use the implicit function theorem to re-do Problem 2-15(c). And, Problem 2-15(c) is: Problem 2-15(c). If $\det(a_{ij}(t)) \neq 0$ for all $t$ and $b_1,\dots,b_n : \mathbb{R} \to \mathbb{R}$ are differentiable, let $s_1,\dots,s_n : \mathbb{R} \to \mathbb{R}$ be the functions such that $s_1(t),\dots,s_n(t)$ are the solutions of the equations   $$ \sum_{j=1}^n a_{ji}(t) s_j(t) = b_i(t) \qquad i = 1,\dots,n. $$   Show that $s_i$ is differentiable and find ${s_i}'(t)$. We assume that the functions $a_{ij}$ and $b_i$ are all continuously differentiable. I believe I have solved this problem correctly, but I am not sure that my method is what is expected. One reason is that the final expression for ${s_i}'(t)$ appears awkward. Another reason is that this expression is also quite different from the one I arrived at when solving Problem 2-15(c) earlier without using the implicit function theorem. My previous solution is given in an answer to this question: Differentiation of solution to time-dependent system of equations: Problem 2-15(c) from Spivak's Calculus on Manifolds . It would be great if someone can go through my solution below and give me comments or suggestions. Thanks. Soln. Let $f: \mathbb{R} \times \mathbb{R}^n \to \mathbb{R}^n$ be defined by $$ f^i(t,x) = -b_i(t) + \sum_{j=1}^n a_{ji}(t) x^j, \qquad 1 \leq i \leq n. $$ Clearly $f$ is continuously differentiable. The $n \times n$ matrix $M(t,x) = \left(D_{1+j} f^i(t,x)\right)$ equals $\left(a_{ji}(t)\right)$. So, if $t_0 \in \mathbb{R}$ and $x_0 \in \mathbb{R}^n$ such that $f(t_0,x_0) = 0$, then $$ \det M(t_0,x_0) = \det \left(D_{1+j}f^i(t_0,x_0) \right) = \det \left(a_{ji}(t_0) \right) \neq 0. $$ Therefore, by the implicit function theorem, there exists a function $g : \mathbb{R} \to \mathbb{R}^n$ such that $f(t,g(t)) = 0$ for all $t \in (t_0 - \epsilon, t_0 + \epsilon)$, for some $\epsilon > 0$. But, this means that $g^1(t),\dots,g^n(t)$ is a solution to the system of equations $$ \sum_{j=1}^n a_{ji}(t) s_j(t) = b_i(t), \qquad i = 1,\dots,n. $$ Since the solution is unique for every $t \in (t_0 - \epsilon, t_0 + \epsilon)$, $g^i(t) = s_i(t)$ in this interval. But since $t_0 \in \mathbb{R}$ is arbitrary, we have that $f(t,s_1(t),\dots,s_n(t)) = 0$ for all $t \in \mathbb{R}$. Let $s : \mathbb{R} \to \mathbb{R}^n$ be defined by $s(t) = (s_1(t),\dots,s_n(t))$. By the implicit function theorem, $s$ is differentiable, and so the $s_i$ are differentiable. To calculate ${s_i}'(t)$, we will take the derivative on both sides of the expression $f^i(t,s(t)) = 0$, for each $1 \leq i \leq n$. This gives us $$ 0 = D_1 f^i(t,s(t)) + \sum_{k = 1}^n D_{1+k} f^i(t,s(t)) \cdot D_1 s_k(t), \qquad 1 \leq i \leq n. $$ This is precisely the system of equations given by $$ M(t,s(t)) \cdot s'(t) = B(t), $$ where $B(t)$ is an $n \times 1$ matrix with the entry in the $i$th row equal to $$ {b_i}'(t) - \sum_{j=1}^n {a_{ji}}'(t)s_j(t). $$ Since $M(t,s(t)) = \left( a_{ji}(t) \right)$ and $\det \left( a_{ji}(t) \right) \neq 0$ for all $t \in \mathbb{R}$, $M(t,s(t))$ is invertible. So, $$ s'(t) = \left( a_{ji}(t) \right)^{-1} B(t). \tag*{$\blacksquare$} $$ Is the final expression the best one can do using the implicit function theorem? I haven't found the precise expression for each ${s_i}'(t)$ through this method. It feels like I was able to arrive at a more concrete result without using the implicit function theorem as in my previous attempt . Again, any comments or suggestions are appreciated.","At the end of the section on Implicit Functions in Spivak's Calculus on Manifolds , we have Problem 2-40: Problem 2-40. Use the implicit function theorem to re-do Problem 2-15(c). And, Problem 2-15(c) is: Problem 2-15(c). If $\det(a_{ij}(t)) \neq 0$ for all $t$ and $b_1,\dots,b_n : \mathbb{R} \to \mathbb{R}$ are differentiable, let $s_1,\dots,s_n : \mathbb{R} \to \mathbb{R}$ be the functions such that $s_1(t),\dots,s_n(t)$ are the solutions of the equations   $$ \sum_{j=1}^n a_{ji}(t) s_j(t) = b_i(t) \qquad i = 1,\dots,n. $$   Show that $s_i$ is differentiable and find ${s_i}'(t)$. We assume that the functions $a_{ij}$ and $b_i$ are all continuously differentiable. I believe I have solved this problem correctly, but I am not sure that my method is what is expected. One reason is that the final expression for ${s_i}'(t)$ appears awkward. Another reason is that this expression is also quite different from the one I arrived at when solving Problem 2-15(c) earlier without using the implicit function theorem. My previous solution is given in an answer to this question: Differentiation of solution to time-dependent system of equations: Problem 2-15(c) from Spivak's Calculus on Manifolds . It would be great if someone can go through my solution below and give me comments or suggestions. Thanks. Soln. Let $f: \mathbb{R} \times \mathbb{R}^n \to \mathbb{R}^n$ be defined by $$ f^i(t,x) = -b_i(t) + \sum_{j=1}^n a_{ji}(t) x^j, \qquad 1 \leq i \leq n. $$ Clearly $f$ is continuously differentiable. The $n \times n$ matrix $M(t,x) = \left(D_{1+j} f^i(t,x)\right)$ equals $\left(a_{ji}(t)\right)$. So, if $t_0 \in \mathbb{R}$ and $x_0 \in \mathbb{R}^n$ such that $f(t_0,x_0) = 0$, then $$ \det M(t_0,x_0) = \det \left(D_{1+j}f^i(t_0,x_0) \right) = \det \left(a_{ji}(t_0) \right) \neq 0. $$ Therefore, by the implicit function theorem, there exists a function $g : \mathbb{R} \to \mathbb{R}^n$ such that $f(t,g(t)) = 0$ for all $t \in (t_0 - \epsilon, t_0 + \epsilon)$, for some $\epsilon > 0$. But, this means that $g^1(t),\dots,g^n(t)$ is a solution to the system of equations $$ \sum_{j=1}^n a_{ji}(t) s_j(t) = b_i(t), \qquad i = 1,\dots,n. $$ Since the solution is unique for every $t \in (t_0 - \epsilon, t_0 + \epsilon)$, $g^i(t) = s_i(t)$ in this interval. But since $t_0 \in \mathbb{R}$ is arbitrary, we have that $f(t,s_1(t),\dots,s_n(t)) = 0$ for all $t \in \mathbb{R}$. Let $s : \mathbb{R} \to \mathbb{R}^n$ be defined by $s(t) = (s_1(t),\dots,s_n(t))$. By the implicit function theorem, $s$ is differentiable, and so the $s_i$ are differentiable. To calculate ${s_i}'(t)$, we will take the derivative on both sides of the expression $f^i(t,s(t)) = 0$, for each $1 \leq i \leq n$. This gives us $$ 0 = D_1 f^i(t,s(t)) + \sum_{k = 1}^n D_{1+k} f^i(t,s(t)) \cdot D_1 s_k(t), \qquad 1 \leq i \leq n. $$ This is precisely the system of equations given by $$ M(t,s(t)) \cdot s'(t) = B(t), $$ where $B(t)$ is an $n \times 1$ matrix with the entry in the $i$th row equal to $$ {b_i}'(t) - \sum_{j=1}^n {a_{ji}}'(t)s_j(t). $$ Since $M(t,s(t)) = \left( a_{ji}(t) \right)$ and $\det \left( a_{ji}(t) \right) \neq 0$ for all $t \in \mathbb{R}$, $M(t,s(t))$ is invertible. So, $$ s'(t) = \left( a_{ji}(t) \right)^{-1} B(t). \tag*{$\blacksquare$} $$ Is the final expression the best one can do using the implicit function theorem? I haven't found the precise expression for each ${s_i}'(t)$ through this method. It feels like I was able to arrive at a more concrete result without using the implicit function theorem as in my previous attempt . Again, any comments or suggestions are appreciated.",,"['multivariable-calculus', 'proof-verification']"
29,Evaluate Integral Using Stokes Theorem,Evaluate Integral Using Stokes Theorem,,"Using Stokes theorem evaluate $$\int_\Gamma(z-y)\,dx-(x+z)\,dy-(x+y)\,dz$$ where $\Gamma$ is the intersection of $x^2+y^2+z^2=4$ with the plane $z=y$ with anticlockwise orientation when looking on $z$ from the positive side If we take $\nabla\times F=(0,2,0)$ but how can we find $\hat{n}$","Using Stokes theorem evaluate $$\int_\Gamma(z-y)\,dx-(x+z)\,dy-(x+y)\,dz$$ where $\Gamma$ is the intersection of $x^2+y^2+z^2=4$ with the plane $z=y$ with anticlockwise orientation when looking on $z$ from the positive side If we take $\nabla\times F=(0,2,0)$ but how can we find $\hat{n}$",,"['calculus', 'multivariable-calculus', 'stokes-theorem']"
30,Find surface area of $z=x+3$ with $x^2+y^2\leq 1$,Find surface area of  with,z=x+3 x^2+y^2\leq 1,"Find the surface area of $z=x+3$ with $\{(x,y,z)\mid x^2+y^2\leq 1\}$ So we first look at the projection of $\phi(x,y)=(x,y,x+3)$ on $xy$ Then area element is $\sqrt{1+f_x^2+f_y^2}=\sqrt{1+1^2+0^2}=\sqrt{2}$ $$\sqrt{2}\int\int dx\,dy$$ $x=r \cos t,y=r \sin t$ So $$\sqrt{2}\int_{0}^{1}\int_{0}^{2\pi} r \,dt\,dr=\sqrt{2}\pi$$ Is it correct? can we use Green/Stokes to solve it?","Find the surface area of $z=x+3$ with $\{(x,y,z)\mid x^2+y^2\leq 1\}$ So we first look at the projection of $\phi(x,y)=(x,y,x+3)$ on $xy$ Then area element is $\sqrt{1+f_x^2+f_y^2}=\sqrt{1+1^2+0^2}=\sqrt{2}$ $$\sqrt{2}\int\int dx\,dy$$ $x=r \cos t,y=r \sin t$ So $$\sqrt{2}\int_{0}^{1}\int_{0}^{2\pi} r \,dt\,dr=\sqrt{2}\pi$$ Is it correct? can we use Green/Stokes to solve it?",,"['calculus', 'multivariable-calculus', 'surface-integrals']"
31,Fubini's Theorem about double integration in polar coordinates,Fubini's Theorem about double integration in polar coordinates,,"So I am studying James Stewart's Calculus 8th edition and Fubini's theorem is defined in the following way... If f is continuous on the rectangle R = {(x, y) | a ≤ x ≤ b, c ≤ y ≤ d}.  Then the integral over this region R is... $$\int\int_Rf(x, y)\space dA =\int_a^b\int_c^df(x,y) \space dydx = \int_c^d\int_a^bf(x,y) \space dxdy$$ Which I think means that the double integral over a rectangular region can be calculated using a iterated integral. This makes sense. However, in a later section it derives the method of computing double integrals over regions defined by polar coordinates. The following is where I am struggling to understand their logic. So they discuss defining circular regions using polar coordinates and splitting it up into m x n different regions referred to as 'polar rectangles'. Similar to this image here, From there, they take a sample point within each polar rectangle and multiply the value of the function at that sample point by the area of the polar rectangle. They do this for all of the polar rectangles in the region  and sum them together thus arriving at the following Riemann sum. $$\sum_{i=1}^{m}\sum_{j=1}^{n}f(r_i^*\cosθ_j^*,r_i^*\sinθ_j^*)ΔA_i= \sum_{i=1}^{m}\sum_{j=1}^{n}f(r_i^*\cosθ_j^*,r_i^*\sinθ_j^*)r_i^*ΔrΔθ$$ Where $$ΔA_i = r_i^*ΔrΔθ$$ is the area of these polar rectangles. From there they make the substitution, $$g(r, \theta) = rf(r\cos\theta, r\sin\theta)$$ So the above Riemann sum becomes, $$\sum_{i=1}^{m}\sum_{j=1}^{n}g(r_i^*, \theta_j^*) ΔrΔ\theta$$ Which is, with no explanation and presumably after taking the limit of this sum as n and m tend to infinity, equated to the double integral, $$\int_\alpha^\beta\int_a^bg(r, \theta)\space drd\theta$$ Where the region in polar coordinates is defined as $$R = \{(r, \theta) \space |\space \alpha ≤ \theta ≤ \beta , a ≤ r ≤ b\} $$ So finally my question is how can Fubini's theorem be used to relate the above Riemann sum to the iterated integral of g when the theorem states that the region must be a rectangle. My intuition tells me that even though the polar region is not a rectangle, it is contained between constant numbers (like a rectangle) so perhaps Fubini's theorem will still hold.","So I am studying James Stewart's Calculus 8th edition and Fubini's theorem is defined in the following way... If f is continuous on the rectangle R = {(x, y) | a ≤ x ≤ b, c ≤ y ≤ d}.  Then the integral over this region R is... $$\int\int_Rf(x, y)\space dA =\int_a^b\int_c^df(x,y) \space dydx = \int_c^d\int_a^bf(x,y) \space dxdy$$ Which I think means that the double integral over a rectangular region can be calculated using a iterated integral. This makes sense. However, in a later section it derives the method of computing double integrals over regions defined by polar coordinates. The following is where I am struggling to understand their logic. So they discuss defining circular regions using polar coordinates and splitting it up into m x n different regions referred to as 'polar rectangles'. Similar to this image here, From there, they take a sample point within each polar rectangle and multiply the value of the function at that sample point by the area of the polar rectangle. They do this for all of the polar rectangles in the region  and sum them together thus arriving at the following Riemann sum. $$\sum_{i=1}^{m}\sum_{j=1}^{n}f(r_i^*\cosθ_j^*,r_i^*\sinθ_j^*)ΔA_i= \sum_{i=1}^{m}\sum_{j=1}^{n}f(r_i^*\cosθ_j^*,r_i^*\sinθ_j^*)r_i^*ΔrΔθ$$ Where $$ΔA_i = r_i^*ΔrΔθ$$ is the area of these polar rectangles. From there they make the substitution, $$g(r, \theta) = rf(r\cos\theta, r\sin\theta)$$ So the above Riemann sum becomes, $$\sum_{i=1}^{m}\sum_{j=1}^{n}g(r_i^*, \theta_j^*) ΔrΔ\theta$$ Which is, with no explanation and presumably after taking the limit of this sum as n and m tend to infinity, equated to the double integral, $$\int_\alpha^\beta\int_a^bg(r, \theta)\space drd\theta$$ Where the region in polar coordinates is defined as $$R = \{(r, \theta) \space |\space \alpha ≤ \theta ≤ \beta , a ≤ r ≤ b\} $$ So finally my question is how can Fubini's theorem be used to relate the above Riemann sum to the iterated integral of g when the theorem states that the region must be a rectangle. My intuition tells me that even though the polar region is not a rectangle, it is contained between constant numbers (like a rectangle) so perhaps Fubini's theorem will still hold.",,"['multivariable-calculus', 'polar-coordinates', 'multiple-integral']"
32,Leibniz rule for double integral,Leibniz rule for double integral,,"I'm attempting to differentiate the following double integral with respect to $u$: $$I(u) = \int_a^u\int_b^v [(y-u) + (v - x)] f(x,y)\,dx \,dy$$ where $f(x,y)$ is the joint density function of RV $X$ and $Y$. I'm trying to find $I'(u)$, that is, the derivative of $I$ with respect to $u$. I know this involves Leibniz integral rule , however I'm getting stuck on the double integral part. If I let $g(x,y,u) = \int_b^v [(y-u) + (v - x)] f(x,y)\,dx$ then I think the problem to solve is: $$\frac{d}{du}\bigg(\int_a^ug(u,x,y)\,dy\bigg) = g(u,x,u) + \int_a^u\frac{\partial}{\partial u}g(u,x,y)\,dy$$ However I'm getting stuck evaluating the two expressions on the RHS. Any help?","I'm attempting to differentiate the following double integral with respect to $u$: $$I(u) = \int_a^u\int_b^v [(y-u) + (v - x)] f(x,y)\,dx \,dy$$ where $f(x,y)$ is the joint density function of RV $X$ and $Y$. I'm trying to find $I'(u)$, that is, the derivative of $I$ with respect to $u$. I know this involves Leibniz integral rule , however I'm getting stuck on the double integral part. If I let $g(x,y,u) = \int_b^v [(y-u) + (v - x)] f(x,y)\,dx$ then I think the problem to solve is: $$\frac{d}{du}\bigg(\int_a^ug(u,x,y)\,dy\bigg) = g(u,x,u) + \int_a^u\frac{\partial}{\partial u}g(u,x,y)\,dy$$ However I'm getting stuck evaluating the two expressions on the RHS. Any help?",,"['calculus', 'integration', 'multivariable-calculus', 'derivatives']"
33,"find extrema of $f(x,y,z)=z$ with domain",find extrema of  with domain,"f(x,y,z)=z","$D=\{(y^2+z^2)/6\le x,x^2+y^2=z^2+16\}$ in $\mathbb R$ on $f:D \to R ,f(x,y,z)=z$ First off just with a brief look at my function I can say that there are no critical points ($f_x=0,f_y=0,f_z=1$), (morover its an open set which means that the max and min cannot occur ? correct me if I'm wrong) I can now analyze the domain $D$, what I can say is that : $(y^2+z^2)/6\le x\Longrightarrow$ It's a kind of Paraboloid $x^2+y^2=z^2+16 \Longrightarrow$ It's a Hyperboloid of One Sheet If I want to find the boundary of $D$, I need to put those two equations in a system, finding: $x^2-2z^2+6x-16=0$ which is the intersection between those two surfaces. Now in order to find possible min/max points, I can use the Lagrange multiplier system with  $x^2-2z^2+6x-16=0$ as a constraint. $$\lambda (2x+6)=0$$ $$0=0$$ $$1+\lambda (-2z)=0$$ $$x^2-2z^2+6x-16=0$$ From the first equation I can say that it is TRUE for $\lambda = 0$ or $x=-3$ , but $\lambda$ cannot be zero becouse It doesn't satisfy the third equation . What I can do instead is using $x=-3$ in the forth equation , but here is the problem : I remain with $-z^2=25$ and I conclude that I didn't find any points. Even if I use the rhird equation finding $z$ and putting it inside the forth equation it still gives me something like $\lambda^2 $  equal to a negative number. Where did I make the mistake? (maybe the boundary ?)","$D=\{(y^2+z^2)/6\le x,x^2+y^2=z^2+16\}$ in $\mathbb R$ on $f:D \to R ,f(x,y,z)=z$ First off just with a brief look at my function I can say that there are no critical points ($f_x=0,f_y=0,f_z=1$), (morover its an open set which means that the max and min cannot occur ? correct me if I'm wrong) I can now analyze the domain $D$, what I can say is that : $(y^2+z^2)/6\le x\Longrightarrow$ It's a kind of Paraboloid $x^2+y^2=z^2+16 \Longrightarrow$ It's a Hyperboloid of One Sheet If I want to find the boundary of $D$, I need to put those two equations in a system, finding: $x^2-2z^2+6x-16=0$ which is the intersection between those two surfaces. Now in order to find possible min/max points, I can use the Lagrange multiplier system with  $x^2-2z^2+6x-16=0$ as a constraint. $$\lambda (2x+6)=0$$ $$0=0$$ $$1+\lambda (-2z)=0$$ $$x^2-2z^2+6x-16=0$$ From the first equation I can say that it is TRUE for $\lambda = 0$ or $x=-3$ , but $\lambda$ cannot be zero becouse It doesn't satisfy the third equation . What I can do instead is using $x=-3$ in the forth equation , but here is the problem : I remain with $-z^2=25$ and I conclude that I didn't find any points. Even if I use the rhird equation finding $z$ and putting it inside the forth equation it still gives me something like $\lambda^2 $  equal to a negative number. Where did I make the mistake? (maybe the boundary ?)",,"['calculus', 'multivariable-calculus', 'lagrange-multiplier']"
34,Doubt in The inverse function theorem of Rudin's Principles of mathematical Analysis (Theorem 9.24),Doubt in The inverse function theorem of Rudin's Principles of mathematical Analysis (Theorem 9.24),,"Theorem 9.24: Suppose $\textbf{f}$ is a $C'-$ mapping of an open set $E \subset \mathbb{R^n}$ into $\mathbb{R^n}$.$\textbf{f} \thinspace^\prime\textbf{ (a)}$ is invertible for some $\textbf{a}\in E$ and $\textbf{b=f(a)}$ then (a) there exist open sets $U$ and $V$ in $\mathbb{R^n }$ such that $\textbf{a}\in U, \textbf{b}\in V,\textbf{f}$ is one-to one on $U$ and $\textbf{f}(U)=V$ (b) if $\textbf{g}$ is the inverse of $\textbf{f}$[which exist , by (a)], definedin $V$ by $\textbf{g(f(x))=x }$$\space $ $(\textbf{x}\in U)$ then $\textbf{g}\in C'(V)$ To prove (a) Rudin took a function $\varphi$ which is defined as to each $\textbf{y}\in \mathbb{R^n },\varphi(\textbf{x})=\textbf{x}+A^{-1}(\textbf{y}\thinspace \textbf{-f(x))}\space \space (\textbf{x}\in E )$(Equation 48) Then he states that $\varphi '(\textbf{x})=I- A^{-1}\textbf{f}\space'(x)$.I didn't got how rudin found  $\varphi '(\textbf{x})$.Please help me to understand.","Theorem 9.24: Suppose $\textbf{f}$ is a $C'-$ mapping of an open set $E \subset \mathbb{R^n}$ into $\mathbb{R^n}$.$\textbf{f} \thinspace^\prime\textbf{ (a)}$ is invertible for some $\textbf{a}\in E$ and $\textbf{b=f(a)}$ then (a) there exist open sets $U$ and $V$ in $\mathbb{R^n }$ such that $\textbf{a}\in U, \textbf{b}\in V,\textbf{f}$ is one-to one on $U$ and $\textbf{f}(U)=V$ (b) if $\textbf{g}$ is the inverse of $\textbf{f}$[which exist , by (a)], definedin $V$ by $\textbf{g(f(x))=x }$$\space $ $(\textbf{x}\in U)$ then $\textbf{g}\in C'(V)$ To prove (a) Rudin took a function $\varphi$ which is defined as to each $\textbf{y}\in \mathbb{R^n },\varphi(\textbf{x})=\textbf{x}+A^{-1}(\textbf{y}\thinspace \textbf{-f(x))}\space \space (\textbf{x}\in E )$(Equation 48) Then he states that $\varphi '(\textbf{x})=I- A^{-1}\textbf{f}\space'(x)$.I didn't got how rudin found  $\varphi '(\textbf{x})$.Please help me to understand.",,"['multivariable-calculus', 'derivatives', 'proof-explanation']"
35,Calculating a flux integral,Calculating a flux integral,,"Let $$F=(xe^{xy}-2xz+2xy\cos^2 z, y^2\sin^2 z-y e^{xy}+y, x^2+y^2+z^2)$$ and $V$ be the solid in space bounded by $z=9-x^2-y^2$ and $z=0$. I am trying to compute the flux integral $\iint_{\partial V}F\cdot n \ dS$, $n$ being the outward unit normal. Setting $r(x,y)=(x,y,9-x^2-y^2)$, I found that $r_x\times r_v=(2x,2y,1)$ and $$\iint_{\partial V}F\cdot n \ dS=\iint_D F\cdot r_x\times r_y \ dA$$ where $$F\cdot r_x\times r_y=2e^{xy}(x^2-y^2)-36x^2+4x^4+4x^2y^2+4x^2y\cos^2(9-x^2-y^2)+2y^3\sin^2(9-x^2-y^2)+2y^2;$$ after the substitution $x=r\cos t, \ y=r\sin t$ I get $$f(r,t)=r(F\cdot r_x\times r)=2r^3e^{r^2\sin t \cos t}(\cos^2 t-\sin^2 t)-36r^3\cos^2 4r^5\cos^2 t + \\4r^4\cos^2 t\sin t \cos^2{(9-r^2)}+2r^4\sin^3 t \sin^2(9-r^2)+2r^3\sin^2 t $$ and I need to compute $$\int_0^{2\pi}\int_0^3 f(r,t)drdt$$ I wonder whether I can further simplify $f(r,t)$? The current expression looks to cumbersome and it seems like a hassle to compute the integral if no simplifications can be made.","Let $$F=(xe^{xy}-2xz+2xy\cos^2 z, y^2\sin^2 z-y e^{xy}+y, x^2+y^2+z^2)$$ and $V$ be the solid in space bounded by $z=9-x^2-y^2$ and $z=0$. I am trying to compute the flux integral $\iint_{\partial V}F\cdot n \ dS$, $n$ being the outward unit normal. Setting $r(x,y)=(x,y,9-x^2-y^2)$, I found that $r_x\times r_v=(2x,2y,1)$ and $$\iint_{\partial V}F\cdot n \ dS=\iint_D F\cdot r_x\times r_y \ dA$$ where $$F\cdot r_x\times r_y=2e^{xy}(x^2-y^2)-36x^2+4x^4+4x^2y^2+4x^2y\cos^2(9-x^2-y^2)+2y^3\sin^2(9-x^2-y^2)+2y^2;$$ after the substitution $x=r\cos t, \ y=r\sin t$ I get $$f(r,t)=r(F\cdot r_x\times r)=2r^3e^{r^2\sin t \cos t}(\cos^2 t-\sin^2 t)-36r^3\cos^2 4r^5\cos^2 t + \\4r^4\cos^2 t\sin t \cos^2{(9-r^2)}+2r^4\sin^3 t \sin^2(9-r^2)+2r^3\sin^2 t $$ and I need to compute $$\int_0^{2\pi}\int_0^3 f(r,t)drdt$$ I wonder whether I can further simplify $f(r,t)$? The current expression looks to cumbersome and it seems like a hassle to compute the integral if no simplifications can be made.",,"['calculus', 'real-analysis', 'integration', 'multivariable-calculus']"
36,When does Order of Second Partial Derivatives Matter? [duplicate],When does Order of Second Partial Derivatives Matter? [duplicate],,"This question already has an answer here : When does order of partial derivatives matter? (1 answer) Closed 5 years ago . My professor was saying that, for a function of multiple variables, usually the order in which you take the order of partial derivatives did not matter. (ex: $f_{xy} = f_{yx}$). Under what circumstances is this not true?","This question already has an answer here : When does order of partial derivatives matter? (1 answer) Closed 5 years ago . My professor was saying that, for a function of multiple variables, usually the order in which you take the order of partial derivatives did not matter. (ex: $f_{xy} = f_{yx}$). Under what circumstances is this not true?",,['multivariable-calculus']
37,Sketch of Spivaks proof of the Inverse Function Theorem for Multivariable functions,Sketch of Spivaks proof of the Inverse Function Theorem for Multivariable functions,,"Below is the proof of the inverse function theorem for multivariable functions given in Spivaks ""Calculus on Manifolds"".  I have made a few interpretations regarding his reasoning and want to know if I have interpreted correctly. He spends the majority of the first part of the proof proving that $f$ is bijective, as this implies the existence of an inverse. The only time we use that $Df(a)=I$ is when we define $\mu=Df(a)$ and write $\mu^{-1}\big(f(x_1)-f(x)\big)=\mu^{-1}\big(\mu(x_1-x)+\phi(x_1-x)\big)$ $=$ $\mu^{-1}\big(\mu(x_1-x)\big)+\mu^{-1}\big(\phi(x_1-x)\big)$, since $\mu=I$ implies that $\mu^{-1}=\mu=I$ ,which gives that $\mu^{-1}$ is linear.  Besides that I don't see any other application of the property $Df(a)=I$ we established in the opening lines of the proof. $detf'(a)\neq 0$ is only used in the proof to guarantee that there exists a neighborhood around $f'(a)$ such that every element is non-zero; and then this implication is used to show that we must have $y^{i}-f^i(x)=0$, $\forall i$ Thanks in advance!","Below is the proof of the inverse function theorem for multivariable functions given in Spivaks ""Calculus on Manifolds"".  I have made a few interpretations regarding his reasoning and want to know if I have interpreted correctly. He spends the majority of the first part of the proof proving that $f$ is bijective, as this implies the existence of an inverse. The only time we use that $Df(a)=I$ is when we define $\mu=Df(a)$ and write $\mu^{-1}\big(f(x_1)-f(x)\big)=\mu^{-1}\big(\mu(x_1-x)+\phi(x_1-x)\big)$ $=$ $\mu^{-1}\big(\mu(x_1-x)\big)+\mu^{-1}\big(\phi(x_1-x)\big)$, since $\mu=I$ implies that $\mu^{-1}=\mu=I$ ,which gives that $\mu^{-1}$ is linear.  Besides that I don't see any other application of the property $Df(a)=I$ we established in the opening lines of the proof. $detf'(a)\neq 0$ is only used in the proof to guarantee that there exists a neighborhood around $f'(a)$ such that every element is non-zero; and then this implication is used to show that we must have $y^{i}-f^i(x)=0$, $\forall i$ Thanks in advance!",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'proof-verification']"
38,integral of a function in a curve,integral of a function in a curve,,"let $F=\left(\frac{-y}{x^2+y^2},\frac{x}{x^2+y^2}\right)$ and $R(t)=(\cos t,\sin t)$ (the curve is a circle with radius 1) now: \begin{equation} \int_{R}F_1.dx+F_2.dy = \int_{0}^{2\pi}-\sin t\ dt  + \int_{0}^{2\pi}\cos t\ dt  = 0 \end{equation} but the book concluded from the Green's theorem that the answer is $2\pi$ (the book take a neighborhood near $0$ (because $0$ is not in the domain) and ...) so why my answer is wrong and can you give me a solution because I can't understand the solution in the book","let $F=\left(\frac{-y}{x^2+y^2},\frac{x}{x^2+y^2}\right)$ and $R(t)=(\cos t,\sin t)$ (the curve is a circle with radius 1) now: \begin{equation} \int_{R}F_1.dx+F_2.dy = \int_{0}^{2\pi}-\sin t\ dt  + \int_{0}^{2\pi}\cos t\ dt  = 0 \end{equation} but the book concluded from the Green's theorem that the answer is $2\pi$ (the book take a neighborhood near $0$ (because $0$ is not in the domain) and ...) so why my answer is wrong and can you give me a solution because I can't understand the solution in the book",,"['integration', 'multivariable-calculus', 'greens-theorem']"
39,"Find $\iiint_R(x^2+y^2+z^2) \,dV$, where $R$ is the region that lies above the cone $z=c\sqrt{x^2+y^2}$ and inside the sphere $x^2+y^2+z^2=a^2$.","Find , where  is the region that lies above the cone  and inside the sphere .","\iiint_R(x^2+y^2+z^2) \,dV R z=c\sqrt{x^2+y^2} x^2+y^2+z^2=a^2","Find the integral $$\iiint_R(x^2+y^2+z^2) \,dV$$ where $R$ is the region that lies above the cone $z=c\sqrt{x^2+y^2}$ and inside the sphere $x^2+y^2+z^2=a^2$. (Here a and c are positive constants.) Simplify as far as possible. I think I'm really close to the answer but can't seem to make it the last tiny part. $$\iiint_R(x^2+y^2+z^2) \,dV = \int_0^adR\int d\phi \int_0^{2\pi}R^4\sin(\phi) d\theta$$ And also, $z = c\sqrt{x^2+y^2} = cR\sin(\phi)$. The only bit I've been stuck at for a long time now is: what should the integral bounds be for the $\int d\phi$? If something is wrong or not in the right direction, please correct me. Thanks.","Find the integral $$\iiint_R(x^2+y^2+z^2) \,dV$$ where $R$ is the region that lies above the cone $z=c\sqrt{x^2+y^2}$ and inside the sphere $x^2+y^2+z^2=a^2$. (Here a and c are positive constants.) Simplify as far as possible. I think I'm really close to the answer but can't seem to make it the last tiny part. $$\iiint_R(x^2+y^2+z^2) \,dV = \int_0^adR\int d\phi \int_0^{2\pi}R^4\sin(\phi) d\theta$$ And also, $z = c\sqrt{x^2+y^2} = cR\sin(\phi)$. The only bit I've been stuck at for a long time now is: what should the integral bounds be for the $\int d\phi$? If something is wrong or not in the right direction, please correct me. Thanks.",,"['multivariable-calculus', 'multiple-integral']"
40,"If a scalar field is continuous at a point, then is it also differentiable at that point?","If a scalar field is continuous at a point, then is it also differentiable at that point?",,"Let $S$ be a non-empty subset of $\mathbb{R}^n$, let $\mathbf{a}$ be an interior point of $S$, and let $f \colon S \to \mathbb{R}$ be a scalar field such that $f$ is continuous at $\mathbf{a}$. Then is $f$ also differentiable at $\mathbf{a}$? I know that the answer to this question is in the negative if $n= 1$. For example the function $f \colon \mathbb{R} \to \mathbb{R}$ defined by $$ f(x) \colon= \lvert x \rvert \ \mbox{ for all } x \in \mathbb{R} $$ is continuous at $x=0$ but is not differentiable at that point. What if $n > 1$? I also know that if $f$ is differentiable at $\mathbf{a}$, then $f$ is also continuous at $\mathbf{a}$.","Let $S$ be a non-empty subset of $\mathbb{R}^n$, let $\mathbf{a}$ be an interior point of $S$, and let $f \colon S \to \mathbb{R}$ be a scalar field such that $f$ is continuous at $\mathbf{a}$. Then is $f$ also differentiable at $\mathbf{a}$? I know that the answer to this question is in the negative if $n= 1$. For example the function $f \colon \mathbb{R} \to \mathbb{R}$ defined by $$ f(x) \colon= \lvert x \rvert \ \mbox{ for all } x \in \mathbb{R} $$ is continuous at $x=0$ but is not differentiable at that point. What if $n > 1$? I also know that if $f$ is differentiable at $\mathbf{a}$, then $f$ is also continuous at $\mathbf{a}$.",,"['calculus', 'real-analysis', 'multivariable-calculus', 'derivatives', 'continuity']"
41,Why is a line integral of a conservative vector field independent of path?,Why is a line integral of a conservative vector field independent of path?,,"I am looking for intuition behind why the ability of a vector field $\vec{F}(x,y)$ to be written in the form $\vec{F}(x,y)=\nabla f$ where $f$ is simply a function of $x,y,$ and $z$ implies that a line integral on that conservative vector field is independent of path. Furthermore, what does a conservative vector field look like visually ? What properties do conservative vector fields have in the $xy$ plane that non-conservative vector fields don't? As a window to complex analysis, does there exist a relationship between the fact that line integrals of conservative vector fields in $\mathbb{R}^n$ on closed curves  is $0$ and Cauchy's Integral Theorem?","I am looking for intuition behind why the ability of a vector field $\vec{F}(x,y)$ to be written in the form $\vec{F}(x,y)=\nabla f$ where $f$ is simply a function of $x,y,$ and $z$ implies that a line integral on that conservative vector field is independent of path. Furthermore, what does a conservative vector field look like visually ? What properties do conservative vector fields have in the $xy$ plane that non-conservative vector fields don't? As a window to complex analysis, does there exist a relationship between the fact that line integrals of conservative vector fields in $\mathbb{R}^n$ on closed curves  is $0$ and Cauchy's Integral Theorem?",,"['complex-analysis', 'multivariable-calculus']"
42,A surjective map $S^{2} \longrightarrow S^{2}$,A surjective map,S^{2} \longrightarrow S^{2},"Is there any continuous surjective map  $S^{2} \longrightarrow S^{2}$ such that it sends one of the meridians of the first sphere into the south pole of the second one? I can see that is true, but I'm having a hard time figuring out a proper ""analytical"" definition (with some kind of coordinates etc.). EDIT: For a meridian I mean the shortest arc from the north pole to the south pole (with poles included).","Is there any continuous surjective map  $S^{2} \longrightarrow S^{2}$ such that it sends one of the meridians of the first sphere into the south pole of the second one? I can see that is true, but I'm having a hard time figuring out a proper ""analytical"" definition (with some kind of coordinates etc.). EDIT: For a meridian I mean the shortest arc from the north pole to the south pole (with poles included).",,['general-topology']
43,Does not existence of partial derivatives at a point tells us that the function is not differentiable?,Does not existence of partial derivatives at a point tells us that the function is not differentiable?,,"This question arose after my calculus test in which the told us: Show that $$f(x,y)=\begin{cases}\frac{2x^3}{x^2 +y^2}&\text{ if }(x,y)\neq(0,0)\\0&\text{ if }(x,y)=(0,0).\end{cases}$$ is not differentiable at $(0,0)$. I showed that the partials do not exist in $(0,0)$ thus the function is not differentiable.","This question arose after my calculus test in which the told us: Show that $$f(x,y)=\begin{cases}\frac{2x^3}{x^2 +y^2}&\text{ if }(x,y)\neq(0,0)\\0&\text{ if }(x,y)=(0,0).\end{cases}$$ is not differentiable at $(0,0)$. I showed that the partials do not exist in $(0,0)$ thus the function is not differentiable.",,"['multivariable-calculus', 'derivatives', 'partial-derivative']"
44,Module of differential forms over the ring of smooth functions,Module of differential forms over the ring of smooth functions,,"I'm following Theodore Shifrin's course Math 3510 on youtube. So I will use his notations. If $\omega \in \Lambda^k(\mathbb{R}^n)^*$ is an alternating multilinear map (i.e., a $k$-form on $\mathbb{R}^n$) I visualize its geometrical meaning. Moreover since a $k$-form is a function, I know how to evaluate it on $k$ input vectors. For example if $\omega = 3dx \wedge dy + 5dx \wedge dz + 7dy \wedge dz = 3dx_{12} + 5dx_{13} + 7dx_{23}$ is a 2-form on $\mathbb{R}^3$ and $v = \begin{pmatrix} 1 \\ 3 \\ 4\end{pmatrix}$ and $u = \begin{pmatrix} 5 \\ 2 \\ 11\end{pmatrix}$ then $$ \omega(v,u) =  3\begin{vmatrix} 1 & 5 \\  3 & 2  \end{vmatrix} + 5\begin{vmatrix} 1 & 5 \\  4 & 11  \end{vmatrix} + 7\begin{vmatrix} 3 & 2 \\  4 & 11  \end{vmatrix}  = 91 $$ Let $\mathcal{C}^\infty(\mathbb{R}^n)$ be the ring of smooth scalar functions from  $\mathbb{R}^n$ to $\mathbb{R}$. At the beginning of this lecture prof. Shifrin defines the set $\mathcal{A}^k(\mathbb{R}^n)$ of differential $k$-forms on $\mathbb{R}^n$ as the module over the ring $\mathcal{C}^\infty(\mathbb{R}^n)$ generated by $\Lambda^k(\mathbb{R}^n)^*$.  An element of $\mathcal{A}^k(\mathbb{R}^n)$ is a sum $\eta = \sum_{I} f_Idx_I$ where each $I$ in the summation is a multi-index of cardinality $k$. How should I think about $\eta$ ? Can I look at $\eta$ as a function that takes vectors as inputs and returns a real number?","I'm following Theodore Shifrin's course Math 3510 on youtube. So I will use his notations. If $\omega \in \Lambda^k(\mathbb{R}^n)^*$ is an alternating multilinear map (i.e., a $k$-form on $\mathbb{R}^n$) I visualize its geometrical meaning. Moreover since a $k$-form is a function, I know how to evaluate it on $k$ input vectors. For example if $\omega = 3dx \wedge dy + 5dx \wedge dz + 7dy \wedge dz = 3dx_{12} + 5dx_{13} + 7dx_{23}$ is a 2-form on $\mathbb{R}^3$ and $v = \begin{pmatrix} 1 \\ 3 \\ 4\end{pmatrix}$ and $u = \begin{pmatrix} 5 \\ 2 \\ 11\end{pmatrix}$ then $$ \omega(v,u) =  3\begin{vmatrix} 1 & 5 \\  3 & 2  \end{vmatrix} + 5\begin{vmatrix} 1 & 5 \\  4 & 11  \end{vmatrix} + 7\begin{vmatrix} 3 & 2 \\  4 & 11  \end{vmatrix}  = 91 $$ Let $\mathcal{C}^\infty(\mathbb{R}^n)$ be the ring of smooth scalar functions from  $\mathbb{R}^n$ to $\mathbb{R}$. At the beginning of this lecture prof. Shifrin defines the set $\mathcal{A}^k(\mathbb{R}^n)$ of differential $k$-forms on $\mathbb{R}^n$ as the module over the ring $\mathcal{C}^\infty(\mathbb{R}^n)$ generated by $\Lambda^k(\mathbb{R}^n)^*$.  An element of $\mathcal{A}^k(\mathbb{R}^n)$ is a sum $\eta = \sum_{I} f_Idx_I$ where each $I$ in the summation is a multi-index of cardinality $k$. How should I think about $\eta$ ? Can I look at $\eta$ as a function that takes vectors as inputs and returns a real number?",,"['calculus', 'multivariable-calculus', 'differential-geometry', 'multilinear-algebra']"
45,Why $Dom(f)$ needs to be open for $Df(a)$ to work?,Why  needs to be open for  to work?,Dom(f) Df(a),M. Spivak in Calculus on Manifolds defined differentiability as: $f:\mathbb R^n\to\mathbb R^m$ is differentiable at $a\in\mathbb R^n$ if there exists a linear transformation $\lambda:\mathbb R^n\to\mathbb R^m$ such that $$\lim_{h\to0}\dfrac{|f(a+h)-f(a)-\lambda(h)|}{|h|}=0$$ $\lambda$ is denoted by $Df(a)$ which is called the derivative of $f$ at $a.$ He made the following remark regarding the above definition: The definition of $Df(a)$ could be made if $f$ were defined only in some open set containing $a.$ Here is the problem I am facing. Why the domain of $f$ needs to be open for the definition to work?,M. Spivak in Calculus on Manifolds defined differentiability as: $f:\mathbb R^n\to\mathbb R^m$ is differentiable at $a\in\mathbb R^n$ if there exists a linear transformation $\lambda:\mathbb R^n\to\mathbb R^m$ such that $$\lim_{h\to0}\dfrac{|f(a+h)-f(a)-\lambda(h)|}{|h|}=0$$ $\lambda$ is denoted by $Df(a)$ which is called the derivative of $f$ at $a.$ He made the following remark regarding the above definition: The definition of $Df(a)$ could be made if $f$ were defined only in some open set containing $a.$ Here is the problem I am facing. Why the domain of $f$ needs to be open for the definition to work?,,['multivariable-calculus']
46,On Kelvin-Stokes proof without differential forms,On Kelvin-Stokes proof without differential forms,,"I was reading a proof of the Kelvin-Stokes theorem (without differential forms) and the first step was defining a Jordan curve $\gamma:[a,b]\rightarrow\mathbb{R}^2$ and a surface $\psi:D\rightarrow\mathbb{R}^3$ where $D$ is the interior of the curve (i.e. the compact portion of $\mathbb{R}^2$). The surface is $S:=\psi(D)$ and the surface boundary is defined $\Gamma(t)=\psi(\gamma(t))$.  Then, by definition of line integral we have \begin{align*} \oint_{\partial S=\Gamma}\vec{F}\cdot\vec{d\vec{\Gamma}} & = \int\limits_{a}^b\big\langle(F\circ\Gamma(t))|\frac{d\Gamma}{dt}(t) \big\rangle dt \\ & = \int\limits_{a}^b \big\langle(F\circ\Gamma(t))|\frac{d(\psi\circ\gamma)}{dt}(t) \big\rangle dt \\ & = \int\limits_{a}^b \big\langle(F\circ\Gamma(t))|(J_\psi)_{\gamma(t)}\cdot\frac{d\gamma}{dt}(t) \big\rangle dt \end{align*} Where $J_\psi$ is the Jacobian matrix of $\psi$.  I'm not sure what $(J_\psi)_{\gamma(t)}$ means though, or why it can be used here. Also, the next few lines of the proof are \begin{align*} \big\langle(F\circ\Gamma(t))|(J_\psi)_{\gamma(t)}\cdot\frac{d\gamma}{dt}(t) \big\rangle &  = \big\langle(F\circ\Gamma(t))|(J_\psi)_{\gamma(t)}|\frac{d\gamma}{dt}(t) \big\rangle\ \\ & = \big\langle(^tF\circ\Gamma(t))\cdot(J_\psi)_{\gamma(t)}|\frac{d\gamma}{dt}(t) \big\rangle \end{align*} I'm wondering why the ""|"" symbol is moving around in the interior product, and also what the superscript $t$ in $(^tF)$ means.","I was reading a proof of the Kelvin-Stokes theorem (without differential forms) and the first step was defining a Jordan curve $\gamma:[a,b]\rightarrow\mathbb{R}^2$ and a surface $\psi:D\rightarrow\mathbb{R}^3$ where $D$ is the interior of the curve (i.e. the compact portion of $\mathbb{R}^2$). The surface is $S:=\psi(D)$ and the surface boundary is defined $\Gamma(t)=\psi(\gamma(t))$.  Then, by definition of line integral we have \begin{align*} \oint_{\partial S=\Gamma}\vec{F}\cdot\vec{d\vec{\Gamma}} & = \int\limits_{a}^b\big\langle(F\circ\Gamma(t))|\frac{d\Gamma}{dt}(t) \big\rangle dt \\ & = \int\limits_{a}^b \big\langle(F\circ\Gamma(t))|\frac{d(\psi\circ\gamma)}{dt}(t) \big\rangle dt \\ & = \int\limits_{a}^b \big\langle(F\circ\Gamma(t))|(J_\psi)_{\gamma(t)}\cdot\frac{d\gamma}{dt}(t) \big\rangle dt \end{align*} Where $J_\psi$ is the Jacobian matrix of $\psi$.  I'm not sure what $(J_\psi)_{\gamma(t)}$ means though, or why it can be used here. Also, the next few lines of the proof are \begin{align*} \big\langle(F\circ\Gamma(t))|(J_\psi)_{\gamma(t)}\cdot\frac{d\gamma}{dt}(t) \big\rangle &  = \big\langle(F\circ\Gamma(t))|(J_\psi)_{\gamma(t)}|\frac{d\gamma}{dt}(t) \big\rangle\ \\ & = \big\langle(^tF\circ\Gamma(t))\cdot(J_\psi)_{\gamma(t)}|\frac{d\gamma}{dt}(t) \big\rangle \end{align*} I'm wondering why the ""|"" symbol is moving around in the interior product, and also what the superscript $t$ in $(^tF)$ means.",,"['multivariable-calculus', 'proof-explanation', 'stokes-theorem']"
47,Every conservative vector field is irrotational,Every conservative vector field is irrotational,,"I have done an example where I needed to show that every conservative $C^2$ vector field is irrotational. However, there is something unclear in the solutions: Namely, I am uncertain what does the following sentence at the end of the solution mean: ""since second partial derivatives are independent of the order (for smooth functions)"", and I was wondering how does that imply that the equality before that is 0?","I have done an example where I needed to show that every conservative $C^2$ vector field is irrotational. However, there is something unclear in the solutions: Namely, I am uncertain what does the following sentence at the end of the solution mean: ""since second partial derivatives are independent of the order (for smooth functions)"", and I was wondering how does that imply that the equality before that is 0?",,"['calculus', 'multivariable-calculus', 'vector-fields']"
48,"What kind of integral is this, and how do you solve it?","What kind of integral is this, and how do you solve it?",,"I have to solve this integral: $\int_T(x^2+y^2)dx dy \ \ \ $ where $\ \ \ T=\{(x,y,z):x^2+y^2+z^2<1\}$ Now, what gets me confused is that i have to integrate only with respect to $x$ and $y$ but not $z$ . The fact is that i have no idea on what this even means geometrically, is this a double integral on a 3d domain? At first i didn't even noticed that and i managed to solve the integral (at least i believe) as if it was: $\int_T(x^2+y^2)dx dy dz \ \ \ $ where $\ \ \ T=\{(x,y,z):x^2+y^2+z^2<1\}$ switching to spherical polar coordinates since $T$ represents a sphere of radius $1$: $=\int_0^\pi d\theta\int_0^1 d\rho\int_0^{2\pi} \rho^2\sin^2(\theta)(\cos^2(\phi)+sin^2(\phi)) \ d\phi= \\  =\int_0^\pi d\theta\int_0^1 2\pi\rho^2\sin^2(\theta) \ d\rho= \\  =\int_0^\pi\frac{2}{3}\pi\sin^2(\theta) \ d\theta = \frac{\pi^2}{3}$ But the correct answer is $\frac{8\pi}{15}$ and i don't know how to get to that.","I have to solve this integral: $\int_T(x^2+y^2)dx dy \ \ \ $ where $\ \ \ T=\{(x,y,z):x^2+y^2+z^2<1\}$ Now, what gets me confused is that i have to integrate only with respect to $x$ and $y$ but not $z$ . The fact is that i have no idea on what this even means geometrically, is this a double integral on a 3d domain? At first i didn't even noticed that and i managed to solve the integral (at least i believe) as if it was: $\int_T(x^2+y^2)dx dy dz \ \ \ $ where $\ \ \ T=\{(x,y,z):x^2+y^2+z^2<1\}$ switching to spherical polar coordinates since $T$ represents a sphere of radius $1$: $=\int_0^\pi d\theta\int_0^1 d\rho\int_0^{2\pi} \rho^2\sin^2(\theta)(\cos^2(\phi)+sin^2(\phi)) \ d\phi= \\  =\int_0^\pi d\theta\int_0^1 2\pi\rho^2\sin^2(\theta) \ d\rho= \\  =\int_0^\pi\frac{2}{3}\pi\sin^2(\theta) \ d\theta = \frac{\pi^2}{3}$ But the correct answer is $\frac{8\pi}{15}$ and i don't know how to get to that.",,"['integration', 'multivariable-calculus', 'spherical-coordinates']"
49,Inhomogeneous Wave Equation Energy Method,Inhomogeneous Wave Equation Energy Method,,"Let $u$ be a solution to the following equation: $$ u_{tt}=u_{xx}−u^3 $$ Assume that $u(x,0) =u_t(x,0) = 0$ for all $x\in[a, b]$.  Prove that $u(x, t) = 0$ if $a+t < x < b−t$. I initially thought of using Duhamel's principle on this but realized that the cube in the integrand would make things a little tricky. I was also thinking of using an energy functional and showing that it was $0$ everywhere and nonincreasing. What would the functional be, and what would/should the domain of integration be?","Let $u$ be a solution to the following equation: $$ u_{tt}=u_{xx}−u^3 $$ Assume that $u(x,0) =u_t(x,0) = 0$ for all $x\in[a, b]$.  Prove that $u(x, t) = 0$ if $a+t < x < b−t$. I initially thought of using Duhamel's principle on this but realized that the cube in the integrand would make things a little tricky. I was also thinking of using an energy functional and showing that it was $0$ everywhere and nonincreasing. What would the functional be, and what would/should the domain of integration be?",,"['multivariable-calculus', 'partial-differential-equations', 'calculus-of-variations', 'wave-equation']"
50,Evaluate line integral and determine if independent of path,Evaluate line integral and determine if independent of path,,"The exercise consists of calculating the integral $\int_{\gamma} \mathbf{F}   \cdot d \mathbf{r}$ where $\mathbf{F}(x,y)=\left( - \frac{y-1}{(x-1)^2 + (y-1)^2}, \frac{x-1}{(x-1)^2 + (y-1)^2} \right) + \left(\frac{y}{x^2 + y^2}, \frac{-x}{x^2 + y^2} \right)$ and $\gamma$ is the circle $x^2 + y^2 = 4$ oriented anticlockwise, and to determine if the integral is independent of its path on the set $D=\{(x,y): (x,y) \neq (0,0) \text{ and } (x,y) \neq (1,1) \}$ For the first part, I concluded that using Green's theorem is not an option, since the disc $x^2 + y^2 \leq 4$ contains singular points. Since it is appears to be a bit of a tricky function, I also chose not to parametrise it directly. Instead I opted to determine the potential function to the integral. I show that $\frac{\partial P}{\partial y} = \frac{\partial Q}{\partial x}$, and, solving it like a system of equations, found that the potential function is $U(x,y) = \arctan{(\frac{y-1}{x-1})} - \arctan{(\frac{y}{x})}$. If I $U$ actually is a conservative vector field, I can then evaluate the integral simply by using the starting points and end points of $U$, by letting the starting point and end point to be, for instance, $(\sqrt{2}, \sqrt{2})$. Since $\gamma$ is a closed curve, the integral is zero. I then argue that, on the set $D$, the integral is independent of its path, since $\mathbf{F}$ is conservative, and the potential function $U$ does not have any singular points on $D$. I am a bit unsure whether this is the right approach, but I cannot see how else I would solve this exercise. Any input would be much appreciated.","The exercise consists of calculating the integral $\int_{\gamma} \mathbf{F}   \cdot d \mathbf{r}$ where $\mathbf{F}(x,y)=\left( - \frac{y-1}{(x-1)^2 + (y-1)^2}, \frac{x-1}{(x-1)^2 + (y-1)^2} \right) + \left(\frac{y}{x^2 + y^2}, \frac{-x}{x^2 + y^2} \right)$ and $\gamma$ is the circle $x^2 + y^2 = 4$ oriented anticlockwise, and to determine if the integral is independent of its path on the set $D=\{(x,y): (x,y) \neq (0,0) \text{ and } (x,y) \neq (1,1) \}$ For the first part, I concluded that using Green's theorem is not an option, since the disc $x^2 + y^2 \leq 4$ contains singular points. Since it is appears to be a bit of a tricky function, I also chose not to parametrise it directly. Instead I opted to determine the potential function to the integral. I show that $\frac{\partial P}{\partial y} = \frac{\partial Q}{\partial x}$, and, solving it like a system of equations, found that the potential function is $U(x,y) = \arctan{(\frac{y-1}{x-1})} - \arctan{(\frac{y}{x})}$. If I $U$ actually is a conservative vector field, I can then evaluate the integral simply by using the starting points and end points of $U$, by letting the starting point and end point to be, for instance, $(\sqrt{2}, \sqrt{2})$. Since $\gamma$ is a closed curve, the integral is zero. I then argue that, on the set $D$, the integral is independent of its path, since $\mathbf{F}$ is conservative, and the potential function $U$ does not have any singular points on $D$. I am a bit unsure whether this is the right approach, but I cannot see how else I would solve this exercise. Any input would be much appreciated.",,"['integration', 'multivariable-calculus', 'vector-fields']"
51,How do I perform a change in order of integration here?,How do I perform a change in order of integration here?,,I have a function $$\int^1_{y=0}\int^1_{x=y}e^{x^2}dx\ dy$$ Which I want to perform a change in order of integration. I have plotted the graph: And it seems it's the area bounded by the y-axis and x-axis. The answer I know is $(e-1)/2$ but it doesn't make sense since a quick check can tell the area of the triangle under is $1/2.$ The limits to be changed to is: $$\int^1_{x=0}\int^x_{y=0}e^{x^2}dy\ dx$$ Giving $$ \left[ \frac{1}{2}e^{x^2} \right]^1_0 = \frac{1}{2}(e-1)$$,I have a function $$\int^1_{y=0}\int^1_{x=y}e^{x^2}dx\ dy$$ Which I want to perform a change in order of integration. I have plotted the graph: And it seems it's the area bounded by the y-axis and x-axis. The answer I know is $(e-1)/2$ but it doesn't make sense since a quick check can tell the area of the triangle under is $1/2.$ The limits to be changed to is: $$\int^1_{x=0}\int^x_{y=0}e^{x^2}dy\ dx$$ Giving $$ \left[ \frac{1}{2}e^{x^2} \right]^1_0 = \frac{1}{2}(e-1)$$,,"['multivariable-calculus', 'multiple-integral', 'iterated-integrals']"
52,"A ""cracked glass"" Riemann Sum - Double integrals","A ""cracked glass"" Riemann Sum - Double integrals",,"For a 1-dimensional domain, you can partition a segment $(a,b)$ into uneven intervals, choose sample points in the uneven intervals, and form an integral in the limit that the uneven intervals go to $0$. For a 2-dimensional domain, it would be easiest to partition such a domain into squares. However, to be as crazy as possible, could you break the domain down into a union of circles, parallelograms, triangles, and trapezoids? From this crazy partition, would a double integral be attainable and would it be equal to the double integral obtained by ""normal"" partitions? Now lets focus on a single fragment in this ""cracked glass"" domain and a function $f(x,y)$ above this domain. Let the fragment be a circular fragment. Does the volume above this single circular fragment $$(f(\text{sample point}))\; (\text{area of circle fragment})$$ in the limit that the fragment goes to zero, equal the same value if the fragment was a parallelogram that went to zero [probably ""yes"" as both numbers by themselves are $0$. Maybe 1 tends to zero faster than the other, I don't know. But if we consider two different ""cracked glass"" patterns in which the only difference is one fragment is a circle while the other is a parallelogram, does the total Riemann sum remain the same]?","For a 1-dimensional domain, you can partition a segment $(a,b)$ into uneven intervals, choose sample points in the uneven intervals, and form an integral in the limit that the uneven intervals go to $0$. For a 2-dimensional domain, it would be easiest to partition such a domain into squares. However, to be as crazy as possible, could you break the domain down into a union of circles, parallelograms, triangles, and trapezoids? From this crazy partition, would a double integral be attainable and would it be equal to the double integral obtained by ""normal"" partitions? Now lets focus on a single fragment in this ""cracked glass"" domain and a function $f(x,y)$ above this domain. Let the fragment be a circular fragment. Does the volume above this single circular fragment $$(f(\text{sample point}))\; (\text{area of circle fragment})$$ in the limit that the fragment goes to zero, equal the same value if the fragment was a parallelogram that went to zero [probably ""yes"" as both numbers by themselves are $0$. Maybe 1 tends to zero faster than the other, I don't know. But if we consider two different ""cracked glass"" patterns in which the only difference is one fragment is a circle while the other is a parallelogram, does the total Riemann sum remain the same]?",,['multivariable-calculus']
53,Second Derivatives Continuous but Mixed Partials not Equal,Second Derivatives Continuous but Mixed Partials not Equal,,"I'm working on the following problem: Consider the $f:\mathbb{R}^2 \mapsto \mathbb{R}$ defined by $$f(x,y) = \frac{xy(x^2-y^2)}{x^2+y^2}$$ for everywhere outside the origin and $f(0,0)=0$ at the origin is. Show that the second derivative of $f$ exists but the mixed partials are not equal at the origin. My work: I'm getting for $\nabla f = <\frac{yx^4+4x^2y^3-y^5}{(x^2+y^2)^2},\frac{x^5-4x^3y^2-xy^4}{(x^2+y^2)^2}>$. My analysis show that the partials are continuous at zero and take the value $0$ at the origin. I'm having trouble analyzing the second derivative: $$ A = \begin{bmatrix}f_{xx}&f_{xy}\\f_{yx}&f_{yy}\end{bmatrix} = \begin{bmatrix}\frac{-4xy^3(x^2-3y^2)}{(x^2+y^2)^3}&\frac{x^6+9x^4y^2-9x^2y^4-y^6}{(x^2+y^2)^3}\\\frac{x^6+9x^4y^2-9x^2y^4-y^6}{(x^2+y^2)^3}&\frac{4yx^3(y^2-3x^2)}{(x^2+y^2)^3}\end{bmatrix} $$ $$\lim_{\|h\|\rightarrow 0} \frac{\nabla f((0,0)+h)-\nabla f((0,0))- A \cdot h\|}{\|h\|} \\ = \lim_{\|h\|\rightarrow 0} \frac{\|\nabla f(h)-\langle f_{xx}h_x+f_{xy}h_y,f_{yx}h_x+f_{yy}h_y\rangle\|}{\|h\|} \\= \lim_{\|h\|\rightarrow 0} \frac{\|\nabla f(h)-\langle\frac{-4xy^3(x^2-3y^2)}{(x^2+y^2)^3}h_x+\frac{x^6+9x^4y^2-9x^2y^4-y^6}{(x^2+y^2)^3}h_y,\frac{x^6+9x^4y^2-9x^2y^4-y^6}{(x^2+y^2)^3}h_x+\frac{4yx^3(y^2-3x^2)}{(x^2+y^2)^3}h_y\rangle\|}{\|h\|}$$ When I do a polar conversion on this thing and send $r \rightarrow 0$ (uniform for all $\theta$), I get all my $r$'s canceling out -- leaving a limit dependent on theta. But according to the problem, this ratio should go to zero though. Does anyone know what's going on? I've seen other problems like this that show partials are not continuous at the the origin. That's different though from just establishing existance of the total derivative, right?","I'm working on the following problem: Consider the $f:\mathbb{R}^2 \mapsto \mathbb{R}$ defined by $$f(x,y) = \frac{xy(x^2-y^2)}{x^2+y^2}$$ for everywhere outside the origin and $f(0,0)=0$ at the origin is. Show that the second derivative of $f$ exists but the mixed partials are not equal at the origin. My work: I'm getting for $\nabla f = <\frac{yx^4+4x^2y^3-y^5}{(x^2+y^2)^2},\frac{x^5-4x^3y^2-xy^4}{(x^2+y^2)^2}>$. My analysis show that the partials are continuous at zero and take the value $0$ at the origin. I'm having trouble analyzing the second derivative: $$ A = \begin{bmatrix}f_{xx}&f_{xy}\\f_{yx}&f_{yy}\end{bmatrix} = \begin{bmatrix}\frac{-4xy^3(x^2-3y^2)}{(x^2+y^2)^3}&\frac{x^6+9x^4y^2-9x^2y^4-y^6}{(x^2+y^2)^3}\\\frac{x^6+9x^4y^2-9x^2y^4-y^6}{(x^2+y^2)^3}&\frac{4yx^3(y^2-3x^2)}{(x^2+y^2)^3}\end{bmatrix} $$ $$\lim_{\|h\|\rightarrow 0} \frac{\nabla f((0,0)+h)-\nabla f((0,0))- A \cdot h\|}{\|h\|} \\ = \lim_{\|h\|\rightarrow 0} \frac{\|\nabla f(h)-\langle f_{xx}h_x+f_{xy}h_y,f_{yx}h_x+f_{yy}h_y\rangle\|}{\|h\|} \\= \lim_{\|h\|\rightarrow 0} \frac{\|\nabla f(h)-\langle\frac{-4xy^3(x^2-3y^2)}{(x^2+y^2)^3}h_x+\frac{x^6+9x^4y^2-9x^2y^4-y^6}{(x^2+y^2)^3}h_y,\frac{x^6+9x^4y^2-9x^2y^4-y^6}{(x^2+y^2)^3}h_x+\frac{4yx^3(y^2-3x^2)}{(x^2+y^2)^3}h_y\rangle\|}{\|h\|}$$ When I do a polar conversion on this thing and send $r \rightarrow 0$ (uniform for all $\theta$), I get all my $r$'s canceling out -- leaving a limit dependent on theta. But according to the problem, this ratio should go to zero though. Does anyone know what's going on? I've seen other problems like this that show partials are not continuous at the the origin. That's different though from just establishing existance of the total derivative, right?",,"['multivariable-calculus', 'derivatives', 'partial-derivative']"
54,Studying uniform continuity on a multivariable function.,Studying uniform continuity on a multivariable function.,,"Prove that the function $f(x,y)=\frac{1}{x^2+y^2}$ is not uniformly continuous over the domain: $D$={$(x,y):x^2+(y-2)^2 <2^2$} Using the definition: $\forall \delta>0 , \exists \epsilon(\delta) > 0 / \forall (x_1,y_1) , (x_2,y_2)  \in D  : 0< ||(x_1,y_1),(x_2,y_2)||<\delta \implies |f(x_1,y_1)-f(x_2,y_2)|>\epsilon$ Let : $(x_1,y_1)=(0,\frac{\delta}{2})$ $(x_2,y_2)=(\frac{\delta}{4},0)$ Conditions on $\delta$: $x_1^2+(y_1-2)^2<2^2 \implies 0<\delta<8$ $x_2^2+(y_2-2)^2<2^2 \implies 0<\delta<8$ $|f(x_1,y_1)-f(x_2,y_2)|=\frac{12}{\delta^2}>\epsilon$ $\forall \delta>0 , \exists \epsilon(\delta) \in ]0,\frac{12}{\delta^2}[  / \forall (x_1,y_1) , (x_2,y_2)  \in D  : 0< ||(x_1,y_1),(x_2,y_2)||<\delta \implies |f(x_1,y_1)-f(x_2,y_2)|>\epsilon$ However I'm not sure if this is correct nor I am sure if It's possible to put conditions on $\delta$ while proving uniform continuity. I would be grateful to whoever can point out my mistakes or whoever has a much cleaner solution to this question. Thanks in advance.","Prove that the function $f(x,y)=\frac{1}{x^2+y^2}$ is not uniformly continuous over the domain: $D$={$(x,y):x^2+(y-2)^2 <2^2$} Using the definition: $\forall \delta>0 , \exists \epsilon(\delta) > 0 / \forall (x_1,y_1) , (x_2,y_2)  \in D  : 0< ||(x_1,y_1),(x_2,y_2)||<\delta \implies |f(x_1,y_1)-f(x_2,y_2)|>\epsilon$ Let : $(x_1,y_1)=(0,\frac{\delta}{2})$ $(x_2,y_2)=(\frac{\delta}{4},0)$ Conditions on $\delta$: $x_1^2+(y_1-2)^2<2^2 \implies 0<\delta<8$ $x_2^2+(y_2-2)^2<2^2 \implies 0<\delta<8$ $|f(x_1,y_1)-f(x_2,y_2)|=\frac{12}{\delta^2}>\epsilon$ $\forall \delta>0 , \exists \epsilon(\delta) \in ]0,\frac{12}{\delta^2}[  / \forall (x_1,y_1) , (x_2,y_2)  \in D  : 0< ||(x_1,y_1),(x_2,y_2)||<\delta \implies |f(x_1,y_1)-f(x_2,y_2)|>\epsilon$ However I'm not sure if this is correct nor I am sure if It's possible to put conditions on $\delta$ while proving uniform continuity. I would be grateful to whoever can point out my mistakes or whoever has a much cleaner solution to this question. Thanks in advance.",,"['multivariable-calculus', 'proof-verification', 'epsilon-delta', 'uniform-continuity']"
55,"Gradient of largest eigenvalue of matrix, with respect to individual elements of the matrix","Gradient of largest eigenvalue of matrix, with respect to individual elements of the matrix",,"If $M$ is a $n\times n$ matrix, let $f(M)$ denote the largest eigenvalue (in absolute value) of $M$.  In other words, if $\lambda_1,\dots,\lambda_n$ are the eigenvalues of $M$, define $$f(M) = \max(|\lambda_1|,\dots,|\lambda_n|).$$ This can be viewed as a function $f:\mathbb{R}^{n^2} \to \mathbb{R}$ on a $n^2$-dimensional input. Now given a matrix $M$, I'd like to compute the gradient $\nabla f(M)$.  How do I do that? Equivalently, for each $i,j$, I want to compute the derivative ${\partial  \over \partial M_{i,j}} f(M)$ of $f(M)$ with respect to the $i,j$-th entry of the matrix.  I can't figure out a clean way to compute this, as computing the eigenvalues involves Gaussian elimination, and it's not clear how to differentiate through that process. (This is based on application where I want to do gradient descent on a function with a term of the form $f(M)$, so I need to be able to compute the gradient to do that.)","If $M$ is a $n\times n$ matrix, let $f(M)$ denote the largest eigenvalue (in absolute value) of $M$.  In other words, if $\lambda_1,\dots,\lambda_n$ are the eigenvalues of $M$, define $$f(M) = \max(|\lambda_1|,\dots,|\lambda_n|).$$ This can be viewed as a function $f:\mathbb{R}^{n^2} \to \mathbb{R}$ on a $n^2$-dimensional input. Now given a matrix $M$, I'd like to compute the gradient $\nabla f(M)$.  How do I do that? Equivalently, for each $i,j$, I want to compute the derivative ${\partial  \over \partial M_{i,j}} f(M)$ of $f(M)$ with respect to the $i,j$-th entry of the matrix.  I can't figure out a clean way to compute this, as computing the eigenvalues involves Gaussian elimination, and it's not clear how to differentiate through that process. (This is based on application where I want to do gradient descent on a function with a term of the form $f(M)$, so I need to be able to compute the gradient to do that.)",,"['multivariable-calculus', 'eigenvalues-eigenvectors', 'vector-analysis', 'matrix-calculus', 'spectral-radius']"
56,Stokes’ Theorem for arbitrary surface and boundary curve in $xz$-plane?,Stokes’ Theorem for arbitrary surface and boundary curve in -plane?,xz,"I am tasked to find $\iint (\nabla \times {\bf V}) \cdot d{\bf S}$ for any surface whose bounding curve is in the $xz$-plane, where ${\bf V} = (xy + e^x) {\bf i}+ (x^2 -3y){\bf j} + (y^2 + z^2) {\bf k}$. I have attempted this via two methods and am stuck on both: 1) I’ve tried to employ Stokes’ Theorem directly. In the $xz$-plane, $y=0$, so {\bf V} becomes $e^x {\bf i}+ x^2 {\bf j}+ z^2 {\bf k}$, and $dy=0$ which makes the dot product $e^x dx + z^2 dz$. The problem is parametrising afterward. I’m unsure of how to approach this for an arbitrary curve. My intuition tells me that because this is a closed curve, the integral will sum up to zero, but I don’t know how to mathematically express this. 2) I also attempted to directly integrate the curl. The only interesting point to note is that the $y$-component is 0. Apart from that, I am unable to figure out how to obtain the normal vector to the surface to perform the integral. Any insight is appreciated, thank you!","I am tasked to find $\iint (\nabla \times {\bf V}) \cdot d{\bf S}$ for any surface whose bounding curve is in the $xz$-plane, where ${\bf V} = (xy + e^x) {\bf i}+ (x^2 -3y){\bf j} + (y^2 + z^2) {\bf k}$. I have attempted this via two methods and am stuck on both: 1) I’ve tried to employ Stokes’ Theorem directly. In the $xz$-plane, $y=0$, so {\bf V} becomes $e^x {\bf i}+ x^2 {\bf j}+ z^2 {\bf k}$, and $dy=0$ which makes the dot product $e^x dx + z^2 dz$. The problem is parametrising afterward. I’m unsure of how to approach this for an arbitrary curve. My intuition tells me that because this is a closed curve, the integral will sum up to zero, but I don’t know how to mathematically express this. 2) I also attempted to directly integrate the curl. The only interesting point to note is that the $y$-component is 0. Apart from that, I am unable to figure out how to obtain the normal vector to the surface to perform the integral. Any insight is appreciated, thank you!",,"['multivariable-calculus', 'vector-fields', 'surface-integrals', 'stokes-theorem']"
57,For what $\alpha$ does $\iint_D\frac{1}{(x+y)^{\alpha}}\ dxdy$ converge?,For what  does  converge?,\alpha \iint_D\frac{1}{(x+y)^{\alpha}}\ dxdy,"For what values of $\alpha$ does $$\iint_D\frac{1}{(x+y)^{\alpha}}\  dxdy$$ converge? $D=\{0\leq y \leq 1-x, \quad 0 \leq \ x \leq 1\}.$ The double integral can be written as $$\int_{0}^{1}\left(\int_{0}^{1-x} \frac{1}{(x+y)^{\alpha}} \ dy\right)dx.$$ How does one find a primitive to the inner integral?","For what values of $\alpha$ does $$\iint_D\frac{1}{(x+y)^{\alpha}}\  dxdy$$ converge? $D=\{0\leq y \leq 1-x, \quad 0 \leq \ x \leq 1\}.$ The double integral can be written as $$\int_{0}^{1}\left(\int_{0}^{1-x} \frac{1}{(x+y)^{\alpha}} \ dy\right)dx.$$ How does one find a primitive to the inner integral?",,['multivariable-calculus']
58,Proving AM-GM with the method of Lagrange multipliers,Proving AM-GM with the method of Lagrange multipliers,,"In my calculus book, there is a question that basically says ""use the method of Lagrange multipliers on $f(x,y,z)=xyz$  with constraint $g(x,y,z)=x+y+z=C$, $C$ being a constant, and use this to prove AM-GM for three variables."" The next question asks to generalize this result. It is very easy to see that the only possible location for an extremum of f along the constraint is when all three variables equal to each other, and that an other value of f along the constraint is less than this, but I couldn't think of a way to show that f actually has a maximum along the constraint short of independently proving AM-GM and applying it here.","In my calculus book, there is a question that basically says ""use the method of Lagrange multipliers on $f(x,y,z)=xyz$  with constraint $g(x,y,z)=x+y+z=C$, $C$ being a constant, and use this to prove AM-GM for three variables."" The next question asks to generalize this result. It is very easy to see that the only possible location for an extremum of f along the constraint is when all three variables equal to each other, and that an other value of f along the constraint is less than this, but I couldn't think of a way to show that f actually has a maximum along the constraint short of independently proving AM-GM and applying it here.",,['multivariable-calculus']
59,"How do you invert this function $f(x,y)=\left( \frac{x}{x^2+y^2},\frac{y}{x^2+y^2}\right)$?",How do you invert this function ?,"f(x,y)=\left( \frac{x}{x^2+y^2},\frac{y}{x^2+y^2}\right)","Does there exist an inverse of the following function with given domain? $$f(x,y)=\left( \frac{x}{x^2+y^2},\frac{y}{x^2+y^2}\right), \quad (x,y)  \in \mathbb{R}^2$$ $$(\mathbb{R}^2= \{ (x,y):x,y \text{   are real numbers}, \text{ excluding } (x,y)=(0,0) \})$$ I know when the function is of a single variable its inverse can be visualised flipping around the $x=f(x)$ line in the $(x,f(x))$ plane, but how would you interpret the inverse of a function as above (more dimensions)?","Does there exist an inverse of the following function with given domain? $$f(x,y)=\left( \frac{x}{x^2+y^2},\frac{y}{x^2+y^2}\right), \quad (x,y)  \in \mathbb{R}^2$$ $$(\mathbb{R}^2= \{ (x,y):x,y \text{   are real numbers}, \text{ excluding } (x,y)=(0,0) \})$$ I know when the function is of a single variable its inverse can be visualised flipping around the $x=f(x)$ line in the $(x,f(x))$ plane, but how would you interpret the inverse of a function as above (more dimensions)?",,['multivariable-calculus']
60,Understanding extended Riemann integral in Munkres.,Understanding extended Riemann integral in Munkres.,,"I'm self-studying Analysis on Manifolds by Munkres.  I understood the theory of the Riemann integral over bounded rectangles and more general rectifiable sets in $\mathbb{R}^n$.  In the part on improper integrals, I am becoming confused. Munkres defines the extended or improper integral of a continuous function $f$ over an open set $A \subset \mathbf{R}^n$.  He chooses any sequence $C_N$ of compact rectifiable sets such that $A = \bigcup_N C_N$   and $C_N \subset \text{Int }C_{N+1}$ for all $N$ and states that the extended integral exists if and only if the sequence $\int_{C_N} |f|$ is bounded and $$\int_Af = \lim_{N \to \infty} \int_{C_N} f.$$ He states ""...if the ordinary integral exists, then so does the extended integral and the two integrals are equal"", but then "" ...the extended integral may exist when the ordinary integral does not."" This makes sense if you think about integrals over intervals in $\mathbf {R}$.  If a function is unbounded or the interval is unbounded then the Riemann integral does not exist but the improper integral can. But Munkres claims this even if $A$ is a bounded, open set and $f:A \to \mathbf{R}$ is a bounded, continuous function. How is this possible?","I'm self-studying Analysis on Manifolds by Munkres.  I understood the theory of the Riemann integral over bounded rectangles and more general rectifiable sets in $\mathbb{R}^n$.  In the part on improper integrals, I am becoming confused. Munkres defines the extended or improper integral of a continuous function $f$ over an open set $A \subset \mathbf{R}^n$.  He chooses any sequence $C_N$ of compact rectifiable sets such that $A = \bigcup_N C_N$   and $C_N \subset \text{Int }C_{N+1}$ for all $N$ and states that the extended integral exists if and only if the sequence $\int_{C_N} |f|$ is bounded and $$\int_Af = \lim_{N \to \infty} \int_{C_N} f.$$ He states ""...if the ordinary integral exists, then so does the extended integral and the two integrals are equal"", but then "" ...the extended integral may exist when the ordinary integral does not."" This makes sense if you think about integrals over intervals in $\mathbf {R}$.  If a function is unbounded or the interval is unbounded then the Riemann integral does not exist but the improper integral can. But Munkres claims this even if $A$ is a bounded, open set and $f:A \to \mathbf{R}$ is a bounded, continuous function. How is this possible?",,"['real-analysis', 'multivariable-calculus', 'improper-integrals', 'riemann-integration']"
61,multivariable continuous,multivariable continuous,,"I wanna show that for $\alpha+\beta-2\gamma>0$: $\lim_{(x,y)\rightarrow(0,0)}\frac{\vert x\vert^{\alpha}\cdot\vert y\vert^{\beta}}{(x^{2}+y^{2})^{\gamma}}=0$ I thought about proving it via Sandwhich theory but I have no idea how to simplify this expression. Thanks in advance!","I wanna show that for $\alpha+\beta-2\gamma>0$: $\lim_{(x,y)\rightarrow(0,0)}\frac{\vert x\vert^{\alpha}\cdot\vert y\vert^{\beta}}{(x^{2}+y^{2})^{\gamma}}=0$ I thought about proving it via Sandwhich theory but I have no idea how to simplify this expression. Thanks in advance!",,"['multivariable-calculus', 'continuity']"
62,Assorted Questions from a proof in Evans’ book regarding the Laplace and Poisson’s Equations,Assorted Questions from a proof in Evans’ book regarding the Laplace and Poisson’s Equations,,"I’m trying to understand a proof in Evan’s book, “Partial Differential Equations.” On page 23, he states If $u$ is the fundamental solution to Laplace’s Equation then $ u \in C^2(\mathbb{R}^n)$ and $u$ satisfies Poisson’s Equation. I have assorted questions throughout the course of the proof which I couldn’t find the answers to anywhere on stack exchange. I’m also not sure how to google them because they’re so specific. We have  $$u(x) = \int_{\mathbb{R}^n} \Phi(x-y) f(y) \ dy =\int_{\mathbb{R}^n} f(x-y) \Phi(y) \ dy. \ \ (1)$$ My first question arises in this equality: Why is it true? I know the reasoning above the proof in the text suggests that $u(x)$ is the convolution of the fundamental solution and $f(x),$ but why is it justified to change their arguments and let them equal one another? Next we have $$\frac{u(x+he_i)-u(x)}{h} = \int_{\mathbb{R}^n} \Phi(y) \left[\frac{f(x+he_i-y) -f(x-y)}{h}\right]dy. \ \ (2)$$ I believe what he did here (correct me if I’m wrong) is approximate the derivatives on either side. He then writes $$\frac{\partial u}{\partial x_i}(x) = \int_{\mathbb{R}^n} \Phi(y) \frac{\partial f}{\partial x_i} \ dy. \ \ (3)$$ He uses the fact that $\frac{f(x+he_i-y)-f(x-y)}{h} \rightarrow \frac{\partial f}{\partial x_i}(x-y)$ as $h \rightarrow 0$ which I also don’t see because at $h = 0,$ the $\frac{f(x+he_i-y)-f(x-y)}{h}$  term blows up. Once again, I tried but, given its specificity, finding the answers to these questions online is impossible. Now, he computes the second partial derivative and concludes $u \in C^2(\mathbb{R}^n).$ The other half of the proof I understand. I was able to find existing questions on this site pertaining to that. I know I asked a lot of questions, so to recap: Why is it justified to switch the arguments of the functions $\Phi$ and $f$ in equation (1)? Why are the two integrals equal? Is it true that all Evans did in equation two is change the differential terms to approximations, or is there more to it than that? Why is equation (3) true? Why does $\frac{f(x+he_i-y)-f(x-y)}{h} \rightarrow \frac{\partial f}{x_i}$ as $h \rightarrow 0$ even with the rational term blowing up at $h = 0 Thanks in advance.","I’m trying to understand a proof in Evan’s book, “Partial Differential Equations.” On page 23, he states If $u$ is the fundamental solution to Laplace’s Equation then $ u \in C^2(\mathbb{R}^n)$ and $u$ satisfies Poisson’s Equation. I have assorted questions throughout the course of the proof which I couldn’t find the answers to anywhere on stack exchange. I’m also not sure how to google them because they’re so specific. We have  $$u(x) = \int_{\mathbb{R}^n} \Phi(x-y) f(y) \ dy =\int_{\mathbb{R}^n} f(x-y) \Phi(y) \ dy. \ \ (1)$$ My first question arises in this equality: Why is it true? I know the reasoning above the proof in the text suggests that $u(x)$ is the convolution of the fundamental solution and $f(x),$ but why is it justified to change their arguments and let them equal one another? Next we have $$\frac{u(x+he_i)-u(x)}{h} = \int_{\mathbb{R}^n} \Phi(y) \left[\frac{f(x+he_i-y) -f(x-y)}{h}\right]dy. \ \ (2)$$ I believe what he did here (correct me if I’m wrong) is approximate the derivatives on either side. He then writes $$\frac{\partial u}{\partial x_i}(x) = \int_{\mathbb{R}^n} \Phi(y) \frac{\partial f}{\partial x_i} \ dy. \ \ (3)$$ He uses the fact that $\frac{f(x+he_i-y)-f(x-y)}{h} \rightarrow \frac{\partial f}{\partial x_i}(x-y)$ as $h \rightarrow 0$ which I also don’t see because at $h = 0,$ the $\frac{f(x+he_i-y)-f(x-y)}{h}$  term blows up. Once again, I tried but, given its specificity, finding the answers to these questions online is impossible. Now, he computes the second partial derivative and concludes $u \in C^2(\mathbb{R}^n).$ The other half of the proof I understand. I was able to find existing questions on this site pertaining to that. I know I asked a lot of questions, so to recap: Why is it justified to switch the arguments of the functions $\Phi$ and $f$ in equation (1)? Why are the two integrals equal? Is it true that all Evans did in equation two is change the differential terms to approximations, or is there more to it than that? Why is equation (3) true? Why does $\frac{f(x+he_i-y)-f(x-y)}{h} \rightarrow \frac{\partial f}{x_i}$ as $h \rightarrow 0$ even with the rational term blowing up at $h = 0 Thanks in advance.",,['calculus']
63,Line integral over a non-central ellipse.,Line integral over a non-central ellipse.,,"I have to find the integral $$ \int_C ydx + x^2dy$$ over the curve $C$ given as a intersection of plane $z=0$ and surface $\frac{x^2}{a^2} + \frac{y^2}{b^2}=\frac{x}{a}+\frac{y}{b} $, curve $C$ is positively oriented $(a\geq b>0)$. This is what i have this far: given plane is $xy$ plane, it's intersection with this surface (whatever it is in the three-dimensional space) should be some sort of ellipse (non-origin ellipse) obviously, and the limits of integration should be from $0$ to $2\pi$. Now, after little bit of algebraic manipulation of the surface equation i got the following equation: $$\frac{(x-\frac{a}{2})^2}{\frac{a^2(a^2+b^2)}{4}} + \frac{(x-\frac{b}{2})^2}{\frac{b^2(b^2+a^2)}{4}}=1$$ Which is indeed an ellipse eqation, now , i suppose i should introduce polar coordinates here, in order to get the parametric equations for this curve, for this ellipse they should look something like this: $x=\frac{a}{2}+\frac{a^2(a^2+b^2)}{4}\cos t \\y=\frac{b}{2}+\frac{b^2(a^2+b^2)}{4}\sin t \\ dx=-\frac{a^2(a^2+b^2)}{4}\sin t \\dy=\frac{b^2(a^2+b^2)}{4}\cos t $ Now, all i should do is to insert this into the given integral, but, i am not quite sure is this legitimate approach. Any suggestions or comments if this is incorrect is appreciated or if it is correct, any advice on how to do this more easily is appreciated too.","I have to find the integral $$ \int_C ydx + x^2dy$$ over the curve $C$ given as a intersection of plane $z=0$ and surface $\frac{x^2}{a^2} + \frac{y^2}{b^2}=\frac{x}{a}+\frac{y}{b} $, curve $C$ is positively oriented $(a\geq b>0)$. This is what i have this far: given plane is $xy$ plane, it's intersection with this surface (whatever it is in the three-dimensional space) should be some sort of ellipse (non-origin ellipse) obviously, and the limits of integration should be from $0$ to $2\pi$. Now, after little bit of algebraic manipulation of the surface equation i got the following equation: $$\frac{(x-\frac{a}{2})^2}{\frac{a^2(a^2+b^2)}{4}} + \frac{(x-\frac{b}{2})^2}{\frac{b^2(b^2+a^2)}{4}}=1$$ Which is indeed an ellipse eqation, now , i suppose i should introduce polar coordinates here, in order to get the parametric equations for this curve, for this ellipse they should look something like this: $x=\frac{a}{2}+\frac{a^2(a^2+b^2)}{4}\cos t \\y=\frac{b}{2}+\frac{b^2(a^2+b^2)}{4}\sin t \\ dx=-\frac{a^2(a^2+b^2)}{4}\sin t \\dy=\frac{b^2(a^2+b^2)}{4}\cos t $ Now, all i should do is to insert this into the given integral, but, i am not quite sure is this legitimate approach. Any suggestions or comments if this is incorrect is appreciated or if it is correct, any advice on how to do this more easily is appreciated too.",,"['integration', 'multivariable-calculus']"
64,Example of multivalued function that attains maximum when values form evenly spaced vector,Example of multivalued function that attains maximum when values form evenly spaced vector,,"I have four variables $x,y,z,w$, such that $x\ge1,w\le7, x\le y\le z\le w$. I need to find a function that attains maximum when $x,y,z,w$ are evenly spaced on $[1,7]$, i.e. $(x,y,z,w)=\left(1,3,5,7\right)$ and minimum when all variables are equal. $(4-(y-x))^2+(4-(z-x))^2+(4-(z-y))^2+(4-(w-x))^2+(4-(w-y))^2+(4-(w-z))^2$ is an example of such function, but depending on values $4-(y-x)$ can be either positive or negative. I need it to be either positive or negative. Perhaps, somebody knows other examples of such functions or relevant literature. Thank you in advance.","I have four variables $x,y,z,w$, such that $x\ge1,w\le7, x\le y\le z\le w$. I need to find a function that attains maximum when $x,y,z,w$ are evenly spaced on $[1,7]$, i.e. $(x,y,z,w)=\left(1,3,5,7\right)$ and minimum when all variables are equal. $(4-(y-x))^2+(4-(z-x))^2+(4-(z-y))^2+(4-(w-x))^2+(4-(w-y))^2+(4-(w-z))^2$ is an example of such function, but depending on values $4-(y-x)$ can be either positive or negative. I need it to be either positive or negative. Perhaps, somebody knows other examples of such functions or relevant literature. Thank you in advance.",,"['calculus', 'multivariable-calculus', 'optimization', 'partial-derivative', 'nonlinear-optimization']"
65,Find the mapping such that the given region is mapped onto a rectangle,Find the mapping such that the given region is mapped onto a rectangle,,"I want to find a continuously differentiable and one-to-one mapping from the first quadrant of $\Bbb{R}^2$ to itself such that the region bounded by $x^2\le y\le 2x^2$ and $1\le xy \le 3$ is mapped to a rectangle. Is there some systematic way to find such mappings? Right now I am just guessing and it is not very effective. In other words what change of variables $(x,y)\to(u,v)$ will change the curve of $y=ax^2$ to $y= C_1$ and $y=b/x$ to $x=C_2$ for some constants $C_1,C_2$. Is the best way just by inspection?","I want to find a continuously differentiable and one-to-one mapping from the first quadrant of $\Bbb{R}^2$ to itself such that the region bounded by $x^2\le y\le 2x^2$ and $1\le xy \le 3$ is mapped to a rectangle. Is there some systematic way to find such mappings? Right now I am just guessing and it is not very effective. In other words what change of variables $(x,y)\to(u,v)$ will change the curve of $y=ax^2$ to $y= C_1$ and $y=b/x$ to $x=C_2$ for some constants $C_1,C_2$. Is the best way just by inspection?",,['multivariable-calculus']
66,Are the components of the curvature tensor w.r.t *changing* normal coordinates smooth?,Are the components of the curvature tensor w.r.t *changing* normal coordinates smooth?,,"Let $M$ be a smooth Riemannian manifold, and let $p \in M$. Let $U$ be a sufficiently small neighbourhood of $p$, such that $U$ is a normal neighbourhood of each of its points. We can choose $U$ in such a way that for every $q\in U$, $\exp_q: B_{h_0}^q(0) \to B_{h_0}(q)$ is a diffeomorphism. Now, we fix a smooth orthonormal frame $F$ of $TM|_U$. For every $q\in U$, we identify $T_qM \sim \mathbb{R}^d$ using $F_q$, so that $\exp_q$ defines normal coordinates on $B_{h_0}(q)$. Finally, let $\mathcal{R}_{ijkl}(q)$ be the components of the curvature tensor of $M$ at the point $q$, calculated w.r.t the normal coordinates centered around $q$ (using $\exp_q$ and $F_q$ as described above). I am trying to show the map $q\mapsto \mathcal{R}_{ijkl}(q)$ is smooth, or at least continuous. I know that the exponential map is smooth when regarded as $\exp:TM|_U \to M$, but I am not sure how to use this here. Any ideas?","Let $M$ be a smooth Riemannian manifold, and let $p \in M$. Let $U$ be a sufficiently small neighbourhood of $p$, such that $U$ is a normal neighbourhood of each of its points. We can choose $U$ in such a way that for every $q\in U$, $\exp_q: B_{h_0}^q(0) \to B_{h_0}(q)$ is a diffeomorphism. Now, we fix a smooth orthonormal frame $F$ of $TM|_U$. For every $q\in U$, we identify $T_qM \sim \mathbb{R}^d$ using $F_q$, so that $\exp_q$ defines normal coordinates on $B_{h_0}(q)$. Finally, let $\mathcal{R}_{ijkl}(q)$ be the components of the curvature tensor of $M$ at the point $q$, calculated w.r.t the normal coordinates centered around $q$ (using $\exp_q$ and $F_q$ as described above). I am trying to show the map $q\mapsto \mathcal{R}_{ijkl}(q)$ is smooth, or at least continuous. I know that the exponential map is smooth when regarded as $\exp:TM|_U \to M$, but I am not sure how to use this here. Any ideas?",,"['multivariable-calculus', 'riemannian-geometry', 'smooth-manifolds', 'coordinate-systems']"
67,How are the two forms of Green's theorem are equivalent?,How are the two forms of Green's theorem are equivalent?,,"By the book's reasoning the two forms of Green's theorem are equivalent because if let F= G1 for the tangential form, we'd obtain the equation of the normal form of green's theorem and if assumed F=G2 in the Normal Form, we'd obtain the equation of the Tangential Form. How does being able to assume different vector fields F and plugging that vector field F in to obtain the other counterpart/side of Green's theorem/ the other side of the coin of Green's theorem imply that the tangential forma and normal form are equivalent? Why/how are there two versions of Green's theorem that are equivalent? The two forms don't look the same to me. We're substituting different vector field Fs in to make the circulation convertible to normal form and vise versa to show that theorem 4 equivalent to theorem 5 and that doesn't make sense or sound correct as a proof. Below: they used this switching technique of vector fields that I'm uncomfortable with.  I don't think what they're doing makes sense:","By the book's reasoning the two forms of Green's theorem are equivalent because if let F= G1 for the tangential form, we'd obtain the equation of the normal form of green's theorem and if assumed F=G2 in the Normal Form, we'd obtain the equation of the Tangential Form. How does being able to assume different vector fields F and plugging that vector field F in to obtain the other counterpart/side of Green's theorem/ the other side of the coin of Green's theorem imply that the tangential forma and normal form are equivalent? Why/how are there two versions of Green's theorem that are equivalent? The two forms don't look the same to me. We're substituting different vector field Fs in to make the circulation convertible to normal form and vise versa to show that theorem 4 equivalent to theorem 5 and that doesn't make sense or sound correct as a proof. Below: they used this switching technique of vector fields that I'm uncomfortable with.  I don't think what they're doing makes sense:",,"['multivariable-calculus', 'vector-analysis', 'greens-theorem']"
68,"In a conservative line integral, do flipping the points make the integral negative?","In a conservative line integral, do flipping the points make the integral negative?",,"I know that conservative line integrals are path independent. But what do flipping the points change the value of the integral. For example any path from A to B in a certain domain will be the same, but will B to A just be the negative of it? I know that for vector fields, it's generally turns negative and for scalar fields it's the same value regardless of the starting and ending points. But for conservative vector fields, is it any different? Yeah I just realized I was stupid. You're going with the vector or against the vector so the 'work' or the value will be negative if you flip the points. Thanks for the comments people!","I know that conservative line integrals are path independent. But what do flipping the points change the value of the integral. For example any path from A to B in a certain domain will be the same, but will B to A just be the negative of it? I know that for vector fields, it's generally turns negative and for scalar fields it's the same value regardless of the starting and ending points. But for conservative vector fields, is it any different? Yeah I just realized I was stupid. You're going with the vector or against the vector so the 'work' or the value will be negative if you flip the points. Thanks for the comments people!",,"['integration', 'multivariable-calculus', 'partial-derivative']"
69,Evaluating iterated integral with 3 variables,Evaluating iterated integral with 3 variables,,"I am asked to evaluate the integral by hand. I do not know how to start $$\int_0^4\int_x^4\int_0^y\frac{6}{1 + 48z - z^3}\, dz\, dy\, dx$$ Note: This is a homework question. Explicit permission to seek help from others is given. I do not see a suitable $u$-sub nor can I factor out $z$ for an easier integral. Changing the order of integration does not seem to help either (i.e., to $dy\, dz\, dx$). Even WolframAlpha can only show it is approximately equal to $4.859\,81$. Symbolab says steps are not supported for this type of question.","I am asked to evaluate the integral by hand. I do not know how to start $$\int_0^4\int_x^4\int_0^y\frac{6}{1 + 48z - z^3}\, dz\, dy\, dx$$ Note: This is a homework question. Explicit permission to seek help from others is given. I do not see a suitable $u$-sub nor can I factor out $z$ for an easier integral. Changing the order of integration does not seem to help either (i.e., to $dy\, dz\, dx$). Even WolframAlpha can only show it is approximately equal to $4.859\,81$. Symbolab says steps are not supported for this type of question.",,"['integration', 'multivariable-calculus', 'definite-integrals', '3d']"
70,Converting this particular double integral to an iterated polar,Converting this particular double integral to an iterated polar,,"In class, the professor converted this double integral: $$\int_{-4}^0 \int_{-\sqrt{16-x^2}}^{\sqrt{16-x^2}} f(x,y) \,dy\,dx$$ into the following iterated polar: $$\int_{\pi/2}^{\pi} \int_{0}^{4} f(r,\theta) r\,dr\,d\theta$$ My question is this: why is the outer integral for the iterated polar from $\pi$/2 to $\pi$? Shouldn't it extend from $\pi$/2 to $\frac{3{\pi}}{2}$? In other words, shouldn't the iterated polar read: $$\int_{\pi/2}^{\frac{3{\pi}}{2}} \int_{0}^{4} f(r,\theta) r\,dr\,d\theta$$ Any insight would be deeply appreciated. Thank you. Edit 1: Reformatted iterated polar to represent proper values.","In class, the professor converted this double integral: $$\int_{-4}^0 \int_{-\sqrt{16-x^2}}^{\sqrt{16-x^2}} f(x,y) \,dy\,dx$$ into the following iterated polar: $$\int_{\pi/2}^{\pi} \int_{0}^{4} f(r,\theta) r\,dr\,d\theta$$ My question is this: why is the outer integral for the iterated polar from $\pi$/2 to $\pi$? Shouldn't it extend from $\pi$/2 to $\frac{3{\pi}}{2}$? In other words, shouldn't the iterated polar read: $$\int_{\pi/2}^{\frac{3{\pi}}{2}} \int_{0}^{4} f(r,\theta) r\,dr\,d\theta$$ Any insight would be deeply appreciated. Thank you. Edit 1: Reformatted iterated polar to represent proper values.",,['multivariable-calculus']
71,"Find the tangent plane to the surface with parametric equations $x=u^2, y=v^2, z=u+2v$ at the point $s(1, 1, 3)$.",Find the tangent plane to the surface with parametric equations  at the point .,"x=u^2, y=v^2, z=u+2v s(1, 1, 3)","My prof in the class got $x=1+2s$, $y=1+2t$ and $z=3+s+2t$ but in the book says  $x+2y-2z+3=0$. Both of them were first solving for $r(u)$ and $r(v)$ and I get that part but in the book they were using the cross product then and my prof wasn't, he was substituting $s$ and $t$. Can someone tell me what the correct answer is or if they are both correct?","My prof in the class got $x=1+2s$, $y=1+2t$ and $z=3+s+2t$ but in the book says  $x+2y-2z+3=0$. Both of them were first solving for $r(u)$ and $r(v)$ and I get that part but in the book they were using the cross product then and my prof wasn't, he was substituting $s$ and $t$. Can someone tell me what the correct answer is or if they are both correct?",,"['multivariable-calculus', 'vectors', 'tangent-line', 'parametrization']"
72,"With $f$ defined on the unit disc $D: x^2 +y^2 \leq 1$, and given that $\nabla f(1,0) = (1, 1)$, could $f$ reach its maximal value on $D$ at $(1, 0)$?","With  defined on the unit disc , and given that , could  reach its maximal value on  at ?","f D: x^2 +y^2 \leq 1 \nabla f(1,0) = (1, 1) f D (1, 0)","$f$ is a continuous function with continuous partial derivatives. With $f$ defined on the unit disc $D: x^2 +y^2 \leq 1$ , and given that $\nabla f(1,0) = (1, 1)$ , could $f$ reach its maximal value on $D$ at $(1, 0)$ ? I know that the answer is no, but I am trying to prove how. I would love to share my input, but I'm at a complete loss as to what to do here. Any hint or suggestion would be welcome.","is a continuous function with continuous partial derivatives. With defined on the unit disc , and given that , could reach its maximal value on at ? I know that the answer is no, but I am trying to prove how. I would love to share my input, but I'm at a complete loss as to what to do here. Any hint or suggestion would be welcome.","f f D: x^2 +y^2 \leq 1 \nabla f(1,0) = (1, 1) f D (1, 0)","['multivariable-calculus', 'partial-derivative', 'maxima-minima']"
73,Book reference for Double/ triple integrals,Book reference for Double/ triple integrals,,"Can someone please suggest me a Calculus book that includes Double integrals, triple integrals, volume bounded between two curves, line integrals and surface inetgrals? I am looking for a book with plenty of examples and with geometrical approach. Thanks","Can someone please suggest me a Calculus book that includes Double integrals, triple integrals, volume bounded between two curves, line integrals and surface inetgrals? I am looking for a book with plenty of examples and with geometrical approach. Thanks",,"['calculus', 'multivariable-calculus', 'reference-request', 'book-recommendation']"
74,Multivariable Chain Rule / Partial Derivatives,Multivariable Chain Rule / Partial Derivatives,,"If $z = f(x − y)$ and $g(x, y) = x − y$, so that $z =f∘g$ Why/How does the chain rule imply this? $$\frac{∂z}{∂x} =\frac{∂f}{∂g}\frac{∂g}{∂x} =\frac{∂f}{∂g}$$ and $$\frac{∂z}{∂y}=\frac{∂f}{∂g}\frac{∂g}{∂y} =-\frac{∂f}{∂g}$$","If $z = f(x − y)$ and $g(x, y) = x − y$, so that $z =f∘g$ Why/How does the chain rule imply this? $$\frac{∂z}{∂x} =\frac{∂f}{∂g}\frac{∂g}{∂x} =\frac{∂f}{∂g}$$ and $$\frac{∂z}{∂y}=\frac{∂f}{∂g}\frac{∂g}{∂y} =-\frac{∂f}{∂g}$$",,"['calculus', 'multivariable-calculus', 'partial-derivative', 'chain-rule']"
75,"Unsolvable iterated integral: $\int_0^8\int_{y^{1/3}}^2 4e^{x^4} dx\,dy$",Unsolvable iterated integral:,"\int_0^8\int_{y^{1/3}}^2 4e^{x^4} dx\,dy","Having issues solving the following iterated integral, which appears to not have a tangible antiderivative. $$\int_0^8\int_{y^{1/3}}^2 4e^{x^4} dx\,dy$$ So, I figure it as a graph with the domain of $(x,y): 0≤y≤8, y{^{\frac{1}{3}}≤x≤2}$, making the graph bound by the lines $y=x^3, x=2$, and $y=8$. However, switching the integrals around does not clarify this equation whatsoever. Should I take the ln of the entire function? Please help.","Having issues solving the following iterated integral, which appears to not have a tangible antiderivative. $$\int_0^8\int_{y^{1/3}}^2 4e^{x^4} dx\,dy$$ So, I figure it as a graph with the domain of $(x,y): 0≤y≤8, y{^{\frac{1}{3}}≤x≤2}$, making the graph bound by the lines $y=x^3, x=2$, and $y=8$. However, switching the integrals around does not clarify this equation whatsoever. Should I take the ln of the entire function? Please help.",,"['calculus', 'integration', 'multivariable-calculus', 'iterated-integrals']"
76,"If $A$ is a linear map, then $A(\alpha(t))'=A(\alpha'(t))$ [duplicate]","If  is a linear map, then  [duplicate]",A A(\alpha(t))'=A(\alpha'(t)),"This question already has answers here : Derivative of a linear transformation. (2 answers) Closed 6 years ago . Let $A:\mathbb R^3\to \mathbb R^3$ be a linear map. I need the following technical result to solve a problem I'm solving: $$A(\alpha(t))'=A(\alpha'(t))$$ Where $\alpha:I\to \mathbb R^3$ is a parametric curve. Intuitively, I know this is true, but I would like to prove it formally.","This question already has answers here : Derivative of a linear transformation. (2 answers) Closed 6 years ago . Let $A:\mathbb R^3\to \mathbb R^3$ be a linear map. I need the following technical result to solve a problem I'm solving: $$A(\alpha(t))'=A(\alpha'(t))$$ Where $\alpha:I\to \mathbb R^3$ is a parametric curve. Intuitively, I know this is true, but I would like to prove it formally.",,"['multivariable-calculus', 'differential-geometry']"
77,"What is meant by $d(x,y)$ in Fubini’s theorem?",What is meant by  in Fubini’s theorem?,"d(x,y)","According to Wolfram MathWorld , Fubini’s theorem takes a multiple integral over a region $R=\{(x,y):x\in[a,b]\wedge y\in[c,d]\}$ and turns it into an iterated integral by the relationship $$\iint_Rf(x,y)\,d(x,y)=\int_a^b\int_c^df(x,y)\,dy\,dx$$ I have never seen the differential $d(x,y)$ before. Normally, I see something of the nature $dA=dx\,dy$. Could someone explain what $d(x,y)$ means and whence it came?","According to Wolfram MathWorld , Fubini’s theorem takes a multiple integral over a region $R=\{(x,y):x\in[a,b]\wedge y\in[c,d]\}$ and turns it into an iterated integral by the relationship $$\iint_Rf(x,y)\,d(x,y)=\int_a^b\int_c^df(x,y)\,dy\,dx$$ I have never seen the differential $d(x,y)$ before. Normally, I see something of the nature $dA=dx\,dy$. Could someone explain what $d(x,y)$ means and whence it came?",,"['multivariable-calculus', 'multiple-integral', 'iterated-integrals']"
78,"For $f(x)=\sin(x^2-y^2)$, sketch a picture showing regions in $\mathbb{R}^2$ where the expression is positive or negative.","For , sketch a picture showing regions in  where the expression is positive or negative.",f(x)=\sin(x^2-y^2) \mathbb{R}^2,"For $f(x)=\sin(x^2-y^2)$, sketch a picture showing regions in $\mathbb{R}^2$ where the expression is positive or negative, zero, or not defined. $z=\sin(x^2-y^2)$ $z=0\to x^2 = y^2$, its a $\times$ at the origin, two crossed lines If $z\neq 0$, then we get the form $\sin(x^2-y^2)=c$, where $c$ is either positive or negative. Notice that $-1\leq c \leq 1$. So then we get that $x^2-y^2=\sin^{-1}(c)$ If the $\sin^{-1}(c)>0$, then we have a hyperbola facing sideways, and if $\sin^{-1}(c)<0$, we have hyperbola facing up/down. So our contour plot is something like this. $z=0$ when we are in those ""crossed lines at the origin"" What about when $z>0, z<0$, this how do I find out? How do I show the values where $\sin^{-1}c > 0$, and $\sin^{-1}c < 0$?","For $f(x)=\sin(x^2-y^2)$, sketch a picture showing regions in $\mathbb{R}^2$ where the expression is positive or negative, zero, or not defined. $z=\sin(x^2-y^2)$ $z=0\to x^2 = y^2$, its a $\times$ at the origin, two crossed lines If $z\neq 0$, then we get the form $\sin(x^2-y^2)=c$, where $c$ is either positive or negative. Notice that $-1\leq c \leq 1$. So then we get that $x^2-y^2=\sin^{-1}(c)$ If the $\sin^{-1}(c)>0$, then we have a hyperbola facing sideways, and if $\sin^{-1}(c)<0$, we have hyperbola facing up/down. So our contour plot is something like this. $z=0$ when we are in those ""crossed lines at the origin"" What about when $z>0, z<0$, this how do I find out? How do I show the values where $\sin^{-1}c > 0$, and $\sin^{-1}c < 0$?",,"['calculus', 'multivariable-calculus', 'trigonometry', 'contour-integration', '3d']"
79,An inequality in four variables [closed],An inequality in four variables [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Let $a$, $b$, $c$ and $d$ be positive real numbers. Prove that:  $$\frac{ab+bc+ca}{a^3+b^3+c^3}+\frac{ab+bd+da}{a^3+b^3+d^3}+\frac{ac+cd+da}{a^3+c^3+d^3}+\frac{bc+cd+db}{b^3+c^3+d^3}\le\min\left [\frac{a^2+b^2}{(ab)^{\frac 32}}+\frac{c^2+d^2}{(cd)^{\frac 32}},\frac{a^2+c^2}{(ac)^{\frac 32}}+\frac{b^2+d^2}{(bd)^{\frac 32}},\frac{a^2+d^2}{(ad)^{\frac 32}}+\frac{b^2+c^2}{(bc)^{\frac 32}}\right ].$$ My Solution as follows: $\frac{ab+bc+ca}{a^3+b^3+c^3}\le \frac{ab+bc+ca}{3abc}=\frac 13\left(\frac 1a+\frac 1b+\frac 1c\right)$ $\implies\frac{ab+bc+ca}{a^3+b^3+c^3}+\frac{ab+bd+da}{a^3+b^3+d^3}+\frac{ac+cd+da}{a^3+c^3+d^3}+\frac{bc+cd+db}{b^3+c^3+d^3}\le\left(\frac 1a+\frac 1b+\frac 1c+\frac 1d\right)$ And  $a^2+b^2\ge a^{\frac32}b^{\frac12}+a^{\frac12}b^{\frac32}$ (By Rearrangement) $\implies \frac{a^2+b^2}{(ab)^{\frac 32}}\ge\frac{a+b}{ab}=\frac 1a+\frac 1b$ $\implies \frac{a^2+b^2}{(ab)^{\frac 32}}+\frac{c^2+d^2}{(cd)^{\frac 32}}\ge\frac1a+\frac1b+\frac1c+\frac1d$ Similarly each term of Right hand side is greater than or equal to $\frac1a+\frac1b+\frac1c+\frac1d$ Hence the inequality.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Let $a$, $b$, $c$ and $d$ be positive real numbers. Prove that:  $$\frac{ab+bc+ca}{a^3+b^3+c^3}+\frac{ab+bd+da}{a^3+b^3+d^3}+\frac{ac+cd+da}{a^3+c^3+d^3}+\frac{bc+cd+db}{b^3+c^3+d^3}\le\min\left [\frac{a^2+b^2}{(ab)^{\frac 32}}+\frac{c^2+d^2}{(cd)^{\frac 32}},\frac{a^2+c^2}{(ac)^{\frac 32}}+\frac{b^2+d^2}{(bd)^{\frac 32}},\frac{a^2+d^2}{(ad)^{\frac 32}}+\frac{b^2+c^2}{(bc)^{\frac 32}}\right ].$$ My Solution as follows: $\frac{ab+bc+ca}{a^3+b^3+c^3}\le \frac{ab+bc+ca}{3abc}=\frac 13\left(\frac 1a+\frac 1b+\frac 1c\right)$ $\implies\frac{ab+bc+ca}{a^3+b^3+c^3}+\frac{ab+bd+da}{a^3+b^3+d^3}+\frac{ac+cd+da}{a^3+c^3+d^3}+\frac{bc+cd+db}{b^3+c^3+d^3}\le\left(\frac 1a+\frac 1b+\frac 1c+\frac 1d\right)$ And  $a^2+b^2\ge a^{\frac32}b^{\frac12}+a^{\frac12}b^{\frac32}$ (By Rearrangement) $\implies \frac{a^2+b^2}{(ab)^{\frac 32}}\ge\frac{a+b}{ab}=\frac 1a+\frac 1b$ $\implies \frac{a^2+b^2}{(ab)^{\frac 32}}+\frac{c^2+d^2}{(cd)^{\frac 32}}\ge\frac1a+\frac1b+\frac1c+\frac1d$ Similarly each term of Right hand side is greater than or equal to $\frac1a+\frac1b+\frac1c+\frac1d$ Hence the inequality.",,"['multivariable-calculus', 'inequality', 'radicals', 'fractions', 'muirhead-inequality']"
80,"Find the limit to $\lim\limits_{(x,y)\to(0,0)}\frac{x^5+y^2}{x^4+|y|}$",Find the limit to,"\lim\limits_{(x,y)\to(0,0)}\frac{x^5+y^2}{x^4+|y|}","My problem is evaluating the following limit: $$\lim_{(x,y)\to(0,0)}\frac{x^5+y^2}{x^4+|y|}$$ The answer should be 0. I tried to convert the limit into polar form, but it didn't help because I couldn't isolate the $r$ and $\theta$-variables of the expression. My ""toolbox"" for solving problems like these is very limited... If polar form doesn't work, then I usually have no clue on how to continue. Edit : I think this is the solution.  $$ \lim_{(x,y)\to(0,0)}\left|\frac{x^5+y^2}{x^4+|y|}\right| = \frac{|x^5+y^2|}{|x^4+|y||} $$ Applying the triangle inequality gives $$ \frac{|x^5+y^2|}{|x^4+|y||} \leq \left|\frac{x^5}{x^4+|y|}\right| + \left|\frac{y^2}{x^4+|y|}\right| $$ Inspecting the denominators on the RHS gives: $$ \left|\frac{x^5}{x^4+|y|}\right| \leq |x|, \quad\left|\frac{y^2}{x^4+|y|}\right| \leq |y| $$ So $$ \left|\frac{x^5}{x^4+|y|}\right| + \left|\frac{y^2}{x^4+|y|}\right| \leq |x| + |y| $$ Since $|x| + |y| \to 0$ when $x,y\to 0$, the sandwich theorem states that $|\frac{x^5+y^2}{x^4+|y|}| \to 0$. And if $\lim |f(x)|=0$ then $\lim f(x)=0$ which solves the original problem.","My problem is evaluating the following limit: $$\lim_{(x,y)\to(0,0)}\frac{x^5+y^2}{x^4+|y|}$$ The answer should be 0. I tried to convert the limit into polar form, but it didn't help because I couldn't isolate the $r$ and $\theta$-variables of the expression. My ""toolbox"" for solving problems like these is very limited... If polar form doesn't work, then I usually have no clue on how to continue. Edit : I think this is the solution.  $$ \lim_{(x,y)\to(0,0)}\left|\frac{x^5+y^2}{x^4+|y|}\right| = \frac{|x^5+y^2|}{|x^4+|y||} $$ Applying the triangle inequality gives $$ \frac{|x^5+y^2|}{|x^4+|y||} \leq \left|\frac{x^5}{x^4+|y|}\right| + \left|\frac{y^2}{x^4+|y|}\right| $$ Inspecting the denominators on the RHS gives: $$ \left|\frac{x^5}{x^4+|y|}\right| \leq |x|, \quad\left|\frac{y^2}{x^4+|y|}\right| \leq |y| $$ So $$ \left|\frac{x^5}{x^4+|y|}\right| + \left|\frac{y^2}{x^4+|y|}\right| \leq |x| + |y| $$ Since $|x| + |y| \to 0$ when $x,y\to 0$, the sandwich theorem states that $|\frac{x^5+y^2}{x^4+|y|}| \to 0$. And if $\lim |f(x)|=0$ then $\lim f(x)=0$ which solves the original problem.",,"['limits', 'multivariable-calculus']"
81,Find a particular path for Vanishing the integral,Find a particular path for Vanishing the integral,,"Is it possible to find a path from P to Q such that: $$\int_C xy^2dx+ydy=0;\, P=(0,0)\, \text{and}\, Q=(1,1)$$ $$\int_C \frac{-ydx+xdy}{x^2+y^2}=0;\, P(-1,0)\, \text{and}\, Q=(1,0)$$ I've asked before for the possibility to find a path for vanishing in general, but now I want to know if there's a path for that particular points, Hints?","Is it possible to find a path from P to Q such that: I've asked before for the possibility to find a path for vanishing in general, but now I want to know if there's a path for that particular points, Hints?","\int_C xy^2dx+ydy=0;\, P=(0,0)\, \text{and}\, Q=(1,1) \int_C \frac{-ydx+xdy}{x^2+y^2}=0;\, P(-1,0)\, \text{and}\, Q=(1,0)","['integration', 'multivariable-calculus', 'vector-analysis', 'contour-integration']"
82,Show that if $A^*A=I$ then $\Delta(u\circ A)=\Delta u\circ A$,Show that if  then,A^*A=I \Delta(u\circ A)=\Delta u\circ A,"Let $u\in C^2(\Bbb R^n,\Bbb R)$ and $A\in\mathcal L(\Bbb R^n)$. Show that if $A^*A=I$ then $\Delta(u\circ A)=\Delta u\circ A$. Here $\Delta$ is the laplacian operator ( here is a similar question but this doesnt help me so much). What I did was: $$\partial^2(u\circ A)=(\partial^2 u\circ A)[\partial A]^2+(\partial u\circ A)\partial^2 A$$ and because $\partial^2 A=0$ and $\partial Ax=A$ we found that $$\partial^2(u\circ A)(x)=(\partial^2 u\circ A)(x)A^2$$ However $A^2$ is not necessarily the identity, so I dont know exactly how to continue. Some help will be appreciated, thank you.","Let $u\in C^2(\Bbb R^n,\Bbb R)$ and $A\in\mathcal L(\Bbb R^n)$. Show that if $A^*A=I$ then $\Delta(u\circ A)=\Delta u\circ A$. Here $\Delta$ is the laplacian operator ( here is a similar question but this doesnt help me so much). What I did was: $$\partial^2(u\circ A)=(\partial^2 u\circ A)[\partial A]^2+(\partial u\circ A)\partial^2 A$$ and because $\partial^2 A=0$ and $\partial Ax=A$ we found that $$\partial^2(u\circ A)(x)=(\partial^2 u\circ A)(x)A^2$$ However $A^2$ is not necessarily the identity, so I dont know exactly how to continue. Some help will be appreciated, thank you.",,"['multivariable-calculus', 'laplacian']"
83,"Determine whether $A (2, 2, 3)$, $B(4, 0, 7)$, $C (6, 3, 1)$ and $D (2, −3, 11)$ are in the same plane.","Determine whether , ,  and  are in the same plane.","A (2, 2, 3) B(4, 0, 7) C (6, 3, 1) D (2, −3, 11)","a) Compute a suitable volume to determine whether $A  (2, 2, 3)$, $B  (4, 0, 7)$, $C  (6, 3, 1)$ and $D  (2, −3, 11)$ are in the same plane. b) Find the distance between the line $L$ through $A$, $B$ and the   line $M$ through $C$, $D$. My answer: V = |(a-d) · ((b-d)x(c-d))| / 6 = -15/2 Thoughts? Help?!?!","a) Compute a suitable volume to determine whether $A  (2, 2, 3)$, $B  (4, 0, 7)$, $C  (6, 3, 1)$ and $D  (2, −3, 11)$ are in the same plane. b) Find the distance between the line $L$ through $A$, $B$ and the   line $M$ through $C$, $D$. My answer: V = |(a-d) · ((b-d)x(c-d))| / 6 = -15/2 Thoughts? Help?!?!",,"['multivariable-calculus', 'vectors']"
84,Distance between two lines in parametric form,Distance between two lines in parametric form,,"a) Parametrize the line $L$ through $P = (2, 1, 2)$ that intersects the   line $x = 1 + t$, $y = 1 − t$, $z = 2t$ perpendicularly. b) Parametrize the $z$ axis. c) What is the distance from this line $L$ to the $z$-axis? My work For the part a), I got the equation was $x = 2 + \frac{t}{6}$ ; $y = 1 + \frac{5t}{6}$ ; $z = 2 + \frac{t}{3}$. Stuck with the last 2 parts. Anyone want to give this math novice a hand? :D","a) Parametrize the line $L$ through $P = (2, 1, 2)$ that intersects the   line $x = 1 + t$, $y = 1 − t$, $z = 2t$ perpendicularly. b) Parametrize the $z$ axis. c) What is the distance from this line $L$ to the $z$-axis? My work For the part a), I got the equation was $x = 2 + \frac{t}{6}$ ; $y = 1 + \frac{5t}{6}$ ; $z = 2 + \frac{t}{3}$. Stuck with the last 2 parts. Anyone want to give this math novice a hand? :D",,"['calculus', 'multivariable-calculus', 'vectors', 'parametric']"
85,Can we always replace $\sin x$ with $x$ in limit as $x\to 0$,Can we always replace  with  in limit as,\sin x x x\to 0,"Let $D\subset \mathbb{R}^2$, $0$ be a limit point of $D$, and $f:D\to \mathbb{R}$ be a function. How to prove or disprove that $\displaystyle\lim_{x\to 0}f(x,\sin x)=\lim_{x\to 0}f(x, x)$ ? If it is not true, what are the sufficient and necessary conditions such that $\displaystyle\lim_{x\to 0}f(x,\sin x)=\lim_{x\to 0}f(x, x)$ ? Thanks in advances.","Let $D\subset \mathbb{R}^2$, $0$ be a limit point of $D$, and $f:D\to \mathbb{R}$ be a function. How to prove or disprove that $\displaystyle\lim_{x\to 0}f(x,\sin x)=\lim_{x\to 0}f(x, x)$ ? If it is not true, what are the sufficient and necessary conditions such that $\displaystyle\lim_{x\to 0}f(x,\sin x)=\lim_{x\to 0}f(x, x)$ ? Thanks in advances.",,"['calculus', 'limits', 'multivariable-calculus']"
86,"Largest set in which $(x, y)\mapsto \sqrt{xe^y - ye^x}$ is defined",Largest set in which  is defined,"(x, y)\mapsto \sqrt{xe^y - ye^x}","I have to find the biggest $\mathbb{R}^2$ subset in which the function $g(x,y) = \sqrt{xe^y - ye^x}$ is defined. In order to do it I have to study $xe^y - ye^x \ge 0$ but I can't find out a solution. Can someone help me?","I have to find the biggest $\mathbb{R}^2$ subset in which the function $g(x,y) = \sqrt{xe^y - ye^x}$ is defined. In order to do it I have to study $xe^y - ye^x \ge 0$ but I can't find out a solution. Can someone help me?",,"['functions', 'multivariable-calculus', 'exponential-function']"
87,"If $x^y y^x z^z=c$, then find $\frac {\partial^2z}{\partial x \,\partial y}$ at $x=y=z$.","If , then find  at .","x^y y^x z^z=c \frac {\partial^2z}{\partial x \,\partial y} x=y=z","If $x^y y^x z^z=c$, then find $\dfrac {\partial^2z}{\partial x \,\partial y}$ at $x=y=z$. I tried taking $\log$ but that doesn't help. Any hints will be appreciated. Thanks. This is what I have tried:","If $x^y y^x z^z=c$, then find $\dfrac {\partial^2z}{\partial x \,\partial y}$ at $x=y=z$. I tried taking $\log$ but that doesn't help. Any hints will be appreciated. Thanks. This is what I have tried:",,"['multivariable-calculus', 'partial-derivative']"
88,Proving $f(\lVert x\rVert)$ is differentiable in $\Bbb R^n$,Proving  is differentiable in,f(\lVert x\rVert) \Bbb R^n,"Let $f: \Bbb R \to \Bbb R$ be an even, differentiable function,  and let $F: \Bbb R^n \to \Bbb R$ be defined as follows: $F(x) = f(\lVert x\rVert)$ ,when $\lVert x\rVert = \sqrt{\sum_{i=1}^n x_i^2}$ . Prove that $F$ is differentiable in $\Bbb R^n$. What I tried: It's pretty easy to see that $F$ is differentiable in every $x \neq 0$. For $x = 0$ I got for the partial derivative of $x_1$: $\frac{\partial F}{\partial x_1}=\lim_{h\to 0} \frac{F(h,...0)-F(0,..,0)}{h} =  \lim_{h\to 0} \frac{f(|h|)-f(0)}{h}$ = $f'(0)$ When the last equality follows since $f$ is even. Next I want to show that  $\frac{\partial F}{\partial x_1}$ is continuous in $0$, and then finish, but I couldn't prove it. **Edit: since $f$ is even and differentiable, we also have $f'(0) = 0$. Still having trouble finishing the proof.","Let $f: \Bbb R \to \Bbb R$ be an even, differentiable function,  and let $F: \Bbb R^n \to \Bbb R$ be defined as follows: $F(x) = f(\lVert x\rVert)$ ,when $\lVert x\rVert = \sqrt{\sum_{i=1}^n x_i^2}$ . Prove that $F$ is differentiable in $\Bbb R^n$. What I tried: It's pretty easy to see that $F$ is differentiable in every $x \neq 0$. For $x = 0$ I got for the partial derivative of $x_1$: $\frac{\partial F}{\partial x_1}=\lim_{h\to 0} \frac{F(h,...0)-F(0,..,0)}{h} =  \lim_{h\to 0} \frac{f(|h|)-f(0)}{h}$ = $f'(0)$ When the last equality follows since $f$ is even. Next I want to show that  $\frac{\partial F}{\partial x_1}$ is continuous in $0$, and then finish, but I couldn't prove it. **Edit: since $f$ is even and differentiable, we also have $f'(0) = 0$. Still having trouble finishing the proof.",,"['multivariable-calculus', 'derivatives', 'normed-spaces']"
89,Evaluate the area of the surface using double integral,Evaluate the area of the surface using double integral,,"Surface of the cylinder $x^2+y^2=2x$ delimited by $z=0$ and   $z=\sqrt{x^2+y^2}$. I have to say that I found similar posts but none has helped me, some gave me wrong anwears. The surface integral I take always gave me $\pi$ in it. The real anwear is 8. I preffer polar coordinates, but it's ok without.","Surface of the cylinder $x^2+y^2=2x$ delimited by $z=0$ and   $z=\sqrt{x^2+y^2}$. I have to say that I found similar posts but none has helped me, some gave me wrong anwears. The surface integral I take always gave me $\pi$ in it. The real anwear is 8. I preffer polar coordinates, but it's ok without.",,"['multivariable-calculus', 'surface-integrals']"
90,Maximize linear function over disk of radius $2$,Maximize linear function over disk of radius,2,"Maximize $f(x, y) = x + 2y$ with constraint $x^2 + y^2 \le 4$. $f(x, y)$ has no CP's so thats something gone. I considered the boundary/edge of $g(x, y) = x^2 + y^2$. And I got the point $(0, 2), (2, 0)$, which gave $f(0, 2) = f(2, 0) = 4$. However, the point $(2/\sqrt(5), 4/\sqrt(5))$ gives a larger max. How do I approach this to get this point?","Maximize $f(x, y) = x + 2y$ with constraint $x^2 + y^2 \le 4$. $f(x, y)$ has no CP's so thats something gone. I considered the boundary/edge of $g(x, y) = x^2 + y^2$. And I got the point $(0, 2), (2, 0)$, which gave $f(0, 2) = f(2, 0) = 4$. However, the point $(2/\sqrt(5), 4/\sqrt(5))$ gives a larger max. How do I approach this to get this point?",,"['calculus', 'multivariable-calculus', 'optimization', 'quadratic-programming', 'non-convex-optimization']"
91,How to solve this equation of two variables,How to solve this equation of two variables,,"$$ F(X,Y)=(t_1^2+t_2^2+2t_1t_2X)^2Y^2-2t_3^2(t_1^2X+t_2^2X+2t_1t_2)Y+t_3^4 $$ I want to know the point $(X,Y)$ which satisfied with $F(X,Y)=0$. Now, $t_1,t_2,t_3$ are positive numbers. By numerical calculation, I noticed $F(1,(\frac{t_3}{t_1+t_2})^2)=0$ when $\frac{t_3}{t_1+t_2}<1$. I want to know how to know this result analytically. Is it possible??","$$ F(X,Y)=(t_1^2+t_2^2+2t_1t_2X)^2Y^2-2t_3^2(t_1^2X+t_2^2X+2t_1t_2)Y+t_3^4 $$ I want to know the point $(X,Y)$ which satisfied with $F(X,Y)=0$. Now, $t_1,t_2,t_3$ are positive numbers. By numerical calculation, I noticed $F(1,(\frac{t_3}{t_1+t_2})^2)=0$ when $\frac{t_3}{t_1+t_2}<1$. I want to know how to know this result analytically. Is it possible??",,"['real-analysis', 'functions', 'multivariable-calculus', 'maxima-minima']"
92,"The reason behind the trick of assuming $p$ and $q$ are independent, differentiating, then applying the relation.","The reason behind the trick of assuming  and  are independent, differentiating, then applying the relation.",p q,"We want to simplify this expression: $$\bar{n}=\sum_{n=0}^N W(n)n=\sum_{n=0}^N\frac{N!}{n!(N-n)!}p^{n}q^{N-n}n$$ where $q=1-p$. The trick used is to write: $$np^{n}=p\frac{\partial}{\partial p}(p^{n})$$ plugging in, $$\bar{n}=\sum_{n=0}^N\frac{N!}{n!(N-n)!}\left[p\frac{\partial}{\partial p}(p^{n})\right]q^{N-n}$$ But the book does something strange here. $$\implies \bar{n}=p\frac{\partial}{\partial p}\left[\sum_{n=0}^N\frac{N!}{n!(N-n)!}p^{n}q^{N-n}\right]$$ the, according to binomial theorem, $$\bar{n}=p\frac{\partial}{\partial p}(p+q)^N$$ $$\bar{n}=pN(p+q)^{N-1}$$ Now, if we use $q=1-p$, $$\bar{n}=pN$$ The question: Why did we consider $p$ and $q$ independent variables and put the partial derivative out of the sum, then differentiate and at last apply the relation between $p$ and $q$? Why are we allowed to do so? (I don't want to know why we decided to do so (then one would simply say ""because otherwise we couldn't solve it!""), but rather what makes us enable to do so.) The book: Fundamentals of Statistical and Thermal Physics, Frederick Reif, First chapter","We want to simplify this expression: $$\bar{n}=\sum_{n=0}^N W(n)n=\sum_{n=0}^N\frac{N!}{n!(N-n)!}p^{n}q^{N-n}n$$ where $q=1-p$. The trick used is to write: $$np^{n}=p\frac{\partial}{\partial p}(p^{n})$$ plugging in, $$\bar{n}=\sum_{n=0}^N\frac{N!}{n!(N-n)!}\left[p\frac{\partial}{\partial p}(p^{n})\right]q^{N-n}$$ But the book does something strange here. $$\implies \bar{n}=p\frac{\partial}{\partial p}\left[\sum_{n=0}^N\frac{N!}{n!(N-n)!}p^{n}q^{N-n}\right]$$ the, according to binomial theorem, $$\bar{n}=p\frac{\partial}{\partial p}(p+q)^N$$ $$\bar{n}=pN(p+q)^{N-1}$$ Now, if we use $q=1-p$, $$\bar{n}=pN$$ The question: Why did we consider $p$ and $q$ independent variables and put the partial derivative out of the sum, then differentiate and at last apply the relation between $p$ and $q$? Why are we allowed to do so? (I don't want to know why we decided to do so (then one would simply say ""because otherwise we couldn't solve it!""), but rather what makes us enable to do so.) The book: Fundamentals of Statistical and Thermal Physics, Frederick Reif, First chapter",,"['multivariable-calculus', 'partial-derivative']"
93,"Find the derivative of the inverse function, $Dg(0,1)$.","Find the derivative of the inverse function, .","Dg(0,1)","Let $f:\mathbb{R}^2\to \mathbb{R}^2$ be defined by the equation \begin{equation} f(x,y)=(x^2-y^2,2xy)\end{equation} Parts (a) and (b) of the problem asked me to show that $f$ is one-to-one on the set $A$ consisting of all $(x,y)$ with $x>0$, and to find set $B=f(A)$, which was easy enough to do. Part (c) asks If $g$ is the inverse function, find $Dg(0,1)$. What I thought to do was to find the matrix $Df$ and find the inverse of it, but I am finding that very challenging to do (here I thought I had my linear algebra skills in check!) Because what I get is that $Df= \begin{bmatrix} 2x &2y\\ -2y & 2x \end{bmatrix}$ And when I try to find the inverse, i get to the step $ \begin{bmatrix} x & y & 1/2 & 0\\ 0 & \frac{x^2+y^2}{x} & \frac{y}{2x} & 1/2 \end{bmatrix}$ And i feel like maybe this is not the correct method to finding the inverse...? Thank you!","Let $f:\mathbb{R}^2\to \mathbb{R}^2$ be defined by the equation \begin{equation} f(x,y)=(x^2-y^2,2xy)\end{equation} Parts (a) and (b) of the problem asked me to show that $f$ is one-to-one on the set $A$ consisting of all $(x,y)$ with $x>0$, and to find set $B=f(A)$, which was easy enough to do. Part (c) asks If $g$ is the inverse function, find $Dg(0,1)$. What I thought to do was to find the matrix $Df$ and find the inverse of it, but I am finding that very challenging to do (here I thought I had my linear algebra skills in check!) Because what I get is that $Df= \begin{bmatrix} 2x &2y\\ -2y & 2x \end{bmatrix}$ And when I try to find the inverse, i get to the step $ \begin{bmatrix} x & y & 1/2 & 0\\ 0 & \frac{x^2+y^2}{x} & \frac{y}{2x} & 1/2 \end{bmatrix}$ And i feel like maybe this is not the correct method to finding the inverse...? Thank you!",,"['multivariable-calculus', 'differential-geometry']"
94,"Detail of a proof about about trace-zero functions in $W^{1,p}$, Thm 2 p.273 in L. Evans' PDE","Detail of a proof about about trace-zero functions in , Thm 2 p.273 in L. Evans' PDE","W^{1,p}","(PDE, Lawrence Evans, second edition 2010, § 5.5  Theorem 2, p. 273-275) The little step I don't understand in the calculations does not require to state the theorem, but I simply copied it from this question about that same theorem Theorem (Trace-zero functions in $W^{1,p}$): Assume $U$ is bounded and $\partial U$ is $C^1$. Suppose furthermore that $u\in W^{1,p}$. Then $$u\in W^{1,p}_0(U)\quad \Longleftrightarrow\quad  Tu=0\text{ on }\partial U$$ The step I don't understand is the two equations between (8) and (9) p.274. For those who don't have access to the book, I'll rewrite with general notations: Let $\varphi \in C^1(\overline{\mathbb{R}}{}^n_+)$ where $\overline{\mathbb{R}}{}^n_+$ denotes the ""half-space"" with boundary, i.e. $\mathbf{x}:=(x', x_n)\in \overline{\mathbb{R}}{}^n_+\enspace \Leftrightarrow\enspace x' \in \mathbb{R}^{n-1},\ x_n \geq 0$. We have $$ \lvert \varphi(x', x_n) \rvert \leq \lvert \varphi(x', 0) \rvert + \int_0^{x_n}\lvert \varphi_{x_n}(x', t) \rvert\, dt $$ ($\varphi_{x_n}$ means partial derivative w.r.t. $x_n$). That's ok. Now it seems that he raises everything to the power $p$ and then integrates w.r.t. $x'$ to get $$ \int_{\mathbb{R}^{n-1}} \lvert \varphi(x', x_n) \rvert^p\, dx' \leq C \left( \int_{\mathbb{R}^{n-1}} \lvert \varphi(x', 0) \rvert^p\, dx' + x_n^{p-1}\int_0^{x_n}\int_{\mathbb{R}^{n-1}}  \lvert D\varphi(x', t) \rvert^p\, dx'\, dt \right)$$ where $\lvert D\varphi(x', t) \rvert$ is the norm of the gradient following the notations of the book. I naively imagined writing the binomial. Then I somehow need to find an upper bound and transform the product of integrals into an integral of a product (2nd term r.h.s.). Oh, writing things down already helped me see that L. Evans indeed already performed $p-1$ integrations $\int_0^{x_n} (\cdots )\, dt $, but I still don't know how the inequality is justified, i.e. what are the suitable constant $C$","(PDE, Lawrence Evans, second edition 2010, § 5.5  Theorem 2, p. 273-275) The little step I don't understand in the calculations does not require to state the theorem, but I simply copied it from this question about that same theorem Theorem (Trace-zero functions in $W^{1,p}$): Assume $U$ is bounded and $\partial U$ is $C^1$. Suppose furthermore that $u\in W^{1,p}$. Then $$u\in W^{1,p}_0(U)\quad \Longleftrightarrow\quad  Tu=0\text{ on }\partial U$$ The step I don't understand is the two equations between (8) and (9) p.274. For those who don't have access to the book, I'll rewrite with general notations: Let $\varphi \in C^1(\overline{\mathbb{R}}{}^n_+)$ where $\overline{\mathbb{R}}{}^n_+$ denotes the ""half-space"" with boundary, i.e. $\mathbf{x}:=(x', x_n)\in \overline{\mathbb{R}}{}^n_+\enspace \Leftrightarrow\enspace x' \in \mathbb{R}^{n-1},\ x_n \geq 0$. We have $$ \lvert \varphi(x', x_n) \rvert \leq \lvert \varphi(x', 0) \rvert + \int_0^{x_n}\lvert \varphi_{x_n}(x', t) \rvert\, dt $$ ($\varphi_{x_n}$ means partial derivative w.r.t. $x_n$). That's ok. Now it seems that he raises everything to the power $p$ and then integrates w.r.t. $x'$ to get $$ \int_{\mathbb{R}^{n-1}} \lvert \varphi(x', x_n) \rvert^p\, dx' \leq C \left( \int_{\mathbb{R}^{n-1}} \lvert \varphi(x', 0) \rvert^p\, dx' + x_n^{p-1}\int_0^{x_n}\int_{\mathbb{R}^{n-1}}  \lvert D\varphi(x', t) \rvert^p\, dx'\, dt \right)$$ where $\lvert D\varphi(x', t) \rvert$ is the norm of the gradient following the notations of the book. I naively imagined writing the binomial. Then I somehow need to find an upper bound and transform the product of integrals into an integral of a product (2nd term r.h.s.). Oh, writing things down already helped me see that L. Evans indeed already performed $p-1$ integrations $\int_0^{x_n} (\cdots )\, dt $, but I still don't know how the inequality is justified, i.e. what are the suitable constant $C$",,"['functional-analysis', 'multivariable-calculus', 'partial-differential-equations']"
95,Can I solve this integral without spherical coordinates?,Can I solve this integral without spherical coordinates?,,"Def. $$ f: \; [-r, r]^3 \rightarrow \mathbb{R}, \; (x_1, x_2,x_3) \mapsto \begin{cases} \sqrt{r^2 - x_1^2 - x_2^2 - x_3^2}, & \text{for } x_1^2 + x_2^2 + x_3^2 \leq r^2, \\ 0, & \text{else}. \end{cases} $$ with $r > 0$. I need to prove: $$ \int_{-r}^r \int_{-r}^r \int_{-r}^r f(x_1, x_2, x_3) \mathrm{d}x_1 \mathrm{d}x_2 \mathrm{d}x_3 = \frac{\pi^2}{4}r^4. $$ I was able to solve the integral by first transforming $(x_1, x_2, x_3)$ into spherical coordinates $(p, \theta, \varphi)$ but I am still at the point in my studies where I do not know about coordinate transformations of integrals. My question: Is there a way to calculate the integral without spherical coordinates or is there a simple proof such that I can do it in spherical coordinates? What I did: \begin{align} & \int_{-r}^r \int_{-r}^r \int_{-r}^r f(x_1, x_2, x_3) \mathrm{d}x_1 \mathrm{d}x_2 \mathrm{d}x_3 \\ =& \int_{0}^r \int_{0}^\pi \int_{-\pi}^\pi \sqrt{r^2-p^2} \cdot p^2\sin\theta\; \mathrm{d}\varphi \mathrm{d}\theta \mathrm{d}p \\ =& \dots \\ =& 4\pi \int_0^r \sqrt{r^2-p^2} \cdot p^2 \mathrm{d}p \\ =& \dots \\ =& 4\pi \left[ \frac{1}{8} \arcsin\frac{p}{r} - \frac{1}{8}\sin\left( 4\arcsin\frac{p}{r} \right) \right]_{p=0}^{p=r} \\ =& \frac{\pi^2}{4} r^4 \end{align}","Def. $$ f: \; [-r, r]^3 \rightarrow \mathbb{R}, \; (x_1, x_2,x_3) \mapsto \begin{cases} \sqrt{r^2 - x_1^2 - x_2^2 - x_3^2}, & \text{for } x_1^2 + x_2^2 + x_3^2 \leq r^2, \\ 0, & \text{else}. \end{cases} $$ with $r > 0$. I need to prove: $$ \int_{-r}^r \int_{-r}^r \int_{-r}^r f(x_1, x_2, x_3) \mathrm{d}x_1 \mathrm{d}x_2 \mathrm{d}x_3 = \frac{\pi^2}{4}r^4. $$ I was able to solve the integral by first transforming $(x_1, x_2, x_3)$ into spherical coordinates $(p, \theta, \varphi)$ but I am still at the point in my studies where I do not know about coordinate transformations of integrals. My question: Is there a way to calculate the integral without spherical coordinates or is there a simple proof such that I can do it in spherical coordinates? What I did: \begin{align} & \int_{-r}^r \int_{-r}^r \int_{-r}^r f(x_1, x_2, x_3) \mathrm{d}x_1 \mathrm{d}x_2 \mathrm{d}x_3 \\ =& \int_{0}^r \int_{0}^\pi \int_{-\pi}^\pi \sqrt{r^2-p^2} \cdot p^2\sin\theta\; \mathrm{d}\varphi \mathrm{d}\theta \mathrm{d}p \\ =& \dots \\ =& 4\pi \int_0^r \sqrt{r^2-p^2} \cdot p^2 \mathrm{d}p \\ =& \dots \\ =& 4\pi \left[ \frac{1}{8} \arcsin\frac{p}{r} - \frac{1}{8}\sin\left( 4\arcsin\frac{p}{r} \right) \right]_{p=0}^{p=r} \\ =& \frac{\pi^2}{4} r^4 \end{align}",,"['integration', 'multivariable-calculus', 'spherical-coordinates']"
96,Is there a non-affine harmonic map $\mathbb{R}^3 \to \mathbb{R}^3$ with constant Jacobian?,Is there a non-affine harmonic map  with constant Jacobian?,\mathbb{R}^3 \to \mathbb{R}^3,"Does there exist a smooth harmonic map $f:\mathbb{R}^3 \to \mathbb{R}^3$ such that: $\det(df)$ is constant and nonzero. $f$ is not affine. ($f$ is harmonic if each of its three components is a harmonic function). The case of maps $\mathbb{R}^2 \to \mathbb{R}^2$ has been settled here . (There is no such map). However, the proof there used techniques from complex analysis, which are not available in dimension $3$.","Does there exist a smooth harmonic map $f:\mathbb{R}^3 \to \mathbb{R}^3$ such that: $\det(df)$ is constant and nonzero. $f$ is not affine. ($f$ is harmonic if each of its three components is a harmonic function). The case of maps $\mathbb{R}^2 \to \mathbb{R}^2$ has been settled here . (There is no such map). However, the proof there used techniques from complex analysis, which are not available in dimension $3$.",,"['multivariable-calculus', 'partial-differential-equations', 'harmonic-functions', 'jacobian']"
97,Surface integral - spherical,Surface integral - spherical,,"I'm trying to calculate the following surface integral $$\int \int_{s_r} \frac{z-R}{(x^2+y^2+(z-R)^2)^{3/2}} dS $$, where $s_r=\{(x,y,z)\in \mathbb{R}^3 : x^2+y^2+z^2=r^2 \}. $ I've switched to spherical coordinates but don't really know how to do it.","I'm trying to calculate the following surface integral $$\int \int_{s_r} \frac{z-R}{(x^2+y^2+(z-R)^2)^{3/2}} dS $$, where $s_r=\{(x,y,z)\in \mathbb{R}^3 : x^2+y^2+z^2=r^2 \}. $ I've switched to spherical coordinates but don't really know how to do it.",,"['integration', 'multivariable-calculus', 'surface-integrals']"
98,"Given $f(x,y,z)=xy^2z$ find the the max value such that the point $(x,y,z)$ is located in the part of the plane $x+y+z=4$",Given  find the the max value such that the point  is located in the part of the plane,"f(x,y,z)=xy^2z (x,y,z) x+y+z=4","Given $f(x,y,z)=xy^2z$ find the the max value such that the point $(x,y,z)$ is located in the part of the plane $x+y+z=4$ which is in the first octant ($x>0, y>0, z>0$) of the coordinates. I think Lagrange multipliers can be used here. We can say that $x+y+z=4$ is the restriction. Let $g=x+y+z-4$. Then: $$ \nabla f= \lambda\nabla g\\ \nabla f=\langle y^2z,2yxz,xy^2 \rangle\\ \nabla g=\langle 1,1,1 \rangle $$ It follows that: $$ \begin{cases} y^2z=\lambda\\ 2yxz=\lambda\\ xy^2=\lambda \end{cases} $$ Because $x>0, y>0, z>0$ we can solve the system and get: $$ z=x,\quad y=2x $$ Finally, we can rewrite the restriction as: $$ x+2x+x=4 \Rightarrow x=1, y=2, z=1 $$ Is my usage of Lagrange method correct? Can this problem also be solved without Lagrange method by finding the max value using min/max theorems for functions with 2 variables?","Given $f(x,y,z)=xy^2z$ find the the max value such that the point $(x,y,z)$ is located in the part of the plane $x+y+z=4$ which is in the first octant ($x>0, y>0, z>0$) of the coordinates. I think Lagrange multipliers can be used here. We can say that $x+y+z=4$ is the restriction. Let $g=x+y+z-4$. Then: $$ \nabla f= \lambda\nabla g\\ \nabla f=\langle y^2z,2yxz,xy^2 \rangle\\ \nabla g=\langle 1,1,1 \rangle $$ It follows that: $$ \begin{cases} y^2z=\lambda\\ 2yxz=\lambda\\ xy^2=\lambda \end{cases} $$ Because $x>0, y>0, z>0$ we can solve the system and get: $$ z=x,\quad y=2x $$ Finally, we can rewrite the restriction as: $$ x+2x+x=4 \Rightarrow x=1, y=2, z=1 $$ Is my usage of Lagrange method correct? Can this problem also be solved without Lagrange method by finding the max value using min/max theorems for functions with 2 variables?",,"['multivariable-calculus', 'optimization', 'lagrange-multiplier', 'maxima-minima', 'gradient-descent']"
99,How does the gradient/jacobian relate to the first derivative of one dimensional functions?,How does the gradient/jacobian relate to the first derivative of one dimensional functions?,,"My question is about this answer by Simon S ( Is the gradient vector of a function the derivative of the function ) The derivative of a function $f : U \subset \mathbb{R}^m \to \mathbb{R}$ at a point $p \in U$ is a linear map $Df_p : \mathbb{R}^m \to \mathbb{R}$. We can identify the gradient of $f$ with $D_p$, if   everything exists, by $$Df_p(v) = \langle \nabla f(p), v\rangle$$ Equivalently, $Df_p$ is a co-tangent vector, i.e., a member of the set   of linear maps acting on $T_p$, the vector space of tangent vectors at   $p$: i.e., $Df_p = \langle \nabla f(p), \cdot \rangle : T_p \to \mathbb{R}$. This formalism is helpful when we want to generalize from   $\mathbb{R}^n$ to manifolds. In an abstract sense, I think I understand the differential being a linear map. But in $\mathbb{R}$ we just have one derivative $f'(x)$ where we plug in a value for x and get a scalar that tells us the derivative at that point. But in higher dimensions, we establish a gradient or a Jacobian at a point  and then we use the dot product to calculate the derivative as stated above. Of what importance is it at which point we create the gradient and of what importance which vector we multiply with the gradient? Is $\nabla f(3,3)$ the derivative of the function $f:\mathbb{R^2}\rightarrow \mathbb{R}$? Or is it $\langle\nabla f(x,y),(3,3)\rangle$ Or is it  $\langle\nabla f(3,3),(3,3)\rangle$ ? How can one understand this as just the higher dimensional derivative comparable to a single dimensional $f'(3)$? Why do we need that dot product with an additional vector? Where is the connection between the derivative being a linear map and the derivative just being the sum of the partial derivatives? EDIT: If it isn't clear, what I'm implying consider sites like these http://www.solitaryroad.com/c353.html or https://en.wikipedia.org/wiki/Total_derivative which essentially state The total differential is the sum of the partial differentials. But I can not fathom how that is equivalent to the linear map definition above.","My question is about this answer by Simon S ( Is the gradient vector of a function the derivative of the function ) The derivative of a function $f : U \subset \mathbb{R}^m \to \mathbb{R}$ at a point $p \in U$ is a linear map $Df_p : \mathbb{R}^m \to \mathbb{R}$. We can identify the gradient of $f$ with $D_p$, if   everything exists, by $$Df_p(v) = \langle \nabla f(p), v\rangle$$ Equivalently, $Df_p$ is a co-tangent vector, i.e., a member of the set   of linear maps acting on $T_p$, the vector space of tangent vectors at   $p$: i.e., $Df_p = \langle \nabla f(p), \cdot \rangle : T_p \to \mathbb{R}$. This formalism is helpful when we want to generalize from   $\mathbb{R}^n$ to manifolds. In an abstract sense, I think I understand the differential being a linear map. But in $\mathbb{R}$ we just have one derivative $f'(x)$ where we plug in a value for x and get a scalar that tells us the derivative at that point. But in higher dimensions, we establish a gradient or a Jacobian at a point  and then we use the dot product to calculate the derivative as stated above. Of what importance is it at which point we create the gradient and of what importance which vector we multiply with the gradient? Is $\nabla f(3,3)$ the derivative of the function $f:\mathbb{R^2}\rightarrow \mathbb{R}$? Or is it $\langle\nabla f(x,y),(3,3)\rangle$ Or is it  $\langle\nabla f(3,3),(3,3)\rangle$ ? How can one understand this as just the higher dimensional derivative comparable to a single dimensional $f'(3)$? Why do we need that dot product with an additional vector? Where is the connection between the derivative being a linear map and the derivative just being the sum of the partial derivatives? EDIT: If it isn't clear, what I'm implying consider sites like these http://www.solitaryroad.com/c353.html or https://en.wikipedia.org/wiki/Total_derivative which essentially state The total differential is the sum of the partial differentials. But I can not fathom how that is equivalent to the linear map definition above.",,"['calculus', 'multivariable-calculus']"
