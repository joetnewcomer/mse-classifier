,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Proof of a limit of a function. Is it correct?,Proof of a limit of a function. Is it correct?,,"Edit: There is an answer at the bottom by me explaining what is going on in this post. Define a function $f : R \to R$ by $f(x) = 1$ if $x = 0$ and $f(x) = 0$ if $x \ne 0$ . I was attempting to prove that $\lim_{x \to 0; x\in R}f(x)$ is undefined. The following is my proof. Proof: Suppose that $\lim_{x\to 0; x \in R}f(x)=L$ . Then for every $\varepsilon > 0$ there exists a $\delta > 0$ such that for all those $x \in R$ for which $|x-0|<\delta$ we have that $|f(x)-L|<\varepsilon$ . But $|0| < \delta$ and by the Archimedean property we know that for $\delta > 0$ there exists an integer $n>0$ such that $0<|\frac 1 n| < \delta$ . This is a contradiction, as $f(0) = 1$ and $f(\frac 1 n)= 0$ and both are less than $\delta$ . Is the proof correct? Edit: Here a limit is defined using adherent points and not limit points. If we were to use limit points then, $\lim_{x \to 0; x\in R\setminus \{0\}}f(x)=0$ . I have updated the question with correct notation. Edit 2: Most textbooks define limits using limit points. In which case you we would have that $\lim_{x \to 0}f(x)=\lim_{x \to 0; x\in R\setminus \{0\}}f(x)$ . We are considering the definition of the limit where limits are defined using adherent points. Where it really matters whether we are considering $lim_{x\to 0; x\in R\setminus \{0\}}f(x)$ or $lim_{x\to 0; x\in R}f(x)$ . Edit 3: This is the definition of convergence of a function at a point in the book Analysis 1 by Terence Tao. Let $X$ be a subset of $R$ , let $f : X → R$ be a function, let $E$ be a subset of $X$ , $x_0$ be an adherent point of $E$ , and let L be a real number. We say that f converges to $L$ at $x0$ in $E$ , and write $\lim_{x \to x_0;x\in E} f(x) = L$ , iff $f$ , after restricting to $E$ , is ε-close to $L$ near $x_0$ for every $\varepsilon > 0$ . If $f$ does not converge to any number $L$ at $x_0$ , we say that $f$ diverges at $x0$ , and leave $\lim_{x\to x_0;x\in E} f(x)$ undefined. In other words, we have $lim_{x\to x_0;x\in E} f(x) = L$ iff for every $\varepsilon > 0$ , there exists a $\delta > 0$ such that $|f(x) − L| ≤ ε$ for all $x \in E$ such that $|x − x0| < \delta$ . There are a two other things he defines that are used in this definition. That of $\varepsilon$ closeness and local $\varepsilon$ closeness. For those wanting to read those, Here is the link . It is on page 221. Edit 4: It might be useless defining limits without limit points. But that is the definition for which I am trying to prove this.","Edit: There is an answer at the bottom by me explaining what is going on in this post. Define a function by if and if . I was attempting to prove that is undefined. The following is my proof. Proof: Suppose that . Then for every there exists a such that for all those for which we have that . But and by the Archimedean property we know that for there exists an integer such that . This is a contradiction, as and and both are less than . Is the proof correct? Edit: Here a limit is defined using adherent points and not limit points. If we were to use limit points then, . I have updated the question with correct notation. Edit 2: Most textbooks define limits using limit points. In which case you we would have that . We are considering the definition of the limit where limits are defined using adherent points. Where it really matters whether we are considering or . Edit 3: This is the definition of convergence of a function at a point in the book Analysis 1 by Terence Tao. Let be a subset of , let be a function, let be a subset of , be an adherent point of , and let L be a real number. We say that f converges to at in , and write , iff , after restricting to , is ε-close to near for every . If does not converge to any number at , we say that diverges at , and leave undefined. In other words, we have iff for every , there exists a such that for all such that . There are a two other things he defines that are used in this definition. That of closeness and local closeness. For those wanting to read those, Here is the link . It is on page 221. Edit 4: It might be useless defining limits without limit points. But that is the definition for which I am trying to prove this.",f : R \to R f(x) = 1 x = 0 f(x) = 0 x \ne 0 \lim_{x \to 0; x\in R}f(x) \lim_{x\to 0; x \in R}f(x)=L \varepsilon > 0 \delta > 0 x \in R |x-0|<\delta |f(x)-L|<\varepsilon |0| < \delta \delta > 0 n>0 0<|\frac 1 n| < \delta f(0) = 1 f(\frac 1 n)= 0 \delta \lim_{x \to 0; x\in R\setminus \{0\}}f(x)=0 \lim_{x \to 0}f(x)=\lim_{x \to 0; x\in R\setminus \{0\}}f(x) lim_{x\to 0; x\in R\setminus \{0\}}f(x) lim_{x\to 0; x\in R}f(x) X R f : X → R E X x_0 E L x0 E \lim_{x \to x_0;x\in E} f(x) = L f E L x_0 \varepsilon > 0 f L x_0 f x0 \lim_{x\to x_0;x\in E} f(x) lim_{x\to x_0;x\in E} f(x) = L \varepsilon > 0 \delta > 0 |f(x) − L| ≤ ε x \in E |x − x0| < \delta \varepsilon \varepsilon,"['real-analysis', 'limits', 'analysis', 'solution-verification']"
1,How do you find this limit with a relationship to $e$ using Taylor series?,How do you find this limit with a relationship to  using Taylor series?,e,"The limit in question is $$ \lim_{x \to 0}\left(\frac{\sin(x)}{x}\right)^{1/x^2} $$ When I replace $\sin(x)$ with its Taylor series about $0$ and cancel out the $x$ , I get $$ \lim_{x \to 0}\left(1-\frac{x^2}{6} + \frac{x^4}{5!} \mp \cdots\right)^{1/x^2} $$ The answer in the book is $e^\frac{-1}{6}$ . If I only look at the $x^2$ term, I can see where $-1/6$ comes from. I'm just not sure how I can definitively say the answer is $e^\frac{-1}{6}$ . Why can I discount the other powers of $x$ ?","The limit in question is When I replace with its Taylor series about and cancel out the , I get The answer in the book is . If I only look at the term, I can see where comes from. I'm just not sure how I can definitively say the answer is . Why can I discount the other powers of ?","
\lim_{x \to 0}\left(\frac{\sin(x)}{x}\right)^{1/x^2}
 \sin(x) 0 x 
\lim_{x \to 0}\left(1-\frac{x^2}{6} + \frac{x^4}{5!} \mp \cdots\right)^{1/x^2}
 e^\frac{-1}{6} x^2 -1/6 e^\frac{-1}{6} x","['calculus', 'limits', 'taylor-expansion', 'power-series', 'limits-without-lhopital']"
2,"provided $f( x+1) =\ \lim _{n\rightarrow \infty }\left(\frac{n+x}{n-2}\right)^{n}$, what is f(x)?","provided , what is f(x)?",f( x+1) =\ \lim _{n\rightarrow \infty }\left(\frac{n+x}{n-2}\right)^{n},$$ f( x+1) =\ \lim _{n\rightarrow \infty }\left(\frac{n+x}{n-2}\right)^{n} $$ Here is one of the solution from my workbook: $$  \begin{array}{l} f( x+1) \ =\ \lim _{n\rightarrow \infty }\left[\left( 1+\frac{2+x}{n-2}\right)^{\frac{n-2}{2+x}}\right]^{n\left(\frac{2+x}{n-2}\right)} =\ e^{2+x} =e^{1+( x+1)}\\ f( x) \ =\ e^{1+x} \end{array} $$ but is a bit preplexing for me :(,Here is one of the solution from my workbook: but is a bit preplexing for me :(,"
f( x+1) =\ \lim _{n\rightarrow \infty }\left(\frac{n+x}{n-2}\right)^{n}
 
 \begin{array}{l}
f( x+1) \ =\ \lim _{n\rightarrow \infty }\left[\left( 1+\frac{2+x}{n-2}\right)^{\frac{n-2}{2+x}}\right]^{n\left(\frac{2+x}{n-2}\right)} =\ e^{2+x} =e^{1+( x+1)}\\
f( x) \ =\ e^{1+x}
\end{array}
","['real-analysis', 'limits', 'proof-explanation']"
3,Limit of sum/difference condition which are not being followed here,Limit of sum/difference condition which are not being followed here,,"This was given as an example in a book : If $$\lim _{x \rightarrow a}[f(x)+g(x)]=2$$ and $$\lim _{x \rightarrow a}[f(x)-g(x)]=1,$$ then find the value of $$\lim _{x \rightarrow a} f(x) g(x)$$ Solution : $$\lim _{x \rightarrow a}[f(x)+g(x)]=2$$ or $$\lim _{x \rightarrow a} f(x)+\lim _{x \rightarrow a} g(x)=2$$ $\tag{1}$ $$\lim _{x \rightarrow a}[f(x)-g(x)]=1$$ or $$\lim _{x \rightarrow a} f(x)-\lim _{x \rightarrow a} g(x)=1$$ $\tag{2}$ Adding (1) and (2), $$2 \lim _{x \rightarrow a} f(x)=3$$ or $$\lim _{x \rightarrow a} f(x)=\frac{3}{2}$$ Subtracting (2) from (1), $$2 \lim _{x \rightarrow a} g(x) \neq 1$$ or $$\lim _{x \rightarrow a} g(x)=\frac{1}{2}$$ or $$\lim _{x \rightarrow a} f(x) g(x)=\lim _{x \rightarrow a} f(x) \lim _{x \rightarrow a} g(x)=\frac{3}{2} \times \frac{1}{2}=\frac{3}{4}.$$ My query is that isnt the lim of sum = individual limits sum only when we  already know that limit of individual sum  exists ? Here we actually dont know if it actually exists or not so shouldnt this method is totally wrong ?","This was given as an example in a book : If and then find the value of Solution : or or Adding (1) and (2), or Subtracting (2) from (1), or or My query is that isnt the lim of sum = individual limits sum only when we  already know that limit of individual sum  exists ? Here we actually dont know if it actually exists or not so shouldnt this method is totally wrong ?","\lim _{x \rightarrow a}[f(x)+g(x)]=2 \lim _{x \rightarrow a}[f(x)-g(x)]=1, \lim _{x \rightarrow a} f(x) g(x) \lim _{x \rightarrow a}[f(x)+g(x)]=2 \lim _{x \rightarrow a} f(x)+\lim _{x \rightarrow a} g(x)=2 \tag{1} \lim _{x \rightarrow a}[f(x)-g(x)]=1 \lim _{x \rightarrow a} f(x)-\lim _{x \rightarrow a} g(x)=1 \tag{2} 2 \lim _{x \rightarrow a} f(x)=3 \lim _{x \rightarrow a} f(x)=\frac{3}{2} 2 \lim _{x \rightarrow a} g(x) \neq 1 \lim _{x \rightarrow a} g(x)=\frac{1}{2} \lim _{x \rightarrow a} f(x) g(x)=\lim _{x \rightarrow a} f(x) \lim _{x \rightarrow a} g(x)=\frac{3}{2} \times \frac{1}{2}=\frac{3}{4}.","['calculus', 'limits']"
4,"Finding the limit $\displaystyle\lim_{(x,y) \to (0,0)}\frac{\log(x^2 y^2)}{\log(x+y)}$",Finding the limit,"\displaystyle\lim_{(x,y) \to (0,0)}\frac{\log(x^2 y^2)}{\log(x+y)}","Let $\Omega := \left\{ (x,y) \in \mathbb{R}^2 \mid x+y>0, x,y \neq 0 \right\}$ and let function $f: \Omega \to \mathbb{R}$ be defined by $$f(x,y) := \frac{\log\left(x^2 y^2\right)}{\log(x+y)}$$ Find the limit for $(x,y) \to (0,0)$ . I need some help finding this limit. I tried to substitute $y=mx$ , and the candidate is 4 (probably wrong.). As I understand it, I should create a chain of inequalities, arriving at something like $$ \left| f(x,y) - l \right| \leq \left|h(x,y)\right| $$ where $h(x,y)$ is a function that goes to zero. However, I have no clue how to do this here. EDIT: obviously, the $l$ above is the candidate limit.","Let and let function be defined by Find the limit for . I need some help finding this limit. I tried to substitute , and the candidate is 4 (probably wrong.). As I understand it, I should create a chain of inequalities, arriving at something like where is a function that goes to zero. However, I have no clue how to do this here. EDIT: obviously, the above is the candidate limit.","\Omega := \left\{ (x,y) \in \mathbb{R}^2 \mid x+y>0, x,y \neq 0 \right\} f: \Omega \to \mathbb{R} f(x,y) := \frac{\log\left(x^2 y^2\right)}{\log(x+y)} (x,y) \to (0,0) y=mx  \left| f(x,y) - l \right| \leq \left|h(x,y)\right|  h(x,y) l","['limits', 'multivariable-calculus']"
5,Is there any intuitive way to check whether a function is continuous at a given point?,Is there any intuitive way to check whether a function is continuous at a given point?,,"In my exams, the questions on continuity of multivariable functions are framed like ""Discuss the continuity of $f(x,y)$ at $(a,b)$ ..."" or ""Is $f$ continuous at $(a,b)$ ...?"", and likewise. If I know beforehand that the given function is discontinuous at a given point $(a,b)$ , then I just need to find out two paths where the the value of $\lim_{(x,y)\to (a,b)} f(x,y)$ are different or not equal to $f(a,b)$ . On the other hand, if I know that the function is continuous at the given point, then I can use the $\varepsilon-\delta$ definition of continuity to prove continuity. But since the questions don't seem to be giving much away about the continuity of the function at the given point, I'm not sure which approach should I take first while trying to solve the question. Is there any quick and intuitive way to figure out whether a given multivariable function is continuous or not? At least in the cases where the given function is of the form $\frac{p(x,y)}{q(x,y)}$ , where $p$ and $q$ are polynomials in $x$ and $y$ ? (Exceptions are fine. Just a generic and practically useful trick would do.) For example, the function $$f(x,y) =\begin{cases} \frac{x^{4}-y^{4}}{x^{4}+y^{4}} & (x,y)\neq (0,0) \\  0 & (x,y)=(0,0) . \end{cases}$$ is discontinuous at $(0,0)$ , while the function $$g(x,y) =\begin{cases} \frac{x^{2}y^{2}}{x^{2}+y^{2}} & (x,y)\neq (0,0) \\  0 & (x,y)=(0,0) . \end{cases}$$ is continuous at $(0,0)$ . Is there any easy way to pick this just by looking at the functions? The same issue exists with finding a limit and proving the existence of the limit.","In my exams, the questions on continuity of multivariable functions are framed like ""Discuss the continuity of at ..."" or ""Is continuous at ...?"", and likewise. If I know beforehand that the given function is discontinuous at a given point , then I just need to find out two paths where the the value of are different or not equal to . On the other hand, if I know that the function is continuous at the given point, then I can use the definition of continuity to prove continuity. But since the questions don't seem to be giving much away about the continuity of the function at the given point, I'm not sure which approach should I take first while trying to solve the question. Is there any quick and intuitive way to figure out whether a given multivariable function is continuous or not? At least in the cases where the given function is of the form , where and are polynomials in and ? (Exceptions are fine. Just a generic and practically useful trick would do.) For example, the function is discontinuous at , while the function is continuous at . Is there any easy way to pick this just by looking at the functions? The same issue exists with finding a limit and proving the existence of the limit.","f(x,y) (a,b) f (a,b) (a,b) \lim_{(x,y)\to (a,b)} f(x,y) f(a,b) \varepsilon-\delta \frac{p(x,y)}{q(x,y)} p q x y f(x,y) =\begin{cases} \frac{x^{4}-y^{4}}{x^{4}+y^{4}} & (x,y)\neq (0,0) \\  0 & (x,y)=(0,0) . \end{cases} (0,0) g(x,y) =\begin{cases} \frac{x^{2}y^{2}}{x^{2}+y^{2}} & (x,y)\neq (0,0) \\  0 & (x,y)=(0,0) . \end{cases} (0,0)","['limits', 'multivariable-calculus', 'continuity', 'epsilon-delta']"
6,How would one describe $k$ iterations of $\cos(n)$?,How would one describe  iterations of ?,k \cos(n),"What function would one use to describe $k$ iterations of $\cos(n)$ ? I'm pretty sure that the function would be a damped sine wave (as can be seen in the curve fit equation I wrote in the third row), however the actual formula is probably quite complicated as it involves the Dottie number , which, to my knowledge, cannot be expressed in terms of $e$ , $\pi$ , or polynomial roots. Below is an attempt of curve-fitting on $n=1$ . Some of the deficiencies of this fit I've noticed while experimenting with Desmos are that the lines are far too steep (even without the scale factor or with a smaller scale factor), and the fit seems to be weaker for even $n$ (although I presume that this is simply an artifact of approximation). Note that the y-axis has been scaled by a factor of 5 for the sake of graph readability.","What function would one use to describe iterations of ? I'm pretty sure that the function would be a damped sine wave (as can be seen in the curve fit equation I wrote in the third row), however the actual formula is probably quite complicated as it involves the Dottie number , which, to my knowledge, cannot be expressed in terms of , , or polynomial roots. Below is an attempt of curve-fitting on . Some of the deficiencies of this fit I've noticed while experimenting with Desmos are that the lines are far too steep (even without the scale factor or with a smaller scale factor), and the fit seems to be weaker for even (although I presume that this is simply an artifact of approximation). Note that the y-axis has been scaled by a factor of 5 for the sake of graph readability.",k \cos(n) e \pi n=1 n,"['limits', 'trigonometry', 'fixed-point-theorems', 'wave-equation', 'desmos']"
7,"if $\lim_{k\to \infty} a_k=0$, with $a_k \geq 0$, then $\lim_{n\to \infty} \frac{1}{n}\sum_{k=0}^n a_k =0$?","if , with , then ?",\lim_{k\to \infty} a_k=0 a_k \geq 0 \lim_{n\to \infty} \frac{1}{n}\sum_{k=0}^n a_k =0,"I need help to prove the question or find a counterexample. I think that was true. Proof idea: Since $\lim_{k\to \infty} a_k=0$ , given $\epsilon >0$ there is $k_0\in\mathbb{N}$ such that $a_k < \epsilon$ for all $k > k_0$ . Then for $n>k_0$ , $$ \frac{1}{n}\sum_{k=0}^n a_k =\frac{1}{n}\sum_{k=0}^{k_0} a_k+\frac{1}{n}\sum_{k=k_0+1}^n a_k <\frac{1}{n}\sum_{k=0}^{k_0-1} a_k+\frac{n-k_0}{n} \epsilon $$ so $$  \lim_{n\to \infty} \frac{1}{n}\sum_{k=0}^n a_k <\epsilon, $$ and hence the proof is completed.","I need help to prove the question or find a counterexample. I think that was true. Proof idea: Since , given there is such that for all . Then for , so and hence the proof is completed.","\lim_{k\to \infty} a_k=0 \epsilon >0 k_0\in\mathbb{N} a_k < \epsilon k > k_0 n>k_0 
\frac{1}{n}\sum_{k=0}^n a_k =\frac{1}{n}\sum_{k=0}^{k_0} a_k+\frac{1}{n}\sum_{k=k_0+1}^n a_k <\frac{1}{n}\sum_{k=0}^{k_0-1} a_k+\frac{n-k_0}{n} \epsilon
 
 \lim_{n\to \infty} \frac{1}{n}\sum_{k=0}^n a_k <\epsilon,
","['sequences-and-series', 'limits', 'convergence-divergence']"
8,Calculating limit using squeeze theorem,Calculating limit using squeeze theorem,,"$\lim\limits_{n\to\infty }\sqrt {n^2+(-1)^n}-n$ First the domain , $n \geq 1$ since $n^2+(-1)^n \geq n^2-1 \geq 0$ let $a_n =\sqrt {n^2+(-1)^n}-n $ , so I wanted to simplify it by multiplying by the conjugate so $\sqrt {n^2+(-1)^n}+n \geq 0$ and I got ( $\sqrt {n^2+(-1)^n}-n) \cdot \frac{\sqrt {n^2+(-1)^n}+n}{\sqrt {n^2+(-1)^n}+n}$ = $\frac{(-1)^n}{\sqrt{n^2+(-1)^n}+n}$ here is where I was not sure of how to use the squeeze theorem and this is what I did $\frac{-1}{\sqrt{n^2-1}+n}\leq \frac{(-1)^n}{\sqrt{n^2+(-1)^n}+n} \leq \frac{1}{\sqrt{n^2+1}+n} $ let $b_n = \frac{-1}{\sqrt{n^2-1}+n}$ and $c_n = \frac{1}{\sqrt{n^2+1}+n}$ so $\lim\limits_{n\to\infty }b_n = 0$ and $\lim\limits_{n\to\infty }c_n = 0$ and according to squeeze theorem $\lim\limits_{n\to\infty } a_n =0$ but in the book it was solved this way : after multiplying by the conjugate they also got $\frac{(-1)^n}{\sqrt{n^2+(-1)^n}+n}$ then they did $0 \leq|\frac{(-1)^n}{\sqrt{n^2+(-1)^n}+n}| = \frac{|(-1)^n|}{|\sqrt{n^2+(-1)^n}+n|} = \frac{1}{\sqrt{n^2+(-1)^n}+n} \leq \frac{1}{n}$ and then they evaluated the limit $\lim\limits_{n\to\infty }0=0$ and $\lim\limits_{n\to\infty } \frac{1}{n} = 0$ so according to squeeze theorem $\lim\limits_{n\to\infty } a_n =0$ What I did not understand in the book is why out of no where they used an absolute value? when am I allowed to do that? and is my try also correct? or do I need to explain more stuff because the inequality is not necessarily correct because for example in $C_n$ the denominator actually increases so the value decreases (although I think that in this case it doesn't matter because the value is positive while $b_n$ is negative ) but I mean in general ? Thank you EDIT: I tried solving it in another way , as a multiplication of a limit equal to zero and a bounded limit. If I can separate this limit into 2 limits $\frac{(-1)^n}{\sqrt{n^2+(-1)^n}+n}$ let $b_n=(-1)^n$ and $c_n =\frac{1}{\sqrt{n^2+(-1)^n}+n} $ $b_n$ is bounded of course $|b_n|=1 \lt 2 $ and $c_n =\frac{1}{\sqrt{n^2+(-1)^n}+n} \lt \frac{1}{n}$ because as stated in the beginning the domain , $n \geq 1$ since $n^2+(-1)^n \geq n^2-1 \geq 0$ from here we get $\sqrt{n^2-1} +n \geq n$ therefore $\lim\limits_{n\to\infty }b_n \cdot c_n=0$","First the domain , since let , so I wanted to simplify it by multiplying by the conjugate so and I got ( = here is where I was not sure of how to use the squeeze theorem and this is what I did let and so and and according to squeeze theorem but in the book it was solved this way : after multiplying by the conjugate they also got then they did and then they evaluated the limit and so according to squeeze theorem What I did not understand in the book is why out of no where they used an absolute value? when am I allowed to do that? and is my try also correct? or do I need to explain more stuff because the inequality is not necessarily correct because for example in the denominator actually increases so the value decreases (although I think that in this case it doesn't matter because the value is positive while is negative ) but I mean in general ? Thank you EDIT: I tried solving it in another way , as a multiplication of a limit equal to zero and a bounded limit. If I can separate this limit into 2 limits let and is bounded of course and because as stated in the beginning the domain , since from here we get therefore",\lim\limits_{n\to\infty }\sqrt {n^2+(-1)^n}-n n \geq 1 n^2+(-1)^n \geq n^2-1 \geq 0 a_n =\sqrt {n^2+(-1)^n}-n  \sqrt {n^2+(-1)^n}+n \geq 0 \sqrt {n^2+(-1)^n}-n) \cdot \frac{\sqrt {n^2+(-1)^n}+n}{\sqrt {n^2+(-1)^n}+n} \frac{(-1)^n}{\sqrt{n^2+(-1)^n}+n} \frac{-1}{\sqrt{n^2-1}+n}\leq \frac{(-1)^n}{\sqrt{n^2+(-1)^n}+n} \leq \frac{1}{\sqrt{n^2+1}+n}  b_n = \frac{-1}{\sqrt{n^2-1}+n} c_n = \frac{1}{\sqrt{n^2+1}+n} \lim\limits_{n\to\infty }b_n = 0 \lim\limits_{n\to\infty }c_n = 0 \lim\limits_{n\to\infty } a_n =0 \frac{(-1)^n}{\sqrt{n^2+(-1)^n}+n} 0 \leq|\frac{(-1)^n}{\sqrt{n^2+(-1)^n}+n}| = \frac{|(-1)^n|}{|\sqrt{n^2+(-1)^n}+n|} = \frac{1}{\sqrt{n^2+(-1)^n}+n} \leq \frac{1}{n} \lim\limits_{n\to\infty }0=0 \lim\limits_{n\to\infty } \frac{1}{n} = 0 \lim\limits_{n\to\infty } a_n =0 C_n b_n \frac{(-1)^n}{\sqrt{n^2+(-1)^n}+n} b_n=(-1)^n c_n =\frac{1}{\sqrt{n^2+(-1)^n}+n}  b_n |b_n|=1 \lt 2  c_n =\frac{1}{\sqrt{n^2+(-1)^n}+n} \lt \frac{1}{n} n \geq 1 n^2+(-1)^n \geq n^2-1 \geq 0 \sqrt{n^2-1} +n \geq n \lim\limits_{n\to\infty }b_n \cdot c_n=0,"['calculus', 'limits']"
9,Calculate the limit of a quotient,Calculate the limit of a quotient,,I substituted big numbers in calculator as well as graph it. The limit is 0 instead of 1. I don't know which step goes wrong.. Question: $$\lim_{n\to+\infty} \frac{2^n \cdot (\sqrt{4^n+2n} - 2^n)}{n+1}$$ My solution: $$=\lim_{n\to+\infty}\frac{2^n\cdot(\sqrt{4^n+2n}-2^n)\cdot(\sqrt{4^n+2n}+2^n)}{(n+1)\cdot(\sqrt{4^n+2n}+2^n)}$$ $$=\lim_{n\to+\infty}\frac{2^n\cdot(4^n+2n-4^n)\div 2^n}{(n+1)\cdot(\sqrt{4^n+2n}+2^n)\div 2^n}$$ $$=\lim_{n\to+\infty}\frac{2n}{(n+1)\cdot(\sqrt{1+\frac{2n}{4^n}}+1)}$$ $$=\lim_{n\to+\infty}\frac{2}{(1+\frac{1}{n})\cdot(\sqrt{1+{2n}\cdot{(\frac{1}{4})^n}}+1)}$$ $$=\frac{2}{(1+0)\cdot(1+1)}$$ $$=1$$,I substituted big numbers in calculator as well as graph it. The limit is 0 instead of 1. I don't know which step goes wrong.. Question: My solution:,\lim_{n\to+\infty} \frac{2^n \cdot (\sqrt{4^n+2n} - 2^n)}{n+1} =\lim_{n\to+\infty}\frac{2^n\cdot(\sqrt{4^n+2n}-2^n)\cdot(\sqrt{4^n+2n}+2^n)}{(n+1)\cdot(\sqrt{4^n+2n}+2^n)} =\lim_{n\to+\infty}\frac{2^n\cdot(4^n+2n-4^n)\div 2^n}{(n+1)\cdot(\sqrt{4^n+2n}+2^n)\div 2^n} =\lim_{n\to+\infty}\frac{2n}{(n+1)\cdot(\sqrt{1+\frac{2n}{4^n}}+1)} =\lim_{n\to+\infty}\frac{2}{(1+\frac{1}{n})\cdot(\sqrt{1+{2n}\cdot{(\frac{1}{4})^n}}+1)} =\frac{2}{(1+0)\cdot(1+1)} =1,"['limits', 'analysis']"
10,What is the value of $\textstyle{{\sum\limits_{n=1}^{\infty}{\lim\limits_{u\rightarrow\infty}{\frac{(-1)^nu^{n+s}}{(n-1)!(n+s)}}}}}$?,What is the value of ?,\textstyle{{\sum\limits_{n=1}^{\infty}{\lim\limits_{u\rightarrow\infty}{\frac{(-1)^nu^{n+s}}{(n-1)!(n+s)}}}}},"First, I had put $\textstyle{\displaystyle{\sum_{n=1}^{\infty}{\lim_{u\rightarrow\infty}{\frac{(-1)^nu^{n+s}}{(n-1)!(n+s)}}}}}$ into wolfram alpha and got nothing. Then, I thought about inter-changing the limit and the summation which will give us $\textstyle{\displaystyle{\lim_{u\rightarrow\infty}{\sum_{n=1}^{\infty}\frac{(-1)^nu^{n+s}}{(n-1)!(n+s)}}}}$ which did gave me a result again from wolfram alpha. Putting the sum in wolfram alpha gave me $\begin{align}&-\Gamma(s+1,0,u)\\ &=-\Gamma(s+1,0)+\Gamma(s+1,u)\\ &=\Gamma(s+1,u)-\Gamma(s+1)\end{align}$ Then taking the limit gives us $\textstyle{\displaystyle{\lim_{u\rightarrow\infty}(\Gamma(s+1,u)-\Gamma(s+1))}}$ $=-\Gamma(s+1)$ However, I am not sure that I can really just interchange the limit and summation. While searching about this I got this post on this site, which says $\begin{align}\textstyle\displaystyle{\lim_{n\rightarrow\infty}\sum_{m=1}^{\infty}f(m,n)\geq\sum_{m=1}^{\infty}\lim_{n\rightarrow\infty}f(m,n)}\end{align}$ Applying this inequality on my sum gives us $\textstyle\displaystyle{\sum_{n=1}^{\infty}\lim_{u\rightarrow\infty}{\frac{(-1)^nu^{n+s}}{(n-1)!(n+s)}}\leq -\Gamma(s+1)}$ However, this doesn't really give the value of the sum. So, my question is What is the value of $\textstyle{\displaystyle{\sum_{n=1}^{\infty}{\lim_{u\rightarrow\infty}{\frac{(-1)^nu^{n+s}}{(n-1)!(n+s)}}}}}$ ?","First, I had put into wolfram alpha and got nothing. Then, I thought about inter-changing the limit and the summation which will give us which did gave me a result again from wolfram alpha. Putting the sum in wolfram alpha gave me Then taking the limit gives us However, I am not sure that I can really just interchange the limit and summation. While searching about this I got this post on this site, which says Applying this inequality on my sum gives us However, this doesn't really give the value of the sum. So, my question is What is the value of ?","\textstyle{\displaystyle{\sum_{n=1}^{\infty}{\lim_{u\rightarrow\infty}{\frac{(-1)^nu^{n+s}}{(n-1)!(n+s)}}}}} \textstyle{\displaystyle{\lim_{u\rightarrow\infty}{\sum_{n=1}^{\infty}\frac{(-1)^nu^{n+s}}{(n-1)!(n+s)}}}} \begin{align}&-\Gamma(s+1,0,u)\\
&=-\Gamma(s+1,0)+\Gamma(s+1,u)\\
&=\Gamma(s+1,u)-\Gamma(s+1)\end{align} \textstyle{\displaystyle{\lim_{u\rightarrow\infty}(\Gamma(s+1,u)-\Gamma(s+1))}} =-\Gamma(s+1) \begin{align}\textstyle\displaystyle{\lim_{n\rightarrow\infty}\sum_{m=1}^{\infty}f(m,n)\geq\sum_{m=1}^{\infty}\lim_{n\rightarrow\infty}f(m,n)}\end{align} \textstyle\displaystyle{\sum_{n=1}^{\infty}\lim_{u\rightarrow\infty}{\frac{(-1)^nu^{n+s}}{(n-1)!(n+s)}}\leq -\Gamma(s+1)} \textstyle{\displaystyle{\sum_{n=1}^{\infty}{\lim_{u\rightarrow\infty}{\frac{(-1)^nu^{n+s}}{(n-1)!(n+s)}}}}}","['limits', 'summation', 'wolfram-alpha']"
11,Using first principle method to get derivative of $\sin(x°)$,Using first principle method to get derivative of,\sin(x°),"I saw the derivation of the derivative of $\sin x$ when $x$ was in radians but I don't know why we can't use the same derivation to get the derivative of $\sin(x°)$ . Here's my attempt: Note that $x$ and $h$ used below are in degrees. Let $f(x)=\sin(x)$ . So $$f'(x)= \lim_{h\rightarrow 0}\frac{\sin(x+h) - \sin x}{h}$$ Now, $$f'(x) = \lim_{h\rightarrow 0} \frac{2\cdot \cos(x+\frac{h}{2})\cdot \sin(\frac{h}{2})}{h}$$ $\Rightarrow$ $f'(x) = \cos(x)\cdot \lim_{h\rightarrow 0}\frac{\sin(\frac{h}{2})}{(\frac{h}{2})}$ Since $h$ is also in degree we get $f'(x)= \cos (x)$ where $x$ is in degree. Where am I wrong ?","I saw the derivation of the derivative of when was in radians but I don't know why we can't use the same derivation to get the derivative of . Here's my attempt: Note that and used below are in degrees. Let . So Now, Since is also in degree we get where is in degree. Where am I wrong ?",\sin x x \sin(x°) x h f(x)=\sin(x) f'(x)= \lim_{h\rightarrow 0}\frac{\sin(x+h) - \sin x}{h} f'(x) = \lim_{h\rightarrow 0} \frac{2\cdot \cos(x+\frac{h}{2})\cdot \sin(\frac{h}{2})}{h} \Rightarrow f'(x) = \cos(x)\cdot \lim_{h\rightarrow 0}\frac{\sin(\frac{h}{2})}{(\frac{h}{2})} h f'(x)= \cos (x) x,"['limits', 'derivatives']"
12,"If for all $x>0$, we have $\lim_{n\to\infty} g(xz_n)=+\infty$, does this imply $\lim_{x\to\infty} g(x)=\infty$?","If for all , we have , does this imply ?",x>0 \lim_{n\to\infty} g(xz_n)=+\infty \lim_{x\to\infty} g(x)=\infty,"Let $g:\mathbb{R}_{>0}\to\mathbb{R}_{>0}$ be some function (not necessarily continuous), and let $(z_n)_{n\geq 1}$ be an increasing, diverging sequence of positive real numbers with the following property: For all $x>0$ , we have $\displaystyle\lim_{n\to\infty} g(xz_n)=+\infty$ . Then is it true that $\displaystyle\lim_{x\to\infty} g(x)=\infty$ ? Intuitively I would say that this isn’t true because we have only information on a very small subsample of diverging sequences, but still we have a continuum at hand which makes it hard to cook up a counter example.","Let be some function (not necessarily continuous), and let be an increasing, diverging sequence of positive real numbers with the following property: For all , we have . Then is it true that ? Intuitively I would say that this isn’t true because we have only information on a very small subsample of diverging sequences, but still we have a continuum at hand which makes it hard to cook up a counter example.",g:\mathbb{R}_{>0}\to\mathbb{R}_{>0} (z_n)_{n\geq 1} x>0 \displaystyle\lim_{n\to\infty} g(xz_n)=+\infty \displaystyle\lim_{x\to\infty} g(x)=\infty,"['real-analysis', 'sequences-and-series', 'limits']"
13,Limit of the ratio of a nowhere differentiable function to a polynomial,Limit of the ratio of a nowhere differentiable function to a polynomial,,"Let $f(x)$ be a continuous function that— maps the closed interval [0, 1] to [0, 1], equals 0 at 0, does not equal 0 anywhere except at 0, and is nowhere differentiable on its domain. Let $g(x)$ be a polynomial that— maps the closed interval [0, 1] to [0, 1], bounds $f$ from above, and equals 0 at 0. My question is: Does the limit $\lim_{x\to 0^+} f(x)/g(x)$ exist?  If not, what are the weakest conditions required on $f$ for the limit to exist? I know that by L'Hôpital's rule, the limit exists when $f(x)$ is differentiable on some interval $(0, \epsilon)$ , but I don't know whether the limit still exists in this case when $f$ is not required to be differentiable.  This question is neither homework nor a self-study assignment, nor is this coursework.","Let be a continuous function that— maps the closed interval [0, 1] to [0, 1], equals 0 at 0, does not equal 0 anywhere except at 0, and is nowhere differentiable on its domain. Let be a polynomial that— maps the closed interval [0, 1] to [0, 1], bounds from above, and equals 0 at 0. My question is: Does the limit exist?  If not, what are the weakest conditions required on for the limit to exist? I know that by L'Hôpital's rule, the limit exists when is differentiable on some interval , but I don't know whether the limit still exists in this case when is not required to be differentiable.  This question is neither homework nor a self-study assignment, nor is this coursework.","f(x) g(x) f \lim_{x\to 0^+} f(x)/g(x) f f(x) (0, \epsilon) f","['real-analysis', 'limits']"
14,Equivalent condition for the existence of left hand limit.,Equivalent condition for the existence of left hand limit.,,"Let $f : [a,b] \longrightarrow \mathbb R$ be a function. Let $c \in (a,b].$ Then I know that $\lim\limits_{x \to c^-} f(x) = l$ iff for every sequence $\{x_n\}_{n \geq 1}$ converging to $c$ from below we have $f(x_n) \to l.$ Now my question is $:$ Instead of taking any sequence $\{x_n\}_{n \geq 1}$ converging to $c$ from below if we take any sequence $\{x_n \}_{n \geq 1}$ increases to $c$ do we have the same conclusion as above i.e. can it still be said that $\lim\limits_{x \to c^-} f(x) = l\ $ ? In order to show that it is true we need to show that if for any sequence $\{x_n \}_{n \geq 1}$ increases to $c$ the sequence $\{f(x_n)\}_{n \geq 1}$ converges to $l$ then for any sequence $\{x_n\}_{n \geq 1}$ converging to $c$ from below the sequence $\{f(x_n)\}_{n \geq 1}$ also converges to $l.$ But I don't know how to show that. Any suggestion in this regard will be appreciated. Thanks!",Let be a function. Let Then I know that iff for every sequence converging to from below we have Now my question is Instead of taking any sequence converging to from below if we take any sequence increases to do we have the same conclusion as above i.e. can it still be said that ? In order to show that it is true we need to show that if for any sequence increases to the sequence converges to then for any sequence converging to from below the sequence also converges to But I don't know how to show that. Any suggestion in this regard will be appreciated. Thanks!,"f : [a,b] \longrightarrow \mathbb R c \in (a,b]. \lim\limits_{x \to c^-} f(x) = l \{x_n\}_{n \geq 1} c f(x_n) \to l. : \{x_n\}_{n \geq 1} c \{x_n \}_{n \geq 1} c \lim\limits_{x \to c^-} f(x) = l\  \{x_n \}_{n \geq 1} c \{f(x_n)\}_{n \geq 1} l \{x_n\}_{n \geq 1} c \{f(x_n)\}_{n \geq 1} l.","['real-analysis', 'sequences-and-series', 'limits']"
15,limit superior of real value function (Exercise 9.3.4 in Tao Analysis I),limit superior of real value function (Exercise 9.3.4 in Tao Analysis I),,"Here is the exercise: Propose a definition for limit superior $\lim\sup_{x \to x_0 ; x \in E} f(x)$ and limit inferior $\lim\inf_{x \to x_0 ; x \in E} f(x)$ , and then propose an analogue of Proposition 9.3.9 for your definition. (For an additional challenge: prove that analogue.) Here is Proposition 9.3.9: Let $X$ be a subset of $\mathbf{R}$ , let $f : X \to \mathbf{R}$ be a function, let $E$ be a subset of $X$ , let $x_0$ be an adherent point of $E$ , and let $L$ be a real number. Then the following two statements are logically equivalent: (a). $f$ converges to $L$ at $x_0$ in $E$ . (b). For every sequence $(a_n)_{n = 0}^\infty$ which consists entirely of elements of $E$ and converges to $x_0$ , the sequence $(f(a_n))_{n = 0}^\infty$ converges to $L$ . Here is my attemped: Let $X$ be a subset of $\mathbf{R}$ , let $f : X \to \mathbf{R}$ be a function, let $E$ be a subset of $X$ , and let $x_0$ be an adherent point of $E$ . We define limit superior at $x_0$ in $E$ as $$     \limsup_{x \to x_0 ; x \in E} f(x) = \inf\Big\{\sup\big\{f(x) : x \in E \land |x - x_0| < \delta\big\} : \delta \in \mathbf{R}^+\Big\} $$ Let $L \in \mathbf{R} \cup \{-\infty, +\infty\}$ . We claim that the following statements are equivalent: (a). $\limsup_{x \to x_0 ; x \in E} f(x) = L$ (b). For every sequence $(a_n)_{n = 1}^\infty$ which consists entirely of elements of $E$ and converges to $x_0$ , the sequence $(f(a_n))_{n = 1}^\infty$ has limit superior $\limsup_{n \to \infty} f(a_n) \leq L$ There exists a sequence $(b_n)_{n = 1}^\infty$ which consists entirely of elements of $E$ and converges to $x_0$ , and $\limsup_{n \to \infty} f(b_n) = L$ I managed to prove (a) implies (b). But I don't know how to prove (b) implies (a). (Note that $L$ can be $-\infty$ or $+\infty$ ) Any helps are appreciated.","Here is the exercise: Propose a definition for limit superior and limit inferior , and then propose an analogue of Proposition 9.3.9 for your definition. (For an additional challenge: prove that analogue.) Here is Proposition 9.3.9: Let be a subset of , let be a function, let be a subset of , let be an adherent point of , and let be a real number. Then the following two statements are logically equivalent: (a). converges to at in . (b). For every sequence which consists entirely of elements of and converges to , the sequence converges to . Here is my attemped: Let be a subset of , let be a function, let be a subset of , and let be an adherent point of . We define limit superior at in as Let . We claim that the following statements are equivalent: (a). (b). For every sequence which consists entirely of elements of and converges to , the sequence has limit superior There exists a sequence which consists entirely of elements of and converges to , and I managed to prove (a) implies (b). But I don't know how to prove (b) implies (a). (Note that can be or ) Any helps are appreciated.","\lim\sup_{x \to x_0 ; x \in E} f(x) \lim\inf_{x \to x_0 ; x \in E} f(x) X \mathbf{R} f : X \to \mathbf{R} E X x_0 E L f L x_0 E (a_n)_{n = 0}^\infty E x_0 (f(a_n))_{n = 0}^\infty L X \mathbf{R} f : X \to \mathbf{R} E X x_0 E x_0 E 
    \limsup_{x \to x_0 ; x \in E} f(x) = \inf\Big\{\sup\big\{f(x) : x \in E \land |x - x_0| < \delta\big\} : \delta \in \mathbf{R}^+\Big\}
 L \in \mathbf{R} \cup \{-\infty, +\infty\} \limsup_{x \to x_0 ; x \in E} f(x) = L (a_n)_{n = 1}^\infty E x_0 (f(a_n))_{n = 1}^\infty \limsup_{n \to \infty} f(a_n) \leq L (b_n)_{n = 1}^\infty E x_0 \limsup_{n \to \infty} f(b_n) = L L -\infty +\infty","['real-analysis', 'limits', 'limsup-and-liminf']"
16,"Nature of convergence of the sequence of functions $(f_1(x))^{1/2^n}$ where $f_1$ is continuous $f_1:[0,\infty)\rightarrow[1,\infty)$",Nature of convergence of the sequence of functions  where  is continuous,"(f_1(x))^{1/2^n} f_1 f_1:[0,\infty)\rightarrow[1,\infty)","Given a continuous function $f_1:[0,\infty)\rightarrow[1,\infty)$ Define a sequence of functions recursively by $n\geq1$ , $f_{n+1}(x)=\sqrt{f_n(x)}$ Is this sequence of functions pointwise convergent? uniformly convergent? If Yes, find the limit function Attempt: First, we want to see the pattern of the recursive $f_1,f_2,f_3\dots$ $$n=1, f_2(x)=\sqrt{f_1(x)}$$ $$n=2, f_3(x)=\sqrt{\sqrt{f_1(x)}}$$ $$n=3, f_4(x)=\sqrt{\sqrt{\sqrt{f_1(x)}}}$$ $$\vdots$$ $$n\in\mathbb{N},f_{n+1}(x)=\sqrt[2n]{f_1(x)}$$ $$n\in\mathbb{N},f_{n}(x)=\sqrt[2(n-1)]{f_1(x)}$$ $$\sum^{\infty}_{n=1}a_n(x)=\lim_{n\rightarrow\infty}f_n(x)-f_1(x)$$ $$\lim_{n\rightarrow\infty}f_n(x)-f_1(x)=\lim_{n\rightarrow\infty}\sqrt[2(n-1)]{f_1(x)}-f_1(x)$$ I had to stop here because I think there is a mistake, I will appreciate that if anyone enlights me.","Given a continuous function Define a sequence of functions recursively by , Is this sequence of functions pointwise convergent? uniformly convergent? If Yes, find the limit function Attempt: First, we want to see the pattern of the recursive I had to stop here because I think there is a mistake, I will appreciate that if anyone enlights me.","f_1:[0,\infty)\rightarrow[1,\infty) n\geq1 f_{n+1}(x)=\sqrt{f_n(x)} f_1,f_2,f_3\dots n=1, f_2(x)=\sqrt{f_1(x)} n=2, f_3(x)=\sqrt{\sqrt{f_1(x)}} n=3, f_4(x)=\sqrt{\sqrt{\sqrt{f_1(x)}}} \vdots n\in\mathbb{N},f_{n+1}(x)=\sqrt[2n]{f_1(x)} n\in\mathbb{N},f_{n}(x)=\sqrt[2(n-1)]{f_1(x)} \sum^{\infty}_{n=1}a_n(x)=\lim_{n\rightarrow\infty}f_n(x)-f_1(x) \lim_{n\rightarrow\infty}f_n(x)-f_1(x)=\lim_{n\rightarrow\infty}\sqrt[2(n-1)]{f_1(x)}-f_1(x)","['calculus', 'limits', 'functions']"
17,A limit of a given sequence,A limit of a given sequence,,"Let $(a_n)_{n\ge1}$ a sequence of real positive numbers such that $a_n+\frac{n}{a_{n+1}^2} \le a_{n+1} \le \frac{a_{n-1}^2}{a_n}+\frac{n+1}{a_n^2}$ for any $n \ge 1$ . Prove that $b_n = \frac{a_n}{\sqrt[3]{n^2}}$ is a convergent sequence and find its limit. First of all, it's clear that $a_n$ is a increasing sequence from the first inequality given. So $a_n$ has a limit and it is either a finite number or infinity. If we suppose that $a_n$ is convergent and has a finite limit (let it be $l$ ), by using limit in the first inequality we obtain: $l+\infty \le l$ , which is false and it means that $\lim_{n \to \infty} a_n = \infty$ , so we may apply Cesaro-Stolz for finding the limit of $b_n$ , but here I didn't know how to continue. Can you help me?","Let a sequence of real positive numbers such that for any . Prove that is a convergent sequence and find its limit. First of all, it's clear that is a increasing sequence from the first inequality given. So has a limit and it is either a finite number or infinity. If we suppose that is convergent and has a finite limit (let it be ), by using limit in the first inequality we obtain: , which is false and it means that , so we may apply Cesaro-Stolz for finding the limit of , but here I didn't know how to continue. Can you help me?",(a_n)_{n\ge1} a_n+\frac{n}{a_{n+1}^2} \le a_{n+1} \le \frac{a_{n-1}^2}{a_n}+\frac{n+1}{a_n^2} n \ge 1 b_n = \frac{a_n}{\sqrt[3]{n^2}} a_n a_n a_n l l+\infty \le l \lim_{n \to \infty} a_n = \infty b_n,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
18,$\delta$-response for challenging the $\lim_{x \to 10} \frac{1}{[[x]]} = \frac{1}{10}$ with $\epsilon=\frac{1}{2}$,-response for challenging the  with,\delta \lim_{x \to 10} \frac{1}{[[x]]} = \frac{1}{10} \epsilon=\frac{1}{2},"$\newcommand{\absval}[1]{\left\lvert #1 \right\rvert}$ I am self-studying Real Analysis from Stephen Abbott's Understanding Analysis . I'd like to ask if my conclusions pertaining to exercise (a), (b) of the below problem are correct. Notation. $f(x)= [[x]]$ is the box function, the greatest integer less than or equal to $x$ , for all $x \in \mathbf{R}$ . Exercise 4.2.4. Consider the reasonable but erroneous claim that \begin{align*} \lim_{x \to 10} \frac{1}{[[x]]} = \frac{1}{10} \end{align*} (a) Find the largest $\delta$ that represents a proper response to the challenge of $\epsilon = 1/2$ . (b) Find the largest $\delta$ that represents a proper response to $\epsilon = 1/50$ . (c) Find the largest $\epsilon$ challenge for which there is no suitable $\delta$ response possible. Proof. (a) We require \begin{align*} \frac{1}{10} - \frac{1}{2} &< \frac{1}{[[x]]} &< \frac{1}{10} + \frac{1}{2} \\ \frac{-4}{10} &< \frac{1}{[[x]]} &< \frac{6}{10} \end{align*} So, $[[x]] < \frac{-10}{4} < -2$ and $[[x]]>\frac{10}{6} > 1$ . In other words, $x-10 < -12$ and $x-10>-8$ . The absolute distance must satisfy the inequality $\absval{x - 10} < 8$ . Thus, the largest $\delta-$ response to the challenge $\epsilon=1/2$ appears to be $\delta = 8$ . (b) We require \begin{align*} \frac{1}{10} - \frac{1}{50} &< \frac{1}{[[x]]} &< \frac{1}{10} + \frac{1}{50} \\ \frac{4}{50} &< \frac{1}{[[x]]} &< \frac{6}{50} \end{align*} So, $[[x]] < \frac{50}{4} < 13$ and $[[x]]>\frac{50}{6} > 8$ . In other words, $x < 13$ or $x >9$ . The absolute distance must satisfy the inequality $\absval{x - 10} < 1$ . Thus, the largest $\delta-$ response to the challenge $\epsilon=1/50$ appears to be $\delta = 1$ . (c) We would like to have the distance \begin{align*} \absval{\frac{1}{[[x]]} - \frac{1}{10}} > \epsilon \end{align*} no matter what the open interval $(10-\delta,10+\delta)$ in which $x$ lies. Rearranging, I get: \begin{align*} \epsilon < \frac{\absval{[[x]] - 10}}{10 \absval{[[x]]}} \end{align*} I am not sure how to proceed from here. I know that, $\absval{[[x]] - 10}$ . That yields, \begin{align*} \epsilon < \frac{\delta}{10 \absval{[[x]]}} \end{align*} I can further write, $[[x]] > \lceil{10 - \delta}\rceil$ . But, this $\epsilon$ is dependent on the $\delta$ -interval I choose.","I am self-studying Real Analysis from Stephen Abbott's Understanding Analysis . I'd like to ask if my conclusions pertaining to exercise (a), (b) of the below problem are correct. Notation. is the box function, the greatest integer less than or equal to , for all . Exercise 4.2.4. Consider the reasonable but erroneous claim that (a) Find the largest that represents a proper response to the challenge of . (b) Find the largest that represents a proper response to . (c) Find the largest challenge for which there is no suitable response possible. Proof. (a) We require So, and . In other words, and . The absolute distance must satisfy the inequality . Thus, the largest response to the challenge appears to be . (b) We require So, and . In other words, or . The absolute distance must satisfy the inequality . Thus, the largest response to the challenge appears to be . (c) We would like to have the distance no matter what the open interval in which lies. Rearranging, I get: I am not sure how to proceed from here. I know that, . That yields, I can further write, . But, this is dependent on the -interval I choose.","\newcommand{\absval}[1]{\left\lvert #1 \right\rvert} f(x)= [[x]] x x \in \mathbf{R} \begin{align*}
\lim_{x \to 10} \frac{1}{[[x]]} = \frac{1}{10}
\end{align*} \delta \epsilon = 1/2 \delta \epsilon = 1/50 \epsilon \delta \begin{align*}
\frac{1}{10} - \frac{1}{2} &< \frac{1}{[[x]]} &< \frac{1}{10} + \frac{1}{2} \\
\frac{-4}{10} &< \frac{1}{[[x]]} &< \frac{6}{10}
\end{align*} [[x]] < \frac{-10}{4} < -2 [[x]]>\frac{10}{6} > 1 x-10 < -12 x-10>-8 \absval{x - 10} < 8 \delta- \epsilon=1/2 \delta = 8 \begin{align*}
\frac{1}{10} - \frac{1}{50} &< \frac{1}{[[x]]} &< \frac{1}{10} + \frac{1}{50} \\
\frac{4}{50} &< \frac{1}{[[x]]} &< \frac{6}{50}
\end{align*} [[x]] < \frac{50}{4} < 13 [[x]]>\frac{50}{6} > 8 x < 13 x >9 \absval{x - 10} < 1 \delta- \epsilon=1/50 \delta = 1 \begin{align*}
\absval{\frac{1}{[[x]]} - \frac{1}{10}} > \epsilon
\end{align*} (10-\delta,10+\delta) x \begin{align*}
\epsilon < \frac{\absval{[[x]] - 10}}{10 \absval{[[x]]}}
\end{align*} \absval{[[x]] - 10} \begin{align*}
\epsilon < \frac{\delta}{10 \absval{[[x]]}}
\end{align*} [[x]] > \lceil{10 - \delta}\rceil \epsilon \delta","['real-analysis', 'limits', 'solution-verification', 'epsilon-delta']"
19,How do I find the limit of $\lim_{x \to 0} \frac{x-\ln(x+1)}{x(\sin(2x))}$?,How do I find the limit of ?,\lim_{x \to 0} \frac{x-\ln(x+1)}{x(\sin(2x))},"How do I find the limit of $$\lim_{x \to 0} \frac{x-\ln(x+1)}{x(\sin(2x))}$$ I know it is equal to $\frac{1}{4}$ , but how did we get that? Without using L'Hopital's rule. I tried canceling $x$ with $\sin(2x)$ and $\ln(x+1)$ , but still got the wrong  result...","How do I find the limit of I know it is equal to , but how did we get that? Without using L'Hopital's rule. I tried canceling with and , but still got the wrong  result...",\lim_{x \to 0} \frac{x-\ln(x+1)}{x(\sin(2x))} \frac{1}{4} x \sin(2x) \ln(x+1),"['calculus', 'limits', 'logarithms', 'limits-without-lhopital']"
20,"Find the $\mathcal{O}\left(\frac{1}{n}\right)$ so that $a_n-1=\mathcal{O}\left(\frac{1}{n}\right)\,{\rm as}\,n\rightarrow\infty$",Find the  so that,"\mathcal{O}\left(\frac{1}{n}\right) a_n-1=\mathcal{O}\left(\frac{1}{n}\right)\,{\rm as}\,n\rightarrow\infty","Given a recursion $a_{n+ 1}= \dfrac{a_{n}^{2}+ 1}{2}$ with $a_{1}= \dfrac{1}{2}.$ Find the $\mathcal{O}\left ( \dfrac{1}{n} \right )$ so that $$a_{n}- 1\sim\mathcal{O}\left ( \dfrac{1}{n} \right )\,{\rm as}\,n\rightarrow\infty$$ Remark. By this recurrence sequence, Ji Chen proposed 2 inequalities on AoPS, which I can't prove, of course $$1- \frac{2}{n}+ \frac{2}{n^{2}}\ln\frac{n}{3}+ \frac{417}{128n^{2}}\leq a_{n}\leq 1- \frac{2}{n}+ \frac{5\ln n+ 3}{n^{2}}$$ and for non-negative $x_{1\div n}$ quite not related $$\sum_{i= 1}^{n}\frac{x_{i}}{\left ( 1+ \sum_{j= 1}^{i}x_{j} \right )^{2}}\leq a_{n}^{2}$$ Return to the OP, I realise that $$a_{n+ 1}- 1= \frac{1}{2}\left ( a_{n}- 1 \right )^{2}+ a_{n}- 1= \frac{1}{2}\left ( a_{n}- 1 \right )^{2}+ \omega\left ( \left ( a_{n}- 1 \right )^{2} \right )$$ As same as what I told on this topic  * Find the $\mathcal{o}\left(n\right)$ so that $a_n\sim\frac{\mathcal{o}\left(n\right)}{n}+\mathcal{O}\left(\frac{1}{n}\right){\rm as}\lim a_n=\infty$ *, I still can't find such an $\mathcal{o}$ satisfying my wish. I need to your help, thanks.","Given a recursion with Find the so that Remark. By this recurrence sequence, Ji Chen proposed 2 inequalities on AoPS, which I can't prove, of course and for non-negative quite not related Return to the OP, I realise that As same as what I told on this topic  * Find the $\mathcal{o}\left(n\right)$ so that $a_n\sim\frac{\mathcal{o}\left(n\right)}{n}+\mathcal{O}\left(\frac{1}{n}\right){\rm as}\lim a_n=\infty$ *, I still can't find such an satisfying my wish. I need to your help, thanks.","a_{n+ 1}= \dfrac{a_{n}^{2}+ 1}{2} a_{1}= \dfrac{1}{2}. \mathcal{O}\left ( \dfrac{1}{n} \right ) a_{n}- 1\sim\mathcal{O}\left ( \dfrac{1}{n} \right )\,{\rm as}\,n\rightarrow\infty 1- \frac{2}{n}+ \frac{2}{n^{2}}\ln\frac{n}{3}+ \frac{417}{128n^{2}}\leq a_{n}\leq 1- \frac{2}{n}+ \frac{5\ln n+ 3}{n^{2}} x_{1\div n} \sum_{i= 1}^{n}\frac{x_{i}}{\left ( 1+ \sum_{j= 1}^{i}x_{j} \right )^{2}}\leq a_{n}^{2} a_{n+ 1}- 1= \frac{1}{2}\left ( a_{n}- 1 \right )^{2}+ a_{n}- 1= \frac{1}{2}\left ( a_{n}- 1 \right )^{2}+ \omega\left ( \left ( a_{n}- 1 \right )^{2} \right ) \mathcal{o}","['limits', 'recurrence-relations']"
21,$\lim_{n \to \infty} (u_{n+1}-u_n)$ does not converge then $\lim_{n \to \infty} u_n$ does not converge,does not converge then  does not converge,\lim_{n \to \infty} (u_{n+1}-u_n) \lim_{n \to \infty} u_n,"Prove that if $\lim_{n \to \infty} (u_{n+1}-u_n)$ does not converge then $\lim_{n \to \infty} u_n$ does not converge. My try - Contrapositive statement - If $\lim_{n \to \infty} u_n$ converges then $\lim_{n \to \infty} (u_{n+1}-u_n)$ converges. Let , $\lim_{n \to \infty} u_n=L$ Then $\lim_{n \to \infty} u_{n+1}=L$ $\lim_{n \to \infty} (u_{n+1}-u_n)=L-L=0$ Is the proof correct? Any other proof is also desirable.","Prove that if does not converge then does not converge. My try - Contrapositive statement - If converges then converges. Let , Then Is the proof correct? Any other proof is also desirable.",\lim_{n \to \infty} (u_{n+1}-u_n) \lim_{n \to \infty} u_n \lim_{n \to \infty} u_n \lim_{n \to \infty} (u_{n+1}-u_n) \lim_{n \to \infty} u_n=L \lim_{n \to \infty} u_{n+1}=L \lim_{n \to \infty} (u_{n+1}-u_n)=L-L=0,"['real-analysis', 'calculus']"
22,Proving that a function has primitives,Proving that a function has primitives,,"Let f a differentiable function such that $\lim_{x\to\infty}\frac{f(x)}{x}$ = $\lim_{x\to-\infty}\frac{f(x)}{x}=0$ .Prove that the function $g:\Bbb R\to\Bbb R$ admits primitives when g is: $g(x)=f'(1/x)$ , for any $x\neq 0$ and $g(0)=0$ I tried to integrate the function g and to make the substitution $x:=1/x$ but it did not helped me out.","Let f a differentiable function such that = .Prove that the function admits primitives when g is: , for any and I tried to integrate the function g and to make the substitution but it did not helped me out.",\lim_{x\to\infty}\frac{f(x)}{x} \lim_{x\to-\infty}\frac{f(x)}{x}=0 g:\Bbb R\to\Bbb R g(x)=f'(1/x) x\neq 0 g(0)=0 x:=1/x,"['real-analysis', 'integration']"
23,"For $x_{n+1}=x_n-x_n^3$, with $|x_1|>1$. What about the convergence?","For , with . What about the convergence?",x_{n+1}=x_n-x_n^3 |x_1|>1,"I see Let $\{ x_n \}_{n=1}^{\infty}$ such that $x_{n+1}=x_n-x_n^3$ and $0<x_1<1$ that if $|x_1| \le 1$ , then $x_n\to 0$ . However, if $|x_1|>1$ , what can we say about this sequence? It seems hard to find the tendency of it. For $x_1$ near $1$ , it converges to $0$ , but for $x_1$ large, it is not the case.","I see Let $\{ x_n \}_{n=1}^{\infty}$ such that $x_{n+1}=x_n-x_n^3$ and $0<x_1<1$ that if , then . However, if , what can we say about this sequence? It seems hard to find the tendency of it. For near , it converges to , but for large, it is not the case.",|x_1| \le 1 x_n\to 0 |x_1|>1 x_1 1 0 x_1,"['calculus', 'limits']"
24,$\lim_{y \to 0^{+}} \int_{0}^{\infty} e^{-yx} \frac{\sin x}{x} dx = \frac{\pi}{2}$,,\lim_{y \to 0^{+}} \int_{0}^{\infty} e^{-yx} \frac{\sin x}{x} dx = \frac{\pi}{2},"I have shown using the application of Dominated Convergence Theorem (i.e., interchange of differentiation and integral) that $$ \int_{0}^{\infty} e^{-yx} \frac{\sin x}{x} dx = \frac{\pi}{2}-\tan^{-1}y $$ for $(x,y) \in [0,\infty) \times (0,\infty)$ . Now I want to conclude that $$ \int_{0}^{\infty} \frac{\sin x}{x} dx =\int_{0}^{\infty} \lim_{y \to 0+}e^{-yx} \frac{\sin x}{x} dx =\lim_{y \to 0+}\int_{0}^{\infty} e^{-yx} \frac{\sin x}{x} dx = \lim_{y \to 0+} \frac{\pi}{2}-\tan^{-1}y =  \frac{\pi}{2} $$ (The hint which is given with this part is to use DCT.) To interchange the limit and the integral sign, using DCT, I need to find a non-negative function $g$ which dominates $ |(e^{-yx}\sin x) /x|$ and is integrable, i.e., $g \in L^{1}([0,\infty))$ . The only function I could think of is (obviously) $|\sin x/x|$ . But I don't know whether $|\sin x/x|$ is integrable.","I have shown using the application of Dominated Convergence Theorem (i.e., interchange of differentiation and integral) that for . Now I want to conclude that (The hint which is given with this part is to use DCT.) To interchange the limit and the integral sign, using DCT, I need to find a non-negative function which dominates and is integrable, i.e., . The only function I could think of is (obviously) . But I don't know whether is integrable.","
\int_{0}^{\infty} e^{-yx} \frac{\sin x}{x} dx = \frac{\pi}{2}-\tan^{-1}y
 (x,y) \in [0,\infty) \times (0,\infty) 
\int_{0}^{\infty} \frac{\sin x}{x} dx =\int_{0}^{\infty} \lim_{y \to 0+}e^{-yx} \frac{\sin x}{x} dx =\lim_{y \to 0+}\int_{0}^{\infty} e^{-yx} \frac{\sin x}{x} dx = \lim_{y \to 0+} \frac{\pi}{2}-\tan^{-1}y =  \frac{\pi}{2}
 g  |(e^{-yx}\sin x) /x| g \in L^{1}([0,\infty)) |\sin x/x| |\sin x/x|","['limits', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
25,Calculate the following limit:,Calculate the following limit:,,"Calculate $\;\lim\limits_{x\to\infty} \left[x^2\left(1+\dfrac1x\right)^x-ex^3\ln\left(1+\dfrac1x\right)\right]$ . It is a $\frac00$ case of indetermination if we rewrite as $\lim_{x\to\infty} \frac{((1+\frac1x)^x-e\ln(1+\frac1x)^x)}{\frac{1}{x^2}}$ , since $\lim_{x\to\infty} \frac{1}{x^2} = 0$ , $\lim_{x\to\infty} (1+\frac1x)^x = e$ and $\lim_{x\to\infty} \ln(1+\frac1x)^x = 1$ . I think that it is the type that has a solution without l'Hospital's rule, but it's quite difficult to find, so l'Hospital still remains the best try to me. I tried using it with different rewrites, but it seems that it needs to be used multiple times, and the expression gets harder and harder to calculate, so I assume that some other limit must be applied first to make the expression nicer. Also, I futilely tried to use the following known limits by changing $x$ into $y = \frac1x$ if needed (and adding and substracting $ex^2$ in the main parenthesis and trying to use the last 2 limits), but maybe it can help you: $\lim_{x\to0} \frac{a^x-1}{x} = \ln a$ , $\lim_{x\to0} \frac{\ln(1+x)}{x} = 1$ , $\lim_{x\to0} \frac{(1+x)^r-1}{x} = r$ , $\lim_{x\to0} \frac{(1+x)^\frac1x-e}{x} = -\frac{e}{2}$ , $\lim_{x\to\infty} (x-x^2ln(1+\frac1x)) = \frac12$ . Can you help me with this problem?","Calculate . It is a case of indetermination if we rewrite as , since , and . I think that it is the type that has a solution without l'Hospital's rule, but it's quite difficult to find, so l'Hospital still remains the best try to me. I tried using it with different rewrites, but it seems that it needs to be used multiple times, and the expression gets harder and harder to calculate, so I assume that some other limit must be applied first to make the expression nicer. Also, I futilely tried to use the following known limits by changing into if needed (and adding and substracting in the main parenthesis and trying to use the last 2 limits), but maybe it can help you: , , , , . Can you help me with this problem?",\;\lim\limits_{x\to\infty} \left[x^2\left(1+\dfrac1x\right)^x-ex^3\ln\left(1+\dfrac1x\right)\right] \frac00 \lim_{x\to\infty} \frac{((1+\frac1x)^x-e\ln(1+\frac1x)^x)}{\frac{1}{x^2}} \lim_{x\to\infty} \frac{1}{x^2} = 0 \lim_{x\to\infty} (1+\frac1x)^x = e \lim_{x\to\infty} \ln(1+\frac1x)^x = 1 x y = \frac1x ex^2 \lim_{x\to0} \frac{a^x-1}{x} = \ln a \lim_{x\to0} \frac{\ln(1+x)}{x} = 1 \lim_{x\to0} \frac{(1+x)^r-1}{x} = r \lim_{x\to0} \frac{(1+x)^\frac1x-e}{x} = -\frac{e}{2} \lim_{x\to\infty} (x-x^2ln(1+\frac1x)) = \frac12,['limits']
26,"If $x_{n+1}=x_n+x_n^{-m}, x_1>0, m \ge 2 $ then $x_{n}^{m+1} = (m+1)(n+\ln(n))+O_m(1) $",If  then,"x_{n+1}=x_n+x_n^{-m}, x_1>0, m \ge 2  x_{n}^{m+1} = (m+1)(n+\ln(n))+O_m(1) ","This is a generalization of Divergence of a sequence proof Show that if $x_{n+1}=x_n+\dfrac{1}{x_n^m}, x_1>0, m \ge 2 $ then $x_{n}^{m+1} = (m+1)(n+\ln(n))+O_m(1) $ . (Note: The notation $O_m(...)$ means that the constant implied by the big-oh depends on $m$ .) I can show that $x_{n}^{m+1} = (m+1)n+O_m(\ln(n)) $ for integer $m$ , and I am pretty sure that in this case I can show that $x_{n}^{m+1} = (m+1)(n+\ln(n))+O_m(1) $ . I don't have an explicit expression for the $O_m(1)$ and I don't have a proof for real $m \ge 2$ .","This is a generalization of Divergence of a sequence proof Show that if then . (Note: The notation means that the constant implied by the big-oh depends on .) I can show that for integer , and I am pretty sure that in this case I can show that . I don't have an explicit expression for the and I don't have a proof for real .","x_{n+1}=x_n+\dfrac{1}{x_n^m},
x_1>0, m \ge 2
 x_{n}^{m+1}
= (m+1)(n+\ln(n))+O_m(1)
 O_m(...) m x_{n}^{m+1}
= (m+1)n+O_m(\ln(n))
 m x_{n}^{m+1}
= (m+1)(n+\ln(n))+O_m(1)
 O_m(1) m \ge 2","['sequences-and-series', 'limits', 'recurrence-relations', 'nonlinear-dynamics']"
27,Show that $\lim_{n \to \infty} \prod_{k=1}^{k=n} (1+ \frac{1}{n}f(\frac{k}{n})) = e^{\int_{0}^{1}f(x) dx}$ [duplicate],Show that  [duplicate],\lim_{n \to \infty} \prod_{k=1}^{k=n} (1+ \frac{1}{n}f(\frac{k}{n})) = e^{\int_{0}^{1}f(x) dx},"This question already has an answer here : Help finding the limit of $\lim_{n \to \infty}\prod_{k=1}^{n}\left(1+\frac{1}{n}f\left(\frac{k}{n}\right)\right)$. (1 answer) Closed 3 years ago . Let $f:[0,1] \to \mathbb{R}$ be a continuous function. Show that $\lim_{n \to \infty} \prod_{k=1}^{k=n} (1+ \frac{1}{n}f(\frac{k}{n})) = e^{\int_{0}^{1}f(x) dx}$ . Attempt:  let $\lim \prod_{k=1}^{k=n} (1+ \frac{1}{n}f(\frac{k}{n})) = l$ then $\lim_{n \to \infty} \log(\prod_{k=1}^{k=n} (1+ \frac{1}{n}f(\frac{k}{n})))= \log l \implies \lim \sum_{k=1}^{k=n} \log(1+ \frac{1}{n}f(\frac{k}{n})) = \log l$ . and working backwards we should have $\log l = \int_{0}^{1} f(x)dx = \lim_{n \to \infty} \sum_{k=1}^{k=n} \frac{1}{n} f(\frac{k}{n})$ . So here I'm trying to find the link between $\lim \sum_{k=1}^{k=n} \log(1+ \frac{1}{n}f(\frac{k}{n}))$ and $\lim_{n \to \infty} \sum_{k=1}^{k=n} \frac{1}{n} f(\frac{k}{n})$ . but $\log(1+x) \approx x$ for $x <1$ , so in order to use this approximation I have to prove that $\frac{1}{n}f(\frac{k}{n})< 1$ (or maybe I can use the fact that f is continuous and prove that f attains supremum at some point in $[0,1]$ ) after some terms and I'm stuck .  Is there any other approach which avoids this approximation?","This question already has an answer here : Help finding the limit of $\lim_{n \to \infty}\prod_{k=1}^{n}\left(1+\frac{1}{n}f\left(\frac{k}{n}\right)\right)$. (1 answer) Closed 3 years ago . Let be a continuous function. Show that . Attempt:  let then . and working backwards we should have . So here I'm trying to find the link between and . but for , so in order to use this approximation I have to prove that (or maybe I can use the fact that f is continuous and prove that f attains supremum at some point in ) after some terms and I'm stuck .  Is there any other approach which avoids this approximation?","f:[0,1] \to \mathbb{R} \lim_{n \to \infty} \prod_{k=1}^{k=n} (1+ \frac{1}{n}f(\frac{k}{n})) = e^{\int_{0}^{1}f(x) dx} \lim \prod_{k=1}^{k=n} (1+ \frac{1}{n}f(\frac{k}{n})) = l \lim_{n \to \infty} \log(\prod_{k=1}^{k=n} (1+ \frac{1}{n}f(\frac{k}{n})))= \log l \implies \lim \sum_{k=1}^{k=n} \log(1+ \frac{1}{n}f(\frac{k}{n})) = \log l \log l = \int_{0}^{1} f(x)dx = \lim_{n \to \infty} \sum_{k=1}^{k=n} \frac{1}{n} f(\frac{k}{n}) \lim \sum_{k=1}^{k=n} \log(1+ \frac{1}{n}f(\frac{k}{n})) \lim_{n \to \infty} \sum_{k=1}^{k=n} \frac{1}{n} f(\frac{k}{n}) \log(1+x) \approx x x <1 \frac{1}{n}f(\frac{k}{n})< 1 [0,1]","['limits', 'definite-integrals']"
28,Limit of a sequence within a logarithm,Limit of a sequence within a logarithm,,"Let $(x_n)_{n \in \mathbb{N}}$ be a sequence of real numbers. (1) Assume that $x_n>-1 \space \space \forall \space n \in \mathbb{N}$ and $x_n \to 0$ as $n \to \infty$ , show that $\log(x_n+1) \to 0$ as $n \to \infty$ . (2) Assume that $x_n>0 \space \space \forall \space n \in \mathbb{N}$ and $x_n \to x>0$ as $n \to \infty$ , show that $\log(x_n) \to \log(x)$ as $n \to \infty$ . I managed to do the first one but I'm not sure how to go about the second one. A similar method didn't seem to apply. In this question, $\log(x)=y$ where $y$ is the unique real number with $\exp(y)=x$ . Where $\exp(x)$ is the limit of $(1+x/n)^n$ as $n$ tends to infinty. The method I've used so far involves the inequality $1+x\leq e^x \leq 1/(1-x) \space \space \forall \space x <1$ .","Let be a sequence of real numbers. (1) Assume that and as , show that as . (2) Assume that and as , show that as . I managed to do the first one but I'm not sure how to go about the second one. A similar method didn't seem to apply. In this question, where is the unique real number with . Where is the limit of as tends to infinty. The method I've used so far involves the inequality .",(x_n)_{n \in \mathbb{N}} x_n>-1 \space \space \forall \space n \in \mathbb{N} x_n \to 0 n \to \infty \log(x_n+1) \to 0 n \to \infty x_n>0 \space \space \forall \space n \in \mathbb{N} x_n \to x>0 n \to \infty \log(x_n) \to \log(x) n \to \infty \log(x)=y y \exp(y)=x \exp(x) (1+x/n)^n n 1+x\leq e^x \leq 1/(1-x) \space \space \forall \space x <1,"['real-analysis', 'sequences-and-series', 'limits']"
29,Polynomials and Limits,Polynomials and Limits,,Let $P(x)$ be any polynomial with positive real coefficients. Determine the following limit with proof $ \lim \limits_{x \to \infty} \dfrac{\lfloor P(x) \rfloor}{P(\lfloor x \rfloor)}$ I am unable to approach these kind of sums. Anyone please can guide me and help me with this solution?,Let be any polynomial with positive real coefficients. Determine the following limit with proof I am unable to approach these kind of sums. Anyone please can guide me and help me with this solution?,P(x)  \lim \limits_{x \to \infty} \dfrac{\lfloor P(x) \rfloor}{P(\lfloor x \rfloor)},"['calculus', 'limits', 'polynomials', 'ceiling-and-floor-functions']"
30,Find the limit of the series $6^n/n!$ as $n$ tends to infinity.,Find the limit of the series  as  tends to infinity.,6^n/n! n,"I need to find the limit of the following serie: $\lim_\limits{n\to \infty}$$\frac{6^n}{n!}$ I was thinking of the following solution, but i'm not sure it's correct, please let me know your thoughts :) $0$ $<$ $\frac{6^n}{n!}$ $<$ $\frac{6}{1}\cdot$$\frac{6}{2}\cdot$ $\frac{6}{3}\cdot$$\frac{6}{4}\cdot$$\frac{6}{5}\cdot$$\frac{6}{6}\cdot$ $1\cdot$ $1\cdot$ $1\cdot$ .... $\cdot1\cdot$ $\frac{6}{n}$ $=$ $\frac{1944}{5n}$ $\lim_\limits{n\to \infty}$$0$ $= 0$ $\lim_\limits{n\to \infty}$$\frac{1944}{5n}$ $= 0$ Hence, by using the sandwich theorem, $\lim_\limits{n\to \infty}$$\frac{6^n}{n!}$ $= 0$ .","I need to find the limit of the following serie: I was thinking of the following solution, but i'm not sure it's correct, please let me know your thoughts :) .... Hence, by using the sandwich theorem, .",\lim_\limits{n\to \infty}\frac{6^n}{n!} 0 < \frac{6^n}{n!} < \frac{6}{1}\cdot\frac{6}{2}\cdot \frac{6}{3}\cdot\frac{6}{4}\cdot\frac{6}{5}\cdot\frac{6}{6}\cdot 1\cdot 1\cdot 1\cdot \cdot1\cdot \frac{6}{n} = \frac{1944}{5n} \lim_\limits{n\to \infty}0 = 0 \lim_\limits{n\to \infty}\frac{1944}{5n} = 0 \lim_\limits{n\to \infty}\frac{6^n}{n!} = 0,"['calculus', 'sequences-and-series', 'limits', 'convergence-divergence']"
31,Spivak Calculus Chapter 5-33d: $\lim_{x \to\infty}\frac{x^2(1+\sin^2(x))}{(x+\sin(x))^2}$,Spivak Calculus Chapter 5-33d:,\lim_{x \to\infty}\frac{x^2(1+\sin^2(x))}{(x+\sin(x))^2},"Find the following limit: 33d) $$\lim_{x\to\infty}\frac{x^2(1+\sin^2(x))}{(x+\sin(x))^2}$$ I am 99.9% sure this limit doesn't exist because as $x \rightarrow \infty$ , $1+\sin^2(x)$ periodically oscillates between 1 and 2, which means so does the limit. Is this a trick question or am I losing my mind?","Find the following limit: 33d) I am 99.9% sure this limit doesn't exist because as , periodically oscillates between 1 and 2, which means so does the limit. Is this a trick question or am I losing my mind?",\lim_{x\to\infty}\frac{x^2(1+\sin^2(x))}{(x+\sin(x))^2} x \rightarrow \infty 1+\sin^2(x),['calculus']
32,Why do we need continuity in this theorem of path independence of limit of multi variable function?,Why do we need continuity in this theorem of path independence of limit of multi variable function?,,"Let $f:D\to \mathbb{R}$ , where $D\subset\mathbb{R}^2$ such that a deleted neighborhood of a point $(a,b)$ is contained in $D$ . If $\lim_{(x,y)\to(a,b)}f(x,y)=L$ exists then For any real continuous function $g$ if $\lim_{x\to a}g(x)=b$ then $\lim_{x\to a}f(x,g(x))=L$ My question is, Why is the continuity of $g$ required ? Here is how I attempted proof : For any $\epsilon>0$ , $\exists\delta_1>0 : |x-a|<\delta_1 , |u-b|<\delta_1, (x,u)\neq (a,b)\implies|f(x,u)-L|<\epsilon$ And for the function $g$ , $\exists\delta_2>0 : 0<|x-a|<\delta_2 \implies|g(x)-b|<\delta_1$ Hence, $\exists\delta=\min(\delta_1,\delta_2) > 0 : 0<|x-a|<\delta$ $\implies 0<|x-a|<\delta_1$ and $0<|x-a|<\delta_2$ $\implies 0<|x-a|<\delta_1$ and $|g(x)-b|<\delta_1$ $\implies |x-a|<\delta_1$ and $|g(x)-b|<\delta_1$ ; $(x,g(x))\neq (a,b)$ $\implies|f(x,g(x))-L|<\epsilon$ Which step from the above requires continuity ?","Let , where such that a deleted neighborhood of a point is contained in . If exists then For any real continuous function if then My question is, Why is the continuity of required ? Here is how I attempted proof : For any , And for the function , Hence, and and and ; Which step from the above requires continuity ?","f:D\to \mathbb{R} D\subset\mathbb{R}^2 (a,b) D \lim_{(x,y)\to(a,b)}f(x,y)=L g \lim_{x\to a}g(x)=b \lim_{x\to a}f(x,g(x))=L g \epsilon>0 \exists\delta_1>0 : |x-a|<\delta_1 , |u-b|<\delta_1, (x,u)\neq (a,b)\implies|f(x,u)-L|<\epsilon g \exists\delta_2>0 : 0<|x-a|<\delta_2 \implies|g(x)-b|<\delta_1 \exists\delta=\min(\delta_1,\delta_2) > 0 : 0<|x-a|<\delta \implies 0<|x-a|<\delta_1 0<|x-a|<\delta_2 \implies 0<|x-a|<\delta_1 |g(x)-b|<\delta_1 \implies |x-a|<\delta_1 |g(x)-b|<\delta_1 (x,g(x))\neq (a,b) \implies|f(x,g(x))-L|<\epsilon","['real-analysis', 'calculus', 'limits', 'multivariable-calculus', 'continuity']"
33,Limit Using squeeze theorem,Limit Using squeeze theorem,,"Find the limit of the following function as $x \rightarrow 0$ $$ \frac{|x|}{\sqrt{\left(x^{4}+4 x^{2}+7\right)}} \sin \left(\frac{1}{3 \sqrt{x}}\right) $$ $\lim _{x \rightarrow 0} \frac{|x|}{\sqrt{\left(x^{4}+4 x^{2}+77\right)}} \sin \left(\frac{1}{3 \sqrt{x}}\right)$ My approach , applying squeeze theorem, $ -1 \leq \sin \left(\frac{1}{3 \sqrt{x}}\right) \leq 1 $ $-\frac{|x|}{\sqrt{x^{2}+4 x^{2}+7}} \leq \frac{|x|}{\sqrt{x^{4}+4 x^{2}+7}} \sin \left(\frac{1}{\sqrt{3x}}\right) \leq \frac{|x|}{\sqrt{x^{4}+4 x^{2}+7}}$ $\operatorname{Now,}_{\operatorname{limit}_{x \rightarrow 0} \frac{-x}{\sqrt{x^{4}+4 x^{2}+7}}}=\ _{x \rightarrow 0} \frac{x}{\sqrt{x^{4}+4 x^{2}+7}}=0$ Hence answer is zero. Am I correct?","Find the limit of the following function as My approach , applying squeeze theorem, Hence answer is zero. Am I correct?","x \rightarrow 0 
\frac{|x|}{\sqrt{\left(x^{4}+4 x^{2}+7\right)}} \sin \left(\frac{1}{3 \sqrt{x}}\right)
 \lim _{x \rightarrow 0} \frac{|x|}{\sqrt{\left(x^{4}+4 x^{2}+77\right)}} \sin \left(\frac{1}{3 \sqrt{x}}\right) 
-1 \leq \sin \left(\frac{1}{3 \sqrt{x}}\right) \leq 1
 -\frac{|x|}{\sqrt{x^{2}+4 x^{2}+7}} \leq \frac{|x|}{\sqrt{x^{4}+4 x^{2}+7}} \sin \left(\frac{1}{\sqrt{3x}}\right) \leq \frac{|x|}{\sqrt{x^{4}+4 x^{2}+7}} \operatorname{Now,}_{\operatorname{limit}_{x \rightarrow 0} \frac{-x}{\sqrt{x^{4}+4 x^{2}+7}}}=\ _{x \rightarrow 0} \frac{x}{\sqrt{x^{4}+4 x^{2}+7}}=0",['limits']
34,A question about Terence Tao's definition of limiting values of functions: am I grasping it correctly?,A question about Terence Tao's definition of limiting values of functions: am I grasping it correctly?,,"In the book ""Analysis I"", Terence Tao provides the following definition: Let $X$ be a subset of $\textbf{R}$ , let $f:X\to\textbf{R}$ be a function, let $E$ be a subset of $X$ , $x_{0}$ be an adherent point of $E$ , and let $L$ be a real number. We say that $f$ converges to $L$ at $x_{0}$ in $E$ , and write $\lim_{x\to x_{0};x\in E}f(x) = L$ , iff for every $\varepsilon > 0$ , there corresponds a $\delta > 0$ such that for every $x\in E$ one has that \begin{align*} |x - x_{0}| < \delta \Rightarrow |f(x) - L| < \varepsilon \end{align*} Similarly, in the book ''Analysis II'' the same author provides the following definition: Let $(X,d_{X})$ and $(Y,d_{Y})$ be metric spaces, let $E$ be a subset of $X$ , and let $f:X\to Y$ be a function. If $x_{0}\in X$ is an adherent point of $E$ , and $L\in Y$ , we say that $f(x)$ converges to $L$ in $Y$ as $x$ converges to $x_{0}$ in $E$ , or write $\lim_{x\to x_{0};x\in E}f(x) = L$ , if for every $\varepsilon > 0$ there exists $\delta > 0$ such that $d_{Y}(f(x),L) < \varepsilon$ for all $x\in E$ such that $d_{X}(x,x_{0}) < \delta$ . My question about these definitions is the following: what is the role of the set $E$ ? As far as I have understood, the set $E$ tells us how we are approaching $x_{0}$ . Let us consider an example. Let $X = \textbf{R}\backslash\{0\}\subseteq\textbf{R}$ and $f:X\to\textbf{R}$ be given by $f(x) = x/|x|$ . Thus if we consider $E = (0,\infty)$ , $x_{0} = 0\in\textbf{R}$ is an adherent point of $E$ . Hence we have that \begin{align*} \lim_{x\to x_{0};x\in E}f(x) = \lim_{x\to 0;x\in(0,\infty)}\frac{x}{|x|} = \lim_{x\to 0;x\in(0,\infty)} 1 = 1 \end{align*} Similarly, if we choose $E = (-\infty,0)$ , $x_{0} = 0\in\textbf{R}$ is an adherent point of $E$ . Thus it results that \begin{align*} \lim_{x\to x_{0};x\in E}f(x) = \lim_{x\to 0;x\in (-\infty,0)}\frac{x}{|x|} = \lim_{x\to 0;x\in(-\infty,0)} -1 = -1 \end{align*} At last, if we choose $E = X$ , the limit $\lim_{x\to 0;x\in E}f(x)$ is undefined. But I am little bit unsure about this. From the context, I assume that we are immersed in the metric space $(\textbf{R},|\cdot|)\supseteq X\supseteq E$ . Here it is another example which may be enlightening. Let $f:X\to\textbf{R}$ , where $X = \textbf{R}\backslash\{1\}\subseteq\textbf{R}$ , which is defined by \begin{align*} f(x) = \frac{x^{2} - 1}{|x-1|} \end{align*} If we choose $E = (1,+\infty)$ , then $1$ is an adherent point of $E$ . Thus we have that \begin{align*} \lim_{x\to 1;x\in E}f(x) = \lim_{x\to 1;x\in (1,+\infty)}\frac{x^{2}-1}{|x-1|} = \lim_{x\to 1;x\in (1,+\infty)}\frac{x^{2}-1}{x-1} = \lim_{x\to 1;x\in (1,+\infty)} x+1 = 2 \end{align*} Similarly, if we choose $E = (-\infty,1)$ , $1$ stills a adherent point of $E$ . Thus we have \begin{align*} \lim_{x\to 1;x\in E}f(x) = \lim_{x\to 1;x\in (-\infty,1)}\frac{x^{2}-1}{|x-1|} = \lim_{x\to 1;x\in (-\infty,1)}-\frac{x^{2}-1}{x-1} = \lim_{x\to 1;x\in (-\infty,1)} -x-1 = -2 \end{align*} Finally, if we choose $E = X = \textbf{R}\backslash\{1\}$ , the limit $\lim_{x\to 1;x\in E}f(x)$ is not defined. The same reasoning seems to apply to more general settings where we consider metric spaces other than the real line. Am I interpreting it correctly? If not, how should I grasp this concept? I am new to this. So any comment or contribution is appreciated. EDIT Here it is another example from the textbook which may help us understand it properly. Consider $f:\textbf{R}\to\textbf{R}$ to be the function defined by setting $f(x) = 1$ when $x = 0$ and $f(x) = 0$ when $x\neq 0$ . Thus if we choose $E = \textbf{R}\backslash\{0\}$ one has that $\lim_{x\to 0;x\in E}f(x) = 0$ . On the other hand, if $E = \textbf{R}$ , the limit $\lim_{x\to 0;x\in E}f(x)$ is not defined. After this example, he provides the following argument: Some authors only define the limit $\lim_{x\to x_{0};x\in E}f(x)$ when $E$ does not contain $x_{0}$ (so that $x_{0}$ is now a limit point of $E$ rather than an adherent point), or would use $\lim_{x\to x_{0};x\in E}f(x)$ to denote what we would call $\lim_{x\in x_{0};x\in E\backslash\{x_{0}\}}f(x)$ , but we have chosen a slightly more general notation, which allows the possibility that $E$ contains $x_{0}$ .","In the book ""Analysis I"", Terence Tao provides the following definition: Let be a subset of , let be a function, let be a subset of , be an adherent point of , and let be a real number. We say that converges to at in , and write , iff for every , there corresponds a such that for every one has that Similarly, in the book ''Analysis II'' the same author provides the following definition: Let and be metric spaces, let be a subset of , and let be a function. If is an adherent point of , and , we say that converges to in as converges to in , or write , if for every there exists such that for all such that . My question about these definitions is the following: what is the role of the set ? As far as I have understood, the set tells us how we are approaching . Let us consider an example. Let and be given by . Thus if we consider , is an adherent point of . Hence we have that Similarly, if we choose , is an adherent point of . Thus it results that At last, if we choose , the limit is undefined. But I am little bit unsure about this. From the context, I assume that we are immersed in the metric space . Here it is another example which may be enlightening. Let , where , which is defined by If we choose , then is an adherent point of . Thus we have that Similarly, if we choose , stills a adherent point of . Thus we have Finally, if we choose , the limit is not defined. The same reasoning seems to apply to more general settings where we consider metric spaces other than the real line. Am I interpreting it correctly? If not, how should I grasp this concept? I am new to this. So any comment or contribution is appreciated. EDIT Here it is another example from the textbook which may help us understand it properly. Consider to be the function defined by setting when and when . Thus if we choose one has that . On the other hand, if , the limit is not defined. After this example, he provides the following argument: Some authors only define the limit when does not contain (so that is now a limit point of rather than an adherent point), or would use to denote what we would call , but we have chosen a slightly more general notation, which allows the possibility that contains .","X \textbf{R} f:X\to\textbf{R} E X x_{0} E L f L x_{0} E \lim_{x\to x_{0};x\in E}f(x) = L \varepsilon > 0 \delta > 0 x\in E \begin{align*}
|x - x_{0}| < \delta \Rightarrow |f(x) - L| < \varepsilon
\end{align*} (X,d_{X}) (Y,d_{Y}) E X f:X\to Y x_{0}\in X E L\in Y f(x) L Y x x_{0} E \lim_{x\to x_{0};x\in E}f(x) = L \varepsilon > 0 \delta > 0 d_{Y}(f(x),L) < \varepsilon x\in E d_{X}(x,x_{0}) < \delta E E x_{0} X = \textbf{R}\backslash\{0\}\subseteq\textbf{R} f:X\to\textbf{R} f(x) = x/|x| E = (0,\infty) x_{0} = 0\in\textbf{R} E \begin{align*}
\lim_{x\to x_{0};x\in E}f(x) = \lim_{x\to 0;x\in(0,\infty)}\frac{x}{|x|} = \lim_{x\to 0;x\in(0,\infty)} 1 = 1
\end{align*} E = (-\infty,0) x_{0} = 0\in\textbf{R} E \begin{align*}
\lim_{x\to x_{0};x\in E}f(x) = \lim_{x\to 0;x\in (-\infty,0)}\frac{x}{|x|} = \lim_{x\to 0;x\in(-\infty,0)} -1 = -1
\end{align*} E = X \lim_{x\to 0;x\in E}f(x) (\textbf{R},|\cdot|)\supseteq X\supseteq E f:X\to\textbf{R} X = \textbf{R}\backslash\{1\}\subseteq\textbf{R} \begin{align*}
f(x) = \frac{x^{2} - 1}{|x-1|}
\end{align*} E = (1,+\infty) 1 E \begin{align*}
\lim_{x\to 1;x\in E}f(x) = \lim_{x\to 1;x\in (1,+\infty)}\frac{x^{2}-1}{|x-1|} = \lim_{x\to 1;x\in (1,+\infty)}\frac{x^{2}-1}{x-1} = \lim_{x\to 1;x\in (1,+\infty)} x+1 = 2
\end{align*} E = (-\infty,1) 1 E \begin{align*}
\lim_{x\to 1;x\in E}f(x) = \lim_{x\to 1;x\in (-\infty,1)}\frac{x^{2}-1}{|x-1|} = \lim_{x\to 1;x\in (-\infty,1)}-\frac{x^{2}-1}{x-1} = \lim_{x\to 1;x\in (-\infty,1)} -x-1 = -2
\end{align*} E = X = \textbf{R}\backslash\{1\} \lim_{x\to 1;x\in E}f(x) f:\textbf{R}\to\textbf{R} f(x) = 1 x = 0 f(x) = 0 x\neq 0 E = \textbf{R}\backslash\{0\} \lim_{x\to 0;x\in E}f(x) = 0 E = \textbf{R} \lim_{x\to 0;x\in E}f(x) \lim_{x\to x_{0};x\in E}f(x) E x_{0} x_{0} E \lim_{x\to x_{0};x\in E}f(x) \lim_{x\in x_{0};x\in E\backslash\{x_{0}\}}f(x) E x_{0}","['real-analysis', 'limits', 'metric-spaces', 'self-learning']"
35,Question about the proof of Tietze extenstion theorem,Question about the proof of Tietze extenstion theorem,,"In the book ""An Introduction to Mathematical Analysis for Economic Theory and Econometrics"" by Dean Corbae, I saw the proof for Tietze extension theorem, which states that ""If $M$ is a metric space , $F$ is a closed set in $M$ and $g \in C_b(F)$ , then there is a continuous extension $G \in C_b(M)$ such that $\|G \|_{\infty}=\|g \|_{\infty}.$ "" They consider the following extension: $$G(x)=\begin{cases} g(x) &\text{ if } x \in F \\ \sup_{t \in F}\dfrac{g(t)}{(1+d(t,x)^2)^{1/d(x,F)}}& \text{ if } x\notin F \end{cases}$$ For proving $G(x)$ is continuous at $x \in \partial F$ , they consider a sequence $x_n \subset F^c$ which converges to $x$ . For $t \neq x$ , $\dfrac{1}{(1+d(t,x_n)^2)^{1/d(x_n,F)}} \overset{n \to \infty}{\longrightarrow} 0$ and $\dfrac{1}{(1+d(x,x_n)^2)^{1/d(x_n,F)}} \overset{n \to \infty}{\longrightarrow} 1$ . They mention to use the property: $\lim_{s \to 0}f(s)=1$ , where $f(s)$ is strictly decreasing function $$f:(0,+\infty) \to (0,1): s \mapsto f(s)=\dfrac{1}{(1+s^2)^{1/s}}$$ Then $G(x_n)\overset{n \to \infty}{\longrightarrow} G(x)$ . My question is that how they use above property of $f(s)$ to prove $a_n = \dfrac{1}{(1+d(x,x_n)^2)^{1/d(x_n,F)}} \overset{n \to \infty}{\longrightarrow} 1$ . I know that both two terms $d(x,x_n)$ and $d(x_n,F)$ converge to $0$ but they are not the same "" $s$ "" as in function $f(s)$ . Any other way to prove continuity of $G(x)$ is good too. Thanks in advance.","In the book ""An Introduction to Mathematical Analysis for Economic Theory and Econometrics"" by Dean Corbae, I saw the proof for Tietze extension theorem, which states that ""If is a metric space , is a closed set in and , then there is a continuous extension such that "" They consider the following extension: For proving is continuous at , they consider a sequence which converges to . For , and . They mention to use the property: , where is strictly decreasing function Then . My question is that how they use above property of to prove . I know that both two terms and converge to but they are not the same "" "" as in function . Any other way to prove continuity of is good too. Thanks in advance.","M F M g \in C_b(F) G \in C_b(M) \|G \|_{\infty}=\|g \|_{\infty}. G(x)=\begin{cases} g(x) &\text{ if } x \in F \\ \sup_{t \in F}\dfrac{g(t)}{(1+d(t,x)^2)^{1/d(x,F)}}& \text{ if } x\notin F \end{cases} G(x) x \in \partial F x_n \subset F^c x t \neq x \dfrac{1}{(1+d(t,x_n)^2)^{1/d(x_n,F)}} \overset{n \to \infty}{\longrightarrow} 0 \dfrac{1}{(1+d(x,x_n)^2)^{1/d(x_n,F)}} \overset{n \to \infty}{\longrightarrow} 1 \lim_{s \to 0}f(s)=1 f(s) f:(0,+\infty) \to (0,1): s \mapsto f(s)=\dfrac{1}{(1+s^2)^{1/s}} G(x_n)\overset{n \to \infty}{\longrightarrow} G(x) f(s) a_n = \dfrac{1}{(1+d(x,x_n)^2)^{1/d(x_n,F)}} \overset{n \to \infty}{\longrightarrow} 1 d(x,x_n) d(x_n,F) 0 s f(s) G(x)","['limits', 'analysis', 'continuity', 'metric-spaces', 'proof-explanation']"
36,Randomize Fibonacci sequence,Randomize Fibonacci sequence,,"We all know that if $a_n$ if the $n^{th}$ Fibonacci number we can say: $$\lim_{n\rightarrow\infty}\frac{a_{n+1}}{a_n}=φ$$ Now let's build another sequence like that: $b_1=b_2=1$ and $b_{n+1}=b_n+b_{n-1}$ or $b_{n+1}\in [b_n,b_{n-1}]$ randomly with chances of $75\%,25\%$ with proportion. For the second option it's random point in $[b_n,b_{n-1}]$ What will be: $$\lim_{n\rightarrow\infty}\frac{b_{n+1}}{b_n}?$$",We all know that if if the Fibonacci number we can say: Now let's build another sequence like that: and or randomly with chances of with proportion. For the second option it's random point in What will be:,"a_n n^{th} \lim_{n\rightarrow\infty}\frac{a_{n+1}}{a_n}=φ b_1=b_2=1 b_{n+1}=b_n+b_{n-1} b_{n+1}\in [b_n,b_{n-1}] 75\%,25\% [b_n,b_{n-1}] \lim_{n\rightarrow\infty}\frac{b_{n+1}}{b_n}?","['sequences-and-series', 'limits', 'summation', 'fibonacci-numbers']"
37,Why can we define $0\log 0=0$ and $0\log 0/0=0$ if this is not true for every path leading to $0^+$?,Why can we define  and  if this is not true for every path leading to ?,0\log 0=0 0\log 0/0=0 0^+,"Very often in information theory literature we see these two definitions (e.g., Aczel, J. and Daroczy, Z., 1975; Ebanks, Sahoo and Sanders, 1997) $$ 0 \log 0 = 0,$$ $$ 0 \log \frac{0}{0} = 0.$$ I know that the limit $\lim_{p\rightarrow0} p\log\frac{p}{p} = 0$ , however recently I became aware that the limit does not exist for all paths leading to $0^+$ : https://cstheory.stackexchange.com/questions/46406/when-is-it-necessary-to-define-0-log-0-0-and-0-log-0-0-0 e.g., $$\lim_{p\rightarrow0} p\log\frac{p}{e^{-\frac{1}{p}}} = 1.$$ In this case, why are we allowed to make this definition, and what are the consequences to the proofs using this definition?","Very often in information theory literature we see these two definitions (e.g., Aczel, J. and Daroczy, Z., 1975; Ebanks, Sahoo and Sanders, 1997) I know that the limit , however recently I became aware that the limit does not exist for all paths leading to : https://cstheory.stackexchange.com/questions/46406/when-is-it-necessary-to-define-0-log-0-0-and-0-log-0-0-0 e.g., In this case, why are we allowed to make this definition, and what are the consequences to the proofs using this definition?"," 0 \log 0 = 0,  0 \log \frac{0}{0} = 0. \lim_{p\rightarrow0} p\log\frac{p}{p} = 0 0^+ \lim_{p\rightarrow0} p\log\frac{p}{e^{-\frac{1}{p}}} = 1.","['limits', 'information-theory']"
38,A sequence $(a_n)_{n\ge 1}$ such that $a_1>0$ and $a_{n+1}=a_n-\ln(1+a_n)$,A sequence  such that  and,(a_n)_{n\ge 1} a_1>0 a_{n+1}=a_n-\ln(1+a_n),"Let $(a_n)_{n\ge 1}$ be a sequence  such that $a_1>0$ and $$a_{n+1}=a_n-\ln(1+a_n)$$ a) Prove that $a_{n+1}<\frac{a_n^2}{2}, \forall n\in \mathbb{N}$ and $\lim\limits_{n\to \infty} (n^{2019}a_n)=0$ . b) If $a_1<2$ , prove that $a_{12} \in (0,10^{-300})$ . It is easy to see that $a_n >0$ , $\forall n\in \mathbb{N}$ . I considered the function $f:(0,\infty)\to \mathbb{R}$ , $f(x)=x-\ln(1+x)-\frac{x^2}{2}$ and I could easily prove that this function is strictly decreasing, so it follows that $$x-\ln(1+x)<\frac{x^2}{2}, \forall x>0 \tag{*}$$ Now from $(*)$ we get that $a_{n+1}<\frac{a_n^2}{2}, \forall n\in \mathbb{N}$ . I couldn't make much further progress. I could show that $\lim\limits_{n\to \infty}a_n=0$ , but then I got stuck.","Let be a sequence  such that and a) Prove that and . b) If , prove that . It is easy to see that , . I considered the function , and I could easily prove that this function is strictly decreasing, so it follows that Now from we get that . I couldn't make much further progress. I could show that , but then I got stuck.","(a_n)_{n\ge 1} a_1>0 a_{n+1}=a_n-\ln(1+a_n) a_{n+1}<\frac{a_n^2}{2}, \forall n\in \mathbb{N} \lim\limits_{n\to \infty} (n^{2019}a_n)=0 a_1<2 a_{12} \in (0,10^{-300}) a_n >0 \forall n\in \mathbb{N} f:(0,\infty)\to \mathbb{R} f(x)=x-\ln(1+x)-\frac{x^2}{2} x-\ln(1+x)<\frac{x^2}{2}, \forall x>0 \tag{*} (*) a_{n+1}<\frac{a_n^2}{2}, \forall n\in \mathbb{N} \lim\limits_{n\to \infty}a_n=0","['real-analysis', 'sequences-and-series', 'limits']"
39,If $f_n\to f$ uniformly and $\lim_{x \to \infty}f_n(x)=0$ for all $n$. Then $\lim_{x \to \infty}f(x)=0$,If  uniformly and  for all . Then,f_n\to f \lim_{x \to \infty}f_n(x)=0 n \lim_{x \to \infty}f(x)=0,"If $f_n\to f$ uniformly and $\lim_{x \to \infty}f_n(x)=0$ for all $n$ . Then $\lim_{x \to \infty}f(x)=0$ . Is this statement true? My try: Pick $\epsilon>0$ then $\exists N$ s.t $|f_n-f|<\epsilon$ $\forall x$ . Now it is tempting to simply take the limit of both sides where $x$ goes to infinity but i am not sure if that is justified. Instead, we know there exists $M$ s.t if $M<x$ then $|f_N(x)|<\epsilon$ Thus by reverse triangle inequality and large enough $x$ we have $|\epsilon-f(x)|<\epsilon$ Hence $|f(x)|<2\epsilon$ Thus $\lim_{x \to \infty}|f(x)|=0$ and so $\lim_{x \to \infty}f(x)=0$ is this correct?","If uniformly and for all . Then . Is this statement true? My try: Pick then s.t . Now it is tempting to simply take the limit of both sides where goes to infinity but i am not sure if that is justified. Instead, we know there exists s.t if then Thus by reverse triangle inequality and large enough we have Hence Thus and so is this correct?",f_n\to f \lim_{x \to \infty}f_n(x)=0 n \lim_{x \to \infty}f(x)=0 \epsilon>0 \exists N |f_n-f|<\epsilon \forall x x M M<x |f_N(x)|<\epsilon x |\epsilon-f(x)|<\epsilon |f(x)|<2\epsilon \lim_{x \to \infty}|f(x)|=0 \lim_{x \to \infty}f(x)=0,"['real-analysis', 'limits', 'uniform-convergence']"
40,Finding $\lim_{n \to \infty} \left( 1 + 2\int_0^1 \frac{x^n}{x+1} dx \right)^n$,Finding,\lim_{n \to \infty} \left( 1 + 2\int_0^1 \frac{x^n}{x+1} dx \right)^n,"I have to evaluate $$\lim_{n \to \infty} \left( 1 + 2\int_0^1 \frac{x^n}{x+1} dx \right)^n. $$ My progress: Since $x \in (0, 1)$ we can use the series expansion of $\frac{1}{1+x} = 1-x+x^2-x^3+...$ Evaluating that integral in the parantheses (which I shall call $I_n$ ) gives $$I_n = \sum_{k=1}^\infty \frac{(-1)^{k+1}}{n+k} = (-1)^n(\log{2} - A_n) $$ where $A_n$ is the nth partial sum of the alternating harmonic series, $A_n = \sum_{k=1}^n \frac{(-1)^{k+1}}{k}.$ Since $\log{2} - A_n$ goes to 0, it's enough to compute $$2 \lim_{n \to \infty} n(-1)^n(\log{2} - A_n) $$ This is where I got stuck. Any ideas?","I have to evaluate My progress: Since we can use the series expansion of Evaluating that integral in the parantheses (which I shall call ) gives where is the nth partial sum of the alternating harmonic series, Since goes to 0, it's enough to compute This is where I got stuck. Any ideas?","\lim_{n \to \infty} \left( 1 + 2\int_0^1 \frac{x^n}{x+1} dx \right)^n.  x \in (0, 1) \frac{1}{1+x} = 1-x+x^2-x^3+... I_n I_n = \sum_{k=1}^\infty \frac{(-1)^{k+1}}{n+k} = (-1)^n(\log{2} - A_n)  A_n A_n = \sum_{k=1}^n \frac{(-1)^{k+1}}{k}. \log{2} - A_n 2 \lim_{n \to \infty} n(-1)^n(\log{2} - A_n) ","['sequences-and-series', 'limits', 'power-series']"
41,Solution to second order partial differential equation,Solution to second order partial differential equation,,"Suppose $Q$ is the solution to $Q_{tt}-Q_{xx} =0$ . Let $\bar{Q}$ be the solution to $Q_{tt}-Q_{xx}+k Q_t=0$ Is there any known relation between these two solution, given that in the limit of $k$ going to zero, both solution should approach each other?","Suppose is the solution to . Let be the solution to Is there any known relation between these two solution, given that in the limit of going to zero, both solution should approach each other?",Q Q_{tt}-Q_{xx} =0 \bar{Q} Q_{tt}-Q_{xx}+k Q_t=0 k,"['limits', 'partial-differential-equations', 'linear-pde']"
42,"Convergence of $a_{n+2} = \sqrt{7-\sqrt{7+a_n}}$, with $a_1=\sqrt{7}$, $a_2 = \sqrt{7-\sqrt{7}}$?","Convergence of , with , ?",a_{n+2} = \sqrt{7-\sqrt{7+a_n}} a_1=\sqrt{7} a_2 = \sqrt{7-\sqrt{7}},"How to prove the convergence of the real sequence $\{a_n\}$ ,  which is defined by $a_{n+2} = \sqrt{7-\sqrt{7+a_n}}$ ,  with $a_1=\sqrt{7}$ , $a_2 = \sqrt{7-\sqrt{7}}$ ?  Furthermore, how to verifty that 2 is the limit?","How to prove the convergence of the real sequence ,  which is defined by ,  with , ?  Furthermore, how to verifty that 2 is the limit?",\{a_n\} a_{n+2} = \sqrt{7-\sqrt{7+a_n}} a_1=\sqrt{7} a_2 = \sqrt{7-\sqrt{7}},"['real-analysis', 'sequences-and-series', 'limits', 'recurrence-relations']"
43,The limit of the ratio of polygamma functions,The limit of the ratio of polygamma functions,,"I want to calculate this quantity: $$\lim_{x \rightarrow \infty}\frac{\Psi_1 (x)}{\Psi_1 (x + y)}$$ where $$\Psi_1 (x)=\frac{d^2}{dx^2}\log \Gamma (x)=\sum_{k=0}^{\infty}\frac{1}{(x+k)^2}. $$ I guess it is $1$ , but I am not sure about my proof. My proof is following: Let $\epsilon > 0$ be given. Since $\Psi_1 (x)$ is convergent on $(0, \infty)$ and decreasing, for any $x, y >0$ there exist $K=K(\epsilon) < \infty$ such that $\sum_{k=K+1}^{\infty} \frac{1}{(x + y + k)^2} < \sum_{k=K+1}^{\infty} \frac{1}{(x + k)^2} \leq \epsilon$ . Then, we have \begin{align*}        &\frac{\Psi_1 (x)}{\Psi_1 (x + y)} \leq \Bigg( \sum_{k=0}^{K} \frac{1}{(x + k)^2} + \sum_{k=K+1}^{\infty} \frac{1}{(x + k)^2} \Bigg) \Bigg/ \Bigg( \sum_{k=0}^{K} \frac{1}{(x + y + k)^2} \Bigg)\\        %&\leq \Bigg( \sum_{k=0}^{K} \frac{1}{(x + k)^2} - \frac{1}{(x + y + k)^2}  \Bigg) \Bigg/ \Bigg( \sum_{k=0}^{K} \frac{1}{(x + y + k)^2} \Bigg) + 2\epsilon\\        &\leq \Bigg( \frac{K+1}{x^2} + \sum_{k=K+1}^{\infty} \frac{1}{(x + k)^2} \Bigg) \Bigg/ \Bigg( \sum_{k=0}^{K} \frac{1}{(x + y + k)^2} \Bigg)\\        &= \Bigg( \frac{K+1}{x^2} \Bigg) \Bigg/ \Bigg( \sum_{k=0}^{K} \frac{1}{(x + y + k)^2} \Bigg)        + \Bigg( \sum_{k=K+1}^{\infty} \frac{1}{(x + k)^2} \Bigg) \Bigg/ \Bigg( \sum_{k=0}^{K} \frac{1}{(x + y + k)^2} \Bigg)\\        & \leq \Bigg( \frac{K+1}{x^2} \Bigg) \Bigg/ \Bigg( \frac{K+1}{(x + y + K)^2} \Bigg)        + \Bigg( \sum_{k=K+1}^{\infty} \frac{1}{(x + k)^2} \Bigg) \Bigg/ \Bigg( \sum_{k=0}^{K} \frac{1}{(x + y + k)^2} \Bigg)\\        & = \frac{(x + y + K)^2}{x^2}        + \Bigg( \sum_{k=K+1}^{\infty} \frac{1}{(x + k)^2} \Bigg) \Bigg/ \Bigg( \sum_{k=0}^{K} \frac{1}{(x + y + k)^2} \Bigg). \end{align*} In the last line, the first term goes to $1$ as $x \longrightarrow \infty$ for any fixed $K$ . For the second term, since we can choose arbitrarily large $K$ and $\Psi_1$ is convergent, it goes to $0$ as $K \longrightarrow \infty$ for any fixed $x>0$ . Thus, we have $\frac{\Psi_1 (x)}{\Psi_1 (x + y)} \leq 1$ as $x \longrightarrow \infty$ . On the other hand, since $\Psi_1(x)$ is decreasing in $x$ , \begin{equation*}     \frac{\Psi_1 (x)}{\Psi_1 (x + y)} \geq 1 \text { for any } x, y >0. \end{equation*} Thus, \begin{equation*}     \frac{\Psi_1 (x)}{\Psi_1 (x + y)} \longrightarrow 1  \text { as } x \longrightarrow \infty. \end{equation*} Am I correct? I think I am cheating somewhere. I can't convince myself.","I want to calculate this quantity: where I guess it is , but I am not sure about my proof. My proof is following: Let be given. Since is convergent on and decreasing, for any there exist such that . Then, we have In the last line, the first term goes to as for any fixed . For the second term, since we can choose arbitrarily large and is convergent, it goes to as for any fixed . Thus, we have as . On the other hand, since is decreasing in , Thus, Am I correct? I think I am cheating somewhere. I can't convince myself.","\lim_{x \rightarrow \infty}\frac{\Psi_1 (x)}{\Psi_1 (x + y)} \Psi_1 (x)=\frac{d^2}{dx^2}\log \Gamma (x)=\sum_{k=0}^{\infty}\frac{1}{(x+k)^2}.  1 \epsilon > 0 \Psi_1 (x) (0, \infty) x, y >0 K=K(\epsilon) < \infty \sum_{k=K+1}^{\infty} \frac{1}{(x + y + k)^2} < \sum_{k=K+1}^{\infty} \frac{1}{(x + k)^2} \leq \epsilon \begin{align*}
       &\frac{\Psi_1 (x)}{\Psi_1 (x + y)} \leq \Bigg( \sum_{k=0}^{K} \frac{1}{(x + k)^2} + \sum_{k=K+1}^{\infty} \frac{1}{(x + k)^2} \Bigg) \Bigg/ \Bigg( \sum_{k=0}^{K} \frac{1}{(x + y + k)^2} \Bigg)\\
       %&\leq \Bigg( \sum_{k=0}^{K} \frac{1}{(x + k)^2} - \frac{1}{(x + y + k)^2}  \Bigg) \Bigg/ \Bigg( \sum_{k=0}^{K} \frac{1}{(x + y + k)^2} \Bigg) + 2\epsilon\\
       &\leq \Bigg( \frac{K+1}{x^2} + \sum_{k=K+1}^{\infty} \frac{1}{(x + k)^2} \Bigg) \Bigg/ \Bigg( \sum_{k=0}^{K} \frac{1}{(x + y + k)^2} \Bigg)\\
       &= \Bigg( \frac{K+1}{x^2} \Bigg) \Bigg/ \Bigg( \sum_{k=0}^{K} \frac{1}{(x + y + k)^2} \Bigg)
       + \Bigg( \sum_{k=K+1}^{\infty} \frac{1}{(x + k)^2} \Bigg) \Bigg/ \Bigg( \sum_{k=0}^{K} \frac{1}{(x + y + k)^2} \Bigg)\\
       & \leq \Bigg( \frac{K+1}{x^2} \Bigg) \Bigg/ \Bigg( \frac{K+1}{(x + y + K)^2} \Bigg)
       + \Bigg( \sum_{k=K+1}^{\infty} \frac{1}{(x + k)^2} \Bigg) \Bigg/ \Bigg( \sum_{k=0}^{K} \frac{1}{(x + y + k)^2} \Bigg)\\
       & = \frac{(x + y + K)^2}{x^2}
       + \Bigg( \sum_{k=K+1}^{\infty} \frac{1}{(x + k)^2} \Bigg) \Bigg/ \Bigg( \sum_{k=0}^{K} \frac{1}{(x + y + k)^2} \Bigg).
\end{align*} 1 x \longrightarrow \infty K K \Psi_1 0 K \longrightarrow \infty x>0 \frac{\Psi_1 (x)}{\Psi_1 (x + y)} \leq 1 x \longrightarrow \infty \Psi_1(x) x \begin{equation*}
    \frac{\Psi_1 (x)}{\Psi_1 (x + y)} \geq 1 \text { for any } x, y >0.
\end{equation*} \begin{equation*}
    \frac{\Psi_1 (x)}{\Psi_1 (x + y)} \longrightarrow 1  \text { as } x \longrightarrow \infty.
\end{equation*}","['real-analysis', 'limits', 'analysis', 'power-series', 'polygamma']"
44,Problem in the properties of limit: $\lim\limits_{x \to\frac{\pi}{3}}\frac{\sin\left(x-\frac{\pi}{3}\right)}{1-2\cos\left(x\right)}$,Problem in the properties of limit:,\lim\limits_{x \to\frac{\pi}{3}}\frac{\sin\left(x-\frac{\pi}{3}\right)}{1-2\cos\left(x\right)},"$$\lim\limits_{x \to\frac{\pi}{3}}\frac{\sin\left(x-\frac{\pi}{3}\right)}{1-2\cos\left(x\right)}$$ I used the following property: if $$\lim\limits_{\large x \to\frac{\pi}{3}}f(x)=L$$ then $$\lim\limits_{x \to\frac{\pi}{3}}\frac{1}{f\left(x\right)}=\frac{1}{L}$$ where $L$ is a real number and nonzero,hence we have: $$\lim\limits_{\large x \to\frac{\pi}{3}}\frac{1-2\cos\left(x\right)}{\sin\left(x-\frac{\pi}{3}\right)}$$ substititute $x-\frac{\pi}{3}=u$ : $$\lim\limits_{\large u \to 0}\frac{1-2\cos\left(u+\frac{\pi}{3}\right)}{\sin\left(u\right)}$$ $$=\lim\limits_{\large u \to 0}\frac{1-\cos\left(u\right)+\sqrt{2}\sin\left(u\right)}{\sin\left(u\right)}=\lim\limits_{\large u \to 0}\frac{1-\cos\left(u\right)}{\sin\left(u\right)}+\sqrt{2}$$ $$=\lim\limits_{\large u \to 0}\frac{\sin\left(u\right)}{1+\cos\left(u\right)}+\sqrt{2}=\sqrt{2}$$ hence the main limit should be $\frac{1}{\sqrt{2}}$ which is wrong, but I don't know why, also is there any way to solve the problem without using Taylor series or L'hopital's rule?","I used the following property: if then where is a real number and nonzero,hence we have: substititute : hence the main limit should be which is wrong, but I don't know why, also is there any way to solve the problem without using Taylor series or L'hopital's rule?",\lim\limits_{x \to\frac{\pi}{3}}\frac{\sin\left(x-\frac{\pi}{3}\right)}{1-2\cos\left(x\right)} \lim\limits_{\large x \to\frac{\pi}{3}}f(x)=L \lim\limits_{x \to\frac{\pi}{3}}\frac{1}{f\left(x\right)}=\frac{1}{L} L \lim\limits_{\large x \to\frac{\pi}{3}}\frac{1-2\cos\left(x\right)}{\sin\left(x-\frac{\pi}{3}\right)} x-\frac{\pi}{3}=u \lim\limits_{\large u \to 0}\frac{1-2\cos\left(u+\frac{\pi}{3}\right)}{\sin\left(u\right)} =\lim\limits_{\large u \to 0}\frac{1-\cos\left(u\right)+\sqrt{2}\sin\left(u\right)}{\sin\left(u\right)}=\lim\limits_{\large u \to 0}\frac{1-\cos\left(u\right)}{\sin\left(u\right)}+\sqrt{2} =\lim\limits_{\large u \to 0}\frac{\sin\left(u\right)}{1+\cos\left(u\right)}+\sqrt{2}=\sqrt{2} \frac{1}{\sqrt{2}},"['limits', 'trigonometry', 'definition', 'limits-without-lhopital']"
45,Prove that $\lim\limits_{x\to0^+}\frac{f(x)}{f'(x)}=0$.,Prove that .,\lim\limits_{x\to0^+}\frac{f(x)}{f'(x)}=0,"Let $f:(0,\infty)\to\mathbb{R}$ be a twice differentiable function with $f''$ continuous and let $\lim\limits_{x\to0^+}f'(x)=-\infty$ and $\lim\limits_{x\to0^+}f''(x)=+\infty$ . Prove that: $$\lim_{x\to0^+}\frac{f(x)}{f'(x)}=0.$$ My problem is not a proof of this itself (e.g. using $\epsilon-\delta$ definition). I recently found this in an old high-school textbook where no mention of the ""traditional"" $\epsilon-\delta$ definition is made, so, is it possible to find a solution without it? What we can do is find some $a>0$ such that $f$ is strictly decreasing and $f'$ strictly increasing in $(0,a)$ which proves that $$\lim_{x\to0^+}f(x)=\ell$$ exists (either number or $+\infty$ ) and we can easily prove what we want in case $\ell\in\mathbb{R}$ . But that case $\ell=+\infty$ is one I cannot solve without proving some inequality of the form: $f(x)+\epsilon f'(x)<0,$ for $x\in(0,\delta)$ for some $\delta>0$ . But this is not supposed to be the solution in a high school textbook. So, does anyone have a more ""elementary"" solution or an appropriate rephrasing of a current one?","Let be a twice differentiable function with continuous and let and . Prove that: My problem is not a proof of this itself (e.g. using definition). I recently found this in an old high-school textbook where no mention of the ""traditional"" definition is made, so, is it possible to find a solution without it? What we can do is find some such that is strictly decreasing and strictly increasing in which proves that exists (either number or ) and we can easily prove what we want in case . But that case is one I cannot solve without proving some inequality of the form: for for some . But this is not supposed to be the solution in a high school textbook. So, does anyone have a more ""elementary"" solution or an appropriate rephrasing of a current one?","f:(0,\infty)\to\mathbb{R} f'' \lim\limits_{x\to0^+}f'(x)=-\infty \lim\limits_{x\to0^+}f''(x)=+\infty \lim_{x\to0^+}\frac{f(x)}{f'(x)}=0. \epsilon-\delta \epsilon-\delta a>0 f f' (0,a) \lim_{x\to0^+}f(x)=\ell +\infty \ell\in\mathbb{R} \ell=+\infty f(x)+\epsilon f'(x)<0, x\in(0,\delta) \delta>0","['calculus', 'limits']"
46,Finding $\lim_{x\rightarrow 0^+} \frac{x^{-x}-1}{x}$,Finding,\lim_{x\rightarrow 0^+} \frac{x^{-x}-1}{x},I'm trying to solve the limit $$\lim_{x\rightarrow 0^+} \frac{x^{-x}-1}{x}$$ I think we should use L'Hospital rule and the limit becomes $$\lim_{x\rightarrow 0^+} -x^{-x}(\log x + 1)=\lim_{x\rightarrow 0^+} \frac{\log x + 1}{-x^{x}}= +\infty$$ Is it right? I've tried to modify the form and not use L'Hospital's rule but without success.,I'm trying to solve the limit I think we should use L'Hospital rule and the limit becomes Is it right? I've tried to modify the form and not use L'Hospital's rule but without success.,\lim_{x\rightarrow 0^+} \frac{x^{-x}-1}{x} \lim_{x\rightarrow 0^+} -x^{-x}(\log x + 1)=\lim_{x\rightarrow 0^+} \frac{\log x + 1}{-x^{x}}= +\infty,"['real-analysis', 'limits', 'exponential-function', 'limits-without-lhopital']"
47,Proving the well-definedness of $df$: How to place the limit inside the argument of $\psi \circ f \circ \phi^{-1}$?,Proving the well-definedness of : How to place the limit inside the argument of ?,df \psi \circ f \circ \phi^{-1},"In the book of Chillingworth, the author defines the tangent space of a point $p$ in the smooth manifold $M$ as the set of all conjugacy classes of smooth paths with $\alpha (o) = p$ s.t $\alpha \sim \beta$ iff $$\lim_{t\to 0} \frac{\phi \circ \alpha(t) - \phi \circ \beta (t)}{t } = 0,$$ where $\phi$ is a local coordinate chart around $p\in M$ . Now, given a smooth map from $f : M \to N$ , I'm trying to show that $df: T_p M \to T_{f(p)} N$ given by $[\alpha] \mapsto [f\circ \alpha]$ is a well-defined map. However, to show that, I need to show that $$\lim_{t\to 0} \frac{\psi \circ f \circ \alpha(t) - \psi \circ f \circ \beta (t)}{t } = 0,$$ where $\psi$ is a local coordinate chart around $f(p)$ and $[\alpha] = [\beta]$ . In $\mathbb{R}^n$ , I'm aware of this property, but even if I modify the limit as $$\lim_{t\to 0} \frac{\psi \circ f \circ \phi^{-1} \circ [\phi \circ \alpha(t) - \phi \circ \beta (t)]}{t } = 0,$$ how to put the factor $t$ in the denominator inside the argument of $\psi \circ f \circ \phi^{-1}$ ?","In the book of Chillingworth, the author defines the tangent space of a point in the smooth manifold as the set of all conjugacy classes of smooth paths with s.t iff where is a local coordinate chart around . Now, given a smooth map from , I'm trying to show that given by is a well-defined map. However, to show that, I need to show that where is a local coordinate chart around and . In , I'm aware of this property, but even if I modify the limit as how to put the factor in the denominator inside the argument of ?","p M \alpha (o) = p \alpha \sim \beta \lim_{t\to 0} \frac{\phi \circ \alpha(t) - \phi \circ \beta (t)}{t } = 0, \phi p\in M f : M \to N df: T_p M \to T_{f(p)} N [\alpha] \mapsto [f\circ \alpha] \lim_{t\to 0} \frac{\psi \circ f \circ \alpha(t) - \psi \circ f \circ \beta (t)}{t } = 0, \psi f(p) [\alpha] = [\beta] \mathbb{R}^n \lim_{t\to 0} \frac{\psi \circ f \circ \phi^{-1} \circ [\phi \circ \alpha(t) - \phi \circ \beta (t)]}{t } = 0, t \psi \circ f \circ \phi^{-1}","['real-analysis', 'limits', 'smooth-manifolds', 'differential', 'tangent-spaces']"
48,"$\{x_n\}$ is a bounded above sequence such that $x_{n+1} - x_n \ge a_n$, where $\sum a_k$ converges. Prove $x_n$ converges.","is a bounded above sequence such that , where  converges. Prove  converges.",\{x_n\} x_{n+1} - x_n \ge a_n \sum a_k x_n,"$\{x_n\}$ is a bounded above sequence satisfying the following property: $$ x_{n+1} - x_n \ge \alpha_n\tag1 $$ where $\alpha_n$ is such that $$ \exists \lim_{n\to\infty} \sum_{k=1}^n \alpha_k $$ Prove $\{x_n\}$ converges. I'm trying to generalize the idea from this question . Below are some thoughts. First denote: $$ S_n = \sum_{k=1}^n \alpha_k $$ Since $S_n$ is convergent then it must be bounded both below and above. Let: $$ y_n = x_n - S_{n-1} $$ Since $x_n$ is bounded above and $-S_n$ is also bounded above (by convergence of $S_n$ ), then it must follow that $y_n$ is also bounded above: $$ \exists M\in\Bbb R: y_n \le M, \forall n\in\Bbb N \tag2 $$ Rewrite $(1)$ as: $$ x_{n+1} \ge x_n + \alpha_n $$ Now subtract $S_n$ from both sides: $$ \underbrace{x_{n+1} - S_n}_{y_{n+1}} \ge x_n - S_n + \alpha_n = \underbrace{x_n - S_{n-1}}_{y_n} $$ That means $y_n$ is monotonically increasing. By $(2)$ we know $y_n$ is bounded. Finally by monotone convergence theorem: $$ \exists \lim_{n\to\infty}y_n \implies \exists\lim_{n\to\infty}(x_n - S_{n-1}) $$ Which in terms means that $x_n$ is also convergent. I would like to ask for a verification of the proof above or/and point to mistakes in case of any. Thank you!","is a bounded above sequence satisfying the following property: where is such that Prove converges. I'm trying to generalize the idea from this question . Below are some thoughts. First denote: Since is convergent then it must be bounded both below and above. Let: Since is bounded above and is also bounded above (by convergence of ), then it must follow that is also bounded above: Rewrite as: Now subtract from both sides: That means is monotonically increasing. By we know is bounded. Finally by monotone convergence theorem: Which in terms means that is also convergent. I would like to ask for a verification of the proof above or/and point to mistakes in case of any. Thank you!","\{x_n\} 
x_{n+1} - x_n \ge \alpha_n\tag1
 \alpha_n 
\exists \lim_{n\to\infty} \sum_{k=1}^n \alpha_k
 \{x_n\} 
S_n = \sum_{k=1}^n \alpha_k
 S_n 
y_n = x_n - S_{n-1}
 x_n -S_n S_n y_n 
\exists M\in\Bbb R: y_n \le M, \forall n\in\Bbb N \tag2
 (1) 
x_{n+1} \ge x_n + \alpha_n
 S_n 
\underbrace{x_{n+1} - S_n}_{y_{n+1}} \ge x_n - S_n + \alpha_n = \underbrace{x_n - S_{n-1}}_{y_n}
 y_n (2) y_n 
\exists \lim_{n\to\infty}y_n \implies \exists\lim_{n\to\infty}(x_n - S_{n-1})
 x_n","['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'proof-verification']"
49,limit of the sequence $(n+1)^\alpha - n^\alpha$,limit of the sequence,(n+1)^\alpha - n^\alpha,"Let $u_n$ be defined as : $u_n = (n+1)^{\alpha} - n^\alpha$ .   Now I would like to compute $\lim_{n \to \infty} u_n$ (this limit depends on the choice of $\alpha$ ). Here is what I found so far : If $\alpha \in [0,1[$ then $\lim_{n \to \infty} u_n = 0$ . If $\alpha = 1$ then $\lim_{n \to \infty} u_n = 1$ . If $\alpha \geq 2$ , $\lim_{n \to \infty} u_n = +\infty$ . Now for $\alpha = 1$ and $\alpha \geq 2$ it's really easy to get the result. For $\alpha \in [0,1)$ I found it hard and here is what I do : we use the mean value theorem to say that : $$\mid (n+1)^\alpha - n^\alpha \mid \leq \alpha n^{\alpha-1} \to 0$$ And we get the desired result. Now my question is : 1- Is it possible to get the limit when $\alpha\in [0,1)$ with an other technique (without using the mean value theorem) ? 2- How to get the limit when $\alpha \in (1, 2)$ ? Thank you !","Let be defined as : .   Now I would like to compute (this limit depends on the choice of ). Here is what I found so far : If then . If then . If , . Now for and it's really easy to get the result. For I found it hard and here is what I do : we use the mean value theorem to say that : And we get the desired result. Now my question is : 1- Is it possible to get the limit when with an other technique (without using the mean value theorem) ? 2- How to get the limit when ? Thank you !","u_n u_n = (n+1)^{\alpha} - n^\alpha \lim_{n \to \infty} u_n \alpha \alpha \in [0,1[ \lim_{n \to \infty} u_n = 0 \alpha = 1 \lim_{n \to \infty} u_n = 1 \alpha \geq 2 \lim_{n \to \infty} u_n = +\infty \alpha = 1 \alpha \geq 2 \alpha \in [0,1) \mid (n+1)^\alpha - n^\alpha \mid \leq \alpha n^{\alpha-1} \to 0 \alpha\in [0,1) \alpha \in (1, 2)","['real-analysis', 'sequences-and-series', 'limits']"
50,Is it possible for $x$ to appear in the definition of $\delta$ in an $\epsilon-\delta$ proof of limit?,Is it possible for  to appear in the definition of  in an  proof of limit?,x \delta \epsilon-\delta,"Logic tells me it is not (it would be circular, since the allowed interval for $x$ is itself defined by $\delta$ ), but I don't know where is the error in my workings. I'll explain... My doubt arised when studying the $\epsilon-\delta$ definition for non-linear polynomial functions, say a quadratic function, when you want to prove that the limit equals the value of the function at any given value of $x$ . This could be the case when checking continuity, for example. As I've seen so far, the normal way of solving the proof for quadratic polynomials consists in converting the epsilon inequality like so: $$|f(x) - L| < \epsilon = -\epsilon < f(x) - L < \epsilon$$ Then equaling the middle part with de lower side of the $\delta$ inequality and finally choosing the minimum of the two possibilities you get, taken into account you have to assure all values of $x$ will have an image inside the $\epsilon$ interval. But! As I've recently realised, the polynomial you get after subtracting the limit to the function when both equal each other, as this nullifies to 0, is always factorable. Plus, one of the two factors you get already corresponds to the lower side of the delta inequality. Here is the formal proof of this. Given: $$\lim_{x \to n} ax^2 + bx + c = an^2 + bn + c$$ The epsilon inequality of the proof would be: $$|ax^2 + bx + c - (an^2 + bn + c)| < \epsilon$$ And working from that: $$|ax^2 + bx + c - an^2 - bn - c| < \epsilon$$ $$|ax^2 + bx - an^2 - bn| < \epsilon$$ $$|a(x^2 - n^2) + b(x - n)| < \epsilon$$ $$|a(x + n)(x - n) + b(x - n)| < \epsilon$$ $$|(x - n)(a(x + n) + b)| < \epsilon$$ $$|x - n||a(x + n) + b| < \epsilon$$ $$|x - n| < \frac{\epsilon}{|a(x + n) + b|}$$ And there it is! You'll always have the lower side of the delta inequality on the left. Thus, you could define $\delta$ as: $$\delta \le \frac{\epsilon}{|a(x + n) + b|}$$ Et voilà! Here you have $x$ defining delta. Now, obviously something weird is going on here... isn't it? I highly suspect my math is flawed at some point, possibly when handling absolute values. Add to that I have no idea how to connect this method with the one that has to choose between two possible values of $\delta$ . Any help with this mess would be much appreciated!","Logic tells me it is not (it would be circular, since the allowed interval for is itself defined by ), but I don't know where is the error in my workings. I'll explain... My doubt arised when studying the definition for non-linear polynomial functions, say a quadratic function, when you want to prove that the limit equals the value of the function at any given value of . This could be the case when checking continuity, for example. As I've seen so far, the normal way of solving the proof for quadratic polynomials consists in converting the epsilon inequality like so: Then equaling the middle part with de lower side of the inequality and finally choosing the minimum of the two possibilities you get, taken into account you have to assure all values of will have an image inside the interval. But! As I've recently realised, the polynomial you get after subtracting the limit to the function when both equal each other, as this nullifies to 0, is always factorable. Plus, one of the two factors you get already corresponds to the lower side of the delta inequality. Here is the formal proof of this. Given: The epsilon inequality of the proof would be: And working from that: And there it is! You'll always have the lower side of the delta inequality on the left. Thus, you could define as: Et voilà! Here you have defining delta. Now, obviously something weird is going on here... isn't it? I highly suspect my math is flawed at some point, possibly when handling absolute values. Add to that I have no idea how to connect this method with the one that has to choose between two possible values of . Any help with this mess would be much appreciated!",x \delta \epsilon-\delta x |f(x) - L| < \epsilon = -\epsilon < f(x) - L < \epsilon \delta x \epsilon \lim_{x \to n} ax^2 + bx + c = an^2 + bn + c |ax^2 + bx + c - (an^2 + bn + c)| < \epsilon |ax^2 + bx + c - an^2 - bn - c| < \epsilon |ax^2 + bx - an^2 - bn| < \epsilon |a(x^2 - n^2) + b(x - n)| < \epsilon |a(x + n)(x - n) + b(x - n)| < \epsilon |(x - n)(a(x + n) + b)| < \epsilon |x - n||a(x + n) + b| < \epsilon |x - n| < \frac{\epsilon}{|a(x + n) + b|} \delta \delta \le \frac{\epsilon}{|a(x + n) + b|} x \delta,"['calculus', 'limits', 'epsilon-delta']"
51,Convergence to arithmetic mean of the following series,Convergence to arithmetic mean of the following series,,"Let $m \in \mathbb{N}$ be fixed and $\forall i \in \{1,2,...m\}, a_{i}\geq0$ Now, define $S_{n} = \bigg( \sum_{i=1}^{m}\frac{1}{m}a_{i}^{\frac{1}{n}}\bigg)^{n}$ . I want to show that $\lim\limits_{n\to\infty}S_{n} = \frac{1}{m} \sum_{i=1}^{m}a_{i}$ . In the other words, $S_{n}$ converges to arithmetic mean. So, my attempt so far is to to apply sandwich theorem. By using Jensen's inequality, I can obtain $$0\leq S_n \leq \frac{1}{m}\sum_{i=1}^{m}a_{i}$$ My problem is the lower bound since I can only obtain $\frac{1}{m^{n}}\sum_{i=1}^{m}a_{i}$ which tends to $0$ as $n\to\infty$ . Any help to obtain the lower bound or any hint to use another method might be nice. Thank you very much!","Let be fixed and Now, define . I want to show that . In the other words, converges to arithmetic mean. So, my attempt so far is to to apply sandwich theorem. By using Jensen's inequality, I can obtain My problem is the lower bound since I can only obtain which tends to as . Any help to obtain the lower bound or any hint to use another method might be nice. Thank you very much!","m \in \mathbb{N} \forall i \in \{1,2,...m\}, a_{i}\geq0 S_{n} = \bigg( \sum_{i=1}^{m}\frac{1}{m}a_{i}^{\frac{1}{n}}\bigg)^{n} \lim\limits_{n\to\infty}S_{n} = \frac{1}{m} \sum_{i=1}^{m}a_{i} S_{n} 0\leq S_n \leq \frac{1}{m}\sum_{i=1}^{m}a_{i} \frac{1}{m^{n}}\sum_{i=1}^{m}a_{i} 0 n\to\infty","['calculus', 'sequences-and-series', 'limits', 'convergence-divergence']"
52,The limit of $f(x) = \begin{cases} x & x\text{ rational} \\ -x & x\text{ irrational}\end{cases}.$,The limit of,f(x) = \begin{cases} x & x\text{ rational} \\ -x & x\text{ irrational}\end{cases}.,"Let $f : \mathbb{R} \to \mathbb{R}$ be a function defined by $$f(x) = \begin{cases} x & x\in \mathbb{Q} \\ -x & x \in \mathbb{Q}^c \end{cases}.$$ prove that $\lim_{x \to c} f(x)$ exists iff $c=0$ . 1) Suppose that $c=0$ , $\epsilon >0$ , and choose $\delta = \epsilon$ . If $0<|x-0|< \delta$ then $|f(x)-0|=|f(x)|=|x|<\epsilon$ By definition of the limit this means that $\lim_{x \to 0} f(x)=0$ 2) Let $\lim_{x \to c} f(x)$ exists, say $L$ , and we want to show that $c=0$ . $\lim_{x \to c} f(x)=L$ , this means that $\forall \epsilon >0, \exists \delta >0, |x-c|<\delta$ then $|f(x)-L|<\epsilon$ How can I complete that, please?","Let be a function defined by prove that exists iff . 1) Suppose that , , and choose . If then By definition of the limit this means that 2) Let exists, say , and we want to show that . , this means that then How can I complete that, please?","f : \mathbb{R} \to \mathbb{R} f(x) = \begin{cases} x & x\in \mathbb{Q} \\ -x & x \in \mathbb{Q}^c \end{cases}. \lim_{x \to c} f(x) c=0 c=0 \epsilon >0 \delta = \epsilon 0<|x-0|< \delta |f(x)-0|=|f(x)|=|x|<\epsilon \lim_{x \to 0} f(x)=0 \lim_{x \to c} f(x) L c=0 \lim_{x \to c} f(x)=L \forall \epsilon >0, \exists \delta >0, |x-c|<\delta |f(x)-L|<\epsilon","['real-analysis', 'limits']"
53,Radius of convergence of $\sum_{n=0}^{\infty}(2^n)x^{n^2}$,Radius of convergence of,\sum_{n=0}^{\infty}(2^n)x^{n^2},"Radius of convergence of $\sum_{n=0}^{\infty}2^nx^{n^2}$ I will use the Root test, let $c_n = 2^nx^{n^2}$ $\lim_{n\rightarrow \infty}  c_{n}^{1/n} = \lim_{n\rightarrow \infty} 2x^{n} = 0 $ if $-1<x<1$ and $\pm \infty$ if $|x|>1$ Since we want $\lim_{n\rightarrow \infty}  c_{n}^{1/n}<1$ for convergence as according to the Root test, so the radius of convergence must be $1$ . Is this correct?","Radius of convergence of I will use the Root test, let if and if Since we want for convergence as according to the Root test, so the radius of convergence must be . Is this correct?",\sum_{n=0}^{\infty}2^nx^{n^2} c_n = 2^nx^{n^2} \lim_{n\rightarrow \infty}  c_{n}^{1/n} = \lim_{n\rightarrow \infty} 2x^{n} = 0  -1<x<1 \pm \infty |x|>1 \lim_{n\rightarrow \infty}  c_{n}^{1/n}<1 1,"['real-analysis', 'sequences-and-series', 'limits', 'power-series']"
54,Calculate: $\lim\limits_{x\to0^-} \frac1{\ln(1-x)}+\frac1x$ without LHR/Expansions [duplicate],Calculate:  without LHR/Expansions [duplicate],\lim\limits_{x\to0^-} \frac1{\ln(1-x)}+\frac1x,"This question already has answers here : Limit $\lim\limits_{x\to0}\frac1{\ln(x+1)}-\frac1x$ (4 answers) Closed 5 years ago . How to calculate $$\lim\limits_{x\to0^-} \left(\frac1{\ln(1-x)}+\frac1x \right)$$ without using L'Hopital, expansions nor integration? I found the answer: Using the Mean value theorem on: $f(x)=e^x-\frac{x^2}2-x-1$ We get: $0\le\frac{e^x-x-1}{x^2}-\frac1 2\le \frac{e^x-x-1}{x}$ Thus: $\lim\limits_{x\to0^-} \frac{e^x-x-1}{x^2} = \frac12$ By substituting: $t=\ln(1-x)$ in the original limit we get: $\lim\limits_{t\to0^+} \frac{1-e^t+t}{t(1-e^t)} = \lim\limits_{t\to0^+} \frac{e^t-t-1}{t^2}.\frac{t}{e^t-1} = \frac12$","This question already has answers here : Limit $\lim\limits_{x\to0}\frac1{\ln(x+1)}-\frac1x$ (4 answers) Closed 5 years ago . How to calculate without using L'Hopital, expansions nor integration? I found the answer: Using the Mean value theorem on: We get: Thus: By substituting: in the original limit we get:",\lim\limits_{x\to0^-} \left(\frac1{\ln(1-x)}+\frac1x \right) f(x)=e^x-\frac{x^2}2-x-1 0\le\frac{e^x-x-1}{x^2}-\frac1 2\le \frac{e^x-x-1}{x} \lim\limits_{x\to0^-} \frac{e^x-x-1}{x^2} = \frac12 t=\ln(1-x) \lim\limits_{t\to0^+} \frac{1-e^t+t}{t(1-e^t)} = \lim\limits_{t\to0^+} \frac{e^t-t-1}{t^2}.\frac{t}{e^t-1} = \frac12,"['limits', 'logarithms', 'exponential-function']"
55,Finding the monotonicity of simple sequence - how to?,Finding the monotonicity of simple sequence - how to?,,"I'm trying to find the monotonicity (whether it's increasing, decreasing or non-existeng) of such simple sequence: $$a_{n} = \sqrt[n]{2^n+3^n}$$ $$\frac{a_{n}}{a_{n+1}}=\frac{\sqrt[n]{2^n+3^n}}{\sqrt[n+1]{2^{n+1}+3^{n+1}}}$$ I have dealt with such exercises before without problems. This one I have no idea how to proceed further. Help is appreciated, thanks.","I'm trying to find the monotonicity (whether it's increasing, decreasing or non-existeng) of such simple sequence: I have dealt with such exercises before without problems. This one I have no idea how to proceed further. Help is appreciated, thanks.",a_{n} = \sqrt[n]{2^n+3^n} \frac{a_{n}}{a_{n+1}}=\frac{\sqrt[n]{2^n+3^n}}{\sqrt[n+1]{2^{n+1}+3^{n+1}}},"['real-analysis', 'sequences-and-series', 'limits']"
56,Can we refine this asymptotic for Laguerre polynomials?,Can we refine this asymptotic for Laguerre polynomials?,,"I just found an interesting and useful limit for Laguerre polynomials: $$\lim_{n \to \infty} L_n \left( \frac{2r}{n+1/2} \right)=J_0(2 \sqrt{2r})$$ I'm using specifically this form of the argument because it's the one I'm working with in the application. Of course, we can set any fixed number instead of $1/2$ in the denominator. I found this limit in a paper , which references G. Szego. Orthogonal Polynomials. Amer. Math. Soc. Colloq. Publ. 23, Amer. Math. Soc. Providence, RI, 1975. Fourth Edition. , , Theorem 8.1.3. While the limit is useful for very large orders and smallish $r$ , I would really like to know if there's a refinement that could be applied to derive an asymptotic expansion, which would depend on $n$ . Here's an illustration which shows that the limit is not that good for larger $r$ (though it does approximate the roots better than the magnitude): Not sure how we could refine this asymptotic or how the original limit was derived (as I don't have the linked book). One way is considering the differential equations for both functions. There's also an interesting result from Gradshteyn-Ryzhik: $$L_n(z)= \frac{2}{n!} e^z \int_0^\infty e^{-t^2} t^{2n+1} J_0(2t \sqrt{z}) dt$$ Which may or may not be related to the limit above.","I just found an interesting and useful limit for Laguerre polynomials: I'm using specifically this form of the argument because it's the one I'm working with in the application. Of course, we can set any fixed number instead of in the denominator. I found this limit in a paper , which references G. Szego. Orthogonal Polynomials. Amer. Math. Soc. Colloq. Publ. 23, Amer. Math. Soc. Providence, RI, 1975. Fourth Edition. , , Theorem 8.1.3. While the limit is useful for very large orders and smallish , I would really like to know if there's a refinement that could be applied to derive an asymptotic expansion, which would depend on . Here's an illustration which shows that the limit is not that good for larger (though it does approximate the roots better than the magnitude): Not sure how we could refine this asymptotic or how the original limit was derived (as I don't have the linked book). One way is considering the differential equations for both functions. There's also an interesting result from Gradshteyn-Ryzhik: Which may or may not be related to the limit above.",\lim_{n \to \infty} L_n \left( \frac{2r}{n+1/2} \right)=J_0(2 \sqrt{2r}) 1/2 r n r L_n(z)= \frac{2}{n!} e^z \int_0^\infty e^{-t^2} t^{2n+1} J_0(2t \sqrt{z}) dt,"['limits', 'asymptotics', 'approximation', 'bessel-functions', 'orthogonal-polynomials']"
57,Direct proof of sequential characterization of limits,Direct proof of sequential characterization of limits,,In my studies of real analysis the proof of sequential characterization of limits is done by contradiction. The statement is $\lim\limits_{x\to c}f(x)$ iff for every sequence $\{x_n\}\subseteq\mathrm{dom}(f)\setminus\{c\}$ such that $\{x_n\}\to c$ then $\{f(x_n)\}\to L$ . Typically proving the “if every sequence $\{x_n\}$ ... then $\lim\limits_{x\to c}f(x)=L$ ” is done by supposing $\lim\limits_{x\to c}f(x)\neq L$ . I don’t really like this proof by contradiction so I’m wondering if there’s a better direct proof out there.,In my studies of real analysis the proof of sequential characterization of limits is done by contradiction. The statement is iff for every sequence such that then . Typically proving the “if every sequence ... then ” is done by supposing . I don’t really like this proof by contradiction so I’m wondering if there’s a better direct proof out there.,\lim\limits_{x\to c}f(x) \{x_n\}\subseteq\mathrm{dom}(f)\setminus\{c\} \{x_n\}\to c \{f(x_n)\}\to L \{x_n\} \lim\limits_{x\to c}f(x)=L \lim\limits_{x\to c}f(x)\neq L,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
58,Showing $\lim_{x\to 0} \frac{x - 1}{\sqrt{x} - 1} = 1$,Showing,\lim_{x\to 0} \frac{x - 1}{\sqrt{x} - 1} = 1,"I'd like to use the sequential definition of a limit to show $\lim_{x\to 0} \frac{x - 1}{\sqrt{x} - 1} = 1$ . Here's the definition I'm using: Given a function $f : D \rightarrow \mathbb{R}$ and a limit point $x_{0}$ of its domain $D$ , for a number $\ell$ , we write $$\lim_{x \to x_{0}} f(x) = \ell $$ provided that whenever $\{x_{n}\}$ is a sequence in $D - \{x_{0}\}$ that converges to $x_{0}$ , we have $$\lim_{n\to\infty} f(x_{n}) = \ell$$ Note that this is not the standard $\epsilon-\delta$ definition of a limit . Here's my attempt at proving this claim: Let $\{x_{n}\}$ be a sequence in $\mathbb{R} - \{0\}$ that converges to $0$ . For all $\epsilon > 0$ , $\exists N$ such that $$|x_{n} - 0| < \epsilon $$ for all $n \geq N$ . To prove the claim, we require $\forall \epsilon > 0$ , $\exists N'$ such that $$\left|\frac{x_{n} - 1}{\sqrt{x_{n}} - 1} - 1\right| < \epsilon$$ for all $n \geq N'$ . However, $$\left|\frac{x_{n} - 1}{\sqrt{x_{n}} - 1} - 1\right| = \left|\frac{(\sqrt{x_{n}} + 1) (\sqrt{x_{n}} - 1)}{\sqrt{x_{n}} - 1} - 1\right| = $$ $$|\sqrt{x_{n}} + 1 - 1| = |\sqrt{x_{n}}| \leq |x_{n}|  = |x_{n} - 0|,$$ so setting $N' = N$ completes the proof. Is this correct?","I'd like to use the sequential definition of a limit to show . Here's the definition I'm using: Given a function and a limit point of its domain , for a number , we write provided that whenever is a sequence in that converges to , we have Note that this is not the standard definition of a limit . Here's my attempt at proving this claim: Let be a sequence in that converges to . For all , such that for all . To prove the claim, we require , such that for all . However, so setting completes the proof. Is this correct?","\lim_{x\to 0} \frac{x - 1}{\sqrt{x} - 1} = 1 f : D \rightarrow \mathbb{R} x_{0} D \ell \lim_{x \to x_{0}} f(x) = \ell  \{x_{n}\} D - \{x_{0}\} x_{0} \lim_{n\to\infty} f(x_{n}) = \ell \epsilon-\delta \{x_{n}\} \mathbb{R} - \{0\} 0 \epsilon > 0 \exists N |x_{n} - 0| < \epsilon  n \geq N \forall \epsilon > 0 \exists N' \left|\frac{x_{n} - 1}{\sqrt{x_{n}} - 1} - 1\right| < \epsilon n \geq N' \left|\frac{x_{n} - 1}{\sqrt{x_{n}} - 1} - 1\right| = \left|\frac{(\sqrt{x_{n}} + 1) (\sqrt{x_{n}} - 1)}{\sqrt{x_{n}} - 1} - 1\right| =  |\sqrt{x_{n}} + 1 - 1| = |\sqrt{x_{n}}| \leq |x_{n}|  = |x_{n} - 0|, N' = N",['real-analysis']
59,What does $\lim \inf _ { n \rightarrow \infty }$ mean?,What does  mean?,\lim \inf _ { n \rightarrow \infty },"I'm new into Mathematical Analysis, and my textbook says: Consider the series: $\frac { 1 } { 2 } + \frac { 1 } { 3 } + \frac {  1 } { 2 ^ { 2 } } + \frac { 1 } { 3 ^ { 2 } } + \frac { 1 } { 2 ^ { 3  } } + \frac { 1 } { 3 ^ { 3 } } + \frac { 1 } { 2 ^ { 4 } } + \frac {  1 } { 3 ^ { 4 } } + \cdots$ for which $\liminf _ { n \rightarrow \infty } \frac { a _ { n + 1 } } { a _ { n  } } = \lim _ { n \rightarrow \infty } \left( \frac { 2 } { 3 } \right)  ^ { n } = 0$ $\liminf _ { n \rightarrow \infty } \sqrt [ n ] { a _ { n } } = \lim _  { n \rightarrow \infty } \sqrt [ 2 n ] { \frac { 1 } { 3 ^ { n } } } =  \frac { 1 } { \sqrt { 3 } }$ $\limsup _ { n \rightarrow \infty } \sqrt [ n ] { a _ { n } } = \lim _  { n \rightarrow \infty } \sqrt [ 2 n ] { \frac { 1 } { 2 ^ { n } } } =  \frac { 1 } { \sqrt { 2 } }$ $\limsup _ { n \rightarrow \infty } \frac { a _ { n + 1 } } { a _ { n  } } = \lim _ { n \rightarrow \infty } \frac { 1 } { 2 } \left( \frac {  3 } { 2 } \right) ^ { n } = + \infty$ My question is: What does $\lim \inf _ { n \rightarrow \infty }$ mean?  And why $\lim \inf _ { n \rightarrow \infty } \frac { a _ { n + 1 } } { a _ { n > } } = \lim _ { n \rightarrow \infty } \left( \frac { 2 } { 3 } \right)$ ?","I'm new into Mathematical Analysis, and my textbook says: Consider the series: for which My question is: What does mean?  And why ?","\frac { 1 } { 2 } + \frac { 1 } { 3 } + \frac {
 1 } { 2 ^ { 2 } } + \frac { 1 } { 3 ^ { 2 } } + \frac { 1 } { 2 ^ { 3
 } } + \frac { 1 } { 3 ^ { 3 } } + \frac { 1 } { 2 ^ { 4 } } + \frac {
 1 } { 3 ^ { 4 } } + \cdots \liminf _ { n \rightarrow \infty } \frac { a _ { n + 1 } } { a _ { n
 } } = \lim _ { n \rightarrow \infty } \left( \frac { 2 } { 3 } \right)
 ^ { n } = 0 \liminf _ { n \rightarrow \infty } \sqrt [ n ] { a _ { n } } = \lim _
 { n \rightarrow \infty } \sqrt [ 2 n ] { \frac { 1 } { 3 ^ { n } } } =
 \frac { 1 } { \sqrt { 3 } } \limsup _ { n \rightarrow \infty } \sqrt [ n ] { a _ { n } } = \lim _
 { n \rightarrow \infty } \sqrt [ 2 n ] { \frac { 1 } { 2 ^ { n } } } =
 \frac { 1 } { \sqrt { 2 } } \limsup _ { n \rightarrow \infty } \frac { a _ { n + 1 } } { a _ { n
 } } = \lim _ { n \rightarrow \infty } \frac { 1 } { 2 } \left( \frac {
 3 } { 2 } \right) ^ { n } = + \infty \lim \inf _ { n \rightarrow \infty } \lim \inf _ { n \rightarrow \infty } \frac { a _ { n + 1 } } { a _ { n > } } = \lim _ { n \rightarrow \infty } \left( \frac { 2 } { 3 } \right)","['real-analysis', 'sequences-and-series', 'limits']"
60,Computing the limit of $\lim_{t\rightarrow0}tf(g(t))$ assuming $g(0)=0$ and $g'(0)>0$,Computing the limit of  assuming  and,\lim_{t\rightarrow0}tf(g(t)) g(0)=0 g'(0)>0,"Suppose $f:(0,\infty)\rightarrow\mathbb{R}$ is a continuous function and $g:\mathbb{R}\rightarrow\mathbb{R}$ is a $C^1$ function with $g(0)=0$ and $g'(0)>0$ . If the limit $$ \lim_{t\rightarrow0^+} tf(t)=a $$ exists, can we necessarily compute the limit $$ \lim_{t\rightarrow0^+} tf(g(t))? $$ It seems like we can compute it as \begin{align*} \lim_{t\rightarrow0^+} tf(g(t)) &= \lim_{t\rightarrow0^+} tf\left(t\frac{g(t)}{t}\right) \\&= \lim_{t\rightarrow0^+} tf\left(tg'(0)\right) = \frac{1}{g'(0)}\lim_{t\rightarrow0^+}tf(t) = \frac{a}{g'(0)} \end{align*} but I'm not sure the step from line 1 to line 2 is valid. Is it?","Suppose is a continuous function and is a function with and . If the limit exists, can we necessarily compute the limit It seems like we can compute it as but I'm not sure the step from line 1 to line 2 is valid. Is it?","f:(0,\infty)\rightarrow\mathbb{R} g:\mathbb{R}\rightarrow\mathbb{R} C^1 g(0)=0 g'(0)>0 
\lim_{t\rightarrow0^+} tf(t)=a
 
\lim_{t\rightarrow0^+} tf(g(t))?
 \begin{align*}
\lim_{t\rightarrow0^+} tf(g(t)) &= \lim_{t\rightarrow0^+} tf\left(t\frac{g(t)}{t}\right) \\&= \lim_{t\rightarrow0^+} tf\left(tg'(0)\right) = \frac{1}{g'(0)}\lim_{t\rightarrow0^+}tf(t) = \frac{a}{g'(0)}
\end{align*}","['real-analysis', 'limits']"
61,Showing continuity of $xy$ at all points,Showing continuity of  at all points,xy,"My friend asked me to show continuity of $f(x,y) = xy$ at all points in $\Bbb R^2$. I started We need to show $|xy-ab| \lt \epsilon$ whenever $d((a,b), (x,y)) < \delta$. So we have $|x-a| < \delta$ and $|y-b| < \delta$ then $$|xy-ab | = |xy-ab-bx+bx-ay+ay-xy+xy-ab| \\ = |x(y-b) + y(x-a) -(x-a)(y-b)| \\  < |x(y-b)| + |y(x-a)|+|(x-a)(y-b)|$$ Now we have $|x-a| < \delta$ or $a-\delta<x<a+\delta$ or $|x| < \max (|a+\delta|, |a-\delta|)$ and similarly $|y| < \max(|b-\delta|, |b+\delta|)$ So we obtain: $$|xy-ab| < \delta (\max (|a+\delta|, |a-\delta))+\max(|b-\delta|, |b+\delta|)) \delta ^2 < \epsilon$$ Now how to conclude from here?? Do we take four cases? Thanks a lot! Edit As Holo said, i mistakenly didnt write modulus $|a-\delta|$, now I have edited.","My friend asked me to show continuity of $f(x,y) = xy$ at all points in $\Bbb R^2$. I started We need to show $|xy-ab| \lt \epsilon$ whenever $d((a,b), (x,y)) < \delta$. So we have $|x-a| < \delta$ and $|y-b| < \delta$ then $$|xy-ab | = |xy-ab-bx+bx-ay+ay-xy+xy-ab| \\ = |x(y-b) + y(x-a) -(x-a)(y-b)| \\  < |x(y-b)| + |y(x-a)|+|(x-a)(y-b)|$$ Now we have $|x-a| < \delta$ or $a-\delta<x<a+\delta$ or $|x| < \max (|a+\delta|, |a-\delta|)$ and similarly $|y| < \max(|b-\delta|, |b+\delta|)$ So we obtain: $$|xy-ab| < \delta (\max (|a+\delta|, |a-\delta))+\max(|b-\delta|, |b+\delta|)) \delta ^2 < \epsilon$$ Now how to conclude from here?? Do we take four cases? Thanks a lot! Edit As Holo said, i mistakenly didnt write modulus $|a-\delta|$, now I have edited.",,"['limits', 'multivariable-calculus', 'epsilon-delta']"
62,A cornucopia of confusion on limits?,A cornucopia of confusion on limits?,,"I am having a lot of confusion in my multivariable calculus lectures. We usually see that we can try to approach a certain limit with paths and if two paths yield different ""limits"", then the limit does not exist. But finding ""limits"" via any number of different paths does not guarantee the limit exist, in previous lectures, it was not clear if polar coordinates were a considered a kind of path, but the professor didn't say it wasn't. It was said that only the $\epsilon-\delta$ definition actually guarantees the limit exist. In a local calculus book, I found the following theorem (which was also presented in previous calculus lectures): If $\displaystyle \lim_{(x,y)\to {(x_0,y_0)}}f(x,y)=0$ and $|g(x,y)|\leq M$ for $0<|(x,y)-(x_0,y_0)|<r$ with $r>0, M>0$ fixed real numbers, then: $$\lim_{(x,y)\to {(x_0,y_0)}}f(x,y)g(x,y)=0$$ I tried to use it in the following two cases: $$\lim_{(x,y)\to (0,0)} \frac{xy}{\sqrt{x² + y²}} $$ Here we see that: $$\lim_{(x,y)\to (0,0)} \frac{xy}{\sqrt{x² + y²}} = x \frac{y}{\sqrt{x² + y²}} $$ And it's easy to see that $\frac{y}{\sqrt{x² + y²}}$ is bounded and we can use the previous thorem, also Wolfram Alpha asserts it exists. $$\lim_{(x,y)\to (0,0)} x \sin \left( \frac{1}{x²+y²} \right)$$ Now, it seems a lot that $\left |\sin \left( \frac{1}{x²+y²} \right)\right|\leq 1$ and that we can apply our theorem, but Wolfram Alpha says the limit does not exist. Now, as I said before, it was said that approaching via any number of paths does not prove that a limit indeed exists and it was not clear wheather the approach via polar coordinates is considered a ""path"" (although, to me, it looks like something very diferent than the paths $y=mx$ , for example), now there is a new professor who gave us a list of exercises with the following proposition: Given $f: D \subset \Bbb{R}² \to \Bbb{R}$ , then $$\lim_{(x,y)\to0 } f(x,y)=L \iff \lim_{r\to0} f(r\cos \theta,r \sin \theta)=L$$ uniformly in $\theta$ . That is, the limit exists iff the radial limit of $f$ does not depend on $\theta$ . But what is ""uniformly""? What is ""does not depend""? I have tried to use it on the second example, it gives me: $$\lim_{r\to 0} r \cos \theta \sin\left(\frac{1}{r²}\right) $$ And then I assumed that ""does not depends"" means that no part of the formula contains $\theta$ . But upon inspecting my first example, it gives me: $$\lim_{r\to 0} r\cos \theta \sin \theta $$ If I assume this meaning for ""does not depend"", then this limit does not exist, but the other theorem asserts it does and I have also verified on Wolfram Alpha. I have talked with this professor, she said me that the theorem on radial limits is true and that she saw it in a book (I never saw it anywhere), I asked her the title of the book and am still waiting a response, she also said that the polar coordinates are not really a ""path"". From this, I have these questions: Is the first theorem actually a theorem? Is the second theorem actually a theorem? Where can I find a proof of it? I googled a bit something like ""polar coordinate limits"" and ""radial limits"" but didn't find anything that seemed relevant. I am asking if it is true because if it is, I still can't prove it. If the first theorem is true, then where am I making any mistake? I suspect I may be misreading the condition of the distance bounded by $r$ . What is the meaning of ""depend"" and ""uniformly""? I have noticed the following possibility (may be wrong): $$\lim_{r\to 0} r \cos \theta \sin\left(\frac{1}{r²}\right) =\cos \theta \lim_{r\to 0} r  \sin\left(\frac{1}{r²}\right) $$ and I know that $$\lim_{r\to 0} r  \sin\left(\frac{1}{r²}\right)=0$$ and $|\cos x| \leq 1$ , so what could possibly go wrong if we take a different $\theta$ ? It really seems that a change in $\theta$ won't change the limit.","I am having a lot of confusion in my multivariable calculus lectures. We usually see that we can try to approach a certain limit with paths and if two paths yield different ""limits"", then the limit does not exist. But finding ""limits"" via any number of different paths does not guarantee the limit exist, in previous lectures, it was not clear if polar coordinates were a considered a kind of path, but the professor didn't say it wasn't. It was said that only the definition actually guarantees the limit exist. In a local calculus book, I found the following theorem (which was also presented in previous calculus lectures): If and for with fixed real numbers, then: I tried to use it in the following two cases: Here we see that: And it's easy to see that is bounded and we can use the previous thorem, also Wolfram Alpha asserts it exists. Now, it seems a lot that and that we can apply our theorem, but Wolfram Alpha says the limit does not exist. Now, as I said before, it was said that approaching via any number of paths does not prove that a limit indeed exists and it was not clear wheather the approach via polar coordinates is considered a ""path"" (although, to me, it looks like something very diferent than the paths , for example), now there is a new professor who gave us a list of exercises with the following proposition: Given , then uniformly in . That is, the limit exists iff the radial limit of does not depend on . But what is ""uniformly""? What is ""does not depend""? I have tried to use it on the second example, it gives me: And then I assumed that ""does not depends"" means that no part of the formula contains . But upon inspecting my first example, it gives me: If I assume this meaning for ""does not depend"", then this limit does not exist, but the other theorem asserts it does and I have also verified on Wolfram Alpha. I have talked with this professor, she said me that the theorem on radial limits is true and that she saw it in a book (I never saw it anywhere), I asked her the title of the book and am still waiting a response, she also said that the polar coordinates are not really a ""path"". From this, I have these questions: Is the first theorem actually a theorem? Is the second theorem actually a theorem? Where can I find a proof of it? I googled a bit something like ""polar coordinate limits"" and ""radial limits"" but didn't find anything that seemed relevant. I am asking if it is true because if it is, I still can't prove it. If the first theorem is true, then where am I making any mistake? I suspect I may be misreading the condition of the distance bounded by . What is the meaning of ""depend"" and ""uniformly""? I have noticed the following possibility (may be wrong): and I know that and , so what could possibly go wrong if we take a different ? It really seems that a change in won't change the limit.","\epsilon-\delta \displaystyle \lim_{(x,y)\to {(x_0,y_0)}}f(x,y)=0 |g(x,y)|\leq M 0<|(x,y)-(x_0,y_0)|<r r>0, M>0 \lim_{(x,y)\to {(x_0,y_0)}}f(x,y)g(x,y)=0 \lim_{(x,y)\to (0,0)} \frac{xy}{\sqrt{x² + y²}}  \lim_{(x,y)\to (0,0)} \frac{xy}{\sqrt{x² + y²}} = x \frac{y}{\sqrt{x² + y²}}  \frac{y}{\sqrt{x² + y²}} \lim_{(x,y)\to (0,0)} x \sin \left( \frac{1}{x²+y²} \right) \left |\sin \left( \frac{1}{x²+y²} \right)\right|\leq 1 y=mx f: D \subset \Bbb{R}² \to \Bbb{R} \lim_{(x,y)\to0 } f(x,y)=L \iff \lim_{r\to0} f(r\cos \theta,r \sin \theta)=L \theta f \theta \lim_{r\to 0} r \cos \theta \sin\left(\frac{1}{r²}\right)  \theta \lim_{r\to 0} r\cos \theta \sin \theta  r \lim_{r\to 0} r \cos \theta \sin\left(\frac{1}{r²}\right) =\cos \theta \lim_{r\to 0} r  \sin\left(\frac{1}{r²}\right)  \lim_{r\to 0} r  \sin\left(\frac{1}{r²}\right)=0 |\cos x| \leq 1 \theta \theta","['real-analysis', 'limits', 'analysis', 'multivariable-calculus']"
63,Derivative of inverse function proof verification,Derivative of inverse function proof verification,,"Can someone verify whether my attempt to prove this theorem is correct? Notice that I use a generalized definition of derivative: Let $f: E \subseteq \mathbb{R}  \to \mathbb{R}$ be a function. Let $p$ both a point and a limit point of $E$. Then we define the derivative of $f$ at $p$ as the limit $f'(p) = \lim_{x \to p, x \in E \setminus \{p\}} \frac{f(x)-f(p)}{x-p}$, provided the limit exists. Theorem : Let $f: E \subseteq \mathbb{R} \to F \subseteq \mathbb{R}$ be an invertible function that is differentiable at $p \in  E$. Suppose that $f^{-1}: F \to E$ is continuous at $f(p)$ and that   $f'(p) \neq 0$. Then $f^{-1}$ is differentiable at $f(p)$, and we have $$(f^{-1})'(f(p)) = \frac{1}{f'(p)}$$ Proof : Before proving the theorem, we have to check that differentiation at $f(p)$ makes sense: we have to show that $f(p)$ is a limit point of $Y$. Let $\epsilon > 0$. Because $f$ is differentiable at $p$, $f$ is continuous at $p$ and it follows that $|f(p)-f(x)| < \epsilon$ whenever $x \in (p- \delta, p + \delta) \cap E \setminus \{p\}$ for some $\delta > 0$. Notice: $f'(p) \neq 0$ implies that $f$ is not constant on $(p- \delta, p + \delta) \cap E \setminus \{p\}$ (if it were constant, we would have $f'(p) = \lim_{x \to p} \frac{f(x)-f(p)}{x-p} = \lim_{x \to p, x \in E \cap (p- \delta, p + \delta)\setminus \{p\}}\frac{f(x)-f(p)}{x-p} = 0)$. Combining these facts, we deduce that $0 < |f(x)-f(p)| < \epsilon$ for some $x \in E$, and $f(p)$ is a limit point of $F = f(E)$. Define $$F: E \to \mathbb{R}: x \mapsto \begin{cases} \frac{f(x)-f(p)}{x-p} \quad x \neq p \\ f'(p) \quad x = p\end{cases}$$ Clearly, $F$ is continuous at $p$. The theorem now follows from the following easy calculation: $$(f^{-1})'(f(p)) = \lim_{y \to f(p)} \frac{f^{-1}(y)- f^{-1}(f(p))}{y-f(p)}$$ $$= \lim_{y \to f(p)}\frac{1}{\frac{f(f^{-1}(y))-f(p)}{f^{-1}(y)- p}}$$ $$ = \lim_{y \to f(p)} \frac{1}{F(f^{-1}(y))}$$ $$=  \frac{1}{F( \lim_{y \to f(p)} f^{-1}(y))} =  \frac{1}{F(p)} = \frac{1}{f'(p)}$$ However, some equalities need some explanation. The second equality is justified by noticing that $f^{-1}(y) - p = f^{-1}(y) - f^{-1}(f(p))$ is zero only when $y = f(p)$, so there are no trouble with dividing by zero. The fourth equality uses the continuity of $F$ at $p$ and the fifth equality follows from the continuity of $f^{-1}$ at $f(p) \quad \square$","Can someone verify whether my attempt to prove this theorem is correct? Notice that I use a generalized definition of derivative: Let $f: E \subseteq \mathbb{R}  \to \mathbb{R}$ be a function. Let $p$ both a point and a limit point of $E$. Then we define the derivative of $f$ at $p$ as the limit $f'(p) = \lim_{x \to p, x \in E \setminus \{p\}} \frac{f(x)-f(p)}{x-p}$, provided the limit exists. Theorem : Let $f: E \subseteq \mathbb{R} \to F \subseteq \mathbb{R}$ be an invertible function that is differentiable at $p \in  E$. Suppose that $f^{-1}: F \to E$ is continuous at $f(p)$ and that   $f'(p) \neq 0$. Then $f^{-1}$ is differentiable at $f(p)$, and we have $$(f^{-1})'(f(p)) = \frac{1}{f'(p)}$$ Proof : Before proving the theorem, we have to check that differentiation at $f(p)$ makes sense: we have to show that $f(p)$ is a limit point of $Y$. Let $\epsilon > 0$. Because $f$ is differentiable at $p$, $f$ is continuous at $p$ and it follows that $|f(p)-f(x)| < \epsilon$ whenever $x \in (p- \delta, p + \delta) \cap E \setminus \{p\}$ for some $\delta > 0$. Notice: $f'(p) \neq 0$ implies that $f$ is not constant on $(p- \delta, p + \delta) \cap E \setminus \{p\}$ (if it were constant, we would have $f'(p) = \lim_{x \to p} \frac{f(x)-f(p)}{x-p} = \lim_{x \to p, x \in E \cap (p- \delta, p + \delta)\setminus \{p\}}\frac{f(x)-f(p)}{x-p} = 0)$. Combining these facts, we deduce that $0 < |f(x)-f(p)| < \epsilon$ for some $x \in E$, and $f(p)$ is a limit point of $F = f(E)$. Define $$F: E \to \mathbb{R}: x \mapsto \begin{cases} \frac{f(x)-f(p)}{x-p} \quad x \neq p \\ f'(p) \quad x = p\end{cases}$$ Clearly, $F$ is continuous at $p$. The theorem now follows from the following easy calculation: $$(f^{-1})'(f(p)) = \lim_{y \to f(p)} \frac{f^{-1}(y)- f^{-1}(f(p))}{y-f(p)}$$ $$= \lim_{y \to f(p)}\frac{1}{\frac{f(f^{-1}(y))-f(p)}{f^{-1}(y)- p}}$$ $$ = \lim_{y \to f(p)} \frac{1}{F(f^{-1}(y))}$$ $$=  \frac{1}{F( \lim_{y \to f(p)} f^{-1}(y))} =  \frac{1}{F(p)} = \frac{1}{f'(p)}$$ However, some equalities need some explanation. The second equality is justified by noticing that $f^{-1}(y) - p = f^{-1}(y) - f^{-1}(f(p))$ is zero only when $y = f(p)$, so there are no trouble with dividing by zero. The fourth equality uses the continuity of $F$ at $p$ and the fifth equality follows from the continuity of $f^{-1}$ at $f(p) \quad \square$",,"['real-analysis', 'limits']"
64,"If $\{a_n\}$ is a sequence such that $0\leq a_{m+n}\leq a_m+a_n$, show that $\lim_{n\to\infty}\frac{a_n}{n}$ exists [duplicate]","If  is a sequence such that , show that  exists [duplicate]",\{a_n\} 0\leq a_{m+n}\leq a_m+a_n \lim_{n\to\infty}\frac{a_n}{n},"This question already has answers here : Prove $\lim_{n\to\infty} \frac{a_n}{n}$ exists for positive sequence where $a_{n+m} \leq a_n + a_m$ (3 answers) Closed 5 years ago . Let $\{a_n\}$ be a sequence satisfying $\forall m,n\in\Bbb{N}:0\leq a_{m+n}\leq a_m+a_n$. Prove that $$\lim_{n\to\infty}\frac{a_n}{n}$$ exists. At first I tried to do it with some $\epsilon-\delta$. We wish to prove that there exists some $L$ such that $\forall\epsilon>0:\exists n_0\in \Bbb{N}:n>n_0\Rightarrow|\frac{a_n}{n}-L|<\epsilon$. Which then got me to something like this:$$-\epsilon\cdot n<a_n-Ln<\epsilon\cdot n$$ which got me nowhere and I didn't find a way to make use of the inequality given. The sequence is obviously bounded below but non-decreasing, which tells me nothing. I don't see any way to use this fact to prove the existence of given limit. Any useful tips, hints appreciated.","This question already has answers here : Prove $\lim_{n\to\infty} \frac{a_n}{n}$ exists for positive sequence where $a_{n+m} \leq a_n + a_m$ (3 answers) Closed 5 years ago . Let $\{a_n\}$ be a sequence satisfying $\forall m,n\in\Bbb{N}:0\leq a_{m+n}\leq a_m+a_n$. Prove that $$\lim_{n\to\infty}\frac{a_n}{n}$$ exists. At first I tried to do it with some $\epsilon-\delta$. We wish to prove that there exists some $L$ such that $\forall\epsilon>0:\exists n_0\in \Bbb{N}:n>n_0\Rightarrow|\frac{a_n}{n}-L|<\epsilon$. Which then got me to something like this:$$-\epsilon\cdot n<a_n-Ln<\epsilon\cdot n$$ which got me nowhere and I didn't find a way to make use of the inequality given. The sequence is obviously bounded below but non-decreasing, which tells me nothing. I don't see any way to use this fact to prove the existence of given limit. Any useful tips, hints appreciated.",,"['sequences-and-series', 'limits']"
65,"$\lim_{(x,y)\to(1,2)} \left(\frac{\tan^{-1}x+\tan^{-1}\frac1y-\tan^{-1}3}{(x-1)(y-2)}\right)\sin^{-1}(y-2)$ using Polar Coordinates",using Polar Coordinates,"\lim_{(x,y)\to(1,2)} \left(\frac{\tan^{-1}x+\tan^{-1}\frac1y-\tan^{-1}3}{(x-1)(y-2)}\right)\sin^{-1}(y-2)","$$$$ A point $P(x,y)$ lies on a curve $y=f(x)$ such that the limit $$L=\lim_{(x,y)\to(1,2)} \left(\dfrac{\tan^{-1}x+\tan^{-1}\frac1y-\tan^{-1}3}{(x-1)(y-2)}\right)\sin^{-1}(y-2)$$ exists. Find $\lim_{x\to\frac13}\dfrac{f^{-1}(x)}{3x-1}$ . $$$$ First, I shifted the origin to $(1,2)$ ie I set $x=X+1,y=Y+2$ where $(X, Y)$ are the coordinates in the shifted axes. The limit would then be $$L=\lim_{(X,Y)\to(0,0)} \left(\dfrac{\tan^{-1}\left(X+1\right)x+\tan^{-1}\left(\frac1{Y+2}\right)-\tan^{-1}3}{XY}\right)\sin^{-1}(Y)$$ Subsequently I switched to Polar Coordinates $(r,\theta)$ by setting $X=r\cos\theta,Y=r\sin\theta$ . Thus, as $(X,Y)\to0,r\to0$ . $$$$ (Note that I haven't specified the path from which $r\to0$ ie I haven't specified what $\theta$ tends to. This is one of the places where I have doubts). $$$$ Thus the limit would become $$L=\lim_{r\to0}\left(\dfrac{\tan^{-1}\left(r\cos\theta+1\right)x+\tan^{-1}\left(\frac1{r\sin\theta+2}\right)-\tan^{-1}3}{r^2\cos\theta\sin\theta}\right)\sin^{-1}(r\sin\theta)$$ $$=\lim_{r\to0}\left(\dfrac{\tan^{-1}\left(r\cos\theta+1\right)x+\tan^{-1}\left(\frac1{r\sin\theta+2}\right)-\tan^{-1}3}{r\cos\theta}\right)\dfrac{\sin^{-1}(r\sin\theta)}{r\sin\theta}$$","A point lies on a curve such that the limit exists. Find . First, I shifted the origin to ie I set where are the coordinates in the shifted axes. The limit would then be Subsequently I switched to Polar Coordinates by setting . Thus, as . (Note that I haven't specified the path from which ie I haven't specified what tends to. This is one of the places where I have doubts). Thus the limit would become"," P(x,y) y=f(x) L=\lim_{(x,y)\to(1,2)} \left(\dfrac{\tan^{-1}x+\tan^{-1}\frac1y-\tan^{-1}3}{(x-1)(y-2)}\right)\sin^{-1}(y-2) \lim_{x\to\frac13}\dfrac{f^{-1}(x)}{3x-1}  (1,2) x=X+1,y=Y+2 (X, Y) L=\lim_{(X,Y)\to(0,0)} \left(\dfrac{\tan^{-1}\left(X+1\right)x+\tan^{-1}\left(\frac1{Y+2}\right)-\tan^{-1}3}{XY}\right)\sin^{-1}(Y) (r,\theta) X=r\cos\theta,Y=r\sin\theta (X,Y)\to0,r\to0  r\to0 \theta  L=\lim_{r\to0}\left(\dfrac{\tan^{-1}\left(r\cos\theta+1\right)x+\tan^{-1}\left(\frac1{r\sin\theta+2}\right)-\tan^{-1}3}{r^2\cos\theta\sin\theta}\right)\sin^{-1}(r\sin\theta) =\lim_{r\to0}\left(\dfrac{\tan^{-1}\left(r\cos\theta+1\right)x+\tan^{-1}\left(\frac1{r\sin\theta+2}\right)-\tan^{-1}3}{r\cos\theta}\right)\dfrac{\sin^{-1}(r\sin\theta)}{r\sin\theta}","['limits', 'polar-coordinates']"
66,Proof that Integral over the arc vanishes as $R\rightarrow\infty$ in Inverse Mellin Transform of $\Gamma(s)$,Proof that Integral over the arc vanishes as  in Inverse Mellin Transform of,R\rightarrow\infty \Gamma(s),"It´s a very well know result that $$e^{-x}=\frac{1}{2\pi i}\int_{\gamma-i\infty}^{\gamma+i\infty} \Gamma(s)x^{-s}ds$$ In order to solve this integral we have to close the contour to the left and show that the integral over this path vanishes as $R\rightarrow\infty$ . Unfortunately it´s not obvious to me how to proof it.  All textbooks that I came across assume that the integral goes to zero without any proof. What I have tried so far is the following: I assume that we are closing this contour with a semi circle to the left, and I get the following integral:  Let $s=Re^{i\theta}$ and $ds=iRe^{i\theta}d\theta$ , $$\int_{\frac{\pi}{2}}^{\frac{3\pi}{2}} \Gamma(Re^{i\theta})x^{-Re^{i\theta}}iRe^{i\theta}d\theta$$ Using Stirling´s approximation for the Gamma function we have $$\Gamma(Re^{i\theta})\sim\sqrt{2\pi} e^{-Re^{i\theta}}(Re^{i\theta})^{Re^{i\theta}-\frac{1}{2}}$$ and $$\int_{\frac{\pi}{2}}^{\frac{3\pi}{2}} \Gamma(Re^{i\theta})x^{-Re^{i\theta}}iRe^{i\theta}d\theta\sim\ \sqrt{2\pi} iR \int_{\frac{\pi}{2}}^{\frac{3\pi}{2}} e^{-Re^{i\theta}}(Re^{i\theta})^{Re^{i\theta}-\frac{1}{2}}x^{-Re^{i\theta}}e^{i\theta}d\theta$$ I took the modulus of the last integral and arrived in the following expression: $$\sim \sqrt{2 \pi} \int_{\frac{\pi}{2}}^{\frac{3\pi}{2}} e^{Rcos(\theta)(log(R)-log(x)-1)}e^{\frac{1}{2}log(R)}e^{-R \theta sin(\theta)}d\theta$$ But I don´t know how to evaluate the $\lim$ as $R \to \infty$ I really appreciate if someone could show this for me. Thank you in advance.","It´s a very well know result that In order to solve this integral we have to close the contour to the left and show that the integral over this path vanishes as . Unfortunately it´s not obvious to me how to proof it.  All textbooks that I came across assume that the integral goes to zero without any proof. What I have tried so far is the following: I assume that we are closing this contour with a semi circle to the left, and I get the following integral:  Let and , Using Stirling´s approximation for the Gamma function we have and I took the modulus of the last integral and arrived in the following expression: But I don´t know how to evaluate the as I really appreciate if someone could show this for me. Thank you in advance.",e^{-x}=\frac{1}{2\pi i}\int_{\gamma-i\infty}^{\gamma+i\infty} \Gamma(s)x^{-s}ds R\rightarrow\infty s=Re^{i\theta} ds=iRe^{i\theta}d\theta \int_{\frac{\pi}{2}}^{\frac{3\pi}{2}} \Gamma(Re^{i\theta})x^{-Re^{i\theta}}iRe^{i\theta}d\theta \Gamma(Re^{i\theta})\sim\sqrt{2\pi} e^{-Re^{i\theta}}(Re^{i\theta})^{Re^{i\theta}-\frac{1}{2}} \int_{\frac{\pi}{2}}^{\frac{3\pi}{2}} \Gamma(Re^{i\theta})x^{-Re^{i\theta}}iRe^{i\theta}d\theta\sim\ \sqrt{2\pi} iR \int_{\frac{\pi}{2}}^{\frac{3\pi}{2}} e^{-Re^{i\theta}}(Re^{i\theta})^{Re^{i\theta}-\frac{1}{2}}x^{-Re^{i\theta}}e^{i\theta}d\theta \sim \sqrt{2 \pi} \int_{\frac{\pi}{2}}^{\frac{3\pi}{2}} e^{Rcos(\theta)(log(R)-log(x)-1)}e^{\frac{1}{2}log(R)}e^{-R \theta sin(\theta)}d\theta \lim R \to \infty,"['integration', 'limits', 'contour-integration', 'gamma-function', 'mellin-transform']"
67,$\lim_{x\to\ \frac{\pi}{2}^-}\frac{1}{x-\frac{\pi}{2}}+\tan{x} $,,\lim_{x\to\ \frac{\pi}{2}^-}\frac{1}{x-\frac{\pi}{2}}+\tan{x} ,$$\lim_{x\to\ \frac{\pi}{2}^-}\frac{1}{x-\frac{\pi}{2}}+\tan x $$ Can anyone give me a hint I'm struggle. Edit maybe I should say what I've done: I used the following change of variable: $t=x-\frac{\pi}{2}$ so I got: $$\lim_{t\to\ 0^-}\frac{1}{t}+\tan \left(t+\frac{\pi}{2}\right)$$ $$\lim_{t\to\ 0^-} \frac{1+t\tan\left(t+\frac{\pi}{2}\right)}{t} $$ I used B.H and I got something like $\frac{1}{0}$ so it's undefined...,$$\lim_{x\to\ \frac{\pi}{2}^-}\frac{1}{x-\frac{\pi}{2}}+\tan x $$ Can anyone give me a hint I'm struggle. Edit maybe I should say what I've done: I used the following change of variable: $t=x-\frac{\pi}{2}$ so I got: $$\lim_{t\to\ 0^-}\frac{1}{t}+\tan \left(t+\frac{\pi}{2}\right)$$ $$\lim_{t\to\ 0^-} \frac{1+t\tan\left(t+\frac{\pi}{2}\right)}{t} $$ I used B.H and I got something like $\frac{1}{0}$ so it's undefined...,,"['calculus', 'limits']"
68,Limit involving ratios of inverse functions,Limit involving ratios of inverse functions,,Let $H(t)$ be an strictly increasing function such that $\lim_{t\to \infty}H(t)=\infty$. Assuming that $$\lim_{t\to \infty}\frac{H(t)}{\log t}=1$$ I would like to prove or disprove that $$\lim_{y\to \infty}\frac{H^{-1}(y)}{\exp(y)}=1$$ I have not been able to find a counterexample but also not been able to prove if it is true! Looks to me this has to be true!,Let $H(t)$ be an strictly increasing function such that $\lim_{t\to \infty}H(t)=\infty$. Assuming that $$\lim_{t\to \infty}\frac{H(t)}{\log t}=1$$ I would like to prove or disprove that $$\lim_{y\to \infty}\frac{H^{-1}(y)}{\exp(y)}=1$$ I have not been able to find a counterexample but also not been able to prove if it is true! Looks to me this has to be true!,,"['calculus', 'limits', 'inverse-function']"
69,Finding a tight lower bound for $\left(\frac{1+x}{(1+x/2)^2}\right)^n$,Finding a tight lower bound for,\left(\frac{1+x}{(1+x/2)^2}\right)^n,"I am trying to find a tight lower bound for $\left(\frac{1+x}{(1+x/2)^2}\right)^n$ as a function of $x$ and $n$ and for large $n$,  where $x$ changes with $n$  such that $\lim_{n\to\infty}x=0$. I am not sure wether my approach to solve this is right or not, but this is what I did: \begin{align*}\left(\frac{1+x}{(1+x/2)^2}\right)^n&=e^{n(\ln({1+x})-2\ln{(1+x/2)})}\\ &=e^{n(x-\frac{x^2}{2}+\frac{x^3}{3}-\frac{x^4}{4}+\cdots-2(\frac{x}{2}-\frac{x^2}{8}+\frac{x^3}{24}-\cdots))}\\ &=e^{n(-\frac{x^2}{4}+\frac{x^3}{4}-\frac{15x^4}{64}+\cdots)}\\ &\geq  e^{n(-\frac{x^2}{4})}\\ &=1-(n\frac{x^2}{4})+(n\frac{x^2}{4})^2-\cdots \\  &\geq 1-(n\frac{x^2}{4})  \end{align*} We know $\lim_{n\to\infty}x=0$, but we don't know whether $\lim_{n\to\infty}nx^2=0$ . Hence, the last inequality is not necessarily correct,  because the sum of the terms after $1-(n\frac{x^2}{4}) $ may not be greater than zero.","I am trying to find a tight lower bound for $\left(\frac{1+x}{(1+x/2)^2}\right)^n$ as a function of $x$ and $n$ and for large $n$,  where $x$ changes with $n$  such that $\lim_{n\to\infty}x=0$. I am not sure wether my approach to solve this is right or not, but this is what I did: \begin{align*}\left(\frac{1+x}{(1+x/2)^2}\right)^n&=e^{n(\ln({1+x})-2\ln{(1+x/2)})}\\ &=e^{n(x-\frac{x^2}{2}+\frac{x^3}{3}-\frac{x^4}{4}+\cdots-2(\frac{x}{2}-\frac{x^2}{8}+\frac{x^3}{24}-\cdots))}\\ &=e^{n(-\frac{x^2}{4}+\frac{x^3}{4}-\frac{15x^4}{64}+\cdots)}\\ &\geq  e^{n(-\frac{x^2}{4})}\\ &=1-(n\frac{x^2}{4})+(n\frac{x^2}{4})^2-\cdots \\  &\geq 1-(n\frac{x^2}{4})  \end{align*} We know $\lim_{n\to\infty}x=0$, but we don't know whether $\lim_{n\to\infty}nx^2=0$ . Hence, the last inequality is not necessarily correct,  because the sum of the terms after $1-(n\frac{x^2}{4}) $ may not be greater than zero.",,"['sequences-and-series', 'limits', 'asymptotics', 'exponentiation']"
70,Limit of $\frac{x^c-c^x}{x^x-c^c}$ as $x \rightarrow c$,Limit of  as,\frac{x^c-c^x}{x^x-c^c} x \rightarrow c,"My question is: Show that $\lim_{x \rightarrow c} \frac{x^c-c^x}{x^x-c^c}$ exists and find its value. Because the limit is 0/0 I've tried using L'Hopital's rule, but every time I differentiate it I still get 0/0? We're given this hint in the question: use the fact that $a^b=\exp(b\log(a))$ and I've tried doing this to be able to differentiate and use L'Hopital's rule, and also for trying to rearrange it. Can anyone help please?","My question is: Show that $\lim_{x \rightarrow c} \frac{x^c-c^x}{x^x-c^c}$ exists and find its value. Because the limit is 0/0 I've tried using L'Hopital's rule, but every time I differentiate it I still get 0/0? We're given this hint in the question: use the fact that $a^b=\exp(b\log(a))$ and I've tried doing this to be able to differentiate and use L'Hopital's rule, and also for trying to rearrange it. Can anyone help please?",,"['real-analysis', 'limits']"
71,How to find integration with Unknown,How to find integration with Unknown,,I am given the following problem which I have problem to know where to even start: The question: $\lim_{x\to 0} \frac{\int_0^x\frac{t^2}{\sqrt{a+2t^5}}dt} {bx-esinx}=\frac{1}{\pi}$ The part where I will like to know where to start:  $\int_0^x\frac{t^2}{\sqrt{a+2t^5}}dt$ I appreciate suggestions on how I may get about solving the integration part before I move on to solve the limit question as a whole.,I am given the following problem which I have problem to know where to even start: The question: $\lim_{x\to 0} \frac{\int_0^x\frac{t^2}{\sqrt{a+2t^5}}dt} {bx-esinx}=\frac{1}{\pi}$ The part where I will like to know where to start:  $\int_0^x\frac{t^2}{\sqrt{a+2t^5}}dt$ I appreciate suggestions on how I may get about solving the integration part before I move on to solve the limit question as a whole.,,"['integration', 'limits']"
72,Does $\lim_{x\rightarrow 0}\frac{\ln(x)}{\cot(x)}$ exist or not?,Does  exist or not?,\lim_{x\rightarrow 0}\frac{\ln(x)}{\cot(x)},"I stumbled on the following limit in a calculus textbook today: \begin{equation*} \lim_{x\rightarrow 0}\frac{\ln(x)}{\cot(x)} \end{equation*} According to the book's solutions and Mathematica , this limit exists and is equal to 0. I can see why $0$ is obtained using l'Hôpital's rule twice: \begin{equation*} ...=\lim_{x\rightarrow0}\frac{\left(\frac{1}{x}\right)}{-\csc^2(x)}=-\lim_{x\rightarrow0}\frac{\sin^2(x)}{x}=-\lim_{x\rightarrow0}\frac{2\sin(x)\cos(x)}{1}=2\sin(0)\cos(0)=0 \end{equation*} If I recall correctly, l'Hôpital's rule is applicable when we have: \begin{equation*} \lim_{x\rightarrow a}\frac{f(x)}{g(x)} \end{equation*} even if $f$ and $g$ are not derivable at precisely $a$, so there should be no issue in using it on the above limit. However, I can't reconcile the fact that $\ln(x)$ is defined over $]0,+\infty[$ (and usually, only $\lim_{x\rightarrow0^+}\ln(x)$ exists) with the fact that the above limit exists (both as $x\rightarrow0^+$ and as $x\rightarrow0^{-}$). It seems to me that only \begin{equation*} \lim_{x\rightarrow 0^+}\frac{\ln(x)}{\cot(x)} \end{equation*} should exist and thus the ""bilateral limit"" (with $x\rightarrow 0$) does not exist since the limit with $x\rightarrow0^-$ doesn't. Is there something I am missing?","I stumbled on the following limit in a calculus textbook today: \begin{equation*} \lim_{x\rightarrow 0}\frac{\ln(x)}{\cot(x)} \end{equation*} According to the book's solutions and Mathematica , this limit exists and is equal to 0. I can see why $0$ is obtained using l'Hôpital's rule twice: \begin{equation*} ...=\lim_{x\rightarrow0}\frac{\left(\frac{1}{x}\right)}{-\csc^2(x)}=-\lim_{x\rightarrow0}\frac{\sin^2(x)}{x}=-\lim_{x\rightarrow0}\frac{2\sin(x)\cos(x)}{1}=2\sin(0)\cos(0)=0 \end{equation*} If I recall correctly, l'Hôpital's rule is applicable when we have: \begin{equation*} \lim_{x\rightarrow a}\frac{f(x)}{g(x)} \end{equation*} even if $f$ and $g$ are not derivable at precisely $a$, so there should be no issue in using it on the above limit. However, I can't reconcile the fact that $\ln(x)$ is defined over $]0,+\infty[$ (and usually, only $\lim_{x\rightarrow0^+}\ln(x)$ exists) with the fact that the above limit exists (both as $x\rightarrow0^+$ and as $x\rightarrow0^{-}$). It seems to me that only \begin{equation*} \lim_{x\rightarrow 0^+}\frac{\ln(x)}{\cot(x)} \end{equation*} should exist and thus the ""bilateral limit"" (with $x\rightarrow 0$) does not exist since the limit with $x\rightarrow0^-$ doesn't. Is there something I am missing?",,"['real-analysis', 'limits']"
73,Prove that $\lim_{n \to \infty} \frac{1}{2^n}\sum_{k=0}^n(-1)^k {n\choose k}f\left(\frac{k}{n} \right)=0$,Prove that,\lim_{n \to \infty} \frac{1}{2^n}\sum_{k=0}^n(-1)^k {n\choose k}f\left(\frac{k}{n} \right)=0,"Let $f:[0,1] \to \mathbb{R}$ be a continuous function. Prove that   $$\lim_{n \to \infty} \frac{1}{2^n}\sum_{k=0}^n(-1)^k {n\choose k}f\left(\frac{k}{n} \right)=0$$ I know that $f$ is uniformly continuous and I tried to get some inequalities for the terms $f\left(\frac{k}{n} \right)$. For all $\epsilon>0$, we have $|f(0)-f\left(\frac{1}{n} \right)|<\epsilon, \dots, |f\left(\frac{n-1}{n} \right)-f(1)|<\epsilon$ when $n$ is large enough. Then I tried to apply these for the sum in the statement and squeeze it to $0$, but I only got to prove it is less than $\frac{n}{2},$ which doesn't go to $0$.","Let $f:[0,1] \to \mathbb{R}$ be a continuous function. Prove that   $$\lim_{n \to \infty} \frac{1}{2^n}\sum_{k=0}^n(-1)^k {n\choose k}f\left(\frac{k}{n} \right)=0$$ I know that $f$ is uniformly continuous and I tried to get some inequalities for the terms $f\left(\frac{k}{n} \right)$. For all $\epsilon>0$, we have $|f(0)-f\left(\frac{1}{n} \right)|<\epsilon, \dots, |f\left(\frac{n-1}{n} \right)-f(1)|<\epsilon$ when $n$ is large enough. Then I tried to apply these for the sum in the statement and squeeze it to $0$, but I only got to prove it is less than $\frac{n}{2},$ which doesn't go to $0$.",,"['limits', 'functions', 'continuity', 'uniform-continuity']"
74,Epsilon-delta proof of a multivariate limit,Epsilon-delta proof of a multivariate limit,,"I'd like to find $$\lim_{(x,y,z)\to(-2,1,-1)} f(x,y,z)$$ where $$f(x,y,z) = \frac{\sin(x+4y+2z)}{(x+4y+2z)}$$ I know that the limit should be 1, and I have a feeling that I should attempt to find the limit by using the fact that $$\lim_{u\to0} \frac{\sin(u)}{u} = 1$$ but I don't know how to use this fact in the case of multivariable limits. How should I proceed?","I'd like to find $$\lim_{(x,y,z)\to(-2,1,-1)} f(x,y,z)$$ where $$f(x,y,z) = \frac{\sin(x+4y+2z)}{(x+4y+2z)}$$ I know that the limit should be 1, and I have a feeling that I should attempt to find the limit by using the fact that $$\lim_{u\to0} \frac{\sin(u)}{u} = 1$$ but I don't know how to use this fact in the case of multivariable limits. How should I proceed?",,"['limits', 'multivariable-calculus', 'epsilon-delta']"
75,"$\lim_{\,n\to\infty}\frac{S(n)}{n\pi(n)}$",,"\lim_{\,n\to\infty}\frac{S(n)}{n\pi(n)}","Let $S(n)$ be the sum of primes less than or equal to $n$. I guess that  $\lim_{\,n\to\infty}\frac{S(n)}{n\pi(n)}$ exists and it's equal to $0$ but I can't prove it .I've begun to doubt it but If my guess is true I want a proof that doesn't use the prime number theorem.","Let $S(n)$ be the sum of primes less than or equal to $n$. I guess that  $\lim_{\,n\to\infty}\frac{S(n)}{n\pi(n)}$ exists and it's equal to $0$ but I can't prove it .I've begun to doubt it but If my guess is true I want a proof that doesn't use the prime number theorem.",,"['number-theory', 'limits', 'summation', 'prime-numbers']"
76,Normalized integrals over shrinking tubular neighbourhoods converge to an integral on the limit submanifold,Normalized integrals over shrinking tubular neighbourhoods converge to an integral on the limit submanifold,,"$\renewcommand{\S}{\mathcal{S}}$ $\newcommand{\M}{{\mathcal{M}}}$ $\newcommand{\TM}{{T\mathcal{M}}}$ $\newcommand{\TS}{{T\mathcal{S}}}$ $\newcommand{\NS}{{\mathcal{NS}}}$ $\newcommand{\N}{\mathcal{N}}$ $\newcommand{\g}{\mathcal{g}}$ $\newcommand{\Volg}{\text{Vol}_\g}$ $\newcommand{\Vol}{\text{Vol}}$ $\newcommand{\VolgS}{\text{Vol}_{\g|_\S}}$ Let $(\M,\g)$ be a smooth $d$-dimensional Riemannian manifold, and let $\S\subset\M$ be a smooth compact $k$-dimensional oriented submanifold. Let $\NS$ be the normal bundle of $\S$ in $\M$. For a sufficiently small $h>0$, define $\S_h := \{ \exp_p(v) : p\in \S, v\in \NS, |v|\le h \}.$ Let $f:\M \to \mathbb{R}$ be a continuous function. I am trying to prove the following: $$ \lim_{h \to 0} \frac{\int_{\S_h} f}{\Volg(\S_h)}=\frac{\int_{\S} f}{\VolgS(\S)},$$ where the integrals are w.r.t the Riemannian volume forms on $\S_h$,$S$ defined by $\g,\g|_{\S}$ respectively.  (Recall $\S$ is oriented). Edit: Let's start with the case of a $k$-cube embedded in $\mathbb{R}^d$ in the standard way. The general case should follow by an approximation argument, since ""locally, everything is Euclidean"". Here is a proof for the Euclidean case: Let's use Fubini theorem: Suppose $\S \subseteq \mathbb{R}^k \times \{\bar 0^{d-k}\} \subseteq \mathbb{R}^d$, where $S$ is a product of $k$ intervals. Then $\S_h=S \times [-h,h]^{d-k}$. Let $\epsilon >0$, and let $(x,y)\in S\times [-h,h]^{d-k}$. $$\frac{\int_{\S_h} f}{\Vol(\S_h)}= \frac{\int_{[-h,-h]^{d-k}}\big(\int_{S} f(x,y) dx\big)dy }{(2h)^{d-k}\Vol(S)} =\frac{\int_{[-h,-h]^{d-k}} g(y) dy }{\Vol([-h,-h]^{d-k})} ,$$ where $g:[-h,-h]^{d-k} \to \mathbb{R}$ is defined by $$ g(y)=\frac{\int_{x \in S} f(x,y)dx}{\Vol(S)}.$$ Since the domain is compact, $f$ is uniformly continuous, so $g$ is continuous. This implies $$ \lim_{h \to 0} \frac{\int_{[-h,-h]^{d-k}} g(y) dy }{\Vol([-h,-h]^{d-k})}  =g(\bar 0)=\frac{\int_{x \in S} f(x,0)dx}{\Vol(S)}=\frac{\int_{\S} f}{\Volg(\S)}.$$","$\renewcommand{\S}{\mathcal{S}}$ $\newcommand{\M}{{\mathcal{M}}}$ $\newcommand{\TM}{{T\mathcal{M}}}$ $\newcommand{\TS}{{T\mathcal{S}}}$ $\newcommand{\NS}{{\mathcal{NS}}}$ $\newcommand{\N}{\mathcal{N}}$ $\newcommand{\g}{\mathcal{g}}$ $\newcommand{\Volg}{\text{Vol}_\g}$ $\newcommand{\Vol}{\text{Vol}}$ $\newcommand{\VolgS}{\text{Vol}_{\g|_\S}}$ Let $(\M,\g)$ be a smooth $d$-dimensional Riemannian manifold, and let $\S\subset\M$ be a smooth compact $k$-dimensional oriented submanifold. Let $\NS$ be the normal bundle of $\S$ in $\M$. For a sufficiently small $h>0$, define $\S_h := \{ \exp_p(v) : p\in \S, v\in \NS, |v|\le h \}.$ Let $f:\M \to \mathbb{R}$ be a continuous function. I am trying to prove the following: $$ \lim_{h \to 0} \frac{\int_{\S_h} f}{\Volg(\S_h)}=\frac{\int_{\S} f}{\VolgS(\S)},$$ where the integrals are w.r.t the Riemannian volume forms on $\S_h$,$S$ defined by $\g,\g|_{\S}$ respectively.  (Recall $\S$ is oriented). Edit: Let's start with the case of a $k$-cube embedded in $\mathbb{R}^d$ in the standard way. The general case should follow by an approximation argument, since ""locally, everything is Euclidean"". Here is a proof for the Euclidean case: Let's use Fubini theorem: Suppose $\S \subseteq \mathbb{R}^k \times \{\bar 0^{d-k}\} \subseteq \mathbb{R}^d$, where $S$ is a product of $k$ intervals. Then $\S_h=S \times [-h,h]^{d-k}$. Let $\epsilon >0$, and let $(x,y)\in S\times [-h,h]^{d-k}$. $$\frac{\int_{\S_h} f}{\Vol(\S_h)}= \frac{\int_{[-h,-h]^{d-k}}\big(\int_{S} f(x,y) dx\big)dy }{(2h)^{d-k}\Vol(S)} =\frac{\int_{[-h,-h]^{d-k}} g(y) dy }{\Vol([-h,-h]^{d-k})} ,$$ where $g:[-h,-h]^{d-k} \to \mathbb{R}$ is defined by $$ g(y)=\frac{\int_{x \in S} f(x,y)dx}{\Vol(S)}.$$ Since the domain is compact, $f$ is uniformly continuous, so $g$ is continuous. This implies $$ \lim_{h \to 0} \frac{\int_{[-h,-h]^{d-k}} g(y) dy }{\Vol([-h,-h]^{d-k})}  =g(\bar 0)=\frac{\int_{x \in S} f(x,0)dx}{\Vol(S)}=\frac{\int_{\S} f}{\Volg(\S)}.$$",,"['integration', 'limits', 'differential-geometry', 'riemannian-geometry', 'smooth-manifolds']"
77,"Does $\frac{\langle a_k,b_k\rangle}{\|a_k\|}$ converge, if $(a_k)_k$ and $(b_k)_k$ tend to $0$ and $b\neq 0$ respectively?","Does  converge, if  and  tend to  and  respectively?","\frac{\langle a_k,b_k\rangle}{\|a_k\|} (a_k)_k (b_k)_k 0 b\neq 0","If $(a_k)_k$ and $(b_k)_k$ both convergent sequences in $\mathbb{R}^2$ such that their limits are $0$ and $b\neq0$ respectively. Does the sequence. $$\frac{\langle a_k,b_k \rangle}{\|a_k\|}$$ converge? (where $\langle a_k,b_k \rangle$ is the scalar product). I'm not really sure how to go about this, since the numerator and the denominator both go to $0$( all norms are equivalent so it doesn't matter which norm we choose for the denominator). I figured I might try using the sandwich theorem to approximate this but I'm not sure how, since the numerator is basically $a^1_kb^1_k * a^2_kb^2_k$ and if $(a_k)_k$ goes to $(0,0)$ obviously the component sequences both tend to $0$ and i can't separate them from the component sequences of $b_k$. So I'm not sure how to go about this! Any hints at all would be appreciated Thanks in advance!","If $(a_k)_k$ and $(b_k)_k$ both convergent sequences in $\mathbb{R}^2$ such that their limits are $0$ and $b\neq0$ respectively. Does the sequence. $$\frac{\langle a_k,b_k \rangle}{\|a_k\|}$$ converge? (where $\langle a_k,b_k \rangle$ is the scalar product). I'm not really sure how to go about this, since the numerator and the denominator both go to $0$( all norms are equivalent so it doesn't matter which norm we choose for the denominator). I figured I might try using the sandwich theorem to approximate this but I'm not sure how, since the numerator is basically $a^1_kb^1_k * a^2_kb^2_k$ and if $(a_k)_k$ goes to $(0,0)$ obviously the component sequences both tend to $0$ and i can't separate them from the component sequences of $b_k$. So I'm not sure how to go about this! Any hints at all would be appreciated Thanks in advance!",,"['limits', 'multivariable-calculus']"
78,Check the differentiability of the function,Check the differentiability of the function,,"Question Check the differentiability of the function f$\left(x,y\right)=\begin{cases} \frac{2x^{2}y}{x^{2}+y^{2}} & \left(x,y\right)\neq\left(0,0\right)\\ 0 & otherwise, \end{cases}$ My Approach I know a function of two variable is differentiable at $\left(a,b\right)$ $\Leftrightarrow$ f$\left(a+h,b+k\right)-f\left(a,b\right)=Ah$$+Bk+\phi\left(h,k\right)\sqrt{h^{2}+k^{2}},where$ A=$\frac{\partial f}{\partial x}$ =$f_{x}$=0 ,B=$\frac{\partial f}{\partial y}$=$f_{y}=0$ and $\phi\left(h,k\right)$$\longrightarrow0$ as$\left(h,k\right)\longrightarrow\left(0,0\right)$. $f$$\left(a+h,b+k\right)-f\left(a,b\right)=$$\phi\left(h,k\right)\sqrt{h^{2}+k^{2}}$$\Longrightarrow$$\phi\left(h,k\right)\sqrt{h^{2}+k^{2}}=\frac{2h^{2}k}{h^{2}+k^{2}}$ taking h=rcos$\theta$ and   k=rsin$\theta$$\Longrightarrow$$\phi\left(h,k\right)=2cos^{2}\theta$$sin\theta$$\neq$0 $\forall$$\theta$$\Longrightarrow$ $\phi\left(h,k\right)$do not tend to 0 as$\left(h,k\right)\longrightarrow\left(0,0\right)$.So function is not differentiable at origin BOOK's Approach A function of two variable is differentiable at $\left(a,b\right)$ $\Leftrightarrow$f$\left(a+h,b+k\right)-f\left(a,b\right)=Ah$$+Bk+\psi\left(h,k\right)h+\xi\left(h,k\right)k$ (i know this is equivalent to mine rule) $\frac{2h^{2}k}{h^{2}+k^{2}}=$h$\left(\frac{hk}{h^{2}+k^{2}}\right)+k\left(\frac{h^{2}}{h^{2}+k^{2}}\right)$$\Longrightarrow$$\psi\left(h,k\right)=$$\left(\frac{hk}{h^{2}+k^{2}}\right)$and $\xi\left(h,k\right)$=$\left(\frac{h^{2}}{h^{2}+k^{2}}\right)$ Book says as$\left(h,k\right)\rightarrow\left(0,0\right)$$\psi$and $\xi$ also go to $0$.So Function is differentiable at origin.Book uses the word h and k tends to zero simultaneously. But,Lim$_{\left(h,k\right)\longrightarrow\left(0,0\right)}\psi\left(h,k\right)$and Lim$_{\left(h,k\right)\longrightarrow\left(0,0\right)}\xi\left(h,k\right)$ do not exist. Please tell me if book is right , then where i was wrong in my approach.It would be very helpful if the posted answers use my approach and go further.","Question Check the differentiability of the function f$\left(x,y\right)=\begin{cases} \frac{2x^{2}y}{x^{2}+y^{2}} & \left(x,y\right)\neq\left(0,0\right)\\ 0 & otherwise, \end{cases}$ My Approach I know a function of two variable is differentiable at $\left(a,b\right)$ $\Leftrightarrow$ f$\left(a+h,b+k\right)-f\left(a,b\right)=Ah$$+Bk+\phi\left(h,k\right)\sqrt{h^{2}+k^{2}},where$ A=$\frac{\partial f}{\partial x}$ =$f_{x}$=0 ,B=$\frac{\partial f}{\partial y}$=$f_{y}=0$ and $\phi\left(h,k\right)$$\longrightarrow0$ as$\left(h,k\right)\longrightarrow\left(0,0\right)$. $f$$\left(a+h,b+k\right)-f\left(a,b\right)=$$\phi\left(h,k\right)\sqrt{h^{2}+k^{2}}$$\Longrightarrow$$\phi\left(h,k\right)\sqrt{h^{2}+k^{2}}=\frac{2h^{2}k}{h^{2}+k^{2}}$ taking h=rcos$\theta$ and   k=rsin$\theta$$\Longrightarrow$$\phi\left(h,k\right)=2cos^{2}\theta$$sin\theta$$\neq$0 $\forall$$\theta$$\Longrightarrow$ $\phi\left(h,k\right)$do not tend to 0 as$\left(h,k\right)\longrightarrow\left(0,0\right)$.So function is not differentiable at origin BOOK's Approach A function of two variable is differentiable at $\left(a,b\right)$ $\Leftrightarrow$f$\left(a+h,b+k\right)-f\left(a,b\right)=Ah$$+Bk+\psi\left(h,k\right)h+\xi\left(h,k\right)k$ (i know this is equivalent to mine rule) $\frac{2h^{2}k}{h^{2}+k^{2}}=$h$\left(\frac{hk}{h^{2}+k^{2}}\right)+k\left(\frac{h^{2}}{h^{2}+k^{2}}\right)$$\Longrightarrow$$\psi\left(h,k\right)=$$\left(\frac{hk}{h^{2}+k^{2}}\right)$and $\xi\left(h,k\right)$=$\left(\frac{h^{2}}{h^{2}+k^{2}}\right)$ Book says as$\left(h,k\right)\rightarrow\left(0,0\right)$$\psi$and $\xi$ also go to $0$.So Function is differentiable at origin.Book uses the word h and k tends to zero simultaneously. But,Lim$_{\left(h,k\right)\longrightarrow\left(0,0\right)}\psi\left(h,k\right)$and Lim$_{\left(h,k\right)\longrightarrow\left(0,0\right)}\xi\left(h,k\right)$ do not exist. Please tell me if book is right , then where i was wrong in my approach.It would be very helpful if the posted answers use my approach and go further.",,"['limits', 'multivariable-calculus', 'derivatives', 'continuity']"
79,Find $\lim_{n\to \infty}\sqrt[n]{a}$ where 0 < a,Find  where 0 < a,\lim_{n\to \infty}\sqrt[n]{a},"Find $\lim_{n\to \infty}\sqrt[n]{a}$ Disclaimer: Since this is a sequence the fact that n approaches infinity is obvious, therefore I omitted it in the limit notation. My idea is to analyze two cases: 1) $ 1 < a$ and 2) $ 0 < a < 1$. (1) $1 < a$-n $$\sqrt[n]{a} = 1 + t_n$$  And $t_n >0 $Now, I use the binomial formula to get $$a >1+nt_n \Rightarrow t_n<\frac{a-1}{n} \iff 0 <\lim t_n<\lim \frac{a-1}{n} \Rightarrow \lim t_n =0$$ And since $\lim t_n = 0 $ $\lim_{n\to \infty}\sqrt[n]{a}= 1$ Then, I will use the same method to prove the second case. Is it a correct way to do this? If so, is there an easier way?","Find $\lim_{n\to \infty}\sqrt[n]{a}$ Disclaimer: Since this is a sequence the fact that n approaches infinity is obvious, therefore I omitted it in the limit notation. My idea is to analyze two cases: 1) $ 1 < a$ and 2) $ 0 < a < 1$. (1) $1 < a$-n $$\sqrt[n]{a} = 1 + t_n$$  And $t_n >0 $Now, I use the binomial formula to get $$a >1+nt_n \Rightarrow t_n<\frac{a-1}{n} \iff 0 <\lim t_n<\lim \frac{a-1}{n} \Rightarrow \lim t_n =0$$ And since $\lim t_n = 0 $ $\lim_{n\to \infty}\sqrt[n]{a}= 1$ Then, I will use the same method to prove the second case. Is it a correct way to do this? If so, is there an easier way?",,"['calculus', 'sequences-and-series', 'limits', 'radicals']"
80,If $\lim_{x\to \infty}xf(x^2+1)=2$ then find $\lim_{x\to 0}\dfrac{2f'(1/x)}{x\sqrt{x}}$,If  then find,\lim_{x\to \infty}xf(x^2+1)=2 \lim_{x\to 0}\dfrac{2f'(1/x)}{x\sqrt{x}},If $\lim_{x\to \infty}xf(x^2+1)=2$ then find    $$\lim_{x\to 0}\dfrac{2f'(1/x)}{x\sqrt{x}}=?$$ My Try : $$g(x):=xf(x^2+1)\\g'(x)=f(x^2+1)+2xf'(x^2+1)$$ Now what?,If $\lim_{x\to \infty}xf(x^2+1)=2$ then find    $$\lim_{x\to 0}\dfrac{2f'(1/x)}{x\sqrt{x}}=?$$ My Try : $$g(x):=xf(x^2+1)\\g'(x)=f(x^2+1)+2xf'(x^2+1)$$ Now what?,,"['calculus', 'limits']"
81,"Limit of $\frac{\sin(x)}{x}$, as $x \rightarrow \infty$","Limit of , as",\frac{\sin(x)}{x} x \rightarrow \infty,"Recently, I showed my Calculus students how to show that $$  \lim_{x \to \infty} \frac{\sin(x)}{x} = 0, $$ by using the squeeze theorem. An interesting question that I was asked several times was, ""how come we couldn't just conclude that it is zero, without using the squeeze theorem, since it is obvious that the limit is zero."" I told them that the function is still oscillating, even though the oscillations are tiny, and so we need to make it rigorous by applying the squeeze theorem.  They weren't convinced ... What else could I say to them?  Is there a nice counter-example / explanation? (No epsilon-delta arguments, please ... ) Thanks,","Recently, I showed my Calculus students how to show that $$  \lim_{x \to \infty} \frac{\sin(x)}{x} = 0, $$ by using the squeeze theorem. An interesting question that I was asked several times was, ""how come we couldn't just conclude that it is zero, without using the squeeze theorem, since it is obvious that the limit is zero."" I told them that the function is still oscillating, even though the oscillations are tiny, and so we need to make it rigorous by applying the squeeze theorem.  They weren't convinced ... What else could I say to them?  Is there a nice counter-example / explanation? (No epsilon-delta arguments, please ... ) Thanks,",,"['calculus', 'limits']"
82,Prove convergence of the series,Prove convergence of the series,,"Help please to prove the convergence: $$\sum_{n=1}^{\infty}\left(\frac{1}{\sin \frac{1}{n}}-n\cos\frac1n\right)\cos \frac{\pi n(n-1)}2$$ It can be proved with Dirichlet's test, but there are come problems with $$\left(\frac{1}{\sin \frac{1}{n}}-n\cos\frac1n\right)$$ monotone. Other steps in this way are clear. Or I should use another test?","Help please to prove the convergence: $$\sum_{n=1}^{\infty}\left(\frac{1}{\sin \frac{1}{n}}-n\cos\frac1n\right)\cos \frac{\pi n(n-1)}2$$ It can be proved with Dirichlet's test, but there are come problems with $$\left(\frac{1}{\sin \frac{1}{n}}-n\cos\frac1n\right)$$ monotone. Other steps in this way are clear. Or I should use another test?",,"['sequences-and-series', 'limits', 'trigonometry', 'convergence-divergence', 'monotone-functions']"
83,Exercise: Am I doing this correctly?,Exercise: Am I doing this correctly?,,"I'm wondering if my approach to solving this problem is correct and not violating any rules.  So in an exercise I was asked to check if the limit: $$\lim_{\bar x\to \bar 0} \frac{\ln(1+\vert \bar x \vert^2)}{\vert \bar x \vert^2 + \sin(x_1x_2x_3)}$$ exists and if so what it is,  where  $\bar x = (x_1,x_2,x_3)$  and  $\vert \bar x \vert = \sqrt {x_1^2 + x_2^2 + x_3^2} $ Instead of checking different approaching curves I tried to do it by changing to polar coordinates:  $x_1=r \cdot \sin \theta \cdot \cos \varphi $ $x_2=r \cdot \sin \theta \cdot \sin \varphi $ $x_3=r \cdot \cos \theta$ So you instead get the limit: $$\lim_{r\to 0^+}\frac {\ln(1+r^2)}{r^2+\sin (r^3\cdot \lambda)}$$ where $\lambda=\sin (\theta)^2\cos\theta\cos\varphi\sin\varphi$ By then using l'Hopitals rule I got:\begin{align}\lim_{r\to 0^+}\frac {\frac {2r}{1+r^2}}{2r + \lambda 3r^2\cos (r^3\cdot\lambda)}&=\lim_{r\to 0^+}\frac {1}{(1+r^2)\cdot (1 + 3\lambda r \cdot \cos(r^3\lambda))}\\&=\lim_{r\to 0^+}\frac {1}{1+r^2 +  3\lambda r \cdot \cos(r^3\lambda))+3\lambda r^3 \cdot \cos(r^3\lambda))}\\&=1\end{align} If my approach is correct I wonder if I can solve this without l'Hopital maybe with equalities? .","I'm wondering if my approach to solving this problem is correct and not violating any rules.  So in an exercise I was asked to check if the limit: $$\lim_{\bar x\to \bar 0} \frac{\ln(1+\vert \bar x \vert^2)}{\vert \bar x \vert^2 + \sin(x_1x_2x_3)}$$ exists and if so what it is,  where  $\bar x = (x_1,x_2,x_3)$  and  $\vert \bar x \vert = \sqrt {x_1^2 + x_2^2 + x_3^2} $ Instead of checking different approaching curves I tried to do it by changing to polar coordinates:  $x_1=r \cdot \sin \theta \cdot \cos \varphi $ $x_2=r \cdot \sin \theta \cdot \sin \varphi $ $x_3=r \cdot \cos \theta$ So you instead get the limit: $$\lim_{r\to 0^+}\frac {\ln(1+r^2)}{r^2+\sin (r^3\cdot \lambda)}$$ where $\lambda=\sin (\theta)^2\cos\theta\cos\varphi\sin\varphi$ By then using l'Hopitals rule I got:\begin{align}\lim_{r\to 0^+}\frac {\frac {2r}{1+r^2}}{2r + \lambda 3r^2\cos (r^3\cdot\lambda)}&=\lim_{r\to 0^+}\frac {1}{(1+r^2)\cdot (1 + 3\lambda r \cdot \cos(r^3\lambda))}\\&=\lim_{r\to 0^+}\frac {1}{1+r^2 +  3\lambda r \cdot \cos(r^3\lambda))+3\lambda r^3 \cdot \cos(r^3\lambda))}\\&=1\end{align} If my approach is correct I wonder if I can solve this without l'Hopital maybe with equalities? .",,"['limits', 'multivariable-calculus']"
84,Question on limit of power of rational functions,Question on limit of power of rational functions,,"Let $k$ be a fixed number and suppose that $q_k$ is a polynomial with rational coefficients. In other words $$q_k(y) = \sum_{i=0}^{k}b_i y^i$$ where $b_i, \quad 0 \le i \le k$ are rationals. Here is my question: I need want to show that either $$\lim_{n \rightarrow \infty} \displaystyle \frac{2^n}{\displaystyle e^{q_k(n)}} = 0$$ or $$\lim_{n \rightarrow \infty} \displaystyle \frac{\displaystyle e^{q_k(n)}}{2^n} = 0.$$ I know that $$ \lim_{n \rightarrow \infty} \frac{1}{2^n} = 0$$ so the first thing that comes to my mind is that $$\lim_{n \rightarrow \infty} \displaystyle \frac{\displaystyle e^{q_k(n)}}{2^n}$$  has to be zero if the $\lim_{n \rightarrow \infty}\displaystyle e^{q_k(n)}$ is finite which I think is infinty. How can I approach this question? Any help on this will be great.","Let $k$ be a fixed number and suppose that $q_k$ is a polynomial with rational coefficients. In other words $$q_k(y) = \sum_{i=0}^{k}b_i y^i$$ where $b_i, \quad 0 \le i \le k$ are rationals. Here is my question: I need want to show that either $$\lim_{n \rightarrow \infty} \displaystyle \frac{2^n}{\displaystyle e^{q_k(n)}} = 0$$ or $$\lim_{n \rightarrow \infty} \displaystyle \frac{\displaystyle e^{q_k(n)}}{2^n} = 0.$$ I know that $$ \lim_{n \rightarrow \infty} \frac{1}{2^n} = 0$$ so the first thing that comes to my mind is that $$\lim_{n \rightarrow \infty} \displaystyle \frac{\displaystyle e^{q_k(n)}}{2^n}$$  has to be zero if the $\lim_{n \rightarrow \infty}\displaystyle e^{q_k(n)}$ is finite which I think is infinty. How can I approach this question? Any help on this will be great.",,"['calculus', 'real-analysis', 'limits', 'derivatives']"
85,Find $\lim\limits_{n\rightarrow \infty} |\frac{a_n}{b_n}|$,Find,\lim\limits_{n\rightarrow \infty} |\frac{a_n}{b_n}|,"Suppose that $\lim\limits_{n\rightarrow \infty} |\frac{a_{n+1}}{a_n}| = \frac{1}{\alpha}$, $\lim\limits_{n\rightarrow \infty} |\frac{b_{n+1}}{b_n}| = \frac{1}{\beta}$ and $\alpha > \beta$. Does it implies that $\lim\limits_{n\rightarrow \infty} |\frac{a_n}{b_n}| = 0 ?$ I think it is correct because the condition means that increasing rate of $b_n$ greater than increasing rate of $a_n$. Then $\lim\limits_{n\rightarrow \infty} |\frac{a_n}{b_n}| = 0 $ no matter what initial value $a_0$ and $b_0$ are given.","Suppose that $\lim\limits_{n\rightarrow \infty} |\frac{a_{n+1}}{a_n}| = \frac{1}{\alpha}$, $\lim\limits_{n\rightarrow \infty} |\frac{b_{n+1}}{b_n}| = \frac{1}{\beta}$ and $\alpha > \beta$. Does it implies that $\lim\limits_{n\rightarrow \infty} |\frac{a_n}{b_n}| = 0 ?$ I think it is correct because the condition means that increasing rate of $b_n$ greater than increasing rate of $a_n$. Then $\lim\limits_{n\rightarrow \infty} |\frac{a_n}{b_n}| = 0 $ no matter what initial value $a_0$ and $b_0$ are given.",,"['calculus', 'sequences-and-series', 'limits']"
86,How to determine if a $\lim\limits_{n \rightarrow \infty}{(1+{ix\over n})^n}$ would be complex [duplicate],How to determine if a  would be complex [duplicate],\lim\limits_{n \rightarrow \infty}{(1+{ix\over n})^n},"This question already has answers here : Do the polynomials $(1+z/n)^n$ converge compactly to $e^z$ on $\mathbb{C}$? (3 answers) How to prove Euler's formula: $e^{i\varphi}=\cos(\varphi) +i\sin(\varphi)$? (17 answers) Closed 6 years ago . Question Recently, I have been looking at complex limits, The most famous being $e^{ix}$=$\lim\limits_{n \rightarrow \infty}{(1+{ix\over n})^n}$. An example would be that when $x = \pi$ we know that the answer will be -1. But how do you determine this due to the fact that you can always $+1$ which will determine the outcome. I am fully aware that you are able to do this via the $i\cdot \sin(a \ln b) +\cos(a\ln b)$ however, how can you prove this via a limit, because if you test it on a calculator, most of the time you'll end up with some imaginary part. Specifically I have been looking at the representation of $\sin x={ie^{-ix}\over 2}-{ie^{ix}\over 2}$. Everyone would be safe to assume that $\sin x$ is always real, but when you apply a limit then how can you determine if it is only real or imaginary and real?","This question already has answers here : Do the polynomials $(1+z/n)^n$ converge compactly to $e^z$ on $\mathbb{C}$? (3 answers) How to prove Euler's formula: $e^{i\varphi}=\cos(\varphi) +i\sin(\varphi)$? (17 answers) Closed 6 years ago . Question Recently, I have been looking at complex limits, The most famous being $e^{ix}$=$\lim\limits_{n \rightarrow \infty}{(1+{ix\over n})^n}$. An example would be that when $x = \pi$ we know that the answer will be -1. But how do you determine this due to the fact that you can always $+1$ which will determine the outcome. I am fully aware that you are able to do this via the $i\cdot \sin(a \ln b) +\cos(a\ln b)$ however, how can you prove this via a limit, because if you test it on a calculator, most of the time you'll end up with some imaginary part. Specifically I have been looking at the representation of $\sin x={ie^{-ix}\over 2}-{ie^{ix}\over 2}$. Everyone would be safe to assume that $\sin x$ is always real, but when you apply a limit then how can you determine if it is only real or imaginary and real?",,"['limits', 'complex-numbers', 'exponential-function']"
87,About $\int_0^1 f_n(x)dx$ when $f_n$ is $f:x\mapsto2x(1-x)$ composed $n$ times with itself,About  when  is  composed  times with itself,\int_0^1 f_n(x)dx f_n f:x\mapsto2x(1-x) n,"The following question is taken from here exercise $4:$ Let $f(x) = 2x(1-x),x\in\mathbb{R}.$ Define    $$f_n = f \circ f \circ ... \circ f (n \text{ times}), f_n(x)=f(f(...f(x)...)).$$   (a) Find $$\lim_{n\rightarrow\infty} \int_0^1 f_n(x)dx.$$   (b) Compute the integral $$\int_0^1 f_n(x)dx.$$ I use Wolfram Alpha to obtain the answer $\frac{1}{2}$ for (a) and $\frac{2^{n-1}}{1+2^n}$ for (b). However, I have no idea on how to get close to answer. For (a), I try to evaluate the composition directly. However, I have trouble evaluating when $n=3.$ I think we need to interchange the limit and integral but even after that I have no idea.","The following question is taken from here exercise $4:$ Let $f(x) = 2x(1-x),x\in\mathbb{R}.$ Define    $$f_n = f \circ f \circ ... \circ f (n \text{ times}), f_n(x)=f(f(...f(x)...)).$$   (a) Find $$\lim_{n\rightarrow\infty} \int_0^1 f_n(x)dx.$$   (b) Compute the integral $$\int_0^1 f_n(x)dx.$$ I use Wolfram Alpha to obtain the answer $\frac{1}{2}$ for (a) and $\frac{2^{n-1}}{1+2^n}$ for (b). However, I have no idea on how to get close to answer. For (a), I try to evaluate the composition directly. However, I have trouble evaluating when $n=3.$ I think we need to interchange the limit and integral but even after that I have no idea.",,"['real-analysis', 'integration', 'limits', 'contest-math']"
88,Limit of product of sequences is the product of the limits of the sequences,Limit of product of sequences is the product of the limits of the sequences,,"I want to prove that if: $$\lim_{n \to \infty}s_n = L_1, \lim_{n \to \infty}t_n = L_2$$ then $$\lim_{n \to \infty}(s_n t_n) = L_1L_2$$ where $s_n, t_n$ are complex sequences (I'm working through Rudin's baby rudin, and I did this proof slightly different than him, so I would like it if someone took the time to look through my proof and verify whether it's correct) My attempt (scratch proof: finding the appropriate epsilons etc): By definition of limit, there are positive integers $N_1,N_2$ such that: $$n >N_1 \Rightarrow |s_n - L_1| <\epsilon'$$ $$n >N_2 \Rightarrow |t_n - L_2| <\epsilon''$$ where we can freely choose $\epsilon', \epsilon'' >0$ Let $\epsilon > 0$ Now, if $n > \max\{N_1,N_2\}$, then: $$|s_nt_n - L_1L_2| = |s_n(t_n - L_2) + s_nL_2 - L_1L_2|$$ $$\leq |s_n||t_n - L_2| + |L_2||s_n - L_1|$$ $$\leq |s_n|\epsilon'' + |L_2|\epsilon'$$ We want this expression to be smaller than $\epsilon$, but we can't make our $\epsilon$ depend on $n$, therefore we have to find an upper bound for $|s_n|$ There is a positive integer $N_3$ such that: $$n > N_3 \Rightarrow |s_n - L_1| < 1$$ $$\Rightarrow |s_n| < 1 + |L_1|$$ so for $n > \max\{N_1,N_2,N_3\}$, we have: $$ |s_n|\epsilon'' + |L_2|\epsilon' < (1 + |L_1|)\epsilon'' + |L_2|\epsilon'$$ and by chosing $\epsilon'' = \epsilon' = \frac{\epsilon}{1+|L_1|+|L_2|}$, the expression becomes smaller than $\epsilon$ Rigorous proof Let $\epsilon > 0$ By definition of limit, there are positive integers $N_1,N_2, N_3$ such that: $$n >N_1 \Rightarrow |s_n - L_1| <\frac{\epsilon}{1+|L_1|+|L_2|}$$ $$n >N_2 \Rightarrow |t_n - L_2| <\frac{\epsilon}{1+|L_1|+|L_2|}$$ $$n > N_3 \Rightarrow |s_n - L_1| < 1 \Rightarrow |s_n| < 1 + |L_1|$$ and for $n > \max\{N_1,N_2,N_3\}$, we have: $$|s_nt_n - L_1L_2| = |s_n(t_n - L_2) + s_nL_2 - L_1L_2|$$ $$\leq |s_n||t_n - L_2| + |L_2||s_n - L_1|$$ $$< (1 + |L_1|)\frac{\epsilon}{1+|L_1|+|L_2|} + |L_2|\frac{\epsilon}{1+|L_1|+|L_2|} = \frac{\epsilon}{1+|L_1|+|L_2|}(1+ |L_1| + |L_2|) = \epsilon$$ Alternative proof: Let $\epsilon > 0$ $(s_n)$ converges, so $(|s_n|)$ converges and it is bounded, meaning that there is a positive real number $S$ such that $|s_n| \leq S$ for every positive $n$. By definition of limit, there are positive integers $N_1,N_2$ such that: $$n >N_1 \Rightarrow |s_n - L_1| <\frac{\epsilon}{S + |L_2|}$$ $$n >N_2 \Rightarrow |t_n - L_2| <\frac{\epsilon}{S + |L_2|}$$ Now, for $n > \max\{N_1,N_2\}$, we have: $$|s_nt_n - L_1L_2| = |s_n(t_n - L_2) + s_nL_2 - L_1L_2|$$ $$\leq |s_n||t_n - L_2| + |L_2||s_n - L_1|$$ $$< (S+|L_2|)\frac{\epsilon}{S + |L_2|} = \epsilon $$ Note that this wouldn't work whenever $S + |L_2| = 0$. However, if $S=0$, then $s_n$ is the null-sequence and the theorem is trivial, so we can safely assume $S \neq 0$ such that the denominator never becomes $0$. QED Is this correct?","I want to prove that if: $$\lim_{n \to \infty}s_n = L_1, \lim_{n \to \infty}t_n = L_2$$ then $$\lim_{n \to \infty}(s_n t_n) = L_1L_2$$ where $s_n, t_n$ are complex sequences (I'm working through Rudin's baby rudin, and I did this proof slightly different than him, so I would like it if someone took the time to look through my proof and verify whether it's correct) My attempt (scratch proof: finding the appropriate epsilons etc): By definition of limit, there are positive integers $N_1,N_2$ such that: $$n >N_1 \Rightarrow |s_n - L_1| <\epsilon'$$ $$n >N_2 \Rightarrow |t_n - L_2| <\epsilon''$$ where we can freely choose $\epsilon', \epsilon'' >0$ Let $\epsilon > 0$ Now, if $n > \max\{N_1,N_2\}$, then: $$|s_nt_n - L_1L_2| = |s_n(t_n - L_2) + s_nL_2 - L_1L_2|$$ $$\leq |s_n||t_n - L_2| + |L_2||s_n - L_1|$$ $$\leq |s_n|\epsilon'' + |L_2|\epsilon'$$ We want this expression to be smaller than $\epsilon$, but we can't make our $\epsilon$ depend on $n$, therefore we have to find an upper bound for $|s_n|$ There is a positive integer $N_3$ such that: $$n > N_3 \Rightarrow |s_n - L_1| < 1$$ $$\Rightarrow |s_n| < 1 + |L_1|$$ so for $n > \max\{N_1,N_2,N_3\}$, we have: $$ |s_n|\epsilon'' + |L_2|\epsilon' < (1 + |L_1|)\epsilon'' + |L_2|\epsilon'$$ and by chosing $\epsilon'' = \epsilon' = \frac{\epsilon}{1+|L_1|+|L_2|}$, the expression becomes smaller than $\epsilon$ Rigorous proof Let $\epsilon > 0$ By definition of limit, there are positive integers $N_1,N_2, N_3$ such that: $$n >N_1 \Rightarrow |s_n - L_1| <\frac{\epsilon}{1+|L_1|+|L_2|}$$ $$n >N_2 \Rightarrow |t_n - L_2| <\frac{\epsilon}{1+|L_1|+|L_2|}$$ $$n > N_3 \Rightarrow |s_n - L_1| < 1 \Rightarrow |s_n| < 1 + |L_1|$$ and for $n > \max\{N_1,N_2,N_3\}$, we have: $$|s_nt_n - L_1L_2| = |s_n(t_n - L_2) + s_nL_2 - L_1L_2|$$ $$\leq |s_n||t_n - L_2| + |L_2||s_n - L_1|$$ $$< (1 + |L_1|)\frac{\epsilon}{1+|L_1|+|L_2|} + |L_2|\frac{\epsilon}{1+|L_1|+|L_2|} = \frac{\epsilon}{1+|L_1|+|L_2|}(1+ |L_1| + |L_2|) = \epsilon$$ Alternative proof: Let $\epsilon > 0$ $(s_n)$ converges, so $(|s_n|)$ converges and it is bounded, meaning that there is a positive real number $S$ such that $|s_n| \leq S$ for every positive $n$. By definition of limit, there are positive integers $N_1,N_2$ such that: $$n >N_1 \Rightarrow |s_n - L_1| <\frac{\epsilon}{S + |L_2|}$$ $$n >N_2 \Rightarrow |t_n - L_2| <\frac{\epsilon}{S + |L_2|}$$ Now, for $n > \max\{N_1,N_2\}$, we have: $$|s_nt_n - L_1L_2| = |s_n(t_n - L_2) + s_nL_2 - L_1L_2|$$ $$\leq |s_n||t_n - L_2| + |L_2||s_n - L_1|$$ $$< (S+|L_2|)\frac{\epsilon}{S + |L_2|} = \epsilon $$ Note that this wouldn't work whenever $S + |L_2| = 0$. However, if $S=0$, then $s_n$ is the null-sequence and the theorem is trivial, so we can safely assume $S \neq 0$ such that the denominator never becomes $0$. QED Is this correct?",,['real-analysis']
89,Convergence of a sequence defined by $ c_{n+1}=\frac{r_{n+1}\sqrt{c_n}}{3}$,Convergence of a sequence defined by, c_{n+1}=\frac{r_{n+1}\sqrt{c_n}}{3},"Define a sequence $(r_n)$ by $r_0=1$ and $r_{n+1}=(2/3)r_n+1$ for $n\geq 0$. Let the sequence $(c_n)$ be defined by $c_0=1/4$, and   $$ c_{n+1}=\frac{r_{n+1}\sqrt{c_n}}{3} $$   for $n\geq 0$. Prove that $\lim_{n\to\infty}c_n$ exists. It is not difficult to prove that the sequence $(r_n)$ converges to $3$ since it is bounded above and non-decreasing. I don't have a more useful estimate for $(c_n)$ than $c_{n+1}\leq \sqrt{c_n}$. How shall I go on?","Define a sequence $(r_n)$ by $r_0=1$ and $r_{n+1}=(2/3)r_n+1$ for $n\geq 0$. Let the sequence $(c_n)$ be defined by $c_0=1/4$, and   $$ c_{n+1}=\frac{r_{n+1}\sqrt{c_n}}{3} $$   for $n\geq 0$. Prove that $\lim_{n\to\infty}c_n$ exists. It is not difficult to prove that the sequence $(r_n)$ converges to $3$ since it is bounded above and non-decreasing. I don't have a more useful estimate for $(c_n)$ than $c_{n+1}\leq \sqrt{c_n}$. How shall I go on?",,['real-analysis']
90,Solving the limit: $\lim_{x\to0}\left[100\frac{\sin^{-1}(x)}{x}\right]+\left[100\frac{\tan^{-1}(x)}{x}\right]$,Solving the limit:,\lim_{x\to0}\left[100\frac{\sin^{-1}(x)}{x}\right]+\left[100\frac{\tan^{-1}(x)}{x}\right],Find the value of the limit $$\lim_{x\to0}\left[100\frac{\sin^{-1}(x)}{x}\right]+\left[100\frac{\tan^{-1}(x)}{x}\right]$$ where $[\cdot]$ denotes the greatest integer function or the box function. My attempt: I am aware of the standard limits $$\lim_{x\to0}\left(\frac{\sin^{-1}(x)}{x} \right) = 1 $$ and $$\lim_{x\to0}\left(\frac{\tan^{-1}(x)}{x} \right) = 1 $$ but am not sure how will I apply the box function on this limit. Any detailed explanation to help me understand this concept will be appreciated.,Find the value of the limit $$\lim_{x\to0}\left[100\frac{\sin^{-1}(x)}{x}\right]+\left[100\frac{\tan^{-1}(x)}{x}\right]$$ where $[\cdot]$ denotes the greatest integer function or the box function. My attempt: I am aware of the standard limits $$\lim_{x\to0}\left(\frac{\sin^{-1}(x)}{x} \right) = 1 $$ and $$\lim_{x\to0}\left(\frac{\tan^{-1}(x)}{x} \right) = 1 $$ but am not sure how will I apply the box function on this limit. Any detailed explanation to help me understand this concept will be appreciated.,,"['limits', 'limits-without-lhopital']"
91,Prove that $\int_0^x e^{x^2}$ ~ $ \frac{1}{2x}e^{x^2}$ as $x \to \infty$,Prove that  ~  as,\int_0^x e^{x^2}  \frac{1}{2x}e^{x^2} x \to \infty,My solution is: Equivalence means that the limit of quotient is equal to zero. Apply L'Hôpital's rule: $$\lim \limits_{x\to \infty} \frac{\int \limits_0^x e^{x^2}}{\frac{1}{2x}e^{x^2}} = \lim \limits_{x\to \infty} \frac{e^{x^2}}{e^{x^2}(1 - \frac{1}{2x^2})} = \lim \limits_{x\to \infty} \frac{1}{1} = 1$$ Am I missing something in my solution or is it ok?,My solution is: Equivalence means that the limit of quotient is equal to zero. Apply L'Hôpital's rule: $$\lim \limits_{x\to \infty} \frac{\int \limits_0^x e^{x^2}}{\frac{1}{2x}e^{x^2}} = \lim \limits_{x\to \infty} \frac{e^{x^2}}{e^{x^2}(1 - \frac{1}{2x^2})} = \lim \limits_{x\to \infty} \frac{1}{1} = 1$$ Am I missing something in my solution or is it ok?,,"['calculus', 'real-analysis', 'integration', 'limits']"
92,"Taylor series has bounds, but what about Laurent series?","Taylor series has bounds, but what about Laurent series?",,"For a Taylor series, we have Lagrange remainders: $$\left|f(x)-\sum_{k=0}^n\frac{f^{(k)}(c)}{k!}(x-a)^k\right|\le\frac{|f^{(n+1)}(\xi)|}{(n+1)!}|x-c|^{n+1},\ |\xi-c|\le x$$ But do we have a similar thing for Laurent series? $$\left|f(x)-\sum_{k=-n}^na_k(x-c)^k\right|\le~?$$ $$\left|f(x)-\sum_{k=-\infty}^na_k(x-c)^k\right|\le~?$$ $$\left|f(x)-\sum_{k=-n}^{+\infty}a_k(x-c)^k\right|\le~?$$ Such would be fairly useful for solving limits with a squeeze theoerem, for example.","For a Taylor series, we have Lagrange remainders: $$\left|f(x)-\sum_{k=0}^n\frac{f^{(k)}(c)}{k!}(x-a)^k\right|\le\frac{|f^{(n+1)}(\xi)|}{(n+1)!}|x-c|^{n+1},\ |\xi-c|\le x$$ But do we have a similar thing for Laurent series? $$\left|f(x)-\sum_{k=-n}^na_k(x-c)^k\right|\le~?$$ $$\left|f(x)-\sum_{k=-\infty}^na_k(x-c)^k\right|\le~?$$ $$\left|f(x)-\sum_{k=-n}^{+\infty}a_k(x-c)^k\right|\le~?$$ Such would be fairly useful for solving limits with a squeeze theoerem, for example.",,"['limits', 'taylor-expansion', 'laurent-series', 'upper-lower-bounds']"
93,$ \lim_{x \to \infty } ( \sqrt[3]{4 x^{a} + x^{2} } - \sqrt[3]{ x^{a} + x^{2} } )^{x-[x]} $,, \lim_{x \to \infty } ( \sqrt[3]{4 x^{a} + x^{2} } - \sqrt[3]{ x^{a} + x^{2} } )^{x-[x]} ,"fine limit : $$ \lim_{x \to  \infty } ( \sqrt[3]{4 x^{a} + x^{2} } - \sqrt[3]{ x^{a} + x^{2} } )^{x-[x]} $$ such that : $$ a \in (0,2)$$ and : $[x]: \ \ $  floor function My Try : $$f(x):=( \sqrt[3]{4 x^{a} + x^{2} } - \sqrt[3]{ x^{a} + x^{2} } )^{x-[x]}$$ $$\ln f(x)=(x-[x])\ln(\sqrt[3]{4 x^{a} + x^{2} } - \sqrt[3]{ x^{a} + x^{2} } )$$ Now ?please help","fine limit : $$ \lim_{x \to  \infty } ( \sqrt[3]{4 x^{a} + x^{2} } - \sqrt[3]{ x^{a} + x^{2} } )^{x-[x]} $$ such that : $$ a \in (0,2)$$ and : $[x]: \ \ $  floor function My Try : $$f(x):=( \sqrt[3]{4 x^{a} + x^{2} } - \sqrt[3]{ x^{a} + x^{2} } )^{x-[x]}$$ $$\ln f(x)=(x-[x])\ln(\sqrt[3]{4 x^{a} + x^{2} } - \sqrt[3]{ x^{a} + x^{2} } )$$ Now ?please help",,['limits']
94,Limit superior from VJIMC 2016 T-shirt,Limit superior from VJIMC 2016 T-shirt,,"Previous year on VJIMC competition we got T-shirts with following problem Define $$f(\alpha):=\limsup\limits_{n\to\infty,\,n\in\mathbb{N}}(\sin(n))^{n^\alpha}.$$ Find $f(1)$ and $f(2)$. I found some topics dealing with $$\lim_{n\to\infty}(\sin(n))^n,$$ but that wasn't much helpful (or I just overlooked important parts). Any advice how to solve it? Thx.","Previous year on VJIMC competition we got T-shirts with following problem Define $$f(\alpha):=\limsup\limits_{n\to\infty,\,n\in\mathbb{N}}(\sin(n))^{n^\alpha}.$$ Find $f(1)$ and $f(2)$. I found some topics dealing with $$\lim_{n\to\infty}(\sin(n))^n,$$ but that wasn't much helpful (or I just overlooked important parts). Any advice how to solve it? Thx.",,"['real-analysis', 'limits', 'contest-math', 'limsup-and-liminf']"
95,Limit of sum when solving random graph problem,Limit of sum when solving random graph problem,,The main problem is to prove that $$   \lim\limits_{n \to +\infty}\Bigg( \frac{1}{\log n} \cdot \sum\limits_{k = 3}^n \frac{n!}{(n - k)!\cdot k \cdot n^k}\Bigg) = \frac12 $$ It is easy to prove that this limit is not bigger than 1 but every attempt to have better result was in vain.,The main problem is to prove that It is easy to prove that this limit is not bigger than 1 but every attempt to have better result was in vain.,"
  \lim\limits_{n \to +\infty}\Bigg( \frac{1}{\log n} \cdot \sum\limits_{k = 3}^n \frac{n!}{(n - k)!\cdot k \cdot n^k}\Bigg) = \frac12
","['calculus', 'limits', 'random-graphs']"
96,Evaluating $\lim\limits_{b\to\infty}b\int_0^1\cos(b x) \cosh^{-1}(\frac{1}{x})dx$,Evaluating,\lim\limits_{b\to\infty}b\int_0^1\cos(b x) \cosh^{-1}(\frac{1}{x})dx,"I came across this limit while integrating Bessel functions: $$\lim_{b\to\infty}b\int_0^1\cos(b x) \cosh^{-1}(\frac{1}{x})dx$$ This integral does not have any standard value I know of for fixed $b$. Graphing it numerically, it appears to converge to $\pi/2$ as $b\to\infty$ (albeit slowly). Does anyone have any ideas for evaluating such an integral/limit?","I came across this limit while integrating Bessel functions: $$\lim_{b\to\infty}b\int_0^1\cos(b x) \cosh^{-1}(\frac{1}{x})dx$$ This integral does not have any standard value I know of for fixed $b$. Graphing it numerically, it appears to converge to $\pi/2$ as $b\to\infty$ (albeit slowly). Does anyone have any ideas for evaluating such an integral/limit?",,"['integration', 'limits', 'definite-integrals', 'improper-integrals', 'hyperbolic-functions']"
97,"Prove $\lim_{(x,y) \to (-1,8)} xy = -8$ using only the definition.",Prove  using only the definition.,"\lim_{(x,y) \to (-1,8)} xy = -8","I'm having problems trying to prove this limit using only the definition with delta and epsilon. I need to see that: $$ \forall\epsilon \  \exists \delta \ : \sqrt{(x+1)^2+(y-8)^2} < \delta \Rightarrow |xy+8|<\epsilon$$ I want to make $|xy+8|$ look like the first half. I start by replacing $t=x+1$, so $x=t-1$ and $s=y-8$, so $y=s+8$. So I get: $$ |(t-1)(s+8)+8| = |ts+8t-s-8+8| = |s(t-1)+8t| \leq |s(t-1)|+8|t|$$ Here I'm stuck, if I use that $\delta < 1$ then I can say that $t-1 > 0$ and since $|s(t-1)| = |s||t-1|$ then $|s||t-1| = |s|t-|s|$, which means I get: $$ |(t-1)(s+8)+8| \leq |s|t-|s|+8|t| \leq |s|t+8|t| $$ I think I need to bound it by $||(s,t)||$, so I can add/substract and simplify, but I can't see how to do it. Any tips you can give me?","I'm having problems trying to prove this limit using only the definition with delta and epsilon. I need to see that: $$ \forall\epsilon \  \exists \delta \ : \sqrt{(x+1)^2+(y-8)^2} < \delta \Rightarrow |xy+8|<\epsilon$$ I want to make $|xy+8|$ look like the first half. I start by replacing $t=x+1$, so $x=t-1$ and $s=y-8$, so $y=s+8$. So I get: $$ |(t-1)(s+8)+8| = |ts+8t-s-8+8| = |s(t-1)+8t| \leq |s(t-1)|+8|t|$$ Here I'm stuck, if I use that $\delta < 1$ then I can say that $t-1 > 0$ and since $|s(t-1)| = |s||t-1|$ then $|s||t-1| = |s|t-|s|$, which means I get: $$ |(t-1)(s+8)+8| \leq |s|t-|s|+8|t| \leq |s|t+8|t| $$ I think I need to bound it by $||(s,t)||$, so I can add/substract and simplify, but I can't see how to do it. Any tips you can give me?",,"['limits', 'multivariable-calculus', 'proof-verification', 'epsilon-delta']"
98,Limit of ratio of integrals,Limit of ratio of integrals,,"Consider three continuous functions $f,u,v:[0,1] \rightarrow (0,1)$ and a sequence of integers $0 \leq a_n \leq n $ such that \begin{equation} \lim_{n \rightarrow + \infty}{\frac{a_n}{n}}=\alpha \in (0,1) \end{equation} Suppose that there is a unique $y$ (resp. $z$) such that $u(y)=\alpha$ (resp. $v(z)=\alpha$). I am trying to show that \begin{equation} \lim_{n \rightarrow +\infty} \frac {\int{f(x) u(x)^{a_n} (1-u (x))^{n-a_n}}dx}{\int{f(x) v(x)^{a_n} (1-v(x))^{n-a_n}} dx} = \frac{f(y)}{f(z)} \end{equation} The reason why I suspect that this property might be valid is because an analogous result holds true in the discrete case. Take $N$ real numbers $(x_1,\cdots,x_N)$ in $[0,1]$ and three sequences $(f_1,\cdots,f_N)$, $(u_1,\cdots,u_N)$ and $(v_1,\cdots,v_N)$ with values in $(0,1)$. Suppose that there is a unique index $i \in \{1,\cdots,N\}$ (resp. $j \in \{1,\cdots,N\}$) such that $u_i=\alpha$ (resp. $v_j=\alpha$). Then it is easy to show that for all $k \ne i$ \begin{equation*} \lim_{n \rightarrow +\infty} \dfrac{f_k u_k^{a_n} (1-u_{k})^{1-a_n}}{f_i u_{i}^{a_n} (1-u_{i})^{1-a_n}} = 0 \end{equation*} And therefore \begin{equation*} \displaystyle \sum_{k=1}^{N}{f_k u_k^{a_n} (1-u_{k})^{1-a_n}} \displaystyle \sim f_i \alpha^{a_n} (1-\alpha)^{1-a_n} \end{equation*} Similarly, \begin{equation*} \sum_{k=1}^{N}{f_k v_k^{a_n} (1-v_{k})^{1-a_n}} \sim f_j \alpha^{a_n} (1-\alpha)^{1-a_n} \end{equation*} And thus \begin{equation*} \dfrac{\sum_{k=1}^{N}{f_k u_k^{a_n} (1-u_{k})^{1-a_n}}}{\sum_{k=1}^{N}{f_k v_k^{a_n} (1-v_{k})^{1-a_n}}} \sim \dfrac{f_i}{f_j} \end{equation*} I have been unable to make any progress in the continuous case. Any advice would be much appreciated. Thanks!","Consider three continuous functions $f,u,v:[0,1] \rightarrow (0,1)$ and a sequence of integers $0 \leq a_n \leq n $ such that \begin{equation} \lim_{n \rightarrow + \infty}{\frac{a_n}{n}}=\alpha \in (0,1) \end{equation} Suppose that there is a unique $y$ (resp. $z$) such that $u(y)=\alpha$ (resp. $v(z)=\alpha$). I am trying to show that \begin{equation} \lim_{n \rightarrow +\infty} \frac {\int{f(x) u(x)^{a_n} (1-u (x))^{n-a_n}}dx}{\int{f(x) v(x)^{a_n} (1-v(x))^{n-a_n}} dx} = \frac{f(y)}{f(z)} \end{equation} The reason why I suspect that this property might be valid is because an analogous result holds true in the discrete case. Take $N$ real numbers $(x_1,\cdots,x_N)$ in $[0,1]$ and three sequences $(f_1,\cdots,f_N)$, $(u_1,\cdots,u_N)$ and $(v_1,\cdots,v_N)$ with values in $(0,1)$. Suppose that there is a unique index $i \in \{1,\cdots,N\}$ (resp. $j \in \{1,\cdots,N\}$) such that $u_i=\alpha$ (resp. $v_j=\alpha$). Then it is easy to show that for all $k \ne i$ \begin{equation*} \lim_{n \rightarrow +\infty} \dfrac{f_k u_k^{a_n} (1-u_{k})^{1-a_n}}{f_i u_{i}^{a_n} (1-u_{i})^{1-a_n}} = 0 \end{equation*} And therefore \begin{equation*} \displaystyle \sum_{k=1}^{N}{f_k u_k^{a_n} (1-u_{k})^{1-a_n}} \displaystyle \sim f_i \alpha^{a_n} (1-\alpha)^{1-a_n} \end{equation*} Similarly, \begin{equation*} \sum_{k=1}^{N}{f_k v_k^{a_n} (1-v_{k})^{1-a_n}} \sim f_j \alpha^{a_n} (1-\alpha)^{1-a_n} \end{equation*} And thus \begin{equation*} \dfrac{\sum_{k=1}^{N}{f_k u_k^{a_n} (1-u_{k})^{1-a_n}}}{\sum_{k=1}^{N}{f_k v_k^{a_n} (1-v_{k})^{1-a_n}}} \sim \dfrac{f_i}{f_j} \end{equation*} I have been unable to make any progress in the continuous case. Any advice would be much appreciated. Thanks!",,"['real-analysis', 'integration', 'limits']"
99,ε-δ: Proving that $\lim_{x\rightarrow a}f(x) = \lim_{h\rightarrow 0} f(a+h) $,ε-δ: Proving that,\lim_{x\rightarrow a}f(x) = \lim_{h\rightarrow 0} f(a+h) ,"Firstly, I do realize that this proof was asked about before here , but the proposed proof was different and the answers were also not complete. To get to the point: I have written my own proof of this simple theorem, however I'm not sure if the proof is correct--the exercise is taken from Spivak, who seems to have provided different and more elaborate proof of that theorem. My proof is following: By the definition of limit (omitting the 'for all's and 'there exists''): $$ \lim_{x\to a} f(x) = L_1 \implies 0 < |x - a| < δ_1 \implies |f(x) - L_1| < ε $$ and $$ \lim_{h\to 0} f(a + h) = L_2 \implies 0 < |h - 0| = |h| < δ_2 \implies |f(a + h) - L_2| < ε $$ Let's work on the second limit: $$|h| = |(a+h) - a|$$ $$\text{Let } y = a + h$$ Now, the second limit takes the following definition: $$ 0 < |y - a| < δ_2 \implies |f(y) - L_2| < ε $$ We see that both limits have the exact same form right now. By Theorem 1 [stating that if limit L exists it is necessarily unique] $L_1 = L_2$ and $\lim_{x\to a} f(x) = \lim_{h\to 0} f(a + h)$ $$Q.E.D.$$ Is my reasoning correct or have I made an error somewhere?","Firstly, I do realize that this proof was asked about before here , but the proposed proof was different and the answers were also not complete. To get to the point: I have written my own proof of this simple theorem, however I'm not sure if the proof is correct--the exercise is taken from Spivak, who seems to have provided different and more elaborate proof of that theorem. My proof is following: By the definition of limit (omitting the 'for all's and 'there exists''): $$ \lim_{x\to a} f(x) = L_1 \implies 0 < |x - a| < δ_1 \implies |f(x) - L_1| < ε $$ and $$ \lim_{h\to 0} f(a + h) = L_2 \implies 0 < |h - 0| = |h| < δ_2 \implies |f(a + h) - L_2| < ε $$ Let's work on the second limit: $$|h| = |(a+h) - a|$$ $$\text{Let } y = a + h$$ Now, the second limit takes the following definition: $$ 0 < |y - a| < δ_2 \implies |f(y) - L_2| < ε $$ We see that both limits have the exact same form right now. By Theorem 1 [stating that if limit L exists it is necessarily unique] $L_1 = L_2$ and $\lim_{x\to a} f(x) = \lim_{h\to 0} f(a + h)$ $$Q.E.D.$$ Is my reasoning correct or have I made an error somewhere?",,"['calculus', 'limits', 'epsilon-delta']"
